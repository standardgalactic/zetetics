www.it-ebooks.info

 
 
 
MEAP Edition 
Manning Early Access Program 
Functional Programming in Scala  
version 10 
 
 
 
 
 
 
 
 
 
Copyright 2013 Manning Publications 
 
For more information on this and other Manning titles go to  
www.manning.com 
www.it-ebooks.info

brief contents 
 
PART 1: INTRODUCTION TO FUNCTIONAL PROGRAMMING 
  1. What is functional programming? 
  2. Getting Started 
  3. Functional data structures 
  4. Handling errors without exceptions 
  5. Strictness and laziness 
  6. Purely functional state 
PART 2: FUNCTIONAL DESIGN AND COMBINATOR  LIBRARIES 
  7. Purely functional parallelism 
  8. Property-based testing 
  9. Parser combinators 
PART 3: FUNCTIONAL DESIGN PATTERNS 
 10. Monoids 
 11. Monads 
 12. Applicative and traversable functors 
PART 4: BREAKING THE RULES: EFFECTS AND I/O 
 13. External effects and I/O 
 14. Local effects and the ST monad 
 15. Stream processing and incremental I/O 
 
www.it-ebooks.info

P
This is not a book about Scala. This book introduces the concepts and techniques
of functional programming (FP)—we use Scala as the vehicle, but the lessons
herein can be applied to programming in any language. Our goal is to give you the
foundations to begin writing substantive functional programs and to comfortably
absorb new FP concepts and techniques beyond those covered here. Throughout
the book we rely heavily on programming exercises, carefully chosen and
sequenced to guide you to discover FP for yourself. Expository text is often just
enough to lead you to the next exercise. Do these exercises and you will learn the
material. Read without doing and you will find yourself lost.
A word of caution: no matter how long you've been programming, learning FP
is challenging. Come prepared to be a beginner once again. FP proceeds from a
startling premise—that we construct programs using only pure functions, or
functions that avoid 
 like writing to a database or reading from a file. In
side effects
the first chapter, we will explain exactly what this means. From this single idea and
its logical consequences emerges a very different way of building programs, one
with its own body of techniques and concepts. We start by relearning how to write
the simplest of programs in a functional way. From this foundation we will build
the tower of techniques necessary for expressing functional programs of greater
complexity. Some of these techniques may feel alien or unnatural at first and the
exercises and questions can be difficult, even brain-bending at times. This is
normal. Don't be deterred. Keep a beginner's mind, try to suspend judgment, and if
Preface
P.1 About this book
1
www.it-ebooks.info

you must be skeptical, don't let this skepticism get in the way of learning. When
you start to feel more fluent at expressing functional programs, then take a step
back and evaluate what you think of the FP approach.
This book does not require any prior experience with Scala, but we won't spend
a lot of time and space discussing Scala's syntax and language features. Instead
we'll introduce them as we go, with a minimum of ceremony, mostly using short
examples, and mostly as a consequence of covering other material. These minimal
introductions to Scala should be enough to get you started with the exercises. If
you have further questions about the Scala language while working on the
exercises, you are expected to do some research and experimentation on your own
or follow some of our links to further reading.
The book is organized into four parts, intended to be read sequentially. Part 1
introduces functional programming, explains what it is, why you should care, and
walks through the basic low-level techniques of FP, including how to organize and
structure small functional programs, define functional data structures, and handle
errors functionally. These techniques will be used as the building blocks for all
subsequent parts. Part 2 introduces functional design using a number of worked
examples of functional libraries. It will become clear that these libraries follow
certain patterns, which highlights the need for new cognitive tools for abstracting
and generalizing code—we introduce these tools and explore concepts related to
them in Part 3. Building on Part 3, Part 4 covers techniques and mechanisms for
writing functional programs that perform I/O (like reading/writing to a database,
files, or the screen) or writing to mutable variables.
Though the book can be read sequentially straight through, the material in Part
3 will make the most sense after you have a strong familiarity with the functional
style of programming developed over parts 1 and 2. After Part 2, it may therefore
be a good idea to take a break and try getting more practice writing functional
programs beyond the shorter exercises we work on throughout the chapters. Part 4
also builds heavily on the ideas and techniques of Part 3, so a second break after
Part 3 to get experience with these techniques in larger projects may be a good idea
before moving on. Of course, how you read this book is ultimately up to you, and
you are free to read ahead if you wish.
Most chapters in this book have similar structure. We introduce and explain
some new idea or technique with an example, then work through a number of
exercises, introducing further material via the exercises. The exercises thus serve
P.2 How to read this book
2
www.it-ebooks.info

two purposes: to help you to understand the ideas being discussed and to guide you
to discover for yourself new ideas that are relevant. Therefore we 
 suggest
strongly
that you download the exercise source code and do the exercises as you go through
each chapter. Exercises, hints and answers are all available at 
. We also encourage you to visit the 
https://github.com/pchiusano/fpinscala
 and the 
 IRC channel on 
scala-functional Google group
#fp-in-scala
 for questions and discussion.
irc.freenode.net
Exercises are marked for both their difficulty and to indicate whether they are
critical or noncritical. We will mark exercises that we think are 
 or that we
hard
consider to be 
 to understanding the material. The 
 designation is our
critical
hard
effort to give you some idea of what to expect—it is only our guess and you may
find some unmarked questions difficult and some questions marked 
 to be
hard
quite easy. The 
 designation is applied to exercises that address concepts
critical
that we will be building on and are therefore important to understand fully.
Noncritical exercises are still informative but can be skipped without impeding
your ability to follow further material.
Examples are given throughout the book and they are meant to be 
 rather
tried
than just read. Before you begin, you should have the Scala interpreter (REPL)
running and ready. We encourage you to experiment on your own with variations
of what you see in the examples. A good way to understand something is to change
it slightly and see how the change affects the outcome.
Sometimes we will show a REPL session to demonstrate the result of running
some code. This will be marked by lines beginning with the 
 prompt of
scala>
the REPL. Code that follows this prompt is to be typed or pasted into the
interpreter, and the line just below will show the interpreter's response, like this:
SIDEBAR
Sidebars
Occasionally throughout the book we will want to highlight the precise
definition of a concept in a sidebar like this one. This lets us give you a
complete and concise definition without breaking the flow of the main
text with overly formal language, and also makes it easy to refer back to
when needed.
There are chapter notes (which includes references to external resources) and
scala> println("Hello, World!")
          Hello, World!
3
www.it-ebooks.info

several appendix chapters after Part 4. Throughout the book we provide references
to this supplementary material, which you can explore on your own if that interests
you.
Have fun and good luck.
4
www.it-ebooks.info

1
Functional programming (FP) is based on a simple premise with far-reaching
implications: We construct our programs using only 
. In other words,
pure functions
functions that have no 
. What does this mean exactly? Performing any
side effects
of the following actions directly would involve a side effect:
Reassigning a variable
Modifying a data structure in place
Setting a field on an object
Throwing an exception or halting with an error
Printing to the console or reading user input
Reading from or writing to a file
Drawing on the screen
Consider what programming would be like without the ability to do these
things. It may be difficult to imagine. How is it even possible to write useful
programs at all? If we can't reassign variables, how do we write simple programs
like loops? What about working with data that changes, or handling errors without
throwing exceptions? How can we perform I/O, like drawing to the screen or
reading from a file?
The answer is that we can still write all of the same programs—programs that
can do all of the above and more—without resorting to side effects. Functional
programming is a restriction on 
 we write programs, but not on 
 programs
how
what
we can write. And it turns out that accepting this restriction is tremendously
What is Functional Programming?
1.1 The fundamental premise of functional programming
5
www.it-ebooks.info

beneficial because of the increase in 
 that we gain from programming
modularity
with pure functions. Because of their modularity, pure functions are easier to test,
to reuse, to parallelize, to generalize, and to reason about.
But reaping these benefits requires that we revisit the act of programming,
starting from the simplest of tasks and building upward from there. In many cases
we discover how programs that seem to necessitate side effects have some purely
functional analogue. In other cases we find ways to structure code so that effects
occur but are not 
 (For example, we can mutate data that is declared
observable
locally in the body of some function if we ensure that it cannot be referenced
outside that function.) Nevertheless, FP is a truly radical shift in how programs are
organized at every level—from the simplest of loops to high-level program
architecture. The style that emerges is quite different, but it is a beautiful and
cohesive approach to programming that we hope you come to appreciate.
In this book, you will learn the concepts and principles of FP as they apply to
every level of programming. We begin in this chapter by explaining what a pure
function is, as well as what it isn't. We also try to give you an idea of just why
purity results in greater modularity and code reuse.
A function with input type  and output type  (written in Scala as a single type: 
A
B
A
) is a computation which relates every value  of type  to exactly one value 
=> B
a
A
 of type  such that  is determined solely by the value of .
b
B
b
a
For example, a function 
 having type 
 will
intToString
Int => String
take every integer to a corresponding string. Furthermore, if it really is a 
,
function
it will do nothing else.
In other words, a function has no observable effect on the execution of the
program other than to compute a result given its inputs; we say that it has no side
effects. We sometimes qualify such functions as 
 functions to make this more
pure
explicit. You already know about pure functions. Consider the addition ( )
+
function on integers. It takes two integer values and returns an integer value. For
any two given integer values it will 
. Another
always return the same integer value
example is the 
 function of a 
 in Java, Scala, and many other
length
String
languages. For any given string, the same length is always returned and nothing
else occurs.
We can formalize this idea of pure functions by using the concept of referential
 (RT). This is a property of 
 in general and not just
transparency
expressions
1.2 Exactly what is a (pure) function?
6
www.it-ebooks.info

functions. For the purposes of our discussion, consider an expression to be any part
of a program that can be evaluated to a result, i.e. anything that you could type into
the Scala interpreter and get an answer. For example, 
 is an expression that
2 + 3
applies the pure function  to the values  and  (which are also expressions). This
+
2
3
has no side effect. The evaluation of this expression results in the same value 5
every time. In fact, if you saw 
 in a program you could simply replace it
2 + 3
with the value  and it would not change a thing about your program.
5
This is all it means for an expression to be referentially transparent—in any
program, the expression can be replaced by its result without changing the meaning
of the program. And we say that a function is 
 if its body is RT, assuming RT
pure
inputs.
SIDEBAR
Referential transparency and purity
An expression  is 
 if for all programs , all
e
referentially transparent
p
occurrences of  in  can be replaced by the result of evaluating ,
e
p
e
without affecting the observable behavior of . A function  is 
 if the
p
f
pure
expression 
 is referentially transparent for all referentially
f(x)
transparent .
x 1
Footnote 1mThere are some subtleties to this definition, and we'll be
refinining it later in this book. See the chapter notes for more discussion.
Referential transparency enables a mode of reasoning about program evaluation
called 
. When expressions are referentially transparent, we
the substitution model
can imagine that computation proceeds very much like we would solve an
algebraic equation. We fully expand every part of an expression, replacing all
variables with their referents, and then reduce it to its simplest form. At each step
we replace a term with an equivalent one; we say that computation proceeds by
substituting 
. In other words, RT enables 
equals for equals
equational reasoning
about programs. This style of reasoning is 
 natural; you use it all the time
extremely
when understanding programs, even in supposedly "non-functional" languages.
Let's look at two examples—one where all expressions are RT and can be
reasoned about using the substitution model, and one where some expressions
violate RT. There is nothing complicated here, part of our goal is to illustrate that
we are just formalizing something you already likely understand on some level.
Let's try the following in the Scala REPL:2
1.3 Functional and non-functional: an example
7
www.it-ebooks.info

Footnote 2mIn Java and in Scala, strings are immutable. If you wish to "modify" a string, you must create a
copy of it.
Suppose we replace all occurrences of the term 
 with the expression
x
referenced by  (its definition), as follows:
x
This transformation does not affect the outcome. The values of 
 and 
 are
r1
r2
the same as before, so  was referentially transparent. What's more, 
 and 
 are
x
r1
r2
referentially transparent as well, so if they appeared in some other part of a larger
program, they could in turn be replaced with their values throughout and it would
have no effect on the program.
Now let's look at a function that is 
 referentially transparent. Consider the 
not
 function on the 
append
scala.collection.mutable.StringBuilder
class. This function operates on the 
 in place. The previous state
StringBuilder
of the 
 is destroyed after a call to 
. Let's try this out:
StringBuilder
append
scala> val x = "Hello, World"
x: java.lang.String = Hello, World
scala> val r1 = x.reverse
r1: String = dlroW ,olleH
scala> val r2 = x.reverse
r2: String = dlroW ,olleH
scala> val r1 = "Hello, World".reverse
r1: String = dlroW ,olleH
val r2 = "Hello, World".reverse
r2: String = dlroW ,olleH
scala> val x = new StringBuilder("Hello")
x: java.lang.StringBuilder = Hello
scala> val y = x.append(", World")
y: java.lang.StringBuilder = Hello, World
scala> val r1 = y.toString
r1: java.lang.String = Hello, World
scala> val r2 = y.toString
r2: java.lang.String = Hello, World
8
www.it-ebooks.info

So far so good. Let's now see how this side effect breaks RT. Suppose we
substitute the call to 
 like we did earlier, replacing all occurrences of 
append
y
with the expression referenced by :
y
This transformation of the program results in a different outcome. We therefore
conclude that 
 is 
 a pure function. What's going on
StringBuilder.append
not
here is that while 
 and 
 look like they are the same expression, they are in
r1
r2
fact referencing two different values of the same 
. By the time 
StringBuilder
 calls 
, 
 will have already mutated the object referenced by . If
r2
x.append r1
x
this seems difficult to think about, that's because it is. Side effects make reasoning
about program behavior more difficult.
Conversely, the substitution model is simple to reason about since effects of
evaluation are purely local (they affect only the expression being evaluated) and
we need not mentally simulate sequences of state updates to understand a block of
code. Understanding requires only 
. Even if you haven't used the
local reasoning
name "substitution model", you have certainly used this mode of reasoning when
thinking about your code.3
Footnote 3mIn practice, programmers don't spend time mechanically applying substitution to determine if
code is pure—it will usually be quite obvious.
We said that applying the discipline of FP buys us greater modularity. Why is this
the case? Though this will become more clear over the course of the book, we can
give some initial insight here.
A modular program consists of components that can be understood and reused
independently of the whole, such that the meaning of the whole depends only on
the meaning of the components and the rules governing their composition; that is,
they are 
. A pure function is modular and composable because it
composable
separates the logic of the computation itself from "what to do with the result" and
scala> val x = new StringBuilder("Hello")
x: java.lang.StringBuilder = Hello
scala> val r1 = x.append(", World").toString
r1: java.lang.String = Hello, World
scala> val r2 = x.append(", World").toString
r2: java.lang.String = Hello, World, World
1.4 Why functional programming?
9
www.it-ebooks.info

"how to obtain the input"; it is a black box. Input is obtained in exactly one way:
via the argument(s) to the function. And the output is simply computed and
returned. By keeping each of these concerns separate, the logic of the computation
is more reusable; we may reuse the logic wherever we want without worrying
about whether the side effect being done with the result or the side effect being
done to request the input is appropriate in all contexts. We also do not need to
mentally track all the state changes that may occur before or after our function's
execution to understand what our function will do; we simply look at the function's
definition and substitute the arguments into its body.
Let's look at a case where factoring code into pure functions helps with reuse.
This is a simple and contrived example, intended only to be illustrative. Suppose
we are writing a computer game and are required to do the following:
If player 1's score property is greater than player 2's, notify the user that player
1 has won, otherwise notify the user that player 2 has won.
We may be tempted to write something like this:
Declares a data type Player with two properties: name, which is a string, and score,
an integer.
Prints the name of the winner to the console.
Takes two Players, compares their scores and declares the winner.
This declares a simple data type 
 with two properties, 
, which is
Player
name
a character string, and 
 which is an integer. The method 
score
declareWinner
takes two 
s, compares their scores and declares the player with the higher
Player
score the winner (unfairly favoring the second player, granted). The 
 method prints the name of the winner to the console. The result
printWinner
type of these methods is 
 indicating that they do not return a meaningful
Unit
result but have a side effect instead.
Let's test this in the REPL:
case class Player(name: String, score: Int)
def printWinner(p: Player): Unit =
  println(p.name + " is the winner!")
def declareWinner(p1: Player, p2: Player): Unit =
  if (p1.score > p2.score) printWinner(p1)
  else printWinner(p2)
scala> val sue = Player("Sue", 7)
sue: Player = Player(Sue, 7)
10
www.it-ebooks.info

While this code closely matches the earlier problem statement, it also
intertwines the branching logic with that of displaying the result, which makes the
reuse of the branching logic difficult. Consider trying to reuse the 
 method to compute and display the sole winner among 
declareWinner
n
players instead of just two. In this case, the comparison logic is simple enough that
we could just inline it, but then we are duplicating logic—what happens when
playtesting reveals that our game unfairly favors one player, and we have to change
the logic for determining the winner? We would have to change it in two places.
And what if we want to use that same logic to sort a historical collection of past
players to display a high score list?
Suppose we refactor the code as follows:
A pure function that takes two players and returns the higher-scoring one.
This version separates the logic of computing the winner from the displaying of
the result. Computing the winner in 
 is referentially transparent and the
winner
impure part—displaying the result—is kept separate in 
. We can
printWinner
now reuse the logic of 
 to compute the winner among a list of players:
winner
Constructs a list of players
scala> val bob = Player("Bob", 8)
bob: Player = Player(Bob, 8)
scala> winner(sue, bob)
Bob is the winner!
def winner(p1: Player, p2: Player): Player =
  if (p1.score > p2.score) p1 else p2
def declareWinner(p1: Player, p2: Player): Unit =
  printWinner(winner(p1, p2))
val players = List(Player("Sue", 7),
                   Player("Bob", 8),
                   Player("Joe", 4))
val p = players.reduceLeft(winner)
printWinner(p)
11
www.it-ebooks.info

Reduces the list to just the player with the highest score.
Prints the name of the winner to the console.
In this example, 
 is a function on the 
 data type from the
reduceLeft
List
standard Scala library. The expression will compare all the players in the list and
return the one with the highest score. Note that we are actually passing our 
 function to 
 as if it were a regular value. We will have a lot
winner
reduceLeft
more to say about passing functions to functions, but for now just observe that
because 
 is a pure function, we are able to reuse it and combine it with
winner
other functions in ways that we didn't necessarily anticipate. In particular, this
usage of 
 would not have been possible when the side effect of displaying
winner
the result was interleaved with the logic for computing the winner.
This was just a simple example, meant to be illustrative, and the sort of
factoring we did here is something you've perhaps done many times before. It's
been said that functional programming, at least in small examples, is just normal
separation of concerns and "good software engineering".
We will be taking the idea of FP to its logical endpoint in this book, and
applying it in situations where is applicability is less obvious. As we'll learn, any
function with side effects can be split into a pure function at the "core" and
possibly a pair of functions with side effects; one on the input side, and one on the
output side. This is what we did when we separated the declaration of the winner
from our pure function 
. This transformation can be repeated to push side
winner
effects to the "outer layers" of the program. Functional programmers often speak of
implementing programs with a pure core and a thin layer on the outside that
handles effects. We will return to this principle again and again throughout the
book.
In this chapter, we introduced functional programming and explained exactly what
FP is and why you might use it. In subsequent chapters, we cover some of the
fundamentals—how do we write loops in FP? Or implement data structures? How
do we deal with errors and exceptions? We need to learn how to do these things
and get comfortable with the low-level idioms of FP. We'll build on this
understanding when we explore functional design techniques in parts 2 and 3.
1.5 Conclusion
12
www.it-ebooks.info

composition
equals for equals
equational reasoning
expression substitution
modularity
program modularity
referential transparency
side effects
substitution
substitution model
Index Terms
13
www.it-ebooks.info

2
Now that we have committed to using only pure functions, a question naturally
emerges: how do we write even the simplest of programs? Most of us are used to
thinking of programs as sequences of instructions that are executed in order, where
each instruction has some kind of effect. In this chapter we will learn how to write
programs in the Scala language just by combining pure functions.
This chapter is mainly intended for those readers who are new to Scala, to
functional programming, or both. As with learning a foreign language, immersion
is a very effective method, so we will start by looking at a small but complete
Scala program. If you have no experience with Scala, you should not expect to
understand the code at first glance. Therefore we will break it down piece by piece
to look at what it does.
We will then look at working with higher-order functions. These are functions
that take other functions as arguments, and may themselves return functions as
their output. This can be brain-bending if you have a lot of experience
programming in a language 
 the ability to pass functions around like that.
without
Remember, it's not crucial that you internalize every single concept in this chapter,
or solve every exercise. In fact, you might find it easier to skip whole sections and
spiral back to them when you have more experience onto which to attach these
concepts.
The following is a complete program listing in Scala.
Getting started
2.1 Introduction
2.2 An example Scala program
// A comment!
14
www.it-ebooks.info

We declare an object (also known as a "module") named 
. This is
MyModule
simply to give our code a place to live, and a name for us to refer to it later. The 
 keyword creates a new 
, which means that 
 is
object
singleton type
MyModule
the only value (or 'inhabitant') of that type. We put our code inside the object,
between curly braces. We will discuss objects, modules, and namespaces in more
detail shortly. For now, let's just look at this particular object.
The 
 object has three methods: 
, 
, and 
.
MyModule
abs formatAbs
main
Each method is introduced by the 
 keyword, followed by the name of the
def
method which is followed by the arguments in parentheses. In this case all three
methods take only one argument. If there were more arguments they would be
separated by commas. Following the closing parenthesis of the argument list, an
optional type annotation indicates the type of the result (the colon is pronounced
"has type").
The body of the method itself comes after an equals ( ) sign. We will
=
sometimes refer to the part of a method declaration that goes before the equals sign
as the 
 or 
, and the code that comes after the equals sign we
left-hand side
signature
will sometimes refer to as the 
 or 
. Note the absence of an
right-hand side
definition
explicit 
 keyword. The value returned from a method is simply the value
return
of its right-hand side.
Let's now go through these methods one by one. The 
 method represents a
abs
pure function that takes an integer and returns its absolute value:1
Footnote 1mAstute readers might notice that this definition won't work for 
, the
Integer.MinValue
smallest negative 32-bit integer, which has no corresponding positive 
. We'll ignore this technicality here.
Int
/* Another comment */
/** A documentation comment */
object MyModule {
  def abs(n: Int): Int =
    if (n < 0) -n
    else n
  private def formatAbs(x: Int) = {
    val msg = "The absolute value of %d is %d"
    msg.format(x, abs(x))
  }
  def main(args: Array[String]): Unit =
    println(formatAbs(-42))
}
def abs(n: Int): Int =
15
www.it-ebooks.info

The abs method takes a single argument n of type Int, and this is declared with n:
Int.
The definition is a single Scala expression that uses the built-in if syntax to negate
n if it's less than zero.
The 
 method represents another pure function:
formatAbs
The format method is a standard library method defined on String. Here we are
calling it on the msg object, passing in the value of x along with the value of abs
applied to x. This results in a new string with the occurrences of %d in msg
replaced with the evaluated results of x and abs(x) respectively. Also see the
sidebar on string interpolation below.
This method is declared 
, which means that it cannot be called from
private
any code outside of the 
 object. This function takes an 
 and returns
MyModule
Int
a 
, but note that the return type is not declared. Scala is usually able to
String
infer the return types of methods, so they can be omitted, but it's generally
considered good style to explicitly declare the return types of methods that you
expect others to use. This method is private to our module, so we can omit the type
annotation.
The body of the method contains more than one statement, so we put them
inside curly braces. A pair of braces containing statements is called a 
.
block
Statements are separated by new lines or by semicolons. In this case we are using a
new line to separate our statements.
The first statement in the block declares a 
 named 
 using the 
String
msg
val
keyword. A 
 is an immutable variable, so inside the body of the 
val
formatAs
method the name 
 will always refer to the same 
 value. The Scala
msg
String
compiler will complain if you try to reassign 
 to a different value in the same
msg
context.
Remember, a method simply returns the value of its right-hand side, which in
this case is a block. And the value of a multi-statement block inside curly braces is
simply the same as the value of its last statement. So the result of the formatAbs
  if (n < 0) -n
  else n
private def formatAbs(x: Int) = {
  val msg = "The absolute value of %d is %d."
  msg.format(x, abs(x))
}
16
www.it-ebooks.info

method is just the value of 
.
msg.format(x, abs(x))
SIDEBAR
String interpolation in Scala
We could have written our 
 function using 
formatAbs
string
 (
) rather than the 
 method on 
interpolation
documentation
format
. Interpolated strings can reference Scala values in scope at the
String
point where they are declared. An interpolated string has an 
 (for
s
'substitute') just before the first 
, for example: 
"
s"The absolute
. See the documentation linked above
value of $x is ${abs(x)}
for more details.
Finally, our 
 method is an "outer shell" that calls into our purely
main
functional core and performs the effect of printing the answer to the console:
The name 
 is special because when you run a program, Scala will look for
main
a method named 
 with a specific signature. It has to take an 
 of 
main
Array
s as its argument, and its return type must be 
. The 
 array will
String
Unit
args
contain the arguments that were given at the command line that ran the program.
The return type of 
 indicates that this method does not return a meaningful
Unit
value. There is only one value of type 
 and it has no inner structure. It's
Unit
written 
, pronounced "unit" just like the type. Usually a return type of 
 is a
()
Unit
hint that the method has a side effect. But since the 
 method itself is called
main
once by the operating environment and never from anywhere in our program,
referential transparency is not violated.
This section discusses the simplest possible way of running your Scala programs,
suitable for short examples. More typically, you'll build and run your Scala code
using sbt, the build tool for Scala, and/or an IDE like IntelliJ or Eclipse. See the
book's source code repo on GitHub for more information on getting set up with sbt.
Sbt is very smart about ensuring only the minimum number of files are recompiled
when changes are made. It also has a number of other nice features which we won't
discuss here.
But the simplest way we can run this Scala program (
) is from the
MyModule
def main(args: Array[String]): Unit =
  println(formatAbs(-42))
2.3 Running our program
17
www.it-ebooks.info

command line, by invoking the Scala compiler directly ourselves. We start by
putting the code in a file called 
 or something similar. We can
MyModule.scala
then compile it to Java bytecode using the 
 compiler:
scalac
This will generate some files ending with the 
 suffix. These files
.class
contain compiled code that can be run with the Java virtual machine. The code can
be executed using the 
 code runner:
scala
Actually, it's not strictly necessary to compile the code first with 
. A
scalac
simple program like the one we have written here can just be run using the Scala
interpreter by passing it to the 
 code runner directly:
scala
This can be handy when using Scala for scripting. The code runner will look for
any object within the file 
 that has a 
 method with the
MyModule.scala
main
appropriate signature, and will then call it.
Lastly, an alternative way is to start the Scala interpreter's interactive mode,
usually referred to as the read-evalulate-print-loop or REPL (pronounced "repple"
like "apple"), and load the file from there (your actual console output may differ
slightly):
> scalac MyModule.scala
> scala MyModule
The absolute value of -42 is 42.
> scala MyModule.scala
The absolute value of -42 is 42.
> scala
Welcome to Scala.
Type in expressions to have them evaluated.
Type :help for more information.
scala> :load MyModule.scala
Loading MyModule.scala...
defined module MyModule
scala> MyModule.main(Array())
The absolute value of -42 is 42.
18
www.it-ebooks.info

main takes an array as its argument and here we are simply passing it an empty
array.
It's possible to simply copy and paste the code into the REPL. It also has a paste
mode (accessed with the 
 command) specifically designed to paste code.
:paste
It's a good idea to get familiar with the REPL and its features.
In the above, notice that in order to refer to our 
 method, we had to say 
main
 because 
 was defined in the 
 object. Aside
MyModule.main
main
MyModule
from a few technical corner cases, every value in Scala is what's called an "object".
An object whose primary purpose is giving its members a namespace is sometimes
called a 
. An object may have zero or more 
. A member can be a
module
members
method declared with the 
 keyword, or it can be another object declared with 
def
 or 
. Objects can also have other kinds of members that we will ignore
val
object
for now.
We dereference the members of objects with the typical object-oriented
dot-notation, which is a namespace (i.e. the name that refers to the object) followed
by a dot (the period character) followed by the name of the member. For example,
to call the 
 method on the 
 object we would say 
abs
MyModule
. To use the 
 member on the object 
, we
MyModule.abs(42)
toString
42
would say 
. The implementations of members within an object can
42.toString
refer to each other unqualified (without prefixing the object name), but if needed
they have access to their enclosing object using a special name: 
.
this
Note that even an expression like 
 is just calling a member of an object.
2 + 1
In that case what is being called is the  member of the object . It is really
+
2
syntactic sugar for the expression 
. We can in general omit the dot and
2.+(1)
parentheses like that when calling a method and applying it to a single argument.
For example, instead of 
 we can say 
MyModule.abs(42)
MyModule abs 42
and get the same result.
An object's member can be brought into scope by importing it, which allows us
to call it unqualified from then on:
2.4 Modules, objects, and namespaces
scala> import MyModule.abs
import MyModule.abs
scala> abs(-42)
res0: 42
19
www.it-ebooks.info

We can bring all of an object's (non-private) members into scope by using the
underscore syntax: import MyModule._
SIDEBAR
Packages
In Scala, there is a language construct called a 
, which is a
package
namespace without an object. The difference between a package and a
module is that a package cannot contain 
 or 
 members and
val
def
can't be passed around as if it were an object.
For example, we can declare a package at the start of our Scala source
file:
And we can thereafter refer to 
 as a qualified
mypackage.MyModule
name, or we can 
 to be able to refer to 
import mypackage._
 unqualified. However, we cannot say 
 to
MyModule
f(mypackage)
pass the package to some function 
, since a package is not a
f
first-class value in Scala.
In Scala, functions are objects too. They can be passed around like any other value,
assigned to variables, stored in data structures, and so on. When writing purely
functional programs, it becomes quite natural to want to accept functions as
arguments to other functions. We are going to look at some rather simple examples
just to illustrate this idea. In the chapters to come we'll see how useful this
capability really is, and how it permeates our programming style. But to start,
suppose we wanted to adapt our program to print out both the absolute value of a
number 
 the factorial of another number. Here's a sample run of such a
and
program:
First, let's write 
, which also happens to be our first example of
factorial
package mypackage
object MyModule {
  ...
}
2.5 Function objects: passing functions to functions
The absolute value of -42 is 42
The factorial of 7 is 5040
20
www.it-ebooks.info

how to write a loop without mutation:2
Footnote 2mWe can also write this using an ordinary 
 loop and a mutable variable. See the chapter
while
code for an example of this.
Int is another primitive type in Scala, representing 32-bit integers
An inner or local function
The way we write loops in Scala is with a recursive function, by convention
often called 
 (or sometimes 
) and which we'll often define local to another
go
loop
function (unlike Java, in Scala, we can define functions inside any block, including
within another function definition). The arguments to 
 are the state for the loop
go
(in this case, the remaining value , and the current accumulated factorial, 
).
n
acc
To advance to the next iteration, we simply call 
 recursively with the new loop
go
state (here, 
), and to exit from the loop we return a value
go(n-1, n*acc)
without a recursive call (here, we return 
 in the case that 
). Scala
acc
n <= 0
detects this sort of 
 and compiles it to the same sort of bytecode as
self-recursion
would be emitted for a 
 loop, so long as the recursive call is in 
.
while
tail position
See the sidebar for the technical details on this, but the basic idea is that this
optimization (called 
) is applied when there is no additional
tail call optimization
work left to do after the recursive call returns.3
Footnote 3mThe name 'tail-call optimization' (TCO) is something of a misnomer. An 'optimization' usually
connotes some nonessential performance improvement, but when we use tail calls to write loops, we generally rely
on their being compiled as iterative loops that do not consume a call stack frame for each iteration (which would
result in a 
 for large inputs).
StackOverflowError
def factorial(n: Int): Int = {
  def go(n: Int, acc: Int): Int =
    if (n <= 0) acc
    else go(n-1, n*acc)
  go(n, 1)
}
21
www.it-ebooks.info

SIDEBAR
Tail calls in Scala
A call is said to be in 'tail position' if the caller does nothing other than
return the value of the recursive call. For example, the recursive call to 
 above is in tail position, since the caller simply returns
go(n-1,n*acc)
the value of this recursive call. If, on the other hand, we said 1 +
, 
 would no longer be in tail position, since the caller
go(n-1,n*acc) go
would still have work to do when 
 returned its result (namely, adding 
go
 to it). Likewise if we said 
 for some function, .
1
f(go(n-1,n*acc))
f
If all recursive calls made by a function are in tail position, Scala
compiles the recursion to iterative loops that do not consume call stack
frames for each iteration. If we are expecting this to occur for a
recursive function we write, we can tell the Scala compiler about this
assumption using an 
 (
), so it can
annotation more information on this
give us a compile error if it is not able to optimize the tail calls of the
function. Here's the syntax for this:
We won't be talking much more about 
 in this book, but we'll
annotations
use 
 extensively.
@annotation.tailrec
EXERCISE 1 (optional): Write a function to get the th Fibonacci number. The
n
first two Fibonacci numbers are  and , and the next number is always the sum of
0
1
the previous two. Your definition should use a local tail-recursive function.4
Footnote 4mNote that the nth Fibonacci number has a 
. Using that would be cheating; the
closed form solution
point here is just to get some practice writing loops using tail-recursive functions.
Now that we have 
, let's edit our program from before:
factorial
The two functions, 
 and 
, are almost
formatAbs
formatFactorial
identical. If we like, we can generalize these to a single function, formatResult
, which accepts as an argument 
 to apply to its argument:
the function
def factorial(n: Int): Int = {
  @annotation.tailrec
  def go(n: Int, acc: Int): Int =
    if (n <= 0) acc
    else go(n-1, n*acc)
  go(n, 1)
}
def fib(n: Int): Int
22
www.it-ebooks.info

There are a few new thing here. First, our 
 function takes
formatResult
multiple arguments. To declare a function with multiple arguments, we just
separate each argument by a comma. Second, our 
 function now
formatResult
takes another function, which we call  (this is a common naming convention in
f
FP; see the sidebar below). A function that takes another function as an argument
is called a 
 (HOF). Like any other function parameter, we
higher-order function
give a type to , the type 
, which indicates that  expects an 
f
Int => Int
f
Int
and will also return an 
. (The type of a function expecting an 
 and a 
Int
Int
 and returning an 
 would be written as 
.)
String
Int
(Int,String) => Int
Next, notice that we call the function  using the same syntax as when we
f
called 
 or 
 directly. Lastly, notice that we can pass a
abs(x)
factorial(n)
reference to 
 and 
 to the 
 function. Our function
abs
factorial
formatResult
 accepts an 
 and returns an 
, which matches the 
abs
Int
Int
Int => Int
requirement on  in 
. And likewise, 
 accepts an 
f
formatResult
factorial
 and returns an 
, which also matches the 
 requirement on .
Int
Int
Int => Int
f
SIDEBAR
Variable naming conventions in FP
It is a common convention to use , , and  as parameter names for
f g
h
functions passed to a HOF. In FP, we tend to use one-letter or very
short variable names, especially when everything there is to say about
a value is implied by its type. Since functions are usually quite short in
FP, many functional programmers feel this makes the code easier to
read, since it makes the structure of the code easier to see. We will
introduce other conventions like this throughout the book.
This example isn't terribly exciting, but the same principles apply in larger
examples, and we can use first-class functions to factor out duplication whenever
we see it. We'll see many more examples of this throughout this book.
def formatResult(name: String, n: Int, f: Int => Int) = {
  val msg = "The %s of %d is %d."
  msg.format(n, f(n))
}
def main(args: Array[String]): Unit = {
  println(formatResult("absolute value", -42, abs))
  println(formatResult("factorial", 7, factorial))
}
23
www.it-ebooks.info

Functions get passed around so often in functional programming that it's
convenient to have a lightweight way to declare a function, locally, without having
to give it a name. Scala provides a syntax for declaring these nameless or 
. (Also often called 
, 
, 
anonymous functions
function literals lambda functions
, or just 
. )
lambda expressions
lambdas 5
Footnote 5mThe name 'lambda' comes from the 
, another theoretical basis for computation.
lambda calculus
Let's look at some examples of anonymous functions:
In this code, 
, 
, 
, and 
(x: Int) => x + 1 (x) => x + 1 x => x + 1
 are all alternate ways of writing the increment function, which has the type
_ + 1
. In this notation, the lambda expression has a left-hand side (
Int => Int
(x:
, 
, and ) and a right-hand side (
 and 
Int) (x)
x
x + 1
{ val result = x +
), separated by an arrow, 
. The left-hand side declares the
1; result }
=>
argument(s) in order (
 is an example of a two-argument
(x,y) => x + y
anonymous function), and the right-hand side, the 
 of the function, is simply
body
what the function will return. The body may of course refer to the arguments.6
Footnote 6mNote that in this case, Scala knows that 
 is expecting an 
 and we
formatResult
Int => Int
can get away with not annotating the type of ; in other cases, Scala may not know the type of the argument
x
and will force you to supply an annotation as in 
.
(x: Int) => ...
We could declare a value of this type like so val f = (x: Int) => x +
2.5.1 Annonymous functions
def main(args: Array[String]): Unit = {
  println(formatResult("absolute value", -42, abs))
  println(formatResult("factorial", 7, factorial))
  println(formatResult("increment", 7, (x: Int) => x + 1))
  println(formatResult("increment2", 7, (x) => x + 1))
  println(formatResult("increment3", 7, x => x + 1))
  println(formatResult("increment4", 7, _ + 1))
  println(formatResult("increment5", 7, x => { val r = x + 1; r }))
}
The absolute value of -42 is 42
The factorial of 7 is 5040
The increment of 7 is 8
The increment2 of 7 is 8
The increment3 of 7 is 8
The increment4 of 7 is 8
The increment5 of 7 is 8
24
www.it-ebooks.info

, but here we are not bothering to declare a local variable for the function, which
1
is quite common in FP. In this last form 
, sometimes called 
_ + 1
underscore
 for a function literal, we are not even bothering to name the argument to the
syntax
function, using  represent the sole argument. When using this notation, we can
_
only reference the function parameter once in the body of the function (if we
mention  again, it refers to another argument to the function).
_
7
Footnote 7mThere are various rules affecting the scope of an  that we won't go over here. See the 
_
Scala
 for the full details. Generally, if you have to think about how an
Language Specification, section 6.23
expression involving 's will be interpreted, it's better to just use the named parameter syntax, as in 
_
x => x
.
+ 1
25
www.it-ebooks.info

SIDEBAR
Functions are ordinary objects
We have said that functions and methods are not exactly the same
thing in Scala. When we define a function literal, what is actually being
defined is an object with a method called 
. Scala has a special
apply
rule for this method name, so that objects that have an 
 method
apply
can be called as if they were themselves methods. When we define a
function literal like 
 this is really syntax sugar for
(a, b) => a < b
object creation:
lessThan has type 
, which is
Function2[Int,Int,Boolean]
usually written 
. Note that the 
(Int,Int) => Boolean
Function2
interface (known in Scala as a "trait") has a single method called apply
. And when we call the 
 function with 
, it
lessThan
lessThan(10, 20)
is really syntax sugar for calling its 
 method:
apply
Function2 is just an ordinary trait (i.e. an interface) provided by the
standard Scala library (
) to represent function objects that
API docs link
take two arguments. Also provided are 
, 
, and
Function1 Function3
others, taking a number of arguments indicated by the name. Because
functions are really just ordinary Scala objects, we say that they are 
 values. We will often use "function" to refer to either such a
first-class
first-class function or a method, depending on context.
val lessThan = new Function2[Int, Int, Boolean] {
  def apply(a: Int, b: Int) = a < b
}
scala> val b = lessThan.apply(10, 20)
b: Boolean = true
26
www.it-ebooks.info

So far we have been defining only 
 functions. That is, functions that
monomorphic
operate on only one type of data. For example, 
, and 
 are specific
abs
factorial
to arguments of type 
, and the higher-order function 
 is also
Int
formatResult
fixed to operate on functions that take arguments of type 
. Very often, we want
Int
to write code which works for 
 type it is given. As an example, here's a
any
definition of binary search, specialized for searching for a 
 in an 
Double
. 
 is another primitive type in Scala, representing
Array[Double] Double
double precision floating point numbers. And 
 is the type
Array[Double]
representing an array of 
 values.
Double
We index into an array using the same syntax as function application
The details of the algorithm aren't too important here. What is important is that
the code for 
 is going to look almost identical if we are searching
binarySearch
for a 
 in an 
, an 
 in an 
, a 
Double
Array[Double]
Int
Array[Int]
String
in an 
, or an 
 in an 
. We can write 
Array[String]
A
Array[A]
 more generally for any type , by accepting a function to use for
binarySearch
A
testing whether an  value is greater than another:
A
2.6 Polymorphic functions: abstracting over types
def binarySearch(ds: Array[Double], key: Double): Int = {
  @annotation.tailrec
  def go(low: Int, mid: Int, high: Int): Int = {
    if (low > high) -mid - 1
    else {
      val mid2 = (low + high) / 2
      val d = ds(mid2)
      if (d == key) mid2
      else if (d > key) go(low, mid2, mid2-1)
      else go(mid2 + 1, mid2, high)
    }
  }
  go(0, 0, ds.length - 1)
}
def binarySearch[A](as: Array[A], key: A, gt: (A,A) => Boolean): Int = {
  @annotation.tailrec
  def go(low: Int, mid: Int, high: Int): Int = {
    if (low > high) -mid - 1
    else {
      val mid2 = (low + high) / 2
      val a = as(mid2)
27
www.it-ebooks.info

This is an example of a 
 function.  We are 
polymorphic
8
abstracting over the
 of the array, and the comparison function used for searching it. To write a
type
polymorphic function, we introduce a comma-separated list of 
,
type parameters
surrounded by 
 (here, just a single 
), following the name of the function, in
[]
[A]
this case 
. We can call the type parameters anything we want—
binarySearch
 and 
 are
[Foo, Bar, Baz]
[TheParameter, another_good_one]
valid type parameter declarations—though by convention we typically use short,
one letter type parameter names, like 
.
[A,B,C]
Footnote 8mWe are using the term 'polymorphism' in a slightly different way than mainstream object-oriented
programming, where that term usually connotes some form of subtyping. There are no interfaces or subtyping here
in this example. One will occasionally see the term 
 used to refer to this form of
parametric polymorphism
polymorphism.
The type parameter list introduces 
 (or sometimes 
type variables
type
) that can be referenced in the rest of the type signature (exactly
parameters
analogous to how variables introduced in the arguments to a function can be
referenced in the body of the function). Here, the type variable  is referenced in
A
three places—the search key is required to have the type , the values of the array
A
are required to have the type  (since it is an 
), and the 
 function
A
Array[A]
gt
must accept two arguments both of type  (since it is an 
).
A
(A,A) => Boolean
The fact that the same type variable is referenced in all three places in the type
signature enforces that the type must be the same for all three arguments, and the
compiler will enforce this fact anywhere we try to call 
. If we try
binarySearch
to search for a 
 in an 
, for instance, we'll get a type
String
Array[Int]
mismatch error.9
Footnote 9mUnfortunately, Scala's use of subtyping means we sometimes get rather cryptic compile errors,
since Scala will try to find a common supertype to use for the  type parameter, and will fall back to using 
A
, the supertype of all types.
Any
EXERCISE 2: Implement 
, which checks whether an 
 is
isSorted
Array[A]
sorted according to a given comparison function.
      val greater = gt(a, key)
      if (!greater && !gt(key,a)) mid2
      else if (greater) go(low, mid2, mid2-1)
      else go(mid2 + 1, mid2, high)
    }
  }
  go(0, 0, as.length - 1)
}
28
www.it-ebooks.info

SIDEBAR
Boxed types and specialization in Scala
A function that is polymorphic in some type is generally forced to
represent values of these types as 
, or non-primitive values,
boxed
meaning they are stored as a pointer to a value on the heap. It is
possible to instruct the Scala compiler to produce specialized versions
of a function for each of the primitive types, just by adding an annotation
to that type parameter:
This can potentially be much more efficient, though the mechanism is
rather fragile, since the polymorphic values will get boxed as soon as
they are passed to any other polymorphic function or data type which is
unspecialized in this way.
As you might have seen when writing 
, the universe of possible
isSorted
implementations is significantly reduced when implementing a polymorphic
function. If a function is polymorphic in some type, , the only operations that can
A
be performed on that  are those passed into the function as arguments (or that can
A
be defined in terms of these given operations).
 In some cases, you'll find that the
10
universe of possibilities for a given polymorphic type is constrained such that there
is only a single implementation!
Footnote 10mTechnically, all values in Scala can be compared for equality (using 
), and we can compute a
==
hash code for them as well. But this is something of a wart inherited from Java.
Let's look at an example of this, a higher-order function for doing what is called
. This function, 
, takes a value and a function of two
partial application
partial1
arguments, and returns a function of one argument as its result. The name comes
from the fact that the function is being applied to some but not all of its required
arguments.
EXERCISE 3 (hard): Implement 
 and write down a concrete usage
partial1
of it. There is only one possible implementation that compiles. We don't have any
def isSorted[A](as: Array[A], gt: (A,A) => Boolean): Boolean
def binarySearch[@specialized A](as: Array[A], key: A,
                                 gt: (A,A) => Boolean): Int
def partial1[A,B,C](a: A, f: (A,B) => C): B => C
29
www.it-ebooks.info

concrete types here, so we can only stick things together using the local 'rules of
the universe' established by the type signature. The style of reasoning required here
is very common in functional programming—we are simply manipulating symbols
in a very abstract way, similar to how we would reason when solving an algebraic
equation.
EXERCISE 4 (hard): Let's look at another example, 
, which converts a
currying
function of 
 arguments into a function of one argument that returns another
N
function as its result.
 Here again, there is only one implementation that
11
typechecks.
Footnote 11mThis is named after the mathematician Haskell Curry, who discovered the principle. It was
independently discovered earlier by Moses Schoenfinkel, but "Schoenfinkelization" didn't catch on.
EXERCISE 5 (optional): Implement 
, which reverses the
uncurry
transformation of 
. Note that since 
 associates to the right, 
curry
=>
A => (B
 can be written as 
.
=> C)
A => B => C
Let's look at a final example, 
, which feeds the output of
function composition
one function in as the input to another function. Again, the implementation of this
function is fully determined by its type signature.
EXERCISE 6: Implement the higher-order function that composes two
functions.
This is such a common thing to want to do that Scala's standard library provides
 as a method on 
. To compose two functions  and , you
compose
Function1
f
g
simply say 
. It also provides an 
 method. 
f compose g12
andThen
f
 is the same as 
:
andThen g
g compose f
Footnote 12mSolving the 
 exercise by using this library function is considered cheating.
compose
def curry[A,B,C](f: (A, B) => C): A => (B => C)
def uncurry[A,B,C](f: A => B => C): (A, B) => C
def compose[A,B,C](f: B => C, g: A => B): A => C
scala> val f = (x: Double) => math.Pi / 2 - x
30
www.it-ebooks.info

Interestingly, functions like 
 do not care whether they are operating
compose
on huge functions backed by millions of lines of code, or a couple of one-line
functions. Polymorphic, higher-order functions often end up being extremely
widely applicable, precisely because they say nothing about any particular domain
and are simply abstracting over a common pattern that occurs in many contexts.
We'll be writing many more such functions over the course of this book, and this is
just a short taste of the style of reasoning and thinking you'll use when writing such
functions.
In this chapter we have learned some preliminary functional programming
concepts, and enough Scala to get going. We learned how to define simple
functions and programs, including how we can express loops using recursion, then
introduced the idea of higher-order functions and got some practice writing
polymorphic functions in Scala. We saw how the implementations of polymorphic
functions are often significantly constrained, such that one can often simply 'follow
the types' to the correct implementation. This is something we'll see a lot more of
in the chapters ahead.
Although we haven't yet written any large or complex programs, the principles
we have discussed here are scalable and apply equally well to programming in the
large as they do to programming in the small.
Next up we will look at using pure functions to manipulate data.
f: Double => Double = <function1>
scala> val cos = f andThen math.sin
cos: Double => Double = <function1>
2.7 Conclusion
31
www.it-ebooks.info

annonymous function
block
curried form
currying
function literals
higher-order function
import
lambda
lambda expression
lambda notation
left-hand side
method definition
method signature
module
monomorphic
monomorphism
namespace
object
package
partial application
proper tail-calls
REPL
right-hand side
self-recursion
singleton type
string interpolation
tail-call optimization
tail position
type parameters
uncurry
uncurry
underscore syntax
val keyword
Index Terms
32
www.it-ebooks.info

3
We said in the introduction that functional programs do not update variables or
modify data structures. This raises pressing questions—what sort of data structures 
 we use in functional programming, how do we define them in Scala, and how
can
do we operate over these data structures? In this chapter we will learn the concept
of a 
 and how to define and work with such structures.
functional data structure
We'll use this as an opportunity to introduce how data types are defined in
functional programming, learn about the related technique of 
, and
pattern matching
get practice writing and generalizing pure functions.
This chapter has a lot of exercises, particularly to help with this last
point—writing and generalizing pure functions. Some of these exercises may be
challenging. As always, if you need to, consult the hints or the answers, or ask for
help online.
A functional data structure is (not surprisingly!) operated on using only pure
functions. Remember, a pure function may only accept some values as input and
yield a value as output. It may not change data in place or perform other side
effects. 
 For example, the
Therefore, functional data structures are immutable.
empty list, (denoted 
 or 
 in Scala) is as eternal and immutable as the
List()
Nil
integer values  or . And just as evaluating 
 results in a new number 
3
4
3 + 4
7
without modifying either  or , concatenating two lists together (the syntax for
3
4
this is 
 for two lists  and ) yields a new list and leaves the two inputs
a ++ b
a
b
unmodified.
Doesn't this mean we end up doing a lot of extra copying of the data?
Functional data structures
3.1 Introduction
3.2 Defining functional data structures
33
www.it-ebooks.info

Somewhat surprisingly, the answer is 'no'. We will return to this issue after
examining the definition of what is perhaps the most ubiquitous of functional data
structures, the singly-linked list. The definition here is identical in spirit to (though
simpler than) the 
 data type defined in Scala's standard library. This code
List
listing makes use of a lot of new syntax and concepts, so don't worry if not
everything makes sense at first—we will talk through it in detail.1
Footnote 1mNote—the implementations of 
 and 
 here are not tail recursive. We will be writing
sum
product
tail recursive versions of these functions later in the chapter.
Listing 3.1 Singly-linked lists
List data type
data constructor for List
List companion object
Pattern matching example
Variadic function syntax
Creating lists
package fpinscala.datastructures
sealed trait List[+A]
case object Nil extends List[Nothing]
case class Cons[+A](head: A, tail: List[A]) extends List[A]
object List {
  def sum(ints: List[Int]): Int = ints match {
    case Nil => 0
    case Cons(x,xs) => x + sum(xs)
  }
 
  def product(ds: List[Double]): Double = ds match {
    case Nil => 1.0
    case Cons(0.0, _) => 0.0
    case Cons(x,xs) => x * product(xs)
  }
 
  def apply[A](as: A*): List[A] =
    if (as.isEmpty) Nil
    else Cons(as.head, apply(as.tail: _*))
 
  val example = Cons(1, Cons(2, Cons(3, Nil)))
  val example2 = List(1,2,3)
  val total = sum(example)
}
34
www.it-ebooks.info

Let's look first at the definition of the data type, which begins with the
keywords 
. In general, we introduce a data type with the 
sealed trait
trait
keyword. A 
 is an abstract interface that may optionally contain
trait
implementations of some methods. Here we are declaring a 
, called 
,
trait
List
with no methods on it. Adding 
 in front means that all implementations of
sealed
our 
 must be declared in this file.
trait
2
Footnote 2mWe could also say 
 here instead of 
. Technically, an 
abstract class
trait
abstract
 can contain 
, in the OO sense, which is what separates it from a 
, which cannot
class
constructors
trait
contain constructors. This distinction is not really relevant for our purposes right now.
There are two such implementations or 
 of 
 (each
data constructors
List
introduced with the keyword 
) declared next, to represent each of the two
case
possible forms a 
 can take—it can be 
, denoted by the data constructor 
List
empty
, or it can be nonempty (the data constructor 
, traditionally short for
Nil
Cons
'construct'), in which case it consists of an initial element, 
, followed by a 
head
 (possibly empty) of remaining elements (the 
).
List
tail
Listing 3.2 The data constructors of List
Just as functions can be polymorphic, data types can be as well, and by adding
the type parameter 
 after 
 and then using that 
[+A]
sealed trait List
A
parameter inside of the 
 data constructor, we have declared the 
 data
Cons
List
type to be polymorphic in the type of elements it contains, which means we can
use this same definition for a list of 
 elements (denoted 
), 
Int
List[Int]
 
elements 
(denoted 
), 
 
elements 
(
Double
List[Double]
String
), and so on (the 
 indicates that the type parameter, 
 is 
List[String]
+
A
—see sidebar 'More about variance' for more info).
covariant
A data constructor declaration gives us a function to construct that form of the
data type (the 
 lets us write 
 to construct an empty 
case object Nil
Nil
List
, and the 
 lets us write 
, 
case class Cons
Cons(1, Nil) Cons(1,
, and so on for nonempty lists), but also introduces a 
Cons(2, Nil))
pattern
that can be used for 
, as in the functions 
 and 
.
pattern matching
sum
product
case object Nil extends List[Nothing]
case class Cons[+A](head: A, tail: List[A]) extends List[A]
35
www.it-ebooks.info

SIDEBAR
More about variance
In the declaration 
, the 
 in front of the type
trait List[+A]
+
parameter  is a 
 which signals that  is a 
A
variance annotation
A
covariant
or 'positive' parameter of 
. This means that, for instance, 
List
 is considered a subtype of 
, assuming 
List[Dog]
List[Animal]
Dog
is a subtype of 
. (More generally, for all types  and , if  is a
Animal
X
Y
X
subtype of  then 
 is a subtype of 
). We could leave
Y
List[X]
List[Y]
out the  in front of the , which would make 
 
 in that type
+
A
List invariant
parameter.
But notice now that 
 extends 
. 
 is a
Nil
List[Nothing]
Nothing
subtype of all types, which means that in conjunction with the variance
, 
 can be considered a 
, a 
,
annotation Nil
List[Int]
List[Double]
and so on, exactly as we want.
These concerns about variance are not very important for the present
discussion and are more of an artifact of how Scala encodes data
constructors via subtyping, so don't worry if this is not completely clear
right now.3
Footnote 3mIt is certainly possible to write code without using variance
annotations at all, and function signatures are sometimes simpler (while type
inference often gets worse). Unless otherwise noted, we will be using variance
annotations throughout this book, but you should feel free to experiment with
both approaches.
Let's look in detail at the functions 
 and 
, which we place in the 
sum
product
, sometimes called the 
 to 
 (see sidebar).
object List
companion object
List
Both these definitions make use of pattern matching.
As you might expect, the 
 function states that the sum of an empty list is ,
sum
0
3.2.1 Pattern matching
def sum(ints: List[Int]): Int = ints match {
  case Nil => 0
  case Cons(x,xs) => x + sum(xs)
}
def product(ds: List[Double]): Double = ds match {
  case Nil => 1.0
  case Cons(0.0, _) => 0.0
  case Cons(x, xs) => x * product(xs)
}
36
www.it-ebooks.info

and the sum of a nonempty list is the first element, 
, plus the sum of the
x
remaining elements, 
.  Likewise the 
 definition states that the product
xs 4
product
of an empty list is 
, the product of any list starting with 
 is 
,  and the
1.0
0.0
0.0 5
product of any other nonempty list is the first element multiplied by the product of
the remaining elements. Notice these are recursive definitions, which are quite
common when writing functions that operate over recursive data types like List
(which refers to itself recursively in its 
 data constructor).
Cons
Footnote 4mWe could call  and 
 anything there, but it is a common convention to use 
, 
, 
, 
 as
x
xs
xs ys as bs
variable names for a sequence of some sort, and , , , , or  as the name for a single element of a
x y z a
b
sequence. Another common naming convention is  for the first element of a list (the "head" of the list),  for
h
t
the remaining elements (the "tail"), and  for an entire list.
l
Footnote 5mLISTS
6 This isn't the most robust test—pattern matching on 
 will match only the
0.0
exact value 
, not 
 or any other value very close to .
0.0
1e-102
0
Footnote 6mLISTS
Pattern matching works a bit like a fancy 
 statement that may descend
switch
into the structure of the expression it examines and extract subexpressions of that
structure (we'll explain this shortly). It is introduced with an expression (the target
or 
), like 
 followed by the keyword 
, and a 
-wrapped
scrutinee
ds
match
{}
sequence of 
. Each case in the match consists of a 
 (like 
cases
pattern
) to the left of the 
 and a 
 (like 
) to
Cons(x,xs)
=>
result
x * product(xs)
the right of the 
. If the target 
 the pattern in a case (see below), the result
=>
matches
of that case becomes the result of the entire match expression. If multiple patterns
match the target, Scala chooses the first matching case.
37
www.it-ebooks.info

SIDEBAR
Companion objects in Scala
We will often declare a 
 in addition to our data type
companion object
and its data constructors. This is just an 
 with the same name
object
as the data type (in this case 
) where we put various convenience
List
functions for creating or working with values of the data type.
If, for instance, we wanted a function def fill[A](n: Int, a:
, that created a 
 with  copies of the element , the 
A): List[A]
List
n
a
 companion object would be a good place for it. Companion
List
objects are more of a convention in Scala.  We could have called this
7
module 
 if we wanted, but calling it 
 makes it clear that the
Foo
List
module contains functions relevant to working with lists, and also gives
us the nice 
 syntax when we define a variadic 
List(1,2,3)
apply
function (see sidebar 'Variadic functions in Scala').
Footnote 7mThere is some special support for them in the language that
isn't really relevant for our purposes.
Let's look at a few more examples of pattern matching:
List(1,2,3) match { case _ => 42 } results in 
. Here we are using a variable
42
pattern, , which matches any expression. We could say  or 
 instead of  but we
_
x
foo
_
usually use  to indicate a variable whose value we ignore in the result of the case.
_
8
Footnote 8mThe  variable pattern is treated somewhat specially in that it may be mentioned multiple
_
times in the pattern to ignore multiple parts of the target.
List(1,2,3) match { case Cons(h,t) => h } results in . Here we are using a data
1
constructor pattern in conjunction with variables to 
 or 
 a subexpression of
capture
bind
the target.
List(1,2,3) match { case Cons(_,t) => t } results in 
.
List(2,3)
List(1,2,3) match { case Nil => 42 } results in a 
 at runtime. A 
MatchError
 indicates that none of the cases in a match expression matched the target.
MatchError
What determines if a pattern matches an expression? A pattern may contain 
, like 
 or 
, 
 like 
 and 
 which match anything,
literals
0.0
"hi" variables
x
xs
indicated by an identifier starting with a lowercase letter or underscore, and data
constructors like 
 or 
, which match only values of the
Cons(x,xs)
Nil
corresponding form (
 as a pattern matches only the value 
, and 
Nil
Nil
 or 
 as a pattern only match 
 values). These
Cons(h,t)
Cons(x,xs)
Cons
components of a pattern may be nested arbitrarily—Cons(x1, Cons(x2,
 and 
 are valid patterns. A
Nil))
Cons(y1, Cons(y2, Cons(y3, _)))
38
www.it-ebooks.info

pattern 
 the target if there exists an assignment of variables in the pattern to
matches
subexpressions of the target that make it 
 to the target. The
structurally equivalent
result expression for a matching case will then have access to these variable
assignments in its local scope.
EXERCISE 1: What will the result of the following match expression be?
You are strongly encouraged to try experimenting with pattern matching in the
REPL to get a sense for how it behaves.
SIDEBAR
Variadic functions in Scala
The function 
 in the listing above is a 
,
List.apply
variadic function
meaning it accepts zero or more arguments of type . For data types, it
A
is a common idiom to have a variadic 
 method in the companion
apply
object to conveniently construct instances of the data type. By calling
this function 
 and placing it in the companion object, we can
apply
invoke it with syntax like 
 or 
,
List(1,2,3,4)
List("hi","bye")
with as many values as we want separated by commas (we sometimes
call this the 
 or just 
 syntax).
list literal
literal
Variadic functions are just providing a little syntax sugar for creating and
passing a 
 of elements explicitly. 
 is the interface in Scala
Seq
Seq
implemented by sequence-like data structures like lists, queues,
vectors, and so forth. Inside 
, 
 will be bound to a 
 (
apply as
Seq[A]
), which has the functions 
 (returns the first
documentation link
head
element) and 
 (returns all elements but the first).
tail
We can convert a 
, , back into something that can be passed
Seq[A] x
to a variadic function using the syntax 
, where  can be any
x: _*
x
expression, for instance: 
 or even 
List(x: _*)
List(List(1,2,3):
.
_*)
val x = List(1,2,3,4,5) match {
  case Cons(x, Cons(2, Cons(4, _))) => x
  case Nil => 42
  case Cons(x, Cons(y, Cons(3, Cons(4, _)))) => x + y
  case Cons(h, t) => h + sum(t)
  case _ => 101
}
39
www.it-ebooks.info

When data is immutable, how do we write functions that, for example, add or
remove elements from a list? The answer is simple. When we add an element  to
1
the front of an existing list, say 
, we return a new list, in this case 
xs
. Since lists are immutable, we don't need to actually copy 
; we
Cons(1,xs)
xs
can just reuse it. This property of immutable data is called 
 or just 
data sharing
. Data sharing of immutable data often lets us implement functions more
sharing
efficiently; we can always return immutable data structures without having to
worry about subsequent code modifying our data. There's no need to
pessimistically make copies to avoid modification or corruption.9
Footnote 9mThis pessimistic copying can become a problem in large programs, when data may be passed
through a chain of loosely components, each of which may be forced to make copies of this data. Using immutable
data structures means never having to copy that data just to share it between two components of a system, which
promotes keeping these components loosely coupled. We find that 
, FP can often achieve greater
in the large
efficiency than approaches that rely on side effects, due to much greater sharing of data and computation.
In the same way, to "remove" an element from the front of a list val mylist
, we simply return 
. There is no real removing going on. The
= Cons(x,xs)
xs
original list, 
 is still available, unharmed. We say that functional data
mylist
structures are 
, meaning that existing references are never changed by
persistent
operations on the data structure.
Let's try implementing a few different functions for "modifying" lists in
different ways. You can place this and other functions we write inside the List
companion object.
EXERCISE 2: Implement the function 
 for "removing" the first element
tail
of a 
. Notice the function takes constant time. What are different choices you
List
could make in your implementation if the 
 is 
? We will return to this
List
Nil
question in the next chapter.
EXERCISE 3: Generalize 
 to the function 
, which removes the first 
tail
drop
 elements from a list.
n
EXERCISE 4: Implement 
,
 which removes elements from the 
dropWhile 10
 prefix as long as they match a predicate. Again, notice these functions take
List
time proportional only to the number of elements being dropped—we do not need
to make a copy of the entire 
.
List
Footnote 10m
 has two argument lists to improve type inference. See sidebar.
dropWhile
3.3 Functional data structures and data sharing
40
www.it-ebooks.info

SIDEBAR
Type inference in Scala
When writing functions like 
, we will often place the 
 in
dropWhile
List
the first argument group, and any functions,  that receive elements of
f
the 
 in a later argument group. We can call this function with two
List
sets of parentheses, like 
, or we can partially
dropWhile(xs)(f)
apply it by supplying only the first argument 
. This
dropWhile(xs)
returns a function that accepts the other argument, . The main reason
f
for grouping the arguments this way is to assist with type inference. If
we do this, Scala can determine the type of  without any annotation,
f
based on what it knows about the type of the 
, which makes the
List
function more convenient to use, especially when passing a function
literal like 
 in for , which would otherwise require an
x => x > 34
f
annotation like 
. (Here it is not so bad, but
(x: Int) => x > 34
when working with more complicated types, it is a pain to have to write
out these types each time we pass a function literal into a higher-order
function like 
). This is an unfortunate restriction of the Scala
dropWhile
compiler; other functional languages like Haskell and OCaml provide 
 inference, meaning type annotations are almost never
complete
required.11
Footnote 11mSee the notes for this chapter for more information and
links to further reading.
EXERCISE 5: Using the same idea, implement the function 
 for
setHead
replacing the first element of a 
 with a different value.
List
Data sharing often brings some more surprising efficiency gains. For instance,
here is a function that adds all the elements of one list to the end of another:
Notice that this definition only copies values until the first list is exhausted, so
def drop[A](l: List[A], n: Int): List[A]
def dropWhile[A](l: List[A])(f: A => Boolean): List[A]
def append[A](a1: List[A], a2: List[A]): List[A] =
  a1 match {
    case Nil => a2
    case Cons(h,t) => Cons(h, append(t, a2))
  }
41
www.it-ebooks.info

its runtime is determined only by the length of 
. The remaining list then just
a1
points to 
. If we were to implement this same function for two arrays, we would
a2
be forced to copy all the elements in both arrays into the result.
EXERCISE 6: Not everything works out so nicely. Implement a function, 
, which returns a 
 consisting of all but the last element of a 
. So,
init
List
List
given 
, 
 will return 
. Why can't this
List(1,2,3,4) init
List(1,2,3)
function be implemented in constant time like 
?
tail
Because of the structure of a singly-linked list, any time we want to replace the 
 of a 
, even if it is the last 
 in the list, we must copy all the
tail
Cons
Cons
previous 
 objects. Writing purely functional data structures that support
Cons
different operations efficiently is all about finding clever ways to exploit data
sharing, which often means working with more tree-like data structures. We are not
going to cover these data structures here; for now, we are content to use the
functional data structures others have written. As an example of what's possible, in
the Scala standard library, there is a purely functional sequence implementation, 
 (
), with constant-time random access, updates, 
, 
Vector documentation link
head
, 
, and constant-time additions to either the front or rear of the
tail init
sequence.
Let's look again at the implementations of sum and product. We've simplified the 
 implementation slightly, so as not to include the "short-circuiting" logic
product
of checking for 
:
0.0
Notice how similar these two definitions are. The only things that differ are the
def init[A](l: List[A]): List[A]
3.4 Recursion over lists and generalizing to higher-order
functions
def sum(ints: List[Int]): Int = ints match {
  case Nil => 0
  case Cons(x,xs) => x + sum(xs)
}
def product(ds: List[Double]): Double = ds match {
  case Nil => 1.0
  case Cons(x, xs) => x * product(xs)
}
42
www.it-ebooks.info

value to return in the case that the list is empty (  in the case of 
, 
 in the
0
sum 1.0
case of 
), and the operation to apply to combine results (  in the case of 
product
+
,  in the case of 
). Whenever you encounter duplication like this, as
sum *
product
we've discussed before, you can generalize it away by pulling subexpressions out
into function arguments. If a subexpression refers to any local variables (the +
operation refers to the local variables  and 
 introduced by the pattern, similarly
x
xs
for 
), turn the subexpression into a function that accepts these variables
product
as arguments. Putting this all together for this case, our function will take as
arguments the value to return in the case of the empty list, and the function to add
an element to the result in the case of a nonempty list:12
Footnote 12mIn the Scala standard library, 
 is a method on 
 and its arguments are curried
foldRight
List
similarly for better type inference.
Listing 3.3 Right folds and simple uses
Again, placing f in its own argument group after l and z lets type inference
determine the input types to f. See earlier sidebar.
foldRight is not specific to any one type of element, and the value that is
returned doesn't have to be of the same type as the elements either. One way of
describing what 
 does is that it replaces the constructors of the list, 
foldRight
 and 
 with 
 and 
, respectively. So the value of 
Nil
Cons
z
f
 becomes 
, and 
foldRight(Cons(a, Nil), z)(f)
f(a, z)
 becomes 
foldRight(Cons(a, Cons(b, Nil)), z)(f)
f(a, f(b,
.
z))
Let's look at an example. We are going to 
 the evaluation of 
trace
, by
foldRight(Cons(1, Cons(2, Cons(3, Nil))), 0)(_ + _)
repeatedly subsituting the definition of 
 in for its usages. We'll use
foldRight
def foldRight[A,B](l: List[A], z: B)(f: (A, B) => B): B =
  l match {
    case Nil => z
    case Cons(x, xs) => f(x, foldRight(xs, z)(f))
  }
def sum2(l: List[Int]) =
  foldRight(l, 0.0)(_ + _)
def product2(l: List[Double]) =
  foldRight(l, 1.0)(_ * _)
43
www.it-ebooks.info

program traces like this throughout this book.
Notice that 
 must traverse all the way to the end of the list
foldRight
(pushing frames onto the call stack as we go) before it can begin collapsing it.
EXERCISE 7: Can 
 implemented using 
 immediately
product
foldRight
halt the recursion and return 
 if it encounters a 
? Why or why not?
0.0
0.0
Consider how any short-circuiting might work if you call 
 with a
foldRight
large list. This is a deeper question that we'll return to a few chapters from now.
EXERCISE 8: See what happens when you pass 
 and 
 themselves to 
Nil
Cons
, 
like 
this: 
foldRight
foldRight(List(1,2,3),
.
 What do you think this says about the
Nil:List[Int])(Cons(_,_)) 13
relationship between 
 and the data constructors of 
?
foldRight
List
Footnote 13mThe type annotation 
 is needed here, because otherwise Scala infers the 
Nil:List[Int]
B
type parameter in 
 as 
.
foldRight
List[Nothing]
EXERCISE 9: Compute the length of a list using 
.
foldRight
EXERCISE 10: 
 is not tail-recursive and will 
foldRight
StackOverflow
for large lists. Convince yourself that this is the case, then write another general
list-recursion function, 
 that is tail-recursive, using the techniques we
foldLeft
discussed in the previous chapter. Here is its signature:14
Footnote 14mAgain, 
 is defined as a method of 
 in the scala standard library, and it is curried
foldLeft
List
similarly for better type inference, so you can write 
.
mylist.foldLeft(0.0)(_ + _)
EXERCISE 11: Write 
, 
, and a function to compute the length of
sum product
a list using 
.
foldLeft
foldRight(Cons(1, Cons(2, Cons(3, Nil))), 0)(_ + _)
1 + foldRight(Cons(2, Cons(3, Nil)), 0)(_ + _)
1 + (2 + foldRight(Cons(3, Nil), 0)(_ + _))
1 + (2 + (3 + (foldRight(Nil, 0)(_ + _))))
1 + (2 + (3 + (0)))
6
def length[A](l: List[A]): Int
def foldLeft[A,B](l: List[A], z: B)(f: (B, A) => B): B
44
www.it-ebooks.info

EXERCISE 12: Write a function that returns the reverse of a list (so given 
 it returns 
). See if you can write it using a fold.
List(1,2,3)
List(3,2,1)
EXERCISE 13 (hard): Can you write 
 in terms of 
?
foldLeft
foldRight
How about the other way around?
EXERCISE 14: Implement 
 in terms of either 
 or 
append
foldLeft
.
foldRight
EXERCISE 15 (hard): Write a function that concatenates a list of lists into a
single list. Its runtime should be linear in the total length of all lists. Try to use
functions we have already defined.
There are many more useful functions for working with lists. We are going to
cover a few more here. After finishing this chapter, we recommend looking
through the 
 to see what other functions there are. If you
scala API documentation
find yourself writing an explicit recursive function for doing some sort of list
manipulation, check the 
 API to see if something like the function you need
List
already exists.
After finishing this section, you're not going to emerge with an automatic sense
of when to use each of these functions. Just get in the habit of looking for possible
ways to generalize any explicit recursive functions you write to process lists. If you
do this, you'll (re)discover these functions for yourself and build more of a sense
for when you'd use each one.
SIDEBAR
Lists in the standard library
List exists in the Scala standard library (
), and we'll
API documentation
use the standard library version in subsequent chapters.
The main difference between the 
 developed here and the
List
standard library version is that 
 is called 
, which is
Cons
::
right-associative (all operators ending in  are right-associative), so 
:
1
 is equal to 
. When pattern matching, 
:: 2 :: Nil
List(1,2,3)
 becomes 
, which avoids having to
case Cons(h,t)
case h :: t
nest parentheses if writing a pattern like 
 to
case h :: h2 :: t
extract more than just the first element of the 
.
List
EXERCISE 16: Write a function that transforms a list of integers by adding 1
to each element. (Reminder: this should be a pure function that returns a new 
!)
List
3.4.1 More functions for working with lists
45
www.it-ebooks.info

EXERCISE 17: Write a function that turns each value in a List[Double]
into a 
.
String
EXERCISE 18: Write a function 
, that generalizes modifying each element
map
in a list while maintaining the structure of the list. Here is its signature:15
Footnote 15mIn the standard library, 
 and 
 are methods of 
.
map
flatMap
List
EXERCISE 19: Write a function 
 that removes elements from a list
filter
unless they satisfy a given predicate. Use it to remote all odd numbers from a 
.
List[Int]
EXERCISE 20: Write a function 
, that works like 
 except that
flatMap
map
the function given will return a list instead of a single result, and that list should be
inserted into the final resulting list. Here is its signature:
For instance 
 should
flatMap(List(1,2,3))(i => List(i,i))
result in 
.
List(1,1,2,2,3,3)
EXERCISE 21: Can you use 
 to implement 
?
flatMap
filter
EXERCISE 22: Write a function that accepts two lists and constructs a new list
by adding corresponding elements. For example, 
 and 
List(1,2,3)
 becomes 
.
List(4,5,6)
List(5,7,9)
EXERCISE 23: Generalize the function you just wrote so that it's not specific to
integers or addition.
There are a number of other useful methods on lists. You may want to try
experimenting with these and other methods in the REPL after reading the API
. These are defined as methods on 
, rather than as
documentation
List[A]
standalone functions as we've done in this chapter.
def take(n: Int): List[A]: returns a list consisting of the first  elements of 
.
n
this
def takeWhile(f: A => Boolean): List[A]: returns a list consisting of the longest
valid prefix of 
 whose elements all pass the predicate .
this
f
def forall(f: A => Boolean): Boolean: returns 
 if and only if all elements of 
true
 pass the predicate .
this
f
def exists(f: A => Boolean): Boolean: returns 
 if any element of 
 passes
true
this
the predicate .
f
def map[A,B](l: List[A])(f: A => B): List[B]
def flatMap[A,B](l: List[A])(f: A => List[B]): List[B]
46
www.it-ebooks.info

scanLeft and 
 are like 
 and 
, but they return the 
 of
scanRight
foldLeft
foldRight
List
partial results, rather than just the final accumulated value.
One of the problems with 
 is that while we can often express operations
List
and algorithms in terms of very general-purpose functions, the resulting
implementation isn't always efficient—we may end up making multiple passes
over the same input, or else have to write explicit recursive loops to allow early
termination.
EXERCISE 24 (hard): As an example, implement 
 for
hasSubsequence
checking whether a 
 contains another 
 as a subsequence. For instance, 
List
List
 would have 
, 
, and 
 as
List(1,2,3,4)
List(1,2) List(2,3)
List(4)
subsequences, among others. You may have some difficulty finding a concise
purely functional implementation that is also efficient. That's okay. Implement the
function however comes most naturally. We will return to this implementation in a
couple of chapters and hopefully improve on it. Note: any two values, , and ,
x
y
can be compared for equality in Scala using the expression 
.
x == y
List is just one example of what is called an 
 (ADT).
algebraic data type
(Somewhat confusingly, ADT is sometimes used in OO to stand for "abstract data
type".) An ADT is just a data type defined by one or more data constructors, each
of which may contain zero or more arguments. We say that the data type is the sum
or 
 of its data constructors, and each data constructor is the 
 of its
union
product
arguments, hence the name 
 data type.
algebraic
16
Footnote 16mThe naming is not coincidental. There is actually a deep connection, beyond the scope of this
book, between the "addition" and "multiplication" of types to form an ADT and addition and multiplication of
numbers.
When you encode a data type as an ADT, the data constructors and associated
patterns form part of that type's API, and other code may be written in terms of
explicit pattern
def hasSubsequence[A](l: List[A], sub: List[A]): Boolean
3.5 Trees
47
www.it-ebooks.info

SIDEBAR
Tuple types in Scala
Pairs and other arity tuples are also algebraic data types. They work
just like the ADTs we've been writing here, but have special syntax:
In this example, 
 is a pair whose type is 
,
("Bob", 42)
(String,Int)
which is syntax sugar for 
 (
). We can
Tuple2[String,Int] API link
extract the first or second element of this pair (a 
 will have a
Tuple3
method 
 and so on), and we can pattern match on this pair much like
_3
any other 
. Higher arity tuples work similarly—try
case class
experimenting with them on the REPL if you're interested.
Algebraic data types can be used to define other data structures. Let's define a
simple binary tree data structure:
Pattern matching again provides a convenient way of operating over elements
of our ADT. Let's try writing a few functions.
EXERCISE 25: Write a function 
 that counts the number of nodes in a
size
tree.
EXERCISE 26: Write a function 
 that returns the maximum element
maximum
in a 
. (Note: in Scala, you can use 
 or 
 to
Tree[Int]
x.max(y)
x max y
compute the maximum of two integers  and .)
x
y
EXERCISE 27: Write a function 
 that returns the maximum path length
depth
from the root of a tree to any leaf.
EXERCISE 28: Write a function 
, analogous to the method of the same
map
scala> val p = ("Bob", 42)
p: (java.lang.String, Int) = (Bob,42)
scala> p._1
res0: java.lang.String = Bob
scala> p._2
res1: Int = 42
scala> p match { case (a,b) => b }
res2: Int = 42
sealed trait Tree[+A]
case class Leaf[A](value: A) extends Tree[A]
case class Branch[A](left: Tree[A], right: Tree[A]) extends Tree[A]
48
www.it-ebooks.info

name on 
, that modifies each element in a tree with a given function.
List
SIDEBAR
ADTs and encapsulation
One might object that algebraic data types violate encapsulation by
making public the internal representation of a type. In FP, we approach
concerns about encapsulation a bit differently—we don't typically have
delicate mutable state which could lead to bugs or violation of invariants
if exposed publicly. Exposing the data constructors of a type is often
fine, and the decision to do so is approached much like any other
decision about what the public API of a data type should be.17
Footnote 17mIt is also possible in Scala to expose patterns like 
 and 
Nil
 
 the actual data constructors of the type.
Cons independent of
We do typically use ADTs for cases where the set of cases is closed
(known to be fixed). For 
 and 
, changing the set of data
List
Tree
constructors would significantly change what these data types are. List 
 a singly-linked list—that is its nature—and the two cases, 
 and 
is
Nil
 form part of its useful public API. We can certainly write code
Cons
which deals with a more abstract API than 
 (we will see examples
List
of this later in the book), but this sort of information hiding can be
handled as a separate layer rather than being baked into 
 directly.
List
EXERCISE 29: Generalize 
, 
, 
, and 
, writing a new
size maximum depth
map
function 
 that abstracts over their similarities. Reimplement them in terms of
fold
this more general function. Can you draw an analogy between this 
 function
fold
and the left and right folds for 
?
List
In this chapter we covered a number of important concepts. We introduced
algebraic data types and pattern matching and showed how to implement purely
functional data structures, including the singly-linked list. Also, through the
exercises in this chapter, we hope you got more comfortable writing pure functions
and generalizing them. We will continue to develop this skill in the chapters ahead.
18
Footnote 18mAs you work through more of the exercises, you may want to read appendix Todo discussing
different techniques for generalizing functions.
3.6 Summary
49
www.it-ebooks.info

companion object
companion object
covariant
data constructor
data sharing
functional data structure
match expression
pattern
pattern
pattern matching
pattern matching
persistence
program trace
tracing
variance
zip
zipWith
Index Terms
50
www.it-ebooks.info

4
In chapter 1 we said that throwing an exception breaks referential transparency.
Let's look at an example:
def failingFn(i: Int): Int = {
  val x: Int = throw new Exception("fail!")
  try {
    val y = 42 + 5
    x + y
  }
  catch { case e: Exception => 43 }
}
Unlike the expression 
, which produces a result of 
, the expression 
42 + 5
47
 does not produce a value at all—its
throw new Exception("fail!")
"result" is to jump to the nearest enclosing 
, which depends on the context
catch
in which it is evaluated. The use of 
 and 
 means we can no longer
throw
catch
reason about our program purely locally by substituting terms with their
definitions—if we replace substitute 
 in 
 with 
x
x + y
throw new
, our program has a different result.
Exception("fail!") + y
How then do we write functional programs which handle errors? That is what
you will learn in this chapter. The technique is based on a simple idea: instead of
throwing an exception, we return a value indicating an exceptional condition has
occurred. This idea might be familiar to anyone who has used return codes in C to
handle exceptions, although in FP it works a bit differently, as we'll see.
Handling errors without exceptions
4.1 Introduction
51
www.it-ebooks.info

Let's consider a more realistic situation where we might use an exception and look
at different approaches we could use instead. Here is an implementation of a
function that computes the mean of a list, which is undefined if the list is empty:
def mean(xs: Seq[Double]): Double =
  if (xs.isEmpty)
    throw new ArithmeticException("mean of empty list!")
  else xs.sum / xs.length
Seq is the common interface of various linear sequence-like collections. Check the
API docs for more information.
sum is defined as a method on Seq using some magic (that we won't get into here)
that makes the method available only if the elements of the sequence are numeric.
mean is an example of what is called a 
: it is not defined for
partial function
some inputs. A function is typically partial because it makes some assumptions
about its inputs that are not implied by the input types.  You may be used to
1
throwing exceptions in this case, but we have a few other options. Let's look at
these for our 
 example:
mean
Footnote 1mA function may also be partial if it does not terminate for some inputs. We aren't going to discuss
this form of partiality here—a running program cannot recover from or detect this nontermination internally, so
there's no question of how best to handle it.
The first possibility is to return some sort of bogus value of type 
. We
Double
could simply return 
 in all cases, and have it result in 
xs.sum / xs.length
 when the input is empty, which is 
, or we could return
0.0/0.0
Double.NaN
some other sentinel value. In other situations we might return 
 instead of a
null
value of the needed type. We reject this solution for a few reasons:
It allows errors to silently propagate—the caller can forget to check this condition and
will not be alerted by the compiler, which might result in subsequent code not working
properly. Often the error won't be detected until much later in the code.
It is not applicable to polymorphic code. For some output types, we might not even have
a sentinel value of that type even if we wanted to! Consider a function like 
 which
max
finds the maximum value in a sequence according to a custom comparison function: def
. If the input were empty, we
max[A](xs: Seq[A])(greater: (A,A) => Boolean): A
cannot invent a value of type . Nor can 
 be used here since 
 is only valid for
A
null
null
non-primitive types, and  is completely unconstrained by this signature.
A
It demands a special policy or calling convention of callers—proper use of the mean
function now requires that callers do something other than simply call 
 and make use
mean
of the result. Giving functions special policies like this makes it difficult to pass them to
4.2 Possible alternatives to exceptions
52
www.it-ebooks.info

higher-order functions, who must treat all functions they receive as arguments uniformly
and will generally only be aware of the types of these functions, not any special policy or
calling convention.
The second possibility is to force the caller to supply an argument which tells
us what to do in case we don't know how to handle the input:
def mean_1(xs: IndexedSeq[Double], onEmpty: Double): Double =
  if (xs.isEmpty) onEmpty
  else xs.sum / xs.length
This makes 
 into a total function, but it has drawbacks—it requires that
mean
immediate callers have direct knowledge of how to handle the undefined case and
limits them to returning a 
. What if 
 is called as part of a larger
Double
mean
computation and we would like to abort that computation if 
 is undefined? Or
mean
perhaps we would like to take some completely different branch in the larger
computation in this case? Simply passing an 
 parameter doesn't give us
onEmpty
this freedom.
We need a way to defer the decision of how to handle undefined cases so that
they can be dealt with at the most appropriate level.
The solution is to represent explicitly in the return type that we may not always
have a defined value. We can think of this as deferring to the caller for the error
handling strategy. We introduce a new type, 
:
Option
sealed trait Option[+A]
case class Some[+A](get: A) extends Option[A]
case object None extends Option[Nothing]
Option has two cases: it can be defined, in which case it will be a 
, or it
Some
can be undefined, in which case it will be 
. We can use this for our definition
None
of 
 like so:
mean
def mean(xs: Seq[Double]): Option[Double] =
  if (xs.isEmpty) None
  else Some(xs.sum / xs.length)
The return type now reflects the possibility that the result is not always defined.
4.3 The Option data type
53
www.it-ebooks.info

We still always return a result of the declared type (now 
)
Option[Double]
from our function, so 
 is now a 
. It takes each value of the input
mean
total function
type to exactly one value of the output type.
Partial functions abound in programming, and 
 (and related data types we
Option
will discuss shortly) is typically how this partiality is dealt with in FP. You'll see 
 used throughout the Scala standard library, for instance:
Option
Map lookup for a given key returns Option
headOption and 
 
 return 
lastOption defined for lists and other iterables
Option
containing the first or last elements of a sequence if it is nonempty.
These aren't the only examples—we'll see 
 come up in many different
Option
situations. What makes 
 convenient is that we can factor out common
Option
patterns of error handling via higher order functions, freeing us from writing the
usual boilerplate that comes with exception-handling code.
Option can be thought of like a 
 that can contain at most one element,
List
and many of the 
 functions we saw earlier have analogous functions on 
List
. Let's look at some of these functions. We are going to do something
Option
slightly different than last chapter. Last chapter we put all the functions that
operated on 
 in the 
 companion object. Here we are going to place our
List
List
functions, when possible, inside the body of the 
 trait, so they can be
Option
called with OO syntax (
 or 
 instead of 
obj.fn(arg1)
obj fn arg1
fn(obj,
). This is a stylistic choice with no real significance, and we'll use both
arg1)
styles throughout this book.2
Footnote 2mIn general, we'll use the OO style where possible for functions that have a single, clear operand
(like 
), and the standalone function style otherwise.
List.map
trait Option[+A] {
  def map[B](f: A => B): Option[B]
  def flatMap[B](f: A => Option[B]): Option[B]
  def getOrElse[B >: A](default: => B): B
  def orElse[B >: A](ob: => Option[B]): Option[B]
  def filter(f: A => Boolean): Option[A]
}
There are a couple new things here. The 
 type annotation in 
default: => B
 (and the similar annotation in 
) indicates the argument will
getOrElse
orElse
4.3.1 Usage patterns for Option
54
www.it-ebooks.info

not be evaluated until it is needed by the function. Don't worry about this for
now—we'll talk much more about it in the next chapter. Also, the 
 parameter
B>:A
on these functions indicates that 
 must be a 
 of 
. It's needed to
B
supertype
A
convince Scala that it is still safe to declare 
 as covariant in 
Option[+A]
A
(which lets the compiler assume things like 
 is a subtype of 
Option[Dog]
). Likewise for 
. This isn't really important to our
Option[Animal]
orElse
purposes; it's mostly an artifact of the OO style of placing the functions that
operate on a type within the body of the 
.
trait
EXERCISE 1: We'll explore when you'd use each of these next. But first, as an
exercise, implement all of the above functions on 
. As you implement
Option
each function, try to think about what it means and in what situations you'd use it.
Here are a few hints:
It is fine to use pattern matching, though you should be able to implement all the
functions besides 
 and 
 without resorting to pattern matching.
map
getOrElse
For 
 and 
, the type signature should be sufficient to determine the
map
flatMap
implementation.
getOrElse returns the result inside the 
 case of the 
, or if the 
 is 
,
Some
Option
Option
None
returns the given default value.
orElse returns the first 
 if it is defined, otherwise, returns the second 
.
Option
Option
Although we can explicitly pattern match on an 
, we will almost
Option
always use the above higher order functions. We'll try to give some guidance for
when to use each of them, but don't worry if it's not totally clear yet. The purpose
here is mostly to get some basic familiarity so you can recognize these patterns as
you start to write more functional code.
The 
 function can be used to transform the result inside an 
, if it
map
Option
exists. We can think of it as proceeding with a computation on the assumption that
an error has not occurred—it is also a way of deferring the error handling to later
code:
case class Employee(name: String, department: String)
val employeesByName: Map[String, Employee] =
  List(Employee("Alice", "R&D"), Employee("Bob", "Accounting")).
  map(e => (e.name, e)).toMap
val dept: Option[String] = employeesByName.get("Joe").map(_.dept)
A dictionary or associative container with String as the key type and Employee as
55
www.it-ebooks.info

the value type
Convenient method for converting a List[(A,B)] to a Map[A,B]
Here, 
 returns an 
, which
employeesByName.get
Option[Employee]
we transform using 
 to pull out the 
 representing the
map
Option[String]
department.
flatMap is similar, except that the function we provide to transform the result
can itself fail.
EXERCISE 2: Implement the 
 function (if the mean is , variance
variance
m
is the mean of 
, see 
) in terms of 
 and 
math.pow(x - m, 2)
definition
mean
.
flatMap 3
Footnote 3mVariance can actually be computed in one pass, but for pedagogical purposes we will compute it
using two passes. The first will compute the mean of the data set, and the second will compute the mean squared
difference from this mean.
def variance(xs: Seq[Double]): Option[Double]
filter can be used to convert successes into failures if the successful values
don't match the given predicate. A common pattern is to transform an 
 via
Option
calls to 
, 
, and/or 
, then use 
 to do error
map flatMap
filter
getOrElse
handling at the end.
val dept: String =
  employeesByName.get("Joe").
  map(_.dept).
  filter(_ != "Accounting").
  getOrElse("Default Dept")
getOrElse is used here to convert from an 
 to a 
Option[String]
, by providing a default department in case the key 
 did not exist in
String
"Joe"
the 
 or if Joe's department was 
.
Map
"Accounting"
orElse is similar to 
, except that we return another 
 if
getOrElse
Option
the first is undefined. This is often useful when we need to chain together possibly
failing computations, trying the second if the first hasn't succeeded.
A 
common 
idiom 
is 
to 
do 
o.getOrElse(throw
 to convert the 
 case of an 
 back to an
AnException("FAIL"))
None
Option
56
www.it-ebooks.info

exception. The general rule of thumb is that we use exceptions only if no
reasonable program would ever catch the exception—if for some callers the
exception might be a recoverable error, we use 
 to give them flexibility.
Option
It may be easy to jump to the conclusion that once we start using 
, it
Option
infects our entire code base. One can imagine how any callers of methods that take
or return 
 will have to be modified to handle either 
 or 
. But
Option
Some
None
this simply doesn't happen, and the reason why is that we can 
 ordinary
lift
functions to become functions that operate on 
.
Option
For example, the 
 function lets us operate on values of type 
map
Option[A]
using a function of type 
, returning 
. Another way of looking
A => B
Option[B]
at this is that 
 turns a function  of type 
 into a function of type 
map
f
A => B
. It might be more clear if we make this more
Option[A] => Option[B]
explicit in the type signature:
def lift[A,B](f: A => B): Option[A] => Option[B] = _ map f
As an example of when you might use 
, let's look at one more example of a
map
function that returns 
:
Option
import java.util.regex._
def pattern(s: String): Option[Pattern] =
  try {
    Some(Pattern.compile(s))
  } catch {
    case e: PatternSyntaxException => None
  }
This example uses the Java standard library's regex package to parse a string
into a regular expression pattern.  If there is a syntax error in the pattern (it's not a
4
valid regular expression), we catch the exception thrown by the library function
and return 
. Methods on the 
 class don't need to know anything
None
Pattern
about 
. We can simply 
 them using the 
 function:
Option
lift
map
Footnote 4mScala runs on the Java Virtual Machine and is completely compatible with all existing Java
libraries. We can therefore call 
, a Java function, exactly as we would any Scala
Pattern.compile
function.
4.3.2 Option composition and lifting
57
www.it-ebooks.info

def mkMatcher(pat: String): Option[String => Boolean] =
  pattern(pat) map (p => (s: String) => p.matcher(s).matches)
The details of this API don't matter too much, but p.matcher(s).matches will check
if the string s matches the pattern p.
Here, the 
 call will return an 
, which
pattern(pat)
Option[Pattern]
will be 
 if 
 is not valid. Notice how we are using the 
 and 
None
pat
matcher
 functions 
 the 
. Because they are inside the 
, they don't
matches
inside
map
map
need to be aware of the outer containing 
. If you don't feel you fully grasp
Option
how this works yet, use the substitution model to execute this on paper step by
step, both for the case where 
 is valid and where it is invalid.
pat
It's also possible to lift a function by using a 
, which in Scala
for-comprehension
is a convenient syntax for writing a sequence of nested calls to 
 and 
map
flatMap
. We'll explain the translation in a minute. First, here are some examples:
def mkMatcher_1(pat: String): Option[String => Boolean] =
  for {
    p <- pattern(pat)
  } yield ((s: String) => p.matcher(s).matches)
def doesMatch(pat: String, s: String): Option[Boolean] =
  for {
    p <- mkMatcher_1(pat)
  } yield p(s)
So far we are only lifting functions that take one argument. But some functions
take more than one argument and we would like to be able to lift them too. The
for-comprehension makes this easy, and we can combine as many options as we
want:
def bothMatch(pat: String, pat2: String, s: String): Option[Boolean] =
  for {
    f <- mkMatcher(pat)
    g <- mkMatcher(pat2)
  } yield f(s) && g(s)
A for-comprehension like this is simply syntax sugar. Internally, Scala will
translate the above to ordinary method calls to 
 and 
:
map
flatMap
def bothMatch_1(pat: String, pat2: String, s: String): Option[Boolean] =
58
www.it-ebooks.info

  mkMatcher(pat) flatMap (f =>
  mkMatcher(pat2) map     (g =>
  f(s) && g(s)))
EXERCISE 3: 
 is an instance of a more general pattern. Write a
bothMatch
generic function 
, that combines two 
 values using a binary function.
map2
Option
If either 
 value is 
, then the return value is too. Here is its signature:
Option
None
def map2[A,B,C](a: Option[A], b: Option[B])(f: (A, B) => C): Option[C]
EXERCISE 4: Re-implement 
 above in terms of this new function,
bothMatch
to the extent possible.
def bothMatch_2(pat1: String, pat2: String, s: String): Option[Boolean]
EXERCISE 5: Write a function 
, that combines a list of Options
sequence
into one option containing a list of all the 
 values in the original list. If the
Some
original list contains 
 even once, the result of the function should be 
,
None
None
otherwise the result should be 
 with a list of all the values. Here is its
Some
signature:5
Footnote 5mThis is a clear instance where it's not possible to define the function in the OO style. This should
not be a method on 
 (which shouldn't need to know anything about 
), and it can't be a method
List
Option
on 
.
Option
def sequence[A](a: List[Option[A]]): Option[List[A]]
Sometimes we will want to map over a list using a function that might fail,
returning 
 if applying it to any element of the list returns None. For example,
None
parsing a whole list of strings into a list of patterns. In that case, we can simply
sequence the results of the 
:
map
def parsePatterns(a: List[String]): Option[List[Pattern]] =
  sequence(a map pattern)
Unfortunately, this is a little inefficient, since it traverses the list twice. Wanting
to sequence the results of a 
 this way is a common enough occurrence to
map
59
www.it-ebooks.info

warrant a new generic function 
, with the following signature:
traverse
def traverse[A, B](a: List[A])(f: A => Option[B]): Option[List[B]]
EXERCISE 6: Implement this function. It is straightforward to do using map
and 
, but try for a more efficient implementation that only looks at the
sequence
list once. In fact, implement 
 in terms of 
.
sequence
traverse
The big idea in this chapter is that we can represent failures and exceptions with
ordinary values, and we can write functions that abstract out common patterns of
error handling and recovery. 
 is not the only data type we could use for
Option
this purpose, and although it gets used quite frequently, it's rather simplistic. One
thing you may have noticed with 
 is that it doesn't tell us very much about
Option
what went wrong in the case of an exceptional condition. All it can do is give us 
 indicating that there is no value to be had. But sometimes we want to know
None
more. For example, we might want a 
 that gives more information, or if an
String
exception was raised, we might want to know what that error actually was.
We can craft a data type that encodes whatever information we want about
failures. Sometimes just knowing whether a failure occurred is sufficient in which
case we can use 
; other times we want more information. In this section,
Option
we'll walk through a simple extension to 
, the 
 data type, which
Option
Either
lets us track a 
 for the failure. Let's look at its definition:
reason
sealed trait Either[+E, +A]
case class Left[+E](value: E) extends Either[E, Nothing]
case class Right[+A](value: A) extends Either[Nothing, A]
Either has only two cases, just like 
. The essential difference is that
Option
both cases carry a value. The 
 data type represents, in a very general way,
Either
values that can be one of two things. We can say that it is a 
 of two
disjoint union
types. When we use it to indicate success or failure, by convention the Left
constructor is reserved for the failure case.6
Footnote 6m
 is also often used more generally to encode one of two possibilities, in cases where it
Either
isn't worth defining a fresh data type. We'll see some examples of this throughout the book.
4.3.3 The Either data type
60
www.it-ebooks.info

SIDEBAR
Option and Either in the standard library
Both 
 and 
 exist in the Scala standard library (
Option
Either
Option
 and 
), and most of the functions we've defined
API link
Either API link
here in this chapter exist for the standard library versions. There are a
few missing functions, though, notably 
, 
, and 
sequence traverse
.
map2
You are encouraged to read through the API for 
 and 
Option
Either
to understand the differences. 
 in particular, does not define a
Either
right-biased 
 directly like we did here, but relies on explicit 
flatMap
 and 
 projection functions. Read the API for details.
left
right
Let's look at the 
 example again, this time returning a 
 in case of
mean
String
failure:
def mean(xs: IndexedSeq[Double]): Either[String, Double] =
  if (xs.isEmpty)
    Left("mean of empty list!")
  else
    Right(xs.sum / xs.length)
Sometimes we might want to include more information about the error, for
example a stack trace showing the location of the error in the source code. In such
cases we can simply return the exception in the 
 side of an 
:
Left
Either
def safeDiv(x: Double, y: Double): Either[Exception, Double] =
  try {
    Right(x / y)
  } catch {
    case e: Exception => Left(e)
  }
EXERCISE 7: Implement versions of 
, 
, 
, and 
 on 
map flatMap orElse
map2
 that operate on the 
 value.
Either
Right
trait Either[+E, +A] {
  def map[B](f: A => B): Either[E, B]
  def flatMap[EE >: E, B](f: A => Either[EE, B]): Either[EE, B]
  def orElse[EE >: E,B >: A](b: => Either[EE, B]): Either[EE, B]
  def map2[EE >: E, B, C](b: Either[EE, B])(f: (A, B) => C):
    Either[EE, C]
}
61
www.it-ebooks.info

Note that with these definitions, 
 can now be used in
Either
for-comprehensions, for instance:
for {
  age <- Right(42)
  name <- Left("invalid name")
  salary <- Right(1000000.0)
} yield employee(name, age, salary)
This will result in 
. Of course, 
Left("invalid name")
Left("invalid
 could be an arbitrary expression like 
 that happens to
name")
foo(x,y,z)
result in a 
.
Left
EXERCISE 8: Implement 
 and 
 for 
.
sequence
traverse
Either
As a final example, here is an application of 
, where the function 
map2
 validates both the given name and the given age before constructing a
mkPerson
valid 
:
Person
case class Person(name: Name, age: Age)
sealed class Name(val value: String)
sealed class Age(val value: Int)
def mkName(name: String): Either[String, Name] =
  if (name == "" || name == null) Left("Name is empty.")
  else Right(new Name(name))
def mkAge(age: Int): Either[String, Age] =
  if (age < 0) Left("Age is out of range.")
  else Right(new Age(age))
def mkPerson(name: String, age: Int): Either[String, Person] =
  mkName(name).map2(mkAge(age))(Person(_, _))
EXERCISE 9: In this implementation, 
 is only able to report one error,
map2
even if both the name and the age are invalid. What would you need to change in
order to report 
 errors? Would you change 
 or the signature of 
both
map2
? Or could you create a new data type that captures this requirement
mkPerson
better than 
 does, with some additional structure? How would 
, 
Either
orElse
, and 
 behave differently for that data type?
traverse
sequence
62
www.it-ebooks.info

disjoint union
for-comprehension
lifting
lifting
Using algebraic data types such as 
 and 
, we can handle errors in
Option
Either
a way that is modular, compositional, and simple to reason about. In this chapter,
we have developed a number of higher-order functions that manipulate errors in
ways that we couldn't otherwise if we were just throwing exceptions. With these
new tools in hand, exceptions should be reserved only for truly unrecoverable
conditions in our programs.
Index Terms
4.4 Conclusion
63
www.it-ebooks.info

5
In chapter 3 we talked about purely functional data structures, using singly-linked
lists as an example. We covered a number of bulk operations on lists — 
, 
map
, 
, 
, 
, etc. We noted that each of these
filter foldLeft foldRight zip
operations makes its own pass over the input and constructs a fresh list for the
output.
Imagine if you had a deck of cards and you were asked to remove the
odd-numbered cards and then remove all the queens. Ideally, you would make a
single pass through the deck, looking for queens and odd-numbered cards at the
same time. This is more efficient than removing the odd cards and then looking for
queens in the remainder. And yet the latter is what Scala is doing in the following
code:1
Footnote 1mWe are using the Scala standard library's 
 type here, where 
 and 
 are methods
List
map
filter
on 
 rather than standalone functions like those we wrote in chapter 3.
List
scala> List(1,2,3,4) map (_ + 10) filter (_ % 2 == 0) map (_ * 3)
List(36,42)
In this expression, 
 will produce an intermediate list that then
map (_ + 10)
gets passed to 
, which in turn constructs a list which
filter (_ % 2 == 0)
gets passed to 
 which then produces the final list. In other words,
map (_ * 3)
each transformation will produce a temporary list that only ever gets used as input
to the next transformation and is then immediately discarded.
Think about how this program will be evaluated. If we manually produce a
trace of its evaluation, the steps would look something like this:
Strictness and laziness
64
www.it-ebooks.info

List(1,2,3,4) map (_ + 10) filter (_ % 2 == 0) map (_ * 3)
List(11,12,13,14) filter (_ % 2 == 0) map (_ * 3)
List(12,14) map (_ * 3)
List(36,42)
Here we are showing the result of each substitution performed to evaluate our
expression (for example, to go from the first line to the second, we have substituted
 with 
, based on
List(1,2,3,4) map (_ + 10)
List(11,12,13,14)
the definition of 
).  This view makes it clear how the calls to 
 and 
map 2
map
 each perform their own traversal of the input and allocate lists for the
filter
output. Wouldn't it be nice if we could somehow fuse sequences of transformations
like this into a single pass and avoid creating temporary data structures? We could
rewrite the code into a while-loop by hand, but ideally we'd like to have this done
automatically while retaining the same high-level compositional style. We want to
use higher-order functions like 
 and 
 instead of manually fusing
map
filter
passes into loops.
Footnote 2mWith program traces like these, it is often more illustrative to not fully trace the evaluation of
every subexpression. For instance, in this case, we've omitted the full expansion of List(1,2,3,4) map
. We could "enter" the definition of 
 and trace its execution but we chose to omit this level of
(_ + 10)
map
detail in this trace.
It turns out that we can accomplish this through the use of 
 (or
non-strictness
more informally, "laziness"). In this chapter, we will explain what exactly this
means, and we'll work through the implementation of a lazy list type that fuses
sequences of transformations. Although building a "better" list is the motivation for
this chapter, we'll see that non-strictness is a fundamental technique for improving
on the efficiency and modularity of functional programs in general.
Before we get to our example of lazy lists, we need to cover some basics. What is
strictness and non-strictness, and how are these concepts expressed in Scala?
SIDEBAR
Termination and strictness
If the evaluation of an expression runs forever or throws an error
instead of returning a definite value, we say that the expression does
not 
, or that it evaluates to 
. A function  is 
 if
terminate
bottom
f
strict
the expression 
 evaluates to bottom for all 
 that evaluate to
f(x)
x
bottom.
Non-strictness is a property of a function. To say a function is non-strict just
5.1 Strict and non-strict functions
65
www.it-ebooks.info

means that the function may choose 
 to evaluate one or more of its arguments.
not
In contrast, a 
 function always evaluates its arguments. Strict functions are the
strict
norm in most programming languages and most languages don't even provide a
way to define non-strict functions. Unless you tell it otherwise, any function
definition in Scala will be strict (and all the functions we have defined so far have
been strict). As an example, consider the following function:
def square(x: Double): Double = x * x
When you invoke 
 square will receive the evaluated
square(41.0 + 1.0)
value of 
 because it is strict. If you were to invoke 
42.0
, you would get an exception, since the 
square(sys.error("failure"))
 expression will be evaluated before entering the
sys.error("failure")
body of 
.
square
Although we haven't yet learned the syntax for indicating non-strictness in
scala, you are almost certainly familiar with non-strictness 
. For
as a concept
example, the Boolean functions 
 and 
 are non-strict. You may be used to
&&
||
thinking of 
 and 
 as built-in syntax, part of the language, but we can also
&&
||
think of them as functions that may choose not to evaluate their arguments. The
function 
 takes two 
 arguments, but only evaluates the second
&&
Boolean
argument if the first is 
:
true
scala> false && { println("!!"); true } // does not print anything
res0: Boolean = false
And 
 only evaluates its second argument if the first is 
:
||
false
scala> true || { println("!!") // doesn't print anything either }
res1: Boolean = true
Another example of non-strictness is the 
 control construct in Scala:
if
val result = if (input.isEmpty) sys.error("empty input") else input
The 
 language construct could also be thought of as a function accepting
if
three parameters: a condition of type 
, an expression of some type  to
Boolean
A
66
www.it-ebooks.info

return in the case that the condition is true, and another expression of the same type
 to return if the condition is false. This 
 function would be non-strict, since it
A
if
will not evaluate all of its arguments. To be more precise, we would say that the 
 function is strict in its condition parameter, since it will always evaluate the
if
condition to determine which branch to take, and non-strict in the two branches for
the 
 and 
 cases, since it will only evaluate one or the other based on
true
false
the condition.
In Scala, we can write non-strict functions by accepting some of our arguments
unevaluated, using the following syntax:
def if2[A](cond: Boolean, onTrue: => A, onFalse: => A): A =
  if (cond) onTrue else onFalse
The arguments we would like to pass unevaluated have 
 immediately before
=>
their type. In the body of the function, we do not need to do anything to evaluate an
argument annotated with 
. We just reference the identifier. We also call this
=>
function with the usual function call syntax:
scala> if2(false, sys.error("fail"), 3)
res2: Int = 3
Scala will take care of making sure that the second and third arguments are
passed unevaluated to the body of 
.
if2 3
Footnote 3mThe unevaluated form of an expression is often called a 
. Thunks are represented at runtime
thunk
in Scala as a value of type 
, which you can see if you're curious by inspecting the
scala.Function0
signature of non-strict functions in the 
 file the Scala compiler generates.
.class
An argument that is passed unevaluated to a function will be evaluated once for
each place it is referenced in the body of the function. That is, Scala will not (by
default) cache the result of evaluating an argument:
scala> def pair(i: => Int) = (i, i)
scala> pair { println("hi"); 1 + 41 }
hi
hi
val res3: (Int, Int) = (42,42)
Here, 
 is referenced twice in the body of 
, and we have made it
i
pair
67
www.it-ebooks.info

particularly obvious that it is evaluated each time by passing the block 
 which prints 
 as a side effect before returning a
println("hi"); 1 + 41
hi
result of 
. The expression 
 will be computed twice as well. Luckily we
42
1 + 41
can cache the value explicitly if we wish to only evaluate the result once, by using
the 
 keyword:
lazy
scala> def pair2(i: => Int) = { lazy val j = i; (j, j) }
scala> pair2 { println("hi"); 1 + 41 }
hi
val res4: (Int, Int) = (42,42)
Adding the 
 keyword to a 
 declaration will cause Scala to delay
lazy
val
evaluation of the right hand side until it is first referenced and will also cache the
result so that subsequent references of  don't trigger repeated evaluation. In this
j
example, we were going to evaluate  on the next line anyway, so we could have
i
just written 
. Despite receiving its argument unevaluated, 
 is
val j = i
pair2
still considered a strict function since it always ends up evaluating its argument. In
other situations, we can use 
 when we don't know if subsequent code
lazy val
will evaluate the expression and simply want to cache the result if it is ever
demanded.
As a final bit of terminology, a non-strict function that evaluates its arguments
each time it references them is said to evaluate those arguments 
; if it
by name
evaluates them only once and then caches their value, it is said to evaluate 
,
by need
or it's said to be 
. We'll often refer to unevaluated parameters in Scala as 
lazy
. Note also that the terms 
 or 
 are
by-name parameters
laziness
lazy evaluation
sometimes used informally to refer to any sort of non-strict evaluation, not
necessarily evaluation by need. When you encounter the word "lazy" in this book,
you can assume that we are using an informal definition.
Let's now return to the problem posed at the beginning of this chapter. We are
going to explore how laziness can be used to improve the efficiency and
modularity of functional programs, using 
, or 
 as an example. We'll
lazy lists
streams
see how chains of transformations on streams are fused into a single pass, through
the use of laziness. Here is a simple 
 definition:
Stream
4
Footnote 4mThere are some subtle possible variations on this definition of 
. We'll touch briefly on
Stream
some of these variations later in this chapter.
5.2 An extended example: lazy lists
68
www.it-ebooks.info

trait Stream[+A] {
  def uncons: Option[(A, Stream[A])]
  def isEmpty: Boolean = uncons.isEmpty
}
object Stream {
def empty[A]: Stream[A] =
  new Stream[A] { def uncons = None }
def cons[A](hd: => A, tl: => Stream[A]): Stream[A] =
  new Stream[A] {
    lazy val uncons = Some((hd, tl))
  }
def apply[A](as: A*): Stream[A] =
  if (as.isEmpty) empty
  else cons(as.head, apply(as.tail: _*))
}
Notice the 
 function is non-strict in its arguments. Thus the head and tail
cons
of the stream will not be evaluated until first requested. We'll sometimes write 
 using the infix, right associative operator 
, so 
cons
#::
1 #:: 2 #:: empty
is equivalent to 
. We could add it to the 
cons(1, cons(2, empty))
Stream
trait in the same way that we added 
 to the 
 trait in the code for chapter 3,
::
List
though there are some annoying additional hoops to jump through to make it
properly non-strict. See the associated code for this chapter if you're interested in
exactly how this syntax is implemented.
Before continuing, let's write a few helper functions to make inspecting streams
easier.
EXERCISE 1: Write a function to convert a 
 to a 
, which will
Stream
List
force its evaluation and let us look at it in the REPL. You can convert to the
regular 
 type in the standard library. You can place this and other functions
List
that accept a 
 inside the 
 trait.
Stream
Stream
def toList: List[A]
EXERCISE 2: Write a function 
 for returning the first  elements of a 
take
n
.
Stream
def take(n: Int): Stream[A]
69
www.it-ebooks.info

EXERCISE 3: Write the function 
 for returning all starting
takeWhile
elements of a 
 that match the given predicate.
Stream
def takeWhile(p: A => Boolean): Stream[A]
You can use 
 and 
 together to inspect the streams we'll be
take
toList
creating. For example, try printing 
.
Stream(1,2,3).take(2).toList
Laziness lets us separate the description of an expression from the evaluation of
that expression. This gives us a powerful ability — we may choose to describe a
"larger" expression than we need, then evaluate only a portion of it. As an
example, consider 
 — we can implement this for 
 much like
foldRight
Stream
we did for 
, but we can implement it lazily:
List
def foldRight[B](z: => B)(f: (A, => B) => B): B =
  uncons match {
    case Some((h, t)) => f(h, t.foldRight(z)(f))
    case None => z
  }
This looks very similar to the 
 we wrote for 
, but notice how
foldRight
List
our combining function, , is non-strict in its second parameter. If  chooses not to
f
f
evaluate its second parameter, this terminates the traversal early. We can see this
by using 
 to implement 
, which checks to see if any value in
foldRight
exists
the 
 matches a given predicate.
Stream
def exists(p: A => Boolean): Boolean =
  foldRight(false)((a, b) => p(a) || b)
Since 
 can terminate the traversal early, we can reuse it to
foldRight
implement 
 rather than writing an explicit recursive function to handle
exists
early termination. This is a simple example where separating the concerns of 
 a computation from the concern of 
 makes our descriptions
describing
evaluation
more reusable than when these concerns are intertwined. This kind of separation of
concerns is a central theme in functional programming.
EXERCISE 4: Implement 
, which checks that all elements in the 
forAll
5.3 Separating program description from evaluation
70
www.it-ebooks.info

 match a given predicate. Your implementation should terminate the
Stream
traversal as soon as it encounters a non-matching value.
def forAll(p: A => Boolean): Boolean
EXERCISE 5: Use 
 to implement 
. This will
foldRight
takeWhile
construct a stream incrementally, and only if the values in the result are demanded
by some other expression.
EXERCISE 6: Implement 
, 
, 
, and 
 using 
map filter append
flatMap
.
foldRight
Because the implementations are incremental, chains of transformations will
avoid fully instantiating the intermediate data structures. Let's look at a simplified
program trace for (a fragment of) the motivating example we started this chapter
with, 
. Take a
Stream(1,2,3,4).map(_ + 10).filter(_ % 2 == 0)
minute to work through this trace to understand what's happening. It's a bit more
challenging than the trace we looked at earlier in this chapter.
Stream(1,2,3,4).map(_ + 10).filter(_ % 2 == 0)
(11 #:: Stream(2,3,4).map(_ + 10)).filter(_ % 2 == 0)
Stream(2,3,4).map(_ + 10).filter(_ % 2 == 0)
(12 #:: Stream(3,4).map(_ + 10)).filter(_ % 2 == 0)
12 #:: Stream(3,4).map(_ + 10).filter(_ % 2 == 0)
12 #:: (13 #:: Stream(4).map(_ + 10)).filter(_ % 2 == 0)
12 #:: Stream(4).map(_ + 10).filter(_ % 2 == 0)
12 #:: (14 #:: Stream().map(_ + 10)).filter(_ % 2 == 0)
12 #:: 14 #:: Stream().map(_ + 10).filter(_ % 2 == 0)
12 #:: 14 #:: Stream()
Apply map to first element
Apply filter to first element
Apply map to second element
Apply filter to second element
Notice how the 
 and 
 transformations are interleaved—the
filter
map
computation alternates between generating a single element of the output of 
,
map
and 
 testing to see if that element is divisible by  (adding it to the output
filter
2
stream if it is), exactly as if we had interleaved these bits of logic in a
special-purpose loop that combined both transformations. Notice we do not fully
71
www.it-ebooks.info

instantiate the intermediate stream that results from the 
. For this reason,
map
people sometimes describe streams as "first-class loops" whose logic can be
combined using higher-order functions like 
 and 
.
map
filter
The incremental nature of stream transformations also has important
consequences for memory usage. In a sequence of stream transformations like this,
the garbage collector can usually reclaim the space needed for each intermediate
stream element, 
 that element is passed on to the next transformation.
as soon as
Here, for instance, the garbage collector can reclaim the space allocated for the
value 
 emitted by 
 as soon as 
 determines it isn't needed. Of course,
13
map
filter
this is a simple example; in other situations we might be dealing with larger
numbers of elements, and the stream elements themselves could be large objects
that retain significant amounts of memory. Being able to reclaim this memory as
quickly as possible can cut down on the amount of memory required by your
program as a whole.5
Footnote 5mWe will have a lot more to say about defining memory-efficient streaming calculations, in
particular calculations that require I/O, in part 4 of this book.
Because they are incremental, the functions we've written also work fine for 
. Here is an example of an infinite 
 of s:
infinite streams
Stream
1
val ones: Stream[Int] = cons(1, ones)
Although 
 is infinite, the functions we've written so far only inspect the
ones
portion of the stream needed to generate the demanded output. For example:
scala> ones.take(5).toList
res0: List[Int] = List(1, 1, 1, 1, 1)
scala> ones.exists(_ % 2 != 0)
res1: Boolean = true
Try playing with a few other examples:
ones.map(_ + 1).exists(_ % 2 == 0)
ones.takeWhile(_ == 1)
ones.forAll(_ != 1)
5.4 Infinite streams and corecursion
72
www.it-ebooks.info

In each case, we get back a result immediately. Be careful though, since it's
easy to write an expression that never terminates. For example ones.forAll(_
 will forever need to inspect more of the series since it will never encounter
== 1)
an element that allows it to terminate with a definite answer.
Let's see what other functions we can discover for generating streams.
EXERCISE 7: Generalize 
 slightly to the function 
 which
ones
constant
returns an infinite 
 of a given value.
Stream
def constant[A](a: A): Stream[A]
EXERCISE 8: Write a function that generates an infinite stream of integers,
starting from , then 
, 
, etc.
n
n + 1 n + 2
def from(n: Int): Stream[Int]
EXERCISE 9: Write a function 
 that generates the infinite stream of
fibs
Fibonacci numbers: 0, 1, 1, 2, 3, 5, 8, and so on.
EXERCISE 10: We can write a more general stream building function. It takes
an initial state, and a function for producing both the next state and the next value
in the generated stream. It is usually called 
:
unfold
def unfold[A, S](z: S)(f: S => Option[(A, S)]): Stream[A]
Option is used to indicate when the 
 should be terminated, if at all. The
Stream
function 
 is the most general 
-building function. Notice how
unfold
Stream
closely it mirrors the structure of the 
 data type.
Stream
unfold and the functions we can implement with it are examples of what is
sometimes called a 
 function. While a recursive function consumes data
corecursive
and eventually terminates, a corecursive function produces data and 
.
coterminates
We say that such a function is 
, which just means that we can always
productive
evaluate more of the result in a finite amount of time (for 
, we just need to
unfold
run the function  one more time to generate the next element). Corecursion is also
f
sometimes called 
. These terms aren't that important to our
guarded recursion
73
www.it-ebooks.info

discussion, but you will hear them used sometimes in the context of functional
programming. If you are curious to learn where they come from and understand
some of the deeper connections, follow the references in the chapter notes.
EXERCISE 11: Write 
, 
, 
, and 
 in terms of 
fibs from constant
ones
.
unfold
EXERCISE 12: Use 
 to implement 
, 
, 
, 
 (as
unfold
map take takeWhile zip
in chapter 3), and 
. The 
 function should continue the traversal as
zipAll
zipAll
long as either stream has more elements — it uses 
 to indicate whether
Option
each stream has been exhausted.
Now that we have some practice writing stream functions, let's return to the
exercise we covered at the end of chapter 3 — a function, 
, to
hasSubsequence
check whether a list contains a given subsequence. With strict lists and
list-processing functions, we were forced to write a rather tricky monolithic loop to
implement this function without doing extra work. Using lazy lists, can you see
how you could implement 
 by combining some other
hasSubsequence
functions we have already written? Try to think about it on your own before
continuing.
EXERCISE 13 (hard): implement 
 using functions you've
startsWith
written. It should check if one 
 is a prefix of another. For instance, 
Stream
 would be 
.
Stream(1,2,3) starsWith Stream(1,2)
true
def startsWith[A](s: Stream[A], s2: Stream[A]): Boolean
EXERCISE 14: implement 
 using 
. For a given 
, 
tails
unfold
Stream
 returns the 
 of suffixes of the input sequence, starting with the
tails
Stream
original 
. So, given 
, it would return 
Stream
Stream(1,2,3)
Stream(Stream(1,2,3), 
Stream(2,3), 
Stream(3),
.
Stream.empty)
def tails: Stream[Stream[A]]
We can now implement 
 using functions we've written:
hasSubsequence
def hasSubsequence[A](s1: Stream[A], s2: Stream[A]): Boolean =
  s1.tails exists (startsWith(_,s2))
74
www.it-ebooks.info

non-strict
strict
thunk
This implementation performs the same number of steps as a more monolithic
implementation using nested loops with logic for breaking out of each loop early.
By using laziness, we can compose this function from simpler components and still
retain the efficiency of the more specialized (and verbose) implementation.
EXERCISE 15 (hard, optional): Generalize 
 to the function 
tails
, which is like a 
 that returns a stream of the
scanRight
foldRight
intermediate results. For example:
scala> Stream(1,2,3).scanRight(0)(_ + _).toList
res0: List[Int] = List(6,5,3,0)
This example would be equivalent to the expression List(1+2+3+0,
. Your function should reuse intermediate results so that
2+3+0, 3+0, 0)
traversing a 
 with  elements always takes time linear in . Can it be
Stream
n
n
implemented using 
? How, or why not? Could it be implemented using
unfold
another function we have written?
In this chapter we have introduced non-strictness as a fundamental technique for
implementing efficient and modular functional programs. As we have seen, while
non-strictness can be thought of as a technique for recovering some efficiency
when writing functional code, it's also a much bigger idea — non-strictness can
improve modularity by separating the description of an expression from the "how
and when" of its evaluation. Keeping these concerns separate lets us reuse a
description in multiple contexts, evaluating different portions of our expression to
obtain different results. We were not able to do that when description and
evaluation were intertwined as they are in strict code. We saw a number of
examples of this principle in action over the course of the chapter and we will see
many more in the remainder of the book.
Index Terms
5.5 Summary
75
www.it-ebooks.info

6
In this chapter, we will be introducing how to write programs that manipulate state
in a purely functional way, using the very simple domain of random number
 as motivation. Although this is an unambitious domain on its own, we'll
generation
be building on what we develop here in chapters to come, and its simplicity makes
it a good place to explore some fundamental issues that you will likely encounter
as you start writing your own functional APIs.
A word before getting started: don't worry if not everything in this chapter sinks
in at first. The goal is more to give you the basic pattern for how to make stateful
APIs purely functional. We will say a lot more about dealing with state and effects
in later parts of the book.
If you need to generate random numbers in Scala, there's a class in Java's standard
library, 
 (
), with a pretty typical imperative API
java.util.Random API link
that relies on side effects. Here's an example of its use:
cala> val rng = new java.util.Random
rng: java.util.Random = java.util.Random@caca6c9
scala> rng.nextDouble
res1: Double = 0.9867076608154569
scala> rng.nextDouble
res2: Double = 0.8455696498024141
scala> rng.nextInt
res3: Int = -623297295
scala> rng.nextInt
res4: Int = 1989531047
Purely functional state
6.1 Generating random numbers using side-effects
76
www.it-ebooks.info

And here's an excerpt of the API, transcribed to Scala:
trait Random {
  def nextInt: Int
  def nextBoolean: Boolean
  def nextDouble: Double
  ...
}
Even if we didn't know anything about what happens inside a 
, we can assume that this class has some internal state that
java.util.Random
gets updated after each invocation, since we would otherwise get the same
"random" value each time. Because these state updates are performed as a side
effect, these methods are not referentially transparent.
What can we do about this? The key to recovering referential transparency is to
make these state updates 
. That is, do not update the state as a side effect,
explicit
but simply return the new state along with the value we are generating. Here's one
possible interface:
trait RNG {
  def nextInt: (Int, RNG)
}
Should generate a random Int. We will later define other functions in terms of
nextInt.
Rather than returning only the generated random number (as is done in 
) and updating some internal state by 
 it in place,
java.util.Random
mutating
we return the random number and the new state, leaving the old state unmodified.
In effect, we separate the 
 of the next state from the concern of 
computing
 that state throughout the program. There is no global mutable memory
propagating
being used—we simply return the next state back to the caller. This leaves the
caller of 
 in complete control of what to do with the new state. Notice
nextInt
that we are still 
 the state, in the sense that users of this API do not
encapsulating
need to know anything about the implementation of the random number generator
itself.
Here is a simple implementation using the same algorithm as 
6.2 Purely functional random number generation
77
www.it-ebooks.info

, which happens to be a kind of random number generator
java.util.Random
called a 
. The details of this implementation aren't
linear congruential generator
really important, but notice that 
 returns both the generated value and a
nextInt
new 
 to use for generating the next value.
RNG
object RNG {
def simple(seed: Long): RNG = new RNG {
  def nextInt = {
    val seed2 = (seed*0x5DEECE66DL + 0xBL) &
                ((1L << 48) - 1)
    ((seed2 >>> 16).asInstanceOf[Int],
     simple(seed2))
  }
}
}
& is bitwise AND
<< is left binary shift
>>> is right binary shift with zero fill
This problem of making seemingly stateful APIs pure, and its solution, of
having the API 
 the next state rather than actually mutate anything, is not
compute
unique to random number generation. It comes up quite frequently, and we can
always deal with it in this same way.1
Footnote 1mThere is an efficiency loss that comes with computing next states using pure functions, because it
means we cannot actually mutate the data in place. (Here, it is not really a problem since the state is just a single 
 that must be copied.) This loss of efficiency can be mitigated by using efficient purely functional data
Long
structures. It's also possible in some cases to mutate the data in place without breaking referential transparency.
We'll be discussing this in Part 4.
For instance, suppose you have a class like this:
class Foo {
  var s: FooState = ...
  def bar: Bar
  def baz: Int
}
Suppose 
 and 
 each mutate  in some way. We can mechanically
bar
baz
s
translate this to the purely functional API:
trait Foo {
  def bar: (Bar, Foo)
  def baz: (Int, Foo)
78
www.it-ebooks.info

}
In both of these cases, we are making state propagation explicit.
Whenever we use this pattern, we make users of the API responsible for
threading the computed next state through their programs. For the pure RNG
interface above, if you reuse a previous 
, it will always generate the same
RNG
value it generated before. For instance:
def randomPair(rng: RNG): (Int,Int) = {
  val (i1,_) = rng.nextInt
  val (i2,_) = rng.nextInt
  (i1,i2)
}
Here, 
 and 
 will be the same! If we want to generate two distinct numbers,
i1
i2
we need to use the 
 returned by the first call to 
 to generate the
RNG
nextInt
second 
.
Int
def randomPair(rng: RNG): ((Int,Int), RNG) = {
  val (i1,rng2) = rng.nextInt
  val (i2,rng3) = rng2.nextInt
  ((i1,i2), rng3)
}
Notice use of rng2 here.
Notice we return the final state, after generating the two random numbers. This lets
the caller generate more random values using the new state.
You can see the general pattern, and perhaps you can also see how it might get
somewhat tedious to use this API directly. Let's write a few functions to generate
random values and see if we notice any patterns we can factor out.
EXERCISE 1: Write a function to generate a random positive integer. Note:
you can use 
 to take the absolute value of an 
, . Make sure to handle
x.abs
Int x
the corner case 
, which doesn't have a positive counterpart.
Int.MinValue
def positiveInt(rng: RNG): (Int, RNG)
79
www.it-ebooks.info

SIDEBAR
Dealing with awkwardness in FP
As you write more functional programs, you'll sometimes encounter
situations like this where the functional way of expressing a program
feels awkward or tedious. Does this imply that FP is the programming
equivalent of trying to write an entire novel without using the letter 'e'?
Of course not. Awkwardness like this is almost always a sign of some
missing abstraction waiting to be discovered!
When you encounter these situations, we encourage you to plow ahead
and look for common patterns you can factor out. Most likely, this is a
problem others have encountered, and you may even rediscover the
"standard" solution yourself. Even if you get stuck, struggling to puzzle
out a clean solution yourself will help you to better understand what
solutions others have discovered to deal with the same or similar
problems.
With practice, experience, and more familiarity with the idioms of FP,
expressing a program functionally will become effortless and natural.2
Footnote 2mOf course, good design is still hard, but programming using
pure functions becomes easy with experience.
EXERCISE 2: Write a function to generate a 
 between  and , not
Double
0
1
including . Note: you can use 
 to obtain the maximum positive
1
Int.MaxValue
integer value and you can use 
 to convert an 
, , to a 
.
x.toDouble
Int x
Double
def double(rng: RNG): (Double, RNG)
EXERCISE 3: Write functions to generate an 
 pair, a 
(Int, Double)
 pair, and a 
 3-tuple. You
(Double, Int)
(Double, Double, Double)
should be able to reuse the functions you've already written.
def intDouble(rng: RNG): ((Int,Double), RNG)
def doubleInt(rng: RNG): ((Double,Int), RNG)
def double3(rng: RNG): ((Double,Double,Double), RNG)
EXERCISE 4: Write a function to generate a list of random integers.
def ints(count: Int)(rng: RNG): (List[Int], RNG)
80
www.it-ebooks.info

Looking back at our implementations, we notice a common pattern: each of our
functions has a type of the form 
 for some type . Functions
RNG => (A, RNG)
A
of this type describe 
 that transform 
 states, and these state actions
state actions
RNG
can be built up and combined using general-purpose functions.
To make them convenient to talk about, let's make a type alias for the 
 state
RNG
action data type:
type Rand[+A] = RNG => (A, RNG)
We can now turn methods such as 
's 
 into values of this type:
RNG
nextInt
val int: Rand[Int] = _.nextInt
We want to start writing combinators that let us avoid explicitly passing along
the 
 state. This will become a kind of domain-specific language that does all of
RNG
this passing for us. For example, a simple 
-transition is the 
 action,
RNG
unit
which passes the 
 state through without using it, always returning a constant
RNG
value rather than a random value.
def unit[A](a: A): Rand[A] =
  rng => (a, rng)
There is also 
, for transforming the output of a state action without
map
modifying the state itself. Remember, 
 is just a type alias for a function
Rand[A]
type 
, so this is just a kind of function composition.
RNG => (A, RNG)
def map[A,B](s: Rand[A])(f: A => B): Rand[B] =
  rng => {
    val (a, rng2) = s(rng)
    (f(a), rng2)
  }
EXERCISE 5: Use 
 to generate an 
 between  and , inclusive:
map
Int
0
n
def positiveMax(n: Int): Rand[Int]
6.3 A better API for state actions
81
www.it-ebooks.info

EXERCISE 6: Use 
 to reimplement 
 in a more elegant way.
map
RNG.double
EXERCISE 7: Unfortunately, 
 is not powerful enough to implement 
map
 and 
 from before. What we need is a new combinator 
intDouble
doubleInt
, that can combine two RNG actions into one using a binary rather than unary
map2
function. Write its implementation and then use it to reimplement the intDouble
and 
 functions.
doubleInt
def map2[A,B,C](ra: Rand[A], rb: Rand[B])(f: (A, B) => C): Rand[C]
EXERCISE 8 (hard): If we can combine two RNG transitions, we should be
able to combine a whole list of them. Implement 
, for combining a 
sequence
 of transitions into a single transition. Use it to reimplement the 
List
ints
function you wrote before. For the latter, you can use the standard library function 
 to make a list with  repeated  times.
List.fill(n)(x)
x
n
def sequence[A](fs: List[Rand[A]]): Rand[List[A]]
We're starting to see a pattern: We're progressing towards implementations that
don't explicitly mention or pass along the 
 value. The 
 and 
RNG
map
map2
combinators allowed us to implement, in a rather succinct and elegant way,
functions that were otherwise tedious and error-prone to write. But there are some
functions that we can't very well write in terms of 
 and 
.
map
map2
Let's go back to 
 and see if it can be implemented in terms of 
positiveInt
. It's possible to get most of the way there, but what do we do in the case that 
map
 is generated? It doesn't have a positive counterpart and we can't
Int.MinValue
just select an arbitrary number:
def positiveInt: Rand[Int] = {
  map(int) { i =>
    if (i != Int.MinValue) i.abs else ??
  }
}
What goes here?
We want to retry the generator in the case of 
, but we don't
Int.MinValue
actually have an 
! Besides, anything except an 
 there would have the
RNG
Int
82
www.it-ebooks.info

wrong type. So we clearly need a more powerful combinator than 
.
map
EXERCISE 9: Implement 
, then use it to reimplement 
flatMap
.
positiveInt
def flatMap[A,B](f: Rand[A])(g: A => Rand[B]): Rand[B]
EXERCISE 10: Reimplement 
 and 
 in terms of 
.
map
map2
flatMap
The functions you've just written, 
, 
, 
, 
, and 
unit
map
map2
flatMap
, are not really specific to random number generation at all. They are
sequence
general-purpose functions for working with state actions, and don't actually care
about the type of the state. Notice that, for instance, 
 does not care that it is
map
dealing with 
 state actions and we can give it a more general signature:
RNG
def map[S,A,B](a: S => (A,S))(f: A => B): S => (B,S)
Changing this signature doesn't require modifying the implementation of 
!
map
The more general signature was there all along, we just didn't see it.
We should then come up with a more general type than 
, for handling any
Rand
type of state:
type State[S,+A] = S => (A,S)
Here, 
 is short for "state action" (or even "state transition"). We might
State
even want to write it as its own class, wrapping the underlying function like this:
case class State[S,+A](run: S => (A,S))
The representation doesn't matter too much. What is important is that we have a
single, general-purpose type and using this type we can write general-purpose
functions for capturing common patterns of handling and propagating state.
In fact, we could just make 
 a type alias for 
:
Rand
State
type Rand[A] = State[RNG, A]
EXERCISE 11: Generalize the functions 
, 
, 
, 
, and 
unit map map2 flatMap
83
www.it-ebooks.info

. Add them as methods on the 
 case class where possible.
sequence
State
Otherwise you should put them in a 
 companion object.
State
The functions we've written here capture only a few of the most common
patterns. As you write more functional code, you'll likely encounter other patterns
and discover other functions to capture them.
In the sections above, we were writing functions that followed a definite pattern.
We would run a state action, assign its result to a 
, then run another state
val
action that used that 
, assign its result to another 
, and so on. It looks a lot
val
val
like 
 programming.
imperative
In the imperative programming paradigm, a program is a sequence of
statements where each statement may modify the program state. That's exactly
what we have been doing, except that our "statements" are really state actions,
which are really functions. As functions, they read the current program state simply
by receiving it in their argument, and they write to the program state simply by
returning a value.
SIDEBAR
Aren't imperative and functional programming opposites?
Absolutely not. Remember, functional programming is simply
programming without side-effects. Imperative programming is about
programming with statements that modify some program state, and as
we've seen it's entirely reasonable to maintain state without
side-effects.
Functional programming has excellent support for writing imperative
programs, with the added benefit that such programs can be reasoned
about equationally because they are referentially transparent.
We implemented some combinators like 
, 
, and ultimately 
,
map map2
flatMap
to handle the propagation of the state from one statement to the next. But in doing
so, we seem to have lost a bit of the imperative mood.
Consider as an example the following (which assumes that we have made 
 a type alias for 
):
Rand[A]
State[RNG, A]
int.flatMap(x =>
int.flatMap(y =>
ints(x).map(xs =>
xs.map(_ % y))))
6.4 Purely functional imperative programming
84
www.it-ebooks.info

It's not very clear what's going on here. But since we have 
 and 
map
flatMap
defined, we can use 
-comprehension to recover the imperative style:
for
for {
  x <- int
  y <- int
  xs <- ints(x)
} yield xs.map(_ % y)
This code is much easier to read (and write), and it looks like what it is—an
imperative program that maintains some state. But it is 
. We get the
the same code
next 
 and assign it to , get the next 
 after that and assign it to , then
Int
x
Int
y
generate a list of length , and finally return the list with all of its elements
x
wrapped around the modulus .
y
To facilitate this kind of imperative programming with 
-comprehensions
for
(or 
s), we really only need two primitive 
 combinators—one for
flatMap
State
reading the state and one for writing the state. If we imagine that we have a
combinator 
 for getting the current state, and a combinator 
 for setting a
get
set
new state, we could implement a combinator that can modify the state in arbitrary
ways:
def modify[S](f: S => S): State[S, Unit] = for {
  s <- get
  _ <- set(f(s))
} yield ()
Gets the current state and assigns it to `s`.
Sets the new state to `f` applied to `s`.
This method returns a 
 action that modifies the current state by the
State
function . It yields 
 to indicate that it doesn't have a return value other than
f
Unit
the state.
EXERCISE 12: Come up with the signatures for 
 and 
, then write their
get
set
implementations.
EXERCISE 13 (hard): To gain experience with the use of 
, implement a
State
simulation of a simple candy dispenser. The machine has two types of input: You
can insert a coin, or you can turn the knob to dispense candy. It can be in one of
two states: locked or unlocked. It also tracks how many candies are left and how
many coins it contains.
85
www.it-ebooks.info

sealed trait Input
case object Coin extends Input
case object Turn extends Input
case class Machine(locked: Boolean, candies: Int, coins: Int)
The rules of the machine are as follows:
Inserting a coin into a locked machine will cause it to unlock if there is
any candy left.
Turning the knob on an unlocked machine will cause it to dispense candy
and become locked.
Turning the knob on a locked machine or inserting a coin into an
unlocked machine does nothing.
A machine that is out of candy ignores all inputs.
The method 
 should operate the machine based on the list
simulateMachine
of inputs and return the number of coins left in the machine at the end. Note that if
the input 
 has 
 coins in it, and a net total of  coins are added in the
Machine
10
4
inputs, the output will be 
.
14
def simulateMachine(inputs: List[Input]): State[Machine, Int]
In this chapter, we touched on the subject of how to deal with state and state
propagation. We used random number generation as the motivating example, but
the overall pattern comes up in many different domains, and this chapter illustrated
the basic idea of how to handle state in a purely functional way. The idea is very
simple: we use a pure function that accepts a state as its argument, and it returns
the new state alongside its result. Next time you encounter an imperative API that
relies on side effects, see if you can provide a purely functional version of it, and
use some of the functions we wrote here to make working with it more convenient.
6.5 Summary
86
www.it-ebooks.info

7
In this chapter, we are going to build a library for creating and composing parallel
and asynchronous computations. We are going to work iteratively, refining our
design and implementation as we gain a better understanding of the domain and the
design space.
Before we begin, let's think back to the libraries we wrote in Part 1, for example
the functions we wrote for 
 and 
. In each case we defined a data
Option
Stream
type and wrote a number of useful functions for creating and manipulating values
of that type. But there was something interesting about the functions we wrote. For
instance consider 
—if you look back, you'll notice we wrote only a few 
Stream
 functions (like 
 and 
) which required knowledge of
primitive
foldRight
unfold
the internal representation of 
 (consisting of its 
 method). We
Stream
uncons
then wrote a large number of 
 or 
 without
derived operations
combinators
introducing additional primitives, just by combining existing functions.
In Part 1, very little design effort went into creating these nicely compositional
libraries. We created our data types and found, perhaps surprisingly, that it was
possible to define a large number of useful operations over these data types, just by
combining existing functions. When you create a library for a new domain, the
design process won't always be this easy. You will need to choose data types and
functions that 
, and this is what makes
facilitate this compositional structure
functional design both challenging and interesting.
Purely functional parallelism
7.1 Introduction
87
www.it-ebooks.info

SIDEBAR
What is a combinator library?
The libraries we wrote in Part 1, and the libraries we will write in Part 2
are sometimes called 
. This is a somewhat informal
combinator libraries
term in FP. Generally, it's used to describe a library consisting of one or
more data types, along with a collection of (often higher-order) functions
for creating, manipulating, and combining values of these types. In
particular, the name is usually applied to libraries with a very
compositional structure, where functions are combined in different ways
to produce richer and more complex functionality. Though because this
style of organization is so common in FP, we sometimes don't bother to
distinguish between an ordinary functional library and a "combinator
library".
Our goal in this section is to discover a data type and a set of primitive functions
for our domain, and derive some useful combinators. This will be a somewhat
meandering journey. Functional design can be a messy, iterative process. We hope
to show at least a stylized view of this messiness that nonetheless gives some
insight into how functional design proceeds in the real world. Don't worry if you
don't follow absolutely every bit of discussion throughout this process. This
chapter is a bit like peering over the shoulder of someone as they think through
possible designs. And because no two people approach this process the same way,
the particular path we walk here might not strike you as the most natural
one—perhaps it considers issues in what seems like an odd order, skips too fast or
goes too slow. Keep in mind that when you design your own functional libraries,
you get to do it at your own pace, take whatever path you want, and whenever
questions come up about design choices, you get to think through the consequences
in whatever way makes sense for you, which could include running little
experiments, creating prototypes, and so on.
With that as disclaimer, why don't we get started? When you begin designing a
functional library, you usually have some ideas about what you generally want to
be able 
, and the difficulty in the design process is in refining these ideas and
to do
finding a data type that enables the functionality you want. In our case, we'd like to
be able to "create parallel computations", but what does that mean exactly? Let's
7.2 Choosing data types and functions
88
www.it-ebooks.info

try to refine this into something we can implement by examining a simple,
parallelizable computation—summing a sequence of values. Here's a sequential
function for this in Scala:
def sum(as: IndexedSeq[Int]): Int =
  if (as.size <= 1) as.headOption getOrElse 0
  else {
    val (l,r) = as.splitAt(as.length/2)
    sum(l) + sum(r)
  }
headOption is a method defined on all collections in Scala (API docs). It returns
the first element in the collection, or None if the collection is empty.
This implementation isn't the usual left fold, 
;
as.foldLeft(0)(_ + _)
instead we are dividing the sequence in half using the 
 function,
splitAt
recursively summing both halves, then combining their results. And unlike the 
-based implementation, this implementation can be parallelized—the
foldLeft
two halves can be summed in parallel.1
Footnote 1mAssuming we have  elements and  CPU cores, we could in principle compute 
 in 
n
n
sum
O(log n)
time. This is meant to be a simple example (see sidebar) though; in this instance the overhead of
parallelization is unlikely to pay for itself.
SIDEBAR
The importance of simple examples
Summing integers is in practice probably so fast that parallelization
imposes more overhead than it saves. But simple examples like this are
 the sort that are most helpful to consider when designing a
exactly
functional library. Complicated examples include all sorts of incidental
structure and extraneous detail that can confuse the initial design
process. We are trying to understand the essence of the problem
domain, and a good way to do this is to work with very small examples,
factor out common concerns across these examples, and gradually add
complexity. In functional design, our goal is to achieve expressiveness
not with mountains of special cases, but by building a simple and 
 set of core data types and functions.
composable
As we think about how what sort of data types and functions could enable
parallelizing this computation, we can shift our perspective. Rather than focusing
on 
 this parallelism will ultimately be implemented (likely using 
how
 and 
 and related types) and
java.lang.Thread
java.lang.Runnable
forcing ourselves to work with those APIs directly, we are instead going to design
89
www.it-ebooks.info

our 'ideal' API as illuminated by our examples and work backwards from there to
an implementation.
Look at the line 
, which invokes 
 on the two halves
sum(l) + sum(r)
sum
recursively. Just from looking at this single line, we can see that 
 data
whatever
type we choose to represent our parallel computations needs to be able to contain a
, that result will have some meaningful type (in this case 
), and we
result
Int
require some way of extracting this result. Let's apply this newfound knowledge to
our implementation. For now, let's just 
 a container type for our result, 
invent
 (for "parallel"), and assume the existence of the functions we need:
Par[A]
def unit[A](a: => A): Par[A], for taking an unevaluated  and returning a parallel
A
computation that yields an .
A
def get[A](a: Par[A]): A, for extracting the resulting value from a parallel
computation.
Can we really just do this? Yes, of course! For now, we don't need to worry
about what other functions we require, what the internal representation of Par
might be, or how these functions are implemented. We are simply reading off the
needed data types and functions by inspecting our simple example. Let's update
this example now:
def sum(as: IndexedSeq[Int]): Int =
  if (as.size <= 1) as.headOption getOrElse 0
  else {
    val (l,r) = as.splitAt(as.length/2)
    val sumL: Par[Int] = Par.unit(sum(l))
    val sumR: Par[Int] = Par.unit(sum(r))
    Par.get(sumL) + Par.get(sumR)
  }
We've wrapped the two recursive calls to 
 in calls to 
, and we are
sum
unit
calling 
 to extract the two results from the two subcomputations.
get
90
www.it-ebooks.info

SIDEBAR
The problem with using threads directly
What of 
, or 
? Let's take a look at these class
java.lang.Thread
Runnable
Here is a partial excerpt of their API, transcribed to Scala:
trait Runnable { def run: Unit }
class Thread(r: Runnable) {
  def start: Unit
  def join: Unit
}
Begin executing r in a separate thread.
Wait until r finishes executing before returning.
Already, we can see a problem with both of these types—none of the metho
return a meaningful value. Therefore, if we want to get any information out of
, it has to have some side effect, like mutating some internal state t
Runnable
we know about. This is bad for composability—we cannot manipulate Runnab
objects generically, we need to always know something about their inter
behavior to get any useful information out of them. 
 also has t
Thread
downside that it maps directly onto operating system threads, which are a sca
resource. We'd prefer to create as many 'logical' parallel computations as
natural for our problem, and later deal with mapping these logical computatio
onto actual OS threads.
We now have a choice about the meaning of 
 and 
—
 could
unit
get
unit
begin evaluating its argument immediately in a separate (logical) thread,  or it
2
could simply hold onto its argument until 
 is called and begin evaluation then.
get
But notice that in this example, if we want to obtain any degree of parallelism, we
require that 
 begin evaluating its argument immediately. Can you see why?
unit
3
Footnote 2mWe'll use the term "logical thread" somewhat informally throughout this chapter, to mean a chunk
of computation that runs concurrent to the main execution thread of our program. There need not be a one-to-one
correspondence between logical threads and OS threads. We may have a large number of logical threads mapped
onto a smaller number of OS threads via thread pooling, for instance.
Footnote 3mFunction arguments in Scala are strictly evaluated from left to right, so if 
 delays execution
unit
until 
 is called, we will both spawn the parallel computation and wait for it to finish before spawning the
get
second parallel computation. This means the computation is effectively sequential!
But if 
 begins evaluating its argument immediately, then calling 
unit
get
arguably breaks referential transparency. We can see this by replacing 
 and 
sumL
91
www.it-ebooks.info

 with their definitions—if we do so, we still get the same result, but our
sumR
program is no longer parallel:
Par.get(Par.unit(sum(l))) + Par.get(Par.unit(sum(r)))
Given what we have decided so far, 
 will start evaluating its argument
unit
right away. And the very next thing to happen is that 
 will wait for that
get
evaluation to complete. So the two sides of the  sign will not run in parallel if we
+
simply inline the 
 and 
 variables. Here we can see that 
 has a very
sumL
sumR
unit
definite side-effect, but only 
 
. That is, 
 simply returns a 
with regard to get
unit
 in this case, representing an asynchronous computation. But as soon as
Par[Int]
we pass that 
 to 
, we explicitly wait for it, exposing the side-effect. So it
Par
get
seems that we want to avoid calling 
, or at least delay calling it until the very
get
end. We want to be able to combine asynchronous computations without waiting
for them to finish.
Before we continue, notice what we have done. First, we conjured up a simple,
almost trivial example. We next explored this example a bit to uncover a design
choice. Then, via some experimentation, we discovered an interesting consequence
of one option and in the process learned something fundamental about the nature of
our problem domain! The overall design process is a series of these little
adventures. You don't need any special license to do this sort of exploration, and
you don't need to be an expert in functional programming either. Just dive in and
see what you find.
Let's see if we can avoid the above pitfall of combining 
 and 
. If we
unit
get
don't call 
, that implies that our 
 function must return a 
. What
get
sum
Par[Int]
consequences does this change reveal? Again, let's just invent functions with the
required signatures:
def sum(as: IndexedSeq[Int]): Par[Int] =
  if (as.size <= 1) Par.unit(as.headOption getOrElse 0)
  else {
    val (l,r) = as.splitAt(as.length/2)
    Par.map2(sum(l), sum(r))(_ + _)
  }
EXERCISE 1: 
 is a new higher-order function for combining the
Par.map2
result of two parallel computations. What is its signature? Give the most general
92
www.it-ebooks.info

signature possible (that is, do not assume it works only for 
).
Par[Int]
Observe that we are no longer calling 
 in the recursive case, and it isn't
unit
clear whether 
 should accept its argument lazily anymore. In this example,
unit
accepting the argument lazily doesn't seem to provide any benefit, but perhaps this
isn't always the case. Let's try coming back to this question in a minute.
What about 
—should it take its arguments lazily? It would make sense for
map2
 to run both sides of the computation in parallel, giving each side equal
map2
opportunity to run (it would seem arbitrary for the order of the 
 arguments to
map2
matter—we simply want 
 to indicate that the two computations being
map2
combined are independent, and can be run in parallel). What choice lets us
implement this meaning? As a simple test case, consider what happens if 
 is
map2
strict in both arguments, and we are evaluating sum(IndexedSeq(1,2,3,4))
. Take a minute to work through and understand the following (somewhat stylized)
program trace:
sum(IndexedSeq(1,2,3,4))
map2(
  sum(IndexedSeq(1,2)),
  sum(IndexedSeq(3,4)))(_ + _)
map2(
  map2(
    sum(IndexedSeq(1)),
    sum(IndexedSeq(2)))(_ + _),
  sum(IndexedSeq(3,4)))(_ + _)
map2(
  map2(
    unit(1),
    unit(2))(_ + _),
  sum(IndexedSeq(3,4)))(_ + _)
map2(
  map2(
    unit(1),
    unit(2))(_ + _),
  map2(
    sum(IndexedSeq(3)),
    sum(IndexedSeq(4)))(_ + _))(_ + _)
...
In this trace, to evaluate 
, we substitute  into the definition of 
, as
sum(x)
x
sum
we've done in previous chapters. Because 
 is strict, and Scala evaluates
map2
arguments left to right, whenever we encounter map2(sum(x),sum(y))(_ +
, we have to then evaluate 
, and so on recursively. This has the rather
_)
sum(x)
unfortunate consequence that we will strictly construct the entire left half of the
93
www.it-ebooks.info

tree of summations first before moving on to (strictly) constructing the right half
(here, 
 gets fully expanded before we consider 
sum(IndexedSeq(1,2))
). And if 
 begins evaluating its arguments
sum(IndexedSeq(3,4))
map2
immediately (using whatever resource is being used to implement the parallelism,
like a thread pool), that implies the left half of our computation will begin its
execution before we even begin constructing the right half of our computation.
What if we keep 
 strict, but 
 have it begin execution immediately?
map2
don't
Does this help? If 
 doesn't begin evaluation immediately, this implies a 
map2
Par
value is merely constructing a 
 of what needs to be computed in
description
parallel. Nothing actually occurs until we 
 this description, perhaps using a
evaluate
-like function. The problem is that if we construct our descriptions strictly,
get
they are going to be rather heavyweight objects. Looking back at our trace, our
description is going to have to contain the full tree of operations to be performed:
map2(
  map2(
    unit(1),
    unit(2))(_ + _),
  map2(
    unit(3),
    unit(4))(_ + _))(_ + _)
Whatever data structure we use to store this description, it will likely occupy
more space than the original list itself! It would be nice if our descriptions were a
bit more lightweight.
It seems we should make 
 lazy and have it begin immediate execution of
map2
both sides in parallel (this also addresses the problem of giving each side equal
"weight"), but something still doesn't feel right about this. Is it 
 the case that
always
we want to evaluate the two arguments to 
 in parallel? Probably not.
map2
Consider this simple hypothetical example:
Par.map2(Par.unit(1), Par.unit(1))(_ + _)
In this case, we happen to know that the two computations we're combining
will execute so quickly that there isn't much point in spawning off a separate
logical thread to evaluate them. But our API doesn't give us any way of providing
this sort of information. That is, our current API is very 
 about when
implicit
computations get forked off the main thread—the programmer does not get to
94
www.it-ebooks.info

specify where this forking should occur. What if we make this forking more
explicit? We can do that by inventing another function, def fork[A](a: =>
, which we can take to mean that the given 
 should be
Par[A]): Par[A]
Par
run in a separate logical thread:
def sum(as: IndexedSeq[Int]): Par[Int] =
  if (as.isEmpty) Par.unit(0)
  else {
    val (l,r) = as.splitAt(as.length/2)
    Par.map2(Par.fork(sum(l)), Par.fork(sum(r)))(_ + _)
  }
With 
, we can now make 
 strict, leaving it up to the programmer to
fork
map2
wrap arguments if they wish. A function like 
 solves the problem of
fork
instantiating our parallel computations too strictly (as an exercise, try revisiting the
hypothetical trace from earlier), but more fundamentally it makes the parallelism
more explicit and under programmer control. There are really two separate
concerns being addressed here. The first is that we need some way to indicate that
the results of the two parallel tasks should be combined. Separate from this, we
have the choice of whether a particular task should be performed asynchronously.
By keeping these concerns separate, we avoid having any sort of global policy for
parallelism attached to 
 and other combinators we write, which would mean
map2
making tough (and ultimately arbitrary) choices about what global policy is best.
Such a policy may in practice be inappropriate in many cases.
Let's now return to the question of whether 
 should be strict or lazy. With 
unit
, we can now make 
 strict without any loss of expressiveness. A
fork
unit
non-strict version of it, let's call it 
, can be implemented using 
 and 
async
unit
.
fork
def unit[A](a: A): Par[A]
def async[A](a: => A): Par[A] = fork(unit(a))
The function 
 is a simple example of a 
 combinator, as opposed
async
derived
to a 
 combinator like 
. We were able to define 
 just in terms
primitive
unit
async
of other operations. Later, when we pick a representation for 
, 
 will not
Par async
need to know anything about this representation—its only knowledge of 
 is
Par
through the operations 
 and 
 that are defined on 
.
fork
unit
Par 4
95
www.it-ebooks.info

Footnote 4mThis sort of indifference to representation is a hint that the operations are actually more general,
and can be abstracted to work for types other than just 
. We will explore this topic in detail in part 3.
Par
We still have the question of whether 
 should begin evaluating its
fork
argument immediately, or wait until the computation is 
 later using
forced
something like 
. When you are unsure about a meaning to assign to some
get
function in your API, you can always continue with the design process—at some
point later the tradeoffs of different choices of meaning may become clear. Here,
we make use of a helpful trick—we are going to think about what sort of
 is required to implement 
 with various meanings.
information
fork
If 
 begins evaluating its argument immediately in parallel, the
fork
implementation must clearly know something (either directly or indirectly) about
how to create threads or submit tasks to some sort of thread pool. Moreover, if 
 is just a standalone function (as it currently is on 
), it implies that
fork
Par
whatever resource is used to implement the parallelism must be globally accessible
. This means we lose the ability to control the parallelism strategy used for
different parts of our program. And while there's nothing inherently wrong with
having a global resource for executing parallel tasks, we can imagine how it would
be useful to have more fine-grained control over what implementations are used
where (we might like for each subsystem of a large application to get its own
thread pool with different parameters, say).
Notice that coming to these conclusions didn't require knowing exactly how 
 would be implemented, or even what the representation of 
 will be. We
fork
Par
just reasoned informally about the sort of information required to actually spawn a
parallel task, and examined the consequences of having 
 values know about
Par
this information.
In contrast, if 
 simply holds onto the computation until later, this requires
fork
no access to the mechanism for implementing parallelism. Let's tentatively assume
this meaning then for 
. With this model, 
 itself does not know how to
fork
Par
actually 
 the parallelism. It is more a 
 of a parallel
implement
description
computation. This is a big shift from before, where we were considering 
 to be
Par
a "container" of a value that we could "get". Now it's more of a first-class program
that we can 
. So let's rename our 
 function to 
.
run
get
run
def run[A](a: Par[A]): A
96
www.it-ebooks.info

Because 
 now just a pure data structure, we will assume that 
 has some
Par
run
means of implementing the parallelism, whether it spawns new threads, delegates
tasks to a thread pool, or uses some other mechanism.
Just by exploring this simple example and thinking through the consequences of
different choices, we've sketched out the following API:
def unit[A](a: A): Par[A]
def map2[A,B,C](a: Par[A], b: Par[B])(f: (A,B) => C): Par[C]
def fork[A](a: => Par[A]): Par[A]
def async[A](a: => A): Par[A] = fork(unit(a))
def run[A](a: Par[A]): A
We've also loosely assigned meaning to these various functions:
unit injects a constant into a parallel computation.
map2 combines the results of two parallel computations with a binary function.
fork spawns a parallel computation. The computation will not be spawned until forced
by 
.
run
run extracts a value from a 
 by actually performing the computation.
Par
At any point while sketching out an API, you can start thinking about possible 
 for the abstract types that appear.
representations
EXERCISE 2: Before continuing, try to come up with representations for Par
and 
 that make it possible to implement the functions of our API.
Strategy
Let's see if we can come up with a representation. We know 
 needs to
run
execute asynchronous tasks somehow. We could write our own API, but there's
already a class for this in 
, 
.
java.util.concurrent ExecutorService
Here it its API, excerpted and transcribed to Scala:
class ExecutorService {
  def submit[A](a: Callable[A]): Future[A]
}
trait Future[A] {
  def get: A
  def get(timeout: Long, unit: TimeUnit): A
  def cancel(evenIfRunning: Boolean): Boolean
  def isDone: Boolean
  def isCancelled: Boolean
}
7.2.1 Picking a representation
97
www.it-ebooks.info

So, 
 lets us submit a 
 value (in Scala we'd
ExecutorService
Callable
probably just use a lazy argument to 
), and get back a corresponding 
submit
. We can block to obtain a value from a 
 with its 
 method,
Future
Future
get
and it has some extra features for cancellation, only blocking for a certain amount
of time, and so on.
Let's try assuming that our 
 function has an 
 and see
run
ExecutorService
if that suggests anything about the representation for 
:
Par
def run[A](s: ExecutorService)(a: Par[A]): A
The simplest possible model for 
 might just be 
Par[A]
ExecutorService
. This would obviously make 
 trivial to implement. But it might be nice
=> A
run
to defer the decision of how long to wait for a computation or whether to cancel it
to the caller of 
:
run
type Par[A] = ExecutorService => Future[A]
def run[A](s: ExecutorService)(a: Par[A]): Future[A] = a(s)
Is it really that simple? Let's assume it is for now, and revise our model if we
decide it doesn't allow some functionality we'd like.
The way we've worked so far is actually a bit artificial. In practice, there aren't
such clear boundaries between designing your API and choosing a representation,
and one does not necessarily precede the other. Ideas for a representation can
inform the API you develop, the API you develop can inform the choice of
representation, and it's natural to shift fluidly between these two perspectives, run
experiments as questions arise, build prototypes, and so on.
We are going to devote the rest of this section to exploring our API. Though we
got a lot of mileage out of considering a simple example, before we add any new
primitive operations let's try to learn more about what is expressible using those we
already have. With our primitives and choices of meaning for them, we have
carved out a little universe for ourselves. We now get to discover what ideas are
expressible in this universe. This can and should be a fluid process—we can
7.2.2 Exploring and refining the API
98
www.it-ebooks.info

change the rules of our universe at any time, make a fundamental change to our
representation or introduce a new primitive, and explore how our creation then
behaves.
EXERCISE 3: Let's begin by implementing the functions of the API we've
developed so far. Now that we have a representation for 
, we should be able to
Par
fill these in. Optional (hard): try to ensure your implementations respect the
contract of the 
 method on 
 that accepts a timeout.
get
Future
def unit[A](a: A): Par[A]
def map2[A,B,C](a: Par[A], b: Par[B])(f: (A,B) => C): Par[C]
def fork[A](a: => Par[A]): Par[A]
You can place these functions and other functions we write inside an object
called 
, like so:
Par
object Par {
  /* Functions go here */
}
SIDEBAR
Adding infix syntax using implicit conversions
If 
 were an actual data type, functions like 
 could be placed in
Par
map2
the class body and then called with infix syntax like x.map2(y)(f)
(much like we did for 
 and 
). But since 
 is just a type
Stream
Option
Par
alias we can't do this directly. There is, however, a trick to add infix
syntax to 
 type using 
. We won't discuss that
any
implicit conversions
here since it isn't all that relevant to what we're trying to cover, but if
you're interested, check out the code associated with this chapter and
also the appendix.
EXERCISE 4: This API already enables a rich set of operations. Here's a
simple example: using 
, write a function to convert any function 
async
A => B
to one that evaluates its result asynchronously:
def asyncF[A,B](f: A => B): A => Par[B]
What else can we express with our existing combinators? Let's look at a more
concrete example.
Suppose we have a 
 representing a parallel computation
Par[List[Int]]
99
www.it-ebooks.info

producing a 
 and we would like to convert this to a 
List[Int]
 whose result is now sorted:
Par[List[Int]]
def sortPar(l: Par[List[Int]]): Par[List[Int]]
We could of course 
 the 
, sort the resulting list, and re-package it in a 
run
Par
 with 
. But we want to avoid calling 
. The only other combinator we
Par
unit
run
have that allows us to manipulate the value of a 
 in any way is 
. So if we
Par
map2
passed  to one side of 
, we would be able to gain access to the 
 inside
l
map2
List
and sort it. And we can pass whatever we want to the other side of 
, so let's
map2
just pass a no-op:
def sortPar(l: Par[List[Int]]): Par[List[Int]] =
  map2(l, unit(()))((a, _) => a.sorted)
Nice. We can now tell a 
 that we would like that list sorted.
Par[List[Int]]
But we can easily generalize this further. We can "lift" any function of type A =>
 to become a function that takes 
 and returns 
. That is, we can 
B
Par[A]
Par[B]
 any function over a 
:
map
Par
def map[A,B](fa: Par[A])(f: A => B): Par[B] =
  map2(fa, unit(()))((a,_) => f(a))
For instance, 
 is now simply this:
sortPar
def sortPar(l: Par[List[Int]]) = map(l)(_.sorted)
This was almost too easy. We just combined the operations to make the types
line up. And yet, if you look at the implementations of 
 and 
, it should
map2
unit
be clear this implementation of 
 
 something sensible.
map means
Was it cheating to pass a bogus value, 
 as an argument to 
,
unit(())
map2
only to ignore its value? Not at all! It shows that 
 is strictly more powerful
map2
than 
. This sort of thing can be a hint that 
 can be further decomposed
map
map2
into primitive operations. And when we consider it, 
 is actually doing two
map2
things—it is creating a parallel computation that waits for the result of two other
100
www.it-ebooks.info

computations 
 it is combining their results using some function. We could
and then
split this into two functions, 
 and 
:
product
map
def product[A,B](fa: Par[A], fb: Par[B]): Par[(A,B)]
def map[A,B](fa: Par[A])(f: A => B): Par[B]
EXERCISE 5 (optional): Implement 
 and 
 as primitives, then
product
map
define 
 in terms of them.
map2
This is sort of an interesting little discovery that we can factor things this way.
Is it an improvement? On the one hand, 
 is "doing one thing only", and 
product
 is now completely orthogonal. But if you look at your implementation of 
map
, you can almost see 
 hiding inside, except that you are always
product
map2
supplying a function of type 
 that sticks its arguments in a
(A,B) => (A,B)
pair. If we're going to have to write that function, we might as well expose the
most general version of it, 
. So let's keep 
 as primitive for now, and
map2
map2
define 
 in terms of it as above. This is one example where there is a choice of
map
which things we consider primitive and which things are derived.
What else can we implement using our API? Could we 
 over a list in
map
parallel? Unlike 
, which combines two parallel computations, 
 (let's
map2
parMap
call it) needs to combine  parallel computations. Still, it seems like this should
N
somehow be expressible.
EXERCISE 6: Note that we could always just write 
 as a new
parMap
primitive. See if you can implement it this way. Remember that 
 is simply
Par[A]
an alias for 
. Here is the signature for 
ExecutorService => Future[A]
:
parMap
def parMap[A,B](l: List[A])(f: A => B): Par[List[B]]
There's nothing wrong with implementing operations as new primitives. In
some cases we can even implement the operations more efficiently, by assuming
something about the underlying representation of the data types we are working
with. But we're interested in exploring what operations are expressible using our
existing API, and understanding the relationships between the various operations
101
www.it-ebooks.info

we've defined. Knowledge of what combinators are truly primitive will become
more important in Part 3, when we learn to abstract over common patterns across
libraries.5
Footnote 5mIn this case, there's another good reason not to implement 
 as a new primitive—it's
parMap
challenging to do correctly, particularly if we want to properly respect timeouts. It's frequently the case that
primitive combinators encapsulate some rather tricky logic, and reusing them means we don't have to duplicate this
logic.
Let's see how far we can get implementing 
 in terms of existing
parMap
combinators:
def parMap[A,B](l: List[A])(f: A => B): Par[List[B]] = {
  val fbs: List[Par[B]] = l.map(asyncF(f))
  ...
}
Remember, 
 converts an 
 to a 
, by forking a
asyncF
A => B
A => Par[B]
parallel computation to produce the result. So we can fork off our 
 parallel
N
computations pretty easily, but we need some way of collecting up their results.
Are we stuck? Well, just from inspecting the types, we can see that we need some
way of converting our 
 to the 
 required by the
List[Par[B]]
Par[List[B]]
return type of 
.
parMap
EXERCISE 7 (hard): Let's write this function, typically called 
. No
sequence
additional primitives are required.
def sequence[A](l: List[Par[A]]): Par[List[A]]
Once we have 
, we can complete our implementation of 
:
sequence
parMap
def parMap[A,B](l: List[A])(f: A => B): Par[List[B]] = fork {
  val fbs: List[Par[B]] = l.map(asyncF(f))
  sequence(fbs)
}
Notice that we've wrapped our implementation in a call to 
. With this
fork
implementation, 
 will return immediately, even for a huge input list. When
parMap
we later call 
, it will fork a single asynchronous computation which itself
run
spawns 
 parallel computations then waits for these computations to finish,
N
collecting their results up into a list. If you look back at your previous
102
www.it-ebooks.info

implementation of 
, the one that had knowledge of the internal
parMap
representation of 
, you'll see that it's doing the same thing.
Par
EXERCISE 8: Implement 
, which filters elements of a list in
parFilter
parallel.
def parFilter[A](l: List[A])(f: A => Boolean): Par[List[A]]
Can you think of any other useful functions to write? Experiment with writing a
few parallel computations of your own to see which ones can be expressed without
additional primitives. Here are some ideas to try:
Is there a more general version of the parallel summation function we wrote at the
beginning of this chapter? Try using it to find the maximum value of an 
 in
IndexedSeq
parallel.
Write a function that takes a list of paragraphs (a 
), and returns the total
List[String]
number of words across all paragraphs, in parallel. Generalize this function as much as
possible.
Implement 
, 
, and 
, in terms of 
.
map3 map4
map5
map2
As the previous section demonstrates, we often get quite far by treating this all as a
game of (what seems like) meaningless symbol manipulation! We write down the
type signature for an operation we want, then "follow the types" to an
implementation. Quite often when working this way we can almost forget the
concrete domain (for instance, when we implemented 
 in terms of 
 and 
map
map2
) and just focus on lining up types. This isn't cheating; it's a natural but
unit
different style of reasoning, analogous to the reasoning one does when simplifying
an algebraic equation like . We are treating the API as an 
,  or an abstract
x
algebra 6
set of operations along with a set of 
 or properties we assume true, and simply
laws
doing formal symbol manipulation following the "rules of the game" specified by
this algebra.
Footnote 6mWe do mean algebra in the mathematical sense of one or more sets, together with a collection of
functions operating on objects of these sets, and a set of 
. (Axioms are statements assumed true, from
axioms
which we can derive other 
 that must also be true.) In our case, the sets are values with particular
theorems
types, like 
, 
, the functions are operations like 
, 
, and 
.
Par[A] List[Par[A]]
map2 unit
sequence
Up until now, we have been reasoning somewhat informally about our API.
There's nothing wrong with this, but it can be helpful to take a step back and
formalize what laws you expect to hold (or would like to hold) for your API.7
7.2.3 The algebra of an API
103
www.it-ebooks.info

Without realizing it, you have probably mentally built up a model of what
properties or laws you expect. Actually writing these down and making them
precise can highlight design choices that wouldn't be otherwise apparent when
reasoning informally.
Footnote 7mWe'll have much more to say about this throughout the rest of this book. In the next chapter, we'll
be designing a declarative testing library that lets us define properties we expect functions to satisfy, and
automatically generates test cases to check these properties. And in Part 3 we'll introduce abstract interfaces
specified 
 by sets of laws.
only
Like any design choice, choosing laws has 
—it places constraints
consequences
on what the operations can mean, what implementation choices are possible,
affects what other properties can be true or false, and so on. Let's look at an
example. We are going to simply 
 a possible law that seems reasonable.
conjure up
This might be used as a test case if we were creating unit tests for our library:
map(unit(1))(_ + 1) == unit(2)
We are saying that mapping over 
 with the 
 function is in
unit(1)
_ + 1
some sense equivalent to 
. (Laws often start out this way, as concrete
unit(2)
examples of 
 we expect to hold.) In what sense are they equivalent? This
identities8
is somewhat of an interesting question. For now, let's say two 
 objects are
Par
equivalent if 
 
 argument, the 
 they
for any valid ExecutorService
Future
return results in the same value.
Footnote 8mHere we mean 'identity' in the mathematical sense of a statement that two expressions are
identical or equivalent.
We can check that this holds for a particular 
 with a
ExecutorService
function like:
def equal[A](e: ExecutorService)(p: Par[A], p2: Par[A]): Boolean =
  p(e).get == p2(e).get
Laws and functions share much in common. Just as we can generalize
functions, we can generalize laws. For instance, the above could be generalized:
map(unit(x))(f) == unit(f(x))
Here we are saying this should hold for 
 choice of  and . This places
any
x
f
104
www.it-ebooks.info

some constraints on our implementation. Our implementation of 
 cannot, say,
unit
inspect the value it receives and decide to return a parallel computation with a
result of 
 when the input is —it can only pass along whatever it receives.
42
1
Similarly for our 
—when we submit 
 objects to
ExecutorService
Callable
it for execution, it cannot make any assumptions or change behavior based on the
values it receives.  More concretely, this law disallows downcasting or 
9
 checks (often grouped under the term 
) in the
isInstanceOf
typecasing
implementations of 
 and 
.
map
unit
Footnote 9mHints and standalone answers
Much like we strive to define functions in terms of simpler functions, each of
which 
 just one thing, we can define laws in terms of simpler laws that each 
do
say
just one thing. Let's see if we can simplify this law further. We said we wanted this
law to hold for 
 choice of 
 and 
. Something interesting happens if we
any
x
f
substitute the identity function for 
. We can simplify both sides of the equation
f10
and get a new law which is considerably simpler:11
Footnote 10mThe identity function has the signature 
.
def id[A](a: A): A = a
Footnote 11mThis is the same sort of substitution and simplification one might do when solving an algebraic
equation.
map(unit(x))(f) == unit(f(x))
map(unit(x))(id) == unit(id(x))
map(unit(x))(id) == unit(x)
map(y)(id) == y
Substitute identity function for f
Simplify
Substitute y for unit(x) on both sides
Fascinating! Our new, simpler law talks only about 
—apparently the
map
mention of 
 was an extraneous detail. To get some insight into what this new
unit
law is saying, let's think about what 
 
 do. It cannot, say, throw an
map cannot
exception and crash the computation before applying the function to the result (can
you see why this violates the law?). All it can do is apply the function  to the
f
result of , which of course, leaves  unaffected in the case that function is 
.
y
y
id 12
Even more interesting, given 
, we can perform the
map(y)(id) == y
substitutions in the other direction to get back our original, more complex law.
105
www.it-ebooks.info

(Try it!) Logically, we have the freedom to do so because 
 cannot possibly
map
behave differently for different function types it receives. Thus, given 
, it must be true that 
map(y)(id) == y
map(unit(x))(f) ==
. Since we get this second law or theorem "for free", simply because
unit(f(x))
of the parametricity of 
, it is sometimes called a 
.
map
free theorem 13
Footnote 12mWe say that 
 is required to be 
, in that it does not alter the structure of
map
structure-preserving
the parallel computation, only the value "inside" the computation.
Footnote 13mThe idea of free theorems was introduced by Philip Wadler in a classic paper called Theorems
for free!
EXERCISE 9 (hard, optional): Given 
, it is a free
map(y)(id) == y
theorem that 
. (This is
map(map(y)(g))(f) == map(y)(f compose g)
sometimes called 
, and it can be used as an optimization—rather than
map fusion
spawning a separate parallel computation to compute the second mapping, we can
fold it into the first mapping.)
 Can you construct a proof? You may want to read
14
the paper 
 to better understand the "trick" of free theorems.
Theorems for Free!
Footnote 14mOur representation of 
 does not give us the ability to implement this optimization, since it is
Par
an opaque function. If it were reified as a data type, we could pattern match and discover opportunities to apply this
rule. You may want to try experimenting with this idea on your own.
As interesting as all this is, these laws don't do much to constrain our
implementation. You have probably been assuming these properties without even
realizing it (it would be rather strange to have any special cases in the
implementations of 
, 
 or 
, or have 
map unit
ExecutorService.submit
map
randomly throwing exceptions). Let's consider a stronger property, namely that 
 should not affect the result of a parallel computation:
fork
fork(x) == x
This seems like it should be obviously true of our implementation, and it is
clearly a desirable property, consistent with our expectation of how 
 should
fork
work. 
 should 'do the same thing' as , but asynchronously, in a logical
fork(x)
x
thread separate from the main thread. If this law didn't always hold, we'd have to
somehow know when it was safe to call without changing meaning, without any
help from the type system.
Surprisingly, this simple property places very strong constraints on our
implementation of 
. After you've written down a law like this, take off your
fork
106
www.it-ebooks.info

implementer hat, put on your debugging hat, and try to break your law. Think
through any possible corner cases, try to come up with counterexamples, and even
construct an informal proof that the law holds—at least enough to convince a
skeptical fellow programmer.
Let's try this mode of thinking. We are expecting that 
 for 
fork(x) == x
all
choices of , and any choice of 
. We have a pretty good
x
ExecutorService
sense of what  could be—it's some expression making use of 
, 
, and 
x
fork unit
 (and other combinators derived from these). What about 
map2
? What are some possible implementations of it? There's a
ExecutorService
good listing of different implementations in the class 
 (
).
Executors API link
EXERCISE 10 (hard, optional): Take a look through the various static methods
in 
 to get a feel for the different implementations of 
Executors
 that exist. Then, before continuing, go back and revisit your
ExecutorService
implementation of 
 and try to find a counterexample or convince yourself
fork
that the law holds for your implementation.
SIDEBAR
Why laws about code and proofs are important
It may seem unusual to state and prove properties about an API. This
certainly isn't something typically done in ordinary programming. Why is
it important in FP?
In functional programming, it is easy, and expected, that we will factor
out common functionality into generic, reusable, components that can
be 
. Side effects hurt compositionality, but more generally,
composed
any hidden or out-of-band assumptions or behavior that prevent us from
treating our components (be they functions or anything else) as black
 make composition difficult or impossible.
boxes
In our example of the law for 
, we can see that if the law we
fork
posited did not hold, many of our general purpose combinators, like 
, would no longer be sound (and their usage might be
parMap
dangerous, since they could, depending on the broader parallel
computation they were used in, result in deadlocks).
Giving our APIs an algebra, with laws that are meaningful and aid
reasoning, make the API more usable for clients, but also mean we can
treat the objects of our API as black boxes. As we'll see in Part 3, this is
crucial for our ability to factor out common patterns across the different
libraries we've written.
There's actually a problem with most implementations of 
. It's a rather
fork
107
www.it-ebooks.info

subtle deadlock that occurs when using an 
 backed by a
ExecutorService
t h r e a d  
p o o l  
o f  
b o u n d e d  
s i z e  
( s e e  
).
 Suppose we have an 
ExecutorService.newFixedThreadPool 15
 backed by a thread pool where the maximum number of
ExecutorService
threads is bounded to 1. Try running the following example using your current
implementation:
Footnote 15mIn the next chapter we'll be writing a combinator library for testing that can help discover
problems like these automatically.
val a = async(42 + 1)
val S = Executors.newFixedSizeThreadPool(1)
println(Par.equal(S)(a, fork(a)))
Most implementations of 
 will result in this code deadlocking. Can you
fork
see why? Most likely, your implementation of 
 looks something like this:
fork
16
Footnote 16mThere's actually another minor problem with this implementation—we are just calling 
 on
get
the inner 
 returned from 
. This means we are not properly respecting any timeouts that have been
Future
fa
placed on the outer 
.
Future
def fork_simple[A](a: => Par[A]): Par[A] =
  es => es.submit(new Callable[A] {
    def call = a(es).get
  })
The bug is somewhat subtle. Notice that we are submitting the Callable
first, and 
, we are submitting another 
 to the 
within that callable
Callable
 and blocking on its result (recall that 
 will submit a 
ExecutorService
a(es)
 to the 
 and get back a 
). This is a
Callable
ExecutorService
Future
problem if our thread pool has size 1. The outer 
 gets submitted and
Callable
picked up by the sole thread. Within that thread, before it will complete, we submit
and block waiting for the result of another 
. But there are no threads
Callable
available to run this 
. Our code therefore deadlocks.
Callable
EXERCISE 11 (hard, optional): Can you show that any fixed size thread pool
can be made to deadlock given this implementation of 
?
fork
When you find counterexamples like this, you have two choices—you can try
to fix your implementation such that the law holds, or you can refine your law a
bit, to state more explicitly the conditions under which it holds (we could simply
108
www.it-ebooks.info

stipulate that we require thread pools that can grow unbounded). Even this is a
good exercise—it forces you to document invariants or assumptions that were
previously implicit.
We are going to try to fix our implementation, since being able to run our
parallel computations on fixed size thread pools seems like a useful capability. The
problem with the above implementation of 
 is that we are invoking 
 
fork
submit
 a callable, and we are 
 on the result of what we've submitted. This
inside
blocking
leads to deadlock when there aren't any remaining threads to run the task we are
submitting. So it seems we have a simple rule we can follow to avoid deadlock:
A 
 should never submit and then block on the result of a 
Callable
.
Callable
You may want to take a minute to prove to yourself that our parallel tasks
cannot deadlock, even with a fixed-size thread pool, so long as this rule is
followed.
Let's look at a different implementation of 
:
fork
def fork[A](fa: => Par[A]): Par[A] =
  es => fa(es)
This certainly avoids deadlock. The only problem is that we aren't actually
forking 
a 
separate 
logical 
thread 
to 
evaluate 
. 
So, 
fa
 for some 
, 
, runs 
fork(hugeComputation)(es)
ExecutorStrategy es
 in the main thread, which is exactly what we wanted to
hugeComputation
avoid by calling 
. This is still a useful combinator, though, since it lets us
fork
delay instantiation of a parallel computation until it is actually needed. Let's give it
a name, 
:
delay
def delay[A](fa: => Par[A]): Par[A] =
  es => fa(es)
EXERCISE 12 (hard, optional): Can you figure out a way to still evaluate 
 in
fa
a separate logical thread, but avoid deadlock? You may have to resort to some
mutation or imperative tricks behind the scenes. There's absolutely nothing wrong
with doing this, so long as these local violations of referential transparency aren't
109
www.it-ebooks.info

observable to users of the API. The details of this are quite finicky to get right. The
nice thing is that we are confining this detail to one small block of code, rather than
forcing users to have to think about these issues throughout their use of the API.
Taking a step back from these details, the purpose here is not necessarily to
figure out the best, nonblocking implementation of 
, but more to show that
fork
laws are important. They give us another angle to consider when thinking about the
design of a library. If we hadn't tried writing some of these laws out, we may not
have discovered this behavior of 
 until much later.
fork
In general, there are multiple approaches you can consider when choosing laws
for your API. You can think about your conceptual model, and reason from there to
postulate laws that should hold. You can also 
 laws you think are 
conjure up
useful
 (like we did with our 
 law), and see if it is
for reasoning or compositionality
fork
possible and sensible to ensure they hold for your model and implementation. And
lastly, you can look at your 
 and come up with laws you expect to
implementation
hold based on your implementation.17
Footnote 17mThis last way of generating laws is probably the weakest, since it can be a little too easy to just
have the laws reflect the implementation, even if the implementation is buggy or requires all sorts of unusual side
conditions that make composition difficult.
EXERCISE 13: Can you think of other laws that should hold for your
implementation of 
, 
, and 
? Do any of them have interesting
unit fork
map2
consequences?
Functional design is an iterative process. After you've written down your API and
have at least a prototype implementation, try using it for progressively more
complex or realistic scenarios. Often you'll find that these scenarios require only
some combination of existing primitive or derived combinators, and this is a
chance to factor out common usage patterns into other combinators; occasionally
you'll find situations where your existing primitives are insufficient. We say in this
case that the API is not expressive enough.
Let's look at an example of this.
EXERCISE 14: Try writing a function to choose between two forking
computations based on the result of an initial computation. Can this be
implemented in terms of existing combinators or is a new primitive required?
def choice[A](a: Par[Boolean])(ifTrue: Par[A], ifFalse: Par[A]): Par[A]
7.2.4 Expressiveness and the limitations of an algebra
110
www.it-ebooks.info

Notice what happens when we try to define this using only 
. If we try 
map
, we obtain the type 
map(a)(a => if (a) ifTrue else ifFalse)
. If we had an 
 we could force the outer 
Par[Par[A]]
ExecutorStrategy
 to obtain a 
, but we'd like to keep our 
 values agnostic to the 
Par
Par[A]
Par
. That is, we don't want to actually execute our parallel
ExecutorStrategy
computation, we simply want to 
 a parallel computation that 
 runs one
describe
first
parallel computation and 
 uses the result of that computation to choose what
then
computation to run next.
This is a case where our existing primitives are insufficient. When you
encounter these situations, you can respond by simply introducing the exact
combinator needed (for instance, we could simply write 
 as a new
choice
primitive, using the fact that 
 is merely an alias for 
Par[A]
ExecutorService
 rather than relying only on 
, 
, and 
). But quite
=> Future[A]
unit fork
map2
often, the example that motivates the need for a new primitive will not be minimal
—it will have some incidental features that aren't really relevant to the essence of
the API's limitation. It's a good idea to try to explore some related examples around
the particular one that cannot be expressed, to see if a common pattern emerges.
Let's do that here.
If it's useful to be able to choose between 
 parallel computations based on
two
the results of a first, it should be useful to choose between  computations:
N
def choiceN[A](a: Par[Int])(choices: List[Par[A]]): Par[A]
Let's say that 
 runs , then uses that to select a parallel computation
choiceN
a
from 
. This is a bit more general than 
.
choices
choice
EXERCISE 15: Implement 
 and then 
 in terms of 
choiceN
choice
choiceN
.
EXERCISE 16: Still, let's keep looking at some variations. Try implementing
the following combinator. Here, instead of a list of computations, we have a Map
of them:18
Footnote 18m
 (
) is a purely functional data structure.
Map API link
def choiceMap[A,B](a: Par[A])(choices: Map[A,Par[B]]): Par[B]
If you want, stop reading here and see if you can come up with a combinator
111
www.it-ebooks.info

that generalizes these examples.
Something about these combinators seems a bit arbitrary. The 
 encoding of
Map
the set of possible choices feels too specific. If you look at your implementation of 
, you can see you aren't really using much of the API of 
. Really,
choiceMap
Map
we are just using the 
 to provide a function, 
.
Map[A,Par[B]]
A => Par[B]
And looking back at 
 and 
, we can see that for 
, the
choice
choiceN
choice
pair of arguments was just being used as a 
 (where the
Boolean => Par[A]
boolean selects either the first or second element of the pair), and for 
,
choiceN
the list was just being used as an 
.
Int => Par[A]
Let's take a look at the signature.
def chooser[A,B](a: Par[A])(choices: A => Par[B]): Par[B]
EXERCISE 17: Implement this new primitive 
, then use it to
chooser
implement 
 and 
.
choice
choiceN
Whenever you generalize functions like this, take a look at the result when
you're finished. Although the function you've written may have been motivated by
some very specific use case, the signature and implementation may have a more
general meaning. In this case, 
 is perhaps no longer the most appropriate
chooser
name for this operation, which is actually quite general—it is a parallel
computation that, when run, will run an initial computation whose result is used to
determine a second computation. Nothing says that this second computation needs
to even exist before the first computation's result is available. Perhaps it is being 
 from whole cloth using the result of the first computation. This function,
generated
which comes up quite often in combinator libraries, is usually called 
 or 
bind
:
flatMap
def flatMap[A,B](a: Par[A])(f: A => Par[B]): Par[B]
Is 
 really the most primitive possible function? Let's play around with
flatMap
it a bit more. Recall when we first tried to implement 
, we ended up with a
choice
. From there we took a step back, tried some related examples, and
Par[Par[A]]
eventually discovered 
. But suppose instead we simply 
 another
flatMap
conjured
combinator, let's call it 
, for converting 
 to 
:
join
Par[Par[A]]
Par[A]
112
www.it-ebooks.info

def join[A](a: Par[Par[A]]): Par[A]
We'll call it 
 since conceptually, it is a parallel computation that when
join
run, will run the inner computation, wait for it to finish (much like Thread.join
), then return its result. Again, we are just following the types here. We have an
example that demands a function with the given signature, and so we just bring it
into existence.
EXERCISE 18: Implement 
. Optional: can it be implemented in a way
join
that avoids deadlock, even when run on bounded thread pools as in 
? Can
fork
you see how to implement 
 using 
? And can you implement 
flatMap
join
join
using 
?
flatMap
We are going to stop here, but you are encouraged to try exploring this algebra
further. Try more complicated examples, discover new combinators, and see what
you find! Here are some questions to consider:
Can you implement a function with the same signature as 
, but using 
 and 
?
map2
bind
unit
How is its meaning different than that of 
?
map2
Can you think of laws relating 
 to the other primitives of the algebra?
join
Are there parallel computations that cannot be expressed using this algebra? Can you
think of any computations that cannot even be expressed by adding new primitives to the
algebra?
In this chapter, we've chosen a "pull" model for our parallel computations. That is, when
we run a computation, we get back a 
, and we can block to obtain the result of this
Future
. Are there alternative models for 
 that don't require us to ever block on a 
Future
Par
?
Future
In this chapter, we worked through the design of a library for defining parallel and
asynchronous computations. Although this domain is interesting, the goal of this
chapter was to give you a window into the process of functional design, to give you
a sense of the sorts of issues you're likely to encounter and to give you ideas for
how you can handle them. If you didn't follow absolutely every part of this, or if
certain conclusions felt like logical leaps, don't worry. No two people take the
same path when designing a library, and as you get more practice with functional
design, you'll start to develop your own tricks and techniques for exploring a
problem and possible designs.
In the next chapter, we are going to look at a completely different domain, and
take yet another meandering journey toward discovering an API for that domain.
7.3 Conclusion
113
www.it-ebooks.info

free theorem
map fusion
parametricity
shape
structure-preserving
typecasing
Index Terms
114
www.it-ebooks.info

8
In the last chapter, we worked through the design of a functional library for
expressing parallel computations. There, we introduced the idea of an API forming
an 
—that is, a collection of data types, functions over these data types, and
algebra
importantly, 
 or 
 that express relationships between these functions.
laws
properties
We also hinted at the idea that it might be possible to somehow check these laws
automatically.
This chapter will work up to the design and implementation of a simple but
powerful 
 library. What does this mean? The general idea of
property-based testing
such a library is to decouple the specification of program behavior from the
creation of test cases. The programmer focuses on specifying the behavior and
giving high-level constraints on the test cases; the framework then handles
generating (often 
) test cases satisfying the constraints and checking that
random
programs behave as specified for each case.
As an example, in 
, a property-based testing library for Scala, a
ScalaCheck
property looks something like:
val intList = Gen.listOf(Gen.choose(0,100))
val prop =
  forAll(intList)(l => l.reverse.reverse == l) &&
  forAll(intList)(l => l.headOption == l.reverse.lastOption)
val failingProp = forAll(intList)(l => l.reverse == l)
A generator of lists of integers between 0 and 100
A property which specifies the behavior of the List.reverse method
Property-based testing
8.1 Introduction
8.2 A brief tour of property-based testing
115
www.it-ebooks.info

Check that reversing a list twice gives back the original list
Check that the first element becomes the last element after reversal
A property which is obviously false
And we can check properties like so:
scala> prop.check
+ OK, passed 100 tests.
scala> failingProp.check
! Falsified after 6 passed tests.
> ARG_0: List("0", "1")
Here, 
 is not a 
, but a 
, which is
intList
List[Int]
Gen[List[Int]]
something that knows how to generate test data of type 
. We can 
List[Int]
 from this generator, and it will produce lists of different lengths, filled with
sample
random numbers between 
 and 
. Generators in a property-based testing
0
100
library have a rich API. We can combine and compose generators in different
ways, reuse them, and so on.
The function 
 creates a 
 by combining a 
 with some
forAll
property
Gen[A]
predicate 
 to check for each value generated. Like 
,
A => Boolean
Gen
properties can also have a rich API. Here in this simple example we have used &&
to combine two properties. The resulting property will hold only if neither property
can be 
 by any of the generated test cases. Together, the two properties
falsified
form a 
 specification of the correct behavior of the 
 method.
partial
reverse
1
Footnote 1mHints and standalone answers
When we invoke 
, ScalaCheck will randomly generate 
prop.check
 values and ensure that each passes the predicates we have supplied.
List[Int]
The output indicates that ScalaCheck has generated 100 test cases (of type 
) and that the predicates were satisfied for each. Properties can of
List[Int]
course fail—the output of 
 indicates that the predicate
failingProp.check
tested false for some input, which is helpfully printed out to facilitate further
testing or debugging.
EXERCISE 1: To get more of a feel for how to approach testing in this way, try
thinking of properties to specify the implementation of a sum: List[Int] =>
 function. Don't try writing your properties down as executable ScalaCheck
Int
code, an informal description is fine. Here are some ideas to get you started:
116
www.it-ebooks.info

Reversing a list and summing it should give the same result as summing
the original, non-reversed list.
What should the sum be if all elements of the list are the same value?
Can you think of other properties?
EXERCISE 2: What are properties that specify the function that finds the
maximum of a 
?
List[Int]
Property-based testing libraries often come equipped with other useful features.
We'll talk more about these features later, but just to give an idea of what is
possible:
: In the event of discovering a failing test, the test runner tries
Test case minimization
smaller sizes until finding the 
 size test case that also fails, which is more
minimal
illuminating for debugging purposes. For instance, if a property fails for a list of size 10,
the test runner checks smaller lists and reports the smallest failure.
Exhaustive test case generation: We call the set of possible values that could be produced
by some 
 the 
.  When the domain is small enough (for instance, suppose
Gen[A]
domain 2
the domain were all even integers less than 100), we may exhaustively generate all its
values, rather than just randomly from it. If the property holds for all values in a domain,
we have an actual proof, rather than just the absence of evidence to the contrary.
Footnote 2mThis is the same usage of 'domain' as 
—generators describe
the domain of a function
possible inputs to functions we would like to test. Note that we will also still sometimes use 'domain' in the
more colloquial sense, to refer to a subject or area of interest, e.g. "the domain of functional parallelism" or
"the error-handling domain".
ScalaCheck is just one property-based testing library. And while there's nothing
wrong with it, we are going to be deriving our own library in this chapter, starting
from scratch. This is partially for pedagogical purposes, but there's another reason:
we want to encourage the view that no existing library (even one designed by
supposed experts) is authoritative. Don't treat existing libraries as a cookbook to be
followed. Most libraries contain a whole lot of 
 design choices, many
arbitrary
made unintentionally. Look back to the previous chapter—notice how on several
occasions, we did some informal reasoning to rule out entire classes of possible
designs. This sort of thing is an inevitable part of the design process (it is
impossible to fully explore every conceivable path), but it means it's easy to miss
out on workable designs.
When you start from scratch, you get to revisit all the fundamental assumptions
that went into designing the library, take a different path, and discover things about
117
www.it-ebooks.info

the domain and the problem space that others may not have even considered. As a
result, you might arrive at a design that's much better for your purposes. But even
if you decide you like the existing library's solution, spending an hour or two of
playing with designs and writing down some type signatures is a great way to learn
more about a domain, understand the design tradeoffs, and improve your ability to
think through design problems.
In this section, we will embark on another messy, iterative process of discovering
data types and a set of primitive functions and combinators for doing
property-based testing. As before, this is a chance to peer over the shoulder of
someone working through possible designs. The particular path we take and the
library we arrive at isn't necessarily the same as what you would discover. If
property-based testing is unfamiliar to you, even better; this is a chance to explore
a new domain and its design space, and make your own discoveries about it. If at
any point, you're feeling inspired or have ideas of your own about how to design a
library like this, don't wait for an exercise to prompt you—
 and
put the book down
go off to implement and play with your ideas. You can always come back to this
chapter if you want ideas or get stuck on how to proceed.
With that said, let's get started. What data types should we use for our testing
library? What primitives should we define, and what should their meanings be?
What laws should our functions satisfy? As before, we can look at a simple
example and "read off" the needed data types and functions, and see what we find.
For inspiration, let's look at the ScalaCheck example we showed earlier:
val intList = Gen.listOf(Gen.choose(0,100))
val prop =
  forAll(intList)(l => l.reverse.reverse == l) &&
  forAll(intList)(l => l.headOption == l.reverse.lastOption)
Without knowing anything about the implementation of 
 or 
Gen.choose
, we can guess that whatever data type they return (let's call it 
,
Gen.listOf
Gen
short for "generator") must be parametric in some type. That is, 
 probably returns a 
, and 
 is
Gen.choose(0,100)
Gen[Int]
Gen.listOf
then a function with the signature 
. But
Gen[Int] => Gen[List[Int]]
8.3 Choosing data types and functions
118
www.it-ebooks.info

since it doesn't seem like 
 should care about the type of the 
 it
Gen.listOf
Gen
receives as input (it would be odd to require separate combinators for creating lists
of 
, 
, 
, and so on), let's go ahead and make it polymorphic:
Int Double String
def listOf[A](a: Gen[A]): Gen[List[A]]
We can learn many things by looking at this signature. Notice what we are not
specifying—the size of the list to generate. For this to be implementable, this
implies our generator must either assume or be told this size. Assuming a size
seems a bit inflexible—whatever we assume is unlikely to be appropriate in all
contexts. So it seems that generators must be told the size of test cases to generate.
We could certainly imagine an API where this were explicit:
def listOfN[A](n: Int, a: Gen[A]): Gen[List[A]]
This would certainly be a useful combinator, but 
 having to explicitly
not
specify sizes is powerful as well—it means the test runner has the freedom to
choose test case sizes, which opens up the possibility of doing the test case
minimization we mentioned earlier. If the sizes are always fixed and specified by
the programmer, the test runner won't have this flexibility. Keep this concern in
mind as we get further along in our design.
What about the rest of this example? The 
 function looks interesting.
forAll
We can see that it accepts a 
 and what looks to be a
Gen[List[Int]]
corresponding predicate, 
. But again, it doesn't seem
List[Int] => Boolean
like 
 should care about the types of the generator and the predicate, as
forAll
long as they match up. We can express this with the type:
def forAll[A](a: Gen[A])(f: A => Boolean): Prop
Here, we've simply invented a new type, 
 (short for "property", following
Prop
the ScalaCheck naming), for the result of binding a 
 to a predicate. We might
Gen
not know the internal representation of 
 or what other functions it supports
Prop
but based on this example, we can see that it has an 
 operator, so let's introduce
&&
that:
119
www.it-ebooks.info

trait Prop { def &&(p: Prop): Prop }
Now that we have a few fragments of an API, let's discuss what we want our types
and functions to 
. First, consider 
. We know there exist functions 
mean
Prop
 (for creating a property) and 
 (for running a property). In
forAll
check
ScalaCheck, this 
 method has a side effect of printing to console. This is
check
probably fine to expose as a convenience function, but it's not a basis for
composition. For instance, we couldn't implement 
 for 
 if its only API
&&
Prop
were the 
 method:
check
3
Footnote 3mThis might remind you of similar problems that we discussed last chapter, when we looked at
directly using 
 and 
 for parallelism.
Thread
Runnable
trait Prop {
  def check: Unit
  def &&(p: Prop): Prop = ???
}
In order to combine 
 values using combinators like 
, we need 
Prop
&&
check
(or whatever function "runs" properties) to return some meaningful value. What
type should that value be? Well, let's consider what sort of information we'd like to
get out of checking our properties. At a minimum, we need to know whether the
property succeeded or failed. This lets us implement 
.
&&
EXERCISE 3: Assuming the following definition of 
, implement 
 as a
Prop
&&
method of 
:
Prop
trait Prop { def check: Boolean }
In this representation, 
 is nothing more than a non-strict 
, and
Prop
Boolean
any of the usual 
 functions ('and', 'or', 'not', 'xor', etc) can be defined for 
Boolean
. But a 
 alone is probably insufficient. If a property fails, we'd like
Prop
Boolean
to perhaps know how many tests succeeded first, and what the arguments were that
resulted in the failure. And if it succeeded, we might like to know how many tests
it ran. Let's try to return an 
 to indicate this success or failure:
Either
object Prop {
8.3.1 The meaning and API of properties
120
www.it-ebooks.info

  type SuccessCount = Int
  ...
}
trait Prop { def check: Either[???,SuccessCount] }
Type aliases like this can help readability of an API
There's a problem, what type do we return in the failure case? We don't know
anything about the type of the test cases being generated internal to the 
.
Prop
Should we add a type parameter to 
, make it a 
? Then 
Prop
Prop[A]
check
could return 
. Before going too far down this path, let's ask
Either[A,Int]
ourselves, do we really care though about the 
 of the value that caused the
type
property to fail? Not exactly. We would only care about the type if we were going
to do further computation with this value. Most likely we are just going to end up
printing this value to the screen, for inspection by the person running these tests.
After all, the goal here is a) to find bugs, and b) to indicate to a person what test
cases trigger those bugs, so they can go fix them. This suggests we can get away
with the following type:
object Prop {
  type FailedCase = String
  type SuccessCount = Int
}
trait Prop { def check: Either[FailedCase,SuccessCount] }
In the case of failure, 
 returns a 
, where  is some 
check
Left(s)
s
String
representation of the value that caused the property to fail. As a general rule,
whenever you return a value of some type  from a function, think about what
A
callers of your function are likely to do with that value. Will any of them care that
the value is of type , or will they always convert your  to some other uniform
A
A
representation (like 
 in this case)? If you have a good understanding of all
String
the ways callers will use your function, and they all involve converting your value
to some other type like 
, there's often no loss in expressiveness to simply
String
return that 
 directly. The uniformity of representation makes composition
String
easier.
That takes care of the return value of 
, at least for now, but what about
check
the arguments to 
? Right now, the 
 method takes no arguments. Is
check
check
this sufficient? We can think about what information 
 will have access to just
Prop
121
www.it-ebooks.info

by inspecting the way 
 values are created. In particular, let's look at 
:
Prop
forAll
def forAll[A](a: Gen[A])(f: A => Boolean): Prop
Without knowing more about the representation of 
, it's hard to say whether
Gen
there's enough information here to be able to generate values of type  (which is
A
what we need to implement 
). So for now let's turn our attention to 
, to
check
Gen
get a better idea of what it means and what its dependencies might be.
Let's take a step back to reflect on what we've learned so far. We've certainly made
some progress. By inspecting a simple example, we learned that our library deals
with at least two fundamental types, 
, and 
, and we've loosely assigned
Gen
Prop
meanings to these types. In looking at 
, we made what seems like an important
Gen
distinction between generators whose "size" is chosen explicitly by the
programmer (as in 
), and generators where the testing framework is
listOfN
allowed to pick sizes (as in 
). We noted this as something to keep in mind
listOf
for later.
Somewhat arbitrarily, we then chose to look at 
 first, and determined we
Prop
couldn't commit to a concrete representation for 
 without first knowing the
Prop
representation of 
. We've made a note of this, and plan on returning to 
Gen
Prop
shortly. Have we made a mistake by starting with 
? Not at all. This sort of
Prop
thing happens all the time. It doesn't matter much where we begin our inquiry—the
domain will inexorably guide us to make all the design choices that are required.
As you do more functional design, you'll develop a better intuition for where a
good place is to start.
Let's press on. We determined earlier that a 
 was something that knows
Gen[A]
how to generate values of type . What are some ways it could do that? Well, it
A
could 
 generate these values. Look back at the example from chapter
randomly
six—there, we gave an interface for a purely functional random number generator
and showed how to make it convenient to combine computations that made use of
it. We could have 
 build on this:
Gen
4
Footnote 4mRecall 
.
case class State[S,A](run: S => (A,S))
8.3.2 The meaning and API of generators
TWO WAYS OF GENERATING VALUES
122
www.it-ebooks.info

type Gen[A] = State[RNG,A]
EXERCISE 4: Implement 
 using this representation of 
. Feel
Gen.choose
Gen
free to use functions you've already written.
def choose(start: Int, stopExclusive: Int): Gen[Int]
But in addition to randomly generating values, it might be nice when possible
to exhaustively enumerate a sequence of values, and be notified somehow that all
possible values have been generated. If we can get through all possible values
without finding a failing case, this is an actual 
 that our property holds over
proof
its domain. Clearly exhaustive generation won't always be feasible, but in some
cases it might be. As a simple example, a 
 could first generate 
Gen[Boolean]
, then 
, then report it was done generating values. Likewise for the
true
false
expression 
. Note that if we can't exhaustively enumerate
Gen.choose(1,10)
all possible values, we'll likely want to sample our values differently—for instance,
if we have 
, we could generate 
 in
Gen.choose(1,10)
1, 2, ... 10
sequence, but if we have 
, we won't be able to
Gen.choose(0,1000000000)
enumerate all possible values and should probably sample our random values more
uniformly from the range.
If we want to support both modes of test case generation (random and
exhaustive), we need to extend 
. 
 gives us all we need to
Gen State[RNG,A]
support random test case generation, but what about exhaustive generation? Well,
the simplest type we could use to encode a generator for an exhaustive list of
values is... a 
 of these values:
List
type Gen[+A] = (State[RNG,A], List[A])
The first element of the pair is the generator of random values, the second is an
exhaustive list of values. Note that with this representation, the test runner will
likely have to choose between the two modes based on the number of test cases it
is running. For instance, if the test runner is running 1000 tests, it could 'spend' up
to the first 300 of these tests working through the domain exhaustively, then switch
to randomly sampling the domain if the domain has not been fully enumerated.
We'll get to writing this logic a bit later, after we nail down exactly how to
represent our "dual-mode" generators.
123
www.it-ebooks.info

There's a few problems with the current encoding of these dual-mode
generators. For one, it would be a shame to have to exhaustively generate all these
values if we end up having to resort to random test case generation (and the full set
of values may be huge or even infinite!). So let's use a 
 in place of 
.
Stream
List
We are going to use the 
 type we developed last chapter and also promote
Stream
our type alias to a data type.5
Footnote 5mWe aren't using Scala's standard library streams here. They have an unfortunate "off-by-one"
error—they strictly evaluate their first element. Generic code like what we are writing in this chapter can't assume it
is desireable to evaluate any part of the stream until it is explicitly requested.
case class Gen[+A](sample: State[RNG,A], exhaustive: Stream[A])
EXERCISE 5: Let's see what we can implement using this representation of 
. Try implementing 
, 
, 
, and 
.
Gen
unit boolean choose
listOfN
def unit[A](a: => A): Gen[A]
def boolean: Gen[Boolean]
def choose(start: Int, stopExclusive: Int): Gen[Int]
/** Generate lists of length n, using the given generator. */
def listOfN[A](n: Int, g: Gen[A]): Gen[List[A]]
So far so good. But these domains are all finite. What should we do about
infinite domains, like a 
 generator in some range:
Double
/** Between 0 and 1, not including 1. */
def uniform: Gen[Double]
/** Between `i` and `j`, not including `j`. */
def choose(i: Double, j: Double): Gen[Double]
To randomly sample from these domains is straightforward, but what should we
124
www.it-ebooks.info

do for 
? Return the empty 
? No, probably not. Previously,
exhaustive
Stream
we made a choice about the meaning of an empty stream—we interpreted it to
mean that we have finished exhaustively generating values in our domain and there
are no more values to generate. We could change its meaning to "the domain is
infinite, use random sampling to generate test cases", but then we lose the ability to
determine that we have exhaustively enumerated our domain, or that the domain is
simply empty. How can we distinguish these cases? One simple way to do this is
with 
:
Option
case class Gen[+A](sample: State[RNG,A], exhaustive: Stream[Option[A]])
We'll adopt the convention that a 
 in 
 signals to the test
None
exhaustive
runner should switch to random sampling, because the domain is infinite or
otherwise not worth fully enumerating.  If the domain can be fully enumerated, 
6
 will be a finite stream of 
 values. Note that this is a pretty
exhaustive
Some
typical usage of 
. Although we introduced 
 as a way of doing
Option
Option
error handling, 
 gets used a lot whenever we need a simple way of
Option
encoding one of two possible cases.
Footnote 6mWe could also choose to put the 
 on the outside: 
. You may
Option
Option[Stream[A]]
want to explore this representation on your own. You will find that it doesn't work out so well as it requires that we
be able to decide 
 that the domain is not worth enumerating. We will see examples later of generators
up front
where it isn't possible to make this determination.
125
www.it-ebooks.info

SIDEBAR
Using the simplest possible types
We could create a new data type to wrap our Stream[Option[A]]
representing our domain for 
, but we will sometimes hold
exhaustive
off on doing this until it feels justified (say, when we accumulate some
functions over this representation that aren't trivially defined in terms of 
 and 
, or we really want to hide this representation in
Option
Stream
the interest of modularity). When using a type like this with no particular
meaning attached to it, it can be a good practice to define type aliases
and functions which help codify and document the meaning you have
assigned to the type:
type Domain[+A] = Stream[Option[A]]
def bounded[A](a: Stream[A]): Domain[A] = a map (Some(_))
def unbounded: Domain[Nothing] = Stream(None)
This is pretty low cost—it doesn't require us to reimplement or replicate
the API of 
 or 
. But it helps with documentation of the
Option
Stream
API and makes it easier to implement a refactoring later which
promotes 
 to its own data type.
Domain
EXERCISE 6: With this representation, reimplement the operations 
, 
boolean
 (the 
 version), and 
 (hard) from before, then implement 
choose
Int
listOfN
 and 
 (the 
 version).
uniform
choose
Double
As we discussed last chapter, we are interested in understanding what operations
are 
, and what operations are 
, and in finding a small yet
primitive
derived
expressive set of primitives. A good way to explore what is expressible with a
given set of primitives is to pick some concrete examples you'd like to express, and
see if you can assemble the functionality you want. As you do so, look for patterns,
try factoring out these patterns into combinators, and refine your set of primitives.
We encourage you to stop reading here and simply 
 with the primitives and
play
combinators we've written so far. If you want some concrete examples to inspire
you, here are some ideas:
If we can generate a single 
 in some range, do we need a new primitive to generate an 
Int
 pair in some range?
(Int,Int)
Can we produce a 
 from a 
? What about a 
 from a 
Gen[Option[A]]
Gen[A]
Gen[A]
?
Gen[Option[A]]
REFININING THE PRIMITIVES
126
www.it-ebooks.info

Can we generate strings somehow using our existing primitives?
SIDEBAR
The importance of play
You don't have to wait around for a concrete example to force
exploration of the design space. In fact, if you rely exclusively on
concrete, obviously "useful" or "important" examples to design your API,
you'll often miss out on aspects of the design space and generate
designs with ad hoc, overly specific features. We don't want to overfit
our API to the particular examples we happen to think of 
. We
right now
want to reduce the problem to its 
, and sometimes the best way
essence
to do this is 
. Don't try to solve important problems or produce
play
useful functionality. Not right away. Just experiment with different
representations, primitives, and operations, let questions naturally arise,
and explore whatever piques your curiosity. ("These two functions seem
similar. I wonder if there's some more general operation hiding inside."
or "Would it make sense to make this data type polymorphic?" or "What
would it mean to change this aspect of the representation from a single
value to a 
 of values?") There's no right or wrong way to do this,
List
but there are so many different design choices that it's impossible 
 to
not
run headlong into fascinating little questions to play with.
Here, we are going to take a bit of a shortcut. Notice that 
 is composed of a
Gen
few other types, 
, 
, and 
. This can often be a hint that the
Stream State
Option
API of 
 is going to have many of the same operations as these types. Let's see
Gen
if there's some familiar operations from 
, 
, and 
 that we
Stream State
Option
can also define for 
.
Gen
EXERCISE 7: Aha! Our 
 data type supports both 
 and 
.  See if
Gen
map
map2 7
you can implement these. Your implementation should be almost trivially defined
in terms of the 
 and 
 functions on 
, 
, and 
.  You
map
map2
Stream State
Option 8
can add them as methods of the 
 type, as we've done before, or write them as
Gen
standalone functions in the 
 companion object. After you've implemented 
Gen
map
, you may want to revisit your implementation of 
 for 
 and define
choose
Double
it in terms of 
 and 
.
uniform
map
Footnote 7mYou've probably noticed by now that many data types support 
, 
, and 
. We'll
map map2
flatMap
be discussing how to abstract over these similarities in part 3 of the book.
Footnote 8mIn part 3 we will also learn how to derive these implementations automatically. That is, by
composing our data types in certain well-defined ways, we can obtain the implementations of 
, 
, and
map map2
so on for free, without having to write any additional code!
127
www.it-ebooks.info

def map[B](f: A => B): Gen[B]
def map2[B,C](g: Gen[B])(f: (A,B) => C): Gen[C]
So far so good. But 
 and 
 are not expressive enough to encode some
map
map2
generators. Suppose we'd like a 
 where both integers are odd,
Gen[(Int,Int)]
or both are even. Or a 
 where we first generate a length
Gen[List[Int]]
between 0 and 11, then generate a 
 of the chosen length. In both
List[Double]
these cases there is a dependency—we generate a value, then use that value to
determine what generator to use next.  For this we need 
, another
9
flatMap
function we've seen before.
Footnote 9mTechnically, this first case can be implemented by generating the two integers separately, and
using 
 to make them both odd or both even. But a more natural way is to choose an even or odd
map2
generator based on the first value generated.
EXERCISE 8: Implement 
, then use it to implement the generators
flatMap
mentioned above. You can make 
 and this version of 
 in the 
flatMap
listOfN
 class, and 
 in the 
 companion object.
Gen
sameParity
Gen
def flatMap[B](f: A => Gen[B]): Gen[B]
def sameParity(from: Int, to: Int): Gen[(Int,Int)]
def listOfN(size: Gen[Int]): Gen[List[A]]
EXERCISE 9 (hard, optional): Try implementing the version of listOfN
which picks its size up front purely in terms of other primitives. Is it possible? If
yes, give an implementation. Otherwise, explain why not:
def listOfN[A](n: Int, g: Gen[A]): Gen[List[A]]
EXERCISE 10 (hard): Implement 
, for combining two generators of the
union
same type into one, by pulling values from each generator with equal likelihood.
What are some possible ways you could combine the two exhaustive streams? Can
128
www.it-ebooks.info

you think of any reason to prefer one implementation over another?
def union(g1: Gen[A], g2: Gen[A]): Gen[A]
EXERCISE 11 (hard, optional): Implement 
, a version of 
weighted
union
which accepts a weight for each 
 and generates values from each 
 with
Gen
Gen
probability proportional to its weight.
def weighted(g1: (Gen[A],Double), g2: (Gen[A],Double)): Gen[A]
Now that we know more about our representation of generators, let's return to our
definition of 
. Our 
 representation has revealed information about the
Prop
Gen
requirements for 
. First, we notice that our properties can succeed in one of
Prop
two ways—they can be 
 correct, by exhaustive enumeration, or they can
proven
succeed when no counterexamples are found via random generation. Our current
definition of 
 doesn't distinguish between these possibilities:
Prop
trait Prop { def check: Either[FailedCase,SuccessCount] }
It only includes two cases, one for success, and one for failure. We can create a
new data type, 
, to represent the two ways a test can succeed:
Status
trait Status
case object Proven extends Status
case object Unfalsified extends Status
trait Prop { def run: Either[FailedCase,(Status,SuccessCount)] }
A test can succeed by being 
, if the domain has been fully enumerated
proven
and no counterexamples found, or it can be merely 
, if the test runner
unfalsified
had to resort to random sampling.
Prop is now nothing more than a non-strict 
. But 
 is still
Either
Prop
missing some information—we have not specified how many test cases to examine
before we consider the property to be passed. We could certainly hardcode
something, but it would be better to propagate this dependency out:
REFININING THE PROP DATA TYPE
129
www.it-ebooks.info

type TestCases = Int
type Result = Either[FailedCase, (Status,SuccessCount)]
case class Prop(run: TestCases => Result)
Is this sufficient? Let's take another look at 
. Can 
 be
forAll
forAll
implemented? Why not?
def forAll[A](a: Gen[A])(f: A => Boolean): Prop
We can see that 
 does not have enough information to return a 
.
forAll
Prop
Besides the number of test cases to run, 
 must have all the information
Prop
needed for generated test cases to return a 
, but if it needs to generate
Status
random test cases, it is going to need an 
. Let's go ahead and propagate that
RNG
dependency to 
:
Prop
case class Prop(run: (TestCases,RNG) => Result)
We can start to see a pattern here. There are certain parameters that go into
generating test cases, and if we think of other parameters besides the number of
test cases and the source of randomness, we can just add these as extra arguments
to 
.
Prop
We now have enough information to actually implement 
. Here is a
forAll
simple implementation. We have a choice about whether to use exhaustive or
random test case generation. For our implementation, we'll spend a third of our test
cases examining elements from the exhaustive 
. If we reach the end of that
Stream
 or find a counterexample, we return immediately, otherwise, we fall back
Stream
to generating random test cases for our remaining test cases:10
Footnote 10mHere is a question to explore—might there be a way to track the expected size of the exhaustive
stream, such that the decision to use random data could be made up front? For some primitives, it is certainly
possible, but is it possible for all our primitives?
def forAll[A](a: Gen[A])(f: A => Boolean): Prop = Prop {
  (n,rng) => {
    def go(i: Int, j: Int, s: Stream[Option[A]], onEnd: Int => Result):
    Result =
      if (i == j) Right((Unfalsified, i))
      else s.uncons match {
        case Some((Some(h),t)) =>
          try { if (f(h)) go(i+1,j,s,onEnd)
130
www.it-ebooks.info

                else Left(h.toString) }
          catch { case e: Exception => Left(buildMsg(h, e)) }
        case Some((None,_)) => Right((Unfalsified,i))
        case None        => onEnd(i)
      }
    go(0, n/3, a.exhaustive, i => Right((Proven, i))) match {
      case Right((Unfalsified,_)) =>
        val rands = randomStream(a)(rng).map(Some(_))
        go(n/3, n, rands, i => Right((Unfalsified, i)))
      case s => s
    }
  }
}
If proven or failed, stop immediately
def buildMsg[A](s: A, e: Exception): String =
  "test case: " + s + "\n" +
  "generated an exception: " + e.getMessage + "\n" +
  "stack trace:\n" + e.getStackTrace.mkString("\n")
Notice we are catching exceptions and reporting them as test failures, rather
than bringing down the test runner (which would lose information about what
argument triggered the failure).
EXERCISE 12: Now that we have a representation of 
, implement 
,
Prop
&&
and 
 for manipulating 
 values. While we can implement 
, notice that in
||
Prop
&&
the case of failure we aren't informed which property was responsible, the left or
the right. (Optional): Can you devise a way of handling this, perhaps by allowing 
 values to be assigned a tag or label which gets displayed in the event of a
Prop
failure?
def &&(p: Prop): Prop
def ||(p: Prop): Prop
Earlier, we mentioned the idea of test case minimization. That is, we would ideally
like our framework to find the 
 or simplest failing test case, to better
smallest
illustrate the problem and facilitate debugging. Let's see if we can tweak our
representations to support this. There are two general approaches we could take:
8.3.3 Test case minimization
131
www.it-ebooks.info

Shrinking: After we have found a failing test case, we can run a separate procedure to
minimize the test case, by successively decreasing its "size" until it no longer fails. This
is called 
, and it usually requires us to write separate code for each data type to
shrinking
implement this minimization process.
Sized generation: Rather than shrinking test cases after the fact, we simply generate our
test cases in order of increasing size and complexity. So, we start small and increase size
until finding a failure. This idea can be extended in various ways, to allow the test runner
to make larger jumps in the space of possible sizes while still making it possible to find
the smallest failing test.
ScalaCheck, incidentally, takes this first approach of 
. There's nothing
shrinking
wrong with this approach (it is also used by the Haskell library 
 that
QuickCheck
ScalaCheck is based on) but we are going to see what we can do with sized
generation. It's a bit simpler and in some ways more modular—our generators only
need to be knowledgeable about how to generate a test case of a given size, they
don't need to be aware of the 'schedule' used to search the space of test cases and
the test runner therefore has the freedom to choose this schedule. We'll see how
this plays out shortly.
We are going to do something different. Rather than modifying our 
 data
Gen
type, for which we've already written a number of useful combinators, we are
going to introduce sized generation as a separate layer in our library. That is, we
are going to introduce a type, 
, for representing sized generators:
SGen
case class SGen[+A](forSize: Int => Gen[A])
EXERCISE 13: Implement helper functions for converting 
 to 
. You
Gen
SGen
can add this as a method to 
.
Gen
def unsized: SGen[A]
EXERCISE 14 (optional): Not surprisingly, 
 at a minimum supports many
SGen
of the same operations as 
, and the implementations are rather mechanical.
Gen
You may want to define some convenience functions on 
 that simply
SGen
delegate to the corresponding functions on 
.
Gen 11
Footnote 11mAgain, we are going to discuss in Part 3 ways of factoring out this sort of duplication.
EXERCISE 15: We can now implement a 
 combinator that does not
listOf
accept an explicit size. It can return an 
 instead of a 
. The
SGen
Gen
132
www.it-ebooks.info

implementation can generate lists of the requested size.
def listOf[A](g: Gen[A]): SGen[List[A]]
Let's see how 
 affects the definition of 
 and 
. The 
SGen
Prop
Prop.forAll
 version of 
 looks like this:
SGen
forAll
def forAll[A](g: SGen[A])(f: A => Boolean): Prop
Can you see how this function is not possible to implement? 
 is expecting
SGen
to be told a size, but 
 does not receive any size information. Much like we
Prop
did with the source of randomness and number of test cases, we simply need to
propagate this dependency to 
. But rather than just propagating this
Prop
dependency 
 to the caller of 
, we are going to have 
 accept a 
as is
Prop
Prop
 size. This puts 
 in charge of invoking the underlying generators
maximum
Prop
with various sizes, up to and including the maximum specified size, which means it
can also search for the smallest failing test case. Let's see how this works out:12
Footnote 12mThis rather simplistic implementation gives an equal number of test cases to each size being
generated, and increases the size by  starting from . We could imagine a more sophisticated implementation
1
0
that does something more like a binary search for a failing test case size—starting with sizes 
 then narrowing in on smaller sizes in the event of a failure.
0,1,2,4,8,16...
case class Prop(run: (MaxSize,TestCases,RNG) => Status)
def forAll[A](g: SGen[A])(f: A => Boolean): Prop =
  forAll(g.forSize)(f)
def forAll[A](g: Int => Gen[A])(f: A => Boolean): Prop = Prop {
  (max,n,rng) =>
    val casesPerSize = n / max + 1
    val props: List[Prop] =
      Stream.from(0).take(max+1).map(i => forAll(g(i))(f)).toList
    props.map(p => Prop((max,n,rng) => p.run(max,casesPerSize,rng))).
          reduceLeft(_ && _)(max,n,rng)
}
This implementation highlights a couple minor problems with our
representation of 
. For one, there are actually now 
 ways that a property
Prop
three
can succeed. It can be 
, if the domain of the generator has been fully
proven
133
www.it-ebooks.info

examined (for instance, a 
 can only ever generate two distinct
SGen[Boolean]
values, regardless of size). It can be 
, if the domain of the generator has
exhausted
been fully examined, but only up through the maximum size. Or it could be merely
, if we had to resort to random generation and no counterexamples were
unfalsified
found. Let's add this to our 
 representation:
Status
case object Exhausted extends Status
case object Proven extends Status
case object Unfalsified extends Status
EXERCISE 16: Try to reimplement 
, assuming this definition of 
forAll
. Notice that we lack a way to distinguish between 
 and 
Status
Proven
. Why is that? Can you see how to fix it?
Exhausted
The problem is that 
 is a totally opaque function from 
 to 
, and
SGen
Size
Gen
so the test runner in 
 cannot distinguish a generator that has been 
forAll
fully
exhausted from a generator that has merely been exhausted for the given size. We
can add this distinction by making 
 into a data type with two cases:
SGen
trait SGen[+A]
case class Sized[+A](forSize: Size => Gen[A]) extends SGen[A]
case class Unsized[+A](get: Gen[A]) extends SGen[A]
EXERCISE 17: Implement 
 for this representation of 
 and any
forAll
SGen
other functions you've implemented that require updating to reflect these changes
in representation. Notice that we only need to update primitive
combinators—derived combinators get their new behavior "for free", based on the
updated implementation of primitives.
We have converged on what seems like a reasonable API. We could keep tinkering
with it, but at this point let's try 
 the library to construct tests and see if we
using
notice any deficiencies, either in what it can express or its general usability.
Usability is somewhat subjective, but we generally like to provide convenient 
 and appropriate 
 which abstract out common patterns that
syntax
helper functions
occur in client usage of the library. We aren't necessarily aiming here to make the
library more expressive, we simply want to make it more pleasant to use.
Let's revisit an example that we mentioned at the start of this
8.3.4 Using the library, improving its usability, and future directions
134
www.it-ebooks.info

chapter—specifying the behavior of the function 
, available as a method on 
max
 (
). The maximum of a list should be greater than or equal to
List API docs link
every other element in the list. Let's specify this:
val smallInt = Gen.choose(-10,10)
val maxProp = forAll(listOf(smallInt)) { l =>
  val max = l.max
  !l.exists(_ > max)
}
No value greater than max should exist in l
We can introduce a helper function in 
 for actually 
 our 
Prop
running
Prop
values and printing their result to the console in a useful format:
def run(p: Prop,
        maxSize: Int = 100,
        testCases: Int = 100,
        rng: RNG = RNG.simple(System.currentTimeMillis)): Unit = {
  p.run(maxSize, testCases, rng) match {
    case Left(msg) => println("! test failed:\n" + msg)
    case Right((Unfalsified,n)) =>
      println("+ property unfalsified, ran " + n + " tests")
    case Right((Proven,n)) =>
      println("+ property proven, ran " + n + " tests")
    case Right((Exhausted,n)) =>
      println("+ property unfalsified up to max size, ran " +
               n + " tests")
  }
}
A default argument of 200
We are using default arguments here to make it more convenient to call in the
case that the defaults are fine.
EXERCISE 18: Try running 
. Notice that it fails!
Prop.run(maxProp)
Property-based testing has a way of revealing all sorts of hidden assumptions
we have about our code, and forcing us to be much more explicit about these
assumptions. The Scala standard library implementation of 
 crashes when
max
given the empty list (rather than returning an 
).
Option
EXERCISE 19: Define 
, for generating nonempty lists, then update
listOf1
your specification of 
 to use this generator.
max
Let's try a few more examples.
135
www.it-ebooks.info

EXERCISE 20: Write a property to verify the behavior of 
 (
List.sorted
), which you can use to sort (among other things) a 
.
API docs link
List[Int] 13
For instance, 
.
List(2,1,3).sorted == List(1,2,3)
Footnote 13m
 takes an 
 
 for the elements of the list, to control the sorting strategy.
sorted
implicit Ordering
val sortedProp = forAll(listOf(smallInt)) { l =>
  val ls = l.sorted
  l.isEmpty || !l.zip(l.tail).exists { case (a,b) => a > b }
}
Recall that in the previous chapter we looked at laws we expected to hold for
our parallel computations. Can we express these laws with our library? The first
"law" we looked at was actually a particular test case:
map(unit(1))(_ + 1) == unit(2)
We certainly can express this, but the result is somewhat ugly.14
Footnote 14mRecall that 
.
type Par[A] = ExecutorService => Future[A]
val ES: ExecutorService = Executors.newCachedThreadPool
val p1 = Prop.forAll(Gen.unit(Par.unit(1)))(i =>
  Par.map(i)(_ + 1)(ES).get == Par.unit(2)(ES).get)
We've expressed the test, but it's verbose, cluttered, and the "idea" of the test is
obscured by details that aren't really relevant here. Notice that this isn't a question
of the API being expressive enough—yes we can express what we want, but a
combination of missing helper functions and poor syntax obscures the intent.
Let's improve on this. Our first observation is that 
 is a bit too general
forAll
for this test case. We aren't varying the input to this test, we just have a hardcoded
example. Hardcoded examples should be just as convenient to write as in a
traditional unit testing library. Let's introduce a combinator for it:
def check(p: => Boolean): Prop =
  forAll(unit(()))(_ => p)
Note that we are non-strict here
136
www.it-ebooks.info

Is this cheating? Not at all. We provide the unit generator, which, of course,
only generates a single value. The value will be ignored in this case, simply used to
drive the evaluation of the given 
. Notice that this combinator is
Boolean
general-purpose, having nothing to do with 
—we can go ahead and move it
Par
into the 
 companion object. Updating our test case to use it gives us:
Prop
val p2 = check {
  val p = Par.map(Par.unit(1))(_ + 1)
  val p2 = Par.unit(2)
  p(ES).get == p2(ES).get
}
Better. Can we do something about the 
 and 
 noise?
p(ES).get
p2(ES).get
There's something rather unsatisfying about it. For one, we're forcing this code to
be aware of the internal implementation details of 
, simply to compare two 
Par
 values for equality. One improvement is to move the equality comparison into
Par
, and into a helper function, which means we only have to run a single 
 at
Par
Par
the end to get our result:
def equal[A](p: Par[A], p2: Par[A]): Par[Boolean] =
  Par.map2(p,p2)(_ == _)
val p3 = check {
  equal (
    Par.map(Par.unit(1))(_ + 1),
    Par.unit(2)
  ) (ES) get
}
So, we are 
 equality to operate in 
, which is a bit nicer than having to
lifting
Par
run each side separately. But while we're at it, why don't we move the 
 of 
running
 out into a separate function, 
, and the analogous 
.
Par
forAllPar
checkPar
This also gives us a good place to insert variation 
 different parallel
across
strategies, without it cluttering up the property we are specifying:
val S = weighted(
  choose(1,4).map(Executors.newFixedThreadPool) -> .75,
  unit(Executors.newCachedThreadPool) -> .25)
def forAllPar[A](g: Gen[A])(f: A => Par[Boolean]): Prop =
  forAll(S.map2(g)((_,_))) { case (s,a) => f(a)(s).get }
137
www.it-ebooks.info

def checkPar(p: Par[Boolean]): Prop =
  forAllPar(Gen.unit(()))(_ => p)
a -> b is syntax sugar for (a,b)
S.map2(g)((_,_)) is a rather noisy way of combining two generators to
produce a pair of their outputs. Let's quickly introduce a combinator to clean that
up:15
Footnote 15mHints and standalone answers
def **[B](g: Gen[B]): Gen[(A,B)] =
  (this map2 g)((_,_))
Much nicer:
def forAllPar2[A](g: Gen[A])(f: A => Par[Boolean]): Prop =
  forAll(S ** g) { case (s,a) => f(a)(s).get }
We can even introduce 
 as a pattern 
, which lets us
**
using custom extractors
write:
def forAllPar3[A](g: Gen[A])(f: A => Par[Boolean]): Prop =
  forAll(S ** g) { case s ** a => f(a)(s).get }
This syntax works nicely when tupling up multiple generators—when pattern
matching, we don't have to nest parentheses like would be required when using the
tuple pattern directly. To enable 
 as a pattern, we define an object called 
**
**
with an 
 function:
unapply
object ** {
  def unapply[A,B](p: (A,B)) = Some(p)
}
See the 
 documentation for more details.
custom extractors
So,  is a 
 that will vary over fixed sized thread
S
Gen[ExecutorService]
pools from 1-4 threads, and also consider an unbounded thread pool. And now our
property looks a lot cleaner:16
138
www.it-ebooks.info

Footnote 16mWe cannot use the standard Java/Scala 
 method, or the 
 method in Scala (which
equals
==
delegates to the 
 method), since that method returns a 
 directly, and we need to return a 
equals
Boolean
. Some infix syntax for 
 might be nice. See the chapter code for the previous chapter
Par[Boolean]
equal
on purely functional parallelism for an example of how to do this.
val p2 = checkPar {
  equal (
    Par.map(Par.unit(1))(_ + 1),
    Par.unit(2)
  )
}
These might seem like minor changes, but this sort of factoring and cleanup can
greatly improve the usability of your library, and the helper functions we've written
make the properties easier to read and more pleasant to write. You may want to add
versions of 
 and 
 for sized generators as well.
forAllPar
checkPar
Let's look at some other properties from the previous chapter. Recall that we
generalized our test case:
map(unit(x))(f) == unit(f(x))
We then simplified it to the law:
map(y)(id) == y
Can we express this? Not exactly. This property implicitly states that the
equality holds 
 choices of , for all types. We are forced to pick particular
for all
y
values for :
y
val pint = Gen.choose(0,10) map (Par.unit(_))
val p4 =
  forAllPar(pint)(n => equal(Par.map(n)(y => y), n))
We can certainly range over more choices of , but what we have here is
y
probably good enough. The implementation of 
 cannot care about the values of
map
our parallel computation, so there isn't much point in constructing the same test for 
, 
, and so on. What 
 
 be affected by is the 
 of the
Double String
map can
structure
parallel computation. If we wanted greater assurance that our property held, we
139
www.it-ebooks.info

could provide richer generators for the structure. Here, we are only supplying Par
expressions with one level of nesting.
EXERCISE 21 (hard): Writer a richer generator for 
, which builds
Par[Int]
more deeply nested parallel computations than the simple ones we gave above.
EXERCISE 22: Express the property about 
 from last chapter, that 
fork
.
fork(x) == x
So far, our library seems quite expressive, but there's one area where it's
lacking: we don't currently have a good way to test higher-order functions. While
we have lots of ways of generating 
, using our generators, we don't really have
data
a good way of generating 
.
functions
For instance, let's consider the 
 function defined for 
 and 
takeWhile
List
. Recall that this function returns the longest prefix of its input whose
Stream
elements all satisfy a predicate. For instance, List(1,2,3).takeWhile(_ <
 results in 
. A simple property we'd like to check is that for any
3)
List(1,2)
stream, 
, 
and 
any 
, 
s: 
List[A]
f: 
A 
=> 
Boolean
, that is, every element in the returned stream
s.takeWhile(f).forall(f)
satisfies the predicate.17
Footnote 17mIn the Scala standard library, 
 is a method on 
 and 
 with the signature 
forall
List
Stream
.
def forall[A](f: A => Boolean): Boolean
EXERCISE 23: Come up with some other properties that 
 should
takeWhile
satisfy. Can you think of a good property expressing the relationship between 
 and 
?
takeWhile
dropWhile
We could certainly take the approach of only examining 
 arguments
particular
when testing HOFs like 
. For instance, here's a more specific property
takeWhile
for 
:
takeWhile
val isEven = (i: Int) => i%2 == 0
val takeWhileProp =
  Prop.forAll(Gen.listOf(int))(l => l.takeWhile(isEven).forall(isEven))
This works, but is there a way we could let the testing framework handle
generating functions to use with 
?
 Let's consider our options. To
takeWhile 18
make this concrete, let's suppose we have a 
 and would like to produce
Gen[Int]
140
www.it-ebooks.info

a 
. What are some ways we could do that? Well, we
Gen[String => Int]
could produce 
 functions that simply ignore their input string
String => Int
and delegate to the underlying 
.
Gen[Int]
Footnote 18mRecall that in the previous chapter we introduced the idea of 
 and discussed how
free theorems
parametricity frees us somewhat from having to inspect the behavior of a function for every possible type argument.
Still, there are many situations where being able to generate functions for testing is useful.
EXERCISE 24: Implement a function to do this conversion.
def genStringIntFn(g: Gen[Int]): Gen[String => Int]
This approach isn't sufficient though. We are simply generating constant
functions that ignore their input. In the case of 
, where we need a
takeWhile
function that returns a 
, this will be a function that always returns 
Boolean
true
or always returns 
—clearly not very interesting for testing the behavior of
false
our function.
EXERCISE 25 (hard, optional): We clearly want to generate a function that 
 in some way to select which 
 to return. Can you think of a
uses its argument
Int
good way of expressing this? This is a very open-ended and challenging design
exercise. See what you can discover about this problem and if there is a nice
general solution that you can incorporate into the library we've developed so far.
This exercise is optional, but you may find it interesting to work on.
EXERCISE 26: You are strongly encouraged to venture out and try using the
library we've developed! See what else you can test with it, and see if you discover
any new idioms for its use or perhaps ways it could be extended further or made
more convenient. Here are a few ideas to get you started:
Try writing properties to specify the behavior of some of the other functions we wrote for
 and 
, for instance 
, 
, 
, and 
.
List
Stream
take drop filter
unfold
Try writing a sized generator for producing the 
 data type we defined in chapter 3,
Tree
then use this to specify the behavior of the 
 function we defined for 
. Can you
fold
Tree
think of ways to improve the API to make this easier?
Try writing properties to specify the behavior of the 
 function we defined for 
sequence
 and 
.
Option
Either
141
www.it-ebooks.info

Isn't it interesting that many of the functions we've implemented here, for our Gen
type, look quite similar to other functions we've defined on 
, 
, 
,
Par List Stream
and 
? As an example, for 
 we defined:
Option
Par
def map[A,B](a: Par[A])(f: A => B): Par[B]
And in this chapter we defined 
 for 
 (as a method on 
):
map
Gen
Gen[A]
def map[B](f: A => B): Gen[B]
And we've defined very similar-looking functions for other data types. We have
to wonder, is it merely that our functions share similar-looking signatures, or do
they satisfy the same 
 as well? Let's look at a law we introduced for 
 in the
laws
Par
previous chapter:
map(x)(id) == x
EXERCISE 27: Does this law hold for our implementation of 
? What
Gen.map
about for 
, 
, 
 and 
?
Stream List Option
State
Fascinating! Not only do these functions share similar-looking signatures, they
also in some sense have analogous meanings in their respective domains.
EXERCISE 28 (hard, optional): Spend a little while thinking up laws for some
of the functions with similar signatures you've written for 
, 
, and 
List Option
. For each law, see if an analogous law holds for 
.
State
Gen
It appears there are deeper forces at work! We are uncovering some rather
fundamental patterns that cut across domains. In part 3, we'll learn the names for
these patterns, discover the laws that govern them, and understand what it all
means.19
Footnote 19mIf curiosity is really getting the better of you, feel free to peek ahead at Part 3.
8.3.5 The laws of generators
142
www.it-ebooks.info

lifting
primitive vs. derived operations
shrinking, test cases
test case minimization
test case minimization
In this chapter, we worked through another extended exercise in functional library
design, using the domain of property-based testing as inspiration. Once again, we
reiterate that our goal was not necessarily to learn about property-based testing per
, but to give a window into the process of functional design. We hope these
se
chapters are giving you ideas about how to approach functional library design in
your own way and preparing you for the sorts of issues and questions you'll
encounter. Developing an understanding of the overall process is much more
important than following absolutely every small design decision we made as we
explored the space of this particular domain.
In the next chapter, we'll look at another domain, 
, with its own set of
parsing
challenges and design questions. As we'll see, similar patterns will emerge.
Index Terms
8.4 Conclusion
143
www.it-ebooks.info

9
In this chapter, we will work through the design of a combinator library for
creating 
, using 
 parsing as a motivating use case. As in the past two
parsers
JSON
chapters, we will use this opportunity to provide insight into the process of
functional design and notice common patterns that we'll discuss more in part 3.
This chapter will introduce a design approach called 
. This is
algebraic design
just a natural evolution of what we've already been doing to different degrees in
past chapters—designing our interface and associated laws first and letting this
guide our choice of data type representations. Here, we take this idea to its logical
limit to see what it buys us.
At a few key points during this chapter, we will be giving more open-ended
exercises, intended to mimic the scenarios you might encounter when designing
and implementing your own libraries from scratch. You'll get the most out of this
chapter if you use these opportunities to put the book down and spend some time
investigating possible approaches. When you design your own libraries, you won't
be handed a nicely chosen sequence of type signatures to fill in with
implementations. You will have to make the decisions about what types and
combinators should even exist and a goal in part 2 of this book has been to prepare
you for doing this on your own. As always, in this chapter, if you get stuck on one
of the exercises or want some more ideas, you can keep reading or consult the
answers.
Parser combinators
9.1 Introduction
144
www.it-ebooks.info

SIDEBAR
Parser combinators vs. parser generators
You might be familiar with 
 libraries like 
 or similar
parser generator
Yacc
libraries in other languages (for instance, 
 in Java). These
ANTLR
libraries 
 code for a parser based on a specification of the
generate
grammar. This approach works fine and can be quite efficient, but
comes with all the usual problems of code generation—they produce as
as their output a monolithic chunk of code that is difficult to debug. It's
also quite difficult to reuse fragments of logic since we cannot introduce
new combinators or helper functions to abstract over common patterns
in our parsers.
In a parser combinator library, parsers are ordinary values that can be
created and manipulated in a first-class way within the language.
Reusing parsing logic is trivial (we simply introduce a new combinator),
and we don't have to delegate to any sort of separate external tool.
Recall that we defined an algebra to mean a collection of functions operating over
some data type(s), 
 specifying relationships between these
along with a set of laws
functions. In past chapters, we moved rather fluidly between inventing functions in
our algebra, refining the set of functions, and tweaking our data type
representations. Laws were somewhat of an afterthought—we worked out the laws
only after we had a representation and an API fleshed out. There's absolutely
nothing wrong with this style of design , but here we are going to take a different
1
approach. We will 
 with the algebra (including its laws) and decide on a
start
representation later. This is an approach we'll call 
. This approach
algebraic design
can be used for any design problem but works particularly well for parsing,
because it's easy to imagine what combinators are required to be able to parse
different inputs.  Overall, you might find this approach very natural, or you might
2
find it extremely disconcerting to do so much work without committing to any
concrete representations. Either way, try to keep an open mind.
Footnote 1mHints and standalone answers
Footnote 2mFull source files
There are a lot of different parsing libraries.  Ours will be designed for
3
expressiveness (we'd like to be able to parse arbitrary grammars), speed, and good
error reporting. This last point is important—if there are parse errors, we want to
be able to indicate exactly where the error is and accurately indicate its cause.
9.2 Designing an algebra, first
145
www.it-ebooks.info

Error reporting is often somewhat of a second-class citizen in parsing libraries, but
we are going to make sure we give careful thought to it.
Footnote 3mIncluding a parser combinator library in Scala's standard library. As in the previous chapter, we
are deriving our own library from first principles partially for pedagogical purposes, and to further encourage the
idea that no library is authoritative. Scala's parser combinators don't really satisfy our goals of providing speed and
good error reporting (see the chapter notes for some additional discussion).
OK, let's begin. For simplicity and for speed, our library will create parsers that
operate on strings as input.  We need to pick some parsing tasks to help us
4
discover a good algebra for our parsers. What should we look at first? Something
practical like parsing an email address, JSON, or HTML? No! These can come
later. For now we are content to focus on a pure, simple domain of parsing various
combinations of repeated letters and jibberish words like 
 and 
"abracadabra"
. As silly as this sounds, we've seen before how simple examples like this
"abba"
help us ignore extraneous details and focus on the essence of the problem.
Footnote 4mThis is certainly a simplifying design choice. We can make the parsing library more generic, at
some cost. See the chapter notes for more discussion.
So let's start with the simplest of parsers, one that recognizes the single
character input 
. As we've done in past chapters, we can just 
 a
"a"
invent
combinator for the task, 
:
char
def char(c: Char): Parser[Char]
What have we done here? We have conjured up a type, 
, which is
Parser
parameterized on a single parameter indicating the 
 of the 
. That
result type
Parser
is, running a parser should not simply yield a yes/no response to the input—if it
succeeds, we want to get back a 
 of some useful type, and if it fails we expect
result
. The 
 parser will succeed only if the
information about the failure
char('a')
input is the string 
 and we have chosen (somewhat arbitrarily) to have it return
"a"
that same character 
 as its result.
'a'
This talk of "running a parser" makes it clear our algebra needs to be extended
somehow to support that. Let's invent another function for it:
def run[A](p: Parser[A])(input: String): Either[ParseError,A]
Wait a minute, what is 
? It's another type we've just conjured
ParseError
146
www.it-ebooks.info

into existence! At this point, we don't care about the representation of 
 or 
 for that matter. We are in the process of specifying an 
ParseError
Parser
 that happens to make use of two types whose representation or
interface
implementation details we choose to remain ignorant of for now. Let's make this
explicit with a 
:
trait
trait Parsers[ParseError, Parser[_]] {
  def run[A](p: Parser[A])(input: String): Either[ParseError,A]
  def char(c: Char): Parser[Char]
}
A type constructor type argument
What's with the funny 
 type argument? It's not too important for
Parser[_]
right now, but that is Scala's syntax for a type parameter that is itself a type
constructor.  Just like making 
 a type argument lets the 
5
ParseError
Parsers
interface work for multiple representations of 
, making 
ParseError
 a type parameter means the interface works for multiple
Parser[_]
representations of 
, which itself can be applied to one type argument.
Parser
6
This code will compile as is, without us having to pick a representation for 
 or 
, and you can continue placing additional combinators
ParseError
Parser
in the body of this trait.
Footnote 5mWe will say much more about this in the next chapter. We can indicate that the 
 type
Parser[_]
parameter should be covariant in its argument with the syntax 
.
Parser[+_]
Footnote 6mWe will say much more about this in the next chapter. We can indicate that the 
 type
Parser[_]
parameter should be covariant in its argument with the syntax 
.
Parser[+_]
Let's continue. We can recognize the single character 
, but what if we want
'a'
to recognize the string 
? We don't have a way of recognizing
"abracadabra"
entire strings right now, so let's add that:
def string(s: String): Parser[String]
What if we want to recognize either the string 
 
 the string 
"abra" or
? We could certainly add a very specialized combinator for it:
"cadabra"
def orString(s1: String, s2: String): Parser[String]
147
www.it-ebooks.info

But choosing between two parsers seems like something that would be more
generally useful, regardless of their result type, so let's go ahead and make this
polymorphic:
def or[A](s1: Parser[A], s2: Parser[A]): Parser[A]
Incidentally, we can give this nice infix syntax like 
 or alternately 
s1 | s2
s1
, using implicits like we did in chapter 7:
or s2
Listing 9.1 Adding infix syntax to parsers
trait Parsers[ParseError, Parser[+_]] {
  ...
  def or[A](s1: Parser[A], s2: Parser[A]): Parser[A]
  implicit def string(s: String): Parser[String]
  implicit def operators[A](p: Parser[A]) = ParserOps[A](p)
  implicit def asStringParser[A](a: A)(implicit f: A => Parser[String]):
    ParserOps[String] = ParserOps(f(a))
  case class ParserOps[A](p: Parser[A]) {
        def |[B>:A](p2: Parser[B]): Parser[B] = self.or(p,p2)
    def or[B>:A](p2: => Parser[B]): Parser[B] = self.or(p,p2)
  }
}
use self to explicitly disambiguate reference to the or method on the trait
We have also made 
 an implicit conversion and added another implicit 
string
. With these two functions, Scala will automatically promote a
asStringParser
 to a 
, and we get infix operators for any type,  that can be
String
Parser
A
converted to a 
. So given a 
, we can then 
Parser[String]
val P: Parsers
 to let us write expressions like 
 or 
import P._
"abra" or "cadabra"
"a"
 to create parsers. This will work for 
 implementations of 
.
| "bbb"
all
Parsers 7
Other binary operators or methods can be added to the body of 
. We
ParserOps
are going to follow the discipline of keeping the primary definition directly in 
 and delegating in 
 to this primary definition. See the code
Parsers
ParserOps
for this chapter for more examples. We'll be using the 
 syntax liberally
a | b
throughout the rest of this chapter to mean 
.
or(a,b)
Footnote 7mSee the appendix 
 for more discussion of these
Scalaz, implicits, and large library organization
issues.
148
www.it-ebooks.info

We can now recognize various strings, but we don't have any way of talking
about repetition. For instance, how would we recognize three repetitions of our 
 parser? Once again, let's add a combinator for it:
"abra" | "cadabra"
8
Footnote 8mThis should remind you of a very similar function we wrote in the previous chapter.
def listOfN[A](n: Int, p: Parser[A]): Parser[List[A]]
We made 
 parametric in the choice of , since it doesn't seem like it
listOfN
A
should care whether we have a 
, a 
, or some
Parser[String]
Parser[Char]
other type of parser.
At this point, we have just been collecting up required combinators, but we
haven't tried to refine our algebra into a minimal set of primitives, and we haven't
talked about laws at all. We are going to start doing this next, but rather than give
away the "answer", we are going to ask you to examine a few more simple use
cases yourself and try to design a minimal algebra with associated laws. This
should be a challenging exercise, but enjoy struggling with it and see what you can
come up with.
Here are additional parsing tasks to consider, along with some guiding
questions:
A 
 that recognizes zero or more 
 characters, and whose result value is
Parser[Int]
'a'
the number of 
 characters it has seen. For instance, given 
, the parser results in ,
'a'
"aa"
2
given 
 or 
 (a string not starting with 
), it results in , and so on.
""
"b123"
'a'
0
A 
 that recognizes 
 or more 
 characters, and whose result value is the
Parser[Int]
one
'a'
number of 
 characters it has seen. (Is this defined somehow in terms of the same
'a'
combinators as the parser for 
 repeated zero or more times?) The parser should fail
'a'
when given a string without a starting 
. How would you like to handle error reporting
'a'
in this case? Could the API support giving an explicit message like "Expected one or
 in the case of failure?
more 'a'"
A parser that recognizes zero or more 
, followed by one or more 
, and which
'a'
'b'
results in the pair of counts of characters seen. For instance, given 
, we get 
,
"bbb"
(0,3)
given 
, we get 
, and so on.
"aaaab"
(4,1)
And additional considerations:
If we are trying to parse a sequence of zero or more 
 and are only interested in the
"a"
number of characters seen, it seems inefficient to have to build up, say, a List[Char]
only to throw it away and extract the length. Could something be done about this?
Are the various forms of repetition primitive in your algebra, or could they be defined in
terms of something simpler?
We introduced a type 
 earlier, but so far we haven't chosen any functions for
ParseError
149
www.it-ebooks.info

the API of 
 and our algebra doesn't have any ways of letting the programmer
ParseError
control what errors are reported. This seems like a limitation given that we'd like
meaningful error messages from our parsers. Can you do something about it?
Does 
 mean the same thing as 
? This is a choice you get to make. What are
a | b
b | a
the consequences if the answer is yes? What about if the answer is no?
Does 
 mean the same thing as 
? If yes, is this a primitive law
a | (b | c)
(a | b) | c
for your algebra, or is it implied by something simpler?
Try to come up with a set of laws to specify your algebra. You don't necessarily need the
laws to be complete, just write down some laws that you expect should hold for any 
 implementation.
Parsers
Spend some time coming up with combinators and possible laws based on this
guidance. When you feel stuck or at a good stopping point, then continue reading
to the next section, which walks through a possible design.
SIDEBAR
The advantages of algebraic design
When you design the algebra of a library first, representations for the
data types of the algebra don't matter as much. As long as they support
the required laws and functions, you do not even need to make your
representations public. This, as we'll see later, makes it easy for
primitive combinators to use cheap tricks internally that might otherwise
break referential transparency.
There is a powerful idea here, namely, that a type is given meaning
based on its relationship to other types (which are specified by the set
of functions and their laws), rather than its internal representation.  This
9
is a viewpoint often associated with category theory, a branch of
mathematics we've mentioned before. See the chapter notes for more
on this connection if you're interested.
Footnote 9mThis sort of viewpoint might also be associated with
object-oriented design, although OO has not traditionally placed much
emphasis on algebraic laws. Furthermore, a big reason for encapsulation in
OO is that objects often have some mutable state and making this public
would allow client code to violate invariants, a concern that is not as relevant
in FP.
We are going to walk through discovering a set of combinators for the parsing
tasks mentioned above. If you worked through this design task yourself, you will
likely have taken a different path and may have ended up with a different set of
combinators, which is absolutely fine.
9.2.1 A possible algebra
150
www.it-ebooks.info

First, let's consider the parser that recognizes zero or more repetitions of the
character 
, and returns the number of characters it has seen. We can start by
'a'
adding a primitive combinator for it, let's call it 
:
many
def many[A](p: Parser[A]): Parser[List[A]]
This isn't quite right, though—we need a 
 that counts the
Parser[Int]
number of elements. We could change the 
 combinator to return a 
many
, but that feels a little too specific—undoubtedly there will be
Parser[Int]
occasions where we do care about more than just the list length. Better to introduce
another combinator that should be familiar by now, 
:
map
def map[A,B](a: Parser[A])(f: A => B): Parser[B]
We can now define our parser as 
.
map(many(char('a')))(_.length)
And let's go ahead and add 
 and 
 as methods in 
, so we can
map
many
ParserOps
write this as: 
.
char('a').many.map(_.length)
We have a strong expectation for the behavior of 
—it should merely
map
transform the result value if the 
 was successful. No additional input
Parser
characters should be examined by a 
, a failing parser cannot become a
map
successful one via a 
 and vice versa, and in general, we expect 
 to be 
map
map
 much like we required for 
 and 
. Let's go ahead and
structure preserving
Par
Gen
formalize this by stipulating the now-familiar law:
map(p)(id) == p
How should we document this law that we've just added? We could put it in a
documentation comment, but in the preceding chapter we developed a way to make
our laws 
. Let's make use of that library here:
executable
def equal[A](p1: Parser[A], p2: Parser[A])(in: Gen[String]): Prop =
  forAll(in)(s => run(p1)(s) == run(p2)(s))
def mapLaw[A](p: Parser[A])(in: Gen[String]): Prop =
  equal(p, p.map(a => a))(in)
151
www.it-ebooks.info

import fpinscala.testing._
trait Parsers {
  ...
  object Laws {
    def equal[A](p1: Parser[A], p2: Parser[A])(in: Gen[String]): Prop =
      forAll(in)(s => run(p1)(s) == run(p2)(s))
   
    def mapLaw[A](p: Parser[A])(in: Gen[String]): Prop =
      equal(p, p.map(a => a))(in)
  }
}
This will come in handy later when we go to test that our implementation of 
 behaves as we expect. As we discuss other laws, you are encouraged to
Parsers
write them out as actual properties inside the 
 object.
Laws
10
Footnote 10mAgain, see the chapter code for more examples. In the interest of keeping this chapter shorter,
we won't be giving 
 implementations of all the laws, but that doesn't mean you shouldn't try writing
Prop
them out yourself!
Incidentally, now that we have 
, we can actually implement 
 in terms
map
char
of 
:
string
def char(c: Char): Parser[Char] =
  string(c.toString) map (_.charAt(0))
And similarly, there's another combinator, 
, that can be defined in
succeed
terms of 
 and 
:
string
map
def succeed[A](a: A): Parser[A] =
  string("") map (_ => a)
This parser always succeeds with the value , regardless of the input string
a
(since 
 will always succeed, even if the input is empty). Does this
string("")
combinator seem familiar to you? We can specify its behavior with a law:
run(succeed(a))(s) == Right(a)
152
www.it-ebooks.info

The combination of 
 and 
 certainly lets us express the parsing task of
many
map
counting the number of 
 characters, but it seems inefficient to be constructing
'a'
a 
 only to discard its values and extract its length. It would be nice if
List[Char]
we could run a 
 purely to see what portion of the input string it examines.
Parser
Let's conjure up a combinator for that purpose:
def slice[A](p: Parser[A]): Parser[String]
We call it 
 since it returns the portion of the input string examined by
slice
the 
parser 
if 
successful. 
As 
an 
example, 
 
results 
in 
run(slice(or('a','b').many))("aaba")
—that is, we ignore the list accumulated by 
 and simply
Right("aaba")
many
return the portion of the input string matched by the parser.
With 
, 
our 
parser 
can 
now 
be 
written 
as 
slice
 (again, assuming we add an
char('a').many.slice.map(_.length)
alias for 
 to 
). The 
 function here is now
slice
ParserOps
_.length
referencing the 
 method on 
, rather than the 
 method on 
length
String
length
.
List
Let's consider the next use case. What if we want to recognize 
 or more 
one
'a'
characters? First, we introduce a new combinator for it, 
:
many1
def many1[A](p: Parser[A]): Parser[List[A]]
It feels like 
 should not have to be primitive, but should be defined
many1
somehow in terms of 
. Really, 
 is just  
 
.
many
many1(p)
p followed by many(p)
So it seems we need some way of running one parser, followed by another,
assuming the first is successful. Let's add that:
def product[A,B](p: Parser[A], p2: Parser[B]): Parser[(A,B)]
We can add 
 and 
 as methods on 
, where 
**
product
ParserOps
a ** b
and 
 both delegate to 
.
a product b
product(a,b)
EXERCISE 1: Using 
, implement the now-familiar combinator 
product
map2
SLICING AND NONEMPTY REPETITION
153
www.it-ebooks.info

and then use this to implement 
 in terms of 
. (Note that we could have
many1
many
chosen to make 
 primitive and defined 
 in terms of 
 as we've
map2
product
map2
done in the previous chapters)
def map2[A,B,C](p: Parser[A], p2: Parser[B])(f: (A,B) => C): Parser[C]
def map2[A,B,C](p: Parser[A], p2: Parser[B])(
                f: (A,B) => C): Parser[C] =
  map(product(p, p2))(f.tupled)
def many1[A](p: Parser[A]): Parser[List[A]] =
  map2(p, many(p))(_ :: _)
With 
, we can now implement the parser for zero or more 
 followed
many1
'a'
by one or more 
 as:
'b'
char('a').many.slice.map(_.length) **
char('b').many1.slice.map(_.length)
EXERCISE 2 (hard): Try coming up with properties to specify the behavior of 
.
product
Now that we have 
, is 
 really primitive? Let's think about what 
map2
many
 will do. It tries running , 
 another , and another, and so on
many(p)
p followed by
p
until attempting to parse  fails, at which point the parser returns the empty 
p
List
and combines the results of the successful parses.
EXERCISE 3 (hard): Before continuing, see if you can define 
 in terms of
many
, 
, and 
.
or map2
succeed
EXERCISE 4 (hard, optional): Using 
 and 
, implement the 
map2
succeed
 combinator from earlier.
listOfN
Now let's try writing 
:
many
def many[A](p: Parser[A]): Parser[List[A]] =
  map2(p, many(p))(_ :: _) or succeed(List())
This looks pretty, but there's a problem with it. Can you spot what it is? We are
calling 
 recursively in the second argument to 
, which is 
 in
many
map2
strict
154
www.it-ebooks.info

evaluating its second argument. Consider a simplified program trace of the
evaluation of 
 for some parser . We are only showing the expansion of
many(p)
p
the left side of the 
 here:
or
many(p)
map2(p, many(p))(_ :: _)
map2(p, map2(p, many(p))(_ :: _))(_ :: _)
map2(p, map2(p, map2(p, many(p))(_ :: _))(_ :: _))(_ :: _)
...
Because a call to 
 always evaluates its second argument, our 
map2
many
function will never terminate! That's no good. Let's go ahead and make product
and 
 non-strict in their second argument:
map2
def product[A,B](p: Parser[A], p2: => Parser[B]): Parser[(A,B)]
def map2[A,B,C](p: Parser[A], p2: => Parser[B])(
                f: (A,B) => C): Parser[C] =
  product(p, p2) map (f.tupled)
EXERCISE 5 (optional): We could also deal with non-strictness with a separate
combinator like we did in chapter 7. Try this here and make the necessary changes
to your existing combinators. What do you think of that approach in this instance?
Now our implementation of 
 should work fine. Conceptually, 
many
product
feels like it should have been non-strict in its second argument anyway, since if the
first 
 fails, the second will not even be consulted.
Parser
Now that we're considering whether combinators should be non-strict, let's
revisit 
:
or
def or[A](p1: Parser[A], p2: Parser[A]): Parser[A]
We will assume that 
 is left-biased, meaning it tries running 
 on the input
or
p1
then tries 
 only if 
 fails.
 In this case, we ought to make it non-strict in its
p2
p1
11
second argument, which may never even be consulted:
Footnote 11mThis is a design choice. You may wish to think about the consequences of having a version of 
 that always runs both 
 and 
.
or
p1
p2
155
www.it-ebooks.info

def or[A](p1: Parser[A], p2: => Parser[A]): Parser[A]
EXERCISE 6: Given this choice of meaning for 
, is it associative? That is,
or
should 
 equal 
 for all choices of , , and ?
a or (b or c)
(a or b) or c
a b
c
We'll come back to this question when refining the laws for our algebra.
Let's take a step back and look at the primitives we have so far:
string(s): Recognize and return a single String
slice(p): Return the portion of input inspected by  if successful
p
succeed(a): Always succeed with the value a
map(p)(f): Apply the function  to the result of , if successful
f
p
product(p1,p2): Sequence two parsers, running 
, then 
 and return the pair of their
p1
p2
results if both succeed
or(p1,p2): Choose between two parsers, first attempting 
, then 
 if 
 fails
p1
p2
p1
Using these primitives, we can express repetition and nonempty repetition (
, 
 and 
) as well as combinators like 
 and 
. What
many listOfN
many1
char
map2
else could we express?
Surprisingly, these primitives are sufficient for parsing 
 context-free
any
grammar, including JSON! We'll get to writing that JSON parser soon.
These combinators are not without limitations, though. Suppose we want to
parse a single digit, like 
, followed by 
 
 characters (this sort of
'4'
that many 'a'
problem should feel familiar). Examples of valid input are 
, 
, 
, 
"0" "1a" "2aa"
, and so on. This is an example of a context-sensitive grammar. It can't
"4aaaa"
be expressed with 
 because our choice of the second parser 
product
depends on
the result of the first (the second parser depends on its context). We want to run the
first parser, then do a 
 using the number extracted from the first parser's
listOfN
result. Can you see why 
 cannot express this?
product
EXERCISE 7: Before continuing, think of a primitive that makes it possible to
parse this grammar, and revisit your existing primitives as needed.
This progression might feel familiar to you. In past chapters, we encountered
similar expressiveness limitations and dealt with it by introducing a new primitive, 
. Let's introduce that here (and we'll add an alias to 
 so we
flatMap
ParserOps
can writer parsers using 
-comprehensions):
for
def flatMap[A,B](p: Parser[A])(f: A => Parser[B]): Parser[B]
9.2.2 Handling context-sensitivity
156
www.it-ebooks.info

Can you see how this signature implies an ability to sequence parsers?
EXERCISE 8: Using 
 and any other combinators, write the
flatMap
context-sensitive parser we could not express above. To parse the digits, you can
make use of a new primitive, 
, which promotes a regular expression to a 
regex
:
 In Scala, given a string, , it can be promoted to a 
 object
Parser 12
s
Regex
(which has methods for performing matching) using 
, for instance: 
s.r
"[a-zA-Z_][a-zA-Z0-9_]*".r
Footnote 12mIn theory this isn't necessary, we could write out 
 to recognize a
"0" | "1" | ... "9"
single digit, but this isn't likely to be very efficient.
implicit def regex(r: Regex): Parser[String]
EXERCISE 9: Implement 
 and 
 in terms of 
.
product
map2
flatMap
EXERCISE 10: 
 is no longer primitive. Express it in terms of 
map
flatMap
and/or other combinators.
So it appears we have a new primitive, 
, which enables
flatMap
context-sensitive parsing and lets us implement 
 and 
. This is not the first
map
map2
time 
 has made an appearance, but up until now we have not really tried
flatMap
to pin down any laws for it. We'll be working through that in this chapter in a later
section.
So far we have not discussed error reporting at all. We've been focused exclusively
on discovering a set of primitives that let us express parsers for different grammars.
But besides just being able to parse a grammar, we want to be able to determine
how the parser should respond when given unexpected input.
Even without knowing what an implementation of 
 will look like, we
Parsers
can reason very abstractly about 
 by a set of
what information is being specified
combinators. None of the combinators we have introduced so far say anything
about 
 should be reported in the event of failure or what other
what error message
information a 
 should contain. Our existing combinators only
ParseError
specify what the grammar is and what to do with the result if successful. If we
were to declare ourselves done and move to implementation at this point, we
would have to make some arbitrary decisions about error reporting and error
messages that are unlikely to be universally appropriate.
9.2.3 Error reporting
157
www.it-ebooks.info

EXERCISE 11 (hard): If you have not already done so, spend some time
discovering a nice set of combinators for expressing what errors get reported by a 
. For each combinator, try to come up with laws specifying what its
Parser
behavior should be. This is a very open-ended design task. Here are some guiding
questions:
Given the parser 
, what
"abra" ** " ".many ** "cadabra"
sort of error would you like to report given the input "abra cAdabra"
(note the capital 
)? Only something like 
? Or 
'A'
Expected 'a'
? What if we wanted to choose a different error
Expected "cadabra"
message like 
?
"Expected The Magic Word (TM)"
Given 
, if  fails on the input, do we 
 want to run , or
a or b
a
always
b
are the cases where we might not want to? If there are cases, can you think
of additional combinators that would allow the programmer to specify when 
 should consider the second parser?
or
Given 
, if  and  both fail on the input, might we want to
a or b
a
b
support reporting both errors? And do we 
 want to report both errors,
always
or do we want to give the programmer a way to specify which of the two
errors are reported?
How do you want to handle reporting the 
 of errors?
location
Once you are satisfied with your design, you can continue reading. The next
section works through a possible design in detail.
SIDEBAR
Combinators specify information
In a typical library design scenario, where one has a concrete
representation at least in mind, we often think of functions in terms of
how they will affect this representation. By starting with the algebra first,
we are forced to think differently—we must think of functions in terms of
 to a possible implementation. The
what information they specify
signatures specify what information is given to the implementation, and
the implementation is free to use this information however it wants as
long as it respects the specified laws.
158
www.it-ebooks.info

Now that you've spent some time coming up with some good error reporting
combinators, we are now going to work through one possible design. Again, you
may have arrived at a different design, which is absolutely fine. This is just another
opportunity to see a worked design process.
We are going to progressively introduce our error reporting combinators. To
start, let's introduce an obvious one. None of the primitives so far let us assign an
error message to a parser. We can introduce a primitive combinator for this, 
:
label
def label[A](msg: String)(p: Parser[A]): Parser[A]
The intended meaning of 
 is that if 
 fails, its 
 will
label
p
ParseError
"somehow incorporate" 
. What does this mean exactly? Well, we could just
msg
assume 
 and that the returned 
type ParseError = String
ParseError
will 
 the label. The problem with this is that we'd like our parse error to also
equal
tell us 
 the problem occurred. Let's tentatively add this to our algebra:
where
case class Location(input: String, offset: Int = 0) {
  lazy val line = input.slice(0,offset+1).count(_ == '\n') + 1
  lazy val col = input.slice(0,offset+1).reverse.indexOf('\n')
}
def errorLocation(e: ParseError): Location
def errorMessage(e: ParseError): String
We've picked a concrete representation for 
 here that includes the
Location
full input, an offset into this input, and the line and column numbers (which can be
computed, lazily, derived from the full input and offset). We can now say more
precisely what we expect from 
—in the event of failure with 
, 
label
Left(e)
 will equal the message set by 
. This can be specified
errorMessage(e)
label
with a 
 if we like:
Prop
def labelLaw[A](p: Parser[A], inputs: SGen[String]): Prop =
  forAll(inputs ** Gen.string) { case (input, msg) =>   
    run(label(msg)(p))(input) match {
      case Left(e) => errorMessage(e) == msg
A POSSIBLE DESIGN
159
www.it-ebooks.info

      case _ => true
    }
  }
What about the 
? We would like for this to be filled in by the 
Location
 implementation with the location where the error occurred. This notion
Parsers
is still a bit fuzzy—if we have 
 and both parsers fail on the input, which
a or b
location is reported, and which label(s)? We will discuss this in the next section.
Is the 
 combinator sufficient for all our error reporting needs? Not quite.
label
Let's look at an example:
val p = label("first magic word")("abra") **
        " ".many **
        label("second magic word")("cadabra")
Skip whitespace
What sort of 
 would we like to get back from 
ParseError
run(p)("abra
? (Note the capital 
 in 
) The immediate cause is that
cAdabra")
A
cAdabra
capital 
 instead of the expected lowercase 
. That error will have a location,
'A'
'a'
and it might be nice to report it somehow. But reporting 
 that low-level error
only
would not be very informative, especially if this were part of a large grammar and
the parser were being run on a larger input. We have some more context that would
be useful to know—the immediate error occurred while in the 
 labeled 
Parser
. This is certainly helpful information. Ideally, we
"second magic word"
could be told that while parsing 
, we encountered an
"second magic word"
error due to an unexpected capital 
, which pinpoints the error and gives us the
'A'
context needed to understand it. And we can imagine that perhaps the top-level
parser (  in this case) might be able to provide an even higher-level description of
p
what the parser was doing when it failed (
, say),
"parsing magic spell"
which could also be informative.
So, it seems wrong to assume that one level of error reporting will always be
sufficient. Let's therefore provide a way to 
 labels:
nest
def scope[A](msg: String)(p: Parser[A]): Parser[A]
ERROR NESTING
160
www.it-ebooks.info

Unlike 
, 
 does not throw away the label(s) attached to —it
label scope
p
merely adds additional information in the event that  fails. Let's specify what this
p
means exactly. First, we modify the functions that pull information out of a 
. Rather than containing just a single 
 and 
ParseError
Location
String
message, we should get a 
:
List[(Location,String)]
def errorStack(e: ParseError): List[(Location,String)]
This is a stack of error messages indicating what the 
 was doing when
Parser
it failed. We can now specify what 
 does—if 
 is 
,
scope
run(p)(s)
Left(e)
then 
 is 
, where 
 will
run(scope(msg)(p))
Left(e2)
errorStack(e2)
have at the top of the stack the message 
, followed by any messages added by 
msg
 itself.
p
We can take this one step further. A stack does not fully capture what the parser
was doing at the time it failed. Consider the parser scope("abc")(a or b
. If , , and  all fail, which error goes at the top of the stack? We could
or c)
a b
c
adopt some global convention, like always reporting the last parser's error (in this
case ) or perhaps reporting whichever parser examined more of the input, but it
c
might be nice to allow the implementation to return all the errors if it chooses:
case class ParseError(stack: List[(Location,String)] = List(),
                      otherFailures: List[ParseError] = List())
This is a somewhat unusual data structure—we have 
, the current stack,
stack
but also a list of other failures (
) that occurred previously in a
otherFailures
chain of 
 combinators.
 This is potentially a lot of information, capturing not
or
13
only the current path in the grammar, but also all the previous failing paths. We
can write helper functions later to make constructing and manipulating 
 values more convenient and to deal with formatting them nicely for
ParseError
human consumption. For now, our concern is just making sure it contains all the
potentially relevant information for error reporting, and it seems like 
 will be more than sufficient. Let's go ahead and pick this as our
ParseError
concrete representation. We can remove the type parameter from 
:
Parsers
161
www.it-ebooks.info

Footnote 13mWe could also represent 
 as a trie in which shared prefixes of the error stack are
ParseError
not duplicated, at a cost of having more expensive inserts. It is easier to recover this sharing information during
formatting of errors, which happens only once.
trait Parsers[Parser[+_]] {
  def run[A](p: Parser[A])(input: String): Either[ParseError,A]
  ...
}
Now we are giving the 
 implementation all the information it needs
Parsers
to construct nice, hierarchical errors if it chooses. As a user of 
, we will
Parsers
judiciously sprinkle our grammar with 
 and 
 calls which the 
label
scope
 implementation can use when constructing parse errors. Note that it
Parsers
would be perfectly reasonable for implementations of 
 to not use the full
Parsers
power of 
 and retain only basic information about the cause and
ParseError
location of errors.14
Footnote 14mWe may want to explicitly acknowledge this by relaxing the laws specified for Parsers
implementations, or making certain laws optional.
There is one last concern regarding error reporting that we need to address. As we
just discussed, when we have an error that occurs inside an 
 combinator, we
or
need some way of determining which error(s) to report. We don't want to 
 have
only
a global convention for this, we sometimes want to allow the programmer to
control this choice. Let's look at a more concrete motivating example:
val spaces = " ".many
val p1 = scope("magic spell") {
  "abra" ** spaces ** "cadabra"
}
val p2 = scope("jibberish") {
  "abba" ** spaces ** "babba"
}
val p = p1 or p2
What 
 would we like to get back from 
ParseError
run(p)("abra
? (Again, note the capital  in 
.) Both branches of the 
cAdabra")
A
cAdabra
or
will produce errors on the input. The 
-labeled parser will report an
"jibberish"
error due to expecting the first word to be 
, and the 
"abba"
"magic spell"
CONTROLLING BRANCHING AND BACKTRACKING
162
www.it-ebooks.info

parser will report an error due to the accidental capitalization in 
.
"cAdabra"
Which of these errors do we want to report back to the user?
In this instance, we happen to want the 
 parse error—after
"magic spell"
successfully parsing the 
 word, we are 
 to the 
"abra"
committed
"magic
 branch of the 
, which means if we encounter a parse error we do not
spell"
or
examine the next branch of the 
. In other instances, we may want to allow the
or
parser to consider the next branch of the 
.
or
So, it appears we need a primitive for letting the programmer indicate when to
commit to a particular parsing branch. Recall that we loosely assigned p1 or p2
to mean try running p1 on the input, then try running p2 on the same input if p1
. We can change its meaning to: 
fails
try running p1 on the input, and if it fails in
an uncommitted state, try running p2 on the same input, otherwise report the
. This is useful for more than just providing good error messages—it also
failure
improves efficiency by letting the implementation avoid lots of possible parsing
branches.
One common solution to this problem is to have all parsers 
 if
commit by default
they examine at least one character to produce a result.
 We then introduce a
15
combinator, 
, which delays committing to a parse:
attempt
Footnote 15mThough see the chapter notes for more discussion of this.
def attempt[A](p: Parser[A]): Parser[A]
It should satisfy something like:16
Footnote 16mThis is not quite an equality. Even though we want to run 
 if the attempted parser fails, we
p2
may want 
 to somehow incorporate the errors from both branches if it fails.
p2
attempt(p flatMap (_ => fail)) or p2 == p2
Where 
 is a parser that always fails (we could introduce this as a primitive
fail
combinator if we like). That is, even if  fails midway through examining the
p
input, 
 reverts the commit to that parse and allows 
 to be run. The 
attempt
p2
 combinator can be used whenever there is ambiguity in the grammar
attempt
and multiple tokens may have to be examined before the ambiguity can be resolved
and parsing can commit to a single branch. As an example, we might write:
163
www.it-ebooks.info

(attempt("abra" ** spaces ** "abra") ** "cadabra") or (
 "abra" ** spaces "cadabra!")
Suppose this parser is run on 
—after parsing the first 
"abra cadabra!"
, we don't know whether to expect another 
 (the first branch) or 
"abra"
"abra"
 (the second branch). By wrapping an 
 around 
"cadabra!"
attempt
"abra"
, we allow the second branch to be considered up until
** spaces ** "abra"
we have finished parsing the second 
, at which point we commit to that
"abra"
branch.
EXERCISE 12: Can you think of any other primitives that might be useful for
letting the programmer specify what error(s) in an 
 chain get reported?
or
Note that we still haven't written an implementation of our algebra! But this
exercise has been more about making sure our combinators provide a way for users
of our library to convey the right information to the implementation. It is up to the
implementation to figure out how to use this information in a way that satisfies the
laws we've stipulated.
Let's write that JSON parser now, shall we? We don't have an implementation of
our algebra yet, but that doesn't actually matter! Our JSON parser doesn't need to
know the internal details of how parsers are represented. It will be constructing a
parser purely using the set of primitives we've defined and any derived
combinators. Of course, we will not actually be able to run our parser until we have
a concrete implementation of the 
 interface.
Parsers
Recall that we have built up the following set of primitives:
string(s): Recognizes and returns a single String
slice(p): Returns the portion of input inspected by  if successful
p
succeed(a): Always succeed with the value a
label(e)(p): In the event of failure, replace the assigned message with e
scope(e)(p): In the event of failure, add  to the error stack returned by 
e
p
seq(p)(f): Run a parser, then use its result to select a second parser to run in sequence
attempt(p): Delays committing to  until after it succeeds
p
or(p1,p2): Chooses between two parsers, first attempting 
, then 
 if 
 fails in an
p1
p2
p1
uncommitted state on the input
We've used these primitives to define a number of combinators like 
, 
map map2
, 
, 
, and 
.
flatMap many
many1
Again, we will be asking 
 to drive the process of writing this parser. After a
you
9.3 Writing a JSON parser
164
www.it-ebooks.info

brief introduction to JSON and the data type we'll use for the parse result, it's up to
you to go off on your own to write the parser.
If you aren't already familiar with the JSON format, you may want to read the 
 and the 
. Here's an example
Wikipedia page description
grammar specification
JSON document:
{
  "Company name" : "Microsoft Corporation",
  "Ticker"  : "MSFT",
  "Active"  : true,
  "Price"   : 30.66,
  "Shares outstanding" : 8.38e9,
  "Related companies" :
    [ "HPQ", "IBM", "YHOO", "DELL", "GOOG" ]
}
A 
 in JSON can be one of several types. An 
 in JSON is a 
value
object
{}
-wrapped, comma separated sequence of key-value pairs. The keys must be strings
like 
 or 
, and the values can be either another object, an 
"Ticker"
"Price"
 like 
 which contains further values, or a 
array
["HPQ", "IBM" ... ]
literal
like 
, 
, 
, or 
.
"MSFT" true null
30.66
We are going to write a rather dumb parser that simply parses a syntax tree
from the document without doing any further processing.
 We'll need a
17
representation for a parsed JSON document. Let's introduce a data type for this:
Footnote 17mSee the chapter notes for discussion of alternate approaches.
trait JSON
object JSON {
  case object JNull extends JSON
  case class JNumber(get: Double) extends JSON
  case class JString(get: String) extends JSON
  case class JBool(get: Boolean) extends JSON
  case class JArray(get: IndexedSeq[JSON]) extends JSON
  case class JObject(get: Map[String, JSON]) extends JSON
}
The task is two write a 
. We want this to work for whatever
Parser[JSON]
the chosen 
 implementation. To allow this, we can place the
Parsers
implementation in a regular function that takes a 
 value as its argument:
Parsers
9.3.1 The JSON format
165
www.it-ebooks.info

18
Footnote 18mUnfortunately, we have to mention the covariance of the 
 type constructor again, rather
Parser
than inferring the variance from the definition of 
.
Parsers
def jsonParser[Parser[+_]](P: Parsers[Parser]): Parser[JSON] = {
  import P._
  val spaces = char(' ').many.slice
  ...
}
gives access to all the combinators
EXERCISE 13 (hard): At this point, 
 are going to take over the process.
you
You will be creating a 
 from scratch using the primitives we have
Parser[JSON]
defined. You don't need to worry (yet) about the representation of 
. As
Parser
you go, you will undoubtedly discover additional combinators and idioms, notice
and factor out common patterns, and so on. Use the skills and knowledge you've
been developing throughout this book, and have fun! If you get stuck, you can
always consult the answers.
Here is some minimal guidance:
Any general purpose combinators you discover can be added to the 
 
Parsers trait
directly.
You will probably want to introduce combinators that make it easier to parse the tokens
of the JSON format (like string literals and numbers). For this you could use the regex
primitive we introduced earlier. You could also add a few primitives like 
, 
, 
letter digit
, and so on, for building up your token parsers.
whitespace
Consult the hints if you'd like a bit more guidance.
EXERCISE 14 (hard): You are now (hopefully) finished with your 
. Aren't you curious to try testing it? Now it's finally time to
Parser[JSON]
come up with a representation for 
 and implement the 
 interface
Parser
Parsers
using this representation. This is a very open-ended design task, but the algebra we
have designed places very strong constraints on possible representations. You
should be able to come up with a simple, purely functional representation of 
 that can be used to implement the 
 interface.
Parser
Parsers
19
Footnote 19mNote that if you try running your JSON parser once you have an implementation of 
,
Parsers
you may get a stack overflow error. See the end of the next section for a discussion of this.
Your code will likely look something like this:
166
www.it-ebooks.info

class MyParser[+A](...) { ... }
object MyParsers extends Parsers[MyParser] {
  // implementations of primitives go here
}
Replace 
 with whatever data type you use for representing your
MyParser
parsers. When you have something you are satisfied with, get stuck, or want some
more ideas, keep reading.
In the next section, we will work through an implementation of our Parsers
interface. Here, we are going to spend some time refining the laws for just two of
our combinators, 
 and 
. We've introduced these combinators
flatMap
succeed
several times before (though 
 was previously called 
), for different
succeed
unit
data types. This is more than a coincidence: besides having similar type signatures,
these combinators give rise to laws that are 
, something
ubiquitous in programming
we will explore further in part 3 of this book when abstracting over the combinator
libraries we have written. This section can be safely skipped but we are hoping to
pique your curiosity a bit more in preparation for part 3. Don't worry if you don't
follow everything here or it feels very abstract—we will be discussing this much
more extensively in part 3.
Earlier, as an exercise, we asked the question: is 
 associative? That is, do we
or
expect that 
 is equal to 
? As designers of the
(a or b) or c
a or (b or c)
API, we get to choose whatever laws we wish—let's choose to make the answer 
. Associativity is a nice property to expect of operations in an algebra. If 
 is
yes
or
associative, it means we do not need to have knowledge of a parser's context (what
parser it is grouped with) to understand its behavior.
The associativity of 
 makes us wonder: is there a kind of associativity law
or
for 
? Yes, there is. The formulation is usually not given directly in terms
flatMap
of 
. The problem with 
 is the "shape" of the two sides is
flatMap
flatMap
different:
def flatMap[A,B](p: Parser[A])(f: A => Parser[B]): Parser[B]
On the left side we have a 
, but on the right side we have a
Parser[A]
function. The expression 
 would not even be
flatMap(a,flatMap(b,c))
9.4 Refining the laws
167
www.it-ebooks.info

well-typed. However, there is a combinator, let's call it 
, that is equivalent in
seq
expressiveness to 
, which has a function on 
 sides:
flatMap
both
def seq[U,A,B](f: U => Parser[A])(g: A => Parser[B]): U => Parser[B]
This might seem like a rather odd-looking combinator, but it can be used to
express 
, if we substitute 
 in for :
flatMap
Unit
U 20
Footnote 20mRecall that the type 
 has just a single value, written 
. That is, 
.
Unit
()
val u: Unit = ()
def flatMap[A,B](p: Parser[A])(f: A => Parser[B]): Parser[B] =
  seq((u: Unit) => p)(f)(())
The key insight here was that 
 could be converted to 
Unit => Parser[A]
 and vice versa.
Parser[A]
EXERCISE 15: To complete the demonstration that 
 and 
 are
seq
flatMap
equivalent in expressiveness, define 
 in terms of 
.
seq
flatMap
Now that we have an operation with the same shape on both sides, we can ask
whether it should be associative as well. That is, do we expect the following:
seq(seq(f, g), h) == seq(f, seq(g, h))
EXERCISE 16 (hard): That answer is again, 
, but can you see why? Think
yes
about this law and write down an explanation in your own words of why it should
hold, and what it means for a 
. We will discuss this more in chapter 11.
Parser
EXERCISE 17 (hard): Come up with a law to specify the relationship between 
 and 
.
seq
succeed
EXERCISE 18: The 
 and 
 laws specified are quite common and
seq
succeed
arise in many different guises. Choose a data type, such as 
 or 
.
Option
List
Define functions analogous to 
 and 
 for this type and show that the
seq
succeed
implementations satisfy the same laws. We will discuss this connection further in
chapter 11.
EXERCISE 19 (hard, optional): We can define a function, 
, analogous to 
kor
 but using 
 to combine the results of the two functions:
seq
or
def kor[A,B](f: A => Parser[B], g: A => Parser[B]): A => Parser[B] =
168
www.it-ebooks.info

  a => f(a) or g(a)
Can you think of any laws to specify the relationship between 
 and 
?
seq
kor
We are now going to discuss an implementation of 
. Our parsing algebra
Parsers
supports a lot of features. Rather than jumping right to the final representation of 
, we will build it up gradually by inspecting the primitives of the algebra
Parser
and reasoning about the information that will be required to support each one.
Let's begin with the 
 combinator:
string
def string(s: String): Parser[A]
We know we need to support the function 
:
run
def run[A](p: Parser[A])(input: String): Either[ParseError,A]
As a first guess, we can assume that our 
  simply the implementation
Parser is
of the 
 function:
run
type Parser[+A] = String => Either[ParseError,A]
We could certainly use this to implement 
:
string
def string(s: String): Parser[A] =
  (input: String) =>
    if (input.startsWith(s))
      Right(s)
    else
      Left(Location(input).toError("Expected: " + s))
The 
 branch has to build up a 
, which are a little
else
ParseError
inconvenient to construct right now, so we've introduced a helper function on 
 for it, 
:
Location
toError
def toError(msg: String): ParseError =
  ParseError(List((this, msg)))
9.5 Implementing the algebra
169
www.it-ebooks.info

So far so good. We have a representation for 
 that at least supports 
Parser
. Let's move on to sequencing of parsers. Unfortunately, to represent a
string
parser like 
, our existing representation is not going to
"abra" ** "cadabra"
be sufficient. If the parse of 
 is successful, then we want to consider those
"abra"
characters 
 and run the 
 parser on the remaining characters.
consumed
"cadabra"
So in order to support sequencing, we require a way of letting a 
 indicate
Parser
how many characters it consumed. Let's try capturing this:21
Footnote 21mRecall that 
 contains the full input string and an offset into this string.
Location
type Parser[+A] = Location => Result[A]
trait Result[+A]
case class Success[+A](get: A, charsConsumed: Int) extends Result[A]
case class Failure(get: ParseError) extends Result[Nothing]
We introduced a new type here, 
, rather than just using 
. In
Result
Either
the event of success, we return a value of type 
 as well as the number of
A
characters of input consumed, which can be used by the caller to update the 
 state.
. This type is starting to get at the essence of what a 
Location
22
Parser
is—it is a kind of state action that can fail, similar to what we built in chapter 6.23
It receives an input state, and if successful returns a value as well enough
information to control how the state should be updated.
Footnote 22mNote that returning an 
 would give 
 the ability to set the 
,
(A,Location)
Parser
input
which is granting it too much power.
Footnote 23mThe precise relationship between these two types will be further explored in part 3 when we
discuss what are called 
.
monad transformers
This understanding gives us a way of framing how to build a fleshed out
representation supporting all the fancy combinators and laws we have stipulated.
We simply consider what each primitive requires that we track in our state type
(just a 
 may not be sufficient), and work through the details of how
Location
each combinator transforms this state.
Let's again recall our set of primitives:
string(s): Recognizes and returns a single String
slice(p): Returns the portion of input inspected by  if successful
p
label(e)(p): In the event of failure, replace the assigned message with e
scope(e)(p): In the event of failure, add  to the error stack returned by 
e
p
170
www.it-ebooks.info

flatMap(p)(f): Run a parser, then use its result to select a second parser to run in
sequence
attempt(p): Delays committing to  until after it succeeds
p
or(p1,p2): Chooses between two parsers, first attempting 
, then 
 if 
 fails in an
p1
p2
p1
uncommitted state on the input
EXERCISE 20: Implement 
, 
, 
, and 
 for
string succeed slice
flatMap
this initial representation of 
. Notice that 
 is a bit less efficient than
Parser
slice
it could be, since it must still construct a value only to discard it. We will return to
this later.
Let's look at 
 next. In the event of failure, we want to push a new
scope
message onto the 
 stack. Let's introduce a helper function for this on
ParseError
, we'll call it 
:
ParseError
push 24
Footnote 24mThe 
 method comes for free with a 
. It returns an updated copy of the object
copy
case class
with one or more arguments replaced, using the usual default argument mechanism in Scala. If no new value is
specified for a field, it will have the same value as in the original object.
def push(loc: Location, msg: String): ParseError =
  copy(stack = (loc,msg) :: stack)
With this we can implement 
:
scope
def scope[A](msg: String)(p: Parser[A]): Parser[A] =
  s => p(s).mapError(_.push(s.loc,msg))
The function 
 is defined on 
—it just applies a function to
mapError
Result
the failing case:
def mapError(f: ParseError => ParseError): Result[A] = this match {
  case Failure(e,c) => Failure(f(e),c)
  case _ => this
}
Because we push onto the stack after the inner parser has returned, the bottom
of the stack will have more detailed messages that occurred later in parsing.
(Consider the 
 that will result if 
ParseError
scope(msg1)(a **
 fails while parsing .)
scope(msg2)(b))
b
And we can implement 
 similarly. In the event of failure, we want to
label
171
www.it-ebooks.info

replace the failure message. We can write this again using 
:
mapError
def label[A](msg: String)(p: Parser[A]): Parser[A] =
  s => p(s).mapError(_.label(msg))
We added a helper function to 
, also called 
. We will
ParseError
label
make a design decision that 
 trims the error stack, cutting off more detailed
label
messages from inner scopes, using only the most recent location from the bottom
of the stack:
def label[A](s: String): ParseError =
  ParseError(latestLoc.map((_,s)).toList,
             otherFailures map (_.label(s)))
def latest: Option[(Location,String)] =
  stack.lastOption
def latestLoc: Option[Location] =
  latest map (_._1)
The 
 combinator can be used to implement 
, which prunes
latest
furthest
a 
 to the error that occurred after consuming the most characters.
ParseError
We can use this to implement the 
 combinator for parsers:
furthest
def furthest: ParseError =
  copy(otherFailures = List()) ::
  otherFailures maxBy (_.latest.map(_._1.offset))
def furthest[A](p: Parser[A]): Parser[A] =
  s => p(s).mapError(_.furthest)
EXERCISE 21: Revise your implementation of 
 and 
 to
string
flatMap
work with the new representation of 
.
Parser
Let's now consider 
 and 
. Recall what we specified for the
or
attempt
expected behavior of 
: it should run the first parser, and if it fails 
or
in an
, it should run the second parser on the same input. We said that
uncommitted state
consuming at least one character should result in a committed parse, and that 
 converts committed failures of  to uncommitted failures.
attempt(p)
p
172
www.it-ebooks.info

We can support the behavior we want by adding one more piece of information
to the 
 case of 
—a 
 value indicating whether the
Failure
Result
Boolean
parser failed in a committed state:
case class Failure(get: ParseError,
                   isAttempted: Boolean) extends Result[Nothing]
The implementation of 
 just 'uncommits' any failures that occur. It
attempt
uses a helper function, 
, which we can define on 
:
uncommit
Result
def attempt[A](p: Parser[A]): Parser[A] =
  s => p(s).uncommit
def uncommit: Result[A] = this match {
  case Failure(e,true) => Failure(e,false)
  case _ => this
}
And the implementation of 
 simply checks the 
 flag before
or
isCommitted
running the second parser—if the first parser fails in a committed state, then we
skip running the second parser. Again we first introduce a helper function on 
 to accumulate a failure into the 
 list:
ParseError
otherFailures
def addFailure(e: ParseError): ParseError =
  this.copy(otherFailures = e :: this.otherFailures)
def or[A](p: Parser[A], p2: => Parser[A]): Parser[A] =
  s => p(s) match {
    case r@Failure(e,committed) if committed =>
      p2(s).mapError(_.addFailure(e))
    case r => r
  }
committed failure or success skips running p2
What about 
? The implementation is simple, we just advance the
flatMap
location before calling the second parser. Again we use a helper function, 
173
www.it-ebooks.info

, on 
. There is one subtlety—if the first parser consumes
advanceBy
Location
any characters, we ensure that the second parser is committed, using a helper
function, 
 on 
:
addCommit
ParseError
def flatMap[A,B](f: Parser[A])(g: A => Parser[B]): Parser[B] =
  s => f(s) match {
    case Success(a,n) => g(a)(s.advanceBy(n)).addCommit(n == 0)
    case f@Failure(_,_) => f
  }
advanceBy has the obvious implementation:
def advanceBy(n: Int): Location =
  copy(offset = offset+n)
Likewise, 
, defined on 
, is straightforward:
addCommit
ParseError
def addCommit(isCommitted: Boolean): Result[A] = this match {
  case Failure(e,false) if isCommitted => Failure(e, true)
  case _ => this
}
EXERCISE 22: Implement the rest of the primitives, including 
 using this
run
representation of 
 and try running your JSON parser on various inputs.
Parser
25
Footnote 25mYou will find, unfortunately, that it stack overflows for large inputs (for instance, 
). One simple solution to this is to provide a specialized implementation of 
 that
[1,2,3,...10000]
many
avoids using a stack frame for each element of the list being built up. So long as any combinators that do repetition
are defined in terms of 
 (which they all can be), this solves the problem. See the answers for discussion
many
of more general approaches.
EXERCISE 23: Come up with a nice way of formatting a 
 for
ParseError
human consumption. There are a lot of choices to make, but a key insight is that we
typically want to combine or group labels attached to the same location when
presenting the error as a 
 for display.
String
EXERCISE 24 (hard, optional): The 
 combinator is still less efficient
slice
than it could be. For instance, 
 will still build up a 
many(char('a')).slice
 only to discard it. Can you think of a way of modifying the 
List[Char]
 representation to make slicing more efficient?
Parser
174
www.it-ebooks.info

algebra
algebraic design
backtracking
combinator parsing
commit
laws
parser combinators
In this chapter, we introduced an approach to writing combinator libraries called
algebraic design, and used it to design a parser combinator library and implement a
JSON parser. Along the way, we discovered a number of very similar combinators
to previous chapters, which again were related by familiar laws. In part 3, we will
finally understand the nature of the connection between these libraries and learn
how to abstract over their common structure.
This is the final chapter in part 2. We hope you have come away from these
chapters with a basic sense for how functional design can proceed, and more
importantly, we hope these chapters have motivated you to try your hand at 
, for whatever domains interest 
.
designing your own functional libraries
you
Functional design is not something reserved only for FP experts—it should be part
of the day-to-day work done by functional programmers at all experience levels.
Before starting on part 3, we encourage you to venture beyond this book and try
writing some more functional code and designing some of your own libraries.
Have fun, enjoy struggling with design problems that come up, and see what you
discover. Next we will begin to explore the universe of patterns and abstractions
which the chapters so far have only hinted at.
Index Terms
9.6 Conclusion
175
www.it-ebooks.info

10
By the end of Part 2, we were getting comfortable with considering data types in
terms of their 
—that is, the operations they support and the laws that
algebras
govern those operations. Hopefully you will have noticed that the algebras of very
different data types tend to follow certain patterns that they have in common. In
this chapter, we are going to begin identifying these patterns and taking advantage
of them. We will consider algebras in the abstract, by writing code that doesn't just
operate on one data type or another but on 
 data types that share a common
all
algebra.
The first such abstraction that we will introduce is the 
. We choose to
monoid1
start with monoids because they are very simple and because they are ubiquitous.
Monoids come up all the time in everyday programming, whether we're aware of
them or not. Whenever you are working with a list or concatenating strings or
accumulating the result of a loop, you are almost certainly using a monoid.
Footnote 1mThe name "monoid" comes from mathematics. The prefix "mon-" means "one", and in category
theory a monoid is a category with one object. See the chapter notes for more information.
Let's consider the algebra of string concatenation. We can add "foo" + "bar"
to get 
, and the empty string is an 
 for that operation.
"foobar"
identity element
That is, if we say 
 or 
, the result is always . Furthermore,
(s + "")
("" + s)
s
if we combine three strings by saying 
, the operation is 
(r + s + t)
associative
—it doesn't matter whether we parenthesize it 
 or 
((r + s) + t)
(r + (s +
.
t))
The exact same rules govern integer addition. It's associative and it has an
identity element, , which "does nothing" when added to another integer. Ditto for
0
Monoids
10.1 What is a monoid?
176
www.it-ebooks.info

multiplication, whose identity element is .
1
The Boolean operators, 
 and 
 are likewise associative, and they have
&&
||
identity elements 
 and 
, respectively.
true
false
These are just a few simple examples, but algebras like this are virtually
everywhere. The term for this kind of algebra is 
. The laws of
"monoid"
associativity and identity are collectively called the 
. A monoid
monoid laws
consists of:
Some type A
A binary associative operation that takes two values of type  and combines them into
A
one.
A value of type  that is an identity for that operation.
A
We can express this with a Scala trait:
trait Monoid[A] {
  def op(a1: A, a2: A): A
  def zero: A
}
An example instance of this trait is the 
 monoid:
String
val stringMonoid = new Monoid[String] {
  def op(a1: String, a2: String) = a1 + a2
  def zero = ""
}
List concatenation also forms a monoid:
def listMonoid[A] = new Monoid[List[A]] {
  def op(a1: List[A], a2: List[A]) = a1 ++ a2
  def zero = Nil
}
EXERCISE 1: Give 
 instances for integer addition and multiplication
Monoid
as well as the Boolean operators.
val intAddition: Monoid[Int]
val intMultiplication: Monoid[Int]
val booleanOr: Monoid[Boolean]
val booleanAnd: Monoid[Boolean]
177
www.it-ebooks.info

EXERCISE 2: Give a 
 instance for combining 
s:
Monoid
Option
def optionMonoid[A]: Monoid[Option[A]]
EXERCISE 3: A function having the same argument and return type is called
an 
. Write a monoid for endofunctions:
endofunction2
def EndoMonoid[A]: Monoid[A => A]
Footnote 2mThe Greek prefix "endo-" means "within", in the sense that an endofunction's codomain is within
its domain.
EXERCISE 4: Use the property-based testing framework we developed in Part
2 to implement a property for the monoid laws. Use your property to test the
monoids we have written.
val monoidLaws[A](m: Monoid[A]): Prop
SIDEBAR
Having vs. being a monoid
There is a slight terminology mismatch between programmers and
mathematicians, when they talk about a type 
 a monoid as against
being
 a monoid instance. As a programmer, it's natural to think of the
having
instance of type 
 as being 
. But that is not
Monoid[A]
a monoid
accurate terminology. The monoid is actually both things—the type
together with the instance. When we say that a method accepts a value
of type 
, we don't say that it takes a monoid, but that it
Monoid[A]
takes 
 that the type  is a monoid.
evidence
A
Just what 
 a monoid, then? It is simply an implementation of an interface
is
governed by some laws. Stated tersely, a monoid is a type together with an
.
associative binary operation (
) which has an identity element (
)
op
zero
What does this buy us? Can we write any interesting programs over 
 data
any
type, knowing nothing about that type other than that it's a monoid? Absolutely!
Let's look at an example.
178
www.it-ebooks.info

Monoids have an intimate connection with lists. If you look at the signatures for 
 and 
 on 
, you might notice something about the
foldLeft
foldRight
List
argument types.
def foldRight[B](z: B)(f: (A, B) => B): B
def foldLeft[B](z: B)(f: (B, A) => B): B
What happens when  and  are the same type?
A
B
def foldRight(z: A)(f: (A, A) => A): A
def foldLeft(z: A)(f: (A, A) => A): A
The components of a monoid fit these argument types like a glove. So if we had
a list of 
s, we could simply pass the 
 and 
 of the 
String
op
zero
 in order to reduce the list with the monoid.
stringMonoid
scala> val words = List("Hic", "Est", "Index")
words: List[String] = List(Hic, Est, Index)
scala> val s = words.foldRight(stringMonoid.zero)(stringMonoid.op)
s: String = "HicEstIndex"
scala> val t = words.foldLeft(stringMonoid.zero)(stringMonoid.op)
t: String = "HicEstIndex"
Notice that it doesn't matter if we choose 
 or 
 when
foldLeft
foldRight
folding with a monoid —we should get the same result. This is precisely because
3
the laws of associativity and identity hold. A left fold associates operations to the
left while a right fold associates to the right, with the identity element on the left
and right respectively:
Footnote 3mAs of this writing, with Scala at version 2.9.2, the implementation of 
 in the standard
foldRight
library is not tail-recursive, so for large lists it matters operationally which we choose since they have different
memory usage characteristics.
words.foldLeft("")(_ + _)  == (("" + "Hic") + "Est") + "Index"
words.foldRight("")(_ + _) == "Hic" + ("Est" + ("Index" + ""))
10.2 Folding lists with monoids
179
www.it-ebooks.info

EXERCISE 5: Write a monoid instance for 
 that inserts spaces
String
between words unless there already is one, and trims spaces off the ends of the
result. For example:
op("Hic", op("est ", "chorda ")) == "Hic est chorda"
op("Hic ", op(" est"), "chorda") == "Hic est chorda"
def wordsMonoid(s: String): Monoid[String]
EXERCISE 6: Implement 
, a function that folds a list with a
concatenate
monoid:
def concatenate[A](as: List[A], m: Monoid[A]): A
But what if our list has an element type that doesn't have a 
 instance?
Monoid
Well, we can always 
 over the list to turn it into a type that does.
map
def foldMap[A,B](as: List[A], m: Monoid[B])(f: A => B): B
EXERCISE 7: Write this function.
EXERCISE 8 (hard): The 
 function can be implemented using either 
foldMap
 or 
. But you can also write 
 and 
foldLeft
foldRight
foldLeft
foldRight
using 
! Try it.
foldMap
The fact that a monoid's operation is associative means that we have a great deal of
flexibility in how we fold a data structure like a list. We have already seen that
operations can be associated to the left or right to reduce a list sequentially with 
 and 
. But we could instead split the data into chunks, fold
foldLeft
foldRight
them 
, and then combine the chunks with the monoid. Folding to the
in parallel
right, the combination of chunks , , , and  would look like this:
a b c
d
op(a, op(b, op(c, d)))
Folding to the left would look like this:
10.3 Associativity and parallelism
180
www.it-ebooks.info

op(op(op(a, b), c), d)
But folding in parallel looks like this:
op(op(a, b), op(c, d))
This parallelism might give us some efficiency gains, because the two inner op
s could be run simultaneously in separate threads.
As a nontrivial use case, let's say that we wanted to count the number of words in a
. This is a fairly simple parsing problem. We could scan the string
String
character by character, looking for whitespace and counting up the number of runs
of consecutive non-whitespace characters. Parsing sequentially like that, the parser
state could be as simple as tracking whether the last character seen was a
whitespace.
But imagine doing this not for just a short string, but an enormous text file. It
would be nice if we could work with chunks of the file in parallel. The strategy
would be to split the file into manageable chunks, process several chunks in
parallel, and then combine the results. In that case, the parser state needs to be
slightly more complicated, and we need to be able to combine intermediate results
regardless of whether the section we're looking at is at the beginning, end, or
middle of the file. In other words, we want that combination to be associative.
To keep things simple and concrete, let's consider a short string and pretend it's
a large file:
"lorem ipsum dolor sit amet, "
If we split this string roughly in half, we might split it in the middle of a word.
In the case of our string above, that would yield 
 and 
"lorem ipsum do"
"lor
. When we add up the results of counting the words in these
sit amet, "
strings, we want to avoid double-counting the word 
. Clearly, just counting
dolor
the words as an 
 is not sufficient. We need to find a data structure that can
Int
handle partial results like the half words 
 and 
, and can track the complete
do
lor
words seen so far, like 
, 
, and 
.
ipsum sit
amet
The partial result of the word count could be represented by an algebraic data
10.3.1 Example: Parallel parsing
181
www.it-ebooks.info

type:
sealed trait WC
case class Stub(chars: String) extends WC
case class Part(lStub: String, words: Int, rStub: String) extends WC
A 
 is the simplest case, where we have not seen any complete words yet.
Stub
But a 
 keeps the number of complete words we have seen so far, in 
.
Part
words
The value 
 holds any partial word we have seen to the left of those words,
lStub
and 
 holds the ones on the right.
rStub
For example, counting over the string 
 would result in 
"lorem ipsum do"
 since there is one complete word. And since there
Part("lorem", 1, "do")
is no whitespace to the left of 
 or right of 
, we can't be sure if they are
lorem
do
complete words, so we can't count them yet. Counting over "lor sit amet,
 would result in 
.
"
Part("lor", 2, "")
EXERCISE 9: Write a monoid instance for 
 and make sure that it meets the
WC
monoid laws.
val wcMonoid: Monoid[WC]
EXERCISE 10: Use the 
 monoid to implement a function that counts words
WC
in a 
 by recursively splitting it into substrings and counting the words in
String
those substrings.
182
www.it-ebooks.info

SIDEBAR
Monoid homomorphisms
If you have your law-discovering cap on while reading this chapter, you
may notice that there is a law that holds for some functions between
monoids. Take the 
 concatenation monoid and the integer
String
addition monoid. If you take the length of two strings and add them up,
it's the same as taking the length of the concatenation of those two
strings:
"foo".length + "bar".length == ("foo" + "bar").length
Here, 
 is a function from 
 to 
 that 
length
String
Int
preserves the
. Such a function is called a 
.
monoid structure
monoid homomorphism4
A monoid homomorphism 
 between monoids 
 and 
 obeys the
f
M
N
following general law for all values  and :
x
y
Footnote 4mThis word comes from Greek, "homo" meaning "same" and
"morphe" meaning "shape".
M.op(f(x), f(y)) == f(N.op(x, y))
The same law holds for the homomorphism from 
 to 
 in the
String
WC
current example.
This property can be very useful when designing your own libraries. If
two types that your library uses are monoids, and there exist functions
between them, it's a good idea to think about whether those functions
are expected to preserve the monoid structure and to check the monoid
homomorphism law with automated tests.
There is a higher-order function that can take any function of type A =>
, where  is a monoid, and turn it in to a monoid homomorphism from 
B
B
 to .
List[A]
B
Sometimes there will be a homomorphism in both directions between
two monoids. Such a relationship is called a 
 ("iso-"
monoid isomorphism
meaning "equal") and we say that the two monoids are isomorphic.
Associativity can also be exploited to gain efficiency. For example, if we want
to concatenate a list of strings, doing so with a left or right fold can be less efficient
than we would like. Consider the memory usage of this expression:
List("lorem", "ipsum", "dolor", "sit").foldLeft("")(_ + _)
183
www.it-ebooks.info

At every step of the fold, we are allocating the full intermediate 
 only
String
to discard it and allocate a larger string in the next step:
List("lorem", ipsum", "dolor", "sit").foldLeft("")(_ + _)
List("ipsum", "dolor", "sit").foldLeft("lorem")(_ + _)
List("dolor", "sit").foldLeft("loremipsum")(_ + _)
List("sit").foldLeft("loremipsumdolor")(_ + _)
List().foldLeft("loremipsumdolorsit")(_ + _)
"loremipsumdolorsit"
A more efficient strategy would be to combine the list by halves. That is, to first
construct 
 and 
, then add those together. But a 
"loremipsum"
"dolorsit"
 is inherently sequential, so there is not an efficient way of splitting it in half.
List
Fortunately, there are other structures that allow much more efficient random
access, such as the standard Scala library's 
 data type which provides very
Vector
efficient 
 and 
 methods.
length
splitAt
EXERCISE 11: Implement an efficient 
 for 
, a
foldMap
IndexedSeq
common supertype for various data structures that provide efficient random access.
def foldMapV[A,B](v: IndexedSeq[A], m: Monoid[B])(f: A => B): B
EXERCISE 12: Use 
 to detect whether a given 
foldMap
 is ordered. You will need to come up with a creative 
IndexedSeq[Int]
 instance.
Monoid
In chapter 3, we implemented the data structures 
 and 
, both of which
List
Tree
could be folded. In chapter 5, we wrote 
, a lazy structure that also can be
Stream
folded much like a 
 can, and now we have just written a fold for 
List
.
IndexedSeq
When we're writing code that needs to process data contained in one of these
structures, we often don't care about the shape of the structure (whether it's a tree
or a list), or whether it's lazy or not, or provides efficient random access, etc.
For example, if we have a structure full of integers and want to calculate their
sum, we can use foldRight:
ints.foldRight(0)(_ + _)
10.4 Foldable data structures
184
www.it-ebooks.info

Looking at just this code snippet, we don't really know the type of 
. It
ints
could be a 
, a 
, or a 
, or anything at all with a 
Vector
Stream
List
foldRight
method. We can capture this commonality in a trait:
trait Foldable[F[_]] {
  def foldRight[A, B](as: F[A])(f: (A, B) => B): B
  def foldLeft[A, B](as: F[A])(f: (B, A) => B): B
  def foldMap[A, B](as: F[A])(f: A => B)(mb: Monoid[B]): B
  def concatenate[A](as: F[A])(m: Monoid[A]): A =
    as.foldLeft(m.zero)(m.op)
}
Here we are abstracting over a type constructor , much like we did with the 
F
 type in the previous chapter. We write it as 
, where the underscore
Parser
F[_]
indicates that  is not a type but a 
 that takes one type argument.
F
type constructor
Just like functions that take other functions as arguments are called higher-order
functions, something like 
 is a 
 or a 
Foldable
higher-order type constructor
.
higher-kinded type5
Footnote 5mJust like values and functions have types, types and type constructors have 
. Scala uses kinds
kinds
to track how many type arguments a type constructor takes, whether it is co- or contravariant in those arguments,
and what their kinds are.
EXERCISE 13: Implement 
, 
Foldable[List] Foldable[IndexedSeq]
, and 
. Rembember that 
, 
 and 
Foldable[Stream]
foldRight foldLeft
 can all be implemented in terms of each other, but that might not be the
foldMap
most efficient implementation.
EXERCISE 14: Recall the 
 data type from Chapter 3. Implement a 
Tree
 instance for it.
Foldable
sealed trait Tree[+A]
case object Leaf[A](value: A) extends Tree[A]
case class Branch[A](left: Tree[A], right: Tree[A]) extends Tree[A]
EXERCISE 15: Write a 
 instance.
Foldable[Option]
EXERCISE 16: Any foldable structure can be turned into a 
. Write this
List
conversion in a generic way:
def toList[A](fa: F[A]): List[A]
185
www.it-ebooks.info

The 
 abstraction by itself is not all that compelling, and with the
Monoid
generalized 
 it's only slightly more interesting. The real power of
foldMap
monoids comes from the fact that they 
.
compose
This means, for example, that if types  and  are monoids, then the tuple type 
A
B
 is also a monoid (called their 
).
(A, B)
product
EXERCISE 17: Prove it.
def productMonoid[A,B](A: Monoid[A], B: Monoid[B]): Monoid[(A,B)]
EXERCISE 18: Do the same with 
. This is called a monoid 
.
Either
coproduct
def coproductMonoid[A,B](A: Monoid[A],
                         B: Monoid[B]): Monoid[Either[A,B]]
Some data structures also have interesting monoids as long as their value types are
monoids. For instance, there is a monoid for merging key-value 
s, as long as
Map
the value type is a monoid.
def mapMergeMonoid[K,V](V: Monoid[V]): Monoid[Map[K, V]] =
  new Monoid[Map[K, V]] {
    def zero = Map()
    def op(a: Map[K, V], b: Map[K, V]) =
      a.map {
        case (k, v) => (k, V.op(v, b.get(k) getOrElse V.zero))
      }
  }
Using these simple combinators, we can assemble more complex monoids
fairly easily:
scala> val M: Monoid[Map[String, Map[String, Int]]] =
     | mapMergeMonoid(mapMergeMonoid(intAddition))
M: Monoid[Map[String, Map[String, Int]]] = $anon$1@21dfac82
This allows us to combine nested expressions using the monoid, with no
10.5 Monoids compose
10.5.1 Assembling more complex monoids
186
www.it-ebooks.info

additional programming:
scala> val m1 = Map("o1" -> Map("i1" -> 1, "i2" -> 2))
m1: Map[String,Map[String,Int]] = Map(o1 -> Map(i1 -> 1, i2 -> 2))
scala> val m2 = Map("o1" -> Map("i2" -> 3))
m2: Map[String,Map[String,Int]] = Map(o1 -> Map(i2 -> 3))
scala> val m3 = M.op(m1, m2)
m3: Map[String,Map[String,Int]] = Map(o1 -> Map(i1 -> 1, i2 -> 5))
EXERCISE 19: Write a monoid instance for functions whose results are
monoids.
def functionMonoid[A,B](B: Monoid[B]): Monoid[A => B]
EXERCISE 20: Use monoids to compute a frequency map of words in an 
 of 
s.
IndexedSeq
String
def frequencyMap(strings: IndexedSeq[String]): Map[String, Int]
A frequency map contains one entry per word, with that word as the key, and
the number of times that word appears as the value under that key. For example:
scala> frequencyMap(Vector("a rose", "is a", "rose is", "a rose"))
res0: Map[String,Int] = Map(a -> 3, rose -> 3, is -> 2)
The fact that multiple monoids can be composed into one means that we can
perform multiple calculations simultaneously when folding a data structure. For
example, we can take the length and sum of a list at the same time in order to
calculate the mean:
scala> val m = productMonoid(intAddition, intAddition)
m: Monoid[(Int, Int)] = $anon$1@8ff557a
scala> val p = listFoldable.foldMap(List(1,2,3,4))(a => (1, a))(m)
p: (Int, Int) = (10, 4)
scala> val mean = p._1 / p._2.toDouble
mean: Double = 2.5
10.5.2 Using composed monoids to fuse traversals
187
www.it-ebooks.info

The ability to compose monoids really comes into its own when you have
complicated nested structures. But it can be somewhat tedious to assemble them by
hand in each case. Fortunately, Scala has a facility that can make monoids a lot
more pleasant to work with by making instances implicit:
implicit val stringMonoid: Monoid[String] = ...
Such implicit instances can then be used to implicitly construct more
complicated instances:
implicit def productMonoid[A](implicit A: Monoid[A]): Monoid[List[A]] =
  ...
If we make all of our monoid instances implicit, a simple function definition
with an implicit argument can automatically assemble arbitrarily complex monoids
for us.
def monoid[A](implicit A: Monoid[A]): Monoid[A] = A
All we have to do is specify the type of the monoid we want. As long as that
type has an implicit 
 instance, it will be returned to us.
Monoid
scala> monoid[Map[List[String], Int => Map[String, List[Boolean]]]]
res0: Monoid[Map[List[String], Int => Map[String, List[Boolean]]]] =
$anon$1@356380e8
But what about types that have more than once 
 instance? For
Monoid
example, a valid monoid for 
 could be either addition (with 0 as the identity)
Int
or multiplication (with 1 as the identity). A common solution is to put values in a
simple wrapper type and make that type the monoid.
case class Product(value: Int)
implicit val productMonoid: Monoid[Product] = new Monoid[Product] {
  def op(a: Product, b: Product) = Product(a.value * b.value)
  def zero = 1
10.5.3 Implicit monoids
188
www.it-ebooks.info

associativity
higher-kinded types
identity element
implicit instances
monoid
monoid coproduct
monoid homomorphism
monoid laws
monoid product
type constructor polymorphism
}
case class Sum(value: Int)
implicit val sumMonoid: Monoid[Sum] = new Monoid[Sum] {
  def op(a: Sum, b: Sum) = Sum(a.value + b.value)
  def zero = 0
}
This plays nicely with 
 on 
. For example if we have a list 
foldMap
Foldable
 of integers and we want to sum them:
ints
listFoldable.foldMap(ints)(Sum(_))
Or to take their product:
listFoldable.foldMap(ints)(Product(_))
In this chapter we introduced the concept of a monoid, a simple and common type
of abstract algebra. When you start looking for it, you will find ample opportunity
to exploit the monoidal structure of your own libraries. The associative property
lets you fold any 
 data type and gives you the flexibility of doing so in
Foldable
parallel. Monoids are also compositional, and therefore they let you write folds in a
declarative and reusable way.
Monoid has been our first totally abstract trait, defined only in terms of its
abstract operations and the laws that govern them. This gave us the ability to write
useful functions that know nothing about their arguments except that their type
happens to be a monoid.
Index Terms
10.6 Summary
189
www.it-ebooks.info

11
In Part 2, we implemented several different combinator libraries. In each case, we
proceeded by writing a small set of primitives and then a number of derived
combinators defined purely in terms of existing combinators. You probably noticed
some similarities between implementations of different derived combinators across
the combinator libraries we wrote. For instance, we implemented a 
 function
map
for each data type, to lift a function taking one argument. For 
, 
, and 
Gen Parser
, the type signatures were as follows:
Option
These type signatures are very similar. The only difference is the concrete type
involved. We can capture, as a Scala trait, the idea of "a data type that implements 
".
map
If a data type  implements 
 with a signature like above, we say that  is a 
F
map
F
. Which is to say that we can provide an implementation of the following
functor
trait:
Monads
def map[A,B](ga: Gen[A])(f: A => B): Gen[B]
def map[A,B](pa: Parser[A])(f: A => B): Parser[B]
def map[A,B](oa: Option[A])(f: A => B): Option[A]
11.1 Functors: Generalizing the map function
trait Functor[F[_]] {
  def map[A,B](fa: F[A])(f: A => B): F[B]
}
190
www.it-ebooks.info

Much like we did with 
 in the previous chapter, we introduce a trait 
Foldable
 that is parameterized on a type constructor 
. Here is an instance
Functor
F[_]
for 
:
List
What can we do with this abstraction? There happens to be a small but useful
library of functions that we can write using just 
. For example, if we have 
map
 where  is a functor, we can "distribute" the  over the pair to get 
F[(A, B)]
F
F
:
(F[A], F[B])
It's all well and good to introduce a new combinator like this in the abstract, but
we should think about what it 
 for concrete data types like 
, 
,
means
Gen Option
etc. For example, if we 
 a 
, we get two lists of the
distribute
List[(A, B)]
same length, one with all the s and the other with all the s. That operation is
A
B
sometimes called "unzip". So we just wrote a generic unzip that works not just for
lists, but for any functor!
Whenever we create an abstraction like this, we should consider not only what
abstract methods it should have, but which 
 we expect should hold for the
laws
implementations. If you remember, back in chapter 7 (on parallelism) we found a
general law for 
:
map
Later in Part 2, we found that this law is not specific to 
. In fact, we would
Par
expect it to hold for all implementations of 
 that are "structure-preserving".
map
This law (and its corollaries given by parametricity) is part of the specification of
what it means to "map". It would be very strange if it didn't hold. It's part of what a
functor .
is
val listFunctor extends Functor[List] {
  def map[A,B](as: List[A])(f: A => B): List[B] = as map f
}
def distribute[A,B](fab: F[(A, B)]): (F[A], F[B]) =
  (map(fab)(_._1), map(fab)(_._2))
map(v)(x => x) == v
191
www.it-ebooks.info

Not only did we implement 
 for many data types, we also implemented a 
map
 to lift a function taking two arguments. For 
, 
, and 
,
map2
Gen Parser
Option
the 
 function could be implemented as follows.
map2
Makes a generator of a random C that runs random generators ga and gb,
combining their results with the function f.
Makes a parser that produces C by combining the results of parsers pa and pb with
the function f.
Combines two Options with the function f, if both have a value. Otherwise returns
None.
These functions have more in common than just the name. In spite of operating
on data types that seemingly have nothing to do with one another, the
implementations are exactly identical! And again the only thing that differs in the
type signatures is the particular data type being operated on. This confirms what
we have been suspecting all along—that these are particular instances of some
more general pattern. We should be able to exploit that fact to avoid repeating
ourselves. For example, we should be able to write 
 once and for all in such a
map2
way that it can be reused for all of these data types.
We've made the code duplication particularly obvious here by choosing
uniform names for our functions, taking the arguments in the same order, and so
on. It may be more difficult to spot in your everyday work. But the more
combinator libraries you write, the better you will get at identifying patterns that
you can factor out into a common abstraction. In this chapter, we will look at an
abstraction that unites 
, 
, 
, 
, and some other data types
Parser Gen Par Option
we have already looked at: They are all 
 We will explain in a moment
monads.
exactly what that means.
11.2 Generalizing the flatMap and unit functions
def map2[A,B,C](
      ga: Gen[A], gb: Gen[B])(f: (A,B) => C): Gen[C] =
  ga flatMap (a => fb map (b => f(a,b)))
def map2[A,B,C](
      pa: Parser[A], pb: Parser[B])(f: (A,B) => C): Parser[C] =
  pa flatMap (a => pb map (b => f(a,b)))
def map2[A,B,C](
      pa: Option[A], pb: Option[B])(f: (A,B) => C): Option[C] =
  pa flatMap (a => pb map (b => f(a,b)))
192
www.it-ebooks.info

In both parts 1 and 2 of this book, we concerned ourselves with finding a minimal
set of primitive operations for various data types. We implemented 
 for many
map2
data types, but as we know by now, 
 can be implemented in terms of 
map2
map
and 
. Is that a minimal set? Well, the data types in question all had a 
flatMap
, and we know that 
 can be implemented in terms of 
 and 
unit
map
flatMap
. For example, on 
:
unit
Gen
So let's pick 
 and 
 as our minimal set. We will unify under a
unit
flatMap
single concept all data types that have these combinators defined. 
 has 
Monad
 and 
 abstract, and provides default implementations for 
 and 
flatMap
unit
map
.
map2
Since Monad provides a default implementation of map, it can extend Functor. All
monads are functors, but not all functors are monads.
SIDEBAR
The name "monad"
We could have called 
 anything at all, like 
 or
Monad
FlatMappable
whatever. But "monad" is already a perfectly good name in common
use. The name comes from category theory, a branch of mathematics
that has inspired a lot of functional programming concepts. The name
"monad" is intentionally similar to "monoid", and the two concepts are
related in a deep way. See the chapter notes for more information.
To tie this back to a concrete data type, we can implement the 
 instance
Monad
for 
:
Gen
11.2.1 The Monad trait
def map[A,B](f: A => B): Gen[B] =
  flatMap(a => unit(f(a)))
trait Monad[M[_]] extends Functor[M] {
  def unit[A](a: => A): M[A]
  def flatMap[A,B](ma: M[A])(f: A => M[B]): M[B]
  def map[A,B](ma: M[A])(f: A => B): M[B] =
    flatMap(ma)(a => unit(f(a)))
  def map2[A,B,C](ma: M[A], mb: M[B])(f: (A, B) => C): M[C] =
    flatMap(ma)(a => map(mb)(b => f(a, b)))
}
193
www.it-ebooks.info

We only need to implement 
 and 
 and we get 
 and 
 at
unit
flatMap
map
map2
no additional cost. We have implemented them once and for all, for any data type
for which it is possible to supply an instance of 
! But we're just getting
Monad
started. There are many more combinators that we can implement once and for all
in this manner.
EXERCISE 1: Write monad instances for 
, 
, 
, 
,
Par Parser Option Stream
and 
.
List
EXERCISE 2 (optional, hard): 
 looks like it would be a monad too, but
State
it takes two type arguments and you need a type constructor of one argument to
implement 
. Try to implement a 
 monad, see what issues you run
Monad
State
into, and think about possible solutions. We will discuss the solution later in this
chapter.
Now that we have our primitive combinators for monads, we can look back at
previous chapters and see if there were some other combinators that we
implemented for each of our monadic data types. Many of of them can be
implemented once for all monads, so let's do that now.
EXERCISE 3: The 
 and 
 combinators should be pretty
sequence
traverse
familiar to you by now, and your implementations of them from various prior
chapters are probably all very similar. Implement them once and for all on 
:
Monad[M]
One combinator we saw for e.g. 
 and 
 was 
, which
Gen
Parser
listOfN
allowed us to replicate a parser or generator  times to get a parser or generator of
n
object Monad {
  val genMonad = new Monad[Gen] {
    def unit[A](a: => A): Gen[A] = Gen.unit(a)
    def flatMap[A,B](ma: Gen[A])(f: A => Gen[B]): Gen[B] =
      ma flatMap f
  }
}
11.3 Monadic combinators
def sequence[A](lma: List[M[A]]): M[List[A]]
def traverse[A,B](la: List[A])(f: A => M[B]): M[List[B]]
194
www.it-ebooks.info

lists of that length. We can implement this combinator for all monads  by adding
M
it to our 
 trait. We should also give it a more generic name such as 
Monad
.
replicateM
EXERCISE 4: Implement 
:
replicateM
EXERCISE 5: Think about how 
 will behave for various choices
replicateM
of . For example, how does it behave in the 
 monad? What about 
?
M
List
Option
Describe in your own words the general meaning of 
.
replicateM
There was also a combinator 
 for our 
 data type to take two
product
Gen
generators and turn them into a generator of pairs, and we did the same thing for 
 computations. In both cases, we implemented 
 in terms of 
.
Par
product
map2
So we can definitely write it generically for any monad . This combinator might
M
more appropriately be called 
, since we are "factoring out" , or "pushing 
factor
M
 to the outer layer":
M
We don't have to restrict ourselves to combinators that we have seen already.
It's important to 
 and see what we find. Now that we're thinking in the
play around
abstract about monads, we can take a higher perspective on things.
We know that 
 and 
 let us combine pairs, or when we otherwise
factor
map2
have 
 of two things. But what about when we have 
 of two things
both
either
(sometimes called their 
)? What happens then? We would have
coproduct
something like this:
EXERCISE 6: Implement the function 
. Explain to yourself what it
cofactor
does.
The combinators we have seen here are only a small sample of the full library
that 
 lets us implement once and for all.
Monad
def replicateM[A](n: Int, ma: M[A]): M[List[A]]
def factor[A,B](ma: M[A], mb: M[B]): M[(A, B)] = map2(ma, mb)((_, _))
def cofactor[A,B](e: Either[M[A], M[B]]): M[Either[A, B]]
195
www.it-ebooks.info

In chapters past, we found that the 
 function, for all of the data types that
map
support it, obeys the functor law. It would be very strange if it didn't hold. It's part
of the meaning we ascribe to 
. It's what a 
 
map
Functor is.
But what about 
? Certainly we would expect the functor laws to also
Monad
hold for 
, but what else would we expect? What laws should govern 
Monad
 and 
?
flatMap
unit
For example, if we wanted to combine three monadic values into one, which two
would we expect to be combined first? Would it matter? To answer this question,
let's for a moment take a step down from the abstract level and look at a simple
concrete example of using the 
 monad.
Gen
Say we are testing a product order system and we need to mock up some orders.
We might have an 
 case class and a generator for that class.
Order
Above, we are generating the 
 inline, but there might be places where we
Item
want to generate an 
 separately. So we could pull that into its own generator:
Item
Then we can use that in 
:
genOrder
11.4 Monad laws
11.4.1 The associative law
case class Order(item: Item, quantity: Int)
case class Item(name: String, price: Double)
val genOrder: Gen[Order] = for {
  name <- Gen.nextString
  price <- Gen.nextDouble
  quantity <- Gen.nextInt
} yield Order(Item(name, price), quantity)
val genItem: Gen[Item] = for {
  name <- Gen.nextString
  price <- Gen.nextDouble
} yield Item(name, price)
val genOrder: Gen[Order] = for {
  item <- genItem
  quantity <- Gen.nextInt
} yield Order(item, quantity)
196
www.it-ebooks.info

And that should do exactly the same thing, right? It seems safe to assume that.
But not so fast. How can we be sure? It's not exactly the same code.
EXERCISE 7 (optional): Expand both implementations of 
 to 
genOrder
map
and 
 calls to see the true difference.
flatMap
Once you expand them out, those two implementations are clearly not identical.
And yet when we look at the 
-comprehension, it seems perfectly reasonable to
for
assume that the two implementations do exactly the same thing. In fact, it would be
surprising and weird if they didn't. It's because we are assuming that flatMap
obeys an 
:
associative law
And this law should hold for all values , , and  of the appropriate types.
x f
g
EXERCISE 8: 
 that this law holds for 
.
Prove
Option
EXERCISE 9 (hard): Show that the equivalence of the two genOrder
implementations above follows from this law.
It's not so easy to see that the law we have discovered is an 
 law.
associative
Remember the associative law for monoids? That was very clear:
But our associative law for monads doesn't look anything like that! Fortunately,
there's a way we can make the law clearer if we consider not the monadic values of
types like 
, but monadic 
 of types like 
. Functions like
M[A]
functions
A => M[B]
that are called 
, and they can be composed with one another:
Kleisli arrows1
Footnote 1mThis name comes from category theory and is after the Swiss mathematician Heinrich Kleisli.
EXERCISE 10: Implement this function.
We can now state the associative law for monads in a much more symmetric
way:
x.flatMap(f).flatMap(g) == x.flatMap(a => f(a).flatMap(g))
11.4.2 Kleisli composition
append(append(a, b), c) == append(a, append(b, c))
def compose[A,B,C](f: A => M[B], g: B => M[C]): A => M[C]
compose(compose(f, g), h) == compose(f, compose(g, h))
197
www.it-ebooks.info

EXERCISE 11: Implement 
 in terms of 
. It seems that we
flatMap
compose
have found another minimal set of monad combinators: 
 and 
.
compose
unit
EXERCISE 12 (optional): Show that the two formulations of the associative
law, the one in terms of 
 and the one in terms of 
, are
flatMap
compose
equivalent.
The other monad law is now pretty easy to see. Just like 
 was an 
zero
identity
 for 
 in a monoid, there is an identity element for 
 in a
element
append
compose
monad. Indeed, that is exactly what 
 is, and that is why we chose its name the
unit
way we did:
This function has exactly the right type to be passed to 
. The effect
compose
should be that anything composed with 
 is that same thing. This usually takes
unit
the form of two laws, 
 and 
:
left identity
right identity
EXERCISE 13: Rewrite these monad identity laws in terms of 
.
flatMap
EXERCISE 14: There is a third minimal set of monadic combinators: 
, 
map
, and 
. Implement 
.
unit
join
join
EXERCISE 15: Implement either 
 or 
 in terms of 
.
flatMap
compose
join
EXERCISE 16: Use 
 to restate the monad laws.
join
EXERCISE 17 (optional): Write down an explanation, in your own words, of
what the associative law means for 
 and 
.
Par
Parser
EXERCISE 18 (optional): Explain in your own words what the identity laws
are stating in concrete terms for 
 and 
.
Gen
List
11.4.3 The identity laws
def unit[A](a: => A): M[A]
compose(f, unit) == f
compose(unit, f) == f
def join[A](mma: M[M[A]]): M[A]
198
www.it-ebooks.info

Let us now take a wider perspective. There is something unusual about the Monad
interface. The data types for which we've given monad instances don't seem to
have very much to do with each other. Yes, 
 factors out code duplication
Monad
among them, but what  a monad exactly? What does "monad" 
?
is
mean
You may be used to thinking of interfaces as providing a relatively complete
API for an abstract data type, merely abstracting over the specific representation.
After all, a singly-linked list and an array-based list may be implemented
differently behind the scenes, but they will share a common interface in terms of
which a lot of useful and concrete application code can be written. 
 is much
Monad
more abstract than that. The 
 combinators are often just a small fragment of
Monad
the full API for a given data type that happens to be a monad. So 
 doesn't
Monad
generalize one type or another; rather, many vastly different data types can satisfy
the 
 interface.
Monad
We have discovered that there are at least three minimal sets of primitive 
 combinators, and instances of 
 will have to provide
Monad
Monad
implementations of one of these sets:
unit and flatMap
unit and compose
unit, 
, and 
map
join
And we know that there are two monad laws to be satisfied, associativity and
identity, that can be formulated in various ways. So we can state quite plainly what
a monad :
is
A monad is an implementation of one of the minimal sets of monadic
combinators, satisfying the laws of associativity and identity.
That's a perfectly respectable, precise, and terse definition. But it's a little
dissatisfying. It doesn't say very much about what it implies—what a monad 
 The problem is that it's a 
 definition. Even if you're a
means.
self-contained
beginning programmer, you have by now obtained a vast amount of knowledge
about programming, and this definition integrates with none of that. In order to
really 
 what's going on with monads (or with anything for that matter),
understand
we need to think about them in terms of things we already understand. We need to
connect this new knowledge into a wider context.
To say "this data type is a monad" is to say something very specific about how
11.5 Just what is a monad?
199
www.it-ebooks.info

it behaves. But what exactly? To begin to answer the question of what monads 
, let's look at another couple of monads and compare their behavior.
mean
To really distill monads to their essentials, we should look at the simplest
interesting specimen, the identity monad, given by the following type:
EXERCISE 19: Implement 
 and 
 as methods on this class, and
map
flatMap
give an implementation for 
.
Monad[Id]
Now, 
 is just a simple wrapper. It doesn't really add anything. Applying 
Id
Id
to  is an identity since the wrapped type and the unwrapped type are totally
A
isomorphic (we can go from one to the other and back again without any loss of
information). But what is the meaning of the identity 
? Let's try using it in
monad
the REPL:
When we write the exact same thing with a 
-comprehension, it might be
for
clearer:
So what is the 
 of 
 for the identity monad? It's simply variable
action
flatMap
substitution. The variables  and  get bound to 
 and 
,
a
b
"Hello, "
"monad!"
respectively, and then they get substituted into the expression 
. In fact, we
a + b
could have written the same thing without the 
 wrapper, just using Scala's own
Id
variables:
11.5.1 The identity monad
case class Id[A](value: A)
scala> Id("Hello, ") flatMap (a =>
     |   Id("monad!") flatMap (b =>
     |     Id(a + b)))
res0: Id[java.lang.String] = Id(Hello, monad!)
scala> for {
     |   a <- Id("Hello, ")
     |   b <- Id("monad!")
     | } yield a + b
res1: Id[java.lang.String] = Id(Hello, monad!)
scala> val a = "Hello, "
a: java.lang.String = "Hello, "
200
www.it-ebooks.info

Besides the 
 wrapper, there is no difference. So now we have at least a
Id
partial answer to the question of what monads mean. We could say that monads
provide a scheme for binding variables. Stated more dramatically, a monad is a
kind of programming language that supports variable substitution.
Let's see if we can get the rest of the answer.
Look back at the chapter on the 
 data type. Recall that we implemented
State
some combinators for 
, including 
 and 
:
State
map
flatMap
It looks like 
 definitely fits the profile for being a monad. But its type
State
constructor takes two type arguments and 
 requires a type constructor of
Monad
one argument, so we can't just say 
. But if we choose some
Monad[State]
particular  then we have something like 
, which is the kind of
S
State[S, _]
thing expected by 
. So 
 doesn't just have one monad instance but a
Monad
State
whole family of them, one for each choice of . We would like to be able to
S
partially apply 
 to where the  type argument is fixed to be some concrete
State
S
type.
This is very much like how you would partially apply a function, except at the
type level. For example, we can create an 
 type constructor, which is an
IntState
alias for 
 with its first type argument fixed to be 
:
State
Int
scala> val b = "monad!"
b: java.lang.String = monad!
scala> a + b
res2: java.lang.String = Hello, monad!
11.5.2 The State monad and partial type application
case class State[S, A](run: S => (A, S)) {
  def map[B](f: A => B): State[S, B] =
    State(s => {
      val (a, s1) = run(s)
      (f(a), s1)
    })
  def flatMap[B](f: A => State[S, B]): State[S, B] =
    State(s => {
      val (a, s1) = run(s)
      f(a).run(s1)
    })
}
201
www.it-ebooks.info

And 
 is exactly the kind of thing that we can build a 
 for:
IntState
Monad
Of course, it would be really repetitive if we had to manually write a separate 
 instance for each specific state type. Unfortunately, Scala does not allow us
Monad
to use underscore syntax to simply say 
 to create an anonymous
State[Int, _]
type constructor like we create anonymous functions with the underscore syntax.
But instead we can use something similar to lambda syntax at the type level. For
example, we could have declared 
 directly inline like this:
IntState
This syntax can be a little jarring when you first see it. But all we are doing is
declaring an anonymous type within parentheses. This anonymous type has, as one
of its members, the type alias 
, which looks just like before. Outside
IntState
the parentheses we are then accessing its 
 member with the  syntax.
IntState
#
Just like we can use a "dot" ( ) to access a member of an object at the value level,
.
we can use the  symbol to access a type member (
#
See the "Type Member" section
).
of the Scala Language Specification
A type constructor declared inline like this is often called a 
 in
type lambda
Scala. We can use this trick to partially apply the 
 type constructor and
State
declare a 
 trait. An instance of 
 is then a monad
StateMonad
StateMonad[S]
instance for the given state type .
S
type IntState[A] = State[Int, A]
object IntStateMonad extends Monad[IntState] {
  def unit[A](a: => A): IntState[A] = State(s => (a, s))
  def flatMap[A,B](st: IntState[A])(f: A => IntState[B]): IntState[B] =
    st flatMap f
}
object IntStateMonad extends
  Monad[({type IntState[A] = State[Int, A]})#IntState] {
  ...
}
def stateMonad[S] = new Monad[({type lambda[x] = State[S,x]})#lambda] {
  def unit[A](a: => A): State[S,A] = State(s => (a, s))
  def flatMap[A,B](st: State[S,A])(f: A => State[S,B]): State[S,B] =
    st flatMap f
}
202
www.it-ebooks.info

Again, just by giving implementations of 
 and 
, we get
unit
flatMap
implementations of all the other monadic combinators for free.
EXERCISE 20: Now that we have a 
 monad, you should try it out to see
State
how it behaves. What is the meaning of 
 in the 
 monad?
replicateM
State
How does 
 behave? What about 
?
map2
sequence
Let's now look at the difference between the 
 monad and the 
 monad.
Id
State
Remember that the primitive operations on 
 (besides the monadic
State
operations 
 and 
) are that we can read the current state with 
unit
flatMap
 and we can set a new state with 
.
getState
setState
Remember that we also discovered that these combinators constitute a minimal
set of primitive operations for 
. So together with the monadic primitives (
State
 and 
), they 
 everything that we can do with the 
unit
flatMap
completely specify
 data type.
State
EXERCISE 21: What laws do you expect to mutually hold for 
, 
getState
, 
 and 
?
setState unit
flatMap
What does this tell us about the meaning of the 
 
? Let's study a
State monad
simple example:
This function numbers all the elements in a list using a 
 action. It keeps
State
a state that is an 
 which is incremented at each step. The whole composite state
Int
action is run starting from 0. We are then reversing the result since we constructed
it in reverse order .
2
Footnote 2mThis is asymptotically faster than appending to the list in the loop.
The details of this code are not really important. What is important is what's
def getState[S]: State[S, S]
def setState[S](s: => S): State[S, Unit]
val M = new StateMonad[Int]
def zipWithIndex[A](as: List[A]): List[(Int,A)] =
  as.foldLeft(M.unit(List[(Int, A)]()))((acc,a) => for {
    n  <- getState
    xs <- acc
    _  <- setState(n + 1)
} yield (n, a) :: xs).run(0)._1.reverse
203
www.it-ebooks.info

going on with 
 and 
 in the 
-comprehension. We are
getState
setState
for
obviously getting variable binding just like in the 
 monad—we are binding the
Id
value of each successive state action (
, 
, and then 
) to
getState acc
setState
variables. But there is more going on, literally 
 At each line in the
between the lines.
-comprehension, the implementation of 
 is making sure that the
for
flatMap
current state is available to 
, and that the new state gets propagated to
getState
all actions that follow a 
.
setState
What does the difference between the action of 
 and the action of 
 tell
Id
State
us about monads in general? We can see that a chain of 
s (or an
flatMap
equivalent 
-comprehension) is like an imperative program with statements that
for
assign to variables, and the monad specifies what occurs at statement boundaries.
For example, with 
, nothing at all occurs except unwrapping and re-wrapping in
Id
the 
 constructor. With 
, the most current state gets passed from one
Id
State
statement to the next. With the 
 monad, a statement may return 
 and
Option
None
terminate the program. With the 
 monad a statement may return many
List
results, which causes statements that follow it to potentially run multiple times,
once for each result.
EXERCISE 22: To cement your understanding of monads, give a monad
instance for the following type, and explain what it means. What are its primitive
operations? What is the action of 
? What meaning does it give to
flatMap
monadic functions like 
, 
, and 
? What meaning
sequence join
replicateM
does it give to the monad laws?
case class Reader[R, A](run: R => A)
object Reader {
  def readerMonad[R] = new Monad[({type f[x] = Reader[R,x]})#f] {
    def unit[A](a: => A): Reader[R,A]
    def flatMap[A,B](st: Reader[R,A])(f: A => Reader[R,B]): Reader[R,B]
  }
}
204
www.it-ebooks.info

functor
functor law
identity element
Kleisli arrow
left identity law for monads
listOfN
monad
monadic join
monad identity law
monad laws
replicateM
right identity law for monads
type lambda
unit law
In this chapter, we took a pattern that we had seen repeated throughout the book
and we unified it under a single concept: monad. This allowed us to write a number
of combinators once and for all, for many different data types that at first glance
don't seem to have anything in common. We discussed laws that they all satisfy,
the monad laws, from various perspectives, and we tried to gain some insight into
what it all means.
An abstract topic like this cannot be fully understood all at once. It requires an
iterative approach where you keep revisiting the topic from new perspectives.
When you discover new monads, new applications of them, or see them appear in a
new context, you will inevitably gain new insight. And each time it happens you
might think to yourself: "OK, I thought I understood monads before, but now I 
 get it."
really
In the next chapter, we will explore a slight variation on the theme of monads,
and develop your ability to discover new abstractions on your own.
Index Terms
11.6 Conclusion
205
www.it-ebooks.info

12
In the previous chapter on monads we saw how a lot of the functions we have been
writing for different combinator libraries can be expressed in terms of a single
interface, 
.
Monad
Monads provide a very powerful interface, as evidenced by the fact that we can
use 
 to essentially write imperative programs in a purely functional way.
flatMap
But sometimes this is more power than we need, and such power comes at a price.
As we will see in this chapter, the price is that we lose some compositionality. We
can reclaim it by instead using 
, which are simpler and more
applicative functors
general than monads.
By now you have seen 
 and 
 many times for different
sequence
traverse
monads, implemented them in terms of each other, and generalized them to any
monad :
M
Here, the implementation of 
 is using 
 and 
, and we
traverse
map2
unit
have seen that 
 can be implemented in terms of 
:
map2
flatMap
Applicative and traversable functors
12.1 Generalizing monads
def sequence[A](lma: List[M[A]]): M[List[A]]
  traverse(mas, ma => ma)
def traverse[A,B](as: List[A])(f: A => M[B]): M[List[B]]
  as.foldRight(unit(List[B]()))((a, mbs) => map2(f(a), mbs)(_ :: _))
def map2[A,B,C](ma: M[A], mb: M[B])(f: (A,B) => C): M[C] =
  flatMap(ma)(a => map(mb)(b => f(a,b)))
206
www.it-ebooks.info

What you may not have noticed is that almost all of the useful combinators we
wrote in the previous chapter can be implemented just in terms of 
 and 
map2
unit
. What's more, for all the data types we have discussed, 
 can be written 
map2
 resorting to 
.
without
flatMap
Is 
 with 
 then just another minimal set of operations for monads?
map2
unit
No it's not, because there are monadic combinators such as 
 and 
join
flatMap
that cannot be implemented with just 
 and 
. This suggests that 
map2
unit
map2
and 
 is a less powerful subset of the 
 interface, that nonetheless is
unit
Monad
very useful on its own.
Since so many combinators can be written in terms of just these two functions,
there is a name for data types that can implement 
 and 
. They are called
map2
unit
:
applicative functors
There is an additional combinator, 
, that we haven't yet discussed. The 
apply
 and 
 combinators can be implemented in terms of each other, so a
map2
apply
minimal implementation of 
 must provide one or the other.
Applicative
EXERCISE 1: Implement 
 in terms of 
 and 
. Then
map2
apply
unit
implement 
 in terms of 
 and 
.
apply
map2
unit
Applicative also extends 
. This implies that 
 can be written
Functor
map
in terms of 
, 
, and 
. Here it is in terms of 
 and 
:
map2 apply
unit
apply
unit
And here in terms of 
 and 
:
map2
unit
You should already have a good sense of what 
 means. But what is the
map2
12.2 The Applicative trait
trait Applicative[F[_]] extends Functor[F] {
  def map2[A,B](fa: F[A], fb: F[B])(f: (A, B) => C): F[C]
  def apply[A,B](fab: F[A => B])(fa: F[A]): F[B]
  def unit[A](a: A): F[A]
}
def map[A,B](a: F[A])(f: A => B): F[B] =
  apply(unit(f))(a)
def map[A,B](a: F[A])(f: A => B): F[B] =
  map2(a, unit(f))(_(_))
207
www.it-ebooks.info

meaning of 
? At this point it's a little bit of a floating abstraction, and we
apply
need an example. Let's drop down to 
 and look at a concrete
Option
implementation:
You can see that this method combines two 
s. But one of them
Option
contains a 
 (unless it is 
 of course). The action of 
 is to apply
function
None
apply
the function inside one argument to the value inside the other. This is the origin of
the name "applicative". This operation is sometimes called idiomatic function
 since it occurs within some idiom or context. In the case of the 
application
 idiom from the previous chapter, the implementation is literally just
Identity
function application. In the example above, the idiom is 
 so the method
Option
additionally encodes the rule for 
, which handles the case where either the
None
function or the value are absent.
The action of 
 is similar to the familiar 
. Both are a kind of function
apply
map
application in a context, but there's a very specific difference. In the case of 
, the function being applied might be affected by the context. For example,
apply
if the second argument to 
 is 
 in the case of 
 then there is no
apply
None
Option
function at all. But in the case of 
, the function must exist independently of the
map
context. This is easy to see if we rearrange the type signature of 
 a little and
map
compare it to 
:
apply
The only difference is the  around the function argument type. The 
F
apply
function is strictly more powerful, since it can have that added  effect. It makes
F
sense that we can implement 
 in terms of 
, but not the other way
map
apply
around.
We have also seen that we can implement 
 as well in terms of 
.
map2
apply
We can extrapolate that pattern and implement 
, 
, etc. In fact, 
map3 map4
apply
can be seen as a general function lifting combinator. We can use it to lift a function
def apply[A,B](oab: Option[A => B])(oa: Option[A]): Option[B] =
  (oab, oa) match {
    case (Some(f), Some(a)) => Some(f(a))
    case _ => None
  }
def   map[A,B](f:   A => B )(a: F[A]): F[B]
def apply[A,B](f: F[A => B])(a: F[A]): F[B]
208
www.it-ebooks.info

of any arity into our applicative functor. Let's look at 
 on 
 as an
map3
Option
example.
We start out with a ternary function like 
, which just adds 3
(_ + _ + _)
integers:
If we Curry this function, we get a slightly different type. It's the same function,
but one that can be gradually applied to arguments one at a time.
Pass that to 
 (which is just 
 in the case of 
) and we have:
unit
Some
Option
We can now use idiomatic function application three times to apply this to three
 values. To complete the picture, here is the fully general implementation
Option
of 
 in terms of 
.
map3
apply
The pattern is simple. We just Curry the the function we want to lift, pass the
result to 
, and then 
 as many times as there are arguments. Each call
unit
apply
to 
 is a partial application of the function .
apply
1
Footnote 1mNotice that in this sense 
 can be seen as 
, since it's the case of "lifting" where 
unit
map0
apply
is called zero times.
Another interesting fact is that we have just discovered a new minimal
definition of 
! We already know that a monad can be given by
Monad
implementations of 
, 
, and 
. Well, since we can implement 
 in
map unit
join
map
terms of 
, we now know that a 
 can be given by 
, 
, and
apply
Monad
unit join
either 
 or 
. This gives us further insight into what a monad is! A
map2
apply
monad is just an applicative functor with an additional combinator, 
.
join
val f: (Int, Int, Int) => Int = (_ + _ + _)
val g: Int => Int => Int => Int = f.curried
val h: Option[Int => Int => Int => Int] = unit(g)
def map3[A,B,C,D](fa: F[A],
                  fb: F[B],
                  fc: F[C])(f: (A, B, C) => D): F[D] =
  apply(apply(apply(unit(f.curried))(fa))(fb))(fc)
209
www.it-ebooks.info

We know that we can implement 
 using 
. And following the
map2
flatMap
reasoning above, this means that 
. Since we
all monads are applicative functors
have already implemented 
 instances for a great many data types, we don't
Monad
have to write 
 instances for them. We can simply change 
Applicative
Monad
so that it extends 
 and overrides 
:
Applicative
apply
But the converse is not true—not all applicative functors are monads. Using 
 together with either 
 or 
, we cannot implement 
 or 
unit
map2
apply
flatMap
. If we try it, we get stuck pretty quickly.
join
Let's look at the signatures of 
 and 
 side by side, rearranging
flatMap
apply
the signature of 
 slightly from what we're used to in order to make things
flatMap
clearer:
The difference is in the type of the function argument. In 
, that argument
apply
is fully contained inside 
. So the only way to pass the 
 from the second
F
A
argument to the function of type 
 in the first argument is to somehow
A => B
consider both  contexts. For example, if  is 
 we pattern-match on both
F
F
Option
arguments to determine if they are 
 or 
:
Some
None
12.3 The difference between monads and applicative functors
trait Monad[M[_]] extends Applicative[M] {
  def flatMap[A,B](ma: M[A])(f: A => M[B]): M[B] = join(map(ma)(f))
  def join[A](mma: M[M[A]]): M[A] = flatMap(mma)(ma => ma)
  def compose[A,B,C](f: A => M[B], g: B => M[C]): A => M[C] =
    a => flatMap(f(a))(g)
  override def apply(mf: M[A => B])(ma: M[A]): M[B] =
    flatMap(mf)(f => flatMap(ma)(a => f(a))
}
def apply[A,B](fab: F[A => B])(fa: F[A]): F[B]
def flatMap[A,B](f: A => F[B])(fa: F[A]): F[B]
def apply[A,B](oa: Option[A], oab: Option[A => B]): Option[B] =
  (oa, oab) match {
    case (Some(a), Some(f)) => Some(f(a))
    case _ => None
  }
210
www.it-ebooks.info

Likewise with 
 the function argument  is only invoked if both of the 
map2
f
 arguments are 
. The important thing to note is that whether the
Option
Some
answer is 
 or 
 is entirely determined by whether the inputs are both 
Some
None
.
Some
But in 
, the second argument is a function that produces an , and its
flatMap
F
structure 
 on the value of the  from the first argument. So in the 
depends
A
Option
case, we would match on the first argument, and if it's 
 we apply the function
Some
 to get a value of type 
. And whether 
 result is 
 or 
f
Option[B]
that
Some
None
actually depends on the value inside the first 
:
Some
One way to put this difference into words is to say that applicative operations
 while monadic operations may alter a structure. What does this
preserve structure
mean in concrete terms? Well, for example, if you 
 over a 
 with three
map
List
elements, the result will always also have three elements. But if you flatMap
over a list with three elements, you may get many more since each function
application can introduce a whole 
 of new values.
List
EXERCISE 2: Transplant the implementations of as many combinators as you
can from 
 to 
, using only 
, 
, and 
, or
Monad
Applicative
map2 apply
unit
methods implemented in terms of them.
def map2[A,B](x: Option[A], y: Option[B])(f: (A, B) => C): Option[C] =
  (x, y) match {
    case (Some(a), Some(b)) => Some(f(a, b))
    case _ => None
  }
def flatMap[A,B](oa: Option[A])(f: A => Option[B]): Option[B] =
  oa match {
    case Some(a) => f(a)
    case _ => None
  }
def sequence[A](fas: List[F[A]]): F[List[A]]
def traverse[A](as: List[A])(f: A => F[B]): F[List[B]]
def replicateM[A](n: Int, fa: F[A]): F[List[A]]
def factor[A,B](fa: F[A], fb: F[A]): F[(A,B)]
211
www.it-ebooks.info

To further illustrate the difference between applicative functors and monads,
getting a better understanding of both in the process, we should look at an example
of an applicative functor that is not a monad.
In chapter 4, we looked at the 
 data type and considered the question of
Either
how such a data type would have to be modified to allow us to report multiple
errors. To make this concrete, think of validating a web form submission. You
would want to give an error to the user if some of the information was entered
incorrectly, and you would want to tell the user about all such errors at the same
time. It would be terribly inefficient if we only reported the first error. The user
would have to repeatedly submit the form and fix one error at a time.
This is the situation with 
 if we use it monadically. First, let's actually
Either
write the monad for the partially applied 
 type.
Either
EXERCISE 3: Write a monad instance for 
:
Either
Now consider what happens in a sequence of 
s like this, where each
flatMap
of the functions 
, 
, and 
validateEmail validPhone
validatePostcode
has type 
 for a given type :
Either[String, T]
T
If 
 fails with an error, then 
 and 
validName
validBirthdate
 won't even run. The computation with 
 is inherently
validPhone
flatMap
sequential and causal. The variable 
 will never be bound to anything unless 
f1
 succeeds.
validName
Now think of doing the same thing with 
:
apply
12.4 Not all applicative functors are monads
def eitherMonad[E]: Monad[({type f[x] = Either[E, x]})#f]
validName(field1) flatMap (f1 =>
validBirthdate(field2) flatMap (f2 =>
validPhone(field3) map (WebForm(_, _, _))
apply(apply(apply((WebForm(_, _, _)).curried)(
  validName(field1)))(
  validBirthdate(field2)))(
  validPhone(field3))
212
www.it-ebooks.info

Here, there is no causality or sequence implied. We could run the validation
functions in any order, and there's no reason we couldn't run them all first and then
combine any errors we get. Perhaps by collecting them in a 
 or 
. But
List
Vector
the 
 monad we have is not sufficient for this. Let's invent a new data type, 
Either
, that is very much like 
 except that it can explicitly handle
Validation
Either
more than one error.
EXERCISE 4: Write an 
 instance for 
 that
Applicative
Validation
accumulates errors in 
. Notice that in the case of 
 there is
Failure
Failure
always at least one error, stored in 
. The rest of the errors accumulate in the 
head
.
tail
To continue the example above, consider a web form that requires an email
address, a phone number, and a post code.
This data will likely be collected from the user as strings, and we must make
sure that the data meets a certain specification, or give a list of errors to the user
indicating how to fix the problem. The specification might say that 
 cannot
name
be empty, that 
 must be in the form 
, and that 
birthdate
"yyyy-MM-dd"
 must contain exactly 10 digits:
phoneNumber
sealed trait Validation[+E, +A]
case class Failure[E](head: E, tail: Vector[E])
  extends Validation[E, Nothing]
case class Success[A](a: A) extends Validation[Nothing, A]
case class WebForm(name: String, birthdate: Date, phoneNumber: String)
def validName(name: String): Validation[String, String] =
  if (name != "")
       Success(name)
  else Falure("Name cannot be empty", List())
def validBirthdate(birthdate: String): Validation[String, Date] =
  try {
    import java.text._
    Success((new SimpleDateFormat("yyyy-MM-dd")).parse(birthdate))
  } catch {
    Failure("Birthdate must be in the form yyyy-MM-dd", List())
  }
213
www.it-ebooks.info

And to validate an entire web form, we can simply lift the WebForm
constructor with 
:
apply
If any or all of the functions produce 
, the whole 
Failure
validWebForm
method will return all of those failures combined.
What sort of laws should we expect applicative functors to obey? Well, we should
definitely expect them to obey the functor law:
What does this mean for applicative functors? Let's remind ourselves of our
implementation of 
:
map
This definition of 
 simply shows that an applicative functor 
 in a
map
is a functor
specific way. And this definition, together with the functor law, imply the first
applicative law. This is the 
 for applicative functors:
law of identity
This law demands that 
 preserve identities. Putting the identity function
unit
through 
 and then 
 results in the identity function itself. But this really
unit
apply
def validPhone(phoneNumber: String): Validation[String, String] =
  if (phoneNumber.matches("[0-9]{10}"))
       Success(phoneNumber)
  else Failure("Phone number must be 10 digits")
def validWebForm(name: String,
                 birthdate: String,
                 phone: String): Validation[String, WebForm] =
  apply(apply(apply((WebForm(_, _, _)).curried)(
    validName(name)))(
    validBirthdate(birthdate)))(
    validPhone(phone))
12.5 The applicative laws
map(v)(x => x) == v
def map[A,B](a: F[A])(f: A => B): F[B] =
  apply(unit(f))(a)
apply(unit(x => x))(v) == v
214
www.it-ebooks.info

is just the functor law with an applicative accent, since by our definition above, 
 is the same as 
 followed by 
.
map
unit
apply
The second thing that the laws demand of applicative functors is that they 
. This is stated as the 
:
preserve function composition
composition law
Here,  and  have types like 
 and 
, and  is a value
f
g
F[A => B]
F[B => C]
x
of type 
. Both sides of the 
 sign are applying  to  and then applying  to
F[A]
==
g
x
f
the result of that. In other words they are applying the composition of  and  to .
f
g
x
All that this law is saying is that these two ways of lifting function composition
should be equivalent. In fact, we can write this more tersely using 
:
map2
Or even more tersely than that, using 
:
map3
This makes it much more obvious what's going on. We're lifting the
higher-order function 
 which, given the arguments 
, , and  will
_(_(_))
f g
x
return 
. And the composition law essentially says that even though there
f(g(x))
might be different ways to implement 
 for an applicative functor, they should
map3
all be equivalent.
The two remaining applicative laws establish the relationship between unit
and 
. These are the laws of 
 and 
. The
apply
homomorphism
interchange
homomorphism law states that passing a function and a value through 
,
unit
followed by idiomatic application, is the same as passing the result of regular
application through 
:
unit
We know that passing a function to 
 and then 
 is the same as
unit
apply
simply calling 
, so we can restate the homomorphism law:
map
   apply(apply(apply(unit(a => b => c => a(b(c)))))(g))(x)
== apply(f)(apply(g)(x))
apply(map2(f, g)(_compose _))(x) == apply(f)(apply(g)(x))
map3(f, g, x)(_(_(_)))
apply(unit(f))(unit(x)) == unit(f(x))
215
www.it-ebooks.info

The 
 completes the picture by saying that 
 should have
interchange law
unit
the same effect whether applied to the first or the second argument of 
:
apply
The interchange law can be stated a different way, showing a relationship
between 
 and 
 with regard to lifted function application:
map2
map
The applicative laws are not surprising or profound. Just like the monad laws,
these are simple sanity checks that the applicative functor works in the way that we
would expect. They ensure that 
, 
, 
, and 
 behave in a
apply unit map
map2
consistent manner.
EXERCISE 5 (optional, hard): Prove that all monads are applicative functors,
by showing that the applicative laws are implied by the monad laws.
EXERCISE 6: Just like we can take the product of two monoids  and  to give
A
B
the monoid 
, we can take the product of two applicative functors.
(A, B)
Implement this function:
EXERCISE 7 (hard): Applicative functors also compose another way! If F[_]
and 
 are applicative functors, then so is 
. Prove this.
G[_]
F[G[_]]
EXERCISE 8 (optional): Try to write 
 on 
. It is not possible,
compose
Monad
but it is instructive to attempt it and understand why this is the case.
map(unit(x))(f) == unit(f(x))
apply(u)(unit(y)) == apply(unit(_(y)))(u)
map2(u, unit(y))(_(_)) == map(u)(_(y))
def product[G[_]](G: Applicative[G]):
  Applicative[({type f[x] = (F[x], G[x])})#f]
def compose[G[_]](G: Applicative[G]):
  Applicative[({type f[x] = F[G[x]]})#f]
def compose[N[_]](N: Monad[N]): Monad[({type f[x] = M[N[x]]})#f]
216
www.it-ebooks.info

Look again at the signatures of 
 and 
:
traverse
sequence
Can we generalize this further? Recall that there were a number of data types
other than 
 that were 
 in chapter 10. Are there data types other
List
Foldable
than 
 that are 
? Of course!
List
traversable
EXERCISE 9: On the 
 trait, implement 
 over a 
Applicative
sequence
Map
rather than a 
:
List
But traversable data types are too numerous for us to write specialized 
 and 
 methods for each of them. What we need is a new
sequence
traverse
interface. We will call it 
:
Traverse2
Footnote 2mThe name 
 is already taken by an unrelated trait in the Scala standard library.
Traversable
The really interesting operation here is 
. Look at its signature
sequence
closely. It takes 
 and swaps the order of  and . In other words it 
F[M[A]]
F
M
  and  past each other to give 
. This is as general as a type
commutes F
M
M[F[A]]
signature gets. What 
 is saying is that  can be swapped with an 
Traverse[F]
F
M
inside of it, as long as that  is an applicative functor. We have seen lots of
M
examples of this in past chapters, but now we are seeing the general principle.
The 
 method, given an  full of s, will turn each  into an 
traverse
F
A
A
M[B]
and then use the applicative functor to combine the 
s into an  full of s.
M[B]
F
B
EXERCISE 10: Write 
 instances for 
, 
, and 
:
Traverse
List Option
Tree
12.6 Traversable functors
def traverse[F[_],A,B](as: List[A], f: A => F[B]): F[List[B]]
def sequence[F[_],A](fas: List[F[A]]): F[List[A]]
def sequenceMap[K,V](ofa: Map[K,F[V]]): F[Map[K,V]]
trait Traverse[F[_]] {
  def traverse[M[_]:Applicative,A,B](fa: F[A])(f: A => M[B]): M[F[B]] =
    sequence(map(fa)(f))
  def sequence[M[_]:Applicative,A](fma: F[M[A]]): M[F[A]] =
    traverse(fma)(ma => ma)
}
217
www.it-ebooks.info

At this point you might be asking yourself what the difference is between a
traversal and a fold. Both of them take some data structure and apply a function to
the data within in order to produce a result. The difference is that 
 
traverse
, while 
 actually throws the structure
preserves the original structure
foldMap
away and replaces it with the operations of a monoid.
For example, when traversing a 
 with an 
-valued function, we
List
Option
would expect the result to always either be 
 or to contain a list of the same
None
length as the input list. This sounds like it might be a law! But we can choose a
simpler applicative functor than 
 for our law. Let's choose the simplest
Option
possible one, the identity functor:
We already know that 
 is a monad so it's also an applicative functor:
Id
Then our law, where 
 is an 
 for some 
, can be written
xs
F[A]
Traverse[F]
like this:
If we replace 
 with 
 here, then this is just the functor identity
traverse
map
law! This means that in the context of the 
 applicative functor, 
 and 
Id
map
 are the same operation. This implies that 
 can extend 
traverse
Traverse
 and we can write a default implementation of 
 in terms of 
Functor
map
 through 
, and 
 itself can be given a default
traverse
Id
traverse
implementation in terms of 
 and 
:
sequence
map
case class Tree[+A](head: A, tail: List[Tree[A]])
type Id[A] = A
val idMonad = new Monad[Id] {
  def unit[A](a: => A) = a
  override def flatMap[A,B](a: A)(f: A => B): B = f(a)
}
traverse[Id, A, A](xs)(x => x) == xs
trait Traverse[F[_]] extends Functor[F] {
  def traverse[M[_]:Applicative,A,B](fa: F[A])(f: A => M[B]): M[F[B]] =
    sequence(map(fa)(f))
218
www.it-ebooks.info

A valid instance of 
 must then override at least either 
 or
Traverse
sequence
.
traverse
But what is the relationship between 
 and 
? The answer
Traverse
Foldable
involves a connection between 
 and 
.
Applicative
Monoid
Take another look at the signature of 
:
traverse
Suppose that our  were a type constructor 
 that takes any type to 
M
ConstInt
, so that 
 throws away its type argument  and just gives us 
Int
ConstInt[A]
A
:
Int
Then in the type signature for 
, if we instantiate  to be 
traverse
M
ConstInt
, it becomes:
This looks a lot like 
 from 
. Indeed, if  is something like
foldMap
Foldable
F
 then what we need to implement this signature is a way of combining the 
List
 values returned by  for each element of the list, and a "starting" value for
Int
f
handling the empty list. In other words, we only need a 
. And
Monoid[Int]
that's easy to come by.
Indeed, given a constant functor like above, we can turn any 
 into an 
Monoid
:
Applicative
  def sequence[M[_]:Applicative,A](fma: F[M[A]]): M[F[A]] =
    traverse(fma)(ma => ma)
  def map[A,B](fa: F[A])(f: A => B): F[B] =
    traverse[Id, A, B](fa)(f)(idMonad)
}
12.6.1 From monoids to applicative functors
def traverse[M[_]:Applicative,A,B](fa: F[A])(f: A => M[B]): M[F[B]]
type ConstInt[A] = Int
def traverse[A,B](fa: F[A])(f: A => Int): Int
type Const[A, B] = A
implicit def monoidApplicative[M](M: Monoid[M]) =
219
www.it-ebooks.info

This is ConstInt generalized to any A, not just Int.
For this reason, applicative functors are sometimes called "monoidal functors".
The operations of a monoid map directly onto the operations of an applicative.
This means that 
 can extend 
 and we can give a default
Traverse
Foldable
implementation of 
 in terms of 
:
foldMap
traverse
Note that 
 now extends both 
 
 
!
Traverse
Foldable and Functor
Importantly, 
 itself cannot extend 
. Even though it's possible
Foldable
Functor
to write 
 in terms of a fold for most foldable data structures like 
, it is not
map
List
possible 
.
in general
EXERCISE 11: Answer, to your own satisfaction, the question of why it's not
possible for 
 to extend 
. Can you think of a 
 that
Foldable
Functor
Foldable
is not a functor?
So what is 
 really for? We have already seen practical applications of
Traverse
particular instances, such as turning a list of parsers into a parser that produces a
list. But in what kinds of cases do we want the 
? What sort of
generalization
generalized library does 
 allow us to write?
Traverse
The 
 applicative functor is a particularly powerful one. Using a 
State
State
action to 
 a collection, we can implement complex traversals that keep
traverse
some kind of internal state.
There's an unfortunate amount of type annotation necessary in order to partially
apply 
 in the proper way, but traversing with 
 is common enough
State
State
that we can just have a special method for it and write those type annotations once
and for all:
  new Applicative[({ type f[x] = Const[M, x] })#f] {
    def unit[A](a: => A): M = M.zero
    override def apply[A,B](m1: M)(m2: M): M = M.op(m1, m2)
  }
override def foldMap[A,B](as: F[A])(f: A => B)(mb: Monoid[B]): B =
  traverse[({type f[x] = Const[B,x]})#f,A,Nothing](
    as)(f)(monoidApplicative(mb))
12.7 Uses of Traverse
12.7.1 Traversals with State
220
www.it-ebooks.info

To demonstrate this, here is a 
 traversal that labels every element with
State
its position. We keep an integer state, starting with 0, and add 1 at each step:
By the same token, we can keep a state of type 
, to turn any
List[A]
traversable functor into a List:
Get the current state, the accumulated list.
Add the current element and set the new list as the new state.
It begins with the empty list 
 as the initial state, and at every element in the
Nil
traversal it adds it to the front of the accumulated list. This will of course construct
the list in the reverse order of the traversal, so we end by reversing the list we get
from running the completed state action. Note that we 
 because in this
yield ()
instance we don't want to return any value other than the state.
Of course, the code for 
 and 
 is nearly identical. And
toList
zipWithIndex
in fact most traversals with 
 will follow this exact pattern: We get the
State
current state, compute the next state, set it, and yield some value. We should
capture that in a function:
def traverseS[S,A,B](fa: F[A])(f: A => State[S, B]): State[S, F[B]] =
  traverse[({type f[x] = State[S,x]})#f,A,B](fa)(f)(Monad.stateMonad)
def zipWithIndex[A](ta: F[A]): F[(A,Int)] =
  traverseS(ta)((a: A) => (for {
    i <- get[Int]
    _ <- set(i + 1)
  } yield (a, i))).run(0)._1
def toList[A](fa: F[A]): List[A] =
  traverseS(fa)((a: A) => (for {
    as <- get[List[A]]
    _  <- set(a :: as)
  } yield ()).run(Nil)._2.reverse
def mapAccum[S,A,B](fa: F[A], s: S)(f: (A, S) => (B, S)): (F[B], S) =
  traverseS(fa)((a: A) => (for {
    s1 <- get[S]
    (b, s2) = f(a, s)
    _  <- set(s2)
  } yield b)).run(s)
override def toList[A](fa: F[A]): List[A] =
221
www.it-ebooks.info

EXERCISE 12: An interesting consequence of being able to turn any
traversable functor into a 
 list is that we can write, once and for all, a
reversed
function to reverse any traversable functor! Write this function.
It should obey the following law, for all  and  of the appropriate types:
x
y
EXERCISE 13: Use 
 to give a default implementation of 
mapAccum
 for the 
 trait.
foldLeft
Traverse
It is in the nature of a traversal that it must preserve the shape of its argument. This
is both its strength and its weakness. This is well demonstrated when we try to
combine two structures into one.
Given 
 can we combine a value of some type 
 and
Traverse[F]
F[A]
another of some type 
 into an 
? We could try using 
 to write
F[B]
F[C]
mapAccum
a generic version of 
:
zip
Notice that this version of 
 is not able to handle arguments of different
zip
"shapes". For example if  is 
 then it can't handle lists of different lengths. In
F
List
this implementation, the list 
 must be at least as long as 
. If  is 
, then 
fb
fa
F
Tree
 must have at least the same number of branches as 
 at every level.
fb
fa
We can change the generic 
 slightly and provide two versions so that the
zip
shape of one side or the other is dominant:
  mapAccum(fa, List[A]())((a, s) => ((), a :: s))._2.reverse
def zipWithIndex[A](fa: F[A]): F[(A, Int)] =
  mapAccum(fa, 0)((a, s) => ((a, s), s + 1))._1
def reverse[A](fa: F[A]): F[A]
toList(reverse(x)) ++ toList(reverse(y)) ==
 reverse(toList(y) ++ toList(x))
12.7.2 Combining traversable structures
def zip[A,B](fa: F[A], fb: F[B]): F[(A, B)] =
  (mapAccum(fa, toList(fb)) {
    case (a, Nil) => sys.error("zip: Incompatible shapes.")
    case (a, b :: bs) => ((a, b), bs)
  })._1
222
www.it-ebooks.info

In the case of 
 for example, the result of 
 will have the shape of the 
List
zipR
 argument, and it will be padded with 
 on the left if 
 is longer than 
.
fb
None
fb
fa
In the case of 
, the result of 
 will have the shape of the 
 tree, and it
Tree
zipR
fb
will have 
 on the  side only where the shapes of the two trees intersect.
Some(a)
A
In the chapter on streams and laziness we talked about how multiple passes over a
structure can be fused into one. In the chapter on monoids, we looked at how we
can use monoid products to carry out multiple computations over a foldable
structure in a single pass. Using products of applicative functors, we can likewise
fuse multiple traversals of a traversable structure.
EXERCISE 14: Use applicative functor products to write the fusion of two
traversals. This function will, given two functions  and , traverse 
 a single
f
g
fa
time, collecting the results of both functions at once.
Not only can we use composed applicative functors to fuse traversals, traversable
functors themselves compose. If we have a nested structure like 
 then we can traverse the map, the option, and the
Map[K,Option[List[V]]]
list at the same time and easily get to the  value inside, because 
, 
,
V
Map Option
and 
 are all traversable.
List
EXERCISE 15: Implement the composition of two 
 instances.
Traverse
def zipL[A,B](fa: F[A], fb: F[B]): F[(A, Option[B])] =
  (mapAccum(fa, toList(fb)) {
    case (a, Nil) => ((a, None), Nil)
    case (a, b :: bs) => ((a, Some(b)), bs)
  })._1
def zipR[A,B](fa: F[A], fb: F[B]): F[(Option[A], B)] =
  (mapAccum(fb, toList(fa)) {
    case (b, Nil) => ((None, b), Nil)
    case (b, a :: as) => ((Some(a), b), as)
  })._1
12.7.3 Traversal fusion
def fuse[M[_],N[_],A,B](fa: F[A])(f: A => M[B], g: A => N[B])
                       (M: Applicative[M], N: Applicative[N]):
                       (M[F[B]], N[F[B]])
12.7.4 Nested traversals
223
www.it-ebooks.info

Let's now return to the issue of composing monads. As we saw earlier in this
chapter, 
 instances always compose, but 
 instances do not.
Applicative
Monad
If you tried before to implement general monad composition, then you would have
found that in order to implement 
 for nested monads  and , you would
join
M
N
have to write something of a type like 
. And
M[N[M[N[A]]]] => M[N[A]]
that can't be written generically. But if  also happens to have a 
N
Traverse
instance, we can 
 to turn 
 into 
, leading to 
sequence
N[M[_]]
M[N[_]]
. Then we can join the adjacent  layers as well as the adjacent 
M[M[N[N[A]]]]
M
 layers using their respective 
 instances.
N
Monad
EXERCISE 16: Implement the composition of two monads where one of them
is traversable:
Expressivity and power often comes at the price of compositionality and
modularity. The issue of composing monads is often addressed with a
custom-written version of each monad that is specifically constructed for
composition. This kind of thing is called a 
. For example, the 
monad transformer
 monad transformer composes 
 with any other monad:
OptionT
Option
The 
 definition here maps over both the  and the 
, and
flatMap
M
Option
flattens structures like 
 to just 
.
M[Option[M[Option[A]]]]
M[Option[A]]
But this particular implementation is specific to 
. And the general strategy
Option
of taking advantage of 
 works only with traversable functors. To
Traverse
compose with 
 for example (which cannot be traversed), a specialized 
State
def compose[G[_]](implicit G: Traverse[G]):
    Traverse[({type f[x] = F[G[x]]})#f]
12.7.5 Monad composition
def composeM[M[_],N[_]](M: Monad[M], N: Monad[N], T: Traverse[N]):
    Monad[({type f[x] = M[N[x]]})#f]
case class OptionT[M[_],A](value: M[Option[A]])(implicit M: Monad[M]) {
  def flatMap[B](f: A => OptionT[M, A]): OptionT[M, B] =
    OptionT(value flatMap {
      case None => M.unit(None)
      case Some(a) => f(a).value
    })
}
224
www.it-ebooks.info

applicative functor laws
composition law
homomorphism law
identity law
interchange law
law of composition
law of homomorphism
law of identity
law of interchange
laws of applicative functors
monad transformer
 monad transformer has to be written. There is no generic composition
StateT
strategy that works for every monad.
See the chapter notes for more information about monad transformers.
Applicative functors are a very useful abstraction that is highly modular and
compositional. The functions 
 and 
 allow us to lift functions and values,
unit
map
while 
 and 
 give us the power to lift functions of higher arities. This
apply
map2
in turn enables 
 and 
 to be generalized to traversable
traverse
sequence
functors. Together, 
 and 
 let us construct complex
Applicative
Traverse
nested and parallel traversals out of simple elements that need only be written
once.
This is in stark contrast to the situation with monads, where each monad's
composition with others becomes a special case. It's a wonder that monads have
historically received so much attention and applicative functors have received so
little.
Index Terms
12.8 Summary
225
www.it-ebooks.info

13
In this chapter, we will introduce the I/O monad (usually written 'the 
 monad'),
IO
which extends what we've learned so far to handle 
, like writing to a
external effects
file, reading from a database, etc, in a purely functional way. The 
 monad will
IO
be important for two reasons:
It provides the most straightforward way of embedding imperative programming into FP,
while preserving referential transparency and keeping pure code separate from what we'll
call 
 code. We will be making an important distinction here in this chapter
effectful
between 
 and 
.
effects
side effects
It illustrates a key technique for dealing with external effects—using pure functions to
compute a 
 of an imperative computation, which is then executed by a
description
separate 
. Essentially, we are crafting an embedded domain specific language
interpreter
(EDSL) for imperative programming. This is a powerful technique we'll be using
throughout part 4; part of our goal is to equip you with the skills needed to begin crafting
your own descriptions for interesting effectful programs you are faced with.
We are going to work our way up to the 
 monad by first considering a very
IO
simple example, one we discussed in chapter 1 of this book:
External effects and I/O
13.1 Introduction
13.2 Factoring effects
case class Player(name: String, score: Int)
def printWinner(p: Player): Unit =
  println(p.name + " is the winner!")
def declareWinner(p1: Player, p2: Player): Unit =
  if (p1.score > p2.score) printWinner(p1)
  else printWinner(p2)
226
www.it-ebooks.info

In chapter 1, we factored out the logic for computing the winner into a function
separate from displaying the winner:
We commented in chapter 1 that it was 
 possible to factor an impure
always
function into a pure 'core' and two functions with side effects, (possibly) one that
produces the pure function's input and (possibly) one that accepts the pure
function's output. In this case, we factored the pure function 
 out of 
winner
—conceptually, 
 was doing 
, it
declareWinner
declareWinner
two things
was 
 the winner, and it was 
 the winner that was computed.
computing
displaying
Interestingly, we can continue this sort of factoring—the printWinner
function is also doing two things—it is computing a message, and then printing
that message to the console. We could factor out a pure function here as well,
which might be beneficial if later on we decide we want to display the winner
message in some sort of UI or write it to a file instead:
These might seem like silly examples, but the same principles apply in larger,
more complex programs and we hope you can see how this sort of factoring is
quite natural. We aren't changing what our program does, just the internal details of
how it is factored into smaller functions. The insight here is that inside every
. We can even
function with side effects is a pure function waiting to get out
formalize this a bit. Given an impure function of type 
, we can often split
A => B
this into two functions:1
Footnote 1mWe will see many more examples of this in this chapter and in the rest of part 4.
def printWinner(p: Player): Unit =
  println(p.name + " is the winner!")
def winner(p1: Player, p2: Player): Player =
  if (p1.score > p2.score) p1 else p2
def declareWinner(p1: Player, p2: Player): Unit =
  printWinner(winner(p1, p2))
def winnerMsg(p: Player): String =
  p.name + " is the winner!"
def printWinner(p: Player): Unit =
  println(winnerMsg(p))
227
www.it-ebooks.info

A 
 function 
, where  is some 
pure
A => D
D
description
An 
 function 
, which can be thought of as an 
 of these
impure
D => B
interpreter
descriptions
We will extend this to handle 'input' side effects shortly. For now, though,
consider applying this strategy repeatedly to a program. Each time we apply it, we
make more functions pure and push side effects to the outer layers of the program.
We sometimes call these impure functions the 'imperative shell' around the pure
core of the program. Eventually, we reach functions that seem to 
 side
necessitate
effects like the built-in 
, which has type 
. What do
println
String => Unit
we do then?
It turns out that even functions like 
 can be factored into a pure core, by
println
introducing a new data type we'll call 
:
IO
Our 
 function is now pure—it returns an 
 value, which
printWinner
IO
simply describes an action that is to take place without actually 
 it. We
executing
say that 
 has or produces an 
 or is 
, but it is only the
printWinner
effect
effectful
interpreter of 
 (its 
 function) that actually has 
 effects.
IO
run
side
Other than technically satisfying the requirements of FP, has the 
 type
IO
actually bought us anything? This is a subjective question, but as with any other
data type, we can access the merits of 
 by considering what sort of algebra it
IO
provides—is it something interesting, from which we can define a large number of
useful operations and assemble useful programs, with nice laws that give us the
ability to reason about what these larger programs will do? Not really. Let's look at
the operations we can define:
13.3 A simple IO type
trait IO { def run: Unit }
def PrintLine(msg: String): IO =
  new IO { def run = println(msg) }
def printWinner(p: Player): IO =
  PrintLine(winnerMsg(p))
trait IO { self => 
  def run: Unit
  def ++(io: IO): IO = new IO {
    def run = { self.run; io.run }
228
www.it-ebooks.info

The only thing we can say about 
 as it stands right now is that it forms a 
IO
 (
 is the identity, and 
 is the associative operation). So if we
Monoid empty
++
have for instance a 
, we can reduce that to an 
, and the associativity
List[IO]
IO
of 
 means we can do this by folding left or folding right. On its own, this isn't
++
very interesting. All it seems to have given us is the ability to delay when a side
effect gets 'paid for'.
Now we will let you in on a secret—you, as the programmer, get to invent
whatever API you wish to represent your computations, including those that
interact with the universe external to your program. This process of crafting
pleasing, useful, and composable descriptions of what you want your programs to
do is at its core 
. You are crafting a little language and an
language design
associated 
 that will allow you to express various programs. If you don't
interpreter
like something about this language you have created, change it! You should
approach this task just like any other combinator library design task, and by now
you've had plenty of experience designing and writing such libraries.
As we have seen many times before, sometimes, when building up your little
language, you will encounter a program that cannot be expressed. So far, our IO
type can represent only 'output' effects. There is no way to express IO
computations that must, at various points, wait for input from some external
source. Suppose we would like to write a program that prompts the user for a
temperature in degrees fahrenheit, then converts this value to celsius and echoes it
to the user. A typical imperative program might look something like:2
Footnote 2mWe are not doing any sort of error handling here. This is just meant to be an illustrative example.
  }
}
object IO {
  def empty: IO = new IO { def run = () }
}
13.3.1 Handling input effects
def farenheitToCelsius(f: Double): Double =
  (f - 32) * 5.0/9.0
def converter: Unit = {
  println("Enter a temperature in degrees fahrenheit: ")
  val d = readLine.toDouble
  println(fahrenheitToCelsius(d))
}
229
www.it-ebooks.info

Unfortunately, we run into problems if we want to make 
 into a
converter
pure function that returns an 
:
IO
In Scala, 
 is a 
 with a side effect of capturing a line of input
readLine
def
from the console. It returns a 
. We could wrap a call to this in an 
, but
String
IO
we have nowhere to put the result! We don't yet have a way of representing this
sort of effect with our current 
 type. The problem is we cannot express 
IO
IO
computations that 
 of some meaningful type—our interpreter of 
yield a value
IO
just produces 
 as its output. Should we give up on our 
 type and resort to
Unit
IO
using side effects? Of course not! We extend our 
 type to allow 
, by adding
IO
input
a type parameter to 
:
IO
An 
 computation can now return a meaningful value. Notice we've added 
IO
 and 
 functions so 
 can be used in for-comprehensions. And 
map
flatMap
IO
IO
now forms a 
:
Monad
syntax for IO ..
def fahrenheitToCelsius(f: Double): Double =
  (f - 32) * 5.0/9.0
def converter: IO = {
  val prompt: IO = PrintLine(
    "Enter a temperature in degrees fahrenheit: ")
  // now what ???
}
trait IO[+A] { self => 
  def run: A
  def map[B](f: A => B): IO[B] =
    new IO[B] { def run = f(self.run) }
  def flatMap[B](f: A => IO[B]): IO[B] =
    new IO[B] { def run = f(self.run).run }
}
object IO extends Monad[IO] {
  def unit[A](a: => A): IO[A] = new IO[A] { def run = a }
  def flatMap[A,B](fa: IO[A])(f: A => IO[B]) = fa flatMap f
  def apply[A](a: => A): IO[A] = unit(a)
}
230
www.it-ebooks.info

We can now write our 
 example:
converter
Our 
 definition no longer has side effects—it is a RT 
converter
description
of a computation that will have side effects when interpreted via 
. And because it forms a 
, we can use all the
converter.run
Monad
combinators we've written previously. Here are some other example usages of 
:
IO
val echo = ReadLine.flatMap(PrintLine): An 
 that reads a line from the
IO[Unit]
console and echoes it back.
val readInt = ReadLine.map(_.toInt): An 
 that parses an 
 by reading a
IO[Int]
Int
line from the console.
val readInts = readInt ** readInt: An 
 that parses an 
IO[(Int,Int)]
(Int,Int)
by reading two lines from the console.
replicateM_(5)(converter): An 
 that will repeat 
 5 times,
IO[Unit]
converter
discarding the results (which are just 
).  We can replace 
 here with any 
Unit 3
converter
IO
action we wished to repeat 5 times (for instance, 
 or 
).
echo
readInts
Footnote 3mRecall that 
 is the same as 
.
replicateM(3)(fa)
sequence(List(fa,fa,fa))
replicateM(10)(ReadLine): An 
 that will read 10 lines from the
IO[List[String]]
console and return the list of results.
Here's a larger example, an interactive program which prompts the user for
input in a loop, then computes the factorial of the input. Here's an example run:
This code uses a few 
 functions we haven't seen yet, 
, 
,
Monad
when foreachM
def ReadLine: IO[String] = IO { readLine }
def PrintLine(msg: String): IO[Unit] = IO { println(msg) }
def converter: IO[Unit] = for {
  _ <- PrintLine("Enter a temperature in degrees fahrenheit: ")
  d <- ReadLine.map(_.toDouble)
  _ <- PrintLine(fahrenheitToCelsius(d).toString)
} yield ()
The Amazing Factorial REPL, v2.0
q - quit
<number> - compute the factorial of the given number
<anything else> - bomb with horrible error
3
factorial: 6
7
factorial: 5040
q
231
www.it-ebooks.info

and 
, discussed in the sidebar below. For the full listing, see the
sequence_
associated chapter code. The details of this code aren't too important; the point here
is just to demonstrate how we can embed an imperative programming language
into the purely functional subset of Scala. All the usual imperative programming
tools are here—we can write loops, perform I/O, and so on. If you squint, it looks a
bit like normal imperative code.
Imperative factorial using a mutable IO reference
Allocation a mutable reference
Modify reference in a loop
Dereference to obtain the value inside a reference
See sidebar
def factorial(n: Int): IO[Int] = for {
  acc <- ref(1)
  _ <- foreachM (1 to n toStream) (i => acc.modify(_ * i).skip)
  result <- acc.get
} yield result
val factorialREPL: IO[Unit] = sequence_(
  IO { println(helpstring) },
  doWhile { IO { readLine } } { line =>
    val ok = line != "q"
    when (ok) { for {
      n <- factorial(line.toInt)
      _ <- IO { println("factorial: " + n) }
    } yield () }
  })
232
www.it-ebooks.info

SIDEBAR
Additional monad combinators
The above example makes use of some monad combinators we haven't
seen before, although they can be defined for any 
. You may
Monad
want to think about what these combinators mean for types other than 
. Notice that not all these combinators make sense for every monadic
IO
type. (For instance, what does 
 mean for 
? For 
forever
Option
?)
Stream
We don't necessarily endorse writing code this way.  What this does
4
demonstrate, however, is that FP is not in any way limited in its
expressiveness—any program that can be expressed can be expressed in FP, even
if that functional program is a straightforward embedding of the imperative
program into the 
 monad.
IO
Footnote 4mIf you have a monolithic block of impure code like this, you can always just write a definition
which performs actual side effects then wrap it in 
—this will be more efficient, and the syntax is nicer than
IO
what is provided using a combination of for-comprehension syntax and the various 
 combinators.
Monad
def when[A](b: Boolean)(fa: => F[A]): F[Boolean] =
  if (b) as(fa)(true) else unit(false)
def doWhile[A](a: F[A])(cond: A => F[Boolean]): F[Unit] = for {
  a1 <- a
  ok <- cond(a1)
  _ <- if (ok) doWhile(a)(cond) else unit(())
} yield ()
def forever[A,B](a: F[A]): F[B] = {
  lazy val t: F[B] = forever(a)
  a flatMap (_ => t)
}
def foreachM[A](l: Stream[A])(f: A => F[Unit]): F[Unit] =
  foldM_(l)(())((u,a) => skip(f(a)))
def foldM_[A,B](l: Stream[A])(z: B)(f: (B,A) => F[B]): F[Unit] =
  skip { foldM(l)(z)(f) }
def foldM[A,B](l: Stream[A])(z: B)(f: (B,A) => F[B]): F[B] =
  l match {
    case h #:: t => f(z,h) flatMap (z2 => foldM(t)(z2)(f))
    case _ => unit(z)
  }
233
www.it-ebooks.info

The 
 monad is a kind of least common denominator for expressing programs
IO
with external effects. Its usage is important mainly because it clearly separates
, forcing us to be honest about where interactions with
pure code from impure code
the outside world are occurring and also encouraging the beneficial factoring of
effects that we discussed earlier. When programming 
 the 
 monad, we
within
IO
have many of the same problems and difficulties as we would in ordinary
imperative programming, which has motivated functional programmers to search
for more composable ways of describing effectful programs.  Nonetheless, the 
5
IO
monad does provide some real benefits:
Footnote 5mAs we'll see in chapter 15, we don't need to give up on all the nice compositionality we've come
to expect from FP just to interact with the outside world.
IO computations are ordinary 
. We can store them in lists, pass them to functions,
values
create them dynamically, etc. Any common pattern we notice can be wrapped up in a
function and reused. This is one reason why it is sometimes argued that functional
languages provide 
, as compared to
more powerful tools for imperative programming
languages like C or Java.
Reifying 
 computations as values means we can craft a more interesting interpreter
IO
than the simple 
-based "interpreter" baked into the 
 type itself. Later on in this
run
IO
chapter, we will build a more refined 
 type and sketch out an interpreter that uses
IO
nonblocking I/O in its implementation. Interestingly, client code like our converter
example remains identical—we do not expose callbacks to the programmer at all! They
are entirely an implementation detail of our 
 interpreter.
IO
SIDEBAR
IO computation reuse in practice
In practice, the amount of reuse we get by factoring out common
patterns of 
 usage is limited (you will notice this yourself if you start
IO
writing programs with 
). An 
 is a completely opaque
IO
IO[A]
description of a computation that yields a single . Most of the general
A
purpose combinators for 
 are functions that can be defined for any
IO
monad—
 itself is not contributing anything new, which means we
IO
only have the monad laws to reason with.
What this means in practice is that we generally need to know
something more about an 
 than just its type to compose it with
IO[A]
other computations (contrast this with, say, 
). Reuse is
Stream[A]
extremely limited as a result.
13.3.2 Has this bought us anything?
234
www.it-ebooks.info

In this section, we are going to explore what an 
 
 and introduce a
IO[A] means
more nuanced 
 type that clarifies what is actually going on in a functional
IO
program that performs I/O. A primary purpose here is to get you thinking, and to
make it clear how it is possible to craft more interesting interpreters of 
 than the
IO
simple one we baked into the 
 function. Don't worry too much about following
run
absolutely everything in this section.
With that said, let's see what we can learn about the meaning of 
. You may
IO
have noticed that our 
 type is roughly equivalent to our 
 type, introduced in
IO
Id
chapter 13 as the simplest possible type that gives rise to a monad. Let's compare
the two types side-by-side:
Aside from the fact that 
 is non-strict, and that retrieving the value is done
IO
using 
 instead of 
, the types are identical! Our "IO" type is just a
run
value
non-strict value. This is rather unsatisfying. What's going on here?
We have actually cheated a bit with our 
 type. We are relying on the fact that
IO
Scala allows unrestricted side effects at any point in our programs. But let's think
about what happens when we evaluate the 
 function of an 
. During
run
IO
evaluation of 
, the pure part of our program will occasionally make requests of
run
the outside world (like when it invokes 
), wait for a result, and then
readLine
pass this result to some further pure computation (which may subsequently make
some further requests of the outside world). Our current 
 type is completely
IO
inexplicit about where these interactions are occurring. But we can model these
interactions more explicitly if we choose:
This type separates the pure and effectful parts of an 
 computation. We'll get
IO
to writing its 
 instance shortly. An 
 can be a pure value, or it can be
Monad
IO[A]
a request of the external computation. The type 
 defines the 
External
protocol
13.4 The meaning of the IO type
case class Id[+A](value: A)
trait IO[+A] { def run: A }
trait IO[+A]
case class Pure[+A](a: A) extends IO[A]
case class Request[I,+A](expr: External[I],
                         receive: I => IO[A]) extends IO[A]
235
www.it-ebooks.info

—it encodes what possible external requests our program can make. We can think
of 
 much like an expression of type , but it is an expression that is
External[I]
I
"external" that must be evaluated by whatever program is running this 
 action.
IO
The 
 function defines what to do when the result of the request becomes
receive
available, it is sometimes called the 
. We'll see a bit later how this can
continuation
be exploited to write an interpreter for 
 that uses nonblocking I/O internally.
IO
The simplest possible representation of 
 would be simply a
External[I]
nonstrict value:6
Footnote 6mSimilar to our old 
 type!
IO
This implies that our 
 type can call absolutely any impure Scala function,
IO
since we can wrap any expression at all in 
 (for instance, 
Delay
Delay {
). If we want to restrict access to only certain
println("Side effect!") }
functions, we can parameterize our 
 type on the choice of 
.
IO
External 7
Footnote 7mOf course, Scala will not technically prevent us from invoking a function with side effects at any
point in our program. This discussion assumes we are following the discipline of not allowing side effects unless
this information is tracked in the type.
We have renamed 
 to just  here. With this representation, we can
External
F
define an  type that grants access to exactly the effects we want:
F
Now an 
 is an 
 computation that can only read from and
IO[Console,A]
IO
write to the console.  We can introduce other types for different I/O capabilities—a
8
file system , granting read/write access (or even just read access) to the file
F
trait Runnable[A] { def run: A }
object Delay { def apply[A](a: => A) = new Runnable[A] { def run = a }}
trait IO[F[_], +A]
case class Pure[F[_], +A](get: A) extends IO[F,A]
case class Request[F[_], I, +A](
    expr: F[I],
    receive: I => IO[F,A]) extends IO[F,A]
trait Console[A]
case object ReadLine extends Console[Option[String]]
case class PrintLine(s: String) extends Console[Unit]
236
www.it-ebooks.info

system, a network  granting the ability to open network connections and read
F
from them, and so on. Notice, interestingly, that nothing about 
 implies
Console
that any side effects must actually occur! That is a property of the 
 of 
interpreter
F
values now required to actually run an 
:
IO[F,A]
Footnote 8m
A completely valid 
 could ignore 
 requests and
Run[Console]
PrintLine
always return the string 
 in response to 
 requests:
"Hello world!"
ReadLine
Ignored!
While the real 
 could actually execute the effects:
Run[Console]
def printWinner(p: Player): Unit =
  println(p.name + " is the winner!")
def winner(p1: Player, p2: Player): Player =
  if (p1.score > p2.score) p1 else p2
def declareWinner(p1: Player, p2: Player): Unit =
  printWinner(winner(p1, p2))
trait Run[F[_]] {
  def apply[A](expr: F[A]): (A, Run[F])
}
object IO {
  @annotation.tailrec
  def run[F[_],A](R: Run[F])(io: IO[F,A]): A = io match {
    case Pure(a) => a
    case Request(expr,recv) =>
      R(expr) match { case (e,r2) => run(r2)(recv(e)) }
  }
}
object RunConsoleMock extends Run[Console] {
  def apply[A](c: Console[A]) = c match {
    case ReadLine => (Some("Hello world!"), RunConsoleMock)
    case PrintLine(_) => ((), RunConsoleMock)
  }
}
object RunConsole extends Run[Console] {
237
www.it-ebooks.info

EXERCISE 1: Give the 
 instance for 
.  You may want to
Monad
IO[F,_] 9
override the default implementations of various functions with more efficient
versions.
Footnote 9mNote we must use the same trick discussed in chapter 10, to partially apply the 
 type
IO
constructor.
EXERCISE 2: Implement a 
 which uses elements from a 
Run[Console]
 to generate results from calls to 
 (it can ignore 
List[String]
ReadLine
 calls):
PrintLine
EXERCISE 3: Our definition of 
 is overly specific. We only need a 
Run
 to implement 
! Implement 
 given an arbitrary 
.
Monad[F]
run
run
Monad[F] 10
With this definition, the interpreter is no longer tail recursive, and it is up to the 
 to ensure that the chains of 
 calls built up during
Monad[F]
flatMap
interpretation do not result in stack overflows. We will discuss this further in the
next section.
Footnote 10mNote on this signature: when passing a 
 or 
 to a function, we typically
Monad[F]
Monad[G]
give the variable the same name as its type parameter (  or  in this case). We can then write 
F
G
 almost as if  were the companion object of a type .
F.map(expr)(f)
F
F
These last two exercises demonstrate something rather amazing—there is
nothing about our 
 type nor our 
 or  which require side effects of any
IO
Run[F]
F
  def apply[A](c: Console[A]) = c match {
    case ReadLine =>
      val r = try Some(readLine) catch { case _ => None }
      (r, RunConsole)
    case PrintLine(s) => (println(s), RunConsole)
  }
}
def monad[F[_]] = new Monad[({ type f[a] = IO[F,a]})#f] {
  ...
}
def console(lines: List[String]): Run[Console]
def run[F[_],A](F: Monad[F])(io: IO[F,A]): F[A]
238
www.it-ebooks.info

kind. From the perspective of our 
 programs, we cannot even 
 an
IO
distinguish
interpreter that uses 'real' side effects from one like 
 that doesn't. 
console
IO
values are simply pure computations that may occasionally make requests of some
interpreter that exists outside the universe established by the program.
With our 
 type, the behavior of this external interpreter is modeled only in a
IO
very opaque way—we say that each evaluation we request from the interpreter
results in a new interpreter state and beyond that make no assumptions about how
the sequence of requests made will affect future behavior.
 As we will learn in
11
chapter 15, creating more composable abstractions for dealing with I/O and other
effects is all about designing types that codify additional assumptions about the
nature of the interactions between the 'internal' and 'external' computations.
Footnote 11mOur definition for an arbitrary 
 is telling us the same thing, since a 
Monad[F]
Monad[F]
provides us with a strategy for sequencing programs, but makes no additional assumptions about the nature of
this sequencing.
In this section we will develop an 
 type and associated interpreter that uses
IO
nonblocking I/O internally. We will also demonstrate a general technique, called 
, for avoiding stack overflow errors in monadic code.
 Trampolining
trampolining
12
is particularly important for 
 since 
 is often used for defining top-level
IO
IO
program loops and other large or long-running computations that are susceptible to
excessive stack usage.
Footnote 12mSee chapter notes for more discussion and links to further reading on this.
Our current 
 type will stack overflow for some programs. In this section, we will
IO
develop an 
 type and a 
 function that uses 
. By this we
IO
run
constant stack space
mean that the stack usage of 
 is not dependent on the input in any way. To
run
start, let's look at a few examples of 
 programs that can be problematic. We'll
IO
make use of a helper function, 
, which constructs an 
IO.apply
 from a nonstrict :
IO[Runnable,A]
A
13.5 A realistic I/O data type and interpreter
13.5.1 A trampolined IO type and a tail-recursive interpreter
object IO {
  ...
  def apply[A](a: => A): IO[Runnable,A] =
    Request(Delay(a), (a:A) => Pure(a))
}
239
www.it-ebooks.info

Here are some perfectly reasonable 
 programs that can present problems:
IO
Bring Monad combinators into scope
Generates a large list of random values
Prints the numbers from 1 to 100,000
Prints "hi" in an infinite loop
Prints out "hi" 100,000 times, but the IO action is built up using a left fold so that
calls to flatMap are associated to the left
EXERCISE 4 (hard): Try running these examples for your current
implementation of 
, and also (optional) for the simple 
 type we defined first
IO
IO
in this chapter. For those that fail, can you explain why? It can be a little difficult
to tell what's going on just by looking at the stack trace. You may want to instead
trace on paper the evaluation of these expressions until a pattern becomes clear.
To fix the problem, we are going to build what is called a 
 version
trampolined
of 
. To illustrate the idea, let's first look at the type 
.
IO
Trampoline 13
Footnote 13mWe are not going to work through derivation of this type, but if you are interested, follow the
references in the chapter notes for more information.
What is 
? First, note that we can extract an 
 from a 
Trampoline
A
 using a tail-recursive function, 
:
Trampoline[A]
run
EXERCISE 5: Write the tail-recursive function, 
, to convert a 
run
 to an 
. Just follow the types—there is only one
Trampoline[A]
A
implementation that typechecks!
val F = monad[Runnable]; import F._
val ex1 = sequence_(Stream.fill(100000)(IO { math.random }))
val ex2 = foreachM(Stream.range(1,100000))(i => IO { println(i) })
val ex3 = forever(IO { println("hi") })
val ex4 = Stream.fill(100000)("hi").foldLeft(IO { () })(
  (acc,s) => acc *> IO { println(s) })
trait Trampoline[+A]
case class Done[+A](get: A) extends Trampoline[A]
case class More[+A](force: () => Trampoline[A]) extends Trampoline[A]
case class Bind[A,+B](force: () => Trampoline[A],
                      f: A => Trampoline[B]) extends Trampoline[B]
@annotation.tailrec
def run[A](t: Trampoline[A]): A
240
www.it-ebooks.info

So a 
 can be used in place of . But even more interestingly,
Trampoline[A]
A
 forms a 
, and if implemented correctly, arbitrary monadic
Trampoline
Monad
expressions can be guaranteed not to stack overflow.
EXERCISE 6 (hard): Implement 
. (You may want to
Monad[Trampoline]
add 
 and 
 methods to 
.) A valid implementation will
map
flatMap
Trampoline
never evaluate 
 or call 
 in the implementation of 
 or 
.
force()
run
flatMap
map
Can you see how it works? 
 lets us return control to the 
 function, and
More
run
 represents a call to 
 as an ordinary value. If implemented
Bind
flatMap
correctly, 
 will always return control to the 
 function after a constant
flatMap
run
number of steps, and the 
 function is guaranteed to make progress.
run
EXERCISE 7 (hard, optional): Show that 
 always returns after doing
flatMap
a constant amount of work, and that 
 will always call itself after a single call to
run
.
force()
Can you see why this is called trampolining? The name comes from the fact
that interpretation of a trampolined program bounces back and forth between the
central 
 loop and the functions being called. 
 is essentially
run
Trampoline
providing an interpreter for function application that uses the heap instead of the
usual call stack. The key to making it work is that we reify 
 calls as data,
flatMap
but associate these calls to the right, which lets us implement the 
 function as a
run
tail-recursive function. There is some overhead to using it, but its advantage is that
we gain predictable stack usage.14
Footnote 14mThere are some interesting optimizations to this scheme—it isn't necessary to return to the
central loop after every function call, only periodically to avoid stack overflows. See the chapter notes for more
information.
We can now add the same trampolining behavior directly to our 
 type:
IO
trait IO[F[_], +A]
case class Pure[F[_], +A](get: A) extends IO[F,A]
case class Request[F[_],I,+A](
    expr: F[I],
    receive: I => IO[F,A]) extends IO[F,A]
case class BindMore[F[_],A,+B](
    force: () => IO[F,A],
    f: A => IO[F,B]) extends IO[F,B]
case class BindRequest[F[_],I,A,+B](
    expr: F[I], receive: I => IO[F,A],
    f: A => IO[F,B]) extends IO[F,B]
case class More[F[_],A](force: () => IO[F,A]) extends IO[F,A]
241
www.it-ebooks.info

We have a 
 constructor, as before, but we now have two additional 
More
Bind
constructors.
EXERCISE 8: Implement both versions of 
 for this new version of 
.
run
IO
EXERCISE 9 (hard): Implement 
 for this new version of 
. Once
Monad
IO
again, a correct implementation of 
 will not invoke 
 or 
.
flatMap
run
force()
Test your implementation using the examples we gave above. Can you see why it
works? Again, you may want to try tracing their execution until the pattern
becomes clear, or even construct a proof that stack usage is guaranteed to be
bounded.
SIDEBAR
Enabling infix monadic syntax for IO
When compiling an expression like 
, if  does not have a
a.map(f)
a
function 
, Scala will look in the companion object of the  type for
map
a
an implicit conversion to some type with a 
 function and call that if
map
such a conversion exists. We can use this to expose infix syntax for all
the various 
 combinators—we just add the following function to
Monad
the 
 companion object:
IO
With this definition in the companion object, we we can write
expressions like 
, 
, 
a replicateM 10 a as "hello!" (a map2
 for any 
 values  and .
b)(_ + _)
IO
a
b
The  type used for 
 frequently includes operations that can take a long time to
F
IO
complete and which do not occupy the CPU, like accepting a network connection
from a server socket, reading a chunk of bytes from an input stream, writing a large
number of bytes to a file, and so on. Our current 
 
 function will wait idly for
IO run
these operations to complete:
def run[F[_],A](R: Run[F])(io: IO[F,A]): A
def run[F[_],A](F: Monad[F])(io: IO[F,A]): F[A]
def monad[F[_]]: Monad[({ type f[x] = IO[F,x]})#f]
implicit def toMonadic[F[_],A](a: IO[F,A]) = monad[F].toMonadic(a)
13.5.2 A nonblocking I/O interpreter
242
www.it-ebooks.info

Might take a long time
We can do better. There are I/O libraries that support 
 I/O. The
nonblocking
details of these libraries vary, but to give the general idea, a nonblocking 
 of
source
bytes might have an interface like this:
Here, it is assumed that 
 returns immediately. We give 
requestBytes
 a callback function indicating what to do when the result
requestBytes
becomes available or an error is encountered. Using this sort of library directly is
quite painful, for obvious reasons.15
Footnote 15mEven this API is rather nicer than what is offered directly by the 
 package in Java (
nio
API link
), which supports nonblocking I/O.
One of the nice things about making I/O computations into values is that we
can build more interesting interpreters for these values than the default 'interpreter'
which performs I/O actions directly. Here we will sketch out an interpreter that
uses nonblocking I/O internally. The ugly details of this interpreter are completely
hidden from code that uses the 
 type, and code that uses 
 can be written in a
IO
IO
much more natural style without explicit callbacks.
Recall that earlier, we implemented a function with the following signature:
We are going to simply choose 
 for our , though we will be using a
Future
F
different definition than the 
 introduced in
java.util.concurrent.Future
chapter 7. The version we use here can be backed by a nonblocking I/O primitive.
It is trampolined, as we've seen before.16
case Request(expr, recv) =>
  val (expensive, r2) = R(expr)
  run(r2)(recv(expensive))
trait Source {
  def requestBytes(
    nBytes: Int,
    callback: Either[Throwable, Array[Byte]] => Unit): Unit
}
def run[F[_],A](F: Monad[F])(io: IO[F,A]): F[A]
243
www.it-ebooks.info

Footnote 16mThis definition can be extended to handle timeouts, cancellation, and deregistering callbacks.
We won't be discussing these extensions here, but you may be interested to explore this on your own. See the 
 for an extension to 
 supporting proper error handling.
Task.scala
Future
Future looks almost identical to 
, except we have replaced the 
IO
Request
and 
 cases with 
 and 
 constructors. The 
BindRequest
Later
BindLater
 function of 
 is presumed to have some side effect—perhaps
listen
Later
adding 
 to a mutable list of listeners to notify when the result becomes
callback
available. We will only be using it here as an implementation detail of the run
function for 
 (this could be enforced using access modifiers).
IO
EXERCISE 10 (hard, optional): Implement 
. We will need it
Monad[Future]
to implement our nonblocking 
 interpreter. Also implement 
, for an
IO
runAsync
asynchronous evaluator for 
, and 
, the synchronous evaluator:
Future
run
With this in place, we can asynchronously evaluate any 
 to a 
IO[Future,A]
. But what do we do if we have an 
, say, and we
Future[A]
IO[Console,A]
wish to evaluate it asynchronously to a 
? We need a way to translate
Future[A]
between 
 and 
, which we can think of as a
IO[Console,A]
IO[Future,A]
form of compilation—we are replacing the abstract requests for 
 with
Console
concrete requests that will actually read from and write to the standard input and
output streams. We will use the following 
:
trait
trait Future[+A]
object Future {
  case class Now[+A](get: A) extends Future[A]
  case class Later[+A](listen: (A => Unit) => Unit) extends Future[A]
  case class More[+A](force: () => Future[A]) extends Future[A]
  case class BindLater[A,+B](listen: (A => Unit) => Unit,
                             f: A => Future[B]) extends Future[B]
  case class BindMore[A,+B](force: () => Future[A],
                            f: A => Future[B]) extends Future[B]
}
def runAsync[A](a: Future[A])(onFinish: A => Unit): Future[Unit]
def run[A](a: Future[A]): A
trait Trans[F[_], G[_]] {
  def apply[A](f: F[A]): G[A]
}
244
www.it-ebooks.info

EXERCISE 11: Implement a version of 
 which translates from  to  as it
run
F
G
interprets the 
 action:
IO
With this in place, we can now write the following to asynchronously evaluate
an 
:
IO[Console,A]
That's it! In general, for any  type, we just require a way to translate that  to a
F
F
 and we can then evaluate 
 programs asynchronously. Let's look
Future
IO[F,A]
at some possible definitions of 
:
RunConsole
First, if we wish, we can always implement 
 using ordinary
RunConsole
blocking I/O calls:17
Footnote 17mRecall that 
 doesn't do anything special with its argument—the argument will
Future.unit
still be evaluated in the main thread using ordinary function calls.
But because we are returning a 
, we have more flexibility. Here, we
Future
delegate 
 interpretation to a shared thread pool, to avoid blocking the
ReadLine
main computation thread while waiting for input. Notice that nothing requires us to
implement nonblocking calls everywhere in our interpreter—
 uses the
PrintLine
ordinary blocking 
 call directly (with the assumption that submitting this
println
as a task to a thread pool is not worth the overhead).
def run[F[_],G[_],A](T: Trans[F,G])(G: Monad[G])(io: IO[F,A]): G[A]
val RunConsole: Trans[Console,Future] = ...
val prog: IO[Console,A] = ...
val result: Future[A] = run(RunConsole)(Future)(prog)
object RunConsole extends Trans[Console,Future] {
  def apply[A](c: Console[A]): Future[A] =
    c match {
      case ReadLine => 
        Future.unit {
          try Some(readLine)
          catch { case _: Exception => None }
        }
      case PrintLine(a) =>
        Future.unit { println(a) }
    }
}
245
www.it-ebooks.info

In the 
 companion object, we define an 
 function, which
Future
apply
evaluates its argument in a thread pool. It returns a 
 which notifies any
Later
listeners once the result is available. The implementation is somewhat involved;
see the answer code for this chapter if you are interested.
EXERCISE 12 (hard, optional): Going one step further, we can use an API that
directly supports nonblocking I/O. We are not going to work through such an
implementation here, but you may be interested to explore this on your own by
building off the 
 library (
. As a start, try implementing an
java.nio
API link
asynchronous read from an 
 (
).
AsynchronousFileChannel API link 18
Footnote 18mThis requires Java 7.
What is remarkable is that regardless of the implementation, we get to retain the
same straightforward style in code that uses 
, rather than being forced to
IO
program with callbacks directly.
object RunConsole extends Trans[Console,Future] {
  def apply[A](c: Console[A]): Future[A] =
    c match {
      case ReadLine => 
        Future {
          try Some(readLine)
          catch { case _: Exception => None }
        }
      case PrintLine(a) =>
        Future.unit { println(a) }
    }
}
object Future {
  ...
  def apply[A](a: => A): Future[A] = { ... }
}
def read(file: AsynchronousFileChannel,
         fromPosition: Long,
         nBytes: Int): Future[Either[Throwable, Array[Byte]]]
246
www.it-ebooks.info

Despite the fancy implementation of our 
 interpreter and the advantage of
IO
having first-class 
 values, the 
 type fundamentally present us the same level
IO
IO
of abstraction as ordinary imperative programming. This means that writing
efficient, streaming I/O will generally involve monolithic loops.
Let's look at an example. Suppose we wanted to write a program to convert a
file, 
, containing a sequence of temperatures in degrees
fahrenheit.txt
fahrenheit, separated by blank lines, to a new file, 
, containing the
celsius.txt
same temperatures in degrees celsius. An  type for this might look something
F
like:19
Footnote 19mWe are ignoring exception handling in this API.
This works, although it requires loading the contents of fahrenheit.txt
entirely into memory to work on it, which could be problematic if the file is very
large. We would prefer to perform this task using roughly constant memory—read
a line or a fixed size buffer full of lines from 
, convert to
farenheit.txt
celsius, dump to 
, and repeat. To achieve this efficiency we could
celsius.txt
expose a lower-level file API that gives access to I/O handles:
13.6 Why the IO type is insufficient for streaming I/O
trait Files[A]
case class ReadLines(file: String) extends Files[List[String]]
case class WriteLines(file: String, lines: List[String])
  extends Files[Unit]
for {
  lines <- request(ReadLines("fahrenheit.txt"))
  cs = lines.map(s => fahrenheitToCelsius(s.toDouble).toString)
  _ <- request(WriteLines("celsius.txt", cs))
} yield ()
trait Files[A]
case class OpenRead(file: String) extends Files[HandleR]
case class OpenWrite(file: String) extends Files[HandleW]
case class ReadLine(h: HandleR) extends Files[Option[String]]
case class WriteLine(h: HandleW, line: String) extends Files[Unit]
trait HandleR
trait HandleW
247
www.it-ebooks.info

The only problem with this is we now need to write a monolithic loop:
There's nothing inherently wrong with writing a monolithic loop like this, but
it's not composable. Suppose we decide later that we'd like to compute a 5-element
moving average of the temperatures. Modifying our 
 function to do this
loop
would be somewhat painful. Compare that to the equivalent change we'd make to
our 
-based code, where we could define a 
 function and just
List
movingAvg
stick it before or after our conversion to celsius:
Even 
 could be composed from smaller pieces—we could build it
movingAvg
using a generic combinator, 
:
windowed 20
Footnote 20mThis can actually be implemented for any 
. Also, if we have a 
 instead of just a
Foldable
group
monoid it can be implemented more efficiently. A group defines an additional operation, negation, for which 
.
op(a,neg(a)) == zero
The point to all this is that programming with a composable abstraction like 
 is much nicer than programming directly with the primitive I/O operations.
List
Lists are not really special—they are just one instance of a composable API that is
pleasant to use. We should not have to give up all the nice compositionality we've
def loop(f: HandleR, c: HandleW): IO[Files,Boolean] = for {
  line <- request(ReadLine(f))
  b <- line.map(s =>
    request(WriteLine(c,
            fahrenheitToCelsius(s.toDouble).toString)) as true) .
    getOrElse (IO.unit(false))
} yield b
def convertFiles = for {
  f <- request(OpenRead("fahrenheit.txt"))
  c <- request(OpenWrite("celsius.txt"))
  _ <- while_(loop(f,c))
} yield ()
def movingAvg(n: Int)(l: List[Double]): List[Double]
cs = movingAvg(5)(lines.map(s => fahrenheitToCelsius(s.toDouble))).
     map(_.toString)
def windowed[A](n: Int, l: List[A])(f: A => B)(m: Monoid[B]): List[B]
248
www.it-ebooks.info

..
continuation
effect scoping
trampolining
unobservable
come to expect from FP just to write programs that make use of efficient,
streaming I/O.
 Luckily we don't have to. As we will see in chapter 15, we get to
21
build 
 abstractions we want for creating computations that perform I/O. If
whatever
we like the metaphor of lists or streams, we can find a way to encode a list-like
API for expressing I/O computations. If we invent or discover some other
composable abstraction, we can often find some way of using that.
Footnote 21mOne might ask—could we just have various 
 operations return the 
 type we
Files
Stream
defined in chapter 5? This is called 
, and it is problematic for several reasons we'll discuss more in
lazy I/O
chapter 15.
This chapter introduced the simplest model for how external effects and I/O can be
handled in a purely functional way. We began with a discussion of effect-factoring
and demonstrated how effects can be moved to the outer layers of a program. From
there, we defined two 
 data types, one with a simple interpreter built into the
IO
type, and another which made interactions with the external universe more explicit.
This second representation allowed us to implement a more interesting interpreter
of 
 values that used nonblocking I/O internally.
IO
The 
 monad is not the final word in writing effectful programs. It is
IO
important because it represents a kind of lowest common denominator. We don't
normally want to program with 
 directly, and in chapter 15 we will discuss how
IO
to build nicer, more composable abstractions.
Before getting to that, in our next chapter, we will apply what we've learned so
far to fill in the other missing piece of the puzzle: 
. At various places
local effects
throughout this book, we've made use of local mutation rather casually, with the
assumption that these effects were not 
. Next chapter we explore what
observable
this means in more detail, show more example usages of local effects, and show
how 
 can be enforced by the type system.
effect scoping
Index Terms
13.7 Conclusion
249
www.it-ebooks.info

14
In the first chapter of this book we introduced the concept of referential
transparency, setting the premise for purely functional programming. We declared
that pure functions cannot mutate data in place or interact with the external world.
In the previous chapter on I/O, we learned that this is not exactly true. We can
write purely functional and compositional programs that describe interactions with
the outside world. These programs are unaware that they can be interpreted with an
evaluator that has side-effects.
In this chapter, we will develop a more mature concept of referential
transparency. We will consider the idea that effects can occur 
 inside an
locally
expression, and that we can guarantee that no other part of the larger program can
observe these effects occurring.
We will also introduce the idea that expressions can be referentially transparent 
 some programs and not others.
with regard to
Up until this point, you may have had the impression that in purely functional
programming we're not allowed to use mutable state. But if we look carefully,
there is nothing about the definitions of referential transparency and purity that
disallows mutation of 
 state. Let's remind ourselves of our definitions from
local
chapter 1:
Local effects and mutable state
14.1 Introduction
14.2 Purely functional mutable state
250
www.it-ebooks.info

SIDEBAR
Definition of referential transparency and purity
An expression  is referentially transparent if for all programs , every
e
p
occurrence of  in  can be replaced with the result of evaluating 
e
p
e
without changing the result of evaluating .
p
A function  is 
 if the expression 
 is referentially transparent
f
pure
f(x)
for all referentially transparent inputs .
x
By that definition, the following function is pure, even though it uses a while
loop, an updatable 
, and a mutable array:
var
The 
 function sorts a list by turning it into a mutable array, sorting
quicksort
the array in place using the well-known Quicksort algorithm, and then turning the
array back into a list. It's not possible for any caller to know that the individual
subexpressions inside the body of 
 are not referentially transparent or
quicksort
that the local methods 
, 
, and 
 are not pure, because at no
swap partition
qs
point does any code outside the 
 function hold a reference to the
quicksort
mutable array. Since all of the mutation is locally scoped, the overall function is
def quicksort(xs: List[Int]): List[Int] = if (xs.isEmpty) xs else {
  val arr = xs.toArray
  def swap(x: Int, y: Int) = {
    val tmp = arr(x)
    arr(x) = arr(y)
    arr(y) = tmp
  }
  def partition(l: Int, r: Int, pivot: Int) = {
    val pivotVal = arr(pivot)
    swap(pivot, r)
    var j = l
    for (i <- l until r) if (arr(i) < pivotVal) {
      swap(i, j)
      j += 1
    }
    swap(j, r)
    j
  }
  def qs(l: Int, r: Int): Unit = if (l < r) {
    val pi = partition(l, r, l + (l - r) / 2)
    qs(l, pi - 1)
    qs(pi + 1, r)
  }
  qs(0, arr.length - 1)
  arr.toList
}
251
www.it-ebooks.info

pure. That is, for any referentially transparent expression 
 of type 
,
xs
List[Int]
the expression 
 is also referentially transparent.
quicksort(xs)
Some algorithms, like Quicksort, need to mutate data in place in order to work
correctly or efficiently. Fortunately for us, we can always safely mutate data that is
created locally. Any function can use side-effecting components internally and still
present a pure external interface to its callers.
But working with mutable data can force us to take a monolithic approach. For
example, the constituent parts of 
 above would have direct
quicksort
side-effects if used on their own, which makes it impossible to compose them in
the same way that we would compose pure functions. Of course, we could just
make them work in 
, but that's really not appropriate for local mutable state. If 
IO
 returned 
 then it would be an 
 action that is
quicksort
IO[List[Int]]
IO
perfectly safe to run and would have no side-effects, which is not the case in
general for 
 actions. We want to be able to distinguish between effects that are
IO
safe to run (like locally mutable state) and external effects like I/O. So a new data
type is in order.
The most natural approach is to make a little language for talking about mutable
state. Writing and reading a state is something we can already do with the 
 monad, which you will recall is just a function of type 
State[S,A]
S => (A,
 that takes an input state and produces a result and an output state. But when
S)
we're talking about mutating the state 
, we're not really passing it from one
in place
action to the next. What we'll pass instead is a kind of token marked with the type 
. A function called with the token then has the authority to mutate data that is
S
tagged with the same type .
S
This new data type will employ Scala's type system to gain two static
guarantees. That is, we want code that violates these invariants to 
:
not complile
A mutable object can never be observed outside of the scope in which it was created.
If we hold a reference to a mutable object, then nothing can observe us mutating it.
We will call this new local effects monad 
, which could stand for "State
ST
Thread", "State Transition", "State Token", or "State Tag". It's different from the 
 monad in that its 
 method is protected, but otherwise its structure is
State
run
exactly the same.
14.2.1 Composable mutations
252
www.it-ebooks.info

Cache the value in case run is called more than once.
The reason the 
 method is protected is that an  represents the ability to 
run
S
 state, and we don't want the mutation to escape. So how do we then run an 
mutate
 action, giving it an initial state? These are really two questions. We will start by
ST
answering the question of how we specify the initial state.
As always, don't feel compelled to understand every detail of the
implementation of 
. What matters is the idea that we can use the type system to
ST
constrain the scope of mutable state.
Our first example of an application for the 
 monad is a little language for talking
ST
about mutable references. This takes the form of a combinator library with some
primitive combinators. The language for talking about mutable memory cells
should have these primitive commands:
Allocate a new mutable cell
Write to a mutable cell
Read from a mutable cell
The data structure we'll use for mutable references is just a wrapper around a
protected 
:
var
sealed trait ST[S,A] { self =>
  protected def run(s: S): (A,S)
  def map[B](f: A => B): ST[S,B] = new ST[S,B] {
    def run(s: S) = {
      val (a, s1) = self.run(s)
      (f(a), s1)
    }
  }
  def flatMap[B](f: A => ST[S,B]): ST[S,B] = new ST[S,B] {
    def run(s: S) = {
      val (a, s1) = self.run(s)
      f(a).run(s1)
    }
  }
}
object ST {
  def apply[S,A](a: => A) = {
    lazy val memo = a
    new ST[S,A] {
      def run(s: S) = (memo, s)
    }
  }
}
14.2.2 An algebra of mutable references
253
www.it-ebooks.info

The methods on 
 to read and write the cell are pure since they just return
STRef
 actions. Notice that the type  is 
 the type of the cell that's being mutated,
ST
S
not
and we never actually use the value of type . Nevertheless, in order to call 
S
apply
and actually run one of these 
 actions, you do need to have a value of type .
ST
S
That value therefore serves as a kind of token—an authorization to mutate or
access the cell, but it serves no other purpose.
The question of how to give an initial state is answered by the 
 method
apply
on the 
 companion object. The 
 is constructed with an initial value
STRef
STRef
for the cell, of type . But what is returned is not a naked 
, but an 
 action
A
STRef
ST
that constructs the 
 when run. That is, when given the token of type .
STRef
S
At this point, let's try writing a trivial 
 program. It's a little awkward right
ST
now because we have to choose a type  arbitrarily. Here, we arbitrarily choose 
S
:
Nothing
This little program allocates two mutable 
 cells, swaps their contents, adds
Int
sealed trait STRef[S,A] {
  protected var cell: A
  def read: ST[S,A] = ST(cell)
  def write(a: => A): ST[S,Unit] = new ST[S,Unit] {
    def run(s: S) = {
      cell = a
      ((), s)
    }
  }
}
object STRef {
  def apply[S,A](a: A): ST[S, STRef[S,A]] = ST(new STRef[S,A] {
    var cell = a
  })
}
for {
  r1 <- STRef[Nothing,Int](1)
  r2 <- STRef[Nothing,Int](1)
  x  <- r1.read
  y  <- r2.read
  _  <- r1.write(y+1)
  _  <- r2.write(x+1)
  a  <- r1.read
  b  <- r2.read
} yield (a,b)
254
www.it-ebooks.info

one to both, and then reads their new values. But we can't yet 
 this program
run
because 
 is still protected (and we could never actually pass it a value of type 
run
 anyway). Let's work on that.
Nothing
By now you will have figured out the plot with the 
 monad. The plan is to use 
ST
 to build up a computation that, when run, allocates some local mutable state,
ST
proceeds to mutate it to accomplish some task, and then discards the mutable state.
The whole computation is referentially transparent because all the mutable state is
private and locally scoped. But we want to be able to 
 that. For example,
guarantee
an 
 contains a mutable 
 and we want Scala's type system to guarantee
STRef
var
that we can never extract an 
 out of an 
 action because that would violate
STRef
ST
the invariant that the mutable reference is local to the 
 action, violating
ST
referential transparency in the process.
So how do we safely run 
 actions? First we must differentiate between
ST
actions that are safe to run and ones that aren't. Spot the difference between these
types:
ST[S, STRef[S, Int]] (not safe to run)
ST[S, Int] (completely safe to run)
The former is an 
 action that contains a mutable reference. But the latter is
ST
quite different. A value of type 
 is quite literally just an 
, even
ST[S,Int]
Int
though computing the 
 may involve some local mutable state. Fortunately for
Int
us, there's an exploitable difference between these two types. The 
 involves
STRef
the type , but 
 does not.
S
Int
We want to disallow running an action of type ST[S, STRef[S,A]]
because that would expose the 
. And in general we want to disallow
STRef
running any 
 where  involves the type . On the other hand, it's easy to
ST[S,T]
T
S
see that it should always be safe to run an 
 action that doesn't expose a mutable
ST
object. If we have such a pure action of a type like 
, it should be safe
ST[S,Int]
to pass it an  to get the 
 out of it. Furthermore, 
S
Int
we don't care what  actually
S
 in that case because we are going to throw it away. The value might as well be
is
polymorphic in .
S
In order to represent this, we will introduce a new trait that represents ST
actions that are safe to run. In other words, actions that are polymorphic in :
S
14.2.3 Running mutable state actions
255
www.it-ebooks.info

This is very similar to the idea behind the 
 trait from the previous
Trans
chapter. A value of type 
 is a function that takes a 
  and
RunnableST[A]
type S
produces a 
 of type 
.
value
ST[S,A]
In the section above we arbitrarily chose 
 as our  type. Let's instead
Nothing
S
wrap it in 
 making it polymorphic in . Then we do not have to
RunnableST
S
choose the type  at all. It will be supplied by whatever calls 
.
S
apply
We are now ready to write the 
 function that will call 
 on any
runST
apply
polymorphic 
 by arbitrarily choosing a type for 
. Since the 
RunnableST
S
 action is polymorphic in , it's guaranteed to not make use of the
RunnableST
S
value that gets passed in. So it's actually completely safe to pass 
!
null
The 
 function must go on the 
 companion object. Since 
 is
runST
ST
run
protected on the 
 trait, it's accessible from the companion object but nowhere
ST
else:
trait RunnableST[A] {
  def apply[S]: ST[S,A]
}
val p = new RunnableST[(Int, Int)] {
  def apply[S] = for {
    r1 <- STRef(1)
    r2 <- STRef(2)
    x  <- r1.read
    y  <- r2.read
    _  <- r1.write(y+1)
    _  <- r2.write(x+1)
    a  <- r1.read
    b  <- r2.read
  } yield (a,b)
}
object ST {
  def apply[S,A](a: => A) = {
    lazy val memo = a
    new ST[S,A] {
      def run(s: S) = (memo, s)
    }
  }
  def runST[A](st: RunnableST[A]): A =
    st[Null].run(null)._1
}
256
www.it-ebooks.info

We can now run our trivial program  from above:
p
The expression 
 uses mutable state internally but it does not have
runST(p)
any side-effects. As far as any other expression is concerned, it's just a pair of
integers like any other. It will always return the same pair of integers and it will do
nothing else.
But this is not the most important part. Most importantly, we 
 run a
cannot
program that tries to return a mutable reference. It's not possible to create a 
 that returns a naked 
.
RunnableST
STRef
In this example, we arbitrarily choose 
 just to illustrate the point. The
Nothing
point is that the type  is bound in the 
 method, so when we say 
S
apply
new
, that type is not accessible.
RunnableST
Because an 
 is always tagged with the type  of the 
 action that it
STRef
S
ST
lives in, it can never escape. And this is guaranteed by Scala's type system! As a
corollary, the fact that you cannot get an 
 out of an 
 action guarantees
STRef
ST
that if you have an 
 then you are inside of the 
 action that created it, so
STRef
ST
it's always safe to mutate the reference.
scala> val p = new RunnableST[(Int, Int)] {
     |   def apply[S] = for {
     |     r1 <- STRef(1)
     |     r2 <- STRef(2)
     |     x  <- r1.read
     |     y  <- r2.read
     |     _  <- r1.write(y+1)
     |     _  <- r2.write(x+1)
     |     a  <- r1.read
     |     b  <- r2.read
     |   } yield (a,b)
     | }
p: RunnableST[(Int, Int)] = $anon$1@e3a7d65
scala> val r = ST.runST(p)
r: (Int, Int) = (3,2)
scala> new RunnableST[STRef[Nothing,Int]] {
     |   def apply[S] = STRef(1)
     | }
<console>:17: error: type mismatch;
 found   : ST[S,STRef[S,Int]]
 required: ST[S,STRef[Nothing,Int]]
                def apply[S] = STRef(1)
257
www.it-ebooks.info

SIDEBAR
A note on the wildcard type
It is possible to bypass the type system in 
 by using the 
runST
wildcard
. If we pass it a 
, this will allow an 
type
RunnableST[STRef[_,Int]]
 to escape:
STRef
The wildcard type is an artifact of Scala's interoperability with Java's
type system. Fortunately, when you have an 
, using it
STRef[_,Int]
will cause a type error:
This type error is caused by the fact that the wildcard type in ref
represents some concrete type that only 
 knows about. In this case
ref
it's the  type that was bound in the 
 method of the 
S
apply
RunnableST
where it was created. Scala is unable to prove that this is the same type
as . Therefore, even though it's possible to abuse the wildcard type to
R
get the naked 
 out, this is still safe since we can't use it to mutate
STRef
or access the state.
Mutable references on their own are not all that useful. A more useful application
of mutable state is arrays. In this section we will define an algebra for manipulating
mutable arrays in the 
 monad and then write an in-place QuickSort algorithm
ST
compositionally. We will need primitive combinators to allocate, read, and write
mutable arrays:
scala> val ref = ST.runST(new RunnableST[STRef[_, Int]] {
     |   def apply[S] = for {
     |     r1 <- STRef(1)
     |   } yield r1
     | })
ref: STRef[_, Int] = STRef$$anonfun$apply$1$$anon$6@20e88a41
scala> new RunnableST[Int] {
     |   def apply[R] = for { x <- ref.read } yield x }
 error   : type mismatch;
 found   : ST[_$1,Int]
 required: ST[R,Int]
              def apply[R] = for { x <- ref.read } yield x }
                                     ^
14.2.4 Mutable arrays
sealed abstract class STArray[S,A](implicit manifest: Manifest[A]) {
258
www.it-ebooks.info

Scala requires an implicit Manifest for constructing arrays.
Write a value at the give index of the array
Read the value at the given index of the array
Turn the array into an immutable list
Construct an array of the given size filled with the value v
A thing to note is that Scala cannot create arrays for every type. It requires that
there exist a 
 for the type in implicit scope. Scala's standard library
Manifest
provides manifests for most types that you would in practice want to put in an
array. The 
 function simply gets that manifest out of implicit scope.
implicitly
Just like with 
s, we always return 
s packaged in an 
 action
STRef
STArray
ST
with a corresponding  type, and any manipulation of the array (even reading it), is
S
an 
 action tagged with the same type . It's therefore impossible to observe a
ST
S
naked 
 outside of the 
 monad (except in the Scala source file in which
STArray
ST
the 
 data type itself is declared).
STArray
Using these primitives, we can write more complex functions on arrays.
EXERCISE 1: Add a combinator on 
 to fill the array from a 
STArray
Map
where each key in the map represents an index into the array, and the value under
that key is written to the array at that index. For example, fill(Map(0->"a",
 should write the value 
 at index  in the array and 
 at index .
2->"b"))
"a"
0
"b"
2
Use the existing combinators to write your implementation.
  protected def value: Array[A]
  def size: ST[S,Int] = ST(value.size)
  def write(i: Int, a: A): ST[S,Unit] = new ST[S,Unit] {
    def run(s: S) = {
      value(i) = a
      ((), s)
    }
  }
  def read(i: Int): ST[S,A] = ST(value(i))
  def freeze: ST[S,List[A]] = ST(value.toList)
}
object STArray {
  def apply[S,A:Manifest](sz: Int, v: A): ST[S, STArray[S,A]] =
    new STArray[S,A] {
      lazy val value = Array.fill(sz)(v)
    }
}
259
www.it-ebooks.info

Not everything can be done efficiently using these existing combinators. For
example, the Scala library already has an efficient way of turning a list into an
array. Let's make that primitive as well:
The components for 
 are now easy to write in 
. For example, the 
quicksort
ST
 function that swaps two elements of the array:
swap
EXERCISE 2: Write the purely functional versions of 
 and 
.
partition
qs
With those components written, quicksort can now be assembled out of them in
the 
 monad:
ST
def fill(xs: Map[Int,A]): ST[S,Unit]
def fromList[S,A:Manifest](xs: List[A]): ST[S, STArray[S,A]] =
  ST(new STArray[S,A] {
    lazy val value = xs.toArray
  })
14.2.5 A purely functional in-place quicksort
def swap[S](i: Int, j: Int): ST[S,Unit] = for {
  x <- read(i)
  y <- read(j)
  _ <- write(i, y)
  _ <- write(j, x)
} yield ()
def partition[S](arr: STArray[S,Int],
                 l: Int, r: Int, pivot: Int): ST[S,Int]
def qs[S](a: STArray[S,Int], l: Int, r: Int): ST[S,Unit]
def quicksort(xs: List[Int]): List[Int] =
  if (xs.isEmpty) xs else ST.runST(new RunnableST[List[Int]] {
    def apply[S] = for {
      arr    <- STArray.fromList(xs)
      size   <- arr.size
      _      <- qs(arr, 0, size - 1)
      sorted <- arr.freeze
    } yield sorted
})
260
www.it-ebooks.info

As you can see, the 
 monad allows us to write pure functions that
ST
nevertheless mutate the data they receive. Scala's type system ensures that we don't
combine things in an unsafe way.
EXERCISE 
3: 
Give 
the 
same 
treatment 
to 
 as we have given here to
scala.collection.mutable.HashMap
references and arrays. Come up with a minimal set of primitive combinators for
creating and manipulating hash maps.
In the preceding section, we talked about effects that are not observable because
they are entirely local to some scope. There are no programs that can observe the
mutation of data to which it doesn't hold a reference.
But there are other effects that may be non-observable, depending on who is
looking. As a simple example let's take a kind of side-effect that occurs all the time
in ordinary Scala programs, even ones that we would usually consider purely
functional.
Here, 
 looks pretty innocent. We could be forgiven if we
Foo("hello")
assumed that it was a completely referentially transparent expression. But each
time it appears, it produces a 
 
 in a certain sense. If we test 
different Foo
 for equality using the 
 function, we get 
 as we would
Foo("hello")
==
true
expect. But testing for 
 (a notion inherited from the Java
reference equality
language) with 
, we get 
. The two appearances of 
 are
eq
false
Foo("hello")
not references to the "same object" if we look under the hood.
Notice that if we evaluate 
 and store the result as , then
Foo("hello")
x
substitute  to get the expression 
, it has a different result.
x
x eq x
14.3 Purity is contextual
scala> case class Foo(s: String)
cala> val b = Foo("hello") == Foo("hello")
b: Boolean = true
scala> val c = Foo("hello") eq Foo("hello")
c: Boolean = false
scala> val x = Foo("hello")
x: Foo = Foo(hello)
scala> val d = x eq x
d: Boolean = true
261
www.it-ebooks.info

Therefore, by our original definition of referential transparency, every data
. The effect is that a new and unique object is
constructor in Scala has a side-effect
created in memory, and the data constructor returns a reference to that new object.
For most programs this makes no difference because most programs do not
check for reference equality. It is only the 
 method that allows our programs to
eq
observe this side-effect occurring. We could therefore say that it's not a side-effect
at all in the context of the vast majority of programs.
Our definition of referential transparency doesn't take this into account. It
seems like we need the definition to be more general:
SIDEBAR
More general definition of referential transparency
An expression  is referentially transparent with regard to program  if
e
p
every occurrence of  in  can be replaced with the result of evaluating 
e
p
 without changing the result of evaluating .
e
p
This definition is only slightly modified to reflect the fact that not all programs
can observe the same effects. We say that an effect of  is 
 by  if
e
non-observable
p
it doesn't affect the referential transparency of  with regard to .
e
p
We should also note that this definition makes some assumptions. What is
meant by "evaluating"? And what is the standard by which we determine whether
the results of two evaluations are the same?
In Scala, there is a kind of standard answer to these questions. Generally,
evaluation means 
. Since Scala is a strictly
reduction to some normal form
evaluated language, we can force the evaluation of an expression  to normal form
e
in Scala by assigning it to a 
:
val
And referential transparency of  with regard to a program  means that we can
e
p
rewrite  replacing every appearance of  with .
p
e
v
But what do we mean by "changing the result"? We mean that the two results,
before and after rewriting, are in some sense equivalent. And what it means for two
expressions to be equal is a little more nuanced than it might at first appear. Do we
val v = e
262
www.it-ebooks.info

mean equality by the 
 operator? Or do we mean reference equality by the 
==
eq
operator? And what about the case where  is a function? When are two functions
e
equal?
Again, there is a standard answer. In Scala we usually refer to extensional
 when talking about whether two functions are equivalent. We say that two
equality
functions,  and  are extensionally equal when 
 equals 
 for all inputs 
f
g
f(x)
g(x)
. We could just as well take the position of requiring 
 equality which
x
intensional
means that  and  would have to have the same implementation in order to be
f
g
considered equal. But whatever context we choose, the point is that we must
choose 
 context.
some
Above, we talked about how the 
 method is able to 
 the side-effect of
eq
observe
object creation. Let's look more closely at this idea of observable behavior. It
requires that we delimit what we consider observable and what we don't. Take for
example this method that has a definite side-effect:
If you replace 
 with  in your program, you do not have the
timesTwo(1)
2
same program in every respect. It may compute the same result, but we can say
that the observable behavior of the program has changed. But this is not true for all
programs that call 
, nor for all notions of program equivalence.
timesTwo
We need to decide up front whether changes in standard output are something
we care to observe—whether it's part of the changes in behavior that 
 in our
matter
context. In this case it's exceedingly unlikely that any other part of the program
will be able to observe that 
 side-effect occurring inside 
.
println
timesTwo
Of course, 
 has a 
 on the I/O subsystem. It
timesTwo
hidden dependency
requires access to the standard output stream. But as we have seen above, most
programs that we would consider purely functional also require access to some of
the underlying machinery of Scala's environment, like being able to construct
objects in memory and discard them. At the end of the day, we have to decide for
ourselves which effects are important enough to track. We could use the 
 monad
IO
to track 
 calls, but maybe we don't want to bother. If we're just using the
println
14.3.1 What counts as a side-effect?
def timesTwo(x: Int) = {
  if (x < 0) println("Got a negative number")
  x * 2
}
263
www.it-ebooks.info

console to do some temporary debug logging, it seems like a waste of time to track
that. But if the program's correct behavior depends in some way on the what it
prints to the console (like if it's a UNIX utility), then we definitely want to track it.
This brings us to an essential point: Keeping track of effects is a 
 we
choice
make as programmers. It's a value judgement, and there are trade-offs associated
with how we choose. We can take it as far as we want. But as with the context of
referential transparency, in Scala there is a kind of standard choice. For example it
would be completely valid and possible to track memory allocations in the type
system if that really mattered to us. But in Scala we have the benefit of automatic
memory management so the cost of explicit tracking is usually higher than the
benefit.
The policy we should adopt is to track those effects that program correctness
. If a program is fundamentally about reading and writing files, then file
depends on
I/O should be tracked in the type system to the extent feasible. If a program relies
on object reference equality, it would be nice to know that statically as well. Static
type information lets us know what kinds of effects are involved, and thereby lets
us make educated decisions about whether they matter to us in a given context.
The 
 type in this chapter and the 
 monad in the previous chapter should
ST
IO
have given you a taste for what it's like to track effects in the type system. But this
is not the end of the road. You're limited only by your imagination and the
expressiveness of Scala's types.
In this chapter, we discussed two different implications of referential transparency.
We saw that we can get away with mutating data that never escapes a local
scope. At first blush it may seem that mutating state can't be compatible with pure
functions. But as we have seen, we can write components that have a pure interface
and mutate local state behind the scenes, using Scala's type system to guarantee
purity.
We also discussed that what counts as a side-effect is actually a choice made by
the programmer or language designer. When we talk about functions being pure,
we should have already chosen a context that establishes what it means for two
things to be equal, what it means to execute a program, and which effects we care
to take into account when observing the program's behavior.
14.4 Summary
264
www.it-ebooks.info

extensional equality
extensionality
intensional equality
intensionality
Index Terms
265
www.it-ebooks.info

15
We said in the introduction to part 4 that functional programming is a complete
paradigm. Any program that can be imagined can be expressed functionally,
including those that interact with the external world. But it would be disappointing
if the 
 type were our only way of constructing such programs. 
 (and 
) work
IO
IO
ST
by simply 
 an imperative programming language into the purely
embedding
functional subset of Scala. While programming 
 the 
 monad, we have to
within
IO
reason about our programs much like we would in ordinary imperative
programming.
We can do better. Not only can functional programs embed arbitrary imperative
programs; in this chapter we show how to recover the high-level, compositional
style developed in parts 1-3 of this book, even for programs that interact with the
outside world. The design space in this area is enormous, and our goal here is more
to convey ideas and give a sense of what is possible.1
Footnote 1mAs always, there is more discussion and links to further reading in the chapter notes.
Rather than simply giving the 'answer' up front, we will build up a library for
streaming, composable I/O incrementally. We are going to start by considering a
very simple, concrete usage scenario, which we'll use to highlight some of the
problems with imperative I/O embedded in the 
 monad:
IO
Check whether the number of lines in a file is greater than 40,000.
This is a rather simplistic task, intended to be illustrative and help us get at the
essence of the problem we are trying to solve with our library. We could certainly
Stream processing and incremental I/O
15.1 Introduction
15.2 Problems with imperative I/O: an example
266
www.it-ebooks.info

accomplish this task with ordinary imperative code, inside the 
 monad. Let's
IO
look at that first:2
Footnote 2mFor simplicity, in this chapter we are not going to parameterize our 
 type on the  language
IO
F
used. That is, let's assume that 
, where 
type IO[A] = fpinscala.iomonad.IO[Task,A]
Task[A]
just wraps a 
 with some functions for error handling. This should be
Future[Either[Throwable,A]]
taken to mean that within an 
 we can make use of any impure Scala function. See the chapter code for
IO[A]
details.
There are a number of convenience functions in scala.io.Source for reading from
external sources like files.
Obtain a stateful Iterator from the Source
Has side effect of advancing to next element
Although this code is rather low-level, there are a number of 
 things about
good
it. First, it is 
—the entire file is not loaded into memory up front.
incremental
Instead, lines are fetched from the file only when needed. If we didn't buffer our
input, we could keep as little as a single line of the file in memory at a time. It also
terminates early, as soon as the answer is known, rather than reading the entire file
and then returning an answer.
There are some bad things about this code, too. For one, we have to remember
to close the file when we're done. This might seem obvious, but if we forget to do
this or (more commonly) if we close the file outside of a 
 block and an
finally
exception occurs first, the file will remain open.  This is called a 
. A
3
resource leak
file handle is an example of a scarce 
—the operating system can only have
resource
a limited number of files open at any given time. If this task were part of a larger
def linesGt40k(filename: String): IO[Boolean] = IO {
  val src = io.Source.fromFile(filename)
  try {
    var count = 0
    val lines: Iterator[String] = src.getLines
    while (count <= 40000 && lines.hasNext) {
      lines.next
      count += 1
    }
    count > 40000
  }
  finally src.close
}
267
www.it-ebooks.info

program, say we were scanning an entire directory recursively, building up a list of
all files with more than 40,000 lines, our larger program could easily fail because
too many files were left open.
Footnote 3mThe JVM will actually close an 
 (which is what backs a 
)
InputStream
scala.io.Source
when it is garbage collected, but there is no way to guarantee this will occur in a timely manner, or at all! This is
especially true in generational garbage collectors that perform 'full' collections infrequently.
We want to write programs that are 
—that is, they should close
resource safe
file handles as soon as they are finished with them (whether because of normal
termination or an exception), and they should not attempt to read from a closed
file. Likewise for other resources like network connections, database connections,
and so on. Using 
 directly can be problematic because it means our programs
IO
are 
, and we get no help from the
entirely responsible for ensuring resource safety
compiler in making sure of this. It would be nice if our library would ensure
resource safety by construction.
SIDEBAR
The bracket combinator
A commonly used combinator that helps with ensuring resource safety
in 
 code is 
:
IO
bracket
The exact implementation of this combinator depends on the
representation of 
, but the implementation should ensure that the
IO
resource is released, either just after the 
 action finishes
using
successfully or immediately if an exception occurs. As an exercise, you
may wish to implement 
 for our existing 
 type.
bracket
IO
But even aside from the problems with resource safety, there is something
rather low-level and unsatisfying about this code. We should be able to express the 
—of counting elements and stopping with a response as soon as we hit
algorithm
40,000, independent of 
 we are to obtain these elements. Opening and closing
how
files and catching exceptions is a separate concern from the fundamental algorithm
being expressed, but this code intertwines these concerns. This isn't just ugly, it's
not 
, and our code will be difficult to extend later. For instance,
compositional
consider a few variations of this scenario:
Check whether the number of 
 lines in the file exceeds 40,000
nonempty
def bracket[A,B](acquire: IO[A], release: A => IO[Unit])(
                 using: A => IO[B]): IO[B]
268
www.it-ebooks.info

Find a line index before 40,000 where the first letter of consecutive lines spells out 
.
"abracadabra"
For this first case, we could imagine passing a 
 into
String => Boolean
our 
 function. But for the second case, we would need to modify our
linesGt40k
loop to keep track of some further state, and besides being uglier, the resulting
code will likely be tricky to get right. In general, writing efficient code in the IO
monad generally means writing monolithic loops, and monolithic loops are not
composable.
Let's compare this to the case where we have a 
 for the
Stream[String]
lines being analyzed.
Much nicer! With a 
, we get to assemble our program from preexisting
Stream
combinators, 
 and 
. If we want to filter these lines, we
zipWithIndex
exists
can do so easily:
And for the second scenario, we can use the 
 function defined
indexOfSlice
on 
,  in conjunction with 
 (to terminate the search after 40,000 lines)
Stream 4
take
and 
 (to pull out the first character of each line):
map
Footnote 4mIf the argument to 
 does not exist as a subsequence of the input, 
 is returned.
indexOfSlice
-1
See the API docs for details, or experiment with this function in the REPL.
A natural question to ask is, could we just write the above programs if reading
from an actual file? Not quite. The problem is we don't have a 
, we have a file from which we can read a line at a time. We
Stream[String]
could cheat by writing a function, 
, which returns an 
lines
:
IO[Stream[String]]
lines.zipWithIndex.exists(_._2 + 1 >= 40000)
lines.filter(!_.trim.isEmpty).zipWithIndex.exists(_._2 + 1 >= 40000)
lines.take(40000).map(_.head).indexOfSlice("abracadabra".toList)
def lines(filename: String): IO[Stream[String]] = IO {
  val src = io.Source.fromFile(filename)
269
www.it-ebooks.info

This is called 
. We are cheating because the 
 inside
lazy I/O
Stream[String]
the 
 is not actually a pure value. As elements of the stream are forced, it will
IO
execute side effects of reading from the file, and only if we examine the entire
stream and reach its end will we close the file. Although it is appealing that lazy
I/O lets us recover the compositional style to some extent, it is problematic for
several reasons:
It isn't resource safe. The resource (in this case, a file) will be released only if we traverse
to the end of the stream. But we will frequently want to terminate traversal early (here, 
 will stop traversing the 
 as soon as it finds a match) and we certainly don't
exists
Stream
want to leak resources every time we do this.
Nothing stops us from traversing that same 
 again, after the file has been closed,
Stream
resulting in either excessive memory usage (if the 
 is one that caches or 
Stream
memoizes
its values once forced) or an error if the 
 is unmemoized and this causes a read
Stream
from a closed file handle. Also, having two threads traverse an unmemoized 
 at
Stream
the same time can result in unpredictable behavior.
In more realistic scenarios, we won't necessarily have full knowledge of what is
happening with the 
 we created. It could be passed on to some other
Stream[String]
function we don't control, which might store it in a data structure for a long period of
time before ever examining it, etc. Proper usage now requires some out-of-band
knowledge—we cannot necessarily just manipulate this 
 like a typical
Stream[String]
pure value, we have to know something about its origin. This is bad for the compositional
style typically used in FP, where most of our code won't know anything about a value
other than its type.
Lazy I/O is problematic, but it would be nice to recover the high-level style we
are accustomed to from our usage of 
 and 
. In the next section, we'll
Stream
List
introduce the notion of 
 or 
, which is our first
stream transducers
stream processors
step toward achieving this.
A stream transducer specifies a transformation from one stream to another. We are
using the term 
 more generally here, to refer to a sequence, possibly lazily
stream
generated or supplied by an external source (for instance, a stream of lines from a
file, a stream of HTTP requests, a stream of mouse click positions, etc). Let's
consider a simple data type, 
, that lets us express stream transformations:
Process
5
  src.getLines.toStream append { src.close; Stream.empty }
}
15.3 Simple stream transducers
270
www.it-ebooks.info

Footnote 5mWe have chosen to omit variance annotations in this chapter for simplicity, but it is possible to
write this as 
.
Process[-I,+O]
A 
 can be used to transform a stream containing  values to a
Process[I,O]
I
stream of  values (this is sometimes called a stream transducer). The viewpoint of
O
 is somewhat inverted from how we might be used to thinking of things. 
Process
 is not a typical function 
, which
Process[I,O]
Stream[I] => Stream[O]
could pattern match on the input and construct the output itself. Instead, we have a
state machine which must be interpreted by a 
. There are three possible states
driver
a 
 can be in, each of which signals something to the driver:
Process
Emit(head,tail) indicates to the driver that the 
 values should be emitted to the
head
output stream, and that 
 should be the next state following that.
tail
6
Footnote 6mWe could choose to have 
 produce just a single value. The use of 
 avoids stack
Emit
Seq
overflows for certain 
 definitions.
Process
Await(recv,finalizer) requests a value from the input stream, indicating that recv
should be used by the driver to produce the next state, and that 
 should be
finalizer
consulted if the input has no more elements available.
Halt indicates to the driver that no more elements should be read from the input stream
or emitted to the output.
Let's look at a sample driver that will actually interpret these requests. Here is
one that actually transforms a 
. We can implement this as a function on 
Stream
:
Process
trait Process[I,O]
case class Emit[I,O](
    head: Seq[O],
    tail: Process[I,O] = Halt[I,O]())
  extends Process[I,O]
case class Await[I,O](
    recv: I => Process[I,O],
    finalizer: Process[I,O] = Halt[I,O]())
  extends Process[I,O]
case class Halt[I,O]() extends Process[I,O]
def apply(s: Stream[I]): Stream[O] = this match {
  case Halt() => Stream()
  case Await(recv, finalizer) => s match {
    case h #:: t => recv(h)(t)
    case _ => finalizer(s) // Stream is empty
  }
271
www.it-ebooks.info

Thus, given 
 and 
, 
 produces a 
p: Process[I,O]
in: Stream[I] p(in)
. What is interesting is that 
 is agnostic to how it is fed
Stream[O]
Process
input. We have written a driver that feeds a 
 from a 
, but we can
Process
Stream
also write drivers that perform 
. We'll get to writing such a driver a bit later, but
IO
first, we are going to explore the large number of operations expressible with 
.
Process
We can think about 
, on the one hand, as a sequence of  values,
Process[I,O]
O
and many of the operations defined for sequences are defined for 
 as
Process
well. Let's start with a familiar one, 
:
map
The implementation simply calls 
 on any values produced by the 
map
Process
. As with lists, we can also 
 processes. Given two processes,  and , 
append
x
y x ++
 runs  to completion, then runs  to completion on whatever input remains after
y
x
y
the first has halted. For the implementation, we simply replace the 
 of  with 
Halt
x
 (much like how 
 on 
 replaces the 
 of the first list with the second
y
++
List
Nil
list):
Helper function described below
This uses a helper function, 
, which behaves just like the 
emitAll
Emit
constructor but combines adjacent emit states into a single 
. For instance, 
Emit
 becomes 
. (A
emitAll(h1, Emit(h2, t))
Emit(h1 ++ h2, t)
  case Emit(h,t) => h.toStream append t(s)
}
15.3.1 Operations on Process
def map[O2](f: O => O2): Process[I,O2] = this match {
  case Halt() => Halt()
  case Emit(h, t) => Emit(h map f, t map f)
  case Await(recv,fb) => Await(recv andThen (_ map f), fb map f)
}
def ++(p: => Process[I,O]): Process[I,O] = this match {
  case Halt() => p
  case Emit(h, t) => emitAll(h, t ++ p)
  case Await(recv,fb) => Await(recv andThen (_ ++ p), fb ++ p)
}
272
www.it-ebooks.info

function like this that just calls some constructor of a data type but enforces some
addition invariant is often called a 
.)
smart constructor
Consistent use of 
 lets us assume that an 
 will always be
emitAll
Emit
followed by an 
 or a 
, which avoids stack overflow errors in certain 
Await
Halt
 definitions.
Process
With the help of 
 on 
, we can define 
:
++
Process
flatMap
Incidentally, 
 forms a 
. The 
 function just emits a single
Process
Monad
unit
value, then halts:
To write the 
 instance, we have to partially apply the  parameter of 
Monad
I
, which we've seen before:
Process
def emitAll[I,O](head: Seq[O],
                 tail: Process[I,O] = Halt[I,O]()): Process[I,O] =
  tail match {
    case Emit(h2, tl) => Emit(head ++ h2, tl)
    case _ => Emit(head, tail)
  }
def emit[I,O](head: O,
               tail: Process[I,O] = Halt[I,O]()): Process[I,O] =
      emitAll(Stream(head), tail)
def flatMap[O2](f: O => Process[I,O2]): Process[I,O2] = this match {
  case Halt() => Halt()
  case Emit(h, t) =>
    if (h.isEmpty) t flatMap f
    else f(h.head) ++ emitAll(h.tail, t).flatMap(f)
  case Await(recv,fb) =>
    Await(recv andThen (_ flatMap f), fb flatMap f)
}
def unit[O](o: => O): Process[I,O] = emit(o)
def monad[I]: Monad[({ type f[x] = Process[I,x]})#f] =
  new Monad[({ type f[x] = Process[I,x]})#f] {
    def unit[O](o: => O): Process[I,O] = Emit(o)
    def flatMap[O,O2](p: Process[I,O])(
                      f: O => Process[I,O2]): Process[I,O2] =
      p flatMap f
  }
273
www.it-ebooks.info

We use the same trick introduced in chapter 13 of placing a toMonadic
implicit conversion in the companion object to give us infix syntax for the Monad
combinators:
This lets us write, for instance: 
.
emit(42) as "hello!" 7
Footnote 7mRecall that 
 is equal to 
.
a as b
a map (_ => b)
The 
 instance is exactly the same 'idea' as the 
 for 
. What
Monad
Monad
List
makes 
 more interesting than just 
 is it can accept 
. And it can
Process
List
input
transform that input through mapping, filtering, folding, grouping, and so on. It
turns out that 
 can express almost any stream transformation, all while
Process
remaining agnostic to how exactly it is obtaining its input or what should happen
with its output.
The way we will build up complex stream transformations is by 
 
composing
 values. Given two 
 values,  and , we can feed the output 
Process
Process
f
g
f
into the input of . We'll call this operation 
 (pronounced 'pipe' or 'compose')
g
|>
and implement it as a function on 
.  It has the nice property that 
Process 8
f |>
 
 transformations of  and . As soon as values are emitted by , they are
g fuses
f
g
f
transformed by .
g
Footnote 8mThis operation might remind you of function composition, which feeds the (single) output of a
function in as the (single) input to another function. Both 
 and functions form 
. We won't
Process
categories
be discussing that much here, but see the chapter notes.
EXERCISE 1 (hard): Implement 
. Let the types guide your implementation.
|>
We can convert any function 
 to a 
. We
f: I => O
Process[I,O]
repeatedly 
, then 
 the value received, transformed by .
Await
Emit
f
implicit def toMonadic[I,O](a: Process[I,O]) = monad[I].toMonadic(a)
PROCESS COMPOSITION, LIFTING, AND REPETITION
def |>[O2](p2: Process[O,O2]): Process[I,O2]
def lift[I,O](f: I => O): Process[I,O] =
  Await((i: I) => emit(f(i), lift(f)))
274
www.it-ebooks.info

This pattern is quite common—we often have some 
 whose steps we
Process
wish to repeat forever. We can write a combinator for it, 
:
repeat
This is very typical of 
 definitions. We define a recursive internal
Process
function (often called 
 or 
) whose parameters are the 
 used for the
go
loop
state
transformation. (In the case of 
, the only piece of state is the current 
repeat
; for other transformations the state may be more complicated.) We then
Process
call this internal function with some initial state. Let's use 
 to write 
repeat
, which constructs a 
 that filters its input:
filter
Process
We can now write expressions like filter(_ % 2 == 0) |> lift(_
 to filter and map in a single transformation. We'll sometimes call a sequence
+ 1)
of transformations like this a 
.
pipeline
There are a huge number of other combinators we can write for 
.
Process
Let's look at another one, 
, which outputs a running total of the values seen so
sum
far:
Again, we use the same pattern of an inner function which tracks the current
state (in this case, the total so far). Here's an example of its use in the REPL:
def repeat: Process[I,O] = {
  def go(p: Process[I,O]): Process[I,O] = p match {
    case Halt() => go(this)
    case Await(recv,fb) => Await(recv andThen go, fb)
    case Emit(h, t) => Emit(h, go(t))
  }
  go(this)
}
def filter[I](f: I => Boolean): Process[I,I] =
  Await[I,I](i => if (f(i)) emit(i) else Halt()) repeat
def sum: Process[Double,Double] = {
  def go(acc: Double): Process[Double,Double] =
    Await((d: Double) => emit(d+acc, go(d+acc)))
  go(0.0)
}
scala> sum(Stream(1.0, 2.0, 3.0, 4.0)).toList
val res0: List[Double] = List(1.0, 3.0, 6.0, 10.0)
275
www.it-ebooks.info

Let's get write some more 
 combinators to get accustomed to this
Process
style of programming. Try to work through implementations of at least some of
these exercises until you get the hang of it.
EXERCISE 2: Implement 
, which halts the 
 after it encounters
take
Process
the given number of elements, and 
, which ignores the given number of
drop
arguments, then emits the rest. Optional: implement 
 and 
takeWhile
.
dropWhile
EXERCISE 3: Implement 
. It should emit the number of elements seen
count
so far, for instance, 
 should yield 
count(Stream("a", "b", "c"))
 (or 
, your choice). Feel free to
Stream(1, 2, 3)
Stream(0, 1, 2, 3)
use existing combinators.
EXERCISE 4: Implement 
. It should emit a running average of the values
mean
seen so far.
Just as we have seen many times before throughout this book, when we notice
common patterns when defining a series of functions, we can factor these patterns
out into generic combinators. The functions 
, 
 and 
 all share a
sum count
mean
def take[I](n: Int): Process[I,I]
def drop[I](n: Int): Process[I,I]
def takeWhile[I](f: I => Boolean): Process[I,I]
def dropWhile[I](f: I => Boolean): Process[I,I]
def count[I]: Process[I,Int]
def mean: Process[Double,Double]
276
www.it-ebooks.info

common pattern. Each has a single piece of state, and a state transition function
that updates this state in response to input and produces a single output. We can
generalize this to a combinator, 
:
loop
Using 
, 
we 
can, 
for 
instance, 
express 
 
as 
loop
sum
.
loop(0)((n:Double,acc) => (acc,n+acc))
EXERCISE 5 (optional): Write 
 and 
 in terms of 
.
sum
count
loop
EXERCISE 6 (hard, optional): Come up with a generic combinator that lets us
express 
 in terms of 
 and 
. Define this combinator and implement 
mean
sum
count
 in terms of it.
mean
EXERCISE 7 (optional): Implement 
. It emits a running count
zipWithIndex
of 
values 
emitted 
along 
with 
each 
value. 
For 
example: 
 
yields 
Process("a","b").zipWithIndex
Process(("a",0),
.
("b",1))
EXERCISE 8 (optional): Implement 
. There are multiple ways to
exists
implement it. Given 
 could
exists(_ % 2 == 0)(Stream(1,3,5,6,7))
produce 
 (halting, and only yielding the final result), 
Stream(true)
 (halting, and yielding all intermediate
Stream(false,false,false,true)
results), or 
 (
 halting, and
Stream(false,false,false,true,true) not
yielding all the intermediate results). Note that because 
 fuses, there is no
|>
penalty to implementing the 'trimming' of this last form with a separate
combinator.
We can now express the core stream transducer for our line-counting problem
as 
. Of course, it's easy to attach filters and
count |> exists(_ > 40000)
other transformations to our pipeline.
def loop[S,I,O](z: S)(f: (I,S) => (O,S)): Process[I,O] =
  Await((i: I) => f(i,z) match {
    case (o,s2) => emit(o, loop(s2)(f))
  })
def exists[I](f: I => Boolean): Process[I,Boolean]
277
www.it-ebooks.info

We can use an external source to drive a 
. We'll look first at a simplistic
Process
approach in which sources are a completely separate type from 
; later,
Process
we will consider a generalized 
 type which can represent sources as well
Process
as single-input stream transducers.
As the definitions of 
 and 
 demonstrate, we can implement various
filter
map
operations with helper functions that simply attach the appropriate 
 onto
Process
the output of the 
. Using this approach we can implement 
, 
Source
take
, and lots of other typical list processing functions, almost as if 
takeWhile
 were an actual 
. We only need to provide an 
 for 
Source
List
interpreter
 that actually performs the 
 actions and feeds them to the transducer,
Source
IO
but this is absolutely straightforward:
15.3.2 External sources
trait Source[O] {
  def |>[O2](p: Process[O,O2]): Source[O2]
  def filter(f: O => Boolean) = this |> Process.filter(f)
  def map[O2](f: O => O2) = this |> Process.lift(f)
}
case class ResourceR[R,I,O]( // A resource from which we can read values
  acquire: IO[R],
  release: R => IO[Unit],
  step: R => IO[Option[I]],
  trans: Process[I,O]) extends Source[O] {
  def |>[O2](p: Process[O,O2]) =
    ResourceR(acquire, release, step, trans |> p)
}
def collect: IO[IndexedSeq[O]] = {
  def tryOr[A](a: => A)(cleanup: IO[Unit]) =
    try a catch { case e: Exception => cleanup.run; throw e }
  @annotation.tailrec
  def go(acc: IndexedSeq[O],
         step: IO[Option[I]],
         p: Process[I,O],
         release: IO[Unit]): IndexedSeq[O] =
    p match {
      case Halt() => release.run; acc
      case Emit(h, t) =>
        go(tryOr(acc ++ h)(release), step, t, release)
      case Await(recv, fb) => tryOr(step.run)(release) match {
        case None => go(acc, IO(None), fb, release)
278
www.it-ebooks.info

Helper function: evaluates a, and runs cleanup if an exception occurs.
We tryOr(acc ++ h) since h may be a non-strict Seq like Stream which forces some
computations that can fail.
Our IO computation can of course fail during evaluation.
Notice we are guaranteed to run the 
 action, whether we terminate
release
normally or if an exception occurs during processing.  This is important since we
9
will often construct 
 values backed by some resource like a file handle we
Source
want to ensure gets closed. Here is an example of a primitive 
, created
Source
from the lines of a file:
Footnote 9mOne might reasonably ask—if we are eliminating usage of exceptions by using 
 and 
Either
 throughout our code, is this really necessary? Yes. For one, not all functions in our programs are
Option
defined for all inputs and we typically still use exceptions to signal unrecoverable errors. We may also be using
some third-party API which may throw exceptions or errors. And lastly, exceptions may be triggered 
, through no fault of the program—for instance, when the thread a program runs in is killed,
asynchronously
this generates a 
 exception to give the program the opportunity to clean up.
ThreadInterruped
Our code for checking whether the number of lines in a file exceeds 40,0000
now looks like Source.lines("input.txt").count.exists(_ >
. This is nicely compositional, and we are assured that calling 
40000)
collect
on this 
 will open the file and guarantee it is closed, regardless of whether
Source
exceptions occur. We deal with resource safety in just two places, the collect
function we wrote earlier, and the definition of 
—the knowledge of how to
lines
allocate and release a resource is encapsulated in a single type, 
, and 
Source
 is the sole driver that must take care to use this information to ensure
collect
        case Some(i) => go(acc, step, recv(i), release)
      }
    }
  acquire map (res =>
    go(IndexedSeq(), step(res), trans, release(res)))
}
def lines(filename: String): Source[String] =
  ResourceR(
    IO(io.Source.fromFile(filename)),
    (src: io.Source) => IO(src.close),
    (src: io.Source) => {
      lazy val iter = src.getLines 
      IO { if (iter.hasNext) Some(iter.next) else None }
    },
    Process.id[String])
279
www.it-ebooks.info

resource safety. This is in contrast to ordinary imperative I/O (in the 
 monad or
IO
otherwise) where any code that reads from files must repeat the same (error-prone)
patterns to ensure resource safety.
Although we can get quite far with 
 and 
, and the simple
Process
Source
way we have combined them here is resource safe, these data types are too simple
to express a number of interesting and important use cases. Let's look at one of
those next:
Transform 
, a file containing temperatures in degrees
fahrenheit.txt
fahrenheit, to 
, a file containing the same temperatures in degrees
celsius.txt
celsius.
Here's a hypothetical 
:
fahrenheit.txt
We'd like to write a program that reads this and produces 
:
celsius.txt
Our program should work in a streaming fashion, emitting to the output file as
lines are read from the input file, while staying resource safe. With the library we
have so far, we can certainly produce a 
 containing the
Source[Double]
temperatures we need to output to 
:
celsius.txt
15.3.3 External sinks
# a comment - any line starting with #
# temperatures in fahrenheit
85.2
83.1
80.0
71.9
...
29.5556
28.38889
26.6667
22.16667
...
val tempsC: Source[Double] =
  Source.lines("fahrenheit.txt").
         filter(!_.startsWith("#")).
         map(s => fahrenheitToCelsius(s.toDouble))
280
www.it-ebooks.info

Unfortunately, 
 lacks the ability to actually write these lines to the
Source
output file. In general, one way we can handle these expressiveness problems is by
adding extra cases to 
. Here, we could try solving our immediate problem
Source
by first introducing a new type, 
, analogous to 
:
Sink
Source
Here's a simple 
 combinator, for writing to a file:
Sink
How might we integrate this into our 
 API? Let's imagine a new
Source
combinator, 
:
observe
Implementing this combinator will likely require an additional Source
constructor and updates to our 
 implementation (taking care to ensure
collect
resource safety in our usage of the 
). Assuming we do this, our complete
Sink
scenario now looks something like:
trait Sink[I] {
  def <|[I0](p: Process[I0,I]): Sink[I0]
  def filter(f: I => Boolean) = this <| Process.filter(f)
  ...
}
case class ResourceW[R,I,I2](
    acquire: IO[R],
    release: R => IO[Unit],
    recv: R => (I2 => IO[Unit]),
    trans: Process[I,I2]) extends Sink[I] {
  def <|[I0](p: Process[I0,I]) =
    ResourceW(acquire, release, recv, p |> trans)
}
def file(filename: String, append: Boolean = false): Sink[String] =
  ResourceW(
    IO(new FileWriter(filename, append)),
    (w: FileWriter) => IO(w.close),
    (w: FileWriter) => (s: String) => IO(w.write(s)),
    Process.id[String]
  )
def observe(snk: Sink[O]): Source[O]
val convert: IO[Unit] =
  Source.lines("fahrenheit.txt").
         filter(!_.startsWith("#")).
281
www.it-ebooks.info

add line separators back in
This uses the helper function 
, which ignores the output of a 
,
run
Source
evaluating it only for its effects. See the chapter code for its implementation.
Ultimately, this approach of adding special cases to 
 starts getting
Source
rather ugly. Let's take a step back and consider some additional scenarios for which
our existing API is insufficient. These are just informal descriptions.
Multi-source input/zipping: 'Zip' together two files, 
 and 
, each containing
f1.txt
f2.txt
temperatures in degrees fahrenheit, one per line. Add corresponding temperatures
together, convert the result to celsius, apply a 5-element moving average, and output to 
.
celsius.txt
Concatenation: Concatenate two files, 
, and 
, into a
fahrenheit1.txt
fahrenheit2.txt
single logical stream, apply the same transformation as above and output to celsius.txt
.
Dynamic resource allocation: Read a file, 
, containing a list of
fahrenheits.txt
filenames. Concatenate these files into a single logical stream, convert this stream to
celsius, and output the joined stream to 
.
celsius.txt
Multi-sink output: As above, but rather than producing a single output file, produce an
output file for each input file in 
. Name the output file by appending 
fahrenheits.txt
 onto the input file name.
.celsius
Internal effects: Given a stream of HTTP requests, parse each into some object and use it
to construct a database query. Execute this query, generating a stream of rows, which are
further processed using other stream transformations before being assembled into an
HTTP response. Here, the effect is no longer just a sink—we need to get back a result
and continue processing.
These scenarios can't be expressed with our existing API without dropping
down into normal, low-level 
 monad programming (can you see why?).
IO
Although we can try just adding more special cases to 
 (perhaps a 
Source
Zip
constructor, then an 
 constructor, etc.), we can see this getting ugly,
Append
especially if done naively.
 It seems we need a more principled way of extending 
10
. This is what we will consider next.
Process
         map(s => fahrenheitToCelsius(s.toDouble)).
         map(d => d.toString + "\n").
         observe(Sink.file("celsius.txt")).
         run
282
www.it-ebooks.info

Footnote 10mStill, you may be interested to explore this approach. It is a challenging design exercise—the
problem is coming up with a nice, small set of primitive combinators that lets us express all the programs we wish
to write. We don't want to have 50 special cases which, in addition to being ugly, makes writing the collect
function extremely complicated. If you decide to experiment with this approach, think about what combinators
are needed to express each of these scenarios and any others you can think of. Can the combinator be expressed
using existing primitives in a resource safe way? If not, you can try adding another primitive case for it, refining
your primitives as we did throughout part 2, and updating your 
 function to handle additional cases
collect
in a resource-safe way.
Our existing 
 type implicitly assumes an 
 or 
Process
environment
context
containing a single stream of values. Furthermore, the 
 for communicating
protocol
with the driver is also fixed—a 
 can only issue three instructions to the
Process
driver, 
, 
, and 
, and there is no way to extend this protocol short
Halt Emit
Await
of defining a completely new type. In order to make 
 extensible, we are
Process
going to parameterize on the protocol used for issuing requests of the driver. This
works in much the same way as the 
 type we covered in chapter 13:
IO
Unlike 
, a 
 represents a 
 of 
 values ('O' for
IO
Process[F,O]
stream
O
'output'), produced by (possibly) making external requests using the protocol .
F
Otherwise, the  parameter serves the same role here as the  type constructor we
F
F
used for 
.
IO
This type is more general than the previous 
 (which we'll refer to
Process
from now on as a 'single-input 
' or a 
), and we can represent
Process
Process1
single-input 
 as a special instance of this generalized 
 type.
Process
Process
We'll see how this works in a later section.
First, let's note that a number of operations are defined for 
 
Process
15.4 An extensible process type
trait Process[F[_],O]
object Process {
  case class Await[F[_],A,O](
    req: F[A], recv: A => Process[F,O],
    finalizer: Process[F,O]) extends Process[F,O]
  case class Emit[F[_],O](
    head: Stream[O],
    tail: Process[F,O]) extends Process[F,O]
  case class Halt[F[_],O]() extends Process[F,O]
}
283
www.it-ebooks.info

 of the choice of . We can still define 
 ('append'), 
, 
regardless
F
++
flatMap map
and 
 for 
, and the definitions are almost identical to before.
filter
Process
Here's 
 and 
 (see chapter code for other functions, including 
,
++
flatMap
repeat
, and 
):
map
filter
We use the same smart constructors as before, 
 and 
, with
emitAll
emit
similar definitions:
We will also introduce the helper function, 
, which just curries the 
await
 constructor for better type inference:
Await
Again, using 
, we define 
:
++
flatMap
def ++(p: => Process[F,O]): Process[F,O] = this match {
  case Halt() => p
  case Emit(h, t) => emitAll(h, t ++ p)
  case Await(req,recv,fb) =>
    Await(req, recv andThen (_ ++ p), fb ++ p)
}
def emitAll[F[_],O](head: Seq[O], tail: Process[F,O] = Halt[F,O]()) =
  tail match {
    case Emit(h2,t) => Emit(head ++ h2, t)
    case _ => Emit(head, tail)
  }
def emit[F[_],O](head: O, tail: Process[F,O] = Halt[F,O]()) =
  emitAll(Stream(head), tail)
def await[F[_],A,O](req: F[A])(
    recv: A => Process[F,O] = (a: A) => Halt[F,O](),
    fallback: Process[F,O] = Halt[F,O](),
    cleanup: Process[F,O] = Halt[F,O]()): Process[F,O] =
  Await(req, recv, fallback, cleanup)
def flatMap[O2](f: O => Process[F,O2]): Process[F,O2] =
  this match {
    case Halt() => Halt()
    case Emit(o, t) =>
      if (o.isEmpty) t.flatMap(f)
      else f(o.head) ++ emitAll(o.tail, t).flatMap(f)
    case Await(req,recv,fb,c) =>
      Await(req, recv andThen (_ flatMap f),
            fb flatMap f, c flatMap f)
  }
284
www.it-ebooks.info

Let's see what else we can express with this new 
 type. The 
Process
F
parameter gives us a lot of flexibility.
Before, we were forced to introduce a separate type to represent sources. Now, we
can represent an effectful source using a 
.
Process[IO,O] 11
Footnote 11mThere are some issues with making this representation resource-safe that we'll discuss shortly.
Whereas before, 
 was a completely separate type from 
, now
Source
Process
it is merely a particular instance of it! To see how 
 is indeed a
Process[IO,O]
source of  values, consider what the 
 constructor looks like when we
O
Await
substitute 
 for :
IO
F
Thus, any requests of the 'external' world can be satisfied, just by running the 
 action. If this action returns an  successfully, we invoke the 
 function
IO
A
recv
with this result. If the action throws a special exception (perhaps called 
) it
End
indicates normal termination and we switch to the 
 state. And if the
fallback
action throws any other exception, we switch to the 
 state. Below is
cleanup
simple interpreter of 
 which collects up all the values emitted:
Source
Here is the exception type 
 that we use for signaling normal termination.
End
12
Footnote 12mThere are some design decisions here—we are using an exception, 
, for control flow, but we
End
could choose to indicate normal termination with 
, say with 
Option
type Step[A] =
, then having 
 represent sources. We could also choose to pass the
IO[Option[A]]
Process[Step,O]
exception along to the 
 function, requiring the 
 function to take an 
.
recv
recv
Either[Throwable,A]
We are adopting the convention that any exceptions that bubble all the way up to 
 are by definition
collect
unrecoverable. Programs can certainly choose to throw and catch exceptions internally if they wish.
With that we define 
:
collect
15.4.1 Sources
case class Await[A,O](
  req: IO[A], recv: A => Process[F,O],
  fallback: Process[IO,O],
  cleanup: Process[IO,O]) extends Process[Step,O]
case object End extends Exception
def collect[O](src: Process[IO,O]): IndexedSeq[O] = {
  @annotation.tailrec
285
www.it-ebooks.info

Normal termination
Helper function, defined below
This uses a helper function, 
:
failIO
Importantly, we are guaranteed to run either 
 or 
 before
fallback
cleanup
halting the 
, regardless of whether exceptions occur. We'll see later how
Process
this allows us to define a 
 backed by some resource like a file handle that
Process
we want to close in a prompt, deterministic fashion.
Notice how in the 
 case, we run 
 and block waiting for its result
Await
req
before continuing the interpretation. It turns out that we don't require 
 in
IO
particular, any  will do, so long as we have a 
 and as long as 
F
Monad[F]
F
supports catching (and throwing) exceptions. We'll introduce a new interface for
this, 
:
Partial
Rather than invoking 
 on our 
 values, we can simply 
 into the 
run
IO
flatMap
 to obtain the result. We define a function on 
 to produce an 
req
Process[F,O]
 given a 
 and a 
:
F[IndexedSeq[O]]
Monad[F]
Partial[F]
  def go(cur: Process[IO,O], acc: IndexedSeq[O]): IndexedSeq[O] =
    cur match {
      case Emit(h,t) => go(t, acc ++ h)
      case Halt() => acc
      case Await(req,recv,fb,err) =>
        val next =
          try recv(req.run)
          catch {
            case End => fb
            case e: Exception => err ++ failIO(e)
          }
        go(next, acc)
    }
  go(src, IndexedSeq())
}
def failIO[O](e: Throwable): Process[IO,O] =
  await[IO,O,O](IO(throw e))()
trait Partial[F[_]] {
  def attempt[A](a: F[A]): F[Either[Throwable,A]]
  def fail[A](t: Throwable): F[A]
}
286
www.it-ebooks.info

Unlike the simple tail recursive 
 function above, this implementation
collect
is no longer tail recursive, which means our 
 instance is now responsible for
Monad
ensuring constant stack usage. Luckily, the 
 type we developed in chapter 13 is
IO
already suitable for this, and as an added bonus, it supports the use of
asynchronous I/O primitives as well.
Source can be used for talking to external resources like files and database
connections, but care must be taken to ensure resource safety—we want all file
handles to be closed, database connections released, and so on, even (especially!) if
exceptions occur. Let's look at what's needed to make this happen.
To make the discussion concrete, suppose we have 
lines:
 representing the lines of some large file. This implicitly
Process[IO,String]
references a resource (a file handle) that we want to ensure is closed. When should
we close the file handle? At the very end of our program? No, ideally we would
close the file once we know we are done reading from 
. We are certainly
lines
done if we reach the last line of the file—at that point there are no more values to
produce and it is certainly safe to close the file. So this gives us our first simple
rule to follow: a resource should close itself immediately after emitting its final
value.
How do we do this? We do this by placing the file-closing action in the 
 argument of any 
, and the 
 function(s) above will
fallback
Await
collect
ensure this gets called before halting (via catching of the 
 exception). But this
End
is not sufficient—we also want to ensure that the file-closing action is run in the
event of an uncaught exception. Thus we place the same file-closing action in the 
def collect(implicit F: Monad[F], P: Partial[F]): F[IndexedSeq[O]] = {
  def go(cur: Process[F,O], acc: IndexedSeq[O]): F[IndexedSeq[O]] =
    cur match {
      case Emit(h,t) => go(t, acc ++ h)
      case Halt() => F.unit(acc)
      case Await(req,recv,fb,c) =>
         F.flatMap (P.attempt(req)) {
           case Left(End) => go(fb, acc)
           case Left(err) =>
             go(c ++ await[F,Nothing,O](P.fail(err))(), acc)
           case Right(o) => go(recv(o), acc)
         }
    }
  go(this, IndexedSeq())
}
15.4.2 Ensuring resource safety
287
www.it-ebooks.info

 argument to the 
, which again the 
 function will
cleanup
Await
collect
ensure gets called should errors occur.
As an example, let's use this policy to create a 
 backed by
Process[IO,O]
the lines of a file. We define it terms of the more general combinator, 
,
resource
the 
 analogue of the 
 function we introduced earlier for 
:
Process
bracket
IO
Emit the value and repeat the step action
Release resource when exhausted
Also release in event of error
We can now write 
 in terms of 
:
lines
resource
So far so good. However, we cannot 
 make sure that 
 keeps its 
only
lines
 and 
 parameters up to date whenever it produces an 
fallback
cleanup
Await
—we need to make sure they actually get called. To see a potential problem,
consider 
. The 
collect(lines("names.txt") |> take(5))
take(5)
process will halt early after only 5 elements have been received, possibly before
the file has been exhausted. It must therefore make sure before halting that 
 of 
 is run. Note that 
 cannot be responsible for this,
cleanup
lines
collect
since 
 has no idea that the 
 it is interpreting is internally
collect
Process
composed of two other 
 values, one of which requires finalization.
Process
Thus, we have our second simple rule to follow: any process, , which pulls
d
def resource[R,O](acquire: IO[R])(
                  release: R => IO[Unit])(
                  step: R => IO[O]): Process[IO,O] = {
  def go(step: IO[O], onExit: IO[Unit]): Process[IO,O] =
    await[IO,O,O](step) (
      o => emit(o, go(step, onExit))
    , await[IO,Unit,O](onExit)()
    , await[IO,Unit,O](onExit)())
  await(acquire) ( r => go(step(r), release(r)), Halt(), Halt() )
}
def lines(filename: String): Process[IO,String] =
  resource(IO(io.Source.fromFile(filename)))(
           src => IO(src.close)) { src =>
    lazy val lines = src.getLines // A stateful iterator
    IO { if (lines.hasNext) lines.next else throw End }
  }
288
www.it-ebooks.info

values from another process, , must ensure the cleanup action of  is run before 
p
p
 halts.
d
This sounds rather error prone, but luckily, we get to deal with this concern in
just a single place, the 
 combinator. We'll show how that works shortly in the
|>
next section, when we show how to encode single-input processes using our
general 
 type.
Process
We now have nice, resource-safe sources, but we don't yet have any way to apply
transformations to them. Fortunately, our 
 type can also represent the
Process
single-input processes we introduced earlier in this chapter. To represent 
, we craft an appropriate  that only allows the 
 to
Process1[I,O]
F
Process
make requests for elements of type . Lets look at how this works—the encoding
I
is a bit unusual in Scala, but there's nothing fundamentally new here:
Evidence that types A and B are equal
It is a bit strange to define the type  inside of 
. Let's unpack what's going
f
One
on. Notice that  takes one parameters, , but we have just one instance, 
,
f
X
Get
which fixes  to be the  in the outer 
. Therefore, the type 
X
I
One[I]
One[I]#f13
can only ever be a request for a value of type ! Moreover, we get 
 that 
I
evidence
X
is equal to  in the form of the 
 which comes equipped with a pair of
I
Eq[X,I]
functions to convert between the two types.
 We'll see how the 
 value gets used
14
Eq
a bit later during pattern matching. But now that we have all this, we can define 
 as just a type alias:
Process1
Footnote 13mNote on syntax: recall that if  is a type, 
 references the type 
 defined inside .
x
x#foo
foo
x
Footnote 14mWe are prevented from instantiating an 
, say, because there is only one
Eq[Int,String]
public constructor, 
, which takes just a single type parameter and uses the identity function for
Eq.refl[A]
both 
 and 
.
to
from
15.4.3 Single-input processes
case class Is[I]() {
  sealed trait f[X] { def is: Eq[X,I] }
  val Get = new f[I] { def is = Eq.refl }
}
case class Eq[A,B] private(to: A => B, from: B => A)
object Eq { def refl[A]: Eq[A,A] = Eq(identity, identity) }
289
www.it-ebooks.info

To see what's going on, it helps to substitute the definition of 
 into a
Is[I]#f
call to 
:
Await
From the definition of 
, we can see that 
 has just one possible
One[I]#f
req
value, 
. Therefore, 
 must accept an  as its argument, which
Get: f[I]
recv
I
means that 
 can only be used to request  values. This is important to
Await
I
understand—if this explanation didn't make sense, try working through these
definitions on paper, substituting the type definitions.
Our 
 alias supports all the same operations as our old single-input 
Process1
. Let's look at a couple. We first introduce a few helper functions to
Process
improve type inference when calling the 
 constructors:
Process
Using these, our definitions of, for instance, 
 and 
 look almost
lift
filter
identical to before, except they return a 
:
Process1
type Process1[I,O] = Process[Is[I]#f, O]
case class Await[A,O](
  req: Is[I]#f[A], recv: A => Process[F,O],
  fallback: Process[Is[I]#f,O] = Halt[F,O](),
  cleanup: Process[Is[I]#f,O] = Halt[F,O]()) extends Process[Is[I]#f,R]
def await1[I,O](recv: I => Process1[I,O],
                fb: Process1[I,O] = Halt()): Process1[I,O] =
  await(Get[I])(recv, fb)
def emit1[I,O](h: O,
               tl: Process1[I,O] = halt1[I,O]): Process1[I,O] =
  emit(h, tl)
def emitAll1[I,O](h: Seq[O],
                  tl: Process1[I,O] = halt1[I,O]): Process1[I,O] =
  emitAll(h, tl)
def halt1[I,O]: Process1[I,O] = Halt[Is[I]#f, O]()
def lift[I,O](f: I => O): Process1[I,O] =
  await1[I,O](i => emit(f(i))) repeat
def filter[I](f: I => Boolean): Process1[I,I] =
  await1[I,I](i => if (f(i)) emit(i) else halt1) repeat
290
www.it-ebooks.info

Let's look at process composition next. The implementation looks very similar
to before, but we make sure to run the 
 of the left process before the
finalizer
right process halts. (Recall that we are using the 
 argument of 
finalizer
Await
to finalize resources—see the implementation of the 
 combinator from
resource
earlier.)
We use a helper function, 
—it runs the 
 of a 
 but
kill
cleanup
Process
ignores any of its remaining output:
def |>[O2](p2: Process1[O,O2]): Process[F,O2] = {
  // if this is emitting values and p2 is consuming values,
  // we feed p2 in a loop to avoid using stack space
  @annotation.tailrec
  def feed(emit: Seq[O], tail: Process[F,O], recv: O => Process1[O,O2],
           fb: Process1[O,O2], cleanup: Process1[O,O2]): Process[F,O2] =
    if (emit isEmpty) tail |> await1(recv, fb)
    else recv(emit.head) match {
      case Await(_, recv2, fb2, c2) =>
        feed(emit.tail, tail, recv2, fb2, c2)
      case p => Emit(emit.tail, tail) |> p
    }
  p2 match {
    case Halt() => this.kill ++ Halt()
    case Emit(h,t) => emitAll(h, this |> t)
    case Await(req,recv,fb,c) => this match {
      case Emit(h,t) => feed(h, t, recv, fb, c)
      case Halt() => Halt() |> fb
      case Await(req0,recv0,fb0,c0) =>
        await(req0)(i => recv0(i) |> p2, fb0 |> fb, c0 |> c)
    }
  }
}
@annotation.tailrec
final def kill[O2]: Process[F,O2] = this match {
  case Await(req,recv,fb,c) => c.drain
  case Halt() => Halt()
  case Emit(h, t) => t.kill
}
def drain[O2]: Process[F,O2] = this match {
  case Halt() => Halt()
  case Emit(h, t) => t.drain
  case Await(req,recv,fb,c) => Await(
    req, recv andThen (_.drain),
    fb.drain, c.drain)
}
291
www.it-ebooks.info

Note that 
 is defined for any 
 type, so this operation works
|>
Process[F,O]
for transforming a 
 value, an effectful 
, and the
Process1
Process[IO,O]
two-input 
 type we will discuss next.
Process
With 
, we can add convenience functions on 
 for attaching various
|>
Process
 transformations to the output. For instance, here's 
, defined
Process1
filter
for any 
:
Process[F,O]
We can add similar convenience functions for 
, 
, and so on.
take takeWhile
See the chapter code for more examples.
One of the scenarios we mentioned earlier was zipping or merging of input
streams:
'Zip' together two files, 
 and 
, add corresponding
f1.txt
f2.txt
temperatures together, convert the result to celsius, apply a 5-element moving
average, and output to 
.
celsius.txt
We can address these sorts of scenarios with 
 as well. Much like
Process
sources and 
 were just a specific instance of our general 
Process1
Process
type, a 
, which combines two input streams in some way,
 can also be
Tee
15
expressed by our 
 type. Once again, we simply craft an appropriate
Process
choice of :
F
Footnote 15mThe name 'Tee' comes from the letter 'T', which approximates a diagram merging two inputs (the
top of the 'T') into a single output.
This looks quite similar to our 
 type from earlier, except that we now have
Is
two possible values,  and , and we get an 
L
R
Either[Eq[X,I], Eq[X,I2]]
for pattern matching. With , we can now define a type alias, 
, which accepts
T
Tee
def filter(f: O => Boolean): Process[F,O] =
  this |> Process.filter(f)
15.4.4 Multiple input streams
case class T[I,I2]() {
  sealed trait f[X] { def get: Either[Eq[X,I], Eq[X,I2]] }
  val L = new f[I] { def get = Left(Eq.refl) }
  val R = new f[I2] { def get = Right(Eq.refl) }
}
def L[I,I2] = T[I,I2]().L
def R[I,I2] = T[I,I2]().R
292
www.it-ebooks.info

two inputs:
Once again, we define a few convenience functions for building these particular
types of 
:
Process
Let's define some 
 combinators. Zipping is a special case of 
—we read
Tee
Tee
from the left, then the right (or vice versa), then emit the pair. Notice we get to be
explicit about the order we read from the inputs, a capability that can be important
when a 
 is talking to streams with external effects.
Tee
16
Footnote 16mWe may also wish to be 
 about the order of the effects, allowing the driver to choose
inexplicit
nondeterministically and allowing for the possibility that the driver will execute both effects concurrently. See the
chapter notes and chapter code for some additional discussion of this.
This transducer will halt as soon as either input is exhausted, just like the zip
funtion on 
. Let's define a 
 which continues as long as either
List
zipWithAll
input has elements. We accept a value to 'pad' each side with when its elements run
out:
type Tee[I,I2,O] = Process[T[I,I2]#f, O]
def awaitL[I,I2,O](
    recv: I => Tee[I,I2,O],
    fallback: Tee[I,I2,O] = haltT[I,I2,O]): Tee[I,I2,O] =
  await[T[I,I2]#f,I,O](L)(recv, fallback)
def awaitR[I,I2,O](
    recv: I2 => Tee[I,I2,O],
    fallback: Tee[I,I2,O] = haltT[I,I2,O]): Tee[I,I2,O] =
  await[T[I,I2]#f,I2,O](R)(recv, fallback)
def haltT[I,I2,O]: Tee[I,I2,O] =
  Halt[T[I,I2]#f,O]()
def emitT[I,I2,O](h: O, tl: Tee[I,I2,O] = haltT[I,I2,O]): Tee[I,I2,O] =
  emit(h, tl)
def zipWith[I,I2,O](f: (I,I2) => O): Tee[I,I2,O] =
  awaitL[I,I2,O](i  =>
  awaitR        (i2 => emitT(f(i,i2)))) repeat
def zip[I,I2]: Tee[I,I2,(I,I2)] = zipWith((_,_))
293
www.it-ebooks.info

This uses a few helper functions—
 and 
 ignore one of the inputs
passR
passL
to a 
 and echo the other branch.
Tee
awaitLOr and 
 just call 
 with the 
 argument as
awaitROr
await
fallback
the first argument, which is a bit more readable here.
There are a lot of other 
 combinators we could write. Nothing requires that
Tee
we read values from each input in lockstep. We could read from one input until
some condition is met, then switch to the other; we could read five values from the
left, then ten values from the right, read a value from the left then use it to
determine how many values to read from the right, and so on.
We will typically want to feed a 
 by connecting it to two processes. We can
Tee
define a function on 
 that combines 
 processes using a 
. This
Process
two
Tee
function works for any 
 type:
Process
def zipWithAll[I,I2,O](padI: I, padI2: I2)(
                       f: (I,I2) => O): Tee[I,I2,O] = {
  val fbR = passR[I,I2] map (f(padI, _    ))
  val fbL = passL[I,I2] map (f(_   , padI2))
  awaitLOr(fbR)(i =>
  awaitROr(fbL)(i2 => emitT(f(i,i2)))) repeat
}
def passR[I,I2]: Tee[I,I2,I2] = awaitR(emitT(_, passR))
def passL[I,I2]: Tee[I,I2,I] = awaitL(emitT(_, passL))
def awaitLOr[I,I2,O](fallback: Tee[I,I2,O])(
                     recvL: I => Tee[I,I2,O]): Tee[I,I2,O] =
  awaitL(recvL, fallback)
def awaitROr[I,I2,O](fallback: Tee[I,I2,O])(
                     recvR: I2 => Tee[I,I2,O]): Tee[I,I2,O] =
  awaitR(recvR, fallback)
def tee[O2,O3](p2: Process[F,O2])(t: Tee[O,O2,O3]): Process[F,O3] =
  t match {
    case Halt() => this.kill ++ p2.kill ++ Halt()
    case Emit(h,t) => Emit(h, (this tee p2)(t))
    case Await(side, recv, fb, c) => side.get match {
      case Left(isO) => this match {
        case Halt() => p2.kill ++ Halt()
        case Emit(o,ot) =>
          feedL(o, ot, p2, isO.to andThen recv, fb, c)
        case Await(reqL, recvL, fbL, cL) =>
294
www.it-ebooks.info

This uses two helper functions, 
 and 
, which serve the same
feedL
feedR
purpose as before—to feed the 
 in a loop as long as it expects values from
Tee
either side. See the chapter code for the full definition.
The one subtlety in this definition is we make sure to run cleanup for both
inputs before halting. What is nice about this overall approach is that we have
exactly four places in the library where we must do anything to ensure resource
safety: 
, 
, 
 and the 
 interpreter. All the other client
tee |> resource
collect
code that uses these and other combinators is guaranteed to be resource safe.
How do we perform output using our 
 type? We will often want to send
Process
the output of a 
 to some 
 (perhaps sending a 
Source[O]
Sink
 to an output file). Somewhat surprisingly, we can represent
Source[String]
sinks in terms of sources!
This makes a certain kind of sense. A 
 provides a sequence
Sink[F[_], O]
of functions to call with the input type . The function returns 
 (later,
O
F[Unit]
we'll see how to get back values from sinks). Let's look at a file 
 that writes
Sink
strings to a file:
          Await(reqL, recvL andThen (this2 => (this2 tee p2)(t)),
                (fbL tee p2)(t), (cL tee p2)(t))
      }
      case Right(isO2) => p2 match {
        case Halt() => this.kill ++ Halt()
        case Emit(o,ot) =>
          feedR(o, ot, this, isO2.to andThen recv, fb, c)
        case Await(reqR, recvR, fbR, cR) =>
          Await(reqR, recvR andThen (p3 => (this tee p3)(t)),
                (this tee fbR)(t), (this tee cR)(t))
      }
    }
  }
15.4.5 Sinks
type Sink[F[_],O] = Process[F[_], O => F[Unit]]
def fileW(file: String, append: Boolean = false): Sink[IO,String] =
  resource(IO { new java.io.FileWriter(file, append) })(
           w => IO(w.close)) {
    w => IO { (s: String) => IO(w.write(s)) }
  }
295
www.it-ebooks.info

That was easy. And notice what 
 included—there is no exception handling
isn't
code here—the combinators we are using guarantee that the 
 will be
FileWriter
closed if exceptions occur or when whatever is feeding the 
 signals it is done.
Sink
We can use 
 to implement a combinator 
, which pipes the output of a 
tee
to
 to a 
:
Process
Sink
EXERCISE 9: The definition of 
 uses a new combinator, 
, defined for
to
eval
any 
, which just runs all the actions emitted. Implement 
.
Process
eval
Using 
, we can now write programs like:
to
When run via 
, this will open the input file and the output file and
collect
incrementally transform the input stream, ignoring commented lines.
We can generalize 
 to allow responses other than 
. The implementation is
to
Unit
identical! The operation had a more general type than we gave it before. Let's call
this operation 
:
through
Let's introduce a type alias for this pattern:
def to[O2](snk: Sink[F,O]): Process[F,Unit] =
  eval { (this zipWith p2)((o,f) => f(o) }
def eval[F[_],O](p: Process[F, F[O]]): Process[F,O]
val converter: Process[IO,Unit] =
  lines("fahrenheit.txt").
  filter(!_.startsWith("#")).
  map(line => fahrenheitToCelsius(line.toDouble).toString).
  to(fileW("celsius.txt")).
  drain
15.4.6 Effectful channels
def through[O2](p2: Process[F, O => F[O2]]): Process[F,O2] =
  eval { (this zipWith p2)((o,f) => f(o)) }
type Channel[F[_],I,O] = Process[F, I => F[O]]
296
www.it-ebooks.info

Channel is useful when a pure pipeline must execute some I/O action as one
of its stages. A typical example might be an application that needs to execute
database queries. It would be nice if our database queries could return a 
, where 
 is some representation of a database row. This would
Source[Row]
Row
allow the program to process the result set of a query using all the fancy stream
transducers we've built up so far.
Here's a very simple query executor, which uses 
 as the
Map[String,Any]
(untyped) row representation:
We 
could 
certainly 
write 
a 
Channel[PreparedStatement,
, why don't we do that? Because we don't want
Source[Map[String,Any]]]
code that uses our 
 to have to worry about how to obtain a 
Channel
 (which is needed to build a 
). That
Connection
PreparedStatement
dependency is managed entirely by the 
 itself, which also takes care of
Channel
closing the connection when it is finished executing queries (this is guaranteed by
the implementation of 
).
resource
This implementation is is directly closing the connection when finished. A real
application may obtain the connections from some sort of connection pool and
release the connection back to the pool when finished. This can be done just by
passing different arguments to the 
 combinator.
resource
import java.sql.{Connection, PreparedStatement, ResultSet}
def query(conn: IO[Connection]):
    Channel[IO, Connection => PreparedStatement,
                Process[IO,Map[String,Any]]] =
  resource(conn)(c => IO(c.close)) { conn => IO {
    (q: Connection => PreparedStatement) => {
      IO { resource ( IO {
        val rs = q(conn).executeQuery
        val ncols = rs.getMetaData.getColumnCount
        val colnames = (1 to ncols).map(rs.getMetaData.getColumnName)
        (rs, colnames)
      }) ( p => IO { p._1.close } ) { // close the ResultSet
        case (rs, cols) => IO {
          if (!rs.next) throw End
          else cols.map(c => (c, rs.getObject(c): Any)).toMap
        }
      }}
    }
  }}
297
www.it-ebooks.info

Realistic programs may need to allocate resources dynamically, while transforming
some input stream. Recall the scenarios we mentioned earlier:
Dynamic resource allocation: Read a file, 
, containing a list of
fahrenheits.txt
filenames. Concatenate these files into a single logical stream, convert this stream to
celsius, and output the joined stream to 
.
celsius.txt
Multi-sink output: As above, but rather than producing a single output file, produce an
output file for each input file in 
. Name the output file by appending 
fahrenheits.txt
 onto the input file name.
.celsius
Can these capabilities be incorporated into our definition of 
, in a
Process
way that preserves resource safety? Yes, they can! We actually already have the
power to do these things, using the 
 combinator that we have already
flatMap
defined for an arbitrary 
 type.
Process
For instance, 
 plus our existing combinators let us write this first
flatMap
scenario as:
Trim the stream to at most a single element; see chapter code
We can give eval infix syntax using implicits; see chapter code for details
This code is completely resource-safe—all file handles will be closed
automatically by the runner as soon as they are finished, even in the presence of
exceptions. Any exceptions encountered will be thrown to the 
 function
collect
when invoked.
We can write to multiple files just by switching the order of the calls to 
:
flatMap
15.4.7 Dynamic resource allocation
val convertAll: Process[IO,Unit] = (for {
  out <- fileW("celsius.txt").once
  file <- lines("fahrenheits.txt")
  _ <- lines(file).
       map(line => fahrenheitToCelsius(line.toDouble)).
       map(celsius => out(celsius.toString)).
       eval
} yield ()) drain
val convertMultisink: Process[IO,Unit] = (for {
  file <- lines("fahrenheits.txt")
  _ <- lines(file).
       map(line => fahrenheitToCelsius(line.toDouble)).
298
www.it-ebooks.info

And of course, we can attach transformations, mapping, filtering and so on at
any point in the process:
There are additional examples using this library in the chapter code.
The ideas presented in this chapter are extremely widely applicable. A surprising
number of programs can be cast in terms of stream processing—once you are
aware of the abstraction, you begin seeing it everywhere. Let's look at some
domains where it is applicable:
File I/O: We've already demonstrated how to use stream processing for file I/O. Although
we have focused on line-by-line reading and writing for the examples here, we can also
use the library for processing binary files.
Message processing, state machines, and actors: Large systems are often organized as a
system of loosely-coupled components that communicate via message passing. These
systems are often expressed in terms of 
, which communicate via explicit message
actors
sends and receives. We can express components in these architectures as stream
processors, which lets us describe extremely complex state machines and behaviors while
retaining a high-level, compositional API.
Servers, web applications: A web application can be thought of as converting a stream of
HTTP requests to a stream HTTP responses.
UI programming: We can view individual UI events such as mouseclicks as streams, and
the UI as one large network of stream processors determining how the UI responds to
user interaction.
Big data, distributed systems: Stream processing libraries can be 
 and 
distributed
 for processing large amounts of data. The key insight here is that 
parallelized
Process
values being composed need not all live on the same machine.
If you're curious to learn more about these applications (and others), see the
chapter notes for additional discussion and links to further reading. The chapter
       map(_ toString).
       to(fileW(file + ".celsius"))
} yield ()) drain
val convertMultisink2: Process[IO,Unit] = (for {
  file <- lines("fahrenheits.txt")
  _ <- lines(file).
       filter(!_.startsWith("#")).
       map(line => fahrenheitToCelsius(line.toDouble)).
       filter(_ > 0). // ignore below zero temperatures
       map(_ toString).
       to(fileW(file + ".celsius"))
} yield ()) drain
15.5 Applications
299
www.it-ebooks.info

notes and code also discuss some extensions to the 
 type we discussed
Process
here, including the introduction of 
 which allows for
nondeterministic choice
concurrent evaluation in the execution of a 
.
Process
We began this book with the introduction of a simple premise: that we assemble
our programs using only pure functions. From this sole premise and its
consequences we were led to develop a new approach to programming, one with its
own ideas, techniques, and abstractions. In this final chapter, we constructed a
library for stream processing and incremental I/O, demonstrating that we can retain
the compositional style developed throughout this book even for programs that
interact with the outside world. 
 of how to use FP to
Our story is now complete
architect programs both large and small.
While good design is always hard, over time, expressing code functionally
becomes effortless. By this point, you have all the tools needed to start functional
, no matter the programming task. FP is a deep subject, and as you
programming
apply it to more problems, new ideas and techniques will emerge. Enjoy the
journey, keep learning, and good luck!
15.6 Conclusion
300
www.it-ebooks.info

causal streams
compositionality
compositional style
composition of processes
driver
early termination
equality witness
fusion
imperative programming
incremental I/O
lazy I/O
memoization
multi-input transducer
pipe
pipeline
resource
resource leak
resource safety
resource safety
resource safety
resource safety
smart constructor
sources
state machine
stream processor
stream transducer
Tee
Wye
Index Terms
301
www.it-ebooks.info

