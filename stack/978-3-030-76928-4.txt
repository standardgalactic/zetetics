Emergence, Complexity and Computation ECC
Alexey Piunovskiy
Yi Zhang   Editors
Modern Trends 
in Controlled 
Stochastic 
Processes 
Theory and Applications, V.III

Emergence, Complexity and Computation
Volume 41
Series Editors
Ivan Zelinka, Technical University of Ostrava, Ostrava, Czech Republic
Andrew Adamatzky, University of the West of England, Bristol, UK
Guanrong Chen, City University of Hong Kong, Hong Kong, China
Editorial Board Members
Ajith Abraham, MirLabs, USA
Ana Lucia, Universidade Federal do Rio Grande do Sul, Porto Alegre, Rio Grande
do Sul, Brazil
Juan C. Burguillo, University of Vigo, Spain
Sergej Čelikovský, Academy of Sciences of the Czech Republic, Czech Republic
Mohammed Chadli, University of Jules Verne, France
Emilio Corchado, University of Salamanca, Spain
Donald Davendra, Technical University of Ostrava, Czech Republic
Andrew Ilachinski, Center for Naval Analyses, USA
Jouni Lampinen, University of Vaasa, Finland
Martin Middendorf, University of Leipzig, Germany
Edward Ott, University of Maryland, USA
Linqiang Pan, Huazhong University of Science and Technology, Wuhan, China
Gheorghe Păun, Romanian Academy, Bucharest, Romania
Hendrik Richter, HTWK Leipzig University of Applied Sciences, Germany
Juan A. Rodriguez-Aguilar
, IIIA-CSIC, Spain
Otto Rössler, Institute of Physical and Theoretical Chemistry, Tübingen, Germany
Vaclav Snasel, Technical University of Ostrava, Czech Republic
Ivo Vondrák, Technical University of Ostrava, Czech Republic
Hector Zenil, Karolinska Institute, Sweden

The Emergence, Complexity and Computation (ECC) series publishes new
developments, advancements and selected topics in the ﬁelds of complexity,
computation and emergence. The series focuses on all aspects of reality-based
computation approaches from an interdisciplinary point of view especially from
applied sciences, biology, physics, or chemistry. It presents new ideas and
interdisciplinary insight on the mutual intersection of subareas of computation,
complexity and emergence and its impact and limits to any computing based on
physical limits (thermodynamic and quantum limits, Bremermann’s limit, Seth
Lloyd limits…) as well as algorithmic limits (Gödel’s proof and its impact on
calculation, algorithmic complexity, the Chaitin’s Omega number and Kolmogorov
complexity, non-traditional calculations like Turing machine process and its
consequences,…) and limitations arising in artiﬁcial intelligence. The topics are
(but not limited to) membrane computing, DNA computing, immune computing,
quantum computing, swarm computing, analogic computing, chaos computing and
computing on the edge of chaos, computational aspects of dynamics of complex
systems (systems with self-organization, multiagent systems, cellular automata,
artiﬁcial life,…), emergence of complex systems and its computational aspects, and
agent based computation. The main aim of this series is to discuss the above
mentioned topics from an interdisciplinary point of view and present new ideas
coming from mutual intersection of classical as well as modern methods of
computation. Within the scope of the series are monographs, lecture notes, selected
contributions from specialized conferences and workshops, special contribution
from international experts.
Indexed by zbMATH.
More information about this series at http://www.palgrave.com/gp/series/10624

Alexey Piunovskiy
• Yi Zhang
Editors
Modern Trends in Controlled
Stochastic Processes
Theory and Applications, V.III
123

Editors
Alexey Piunovskiy
Department of Mathematical Sciences
University of Liverpool
Liverpool, UK
Yi Zhang
Department of Mathematical Sciences
University of Liverpool
Liverpool, UK
ISSN 2194-7287
ISSN 2194-7295
(electronic)
Emergence, Complexity and Computation
ISBN 978-3-030-76927-7
ISBN 978-3-030-76928-4
(eBook)
https://doi.org/10.1007/978-3-030-76928-4
© The Editor(s) (if applicable) and The Author(s), under exclusive license
to Springer Nature Switzerland AG 2021
This work is subject to copyright. All rights are solely and exclusively licensed by the Publisher, whether
the whole or part of the material is concerned, speciﬁcally the rights of translation, reprinting, reuse of
illustrations, recitation, broadcasting, reproduction on microﬁlms or in any other physical way, and
transmission or information storage and retrieval, electronic adaptation, computer software, or by similar
or dissimilar methodology now known or hereafter developed.
The use of general descriptive names, registered names, trademarks, service marks, etc. in this
publication does not imply, even in the absence of a speciﬁc statement, that such names are exempt from
the relevant protective laws and regulations and therefore free for general use.
The publisher, the authors and the editors are safe to assume that the advice and information in this
book are believed to be true and accurate at the date of publication. Neither the publisher nor the
authors or the editors give a warranty, expressed or implied, with respect to the material contained
herein or for any errors or omissions that may have been made. The publisher remains neutral with regard
to jurisdictional claims in published maps and institutional afﬁliations.
This Springer imprint is published by the registered company Springer Nature Switzerland AG
The registered company address is: Gewerbestrasse 11, 6330 Cham, Switzerland

Preface
This book contains extended versions of selected reports presented at the traditional
Liverpool workshop on controlled stochastic processes in July 2021. These are
independent research papers on Markov decision processes, optimal stopping
problems, stochastic games, reinforcement learning, optimization algorithms, sys-
tem control theory, queueing networks, scheduling, etc. Along with new theoretical
results and open problems, many chapters contain case studies and applications to
real-life problems. This book can be useful for active researchers in the afore-
mentioned ﬁelds and also to practitioners interested in applying mathematical
methods to the problems arising in ﬁnance, economics, queueing systems,
telecommunication, and so on.
v

Introduction
Alexey B. Piunovskiy& and Yi Zhang
University of Liverpool, Department of Mathematical Sciences,
Liverpool L69 7ZL, UK
piunov@liv.ac.uk, yi.zhang@liverpool.ac.uk
The traditional workshop in Liverpool was initially scheduled for the summer 2020.
Because of the COVID-19 pandemic, it was postponed till July 2021. Like in 2010
and 2015, we expect that world-class and active experts will be able to meet in
Liverpool or at least to participate in a series of Zoom meetings to discuss inter-
esting and challenging problems of stochastic optimal control. This book contains
several extended reports from the mentioned forthcoming workshop. We hope, it
will enable researchers, academics, and research students to get a sense of novel and
interesting results, concepts, models, methods, and applications of controlled
stochastic processes. Below, we brieﬂy describe the topics touched in the further
chapters. Roughly speaking, chapters [3–6, 8, 10–12, 15, 18, 19] are mainly the-
oretical, although include a lot of meaningful examples. Chapters [1, 2, 7, 9, 13, 14,
20] are more problem-oriented and contain case studies.
Models and Methods. Classical discrete-time Markov decision processes
(MDPs) are considered in [3, 4, 6, 8, 9, 12, 15]; continuous-time Markov,
semi-Markov, and more general processes are considered in [2, 5, 10, 11, 19].
Chapters [4, 14, 18] are about various types of stochastic games, including the game
against the nature [4]. Let us underline that many authors investigate the models
with partial information [3–5, 9, 12, 18, 19] which are deservedly considered to be
more challenging.
As for the methods, dynamic programming is useful on many occasions [3, 4, 6,
8–10, 15]. When some probabilities (e.g., describing the dynamics of the process)
are not precisely known, the Bayesian approach [9, 12, 14], Q-learning [3, 4],
optimal ﬁltering [19], robust control [1, 4, 12], and H2 control [5] can be useful. Let
us also mention variational inequalities [11] and self-organizing algorithms [7].
Many authors suggested new effective numerical methods for tackling optimal
control problems [3, 4, 6, 7, 10, 12, 13], especially arising from real-life case
studies. Results of essential computer calculations and simulations are presented in
[1, 3–5, 7, 9, 10, 13, 14, 18, 20].
Compared with the workshops in 2010 and 2015 [16, 17], we decided to give
more attention to applications of the optimal control theory to real-life problems. As
a result, the following case studies and meaningful examples are presented:
vii

• regulation of the adaptive immune response [1];
• efﬁciency of allocating the same job(s) to several servers in queueing systems
(survey) [2];
• forest management [3];
• control of moving objects [3,7];
• control of water resources [4];
• control of an unmanned aircraft subject to actuator faults [5];
• optimal economic growth [6];
• screening program for women breast cancer [9];
• portfolio optimization [10];
• scheduling theory [13];
• optimization of the strategies of a defender and an attacker (terrorist) in a
generalized Blotto game [14];
• optimization of advertising efforts [18];
• Jackson networks [19];
• optimization of the targeted drug delivery system [20].
Acknowledgements. All the authors are thankful to the Engineering and Physical
Sciences Research Council (EPSRC, UK, grant EP/T018216/1) and to the Research
Centre in Mathematics and Modelling (RCMM, Uni. of Liverpool) for the ﬁnancial
support of the workshop “Modern Trends in Controlled Stochastic Processes:
Theory and Applications” to be held at the Dept. of Mathematical Sciences of the
University of Liverpool in July 2021.
References
1. Almudevar, A.: A regulatory principle for robust reciprocal-time decay of the adaptive
immune response. In: Piunovskiy, A., Zhang, Y. (eds.) Modern Trends in Controlled
Stochastic Processes. ECC, vol. 41, pp. 298–312. Springer, Cham (2021). https://doi.org/10.
1007/978-3-030-76928-4_15
2. Anton, E., Ayesta, U., Jonckheere, M., Verloop, I.M.: A survey of stability results for
redundancy systems. In: Piunovskiy, A., Zhang, Y. (eds.) Modern Trends in Controlled
Stochastic Processes. ECC, vol. 41, pp. 266–283. Springer, Cham (2021). https://doi.org/10.
1007/978-3-030-76928-4_13
3. Avrachenkov, K.E., Borkar, V.S., Dolhare, H.P., Patil, K.: Full gradient DQN reinforcement
learning: a provably convergent scheme. In: Piunovskiy, A., Zhang, Y. (eds.) Modern Trends
in Controlled Stochastic Processes. ECC, vol. 41, pp. 192–220. Springer, Cham (2021).
https://doi.org/10.1007/978-3-030-76928-4_10
4. Bäuerle, N., Glauner, A.: Q-learning for distributionally robust Markov decision processes. In:
Piunovskiy, A., Zhang, Y. (eds.) Modern Trends in Controlled Stochastic Processes. ECC, vol.
41, pp. 108–128. Springer, Cham (2021). https://doi.org/10.1007/978-3-030-76928-4_6
5. de Oliveira, A.M., Costa, O.L.V.: Control of continuous-time Markov jump linear systems
with partial information. In: Piunovskiy, A.., Zhang, Y. (eds.) Modern Trends in Controlled
Stochastic Processes. ECC, vol. 41, pp. 87–107. Springer, Cham (2021). https://doi.org/10.
1007/978-3-030-76928-4_5
6. Deng, F., Guo, X., Zhang, Y.: On ﬁnite approximations to Markov decision processes with
recursive and nonlinear discounting. In: Piunovskiy, A., Zhang, Y. (eds.) Modern Trends in
Controlled Stochastic Processes. ECC, vol. 41, pp. 221–247. Springer, Cham (2021). https://
doi.org/10.1007/978-3-030-76928-4_11
viii
A. B. Piunovskiy and Y. Zhang

7. Diep, Q.B., Truong, T.C., Zelinka, I.: Swarm intelligence and swarm robotics in the path
planning problem. In: Piunovskiy, A., Zhang, Y. (eds.) Modern Trends in Controlled
Stochastic Processes. ECC, vol. 41, pp. 313–327. Springer, Cham (2021). https://doi.org/10.
1007/978-3-030-76928-4_16
8. Feinberg, E.A., Kasyanov, P.O., Zgurovsky, M.Z.: Average cost Markov decision processes
with semi-uniform Feller transition probabilities. In: Piunovskiy, A., Zhang, Y. (eds.) Modern
Trends in Controlled Stochastic Processes. ECC, vol. 41, pp. 1–18. Springer, Cham (2021).
https://doi.org/10.1007/978-3-030-76928-4_1
9. Horiguchi, M.: On an approach to evaluation of health care programme by Markov decision
model. In: Piunovskiy, A., Zhang, Y. (eds.) Modern Trends in Controlled Stochastic
Processes. ECC, vol. 41, pp. 341–354. Springer, Cham (2021). https://doi.org/10.1007/978-3-
030-76928-4_18
10. Huo, H., Wen, X.: First passage exponential optimality problem for semi-Markov decision
processes. In: Piunovskiy, A., Zhang, Y. (eds.) Modern Trends in Controlled Stochastic
Processes. ECC, vol. 41, pp. 19–37. Springer, Cham (2021). https://doi.org/10.1007/978-3-
030-76928-4_2
11. Jasso-Fuentes, H., Menaldi, J.-L., Vásquez-Rojas, F.: Optimal stopping problems for a family
of continuous-time Markov processes. In: Piunovskiy, A., Zhang, Y. (eds.) Modern Trends in
Controlled Stochastic Processes. ECC, vol. 41, pp. 57–86. Springer, Cham (2021). https://doi.
org/10.1007/978-3-030-76928-4_4
12. Kara, A.D., Yüksel, S.: Robustness to approximations and model learning in MDPs and
POMDPs. In: Piunovskiy, A.., Zhang, Y. (eds.) Modern Trends in Controlled Stochastic
Processes. ECC, vol. 41, pp. 166–191. Springer, Cham (2021). https://doi.org/10.1007/978-3-
030-76928-4_9
13. Lipets, V., Zadorojniy, A.: IBM crew pairing and rostering optimization (C-PRO) technology
with MDP for optimization ﬂow orchestration. In: Piunovskiy, A., Zhang, Y. (eds.) Modern
Trends in Controlled Stochastic Processes. ECC, vol. 41, pp. 284–297. Springer, Cham
(2021). https://doi.org/10.1007/978-3-030-76928-4_14
14. Liu, L., Sonin, I.M.: Locks, bombs and testing: the case of independent locks. In: Piunovskiy,
A., Zhang, Y. (eds.) Modern Trends in Controlled Stochastic Processes. ECC, vol. 41,
pp. 248–265. Springer, Cham (2021). https://doi.org/10.1007/978-3-030-76928-4_12
15. Piunovskiy, A.B.: Controlled random walk: conjecture and counter-example. In: Piunovskiy,
A., Zhang, Y. (eds.) Modern Trends in Controlled Stochastic Processes. ECC, vol. 41, pp. 38–
56. Springer, Cham (2021). https://doi.org/10.1007/978-3-030-76928-4_3
16. Piunovskiy, A.B. (ed.): Modern Trends in Controlled Stochastic Processes: Theory and
Applications. Luniver Press, Frome (2010)
17. Piunovskiy, A.B. (ed.): Modern Trends in Controlled Stochastic Processes: Theory and
Applications, V.II. Luniver Press, Frome (2015)
18. Robles-Aguilar, A.D., González-Sánchez, D., Minjárez-Sosa, J.A.: Estimation of equilibria in
an advertising game with unknown distribution of the response to advertising efforts. In:
Piunovskiy, A., Zhang, Y. (eds.) Modern Trends in Controlled Stochastic Processes. ECC, vol.
41, pp. 148–165. Springer, Cham (2021). https://doi.org/10.1007/978-3-030-76928-4_8
19. Semenikhin, K.V.: State estimation in partially observed stochastic networks with queueing
applications. In: Piunovskiy, A., Zhang, Y. (eds.) Modern Trends in Controlled Stochastic
Processes. ECC, vol. 41, pp. 129–147. Springer, Cham (2021). https://doi.org/10.1007/978-3-
030-76928-4_7
20. Tsompanas, M.-A., Bull, L., Adamatzky, A., Balaz, I.: Utilizing differential evolution into
optimizing targeted cancer treatments. In: Piunovskiy, A., Zhang, Y. (eds.) Modern Trends in
Controlled Stochastic Processes. ECC, vol. 41, pp. 328–340. Springer, Cham (2021). https://
doi.org/10.1007/978-3-030-76928-4_17
Introduction
ix

Contents
Average Cost Markov Decision Processes with Semi-Uniform Feller
Transition Probabilities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1
Eugene A. Feinberg, Pavlo O. Kasyanov, and Michael Z. Zgurovsky
First Passage Exponential Optimality Problem for Semi-Markov
Decision Processes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
19
Haifeng Huo and Xian Wen
Controlled Random Walk: Conjecture and Counter-Example . . . . . . . .
38
Alexey B. Piunovskiy
Optimal Stopping Problems for a Family of Continuous-Time
Markov Processes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
57
Héctor Jasso-Fuentes, Jose-Luis Menaldi, and Fidel Vásquez-Rojas
Control of Continuous-Time Markov Jump Linear Systems
with Partial Information . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
87
André Marcorin de Oliveira and Oswaldo Luiz do Valle Costa
Q-Learning for Distributionally Robust Markov Decision Processes . . .
108
Nicole Bäuerle and Alexander Glauner
State Estimation in Partially Observed Stochastic Networks
with Queueing Applications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
129
Konstantin V. Semenikhin
Estimation of Equilibria in an Advertising Game with Unknown
Distribution of the Response to Advertising Efforts . . . . . . . . . . . . . . . .
148
Alan D. Robles-Aguilar, David González-Sánchez,
and J. Adolfo Minjárez-Sosa
Robustness to Approximations and Model Learning in MDPs
and POMDPs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
166
Ali Devran Kara and Serdar Yüksel
xi

Full Gradient DQN Reinforcement Learning: A Provably
Convergent Scheme . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
192
Konstantin E. Avrachenkov, Vivek S. Borkar, Hars P. Dolhare,
and Kishor Patil
On Finite Approximations to Markov Decision Processes
with Recursive and Nonlinear Discounting . . . . . . . . . . . . . . . . . . . . . . .
221
Fan Deng, Xin Guo, and Yi Zhang
Locks, Bombs and Testing: The Case of Independent Locks . . . . . . . . .
248
Li Liu and Isaac M. Sonin
A Survey of Stability Results for Redundancy Systems . . . . . . . . . . . . .
266
Elene Anton, Urtzi Ayesta, Matthieu Jonckheere, and Ina Maria Verloop
IBM Crew Pairing and Rostering Optimization (C-PRO) Technology
with MDP for Optimization Flow Orchestration . . . . . . . . . . . . . . . . . .
284
Vladimir Lipets and Alexander Zadorojniy
A Regulatory Principle for Robust Reciprocal-Time Decay
of the Adaptive Immune Response . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
298
Anthony Almudevar
Swarm Intelligence and Swarm Robotics in the Path
Planning Problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
313
Quoc Bao Diep, Thanh Cong Truong, and Ivan Zelinka
Utilizing Differential Evolution into Optimizing Targeted
Cancer Treatments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
328
Michail-Antisthenis Tsompanas, Larry Bull, Andrew Adamatzky,
and Igor Balaz
On an Approach to Evaluation of Health Care Programme
by Markov Decision Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
341
Masayuki Horiguchi
Author Index. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
355
xii
Contents

Average Cost Markov Decision Processes
with Semi-Uniform Feller Transition
Probabilities
Eugene A. Feinberg1(B), Pavlo O. Kasyanov2, and Michael Z. Zgurovsky2
1 Department of Applied Mathematics and Statistics, Stony Brook University,
Stony Brook, NY 11794-3600, USA
eugene.feinberg@sunysb.edu
2 Institute for Applied System Analysis, National Technical University
of Ukraine “Kyiv Polytechnic Institute”, Kyiv, Ukraine
kasyanov@i.ua, mzz@kpi.ua
http://www.ams.sunysb.edu/~feinberg/
Abstract. This paper studies average-cost Markov decision processes
with semi-uniform Feller transition probabilities. This class of MDPs
was recently introduced by the authors to study MDPs with incomplete
information. This paper studies the validity of optimality inequalities, the
existence of optimal policies, and the approximations of optimal policies
by policies optimizing total discounted costs.
Keywords: MDP · Average-cost · Semi-uniform Feller transition
probabilities
AMS(2020)
subject
classiﬁcation: Primary 90C40 · Secondary
90C39
1
Introduction
This paper establishes the validity of the optimality inequality and the existence
of stationary optimal policies for Markov Decision Processes (MDPs) with semi-
uniform Feller transition probabilities. It also investigates approximations of
optimal policies by policies minimizing discounted costs when the discount factor
tends to 1. This class of MDPs with semi-uniform Feller transition probabilities
was introduced in [12] because signiﬁcant classes of problems with incomplete
information can be reduced to belief MDPs with semi-uniform Feller transition
probabilities.
The paper deals with MDPs with possibly unbounded cost functions and
noncompact action sets. Such problems were studied in [11] for MDPs with
weakly continuous transition probabilities and in [6,17] for MDPs with setwise
continuous transition probabilities. For MDPs with compact action sets, the
models with weakly and setwise continuous probabilities were studied in [21].
c
⃝The Author(s), under exclusive license to Springer Nature Switzerland AG 2021
A. Piunovskiy and Y. Zhang (Eds.): Modern Trends in Controlled Stochastic Processes,
ECC 41, pp. 1–18, 2021. https://doi.org/10.1007/978-3-030-76928-4_1

2
E. A. Feinberg et al.
2
Model Description
For a metric space S = (S, ρS), where ρS is a metric, let τ(S) be the topology of
S (the family of all open subsets of S), and let B(S) be its Borel σ-ﬁeld, that is,
the σ-ﬁeld generated by all open subsets of the metric space S. For s ∈S and
δ > 0 denote by Bδ(s) and ¯Bδ(s) respectively the open and closed balls in the
metric space S of radius δ with center s and by Sδ(s) the sphere in S of radius δ
with center s. Note that Sδ(s) = ¯Bδ(s) \ Bδ(s). For a subset S of S let ¯S denote
the closure of S and So the interior of S. Then So ⊂S ⊂¯S. So is open and ¯S
is closed. ∂S := ¯S \So denotes the boundary of S. In particular, ∂Bδ(s) = Sδ(s).
We denote by P(S) the set of probability measures on (S, B(S)). A sequence of
probability measures {μ(n)}n = 1,2,... from P(S) converges weakly to μ ∈P(S) if
for any bounded continuous function f on S

S
f(s)μ(n)(ds) →

S
f(s)μ(ds)
as
n →∞.
A sequence of probability measures {μ(n)}n = 1,2,... from P(S) converges in total
variation to μ ∈P(S) if
sup
C∈B(S)
|μ(n)(C) −μ(C)| →0 as n →∞;
(1)
see [3,10,13] for properties of these types of convergence of probability measures.
Note that P(S) is a separable metric space with respect to the topology of weak
convergence for probability measures, when S is a separable metric space; [20,
Chapter II]. Moreover, according to [4, Theorem 8.3.2], if the metric space S
is separable, then the topology of weak convergence of probability measures on
(S, B(S)) coincides with the topology generated by the Kantorovich-Rubinshtein
metric
ρP(S)(μ, ν) :=
sup

S
f(s)μ(ds) −

S
f(s)ν(ds)
 f ∈Lip1(S), sup
s∈S
|f(s)| ≤1

,
(2)
μ, ν ∈P(S), where
Lip1(S) := {f : S →R, |f(s1) −f(s2)| ≤ρS(s1, s2), ∀s1, s2 ∈S}.
For a Borel subset S of a metric space (S, ρS), where ρS is a metric, we always
consider the metric space (S, ρS), where ρS := ρS

S×S. A subset B of S is called
open (closed) in S if B is open (closed respectively) in (S, ρ). Of course, if S = S,
we omit “in S”. Observe that, in general, an open (closed) set in S may not be
open (closed respectively). For S ∈B(S) we denote by B(S) the Borel σ-ﬁeld on
(S, ρS). Observe that B(S) = {S ∩B : B ∈B(S)}.
For metric spaces S1 and S2, a (Borel-measurable) stochastic kernel Ψ(ds1|s2)
on S1 given S2 is a mapping Ψ( · | · ) : B(S1) × S2 →[0, 1], such that Ψ( · |s2) is

Average Cost MDPs with Semi-Uniform Feller Kernels
3
a probability measure on S1 for any s2 ∈S2, and Ψ(B| · ) is a Borel-measurable
function on S2 for any Borel set B ∈B(S1). A stochastic kernel Ψ(ds1|s2) on
S1 given S2 deﬁnes a Borel measurable mapping s2 →Ψ( · |s2) of S2 to the
metric space P(S1) endowed with the topology of weak convergence. A stochastic
kernel Ψ(ds1|s2) on S1 given S2 is called weakly continuous (continuous in total
variation), if Ψ( · |s(n)) converges weakly (in total variation) to Ψ( · |s) whenever
s(n) converges to s in S2. For one-point sets {s1} ⊂S1, we sometimes write
Ψ(s1|s2) instead of Ψ({s1}|s2). Sometimes a weakly continuous stochastic kernel
is called Feller, and a stochastic kernel continuous in total variation is called
uniformly Feller [19].
Let S1, S2, and S3 be Borel subsets of Polish spaces (a Polish space is a
complete separable metric space), and Ψ on S1 × S2 given S3 be a stochastic
kernel. For each A ∈B(S1), B ∈B(S2), and s3 ∈S3, let:
Ψ(A, B|s3) := Ψ(A × B|s3).
(3)
In particular, we consider marginal stochastic kernels Ψ(S1, · | · ) on S2 given S3
and Ψ( · , S2| · ) on S1 given S3.
In this paper we consider a discrete-time Markov decision process, which is
speciﬁed by a tuple (X, A, P, c), where
(i) the state space X equals to XW × XY , where XW and XY are Borel subsets
of Polish spaces;
(ii) A is the action space, which is assumed to be a Borel subset of a Polish
space;
(iii) P is a stochastic kernel on XW ×XY given XW ×XY ×A, which determines
the distribution of the new state P( · |w, y, a) on XW × XY , if (w, y) ∈
XW × XY is the current state and a ∈A is the current action, and it is
assumed that the stochastic kernel P on X given XW × XY × A is weakly
continuous in (w, y, a) ∈XW × XY × A;
(iv) x0 = (w0, y0) is the initial state;
(v) c : XW × XY × A →R+ = [0, +∞] is a one-step cost function.
The Markov decision process evolves as follows. At time t = 0, the initial
state x0 = (w0, y0) is given. At each time epoch t = 0, 1, . . ., if the state of the
system is (wt, yt) ∈XW × XY and the decision-maker chooses an action at ∈A,
then the cost c(wt, yt, at) is incurred and the system moves to state (wt+1, yt+1)
according to the transition law P( · |wt, yt, at).
Deﬁne the histories: h0 := (w0, y0) ∈H0 and ht := (w0, y0, a0, w1, y1,
a1, . . . , wt−1, yt−1, at−1, wt, yt) ∈Ht for all t = 1, 2, . . . , where H0 := X and
Ht := Ht−1 × A × X if t = 1, 2, . . . . Then a policy is deﬁned as a sequence
π = {πt} such that, for each t = 0, 1, . . . , πt is a transition kernel on A given
Ht. Moreover, π is called nonrandomized if each probability measure πt( · |ht) is
concentrated at one point. A nonrandomized policy is called Markov if all of the
decisions depend only on the current state and time. A Markov policy is called
stationary if all the decisions depend only on the current state. The set of all
policies is denoted by Π. The Ionescu Tulcea theorem ([2, pp. 140–141] or [18, p.

4
E. A. Feinberg et al.
178]) implies that a policy π ∈Π, and initial state x0 = (w0, y0) together with
the transition kernel P determine a unique probability measure P π
x0 on the set
of all trajectories H∞= (XW × XY × A)∞endowed with the product of σ-ﬁeld
deﬁned by Borel σ-ﬁelds of XW , XY , and A respectively. The expectation with
respect to this probability measure is denoted by Eπ
x0 = Eπ
w0,y0.
Let us specify the performance criterion. For a ﬁnite horizon T = 0, 1, . . .,
and for a policy π ∈Π, let the expected total discounted costs be
vπ
T,α(x0) := Eπ
x0
T −1

t = 0
αtc(wt, yt, at),
x0 ∈X,
(4)
where α ≥0 is the discount factor, vπ
0,α(x0) = 0. When T = ∞, (4) deﬁnes an
inﬁnite horizon expected total discounted cost, and we denote it by vπ
α(x0). The
average cost per unit time is deﬁned as
wπ(x0) := lim sup
T →∞
1
T vπ
T,1(x0),
x0 ∈X.
(5)
For any function gπ(x0), including gπ(x0) = vπ
T,α(x0), gπ(x0) = vπ
α(x0), and
gπ(x0) = wπ(x0) deﬁne the optimal cost g(x0) := inf
π∈Π gπ(x0), x0 ∈X. A policy π
is called optimal for the respective criterion, if gπ(x0) = g(x0) for all x0 ∈X. For
gπ = vπ
t,α, the optimal policy is called t-horizon discount-optimal; for gπ = vπ
α,
it is called discount-optimal; and for gπ = wπ, it is called average-cost optimal.
It is well known (see, e.g., [2, Proposition 8.2]) that the functions vt,α(x)
recursively satisfy the following optimality equations with v0,α(x) = 0 for all
x ∈X,
vt+1,α(x) = inf
a

c(x, a) + α

X
vt,α(z)q(dz|x, a)

,
x ∈X, t = 0, 1, ... .
(6)
In addition, a Markov policy φ, deﬁned at the ﬁrst T steps by the mappings
φ0, ...φT −1, that satisfy for all t = 1, ..., T the equations
vt,α(x) = c(x, φT −t(x)) + α

X
vt−1,α(z)q(dz|x, φT −t(x)),
x ∈X,
(7)
is optimal for the horizon T; see, e.g., [2, Lemma 8.7].
It is also well known ([2, Propositions 9.8 and 9.12] or [1,5]) that vα, where
α ∈(0, 1], satisﬁes the following discounted cost optimality equation (DCOE):
vα(x) = inf
a

c(x, a) + α

X
vα(z)q(dz|x, a)

,
x ∈X,
(8)
and a stationary policy φα is discount-optimal if and only if
vα(x) = c(x, φα(x)) + α

X
vα(z)q(dz|x, φα(x)),
x ∈X.
(9)

Average Cost MDPs with Semi-Uniform Feller Kernels
5
3
Properties of Semi-Uniform Feller Stochastic Kernels
Let us consider some basic deﬁnitions.
Deﬁnition 1. Let S be a metric space. A function f : S →R is called
(i) lower semi-continuous (l.s.c.) at a point s ∈S if lim inf
s′→s f(s′) ≥f(s);
(ii) upper semi-continuous at s ∈S if −f is lower semi-continuous at s;
(iii) continuous at s ∈S if f is both lower and upper semi-continuous at s;
(iv) lower/upper semi-continuous (continuous respectively) (on S) if f is
lower/upper semi-continuous (continuous respectively) at each s ∈S.
For a metric space S, let F(S), L(S), and C(S) be the spaces of all real-valued
functions, all real-valued lower semi-continuous functions, and all real-valued
continuous functions respectively deﬁned on the metric space S. The following
deﬁnitions are taken from [7].
Deﬁnition 2. A set F ⊂F(S) of real-valued functions on a metric space S is
called
(i) lower semi-equicontinuous at a point s ∈S if lim inf
s′→s inf
f∈F(f(s′) −f(s)) ≥0;
(ii) upper semi-equicontinuous at a point s ∈S if the set {−f : f ∈F} is lower
semi-equicontinuous at s ∈S;
(iii) equicontinuous at a point s ∈S, if F is both lower and upper semi-
equicontinuous at s ∈S, that is, lim
s′→s sup
f∈F
|f(s′) −f(s)| = 0;
(iv) lower/upper semi-equicontinuous (equicontinuous respectively) (on S) if it is
lower/upper semi-equicontinuous (equicontinuous respectively) at all s ∈S;
(v) uniformly bounded (on S), if there exists a constant M < +∞such that
|f(s)| ≤M for all s ∈S and for all f ∈F.
Obviously, if a set F ⊂F(S) is lower semi-equicontinuous, then F ⊂L(S).
Moreover, if F is equicontinuous, then F ⊂C(S).
Let S1, S2, and S3 be Borel subsets of Polish spaces, and Ψ on S1 × S2 given
S3 be a stochastic kernel.
Deﬁnition 3. ([12]) A stochastic kernel Ψ on S1 × S2 given S3 is semi-uniform
Feller if, for each sequence {s(n)
3 }n = 1,2,... ⊂S3 that converges to s3 in S3 and
for each bounded continuous function f on S1,
lim
n→∞
sup
B∈B(S2)


S1
f(s1)Ψ(ds1, B|s(n)
3 ) −

S1
f(s1)Ψ(ds1, B|s3)
 = 0.
(10)
We recall that the marginal measure Ψ(ds1, B|s3), s3 ∈S3, is deﬁned in (3).
The term “semi-uniform” is used in Deﬁnition 3 because the uniform property
holds in (10) only with respect to the ﬁrst coordinate. If the uniform property
holds with respect to both coordinates, then the stochastic kernel Ψ on S1 × S2
given S3 is continuous in total variation. Stochastic kernels continuous in total

6
E. A. Feinberg et al.
variation are sometimes called uniformly Feller [19]. According to Corollary 1, a
semi-uniform Feller stochastic kernel is weakly continuous.
By [2, Proposition 7.27], there exists a stochastic kernel Φ on S1 given S2×S3
such that
Ψ(A × B|s3) =

B
Φ(A|s2, s3)Ψ(S1, ds2|s3),
(11)
A ∈B(S1), B ∈B(S2), s3 ∈S3. The stochastic kernel Φ( · |s2, s3) on S1 given
S2 ×S3 deﬁnes a measurable mapping Φ : S2 ×S3 →P(S1), where Φ(s2, s3)( · ) =
Φ( · |s2, s3). According to [2, Corollary 7.27.1], for each s3 ∈S3 the mapping
Φ( · , s3) : S2 →P(S1) is deﬁned Ψ(S1, · |s3)-almost surely uniquely in s2 ∈S2.
Consider the stochastic kernel
φ(D × B|s3) :=

B
I{Φ(s2, s3) ∈D}Ψ(S1, ds2|s3),
(12)
D ∈B(P(S1)), B ∈B(S2), s3 ∈S3. In models for decision making with incom-
plete information, φ is the transition probability between belief states, which
are posterior distributions of states. Continuity properties of φ play the funda-
mental role in the studies of models with incomplete information. Theorem 1
characterizes such properties, and this is the reason for the title of this section.
According to [2, Corollary 7.27.1], the particular choice of a stochastic kernel
Φ satisfying (11) does not eﬀect the deﬁnition of φ in (12) because for each
s3 ∈S3 the mapping Φ( · , s3) : S2 →P(S1) is deﬁned Ψ(S1, · |s3)-almost surely
uniquely in s2 ∈S2.
Consider the following assumption.
Assumption 1 There exists a stochastic kernel Φ on S1 given S2×S3 satisfying
(11) such that, if a sequence {s(n)
3 }n = 1,2,... ⊂S3 converges to s3 ∈S3 as n →∞,
then there exists a subsequence {s(nk)
3
}k = 1,2,... ⊂{s(n)
3 }n = 1,2,... and a measurable
subset B of S2 such that Ψ(S1 × B|s3) = 1 and
Φ(s2, s(nk)
3
) converges weakly to Φ(s2, s3),
for all s2 ∈B.
(13)
In other words, the convergence in (13) holds Ψ(S1, ds2|s3)-almost surely.
The following theorem provides necessary and suﬃcient conditions for semi-
uniform Fellerness of a stochastic kernel φ in terms of the properties of a given
stochastic kernel Ψ. This theorem describes the necessary and suﬃcient condi-
tions for the semi-uniform Feller property of the belief-MDPs in terms of the
conditions on the transition kernel in the initial model for decision making with
incomplete information.
Theorem 1. ([12, Theorem 5.14]) For a given stochastic kernel Ψ on S1 × S2
given S3, let the marginal kernel Ψ(S1, · | · ) on S2 given S3 is continuous in total
variation. Then the following conditions are equivalent:
(a) the stochastic kernel Ψ on S1 × S2 given S3 is semi-uniform Feller;

Average Cost MDPs with Semi-Uniform Feller Kernels
7
(b) Assumption 1 holds;
(c) if a sequence {s(n)
3 }n = 1,2,... ⊂S3 converges to s3 ∈S3 as n →∞, then
ρP(S1)(Φ(s2, s(n)
3 ), Φ(s2, s3)) →0 in probability Ψ(S1, ds2|s3),
(14)
where ρP(S1) is the Kantorovich-Rubinshtein metric deﬁned in (2);
(d) the stochastic kernel φ on P(S1) × S2 given S3 is semi-uniform Feller;
and each of these statements implies that the stochastic kernels Ψ on S1 × S2
given S3 and φ on P(S1) × S2 given S3 are weakly continuous.
Corollary 1. ([12, Corollary 5.15]) A semi-uniform Feller stochastic kernel Ψ
on S1 × S2 given S3 is weakly continuous.
For other properties of semi-uniform Feller stochastic kernels we refer to [12,
Section 5].
4
Expected Discounted Costs
For a metric space U, we denote by K(U) the family of all nonempty compact
subsets of U.
For an R-valued function f, deﬁned on a nonempty subset U of a metric
space U, consider the level sets
Df(λ; U) = {y ∈U : f(y) ≤λ},
λ ∈R.
(15)
We recall that a function f is inf-compact on U if all the level sets Df(λ; U) are
compact.
Let S1, S2, and S3 be Borel subsets of Polish spaces. Let LW(S1; S2) be the
class of all nonnegative Borel-measurable functions ϕ : S1 × S2 →R such that
s1 →ϕ(s1, s2) is lower semi-continuous on S1 for each s2 ∈S2.
Deﬁnition 4. ([12]) A function u : S1 ×S2 ×S3 →R is called measurable K-inf-
compact if it is Borel-measurable and for each s2 ∈S2 the function (s1, s3) →
u(s1, s2; s3) is K-inf-compact on S1 × S3, that is, for each s2 ∈S2 the function
(s1, s3) →u(s1, s2; s3) is inf-compact on K × S3 for each K ∈K(S1).
Consider a discrete-time MDP (X, A, q, c) with the state space X = XW ×XY ,
an action space A, one-step costs c, and transition probabilities q. Assume that
XW , XY , and A are Borel subsets of Polish spaces. For any α ≥0 and u ∈
LW(XW ; XY ), we consider:
ηα
u(x, a) = c(x, a) + α

X
u(˜x)q(d˜x|x, a),
(x, a) ∈X × A.
(16)
The following assumption is used in this paper to prove the existence of
optimal policies.

8
E. A. Feinberg et al.
Assumption 2 Let the following two conditions hold:
(i) the function c : X × A →R is nonnegative and measurable K-inf-compact
with S1 := XW , S2 := XY , S3 := A, and u = c;
(ii) the stochastic kernel q on XW × XY given XW × XY × A is semi-uniform
Feller.
The following theorem, which is stronger theorem than Theorem 6.2 in [12],
is the main result of this section.
Theorem 2. Let Assumption 2 hold. Then
(i) the functions vα(w, y) and vt,α(w, y), t = 0, 1, . . ., belongs to LW(XW ×
[0, 1]; XY ), and vt,α(x) ↑vα(x) as t →∞for all (x, α) ∈X × [0, 1];
(ii) for each x ∈X the functions α →vα(x) and α →vt,α(x), t = 0, 1, . . ., where
α ∈[0, 1], are nondecreasing, and they are continuous on the interiors of
their domains;
(iii) if t = 0, 1, . . ., α ∈[0, 1], and x ∈X, then vt+1,α(x) = min
a∈A ηα
vt,α(x, a),
and the nonempty sets At,α(x) := {a ∈A : vt+1,α(x) = ηα
vt,α(x, a)} satisfy
the properties: (a) Gr(At,α) ∈B(X × [0, 1] × A), and (b) At,α(x) = A, if
vt+1,α(x) = +∞, and At,α(x) is compact if vt+1,α(x) < +∞;
(iv) for T
=
1, 2, . . . and α
∈
[0, 1], if for a T-horizon Markov policy
(φ0, . . . , φT −1) the inclusions φT −1−t(x) ∈At,α(x) hold for all x ∈X and
for all t = 0, . . . , T −1, then this policy is T-horizon optimal for the discount
factor α, and, in addition, there exist Markov optimal T-horizon policies
(φα
0 , . . . , φα
T −1) for the discount factor α such φα
t (x) : X × [0, 1] →A is
Borel measurable for each t = 0, . . . , T −1;
(v) if α ∈[0, 1] and x ∈X, then vα(x) = min
a∈A ηα
vα(x, a), and the nonempty
sets Aα(x) := {a ∈A : vα(x) = ηα
vα(x, a)} satisfy the properties: (a)
Gr(Aα) ∈B(X×[0, 1]×A), and (b) Aα(x) = A, if vα(x) = +∞, and Aα(x)
is compact if vα(x) < +∞;
(vi) for a discount factor α ∈[0, 1], a stationary policy φ is optimal for an
inﬁnite-horizon problem with this discount factor if and only if φ(x) ∈
Aα(x) for all x ∈X, and there exists a Borel measurable mapping φα :
X →A, such that for each α ∈[0, 1] the stationary policy φα is optimal for
the inﬁnite-horizon problem with the discount factor α.
Before the proof of Theorem 2, we provide Lemma 1, which is useful for
establishing continuity properties of the value functions vt,α and vα(x). The proof
of this lemma uses Theorem 2.2 from [6]. For each (w, y, α) →wα(w, y) from
LW(XW × R+; XY ), where R+ := [0, +∞), we consider the function (w, y, α) →
w∗
α(w, y) := inf
a∈A ηα
wα(w, y, a) on XW × XY × R+. We observe that, if for some
x ∈X the function α →wα(x) is nondecreasing, then the interior of its domain
is the open interval (0, α(x)).
Lemma 1. Let Assumption 2 hold, and let (w, y, α) →wα(x) be a function
from LW(XW × R+; XY ) such that for each x ∈X the function α →wα(x) is
nondecreasing, and it is continuous on the interior of its domain. Then:

Average Cost MDPs with Semi-Uniform Feller Kernels
9
(i) the function (x, a, α) →ηα
wα(x, a) is Borel measurable on X × A × R+, and
for each y ∈XY the function (w, α; a) →ηα
wα(w, y, a) is K-inf-compact on
(XW × R+) × A;
(ii) for each (x, a) ∈X × A the function α →ηα
wα(x, a) is nondecreasing and
continuous in α on the interior of its domain;
(iii) the function (w, y, α) →w∗
α(w, y) belongs to LW(XW × R+; XY );
(iv) for each x ∈X the function α →w∗
α(x) is nondecreasing and continuous
on the interior of its domain;
(v) there exists a Borel mapping (x, α) →fα(x) of X × R+ into A such that
fα(x) ∈A and w∗
α(x) = ηα
wα(x, fα(x)) for all x ∈X and α ≥0;
(vi) the nonempty sets A∗
α(x) =

a ∈A : w∗
α(x) = ηα
wα(x, a)

, (x, α) ∈X × R+,
satisfy the following properties: (a) Gr(A∗
α) ∈B(X × R+ × A); (b) A∗
α(x) =
A, if w∗
α(x) = +∞, and A∗
α(x) is compact if w∗
α(x) < +∞.
Proof. (i). The function (x, a, α) →ηα
wα(x, a) is nonnegative and nondecreasing
in α because (x, a) →c(x, a) and (x, α) →wα(x) are nonnegative and nonde-
creasing in α. Borel-measurability and continuity properties of (x, α) →wα(x)
and regularity of the transition kernel q imply that the function (x, a, α) →
	
X wα(z)q(dz|x, a) is Borel measurable on X × A × R+, which implies that the
function (x, a, α) →ηα
wα(x, a) is Borel measurable on X × A × R+.
Fix an arbitrary y ∈XY and prove that the function (w, α; a) →ηα
wα(w, y, a)
is K-inf-compact on (XW ×R+)×A. According to Assumption 2(i), the function
(w, a) →c(x, y, a) is K-inf-compact on XW × A. If

X
wα(˜x)q(d˜x|w, y, a) ≤lim inf
n→∞

X
wα(n)(˜x)q(d˜x|w(n), y, a(n)),
(17)
for all (w, a, α) ∈XW × A × R+ and {w(n), a(n), α(n)}n = 1,2,... converging to
(w, a, α), then the function (w, α; a) →ηα
wα(w, y, a) is K-inf-compact on (XW ×
R+) × A since it is a sum of a K-inf-compact function and a nonnegative lower
semi-continuous function. Let us prove that (17) holds. On the contrary, there
exist a sequence {(w(n), a(n), α(n))}n = 1,2,... ⊂XW × A × R+ that converges to
some (w, a, α) ∈XW × A × R+ and a constant λ such that

X
wα(n)(˜x)q(d˜x|w(n), y, a(n)) ≤λ <

X
wα(˜x)q(d˜x|w, y, a),
(18)
for each n = 1, 2, . . . . Since the function α →wα(x) is nondecreasing, without
loss of generality, assume that α(n) ↑α as n →∞. According to Theorem 1(a,
b) applied to Ψ := q, S1 := XW , S2 := XY , S3 := XW × {y} × A, there exists
a stochastic kernel Φ on XW given XY × XW × {y} × A such that (11) and
Assumption 1 hold. In particular, (18) implies that

XY


XW
wα(n)( ˜w, ˜y)Φ(d ˜w|˜y, w(n), y, a(n))

q(XW , d˜y|w(n), y, a(n)) ≤λ,
(19)
for each n = 1, 2, . . ., and there exist a subsequence {(w(nk), a(nk))}k=1,2,... ⊂
{(w(n), a(n))}n = 1,2,... and a Borel set Y ∈B(XY ) such that q(XW ×Y |w, y, a) = 1

10
E. A. Feinberg et al.
and Φ(˜y, w(n), y, a(n)) converges weakly to Φ(˜y, w, y, a) in P(XW ) as k →∞, for
all ˜y ∈Y. Therefore, since the function ˜w →wα(p)( ˜w, ˜y) is nonnegative and
lower semi-continuous for each ˜y ∈Y and p = 1, 2, . . ., Fatou’s lemma for weakly
converging probabilities [14, Theorem 1.1] implies that

XW
wα(m)( ˜w, ˜y)Φ(d ˜w|˜y, w, y, a) ≤
lim inf
k→∞

XW
wα(m)( ˜w, ˜y)Φ(d ˜w|˜y, w(nk), y, a(nk)) ≤
lim inf
k→∞

XW
wα(nk)( ˜w, ˜y)Φ(d ˜w|˜y, w(nk), y, a(nk)),
for each m = 1, 2, . . . and ˜y ∈Y , where the second inequality holds, since
α(nk) ↑α as k →∞, and the function α →wα(x) is nondecreasing. Therefore,
the monotone convergence theorem implies

XW
wα( ˜w, ˜y)Φ(d ˜w|˜y, w, y, a) ≤lim inf
k→∞

XW
wα(nk)( ˜w, ˜y)Φ(d ˜w|˜y, w(nk), y, a(nk)),
for each ˜y ∈Y. For a ﬁxed N = 1, 2, . . . we set
ϕN
k (˜y) := min{

XW
wα(nk)( ˜w, ˜y)Φ(d ˜w|˜y, w(nk), y, a(nk)), N},
ϕN(˜y) := min{

XW
wα( ˜w, ˜y)Φ(d ˜w|˜y, w, y, a), N},
˜y ∈Y , k = 1, 2, . . . . Note that ϕN(˜y) ≤lim inf k→∞ϕN
k (˜y), for each ˜y ∈Y.
Therefore, uniform Fatou’s lemma [13, Corollary 2.3] implies that

XY
ϕN(˜y)q(XW , d˜y|w, y, a) ≤lim inf
k→∞

XY
ϕN
k (˜y)q(XW , d˜y|w(nk), y, a(nk)) ≤λ,
for each N = 1, 2, . . ., where the second inequality follows from (19) since
ϕN
k (˜y) ≤
	
XW wα(nk)( ˜w, ˜y)Φ(d ˜w|˜y, w(nk), y, a(nk)) for each ˜y ∈Y , and k =
1, 2, . . . . Thus, the monotone convergence theorem implies that

X
wα(˜x)q(d˜x|w, y, a) = lim
N→∞

XY
ϕN(˜y)q(XW , d˜y|w, y, a) ≤λ.
This is a contradiction to (18). Therefore, the function (w, α; a) →ηα
wα(w, y, a)
is K-inf-compact on (XW × R+) × A.
(iii, v, vi). Statement (i) and Berge’s theorem for noncompact action sets [9,
Theorem 1.2] imply that the function (w, α) →w∗
α(w, y) is lower semi-continuous
for each y ∈XY . Moreover, [6, Theorem 2.2, and Corollary 2.3 (i)] directly imply
that the function (w, y, α) →w∗
α(w, y) is Borel measurable and statements (v)
hold. Property (vi)(a) follows from Borel measurability of (x, a, α) →ηα
wα(x, a)
on X×A×R+ and (x, α) →w∗
α(x) on X×R+; and property (vi)(b) follows from
inf-compactness of a →ηα
wα(x, a) on A for each (x, α) ∈X × R+.

Average Cost MDPs with Semi-Uniform Feller Kernels
11
(ii). The function α →α
	
X wα(z)q(dz|x, a) is continuous on the interior of
its domain for each (x, a) ∈X × A. This follows from Assumption 2 (ii) and [7,
Theorem 6.1] because, according to Corollary 1, the stochastic kernel q is weakly
continuous. So, the function α →ηα
wα(x, a) is continuous in α on the interior of
its domain.
(iv). Fix an arbitrary x ∈X. Statement (ii) implies that the function α →
w∗
α(x) is nondecreasing. The continuity statement is nontrivial only if the interior
of the domain of this function is not empty. Let (0, α(x)) be the interior domain
of α →w∗
α(x). We shall prove that the function w∗
α(x) is continuous on (0, α(x)).
Let us ﬁx an arbitrary α′ ∈(0, α(x)). We choose an arbitrary β ∈(α′, α(x)).
Then w∗
β(x) < +∞, and therefore ηβ
wβ(x, aβ) < +∞for some aβ ∈A. Then
ηα
wα(x, aβ) ≤ηβ
wβ(x, aβ) < +∞for all α ∈(0, β]. For each a ∈A the function
g(α, a) = min{ηα
wα(x, a), ηα
wα(x, aβ)} is continuous in α ∈(0, β] as a minimum
of two continuous functions, and w∗
α(x) = infa∈A g(α, a). Since the inﬁmum
of upper semi-continuous functions is an upper semi-continuous function, the
function α →w∗
α(x) is upper semi-continuous on (0, β], and therefore it is upper
semi-continuous on (0, α(x)). According to statement (iii), the function α →
w∗
α(x) is lower semi-continuous on R+. So, statement (iv) holds.
□
Proof of Theorem 2. According to (6), the functions vt,α(x), t = 0, 1, . . ., recur-
sively satisfy the optimality equations vt+1,α(x) = inf
a∈A ηα
vt,α(x, a) with v0,α(x) =
0, for all (x, α) ∈X × [0, 1]). So, Lemma 1 (i, ii) sequentially applied to the
functions v0,α(x), v1,α(x), . . ., imply statements (i,ii) of the theorem. In par-
ticular, statement (ii) of the theorem implies that these functions are lower
semi-continuous in α on the interiors of their domains. According to [2, Propo-
sition 9.17], vt,α(x) ↑vα(x) as t →+∞for each (x, α) ∈X × [0, 1]. There-
fore, vα(x) ∈LW(XW × [0, 1]; XY ), and vα(x) is nondecreasing and lower semi-
continuous in α on the interior of its domain. Thus, statement (i) is proved.
In addition, (7) imply that a Markov policy deﬁned at the ﬁrst T steps by the
mappings φα
0 , ...φα
T −1, that satisfy for all t = 1, . . . , T the equations vt,α(x) =
ηα
vt−1,α(x, φα
T −t(x)), for each (x, α) ∈X × [0, 1], is optimal for the horizon T.
According to (8) and (9), vα(x) satisﬁes the discounted cost optimality equation
vα(x) = inf
a∈A ηα
vα(x, a) for each (x, α) ∈X×[0, 1]; and a stationary policy φα(x) is
discount-optimal if and only if vα(x) = ηα
vα(x, φα(x)) for each x ∈X. Statements
(iii–vi) follow from these facts and Lemma 1 (v, vi).
To complete the proof of statement (ii), we need to show that for each ﬁxed
x ∈X the function α →vα(x) is upper semi-continuous in the interior of its
domain. Since vα(x) is nondecreasing and lower semi-continuous in α on the
interior of its domain, this means that we need to show that vα(x) is right-
continuous in α ∈(0, 1) if vα(x) < +∞. Indeed, if vα(x) < +∞, let us consider
a stationary optimal stationary policy φα whose existence is claimed in statement
(vi). Then the function vφα
α+Δ(x) is continuous in Δ as a value of a converging
power series. Therefore,
0 ≤vα+Δ(x) −vα(x) = vα+Δ(x) −vφα
α (x) ≤vφα
α+Δ(x) −vφα
α (x) ↓0
as Δ ↓0.
□

12
E. A. Feinberg et al.
5
Average Costs per Unit Time
Following [21], we assume that w∗:= inf
x∈X w(x) < +∞, that is, there exist x ∈X
and π ∈Π with wπ(x) < +∞. Otherwise, if this assumption does not hold, then
the problem is trivial, because w(x) = +∞for all x ∈X and any policy π is
average-cost optimal.
Deﬁne the following quantities for α ∈[0, 1):
mα = inf
x∈X vα(x),
uα(x) = vα(x) −mα,
w = lim inf
α↑1
(1 −α)mα,
w = lim sup
α↑1
(1 −α)mα.
According to [21, Lemma 1.2],
0 ≤w ≤w ≤w∗< +∞.
(20)
In this section we show that Assumption 2 and boundedness assumption
Assumption B on the function uα introduced in [8], which is weaker than
boundedness Assumption B introduced in [21], lead to the validity of station-
ary average-cost optimal inequalities and the existence of stationary policies.
Stronger results hold under Assumption B.
Assumption B. lim inf
α↑1
uα(x) < +∞for all x ∈X.
The above is weaker than the following assumption.
Assumption B. supα∈[0,1) uα(x) < +∞for all x ∈X.
In the rest of this paper we assume that Assumption B holds. In view of
Theorem 2 (i), if vα(x) = +∞for some (x, α) ∈X×[0, 1), then uβ(x) = vβ(x) =
+∞for all β ∈[α, 1), and u(x) = +∞, where mβ is ﬁnite in view of (20). Thus
Assumption B implies that vα(x) < +∞, and therefore uα(x) < +∞for all
(x, α) ∈X × [0, 1). Under Assumption 2, in view of (20) and Theorem 2(i,ii),
mα : [0, 1) →R+ is a nondecreasing upper semi-continuous function as an
inﬁmum of the family of the continuous functions, and therefore uα(w, y) =
vα(w, y) −mα ∈LW(XW × [0, 1); XY ).
Let us deﬁne the following nonnegative functions on XW × XY :
Uβ(w, y) :=
inf
α∈[β,1) uα(w, y),
U
∼β(w, y) := lim inf
w′→w Uβ(w′, y),
u(w, y) :=
sup
β∈[0,1) U
∼β(w, y),
(21)
β ∈[0, 1), x ∈X. To establish the Borel measurable properties for these functions
we need to make the following assumption.

Average Cost MDPs with Semi-Uniform Feller Kernels
13
Assumption 3 The space XW is σ-compact.
Lemma 2. Let β ∈[0, 1). Under Assumptions 2 and 3, the functions Uβ, U
∼β, u :
X →R+ deﬁned in (21) are Borel measurable on X. Moreover, the functions
U
∼β(w, y) and u(w, y) are lower semi-continuous in w for each y ∈XY .
Proof. Fix β ∈[0, 1). Borel measurability of (w, y) →Uβ(w, y) follows from (21)
and [6, Theorem 2.1] applied to the Borel spaces X and [0, 1), set-valued map
B(x) = [β, 1) for all x ∈X, and the function u(x, α) := uα(x) ∈LW([0, 1); X).
Let us prove the Borel measurability of (w, y) →U
∼β(w, y). Indeed, consider the
function
u(w′, α, w, y, δ) := uα(w′, y)χ{w′ ∈¯Bδ(w)},
w′, w ∈XW , y ∈XY , α ∈[β, 1), δ > 0, where χ{“True′′} := 0, and χ{“False′′} :=
+∞. Since the nonnegative functions (w′, α, y) →uα(w′, y) and (w′, w, δ) →
χ{w′ ∈¯Bδ(w)} belong to LW(XW × [0, 1); XY ) and LW(XW ; XW × (0, +∞))
respectively, then the function u belongs to LW(XW ×[0, 1); X×(0, +∞)). There-
fore, according to Feinberg and Kasyanov [6, Theorem 2.1] applied to the Borel
space X := X × (0, +∞), σ-compact space A := XW × [0, 1), set-valued map
B(w, δ) = XW × [β, 1) for all w ∈XW , and the function u ∈LW(A; X), we have
that the function
(w, y) →U(w, y) :=
inf
w′∈¯
Bδ(w)
inf
α∈[β,1) uα(w′, y)
is Borel measurable. Therefore, the function (w, y) →U
∼β(w, y) is Borel measur-
able because
U
∼β(w, y) =
sup
n = 1,2,...
inf
w′∈¯
B 1
n (w)
inf
α∈[β,1) uα(w′, y),
w ∈XW and y ∈XY , and a supremum of countable family of Borel measurable
functions is Borel measurable. Note that lower semi-continuity of U
∼β(w, y) in w
directly follows from its deﬁnition (21). Therefore, according to (21), the function
(w, y) →u(w, y) is Borel measurable and it is lower semi-continuous in w for
each y ∈XY as a supremum of countable family of Borel measurable functions
{U
∼1−1
n (w, y)}n = 1,2... which are lower semi-continuous in w.
□
In view of the deﬁnition of u in Assumption B,
u(w, y) = lim
β↑1 U
∼β(w, y),
w ∈XW , y ∈XY .
(22)
Under Assumptions 2 and 3 the following sets can be deﬁned for u introduced
in (21):
Au(x) :=

a ∈A : w + u(x) ≥η1
u(x, a)

,
Au(x) :=

a ∈A : min
a∗∈A η1
u(x, a∗) = η1
u(x, a)

,
x ∈X.

14
E. A. Feinberg et al.
In view of Lemma 1, the sets Au(x) are nonempty and compact for all x ∈X.
In the following theorem we show that Assumption 2 and boundedness assump-
tion Assumptions B on the functions {uα}α∈[0,1) lead to the validity of stationary
average-cost optimal inequalities and the existence of stationary policies. [8, The-
orems 3 and 4] are respectively counterparts to Theorem 3.3 and the main result
in [16] for MDPs with weakly continuous transition probabilities. Assumption B
and some additional conditions lead to the validity of optimality equations for
average-costs MDPs. In [15] such suﬃcient conditions are provided for MDPs
with weakly continuous transition probabilities and applied to inventory con-
trol. More general suﬃcient conditions for validity of optimality equations are
provided in [7, Section 7] for MDPs with weakly and setwise continuous transi-
tion probabilities.
Theorem 3. Let Assumptions 2, 3, and B hold. Then for inﬁnite-horizon aver-
age costs per unit time there exists a stationary optimal policy φ satisfying
w + u(x) ≥η1
u(x, φ(x)),
x ∈X,
(23)
with u deﬁned in (21), and for this policy
w(x) = wφ(x) = lim sup
α↑1
(1 −α)vα(x) = w = w∗,
x ∈X.
(24)
Moreover, the following statements hold:
(a) the function u : X →R+ deﬁned in (21) is Borel measurable;
(b) the nonempty sets Au(x), x ∈X, satisfy the following properties: (b1)
Gr(Au) ∈B(X × A); (b2) for each x ∈X the set Au(x) is compact;
(c) if ϕ(x) ∈Au(x) for all x ∈X for a stationary policy ϕ, then ϕ satisﬁes
(23) and (24), with u deﬁned in (21) and with φ = ϕ, and ϕ is optimal for
average costs per unit time;
(d) the sets Au(x) are compact and Au(x) ⊂Au(x) for all x ∈X, and there
exists a stationary policy ϕ with ϕ(x) ∈Au(x) ⊂Au(x) for all x ∈X.
The proof of Theorem 3 uses the following statement.
Lemma 3. Under Assumptions 2, 3, and B,
w + u(x) ≥min
a∈A η1
u(x, a),
x ∈X.
(25)
Proof. Fix an arbitrary ε∗> 0. Due to the deﬁnition of w, there exists α0 ∈(0, 1)
such that
w + ε∗> (1 −α)mα,
α ∈[α0, 1).
(26)
According to Lemma 2, the R+-valued function (w, y) →U
∼α(w, y) is Borel
measurable for all α ∈(0, 1). Therefore, the function ηα
U
∼α(x, a) is well-deﬁned.
Let us prove that
w + ε∗+ u(x) ≥min
a∈A ηα
U
∼α(x, a),
x ∈X, α ∈[α0, 1).
(27)

Average Cost MDPs with Semi-Uniform Feller Kernels
15
Indeed, Theorem 2 (v) and (26) imply that
w + ε∗+ uβ(w, y) > (1 −β)mβ + uβ(w, y) = vβ(w, y) −βmβ
= min
a∈A ηβ
uβ(w, y, a) ≥min
a∈A ηα
U
∼α(w, y, a),
for each w ∈XW , y ∈XY , and α, β ∈[α0, 1) such that β ≥α. Since the right-
hand side of the above inequality does not depend on β ∈[α, 1), by taking the
inﬁmum in β ∈[α, 1), we obtain that
w + ε∗+ Uα(w, y) ≥min
a∈A ηα
Uα(w, y, a) ≥min
a∈A ηα
U
∼α(w, y, a),
(28)
for all w ∈XW , y ∈XY , and α ∈[α0, 1). Since the function c is measurable
K-inf-compact and, due to Lemma 2, U
∼α ∈LW(XW ; XY ), and the function
w →min
a∈A ηα
U
∼α(w, y, a) is nonnegative lower semi-continuous function for each
y ∈XY . Therefore, (28) implies that
w + ε∗+ U
∼α(w, y) ≥min
a∈A ηα
U
∼α(w, y, a),
(29)
for all w ∈XW , y ∈XY , and α ∈[α0, 1). Thus, since the function U
∼α(w, y) is
nonincreasing in α ∈[0, 1), inequalities (27) hold in view of (22).
Let us ﬁx an arbitrary x ∈X. By Lemma 1 (v, vi), for every α ∈[0, 1)
there exists aα ∈A such that min
a∈A ηα
U
∼α(x, a) = ηα
U
∼α(x, aα). Since U
∼α ≥0, for
α ∈[α0, 1), inequality (27) can be continued as
w + ε∗+ u(x) ≥ηα
U
∼α(x, aα) ≥c(x, aα).
(30)
Thus, for all α ∈[α0, 1)
aα ∈Dηα
U
∼α(x, · )(w + ε∗+ u(x)) ⊂Dc(x, · )(w + ε∗+ u(x)) ⊂A.
Since the function c(x, · ) is inf-compact, the nonempty set Dc(x, · )(w+ε∗+u(x))
is compact. Therefore, for every sequence β(n) ↑1 of numbers from [α0, 1) there
is a subsequence {α(n)}n≥1 such that the sequence {aα(n)}n≥1 converges and
a∗:= limn→∞aα(n) ∈A. Consider a sequence α(n) ↑1 such that aα(n) →a∗
for some a∗∈A. Due to (22) and Lemma 2, similarly to the proof of (17), we
obtain that
lim inf
n→∞α(n)

X
U
∼α(n)(z)q(dz|x, a(n)) ≥

X
u(z)q(dz|x, a∗).
(31)
Therefore, since the function c is lower semi-continuous in a, (30) imply
w + ε∗+ u(x) ≥lim inf
n→∞ηα(n)
U
∼α(n)(x, aα(n))
≥c(x, a∗) +

X
u(z)q(dz|x, a∗) ≥min
a∈A η1
u(x, a∗),
which implies (25) because ε∗> 0 is arbitrary.
□

16
E. A. Feinberg et al.
Proof of Theorem 3. For statement (a) see (22) and the following sentence. Since
Gr(Au) = {(x, a) ∈Gr(A) : g(x, a) ≥0}, where g(x, a) = w + u(x) −c(x, a) −
	
X u(y)q(dy|x, a) is a Borel function, the set Gr(Au) is Borel. The sets Au(x),
x ∈X, are compact because for each x ∈X the function a →η1
u(x, a) is inf-
compact on A as a sum of inf-compact and nonnegative lower semi-continuous
functions. Thus, statement (b) is proved. The Arsenin-Kunugui theorem implies
the existence of a stationary policy φ such that φ(x) ∈Au(x) for all x ∈X.
Statement (d) follows from and Lemma 1(v) because each a∗∈Au(x) satis-
ﬁes η1
u(x, a∗) = mina∗∈A η1
u(x, a∗) ≤w + u(x), where the inequality holds since
Au(x) ̸= ∅. The remaining conclusions of Theorem 3 follow from Lemma 3 and
[21, Proposition 1.3] stating that inequalities (23) imply optimality of the policy
φ and (24).
□
Under Assumptions 2, 3, and B, consider the sequence α(n) ↑1 such that
(1−α(n))mα(n) →w as n →∞. Let us deﬁne the following nonnegative functions
on XW × XY :
Um(w, y) := inf
n≥m uα(n)(w, y),
U
∼m(w, y) := lim inf
w′→w Um(w′, y),
u(w, y) := sup
m→∞U
∼m(w, y),
(32)
m = 1, 2, . . ., x ∈X.
Theorem 4. Suppose Assumptions 2, 3, and B hold. Then all the conclusions
of Theorem 3 hold and, in addition, for a stationary policy φ satisfying (23) with
u deﬁned in (32),
wφ(x) = w = lim
α↑1(1 −α)vα(x) = lim
N→∞
1
N vφ
N,1(x),
x ∈X.
(33)
Proof repeats the proof of Theorem 3 if we replace [α, 1) with {α(n)}n≥m; cf.
[11, Theorem 4].
□
6
Approximation of Average Cost Optimal Policies by
α-discount Optimal Policies
Under Assumptions 2, 3, and B, consider a nondecreasing sequence α(n) ↑1 such
that (1−α(n))mα(n) →w as n →∞. Consider the nonnegative functions deﬁned
in (32). For a family of sets {Gr(Aα(n))}n = 1,2,..., x ∈X, from Theorem 2, let us
set:
Aapp(x) :=

a ∈Au(x) : (x, a) ∈˜A

,
x ∈X,
where (w, y, a) ∈˜A if and only if there exist a subsequence {γ(n)}n = 1,2,... ⊂
{α(n)}n = 1,2,... and a sequence {w(n), a(n)}n = 1,2,... ⊂Gr(Aα(n)) that converges
to (w, a) as n →∞.

Average Cost MDPs with Semi-Uniform Feller Kernels
17
Theorem 5. Under Assumptions 2, 3, and B, the graph Gr(Aapp) is a Borel
subset of Gr(A∗), and for each x ∈X the set Aapp(x) is nonempty and compact.
Furthermore, there exists a stationary policy φapp such that φapp(x) ∈Aapp(x)
for all x ∈X, and any such policy is average-cost optimal.
Proof is similar to the proof of [8, Theorem 5] with minor changes; cf. the proof
of Theorem 3.
□
Corollary 2. (cf. [8, Corollary 3]) Under Assumptions 2, 3, and B, for any
stationary average-cost optimal policy φapp, such that φapp(x) ∈Aapp(x) for all
x ∈X, for every (w, y) ∈X there exist αn ↑1 and wn →w as n →+∞such
that for some an ∈Aαn(wn, y), n ≥1, the equality φapp(w, y) = limn→+∞an
holds.
References
1. B¨auerle, N., Rieder, U.: Markov Decision Processes with Applications to Finance.
Springer, Berlin (2011)
2. Bertsekas, D.P., Shreve, S.E.: Stochastic Optimal Control: The Discrete-Time
Case. Academic Press, New York (1978)
3. Billingsley, P.: Convergence of Probability Measures. Wiley, New York (1968)
4. Bogachev, V.I.: Measure Theory, vol. II. Springer, Berlin (2007)
5. Dynkin, E.B., Yushkevich, A.A.: Controlled Markov Processes. Springer, New York
(1979)
6. Feinberg, E.A., Kasyanov, P.O.: MDPs with setwise continuous transition proba-
bilities. arXiv:2011.01325 (2020)
7. Feinberg, E.A., Kasyanov, P.O., Liang, Y.: Fatou’s lemma in its classical form
and Lebesgue’s convergence theorems for varying measures with applications to
Markov decision processes. Theory Probab. Appl. 65(2), 270–291 (2020)
8. Feinberg, E.A., Kasyanov, P.O., Zadoianchuk, N.V.: Average-cost Markov decision
processes with weakly continuous transition probabilities. Math. Oper. Res. 37(4),
591–607 (2012)
9. Feinberg, E.A., Kasyanov, P.O., Zadoianchuk, N.V.: Berge’s theorem for noncom-
pact image sets. J. Math. Anal. Appl. 397(1), 255–259 (2013)
10. Feinberg, E.A., Kasyanov, P.O., Zgurovsky, M.Z.: Convergence of probability mea-
sures and Markov decision models with incomplete information. Proc. Steklov Inst.
Math. 287(1), 96–117 (2014)
11. Feinberg, E.A., Kasyanov, P.O., Zgurovsky, M.Z.: Partially observable total-cost
Markov decision processes with weakly continuous transition probabilities. Math.
Oper. Res. 41(2), 656–681 (2016)
12. Feinberg, E.A., Kasyanov, P.O., Zgurovsky, M.Z.: Markov decision processes with
incomplete information and semi-uniform Feller transition probabilities. In prepa-
ration (2021)
13. Feinberg, E.A., Kasyanov, P.O., Zgurovsky, M.Z.: Uniform Fatou’s lemma. J. Math.
Anal. Appl. 444(1), 550–567 (2016)
14. Feinberg, E.A., Kasyanov, P.O., Zadoianchuk, N.V.: Fatou’s lemma for weakly
converging probabilities. Theory Probab. Appl. 58(4), 683–689 (2014)
15. Feinberg, E.A., Liang, Y.: On the optimality equation for average cost Markov
decision processes and its validity for inventory control. Ann. Oper. Res. (2017).
https://doi.org/10.1007/s10479-017-2561-9

18
E. A. Feinberg et al.
16. Hern´andez-Lerma, O.: Adaptive Markov Control Processes. Springer, New York
(1989)
17. Hern´andez-Lerma, O.: Average optimality in dynamic programming on Borel
spaces - Unbounded costs and controls. Syst. Control Lett. 17(3), 237–242 (1991)
18. Hern´andez-Lerma, O., Lassere, J.B.: Discrete-Time Markov Control Processes:
Basic Optimality Criteria. Springer, New York (1996)
19. Papanicolaou, G.C.: Asymptotic analysis of stochastic equations. In: Rosenblatt,
M. (ed.) Studies in Probability Theory, pp. 111–179. Mathematical Association of
America, Washington DC (1978)
20. Parthasarathy, K.R.: Probability Measures on Metric Spaces. Academic Press, New
York (1967)
21. Sch¨al, M.: Average optimality in dynamic programming with general state space.
Math. Oper. Res. 18(1), 163–172 (1993)

First Passage Exponential Optimality
Problem for Semi-Markov Decision
Processes
Haifeng Huo(B) and Xian Wen
Department of School of Science, Guangxi University of Science and Technology,
Liuzhou 5451006, China
xiaohuo08ok@163.com, wenxian879@163.com
Abstract. This paper deals with the exponential utility maximization
problem for semi-Markov decision process with Borel state and action
spaces, and nonnegative reward rates. The criterion to be optimized is the
expected exponential utility of the total rewards before the system state
enters the target set. Under the regular and compactness-continuity con-
ditions, we establish the corresponding optimality equation, and prove
the existence of an exponential utility optimal stationary policy by an
invariant embedding technique. Moreover, we provide an iterative algo-
rithm for calculating the value function as well as the optimal policies.
Finally, we illustrate the computational aspects of an optimal policy with
an example.
Keywords: Semi-Markov decision processes · Exponential utility ·
First passage time · Value iterative approach · Optimality equation ·
Optimal policy
AMS(2020)
subject
classiﬁcation: Primary 90C40 · Secondary
90C39
1
Introduction
Semi-Markov decision processes (SMDPs), as an important class of stochas-
tic control problems, have been widely studied [1,10,11,15,20,28,31]. The
commonly used criteria for SMDPs are the ﬁnite horizon expected criterion
[8,14,26,28], the expected discounted criterion [1,3,10,13,25,27], and the aver-
age criterion [10,23,31–33]. These criteria are linear utility functions of the total
rewards (i.e. are risk-neutral), which only focus on the expected total rewards
of a system during a ﬁxed or a random horizon, and therefore cannot reﬂect the
decision maker’s attitude toward risk.
To exhibit the attitude of a decision maker in the face of risk (i.e. risk-
seeking or risk-averse), the risk sensitive criteria, which include the exponential
utility criterion, have been considered for discrete-time MDPs (DTMDPs) [2,4–
6,21,22], and continuous-time MDPs (CTMDPs) [7,9,30,34]. Speciﬁcally, Jaque-
tte [21] ﬁrst introduced the exponential utility to DTMDPs. For the resulting
c
⃝The Author(s), under exclusive license to Springer Nature Switzerland AG 2021
A. Piunovskiy and Y. Zhang (Eds.): Modern Trends in Controlled Stochastic Processes,
ECC 41, pp. 19–37, 2021. https://doi.org/10.1007/978-3-030-76928-4_2

20
H. Huo and X. Wen
optimization problem, Chung and Sobel [6] established the corresponding opti-
mality equation by means of the Banach ﬁxed point theorem. Cavazos-Cadena
and Montes-De-Oca [4,5] gave conditions ensuring the existence of optimal poli-
cies for the positive dynamic programming, where the state space is considered
to be ﬁnite in [4], and denumerable in [5]. Ja´skiewicz [22] considered the Borel
state and action spaces, and establish the convergence of the n-stage optimal
expected total reward and the existence of an optimal stationary policy. Ba¨uerle
and Rieder [2] considered a more general problem than the classic risk sensi-
tive optimization problem, which is called minimizing a certainty equivalent.
They solved the optimization problem by an ordinary MDP with extended state
space, and proved the existence of an optimal policy under some suitable con-
ditions. For the case of CTMDPs, Ghosh and Saha [7] studied the risk sensitive
control in discrete state space. They obtain the value function as a solution to
the Hamilton Jacobi Bellman equation, and proved the existence of an opti-
mal Markov control for ﬁnite horizon problem, and the existence of an optimal
stationary control for inﬁnite horizon problem. Wei [30] dealt with risk sen-
sitive cost criterion for ﬁnite horizon CTMDPs with denumerable state space
and Borel action space. Under suitable conditions, he proved the existence of
the Feynman-Kac formula and an optimal deterministic Markov policy. For the
same problem as in [30], Guo, Liu and Zhang [9] investigated the case when
the transition and cost rates may be unbounded. They proved that the value
function is the unique solution to the optimality equation, and showed the exis-
tence of an optimal policy via the Feynman-Kac formula. Few literature [34]
applied the uniformization technique to reducing the CTMDPs problem with
exponential utility to an equivalent DTMDPs. Recently, Huang, Lian and Guo
[17] considered the risk sensitive unconstrained and constrained problems for
SMDPs with Borel state space, unbounded cost rates and general utility func-
tions, and proved the existence of the Bellman equation and the optimal policies
under some continuity-compactness conditions by using the occupation measure
approach.
All of this existing literature shows that all the aforementioned MDPs for the
risk-sensitive criterion have two common features: the horizon is ﬁnite or inﬁnite,
the control model is DTMDPs or CTMDPs. However, such as those encoun-
tered in many real world situations, many models in ruin problems [20,29],
reliability [20,24], and maintenance [20] are considered with a random hori-
zon, and described as SMDPs. Moreover, compared to DTMDPs and CTMDPs
(under stationary policies), SMDPs are more general stochastic optimal models,
in which the holding time of the system state can be allowed to follow any arbi-
trary probability distribution. This is the main reason for considering a random
horizon for SMDPs in this paper.
Compared with the existing research work for risk-sensitive SMDPs in [17],
this paper has some new features as follows: First, in order to make the conclusion
more closely ﬁt the actual situation, we pay more attention to the time horizon is
the random ﬁrst passage time, which is more general than those in [17]. Second,
since the random ﬁrst passage time is considered in our control model, by Remark

First Passage Exponential Optimality Problem for SMDPs
21
4.2 in [17], we know that the occupation measure approach is not suitable for our
model, because the deﬁnition of the occupation measure is based on the discount
factor. Instead, we use a so-called minimum nonnegative solution approach to
establish the optimality equation and prove the existence of optimal policies.
Third, we are mainly concerned with the calculation and existence of the optimal
policies, while the purpose of the works in [17] is to establish the existence
condition of the optimal policies. Due to these, we develop a value iteration
algorithm to calculate the value function and the optimal policy, which is new
and the key feature in our paper.
To the best of our knowledge, the risk-sensitive optimality problem for
SMDPs in ﬁrst passage has not been studied yet.
Motivated by the above discussion, we investigate in this paper the ﬁrst pas-
sage risk-sensitive optimality problems for SMDPs. We focus on both the exis-
tence conditions and the computational algorithms of an optimal policy, thus
we limit the choice of risk-sensitive criteria to the exponential utility criterion
(e.g. [2,6,21,34]), which maximizes the expected exponential utility of the total
rewards before the state of system enters the target set. More precisely, in order
to ensure the existence of an optimal stationary policy, we impose the standard
regular condition to ensure that the state process is non-explosive, which is sim-
ilar to those given in [13–15,18] for SMDPs (see Lemma 1). Second, compared
with [13–15,18], which are mainly limited to denumerable state space and ﬁnite
action set, we consider more general Borel state and action spaces. Then, we
need to introduce a new continuity-compactness condition (see Assumption 2).
Under the regular and continuity-compactness conditions, we establish the cor-
responding optimality equation, and prove that the value function is a solution
to this optimality equation. Moreover, we show the existence of an exponen-
tial utility optimal stationary policy by using an invariant embedding technique
(see Assumption 1). Furthermore, a value iteration algorithm for computing the
value function as well as the optimal policies, in a ﬁnite number of iterations, is
provided. Finally, an example illustrating the computational methodology of an
optimal stationary policy and the value function is given.
The rest of this paper is organized as follows. In Sect. 2, we introduce the semi-
Markov decision model and state the ﬁrst passage exponential utility optimality
problem. The main optimality results are stated and proved in Sect. 3. In Sect. 4,
an example is provided to illustrate the computational aspects of an optimal
policy.
2
Model Description
Models of ﬁrst passage exponential utility SMDPs are deﬁned by
{S, A, (A(x), x ∈S), Q(u, y|x, a), B, r(x, a)}
(1)
with the following components:
(a) S denotes a Borel state space, endowed with the Borel σ-algebras B(S).

22
H. Huo and X. Wen
(b) A denotes a Borel action space, endowed with the Borel σ-algebras B(A).
(c) A(x) ∈B(A) represents the set of allowable actions when the system is at
state x ∈S. K := {(x, a)|x ∈S, a ∈A(x)} represents the set of all feasible
pairs of states and actions.
(d) Q(·, ·|x, a) is a semi-Markov kernel on R+ ×S given K, where R+ := [0, ∞).
For any u ∈R+, D ∈B(S), when the action a ∈A(x) is taken in state
x, Q(u, D|x, a) denotes the joint probability that the holding time of the
system is no more than u ∈R+ and the state x changes into the set D. The
semi-Markov kernel Q(·, ·|x, a), (x, a) ∈K has the following features:
(i) For any D ∈B(S), Q(·, D|x, a) is a non-decreasing, right continuous
function from R+ to [0, 1] with Q(0, D|x, a) = 0 .
(ii) For any u ∈R+, Q(u, ·|x, a) is a sub-stochastic kernel on the state space
S.
(e) B is target set, which is a measurable subset of S, and usually represents
the set of failure (or ruin) states of a system.
(f) r(x, a) denotes the reward rate, which is assumed to be nonnegative mea-
surable function on K such that r(x, ·) ≡0 for all x ∈B.
The ﬁrst passage SMDP with exponential utility evolves as follows: When the
system state is x0 ∈Bc at time t0 = 0, the decision maker selects an admissible
action a0 from the action set A(x0), where Bc denotes the complement of B.
Consequently, the system stays in the state x0 up to time t1. At this point
the system jumps to state x1 with probability p(x1|x0, a0), and earns a reward
r(x0, a0)(t1 −t0). If the state x1 ∈B, the system will stay at the target set B
forever. If the state x1 ∈Bc, a new decision epoch t1 comes along. Then, based
on the present state x1 and the previous state x0, the decision maker chooses
an action a1 ∈A(x1) and the process is repeated. Thus, during its evolution,
the system receives a series of rewards. The decision maker aims at maximizing
the exponential utility of the total rewards before the state of the system ﬁrst
reaches the target set B.
Let
hk := (x0, a0, t1, x1, a1, . . . , tk, xk),
(2)
be an admissible history up to the k-th decision epoch, where tm+1 ≥tm ≥0,
xm ∈S, am ∈A(xm) for m = 0, 1, . . . , k −1, xk ∈S. From the evolution of
SMDPs, we know that tk+1 (k ≥0) denotes the (k + 1)-th decision epoch, xk
denotes the state of the system on [tk, tk+1), ak denotes an action, which is
chosen by the decision maker at time tk. θk+1 := tk+1 −tk denotes the sojourn
time at state xk, which may follow any given probability distribution.
The set of all admissible histories hk is denoted by Hk, that is H0 := S and
Hk := (S × A × (0, +∞])k × S.
For the sake of the optimality problem, we shall pay close attention to some
classes of policies that we introduce below.
Deﬁnition 1. A sequence π = {πk, k ≥0} is called
stochastic history-
dependent policy if, for any k = 0, 1, 2 . . ., the stochastic kernel πk on A(xk)
given Hk satisﬁes

First Passage Exponential Optimality Problem for SMDPs
23
πk(A(xk)|hk) = 1 for any hk ∈Hk.
Denote by Π the set of all stochastic history-dependent policies, φ the set of all
stochastic kernels ϕ on A(x) given S such that ϕ(A(x)|x) = 1, and F the family
of all Borel measurable functions f from S to A(x) for all x ∈S.
Deﬁnition 2. A policy π = {πk} ∈Π is called stochastic Markov if there
exists a sequence of stochastic kernels {ϕk} such that πk(·|hk) = ϕk(·|xk) for
k ≥0, hk ∈Hk, and ϕk ∈φ. For simplicity, we denote such a policy by π = {ϕk}.
A stochastic Markov policy π = {ϕk} is called
stochastic stationary if all
the ϕk are independent of k. Such a policy is denoted by ϕ, for simplicity.
A stochastic Markov policy π = {ϕk} is called deterministic Markov if each
ϕk(·|xk) is concentrated at fk(xk) ∈A(xk) for some measurable functions {fk}
with k ≥0, xk ∈S, and fk ∈F.
A deterministic Markov policy π = {fk} is called
deterministic stationary
if all the measurable functions fk are independent of k. For simplicity, such a
policy is denoted by f.
The class of all stochastic Markov, stochastic stationary, deterministic
Markov, and deterministic stationary policies are, respectively, denoted by
ΠRM, ΠRS, ΠDM and ΠDS. Clearly, φ = ΠRS ⊂ΠRM ⊂Π and F = ΠDS ⊂
ΠDM ⊂Π.
For the sake of mathematical rigor, we need to construct a well-suited prob-
ability space. Deﬁne a sample space Ω := {(x0, a0, t1, x1, a1, . . . , tk, xk, ak, . . .)|
x0 ∈S, a0 ∈A(x0), tl ∈(0, ∞], xl ∈S, al ∈A(xl) for each 1 ≤l ≤k, k ≥1}. Let
F be the Borel σ-algebra of the sample space Ω. For any ω := (x0, a0, t1, x1, a1,
. . . , tk, xk, ak, . . .) ∈Ω, we deﬁne the random variables Tk, Xk, Ak on (Ω, F) as
follows:
Tk(ω) := tk, Xk(ω) := xk, Ak(ω) := ak, T∞(ω) := lim
k→∞Tk(ω).
(3)
In what follows, for the purpose of simplicity, we omit the argument ω.
Moreover, we deﬁne the state process {xt, t ≥0} and the action process
{At, t ≥0} on (Ω, F) by
xt :=

k≥0
I{Tk≤t<Tk+1}Xk + ΔI{t≥T∞},
At :=

k≥0
I{Tk≤t<Tk+1}Ak + aΔI{t≥T∞},
where ID(·) denotes the indicator function on the set D, Δ ̸∈E is a cemetery
state, and aΔ is an isolated point.
For any policy π ∈Π and initial state x ∈S, in the light of the Ionescu
Tulcea theorem (e.g., Proposition C.10 in [11]), there exist a unique probability
measure P π
x on the measurable space (Ω, F) such that,
P π
x (Ak ∈Γ|T0, X0, A0, . . . , Tk, Xk) = πk(Γ|T0, X0, A0, . . . , Tk, Xk),
(4)

24
H. Huo and X. Wen
P π
x (Tk+1 −Tk ≤u, Xk+1 ∈D|T0, X0, A0, . . . , Tk, Xk, Ak) = Q(u, D|Xk, Ak),
for each u ∈R+, Γ ∈B(A), D ∈B(S), k ≥0. We shall use Eπ
x to represent the
expectation operator with respect to P π
x .
To avoid the possibility that the system generates an inﬁnite number of jumps
within a ﬁxed ﬁnite horizon, we need to impose the following condition.
Assumption 1 For any π ∈Π, x ∈S, P π
x (T∞= ∞) = 1.
To ease the veriﬁcation of Assumption 1, we state the following suﬃcient
condition for its validity.
Lemma 1. If Q(δ, S|x, a) ≤1 −ε with some constants δ, ε > 0 and (x, a) ∈K,
then Assumption 1 holds.
Proof. The proof follows directly from Proposition 2.1 in [14].
⊓⊔
Remark 1.(a) A key feature of Lemma 1 is that the condition is imposed on
the semi-Markov kernel, and can be directly veriﬁed.
(b) Lemma 1 is the standard regular condition, which is similar to the classic
expected criteria for SMDPs, see, for instance [13–15,18].
The random variable τB is given by
τB =

inf{t ≥0 : xt ∈B},
if {t ≥0 : xt ∈B} ̸= ∅;
+∞,
otherwise.
(5)
represents the ﬁrst passage time for which the state process {xt, t ≥0} ﬁrst
enters the target set B.
For any x ∈S and π ∈Π, we deﬁne the ﬁrst passage exponential utility
criterion by
V π(x) := Eπ
x

e−γ
 τB
0
r(xt,At)dt
,
(6)
where γ > 0 represents the risk aversion coeﬃcient, which expresses the degree
of risk aversion that the decision makers face to the level of the total rewards
before the state of the system ﬁrst enters the target set.
Deﬁnition 3. A policy π∗∈Π is called an optimal policy, if
V π∗(x) = sup
π∈Π
V π(x), x ∈S.
(7)
The corresponding value function is given by
V ∗(x) := sup
π∈Π
V π(x), x ∈S.
(8)
Remark 2. Note that for any π ∈Π and initial state x ∈B, in view of (5), (6)
and (8), we have τB = 0 and V ∗(x) = V π(x) = 1. In order to avoid this trivial
case, our arguments consider only the case x ∈Bc.

First Passage Exponential Optimality Problem for SMDPs
25
3
Main Results
In this section, we will state the main results concerning the ﬁrst passage expo-
nential utility optimality problem for SMDPs.
Notation: Let Vm denotes the set of all Borel measurable functions from S
to [0, 1]. For any x ∈Bc, V ∈Vm, ϕ ∈φ, a ∈A(x), we deﬁne the operators
M aV, M ϕV and MV as follows:
M aV (x) :=

B
 +∞
0
e−γr(x,a)uQ(du, dy|x, a)
+

Bc
 +∞
0
e−γr(x,a)uV (y)Q(du, dy|x, a),
M ϕV (x) :=

A(x)
ϕ(da|x)M aV (x),
MV (x) :=
sup
a∈A(x)
M aV (x).
For any ϕ ∈φ, we also deﬁne the operators (M nV, n ≥1), ((M ϕ)nV, n ≥1)
as follows:
M n+1V = M(M nV ), (M ϕ)n+1V = M ϕ((M ϕ)nV ), n ≥1.
Since the state and action space are Borel space, in order to ensure the
existence of optimal policies, it follows from [28,31,32], we need establish the
following continuity-compactness condition, and which is trivially satisﬁed for
the case of denumerable state space and ﬁnite action set A(x) with x ∈S.
Assumption 2. (a) For any x ∈Bc, A(x) is compact;
(b) For each ﬁxed V
∈Vm,

y∈S
 +∞
0
e−γr(x,a)uV (y)Q(du, dy|x, a) is upper
semicontinuous and inf-compact on K.
Lemma 2. Suppose that Assumptions 1 and 2 hold. Then the operators M a and
M have the following properties:
(a) For any U, V ∈Vm, if U ≥V , then M aU(x) ≥M aV (x) and MU(x) ≥
MV (x) for any x ∈S and a ∈A(x).
(b) For any V ∈Vm, there exists a policy f ∈ΠDS such that MV (x) = M fV (x)
for any x ∈S.
Proof. (a) This statement follows from the deﬁnitions of operators M a and M.
(b) Assuming the validity of Assumption 1 and 2, and invoking the measurable
selection theorem (Theorem B.6 in [28]), we conclude that, for each x ∈S,
there is a stationary policy f ∈F with M fV (x) = MV (x) = supa∈A(x) M a
V (x).
⊓⊔

26
H. Huo and X. Wen
Since state process {xt, t ≥0} is non-explosive and the reward rate is non-
negative, in view of the monotone convergence theorem, we can rewrite V π(x)
as follows:
V π(x) = Eπ
x

e−γ
 τB
0
r(xt,At)dt
= Eπ
x

e−γ ∞
m=0
 Tm+1
Tm
I{τB>t}r(xt,At)dt
= Eπ
x

e
−γ ∞
m=0
 Tm+1
Tm
I{m
k=0{xTk ∈Bc}}r(xt,At)dt
(9)
= lim
n→∞Eπ
x

e
−γ n
m=0
 Tm+1
Tm
I{m
k=0{xTk ∈Bc}}r(xt,At)dt
.
We shall ﬁnd it essential to deﬁne the sequence {V π
n (x), n = −1, 0, 1, . . .} by
V π
−1(x) := 1,
V π
n (x) := Eπ
x

e
−γ n
m=0
 Tm+1
Tm
I{m
k=0{xTk ∈Bc)}}r(xt,At)dt
.
Obviously, V π
n (x) ≥V π
n+1(x) for any n ≥−1 and limn→∞V π
n (x) = V π(x) for all
x ∈Bc.
Proposition 1. For each π = {π0, π1, . . .} ∈Π and x ∈S. Then, there exists
a policy π
′ = {ϕ0, ϕ1, . . .} ∈ΠRM, satisfying V π(x) = V π
′
(x).
Proof. Since V π(x) = Eπ
x

e
−γ ∞
m=0
 Tm+1
Tm
I{m
k=0{xTk ∈Bc}}r(xt,At)dt
in (9), to
prove this proposition we need to prove that, for each x ∈S, there exists a
randomized Markov policy π
′ = {ϕ0, ϕ1, . . .} ∈ΠRM such that
P π
′
x (Xk ∈D, Tn+1 −Tn > u, Ak ∈Γ)
= P π
x (Xk ∈D, Tn+1 −Tn > u, Ak ∈Γ)
with k = 0, 1, . . . , u ∈R+, D ∈B(S), Γ ∈B(A).
Thus, in view of property (4), it suﬃces to show that
P π
′
x (Xk ∈D, Ak ∈Γ) = P π
x (Xk ∈D, Ak ∈Γ).
(10)
Along the same arguments as in the proof of Theorem 5.5.1 in [28], one can
prove (10) by induction on the integer k.
⊓⊔
Proposition 1 states, in particular, that in seeking optimal policies for (7), it
is suﬃcient to limit the search to the set of randomized Markov policies. Thus,
from now on, we will limit our attention to ΠRM.
The following lemma is required to establish the optimality equation.
Lemma 3. Under Assumption 1 and 2, for any x ∈S, n ≥−1, and π =
{ϕ0, ϕ1, . . .} ∈ΠRM, the following statements hold.

First Passage Exponential Optimality Problem for SMDPs
27
(a) V π
n ∈Vm and V π ∈Vm.
(b) V π
n+1(x) = M ϕ0V
1π
n (x) and V π(x) = M ϕ0V
1π(x), with 1π := {ϕ1, ϕ2, . . .}
being the 1-shift policy of π.
In particular, for any f ∈F, V f
n+1(x) = M fV f
n (x) and V f(x) = M fV f(x).
Proof. (a) We shall prove the ﬁrst statement of (a) by induction on the integer
n ≥−1. The statement is trivial for n = −1 since V π
−1(x) = 1 ∈Vm for any
x ∈S and π ∈ΠRM. Assume the statement holds for any n < k. Then, by (4)
and the property of conditional expectation, we have
V π
k+1(x)
= Eπ
x

e
−γ k+1
m=0
 Tm+1
Tm
I{m
k=0{xTk ∈Bc}}r(xt,At)dt
= Eπ
x[Eπ
x[e
−γ k+1
m=0
 Tm+1
Tm
I{m
k=0{xTk ∈Bc}}r(xt,At)dt|T0, xT0, A0, T1, xT1]]
=

A(x)
ϕ0(da|x)
×

S
 +∞
0
Eπ
x

e
−γ(
 T1
0
r(xt,At)dt+k+1
m=1
 Tm+1
Tm
I{m
k=1{xTk ∈Bc}}r(xt,At)dt)
|T0 = 0, xT0 = x, A0 = a, T1 = u, xT1 = y

Q(du, dy|x, a)
=

A(x)
ϕ0(da|x)

B
 +∞
0
e−γr(x,a)uQ(du, dy|x, a) +

A(x)
ϕ0(da|x)
×

Bc
 +∞
0
Eπ
x

e
−γ(
 T1
0
r(xt,At)dt+k+1
m=1
 Tm+1
Tm
I{m
k=1{xTk ∈Bc}}r(xt,At)dt)
|T0 = 0, xT0 = x, A0 = a, T1 = u, xT1 = y

Q(du, dy|x, a)
=

A(x)
ϕ0(da|x)[

B
 +∞
0
e−γr(x,a)uQ(du, j|x, a)
+

Bc
 +∞
0
e−γr(x,a)uE
1π
y

e
−γ k
m=0
 Tm+1
Tm
I{m
k=0{xTk ∈Bc}}r(xt,At)dt
×Q(du, dy|x, a)]
=

A(x)
ϕ0(da|x)[

B
 +∞
0
e−γr(x,a)uQ(du, dy|x, a)
+

Bc
 +∞
0
e−γr(x,a)uV
1π
k (y)Q(du, dy|x, a)]
:= M ϕ0V
1π
k (x)
which together with induction hypothesis implies that V π
k+1(x) is a measurable
function and V π
k+1(x) ≤1. Thus, V π
n ∈Vm for all n ≥−1. Since the limit of a
convergent sequence of measurable functions is itself a measurable function, we
obtain limn→∞V π
n = V π ∈Vm. This concludes the proof of (a).

28
H. Huo and X. Wen
(b) From the proof of part (a), we can deduce that, for any x ∈Bc and
n ≥−1,
V π
n+1(x) = M ϕ0V
1π
n (x).
(11)
Letting n →∞in (11) and using the monotone convergence theorem, we obtain
V π(x) = M ϕ0V
1π(x).
In particular, for π = f ∈F, we have V f(x) = M fV f(x).
⊓⊔
Remark 3. For any x ∈Bc and f ∈F, one can use Lemma 3 to develop an
eﬃcient iteration algorithm for the computation of the function V f(x) based
on the following: V f(x) = limn→∞V f
n (x) where V f
−1(x) := 1 and V f
n+1(x) =
M fV f
n (x) for n ≥0.
The following theorem states the existence of an optimality equation.
Theorem 1. Under Assumption 1 and 2, the following hold.
(a) For each n ≥−1, let V ∗
n+1 := MV ∗
n with V ∗
−1 := 1. Then, limn→∞V ∗
n =
V ∗∈Vm.
(b) The value function V ∗is a solution to the optimality equation V ∗= MV ∗.
(c) There is a policy f ∗∈F such that V ∗(x) = M fV ∗(x), x ∈Bc.
Proof. (a) Using Lemma 2(a) and the deﬁnition of the operator M, we obtain
0 ≤V ∗
n+1(x) ≤V ∗
n (x) ≤1 and V ∗
n ∈Vm, n ≥−1, for any x ∈Bc. Thus,
˜V := limn→∞V ∗
n ∈Vm, since the limit of a convergent sequence of measurable
function is also measurable. To complete the proof of part (a), we need to prove
that ˜V = V ∗.
We ﬁrst show by induction on n ≥−1 that for any x ∈Bc and π =
{ϕ0, ϕ1, . . .} ∈ΠRM
V ∗
n (x) ≥V π
n (x).
(12)
It is clear that V ∗
−1 = V π
−1 = 1 for any π ∈ΠRM. Suppose that (12) holds for
any n ≤k. By the induction hypothesis, the deﬁnition of the operator M and
Lemma 3(b), we have
V ∗
k+1(x) = MV ∗
k (x) ≥MV
1π
k (x) ≥M ϕ0V
1π
k (x) = V π
k+1(x).
Letting n →∞in (12), we obtain ˜V (x) = limn→∞V ∗
n (x) ≥V π(x) with π ∈
ΠRM. Since π is arbitrary, we conclude that ˜V (x) ≥V ∗(x).
We need, now, to prove the reverse inequality ˜V (x) ≤V ∗(x). For any
x ∈Bc, n ≥−1, let An := {a ∈A(x)|M aV ∗
n (x) ≥M ˜V (x)} and A∗:=
{a ∈A(x)|M a ˜V (x) = M ˜V (x)}. By the compactness-continuity condition in
Assumption 2 and the convergence V ∗
n ↓˜V , we conclude that An and A∗are
nonempty and compact, and that An ↓A∗. It follows from the measurable selec-
tion theorem (Theorem B.6 in [28]) that, for each n ≥1, there exist an ∈An

First Passage Exponential Optimality Problem for SMDPs
29
such that M anV ∗
n (x) = MV ∗
n (x). Hence, using compactness and the convergence
An ↓A∗, we deduce that there exist an a∗∈A∗and a subsequence {ank} of
{an} such that ank →a∗. Since V ∗
n ↓˜V , by Lemma 3(a), for any given n ≥1,
we have
M ank V ∗
nk(x) ≤M ank V ∗
n (x) ∀nk ≥n.
Letting k →∞and using the upper semicontinuity condition in Assumption 2
give
˜V ∗(x) ≤M a∗V ∗
n (x),
which together with the convergence V ∗
n ↓˜V imply
˜V ∗(x) ≤M a∗˜V (x) ≤M ˜V (x),
By Lemma 2(b), there exists a stationary policy f ∈F such that
˜V (x) ≤M ˜V (x) = M f ˜V (x).
Moreover, using Lemma 2(a), Lemma 3(b) and Remark 3, we obtain
˜V (x) ≤(M f)n ˜V (x) ≤(M f)nV f
−1(x) = V f
n−1(x).
Letting n →∞, and invoking Remark 3, we obtain ˜V (x) ≤V f(x) ≤V ∗(x),
which proves the part (a) of the theorem.
(b) By virtue of Lemma 3(b), we know that for any x ∈Bc and π ∈ΠRM,
we have
V π(x) = M ϕ0V
1π(x) ≤M ϕ0V ∗(x) ≤MV ∗(x).
Taking the supremum over all policies π ∈ΠRM implies V ∗(x) ≤MV ∗(x).
The reverse inequality is proved as follows: From the deﬁnition of V ∗
n , for any
x ∈Bc and a ∈A(x),
V ∗
n+1(x) = MV ∗
n (x) ≥M aV ∗
n (x).
Letting n →∞and using the monotone convergence theorem, we obtain
V ∗(x) ≥M aV ∗(x),
which implies that V ∗(x) ≥MV ∗(x) since a ∈A(x) is arbitrary. This proves
V ∗= MV ∗.
(c) The statement in (c) follows from Lemma 2.
⊓⊔
To guarantee the uniqueness of solution of the optimality equation and the
existence of the optimal policies, we require the following additional condition
(i.e., Assumption 3).
Assumption 3 For any x ∈Bc, f ∈Πs, P f
x (τB < +∞) = 1.

30
H. Huo and X. Wen
Remark 4.(a) Assumption 3 means that, when the initial state of such system
is X0 = x ∈S, the controlled state process {xt, t ≥0} will eventually enter
the target set B under the policy f ∈F.
(b) Letting Xn := xTn, n = 0, 1, . . ., Tn denotes the jump epoch. Then, we
obtain a discrete-time embedded chain {Xn, n ≥0}. For every x ∈Bc,
using Theorem 3.3 in [16], we know that Assumption 3 can be rewritten as
follows:.
P f
x (τB < +∞) = P f
x (
∞

n = 1
{Xn ∈B}) = 1,
which is equivalent to
P f
x (
∞
	
n=1
{Xn ∈Bc}) = 0.
(13)
(c) Using Proposition 3.3 in [19], we also obtain a suﬃcient condition to verify
Assumption 3. There exist a constant α > 0 such that

B P(dy|x, a) ≥α for
(x, a) ∈Bc × A(x), then Assumption 3 holds.
Lemma 4. Suppose that Assumptions 1 and 3 hold.
(a) If U, V ∈Vm are such that U(x) −V (x) ≤M f(U −V )(x) with x ∈Bc, f ∈
Πs, then U(x) ≤V (x).
(b) For any f ∈Πs, V f ∈Vm is the unique solution to the equation V = M fV .
Proof. (a) For any U, V ∈Vm, x ∈Bc, f ∈Πs, we will show the following
conclusion by induction,
(M f)n(U −V )(x) ≤P f
x (
n
	
k=1
{Xk ∈Bc}), n ≥1.
(14)
For n = 1, it follows from U, V ∈Vm that
M f(U −V )(x) = M fU(x) −M fV (x)
=

Bc
 +∞
0
e−γr(x,f)u(U −V )(y)Q(du, dy|x, a)
≤

Bc
 +∞
0
Q(du, dy|x, a)
= P f
x (X1 ∈Bc).
Suppose that (14) holds for n = k. Then, by using the induction hypothesis
and the nonnegativity of the reward rate, we have

First Passage Exponential Optimality Problem for SMDPs
31
(M f)k+1(U −V )(x) = M f(M f)k(U −V )(x)
=

Bc
 +∞
0
e−γr(x,f)u(M f)k(U −V )(y)
×Q(du, dy|x, a)
=

Bc
 +∞
0
e−γr(x,f)uP f
y (
k	
l=1
{Xl ∈Bc})
×Q(du, dy|x, a)
≤

Bc
 +∞
0
P f
y (
k	
l=1
{Xl ∈Bc})Q(du, dy|x, a).
(15)
On the other hand,
P f
x (
k+1
	
l=1
{Xl ∈Bc})
= Ef
x[I{k+1
l=1 {Xl∈Bc}}]
= Ef
x[Ef
x[Ik+1
l=1 {Xl∈Bc}|X0, X1]
=

Bc
 +∞
0
P f
x
 k+1
	
l=1
{Xl ∈Bc}|X0 = x, X1 = y

Q(du, dy|x, a)
=

Bc
 +∞
0
P f
y

k	
l=1
{Xl ∈Bc}

Q(du, dy|x, a),
from which together with (15) and the induction, we have for all n ≥1,
U(x) −V (x) ≤(M f)n(U(x) −V (x)) ≤P f
x (
n
	
k=1
{Xk ∈Bc}).
(16)
Letting n →∞, using (13), we obtain
U(x) −V (x) ≤P f
x (
∞
	
k=1
{Xk ∈Bc}) = 0.
Then, U(x) ≤V (x), for x ∈S.
(b) For any x ∈S, f ∈F, it follows from Lemma 2(b) that V f(x) ∈Vm
satisﬁes the equation V (x) = M fV (x). If U(x) is another solution to the equa-
tion U(x) = M fU(x) on S, and thus U(x) −V f(x) = M f(U(x) −V f(x)),
which together with the statement in part (a), we know U(x) = V f(x) and the
uniqueness of solution to the equation is proved.
⊓⊔
Theorem 2. Suppose that Assumption 1,2 and 3 hold. Then, the following
statements hold.

32
H. Huo and X. Wen
(a) The value function V ∗is the unique solution to the optimality equation
V ∗= MV ∗.
(b) There is a policy f ∗∈F which satisﬁes V ∗= M f ∗V ∗, V ∗= V f ∗and such
a policy f ∗∈F is optimal.
Proof. (a) It follows from Lemma 3 (b) that V ∗satisﬁes the equation V ∗=
MV ∗. Then, by Lemma 2(b), there exists a stationary policy f ∗∈F such
that V ∗= M f ∗V ∗. Moreover, U is another solution of the equation U = MU.
Similarly, the existence of a policy f
′ ∈F satisfying U = M f
′
U is ensured by
Lemma 2(b). Then, we have V ∗−U ≤M f ∗(V ∗−U). Combining this inequality
and Lemma 4 yields that V ∗≤U. Similarly, we obtain U −V ∗≤M f
′
(U −V ∗)
and U ≤V ∗, which implies U = V ∗and the uniqueness of V ∗is achieved.
(b) Since V ∗∈Vm, for any x ∈Bc, Lemma 2 guarantees the existence of a
stationary policy f ∗∈F such that
V ∗(x) = M f ∗V ∗(x),
which together with Lemma 3 and Remark 11 yield
V ∗= lim
n→∞(M f ∗)nV ∗≤lim
n→∞(M f ∗)nV f ∗
−1 = lim
n→∞V f ∗
n−1 = V f ∗.
This implies the optimality of f ∗.
⊓⊔
Theorem 1 leads to the following iterative algorithm for computing the value
function and the corresponding optimal policies.
The value iteration algorithm procedure:
Step 1: For any x ∈Bc, set V ∗
−1(x) := 1.
Step 2: According to Theorem 1, the value V ∗
n+1(x), n ≥1, is iteratively
computed as:
M aV ∗
n (x) =

B
 +∞
0
e−γr(x,f)uQ(du, dy|x, a)
+

Bc
 +∞
0
e−γr(x,f)uV ∗
n (y)Q(du, dy|x, a),
V ∗
n+1(x) =
sup
a∈A(x)
{M aV ∗
n (x)}.
Step 3: When |V ∗
n+1 −V ∗
n | < 10−12, the iteration stops. Since V ∗
n is very
close to V ∗
n+1, one can view V ∗
n+1 as a good approximation of the value function
V ∗. In addition, Lemma 2 and Theorem 2 ensure the existence of a policy f ∗∈F
such that MV ∗= M f ∗V ∗, and this policy f ∗is optimal. Or else, go back to
step 2 and replace n with n + 1.
4
Example
In this section, an example is given to illustrate our main results, and to demon-
strate the computation of an optimal stationary policy and the corresponding
value function using the above described iterative algorithm.

First Passage Exponential Optimality Problem for SMDPs
33
Example 1. Consider a company using idle funds for ﬁnancial management.
When the company has some idle funds (which is denoted by state 1), the
decision maker gets the reward at the rate of return r(1, a11) ≥0 through
deposit method a11 or the reward at the rate of return r(1, a12) ≥0 through
another deposit method a12. When the company has plenty of idle funds (which
is denoted by state 2), the decision maker can choose a ﬁnancial management a21
earning in a reward rate r(2, a21) ≥0 or another ﬁnancing way a22 earning in a
reward rate r(2, a22) ≥0. When the company goes bankrupt (which is denoted
by state 0), the decision-maker does not need to choose any way of ﬁnancing a01
and cannot get any reward r(0, a01) = 0.
Suppose that the evolution mechanism of this system is described as a SMDP.
When the system state is 1, the decision maker selects an admissible action
a1n, n = 1, 2. Then, the system stays at the state 1 with a random time satisfying
the uniform distribution in the region [0, u(1, a1n)], n = 1, 2. After the system
state lingers for a period of time, it will move to a new state j ∈{0, 2} with
the probability p(j|1, a1n), n = 1, 2. When the action a2n is selected n = 1, 2,
the system stays at 2 with a random time satisfying the exponential distribution
with the parameter λ(2, a2n). Consequently, the system jumps to state j ∈{0, 1}
with the probability p(j|2, a2n), n = 1, 2.
The corresponding parameters of this SMDPs are given as follows: The state
space S = {0, 1, 2}, the target set B = {0} and the admissible action sets A(0) =
{a01}, A(1) = {a11, a12}, A(2) = {a21, a22}, the risk-sensitivity coeﬃcient γ = 1.
The transition probabilities are assumed to be given
p(0|0, a01) = 1,
p(0|1, a11) = 1
2,
p(2|1, a11) = 1
2,
p(0|1, a12) = 2
3,
p(2|1, a12) = 1
3,
p(0|2, a21) = 3
10,
(17)
p(1|2, a21) = 7
10,
p(0|2, a22) = 2
5,
p(1|2, a22) = 3
5.
In addition, the corresponding distribution parameters are given by
u(1, a11) = 30,
u(1, a12) = 40,
λ(2, a21) = 0.11,
λ(2, a22) = 0.13.
(18)
and the reward rates are given by
r(1, a11) = 0.0035,
r(1, a12) = 0.011,
r(2, a21) = 0.013,
r(2, a22) = 0.015.
In this model, we mainly focus on the existence and calculation parts of an
optimal policy and the value function for ﬁrst passage exponential utility crite-
rion. As can be seen from the discussion in Sect. 3 above, we ﬁrst need to verify
Assumption 1, 2 and 3. Indeed, by (17) and (18), we know that Assumption 1
and 3 are satisﬁed. Moreover, since the state space is denumerable and the action
space A is ﬁnite, Assumption 2 is trivially satisﬁed. Thus, by Theorem 1 and 2,

34
H. Huo and X. Wen
the value iteration technique can be used for evaluating the value function and
the exponential optimal policies as follows:
Step 1: Let V ∗
−1(x) := 1, x = 1, 2.
Step 2: For x = 1, 2, n ≥1, using Theorem 1 (a), we obtain
V ∗
n (1) = MV ∗
n−1(1),
= max

1
2 × 1
30 ×
 30
0
e−0.0035udu
+1
2 × 1
30 ×
 30
0
e−0.0035udu × V ∗
n−1(2),
2
3 × 1
40 ×
 40
0
e−0.011udu + 1
3 × 1
40 ×
 40
0
e−0.011udu × V ∗
n−1(2)

V ∗
n (2) = MV ∗
n−1(2),
= max

 3
10 × 0.11 ×
 +∞
0
e−0.123udu
+ 7
10 × 0.11 ×
 +∞
0
e−0.123udu × V ∗
n−1(1),
2
5 × 0.13 ×
 +∞
0
e−0.145udu + 3
5 × 0.13 ×
 +∞
0
e−0.145udu × V ∗
n−1(1)

Step 3: When |V ∗
n −V ∗
n−1| < 10−12, go to step 4, the value V ∗
n is usually
approximated as V ∗; otherwise, go to step n + 1 and go back to step 2.
0
5
10
15
20
25
30
0.8
0.82
0.84
0.86
0.88
0.9
0.92
0.94
0.96
0.98
1
Step n 
MaVn
*(1)
Ma
11Vn
*(1)
Ma
12Vn
*(1)
Fig. 1. The function M aV ∗
n (1)
Step 4: Plot out the graphs of the value functions M aijV ∗
n (i) and V ∗
n (i), i =
1, 2; j = 1, 2, see Figs. 1, 2 and 3.

First Passage Exponential Optimality Problem for SMDPs
35
0
5
10
15
20
25
30
0.8
0.82
0.84
0.86
0.88
0.9
0.92
0.94
0.96
0.98
1
Step n 
MaVn
*(2)
Ma
21Vn
*(2)
Ma
22Vn
*(2)
Fig. 2. The function M aV ∗
n (2)
0
5
10
15
20
25
30
0.82
0.84
0.86
0.88
0.9
0.92
0.94
0.96
0.98
1
Step n
The value Vn
*(i)
Vn
*(1)
Vn
*(2)
Fig. 3. The value function V ∗
n (i)
Moreover, for x = 1, using Theorem 1, 2, Fig. 1 and Fig. 2, we know that
MV ∗(1) = V ∗(1) = M a11V ∗(1).
For x = 2, we also obtain
MV ∗(2) = V ∗(2) = M a22V ∗(2).
According to the above analysis and Theorem 2, we obtain the optimal
stationary policy f ∗(1) = a12, f ∗(2) = a21 and the value function V ∗(1) =
0.8660,V ∗(2) = 0.8245.

36
H. Huo and X. Wen
Acknowledgement. This work was supported by National Natural Science Foun-
dation of China (Grant No. 11961005, 11801590); Foundation of Guangxi Educational
Committee (Grant No. KY2019YB0369); Ph.D. research startup foundation of Guangxi
University of Science and Technology (Grant No. 18Z06); Guangxi Natural Science
Foundation Program (Grant No. 2020GXNSFAA297196).
References
1. Ba¨uerle, N., Rieder, U.: Markov Decision Processes with Applications to Finance.
Springer, Heidelberg (2011)
2. Ba¨uerle, N., Rieder, U.: More risk-sensitive Markov decision processes. Math. Oper.
Res. 39, 105–120 (2014)
3. Cao, X.R.: Semi-Markov decision problems and performance sensitivity analysis.
IEEE Trans. Autom. Control 48, 758–769 (2003)
4. Cavazos-Cadena, R., Montes-De-Oca, R.: Optimal stationary policies in risk-
sensitive dynamic programs with ﬁnite state space and nonnegative rewards. Appl.
Math. (Warsaw) 27, 167–185 (2000)
5. Cavazos-Cadena, R., Montes-De-Oca, R.: Nearly optimal policies in risk-sensitive
positive dynamic programming on discrete spaces. Math. Meth. Oper. Res. 52,
133–167 (2000)
6. Chung, K.J., Sobel, M.J.: Discounted MDP’s: distribution functions and exponen-
tial utility maximization. SIAM J. Control Optim. 25, 49–62 (1987)
7. Ghosh, M.K., Saha, S.: Risk-sensitive control of continuous time Markov chains.
Stochastics 86, 655–675 (2014)
8. Ghosh, M.K., Saha, S.: Non-stationary semi-Markov decision processes on a ﬁnite
horizon. Stoch. Anal. Appl. 31, 183–190 (2013)
9. Guo, X., Liu, Q.L., Zhang, Y.: Finite horizon risk-sensitive continuous-time Markov
decision processes with unbounded transition and cost rates. 4OR 17, 427–442
(2019)
10. Guo, X.P., Hern´andez-Lerma, O.: Continuous-Time Markov Decision Processes:
Theory and Applications. Springer, Berlin (2009)
11. Hern´andez-Lerma, O., Lasserre, J.B.: Discrete-Time Markov Control Processes:
Basic Optimality Criteria. Springer, New York (1996)
12. Howard, R.A., Matheson, J.E.: Risk-sensitive Markov decision processes. Manage.
Sci. 18, 356–369 (1972)
13. Huang, Y.H., Guo, X.P.: Discounted semi-Markov decision processes with nonneg-
ative costs. Acta Math. Sin. (Chinese Ser.) 53, 503–514 (2010)
14. Huang, Y.H., Guo, X.P.: Finite horizon semi-Markov decision processes with appli-
cation to maintenance systems. Eur. J. Oper. Res. 212, 131–140 (2011)
15. Huang, Y.H., Guo, X.P.: Mean-variance problems for ﬁnite horizon semi-Markov
decision processes. Appl. Math. Optim. 72, 233–259 (2015)
16. Huang, Y.H., Guo, X.P., Song, X.Y.: Performance analysis for controlled semi-
Markov process. J. Optim. Theory Appl. 150, 395–415 (2011)
17. Huang, Y.H., Lian, Z.T., Guo, X.P.: Risk-sensitive semi-Markov decision processes
with general utilities and multiple criteria. Adv. Appl. Probab. 50, 783–804 (2018)
18. Huang, X.X., Zou, X.L., Guo, X.P.: A minimization problem of the risk probability
in ﬁrst passage semi-Markov decision processes with loss rates. Sci. China Math.
58, 1923–1938 (2015)

First Passage Exponential Optimality Problem for SMDPs
37
19. Huo, H.F., Zou, X.L., Guo, X.P.: The risk probability criterion for discounted
continuous-time Markov decision processes. Discrete Event Dyn. Syst. 27, 675–
699 (2017)
20. Janssen, J., Manca, R.: Semi-Markov Risk Models for Finance, Insurance, and
Reliability. Springer, New York (2006)
21. Jaquette, S.C.: A utility criterion for Markov decision processes. Manage. Sci. 23,
43–49 (1976)
22. Ja´skiewicz, A.: A note on negative dynamic programming for risk-sensitive control.
Oper. Res. Lett. 36, 531–534 (2008)
23. Ja´skiewicz, A.: On the equivalence of two expected average cost criteria for semi
Markov control processes. Math. Oper. Res. 29, 326–338 (2013)
24. Limnios, N., Oprisan, G.: Semi-Markov Processes and Reliability. Birkh¨auser,
Boston (2001)
25. Luque-V´asquez, F., Minj´arez-Sosa, J.A.: Semi-Markov control processes with
unknown holding times distribution under a discounted criterion. Math. Meth.
Oper. Res. 61, 455–468 (2005)
26. Mamer, J.W.: Successive approximations for ﬁnite horizon semi-Markov decision
processes with application to asset liquidation. Oper. Res. 34, 638–644 (1986)
27. Nollau, V.: Solution of a discounted semi-Markovian decision problem by successive
overrelaxation. Optimization 39, 85–97 (1997)
28. Puterman, M.L.: Markov Decision Processes: Discrete Stochastic Dynamic Pro-
gramming. Wiley, New York (1994)
29. Sch¨al, M.: Control of ruin probabilities by discrete-time investments. Math. Meth.
Oper. Res. 70, 141–158 (2005)
30. Wei, Q.D.: Continuous-time Markov decision processes with risk-sensitive ﬁnite-
horizon cost criterion. Math. Meth. Oper. Res. 84, 1–27 (2016)
31. Wei, Q.D., Guo, X.P.: New average optimality conditions for semi-Markov decision
processes in Borel spaces. J. Optim. Theory Appl. 153, 709–732 (2012)
32. Wei, Q.D., Guo, X.P.: Constrained semi-Markov decision processes with ratio and
time expected average criteria in Polish spaces. Optimization 64, 1593–1623 (2015)
33. Yushkevich, A.A.: On semi-Markov controlled models with average reward crite-
rion. Theory Probab. Appl. 26, 808–815 (1982)
34. Zhang, Y.: Continuous-time Markov decision processes with exponential utility.
SIAM J. Control Optim. 55, 1–24 (2017)

Controlled Random Walk: Conjecture
and Counter-Example
Alexey B. Piunovskiy(B)
Department of Mathematical Sciences, University of Liverpool,
Liverpool L69 7ZL, UK
piunov@liv.ac.uk
Abstract. In this paper we investigate the following conjecture about
the random walk on the positive integer lattice, starting from a large
point i > 0 and up to the absorption at negative points: on the ﬁrst steps,
one has to maximize the expected reward coming from passing through
one point on the lattice. Under appropriate conditions, this conjecture is
true. The counter-example shows that sometimes it is not valid.
Keywords: Random walk · Markov decision process · Turnpike ·
Total expected cost
AMS(2020) Subject Classiﬁcation: Primary 90C40 · Secondary
90C39
1
Introduction
The current article is an attempt to study the conjecture formulated by Prof.
I.Sonin in a private communication.
The random walk on the positive integer lattice, starting from X0 = i, is
deﬁned by equation
Xt = Xt−1 −Zt(at)
and is terminated as soon as Xt < 0. Here {Zt(a)}∞
t=1 are mutually independent
positive integer-valued random variables depending on the action a ∈A =
{a1, a2, . . . , aN}, with the given probability distribution
P(Z(a) = m) = pm(a),
m = 1, 2, . . . , M.
See Fig. 1.
On each step t, the associated expected reward equals Rat. For example, if
RZ(a)(a) is the reward associated with the action a ∈A and the value Z(a),
then
Ra =
M

m=1
Rm(a)pm(a).
c
⃝The Author(s), under exclusive license to Springer Nature Switzerland AG 2021
A. Piunovskiy and Y. Zhang (Eds.): Modern Trends in Controlled Stochastic Processes,
ECC 41, pp. 38–56, 2021. https://doi.org/10.1007/978-3-030-76928-4_3

Controlled Random Walk: Conjecture and Counter-Example
39
Absorbing states
Fig. 1. Random walk.
For a ﬁxed a ∈A, for a large initial state i, the total expected number of steps
up to the absorption equals ≈
i
La , where La := E[Z(a)] = M
m=1 mpm(a). Thus,
the total expected reward is ≈i Ra
La . To put it slightly diﬀerent, the expected
reward, coming from passing through one point on the lattice, equals ≈Ra
La . The
conjecture to be investigated reads as follows:
There is such I < ∞that, if Xt−1 ≥I , then the optimal action at ∈A∗,
where A∗:=

a ∈A :
Ra
La = c∗:= maxa∈A Ra
La

(1)
In the current article, we provide suﬃcient conditions for this conjecture to
be valid. The numerical example in Sect. 4 shows that in general it is not the
case. In Sect. 5, we formulate the similar statement for the discounted version of
the described model. All the proofs are presented in the Appendix.
In what follows, if P is a matrix, then Ps,· denotes its s-th row. We say that
a stochastic matrix P is ergodic or aperiodic if the corresponding Markov chain
is so. The maximum (minimum) over the empty set equals −∞(+∞).
2
MDP Formulation and Preliminaries
Obviously, we deal with the Markov decision process (MDP) with the state space
X := {−M, −M + 1, . . . , −1, 0, 1, 2, . . .},
action space
A := {a1, a2, . . . , aN},
the transition probability
˜Pi,j(a) =

pm(a), if i ≥0, j = i −m, m = 1, 2, . . . , M;
0
otherwise,
and the reward function
ri(a) :=

0,
if i < 0;
Ra, if i ≥0.

40
A. B. Piunovskiy
The initial state is i ∈X, and we consider MDP ⟨X, A, ˜P, r⟩with the total
expected reward, with (random) states and actions
X0 = i, A1, X1, A2, . . . .
The deﬁnition of a strategy π (past-dependent, randomized) is conventional
[1,2,4,6]; Eπ
i is the corresponding mathematical expectation;
V (i) := sup
π Eπ
i
 ∞

t=1
rXt−1(At)

(2)
is the Bellman function for this MDP; i ∈X. Since the reward r is bounded and
the process Xt is ultimately absorbed at {−M, −M+1, . . . , −1} after (maximum)
i + 1 time steps, the function V is ﬁnite-valued. It is well known (see, e.g.,
[2, Ch.4] or [1, §9.5]) that the function V is the unique solution to the optimality
(Bellman) equation
V (i) = max
a∈A
	
Ra +
M

m=1
V (i −m)pm(a)

for i ≥0;
(3)
V (i) = 0
for i = −M, −M + 1, . . . , −1,
which can be solved successively for i = 0, 1, . . .. Now the conjecture (1) is
reformulated as follows:
There is such I < ∞that, for all i ≥I,
(4)
the maximum in (3) is only provided by a ∈A∗.
It is natural to call the interval {I, I + 1, . . .} ‘Turnpike’.
Lemma 1. Function
˜W(i) := V (i) −c∗i,
i ∈X
(5)
is the (unique) uniformly bounded function satisfying equation
˜W(i) = −c∗i
for i = −M, −M + 1, . . . , −1;
˜W(i) = max
a∈A
	
La
Ra
La −c∗

+
M

m=1
˜W(i −m)pm(a)

for i ≥0,
(6)
which can be solved successively for i = 0, 1, . . .. Hence
V (i) = c∗i + O(1)
when i →∞.
Moreover, for each i ∈X, the maxima in (3) and in (6) are provided by the
same values of a ∈A.

Controlled Random Walk: Conjecture and Counter-Example
41
Every value of i ∈X can be uniquely represented as
i = (k −1)M + s,
where s ∈S := {0, 1, . . . , M −1}, k = 0, 1, 2, . . . .
(7)
For each i ∈X with the corresponding values of k and s, we denote ˜W(i),
introduced in (5), as W k(s). Now Eq. (6) takes the following form:
W 0(s) = −c∗(−M + s)
for s ∈S;
W k+1(0) = max
a∈A

La
Ra
La −c∗

+
M−1

j=0
W k(j)pM−j(a)

,
(8)
W k+1(1) = max
a∈A

La
Ra
La −c∗

+ W k+1(0)p1(a) +
M−1

j=1
W k(j)pM−j+1(a)

,
. . .
W k+1(M −1) = max
a∈A

La
Ra
La −c∗

+ W k+1(M −2)p1(a) + W k+1(M −3)p2(a)
+ . . . +W k+1(0)pM−1(a) + W k(M −1)pM(a)

, k = 0, 1, . . . .
After we introduce the stochastic matrix
P(a) :=
⎛
⎜
⎜
⎝
P0,0(a) = pM(a)
P0,1(a) = pM−1(a)
. . . P0,M−1(a) = p1(a)
P1,0(a) = p1(a)
P1,1(a) = pM(a)
. . . P1,M−1(a) = p2(a)
. . .
. . .
PM−1,0(a) = pM−1(a) PM−1,1(a) = pM−2(a) . . . PM−1,M−1(a) = pM(a)
⎞
⎟
⎟
⎠,
(9)
the obtained equations for W k(s) can be rewritten as
W 0(s) = −c∗(−M + s)
for s ∈S;
(10)
W k+1(s) = max
a∈A

La
Ra
La −c∗

+
s−1

j=0
W k+1(j)Ps,j(a) +
M−1

j=s
W k(j)Ps,j(a)

for s ∈S, k ≥0.
Iterations (10) are similar to the Gauss-Seidel version of the value iteration
algorithm for the average reward MDP (see [6, §6.3.3]). For a ﬁxed k ≥0, we
substitute the expression for W k+1(0) in the formula for W k+1(1), the expression
for W k+1(0) and the modiﬁed expression for W k+1(1) in the formula for W k+1(2)
and so on. After we denote D the ﬁnite set of all mappings from S to A, called
below ‘decisions’, iterations (10) can be represented in the form
W 0(s) = −c∗(−M + s)
for s ∈S;
W k+1(s) = max
d∈D Ud ◦W k(s) for s ∈S, k ≥0,

42
A. B. Piunovskiy
where
Ud ◦W k(0) := Ld(0)
Rd(0)
Ld(0) −c∗

+
M−1

j=0
W k(j)P0,j(d(0))
Ud ◦W k(1) := Ld(1)
Rd(1)
Ld(1) −c∗

+ P1,0(d(1))Ld(0)
Rd(0)
Ld(0) −c∗

+P1,0(d(1))
M−1

j=0
W k(j)P0,j(d(0)) +
M−1

j=1
W k(j)P1,j(d(1))
and so on, up to Ud ◦W k(M −1).
In what follows, each function W : S →R is identiﬁed with the column
vector W ∈RM, and the both notations W(s) = Ws are in use. Now one can
rewrite iterations (10) in the matrix form:
W 0(s) = −c∗(−M + s),
s ∈S;
W k+1 = max
d∈D Ud ◦W k = max
d∈D{R(d) + Q(d)W k}, k = 0, 1, . . . ,
(11)
where, for ﬁxed d ∈D, the column vector R(d) ∈RM and the rows of the square
M × M matrix Q(d) are deﬁned recursively:
R0(d) = Ld(0)
Rd(0)
Ld(0) −c∗

;
Rl+1(d) = Ld(l+1)
Rd(l+1)
Ld(l+1) −c∗

+
l

j=0
Pl+1,j(d(l + 1))Rj(d),
(12)
l = 0, . . . , M −2;
Q0,j(d) = P0,j(d(0)),
j = 0, 1, . . . , M −1;
Ql+1,j(d) =
 l
i=0

Pl+1,i(d(l + 1))Qi,j(d)

for j < l + 1;
l
i=0

Pl+1,i(d(l + 1))Qi,j(d)

+ Pl+1,j(d(l + 1)) for j ≥l + 1.
(13)
l = 0, . . . , M −2;
Clearly, Rs(d) ≤0 for all s ∈S and d ∈D. We underline that the rows Ql,·(d)
of the matrix Q(d) with l ≤s do not depend on the values of d(s + 1), d(s +
2), . . . , d(M −1). Note also that, for every function W on S, there is ˆd ∈D
providing the component-wise maximum to Ud ◦W. Indeed, the values ˆd(s) can
be calculated successively for s = 0, 1, . . . , M −1, and any other mapping d is
such that Ud ◦W ≤T ˆd ◦W component-wise. In other words, ˆd solves the vector
optimization problem Ud ◦W →maxd∈D, i.e., this problem is well deﬁned: the
Pareto set contains the unique point T ˆd ◦W.

Controlled Random Walk: Conjecture and Counter-Example
43
Lemma 2. (a) The matrix Q(d) is stochastic for all d ∈D, provided the original
matrix P(a) is stochastic for all a ∈A.
(b) Suppose d ∈D is such that Ps,s(d(s)) > 0 for all s ∈S. Then, for all
s, l ∈S, Qs,l(d) > 0 provided Ps,l(d(s)) > 0.
Deﬁnition 1. Decisions d ∈D satisfying the property d(s) ∈A∗for all s ∈S
will be called trivial. Equivalently, a decision d ∈D is trivial if and only if
R(d) = 0. Here and below, 0 ∈RM is the zero vector. The set of all trivial
decisions is denoted as D∗.
The conjecture (4), and also (1) is now reformulated as follows:
There exists K such that, for all k ≥K, the maximum in (11)
(14)
is only provided by the trivial decisions d ∈D∗.
Note that all the vectors W 0, W 1, . . . are uniformly bounded by Lemma 1, and
the maxima in (4) and (10) are provided by the same values of a ∈A.
3
Main Results
Condition 1. There exists J
such that, for every sequence of mappings
ˆd1, ˆd2, . . . , ˆdJ ∈D∗, the matrix Q( ˆd1)Q( ˆd2) . . . Q( ˆdJ) contains no zeroes.
Theorem 1. If Condition 1 is satisﬁed then the conjecture (14) (and also (4)
and (1)) is valid.
In the following statements, the suﬃcient conditions for the conjecture (14)
to be valid are given in terms of the original matrix P(a).
Corollary 1. Suppose A∗= {a∗} is a singleton (consequently D∗= {d∗} is a
singleton with d∗(s) ≡a∗). Let the matrix P(a∗) be ergodic. Assume additionally
that pM(a∗) > 0. Then Condition 1 is satisﬁed, and hence the conjecture (14)
(and also (4) and (1)) is valid.
When using a diﬀerent method of attack, one can prove the following state-
ment. (See [5, Cor.2].)
Proposition 1. Suppose pM(a) > 0 for all a ∈A and, for any two states
i, j ∈S, there exists a path i0 = i →i1 →. . . →iN = j in S such that, for any
a0, a1, . . . , aN−1 ∈A,
Pi0,i1(a0)Pi1,i2(a1) . . . PiN−1,iN (aN−1) > 0.
Then the conjecture (14) (and also (4) and (1)) is valid.

44
A. B. Piunovskiy
The matrix P(a) has a cyclic structure. Thus, the conditions of Proposition 1
are satisﬁed if there is m < M having no common divisors with M such that
pM(a) > 0 and pm(a) > 0 for all a ∈A.
Let us brieﬂy discuss the connection of the conjecture (14) and the Turnpike
Theorem for the average reward MDP established in [5]. During the proof of
Theorem 1, it was shown that
lim
k→∞sp(W k+1 −W k) = 0,
(15)
where sp(W) := maxs∈S W(s)−mins∈S W(s) is the ‘span-seminorm’ and the vec-
tors W k come from the iterations (11). Condition (15) is suﬃcient for the Turn-
pike Theorem [5, Thm.1] which is strictly connected with the conjecture (14).
Namely, under mild additional requirements that Turnpike Theorem implies the
validity of the conjecture (14): see [5, Thm.3]. By the way, Proposition 1 also fol-
lows from the Turnpike Theorem [5, Thm.1]. One can show that in the example
from Sect. 4 limk→∞sp(W k+1 −W k) = 2

1 −h−2
ε

> 0: see [5, §5.3].
Suppose Condition 1 is satisﬁed, K is as in the proof of Theorem 1, k ≥K
is arbitrarily ﬁxed and d /∈D∗. Then, according to the proof of Theorem 1 (see
(24) and (25)), for each s ∈S such that d(s) /∈A∗,
Rs(d) + Qs,·(d)W k < max
d∈D∗{Rs(d) + Qs,·(d)W k} = W k+1(s).
Therefore, going back to (10), we again have the strict inequality
W k+1(s) > max
a∈A\A∗
⎧
⎨
⎩La
Ra
La −c∗

+
s−1

j=0
W k+1(j)Ps,j(a) +
M−1

j=s
W k(j)Ps,j(a)
⎫
⎬
⎭
and ﬁnally, going back to (3):
for all i = (k −1)M + s,
V (i) >
max
a∈A\A∗
	
Ra +
M

m=1
V (i −m)pm(a)

,
i.e., for the valid conjecture (4) we have the following:
max
a∈A
	
Ra +
M

m=1
V (i −m)pm(a)

max
a∈A\A∗
	
Ra +
M

m=1
V (i −m)pm(a)

(16)
for all i ≥I := (K −1)M.
4
Counter-Example
In this subsection, we show that the conjecture (4) (and also (1) and (14)) may
be not valid if the conditions formulated in Sect. 3 are not satisﬁed.
Put
A := {a1, a2}, M := 3, ε ∈(0, 1), p2(a1) = 1, p2(a2) = 1 −ε, p3(a2) = ε,

Controlled Random Walk: Conjecture and Counter-Example
45
where ε ∈(0, 1); other probabilities being zero. Finally, let Ra1 := 2 and Ra2 :=
h ∈(2, 2 + ε). Now
La1 = 2, La2 = 2 + ε, Ra1
La1 = 1, Ra2
La2 =
h
2 + ε < 1, c∗= 1, A∗= {a1}.
Below, we study the iterations (3).
Since h > 2, obvious calculations lead to the following expressions:
V (0) = V (1) = max{2, h} = h;
V (2) = max{2+V (0) = 2+h; h+(1−ε)V (0)+εV (−1) = h+(1−ε)h} = 2+h
because
2
1 −ε = 2[1 + ε + ε2 + . . .] > 2 + ε > h =⇒2 > (1 −ε)h.
V (3) = max{2 + V (1) = 2 + h;
h + (1 −ε)V (1) + εV (0) = 2h} = 2h. Further
properties of the function V are given in the following lemma.
Lemma 3. For all j ≥1, the following statements hold.
(i) For even steps i = 2j,
V (2j) = 2j + h,
and maximum in (3) is provided by a1 only.
(ii) For odd steps i = 2j −1,
V (2j −1) < 2ε(j −1) + (1 + ε)h −2
ε
.
(iii) For odd steps i = 2j + 1,
V (2j + 1) = (1 −ε)V (2j −1) + (1 + ε)h + 2ε(j −1),
and maximum in (3) is provided by a2 only.
Therefore, for all odd values of i, the maximum in (3) is provided only by
a2 /∈A∗. The conjecture (4) (and also (1) and (14)) is not valid.
In this example, D∗= {d∗} with d∗(s) ≡a∗= a1. The matrices P(a∗) and
Q(d∗) look as follows:
P(a∗) =
⎛
⎝
0 1 0
0 0 1
1 0 0
⎞
⎠; Q(d∗) =
⎛
⎝
0 1 0
0 0 1
0 1 0
⎞
⎠
and are periodic; pM(a∗) = 0. Thus, Theorem 1, Corollary 1 and Proposition 1
are not applicable.

46
A. B. Piunovskiy
5
Discounted Model
In this section, β ∈(0, 1) is the discount factor, expression (2) is replaced with
V β(i) := sup
π Eπ
i
 ∞

t=1
βt−1rXt−1(At)

,
(17)
and the optimality equation looks like
V β(i) = max
a∈A
	
Ra + β
M

m=1
V β(i −m)pm(a)

for i ≥0;
(18)
V β(i) = 0
for i = −M, −M + 1, . . . , −1.
Like previously, it can be solved successively for i = 0, 1, . . .. We put R∗:=
maxa∈A Ra and
A∗:= {a ∈A : Ra = R∗}.
(19)
The so-called turnpike theory in discounted models (see [6, §6.8)],[7]) leads
to the following statement (cf (4)):
Theorem 2. There is such I < ∞that, for all i ≥I, the maximum in (18) is
only provided by a ∈A∗.
Some recent developments of the turnpike theory for discounted MDPs can
be found in [3].
It is interesting to look at what happens if the discount factor β is close
to 1, assuming that the Condition 1 is satisﬁed (more generally, assuming the
conjecture (4) to be valid for β = 1). Denote the corresponding I as I1, i.e.,
I1 = (K −1)M with K as in the proof of Theorem 1 (see the end of Sect. 3),
and ﬁx an arbitrary I2 > I1. Obviously, for each i ∈X, limβ→1−V β(i) = V (i)
with V as in (3). According to (16), there is β0 ∈(0, 1) such that, for all i =
I1, I1 + 1, . . . , I2, for all β ∈[β0, 1]
max
a∈A
	
Ra +
M

m=1
V β(i −m)pm(a)

>
max
a∈A\A∗
	
Ra +
M

m=1
V β(i −m)pm(a)

.
Thus, for a ﬁxed β ∈[β0, 1], for all i = I1, I1 + 1, . . . , I2, the maximum in (18) is
only provided by a ∈A∗. Of course, by Theorem 2, there is a ﬁnite I3 > I2 such
that, for all i ≥I3, the maximum in (18) is only provided by a ∈A∗. Recall
that A∗and A∗are given by (1) and (19). One can say that, for β close to 1,
there are two turnpikes, where only actions from A∗and A∗are optimal in the
model (17): see Fig. 2. When β approaches 1, I2 and I3 go to inﬁnity, and in the
limiting case β = 1 we have just the conjecture (4).

Controlled Random Walk: Conjecture and Counter-Example
47
Only acons from       
are opmal
Only acons from       
are opmal
Turnpike 1
Turnpike 2
Fig. 2. Turnpikes for β ≈1.
6
Summary
In this article, we studied the conjecture (1) (equivalent to (4) and (14)) and
showed that in general it is not valid. In the discounted case, Turnpike Theorem 2
always holds. Under appropriate conditions, when the discount factor is close to
1, there are two turnpikes.
Appendix
Proof of Lemma 1. The case of i = −M, −M + 1, . . . , −1 is obvious.
For i ≥0,
˜W(i) = max
a∈A
	
Ra +
M

m=1
[ ˜W(i −m) + c∗(i −m)]pm(a)

−c∗i
= max
a∈A
	
Ra −c∗La +
M

m=1
˜W(i −m)pm(a)

.
Equalities (6) are proved, and the maxima in (4) and in (6) are provided by the
same values of a ∈A.
Finally, keeping in mind that
• | ˜W(i)| ≤|c∗|M for i < 0,
•
Ra
La −c∗≤0 for all a ∈A, and
•
Ra
La −c∗= 0 for a ∈A∗̸= ∅,
it is easy to prove by induction that | ˜W(i)| ≤|c∗|M for all i = 0, 1, 2, . . ..
2
Proof of Lemma 2. (a) All the elements of the matrix Q(d) are obviously non-
negative.
Clearly,
M−1

j=0
Q0,j(d) =
M−1

j=0
P0,j(d(0)) = 1

48
A. B. Piunovskiy
Suppose M−1
j=0 Qi,j(d) = 1 for all i ≤l for some l ∈{0, 1, . . . , M −2} and
consider l + 1:
M−1

j=0
Ql+1,j(d) =
l

j=0
l

i=0
[Pl+1,i(d(l + 1))Qi,j(d)]
+
M−1

j=l+1

l

i=0
[Pl+1,i(d(l + 1))Qi,j(d)] + Pl+1,j(d(l + 1))

=
l

i=0
⎛
⎝
M−1

j=0
Qi,j(d)
⎞
⎠Pl+1,i(d(l + 1)) +
M−1

j=l+1
Pl+1,j(d(l + 1))
=
M−1

i=0
Pl+1,i(d(l + 1)) = 1.
The last equality is by the induction supposition.
(b) If l ≥s then this statement follows directly from the deﬁnition (13):
Qs,l(d) ≥Ps,l(d(s)).
Suppose l < s. Then, again using (13), we have Qs,l(d) ≥Ps,l(d(s))Ql,l(d).
Since Ql,l(d) ≥Pl,l(d(l)) > 0, we ﬁnally obtain that
Qs,l(d) > 0,
if Ps,l(d(s)) > 0.
2
For the proof of Theorem 1 we need the following lemma.
Lemma 4. Suppose
→α∈RM is a substochastic row vector and M−1
i=0 αiRi(d) =
0 for some d ∈D. Then the row vector
→α Q(d) coincides with the row vector
→α Q( ˆd) for some ˆd ∈D∗with ˆd(i) = d(i) if αi > 0.
Proof. Suppose α0 ∈[0, 1] and consider the substochastic row vector
→α:=
(α0, 0, . . . , 0) ∈RM such that M−1
i=0 αiRi(d) = 0, where d ∈D is ﬁxed.
Then the row vector
→α Q(d) coincides with the row vector
→α Q( ˆd) for some
ˆd ∈D∗with ˆd(0) = d(0) if α0 > 0. Indeed, for α0 > 0, R0(d) = 0, and we put
ˆd(0) := d(0) ∈A∗. The other values ˆd(i) ∈A∗for i = 1, 2, . . . , M −1 can be
taken arbitrarily leading to equalities
→α Q( ˆd) =

α0P0,j( ˆd(0))
M−1
j=0
= (α0P0,j(d(0)))M−1
j=0
=
→α Q(d).
If α0 = 0 then one can take an arbitrary ˆd ∈D∗:
→α Q( ˆd) =
→α Q(d) = 0.
We proceed further by induction. Suppose the statement of the lemma is
valid for all
→α satisfying condition αl = αl+1 = . . . = αM−1 = 0 for some

Controlled Random Walk: Conjecture and Counter-Example
49
1 ≤l ≤M −1 and ˆd(l), ˆd(l + 1), . . . , ˆd(M −1) can be arbitrary in A∗. Consider
a vector
→α with αl+1 = αl+2 = . . . = αM−1 = 0 and let d ∈D be such that
M−1
i=0 αiRi(d) = 0.
Since, for each ﬁxed ˜d ∈D, the product
→α Q( ˜d) does not depend on the
rows Qj,·( ˜d) with j ≥l + 1, the values ˜d(j) with such j do not aﬀect the value
of
→α Q( ˜d). (See (13)) Hence one can put ˆd(l + 1), ˆd(l + 1), . . . , ˆd(M −1) ∈A∗
arbitrarily.
In case αl = 0 the statement of the lemma holds by the induction supposition.
Below, αl > 0 and hence Rl(d) = 0 and d(l) ∈A∗. Therefore, we put ˆd(l) := d(l).
Moreover, in the current situation Rj(d) = 0 for all j ∈{0, 1, . . . , l −1} with
positive values of Pl,j(d(l)): see (12). Thus, for the row vector
→
α′:= (α0 + αlPl,0(d(l)), . . . , αl−1 + αlPl,l−1(d(l)), 0, . . . , 0) ∈RM
we have M−1
i=0 α′
iRi(d) = 0, and we will use the induction supposition for
→
α′ to
complete the proof. Note also that the vector
→
α′ is substochastic.
For any ˜d ∈D, according to (13), the elements of the row vector
→α Q( ˜d) are
as follows:
l

j=0
αjQj,0( ˜d) =
l−1

j=0
αjQj,0( ˜d) + αl
l−1

i=0
[Pl,i( ˜d(l))Qi,0( ˜d)]
=
l−1

j=0
{αj + αlPl,j( ˜d(l))}Qj,0( ˜d);
l

j=0
αlQj,1( ˜d) =
l−1

j=0
{αj + αlPl,j( ˜d(l))}Qj,1( ˜d);
. . .
. . .
. . .
l

j=0
αlQj,l−1( ˜d) =
l−1

j=0
{αj + αlPl,j( ˜d(l))}Qj,l−1( ˜d);
l

j=0
αlQj,l( ˜d) =
l−1

j=0
{αj + αlPl,j( ˜d(l))}Qj,l( ˜d) + αlPl,l( ˜d(l));
. . .
. . .
. . .
l

j=0
αlQj,M−1( ˜d) =
l−1

j=0
{αj + αlPl,j( ˜d(l))}Qj,M−1( ˜d) + αlPl,M−1( ˜d(l)).
To put it diﬀerently, for the mapping d we have
→α Q(d) =
→
α′ Q(d) + (0, . . . , 0, αlPl,l(d(l)), . . . , αlPl,M−1(d(l)).
According to the induction supposition, there is ˆd ∈D∗(with ﬁxed ˆd(l) = d(l) ∈
A∗and arbitrary values ˆd(l + 1), . . . , ˆd(M −1) ∈A∗which do not appear in the
provided expressions) such that

50
A. B. Piunovskiy
→
α′ Q(d) =
→
α′ Q( ˆd).
Therefore,
→α Q(d) =
→α Q( ˆd).
Besides, for i = 0, 1, . . . , l −1, if α′
i > 0 then ˆd(i) = d(i); hence, if αi > 0 then
ˆd(i) = d(i) for all i = 0, 1, . . . , l −1, l.
The proof is completed.
2
Proof of Theorem 1. Let Tk := maxs∈S W k(s) and tk := mins∈S W k(s) and let us
show that the sequence {Tk}∞
k=0 decreases and the sequence {tk}∞
k=0 increases.
Since, for every d ∈D, R(d) ≤0 and the matrix Q(d) is stochastic (see
Lemma 2(a)),
{R(d) + Q(d)W k}s ≤Tk
for all s ∈S.
Hence, Tk+1 ≤Tk.
For every d ∈D∗, R(d) = 0, so
{R(d) + Q(d)W k}s ≥tk
for all s ∈S.
Hence, W k+1(s) ≥tk and tk+1 ≥tk.
Therefore, there exist limits T ∞:= limk→∞Tk ≥t∞:= limk→∞tk which
are ﬁnite because of Lemma 1. Later, under the imposed condition, it will be
clear that these limits coincide.
Let
˜
Δ := −
max
d∈D,s∈S: Rs(d)<0 Rs(d)
and q :=
min
d∈D, s,l∈S: Qs,l(d)>0 Qs,l(d).
Since the sets D and S are ﬁnite, ˜
Δ > 0, q ∈(0, 1], and we introduce an arbitrary
Δ ∈(0, ˜
Δ) and
ε := Δ
2

q
2 −q
J
> 0.
Let K be such that
TK < T ∞+ ε
(hence TK < T ∞+ Δ
2 ).
Since the sequence {Tk}∞
k=0 decreases to T ∞, we conclude that
Tk < T ∞+ ε < T ∞+ Δ
2
for all k ≥K.
(20)
We intend to show that
∀s ∈S
W K(s) > T ∞−Δ
2 .
(21)
To do this, ﬁx ˜s ∈S such that W K+J(˜s) = TK+J and let us prove by induction
the following statement

Controlled Random Walk: Conjecture and Counter-Example
51
A. For each j = 0, 1, . . . , J, there exist mappings ˆd1, ˆd2, . . . , ˆdj ∈D∗
such that, for the ˜s-th row of the stochastic matrix Q( ˆd1)Q( ˆd2) . . . Q( ˆdj)
denoted below as
→
γj, if γj
s > 0, then W K+J−j(s) > T ∞−

2−q
q
j
ε.
Let j = 0. Then no mappings ˆd ∈D∗are considered,
→
γ0= (δ˜s,s)M−1
s=0
is the
basic row vector with element 1 on the ˜s-th place, and W K+J(˜s) > T ∞−ε
because
– W K+J(˜s) = TK+J by the deﬁnition of ˜s;
– and TK+J ≥T ∞because the sequence {Tk}∞
k=0 decreases to T ∞.
Suppose the statement A is valid for some j ∈{0, 1, . . . , J −1} and the
mappings ˆd1, ˆd2, . . . , ˆdj ∈D∗are ﬁxed;
→
γj=
→
γ0 Q( ˆd1)Q( ˆd2) . . . Q( ˆdj),
where
→
γ0= (δ˜s,s)M−1
s=0
as before. Let dj+1 ∈D be such that
W K+J−j = R(dj+1) + Q(dj+1)W K+J−(j+1).
For s ∈S such that γj
s > 0 we have inequality
W K+J−j(s) > T ∞−
2 −q
q
j
ε
(22)
according to the inductive supposition. For such value of s, suppose Rs(dj+1) <
0. Then
W K+J−j(s) ≤−Δ + TK+J−(j+1) < −Δ + T ∞+ Δ
2 = T ∞−Δ
2
because of (20): remember, J −(j + 1) ≥0. Further,
W K+J−j(s) < T ∞−
2 −q
q
J
ε ≤T ∞−
2 −q
q
j
ε
which contradicts (22). Therefore, if γj
s
>
0 then Rs(dj+1)
=
0 and
M−1
s=0 γj
sRs(dj+1) = 0.
Using Lemma 4 with the (sub)stochastic vector
→
γj, we conclude that the row
vector
→
γj Q(dj+1) coincides with the row vector
→
γj Q( ˆdj+1) for some ˆdj+1 ∈D∗.
Now the ˜s-th row of the matrix Q( ˆd1)Q( ˆd2) . . . Q( ˆdj+1) equals
−−→
γj+1 =
→
γj Q( ˆdj+1) =
→
γj Q(dj+1).
Suppose γj+1
l
> 0. Then there is at least one index s such that γj
s > 0 and
Qs,l(dj+1) > 0; hence Qs,l(dj+1) ≥q. As was proved above, Rs(dj+1) = 0.
Consider equality
W K+J−j(s) = Rs(dj+1) + Qs,·(dj+1)W K+J−(j+1) = Qs,·(dj+1)W K+J−(j+1).

52
A. B. Piunovskiy
Since TK+J−(j+1) < T ∞+ ε ≤T ∞+

2−q
q
j
ε (see (20)),
W K+J−j(s) ≤Qs,l(dj+1)W K+J−(j+1)(l)+(1−Qs,l(dj+1))

T ∞+
2 −q
q
j
ε

.
In case
W K+J−(j+1)(l) ≤T ∞−
2 −q
q
j+1
ε
we have
W K+J−j(s) ≤Qs,l(dj+1)

T ∞−
2 −q
q
j+1
ε

+(1 −Qs,l(dj+1))

T ∞+
2 −q
q
j
ε

= T ∞+
2 −q
q
j
ε −Qs,l(dj+1)
2 −q
q
j
ε +
2 −q
q
j+1
ε

≤T ∞+
2 −q
q
j
ε −q
2 −q
q
j
ε

1 + 2 −q
q

(because Qs,l(dj+1) ≥q)
= T ∞+
2 −q
q
j
ε[1 −q −(2 −q)] = T ∞−
2 −q
q
j
ε
which contradicts (22). Therefore
W K+J−(j+1)(l) > T ∞−
2 −q
q
j+1
ε,
and the statement A is proved for j + 1.
When j = J, the vector
→
γJ contains no zeroes; hence
W K(s) > T ∞−
2 −q
q
J
ε = T ∞−Δ
2
for all s ∈S
by the deﬁnition of ε. Inequality (21) is proved.
Note also that inequality (21) implies that tK > T ∞−Δ
2 and, since the
sequence {tk}∞
k=0 increases to t∞, t∞≥T ∞−Δ
2 . Noting that Δ ∈(0, ˜
Δ) was
arbitrary, we conclude that t∞= T ∞.
Now, if d /∈D∗then, for some s ∈S,
Rs(d) + Qs,·(d)W K < −Δ + T ∞+ Δ
2 = T ∞−Δ
2
according to (20). On the other hand, for each d ∈D∗, for all s ∈S,
Qs,·(d)W K > T ∞−Δ
2 because of (21), meaning that

Controlled Random Walk: Conjecture and Counter-Example
53
W K+1 = max
d∈D{R(d) + Q(d)W K} = R(dK+1
∗
) + Q(dK+1
∗
)W K = Q(dK+1
∗
)W K
with dK+1
∗
∈D∗.
The following statement can be easily proved by induction.
B. For each k ≥K
W k(s) > T ∞−Δ
2
for all s ∈S
and the maximum in equation
W k+1 = max
d∈D{R(d) + Q(d)W k}
(23)
is provided necessarily by dk+1
∗
∈D∗.
As was shown above, this statement is valid for k = K.
Suppose it is valid for some k −1 ≥K and consider the case of k. Firstly,
the vector
W k = Q(dk
∗)W k−1
is such that (for all s ∈S) W k(s) > T ∞−Δ
2 because of the inductive supposition:
W k−1(s) > T ∞−Δ
2 for all s ∈S. Secondly, like previously, if d /∈D∗, then, for
some s ∈S,
Rs(d) + Qs,·(d)W k < −Δ + T ∞+ Δ
2 = T ∞−Δ
2 .
(24)
(This inequality holds for each s ∈S with d(s) /∈A∗=⇒Rs(d) < 0 =⇒Rs(d) <
−Δ.) And, for each d ∈D∗, for all s ∈S,
Rs(d) + Qs,·(d)W k = Qs,·(d)W k > T ∞−Δ
2 .
(25)
Thus the maximum in (23) at k is provided necessarily by dk+1
∗
∈D∗.
The proof is completed.
2
Proof of Corollary 1. It is suﬃcient to show that the matrix Q(d∗) is ergodic.
The mapping d∗satisﬁes the condition of Lemma 2(b): Ps,s(d∗(s)) = Ps,s(a∗) =
pM(a∗) > 0 for all s ∈S. Hence, for all s, l ∈S, if Ps,l(d∗(i)) = Ps,l(a∗) > 0 then
Qs,l(d∗) > 0. Therefore, the matrix Q(d∗) is ergodic because the matrix P(a∗)
is ergodic. The proof is completed.
2
Proof of Lemma 3. When j = 1, Items (i) and (iii) are valid by the preliminary
calculations, and Item (ii) comes from the following:
2ε(j −1) + (1 + ε)h −2 −εV (2j −1) = (1 + ε)h −2 −εh = h −2 > 0.
Suppose statements (i), (ii) and (iii) hold for some j ≥1 and consider j + 1.

54
A. B. Piunovskiy
(i) For i = 2(j+1), using the induction supposition, we estimate the diﬀerence
2 + V (2j) −[h + (1 −ε)V (2j) + εV (2j −1)]
> 2 + 2j + h −h −(1 −ε)(2j + h) −ε2ε(j −1) + (1 + ε)h −2
ε
= 4 + 2ε −2h = 2[2 + ε −h] > 0.
The inequality is according to statement (ii) at j.
Thus, V (2(j + 1)) = 2(j + 1) + h, and the maximum in (3) is provided only
by a1.
(ii)
V (2j + 1) = (1 −ε)V (2j −1) + (1 + ε)h + 2ε(j −1)
< (1 −ε)2ε(j −1) + (1 + ε)h −2
ε
+ (1 + ε)h + 2ε(j −1)
= 2εj + (1 + ε)h −2
ε
,
so that statement (ii) is valid for j + 1.
(iii) For i = 2(j + 1) + 1, using the induction supposition, we estimate the
diﬀerence
h + (1 −ε)V (2j + 1) + εV (2j) −[2 + V (2j + 1)]
= h −εV (2j + 1) + ε[2j + h] −2
> h(1 + ε) −[2εj + h −2 + εh] + 2εj −2 = 0,
where the inequality is by the proved above Item (ii) for j + 1. Recall also
that V (2j) = 2j + h. Therefore,
V (2(j + 1) + 1) = h + (1 −ε)V (2j + 1) + ε[2j + h),
and we see that statement (iii) is valid for j + 1 and the maximum in (3) is
provided only by a2.
2
Proof of Theorem 2. Introduce function
˜W(i) := V β(i) −
R∗
1 −β ,
i ∈X,
which obviously satisﬁes equation
˜W(i) = −R∗
1 −β
for i = −M, −M + 1, . . . , −1;
˜W(i) = max
a∈A
	
(Ra −R∗) + β
M

m=1
˜W(i −m)pm(a)

for i ≥0.
Like previously, (see (7)), we replace the argument i with k = 0, 1, 2, . . . and
s ∈S = {0, 1, . . . , M −1}, denote W k(s) := ˜W(i) and ﬁnish with equations

Controlled Random Walk: Conjecture and Counter-Example
55
like (8). The only diﬀerence is that pm(a) is replaced by βpm(a), and the initial
condition is
W 0(s) = −R∗
1 −β
for s ∈S.
We obtain iterations (cf (11))
W 0(s) = −R∗
1 −β ,
s ∈S;
W k+1 = max
d∈D{Rβ(d) + Qβ(d)W k}, k = 0, 1, . . . ,
(26)
where
Rβ
0(d) = Rd(0) −R∗;
Rβ
l+1(d) = Rd(l+1) −R∗+ β
l

j=0
Pl+1,j(d(l + 1))Rβ
j (d),
l = 0, . . . , M −2,
and the matrix Qβ is given by (13) with P being replaced by βP. Similarly to
(11), the maximum in the expression
U ◦W := max
d∈D{Rβ(d) + Qβ(d)W}
is provided by some ˆd ∈D for each W ∈RM: the values ˆd(s) can be calculated
successively for s = 0, 1, . . . , M −1. Note also that Rβ(d) ≤0 for all d ∈D.
The matrix Qβ(d) is (uniformly with respect to d) strictly substochastic, i.e.,
0 <
M−1

j=0
Qβ
l,,j(d) ≤β < 1
for all l ∈S :
the proof is identical to the proof of Lemma 2(a). Therefore, the mapping U is
a contraction in the space RM with the uniform norm: see the proof of Propo-
sition 6.2.4 in [6]. The maximum maxd∈D Rβ(d) = 0 (the zero vector in RM) is
provided by those and only those d ∈D, for which d(s) ∈A∗for all s ∈S. There-
fore, the unique ﬁxed point of the operator U is W ∞= 0 and limk→∞W k = 0.
Below,
D∗:= {d ∈D : d(s) ∈A∗for all s ∈S},
and
U ◦W ∞= Rβ(d) + Qβ(d)W ∞= Rβ(d) = W ∞= 0
if and only if d ∈D∗. The theorem will be proved if we show that, for some
K < ∞, the maximum in (26) at all k ≥K is only provided by d ∈D∗.
Denote
Δ :=
min
d∈D\D∗
min
s∈S:Rβ
s (d)<0
{−Rβ
s (d)}.
The spaces D and S are ﬁnite, and Δ > 0.

56
A. B. Piunovskiy
If Δ = +∞then D∗= D and the proof is ﬁnished. (One can put K = 0.)
Suppose Δ < +∞. Then, for each d ∈D \ D∗̸= ∅, for each s ∈S such that
Rβ
s (d) < 0,
Rβ
s (d) ≤−Δ.
Let us choose 0 < ε < Δ
2 and ﬁx K ≥0 such that
max
j∈S |W k(j)| < ε
for all k ≥K.
Now, for each k ≥K, if d /∈D∗provides the maximum in (26), then there is
s ∈S such that Rβ
s (d) < 0, and, for each such s,
W k+1(s) = Rβ
s (d) +
M−1

j=0
Qβ
s,j(d)W k(j) ≤Rβ
s (d) + ε ≤−Δ + ε.
(Recall that Qβ(d) is a substochastic matrix.) Since W k+1(s) > −ε, we obtain
the strict inequalities
W k+1(s) < W k+1(s) + ε −Δ + ε < W k+1(s).
The obtained contradiction shows that, for all k ≥K, only the decisions from
D∗provide the maximum in (26).
2
References
1. Hernandez-Lerma, O., Lasserre, J.B.: Further Topics on Discrete-Time Markov Con-
trol Processes. Springer, New York (1999)
2. Kallenberg, L.C.M.: Markov Decision Processes. Lecture Notes. University of Lei-
den, The Netherlands (2010)
3. Lewis, M.E., Paul, A.: Uniform turnpike theorems for ﬁnite Markov decision pro-
cesses. Math. Oper. Res. 44, 1145–1160 (2019)
4. Piunovskiy, A.: Optimal Control of Random Sequences in Problems with Con-
straints. Kluwer, Dordrecht (1997)
5. Piunovskiy, A.: Turnpikes and random walk. arXiv:2102.09341 (2021)
6. Puterman, M.: Markov Decision Processes. Wiley, New York - Chichester - Brisbane
- Toronto - Singapore (1994)
7. Shapiro, J.F.: Turnpike planning horizons for a Markovian decision model. Manag.
Sci. 14, 292–300 (1968)

Optimal Stopping Problems for a Family
of Continuous-Time Markov Processes
H´ector Jasso-Fuentes1(B), Jose-Luis Menaldi2, and Fidel V´asquez-Rojas1
1 Department of Mathematics, CINVESTAV-IPN, Apartado Postal 14-740,
07000 Mexico City, Mexico
hjasso@math.cinvestav.mx, fvasquez@math.cinvestav.mx
2 Department of Mathematics, Wayne State University, Detroit, MI 48202, USA
menaldi@wayne.edu
Abstract. In this chapter we study the well-known optimal stopping
problem applied to a general family of continuous-time Markov processes.
The approach to follow is merely analytic and it is based on the charac-
terization of stopping problems through the study of a certain variational
inequality; namely one solution of this inequality will coincide with the
optimal value of the stopping problem. In addition, by means of this
characterization, it is possible to ﬁnd the so-named continuation region,
and as a byproduct obtaining the optimal stopping time. The most of the
material is based on the semigroup theory, inﬁnitesimal generators and
resolvents. The chapter is a complete version of the former presentation
without detailed proofs in [25].
Keywords: Optimal stopping times · Continuous-time Markov
processes · Variational inequalities
AMS(2020) Subject Classiﬁcation: Primary 60G40 · Secondary
60J25 · 49J40
1
Introduction
Optimal stopping problems are perhaps one of the most interesting and studied
problems in the theory of stochastic processes. Successful methods have been
developed during decades to show the existence and several characterizations of
optimal stopping times. The most studied methods to address these problems
are deﬁnitely the theory of Snell envelopes and backward-reﬂected stochastic
diﬀerential equations—see [7,14,15,17,18], but on the other hand, there is also
another useful method that tackles stopping problems from a merely analytical
viewpoint—see [3,5,22,23,29,31,32], among others.
One of the main diﬀerences of the second method with respect to the former
is the assumption of a Markovian structure of the process, so in principle it
could seem more restrictive. However, its analytical nature allows the use of
sophisticated tools of functional analysis, set topology, or even more, the use
of numerical approximations of the original (and theoretic) problem—see for
instance [16].
c
⃝The Author(s), under exclusive license to Springer Nature Switzerland AG 2021
A. Piunovskiy and Y. Zhang (Eds.): Modern Trends in Controlled Stochastic Processes,
ECC 41, pp. 57–86, 2021. https://doi.org/10.1007/978-3-030-76928-4_4

58
H. Jasso-Fuentes et al.
In this work we shall apply the analytical approach we have already men-
tioned, and extend several works on this line. But before to specify the details,
we can depart to mentioning some pioneer works on this analytical direction,
such as [3–5]. All these works were focused on the study of both optimal stop-
ping and impulsive control problems associated to non-degenerated diﬀusion
processes. Based on these works, several authors followed the same line (with
both/either theoretical and/or applied viewpoints) that have produced during
decades a spread of knowledge on this ﬁeld.
Other former but nor less important works were developed by Robin [31,32]
and later by Stettner [34] that also applied analytical tools for solving optimal
stopping problems on general continuous-time Markov-Feller processes. Within
the analysis of the aforementioned papers, we highlight the assumptions on the
state space of either type: locally compact or compact.
Following with the description of the former literature, we can quote
Menaldi’s works [22–24] as well as the one by Menaldi and Sritharan [28], in
which the authors analyzed two great families of Markov-Feller process: (1)
degenerate stochastic diﬀerential equations with either jumps or without jumps,
and (2) Navier-Stokes equations; in all these mentioned works the authors take
advantage to the particularities of the model in order to explore the regularity of
the optimal values. As for the discrete-time models there is a handful of works
such as [6,19,20,30],
In this work we use the same line as Robin’s works [31,32] but we drop the
local-compactness assumption of the state space. It is important to say that our
model is based on the existence of a Markov process that lives on a ﬁxed proba-
bility space, whereas in the aforementioned references, this space is constructed
through the canonical space. This implies that both works are not a special case
of each other. Actually, we are somehow inspired from the ideas scattered in
reference [25]. One diﬀerence of this reference with respect to this proposal, is
the nature of the dynamical system and also the general details, since in this
work we detail point by point all the arguments of the proofs.
The content of this paper is organized as follows: In Sect. 2, we describe the
class of Markov processes we are interested in, and its associated semigroup.
Due to a minimal set of assumptions imposed to this process, we will be forced
to introduce a seminorm that measures the maximum value of functions along
the trajectories (rather than over the whole space, that is the usual case of the
supremum norm). This seminorm, produces some properties of the aforemen-
tioned semigroup such as a kind of Feller version that is measured through this
seminorm. By the end of the section, we will deﬁne the corresponding inﬁnitesi-
mal generator and the resolvent operators that both together play a substantial
role within the analysis of the optimal stopping problems. In Sect. 3, we will
turn our attention to the study of the so-called penalized problem, whose main
characteristic is the associated parametric family of functional equations that
will be analyzed in this part; in particular, the existence and regularity of these
functional equations are ensured. Later we will consider a certain variational
inequality. This inequality satisﬁes the following two nice properties: (i) one of

Optimal Stopping Problems
59
its subsolutions becomes the limit of the (unique) solutions of the aforemen-
tioned family of parametric equations and (ii) the maximal sub-solution of this
inequality is just the minimal cost of our stopping problem; this last property
will be proved later in Sect. 4, in which we will also provide a characterization
of the optimal stopping time as a hitting time associated to a given set so-called
continuation region or contact set.
2
A Family of Markov Processes
In this section we introduce the dynamics of our stopping problem. This dynam-
ics consists of a continuous-time Markov process that in turn deﬁnes a family
of operators so-called the semigroup of the process. With these elements it is
possible to introduce both an inﬁnitesimal generator and a resolvent operator
related to that semigroup. These latter operators will play a substantial role for
the analysis of the optimal stopping problem. The way to construct the above
mentioned mathematical objects is not straightforward due to the generality of
the state space.
2.1
Preliminaries
Let E := (Ω, F, {Ft}t≥0, P) be a ﬁxed ﬁltered probability space, satisfying the
usual conditions (i.e., the ﬁltration {Ft}t≥0 is right-continuous and F0 contains
all subsets of the P-null sets). Besides, let us consider an open subset O of a
Banach space with norm |·|. Throughout this work we will be working with
an abstract homogeneous O-valued stochastic process {y(t, x)}t≥0, with initial
condition x ∈O (i.e. P(y(0, x) = x) = 1), deﬁned on E.
A ﬁrst consequence of the above mathematical objects is the deﬁnition of the
space B(O) consisting of all measurable functions h : O →R such that
h(y(t, x)) ∈L1(Ω, R),
∀t ≥0, x ∈O;
(1)
we note that every bounded measurable function belongs to this space.
With these preliminary elements, we can establish the following assumptions
for {y(t, x)}t≥0:
Assumption 1. The mapping
(t, x) →P(y(t, x) ∈B)
is measurable ∀B ∈B(O),
(2)
where B(O) denotes the σ-algebra generated by O. In addition:
(a) There exist constants α0 > 0, and k ≥1, as well as a measurable function
w : O →[1, +∞) satisfying lim|x|→∞w(x) = ∞, such that all together satisfy
the following:
(a.1)
E

sup
s≥0

e−α0sw(y(s, x))
 
≤kw(x),
∀x ∈O,
and
(3)

60
H. Jasso-Fuentes et al.
(a.2)
E

e−α0sw(y(s, x))

≤w(x),
∀x ∈O
and
∀s ≥0,
(4)
where E[·] is the expectation associated to P.
(b) The Markov property:
P(y(t + s, x) ∈B|Fs) = P(y(t, y(s, x)) ∈B),
a.s.
∀t, s ≥0, B ∈B(O).
(5)
The right-hand side of the above equality is understood as the evaluation of
the mapping z →P(y(t, z) ∈B) at z = y(s, x).
(c) The following relation holds true for all s, t ≥0, x ∈O
E[h(y(t, y(s, x)))] = E[h(y(s, y(t, x)))],
a.s.
∀h ∈B(O),
(6)
where the left-hand side means the evaluation of z →E[h(y(t, z))] at z =
y(s, x), and the right-hand side is the evaluation of f →E[f(y(t, x))] at
f = h(y(s, ·)).
(d) For each x ∈O, t →y(t, x) has not discontinuities of second kind. Moreover,
for all x ∈O and ε > 0 there is δ > 0 such that if 0 ≤t ≤δ then
P( sup
0≤s≤1
ε
|y(t + s, x) −y(s, x)| ≥ε) < ε.
(7)
Remark 1.(a) The measurability assumption (2), is a well known fact, as it is
established in Dellacherie and Meyer [12], Ethier and Kurtz [13], Rogers and
Williams [33]. A clear consequence of the above property is that (t, x) →
E[h(y(t, x))] is measurable for every simple function h : O →R. Thus, a
standard convergence procedure to each h ∈B(O) from sequences of simple
functions, yields that
(t, x) →E[h(y(t, x))]
is measurable ∀h ∈B(O).
(8)
In particular, equation (6) is well-deﬁned.
(b) It is worth to say that properties (3), (4), and (7) are common in special
cases of Markov processes, such as those that come from solutions of both
ordinary and partial stochastic diﬀerential equations—see Bensoussan and
Lions[3,5], Bensoussan [4], Menaldi [22–24], Menaldi and Sritharan [26–28].
(c) It is not diﬃcult to prove that the Markov property (5) is equivalent to this
one:
E[h(y(t, y(s, x)))] = E[h(y(t + s, x))|Fs]
∀t ≥s ≥0, x ∈O,
∀h ∈B(O).
(9)
(d) Condition (6) is a kind of uniqueness on the paths. This type of relation is
satisﬁed for a big family of Markov processes {y(t, x)}t≥0, for instance the
well-known family of Ito’s process (with or without jumps, of ﬁnite or inﬁnite
dimension)—see Bensoussan and Lions [3,5], Bensoussan [4], Menaldi [22–
24], Menaldi and Sritharan [26–28], Da Prato [10,11], among others.

Optimal Stopping Problems
61
(e) By writing the set of right-discontinuities of {y(t, x)}t≥0 as
∪ε>0 ∩δ>0 { sup
0≤t≤δ
|y(t + s, x) −y(s, x)| ≥ε},
and with the aid of (7), it is not diﬃcult to show that {y(t, x)}t≥0 has
right-continuous paths. Also, since the process has not second order discon-
tinuities, we can conclude that it is c`adl`ag.
We will also think over the space of functions h ∈B(O) with the property of
sup
x∈O
|h(x)|
w(x) < ∞.
(10)
This space is denoted by Bw(O) that will be endowed with the norm
∥h∥w := sup
x∈O
|h(x)|
w(x) .
(11)
It is common to say that every function in Bw(O) satisﬁes a ﬁnite w-growth. In
addition, it is not diﬃcult to show that (Bw(O), ∥·∥w) is a Banach space.
Finally, using the (ﬁxed) constant α0 > 0 appearing in (3), we introduce the
family of seminorms {p(·, x)}x∈O on B(O) by
p(h, x) = E

sup
s≥0

e−α0s|h(y(s, x))|

,
∀x ∈O.
(12)
Each element of the above family is in fact a seminorm because p(h, x) ≥0,
p(ah, x) = |a|p(h, x) for all a ∈R and p(h + g, x) ≤p(h, x) + p(g, x), but if
p(h, x) = 0 then {h(y(s, x))}s≥0 is indistinguishable of the constant process
equal to zero. Using this seminorm, we shall denote by Bp(O) the subspace of
B(O) consisting of functions h satisfying
p(h, x) < ∞,
∀x ∈O.
(13)
Note that the deﬁnition of this later space, together with the deﬁnition of
Bw(O) in (11), and the assumption in (3), all together yield that Bw(O) ⊆
Bp(O) ⊆B(O).
2.2
The Associated Semigroup
For α ≥α0, with α0 as in (3), we deﬁne the family of operators {Φα(t)}t≥0 on
Bp(O) by
Φα(t)h(x) = E[e−αth(y(t, x))],
∀x ∈O, h ∈Bp(O), t ≥0.
(14)
In view of Φα(t) is essentially an integral (with respect to the probability measure
P), we have that it is monotone, that is, h ≥0 implies Φα(t)h ≥0 for any t ≥0.
Besides, from the deﬁnition of Φα(t) in (14), it is clear that Φα(0)h = h. This

62
H. Jasso-Fuentes et al.
family of operators also satisﬁes the semigroup property Φα(t)Φα(s) = Φα(t + s)
that follows directly from the Markov property, namely for h ∈Bp(O),
Φα(t)Φα(s)h(x) = E[e−αtΦα(s)h(y(t, x))] = E[e−αtE[e−αsh(y(t, y(s, x)))]]
= E[e−α(t+s)E[h(y(t + s, x))|Fs]] = E[e−α(t+s)h(y(t + s, x))]
= Φα(t + s)h(x).
If h ∈Bw(O) then, using the inequality (4) as well as the norm in (11), we get
the following
|Φα(t)h(x)| ≤E[e−αt|h(y(t, x))|] = E

e−αt |h(y(t, x))|
w(y(t, x)) w(y(t, x))

≤∥h∥w E[e−α0tw(y(t, x))] ≤∥h∥w w(x),
∀x ∈O.
Hence,
∥Φα(t)h∥w ≤∥h∥w .
(15)
The semigroup property naturally arises when the operators Φα(t) are deﬁned
as an integral with respect to a given transition probability kernel q(x, t, ·) =
P[y(t, x) ∈·] that in turn satisﬁes the well-known Chapman-Kolmogorov equa-
tions. This last type of equations is very common in speciﬁc models, such as
continuous-time Markov chains, L´evy Processes, partial stochastic diﬀerential
equations, to mention a few. (See [1,2,10,11], among others). The family of the
operators Φα deﬁned in (14) will be called throughout this work as the associated
semigroup of the Markov process {y(t, x)}t≥0.
As we will see in the following result, the semigroup Φα satisﬁes the contrac-
tion property with respect to the seminorm p(·, x). The details are as follows.
Proposition 1. For each h ∈Bp(O), t, s ≥0 and x ∈O we have that
p(Φα(t)h, x) ≤p(h, x).
Proof. Fixed h ∈Bp(O), t, s ≥0 and x ∈O, we have
p(Φ(t)h, x) = E

sup
s≥0
{e−α0s|E[e−αth(y(t, y(s, x)))]|}

= E

sup
s≥0
{e−α0s|E[e−αth(y(s, y(t, x)))]|}

(by (6))
≤E

E[sup
s≥0
{e−α0se−αt|h(y(s, y(t, x)))|}]

.
On the other hand, it is not diﬃcult to prove that the Markov property in (1)
implies the Markov property (see e.g. [35, Section 5.2.2.]) in the following sense
E[sup
s≥0
{e−α0se−αt|h(y(s, y(t, x)))|}] = E[sup
s≥0
{e−α0se−αt|h(y(s + t, x))|}
Ft].

Optimal Stopping Problems
63
Then we can conclude that
p(Φ(t)h, x) ≤E

E[sup
s≥0
{e−α0se−αt|h(y(s + t, x))|}
Ft]

= E[sup
s≥0
{e−α0se−αt|h(y(s + t, x))|}]
≤E[sup
s≥0
{e−α0s|h(y(s, x))|}] = p(h, x).
⊓⊔
Remark 2.(a) The assumption in (2) together with (15), give us that Φα(t)
leaves invariant the space Bw(O); actually, our family of operators t →Φα(t)
satisﬁes the properties of the so-called monotone semigroup of contractions
deﬁned on Bw(O).
(b) Even more, (2) and Proposition 1 also give the invariance of the semigroup
Φα over the set Bp(O).
Continuity of the Semigroup. In many situations, the above semigroup sat-
isﬁes the so-called strong continuity (see [1,2,8,10] among others)
∥Φα(t)h −h∥→0,
as t ↓0,
(16)
applied to a suitable space of functions h—for example, the set of continuous
functions that vanish at inﬁnity. The above case is very common when the dimen-
sion of O is either ﬁnite-dimensional or locally compact. However, there exist sit-
uations when O does not hold the previous two properties—for example, assume
that O is a Hilbert space as in references [26–28]), so convergence (16) is no
longer valid. However, it is possible to obtain a sort of continuity type in the
next weaker sense (see, for instance B¨ottcher et al. [8], Menaldi [25], or Menaldi
and Sritharan [28]).
Φα(t)h(x) −h(x) →0,
as t ↓0
∀x ∈O,
(17)
where h is Borel measurable. One of the disadvantages of this later continuity is
that it produces a lack of regularity of some sophisticated mathematical objects
(i.e., inﬁnitesimal generator, the resolvent operator, among others), whose deﬁ-
nitions depend strongly from the convergence in (17).
Since our hypotheses of the state space O are not restricted to the cases of
ﬁnite dimension nor local compactness, it is expected to not obtain convergence
of type (16), even when we could use the norm ∥· ∥w. To avoid this drawback,
we shall seek an intermediate convergence, weaker than (16) but a little stronger
than (17) so that we are in conditions to achieve regularity properties for the
inﬁnitesimal generator and on the resolvent operator. The key point is to deﬁne
a suitable functions set whose elements are continuous in certain sense but at
the same time, the semigroup applied to this set can be continuous in seminorm
(see Deﬁnition 2 below).
Let us now deﬁne the concept of convergence in seminorm that is crucial to
deﬁne continuity in seminorm sense.

64
H. Jasso-Fuentes et al.
Deﬁnition 1. We say that a sequence hn in Bp(O) converges in seminorm to
some h in Bp(O) as n →∞, denoted by s −limn→∞hn = h, if
lim
n→∞p(hn −h, x) = 0, ∀x ∈O.
(18)
Moreover, if the elements of the above sequence are in Bw(O) then we say that hn
converges boundedly in seminorm to h as n →∞, denoted by bs −limn→∞hn =
h, provided the following conditions are satisﬁed
	supn∈N ∥hn∥w < ∞;
s −limn→∞hn = h.
(19)
Note that for each x ∈O, t ≥0, and h ∈Bw(O), a simple use of the bound
(3) yields that
p(h, x) = E[sup
s≥0
e−α0s|h(y(s, x))|]
≤E[sup
s≥0
e−α0s ∥h∥w w(y(s, x))] ≤k ∥h∥w w(x) < ∞.
(20)
The above relation means that convergence in norm implies convergence in semi-
norm which, at the same time, implies pointwise convergence.
Deﬁnition 2. We deﬁne the subspace Cp(O) of Bp(O) that is conformed by the
functions h such that:
(a) s −limt↓0 Φα(t)h = h,
(b) for each x ∈O we have that {h(y(s, x))}s≥0 is a c`adl`ag process.
We also denote the intersection Cp(O) ∩Bw(O) by Cw
p (O).
The next proposition shows further properties of the sets Cp(O) and Cw
p (O).
Proposition 2. Under Assumption 1, we have
(a) The sets Cp(O) and Cw
p (O) are non-empty.
(b) For every t ≥0:
(b.1) Φα(t)h ∈Cp(O) when h ∈Cp(O),
(b.2) Φα(t)h ∈Cw
p (O) when h ∈Cw
p (O).
Proof. (a) Let Cu(O) be the space of bounded uniformly continuous functions
and take h ∈Cu(O). Note that
p(Φα(t)h −h, x) ≤p(Φα(t)h −e−αth, x) + p(e−αth −h, x)
≤p(Φα(t)h −e−αth, x) + (e−αt −1)p(h, x),
where (e−αt −1) →0 when t ↓0. So, we aim to show
p(Φα(t)h −e−αth, x) →0

Optimal Stopping Problems
65
when t ↓0. Namely, for any t ≥0 and x ∈O, we have
p(Φα(t)h −e−αth, x) ≤E

sup
s≥0
e−α0s |h(y(t + s, x)) −h(y(s, x))|

≤E

sup
0≤s≤T
e−α0s |h(y(t + s, x)) −h(y(s, x))|

+ E

sup
s≥T
e−α0s |h(y(t + s, x)) −h(y(s, x))|

(21)
for any T > 0. In order to bound this expression, let us take ε > 0 and choose
0 < δ1 ≤ε such that |x −¯x| < δ1 implies |h(x) −h(¯x)| < ε. In turn, in virtue of
(7), let us choose 0 < δ0 ≤δ1 such that for all 0 ≤t ≤δ0 we have
P( sup
0≤s≤1
δ1
|y(t + s, x) −y(s, x)| ≥δ1) < δ1.
Letting T =
1
δ0 we get
E

sup
0≤s≤1
δ0
e−α0s e−αth(y(t + s, x)) −h(y(s, x))
 
≤E

sup
0≤s≤1
δ0
e−α0s e−αth(y(t + s, x)) −h(y(s, x))

× 1sup0≤s≤1
δ0
|y(t+s,x)−y(s,x)|<δ0

+ E

sup
0≤s≤1
δ0
e−α0s e−αth(y(t + s, x)) −h(y(s, x))

× 1sup0≤s≤1
δ0
|y(t+s,x)−y(s,x)|≥δ0

.
The fact that h is bounded (uniformly), gives us
E

sup
0≤s≤1
δ0
e−α0s e−αth(y(t + s, x)) −h(y(s, x))
 1sup0≤s≤1
δ0
|y(t+s,x)−y(s,x)|≥δ0

≤2 ∥h∥∞P( sup
0≤s≤1
δ0
|y(t + s, x) −y(s, x)| ≥δ0) < 2 ∥h∥∞ε,
(22)
where we have denoted by ∥· ∥∞the supremum norm. On the other hand, the
uniform continuity of h gives us
E

sup
0≤s≤1
δ0
e−α0s e−αth(y(t + s, x)) −h(y(s, x))

× 1sup0≤s≤1
δ0
|y(t+s,x)−y(s,x)|<δ0

< ε.
(23)
We have for the second term in the right-hand side of (21)
E

sup
s≥1
δ0
e−α0se−αt |h(y(t + s, x)) −h(y(s, x))|

≤sup
s≥1
δ0
2e−α0s ∥h∥∞
≤2e−α0 1
ε ∥h∥∞.
(24)

66
H. Jasso-Fuentes et al.
Using the estimations (22), (23) and (24) in (21) we get p(Φα(t)h−e−αth, x) →0
as t ↓0. This proves that h satisﬁes part (a) of Deﬁnition 2. But also note that
h trivially satisﬁes Deﬁnition 2(b) because {y(t, x)}t≥0 is c`adl`ag. Therefore, we
can easily conclude that Cu(O) ⊂Cw
p (O) ⊂Cp(O), which proves part (a) of this
proposition.
(b.1) Let h ∈Cp(O). Now, in virtue of Proposition 1, we have that
p(Φα(t)h, x) ≤p(h, x),
for each x ∈O and t ≥0. Hence
p(Φα(s)Φα(r)h −Φα(r)h, x) = p(Φα(r)(Φα(s)h −h), x)
≤p(Φα(s)h −h, x) →0,
s ↓0.
This shows that Φα(r)h ∈Cp(O) for all r ≥0, for every element h ∈Cp(O). It
remains to prove that the process {Φα(t)h(y(s, x))}s≥0 is c`adl`ag for each x ∈O
and t ≥0. To do this, let s0 ≥0 and {sn}n∈N be a decreasing sequence in [0, ∞)
converging to s0. Take t ≥0 and x ∈O. We have that {h(y(s + t, x))}s≥0 is
a c`adl`ag process and sups≥0 e−α0s|h(y(s + t, x))| ∈L1(Ω) because h ∈Cp(O)
and satisﬁes (13). Hence, applying Theorem 45 in [12], the right continuity of
both the ﬁltration and the process h(y(s, x)), as well as the Markov property,
we deduce
lim
n→∞e−α0sneαtΦα(t)h(y(sn, x)) = lim
n→∞E[e−α0snh(y(sn + t, x))|Fsn]
= E[e−α0s0h(y(s0 + t, x))|Fs0] = e−α0s0eαtΦα(t)h(y(s0, x)),
a.s.
Due to the continuity of the exponential function, from the above we deduce
that lims↓s0 Φα(t)h(y(s, x)) = Φα(t)h(y(s0, x)), a.s. On the other hand, using
again Theorem 45 in [12] and the existence of left-limits of the process h(y(s, x))
we get lims↑s0 Φα(t)h(y(s, x)) = E[e−αth(y(t + s−
0 , x))|Fs−
0 ], a.s. Therefore, the
process {Φα(t)h(y(s, x))}s≥0 is c`adl`ag.
(b.2) If h ∈Cw
p (O), then we have that ∥Φα(t)h∥w ≤∥h∥w < ∞due to (15),
yielding that Φα(t)h ∈Cw
p (O).
⊓⊔
Our next target is to describe a closedness properties of both Cp(O) and
Cw
p (O) under (boundedly) seminorm-convergence. For this end, we will prove
the next ancillary result.
Lemma 1. Consider a sequence of functions {hn}n∈N together with a function
h all contained in B(O). For each x ∈O, suppose that limn↓0 p(hn −h, x) = 0.
Then, there exists a subsequence {nk}k∈N (dependent of x), such that
lim
k→∞sup
s≥0

e−α0s|hnk(y(s, x)) −h(y(s, x))|

= 0,
a.s.
Proof. We note that convergence in seminorm implies that
sup
s≥0

e−α0s|hn(y(s, x)) −h(y(s, x))|

→0
as n →∞,
(25)

Optimal Stopping Problems
67
where the last convergence is of L1(Ω, R) type. Then, the above sequence con-
verges also in measure and this yields the existence of a subsequence which
converges a.s.
⊓⊔
Theorem 1. Let h and {hn}n∈N be functions all in B(O). Then, under Assump-
tion 1, the following assertions hold true.
(a) If hn ∈Bp(O) and s −limn→∞hn = h then h ∈Bp(O).
(b) If hn ∈Cp(O) and s −limn→∞hn = h then h ∈Cp(O).
(c) If hn ∈Cw
p (O) and bs −limn→∞hn = h then h ∈Cw
p (O).
Proof. (a) Given x ∈O, there exists n ∈N such that p(h −hn, x) ≤1 and
we have that |h| ≤|h −hn| + |hn|. Then due to the triangular inequality of the
seminorm, we get p(h, x) ≤p(h−hn, x)+p(hn, x) < ∞and therefore h ∈Bp(O).
(b) Let us suppose hn ∈Cp(O) and s −limn→∞hn = h. Then we have that
p(Φα(t)h −h, x) ≤p(Φα(t)h −Φα(t)hn, x) + p(Φα(t)hn −hn, x) + p(hn −h, x)
≤2p(hn −h, x) + p(Φα(t)hn −hn, x).
Letting t ↓0 and hence n →∞to the last expression, we get limt↓0 p(Φα(t)h −
h, x) = 0, for each x ∈O. On the other hand, a simple use of Lemma 1 ensures
the existence of a subsequence {nk}k∈N such that
sup
s≥0

e−α0s|hnk(y(s, x)) −h(y(s, x))|

→0,
a.s.
(26)
when k →∞. Note that this subsequence x. Let t0 ≥0, we have that
|h(y(t, x)) −h(y(t0, x))|
≤|h(y(t, x)) −hnk(y(t, x))| + |hnk(y(t, x)) −hnk(y(t0, x))|
+|hnk(y(t0, x)) −h(y(t0, x))|
≤(eα0t + eα0t0) sup
s≥0
e−α0s|hnk(y(s, x)) −h(y(s, x))|
+|hnk(y(t, x)) −hnk(y(t0, x))|.
(27)
Since t →hnk(y(t, x)) is right-continuous and considering the convergence (26),
we then apply the limits t ↓t0 and hence k →∞on the last expression and
obtain limt↓t0 |h(y(t, x)) −h(y(t0, x))| = 0 a.s. On the other hand, in the same
way as in (27), we get
|h(y(t, x)) −h(y(t−
0 , x))| ≤(eα0t + eα0t0) sup
s≥0
e−α0s|hnk(y(s, x)) −h(y(s, x))|
+ |hnk(y(t, x)) −hnk(y(t−
0 , x))|.
We apply the limits t ↑t0 and hence k →∞on the last expression and obtain
limt↑t0 |h(y(t, x)) −h(y(t−
0 , x))| = 0 a.s. due to the left-limits existence.
(c) If hn ∈Cp(O) and bs −limn→∞hn = h, then we have that
s −lim
n→∞hn = h

68
H. Jasso-Fuentes et al.
and supn∈N ∥hn∥w, then due to part (b), we have that h ∈Cp(O) and we need
to demonstrate ∥h∥w < ∞. Namely, we have that seminorm convergence implies
pointwise convergence, hence
|h(x)|
w(x)
= limn→∞
|hn(x)|
w(x)
≤supn∈N ∥hn∥w < ∞
implying ∥h∥w < ∞and therefore h ∈Cw
p (O).
⊓⊔
2.3
The Inﬁnitesimal Generator and the Resolvent
We deﬁne the inﬁnitesimal generator (D(Aα), Aα) associated to the semigroup
Φα as follows

D(Aα) := {h ∈Cw
p (O) : ∃bs −limt↓0
h−Φα(t)h
t
};
Aαh
:= bs −limt↓0
h−Φα(t)h
t
.
(28)
Remark 3. In virtue of Deﬁnition 2 and Theorem 1, every limit in (28) belongs
to Cw
p (O).
Recall from Assumption 1 that t →Φα(t)h(x) is measurable for every h ∈
B(O) and x ∈O, then we are in conditions to deﬁne the resolvent operator
{Rα}α>α0 by
Rαh(x) =
 ∞
0
Φα(t)h(x) dt, ∀x ∈O, h ∈B(O),
(29)
where the integral is taken in the Lebesgue sense for real valued functions. A
direct consequence of this deﬁnition is that Rαh is Borel measurable, for each
ﬁxed α. Also, if h ∈Bp(O) then Fubini’s Theorem along with Proposition 1
yield
p(Rαh, x) ≤E

sup
s≥0
e−α0s
 ∞
0
|Φα(t)h(y(s, x))| dt

≤
 ∞
0
E

sup
s≥0
e−α0s|Φα(t)h(y(s, x))|

dt =
 ∞
0
p(Φα(t)h, x) dt
=
 ∞
0
e−(α−α0)tp(Φα0(t)h, x) dt ≤
1
α −α0
p(h, x),
(30)
which implies Rαh ∈Bp(O). Moreover, if h ∈Bw(O) then we have
∥Rαh∥w ≤
 ∞
0
e−(α−α0)t ∥Φα0(t)h∥w dt ≤
1
α −α0
∥h∥w < ∞,
(31)
and so Rαh ∈Bw(O).
Our next goal is to prove the stronger fact that Rh ∈Cw
p (O) when h ∈
Cw
p (O), that is, we will show that Rα maps Cw
p (O) into itself. Such results will
be provided in Theorem 2 below. Before doing this, we will check some useful
properties:

Optimal Stopping Problems
69
In the same way as in (30) it is easy to demonstrate that
p(
 b
a
Φα(t)h dt, x) ≤
 b
a
p(Φα(t)h, x) dt,
for every 0 ≤a ≤b ≤∞.
(32)
Besides, we can interchange the semigroup and the resolvent; namely, for every
β > α0 and α ≥α0, using Fubini’s Theorem we get
RβΦα(t)h(x) =
 ∞
0
Φα(t)Φβ(s)h(x) ds =
 ∞
0
E[e−αtΦβ(s)h(y(t, x))] ds
= E

e−αt
 ∞
0
Φβ(s)h(y(t, x)) ds

= Φα(t)Rβh(x).
(33)
The use of Fubini’s Theorem is justiﬁed since
 ∞
0
E[e−αt|Φβ(s)h(y(t, x))|] ds ≤Φα(t)Rβ|h|(x) ≤
1
β −α0
∥h∥w w(x).
Our next result uses the following notation:
u(t) = Φα(t)h, for a given h ∈Cp(O) and α > α0.
(34)
Lemma 2. Fix x ∈O. Then:
(a) For all t0 ≥0 we have that limt→t0 p(u(t) −u(t0), x) = 0.
(b) We have limt→∞p(u(t), x) = 0.
(c) For all ε > 0 there exists δ = δ(x, ε) > 0 such that if |t −s| ≤δ then
p(u(t) −u(s), x) ≤ε.
Proof. (a) Using Proposition 1, the semigroup property and the continuity in
seminorm at t = 0 of the semigroup, it is straightforward to show that
lim
t→t+
0
p

Φα(t)h −Φα(t0)h, x

= lim
t→t−
0
p

Φα(t)h −Φα(t0)h, x

= 0,
which proves (a).
(b) By Proposition 1, we have that p(u(t), x) = p(e−(α−α0)tΦα0(t)h, x) ≤
e−(α−α0)tp(h, x). Due to α −α0 > 0, we can take T > 0 such that
e−(α−α0)tp(h, x) ≤ε
for all t ≥T.
(c) Using part (b) above, we can take T > 0 large enough such that for all
t ≥T, p(u(t), x) ≤ε
2. Also, by the compactness of [0, T] and (a) above, we can
ﬁnd δ > 0 such that |t −s| < δ and t, s ≤T imply p(u(t) −u(s), x) < ε. On the
other hand, if s, t ≥T we get p(u(t) −u(s), x) ≤p(u(t), x) + p(u(s), x) ≤ε.
⊓⊔
Lemma 3. For each u as in (34), there exists a sequence of functions un :
[0, ∞) →Cp(O) such that
lim
n→∞sup
t≥0
p(u(t) −un(t), x) = 0.
(35)
Moreover, if h ∈Cw
p (O) then we can choose the above sequence such that un(t) ∈
Cw
p (O), for all n ∈N and t ≥0.

70
H. Jasso-Fuentes et al.
Proof. For ﬁxed x ∈O and n ∈N, we deﬁne
En,k := [k −1
n
, k
n),
k = 1, . . . , n2,
Fn := [n, ∞).
Deﬁne also the sequence of functions
un(t) :=
n2

k=1
u(tk)1En,k(t) + u(n)1Fn(t),
(36)
with tk = k−1
n . Note by Proposition 2, for all n ∈N and t ≥0, each un(t) is in
Cp(O) because they are linear combination of functions in Cp(O). In the same
way, if h ∈Cw
p (O) in (34) then in virtue of this same proposition, u ∈Cw
p (O),
yielding also un(t) ∈Cw
p (O). The limit (35) follows easily from estimations in
Lemma 2.
⊓⊔
Remark 4. We know that un(t) belongs to Cp(O) (resp. to Cw
p (O)) if h ∈Cp(O)
(resp. ∈Cw
p (O)). Also, because of the deﬁnition of un in (36) we have that for
each x ∈R the function t →un(t)(x) = n2
k=1 u(tk)(x)1En,k(t) + u(n)(x)1Fn(t)
is simple and real valued. Hence, given β > 0 the function t →e−βtun(t)(x) is
Lebesgue integrable with integral given by
 b
a
e−βtun(t)(x) dt =
n2

k=1
u(tk)(x)

En,k∩[a,b]
e−βt dt + u(n)(x)

Fn∩[a,b]
e−βt dt.
(37)
We note that the above integral, as a function of x, belongs to Cp(O) (resp.
to Cw
p (O)), because it is a sum of functions in Cp(O) (resp. Cw
p (O)). Then, we
simply denote this integral by
 b
a e−βtun(t) dt.
We have arrived to our ﬁrst main result regarding the regularity of the resolvent
Rα, when the integrand satisﬁes that regularity.
Theorem 2. Assume that Assumption 1 is valid. Then, for all 0 ≤a ≤b ≤∞,
and β > 0, the next relation holds true
s −lim
n→∞
 b
a
e−βtun(t) dt =
 b
a
e−βtu(t) dt,
(38)
for the functions u and {un} introduced in Lemma 3. In particular, we have that
Rαh is in Cp(O). Analogously, we obtain the same result with Cw
p (O) instead of
Cp(O) if h ∈Cw
p (O) with bs −lim instead of s −lim in (38).
Proof. By the inequality in (32) as long with Lemma 3, we get
p
  b
a
e−βtun(t) dt −
 b
a
e−βtu(t) dt, x

≤
 b
a
e−βtp

un(t) −u(t), x

dt
≤sup
t∈[a,b]
p

un(t) −u(t), x
  b
a
e−βt dt →0,

Optimal Stopping Problems
71
when n →∞. That is s −limn→∞
 b
a e−βtun(t) dt =
 b
a e−βtu(t) dt, that implies
 b
a e−βtu(t) dt ∈Cp(O) due to Theorem 1. Moreover, in the case of h ∈Cw
p (O)
we have that u(t) ∈Cw
p (O) and ∥u(t)∥w = ∥Φα(t)h∥w ≤∥h∥w for all t ≥0.
Using this last inequality together with (37) we get

 b
a
e−βtun(t) dt

w ≤
n2

k=1
∥h∥w

En,k∩[a,b]
e−βt dt + ∥h∥w

Fn∩[a,b]
e−βt dt
= ∥h∥w
 b
a
e−βt dt < ∞.
(39)
Hence, supn∈N

 b
a e−βtun(t) dt

w < ∞, and therefore
bs −lim
n→∞
 b
a
e−βtun(t) dt =
 b
a
e−βtu(t) dt,
that implies
 b
a e−βtu(t) dt ∈Cw
p (O), again due to Theorem 1. In particular,
taking β = α−α0
2
> 0, u(t) = Φβ+α0(t)h, a = 0, and b = ∞, we obtain
Rαh(x) =
 ∞
0
Φα(t)h(x) dt =
 ∞
0
e−(α−α0)tΦα0(t)h(x) dt
=
 ∞
0
e−α−α0
2
tΦ α−α0
2
+α0(t)h(x) dt =
 ∞
0
e−βtu(t)(x) dt.
Thus, Rαh is in Cp(O) (resp. in Cw
p (O) when h ∈Cw
p (O)).
⊓⊔
The next result is a useful property of the integrals of semigroups that is
very common in ﬁnite-dimensional spaces.
Lemma 4. Let h ∈Cw
p (O). For any t0 ≥0 we have
bs −lim
t↓0
1
t
 t0+t
t0
Φα(s)h ds = Φα(t0)h.
(40)
Proof. Let t0 ≥0 and ﬁx x ∈O. By Theorem 1 (c), we get that 1
t
 t0+t
t0
Φα(s)h ∈
Cw
p (O). Since t →Φα(t)h is continuous in seminorm, given ε > 0 we consider
δ > 0 such that |t0 −s| < δ implies p(Φα(s)h −Φα(t0)h, x) < ε. Hence, if |t| ≤δ
then, by (32) we get
p
1
t
 t0+t
t0
Φα(s)h ds −Φα(t0)h, x

= p
1
t
 t0+t
t0
[Φα(s)h −Φα(t0)h] ds, x

≤1
t
 t0+t
t0
p(Φα(s)h −Φα(t0)h, x) ds < ε.
On the other hand, using (15) we get
1
t
 t0+t
t0
Φα(t)h ds

w ≤1
t
 t0+t
t0
∥Φα(t)h∥w ds ≤1
t
 t0+t
t0
∥h∥w ds = ∥h∥w .
(41)
Thus, we have proved bs −limt↓0 1
t
 t0+t
t0
Φα(t)h ds = Φα(t0)h.
⊓⊔

72
H. Jasso-Fuentes et al.
Our next deﬁnition has to do with the diﬀerentiability of semigroups.
Deﬁnition 3. We say that t →Φα(t)h is boundedly diﬀerentiable in seminorm
in a ﬁxed point r ≥0 if the limit
bs −lim
t→0
Φα(t + r)h −Φα(r)h
t
exists in Cw
p (O).
Remark 5.(a) If h ∈Cw
p (O) and the above limit exists, then Theorem 1(c)
ensures that this limit belongs to Cw
p (O).
(b) The boundedly diﬀerentiability in seminorm implies the pointwise diﬀeren-
tiability; i.e., for each x ∈O, limt↓0
Φα(t+r)h(x)−Φα(t)h(x)
t
.
The next theorem shows a relation between the semigroup Φα and the
inﬁnitesimal generator Aα, among other important properties.
Theorem 3. Suppose that Assumption 1 is valid. Then, for each h ∈D(Aα), we
have that Φα(t)h ∈D(Aα) for all t > 0. Furthermore, the function t →Φα(t)h is
boundedly diﬀerentiable in seminorm on (0, ∞), and the following relation holds
−d
dt(Φα(t)h) = AαΦα(t)h = Φα(t)Aαh,
∀t > 0.
(42)
(The derivative on the left-hand side is understood in the sense of boundedly
diﬀerentiability in seminorm.)
Proof. First note that
1
s(Φα(t)h −Φα(t + s)h) = Φα(t)1
s(h −Φα(s)h).
(43)
Next, by using the fact of s −lims↓0 1
s(h −Φα(s)h) = Aαh as long with
Proposition 1, we have that
−d+
dt Φα(t)h = s −lim
s↓0
1
s(Φα(t)h −Φα(t + s)h) = Φα(t)Aαh.
On the other hand, taking into account (43) we get
1
s(Φα(t)h −Φα(t + s)h)

w ≤1
s ∥Φα(t)(h −Φα(s)h)∥w
≤1
s ∥h −Φα(s)h∥w ≤sup
s≥0
1
s ∥h −Φα(s)h∥w < ∞.
The last inequality is due to the boundedly convergence in seminorm bs−lims↓0
1
s(h−Φα(s)h) in (19) applied to the deﬁnition of Aα. Hence Φα(t)h ∈D(Aα) and
AαΦα(t)h = Φα(t)Aαh. In the same way it is possible to show that −d
dtΦα(t)h =
AαΦα(t)h, which proves (42).
⊓⊔

Optimal Stopping Problems
73
The next two results are crucial for our analysis: the ﬁrst one shows the
denseness of the domain D(Aα) into the space Cw
p (O), whereas the second proves
that the resolvent is the inverse operator of the generator; that is, A−1
α
= Rα.
Theorem 4. Under the assumption of Theorem 3, the domain D(Aα) is dense
in Cw
p (O) in the sense of the boundedly seminorm-convergence.
Proof. Take h ∈Cw
p (O) and deﬁne hn := n
 1
n
0 Φα(s)h ds. By the proof of
Lemma 4, we know that hn ∈Cw
p (O) and bs −limn→∞hn = h, so it is suﬃcient
to show that hn ∈D(Aα). Indeed, using Fubini’s Theorem, we have that
Φα(t)hn(x)
= E

e−αtn

1
n
0
Φα(s)h(y(t, x)) ds

= n

1
n
0
E[e−αtΦα(s)h(y(t, x))] ds
= n

1
n
0
Φα(s + t)h(x) ds = n
 t+ 1
n
t
Φα(s)h(x) ds.
Then, we obtain
1
t (hn −Φα(t)hn) = n
1
t

1
n
0
Φα(s)h ds −1
t
 t+ 1
n
t
Φα(s)h ds

= n
1
t
 t
0
Φα(s)h ds −1
t
 t+ 1
n
1
n
Φα(s)h ds

.
Using this last fact together with Lemma 4, we get s −limt↓0 1
t (hn −Φα(t)hn) =
n(h −Φα( 1
n)h). We have also the relation
1
t (hn −Φα(t)hn)

w ≤n
t

 t
0
Φα(s)h ds

w + n
t

 t+ 1
n
1
n
Φα(s)h ds

w
≤n
t
 t
0
∥Φα(s)h∥w ds + n
t
 t+ 1
n
1
n
∥Φα(s)h∥w ds ≤2n ∥h∥w .
Hence, hn ∈D(Aα).
Theorem 5. Let Assumption 1 hold true. Then, for each α > 0, the operator
Aα from D(Aα) to Cw
p (O) is bijective. Besides, the following identity is satisﬁed
A−1
α
= Rα.
Proof. Let us show ﬁrst that Aα is surjective. Let h ∈Cw
p (O) and s ≥0. Using
(33) we obtain
Φα(s)Rαh(x) = RαΦα(s)h(x) =
 ∞
0
Φα(t + s)h(x) dt =
 ∞
s
Φα(t)h(x) dt.

74
H. Jasso-Fuentes et al.
Then,
1
s(Rαh −Φα(s)Rαh) = 1
s
 ∞
0
Φα(t)h(x) dt −1
s
 ∞
s
Φα(t)h(x) dt
= 1
s
 s
0
Φα(t)h(x) dt.
By Lemma 4, we deduce that bs−lims↓0 1
s(Rαh−Φα(s)Rαh) = h which implies
Rαh ∈D(Aα) and AαRαh = h, and therefore Aα is surjective. Now, let us show
that Aα is injective. Take h ∈D(Aα) such that Aαh = 0. By Theorem 3, we
have that
d
dtΦα(t)h(x) = −Φα(t)Aαh(x) = 0 for all x ∈O, which implies that
t →Φα(t)h(x) is a real constant. But, |Φα(t)h(x)| ≤e−(α−α0)t ∥h∥w w(x), so,
limt→∞Φα(t)h(x) = 0. Moreover, we have Φα(0)h(x) = h(x) and then, h(x) = 0
for all x ∈O. Thus, we have concluded that Aα is invertible with inverse given
by Rα.
⊓⊔
As a direct consequence of both Theorems 5 and 3 we can get, for all h ∈
Cw
p (O), the relation
Rαh −Φα(t)Rαh =
 t
0
Φα(s)h ds =
 t
0
Φα(s)AαRαh ds.
(44)
We conclude this section by providing some properties of the operators Aα
and Rα.
Proposition 3. For all h ∈Cw
p (O) and β > 0, we have the next relation
bs −lim
α→∞αRα+βh = h.
(45)
Proof. By deﬁnition of the resolvent and (33) we get the resolvent equation:
RαRβ =
1
α −β (Rβ −Rα).
(46)
Next, we will prove limα→∞p(αRαh −h, x) = 0 for all h ∈Cw
p (O). Let us
assume ﬁrst that h ∈D(Aα) and let us take g ∈Cw
p (O) such that h = Rβg. We
have
αRαh = αRαRβg =
α
α −β (Rβg −Rαg) =
α
α −β h −
α
α −β Rαg.
It is easy to see that limα→∞

α
α−β h −h

w = 0 and limα→∞

α
α−β Rαg

w = 0,
where the last limit is due to (31). Therefore,
lim
α→∞∥αRαh −h∥w = 0.
By (20) we see that the above convergence in norm implies the convergence in
seminorm: s −limα→∞αRαh = h. Now, consider the general case h ∈Cw
p (O).
Let hn be a sequence in D(Aα) such that bs −limn→∞hn = h. We have
|αRαh −h| ≤|αRαh −αRαhn| + |αRαhn −hn| + |hn −h|,

Optimal Stopping Problems
75
applying (30) to the above inequality we get
0 ≤p(αRαh −h, x) ≤
α
α −α0
p(h −hn, x) + p(αRαhn −hn, x) + p(hn −h, x).
Letting α →∞and hence n →∞in the last inequality, we easily deduce that
limα→∞p(αRαh −h, x) = 0; in other words s −limα→∞αRαh = h. Moreover,
by (31) we get ∥αRαh∥w ≤α/(α −α0) ∥h∥w, and so bs −limα→∞αRαh = h.
It remains to show (45). For this purpose, let β > 0 and note that αRα+β =
(α + β)Rα+β −βRα+β, we know that bs −limα→∞(α + β)Rα+βh = h and
bs −limα→∞βRα+β = 0, hence bs −limα→∞αRα+βh = h.
⊓⊔
Proposition 4. Given α > α0 and β ≥0, we have
Aα+β = Aα + βI.
(47)
Proof. Let h ∈D(Aα). Then,
h −Φα+β(t)h =h −e−βtΦα(t)h = h −Φα(t)h + (1 −e−βt)Φα(t)h.
Multiplying by 1
t the last expression, and hence letting t ↓0, we get Aα+βh :=
bs −limt↓0 1
t (h −Φα+β(t)h) = Aαh + βh.
⊓⊔
3
The Optimal Stopping Problem
This section deals with an optimal stopping control problem whose dynamical
system is of Markov type studied in Sect. 2. The total cost consists of both
a running cost that is paid when the dynamic is still running and a stopping
cost that must to be paid once the dynamic is stopped. The way to tackle
this problem is through a characterization of the optimal cost (value function)
regarded as the maximal subsolution of a variational inequality deﬁned later. In
addition, by means of this characterization, it is also possible to ﬁnd the well-
known continuation region that in turn provides the associated optimal stopping
time viewed as the ﬁrst hitting time of that region.
3.1
The Statement of the Problem
In this subsection we start our analysis recalling some mathematical objects
introduced in Sect. 2. Namely, we recall the underlying stochastic process, con-
sisting of the homogeneous Markov process {y(t, x)}t≥0, x ∈O deﬁned on the
probability space E := (Ω, F, {Ft}t≥0, P), with state space (O, |·|), satisfying
P(y(0, x) = x) = 1 as well as the properties established in Assumption 1.
We bring to mind that a stopping time is a random variable τ with values in
the no-negative real numbers set such that the event {τ ≤t} is Ft measurable
for every t ≥0, with Ft the associated ﬁltration to the space E.

76
H. Jasso-Fuentes et al.
Let T be the set consisting of all stopping times introduced in the above
paragraph. With this in mind, for x ∈O, f, ϕ ∈Cw
p (O), τ ∈T , and α > α0 > 0,
we deﬁne the following cost function
J(x, τ) := E
 τ
0
f(y(t, x))e−αtdt + ϕ(y(τ, x))e−ατ1τ∞

,
(48)
where as mentioned above, f and ϕ represent the running and stopping cost per
unit of time respectively, and e−α· denotes the discount factor at each instant of
time.
The optimal cost, also known as the value function, is then deﬁned as
ˆu(x) = inf
τ∈T J(x, τ).
(49)
We will say that the random variable ˆτ ∈T is an optimal stopping time if it
minimizes the cost (48) in the following way
ˆu(x) = J(x, ˆτ).
(50)
One of the goals of this section will consist to showing that the value function
ˆu deﬁned in (49) does exist in Cw
p (O). Furthermore, this function satisﬁes the
next variational inequality (VI) in the integral (or weak) form:
ˆu ≤ϕ,
ˆu ≤
 t
0
Φα(s)f ds + Φα(t)ˆu,
∀t ≥0.
(51)
3.2
Penalized Method
We start our analysis by studying an ancillary problem so-called penalized prob-
lem. This problem consists of searching for a unique solution of the following
penalized equations
Aαuε + 1
ε(uε −ϕ)+ = f,
for each ε > 0,
(52)
with
(uε −ϕ)+ =
	uε −ϕ, if uε −ϕ ≥0;
0,
if uε −ϕ ≤0.
Our goal is to prove that one subsolution of the inequality (51) can be charac-
terized as the limit as ε ↓0 of the sequence of solutions uε associated to (52).
This limit function will be the “good one” for us.
Note that (uε −ϕ)+ = uε −(uε ∧ϕ). Hence, Proposition 4 together with (52),
imply
Aα+ 1
ε uε = f + 1
ε(uε ∧ϕ).
(53)
Applying Rα+ 1
ε to the last equation we get
uε = Rα+ 1
ε (f + 1
ε(uε ∧ϕ)).
(54)

Optimal Stopping Problems
77
As mentioned earlier, we will prove that u0 := s−limε↓0 uε veriﬁes the VI (51) as
well as its corresponding regularity. To this end, we need the following technical
result.
Lemma 5. The following inequality holds for any measurable functions f, g, h
from O to R:
|f ∧h −g ∧h| ≤|f −g|.
Proof. We have both −|f −g| + g ∧h ≤f −g + g = f and −|f −g| + g ∧h ≤h
that imply −|f −g|+g∧h ≤f ∧h. Analogously, we have −|f −g|+f ∧h ≤g∧h,
and joining the two obtained inequalities we get |f ∧h −g ∧h| ≤|f −g|.
⊓⊔
Theorem 6. Assume that f, ϕ ∈Cw
p (O). Then, Assumption 1 implies the fol-
lowing.
(a) There exists a unique solution uε ∈D(Aα) of the penalized equation (52)
for each ε > 0.
(b) For all 0 < ε′ < ε we have that
0 ≤uε −uε′ ≤(uε −ϕ)+ ≤|Rα+ 1
ε f + Rα+ 1
ε ϕ −ϕ|.
(55)
Furthermore, there exists the limit u0 := s −limε↓0 uε and therefore, u0 ∈
Cp(O).
Proof. First, we will show the existence of a unique solution uε of the penalized
problem. Namely, based on (54), we deﬁne the nonlinear operator Tε : Bw(O) →
Bw(O) given by Tεh := Rα+1/ε(f + 1
ε(h ∧ϕ)). We will prove that Tε is a
contraction map. Indeed, as h, g ∈Bw(O), we have
Tεh −Tεg =1
εRα+ 1
ε (h ∧ϕ −g ∧ϕ).
Using the monotony of the resolvent together with Lemma 5 we get
1
ε|Rα+ 1
ε (h ∧ϕ −g ∧ϕ)| ≤1
εRα+ 1
ε |h ∧ϕ −g ∧ϕ| ≤1
εRα+ 1
ε |h −g|.
Now use (31) to obtain
∥Tεh −Tεg∥w ≤
1
ε
α −α0 + 1
ε
∥h −g∥w .
We know that
1
ε
α−α0+ 1
ε < 1. Then Tε is a contraction map on the Banach space
Bw(O), so there exist a unique uε in Bw(O) such that Tεuε = uε, this implies
that uε solves (52). Moreover, we have that limn→∞∥T n
ε h −uε∥w = 0 that
implies convergence in seminorm.
On the other hand, using the fact that f, ϕ ∈Cw
p (O), and taking h ∈Cw
p (O),
all together allow us to apply Theorem 5 to claim that Tεh = Rα+ 1
ε (f + 1
ε(h ∧
ϕ)) ∈D(Aα). Iterating n-times the operator Tε, it is easy to see that T n
ε h ∈

78
H. Jasso-Fuentes et al.
D(Aα) for all n ∈N. Hence, in virtue of Theorem 1 we have uε ∈Cw
p (O),
yielding that uε = Rα+ 1
ε (f + 1
ε(uε ∧ϕ)) ∈D(Aα).
Let us prove now the inequalities (55). Namely, let 0 < ε′ < ε. Then from
(53) we obtain
Aα+ 1
ε uε′ = Aα+ 1
ε′ uε′ + (1
ε −1
ε′ )uε′ = f + 1
ε′ (uε′ ∧ϕ) + (1
ε −1
ε′ )uε′
= f + 1
ε′ uε′ −1
ε′ (uε′ −ϕ)+ + (1
ε −1
ε′ )uε′
= f −1
ε′ (uε′ −ϕ)+ + 1
εuε′ ≤f −1
ε(uε′ −ϕ)+ + 1
εuε′ = f + 1
ε(uε′ ∧ϕ).
Applying Rα+ 1
ε to the last inequality we obtain
uε′ ≤Tεuε′.
Iterating, we get uε′ ≤T n
ε uε′. Therefore, letting n →∞we obtain uε′ ≤uε.
Next, we will show that uε −uε′ ≤(uε −ϕ)+ ≤|Rα+ 1
ε f + Rα+ 1
ε ϕ −ϕ|.
Namely, assuming uε′ ≥ϕ we get uε −uε′ ≤uε −ϕ ≤(uε −ϕ)+. Otherwise, if
ϕ ≥uε′ then from (52) we obtain Aα(uε−uε′) = −1
ε(uε−ϕ)+ ≤0, and applying
Rα to the last inequality we get uε −uε′ ≤0 ≤(uε −ϕ)+. Moreover, from (54)
we obtain
uε −ϕ = Rα+ 1
ε (f + 1
ε(uε ∧ϕ)) −ϕ
= Rα+ 1
ε (f + 1
ε(uε ∧ϕ) −1
εϕ) + 1
εRα+ 1
ε ϕ −ϕ
= Rα+ 1
ε (f −1
ε(ϕ −uε)+) + 1
εRα+ 1
ε ϕ −ϕ ≤Rα+ 1
ε f + 1
εRα+ 1
ε ϕ −ϕ. (56)
Hence,
0 ≤uε −uε′ ≤(uε −ϕ)+ ≤
Rα+ 1
ε f + 1
εRα+ 1
ε ϕ −ϕ
.
(57)
Let ε > ε′ > 0. Using uε′ ≤uε and 0 ≤uε −uε′ ≤(uε −ϕ)+, we obtain that
there exists the pointwise monotone limit u0 := limε↓0 uε and u0 > −∞. Letting
ε′ ↓0 in (57), we get 0 ≤uε −u0 ≤
Rα+ 1
ε f + 1
εRα+ 1
ε ϕ −ϕ
. Thus, in virtue of
the relations (30) and (45) we get
p(uε −u0, x) ≤
1
α + 1
ε −α0
p(f, x) + p
1
εRα+ 1
ε ϕ −ϕ, x

→0,
as ε ↓0,
and so s −limε↓0 uε
=
u0; this implies that u0
∈
Cp(O) after using
Theorem 1(b).
⊓⊔
3.3
Variational Inequalities
Let f, ϕ ∈Cw
p (O). We say that u ∈Cw
p (O) satisﬁes the variational inequalities
(VI) if:
	
u ≤
 t
0 Φα(s)f ds + Φα(t)u, ∀t ≥0;
u ≤ϕ.
(58)

Optimal Stopping Problems
79
Any function u ∈Cw
p (O) that satisﬁes the VI above, will be referred to as a
subsolution.
On the other hand, by deﬁnition of w in (3), it is obvious that w ∈Bw(O).
For the later purposes, we need the next assumption in order to guarantee that
the subsolution of interest associated to (58) is regular enough.
Assumption 2. We suppose that w deﬁned in (3), belongs to Cw
p (O).
Remark 6. The above assumption is veriﬁed in particular models, see for
instance, Menaldi [24,25] or Menaldi and Sritharan [28], where the authors use
a polynomial function of type w(x) = k1(k2 + |x|2)p, for some constants k1 ≥1,
k2 ≥0.
Now let u := Rαf −(∥ϕ∥w +
1
α−α0 ∥f∥w)w ∈Cw
p (O). Note that Rαf −
1
α−α0 ∥f∥w w ≤0 because of (31), then u ≤−∥ϕ∥w w ≤ϕ. We also have that
Φα(t)u ≥Φα(t)Rαf −(∥ϕ∥w +
1
α −α0
∥f∥w)w.
Using (44), we obtain
 t
0
Φα(s)f ds + Φα(t)u = Rαf −Φα(t)Rαf + Φα(t)u
≥Rαf −(∥ϕ∥w +
1
α −α0
∥f∥w)w = u.
Therefore, we have proved that u ∈Cw
p (O) deﬁned in the previous paragraph
satisﬁes the VI (58).
We will see next that the limit function u0 obtained in the past subsection,
is the maximal subsolution on Cw
p (O) of the VI (58) and ∥u0∥w < ∞as it is
established in the following theorem.
Theorem 7. Under Assumptions 1 and 2, the limit function u0 introduced in
Theorem 6 veriﬁes the VI (58). Moreover, every u ∈Cw
p (O) that is also a
subsolution of (58) satisﬁes u ≤u0; as a consequence u0 ∈Cw
p (O).
Proof. From (52) and (44), we obtain
uε = Rα(f −1
ε(uε −ϕ)+) =
 t
0
Φα(s)(f −1
ε(uε −ϕ)+) ds + Φα(t)uε
≤
 t
0
Φα(s)f ds + Φα(t)uε.
Moreover, for each t ≥0, we have that Φα(t)uε →Φα(t)u0 pointwise as ε ↓0,
because p(Φα(t)uε −Φα(t)u0, x) ≤p(uε −u0, x) →0, as ε ↓0. So, letting ε ↓0
in the last inequality we get
u0 ≤
 t
0
Φα(s)f ds + Φα(t)u0.

80
H. Jasso-Fuentes et al.
On the other hand from (54) we have
uε = Rα+ 1
ε (f + 1
ε(uε ∧ϕ)) ≤Rα+ 1
ε (f + 1
εϕ).
(59)
In virtue of (30) and (45), we have
p

Rα+ 1
ε (f + 1
εϕ) −ϕ, x

≤
1
α + 1
ε −α0
p(f, x) + p
1
εRα+ 1
ε ϕ −ϕ, x

→0,
as ε ↓0.
The last relation implies in particular that Rα+ 1
ε (f + 1
εϕ) →ϕ pointwise, as
ε ↓0. Hence, letting ε ↓0 in (59) we get
u0 ≤ϕ,
which implies that u0 satisﬁes (58).
It only remains to show that u0 the maximal subsolution. Indeed, take u ∈
Cw
p (O) that satisﬁes (58). Then, u satisﬁes: u −Φα(t) ≤
 t
0 Φα(s)f ds. Apply
then Rα+ 1
ε to both sides of the last inequality and hence multiply by 1
t , so that
1
t (Rα+ 1
ε u −Φα(t)Rα+ 1
ε u) ≤Rα+ 1
ε
1
t
 t
0
Φα(s)f ds.
The commutative property between Φα(t) and Rα+ 1
ε is due to (33). Using again
(44), the fact that α →Rα is a family of commutative operators given in (46),
as well as the relation (33), we deduce
1
t (Rα+ 1
ε u −Φα(t)Rα+ 1
ε u) ≤1
t (RαRα+ 1
ε f −Φα(t)RαRα+ 1
ε f),
thus letting t ↓0 we get
AαRα+ 1
ε u ≤AαRαRα+ 1
ε f = Rα+ 1
ε f.
(60)
In virtue of Proposition 4, we know that ( 1
εI + Aα)Rα+ 1
ε = Aα+ 1
ε Rα+ 1
ε = I,
then
AαRα+ 1
ε = I −1
εRα+ 1
ε .
(61)
This last fact, together with the relation u = u ∧ϕ (recall that u ≤ϕ), and (60)
yield that
u ≤Rα+ 1
ε f + 1
εRα+ 1
ε u = Rα+ 1
ε f + 1
εRα+ 1
ε (u ∧ϕ) = Tεu.
Iterating the last expression, we obtain that u ≤T n
ε u, implying that u ≤uε.
Letting ε ↓0, we obtain u ≤u0.
Finally, take u ∈Cw
p (O) that satisﬁes the VI (58) (we know that there exist
at least a function in Cw
p (O) satisfying the VI). Then we have u ≤u0 ≤ϕ that
implies |u0| ≤|u0 −u| + |u| ≤|ϕ −u| + |u|. Since ϕ −u and u belong to Cw
p (O)
we get that ∥u0∥w ≤∥ϕ −u∥w + ∥u∥w < ∞. So, we conclude that u0 ∈Cw
p (O)
is the maximal subsolution on Cw
p (O) of the VI (58).
⊓⊔

Optimal Stopping Problems
81
4
Solution of the Stopping Problem
In this section we will analyze the optimal control problem through the solution
of the VI (58). In addition, we provide the way to ﬁnd an optimal stopping time
in terms of so-named continuation region or contact set.
To begin with, we will show the next result regarding the strong Markov
property of the process y(t, x). Its proof has been inspired from Proposition 8.9
and Theorem 19.17 in [21].
Proposition 5. The Markov process {y(t, x)}t≥0 satisﬁes the strong Markov
property in the following sense: for all stopping time τ ∈T and h ∈B(O) we
have
E[h(y(s + τ, x))|Fτ] = E[h(y(s, y(τ, x)))],
(62)
where Fτ is the σ-algebra generated of events A ∈F for which A ∩{τ ≤t} ∈Ft
for every t ≥0.
Proof. First, let us suppose that τ has a denumerable state space D in ¯R. Then,
we have that
E[e−α(t+τ)h(y(t + τ, x))|Fτ] =

s∈D
1τ=sE[e−α(t+τ)h(y(t + τ, x))|Fτ]
=

s∈D
1τ=sE[e−α(t+τ)h(y(t + s, x))|Fs]
=

s∈D
1τ=sE[e−α(t+τ)h(y(t, y(s, x)))] = E[e−α(t+τ)h(y(t, y(τ, x)))].
Note that every conditional expectation above is well deﬁned since
E[e−α(t+τ)|h(y(t + τ, x))|] ≤E[sup
s≥0
e−αs|h(y(s, x))|] < ∞.
In the general case, by Lemma 7.4 in [21] we can take a sequence of stopping
times τn with denumerable state space such that τn ↓τ. So, we have that
E[h(y(t + τn, x))|Fτn] = E[h(y(t, y(τn, x)))]
which implies
E[e−τnh(y(t + τn, x))|Fτn] = e−ατnE[h(y(t, y(τn, x)))]
= e−α(τn−t)Φα(t)h(y(τn, x)).
(63)
By the right continuity of s →Φα(t)h(y(s, x)) and the fact that τn ↓τ we get
e−α(τn−t)Φα(t)h(y(τn, x)) →e−α(τ−t)Φα(t)h(y(τ, x)) when n →∞. By Lemma
7.3 in [21], we have Fτ = ∩n∈NFτn which together with Theorem 45 in [12] give
us
E[e−τnh(y(t + τn, x))|Fτn] →e−ατE[h(y(t + τ, x))|Fτ]
when n →∞. Using this last fact along with (63) we conclude that
E[h(y(t + τ, x))|Fτ] = E[h(y(t, y(τ, x)))].
⊓⊔

82
H. Jasso-Fuentes et al.
In order to characterize the optimal stopping time as the hitting time of certain
region of the state space, we will also need the following property of our process
{y(t, x)}t≥0.
Assumption 3. The process {y(t, x)}t≥0 is quasi-left continuous, that is, for
every stopping time τ and any sequence of stopping times τ1, τ2, . . . such that
τn ↑τ we have that y(τn, x) →y(τ, x) P-a.s. on {τ < ∞}.
Remark 7.(a) Assumption 3 is a little variation of the Hunt process deﬁnition.
(b) It is well-known that a Markov process associated to a strong Feller semi-
group is a Hunt process—for further details, see Chung [9], Chapter 3.
Let us now establish the main result of this section.
Theorem 8. Under Assumptions 1, 2, and 3, the following statements hold
true.
(a) The optimal cost ˆu in (49) is equal to the limit function u0.
(b) The optimal stopping time can be regarded as the ﬁrst hitting time of the
so-called continuation region (a.k.a. contact set). That is, for all x ∈O,
ˆτ(x) := inf{t ≥0 : ˆu(y(t, x)) = ϕ(y(t, x))}
(continuation region),
(64)
satisfying ˆu(x) = J(x, ˆτ(x)).
(c) If the stopping cost ϕ ∈D(Aα), then
Rα(f ∧Aαϕ) ≤ˆu ≤Rαf ∧ϕ.
(65)
Proof. (a) Take τ ∈T , where T is the set of stopping times deﬁned at the
beginning of the section. Moreover, deﬁne u := f −1
ε(uε −ϕ)+. Then, from (52)
we have
uε(x) = Rαu(x) =
 ∞
0
E[e−αsu(y(s, x))]ds = E[
 ∞
0
e−αsu(y(s, x))ds]
= E[
 τ
0
e−αsu(y(s, x))ds] + E[
 ∞
τ
e−αsu(y(s, x))ds].
(66)
Let
us
analyze
the
last
term
of
(66).
We
have
that
 ∞
0
E[e−α(s+τ)
|u(y(s + τ, x))|]ds ≤
 ∞
0
e−(α−α0)sp(h, x)ds < ∞. Then using Fubini Theorem
and strong Markov property (62) we get
E[
 ∞
τ
e−αsu(y(s, x))ds] =
 ∞
0
E[e−α(s+τ)u(y(s + τ, x))]ds
=
 ∞
0
E[E[e−α(s+τ)u(y(s + τ, x))|Fτ]]ds =
 ∞
0
E[e−ατΦα(s)u(y(τ, x))]ds
= E[e−ατRαu(y(τ, x))] = E[e−ατuε(y(τ, x))].
(67)

Optimal Stopping Problems
83
Hence, in virtue of (66) and (67), we have that
uε(x) = E[
 τ
0
e−αsu(y(s, x))ds] + E[
 ∞
τ
e−αsu(y(s, x))ds]
= E[
 τ
0
e−αsu(y(s, x))ds] + E[e−ατuε(y(τ, x))]
= E
  τ
0
e−αs[f −1
ε(uε −ϕ)+](y(s, x))ds + e−ατuε(y(τ, x))

.
(68)
On the other hand, from the deﬁnition of the seminorm p, it is evident that
E

e−ατ|uε(y(τ, x))−u0(y(τ, x))|

≤p(uε −u0, x) →0 when ε ↓0, where the last
convergence is due to Theorem 6. Then, using this last fact along with (68) and
Theorem 7, we obtain
u0(x) = lim
ε↓0 uε(x) ≤lim
ε↓0 E[
 τ
0
e−αsf(y(s, x))ds + e−ατuε(y(τ, x))]
= E[
 τ
0
e−αsf(y(s, x))ds + e−ατu0(y(τ, x))]
≤E[
 τ
0
e−αsf(y(s, x))ds + e−ατϕ(y(τ, x))] = J(x, τ).
Therefore, u0 ≤ˆu, after applying the inﬁmum over all τ in last rightmost term.
On the other hand, for each ε > 0, let us consider the stopping time
τε(x) := inf{t ≥0 : uε(y(t, x)) ≥ϕ(y(t, x))}.
Now take a sequence {tn}n∈N in [0, ∞) such that tn ↓τε(x) (pointwise w.r.t.
ω ∈Ω) and uε(y(tn, x)) ≥ϕ(y(tn, x)). Since t →uε(y(t, x)) −ϕ(y(t, x)) is
continuous a.s., we obtain uε(y(τε(x), x)) ≥ϕ(y(τε(x), x)) when tn ↓τε(x).
Then by (68), we deduce
uε(x) = E[
 τε
0
e−αs[f −1
ε(uε −ϕ)+](y(s, x))ds + e−ατεuε(y(τε, x))]
= E[
 τε
0
e−αsf(y(s, x))ds + e−ατεϕ(y(τε, x))] = J(τε, x) ≥ˆu(x).
This shows that u0 = limε↓0 uε ≥ˆu. Joining the pieces, we conclude that u0 = ˆu.
(b) Given ε > ε′, we know by the proof of Theorem 6 that uε ≥uε′ then we
have the expression
{s ≥0 : uε′(y(s, x)) ≥ϕ(y(s, x))} ⊆{s ≥0 : uε(y(s, x)) ≥ϕ(y(s, x))},
implying τε ≤τε′. So, there exists the monotone limit τε ↑τ0, as ε ↓0.
Also, because of the continuity of s →u0(y(s, x)) on [0, ∞) a.s., we have that
ϕ(y(ˆτ, x)) = u0(y(ˆτ, x)) ≤uε(y(ˆτ, x)), where ˆτ was deﬁned in (64). Hence, we
obtain τε ≤ˆτ that implies τ0 ≤ˆτ.

84
H. Jasso-Fuentes et al.
On the other hand, the fact s−lim uε = u0 gives us the existence of a sequence
εn, n ∈N, such that εn ↓0 and
lim
n→∞sup
s≥0
e−α0s|uεn(y(s, x)) −u0(y(s, x))| = 0,
a.s.,
(69)
where this last assertion is due to Lemma 1. Also, because of uεn ≥u0, we have
that
0 ≤uεn(y(τεn, x)) −u0(y(τεn, x)) ≤eα0τεn sup
s≥0
e−α0s|uεn(y(s, x)) −u0(y(s, x))|.
(70)
If τ0 = ∞then ∞= τ0 ≤ˆτ, so τ0 = ˆτ. Now, suppose τ0 < ∞a.s., then we have
that eα0τεn →eα0τ0, when n →∞. Hence, using (69), the right hand side of
inequality (70) converges to 0 when n →∞. Using Assumption 3 we deduce
ϕ(y(τ0, x)) = lim
n→∞ϕ(y(τεn, x)) ≤lim
n→∞uεn(y(τεn, x)) = u0(y(τ0, x)),
a.s.
Thus, the deﬁnition of ˆτ yields to ˆτ ≤τ0 and so, ˆτ = τ0. It remains to show that
u0(x) = J(ˆτ(x), x). Namely, consider ε0 > 0 ﬁxed. Given 0 < ε ≤ε0 and t ≤τε0
we know that t ≤τε and uε(y(t, x)) < ϕ(y(t, x)). Then the relation (68) leads to
uε(x) = E[
 τε0
0
e−αsf(y(s, x)) ds + e−ατε0 uε(y(τε0, x))].
By monotone convergence and quasi-left continuity of y(s, x), letting ε ↓0 and
hence ε0 ↓0, we obtain
u0(x) = E[
 τ0
0
e−αsf(y(s, x)) ds + e−ατ0ϕ(y(τ0, x))] = J(τ0, x).
Thus, we conclude that ˆτ is the optimal stopping time for the problem (48)–(50).
(c) Suppose ϕ ∈D(Aα) and let vε := 1
εRα+ 1
ε (f −Aαϕ)+. In virtue of (56)
and a variation of (61) we obtain
uε −ϕ ≤Rα+ 1
ε f + 1
εRα+ 1
ε ϕ −ϕ = Rα+ 1
ε f −Rα+ 1
ε Aαϕ,
which in turn gives (uε −ϕ)+ ≤Rα+ 1
ε (f −Aαϕ)+. Using this last inequality
together with (52), we get
f −Aαuε = 1
ε(uε −ϕ)+ ≤1
εRα+ 1
ε (f −Aαϕ)+ = vε,
or equivalently, f −vε ≤Aαuε, yielding that
Rα(f −vε) ≤uε,
(71)
after applying the resolvent operator in both sides of this later expression. Also
note that by (45), we know that bs −limε↓0 vε = (f −Aαϕ)+. Using this last

Optimal Stopping Problems
85
property, we can let ε ↓0 at (71) to deduce Rα(f−(f−Aαϕ)+) = Rα(f∧Aαϕ) ≤
u0.
On the other hand, using (52) again we have that Aαuε ≤f, or equivalently,
uε ≤Rαf. Letting ε ↓0, we obtain u0 ≤Rαf but also we have that u0 ≤ϕ
because it is a subsolution of (58). Then ˆu = u0 ≤Rαf ∧ϕ. Hence, we conclude
that
Rα(f ∧Aαϕ) ≤u0 ≤Rαf ∧ϕ.
⊓⊔
Acknowledgement. This research was partially founded by CONACyT grant no.
87787.
References
1. Anderson, W.: Continuous-Time Markov Chains. Springer, New York (1991)
2. Applebaum, D.: L´evy Processes and Stochastic Calculus. Cambridge University
Press, Cambridge (2009)
3. Bensoussan, A., Lions, J.L.: Applications des In´equations Variationnelles en
Contrˆole Stochastique. Dunod, Paris (1978)
4. Bensoussan, A.: Stochastic Control by Functional Analysis Methods. North-
Holland Publishing Co., Amsterdam (1982)
5. Bensoussan, A., Lions, J.L.: Applications of Variational Inequalities in Stochastic
Control. North-Holland Publishing Co., Amsterdam (1982)
6. Bensoussan, A.: Dynamic Programming and Inventory Control. IOS Press, Ams-
terdam (2011)
7. Bickel, P.J., El-Karoui, N., Yor, M.: Ecole d’Et´e de Probabilit´es de Saint-Flour XI
-1979. Springer, Berlin (1981)
8. B¨ottcher, B., Schilling, R., Wang, J.: L´evy Matters III. Springer, Cham (2013)
9. Chung, K.L.: Lectures from Markov Processes to Brownian Motion. Springer, New
York (1982)
10. Da Prato, G.: An Introduction to Inﬁnite-Dimensional Analysis. Springer, Berlin
(2006)
11. Da Prato, G., Zabczyk, J.: Stochastic Equations in Inﬁnite Dimensions. Cambridge
University Press, Cambridge (2014)
12. Dellacherie, C., Meyer, P.A.: Probabilit´es et Potentiel. Chapitres V `a VIII. Her-
mann, Paris (1980)
13. Ethier, S.N., Kurtz, T.G.: Markov Processes: Characterization and Convergence.
Wiley, New Jersey (1986)
14. El-Karoui, N., Kapoudjian, C., Pardoux, E., Peng, S., Quenez, M.C.: Reﬂected
solutions of backward SDEs and related obstacle problems for PDEs. Ann. Probab.
25, 702–737 (1997)
15. El-Karoui, N., Peng, S., Quenez, M.C.: Backward stochastic diﬀerential equation
in ﬁnance. Math. Financ. 7, 1–71 (1997)
16. Glowinski, R., Lions, J.L., Tr´emoli`eres, R.: Numerical Analysis of Variational
Inequalities. North-Holland Publishing Co., Amsterdam (1981)
17. Goran,
P.,
Shiryaev,
A.:
Optimal
Stopping
and
Free-Boundary
Problems.
Birkh¨auser Verlag, Basel (2006)

86
H. Jasso-Fuentes et al.
18. Hamad`ene, S., Jeanblanc, M.: On the starting and stopping problem: application
in reversible investments. Math. Oper. Res. 32, 182–192 (2007)
19. Horiguchi, M.: Stopped Markov decision processes with a stopping time constraint.
Math. Meth. Oper. Res. 53, 279–295 (2001)
20. Jasso-Fuentes, H., Menaldi, J.L., Prieto-Rumeau, T.: Discrete-time control with
non-constant discount factor. Math. Meth. Oper. Res. 92, 377–399 (2020)
21. Kallenberg, O.: Foundations of Modern Probabiliy. Springer, New York (2002)
22. Menaldi, J.L.: On the optimal stopping problem for degenerate diﬀusions. SIAM
J. Control. Optim. 18, 697–721 (1980)
23. Menaldi, J.L.: On the optimal impulse control problem for degenerate diﬀusions.
SIAM J. Control. Optim. 18, 722–739 (1980)
24. Menaldi, J.L.: Optimal impulse control problems for degenerate diﬀusions with
jumps. Acta Appl. Math. 8, 165–198 (1987)
25. Menaldi, J.L.: Stochastic hybrid optimal control models. In: Stochastic Models
(Guanajuato, 2000), II Aportaciones Mat. Investig., pp. 205–250. Soc. Mat. Mex-
icana, M´exico (2001)
26. Menaldi, J.L., Sritharan, S.S.: Stochastic 2-D Navier-Stokes equation. Appl. Math.
Optim. 46, 31–53 (2002)
27. Menaldi, J.L., Sritharan, S.S.: Remarks on impulse control problems for the
stochastic Navier-Stokes equations. In: Diﬀerential Equations and Control The-
ory (Athens, OH). Lecture Notes in Pure and Applied Mathematics, vol. 225, pp.
245–255. Dekker, New York (2002)
28. Menaldi, J.L., Sritharan, S.S.: Impulse control of stochastic Navier-Stokes equa-
tions. Nonlinear Anal. 52, 357–381 (2003)
29. Oksendal, B., Sulem, A.: Applied Stochastic Control of Jump Diﬀusions. Springer,
Berlin (2005)
30. Rieder, U.: On stopped decision processes with discrete time parameter. Stoch.
Proc. Appl. 3, 365–383 (1975)
31. Robin, M.: Contrˆole impulsionnel avec retard pour des processes de Markov. Ann.
Sci. Univ. Clermont 14, 115–128 (1976)
32. Robin, M.: Contr¨ole impulsionnel des processus de Markov. Thesis INRIA, TE-035,
Paris France (1977)
33. Rogers, L.C.G., Williams, D.: Diﬀusion, Markov Processes and Martingales. Cam-
bridge University Press, Cambridge (2000)
34. Stettner, L.: On some stopping and impulsive control problems with a general
discount rate criterion. Probab. Math. Statist. 10, 223–245 (1989)
35. Tudor, C.: Procesos estoc´asticos. Sociedad Matem´atica Mexicana, M´exico (2002)

Control of Continuous-Time Markov
Jump Linear Systems with Partial
Information
Andr´e Marcorin de Oliveira1 and Oswaldo Luiz do Valle Costa2(B)
1 Institute of Science and Technology,
Federal University of Sao Paulo (UNIFESP), S˜ao Jos´e dos Campos, SP, Brazil
andre.marcorin@unifesp.br
2 Escola Polit´ecnica da Universidade de S˜ao Paulo, S˜ao Paulo, Brazil
oswaldo@lac.usp.br
Abstract. In this paper we study the H2 state-feedback control of
continuous-time Markov jump linear systems considering that the mode
of operation cannot be directly measured. Instead we assume that there
is a detector that provides the only information concerning the main
jump process, so that the jump processes are modelled by a continuous-
time exponential hidden Markov model. We present a new convex design
condition for calculating a state-feedback controller depending only on
the detector which guarantees stability in the mean-square sense of the
closed-loop system, as well as a suitable bound on its H2 norm. We
present an illustrative example in the context of systems subject to faults
and compare our results with the current literature.
Keywords: H2 control · Hybrid systems · Continuous-time Markov
chain · Linear matrix inequalities
AMS(2020) Subject Classiﬁcation: Primary 93E03 · Secondary
90C25
1
Introduction
Lately a great deal of attention has been given to systems subject to sudden
changes in their dynamic behavior. This is due in part to the fact that real
worlds systems are subject to various alterations which can be caused inter-
nally or externally as, for instance, due to environmental conditions, faults in
dynamical systems, or changes to new operation points. Bearing that in mind,
modern control systems have to be designed with the capability of maintaining
an acceptable behavior and meeting some performance requirements even in the
presence of abrupt changes in the system dynamics. In order to derive treat-
able mathematical models for these situations, a class of systems that has been
recently intensively studied in the literature is of linear systems in which the
changes in their dynamics are modeled by a Markov chain (known as Markov
c
⃝The Author(s), under exclusive license to Springer Nature Switzerland AG 2021
A. Piunovskiy and Y. Zhang (Eds.): Modern Trends in Controlled Stochastic Processes,
ECC 41, pp. 87–107, 2021. https://doi.org/10.1007/978-3-030-76928-4_5

88
A. M. de Oliveira and O. L. do Valle Costa
jump linear systems, MJLS). It has gained a great boost in the early 1990s when,
among other applications, it was used to model fault-tolerant control systems
(see, for instance, [21,27]). We refer to [1,5,7,14,16,22,26] and references therein
for a general overview on MJLS and [20] for the application of MJLS in active
fault-tolerant control.
The literature on control of MJLS for the case in which the current state of
the Markov process (mode of operation) is perfectly known is nowadays quite
comprehensive but the case in which there is only a partial information on this
parameter is more scarce. Recently, there have been proposed some approaches
in the specialized literature to deal with the control problem for MJLS with
partial observations of the Markov chain, under diﬀerent names as the detector-
based approach (see, e.g., [6,30]); MJLS with hidden Markov models (see e.g.
[4,12,18]); or asynchronous MJLS (see [19,25]). It has a close connection to
the so-called active fault-tolerant control systems (AFTCS) in the sense that
it is assumed that the Markov chain θ is a failure process and we would have
access only to a type of failure detector ˆθ for designing the controller. In this
context, it was studied in [6] the H2-control (or quadratic control) problem of
discrete-time MJLS employing a detector-based approach for ˆθ. It was shown
in [6] that this approach encompasses the cases with perfect information, no
information and the cluster observations of the Markov parameter. Analogously,
the H∞control problem was studied in [30]. In [12], the mixed H2/H∞dynamic
output feedback control for discrete-time hidden MJLS was studied through a
type of iterative separation procedure. The continuous-time counterpart of the
H∞control problem was studied in [24], and [29], while the H2-control problem
was dealt with in [28], and the dynamic output control case for both the H2 as
well as the H∞was analyzed in [13]. In all these cases, it was assumed that the
dynamics of the detector follows a probabilistic Markov type assumption and an
explicit analytical expression for that has been exhibited.
More speciﬁcally, the mathematical representation of the model considered
in this chapter is given by a continuous-time linear system following the class of
diﬀerential equations given by
˙x(t) = Aθ(t)x(t) + Bθ(t)u(t), x(0) = x0, θ(0) = θ0.
(1)
where it is assumed that there is a continuous-time hidden Markov model (see,
for instance, [17]) Z(t) = (θ(t), ˆθ(t)) in which the change on the mode of oper-
ation (due, for instance, to a component failure), is modeled by the unobserved
component θ(t), while the observed component ˆθ(t) plays the role of the detector,
which provides the information on this change on the mode of operation (a fail-
ure detection and identiﬁcation device in the case of failures). In this problem we
are interested in controlling (1) under partial information on the mode of oper-
ation θ(t), that is, the goal is to ﬁnd a state-feedback control u(t) = Kˆθ(t)x(t)
such that the closed loop system
˙x(t) = (Aθ(t) + Bθ(t)Kˆθ(t))x(t)
(2)

Control of CT MJLS with Partial Information
89
meets some stability and performance index requirements. It was proposed in
[28] a linear matrix inequality (LMI) optimization formulation for the design
of a stochastic stabilizing feedback control with guaranteed H2-cost. For the
perfect information case (that is, ˆθ(t) = θ(t)) it was shown in [28] that these
results recast the usual ones for the H2 control of continuous-time-time MJLS
as presented in [5] provided a design parameter is made suﬃciently large. It was
also shown in [28] that this modeling encompasses the mode independent and
cluster observation cases considered in [31] for the discrete-time case.
The goal of this chapter is re-visit the H2-control problem studied in [28]
and derive a new set of conditions to design a stochastic stabilizing feedback
control with guaranteed H2-cost. Notice that for the general hidden Markov
model Z(t) = (θ(t), ˆθ(t)) the set of conditions obtained here and in [28] are
independent in the sense that it is not possible to say that one implies the other.
But, as in [28], we show that for the perfect information case (ˆθ(t) = θ(t)) these
conditions also recast the usual ones for the H2 control of continuous-time-time
MJLS as presented in [5] as long as a design parameter is made suﬃciently large.
This chapter is organized as follows. In Sect. 2 we introduce some notation
and auxiliary results that will be needed throughout this chapter. In Sect. 3 we
present the stochastic model, the concept of mean square stability needed in this
work, the quadratic performance index to be minimized, and the general opti-
mization problem. The main result of this chapter is presented in Sect. 4. For the
general partial observation case, Theorem 1 shows that if a set of LMI inequali-
ties are satisﬁed then we get a state-feedback control such that the closed loop
system is mean square stable and the associated quadratic performance index
satisﬁes an upper bound value. Moreover it will be shown that whenever we
assume the perfect information case (that is, ˆθ(t) = θ(t)), we recast the opti-
mal non-conservative solution for the control problem, provided that an input
parameter of the LMI inequalities is made suﬃciently large. Section 5 presents
an illustrative numerical example and Sect. 6 concludes the chapter with some
ﬁnal comments.
2
Notation and Preliminaries
The real Euclidean space of dimension n is represented by Rn, and the space of
real matrices of dimension m × n, by B(Rn, Rm), with B(Rn) ≜B(Rn, Rn). The
identity matrix of size n × n is given by In (or simply I), (· · · )′ is the transpose
operator and, for a square matrix G, we set Her(G) ≜G + G′, and Tr(·) is
the trace operator. Given positive integers N and M, we set N ≜{1, . . . , N},
M ≜{1, . . . , M}, and V ⊆N × M. The linear space composed by all sequence
of matrices V = (Vik ∈B(Rn, Rm); (ik) ∈V) is represented by Hn,m, and for
ease of notation we set Hn ≜Hn,n and Hn+ ≜{V ∈Hn : Vik ≥0, (ik) ∈V}.
Similarly we deﬁne the set Mn,m ≜{Mk ∈B(Rn, Rm), k ∈M}, Mn ≜Mn,n,
and Mn+ accordingly. For V ∈Hn+, by V > 0 we mean that Vik > 0 for all
(ik) ∈V (similarly for P > 0 in Mn+). We denote by o(h) any function f
such that limh→0
f(h)
h
= 0. (Ω, F, Prob) is a probability space equipped with

90
A. M. de Oliveira and O. L. do Valle Costa
a measurable right-continuous ﬁltration Ft. The expectation in this space if
represented by E(·), and the conditional expectation, by E(· | ·).
We recall the following results that will be useful along this chapter. For given
symmetric matrices Fi, i = 0, . . . , m, a strict linear matrix inequality (LMI) has
the form
F(x) = F0 +
m

i=1
xiFi > 0
where x =

x1 . . . xm
′ ∈Rm, xi ∈R, i = 1, . . . , m are the variables. A Semidef-
inite optimization programming (SDP optimization problem), also referred to as
an LMI optimization problem, consists of ﬁnding a feasible x (that is, ﬁnd x such
that F(x) > 0) which minimizes a linear function c′x. LMI optimization prob-
lems are tractable both from theoretical and numerical point of view (e.g. [2]).
A key result for converting nonlinear convex inequalities into LMI formulation
is the Schur complement, presented next.
Proposition 1. (Schur’s complement) The following aﬃrmatives are equiva-
lent:
a) Q=
Q11 Q12
Q′
12 Q22

> 0.
b) Q22 > 0 and Q11 −Q12Q−1
22 Q′
12 > 0.
c) Q11 > 0 and Q22 −Q′
12Q−1
11 Q12 > 0.
Notice that a) in Proposition 1 is in the form of a LMI, and b), c) is in
the form of nonlinear convex inequalities. SDP optimization problems include
several important standard classes of convex optimization problems, such as lin-
ear programming, quadratic programming, quadratically constrained quadratic
program, and second-order cone programming problems.
Some important results that will be used in this chapter are as follows.
Proposition 2 ([8,9]). For G ∈B(Rn) and P ∈B(Rn) such that P > 0, we get
that
G′P −1G ≥Her(G) −P.
(3)
Proposition 3 (Projection Lemma [2]). Given P, U, and V , there exists G
such that
P + Her(UGV ′) > 0
if and only if
˜U ′P ˜U > 0, ˜V ′P ˜V > 0
where ˜U and ˜V are, respectively, orthogonal complements of U and V .

Control of CT MJLS with Partial Information
91
3
Problem Formulation
In a probability space (Ω, F, Prob) we consider a continuous-time hidden Markov
model (CT-HMM) Z(t) = (θ(t), ˆθ(t)), t ∈R+, formed by two components, the
hidden state θ(t) taking values in the set N, and the observation state ˆθ(t) taking
values in the set M. It is assumed that Z(t) is a homogeneous Markov process
taking values in N ×M and having transition rates ν(ik)(jℓ), with ν(ik)(jℓ) ≥0 for
(jℓ) ̸= (ik) and −ν(ik)(ik) = 
(jℓ)̸=(ik) ν(ik)(jℓ). We assume that the transition
rates ν(ik)(jℓ) of Z(t) = (θ(t), ˆθ(t)), are given by
Prob(Z(t + h) = (jℓ) | Z(t) = (ik)) =

ν(ik)(jℓ)h + o(h), (jℓ) ̸= (ik)
1 + ν(ik)(ik)h + o(h), (jℓ) = (ik)
where
ν(ik)(jℓ) =
⎧
⎪
⎪
⎨
⎪
⎪
⎩
αk
jℓλij, i ̸= j, ℓ∈M,
qi
kℓ, j = i, ℓ̸= k, i ∈N,
λii + qi
kk, j = i, ℓ= k,
0, otherwise
and 
ℓ∈M αk
jℓ= 1, λij ≥0 for all i ̸= j, qi
kℓ≥0, ℓ̸= k, λii = −
j∈N i λij,
qi
kk = −
ℓ∈Mi qi
kℓ.
We represent by V ⊂N × M an invariant set of Z(t), that is, Prob(Z(t) ∈
V) = 1 provided that Z(0) ∈V.
Remark 1. Recalling that λij represents the transition rate of θ(t), we get that
αk
jℓand qi
kℓmodels simultaneous and spontaneous jumps of
ˆ
θ(t), that is, for
small h > 0, Prob(ˆθ(t + h) = ℓ| θ(t + h) = j, Z(t) = (ik)) = αk
jℓ+ r(h) for
some function such that limh→0 r(h) = 0, and Prob(ˆθ(t + h) = ℓ| θ(t + h) =
i, Z(t) = (ik)) = qi
kℓh + o(h), see [13,28], and the references therein for more
information.
As mentioned in Sect. 1 we consider the continuous-time MJLS (1) where
Ai ∈B (Rn), Bi ∈B(Rm, Rn), i ∈N, and with the state vector denoted by
x(t) ∈Rn and the control input by u(t) ∈Rm. We aim at designing the following
state-feedback controller
u(t) = Kˆθ(t)x(t)
(4)
that depends only on the observed variable ˆθ(t) taking values in M, such that
the closed loop system (2) is mean square stable (see Deﬁnition 1 below) and
has a guaranteed H2 cost (see Deﬁnition 2 below). In what follows we set K =
(K1, . . . , KM) and for i ∈N, ℓ∈M,
Aiℓ= Ai + BiKℓ.
(5)

92
A. M. de Oliveira and O. L. do Valle Costa
Remark 2. The following cases found in the literature can be recasted from the
approach presented above (see, for instance, [29]):
– Mode-dependent case: M = N, qi
kℓ= 0, αk
jj = 1, and αk
jℓ= 0 for j ̸= ℓ, with
invariant set V = {(ii) ∈N ×N}. In this case, we get that θ(t) = ˆθ(t) almost
surely (as).
– Mode-independent case: M = {1}, qi
kℓ= 0, and α1
j1 = 1. Then, ˆθ(t) = 1 and
θ(t1) and ˆθ(t2) jump with t1 = t2 as.
– No Mutual Jumps: αk
jk = 1 and αk
jℓ= 0 for k ̸= ℓ.
– The Cluster Case: We partition the Markov chain states as the union of M ≤
N disjoint sets (clusters) Ni so that N = ∪i∈MNi. Considering the function
g : N →M such that g(i) = j that represents the cluster where the Markov
state belongs to, the controller would have access to g(i). Equivalently, by
taking qi
kℓ= 0 and αk
ig(i) = 1, so that whenever θ(t) jumps to i, ˆθ(t) will
jump simultaneously to g(i).
We introduce next the concept of mean-square stability and the H2 norm.
Deﬁnition 1 (Mean-square stability MSS, adapted from [5]). System (2)
is said to be MSS if limt→∞E(∥x(t)∥2) = 0 for arbitrary x0 and Z(0).
We now introduce conditions for verifying the MSS of (2). For that we deﬁne
the linear operator T from Hn to Hn such that
Tik(P) ≜Her(A′
ikPik) +

(jℓ)∈V
ν(ik)(jℓ)Pjℓ
(6)
for P ∈Hn. We have the following lemma derived in Theorem 1 of [28].
Lemma 1. The system ˙x(t) = Aθ(t)ˆθ(t)x(t), x(0) = x0 ∈Rn, is MSS if and only
if there exists P ∈Hn+ such that
P > 0, T (P) < 0.
(7)
The set of admissible controllers (4) is given by
K ≜{K = (K1, . . . , KM) : such that (7) holds for Aiℓas in (5)}
We deﬁne next the concept of the H2 norm. In order to do that we consider
the following MJLS in the probability space (Ω, F, Prob)
G :

˙x(t) = Aθ(t)x(t) + Bθ(t)u(t) + Jθ(t)w(t)
z(t) = Cθ(t)x(t) + Dθ(t)u(t)
(8)
where, as before, x(t) ∈Rn, u(t) ∈Rm, and z(t) ∈Rq, w(t) ∈Rr. We also
consider that x(0) = 0 and that the initial probability distribution for θ0 satisﬁes

Control of CT MJLS with Partial Information
93
Prob(θ0 = i) = μi > 0. By plugging (8) and (4), we get the closed-loop system
yielding to
GK :

˙x(t) = Aθ(t)ˆθ(t)x(t) + Jθ(t)w(t)
z(t) = Cθ(t)ˆθ(t)x(t)
(9)
where Aiℓis as in (5) and
Ciℓ= Ci + DiKℓ.
(10)
We introduce the deﬁnition of the H2 norm next. Notice that the H2 norm
is associated to the minimization over K ∈K of the inﬁnite horizon quadratic
cost JK(x0, Z0) deﬁned by
JK(x0, Z0) ≜
 ∞
0
E

∥z(t)∥2
dt,
(11)
where x(0) = x0 and Z0 = (θ0, ˆθ0), see [5] for further details.
Deﬁnition 2 (H2 norm). Considering that (9) is MSS and x(0) = 0, the H2
norm of (9) is given by ∥GK∥2
2 ≜r
s=1 ∥zs∥2
2, where zs(k) is the controlled
output of (9) for w(t) = vsδ(t), vs is the s−th element of the standard basis of
Rr.
For calculating the H2 norm of (9) for a given stabilizing controller K, we
can resort to the following (convex) optimization problem
∥GK∥2
2 =
inf
Qik>0,γ γ2,
(12)

(ik)∈V
μikTr(J′
ikQikJik) < γ2
(13)
Her(QikAik) +

(jℓ)∈V
ν(ik)(jℓ)Qjℓ+ C′
ikCik < 0,
(14)
where Q ∈Hn+ and we recall that Prob(Z(0) = (ik)) = μik > 0, (ik) ∈V.
We are now able to formally state the main goal of this work, that is, for a
given γ > 0,
ﬁnd K ∈K such that ∥GK∥2 < γ.
(15)
For the perfect observation case, described in Remark 2 as the mode-depen-
dent case (that is, we have perfect information of θ(t) which corresponds to the
situation ˆθ(t) = θ(t)) we can obtain the optimal H2 controller by two methods,
the classical Riccati equation approach and the LMI approach. Both methods
are described in [5] as well as a connection between them, so that, the solution
for this case is not conservative in the sense that the optimal controller can be
numerically derived. On the other hand, for the more general case in which we

94
A. M. de Oliveira and O. L. do Valle Costa
could have a mismatch between ˆθ(t) and θ(t), optimality is lost at the expense
of a tractable (convex) formulation to the control problem so that only a bound
γ on the H2 norm of (9), which can be minimized, is guaranteed.
A question that naturally arises is that if the numerical procedure that we
derive for the general case recast the optimal solution whenever we assume the
perfect information case. In [28] we derived a numerical procedure based on
a LMI optimization problem that achieved this property. In the next section
we present a diﬀerent LMI optimization that also has this property, that is,
the results in Sect. 4 yield to the optimal H2 control whenever the assumptions
for the mode-dependent case described in Remark 2 are fulﬁlled. In this case,
we show that the conditions presented in Sect. 4 are equivalent to (13)–(14)
considering K as a decision variable in the (non-convex) optimization problem
∥G∗∥2
2 =
inf
K∈K,Qik>0,γ{γ2; such that the conditions in (13)−(14) hold}.
Remark 3. For K ∈K, we get that
JK(x0, Z0) ≤E(x′
0Qθ0 ˆθ0x0),
(16)
where JK(x0, θ0) is the quadratic cost deﬁned in (11) and Qik > 0 is any solution
of (14). Considering that x0 ∈Rn is a given known initial condition and recalling
that Prob(Z(0) = (ik)) = μik, we get that
JK(x0, Z0) ≤E(x′
0Qθ0 ˆθ0x0) = x′
0E(Qθ0 ˆθ0)x0 = x′
0

(ik)∈V
μikQikx0,
(17)
which is precisely the left-hand side of (13) for Jik = x0, (ik) ∈V. In this case,
it readily follows that JK(x0, Z0) = ∥GK∥2
2.
4
Main Result
In this section we present the main results of this chapter which consist
of obtaining, through an LMI optimization problem, a state-feedback control
u(t) = Kˆθ(t)x(t) such that the closed loop system (2) is MSS and the associated
H2 norm satisﬁes an upper bound value. Moreover it will be shown that when-
ever we assume the perfect information case (that is, ˆθ(t) = θ(t)), we recast the
optimal solution for the H2 control problem, provided that a design parameter is
made suﬃciently large. These results will be presented in Theorem 1, while the
LMI for the optimization problem will be deﬁned next. Consider the following
inequalities for (ik) ∈V,

Control of CT MJLS with Partial Information
95

(ik)∈V
μikTr(Wik) < ς
(18)
Wik
•
Ji Xik

> 0,
(19)
Hik + Her(ΨikΦik) < 0
(20)
Z(ik)(jl)
•
Hik
Xjl

> 0
(21)
Xik > 0,
(22)
where
Hik ≜
⎡
⎢⎢⎣
ν(ik)(ik)Xik
•
•
•
Xik
0n×n
•
•
Xik
0n×n −Her(Hik) + 
(jℓ)∈V(ik) ν(ik),(jℓ)Z(ik),(jℓ)
•
0q×n
0q×n
0q×n
−Iq
⎤
⎥⎥⎦,
Ψ ′
ik ≜

Inζik In 0n×n 0n×q

,
Φik ≜

(AiGk + BiYk)′ −G′
k 0n×n (CiGk + DiYk)′
,
with V(ik) = {(jℓ) ∈V : ν(ik),(jℓ) > 0}.
In the ﬁrst part of the next theorem we have a design LMI procedure based
on (18)–(22) for obtaining K satisfying (15), while in the second part we show
that for the perfect information case the existence of a solution for the LMI
inequalities (18)–(22) is also necessary for (15) provided that the parameters ζik
are made suﬃciently large.
Theorem 1. We have the following statements:
1. There exist ς > 0, ζik > ν(ik),(ik)/2, Z(ik),(jℓ), Hik, Wik, Xik, Gk, Yk, (ik) ∈V
such that (18)–(22) hold.
2. There exists K ∈K such that ∥GK∥2 < ς1/2.
We get that 1. =⇒2. with K = (K1, . . . , KM), Kk = YkG−1
k , k ∈M. Besides,
if the complete observation assumption of Remark 2 is fulﬁlled, by taking ζik
suﬃciently large, we get that 2. =⇒1.
Proof. 1.
=⇒
2.: Given that (18)–(22) holds, we get, by setting γ2 = ς and
Yk = KkGk, that

(ik)∈V
μikTr(Wik) < γ2,
(23)
Hik + Her(ΨikG′
k ¯Φik) < 0
(24)
holds, where
¯Φik ≜
A′
ik −In 0n×n C′
ik

.

96
A. M. de Oliveira and O. L. do Valle Costa
By deﬁning
Nik =
⎡
⎢⎢⎣
In
0
0
A′
ik 0 C′
ik
0 In
0
0
0
Iq
⎤
⎥⎥⎦
so that Rank(Nik) = 2n + q, we get that Nik is the orthogonal complement of
¯Φ
′
ik. From Proposition 3 (or by directly multiplying (24) to the left-hand side by
N
′
ik and to the right-hand side by Nik), we obtain that
Cik ≜N
′
ikHikNik =
⎡
⎣
ν(ik)(ik)Xik + Her(AikXik) Xik XikC′
ik
Xik
Mik
0n×q
CikXik
0q×n
−Iq
⎤
⎦< 0
(25)
holds, where Mik ≜−Her(Hik) + 
(jℓ)∈V(ik) ν(ik),(jℓ)Z(ik),(jℓ). Considering a
similar reasoning as employed in [3], by the Schur complement (see Proposi-
tion 1), we get that (21) yields Z(ik)(jℓ) > H′
ikX−1
jℓHik so that
Her(Hik) −H′
ik
⎛
⎝

(jℓ)∈V(ik)
ν(ik),(jℓ)X−1
jℓ
⎞
⎠Hik ≥−Mik.
By setting
G = Hik, P =


(jℓ)∈V(ik)
ν(ik),(jℓ)X−1
jℓ
−1
(26)
in Proposition 2, we get that P ≥Her(G) −G′P −1G ≥−Mik. Thus,
⎡
⎢⎣
ν(ik)(ik)Xik + Her(AikXik)
Xik
XikC′
ik
Xik
−

(jℓ)∈V(ik) ν(ik),(jℓ)X−1
jℓ
−1
0n×q
CikXik
0q×n
−Iq
⎤
⎥⎦< 0.
By applying the congruence transformation diag(X−1
ik , In, Iq), permuting some
rows and columns, and using the Schur complement (Proposition 1) in the last
inequality, we get that (14) holds for Qik = X−1
ik . Similarly, by applying the
Schur complement (see Proposition 1) to (19), we get that Wik > J′
iX−1
ik Ji, then
by multiplying this equation by μik, summing everything up for all (ik) ∈V and
considering (23), we get (13), and the claim follows.
2. =⇒1.: If the complete observation hypothesis of Remark 2 is fulﬁlled
(that is, ˆθ(t) = θ(t)), we get that V = {(ii) : i = 1, . . . , N}, ν(ii)(jj) = λij and

(ii)∈V
μiiTr(J′
iiQiiJii) < γ2
2
(27)
Her(QiiAii) +

(jj)∈V
ν(ii)(jj)Qjj + C′
iiCii < 0,
(28)
Qii > 0
(29)

Control of CT MJLS with Partial Information
97
holds for some mode-dependent MS-stabilizing controller K = (K1, . . . , KN).
Considering a similar reasoning as presented in [3] and the references therein,
we deﬁne Xii ≜Q−1
ii
along with
Z(ii)(jj) ≜
⎛
⎝
(jj)∈V
ν(ii),(jj)X−1
jj
⎞
⎠
−1
X−1
jj
⎛
⎝
(jj)∈V
ν(ii),(jj)X−1
jj
⎞
⎠
−1
+ Iϵ
>
⎛
⎝
(jj)∈V
ν(ii),(jj)X−1
jj
⎞
⎠
−1
X−1
jj
⎛
⎝
(jj)∈V
ν(ii),(jj)X−1
jj
⎞
⎠
−1
(30)
for some small ϵ > 0. Then, after applying the Schur complement (see Proposi-
tion 1) to (30) and setting Hii =

(jj)∈V(ii) ν(ii),(jj)X−1
jj
−1
, we get that (21)
holds. By directly applying the Schur complement, Proposition 1, to (28), we
get that
⎡
⎢⎣
Qiiν(ii)(jj) + Her(QiiAii)
•
•
I
−

j∈V(ii) ν(ii)(jj)Q−1
jj
−1
•
Cii
0
−I
⎤
⎥⎦< 0
(31)
Multiplying (31) by diag(Xii, I, I), we then get that
⎡
⎢⎣
Xiiν(ii)(jj) + Her(AiiXii)
•
•
Xii
−

j∈V(ii) ν(ii)(jj)X−1
jj
−1
•
CiiXii
0
−I
⎤
⎥⎦< 0
(32)
holds. Besides,
−Mii = Her(Hii) −

(jj)∈V(ii)
ν(ii)(jj)HiiX−1
jj Hii + ν(ii)ϵI
= Hii + ν(ii)ϵI.
(33)
By choosing the small perturbation ϵ > 0 to (32) such that
⎡
⎣
Xiiν(ii)(jj) + Her(AiiXii)
•
•
Xii
−

Hii + ν(ii)ϵI

•
CiiXii
0
−I
⎤
⎦< 0
(34)
still holds, and using (33), we get (25). The remainder of the proof is partially
inspired in [23]. We now deﬁne
Mi ≜Cii + Dii

98
A. M. de Oliveira and O. L. do Valle Costa
where Cii is given by (25) and
Dii ≜
1
2ζii
⎡
⎣
Aii
0
Cii
⎤
⎦Xii

A′
ii 0 C′
ii

=
⎡
⎣
Aii
Xii
ζii
0
C′
ii
Xii
ζii
⎤
⎦ζii
2 X−1
ii

Xii
ζii A′
ii 0 Xii
ζii C′
ii
 
≥0,
(35)
since Xii > 0. Note that limζii→∞Mi = Cii < 0, so that, by taking suitable
ζii large enough, we get that Mi < 0. Deﬁne Gi ≜Xii/ζii, so that Her(Gi) =
2Xii/ζii. For this suitable choice of ζii > 0, we get that
Mii = Cii +
⎡
⎣
AiiGi
0
CiiGi
⎤
⎦Her(Gi)−1 
G′
iA′
ii 0 G′
iC′
ii

< 0.
By applying the Schur complement (see Proposition 1) to the last inequality,
and recalling that Xii = Giζii, we get that
⎡
⎢⎢⎣
ν(ii)(ii)Xii + ζiiHer(AiiGi)
•
•
•
Xii
Mii
•
•
CiiGiζii
0q×n −Iq
•
G′
iA′
ii
0
G′
iC′
ii −Her(Gi)
⎤
⎥⎥⎦< 0
(36)
holds. By commuting suitable rows and columns, we get that
⎡
⎢⎢⎣
ν(ii)(ii)Xii + ζiiHer(AiiGi)
•
•
•
G′
iA′
ii
−Her(Gi)
•
•
Xii
0
Mii
•
CiiGiζii
CiiGi
0q×n −Iq
⎤
⎥⎥⎦< 0.
(37)
Finally, by deﬁning Yi = KiGi and recalling that Xii −Giζii = 0, we get that
(20) holds, and the claim follows.
2
The best upper bound for our main goal (15) can be calculated by solving
the following LMI optimization problem
inf
ξ∈Ξ(ζ) ς
(38)
where ξ = (ς, Z(ik),(jℓ), Hik, Wik, Xik, Gk, Yk) and Ξ(ζ) is the set of solutions of
(18)–(22) for a given ζ = (ζik : (ik) ∈V).
Remark 4. Note that if ζik ≤ν(ik),(ik)/2, then (19) is unfeasible. Indeed, through
the Projection Lemma of Proposition 3, by setting U = Φik and taking the
orthogonal complement ˜U as
˜U =
⎡
⎢⎢⎣
−In 0 0
Inζik 0 0
0
In 0
0
0 Iq
⎤
⎥⎥⎦,

Control of CT MJLS with Partial Information
99
we get that if (19) holds, then
⎡
⎣
ν(ik)(ik)Xik −2Xikζik
•
•
−Xik
−Her(Hik) + 
(jℓ)∈V(ik) ν(ik),(jℓ)Z(ik),(jℓ)
•
0
0
−Iq
⎤
⎦< 0
also holds. Therefore, a necessary condition for the last inequality to hold is that
Xik(ν(ik)(ik) −2ζik) < 0. Since Xik > 0, we must have that ζik > ν(ik)(ik)/2, for
all (ik) ∈V.
5
Illustrative Example
In this example, we consider the linearized model of the unstable lateral dynamics
of an unmanned aircraft discussed in [15]. The original, nonlinear, model is
obtained by considering a rigid-body motion, assuming that Earth is locally
ﬂat, so that centripetal acceleration caused by the its curvature is neglected, and
also that Earth is an inertial (Galilean) frame so that the Coriolis acceleration
is ignored. Then the nonlinear model follows by using classical (Newtonian)
mechanics. The state x =

¯p ¯r β φ

is composed by variations on the roll rate
¯p, the yaw rate ¯r, the sideslip angle β, and the roll angle φ. The control input
u′ =
δa δr

is given by variations on the aileron δa and on the rudder δr.
The linearization is performed around the nominal conditions ¯pnom = ¯qnom =
¯rnom = 0, θnom = αnom, βnom = 0, and φnom = 0, where ¯qnom is the nominal
pitch rate, θnom is the nominal pitch angle, and αnom is the nominal angle of
attack, considering that the aircraft ﬂies at a straight and level ﬂight, a constant
altitude of 500 above sea level, assuming a constant speed of 30 m/s. Therefore,
the nominal matrices are given by
Anom =
⎡
⎢⎢⎣
−11.4540 2.7185 −19.4399
0
0.5068
−2.9875 23.3434
0
0.0922
−0.9957 −0.4680 0.3256
1
0.0926
0
0
⎤
⎥⎥⎦, Bnom =
⎡
⎢⎢⎣
78.4002 −2.7282
−3.4690 13.9685
0
0
0
0
⎤
⎥⎥⎦.
(39)
We consider that the aircraft is subject to actuator faults that can be modeled by
the Markov chain θ(t) whose states represent three possible modes of operation:
the nominal one = θ(k) = 1 so that B1 = Bnom; the case in which the actuator
power is reduced to 50% θ(t) = 0, B2 = 0.5Bnom; and the case in which the
actuator power is reduced to 10% B3 = 0.1Bnom0. That is, N = {1, 2, 3}. Also,
Ai = Anom, ∀i ∈N. Similarly to [13], we consider that the fault rates are given
by
[λij] =
⎡
⎣
−0.3 0.2
0.1
1.1 −1.5 0.4
1.0
1.0 −2.0
⎤
⎦.
(40)

100
A. M. de Oliveira and O. L. do Valle Costa
Fig. 1. γ and ∥GK∥2 against ¯ζ for the complete observation case
The main goal is to investigate the H2 control through the lens of the LQR
control as discussed in Remark 3. Then, we set
Ci =
 I4
02×4

, Di =
04×2
I2

(41)
for all i ∈N, that is, we choose the same weights for all states and control
inputs. We consider the initial condition
x0 =

0 0 0.087 −0.087
′
(42)
so that Ji = x0, i ∈V, considering the reasoning of Remark 3.
Let us ﬁrst assume that we have a perfect fault detector so that ˆθ(t) = θ(t)
for all t and consider the invariant set
V = {(1, 1), (2, 2), (3, 3)}
(43)
μ =
0.7808 0.1502 0.0691
. Then, we calculate the optimal H2 control by solving
(38) for ¯ζii = ζ ∈{10−5, 10−4, 10−3, 10−2, 10−1, 1}, i ∈N. Finally, for the control
K ∈K obtained by solving (38), we calculate the H2 norm of the closed-loop
system resorting to (12)–(14). The upper bound γ and ∥GK∥2 is shown in Fig. 1.
In this example, we note that the conservatism between the upper bound
yielded by (38) and the actual H2 norm is readily decreased by increasing ¯ζ, as
discussed in Theorem 1.
We now study the partial observation case and consider three possible detec-
tor outputs so that M = N. We consider that the detector can perfectly detect

Control of CT MJLS with Partial Information
101
the nominal mode of operation, that is, ˆθ(t) = 1 whenever θ(t) = 1. However, the
detector may have diﬃculties in distinguishing between θ(t) = 2 and θ(t) = 3.
In this case, the invariant set is given by
V = {(11), (22), (23), (32), (33)}
(44)
and the transition rate matrix is given by
[ν(ik),(jℓ)] =
⎡
⎢⎢⎢⎢⎣
λ11
λ12α1
22
λ12α1
23
λ13α1
32
λ13α1
33
λ21 λ22 + q2
22
q2
23
λ23α2
32
λ23α2
33
λ21
q2
32
λ22 + q2
33
λ23α3
32
λ23α3
33
λ31
λ32α2
22
λ32α2
23
λ33 + q3
22
q3
23
λ31
λ32α3
22
λ32α3
23
q3
32
λ33 + q3
33
⎤
⎥⎥⎥⎥⎦
,
(45)
where the states sequence in the transition matrix is given by (11), (22), (23),
(32), and (33). We note that, by restricting the invariant set to (44), we auto-
matically impose that q1
11 = 0 and αk
11 = 1, k ∈M.
We ﬁrst investigate the case in which only simultaneous jumps occur by
varying αk
jℓ, that is, the probability of the detector going to ℓgiven that its
current state is k and the next Markov state is j. We also consider that
αk
22 = ¯α2, αk
33 = ¯α3, k ∈M,
for 0 < ¯αi < 1, i ∈{2, 3}, along with the following regions
Region 1: ¯αi = 0, V = {(11), (23), (32)}
Region 2: 0 < ¯αi < 1, V as in (44)
Region 3: ¯αi = 1, V as in (43)
for i ∈{2, 3}. The spontaneous rates are set to zero, that is, qi
kℓ= 0. We
solve (38) by varying ¯α2 and ¯α3 and calculate the actual H2 norm for each
case. In each iteration, we set the initial distribution of Z(t) as the stationary
distribution and ζik = 10, (ik) ∈{(11), (22), (23), (32), (33)}. The upper bounds
γ and ∥GK∥2 are shown in Fig. 2 against ¯α3 and ¯α2. The result of this simulation
traces a parallel to the discrete-time hidden MJLS approach of [12] considering
the behavior of γ and ∥GK∥2 with respect to variations on αk
jℓ(αil for the discrete-
time formulation). We note that we get the perfect observation case in Region
3. Interestingly the same conﬁguration is obtained in Region 1: since we know
for sure that the detector will jump to ˆθ(t + h) = 3 if θ(t + h) = 2 (and vice-
versa), then we know which mode of operation we have in this situation. Finally,
there is a worst-case line for α3 = 1 −α2 in which all costs and controllers are
numerically close and achieves their maximum value, that is, γ = 0.1345 and
∥GK∥2 = 0.1329, with control gains given by
K1 =

−0.8814 −0.0167 −0.1129 −1.0753
−0.0087 −0.8221 −0.0084
0.0281

,
K2 ≈K3 ≈
−0.5170 −0.0437 −0.0681 −0.6962
−0.1603 −0.3536 −0.1232 −0.0985

.

102
A. M. de Oliveira and O. L. do Valle Costa
Fig. 2. γ and ∥GK∥2 against ¯α2 and ¯α3
By analysing the control gains, we note that there are two clusters (sets), {1}
and {2, 3} that naturally arises from solving (38) with the given probabilities.
All those cases we previously explained are similar to the ones studied in [10–12],
and the references therein, for discrete-time hidden MJLS.
Let us now study the case in which only spontaneous jumps (no mutual
jumps, see Remark 2) occur for the modes {2, 3} so that αk
jk = 1 for all j ∈{2, 3},
k ∈{2, 3} (recalling that 
ℓ∈{2,3} αk
jℓ= 1, j ∈{2, 3}, k ∈{2, 3}). We set
q2
22 = q2
33 = q3
22 = q3
33 = −¯q
so that
[ν(ik),(jℓ)] =
⎡
⎢⎢⎢⎢⎣
λ11 λ12α1
22 λ12α1
23 λ13α1
32 λ13α1
33
λ21 λ22 −¯q
¯q
λ23
0
λ21
¯q
λ22 −¯q
0
λ23
λ31
λ32
0
λ33 −¯q
¯q
λ31
0
λ32
¯q
λ33 −¯q
⎤
⎥⎥⎥⎥⎦
,
(46)
and again, the states sequence in the transition matrix is given by (11), (22),
(23), (32), and (33). By inspecting (46), we note that the choice of V as in (44)
imposes that simultaneous jumps will occur if θ(t) = 1 and θ(t + h) = 2 (or
θ(t + h) = 3). In this case, we set α1
22 = α1
33 = ¯α. By varying ¯q ∈

0.01 1.00

for ¯α ∈

0.05 0.95

, we solve (38) and calculate the H2 norm of the resulting
closed-loop system with (12). The upper bound γ2 against ¯q and ¯α are shown in
Fig. 3. By ﬁxing ¯q, we note that the behavior of γ2 is similar to the one displayed
in Fig. 2 for ¯α3 = ¯α2. That is, the smallest upper bounds are obtained if ¯α →0

Control of CT MJLS with Partial Information
103
Fig. 3. γ2 against ¯q and ¯α
or ¯α →1. Conversely, the worst-case scenario is also given by ¯α = 0.5. On the
other hand, we note that, by increasing ¯q, we get that γ2 also increases, since ¯q
increases the uncertainty of the detector, as discussed in [28].
Let us now suppose a more general case in which V = {(ik), i ∈N, k ∈M},
that is, we consider all possible combinations of i and k. We set
αk
11 = 1, αk
22 = αk
33 = 0.7, k ∈M,
(47)
for all k ∈M, along with
[qi
kℓ] =
⎡
⎣
−1 1/3 2/3
1/3 −1 2/3
1/3 2/3 −1
⎤
⎦
(48)
for all i ∈N. We now compare our results to the ones given in [28]. By varying
the parameter ζik = ¯ζ of Theorem 1, for all (ik) ∈V, and ζℓ= ¯ζ of Theorem
5 of [28], for all ℓ∈M, for ζ > 0, we obtain the upper bounds γ1 and γ2, as
well as ∥G(1)
K ∥2 and ∥G(2)
K ∥2, shown in Fig. 4. In this example, we note that the
upper bounds γ1 obtained through Theorem 1 are smaller compared to the ones,
γ2, yielded by Theorem 5 of [28].The smallest upper bound obtained through
(38) is given by γ∗
1 = 0.1403, for an actual H2 norm of ∥G1
K∥2 = 0.1334 whereas
we get that γ∗
2 = 0.2059 and ∥G2
K∥2 = 0.1424 obtained through Theorem 5 of
[28], both for ¯ζ = 4. Concerning the conservatism of both results, that is, the
distance between the upper bounds and the actual H2 norm, we note that it
tends to decrease as we increase ζ, albeit not necessarily monotonically. Besides,
we note that γ∗
1/∥G1
K∥2 = 1.0517 and γ∗
2/∥G2
K∥2 = 1.4458 for ¯ζ = 4. Thus, for this
example, the conservatism yielded by the conditions of Theorem 1 are smaller
compared to Theorem 5 of [28].

104
A. M. de Oliveira and O. L. do Valle Costa
Fig. 4. Top ﬁgure: γ1 (full line) and ∥G(1)
K ∥2 (dashed line) calculated through (38)
against ¯ζ; Bottom ﬁgure: γ2 (full line) and ∥G(2)
K ∥2 (dashed line) against ¯ζ calculated
through Theorem 5 of [28].
Fig. 5. ∥z(t)∥2 (grey lines) and E(∥z(t)∥2) (black line) against t for a Monte Carlo
simulation of 500 rounds.

Control of CT MJLS with Partial Information
105
Finally, we run a Monte Carlo simulation of 500 rounds and take the trajec-
tories ∥z(t)∥2 against time for the detector probabilities given in (47) and (48),
respectively. The initial condition is given in (42) and we set ¯ζ = ζik = 4. The
∥z(t)∥2 curves, along with E(∥z(t)∥2) are shown in Fig. 5.
By numerically integrating E(∥z(t)∥2), we get that
JK(x0, θ0) ≈0.0179.
(49)
Considering Remark 3, we get, by simulation, that ∥GK∥2 =
!
JK(x0, θ0) ≈
0.134, whereas the actual H2 norm value is given by ∥GK∥2 = 0.133.
6
Conclusion
In this chapter, we revisited the H2 state-feedback control of continuous-time
Markov jump linear systems considering that the main Markov chain cannot be
directly measured. We consider that the only information available of the main
jump process comes from a detector. We assume that the joint process of the pro-
cess of the plant and the detector follows an extended exponential Markov pro-
cess, the so-called Exponential Hidden Markov Model. This modelling is appeal-
ing to represent systems subject to faults. We present new suﬃcient conditions
for calculating state-feedback controllers depending on the detector that stabilize
the closed-loop system while guaranteeing a bound on its H2 norm. In the case
in which the detector is able to provide the correct information regarding the
jump process of the plant, the so-called perfect observation case, our conditions
also become necessary, leading to the optimal H2 state-feedback controller. We
numerically compare our conditions to the ones already presented in the litera-
ture through illustrative examples in the context of networked control systems
and systems subject to faults.
Acknowledgement. The second author was partially supported by Conselho
Nacional de Desenvolvimento Cient´ıﬁco e Tecnol´ogico (CNPq), process No. 304149/
2019 −5, by FAPESP/Shell Research Center for Gas Innovation (RCGI) process
FAPESP No. 2014/50279 −4, and by Instituto Nacional de Ciˆencia e Tecnologia para
Sistemas Autˆonomos Cooperativos (INSAC), process CNPq/INCT −465755/2014 −3
and FAPESP/INCT-2014/50851 −0.
References
1. Boukas, E.K.: Stochastic Switching Systems: Analysis and Design. Birkh¨auser,
Basel (2006)
2. Boyd, S., El Ghaoui, L., Feron, E., Balakrishnan, V.: Linear Matrix Inequalities in
System and Control Theory. SIAM Studies in Applied and Numerical Mathematics.
SIAM, Philadelphia (1994)
3. Cardeliquio, C.B., Fioravanti, A.R., Gon¸calves A.P.C.: H2 and H∞state-feedback
control of continuous-time MJLS with uncertain transition rates. In: 2014 ECC,
pp. 2237–2241 (2014)

106
A. M. de Oliveira and O. L. do Valle Costa
4. Cheng, J., Ahn, C.K., Karimi, H.R., Cao, J., Qi, W.: An event-based asynchronous
approach to Markov jump systems with hidden mode detections and missing mea-
surements. IEEE Trans. Syst. Man Cybern. Syst. 49(9), 1749–1758 (2019)
5. Costa, O.L.V., Fragoso, M.D., Todorov, M.G.: Continuous-Time Markov Jump
Linear Systems. Springer, Berlin-Heidelberg-New York (2013)
6. Costa, O.L.V., Fragoso, M.D., Todorov, M.G.: A detector-based approach for the
H2 control of Markov jump linear systems with partial information. IEEE Trans.
Automat. Contr. 60(5), 1219–1234 (2015)
7. Costa, O.L.V., Tuesta, E.F.: H2-control and the separation principle for discrete-
time Markovian jump linear systems. Math. Control Signals Syst. 16(4), 320–350
(2004)
8. Daafouz, J., Bernussou, J.: Parameter dependent Lyapunov functions for discrete
time systems with time varying parametric uncertainties. Syst. Control Lett. 43(5),
355–359 (2001)
9. de Oliveira, M.C., Bernussou, J., Geromel, J.C.: A new discrete-time robust sta-
bility condition. Syst. Control Lett. 37(4), 261–265 (1999)
10. de Oliveira, A.M., Costa, O.L.V.: H2-ﬁltering for discrete-time hidden Markov
jump systems. Int. J. Control 90(3), 599–615 (2017)
11. de Oliveira, A.M., Costa, O.L.V.: Mixed H2/H∞control of hidden Markov jump
systems. Int. J. Robust Nonlinear Control 28(4), 1261–1280 (2018)
12. de Oliveira, A.M., Costa, O.L.V.: An iterative approach for the discrete-time
dynamic control of Markov jump linear systems with partial information. Int. J.
Robust Nonlinear Control 30(2), 495–511 (2020)
13. de Oliveira, A.M., Costa, O.L.V., Fragoso, M.D., Stadtmann, F.: Dynamic out-
put feedback control for continuous-time Markov jump linear systems with hidden
Markov models. Int. J. Control (2021, in print)
14. Dragan, V., Morozan, T., Stoica, A.-M.: Mathematical Methods in Robust Control
of Linear Stochastic Systems (Mathematical Concepts and Methods in Science and
Engineering). Springer, New York (2010)
15. Ducard, G.J.J.: Fault-Tolerant Flight Control and Guidance Systems. Springer,
London-New York (2009)
16. Dufour, F., Elliott, R.J.: Adaptive control of linear systems with Markov pertur-
bations. IEEE Trans. Automat. Contr. 43(3), 351–372 (1998)
17. Elliott, R.J., Aggoun, L., Moore, J.B.: Hidden Markov Models. Springer, New York
(1995)
18. Li, F., Xu, S., Zhang, B.: Resilient asynchronous H∞control for discrete-time
Markov jump singularly perturbed systems based on hidden Markov model. IEEE
Trans. Syst. Man Cybern. Syst. 50(8), 2860–2869 (2020)
19. Liu, X., Ma, G., Pagilla, P.R., Ge, S.S.: Dynamic output feedback asynchronous
control of networked Markovian jump systems. IEEE Trans. Syst. Man Cybern.
Syst. 50(7), 2705–2715 (2020)
20. Mahmoud, M., Jiang, J., Zhang, Y.: Active Fault Tolerant Control Systems -
Stochastic Analysis and Synthesis. Springer, Germany (2003)
21. Mariton, M.: Detection delays, false alarm rates and the reconﬁguration of control
systems. Int. J. Control 49, 981–992 (1989)
22. Mariton, M.: Jump Linear Systems in Automatic Control. CRC Press, New York
(1990)
23. Morais, C.F., Braga, M.F., Oliveira, R.C.L.F., Peres, P.L.D.: H2 and H∞control
design for polytopic continuous-time Markov jump linear systems with uncertain
transition rates. Int. J. Robust Nonlinear Control 26(3), 599–612 (2016)

Control of CT MJLS with Partial Information
107
24. Rodrigues, C.C.G., Todorov, M.G., Fragoso, D.M.: H∞control of continuous-time
Markov jump linear systems with detector-based mode information. Int. J. Control
90(10), 2178–2196 (2017)
25. Shen, Y., Wu, Z., Shi, P., Su, H., Huang, T.: Asynchronous ﬁltering for Markov
jump neural networks with quantized outputs. IEEE Trans. Syst. Man Cybern.
Syst. 49(2), 433–443 (2019)
26. Shi, P., Li, F.: A survey on Markovian jump systems: modeling and design. Int. J.
Control Autom. Syst. 13(1), 1–16 (2015)
27. Srichander, R., Walker, B.K.: Stochastic stability analysis for continuous-time fault
tolerant control systems. Int. J. Control 57(2), 433–452 (1993)
28. Stadtmann, F., Costa, O.L.V.: H2-control of continuous-time hidden Markov jump
linear systems. IEEE Trans. Automat. Contr. 62(8), 4031–4037 (2017)
29. Stadtmann, F., Costa, O.L.V.: Exponential hidden Markov models for H∞control
of jumping systems. IEEE Control Syst. Lett. 2(4), 845–850 (2018)
30. Todorov, M.G., Fragoso, M.D., Costa, O.L.V.: Detector-based H∞results for
discrete-time Markov jump linear systems with partial observations. Automatica
91, 159–172 (2018)
31. Val, J.B.R., Geromel, J.C., Gon¸calves, A.P.C.: The H2-control for jump linear sys-
tems: cluster observations of the Markov state. Automatica 38(2), 343–349 (2002)

Q-Learning for Distributionally Robust
Markov Decision Processes
Nicole B¨auerle(B) and Alexander Glauner
Department of Mathematics, Karlsruhe Institute of Technology,
76128 Karlsruhe, Germany
{nicole.baeuerle,alexander.glauner}@kit.edu
Abstract. In this paper, we consider distributionally robust Markov
Decision Processes with Borel state and action spaces and inﬁnite time
horizon. The problem is formulated as a Stackelberg game where nature
as a second player chooses the least favorable disturbance density in each
scenario. Under suitable assumptions, we prove that the value function
is the unique ﬁxed point of an operator and that minimizers respectively,
maximizers lead to optimal policies for the decision maker and nature.
Based on this result, we introduce a Q-learning approach to solve the
problem via simulation-based techniques. We prove the convergence of
the Q-learning algorithm and study its performance using a distribution-
ally robust irrigation problem.
Keywords: Markov decision process · Robust optimization ·
Q-learning
AMS (2020) Subject Classiﬁcation: Primary 90C40 · Secondary
68T05 · 90C17
1
Introduction
The theory of Markov Decision Processes (MDPs) which developed after the
groundbreaking work by Richard Bellman (see e.g. [3] or the reprint [4]) has
been shown to be extremely useful for solving stochastic dynamic decision prob-
lems. Areas of application are among others production planning, operations
management, control of robots, scheduling in queueing networks, investment
management and health care decisions. The starting point of the theory is a
model where the state transition function, the cost function and the distribu-
tion of the disturbances are known or can be estimated with suﬃcient precision.
Whereas the transition function is often given due to physical laws, in many
cases it might not be possible or very costly to determine the true distribution
of the disturbances. Hence, there is some kind of model uncertainty or ambigu-
ity in the problem. There are various ways to deal with this uncertainty (for
an overview in the ﬁeld of economics see e.g. [13]). In this paper, we approach
the problem by considering distributionally robust MDPs. More precisely, this
c
⃝The Author(s), under exclusive license to Springer Nature Switzerland AG 2021
A. Piunovskiy and Y. Zhang (Eds.): Modern Trends in Controlled Stochastic Processes,
ECC 41, pp. 108–128, 2021. https://doi.org/10.1007/978-3-030-76928-4_6

Q-Learning for Distributionally Robust MDP
109
means that we consider a stochastic dynamic game against nature where nature
as a second player tries to choose the least favorable disturbance distribution
whereas the decision maker tries to minimize her expected discounted cost. We
implement this as a Stackelberg game where the decision maker has to reveal
her action ﬁrst and then nature chooses the disturbance distribution. This can
be seen as a worst-case approach.
Distributionally robust MDPs with ﬁnite state space have been considered
before in [11,19] on a theoretical basis, both for ﬁnite and inﬁnite planning
horizon. In [1] the ﬁnite horizon case has been extended to a situation with Borel
state and action spaces and unbounded cost function. The major obstacle here is
a sensible introduction of policies for nature. A similar situation is also considered
in [7,12]. In both papers, there is a classical game structure with a predetermined
order of actions of both players. In particular, the model assumptions and the
choice of the ambiguity set are diﬀerent from our paper. In the present paper, we
consider as ambiguity set the set of densities and thus use a diﬀerent topology.
The advantage is to obtain some relations to dynamic risk measures, see [1].
Indeed, relations like this have been discovered in the economic literature before.
There, it is common to speak of model ambiguity. For an overview of the recent
literature see [8]. We also use diﬀerent, two-sided bounding functions. In [20]
another approach is used, where nested uncertainty sets for the transition laws
are considered which correspond to conﬁdence sets.
In the current paper, we will extend the results in [1] to a setting with inﬁnite
time horizon. Under some assumptions on the continuity and compactness of
the model data and under some growth conditions we will show that the value
function of the model is the unique ﬁxed point of a certain operator and that
minimizers respectively, maximizers in the optimality equation lead to optimal
policies for the decision maker and nature. Based on this result, we provide
a Q-learning approach to solve the problem numerically via simulation-based
techniques. To the best of our knowledge, this has not been done before. Q-
learning can be seen as a combination of value iteration and simulation and also
works in the case of a game. Other MDP algorithms like policy improvement
cannot be generalized to games in an easy way. Q-learning determines the so-
called Q-function from which we can derive the value function immediately.
We prove the convergence of the algorithm and study its performance using a
distributionally robust irrigation problem. The model is considered with diﬀerent
sizes of the state space and diﬀerent learning rates. In this application, the state
space and the action space of the decision maker are ﬁnite.
The paper is organized as follows: In the next section, we introduce our model
and the optimization problem. We clarify in particular our ambiguity set. In
Sect. 3, we summarize our assumptions and explain the solution theorem which
consists of a ﬁxed point statement. We use the weighted supremum norm to deal
with the unbounded cost function and rely on Banach’s ﬁxed point theorem.
In the subsequent section, we discuss the relation of our optimization criterion
to risk measures. Section 5 contains the theory of the Q-learning algorithm and

110
N. B¨auerle and A. Glauner
proves in particular its convergence. In Sect. 6, the algorithm is applied to the
irrigation example. In particular, the inﬂuence of the learning rate is discussed.
2
The Markov Decision Model
We consider the following stationary Markov Decision Process with state space
E, action space A and inﬁnite planning horizon. Both state and action space are
assumed to be Borel spaces with Borel σ-algebras B(E) and B(A), respectively.
The possible state-action combinations are a measurable subset D ⊂E ×A such
that D contains the graph of a measurable mapping. The x-section
D(x) = {a ∈A : (x, a) ∈D}
is the set of admissible actions in state x ∈E. We assume that the dynamics of
the MDP depend on disturbances Z1, Z2, . . . which are i.i.d. random elements
on a common probability space ⊗∞
n=1(Ω, A, P) with values in a measurable space
(Z, Z). W.l.o.g. we assume that Zn((ω1, ω2, . . .)) = ωn. Let Z be a representative
of the disturbance variables. When the current state is xn, the controller chooses
action an ∈D(xn) and zn+1 is the realization of Zn+1, then the next state is
given by
xn+1 = T(xn, an, zn+1),
where T : D × Z →E is a measurable transition function. The one-stage cost
function c : D × E →R gives the cost c(x, a, x′) for choosing action a if the
system is in state x and the next state is x′.
In what follows we will restrict w.l.o.g. to deterministic Markovian policies,
for more details see [1].
Deﬁnition 1. A measurable mapping d : E →A with d(x) ∈D(x) for every
x ∈E is called decision rule. A sequence π = (d0, d1, . . . ) is called
policy. The
set of all policies is denoted by Π. A policy π is called stationary if π = (d, d, . . . )
for some decision rule d.
We denote by (Xn), (An) the random state and action processes. In the
sequel, we will require P to be separable. The transition kernel is given by
Q(B|x, a) :=

1B

T(x, a, z)

P(dz),
B ∈B(E), (x, a) ∈D.
(1)
We assume now that there is some uncertainty about P, e.g. because it cannot
be estimated properly. Moreover, the decision maker is very risk averse and tries
to minimize the expected cost on a worst case basis. We denote by M1(Ω, A, P)
the set of probability measures on (Ω, A) which are absolutely continuous with
respect to P and deﬁne for q ∈(1, ∞]
Mq
1(Ω, A, P) :=

Q ∈M1(Ω, A, P) : dQ
dP ∈Lq(Ω, A, P)

.

Q-Learning for Distributionally Robust MDP
111
Henceforth, we ﬁx a non-empty subset Q ⊆Mq
1(Ω, A, P) which is referred to as
ambiguity set. This can be seen as the set of probability measures which may
reﬂect the law of motion. Due to absolute continuity, we can identify Q with the
set of corresponding densities w.r.t. P
Qd :=
dQ
dP ∈Lq(Ω, A, P) : Q ∈Q

.
Accordingly, we view Q as a subset of Lq(Ω, A, P) and endow it with the trace
topolgy of the weak* topolgy σ(Lq, Lp) on Lq(Ω, A, P) where 1
p + 1
q = 1. The
weak* topology in turn induces a Borel σ-algebra on Q making it a measurable
space. We obtain the following result (for a proof see the appendix of [1]).
Lemma 1. Let the ambiguity set be norm-bounded (see (A)(vi) below) and the
probability measure P on (Ω, A) be separable. Then Q endowed with the weak*
topology σ(Lq, Lp) is a separable metrizable space. If Q is additionally weak*
closed, it is even a compact Borel space.
The controller only knows that the transition kernel (1) at each stage is
deﬁned by some Q ∈Q instead of P but not which one exactly. For example it
could be known that the disturbances are normally distributed, but mean and
variance are not precisely known, i.e.
Q =

N(μ, σ2) : μ ∈[μ1, μ2], σ ∈[σ1, σ2]

.
Since all moments of the normal distribution exist, such an ambiguity set with
compact parameter intervals is bounded in the Lq-norm and Lemma 1 applies.
The controller’s worst-case approach can be interpreted as a dynamic game
against nature. This means that nature reacts to the controller’s action a ∈D(x)
at time n with a measurable decision rule γn : D →Q. A policy of nature is a
sequence of such decision rules γ = (γ0, γ1, . . . ). Let Γ be the set of all policies
of nature. Thus, we are faced with a Stackelberg game where the controller is
the mover and nature is the follower. A proof of the next lemma can be found
in the appendix of [1].
Lemma 2. A decision rule γ : D →Q induces a stochastic kernel from D to Ω
by
γ(B|x, a) := γ(x, a)(B),
B ∈A, (x, a) ∈D.
As in the case without ambiguity, the Theorem of Ionescu-Tulcea yields that
each initial state x ∈E and pair of policies of the controller and nature (π, γ) ∈
Π × Γ induce a unique law of motion
Qπγ
x
:= δx ⊗γ0(·|x0, d0(x0)) ⊗γ1(·|x1, d1(x1)) ⊗. . .
with corresponding expectation operator Eπγ
x .

112
N. B¨auerle and A. Glauner
The value of a policy pair (π, γ) ∈Π × Γ under an inﬁnite planning horizon
is deﬁned as
J∞πγ(x) := Eπγ
x
	 ∞

k=0
βkc(Xk, dk(Xk), Xk+1)

,
x ∈E.
(2)
The corresponding robust value of a policy π ∈Π of the controller is then the
worst case cost
J∞π(x) := sup
γ∈Γ
J∞πγ(x),
x ∈E.
Hence, the optimality criterion is to minimize this worst case cost
J∞(x) := inf
π∈Π J∞π(x),
x ∈E.
(3)
3
Solution Theory for the Distributionally Robust MDP
In order to solve the problem we make the following assumptions:
Assumptions (A)
(i) The set-valued mapping E ∋x 	→D(x) is upper semicontinuous and
compact-valued.
(ii) The transition function T is continuous in (x, a).
(iii) The one-stage cost function c is lower semicontinuous.
(iv) There exist α, ϵ, ¯ϵ ≥0 with ϵ + ¯ϵ = 1 and measurable functions b : E →
(−∞, −ϵ] and ¯b : E →[¯ϵ, ∞) such that for all Q ∈Q and (x, a) ∈D
EQ 
−c−(x, a, T(x, a, Z))

≥b(x),
EQ [b(T(x, a, Z))] ≥αb(x),
EQ 
c+(x, a, T(x, a, Z))

≤¯b(x),
EQ ¯b(T(x, a, Z))

≤α¯b(x).
(v) We deﬁne b : E →[1, ∞), b(x) := ¯b(x) −b(x). For all (¯x, ¯a) ∈D there
exists an ϵ > 0 and measurable functions Θ¯x,¯a
1
, Θ¯x,¯a
2
: Z →R+ such that
Θ¯x,¯a
1
(Z), Θ¯x,¯a
2
(Z) ∈Lp(Ω, A, P) and
|c(x, a, T(x, a, z))| ≤Θ¯x,¯a
1
(z),
b(T(x, a, z)) ≤Θ¯x,¯a
2
(z)
for all z ∈Z and (x, a) ∈Bϵ(¯x, ¯a) ∩D. Here, Bϵ(¯x, ¯a) is the closed ball
around (¯x, ¯a) w.r.t. an arbitrary product metric on E × A.
(vi) The ambiguity set Q is norm bounded, i.e. ∃K ∈[1, ∞) such that
E

dQ
dP

q
≤K
for all Q ∈Q.
(vi) The discount factor β satisﬁes αβ < 1 with α from (iv).

Q-Learning for Distributionally Robust MDP
113
Remark 1. a) Conditions (i)–(iii) and (v) are needed to ensure the existence of
optimal policies. Condition (iv) guarantees that the value functions we are
interested in have a ﬁnite weighted supremum norm with weight function b.
Condition (vi) is a requirement for Lemma 1 and the last condition ensures
the contraction property of the optimality operator.
b) Note that when E and A are ﬁnite, conditions (i)–(vi) are automatically
satisﬁed. In particular b can be chosen as a constant and α = 1.
It is convenient to introduce the corresponding ﬁnite horizon problems. For
horizon N ∈N and policies π ∈Π, γ ∈Γ, we set
JNπγ(x) := Eπγ
x
	N−1

k=0
βkc(Xk, dk(Xk), Xk+1)

,
x ∈E.
Moreover, let JNπ = supγ∈Γ JNπγ and JN = infπ∈Π JNπ. We ﬁrst make sure
that (2) is well-deﬁned.
Lemma 3. Under Assumptions (A) the sequences {JNπγ}N∈N, {JNπ}N∈N and
{JN}N∈N converge pointwise for every policy pair (π, γ) ∈Π ×Γ to limits which
are bounded by
1
1−αβ b from below and by
1
1−αβ¯b from above. Moreover, it holds
lim
N→∞JNπγ = J∞πγ(x),
x ∈E.
Proof. We have for 1 ≤m ≤N
JNπγ(x) = Jmπγ(x) +
N−1

k=m+1
βkEπγ
x [c(Xk, dk(Xk), Xk+1)]
≥Jmπγ(x) +
N−1

k=m+1
βkEπγ
x

−c−(Xk, dk(Xk), Xk+1)

≥Jmπγ(x) + b(x)
N−1

k=m+1
(αβ)k
≥Jmπγ(x) + b(x)
∞

k=m
(αβ)k
=: Jmπγ(x) + δm(x)
(4)
where δm is a non-positive function with limm→∞δm(x) = 0 for all x ∈E.
Hence, the sequence of functions {JNπγ}N∈N is weakly increasing. Taking the
supremum over γ (and inﬁmum over π) on both sides of (4), yields that the
sequences {JNπ}N∈N and {JN}N∈N are weakly increasing, too. By Theorem
A.1.6 in [2] all three sequences are convergent. Moreover, we can apply Theorem
A3 in [10] which yields
J∞πγ(x) = lim
N→∞Eπγ
x
	N−1

k=0
βkc(Xk, dk(Xk), Xk+1)

= lim
N→∞JNπγ(x).

114
N. B¨auerle and A. Glauner
In the same way as (4) we can prove that
JNπγ(x) ≤Jmπγ(x) + ¯b(x)
∞

k=m
(αβ)k
(5)
Choosing m = 0 and taking the limit N →∞in (4) and (5) yields
1
1 −αβ b(x) ≤J∞πγ(x) ≤
1
1 −αβ
¯b(x).
For the other limits the same bounds obviously hold, too.
⊓⊔
The pointwise limits
Jπ(x) := lim
N→∞JNπ(x)
and
J(x) := lim
N→∞JN(x),
x ∈E,
are referred to as limit robust policy value of π ∈Π and limit value function,
respectively.
Remark 2. The inﬁnite horizon and limit robust policy values and value func-
tions have the following relations.
(i) It holds for any policy pair (π, γ) ∈Π × Γ that JNπγ ≤JNπ. By taking the
limit N →∞it follows that J∞πγ ≤Jπ and ﬁnally by taking the supremum
over γ ∈Γ
J∞π(x) ≤Jπ(x),
x ∈E.
(ii) It holds for any policy π ∈Π that JN ≤JNπ. Taking limits yields
J(x) ≤Jπ(x),
x ∈E.
With the bounding function b = ¯b −b we deﬁne the function space
Bb := {v : E →R | v measurable, ∃λ ∈R+ s.t. |v(x)| ≤λ b(x) ∀x ∈E} .
Endowing Bb with the weighted supremum norm
∥v∥b := sup
x∈E
|v(x)|
b(x)
makes (Bb, ∥·∥b) a Banach space, cf. Proposition 7.2.1 in [9]. Note that according
to Lemma 3 and Theorem 3.6 and 3.10 in [1] we have J, Jπ, J∞πγ ∈Bb. To ease
the notation we introduce the following operators.
Deﬁnition 2. For functions v ∈Bb and for all (x, a) ∈D, Q ∈Q and decision
rules d,γ let
Lv(x, a, Q) :=

c

x, a, T(x, a, z)

+ βv

T(x, a, z)

Q(dz),
Td,γv(x) := Lv(x, d(x), γ(x, d(x))),
Tdv(x) := sup
Q∈Q
Lv(x, d(x), Q),
T v(x) :=
inf
a∈D(x) sup
Q∈Q
Lv(x, a, Q).

Q-Learning for Distributionally Robust MDP
115
For the next result deﬁne
B := {v ∈Bb | v lower semicontinuous}
which is again a complete metric space.
Lemma 4. Given Assumptions (A), the Bellman operator T is a contraction
on B with modulus αβ ∈(0, 1).
Proof. Let v ∈B. It has been established in the proof of Theorem 3.10 in [1]
that T v is lower semicontinuous. Furthermore,
|T v(x)| =
 inf
a∈D(x) sup
Q∈Q
EQ 
c

x, a, T(x, a, Z)

+ βv

T(x, a, Z)

≤
inf
a∈D(x) sup
Q∈Q
EQ c

x, a, T(x, a, Z)

+ βEQ v

T(x, a, Z)

≤
inf
a∈D(x) sup
Q∈Q
EQ c

x, a, T(x, a, Z)

+ βλEQ 
b

T(x, a, Z)

≤(1 + αβλ)b(x),
Hence, the operator T is an endofunction on B and it remains to verify the
Lipschitz constant αβ. It holds for v1, v2 ∈B
T v1(x) −T v2(x) ≤
sup
a∈D(x)

sup
Q∈Q
EQ 
c

x, a, T(x, a, Z)

+ βv1

T(x, a, Z)

−sup
Q∈Q
EQ 
c

x, a, T(x, a, Z)

+ βv2

T(x, a, Z)
 
≤β
sup
a∈D(x)
sup
Q∈Q
EQ 
v1

T(x, a, Z)

−v2

T(x, a, Z)

≤β∥v1 −v2∥b
sup
a∈D(x)
sup
Q∈Q
EQ 
b

T(x, a, Z)

≤αβ∥v1 −v2∥bb(x).
Interchanging the roles of v1 and v2 yields
|T v1(x) −T v2(x)| ≤αβ∥v1 −v2∥bb(x).
Now, dividing by b(x) and taking the supremum over x ∈E on the left hand
side completes the proof.
⊓⊔
The following theorem is a consequence of Proposition 3.5, Theorem 3.6 and
Theorem 3.10 in [1]. It is crucial for our main result.
Theorem 1. Let Assumptions (A) be satisﬁed and policies π = (d0, d1, . . .) ∈Π
and γ = (γ0, γ1, . . . ) ∈Γ be given with ¯π := (d1, d2, . . .), ¯γ := (γ1, γ2, . . . ).
a) For all N ∈N we have JNπγ = Td0,γ0JN−1¯π¯γ.
b) For all N ∈N we have JNπ = Td0JN−1¯π.

116
N. B¨auerle and A. Glauner
c) For all N ∈N we have JN = T JN−1 and JN ∈B.
The next theorem is our main result. It characterizes the value function and
explains how optimal policies for the decision maker and nature can be obtained.
Theorem 2. Let Assumptions (A) be satisﬁed.
a) The limit value function J is the unique ﬁxed point of the Bellman operator
T in B.
b) There exists a decision rule d∗: E →A of the controller such that
Td∗J(x) = T J(x),
x ∈E.
Moreover, for every ϵ > 0 there exists an ϵ-optimal decision rule ˆγ0 : D →Q
of nature such that
Td∗ˆγ0J(x) + ϵ ≥T J(x),
x ∈E.
c) If the ambiguity set Q is weak* closed, there exists a decision rule γ∗
0 : D →Q
of nature such that
Td∗γ∗
0 J(x) = T J(x),
x ∈E.
d) Each stationary policy π∗= (d∗, d∗, . . . ) induced by a decision rule d∗as in
part b) is optimal for optimization problem (3) and it holds J∞= J.
e) If the ambiguity set Q is weak* closed, each stationary policy γ∗= (γ∗
0, γ∗
0, . . . )
induced by a decision rule γ∗
0 as in part c) is an optimal response of nature
to π∗, i.e. J∞π∗γ∗= J∞.
Proof. a) The fact that J is the unique ﬁxed point of the operator T in B follows
directly from Banach’s Fixed Point Theorem using Lemma 4.
b) The existence of a minimizing decision rule of the controller and an ϵ-optimal
decision rule of nature follow from the respective results in the ﬁnite horizon
case, cf. Theorem 3.10 a) in [1].
c) This follows analogously from Theorem 3.10 b) in [1].
d) Let d∗, ˆγ0 be decision rules as in part b) and π∗:= (d∗, d∗, . . . ), ˆγ :=
(ˆγ0, ˆγ0, . . . ). It has to be shown that
J∞π∗(x) = J∞(x) = J(x),
x ∈E.
(6)
We proceed in two steps. Firstly, we prove that
J(x) ≥J∞π∗(x),
x ∈E
(7)
and secondly we prove that
J(x) ≤J∞π(x),
x ∈E,
for all π ∈Π.
(8)
From (7) we get J ≥J∞π∗≥J∞. On the other hand, taking the inﬁmum
over π ∈Π in (8) yields J ≤J∞. Together, these inequalities imply (6) and

Q-Learning for Distributionally Robust MDP
117
the assertion is proven.
Step 1: We show by induction that for all N ∈N0
J(x) ≥JNπ∗(x) + (αβ)N
1 −αβ b(x),
x ∈E.
Then letting N →∞yields (7). The case N = 0 follows from Lemma 3. For
N ≥1 it follows from the induction hypothesis
J(x) = Td∗J(x)
= sup
Q∈Q
EQ 
c

x, d∗(x), T(x, d∗(x), Z)

+ βJ

T(x, d∗(x), Z)

≥sup
Q∈Q
EQ
	
c

x, d∗(x), T(x, d∗(x), Z)

+ βJN−1π∗
T(x, d∗(x), Z)

+ β (αβ)N−1
1 −αβ b

T(x, d∗(x), Z)


≥sup
Q∈Q
EQ
	
c

x, d∗(x), T(x, d∗(x), Z)

+ βJN−1π∗
T(x, d∗(x), Z)


+ (αβ)N
1 −αβ b(x)
= JNπ∗(x) + (αβ)N
1 −αβ b(x).
Note that the last inequality is by Assumption (A) (ii) and the last equality
by Theorem 1 b).
Step 2: Let π = (d0, d1, . . . ) ∈Π be arbitrary. We show by induction for ε
and ˆγ from b) that for all N ∈N0
J(x) ≤JNπˆγ(x) +
ϵ
1 −β + (αβ)N
1 −αβ
¯b(x),
x ∈E.
Then letting N →∞yields J ≤J∞πˆγ +
ϵ
1−β . Since ϵ > 0 is arbitrarily small,
it follows that J ≤J∞π, i.e. (8) holds. The case N = 0 follows again from

118
N. B¨auerle and A. Glauner
Lemma 3. For N ≥1 we have
J(x) = T J(x) ≤Td∗ˆγ0J(x) + ϵ ≤Td0ˆγ0J(x) + ϵ
≤Td0ˆγ0

JN−1¯πˆγ(x) +
ϵ
1 −β + (αβ)N−1
1 −αβ
¯b(x)

+ ϵ
=

c

x, d0(x), T(x, d0(x), z)

+ βJN−1¯πˆγ

T(x, d0(x), z)

+ β (αβ)N−1
1 −αβ
¯b

T(x, d0(x), z)

ˆγ0(dz|x, d0(x)) +

1 +
β
1 −β

ϵ
= JNπˆγ(x) + β (αβ)N−1
1 −αβ

¯b

T(x, d0(x), z)

ˆγ0(dz|x, d0(x)) +
ϵ
1 −β
≤JNπˆγ(x) + β (αβ)N−1
1 −αβ
sup
Q∈Q
EQ ¯b

T(x, d0(x), Z)

+
ϵ
1 −β
≤JNπˆγ(x) + (αβ)N
1 −αβ
¯b(x) +
ϵ
1 −β .
We used that π ∈Π is arbitrary, so it is no problem to apply the induction
hypothesis to the shifted policy ¯π. The third equality is by Theorem 1 a).
e) Replacing the ϵ-optimal decision rule ˆγ0 by the optimal one γ∗
0 in step 2 of
part d) yields J ≤J∞πγ∗for all π ∈Π, so especially J ≤J∞π∗γ∗. Combining
this with (6), we get
J ≤J∞π∗γ∗≤J∞π∗= J∞= J,
which concludes the proof.
⊓⊔
Remark 3. Note that we do not have a classical game here. In particular it is
not possible in general to interchange sup and inf. Additional properties like
convexity are required to achieve this. For a discussion and examples, see [1].
4
Connection to Risk Measures
In this section, we outline how distributionally robust MDPs are related to the
minimization of coherent risk measures. This provides another interpretation
of the optimality criterion (3) in addition to the worst-case approach and the
dynamic Stackelberg game. A risk measure is a functional ρ : Lp(Ω, A, P) →¯R
which determines the necessary capital to make holding a risky position X ∈
Lp(Ω, A, P) acceptable. The following properties are important.
Deﬁnition 3. A risk measure ρ : Lp(Ω, A, P) →¯R is
a) monotone if X ≤Y implies ρ(X) ≤ρ(Y ).
b) translation invariant if ρ(X + m) = ρ(X) + m for all m ∈R.
c) positive homogeneous if ρ(λX) = λρ(X) for all λ ∈R+.

Q-Learning for Distributionally Robust MDP
119
d) subadditive if ρ(X + Y ) ≤ρ(X) + ρ(Y ) for all X, Y .
e) coherent if it has properties a)–d).
f) said to have the Fatou property, if for every sequence {Xn}n∈N ⊆Lp with
|Xn| ≤Y P-a.s. for some Y ∈Lp and Xn →X P-a.s. for some X ∈Lp it
holds
lim inf
n→∞ρ(Xn) ≥ρ(X).
Recall that an extended real-valued convex functional is called proper if it
never attains −∞and is strictly smaller than +∞in at least one point. Coherent
risk measures have the following dual or robust representation, cf. Theorem 7.20
in [16].
Theorem 3. A functional Lp(Ω, A, P) →¯R is a proper coherent risk measure
with the Fatou property if and only if there exists a subset Q ⊆Mq
1(Ω, A, P)
such that
ρ(X) = sup
Q∈Q
EQ[X],
X ∈Lp.
(9)
The supremum is attained since the subset Q ⊆Mq
1(Ω, A, P) can be chosen
σ(Lq, Lp)-compact and the functional Q 	→EQ[X] is σ(Lq, Lp)-continuous.
With this duality result we can reformulate the right hand side of the ﬁxed
point equation J = T J from Theorem 2 to
J(x) =
inf
a∈D(x) ρ

c

x, a, T(x, a, Z)

+ βJ

T(x, a, Z)

(10)
for some proper coherent risk measure ρ with the Fatou property if and only if
the ambiguity set Q is weak* closed. Note that we already require Q to be norm
bounded, cf. Assumption (A) (vi), and by the Theorem of Banach-Alaoglu weak*
compact is equivalent to norm bounded and weak* closed. Equation (10) shows
that for a weak* closed ambiguity set the distributionally robust optimality
criterion is equivalent to the stage-wise minimization of a coherent risk measure.
Due to this connection, the dual representations of coherent risk measures
are a natural source for ambiguity sets. A particular advantage is that there are
often explicit formulas for nature’s maximizing probability measure. We present
two examples. Since the probability measures in Q are absolutely continuous
w.r.t. P, we can consider the set of densities Qd.
(i) Expected Shortfall is deﬁned on L1(Ω, A, P) as
ESα(X) :=
1
1 −α
 1
α
F −1
X (u)du,
α ∈[0, 1),
with F −1
X
denoting the quantile function of X. Its dual representation is based
on the set of densities
Qd =

Y ∈L∞(Ω, A, P) : E[Y ] = 1, Y ≤
1
1 −α

.

120
N. B¨auerle and A. Glauner
The supremum (9) is attained in
Y = 1{X > F −1
X (α)} + κ1{X = F −1
X (α)}
1 −α
with κ = 1−α−P(X>F −1
X (α))
P(X=F −1
X (α))
1

P(X = F −1
X (α)) > 0

, see Remark 8.15 in [14].
(ii) A superclass are the spectral risk measures ρφ : Lp(Ω, A, P) →¯R. They are
of the form
ρφ(X) :=
 1
0
F −1
X (u)φ(u)du,
where φ : [0, 1] →R+ is an increasing function with ∥φ∥q < ∞and
 1
0 φ(u)du = 1 called spectrum. Expected Shortfall is a special case with
spectrum φ(u) = 1{u≥α}
1−α . The dual representation of spectral risk measures
is given by the set of densities
Qd = {Y ∈Lq(Ω, A, P) : Y ≤cx φ(U), U ∼U(0, 1)} .
The maximizing density in (9) is φ(UX), where UX is the generalized distribu-
tional transform of X, i.e. a uniformly distributed random variable satisfying
almost surely F −1
X (UX) = X, see Corollary 12 in [15].
The connection of distributionally robust MDPs to coherent risk measures
goes beyond the stage-wise perspective of (10). The optimality criterion (3) can
be written as
J∞(x) = inf
π∈Π sup
Q∈Qπ
EQ
x
	 ∞

k=0
βkc(Xk, dk(Xk), Xk+1)

,
x ∈E,
where Qπ = {Qπγ
x
: γ ∈Γ}. By direct veriﬁcation of the axioms one can
see that for a ﬁxed policy π ∈Π of the controller ˜ρ(X) = supQ∈Qπ EQ[X],
X ∈Lp(Ω, A, P), deﬁnes a coherent risk measure. I.e. in some sense the stage-
wise connection (10) holds also globally. If the ambiguity set Q is induced by a
spectral risk measure and the model data has certain monotonicity properties,
Q is independent of π, cf. Lemma 6.8 and subsequent remarks in [1]. In this
case, the distributionally robust expected cost optimization is equivalent to the
minimization of a coherent risk measure applied to the total cost.
5
Q-Learning for Distributionally Robust Models
We want to obtain J = J∞and the optimal policy numerically. In order to
achieve this, we use a Q-learning algorithm. For simplicity let us assume now that
state and action space are ﬁnite as well as the ambiguity set. Thus Assumptions
(A) (i)–(vi) are automatically satisﬁed (see Remark 1). We only have to assume

Q-Learning for Distributionally Robust MDP
121
that β < 1. In what follows it will be more convenient to work with the densities
Qd instead of Q. The ﬁxed point equation of Theorem 2 a) reads
J(x) = T J(x) =
inf
a∈D(x) sup
Y ∈Qd

z
P(z)Y (z)

c(x, a, T(x, a, z)) + βJ(T(x, a, z))

.
The Q-function of the problem is for (x, a, Y ) ∈D × Qd given by
Q(x, a, Y ) := LJ(x, a, Y ).
It is the value when we take the pair (a, Y ) as the ﬁrst action of the decision
maker and nature and act optimally afterwards. In particular, we have J(x) =
T J(x) = infa∈D(x) supY ∈Qd Q(x, a, Y ). Thus, we obtain
Q(x, a, Y ) =

z
P(z)Y (z)

c(x, a, T(x, a, z)) + β
inf
a′∈D(x) sup
Y ′∈QdQ(T(x, a, z), a′, Y ′)

=: HQ(x, a, Y )
(11)
The operator H is slightly diﬀerent to T , however they share the following
important property. In what follows we denote by ∥·∥∞the supremum norm on
the Banach space of bounded functions.
Theorem 4. The operator H is a contraction on the space of bounded functions
with modulus β ∈(0, 1) and Q is the unique ﬁxed point of the H-operator in the
set of bounded functions.
Proof. First note that when Q is bounded, HQ is bounded, too. Now take two
bounded functions Q1, Q2 on D × Qd. Then
(HQ1 −HQ2)(x, a, Y )
= β

z
P(z)Y (z)

inf
a′ sup
Y ′ Q1(T(x, a, z), a′, Y ′) −inf
a′ sup
Y ′ Q2(T(x, a, z), a′, Y ′)

≤β

z
P(z)Y (z) sup
a′ sup
Y ′

Q1(T(x, a, z), a′, Y ′) −Q2(T(x, a, z), a′, Y ′)

≤β∥Q1 −Q2∥∞
Interchanging Q1 and Q2 ﬁnally yields ∥HQ1 −HQ2∥∞≤β∥Q1 −Q2∥∞and
implies that H is contracting. Thus, it follows from Banach’s ﬁxed point theorem
that the ﬁxed point of H in the set of bounded functions is unique. That Q is a
ﬁxed point follows from (11).
⊓⊔
We consider the following iteration with numbers αt ≥0 called learning rate
and satisfying limt→∞αt = 0. We start with Q(0) ≡0. In each step, we choose
randomly a feasible pair (x, a, Y ), generate z according to P and update Q(t).

122
N. B¨auerle and A. Glauner
Algorithm:
1. Set Q(0) ≡0.
2. Choose a pair (x, a, Y ) at random (uniformly over D × Qd) and generate z
according to P.
3. Update the value at (x, a, Y ) by
Q(t+1)(x, a, Y ) = (1 −αt)Q(t)(x, a, Y ) + αtY (z)

c(x, a, T(x, a, z))
+β min
a′ max
Y ′ Q(t)(T(x, a, z), a′, Y ′)

and set Q(t+1)(·) = Q(t)(·) for all other arguments.
It is now possible to prove that the iteration converges to the Q-function.
Theorem 5. If the numbers (αt) are chosen such that
∞

t=0
αt = ∞
and
∞

t=0
α2
t < ∞,
then {Q(t)}t∈N0 converges with probability 1 to Q for t →∞.
Proof. Note that we can write the iteration as
Q(t+1)(x, a, Y ) = (1 −αt)Q(t)(x, a, Y )
+αtY (z)

c(x, a, T(x, a, z)) + β min
a′ max
Y ′ Q(t)(T(x, a, z), a′, Y ′)

= (1 −αt)Q(t)(x, a, Y ) + αt

HQ(t)(x, a, Y ) + wt(x, a, Y )

where
wt(x, a, Y ) = Y (z)

c(x, a, T(x, a, z)) + β min
a′ max
Y ′ Q(t)(T(x, a, z), a′, Y ′)

−HQ(t)(x, a, Y ).
The statement follows from Proposition 4.4 in [5] since H is contracting and the
random variables Wt(x, a, Y ) which are obtained from wt(x, a, Y ) by replacing
the realisation z by its random counterpart Z satisfy
(i) EWt(x, a, Y ) = 0 by deﬁnition of the H-operator.
(ii) EW 2
t (x, a, Y ) is bounded.
Thus, we can apply Proposition 4.4 in [5].
⊓⊔
Once we have obtained Q we can compute J and the minimizer d∗and maximizer
γ∗which yields the optimal policies.
Remark 4. Note that the Q-learning algorithm is model-free in the sense that it
is not necessary to know the probability law P. Instead of simulating z one can
of course use observed model data if available.

Q-Learning for Distributionally Robust MDP
123
6
Numerical Example
In this section, we apply the distributionally robust Q-learning algorithm to an
agricultural irrigation management problem. With progressing climate change,
water becomes a scarce resource in many regions of the world which must be
carefully managed. Therefore, mathematical optimization may be needed where
simple rules of thumb have been suﬃcient in the past. The stylized setting of
this example is designed to illustrate the performance of our algorithm. It can
be easily extended to a practical model. We refer the interested reader to [17,18]
for some approaches in continuous time.
Consider a greenhouse that is irrigated from a water reservoir with capacity
¯s ∈N. One unit of water is needed for every irrigation procedure. The crops
rot when irrigated on two consecutive days and wither if they are not watered
again within ¯x ≥2 days. Both events destroy the harvest and a ﬁxed cost
c > 0 is incurred. Precipitation may occur on each day independently with
probability p ∈[p1, p2] ⊂(0, 1) and add one unit of water to the reservoir.
The true rain probability is unknown and it is therefore prudent to work with
the conﬁdence interval [p1, p2] instead of a single estimate. I.e. Q consists of all
Bernoulli distributions with parameter between p1 and p2. Thus, we can identify
Qd with the parameter set [p1, p2]. If the maximal capacity of the reservoir is
exceeded, the spillover goes into the greenhouse like a regular irrigation. The
corresponding Markov decision model is given by the following data.
(i) The state space is

{0, . . . , ¯x} × {0, . . . , ¯s}

∪{∞}. The ﬁrst component of
a state (x, s) gives the days since the last irrigation and the second one the
current level of the water reservoir. The absorbing state ∞corresponds to
a destroyed harvest.
(ii) The action space is {0, 1}. Action a = 1 means that the crops are watered
and a = 0 that they are not. The decision maker faces no constraint.
(iii) The i.i.d. disturbances Z1, Z2, · · · ∼Bin(1, p), p ∈[p1, p2] model the amount
of daily precipitation.
(iv) The transition function T(x, s, a, z) is given by
T(x, s, 0, 0) =

(x + 1, s),
x < ¯x
∞,
x = ¯x
T(x, s, 1, 0) =
⎧
⎪
⎨
⎪
⎩
(0, s −1),
x > 0, s > 0
(x + 1, 0),
x < ¯x, s = 0
∞,
x = ¯x, s = 0 or x = 0, s > 0
T(x, s, 0, 1) =
⎧
⎪
⎨
⎪
⎩
(x + 1, s + 1),
x < ¯x, s < ¯s
∞,
x = ¯x, s < ¯s or x = 0, s = ¯s
(0, ¯s),
x > 0, s = ¯s
T(x, s, 1, 1) =

(0, s),
x > 0
∞,
x = 0

124
N. B¨auerle and A. Glauner
and T(∞, a, z) = ∞.
(v) The one-stage cost function is c(x, s, x′, s′) = c 1{(x, s) ̸= ∞, (x′, s′) = ∞}.
The model clearly satisﬁes Assumptions (A). The target of the decision maker
is to minimize the expected discounted cost
J∞(x, s) = inf
π∈Π sup
γ∈Γ
Eπγ
x,s
	 ∞

k=0
βkc(Xk, Sk, Xk+1, Sk+1)

under the assumption of being confronted with the most adverse precipitation
probability p on each day. This means that the decision maker tries to avoid
ruin if ever possible or to delay it to a later time point. His opponent in the
dynamic Stackelberg game is nature in the proper meaning of the word. She
selects the rain probability knowing the current state and the decision maker’s
action. Since expectation is linear in the measure, her optimal action can only
be at the boundary, i.e. p1 or p2. So we have a robust point of view here.
For the implementation of the Q-learning algorithm we selected β = 0.9 as
discount factor, c = 10 as ﬁxed cost, ¯x = 3 as time until withering, p1 = 0.2,
p2 = 0.3 and 0.25 as reference probability for the two densities Y1(z) =
0.2
0.251{z =
1} +
0.8
0.751{z = 0} and Y2(z) =
0.3
0.251{z = 1} +
0.7
0.751{z = 0} that nature may
select. At ﬁrst, the maximal capacity of the reservoir is ¯s = 3.
Fig. 1. Approximation of the value function in diﬀerent states as a function of the
number of iterations.
Figure 1 shows the convergence of the approximated value function
J(t)(x, s) = min
a max
Y
Q(t)(x, s, a, Y ),
t = 0, . . . , 100000

Q-Learning for Distributionally Robust MDP
125
in four exemplary states. State (0, 3) represents an imminent spillover, (3, 0)
imminent withering and (1, 1), (2, 2) are two moderate situations. We compared
three diﬀerent learning rates.
black curve:
dark grey curve:
light grey curve:
αt =
0.5
1 + 0.01t
αt =
0.5
1 + 0.001t
αt =
0.5
1 + 0.0001t
The same color code is used in all other ﬁgures, too. The faster the learning
rate goes to zero, the earlier the approximate cost stabilizes. In the two extreme
states (0, 3) and (3, 0), where the optimal action of both players is obvious, even
the learning rate with the strongest decay yields a good approximation. In the
two moderate states, where the path to ruin is longer, the strong decay essen-
tially terminates the approximation too early. On the other hand, the slowest
decay works well in case of a long path to ruin while convergence in the two
extreme states takes unnecessarily long. The medium decay seems to be a suit-
able compromise for all states.
Fig. 2. Approximation of the decision maker’s optimal policy in diﬀerent states as a
function of the number of iterations.
Figure 2 shows the convergence of the approximated optimal policy of the
decision maker
π(t)(x, s) = arg min
a max
Y
Q(t)(x, s, a, Y ),
t = 0, . . . , 100000
in the same states and for the same learning rates. In (0, 3) and (1, 1) the learn-
ing rates are indistinguishable which is also true in (3, 0) from iteration 50000
onward. Only in state (2, 2) the minimizing argument remains rather unstable
despite the fast stabilization of the minimal value shown in Fig. 1. I.e. here the
two actions lead to almost the same cost.
Figure 3 displays the convergence of the approximated optimal policy of
nature
γ(t)(x, s, a) = arg max
Y
Q(t)(x, s, a, Y ),
t = 0, . . . , 100000

126
N. B¨auerle and A. Glauner
Fig. 3. Approximation of nature’s optimal policy in diﬀerent state-action combinations
as a function of the number of iterations.
again in the same states and for the same learning rates. With the third learn-
ing rate, nature’s optimal action does not stabilize during the ﬁrst 100000
iterations in all four states. The other two learning rates perform better. In
the relevant scenarios given optimal behavior of the decision maker (x, s, a) =
(0, 3, 0), (1, 1, 0), (3, 0, 1) we observe an early stabilization under the two learning
rates with faster decay. In state (2, 2) the stabilization is good at least for action
a = 0.
All in all, the second learning rate appears to be the best choice in this
application with fast convergence of the value function to the true optimal cost
and a relatively good stabilization of the optimizing arguments.
In Fig. 4, we compare the convergence of the distributionally robust Q-
learning algorithm with the classical risk-neutral version (with rain probability
p = 0.25) in terms of the absolute step sizes
δ(t) = ∥Q(t+1) −Q(t)∥∞
= αtYt(zt)
c(xt, st, T(xt, st, at, zt)) + β min
a′ max
Y ′ Q(t)(T(xt, st, at, zt), a′, Y ′)
−Q(t)(xt, st, at, Yt)
.
Here, (xt, st, at, Yt, zt) is the state-action-disturbance combination sampled in
iteration t. The plots show the moving averages
Δ(t) =
1
100
t

k=t−99
δ(k),
t = 99, . . . , 100000

Q-Learning for Distributionally Robust MDP
127
Fig. 4. Moving average of the absolute step sizes as a function of the number of itera-
tions.
of the step sizes both for the risk-neutral and the distributionally robust algo-
rithm as well as the small reservoir ¯s = 3 and a larger one with ¯s = 10. First,
we observe that the distributionally robust algorithm performs as good as its
classical counterpart. Besides, the fast convergence also holds for larger models.
We can also note that the learning rate with intermediate decay combines fast
convergence with the good approximation results shown above.
Table 1. Robust optimal policy of the decision maker in all states (x, s) with additional
1’s compared to the risk-neutral case in bold print.
x\s 0 1 2 3 4 5 6 7 8 9 10
0
0 0 0 0 0 0 0 0 0 0 0
1
0 0 0 0 0 0 1 1 1 1 1
2
0 0 0 0 0 0 1 1 1 1 1
3
1 1 1 1 1 1 1 1 1 1 1
To illustrate the diﬀerence between the classical risk-neutral and the distribu-
tionally robust cost minimization criterion for the decision maker, Table 1 shows
his optimal policy for the model with larger reservoir. Optimal actions that diﬀer
under the two optimization targets are in bold print. The two bold 1’s belong
to the robust case and must be zero in the risk-neutral case. In order to prevent

128
N. B¨auerle and A. Glauner
a spillover destroying the harvest, the more conservative decision maker in the
robust model irrigates the crops already at water level 6 where a risk-neutral
controller would not take action yet.
References
1. B¨auerle, N., Glauner, A.: Distributionally robust Markov decision processes and
their connection to risk measures. arXiv:2007.13103 (2020)
2. B¨auerle, N., Rieder, U.: Markov Decision Processes with Applications to Finance.
Springer, Heidelberg (2011)
3. Bellman, R.: Dynamic Programming. Princeton University Press, Princeton (1957)
4. Bellman, R.: Dynamic Programming. Dover Publications, Mineola (2003)
5. Bertsekas, D., Tsitsiklis, J.N.: Neuro-Dynamic Programming. Athena Scientiﬁc,
Belmont (1996)
6. Glauner, A.: Robust and Risk-sensitive Markov decision processes with applications
to dynamic optimal reinsurance. Ph.D. thesis, Karlsruhe Institute of Technology
(2020). https://doi.org/10.5445/IR/1000126170
7. Gonz´alez-Trejo, J.I., Hern´andez-Lerma, O., Hoyos-Reyes, L.F.: Minimax control of
discrete-time stochastic systems. SIAM J. Control Optim. 41(5), 1626–1659 (2002)
8. Guidolin, M., Rinaldi, F.: Ambiguity in asset pricing and portfolio choice: a review
of the literature. Theory Decis. 74(2), 183–217 (2013)
9. Hern´andez-Lerma, O., Lasserre, J.B.: Further Topics on Discrete-Time Markov
Control Processes. Springer, New York (1999)
10. Hinderer, K.: Foundations of Non-stationary Dynamic Programming with Discrete
Time Parameter. Springer, Heidelberg (1970)
11. Iyengar, G.N.: Robust dynamic programming. Math. Oper. Res. 30(2), 257–280
(2005)
12. Ja´skiewicz, A., Nowak, A.S.: Robust Markov control processes. J. Math. Anal.
Appl. 420(2), 1337–1353 (2014)
13. Maccheroni, F., Marinacci, M., Rustichini, A.: Ambiguity aversion, robustness,
and the variational representation of preferences. Econometrica 74(6), 1447–1498
(2006)
14. McNeil, A.J., Frey, R., Embrechts, P.: Quantitative Risk Management: Concepts,
Techniques and Tools, revised Princeton University Press, Princeton and Oxford
(2015)
15. Pichler, A.: Premiums and reserves, adjusted by distortions. Scand. Actuar. J.
2015(4), 332–351 (2015)
16. R¨uschendorf, L.: Mathematical Risk Analysis: Dependence, Risk Bounds, Optimal
Allocations and Portfolios. Springer, Heidelberg (2013)
17. Unami, K., Mohawesh, O., Shariﬁ, E., Takeuchi, J., Fujihara, M.: Stochastic mod-
elling and control of rainwater harvesting systems for irrigation during dry spells.
J. Clean. Prod. 88, 185–195 (2015)
18. Unami, K., Yangyuoru, M., Alam, A.H.M.B., Kranjac-Berisavljevic, G.: Stochastic
control of a micro-dam irrigation scheme for dry season farming. Stoch. Environ.
Res. Risk Assess. 27(1), 77–89 (2013)
19. Wiesemann, W., Kuhn, D., Rustem, B.: Robust Markov Decision Processes. Math.
Oper. Res. 38(1), 153–183 (2012)
20. Xu, H., Mannor, S.: Distributionally robust Markov decision processes. Adv. Neural
Inform. Process. Syst. 23, 2505–2513 (2010)

State Estimation in Partially Observed
Stochastic Networks with Queueing
Applications
Konstantin V. Semenikhin(B)
Department Probability Theory and Computer Modeling, Moscow Aviation Institute,
Volokolamskoye shosse, 4, Moscow, Russia
siemenkv@mail.ru
https://www.researchgate.net/profile/Konstantin Siemenikhin
Abstract. The problem of ﬁlter-based state estimation for a partially
observed stochastic network is considered in this paper, using the mea-
sure change approach. The network is assumed to have two types of
nodes: observed and hidden. Their dynamics are deﬁned by a set of count-
ing processes with state-dependent intensities. The goal is to derive the
nonlinear optimal ﬁlter and to propose a numerical scheme for its prac-
tical implementation. Network models that allow the optimal ﬁlter to be
ﬁnite-dimensional are also considered. The theoretical results are applied
to a retrial queuing system to track changes in two hidden stations: one
accumulates blocked customers and the other contains unsatisﬁed cus-
tomers.
Keywords: Partially observed stochastic network · Point process ·
Filtering · Martingale · Change of measure · Retrial queueing system
AMS (2020) Subject Classiﬁcation: Primary 93E11 · Secondary
90B15
1
Introduction
First publications on stochastic ﬁltering in queueing systems and networks were
aimed at proving and enhancing classical results of the queueing theory (such as
Burke’s output theorem and Arrivals-See-Time-Averages properties) to a wider
class of point processes, using martingale methods [5,8,18]. Martingale theory
together with a reference probability approach has obtained numerous applica-
tions in estimation, control and optimization for stochastic systems described
by jump Markov processes [6,7,14–16]. However in the ﬁeld of queueing systems
there has been little work on the applications of ﬁltering theory [3,4,13,16]. This
can be explained, in part, by the opinion that rational queueing does not need
complicated estimation algorithms even if dealing with strategic customers who
can observe the queue length [9]. Nevertheless, recovery of unknown parameters
c
⃝The Author(s), under exclusive license to Springer Nature Switzerland AG 2021
A. Piunovskiy and Y. Zhang (Eds.): Modern Trends in Controlled Stochastic Processes,
ECC 41, pp. 129–147, 2021. https://doi.org/10.1007/978-3-030-76928-4_7

130
K. V. Semenikhin
and hidden states based on partially observed dynamics constitutes an important
class of inverse problems in the queueing theory [2]. In communication network
applications, especially in wireless congestion control, ﬁlter-based estimates have
recently received considerable attention to cope with time-varying behavior of
packet arrival rates [12,17]. This problem known as bandwidth estimation is for-
mulated in the form of a nonlinear ﬁltering problem to track changes in incoming
data ﬂows given measurements of buﬀer occupancy.
In this paper, we consider a Jackson-type stochastic network with observed
and hidden nodes. The number of units at each hidden node is to be estimated
from changes in states of the observed nodes. Instead of using the inﬁnite-
dimensional diﬀerential system for conditional probabilities, we adopt the refer-
ence probability method to derive underlying equations for the conditional expec-
tation and covariances. Although these equations, in general, have no closed-form
solution we present a particular class of the network model that provides a ﬁnite-
dimensional ﬁlter. For practical implementation of the state estimation method
we propose a numerical scheme based on regularization of the optimal ﬁltering
equations. To justify the estimation algorithm we consider a call center model
described by the main station (a single-server ﬁnite queueing system) and two
additional stations (“orbits”) whose states are to be estimated given the observed
queue length at the main system.
2
Model Description and Problem Formulation
We study a stochastic network with the set of nodes S = {1, 2 . . . , d}. Each node
receives units (jobs, customers and so on) from other nodes and from outside.
An additional node 0 is used as a source of external arrivals or a sink in the
case of service completion. Network dynamics are determined by a continuous-
time process X(t) = (X1(t), . . . , Xd(t)) deﬁned on a probability space (Ω, F, P),
where Xi(t) denotes the number of units at node i at time t ≥0. Any change in
the network state is caused by one of three possible single-unit movements:
a) a unit moves from one node i ∈S to another j ∈S;
b) a unit ﬁnishes a service at node i ∈S;
c) a unit arrives to node j ∈S from outside.
These transitions are described by the respective point processes Ni,j(t),
Ni,0(t), and N0,j(t) which have right-continuous sample paths and unit jumps.
We do not consider instantaneous transitions within the same node, so the pro-
cesses Ni,i or N0,0 are not used in the paper.
Assume all these processes {Nα,β} are adapted to some right-continuous
complete ﬁltration F = {Ft}t≥0 and have the following representation:
Nα,β(t) =
 t
0
να,β(s) ds + Mα,β(t),
(1)
where Mα,β is a square-integrable F-martingale and να,β is a nonnegative F-
predictable function [6].

State Estimation in Partially Observed Stochastic Networks
131
We suppose the martingales Mα,β and Mα′,β′ are orthogonal for (α, β) ̸=
(α′, β′). This is equivalent to the condition that jumps of Nα,β and Nα′,β′ (i.e.,
any two diﬀerent transitions including arrivals and departures) do not occur at
the same time.
Then the state of node k ∈S can be expressed as follows:
Xk(t) = Xk(0) +

α
Nα,k(t) −

β
Nk,β(t),
where α and β run over S ∪{0}.
To complete the description of the network model, it remains to deﬁne how
the transition intensities να,β depend on the current state or previous evolution
of the network.
For Jackson networks, given an ·/Mμi/mi queueing system at each node i,
constant arrival rates λj, service rates μi, and routing probabilities ri,j, i, j ∈S,
we obtain the transition intensities: ν0,j = λj, νi,j(t) = μi(Xi(t−) ∧mi)ri,j, and
νi,0(t) = μi(Xi(t−) ∧mi)

1 −
j∈S ri,j

. In the case of loss networks, there is a
station j with ﬁnite capacity Kj, so the routing probabilities {ri,j}j∈S must be
multiplied by the indicator I{Xj(t−) < Kj}. If the network is considered in a
control setting, all three sets of parameters {λj}, {μi}, and {ri,j} can be deﬁned
by access, service, and routing control policies, respectively.
In this paper, we study a partially observed stochastic network. To this end,
let us split the set of nodes into two subsets: S = J ⊔H, where J will denote the
set of all observed nodes while H will contain hidden nodes of the network except
for the ﬁctitious node 0 which will also be treated as unobservable. The only
information about the network evolution is given by the state of the observed
nodes Y (t) = {Xi(t)}i∈J including the initial state of the entire network X(0).
Then, write
Yt = σ{X(0), Y (s): s ≤t}
and
Yt−= σ{X(0), Y (s): s < t}
for complete sigma-algebras generated by the observations and Y = {Yt}t≥0 for
the corresponding ﬁltration.
We make an additional assumption on transitions from the observed nodes:
the intensities νiβ must be Y-predictable for all i ∈J and β ∈S ∪{0}. This
means that we not only know the true state of nodes i ∈J at each time; we
also have direct information on the rate at which units move from these nodes.
This condition is fulﬁlled for Jackson-type stochastic networks whenever service
rates μi and routing probabilities ri,β are Y-predictable for all observed nodes
i ∈J. In contrast, it does not hold for loss networks if there is a transition from
one observed node i ∈J to some hidden station k ∈H with ﬁnite capacity.
The goal of the optimal ﬁltering problem for the partially observed network
is to ﬁnd the conditional expectation ˆZ(t) = E{Z(t) | Yt} of the network’s hidden
part Z(t) = {Xk(t)}k∈H given the observations available up to the current time t.
Since we are going to solve this problem without ﬁnding the whole posterior dis-
tribution

P{Z(t) = z | Yt}: z = {xk}k∈H

, we will use the conditional covari-
ance matrix Q(t) = cov{Z(t), Z(t) | Yt} to characterize the estimation accuracy.

132
K. V. Semenikhin
Our aim is to determine ˆX(t) and Q(t) in a recursive manner which is suitable
for practical implementation including approximation schemes.
This setting is motivated by optimization problems that arise in the design of
queueing systems, such as contact centers. The lack of exact information about
how many customers are blocked by the system or unsatisﬁed with the quality
of service makes diﬃcult to improve the eﬃciency of the system. Thus, ﬁltered
estimates of unobservable interactions can be used to tune the tradeoﬀbetween
customer satisfaction and personnel-related operating costs.
Another application where partially observed stochastic networks can be use-
ful is related to a bottleneck link problem in data transmission. Some nodes of
wireless communication networks, especially over a multi-hop path, are often
hidden from direct measurements of service rate and buﬀer occupancy, so to ade-
quately track end-to-end throughput one needs to develop recursive algorithms
for on-line estimating actual states and parameters of unobservable nodes.
3
Optimal Filter for a Process with Network Dynamics
We start with a simple but important remark on the observable dynamics: the
ﬁltration Y can be deﬁned as that generated only by the point processes
Ni,j,
N a
j =

k/∈J
Nk,j,
and
N d
i =

k/∈J
Ni,k
(i, j ∈J)
together with the initial state X(0). Note that {Ni,j} describe transitions inside
the set of observed nodes J, while N a
j and N d
i count arrivals to j ∈J from any
unobservable node k /∈J and departures from i ∈J to any k /∈J, respectively.
The intensities of observed arrivals and departures are the following:
νa
j =

k/∈J
νk,j
and
νd
i =

k/∈J
νi,k.
To derive equations for the optimal ﬁlter we will use the reference probability
method [6]. To this end, deﬁne a measure P on (Ω, F) such that under P, all
point processes {Nα,β} are mutually independent Poisson processes with unit
intensity. The measure P is called a reference probability and the corresponding
expectation is denoted by E.
The lemma below shows that expectations under P are computed in an easy
way. To simplify notation, we write E{ξ dt | Yt} = dη as shorthand for the integral
equation E
	 t
0ξ(s) ds


 Yt

=
	 t
0dη(s) if it holds for all t ∈(0, ∞).
Lemma 1. Suppose that F is a complete ﬁltration generated by all point pro-
cesses {Nα,β}. If ξ(t) is an F-predictable process such that
	 t
0 E|ξ(s)| ds < ∞for
any t < ∞, then
E

ξ dt


 Yt

= ξ dt,
(2)

State Estimation in Partially Observed Stochastic Networks
133
E

ξ dNi,j


 Yt

= ξ dNi,j,
i, j ∈J,
(3)
E

ξ dNi,k


 Yt

= ξ
p dN d
i ,
i ∈J,
k /∈J,
(4)
E

ξ dNk,j


 Yt

= ξ
p dN a
j ,
k /∈J,
j ∈J,
(5)
E

ξ dNk,l


 Yt

= ξ dt,
k, l /∈J,
(6)
where ξ(t) denotes a Y-predictable version of E{ξ(t) | Yt−} and p equals the
number of unobservable nodes H ∪{0}. Furthermore, after replacing each point
process with the centered counterpart
◦
N α,β(t) = Nα,β(t) −t,
◦
N d
i (t) = N d
i (t) −pt,
◦
N a
j (t) = N a
j (t) −pt,
(3), (4), and (5) remain to be valid whereas (6) yields zero.
To return to the “real-world” model, one needs to deﬁne a probability mea-
sure P, under which the stochastic network will have the original transition
intensities να,β given by (1). To do this, we ﬁrst consider a stochastic exponen-
tial
dΘ(t) = Θ(t−) dM(t),
t > 0,
Θ(0) = 1,
M(t) =
t

0

α,β
(να,β(s) −1) d
◦
N α,β(s),
and then put
P(A) = E{IAΘ(t)},
A ∈Ft,
t ≥0.
(7)
The next lemma conﬁrms the fact that (7) determines the original model
described in Sect. 2.
Lemma 2. Assume F = σ

t≥0 Ft

and the intensities satisfy two conditions:
να,β > 0
whenever
ΔNα,β > 0;
(8)
∃C = const:

α,β
να,β ≤C

α,β
Nα,β.
(9)

134
K. V. Semenikhin
Then
1. Θ(t) is a positive P-martingale with EΘ(t) = 1;
2. (7) is a probability measure uniquely deﬁned on F;
3. Any F-adapted process ξ(t) with E|ξ(t)| < ∞is a P-martingale if and only if
ξ(t)Θ(t) is a P-martingale;
4. Under P, conditional expectations are calculated using Bayes’ rule
E{ξ | Yt} = E{ξΘ(t) | Yt}

θ(t),
θ(t) = E{Θ(t) | Yt},
(10)
where ξ is a random variable such that E|ξ| < ∞;
5. Under P, each point process Nα,β has the martingale representations (1).
Let us consider a process ξ(t) with jumps generated by the stochastic net-
work:
dξ = η dt +

i∈J, k/∈J
ζi,k dNi,k +

j∈J, k/∈J
ζk,j dNk,j +

k,l/∈J
ζk,l dNk,l
(11)
where η(t) and {ζα,β(t)} are F-predictable processes and ξ(0) is a Y0-measurable
initial state. The terms related to transitions within the observable part of the
network {ζi,j dNi,j, i, j ∈J} are not used in the paper, so they are omitted
in (11).
Our goal now is to obtain equations for the unnormalized estimate ξ(t) and
the conditional expectation ˆξ(t):
ξ(t) = E{ξ(t)Θ(t) | Yt}
and
ˆξ(t) = E{ξ(t) | Yt}.
From now on we use this notation for the estimates of any F-adapted corlol
process ξ [19]. In the case of an F-predictable process, say η, we denote Y-
predictable versions of E{η(t)Θ(t−) | Yt−} and E{η(t) | Yt−} by η(t) and ˆη(t),
respectively. In the case of the product, we write 
ξη(t) and 
ξη(t) for the corre-
sponding estimates of the F-predictable process ξ(t−)η(t).
The theorem below is the main tool for deriving ﬁltered estimates of any
process governed by the network dynamics.
Theorem 1. Under the assumptions of Lemmas 1 and 2, the estimates of (11)
satisfy the following equations:
dˆξ =

ˆη +

k,l/∈J

ζk,lνk,l −

j∈J
ˆca
j

dt +

i∈J
ˆζd
i
νd
i
dN d
i +

j∈J
ˆζa
j + ˆca
j
ˆνa
j
dN a
j ,
(12)
ζd
i =

k/∈J
ζi,kνi,k,
ζa
j =

k/∈J
ζk,jνk,j,
ˆca
j = 
ξνa
j −ˆξ(t−)ˆνa
j

State Estimation in Partially Observed Stochastic Networks
135
and
dξ = η dt + ξ(t−) dM′ +

j∈J
(
ξνa
j /p −ξ(t−)) d
◦
N a
j
+ 1
p

i∈J
ζd
i dN d
i + 1
p

j∈J
ζa
j dN a
j +

k,l/∈J

ζk,lνk,l dt,
(13)
dM′ =

i,j∈J
(νi,j −1) d
◦
N i,j +

i∈J
(νd
i /p −1) d
◦
N d
i .
with initial conditions ˆξ(0) = ξ(0) = ξ(0) and M′(0) = 0.
Proof. We ﬁrst apply Ito’s rule:
d(ξΘ) = Θ(t−) dξ + ξ(t−) dΘ + ΔξΔΘ
= Θ(t−)η dt +

Θ(t−)ζα,β dNα,β +

i,j∈J
ξ(t−)Θ(t−)(νi,j −1) d
◦
N i,j
+

ξ(t−)Θ(t−)(να,β −1) d
◦
N α,β +

ζα,βΘ(t−)(να,β −1) dNα,β
where all sums without subscripts are taken over (α, β) /∈J × J. Then, using
Lemma 1, we obtain
dξ = η dt +

i,j∈J
ξ(t−)(νi,j −1) d
◦
N i,j
+ 1
p

i∈J,k/∈J
ξ(t−)(νi,k −1) d
◦
N d
i + 1
p

j∈J,k/∈J
(
ξνk,j −ξ(t−)) d
◦
N a
j
+ 1
p

i∈J,k/∈J
ζi,kνi,k dN d
i + 1
p

j∈J,k/∈J

ζk,jνk,j dN a
j +

k,l/∈J

ζk,lνk,l dt
which coincides with (13).
In particular, we can now write the equation for θ(t) = E{Θ(t) | Yt}:
dθ = θ(t−) dM′ +

j∈J
(νa
j /p −θ(t−)) d
◦
N a
j .
(14)
To derive the estimate ˆξ(t) = ξ(t)/θ(t), we use the expression
dˆξ =
dξ
θ(t−) −
ˆξ(t−) dθ
θ(t−)
−ΔˆξΔθ
θ(t−) .
(15)
Then, from (13) and (14) it follows that
dξ
θ(t−) −
ˆξ(t−) dθ
θ(t−)
= ˆη dt + 1
p

j∈J
(
ξνa
j −ˆξ(t−)ˆνa
j ) d
◦
N a
j
+ 1
p

i∈J
ˆζd
i dN d
i + 1
p

j∈J
ˆζa
j dN a
j +

k,l/∈J

ζk,lνk,l dt
(16)

136
K. V. Semenikhin
The last term in (15) may be nonzero only in three cases:
a) ΔNi,j ̸= 0 (i, j ∈J),
b) ΔN d
i ̸= 0 (i ∈J),
c) ΔN a
j ̸= 0 (j ∈J).
Then, Δξ = ξ(t−)(g −1) + b and Δθ = θ(t−)(a −1), where
a) g = a, b = 0,
b) g = a = νd
i
p , b =
ξd
i
p ,
c) g =

ξνa
j
p ξ(t−)
, a = ˆνa
j
p , b =
ξa
j
p .
It is easy to show that
ΔˆξΔθ
θ(t−) = (a −1)

ˆξ(t−)(g/a −1) +
b
aθ(t−)

.
This yields zero in case a) and
ΔˆξΔθ
θ(t−) =
⎧
⎨
⎩

1/p −1/νd
i
ˆζd
i
in case b),

1/p −1/ˆνa
j


ξνa
j −ˆξ(t−)ˆνa
j + ˆζa
j

in case c).
(17)
Subtracting (17) from (16), we obtain (12).
□
Remark 1. The structure of the estimate (12) can be explained as follows. The
unobservable dynamics {ζk,l dNk,l, k, l /∈J} aﬀect only the drift coeﬃcient of
the ﬁlter. The term ˆζd
i /νd
i deﬁnes the average eﬀect of jumps {ζi,k}k/∈J caused
by transitions from node i to the unobservable part of the network. Analogously,
ˆζa
j /ˆνa
j is a mixture of terms {ζk,j}k/∈J related to transitions from unobservable
nodes to station j. The only diﬀerence between these two types of transitions is
the correction term
ˆca
j = cov{ξ(t−), νa
j | Yt−}
(18)
which is added to the coeﬃcient of jump dN a
j and subtracted from the drift.
4
State Estimation for Hidden Nodes
In this section, we focus on deriving a ﬁltering algorithm for state estimation of
unobservable nodes in the stochastic network.
For any node k ∈H, its state can be represented in the form of (11):
dXk =

i∈J
dNi,k −

j∈J
dNk,j +

m/∈J
(dNm,k −dNk,m).
From Theorem 1, we have immediately
d ˆXk =

ˆνa
k −ˆνd
k −

j∈J
ˆck,j

dt +

i∈J
ξd
i,k dN d
i +

j∈J
ξa
k,j dN a
j ,
(19)

State Estimation in Partially Observed Stochastic Networks
137
ˆνa
k =

m/∈J
ˆνm,k,
ˆνd
k =

m/∈J
ˆνk,m,
(20)
ξd
i,k = νi,k
νd
i
,
ξa
k,j = ˆck,j −ˆνk,j
ˆν a
j
,
(21)
where the coeﬃcients {ˆck,j} are analogous to (18):
ˆck,j = cov{Xk(t−), νa
j | Yt−} = 
Xkνa
j −ˆX(t−)ˆνa
j .
(22)
Since by assumption {νi,k} are Y-predictable for any observed node i, the
terms {ξd
i,k} related to departures from i ∈J do not require to be estimated.
In addition to the estimates { ˆXk(t)}k∈H, we also describe their errors
εk(t) = Xk(t) −ˆXk(t)
using conditional variances and covariances.
Theorem 2. Under the conditions of Lemmas 1 and 2, the following statements
hold:
1. For k ∈H, the estimate ˆXk(t) = E{Xk(t) | Yt} satisﬁes (19);
2. For k ∈H, the conditional error variance Qk,k(t) = E{ε2(t) | Yt} has the form
dQk,k =

ˆνa
k + ˆνd
k + 2ˆbk,k −

j∈J
ˆτk,k,j

dt +

i∈J

1 −ξd
i,k

ξd
i,k dN d
i
+

j∈J
 1
ˆνa
j

ˆτk,k,j + ˆνk,j −2ˆκk,k,j

−

ξa
k,j
2
dN a
j ;
(23)
3. For k, l ∈H such that k ̸= l, the conditional error covariance Qk,l(t) =
E{εk(t)εl(t) | Yt} is given by the equation
dQk,l =
ˆbk,l + ˆbl,k −ˆνk,l −ˆνl,k −

j∈J
ˆτk,l,j

dt −

i∈J
ξd
i,kξd
i,l dN d
i
+

j∈J
 1
ˆνa
j

ˆτk,l,j −ˆκ k,l,j −ˆκ l,k,j

−ξa
k,jξa
l,j

dN a
j .
(24)
The above coeﬃcients ˆκ k,l,j, ˆbk,l, and ˆτk,l,j are Y-predictable versions of the
conditional covariances:
ˆκ k,l,j = cov{Xk(t−), νl,j | Yt−},
(25)
ˆbk,l = cov{Xk(t−), νa
l −νd
l | Yt−} =

m/∈J

ˆκ k,m,l −ˆκ k,l,m

,
(26)
ˆτk,l,j = cov{εk(t−)εl(t−), νa
j | Yt−}.
(27)

138
K. V. Semenikhin
Proof. We start by representing the estimation error in the form (11):
dεk = dXk −d ˆXk = ηk dt +

m/∈J
(dNm,k −dNk,m)
+

i∈J

m/∈J
(δm,k −ξd
i,k) dNi,m −

j∈J

m/∈J
(δm,k + ξa
k,j) dNm,j,
where ηk is some Y-predictable coeﬃcient and δm,k is a Kronecker’s symbol.
Given any k, l ∈H, we apply Ito’s product formula
d(εkεl) = εk(t−) dεl + εl(t−) dεk + ΔεkΔεl
= (εk(t−)ηl + εl(t−)ηk) dt + ΔεkΔεl + εl(t−)Δεk + εk(t−)Δεl.
Since 
εkηl = εk(t−)ηl = 0, the drift term in dQk,l can be omitted. So we are
interested in calculating only the discontinuous component Δ(εkεl). It consists
of three parts. The ﬁrst is related to completely unobservable jumps:
δk,l

m/∈J
(dNm,k + dNk,m) −(1 −δk,l)(dNk,l + dNl,k)
+

m/∈J
{εl(t−) (dNm,k −dNk,m) + εk(t−) (dNm,l −dNl,m)}.
The second part is a sum of {dNi,m, i ∈J, m /∈J} with the coeﬃcients:
(δm,k −ξd
i,k)(δm,l −ξd
i,l) + εl(t−) (δm,k −ξd
i,k) + εk(t−) (δm,l −ξd
i,l).
The third part contains {dNm,j, j ∈J, m /∈J} with the coeﬃcients:
(δm,k + ξa
k,j)(δm,l + ξa
l,j) −εl(t−) (δm,k + ξa
k,j) −εk(t−) (δm,l + ξa
l,j).
From Theorem 1 it follows that the ﬁrst two parts can be estimated sepa-
rately. The estimate of the ﬁrst part is

δk,l

m/∈J
(ˆνm,k + ˆνk,m) −(1 −δk,l)(ˆνk,l + ˆνl,k)
+

m/∈J
(ˆκ l,m,k −ˆκ l,k,m + ˆκ k,m,l −ˆκ k,l,m)

dt
which coincides with the drift of (23) and (24) except the correction term

j∈J ˆτk,l,j.
Using 
εlνi,m = εl(t−)νi,m = 0, we obtain the estimate of the second part

i∈J

m/∈J
(δm,k −ξd
i,k)(δm,l −ξd
i,l)νi,m
νd
i
dN d
i .
(28)

State Estimation in Partially Observed Stochastic Networks
139
Taking into account the correction term, the estimate of the third part takes
the form:
−

j∈J
ˆτk,l,j dt +

j∈J
1
ˆνa

ˆτk,l,j +

m/∈J

(δm,k + ξa
k,j)(δm,l + ξa
l,j)ˆνm,j
−(δm,k + ξa
k,j)ˆκ l,m,j −(δm,l + ξa
l,j)ˆκ k,m,j

dN a
j .
(29)
Simple calculations show that (28) and (29) yield the corresponding terms
in (23) and (24).
□
The following proposition describes a class of stochastic networks that admit
a closed-form optimal ﬁlter for state estimates of hidden nodes.
Corollary 1. If
a) transitions from hidden to observed nodes have Y-predict-
able intensities {νm,j, m /∈J, j ∈J};
b) transitions within the unobservable
part of the network are linear functions of the states:
νm,n = μm,n,0 +

α∈H
μm,n,αXα(t−)
(m, n /∈J)
with Y-predictable coeﬃcients {μm,n,α}, then the optimal estimates { ˆXk}k∈H
are described by a ﬁnite-dimensional ﬁlter:
d ˆXk =

ˆνa
k −ˆνd
k

dt +

i∈J
νi,k
νd
i
dN d
i −

j∈J
νk,j
ν a
j
dN a
j .
Furthermore, taking into account
ˆτk,l,j = ˆκk,l,j = 0
(k, l ∈H, j ∈J)
ˆκk,m,n =

α∈H
μm,n,αQk,α(t−)
(k ∈H, m, n /∈J)
the conditional error covariance matrix {Qk,l}k,l∈H satisﬁes the closed-form sys-
tem (23), (24).
Assumption a) and b) look rather restrictive in view of applications to queue-
ing models. Even if we have a simple tandem system Mλ|Mμ1|m1 →· |Mμ1|m2
where station 1 is to be estimated given the observed state of station 2, both
conditions a) and b) are violated.
So we need an approximation scheme to practically implement ﬁltering equa-
tions derived above. To this end, we consider a stochastic network that has
Jackson-like transition intensities at least for hidden nodes:
νk,β = μk,βXk(t−),
k ∈H,
ν0,β = λβ
where μk,β and λβ are Y-predictable coeﬃcients.
For such a network, we have ˆck,j = 
m∈H Qk,m(t−)μm,j, ˆνk,β = μk,β ˆXk(t−).
So all coeﬃcients of the optimal ﬁlter (19) can be expressed in terms of the state
estimates { ˆXk}k∈H and conditional covariances {Qk,l}k,l∈H.

140
K. V. Semenikhin
To simplify equations for {Qk,l}, we propose to exclude third-order terms
M τ
k,l(t) =
 t
0

j∈J
ˆτk,l,j
dN a
j
ˆνa
j
−ds

.
Since {M τ
k,l} are zero-mean martingales, this operation can be considered as
a projection. Other coeﬃcients of (23) and (24) are represented via the state
estimates and error covariances (e.g., ˆκk,l,β = Qk,l(t−)μl,β).
So after this simpliﬁcation we obtain a closed-form counterpart of the system
(19), (23), (24). Between jump times of {N d
i } and {N a
j }, it is described by the
system of linear ordinary diﬀerential equations:
˙ˆZ = Λ⊤ˆZ + λ −Qγ,
˙Q = (Q −diag[ ˆZ])Λ + Λ⊤(Q −diag[ ˆZ]) + diag[Λ⊤ˆZ + λ],
where the column vector ˆZ and the matrix Q are approximations of the state
estimate and conditional error covariance, respectively; λ = {λk}k∈H and γ =
{γk}k∈H are column vectors and Λ = {λk,l}k,l∈H is a square matrix such that
γk =

j∈J
μk,j,
λk,l = μk,l −δk,l

m/∈J
μk,m.
Fig. 1. Retrial queueing system as a stochastic network.
5
State Estimation in a Retrial Queueing System
In this section, we study a partially observed network model of inbound call
centers.
Figure 1 depicts a call center model in the form of a queueing network with
three stations. Station 1 is the main queueing system providing service for incom-
ing customers by m independent agents. For each agent, the processing time

State Estimation in Partially Observed Stochastic Networks
141
is exponentially distributed with mean 1/μ1. Customers arrive at the system
according to a Poisson stream with rate λ. The maximum number of customers
in the system is ﬁnite and denoted by K.
Station 2 contains blocked customers: they are not served in the main sys-
tem because all agents are busy, so they try to call again after a random time
exponentially distributed with mean 1/μ2.
Station 3 includes unsatisﬁed customers: after being served, they try to call
again to get additional information or extra service from the agents; such retri-
als occur after a random delay exponentially distributed with mean 1/μ3. The
probability that a customer will remain unsatisﬁed with the service is r1,3. If the
main system is busy, unsatisﬁed customers join station 2.
The initial state is assumed to be zero for all stations of the network.
Since the network belongs to the class of retrial queueing systems [1], we refer
to stations 2 and 3 as the orbits. The number of customers in the both orbits
are not observed directly; rather the state of the main system is known exactly
at each time.
Our goal is to apply the ﬁltering scheme designed above to state estimation
for two unobservable stations given the on-line information on the main queueing
system.
The queueing network we study has one observed node J = {1} and two
hidden nodes H = {2, 3}. The number of customers at node i is denoted by Xi
(i = 1, 2, 3). The transition intensities are as follows:
ν0,1 = λ (1 −β),
ν0,2 = λ β,
ν1,0 = μ1(1 −r1,3)(X1(t−) ∧m),
ν1,3 = μ1r1,3(X1(t−) ∧m),
ν2,1 = μ2X2(t−) (1 −β),
ν3,1 = μ3X3(t−) (1 −β),
ν3,2 = μ3X3(t−) β,
where β(t) = I{X1(t−) = K}.
Using point processes {Ni,j}, we can write the state dynamics
dX1 = dN0,1 + dN2,1 + dN3,1 −(dN1,0 + dN1,3),
dX2 = dN0,2 + dN3,2 −dN2,1,
dX3 = dN1,3 −(dN3,1 + dN3,2).
We have the two observed point processes with the corresponding intensities:
N a
1 = N0,1 + N2,1 + N3,1,
N d
1 = N1,0 + N1,3,
νa
1 = (λ + μ2X2(t−) + μ3X3(t−)) (1 −β),
νd
1 = μ1(X1(t−) ∧m).

142
K. V. Semenikhin
Due to (20), (21), and (22), the coeﬃcients of the state estimates ˆX2, ˆX3 take
the form:
ˆck,1 = cov{Xk(t−), νa
1 | Yt−} = (Qk,2(t−)μ2 + Qk,3(t−)μ3) (1 −β),
k = 2, 3,
ˆνa
2 = ˆν0,2 + ˆν3,2 = (λ + μ3 ˆX3(t−)) β,
ˆνd
3 = ˆν3,2 = μ3 ˆX3(t−) β,
ˆνd
2 = ˆνa
3 = 0,
ξa
k,1 = Qk,2(t−)μ2 + Qk,3(t−)μ3 −ˆXk(t−)μk
λ + μ2 ˆX2(t−) + μ3 ˆX3(t−)
,
ξd
1,2 = 0,
ξd
1,3 = r1,3.
From (25) and (26) we obtain the coeﬃcients of equations for the error covari-
ances {Qk,l}:
ˆκk,l,1 = cov{Xk(t−), νl,1 | Yt−} = Qk,l(t−)μl (1 −β),
k, l = 2, 3,
ˆb2,2 = cov{X2(t−), νa
2 | Yt−} = Q2,3(t−)μ3 β,
ˆb3,3 = cov{X3(t−), −νd
3 | Yt−} = −Q3,3(t−)μ3 β,
ˆb3,2 + ˆb2,3 = (Q3,3(t−) −Q2,3(t−))μ3 β.
Now we are ready to present the ﬁltering equations:
d ˆX2 =

(λ + μ3 ˆX3)β −(Q2,2μ2 + Q2,3μ3)(1 −β)

dt + ξa
2,1 dN a
1 ,
d ˆX3 =

−μ3 ˆX3β −(Q3,2μ2 + Q3,3μ3)(1 −β)

dt + ξa
3,1 dN a
1 + r1,3 dN d
1 ,
dQ2,2 = μ3(λ/μ3 + ˆX3 + 2Q2,3) β dt +

(ˆν2,1 −2ˆκ2,2,1)/ˆνa
1 −

ξa
2,1
2
dN a
1 ,
dQ3,3 = μ3
 ˆX3 −2Q3,3

β dt +

(ˆν3,1 −2ˆκ3,3,1)/ˆνa
1 −

ξa
3,1
2
dN a
1
+ (1 −r1,3)r1,3 dN d
1 ,
dQ2,3 = μ3

Q3,3 −Q2,3 −ˆX3

β dt −

(ˆκ2,3,1 + ˆκ3,2,1)/ˆνa
1 + ξa
2,1ξa
3,1

dN a
1 .
These equations will be referred to as the suboptimal ﬁlter (SF).
It is worth noting that just before the jump ΔN a
1 > 0 the main system has
a vacant place, so that β = 0 in all terms related to dN a
1 . In contrast, each error
(co)variance Qk,l has a non-zero drift only if the main system is full, i.e. β = 1.
To provide a comparative analysis of the estimation accuracy, we also propose
two additional ﬁltering schemes. The ﬁrst is called the truncated ﬁlter (TF)
because it is obtained by truncation of the ﬁltering equations, speciﬁcally, by
letting ˆck,j = 0 in (19). The TF estimates denoted by { ˇXk} are described by the
following equations:
d ˇX2 = (λ + μ3 ˇX3)β dt −
ˇX2(t−)μ2
λ + μ2 ˇX2(t−) + μ3 ˇX3(t−) dN a
1 ,
d ˇX3 = −μ3 ˇX3β dt −
ˇX3(t−)μ3
λ + μ2 ˇX2(t−) + μ3 ˇX3(t−) dN a
1 + r1,3 dN d
1 .
The second ﬁlter used for comparison is the drift-based ﬁlter (DF). The DF
estimates are denoted by { ¯Xk}. To deﬁne them, we replace each point process

State Estimation in Partially Observed Stochastic Networks
143
dNk,l in the dynamics of dXk with the drift term ¯νk,l dt. So we obtain a system
of linear ODEs:
˙¯X2 = (λ + μ3 ¯X3)β −μ2 ¯X2(1 −β),
˙¯X3 = μ1r1,3(X1(t−) ∧m) −μ3 ¯X3.
For numerical experiments we choose the following parameters:
m = 20,
K = 25,
λ = 21,
μ1 = 2,
μ2 = 10.5,
μ3 = 4.2,
r1,3 = 0.45.
We take λ less than but close to μ1(1−r1,3)m in order for the main system to be
near the loaded state. In this case, customers are blocked more frequently but
the load of station 2 behaves stable.
Table 1 contains root-mean square errors (RMSEs) obtained in one experi-
ment. The estimation accuracy has been evaluated on 10 time intervals (with
1000 jumps of the network process in each interval). Figure 2 shows sample paths
of the states and suboptimal estimates on two time intervals.
Table 1. Estimation errors over several segments along one sample path
Segment:
1
2
3
4
5
6
7
8
9
10
Total
RMSE of ˇ
X2: 3.976 0.733 1.337 5.107 8.097 9.849 0.167 2.310 2.401 3.497 4.631
RMSE of ¯
X2: 2.396 1.043 1.617 1.891 3.806 2.815 0.201 1.424 2.465 2.267 2.145
RMSE of ˆ
X2: 2.145 0.840 1.305 1.933 2.755 2.763 0.188 1.191 2.118 2.193 1.867
RMSE of ˇ
X3: 1.939 1.830 1.648 1.772 1.926 2.283 2.042 1.949 1.977 1.935 1.934
RMSE of ¯
X3: 2.104 1.716 1.975 1.906 1.973 2.112 1.986 2.166 1.691 1.959 1.962
RMSE of ˆ
X3: 1.692 1.609 1.771 1.569 1.489 1.830 1.663 1.947 1.555 1.852 1.705
Our experiment shows the superiority of the suboptimal scheme over two
other ﬁltering algorithms. However it should be noted that the drift-based scheme
demonstrates relatively close results: its RMSE ranges within 10–15% in com-
parison with the suboptimal ﬁlter for both hidden stations. In contrast, the
truncated scheme turns to be much worse in estimating the number of blocked
customers.
Figure 3 depicts RMSE trajectories evaluated on the basis of 1000 Monte
Carlo runs. Basically, this experiment conﬁrms the results obtained along one
sample path, though the accuracy of SF and DF estimates become more similar
for station 2 before achieving the steady state mode.
6
Appendix
Proof of Lemma 1. Due to the monotone class theorem and the dominated-
convergence theorem, it suﬃces to consider an F-predictable step process

144
K. V. Semenikhin
Fig. 2. Sample paths of states (shown as solid lines) and SF-estimates (shown as dashed
lines).

State Estimation in Partially Observed Stochastic Networks
145
Fig. 3. RMSE of three ﬁlters SF (solid), DF (dashed), and TF (dotted) for two states
X2 (red) and X3 (blue).
ξ(s) = U I(t1,t2](s), where U is a bounded Ft1-measurable random variable. Then,
E
 t
0
ξ dNα,β



 Yt

= E

U(Nα,β(t2) −Nα,β(t1))


 Yt

∀t ≥t2.
(30)
We introduce two σ-algebras Ft1,t2 and Yt1,t2. Both of them are generated by
the increments {N(s) −N(t1): t1 ≤s ≤t2}, where for Ft1,t2, N is any of Nα,β,
whereas for Yt1,t2, N is any observed process Ni,j, N d
i , or N a
j (i, j ∈J).
It is important that Ft1 and Ft1,t2 are independent under P.
Note that Ys−is generated by events AB such that A ∈Yt1 and B ∈Yt1,s′
for some s′ ∈(t1, s). Since A, B are independent, we have
E

E{U | Yt1} IAB

= E

E{U | Yt1} IA

E{IB} = E{U IA} E{IB} = E{U IAB}
and hence
E

U


 Yt1

= E

U


 Ys−

∀s > t1.
Therefore, the right-hand side of the integral equalities in (6) is
E

U


 Yt1

(t2 −t1).
(31)

146
K. V. Semenikhin
For the right-hand side of (3) and (4)–(5) we have
E

U


 Yt1

(Ni,j(t2) −Ni,j(t1))
and
E

U


 Yt1

(N(t2) −N(t1))/p,
(32)
respectively, where N stands for N d
i or N a
j .
To prove that (30) equals (31) or (32), we consider an event AB ∈Yt such
that A ∈Yt1 and B ∈Yt1,t.
The random variable Dk,l = Nk,l(t2) −Nk,l(t1) (k, l /∈J) and σ-algebras Ft1
and Yt1,t are mutually independent. This implies
E{UDk,l IAB} = E{U IA}E{IB}E{Dk,l}
= E{E(U | Yt1) IA}E{IB}(t2 −t1) = E{E(U | Yt1)(t2 −t1) IAB}.
In the case i, j ∈J, pairs {U, A} and {Di,j, B} are independent. Therefore,
we obtain
E{UDi,j IAB} = E{U IA}E{Di,j IB} = E{E(U | Yt1) IA}E{Di,j IB}
= E{E(U | Yt1)Di,j IAB}.
In the case i ∈J, k /∈J, we use the same independence:
E{UDi,k IAB} = E{E(U | Yt1) IA}E{Di,k IB}.
It remains to note that E{Di,k | Yt1,t} = Di/p, where Di = N d
i (t2) −N d
i (t1).
This follows from two facts:
1) N d
i is a sum of the processes {Ni,l, l /∈J} that
are independent of all observed processes except for N d
i ;
2) Di,k and Di −Di,k
are independent Poisson variables with parameters proportional to 1 and p −1,
respectively, and hence E{Di,k | Di} = Di/p.
Thus, we have established (6), (3), and (4). Equality (5) can be veriﬁed
similarly to (4). A proof of (2) can be found in [19, Lemma 7.3.2].
□
Proof of Lemma 2. The exponential Θ(t) is positive due to condition (8) [10,
4.62]. To prove the martingale property for Θ(t), we can apply [11, Th.5.1]:
it suﬃces to note that Θ(t) is deﬁned by a local P-martingale M(t) with the
integrand that grows no faster than a linear function of the state X(t). The last
condition coincides with (9). Statements 2–4 can be proved similarly to [19].
To prove the last part, we need to verify that the process Mα,β satisfying (1)
is a P-martingale. To do this, we will prove that Mα,βΘ is a P-martingale.
Applying Ito’s rule, we obtain
d(Mα,βΘ) = Mα,β(t−) dΘ + Θ(t−) dMα,β + ΔMα,βΔΘ.
Since the ﬁrst term in the right-hand side deﬁnes a P-martingale, it remains to
see that the other terms yield a P-martingale as well:
Θ(t−)(dNα,β −να,β dt) + Θ(t−)(να,β −1) dNα,β = Θ(t−)να,β d
◦
N α,β.
□

State Estimation in Partially Observed Stochastic Networks
147
References
1. Artalejo, J.R., Gomez-Corral, A.: Retrial Queueing Systems: A Computational
Approach. Springer, Berlin (2008)
2. Baccelli, F., Kauﬀmann, B., Veitch, D.: Inverse problems in queueing theory and
Internet probing. Queueing Syst. 63, 59–107 (2009)
3. Bensoussan, A., Cakanyildirim, M., Sethi, S.P., Shi, R.: An incomplete information
inventory model with presence of inventories or backorders as only observations.
J. Optim. Theory Appl. 146(3), 544–580 (2010)
4. Borisov, A.V.: Application of optimal ﬁltering methods for on-line of queueing
network states. Autom. Remote. Control. 77, 277–296 (2016)
5. Bremaud, P.: On the output theorem of queueing theory, via ﬁltering. J. Appl.
Probab. 15(2), 397–405 (1978)
6. Elliott, R.J., Aggoun, L., Moore, J.B.: Hidden Markov Models, Estimation and
Control. Springer, New York (2008)
7. Elliott, R.J., Dufour, F., Malcolm, W.P.: State and mode estimation for discrete-
time jump Markov systems. SIAM J. Control Optim. 44(3), 1081–1104 (2005)
8. El-Taha, M., Stidham, S.: A ﬁltered ASTA property. Queueing Syst. 11, 211–222
(1992)
9. Hassin, R.: Rational Queueing. CRC Press, Boca Raton (2016)
10. Jacod, J., Shiryaev, A.N.: Limit Theorems for Stochastic Processes, 2nd edn.
Springer, New York (2003)
11. Klebaner, F., Liptser, R.: When a stochastic exponential is a true martingale.
Extension of the Beneˇs method. Theory Probab. Appl. 58(1), 38–62 (2014)
12. Li, X., Youseﬁ’zadeh, H.: Robust EKF-based wireless congestion control. IEEE
Trans. Commun. 61(12), 5090–5102 (2013)
13. Lukashuk, L.I., Semenchenko, Y.A.: Filtering of a semi-Markov queueing system
with retrials. Cybern. Syst. Anal. 27(4), 627–631 (1991)
14. Miller, B.M., Avrachenkov, K.E., Stepanyan, K.V., Miller, G.B.: Flow control
as a stochastic optimal control problem with incomplete information. Probl. Inf.
Transm. 41(2), 150–170 (2005)
15. Miller, B.M., Miller, G.B., Semenikhin, K.V.: Optimal channel choice for lossy data
ﬂow transmission. Autom. Remote. Control. 79(1), 66–77 (2018)
16. Rieder, U., Winter, J.: Optimal control of Markovian jump processes with partial
information and applications to a parallel queueing model. Math. Meth. Oper. Res.
70, 567–596 (2009)
17. Stuckey, N., Vasquez, J., Graham, S., Maybeck, P.: Stochastic control of computer
networks. IET Control Theory Appl. 6(3), 403–411 (2012)
18. Walrand, J., Varaiya, P.: Flows in queueing networks: a martingale approach. Math.
Oper. Res. 6(3), 387–404 (1981)
19. Wong, E., Hajek, B.: Stochastic Processes in Engineering Systems. Springer, New
York (1985)

Estimation of Equilibria in an Advertising
Game with Unknown Distribution
of the Response to Advertising Eﬀorts
Alan D. Robles-Aguilar1, David Gonz´alez-S´anchez2,
and J. Adolfo Minj´arez-Sosa3(B)
1 Instituto Tecnol´ogico de Sonora, Cd. Obreg´on, Sonora, Mexico
alan_daniel@yahoo.com
2 CONACYT–Universidad de Sonora, Rosales s/n,
83000 Hermosillo, Sonora, Mexico
david.glzsnz@gmail.com
3 Departamento de Matem´aticas, Universidad de Sonora, Rosales s/n,
83000 Hermosillo, Sonora, Mexico
aminjare@gauss.mat.uson.mx
Abstract. We study a class of discrete-time advertising game with ran-
dom responses to the advertising eﬀorts made by a duopoly. The ﬁrms
are assumed to observe the values of the random responses but they do
not know their distributions. With the recorded values, ﬁrms estimate
distributions and play estimated equilibrium strategies. Under suitable
assumptions, we prove that the estimated equilibrium strategies con-
verge to equilibria of the advertising game with the true distributions.
Our results are numerically illustrated for speciﬁc cases.
Keywords: Advertising games · Lanchester model · Markov games ·
Empirical distribution
AMS (2020) Subject Classiﬁcation: Primary 91A15 · Secondary
91A80
1
Introduction
We consider a dynamic noncooperative game of advertising where the market
shares of the ﬁrms follow a stochastic diﬀerence equation. The stochastic behav-
ior in the market shares comes from the uncertain responses to advertising eﬀorts
modeled by a sequence of random variables. Further, we assume that ﬁrms can
observe the values of such random variables a posteriori but they do not know the
distributions. In this sense, by using appropriate statistical estimation methods
to approximate the distributions of the random variables, ﬁrms can play Nash
equilibrium strategies of the estimated games. When these equilibrium strate-
gies converge, the question we aim to answer is whether the limit strategies are
equilibria for the game with the true distributions of the responses to advertising
eﬀorts.
c
⃝The Author(s), under exclusive license to Springer Nature Switzerland AG 2021
A. Piunovskiy and Y. Zhang (Eds.): Modern Trends in Controlled Stochastic Processes,
ECC 41, pp. 148–165, 2021. https://doi.org/10.1007/978-3-030-76928-4_8

Estimation of Equilibria Strategies
149
The literature about dynamic models of advertising and marketing games is
very large; we can mention the papers [4,6,7,21] and the books [2,8]. Most of
these references mainly focus on deterministic diﬀerential game models; instead
there are few works that deal with stochastic diﬀerential game models and deter-
ministic discrete-time models, we can cite, for instance, [1,18]. On the other
hand, discrete-time stochastic zero-sum games with incomplete information have
been studied under several context, see, e.g., [5,10,12–16,22,23], which include
the case when the transition law among states is unknown. However, to the best
of our knowledge, the only work dealing on estimation problem for nonzero-
sum Markov games is [19]. Speciﬁcally, in [19] is used the empirical distribution
of the disturbance process to obtain an almost surely convergent procedure to
approximate Nash equilibria under the discounted criterion.
In this chapter we analyze the stochastic version of the advertising Lanch-
ester model introduced in [1]. Additionally, we assume that the random variables
modeling the uncertainty in responses to advertising eﬀorts have unknown distri-
butions. Under this scenario, using the empirical distribution as an estimator and
considering ﬁnite action sets for players, we apply similar ideas to [19] to simulate
values of the advertising responses, estimate equilibrium strategies, and prove
that these equilibria converge in some sense to an equilibrium of the advertising
game with full information. In order to introduce the model and compare our
results, previously we analyze the advertising game with full information, where
we numerically compute the Nash equilibria in mixed stationary strategies.
The remaining of the paper is organized as follows. The stochastic advertising
game we deal with is described in Sect. 2 as well as the numerical algorithm we
use to compute the Nash equilibria. Section 3 is devoted to the stochastic game
with unknown distributions of the advertising responses. Finally, in Sect. 4, we
give some conclusions.
2
A Discrete-Time Stochastic Game of Advertising
Essentially, Lanchester model is an ordinary-diﬀerential-equation model of war-
fare [11]. Over time, this model has been adapted to study diﬀerent conﬂict
situations, including advertising models. In this section, we introduce a discrete-
time stochastic version of the Lanchester model in the context of the models
that appear in [1] and [8, pp. 29–31]. We also give a numerical algorithm to ﬁnd
Nash equilibria in stationary strategies of the proposed model.
2.1
The Advertising Game Model
Consider a duopoly competing for the market share by making advertising
eﬀorts. Let x be the market share of Firm 1 and let a and b be the adver-
tising eﬀorts of Firm 1 and Firm 2, respectively, at some decision epoch. The
market share of Firm 2 is 1−x. Then the market share of Firm 1 at the beginning
of the next decision epoch is determined by the mapping
(x, a, b) →x + (1 −x)d(ξ, a) −xe(ζ, b)
(1)

150
A. D. Robles-Aguilar et al.
where d(ξ, a) and e(ζ, b) are the advertising responses to a and b, respectively,
and (ξ, ζ) is a pair of random variables. The functions d(i, ·) and e(j, ·)—for ﬁxed
values of i and j—are production functions, that is, they are increasing, have
diminishing marginal eﬀects, and take nonnegative values. Typical advertising
responses are
d(ξ, a) = ξ√a,
e(ζ, b) = ζ
√
b.
(2)
The evolution of the state system is given by the mapping (1) and has the
following interpretation: the advertising of Firm 1 aims to attract customers
from Firm 2, thus the increment of the market share is proportional to (1 −x),
and analogously for the advertising made by Firm 2.
For the purposes of this paper, we assume that the triples (x, a, b) belong to
a ﬁnite set X × A × B. Thus the image of the mapping (1)—with the advertising
responses (2), for instance—is not necessarily a subset of X. In such a case, we
map x+(1−x)d(ξ, a)−xe(ζ, b) to the nearest state in X. Although, for simplicity,
we write
xk+1 = xk + (1 −xk)d(ξk, ak) −xke(ζk, bk),
k = 0, 1, ...,
(3)
where x0 ∈X is given. In addition, the so-called disturbance processes {ξk} and
{ζk} consist of independent and identically distributed (i.i.d.) random variables,
which take values in the ﬁnite sets S1 and S2 respectively. The process {(ξk, ζk)}
is deﬁned on some underlying probability space (Ω, F, P). The common proba-
bility functions of the random variables {ξk} and {ζk} are, respectively, θ and
ϑ, that is,

θ(i) = P[ξk = i]
∀i ∈S1, k ∈N0,
ϑ(j) = P[ζk = j]
∀j ∈S2, k ∈N0.
(4)
We use the notation K := {(x, a, b) : x ∈X, a ∈A, b ∈B}. Combining
(3) and (4), we obtain the transition law among the states as follows. For each
(x, a, b) ∈K,
Px,y[a, b] := P[xk+1 = y | xk = x, ak = a, bk = b] =

(i,j)∈SF
θ(i)ϑ(j),
y ∈X
(5)
where
SF := {(s, t) ∈S1 × S2 : x + (1 −x)d(s, a) −xe(t, b) = y} .
Finally, ri : K →R is the one-stage payoﬀfunction for the Firm i = 1, 2,

r1(x, a, b) = p1x −a
r2(x, a, b) = p2(1 −x) −b
(6)

Estimation of Equilibria Strategies
151
where p1 and p2 are the gross proﬁt rate of Firms 1 and 2 respectively. In what
follows, the probability space (Ω, F, P) is ﬁxed and a.s. means almost surely
with respect to P.
Putting together all the elements described above, we deﬁne the advertising
game model as
Gθ,ϑ := (X, A, B, S1, S2, θ, ϑ, r1, r2)
(7)
The model is a representation of a dynamic game which is played as follows.
At each stage k ∈N0, when the game is in state xk ∈X, the ﬁrms independently
choose actions ak = a ∈A and bk = b ∈B. Consequently, the following happens:
ﬁrst, Firm i receives payoﬀs of ri(x, a, b), i = 1, 2; and second, the system moves
to the next state xk+1 ∈X according to probability transition (5). Once the
system reaches the next state, the process repeats. In addition, the payoﬀs are
accumulated according to a discounted criterion, as we will deﬁne below.
Let PA and PB consist of the set of all probability functions on A and
B respectively. That is, PA is the set of functions σ : A →[0, 1] such that

a∈A σ(a) = 1. Similarly for PB. By convention, for each σ ∈PA, τ ∈PB, we
denote
v(x, σ, τ) :=

a∈A

b∈B
v(x, a, b)σ(a)τ(b),
x ∈X
(8)
for any function v : K →R. Likewise, for σ ∈PA, τ ∈PB
[x+(1−x)d(s, σ)−xe(t, τ)] :=

a∈A

b∈B
[x+(1−x)d(s, a)−xe(t, b)]σ(a)τ(b), (9)
where x ∈X, s ∈S1, and t ∈S2.
A strategy played by Firm 1 is a sequence π = {πk} where πk is a probability
function over A conditioned on the history hk := (x0, a0, b0, ..., ak1, bk1, xk) That
is, for each history hk, πk(·|hk) ∈PA. The set of all strategies for Firm 1 is
denoted by Π. A strategy π ∈Π is said to be a Markov strategy if there is
a probability function fk over A such that πk(·|hk) = fk(·|xk) for all k ∈N0.
Further, a Markov strategy π = {fk} is stationary if fk = f for all k ∈N0; in
this case, we use this notation
f ∞:= {f, f, f, ...}.
We denote by ΠM and F the sets of Markov strategies and stationary strate-
gies, respectively, for Firm 1. The sets Γ, ΓM, and G of all strategies, Markov
strategies, and stationary strategies for Firm 2 are deﬁned similarly.

152
A. D. Robles-Aguilar et al.
Let π = {πk} ∈Π and γ = {γk} ∈Γ be a pair of strategies. For each
initial state x ∈X, we deﬁne the discounted criterion, also known as expected
discounted payoﬀ, for Firm i = 1, 2, as
⎧
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎩
Jθ,ϑ
1
= E(π,γ)
x
	 ∞

k=0
βk{p1xk −ak}

Jθ,ϑ
2
= E(π,γ)
x
	 ∞

k=0
βk{p2(1 −xk) −bk}

(10)
where β ∈(0, 1) is the discount factor and E(π,γ)
x
denotes the expectation oper-
ator corresponding to the unique probability measure P (π,γ)
x
induced by x ∈X
and (π, γ) ∈Π × Γ, (see [3]).
2.2
Stationary Nash Equilibrium in Discounted Games
Deﬁnition 1. A pair of strategies (π∗, γ∗) ∈Π × Γ is a Nash equilibrium if,
for all x ∈X,
Jθ,ϑ
1
(x, π∗, γ∗) ≥Jθ,ϑ
1
(x, π, γ∗),
∀π ∈Π
and
Jθ,ϑ
2
(x, π∗, γ∗) ≥Jθ,ϑ
2
(x, π∗, γ),
∀γ ∈Γ.
The equilibrium payoﬀs of the game, with initial state x, are Jθ,ϑ
1
(x, π∗, γ∗) and
Jθ,ϑ
2
(x, π∗, γ∗).
The following lemma about the existence of Nash equilibria in Markov strate-
gies for this model is well known. For instance, see [17, Theorem 5.1].
Lemma 1. The game model, with discounted payoﬀs Jθ,ϑ
1
and Jθ,ϑ
2
, has a Nash
equilibrium in stationary strategies. That is, there exists (f ∞, g∞) ∈F × G such
that for each x ∈X,
Jθ,ϑ
1
(x, f ∞, g∞) ≥Jθ,ϑ
1
(x, π, g∞),
∀π ∈Π
and
Jθ,ϑ
2
(x, f ∞, g∞) ≥Jθ,ϑ
2
(x, f ∞, γ),
∀γ ∈Γ.
Observe that once f ∞∈F and g∞∈G are ﬁxed,
J1(x, π) := Jθ,ϑ
1
(x, π, g∞),
π ∈Π,
x ∈X

Estimation of Equilibria Strategies
153
and
J2(x, γ) := Jθ,ϑ
2
(x, f ∞, γ),
γ ∈Γ,
x ∈X
constitute performance indices, where each of them corresponds to an optimal
control problem. Hence, the value functions
V (x) := max
π∈Π J1(x, π),
x ∈X
(11)
and
W(x) := max
γ∈Γ J2(x, γ),
x ∈X,
(12)
satisfy, respectively, the Dynamic Programming equations
V (x) = max
μ∈PA
⎡
⎣[p1x −μ] + β

(i,j)∈S1×S2
V [x + (1 −x)d(i, μ) −xe(j, g)]θ(i)ϑ(j)
⎤
⎦
(13)
= [p1x−f]+β

(i,j)∈S1×S2
V [x+(1−x)d(i, f)−xe(j, g)]θ(i)ϑ(j),
∀x ∈X, (14)
and
W(x)
= max
λ∈PB
⎡
⎣[p2(1 −x) −λ] + β

(i,j)∈S1×S2
W[x + (1 −x)d(i, f) −xe(j, λ)]θ(i)ϑ(j)
⎤
⎦(15)
= [p2(1 −x) −g] + β

(i,j)∈S1×S2
W[x + (1 −x)d(i, f) −xe(j, g)]θ(i)ϑ(j), ∀x ∈X.
(16)
Remark 1. By considering standard dynamic programming arguments, if there
are functions V and W and a pair (f, g) satisfying (13)–(16), then (f ∞, g∞) ∈
F × G is a stationary Nash equilibrium for the game with discounted pay-
oﬀs (10). Further, the equilibrium payoﬀs are Jθ,ϑ
1
(x, f ∞, g∞) = V (x) and
Jθ,ϑ
2
(x, f ∞, g∞) = W(x).
2.3
Numerical Examples
We compute the equilibria in Markov strategies for an advertising game with
the data of Table 1.
The equilibrium strategies are found using and adaptation of the well-
known value iteration algorithm from discounted dynamic programming. In each

154
A. D. Robles-Aguilar et al.
1.0
0.95
0.9
0.85
0.8
0.75
0.7
0.65
0.6
0.55
0.5
0.45
0.4
0.35
0.3
0.25
0.2
0.15
0.1
0.05
0.0
State
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
0.55
0.6
0.65
0.7
0.75
0.8
0.85
0.9
0.95
1
Mixed Strategies
Equilibrium Strategies Firm 1
Action 0.01
Action 0.02
Action 0.03
Action 0.04
1.0
0.95
0.9
0.85
0.8
0.75
0.7
0.65
0.6
0.55
0.5
0.45
0.4
0.35
0.3
0.25
0.2
0.15
0.1
0.05
0.0
State
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
0.55
0.6
0.65
0.7
0.75
0.8
0.85
0.9
0.95
1
Mixed Strategies
Equilibrium Strategies Firm 2
Action 0.01
Action 0.02
Action 0.03
Action 0.04
Fig. 1. Equilibrium strategies f and g in the full-information game with data of Table 1.
The height of each action is the probability it is played with.
iteration we get the equilibrium by minimizing McKelvey’s function, see
[9, p. 133]. For the parameters given above, the iteration algorithm converges.
The algorithm is implemented in Python and the code is available at
https://github.com/adra1973/
The limit strategies (f, g), that form the stationary equilibrium (f ∞, g∞), are
plotted in Fig. 1 and 2. Since we are using exactly the same parameters for both
ﬁrms, in Fig. 1 we can observe for each state an eﬀect of “mirror” in the strategies
for both ﬁrms.

Estimation of Equilibria Strategies
155
1.0
0.95
0.9
0.85
0.8
0.75
0.7
0.65
0.6
0.55
0.5
0.45
0.4
0.35
0.3
0.25
0.2
0.15
0.1
0.05
0.0
State
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
0.55
0.6
0.65
0.7
0.75
0.8
0.85
0.9
0.95
1
Mixed Strategies
Equilibrium Strategies Firm 1
Action 0.01
Action 0.02
Action 0.03
Action 0.04
1.0
0.95
0.9
0.85
0.8
0.75
0.7
0.65
0.6
0.55
0.5
0.45
0.4
0.35
0.3
0.25
0.2
0.15
0.1
0.05
0.0
State
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
0.55
0.6
0.65
0.7
0.75
0.8
0.85
0.9
0.95
1
Mixed Strategies
Equilibrium Strategies Firm 2
Action 0.03
Action 0.04
Action 0.05
Action 0.06
Fig. 2. Equilibrium strategies f and g in the full-information game with data from
Table 1 but the set of actions for Firm 2 is replaced by (17).
In Fig. 2 we plot the equilibrium strategies for the game with the same data
of Table 1 but the set of actions for Firm 2 now is
B = {0.03, 0.04, 0.05, 0.06}
(17)
and thus the behavior of the strategies breaks the “mirror” observed before.

156
A. D. Robles-Aguilar et al.
Table 1. Data for the advertising game.
Variable Description
X
Space of 21 states of market shares,
{0.0, 0.05, 0.1, 0.15, 0.2, ..., 0.8, 0.85, 0.9, 0.95, 1.0},
A
Set of 4 actions for advertising eﬀort of Firm 1,
A = {0.01, 0.02, 0.03, 0.04},
B
Set of 4 actions for advertising eﬀort of Firm 2,
B = {0.01, 0.02, 0.03, 0.04},
S1
Set of 10 values of Firm 1, S1 = {0.95, ..., 1.05}
S2
Set of 10 values of Firm 2, S2 = {0.95, ..., 1.05}
ξ
Random variable of Firm 1 that take values in S1 with
probability θ(i), i ∈S1, ξ ∼Binomial(10, 0.4)
ζ
Random variable of Firm 2 that take values in S2 with
probability ϑ(j), j ∈S2, ζ ∼Binomial(10, 0.4).
d
Advertising response function of Firm 1, d(ξ, a) = ξ√a,
a ∈A
e
Advertising response function of Firm 2, e(ζ, b) = ζ
√
b,
b ∈B
p1
Gross proﬁt for each product sold by Firm 1, p1 = 1.2
p2
Gross proﬁt for each product sold by Firm 2, p2 = 1.2
β
The discount factor β = 0.95
3
The Advertising Game with Unknown Distribution
In this section, we study the advertising game when the distributions of the
random variables (ξ, ζ) are unknown for the players. We assume that, after the
n−th stage, players have recorded the values ξn := (ξ0, ξ1, ..., ξn) and ζn :=
(ζ0, ζ1, ..., ζn) and use the empirical distributions
θn(i) := 1
n
n−1

t=0
1i(ξt),
i ∈S1,
n ∈N
and
ϑn(j) := 1
n
n−1

t=0
1j(ζt),
j ∈S2,
n ∈N
to estimate equilibrium strategies. More precisely, for each n ∈N, consider the
empirical advertising game
Gθn,ϑn := (X, A, B, S1, S2, θn, ϑn, r1, r2)
(18)
with dynamics (3) and payoﬀs (10), where θ and ϑ are replaced by θn and ϑn,
respectively. Given a stationary Nash equilibrium (f ∞
n , g∞
n ) for the empirical
advertising game (18), by well-known dynamic programming results, there exist
functions Vn and Wn that satisfy the optimality equations

Estimation of Equilibria Strategies
157
Vn(x) = max
μ∈PA
⎡
⎣[p1x −μ] + β

i,j
Vn[x + (1 −x)d(i, μ) −xe(j, gn)]θn(i)ϑn(j)
⎤
⎦
(19)
= [p1x −fn] + β

i,j
Vn[x + (1 −x)d(i, fn) −xe(j, gn)]θn(i)ϑn(j),
x ∈X,
and
Wn(x)
= max
λ∈PB
⎡
⎣[p2(1 −x) −λ] + β

i,j
Wn[x + (1 −x)d(i, fn) −xe(j, λ)]θn(i)ϑn(j)
⎤
⎦
(20)
= [p2(1 −x) −gn] + β

i,j
W[x + (1 −x)d(i, fn) −xe(j, gn)]θn(i)ϑn(j), x ∈X.
Remark 2. Notice that Vn and Wn are deﬁned on X × Ω, thus Vn(x) and Wn(x)
are random variables for each x ∈X. The strategies fn and gn are also random
vectors.
The following proposition is based on [19]; for completeness, we outline a
proof in the scenario of the present work.
Proposition 1. For each n ∈N, let fn, gn, Vn, and Wn satisfy (19) and (20).
If
lim
n→∞(fn, gn) = (f, g)
P −a.s.
(21)
and
lim
n→∞(Vn, Wn) = (V, W)
P −a.s.,
(22)
then (f ∞, g∞) is P −a.s. a Nash equilibrium for the advertising game with
dynamics (3) and payoﬀs (10).
Proof. It is well known that from the strong law of large numbers,
(θn, ϑn) →(θ, ϑ)
P −a.s.
(23)
Now, ﬁx ω in Ω such that the convergence in (21), (22), and (23) holds. Then,
for each μ ∈PA, x ∈X, and n ∈N,

i,j
Vn[x + (1 −x)d(i, μ) −xe(j, gn)]
−V [x + (1 −x)d(i, μ) −xe(j, gn)]
θn(i)ϑn(j)
≤

i,j
max
x∈X
Vn(x) −V (x)
θn(i)ϑn(j)
≤max
x∈X
Vn(x) −V (x)
.
(24)

158
A. D. Robles-Aguilar et al.
and

i,j
V [x + (1 −x)d(i, μ) −xe(j, gn)]
−V [x + (1 −x)d(i, μ) −xe(j, g)]
θn(i)ϑn(j)
≤

i,j

b∈B
V [x + (1 −x)d(i, μ) −xe(j, b)]

gn(b|x) −g(b|x)
θn(i)ϑn(j)
≤max
x∈X |V (x)|

b∈B
gn(b|x) −g(b|x)

(25)
Thus

i,j
Vn[x + (1 −x)d(i, μ) −xe(j, gn)]θn(i)ϑn(j)
−V [x + (1 −x)d(i, μ) −xe(j, g)]θ(i)ϑ(j)

≤

j∈S
Vn[x + (1 −x)d(i, μ) −xe(j, gn)]θn(i)ϑn(j)
−V [x + (1 −x)d(i, μ) −xe(j, gn)]θn(i)ϑn(j)

+

j∈S
V [x + (1 −x)d(i, μ) −xe(j, gn)]θn(i)ϑn(j)
−V [x + (1 −x)d(i, μ) −xe(j, g)]θn(i)ϑn(j)

+

j∈S
V [x + (1 −x)d(i, μ) −xe(j, g)]θn(i)ϑn(j)
−V [x + (1 −x)d(i, μ) −xe(j, g)]θ(i)ϑ(j)
.
Then, (24), (25), and (23) imply
lim
n→∞

i,j
Vn[x + (1 −x)d(i, μ) −xe(j, gn)]θn(i)ϑn(j)
=

i,j
V [x + (1 −x)d(i, μ) −xe(j, g)]θ(i)ϑ(j)
P −a.s.
(26)
for each μ ∈PA and x ∈X. We can also show that
lim
n→∞

i,j
Vn[x + (1 −x)d(i, fn) −xe(j, gn)]θn(i)ϑn(j)
=

i,j
V [x + (1 −x)d(i, f) −xe(j, g)]θ(i)ϑ(j)
P −a.s.
(27)

Estimation of Equilibria Strategies
159
On the other hand, from (24) and (26), we have
Vn(x) ≥[p1x −μ] + β

i,j
Vn[x + (1 −x)d(i, μ) −xe(j, gn)]θ(i)ϑ(j)
∀μ ∈PA
and hence, by letting n →∞,
V (x) ≥[p1x −μ] + β

i,j
V [x + (1 −x)d(i, μ) −xe(j, g)]θ(i)ϑ(j)
∀μ ∈PA.
Furthermore, the second equality in (24) and (27) yield
V (x) = max
μ∈PA
⎡
⎣[p1x −μ] + β

i,j
V [x + (1 −x)d(i, μ) −xe(j, g)]θ(i)ϑ(j)
⎤
⎦
= [p1x −f] + β

i,j
V [x + (1 −x)d(i, f) −xe(j, g)]θ(i)ϑ(j),
P −a.s.
The following equalities are analogously proved
W(x) = max
λ∈PB

[p2(1 −x) −λ] + β

i,j
W[x + (1 −x)d(i, f) −xe(j, λ)]θ(i)ϑ(j)

= [p2(1 −x) −g] + β

i,j
W[x + (1 −x)d(i, f) −xe(j, g)]θ(i)ϑ(j),
P −a.s.
These optimality equations prove that (f ∞, g∞) is a stationary Nash equilibrium
P −a.s. for the advertising game.
⊓⊔
3.1
Numerical Examples for the Empirical Game Model
In order to generate simulations of the empirical games Gθm,ϑm, we use the
algorithm in [20, p. 56] to produce values from a Binomial random variable. All
parameters are exactly the same as in Table 1 but the pair (θ, ϑ) is replaced
by (θm, ϑm). As in Subsection 2.3, we compute the stationary Nash equilibrium
(f ∞
m , g∞
m ) for each empirical game Gθm,ϑm, with m ∈N0.
For a realization ω ∈Ω and diﬀerent values of m, the equilibrium strategies
(fm, gm) are plotted in Fig. 3 and 4, and equilibrium payoﬀs (Vm, Wm) are shown
in Fig. 5 and 6. By looking at the proof of Proposition 1, if (21) and (22) hold
for a given value of ω, then the limit strategy pair (f, g) determines a stationary
Nash equilibrium of the full information game. The equilibrium strategy (f, g)
or equilibrium payoﬀs (V, W) for the full-information model (7) are also plotted
on the right of each ﬁgure.
A numerical validation of the hypotheses in Proposition 1 would consist in
simulating empirical games for inﬁnitely many realizations of ω, computing the
equilibria along with the payoﬀs, and verifying (21) and (22). From a practical
point of view, however, ﬁrms record the values of the random variables—and

160
A. D. Robles-Aguilar et al.
1e+0
5e+0
1e+1
5e+1
1e+2
5e+2
1e+3
5e+3
1e+4
5e+4
1e+5
5e+5
1e+6
5e+6
1e+7
1e+8
1e+9
Full-Info
m
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
0.55
0.6
0.65
0.7
0.75
0.8
0.85
0.9
0.95
1
Mixed Strategies
State 0.25, Firm 1
Action 0.01
Action 0.02
Action 0.03
Action 0.04
1e+0
5e+0
1e+1
5e+1
1e+2
5e+2
1e+3
5e+3
1e+4
5e+4
1e+5
5e+5
1e+6
5e+6
1e+7
1e+8
1e+9
Full-Info
m
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
0.55
0.6
0.65
0.7
0.75
0.8
0.85
0.9
0.95
1
Mixed Strategies
State 0.5, Firm 1
Action 0.01
Action 0.02
Action 0.03
Action 0.04
1e+0
5e+0
1e+1
5e+1
1e+2
5e+2
1e+3
5e+3
1e+4
5e+4
1e+5
5e+5
1e+6
5e+6
1e+7
1e+8
1e+9
Full-Info
m
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
0.55
0.6
0.65
0.7
0.75
0.8
0.85
0.9
0.95
1
Mixed Strategies
State 0.75, Firm 1
Action 0.01
Action 0.02
Action 0.03
Action 0.04
1e+0
5e+0
1e+1
5e+1
1e+2
5e+2
1e+3
5e+3
1e+4
5e+4
1e+5
5e+5
1e+6
5e+6
1e+7
1e+8
1e+9
Full-Info
m
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
0.55
0.6
0.65
0.7
0.75
0.8
0.85
0.9
0.95
1
Mixed Strategies
State 0.85, Firm 1
Action 0.01
Action 0.02
Action 0.03
Action 0.04
Fig. 3. Estimated equilibrium strategies of Firm 1 for diﬀerent values of m at the states
0.25, 0.5, 0.75, and 0.85.

Estimation of Equilibria Strategies
161
1e+0
5e+0
1e+1
5e+1
1e+2
5e+2
1e+3
5e+3
1e+4
5e+4
1e+5
5e+5
1e+6
5e+6
1e+7
1e+8
1e+9
Full-Info
m
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
0.55
0.6
0.65
0.7
0.75
0.8
0.85
0.9
0.95
1
Mixed Strategies
State 0.25, Firm 2
Action 0.01
Action 0.02
Action 0.03
Action 0.04
1e+0
5e+0
1e+1
5e+1
1e+2
5e+2
1e+3
5e+3
1e+4
5e+4
1e+5
5e+5
1e+6
5e+6
1e+7
1e+8
1e+9
Full-Info
m
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
0.55
0.6
0.65
0.7
0.75
0.8
0.85
0.9
0.95
1
Mixed Strategies
State 0.5, Firm 2
Action 0.01
Action 0.02
Action 0.03
Action 0.04
1e+0
5e+0
1e+1
5e+1
1e+2
5e+2
1e+3
5e+3
1e+4
5e+4
1e+5
5e+5
1e+6
5e+6
1e+7
1e+8
1e+9
Full-Info
m
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
0.55
0.6
0.65
0.7
0.75
0.8
0.85
0.9
0.95
1
Mixed Strategies
State 0.75, Firm 2
Action 0.01
Action 0.02
Action 0.03
Action 0.04
1e+0
5e+0
1e+1
5e+1
1e+2
5e+2
1e+3
5e+3
1e+4
5e+4
1e+5
5e+5
1e+6
5e+6
1e+7
1e+8
1e+9
Full-Info
m
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
0.55
0.6
0.65
0.7
0.75
0.8
0.85
0.9
0.95
1
Mixed Strategies
State 0.85, Firm 2
Action 0.01
Action 0.02
Action 0.03
Action 0.04
Fig. 4. Estimated equilibrium strategies of Firm 2 for diﬀerent values of m at the states
0.25, 0.5, 0.75, and 0.85.

162
A. D. Robles-Aguilar et al.
1e+0
5e+0
1e+1
5e+1
1e+2
5e+2
1e+3
5e+3
1e+4
5e+4
1e+5
5e+5
1e+6
5e+6
1e+7
1e+8
1e+9
Full-Info
m
7.071
7.072
7.073
7.074
7.075
7.076
7.077
7.078
7.079
7.08
7.081
State 0.25, Firm 1
1e+0
5e+0
1e+1
5e+1
1e+2
5e+2
1e+3
5e+3
1e+4
5e+4
1e+5
5e+5
1e+6
5e+6
1e+7
1e+8
1e+9
Full-Info
m
8.24195
8.24196
8.24197
8.24198
8.24199
8.242
8.24201
8.24202
8.24203
8.24204
State 0.5, Firm 1
1e+0
5e+0
1e+1
5e+1
1e+2
5e+2
1e+3
5e+3
1e+4
5e+4
1e+5
5e+5
1e+6
5e+6
1e+7
1e+8
1e+9
Full-Info
m
9.422
9.4225
9.423
9.4235
9.424
9.4245
9.425
State 0.75, Firm 1
1e+0
5e+0
1e+1
5e+1
1e+2
5e+2
1e+3
5e+3
1e+4
5e+4
1e+5
5e+5
1e+6
5e+6
1e+7
1e+8
1e+9
Full-Info
m
9.628
9.6285
9.629
9.6295
9.63
9.6305
9.631
9.6315
State 0.85, Firm 1
Fig. 5. Estimated equilibrium payoﬀs of Firm 1 for diﬀerent values of m at the states
0.25, 0.5, 0.75, and 0.85.
1e+0
5e+0
1e+1
5e+1
1e+2
5e+2
1e+3
5e+3
1e+4
5e+4
1e+5
5e+5
1e+6
5e+6
1e+7
1e+8
1e+9 Full-Info
m
9.417
9.418
9.419
9.42
9.421
9.422
9.423
9.424
State 0.25, Firm 2
1e+0
5e+0
1e+1
5e+1
1e+2
5e+2
1e+3
5e+3
1e+4
5e+4
1e+5
5e+5
1e+6
5e+6
1e+7
1e+8
1e+9
Full-Info
m
8.24193
8.24194
8.24195
8.24196
8.24197
8.24198
8.24199
8.242
8.24201
8.24202
8.24203
State 0.5, Firm 2
1e+0
5e+0
1e+1
5e+1
1e+2
5e+2
1e+3
5e+3
1e+4
5e+4
1e+5
5e+5
1e+6
5e+6
1e+7
1e+8
1e+9
Full-Info
m
7.0714
7.07145
7.0715
7.07155
7.0716
7.07165
7.0717
7.07175
7.0718
State 0.75, Firm 2
1e+0
5e+0
1e+1
5e+1
1e+2
5e+2
1e+3
5e+3
1e+4
5e+4
1e+5
5e+5
1e+6
5e+6
1e+7
1e+8
1e+9
Full-Info
m
6.8749
6.87495
6.875
6.87505
6.8751
6.87515
6.8752
6.87525
State 0.85, Firm 2
Fig. 6. Estimated equilibrium payoﬀs of Firm 2 for diﬀerent values of m at the states
0.25, 0.5, 0.75, and 0.85.

Estimation of Equilibria Strategies
163
ofnI-llu
F
6
+
e
1
5
+
e
1
4
+
e
1
3
+
e
1
2
+
e
1
1
+
e
1
0
+
e
1
m
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
Mixed Strategies
State 0.3, Firm 1
T-1, A-0.01
T-1, A-0.02
T-1, A-0.03
T-1, A-0.04
T-2, A-0.01
T-2, A-0.02
T-2, A-0.03
T-2, A-0.04
T-3, A-0.01
T-3, A-0.02
T-3, A-0.03
T-3, A-0.04
T-4, A-0.01
T-4, A-0.02
T-4, A-0.03
T-4, A-0.04
T-5, A-0.01
T-5, A-0.02
T-5, A-0.03
T-5, A-0.04
T-6, A-0.01
T-6, A-0.02
T-6, A-0.03
T-6, A-0.04
ofnI-llu
F
6
+
e
1
5
+
e
1
4
+
e
1
3
+
e
1
2
+
e
1
1
+
e
1
0
+
e
1
m
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Mixed Strategies
State 0.4, Firm 1
T-1, A-0.01
T-1, A-0.02
T-1, A-0.03
T-1, A-0.04
T-2, A-0.01
T-2, A-0.02
T-2, A-0.03
T-2, A-0.04
T-3, A-0.01
T-3, A-0.02
T-3, A-0.03
T-3, A-0.04
T-4, A-0.01
T-4, A-0.02
T-4, A-0.03
T-4, A-0.04
T-5, A-0.01
T-5, A-0.02
T-5, A-0.03
T-5, A-0.04
T-6, A-0.01
T-6, A-0.02
T-6, A-0.03
T-6, A-0.04
ofnI-llu
F
6
+
e
1
5
+
e
1
4
+
e
1
3
+
e
1
2
+
e
1
1
+
e
1
0
+
e
1
m
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Mixed Strategies
State 0.5, Firm 1
T-1, A-0.01
T-1, A-0.02
T-1, A-0.03
T-1, A-0.04
T-2, A-0.01
T-2, A-0.02
T-2, A-0.03
T-2, A-0.04
T-3, A-0.01
T-3, A-0.02
T-3, A-0.03
T-3, A-0.04
T-4, A-0.01
T-4, A-0.02
T-4, A-0.03
T-4, A-0.04
T-5, A-0.01
T-5, A-0.02
T-5, A-0.03
T-5, A-0.04
T-6, A-0.01
T-6, A-0.02
T-6, A-0.03
T-6, A-0.04
ofnI-llu
F
6
+
e
1
5
+
e
1
4
+
e
1
3
+
e
1
2
+
e
1
1
+
e
1
0
+
e
1
m
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Mixed Strategies
State 0.6, Firm 1
T-1, A-0.01
T-1, A-0.02
T-1, A-0.03
T-1, A-0.04
T-2, A-0.01
T-2, A-0.02
T-2, A-0.03
T-2, A-0.04
T-3, A-0.01
T-3, A-0.02
T-3, A-0.03
T-3, A-0.04
T-4, A-0.01
T-4, A-0.02
T-4, A-0.03
T-4, A-0.04
T-5, A-0.01
T-5, A-0.02
T-5, A-0.03
T-5, A-0.04
T-6, A-0.01
T-6, A-0.02
T-6, A-0.03
T-6, A-0.04
Fig. 7. Estimated equilibrium strategies of Firm 1 for six realizations of ω and diﬀerent
values of m at states 0.3, 0.4, 0.5, and 0.6.
1.382
T-6
1.3825
T-5
Full-Info
Payoffs
1E+6
T-4
1.383
State 0.1, Firm 1
1E+5
T
1E+4
m
T-3
1.3835
1E+3
1E+2
T-2
1E+1
T-1
1E+0
T-1
T-2
T-3
T-4
T-5
T-6
1.522
T-6
1.5225
T-5
Full-Info
Payoffs
1E+6
T-4
1.523
State 0.2, Firm 1
1E+5
T
1E+4
m
T-3
1.5235
1E+3
1E+2
T-2
1E+1
T-1
1E+0
T-1
T-2
T-3
T-4
T-5
T-6
1.876
T-6
1.8765
T-5
1.877
Full-Info
Payoffs
1E+6
1.8775
T-4
State 0.4, Firm 1
1E+5
T
1.878
1E+4
m
T-3
1.8785
1E+3
1E+2
T-2
1E+1
T-1
1E+0
T-1
T-2
T-3
T-4
T-5
T-6
2.315
T-6
2.32
2.325
2.33
T-5
Full-Info
2.335
Payoffs
2.34
1E+6
T-4
2.345
State 0.5, Firm 1
1E+5
T
2.35
1E+4
2.355
m
T-3
2.36
1E+3
1E+2
T-2
1E+1
T-1
1E+0
T-1
T-2
T-3
T-4
T-5
T-6
Fig. 8. Estimated equilibrium payoﬀs of Firm 1 for six realizations of ω and diﬀerent
values of m at states 0.1, 0.2, 0.4, and 0.5.

164
A. D. Robles-Aguilar et al.
play the corresponding equilibrium strategies—of a single realization ω. If the
strategies converge, then Proposition 1 asserts that, with probability 1, the esti-
mated equilibrium strategies are close to an equilibrium of the full-information
game.
For illustrative purposes, in Fig. 7, we plot the equilibrium strategies cor-
responding to six diﬀerent realizations of ω. The game model components are
given in Table 1, except for β = 0.75 and X = {0.0, 0.1, 0.2, 0.3, . . . , 1.0}. The
associated payoﬀs are shown in Fig. 8. We plot data for some states of Firm 1
only. An interesting feature we can observe in this numerical experiment, pos-
sibly due to the uniqueness of equilibrium in the full-information game, is that
the limits of the estimated equilibrium strategies and the estimated payoﬀs are
independent of ω.
4
Conclusions
We have shown how to estimate equilibrium strategies in a stochastic advertis-
ing game with unknown distributions of the response to advertising eﬀorts. From
the numerical results, it is worth remarking some features of our model. First,
since we deal with a ﬁnite game, the equilibrium strategies are mixed instead
of pure strategies—obtained in most of the deterministic diﬀerential games of
advertising—because the corresponding action spaces in those models are con-
vex. Second, the qualitative behavior of the equilibrium strategies we found cor-
responds to that in the existing literature, namely, for higher market shares the
advertising eﬀorts are also higher. Third, we assume that at the m−th decision
epoch, ﬁrms have recorded m values of the advertising responses; hence ﬁrms
have good estimators (θn, ϑn) only when m is large enough. However, ﬁrms can
improve the estimators by using information of previous advertising campaigns
as well as information acquired between decision epochs. With such improved
estimators, the conclusion of Proposition 1 does not change. Finally, the problem
of multiple equilibria and/or the non convergence of the estimated equilibrium
strategies can be overcame by passing to a subsequence as is shown in [19].
Acknowledgement. This work was partially supported by Consejo Nacional de Cien-
cia y Tecnolog´ıa (CONACYT-M´exico) under grant Ciencia Frontera 2019–87787.
References
1. Breton, M., Jarrar, R., Zaccour, G.: A note on feedback sequential equilibria in a
Lanchester model with empirical application. Manage. Sci. 52(5), 804–811 (2006)
2. Dockner, E.J., Jørgensen, S., Van Long, N., Sorger, G.: Diﬀerential Games in Eco-
nomics and Management Science. Cambridge University Press, Cambridge (2000)
3. Dynkin, E.B., Yushkevich, A.A.: Controlled Markov Processes. Springer, New York
(1979)
4. Feichtinger, G., Hartl, R.F., Sethi, S.P.: Dynamic optimal control models in adver-
tising: recent developments. Manage. Sci. 40(2), 195–226 (1994)

Estimation of Equilibria Strategies
165
5. Ghosh, M., McDonald, D., Sinha, S.: Zero-sum stochastic games with partial infor-
mation. J. Optim. Theory Appl. 121(1), 99–118 (2004)
6. Huang, J., Leng, M., Liang, L.: Recent developments in dynamic advertising
research. European J. Oper. Res. 220, 591–609 (2012)
7. Jørgensen, S., Zaccour, G.: A survey of game-theoretic models of cooperative adver-
tising. European J. Oper. Res. 237, 1–14 (2014)
8. Jørgensen, S., Zaccour, G.: Diﬀerential Games in Marketing. Kluwer Academic
Publishers, Springer, Boston (2004)
9. Judd, K.L.: Numerical Methods in Economics. MIT Press, Cambridge (1998)
10. Krausz, A., Rieder, U.: Markov games with incomplete information. Math. Meth.
Oper. Res. 46(2), 263–279 (1997)
11. Lanchester, F.W.: Mathematics in warfare. In: J. Newman, J. (ed.) The World of
Mathematics, vol. 4, pp. 2138–2157. Simon and Schuster, New York (1956)
12. Luque-V´asquez, F., Minj´arez-Sosa, J.A.: Empirical approximation in Markov
games under unbounded payoﬀ: discounted and average criteria. Kybernetika
53(4), 694–716 (2017)
13. Minj´arez-Sosa, J.A.: Zero-Sum Discrete-Time Markov Games with Unknown Dis-
turbance Distribution. Springer, Cham (2020)
14. Minj´arez-Sosa, J.A., Luque-V´asquez, F.: Two person zero-sum semi-Markov games
with unknown holding times distribution on one side: a discounted payoﬀcriterion.
Appl. Math. Optim. 57(3), 289–305 (2008)
15. Minj´arez-Sosa, J.A., Vega-Amaya, ´O.: Asymptotically optimal strategies for adap-
tive zero-sum discounted Markov games. SIAM J. Control. Optim. 48(3), 1405–
1421 (2009)
16. Minj´arez-Sosa, J.A., Vega-Amaya, ´O.: Optimal strategies for adaptive zero-sum
average Markov games. J. Math. Anal. Appl. 402(1), 44–56 (2013)
17. Parthasarathy, T.: Discounted, positive, and noncooperative stochastic games. Int.
J. Game Theory 2(1), 25–37 (1973)
18. Prasad, A., Sethi, S.P.: Competitive advertising under uncertainty: a stochastic
diﬀerential game approach. J. Optim. Theory Appl. 123, 163–185 (2004)
19. Robles-Aguilar, A.D., Gonz´alez-S´anchez, D., Minj´arez-Sosa, J.A.: Empirical
approximation of Nash equilibria in ﬁnite Markov games with discounted payoﬀs.
Submitted for publication (2021)
20. Ross, S.M.: Simulation, 5th edn. Elsevier/Academic Press, Amsterdam (2013)
21. Sethi, S.P.: Dynamic optimal control models in advertising: a survey. SIAM Rev.
19, 685–725 (1977)
22. Shimkin, N., Shwartz, A.: Asymptotically eﬃcient adaptive strategies in repeated
games Part I: certainty equivalence strategies. Math. Oper. Res. 20, 743–767 (1995)
23. Shimkin, N., Shwartz, A.: Asymptotically eﬃcient adaptive strategies in repeated
games Part II: asymptotic optimality. Math. Oper. Res. 21(2), 487–512 (1996)

Robustness to Approximations and Model
Learning in MDPs and POMDPs
Ali Devran Kara(B) and Serdar Y¨uksel
Department of Mathematics and Statistics, Queen’s University,
Kingston, ON, Canada
{16adk,yuksel}@queensu.ca
Abstract. In stochastic control applications, typically only an ideal
model (controlled transition kernel) is assumed and the control design is
based on the given model, raising the problem of performance loss due to
the mismatch between the assumed model and the actual model. In some
further setups, an exact model may be known, but this model may entail
computationally challenging optimality analysis leading to the solution
of some approximate model being implemented. With such a motiva-
tion, we study continuity properties of discrete-time stochastic control
problems with respect to system models and robustness of optimal con-
trol policies designed for incorrect models applied to the true system. We
study both fully observed and partially observed setups under an inﬁnite
horizon discounted expected cost criterion. We show that continuity can
be established under total variation convergence of the transition kernels
under mild assumptions and with further restrictions on the dynamics
and observation model under weak and setwise convergence of the tran-
sition kernels. Using these, we establish convergence results and error
bounds due to mismatch that occurs by the application of a control policy
which is designed for an incorrectly estimated system model to the actual
system, thus establishing results on robustness. These entail implications
on empirical learning in (data-driven) stochastic control since often sys-
tem models are learned through empirical training data where typically
the weak convergence criterion applies but stronger convergence crite-
ria do not. We ﬁnally view and establish approximation as a particular
instance of robustness.
Keywords: Markov decision processes · Robust stochastic control ·
Approximate models · Empirical learning · POMDPs
AMS(2020)
subject
classiﬁcation: Primary 93E20 · Secondary
90C40 · 90C39
1
Introduction and Problem Deﬁnition
In this article, we study the robustness problem of Markov Decision Processes
(MDPs) and partially observed Markov decision processes (POMDPs) with
c
⃝The Author(s), under exclusive license to Springer Nature Switzerland AG 2021
A. Piunovskiy and Y. Zhang (Eds.): Modern Trends in Controlled Stochastic Processes,
ECC 41, pp. 166–191, 2021. https://doi.org/10.1007/978-3-030-76928-4_9

Robustness to Incorrect Models and Approximations
167
incomplete/incorrect characterization, and view learning and approximate mod-
eling as instances of the robustness problem. The article builds on some recent
work of the authors but the models considered here are more general (involving
changing cost functions also in the MDP models), and the unifying relationship
between robustness and ﬁnite model approximations involving standard Borel
models has not been studied elsewhere, to our knowledge.
Let X ⊂Rm denote a Borel set which is the state space of a partially observed
controlled Markov process. Here and throughout the paper Z+ denotes the set
of non-negative integers and N denotes the set of positive integers. Let Y ⊂Rn
be a Borel set denoting the observation space of the model, and let the state
be observed through an observation channel Q. The observation channel, Q,
is deﬁned as a stochastic kernel (regular conditional probability) from X to Y,
such that Q( · |x) is a probability measure on the (Borel) σ-algebra B(Y) of Y
for every x ∈X, and Q(A| · ) : X →[0, 1] is a Borel measurable function for
every A ∈B(Y). A decision maker (DM) is located at the output of the channel
Q, and hence it only sees the observations {Yt, t ∈Z+} and chooses its actions
from U, the action space which is a Borel subset of some Euclidean space. An
admissible policy γ is a sequence of control functions {γt, t ∈Z+} such that γt is
measurable with respect to the σ-algebra generated by the information variables
It = {Y[0,t], U[0,t−1]},
t ∈N,
I0 = {Y0},
where
Ut = γt(It),
t ∈Z+,
(1)
are the U-valued control actions and
Y[0,t] = {Ys, 0 ≤s ≤t},
U[0,t−1] = {Us, 0 ≤s ≤t −1}.
We deﬁne Γ to be the set of all such admissible policies. The update rules of the
system are determined by (1) and the following:
Pr

(X0, Y0) ∈B

=

B
P(dx0)Q(dy0|x0),
B ∈B(X × Y),
where P is the (prior) distribution of the initial state X0, and
Pr

(Xt, Yt) ∈B
 (X, Y, U)[0,t−1] = (x, y, u)[0,t−1]

=

B
T (dxt|xt−1, ut−1)Q(dyt|xt), B ∈B(X × Y), t ∈N,
where T is the transition kernel of the model. The objective of the agent (decision
maker) is the minimization of the inﬁnite horizon discounted cost,
Jβ(c, T , γ) = ET ,γ
P
 ∞
	
t=0
βtc(Xt, Ut)


168
A. D. Kara and S. Y¨uksel
for some discount factor β ∈(0, 1), over the set of admissible policies γ ∈Γ,
where c : X × U →R is a Borel-measurable stage-wise cost function and ET ,γ
P
denotes the expectation with initial state probability measure P and transition
kernel T under policy γ. Note that we write the inﬁnite horizon discounted cost
as a function of the transition kernels and the stage-wise cost function since we
will analyze the cost under the changes on those variables.
We deﬁne the optimal cost for the discounted inﬁnite horizon setup as a
function of the stage-wise cost function and the transition kernels as
J∗
β(c, T ) = inf
γ∈Γ Jβ(c, T , γ).
Problem P1: Continuity of J∗
β(c, T ) under the Convergence of the
Models. Let {Tn, n ∈N} be a sequence of transition kernels which converges
in some sense to another transition kernel T and {cn, n ∈N} be a sequence of
stage-wise cost functions corresponding to Tn which converge in some sense to
another cost function c. Does that imply that
J∗
β(cn, Tn) →J∗
β(c, T )?
Problem P2: Robustness to Incorrect Models. A problem of major practi-
cal importance is robustness of an optimal controller to modeling errors. Suppose
that an optimal policy is constructed according to a model which is incorrect:
how does the application of the control to the true model aﬀect the system per-
formance and does the error decrease to zero as the models become closer to each
other? In particular, suppose that γ∗
n is an optimal policy designed for Tn and
cn, an incorrect model for a true model T and c. Is it the case that if Tn →T
and cn →c, then Jβ(c, T , γ∗
n) →J∗
β(c, T )?
Problem P3: Empirical Consistency of Learned Probabilistic Models
and Data-Driven Stochastic Control. Let T (·|x, u) be a transition kernel
given previous state and action variables x ∈X, u ∈U, which is unknown to the
decision maker (DM). Suppose the DM builds a model for the transition kernels,
Tn(·|x, u), for all possible x ∈X, u ∈U by collecting training data (e.g. from
the evolving system). Do we have that the cost calculated under Tn converges to
the true cost (i.e., do we have that the cost obtained from applying the optimal
policy for the empirical model converges to the true cost as the training length
increases)?
Problem P4: Approximation by Finite MDPs as an Instance of
Robustness to Incorrect Models. Can we view the approximation problem
of a continuous space MDP model with a ﬁnite model (in particular [22, Theo-
rem 2.2], [22, Theorem 4.1] or [23, Theorem 3.2]) as an instance of the robustness
problem?

Robustness to Incorrect Models and Approximations
169
Brief Literature Review. Robustness is a desired property for the opti-
mal control of stochastic or deterministic systems when a given model does
not reﬂect the actual system perfectly, as is usually the case in practice.
This is a classical problem, and there is a very large literature on robust
stochastic control and its application to learning-theoretic methods; see e.g.
[1,2,7,8,14,16,18,20,21,25,26]. A rather comprehensive literature review is pre-
sented in [18]. The article builds on [16,18], but the models considered considered
here are more general (involving changing cost functions also in the MDP mod-
els), and the unifying relationship between robustness and ﬁnite model approxi-
mations involving standard Borel models has not been studied elsewhere, to our
knowledge.
1.1
Some Examples and Convergence Criteria for Transition
Kernels
Convergence Criteria for Transition Kernels. Before presenting conver-
gence criteria for controlled transition kernels, we ﬁrst review the convergence
of probability measures. Three important notions of convergences for sets of
probability measures to be studied in the paper are weak convergence, setwise
convergence, and convergence under total variation. For N ∈N, a sequence
{μn, n ∈N} in P(RN) is said to converge to μ ∈P(RN) weakly if

RN c(x)μn(dx) →

RN c(x)μ(dx)
(∗)
for every continuous and bounded c : RN →R. {μn} is said to converge setwise
to μ ∈P(RN) if (*) holds for all measurable and bounded c : RN →R. For
probability measures μ, ν ∈P(RN), the total variation metric is given by
∥μ −ν∥T V = 2
sup
B∈B(RN )
|μ(B) −ν(B)| =
sup
f:∥f∥∞≤1
|

f(x)μ(dx) −

f(x)ν(dx)|,
where the supremum is taken over all measurable real f such that ∥f∥∞=
supx∈RN |f(x)| ≤1. A sequence {μn} is said to converge in total variation to
μ ∈P(RN) if ∥μn −μ∥T V →0. Total variation deﬁnes a stringent metric for
convergence; for example, a sequence of discrete probability measures does not
converge in total variation to a probability measure which admits a density func-
tion. Setwise convergence, though, induces a topology on the space of probability
measures which is not metrizable [10, p. 59]. However, the space of probability
measures on a complete, separable, metric (Polish) space endowed with the topol-
ogy of weak convergence is itself complete, separable, and metric [19]. We also
note here that relative entropy convergence, through Pinsker’s inequality [11,
Lemma 5.2.8], is stronger than even total variation convergence, which has also
been studied in robust stochastic control. Another metric for probability mea-
sures is the Wasserstein distance: For compact spaces, the Wasserstein distance
of order 1 metrizes the weak topology and for non-compact spaces convergence

170
A. D. Kara and S. Y¨uksel
in the W1 metric implies weak convergence. Considering these relations, our
results in this paper can be directly generalized to the relative entropy distance
or the Wasserstein distance. Building on the above, we introduce the following
convergence notions for (controlled) transition kernels.
Deﬁnition 1. For a sequence of transition kernels {Tn, n ∈N}, we say that
– Tn →T weakly if Tn(·|x, u) →T (·|x, u) weakly, for all x ∈X and u ∈U,
– Tn →T setwise if Tn(·|x, u) →T (·|x, u) setwise, for all x ∈X and u ∈U,
– Tn →T under the total variation distance if Tn(·|x, u) →T (·|x, u) under
total variation for all x ∈X and u ∈U.
Examples [18]. Let a controlled model be given as xt+1 = F(xt, ut, wt), where
{wt} is an i.i.d. noise process. The uncertainty on the transition kernel for such
a system may arise from lack of information on F or the i.i.d. noise process wt
or both:
(i) Let {Fn} denote an approximating sequence for F, so that Fn(x, u, w) →
F(x, u, w) pointwise. Assume that the probability measure of the noise
is known. Then, corresponding kernels Tn converge weakly to T : If we
denote the probability measure of w with μ, for any g ∈Cb(X) and for
any (x0, u0) ∈X × U using the dominated convergence theorem we have
lim
n→∞

g(x1)Tn(dx1|x0, u0) = lim
n→∞

g(Fn(x0, u0, w))μ(dw)
=

g(F(x0, u0, w))μ(dw) =

g(x1)T (dx1|x0, u0).
(ii) Much of the robust control literature deals with deterministic systems where
the nominal model is a deterministic perturbation of the actual model (see e.g.
[24]). The considered model is in the following form: ˜F(xt, ut) = F(xt, ut)
+ ΔF(xt, ut), where F represents the nominal model and ΔF is the model
uncertainty satisfying some norm bounds. For such deterministic systems,
pointwise convergence of ˜F to the nominal model F, i.e. ΔF(xt, ut) →0,
can be viewed as weak convergence for deterministic systems by the discus-
sion in (i). It is evident, however, that total variation convergence would be
too strong for such a convergence criterion, since δ ˜
F (xt,ut) →δF (xt,ut) weakly
but ∥δ ˜
F (xt,ut) −δF (xt,ut)∥T V = 2 for all ΔF(xt, ut) ̸= 0 where δ denotes the
Dirac measure.
(iii) Let F(xt, ut, wt) = f(xt, ut) + wt be such that the function f is known
and wt ∼μ is not known correctly and an incorrect model μn is assumed.
If μn →μ weakly, setwise, or in total variation, then the corresponding
transition kernels Tn converge in the same sense to T . Observe the following:

g(x1)Tn(dx1|x0, u0) −

g(x1)T (dx1|x0, u0)
=

g(w0 + f(x0, u0))μn(dw0) −

g(w0 + f(x0, u0))μ(dw0).
(2)

Robustness to Incorrect Models and Approximations
171
(a) Suppose μn →μ weakly. If g is a continuous and bounded function,
then g(·+f(x0, u0)) is a continuous and bounded function for all (x0, u0) ∈
X×U. Thus, (2) goes to 0. Note that f does not need to be continuous. (b)
Suppose μn →μ setwise. If g is a measurable and bounded function, then
g(· + f(x0, u0)) measurable and bounded for all (x0, u0) ∈X × U. Thus, (2)
goes to 0. (c) Finally, assume μn →μ in total variation. If g is bounded,
(2) converges to 0, as in item (b). As a special case, assume that μn and μ
admit densities hn and h, respectively; then the pointwise convergence of
hn to h implies the convergence of μn to μ in total variation by Scheﬀ´e’s
theorem.
(iv) Suppose now neither F nor the probability model of wt is known per-
fectly. It is assumed that wt admits a measure μn and μn →μ weakly.
For the function F we again have an approximating sequence {Fn}. If
Fn(x, u, wn) →F(x, u, w) for all (x, u) ∈X × U and for any wn →w, then
the transition kernel Tn corresponding to the model Fn converges weakly
to the one of F, T : For any g ∈Cb(X),
lim
n→∞

g(x1)Tn(dx1|x0, u0) = lim
n→∞

g(Fn(x0, u0, w))μn(dw)
=

g(F(x0, u0, w))μ(dw) =

g(x1)T (dx1|x0, u0).
(v) Let again {Fn} denote an approximating sequence for F and suppose now
Fx0,u0,n(·) := Fn(x0, u0, ·) : W →X is invertible for all x0, u0 ∈X × U
and F −1
(x0,u0),n(·) denotes the inverse for ﬁxed (x0, u0). It is assumed that
F −1
(x0,u0),n(x1) →F −1
x0,u0(x1) pointwise for all (x0, u0). Suppose further that
the noise process wt admits a continuous density fW (w). The Jacobian
matrix, ∂x1
∂w , is the matrix whose components are the partial derivatives of
x1, i.e. with x1 ∈X ⊂Rm and w ∈W ⊂Rm, it is an m × m matrix with
components
∂(x1)i
∂wj , 1 ≤i, j ≤m . If the Jacobian matrix of derivatives
∂x1
∂w (w) is continuous in w and nonsingular for all w, then we have that the
density of the state variables can be written as
fX1,n,(x0,u0)(x1) = fW (F −1
x0,u0,n(x1))
∂x1
∂w (F −1
x0,u0,n(x1))
−1,
fX1,(x0,u0)(x1) = fW (F −1
x0,u0(x1))
∂x1
∂w (F −1
x0,u0(x1))
−1.
With the above, fX1,n,(x0,u0)(x1)
→
fX1,(x0,u0)(x1) pointwise for all
ﬁxed (x0, u0). Therefore, by Scheﬀ´e’s theorem, the corresponding kernels
Tn(·|x0, u0) →T (·|x0, u0) in total variation for all (x0, u0).
(vi) These examples will be utilized in Sect. 5.1, where data-driven stochastic
control problems will be considered where estimated models are obtained
through empirical measurements of the state action variables.

172
A. D. Kara and S. Y¨uksel
1.2
Summary
We now introduce the main assumptions that will be occasionally used for our
technical results in the article.
Assumption 1.(a) The sequence of transition kernels Tn satisﬁes the follow-
ing: {Tn(·|xn, un), n ∈N} converges weakly to T (·|x, u) for any sequence
{xn, un} ⊂X × U and x, u ∈X × U such that (xn, un) →(x, u).
(b) The stochastic kernel T (·|x, u) is weakly continuous in (x, u).
(c) The sequence of stage-wise cost functions cn satisﬁes the following: cn(xn,
un) →c(x, u) for any sequence {xn, un} ⊂X×U and x, u ∈X×U such that
(xn, un) →(x, u).
(d) The stage-wise cost function c(x, u) is non-negative, bounded, and continu-
ous on X × U.
(e) U is compact.
Assumption 2. The observation channel Q(·|x) is continuous in total variation
i.e., if xn →x, then Q( · |xn) →Q( · |x) in total variation (only for partially
observed models).
Assumption 3.(a) The sequence of transition kernels Tn satisﬁes the follow-
ing: {Tn(·|x, un), n ∈N} converges setwise to T (·|x, u) for any sequence
{un} ⊂U and x, u ∈X × U such that un →u.
(b) The stochastic kernel T (·|x, u) is setwise continuous in u.
(c) The sequence of stage-wise cost functions cn satisﬁes the following: cn(x, un)
→c(x, u) for any sequence {un} ⊂U and x, u ∈X × U such that un →u.
(d) The stage-wise cost function c(x, u) is non-negative, bounded, and continu-
ous on U.
(e) U is compact.
Assumption 4.(a) The sequence of transition kernels Tn satisﬁes the following:
∥Tn(·|x, un)−T (·|x, u)∥T V →0 for any sequence {un} ⊂U and x, u ∈X×U
such that un →u.
(b) The stochastic kernel T (·|x, u) is continuous in total variation in u.
(c) The sequence of stage-wise cost functions cn satisﬁes the following: cn(x, un)
→c(x, u) for any sequence {un} ⊂U and x, u ∈X × U such that un →u.
(d) The stage-wise cost function c(x, u) is non-negative, bounded, and continu-
ous on U.
(e) U is compact.
In Sects. 2 and 3 we study continuity (Problem P1) and robustness (Problem
P2) for partially observed models. In particular we show the following:
(a) Continuity and robustness do not hold in general under weak convergence
of kernels (Theorem 1).
(b) Under Assumptions 1 and 2, continuity and robustness hold (Theorem 4,
Theorem 8).
(c) Continuity and robustness do not hold in general under setwise convergence
of the kernels (Theorem 5).

Robustness to Incorrect Models and Approximations
173
(d) Continuity and robustness do not hold in general under total variation con-
vergence of the kernels (Example 1).
(f) Under Assumption 4, continuity and robustness hold (Theorem 6, Theo-
rem 7).
In Sect. 4, we study continuity (Problem P1) and robustness (Problem P2)
for fully observed models. In particular we show the following
(a) Continuity and robustness do not hold in general under weak convergence
of kernels (Theorem 9, Example 1).
(b) Under Assumption 1, continuity holds (Theorem 10), under Assumption 1,
robustness holds if the optimal policies for every initial point are identical
(Theorem 11).
(c) Continuity and robustness do not hold in general under setwise convergence
of the kernels (Theorem 12, Theorem 14).
(d) Under Assumption 3, continuity holds (Theorem 13), and under Assumption
3, robustness holds if the optimal policies for every initial point are identical
(Theorem 15).
(e) Continuity and robustness do not hold in general under total variation con-
vergence of the kernels (Example 1).
(f) Under Assumption 4, continuity and robustness hold (Subsect. 4.3).
In Sect. 5, we study applications to empirical learning (in Sect. 5.1) where we
establish the positive relevance of Theorem 10, and then applications to ﬁnite
model approximations under the perspective of robustness in Sect. 5.2. Here, we
restrict the analysis to the case with weakly continuous kernels.
2
Continuity of Optimal Cost in Convergence of Models
(POMDP Case)
In this section, we will study continuity of the optimal discounted cost under
the convergence of transition kernels and cost functions.
2.1
Weak Convergence
Absence of Continuity Under Weak Convergence. The following shows
that the optimal cost may not be continuous under weak convergence of transi-
tion kernels.
Theorem 1 [18]. Let Tn →T
weakly, then it is not necessarily true that
J∗
β(c, Tn) →J∗
β(c, T ) even when the prior distributions are the same, the mea-
surement channel Q is continuous in total variation, and c(x, u) is continuous
and bounded on X × U.
We prove the result with a counterexample [18]. Letting X = U = Y =
[−1, 1] and c(x, u) = (x −u)2, the observation channel is chosen to be uniformly
distributed over [−1, 1], Q ∼U([−1, 1]), the initial distributions of the state

174
A. D. Kara and S. Y¨uksel
variable are chosen to be same as P ∼δ1, where δx(A) := 1{x∈A} for Borel A,
and the transition kernels are:
T (·|x, u) = δ−1(x)[1
2δ1(·) + 1
2δ−1(·)] + δ1(x)[1
2δ1(·) + 1
2δ−1(·)]
+ (1 −δ−1(x))(1 −δ1(x))δ0(·)
Tn(·|x, u) = δ−1(x)[1
2δ(1−1/n)(·) + 1
2δ(−1+1/n)(·)] + δ1(x)[1
2δ(1−1/n)(·)
+ 1
2δ(−1+1/n)(·)] + (1 −δ−1(x))(1 −δ1(x))δ0(·).
It can be seen that Tn →T weakly according to Deﬁnition 1(i). Note that the
cost function is continuous, and the measurement channel is continuous in total
variation. The optimal discounted costs can be found as
J∗
β(c, T ) =
∞
	
k=1
ET
P [βkX2
k] =
∞
	
k=1
βk =
β
1 −β
J∗
β(c, Tn) =
∞
	
k=1
ETn
P [βkX2
k] = β[1
2(1 −1
n)2 + 1
2(−1 + 1
n)2].
Then we have J∗
β(c, Tn) →β ̸=
β
1−β .
2.2
A Suﬃcient Condition for Continuity Under Weak Convergence
In the following, we will establish and utilize some regularity properties for the
optimal cost with respect to the convergence of transition kernels.
Assumption 5.(a) The stochastic kernel T (·|x, u) is weakly continuous in
(x, u), i.e. if (xn, un) →(x, u), then T (·|xn, un) →T (·|x, u) weakly.
(b) The observation channel Q(·|x) is continuous in total variation, i.e., if xn →
x, then Q( · |xn) →Q( · |x) in total variation.
(c) The stage-wise cost function c(x, u) is non-negative, bounded and continuous
on X × U
(d) U is compact.
It is a well known result that, any POMDP can be reduced to a (completely
observable) MDP, whose states are the posterior state distributions or beliefs of
the observer; that is, the state at time t is Zt( · ) := Pr{Xt ∈· |Y0, . . . , Yt, U0, . . . ,
Ut−1} ∈P(X). We call this equivalent MDP the belief-MDP . The belief-MDP
has state space Z = P(X) and action space U. Under the topology of weak
convergence, since X is a Borel space, Z is metrizable with the Prokhorov metric
which makes Z into a Borel space [19]. The transition probability η of the belief-
MDP can be constructed through non-linear ﬁltering equations.
The one-stage cost function c of the belief-MDP is given by ˜c(z, u)
:=

X c(x, u)z(dx). Under the regularity of the belief-MDP, we have that the
discounted cost optimality operator T : Cb(Z) →Cb(Z)
(T(f))(z) = min
u (˜c(z, u) + βE[f(z1)|z0 = z, u0 = u])
(3)

Robustness to Incorrect Models and Approximations
175
is a contraction from Cb(Z) to itself under the supremum norm. As a result, there
exists a ﬁxed point, the value function, and an optimal control policy exists. In
view of this existence result, in the following we will consider optimal policies.
The following result is key to proving the main result of this section whose
detailed analysis can be found in [18].
Theorem 2. Suppose we have a uniformly bounded family of functions {f γ
n :
X →R, γ ∈Γ, n > 0} such that ∥f γ
n∥∞< C for all γ ∈Γ and for all n > 0 for
some C < ∞.
Further suppose we have another uniformly bounded family of functions {f γ :
X →R, γ ∈Γ} such that ∥f γ∥∞< C for all γ ∈Γ for some C < ∞. Under the
following assumptions,
(i) For any xn →x
sup
γ∈Γ
f γ
n(xn) −f γ(x)
 →0,
sup
γ∈Γ
f γ(xn) −f γ(x)
 →0,
(ii) supγ ρ(μγ
n, μγ) →0 where ρ is some metric for the weak convergence topol-
ogy,
we have
sup
γ∈Γ


f γ
n(x)μγ
n(dx) −

f γ(x)μγ(dx)
 →0.
Theorem 3. Under Assumptions 1 and 2,
sup
γ∈Γ
|Jβ(cn, Tn, γ) −Jβ(c, T , γ)| →0.
Proof Sketch.
sup
γ∈Γ
|Jβ(cn, Tn, γ) −Jβ(c, T , γ)|
= sup
γ∈Γ

∞
	
t=0
βt

ETn
P

cn

Xt, γ(Y[0,t])

−ET
P

c

Xt, γ(Y[0,t])

≤
∞
	
t=0
βt sup
γ∈Γ
ETn
P

cn

Xt, γ(Y[0,t])

−ET
P

c

Xt, γ(Y[0,t])
.
Recall that an admissible policy γ is a sequence of control functions {γt, t ∈Z+}.
In the last step above, we make a slight abuse of notation; the sup at the ﬁrst
step is over all sequence of control functions {γt, t ∈Z+} whereas the sup at the
last step is over all sequence of control functions {γt′, t′ ≤t}, but we will use
the same notation, γ, in the rest of the proof.
For any ϵ > 0, we choose a K < ∞such that ∞
t=K+1 βk2∥c∥∞≤ϵ/2. For
the chosen K, we choose an N < ∞such that
sup
γ∈Γ
ETn
P

cn

Xt, γ(Y[0,t])

−ET
P

c

Xt, γ(Y[0,t])
 ≤ϵ/2K

176
A. D. Kara and S. Y¨uksel
for all t ≤K and for all n > N. We note that in [18] a ﬁxed c function was
considered, but by considering the additional term
sup
γ∈Γ
ETn
P

cn

Xt, γ(Y[0,t])

−ET
P

cn

Xt, γ(Y[0,t])

and noting that supγ |

Q(dy|xn)cn(xn, γ(y)) −

Q(dy|x)c(x, γ(y))| →0, for
every xn →x, by a generalized dominated convergence theorem as Q is con-
tinuous in total variation, a triangle inequality argument shows that the same
result applies. This follows from a generalized dominated convergence theorem
as stated in Theorem 2 whose detailed analysis can be found in [18]. Thus,
supγ∈Γ
Jβ(cn, Tn, γ) −Jβ(c, T , γ) →0 as n →∞.
⊓⊔
Theorem 4. Suppose the conditions of Theorem 3 hold. Then limn→∞|J∗
β
(cn, Tn) −J∗
β(c, T )| = 0.
Proof Sketch. We start with the following bound:
|J∗
β(cn, Tn) −J∗
β(c, T )|
(4)
≤max

Jβ(cn, Tn, γ∗) −Jβ(c, T , γ∗), Jβ(c, T , γ∗
n) −Jβ(cn, Tn, γ∗
n)

,
where γ∗and γ∗
n are the optimal policies, respectively, for T and Tn. Both terms
go to 0 by Theorem 3.
⊓⊔
2.3
Absence of Continuity Under Setwise Convergence
We now show that continuity of optimal costs may fail under the setwise con-
vergence of transition kernels. Theorem 12 in the next section establishes this
result for fully observed models, which serves as a proof for this setup also.
Theorem 5. Let Tn →T setwise. Then, it is not true in general that J∗
β(c, Tn)
→J∗
β(c, T ), even when X, Y, and U are compact and c(x, u) is continuous and
bounded in X × U.
2.4
Continuity Under Total Variation
Theorem 6. Under Assumption 4, J∗
β(cn, Tn) →J∗
β(c, T ).
Proof Sketch. We start with the following bound:
|J∗
β(cn, Tn) −J∗
β(c, T )| ≤max

Jβ(cn, Tn, γ∗) −Jβ(c, T , γ∗), Jβ(cn, Tn, γ∗
n)
−Jβ(c, T , γ∗
n)

,
where γ∗and γ∗
n are the optimal policies, respectively, for T and Tn.

Robustness to Incorrect Models and Approximations
177
We now study the following:
sup
γ∈Γ
|Jβ(cn, Tn, γ) −Jβ(c, T , γ)|
= sup
γ∈Γ

∞
	
t=0
βt

ETn
P

cn

Xt, γ(Y[0,t])

−ET
P

c

Xt, γ(Y[0,t])

≤
∞
	
t=0
βt sup
γ∈Γ
ETn
P

cn

Xt, γ(Y[0,t])

−ET
P

c

Xt, γ(Y[0,t])
.
It can be shown that [18]
sup
γ∈Γ
ETn
P

cn

Xt, γ(Y[0,t])

−ET
P

c

Xt, γ(Y[0,t])
 →0.
(5)
This was shown in [18] for ﬁxed c. The extension to varying cn follows from a
triangle inequality step with the assumption that Tn(·|x, un) →T (·|x, u) setwise,
and cn(x, un) →c(x, u) for any un →u. Therefore, using identical steps as in
the proof of Theorem 3 we have supγ∈Γ
Jβ(cn, Tn, γ) −Jβ(c, T , γ)
 →0.
⊓⊔
3
Robustness to Incorrect Models (POMDP Case)
Here, we consider the robustness problem P2: Suppose we design an optimal
policy, γ∗
n, for a transition kernel, Tn and a cost function cn, assuming they are
the correct model and apply the policy to the true model whose transition kernel
is T and whose cost function is c. We study the robustness of the sub-optimal
policy γ∗
n.
3.1
Total Variation
The next theorem gives an asymptotic robustness result.
Theorem 7. Under Assumption 4
|Jβ(cn, T , γ∗
n) −J∗
β(c, T )| →0,
where γ∗
n is the optimal policy designed for the kernel Tn.
Proof Sketch. We write the following:
|Jβ(c, T , γ∗
n) −J∗
β(c, T )| ≤|Jβ(c, T , γ∗
n) −J∗
β(cn, Tn)| + |J∗
β(cn, Tn) −J∗
β(c, T )|.
Both terms can be shown to go to 0 using (5).
⊓⊔
3.2
Setwise Convergence
Theorem 14 in the next section establishes the lack of robustness under the
setwise convergence of kernels. As we note later, a fully observed system can
be viewed as a partially observed system with the measurement being the state
itself, (see (6)).

178
A. D. Kara and S. Y¨uksel
3.3
Weak Convergence
Theorem 8. Under Assumptions 1 and 2, |Jβ(c, T , γ∗
n) −J∗
β(c, T )| →0, where
γ∗
n is the optimal policy designed for the transition kernel Tn.
Proof Sketch. We write
|Jβ(c, T , γ∗
n) −J∗
β(c, T )| ≤|Jβ(c, T , γ∗
n) −Jβ(cn, Tn, γ∗
n)| + |Jβ(cn, Tn, γ∗
n)
−Jβ(T , γ∗)|.
The ﬁrst term goes to 0 by Theorem 3. For the second term we use Theorem 4.
⊓⊔
4
Continuity and Robustness in the Fully Observed Case
In this section, we consider the fully observed case where the controller has direct
access to the state variables. We present the results for this case separately, since
here we cannot utilize the regularity properties of measurement channels which
allows for stronger continuity and robustness results. Under measurable selection
conditions due to weak or strong (setwise) continuity of transition kernels [13,
Section 3.3], for inﬁnite horizon discounted cost problems optimal policies can
be selected from those which are stationary and deterministic. Therefore we
will restrict the policies to be stationary and deterministic so that Ut = γ(Xt)
for some measurable function γ. Notice also that fully observed models can be
viewed as partially observed with the measurement channel thought to be
Q(·|x) = δx(·),
(6)
which is only weakly continuous, thus it does not satisfy Assumption 2.
4.1
Weak Convergence
Absence of Continuity Under Weak Convergence. We start with a neg-
ative result.
Theorem 9. For Tn →T weakly, it is not necessarily true that J∗
β(c, Tn) →
J∗
β(c, T ) even when the prior distributions are the same and c(x, u) is continuous
and bounded in X × U.
Proof. We prove the result with a counterexample, similar to the model used in
the proof of Theorem 1 Letting X = [−1, 1], U = {−1, 1} and c(x, u) = (x −u)2,
the initial distributions are given by P ∼δ1, that is, X0 = 1, and the transition
kernels are
T (·|x, u) = δ−1(x)[1
2δ1(·) + 1
2δ−1(·)] + δ1(x)[1
2δ1(·) + 1
2δ−1(·)]
+ (1 −δ−1(x))(1 −δ1(x))δ0(·),
Tn(·|x, u) = δ−1(x)[1
2δ(1−1/n)(·) + 1
2δ(−1+1/n)(·)] + δ1(x)[1
2δ(1−1/n)(·)
+ 1
2δ(−1+1/n)(·)] + (1 −δ−1(x))(1 −δ1(x))δ0(·).

Robustness to Incorrect Models and Approximations
179
It can be seen that Tn →T weakly according to Deﬁnition 1(i). Under this setup
we can calculate the optimal costs as follows:
J∗
β(c, Tn) = 1
n2 +
∞
	
k=2
βk = 1
n2 +
β2
1 −β ,
and J∗
β(c, T ) = 0. Thus, continuity does not hold.
⊓⊔
We now present another counter example emphasizing the importance of
continuous convergence in the actions. The following counter example shows that
without the continuous convergence and regularity assumptions on the kernel
T , continuity fails even when Tn(·|x, u) →T (·|x, u) pointwise (for x, u) in total
variation (also setwise and weakly) and even when the cost function c(x, u) is
continuous and bounded. Notice that this example also holds for both setwise
and weak convergence.
Example 1. Assume that the kernels are given by
Tn(·|x, u) ∼U([un, 1 + un]),
T (·|x, u) ∼

U([0, 1])
if u ̸= 1,
U([1, 2])
if u = 1,
where U = [0, 1] and X = R. We note ﬁrst that Tn(·|x, u) →T (·|x, u) in total
variation for every ﬁxed x and u.
The cost function is in the following form:
c(x, u) =
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎩
2
if x ≤1
e,
2 −x−1
e
0.1
if 1
e < x ≤0.1 + 1
e,
1
if 0.1 + 1
e < x ≤1 + 1
e −0.1,
2 −1+ 1
e −x
0.1
if 1 + 1
e −0.1 < x ≤1 + 1
e,
2
if 1 + 1
e < x.
Notice that c(x, u) is a continuous function.
With this setup, γ∗(x) = 0 is an optimal policy for T since on the [0, 1]
interval the induced cost is less than the cost induced on the [1, 2] interval. The
cost under this policy is
J∗
β(c, T ) =
∞
	
t=0
βt

2 × 1
e + 0.3
2 + 0.9 −1
e

=
1
1 −β

1.05 + 1
e

.
For Tn, γ∗
n(x) = e−1
n is an optimal policy for every n as e−1
n ×n = 1
e and thus
the state is distributed between 1
e < x ≤1 + 1
e in which interval the cost is the
least. Hence, we can write
lim
n→∞Jβ(c, Tn, γ∗
n) =
∞
	
t=0
βt

0.3 + 1 −0.2

=
1.1
1 −β ̸=
1
1 −β

1.05 + 1
e

= J∗
β(c, T ).

180
A. D. Kara and S. Y¨uksel
A Suﬃcient Condition for Continuity Under Weak Convergence. We
will now establish that if the kernels and the model components have some
further regularity, continuity does hold. The assumptions of the following result
are the same as the assumptions for the partially observed case (Theorem 4)
except for the assumption on the measurement channel Q.
Theorem 10. Under Assumption 1, Jβ(cn, Tn, γ∗
n) →Jβ(c, T , γ∗) for any ini-
tial state x0, as n →∞.
Proof. We will use the successive approximations for an inductive argument.
Recall discounted cost optimality operator T : Cb(Z) →Cb(Z) from (3)
(T(v))(x) = inf
u∈U

c(x, u) + βE[v(x1)|x0 = x, u0 = u]

,
which is a contraction from Cb(X) to itself under the supremum norm and has
a ﬁxed point, the value function. For the kernel T , we will denote the approxi-
mation functions by
vk(x) = T(vk−1)(x),
and for the kernel Tn we will use vk
n(x) to denote the approximation functions,
notice that the operator T also depends on n for the model Tn, but we will
continue using it as T in what follows.
We wish to show that the approximation functions for Tn continuously con-
verge to the ones for T . Then, for the ﬁrst step of the induction we have
v1(x) = c(x, u∗),
v1
n(xn) = cn(xn, u∗
n),
and thus we can write,
|v1(x) −v1
n(xn)| ≤sup
u∈U
c(x, u) −cn(xn, u)

since cn(xn, un) →c(x, u) for all (xn, un) →(x, u) and the action space, U, is
compact, the ﬁrst step of the induction holds, i.e. limn→∞|v1(x) −v1
n(xn)| = 0.
For the kth step we have
vk(x) = T(vk−1)(x) = inf
u

c(x, u) + β

X
vk−1(x1)T (dx1|x, u)

,
vk
n(xn) = T(vk−1
n
)(xn) = inf
u

cn(xn, u) + β

X
vk−1
n
(x1)Tn(dx1|xn, u)

.
Note that the assumptions of the theorem satisfy the measurable selection crite-
ria and hence we can choose minimizing selectors [13, Section 3.3]. If we denote

Robustness to Incorrect Models and Approximations
181
the selectors by u∗and u∗
n, we can write
|vk(x) −vk
n(xn)|
≤max

|c(x, u∗) −cn(xn, u∗)|
+ β|

X
vk−1(x1)T (dx1|x, u∗) −

X
vk−1
n
(x1)Tn(dx1|xn, u∗)|

,

|c(x, u∗
n) −cn(xn, u∗
n)|
+ β|

X
vk−1(x1)T (dx1|x, u∗
n) −

X
vk−1
n
(x1)Tn(dx1|xn, u∗
n)|

.
Hence, we can write
|vk(x) −vk
n(xn)|
(7)
≤sup
u∈U

|c(x, u) −cn(xn, u)|
+ β|

X
vk−1(x1)T (dx1|x, u) −

X
vk−1
n
(x1)Tn(dx1|xn, u)|

,
above, the ﬁrst term goes to 0 as cn(xn, un) →c(x, u) for all (xn, un) →(x, u)
and the action space, U, is compact. For the second term we write,
sup
u∈U
|

X
vk−1(x1)T (dx1|x, u) −

X
vk−1
n
(x1)Tn(dx1|xn, u)|
≤sup
u∈U
|

X

vk−1(x1) −vk−1
n
(x1)

Tn(dx1|xn, u)|
+ sup
u∈U
|

X
vk−1(x1)T (dx1|x, u) −

X
vk−1(x1)Tn(dx1|xn, u)|
above, for the ﬁrst term, by the induction argument for any x1
n →x1,
vk−1(x1)−
vk−1
n
(x1
n)
 →0 (i.e., we have continuous convergence). We also have that
Tn(·|xn, u) →T (·|x, u) weakly uniformly over u ∈U as U is compact. Therefore,
using Theorem 2 the ﬁrst term goes to 0. For the second term we again use that
Tn(·|xn, u) converges weakly to T (·|x, u) uniformly over u ∈U. With an almost
identical induction argument it can also be shown that vk−1(x1) is continuous
in x1, thus the second term also goes to 0.
So far, we have showed that for any k ∈N, limn→∞
vk
n(xn) −vk(x)
 = 0 for
any xn →x, in particular it is also true that limn→∞
vk
n(x) −vk(x)
 = 0 for
any x.
As we have stated earlier, it can be shown that the approximation operator, T
is a contractive operator under supremum norm with modulus β and it converges

182
A. D. Kara and S. Y¨uksel
to a ﬁxed point which is the value function. Thus, we have
Jβ(c, T , γ∗) −vk(x)
 ≤∥c∥∞
βk
1 −β ,
J∗
β(cn, Tn, γ∗
n) −vk
n(x)
 ≤∥c∥∞
βk
1 −β .
(8)
Combining the results,
|Jβ(cn, Tn, γ∗
n) −|Jβ(c, T , γ∗)| ≤|Jβ(cn, Tn, γ∗
n) −vk
n(x)| + |vk
n(x) −vk(x)|
+ |Jβ(c, T , γ∗) −vk(x)|.
Note that the ﬁrst and the last term can be made arbitrarily small since (8)
holds for all k ∈N; the second term goes to 0 with an inductive argument for
all k ∈N.
⊓⊔
A Suﬃcient Condition for Robustness Under Weak Convergence. We
now present a result that establishes robustness if the optimal policies for every
initial point are identical. That is, for every n, γ∗
n is optimal for every x0 ∈X
(under the model Tn). A suﬃcient condition for this property is that γ∗
n solves
the discounted cost optimality equation (DCOE) for every initial point.
A policy γ∗∈Γ solves the discounted cost optimality equation and is optimal
if it satisﬁes
J∗
β(c, T , x) = c(x, γ∗(x)) + β

J∗
β(c, T , x1)T (dx1|x, γ∗(x)).
Thus, a policy is optimal for every initial point if it satisﬁes the DCOE for all
initial points x ∈X. The following generalizes [18].
Theorem 11. Under Assumption 1, Jβ(c, T , γ∗
n) →Jβ(c, T , γ∗) for any initial
point x0 if γ∗
n is optimal for any initial point for the kernel Tn and for the
stage-wise cost function cn.
Remark 1. For the partially observed case, the proof approach we use makes use
of policy exchange (e.g. (4)) and for this approach the total variation continuity
of channel Q(·|x) is a key step to deal with the uniform convergence over policies.
As we stated before, the channel for fully observed models can be considered in
the form of (6) which is only weakly continuous and not continuous in total vari-
ation. Thus, obtaining a result uniformly over all policies may not be possible.
However, for the fully observed models we can reach continuity and robustness
(Theorem 10, Theorem 11) using a value iteration approach. With this approach,
instead of exchanging policies and analyzing uniform convergence over all poli-
cies, we can exchange control actions (e.g. (7)) and analyze uniform convergence
over the action space U by using the discounted optimality operator (3). Hence,
we are only able to show convergence over optimal policies for the fully observed
case, i.e. Jβ(cn, Tn, γ∗
n) →Jβ(c, T , γ∗) or Jβ(c, T , γ∗
n) →Jβ(c, T , γ∗) where γ∗
n
and γ∗are optimal policies, whereas, for partially observed models, regularity
of the channel allows us to show convergence over any sequence of policies, i.e.
supγ∈Γ |Jβ(cn, Tn, γ) −Jβ(c, T , γ)| →0.

Robustness to Incorrect Models and Approximations
183
Remark 2. As we have discussed in Subsect. 2.2, a partially observed model can
be reduced to a fully observed process where the state process (beliefs) becomes
probability measure valued. Consider the partially observed models with transi-
tion kernels Tn and T (with a channel Q) and their corresponding fully observed
transition kernels ηn and η: following the discussions and techniques in [9] and
[15], one can show that ηn and η satisfy the conditions of Theorem 11 and
Theorem 10 that is ηn(·|zn, un) →η(·|z, u) for any (zn, un) →(z, u) under the
following set of assumptions
– Tn(·|xn, un) →T (·|x, u) for any (xn, un) →(x, u),
– Q(·|x) is continuous on total variation in x.
We remark that these conditions also agree with the conditions presented for
continuity and robustness of the partially observed models (Theorem 4 and
Theorem 8).
4.2
Setwise Convergence
Absence of Continuity Under Setwise Convergence. We give a negative
result similar to Theorem 5, via Example 1:
Theorem 12. Letting Tn →T setwise, then it is not necessarily true that
J∗
β(c, Tn) →J∗
β(c, T ) even when c(x, u) is continuous and bounded in X × U.
A Suﬃcient Condition for Continuity Under Setwise Convergence.
Theorem 13. Under Assumption 3 Jβ(cn, Tn, γ∗
n) →Jβ(c, T , γ∗), for any ini-
tial state x0, as n →∞.
Proof. We use the same value iteration technique that we used to prove Theorem
10. See [18].
⊓⊔
Absence of Robustness Under Setwise Convergence. Now, we give a
result showing that even if the continuity holds under the setwise convergence
of the kernels, the robustness may not be satisﬁed (see [18, Theorem 4.7]).
Theorem 14. Supposing Tn(·|xn, un) →T (·|x, u) setwise for every x ∈X and
u ∈U and (xn, un) →(x, u), then it is not true in general that Jβ(c, T , γ∗
n) →
Jβ(c, T , γ∗), even when X and U are compact and c(x, u) is continuous and
bounded in X × U.
A Suﬃcient Condition for Robustness Under Setwise Convergence.
We now present a similar result to Theorem 11 that is we show that under the
conditions of Theorem 13, if further for every n, γ∗
n is optimal for every x0 ∈X
(under the model Tn) then robustness holds under setwise convergence.
Theorem 15. Supposing Assumption 3 holds, if further we have that for every
n, γ∗
n is optimal for every x0 ∈X (under the model Tn) then Jβ(c, T , γ∗
n) →
Jβ(c, T , γ∗).

184
A. D. Kara and S. Y¨uksel
4.3
Total Variation
The continuity result in Theorem 6 and the robustness result in Theorem 7 apply
to this case since the fully observed model may be viewed as a partially observed
model with the measurement channel Q given in (6).
5
Applications to Data-Driven Learning and Finite
Model Approximations
5.1
Application of Robustness Results to Data-Driven Learning
In practice, one may estimate the kernel of a controlled Markov chain using
empirical data; see e.g. [3,12] for some related literature in the control-free and
controlled contexts.
Let us brieﬂy review the basic case where an i.i.d. sequence of random vari-
ables is repeatedly observed, but its probability measure is not known apriori.
Let {(Xi), i ∈N} be an X-valued i.i.d. random variable sequence generated
according to some distribution μ. Deﬁning for every (ﬁxed) Borel B ⊂X, and
n ∈N, the empirical occupation measures μn(B) =
1
n
n
i=1 1{Xi∈B}, one has
μn(B) →μ(B) almost surely by the strong law of large numbers. It then follows
that μn →μ weakly with probability one [6, Theorem 11.4.1], . However, μn
does not converge to μ in total variation or setwise, in general. On the other
hand, if we know that μ admits a density, we can ﬁnd estimators to estimate μ
under total variation [5, Chapter 3]. For a more detailed discussion, see [17, pp.
1950–1951]. In the previous sections, we established robustness results under the
convergence of transition kernels in the topology of weak convergence and total
variation. We build on these observations.
Corollary 1 (to Theorem 6 and Theorem 7). Suppose we are given the
following dynamics for ﬁnite state space, X, and ﬁnite action space, U,
xt+1 = f(xt, ut, wt),
yt = g(xt, vt)
where {wt} and {vt} are i.i.d.noise processes and the noise models are unknown.
Suppose that there is an initial training period so that under some policy, every
x, u pair is visited inﬁnitely often if training were to continue indeﬁnitely, but that
the training ends at some ﬁnite time. Let us assume that, through this training,
we empirically learn the transition dynamics such that for every (ﬁxed) Borel
B ⊂X, for every x ∈X, u ∈U and n ∈N, the empirical occupation measures
are
Tn(B|x0 = x, u0 = u) =
n
i=1 1{Xi∈B,Xi−1=x,Ui−1=u}
n
i=1 1{Xi−1=x,Ui−1=u}
.
Then we have that J∗
β(Tn) →J∗
β(T ) and Jβ(T , γ∗
n) →J∗
β(T ), where γ∗
n is the
optimal policy designed for Tn. Since the channel model g has no restrictions,
this result also applies to the fully observed model setup by taking g(xt, vt) = xt.

Robustness to Incorrect Models and Approximations
185
Proof. We have that Tn(·|x, u) →T (·|x, u) weakly for every x ∈X, u ∈U
almost surely by law of large numbers. Since the spaces are ﬁnite, we also have
Tn(·|x, u) →T (·|x, u) under total variation. By Theorem 6 and Theorem 7, the
results follow.
⊓⊔
The following holds for more general spaces.
Corollary 2 (to Theorems 8, 4, 10 and 11). Suppose we are given the fol-
lowing dynamics with state space X and action space U,
xt+1 = f(xt, ut, wt),
yt = g(xt, vt),
where {wt} and {vt} are i.i.d.noise processes and the noise models are unknown.
Suppose that f(x, u, ·) : W →X is invertible for all ﬁxed (x, u) and f(x, u, w)
is continuous and bounded on X × U × W. We construct the empirical measures
for the noise process wt such that for every (ﬁxed) Borel B ⊂W, and for every
n ∈N, the empirical occupation measures are
μn(B) = 1
n
n
	
i=1
1{f −1
xi−1,ui−1(xi)∈B}
(9)
where f −1
xi−1,ui−1(xi) denotes the inverse of f(xi−1, ui−1, w) : W →X for given
(xi−1, ui−1). Using the noise measurements, we construct the empirical transi-
tion kernel estimates for any (x0, u0) and Borel B as
Tn(B|x0, u0) = μn(f −1
x0,u0(B)).
(i) If the measurement channel (represented by the function g) is continuous
in total variation then J∗
β(Tn) →J∗
β(T ) and Jβ(T , γ∗
n) →J∗
β(T ), where γ∗
n
is the optimal policy designed for Tn for all initial points.
(ii) If the measurement channel is in the form g(xt, vt) = xt (i.e. fully observed)
then J∗
β(Tn) →J∗
β(T ) and if further for every n, γ∗
n is optimal for every
x0 ∈X (under the model Tn) then Jβ(T , γ∗
n) →J∗
β(T ).
Proof. We have μn →μ weakly with probability one where μ is the model. We
claim that the transition kernels are such that Tn(·|xn, un) →T (·|x, u) weakly
for any (xn, un) →(x, u). To see that observe the following for h ∈Cb(X)

h(x1)Tn(dx1|xn, un) −

h(x1)T (dx1|x, u)
=

h(f(xn, un, w))μn(dw) −

h(f(x, u, w))μ(dw) →0,
where μn is the empirical measure for wt and μ is the true measure again. For the
last step, we used that μn →μ weakly and h(f(xn, un, w)) continuously converge
to h(f(x, u, w)) i.e. h(f(xn, un, wn)) →h(f(x, u, w) for some wn →w since f
and g are continuous functions. Similarly, it can be also shown that Tn(·|x, u) and
T (·|x, u) are weakly continuous on (x, u). Thus, for the case where the channel is

186
A. D. Kara and S. Y¨uksel
continuous in total variation by Theorem 8 and Theorem 4 if c(x, u) is bounded
and U is compact the result follows.
For the fully observed case, J∗
β(Tn) →J∗
β(T ) by Theorem 10 and Jβ(T , γ∗
n) →
J∗
β(T ) by Theorem 11.
⊓⊔
Remark 3. We note here that the moment estimation method can also lead to
consistency. Suppose that the distribution of W is determined by its moments,
such that estimate models Wn have moments of all orders and limn = E[W r
n] =
E[W r] for all r ∈Z+. Then, we have that [4, Theorem 30.2] Wn →W weakly
and thus Tn(·|xn, un) →T (·|x, u) weakly for any (xn, un) →(x, u) under the
assumptions of above corollary. Hence, we reach continuity and robustness using
the same arguments as in the previous result (Corollary 2).
Now, we give a similar result with the assumption that the noise process of
the dynamics admits a continuous probability density function.
Corollary 3 (to Theorem 6 and Theorem 7). Suppose we are given the
following dynamics for real vector state space X and action space U
xt+1 = f(xt, ut, wt),
yt = g(xt, vt),
where {wt} and {vt} are i.i.d.noise processes and the noise models are unknown
but it is known that the noise wt admits a continuous probability density function.
Suppose that f(x, u, ·) : W →X is invertible for all (x, u). We collect i.i.d.
samples of {wt} as in (9) and use them to construct an estimator, ˜μn , as
described in [5] which consistently estimates μ in total variation. Using these
empirical estimates, we construct the empirical transition kernel estimates for
any (x0, u0) and Borel B as
Tn(B|x0, u0) = ˜μn(f −1
x0,u0(B)).
Then independent of the channel, J∗
β(Tn) →J∗
β(T ) and Jβ(T , γ∗
n) →J∗
β(T ),
where γ∗
n is the optimal policy designed for Tn. Since the channel model g has no
restrictions, this result also applies to the fully observed model setup by taking
g(xt, vt) = xt.
Proof. By [5] we can estimate μ in total variation so that almost surely
limn→∞∥˜μn −μ∥T V = 0. We claim that the convergence of ˜μn to μ under total
variation metric implies the convergence of Tn to T in total variation uniformly
over all x ∈X and u ∈U i.e. limn→∞supx,u ∥Tn(·|x, u) −T (·|x, u)∥T V = 0.
Observe the following:
sup
x,u ∥Tn(·|x, u) −T (·|x, u)∥T V
= sup
x,u
sup
||h||∞≤1


h(x1)Tn(dx1|x, u) −

h(x1)T (dx1|x, u)

= sup
x,u
sup
||h||∞≤1


h(f(x, u, w))˜μn(dw) −

h(f(x, u, w))μ(dw)

≤∥˜μn −μ∥T V →0.

Robustness to Incorrect Models and Approximations
187
Thus, by Theorem 6 and Theorem 7, the result follows.
⊓⊔
The following example presents some system and channel models which sat-
isfy the requirements of the above corollaries.
Example 2. Let X, Y, U be real vector spaces with
xt+1 = f(xt, ut) + wt,
yt = h(xt, vt)
for unknown i.i.d. noise processes {wt} and {vt}.
1. Suppose the channel is in the following form; yt = h(xt, vt) = xt +vt where vt
admits a density (e.g. Gaussian density). It can be shown by an application
of Scheﬀ´e’s theorem that the channels in this form are continuous in total
variation. If further f(xt, ut) is continuous and bounded then the requirements
of Corollary 2 hold for partially observed models.
2. If the channel is in the following form; xt = h(xt, vt) then the system is fully
observed. If further f(xt, ut) is continuous and bounded then the requirements
of Corollary 2 holds for fully observed models.
3. Suppose the function f(xt, ut) is known, if the noise process wt admits a con-
tinuous density, then one can estimate the noise model in total variation in a
consistent way (see [5]). Hence, the conditions of Corollary 3 holds indepen-
dent of the channel model.
5.2
Application to Approximations of MDPs and POMDPs
with Weakly Continuous Kernels
We now discuss Problem P4, that is whether approximation of an MDP model
with a standard Borel space with a ﬁnite MDPs can be viewed an instance of
robustness problem to incorrect models and whether our results can be applied.
Review of Finitely Quantized Approximations to Standard Borel
MDPs. Consider an MDP which is quantized as follows.
Finite State Approximate MDP: Quantization of the State Space. Let
dX denote the metric on X. For each n ≥1, there exists a ﬁnite subset {xn,i}kn
i=1
of X such that
min
i∈{1,...,kn} dX(x, xn,i) < 1/n for all x ∈X.
Let Xn := {xn,1, . . . , xn,kn} and deﬁne Qn mapping any x ∈X to the nearest
element of Xn, i.e.,
Qn(x) := arg min
xn,i∈Xn
dX(x, xn,i).

188
A. D. Kara and S. Y¨uksel
For each n, a partition {Sn,i}kn
i=1 of the state space X is induced by Qn by
setting
Sn,i = {x ∈X : Qn(x) = xn,i}.
Let ψ be a probability measure on X which satisﬁes
ψ(Sn,i) > 0 for all i, n,
and deﬁne probability measures ψn,i on Sn,i by restricting ψ to Sn,i:
ψn,i( · ) := ψ( · )/ψ(Sn,i).
Using {ψn,i}, we deﬁne a sequence of ﬁnite-state MDPs, denoted as f-MDPm,
to approximate the compact-state MDP.
For each m, f-MDPm is deﬁned as:

Xn, U, Tn, cn

, and the one-stage cost
function cn : Xn × U →[0, ∞) and the transition probability Tn on Xn given
Xn × U are given by
cn(xn,i, a) :=

Sn,i
c(x, a)ψn,i(dx)
Tn( · |xn,i, a) :=

Sn,i
Qn ∗T ( · |x, a)ψn,i(dx),
where Qn ∗T ( · |x, a) ∈P(Xn) is the pushforward of the measure T ( · |x, a) with
respect to Qn; that is,
Qn ∗T (zn,j|x, a) = T

{y ∈X : Qn(y) = xn,j}|x, a

,
for all xn,j ∈Xn.
Finite Action Approximate MDP: Quantization of the Action Space.
Let dU denote the metric on U. Since the action space U is compact and thus
totally bounded, one can ﬁnd a sequence of ﬁnite sets Λn = {an,1, . . . , an,kn} ⊂U
such that for all n,
min
i∈{1,...,kn} dU(a, an,i) < 1/n for all a ∈U.
In other words, Λn is a 1/n-net in U. Let us assume that the sequence {Λn}n≥1
is ﬁxed. To ease the notation in the sequel, let us deﬁne the mapping Υn
Υn(f)(x) := arg min
a∈Λn
dU(f(x), a),
(10)
where ties are broken so that Υn(f)(x) is measurable.
It is known that ﬁnite quantization policies are nearly optimal under the
conditions to be presented below, see [23, Theorem 3.2]. Thus, to make the
presentation shorter, we will either assume that the action set is ﬁnite, or it
has been approximated by a ﬁnite action space through the construction above.
Assuming ﬁnite action sets will help us avoid measurability issues (see universal
measurability discussions in [22]) as well as issues with existence of optimal
policies.

Robustness to Incorrect Models and Approximations
189
Assumption 6.(a) The one stage cost function c is nonnegative and continu-
ous.
(b) The stochastic kernel T ( · |x, a) is weakly continuous in (x, a) ∈X × U.
(c) U is ﬁnite.
(d) X is compact.
We note that condition (d) in Assumption 6 as presented in [22] was more
general, but we have used the simpler version here for clarity in exposition.
One can write the following ﬁxed point equation for the ﬁnite MDP
Jn
β (x) = min
a∈U

cn(x, a) + β
	
x1∈Xn
Jn
β (x1)Tn(x1|x, a)

where Tn is the transition model for the ﬁnite MDP and cn is the cost function
deﬁned on the ﬁnite model. Since the acton space is ﬁnite, we can ﬁnd an optimal
policy, say f ∗
n for this ﬁxed point equation. One can also simply extend Jn
β and
f ∗
n, which are deﬁned on Xn to the entire state space X by taking them constant
over the quantization bins Sn,i. If we call the extended versions ˆJn
β and ˆfn, the
following result can be established:
Theorem 16. [22, Theorem 2.2 and 4.1] Suppose Assumption 6 holds. Then,
for any β ∈(0, 1) the discounted cost of the deterministic stationary policy ˆfn,
obtained by extending the discounted optimal policy f ∗
n of f-MDPm to X (i.e.,
ˆfn = f ∗
n ◦Qn), converges to the discounted value function J∗of the compact-
state MDP:
lim
n→∞∥ˆJn
β (· ) −J∗
β(· )∥= 0
and
lim
n→∞∥Jβ( ˆfn, · ) −J∗
β∥= 0.
(11)
Theorems 16 shows that under Assumption 6, an optimal solution can be
approximated via the solutions of ﬁnite models. We now show that the above
approximation scheme can be viewed in relation to our robustness results.
Proof Sketch of Theorem 16 via results from Sect. 4. With the introduced setup,
one can see that the extended value function and optimal policy for the ﬁnite
model satisfy the following:
ˆJn
β (x) = min
a∈U

ˆcn(x, u) + β

ˆJn
β (x1) ˆTn(dx1|x, u)

where ˆcn is the extended version of cn to the state space X by making it constant
over the quantization bins {Sn,i}i and ˆTn is such that for any function f

f(x1) ˆTn(dx1|x, u) :=

x1∈X

z∈Sn,i
f(x1)T (dx1|z, u)ψn,i(dz)
where Sn,i is the quantization bin that x belongs to.

190
A. D. Kara and S. Y¨uksel
With this setup, one can see that for any xn →x we have ˆcn(xn, u) →c(x, u)
and for any continuous and bounded f

f(x1) ˆTn(dx1|xn, u) :=

x1∈X

z∈Sn,i
f(x1)T (dx1|z, u)ψn,i(dz)
→

f(x1)T (dx1|x, u).
Hence, Assumption 1 holds under Assumption 6, and we can conclude the
proof using Theorem 11 and Theorem 10.
⊓⊔
6
Concluding Remarks
We studied regularity properties of optimal stochastic control on the space of
transition kernels, and applications to robustness of optimal control policies
designed for an incorrect model applied to an actual system. We also presented
applications to data-driven learning and related the robustness problem to ﬁnite
MDP approximation techniques. For the problems presented in this article, our
focus was on inﬁnite horizon discounted cost setup. However, we note that the
results can be extended to the inﬁnite horizon average cost setup under various
forms of ergodicity properties on the state process.
References
1. Backhoﬀ-Veraguas, J., Bartl, D., Beiglb¨ock, M., Eder, M.: Adapted Wasserstein
distances and stability in mathematical ﬁnance. Financ. Stoch. 24, 3601–632 (2020)
2. Bayraktar, E., Dolinsky, Y., Guo, J.: Continuity of utility maximization under
weak convergence. Math. Financial Econ. 14(4), 1–33 (2020)
3. Billingsley, P.: Statistical methods in Markov chains. Ann. Math. Statist. 32, 12–40
(1961)
4. Billingsley, P.: Probability and Measure, 3rd edn. Wiley, New York (1995)
5. Devroye, L., Gy¨orﬁ, L.: Non-parametric Density Estimation: The L1 View. Wiley,
New York (1985)
6. Dudley, R.M.: Real Analysis and Probability, 2nd edn. Cambridge University Press,
Cambridge (2002)
7. Dupuis, P., James, M.R., Petersen, I.: Robust properties of risk-sensitive control.
Math. Control Signals Syst. 13(4), 318–332 (2000)
8. Esfahani, P.M., Kuhn, D.: Data-driven distributionally robust optimization using
the Wasserstein metric: performance guarantees and tractable reformulations.
Math. Program. 171(1), 1–52 (2018)
9. Feinberg, E., Kasyanov, P., Zgurovsky, M.: Partially observable total-cost Markov
decision process with weakly continuous transition probabilities. Math. Oper. Res.
41(2), 656–681 (2016)
10. Ghosh, J.K., Ramamoorthi, R.V.: Bayesian Nonparametrics. Springer, New York
(2003)
11. Gray, R.M.: Entropy and Information Theory. Springer-Verlag, New York (1990)

Robustness to Incorrect Models and Approximations
191
12. Gy¨orﬁ, L., Kohler, M.: Nonparametric estimation of conditional distributions.
IEEE Trans. Inf. Theory 53(5), 1872–1879 (2007)
13. Hernandez-Lerma, O., Lasserre, J.: Discrete-Time Markov Control Processes.
Springer, New York (1996)
14. Jacobson, D.: Optimal stochastic linear systems with exponential performance cri-
teria and their relation to deterministic diﬀerential games. IEEE Trans. Automat.
Contr. 18(2), 124–131 (1973)
15. Kara, A.D., Saldi, N., Y¨uksel, S.: Weak Feller property of non-linear ﬁlters. Syst.
Control Lett. 134, 104–512 (2019)
16. Kara, A. D., Y¨uksel, S.: Robustness to incorrect system models in stochastic control
and application to data-driven learning. In: 2018 IEEE Conference on Decision and
Control (CDC), pp. 2753–2758 (2018)
17. Kara, A.D., Y¨uksel, S.: Robustness to incorrect priors in partially observed stochas-
tic control. SIAM J. Control. Optim. 57(3), 1929–1964 (2019)
18. Kara, A.D., Y¨uksel, S.: Robustness to incorrect system models in stochastic control.
SIAM J. Control. Optim. 58(2), 1144–1182 (2020)
19. Parthasarathy, K.: Probability Measures on Metric Spaces. AMS, Providence
(2005)
20. Petersen, I., James, M.R., Dupuis, P.: Minimax optimal control of stochastic
uncertain systems with relative entropy constraints. IEEE Trans. Automat. Contr.
45(3), 398–412 (2000)
21. Pra, P.D., Meneghini, L., Runggaldier, W.J.: Connections between stochastic con-
trol and dynamic games. Math. Control Signals Syst. 9(4), 303–326 (1996)
22. Saldi, N., Y¨uksel, S., Linder, T.: On the asymptotic optimality of ﬁnite approxi-
mations to Markov decision processes with Borel spaces. Math. Oper. Res. 42(4),
945–978 (2017)
23. Saldi, N., Y¨uksel, S., Linder, T.: Near optimality of quantized policies in stochastic
control under weak continuity conditions. J. Math. Anal. Appl. 435(1), 321–337
(2015)
24. Savkin, A.V., Petersen, I.R.: Robust control of uncertain systems with structured
uncertainty. J. Math. Syst. Est. Control 6(3), 1–14 (1996)
25. Sun, H., Xu, H.: Convergence analysis for distributionally robust optimization and
equilibrium problems. Math. Oper. Res. 41(2), 377–401 (2016)
26. Ugrinovskii, V.A:. Robust H-inﬁnity control in the presence of stochastic uncer-
tainty. Int. J. Control 71(2), 219–237 (1998)

Full Gradient DQN Reinforcement
Learning: A Provably Convergent Scheme
Konstantin E. Avrachenkov1(B), Vivek S. Borkar2, Hars P. Dolhare2,
and Kishor Patil1
1 INRIA Sophia Antipolis, Valbonne 06902, France
{k.avrachenkov,kishor.patil}@inria.fr
2 Indian Institute of Technology Bombay, Mumbai 400076, India
borkar.vs@gmail.com,harshdolhare99@gmail.com
Abstract. We analyze the DQN reinforcement learning algorithm as a
stochastic approximation scheme using the o.d.e. (for ‘ordinary diﬀer-
ential equation’) approach and point out certain theoretical issues. We
then propose a modiﬁed scheme called Full Gradient DQN (FG-DQN,
for short) that has a sound theoretical basis and compare it with the
original scheme on sample problems. We observe a better performance
for FG-DQN.
Keywords: Markov decision process (MDP) · Approximate dynamic
programming · Deep Reinforcement Learning (DRL) · Stochastic
approximation · Deep Q-network (DQN) · Full Gradient DQN ·
Bellman error minimization
AMS(2000) Subject Classiﬁcation: Primary 93E35 · Secondary
68T05 · 90C40 · 93E35
1
Introduction
Recently we have witnessed tremendous success of Deep Reinforcement Learn-
ing algorithms in various application domains. Just to name a few examples,
DRL has achieved superhuman performance in playing Go [41], Chess [42] and
many Atari video games [31,32]. In Chess, DRL algorithms have also beaten
the state of the art computer programs, which are based on more or less brute-
force enumeration of moves. Moreover, playing Go and Chess, DRL surprised
experts with new insights and beautiful strategies [41,42]. We would also like
to mention the impressive progress of DRL applications in robotics [23,24,33],
telecommunications [29,36,51] and medicine [26,34].
The use of Deep Neural Networks is of course an essential part of DRL.
However, there are other paramount elements that contributed to the success of
DRL. A starting point for DRL was the Q-learning algorithm of Watkins [49],
which in its original form can suﬀer from the proverbial curse of dimension-
ality. In [25,45] the convergence of Q-learning has been rigorously established.
c
⃝The Author(s), under exclusive license to Springer Nature Switzerland AG 2021
A. Piunovskiy and Y. Zhang (Eds.): Modern Trends in Controlled Stochastic Processes,
ECC 41, pp. 192–220, 2021. https://doi.org/10.1007/978-3-030-76928-4_10

Full Gradient DQN Reinforcement Learning
193
Then, in [21,22] Gordon has proposed and analyzed ﬁtted Q-learning using a
novel architecture based on what he calls ‘averager’ maps. In [38] Riedmiller
has proposed using a neural network for approximating Q-values. There he has
also suggested that we treat the right hand side of the dynamic programming
equation for Q-values (see Eq. (5) below) as the ‘target’ to be chased by the left
hand side, i.e., the Q-value itself, and then seek to minimize the mean squared
error between the two. The right hand side in question also involves the Q-value
approximation and ipso facto the parameter itself, which is treated as a ‘given’
for this purpose, as a part of the target, and the minimization is carried out
only over the same parameter appearing in the left hand side. This leads to a
scheme reminiscent of temporal diﬀerence learning, albeit a nonlinear variant of
it. The parameter dependence of the target leads to some diﬃculties because of
the permanent shifting of the target itself, what one might call the ‘dog chasing
its own tail’ phenomenon. Already in [38], frequent instability of the algorithm
has been reported.
The next big step in improvement of DRL performance was carried out by
DeepMind researchers, who elaborated the Deep Q-Network (DQN) scheme [31],
[32]. Firstly, to improve the stability of the algortihm in [38], they suggested
freezing the parameter value in the target network for several iterates. Thus in
DQN, the target network evolves on a slower timescale. The second successful
tweak for DQN has been the use of ‘experience replay’, or averaging over some
relevant traces from the past, a notion introduced in [27,28]. Then, in [47,48] it
was suggested that we introduce a separation of policy estimation and evaluation
to further improve stability. The latter scheme is called Double DQN. While
various success stories of DQN and Double DQN schemes have been reported,
this does not completely ﬁx the theoretical and practical issues.
Let us mention that apart from Q-value based methods in DRL, there is
another large family of methods based on policy gradient. Each family has its
own positive and negative features (for background on RL and DRL methods
we recommend the texts [7,20,43]). While there has been a notable progress in
the theoretical analysis of the policy gradient methods [1,2,8,13,30,44], there
are no works establishing convergence of the neural Q-value based methods to
the best of our knowledge.
In this work, we revisit DQN and scrutinize it as a stochastic approxima-
tion algorithm, using the ‘o.d.e.’ (for ‘ordinary diﬀerential equation’) approach
for its convergence analysis (see [11] for a textbook treatment). In fact, we go
beyond the basic o.d.e. approach to its generalization based on diﬀerential inclu-
sions, involving in particular non-smooth analysis. This clariﬁes the underlying
diﬃculties regarding theoretical guarantees of convergence and also suggests a
modiﬁcation, which we call the Full Gradient DQN, or FG-DQN. We estab-
lish theoretical convergence guarantees for FG-DQN and compare it empirically
with DQN on sample problems (forest management [14,16] and cartpole [5,19]),
where it gives better performance at the expense of some additional computa-
tional overhead per iteration.
As was noticed above, another successful tweak for DQN has been the use
of ‘experience replay’. We too incorporate this in our scheme. Many advantages

194
K. E. Avrachenkov et al.
of experience replay have been cited in literature, which we review later in this
article. We also unearth an interesting additional advantage of ‘experience replay’
for Bellman error minimization using gradient descent and compare it with the
‘double sampling’ technique of [3]. See Sects. 4.2 and 5.1 below.
2
DQN Reinforcement Learning
2.1
Q-learning
We begin by recalling the derivation of the original Q-learning scheme [49]
to set up the context. Consider a Markov chain {Xn} on a ﬁnite state space
S := {1, 2, · · · , s}, controlled by a control process {Un} taking values in a ﬁnite
action space A = {1, 2, · · · , a}. Its transition probability function is denoted by
(x, y, u) ∈S2 × A →p(y|x, u) ∈[0, 1] such that 
y p(y|x, u) = 1 ∀x, u. The
controlled Markov property then is
P(Xn+1 = y|Xm, Um, m ≤n) = p(y|Xn, Un)
∀n ≥0, y ∈S.
We call {Un} an admissible control policy. It is called a stationary policy if
Un = v(Xn) ∀n for some v : S →A. A more general notion is that of a stationary
randomized policy wherein one chooses the control Un at time n probabilistically
with a conditional law given the σ-ﬁeld Fn := σ(Xm, Um, m < n; Xn) that
depends only on Xn. That is,
ϕ(u|Xn) := P(Un = u|Fn) = P(Un = u|Xn)
for a prescribed map x ∈S →ϕ(·|x) ∈P(A) := the simplex of probability
vectors on A. One identiﬁes such a policy with the map ϕ. Denote the set of
stationary randomized policies by USR. In anticipation of the learning schemes
we discuss, we impose the ‘frequent updates’ or ‘suﬃcient exploration’ condition
lim inf
n↑∞
1
n
n−1

m=0
I{Xm = x, Um = u} > 0
a.s.
∀x, u.
(1)
Given a per stage reward (x, u) →r(x, u) and a discount factor γ ∈(0, 1), the
objective is to maximize the inﬁnite horizon expected discounted reward
E
 ∞

m=0
γmr(Xm, Um)

.
The ‘value function’ V : S →R deﬁned as
V (x) = max E
 ∞

m=0
γmr(Xm, Um)
X0 = x

,
x ∈S,
(2)
then satisﬁes the dynamic programming equation
V (x) = max
u

r(x, u) + γ

y
p(y|x, u)V (y)

,
x ∈S.
(3)

Full Gradient DQN Reinforcement Learning
195
Furthermore, the maximizer v∗(x) on the right (chosen arbitrarily if not unique)
deﬁnes a stationary policy v∗: S →A that is optimal, i.e., achieves the maximum
in (2). Equation (3) is a ﬁxed point equation of the form V = F(V ) (which deﬁnes
the map F : Rs →Rs) and can be solved by the ‘value iteration’ algorithm
Vn+1(x) = max
u

r(x, u) + γ

y
p(y|x, u)Vn(y)

,
n ≥0,
(4)
beginning with any V0 ∈Rs. F can be shown to satisfy
∥F(x) −F(y)∥∞≤γ∥x −y∥∞,
i.e., it is an ∥· ∥∞-norm contraction. Then (4) is a standard ﬁxed point iteration
of a contraction map and converges exponentially to its unique ﬁxed point V .
Now deﬁne Q-values as the expression in square brackets in (3), i.e.,
Q(x, u) = r(x, u) + γ

y
p(y|x, u)V (y),
x ∈S, u ∈A.
If the function Q(·, ·) is known, then the optimal control at state x is found by
simply maximizing Q(x, ·) without requiring the knowledge of reward or transi-
tion probabilities. This makes it suitable for data-driven algorithms of reinforce-
ment learning. By (3), V (x) = maxu Q(x, u). The Q-values then satisfy their
own dynamic programming equation
Q(x, u) = r(x, u) + γ

y
p(y|x, u) max
v
Q(y, v),
(5)
which in turn can be solved by the ‘Q-value iteration’
Qn+1(x, u) = r(x, u) + γ

y
p(y|x, u) max
v
Qn(y, v),
x ∈S, u ∈A.
(6)
What we have gained at the expense of increased dimensionality is that the
nonlinearity is now inside the conditional expectation w.r.t. the transition prob-
ability function. This facilitates a stochastic approximation algorithm [11] where
we ﬁrst replace this conditional expectation by actual evaluation at a real or sim-
ulated random variable ζn+1(x, u) with law p(·|x, u), and then make an incre-
mental correction to the current guess based on it. That is, replace (6) by
Qn+1(x, u) = (1−a(n))Qn(x, u)+a(n)

r(x, u) + γ max
v
Qn(ζn+1(x, v), v)

(7)
for some a(n) > 0. The Q-learning algorithm does so using a single run of a real
or simulated controlled Markov chain (Xn, Un), n ≥0, so that:
• at each time instant n, (Xn, Un) are observed and the (Xn, Un)th component
of Q is updated, leaving other components of Qn(·, ·) unchanged,

196
K. E. Avrachenkov et al.
• this update follows (7) where ζn+1(x, u) with x = Xn, u = Un, gets replaced
by Xn+1, which indeed has the conditional law p(·|Xn, Un) as required,
• {a(n)} are positive scalars in (0, 1) chosen to satisfy the standard Robbins-
Monro conditions of stochastic approximation [11], i.e.,

n
a(n) = ∞,

n
a(n)2 < ∞.
(8)
It is more convenient to write the resulting Q-learning algorithm as
Qn+1(x, u) = Qn(x, u) + a(n)I{Xn = x, Un = u}
	
r(x, u)
+ γ max
v
Qn(Xn+1, v) −Qn(x, u)

∀x, u,
(9)
where I{· · · } := the indicator random variable that equals 1 if ‘· · · ’ holds and
0 if not. The fact that only one component is being updated at a time makes
this an asynchronous stochastic approximation. Nevertheless, it exhibits the well
known ‘averaging eﬀect’ of stochastic approximation whereby it is a data-driven
scheme that emulates (6) and exhibits convergence a.s. to the same limit, viz.,
Q. For formal proofs, see [25,45,50].
2.2
DQN Learning
The raw Q-learning scheme (9), however, does inherit the ‘curse of dimension-
ality’ of MDPs. One common ﬁx is to replace Q by a parametrized family
(x, u, θ) →Q(x, u; θ) (where we again use the notation Q(·, ·; ·) by abuse of
terminology so as to match standard usage). Here θ ∈Θ ⊂Rd for a moderate
d ≥1 and the objective is to learn the ‘optimal’ approximation Q(·, ·; θ∗) by iter-
ating in Θ. For simplicity, we take Θ = Rd. One natural performance measure
is the ‘DQN Bellman error’
E(θ) := E

(Zn −Q(Xn, Un; θ))2
,
(10)
where
Zn := r(Xn, Un) + γ max
v
Q(Xn+1, v; θn)
is the ‘target’ that is taken as a given quantity and the expectation is w.r.t. the
stationary law of (Xn, Un). For later reference, note that this is diﬀerent from
the ‘true Bellman error’
¯E(θ) := E

r(Xn, Un) + γ

y p(y|Xn, Un) max
v
Q(y, v; θ) −Q(Xn, Un; θ)
2
.
(11)
The stochastic gradient type scheme based on the empirical semi-gradient of
E(·) then becomes
θn+1 = θn + a(n)(Zn −Q(Xn, Un; θn))∇θQ(Xn, Un; θn),
n ≥0.
(12)

Full Gradient DQN Reinforcement Learning
197
2.3
Experience Replay
An important modiﬁcation of the DQN scheme has been the incorporation of
‘experience replay’. The idea is to replace the term multiplying a(n) on the right
hand side of (12) by an empirical average over traces of transitions from past
that are stored in memory. The algorithm then becomes
θn+1 = θn + a(n)
M
×
M

m=1

(Zn(m) −Q(Xn(m), Un(m)))∇θQ(Xn(m), Un(m); θn(m))

, n ≥0,
(13)
where (Xn(m), Un(m)), 1 ≤m ≤N, are samples from past. This has multiple
advantages. Some that have been cited in literature are as follows.
1. As in the mini-batch stochastic gradient descent for empirical risk minimiza-
tion in machine learning, it helps reduce variance. It also diminishes eﬀects
of anomalous transitions.
2. Training based on only the immediate experiences (≈samples) tends to over-
ﬁt the model to current data. This is prevented by experience replay. In
particular, if past samples are randomly picked, they are less correlated.
3. The re-use of data leads to data eﬃciency.
4. Experience replay is better suited for delayed rewards or costs, e.g., when the
latter are realized only at the end of a long episode or epoch.
There are also variants of basic experience replay, e.g., [40], which replaces
purely random sampling from past by a non-uniform sampling which picks a
sample with probability proportional to its absolute Bellman error.
We shall be implementing experience replay a little diﬀerently in the variant
we describe next, which has yet another major advantage from a theoretical
standpoint in the speciﬁc context of our scheme.
2.4
Double DQN Learning
One more modiﬁcation of the vanilla DQN scheme is doing the policy selection
according the local network [47,48]. The target network is still used in Zn and is
updated on a slower time scale. The latter can be represented with another set
of parameters ¯θn. Thus, the iterate for the Double DQN scheme can be written
as follows:
θn+1 = θn + a(n)(Zn −Q(Xn, Un; θn))∇θQ(Xn, Un; θn),
n ≥0,
(14)
with
Zn := r(Xn, Un) + γQ(Xn+1, v; ¯θn)

v=argmaxv′Q(Xn+1,v′;θn).
For the sake of comparison, in the vanilla DQN one has:
Zn := r(Xn, Un) + γQ(Xn+1, v; ¯θn)

v=argmaxv′Q(Xn+1,v′;¯θn).

198
K. E. Avrachenkov et al.
Note that in Double DQN, the selection and evaluation of the policy is done
separately. According to [47,48] this modiﬁcation improves the stability of the
DQN learning. One can also combine Double DQN with experience replay [48].
3
The Issues with DQN Learning
The expression for DQN learning scheme is appealing because of its apparent
similarity with the very successful temporal diﬀerence learning for policy eval-
uation [46], not to mention its empirical successes, including some high proﬁle
ones such as [32]. Nevertheless, a good theoretical justiﬁcation seems lacking.
The diﬃculty arises from the fact that the ‘target’ Zn is not something extrane-
ous, but is also a function of the operative parameter θn. In fact, this becomes
apparent once we expand Zn in (12) to write
θn+1 = θn + a(n)(r(Xn, Un) + γ max
v
Q(Xn+1, v; θn) −Q(Xn, Un; θn))
× ∇θQ(Xn, Un; θn), n ≥0.
(15)
Write
˜E(θ, ¯θ) := E

r(Xn, Un) + γ max
v
Q(Xn+1, v; ¯θ) −Q(Xn, Un; θ)
2
,
(16)
where E[·] is the stationary expectation as before. Consider the ‘oﬀ-policy’ case,
i.e., {(Xn, Un)} is the state-action sequence of a controlled Markov chain satisfy-
ing (1) with a pre-speciﬁed stationary randomized policy that does not depend
on the iterates. (As we point out later, the ‘on-policy’ version, which allows for
the latter adaptation, has additional issues.) If we apply the ‘o.d.e. approach’ for
analysis of stochastic approximation (see, e.g., [11] for a textbook treatment),
we get the limiting o.d.e. as
˙θ(t) = −∇1 ˜E(θ(t), θ(t)),
where ∇i denotes gradient with respect to the ith argument of ˜E(·, ·) for i = 1, 2.
Thus it is a partial stochastic gradient descent wherein only the gradient with
respect to the ﬁrst occurrence of the variable is used. Unlike gradient dynamics,
there is no reason why such dynamics should converge. It was already mentioned
that in case of linear function approximation, the DQN iteration bears a simi-
larity with TD(0), except for the nonlinear ‘max’ term. The o.d.e. proof of con-
vergence for TD(0) does not carry over to DQN precisely because the stochastic
approximation version leads to the interchange of the conditional expectation
and max operators. The other issue is that in TD(0), the linear operator in
question is a contraction w.r.t. the weighted L2-norm weighted by the station-
ary distribution. That argument also fails for DQN because of presence of the
max operator.
That said, there is already a tweak that treats the ﬁrst occurrence of θ on the
RHS, i.e., that inside the maximizer, as the ‘target’ being followed, and updates

Full Gradient DQN Reinforcement Learning
199
it only after several (say, K) iterates. In principle, this implies a delay in the
corresponding input to the iteration and with decreasing stepsizes, introduces
only an asymptotically negligible additional error, so that the limiting o.d.e.
remains the same ([11], Chapter 6). This is also the case for Double DQN.
Suppose on the other hand that in DQN or Double DQN we consider a small
constant stepsize a(n) ≡a > 0 and let K be large, so that with a ﬁxed target
value, the algorithm nearly minimizes the Bellman error before the target is
updated. Then, assuming the simpler ‘oﬀ-policy’ case again, the limiting o.d.e.
for the target, treating the multiple iterates between its successive iterates as a
subroutine, is
˙θ(t) = −∇1 ˜E(x, θ(t))

x=argmax( ˜E(·,θ(t))).
(17)
There is no obvious reason why this should converge either. In fact the right hand
side would be ≈the zero vector near the current maximizer and the evolution
of the o.d.e. and the iteration would be very slow. Of course, this is a limiting
case of academic interest only, stated to underscore the fact that it is diﬃcult to
get convergent dynamics out of the DQN learning scheme. This motivates our
modiﬁcation, which we state in the next section.
4
Full Gradient DQN
We propose the obvious, viz., to treat both occurrences of the variable θ on equal
footing, i.e., treat it as a single variable, and then take the full gradient with
respect to it. The iteration now is
θn+1 = θn −a(n)

r(Xn, Un) + γ max
v
Q(Xn+1, v; θn) −Q(Xn, Un; θn)

× (γ∇θQ(Xn+1, vn; θn) −∇θQ(Xn, Un; θn))
(18)
for n ≥0, where vn ∈ArgmaxQ(Xn+1, ·; θn) chosen according to some tie-
breaking rule when necessary. Note that when the maximizer in the term involv-
ing the max operator is not unique, one may lose its diﬀerentiability, but the
expression above still makes sense in terms of the Frechet sub-diﬀerential, see
Appendix. We assume throughout that {Xn} is a Markov chain controlled by
the control process {Un} generated according to a ﬁxed stationary randomized
policy ϕ ∈USR. Other simulation scenarios are possible for the oﬀ-policy set-
up. For example, we may replace the triplets (Xn, Un, Xn+1) on the right hand
side by triplets (X′
n, U ′
n, Y ′
n) where {X′
n} are generated i.i.d. according to some
distribution with full support and (U ′
n, Y ′
n) are generated with conditional law
P(U ′
n = u, Y ′
n = y|X′
n = x) = ϕ(u|x)p(y|x, u), conditionally independent of all
other random variables generated till n given X′
n. The analysis will be similar.
Yet another possibility is that of going through the relevant pairs (x, u) in a
round robin fashion.

200
K. E. Avrachenkov et al.
We modify (18) further by replacing the right hand side as follows:
θn+1 = θn −a(n)
	
(r(Xn, Un) + γ max
v
Q(Xn+1, v; θn) −Q(Xn, Un; θn))
× (γ∇θQ(Xn+1, vn; θn) −∇θQ(Xn, Un; θn)) + ξn+1

(19)
for n ≥0, where {ξn} is extraneous i.i.d. noise componentwise distributed inde-
pendently and uniformly on [−1, 1], and the overline stands for a modiﬁed form
of experience replay which comprises of averaging at time n over past traces
sampled from (Xk, Uk, Xk+1), k ≤n, for which Xk = Xn, Uk = Un. We analyze
the asymptotic behavior of this scheme in the remainder of this section in the
‘oﬀ-policy’ case, i.e., we use a prescribed stationary randomized policy ϕ ∈USR.
We make the following key assumptions:
(C1) (Assumptions regarding the function Q(·, ·; ·))
1. The map (x, u; θ) →Q(x, u; θ) is bounded and twice continuously diﬀeren-
tiable in θ with bounded ﬁrst and second derivatives;
2. For each choice of x ∈S, the set of θ for which the maximizer of Q(x, ·; θ)
is not unique, is the complement of an open and dense set and has Lebesgue
measure zero;
3. Call ˆθ a critical point of E(·) (which is deﬁned in terms of Q) if the zero vector
is contained in the (Frechet) subdiﬀerential ∂−E(ˆθ) (see the Appendix for a
deﬁnition). We assume that there are at most ﬁnitely many such points.
We also assume:
(C2) (Stability assumption)
The iterates remain a.s. bounded, i.e.,
sup
n ∥θn∥< ∞a.s.
(20)
Our ﬁnal assumption is a bit more technical. Rewrite the term
(r(Xn, Un) + γ max
v
Q(Xn+1, v; θn) −Q(Xn, Un; θn))
as

y
p(y|Xn, Un)

r(Xn, Un) + γ max
v
Q(y, v; θn) −Q(Xn, Un; θn)

+ ε(Xn, Un, θn)
where the error term ε(Xn, Un, θn) captures the diﬀerence between the empir-
ical conditional expectation using experience replay and the actual conditional
expectation. We assume that:

Full Gradient DQN Reinforcement Learning
201
(C3) (Assumption regarding the residual error in experience replay)
The error terms {ε(Xn, Un, θn)} satisfy
ε(Xn, Un, θn) →0 a.s. and

n
a(n)E[|ε(Xn, Un, θ)|]|θ=θn < ∞a.s.,
where the expectation is taken w.r.t. the stationary distribution of the state-
action pairs.
We comment on these assumptions later. Recall the true Bellman error ¯E(·)
deﬁned in (11).
Theorem 1. The sequence {θn} generated by FG-DQN converges a.s. to a sam-
ple path dependent critical point of ¯E(·).
Proof: For notational ease, write
ϵ(n) := −ε(Xn, Un, θn) (γ∇θQ(Xn+1, vn; θn) −∇θQ(Xn, Un; θn)) ,
where vn is chosen from Argmax Q(Xn+1, ·; θn) as described earlier. Consider
the iteration
θn+1 = θn −a(n)
×
	 
y
p(y|Xn, Un)(r(Xn, Un) + γ max
v
Q(y, v; θn) −Q(Xn, Un; θn))

× (γ∇θQ(Xn+1, vn; θn) −∇θQ(Xn, Un; θn)) + ϵ(n) + ξn+1

(21)
for n ≥0. Adding and subtracting the one step conditional expectation of the
RHS with respect to F′
n := σ(Xm, Um, m ≤n), we have
θn+1 = θn −a(n)
×
	
y
p(y|Xn, Un)(r(Xn, Un) + γ max
v
Q(y, v; θn) −Q(Xn, Un; θn))

×
	
y
p(y|Xn, Un) (γ∇θQ(y, un(y); θn) −∇θQ(Xn, Un; θn))

+ a(n)ϵ(n) + a(n)Mn+1(θn)
(22)
where un(y) ∈Argmax Q(y, ·; θn) is chosen as described earlier, and {Mn(θn−1)}
is a martingale diﬀerence sequence w.r.t. the sigma ﬁelds {F′
n}, given by
Mn+1(θn) =
	 
y
p(y|Xn, Un)(r(Xn, Un) + γ max
v
Q(y, v; θn) −Q(Xn, Un; θn))

×
	
γ∇θQ(Xn+1, vn; θn) −

y
p(y|Xn, Un)γ∇θQ(y, un(y); θn)

+ ξn+1

.

202
K. E. Avrachenkov et al.
Because of our assumptions on Q(·, ·; ·) and {ξn}, Mn(·) will have derivatives
uniformly bounded in n and therefore a uniform linear growth w.r.t. θ. The
same holds for the expression multiplying a(n) in the ﬁrst term on the right.
We shall analyze this iteration as a stochastic approximation with Markov noise
(Xn, Un), n ≥0, and martingale diﬀerence noise Mn+1, n ≥0 ([11], Chapter 6).
The diﬃcult terms are those of the form γ∇θQ(y, u; θ) above, because all we
can say about them is that:
∇θQ(y, u; θ) ∈G(y, θ) :=

v
ψ(v|y)∇θQ(y, v; θ) : ψ(·|y) ∈Argmaxφ(·|y)
	
u
φ(u|y)Q(y, u; θ)


.
Deﬁne correspondingly the set-valued map
(x, u, θ) →H(x, u, θ)
by
H(x, u; θ) := co
	 
y
p(y|x, u)(r(x, u) + γ max
v
Q(y, v; θ) −Q(x, u; θ))

×

y
p(y|x, u) (γ∇θQ(y, vj; θ) −∇θQ(x, u; θ)) : vj ∈ArgmaxQ(y, ·; θ)

=
 
y
p(y|x, u)(r(x, u) + γ max
v
Q(y, v; θ) −Q(x, u; θ))

×

y
p(y|x, u)

γ∇θ ¯Q(y, πy; θ) −∇θQ(x, u; θ)

: πy ∈Argmax ¯Q(y, ·; θ)

where ¯Q(y, ψ; θ) := 
u ψ(u|y)Q(y, u; θ) for ψ ∈USR. Then (22) can be written
in the more convenient form as the stochastic recursive inclusion ([11], Chapter
5) given by
θn+1 ∈θn −a(n)
	
H(Xn, Un; θn) + ϵ(n) + Mn+1(θn)

.
(23)
We shall now use Theorem 7.1 of [52], pp. 355, for which we need to verify the
assumptions (A1)–(A5), pp. 331-2, therein. We do this next.

Full Gradient DQN Reinforcement Learning
203
• (A1) requires H(y, φ, θ) to be nonempty convex compact valued and upper
semicontinuous, which is easily veriﬁed. It is also bounded by our assumptions
on Q(·, ·; ·).
• Sn of [52] corresponds to our (Xn, Un) and (A2) can be veriﬁed easily.
• (A3) are the standard conditions on {a(n)} also used here.
• Mn+1(θn), n ≥0, deﬁned above, has linear growth in ∥θn∥as observed above.
Thus (20) implies that for some K ∈(0, ∞),
n

m=0
a(m)2E

∥Mm+1(θm)∥2|Fm

≤K(1 + sup
m ∥θm∥2)

m
a(n)2 < ∞a.s.
This implies that n−1
m=0 a(m)Mm+1(θm) is an a.s. convergent martingale by
Theorem 3.3.4, pp. 53-4, [10]. This veriﬁes (A4).
• (A5) is the same as (20) above.
Let μ(x, u) := the stationary probability P(Xn = x, Un = u) under ϕ. Then
Theorem 7.1 of [52] applies and allows us to conclude that the iterates will track
the asymptotic behavior of the diﬀerential inclusion
˙θ(t) ∈−

x,u
μ(x, u)H(x, u, θ(t)).
(24)
Now we make the important observation that under our hypotheses on the
function Q(·, ·; ·) (see 2. of (C1)), for all x, u and Lebesgue-a.e. θ belonging
to some open dense set O, H(x, u, θ) is the singleton corresponding to Argmax
Q(x, ·; θ) = {u} for some u ∈A. Furthermore, in this case, the RHS of (24)
reduces to −∇E(θ(t)). Since {ξn} has density w.r.t. the Lebesgue measure, so
will {θn} and therefore by (C1), θn ∈O ∀n, a.s. Let
L(x, u; θ) := 1
2
	
r(x, u) + γ

y
p(y|x, u) max
v
Q(y, v; θ) −Q(x, u; θ)

2
denote the instantaneous Bellman error. Then
¯E(θ) =

x,u
μ(x, u)L(x, u; θ).
Write ˆE(θ′) for ¯E(θ) evaluated at a possibly random θ′, in order to emphasize the
fact that while ¯E(·) is deﬁned in terms of an expectation, a random argument of
ˆE(·) is not being averaged over. We use an analogous notation for other quantities
in what follows. Applying the Taylor formula to ¯E(·), we have,
ˆE(θn+1) = ˆE(θn) +

x,u
μ(x, u)⟨∇θL(x, u; θ), θn+1 −θn⟩+ O(a(n)2).

204
K. E. Avrachenkov et al.
But by (22), a.s.,
θn+1 −θn = a(n)

−∇θL(Xn, Un; θn) + ϵ(n) + Mn+1(θn)

= a(n)
	
−

x,u
μ(x, u)∇θL(x, u; θn) + ϵ(n) + 
Mn+1(θn) + O(a(n)2)

,
where we have replaced ∇θL(Xn, Un; θn) with 
i,u μ(i, u)∇θL(i, u; θn), i.e.,
with the state-action process (Xn, Zn) averaged w.r.t. its stationary distribu-
tion (recall that under our randomized stationary Markov policy, it is a Markov
chain). This uses a standard (though lengthy) argument for stochastic approx-
imation with Markov noise that converts it to a stochastic approximation with
martingale diﬀerence noise using the associated parametrized Poisson equation,
at the expense of: (i) adding an additional martingale diﬀerence noise term
that we have added to Mn+1(θn) to obtain the combined martingale diﬀerence
noise 
Mn+1(θn), and, (ii) another O(a(n)2) term that comes from the diﬀer-
ence of the solution of the Poisson equation evaluated at θn and θn+1, which is
O(∥θn+1 −θn∥) = O(a(n)), multiplied further by an additional a(n) from (22) to
give a net error that is O(a(n)2). See [6] for a classical treatment of this passage.
Hence for suitable constants 0 < K1, K′
1 < ∞,
E[ ˆE(θn+1)|F′
n] ≤ˆE(θn) + a(n)
	
−∥

x,u
μ(x, u)∇θL(x, u; θn)∥2
+ K1

x,u
μ(x, u)|ε(x, u, θn)| + K2a(n)2

≤ˆE(θn) + a(n)
	
K1

x,u
μ(x, u)|ε(x, u, θn)| + K2a(n)2

, (25)
where we have used (C1). In view of (C3) and the fact 
n a(n)2 < ∞, the ‘almost
supermartingale’ convergence theorem (Theorem 3.3.6, p. 54, [10]) implies that
ˆE(θn) converges a.s. This is possible only if
θn →

θ : the zero vector is in

x,u
μ(x, u)H(x, u; θ)

=

θ : θ is a critical point of

x,u
μ(x, u)H(x, u; θ)

.
By property (P4) of the Appendix, it follows that H(i, u; θ) ⊂∂−L(i, u; θ).
By property (P3) of the Appendix, it then follows that 
i,u μ(i, u)H(i, u; θ) ⊂
∂−¯E(θ). The claim follows from item 3 in (C1) given that any limit point of θn
as n ↑∞must be a critical point of ∂−¯E(·) in view of the foregoing.
□

Full Gradient DQN Reinforcement Learning
205
Some comments regarding our assumptions are in order.
1. The vanilla Q-learning iterates, being convex combinations of previous iter-
ates with a bounded quantity, remain bounded. Thus the boundedness
assumption on Q in (C1) is reasonable. The twice continuous diﬀerentiability
of Q in θ is reasonable when the neural network uses a smooth nonlinearity
such as SmoothReLU, GELU or a sigmoid function. As we point out later,
using standard ReLU adds another layer of non-smooth analysis which we
avoid here for the sake of simplicity of exposition. The last condition in (C1)
is also reasonable, e.g., when the graphs of Q(x, u; ·), Q(x, u′; ·) cross along a
ﬁnite union of lower dimensional submanifolds.
2. (C2) assumes stability of iterates, i.e., supn ∥θn∥< ∞a.s. There is an assort-
ment of tests to verify this. See, e.g., [11], Chapter 3. Also, one can enforce
this condition by projection onto a convenient large convex set every time the
iterates exit this set, see ibid., Chapter 7.
3. (C3) entails that we perform successive experience replays over larger and
larger batches of past samples so that the error in applying the strong law
of large numbers decreases suﬃciently fast. While this is possible in principle
because of the increasing pool of past traces with time, this will be an ideal-
ization in practice. It seems possible that the additional error in absence of
this can be analyzed as in [37]. Note also that for deterministic control prob-
lems, experience replay is not needed for our purposes. The cartpole model
studied in the next section is an example of this.
It is worth noting that bulk of the argument above is indeed the classical
argument for convergence of stochastic gradient descent with both Markov and
martingale diﬀerence noise, except that our iteration ﬁts this paradigm only
‘a.s.’. The missing piece is that the (possibly random) point it converges to need
not be a point of diﬀerentiability of ¯E(·), and therefore not a classical critical
point thereof. This is what calls for the back and forth between the classical proof
and the diﬀerential inclusion limit for stochastic gradient descent to minimize a
non-smooth objective function.
Before we proceed, we would like to underscore a subtle point, viz., the role
of experience replay here. Consider the scheme without the experience replay as
above, given by
θn+1 = θn −a(n)(r(Xn, Un) + γ max
v
Q(Xn+1, v; θn) −Q(Xn, Un; θn))
×

γ∇θQ(Xn+1, v; θn)

v=argmaxQ(Xn+1,·;θn) −∇θQ(Xn, Un; θn)

.
(26)
The limiting o.d.e. for this is
˙θ(t) = E
 
y
p(y|Xn, Un)
	
(r(Xn, Un) + γ max
v
Q(y, v; θ(t)) −Q(Xn, Un; θn))
×

γ∇θQ(y, v; θ(t))

v=argmax Q(y,·;θ(t)) −∇θQ(Xn, Un; θ(t))
 

,(27)

206
K. E. Avrachenkov et al.
where E[·] denotes the stationary expectation. This is again not in a form where
the convergence is apparent. The problem, typical of naive Bellman error gra-
dient methods, is that we have a conditional expectation (w.r.t. p(·|Xn, Un)) of
a product instead of a product of conditional expectations, as warranted by the
actual Bellman error formula. The experience replay suggested above does one of
the conditional expectations ahead of time, albeit approximately, and therefore
renders (approximately) the expression a product of conditional expectations.
Observe that this is so because we average over past traces (Xm, Um, Xm+1)
where Xm, Um are ﬁxed at the current Xn, Un, so that it is truly a Monte Carlo
evaluation of a conditional expectation. If we were to average over such traces
without ﬁxing Xn, Un, we would get the o.d.e.
˙θ(t) = E

r(Xn, Un) + γ max
v
Q(Xn+1, v; θ(t)) −Q(Xn, Un; θn)
	
× E

γ∇θQ(Xn+1, v; θ(t))

v=argmaxQ(Xn+1,·;θ(t)) −∇θQ(Xn, Un; θ(t))

, (28)
where E[·] denotes the stationary expectation. Here the problem is that the
desired ‘expectation of a product of conditional expectations’ has been split
into a product of expectations, which too is wrong. This discussion underscores
an additional advantage of experience replay in the context of Bellman error
gradient methods, over and above its traditional advantages listed earlier.
4.1
Comments About ‘On-Policy’ Schemes
An ‘on-policy’ scheme has an additional complication, viz., the expectation oper-
ator in the deﬁnition of ¯E(·) itself depends on the parameter θ. This is because
the policy with which the state-action pairs (Xn, Un) are being sampled depends
at time n on the current iterate θn. Therefore there is explicit θ dependence for
the probability measure μ(·, ·), now written as μθ(·, ·). The framework of [52] is
broad enough to allow this ‘iterate dependence’ and we get the counterpart of
(24) with μ(·, ·) replaced by μθ(t)(·, ·), leading to the limiting diﬀerential inclusion
˙θ(t) ∈−∇∗¯Eθ(t)(θ(t)).
(29)
Here ∇∗denotes the Frechet subdiﬀerential with respect to only the argument in
parentheses, not the subscript. Hence it is not the full subdiﬀerential and the the-
oretical issues we pointed out regarding DQN come back to haunt us. This is true,
e.g., when you use the ϵ-greedy policy that picks the control argmax(Q(Xn, ·; θn))
with probability 1 −ϵ, and chooses a control independently and with uniform
probability from A, with probability ϵ.

Full Gradient DQN Reinforcement Learning
207
Clearly, a scheme such as (29) that performs gradient descent for the sta-
tionary expectation of a parametrized cost function w.r.t. the parameter, but
ignoring the parameter dependence of the stationary law itself on the parame-
ter, is not guaranteed to converge. There are special situations such as the EM
algorithm [18] where additional structure of the problem makes it work. In gen-
eral, policy gradient methods based on suitable sensitivity formulas for Markov
decision processes seem to provide the most ﬂexible approach in such situations,
see, e.g., [30].
4.2
Comparison with Double Sampling
To recapitulate, DQN can be viewed as an instance of a broader class of schemes
known as Bellman error minimization or Bellman residual minimization [3]. The
commonality between such schemes is that they ﬁrst replace the candidate value
function by a parsimoniously parametrized family of functions, e.g., linear com-
binations of basis functions or neural networks. The original equation then need
not hold, so one seeks to minimize the ‘Bellman error’, i.e., the squared diﬀer-
ence between the right and left hand sides of the approximate Bellman quation.
Its gradient involves a product of conditional expectations. If one uses the naive
strategy of replacing them by evaluation at actual samples, the gradient of the
resulting ‘empirical Bellman error’ leads to an (approximate) expectation of a
product in the averaged dynamics where it should have been the expectation of
a product of conditional expectations. That is, product and conditional expecta-
tion get interchanged, causing bias to creep in. In fact, [3] already containes a way
to avoid this. This is the ‘double sampling’ scheme that simulates two transitions
simultaneously at each time instant for the current state-action pair. These are
simulated independently with the same conditional law. One then performs the
function evaluations for next state in the two terms of the product in Bellman
error gradient using the two diﬀerent samples thus generated. While this has
been used subsequently (see, e.g., [9,35]), it can be very awkward to implement
in some simulation environments and is certainly untenable in on-line mode.
Also, it increases the variance as we note below in numerical experiments. One
of the contributions of the present work is to circumvent this by using a variant
of experience replay. This can be executed with a single simulation run with
buﬀered data and also has the advantage of lower variance due to averaging.
As for the mathematical analysis, the error process {ϵ(n)} in the application
of the strong law of large numbers to experience replay drops out and assump-
tion (C3) becomes redundant if no experience replay is used. With pure double
sampling without experience replay, we have only the martingale diﬀerence noise
obtained by subtracting from the right hand side of the iteration its one step
conditional expectation. This will be a little diﬀerent from the martingale dif-
ference noise {Mn+1(θn)} above due to the additional simulated transition and
perforce will have higher variance.

208
K. E. Avrachenkov et al.
A recent work [39] treats the empirical Bellman error as a deterministic
function of the parameter and minimizes it using the full gradient as described
here. It does not, however, use either double sampling or experience replay and
therefore retains the problem of replacing a product of conditional expectations
by conditional expectation of a product.
For deterministic systems, double sampling is redundant as there is no con-
ditional expectation in the Bellman equation. Experience replay may still be
desirable for its other advantages mentioned earlier, but is not required on above
grounds.
5
Numerical Results
In this section, we compare on two realistic examples the performance of FG-
DQN with respect to that of the standard DQN scheme [32]. In particular, we
investigate the behaviour of Bellman error, Hamming distance from the optimal
policy (if the optimal policy is known) and the average reward. The pseudo-code
for FG-DQN is described in Algorithm 1.
5.1
Forest Management Problem
Consider a Markov decision process framework for a simple forest manage-
ment problem [14,16]. The objective is to maintain an old forest for wildlife
and make money by selling the cut wood. We consider discounted inﬁnite hori-
zon discrete-time problem. The state of the forest at time n is represented by
Xn ∈{0, 1, 2, 3, · · · , M} where the value of the state represents the age of the
forest; 0 being the youngest and M being the oldest. The forest is managed by
two actions: ‘Wait’ and ‘Cut’. An action is applied at each time at the beginning
of the time slot. If we apply the action ‘Cut’ at any state, the forest will return
to its youngest age, i.e., state 0. On the other hand, when the action ‘Wait’
is applied, the forest will grow and move to the next state if no ﬁre occurred.
Otherwise, with probability p, the ﬁre burns the forest after applying the ‘Wait’
action, leaving it at its youngest age (state 0). Note that if the forest reaches
its maximum age, it will remain there unless there is a ﬁre or action ‘Cut’ is
performed. Lastly, we only get a reward when the ‘Cut’ action is performed. In
this case, the reward is equal to the age of the forest. There is no reward for the
action ‘Wait’.

Full Gradient DQN Reinforcement Learning
209
Algorithm 1: Full Gradient DQN (FG-DQN)
Input: replay memory D of size M, minibatch size B, number of episodes
N, maximal length of an episode T, discount factor γ, exploration
probability ϵ.
Initialise the weights θ randomly for the Q-Network.
for Episode = 1 to N do
Receive initial observation s1.
for n = 1 to T do
if Uni[0,1] < ϵ then
Select action Un at random.
else
Un = ArgmaxuQ(Xn, u; θ)
end
Execute the action and take a step in the RL environment.
Observe the reward Rn and obtain next state Xn+1.
Store the tuple (Xn, Un, Rn, Xn+1) in D.
Sample random minibatch of B tuples from D.
for k = 1 to B do
Sample all tuples (Xj, Uj, Rj, Xj+1) with a ﬁx state-action pair
(Xj = Xk, Uj = Uk) from D
Set Zj =

Rj,
for terminal state,
Rj + γ maxu Q(Xj+1, u; θ),
otherwise.
Compute gradients and using Eq. (19) update parameters θ.
end
end
end
Since the objective is to maximize the discounted proﬁt obtained by selling
wood, we may want to keep waiting to get the maximum possible reward, but
there is an increasing chance that the forest will get burned down.
For numerical simulations, we assume that the maximum age of the forest is
M = 10. Then, we implement standard DQN and FG-DQN to analyse the policy
obtained from the algorithm and the Bellman error. We use a neural network
with one hidden layer to approximate the Q-value. The number of neurons for
this hidden layer is 2000, and we use ReLU for nonlinear activation. It has been
recently advocated to use a neural network with one but very wide hidden layer
[2,15]. The input to the neural network is the state of the forest and the action.
Furthermore, the batch size to draw the samples for the experience replay is
ﬁxed to 25. We test both the algorithms for the oﬀ-policy scheme, i.e., we run
through all possible state-action pairs in round-robin fashion to train the neural
network.
We run two diﬀerent simulations - i) with low discounting factor γ = 0.8
and ii) with high discounting factor γ = 0.95. Figure 1 depicts the simulation

210
K. E. Avrachenkov et al.
(a)
(b)
Fig. 1. Forest management problem with γ = 0.8 and p = 0.05

Full Gradient DQN Reinforcement Learning
211
(a)
(b)
Fig. 2. Forest management problem with γ = 0.95 and p = 0.01

212
K. E. Avrachenkov et al.
results for case i) with forest ﬁre probability p = 0.05. We run the experiment
10 times and plot the running average of Bellman error across iterations in
Fig. 1(a). We also calculate the standard deviation of the Bellman error. The
shaded region in the plot denotes the 95% conﬁdence interval. We observe that
FG-DQN converges much faster than DQN. Furthermore, the variance for FG-
DQN is relatively low.
We now analyse how far the answer of each algorithm is from the optimal
policy. To do this, we ﬁrst ﬁnd the optimal policy for this setting by policy
iteration algorithm. The optimal policy has a threshold structure as follows:
π∗= [0, 0, 1, 1, 1, 1, 1, 1, 1, 1] for γ = 0.8 and p = 0.05.
After each iteration, we now evaluate the Q-network and calculate the Ham-
ming distance between the current policy and the optimal policy π∗, which gives
us the count of the number of states where optimal action is not taken. We run
the simulations 10 times and plot the average Hamming distance for DQN and
FG-DQN in Fig. 1(b). Note that we plot every 50th value of the average Ham-
ming distance in the ﬁgure. It is to avoid the squeezing of rare spikes obtained at
later time steps of the simulations. The shaded region denotes the 95% conﬁdence
interval for the averaged Hamming distance. We observe from the ﬁgure that the
policy obtained by FG-DQN starts converging to the optimal policy at around
8000 iterations. In comparison, for DQN, we observe a lot of spikes during later
iterations. The occurrence of these spikes means that there is a one-bit error in
the policy obtained by DQN. Further analysis shows that the DQN policy in this
case which has one bit error resembles to myopic policy [0, 1, 1, 1, 1, 1, 1, 1, 1, 1].
We next observe the impact of a high discounting factor on the performance
of our algorithm and how well it performs as compared to the standard DQN
scheme. We set γ = 0.95 and forest ﬁre probability p = 0.01. The optimal policy
obtained by exact policy iteration for this case is π∗= [0, 0, 0, 0, 0, 1, 1, 1, 1, 1].
Figure 2(a) shows the mean loss for 10 simulations and the corresponding 95%
conﬁdence interval. We observe similar behaviour as before, i.e., the variance
for FG-DQN is low. Figure 2(b) shows the averaged Hamming distance between
the policy obtained by the algorithm and the optimal policy. It is clear from
the ﬁgure that the variance for DQN is very high throughout the simulation. It
means we may end up with a policy that can have a 3 or 4 bits error at the end of
our simulation runs. On the other hand, FG-DQN is more stable since it shows
fewer variations with the increasing number of iterations. Thus, we are more
likely to get the policy with a 2 bits error on average. The shaded region in the
plot shows the 95% conﬁdence interval for 10 simulations which demonstrates
that the behaviour is consistent across the multiple simulations.
Let us present an additional simulation to evaluate the performance of FG-
DQN versus double sampling scheme [3]. We note that the double sampling
scheme requires to generate two independent samples at each time step. This
becomes diﬃcult in many simulation environments and impossible in on-policy
mode. We further note that if the underlying environment is deterministic,
both these schemes become exactly identical. Therefore, in order to investigate
the diﬀerence in their performance, we slightly modify the forest management

Full Gradient DQN Reinforcement Learning
213
problem to have more stochasticity in its dynamics. Namely, the dynamics
remain the same except for the following change. With probability p, the ﬁre
burns a fraction of the forest after applying the ‘Wait’ action. The fraction of
the forest burnt follows a uniform distribution. In this simulation, we set p = 0.2
and the discount factor γ = 0.9. The optimal policy obtained by the exact policy
iteration for this case is π∗= [0, 0, 1, 1, 1, 1, 1, 1, 1, 1]. Figure 3 shows the compar-
ison of averaged Hamming distance between the policy obtained by respective
algorithm and the optimal policy. Note that we run the simulations 10 times and
also plot the 95% conﬁdence intervals. We observe that the policy obtained from
FG-DQN approaches quicker the optimal policy and the performance is more
stable. On the other hand, the double sampling policy has signiﬁcant ﬂuctua-
tions even after 30000 iterations. The ﬁgure also shows that the double sampling
policy has a 2–4 bits error at the end of our simulation runs.
Fig. 3. Comparison with the double sampling scheme. Average Hamming distance from
the optimal policy for the forest management problem with resetting to a uniform value
and with γ = 0.9 and p = 0.2.

214
K. E. Avrachenkov et al.
Fig. 4. Cartpole system
5.2
Cartpole - OpenAI Gym Model
We now test our algorithm for a more complex example, the Cartpole-v0 model
from OpenAI gym [12]. The environment description is as follows. The state of
the system is deﬁned by a four dimensional tuple that represents cart position x,
cart velocity ˙x, pole angle α and angular velocity ˙α (See Fig. 4). The pole starts
upright and the aim is to prevent it from falling over by pushing the cart to the
left or to the right (binary action space). The cart moves without friction along
the x-axis.
We run multiple simulations, each with 1500 episodes for DQN and FG-
DQN. For every time-step while an episode is running, we get the reward of
+1. The episode ends if any of the following conditions holds: the pole is more
than 12◦from the vertical axis, the cart moves more than 2.4 units from the
centre, or the episode length is more than 200. The model is considered to be
trained well when the discounted reward is greater than or equal to 195.0 over
100 consecutive trials.
In this example, we used the ‘on-policy’ version with the popular ‘ϵ-greedy’
scheme which picks the current guess for the optimal (i.e., the control that
maximizes Q(Xn, ·; θn)) with probability 1 −ϵ and chooses a control uniformly
with probability ϵ for a prescribed ϵ > 0. We use ϵ = 0.1. As we see below,
FG-DQN continues to do much better than DQN even in this on-policy scheme
for which we do not have a convergence proof as yet.
We use a neural network with three hidden layers. The number of nodes for
the hidden layers are 16, 32, and 32, respectively. For non-linearity, we use ReLU
activation after each hidden layer.
We now compare the performances of FG-DQN and DQN for a very high
discounting factor of 0.99. Note that the Cartpole example is deterministic,

Full Gradient DQN Reinforcement Learning
215
(a)
(b)
Fig. 5. Cartpole example with γ = 0.99

216
K. E. Avrachenkov et al.
meaning that for a ﬁxed state-action pair (Xn, Un), the pole moves to state
X′
n with probability 1. As a result, there will be no averaging in Eq. (18) and
no need for ‘experience replay’. Since this example is complex with signiﬁcant
non-linearity, we use the batch size of 128 for both DQN and FG-DQN to update
the parameters of the neural network inside one iteration.
Figure 5(a) depicts the reward behaviour for a single typical run of DQN
and FG-DQN. We see that the ﬂuctuations for reward per episode for both the
algorithms are high, and thus, we also plot the moving average of rewards with a
window of 100 episodes. It is clear from the ﬁgure that FG-DQN starts achieving
the maximum reward of 200 after 800 episodes regularly, however, DQN hardly
attains the maximum reward during 1000 episodes. To check the consistency of
the behaviour of our algorithm, we run the experiment 10 times and plot the
average reward and 95% conﬁdence interval in Fig. 5(b). We see that FG-DQN
performs much better than DQN with an average reward after 1000 episodes
lying around 175. In comparison, the average reward for DQN is between 50 and
75.
6
Conclusions and Future Directions
We proposed and analyzed a variant of the popular DQN algorithm that we call
Full Gradient DQN or FG-DQN wherein we also include the parametric gradient
(in a generalized sense) of the target. This leads to a provably convergent scheme
with sound theoretical basis which also shows improved performance over test
cases. There is ample opportunity for further research in this direction, both
theoretically and in terms of actual implementations. To highlight opportunities,
we state here some additional remarks, which also contain a few pointers to future
research directions.
1. Since the critical points are isolated, we get a.s. convergence to a single sample
path dependent critical point. This situation is generic in the sense that it
holds true for the problem parameters in an open dense set thereof, by a
standard fact from Morse theory in the smooth case. However, connected
sets of non-isolated equilibria can occur due to overparametrization and it
will be interesting to develop suﬃcient conditions for point convergence.
2. Thanks to the addition of {ξn}, the noise in FG-DQN is ‘rich enough’ in
all directions in a certain sense. One then expects it to ensure that under
reasonable assumptions, the unstable equilibria, here the critical points other
than local minima of the Bellman error, will be avoided with probability one.
That is, a.s. convergence to a local minimum can be claimed. See Sect. 4.3 of
[11] for a result of this ﬂavor under suitable technical conditions. We expect
a similar result to hold here. In practice, the extraneous noise {ξn} is usually
unnecessary and the inherent numerical errors and noise of the iterations
suﬃce.
3. We can also use approximation of the ‘max’ operator by ‘softmax’, i.e.,
by picking the control with a probability distribution that concentrates on

Full Gradient DQN Reinforcement Learning
217
argmax and depends smoothly on the parameter θ. Then we can work with
a legitimate gradient in place of a set-valued map in the o.d.e. limit, at the
expense of picking up an additional bounded error term. Then the conver-
gence to a small neighborhood of an equilibrium may be expected, the size
of which will be dictated in turn by the bound on this error, see, e.g., [37].
There is a similar issue if we drop (C3) and let a persistent small error due to
the use of approximate conditional expectation by experience replay remain.
4. Working with nondiﬀerentiable nonlinearities such as ReLU raises further
technical issues in analysis that need to be explored. This will require further
use of non-smooth analysis.
5. As we have pointed out while describing our numerical experiments on the
cartpole example, FG-DQN gives a signiﬁcantly better performance than
DQN, in an ‘on-policy’ scenario for which we do not have rigorous theory yet.
This is another promising and important research direction for the future.
Acknowledgement. The authors are greatly obliged to Prof. K. S. Mallikarjuna Rao
for pointers to the relevant literature on non-smooth analysis. The work of VSB was
supported in part by an S. S. Bhatnagar Fellowship from the Council of Scientiﬁc and
Industrial Research, Government of India. The work of KP and KA is partly supported
by ANSWER project PIA FSN2 (P15 9564-266178/DOS0060094) and the project of
Inria - Nokia Bell Labs “Distributed Learning and Control for Network Analysis”. This
work is also partly supported by the project IFC/DST-Inria-2016-01/448 “Machine
Learning for Network Analytics”.
Appendix: Elements of Non-smooth Analysis
The (Frechet) sub/super-diﬀerentials of a map f : Rd →R are deﬁned by
∂−f(x) :=

z ∈Rd : lim inf
y→x
f(y) −f(x) −⟨z, y −x⟩
|x −y|
≥0

,
∂+f(x) :=

z ∈Rd : lim sup
y→x
f(y) −f(x) −⟨z, y −x⟩
|x −y|
≤0

,
respectively. Assume f, g is Lipschitz. Some of the properties of ∂±f are as
follows.
• (P1) Both ∂−f(x), ∂+f(x) are closed convex and are nonempty on dense
sets.
• (P2) If f is diﬀerentiable at x, both equal the singleton {∇f(x)}. Conversely,
if both are nonempty at x, f is diﬀerentiable at x and they equal {∇f(x)}.
• (P3) ∂−f + ∂−g ⊂∂−(f + g), ∂+f + ∂+g ⊂∂+(f + g).
The ﬁrst two are proved in [4], pp. 30-1. The third follows from the deﬁnition.
Next consider a continuous function f : Rd × B →R where B is a compact
metric space. Suppose f(·, y) is continuously diﬀerentiable uniformly w.r.t. y. Let

218
K. E. Avrachenkov et al.
∇xf(x, y) denote the gradient of f(·, y) at x. Let g(x) := maxy f(x, y), h(x) :=
miny f(x, y) with
M(x) := {∇xf(x, y), y ∈Argmaxf(x, ·)}
and
N(x) := {∇xf(x, y), y ∈Argminf(x, ·)}.
Then N(x), M(x) are compact nonempty subsets of B which are upper semi-
continuous in x as set-valued maps. We then have the following general version
of Danskin’s theorem [17]:
• (P4) ∂−g(x) = co(M(x)), ∂+g(x) = y if M(x) = {y}, = φ otherwise, and g
has a directional derivative in any direction z given by maxy∈M(x)⟨y, z⟩.
• (P5) ∂+h(x) = co(N(x)), ∂−h(x) = y if N(x) = {y}, = φ otherwise, and h
has a directional derivative in any direction z given by miny∈N(x)⟨y, z⟩.
The latter is proved in [4], pp. 44-6, the former follows by a symmetric argu-
ment.
References
1. Agarwal, A., Kakade, S.M., Jason D.L., Mahajan, G.: Optimality and approxima-
tion with policy gradient methods in Markov decision processes. In: Conference on
Learning Theory, PMLR, pp. 64–66 (2020)
2. Agazzi, A., Lu, J.: Global optimality of softmax policy gradient with single hidden
layer neural networks in the mean-ﬁeld regime. arXiv preprint arXiv:2010.11858
(2020)
3. Baird, L.: Residual algorithms: reinforcement learning with function approxima-
tion. In: Machine Learning Proceedings, vol. 30–37 (1995)
4. Bardi, M., Capuzzo-Dolcetta, I.: Optimal Control and Viscosity Solutions of
Hamilton-Jacobi-Bellman Equations. Birkh¨auser, Boston (2018)
5. Barto, A.G., Sutton, R.S., Anderson, C.W.: Neuronlike adaptive elements that can
solve diﬃcult learning control problems. IEEE Trans. Syst. Man Cybern. Syst. 5,
834–846 (1983)
6. Benveniste, A., Metivier, M., Priouret, P.: Adaptive Algorithms and Stochastic
Approximations. Springer, Heidelberg (1991). https://doi.org/10.1007/978-3-642-
75894-2 9
7. Bertsekas, D.P.: Reinforcement Learning and Optimal Control. Athena Scientiﬁc
(2019)
8. Bhandari, J., Russo, D.: Global optimality guarantees for policy gradient methods.
arXiv preprint arXiv:1906.01786 (2019)
9. Bhatnagar, S., Borkar, V.S., Prabuchandran, K.J.: Feature search in the Grass-
manian in online reinforcement learning. IEEE J. Sel. Top. Signal Process. 7(5),
746–758 (2013)
10. Borkar, V.S.: Probability Theory: An Advanced Course. Springer, New York (1995)
11. Borkar, V.S.: Stochastic Approximation: A Dynamical Systems Viewpoint. Hindus-
tan Publishing Agency, New Delhi, and Cambridge University Press, Cambridge,
UK (2008)

Full Gradient DQN Reinforcement Learning
219
12. Brockman, G., Cheung, V., Pettersson, L., Schneider, J., Schulman, J., Tang, J.,
Zaremba, W.: OpenAI Gym. ArXiv preprint arXiv:1606.01540 (2016)
13. Cai, Q., Yang, Z., Lee, J.D., Wang, Z.: Neural temporal-diﬀerence learning con-
verges to global optima. Adv. Neural Inf. Process. Syst. 32 (2019)
14. Chad`es, I., Chapron, G., Cros, M.J., Garcia, F., Sabbadin, R.: MDPtoolbox: a
multi-platform toolbox to solve stochastic dynamic programming problems. Ecog-
raphy 37, 916–920 (2014)
15. Chizat, L., Bach, F.: On the global convergence of gradient descent for over-
parameterized models using optimal transport. In: Proceedings of Neural Infor-
mation Processing Systems, pp. 3040–3050 (2018)
16. Couture, S., Cros, M.J., Sabbadin, R.: Risk aversion and optimal management of an
uneven-aged forest under risk of windthrow: a Markov decision process approach.
J. For. Econ. 25, 94–114 (2016)
17. Danskin, J.M.: The theory of max-min, with applications. SIAM J. Appl. Math.
14, 641–664 (1966)
18. Delyon, B., Lavielle, M., Moulines, E.: Convergence of a stochastic approximation
version of the EM algorithm. Ann. Stat. 27, 94–128 (1999)
19. Florian, R.V.: Correct equations for the dynamics of the cart-pole system. Roma-
nia, Center for Cognitive and Neural Studies (Coneural) (2007)
20. Fran¸cois-Lavet, V., Henderson, P., Islam, R., Bellemare, M.G., Pineau, J.: An
introduction to deep reinforcement learning. Found. Trends Mach. Learn. 11(3–4),
219–354 (2018)
21. Gordon, G. J.: Stable ﬁtted reinforcement learning. In: Advances in Neural Infor-
mation Processing Systems, pp. 1052–1058 (1996)
22. Gordon, G. J.: Approximate solutions to Markov decision processes. Ph.D. Thesis,
Carnegie-Mellon University (1999)
23. Gu, S., Holly, E., Lillicrap, T., Levine, S.: Deep reinforcement learning for robotic
manipulation with asynchronous oﬀ-policy updates. In: Proceedings of IEEE Inter-
national Conference on Robotics and Automation, pp. 3389–3396 (2017)
24. Haarnoja, T., Ha, S., Zhou, A., Tan, J., Tucker, G., Levine, S.: Learning to walk
via deep reinforcement learning. ArXiv preprint arXiv:1812.11103 (2018)
25. Jaakola, T., Jordan, M.I., Singh, S.P.: On the convergence of stochastic iterative
dynamic programming algorithms. Neural Comput. 6, 1185–1201 (1994)
26. Jonsson, A.: Deep reinforcement learning in medicine. Kidney Diseas. 5, 18–22
(2019)
27. Lin, L.J.: Self-improving reactive agents based on reinforcement learning, planning
and teaching. Mach. Learn. 8(3–4), 293–321 (1992)
28. Lin, L.-J.: Reinforcement learning for robots using neural networks. Ph.D. Thesis
School of Computer Science, Carnegie-Mellon University, Pittsburgh (1993)
29. Luong, N.C., Hoang, D.T., Gong, S., Niyato, D., Wang, P., Liang, Y.C., Kim, D.I.:
Applications of deep reinforcement learning in communications and networking: a
survey. IEEE Commun. Surv. Tutor. 21, 3133–3174 (2019)
30. Marbach, P., Tsitsiklis, J.N.: Simulation-based optimization of Markov reward pro-
cesses. IEEE Trans. Automat. Contr. 46, 191–209 (2001)
31. Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D.,
Riedmiller, M.: Playing Atari with deep reinforcement learning. arXiv preprint
arXiv:1312.5602 (2013)
32. Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A.A., Veness, J., Bellemare, M.G.,
Graves, A., Riedmiller, M., Fidjeland, A.K., Ostrovski, G., Petersen, S.: Human-
level control through deep reinforcement learning. Nature 518, 529–533 (2015)

220
K. E. Avrachenkov et al.
33. Peng, X.B., Berseth, G., Yin, K., van de Panne, M.: Deeploco: dynamic locomotion
skills using hierarchical deep reinforcement learning. ACM Trans. Graph. 36, 1–13
(2017)
34. Popova, M., Isayev, O., Tropsha, A.: Deep reinforcement learning for de novo drug
design. Sci. Adv. 4(7), eaap7885 (2018)
35. Prabuchandran, K.J., Bhatnagar, S., Borkar, V.S.: Actor-critic algorithms with
online feature adaptation. ACM Trans. Model. Comput. Simul. (TOMACS) 26(4),
1–26 (2016)
36. Qian, Y., Wu, J., Wang, R., Zhu, F., Zhang, W.: Survey on reinforcement learning
applications in communication networks. J. Commun. Netw. 4, 30–39 (2019)
37. Ramaswamy, A., Bhatnagar, S.: Analysis of gradient descent methods with nondi-
minishing bounded errors. IEEE Trans. Automat. Contr. 63, 1465–1471 (2018)
38. Riedmiller, M.: Neural ﬁtted Q iteration–ﬁrst experiences with a data eﬃcient neu-
ral reinforcement learning method. Machine Learning: ECML, pp. 317–328 (2005)
39. Saleh, E., Jiang, N.: Deterministic Bellman residual minimization. In: Proceedings
of Optimization Foundations for Reinforcement Learning Workshop at NeurIPS
(2019)
40. Schaul, T., Quan, J., Antonoglou, I., Silver, D.: Prioritized experience replay. arXiv
preprint arXiv:1511.05952 (2015)
41. Silver, D., Huang, A., Maddison, C.J., Guez, A., Sifre, L., van den Driessche, G.,
Hassabis, D.: Mastering the game of Go with deep neural networks and tree search.
Nature 529, 484–489 (2016)
42. Silver, D., Hubert, T., Schrittwieser, J., Antonoglou, I., Lai, M., Guez, A., Hassabis,
D.: A general reinforcement learning algorithm that masters chess, Shogi, and Go
through self-play. Science 362, 1140–1144 (2018)
43. Sutton, R.S., Barto, A.G.: Reinforcement Learning: An Introduction, 2nd edn.
MIT Press, Cambridge (2018)
44. Sutton, R. S., McAllester, D. A., Singh, S. P., Mansour, Y.: Policy gradient methods
for reinforcement learning with function approximation. In: Neural Information
Processing Systems Proceedings, pp. 1057–1063 (1999)
45. Tsitsiklis, J.N.: Asynchronous stochastic approximation and Q-learning. Mach.
Learn. 16, 185–202 (1994)
46. Tsitsiklis, J.N., Van Roy, B.: An analysis of temporal-diﬀerence learning with func-
tion approximation. IEEE Trans. Automat. Contr. 42, 674–690 (1997)
47. van Hasselt, H.: Double Q-learning. Adv. Neural. Inf. Process. Syst. 23, 2613–2621
(2010)
48. van Hasselt, H., Guez, A., Silver, D.: Deep reinforcement learning with double Q-
learning. In: Proceedings of the 30th AAAI Conference on Artiﬁcial Intelligence,
vol. 30, pp. 2094–2100 (2016)
49. Watkins, C.J.C.H.: Learning from delayed rewards. Ph.D. Thesis, King’s College,
University of Cambridge, UK (1989)
50. Watkins, C.J., Dayan, P.: Q-learning. Mach. Learn. 8(3–4), 279–292 (1992)
51. Xiong, Z., Zhang, Y., Niyato, D., Deng, R., Wang, P., Wang, L.C.: Deep rein-
forcement learning for mobile 5G and beyond: fundamentals, applications, and
challenges. IEEE Veh. Technol. Mag. 14, 44–52 (2019)
52. Yaji, V.G., Bhatnagar, S.: Stochastic recursive inclusions with non-additive iterate-
dependent Markov noise. Stochastics 90, 330–363 (2018)

On Finite Approximations to Markov
Decision Processes with Recursive
and Nonlinear Discounting
Fan Deng1, Xin Guo2, and Yi Zhang1(B)
1 Department of Mathematical Sciences, University of Liverpool, Liverpool, UK
{Fan.Deng,yi.zhang}@liverpool.ac.uk
2 School of Economics and Management, Tsinghua University, Beijing, China
guoxin5@sem.tsinghua.edu.cn
Abstract. In this paper, ﬁnite approximation schemes are justiﬁed for
Markov decision processes in Borel spaces with recursive and nonlinear
discounting. Explicit error bounds are obtained in terms of the system
primitives. This allows one to solve the original problem approximately
up to any given accuracy, by solving a sequence of problems in ﬁnite
spaces.
Keywords: Finite approximations · Error bound · Markov decision
processes · Nonlinear discounting
AMS (2020) Subject Classiﬁcation: Primary 90C40 · Secondary
90C59
1
Introduction
In this paper, we justify a ﬁnite approximation scheme to solve numerically
Markov decision processes (MDP) with recursive and nonlinear discounting.
The deterministic dynamic programming problem (as a special MDP model)
with recursive and nonlinear discounting was considered in [11,12], which found
many applications to economics. A more recent development is [5], which also
demonstrates the connections of this model with several other relevant problems.
There are two possible ways of extending this model from the deterministic to
the stochastic dynamic programming setup. The latter term is used interchange-
ably below with an MDP. One way of extension was carried out in [10], where the
total cost is discounted ﬁrstly along each sample path and then the expectation
is applied. For the resulting MDP problem, in general, stationary policies do not
form a suﬃcient class. A second possible extension was published more recently,
see [3], where the conditional expected discounted cost is aggregated recursively.
In [3], it was shown that stationary optimal policies exist under the conditions
imposed therein, along with some meaningful examples in e.g., optimal growth
problems. We mention that the models in both [3,10] cover the standard linear
discounting as a special case.
c
⃝The Author(s), under exclusive license to Springer Nature Switzerland AG 2021
A. Piunovskiy and Y. Zhang (Eds.): Modern Trends in Controlled Stochastic Processes,
ECC 41, pp. 221–247, 2021. https://doi.org/10.1007/978-3-030-76928-4_11

222
F. Deng et al.
The purpose of this paper is to justify a ﬁnite approximation scheme with
an explicit error bound to the MDP problem considered in [3]. Finite approxi-
mations to MDP models with standard discounting have been considered inten-
sively in the literature, and we conﬁne ourselves to the most relevant ones here.
Most early literature provides convergence results without an explicit estimate
of the error bound. For models with denumerable state and action spaces, see
[2,15,17] and the discussion therein. A most recent development is [13]. Finite
approximations to MDP models in Borel spaces with standard linear discount-
ing were considered and justiﬁed in e.g., [4], where an explicit error bound was
provided for the underlying approximation scheme. More recent developments
in this direction can be found in e.g., [6,7,16]. An error bound is desirable for
practical implementations and computations, but establishing it usually requires
stronger conditions on the model.
In the present paper, we extend the method in [6,7] to MDP problems with
recursive and nonlinear discounting. The model considered here is with state and
action spaces being both Borel spaces. Besides the compactness-continuity and
growth conditions imposed in [3], which are needed for establishing basic opti-
mality results (solvability and the dynamic programming equation), we assume
further that the model has Lipschitz continuous initial data. Like in [4,6,7], this
allows us to obtain an explicit error bound. The imposed conditions are satisﬁed
by a version of the stochastic optimal growth problem formulated in [3], which
can also serve as a motivation of this paper.
The rest of the paper is organized as follows. In Sect. 2, we describe the
model, impose the conditions on it, as well as brieﬂy present the relevant facts
established in [3]. In Sect. 3 we present the main results, whose proofs are post-
poned to Sect. 5. An example is presented in Sect. 4 to illustrate the veriﬁcation
of the imposed conditions.
2
Model Description
In this section, we present the concerned model, and introduce the conditions on
the system primitives. To ease the reading, we also formulate the relevant state-
ments and facts, primarily from [3], which will be referred to in the subsequent
sections. In what follows, unless stated otherwise, measurability is understood
with respect to underlying Borel σ-algebra, and δx denotes the Dirac measure.
The system primitives of our model are as follows:
– X is the state space, assumed to be a locally compact (topological) Borel
space. A (topological) Borel space is a Borel subset (endowed with the relative
topology) of a complete separable metric space. Let dX be the metric on X,
and we endow X with its Borel σ-algebra B(X).
– A is the action space, assumed to be a locally compact Borel space, with the
metric dA and the Borel σ-algebra B(A).
– A(x) ∈B(A) is the nonempty set of admissible actions at the state x ∈X.
That is, A(x) deﬁnes a multifunction on X, denoted by A. Assume that
D := {(x, a) : x ∈X, a ∈A(x)}

Finite Approximations
223
is a Borel subset of X ×A such that it contains the graph of some measurable
mapping from X to A, say f ∞. Here and below, X × A is endowed with the
metric dX + dA deﬁned by dX(x1, x2) + dA(a1, a2) for all x1, x2 ∈X and
a1, a2 ∈A.
– q(dy|x, a) is a stochastic kernel on X given (x, a) ∈D, representing the con-
trolled transition probability.
– u is an R-valued measurable function on D with u(x, a) representing the
utility associated with the current state x ∈X and action a ∈A(x).
– δ is an R-valued increasing (and thus measurable) function on R such that
δ(0) = 0, with δ(v) representing the discounted value if the continuing value
of the utility at the next stage is v. (The standard linear discounting with a
constant discount factor β ∈(0, 1) is retrieved if δ(v) = βv.)
Let us describe the controlled and controlling processes as follows. Let H0 :=
X and Hn := Dn × X for all 1 ≤n < ∞. We put H := D∞as the countably
inﬁnite product. For each 1 ≤n < ∞, Hn is a Borel space and is endowed with
the corresponding Borel σ-algebra B(Hn). The similar assertion applies to H.
Deﬁnition 1. (a) A policy π = {πn}n≥0 is given by a sequence of A-valued
measurable mappings πn on Hn such that πn(hn) ∈A(xn) for each hn =
(x0, a0, x1, a1, . . . , xn) ∈Hn.
(b) A policy π = {πn}n≥0 is called Markov and is written as {fn}n≥0 with
fn being measurable on X if πn(hn) = fn(xn) for all n ≥0 and hn =
(x0, a0, x1, a1, . . . , xn) ∈Hn.
(c) A policy π = {πn}n≥0 is called stationary and is written as f if for some
measurable mapping f on X, πn(hn) = f(xn) for all n ≥0 and hn =
(x0, a0, x1, a1, . . . , xn) ∈Hn.
The above policies are called pure or deterministic. For simplicity we do not
consider randomized strategies, in which case πn would be stochastic kernels on
A given hn = (x0, a0, . . . , xn) ∈Hn concentrated on A(xn).
Take (H, B(H)) as the sample space. Given an initial state x ∈X and policy
π = {πn}n≥0, by the Ionescu-Tulcea theorem, there is a unique probability
measure Pπ
x deﬁned thereon such that
Pπ
x(x0 ∈dy) = δx(dy);
Pπ
x(xn+1 ∈dy|hn, an) = q(dy|xn, an); Pπ
x(an ∈da|hn) = δπn(hn)(da) ∀n ≥0.
Here we use interchangeably xn and the random element deﬁned by Xn(h) = xn
for each h = (x0, a0, . . . , xn, an, . . . ) ∈H, and the same concerns the use of an.
The context excludes any confusion.
We shall impose the following conditions to guarantee the performance mea-
sure introduced below to be well deﬁned.
Condition 1. There is some [1, ∞)-valued measurable function w on X such
that the following are veriﬁed.
(a) For some constant b ≥0, |u(x)| ≤bw(x) for all x ∈X.

224
F. Deng et al.
(b) There is some [0, ∞)-valued increasing and continuous function γ on [0, ∞)
satisfying
(i) γ(0) = 0 and γ(x) < x for all x ∈(0, ∞).
(ii) |δ(x1) −δ(x2)| ≤γ(|x1 −x2|) for all x1, x2 ∈R.
(iii) γ(x1 + x2) ≤γ(x1) + γ(x2) for all x1, x2 ∈[0, ∞).
(iv) γ(w(x)y) ≤w(x)γ(y) for all x ∈X and y ∈[0, ∞).
(v) For some α ∈(0, ∞),

X w(y)q(dy|x, a) ≤αw(x) for all (x, a) ∈D, and
αγ(y) < y for all y ∈(0, ∞).
In the forthcoming discussions, we assume that Condition 1 is satisﬁed unless
stated otherwise. Let us list down some immediate consequences of the above
condition.
Condition 1(b, i) implies that
lim
n→∞γ(n)(y) = 0 ∀y ∈[0, ∞),
where γ(n)(y) := γ(γ(n−1))(y) for each n ≥2. Indeed, this is automatic if y = 0
for γ(0) = 0. Consider y > 0. Since γ(n)(y) decreases in n, limn→∞γ(n)(y) = c ≥
0 exists. If c > 0, then c > γ(c) = γ(limn→∞γ(n)(y)) = limn→∞γ(n+1)(y) = c,
which is a contradiction. Now Condition 1(b, i, v) implies
lim
n→∞˜γ(n)(y) = 0 ∀y ∈[0, ∞),
(1)
for
˜γ := αγ.
(2)
Condition 1(b, iii) asserts that γ is a sub-additive function on [0, ∞), which
together with Condition 1(b, i), implies that the next result applies to γ and ˜γ.
Proposition 1. Let ψ be a [0, ∞)-valued increasing sub-additive continuous
function on [0, ∞) satisfying ψ(y) < y for all y ∈(0, ∞) (so that ψ(0) = 0).
Deﬁne for all y ∈[0, ∞),
ψ0(y) := 0, ψ1(y) := ψ(y) := y; ψn+1(y) := y + ψ(ψn(y)) ∀n ≥0,
(3)
Then for each y ∈[0, ∞), ψn(y) is increasing in n, and
ψ∞(y) := lim
n→∞ψn(y)
exists and is ﬁnite. In particular, ψ∞(y) = y+ψ(ψ∞(y)) for all n ≥0. Moreover,
ψ∞is continuous on [0, ∞).
Proof. See Lemma 4.6 of [3]. (For the last assertion, by inspecting the proof of
Lemma 4.6 of [3], we see that ψn converges to ψ∞uniformly on each compact
subset, and thus the continuity of ψ∞follows from the continuity of ψn.)
⊓⊔

Finite Approximations
225
One can recognize that ψn(z) = z + ψ(z + ψ(z + · · · + ψ(z) . . . )), where z
appears n times. Proposition 1 will be instrumental on several occasions in the
main text below. In particular, we may legitimately consider
γ∞(z) := lim
n→∞γn(z) = sup
n≥1
γn(z) ∈[0, ∞) ∀z ∈[0, ∞);
˜γ∞(z) := lim
n→∞˜γn(z) = sup
n≥1
˜γn(z) ∈[0, ∞) ∀z ∈[0, ∞)
with γn(z) and ˜γn(z) as deﬁned in (3) with γ and ˜γ in lieu of ψ.
For any [1, ∞)-valued measurable function w on a (measurable) space
E, let Bw(E) be the collection of measurable functions v on E such that
||v||w := supx∈E
|v(x)|
w(x) < ∞. Such a function v will be called w-bounded (on E).
Condition 1(a) asserts that u is w-bounded with ||u||w ≤b.
To introduce the performance measure of a policy π = {πn}n≥0, for each
n ≥0, we consider the operators Tπn and Qγ
πn deﬁned as follows. For each
w-bounded function v on Hn+1 (n ≥0),
Tπnv(hn) := u(xn, πn(hn)) +

X
δ(v(hn, πn(hn), xn+1))q(dxn+1|xn, πn(hn)),
Qγ
πnv(hn) :=

X
γ(v(hn, πn(hn), xn+1))q(dxn+1|xn, πn(hn)) ∀hn ∈Hn.
(4)
Condition 1 implies that Tπn|v| is w-bounded on Hn. Consequently,
U π
1 (x) := Tπ00(x); U π
n (x) := Tπ0Tπ1 . . . Tπn−10(x) ∀n ≥2, x ∈X
are well deﬁned and in Bw(X). In fact, the next upper bound of the w-norm of
U π
n will be needed below.
Proposition 2. Suppose Condition 1 is satisﬁed. For each n ≥1 and policy π,
|U π
n (x)| ≤w(x)˜γn(||u||w) ≤w(x)˜γ∞(||u||w) ∀x ∈X.
Proof. See the proof of Lemma 5.3 of [3].
⊓⊔
The above-deﬁned U π
n is called the n-stage total recursively discounted utility
of the policy π, or say the total recursively discounted utility for the n-stage
problem. The discounting is non-linear. In case π = {fn}n≥0 is a Markov policy,
it is informative to write down that
U π
3 (x) = u(x, f0(x))
+

X
δ

u(x1, f1(x1)) +

X
δ(u(x2, f2(x2)))q(dx2|x1, f1(x1))

q(dx1|x, f0(x)).
The next proposition allows us to deﬁne the inﬁnite horizon total recursively
discounted utility of a policy as the limit of the n-stage performance measure.

226
F. Deng et al.
Proposition 3. Suppose Condition 1 is satisﬁed. Then
U π(x) := lim
n→∞U π
n (x) ∀x ∈X,
exists for each policy π, so that |U π(x)| ≤w(x)˜γ∞(||u||w) for all x ∈X. In
particular, U π ∈Bw(X) for each policy π. Moreover, the convergence also holds
in Bw(X): in fact, ||U π −U π
n ||w ≤˜γ(n)(˜γ∞(||u||w)) →0 as n →∞.
Proof. See Lemma 5.3 of [3] for the convergence, and Proposition 1 and (5.5) of
[3] for the bound of |U π| and ||U π −U π
n ||w.
⊓⊔
According to Proposition 3, we may legitimately consider
U π(x) = lim
n→∞Tπ0Tπ1 . . . Tπn−10(x) ∀x ∈X.
It is useful to observe that in the above deﬁnition of U π, we may replace 0 with
any function v ∈Bw(X), as stated in the next lemma.
Lemma 1. Suppose Condition 1 is satisﬁed. Then for any policy π and v ∈
Bw(X),
U π(x) = lim
n→∞Tπ0Tπ1 . . . Tπn−1v(x) ∀x ∈X.
Proof. Note that
|Tπ0Tπ1 . . . Tπn−10(x) −Tπ0Tπ1 . . . Tπn−1v(x)| ≤Qγ
π0Qγ
π1 . . . Qγ
πn−1|v|(x)
≤Qγ
π0Qγ
π1 . . . Qγ
πn−1(w||v||w)(x) ≤˜γ(n)(||v||w)w(x),
where the operator Qγ
πn was deﬁned in (4), the ﬁrst inequality is by Condition
1(b, i, ii), and the last inequality is by applying Condition 1(b, iv, v); recall that
˜γ was deﬁned by (2). It remains to recall that limn→∞˜γ(n)(||v||w) = 0 by (1). ⊓⊔
The concerned optimal control problem can be now stated as
Maximize over all π: U π(x).
(5)
The value function U is deﬁned by U(x) := supπ U π(x) for all x ∈X.
Deﬁnition 2. We call a policy π uniformly optimal if U π(x) = U(x) for all
x ∈X, and uniformly optimal on a subset E ⊆X if U π(x) = U(x) for all x ∈E.
If E = {x} is a singleton, we call the policy optimal at x. For a given ϵ > 0, we
call a policy uniformly ϵ-optimal on a subset E ⊆X if U π(x) + ϵ ≥U(x) for all
x ∈E. If E = X, then it is called uniformly ϵ-optimal.
The objective here is to provide an implementable scheme to obtain a uni-
formly ϵ-optimal policy on a given compact subset of the state space. To this
end, we impose further conditions on the model.
Condition 2.(a) The multifunction A is compact-valued, i.e., A(x) is a com-
pact subset of A for each x ∈X.

Finite Approximations
227
(b) For some constant LA ∈[0, ∞), the multifunction A satisﬁes
dH(A(x), A(y)) ≤LAdX(x, y) ∀x, y ∈X,
where dH is the Hausdorﬀmetric on the space of nonempty compact subsets
of A, so that
dH(A(x), A(y)) :=
sup
a∈A(x)
inf
b∈A(y) dA(a, b) ∨sup
b∈A(y)
inf
a∈A(x) dA(a, b).
(c) The function w from Condition 1 is continuous on X, and the function u is
Lipschitz continuous on D, i.e., for some constant Lu ∈[0, ∞),
|u(x, a) −u(y, b)| ≤Lu(dX(x, y) + dA(a, b)) ∀x, y ∈X, a ∈A(x), b ∈A(y).
(d)

X v(y)q(dy|x, a) is continuous in (x, a) ∈D for each bounded continuous
function v on X, and for v = w.
Condition 2(a, b) implies that the multifunction A is upper semicontinuous
(in fact, continuous), according to Lemma 2.6 of [7]. Therefore, Condition 2 is
stronger than Condition (W) in [3], which together with Condition 1, in turn
implies the following result.
Proposition 4. Suppose Conditions 1 and 2 are satisﬁed. Then the following
assertions hold.
(a) There is a stationary uniformly optimal policy for problem (5).
(b) |U(x)| ≤w(x)˜γ∞(||u||w) for all x ∈X, U is upper semicontinuous on X,
and is the unique solution to TU = U out of the set of upper semicontinuous
functions in Bw(X), where T is deﬁned for each v ∈Bw(X) by
Tv(x) :=
sup
a∈A(x)

u(x, a) +

X
δ(v(y))q(dy|x, a)

∀x ∈X.
Moreover, U = limn→∞T (n)v for any upper semicontinuous v ∈Bw(X),
where the convergence is in Bw(X).
(c) Deﬁne the functions {Un}N
n=0 on X by
U0 ≡0; Un(x) :=
sup
a∈A(x)

u(x, a) +

X
δ(Un−1(y))q(dy|x, a)

∀x ∈X, 1 ≤n ≤N.
Then for each 0 ≤n ≤N, Un = supπ U π
n , Un ∈Bw(X) and is upper
semicontinuous on X, and ||Un||w ≤˜γn(||u||w) ≤˜γ∞(||u||w), ||Un −U||w ≤
˜γ(n)(˜γ∞(||u||w)).
Proof. For parts (a,b), see Theorem 5.1 of [3]. For part (c), Un = supπ U π
n is
by a standard dynamic programming argument. The rest was established in the
proof of Theorem 5.1 of [3].
⊓⊔

228
F. Deng et al.
The operator T will be referred to frequently below. Under the conditions
of Proposition 4, by an extension of the Berge theorem, see [8,9], it maps any
upper semicontinuous function v ∈Bw(X) to an upper semicontinuous function
in Bw(X). We impose an additional condition, under which it will be veriﬁed
below that T is an operator from the space of Lipschitz continuous function
v ∈Bw(X) to itself, and U is a Lipschitz continuous function.
Condition 3. There is some constant Lq ∈[0, ∞) such that the following are
satisﬁed.
(a) For each Lipschitz continuous function v ∈Bw(X) with a Lipschitz constant
Lv,


X
δ(v(z))q(dz|x, a) −

X
δ(v(z))q(dz|y, b)

≤γ(LqLv)(dX(x, y) + dA(a, b)) ∀x, y ∈X, a ∈A(x), b ∈A(y).
(b) γ(Lqy)(1 + LA) < y for all y > 0.
For brevity, we put
ϕ(y) := γ(Lqy)(1 + LA), y ≥0,
(6)
so that ϕ(0) = 0. Under Conditions 1 and 3, Proposition 1 applies to ϕ in lieu
with ψ, so that for each y ≥0, ϕ∞(y) is deﬁned, and is ﬁnite.
Observe that when δ(x) = βx = γ(x) for all x ∈X and some β ∈[0, 1),
Condition 3(a) is the same as the next condition.
Condition 4. There is some constant Lq ∈[0, ∞) such that for each Lipschitz
continuous function v ∈Bw(X) with a Lipschitz constant Lv,


X
v(z)q(dz|x, a) −

X
v(z)q(dz|y, b)
 ≤LqLv(dX(d, y) + dA(a, b))
∀x, y ∈X, a ∈A(x), b ∈A(y).
3
Main Statement
In what follows, let K0 be a ﬁxed compact subset of X, and we present schemes
for obtaining stationary and Markov policies that are uniformly ϵ-optimal on
the arbitrarily ﬁxed set K0. The schemes are similar to those in [6,7] for linearly
discounted model and ﬁnite horizon model. They are based on solving a sequence
of models in ﬁnite state and action spaces, and are implementable in the sense of
Remark 1. In particular, the expression of the Markov policy that is uniformly
ϵ-optimal on K0 can be explicitly obtained.
Let ζ, ζX, ζA ∈(0, ∞) be ﬁxed. Then according to the proof of Lemma 2.9 of
[7], there is a sequence of compact subsets {Kn}n≥1 of X satisfying
sup
x∈Kn,a∈A(x)

X\Kn+1
w(y)q(dy|x, a) < ζ ∀n ≥0.
(7)

Finite Approximations
229
For each n ≥0, since Kn is compact, it has a ﬁnite ζX-net
Xn := {z1, . . . , zkn}
of Kn, and an associated measurable partition {Ki
n}kn
i=1 of Kn such that zi ∈Ki
n,
and for each z ∈Ki
n, dX(x, zi) < ζX. Let pKn
Xn(x) = zi for each x ∈Ki
n. Similarly,
for each x ∈X, since A(x) is compact, it has a ﬁnite ζA-net
B(x) := {b1, . . . , bk(x)}.
Let N ≥1 be ﬁxed. Deﬁne recursively the following functions:
ˆUN−1,N(x) := 0 ∀x ∈XN,
ˆUN−1,n(x) := max
a∈B(x)
⎧
⎨
⎩u(x, a) +

y∈Xn+1
δ( ˆUN−1,n+1(y))q((pKn+1
Xn+1)−1(y)|x, a)
⎫
⎬
⎭
∀x ∈Xn, 0 ≤n ≤N −1.
For each x ∈Xn, 0 ≤n ≤N −1, there is some cN,n(x) ∈B(x) such that
ˆUN−1,n(x) := u(x, cN,n(x)) +

y∈Xn+1
δ( ˆUN−1,n+1(y))q((pKn+1
Xn+1)−1(y)|x, cN,n(x)).
For each N ≥0, we deﬁne a Markov policy gN = {f N
n }n≥0 by
f N
n (x) := argmina∈A(x){dA(a, cN,n(pKn
Xn(x)))} ∀x ∈Kn,
f N
n (x) := f ∞(x) ∀x ∈X\Kn
for all 0 ≤n ≤N −1, and f N
n (x) := f ∞(x) for all x ∈X and n ≥N, where
f ∞is an arbitrarily ﬁxed stationary policy. The above deﬁnition is legitimate
because dA(a, cN,n(pKn
Xn(x))) is continuous in a ∈A(x) and measurable in x ∈Kn
and thus jointly measurable in (x, a) by [1, Lem.4.51] or [14, Prop.B.1.38], and
A is compact-valued and upper semicontinuous by [7, Lem.2.6]. In particular,
f N
n (x) := cN,n(x) for all x ∈Xn and n ≤N −1. This Markov policy gN will be
shown to be a required uniformly ϵ-optimal policy on the given compact set K0
when ζ, ζX, ζA and N are suitably chosen.
Finally, we deﬁne a stationary policy f N that will be shown to be a required
uniformly ϵ-optimal policy on the given compact set K0 when ζ, ζX, ζA and N
are suitably chosen. Let C0 := K0,
Cn :=
n−1

i=0
(X\Ci) ∩Kn n ≥1,
and C∞:= X \ (
n≥0 Cn). Then {Cn}n=0,1,...,∞is a (disjoint) partition of X
satisfying 
n≥0 Cn = 
n≥0 Ki. For the ﬁxed N ≥1, deﬁne a stationary policy
f N as follows:
f N(x) := f N+n
n
(x) ∀x ∈Cn, f N(x) := f ∞(x) ∀x ∈C∞.

230
F. Deng et al.
Theorem 1. Suppose Conditions 1, 2 and 3 are satisﬁed. Let ζ, ζX, ζA ∈(0, ∞)
and an integer N ≥1 be ﬁxed. Let K0 be any compact subset of X. Then, for
the stationary policy f N deﬁned above, the following holds:
sup
x∈K0
|U f N (x) −U(x)| ≤γ∞(LUζX + 3ζγ(˜γ∞(||u||w)) + 2γ∞( ˜Λ))
+ ˜γ∞(2˜γ(N)(˜γ∞(||u||w))) sup
x∈K0
w(x),
where
LU := ϕ∞(Λ), ˜Λ = LU(ζA + ζX) + γ(˜γ∞(||u||w))ζ
with ϕ being deﬁned by (6), and
Λ := Lu(1 + LA) ≥0.
(8)
The proofs of this theorem and Theorem 2 below are postponed to Sect. 5.
Remark 1.(a) As ζA, ζX, ζ →0, ˜Λ →0, and by Proposition 1, lim ˜
Λ→0 γ∞( ˜Λ) =
0. It follows that, for any given ϵ > 0, one may take small enough constants
ζ, ζX, ζA ∈(0, ∞) and a large enough integer N ≥1 such that the right-hand
side of the inequality in Theorem 1 is majorized by ϵ. The corresponding
stationary policy f N is uniformly ϵ-optimal on the given compact set K0.
Given the current state x ∈X, there is a unique n ∈{0, 1, . . . , ∞} such that
x ∈Cn, and according to that n, one can compute f N(x) = f N+n
n
(x) as the
action that should be chosen.
(b) The proof of the previous statement reveals that, for each x ∈
n≥0 Kn,
|U f N (x) −U(x)| ≤γ∞(LUζX + 3ζγ(˜γ∞(||u||w)) + 2γ∞( ˜Λ))
+ ˜γ∞(2˜γ(N)(˜γ∞(||u||w)))w(x).
The next statement asserts that the Markov policy gN is a required uniformly
ϵ-optimal policy on the given compact set K0 when ζ, ζX, ζA and N are suitably
chosen.
Theorem 2. Suppose Conditions 1, 2 and 4 are satisﬁed. Let ζ, ζX, ζA ∈(0, ∞)
and an integer N ≥1 be ﬁxed. Let K0 be any compact subset of X. For the
Markov policy gN deﬁned above, the following holds: Then
sup
x∈K0
|U gN (x) −U(x)| ≤2˜γ(N)(˜γ∞(||u||w)) sup
x∈K0
w(x) + γN(G)
with G := 2γN( ˜ΛN) + ϕ′
N(Λ)ζX + 3ζ˜γN(||u||w), where
ϕ′(y) := (1 + LA)Lqy ∀y ≥0, ˜ΛN := ϕ′
N(Λ)(ζA + ζX) + γ(˜γN(||u||w))ζ.
Obviously, a similar remark to Remark 1(a) can be formulated.

Finite Approximations
231
4
Example
We take a stochastic optimal growth model from [3] (see Example 7.1 therein) as
an example, to which the approximation schemes in this paper can be applied.
Example 1. The state x ∈X = [0, ∞) represents the wealth. At each stage, one
has to decide the amount a ∈A(x) = [0, x] to be consumed. Let A = [0, ∞).
The unconsumed wealth will be invested. If y is invested in this stage, then
the wealth in the next stage is yS, where S, representing the random shock, is
a [0, ∞)-valued random variable, whose distribution is ν. We assume that the
random shocks are all independent and identically distributed and with a ﬁnite
mean
s :=

[0,∞)
sν(ds) < ∞.
Therefore, we may take
q(dy|x, a) =

[0,∞)
δ(x−a)s(dy)ν(ds).
Proposition 5.(a) Consider u(x, a) = √1 + a for all x ∈X and a ∈A(x),
and
δ(x) = ((1 −ε)x + ε ln(1 + ε))I{x ≥0}
with ε ∈(0, 1) being a constant. Then Conditions 1, 2 and 4 are satisﬁed with
γ = δ on [0, ∞), w(x) = √1 + x, α = 1, LA = Lu = 1, Lq = ¯s.
(b) Consider u(x, a) = √1 + a −2 for all x ∈X and a ∈A(x), and
δ(x) =

β1x x ≤0
β2x x ≥0
for some constant β1, β2 ∈(0, 1). Assume 2β¯s < 1. Then Conditions 1, 2 and
3 are satisﬁed with γ(x) = βx, β = max{β1, β2}, w(x) = √1 + x, α = 1,
LA = Lu = 1, Lq = ¯s.
Proof. Condition 1(a) and (b, i), as well as Condition 2(a, b) are evidently
satisﬁed, whereas Condition 1(b, ii–v) and Condition 2(d) were veriﬁed by the
given function w and constant α in Example 7.1 of [3]. For example, Condition
1(v) holds according to the calculation

X
w(y)q(dy|x, a) =

[0,∞)

(x −a)s + 1ν(ds) ≤
√
x + 1.
Condition 2(c) holds because the derivative of √1 + a with respect to a is
bounded by 1, and |√1 + a −1| ≤a −0 for all a ∈[0, ∞). Finally, regard-
ing Condition 4, for a Lipschitz continuous function v ∈Bw(X) with a Lipschitz

232
F. Deng et al.
constant Lv, we note that


X
v(z)q(dz|x, a) −

X
v(z)q(dz|y, b)

=


X
v((x −a)s)ν(ds) −

X
v((y −b)s)ν(ds)

≤

X
Lv(|x −y| + |a −b|)sν(ds) = Lv¯s(|x −y| + |a −b|)
so that we may take Lq = ¯s.
(b) Conditions 1 and 2 can be seen to be satisﬁed as in part (a). Regarding
Condition 3(a), for a Lipschitz continuous function v ∈Bw(X) with a Lipschitz
constant Lv,


X
δ(v(z))q(dz|x, a) −

X
δ(v(z))q(dz|y, b)

≤

[0,∞)
γ(Lv(|x −y| + |a −b|)s)ν(ds)
= βLv¯s(|x −y| + |a −b|) = γ(¯sLv)(|x −y| + |a −b|)
and so we may take Lq = ¯s. Condition 3(b) holds because 2β¯s < 1.
⊓⊔
5
Proof of Main Statements
In this section, we provide the detailed proof of Theorem 1. The proof of
Theorem 2 is similar to the proof of Theorem 1, and will be sketched.
5.1
Proof of Theorem 1
Throughout this subsection, we suppose that Conditions 1, 2 and 3 are satisﬁed,
without explicit indications.
Lemma 2. Let v ∈Bw(X) be Lipschitz continuous with a Lipschitz constant
Lv. Then Tv ∈Bw(X) is also Lipschitz continuous with a Lipschitz constant
LT v = (Lu + γ(LqLv))(1 + LA).
Proof. In view of the remarks below Proposition 4, we only need to check the
claimed Lipschitz continuity of Tv as follows. Let x, z ∈X and some Lipschitz
continuous v ∈Bw(X) with a Lipschitz constant Lv be ﬁxed. Then

Finite Approximations
233
|Tv(x) −Tv(z)|
(9)
≤max

sup
a∈A(x)
inf
b∈A(z) {|u(x, a) −u(y, b)|
+


X
δ(v(y))q(dy|x, a) −

X
δ(v(y))q(dy|z, b)


,
sup
b∈A(z)
inf
a∈A(x) {|u(x, a) −u(y, b)|
+


X
δ(v(y))q(dy|x, a) −

X
δ(v(y))q(dy|z, b)


.
Indeed, in case |Tv(x) −Tv(z)| = Tv(x) −Tv(z), for any ﬁxed ϵ > 0, there is
some a∗∈A(x) such that Tv(x) ≤u(x, a∗)+

X δ(v(y))q(dy|x, a∗)+ϵ and thus
|Tv(x) −Tv(z)| ≤u(x, a∗) +

X
δ(v(y))q(dy|x, a∗) + ϵ
+
inf
b∈A(z)

−u(z, b) −

X
δ(v(y))q(dy|z, b)

≤
sup
a∈A(x)
inf
b∈A(z) {|u(x, a) −u(z, b)|
+


X
δ(v(y))q(dy|x, a) −

X
δ(v(y))q(dy|z, b)


+ ϵ.
Since ϵ > 0 was arbitrarily ﬁxed,
|Tv(x) −Tv(z)| ≤
sup
a∈A(x)
inf
b∈A(z) {|u(x, a) −u(z, b)|
+


X
δ(v(y))q(dy|x, a) −

X
δ(v(y))q(dy|z, b)


.
In case |Tv(x) −Tv(z)| = Tv(z) −Tv(x), we analogously see
|Tv(x) −Tv(z)| ≤
sup
b∈A(z)
inf
a∈A(x) {|u(x, a) −u(z, b)|
+


X
δ(v(y))q(dy|x, a) −

X
δ(v(y))q(dy|z, b)


,
and hence (9) holds. By Conditions 2 and 3
|u(x, a) −u(z, b)| +


X
δ(v(y))q(dy|x, a) −

X
δ(v(y))q(dy|z, b)

≤Lu(dX(x, z) + dA(a, b)) + γ(LqLv)(dX(x, z) + dA(a, b))
= (Lu + γ(LqLv))(dX(x, z) + dA(a, b))

234
F. Deng et al.
and so by (9),
|Tv(x) −Tv(z)|
≤(Lu + γ(LqLv))(dX(x, z) +
sup
a∈A(x)
inf
b∈A(z) dA(a, b) ∨sup
b∈A(z)
inf
a∈A(x) dA(a, b))
= (Lu + γ(LqLv))(dX(x, z) + dH(A(x), A(z)))
≤(Lu + γ(LqLv))(1 + LA)dX(x, z),
where the last inequality is by Condition 2(b).
⊓⊔
As a consequence of the previous lemma, we deduce the Lipschitz continuity
of the value function U.
Lemma 3. Let v ∈Bw(X) be a Lipschitz continuous function with a Lipschitz
constant Lv. Then the following assertions hold.
(a) For each n ≥1, T nv ∈Bw(X) is with a Lipschitz constant ϕn(Λ)+ϕ(n)(Lv),
where ϕn is deﬁned by (3) with ϕ in lieu of ψ. In particular, Un = T n0 is
Lipschitz continuous with a Lipschitz constant ϕn(Λ) ≤ϕ∞(Λ).
(b) The value function U is Lipschitz continuous with a Lipschitz constant LU =
ϕ∞(Λ).
Proof. (a) By Lemma 2, T nv ∈Bw(X) and is Lipschitz continuous for each
n ≥0, and we may take the following as a Lipschitz constant of Tv:
Lu(1 + LA) + γ(LqLv)(1 + LA) = Λ + ϕ(Lv),
and thus the claimed relation holds for n = 1. Assume it holds for n. Now, by
Lemma 2 and the inductive supposition, we may take the following as a Lipschitz
constant of T n+1v = T(T nv):
Λ + ϕ(ϕn(Λ) + ϕ(n)(Lv)) ≤Λ + ϕ(ϕn(Λ)) + ϕ(n+1)(Lv)
= ϕn+1(Λ) + ϕ(n+1)(Lv),
where the inequality is by the sub-additivity of ϕ. The statement follows from
this and the induction.
(b) For each x, y ∈X, by Proposition 4 and the assertion in (a) with v ≡
0 = ϕ(0) = Lv,
|U(x) −U(y)| ≤lim
n→∞|T n0(x) −T n0(y)| ≤lim
n→∞ϕn(Λ)dX(x, y)
= ϕ∞(Λ)dX(x, y),
where the limit ϕ∞(Λ) is ﬁnite and exists by applying Proposition 1 to ϕ, which
is valid under Conditions 1 and 3. The statement follows now.
⊓⊔
For the forthcoming discussions and statements, for each ﬁxed N ≥1 and
0 ≤n ≤N, we extend the deﬁnition of ˆUN−1,n from Xn to Kn by putting

Finite Approximations
235
ˆUN−1,n(x) := ˆUN−1,n(pKn
Xn(x)) for all x ∈Kn \ Xn. Then for all x ∈Xn and
0 ≤n ≤N −1,
ˆUN−1,n(x) = max
a∈B(x)

u(x, a) +

Kn+1
δ( ˆUN−1,n+1(y))q(dy|x, a)

.
(10)
Lemma 4. Let N ≥1 and 0 ≤n ≤N be ﬁxed. Then supx∈Kn | ˆUN−1,n(x) −
UN−n(x)| ≤γ∞( ˜Λ) with ˜Λ = LU(ζA + ζX) + γ(˜γ∞(||u||w))ζ.
Proof. The case of n = N is trivial. Let 0 ≤n ≤N −1 be ﬁxed, and consider
ﬁrstly some x ∈Xn. Then
| ˆUN−1,n(x) −UN−n(x)| =
 max
a∈B(x)

u(x, a) +

Kn+1
δ( ˆUN−1,n+1(y))q(dy|x, a)

−sup
b∈A(x)

u(x, b) +

X
δ(UN−n−1(y))q(dy|x, b)
 .
The same argument as in the justiﬁcation of (9) shows
| ˆUN−1,n(x) −UN−n(x)|
≤max

sup
b∈A(x)
inf
a∈B(x)

|u(x, b) −u(x, a)| +


X
δ(UN−n−1(y))q(dy|x, b)
−

Kn+1
δ( ˆUN−1,n+1(y))q(dy|x, a)


,
sup
a∈B(x)
inf
b∈A(x)

|u(x, b) −u(x, a)| +


X
δ(UN−n−1(y))q(dy|x, b)
−

Kn+1
δ( ˆUN−1,n+1(y))q(dy|x, a)


.
Recall from Condition 2(c) that |u(x, a)−u(x, b)| ≤LudA(a, b) for each a ∈B(x)
and b ∈A(x). Also, for each a ∈B(x) and b ∈A(x),


X
δ(UN−n−1(y))q(dy|x, b) −

Kn+1
δ( ˆUN−1,n+1(y))q(dy|x, a)

≤


X
δ(UN−n−1(y))q(dy|x, b) −

X
δ(UN−n−1(y))q(dy|x, a)

+


X
δ(UN−n−1(y))q(dy|x, a) −

Kn+1
δ( ˆUN−1,n+1(y))q(dy|x, a)
 . (11)
For the ﬁrst summand, since UN−n−1 ∈Bw(X) and is Lipschitz continuous
with a Lipschitz constant ϕN−n−1(Λ) by Lemma 3 and Proposition 4, applying

236
F. Deng et al.
Condition 3 to it gives


X
δ(UN−n−1(y))q(dy|x, b) −

X
δ(UN−n−1(y))q(dy|x, a)

≤γ(LqϕN−n−1(Λ))dA(a, b).
For the second summand in (11),


X
δ(UN−n−1(y))q(dy|x, a) −

Kn+1
δ( ˆUN−1,n+1(y))q(dy|x, a)

≤

Kn+1
|δ(UN−n−1(y)) −δ( ˆUN−1,n+1(y))|q(dy|x, a)
+

X\Kn+1
|δ(UN−n−1(y))|q(dy|x, a)
≤
sup
y∈Kn+1
|δ(UN−n−1(y)) −δ( ˆUN−1,n+1(y))|
+ γ(˜γN−n−1(||u||w))

X\Kn+1
w(y)q(dy|x, a)
≤
sup
y∈Kn+1
γ(|UN−n−1(y) −ˆUN−1,n+1(y)|) + γ(˜γN−n−1(||u||w))ζ,
where the second inequality holds because
|δ(UN−n−1)| ≤γ(|UN−n−1|) ≤wγ(||UN−n−1||w)
≤wγ(˜γN−n−1(||u||w)) ∈Bw(X)
by Proposition 4 and Condition 1, and the last inequality holds by Condition 1
and (7).
Now
| ˆUN−1,n(x) −UN−n(x)|
≤max{ sup
b∈A(x)
inf
a∈B(x){LudA(a, b) + γ(LqϕN−n−1(Λ))dA(a, b)
+
sup
y∈Kn+1
γ(|UN−n−1(y) −ˆUN−1,n+1(y)|) + γ(˜γN−n−1(||u||w))ζ},
sup
a∈B(x)
inf
b∈A(x){LudA(a, b) + γ(LqϕN−n−1(Λ))dA(a, b)
+
sup
y∈Kn+1
γ(|UN−n−1(y) −ˆUN−1,n+1(y)|) + γ(˜γN−n−1(||u||w))ζ}}
= (Lu + γ(LqϕN−n−1(Λ)))ζA +
sup
y∈Kn+1
γ(|UN−n−1(y) −ˆUN−1,n+1(y)|)
+ γ(˜γN−n−1(||u||w))ζ
≤(Lu + γ(Lqϕ∞(Λ)))(1 + LA)ζA +
sup
y∈Kn+1
γ(|UN−n−1(y) −ˆUN−1,n+1(y)|)
+ γ(˜γ∞(||u||w))ζ.

Finite Approximations
237
Having recognized (Lu +γ(Lqϕ∞(Λ)))(1+LA) = Λ+ϕ(ϕ∞(Λ)) = ϕ∞(Λ) = LU
(recall (8) and (6), Proposition 1 and Lemma 3), we see now
| ˆUN−1,n(x) −UN−n(x)| ≤LUζA + γ(˜γ∞(||u||w))ζ
+
sup
y∈Kn+1
γ(|UN−n−1(y) −ˆUN−1,n+1(y)|)
= LUζA + γ(˜γ∞(||u||w))ζ + γ( sup
y∈Kn+1
|UN−n−1(y) −ˆUN−1,n+1(y)|) ∀x ∈Xn,
where the last equality holds because γ is increasing.
Next, we arbitrarily ﬁx some x ∈Kn and z = pKn
Xn(x) ∈Xn. Then
| ˆUN−1,n(x) −UN−n(x)| = | ˆUN−1,n(z) −UN−n(x)|
≤| ˆUN−1,n(z) −UN−n(z)| + |UN−n(z) −UN−n(x)|
≤LUζA + γ(˜γ∞(||u||w))ζ + γ( sup
y∈Kn+1
|UN−n−1(y) −ˆUN−1,n+1(y)|)
+ ϕ∞(Λ)dX(x, z)
≤ϕ∞(Λ)(ζA + ζX) + γ(˜γ∞(||u||w))ζ + γ( sup
y∈Kn+1
|UN−n−1(y) −ˆUN−1,n+1(y)|)
where the second inequality is by (12) and Lemma 3. Hence,
sup
x∈Kn
| ˆUN−1,n(x) −UN−n(x)| ≤ϕ∞(Λ)(ζA + ζX) + γ(˜γ∞(||u||w))ζ
+ γ( sup
y∈Kn+1
|UN−n−1(y) −ˆUN−1,n+1(y)|)
= ˜Λ + γ( sup
y∈Kn+1
|UN−n−1(y) −ˆUN−1,n+1(y)|)
≤˜Λ + γ( ˜Λ + γ( sup
y∈Kn+2
|UN−n−2(y) −ˆUN−1,n+2(y)|)),
and by iteration, we see from the sub-additivity of γ that
sup
x∈Kn
| ˆUN−1,n(x) −UN−n(x)|
≤γN−n( ˜Λ) + γ(N−n)( sup
x∈KN
| ˆUN−1,N(x) −U0(x)|) = γN−n( ˜Λ) ≤γ∞( ˜Λ)
with the last equality following from γ(0) = 0 and that γn( ˜Λ) increases in n. ⊓⊔
Corollary 1. For each N ≥1,
sup
x∈K0
| ˆUN−1,0(x) −U(x)| ≤γ∞( ˜Λ) + w(x)˜γ(N)(˜γ∞(||u||w)),
where ˜Λ := γ∞(Λ)(ζA + ζX) + γ(˜γ∞(||u||w))ζ.

238
F. Deng et al.
Proof. This follows from
| ˆUN−1,0(x) −U(x)| ≤| ˆUN−1,0(x) −UN(x)| + |UN(x) −U(x)|,
Lemma 4 and Proposition 4(c).
⊓⊔
Lemma 5. Let N ≥1, 0 ≤n ≤N −1 and x ∈Kn be ﬁxed. Then
U(x) −LUζX −2ζγ(˜γ∞(||u||w)) −2γ∞( ˜Λ) −2w(x)˜γ(N−n)(˜γ∞(||u||w))
≤u(x, f N
n (x)) +

Kn+1
δ(U(y))q(dy|x, f N
n (x)).
Proof. Let x ∈Kn and z = pKn
Xn(x) ∈Xn be ﬁxed.
Recall from Proposition 4 that
U(x) ≤UN−n(x) + w(x)˜γ(N−n)(˜γ∞(||u||w))
≤ˆUN−1,n(x) + γ∞( ˜Λ) + w(x)˜γ(N−n)(˜γ∞(||u||w))
= ˆUN−1,n(z) + γ∞( ˜Λ) + w(x)˜γ(N−n)(˜γ∞(||u||w)),
where the inequality is by Lemma 4 and the last equality is by the deﬁnition
of ˆUN−1,n(x) for x ∈Kn. For ˆUN−1,n(z), recall from (10) and the deﬁnition of
f N
n ,
ˆUN−1,n(z) = u(z, f N
n (z)) +

Kn+1
δ( ˆUN−1,n+1(y))q(dy|z, f N
n (z))
≤u(z, f N
n (z)) +

Kn+1
δ(UN−(n+1)(y) + γ∞( ˜Λ))q(dy|z, f N
n (z))
≤u(z, f N
n (z)) +

Kn+1
δ(UN−(n+1)(y))q(dy|z, f N
n (z))
+

Kn+1
γ(γ∞( ˜Λ))q(dy|z, f N
n (z))
≤u(z, f N
n (z)) +

Kn+1
δ(UN−(n+1)(y))q(dy|z, f N
n (z)) + γ∞( ˜Λ),
where the ﬁrst inequality is by Lemma 4, the second inequality is by the following
consequence of Condition 1(b, ii): |δ(x1+x2)−δ(x1)| ≤γ(|x2|) for all x1, x2 ∈R,
and the last inequality is by Condition 1(b,i). Now
U(x) ≤u(z, f N
n (z)) +

Kn+1
δ(UN−(n+1)(y))q(dy|z, f N
n (z)) + 2γ∞( ˜Λ)
+ w(x)˜γ(N−n)(˜γ∞(||u||w)),

Finite Approximations
239
and so
U(x) −2γ∞( ˜Λ) −w(x)˜γ(N−n)(˜γ∞(||u||w)) −
u(x, f N
n (x)) −u(z, f N
n (z))

−


X
δ(UN−n−1(y))q(dy|x, f N
n (x)) −

X
δ(UN−n−1(y))q(dy|z, f N
n (z))

≤u(z, f N
n (z)) +

Kn+1
δ(UN−(n+1)(y))q(dy|z, f N
n (z))
+ u(x, f N
n (x)) −u(z, f N
n (z))
+

X
δ(UN−n−1(y))q(dy|x, f N
n (x)) −

X
δ(UN−n−1(y))q(dy|z, f N
n (z))
= u(x, f N
n (x)) −

X\Kn+1
δ(UN−n−1(y))q(dy|z, f N
n (z))
+

X
δ(UN−n−1(y))q(dy|x, f N
n (x)).
(12)
Note that
u(x, f N
n (x)) −u(z, f N
n (z))

+


X
δ(UN−n−1(y))q(dy|x, f N
n (x)) −

X
δ(UN−n−1(y))q(dy|z, f N
n (z))

≤Lu(dX(x, z) + dA(f N
n (x), f N
n (z)))
+ γ(LqLUN−n−1)(dX(x, z) + dA(f N
n (x), f N
n (z)))
= (Lu + γ(LqLUN−n−1))(dX(x, z) +
inf
a∈A(x) dA(a, f N
n (z)))
≤(Lu + γ(LqLUN−n−1))(dX(x, z) + dH(A(x), B(z)))
≤(Lu + γ(LqLUN−n−1))(1 + LA)dX(x, z)
≤(Lu + γ(LqLUN−n−1))(1 + LA)ζX ≤(Lu(1 + LA) + (1 + LA)γ(LqLU))ζX
= (Λ + ϕ(ϕ∞(Λ)))ζX,
where the ﬁrst inequality is by Condition 2 applied to u and Condition 3 applied
to UN−n−1, which is Lipschitz and in Bw(X) by Lemma 3, the ﬁrst equality
holds by the deﬁnition of f N
n , the second inequality holds by the deﬁnition
of the Hausdorﬀmetric, the third inequality is by Condition 2 regarding the
multifunction A, the fourth inequality holds because of the deﬁnition of z, and
the ﬁfth inequality is by Lemma 3. That is, applying Proposition 1 to ϕ, we
recognize
u(x, f N
n (x)) −u(z, f N
n (z))

+


X
δ(UN−n−1(y))q(dy|x, f N
n (x)) −

X
δ(UN−n−1(y))q(dy|z, f N
n (z))

≤ϕ∞(Λ)ζX = LUζX.

240
F. Deng et al.
Consequently, from (12) we see
U(x) −2γ∞( ˜Λ) −w(x)˜γ(N−n)(˜γ∞(||u||w)) −LUζX
≤u(x, f N
n (x)) −

X\Kn+1
δ(UN−n−1(y))q(dy|z, f N
n (z))
+

Kn+1
δ(UN−n−1(y))q(dy|x, f N
n (x))
+

X\Kn+1
δ(UN−n−1(y))q(dy|x, f N
n (x))
≤u(x, f N
n (x)) + γ(˜γ∞(||u||w))

X\Kn+1
w(y)q(dy|z, f N
n (z))
+

Kn+1
δ(UN−n−1(y))q(dy|x, f N
n (x))
+ γ(˜γ∞(||u||w))

X\Kn+1
w(y)q(dy|x, f N
n (x))
≤u(x, f N
n (x)) + 2γ(˜γ∞(||u||w))ζX +

Kn+1
δ(UN−n−1(y))q(dy|x, f N
n (x))
≤u(x, f N
n (x)) + 2γ(˜γ∞(||u||w))ζX +

Kn+1
δ(U(y))q(dy|x, f N
n (x))
+

Kn+1
|δ(UN−n−1(y)) −δ(U(y))|q(dy|x, f N
n (x)),
where the third inequality is by (7), and the second inequality follows from the
calculation
δ(UN−n−1(y)) ≤γ(||UN−n−1||ww(y)) ≤w(y)γ(||UN−n−1||w)
≤w(y)γ(˜γ∞(||u||w))
by Proposition 4. That is,
U(x) −2γ∞( ˜Λ) −2γ(˜γ∞(||u||w))ζX −w(x)˜γ(N−n)(˜γ∞(||u||w)) −LUζX
≤u(x, f N
n (x)) +

Kn+1
δ(U(y))q(dy|x, f N
n (x))
+

Kn+1
|δ(UN−n−1(y)) −δ(U(y))|q(dy|x, f N
n (x))
≤u(x, f N
n (x)) +

Kn+1
δ(U(y))q(dy|x, f N
n (x))
+

Kn+1
γ(|(UN−n−1(y)) −U(y)|)q(dy|x, f N
n (x))

Finite Approximations
241
≤u(x, f N
n (x)) +

Kn+1
δ(U(y))q(dy|x, f N
n (x))
+

X
γ(˜γ(N−n−1)(˜γ∞(||u||w))w(y))q(dy|x, f N
n (x))
≤u(x, f N
n (x)) +

Kn+1
δ(U(y))q(dy|x, f N
n (x))
+ γ(˜γ(N−n−1)(˜γ∞(||u||w)))

X
w(y)q(dy|x, f N
n (x))
≤u(x, f N
n (x)) +

Kn+1
δ(U(y))q(dy|x, f N
n (x))
+ w(x)˜γ(N−n)(˜γ∞(||u||w)),
where the last two inequalities hold by Condition 1. Now the statement
follows.
⊓⊔
In the next statement, let the function ¯U on X be deﬁned by
¯U(x) := U(x) ∀x ∈

n≥0
Kn, ¯U(x) := −w(x)˜γ∞(||u||w) ∀x ∈X \

n≥0
Kn.
Lemma 6. For each N ≥1,
¯U(x) −(LUζX + 3ζγ(˜γ∞(||u||w)) + 2γ∞( ˜Λ)) −2w(x)˜γ(N)(˜γ∞(||u||w))
≤u(x, f N(x)) +

X
¯U(y)q(dy|x, f N(x)) ∀x ∈X.
Proof. Note that | ¯U(x)| ≤w(x)˜γ∞(||u||w) for all x ∈X, according to Proposi-
tion 4, and consequently ¯U ∈Bw(X).
For x ∈X \ 
n≥0 Kn = C∞, it holds that
u(x, f N(x)) +

X
δ( ¯U(y))q(dy|x, f N(x))
≥−||u||ww(x) −

X
γ(˜γ∞(||u||w))w(y)q(dy|x, f N(x))
≥−||u||ww(x) −˜γ(˜γ∞(||u||w))w(x) = −w(x){||u||w + ˜γ(˜γ∞(||u||w))}
= −w(x)˜γ∞(||u||w) = ¯U(x),
where the second inequality is by Condition 1, and the last inequality is by
Proposition 1. Therefore, the claimed relation in the lemma holds for x ∈
X\ 
n≥0 Kn = C∞.
Now let x ∈Cn be ﬁxed for some n ∈{0, 1, . . . }. Since Cn ⊆Kn, f N(x) =
f N+n
n
(x), and we have from the deﬁnition of ¯U and Lemma 5 with N + n in lieu

242
F. Deng et al.
of N therein that
¯U(x) −LUζX −2ζγ(˜γ∞(||u||w)) −2γ∞( ˜Λ) −2w(x)˜γ(N)(˜γ∞(||u||w))
≤u(x, f N(x)) +

Kn+1
δ( ¯U(y))q(dy|x, f N(x))
= u(x, f N(x)) +

Kn+1
δ( ¯U(y))q(dy|x, f N(x)) −

X
δ( ¯U(y))q(dy|x, f N(x))
+

X
δ( ¯U(y))q(dy|x, f N(x)),
and so
¯U(x) −LUζX −2ζγ(˜γ∞(||u||w)) −2γ∞( ˜Λ) −2w(x)˜γ(N)(˜γ∞(||u||w))
−

X\Kn+1
|δ( ¯U(y))|q(dy|x, f N(x)) ≤u(x, f N(x)) +

X
δ( ¯U(y))q(dy|x, f N(x)).
Observe that on the left hand side of the above inequality,

X\Kn+1
|δ( ¯U(y))|q(dy|x, f N(x))
≤γ(˜γ∞(||u||w))

X\Kn+1
w(y)q(dy|x, f N(x)) ≤ζγ(˜γ∞(||u||w)).
Now the statement follows.
⊓⊔
Lemma 7. If for some stationary policy f and constants R, Q ∈[0, ∞),
¯U(x) ≤u(x, f(x)) +

X
δ( ¯U(y))q(dy|x, f(x)) + R + Qw(x) ∀x ∈X,
then
¯U(x) ≤T n
f ¯U(x) + γn(R) + ˜γn(Q)w(x) ∀x ∈X
for all n ≥1.
Proof. Let x ∈X be ﬁxed, and we prove the statement by induction, as follows.
When n = 1, the claimed relation holds because γ1(R) = R and ˜γ1(Q) = Q.
Assume the claimed relation holds for n. Then
U(x) ≤Tf ¯U(x) + R + Qw(x)
≤Tf(T n
f ¯U + γn(R) + ˜γn(Q)w)(x) + R + Qw(x)
= u(x, f(x)) +

X
δ(T n
f ¯U(y) + γn(R) + ˜γn(Q)w(y))q(dy|x, f(x)) + R + Qw(x)

Finite Approximations
243
≤u(x, f(x)) +

X
(δ(T n
f ¯U(y)) + γ(γn(R) + ˜γn(Q)w(y)))q(dy|x, f(x))
+ R + Qw(x)
≤u(x, f(x)) +

X
δ(T n
f ¯U(y))q(dy|x, f(x)) +

X
(γ(γn(R))
+ γ(˜γn(Q)w(y)))q(dy|x, f(x)) + R + Qw(x)
≤u(x, f(x)) +

X
δ(T n
f ¯U(y))q(dy|x, f(x)) + (R + γ(γn(R)))
+ (Q + ˜γ(˜γn(Q)))w(x),
where the second inequality is by the inductive supposition, the third, fourth
and ﬁfth inequalities are all by Condition 1. That is, by (3) applied to γ and ˜γ,
U(x) ≤T n+1
f
¯U(x) + γn+1(R) + ˜γn+1(Q)w(x),
as required.
⊓⊔
Proof of Theorem 1. Lemma 6 asserts that
¯U(x) ≤u(x, f N(x)) +

X
δ( ¯U(y))q(dy|x, f N(x)) + R + Qw(x) ∀x ∈X
with
R = (LUζX + 3ζγ(˜γ∞(||u||w)) + 2γ∞( ˜Λ)), Q = 2˜γ(N)(˜γ∞(||u||w)).
By Lemma 7, for each x ∈
n≥0 Kn,
U(x) = U(x) ≤lim
n→∞

T n+1
f N
¯U(x) + γn+1(R) + ˜γn+1(Q)w(x)

= U f N (x) + γ∞(R) + ˜γ∞(Q)w(x),
where the ﬁrst equality is by the deﬁnition of ¯U, and the last equality is by
Lemma 1 and Proposition 1. The statement follows now because U f N (x) ≤U(x)
for each x ∈X.
⊓⊔
5.2
Proof of Theorem 2
We now sketch the proof of Theorem 2.
Proof of Theorem 2. One can show that
|Tv(x) −Tv(y)| ≤(Lu + LqLv)(1 + LA)dX(x, z) ∀x, z ∈X,
Un has a Lipschitz constant L′
Un := ϕ′
n(Λ), and
sup
x∈Kn
| ˆUN−1,n(x) −UN−n(x)| ≤γN−n( ˜ΛN) ∀N ≥1, 0 ≤n ≤N.
(13)

244
F. Deng et al.
The above relations correspond to and can be established as in Lemma 2, Lemma
3(a) and Lemma 4, and ϕ′ and ˜ΛN correspond to ϕ and ˜Λ.
Let 0 ≤n ≤N −1 be ﬁxed, and consider some x ∈Kn for now. Let
z = pKn
Zn (x). Then ˆUN−1,n(x) = ˆUN−1,n(z), and
U gN
N−n(x) −ˆUN−1,n(x)
= u(x, f N
n (x)) +

X
δ(U gN
N−(n+1)(y))q(dy|x, f N
n (x)) −u(z, f N
n (z))
−

Kn+1
δ( ˆUN−1,n+1(y))q(dy|z, f N
n (z))
= u(x, f N
n (x)) +

X
δ(UN−(n+1)(y))q(dy|x, f N
n (x))
−

X
δ(UN−(n+1)(y))q(dy|x, f N
n (x))
+

X
δ(U gN
N−(n+1)(y))q(dy|x, f N
n (x)) −u(z, f N
n (z))
−

Kn+1
δ( ˆUN−1,n+1(y))q(dy|z, f N
n (z))
+

Kn+1
δ(UN−1,n+1(y))q(dy|z, f N
n (z))
−

X
δ(UN−1,n+1(y))q(dy|z, f N
n (z))
+

X\Kn+1
δ(UN−1,n+1(y))q(dy|z, f N
n (z)).
Consequently,
|U gN
N−n(x) −ˆUN−1,n(x)|
≤|u(x, f N
n (x)) −u(z, f N
n (z))|
+


X
δ(UN−(n+1)(y))q(dy|x, f N
n (x)) −

X
δ(UN−1,n+1(y))q(dy|z, f N
n (z))

+

X
δ(U gN
N−(n+1)(y)) −δ(UN−(n+1)(y))
 q(dy|x, f N
n (x))
+

Kn+1
δ( ˆUN−1,n+1(y)) −δ(UN−1,n+1(y))
 q(dy|z, f N
n (z))
+

X\Kn+1
|δ(UN−1,n+1(y))|q(dy|z, f N
n (z)),

Finite Approximations
245
where
X
δ(U gN
N−(n+1)(y)) −δ(UN−(n+1)(y))
 q(dy|x, f N
n (x))
=

X\Kn+1
δ(U gN
N−(n+1)(y)) −δ(UN−(n+1)(y))
 q(dy|x, f N
n (x))
+

Kn+1
δ(U gN
N−(n+1)(y)) −δ(UN−(n+1)(y))
 q(dy|x, f N
n (x))
≤γ(||U gN
N−(n+1) −UN−(n+1)||w)ζ + γ( sup
x∈Kn+1
|UN−(n+1)(x) −U gN
N−(n+1)(x)|)
by (7). Applying Condition 2 to u and Condition 4, we see
|U gN
N−n(x) −ˆUN−1,n(x)|
≤Lu(1 + LA)ζX + LqL′
UN−(n+1)(dX(x, z) + dA(f N
n (x), f N
n (z)))
+ γ(||U gN
N−(n+1) −UN−(n+1)||w)ζ + γ( sup
x∈Kn+1
|UN−(n+1)(x) −U gN
N−(n+1)(x)|)
+γ( sup
x∈Kn+1
| ˆUN−(n+1)(y) −UN−(n+1)(y)|) + γ(||UN−(n+1)||w)ζ
≤ζX(Lu + LqL′
UN−(n+1))(1 + LA) + ζ(γ(||U gN
N−(n+1) −UN−(n+1)||w)
+ γ(||UN−(n+1)||w))
+ γ(γN−(n+1)( ˜ΛN)) + γ( sup
x∈Kn+1
|UN−(n+1)(x) −U gN
N−(n+1)(x)|)
≤ϕ′
N−n(Λ)ζX + 3ζ˜γN(||u||w) + γN( ˜ΛN)
+ γ( sup
x∈Kn+1
|UN−(n+1)(x) −U gN
N−(n+1)(x)|),
where the second inequality is by (13), and the third inequality is by
Proposition 2.
Now the previous inequality and (13) imply
sup
x∈Kn
|U gN
N−n(x) −UN−n(x)|
≤sup
x∈Kn
|UN−n(x) −ˆUN−1,n(x)| + sup
x∈Kn
| ˆUN−1,n(x) −U gN
N−n(x)|
≤ϕ′
N(Λ)ζX + 3ζ˜γN(||u||w) + 2γN( ˜ΛN)
+ γ( sup
x∈Kn+1
|UN−(n+1)(x) −U gN
N−(n+1)(x)|)
= G + γ( sup
x∈Kn+1
|UN−(n+1)(x) −U gN
N−(n+1)(x)|),
and by iteration, we see
sup
x∈Kn
|U gN
N−n(x) −UN−n(x)| ≤γN−n(G) ≤γN(G).

246
F. Deng et al.
Finally,
sup
x∈K0
|U gN (x) −U(x)|
≤sup
x∈K0
|U gN (x) −U gN
N (x)| + sup
x∈K0
|U gN
N (x) −UN(x)| + sup
x∈K0
|UN(x) −U(x)|
≤2˜γ(N)(˜γ∞(||u||w)) sup
x∈K0
w(x) + γN(G),
where the last inequality is by Propositions 3 and 4.
⊓⊔
References
1. Aliprantis, C., Border, K.: Inﬁnite Dimensional Analysis. Springer, Heidelberg
(2006)
2. Altman, E.: Constrained Markov Decision Processes. Chapman and Hall/CRC,
Boca Raton (1999)
3. B¨auerle, N., Ja´skiewicz, A., Nowak, A.: Stochastic dynamic programming with non-
linear discounting. Appl. Math. Optim. (2020). https://doi.org/10.1007/s00245-
020-09731-x
4. Bertsekas, D.: Convergence of discretization procedures in dynamic programming.
IEEE Trans. Autom. Control 20, 415–419 (1975)
5. Cioletti, L., Oliveira, E.: Applications of variable discounting dynamic program-
ming to iterated function systems and related problems. Nonlinearity 32, 853–883
(2019)
6. Dufour, F., Prieto-Rumeau, T.: Approximation of inﬁnite horizon discounted
cost Markov decision processes. In: Hern´andez-Hern´andez, D., Minj´arez-Sosa, J.
(eds.) Optimization, Control, and Applications of Stochastic Systems, pp. 59–76.
Birkh¨auser, Boston (2012)
7. Dufour, F., Prieto-Rumeau, T.: Approximation of Markov decision processes with
general state space. J. Math. Anal. Appl. 388, 1254–1267 (2012)
8. Feinberg, E.A., Kasyanov, P., Zadoianchuk, N.: Berge’s theorem for noncompact
image sets. J. Math. Anal. Appl. 397, 255–259 (2013)
9. Hern´andez-Lerma, O., Lasserre, J.: Discrete-Time Markov Control Processes.
Springer, New York (1996)
10. Ja´skiewicz, A., Matkowski, J., Nowak, A.: Persistently optimal policies in stochastic
dynamic programming with generalized discounting. Math. Oper. Res. 38, 108–121
(2013)
11. Ja´skiewicz, A., Matkowski, J., Nowak, A.: On variable discounting in dynamic
programming: applications to resource extraction and other economic models. Ann.
Oper. Res. 220, 263–278 (2014)
12. Ja´skiewicz, A., Matkowski, J., Nowak, A.: Generalized discounting in dynamic
programming with unbounded returns. Oper. Res. Lett. 42, 231–233 (2014)
13. Kuntz, J., Thomas, P., Stan, G., Barahona, M.: Approximations of countably-
inﬁnite linear programs over bounded measure spaces. SIAM J. Optim. (2020).
Preprint available via arXiv:1810.03658v3
14. Piunovskiy, A., Zhang, Y.: Continuous-Time Markov Decision Processes. Springer,
Cham (2020)

Finite Approximations
247
15. Puterman, M.: Markov Decision Processes. Wiley, New York (1994)
16. Saldi, N., Linder, T., Y¨uksel, S.: Finite Approximations in Discrete-Time Stochas-
tic Control. Springer, Cham (2018)
17. Sennott, L.: Stochastic Dynamic Programming and the Control of Queueing Sys-
tems. Wiley, New York (1999)

Locks, Bombs and Testing: The Case
of Independent Locks
Li Liu(B) and Isaac M. Sonin
University of North Carolina at Charlotte, Charlotte, NC 28223, USA
{lliu26,imsonin}@uncc.edu
https://webpages.uncc.edu/imsonin/
Abstract. We present a Defense/Attack resource allocation model,
where Defender has some number of “locks” to protect n vulnerable
boxes (sites), and Attacker is trying to destroy these boxes, having m
“bombs,” which can be placed into boxes. Similar models were studied
in game theory - (Colonel) Blotto games, but our model has a feature
absent in previous literature. Attackers test the vulnerability of all sites
before allocating their resources, and these tests are not perfect, i.e., a
test can give plus for a box without a lock and minus for a box with
a lock. We describe the optimal strategies for a version of this Locks-
Bombs-Testing (LBT) model when locks appear independently in each
box with the same probability.
Keywords: Defense/attack model · Blotto game · Search · Testing
AMS(2020) Subject Classiﬁcation: Primary 91A27 · Secondary
90B40
1
Introduction
The problem of allocation of limited resources between diﬀerent tasks is a clas-
sical problem in many areas of Operations Research, Economics, Finance and
Engineering. This problem with a few players (participants) is an important
ﬁeld in Game Theory. In a classical Blotto game, two players distribute limited
resources between diﬀerent sites (battleﬁelds) with the goal to win more sites,
winning a site if you have more resources on this site than your opponent. There
is substantial literature on this topic, with classic paper [13] and more recent
publications, such as [12], where a complete solution of the “continuous” version
was given, as well as [4], where some interesting extensions are discussed. In a
comprehensive and detailed survey [5] dedicated to Search Games, the Blotto
game is classiﬁed as an attack-defense game. There are even more papers dedi-
cated to these games and as in all of Operations Research all classiﬁcations have
many overlapping parts. As an example of an important paper on an attack-
defense game we mention [11].
c
⃝The Author(s), under exclusive license to Springer Nature Switzerland AG 2021
A. Piunovskiy and Y. Zhang (Eds.): Modern Trends in Controlled Stochastic Processes,
ECC 41, pp. 248–265, 2021. https://doi.org/10.1007/978-3-030-76928-4_12

Locks, Bombs and Testing
249
The inspiration for the model in this paper and some basic ideas can be
traced to the paper by K. Sonin and A. Wright [14], where they provided a
model of intelligence gathering in combat and used highly detailed data about
Afghan rebel attacks, insurgent-led spy networks, and counterinsurgency opera-
tions. This theoretical model was a novel version of the Colonel Blotto’s game.
First, the government allocates its scarce defense resources across possible tar-
gets. Then, each target is independently tested for vulnerability. Finally, the
rebels base their choice of the targets on the results of these tests. Empirically,
the paper demonstrated a robust link between local economic conditions and the
patterns of rebel attacks.
A more general and abstract mathematical model called the Locks, Bombs
and Testing (LBT) model was described in [15,16], where one important special
case was solved. The solution for the other important case was obtained in the
PhD thesis of Liu Li [8]. This thesis in a modiﬁed form is a substantial part of
this paper.
First, we describe a Symmetrical LBT model, where all boxes are identical. As
in most attack-defense games, the two players play quite diﬀerent roles. We call
one of them Defender (DF) and the other, Attacker (AT). There are n “boxes”
(sites, battleﬁelds, cells, targets, time slots, etc.) with equal values for both players.
AT is trying to destroy these boxes by placing “bombs” that can result in explo-
sions (destructions). One or more bombs can be placed into the same box. AT has
m, m = 1, 2, ..., bombs to allocate among n boxes. A box is destroyed if at least
one explosion occurs, and the explosions of diﬀerent bombs in the same site or in
diﬀerent sites are independent. We denote by p the probability of explosion.
DF is trying to protect the boxes by distributing “locks” among them. A
lock is a protection device which, when placed in a box, prevents its destruction
with any number of bombs in it. Obviously, locks and bombs are just the names
of discrete units of resources of protection and destruction. The number of locks
k, k < n, can be ﬁxed, in which case it is an A(n, k) problem, the subject of
paper [16], or it can be a random variable obtained when a lock appears in site
i with probability λ independently of other boxes, in which case it is a B(n, λ)
problem, the main subject of this paper. The latter assumption can represent
either the uncertainty of DF about resources that will be available to her or the
uncertainty of AT about how many locks will be distributed.
The important feature of both models in contrast to classical Blotto games
is that AT can and will test every box, trying to ﬁnd boxes without locks.
This testing is not perfect: a test of site i may have a positive result, Si = 1,
even if there is no lock at the site, Ti = 0, and negative, Si = 0, even if there
is a lock, Ti = 1. The probabilities of correct identiﬁcation of both types, in
statistical language the sensitivity and speciﬁcity, P(Si = 1|Ti = 1) = a and
P(Si = 0|Ti = 0) = b, are known to both players. The result of testing is a
vector of signals s = (s1, ..., sn), where each si = 0, 1 is known to AT. Hereafter
we refer to this vector as signal s.
When the number of available locks k, 0 ≤k ≤n, became known to DF, then
her strategy is a probability distribution bk(γ) on a set of all possible positions

250
L. Liu and I. M. Sonin
of locks {γ}, where γ = (i1, i2, ..., ik) with 1 ≤i1 < . . . < ik ≤n. The case
k = 0 means that no locks are allocated. The collection of bk(γ), k = 0, 1, 2, ..., n
deﬁnes the strategy of DF, hereafter b(γ).
In our Bayesian setting we assume that the parameter k in problem A or
parameter λ specifying the distribution of the random number of locks K, and
prior distribution b(γ) are known to AT, although the positions of locks and
their actual number k in problem B are not. After the locks are allocated by
DF, AT tests all boxes, receives signal s, and then, using prior distribution
b(γ) and the probabilities of signals p(s) ≡p(s|b(γ)), calculates the aposterior
distribution of the positions of locks (ADL) b(γ|s). Then for each signal s and
each m, AT solves the problem of optimal allocation of m bombs uopt(s|b(γ)) =
(u1(s), ..., un(s))|b(γ)), 
i ui = m, trying to maximize the expected number of
destroyed sites.
Note that our model for both problems has one special and important feature.
AT’s analysis and the solution consists of two parts. In the ﬁrst part AT considers
a statistical problem to ﬁnd the posterior distribution of locks, given signal s and
prior information. In this statistical problem the probability of explosion p and
the bombs allocation do not participate at all.
The second part is to optimize the allocation of m bombs among n sites.
Such allocation can be deterministic or use some randomization. WLOG, we
can assume that the allocation of bombs is deterministic and an optimal strat-
egy of AT πopt(b(γ)), with respect to the strategy of DF b(γ), is a collection of
her optimal deterministic responses uopt(s|b(γ)) ≡uopt(s) = (u1(s), ..., un(s)) to
each signal s, where ui(s) is the number of bombs placed into site i, i = 1, ..., n,

i ui(s) = m. This optimal strategy πopt(b(γ)) together with the prior distribu-
tion b(γ) results in the corresponding total expected damage (loss), Lopt(b(γ).
The goal of DF is to select a prior distribution of locks b∗(γ) to minimize this
loss. We assume that DF knows the parameters of testing a and b and the number
of bombs m. Then the pair (b∗(γ), π∗), where π∗= πopt(b∗(γ)) is an optimal
strategy of AT with respect to strategy b∗(γ), forms a classical Nash equilibrium
(NE) point. The corresponding value of the game is v∗= L(b∗(γ), π∗)). Though
b∗(γ) are not unique, they all have common properties that result in a unique
(up to some randomization) AT strategy π∗, and thus a speciﬁc value of v∗.
We call this game the symmetrical LBT game (model) (S-LBT game) A(n, k)
or B(n, λ) with parameters (n, k, m, a, b), or correspondingly (n, λ, m, a, b), where
n is the number of sites, k is the ﬁxed number of locks, and λ is the probability
of a lock being present in the box.
In a more general setting parameter λ can be replaced by vector Λ =
(λ1, λ2, ...λn), where λi is the probability of presence of a lock in box i, param-
eters a and b are replaced by vectors a = (ai), 1 ≤i ≤n and b = (bi), 1 ≤i ≤n,
and parameter ci = 1 by an n-dimensional vector c = (ci), 1 ≤i ≤n, where vec-
tors a, b, c represent the sensitivities and the speciﬁcities of testing, and the values
of all sites, i.e. P(Si = 1|Ti = 1) = ai and P(Si = 0|Ti = 0) = bi, 1 ≤i ≤n.
Hereafter we also refer to the general model as the general LBT model. The
additional justiﬁcation to limit the consideration to the symmetrical case is the

Locks, Bombs and Testing
251
following. The general LBT game and the straightforward approach to solving
it, described above, have two basic drawbacks. First, the set of possible positions
for locks, i.e., the set of subsets of an n element set, generally has order 2n, and
so does the set of potential signals. As a result, the calculations of posterior
distributions b(γ|s) and their marginal distributions αi(s) = P(Ti = 0|s) which
play a crucial role in the description of optimal strategies, become cumbersome
for large n. The second problem is that the knowledge of detailed information
about the values of ci, ai, and bi in many cases is unrealistic. As a result, the
main focus in [8,16] was on the analysis of a simpler S-LBT model, where all
sites have identical values ci = 1, and all ai = a, bi = b.
The main goal of our paper is to present the complete solution of the B(n, λ)
S-LBT game. One of our main results about S-LBT is that the optimal strategy
of AT π(·|m, s) depends only on probability of an explosion p, value x of rv N,
the number of minuses in a signal, and the ratio r ≡rB = P (T =0|S=0)
P (T =0|S=1). A similar
statement is true for problem A with the ratio r(x) ≡rA(x) = P (T =0|S=0,x)
P (T =0|S=1,x).
Both ratios depend on the parameters of the model, n, k, λ, a, b.
When the parameters of sensitivity a and speciﬁcity b are “informative”,
these ratios are more than one. We show later that “informative” means that
a + b > 1. This immediately implies that if there is only one bomb and signal
s has pluses and minuses then a bomb goes to a minus box. When the number
of bombs m exceeds x, optimal strategies can be expressed through rB, rA(x),
and other parameters.
As a result, the optimal strategy in both problems will have a much simpler
structure than in the general case, symmetrical with respect to all sites with
minus signals, and correspondingly for sites with plus signals. We describe this
strategy on a heuristic level immediately.
The optimal strategy in both problems depends on the number m of bombs
available and, given N = x, 0 ≤x ≤n, has the following structure. Initially,
all bombs are placed one by one into each of x minus boxes until the threshold
level d, dA(x) in Problem A or level dB in Problem B is reached in each of them
or the bombs are exhausted. Afterwards, the bombs are added one by one to
plus boxes until there is a bomb in each of them. Then, bombs are added one
by one into minus boxes until each of these boxes has d + 1 bombs in each of
them, then back to plus boxes until each has at least 2 bombs, etc. This “ﬁll and
switch” process stops when AT runs out of bombs. We will call such a strategy
a d-uniform as possible strategy (hereafter, a “d-UAP strategy”). If x = 0 or
n, then all boxes are simply ﬁlled sequentially, and this is a 0-UAP strategy.
The outcome of this process will be an allocation in which either all plus boxes
will have the same number of bombs in each of them, and then all minus boxes
either also have the same number of bombs in each of them, or some minus
boxes have one extra bomb in comparison with the other minus boxes. A similar
symmetrical situation takes place when all minus boxes have the same number
of bombs in each of them. If all boxes with plus signals have no bombs, then
the number of bombs in minus boxes does not exceed d. Note that though in
problem B the ratio rB and the threshold value dB do not depend on the number

252
L. Liu and I. M. Sonin
of minus boxes x, the allocation of bombs and then the value function for each
signal do depend on that parameter. In a sense, the values N = x and rB(λ)
(rA(x)) play the role of suﬃcient statistics in the optimization problem. The
value of the threshold dB represents the “advantage level” of a minus box over
a plus box. A similar interpretation can be given to the threshold dA(x) given
that x minuses were observed.
In an example with n = 5, x = 3, m = 12, and d = 3, the 3-UAP strategy is
to place 3 bombs into 2 minus boxes, 4 into the third minus box, and 1 bomb
into each of the 2 plus boxes. When d = 2, each of the minus boxes has 3 bombs,
1 plus box has 1, and the second plus box has 2.
Though the solutions of both problems have certain similarities, some of their
features are very distinct. For example, an interesting and even counter intuitive
property is that in the problem A(n, k), the function rA(x) and therefore the
optimal strategy and the value function, depend on a and b only through the
value c =
a
1 −a
b
1 −b, a combined characteristic of the quality of testing. In the
problem B(n, λ), this property does not hold with respect to the value rB(λ).
The other important distinction between these two problems is that in the former
problem the minuses and pluses in diﬀerent boxes are not independent, but in
the latter problem they are.
Note that sites, locks, bombs and testing in this and more general models are
rather abstract terms and may have very diﬀerent interpretations beyond our
initial exposition of the DF and AT defense-attack game. We refer to [14,15] for
a more detailed exposition. It is easy to extend the LBT model in many diﬀerent
directions. Here we mention only that the dynamic version of the LBT model will
have common features and in a sense will be a very broad generalization of the
well-known model in Applied Probability—the Multi Armed Bandit problems.
This model was studied in many papers and a few books—D. Berry and B.
Fristedt (1985), E. Presman and I. Sonin (1987, 1990), J. Gittins (1989), J.
Gittins, K. Glazebrook and R. Weber (2011), and the current internet version
by T. Lattimore and C. Szepesvari (2019). The full solution of the general LBT
game is a diﬃcult task though some special cases were presented in [15].
The structure of our paper is as follows. In Sect. 2 we present some prelim-
inary formulas and auxiliary results. In Sect. 3 we obtain optimal strategies for
problem B, and in Sect. 4 we consider corresponding examples.
We thank Michael Grabchak, Ernst Presman, Mark Whitmeyer and Fedor
Sandomirsky for their valuable remarks and helpful discussions, and patience
with reading numerous drafts.
2
Preliminary Formulas and Auxiliary Results
Though the main focus of our paper is on the problem B ≡B(n, λ), we also pro-
vide for the comparison some details from [16] about model A ≡A(n, k). Also
of possible interest is to compare the strategies and the value functions for both

Locks, Bombs and Testing
253
problems for the “matching” values of k and λ, i.e. when the expected number of
locks in n boxes is the same, λn = k. We present some numerical results in Sect. 4.
The following notation is used throughout the paper. Deﬁne random variable
Ti, Si, Ci, each taking two values 0 and 1: Ti = 1 if and only if the ith box is pro-
tected; Si = 1 if and only if the ith box test is positive, i.e. the protection is present,
(si = 1), and Ci = 1 if and only if the ith box is destroyed. The absence of subindex
i means that the formula applies to any box. Our assumptions imply the following
basic equations:
P(Si = 1 Ti = 1) = a,
P(Si = 0 Ti = 0) = b,
P(Ci = 1 Ti = 1) = 0,
P(Ci = 1 Ti = 0, ui) = p(ui),
(1)
where ui is the number of bombs in box i, and p(u) is the success function, the
probability of at least one explosion in a box with u bombs. As we assumed that
the success is independent across bombs, p(u) = 1 −(1 −p)u. The function p(u)
is increasing and upward concave, and the function Δp(u) ≡p(u + 1) −p(u) is
decreasing. The diminishing eﬀect of each extra bomb will play an important role
in determining the optimal strategy.
2.1
Basic Notation and Lemma 1
Of possible interest in both models are the aposterior probabilities P(Ti
=
0|s), s = (s1, ..., sn) and the aposterior distribution of locks (ADL) b(γ|s) =
P(Ti = 1, i ∈γ, Ti = 0, i /∈γ|Si = si, i = 1, ..., n).
To describe these distributions, given that the number of locks k is ﬁxed, let
us introduce rvs N1, the number of minuses in locked boxes, i.e. the number of
false minuses, or equivalently, the number of locks in boxes with minuses, N2, the
number of minuses in unlocked boxes, i.e. the number of correct minuses, and N =
N1 + N2, the total number of minuses after testing. The rv N1 is a binomial rv
with k trials and probability of success 1 −a, the rv N2 is a binomial rv with n −k
trials and probability of success b. These two random variables are independent,
and unless b ̸= 1−a, rv N = N1 +N2, taking values 0, 1, ..., n, is not a binomial rv.
Sometimes the distribution of N is called the Poisson binomial distribution. The
signal s = (s1, ..., sn) and the value N = x are observable in contrast to the values
of N1 and N2, which are not. To stress this point, sometimes we use the notation
t = N1(γ, s), x = N(s).
We denote by pi(j) the pmf (probability mass function) of the binomial rvs
Ni, i = 1, 2 and p(j|r, p), j = 0, 1, ..., r, the pmf of a binomial distribution with r
trials and probability of success p. Thus p1(j) = p(j|k, 1 −a) and p2(j) = p(j|n −
k, b). Then the pmf of rv N in problem A, gA(x) ≡gn,k(x), 0 ≤x ≤n, can be
calculated by standard discrete convolution formula, the ﬁrst formula below.
In problem B the number of locks is rv K with a binomial distribution with
n trials and probability of success λ. Thus rv K has distribution p(k|n, λ), k =
0, 1, ..., n. Given K = k, rv N has conditional distribution gn,k(x), and then
gB(x) ≡P(N = x) can be calculated by the second formula below

254
L. Liu and I. M. Sonin
gA(x) ≡gn,k(x) =

j p1(j)p2(x −j) ≡

t p1(x −t)p2(t),
gB(x) =
n
k=0 p(k|n, λ)gn,k(x).
(2)
The summation over j in the convolution formula above is taken over values j
such that 0 ≤j ≤k, 0 ≤x −j ≤n −k. Similar holds for the summation over t,
where 0 ≤x −t ≤k, 0 ≤t ≤n −k. Further in all convolution formulas we may
omit the exact range of summation assuming that all probabilities involved in the
sums are well deﬁned.
The distribution of locks and the testing may be viewed as a two-stage ran-
dom experiment with outcomes represented by pairs (γ, s), where γk ≡γ =
(i1, i2, ..., ik) with 1 ≤i1 < . . . < ik ≤n is a (vector) allocation of k locks and
s = (s1, ..., sn) is a (vector) signal about boxes vulnerability. In Problem A k is a
ﬁxed number, and in Problem B, 0 ≤k ≤n is a result of a binomial experiment.
The probability of each outcome is P(γ, s) = b(γ)P(s|γ), where b(γ) is prior distri-
bution of locks, and P(s|γ) = P(S1 = s1, ..., Sn = sn|γ). Given γ = (i1, i2, ..., ik)
and the prior distributions of locks, AT, using Bayes’ formula can obtain the pos-
terior distributions of locks bk(γ|s) = P(Ti = 1, i ∈γ, Ti = 0, i /∈γ|s). The
collections of these probabilities for diﬀerent signals s and k in Problem B give
b(γ) and b(γ|s).
In our model, DF has no information about the allocation of bombs by AT.
Therefore, her strategy in a Nash equilibrium point is straightforward: distribute
k available locks between n boxes uniformly, i.e. the prior distribution bk(γ) is uni-
form. In statistical physics, this distribution is called the Fermi-Dirac statistics:
any combination of k protected boxes has the same probability 1/
n
k

. It is easy to
see that the probability of protection for each individual box is t = k
n. The similar
probability for Problem B is λ. The substantial diﬀerence between models is that
the rvs Ti are independent in B but not in A.
Thus, our main interest is in AT’s strategy. Given signal s and prior distribu-
tion of locks b(γ), AT can obtain aposterior distribution of locks (ADL) b(γ|s).
To construct an optimal allocation for each signal s, AT has to obtain values
P(Ti = 0|si = 0, s−i) and P(Ti = 0|si = 1, s−i) and their ratios, where s−i is
an n −1-dimensional vector s = (s1, ..., sn) without coordinate si. In both prob-
lems the symmetry of minus and plus boxes gives a hint that for S-LBT the only
information necessary besides the signal in a particular box is the total number of
minuses. To justify this assertion we need a few results.
We start with the following lemma with two intuitively appealing observations.
First, the posterior probability of signal distribution is uniform conditional on the
number x of minus signals, and second, the posterior probability that box i has no
lock conditional on the full vector signal s = (s1, ..., sn) is equal to the conditional
probability that box i has no lock conditional only on the individual signal si and
the total number of minus signals.

Locks, Bombs and Testing
255
Lemma 1. a) For problems A and B, for any signal s and any x = 0, 1, ..., n
P(s|N = x) = 1/
n
x

.
(3)
b) For problem B, for any signal s and any x = 0, 1, ..., n
P(Ti = 0|s, N = x) = P(Ti = 0|si).
(4)
c) For problem A, for any signal s and any x = 0, 1, ..., n
P(Ti = 0|s, N = x) = P(Ti = 0|si, N = x),
(5)
Proof. a) The symmetry of signals and boxes implies that P(s|N = x) = c(x),
i.e. all signals with the same number x of signals s = 0 have the same prob-
ability. Let Σ(x) = {s : N(s) = x}. Then, since |Σ(x)| =
n
x

and

s∈Σ(x) P(s|N = x) = 1, we obtain that P(s|N = x) is given by the equality
in (3).
b) For Problem B, the equality in (4) is obvious since the result of the test of box i
does not depend on the presence of locks and the results of the testing in other
boxes.
c) The formal, non-trivial proof of the equality in (5), can be found in
paper [16].
⊓⊔
The formulas in Lemma 1 are at the heart of the intuition behind our main results.
2.2
Lemma 2 and Key Ratio rB
To ﬁnd the optimal strategy of AT we need to know the probabilities of a destruc-
tion of a minus and plus boxes with u bombs in a box, i.e. P(Ci = 1|si, u), si = 0, 1.
These probabilities in turn depend on the probabilities of an absence of a lock in
minus and plus boxes, i.e. P(Ti = 0|si), si = 0, 1. The optimal strategy will be
deﬁned by the likelihood ratio of the latter probabilities, rB = P(Ti = 0|si =
0)/P(Ti = 0|si = 1). They are all described in Lemma 2. This lemma shows that
in Problem B, in contrast to Problem A, the ratio rB does not depend on x but
does depend on parameters a, b, λ and is given by an explicit formula, where we
use the shorthand notation h = a + b −1.
Lemma 2. a) The probabilities of an absence of a lock in a minus and plus boxes,
and the corresponding probabilities of destruction of a minus and plus boxes with
u bombs are
P(Ti = 0|si = 0) = p−=
(1 −λ)b
λ(1 −a) + (1 −λ)b = (1 −λ)b
b −λh ,
(6)
P(Ti = 0|si = 1) = p+ =
(1 −λ)(1 −b)
λa + (1 −λ)(1 −b) = (1 −λ)(1 −b)
1 −b + λh
,
(7)
P(Ci = 1|si = 0, u) = p−p(u),
P(Ci = 1|si = 1, u) = p+p(u);
(8)

256
L. Liu and I. M. Sonin
b) The ratio rB for Problem B is given by the formula
rB = p−
p+ =
b
1 −b
1 −b + λh
b −λh
, 0 < λ < 1;
(9)
c) If a + b > 1 then function rB(λ) is increasing from 1 to
a
1 −a
b
1 −b = c1c2 =
c > 1, when λ is increasing from 0 to 1;
if a + b < 1 then function rB(λ) is decreasing from 1 to c < 1; and if a + b = 1
then rB(λ) = 1.
Proof. The conditional independence of testing and explosions, and equalities in
(1) and (4) imply that the probability of destruction of box i with ui = u ≥1
bombs, given signal s with N = x, Si = si, is
P(Ci = 1|s, x, u) = P(Ti = 0|s, x)P(Ci = 1|Ti = 0, u) = P(Ti = 0|si)p(u).(10)
We also have the equalities
p−= P(T = 0|S = 0) = P(T = 0)P(S = 0|T = 0)/P(S = 0)
= (1 −λ)b/P(S = 0),
P(S = 0) = P(T = 1)P(S = 0|T = 1) + P(T = 0)P(S = 0|T = 0)
= λ(1 −a) + (1 −λ)b,
p+ = P(T = 0|S = 1) = P(T = 0)P(S = 1|T = 0)/P(S = 1)
= (1 −λ)(1 −b)/P(S = 1),
P(S = 1) = P(T = 1)P(S = 1T = 1) + P(T = 0)P(S = 1T = 0)
= λa + (1 −λ)(1 −b).
Using these equalities and formula (10) with si = 0 and si = 1, we obtain all
formulas in points (a) and (b).
To prove c), note that it is easy to check that
d
dλrB(λ) =
b
1 −b
h
f(λ)2 , where
f(λ) = b −λh for all 0 < λ < 1, and that rB(0) = 1, rB(1) =
a
1 −a
b
1 −b =
c1c2 = c. It is easy to check that the inequality a + b > 1 is equivalent to c =
rB(1) > 1. Hereafter we assume that h = a + b −1 > 0 and therefore rB(λ) > 1
for all λ > 0.
⊓⊔
Note that c1 and c2 represent the quality of sensitivity and speciﬁcity, and c rep-
resent the combined quality of testing.
Remark 1. Lemma 2 implies that testing is not very informative if locks are rare,
i.e. λ is small, even when the parameters of the testing are very good, i.e. a and
b are close to 1. When there are many locks, i.e. λ is close to one, the “amount
of information” is limited by the combined characteristic of the quality of testing,
parameter c.

Locks, Bombs and Testing
257
Note also that parameters a and b in function rB are not symmetrical, i.e., though
rB(.5|a, b)rB(.5|b, a) = c and rB(λ|a, b) ≈rB(λ|b, a) ≈c for λ close to 1, generally
rB(a, b) ̸= rB(b, a) for all λ < 1. This asymmetry property is in sharp contrast to
the symmetry of a and b for rA(x) in Problem A.
3
Main Results and Their Proofs
Let B−(s) = {i : si = 0} and B+(s) = {i : si = 1} denote the sets of minus and
plus boxes for signal s. Using the equality in (4), we obtain that, given a strategy
π = (u1, ..., un) for m bombs, and any signal s with N(s) = x, the value of a
strategy π, i.e. the expected number of destroyed boxes, is
wπ(s) ≡wπ(s, x) =
n

i=1
P(Ti = 0 si)p(ui)
= p−

i∈B−(s)
p(ui) + p+

i∈B+(s)
p(ui).
(11)
Let U −≡U −(π|s) = {uj, j ∈B−(s)} and U + ≡U +(π|s) = {uj, j ∈B+(s)}
be the two possible sets of the values of uj at minus and plus boxes. Formula (11)
immediately implies that all strategies obtained by permutations of sets (U −, U +)
among corresponding boxes have the same value denoted as wπ(x) ≡wπ(x, m),
where m is the number of available bombs. Hereafter we use notation wπ(s) =
wπ(x), where x = N(s).
We denote v(x, m) = supπ wπ(x, m), the value function over all strategies,
given m and x, and v(m), the value function over all strategies and all possible
values of x, i.e. v(m) = 
x gB(x)v(x, m), where gB(x) = P(N = x) is given by
the formula in (2).
Formula (11) gives a hint that the proportion of the number of bombs placed
into a minus site to the number of bombs placed into a plus site is deﬁned by ratio
rB = p−/p+, given in formula (9).
Let us assume for simplicity that the number of bombs is the same in all minus
boxes, and a similar statement holds for all plus boxes, i.e.: ui = u−, i ∈B−(s)
and ui = u+, i ∈B+(s). Then the role of ratio r = rB becomes clear, since formula
(11) takes the form of
wπ(s, x) ≡wπ(x) = p+[rBxp(u−) + (n −x)p(u+)],
where p+, p−, r are given by the formulas in (6), (7) and (9).
To obtain an optimal strategy, we use a natural and obviously necessary equi-
librium condition: with an optimal allocation of bombs it is impossible to increase
the payoﬀby moving essentially a bomb from one box to another. Essentially means
changing sets of the values of uj at minus and plus boxes, i.e. sets U −(π|s) and
U +(π|s).
We remind that in the Introduction we heuristically described the potential
structure of optimal strategies in both problems, namely that they should be d-
UAP strategies with some values of d = 0, 1, 2, .... We will prove the optimality of a

258
L. Liu and I. M. Sonin
d-UAP strategy by showing that any other strategy does not satisfy this condition.
Later we give formulas for the optimal value d = dB.
The following two lemmas describe the properties of optimal strategies. Lem-
ma 3 proves that an optimal strategy is nearly uniform inside of minus and plus
boxes: if the signals in two boxes have the same sign, then the optimal number of
bombs can diﬀer at most by 1.
Lemma 3. Let π(x) = (ui, i = 1, 2, ..., n) be an optimal strategy. Then |ur −ut| ≤
1 when the signals in boxes r, t have the same sign, i.e. sr = st.
Proof. In proof of Lemma 2, see formula (10) and after, we obtained the equalities
P(Ct = 1|st = 1, u) = p+p(u),
P(Ct = 1|st = 0, u) = p−p(u) = rBp+p(u).
(12)
Suppose that Lemma 3 is not true—say for boxes 1 and 2 with s1 = s2 =
1, u1 = i, u2 = j and j −i ≥2. The concavity of function p(u) implies that
p(i + 1) + p(j −1) > p(i) + p(j). Then, using the ﬁrst equality in (12) for t = 1
and t = 2, we obtain
P(C1 = 1|s1 = 1, u1 = i + 1, x) + P(C2 = 1|s2 = 1, u2 = j −1, x)
= p+[p(i + 1) + p(j −1))] > p+[p(i) + p(j)]
= P(C1 = 1|s1 = 1, u1 = i, x) + P(C2 = 1|s2 = 1, u2 = j −1, x).
(13)
Thus the initial allocation of bombs is not optimal. The proof for s1 = s2 = 0 is
similar, using the second equality in (12) and replacing p+ by p−= rB(λ)p+.
⊓⊔
Once we have established that the optimal numbers of bombs in boxes with the
same signal can diﬀer by not more than 1, our next step is to ﬁnd the optimal
values of m−, m+, the numbers of bombs in minus and plus boxes. Given N = x,
0 ≤x ≤n, the number m of bombs available and a d-UAP strategy, there is a
unique allocation of bombs, given by tuple (l−, e−, l+, e+), where l−, l+ are the
numbers of “complete layers” of bombs in minus and plus boxes, respectively, and
e−, e+ are the numbers of “extra” bombs in the “incomplete layers”. Hereafter we
use shorthand notation l−= l, e−= e. Note that e indicates also how many minus
boxes have an extra bomb among all minus boxes. The same is true for plus boxes.
Thus 0 ≤e < x, 0 ≤e+ < n −x and e × e+ = 0. All these terms depend on m, x
and d = dB but we do not indicate this explicitly. We have
m−= l × x + e, m+ = l+ × (n −x) + e+
Thus, if e+ > 0 then e = 0, and l −l+ = d, and if e > 0 then e+ = 0 and either
l+ = 0, l −l+ < d or l+ > 0, l −l+ = d.
Let us deﬁne the threshold levels d for Problem B by the formula
dB = min
i≥1

i : rB (1 −p)i < 1
	
.
(14)
The optimality of this level, and hence the optimality of the corresponding strategy
is proved in Lemma 4.

Locks, Bombs and Testing
259
Lemma 4. Let π(x) = (ui, i = 1, 2, ..., n) be an optimal strategy, 0 < x < n,
(u−, u+) a pair of bombs in some pair (minus box, plus box), and d = dB be deﬁned
by formula (14). Then a d-UAP strategy with d = dB deﬁned by formula (14) is an
optimal strategy.
Proof. As always, we assume that a+b > 1 and then rB > 1, and hence u−≥u+.
We will show that if u−−u+ > d for this pair of minus and plus boxes, then a
transfer of one bomb from a minus box from this pair to a plus box will increase
the value of a strategies. Similarly, if u+ ≥1 and u−−u+ < d −1 for such pair,
then the inverse transfer will also increase the value. Let u−= i, u+ = j, P(·|N =
x) = P(·|x), and denote the incremental utilities for minus and plus boxes as
ΔC−(i|x) = P(C = 1|i+1, S = 0, x)−P(C = 1|i, S = 0, x), ΔC+(j|x) = P(C =
1|j + 1, S = 1, x) −P(C = 1|j, S = 1, x). Then, using the formulas in (12) and
(13) with u = i and u = j, we obtain that their diﬀerence for 0 ≤j ≤i is with
q = 1 −p,
Δ(i, j|x) = ΔC−(i|x) −ΔC+(j|x) = pqirBp+ −pqjp+
= pqjp+(rBqi−j −1).
(15)
The deﬁnition of d = dB in (14) implies that Δ(i, j|x) is positive if j = 0, i < d,
or if j ≥1, i−j < d. Similarly, Δ(i, j|x) is negative if j = 0, i ≥d, or if j ≥1, i−j ≥
d. The optimality of d-strategy is proven.
⊓⊔
Note also, that if p = 1, i.e. q = 0, then d = 1 for all 0 < x < n, and if p is decreasing
to zero, then d tends to inﬁnity. Now we are ready to formulate our main theorem
for Problem B. As usual we assume that a + b > 1 and hence rB > 1 and then
0 < x < n.
Theorem 1 (Value function for B(n, λ)). Suppose that, given signal s, the total
number of minus boxes, with the total number of minuses N = x, 0 ≤x ≤n.
a) If x = n, (or x = 0), then the optimal strategy is 0-UAP and the value function
v(m|n) = v(m|0) for m = n × l + e, l = 0, 1, ..., 0 ≤e < n, (l = l−, e = e−) is
given by the formula
v(n|m) = v(0 m) = (1 −λ)[ep(l + 1) + (n −e)p(l)].
(16)
b) If 0 < x < n, then the optimal strategy is d-UAP strategy, where d = dB is
deﬁned by formula (14) and rB is deﬁned by formula (9). The value function
v(x, m) for m = m−+ m+ = l × x + e + l+ × (n −x) + e+, is given by formula
v(x, m) = p+(λ)[rB(λ)(ep(l+1)+(x−e)p(l))+(e+p(l++1)+(n−x−e+)p(l+))].
(17)
(c) The value function v(m), m = 1, 2, ... is given by formula
v(m) =
n

x=0
gB(x)v(x, m), where gB(x) = P(N = x),
(18)
is given by formula in (2).

260
L. Liu and I. M. Sonin
Proof.(a) If x = 0 or n, i.e. all boxes have the same minus or plus sign, and signal
s brings no information, Lemma 4 implies that an optimal strategy is 0-UAP.
When m = n × l + e, where 0 ≤e < n, then 0-UAP means that e boxes have
l + 1 bombs each, and n −e boxes have l bombs each. The probability that a
particular box has no lock is 1 −λ. Then the expected damage in all n boxes is
(1 −λ)[eP(C = 1|l + 1, T = 0) + (n −e)P(C = 1|l, T = 0)]
= (1 −λ)[ep(l + 1) + (n −e)p(l)].
i.e., v(n|m) = v(0|m) is given by formula (16).
(b) If 0 < x < n, then by Lemma 4, an optimal strategy is d-UAP, and hence m−,
m+ satisfy the equalities m−= l × x + e, m+ = l+ × (n −x) + e+, 0 ≤e < x,
0 ≤e+ < n−x, ee+ = 0. Then each of e minus boxes has l+1 bombs each, and
x −e minus boxes have l bombs each, and in plus boxes e+ boxes have l+ + 1
bombs each, and n −x −e+ boxes have l+ bombs each. Then using formulas
in Lemma 2 for the probabilities of destruction for minus and plus boxes of we
obtain formula (17). We proved b). The proof of (c) is straightforward.
⊓⊔
Remark 2. For computational purpose, the formulas in (16), (17), and (18) can be
represented recursively in m.
Remark 3. By deﬁnition of dB, let dB = d, we have rBqd−1 ≥1. If rBqd−1 > 1,
then the d-UAP strategy is the unique optimal strategy. If there is an equality, then
there are other optimal strategies with the allocation of bombs obtained as follows.
When all minus sites are ﬁlled with d −1 full layers, the next bomb, if available,
can be placed either in a minus site or in a plus site. The incremental utility will be
the same. And so on with other extra bombs. As a result, the diﬀerence between
the full layers in the minus and the plus sites can be either d or d −1.
A theorem similar to Theorem 1 holds for the case A. The full version of this
theorem, with formulas for the value functions v(m, x) and v(m) for all 0 ≤x ≤n,
and m, can be found in [16].
4
Examples
We will analyze the following pairs of examples when the expected number of locks
in B(n, λ) matches the ﬁxed number k in A(n, k), i.e. nλ = k.
Example 1 (Ratio r for B(n, λ) and A(n, k)). Let n = 2, a =
7
12, b =
9
12. For
B(2, λ), by formula (9), when λ = 1/2, we obtain rB(1/2) = 15
7 ≈2.143; when
λ = 1, rB(1) = 4.2 = c. With a =
9
12, b =
7
12, we obtain rB(1/2) = 49
25 = 1.96,
rB(1) = 4.2 = c. Hence rB(1/2|a, b)rB(1/2|b, a) = 21
5 = 4.2 = c. When a + b > 1,
ratio rB as a function of λ is increasing, while when a+b < 1, the ratio is decreasing,
as shown in Fig. 1.
Let n = 2, k = 1, a =
7
12, b =
9
12. For A(2, 1), we have rA(1) = 21
5 = 4.2 = c,
rA(0) is not deﬁned, and rA(2) is also not deﬁned. For any problem A(n, n −1) we
have rA(1) = c = 21
5 = 4.2.

Locks, Bombs and Testing
261
Fig. 1. Ratio rB.
We skip the proof of the formula for rA(x) ≡r(x) in Problem A, and the prop-
erties of this coeﬃcient. Some details can be found in [9] and [16].
Example 2 (Optimal strategy for A(n, k) and B(n, λ)). In an example with n = 5,
x = 4, m = 7, a = 7/12, b = 9/12, for B(5, 0.4), we have dB = 1, l = 1, e = 2,
m−= 6, l+ = 1, e+ = 0, m+ = 1. Hence the optimal strategy is to place 1 bomb
in each of the 2 minus boxes, 2 bombs in each of the remaining 2 minus boxes and
1 bomb goes to the 1 plus box. Thus m−= (x −e)l + e(l + 1) = 2 ∗1 + 2 ∗2 = 6,
m+ = (n −x −e+)l+ + e+(l+ + 1) = 1 ∗1 + 0 = 1.
However, for A(5, 2), we have dA = 2, l = 1, e = 3, m−= 7, l+ = e+ = m+ =
0. Hence our optimal strategy is to put 1 bomb in the 1 minus box, 2 bombs in each
of the 3 minus boxes, and no bomb goes in the plus box. Thus m−= (x −e)l +
e(l + 1) = 1 ∗1 + 3 ∗2 = 7.
Assuming that the signals for the ﬁrst 4 boxes are all minus, the signal for the
remaining box is plus, and the two locks are allocated among the boxes. The fol-
lowing table illustrates the idea of the bombs placement.
1
2
3
4
5
γ (Lock position)
⊗
⊗
s (Observed signal)
−−−−+
Bomb placement for B(5, 0.4) 2
2
1
1
1
Bomb placement for A(5, 2)
2
2
2
1
0
Example 3 (Value function for A(n, k) and B(n, λ)). Let a = 7/12, b = 9/12,
number of bombs m = 1, 2, ..., 7, and p = 0.6.
For B(n, λ), from Lemma 2, we know that p+, p−and ratio rB only depend on
λ, a and b, while dB only depends on rB and p. Hence we calculate these values
based on diﬀerent λ (see Table 1).
Now we can compare the value function for:
1. A(2, 1) and B(2, 0.5).
For B(2, 0.5), according to Table 1, for λ = 0.5, we have d = 1, and rB = 2.143.
For A(2, 1), d is changing with respect to x, and so is r(x). The conditional value
function v(x, m) is shown in Table 2. With diﬀerent number of bombs, the value
function is shown in Table 3. We also generate a comparison plot for A(2, 1) and
B(2, 0.5) with respect to diﬀerent m, as shown in Fig. 2.

262
L. Liu and I. M. Sonin
Table 1. B(n, λ): ratio and d for diﬀerent λ.
λ
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
rB(λ) 1
1.186 1.39
1.615 1.865 2.143 2.455 2.806 3.207 3.667 4.2
p−
1
0.942 0.878 0.808 0.73
0.643 0.545 0.435 0.31
0.167 0
p+
1
0.794 0.632 0.5
0.391 0.3
0.222 0.155 0.097 0.046 0
dB
1
1
1
1
1
1
1
2
2
2
2
Table 2. Value function for B(2, 0.5) and A(2, 1) when m = 7.
Bomb placement and value function
x
Problem
d
r
l
e
m−
l+
e+ m+ v(x, m)
g(x)
v(x, m)g(x)
0
B(2, 0.5) 1
2.143 0
0
0
3
1
7
0.955
0.174
0.166
A(2, 1)
1
None 0
0
0
3
1
7
0.955
0.312
0.298
1
B(2, 0.5) 1
2.143 4
0
4
3
0
3
0.907
0.486
0.441
A(2, 1)
2
4.2
4
0
4
3
0
3
0.967
0.542
0.524
2
B(2, 0.5) 1
2.143 3
1
7
0
0
0
0.955
0.34
0.325
A(2, 1)
1
None 3
1
7
0
0
0
0.955
0.146
0.139
Table 3. The value function for diﬀerent number of bombs.
m
1
2
3
4
5
6
7
B(2, 0.5) vB(m) 0.483 0.583 0.72 0.817 0.871 0.91
0.932
A(2, 1)
vA(m) 0.4
0.6
0.76 0.857 0.904 0.943 0.962
Thus, when m = 7, for B(2, 0.5), the value function equals
v(m) =
2

x=0
v(x, m)g(x) = 0.166 + 0.441 + 0.325 = 0.932;
for A(2, 1), the value function equals
v(m) =
2

x=0
v(x, m)g(x) = 0.298 + 0.524 + 0.139 = 0.962.

Locks, Bombs and Testing
263
Fig. 2. The value function for A(2, 1) and B(2, 0.5).
2. A(3, 1) and B(3, 0.33).
The conditional value function v(x, m) is shown in Table 4. With diﬀerent num-
ber of bombs, we have the value function shown in Table 5. We also generate a
comparison plot for A(3, 1) and B(3, 0.33) with respect to diﬀerent m, as shown
in Fig. 3. The expected damage is relatively higher in A(3, 1) than in B(3, 0.33),
except the case when there’s only one bomb.
Table 4. Value function for B(3, 0.33) and A(3, 1) when m = 7.
Bomb Placement and Value Function
x
Problem
d
r
l
e
m−l+ e+ m+ v(x, m)
g(x)
v(x, m)g(x)
0
B(3, 0.33) 1
1.688 0
0
0
2
1
7
1.753
0.047
0.082
A(3, 1)
1
None 0
0
0
2
1
7
1.744
0.13
0.227
1
B(3, 0.33) 1
1.688 3
0
3
2
0
4
1.517
0.249
0.377
A(3, 1)
1
1.615 3
0
3
2
0
4
1.766
0.408
0.72
2
B(3, 0.33) 1
1.688 2
1
5
2
0
2
1.785
0.442
0.79
A(3, 1)
2
2.6
3
0
6
1
0
1
1.764
0.377
0.664
3
B(3, 0.33) 1
1.688 2
1
7
0
0
0
1.753
0.262
0.459
A(3, 1)
1
None 2
1
7
0
0
0
1.744
0.085
0.148
Table 5. Value function for diﬀerent number of bombs.
m
1
2
3
4
5
6
7
B(3, 0.33) v(m) 0.519 0.77
1.128 1.304 1.458 1.579 1.65
A(3, 1)
v(m) 0.483 0.917 1.2
1.397 1.567 1.681 1.759

264
L. Liu and I. M. Sonin
Fig. 3. Value function for A(3, 1) and B(3, 0.33).
Fig. 4. Value function for A(5, 2) and B(5, 0.4).
3. A(5, 2) and B(5, 0.4).
Let’s check the value function for A(5, 2) and B(5, 0.4) for diﬀerent m. The value
function v(m) is shown in Table 6 and the plot of v(m) is on the left in Fig. 4.
We can see that A(5, 2) has a higher expected damage value than B(5, 0.4), but
then they are becoming more and more closer as m gets larger.
The comparison plot of the value function for relatively large m is shown on the
right in Fig. 4. The value function is clearly getting closer as m gets larger.
Table 6. Value function for diﬀerent number of bombs
m
1
2
3
4
5
6
7
B(5, 0.4) v(m) 0.462 0.744 1.042 1.396 1.77 1.943 2.109
A(5, 2)
v(m) 0.451 0.891 1.282 1.576 1.8
1.982 2.158
Conclusion: According to all three comparison plots, for the same small amount
of bombs m, A(n, k) usually has a higher expected damage value than B(n, k/n),
but when m is large, the diﬀerence becomes smaller and smaller.

Locks, Bombs and Testing
265
References
1. Gittins, J., Glazebrook, K., Weber, R.: Multi-Armed Bandit Allocation Indices.
John Wiley & Sons, Chichester (2011)
2. Gross, O., Wagner, R.: A continuous Colonel Blotto game. RAND Corporation RM-
408 (1950)
3. Hart, S.: Discrete colonel blotto and general lotto games. Int. J. Game Theory 36,
441–460 (2008). https://doi.org/10.1007/s00182-007-0099-9
4. Hart, S.: Allocation games with caps: from Captain Lotto to all-pay auctions. Int.
J. Game Theory 45, 37–61 (2016)
5. Hohzaki, R.: Search games: literature and survey. J. Oper. Res. Soc. Jpn. 59(1), 1–34
(2016)
6. Kvasov, D.: Contests with limited resources. J. Econ. Theory 136, 738–748 (2007)
7. Lattimore, T., Szepesvari, C.: Bandit Algorithms. Cambridge University Press,
Cambridge (2020)
8. Liu, L.: Optimal strategies in “Locks, Bombs, and Testing” (LBT) problem for the
case of independent protection. PhD thesis, UNCC (2019). https://math.uncc.edu/
preprint-archive/optimal-strategies-locks-bombs-and-testing-lbt-problem-case-
independent-protection
9. Presman,
E.:
On
one
property
of
an
important
characteristic
in
a
defense/attack problem. Herald of CEMI 1(1), 81–85 (2018). https://cemi.
jes.su/s111111110000118-2-1. (in Russian)
10. Presman, E., Sonin, I.: Sequential Control with Incomplete Information: the
Bayesian Approach to Multi-armed Bandit Problems. Academic Press, London
(1990)
11. Powell, R.: Defending against terrorist attacks with limited resources. Am. Political
Sci. Rev. 101(3), 527–541 (2007)
12. Robertson, B.: The colonel blotto game. Econ. Theory 29, 1–24 (2006)
13. Shubik, M., Weber, R.J.: Systems defense games: Colonel Blotto, command and con-
trol. Nav. Res. Log. Quart. 28, 281–287 (1981)
14. Sonin, K., Wright, A.: Rebel capacity and combat tactics. (2018) http://dx.doi.org/
10.2139/ssrn.3030736
15. Sonin, I.: Bayesian game of locks, bombs and testing. arXiv:1906.01163 (2019)
16. Sonin, I., Sonin, K.: Bayesian game of locks, bombs and testing. (2019) Unpublished
manuscript.

A Survey of Stability Results
for Redundancy Systems
Elene Anton1,3(B), Urtzi Ayesta1,2,3,4, Matthieu Jonckheere5,
and Ina Maria Verloop1,3
1 CNRS, IRIT, 2 Rue Charles Camichel, 31071 Toulouse, France
{elene.anton,urtzi.ayesta,verloop}@irit.fr
2 IKERBASQUE - Basque Foundation for Science, 48011 Bilbao, Spain
3 Universit´e de Toulouse, INP, 31071 Toulouse, France
4 UPV/EHU, University of the Basque Country, 20018 Donostia, Spain
5 Instituto de C´alculo - Conicet, Facultad de Ciencias Exactas y Naturales,
Universidad de Buenos Aires (1428) Pabell´on II, Buenos Aires, Argentina
mjonckhe@dm.uba.ar
Abstract. Redundancy mechanisms consist in sending several copies of
a same job to a subset of servers. It constitutes one of the most promising
ways to exploit diversity in multi-servers applications. However, its pros
and cons are still not suﬃciently understood in the context of realistic
models with generic statistical properties of service-times distributions
and correlation structures of copies. We aim at giving a survey of recent
results concerning the stability - arguably the ﬁrst benchmark of perfor-
mance - of systems with cancel-on-completion redundancy. We also point
out open questions and conjectures.
Keywords: Redundancy · Load balancing · Stability
AMS(2020) Subject Classiﬁcation: Primary 60K25 · Secondary
68M20
1
Introduction
While there are several variants of redundancy-based systems, the general notion
of redundancy is to dispatch multiple copies of each job to a subset of servers
and to consider the result of whichever copy completes service ﬁrst. By allowing
for redundant copies, the aim is to minimize the system latency by exploiting
the variability in the queue lengths of the diﬀerent queues. The potential of
redundancy mechanisms lies in ﬁnding the right trade-oﬀbetween exploiting
variability and the waste of resources induced by having redundant copies.
Several empirical [2,3,11,12,38,41] and numerical studies [15,16,26,29,30]
suggest that redundancy might potentially improve the performance of real-
world computer system applications. In particular, Vulimiri et al. [41] consider
a 10 DNS servers system and compare the system where each arriving query
c
⃝The Author(s), under exclusive license to Springer Nature Switzerland AG 2021
A. Piunovskiy and Y. Zhang (Eds.): Modern Trends in Controlled Stochastic Processes,
ECC 41, pp. 266–283, 2021. https://doi.org/10.1007/978-3-030-76928-4_13

Stability of Redundancy Systems
267
dispatches 10 copies to all the 10 DNS servers, to an alternative system where
queries are assigned to a single server chosen uniformly at random. The authors
observe that the fraction of queries with a service time exceeding 500 ms is
reduced by a factor 6.5, and the fraction exceeding 1.5 is reduced by a fac-
tor 50. Another interesting study is provided by Dean and Barroso [12] who
underline that several redundancy techniques are applied in Google’s BigTable
in order to improve the latency of incoming queries. They show that a redun-
dancy system with two copies reduces the median response time by 16% and
the 99.9th-percentile of the tail of the response time distribution by nearly 40%
compared to the non-redundant system.
Broadly speaking, depending on when replicas are deleted, we can con-
sider two classes of redundancy systems: cancel-on-start (c.o.s.) and cancel-on-
completion (c.o.c.). In redundancy systems with c.o.c., once one of the copies
has completed service, the other copies are deleted and the job is said to have
received service. In redundancy systems with c.o.s., copies are deleted as soon
as one copy starts being served, and as a consequence, c.o.s. does not waste any
computation resources.
In this survey, we will provide an overview on stability results in redundancy
systems. From the point of view of stability, c.o.s. does not have any negative
impact, and for this reason we focus on stability results when c.o.c. is imple-
mented.
Let us illustrate through a simple example how redundancy aﬀects the sta-
bility region. Consider a system with K homogeneous servers in which copies of
each arriving job are dispatched to d ≤K servers chosen uniformly at random.
We assume that jobs arrive according to a Poisson process of rate λ and jobs
have general service times with unit mean. Without redundancy, i.e. d = 1, the
stability condition under any work-conserving policy is given by λ < μK, where
μ is the capacity of the servers. Now, let us assume that the service times are
exponentially distributed, that copies are i.i.d. and that d = K. In this case, the
system behaves as a single server system with arrival rate λ and server capac-
ity μK, and the stability condition is again λ < μK. However, if all the copies
had the same service time as the original job (identical copies), servers are syn-
chronized and the instantaneous departure rate is just μ. Therefore, the system
behaves as a single server system with arrival rate λ and server capacity μ, for
which the stability condition is λ < μ. This simple example illustrates how the
modeling assumptions and the degree of redundancy can dramatically impact
the stability condition of the system.
One of the main lessons we draw from the results available in the literature,
is that the stability region depends strongly on the scheduling policy employed
at the servers and the correlation structure of copies. Somewhat surprisingly,
we also identify situations for which it was shown that adding redundant copies
does not reduce the stability region. Overall, we believe more research is needed
in order to design eﬃcient redundancy algorithms.
The rest of the survey is organized as follows. Section 2 describes the main
model assumptions and notation, Sect. 3 deals with the case in which the service

268
E. Anton et al.
times of the copies are i.i.d., and Sect. 4 with identical and correlated copies. In
Sect. 5, we present a brief account of results on redundancy that, even though not
directly related to stability, are relevant from the performance point of view. We
conclude with Sect. 6 where we discuss several open problems and state various
conjectures.
2
Model Description and Preliminaries
We consider a K parallel heterogeneous server system. That is, we have a set of
servers S = {1, . . . , K} and server s has capacity μs, for s ∈S. Jobs arrive to the
system according to a Poisson process of rate λ. Arriving jobs have service times
that are independent across jobs and are identically distributed with mean 1.
Jobs are labeled by types c = {s1, . . . , si} ⊂S, where i is the number of
copies and c is the set of servers to which this job will dispatch copies. We let
C be the set of all possible types. A job is of type c with probability pc, where

c∈C pc = 1.
We consider redundancy models that are c.o.c., that is, as soon as a copy is
fully served, the additional copies of that job are removed from the system. This
cancellation process induces a correlation in the departure process at the servers.
Thus, within a server s there is a departure of a copy due to the following two
events: i) a local copy departs due to completion in server s, or ii) a copy in
another server completes that induces a departure in server s.
Model Topology. A well-known symmetric topology is the one in which each
job sends a copy to d out of K servers. In case the server are chosen uniformly
at random, that is, pc = 1/
K
d

, and servers have the same capacity μ, we refer
to this model as the redundancy-d model, see Fig. 1 (a). The number of copies,
d, is referred to as the redundancy degree.
µ
µ
µ
µ
λ
µ1
µ2
λ
µ1
µ2
λ
(a)
(b)
(c)
d
Fig. 1. (a) The redundancy-d model for K = 4 and d = 2. (b) The N-model. (c) The
W-model.
Two other examples of redundancy topologies are the so-called N-model
and W-model, see Fig. 1 (b) and (c). Both models are non-symmetric, with two
servers. The set of possible job types is C = {{2}, {1, 2}} in the N-model, and
C = {{1}, {2}, {1, 2}} in the W-model.

Stability of Redundancy Systems
269
When no speciﬁc structure is assumed, we refer to it in the sequel as a general
topology.
Scheduling Policy.
A scheduling policy determines how copies are served
within each server. As we will see, the choice of the scheduling policy can have
a dramatic impact on the stability region. First-Come-First-Served (FCFS) and
Processor Sharing (PS) are widely implemented in real-world computer systems
[21], and are thus common policies considered in the literature on redundancy.
Random-Order-of-Service (ROS) is not a common discipline in systems, but as
we will see in the ensuing, it yields very good performance in terms of stabil-
ity for a redundancy system. These three policies represent the main focus of
our survey. To the best of our knowledge, other policies such as Last-Come-
First-Served (LCFS), Shortest-Remaining-Processing-Time (SRPT), and Least-
Attained-Service (LAS) have not been considered so far.
Correlation Structure Among Copies. This describes how the service times
of the copies of a given job are related. Formally, the service times X1, . . . , Xk of
the copies of one job can be sampled from a joint distribution F(x1, . . . , xk). Two
extreme cases are i.i.d. copies and identical copies. Under i.i.d. copies, all copies
have independent service times sampled from the same distribution, whereas
with identical copies, all the copies of a job have the same service time. Another
interesting framework is the so-called S&X model introduced in [16]. Here, the
service time of each copy is decomposed into two components; the inherent job
size, which is identical for all the copies of a job, and the experienced slowdown
on the server it is being served.
Existing Stability Results. Table 1 summarizes the main stability results for
c.o.c. redundancy models available in the literature and discussed in this survey.
The table is organized by scheduling policy, service time distribution, redun-
dancy topology and correlation structure. In brackets we specify the additional
assumptions that the authors consider in their respective paper. The term “red-
d” refers to the redundancy-d system and the term “gen.” refers to a general
redundancy topology.
Table 1. Stability results for c.o.c redundancy models.

270
E. Anton et al.
The stability condition when jobs have i.i.d. copies is the main topic of Sect. 3,
ﬁrst for exponential service times (Sect. 3.1) and then for scaled Bernoulli dis-
tributions (Sect. 3.2). These are the results in the ﬁrst two columns of Table 1.
Correlated copies are discussed in Sect. 4, ﬁrst for identical copies (Sect. 4.1, mid-
dle two columns in Table 1) and then for general correlation structures (Sect. 4.2,
last two columns of Table 1). In Sect. 6, we discuss open problems and state var-
ious conjectures regarding stability conditions. In Table 1, these conjectures are
indicated with a X.
3
Independent and Identically Distributed Copies
In this section we assume that jobs have i.i.d. copies.
3.1
Exponential Service Times
We ﬁrst discuss results on FCFS and exponentially distributed service times, a
setting studied by Gardner et al. [17,20] and Bonald and Comte [8]. It was shown
in [8] that this model ﬁts the framework of Order Independent queues (see [28,
Chapter 2]), which is a large class of systems that have a product-form steady-
state distribution. This can be seen as follows. Since copies are i.i.d., we can
describe the system through the Markovian state descriptor (cn, cn−1, . . . , c2, c1).
Here, n is the number of jobs in the system, c1 is the type of the eldest job in
the system and ci is the type of the ith eldest job. Because of FCFS, the eldest
job is served in all of its compatible servers c1. The i-th eldest job is in service
at servers s ∈ci\ ∪i−1
j=1 cj, for i = 1, . . . , n. Due to the exponentially distributed
service times and i.i.d. copies, the instantaneous departure rate of the ith job
is given by the sum of the rates in the servers where the job is in service, that
is, 
s∈ci\c1,...,ci−1 μs. Hence, the total instantaneous departure rate out of state
(cn, cn−1, . . . , c2, c1) is 
s∈∪n
j=1cj μs, which depends on the set of classes present
in the system, but not on their ordering in the state descriptor, i.e., the so-called
order independent property.
The characterization of the steady-state distribution facilitates the derivation
of performance measures such as the stability condition and mean response times.
The proposition below states the stability result for this model.
Proposition 1 ([8,20]). For a redundancy system with general topology under
FCFS with exponentially distributed service times and i.i.d. copies, the system
is stable if for all C ⊆C,
λ

c∈C
pc <

s∈S(C)
μs,
(1)
where S(C) = 
c∈C{s ∈c}. The system is unstable if there exists ˜C ⊆C such
that
λ

c∈˜
C
pc >

s∈S( ˜
C)
μs.

Stability of Redundancy Systems
271
Informally, Eq. (1) states that the arrival rate to any subset of job types must
be less than the total capacity of the associated compatible servers. For expo-
nential service times, this is the maximum stability condition, i.e., the system
cannot be stable if one of these inequalities were not satisﬁed. Thus, we conclude
that the stability region is not reduced due to adding redundant copies. The lat-
ter might seem counter-intuitive at ﬁrst, since even if servers waste resources
serving copies that are not fully served, the stability condition is as large as if
there was no redundancy (see also the simple example in the introduction).
Extending Proposition 1 to other scheduling policies is an important open
problem (see Sect. 6 for more details). To the best of our knowledge, this has
only been achieved for the redundancy-d model. In this case, it is easy to see that
Eq. (1) reduces to λ < μK, and it has been shown that this stability condition
remains valid when either PS or ROS is implemented.
Proposition 2 ([4]). For the redundancy-d model under either PS or ROS with
exponentially distributed service times and i.i.d. copies, the system is stable when
λ < Kμ and unstable when λ > Kμ.
Hence, under PS, ROS and FCFS, the redundancy-d model is maximum sta-
ble. This however does not hold true in general. In the example below (originally
in [4]), we describe priority policy that is not maximum stable, i.e., the system
can become unstable even though λ < Kμ.
Example: Priority Policy. Consider the redundancy-d system with K =
3, d
=
2 and μ
=
1. There are three diﬀerent types of jobs: C
=
{{1, 2}, {1, 3}, {2, 3}}. In server 1, FCFS is implemented. In server 2 and server 3,
jobs of types {1, 2} and {1, 3} have preemptive priority over jobs of type {2, 3},
respectively. Additionally, within a type, jobs are served in order of arrival.
In Fig. 2 we plot the sample-path of the number of jobs when λ = 2.9 < 3 =
μK. We observe that the number of type-{2, 3} jobs in the system grows large,
while the number of type-{1, 2} and type-{1, 3} jobs stay close to 0. Hence,
the system is clearly unstable, even though λ < μK. This can intuitively be
explained by the ineﬃciency induced by the priority mechanism as the type-
{2, 3} jobs are preempted by type-{1, 2} and type-{1, 3} jobs in servers 2 and 3,
respectively. We refer to [4] for more details.
3.2
General Service Times
To the best of our knowledge, no stability results exist for general service times
with i.i.d. copies. In this section, we present the stability result obtained for
scaled Bernoulli service times, deﬁned as

X · M, with probability 1/M
0,
with probability 1 −1/M,
where M > 0 and X is a strictly positive random variable with E[X] = 1. In
this setting, Raaijmakers et al. [35] characterize the stability condition for the
redundancy-d model where FCFS is implemented and the number of servers
grows large.

272
E. Anton et al.
Fig. 2. The trajectory of the number of jobs per type when λ = 2.9.
Proposition 3 ([35]). Consider the redundancy-d model under FCFS with
scaled Bernoulli service times and i.i.d. copies. Then, λ <
M d−1
E[min(X1,...,Xd)] is
a suﬃcient stability condition for any M. In addition, for any ϵ, it holds that
(1 −ϵ)λ <
M d−1
E[min(X1,...,Xd)] is a necessary condition, for M suﬃciently large.
We observe that the stability condition is independent of the number of
servers, but strongly depends on the number of copies d. The latter is in contrast
to the exponentially distributed service times, where the stability condition does
dependent on the number of servers but is independent of d (see Proposition 2).
Thus, we observe that when copies are i.i.d., the stability condition strongly
depends on the service time distribution. In addition, we observe that as M
grows large (and hence the variance of the service times grows large), the stability
region increases by a factor M d−1, by taking advantage of a greater diversity in
service times.
4
Correlated Copies
Several studies (e.g., [42]) have shown that the i.i.d. copies assumption can be
unrealistic, since large jobs remain large when replicated. Hence, having addi-
tional copies could lead to high response times and even instability. Motivated by
the latter, stability results with correlated copies have been the focus of recent
literature.
4.1
Identical Copies
In this section, we assume that jobs have identical copies, i.e., all copies belonging
to one job have the same size. This correlation makes that a job can only depart
due to its copy that has received most service so far. Thus, the instantaneous
departure rate of a job depends on its copy that has currently attained most
service.

Stability of Redundancy Systems
273
FCFS Policy. With FCFS, the eldest job in the system will be served at all of
its compatible servers. A job later in the queue will be served at its compatible
servers that are not engaged by earlier jobs in the queue.
The stability condition for the redundancy-d system with FCFS and expo-
nentially distributed service times is characterized in Anton et al. [4], through
the average departure rate per type in the so-called saturated system. The latter
assumes an inﬁnite backlog of jobs waiting for service. The long-run time-average
number of jobs in service in the saturated system is denoted by ¯ℓ. A detailed
description of the saturated system and the characterization of ¯ℓcan be found
in [4].
Proposition 4 ([4]). For the redundancy-d system under FCFS with exponen-
tially distributed service times and identical copies, the system is stable if λ < ¯ℓμ
and unstable if λ > ¯ℓμ.
The value of ¯ℓ, and hence the stability region, can be numerically obtained by
solving the balance equations of the saturated system, see [4] for more details.
We note that the instantaneous departure rate in the saturated system strongly
depends on the types in service. As a consequence, no expression has been derived
so far for ¯ℓfor general K and d values.
¯ℓ/K K = 2 K = 3 K = 4 K = 5 K = 6 K = 7 K = 8
d = 1
1
1
1
1
1
1
1
d = 2
0.5
0.66
0.71
0.74
0.76
0.77
0.77
d = 3
0.33
0.5
0.54
0.57
0.58
0.60
d = 4
0.25
0.4
0.43
0.46
0.47
d = 5
0.2
0.33
0.36
0.38
d = 6
0.16
0.28
0.31
d = 7
0.14
0.25
Fig. 3. The table and ﬁgure show the values of ¯ℓ/K for diﬀerent values of d and K.
Note that the stability condition can equivalently be written as
λ
Kμ <
¯ℓ
K ,
where
λ
Kμ is the traﬃc load. In Fig. 3 (originally in [4]), we provide numerical
values for
¯ℓ
K , that is, the traﬃc supported by the system. The table (left) shows
¯ℓ/K for small values of K and the ﬁgure (right) plots the value of ¯ℓ/K as K
grows large. To obtain the value of ¯ℓfor d ̸= 1, K −2, K −1, K, the authors
simulate the saturated system, rather than solving the balance equations1. It
was proven in [4] that ¯ℓ/K, hence the amount of supported traﬃc, increases
1 When d = K−1, there are d servers that process copies of one job, and the remaining
K −d = 1 server serves one additional job, hence, ¯ℓ= 2. When instead d = 1, there
is no redundancy and each server serves one job in the saturated system, i.e., ¯ℓ= K.
When d = K, the system behaves as a single server with capacity μ, that is, ¯ℓ= 1.

274
E. Anton et al.
when the number of servers (K) grows large, a property that can be observed in
Fig. 3.
PS Policy. Under PS and identical copies, the stability condition is character-
ized in [5]. There it is shown that the stability condition coincides with that of a
K parallel server system where each type-c job is only dispatched to its so-called
least-loaded servers. In order to state this result, we ﬁrst need to deﬁne several
sets of servers and customer types. The ﬁrst subsystem includes all servers, that
is S1 = S. We denote by L1 the set of least-loaded servers in the system S1 = S.
Thus,
L1 =
⎧
⎨
⎩s ∈S1 : s = arg min
˜s∈S1
⎧
⎨
⎩
1
μ˜s

c∈C(˜s)
pc
⎫
⎬
⎭
⎫
⎬
⎭.
For i = 2, . . . , K, we deﬁne recursively
Si := S\ ∪i−1
l=1 Ll,
Ci := {c ∈C : c ⊂Si},
Ci(s) := Ci ∩C(s),
Li :=

s ∈Si : s = arg min˜s∈Si

1
μ˜s

c∈Ci(˜s) pc

.
The Si-subsystem refers to the system consisting of the servers in Si, with
only jobs of types in the set Ci. The Ci(s) is the subset of types that are
served in server s in the Si-subsystem. We let C1 = C. The set Li repre-
sents the set of least-loaded servers in the Si-subsystem. Finally, we denote by
i∗:= arg maxi=1,...,K{Ci : Ci ̸= ∅} the last index i for which the subsystem Si
is not empty of job types.
The stability condition is now characterized in [5] by the least-loaded servers
that can serve each job type.
Proposition 5 ([5]). Assume that the service time distribution is such that it
has no atoms and is light-tailed in the following sense,
lim
r→∞sup
a≥0
E[(X −a)1{X−a>r}|X > a] = 0.
(2)
For a redundancy system with a general topology under PS with identical copies,
the system is stable if λ 
c∈Ci(s) pc < μs, for all s ∈Li, i = 1, . . . , i∗. The
redundancy system is unstable if there exists ι ≤i∗and s ∈Lι such that
λ 
c∈Cι(s) pc > μs.
It can be seen (as observed in [33]) that the light-tailed condition in (2) also
implies
sup
a≥0
E[(X −a)|X > a] ≤Φ < ∞,
(3)
which is a usual light-tailed condition (see [14]). Hence, (2) and (3) exclude
heavy tailed distributions like Pareto, but include large sets of distributions
such as phase type (which are dense in the set of all distributions on R+),

Stability of Redundancy Systems
275
exponential and hyper-exponential distributions, as well as distributions with
bounded support.
For the redundancy-d model, the above stability condition simpliﬁes into
λ < Kμ/d. The latter coincides with the stability condition of a system where
all the copies need to be served, that is, the worst possible stability condition.
ROS Policy. When ROS is implemented in the servers, it was shown in [4] that
the stability condition is not reduced when adding redundant copies. This was
proved for exponentially distributed service times and identical copies for the
redundancy-d model. However, as stated in Sect. 6, we believe that this holds
true for any redundancy structure and any correlation structure.
Proposition 6 ([4]). For the redundancy-d model under ROS with exponen-
tially distributed service times and identical copies, the system is stable if
λ < Kμ.
The intuition behind the above result is as follows. Whenever there are many
jobs in a server, the probability that this server serves a copy of a job that has
also a copy elsewhere in service will be close to zero. Hence, with a probability
close to 1, all highly-loaded servers are serving copies of diﬀerent jobs and their
instantaneous departure rate equals the sum of their capacities.
4.2
Generally Correlated Copies
In this section, we consider redundancy models where the service times of the
copies of each job are correlated according to some general structure.
For FCFS, Raaijmakers et al. [34] consider a general workload model, which
subsumes the S&X model, introduced in [17]. The main diﬀerence is that in
[34] the server capacities are not ﬁxed, but each job samples server capacities
from a discrete and ﬁnite distribution. The authors assume that the server speed
variations (slowdowns) are either distributed according to New-Better-than-Used
(NBU) or New-Worse-than-Used (NWU). See [37] for more details on NBU and
NWU distributions2.
Depending on the random variation in the server speed, the authors prove
that either no replication (d = 1) or full replication (d = K) provides a larger
stability region. Note that here the stability region refers to a wider concept
than what we considered before. That is, it refers to the set of arrival rates such
that there exists a static assignment rule that makes the system stable.
Proposition 7 ([34]). Consider the following model. Each job is routed to d
servers according to some static probabilistic assignment. Servers implement
2
X is said to be New-Better-than-Used (NBU) if for all t1, t2 ∈R, ¯FX(t1 + t2) ≤
¯FX(t1) ¯FX(t2). X is said to be New-Worse-than-Used (NWU) if for all t1, t2 ∈R,
¯FX(t1 + t2) ≥¯FX(t1) ¯FX(t2). A suﬃcient condition for X to be NBU (NWU) is to
have an increasing (a decreasing) hazard rate, i.e., r(x) is increasing (decreasing)
in x.

276
E. Anton et al.
FCFS. Every time a server starts serving a new copy, it samples a speed vari-
ation, which is independent across servers. The type of a job is determined by
the capacities it would obtain in each server. A job has a generally distributed
service time.
– If the probabilistic assignment can depend on the job type, and the speed vari-
ation follows an NBU distribution, then the stability region for d = 1 is larger
or equal than that for d > 1.
– If the probabilistic assignment does not depend on the job type, and the speed
variation follows an NWU distribution, then the stability region for d = K is
larger or equal than that for d = 1.
From the above we observe that the optimal redundancy degree does not
depend on the job size distributions, but rather on the random variation in the
server speeds for a given job among the servers.
A suﬃcient stability condition for the redundancy-d model with FCFS has
been obtained in Mendelson [32]. He considers that the service times of the
copies X1, . . . , Xd are identically distributed with mean 1 and sampled from a
joint distribution F(x1, . . . , xd).
Proposition 8 ([32]). Consider the redundancy-d model where FCFS is imple-
mented and the service times of the copies are sampled from a general joint dis-
tribution F(x1, . . . , xd). Then, λ < λlb is a suﬃcient stability condition, where
λlb :=
μK
d
m=0
d−m
j=1 E[min(X1, . . . , Xj)] + mE[min(X1, . . . , Xd)]

Pm
,
and Pm =
K−d
d−m
 d
m

/
K
d

.
For the special cases d = 1 and d = K, the suﬃcient condition simpliﬁes to
λ < λlb = Kμ and λ < λlb = μ/E[min(X1, . . . , Xd)], respectively, which are in
fact also the necessary stability conditions.
We now consider the redundancy-d model where PS is implemented. Raai-
jmakers et al. [36] characterize the stability condition under any service time
distribution through the minimum of the service times of the copies of a job.
The latter can be heuristically explained as follows: assume that all servers are
equally loaded. Then, due to PS, the copy that completes ﬁrst is the one with
the smallest service time among all copies of the job.
Proposition 9 ([36]). For the redundancy-d model under PS where the service
times of the copies are sampled from a general joint distribution F(x1, . . . , xd),
a necessary stability condition is λdE[min(X1, . . . , Xd)] < Kμ.
In the particular case where copies are identical, the authors in [36] prove
that Proposition 9 gives a suﬃcient and necessary stability condition, which is
given by λd < Kμ. We note that the latter coincides with the stability condition
for light-tailed service times distributions provided in Proposition 5. Moreover,
[36] shows that the stability condition under NWU service time distributions,
respectively NBU service time distributions, is larger, respectively smaller, than
that for exponential service times.

Stability of Redundancy Systems
277
5
Related Work
In this section, we brieﬂy overview relevant papers on redundancy. Even though
the results do not deal directly with stability, they are important pointers for
the reader who wishes to work on redundancy.
5.1
Response Time
The response time (a.k.a. delay) measures the time elapsed between arrival and
departure. It is together with stability the main performance measure, and it has
received considerable attention. The ﬁrst performance analysis of a redundancy
model was for cancel-on-complete (c.o.c.) with exponentially distributed service
times, independent and identically distributed (i.i.d.) copies and FCFS. As dis-
cussed in Sect. 3.1, Gardner et al. [17,20] and Bonald and Comte [8] exploit the
link between this redundancy system and the Order Independent queue [28],
in order to show that the steady-state distribution has a product form. The
paper [17] showed that the mean response time in the system reduces as the
redundancy degree d increases. Redundancy c.o.s. with FCFS and exponentially
distributed job sizes has been analyzed in Ayesta et al. [7], where it was shown
that the steady-state distribution also has a product form. This was achieved by
showing that this model ﬁts within the framework of multi-type jobs and multi-
type servers studied in Visschers et al. [40]. The above results have motivated
researchers to develop unifying frameworks to explain the emergence of product
form distributions in redundancy models. This is done in Ayesta et al. [6] and
Gardner and Righter [19] by extending the frameworks of Visschers et al. [40]
and Order Independent queues [28], respectively.
Comte and Dorsman [10] introduce the Pass-and-Swap queue, not included
in the above unifying frameworks, but for which the product-form of the steady-
state distribution is preserved. The authors provide several examples that fall
into this framework, including a loss variant of the c.o.s. redundancy model.
The response time has also been studied in limiting regimes such as heavy
traﬃc and mean ﬁeld. Cardinaels et al. [9] consider both c.o.c. and c.o.s. and
establish that in heavy traﬃc the joint distribution of the number of jobs of the
various types converges to the product of an exponentially distributed random
variable times a deterministic vector, a phenomenon known as state-space col-
lapse. Hellemans et al. [24,25] consider the mean-ﬁeld regime and characterize
the stationary workload distribution of c.o.c. with FCFS, general service times
and both identical and i.i.d. copies. In Hellemans et al. [22] the authors gener-
alize the previous result to other redundancy scheduling implementations such
as replication if above certain threshold, delayed replication policy or replicate
small jobs. Another mean-ﬁeld result can be found in Hellemans et al. [23] where
the authors analyze the stationary response time and workload distributions of
JSW(d), JSQ(d) and redundancy-d under FCFS and general service times.

278
E. Anton et al.
5.2
Optimizing Redundancy
The stability results presented in this survey show that both the scheduling
policy and the degree of redundancy can have a big impact on the stability region
and hence on the performance of the system. Motivated by this, researchers
have aimed at i) characterizing what is the optimal scheduling policy in the
servers and ii) determining what is the optimum number of copies that should
be created.
One of the ﬁrst papers studying redundancy was by Koole and Righter [27],
which considered a system where jobs can dispatch i.i.d. copies to any subset of
servers in the system. The authors showed that with FCFS and NWU service
time distributions, the best policy is to replicate to all the servers.
Several optimality results have been derived for the Least-Redundant-First
(LRF) scheduling policy, which serves jobs in lower priority as their number of
copies increases (jobs with the same number of copies are served according to
FCFS). In particular, Gardner et al. [15,18] consider nested redundancy models
with exponential service times and i.i.d. copies, and show that the mean response
time is minimized under LRF. We note that a redundancy model is nested if for
all c, c′ ∈C, either i) c ⊂c′ or ii) c′ ⊂c or iii) c ∩c′ = ∅.
Akgun et al. [1] consider a two-server system in which each server has dedi-
cated traﬃc, that is, each server is a unique compatible server for one job type.
The authors consider the DCF (Dedicated-Customers-First) scheduling policy
and analyze the eﬃciency and fairness for both dedicated and redundant jobs.
Sun et al. [39] consider various low-complexity redundancy scheduling tech-
niques for systems where jobs have i.i.d. copies, and investigate when these are
delay-optimal (or nearly-delay optimal) with respect to the stochastic ordering.
These new scheduling techniques are based on job replication and job cancella-
tion decision features. For instance, the authors show that the fewest unassigned
task ﬁrst with low-priority replication and earliest due date ﬁrst with replication
policies are nearly delay-optimal with NBU and NWU distributions, respectively.
5.3
Related Models
Redundancy as considered in this chapter is closely related to the (n, k) fork-join
system. In the latter, there exist n servers each one receiving one of the blocks,
and the job is completed once k < n blocks are served. If k = 1, this model
becomes equivalent to the redundancy-n model with c.o.c..
For the (n, k) fork-join model, Lee et al. [30] provide sequences of systems
that upper and lower bound the original one, and that converge to the original
system. Through these bounds, the authors characterize the mean response time
of the system. Li et al. [31] derive that in the mean-ﬁeld regime, coding always
improves the mean response time compared to the redundancy model, i.e., (n, 1).
In [26], the authors consider the (n, r, k) partial fork-join system, where the
job is sent to r out of n servers uniformly chosen at random and waits for the
ﬁrst k ≤r to complete. They study eﬀective replication strategies for various
scenarios. The authors show that both latency and cost are minimized when r

Stability of Redundancy Systems
279
increases for log-convex (high variable) service time distributions. Duﬀy et al. [13]
compare the tail response time of the (n, r, k) model to that of the redundancy-d
model (with batch arrivals of size r). The authors show that the tail distribution
of the response time under (n, r, k) partial fork-join is smaller than under the
redundancy-d model as long as r −k ≥d, as the number of servers tend to
inﬁnity.
In a recent paper, Zubeldia [43] considers the S&X model where the slow-
down experienced by each copy in service is independent across servers, but
not necessarily independent from the job’s service time. The author provides
a lower-bound on the mean delay for the (n, r, k) partial fork-join system, and
shows that when slowdowns are exponentially distributed and independent of
the service time of the job, the expected delay is minimized in the mean-ﬁeld
limit for a constant r that only depends on the arrival rate and mean slowdown.
6
Conclusions, Open Problems and Conjectures
The literature on the stability analysis of redundancy systems is recent and
growing. However, there are many important cases that have not been analyzed
yet. In this section, we address some of the open problems related to stability,
and state several conjectures that are based on our intuitive understanding of
the system. It is our hope that this survey might encourage more research on
this relevant and timely topic.
6.1
I.i.d. Copies.
As shown in Proposition 1, FCFS is maximum stable with exponential service
times and i.i.d. copies. We believe that this result should remain valid for any
work-conserving scheduling policy with non-preferential treatment across types,
for instance PS, ROS, LCFS, LAS and SRPT. The reason for this is that the i.i.d
assumption combined with the non-preferential treatment across types permits
to take advantage of diversity when the system is close to saturation.
Conjecture 1. Consider a redundancy system with a general topology with
exponentially distributed service times and i.i.d. copies. For any work-conserving
non-preferential scheduling policy, the system is stable if for all C ⊆C,
λ

c∈C
pc <

s∈S(C)
μs,
where S(C) = 
c∈C{s ∈c}.
Open Problem 1. If we relax the exponential service times to general service
time distribution, the stability condition is unknown.

280
E. Anton et al.
6.2
FCFS Scheduling Policy with Identical Copies
In Sect. 4.1, we saw that λ/μK
<
¯ℓ/K is the stability condition of the
redundancy-d system where jobs have identical copies and exponential service
times.
Open Problem 2. If we relax the redundancy-d structure to general topolo-
gies, or the exponential service times to general service times, the stability con-
dition is unknown.
For exponential service times with the redundancy-d structure, we observed
in Fig. 3 that for a given number of copies d, lim
K→∞
¯ℓ/K < 1. Note that λ/μK < 1
is the stability condition for a system with no redundancy. Hence, if it can be
proved that lim
K→∞
¯ℓ/K < 1, this would imply that as the number of servers grows
large, the traﬃc load that a redundancy system can support is smaller than if
no redundancy was implemented.
Conjecture 2. Consider the redundancy-d model where FCFS is implemented
and jobs have exponentially distributed service times and identical copies. Then,
for ﬁxed d, lim
K→∞
¯ℓ/K < 1.
The limit should coincide with the stability condition given in [24], where
the authors develop a numerical method to derive the stability condition in the
mean-ﬁeld limit.
We also observed the following monotonicity property in the number of
redundant copies. More precisely, we conjecture that as the degree of redun-
dancy increases, the stability region becomes smaller.
Conjecture 3. Consider the redundancy-d model where FCFS is implemented
and jobs have exponentially distributed service times and identical copies. Then,
for ﬁxed K, ¯ℓis decreasing in d, and hence, the stability region is decreasing in d.
6.3
ROS Scheduling Policy with Generic Correlation Structure
In the particular case of ROS, we believe that Conjecture 1 will remain valid
even if copies follow a general correlation structure, including identical copies.
So far, this was only proved for the redundancy-d model with exponential dis-
tributed service times with identical copies, see Proposition 6.
Conjecture 4. Consider a redundancy system with a general topology with
exponentially distributed service times and an arbitrary correlation structure
among copies. ROS is stable if for all C ⊆C,
λ

c∈C
pc <

s∈S(C)
μs,
where S(C) = 
c∈C{s ∈c}.

Stability of Redundancy Systems
281
The intuition would be the following. In principle, multiple copies of the same
job could be served simultaneously at various of its compatible servers. Due to
the heterogeneous capacities and the correlation among the copies, the departure
rate of that job depends on the residual service time of each copy. However, when
the number of jobs in the system grows large, the probability that more than one
copy of the same job is simultaneously in service goes to zero. Using ﬂuid-limit
techniques, as done in [4], one then obtains that the ﬂuid limit of the system
equals that of the system where jobs have i.i.d. copies. Hence, if Conjecture 1 is
valid, this would imply that Conjecture 4 is true as well.
6.4
Redundancy-Aware Scheduling
Another interesting, and so far unexplored area, is the impact of redundancy-
aware scheduling policies on the stability region and the performance of the
system. By redundancy-aware we refer to policies like LRF or Most-Redundant-
First that can use information on the number of copies when choosing which copy
to serve in a server. As discussed in Sect. 5.2, the authors of [15,18] consider the
nested model with exponentially distributed service times and i.i.d. copies and
show that LRF minimizes the mean response time. It would be interesting to
explore this further for more general redundancy settings.
Acknowledgement. Research of E. Anton supported and research of M. Jonckheere
partially supported by the French “Agence Nationale de la Recherche (ANR)” through
the project ANR-15-CE25-0004 (ANR JCJC RACON). U. Ayesta has received funding
from the Department of Education of the Basque Government through the Consoli-
dated Research Group MATHMODE (IT1294-19).
References
1. Akgun, O., Righter, R., Wolﬀ, R.: Partial ﬂexibility in routing and scheduling.
Adv. Appl. Probab. 45(3), 673–691 (2013)
2. Ananthanarayanan, G., Ghodsi, A., Shenker, S., Stoica, I.: Why let resources idle?
Aggressive cloning of jobs with dolly. In: Proceedings of the 4th USENIX Confer-
ence on Hot Topics in Cloud Computing, HotCloud’ 12, Article 17, p. 6 (2012)
3. Ananthanarayanan, G., Ghodsi, A., Shenker, S., Stoica, I.: Eﬀective straggler mit-
igation: attack of the clones. In: Proceedings of the 10th USENIX Conference on
Networked Systems Design and Implementation vol. 13, pp. 185–198 (2013)
4. Anton, E., Ayesta, U., Jonckheere, M., Verloop, I.M.: On the stability of redun-
dancy models. Oper. Res. (2021). https://doi.org/10.1287/opre.2020.2030
5. Anton, E., Ayesta, U., Jonckheere, M., Verloop, I.M.: Improving the performance
of heterogeneous data centers through redundancy. In: Proceedings of the ACM on
Measurement and Analysis of Computing Systems – SIGMETRICS 4(3), Article
48, p. 29 (2020)
6. Ayesta, U., Bodas, T., Dorsman, J., Verloop, I.M.: A token-based central queue
with order-independent service rates. Oper. Res., to appear (2021)
7. Ayesta, U., Bodas, T., Verloop, I.M.: On a unifying product form framework for
redundancy models. Perform. Eval. 127–128, 93–119 (2018)

282
E. Anton et al.
8. Bonald, T., Comte, C.: Balanced fair resource sharing in computer clusters. Per-
form. Eval. 116, 70–83 (2017)
9. Cardinaels, E., Borst, S.C., van Leeuwaarden, J.S.H.: Redundancy scheduling with
locally stable compatibility graphs. arXiv:2005.14566 (2020)
10. Comte, C., Dorsman, J.: Pass-and-swap queues. arXiv:2009.12299 (2020)
11. Dean, J.: Achieving rapid response times in large online services. Google Research
(2012). http://research.google.com/people/jeﬀ/latency.html
12. Dean, J., Barroso, L.A.: The tail at scale. Commun. ACM 56, 74–80 (2013)
13. Duﬀy, K.R., Shneer, S.: MDS coding is better than replication for job completion
times. arXiv:1907.11052 (2019)
14. Foss, S., Korshunov, D., Zachary, S.: An Introduction to Heavy-Tailed and Subex-
ponential Distributions. Springer, NY (2013)
15. Gardner, K., Harchol-Balter, M., Hyytia, E., Righter, R.: Scheduling for eﬃciency
and fairness in systems with redundancy. Perform. Eval. 116, 1–25 (2017)
16. Gardner, K., Harchol-Balter, M., Scheller-Wolf, A., van Houdt, B.: A better model
for job redundancy: decoupling server slowdown and job size. IEEE ACM Trans.
Netw. 25(6), 3353–3367 (2017)
17. Gardner, K., Harchol-Balter, M., Scheller-Wolf, A., Velednitsky, M., Zbarsky, S.:
Redundancy-d: the power of d choices for redundancy. Oper. Res. 65, 1078–1094
(2017)
18. Gardner, K., Hyyti¨a, E., Righter, R.: A little redundancy goes a long way: convexity
in redundancy systems. Perform. Eval. 131, 22–42 (2019)
19. Gardner, K., Righter, R.: Product forms for FCFS queueing models with arbitrary
server-job compatibilities: an overview. Queueing Syst. 96(1), 3–51 (2020)
20. Gardner, K., Zbarsky, S., Doroudi, S., Harchol-Balter, M., Hyyti¨a, E., Scheller-
Wolf, A.: Queueing with redundant requests: exact analysis. Queueing Syst. 83(3–
4), 227–259 (2016)
21. Harchol-Balter, M.: Performance Modeling and Design of Computer Systems:
Queueing Theory in Action. Cambridge University Press, NY (2013)
22. Hellemans, T., Bodas, T., van Houdt, B.: Performance analysis of workload depen-
dent load balancing policies. In: International Conference on Measurement and
Modeling of Computer Systems vol. 3(2), Article 35, p. 35 (2019)
23. Hellemans, T., van Houdt, B.: On the Power-of-d-choices with least loaded server
selection. In: Proceedings of the ACM on Measurement and Analysis of Computing
Systems – SIGMETRICS vol. 2(2), Article 27, p. 22 (2018)
24. Hellemans, T., van Houdt, B.: Analysis of redundancy(d) with identical replicas.
ACM Sigmetrics Perform. Eval. Rev. 46(3), 74–79 (2018)
25. Hellemans, T., van Houdt, B.: Performance of redundancy(d) with identi-
cal/independent replicas. In: ACM Transaction on Modeling and Performance
Evaluation of Computing Systems (TOMPECS), vol. 4(2), Article 9, p. 28 (2019)
26. Joshi, G., Soljanin, E., Wornell, G.: Eﬃcient redundancy techniques for latency
reduction in cloud systems. In: ACM Transaction on Modeling and Performance
Evaluation of Computing Systems (TOMPECS), vol. 2(2), Article 12, p. 30 (2017)
27. Koole, G., Righter, R.: Resource allocation in grid computing. J. Sched. 11, 163–
173 (2007)
28. Krzesinski, A.E.: Order independent queues. In: Boucherie, R.J., van Dijk, N.M.
(eds.) Queueing Networks: a Fundamental Approach, pp. 85–120. Springer, Boston,
MA (2011)
29. Lee, K., Shah, N.B., Huang, L., Ramchandran, K.: When do redundant requests
reduce latency? IEEE Trans. Commun. 64(2), 715–722 (2016)

Stability of Redundancy Systems
283
30. Lee, K., Shah, N.B., Huang, L., Ramchandran, K.: The mds queue: analysing the
latency performance of erasure codes. IEEE Trans. Inf. Theory 63(5), 2822–2842
(2017)
31. Li, B., Ramamoorthy, A., Srikant, R.: Mean-ﬁeld-analysis of coding versus replica-
tion in cloud storage systems. In: IEEE INFOCOM 2016 - The 35th Annual IEEE
International Conference on Computer Communications, pp. 1–9 (2016)
32. Mendelson, G.: A lower bound on the stability region of redundancy-d with FIFO
service discipline. Oper. Res. Lett. 49(1), 113–120 (2021)
33. Paganini, F., Tang, A., Ferragut, A., Andrew, L.: Network stability under alpha
fair bandwidth allocation with general ﬁle size distribution. IEEE Trans. Automat.
Contr. 57, 579–591 (2012)
34. Raaijmakers, Y., Borst, S.C.: Achievable stability in redundancy systems. In: Pro-
ceedings of the ACM on Measurement and Analysis of Computing Systems – SIG-
METRICS, vol. 4(3), Article 46, p. 21 (2020)
35. Raaijmakers, Y., Borst, S.C., Boxma, O.: Redundancy scheduling with scaled
Bernoulli service requirements. Queueing Syst. 93, 67–82 (2019)
36. Raaijmakers, Y., Borst, S.C., Boxma, O.: Stability of redundancy systems with pro-
cessor sharing. In: Proceedings of the 13th EAI International Conference on Per-
formance Evaluation Methodologies and Tools, Valuetools 20, pp. 120–127 (2020)
37. Ross, S.M.: Stochastic Processes. Wiley & Sons, NY (1996)
38. Sieber, C., Blenk, A., Hinteregger, M., Kellerer, W.: The cost of aggressive http
adaptive streaming: quantifying youtube’s redundant traﬃc. In: 2015 IFIP/IEEE
Intern. Symp. on Integrated Network Management (IM), pp. 1261–1267 (2015)
39. Sun, Y., Koksal, C.E., Shroﬀ, N.B.: On delay-optimal scheduling in queueing sys-
tems with replications. arXiv:1603.07322 (2016)
40. Visschers, J., Adan, I., Weiss, G.: A product form solution to a system with multi-
type jobs and multi-type servers. Queueing Syst. 70, 269–298 (2012)
41. Vulimiri, A., Godfrey, P.B., Mittal, R., Sherry, J., Ratnasamy, S., Shenker, S.:
Low latency via redundancy. In: Proceedings of the ACM Conference on Emerging
Networking Experiments and Technologies, pp. 283–294 (2013)
42. Vulimiri, A., Michel, O., Godfrey, P.V., Shenker, S.: More is less: reducing latency
via redundancy. In: Proceedings of the 11th ACM Workshop on Hot Topics in
Networks, HotNets’11, vol. 11, pp. 13–18 (2012)
43. Zubeldia, M.: Delay-optimal policies in partial fork-join systems with redundancy
and random slowdowns. In: Proceedings of the ACM on Measurement and Analysis
of Computing Systems – SIGMETRICS, vol. 4(1), Article 2, p. 49 (2020)

IBM Crew Pairing and Rostering
Optimization (C-PRO) Technology
with MDP for Optimization Flow
Orchestration
Vladimir Lipets and Alexander Zadorojniy(B)
IBM Research - Haifa, Haifa, Israel
{lipets,zalex}@il.ibm.com
Abstract. We created the IBM Crew Pairing and Rostering Optimiza-
tion (C-PRO) solution for air crew scheduling. It was deployed at El Al
in 2013 and at Aeroﬂot in 2020. The core of the system is an optimiza-
tion ﬂow, which models the problem using mixed integer linear program-
ming (MILP) with millions of integer variables. The solution is derived
iteratively using heuristics. Most recently, we applied Markov Decision
Process (MDP) in place of the heuristics orchestrator and realized a 30%
improvement in performance.
Keywords: Markov decision process (MDP) · Mixed integer linear
programming (MILP) · Scheduling
AMS(2020) Subject Classiﬁcation: Primary 90C40 · Secondary
90C11
1
Introduction
1.1
Airline Crew Pairing and Rostering Problem
Assigning airline crews to ﬂights – what is commonly referred as pairing and
rostering – is an extremely complex problem that is also very well-studied [2,
9,10]. A pairing is a sequence of ﬂight legs that start and end at the same
location where the crew members live (Fig. 1). It typically spans between one
and ﬁve days; however, in some cases it can be more than one week in duration.
To create assignments for crew members, airline planners start by generating
pairings to cover as many ﬂight legs as possible, with a cost as low as possible.
For each pairing, they specify which types of crew members (e.g., captains,
ﬁrst oﬃcers, ﬂight attendants, etc.) are required and at what quantity, which
is known as a “crew complement” (Fig. 2). Once optimal pairings and their
crew complements are deﬁned, the next phase is to assign crew members to
the pairings, while maintaining compliance with a variety of work regulations
c
⃝The Author(s), under exclusive license to Springer Nature Switzerland AG 2021
A. Piunovskiy and Y. Zhang (Eds.): Modern Trends in Controlled Stochastic Processes,
ECC 41, pp. 284–297, 2021. https://doi.org/10.1007/978-3-030-76928-4_14

IBM MDP CPRO
285
Fig. 1. Pairing Diagram. A ‘duty’ consists of four duties. In the ‘Deadhead Leg’, crew
members ﬂy from their point of origin as regular passengers to the actual start of their
duty. In the “Standby Leg,” the crew members are in standby mode. Next, in the ‘2
Flight Legs’ duty, the crew members ﬂy two ﬂights with a short connection between
them. Finally, in the ‘Flight Leg’ duty, the crew arrives back to their point of origin.
and collective agreements. This part of the problem is called crew assignment.
To solve this problem properly, pairings, rest periods, training periods, annual
leaves, and so forth, must be taken into account to create working schedules
(rosters) for crew members (Fig. 3).
Technology that creates an optimized schedule for ﬂight crews can provide
signiﬁcant savings to airlines. The beneﬁts go beyond cost savings. An equitable,
well-planned and eﬃcient crew roster contributes to ﬂight safety and employee
satisfaction. The challenge is that mainstream airline rostering solutions typ-
ically take days or weeks to generate a single plan for pairing and rostering,
which is typically generated ahead of every month. They also require optimiza-
tion experts to implement any changes to the optimization logic. Researchers at
IBM developed a solution that delivers optimized pairing and rostering (prod-
uct named, ‘C-PRO’) with unparalleled speed and ﬂexibility. Planners can create
assignments and implement many types of changes without relying on optimiza-
tion experts. It is also faster: IBM’s C-PRO can execute multiple iterations in
a single day. This makes it easier to create the best option by enabling cus-
tomers to rapidly improve in quick increments. They can also support changing
business requirements, such as complying with new regulations, and meeting spe-

286
V. Lipets and A. Zadorojniy
cial requests from crew personnel. The optimized crew scheduling also supports
“what if” analysis to predict the impact of various changes; for example, working
hours, vacations, etc. It also provides intuitive explanations, which are presented
in terms of business objects and logic that ordinary users can understand.
Fig. 2. Pairing Diagram from C-PRO. The three ﬁelds ‘Required/Assigned/Open’ show
the values of ‘2/2/0’. This means crew complement requires two crew members, two
crew members have been assigned by the optimization engine already, and there are
zero open positions. Also shown here: 10.50 h is a ﬂight duration, 159.33 is the total
pairing time in hours, P310320 is the pairing id, and ABC is the starting point.
1.2
Challenges We Faced Working on C-PRO
1. Extremely large size of the problem.
Crew scheduling is a very large problem in many aspects:
– There are hundreds of domain-related objects (e.g., stations, regions, legs,
shifts, etc.) that needed to be described mathematically.
– There are dozens of diﬀerent types of requirements that must be described
mathematically and satisﬁed.
– MILP formulation of the problem consist of many millions of integer vari-
ables and constraints; and therefore, cannot be solved ‘as is’ by available
solvers.
2. Rapid turnaround of requests for update by clients.
Customers want to make changes quickly, but in existing solutions, modifying
rules and constraints typically can take weeks or even months. Optimization

IBM MDP CPRO
287
Fig. 3. Rostering from C-PRO. This ﬁgure shows the rostering for each crew member by
crew member ID. The summary ﬁelds represent (in order from left to right) number of
assignments, number of ﬂights, ﬂight time (credit), staying abroad time, average ﬂights
durations performed, number of ﬂight days, available days to work, and utilization per
scheduling period.
experts need to formulate new business rules as new constraints and incorpo-
rate the new model into the application. For the customer, this means waiting
until the next update to obtain the desired modiﬁcation.
3. Explainability.
In the Enterprise Optimization domain, in order to be useful in real world
applications, optimization solution must be interpretable. The system which
cannot ‘explain’ to the end-user (who are domain experts) how tradeoﬀs were
made will not be able to earn trust and will not win adoption by customers.
4. Multi-problem solution capabilities.
Crew pairing and rostering is not a single type of problem. To address eﬀec-
tively, several types of problems need to be considered, including:
– Coverage problem, which ﬁnds optimized set of pairings for covering all
activities.
– Assignment problem, which assigns crew members to the pairings.
– Routing problem, which ﬁnds optimal route for the airplanes, taking into
account their location and required maintenance.
– Shift scheduling problem, which schedules crew member to weekly shifts.
– Personal requests biddings, which optimizes assignment of crew mem-
bers according to personal preferences resolving conﬂicts in an optimized
manner.
5. Reusability.
Development of enterprise level optimization solution is a signiﬁcant invest-
ment. Reusability of the solution for other clients in the same domain (or
clients in adjacent domains) is vital for the market success of the solution.

288
V. Lipets and A. Zadorojniy
2
MILP Problem Formulation
While creating pairings and assignments, we require that
– the cost of pairings should be kept in the minimum,
– pairings must be legal according to speciﬁed regulations,
– all ﬂights and other activities must be assigned exactly once.
This optimization problem is modeled as a MILP problem where pairings are
the variables of the problem, which are either assigned zero (not selected) or one
(selected). Costs of the pairings are the coeﬃcients of the variables. Covering the
ﬂights exactly once is formulated as a constraint of the optimization. Therefore,
min

j∈P
cj · xj
s.t.
xj ∈{0, 1}, ∀j ∈P

j∈P Fi
xj = 1, ∀i ∈F
where P is the set of all possible pairings, F is the set of all ﬂights. ∀i ∈F,
PFi denotes set of pairings containing ﬂight i ∈F. cj is the cost of the pairing
j ∈P. Notice that when xj is assigned to one, it means that pairing j was
selected. The objective is to minimize the total cost of selected pairings. The
constraints guarantee that each ﬂight is covered only once. Although this is a
‘vanilla’ problem formulation, the problem is complicated. There is a signiﬁcant
challenge to eﬀectively build the set P of legal pairings. As the number of ﬂights
increases, the number of potential pairings grows exponentially. To deal with
this, we apply diﬀerent graph theory-based algorithms in the earlier stages to
remove candidates that have a small chance to be selected. Also, each pairing can
consist of a set of duties, each duty may consist of ﬂights, and each ﬂight may
appear only in one duty. Moreover, the crew (captain, ﬁrst oﬃcer, etc.) required
for each duty may change according to diﬀerent properties of the duty such as
ﬂight duration, period of the day, airplane type, etc. For instance, duties with
durations more than 12 h that start in the morning may require two captains
and one ﬁrst oﬃcer; whereas, similar duties at night require two captains and
two ﬁrst oﬃcers. The formulation in this case is as follows:
min

j∈P,r∈R
cj,r · zj,r
s.t.
zj,r ∈Z+, ∀j ∈P, ∀r ∈R
yk ∈{0, 1}, ∀k ∈D

k∈Di
yk = 1, ∀i ∈F

j∈P Dk
zj,r = qk,r · yk, ∀k ∈D, ∀r ∈R

IBM MDP CPRO
289
where R is the set of all ranks, D is the set of all duties, Di denotes set of
duties containing ﬂight i ∈F, PDk denotes set of pairings containing duty
k ∈D, c(j, r) is the cost of the pairing j ∈P for rank r, q(k, r), is the number
(constant) of crew members of rank r required for duty k ∈D. Notice that
when yk is assigned to one, it means that duty k was selected, and zj,r obtains a
positive value, which refers to the number of crew members of rank r scheduled
to pairing j. The objective is to minimize the total cost of the selected pairings
with respect to the ranks. The constraints guarantee that each ﬂight is covered
exactly once by the duty and that each selected duty is covered by the pairings
according to the duty and rank requirements. Extended business rules may be
applied on top of these rules, which for example, may require certain number of
pairings with given properties. For instance, assuming property ‘start on base
B’ we may require a proper balance between staﬃng level on base B and the
number of created pairings starting from this base. Moreover, if we introduce
the conception of acclimatization, which means that qk,r will depend not only
on duty k, but also will be a function of the time passed after the previous duty
of the pairing.
After determining pairings, the next phase is to assign crew members to exe-
cute these pairing for a given time period, while complying with a variety of
work regulations and collective agreements. This is the crew assignment prob-
lem, where pairings, rest periods, training periods, annual leaves, etc., are com-
bined to form working schedules for crew members. The classical MILP problem
formulation for solving assignment problem is based on the assignment of ae,p
decision variable for one or zero, if employee e is assigned to pairing p or not,
respectively. The problem becomes more complicated when we need to satisfy
more advanced rules and constraints. For example, decision variables ae,p are
replaced by variables ae,p,r,k, where additional parameters of rank r and role k
of the assignment are introduced. In another example, we have a rule deﬁning
duration of non-working period within a ﬂoating time window, or set of rules
asserting that if some crew member did pairing of type A in the middle of the
month, then they have to execute pairings of type C within the last day of the
same month. This problem is formulated as MILP. For the sake of compactness,
we skip its detailed formulation.
3
The Solution
In this paper we mostly address Challenge number 1. The core of the system is a
complex optimization ﬂow which models a problem as a largescale mixed integer
linear programming problem (MILP) with millions of integer variables and solves
it using diﬀerent type of algorithms. No MILP solver on the market can solve
a problem of this size in a reasonable time. To create a solution, we used an
approach [5] which allows incorporation of multiple heuristics, including ‘business
heuristics’ and ‘business decompositions’. The uniqueness of this approach, as
opposed to the well-known column generation, is that ‘business decompositions’
take into account a business characteristic of a client objects while the column

290
V. Lipets and A. Zadorojniy
generation would not. ‘Business heuristics’ leverage the structure of input data
to build the schedule in a more eﬃcient way.
The novelty in our approach is applying machine learning as an automated
tool to continuously seek the best problem decomposition and modiﬁcation strat-
egy. Each of the heuristics in the ﬂow is important for the solve to succeed
overall. It starts from a heuristic that ﬁnds an initial feasible solution. Next, a
cruncher heuristic [6] improves the feasible solution iteratively as much as pos-
sible. Finally, polishing is run to improve the solution even further using more
‘delicate’ operations (see Appendix B for more details).
The entire ﬂow is controlled by an orchestrator. The orchestration of the
optimization ﬂow (e.g., which heuristic, when, and with what parameters to
run) is crucial. Finding the right strategy can be a very complex and time-
consuming process. Moreover, a new strategy may be required for each new
deployment in a new domain or even for a diﬀerent type of problem in the same
domain. To address this, we automated the process by using Markov Decision
Process (MDP) framework (see Appendix A for more details). In doing so, we
changed our approach to the heuristics from the rules-based approach which
we implemented initially. In the rule-based approach, the three aforementioned
heuristics are run in series: feasible solution ﬁnder, the cruncher, and the polisher.
When we applied MDP to the process instead, the cruncher and the polisher are
modeled as a single MDP model with an extended set of actions. In this case,
there is no order between the cruncher and the polisher: the order is solely
prescribed by the policy.
The MDP state variables consists of CPLEX time per iteration, gap to opti-
mality, current objective value, and convergence rate. Action space consists of
the number of unﬁxed unassigned integers, the number of unﬁxed assigned inte-
gers and per iteration time upper bound. Immediate cost is deﬁned as relative
objective improvement between consecutive iterations. Transition probability
matrix is estimated from the C-PRO runs using real data (see Appendix B for
the details) from clients, which is interpolated using math properties of the state
features (e.g., continuality, absorbing state knowledge). Reward per state-action
is the relative objective improvement between consecutive iterations. During the
run, the state of the run is estimated, and an optimal action is applied for the
next iteration using pre-solved MDP policy (Fig. 4).
4
Results and Conclusions
We applied the MDP ﬂow orchestrator to the C-PRO deployment at Aeroﬂot
(see Appendix C). The MDP ﬂow orchestrator was run on Airbus A320 and A321
ﬂeets, which consists of over 100 aircraft and over 1,000 employees to be sched-
uled. The MDP ﬂow orchestrator outperformed the existing rule-based orches-
trator by roughly 30% in speed. Additionally, it is more ﬂexible to apply to new
domains or new problems in the same domain. Our future research directions are
focused on building more eﬀective optimization ﬂow concepts, including: a multi-
ﬂow parallelism, incorporating advanced constraints formulations into MDP and

IBM MDP CPRO
291
Fig. 4. Snapshot of Running Trace. This snapshot showing a trace of the run using
MDP policy for orchestration applied for A320 and A321 instance of Aeroﬂot. The ﬁrst
two columns show number of iteration and coverage of the requirements, respectively.
The next four columns correspond to state variables and last three columns correspond
to action variables.
Deep RL, and more automation (with less skill required) for optimization ﬂow
generation and orchestration.
5
Discussion
In this section we describe what we learned from working with real customers
in enterprise optimization area.
– Lesson 1. Improving optimization means improving the entire stack.
Every component of the system needs to be tuned to deliver an eﬀective solu-
tion in enterprise optimization. There is a big gap between an ‘academic’
optimization solution and the real-world. Solving real-world enterprise opti-
mization problem is a ‘multi-dimensional task’. It’s not just optimization,
it’s also: rules description, explainability, short turnaround process and user
interface. This is diﬀerent from a strictly academic approach which mainly
focuses on optimization algorithms. To improve the whole solution, one must
improve the whole ‘stack’: extract, transform, load (ETL) process, ﬂexibil-
ity rules description, and explainability features. Flexibility on rules means
that domain expert can add and modify rules that signiﬁcantly change the

292
V. Lipets and A. Zadorojniy
optimization problem. For example, summer and winter have diﬀerent rules
for crew scheduling. In C-PRO, we enabled conﬁgurable rules which end-
users (domain experts) could modify that would automatically update the
optimization problem.
– Lesson 2. Explainability.
The solution must be self-explainable. Customers will adopt systems not only
because they provide optimal solutions. Customers also demand that the
system can explain and “defend” its choices. Enterprise optimization cannot
be a black box. To achieve this, we applied multiple techniques, including a
verbosity engine, key performing indicators (KPIs) and monitoring tools.
– Lesson 3. Separation between business and algorithmic logic.
We need modularity for large-scale optimization. Input can frequently change
(e.g., format, new features) but this should be transparent to the math model.
Otherwise, the system becomes brittle in the face of change. In C-PRO, we
accomplished this with modules for ‘business objects’, ‘business logic’, and
‘math objects’. We also enabled interim explainability on these objects to
enable customers to see the results of the business logic separate from the
entire optimization. By adding simple declarations in our code, we make the
results of the business logic reviewable by users.
– Lesson 4. Optimization model manageability.
To handle the math model more eﬀectively, we implemented the math model
as a type of database that includes constraints, variables, equations, collectors
(predeﬁned construction for deﬁning sums), indicators (predeﬁned construc-
tion for deﬁning lower and upper bounds), and penalties (predeﬁned construc-
tion for deﬁning objectives). We used business objects as a key for accessing
these math objects. We deﬁned an Application Programming Interface (API)
to access and modify the math model. For instance, we can apply a query
which selects all variables associated with optimization ﬂow orchestration.
– Lesson 5. Reusability.
From the beginning, reusability has been a goal. To recall, in the rule-based
ﬂow orchestration all three heuristics – feasible solution ﬁnder, the cruncher,
and the polisher – must run and in a particular order. When moving to
a new problem, we would need to reconﬁgure a rule-based ﬂow accordingly.
However, with the machine learning orchestration, the algorithm itself adjusts
automatically to the new problem using available data.
Acknowledgment. The authors would like to thank Donny Rose for numerous fruit-
ful discussions.
7
Appendix A - MDP Framework
7.1
Deﬁnition of MDP
An MDP [8] is a 4-tuple ⟨X, U, P, c⟩, where X = {0, . . . , n −1} is a ﬁnite set
of states, U = {0, . . . , k −1} is a ﬁnite set of actions, P : X2 × U →[0, 1] is
a transition probability function, and c : X × U →R is a cost function. The

IBM MDP CPRO
293
probability of transition from state x to state y when action u is chosen is
speciﬁed by the function P and denoted by P(y|x, u). The cost associated with
selecting the action u when in state x equals c(x, u). We often refer to the cost
function as a vector c ∈Rnk. We denote initial states by x0. In fact [8], implies
that the initial state does not aﬀect the optimal policy.
Time is discrete, and in each time unit t, let xt denote the random variable
that equals the state at time t. Similarly, let ut denote the random variable
that equals the action selected at time t. A non-stationary policy is a function
π : X × U × t →[0, 1], such that 
u π(x, t, u) = 1 for every x ∈X for each
time unit t. A stationary policy is a function π : X × U →[0, 1], such that

u π(x, u) = 1 for every x ∈X. A policy controls the action selected in each
state as follows: the probability of selecting action u in state x equals π(x, u).
A policy can be either randomized or deterministic. A randomized policy is a
policy with a state xi for which π(xi, u) > 0 for more than one action u. A
deterministic policy is a policy where for all states x ∈X, there is exactly one
action u ∈U such that π(x, u) = 1. The initial state together with a policy
determine a probability measure on states and actions. The goal is to ﬁnd a
policy that minimizes the cost C(π) deﬁned below. We consider a discounted
cost model with inﬁnite horizon throughout the paper.
Discounted Cost Model. In the discounted cost model, the parameter β ∈(0, 1)
speciﬁes the rate by which future costs are reduced. Let P π(xt = x, ut = u)
denote the probability of the event xt = x and ut = u when the initial state
equals x0 (once set, remains unchanged and omitted from the notation) and the
policy is π. The inﬁnite horizon discounted expected cost C(π) is deﬁned by
C(π)
△= (1 −β) ·
∞

t=0
βt · Eπ[c(xt, ut)].
Occupation Measures. Every policy π induces a probability measure over the
state-action pairs. We call this probability measure the occupation measure cor-
responding to π and denote it by ρπ such that ρπ(x, u)
△=(1−β)·∞
t=0 βt·P π(xt =
x, ut = u) (for simplicity we will omit π from the denotation of ρ).
Given an occupation measure ρ(x, u) over X ×U, the policy πρ induced by ρ
is deﬁned by πρ(x, u)
△=ρ(x, u)/ 
u′ ρ(x, u′). (Note that if 
u′ ρ(x, u′) = 0, then
one may deﬁne πρ(x, u) arbitrarily as long as 
u πρ(x, u) = 1.) A cost can be
rewritten using occupation measure notations such as
C(π)
△=

x∈X,u∈U
c(x, u) · ρπ(x, u).
7.2
Transition Probability Matrix Estimation
We start with a data set from a client. The problems are large and take time to
solve (every iteration takes between 10 to 20 min); and therefore, the amount of

294
V. Lipets and A. Zadorojniy
data samples that can be collected in a reasonable time is limited. We interpolate
the client data using domain knowledge information. The augmentation is used
to generate a transition probability matrix [11] of MDP, combining available his-
torical data and domain knowledge information. Mainly due to continuality of
state variables, we use neighboring state interpolation for the domain knowledge
augmentation. We use an absorbing state check to help eliminate misleading
samples subject to shortage in data availability. We collect new data samples
every time the orchestrator is applied, which we use to augment the historical
batch of data and recalculate an MDP transition probability matrix. In Aeroﬂot
use-case, discretizing the variables, we got 24 states and 92 actions. This resulted
in 24 × 2208 matrix with total of around 53000 entries. These entries were esti-
mated by using just of around 500 ‘real’ samples from C-PRO.
7.3
Linear Programming (LP) Formulation
Publications from the 1960s [3,4,7] proved that MDP can be formulated as an
LP problem. They also proved that there is a stationary optimal deterministic
policy for MDP. Below we show an LP dual formulation for MDP, that appears
to be useful in real-life applications. To formulate the LP, we switch to vectorized
representation such that c and ρ are vectors of length of |X|·|U|, P is a transition
probabilities matrix with |X| rows and |X|·|U| columns, I is an identity matrix,
(1−β, 0, 0, 0 . . . , 0) is a vector that represents the initial states distribution where
1 −β corresponds to state x0, and |X| · |U| rows. To solve the LP problem we
used CPLEX solver.
min
ρ cT · ρ
s.t.
(I −β · P) · ρ = (1 −β, 0, 0, 0 . . . , 0)T
ρ ≥0
8
Appendix B - Building Blocks of the Optimization
Flow
There are three custom heuristics: feasible solution ﬁnder, the cruncher, and the
polisher.
The Feasible Solution Finder. The feasible solution ﬁnder consists of two parts:
– Labeling diﬀerent types of constraints into groups (e.g., pairing cover con-
straints, fairness constraints, etc.).
– Iteratively adding constraints by label to MILP such that all are incorporated,
and a feasible solution is found.

IBM MDP CPRO
295
The Feasible Solution Finder is rule-based (rules how to apply labels and when
to add them to MILP) have been coded based on trail-and-error in creating the
C-PRO solution).
The Cruncher. The cruncher heuristic improves the feasible solution from the
previous stage. It ﬁxes and unﬁxes assigned/unassigned integer variables itera-
tively. In C-PRO, the optimization problems is represented as matrix, in which
each resource (pilot, ﬂight attendant) is either ‘assigned’ or ‘unassigned’ to a
duty. The ‘original’ Cruncher is rule-based and the number of ﬁxed and unﬁxed
variables, run time per iteration, etc., is set by a rule-based orchestrator created
based on trail-and-error in creating the C-PRO solution.
Polisher. The polisher heuristic works similarly to the cruncher but with the
number of variables that are unﬁxed per iteration is generally smaller than in
the cruncher. Instead of working by percentage, it works with tasks which are
more ‘gentle’ (e.g., instead of ‘20% of assigned variables to be unﬁxed to the next
iteration’, the Polisher would say, ‘2 assigned tasks per employee to be unﬁxed
for the next iteration’). Like the Cruncher, the ‘original’ Polisher is operated by
a rule-based orchestrator.
In the rule-based application of three types of heuristics, called one after the
other as it appears on Fig. 5.
Fig. 5. Rule-based order of heuristics applied. First: Feasible solution ﬁnder, Second:
The Cruncher, Third: Polisher.
When MDP is used for orchestration, heuristics type and input conﬁguration
are determined by the MDP.

296
V. Lipets and A. Zadorojniy
9
Appendix C - Aeroﬂot Letter
See Fig. 6.
Fig. 6. Aeroﬂot letter.
References
1. Altman, E.: Constrained Markov Decision Processes. CRC Press, Boca Raton
(1999)
2. Andersson, E., Housos, E., Kohl, N., Wedelin, D.: Crew pairing optimization. In:
Yu, G. (ed.) OR in Airline Industry, pp. 228–258. Kluwer Academic Publishers,
Boston (1999)

IBM MDP CPRO
297
3. de Ghellinck, G.: Les problemes de decisions sequentielles. Cahiers Centre d’Etudes
Recherche Operationnelle 2, 161–179 (1960)
4. Depenoux, F.: A probabilistic production and inventory problem. Manage. Sci.
10(1), 98–108 (1963)
5. Katz, M., Lipets, V., Masin, M., Moshkovich, D., Wasserkrug, S.E.: Reusable Mod-
eling for Solving Problems. US Patent US20170337042A1 (2017)
6. Lipets, V., Schiloach, Y.: Reusable Modeling for Solving Problems. US Patent
US8554704B2 (2010)
7. Manne, A.: Linear programming and sequential decisions. Manage. Sci. 6(3), 259–
267 (1960)
8. Puterman, M.: Markov Decision Processes: Discrete Stochastic Dynamic Program-
ming. Wiley, New York (1994)
9. Ye, X.: Airlines’ crew pairing optimization: a brief review technical report. Depart-
ment of Applied Mathematics and Statistics, Johns Hopkins University (2007)
10. Yu, G., Thengvall, B.G.: Airline optimization. In: Pardalos, P.M., Resende, M.G.C.
(eds.) Handbook of Applied Optimization, pp. 689–703. Oxford University Press,
New York (2002)
11. Zadorojniy, A., Shwartz, A., Wasserkrug, S., Zeltyn, S.: Operational optimization
of wastewater treatment plants: a CMDP based decomposition approach. Ann.
Oper. Res. (2016). https://doi.org/10.1007/s10479-016-2146-z

A Regulatory Principle for Robust
Reciprocal-Time Decay of the Adaptive
Immune Response
Anthony Almudevar(B)
Department of Biostatistics and Computational Biology, University of Rochester,
Rochester, NY 14642, USA
anthony almudevar@urmc.rochester.edu
Abstract. Follicular dendritic cells (FDC) play a crucial role in the
regulation of immunity. They are believed to be responsible for long-
term persistence of humoral antibody following vaccination or infection,
due to their role in antibody response induction and their ability to
retain antigen for long periods. In this paper, a regulatory control model
is described which links persistence of humoral immunity with cellular
processes associated with FDCs. The model predicts universal and stable
reciprocal-time (= 1/t) decay of humoral antibody levels, which has been
widely reported over a range of ages, observation times and vaccine types.
Keywords: Control model · Power-law decay · Homeostasis ·
Adaptive immune response
AMS(2020) Subject Classiﬁcation: Primary 93C95 · Secondary
37N25 · 92C37
1
Introduction
The observation of humoral antibody (Ab) concentrations following vaccination
permits the estimation of post-challenge Ab kinetics, and many such studies
are reported in the literature. One important advantage of these studies is that
observations can be time synchronized to measure Ab decay from a common
challenge starting time. The dynamics of Ab response Ct in time t ≥tmin are
commonly observed to be driven by a period of rapid increase to peak levels,
followed by prolonged periods of decay. This decay process is widely reported to
conform to a power-law decay model
Ct
Cs
=
 t
s
k
, s, t ≥tmin,
(1)
for some k < 0 (see [4,10,21,22]). Power-law decay of Ab response was formu-
lated as a model in some detail in [12], where it was noted that k was close to
−1 in the several examples given for which statistical estimates were available.
c
⃝The Author(s), under exclusive license to Springer Nature Switzerland AG 2021
A. Piunovskiy and Y. Zhang (Eds.): Modern Trends in Controlled Stochastic Processes,
ECC 41, pp. 298–312, 2021. https://doi.org/10.1007/978-3-030-76928-4_15

A Regulatory Principle for Robust Reciprocal-Time Decay
299
In some cases, Ab decay appears to include a nonzero asymptote, probably
due to long-term Ab production by plasma cells which have migrated to bone
marrow (see [5,7,9]). In this case, observed Ab decay is probably a superposition
of two processes,
Ct
Cs
= hs(t) + ν,
(2)
where hs(t) represents the adaptive immune response, which decays to zero, and
ν represents longer term Ab production due to plasma cells. Then the power-law
decay proposed in [12] would be represented by the component hs(t).
Thus, estimation of decay rates must anticipate a nonzero asymptote ν using
appropriate statistical methods. In [1] a literature review of Ab response studies
was undertaken with the purpose of validating the power-law decay model with
the inclusion of a nonzero asymptote ν. Of the 13 Ab time series examined, two
exhibited no variation over time (as a consequence of a poor vaccine response),
while the remaining 11 conformed very closely to the power-law decay model,
with exponent k = −1.
Identifying the regulatory principle by which the immune response terminates
is an important open problem, since unregulated Ab persistence is physiologi-
cally harmful, which is what characterizes auto-immune disease (for a recent
discussion of the issue see [15]). If the adaptive immune response possesses a sin-
gle decay rate for all infection types, this may be a ﬁngerprint of an important
regulatory principle. On the other hand, empirical observations of power-law
decay are often the result of some artifact, rather than the direct observation of
a dynamic law deducible from ﬁrst principles.
We give a brief outline of this paper. The question of the empirical observation
of power-law decay as artifact is considered in Sect. 2. If we were to accept power-
law or reciprocal-time decay as a true model of decay, the question then arises as
to the type of model that would be needed to predict that form of decay. In Sect. 3
we argue that the properties of power-law decay force a careful consideration of
the class of model which would be appropriate. In Sect. 4 we describe a control
model for the regulation of the adaptive immune response proposed in [1]. The
model is based on the functionality of follicular dendritic cells (FDC), which
are found in the B-cell follicles of secondary lymph nodes, the primary site of
the adaptive immune response. The model possesses reciprocal-time decay as a
stable attractor. While the attractor is maintained by homeostatic control, at
a higher level the control model does not rely on feedback. Rather, an FDC
population provides open-loop control by functioning as a timer. The model is
demonstrated by computer simulations in Sect. 5, with a discussion following in
Sect. 6.
2
Empirical Observations of Power-Law Decay
In the literature the term “power-law decay” is used to describe both decay in
a dynamic process Ct and a probability distribution. Of course, the two can be
equated. Given normalization Ct0 = 1, we may set survival curve P(T > t) = Ct

300
A. Almudevar
as the population proportion surviving beyond time t, where T is the cell survival
time. The density function fT of T is the derivative of −P(T > t), so given
r > 0 in (1), we have fT (t) ∝1/tr+1. Therefore, under power-law decay of rate
r survival times possess a Pareto density with parameter r + 1. This means the
commentary on power-law frequencies is generally relevent to power-law decay,
with respective decay rates r + 1 and r.
There are many explanations of empirical power-law oﬀered in the literature.
We next review a few of these.
A Consequence of Underpowered Statistical Analysis. A power-law
between two quantities y ∝xα can be discerned from paired observations (xi, yi),
i = 1, . . . , m by the double log transform log y = α log x + C, which under the
power-law hypothesis will be a straight line. Thus, a linear log-log plot is widely
accepted as a ﬁngerprint of power-law decay. This means that the power-law is
widely accepted as a null hypothesis, if only implicitly. This is analogous to the
common practice of testing for normality in statistical modeling. The diﬀerence
here is that normality is theoretically justiﬁed by the central limit theorem as
the aggregation of additive noise, whereas there is often little ﬁrst principles
justiﬁcation for accepting the power-law as a null hypothesis. As argued in [6],
when data is compatible with a power-law, it may be compatible with any num-
ber of alternative heavy tailed distributions. Therefore, widespread reports of
power-law decay may be partly explained by its acceptance as a null hypothesis.
In [6] 24 data sets reported to conform to the power-law distribution were rean-
alyzed. The power-law was ruled out in 7 of these using a goodness-of-ﬁt test. Of
the remaining data sets, only one (distribution of frequencies of word occurence
in the English language) was convincingly power-law, in the sense that a set of
alternative densities could be rejected.
A Consequence of Aggregation. The exponential model of decay is given
by
Ct
Cs
= e−μ(t−s) t > s,
(3)
where μ is a positive constant. Some models accept (3), but explain empirical
power-law decay as an artifact of observation. One widely reported version of
this eﬀect is the rate mixture model. If f(μ) is a gamma density with shape and
rate parameters α, τ then
 ∞
μ=0
e−μtf(μ)dμ =
1
(1 + τt)α .
(4)
In [17] it is assumed that human memory decays exponentially for individuals,
but with some population variation in rates. Thus, observed power-law decay at
the population level is simply an artifact of statistical averaging over a popula-
tion. Another version of this process is reported in [19], based on observations
of exponentially growing processes at random times. It is ﬁrst noted that if we

A Regulatory Principle for Robust Reciprocal-Time Decay
301
are given a deterministic exponential growth process X(t) = exp(μt) and T is
an exponentially distributed observation time, then X(T) has a power-law dis-
tribution. The idea is extended to a number of stochastic processes commonly
used in modeling which exhibit exponential growth. This raises the possibility
that empirical power-law decay in Ab response studies is due to the averaging
of individual times series with imperfectly synchronized observation times. In [9]
the aggregation model of Eq. (4) was proposed to explain empirical power-law
decay in post-vaccine Ab concentrations, assuming signiﬁcant heterogeneity of
decay rates within the immune response.
A Consequence of Partial Decay. Another reason that exponential decay
may resemble power-law decay is that some portion c of the original concentra-
tion Ctmin is protected from decay. This yields the relationship
(Ct −c) = (Cs −c)e−μ(t−s)
(5)
which would yield exponential decay with an asymptote other than 0. It is easy
to see how this may empirically resemble the much slower power-law decay. This
model was suggested as one of several possible explanations for the empirical
power-law decay of memory function in [26]. In that article (which can be espe-
cially recommended), it is also proposed that power-law observations may follow
from nonlinear measurement of memory function, in particular, that a measure-
ment scale may be less sensitive at lower function. Heterogeneous aggregation of
the type described above is also given as a putative explanation, as well as the
possibility that power-law decay truly is a ﬁrst principles model of memory loss
(see [13,25]).
Thus, when given empirical observation of power-law decay, the possibilities
enumerated above must be considered. Regarding the question speciﬁcally of
Ab decay, it is always possible to model the asymptote ν in Eq. (2) in order
to study the remaining component hs(t) (see [1]). Regarding aggregation, that
eﬀect would explain power-law decay, but not speciﬁcally reciprocal-time decay
implied by k = −1. However, ultimately, to accept power-law decay of the Ab
response, a plausible model must be proposed.
3
Autonomous versus Non-autonomous Dynamic
Systems
We take an autonomous dynamic system to be one in which the dynamic law
is unchanging in time. This characterizes models of biochemical systems with
static decay or interaction rates. This type of model is commonly used to model
the immune response.
For example, in [23] the eﬀect of variable or dynamic vaccine dose admin-
istration on Ab response was studied experimentally, and compared to compu-
tational model predictions. The model includes as state variables CAg, CIgG,
CIgM, CIC and CP C, representing concentrations of antigen, immunoglobulin G

302
A. Almudevar
(IgG) antibodies, immunoglobulin M (IgM) antibodies, immune complexes (IC)
and plasma cells (PC), respectively. The dynamic laws were deﬁned by a sys-
tem of ﬁve ordinary diﬀerential equations. Apart from an endogenous input F(t)
of antigen, representing a designed vaccine schedule, the model is time homo-
geneous, and deﬁned by static decay and interaction rates among the system
variables.
As another example, we consider the model proposed in [16]. It contains only
two state variables: T, the concentration of T-cells; and C, the concentration
of peptide-MHCs (pMHC), which transport antigen fragments for recognition
by T-cell receptors. This is required for stimulation of the adaptive immune
response, speciﬁcally, growth of the T-cell population requires interaction with
pMHC. The model equations are
dT
dt = α
TC
K + T + C −δT,
dC
dt = −μC,
where K is a constant, and α, δ and μ are positive system parameters. We assume
α > δ. Initially, the environment is saturated with pMHC, so that C ≫K + T,
and T ∝e(α−δ)t. Then C decays exponentially at rate −μ, independently of the
other components of the system. Eventually, C ≪K + T, so that T-cells decay
exponentially at rate −δ. The model predicts an interesting relationship at the
time t∗of peak T-cell concentration, in particular,
T(t∗)
T(0) =
C(0)
T(0)
(α−δ)/(α−δ+μ)
.
The prediction that the peak fold increase of T-cells is positively related to the
initial input of antigen, but inversely related to the initial T-cell concentration
was observed experimentally in [18].
However, if Ab decay is truly power-law, then it is diﬃcult to see how it can
be driven by an autonomous dynamic system (we have noted that [9] models
power-law decay as a mixture of autonomous decay models, but this degree of
heterogeneity does not appear to be compatible with known immune response
function).
Cellular processes are believed to possess, in general, robustness properties
which ensure uniformity of outcomes under varying conditions (see, for exam-
ple, [2]). Robustness can take various forms, for example, insensitivity to model
parameter values, as deﬁned in [20]. It seems reasonable, therefore, to model the
adaptive immune response as a control system relying on biologically plausible
control mechanisms. In fact, if Ab decay is not only consistently power-law, but
power-law speciﬁcally with rate ∝1/t, then the argument for this approach is
strengthened all the more.

A Regulatory Principle for Robust Reciprocal-Time Decay
303
4
Control Model for FDC Decay
Follicular dendritic cells (FDC) are found in the B-cell follicles of secondary
lymph nodes, the primary site of the adaptive immune response. Their function
is to capture and retain antigen in immunogenic form, and to induce Ab response
by supporting germinal centers (GC), the sites of B-cell maturation. They are
nonmigratory, and form a reticula network which deﬁnes a microenvironment.
Under the conventional model of the adaptive immune response, it would be
reasonable to conjecture that humoral Ab levels are proportional to GC con-
centration, which is in turn proportional to FDC concentration (for example,
the ratio of FDC antigen retaining reticula and GCs was reported to be 1:1
in mouse lymph tissue in [24]). Therefore, it is plausible that a control model
for FDC concentration can explain reciprocal-time decay of humoral Ab levels
(see [1]).
4.1
Model Deﬁnition
A non-autonomous dynamic model which predicts reciprocal-time decay is quite
easy to construct. For example, if F = tCt is a balance equation for concentration
Ct, and F is held constant, then Ct = F/t. The question, of course, is whether
or not such a balance equation has any relevance to the problem at hand. In
fact, we will argue that the quite unique functionality of the FDC makes this
equation very relevant.
Suppose there exists a population of activated FDCs, the initial size being
a positive real number C0 = N ∈IR. The model system S is partitioned into a
reservoir R and an FDC population F. Flow through S is given by:
External antigen source →R →F →Antigen clearance.
Antigen transport pathways exist in R, while antigen retained in FDCs exists
in F.
Let Ct, Ft be the population size of still active FDCs and the total amount
of antigen in F at time t ∈[0, ∞), respectively. We take Ct ∈[0, N], Ft ∈[0, ∞)
to be real valued, with initial values C0 = N, F0 = 0.
Deﬁne the following rules:
(A1) As long as a unit FDC remains active it ingests antigen at a rate of μ per
unit time.
(A2) A unit FDC may be deactivated at any time, at which point its total
ingested antigen is released.
(A3) No FDC can be created or reactivated.
Under rules (A1)–(A3) the balance equation
Ft = μtCt, t ≥0
(6)
must hold. Diﬀerentiating (6) then gives
dFt
dt = μ

Ct + tdCt
dt

.
(7)

304
A. Almudevar
The terms of Eq. (7) have an intuitive interpretation. Antigen is ingested at a
rate of μ per unit cell, giving the term μCt. At time t a unit FDC has ingested
μt units of antigen, therefore a decay rate of dCt/dt < 0 forces release of antigen
from F at the rate −μtdCt/dt. Thus, the system steady state dFt/dt = 0 is
characterized by both constant antigen retention Ft = F∞and reciprocal-time
decay of the FDC population Ct = F∞/μt.
4.2
A Homeostatic Control Model
The next problem is to introduce a control eﬀector into (7). Deﬁne the double-
logarithmic derivative
kt = d log Ct
d log t = C−1
t
dCt
t−1dt .
The solution to kt ≡k yields the power-law decay of Eq. (1). We may then
rewrite (7) as
dFt
dt = μ · Ct [1 + kt] ,
(8)
from which a simple control eﬀector emerges. Maintaining kt ≡−1 forces
dFt/dt = 0, and kt > −1 or kt < −1 forces increase or decrease in Ft, respec-
tively. Thus, feedback control of kt, which determines the decay rate of Ct,
provides a mechanism for homeostatic maintenance of the system steady state.
Interestingly, the steady state is mathematically equivalent to reciprocal-time
decay of Ct, and therefore of humoral Ab levels. This would predict the univer-
sal observation of reciprocal-time decay reported in [1].
Of course, the problem remains of proposing a biologically plausible control
law for kt with the system steady state as an attractor. It would be reasonable
to assume that control is eﬀected at the individual cell level, taking the form
dCt
dt = −¯λ(Ft, Ct, t)Ct
(9)
for some unit cell decay control function ¯λ ≥0. We can substitute the balance
Eq. eqrefeq.balance into (9) to obtain a ﬁrst-order ordinary diﬀerential equation
(ODE):
dCt
dt = −¯λ(μtCt, Ct, t)Ct.
(10)
In this form, ¯λ could be interpreted as a stochastic FDC failure (deactivation)
rate.
4.3
Exponential Decay Cannot Yield Homeostatic Control
Suppose R always contains suﬃcient antigen for FDC ingestion, and the unit
cell decay rate is constant at ¯λ(Ft, Ct, t) ≡ρ > 0, resulting in exponential
population decay. The solution to (9) is Ct = C0 exp(−ρt), in which case Ft =
μtC0 exp(−ρt). This function possesses a global maximum at t = 1/ρ. Therefore,
Ft increases to peak level Fmax = (μ/ρ)C0 exp(−1) then converges to zero. Thus
a statistic decay rate for Ct cannot yield homeostatic control of the steady state.

A Regulatory Principle for Robust Reciprocal-Time Decay
305
4.4
Balance Equations for Steady State Antigen Flow Through
System S
To construct a plausible homeostatic control we will expand the deﬁnition of
the system. We deﬁne the amount of antigen Et ∈[0, ∞) contained in R. This
is the antigen available for FDC ingestion. The initial reservoir level is then
E0 = R > 0. Let At be the total amount of additional antigen entering R by
time t. Then let Bt be the total antigen released by deactivated FDCs by time
t. We must have
dBt
dt = −μtdCt
dt .
(11)
Assuming Bt is lost to the system, the balance equation may be expanded to
Ft = μtCt,
R + Δt = Et + Ft, t ≥0 where Δt = At −Bt.
(12)
If the net ﬂow of antigen through S is zero, then the additional balance condition
Δt = 0
(13)
holds. Accepting (12) and (13), convergence to the steady state, Ft →F∞can
be then expressed as:
lim
t→∞Et = E∞< R.
(14)
In other words, under the system steady state R is indeﬁntely depleted in part
or in full. In this case F∞= R −E∞, forcing invariant reciprocal-time decay
Ct = (R −E∞)/μt.
The control functon ¯λ may depend on any of the quantities in (12), assuming
they satisfy the balance conditions, and so the system remains governed by the
control equation
dCt
dt = −¯λ(At, Bt, Ct, Et, Ft, t)Ct.
(15)
4.5
Control Based on Allocation of Available Antigen
A reasonable conjecture is that FDC deactivation is upregulated by antigen
scarcity, similar to the model proposed in [16] (Sect. 3). Suppose antigen is made
available to a single FDC by a Poisson arrival process of rate γ. A failure occurs
when an interarrival time exceeds some threshold κ. This failure results in the
deactivation of the FDC, and the release of its retained antigen. Since this failure
rate will depend on both antigen availability (∝Et) and competition for antigen
(∝Ct) this becomes a potential control eﬀector.
A Poisson process is well approximated by a discrete time arrival process.
Independent binary random variables Xi, i = 1, 2, . . . with mean qδ are observed
at times δi, i = 1, 2, . . .. An arrival occurs at time δi if Xi = 1. The constraint
γδ = qδ forces an arrival rate of γ. A failure is initiated at time δi if Xi = 1 and
Xi+1 = . . . = Xi+nδ = 0, where κ = nδδ (we lose no generality in choosing δ

306
A. Almudevar
so that nδ is an integer). The expected number of failure initiations NF in time
interval [0, Nδ] is
E[NF ] =
N

i=1
P(Xi = 1, Xi+1 = . . . = Xi+nδ = 0) = Nqδ(1 −qδ)nδ.
The rate of failure initiation is therefore
ρδ = Nqδ(1 −qδ)nδ
Nδ
= Nγδ(1 −γδ)κ/δ
Nδ
= γ(1 −γδ)κ/δ.
Finally, reﬁning the discrete approximation gives failure rate
ρ = lim
δ→0 ρδ = γ exp(−γκ).
The argument is completed by noting that under general conditions the aggre-
gation of m arrival processes approaches in distribution a Poisson process as
m →∞, so that the model will be reasonably robust with respect to assump-
tions (see [8]). Under the proposed model the antigen arrival rate per FDC is
proportional to Et/Ct, therefore the FDC failure rate would be
¯λ(Ct, Et) =
γEt/Ct exp(−γκEt/Ct) ; Et > 0
∞
; Et = 0 ,
(16)
noting that the population is extinguished essentially instantaneously when Et =
0 (i.e. when R is depleted).
To remain active, the aggregate antigen arrival rate γ∗for an individual FDC
must be larger than μ. Under these conditions, the neighborhood of an FDC is
essentially saturated with available antigen, and therefore able to maintain the
maximum ingestion rate μ. As antigen is depleted the quantity Et/Ct decreases,
forcing γ∗to approach μ, making an ingestion failure event increasingly likely.
Thus, this failure model predicts property (A1). Convergence to reciprocal-
time decay under this control law when net antigen ﬂow is zero is veriﬁed in the
following theorem (see [1] for proof).
Theorem 1. Suppose the control function ¯λ of Eq. (15) is given by Eq. (16).
Suppose balance Eqs. (12) and (13) hold. Then there exists a constant t∗, depen-
dent only on parameters (μ, γ, κ), for which the following statements hold:
(i) For any initial state (t, Ct) = (t0, Ct0) for which t0 > t∗there exists a
positive constant r∗such that for all large enough R∗we have:
0 < Ct <
R∗
μt + r∗,
and therefore Et/Ct > r∗, t ≥t0, where R∗= E0 is taken to be the initial
reservoir quantity.
(ii) Given the initial conditions of statement (i), if E0 = R∗then limt→∞μtCt =
R∗.

A Regulatory Principle for Robust Reciprocal-Time Decay
307
Thus, under the conditions of Theorem 1, Ct possesses reciprocal-time decay
in the limit, and steady state antigen retention limt→∞Ft = R, with complete
reservoir depletion limt→∞Et = 0. The steady state retention level F∞therefore
depends on R but not on the model parameters (μ, γ, κ).
5
Computer Simulations
We next demonstrate the model using computer simulations reported in [1]. We
can observe the convergence of Ct to reciprocal-time decay, as the initial reservoir
R of antigen is depleted and retained in the FDC population F.
Balance Eqs. (12) and (13) are assumed to hold, and we use the control model
with failure rate ¯λ given by Eq. (16). We take time interval to be t ∈[0, 1000],
with initial FDC population C0 = 103. The initial antigen level is varied by
setting R/C0 = 25000, 5000, 1000. The antigen ingestion rate is set to μ = 103.
To determine the parameters for ¯λ consider the case R/C0 = 1000. This gives an
antigen arrival rate per FDC at t = 0 of γE0/C0 = γR/C0 = γ1000. Equating
this to μ gives γ = 1. Given ingestion rate μ it would be reasonable to set κ to
be some factor of μ−1, so we set κ = μ−1 = 1/1000. The model was discretized
by time intervals Δt = 10−4.
Figure 1 shows model pathways for varying initial resource R/C0 = 25000,
5000, 1000 (columns 1–3). In row 1 plots of Ct and Et are shown with a vertical
log scale. Row 2 shows Ct and Et on a log-log scale. Grid lines parallel to t−1
are superimposed. For display E0, C0 are both normalized to equal 100% in rows
1–2. Row 3 gives the double logarithmic decay rate kt as a function of time. Row
4 gives the relative concentration of retained antigen Ft/R.
The behavior for each set of initial conditions is unvarying, and conforms to
the model’s prediction. Each example begins with a short period of decay at kt
close to 0, then approaches kt = −1 by times ranging from t ≈100 −250 (rows
2–3). Ft quickly reaches its predicted steady state level R (row 4).
We next examine the robustness of the model to perturbation. Figure 2 is
based on the same model used for Fig. 1 (R/C0 = 25000) but with various
forms of stochastic noise introduced (columns 1–3). For the “random resource
spikes” model the reservoir R was supplemented by bulk arrivals of 500 antigen
units according to a Poisson process of rate 0.04. For the remaining models
multiplicative noise was incorporated by multiplying dCt/dt by a log-normal
random variable at each computation point (the exponentiated normal random
variates had mean μ = 0 and standard deviations σ = 0.1, 1).
In each case the models exhibit the same limiting behavior seen in Fig. 1,
despite persistent random perturbations. For the random resource spikes model
the assumption of constant system resource R is violated, but without apparent
eﬀect on the approach to the predicted system steady state (Fig. 2 column 1).
For the multiplicative noise model with σ = 0.1 (Fig. 2, column 2), the
behavior diﬀers little from the corresponding noiseless model (Fig. 1, column
1). What is of some interest is the stable ﬂuctuation of kt about the steady state
value k = −1, suggesting an eﬃcient negative feedback control able to maintain

308
A. Almudevar
reciprocal-time decay. Setting σ = 1 results in considerably more noise (Fig. 2,
column 3). The decay rate kt no longer ﬂuctuates about k = −1 in a stable
manner, but instead subjects the system to frequent and extremely large decay
rates. In this case, ﬂuctuation of Et is more evident (rows 1, 2). Despite this,
the system steady state is maintained.
6
Discussion
The model proposed in [1] achieves a number of objectives. First, it predicts the
universal reciprocal-time decay that has been widely reported in the literature.
Furthermore, reciprocal-time decay was demonstrated to be a stable attractor.
Remarkably, the model conforms to the robustness principle of insensitivity to
model parameter values (see [20]) in the sense that the long-term behavior does
not depend on any model parameters other than the initial antigen level R,
provided this value is large enough (Theorem 1).
The remaining questions have to do with the biological plausibility of the
model. In fact, there is a striking concordance between cell properties required
by the model and those widely reported of FDCs, which are generally unique to
this cell type.
The ability of FDCs to retain intact antigen for extended periods has been
consistently reported. This property is frequently conjectured to be related to
long term persistence of Ab concentrations (see [11]).
Regarding properties (A1)–(A2), it was reported in [14] that maintenance
of FDC functionality requires continual lymphotoxin α/β (LT) signalling. Inhibi-
tion of LT signalling not only prevents FDC ingestion of antigen, but eliminates
previously ingested antigen. The authors write that “[a] surprising observation
is that the maintenance of pre-existing FDCs in a diﬀerentiated state requires
continual interaction with B lymphocytes expressing LTαβ”. These B lympho-
cytes (or B-cells) are responsible for transporting antigens to the FDCs, which
themselves produce the B-cell attractant CXCL13. This mechanism is part of
a positive feedback loop (see [3]). Therefore, the assumption that FDCs remain
active only as long as they are able to ingest antigen is well founded, and conforms
remarkably well with experimental observations. This motivates the control law
of Eq. (16), which models FDC deactivation as an interruption of the supply of
antigen.
Thus, the model of [1] is able to unify disparate observations of FDC func-
tion, providing a simple regulatory principle which predicts a robust, universal
reciprocal-time decay rate for any adaptive immune response. Remarkably, under
this principle no feedback is required to terminate the immune response. Rather,
at the highest level the control is open-loop, with the FDC population function-
ing collectively as an immune response timer.

A Regulatory Principle for Robust Reciprocal-Time Decay
309
0
250
500
750
1000
10−3
10−2
10−1
1
t
Concentration
R/C0 = 25,000
10
−3
10
−1
10
1
10
3
10−3
10−2
10−1
1
t
Concentration
0
250
500
750
1000
−1.2
−0.8
−0.4
0.0
t
k t
0
250
500
750
1000
0
1/4
1/2
3/4
1
t
F t/R
0
250
500
750
1000
10−3
10−2
10−1
1
t
Concentration
R/C0 = 5,000
10
−3
10
−1
10
1
10
3
10−3
10−2
10−1
1
t
Concentration
0
250
500
750
1000
−1.2
−0.8
−0.4
0.0
t
k t
0
250
500
750
1000
0
1/4
1/2
3/4
1
t
F t/R
0
250
500
750
1000
10−3
10−2
10−1
1
t
Concentration
Ct
Et
R/C0 = 1,000
10
−3
10
−1
10
1
10
3
10−3
10−2
10−1
1
t
Concentration
0
250
500
750
1000
−1.2
−0.8
−0.4
0.0
t
k t
0
250
500
750
1000
0
1/4
1/2
3/4
1
t
F t/R
Fig. 1. Plots show model pathways for varying total resource R/C0 = 25000, 5000, 1000
(columns 1–3). See Sect. 5 for descriptions. In row 1 plots of Ct and Et are shown with
a vertical log-scale. Row 2 shows Ct and Et on a log-log scale. Grid lines parallel to
t−1 are superimposed. For display purposes E0, C0 are both normalized to equal 100%
in rows 1–2. Row 3 gives the double-logarithmic decay rate kt as a function of time. A
horizontal reference line is included at k = −1. Row 4 gives the relative concentration
of Ft/R. A horizontal reference line is included at Ft/R = 1.

310
A. Almudevar
0
250
500
750
1000
10−3
10−2
10−1
1
t
Concentration
Random Resource Spikes
10
−3
10
−1
10
1
10
3
10−3
10−2
10−1
1
t
Concentration
0
250
500
750
1000
−1.2
−0.8
−0.4
0.0
t
k t
0
250
500
750
1000
0
1/4
1/2
3/4
1
t
F t/R
0
250
500
750
1000
10−3
10−2
10−1
1
t
Concentration
Multiplicative Noise σ = 0.1
10
−3
10
−1
10
1
10
3
10−3
10−2
10−1
1
t
Concentration
0
250
500
750
1000
−1.5
−1.0
−0.5
0.0
t
k t
0
250
500
750
1000
0
1/4
1/2
3/4
1
t
F t/R
0
250
500
750
1000
10−3
10−2
10−1
1
t
Concentration
Ct
Et
Multiplicative Noise σ = 1
10
−3
10
−1
10
1
10
3
10−3
10−2
10−1
1
t
Concentration
0
250
500
750
1000
−80
−40
0
t
k t
0
250
500
750
1000
0
1/4
1/2
3/4
1
t
F t/R
Fig. 2. Plots show model used for Fig. 1 with R/C0 = 25000 incorporating various
forms of stochastic noise (columns 1–3). See Sect. 5 for descriptions. In row 1 plots of
Ct and Et are shown with a vertical log-scale. Row 2 shows Ct and Et on a log-log
scale. Grid lines parallel to t−1 are superimposed. For display purposes E0, C0 are both
normalized to equal 100% in rows 1–2. Row 3 gives the double-logarithmic decay rate
kt as a function of time. A horizontal reference line is included at k = −1. Row 4 gives
the relative concentration of Ft/R. A horizontal reference line is included at Ft/R = 1.

A Regulatory Principle for Robust Reciprocal-Time Decay
311
References
1. Almudevar, A.: A model for the regulation of follicular dendritic cells predicts
invariant reciprocal-time decay of post-vaccine antibody response. Immunol. Cell
Biol. 95(9), 832–842 (2017)
2. Alon, U.: An Introduction to Systems Biology: Design Principles of Biological
Circuits. CRC Press, Boca Raton (2019)
3. Ansel, K.M., Ngo, V.N., Hyman, P.L., Luther, S.A., F¨orster, R., Sedgwick, J.D.,
Browning, J.L., Lipp, M., Cyster, J.G.: A chemokine-driven positive feedback loop
organizes lymphoid follicles. Nature 406(6793), 309–314 (2000)
4. Borrow, R., Andrews, N., Findlow, H., Waight, P., Southern, J., Crowley-Luke,
A., Stapley, L., England, A., Findlow, J., Miller, E.: Kinetics of antibody persis-
tence following administration of a combination meningococcal serogroup C and
haemophilus inﬂuenzae type B conjugate vaccine in healthy infants in the United
Kingdom primed with a monovalent meningococcal serogroup C vaccine. Clin.
Vaccine Immunol. 17(1), 154–159 (2010)
5. Chen, S., Zhou, Z., Wei, F.-X., Huang, S.-J., Tan, Z., Fang, Y., Zhu, F.-C., Wu,
T., Zhang, J., Xia, N.-S.: Modeling the long-term antibody response of a hepatitis
E vaccine. Vaccine 33(33), 4124–4129 (2015)
6. Clauset, A., Shalizi, C.R., Newman, M.E.J.: Power-law distributions in empirical
data. SIAM Rev. 51(4), 661–703 (2009)
7. David, M.-P., Van Herck, K., Hardt, K., Tibaldi, F., Dubin, G., Descamps, D., Van
Damme, P.: Long-term persistence of anti-HPV-16 and -18 antibodies induced by
vaccination with the as04-adjuvanted cervical cancer vaccine: Modeling of sus-
tained antibody responses. Gynecol. Oncol. 115(3), S1–S6 (2009)
8. Feller, W.: Probability Theory and Its Applications, 2nd edn., vol. 2. Wiley,
New York (1971)
9. Fraser, C., Tomassini, J.E., Xi, L., Golm, G., Watson, M., Giuliano, A.R., Barr, E.,
Ault, K.A.: Modeling the long-term antibody response of a human papillomavirus
(HPV) virus-like particle (VLP) type 16 prophylactic vaccine. Vaccine 25(21),
4324–4333 (2007)
10. Gesemann, M., Scheiermann, N.: Quantiﬁcation of hepatitis B vaccine-induced
antibodies as a predictor of anti-HBs persistence. Vaccine 13(5), 443–447 (1995)
11. Heesters, B.A., Myers, R.C., Carroll, M.C.: Follicular dendritic cells: dynamic anti-
gen libraries. Nat. Rev. Immunol. 14(7), 495–504 (2014)
12. Honorati, M., Palareti, A., Dolzani, P., Busachi, C., Rizzoli, R., Facchini, A.: A
mathematical model predicting anti-hepatitis B virus surface antigen (HBs) decay
after vaccination against hepatitis B. Clin. Exp. Immunol. 116(1), 121–126 (1999)
13. Jost, A.: Die Assoziationsfestigkeit in ihrer Abh¨angigkeit von der Verteilung der
Wiederholungen. Leopold Voss, Leipzig (1897)
14. Mackay, F., Browning, J.L.: Turning oﬀfollicular dendritic cells. Nature 395(6697),
26–27 (1998)
15. Marrack, P., Scott-Browne, J., MacLeod, M.K.: Terminating the immune response.
Immunol. Rev. 236, 5–10 (2010)
16. Mayer, A., Zhang, Y., Perelson, A.S., Wingreen, N.S.: Regulation of T cell expan-
sion by antigen presentation dynamics. Proc. Natl. Acad. Sci. U.S.A. 116(13),
5914–919 (2019)
17. Myung, I.J., Kim, C., Pitt, M.A.: Toward an explanation of the power law artifact:
insights from response surface analysis. Mem. Cognit. 28(5), 832–840 (2000)

312
A. Almudevar
18. Quiel, J., Caucheteux, S., Laurence, A., Singh, N.J., Bocharov, G., Ben-Sasson,
S.Z., Grossman, Z., Paul, W.E.: Antigen-stimulated CD4 T-cell expansion is
inversely and log-linearly related to precursor number. Proc. Natl. Acad. Sci.
U.S.A. 108(8), 3312–3317 (2011)
19. Reed, W.J., Hughes, B.D.: From gene families and genera to incomes and internet
ﬁle sizes: why power laws are so common in nature. Phys. Rev. E 66(6), 067103
(2002). 4 pp.
20. Savageau, M.A.: Parameter sensitivity as a criterion for evaluating and comparing
the performance of biochemical systems. Nature 229(5286), 542–544 (1971)
21. Southern, J., McVernon, J., Gelb, D., Andrews, N., Morris, R., Crowley-Luke, A.,
Goldblatt, D., Miller, E.: Immunogenicity of a fourth dose of Haemophilus Inﬂuen-
zae type B (Hib) conjugate vaccine and antibody persistence in young children
from the United Kingdom who were primed with acellular or whole-cell pertus-
sis component-containing Hib combinations in infancy. Clin. Vaccine Immunol.
14(10), 1328–1333 (2007)
22. Swart, E., van Gageldonk, P., de Melker, H., van der Klis, F., Berbers, G., Mollema,
L.: Long-term protection against diphtheria in the Netherlands after 50 years of
vaccination: Results from a seroepidemiological study. PloS One 11(2), e0148605
(2016)
23. Tam, H.H., Melo, M.B., Kang, M., Pelet, J.M., Ruda, V.M., Foley, M.H., Hu, J.K.,
Kumari, S., Crampton, J., Baldeon, A.D., Sanders, R.W., Moore, J.P., Crotty, S.,
Langer, R., Anderson, D.G., Chakraborty, A.K., Irvine, D.J.: Sustained antigen
availability during germinal center initiation enhances antibody responses to vac-
cination. Proc. Natl. Acad. Sci. U.S.A. 113(43), E6639–E6648 (2016)
24. Tew, J.G., Kosco, M.H., Burton, G.F., Szakal, A.K.: Follicular dendritic cells as
accessory cells. Immunol. Rev. 117(1), 185–211 (1990)
25. Wickelgren, W.A.: Trace resistance and the decay of long-term memory. J. Math.
Psychol. 9(4), 418–455 (1972)
26. Wixted, J.T.: On common ground: Jost’s (1897) law of forgetting and Ribot’s
(1881) law of retrograde amnesia. Psychol. Rev. 111(4), 864–879 (2004)

Swarm Intelligence and Swarm Robotics
in the Path Planning Problem
Quoc Bao Diep(B), Thanh Cong Truong, and Ivan Zelinka
Faculty of Electrical Engineering and Computer Science, VSB - Technical University
of Ostrava, 17. Listopadu 15, Ostrava, Czech Republic
diepquocbao@gmail.com,{cong.thanh.truong.st,ivan.zelinka}@vsb.cz
Abstract. In this chapter, we introduce the basic characteristics of
swarm intelligence, the path planning problem for robots, and how to
apply the self-organizing migrating algorithm, a representative of swarm
intelligence to solve that real-world problem. We set up simulations in
the Matlab environment with four common possible scenarios to demon-
strate the eﬀectiveness of the solution.
Keywords: Self-organizing migrating algorithm · SOMA · Path
planning · Swarm intelligence
AMS(2020) Subject Classiﬁcation: Primary 68T40 · Secondary
93C85
1
Introduction
Along with the development of artiﬁcial intelligence, swarm intelligence (SI)
increasingly prove its important role, participating in most of the real-world
technical problems. SI is derived from the observation of the intelligent behavior
of creatures to form algorithms that solve complex problems with simple rules.
The popular SI algorithms can be mentioned as particle swarm optimization
[9], artiﬁcial bee colony [8], ﬁreﬂy algorithm [12], ant colony optimization [5],
especially the self-organizing migrating algorithm [11,13] that will be focused on
in this chapter.
These algorithms have been applied to solve complex problems in many ﬁelds,
such as analysis of the performance of the ﬁsh school search algorithm running in
graphic processing units [10], training the radial basis function network for data
classiﬁcation and disease diagnosis [7], adaptive routing in telecommunications
networks [6], resource allocation scheme for 5G C-RAN [1] and task scheduling
in cloud-based internet of things applications [3].
But what kind of problems can apply SI algorithm to solve? And how to solve
them? Most problems arise in practice that requires optimal solutions, which
are minimum or maximum values, or solutions that satisfy some constraint.
Accordingly, the optimization problems are the objects to be solved by the SI
c
⃝The Author(s), under exclusive license to Springer Nature Switzerland AG 2021
A. Piunovskiy and Y. Zhang (Eds.): Modern Trends in Controlled Stochastic Processes,
ECC 41, pp. 313–327, 2021. https://doi.org/10.1007/978-3-030-76928-4_16

314
Q. B. Diep et al.
algorithms, and one of the most important things to do is modeling the given
problem into a mathematical model described by equations.
This chapter presents how to model the path planning problem for robots
avoiding detected obstacles towards the target by applying the self-organizing
migrating algorithm (SOMA), a representative of the SI. Section 2 presents the
main concept of the SI and introduces the SOMA algorithm. Section 3 deals with
the problem of path planning for robots. Details of the simulated settings in this
research are presented in Sect. 4. Section 5 shows the simulations that prove the
correctness of the solution. Finally, we conclude in Sect. 6.
2
Swarm Intelligence
2.1
General Concept
Swarm intelligence (SI) is a common name referring to the algorithms that oper-
ate on the mechanism simulating the collective intelligence behaviors of the crea-
tures. It works on a population (or some sub-populations) of many individuals
that interact with each other (both competing and cooperating) or with the
environment (migration and survival) to solve speciﬁc problems such as forag-
ing, protecting the nest or moving safely in the natural habitat.
Slightly diﬀerent from the evolutionary algorithms, which operate on Dar-
win’s theory of evolution, individuals in the SI population do not inherit the
genetic properties from generation to generation, but rather will share the knowl-
edge with each other in the same generation under loops. This sharing of infor-
mation is the key for the SI algorithm to ﬁnd the global optimal solution to the
given problems.
A ﬂock of birds, for example, is searching for food in space, and one individual
alerts the remaining members to ﬂy towards its cry (sharing information) when
it ﬁnds the food. On the way the others move to that food source, they can ﬁnd
a more abundant source than the previous signal, they will share it again. And
that process is repeated until the whole ﬂock meets together on where the most
food source is.
Inspired by those observations, the SI algorithms are designed to mimic these
behaviors. The next subsection will present the SOMA algorithm, a representa-
tive of the SI algorithm.
2.2
Self-Organizing Migrating Algorithm
The SOMA algorithm was ﬁrst introduced in [14]. It bore all the characteristics
of the SI that we will analyze below.
The ﬁrst operation of SOMA is to create an initial population containing a
predetermined number of individuals in a given search space, representing the
natural habitat in the foraging bird example above. These individuals are the
solutions to the optimization problem that have been encoded.
The population is then evaluated by the cost function (will be presented in
the next section), and the ﬁtness value represents the amount of food as in the

SOMA Algorithm
315
above example. The best individual is selected, and the remaining individuals
move towards that member. They will likely ﬁnd better positions on the path
they move. At the end of each such migration, a new best individual is selected
again and the process continues until the algorithm has found a solution that
satisﬁes the given requirement [11,13].
In the problem to be addressed in this chapter, SOMA plays the role of
generating a dynamic set of next stops for the robot in real-time. At a speciﬁc
time, based on the necessary information such as the current position of the
robot, the position of the target, and the obstacles detected by sensors, SOMA
calculates the next best position that the robot should move to. These positions
are generated in real-time and become the moving path for the robot.
To execute that description, the algorithm ﬁrst initializes a random group
of individuals around the current position of the robot (in the searching range)
based on Eq. (1). Then the best position is selected after evaluating all individ-
uals (named Leader), and the algorithm goes into the ﬁrst migration loop.
Poindividualith = Poactual + rnd −1→1 rorange,
(1)
where:
– Poindividualith: position of the ith individual,
– Poactual: actual position of robot,
– rorange: maximum moving range of robot,
– rnd −1→1: uniformly distributed random number from −1 to 1.
During this loop, the remaining members will one-by-one move towards the
Leader using the rule given in Eq. 2.
Ponew = Pocurrent + (Poleader −Pocurrent) n PRTV ectorj
(2)
where:
– Ponew: the new position of the current individual,
– Pocurrent : the current position of the current individual,
– Poleader : the Leader position in this migration loop,
– PRTV ectorj: the perturbatively factor, created by Eq. 3,
– n: moving step, from 0 by Step to PathLength.
if rnd 0→1 < PRT; PRTV ectorj = 1;
else, PRTV ectorj = 0.
(3)
After each individual completes its move, the best position on its path is
selected to be compared with the initial. It will replace the initial position if it
has a better ﬁtness value. When the last member completes its job, a new best
individual throughout whole the population is then selected again to replace the
old Leader and a new migration loop begins.
Those processes are terminated when the entire population has achieved a
given number of migrations. And the ﬁnal Leader is the position where the robot

316
Q. B. Diep et al.
-2
0
-5
5
2
fitness
4
y
x
0
0
5
-5
Fig. 1. The migration of individuals in a population of SOMA when the robot detects
obstacles.
will move. Figure 1 depicts the principle of the SOMA algorithm, where the hill
represents an obstacle, the blue points represent the initial individuals, the black
points represent the locations after the migration of the initial population, the
red point is the globally optimal location where the robot will move to.
The next section describes how to build the ﬁtness function.
3
The Path Planning Problem of Swarm Robots
For any optimization problem, the ﬁtness function is an important component, is
the object to solve. In some situations, the ﬁtness function is already given. But
in some cases, we have to build the ﬁtness function by modeling that problem.
In this section, we present how to turn the robot path planning problem into a
ﬁtness function.
Starting with a simple rule, the robot is as close to the target and as far
away from the obstacles as possible. Equation 4 generally describes the elements
X stated, where n is the number of the target and obstacles detected by the
sensors. The goal is to minimize the function f(X).
f(X) =
n

i=1
Xi
(4)
For the principle as close to the target as possible, we see that the value of
the function f(X) should be proportional to the distance from the robot to the
target. Equation 5 constructs the ﬁrst element of the ﬁtness function in detail,
where (xrobot, yrobot) and (xtarget, ytarget) are the current positions of the robot
and target respectively, and a1 is the equilibrium coeﬃcient.
X1 = a1

(xtarget −xrobot)2 + (ytarget −yrobot)2
(5)

SOMA Algorithm
317
Similarly, with the rule that as far as possible from obstacles, the value of the
f(X) function should be inversely proportional to the distance from the robot to
detected obstacles. Equation 6 describes this in detail.
Xi = ai
nobstacle

0
e−(c−robstacle) disobstacle
(6)
where:
disobstacle =

(xobstacle −xrobot)2 + (yobstacle −yrobot)2
– Xi: the obstacle elements of the f(X),
– ai: the equilibrium coeﬃcient,
– nobstacle: the number of detected obstacles,
– c: the inﬂuential coeﬃcient of obstacles,
– robstacle: the radius of detected obstacles,
– disobstacle: the distance from the robot to detected obstacles.
In the framework of this chapter, we do not go into details about robot
kinematics and dynamics. We assume that the robot can move smoothly from
point A to a nearby point without any problem, and the SOMA will generate
the dynamic set of that points [2].
Due to the robot’s physical limitations, the maximum distance between the
two points mentioned is limited, named dlimit. However, no matter how big the
dlimit is, the algorithm quality is completely independent of this distance.
4
Experiment Setup
To rigorously evaluate the feasibility of the proposed solution, we built 4 selective
scenarios, covering most of the basic situations that can occur in the real-world.
4.1
Selective Scenarios
The ﬁrst scenario is the simplest one, having a robot (with a respective target)
and three static obstacles. The location of the obstacles is intentionally arranged
so that they are symmetrical and centered on the line connecting the robot to
the target. The gap between the three obstacles is calculated wide enough for
the robot to move through them. This scenario is set up to test the ability of the
robot to pass through suﬃcient gaps between obstacles (see Map 1 of Fig. 2).
The second scenario is similar to the ﬁrst but the distance between the obsta-
cles has been changed so that they are smaller than the physical size of the robot
(it cannot move through those gaps). The aim is to trap the robot into the local
minima and observe how to escape from the trapped area of the robot (Map 2
of Fig. 2).

318
Q. B. Diep et al.
Obstacles
Target
Robot
moves
Map 1
Map 2
Obstacles
Target
Robot
moves
Map 3
Robot2
Target1
Robot1
Target2
Map 4
Robot1
Robot2
Target1
Target2
Obstacles
Fig. 2. Selective scenarios to test the operability of robots.
In the third scenario, two robots with two respective targets were set. There
is no obstacle in this map, but robots will be obstacles to each other. All of them
were intentionally put in a straight line so that the robots will move in oppo-
site directions. This situation tests the possibility of mutual avoidance between
robots (Map 3 of Fig. 2).
The last one is the most complex scenario. Two robots, two respective targets,
and three obstacles were used. The robots are on the same side of the obstacles,
and the targets are on the opposite side but diagonally. The obstacles located
in the middle are not only to prevent the movement of the robots, but also trap
the robot to the local minimum associated with the other robot. This scenario
tests the generality of the proposed algorithm (Map 4 of Fig. 2).
Table 1. Locations of obstacles and robots in Cartesian coordinates - Map 1 and 2 (in
decimeter)
The object Obstacle 1 Obstacle 2 Obstacle 3 Robot Target
xmap1
04
11
13
01
20
ymap1
11
04
13
01
20
rmap1
03
03
03
–
–
xmap2
06
13
11
01
20
ymap2
13
06
11
01
20
rmap2
02
02
02
–
–
The detailed locations of robots, obstacles, and targets are shown in Tables 1,
2, and 3.

SOMA Algorithm
319
Table 2. Locations of robots in Cartesian coordinates - Map 3 (in decimeter)
The object Robot 1 Robot 2 Target 1 Target 2
xmap3
06
17
20
03
ymap3
06
17
20
03
Table 3. Locations of obstacles and robots in Cartesian coordinates - Map 4 (in
decimeter)
The object Obs 1 Obs 2 Obs 3 Ro 1 Ro 2 Tar 1 Tar 2
xmap4
07
14
11
05
02
17
22
ymap4
14
07
11
02
05
22
17
rmap4
02
02
03
–
–
–
–
4.2
Control Parameters
The objects were drawn using Matlab software R2020b version in Windows 10
Pro Edition 20H2 Version. The SOMA for each robot is also programmed using
Matlab. The control parameters of the algorithm are given in Table 4.
Popsize = 40; Migration = 20: for 2−Dimensional problem and the objective
function to solve is not too complicated, those values are suitable.
PRT = 0.1, Step = 0.11, Pathlength = 3: These options are common to
the SOMA algorithm, and it is selected based on the recommendation from the
original paper [11,13].
All robots used in simulations have a radius rrobot = 0.8 dm. The sensors
have a radius of active range rsensor = rrobot + 2.8 dm. The maximum step of
the robots is dlimit = 0.4 dm.
5
Simulation Results
The simulation results are presented in the form of selected ﬁgures captured
from the robot’s movement in form of 2D and 3D.
In those 2D ﬁgures, robots are plotted on Cartesian coordinates with obsta-
cles and targets also. The circle around the robot represents the working range
of the sensors. Obstacles are drawn in a dark color circle. As the robot moves,
obstacles detected in the sensor’s active area will be represented by bright colors.
These obstacles will turn bright colors when they are detected by sensors located
on the robot.
Table 4. The control parameter values of SOMA.
Migration PopSize Step PRT PathLength
20
40
0.11
0.1
3.0

320
Q. B. Diep et al.
In 3D ﬁgures, the robot is represented as a big black dot, and the robot’s
path is represented by small black dots. Contour lines represent the surrounding
environment, they can change depending on the distance from the robot to the
target and the obstacles.
5.1
Results for Map 1
0
5
10
15
20
25
x
0
5
10
15
20
25
y
   ite 2
Obs 1
Obs 2
Obs 3
  Start 1
  Target 1
Ro1
0
5
10
15
20
25
x
0
5
10
15
20
25
y
   ite 13
Obs 1
Obs 2
Obs 3
  Start 1
  Target 1
Ro1
0
5
10
15
20
25
x
0
5
10
15
20
25
y
   ite 32
Obs 1
Obs 2
Obs 3
  Start 1
  Target 1
Ro1
0
5
10
15
20
25
x
0
5
10
15
20
25
y
   ite 38
Obs 1
Obs 2
Obs 3
  Start 1
  Target 1
Ro1
0
5
10
15
20
25
x
0
5
10
15
20
25
y
   ite 58
Obs 1
Obs 2
Obs 3
  Start 1
  Target 1
Ro1
0
5
10
15
20
25
x
0
5
10
15
20
25
y
   ite 78
Obs 1
Obs 2
Obs 3
  Start 1
  Target 1
Ro1
Fig. 3. The movement process of the robot in Map 1: move through the gaps between
obstacles to hit the target.
Figures 3 and 4 show the robot’s movement in 2D and 3D, respectively. They
were captured at the step of 2nd, 13th, 32nd, 38th, 58th, and 78th. At the 2nd
step in Fig. 3, the robot has not detected the obstacles yet so they are in a dark
color, and the robot tends to move straight towards the target. At this moment,
the contours on the 3D map of Fig. 4 are also “ﬂat” (without hills).

SOMA Algorithm
321
0
5
10
fitness
0
   ite 2
10
y
0
x
10
20
20
0
5
10
fitness
0
   ite 13
10
y
0
x
10
20
20
0
5
10
fitness
0
   ite 32
10
y
0
x
10
20
20
0
5
10
fitness
0
   ite 38
10
y
0
x
10
20
20
0
5
10
fitness
0
   ite 58
10
y
0
x
10
20
20
0
5
10
fitness
0
   ite 78
10
y
0
x
10
20
20
Fig. 4. The movement process of the robot in Map 1 presented in 3D.
However, in steps 13th and 32nd, the obstacles are detected, and they have
changed color, hills appear respectively in the 3D contour maps. The robot will
move along these contour lines from high to low and avoid colliding on the rising
hills (which are obstacles).
In the simple situation of Map 1, the distance between obstacles is large
enough for the robot to pass, so the robot has no trapped between three obstacles.
The robot takes 78 steps to hit its target on this Map.

322
Q. B. Diep et al.
0
5
10
15
20
25
x
0
5
10
15
20
25
y
   ite 21
Obs 1
Obs 2
Obs 3
  Start 1
  Target 1
Ro1
0
5
10
15
20
25
x
0
5
10
15
20
25
y
   ite 26
Obs 1
Obs 2
Obs 3
  Start 1
  Target 1
Ro1
0
5
10
15
20
25
x
0
5
10
15
20
25
y
   ite 33
Obs 1
Obs 2
Obs 3
  Start 1
  Target 1
Ro1
0
5
10
15
20
25
x
0
5
10
15
20
25
y
   ite 40
Obs 1
Obs 2
Obs 3
  Start 1
  Target 1
Ro1
0
5
10
15
20
25
x
0
5
10
15
20
25
y
   ite 75
Obs 1
Obs 2
Obs 3
  Start 1
  Target 1
Ro1
0
5
10
15
20
25
x
0
5
10
15
20
25
y
   ite 83
Obs 1
Obs 2
Obs 3
  Start 1
  Target 1
Ro1
0
5
10
15
20
25
x
0
5
10
15
20
25
y
   ite 98
Obs 1
Obs 2
Obs 3
  Start 1
  Target 1
Ro1
0
5
10
15
20
25
x
0
5
10
15
20
25
y
   ite 129
Obs 1
Obs 2
Obs 3
  Start 1
  Target 1
Ro1
0
5
10
15
20
25
x
0
5
10
15
20
25
y
   ite 183
Obs 1
Obs 2
Obs 3
  Start 1
  Target 1
Ro1
Fig. 5. The movement process of the robot in Map 2: cannot move through the gaps,
escape from the trap to hit the target.

SOMA Algorithm
323
   ite 21
0
10
20
x
0
5
10
15
20
25
y
   ite 26
0
10
20
x
0
5
10
15
20
25
y
   ite 33
0
10
20
x
0
5
10
15
20
25
y
   ite 40
0
10
20
x
0
5
10
15
20
25
y
   ite 75
0
10
20
x
0
5
10
15
20
25
y
   ite 83
0
10
20
x
0
5
10
15
20
25
y
   ite 98
0
10
20
x
0
5
10
15
20
25
y
   ite 129
0
10
20
x
0
5
10
15
20
25
y
   ite 183
0
10
20
x
0
5
10
15
20
25
y
Fig. 6. The movement process of the robot in Map 2 presented in contour map.
5.2
Results for Map 2
Scenario 2 is intentionally arranged so that the distance between obstacles is
not enough for the robot to move through. In this situation, the robot will be
trapped between obstacles and will not be able to move out of the trap zone. To
solve this problem, the equilibrium coeﬃcient will change the value leading to
a change the width of the hill accordingly, thereby escaping the robot from the
trapped area [4].
Figures 5 and 6 show the entire operation of the robot, captured at steps
21st, 26th, 33rd, 40th, 75th, 83rd, 98th, 129th, and 183rd.
In step 21st, similar to map 1, the robot has not detected any obstacles so
it moves straight to the target. However, in step 26th, all three obstacles were
detected, at which time the robot was trapped in the contour as shown in steps

324
Q. B. Diep et al.
26th and 33rd. As mentioned above, the equilibrium coeﬃcients start to change,
resulting in the size of the hills growing up, the contour changing continuously.
The robot follows these contour lines, shown in steps 40th to 98th, and exits the
trap towards the target, shown in steps 129th, and 183rd.
The robot took 183 steps in this scenario to escape the trap and hit the
target.
5.3
Results for Map 3
0
5
10
15
20
25
x
0
5
10
15
20
25
y
   ite 5
  Start 1
  Start 2
  Target 1
  Target 2
Ro1
Ro2
0
5
10
15
20
25
x
0
5
10
15
20
25
y
   ite 16
  Start 1
  Start 2
  Target 1
  Target 2
Ro1
Ro2
0
5
10
15
20
25
x
0
5
10
15
20
25
y
   ite 19
  Start 1
  Start 2
  Target 1
  Target 2
Ro1 Ro2
0
5
10
15
20
25
x
0
5
10
15
20
25
y
   ite 25
  Start 1
  Start 2
  Target 1
  Target 2
Ro1
Ro2
0
5
10
15
20
25
x
0
5
10
15
20
25
y
   ite 30
  Start 1
  Start 2
  Target 1
  Target 2
Ro1
Ro2
0
5
10
15
20
25
x
0
5
10
15
20
25
y
   ite 53
  Start 1
  Start 2
  Target 1
  Target 2
Ro1
Ro2
Fig. 7. The movement process of the robot in Map 3: face-to-face between two robots.
Diﬀerent from map 1 and map 2, map 3 has two robots and two targets respec-
tively. There are no obstacles on this map. Instead, the positions of the robots
and the targets are intentionally arranged so that they are each other’s obstacles.
Figure 7 shows the movement of two robots, captured at steps 5th, 16th, 19th,
25th, 30th, and 53rd. At step 5th, the robots have not detected the other robot
yet so they move straight to their target. But in step 16th, both robots are in
the detection range of sensors, and they avoid each other as shown in steps 19th
to 30th. Finally, they ﬁnish their work on step 53rd.

SOMA Algorithm
325
0
5
10
15
20
25
x
0
5
10
15
20
25
y
   ite 11
Obs 1
Obs 2
Obs 3
  Start 1
  Start 2
  Target 1
  Target 2
Ro1
Ro2
0
5
10
15
20
25
x
0
5
10
15
20
25
y
   ite 20
Obs 1
Obs 2
Obs 3
  Start 1
  Start 2
  Target 1
  Target 2
Ro1
Ro2
0
5
10
15
20
25
x
0
5
10
15
20
25
y
   ite 38
Obs 1
Obs 2
Obs 3
  Start 1
  Start 2
  Target 1
  Target 2
Ro1
Ro2
0
5
10
15
20
25
x
0
5
10
15
20
25
y
   ite 45
Obs 1
Obs 2
Obs 3
  Start 1
  Start 2
  Target 1
  Target 2
Ro1
Ro2
0
5
10
15
20
25
x
0
5
10
15
20
25
y
   ite 52
Obs 1
Obs 2
Obs 3
  Start 1
  Start 2
  Target 1
  Target 2
Ro1
Ro2
0
5
10
15
20
25
x
0
5
10
15
20
25
y
   ite 61
Obs 1
Obs 2
Obs 3
  Start 1
  Start 2
  Target 1
  Target 2
Ro1
Ro2
0
5
10
15
20
25
x
0
5
10
15
20
25
y
   ite 70
Obs 1
Obs 2
Obs 3
  Start 1
  Start 2
  Target 1
  Target 2
Ro1
Ro2
0
5
10
15
20
25
x
0
5
10
15
20
25
y
   ite 88
Obs 1
Obs 2
Obs 3
  Start 1
  Start 2
  Target 1
  Target 2
Ro1
Ro2
0
5
10
15
20
25
x
0
5
10
15
20
25
y
   ite 123
Obs 1
Obs 2
Obs 3
  Start 1
  Start 2
  Target 1
  Target 2
Ro1
Ro2
Fig. 8. The movement process of the robot in Map 4: a complex combination in a
single scenario.
5.4
Results for Map 4
The last scenario is the most complex one to test the general operability of
robots. Two robots, two respective targets, and three obstacles are present on
this map. They are arranged so that robots will be stuck between obstacles and
the remaining robot will be another obstacle, moving around, preventing each
other’s path.
Figure 8 reveals this operation process, captured at steps of 11th, 20th, 38th,
45th, 52nd, 61st, 70th, 88th, and 123rd.
At step 11th, three obstacles have not been detected and two robots are not
in each other’s path so they move towards the targets. However, in step 20th,
three obstacles are blocking the way of both robots. Furthermore, the remaining

326
Q. B. Diep et al.
robot now becomes the fourth obstacle preventing the other robot’s path. In
step 38th, Robot 2 turned its head, moved backward to ﬁnd a way out of the
trap zone, and Robot 1 moved along obstacles.
In steps 45th to 61st, the robots move around in the trap to ﬁnd a way
to escape, and they start out of the trap in step 70th. Once out of the trap
zone, there are no obstacles left, so the robots approach their target without any
problem, as shown in step 88th. They hit the ﬁnal target at step 123rd.
6
Conclusions
In this chapter, we have introduced a common practical application that is using
the self-organizing migrating algorithm to plan the path for the robot in real-
time. For this problem, SOMA plays a role in generating a dynamic set of moving
points from the starting position to the target that the robot must pass through.
When obstacles are detected by sensors, the inversely proportional components
appear in the ﬁtness function and the algorithm will search a next point that
satisﬁes both the criteria of avoiding obstacles and towards the target. The
limitations of the solution such as the parameters in the model are ﬁne-tuned
by experience will be overcome in the next studies.
Acknowledgement. The following grants are acknowledged for the ﬁnancial support
provided for this research: Grant of SGS No. SP2020/78, VSB-Technical University of
Ostrava.
References
1. Ari, A.A.A., Gueroui, A., Titouna, C., Thiare, O., Aliouat, Z.: Resource allocation
scheme for 5G C-RAN: a swarm intelligence based approach. Comp. Netw. 165,
106957 (2019)
2. Bao, D.Q., Zelinka, I.: Obstacle avoidance for swarm robot based on self-organizing
migrating algorithm. Procedia Comput. Sci. 150, 425–432 (2019)
3. Boveiri, H.R., Khayami, R., Elhoseny, M., Gunasekaran, M.: An eﬃcient swarm-
intelligence approach for task scheduling in cloud-based internet of things applica-
tions. J. Ambient Intell. Humaniz. Comput. 10(9), 3469–3479 (2019)
4. Diep, Q.B., Zelinka, I., Senkerik, R.: An algorithm for swarm robot to avoid multi-
ple dynamic obstacles and to catch the moving target. In: International Conference
on Artiﬁcial Intelligence and Soft Computing, pp. 666–675. Springer, Cham (2019)
5. Dorigo, M., Birattari, M., Stutzle, T.: Ant colony optimization. EEE Comput.
Intell. Mag. 1(4), 28–39 (2006)
6. Ducatelle, F., Di Caro, G.A., Gambardella, L.M.: Principles and applications of
swarm intelligence for adaptive routing in telecommunications networks. Swarm
Intell. 4(3), 173–198 (2010)
7. Horng, M.H., Lee, Y.X., Lee, M.C., Liou, R.J.: Fireﬂy metaheuristic algorithm
for training the radial basis function network for data classiﬁcation and disease
diagnosis. Theory New Appl. Swarm Intell. 4(7), 115–132 (2012)
8. Karaboga, D., Basturk, B.: On the performance of artiﬁcial bee colony (ABC)
algorithm. Appl. Soft Comput. 8(1), 687–697 (2008)

SOMA Algorithm
327
9. Kennedy, J., Eberhart, R.: Particle swarm optimization. In: Proceedings of ICNN
1995 International Conference on Neural Networks, vol. 4, pp. 1942–1948. IEEE
(1995)
10. Lins, A., Bastos-Filho, C.J., Nascimento, D.N., Junior, M.A.O., de Lima-Neto,
F.B.: Analysis of the performance of the ﬁsh school search algorithm running in
graphic processing units. In: Parpinelli, R., Lopes, H.S. (eds.) Theory and New
Applications of Swarm Intelligence, pp. 17–32. IntechOpen (2012). https://doi.
org/10.5772/30360
11. Pluhacek, M., Zelinka, I., Senkerik, R., Davendra, D.: Inspired in SOMA: perturba-
tion vector embedded into the chaotic PSO algorithm driven by Lozi chaotic map.
In: Davendra, D., Zelinka, I. (eds.) New Optimization Techniques in Engineering.
Studies in Computational Intelligence. Springer (2016)
12. Yang, X.S.: Fireﬂy algorithms for multimodal optimization. In: International Sym-
posium on Stochastic Algorithms, pp. 169–178. Springer, Heidelberg (2009)
13. Zelinka, I.: SOMA - self-organizing migrating algorithm. In: Onwubolu, G.C.,
Babu, B.V. (eds.) New Optimization Techniques in Engineering, pp. 167–217.
Springer, Heidelberg (2004)
14. Zelinka, I., Lampinen, J.: SOMA – self-organizing migrating algorithm. In:
MENDEL – 6th International Conference on Soft Computing, Brno, Czech Repub-
lic (2000)

Utilizing Diﬀerential Evolution into
Optimizing Targeted Cancer Treatments
Michail-Antisthenis Tsompanas1(B), Larry Bull1, Andrew Adamatzky1,
and Igor Balaz2
1 Unconventional Computing Laboratory, Department of Computer Science
and Creative Technologies, University of the West of England, Bristol, UK
{antisthenis.tsompanas,larry.bull,andrew.adamatzky}@uwe.ac.uk
2 Laboratory for Meteorology, Physics and Biophysics, Faculty of Agriculture,
University of Novi Sad, Novi Sad, Serbia
igor.balaz@df.uns.ac.rs
Abstract. Working towards the development of an evolvable can-
cer treatment simulator, the investigation of including evolutionary
optimization methods was considered. Namely, Diﬀerential Evolution
(DE) is studied here, motivated by the high eﬃciency of variations of
this technique in real-valued problems. A basic DE algorithm, namely
“DE/rand/1” was used to optimize in silico the design of a targeted
drug delivery system (DDS) for tumor treatment on PhysiCell simula-
tor. The suggested approach proved to be more eﬃcient than a standard
Genetic Algorithm (GA), which was not able to escape local minima
after a predeﬁned number of generations. The key attribute of DE that
enables it to outperform standard GAs, is the fact that it keeps the diver-
sity of the population high, throughout all the generations. This work
will be incorporated with ongoing research in a more wide applicability
platform that will design, develop and evaluate targeted DDSs aiming
cancer tumours.
Keywords: Diﬀerential evolution · Cancer treatment · Evolutionary
algorithm · PhysiCell simulator · Optimization
AMS(2020) subject classiﬁcation: Primary 68W50 · Secondary
92-08 · 90C27
1
Introduction
The vast diversity of cell types discovered in cancerous tumours [1,22] and their
ability to resist conventional treatment due the existence of subclonal popu-
lations [2,15], is motivating more complex treatment options. First steps into
using multitarget, multistage and multicomponent nanoparticles are promising
[17] and need to be further investigated as these techniques may hold the key
to eﬀective cancer treatments. Consequently, building computational tools that
could discover the optimum design parameters of a treatment through eﬃciently
c
⃝The Author(s), under exclusive license to Springer Nature Switzerland AG 2021
A. Piunovskiy and Y. Zhang (Eds.): Modern Trends in Controlled Stochastic Processes,
ECC 41, pp. 328–340, 2021. https://doi.org/10.1007/978-3-030-76928-4_17

Utilizing Diﬀerential Evolution into Optimizing Targeted Cancer Treatments
329
explore and exploit large parameter search spaces, are of paramount importance.
In accordance with this concept, the evolutionary in silico optimization of a tar-
geted DDS was investigated utilizing a robust evolutionary algorithm (EA).
DE gained popularity over other well-established EAs, as it follows similar
algorithmic steps with standard EAs, but was able to surpass them in terms
of eﬃciency [16]. There have been several proposals on how to enhance its per-
formance [8,18] and these alternative algorithmic approaches, building on the
initial methodology, were tested in real problems, as well as numerical bench-
mark problems [5,6,14,23].
DE was initially proposed in [16], as an eﬀective methodology of optimiza-
tion over continuous spaces of nonlinear and non diﬀerentiable functions. The
method was introduced as an alternative to previous direct search approaches,
aiming at three main objectives: the ability to ﬁnd the global optimum, the fast
convergence to this optimum and the need of a small amount of control param-
eters for the procedure. Moreover, it was developed to be easily implemented
in parallel computing platforms. The methodology is population based, where
for every generation the new individuals are produced after applying the scaled
diﬀerence (hence the name diﬀerential evolution) of some predeﬁned individuals
to another predeﬁned base individual.
The ability to easily extract attributes of the population, such as the distance
of its members and their directions, through the aforementioned methodology,
is what makes DE so powerful. This characteristic is deﬁned as self-referential
mutation [13]. Given the aforementioned advantages of DE compared with other
EAs, it was chosen to investigate the optimization of a targeted DDS on a cancer
tumour, when simulated with PhysiCell [10].
PhysiCell [10] is a multicellular, agent-based simulator that was designed to
extend the BioFVM [9] framework, to form a virtual laboratory. PhysiCell is
open source and oﬀers several sample projects, one of which is the one studied
in this study. More speciﬁcally, sample project “anti-cancer biorobots” [10] was
developed as a possible tool to investigate the targeted cancer treatment, i.e.
with drugs transported by specialized nanoparticles that would target speciﬁc
cells of the cancer tumours.
Previously, PhysiCell was deployed as a virtual laboratory in the optimiza-
tion process of the design of nanoparticle carriers of cancer treating compounds
[12,19–21] and the process of mapping immunotherapies [11]. More speciﬁcally,
the use of PhysiCell led the training of surrogate-assisted evolutionary algo-
rithms exploring the eﬃcient solutions of the design of nanoparticle-based drug
delivery systems for cancer [12]. A similar application of PhysiCell was exam-
ined in [19], where the optimization was achieved by a novel memetic algorithm
inspired by the fundamental haploid-diploid lifecycle of eukaryotic organisms
[3,4]. Moreover, active learning and genetic algorithms incorporated with the
PhysiCell simulator, enabled the eﬃcient exploration of biological and clinical
constraints for cancer immunotherapy [11].

330
M.-A. Tsompanas et al.
2
Diﬀerential Evolution
The methodology of DE comprises of the following steps: initialization, mutation,
recombination (or crossover) and selection. At ﬁrst, a population of individuals
(possible solutions) is formed by randomly picking values on the D-dimensional
search space of the problem to be optimized. The random function used is mainly
uniformly distributed to cover suﬃciently the search space, which could be lim-
ited with lower and upper boundaries depending on the deﬁnition of the problem.
Then, the mutation operator is employed on the initial population. More
speciﬁcally, for every individual in the population a new individual is generated
(named the mutant individual) by a mathematical expression of the parameters
of randomly chosen or predeﬁned individuals. For example, the mutant individual
(vi) can be the linear combination of three randomly selected individuals as
deﬁned by the following equation, where xr1, xr2 and xr3 are randomly selected
individuals from the population and F is a scaling factor, which is a positive
number.
vi = xr1 + F · (xr2 −xr3)
Note here, that the variations of DE methodology are deﬁned with a notation
in the form of “DE/base/num”. The “base” part of the notation refers to the
technique of choosing the individual that will be used as the base individual to
which the scaled diﬀerence will be added. The “num” part indicates the amount
of pairs of individuals that will produce a scaled diﬀerence each, to be added
on the base individual. Thus, the aforementioned DE variation is denoted as
“DE/rand/1”.
The produced mutant individuals, then, undergo the crossover operator, in
order to be recombined with individuals from the initial population. Each indi-
vidual chosen from the initial population is denoted as the target individual and
the produced individual after the crossover as the trial individual. Two crossover
strategies are mainly used, namely exponential and binomial. However, binomial
is dominant in the literature, where every parameter in the D-dimensional solu-
tion is treated separately to the others, and chosen from the mutant or the target
individual based on a probability deﬁned as the CR parameter. This parameter
is known as the crossover rate.
The ﬁnal step of the algorithm is then evaluation, by the use of the selection
operator. Similar to standard GAs, the trial individuals produced from the pre-
vious step, are evaluated with the ﬁtness function. If their ﬁtness is better than
the one of their target individual, they replace the target individual in the next
generation. Otherwise the target individual is retained. When all trial individu-
als are tested and the appropriate selection is made to form the population of the
next generation, the algorithm runs again the population through the mutation,
crossover and selection operators, until the termination criteria are met (i.e. the
computation budget).

Utilizing Diﬀerential Evolution into Optimizing Targeted Cancer Treatments
331
3
Methodology
The sample project from PhysiCell [10] framework that was investigated, is the
“anti-cancer biorobots”. In this simulation all entities are simulated as agents.
Namely, there are three diﬀerent types of agents with diﬀerent functionalities,
the cancer cells, the chemical compound (deﬁned as cargo agents) and the func-
tionalized nanoparticles (deﬁned as worker agents). The outputs of PhysiCell
include graphical representation of the agents in the simulated areas. An exam-
ple after 10 days of simulated growth and treatment of a tumour is depicted in
Fig. 1. The cancer cells are illustrated as green, the chemical compound as blue,
while the nanoparticles as red agents.
y
1112 agents 
Fig. 1. The graphical representation output of PhysiCell [10] after 10 days of simulated
growth and treatment of a cancer tumour.
The optimization problem here was deﬁned as the discovery of the design of
nanoparticle agents in PhysiCell simulator (v.1.4.1) that will result in lower num-
ber of remaining cancer cells. Each possible solution is mapped in a 6-dimensional
space where the parameters studied and their boundaries are the attached worker
migration bias [0,1], the unattached worker migration bias [0,1], worker relative
adhesion [0,10], worker relative repulsion [0,10], worker motility persistence time
(min) [0,10] and the cargo release O2 threshold (mmHg) [0,20]. These parameters
are selected as they dictate the way the simulated nanoparticle agents behave,
namely the speciﬁcs of worker agent migration (deterministic for 1 and Brownian
for 0) are determined by the parameters attached and unattached worker migra-
tion bias. The simulated time that each worker agent moves in a direction before
changing to a new one is deﬁned by the parameter worker motility persistence
time. The behaviour of the worker agents in accordance with the cargo agents
is described by the rest of the aforementioned parameters. For more details the
reader can refer to [10].

332
M.-A. Tsompanas et al.
Due to the stochastic nature of the simulator, each possible solution is eval-
uated after extracting the average value of the remaining cancer cells of 5 runs.
Each run executes 7 days of growing an initial 200 micron radius tumor and
3 days of applying the treatment. The execution of the simulation on an Intel R⃝
Xeon R⃝CPU E5-2650 at 2.20 GHz (using 8 of the 48 cores) and 64GB RAM was
completed after 5 min.
For comparison reasons, a generic GA was applied to optimize the afore-
mentioned problem. The parameters of the GA were chosen as population size
P = 20, tournament size T = 2 for selection and replacement operations, uniform
crossover with probability X = 80% and per allele mutation rate of µ = 20%
with random step size of s = [−5, 5]%.
On the other hand, the “DE/rand/1” strategy was implemented and tested
for the optimization of a targeted DDS on a cancer tumour, by simulating this
procedure with PhysiCell. The parameters of the DE algorithm were chosen as
population size P = 20, scaling factor F = 0.5 and crossover rate CR = 0.9.
Note that these parameters are not ﬁne-tuned to enhance the performance of
the algorithm, but are most commonly used throughout the literature.
The rest of the parameters used to deﬁne the simulation of the tumour envi-
ronment by PhysiCell were retained unchanged and assigned the same values as
from the developers of the simulator [10]. More speciﬁcally these parameters are
listed in Table 1.
Note here that the computational budget for one test of each evolutionary
methodology was set to 1000 evaluations of the simulator. That translates to
an evaluation of 200 possible solutions, because of the 5 run average used to
partially alleviate the stochasticity eﬀects of the simulator. Thus, having pop-
ulations of 20 individuals, it results to 10 generations. Moreover, note that the
total time required for an evolutionary test (1000 evaluations) reaches 5000 min
or c. 3.5 days. Consequently, the comparison of the minuscule possible overhead
of the DE, when compared to a GA, is not analysed in this application.
4
Results
Three comparison tests were run, each using the same initial population for the
GA and DE optimization. The results of the average ﬁtness (remaining cancer
cells after 10 days of simulation) of all individuals of the population through out
the generations are depicted in Figs. 2, 3 and 4(a). Furthermore, the ﬁtness of the
best individual for both GA and DE throughout the generations was illustrated
in Figs. 2, 3 and 4(b).
In Fig. 2(a) it is apparent that both approaches force the population to con-
verge towards a lower ﬁtness. However, in Fig. 2(b) the fact that DE outperforms
the GA in ﬁnding better solutions from the ﬁrst few generations is shown. More-
over, the GA seems to be stuck from the sixth generation in a local minimum.

Utilizing Diﬀerential Evolution into Optimizing Targeted Cancer Treatments
333
Table 1. Unaltered parameters of PhysiCell simulator.
Parameter
Value
Damage rate
0.03333 min−1
Repair rate
0.004167 min−1
Drug death rate
0.004167 min−1
Elastic coeﬃcient
0.05 min−1
Cargo O2 relative uptake
0.1 min−1
Cargo apoptosis rate
4.065e-5 min−1
Cargo relative adhesion
0
Cargo relative repulsion
5
Maximum relative cell adhesion distance 1.25
Maximum elastic displacement
50 µm
Maximum attachment distance
18 µm
Minimum attachment distance
14 µm
Motility shutdown detection threshold
0.001
Attachment receptor threshold
0.1
Worker migration speed
2 µm/min
Worker apoptosis rate
0 min−1
Worker O2 relative uptake
0.1 min−1
Fig. 2. Average (a) and best ﬁtness (b) of the populations evolved with GA and DE
in the ﬁrst comparison run.

334
M.-A. Tsompanas et al.
Fig. 3. Average (a) and best ﬁtness (b) of the populations evolved with GA and DE
in the second comparison run.
Fig. 4. Average (a) and best ﬁtness (b) of the populations evolved with GA and DE
in the third comparison run.
In Fig. 3(a) the GA approach seems to converge the average ﬁtness of the
population of solutions towards a lower ﬁtness faster than the DE. Also, in
Fig. 3(b) the DE approach is outperformed by GA for the ﬁrst six generations.
On the contrary, the GA is again stuck in a local optimum, while the DE is
continuously evolving towards better solutions and manages to ﬁnd a better one
at the sixth generation.

Utilizing Diﬀerential Evolution into Optimizing Targeted Cancer Treatments
335
Fig. 5. Scatter plot of all individuals tested during the DE approach for the third run.
The red “X” mark denotes the best individual found.
Finally, the results from the third comparison run are presented in Fig. 4. As
in the previous runs, it can be claimed that DE outperforms the GA. Speciﬁcally,
in Fig. 4(b) the DE approach reaches a ﬁttest individual than GA at the very
ﬁrst generations. Furthermore, whereas the DE seems to be stuck in a local
minimum from the third generation and the GA reaches a solution quite similar
to this one, on the last generation the DE manages to escape its minimum and
provide an ever better solution.

336
M.-A. Tsompanas et al.
Fig. 6. Scatter plot of all individuals tested during the GA approach for the third run.
The red “X” mark denotes the best individual found.
The scatter plots of the parameters investigated during the evolution of DE
in the third comparison run are outlined in Fig. 5. It is clear that the individuals
produced with the DE approach cover the search space better than the ones
produced by GA (illustrated in Fig. 6). This is attributed to the fact that DE
is designed to tackle models deﬁned in real-values search spaces, whereas GA
is not. As a result, the GA is heavily limited by the randomly produced initial
population, a disadvantage that is alleviated by the DE methodology.

Utilizing Diﬀerential Evolution into Optimizing Targeted Cancer Treatments
337
Fig. 7. Scatter plot of the individuals in the ﬁnal population for the DE approach for
the third run. The red “X” mark denotes the best individual found.
In addition, to clearly portray the reason why GA is easily trapped in local
optima, while DE manages to escape them, Figs. 7 and 8 are given, that present
the ﬁnal population of the DE and GA approaches for the third run, respectively.
The DE approach succeeds in maintaining a high diversity in the ﬁnal population
as shown in Fig. 7, where no duplicate individuals can be spotted. On the other
hand, in Fig. 8 the ﬁnal population of the GA approach is apparently comprised
by multiple copies of just two individuals, which are also very close to each
other. Consequently, the GA can not escape from these two individuals unless a
dramatic mutation happens (not possible as the mutation step size was set here
to s = [−5, 5]%).

338
M.-A. Tsompanas et al.
Fig. 8. Scatter plot of the individuals in the ﬁnal population for the GA approach for
the third run. The red “X” mark denotes the best individual found.
5
Conclusion
The optimization of the design of targeted DDS on a cancer tumor, simulated by
PhysiCell, was studied by utilizing “DE/rand/1” approach. The DE approach
was compared with a standard steady state GA with the same computation
budget, namely 1000 evaluations. The results derived from the comparison runs
of both approaches equipped with the same initial population unveiled that DE
is a more robust algorithm to reach a better solution within the same amount
of generations. On top of that, DE enables the further exploration of the search
space as it maintains a high diversity of the individuals in the ﬁnal population.

Utilizing Diﬀerential Evolution into Optimizing Targeted Cancer Treatments
339
As an aspect of future work, diﬀerent variations of DE can be investigated
with PhysiCell and under alternative ultimate goals, for instance the ability
of niching is well documented in variants of DE [7]. Finally, the conclusions
driven from this study will be applied on ongoing research towards a more wide
applicability platform that will design, develop and evaluate DDSs aiming cancer
tumours.
Acknowledgement. This project has received funding from the European Union’s
Horizon 2020 FET Open programme under grant agreement No. 800983.
References
1. Aktipis, C., Nesse, R.: Evolutionary foundations for cancer biology. Evol. Appl. 6,
144–159 (2013)
2. Bozic, I., Nowak, M.: Timing and heterogeneity of mutations associated with drug
resistance in metastatic cancers. Proc. Natl. Acad. Sci. U.S.A. 111, 15964–15968
(2014)
3. Bull, L.: On the Baldwin eﬀect. Artif. Life 5, 241–246 (1999)
4. Bull, L.: The evolution of sex through the Baldwin eﬀect. Artif. Life 23, 481–492
(2017)
5. Das, S., Mullick, S.S., Suganthan, P.: Recent advances in diﬀerential evolution -
an updated survey. Swarm Evol. Comput. 27, 1–30 (2016)
6. Das, S., Suganthan, P.N.: Diﬀerential evolution: a survey of the state-of-the-art.
IEEE Trans. Evol. Comput. 15, 4–31 (2010)
7. Epitropakis, M.G., Li, X., Burke, E.: A dynamic archive niching diﬀerential evo-
lution algorithm for multimodal optimization. In: 2013 IEEE Congress on Evolu-
tionary Computation, pp. 79–86 (2013)
8. Epitropakis, M.G., Tasoulis, D., Pavlidis, N., Plagianakos, V., Vrahatis, M.:
Enhancing diﬀerential evolution utilizing proximity-based mutation operators.
IEEE Trans. Evol. Comput. 15, 99–119 (2011)
9. Ghaﬀarizadeh, A., Friedman, S., Macklin, P.: BioFVM: an eﬃcient, parallelized
diﬀusive transport solver for 3-D biological simulations. Bioinformatics 32, 1256–
1258 (2015)
10. Ghaﬀarizadeh, A., Heiland, R., Friedman, S., Mumenthaler, S., Macklin, P.: Physi-
Cell: an open source physics-based cell simulator for 3-D multicellular systems.
PLOS Comput. Biol. 14, e1005991 (2018)
11. Ozik, J., Collier, N., Heiland, R., An, G., Macklin, P.: Learning-accelerated dis-
covery of immune-tumour interactions. Mol. Syst. Des. Eng. 4, 747–760 (2019)
12. Preen, R., Bull, L., Adamatzky, A.: Towards an evolvable cancer treatment simu-
lator. Biosystems 182, 1–7 (2019)
13. Price, K.: Diﬀerential evolution vs. the functions of the 2/sup nd/ICEO. In: Pro-
ceedings of 1997 IEEE International Conference on Evolutionary Computation
(ICEC 1997), pp. 153–157 (1997)
14. Qin, A., Huang, V., Suganthan, P.N.: Diﬀerential evolution algorithm with strategy
adaptation for global numerical optimization. IEEE Trans. Evol. Comput. 13, 398–
417 (2008)
15. Schmitt, M., Loeb, L., Salk, J.: The inﬂuence of subclonal resistance mutations on
targeted cancer therapy. Nat. Rev. Clin. Oncol. 13, 335–347 (2016)

340
M.-A. Tsompanas et al.
16. Storn, R., Price, K.: Diﬀerential evolution-a simple and eﬃcient heuristic for global
optimization over continuous spaces. J. Glob. Optim. 11, 341–359 (1997)
17. Sun, J., Luo, C., Wang, Y., He, Z.: The holistic 3M modality of drug delivery
nanosystems for cancer therapy. Nanoscale 5, 845–859 (2013)
18. Thomsen, R.: Multimodal optimization using crowding-based diﬀerential evolution.
In: Proceedings of the 2004 Congress on Evolutionary Computation (IEEE Cat.
No. 04TH8753) 2, pp. 1382–1389 (2004)
19. Tsompanas, M.A., Bull, L., Adamatzky, A., Balaz, I.: Haploid-diploid evolution:
nature’s memetic algorithm. Preprint https://arxiv.org/abs/1911.07302 (2019)
20. Tsompanas, M.A., Bull, L., Adamatzky, A., Balaz, I.: Novelty search employed
into the development of cancer treatment simulations. Inform. Med. Unlocked 19,
100347 (2020)
21. Tsompanas, M.A., Bull, L., Adamatzky, A., Balaz, I.: In silico optimization of
cancer therapies with multiple types of nanoparticles applied at diﬀerent times.
Comput. Methods Programs Biomed. (2020) https://doi.org/10.1016/j.cmpb.2020.
105886
22. Waclaw, B., Bozic, I., Pittman, M., Hruban, R., Vogelstein, B., Nowak, M.: A spa-
tial model predicts that dispersal and cell turnover limit intratumour heterogeneity.
Nature 525, 261–264 (2015)
23. Wang, H., Rahnamayan, S., Sun, H., Omran, M.: Gaussian bare-bones diﬀerential
evolution. IEEE Trans. Cybern. 43, 634–647 (2013)

On an Approach to Evaluation of Health
Care Programme by Markov Decision
Model
Masayuki Horiguchi(B)
Department of Mathmatics, Faculty of Science, Kanagawa University,
2946 Tsuchiya, Hiratsuka, Kanagawa 259-1293, Japan
horiguchi@kanagawa-u.ac.jp
Abstract. In this paper, we consider the evaluation of periodic screen-
ing programme for woman breast cancer and formulate the model as
a partially observable Markov decision process (POMDP). We convert
a POMDP with ﬁnite state, observation state and action spaces to an
equivalent completely observable MDP with continuous state and ﬁnite
action spaces. By this approach, we have an optimal policy from dynamic
programming (DP) equation in an equivalent MDP, but we focus on con-
sidering the evaluation in scenarios of periodic screening for participants
with silent condition of breast cancer and seeking an answer which pro-
gramme is better than others for themselves. The aim of this paper is, by
using the data sets based on cancer registration and estimated param-
eters of survival rates and other ratios related to screening and diag-
noses in Japan, to evaluate some scenarios of breast cancer screening
programme in POMDP.
Keywords: Finite MDP · Evaluation of health care programme ·
POMDP · Practice by MDP
AMS(2020) Subject Classiﬁcation: Primary 90C40 · Secondary
62C10
1
Introduction
There are many papers of studying of the cost eﬀectiveness analysis and guideline
principles from the standpoint of Markov decision model (e.g., [3,12,14,16,17],
and so on). It is important to consider not only the economic management for
medical treatment, surgery or other therapy, etc., but also to minimize risks
of harm to the patients and people who participate in the health care pro-
gramme such as periodic medical examination at work place, quarantine and
so on. In Japan mammography screening without clinical breast examination is
recommended for woman aged 40–74 years and mammography screening with
clinical breast examination is recommended for woman aged 40–64 years in the
breast cancer screening programme [5]. The recommendations are concluded by
c
⃝The Author(s), under exclusive license to Springer Nature Switzerland AG 2021
A. Piunovskiy and Y. Zhang (Eds.): Modern Trends in Controlled Stochastic Processes,
ECC 41, pp. 341–354, 2021. https://doi.org/10.1007/978-3-030-76928-4_18

342
M. Horiguchi
randomized controlled trial (RCT) and other experiments and comparing the
beneﬁts and risks in method of screening and contents of the programme. From
the cancer statistics in Japan screening rate in periodic screening examination
is about 45% and medical examination rate of participants who recalled after
screening is about 80%. The government and municipal governments set numer-
ical targets for the rates to be more than 50% and 90% respectively in the
screening programme and examination, and moreover they engage in continuous
activities in order to achieve the rates of participants including other cancers,
colon, prostate, etc.
In this paper, we consider the evaluation of periodic screening programme
for woman breast cancer and formulate the model as a POMDP. We convert a
POMDP with ﬁnite state, observation state and action spaces to an equivalent
completely observable MDP with continuous state and ﬁnite action spaces. By
this approach, we have an optimal policy from DP equation in an equivalent
MDP, but we focus on considering the evaluation in scenarios of periodic screen-
ing for participants with silent condition of breast cancer and seeking an answer
which programme is better than others. We do not expect to be the main issue in
this paper to estimate the parameters of survival rates and other ratios related
to screening and diagnoses.
One of the aims of this paper is, by using the data sets based on cancer
registration and estimated parameters of survival rates and others in Japan,
to evaluate some scenarios of breast cancer screening programme in POMDP.
We also search for solution from another point of view to achieve the rate of
participants at least 50% coverage and at least 90% coverage those which are
mentioned above, we try to show the diﬀerence of risk in programme between no
screening and screening. From vital statistics [7,21], screening statistics [10,19]
and estimated value of parameters (KapWeb [9]) we set the estimated values to
our model parameters. We hope that our model in this paper will be an help
to improve the rates of breast cancer screening in recommended programme by
ﬂexibility and beneﬁts of MDP. It is diﬃcult to have data related to untreated
and unscreened breast cancer patients, we use the data by public institution
like by SEER [8] including patients. Our approach is similar to the method
of evaluation of relative mortality risk by Maillert et al. [12]. There are some
recent papers related to breast cancer screening modeled as MDP, see [11,14,20].
Otten et al. [14] had considered the case to change the intervals of screening
dynamically, although we ﬁx intervals of screening by one year. Steimle & Denton
[18] and Zhang & Denton [22] had considered the prostate cancer screening in
POMDP. Other type of economic evaluation in management of hospital, Saur´e &
Puterman [16] had discussed on the problem of patient appointment scheduling.
In Sect. 2 we introduce the notation of POMDP and its converted MDP.
Section 3 presents some preliminaries for numerical examples in the next section.
In Sect. 4 we provides detailed numerical results by comparing the trajectories
of cost function and information of state at each step.
It is not our purpose to estimate and/or use the estimated parameters in the
middle of observing the states in dynamical system, because of our stand point

Evaluation of Health Care Programme by MDP
343
of intending to promote participation in the screening, although if we have more
detailed data such as the false positive ratio etc. on clinical trials it is possible
and important to consider approaches with adaptive control based on preventive
intervention and/or avoiding the excessive treatment with regard to the result
of screening.
Before the beginning of each scenario, important elements to describe the
MDP, such as transition matrices and cost functions have been calculated by
estimated parameters from the population-based data set in advance. We do not
consider adaptive control model in this paper. This paper presents the desirable
scenarios and principles for woman breast screening by comparing the values of
relative mortality risks in every scenario in POMDP.
2
Partially Observed Markov Decision Process
In this section we deﬁne the sequential decision model to be examined by the
similar formulation taken in Monahan [13] (see the generalized and abstract
spaces case: refer to Rieder [15], B¨auelre and Rieder [1], Hinderer et al. [6]).
X = {0, 1, 2, . . . , N} is the ﬁnite state space and Xt ∈X is the state at time
t = 0, 1, 2, . . .. We call the process {Xt} core process. Let A = {1, 2, . . . , K} be
the ﬁnite action space and Y = {1, 2, . . . , M} be the ﬁnite observation space. Let
Yt ∈Y denote the observation state at time t = 1, 2, . . . and we call the process
{Yt} observation process. The observation state Yt = yt at time t represents the
information for the decision maker as imprecisely information of unobservable
state of core process. In other words, we cannot observe the state Xt = xt
directly and deduce it by delivering the information of observing state Yt = yt
to the decision maker and we have renewed the apriori belief probability on X
at the time t −1 to the posteriori belief on X at the present time t by using the
updating operator based on Bayesian mechanism.
We denote by P(X) the set of all probability distributions on X. Pat =
(pat(j|i)) denotes the transition law of core process {Xt} and pat(j|i) is the
probability of state transition from the present state Xt = i ∈S to the next state
Xt+1 = j when an action at ∈A is taken at time t. Let Hn = A×Y ×Hn−1, n =
1, 2, . . . where H0 is arbitrary. hn = (a0, y1, a1, y2, . . . , an−1, yn) ∈Hn is the
history of action and observed state up to time n.
Ca(x, y) is the immediate cost function deﬁned on X × Y × A to R. V0(i) is
the terminal cost function deﬁned on X to R. Let πi(0) = Pr (X0 = i) , i ∈S
the initial distribution of core process.
Let qa(y|i, j) be the transition kernel of observing state y from X × X × Y
to [0, ∞) for a ∈A when at the present time an action a is taken and the last
state i and the present state j are given.
A policy for ﬁnite horizon T is a sequence π = (f0, f1, . . . , fT −1) of functions
fk : Hk →A, 0 ≦k ≦T −1. The set of all policies for ﬁnite horizon T is denoted
by ΠT .
For each policy π ∈ΠT and initial state X0 = x ∈X, a probability measure
that describes the stochastic behavior of the partially observed system is deﬁned
by usual way (cf. [2,4]).

344
M. Horiguchi
Deﬁne μt = (μt(i)) ∈P(X) the information vector at time t, where we have
set
μt(i) = Pr (Xt = i|μ0, a0, y1, a1, y2, . . . , at−1, yt) = Pr (Xt = i|μ0, ht) , i ∈X.
Let μ0 = (μ0(i)) ∈P(X) be the apriori distribution of unobservable states
on X at the beginning period t = 0.
After knowing the information from observation Yt = yt at time t = 1, 2, . . .,
apriori distribution as the information of unobserved state xt is updated by the
operator Φat−1, where
˜qa(j, y|μ) =

i∈X
˜qa(j, y|i)μ(i) =

i∈S
pa(j|i)qa(y|i, j)μ(i)
on X × Y given μ ∈P(X) for each a ∈A:
Φat−1(μt−1, yt)(j) =
˜qat−1(j, yt|μt−1)

j∈X
˜qat−1(j, yt|μt−1)
=

i∈X
pat−1(j|i)qat−1(yt|i, j)μt−1(i)

j∈X

i∈X
pat−1(j|i)qat−1(yt|i, j)μt−1(i)
(1)
and we set μt(j) = Φat−1(μt−1, yt)(j), j ∈X.
Let ht = (a0, y1, a1, y2, . . . , at−1, yt) ∈Ht. Then, the posteriori distribution
μt(ht) is calculated recursively by the updating operator Φai, i = 1, . . . , t as the
following:
μi(hi) = Φai−1(Φai−2(· · · Φa1(Φa0(μ0, y1), y2), · · · , yi−1), yi)
The following theorem is well-known for ﬁnite state and action partially
observable model.
Theorem 1. For any ﬁxed policy π = (a0, a1, . . . , aT −1) the sequence of prob-
ability distributions {μi(·)} , i = 1, 2, · · · T is a Markov process, i.e., for any
measurable set Γ ∈P(X),
Pr (μi ∈Γ|μ0, a0, μ1, . . . , μt−1, at−1) = Pr (μi ∈Γ|μi−1, ai−1) .
We can convert a partially observable Markov decision model to an equivalent
Markov decision model with continuous state space S = P(X).
By the requirement for describing the transitions of observation and unob-
served state in the system, we deﬁne the transition law Qa, a ∈A with stochastic
kernel qa(y|x, x′) from X ×X ×Y to [0, ∞) as follows. For μ ∈P(X) and a ∈A,
Qa(μ; y) =

x′∈X

x∈X
pa(x′|x)qa(y|x, x′)μ(x)

Evaluation of Health Care Programme by MDP
345
and
Qa(μ; x′, y) =

x∈X
pa(x′|x)qa(y|x, x′)μ(x).
The immediate cost function is deﬁned by
Ca(μ) =

x∈X
Ca(x)μ(x), for a ∈A, μ ∈P(X).
The terminal cost function is deﬁned by
V0(μ) =

x∈X
V0(x)μ(x), μ ∈P(X).
We denote by VN,π(μ) as the minimum expected undiscounted total cost
function if there are N steps to go and the initial state is μ and policy π ∈Π is
taken.
For history hN = (a0, y1, a1, y2, . . . , aN−1, yN), an action ai ∈A is taken and
the information of system yi is observed up to time N. Then we know transition
law Qa at time i and states transition of (μi, yi) ∈P(X) × Y occurs according
to Qai−1(μi; x, yi), x ∈X.
Note that the information vector μ = (μi), i ∈X is updated by
μi(x′) = Φai−1(μi, yi)(x′) = Qa(μ; x′, y)
Qa(μ; y) .
Then we have the following Dynamic Programming equation:
Vn(μ) = min
a∈A
⎧
⎨
⎩Ca(μ) +

y∈Y
Vn−1 (Φa(μ, y)) Qa(μ; y)
⎫
⎬
⎭, n = 1, 2, . . . , N,
(2)
where V0 is terminal cost function.
From the results of ﬁnite horizon MDP we have the optimal policy for an
equivalent MDP by solving DP equation recursively and backwards, from step
n = 1 to N.
Denote a∗
N−n the minimizer of (2) for each step n = 1, 2, . . . , N and let
π∗= (a∗
0, a∗
1, . . . , a∗
N−1).
Theorem 2. The sequence of minimizer of (2) is an optimal policy for an equiv-
alent MDP, i.e., VN,π∗(μ0) = supπ∈Π VN,π(μ0).
3
POMDP for Evaluation of Health Care Programmes
We consider the evaluation of health care programme in mass screening of breast
cancer as a POMDP. The state transitions of core process is shown in Fig. 1 and
it is similar to the transition model of Maillart et al. [12]. In addition, we give
subtree of decision making at time n in Fig. 2.

346
M. Horiguchi
The state space is X = {0, 1, 2, 3, 4}. Each state x ∈X represents the state
of health in which screening participant is. Five states are represented as follows:
State 0: no breast cancer, state 1: early breast cancer, state 2: later/advanced
breast cancer, state 3: breast cancer induced death and state 4: non-breast cancer
induced death. Transition matrix P = (pij) leads to the state transitions at each
time t = 1, 2, . . . , and satisﬁes the assumption:
⎧
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎩
pii > 0 for i = 0, 1, 2, 3, 4,
pij > 0 if j = i + 1, i = 0, 1, 2,
pi4 > 0 if i = 0, 1, 2,
pij = 0 otherwise.
The action space is A = {a0(no screening), a1(screening)}. Every year in the
period of screening programme, participants have two alternatives of a0: no
screening and a1:screening, but for simplicity, under the scenario in this paper
participants are arranged in advance which alternative is selected in the pro-
gramme. The observing space is Y = {0, 1} where information (observed) state
0 is negative from the screening test and information state 1 is positive. The
parameters pij(α) = p(j|i)(α) of transition matrices P(α) = (pij(α)) are deter-
mined by the age α of participants. Hence the state transition of core process
occurs by Pa(α), a ∈A and the Markov process of core state is non-stationary
(non-homogeneous) (cf. [2,4]). Let Ii = {n ∈N|25 + 5(i −1) ≤n < 25 + 5i}. By
abuse of notation, we let [αi, αi) stand for Ii.
We set immediate cost functions Cα
a for participant who aged α years and
α ∈Ii = [αi, αi).
Cα
a0(μ) = μ(0)r0(α) + μ(1)r1(α) + μ(2)r2(α),
Cα
a1(μ) = μ(0)d0(α)r0(α) + μ(1)d1(α)r1(α) + μ(2)d2(α)r2(α),
where r0(α): recall rate of screening participants who aged α years, r1(α): a
complementary rate of 10-year relative survival rate e1(α) of patients of breast
cancer who aged α years and is in category 1 of breast density (Breast Imaging
Reporting and Data System BI-RADS) category). Hence we deﬁne r1(α) =
1−e1(α). Also, we deﬁne r2(α) = 1−e2(α), where e2(α) is average (aggregation)
of the survival rates of category 2 and 3. d0(α) denotes predictive value of positive
screening results and d1(α) is the sensitivity of mammography in association with
breast densities in category 1, and d2(α) is average of sensitivities of patients
who is in category 2 or 3. The above values of parameters are surveyed and
estimated by authors as follows: r0, d0: [10,19], r1, r2(e1, e2): [9], d1, d2: [19].

Evaluation of Health Care Programme by MDP
347
p00
p12
p11
p04
p12
p14
p22
p23
p33
p24
p44
0
1
2
3
4
Fig. 1. Transition probabilities of core process
In order to update the information μ and calculate the value functions
VN,π, Vn, we set qat−1(y|i, j), t ≥1 in Eq. (1) when y = 0 (screening test was
negative) by
qat−1(0|0, 0) = qat−1(0|0, 1) = qat−1(0|0, 4) = 1 −r0(αt−1),
qat−1(0|1, 1) = qat−1(0|1, 2) = qat−1(0|1, 4) = 1 −d1(αt−1),
qat−1(0|2, 2) = qat−1(0|2, 3) = qat−1(0|2, 4) = 1 −d2(αt−1),
qat−1(0|i, j) = 0
otherwise,
where αt is age of participant for screening at the t-th period from the beginning
while in screening programme. Moreover, to deduce the transition probabilities
pij(α) = p(j|i)(α) for screening participants who aged α years, we follow the
estimated cancer relative survival rates K1 and K2 of 5 years whose rates are
deﬁned similar to r1(α) and r2(α) respectively at clinical stages by KapWeb
[9] in Japan. We also used mortality rate a of excluding the patients cased
breast cancer and estimated incident rate b of breast cancer from vital and
cancer statistics [7,21] respectively in 2015 and the proportion c of early and non
inﬁltrating breast cancer found in the past screening programme from Annual
report on breast cancer screening in Japan in 2013 [10].

348
M. Horiguchi
information state: µn
A = {a0, a1}
a0: no screening
a1: screening
information state: µn+1
A = {a0, a1}
a0: no screening
a1: screening
further
screening
a0 or a1 is selected
Yn = 0(negative) is observed
a1 is selected, and
Yn = 1(positive) is observed
Fig. 2. Subtree of decision making at time n in POMDP
For the age α ∈Ii = [αi, αi), the transition matrix P(α) = (pij(α)) is
required to satisfy the following equations: let p = p11(α), q = p22(α),
p5 + (1 −p −a)(p4 + p3q + p2q2 + pq3 + q4) = K1(α),
q5 = K2(α),
p12 = bc.
In this paper, we will restrict our attention to compare the scenarios of screen-
ing program for participants who has no symptom of breast cancer or never
develops symptoms within continued period of the screening program, the value
function represents the accumulated mortality risk of people in each scenario.
Hence let Qa0(μ; y) ≡1, Vn−1 (Φa1(μ; 1)) ≡0 and we do not suit the optimal
policy π∗derived from Eq. (2) to the best recommendation for the screening
programme. Instead, under POMDP, we show the property of each programme
including the recommended one by public institution [5].

Evaluation of Health Care Programme by MDP
349
4
Numerical Analysis
Almost of all statistical data used in this section are based on the aggregate
statistics every 5 or 10 years of age of screening participants, the transition
probability matrices and parameters for participants in the programme may
take diﬀerent values at every ﬁve years old (the beginnings of interval are 25,
30, 35, 40, 45, 50, 55, 60, 65, 70, 75 and 80 years old).
We show 9 scenarios in Table 1 and consider the accumulative relative mortal-
ity risk in each scenario. For example, in scenario 1 the start age of participant
is at 40 years old and begins the screening test at the age and continue the
screening until at 84 years old. The duration of the scenario is N = 45. We give
below the transition matrices P(α) of core process for α ∈Ii, i = 1, 2, . . . , 12.
The values of parameters are shown in Table 2 and 3. For each scenario, at
age of the programme begins for participant we set the information state vec-
tor μ′
0 = (1, 0, 0, 0, 0)′. The process of scenario 1 begins by transition matrix
P(α), α ∈I4 = [40, 45), i.e., the ﬁrst transition of state is occurred by P(40).
After updating the vector μt at time t (as information of participant to the
screening programme and in t-th year of the scenario) until the ﬁnal piriod N,
we calculate the values V1, V2, . . . , VN recursively from DP Eq. (2) by backward
induction.
Table 1. scenarios of screening programme
Scenario Age 25–39
Age 40–64 Age 65–84
Duration N(years)
1
−
Screening
45
2
−
Screening
No screening
45
3
−
No screening
45
4
No screening
60
5
No screening Screening
60
6
No screening Screening
No screening
60
7
No screening
60
8
−
Screening: aged 45–74 years 35
9
−
Screening
−
25
α ∈I1 : P(α) =
⎛
⎜
⎜
⎜
⎜
⎝
0.9999291 0.0000645
0
0
0.0000064
0
0.9441060 0.0558876
0
0.0000064
0
0
0.9472137 0.0527799 0.0000064
0
0
0
1
0
0
0
0
0
1
⎞
⎟
⎟
⎟
⎟
⎠

350
M. Horiguchi
Table 2. Calues of the parameters (1)
α ∈Ii I1
I2, I3 I4, I5
I6, I7 I8, I9 I10, I11 I12
r0(α)
0.04
0.053 0.116
0.095 0.072 0.048
0.048
r1(α)
0.947
0.951 0.971
0.969 0.992 1.000
1.000
r2(α)
0.626
0.728 0.768
0.743 0.758 0.801
0.796
d0(α)
0.007
0.016 0.019
0.025 0.041 0.069
0.105
d1(α)
−
−
1.000
0.875 0.912 −
−
d2(α)
−
−
0.632
0.741 0.794 −
−
K1(α) 0.955
0.981 0.992
0.986 1.000 1.000
1.000
K2(α) 0.7625 0.827 0.8785 0.848 0.866 0.894
0.914
Table 3. Values of parameters (2)
α ∈Ii
I1
I2
I3
I4
I5
I6
a
6.45 × 10−6 9.08 × 10−6 1.41 × 10−5 2.47 × 10−5 3.33 × 10−5 4.65 × 10−5
b
8.60 × 10−5 2.46 × 10−4 6.96 × 10−4 1.50 × 10−3 2.31 × 10−3 2.23 × 10−3
c
0.50 0.25
0.685
0.269
0.726
0.256
0.728 0.173
α ∈Ii
I7
I8
I9
I10
I11
I12
a
6.41 × 10−5 1.15 × 10−4 1.99 × 10−4 2.86 × 10−4 7.54 × 10−4 1.07 × 10−4
b
2.18 × 10−3 2.34 × 10−3 2.31 × 10−3 2.25 × 10−3 2.00 × 10−3 1.70 × 10−3
c
0.728 0.173
0.764
0.167
0.778
0.173
0.782 0.091
α ∈I2 : P(α) =
⎛
⎜
⎜
⎜
⎜
⎝
0.9997562 0.0002347
0
0
0.0000091
0
0.9441060 0.0558849
0
0.0000091
0
0
0.9627225 0.0372685 0.0000091
0
0
0
1
0
0
0
0
0
1
⎞
⎟
⎟
⎟
⎟
⎠
α ∈I3 : P(α) =
⎛
⎜
⎜
⎜
⎜
⎝
0.9993219 0.0006640
0
0
0.0000141
0
0.944171 0.0558149
0
0.0000141
0
0
0.9627225 0.0372634 0.0000141
0
0
0
1
0
0
0
0
0
1
⎞
⎟
⎟
⎟
⎟
⎠
α ∈I4 : P(α) =
⎛
⎜
⎜
⎜
⎜
⎝
0.9985003 0.0014750
0
0
0.0000247
0
0.9672830 0.0326923
0
0.0000247
0
0
0.9744249 0.0255504 0.0000247
0
0
0
1
0
0
0
0
0
1
⎞
⎟
⎟
⎟
⎟
⎠

Evaluation of Health Care Programme by MDP
351
α ∈I5 : P(α) =
⎛
⎜
⎜
⎜
⎜
⎝
0.9976973 0.0022694
0
0
0.0000333
0
0.9412310 0.0587357
0
0.0000333
0
0
0.9744249 0.0255418 0.0000333
0
0
0
1
0
0
0
0
0
1
⎞
⎟
⎟
⎟
⎟
⎠
α ∈I6 : P(α) =
⎛
⎜
⎜
⎜
⎜
⎝
0.9979370 0.0020164
0
0
0.0000465
0
0.9539420 0.0460115
0
0.0000465
0
0
0.9675628 0.0323907 0.0000465
0
0
0
1
0
0
0
0
0
1
⎞
⎟
⎟
⎟
⎟
⎠
α ∈I7 : P(α) =
⎛
⎜
⎜
⎜
⎜
⎝
0.9979681 0.0019678
0
0
0.0000641
0
0.9542050 0.0457309
0
0.0000641
0
0
0.9675628 0.0323731 0.0000641
0
0
0
1
0
0
0
0
0
1
⎞
⎟
⎟
⎟
⎟
⎠
α ∈I8 : P(α) =
⎛
⎜
⎜
⎜
⎜
⎝
0.9977112 0.0021739
0
0
0.0001149
0
0.9483570 0.0515281
0
0.0001149
0
0
0.9716360 0.0282491 0.0001149
0
0
0
1
0
0
0
0
0
1
⎞
⎟
⎟
⎟
⎟
⎠
α ∈I9 : P(α) =
⎛
⎜
⎜
⎜
⎜
⎝
0.9976481 0.0021525
0
0
0.0001995
0
0.9498140 0.0499865
0
0.0001995
0
0
0.9716360 0.0281646 0.0001995
0
0
0
1
0
0
0
0
0
1
⎞
⎟
⎟
⎟
⎟
⎠
α ∈I10 : P(α) =
⎛
⎜
⎜
⎜
⎜
⎝
0.9975742 0.0021398
0
0.0002860
0
0.9370820 0.0626320
0
0.0002860
0
0
0.9778393 0.0218746 0.0002860
0
0
0
1
0
0
0
0
0
1
⎞
⎟
⎟
⎟
⎟
⎠
α ∈I11 : P(α) =
⎛
⎜
⎜
⎜
⎜
⎝
0.9976596 0.0019001
0
0
0.0004403
0
0.9405450 0.0590147
0
0.0004403
0
0
0.9778393 0.0217204 0.0004403
0
0
0
1
0
0
0
0
0
1
⎞
⎟
⎟
⎟
⎟
⎠
α ∈I12 : P(α) =
⎛
⎜
⎜
⎜
⎜
⎝
0.9977629 0.0014832
0
0
0.0007538
0
0.9302950 0.0689512
0
0.0007538
0
0
0.9830340 0.0162122 0.0007538
0
0
0
1
0
0
0
0
0
1
⎞
⎟
⎟
⎟
⎟
⎠

352
M. Horiguchi
Fig. 3. Trajectories of accumulated lifetime mortality risks
Fig. 4. Trajectories of information µt(0)
Figure 3 shows the trajectories of value (accumulated lifetime mortality risk)
in each scenario. In Fig. 4, the trajectories of {μt(0)} in scenario 6 and 7
are shown. In contrast to choosing the option of no screening at each period
decreased the information probability μt(0) of no breast cancer in scenario 6,
continuous participation to screening brought almost ﬂat tendency with few
decrease to the information probability μt(0) with respect to the time t. Trajec-
tories of {μt(0)} in other scenarios has similar tendency as the main factor of
those variations is whether the scenario itself has the duration of no screening
or not. So we omit to show trajectories {μt(0)} in the case of other variations.
From the trajectories in Fig. 3, if the longer duration of no screening time
is included in the scenario which participants select, the higher accumulated
risk VN of the scenario is brought. The scenario 8 (mammographic screening
without clinical breast examination) and 9 (mammographic screening without
clinical breast examination) are both recommended for people living in Japan
by National Cancer Center of Japan (NCCJ) (cf. [5]).

Evaluation of Health Care Programme by MDP
353
Acknowledgement. This work was partially supported by MHLW Cancer Control
Promotion Program Grant Number JPMH18EA1003.
References
1. B¨auerle, N., Rieder, U.: Markov Decision Processes with Applications to Finance.
Springer, Heidelberg (2011)
2. Bertsekas, D.P., Shreve, S.E.: Stochastic Optimal Control: The Discrete-Time
Case. Academic Press, New York (1978)
3. Drummond, M.F., Sculpher, M.J., Claxton, K., Stoddart, G.L., Torrance, G.W.:
Methods for the Economic Evaluation of Health Care Programmes. Oxford Uni-
versity Press, Oxford (2015)
4. Dynkin, E.B., Yushkevich, A.A.: Controlled Markov Processes. Springer, New York
(1979)
5. Hamashima, C., et al.: The Japanese guidelines for breast cancer screening. Jpn.
J. Clin. 46, 482–492 (2016)
6. Hinderer, K., Rieder, U., Stieglitz, M.: Dynamic Optimization: Deterministic and
Stochastic Models. Springer, Cham (2016)
7. Hori, M., Matsuda, T., Shibata, A., Katanoda, K., Sobue, T., Nishimoto, H.: Can-
cer incidence and incidence rates in Japan in 2009: a study of 32 population-based
cancer registries for the Monitoring of Cancer Incidence in Japan (MCIJ) project.
Jpn. J. Clin. 45, 884–891 (2015)
8. Howlader, N., Noone, A.M., Krapcho, M., Miller, D., Brest, A., Yu, M., Ruhl,
J., Tatalovich, Z., Mariotto, A., Lewis, D.R., Chen, H.S., Feuer, E.J., Cronin,
K.A. (eds.) SEER Cancer Statistics Review, 1975–2017. National Cancer Institute,
Bethesda, MD, based on November 2019 SEER data submission, posted to the
SEER web site, April 2020. https://seer.cancer.gov/csr/1975 2017/
9. KapWeb: Survival statistics of Japanese association of Clinical Cancer Centers.
https://kapweb.chiba-cancer-registry.org
10. Kasahara, Y., Tsuji, I., Ohnuki, K., Koibuchi, Y., Ban, K., Furukawa, J., Masuoka,
H., Murata, Y., Morita, T., Yoshida, M., Rai, Y.: Annual report 2013 on breast
cancer screening in Japan. J. Jpn. Assoc. Breast Cancer Screen. 23, 84–97 (2014)
11. Madadi, M., Zhang, S.: Cost-eﬀectiveness analysis of breast cancer mammography
screening policies considering uncertainty in women’s adherence. In: Kong, N.,
Zhang, S. (eds.) Decision Analytics and Optimization in Disease Prevention and
Treatment, pp. 223–240. Wiley, New York (2018)
12. Maillart, L.M., Ivy, J.S., Ransom, S., Diehl, K.: Assessing dynamic breast cancer
screening policies. Oper. Res. 56, 1411–1427 (2008)
13. Monahan, G.E.: A survey of partially observable Markov decision processes: theory,
models, and algorithms. Manag. Sci. 28, 1–16 (1982)
14. Otten, J.W.M., Witteveen, A., Vliegen, I.M.H., Siesling, S., Timmer, J.B., IJzer-
man, M.J.: Stratiﬁed breast cancer follow-up using a partially observable Markov
decision process. In: Boucherie,R., van Dijk, N. (eds.) Markov Decision Processes
in Practice, pp. 223–244. Springer, New York (2017)
15. Rieder, U.: Structural results for partially observed control models. ZOR Methods
Models Oper. Res. 35, 473–490 (1991)
16. Saur´e,
A.,
Puterman,
M.L.:
Advance
patient
appointment
scheduling.
In:
Boucherie, R., van Dijk, N. (eds.) Markov Decision Processes in Practice, pp. 245–
168. Springer, New York (2017)

354
M. Horiguchi
17. Siebert, U., Alagoz, O., Bayoumi, A.M., Jahn, B., Owens, D.K., Cohen, D.J.,
Kuntz, K.M.: State-transition modeling: a report of the ISPOR-SMDM modeling
good research practices task force-3. Value Health 15, 812–820 (2012)
18. Steimle, L.N., Denton, B.T.: Markov decision processes for screening and treatment
of chronic diseases. In: Boucherie, R., van Dijk, N. (eds.) Markov Decision Processes
in Practice, pp. 189–222. Springer, New York (2017)
19. Suzuki, A., Kuriyama, S., Kawai, M., Amari, M., Takeda, M., Ishida, T., Ohnuki,
K., Nishino, Y., Tsuji, I., Shibuya, D., Ohuchi, N.: Age-speciﬁc interval breast can-
cers in Japan: estimation of the proper sensitivity of screening using a population-
based cancer registry. Cancer Sci. 99, 2264–2267 (2008)
20. Tunc, S., Alagoz, O., Chhatwal, J., Burnside, E.S.: Using ﬁnite-horizon Markov
decision processes for optimizing post-manmography diagnostic decisions. In:
Kong, N., Zhang, S. (eds.) Decision Analytics and Optimization in Disease Pre-
vention and Treatment, pp. 183–200. Wiley, New York (2018)
21. Vital Statistics Japan (Ministry of Health, Labour and Welfare). https://ganjoho.
jp/en/professional/statistics/table download.html
22. Zhang, J., Denton, B.T.: Partially observable Markov decision processes for
prostate cancer screening, surveillance, and treatment: a budgeted sampling
approximation method. In: Kong, N., Zhang, S. (eds.) Decision Analytics and
Optimization in Disease Prevention and Treatment, pp. 201–222. Wiley, New York
(2018)

Author Index
A
Adamatzky, Andrew, 328
Almudevar, Anthony, 298
Anton, Elene, 266
Avrachenkov, Konstantin E., 192
Ayesta, Urtzi, 266
B
Balaz, Igor, 328
Bäuerle, Nicole, 108
Borkar, Vivek S., 192
Bull, Larry, 328
D
de Oliveira, André Marcorin, 87
Deng, Fan, 221
Diep, Quoc Bao, 313
do Valle Costa, Oswaldo Luiz, 87
Dolhare, Hars P., 192
F
Feinberg, Eugene A., 1
G
Glauner, Alexander, 108
González-Sánchez, David, 148
Guo, Xin, 221
H
Horiguchi, Masayuki, 341
Huo, Haifeng, 19
J
Jasso-Fuentes, Héctor, 57
Jonckheere, Matthieu, 266
K
Kara, Ali Devran, 166
Kasyanov, Pavlo O., 1
L
Lipets, Vladimir, 284
Liu, Li, 248
M
Menaldi, Jose-Luis, 57
Minjárez-Sosa, J. Adolfo, 148
P
Patil, Kishor, 192
Piunovskiy, Alexey B., 38
R
Robles-Aguilar, Alan D., 148
S
Semenikhin, Konstantin V., 129
Sonin, Isaac M., 248
T
Truong, Thanh Cong, 313
Tsompanas, Michail-Antisthenis, 328
© The Editor(s) (if applicable) and The Author(s), under exclusive license
to Springer Nature Switzerland AG 2021
A. Piunovskiy and Y. Zhang (Eds.): Modern Trends in Controlled Stochastic Processes,
ECC 41, pp. 355–356, 2021. https://doi.org/10.1007/978-3-030-76928-4

356
Author Index
V
Vásquez-Rojas, Fidel, 57
Verloop, Ina Maria, 266
W
Wen, Xian, 19
Y
Yüksel, Serdar, 166
Z
Zadorojniy, Alexander, 284
Zelinka, Ivan, 313
Zgurovsky, Michael Z., 1
Zhang, Yi, 221

