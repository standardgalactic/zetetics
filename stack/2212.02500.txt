PhysDiff: Physics-Guided Human Motion Diffusion Model
Ye Yuan
Jiaming Song
Umar Iqbal
Arash Vahdat
Jan Kautz
NVIDIA
https://nvlabs.github.io/PhysDiff
“Person got down and is crawling across the floor.”
“A person walks around two edges of a room.”
Penetration
Floating
Ours
SOTA Motion Diffusion Model
Physics-Based
Motion
Projection
Denoising
Motion
Diffusion
N times
Noise
Text or Action
Denoising
Motion
Diffusion
N times
Noise
Text or Action
Floating
Figure 1. Our PhysDiff model generates physically-plausible motions using a physics-based motion projection in the diffusion process,
eliminating artifacts such as floating, ground penetration, and foot sliding, often observed with state-of-the-art models.
Abstract
Denoising diffusion models hold great promise for gen-
erating diverse and realistic human motions. However, ex-
isting motion diffusion models largely disregard the laws
of physics in the diffusion process and often generate
physically-implausible motions with pronounced artifacts
such as floating, foot sliding, and ground penetration. This
seriously impacts the quality of generated motions and lim-
its their real-world application.
To address this issue,
we present a novel physics-guided motion diffusion model
(PhysDiff), which incorporates physical constraints into
the diffusion process. Specifically, we propose a physics-
based motion projection module that uses motion imitation
in a physics simulator to project the denoised motion of a
diffusion step to a physically-plausible motion. The pro-
jected motion is further used in the next diffusion step to
guide the denoising diffusion process. Intuitively, the use
of physics in our model iteratively pulls the motion toward
a physically-plausible space, which cannot be achieved by
simple post-processing. Experiments on large-scale human
motion datasets show that our approach achieves state-of-
the-art motion quality and improves physical plausibility
drastically (>78% for all datasets).
1. Introduction
Deep learning-based human motion generation is an im-
portant task with numerous applications in animation, gam-
ing, and virtual reality. In common settings such as text-to-
motion synthesis, we need to learn a conditional generative
model that can capture the multi-modal distribution of hu-
man motions. The distribution can be highly complex due
to the high variety of human motions and the intricate in-
teraction between human body parts. Denoising diffusion
models [75, 25, 76] are a class of generative models that are
especially suited for this task due to their strong ability to
model complex distributions, which has been demonstrated
extensively in the image generation domain [68, 63, 67, 13].
These models have exhibited strong mode coverage often
indicated by high test likelihood [77, 33, 81]. They also
arXiv:2212.02500v3  [cs.CV]  18 Aug 2023

One Projection Step
“A person slowly walks forward.”
Four Projection Steps
Figure 2. (Left) Performing one physics-based projection step
(post-processing) at the end yields unnatural motion since the mo-
tion is too physically-implausible to correct. (Right) Our approach
solves this issue by iteratively applying physics and diffusion.
have better training stability compared to generative adver-
sarial networks (GANs [19]) and better sample quality com-
pared to variational autoencoders (VAEs [35]) and normal-
izing flows [74, 81, 80, 4]. Motivated by this, recent works
have proposed motion diffusion models [79, 94] which sig-
nificantly outperform standard deep generative models in
motion generation performance.
However, existing motion diffusion models overlook one
essential aspect of human motion – the underlying laws
of physics.
Even though diffusion models have a supe-
rior ability to model the distribution of human motion, they
still have no explicit mechanisms to enforce physical con-
straints or model the complex dynamics induced by forces
and contact. As a result, the motions they generate often
contain pronounced artifacts such as floating, foot sliding,
and ground penetration. This severely hinders many real-
world applications such as animation and virtual reality,
where humans are highly sensitive to the slightest clue of
physical inaccuracy [64, 27]. In light of this, a critical prob-
lem we need to address is making human motion diffusion
models physics-aware.
To tackle this problem, we propose a novel physics-
guided motion diffusion model (PhysDiff) that instills the
laws of physics into the denoising diffusion process. Specif-
ically, PhysDiff leverages a physics-based motion projec-
tion module (details provided later) that projects an input
motion to a physically-plausible space. During the diffusion
process, we use the motion projection module to project
the denoised motion of a diffusion step into a physically-
plausible motion. This new motion is further used in the
next diffusion step to guide the denoising diffusion process.
Note that it may be tempting to only add the physics-based
projection at the end of the diffusion process, i.e., using
physics as a post-processing step. However, this can pro-
duce unnatural motions since the final denoised kinematic
motion from diffusion may be too physically-implausible to
be corrected by physics (see Fig. 2 for an example) and the
motion may be pushed away from the data distribution. In-
stead, we need to embed the projection in the diffusion pro-
cess and apply physics and diffusion iteratively to keep the
motion close to the data distribution while moving toward
the physically-plausible space (see Sec. 4.4).
The physics-based motion projection module serves the
vital role of enforcing physical constraints in PhysDiff,
which is achieved by motion imitation in a physics simu-
lator. Specifically, using large-scale motion capture data,
we train a motion imitation policy that can control a char-
acter agent in the simulator to mimic a vast range of input
motions. The resulting simulated motion enforces physi-
cal constraints and removes artifacts such as floating, foot
sliding, and ground penetration. Once trained, the motion
imitation policy can be used to mimic the denoised motion
of a diffusion step to output a physically-plausible motion.
We evaluate our model, PhysDiff, on two tasks: text-to-
motion generation and action-to-motion generation. Since
our approach is agnostic to the specific instantiation of the
denoising network used for diffusion, we test two state-of-
the-art (SOTA) motion diffusion models (MDM [79] and
MotionDiffuse [94]) as our model’s denoiser.
For text-
to-motion generation, our model outperforms SOTA mo-
tion diffusion models significantly on the large-scale Hu-
manML3D [22] benchmark, reducing physical errors by
more than 86% while also improving the motion quality
by more than 20% as measured by the Frechet inception
distance (FID). For action-to-motion generation, our model
again improves the physics error metric by more than 78%
on HumanAct12 [23] and 94% on UESTC [29] while also
achieving competitive FID scores.
We further perform extensive experiments to investigate
various schedules of the physics-based projection, i.e., at
which diffusion timesteps to perform the projection. In-
terestingly, we observe a trade-off between physical plau-
sibility and motion quality when varying the number of
physics-based projection steps.
Specifically, while more
projection steps always lead to better physical plausibil-
ity, the motion quality increases before a certain number
of steps and decreases after that, i.e., the resulting motion
satisfies the physical constraints but still may look unnatu-
ral. This observation guides us to use a balanced number
of physics-based projection steps where both high physi-
cal plausibility and motion quality is achieved. We also
find that adding the physics-based projection to late diffu-
sion steps performs better than early steps. We hypothesize
that motions from early diffusion steps may tend toward the
mean motion of the training data and the physics-based pro-
jection could push the motion further away from the data
distribution, thus hampering the diffusion process. Finally,
we also show that our approach outperforms physics-based
post-processing (single or multiple steps) in motion quality
and physical plausibility significantly.
Our contributions are summarized as follows:
• We present a novel physics-guided motion diffusion
model that generates physically-plausible motions by
instilling the laws of physics into the diffusion process.
Its plug-and-play nature makes it flexible to use with

different kinematic diffusion models.
• We propose to leverage human motion imitation in a
physics simulator as a motion projection module to en-
force physical constraints.
• Our model achieves SOTA performance in motion
quality and drastically improves physical plausibility
on large-scale motion datasets. Our extensive analy-
sis also provides insights such as schedules and trade-
offs, and we demonstrate significant improvements
over physics-based post-processing.
2. Related Work
Denoising Diffusion Models. Score-based denoising diffu-
sion models [75, 25, 77, 76] have achieved great successes
in various applications such as image generation [68, 63,
67, 74, 81], text-to-speech synthesis [36], 3D shape gener-
ation [97, 46, 93], machine learning security [55], as well
as human motion generation [79, 94, 66]. These models
are trained via denoising autoencoder objectives that can be
interpreted as score matching [82], and generate samples
via an iterative denoising procedure that may use stochas-
tic updates [6, 5, 15, 96] which solve stochastic differential
equations (SDEs) or deterministic updates [76, 44, 42, 95,
30, 14] which solve ordinary differential equations (ODEs).
To perform conditional generation, the most common
technique is classifier(-free) guidance [13, 26]. However,
it requires training the model specifically over paired data
and conditions. Alternatively, one could use pretrained dif-
fusion models that are trained only for unconditional gener-
ation. For example, SDEdit [53] modifies the initialization
of the diffusion model to synthesize or edit an existing im-
age via colored strokes. In image domains, various methods
solve linear inverse problems by repeatedly injecting known
information to the diffusion process [45, 12, 77, 11, 32, 31].
A similar idea is applied to human motion diffusion models
in the context of motion infilling [79]. In our case, generat-
ing physically-plausible motions with diffusion models has
a different set of challenges. First, the constraint is specified
through a physics simulator, which is non-differentiable.
Second, the physics-based projection itself is relatively ex-
pensive to compute, unlike image-based constraints which
use much less compute than the diffusion model in general.
As a result, we cannot simply apply the physics-based pro-
jection to every step of the sampling process.
Human Motion Generation. Early work on motion gen-
eration adopts deterministic human motion modeling which
only generates a single motion [17, 39, 18, 52, 57, 20, 2,
83, 51, 88]. Since human motions are stochastic in nature,
more work has started to use deep generative models which
avoid the mode averaging problem common in determinis-
tic methods. These methods often use GANs or VAEs to
generate motions from various conditions such as past mo-
tions [7, 85, 3, 90, 89], key frames [24], music [38, 98, 37],
text [1, 9, 22, 62], and action labels [23, 61, 10]. Recently,
denoising diffusion models [75, 25, 76] have emerged as
a new class of generative models that combine the advan-
tages of standard generative models.
Therefore, several
motion diffusion models [79, 94, 66] have been proposed
which demonstrate SOTA motion generation performance.
However, existing motion diffusion models often produce
physically-implausible motions since they disregard phys-
ical constraints in the diffusion process. Our method ad-
dresses this problem by guiding the diffusion process with
a physics-based motion projection module.
Physics-Based Human Motion Modeling. Physics-based
human motion imitation is first applied to learning locomo-
tion skills such as walking, running, and acrobatics with
deep reinforcement learning (RL) [40, 41, 54, 58, 59]. RL-
based motion imitation has also been used to learn user-
controllable policies for character animation [8, 56, 84].
For 3D human pose estimation, recent work has adopted
physics-based trajectory optimization [92, 65, 73, 72] and
motion imitation [87, 88, 28, 91, 86, 47, 48] to model human
dynamics. Unlike previous work, we explore the synergy
between physics simulation and diffusion models, and show
that applying physics and diffusion iteratively can generate
more realistic and physically-plausible motions.
3. Method
Given some conditional information c such as text or
an action label, we aim to generate a physically-plausible
human motion x1:H = {xh}H
h=1 of length H. Each pose
xh ∈RJ×D in the generated motion is represented by the
D-dimensional features of J joints, which can be either the
joint positions or angles. We propose a physics-guided de-
noising diffusion model (PhysDiff) for human motion gen-
eration. Starting from a noisy motion x1:H
T
, PhysDiff mod-
els the denoising distribution q(x1:H
s
|x1:H
t
, Pπ, c) that de-
noises the motion from diffusion timestep t to s (s < t).
Iteratively applying the model denoises the motion into a
clean motion x1:H
0
, which becomes the final output x1:H. A
critical component in the model is a physics-based motion
projection module Pπ that enforces physical constraints. It
leverages a motion imitation policy π to mimic the denoised
motion of a diffusion step in a physics simulator and uses
the simulated motion to further guide the diffusion process.
An overview of our PhysDiff model is provided in Fig. 3. In
the following, we first introduce the physics-guided motion
diffusion process in Sec. 3.1. We then describe the details
of the physics-based motion projection Pπ in Sec. 3.2.
3.1. Physics-Guided Motion Diffusion
Motion Diffusion. To simplify notations, here we some-
times omit the explicit dependence over the condition c.

Physics
Simulator
Denoiser
Physics-Guided Motion Diffusion
Noise
Generated Motion
     Policy Steps
Physics-Based
Motion Projection
Imitaiton
Policy
Physics-Guided
Motion Diffusion
Alg. 2
L9-11
(Scheduler)
Apply
Physics?
Condition    : “A person slowly walks forward.”
N
Y
Physics-Guided
Motion Diffusion
Figure 3. Overview of PhysDiff. Each physics-guided diffusion step denoises a motion from timestep t to s, where physics-based motion
projection is used to enforce physical constraints. The projection is achieved using a motion imitation policy to control a character in a
physics simulator. A scheduler controls when the physics-based projection is applied. The denoiser can be any motion-denoising network.
Note that we can always train diffusion models with some
condition c; even for the unconditional case, we can condi-
tion the model on a universal null token ∅[26].
Let p0(x) denote the data distribution, and define a
series of time-dependent distributions pt(xt) by injecting
i.i.d. Gaussian noise to samples from p0, i.e., pt(xt|x) =
N(x, σ2
t I), where σt defines a series of noise levels that is
increasing over time such that σ0 = 0 and σT for the largest
possible T is much bigger than the data’s standard devia-
tion. Generally, diffusion models draw samples by solving
the following stochastic differential equation (SDE) from
t = T to t = 0 [21, 30, 96]:
dx = −(βt + ˙σt)σt∇x log pt(x)dt +
p
2βtσtdωt,
(1)
where ∇x log pt(x) is the score function, ωt is the standard
Wiener process, and βt controls the amount of stochastic
noise injected in the process; when it is zero, the SDE be-
comes and ordinary differential equation (ODE). A notable
property of the score function ∇xt log pt(xt) is that it re-
covers the minimum mean squared error (MMSE) estimator
of x given xt [78, 16, 69]:
˜x := E[x|xt] = xt + σ2
t ∇xt log pt(xt),
(2)
where we can essentially treat ˜x as a “denoised” version of
xt. Since xt and σt are known during sampling, we can
obtain ∇xt log pt(xt) from ˜x, and vice versa.
Diffusion models approximate the score function with
the following denoising autoencoder objective [82]:
Ex∼p0(x),t∼p(t),ϵ∼p(ϵ)[λ(t)∥x −D(x + σtϵ, t, c)∥2
2] (3)
where D is the denoiser that depends on the noisy data, the
time t and the condition c, ϵ ∼N(0, I), p(t) is a distri-
bution from which time is sampled, and λ(t) is the loss
weighting factor. The optimal solution to D would be one
that recovers the MMSE estimator ˜x according to Eq. (2).
For a detailed characterization of the training procedure, we
refer the reader to Karras et al. [30].
After the denoising diffusion model has been trained,
one can apply it to solve the SDE / ODE in Eq. (1). A par-
ticular approach, DDIM [76], performs a one-step update
from time t to time s (s < t) given the sample xt, which
is described in Algorithm 1. Intuitively, the sample xs at
time s is generated from a Gaussian distribution; its mean
is a linear interpolation between xt and the denoised result
˜x, and its variance depends on a hyperparameter η ∈[0, 1].
Specifically, η = 0 corresponds to denoising diffusion prob-
abilistic models (DDPM [25]) and η = 1 corresponds to
denoising diffusion implicit models (DDIM [76]). We find
η = 0 produces better performance for our model. Since the
above sampling procedure is general, it can also be applied
to human motion data, i.e., x1:H. To incorporate the condi-
tion c during sampling, one can employ classifier-based or
classifier-free guidance [13, 26].
Algorithm 1 DDIM sampling algorithm
1: Input: Denoiser D, sample xt at time t, target time s,
condition c, hyperparameter η ∈[0, 1].
2: Compute the denoised result ˜x := D(xt, t, c).
3: Obtain variance vs as a scalar that depends on η.
4: Obtain mean µs as a linear combination of ˜x and xt:
µs := ˜x +
p
σ2s −vs
σt
(xt −˜x)
5: Draw sample xs ∼N(µs, vsI).
Applying Physical Constraints. Existing diffusion mod-
els for human motions are not necessarily trained on data
that complies with physical constraints, and even if they are,
there is no guarantee that the produced motion samples are
still physically realizable, due to the approximation errors in
the denoiser networks and the stochastic nature of the sam-
pling process. While one may attempt to directly correct the
final motion sample to be physically-plausible, the physical
errors in the motion might be so large that even after such
a correction, the motion is still not ideal (see Fig. 2 for a
concrete example and Sec. 4.4 for comparison).
To address this issue, we exploit the fact that diffusion
models produce intermediate estimates of the desired out-
come, i.e., the denoised motion ˜x1:H of each diffusion step.
In particular, we may apply physical constraints not only to
the final step of the diffusion process, but to the intermedi-
ate steps as well. Concretely, we propose a physics-based

motion projection Pπ : RH×J×D →RH×J×D as a module
that maps the original motion ˜x1:H to a physically-plausible
one, denoted as ˆx1:H. We incorporate the proposed physics-
based projection Pπ into the denoising diffusion sampling
procedure, where the one-step update from time t to time
s is described in Algorithm 2. The process differs from
the DDIM sampler mostly in terms of performing the ad-
ditional physics-based projection; as we will show in the
experiments, this is a simple yet effective approach to en-
forcing physical constraints. Notably, our model, PhysDiff,
is agnostic to the specific instantiation of the denoiser D,
which is often implemented with various network archi-
tectures. The physics-based projection in PhysDiff is also
only applied during diffusion sampling (inference), which
makes PhysDiff generally compatible with different pre-
trained motion diffusion models. In other words, PhysDiff
can be used to improve the physical plausibility of existing
diffusion models without retraining.
Algorithm 2 PhysDiff sampling algorithm for motion.
1: Input: Denoiser D, sample x1:H
t
at time t, condition c,
target time s, physics-based projection Pπ, η ∈[0, 1].
2: Compute the denoised motion ˜x1:H := D(x1:H
t
, t, c).
3: if projection is performed at time t then
4:
ˆx1:H := Pπ(˜x1:H)
# Physics-Based Projection
5: else
6:
ˆx1:H := ˜x1:H
7: end if
8: # The remaining part is similar to DDIM
9: Obtain variance vs as a scalar that depends on η.
10: Obtain mean µs:
µs := ˆx1:H +
p
σ2s −vs
σt
(x1:H
t
−ˆx1:H)
11: Draw sample x1:H
s
∼N(µs, vsI).
Scheduling Physics-based Projection. Due to the use of
physics simulation, the projection Pπ is rather expensive,
and it is infeasible to perform the projection at every diffu-
sion timestep. Therefore, if we have a limited number of
physics-based projection steps to be performed, we need to
prioritize certain timesteps over others. Here, we argue that
we should not perform physics projection when the diffu-
sion noise level is high. This is because the denoiser D by
design gives us ˜x1:H = E[x1:H|x1:H
t
], i.e., the mean mo-
tion of x1:H given the current noisy motion x1:H
t
, and it is
close to the mean of the training data for high noise levels
when the condition x1:H
t
contains little information. Empir-
ically, the mean motion often has little body movement due
to mode averaging while still having some root translations
and rotations, which is clearly physically-implausible. Cor-
recting such a physically-incorrect motion with the physics-
based projection would push the motion further away from
the data distribution and hinder the diffusion process. In
Sec. 4.3, we perform a systematic study that validates this
hypothesis and reveals a favorable scheduling strategy that
balances sample quality and efficiency.
3.2. Physics-Based Motion Projection
An essential component in the physics-guided diffusion
process is the physics-based motion projection Pπ. It is
tasked with projecting the denoised motion ˜x1:H of a dif-
fusion step, which disregards the laws of physics, into a
physically-plausible motion ˆx1:H = Pπ(˜x1:H). The pro-
jection is achieved by learning a motion imitation policy π
that controls a simulated character to mimic the denoised
motion ˜x1:H in a physics simulator. The resulting motion
ˆx1:H from the simulator is considered physically-plausible
since it obeys the laws of physics.
Motion Imitation Formulation. The task of human mo-
tion imitation [58, 90] can be formulated as a Markov
decision process (MDP). The MDP is defined by a tuple
M = (S, A, T , R, γ) of states, actions, transition dynam-
ics, a reward function, and a discount factor. A character
agent acts in a physics simulator according to a motion im-
itation policy π(ah|sh), which models the distribution of
choosing an action ah ∈A given the current state sh ∈S.
The state sh consists of the character’s physical state (e.g.,
joint angles, velocities, positions) as well as the next pose
˜xh+1 from the input motion. Including ˜xh+1 in the state
informs the policy π to choose an action ah that can mimic
˜xh+1 in the simulator. Starting from an initial state s1, the
agent iteratively samples an action ah from the policy π
and the simulator with transition dynamics T (sh+1|sh, ah)
generates the next state sh+1, from which we can extract the
simulated pose ˆxh+1. By running the policy for H steps, we
can obtain the physically-simulated motion ˆx1:H.
Training. During training, a reward rh is also assigned
to the character based on how well the simulated motion
ˆx1:H aligns with the ground-truth motion ¯x1:H. Note that
the motion imitation policy π is trained on large motion
capture datasets where high-quality ground-truth motion is
available. We use reinforcement learning (RL) to learn the
policy π, where the objective is to maximize the expected
discounted return J(π) = Eπ
P
h γhrh
which translates
to mimicking the ground-truth motion as closely as possi-
ble. We adopt a standard RL algorithm (PPO [71]) to solve
for the optimal policy. In the following, we will elaborate
on the design of rewards, states, actions, and the policy.
Rewards. The reward function is designed to encourage
the simulated motion ˆx1:H to match the ground truth ¯x1:H.
Here, we use
¯·
to denote ground-truth quantities. The

reward rh at each timestep consists of four sub-rewards:
rh = wprh
p + wvrh
v + wjrh
j + wqrh
q ,
(4)
rh
p = exp
h
−αp
PJ
j=1 ∥oh
j ⊖¯oh
j ∥2i
,
(5)
rh
v = exp

−αv∥vh −¯vh∥2
,
(6)
rh
j = exp
h
−αj
PJ
j=1 ∥ph
j −¯ph
j ∥2i
,
(7)
rh
q = exp
h
−αq
PJ
j=1 ∥qh
j −¯qh
j ∥2i
.
(8)
where wp, wv, wj, wq, αp, αv, αj, αq are weighting factors.
The pose reward rh
p measures the difference between the
local joint rotations oh
j and the ground truth ¯oh
j , where
⊖denotes the relative rotation between two rotations, and
∥· ∥computes the rotation angle. The velocity reward rh
v
measures the mismatch between joint velocities vh and the
ground truth ¯vh, which are computed via finite difference.
The joint position reward rh
j encourages the 3D world joint
positions ph
j to match the ground truth ¯ph
j .
Finally, the
joint rotation reward rh
q measures the difference between
the global joint rotations qh
j and the ground truth ¯qh
j .
States. The agent state sh consists of the character’s cur-
rent physical state, the input motion’s next pose ˜xh+1, and
a character attribute vector ψ. The character’s physical state
includes its joint angles, joint velocities, and rigid bodies’
positions, rotations, and linear and angular velocities. For
the input pose ˜xh+1, the state sh contains the difference of
˜xh+1 w.r.t. the agent in joint angles as well as rigid body
positions and rotations. Using the difference informs the
policy about the pose residual it needs to compensate for.
All the features are computed in the character’s heading co-
ordinate to ensure rotation and translation invariance. Since
our character is based on the SMPL body model [43], the at-
tribute ψ includes the gender and SMPL shape parameters
to allow the policy to control different characters.
Actions.
We use the target joint angles of proportional
derivative (PD) controllers as the action representation,
which enables robust motion imitation as observed in prior
work [60, 90]. We also add residual forces [90] in the action
space to stabilize the character and compensate for missing
contact forces required to imitate motions such as sitting.
Policy. We use a parametrized Gaussian policy π(ah|sh) =
N(µθ(sh), Σ) where the mean action µθ is output by a
simple multi-layer perceptron (MLP) network with param-
eters θ, and Σ is a fixed diagonal covariance matrix.
4. Experiments
We perform experiments on two standard human mo-
tion generation tasks: text-to-motion and action-to-motion
generation. In particular, our experiments are designed to
answer the following questions: (1) Can PhysDiff achieve
Method
FID ↓
R-Precision ↑Penetrate ↓Float ↓Skate ↓Phys-Err ↓
J2LP [1]
11.020
0.486
-
-
-
-
Text2Gesture [9]
7.664
0.345
-
-
-
-
T2M [22]
1.067
0.740
11.897
7.779
2.908
22.584
MDM [79]
0.544
0.611
11.291
18.876
1.406
31.572
MotionDiffuse (MD) [94]
0.630
0.782
20.278
6.450
3.925
30.652
PhysDiff w/ MD (Ours)
0.551
0.780
0.898
1.368
0.423
2.690
PhysDiff w/ MDM (Ours)
0.433
0.631
0.998
2.601
0.512
4.111
Table 1. Text-to-motion results on HumanML3D [22].
Method
FID ↓Accuracy ↑Penetrate ↓Float ↓Skate ↓Phys-Err ↓
Action2Motion [23]
0.338
0.917
-
-
-
-
ACTOR [61]
0.120
0.955
8.939
14.479
2.277
25.695
INR [10]
0.088
0.973
7.055
13.212
0.928
22.096
MDM [79]
0.100
0.990
5.600
6.703
1.075
13.377
PhysDiff w/ MDM (Ours) 0.096
0.983
0.689
2.002
0.159
2.850
Table 2. Action-to-motion results on HumanAct12 [23].
Method
FID ↓Accuracy ↑Penetrate ↓Float ↓Skate ↓Phys-Err ↓
ACTOR [61]
23.43
0.911
8.441
9.737
1.073
19.251
INR [10]
15.00
0.941
5.999
4.633
0.741
11.373
MDM [79]
12.81
0.950
13.077
13.912
1.383
28.371
PhysDiff w/ MDM (Ours) 13.27
0.956
0.874
0.201
0.389
1.463
Table 3. Action-to-motion results on UESTC [29].
SOTA motion quality and physical plausibility? (2) Can
PhysDiff be applied to different kinematic motion diffusion
models to improve their motion quality and physical plausi-
bility? (3) How do different schedules of the physics-based
projection impact motion generation performance? (4) Can
PhysDiff outperform physics-based post-processing?
Evaluation Metrics.
For text-to-motion generation, we
first use two standard metrics suggested by Guo et al. [22]:
FID measures the distance between the generated and
ground-truth motion distributions; R-Precision assesses the
relevancy of the generated motions to the input text. For
action-to-motion generation, we replace R-Precision with
an Accuracy metric, which measures the accuracy of a
trained action classifier over the generated motion. Addi-
tionally, we also use four physics-based metrics to evalu-
ate the physical plausibility of generated motions: Pene-
trate measures ground penetration; Float measures floating;
Skate measures foot sliding; Phys-Err is an overall physical
error metric that sums the three metrics (all in mm) together.
Please refer to Appendix A for details.
Implementation Details.
Our model uses 50 diffusion
steps with classifier-free guidance [26]. We test PhysDiff
with two SOTA motion diffusion models, MDM [79] and
MotionDiffuse [94], as the denoiser D. By default, MDM
is the denoiser of PhysDiff for qualitative results. We adopt
IsaacGym [50] as the physics simulator for motion imita-
tion. More details are provided in Appendices B and C.
4.1. Text-to-Motion Generation
Data. We use the HumanML3D [22] dataset, which is a
textually annotated subset of two large-scale motion capture
datasets, AMASS [49] and HumanAct12 [23]. It contains
14,616 motions annotated with 44,970 textual descriptions.

PhysDiff (Ours)
MDM
Penetration
Penetration
“Person jogs, stops to bend over, then continues jogging.”
HumanML3D
Floating
“A person runs forward quickly, stops on their left foot, then runs in a different direction.”
HumanML3D
Lift Dumbbell
HumanAct12
Penetration
Floating
Floating
Figure 4. Visual comparison of PhysDiff against the SOTA, MDM [79], on HumanML3D, HumanAct12, and UESTC. PhysDiff reduces
physical artifacts such as floating and penetration significantly. Please refer to the project page for more qualitative comparison.
Results. In Table 1, we compare our method to the SOTA
methods: JL2P [1], Text2Gesture [9], T2M [22], Motion-
Diffuse [94], and MDM [79]. Due to the plug-and-play na-
ture of our method, we design two variants of PhysDiff us-
ing MotionDiffuse (MD) and MDM. PhysDiff with MDM
achieves SOTA FID and also reduces Phys-Err by more
than 86% compared to MDM. Similarly, PhysDiff with MD
achieves SOTA in physics-based metrics while maintain-
ing high R-Precision and improving FID significantly. We
also provide qualitative comparison in Fig. 4, where we can
clearly see that PhysDiff substantially reduces physical ar-
tifacts such as penetration and floating. Please also refer to
the project page for more qualitative results.
4.2. Action-to-Motion Generation
Data.
We evaluate on two datasets: HumanAct12 [23],
which contains around 1200 motion clips for 12 action cat-
egories; UESTC [29], which consists of 40 action classes,
40 subjects, and 25k samples. For both datasets, we use the
sequences provided by Petrovich et al. [61].
Results.
Tables 2 and 3 summarize the results on Hu-
manAct12 and UESTC, respectively, where we compare
PhysDiff against the SOTA methods: MDM [79], INR [10],
Action2Motion [23], and ACTOR [61]. The results show
that our method achieves competitive FID on both datasets
while drastically improving Phys-Err (by 78% on Human-
Act12 and 94% on UESTC). Please refer to Fig. 4 and the
project page for qualitative comparison, where we show
that PhysDiff improves the physical plausibility of gener-
ated motions significantly.
4.3. Schedule of Physics-Based Projection
We perform extensive experiments to analyze the sched-
ule of the physics-based projection, i.e., at which timesteps
we perform the projection in the diffusion process.
Number of Projection Steps.
Since the physics-based
projection is relatively expensive to compute, we first in-
vestigate whether we can reduce the number of projection
steps without sacrificing performance. To this end, we vary
the number of projection steps performed during diffusion
from 50 to 0, where the projection steps are gradually re-
moved from earlier timesteps and applied consecutively.

R-Precision
Number of Physics-Based Projections
Phys-Err
Number of Physics-Based Projections
FID
Number of Physics-Based Projections
Figure 5. Effect of varying the number of physics-based projection
steps for text-to-motion generation on HumanML3D [22].
Schedule
FID ↓R-Precision ↑Penetrate ↓Float ↓Skate ↓Phys-Err ↓
Uniform 4
0.473
0.630
0.979
3.479
0.463
4.921
Start 3, End 1
0.510
0.623
0.918
3.173
0.459
4.550
Start 2, End 2
0.503
0.623
0.918
2.723
0.492
4.133
End 4, Space 3
0.469
0.630
0.990
3.226
0.473
4.689
End 4, Space 2
0.469
0.630
0.990
3.004
0.476
4.470
End 4, Space 1
0.433
0.631
0.998
2.601
0.512
4.111
Table 4. Projection schedule comparison on HumanML3D [22].
We plot the curves of FID, R-Precision, and Phys-Err in
Fig. 5. As can be seen, Phys-Err keeps decreasing with
more physics-based projection steps, which indicates more
projection steps always help improve the physical plausibil-
ity of PhysDiff. Interestingly, both the FID and R-Precision
first improve (FID decreases and R-Precision increases) and
then deteriorate when increasing the number of projection
steps. This suggests that there is a trade-off between phys-
ical plausibility and motion quality when more projection
steps are performed at the early diffusion steps. We hypoth-
esize that motions generated at the early diffusion steps are
denoised to the mean motion of the dataset (with little body
movement) and are often not physically-plausible. As a re-
sult, performing the physics-based projection at these early
steps can push the generated motion away from the data dis-
tribution, thus hindering the diffusion process.
Placement of Projections Steps. Fig. 5 indicates that four
physics-based projection steps yield a good trade-off be-
tween physical plausibility and motion quality. Next, we
investigate the best placement of these projection steps in
the diffusion process. We compare three groups of sched-
ules: (1) Uniform N, which spreads the N projection steps
evenly across the diffusion timesteps i.e., for 50 diffusion
steps and N = 4, the projection steps are performed at
t ∈{0, 15, 30, 45}; (2) Start M, End N, which places M
consecutive projection steps at the beginning of the diffu-
sion process and N projection steps at the end; (3) End
N, Space S, which places N projections steps with time
spacing S at the end of the diffusion process (e.g., for
N = 4, S = 3, the projections steps are performed at
t ∈{0, 3, 6, 9}). We summarize the results in Table 4. We
can see that the schedule Start M, End N has inferior FID
and R-Precision since more physics-based projection steps
are performed at early diffusion steps, which is consistent
with our findings in Fig. 5. The schedule Uniform N works
better in terms of FID and R-Precision but has worse Phys-
R-Precision
Number of Physics-Based Steps
Phys-Err
Number of Physics-Based Steps
FID
Number of Physics-Based Steps
Figure 6. Comparison with post-processing, i.e., applying physics-
based projections after the diffusion process.
Err. This is likely because too many non-physics-based dif-
fusion steps between the physics-based projections undo the
effect of the projection and reintroduce physical errors. This
is also consistent with End 4, Space 3 being worse than
End 4, Space 1 since the former has more diffusion steps
between the physics-based projections. Hence, the results
suggest that it is better to schedule the physics-based pro-
jection steps consecutively toward the end. This guides us
to use End 4, Space 1 for baseline comparison.
Inference Time.
Due to the use of physics simulation,
PhysDiff is 2.5x slower than MDM (51.6s vs. 19.6s) to gen-
erate a single motion, where both methods use 1000 diffu-
sion steps. The gap closes when increasing the batch size to
256 where PhysDiff is only 1.7x slower (280.3s vs. 471.3s),
as the physics simulator benefits more from parallelization.
4.4. Comparing against Post-Processing
To demonstrate the synergy between physics and diffu-
sion, we compare PhysDiff against a post-processing base-
line that applies one or more physics-based projection steps
to the final kinematic motion from diffusion. As shown in
Fig. 6, multiple post-processing steps cannot enhance mo-
tion quality or physical plausibility; instead, they deterio-
rate them. This is because the final kinematic motion may
be too physically implausible for the physics to imitate, e.g.,
the human may lose balance due to wrong gaits. Repeatedly
imitating these implausible motions could amplify the prob-
lem and lead to unstable simulation. PhysDiff overcomes
this issue by iteratively applying diffusion and physics to
recover from bad simulation states and move closer to the
data distribution.
5. Conclusion and Future Work
In this paper, we proposed a novel physics-guided mo-
tion diffusion model (PhysDiff) which instills the laws of
physics into the diffusion process to generate physically-
plausible human motions.
To achieve this, we proposed
a physics-based motion projection module that uses mo-
tion imitation in physics simulation to enforce physical con-
straints. Our approach is agnostic to the denoising network
and can be used to improve SOTA motion diffusion models
without retraining. Experiments on large-scale motion data
demonstrate that PhysDiff achieves SOTA motion quality

and substantially improves physical plausibility.
Due to physics simulation, the inference speed of Phys-
Diff can be two-to-three times slower than SOTA models.
Future work could speed up the model with a faster physics
simulator or improve the physics-based projection to reduce
the number of required projection steps.
References
[1] Chaitanya Ahuja and Louis-Philippe Morency.
Lan-
guage2pose: Natural language grounded pose forecasting.
In 2019 International Conference on 3D Vision (3DV), pages
719–728. IEEE, 2019. 3, 6, 7
[2] Emre Aksan, Manuel Kaufmann, and Otmar Hilliges. Struc-
tured prediction helps 3d human motion modelling. In Pro-
ceedings of the IEEE International Conference on Computer
Vision, pages 7144–7153, 2019. 3
[3] Sadegh Aliakbarian, Fatemeh Sadat Saleh, Mathieu Salz-
mann, Lars Petersson, and Stephen Gould. A stochastic con-
ditioning scheme for diverse human motion prediction. In
Proceedings of the IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition, pages 5223–5232, 2020. 3
[4] Jyoti Aneja, Alex Schwing, Jan Kautz, and Arash Vahdat. A
contrastive learning approach for training variational autoen-
coder priors.
Advances in Neural Information Processing
Systems, 34:480–493, 2021. 2
[5] Fan Bao, Chongxuan Li, Jiacheng Sun, Jun Zhu, and Bo
Zhang.
Estimating the optimal covariance with imperfect
mean in diffusion probabilistic models.
In International
Conference on Machine Learning, 2022. 3
[6] Fan Bao, Chongxuan Li, Jun Zhu, and Bo Zhang. Analytic-
DPM: An analytic estimate of the optimal reverse variance in
diffusion probabilistic models. In International Conference
on Learning Representations, 2022. 3
[7] Emad Barsoum, John Kender, and Zicheng Liu.
Hp-gan:
Probabilistic 3d human motion prediction via gan. In Pro-
ceedings of the IEEE Conference on Computer Vision and
Pattern Recognition Workshops, pages 1418–1427, 2018. 3
[8] Kevin Bergamin,
Simon Clavet,
Daniel Holden,
and
James Richard Forbes.
Drecon:
data-driven responsive
control of physics-based characters. ACM Transactions on
Graphics (TOG), 38(6):1–11, 2019. 3
[9] Uttaran Bhattacharya,
Nicholas Rewkowski,
Abhishek
Banerjee, Pooja Guhan, Aniket Bera, and Dinesh Manocha.
Text2gestures: A transformer-based network for generating
emotive body gestures for virtual agents. In 2021 IEEE Vir-
tual Reality and 3D User Interfaces (VR), pages 1–10. IEEE,
2021. 3, 6, 7
[10] Pablo Cervantes, Yusuke Sekikawa, Ikuro Sato, and Koichi
Shinoda. Implicit neural representations for variable length
human motion generation. In Proceedings of the European
Conference on Computer Vision (ECCV), pages 356–372.
Springer, 2022. 3, 6, 7
[11] Jooyoung Choi, Sungwon Kim, Yonghyun Jeong, Youngjune
Gwon, and Sungroh Yoon.
ILVR: Conditioning method
for denoising diffusion probabilistic models. arXiv preprint
arXiv:2108.02938, August 2021. 3
[12] Hyungjin Chung, Byeongsu Sim, and Jong Chul Ye. Come-
Closer-Diffuse-Faster:
Accelerating conditional diffusion
models for inverse problems through stochastic contraction.
arXiv preprint arXiv:2112.05146, December 2021. 3
[13] Prafulla Dhariwal and Alexander Quinn Nichol. Diffusion
models beat GANs on image synthesis. In Advances in Neu-
ral Information Processing Systems, 2021. 1, 3, 4
[14] Tim Dockhorn, Arash Vahdat, and Karsten Kreis. GENIE:
Higher-order denoising diffusion solvers.
In Advances in
Neural Information Processing Systems, 2022. 3
[15] Tim Dockhorn, Arash Vahdat, and Karsten Kreis.
Score-
based generative modeling with critically-damped Langevin
diffusion. In International Conference on Learning Repre-
sentations, 2022. 3
[16] Bradley Efron. Tweedie’s formula and selection bias. Jour-
nal of the American Statistical Association, 106(496):1602–
1614, 2011. 4
[17] Katerina Fragkiadaki, Sergey Levine, Panna Felsen, and Ji-
tendra Malik. Recurrent network models for human dynam-
ics. In Proceedings of the IEEE International Conference on
Computer Vision, pages 4346–4354, 2015. 3
[18] Partha Ghosh, Jie Song, Emre Aksan, and Otmar Hilliges.
Learning human motion models for long-term predictions.
In 2017 International Conference on 3D Vision (3DV), pages
458–466. IEEE, 2017. 3
[19] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing
Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and
Yoshua Bengio. Generative adversarial nets. In Advances in
Neural Information Processing Systems, pages 2672–2680,
2014. 2
[20] Anand Gopalakrishnan, Ankur Mali, Dan Kifer, Lee Giles,
and Alexander G Ororbia. A neural temporal model for hu-
man motion prediction. In Proceedings of the IEEE Con-
ference on Computer Vision and Pattern Recognition, pages
12116–12125, 2019. 3
[21] Ulf Grenander and Michael I Miller.
Representations of
knowledge in complex systems. Journal of the Royal Sta-
tistical Society: Series B (Methodological), 56(4):549–581,
1994. 4
[22] Chuan Guo, Shihao Zou, Xinxin Zuo, Sen Wang, Wei Ji,
Xingyu Li, and Li Cheng. Generating diverse and natural 3d
human motions from text. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition,
pages 5152–5161, 2022. 2, 3, 6, 7, 8, 13
[23] Chuan Guo, Xinxin Zuo, Sen Wang, Shihao Zou, Qingyao
Sun, Annan Deng, Minglun Gong, and Li Cheng.
Ac-
tion2motion: Conditioned generation of 3d human motions.
In Proceedings of the 28th ACM International Conference on
Multimedia, pages 2021–2029, 2020. 2, 3, 6, 7
[24] Chengan He, Jun Saito, James Zachary, Holly Rushmeier,
and Yi Zhou. Nemf: Neural motion fields for kinematic ani-
mation. arXiv preprint arXiv:2206.03287, 2022. 3
[25] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising dif-
fusion probabilistic models. In Advances in Neural Informa-
tion Processing Systems, 2020. 1, 3, 4
[26] Jonathan Ho and Tim Salimans.
Classifier-free diffusion
guidance. In NeurIPS 2021 Workshop on Deep Generative
Models and Downstream Applications, 2021. 3, 4, 6

[27] Ludovic Hoyet, Rachel McDonnell, and Carol O’Sullivan.
Push it real: Perceiving causality in virtual interactions. ACM
Transactions on Graphics (TOG), 31(4):1–9, 2012. 2
[28] Mariko Isogawa, Ye Yuan, Matthew O’Toole, and Kris M Ki-
tani. Optical non-line-of-sight physics-based 3d human pose
estimation.
In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition, pages 7013–
7022, 2020. 3
[29] Yanli Ji, Feixiang Xu, Yang Yang, Fumin Shen, Heng Tao
Shen, and Wei-Shi Zheng. A large-scale rgb-d database for
arbitrary-view human action recognition.
In Proceedings
of the 26th ACM international Conference on Multimedia,
pages 1510–1518, 2018. 2, 6, 7
[30] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine.
Elucidating the design space of diffusion-based generative
models. In Advances in Neural Information Processing Sys-
tems, 2022. 3, 4
[31] Bahjat Kawar, Michael Elad, Stefano Ermon, and Jiaming
Song. Denoising diffusion restoration models. In Advances
in Neural Information Processing Systems, 2022. 3
[32] Bahjat Kawar, Gregory Vaksman, and Michael Elad. SNIPS:
Solving noisy inverse problems stochastically. arXiv preprint
arXiv:2105.14951, May 2021. 3
[33] Diederik Kingma, Tim Salimans, Ben Poole, and Jonathan
Ho. Variational diffusion models. Advances in Neural Infor-
mation Processing Systems, 34:21696–21707, 2021. 1
[34] Diederik P Kingma and Jimmy Ba. Adam: A method for
stochastic optimization.
arXiv preprint arXiv:1412.6980,
2014. 13
[35] Diederik P Kingma and Max Welling. Auto-encoding varia-
tional bayes. arXiv preprint arXiv:1312.6114, 2013. 2
[36] Zhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, and
Bryan Catanzaro. DiffWave: A versatile diffusion model for
audio synthesis. In International Conference on Learning
Representations, 2021. 3
[37] Buyu Li, Yongchi Zhao, Shi Zhelun, and Lu Sheng. Dance-
former: Music conditioned 3d dance generation with para-
metric motion transformer. In Proceedings of the AAAI Con-
ference on Artificial Intelligence, volume 36, pages 1272–
1279, 2022. 3
[38] Ruilong Li, Shan Yang, David A Ross, and Angjoo
Kanazawa. Ai choreographer: Music conditioned 3d dance
generation with aist++. In Proceedings of the IEEE/CVF In-
ternational Conference on Computer Vision, pages 13401–
13412, 2021. 3
[39] Zimo Li, Yi Zhou, Shuangjiu Xiao, Chong He, Zeng Huang,
and Hao Li.
Auto-conditioned recurrent networks for ex-
tended complex human motion synthesis.
arXiv preprint
arXiv:1707.05363, 2017. 3
[40] Libin Liu and Jessica Hodgins. Learning to schedule con-
trol fragments for physics-based characters using deep q-
learning. ACM Transactions on Graphics (TOG), 36(3):29,
2017. 3
[41] Libin Liu and Jessica Hodgins. Learning basketball dribbling
skills using trajectory optimization and deep reinforcement
learning. ACM Transactions on Graphics (TOG), 37(4):1–
14, 2018. 3
[42] Luping Liu, Yi Ren, Zhijie Lin, and Zhou Zhao.
Pseudo
numerical methods for diffusion models on manifolds. In In-
ternational Conference on Learning Representations, 2022.
3
[43] Matthew Loper, Naureen Mahmood, Javier Romero, Gerard
Pons-Moll, and Michael J Black. Smpl: A skinned multi-
person linear model. ACM transactions on graphics (TOG),
34(6):1–16, 2015. 6, 13
[44] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan
Li, and Jun Zhu. DPM-Solver: A fast ODE solver for dif-
fusion probabilistic model sampling in around 10 steps. In
Advances in Neural Information Processing Systems, 2022.
3
[45] Andreas Lugmayr, Martin Danelljan, Andres Romero, Fisher
Yu, Radu Timofte, and Luc Van Gool. RePaint: Inpainting
using denoising diffusion probabilistic models. In Proceed-
ings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition, 2022. 3
[46] Shitong Luo and Wei Hu. Diffusion probabilistic models for
3D point cloud generation. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition,
2021. 3
[47] Zhengyi Luo, Ryo Hachiuma, Ye Yuan, and Kris Kitani.
Dynamics-regulated kinematic policy for egocentric pose es-
timation. In Advances in Neural Information Processing Sys-
tems, 2021. 3
[48] Zhengyi Luo, Shun Iwase, Ye Yuan, and Kris Kitani. Em-
bodied scene-aware human pose estimation. In Advances in
Neural Information Processing Systems, 2022. 3
[49] Naureen Mahmood, Nima Ghorbani, Nikolaus F Troje, Ger-
ard Pons-Moll, and Michael J Black.
Amass:
Archive
of motion capture as surface shapes.
In Proceedings of
the IEEE/CVF international conference on computer vision,
pages 5442–5451, 2019. 6, 13
[50] Viktor Makoviychuk, Lukasz Wawrzyniak, Yunrong Guo,
Michelle Lu, Kier Storey, Miles Macklin, David Hoeller,
Nikita Rudin, Arthur Allshire, Ankur Handa, et al.
Isaac
gym: High performance gpu based physics simulation for
robot learning. In Thirty-fifth Conference on Neural Infor-
mation Processing Systems Datasets and Benchmarks Track,
2021. 6, 13
[51] Wei Mao, Miaomiao Liu, Mathieu Salzmann, and Hongdong
Li. Learning trajectory dependencies for human motion pre-
diction. In Proceedings of the IEEE International Confer-
ence on Computer Vision, pages 9489–9497, 2019. 3
[52] Julieta Martinez, Michael J Black, and Javier Romero. On
human motion prediction using recurrent neural networks.
In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pages 2891–2900, 2017. 3
[53] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jia-
jun Wu, Jun-Yan Zhu, and Stefano Ermon. SDEdit: Guided
image synthesis and editing with stochastic differential equa-
tions. In International Conference on Learning Representa-
tions, 2022. 3
[54] Josh Merel, Yuval Tassa, Sriram Srinivasan, Jay Lemmon,
Ziyu Wang, Greg Wayne, and Nicolas Heess. Learning hu-
man behaviors from motion capture by adversarial imitation.
arXiv preprint arXiv:1707.02201, 2017. 3

[55] Weili Nie, Brandon Guo, Yujia Huang, Chaowei Xiao, Arash
Vahdat, and Anima Anandkumar. Diffusion models for ad-
versarial purification. In International Conference on Ma-
chine Learning, 2022. 3
[56] Soohwan Park, Hoseok Ryu, Seyoung Lee, Sunmin Lee, and
Jehee Lee. Learning predict-and-simulate policies from un-
organized human motion data. ACM Transactions on Graph-
ics (TOG), 38(6):1–11, 2019. 3
[57] Dario Pavllo, David Grangier, and Michael Auli. Quater-
net: A quaternion-based recurrent model for human motion.
arXiv preprint arXiv:1805.06485, 2018. 3
[58] Xue Bin Peng, Pieter Abbeel, Sergey Levine, and Michiel
van de Panne. Deepmimic: Example-guided deep reinforce-
ment learning of physics-based character skills. ACM Trans-
actions on Graphics (TOG), 37(4):1–14, 2018. 3, 5
[59] Xue Bin Peng, Angjoo Kanazawa, Jitendra Malik, Pieter
Abbeel, and Sergey Levine. Sfv: Reinforcement learning of
physical skills from videos. ACM Transactions on Graphics
(TOG), 37(6):1–14, 2018. 3
[60] Xue Bin Peng and Michiel van de Panne. Learning locomo-
tion skills using deeprl: Does the choice of action space mat-
ter? In Proceedings of the ACM SIGGRAPH/Eurographics
Symposium on Computer Animation, pages 1–13, 2017. 6
[61] Mathis Petrovich, Michael J Black, and G¨ul Varol. Action-
conditioned 3d human motion synthesis with transformer
vae. In Proceedings of the IEEE/CVF International Con-
ference on Computer Vision, pages 10985–10995, 2021. 3,
6, 7
[62] Mathis Petrovich, Michael J. Black, and G¨ul Varol. TEMOS:
Generating diverse human motions from textual descriptions.
In Proceedings of the European Conference on Computer Vi-
sion (ECCV), 2022. 3
[63] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,
and Mark Chen. Hierarchical text-conditional image gener-
ation with CLIP latents. arXiv preprint arXiv:2204.06125,
2022. 1, 3
[64] Paul SA Reitsma and Nancy S Pollard. Perceptual metrics
for character animation: sensitivity to errors in ballistic mo-
tion.
In ACM SIGGRAPH 2003 Papers, pages 537–542,
2003. 2
[65] Davis Rempe, Leonidas J. Guibas, Aaron Hertzmann, Bryan
Russell, Ruben Villegas, and Jimei Yang. Contact and hu-
man dynamics from monocular video. In Proceedings of the
European Conference on Computer Vision (ECCV), 2020. 3
[66] Zhiyuan Ren, Zhihong Pan, Xin Zhou, and Le Kang. Dif-
fusion motion: Generate text-guided 3d human motion by
diffusion model. arXiv preprint arXiv:2210.12315, 2022. 3
[67] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
Patrick Esser, and Bj¨orn Ommer.
High-resolution image
synthesis with latent diffusion models.
In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, 2022. 1, 3
[68] Chitwan Saharia, William Chan, Saurabh Saxena, Lala
Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed
Ghasemipour, Burcu Karagol Ayan, S. Sara Mahdavi,
Rapha Gontijo Lopes, Tim Salimans, Jonathan Ho, David J.
Fleet, and Mohammad Norouzi. Photorealistic text-to-image
diffusion models with deep language understanding. arXiv
preprint arXiv:2205.11487, 2022. 1, 3
[69] Saeed Saremi and Aapo Hyv¨arinen. Neural empirical bayes.
Journal of machine learning research: JMLR, 20(181):1–23,
2019. 4
[70] John Schulman, Philipp Moritz, Sergey Levine, Michael Jor-
dan, and Pieter Abbeel. High-dimensional continuous con-
trol using generalized advantage estimation. arXiv preprint
arXiv:1506.02438, 2015. 13
[71] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Rad-
ford, and Oleg Klimov. Proximal policy optimization algo-
rithms. arXiv preprint arXiv:1707.06347, 2017. 5, 13
[72] Soshi Shimada, Vladislav Golyanik, Weipeng Xu, Patrick
P´erez, and Christian Theobalt. Neural monocular 3d human
motion capture with physical awareness. ACM Transactions
on Graphics (ToG), 40(4):1–15, 2021. 3
[73] Soshi Shimada, Vladislav Golyanik, Weipeng Xu, and Chris-
tian Theobalt. Physcap: Physically plausible monocular 3d
motion capture in real time. ACM Transactions on Graphics
(TOG), 39(6), dec 2020. 3
[74] Abhishek Sinha, Jiaming Song, Chenlin Meng, and Stefano
Ermon. D2C: Diffusion-decoding models for few-shot con-
ditional generation. In Advances in Neural Information Pro-
cessing Systems, 2021. 2, 3
[75] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan,
and Surya Ganguli.
Deep unsupervised learning using
nonequilibrium thermodynamics. In International Confer-
ence on Machine Learning, 2015. 1, 3
[76] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denois-
ing diffusion implicit models. In International Conference
on Learning Representations, 2021. 1, 3, 4
[77] Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Ab-
hishek Kumar, Stefano Ermon, and Ben Poole. Score-based
generative modeling through stochastic differential equa-
tions. In International Conference on Learning Represen-
tations, 2021. 1, 3
[78] Charles M Stein. Estimation of the mean of a multivariate
normal distribution. The annals of Statistics, pages 1135–
1151, 1981. 4
[79] Guy Tevet, Sigal Raab, Brian Gordon, Yonatan Shafir,
Daniel Cohen-Or, and Amit H Bermano. Human motion dif-
fusion model. arXiv preprint arXiv:2209.14916, 2022. 2, 3,
6, 7, 13
[80] Arash Vahdat and Jan Kautz. Nvae: A deep hierarchical vari-
ational autoencoder. Advances in Neural Information Pro-
cessing Systems, 33:19667–19679, 2020. 2
[81] Arash Vahdat, Karsten Kreis, and Jan Kautz. Score-based
generative modeling in latent space. In Advances in Neural
Information Processing Systems, 2021. 1, 2, 3
[82] Pascal Vincent. A connection between score matching and
denoising autoencoders. Neural Computation, 23(7):1661–
1674, 2011. 3, 4
[83] Borui Wang, Ehsan Adeli, Hsu-kuang Chiu, De-An Huang,
and Juan Carlos Niebles. Imitation learning for human pose
prediction. In Proceedings of the IEEE International Con-
ference on Computer Vision, pages 7124–7133, 2019. 3

[84] Jungdam Won, Deepak Gopinath, and Jessica Hodgins. A
scalable approach to control diverse behaviors for physi-
cally simulated characters. ACM Transactions on Graphics
(TOG), 39(4):33–1, 2020. 3
[85] Xinchen Yan, Akash Rastogi, Ruben Villegas, Kalyan
Sunkavalli, Eli Shechtman, Sunil Hadap, Ersin Yumer, and
Honglak Lee. Mt-vae: Learning motion transformations to
generate multimodal human dynamics. In Proceedings of the
European Conference on Computer Vision (ECCV), pages
265–281, 2018. 3
[86] Xinyu Yi, Yuxiao Zhou, Marc Habermann, Soshi Shimada,
Vladislav Golyanik, Christian Theobalt, and Feng Xu. Phys-
ical inertial poser (pip): Physics-aware real-time human mo-
tion tracking from sparse inertial sensors. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pages 13167–13178, 2022. 3
[87] Ye Yuan and Kris Kitani. 3d ego-pose estimation via imita-
tion learning. In Proceedings of the European Conference on
Computer Vision (ECCV), pages 735–750, 2018. 3
[88] Ye Yuan and Kris Kitani. Ego-pose estimation and forecast-
ing as real-time pd control. In Proceedings of the IEEE In-
ternational Conference on Computer Vision, pages 10082–
10092, 2019. 3
[89] Ye Yuan and Kris Kitani. Dlow: Diversifying latent flows for
diverse human motion prediction. In Proceedings of the Eu-
ropean Conference on Computer Vision (ECCV), pages 346–
364. Springer, 2020. 3
[90] Ye Yuan and Kris Kitani. Residual force control for agile
human behavior imitation and extended motion synthesis. In
Advances in Neural Information Processing Systems, 2020.
3, 5, 6
[91] Ye Yuan, Shih-En Wei, Tomas Simon, Kris Kitani, and Jason
Saragih. Simpoe: Simulated character control for 3d human
pose estimation. In Proceedings of the IEEE/CVF Confer-
ence on Computer Vision and Pattern Recognition (CVPR),
2021. 3, 13
[92] Petrissa Zell, Bastian Wandt, and Bodo Rosenhahn. Joint 3d
human motion capture and physical analysis from monocu-
lar videos. In Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition Workshops, pages 17–
26, 2017. 3
[93] Xiaohui Zeng, Arash Vahdat, Francis Williams, Zan Gojcic,
Or Litany, Sanja Fidler, and Karsten Kreis. LION: Latent
point diffusion models for 3D shape generation. In Advances
in Neural Information Processing Systems, 2022. 3
[94] Mingyuan Zhang, Zhongang Cai, Liang Pan, Fangzhou
Hong, Xinying Guo, Lei Yang, and Ziwei Liu. Motiondif-
fuse: Text-driven human motion generation with diffusion
model. arXiv preprint arXiv:2208.15001, 2022. 2, 3, 6, 7,
13
[95] Qinsheng Zhang and Yongxin Chen. Fast sampling of dif-
fusion models with exponential integrator.
arXiv preprint
arXiv:2204.13902, 2022. 3
[96] Qinsheng Zhang, Molei Tao, and Yongxin Chen. gDDIM:
Generalized denoising diffusion implicit models.
arXiv
preprint arXiv:2206.05564, 2022. 3, 4
[97] Linqi Zhou, Yilun Du, and Jiajun Wu. 3D shape generation
and completion through point-voxel diffusion. In Proceed-
ings of the IEEE/CVF International Conference on Com-
puter Vision, 2021. 3
[98] Wenlin Zhuang, Congyi Wang, Jinxiang Chai, Yangang
Wang, Ming Shao, and Siyu Xia. Music2dance: Dancenet
for music-driven dance generation. ACM Transactions on
Multimedia Computing, Communications, and Applications
(TOMM), 18(2):1–21, 2022. 3

A. Details of Evaluation Metrics
We use the open source code1 of MDM [79] to compute
the motion-based metrics: FID, R-Precision, and Accuracy.
The physics-based metrics are implemented as follows. For
ground penetration (Penetrate), we compute the distance
between the ground and the lowest body mesh vertex be-
low the ground. For floating (Float), we compute the dis-
tance between the ground and the lowest body mesh vertex
above the ground. For both Penetrate and Float, we have
a tolerance of 5 mm to account for geometry approxima-
tion. For foot sliding (Skate), we find foot joints that contact
the ground in two adjacent frames and compute their aver-
age horizontal displacement within the frames. The over-
all physics error metric Phys-Err is the sum of Penetrate,
Float, and Skate.
B. Details of Motion Diffusion
As mentioned in the main paper, we tested PhysDiff
with two state-of-the-art denoiser networks, MDM [79] and
MotionDiffuse [94] and showed that PhysDiff can improve
both of them. We directly use the pretrained models in their
codebase. Please refer to their paper and code for additional
details.
For diffusion sampling, we use 50 timesteps with η = 0.
We also use classifier-free guidance with the guidance co-
efficient set to 2.5. For text-to-motion generation on Hu-
manML3D [22], the data is represented by a 263-dim vec-
tor that consists of 3D joint positions, rotations, and veloc-
ities, following Guo et al. [22]. To perform the physics-
based motion projection, we first convert the 3D joint posi-
tions into joint angles of the SMPL model [43] using inverse
kinematics and then apply physics-based motion imitation.
For action-to-motion generation, the data is represented by
joint rotations, so no inverse kinematics is required.
Policy Training. The motion imitation policy uses a three-
layer MLP with hidden dimensions (1024, 1024, 512) and
ReLU activations. The elements of the policy’s diagonal co-
variance matrix Σ are set to 0.173. We also normalize the
policy’s input state using a running estimate of the mean
and variance of the state.
We train the policy using the
AMASS [49] human motion database. Since HumanML3D
is a text-annotated version of AMASS, we use the same
training split as HumanML3D and do not use additional
data for fair comparison. We created 8192 parallel simula-
tion environments in IsaacGym to collect training samples.
Each RL episode has a horizon of 32 frames. We train the
policy for 4000 epochs where each epoch collects 262,144
samples from running all environments for an episode. The
reward weights (wp, wv, wj, wq) are set to (0.6, 0.1, 0.2,
1https://github.com/GuyTevet/
motion-diffusion-model
0.1), and the reward parameters (αp, αv, αj, αq) are set to
(60, 0.2, 100, 40). Proximal policy optimization (PPO [71])
is used to train the policy. The clipping coefficient ϵ in PPO
is set to 0.2. The discount factor γ for the Markov decision
process (MDP) is set to 0.99. We also use the generalized
advantage estimator GAE(λ) [70] to estimate the advantage
for policy gradient, and the GAE coefficient λ is 0.95. At
the end of each epoch, we update the policy by iterating
over the samples for 6 mini-epochs with a mini-batch size
of 512. The update is performed via Adam [34] with a base
learning rate of 2 × 10−5. We clip the gradient if its norm
is larger than 50.
Parameter
Value
Num. of simulation environments
8192
Episode horizon
32
Num. of epochs
4000
Num. of mini-epochs
6
Learning rate
2 × 10−5
PPO clip ϵ
0.2
Discount factor γ
0.99
GAE coefficient λ
0.95
Reward weights (wp, wv, wj, wq)
(0.6, 0.1, 0.2, 0.1)
Reward parameters (αp, αv, αj, αq)
(60, 0.2, 100, 40)
Elements of diagonal covariance Σ
0.173
Table 5. Hyperparameters for physics-based motion imitation.
C. Details of Physics-Based Motion Imitation
Physics Simulation and Character.
We use Isaac-
Gym [50] as our physics simulator for its ability to per-
form massively parallel simulation on GPUs. The simula-
tion runs at 60Hz while the policy controls the character at
30Hz. The character is automatically created from SMPL
parameters following the approach in SimPoE [91].

