

Bayesian Networks

WILEY SERIES IN PROBABILITY AND STATISTICS
Established by WALTER A. SHEWHART and SAMUEL S. WILKS
Editors: David J. Balding, Noel A. C. Cressie, Garrett M. Fitzmaurice, Iain M.
Johnstone, Geert Molenberghs, David W. Scott, Adrian F. M. Smith, Ruey S. Tsay,
Sanford Weisberg, Harvey Goldstein
Editors Emeriti: Vic Barnett, J. Stuart Hunter, Jozef L. Teugels
A complete list of the titles in this series appears at the end of this volume.

Bayesian Networks
An Introduction
Timo Koski
Institutionen f¨or Matematik,
Kungliga Tekniska H¨ogskolan, Stockholm, Sweden
John M. Noble
Matematiska Institutionen,
Link¨opings Tekniska H¨ogskola,
Link¨opings universitet, Link¨oping, Sweden
A John Wiley and Sons, Ltd., Publication

This edition ﬁrst published 2009
2009, John Wiley & Sons, Ltd
Registered ofﬁce
John Wiley & Sons Ltd, The Atrium, Southern Gate, Chichester, West Sussex, PO19 8SQ, United Kingdom
For details of our global editorial ofﬁces, for customer services and for information about how to apply for permission
to reuse the copyright material in this book please see our website at www.wiley.com.
The right of the author to be identiﬁed as the author of this work has been asserted in accordance with the Copyright,
Designs and Patents Act 1988.
All rights reserved. No part of this publication may be reproduced, stored in a retrieval system, or transmitted, in any
form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK
Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.
Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be
available in electronic books.
Designations used by companies to distinguish their products are often claimed as trademarks. All brand names and
product names used in this book are trade names, service marks, trademarks or registered trademarks of their respective
owners. The publisher is not associated with any product or vendor mentioned in this book. This publication is designed
to provide accurate and authoritative information in regard to the subject matter covered. It is sold on the understanding
that the publisher is not engaged in rendering professional services. If professional advice or other expert assistance is
required, the services of a competent professional should be sought.
Library of Congress Cataloging-in-Publication Data
Koski, Timo.
Bayesian networks : an introduction / Timo Koski, John M. Noble.
p. cm. – (Wiley series in probability and statistics)
Includes bibliographical references and index.
ISBN 978-0-470-74304-1 (cloth)
1.
Bayesian statistical decision theory. 2.
Neural networks (Computer science)
I. Noble, John M. II. Title.
QA279.5.K68 2009
519.5′42 – dc22
2009031404
A catalogue record for this book is available from the British Library.
ISBN: 978-0-470-74304-1
TypeSet in 10/12pt Times by Laserwords Private Limited, Chennai, India.
Printed and bound in Great Britain by TJ International Ltd, Padstow, Cornwall.

Contents
Preface
ix
1
Graphical models and probabilistic reasoning
1
1.1
Introduction
1
1.2
Axioms of probability and basic notations
4
1.3
The Bayes update of probability
9
1.4
Inductive learning
11
1.4.1
Bayes’ rule
12
1.4.2
Jeffrey’s rule
13
1.4.3
Pearl’s method of virtual evidence
13
1.5
Interpretations of probability and Bayesian networks
14
1.6
Learning as inference about parameters
15
1.7
Bayesian statistical inference
17
1.8
Tossing a thumb-tack
20
1.9
Multinomial sampling and the Dirichlet integral
24
Notes
28
Exercises: Probabilistic theories of causality, Bayes’ rule, multinomial
sampling and the Dirichlet density
31
2
Conditional independence, graphs and d-separation
37
2.1
Joint probabilities
37
2.2
Conditional independence
38
2.3
Directed acyclic graphs and d-separation
41
2.3.1
Graphs
41
2.3.2
Directed acyclic graphs and probability distributions
45
2.4
The Bayes ball
50
2.4.1
Illustrations
51
2.5
Potentials
53
2.6
Bayesian networks
58
2.7
Object oriented Bayesian networks
63
2.8
d-Separation and conditional independence
66

vi
CONTENTS
2.9
Markov models and Bayesian networks
67
2.10
I-maps and Markov equivalence
69
2.10.1
The trek and a distribution without a faithful graph
72
Notes
73
Exercises: Conditional independence and d-separation
75
3
Evidence, sufﬁciency and Monte Carlo methods
81
3.1
Hard evidence
82
3.2
Soft evidence and virtual evidence
85
3.2.1
Jeffrey’s rule
86
3.2.2
Pearl’s method of virtual evidence
87
3.3
Queries in probabilistic inference
88
3.3.1
The chest clinic problem
89
3.4
Bucket elimination
89
3.5
Bayesian sufﬁcient statistics and prediction sufﬁciency
92
3.5.1
Bayesian sufﬁcient statistics
92
3.5.2
Prediction sufﬁciency
95
3.5.3
Prediction sufﬁciency for a Bayesian network
97
3.6
Time variables
98
3.7
A brief introduction to Markov chain Monte Carlo methods
100
3.7.1
Simulating a Markov chain
103
3.7.2
Irreducibility, aperiodicity and time reversibility
104
3.7.3
The Metropolis-Hastings algorithm
108
3.7.4
The one-dimensional discrete Metropolis algorithm
111
Notes
112
Exercises: Evidence, sufﬁciency and Monte Carlo methods
113
4
Decomposable graphs and chain graphs
123
4.1
Deﬁnitions and notations
124
4.2
Decomposable graphs and triangulation of graphs
127
4.3
Junction trees
131
4.4
Markov equivalence
133
4.5
Markov equivalence, the essential graph and chain graphs
138
Notes
144
Exercises: Decomposable graphs and chain graphs
145
5
Learning the conditional probability potentials
149
5.1
Initial illustration: maximum likelihood estimate for a fork connection
149
5.2
The maximum likelihood estimator for multinomial sampling
151
5.3
MLE for the parameters in a DAG: the general setting
155
5.4
Updating, missing data, fractional updating
160
Notes
161
Exercises: Learning the conditional probability potentials
162
6
Learning the graph structure
167
6.1
Assigning a probability distribution to the graph structure
168

CONTENTS
vii
6.2
Markov equivalence and consistency
171
6.2.1
Establishing the DAG isomorphic property
173
6.3
Reducing the size of the search
176
6.3.1
The Chow-Liu tree
177
6.3.2
The Chow-Liu tree: A predictive approach
179
6.3.3
The K2 structural learning algorithm
183
6.3.4
The MMHC algorithm
184
6.4
Monte Carlo methods for locating the graph structure
186
6.5
Women in mathematics
189
Notes
191
Exercises: Learning the graph structure
192
7
Parameters and sensitivity
197
7.1
Changing parameters in a network
198
7.2
Measures of divergence between probability distributions
201
7.3
The Chan-Darwiche distance measure
202
7.3.1
Comparison with the Kullback-Leibler divergence and
euclidean distance
209
7.3.2
Global bounds for queries
210
7.3.3
Applications to updating
212
7.4
Parameter changes to satisfy query constraints
216
7.4.1
Binary variables
218
7.5
The sensitivity of queries to parameter changes
220
Notes
224
Exercises: Parameters and sensitivity
225
8
Graphical models and exponential families
229
8.1
Introduction to exponential families
229
8.2
Standard examples of exponential families
231
8.3
Graphical models and exponential families
233
8.4
Noisy ‘or’ as an exponential family
234
8.5
Properties of the log partition function
237
8.6
Fenchel Legendre conjugate
239
8.7
Kullback-Leibler divergence
241
8.8
Mean ﬁeld theory
243
8.9
Conditional Gaussian distributions
246
8.9.1
CG potentials
249
8.9.2
Some results on marginalization
249
8.9.3
CG regression
250
Notes
251
Exercises: Graphical models and exponential families
252
9
Causality and intervention calculus
255
9.1
Introduction
255
9.2
Conditioning by observation and by intervention
257
9.3
The intervention calculus for a Bayesian network
258

viii
CONTENTS
9.3.1
Establishing the model via a controlled experiment
262
9.4
Properties of intervention calculus
262
9.5
Transformations of probability
265
9.6
A note on the order of ‘see’ and ‘do’ conditioning
267
9.7
The ‘Sure Thing’ principle
268
9.8
Back door criterion, confounding and identiﬁability
270
Notes
273
Exercises: Causality and intervention calculus
275
10
The junction tree and probability updating
279
10.1
Probability updating using a junction tree
279
10.2
Potentials and the distributive law
280
10.2.1
Marginalization and the distributive law
283
10.3
Elimination and domain graphs
284
10.4
Factorization along an undirected graph
288
10.5
Factorizing along a junction tree
290
10.5.1
Flow of messages initial illustration
292
10.6
Local computation on junction trees
294
10.7
Schedules
296
10.8
Local and global consistency
302
10.9
Message passing for conditional Gaussian distributions
305
10.10 Using a junction tree with virtual evidence and soft evidence
311
Notes
313
Exercises: The junction tree and probability updating
314
11
Factor graphs and the sum product algorithm
319
11.1
Factorization and local potentials
319
11.1.1
Examples of factor graphs
320
11.2
The sum product algorithm
323
11.3
Detailed illustration of the algorithm
329
Notes
332
Exercise: Factor graphs and the sum product algorithm
333
References
335
Index
343

Preface
This book evolved from courses developed at Link¨oping Institute of Technology and
KTH, given by the authors, starting with a graduate course given by Timo Koski in 2002,
who was the Professor of Mathematical Statistics at LiTH at the time and subsequently
developed by both authors. The book has been aimed at senior undergraduate, masters
and beginning Ph.D. students in computer engineering. The students are expected to have
a ﬁrst course in probability and statistics, a ﬁrst course in discrete mathematics and a
ﬁrst course in algorithmics. The book provides an introduction to the theory of graphical
models.
A substantial list of references has been provided, which include the key works for
the reader who wants to advance further in the topic.
We have beneﬁted over the years from discussions on Bayesian networks and Bayesian
statistics with Elja Arjas, Stefan Arnborg, and Jukka Corander. We would like to thank
colleagues from KTH, Jockum Aniansson, Gunnar Englund, Lars Holst and Bo Wahlberg
for participating in (or suffering through) a series of lectures during the third academic
quarter of 2007/2008 based on a preliminary version of the text and suggesting improve-
ments as well as raising issues that needed clariﬁcation. We would also like to thank
Mikael Skoglund for including the course in the ACCESS graduate school program at
KTH. We thank doctoral and undergraduate students Luca Furrer, Maksym Girnyk, Ali
Hamdi, Majid N. Khormuji, M˚arten Marcus and Emil Rehnberg for pointing out several
errors, misprints and bad formulations in the text and in the exercises. We thank Anna
Talarczyk for invaluable help with the ﬁgures. All remaining errors and deﬁciencies are,
of course, wholly our responsibility.


1
Graphical models and
probabilistic reasoning
1.1
Introduction
This text considers the subject of graphical models, which is an interaction between
probability theory and graph theory. The topic provides a natural tool for dealing with
a large class of problems containing uncertainty and complexity. These features occur
throughout applied mathematics and engineering and therefore the material has diverse
applications in the engineering sciences. A complex model is built by combining simpler
parts, an idea known as modularity. The uncertainty in the system is modelled using
probability theory; the graph helps to indicate independence structures that enable the
probability distribution to be decomposed into smaller pieces.
Bayesian networks represent joint probability models among given variables. Each
variable is represented by a node in a graph. The direct dependencies between the
variables are represented by directed edges between the corresponding nodes and the
conditional probabilities for each variable (that is the probabilities conditioned on the
various possible combinations of values for the immediate predecessors in the network)
are stored in potentials (or tables) attached to the dependent nodes. Information about the
observed value of a variable is propagated through the network to update the probability
distributions over other variables that are not observed directly. Using Bayes’ rule, these
inﬂuences may also be identiﬁed in a ‘backwards’ direction, from dependent variables to
their predecessors.
The Bayesian approach to uncertainty ensures that the system as a whole remains
consistent and provides a way to apply the model to data. Graph theory helps to illustrate
Bayesian Networks: An Introduction
T. Koski, J. Noble
2009 John Wiley & Sons, Ltd

2
GRAPHICAL MODELS AND PROBABILISTIC REASONING
and utilize independence structures within interacting sets of variables, hence facilitating
the design of efﬁcient algorithms.
In many situations, the directed edges between variables in a Bayesian network
can have a simple and natural interpretation as graphical representations of causal
relationships. This occurs when a graph is used to model a situation where the values
of the immediate predecessors of a variable in a network are to be interpreted as the
immediate causes of the values taken by that variable. This representation of causal
relationships is probabilistic; the relation between the value taken by a variable and the
values taken by its predecessors is speciﬁed by a conditional probability distribution.
When a graph structure is given and the modelling assumptions permit a causal
interpretation, then the estimates of the conditional probability tables obtained from
data may be used to infer a system of causation from a set of conditional probability
distributions. A Bayesian network is essentially a directed acyclic graph, together with
the associated conditional probability distributions. When a Bayesian network represents
a causal structure between the variables, it may be used to assess the effects of an
intervention, where the manipulation of a cause will inﬂuence the effect.
The ability to infer causal relationships forms the basis for learning and acting in an
intelligent manner in the external world. Statistical and probabilistic techniques may be
used to assess direct associations between variables; some additional common sense and
modelling assumptions, when these are appropriate, enable these direct associations to
be understood as direct causal relations. It is the knowledge of causal relations, rather
than simply statistical associations, that gives a sense of genuine understanding, together
with a sense of potential control resulting from the ability to predict the consequences of
actions that have not been performed, as J. Pearl writes in [1].
K. Pearson, an early, pre-eminent statistician, argued that the only proper goal of
scientiﬁc investigation was to provide descriptions of experience in a mathematical form
(see [2] written by Pearson in 1892); for example, a coefﬁcient of correlation. Any effort
to advance beyond a description of associations and to deduce causal relations meant,
according to this view, to evoke hidden or metaphysical ideas such as causes; he did not
consider such modelling assumptions to be scientiﬁc.
R.A. Fisher, possibly the most inﬂuential statistician, considered that causation could
be inferred from experimental data only when controlled, or randomized experiments were
employed. A majority of statistical studies follow the approach of Fisher and only infer
‘correlation’ or ‘association’ unless randomized experimental trials have been performed.
It is not within the scope of this treatment of Bayesian networks to review the sophis-
ticated attempts at characterizing causality and the ensuing controversies (starting with
David Hume [3]) amongst scholars; the reader is referred to the enlightening treatment
by J. Williamson [4]. This text attempts to take what seems to be a ‘common sense’
point of view. The human mind is capable of detecting and approving causes of events
in an intuitive manner. For example, the nineteenth century physician Ignaz Semmelweis
in Vienna investigated, without knowing about germs, the causes of child bed fever. He
instituted a policy that doctors should use a solution of chlorinated lime to wash their
hands between autopsy work and the examination of patients, with the effect that the
mortality rate at the maternity wards of hospitals dropped substantially. Such reasoning
about causes and effects is not as straightforward for computers and it is not clear that it
is valid in terms of philosophical analysis.

INTRODUCTION
3
Causality statements are often expressed in terms of large and often complicated
objects or populations: for example, ‘smoking causes lung cancer’. Causal connections
at ﬁner levels of detail will have a different context: for example, the processes at the
cell level that cause lung cancer. Causal connections are also contingent on many other
conditions and causal laws than those explicitly under consideration. This introduces a
level of uncertainty and the causal connections are therefore probabilistic.
Correlations or statistical associations between two variables often imply causation,
even if there is not a direct causal relation between the two variables in question; one need
not be the cause of the other. A correlation may indicate the presence of hidden variables,
that are common causes for both the observed variables, so that the two observed variables
are statistically associated.
When there is a possibility that there may be such unknown hidden variables, it is
necessary to separate the ‘cause’ from the extraneous factors that may inﬂuence it in
order to conclude that there is a causal relationship and a randomized (or controlled)
experiment achieves this. For a randomized experiment, the groups for each level of
treatment, and a control group to which no treatment is applied, are chosen at random
so that the allocation of members to treatment groups is not affected by any hidden
variables. Unfortunately, there are situations where it may be unethical to carry out a
randomized experiment. For example, to prove that smoking causes lung cancer, it is
perhaps inappropriate to force a non-smoker to start smoking.
In the example of smoking and lung cancer, the model is unknown and has to be
inferred from data. The statistical analyst would like to establish whether smoking causes
lung cancer, or whether there are additional hidden variables that are causes for both
variables. Note that common sense plays a role here; the possibility that lung cancer may
cause smoking is not considered. In terms of graphical models, where the direction of
the pointed arrow indicates cause to effect, the analyst wants to determine which of the
models in Figure 1.1 are appropriate.
When he carries out a controlled experiment, randomly assigning people to ‘smokers’
and ‘non-smokers’ respectively, the association between the hidden variables and smoking
for this experiment are broken and the causal diagram is therefore given by Figure 1.2.
By carrying out a ‘controlled experiment’, an intervention is made whereby the causal
path from the hidden variable is removed, thus ensuring that only the causal connection
of interest is responsible for an observed association.
In many situations, a controlled experiment seems the only satisfactory way to demon-
strate conclusively that an association between variables is due to a causal link. Without
a controlled experiment, there may remain some doubt. For example, levels of smok-
ing dropped when the ﬁrst announcements were made that smoking caused lung cancer
smoking
hidden
cancer
smoking
hidden
cancer
Figure 1.1
Smoking and lung cancer.

4
GRAPHICAL MODELS AND PROBABILISTIC REASONING
smoking
hidden
cancer
Figure 1.2
Controlled experiment: the control has removed the hidden causes of smok-
ing from consideration.
and levels of lung cancer also dropped substantially. A controlled experiment would have
demonstrated conclusively that the drop in lung cancer was not due to other environmental
factors, such as a decline in heavy polluting industry that occurred at the same time.
Bayesian networks provide a straightforward mathematical language to express rela-
tions between variables in a clear way. In many engineering examples, the variables that
should be present in the model are well deﬁned. From an appropriate model that contains
the hidden (or non-observable) variables and the observable variables, and where it is
clear which variables may be intervened on, it will be possible to verify whether certain
‘identiﬁability’ conditions hold and hence to conclude whether or not there is a causal
relation from the data, without a controlled experiment.
Many of the classical probabilistic systems studied in ﬁelds such as systems engineer-
ing, information theory, pattern recognition and statistical mechanics are problems that
may be expressed as graphical models. Hidden Markov models may be considered as
graphical models. Engineers are also accustomed to using circuit diagrams, signal ﬂow
graphs, and trellises, which may be treated using the framework of graphical models.
Examples of applied ﬁelds where Bayesian networks have recently been found to
provide a natural mathematical framework are reliability engineering [5], software testing
[6], cellular networks [7], and intrusion detection in computer systems [8]. In 1996,
William H. Gates III, a co-founder of Microsoft, stated that expertise in Bayesian networks
had enhanced the competitive advantage of Microsoft. Bayesian networks are used in
software for electrical, ﬁnancial and medical engineering, artiﬁcial intelligence and many
other applications.
1.2
Axioms of probability and basic notations
The basic set notations that will be used will now be established.
Deﬁnition 1.1 (Notations) The following notations will be used throughout:
• The universal set will be denoted by . This is the context of the experiment. In
Bayesian analysis, the unknown parameters are considered to be random. Therefore,
the context  consists of the set of all possible outcomes of an experiment, together
with all possible values of the unknown parameters.
• The notation X will be used to denote the sample space; the set of all possible
outcomes of the experiment.
• ˜ will be used to denote the parameter space, so that  = X × ˜.

AXIOMS OF PROBABILITY AND BASIC NOTATIONS
5
The following notations will be used when considering sets:
• If A and B are two sets, then A ∪B or A ∨B denotes their union. If A1, . . . , An
are a ﬁnite collection of sets, then ∪n
j=1Aj or n
j=1 Aj denotes their union. Also,
A1 ∪. . . ∪An or A1 ∨. . . ∨An may be used to denote their union.
• If A and B are two sets, then A ∩B or AB or A ∧B may be used to denote their
intersection. If A1, . . . , An are a ﬁnite collection of sets, then A1 . . . An, A1 ∩. . . ∩
An, ∩n
j=1Aj or A1 ∧. . . ∧An all denote their intersection.
• A ⊂B denotes that A is a strict subset of B. A ⊆B denotes that A is a subset of
B, possibly equal to B.
• The empty set will be denoted by φ.
• Ac denotes the complement of A; namely, \A, where  denotes the universal set.
The symbol \ denotes exclusion.
• Together with the universal set , an event space F is required. This is a collection
of subsets of . The event space F is an algebra of constructable sets. That is, F
satisﬁes the following:
1. φ ∈F and  ∈F.
2. Each A ∈F may be constructed. That is, for each A ∈F, let iA :  →{0, 1}
denote the mapping such that iA(ω) = 1 if ω ∈A and iA(ω) = 0 if ω ∈Ac.
Then there is a procedure to determine the value of iA(ω) for each ω ∈.1
3. If for a ﬁnite collection of events (Aj)n
j=1 each Aj satisﬁes Aj ∈F, then
∪n
j=1Aj ∈F.
4. If A ∈F, then Ac ∈F, where Ac = \A denotes the complement of A.
5. Each A ∈F satisﬁes A ⊆.
For , F satisfying the above conditions, a probability distribution p over (, F)
(or, in short, a probability distribution over , when F is understood) is a function
p : F →[0, 1] satisfying the following version of the Kolmogorov axioms:
1. p(φ) = 0 and p() = 1.
2. If (Aj)n
j=1 is a ﬁnite collection such that each Aj ∈F and the events satisfy
Aj ∩Ak = φ for all j ̸= k, then
p

∪n
j=1Aj

=
n

j=1
p(Aj).
3. 0 ≤p(A) ≤1 for all A ∈F.
1 A non-constructable set The following example illustrates what is intended by the term constructable
set. Let An,k = ( k
2n −
1
23(n+1) , k
2n +
1
23(n+1) ) and let A = ∪∞
n=1 ∪2n+1
k=0 An,k. Then A is not constructable in the
sense given above. For any number x ∈A ∩[0, 1], there is a well deﬁned algorithm that will show that it is in
A ∩[0, 1] within a ﬁnite number of steps. Consider a number x ∈[0, 1] and take its dyadic expansion. Then,
for each An,k it is clear whether or not x ∈An,k. Therefore, if x ∈A, this may be determined within a ﬁnite
number of steps. But if x ∈Ac, there is no algorithm to determine this within a ﬁnite number of steps.

6
GRAPHICAL MODELS AND PROBABILISTIC REASONING
This is a reduced version of the Kolmogorov axioms. The Kolmogorov axioms require
countable additivity rather than ﬁnite additivity. They invoke axiomatic set theory and
do not therefore require the constructive hypothesis. For the Kolmogorov axioms, the
event space F is taken to be a sigma-algebra and the second axiom requires countable
additivity.
• Let (θ, x) ∈ = ˜ × X. For each A ∈F, let A = {θ ∈˜|(θ, x) ∈A} and let
AX = {x ∈X|(θ, x) ∈A}. Then F will be used to denote the algebra {A|A ∈F}
and FX = {AX|A ∈F}.
Deﬁnition 1.2 (Probability Distribution over X) If X contains a ﬁnite number of elements,
then FX contains a ﬁnite number of elements. In this setting, a probability distribution over
X satisﬁes:
• p({x}) ≥0 for all x ∈X,
• For any A ∈FX, 
x∈A p({x}) = p(A).
• In particular, 
x∈X p({x}) = 1
Deﬁnition 1.3 (Notation) Let X be a ﬁnite state space and let AX denote the set of all
subsets of X (including φ the empty set and X). For probability distribution p : AX →
[0, 1] deﬁned above (Deﬁnition 1.2), p will also be used to denote the function p : X →
[0, 1] such that p(x) = p({x}). The meaning of p will be clear from the context.
The deﬁnitions and notations will now be established for random variables.
Deﬁnition 1.4 (Random Variables and Random Vectors) Discrete random variables, con-
tinuous random variables and random vectors satisfy the following properties:
• For this text, a discrete random variable Y is a function Y :  →C where C is a
countable space, that satisﬁes the following conditions: for each x ∈C, {ω|Y(ω) =
x} ∈F, and there is a function pY : C →R+, known as the probability function of
Y, such that for each x ∈C,
pY(x) = p({ω|Y(ω) = x}).
This is said to be the ‘probability that Y is instantiated at x’. Therefore, for any
subset C ⊆C, pY satisﬁes

x∈C
pY(x) = p({ω|Y(ω) ∈C}).
In particular, taking C = C,

x∈C
pY(x) = 1.

AXIOMS OF PROBABILITY AND BASIC NOTATIONS
7
• A continuous random variable  is deﬁned as a function  :  →R such that
for any set A ⊆R such that the function 1A∩[−N,N] is Riemann integrable for all
N ∈Z+, the set {ω|(ω) ∈A} ∈F, and for which there is a function π, known
as the probability density function, or simply density function, such that for any
A ⊂R with Riemann integrable indicator function,
p({ω|(ω) ∈A}) =

A
π(x)dx.
In particular,
p({ω|(ω) ∈R}) = p() =

R
π(x)dx = 1.
• A random vector Y is a vector of random variables. It will be taken as a row vector
if it represents different characteristics of a single observation; it will be taken as
a column vector if it represents a collection of independent identically distributed
random variables.
This convention is motivated by the way that data is presented in a data matrix.
Each column of a data matrix represents a different attribute, each row represents
a different observation.
A random row vector Y is a collection of random variables that satisﬁes the
following requirements: suppose Y = (Y1, . . . , Ym) and for each j = 1, . . . , m,
Yj is a discrete random variable that takes values in a countable space Cj and
let C = C1 × . . . × Cm, then Y is a random vector if for each (y1, . . . , ym) ∈C,
{ω|(Y1(ω), . . . , Ym(ω))} ∈F, and there is a joint probability function pY1,...,Ym :
C →[0, 1] such that for any set C ⊆C,

(y1,...,ym)∈C
pY1,...,Ym(y1, . . . , ym) = p({ω|(Y1(ω), . . . , Ym(ω)) ∈C}).
In particular,

(y1,...,ym)∈C
pY1,...,Ym(y1, . . . , ym) = 1.
If  := (1, . . . , n) is a collection of n random variables where for each j =
1, . . . , n j is a continuous random variable, then  is a random vector if for each
set A ⊂Rn such that 1A∩[N,N]n is Riemann integrable for each N ∈Z+,
{ω|(1(ω), . . . , n(ω)) ∈A} ∈F,
and there is a Riemann integrable function π1,...,n : Rn →R+, where R+ denotes
the non-negative real numbers such that for each set A ⊂Rn such that 1A∩[N,N]n
is Riemann integrable for each N ∈Z+,
p({ω|(1(ω), . . . , n(ω)) ∈A}) =

A
π1,...,n(x1, . . . , xn)dx1 . . . dxn.

8
GRAPHICAL MODELS AND PROBABILISTIC REASONING
In particular,

Rn π1,...,n(x1, . . . , xn)dx1 . . . dxn = 1.
A collection of random variables Y of length m + n, containing m discrete variables
and n continuous variables, is a random vector if it satisﬁes the following: there
is an ordering σ of 1, . . . , m + n such that (Yσ(1), . . . , Yσ(m)) is a discrete random
vector and (Yσ(m+1), . . . , Yσ(m+n)) is a continuous random vector. Furthermore, let
˜C denote the state space for (Yσ(1), . . . , Yσ(m)), then for each (y1, . . . , ym) ∈˜C, there
is a Riemann integrable function π(σ)
Yσ(m+1),...Yσ(m+n)|y1,...,ym : Rn →R+ such that for
any set A ∈Rn such that 1A∩[N,N]n is Riemann integrable for each N ∈Z+,
p({ω|(Yσ(1), . . . , Yσ(m)) = (y1, . . . , ym), (Yσ(m+1), . . . , Yσ(m+n)) ∈A})
= pYσ(1),...,Yσ(m)(y1, . . . , ym)

A
π(σ)
Yσ(m+1),...Yσ(m+n)|y1,...,ym(x1, . . . , xn)dx1 . . . dxn.
and such that

Rn π(σ)
Yσ(m+1),...Yσ(m+n)|y1,...,ym(x1, . . . , xn)dx1 . . . dxn = 1.
Deﬁnition 1.5 (Marginal Distribution) Let X = (X1, . . . , Xn) be a discrete random
vector, with joint probability function pX1,...,Xn. The probability distribution for
(Xj1, . . . , Xjm), where m ≤n and 1 ≤j1 < . . . < jm ≤n is known as the marginal
distribution, and the marginal probability function is deﬁned as
pXj1,...,Xjm(x1, . . . , xm) =

(y1,...,yn)|(yj1,...,yjm)=(xj1,...,xjm)
pX1,...,Xn(y1, . . . , yn).
In particular, for two discrete variables X and Y taking values in the spaces CX and CY
respectively, with joint probability function pX,Y, the marginal probability function for
the random variable X is deﬁned as
pX(x) =

y∈CY
pX,Y (x, y)
and the marginal probability function for the random variable Y is deﬁned as
pY(y) =

x∈CX
pX,Y(x, y).
If  = (1, . . . , n) is a continuous random vector, with joint probability density func-
tion π1,...,n, then the marginal density function for j1, . . . , jm, where {j1, . . . , jm} ⊂
{1, . . . , n} is deﬁned as
πj1,...,jm(x1, . . . , xm) =

Rn−m π1,...,n(y1, . . . , yn|(yj1, . . . , yjm)
= (x1, . . . , xm))

k̸∈(j1,...jm)
dyk.
□

THE BAYES UPDATE OF PROBABILITY
9
Categorical random variables
In this text, the sample space X will contain a ﬁnite, or
countably inﬁnite, number of outcomes, while usually the parameter space ˜ ⊆Rn, where
n is the number of parameters. Most of the discrete variables arising will be categorical,
in the sense that the outcomes are classiﬁed in several categories. For example, suppose
an urn contains 16 balls, of which four are white, six are green, ﬁve are blue and one is
red. Pick a ball at random and let C denote the colour of the ball, then C is an example
of a discrete random variable. The probability distribution of a categorical variable C is
denoted by pC. Here, for example, p({C = green}) = pC(green) = 6
16 = 3
8;
pC =
white
green
blue
red
1
4
3
8
5
16
1
16
The set-up described above is adequate for this text, where only two types of random vari-
ables are considered; continuous (which, by deﬁnition, have a Riemann integrable density
function) and discrete variables. Statements of uncertainty made within this framework
are consistent and coherent. Any probability function p over F that is to provide a
quantitative assessment of the probability of an event, which is also to be mathemat-
ically coherent over a constructive algebra of events, must satisfy the axioms listed
above. Any set function over an algebra of sets that satisﬁes these axioms will provide
a mathematically coherent measure of uncertainty.
1.3
The Bayes update of probability
The prior probability is the probability distribution p over F before any relevant data is
obtained. The prior probability is related to background information, modelling assump-
tions, or simply a function introduced for mathematical convenience, which is intended
to express a degree of vagueness. It could be written p(K)(.), where K designates what
could be called the context or a frame of knowledge [9]. The notation p(.|K) is often
employed, although this is a little misleading since, formally, conditional probability is
only deﬁned in terms of an initial distribution. Here, the initial distribution is based on
K. Since the notation is now standard, it will be employed in this text, although caution
should be observed.
Consider a prior distribution, denoted by p(.). Suppose the experimenter, or agent,
has the information that B ∈F holds and desires to update the probabilities based on
this piece of information. This update involves introducing a new probability distribution
p∗on F. Suppose also that p(B) > 0. Since it is now known that B is a certainty, the
update requires
p∗(B) = 1
so that p∗(Bc) = 0, where Bc = \B is the complement of B in . The updated prob-
ability is constructed so that the ratio of probabilities for any B1 ⊂B and B2 ⊂B does
not change. That is, for Bi ⊂B, i = 1, 2,
p∗(B1)
p∗(B2) = p(B1)
p(B2).

10
GRAPHICAL MODELS AND PROBABILISTIC REASONING
For arbitrary A ∈F, the axioms yield
p∗(A) = p∗(A ∩B) + p∗(A ∩Bc).
But since p∗(Bc) = 0, it follows that p∗(A ∩Bc) ≤p∗(Bc) = 0. In other words, it fol-
lows that for arbitrary A ∈F, since p∗(B) = 1,
p∗(A) = p∗(A ∩B) + p∗(A ∩Bc)
p∗(B)
= p∗(A ∩B)
p∗(B)
= p(A ∩B)
p(B)
.
The transformation p →p∗is known as the Bayes update of probability. When the
evidence is obtained is precisely that an event B ∈F within the algebra has happened,
Bayes’ rule may be used to update the probability distribution. It is customary to use the
notation p(A|B) to denote p∗(A). Hence
p(A|B)
def
= p∗(A) = p(A ∩B)
p(B)
.
(1.1)
This is called the conditional probability of A given B. This characterization of the
conditional probability p(A|B) follows [10]. Further discussion may be found in [11].
This is the update used when the evidence is precisely that an event B ∈F has occurred.
Different updates are required to incorporate knowledge that cannot be expressed in this
way. This is discussed in [12].
From the deﬁnition of p(A|B), the trivial but important identity
p(A|B)p(B) = p(A ∩B)
(1.2)
follows for any A, B ∈F.
The Bayes factor
Bayes’ rule simply states that for any two events A and C,
p(A|C) = p(C|A)p(A)
p(C)
.
If event C represents new evidence, and p∗(.) = p(.|C) represents the updated probability
distribution, then for any two events A and B, Bayes’ rule yields:
p∗(A)
p∗(B) = p(C|A)
p(C|B)
p(A)
p(B).
The factor p(C|A)
p(C|B) therefore updates the ratio p(A)
p(B) to p∗(A)
p∗(B). Note that
p(C|A)
p(C|B) = p∗(A)/p∗(B)
p(A)/p(B) .
This leads to the following deﬁnition, which will be used later.
Deﬁnition 1.6 Let p and q denote two probability distributions over an algebra A. For
any two events A, B ∈A, the Bayes factor Fq,p(A; B) is deﬁned as
Fq,p(A, B) := q(A)/q(B)
p(A)/p(B).
(1.3)

INDUCTIVE LEARNING
11
Here p plays the role of a probability before updating and q plays the role of an updated
probability. The Bayes factor indicates whether or not the new information has increased
the odds of an event A relative to B.
Bayes’ rule applied to random variables
Let X and Y be two discrete random vari-
ables. Let pX, pY, pX|Y and pY|X denote the probability mass functions of X, Y, X given
Y and Y given X respectively. It follows directly from Bayes’ rule that for all x, y
pX|Y(x|y) = pY|X(y|x)pX(x)
pY (y)
.
(1.4)
If X and Y are continuous random variables with density functions πX and πY , then the
conditional probability density function of X, given Y = y is
πX|Y(x|y) = πY|X(y|x)πX(x)
πY (y)
.
If X is a discrete random variable and  is a continuous random variable with state space
˜, where pX|(.|θ) is the conditional probability function for X given  = θ and  has
density function π, then Bayes’ rule in this context gives
π|X(θ|x) =
pX|(x|θ)π(θ)

˜ pX|(x|θ)π(θ)dθ = pX|(x|θ)π(θ)
pX(x)
.
(1.5)
1.4
Inductive learning
The task of inductive learning is, roughly stated, to ﬁnd a general law, based of a
ﬁnite number of particular examples. Without further information, a law established in
this way cannot be a certainty, but necessarily has a level of uncertainty attached to it.
The assessment uses the idea that future events similar to past events will cause similar
outcomes, so that outcomes from past events may be used to predict outcomes from
future events. There is a subjective component in the assessment of the uncertainty,
which enters through the prior distribution. This is the assessment made before any
particular examples are taken into consideration.
In any real situation within the engineering sciences, a mathematical model is only
ever at best a model and can never present a full description of the situation that is
being modelled. In many situations, the information is not presented in terms of absolute
certainties to which deductive logic may be applied and ‘inductive learning’, as described
above, is the only way to proceed.
For machine learning of situations arising in the engineering sciences, the machine
learns inductively from past examples, from which it makes predictions of future
behaviour and takes appropriate decisions; for example, deciding whether a paper mill
is running abnormally and, if it is, shutting it down and locating the error.
In the following discussion, the second person singular pronoun, ‘You’, will be used to
denote the person, or machine,2 with well deﬁned instructions, analysing some uncertain
statements and making predictions based on this analysis.
2 A more precise description of the machine is the hypothetical artiﬁcial intelligence robot ‘Robbie’ in [13].

12
GRAPHICAL MODELS AND PROBABILISTIC REASONING
The Bayes update rule is now applied to learning from experience. The update is
based on the deﬁnition of the conditional probability p(E|A) of an event E given an
event A. The rule for calculating the conditional probability of A given E (or, given that
E ‘occurs’) is known as Bayes’ rule and is given by
p(A|E) = p(E|A)p(A)
p(E)
.
(1.6)
Equation (1.6) follows immediately from the deﬁnitions introduced in Equations (1.1)
and (1.2). Here E is a mnemonic notation for evidence.
The formula (1.6) was for a long time known more widely as ‘inverse probability’
rather than as ‘Bayes’ rule’. The quantity p(A|E) does not always need to be computed
using this formula; sometimes it is arrived at by other means. For example, the Jeffrey’s
update rule, as will be seen later, uses a particular set of conditional probabilities as
fundamental and derives the other probabilities from it.
1.4.1
Bayes’ rule
A more instructive form of Bayes’ rule is obtained by considering a ﬁnite exhaustive set
of mutually exclusive hypotheses {Hi}m
i=1. The law of total probability gives for any E
p(E) =
m

i=1
p(Hi)p(E|Hi)
(1.7)
and Equation (1.6) yields for any Hi
p(Hi|E) =
p(E|Hi)p(Hi)
m
i=1 p(Hi)p(E|Hi).
(1.8)
Here p(Hi) is the initial or prior probability for a hypothesis, before any evidence E is
obtained. The prior probability will have a subjective element, depending on background
information and how You interpret this information. As discussed earlier, this background
information is often denoted by the letter K. In computer science, this is often referred to
as domain knowledge [14]. The prior is therefore often denoted p(Hi|K). As discussed
earlier, this is misleading, because no application of the formula given in Equation (1.1)
has been made. The notation is simpliﬁed by dropping K.
The quantity in p(Hi|E) in Equation (1.8) denotes how p(Hi) is updated to a new
probability, called the posterior probability, based on this new evidence. This update is
the key concept of Bayesian learning from experience.
There is a basic question: why should probability calculus be used when performing
the inductive logic in the presence of uncertainty described above? This is connected
with the notion of coherence. The probability calculus outlined above ensures that the
uncertainty statements ‘ﬁt together’ or ‘cohere’ and it is the only way to ensure that
mathematical statements of uncertainty are consistent. To ensure coherence, inductive
logic should therefore be expressed through probability. Inference about uncertain events
Hi from an observed event E will be mathematically coherent if and only if they are
made by computing p(Hi|E) in the way described above. All calculations, in order
to achieve coherence, must be made within this probability calculus. Hence, to make

INDUCTIVE LEARNING
13
coherent statements about different events, Your calculations for learning by experience
have to satisfy Bayes’ rule.
The introduction of an arbitrary prior distribution may appear, at ﬁrst sight, to be
ad hoc. The important point here is that all modelling contains some ad hoc element in
the choice of the model. The strength of the Bayesian approach is that once the prior
distribution is declared, this contains the modelling assumptions and the ad hoc element
becomes transparent. The ad hoc element is always present and is stated clearly in the
Bayesian approach.
When an event E occurs that belongs to the well deﬁned algebra of events, over
which there are well deﬁned probabilities, the Bayes rule for updating the probability of
an event A from p(A) to p(A|E) is applied. There are situations where the evidence E
may be given in terms of observations that are less precise (that is, E is not an event that
clearly belongs to the algebra), but nevertheless an update of the the probability function
p(·) is required. Jeffrey’s rule and Pearl’s method of virtual evidence can be useful in
these situations.
1.4.2
Jeffrey’s rule
Suppose that the new evidence implies that You form an exhaustive set of r mutu-
ally exclusive hypotheses (Gi)r
i=1 which, following the soft evidence have probabilities
p∗(Gi). The question is how to update the probability of any event A ∈F. Note that You
cannot use Bayes’ rule (Equation (1.6)), since the evidence has not been expressed in
terms of a well deﬁned event E for which the prior probability value is known. Jeffrey’s
rule may be applied to the situation where it may be assumed that for all events A ∈F,
the probabilities p(A|Gi) remain unchanged. It is only the assessment of the the mutually
exclusive hypotheses (Gi)r
i=1 that changes; no new information is given about the the
relevance of (Gi)r
i=1 to other events.
Deﬁnition 1.7 (Jeffrey’s Update) The Jeffrey’s rule for computing the update of the prob-
ability for any event A, is given by
p∗(A) =
r

i=1
p∗(Gi)p(A|Gi).
(1.9)
This is discussed in [15]. Jeffrey’s rule provides a consistent probability function, such
that p∗(A|Gi) = p(A|Gi) for all i. Equation (1.9) is therefore an expression of the rule
of total probability (Deﬁnition 1.7).
□
1.4.3
Pearl’s method of virtual evidence
In the situation considered by Pearl, new evidence gives information on a set of mutually
exclusive and exhaustive events G1, . . . , Gn, but is not speciﬁed as a set of new probabil-
ities for these events. Instead, for each of the events G1, . . . , Gn, the ratio λj = p(A|Gj )
p(A|G1),
for j = 1, . . . , n is given for an event A. That is, λj represents the likelihood ratio that
the event A occurs given that Gj occurs, compared with G1. Note that λ1 = 1.
Deﬁnition 1.8 Let p denote a probability distribution over a countable space X (Deﬁ-
nition 1.2) and let G1, . . . , Gn ∈F be a mutually exclusive (that is Gi ∩Gj = φ for all

14
GRAPHICAL MODELS AND PROBABILISTIC REASONING
i ̸= j) and exhaustive (that is ∪n
j=1Gj = ) events, where p(Gj) = pj. Set λj = p(A|Gj )
p(A|G1)
for j = 1, . . . , n. Then, for each x ∈X, the Pearl update ˜p is deﬁned as
˜p({x}) = p({x})
λj
n
j=1 λjpj
x ∈Gj,
j = 1, . . . , n.
(1.10)
It is clear that this provides a well deﬁned probability distribution.
Jeffrey’s rule and Pearl’s method for virtual evidence will be discussed further in
Section 3.2. They are methods that, under some circumstances, enable evidence to be
incorporated that does not ﬁt directly into the framework of the chosen statistical model.
1.5
Interpretations of probability and Bayesian networks
Loosely speaking, ‘classical statistics’ proposes a probability distribution over the event
space, where the probability distribution is a member of a parametric family, where the
value of the parameters are unknown. The parameters are estimated, and the estimates
are used to obtain an approximation to the ‘true’ probability, which is unknown. In
Bayesian probability, the lack of knowledge of the parameter is expressed though a
probability distribution over the parameter space, to feature Your personal assessment
of the probability of where the parameter may lie. For this reason, Bayesian probability
is also referred to as personal probability, or epistemological probability. Built into a
Bayesian probability is Your a priori state of knowledge, understanding and assessment
concerning the model and the source of data. So, loosely speaking, classic statistics yields
an approximation to an objectively ‘true’ probability. The ‘true’ probability is ﬁxed, but
unknown, and the estimate of this probability may differ between researchers. Bayesian
statistics yields an exact computation of a subjective probability. It is the probability
itself , rather than the estimate of the probability, that may differ between researchers.
Bayesian networks are frequently implemented as information processing components
of Expert Systems (in artiﬁcial intelligence) [14], where personal and epistemological
probability provides a natural framework for the machine to learn from its experience.
P. Cheeseman in [13] and [16] argues that personal probability as the calculus of plausible
reasoning is the natural paradigm for artiﬁcial intelligence.
J. Williamson [4] and other authors distinguish between subjective and objective
Bayesian probability. Consider two different learning agents, called (for convenience)
Robbieα and Robbieβ. These are two hardware copies of Robbie in [13], with some
different internal representations (i.e. different ways of assessing prior information). A
Bayesian probability is said to be objective if, with the same background information,
the two agents will agree on the probabilities. Bayesian probabilities are subjective if the
two different learning agents, Robbieα and Robbieβ may disagree about the probabilities,
even though they share the same background knowledge, but without either of then being
provably wrong.
It seems rational to require that subjective probabilities should be calibrated with
the external world. This does not follow from requirements of coherence; it is rather a
principle of inference that is imposed in any serious statistical study. For this, one often
cites Lewis’s principal principle, stated in [17]. This is the assertion that Your subjective
probability for an event, conditional upon the knowledge of a physical probability of that

LEARNING AS INFERENCE ABOUT PARAMETERS
15
event, should equal the physical probability. In terms of the formula (where ch denotes
the physical chance),
p(A | ch (A) = x) = x.
There are several theoretical foundations for reconciling subjective opinion to objec-
tive probability. One of the more prominent was given by R.J. Aumann.3 He proved the
following fact. If two agents have same priors, and if their posteriors for an event A are
common knowledge (i.e. Robbieα and Robbieβ know the posterior, and Robbieα knows
that Robbieβ knows and that Robbieβ knows that Robbieα knows that Robbieβ knows
and so on, ad inﬁnitum), then the two posterior probability masses will be identical [18].
This relies on the rational idea that if someone presents an opinion different from
Yours, then this is an important piece of information which should induce You to revise
Your opinion. The result by Aumann quoted above implies that there will be a process
of revision, which will continue until objective probability (an equilibrium of consensus)
is reached. It can be shown that this will happen in a ﬁnite number of steps.
In probabilistic theories of causation there is a set of variables V , over which there
is a probability distribution p. The causal relationships between the variables in V are
the object of study. In [19], the variables are indexed by a time parameter and causality
is reduced to probability. The Bayesian network approach is different; in addition to
the probability distribution p, the variables are nodes of a directed acyclic graph G,
where the edges represent direct causal relationships between the variables in V . This
requires, as pointed out by D. Freedman and P. Humphreys [20], that we already know
the causal structure obtained, for example, by exercise of common sense and knowledge
of the variables V . Both p and G are required [21]. Additional assumptions are therefore
required to infer the direction of cause to effect relationships between variables; there is
no form of Bayesian coherence from which they may be inferred. The role of a directed
graph is to represent information about dependence between variables V . In particular,
the graph may be used to indicate what would happen to values of some variables under
changes to other variables that are called interventions; namely, the variable is forced
to take a particular value irrespective of the state of the rest of the system. The graph
will also indicate how the various different distributions for subsets of V are consistently
connected to each other to yield p.
1.6
Learning as inference about parameters
Consider a random row vector X = (X1, . . . , Xd), denoting d attributes. Suppose that
n independent observations are made on X and the values obtained are recorded in a
matrix x, where xjk denotes the jth recorded value of attribute k. The n × d matrix x
may be regarded as an instantiation of the n × d random matrix X, where each row of
X is an independent copy of X. Suppose that the evidence x is to be used to update the
probability function for another collection of m variables, (Y1, . . . , Ym) from pY1,...,Ym(.)
to pY1,...,Ym|X(.|x).
A fundamental special case, discussed in [22], is that of computing the predictive
probability for the next observation in the univariate setting. That is, d = 1. Here, the
3 2005 Laureate of the Sveriges Riksbank Prize in Economic Sciences in Memory of Alfred Nobel.

16
GRAPHICAL MODELS AND PROBABILISTIC REASONING
matrix X is an n × 1 matrix and may therefore be considered as a column vector X(n) =
(X1, . . . , Xn)t, where Xj, j = 1, . . . , n are independent identically distributed. Here, x =
x(n) = (x1, . . . , xn)t, a vector of n observed values. The random vector Y is simply Xn+1.
The problem is to compute the conditional probability distribution of Y given X(n) = x(n).
The connection between x(n) and y is described by a probability pY|X(n)(y|x(n)) of Y given
X(n) = x(n). Let X(n+1) = (X1, . . . , Xn+1)t and x(n+1) = (x1, . . . xn+1)t. You compute
pXn+1|X(n)(xn+1|x(n)) =
pX(n+1)(x(n+1))
pX(n)(x(n))
.
(1.11)
The progression from here requires a mathematical model containing parameters denoted
θ such that, given θ, the random variables X1, X2, . . . are independent and identically
distributed (i.i.d.). That is, with notation (.|θ) to denote the parameter ﬁxed as θ, there
is a decomposition
pX(n)(x(n)|θ) =
n

1
qXi(xi|θ).
The family of probability functions qXi(.|θ), θ ∈˜ (where ˜ is the space of all per-
missible values of the unknown parameter θ) and the parameter θ need to be speciﬁed.
Since the value of θ is unknown, the Bayesian approach is to consider a probability
distribution over ˜. Thus, θ may be regarded as a realisation of a random variable .
You need to specify a prior distribution over ˜. This discussion conﬁnes itself to the case
where  is considered to be a continuous random variable and hence the prior distribu-
tion is described by a probability density function π : ˜ →R+. The prior predictive
distribution may then be written as
pX(n)(x(n)) =

˜
n

1
qXi(xi|θ)π(θ)dθ.
(1.12)
Deﬁnition 1.9 (Prior Predictive Probability Distribution) The prior distribution pX(n) for
the collection of random variables X(n) (for which x(n) is an observation) is known as the
Prior Predictive Probability Distribution.
B. De Finetti showed in [23] that if the xjs are inﬁnitely exchangeable (and lie in a
reasonable space), then the structure for pX(n)(.) given by Equation (1.12) is the only
possible one.
Inserting (1.12) in the right hand side of (1.11) yields
pXn+1(xn+1|x(n)) =

˜
	n+1
1
qXi(xi|θ)π(θ)dθ

˜
	n
1 qXi(xi|θ)π(θ)dθ
=

˜
qXn+1(xn+1|θ)
	n
1 qXi(xi|θ)π(θ)

˜
	n
1 qXi(xi|θ)π(θ)dθ dθ.

BAYESIAN STATISTICAL INFERENCE
17
The conditional probability density of  given X(n) = x(n) may be obtained by Bayes’
rule:
π|X(n)

θ | x(n)

=
	n
1 qXi(xi|θ)π(θ)

˜
	n
1 qXi(xi|θ)π(θ)dθ .
(1.13)
It follows directly that
pXn+1(xn+1|x(n)) =

˜
qXn+1(xn+1|θ)π|X(n)

θ | x(n)
 dθ.
(1.14)
In Section 1.9, explicit examples of evaluations of Equation (1.14) are given.
The probability density function π, placed over ˜ before any data is observed is
known as the prior density; the probability density function π|X(n) deﬁned by Equation
(1.13) is known as the posterior probability density. Equation (1.14) shows how Bayesian
learning about Xn+1 is based on learning about θ from x(n). Bayesian statistical inference
is the term used to denote Bayesian learning of the posterior distribution of a set of
parameters.
The meaning of causality for K. Pearson, see Chapter 4 in [2], seems to be expressible
by Equation (1.14), as he writes ‘that a certain sequence has occurred and recurred in the
past is a matter of experience to which we give expression in the concept of causation,
that it will recur in the future is a matter of belief to which we give expression in the
concept of probability.’
1.7
Bayesian statistical inference
The aim of learning is to predict the nature of future data based on past experience [22].
One constructs a probabilistic model for a situation where the model contains unknown
parameters. The parameters are only a mechanism to help estimate future behaviour; they
are not an end in themselves.
As stated, the ‘classical’ approach regards a parameter as ﬁxed. It is unknown and has
to be estimated, but it is not considered to be a random variable. One therefore computes
approximations to the unknown parameters, and uses these to compute an approximation
to the probability density. The parameter is considered to be ﬁxed and unknown, because
there is usually a basic assumption that in ideal circumstances, the experiment could be
repeated inﬁnitely often and the estimating procedure would return a precise value for the
parameter. That is, if one increases the number of replications indeﬁnitely, the estimate
of the unknown parameter converges, with probability one, to the true value. This is
known as the ‘frequentist’ interpretation of probability.
Basic to the ‘frequentist’ interpretation is the assumption that an experiment may
be repeated, in an identical manner, an indeﬁnite number of times. With the classical
approach, the sample space  and the event space A of subsets of  have to be deﬁned
in advance. This makes incorporation of ‘soft evidence’ or ‘virtual evidence,’ that will be
considered later in the text, harder; these are situations where the information obtained
cannot be expressed as one of the well deﬁned events of the event space. Then, the
probability distribution is interpreted as follows: for each A ∈A, p(A) is interpreted as

18
GRAPHICAL MODELS AND PROBABILISTIC REASONING
the limit, from observed data that would be obtained if the experiment could be repeated
independently, under identical circumstance, arbitrarily often. That is,
p(A) =
lim
n→+∞
N(n, A)
n
,
where N(n, A) denotes the number of times event A has been observed from n replica-
tions of the experiment.
This interpretation is intuitively appealing, but there is room for caution, since the
inﬁnite independent replications are imagined, and therefore the convergence of relative
frequencies to a limit is hypothetical; the imagined inﬁnite sequence of replications under
exactly the same conditions is a textbook construction and abstraction. In concrete terms,
it is supposed that there are many sources, each with large numbers of data so that
the ‘empirical’ distribution can approximate the limit with arbitrary precision. Despite
the hypothetical element in the formulation, the ‘frequentist’ interpretation of probability
follows basic human common sense; the probability distribution is interpreted as the long
run average. The ‘long run average’ interpretation assumes prior knowledge; when an
agent like Robbie is to compute probability for its actions, it cannot be instructed to wait
for the result in an inﬁnite outcome of experiments and, indeed, it cannot run in ‘real
time’ if it is expected to wait for a large number of outcomes.
Once the existence of a probability measure p over (, A) has been established,
which may be interpreted in the sense of ‘long run averages’, it is then a matter of
computation to prove that the parameter estimates ˆθn based on n observations converge
with probability 1, provided a sensible estimating procedure is used, to a parameter
value θ.
As discussed earlier, the Bayesian approach takes the view that since the parameter is
unknown, it is a random variable as far as You are concerned. A probability distribution,
known as the prior distribution, is put over the parameter space, based on a prior assess-
ment of where the parameter may lie. One then carries out the experiment and, using the
data available, which is necessarily a ﬁnite number of data, one uses the Bayes rule to
compute the posterior distribution in Equation (1.13), which is the updated probability
distribution over the parameter space.
The posterior distribution over the parameter space is then used to compute the
probability distribution for future events, based on past experience. Unlike the classical
approach, this is an exact distribution, but it contains a subjective element. The subjective
element is described by the prior distribution.
In Bayesian statistics, the computation of the posterior distribution usually requires
numerical methods, and Markov chain Monte Carlo methods seem to be the most efﬁcient.
This technique is ‘frequentist’, in the sense that it relies upon an arbitrarily large supply of
independent random numbers to obtain the desired precision. From an engineering point
of view, there are efﬁcient pseudo-random number generators that supply arbitrarily large
sequences of ‘random’ numbers of very good quality. That is, there are tests available
to show whether a sequence ‘behaves’ like an observation of a sequence of suitable
independent random numbers.
Both approaches to statistical inference have an arbitrary element. For the classical
approach, one sees this in the choice of sample space. The sample space is, to use
H. Jeffreys’ [24] vivid description, ‘the class of observations that might have been
obtained, but weren’t’. For some experiments, the sample space is a clear and well

BAYESIAN STATISTICAL INFERENCE
19
deﬁned object, but for others, there is an arbitrary element in the choice of the sample
space. For example, an experiment may be set up with n plants, but some of the plants
may die before the results are established.
Alternative hypotheses
There is no distinction within the Bayesian approach between
the various values of the parameter except in the prior π(θ). The view is one of contrast
between various values of θ. Consider the case where the parameter space consists of
just two values, (θ0, θ1). Dropping subscripts where they are clearly implied, Bayes’ rule
for data x gives
π(θ0|x) = p(x|θ0)π(θ0)
p(x)
and
π(θ1|x) = p(x|θ1)π(θ1)
p(x)
.
It follows that
π(θ0|x)
π(θ1|x) = p(x|θ0)π(θ0)
p(x|θ1)π(θ1).
(1.15)
The likelihood ratiofor two different parameter values is the ratio of the likelihood func-
tions for these parameter values; denoting the likelihood ratio by LR,
LR(θ0, θ1; x) = p(x|θ0)
p(x|θ1).
The prior odds ratio is simply the ratio π(θ0)/π(θ1) and the posterior odds ratio is
simply the ratio π(θ0|x)/π(θ1|x). An odds ratio of greater than 1 indicates support for
the parameter value in the numerator.
Equation (1.15) may be rewritten as
posterior odds
=
LR × prior odds.
The data affect the change of assessment of probabilities through the likelihood ratio,
comparing the probabilities of data on θ0 and θ1. This is in contrast with a sampling
theory, or tail area signiﬁcance test, where only the null hypothesis (say θ0) is considered
by the user of the test.
In ‘classical’ statistics, statements about parameters may be made through conﬁdence
intervals. It is important to note that a conﬁdence interval for θ is not a probability
statement about θ, because in classical statistics θ is not a random variable. It is a ﬁxed,
though unknown, value. The conﬁdence interval is derived from probability statements
about x the observation, namely from p(x|θ).
There is no axiomatic system that leads to conﬁdence measures, while the axioms of
probability are well deﬁned. Operations strictly in accord with the calculus of probability
give coherent conclusions. Ideas outside the probability calculus may give anomalies.
The next two sections give a detailed examination of two probability distributions
that are often central to the analysis of Bayesian networks. Section 1.8 discusses binary
variables, while Section 1.9 discusses multinomial variables. The distributions discussed
in Section 1.8 are a useful special case of those discussed in Section 1.9.

20
GRAPHICAL MODELS AND PROBABILISTIC REASONING
1.8
Tossing a thumb-tack
The discussion of the thumb-tack is taken from D. Heckerman [25].
If a thumb-tack is thrown in the air, it will come to rest either on its point (0) or on
its head (1). Suppose the thumb-tack is ﬂipped n times, making sure that the physical
properties of the thumb-tack and the conditions under which it is ﬂipped remain stable
over time. Let x(n) denote the sequence of outcomes
x(n) = (x1, . . . , xn)t.
Each trial is a Bernoulli trial with probability θ of success (obtaining a 1). This is
denoted by
Xi ∼Be(θ),
i = 1, . . . , n.
Using the Bayesian approach, the parameter θ is be regarded as the outcome of a random
variable, which is denoted by . The outcomes are conditionally independent, given θ.
This is denoted by
Xi ⊥Xj|,
i ̸= j.
When  = θ is given, the random variables X1, . . . , Xn are independent, so that
pX(n)(x(n)|θ) =
n

l=1
θxl(1 −θ)1−xl = θk(1 −θ)n−k
where k = n
l=1 xl.
The problem is to estimate θ, ﬁnding the value that is best for x(n). The Bayesian
approach is, starting with a prior density π(.) over the parameter space ˜ = [0, 1], to
ﬁnd the posterior density π|X(n)(.|x(n)).
π|X(n)(θ|x(n)) =
pX(n)|(x(n)|θ)π(θ)
pX(n)(x(n))
=
pX(n)|(x(n)|θ)π(θ)

pX(n)|(x(n)|φ)π(φ)dφ .
Let π be the uniform density on [0, 1]. This represents that initially You have no
preference concerning θ; all values are equally plausible.4 The choice of prior may seem
arbitrary, but following the computations below, it should be clear that, from a large class
of priors, the ﬁnal answer does not depend much on the choice of prior if the thumb-tack
is thrown a large number of times.
4 As previously stated, the prior distribution contains the ‘ad hoc’ element. The results obtained from any
statistical analysis are only reliable if there is sufﬁcient data so that any inference will be robust under a rather
general choice of prior.
There are well known difﬁculties with the statement that a uniform prior represents no preference con-
cerning the value of θ. If the prior density for  is uniform, then the prior density of 2 will not be uniform,
so ‘no preference’ for values of  indicates that there is a distinct preference among possible initial values
of 2. If π1(x) = 1 for 0 < x < 1 is the density function for  and π2 is the density function for 2, then
π2(x) =
1
2x1/2 for 0 < x < 1.

TOSSING A THUMB-TACK
21
With the uniform prior,
 1
0
pX(n)|(x(n)|θ)π(θ)dθ =
 1
0
θk(1 −θ)n−kdθ = k!(n −k)!
(n + 1)! .
(1.16)
This may be computed using integration by parts, as follows. Set
In,k =
 1
0
θk(1 −θ)n−kdθ,
then
In,0 =
 1
0
(1 −θ)ndθ =
1
n + 1.
Using integration by parts,
In,k =

−θk(1 −θ)n−k+1
n −k + 1
1
θ=0
+
k
n −k + 1In,k−1 =
k
n −k + 1In,k−1.
From this,
In,k =
k!
n(n −1) . . . (n −k + 1)
1
(n + 1) = k!(n −k)!
(n + 1)! .
This is an example of the Beta integral. The posterior distribution is therefore a Beta
density
π|X(n)(θ|x(n)) =



(n+1)!
k!(n−k)!θk(1 −θ)n−k
0 ≤θ ≤1
0
otherwise.
(1.17)
It should be apparent that, in this case, there would have been tremendous difﬁculties
carrying out the integral if the prior had been anything other than the uniform, or a member
of the Beta family. The computational aspects are, or were, prior to the development of
Markov chain Monte Carlo (McMC) methods [26], the main drawback to the Bayesian
approach.
The Beta distribution is not restricted to integer values; the Euler gamma function is
necessary to extend the deﬁnition to positive real numbers.
Deﬁnition 1.10 (Euler Gamma function) The Euler Gamma function (α) : (0, +∞) →
(0, +∞) is deﬁned as
(α) =
 ∞
0
xα−1e−xdx.
(1.18)
The Euler Gamma function satisﬁes the following properties.
Lemma 1.1 For all α > 0, (α + 1) = α(α). If n is an integer satisfying n ≥1, then
(n) = (n −1)!

22
GRAPHICAL MODELS AND PROBABILISTIC REASONING
Proof of Lemma 1.1 Note that (1) =
 ∞
0 e−xdx = 1. For all α > 0, integration by
parts gives
(α + 1) =
 ∞
0
xαe−xdx = α(α).
(1.19)
The result follows directly.
□
Deﬁnition 1.11 (Beta Density) The Beta density Beta(α, β) with parameters α > 0 and
β > 0 is deﬁned as the function
ψ(t) =

(α+β)
(α)(β)tα−1(1 −t)β−1
t ∈[0, 1]
0
t ̸∈[0, 1]
(1.20)
The following results show that the Beta density is a probability density function for all
real α > 0 and β > 0.
Lemma 1.2 Set
B(α, β) =
 1
0
tα−1(1 −t)β−1dt.
Then
B(α, β) = (α)(β)
(α + β) .
Proof of Lemma 1.2 Directly from the deﬁnition of the Gamma function, using the
substitutions u = a2 and v = b2 and, at the end of the argument cos2 θ = t so that
dt
dθ = −2 cos θ sin θ,
(α)(β) =
 ∞
0
 ∞
0
e−uuα−1e−vvβ−1dudv
= 4
 ∞
0
 ∞
0
e−(a2+b2)a2(α−1)b2(β−1)abdadb
=
 ∞
−∞
 ∞
−∞
e−(a2+b2)|a|2α−1|b|2β−1dadb
=
 2π
0
 ∞
0
e−r2r2(α+β)−2| cos θ|2α−1| sin θ|2β−1rdrdθ
= 1
2
 2π
0
| cos θ|2α−1| sin θ|2β−1dθ
  ∞
0
e−uu(α+β)−1du
=

2
 π/2
0
(cos θ)2(α−1)(sin θ)2(β−1) cos θ sin θdθ

(α + β)
=
 1
0
tα−1(1 −t)β−1dt

(α + β)
= B(α, β)(α + β).
The result follows directly.
□

TOSSING A THUMB-TACK
23
Corollary 1.1 Let ψ
denote the Beta density, deﬁned in Equation (1.20), then
 1
0 ψ(θ)dθ = 1.
Proof of Corollary 1.1 This is a direct consequence of Lemma 1.2.
□
It follows that, for binomial sampling, updating may be carried out very easily for any
prior distribution within the Beta family. Suppose the prior distribution π0 is the B(α, β)
density function, n trials are observed, with k taking the value 1 and n −k taking the
value 0. Then
π|X(n)(θ|x(n)) =
pX(n)|(x(n)|θ)π(θ)
pX(n)(x(n))
=
(α + β)
(α)(β)pX(n)(x(n))θα+k−1(1 −θ)β+n−k−1 = cθα+k−1(1 −θ)β+n−k−1.
Since
 1
0 π|X(n)(θ|x(n))dθ = 1, it follows from Lemma 1.2, that
π|X(n)(θ|x(n)) =

(α+β+n)
(α+k)(β+n−k)θα+k(1 −θ)β+n−k
θ ∈(0, 1)
0
θ ̸∈(0, 1).
so that π|X(n)(θ|x(n)) is a B(α + k, β + n −k) density.
□
Recall the deﬁnition of the maximum likelihood estimate: it is the value of θ that
maximizes p(x(n)|θ) = θk(1 −θ)n−k. It is well known that
ˆθMLE

x(n)

= k
n.
The same pattern of thought can be applied to maximize the posterior density.
Deﬁnition 1.12 (Maximum Posterior Estimate) The maximum posterior estimate, ˆθMAP ,
is the value of θ which maximizes the posterior density π|x(n)(θ|x(n)).
When the posterior density is B(k + α, n −k + β), an easy computation gives
ˆθMAP =
α + k
α + β + n.
Note that when the prior density is uniform, as in the case above, the MAP and MLE are
exactly the same. The parameter, of course, is not an end in itself. The parameter ought
to be regarded as a means to computing the predictive probability. The posterior is used
to compute this; c.f. (1.14) above.
The predictive probability for the next toss
Recall that the ‘parameter’ is, in general,
an artiﬁcial introduction, to help compute pXn+1|X(n)(xn+1|x(n)). Suppose that π(θ|x(n))
has a B(α + k, β + n −k) distribution. The predictive probability for the next toss, for
a = 0 or 1, is given by
pXn+1|X(n)(a|x(n)) =
 1
0
pXn+1(a|θ)π|X(n)(θ|x(n))dθ.

24
GRAPHICAL MODELS AND PROBABILISTIC REASONING
Since pXn+1(1|θ) = θ, it follows (using equation (1.19)) that
pXn+1|X(n)(1|x(n)) =
(α + β + n)
(α + k)(β + n −k)
 1
0
θ(α+k+1)(1 −θ)β+n−kdθ
=
(α + β + n)
(α + k)(β + n −k)
(α + k + 1)(β + n −k)
(α + β + n + 1)
=
α + k
α + β + n.
In particular, note that the uniform prior, π0(θ) = 1 for θ ∈(0, 1), is the B(1, 1) density
function, so that for binomial sampling with a uniform prior, the predictive probability is
pXn+1|X(n)(1|x(n)) = k + 1
n + 2;
pXn+1|X(n)(0|x(n)) = n + 1 −k
n + 2
.
(1.21)
This distribution, or more precisely k+1
n+2, is known as the Laplace rule of succession. A
combinatorial derivation for it is given in [27].
Reconciling subjective predictive probabilities
The example of agreeing to disagree,
referred to in the preceding, is due to R.J. Aumann, in [18]. Suppose two agents,
Robbieα and Robbieβ, both toss a thumb-tack once without communicating the outcome
to each other. Both Robbieα and Robbieβ have the same uniform prior on θ. Suppose
Robbieα and Robbieβ communicate the value of their respective predictive (posterior)
probabilities as
p({Xn+1 = 1}|Robbieα) = 2
3;
p({Xn+1 = 1}|Robbieβ) = 1
3.
Note that in the conditional probabilities above Robbieα and Robbieβ actually refer to
respective states of knowledge. Now, since both the number of tosses by each agent and
the predictive probabilities held by the two agents is their common knowledge, they can
revise their opinions by (1.21) to
p({Xn+1 = 1}|Robbieα, Robbieβ) = 1 + 1
2 + 2 = 1
2.
This holds as Robbieα and Robbieβ deduce by (1.21) that exactly one outcome of the
two tosses was 1 (and the other was 0). The revision would not hold if the number of
tosses was not common knowledge.
1.9
Multinomial sampling and the Dirichlet integral
Consider the case of multinomial sampling, where an experiment can take one of k
outcomes, labelled C1, . . . , Ck. Suppose that p(X = Cj) = θj, so that θ1 + . . . + θk = 1.

MULTINOMIAL SAMPLING AND THE DIRICHLET INTEGRAL
25
Consider n independent trials, X1, . . . , Xn. The notation 1A, to denote the indicator
function of a set A, will be used; that is
1A(x) =
 1
x ∈A
0
x ̸∈A.
Let 1Ci(x) = 1 if x = Ci and 0 otherwise. Set
Yi =
n

j=1
1Ci(Xj).
Then Yi denotes the number of trials that result in outcome Ci. Note that
Y1 + . . . + Yk = n.
Then (Y1, . . . , Yk) is said to have a multinomial distribution and
pY1,...,Yk(x1, . . . , xk) =
n!
x1!x2! . . . xk−1!xk!θx1
1 . . . θxk
k ,
where the expression in front of the θx1
1 . . . θxk
k
is the multinomial coefﬁcient.
In the Bayesian approach, a prior distribution is put over θ1, . . . , θk. Then, using the
observations, this is updated using Bayes’ rule to a posterior probability distribution over
θ1, . . . , θk.
A particularly convenient family of distributions to use is the Dirichlet family, deﬁned
as follows.
Deﬁnition 1.13 (Dirichlet Density) The Dirichlet density Dir(a1, . . . , ak) is the function
π(θ1, . . . , θk) =



(a1+...+ak)
	k
j=1 (ak) (	k
j=1 θ
aj −1
j
)
θj ≥0, k
j=1 θj = 1,
0
otherwise,
(1.22)
where  denotes the Euler Gamma function, given in Deﬁnition 1.10. The parameters
(a1, . . . , ak) are all strictly positive and are known as hyper parameters.
This density, and integration with respect to this density function, are to be understood
in the following sense. Since θk = 1 −k−1
j=1 θj, it follows that π may be written as
π(θ1, . . . , θk) = ˜π(θ1, . . . , θk−1), where
˜π(θ1, . . . , θk−1)
=



(a1+...+ak)
	k
j=1 (ak)
	k−1
j=1 θ
aj −1
j
 
1 −k−1
j=1 θj
ak−1
θj ≥0, k−1
j=1 θj ≤1,
0
otherwise.
(1.23)
Clearly, when k = 2, this reduces to the Beta density. The following results show that
the Dirichlet density is a probability density function.

26
GRAPHICAL MODELS AND PROBABILISTIC REASONING
Lemma 1.3 Set
D(a1, . . . , ak) =
 1
0
 1−x1
0
 1−(x1+x2)
0
. . .
 1−k−2
j=1 xj
0


k−1

j=1
x
aj −1
j



1 −
k−1

j=1
xj


ak−1
dxk−1 . . . dx1.
Then
D(a1, . . . , ak) =
	n
j=1 (aj)

k
j=1 aj
.
Proof of Lemma 1.3 Straight from the deﬁnition of the Euler Gamma function, using
the substitutions x2
j = uj,
n

j=1
(aj) =
 ∞
0
. . .
 ∞
0
e−k
j=1 uj
k
j=1
u
aj −1
j
du1 . . . duk
= 2k
 ∞
0
. . .
 ∞
0
e−k
j=1 x2
j
k
j=1
x
2aj −1
j
dx1 . . . dxk
=
 ∞
−∞
. . .
 ∞
−∞
e−k
j=1 x2
j
k
j=1
|xj|2aj−1dx1 . . . dxk.
Now
let
r =
k
j=1 x2
j
and
zj = xj
r
for
1 ≤j ≤k −1.
Using
xj = rzj
for
j = 1, . . . , k −1 and xk = r

1 −k−1
j=1 z2
j, the computation of the Jacobian easy and
is left as an exercise:
J((x1, . . . , xk) →(r, z1, . . . , zk−1)) =
rk−1

1 −k−1
j=1 z2
j
.
Then
n

j=1
(aj) =
 ∞
0
e−r2r2(k
j=1 aj )−krk−1dr
×
 1
−1
 1−z2
1
−(1−z2
1)
. . .
 1−k−2
j=1 z2
j
−(1−k−2
j=1 z2
j )


k−1

j=1
z
2aj −1
j



1 −
k−1

j=1
z2
j


ak−1/2
×
1

1 −k−1
j=1 z2
j
k−1

j=1
dzj

MULTINOMIAL SAMPLING AND THE DIRICHLET INTEGRAL
27
= 


k

j=1
aj


×
 1
0
 1−z2
1
0
. . .
 1−k−2
j=1 z2
j
0


k−1

j=1
z
2(aj −1)
j



1 −
k−1

j=1
z2
j


ak−1 k−1

j=1
2zjdzj
= 


k

j=1
aj

D(a1, . . . , ak)
and the result follows.
□
Theorem 1.1 The function ˜π(θ1, . . . , θk−1) deﬁned by Equation (1.23) satisﬁes
 1
0
 1−θ1
0
. . .
 1−k−2
j=1 θj
0
˜π(θ1, . . . , θk−1)dθk−1 . . . dθ1 = 1,
hence the Dirichlet density (Deﬁnition 1.13) is a well deﬁned probability density function.
Proof This follows directly from the lemma.
□
Properties of the Dirichlet density
Theorem 1.1 shows that the Dirichlet density is a
probability density function.
Another very important property is that the Dirichlet densities Dir(a1, . . . , ak) :
a1 > 0, . . . , ak > 0 form a family of distributions that is closed under sampling.
Consider a prior distribution π ∼Dir(a1, . . . , ak) and suppose that an observation
x := (x1, . . . , xk) is made on Y := (Y1, . . . , Yk) based on n independent trials (i.e.
x1 + . . . + xk = n). Let π|Y denote the posterior distribution. Then, using Bayes’ rule,
π|Y (θ1, . . . , θk) = π(θ1, . . . , θk−1)pY (x|θ1, . . . , θk)
pY(x)
.
It follows that
π|Y(θ1, . . . , θk) =
1
pY(x)
n!
x1!x2! . . . xk−1!xk!θa1+x1−1
1
. . . θak+xk−1
k
,
where θk = 1 −k−1
j=1 θj.
Since the posterior density is a probability density, belonging to the Dirichlet family,
it follows that the constant
1
pY(x)
n!
x1!x2! . . . xk−1!xk! = (a1 + · · · + ak + x1 + · · · + xk)
	k
j=1 (aj + xj)
and hence that
π|Y (θ1, . . . , θk) ∼Dir(a1 + x1, . . . , ak + xk).
The results in this section were perhaps ﬁrst found by G. Lidstone [28].

28
GRAPHICAL MODELS AND PROBABILISTIC REASONING
Later in the text, the Dirichlet density will be written exclusively as a function of
k variables, π(θ1, . . . , θk), where there are k −1 independent variables and θk = 1 −
k−1
j=1 θj.
A question is how to select the hyper parameters a1, . . . , ak for the prior distribution.
The choice of a1 = . . . = ak = 1
k was suggested by W. Perks in [29].
Deﬁnition 1.14 (Conjugate Prior) A prior distribution from a family that is closed under
sampling is known as a conjugate prior.
In [30], I.J. Good proved that exchangeability and sufﬁcientness of samples implied that
the prior is necessarily Dirichlet, if k > 2. The notion of sufﬁcientness was originally
deﬁned by W.E. Johnson and I.J. Good. Loosely speaking, it means that the conditional
probability of seeing case i of k possible in the next sample given n past samples, depends
only on n, the number of times you have seen i in the past and NOT on the other cases.
Notes
Full accounts of the coherence argument may be found, for example, in [4], [31]
and [32]. An introduction to inductive logic is given in [33].
The monograph [34] includes a thorough presentation of the topics of statistical infer-
ence and Bayesian machine learning. The papers [13] and [16] argue for subjective
probability as the appropriate inference and language procedures for artiﬁcial intelligence
agents, see also [14]. The book [35] provides a clear introduction to the application of
Bayesian methods in artiﬁcial intelligence.
The work [36] by Thomas Bayes (1702–1761) and Richard Price was published
posthumously in 1763. This paper makes difﬁcult reading for a modern mathematician.
Consequently, there is a considerable literature investigating the question of what Bayes
actually proved, see, e.g. [22, 37–39] and the references therein. There is, however, a
wide consensus that [36] does contain Equation (1.17).
For this, Bayes deals with billiard balls. Suppose You throw one billiard ball o (orange)
on a square table (e.g. a billiard table without pockets) and measure the shortest distance
from the side of the table, when the side of the table is scaled in size to 1. Let this value
be denoted by p. Then You throw n balls W (white) on the table and note the number
of white balls, say k, to the left of the orange ball. Then it is understood that Bayes
computed the distribution of p given k given by Equation (1.17).
In this setting the uniform prior distribution on p is based on a physical understanding
that is veriﬁable by repeated experimentation.
There is even the question of whether Bayes was the ﬁrst to discover the results
attributed to him. This is discussed in [40]. Another up-to-date report on the life and
thinking of the Reverend Thomas Bayes, by D.R. Bellhouse [41], also discusses the
question of whether he was the ﬁrst to prove these results. The author has discovered
some previously unknown documents. The paper points out that the canonical picture of
Bayes is not proved to be an image of him.5
An alternative procedure on the billiard table is that n + 1 balls W are thrown on the
table. One of them is then selected at random to play the role of the orange ball, and k,
5 see http://www-history.mcs.st-andrews.ac.uk/PictDisplay/Bayes.html.

MULTINOMIAL SAMPLING AND THE DIRICHLET INTEGRAL
29
the number of balls to the left of the orange ball, is counted. Then You have a uniform
distribution
1
n+1 on the values of k.
It has been argued that Bayes demonstrated that a prior density π(θ) satisfying the
equality
n!
k!(n −k)!
 1
0
θk(1 −θ)n−kπ(θ)dθ =
1
n + 1
(1.24)
for all 0 ≤k ≤n and all n must be the uniform density. It may be checked rather easily
using Equation (1.16) that the uniform density indeed satisﬁes this equality. F.H. Murray
in [42] observed that Equation (1.24) implies for k = n that
 1
0
θnπ(θ)dθ =
1
n + 1,
(1.25)
which means that all the moments of π(θ) are given. Murray then went on to show that
these moments determine a unique distribution, which is in fact the uniform distribution.6
The probability in Equation (1.24) is a uniform distribution on the number of successes
in n Bernoulli trials with an unknown parameter. Hence Bayes (or Murray) has shown that
the uniform distribution on the number of successes is equivalent to the uniform density on
the probability of success. But this probability on the number of successes is a predictive
probability on observables. This understanding of the Bayesian inference due to Thomas
Bayes is different from many standard recapitulations of it, as pointed out in [39].
The ultimate question raised by reading of [36] is, ‘what is it all about?’. In other
words, what was the problem that Bayes was actually dealing with?
It is hardly credible that Bayes, a clergyman, should have studied this as a mere
curious speculation, and even less that scoring at a billiard room should have been at the
forefront of his mind. Richard Price writes in [36],
. . . the problem . . . mentioned [is] necessary to be solved in order to provide
a sure foundation for all our reasoning concerning past facts, and what is
likely to be hereafter . . .
For a layman in the history of philosophy the argument in [37] and [43] may carry a
convincing power: Bayes and Price developed an inductive logic as a response to the
critical and, in particular, anti-clerical objections to induction, causation and miracles
advanced by David Hume [3] in his book of 1748; the famous philosopher and scholar
was a contemporary of Bayes and Price.
Further evidence that this consideration may have prompted Bayes to develop a math-
ematical framework for inductive logic is seen from his theological interests. In 1731,
he published the following paper: ‘Divine Benevolence, or an Attempt to Prove That
the Principal End of the Divine Providence and Government is the Happiness of His
Creatures’.
6 The moment problem is a classic problem; whether or not the moments of a distribution uniquely char-
acterize the distribution. The technique usually employed is to check whether the Carlemann conditions are
satisﬁed. For this problem, Murray showed directly that the moments uniquely determined the distribution.

30
GRAPHICAL MODELS AND PROBABILISTIC REASONING
The models that were later to be called Bayesian networks were introduced into
artiﬁcial intelligence by J. Pearl, in the article [44]. Within the artiﬁcial intelligence
literature, this is a seminal article, which concludes with the following statement: The
paper demonstrates that the centuries-old Bayes formula still retains its potency for serving
as the basic belief revising rule in large, multi hypotheses, inference systems.

GRAPHICAL MODELS AND PROBABILISTIC REASONING
31
1.10
Exercises: Probabilistic theories of causality, Bayes’
rule, multinomial sampling and the Dirichlet density
1. This exercise considers the statistical notion of association due to G.U. Yule, who
used it in a classical statistical study to demonstrate the positive effect of innoculation
against cholera.
Here the association between two events A and B, denoted by α (A, B), is deﬁned as
α (A, B) = p (A ∩B) −p(A) · p(B).
(a) Show that
α (A, B) = −α

A, Bc
,
where Bc is the complement of B.
(b) Show that
α (A, B) = α

Ac, Bc
.
Comment: Association is clearly symmetric. That is, for any two events A and
B, α (A, B) = α (B, A). It does not seem reasonable to claim that a decrease in
cholera causes an increase in the number of innoculations. In this case it is common
sense to conclude that there is an underlying causal relation, where innoculation
(say B) causes a decreased prevalence of cholera (A), although without a controlled
experiment, it is not possible to conclude that there is not a hidden factor C that both
causes cholera and makes innoculation less likely.
2. On a probabilistic theory of causality Following the theory of causality due to P.
Suppes [19] an event Bs is deﬁned as a prima facie cause7 of the event At if and
only if the following three statements hold:
• s < t,
• p (Bs) > 0
• p (At | Bs) > p (At).
Here the parameter is considered as a time parameter, and s < t means that Bs occurs
prior to At; a cause occurs before an effect.
An event Bs is deﬁned as a prima facie negative cause of an event At [19] if and
only if the following three statements hold:
• s < t,
• p (Bs) > 0
7 Prima facie is a Latin expression meaning ‘on its ﬁrst appearance’, or ‘by ﬁrst instance’. Literally the
phrase translates as ﬁrst face, ‘prima’ ﬁrst, ‘facie’ face. It is used in modern legal English to signify that on
ﬁrst examination, a matter appears to be self-evident from the facts. In common law jurisdictions, ‘prima facie’
denotes evidence that (unless rebutted) would be sufﬁcient to prove a particular proposition or fact.

32
EXERCISES: PROBABILISTIC THEORIES OF CAUSALITY AND BAYES’ RULE
• p (At | Bs) < p (At).
Intuitively, a negative cause is an event that prevents another event from happening.
For example, the theory and practice of preventive medicine focuses on certain types
of negative causation. In the problems the indices s, t are dropped for ease of writing.
(a) Show that if Bc is a prima facie negative cause of A, then B is a prima facie
cause of A.
(b) Show that if B is a prima facie cause of A, then Bc is a prima facie cause of Ac.
Also, show that if B is a prima facie negative cause of A, then Bc is a prima
facie negative cause of Ac.
(c) Recall the deﬁnition of association from Exercise 1. Show that if B is a prima
facie cause of A, then α(A, B) > 0 and that if B is a prima facie negative cause
of A, then α(A, B) < 0.
3. On odds and the weight of evidence
Let p be a probability distribution over a
space X. The odds of an event A ⊆X given B ⊆X under p, denoted by Op (A | B),
is deﬁned as
Op (A | B) = p (A | B)
p (Ac | B).
(1.26)
The odds ration will play an important role in Chapter 7, which considers sensitivity
analysis. Next, the weight of evidence E in favour of an event A given B, denoted
by W (A : E | B), is deﬁned as
W (A : E | B) = log Op (A | B ∩E)
Op (A | B)
.
(1.27)
Show that if p(E ∩Ac ∩B) > 0, then
W (A : E | B) = log p (E | A ∩B)
p (E | Ac ∩B).
(1.28)
4. On a generalized odds and the weight of evidence
Let p denote a probability
distribution over a space X and let H1 ⊆X, H2 ⊆X, G ⊆X and E ⊆X. The odds
of H1 compared to H2 given G, denoted by Op (H1/H2 | G), is deﬁned as
Op (H1/H2 | G) = p (H1 | G)
p (H2 | G).
(1.29)
The generalized weight of evidence is deﬁned by
W (H1/H2 : E | G) = log Op (H1/H2 | G ∩E)
Op (H1/H2 | G)
.
(1.30)
Show that if p(H1 ∩G ∩E) > 0 and p(H2 ∩G ∩E) > 0 then
W (H1/H2 : E | B) = log p (E | H1 ∩G)
p (E | H2 ∩G).
(1.31)
Clearly this is just a log likelihood ratio and these notions are another expression for
posterior odds = likelihood ratio × prior odds.

GRAPHICAL MODELS AND PROBABILISTIC REASONING
33
5. In [45], I.J. Good discusses the causes of an event that are necessary and sufﬁcient
from probabilistic view point. For example, let E is the event of being hit by a
car and F the event of going for a walk. Then F tends to be a necessary cause
of E. The quantitites Qsuf (E : F | U) and Qnec (E : F | U) are deﬁned to measure
the probabilistic tendency of an event F to be a sufﬁcient and/or necessary cause,
respectively, for an event E with background information U, by the weights of
evidence discussed in the preceding exercise. They are deﬁned respectively by
Qsuf (E : F | U) = W

F c : Ec | U

(1.32)
and
Qnec (E : F | U) = W (F : E | U) .
(1.33)
In view of the preceding deﬁnitions, Qsuf may be read as the weight of evidence
against F provided by non-occurrence of E. Similarly, Qnec is the the weight of
evidence in favour of F given by occurrence of E. Both quantities are computed, to
borrow a philosophical phrase, ‘given the state of universe U just before F occurred’.
(a) If p (Ec | F ∩U) > 0, show that
Qsuf (E : F | U) = log p (Ec | F c ∩U)
p (Ec | F ∩U) .
(1.34)
(b) If p (E | F c ∩U) > 0, show that
Qnec (E : F | U) = log p (E | F ∩U)
p (E | F c ∩U).
(1.35)
6. This exercise considers a few more properties of Qsuf and Qnec. Following Exercise
4 above, set
Qnec (E : F1/F2 | U) = W (F1/F2 : E | U)
which is the necessitivity of E of F1 against F2 and
Qsuf (E : F2/F1 | U) = W 
F1/F2 : Ec | U ,
which is the sufﬁcientivity of E of F1 against F2.
(a) Show that Qsuf (E : F | U) < 0, if and only if Qnec (E : F | U) < 0. Compare
with prima facie negative cause in Exercise 2.
(b) Show that
Qnec (E : F1/F2 | U) = Qsuf

Ec : F2/F1 | U

.
This is called a probabilistic contraposition.

34
EXERCISES: PROBABILISTIC THEORIES OF CAUSALITY AND BAYES’ RULE
(c) Show that
Qnec (E : F | U) = Qsuf

Ec : F c | U

.
This may interpreted along the following lines. Going for a walk F tends to be
a necessary cause for being hit by a vehicle E, whereas staying home tends to
be a sufﬁcient cause for not being hit by a vehicle. (Note that cars and aircraft
are known to have crashed into houses.) Both Qnec and Qsuf should have high
values in this case.
7. Let X = (X1, . . . , Xn)t be an exchangeable sample of Bernoulli trials and let T =
n
j=1 Xj. Show that there is a probability density function π such that
(a)
pT (t) =
 1
0
 n
t

θt(1 −θ)n−tπ(θ)dθ,
t = 0, 1, . . . , n
(b)
E[T ] = n
 1
0
θπ(θ)dθ.
You may use the result of DeFinetti.
8. Consider a sequence of n independent, identically distributed Bernoulli trials, with
unknown parameter θ, the ‘success’ probability. For a uniform prior over θ, show
that the posterior density for θ, if the sequence has k successes, is
π|x

θ | x

=

(n+1)!
k!(n−k)! · θk (1 −θ)n−k
0 ≤θ ≤1
0
elsewhere.
(1.36)
9. Consider the thumb-tack experiment and the conditional independence model for the
problem and the uniform prior density for θ. What is PXn+1|X(n)

head|x(n)

, where
x(n) denotes the outcome of the ﬁrst n throws?
10. Consider multinomial sampling, where θj is the probability that category j is
obtained, with prior density π(θ1, . . . , θL) is the Dirichlet prior Dir(αq1, . . . , αqL)
with L
j=1 qj = 1, deﬁned by
π(θ) =



(α)
	L
j=1 (αqj )
	L
j=1 θ
αqj −1
j
θ1 + . . . + θL = 1, 0 ≤θi ≤1
0
elsewhere.
Show that for multinomial sampling, with the Dirichlet prior, the posterior density
p|x

θ|x; α

is the Dirichlet density
Dir (n1 + αq1, . . . , nL + αqL) ,
which is shorthand for
π|X(n)

θ|x(n); αq

=
 (n + α)
	L
i=1  (αqi + ni)
L

i=1
θni+αqi−1
i
,
(1.37)
where q = (q1, . . . , qL).

GRAPHICAL MODELS AND PROBABILISTIC REASONING
35
11. A useful property of the Dirichlet density is that the predictive distribution of Xn+1
may be computed explicitly by integrating pXn+1| (. | θ) with respect to the posterior
distribution containing the stored experience x(n). Using the previous exercise, show
that
pXn+1|X(n)

xi | x(n)
 =

SL
θiπ

θ1, . . . , θL|x; αq

dθ1 . . . dθL = ni + αqi
n + α . (1.38)
12. Let  = (1, . . . , L) be a continuous random vector with Dir (α1, . . . , αL) distri-
bution. Compute Var (i).
13. Prove the Laplace rule of succession. Namely, let {X1, . . . , Xn+1} be independent,
identically distributed Bernoulli random variables, where pXi(1) = 1 −pXi(0) = θ
and θ ∼U(0, 1). Then the Laplace rule of succession states that
p({Xn+1 = 1}|{X1 + . . . + Xn = s}) = s + 1
n + 2.
14. Let V = (V1, . . . , VK) be a continuous random vector, with
V ∼Dir (a1, . . . , aK) ,
and set
Ui =
Vix−1
i
K
i=1 Vix−1
i
,
, i = 1, . . . , K,
where x = (x1, . . . , xK) is a vector of positive real numbers; that is, xi > 0 for each
i = 1, . . . , K. Show that U = (U1, . . . , UK) has density function

k
i=1 ai

	K
i=1 (ai)
K

i=1
uai−1
i

1
K
i=1 uixi
K
i=1 ai
K

i=1
xai
i .
This density is denoted
U ∼S

a, x

.
This is due to J.L. Savage [46]. Note that the Dirichlet density is obtained as a special
case when xi = c for i = 1, . . . , K.
15. The next two examples illustrate how the Savage distribution of the previous exercise
can arise in Bayesian analysis, for updating an objective distribution over the subjec-
tive assessments of a probability distribution by several different researchers, faced
with a common set of data. Consider several researchers studying an unknown quan-
tity X, where X can take values in {1, 2, . . . , K}. Each researcher has his own initial
assessment of the probability distribution V = (V1, . . . , VK) for the value that X
takes. That is, for a particular researcher,
Vi = pX (i) ,
i = 1, . . . , K.

36
EXERCISES: PROBABILISTIC THEORIES OF CAUSALITY AND BAYES’ RULE
It is assumed that
V ∼Dir (a1, . . . , aK) .
Each researcher observes the same set of data with the common likelihood function
li = p (data|{X = i}) ,
i = 1, . . . , K.
The coherent posterior probability of a researcher is
Ui = p ({X = i} | data) ,
i = 1, 2, . . . , K.
Let U = (U1, . . . , UK). Prove that
U ∼S

a, l−1
,
where a = (a1, . . . , aK) and l−1 =

l−1
1 , . . . , l−1
K

. This is due to J.M. Dickey [47].
16. Show that the family of distributions S

a, l−1
is closed under updating of the
opinion populations. In other words, if
V ∼S

a, z

,
before the data is considered, then
U ∼S

a, z × l−1
,
after the data update, where
z × l−1 =

z1l−1
1 , . . . , zKl−1
K

.

2
Conditional independence, graphs
and d-separation
2.1
Joint probabilities
Consider a random vector X = (X1, . . . , Xd), deﬁned on a state space X = X1 × . . . ×
Xd, where Xj is the state space for Xj, where Xj = {x(1)
j , . . . , x(jk)
j
} for j = 1, . . . , d.
In principle, the joint probability function pX1,...,Xd contains full information about the d
random variables X1, . . . , Xd. But, although mathematically complete, this is not usually
such a useful description in practice; the important features of the distribution may not be
immediately clear in a table that has 	d
j=1 kj elements, if this number is large. Further-
more, in many situations, the elementary building blocks will be low order conditional
probabilities, each deﬁned over small groups of variables.
Deﬁnition 2.1 (Independence) Two discrete random variables X and Y are independent
if and only if
pX,Y = pXpY.
A collection of d random variables {X1, . . . , Xd} is said to be jointly independent if for
any random vector (Xi1, . . . , Xim) where ij ̸= ik for all j ̸= k,
pXi1,...,Xim =
m

j=1
pXij .
Bayesian Networks: An Introduction
T. Koski, J. Noble
2009 John Wiley & Sons, Ltd

38
CONDITIONAL INDEPENDENCE, GRAPHS AND d-SEPARATION
In practice, dependence between variables, or independence, can often be detected or
understood even if the precise numerical values of the joint probability distribution are
unavailable. Likewise, the relationships of conditional dependence (for example, the con-
ditional distribution of X and Y given Z and the distribution of Z) often provide more
convenient basic building blocks than the joint probability function pX,Y,Z, since the
conditional probability distribution is often easier to assess.
In many cases, qualitative dependencies among variables may be asserted relatively
easily, before making numerical assignments for the relevant probabilities. This is simply
an assertion as to whether or not two sets of variables are conditionally independent given
another set of variables. Such a dependence structure may be modelled by a directed
acyclic graph, where the nodes of the graph represent random variables.
In many problems, the directed acyclic graph may be interpreted in the following
way: a directed edge between two variables may be used to indicate the modelling
assumption that there is a direct causal connection between the two variables, the cause
to effect relationship indicated by the direction of the arrow. Lack of any arrow indicates
that there is no direct causal relation between the variables. The dependence structure
between different variables in the network is described by the structure of the directed
acyclic graph which, under certain circumstances, may have a causal interpretation. This
leads to the notion of d-separation of variables (or directed separation), which will be
introduced later.
It will be shown that d-separation characterises the conditional independence state-
ments that can be inferred from a given DAG. A DAG where all conditional independence
statements may be inferred from d-separation ‘faithfully’ represents the probability distri-
bution. That is, when the representation is ‘faithful’, there are no artiﬁcial dependencies
that have to be considered simply through an unfortunate choice of parametrization. In sit-
uations where there is a causal structure between the variables, it can, in many situations,
be modelled by a faithful DAG. This idea is expanded in [48].
2.2
Conditional independence
Conditional independence (CI) is the key probabilistic notion in Bayesian networks. The
following gives a quick summary of some basic properties of CI.
Characterizations of CI
Let (X, Y, Z) be three discrete random vectors, with joint
probability function pX,Y,Z. Let XX, XY and XZ denote the state spaces for X, Y and Z
respectively. The vectors X and Y are said to be conditionally independent given Z if
for all (x, y, z) ∈XX × XY × XZ,
pX,Y,Z(x, y, z) = pX|Z(x|z)pY|Z(y|z)pZ(z).
This will be indicated by the notation
X ⊥Y|Z.
The notation X ⊥Y denotes that X and Y are independent; that is, pX,Y(x, y) =
pX(x)pY(y) for all (x, y) ∈XX × XY . This may be considered as X ⊥Y|φ, where φ
denotes the empty vector. Similarly, for a set V = {X1, . . . , Xd} of random variables, and

CONDITIONAL INDEPENDENCE
39
three subsets A ⊂V , B ⊂V , C ⊂V , the notation A ⊥B|C denotes that the variables
in A are independent of the variables in B once the variables in set C are instantiated.
A ⊥B means that the variables in A are independent of those in B and could be written
A ⊥B|φ, where φ denotes the empty set.
Theorem 2.1 The following are all equivalent to X ⊥Y|Z:
1) For all (x, y, z) ∈XZ × XY × XZ such that pY|Z(y|z) > 0 and pZ(z) > 0,
pX|Y,Z(x|y, z) = pX|Z(x|z).
2) Then there exists a function a : XX × XZ →[0, 1] such that for all (x, y, z) ∈
XX × XY × XZ satisfying pY|Z(y|z) > 0 and pZ(z) > 0,
pX|Y,Z(x|y, z) = a(x, z)
3) There exist functions a : XX × XZ →R and b : XY × XZ →R such that for all
(x, y, z) ∈XX × XY × XZ satisfying pZ(z) > 0,
pX,Y|Z(x, y|z) = a(x, z)b(y, z)
4) For all (x, y, z) ∈XX × XY × XZ such that pZ(z) > 0,
pX,Y,Z(x, y, z) =
pX,Z(x, z)pY,Z(y, z)
pZ(z)
.
5) There exist functions a : XX × XZ →R and b : XY × XZ →R such that
pX,Y,Z(x, y, z) = a(x, z)b(y, z).
Proof of Theorem 2.1
CI ⇒1) This is proved as follows: ﬁrstly,
pX,Y,Z(x, y, z) = pX|Y,Z(x|y, z)pY|Z(y|z)pZ(z).
Recall that CI is deﬁned as
pX,Y,Z(x, y, z) = pX|Z(x|z)pY|Z(y|z)pZ(z)
∀(x, y, z) ∈XX × XY × XZ.
By equating these two expressions, it follows that if CI holds, then for all (x, y, z) such
that pZ(z) > 0, either pY|Z(y|z) = 0 or pX|Z(x|z) = pX|Y,Z(x|y, z) so CI ⇒1).
1) ⇒2) The ﬁrst characterization of CI implies the second by taking a(x, z) =
pX|Z(x|z).
2) ⇒3) The second implies the third by taking
pX,Y|Z(x, y|z) = pX|Y,Z(x|y, z)pY|Z(y|z),
where, from 2), a(x, z) = pX|Y,Z(x|y, z). The result follows by taking b(y, z) =
pY|Z(y|z).

40
CONDITIONAL INDEPENDENCE, GRAPHS AND d-SEPARATION
3) ⇒4) The third implies the fourth as follows: assume there are two functions a and
b such that
pX,Y|Z(x, y|z) = a(x, z)b(y, z).
(2.1)
Set A(z) = 
x∈XX a(x, z) and B(z) = 
y∈XY b(y, z). By summing over XY on both
sides of Equation (2.1), it follows that
pX|Z(x|z) = B(z)a(x, z)
(2.2)
and by summing over XX on both sides of Equation (2.1), it follows that
pY|Z(y|z) = A(z)b(y, z).
It follows, from summing over XX on both sides of Equation (2.2), that B(z)A(z) = 1.
From this, it follows directly that
pX,Y|Z(x, y|z) = a(x, z)b(y, z) = B(z)a(x, z)A(z)b(y, z) = pX|Z(x|z)pY|Z(y|z).
This, incidentally, shows that 3) implies CI. If 3) holds, then since a(x, z)b(y, z) =
pX|Z(x|z)pY|Z(y|z), it follows that
pX,Y,Z(x, y, z) = pX,Y|Z(x, y|z)pZ(z)
= a(x, z)b(y, z)pZ(z) = pX|Z(x|z)pY|Z(y|z)pZ(z) =
pX,Z(x, z)pY,Z(y, z)
pZ(z)
,
and therefore 3) ⇒4) is proved.
4) ⇒5) This is proved by taking (for example) a(x, z) = pX|Z(x|z) and b(y, z) =
pY|Z(y|z)pZ(z).
5) ⇒CI This is proved as follows: 5) gives
pX,Y,Z(x, y, z) = pX|Y,Z(x|y, z)pY|Z(y|z)pZ(z) = a(x, z)b(y, z).
(2.3)
Set C(z) = 
x∈XX a(x, z) and D(z) = 
y∈XY b(y, z). It follows, by summing over XX
ﬁrst and then summing over XY in Equation (2.3), that C(z)D(z) = pZ(z). Set
˜a(x, z) = a(x, z)
A(z) pZ(z)
and
˜b(y, z) =
b(y, z)
B(z) .
It follows that
pX,Y,Z(x, y, z) = pX|Y,Z(x|y, z)pY|Z(y|z)pZ(z) = ˜a(x, z)˜b(y, z)
and summing over XX gives
pY|Z(y|z)pZ(z) = ˜b(y, z)pZ(z).

DIRECTED ACYCLIC GRAPHS AND d-SEPARATION
41
Therefore, for pZ(z) > 0,
˜b(y, z) = pY|Z(y|z). Similarly, it follows that ˜a(x, z) =
pX|Y,Z(x|y, z)pZ(z). For pZ(z) > 0, it follows that pX|Y,Z(x|y, z) = pX|Z(x|z), so that
˜a(x, z) = pX|Z(x|z)pZ(z) and
pX,Y,Z(x, y, z) = pX|Z(x|z)pY|Z(y|z)pZ(z)
thus proving CI. The proof of Theorem 2.1 is complete.
□
2.3
Directed acyclic graphs and d-separation
A graphical model is a representation of a collection of the components of a random
vector X = (X1, . . . , Xd) as nodes of a graph G = (V, E), where important aspects of
the conditional independence structure between the variables may be inferred from the
structure of the graph, and in some cases the whole conditional independence structure
is described by the graph.
2.3.1
Graphs
This section introduces some of the necessary graph theory. The remainder is presented
in Chapter 4.
Deﬁnition 2.2 (Graph, Simple Graph) A graph G = (V, E) consists of a ﬁnite set of nodes
V and an edge set E, where each edge is contained in V × V . The edge set therefore
consists of ordered pairs of nodes.
Let V = {α1, . . . , αd}. A graph is said to be simple if E does not contain any edges
of the form (αj, αj) (that is a loop from the node to itself) and any edge (αj, αk) ∈E
appears exactly once. That is, multiple edges are not permitted.
For any two distinct nodes α and β ∈V , the ordered pair (α, β) ∈E if and only if
there is a directed edge from α to β. An undirected edge will be denoted ⟨α, β⟩. In terms
of directed edges,
⟨α, β⟩∈E ⇔(α, β) ∈E
and
(α, β) ∈E.
For a simple graph that may contain both directed and undirected edges, the edge set E
may be decomposed as E = D ∪U, where D ∩U = φ, the empty set. The sets U and D
are deﬁned by
⟨α, β⟩∈U ⇔(α, β) ∈E
and
(β, α) ∈E.
(α, β) ∈D ⇔(α, β) ∈E
and
(β, α) ̸∈E.
For the deﬁnitions of ‘path’, ‘trail’ and ‘cycle’, an undirected edge will be considered as
a single edge.
All the graphs considered in this text will be simple graphs and the term ‘graph’ will
be used to mean ‘simple graph’. If (αi, αj) ∈D, this is denoted by an arrow going
from αi to αj. If ⟨αi, αj⟩∈U, this is denoted by an edge between the two variables αi
and αj.

42
CONDITIONAL INDEPENDENCE, GRAPHS AND d-SEPARATION
a1
a4
a6
a7
a5
a2
a3
Figure 2.1
Example of a graph illustrating Deﬁnition 2.3.
Figure 2.1 gives an example of a graph that will be used to illustrate the deﬁnitions
that follow. The node set is
V = {α1, α2, α3, α4, α5, α6, α7}
and the edge set is
E = {(α1, α2), (α1, α3), (α1, α4), (α2, α3), (α2, α5), (α3, α5), (α4, α6), (α4, α7)}.
The node α4, for example, has neighbours α1, α6, α7, since the edge set contains
(α1, α4), (α4, α6) and (α4, α7).
Deﬁnition 2.3 (Parent, Child, Directed and Undirected Neighbour, Family) Consider a
graph G = (V, E), where V = {α1, . . . , αd} and let E = D ∪U, where D is the set of
directed edges and U the set of undirected edges. These are deﬁned by
(α, β) ∈D ⇔(α, β) ∈E,
(β, α) ̸∈E
⟨α, β⟩∈U ⇔(α, β) ∈E,
(β, α) ∈E.
Let αj, αk ∈G. If (αj, αk) ∈D, then αk is referred to as a child of αj and αj as a parent
of αk.
For any node α ⊆V , the set of parents is deﬁned as
(α) = {β ∈V | (β, α) ∈D}
(2.4)
and the set of children is deﬁned as
Ch(α) = {β ∈V | (α, β) ∈D}.
(2.5)
For any subset A ⊆V , the set of parents of A is deﬁned as
(A) = ∪α∈A{β ∈V \A | (β, α) ∈D}.
(2.6)
The set of directed neighbours of a node α is deﬁned as
N(d)(α) = (α) ∪Ch(α)
and the set of undirected neighbours of α as
N(u)(α) = {β ∈V | ⟨α, β⟩∈U}.
(2.7)

DIRECTED ACYCLIC GRAPHS AND d-SEPARATION
43
For any subset A ⊆V , the set of undirected neighbours of A is deﬁned as
N(u)(A) = ∪α∈A{β ∈V \A | ⟨α, β⟩∈U}.
(2.8)
For a node α, the set of neighbours N(α) is deﬁned as
N(α) = N(u)(α) ∪N(d)(α).
The family of a node β is the set containing the node β together with its parents and
undirected neighbours. It is denoted:
F(β) = {β} ∪(β) ∪N(u)(β) = {family of β}.
When G is undirected, this reduces to F(β) = {β} ∪N(β).
When the variables have a clear indexing set, for example, the variables of the set
V = {α1, . . . , αd} are clearly indexed by the set ˜V = {1, . . . , d}, the notation j will
also be used to denote the parent set (αj) of variable αj. Similarly with children, family
and neighbour.
The notation α ∼β will be used to denote that α ∈N(β); namely, that α and β are
neighbours. Note that α ∈N(β) ⇒β ∈N(α).
For example, in Figure 2.1, (α1) = 1 = φ where φ denotes the empty set,
(α2) = 2 = {α1},
(α3) = 3 = {α2, α1},
(α4) = 4 = {α1},
(α5) = 5 =
{α2, α3}, (α6) = 6 = {α4} and (α7) = 7 = {α4}.
In this text, a directed edge (αj, αk) is indicated by a pointed arrow from αj to αk;
that is, from the parent to the child. In the graph in Figure 2.1, α4 has a single parent α1
and two children α6 and α7.
Deﬁnition 2.4 (Directed, Undirected Graph) If all edges of a graph are undirected, then
the graph G is said to be undirected. If all edges are directed, then the graph is said to
be directed. The undirected version of a graph G, denoted by ˜G, is obtained by replacing
the directed edges of G by undirected edges.
The graph in Figure 2.1 is a directed graph.
Deﬁnition 2.5 (Trail) Let G = (V, E) be a graph, where E = D ∪U; D ∩U = φ, D
denotes the directed edges and U the undirected edges. A trail τ between two nodes α ∈V
and β ∈V is a collection of nodes τ = (τ1, . . . , τm), where τi ∈V for each i = 1, . . . , m,
τ1 = α and τm = α and such that for each i = 1, . . . , m −1, τi ∼τi+1. That is, for each
i = 1, . . . , m −1, either (τi, τi+1) ∈D or (τi+1, τi) ∈D or ⟨τi, τi+1⟩∈U.
For example, in the graph in Figure 2.1, there is a trail τ = (α3, α1, α4, α7) between α3
and α7, since the edges (α1, α3), (α1, α4), (α4, α7) are contained in the edge set.
Deﬁnition 2.6 (Sub-graph, Induced Sub-graph) Let A ⊆V and EA ⊆E ∩A × A. Then
F = (A, EA) is a sub graph of G.
If A ⊂V and EA = E ∩A × A, then GA = (A, EA) is the sub-graph induced
by A.

44
CONDITIONAL INDEPENDENCE, GRAPHS AND d-SEPARATION
Note that in general it is possible for a sub-graph to contain the same nodes, but fewer
edges, but the sub-graph induced by the same node set will have the same edges.
Deﬁnition 2.7 (Connected Graph, Connected Component) A graph is said to be con-
nected if between any two nodes αj ∈V and αk ∈V there is a trail. A connected com-
ponent of a graph G = (V, E) is an induced sub-graph GA such that GA is connected and
such that if A ̸= V , then for any two nodes (α, β) ∈V × V such that α ∈A and β ∈V \A,
there is no trail between α and β.
It is clear that the graph in Figure 2.1 is connected.
Deﬁnition 2.8 (Path, Directed Path) Let G = (V, E) denote a simple graph, where E =
D ∪U. That is, D ∩U = φ, D denotes the directed edges and U denotes the undirected
edges. A path of length m from a node α to a node β is a sequence of distinct nodes
(τ0, . . . , τm) such that τ0 = α and τm = β such that (τi−1, τi) ∈E for each i = 1, . . . , m.
That is, for each i = 1, . . . , m, either (τi−1, τi) ∈D, or ⟨τi−1, τi⟩∈U.
The path is a directed path if (τi−1, τi) ∈D for each i = 1, . . . , m. That is, there are
no undirected edges along the directed path.
It follows that a trail in G is a sequence of nodes that form a path in the undirected
version ˜G.
Unlike a trail, a directed path (τ0, . . . , τm) requires that the directed edge (τi, τi+1) ∈
D for all i = 0, . . . , m −1. Therefore, in Figure 2.1, there is no path between α3 and
α7, although there is a trail between these two nodes.
Deﬁnition 2.9 (Descendant, Ancestor) Let G = (V, E) be a graph. A node α is a descen-
dant of a node β if and only if there is a directed path from α to β. A node γ is an ancestor
of a node α if and only if there is a directed path from γ to α.
Let E = U ∪D, where U denotes the undirected edges and D denotes the directed
edges. The set of descendants D(α) of a node α is deﬁned as
D(α) = {β ∈V | ∃τ = (τ0, . . . , τk) : τ0 = α, τk = β, (τj, τj+1) ∈D, j = 0, 1, . . . , k}.
(2.9)
The set of ancestors A(α) of a node α is deﬁned as
A(α) = {β ∈V | ∃τ = (τ0, . . . , τk) : τ0 = β, τk = α, (τj, τj+1) ∈D, j = 0, 1, . . . , k}.
(2.10)
In both cases, the paths are directed; they consist of directed edges only; they do not
contain undirected edges.
In Figure 2.1, all the nodes α2, α3, α4, α5, α6, α7 are descendants of α1, while α3 and α5
are the descendants of α2.
Deﬁnition 2.10 (Cycle) Let G = (V, E) be a graph. An m-cycle in G is a sequence of
distinct nodes
τ0, . . . , τm−1
such that τ0, . . . , τm−1, τ0 is a path (Deﬁnition 2.9).

DIRECTED ACYCLIC GRAPHS AND d-SEPARATION
45
Deﬁnition 2.11 (Directed Acyclic Graph (DAG)) A graph G = (V, E) is said to be a
directed acyclic graph if each edge is directed (that is, G is a simple graph such that
for each pair (α, β) ∈V × V , (α, β) ∈E ⇒(β, α) ̸∈E) and for any node α ∈V there
does not exist any set of distinct nodes τ1, . . . , τm such that α ̸= τi for all i = 1, . . . , m
and (α, τ1, . . . , τm, α) forms a directed path. That is, there are no m-cycles in G for any
m ≥1.
The graph in Figure 2.1 is a directed acyclic graph.
Deﬁnition 2.12 (Tree) A tree is a graph G = (V, E) that is connected and such that for
any node α ∈V , there is no trail between α and α and for any two nodes α and β in V
with α ̸= β, there is a unique trail. A leaf of a tree is a node that is connected to exactly
one other node.
The graph in Figure 2.1 is not a tree; τ = (α1, α2, α3, α1) is a trail from α1 to α1.
Figure 2.2 gives an example of a tree.
Deﬁnition 2.13 (Forest) A forest is a graph where all its connected components (Deﬁni-
tion 2.7) are trees.
This is illustrated in Figure 2.3. Each connected component in Figure 2.3 is a tree.
2.3.2
Directed acyclic graphs and probability distributions
Now consider a random vector X = (X1, . . . , Xd). Throughout the text, when the nodes
of the graph represent random variables, they will be labelled by random variables that
a1
a5
a6
a2
a3
a4
a7
Figure 2.2
Example of a tree.
a4
a6
a7
a5
a8
a9
a1
a2
a3
Figure 2.3
Example of a forest.

46
CONDITIONAL INDEPENDENCE, GRAPHS AND d-SEPARATION
they represent. That is, V = {X1, . . . , Xd} will denote the node set for the graph, which
is a set of random variables. A directed acyclic graph G = (V, E) may be used to model
assumptions that certain variables have direct causal relations on others. If a variable Xi
is considered to have a direct causal effect on variable Xj, then (Xi, Xj) ∈E. For a vari-
able Xi, i = {Xj|(Xj, Xi) ∈E}. In situations where immediate associations between
variables are considered to be causal, i, the parent set for variable Xi, is the complete
set of variables in the model whose values are considered to have a direct cause on the
value taken by Xi.
A variable can have any number of states. For example, (red, green, blue, brown)
(four states), or number of children in a family (0, 1, 2, 3, 4, 5, 6, > 6) (eight states). In
this text, attention is restricted to variables with a ﬁnite number of possible states. A
variable is in exactly one of its states, which may or may not be known.
Factorization of a probability distribution
For any collection of random variables
(X1, . . . , Xd), it always holds, as a straightforward consequence of the deﬁnition of
conditional probability, that the probability function pX1,...,Xd may be written as
pX1,...,Xd = pX1pX2|X1pX3|X1,X2 . . . pXd|X1,...Xd−1.
By reordering the variables, it therefore holds that for any ordering σ of (1, . . . , d),
pX1,...,Xd = pXσ(1)pXσ(2)|Xσ(1)pXσ(3)|Xσ(1),Xσ(2) . . . pXσ(d)|Xσ(1),...Xσ(d−1).
This way of writing a probability distribution is referred to as a factorization. A directed
acyclic graph may be used to indicate that certain variables are conditionally independent
of other variables, thus indicating how a factorization may be simpliﬁed.
Deﬁnition 2.14 (Factorization Along a Directed Acyclic Graph) A probability function
pX1,...,Xd over the variables X1, . . . , Xd is said to factorize along a directed acyclic graph
G if the following holds: there is an ordering Xσ(1), . . . , Xσ(d) of the variables such that
• (Xσ(1)) = σ(1) = φ; that is, Xσ(1) has no parents.
• For each j, (Xσ(j)) = σ(j) ⊂{Xσ(1), . . . , Xσ(j−1)}.
• pXσ(j)|Xσ(1),...,Xσ(j−1) = pXσ(j)|σ(j).
For each ordering of the variables, there is a directed acyclic graph that indicates how
to factorize the probability distribution and those variables that may be excluded in the
conditioning.
In some of the applications, the DAGs of interest will be trees (Deﬁnition 2.12) and
forests (Deﬁnition 2.13).
Deﬁnition 2.15 (Instantiated) When the state of variable is known, the variable is said to
be instantiated.
Within a directed acyclic graph, there are three basic ways in which two variables can
be connected via a third variable and the whole graph is built up from these connections.
They are the chain, fork and collider connections respectively.

DIRECTED ACYCLIC GRAPHS AND d-SEPARATION
47
X1
X2
X3
Figure 2.4
Chain connection: X2 is a chain node.
Chain connections
Consider a situation with three random variables (X1, X2, X3),
where X1 inﬂuences X2, which in turn inﬂuences X3, as in Figure 2.4, but there is
no direct inﬂuence from X1 to X3. If the state of X2 is unknown, then information about
X1 will inﬂuence the probability distribution of X2, which then inﬂuences the proba-
bility distribution of X3. Similarly, information about X3 will inﬂuence the probability
distribution of X1 through X2.
If state X2 is known, then the channel is blocked and X1 and X3 become independent
given X2. The DAG indicates that the probability distribution of (X1, X2, X3) may be
factorized as:
pX1,X2,X3 = pX1pX2|X1pX3|X2.
If X2 = x2 is known, then
pX1,X2,X3(., x2, .) = (pX1(.)pX2|X1(x2|.))(pX3|X2(.|x2))
and so, following characterization 5) of conditional independence from Theorem 2.1, the
variables X1 and X3 are conditionally independent, given X2.
From the DAG, the variables X1 and X3 are said to be d-separated given X2. The
full deﬁnition of d-separation is given in Deﬁnition 2.18, found later.
Fork connections
A fork is illustrated in Figure 2.5. Inﬂuence can pass between all
the children of X1 unless the state of X1 is known. If X1 is known, then the variables
X2 and X3 are said to be d-separated given X1. Evidence may be transmitted through a
fork node unless it is instantiated.
The DAG for the fork indicates that the probability distribution may be factorized as
pX1,X2,X3 = pX1pX2|X1pX3|X1.
This implies that if the state of X1 is known, then
pX2,X3|X1(., .|x1) = pX2|X1(.|x1)pX3|X1(.|x1),
so that X2 and X3 are conditionally independent, following characterization 3) from the
characterizations of conditional independence listed in the statement of Theorem 2.1.
Example 2.1
The directed acyclic graph (Figure 2.6) illustrates the following situation,
described by Albert Engstr¨om (1869–1940), a Swedish cartoonist. During a convivial
gathering there is talk of the unhygienic aspect of using galoshes. One of those present
chimes in: ‘Yes, I’ve also noticed this. Every time I’ve woken up with my galoshes on,
I’ve had a headache.’
X2
X1
X3
Figure 2.5
Fork connection: X1 is a fork node.

48
CONDITIONAL INDEPENDENCE, GRAPHS AND d-SEPARATION
drink
galoshes
head
Figure 2.6
Causal relationship between galoshes, head and drink.
But does the footwear that he adopts while sleeping really inﬂuence the state of his
head the next morning? The causal model represented by the graph indicates that if
there is no information about his activities of the previous evening, then the presence of
galoshes on his feet when he awakes indicates that his head will not be in such good
shape. On the other hand, if there is full information concerning the activities of the
previous evening, then the state of the feet gives no further information about the state
of the head.
□
Collider connections
Consider the graph in Figure 2.7. If nothing is known about X1
except that which may be inferred from knowledge about its parents, then the parents
are independent; information about one of them will not affect the assessment of the
probability values of the others. Information about one possible cause of an event does
not give any further information about other possible causes, if there is no information as
to whether or not the event actually happened. But if there is any information concerning
the event, then information about one possible cause may give information about the
other causes. In Figure 2.7, if it is known that the event {X1 = x1} has occurred, and
it is considered a priori that both {X2 = x2} and {X3 = x3} make the event {X1 = x1}
more likely, then the information that the event {X2 = x2} has occurred will, in general,
decrease the probability that the event {X3 = x3} has occurred.
Information may only be transmitted through a collider if information has been
received either about the variable in the connection or about one of its descendants.
The factorization of the distribution pX1,X2,X3 corresponding to the DAG for the
collider is
pX1,X2,X3 = pX2pX3pX1|X2,X3.
Clearly, from the characterizations of conditional independence, X2 and X3 are not con-
ditionally independent given X1, but if X1 is unknown, then X2 and X3 are independent;
for any (x(i)
2 , x(j)
3 ) ∈X2 × X3, where X1, X2, X3 are the state spaces for the random
X2
X3
X1
Figure 2.7
Collider connection: X1 is a collider node.

DIRECTED ACYCLIC GRAPHS AND d-SEPARATION
49
variables X1, X2 and X3 respectively.
pX2,X3(x(i)
2 , x(j)
3 ) =

y∈X1
pX2(x(i)
2 )pX3(x(j)
3 )pX1|X2,X3(y|x(i)
2 , x(j)
3 )
= pX2(x(i)
2 )pX3(x(j)
3 )

y∈X1
pX1|X2,X3(y|x(i)
2 , x(j)
3 )
= pX2(x(i)
2 )pX3(x(j)
3 ).
For a chain or a fork, blocking requires the chain or fork variable respectively to be
instantiated. Opening in the case of a collider holds for any information at all on any
of the descendant variables. Information may be transmitted between nodes of a graph
along an active trail, deﬁned below.
Deﬁnition 2.16 (S-Active Trail) Let G = (V, E) be a directed acyclic graph. Let S ⊂V
and let X, Y ∈V \S. A trail τ between the two variables X and Y is said to be S-active if
1. Every collider node in τ is in S, or has a descendant (Deﬁnition 2.9) in S.
2. Every other node is outside S.
Deﬁnition 2.17 (Blocked Trail) A trail between X and Y that is not S-active is said to be
blocked by S.
The following deﬁnition is basic for all that follows in this chapter.
Deﬁnition 2.18 (D-separation) Let G = (V, E) be a directed acyclic graph, where V =
{X1, . . . , Xd} is a collection of random variables. Let S ⊂V such that all the variables in
S are instantiated and all the variables in V \S are not instantiated. Two distinct variables
Xi and Xj not in S are d-separated by S if all trails between Xi and Xj are blocked by S.
Let C and D denote two sets of variables. If every trail from any variable in C to any
variable in D is blocked by S, then the sets C and D are said to be d-separated by S.
This is written
C ⊥D ∥G S.
(2.11)
The set S blocks every path between C and D.
The terminology d-separation is short for directed separation. The insertion of the letter
‘d’ points out that this is not the standard use of the term ‘separation’ found in graph
theory. The term ‘separation’ will be introduced and used in the standard way in the later
discussion about decomposable graphs (Chapter 4).
In common language, d-separation may be regarded in terms of irrelevance. In other
words, if A and B are d-separated by a set S then, once the variables in the set S are
known, new information on one of the sets A or B does not change the probability
distribution of the other. This has been seen in some basic cases above; a general proof
will be given later.

50
CONDITIONAL INDEPENDENCE, GRAPHS AND d-SEPARATION
Deﬁnition 2.19 (d-connected) If two variables X and Y are not d-separated, they are
said to be d-connected.
A procedure for determining d-separation
The following procedure may be used to
check whether a set of variables S d-separates a set C from a set D.
1. Find all trails connecting the variables in C to the variables in D.
2. Check for each trail, until an active trail is found:
(a) If there a chain or fork node in S on the trail, then the trail is not active.
(b) If there is a collider node on the trail, then check whether any of its descendants
are in S. If not, then the trail is not active.
(c) Otherwise, the trail is active.
3. If an active trail was found, then C and D are not d-separated by S. If none of
the trails are active, they are d-separated by S.
To declare that sets are not d-separated, it is only necessary to ﬁnd a single active trail.
To declare that sets are d-separated, it is necessary to show that all trails are not active.
It is clear that if all the parents of a variable X and all the children of X and all the
variables sharing a child with X are instantiated, then X is d-separated from the rest of
the network. This set of variables is known as the Markov blanket of the variable X:
Deﬁnition 2.20 (Markov Blanket) The Markov blanket of a variable X is the set consist-
ing of the parents of X, the children of X and the variables sharing a child with X.
Example 2.2
Consider the DAG given in Figure 2.8. The Markov blanket of the variable
X1 is the set of nodes {X2, X3, X4, X6}. Let S = {X2, X3, X4, X6}, then X1 ⊥X5∥GS.
There are several interesting algorithmic applications of Markov blankets, e.g. to
selection of variables in pattern recognition; see [49].
2.4
The Bayes ball
The Bayes ball provides a convenient method for deciding whether or not two nodes are
d-separated. The idea was introduced by R. Schachter [50] and is illustrated in Figure
2.9. Variables are d-connected if the Bayes ball can be passed between them employing
the following rule. The uninstantiated (hidden) nodes are represented by circles, the
instantiated nodes as squares. This notation will be used throughout the book.
X1
X3
X2
X6
X5
X4
Figure 2.8
DAG for example of a Markov blanket.

THE BAYES BALL
51
Figure 2.9
Bayes ball.
Consider the three types of connection in a DAG: chain, collider and fork.
• For the chain connection illustrated in Figure 2.4, the Bayes ball algorithm indicates
that if the node is instantiated, then the ball does not move from X1 to X3 through
X2. The communication in the trail is blocked. If the node is not instantiated, then
communication is possible.
• For the fork connection illustrated in Figure 2.5, the algorithm states that if node
X1 is instantiated, then again communication between X2 and X3 is blocked. If the
node is not instantiated, then communication is possible.
• For the collider connection illustrated in Figure 2.7, the Bayes ball algorithm states
that the ball does move from X2 to X3 if node X1 is instantiated. Instantiation of
X1 opens communication between the parents.
For a collider node X1, instantiation of any of the descendants of X1 also opens commu-
nication. If node X1 is uninstantiated, and none of its descendants is instantiated, then
there is no communication.
Explaining away
The ways that the Bayes ball may move in a collider are the opposite
of those for a chain or fork. In Figure 2.7, the nodes X2 and X3 are independent when
X1 is not instantiated, but a link emerges after instantiation. This pattern of reasoning
is known as explaining away. That is, if there are two possible causes for an event and
if one possible cause is known to have happened, then the other possible cause is less
likely to have happened.
2.4.1
Illustrations
The following examples illustrate situations that may be modelled using random variables
with causal relations between them, how these may be expressed as DAGs and inferences
that may be drawn.

52
CONDITIONAL INDEPENDENCE, GRAPHS AND d-SEPARATION
Example 2.3
Consider a situation where a desktop computer is powered by an electricity
supply. The light source operates from the same electricity supply. If the computer is
turned on and nothing happens, then the problem may be the result either of a hardware
malfunction in the computer, or of a fault in the electricity supply.
If the lights are not working, this gives information about electricity failure, since it
is a likely cause of light failure. If the computer does not respond when the electricity is
switched on, it could be a result of a failure in the electricity supply, or a problem with
the computer hardware. A computer malfunction as a cause for lack of response is less
likely if it is known that there is an electricity failure. A network describing the situation
is shown in Figure 2.10.
This is an example of explaining away, as described above. If the ‘effect’ variable is
instantiated, then any particular cause is less likely if another possible cause is known to
have taken place.
Example 2.4
The DAG in Figure 2.11 has been instantiated in X3, X4, X7 and X8, the
set of neighbours of X5. In this graph, the variable X5 is d-connected to X6, through X8;
the connection is a collider and X8 is instantiated.
Example 2.5
The DAG in Figure 2.12 has been instantiated in X2, X3, X4. The node
X6 is d-separated from all the uninstantiated nodes.
electricity
computer
light
malfunction
Figure 2.10
Light, Computer, Electricity.
X1
X2
X3
X4
X5
X6
X7
X8
Figure 2.11
Figure for Example 2.

POTENTIALS
53
X1
X2
X3
X4
X5
X6
X7
Figure 2.12
Figure for Example 2.5.
H1
H2
H3
. . .
. . .
. . .
Hn
S1
S2
S3
Sm
C1
F
C2
Cp
Figure 2.13
Bayesian network for Root Cause Analysis.
Example 2.6 (Root Cause Analysis)
The following example is taken from [51], p.
1999. In large scale and complex industrial processes, the process operator has to isolate
the cause of a failure by analyzing the signals from many sensors.
The root causes in the hardware, (Hi)n
i=1 are the parent variables, which are the
causes of various symptoms, (Sj)m
j=1. The word ‘symptoms’ refers to changes in the
process operation conditions, which affect the equipment performance or the ﬁnal output.
These in turn cause either failure F or conﬁrming events (Ck)p
k=1 that conﬁrm that the
process is abnormal. From the conﬁrming events that occur, the objective is to deduce
which hardware problem is the root cause. A directed acyclic graph for Root Cause
Analysis is given in Figure 2.13.
Root Cause Analysis is a situation where the concept of explaining away appears;
symptoms connected with a root cause may make it less likely that other root causes
(which are not supported by conﬁrming events) are present.
2.5
Potentials
Let V = {X1, . . . , Xd} denote a collection of random variables, where variable Xj
has state space Xj = (x(1)
j , . . . , x
(kj )
j
) for j = 1, . . . , d. Let X = ×d
j=1Xj denote the
state space for X. Let ˜V = {1, . . . , d} denote the indexing set for the variables. For
D ⊂˜V , where D = {j1, . . . , jm}, let XD = ×j∈DX and let XD = (Xj1, . . . , Xjm).
Let x ∈X denote a generic element of X and let xD = (xj1, . . . , xjm) ∈XD, when
x = (x1, . . . , xd) ∈X.
Furthermore, for W ⊂V , let ˜W denote the indexing set for W. The notation XW will
also be used to denote X ˜W, XW to denote X ˜W and xW to denote x ˜W.

54
CONDITIONAL INDEPENDENCE, GRAPHS AND d-SEPARATION
Deﬁnition 2.21 (Potential) A potential φ over a domain XD is deﬁned as a non negative
function φ : XD →R+. The space XD is known as the domain of the potential. If the
domain is the state space of a random vector XD, then the random vector XD may also
be referred to as the domain of the potential.
In this setting, a potential over a domain XD has 	
j∈D kj entries. For W ⊂V , the domain
of a potential XW may also be denoted by the collection of random variables W.
For example, let {X1, X2} ⊂V and let φ denote the joint probability distribution of
(X1, X2), deﬁned by
x(1)
2
x(2)
2
x(3)
2
x(1)
1
0.05
0.10
0.05
x(2)
1
0.15
0.00
0.25
x(3)
1
0.10
0.20
0.10
Here X1 = (x(1)
1 , x(2)
1 , x(3)
1 ) and X2 = (x(1)
2 , x(2)
2 , x(3)
2 ). The potential φ is a function of
two variables. The domain of the potential φ is X1 × X2, which may also be denoted by
(X1, X2). A pair c = (x(i)
1 , x(j)
2 ) is called a conﬁguration of (X1, X2). If, for example
c = (x(1)
1 , x(3)
2 ), then
φ(c) = φ((x(1)
1 , x(3)
2 )) = 0.05.
Consider two potentials over (X1, X2); φ and φ′, given by
φ =
x(1)
2
x(2)
2
x(3)
2
x(1)
1
a1
a2
a3
x(2)
1
b1
b2
b3
x(3)
1
c1
c2
c3
and
φ′ =
x(1)
2
x(2)
2
x(3)
2
x(1)
1
a′
1
a′
2
a′
3
x(2)
1
b′
1
b′
2
b′
3
x(3)
1
c′
1
c′
2
c′
3
These will be used to illustrate the deﬁnitions of multiplication and division of potentials.
Deﬁnition 2.22 (Addition of Potentials) Two potentials φ and φ′ deﬁned over the same
domain XD may be added together. Their sum is deﬁned as the coordinate-wize sum; for
each xD ∈XD,
(φ + φ′)(xD) = φ(xD) + φ′(xD).
Deﬁnition 2.23 (Multiplication of Potentials) Two potentials φ and φ′ may be multiplied
together to yield the potential φ.φ′ if they are both deﬁned over the same domain. Multi-
plication of potentials is deﬁned by multiplying each entry in the conﬁguration.
Hence, in the example above,
φ.φ′ ↔
X\Y
y1
y2
y3
x1
a1a′
1
a2a′
2
a3a′
3
x2
b1b′
1
b2b′
2
b3b′
3
x3
c1c′
1
c2c′
2
c3c′
3
□

POTENTIALS
55
Deﬁnition 2.24 (Division of Potentials) Two potentials φ and φ′ may be divided if they
are deﬁned over the same domain. The division of a potential φ by φ′ to give the potential
φ/φ′ is deﬁned by coordinate-wize division where the deﬁnition is that b = 0 ⇒a
b = 0.
In the example above, provided that none of the entries of potential φ′ are zero, the
potential φ
φ′ is given by
φ/φ′ ↔
X\Y
y1
y2
y3
x1
a1/a′
1
a2/a′
2
a3/a′
3
x2
b1/b′
1
b2/b′
2
b3/b′
3
x3
c1/c′
1
c2/c′
2
c3/c′
3
□
Potentials over different domains
If potential φ1 is deﬁned over domain XD1 and
potential φ2 is deﬁned over domain XD2, then multiplication and division of potentials
may be deﬁned by ﬁrst extending both potentials to the domain XD1∪D2.
Deﬁnition 2.25 (Extending the Domain) Let the potential φ be deﬁned on a domain XD,
where D ⊂˜W ⊆˜V . Then φ, deﬁned over a domain XD, is extended to the domain X ˜W in
the following way. For each x ˜W ∈X ˜W,
φ(x ˜W) = φ(xD),
where xD is the projection of x ˜W onto XD, using the deﬁnition of xD (and hence x ˜W) from
the beginning of Section 2.5. In other words, the extended potential depends on x ˜W only
through xD.
Deﬁnition 2.26 (Addition, Multiplication and Division of Potentials over Different
Domains) Addition, multiplication and division of potentials over different domains is
deﬁned as ﬁrst, extending the domains of deﬁnition, using Deﬁnition 2.25, so that they
are deﬁned over the same domain, and then using Deﬁnition 2.22 for adding, Deﬁnition
2.23 for multiplication and Deﬁnition 2.24 for division.
Multiplication of potentials may be expressed in the following terms: the product φ1.φ2
of potentials φ1 and φ2, deﬁned over domains XD1 and XD2 is deﬁned as
(φ1.φ2)(xD1∪D2) = φ1(xD1∪D2)φ2(xD1∪D2),
where φ1 and φ2 have ﬁrst been extended to XD1∪D2.
Let Dφ denote the index set for the domain variables of a potential φ. Multiplication
has the following properties:
1. Dφ1.φ2 = Dφ1 ∪Dφ2,
2. (Commutative Law): φ1.φ2 = φ2.φ1
3. (Associative law): (φ1.φ2).φ3 = φ1.(φ2.φ3).

56
CONDITIONAL INDEPENDENCE, GRAPHS AND d-SEPARATION
4. (Existence of unit) The number 1 is a potential over the empty domain and 1φ = φ
for all potentials φ. The unit potential is denoted by 1.
Example 2.7
Let
φXY ↔
X\Y
y1
y2
x1
a1
a2
x2
a3
a4
and
φ′
XZ ↔
X\Z
z1
z2
x1
b1
b2
x2
b3
b4
Following Deﬁnition 2.26, the product φXY.φXZ is
[φXY.φXZ](x, y, z)
def
= φXY(x, y)φXZ(x, z).
Thus
φXYφXZ ↔
X\Y\Z
y1
y1
x1
(a1b1, a1b2)
(a2b1, a2b2)
x2
(a3b1, a3b4)
(a4b3, a4b4)
Now consider marginalizing the potential φXYφXZ over the variable Z. The term
marginalizing simply means summing over the state space XZ. This produces a potential
with domain XX × XY ;

XZ
φXYφXZ ↔
X\Y
y1
y2
x1
a1b1 + a1b2
a2b1 + a2b2
x2
a3b1 + a3b4
a4b3 + a4b4
The entry for conﬁguration (x1, y2) is
[

Z
φXY.φXZ](x1, y2) = φXY.φXZ(x1, y2, z1) + φXYφXZ(x1, y2, z2) = a2b1 + a2b2.
Marginalization
The operation of marginalization is now considered more generally.
Let V = {X1, . . . , Xd} denote a set of d random variables, with indexing set
˜V =
{1, . . . , d}. Let U ⊆W ⊆V and let φ be a potential deﬁned over XW. The expres-
sion 
XW\U φ denotes the margin (or the sum margin) of φ over XU and is deﬁned for
xU ∈XU by

W\U
φ

(xU) =



z∈XW \XU
φ

(z, xU),
where the arguments have been rearranged so that those corresponding to W appear
ﬁrst, z ∈XW is the projection of (z, xU) ∈X onto XW and xU ∈XU the projection of
(z, xU) ∈X onto XU. The following notation is also used:
φ↓U =

W\U
φ

.

POTENTIALS
57
The marginalization operation obeys the following rules:
1. The Commutative Law: for any two sets of variables U ⊂V and W ⊂V ,
(φ↓U)↓W = (φ↓W)↓U.
2. The Distributive Law:
If XD1 is the domain of φ1 and D1 ⊆˜V , then (φ1φ2)↓D1 = φ1(φ2)↓D1.
Joint probability distributions
Consider three variables, X1, X2, X3, with state spaces
X1, X2 and X3 respectively, where X1 = (x(1)
1 , x(2)
1 ), X2 = (x(1)
2 , x(2)
2 , x(3)
2 ) and X3 =
(x(1)
3 , x(2)
3 , x(3)
3 ), with joint probability function pX1,X2,X3. The joint probability function
is a three-way potential. In the potential below, the entry at position (i, j) is the triple
(pX1,X2,X3(x(i)
1 , x(j)
2 , x(1)
3 ), pX1,X2,X3(x(i)
1 , x(j)
2 , x(2)
3 ), pX1,X2,X3(x(i)
1 , x(j)
2 , x(3)
3 )).
x(1)
2
x(2)
2
x(3)
2
x(1)
1
(0, 0.05, 0.05)
(0.05, 0.05, 0)
(0.05, 0.05, 0.05)
x(2)
1
(0.1, 0.1, 0)
(0.1, 0, 0.1)
(0.2, 0, 0.05)
In this example, pX1,X2,X3(x(2)
1 , x(3)
2 , x(1)
3 ) = 0.2. The distribution of (for example) X1
and X3, is found by marginalizing over the unwanted variables; here, by summing over
X2. This gives
x(1)
3
x(2)
3
x(3)
3
x(1)
1
0.10
0.15
0.10
x(2)
1
0.40
0.10
0.15
The conditional probability distribution of X2 given X1 = x(2)
1
and X3 = x(1)
3 , for
example, is computed using
pX2|X1,X3(.|x(2)
1 , x(1)
3 ) = pX1,X2,X3(x(2)
1 , ., x(1)
3 )
pX1,X3(x(2)
1 , x(1)
3 )
= (0.25, 0.25, 0.5).
The entire potential giving the conditional probability distribution of X2 given X1 and
X3 may be computed by marginalizing over X2 to give the potential pX1,X3 and then by
division of potentials to give the potential
pX2|X1,X3 = pX1,X2,X3
pX1,X3
.

58
CONDITIONAL INDEPENDENCE, GRAPHS AND d-SEPARATION
2.6
Bayesian networks
The formal deﬁnition of a Bayesian network is given in Deﬁnition 2.27. It consists of
a graphical model (namely, the directed acyclic graph) together with the corresponding
probability potentials.
The result of the following lemma is clear and is inserted for completeness. It is
necessary for the formal deﬁnition and construction of a Bayesian network. The statement
of the lemma follows p.22 in [52].
Lemma 2.1 (Shafer) For any DAG with a ﬁnite number of nodes α1, . . . , αd there is an
ordering (ασ(1), . . . , ασ(d)) of the nodes (not necessarily unique) such that the parents of
ασ(i) are a subset of {ασ(1) . . . , ασ(i−1)}. That is, by renaming the nodes as βj = ασ(j),
j = 1, . . . , d, the parents of βj are a subset of {β1, . . . , βj−1} for each j = 1, . . . , d.
The statement of the lemma does not require the DAG to be connected.
Proof In any DAG with d > 1 nodes, it is possible to ﬁnd at least one node that has no
children. This is easily proved by contradiction. Assume there are no such nodes. Then,
since there are only a ﬁnite number of nodes, it possible to move from any given node to
one of its children. After repeating this d + 1 times, a node already visited is encountered
again. This implies that there is a cycle, and hence a contradiction.
For k = 1, . . . , d do the following: In the DAG with d −k + 1 nodes, choose a node
without a child, call it jk. Let σ(d −k + 1) = jk. Remove the node αjk from the node
set and all edges (αi, αjk) (i.e. those with a directed arrow pointing towards αjk from the
edge set. This leaves a DAG with d −k nodes.
This produces an ordering of the nodes (ασ(1), . . . , ασ(d)) that satisﬁes the given
condition.
□
For any joint probability distribution over a set of variables, with a given ordering
for the variables, there is a directed acyclic graph over which the probability distribution
may be factorized, where for each node Xj, j ⊆{X1, . . . , Xj−1}. The directed acyclic
graph is induced by the ordering of the nodes X1, . . . , Xd and any other ordering of
the nodes Xσ(1), . . . , Xσ(d) will induce a different directed acyclic graph along which the
probability distribution may be satisﬁed such that σ(j) ⊆{Xσ(1), . . . , Xσ(j−1)}. In many
situations of interest, additional causal modelling assumptions are to be incorporated,
which may be modelled into a Bayesian network rather easily, by choosing an ordering
of the variables to reﬂect them; for each i = 1, . . . , d, Xi is chosen in such a way that
all the variables that have a causal effect on Xi are contained in the set of variables
X1, . . . , Xi−1. For a variable Xi, the parent set i will be the subset of {X1, . . . , Xi−1}
containing those variables that, according to the model, may have a direct causal effect
on Xi. The model needs an a priori assessment of the conditional probability potentials
pXi|i for i = 1, . . . , d. The prior probability distribution over the whole set of variables
is obtained by multiplying these together.
The factorization pX = 	d
j=1 pXj |j expresses certain explicit modelling assump-
tions of conditional independence between variables and, if there are assumptions of
direct causal relations between variables, these are also expressed by the factorization.
Further conditional independence relations implied by these modelling assumptions may
be inferred by checking whether nodes of the graph are d-separated. A graph for which

BAYESIAN NETWORKS
59
the only conditional independence relations are those that are given by d-separation
within the graph is said to be faithful to the probability distribution. When a graph is
faithful to a probability distribution, the variables that are d-connected within the graph
are associated. That is, when ‘faithfulness’ holds, associations suggested by the graph are
real associations and not an accident of the parametrization.
Deﬁnition 2.27 (Bayesian Network) A Bayesian network is a pair (G, p), where G =
(V, E) is a directed acyclic graph with node set V = {1, . . . , d} for some d ∈N, E is the
edge set, and p is either a probability distribution or a family of probability distributions,
indexed by a parameter set , over d discrete random variables, {X1, . . . , Xd}. The pair
(G, p) satisﬁes the following criteria:
• For each θ ∈, p(.|θ) is a probability function with the same state space X, where
X has a ﬁnite number of elements. That is, for each θ ∈, p(.|θ) : X →[0, 1] and

x∈X p(x|θ) = 1.
• For each node Xv ∈V with no parent variables, there is assigned a potential
denoted by pXv, giving the probability distribution of the random variable Xv. To
each variable Xv ∈V with a non empty parent set v = (Xb(v)
1 , . . . , Xb(v)
m ), there is
assigned a potential
pXv|v
containing the conditional probability function of Xv
given the variables
{Xb(v)
1 , . . . , Xb(v)
m }. If Xv has no parents, set v = φ, the empty set, so that
pXv = pXv|v. The joint probability function p may be factorized using the
potentials pXv|v thus deﬁned:
pX1,...,Xd =
d
v=1
pXv|v.
• The factorization is minimal in the sense that for an ordering of the variables
such that j ⊆{X1, . . . , Xj−1}, j is the smallest set of variables such that Xj ⊥
c
j|j. That is,
j = ∩{A ⊆{X1, . . . , Xj−1}
such that
Xj ⊥Ac|A}.
Example 2.8
Consider the Bayesian network with four variables X, Y, Z, W given in
Figure 2.14.
W
Y
Z
X
Figure 2.14
A graph on four variables.

60
CONDITIONAL INDEPENDENCE, GRAPHS AND d-SEPARATION
The graph in Figure 2.14 represents a probability distribution which may be factorized
according to
pX,Y,Z,W = pZ|X,YpY|X,WpX|WpW.
The joint distribution has been expressed in terms of conditional probabilities which have
been given. Furthermore, using the notation of Lemma 2.1, the variables may be renamed
as W = β1, X = β2, Y = β3 and Z = β4, giving the variables an ordering that has the
property described in Lemma 2.1. The parent set of W = β1 is empty.
Vorobev’s example
It is important for the calculus developed in this text that there are
no feedback cycles and that the probability distributions factorizes along a directed acyclic
graph. In the context of causal probability calculus, this is interpreted as a requirement
that circular reasoning is not permitted, where an event A has a causal inﬂuence on an
event B, while at the same time B has a causal inﬂuence on an event A. The following
example, due to N. Vorobev [53], illustrates how things can go wrong if cycles are
permitted. This is a fundamental paper in the ﬁeld, but rarely cited.
Consider the cyclic graph in Figure 2.15. Three two-dimensional marginal distribu-
tions are speciﬁed, for a joint probability of three variables, which is to be factorized
according to the directed graph given by Figure 2.15.
The potentials for the joint distributions are:
x2
0
1
x1
0
1/2
0
1
0
1/2
,
x3
0
1
x2
0
0
1/2
1
1/2
0
,
x1
0
1
x3
0
1/2
0
1
0
1/2
Here the potentials yield the marginals pX1(0, 1) = ( 1
2, 1
2), pX2(0, 1) = ( 1
2, 1
2) and
pX3(0, 1) = ( 1
2, 1
2). If these potentials are used to compute the joint distribution
pX1,X2,X3, then, factorizing along the graph,
pX1,X2,X3(0, 0, 0) = pX1|X3(0|0)pX3|X2(0|0)pX2(0)
= pX1|X3(0|0)pX2,X3(0, 0)
= pX1,X3(0, 0)
pX3(0)
× 0
= 1 × 0 = 0,
x1
x2
x3
Figure 2.15
A cyclic graph.

BAYESIAN NETWORKS
61
but
pX1,X2,X3(0, 0, 0) = pX2|X1(0|0)pX1|X3(0|0)pX3(0)
= pX2|X1(0|0)pX1,X3(0, 0)
= pX1,X2(0, 0)
pX1(0)
× 1
2
= 1 × 1
2 = 1
2
and hence a contradiction is reached if cyclic graphs are permitted.
□
Example 2.9
Consider the following three variables: Electricity Failure (E) (1/0), Mal-
function (M) (1/0), Computer Breakdown (C) (1/0) where 1 denotes ‘yes’ and 0 denotes
‘no’. Suppose the causal relation is given by the DAG in Figure 2.16. Then the spec-
iﬁed probabilities for the DAG are pC|E,M, pE and pM. The factorization of the joint
probability along the DAG is
pC,E,M = pC|E,MpEpM.
For this DAG, E and M are independent. Suppose that
pE(1) = 0.1,
pM(1) = 0.2
pC|M,E(1|1, 1) = 1,
pC|M,E(1|0, 1) = 1,
pC|M,E(1|1, 0) = 0.5,
pC|M,E(1|0, 0) = 0.
This gives all the information necessary for a Bayesian network.
Suppose the computer is turned on and nothing happens. Then C = 1 has been instan-
tiated. pC(1) is computed by marginalizing over E and M;
pC(1) =

e,m
pC|E,M(1|e, m)pE(e)pM(m) = 0.19.
The joint distribution of E and M given C = 1 may be computed via Bayes’ theorem,
pM,E|C(m, e|1) = pC|M,E(1|m, e)pM(m)pE(e)
pC(1)
E
M
C
Figure 2.16
Electricity, Malfunction, Breakdown.

62
CONDITIONAL INDEPENDENCE, GRAPHS AND d-SEPARATION
which gives
e
1
0
m
1
0.02
0.19
0.09
0.19
0
0.08
0.19
0
The marginal probabilities may be computed. For example:
pM|C(1|1) =

e
pM,E|C(1, e|1) = 0.58
pE|C(1|1) =

m
pE,M|C(1, m|1) = 0.53.
From this, it is clear that
pM|C(1|1)pE|C(1|1) ̸= pM,E|C(1, 1|1).
That is, M and E are not conditionally independent given C.
□
Example 2.10
Continuing the previous example, suppose that, having observed a com-
puter failure, C = 1, it is also observed that the lights in the room have gone off, L = 1.
The situation is described by the DAG in Figure 2.17, where square nodes represent the
instantiated variables.
Only one more conditional probability, pL|E is required, in addition to the ones given
above, to create a Bayesian network. Suppose that
pL|E(1|1) = 1, pL|E(1|0) = 0.2.
Then
pL(1) = pL|E(1|1)pE(1) + pL|E(1|0)pE(0) = 0.28.
Having ﬁrst observed C = 1 and then L = 1, the previous posterior distribution
pM,E|C(m, e|1) becomes the new prior and Bayes’ rule gives
pM,E|C,L(m, e|1, 1) = pC,L|M,E(1, 1|m, e)pM,E(m, e)
pC,L(1, 1)
=
pC|M,E(1|m, e)pL|E(1|e)pM(m)pE(e)

e,m pC|E,M(1|e, m)pL|E(1|e)pE(e)pM(m)
E
M
L = 1
C = 1
Figure 2.17
Electricity, Malfunction, Breakdown, Lights.

OBJECT ORIENTED BAYESIAN NETWORKS
63
The computation is as follows:
pM(m) =
1
0
0.2
0.8
pE(e) =
1
0
0.1
0.9
so that
pM(m)pE(e) =
M\E
1
0
1
0.02
0.18
0
0.08
0.72
pC|M,E(1|m, e) =
M\E
1
0
1
1
0.5
0
1
0
so that
pM(m)pE(e)pC|M,E(1|m, e) =
M\E
1
0
1
0.02
0.09
0
0.08
0
The potential pL|E is
pL|E(1|e) = 1
0
1
0.2
and multiplication of potentials gives
pM(m)pE(e)pC|M,E(1|m, e)pL|E(1|e) =
M\E
1
0
1
0.02
0.018
0
0.08
0
Finally, the normalizing constant is obtained by summation, which gives 0.118, so that
pM,E|C,L(m, e|1, 1) =
M\E
1
0
1
0.169
0.153
0
0.678
0
Marginalizing gives
pM|C,L(1|1, 1) = 0.322
pE|C,L(1|1, 1) = 0.847.
This is an example of explaining away; here, pM|C(1|1) = 0.58, but pM|C,L(1|1, 1) =
0.322. The observation that there is a light failure reduces the chance of a mechanical
problem with the computer hardware.
□
2.7
Object oriented Bayesian networks
Object oriented Bayesian networks are discussed in [54]. In an object oriented Bayesian
network (OOBN), a node represents an object which is a collection of random variables

64
CONDITIONAL INDEPENDENCE, GRAPHS AND d-SEPARATION
(attributes) rather than a single random variable. The attributes are contained within the
object.
The idea is that if there are several different types of object, which share common
features, then the same potentials and parameters may be used to represent the common
features. In this way, there are fewer parameters and the conditional probability potentials
related to the common features may be updated using all the information available.
This is now illustrated by an example taken from [54]. Old MacDonald (OMD) has a
farm where he keeps two milk cows and two meat cows. OMD wants to model his stock
using OOBN classes. The object, or class, is a cow. A generic cow contains only those
features common to both (Figure 2.18). Since the mother of the cow and the food that
the cow eats both inﬂuence how much milk and meat the cow produces, OMD wants
mother and food to be input nodes. OMD wants milk, the daily output of milk and meat,
the amount of meat on a cow, as output nodes. Nodes in an instantiation that are neither
input nor output nodes are termed normal nodes.
The values taken by an instantiation of the ‘milk’ and ‘meat’ variables are the quanti-
ties of milk and meat respectively produced by the animal. The generic cow contains the
input and normal nodes that both the milk and meat cows have in common. There may
be other nodes peculiar to both. For example, suppose OMD is told by an expert that
music inﬂuences the state of mind of a milk cow, which in turn inﬂuences its metabolism,
while ‘music’ has no inﬂuence on a meat cow, but that the weather does inﬂuence the
state of mind of a meat cow (while not that of a milk cow), hence its metabolism and
hence the quantities of milk and meat produced. Following all the advice from the expert,
the milk cows and meat cows may be represented as illustrated in the DAGs shown in
Figures 2.19 and 2.20 respectively.
Here ‘cow’ is a class and ‘milk cow’ and ‘meat cow’ are subclasses. A class S is
a subclass of a class C if the set of nodes of C is a subset of the set of nodes of S.
This ensures that an instantiation of S may be used anywhere in the OOBN instead of
an instantiation of C.
The idea is that while milk cows and meat cows may be quite different, they never-
theless have some features in common, so that OMD may use the information from the
entire herd to update the CPPs (conditional probability potentials) common to both.
Each node in the subclass inherits the conditional probability potentials of the class,
unless the parent sets differ.
Food
Mother
Metabolism
Milk
Meat
Figure 2.18
A generic cow.

OBJECT ORIENTED BAYESIAN NETWORKS
65
Food
Mother
Music
Metabolism
State of Mind
Milk
Meat
Figure 2.19
A milk cow.
Food
Mother
Weather
Metabolism
State of Mind
Milk
Meat
Figure 2.20
A meat cow.
The structure of a class hierarchy will be a tree, or a forest (recall that a forest is a
collection of disjoint trees).
In the example of OMD’s cattle, suppose that the mothers of two of the cows are
known: Daisy is the mother of one of the meat cows while Matilda is the mother of
one of the milk cows, but the other two mothers are unknown. For the two where the
mother is unknown, this may be accommodated by introducing a third category, ‘mother
unknown’, for the situation where the identity of the mother is missing from the data.
The Object Oriented Assumption is that the CPPs (conditional probability potentials)
of a class are the same, wherever that class appears. In the example of OMD’s farm, the
generic cow potentials are the same for all four cows, the milk cow potentials are the
same for each milk cow and the meat cow potentials are the same for each meat cow.
Let X1 = food, X2 = mother, X3 = Music, X4 = state of mind, X5 = metabolism,
X6 = milk, X7 = meat. X8 = weather. Then the CPPs that need to be speciﬁed for the
generic cow are pX1, pX2, pX5|X1,X2, pX7|X5 and pX6|X5.
The potentials pX7|X5, pX6|X5, pX1 and pX2 remain the same for the milk cows and
meat cows. The potentials connected with food and mother pX1 and pX2 will clearly be
irrelevant in the analysis once these variables are ﬁxed in a particular instantiation.
To specify the milk cow class, the potentials pX3 and pX4|X2,X3 are needed, and the
potential pX5|X1,X2 from the generic cow has to be replaced with pX5|X1,X2,X4.

66
CONDITIONAL INDEPENDENCE, GRAPHS AND d-SEPARATION
To specify the meat cow class, the potentials pX8, pX4|X2,X8, pX5|X1,X4,X8 are needed,
together with the potentials for the generic cow, except for pX5|X1,X2.
2.8
d-Separation and conditional independence
The following result shows that, for a given DAG G, d-separation characterizes those
conditional independence statements that it is possible to infer from that particular DAG.
Theorem 2.2 (d-Separation Implies Conditional Independence) Let G = (V, E) be a
directed acyclic graph and let p be a probability distribution that factorizes along G.
Then for any three disjoint subsets A, B, C ⊂V , it holds that A ⊥B|C (A and B are
independent given C) if A and B are d-separated by C.
Conditions under which a converse to this theorem hold are discussed later in this section,
and presented in Theorem 2.3.
Proof of Theorem 2.2 Let V = {X1, . . . , Xd} denote the set of variables, let A ⊂V ,
B ⊂V and C ⊂V be three disjoint sets of variables and let ˜A, ˜B, ˜C, ˜V denote the
indexing sets of the variables in A, B, C and V respectively. For any set S ⊆V of
variables, let ˜S denote the set of indices. Suppose that A ⊥B∥GC. Let A, B and C
denote also the random vectors XA, XB and XC of variables in A, B and C respectively
and let XA, XB and XC denote their respective state spaces. It is required to show that
for all a ∈XA, b ∈XB and c ∈XC,
pA,B|C(a, b|c) = pA|C(a|c)pB|C(b|c).
Let D = V \(A ∪B ∪C). Let
E1 = {Y ∈V |there is a C-active trail from A to Y}
E2 = {Y ∈V |there is a C-active trail from B to Y}
D1 = D ∩E1 ∩E2, D2 = D ∩E1 ∩Ec
2, D3 = D ∩E2 ∩Ec
1, D4 = D ∩(Dc
1 ∪Dc
2 ∪Dc
3).
Since all the nodes of D are uninstantiated and there is no active trail from A to B,
it follows that any nodes in D1 are colliders with ancestors (Deﬁnition 2.9) in both A
and B, together with all the descendants (Deﬁnition 2.9) of these colliders. Neither the
colliders with ancestors in both A and B nor their descendants are instantiated (that is,
belong to C); neither do the nodes in D1 have descendants that belong to either A or B,
otherwise it is from the deﬁnitions that there would be an active trail between A and B.
From characterization 5) of Theorem 2.1, it is required to show that there are two
functions F and G such that
pA,B,C(a, b, c) = F(a, c)G(b, c).

MARKOV MODELS AND BAYESIAN NETWORKS
67
Let p(Xj|j) denote the conditional probability function of Xj given the parent variables
j. Then
p(X1, . . . , Xn) =

j∈˜A
p(Xj|j)

j∈˜B
p(Xj|j)

j∈˜C
p(Xj|j)
×

j∈˜D1
p(Xj|j)

j∈˜D2
p(Xj|j)

j∈˜D3
p(Xj|j)

j∈˜D4
p(Xj|j).
Any descendant of a variable in D1 is also in D1. Marginalizing over the variables in
D1 does not involve the parent variables of A, B or C, nor does it involve the variables
in D2 or D3 or their ancestors. Furthermore, the parents of variables in D4 are either in
D4 or in C.
Now, using φ to denote the empty set, let C2 = {X ∈C | (X) ∩D2 ̸= φ}, C3 =
{X ∈C | (X) ∩D3 ̸= φ} and C4 = C ∩Cc
2 ∩Cc
3. Then C2 ∩C3 = φ, the empty set,
otherwise there would be a collider node in C that would result in an active trail from
A to B. It is also clear that (C4) ⊆C ∪D4, where (C4) denotes the parent variables
of the variables in C4; that is, (C4) = {Y|(Y, X) ∈E, X ∈C4}. The sets C2, C3, C4
are disjoint. Using XS to denote the state space of the random vector formed from the
variables in a set S, it follows that
p(A, B, C) =


XD1

XD4

j∈˜D1
p(Xj|j)

j∈˜D4
p(Xj|j)

j∈˜C4
p(Xj|j)


×


XD2

j∈˜A
p(Xj|j)

j∈˜C2
p(Xj|j)

j∈˜D2
p(Xj|j)


×


XD3

j∈˜B
p(Xj|j)

j∈˜C3
p(Xj|j)

j∈˜D3
p(Xj|j)


= (ψ1(C)ψ2(A, C))ψ3(B, C)
where the deﬁnitions of ψ1, ψ2 and ψ3 are clear from the context. This factorization
clearly satisﬁes the required criteria. It follows that d-separation implies conditional
independence.
□
2.9
Markov models and Bayesian networks
This section introduces the local directed Markov condition, a necessary and sufﬁcient
condition so that a probability function p over a set of variables V can be factorized
along a graph G.

68
CONDITIONAL INDEPENDENCE, GRAPHS AND d-SEPARATION
Deﬁnition 2.28 (Local
Directed
Markov
Condition,
Locally
G-Markovian) Let
V = {X1, . . . , Xd} be a set of discrete random variables. A probability function p over
the random vector X = (X1, . . . , Xd) satisﬁes the local directed Markov condition with
respect to a DAG G = (V, E) or, equivalently, is said to be locally G-Markovian if and
only if for each j ∈{1, . . . , d}, Xj is conditionally independent, given j (the set of
parents of Xj) of all the variables in the set V \(Vj ∪j), where Vj is the set of all
descendants of Xj. That is,
Vj = {Y ∈V |there is a directed path from Xj to Y}.
(2.12)
That is,
Xj ⊥V \(Vj ∪j)|j.
The terminology Markov model corresponding to a directed acyclic graph G = (V, E),
deﬁned below, was introduced into the literature and may be found in [55].
Deﬁnition 2.29 (Markov Model) Let V = {X1, . . . , Xd} denote a set of variables and let
G = (V, E) be a directed acyclic graph. Let V denote the entire set of subsets of V . Let p
be a probability function for the random vector X = (X1, . . . , Xd). Let
I(p) = {(X, Y, S) ∈V × V × V|X, Y ̸∈S,
X ⊥Y|S}.
Note that φ ∈V and X ⊥Y|φ means that X ⊥Y.
The Markov model MG determined by a directed acyclic graph G = (V, E) is the set
of conditional independence statements
MG = {I|I = I(p)
for some p that is locally G- Markovian}.
That is, the Markov model is the set of all sets I of conditional independence relations
corresponding to locally G-Markovian distributions. A distribution p is said to belong to
the Markov model of G, p ∈MG, if and only if I(p) ∈MG.
Proposition 2.1
Let I(p) denote the entire set of conditional independence statements
satisﬁed by a probability function p for a random vector X = (X1, . . . , Xd). Then I(p) ∈
MG if and only if p factorizes along G.
Proof of Proposition 2.1 Firstly, if I(p) ∈MG then, by deﬁnition, p is locally
G-Markovian; that is, for each j ∈{1, . . . , d}, Xj ⊥V \(Vj ∪j) where Vj is deﬁned
in Equation (2.12). Let πj(x1, . . . , xj−1) denote the instantiation of j when X is
instantiated as (x1, . . . , xd). By characterization 1) of Theorem 2.1, for all j = 1, . . . , d
and any πj such that pj (πj) > 0,
pXj |X1,...,Xj−1(xj|x1, . . . , xj−1) = pXj |j (xj|πj)
with pXj |X1,...Xj−1(xj|x1, . . . , xj−1) = pXj (xj) if j = φ. It follows directly that
pX1,...,Xd =
d
j=1
pXj |j
and hence, by deﬁnition, that p factorizes along G.

I-MAPS AND MARKOV EQUIVALENCE
69
Secondly, suppose that p factorizes along a graph G = (V, E). Then it is clear (for
example by using the Bayes ball algorithm) that
Xj ⊥V \(Vj ∪j)∥Gj
where Vj is the set of variables deﬁned by Equation (2.12). If j is instantiated, then
any trail from Xj to a variable in V \(Vj ∪j) has to pass through a node in j, which
will be either a chain or fork connection. It follows from Theorem 2.2 that
Xj ⊥V \(Vj ∪j)|j,
from which it follows that p is locally G-Markovian.
□
2.10
I -maps and Markov equivalence
A particular directed acyclic graph arises from a particular ordering of the variables.
This ordering may arise from considerations of cause to effect, but often this is not the
case. Faced with a probability distribution pX1,...,Xd(x1, . . . , xd), it is often useful to ﬁnd
a factorization along a directed acyclic graph that expresses the structural dependencies
between the variables, even if there is no clear causal relationship between the variables.
This problem was considered in [56] and the references therein.
As in Deﬁnition 2.29, let V denote the set of all subsets of V . The collection of triples
M = {(X, Y, S) ∈V × V × V | X ⊥Y∥GS}
(using the notation of Deﬁnition 2.17) represents the entire set of conditional indepen-
dence statements that it is possible to infer from the DAG, but this collection does not
necessarily represent the complete set of independence statements that hold for a collec-
tion of variables under a given probability distribution. When it does, it is known as a
perfect I-map.
Deﬁnition 2.30 (Perfect I-Map, Faithful) A DAG G = (V, E) over a set of variables V
is known as a perfect I-map for a probability function p over V if for any three disjoint
subsets of variables A, B and C,
A ⊥B|C ⇔A ⊥B ∥G C,
using the notation introduced in Deﬁnition 2.18, Equation (2.11). If G is a perfect I-map
for p, then G is said to be faithful to p.
If a collection of independence statements is in view, rather than a probability distribution
p, the term consistency is used to mean exactly the same thing.
Deﬁnition 2.31 (Consistency) Let V = {X1, . . . , Xd} be a collection of random variables
and let M denote the entire collection of conditional independence statements: that is, let
V denote the set of all subsets of V . Then for all (Xi, Xj, S) ∈V × V × V : Xi, Xj ̸∈S,
(Xi, Xj, S) ∈M ⇔Xi ⊥Xj|S.

70
CONDITIONAL INDEPENDENCE, GRAPHS AND d-SEPARATION
A directed acyclic graph G = (V, E) is consistent with a set of conditional independence
statements M if and only if
(Xi, Xj, S) ∈M ⇔Xi ⊥Xj∥GS.
Let p denote a probability distribution over a set of variables V = {X1, . . . , Xd} and let
Mp denote the set of conditional independence statements associated with p. That is, for
each (X, Y, S) ∈V × V × V, let (X, Y, S) ∈Mp ⇔X ⊥Y|S. Then a DAG G = (V, E)
is consistent with Mp if and only if G is faithful to p, if and only if G is a perfect I-map
of p.
A set of variables (X1, . . . , Xd), may be ordered in d! ways. Each permutation σ of
1, . . . , d gives an ordering (Xσ(1), . . . , Xσ(d)). Suppose that an ordering σ of the variables
is given and that, for each variable Xσ(j), a minimal set of σ-predecessors (σ)
j
is identi-
ﬁed that renders Xσ(j) independent of all the other σ-predecessors. A σ-predecessor for
variable Xσ(j) is a variable Xσ(i) such that i < j. Let j = {Xσ(1), . . . , Xσ(j)} A mini-
mal set is a set Uj ⊆j such that for any Y ∈Uj, then Xσ(j) will not be independent of
(j\Uj) ∪{Y} given Uj\{Y}. A direct link is then assigned from every variable in (σ)
j
to Xσ(j). The resulting DAG is minimal, in the sense that no edge can be deleted if the
DAG is to represent the probability distribution.
The input for this construction consists of a list L of d conditional independence
statements, one for each variable, all of the form {Xσ(j)} ⊥U σ
j |σ
j , where U σ
j is the
list of σ-predecessors of Xσ(j), without σ
j . This is equivalent to the statement that the
variable Xσ(j) is d-separated from the set of variables U σ
j when the set of variables σ
j
is instantiated.
For a given collection of variables V = {X1, . . . , Xd}, there may be several different
DAGs, each representing the same independence structure. Two DAGs which represent
exactly the same independence structure are said to be I-equivalent.
Deﬁnition 2.32 (I-sub-map, I-map, I-equivalence, Markov Equivalence) Let G1 and G2
be two DAGs over the same variables. The DAG G1 is said to be an I-sub-map of G2 if
any pair of variables d-separated by a set in G1 are also d-separated by the same set in
G2. They are said to be I-equivalent if G1 is an I-sub-map of G2 and G2 is an I-sub-map
of G1.
I-equivalence is also known as Markov equivalence.
Example 2.11
In the following example on three variables, all three factorizations
give the same independence structure. Consider a probability distribution pX1,X2,X3 with
factorization
pX1,X2,X3 = pX1pX2|X1pX3|X2.
It follows that
pX1,X2,X3 = pX2pX1|X2pX3|X1,X2 = pX2pX1|X2pX3|X2,
using Theorem 2.1, since X1 ⊥X3|X2. Also,
pX1,X2,X3 = pX3pX2|X3pX1|X2,X3 = pX3pX2|X3pX1|X2,

I-MAPS AND MARKOV EQUIVALENCE
71
X1
X2
X3
X2
X3
X2
X1
X1
X3
Figure 2.21
Three DAGs, each with the same independence structure.
since X1 ⊥X3|X2. For the ﬁrst and last of these, X2 is a chain node, while in the second
of these X2 is a fork node. The conditional independence structure associated with chains
and forks is the same. The three corresponding DAGs are given in Figure 2.21.
□
In general, the factorizations resulting from different orderings of the variables will
not necessarily give I-equivalent maps. This is illustrated by the following example on
four variables.
Example 2.12
Consider a probability distribution over four variables, which may be
factorized as
pX1,X2,X3,X4 = pX1pX2pX3|X1,X2pX4|X3.
If the distribution is factorized using the ordering (X1, X4, X3, X2), proceeding as out-
lined above, the following factorization results:
pX1,X2,X3,X4 = pX1pX4|X1pX3|X1,X4pX2|X1,X3,X4 = pX1pX4|X1pX3|X1,X4pX2|X1,X3
since X1 ̸⊥X4 and X2 ̸⊥X1|(X3, X4). The corresponding DAG gives less information
on conditional independence; it is not possible, with this ordering of the variables, to
conclude that X4 is conditionally independent of X1, given X3. The two corresponding
DAGs are shown in Figure 2.22.
□
While d-separated variables are conditionally independent conditioned on the sep-
arating set, it does not hold that conditionally independent variables are necessarily
d-separated; a necessary and sufﬁcient condition is that the DAG is a perfect I-map.
This result is stated in the following theorem.
Theorem 2.3 Recall Deﬁnition 2.18 and the notation introduced in Equation (2.11). Let
p be a probability function for a random vector X = (X1, . . . , Xd), factorized along a
DAG G. Then p and G are faithful to each other if and only if for any three disjoint sets
of variables A, B and C
A ⊥B ∥G C ⇔pA,B|C = pA|CpB|C.
X1
X2
X1
X2
X3
X3
X4
X4
Figure 2.22
DAGs with different independence structures, arising from different fac-
torizations of the same distribution.

72
CONDITIONAL INDEPENDENCE, GRAPHS AND d-SEPARATION
In other words, p and G are faithful to each other if and only if for any three disjoint sets
of variables A, B and C,
A ⊥B ∥G C ⇔A ⊥B|C.
Proof of Theorem 2.3 A ⊥B∥GC ⇒A ⊥B|C is the result of Theorem 2.2. If p
and G are faithful and A ⊥B|C then (by deﬁnition of faithfulness) A ⊥B∥GC. This
d-separation follows straight from the deﬁnition of faithfulness and is, indeed, the purpose
of the deﬁnition.
□
A DAG that is faithful to a probability distribution p may be considered as a true
representation of the distribution, in the sense that there are no artiﬁcial dependencies
introduced by the graph. That is, any statement inferred from the graph that two sets of
variables A and B are d-connected when a set S is instantiated implies that the sets of
variables A and B are not independent, given S.
2.10.1
The trek and a distribution without a faithful graph
Theorem 2.3 raises an important question; given a probability distribution pX over a
random vector X = (X1, . . . , Xd), is it always possible to ﬁnd a directed acyclic graph
that is faithful to the independence relations of p? The answer is no, as the following
basic example on four variables illustrates.
Deﬁnition 2.33 (Trek) Let G = (V, E) be a directed acyclic graph. A trek is a sub-graph
with four variables (X1, X2, X3, X4) where there are exactly four directed edges; X1 →
X2, X2 →X4, X1 →X3, X3 →X4. The sub-graph is illustrated in Figure 2.23.
The d-separation statements for the trek are: X1 ⊥X4∥G{X2, X3} and X2 ⊥X3∥GX1.
This is the entire list; X1 ̸⊥X4∥Gφ, X2 ̸⊥X3∥Gφ, X2 ̸⊥X3∥G{X4}, X2 ̸⊥X3∥G{X1, X4}.
These relations may be seen using the Bayes ball. There are exactly three directed acyclic
graphs that share the same d-separation properties as the trek; the other two are shown
in Figure 2.24.
Now consider a distribution over (X1, X2, X3, X4) such that the entire list of con-
ditional independence statements is X1 ⊥X4|{X2, X3}, X1 ⊥X4, X2 ⊥X3|X1. Such a
distribution is given by taking a distribution that factorizes along the trek;
pX1,X2,X3,X4 = pX1pX2|X1pX3|X1pX4|X2,X3,
X2
X1
X4
X3
Figure 2.23
A trek.

I-MAPS AND MARKOV EQUIVALENCE
73
X2
X1
X4
X3
X2
X1
X4
X3
Figure 2.24
Graphs with the same d-separation properties as Figure 2.23.
where X1, X2, X3, X4 are each binary variables, taking values 1 or 0 and where
pX2|X1(1|0) = 1 −pX3|X1(1|1) = a,
pX2|X1(1|1) = 1 −pX3|X1(1|0) = b,
pX4|X2,X3(1|11) = pX4|X2,X3(1|00) = c,
pX4|X2,X3(1|01) = pX4|X2,X3(1|10) = d.
Then
pX4|X1(1|1) = c ((1 −a)b + a(1 −b)) + d (ab + (1 −a)(1 −b))
while
pX4|X1(1|0) = c (a(1 −b) + (1 −a)b) + d ((1 −a)(1 −b) + ab)
so that X4 ⊥X1. Take a ̸= 1
2, b ̸= 1
2, c ̸= 1
2, d ̸= 1
2, a ̸= b, c ̸= d. Then, with these
conditions, X4 ̸⊥X3|S for any S ⊆{X1, X2} and X4 ̸⊥X3|S for any S ⊆{X1, X2}. Fur-
thermore, X3 ̸⊥X1|S for any S ⊆{X2, X4} and X2 ̸⊥X1|S for any S ⊆{X3, X4}. The
independence relation X3 ⊥X2|X1 holds and clearly X1 ⊥X4|{X2, X3} holds.
There does not exist a directed acyclic graph that expresses all the conditional inde-
pendence relations that hold for this distribution and no others. If X1 ⊥X4, then there
can be no edge between X1 and X4. If X2 ⊥X3|X1 then there can be no edge between
X2 and X3. Furthermore, any trail between X1 and X4 must contain a collider connection.
The lack of d-separation implies that there are edges between X1 and X2, X1 and X3,
X2 and X4, X3 and X4. This implies that the nodes X2 and X3 are collider nodes. This
contradicts the requirement that X1 ⊥X4|{X2, X3}, since the instantiation of a collider
connection opens the communication.
Therefore, there is no directed acyclic graph that is faithful to the distribution
described above when a ̸= 1
2, b ̸= 1
2, c ̸= 1
2, d ̸= 1
2, a ̸= b, c ̸= d.
□
Notes
Perhaps the earliest work that uses directed graphs to represent possible depen-
dencies among random variables is that by S. Wright [57]. A recent presentation of the
path analysis methods due to Wright is found in Shipley [58] An early article that con-
sidered the notion of a factorization of a probability distribution along a directed acyclic
graph representing causal dependencies is that by H. Kiiveri, T.P. Speed and J.B. Carlin
[59], where a Markov property for Bayesian networks was deﬁned. This was developed

74
CONDITIONAL INDEPENDENCE, GRAPHS AND d-SEPARATION
by J. Pearl in [9]; d-separation, and the extent to which it characterises independence is
discussed by J. Pearl and T. Verma in [60] and by J. Pearl, D. Geiger and T. Verma in
[56]. The Bayes ball is taken from R.D. Schachter [50]. See, for example, F. Markowetz
and R. Spang [7] for applications of Bayesian networks to cellular models. The applica-
tion to root cause analysis was discussed in [51]. The counter example showing that it is
necessary that the probability function factorizes along a directed acyclic graph is found
in N. Vorobev [53]. Object oriented Bayesian networks are discussed in [54]. The results
for identifying independence in Bayesian networks are taken from D. Geiger, T. Verma
and J. Pearl [61] and the results on Markov equivalence are taken from T. Verma and
J. Pearl [62]. The web page in [63] contains links to an introduction to Bayesian nets
and the Bayes Net Toolbox MATLAB software.

CONDITIONAL INDEPENDENCE, GRAPHS AND d-SEPARATION
75
2.11
Exercises: Conditional independence and
d-separation
The ‘HUGIN’ software (the educational package) may be used for the analysis
of Bayesian networks. A description of the package, and information about how
to obtain it, may be found at http://www.hugin.com/Products_Services/Products/
Academic/Educational/
1. Let (X, Y, W, Z) be discrete random variables, each with a ﬁnite state space. Let X
denote the state space of (X, Y, W, Z).
(a) Prove that if X ⊥(Y, W)|Z then X ⊥Y|Z and X ⊥W|Z.
(b) Prove that if X ⊥Y|Z and X ⊥W|(Y, Z) then X ⊥(W, Y)|Z.
(c) Assume that pX,Y,W,Z(x, y, w, z) > 0 for each (x, y, w, z) ∈X. Prove that if X ⊥
Y | (Z, W) and X ⊥W | (Z, Y), then X ⊥Z | (Y, W).
2. Let A, B and C be binary random random variables, each of which takes values in
{0, 1}. Suppose that the joint probability function for (A, B, C) is given by
pA,B,C (0, 0, 0) = 0.028 pA,B,C (0, 0, 1) = 0.042
pA,B,C (0, 1, 0) = 0.00003 pA,B,C (0, 1, 1) = 0.02997
pA,B,C (1, 0, 0) = 0.072 pA,B,C (1, 0, 1) = 0.108
pA,B,C (1, 1, 0) = 0.00072 pA,B,C (1, 1, 1) = 0.71928
Show that the probability function pA,B,C admits a factorization according to the
DAG given in Figure 2.25.
A
B
C
Figure 2.25
Chain connection.
3. Suppose that the following table gives the values for the joint probability function
pA,B:
pA,B(., .) =
b1
b2
b3
a1
0.02
0.03
0.15
a2
0.10
0.00
0.30
a3
0.05
0.15
0.20
Compute pA, pB, pA|B, pB|A.
4. This is the classic example used to illustrate ‘explaining away’. A former prime
minister and Labour Party leader is now a business consultant. He is in his ofﬁce in
Stockholm, when he receives the news that the burglar alarm in his country mansion
has gone off. Convinced that a burglar has broken in, he starts to drive home. But,
on his way, he hears on the radio that there has been a minor earth tremor in the

76
EXERCISES: CONDITIONAL INDEPENDENCE AND d-SEPARATION
area. Since an earth tremor can set off a burglar alarm, he therefore returns to his
ofﬁce.
(a) Construct the Bayesian network associated with the situation.
(b) Suppose that the variables are listed as R for the radio broadcast (y/n), A for the
alarm (y/n), B for the burglary (y/n) and E for the earthquake (y/n), where y
stands for ‘yes’ and n stands for ‘no’. Suppose that the conditional probability
tables associated with the Bayesian network are
pR|E =
R\E
n
y
n
0.99
0.05
y
0.01
0.95
pA|B,E(y|., .) =
E\B
n
y
n
0.03
0.95
y
0.95
0.98
pB =
n
y
0.99
0.01
pE =
n
y
0.001
0.999
Find
i. pB|A(y|y), pB|A(y|n) and
ii. pB|A,R(y|y, y).
5. a(a) Consider the network given in Figure 2.26, where variables B and J have
been instantiated. Which variables are d-separated from A? Which variables
are d-separated from F? Explain.
J = 1
G
H
D
I
F
B = 1
E
C
A
Figure 2.26
Network.

CONDITIONAL INDEPENDENCE, GRAPHS AND d-SEPARATION
77
B
A
D
E
C
G
H
F
J = 1
I
Figure 2.27
Network.
(b) Consider the network given in Figure 2.27, where variable J has been instanti-
ated. Which variables are d-separated from A? Which variables are d-separated
from F? Explain.
6. Let A be a variable in a DAG. Prove that if all the variables in the Markov blanket of
A are instantiated, then A is d-separated from the remaining uninstantiated variables.
7. Recall the deﬁnition of I-sub-maps and I-equivalence given in Deﬁnition 2.32.
(a) Find those graphs from the four given in Figure 2.28 that are I-equivalent to
each other.
(b) Suppose that the probability distributions p, q and r over the variables A, B and
C may be factorized as pApB|ApC|A, qBqA|BqC|A and rBrCrA|B,C. Do p and q
have the same independence structure? Do p and r have the same independence
structure?
8. Consider the ﬁrst DAG in Figure 2.28 (a fork connection centred at A) and suppose
that pA,B,C may be factorized along the DAG.
(a) Prove that
pB|A,C = pB|A.
A
A
B
C
B
C
A
A
B
C
B
C
Figure 2.28
Which are I equivalent?

78
EXERCISES: CONDITIONAL INDEPENDENCE AND d-SEPARATION
(b) Suppose that pA,B,C = pApB|ApC|A, with probability tables given by
pA =
a1
a2
0.3
0.7
pB|A =
A\B
b1
b2
a1
0.1
0.9
a2
0.8
0.2
pC|A =
A\C
c1
c2
a1
0.6
0.4
a2
0.4
0.6
Using appropriate operations of tables, compute
pA,B, pB, pB,C, pC|B
and
pA,B,C.
9. Let T denote a test result (positive or negative) for the event A (whether or not a
driver has too much alcohol in the blood). Suppose the conditional probabilities pT |A
are given in the following table (where y = ‘yes’ and n = ‘no’):
pT |A(.|.) =
A\T
y
n
y
0.95
0.05
n
0.005
0.995
(a) The police may stop a motorist and perform a blood test on suspicion that the
motorist is driving under the inﬂuence of alcohol. Experience suggests that 15%
of drivers under suspicion do, in fact, drive with too much alcohol. Compute the
table pA|T . A driver is taken and the blood test is positive. What is the probability
that the driver has too much alcohol?
(b) One week, the policy changes so that the police stop drivers randomly and carry
out the same test. It is estimated that one in 2000 drivers stopped at random have
too much alcohol in their blood. Compute the table pA|T . A driver is stopped
and gives a positive test result. What is the probability that he is driving under
the inﬂuence of alcohol?
10. Conditional independence Consider three random variables (X, Y, Z) with joint
probability function pX,Y,Z. Prove that X ⊥Y|Z if and only if
pX,Y,Z(x, y, z)pX,Y,Z(x′, y′, z) = pX,Y,Z(x′, y, z)pX,Y,Z(x, y′, z)
∀(x, y, x′, y′, z).
This observation is due to B. Sturmfels and has important consequences for the
application of techniques from algebraic geometry to the study of Bayesian networks
(see [64]). The quantity
(x, y; x′, y′; z) := pX,Y,Z(x, y, z)pX,Y,Z(x′, y′, z) −pX,Y,Z(x′, y, z)p(x, y′, z)
is known as the cross product difference and is identically zero if and only if X ⊥
Y|Z.

CONDITIONAL INDEPENDENCE, GRAPHS AND d-SEPARATION
79
H1
H2
H3
H4
S1
S2
S3
F
Figure 2.29
Root Cause Analysis.
11. Root Cause Analysis Are the variables F and (H2, H3) d-connected when (S1, S3)
are instantiated in the directed acyclic graph in Figure 2.29?
12. Transitive DAGs A DAG G = (V, E) is deﬁned as a transitive DAG if it satisﬁes
the following additional condition:
(Xi, Xj) ∈E and (Xj, Xr) ∈E ⇒(Xi, Xr) ∈E.
Let an (Xν) denote the set of ancestors of Xν. Check that a DAG G is transitive if
and only if for every ν
 (Xν) = an (Xν) .
13. Transitive DAGs Assume that G = (V, E) is a transitive DAG following the deﬁ-
nition in Exercise 12 and that p a probability distribution over the variables in V in
G = (V, E). Show that p satisﬁes local directed Markov condition with respect to
G (Deﬁnition 2.28) if and only if for all X and Y in V , X ⊥Y | X ∩Y or, equiva-
lently X \ Y ⊥Y \ X | X ∩Y. This is known as the lattice conditional independence
property (LCI). In other words, the LCI is characterized by a Markov model on a
transitive DAG. This result is found in [55].
14. The notation XA is used to denote the random (row) vector of all variables in set
A. Let V = {X1, . . . , Xd} be the d variables of a Bayesian network and assume that
XV \{Xi} = w. That is, all the variables except Xi are instantiated. Assume that Xi is
a binary variable, taking values 0 or 1. Consider the odds
Op

{Xi = 1} | {XV \{Xi} = w}

,
and show that this depends only on the variables in the Markov blanket (Deﬁnition
2.20) of Xi.


3
Evidence, sufﬁciency and Monte
Carlo methods
Let X = (X1, . . . , Xd) denote a random vector, with ﬁnite state space X = ×d
j=1Xj,
where Xj = (x(1)
j , . . . , x
(kj )
j
) is the state space for random variable Xj. This chapter
considers the various kinds of evidence available: hard evidence, soft evidence and virtual
evidence. The deﬁnitions used will be the following:
Deﬁnition 3.1 (Hard Evidence, Soft Evidence, Virtual Evidence) The following deﬁni-
tions will be used:
• A hard ﬁnding is an instantiation, {Xi = x(l)
i } for a particular value of i ∈{1, . . . , d}
and a particular value of l ∈{1, . . . , ki}. This speciﬁes that variable Xi is in state
x(l)
i . It is expressed as a k1 × . . . × kd potential e where
e(x(p1)
1
, . . . , x(pd)
d
) =
 1
pi = l
0
pi ̸= l.
That is, the entries corresponding to conﬁgurations containing the instantiation are
1 and the entries corresponding to all other conﬁgurations are 0.
• Hard evidence is a collection of hard ﬁndings. It is given by a collection of potentials
e = (e1, . . . , em) where em is a hard ﬁnding on one of the variables.
• A soft ﬁnding on a variable Xj speciﬁes the probability distribution of the variable
Xj. That is, the potential pXj |j is replaced by a potential p∗
Xj with domain Xj.
Bayesian Networks: An Introduction
T. Koski, J. Noble
2009 John Wiley & Sons, Ltd

82
EVIDENCE, SUFFICIENCY AND MONTE CARLO METHODS
• Soft evidence is a collection of soft ﬁndings.
• A virtual ﬁnding on variable Xj is a collection of likelihood ratios {L(x(m)
j
), m =
{1, . . . , m}} such that the updated conditional probability potential for Xj|j =
π(n)
j
is, for m = 1, . . . , kj,
p∗
Xj |j (x(m)
j
|π(n)
j ) =
1
kj
q=1 pXj |j (x(q)
j |π(n)
j )L(x(q)
j )
pXj |j (x(m)
j
|π(n)
j )L(x(m)
j
).
(3.1)
• Virtual evidence is a collection of virtual ﬁndings.
Soft evidence and virtual evidence are different. When soft evidence is received on
a variable, the links between the variable and its parents are severed; if soft evidence
is received on variable Xj, then the potential pXj |j is replaced by a new potential
p∗
Xj . When virtual evidence is received, the links are preserved. Virtual evidence may be
modelled by adding a new node to the network.
3.1
Hard evidence
Consider a situation where the domain variables are {X, Y}, XX = {x1, x2, x3} and XY =
{y1, y2, y3}. The following is an example of a ﬁnding e that Y is in state y1.
e ↔
X\Y
y1
y2
y3
x1
1
0
0
x2
1
0
0
x3
1
0
0
Entering hard evidence
Let pX denote a joint probability function over a random
vector X and let e be a ﬁnding corresponding to a piece of evidence. The evidence is
entered into pX by multiplying the potentials together. Let pX;e denote the result;
pX;e = pX.e
(3.2)
Suppose that several hard ﬁndings are received, expressed in potentials (e1, . . . , ek). Then
the hard evidence e = (e1, . . . , ek) is entered by
pX;e = pX.
k
j=1
ej,
where the multiplication is in the sense of potentials, having been extended to the
domain X.
Hard evidence renders certain states impossible, giving value 0 for the corresponding
conﬁgurations in the potential and leaves the other conﬁgurations unaltered. The potential

HARD EVIDENCE
83
	k
j=1 ej (where the domains have been extended to X) may be considered as a hard
evidence potential with domain X. Clearly, multiplication of hard evidence potentials
yields another hard evidence potential.
The potential pX;e is not a probability function, in the sense that the entries do not add
up to 1. To compute the conditional probability potential, conditioned on the evidence
received, it is necessary to compute the probability of the evidence. This is given by
p(e) =

x∈X
pX;e(x)
and the conditional probability potential is given by
pX|e = pX;e
p(e),
(3.3)
where the division is taken in the sense of potentials (Deﬁnition 2.24).
Example 3.1
Consider a joint probability function
pX,Y =
X\Y
y1
y2
y3
x1
0.05
0.10
0.05
x2
0.15
0.00
0.25
x3
0.10
0.20
0.10
and suppose that evidence is received that {Y = y1}. The evidence potential e is
e =
X\Y
y1
y2
y3
x1
1
0
0
x2
1
0
0
x3
1
0
0
Entering the evidence gives
pX,Y;e = pX,Y.e =
X\Y
y1
y2
y3
x1
0.05
0
0
x2
0.15
0
0
x3
0.10
0
0
This potential is not a probability function; the numbers do not add up to 1, but the
conditional probability potential may be computed by ﬁrst computing the probability of
the evidence;
p(e) =

x,y
pX,Y;e(x, y) = 0.05 + 0.15 + 0.10 = 0.30.

84
EVIDENCE, SUFFICIENCY AND MONTE CARLO METHODS
The conditional probability of (X, Y) given the evidence is then computed using the
rules for ‘division of potentials’ (Deﬁnition 2.24):
pX,Y|e = pX,Y;e
p(e) =
X\Y
y1
y2
y3
x1
1
6
0
0
x2
1
2
0
0
x3
1
3
0
0
.
Example 3.2
Consider an example where X, Y and Z each take on values 0 or 1 and
the joint probability function of (X, Y, Z) may be factorized according to the DAG in
Figure 3.1. Z is the parent, X and Y are the children.
The factorization of the joint probability function corresponding to the DAG is
pX,Y,Z = pX|ZpY|ZpZ.
Suppose that a hard ﬁnding is received, that {Y = 0}. The evidence potential is
e =
Z\Y
0
1
0
1
0
1
1
0
Now,
pX,Y,Z;e = pX|ZpY|ZpZe
so that, by the multiplication rules for potentials,
pY|ZpZe ↔
Z\Y
0
1
0
pY|Z(0|0)pZ(0)
0
1
pY|Z(0|1)pZ(1)
0
By multiplication rules for potentials with different domains,
pX|ZpY|ZpZe ↔
X\Z\Y
0
1
0
(a, 0)
(b, 0)
1
(c, 0)
(d, 0)
Z
X
Y
Figure 3.1
Graph for Example 3.2.

SOFT EVIDENCE AND VIRTUAL EVIDENCE
85
where
a = pX|Z(0|0)pY|Z(0|0)pZ(0),
b = pX|Z(1|0)pY|Z(0|0)pZ(0),
c = pX|Z(0|1)pY|Z(0|1)pZ(1),
d = pX|Z(1|1)pY|Z(0|1)pZ(1).
By disregarding the zero elements in the potential, ‘entering the evidence’ has resulted
in a potential deﬁned over the domain XX × XY
pX,Y,Z;e(., 0, .) = pX|Z(.|.)pY|Z(0|.)pZ(.).
Let v denote the set of parent variables of Xv and let πv denote the function such that
πv(x1, . . . , xn) = (xj1, . . . , xjv) if and only if v = {Xj1, . . . , Xjv}, for 1 ≤j1 < . . . <
jv ≤n. Let x = (x1, . . . , xd). The general rule for hard evidence is, for a collection of
hard evidence potentials e = (e1, . . . , em),
pX1,...,Xd;e(x1, . . . , xd) =
n

v=1
pXv|v(xv|πv(x))
m

i=1
ei.
3.2
Soft evidence and virtual evidence
Soft evidence and virtual evidence (Deﬁnition 3.1) are both types of evidence that affect
the probabilities of a state, but which do not enable any claim to be made that the
probability of a state is zero. A virtual ﬁnding on variable Xj affects the probability
potential pXj |j , without affecting the subsequent conditional probability potentials. The
virtual ﬁnding may be considered as an additional variable V (denoting ‘virtual ﬁnding’),
that is added into the network, with a single arrow from V , to the variable affected by
the soft evidence. This is the situation in which Pearl’s method of virtual evidence is
applicable, depending on how the evidence is formulated.
Example 3.3
Consider a DAG on ﬁve variables, X1, X2, X3, X4 and X5, given in
Figure 3.2. Suppose that a piece of virtual evidence is received on the variable X3. This
evidence may be modelled by a variable V , that is inserted to the DAG giving the DAG
in Figure 3.3. The variable X3 is in a certain state, which affects the soft evidence that
is observed.
It is clear that (X1, X2, X4, X5) ⊥V ∥GX3. In particular, the decomposition along
the DAG gives p(V |X1, X2, X3, X4, X5) = p(V |X3) and p(X1, X2, X4, X5|X3, V ) =
p(X1, X2, X4, X5|X3).
In general, consider a set of variables V = {X1, . . . , Xd}, where the joint probability
distribution is factorized as
pX1,...,Xd =
d
j=1
pXj |j .

86
EVIDENCE, SUFFICIENCY AND MONTE CARLO METHODS
X1
X2
X3
X4
X5
Figure 3.2
Before virtual evidence is added.
X1
X2
X3
X4
V
X5
Figure 3.3
After the virtual evidence node is added.
Suppose that virtual evidence is received on variable Xj. This may be expressed as a
variable V and, by d-separation properties, the updated distribution pX1,...,Xd,V has a
factorization
pX1,...,Xd,V =


k̸=j
pXk|k

pXj |j ∪{V }pV .
(3.4)
The variable V is a ‘dummy variable’, in the sense that its state space and distribution
do not need to be deﬁned; the virtual evidence is interpreted as a particular instantiation
{V = v} for this variable and this is the only information that is needed. From Equation
(3.4),
pX1,...,Xd|V (., . . . , .|v) =


k̸=j
pXk|k

pXj |j ∪{V }(.|., v).
From Equation (3.1),
L(x(m)
j
)
kj
i=1 L(x(i)
j )pXj |j (x(i)
j |π(n)
j )
= pXj |j ∪{V }(x(i)
j |π(n)
j , v)
m = 1, . . . , kj.
This is the setting where Pearl’s update method may be applied.
3.2.1
Jeffrey’s rule
The deﬁnition of the Jeffrey’s update was given in Equation (1.9) and is restated here. Let
p denote the original probability and let q denote the probability obtained after the update
is applied. Let G1, . . . , Gr denote the set of mutually exclusive and exhaustive events

SOFT EVIDENCE AND VIRTUAL EVIDENCE
87
for which the updated probability q(Gi) is prescribed. Set λi = q(Gi) and µi = p(Gi).
For any set A ⊂X and any x ∈X, let 1A denote the indicator function;
1A(x) =

1
x ∈A
0
x ̸∈A.
Then Jeffrey’s rule may be rewritten in the following way: for all x ∈X,
q({x}) =
r

j=1
λj
µj
p({x})1Gj (x).
The following example is taken from [65] and discussed in [66].
Example 3.4
A piece of cloth is to be sold on the market. The colour C is either green
(cg), blue (cb) or violet (cv). Tomorrow, the piece of cloth will either be sold (s) or
not (sc); this is denoted by the variable S. Experience gives the following probability
distribution over C,S
pC,S =
S\C
cg
cb
cv
s
0.12
0.12
0.32
sc
0.18
0.18
0.08
The marginal distribution over C is
pC =
cg
cb
cv
0.3
0.3
0.4 .
The piece of cloth is inspected by candle light. Since it cannot be seen perfectly, this
only gives soft evidence. From the inspection by candle light, the probability over C is
assessed as:
qC =
cg
cb
cv
0.7
0.25
0.05 .
This is a situation where Jeffrey’s rule may be used to update the probability. For
example,
qS,C(s, cg) = λg
µg
p(s, cg) = 0.7
0.3 × 0.12 = 0.28.
Updating the whole distribution in this way gives
qC,S =
S\C
cg
cb
cv
s
0.28
0.10
0.04
sc
0.42
0.15
0.01
3.2.2
Pearl’s method of virtual evidence
Pearl’s method deals with virtual evidence. If virtual evidence is received on a variable
in a Bayesian network, it may be treated by adding an additional node V . If virtual

88
EVIDENCE, SUFFICIENCY AND MONTE CARLO METHODS
evidence is received on variable X3 in the network in Figure 3.2, the network may be
extended to the network in Figure 3.3 as before. Pearl’s method of virtual evidence was
introduced in Section 1.4.3 and considers the situation a piece of soft evidence {V = v},
which affects a set of mutually exclusive and exhaustive events G1, . . . , Gr, comes in
the form of an odds ratio. The evidence is not speciﬁed as a set of new probabilities, but
rather for each Gj, j = 1, . . . , r, the ratio λj = q({V =v}|Gj )
q({V =v}|G1), j = 2, . . . , r is given, with
λ1 = 1. The parameter λj therefore represents the likelihood ratio of the event Gj to the
event G1 given the evidence E. Again, using the notation 1Gj (x) = 1 for x ∈Gj and 0
for x ̸∈Gj, Pearl’s method of virtual evidence deﬁnes the update q(.) as
q({x}) = p({x})
r

j=1
λj
r
k=1 p(Gk)λk
1Gj (x),
x ∈X.
Example 3.5 (Burglary)
The following example is taken from [9] and discussed
in [66]. It has become a classic example. A variant is included as Exercise 4 in
Chapter 2.
On any given day, there is a burglary at any given house with probability 10−4. If there
is a burglary, then the alarm will go off with probability 0.95. One day, Professor Noddy
receives a call from his neighbour Jemima, saying that she may have heard Professor
Noddy’s burglar alarm going off. Professor Noddy decides that there is an 80% chance
that Jemima did hear the alarm going off.
Let A denote the event that the alarm goes off, B the event that a burglary takes place
and let E denote the evidence of the telephone call from Jemima. According to Pearl’s
method, this evidence can be interpreted as
λ = p(E|A)
p(E|Ac) = 4.
An application of Pearl’s virtual evidence rule gives the updated probability that a bur-
glary has taken place as q(B) = 3.85 × 10−4.
3.3
Queries in probabilistic inference
The following are examples of queries for a Bayesian network with the variables in U:
Probability updating namely, if evidence e is given on some variables, ﬁnd the posterior
probability potentials for the rest of the variables.
Most probable conﬁguration namely, if evidence e is given on the variables in a set U,
ﬁnd the most probable values of the rest of the variables.
Maximum aposterior (map) hypothesis namely, if evidence e is given on some variables
in a set U, ﬁnd a hypothesis h over a subset of variables which maximizes the probability
p(h|e).

BUCKET ELIMINATION
89
visit Asia
smoking
tuberculosis
lung cancer
bronchitis
tuberculosis or
cancer
breathlessness
positive X ray
Figure 3.4
The Chest Clinic Problem.
3.3.1
The chest clinic problem
The following is an example of a query. It is known as ‘The Chest Clinic Problem’ and
is due to A.P. Dawid. It may be found in [67]. You are ill, short of breath, and you want
to know what is wrong with you. The corresponding DAG is given in Figure 3.4.
Chapters 4 and 10 develop a method for the ﬁrst query listed, updating the probability
distribution when evidence is received. As pointed out by A.P. Dawid, in [68], this method
may equally well be applied to the problem of ‘maximum aposterior hypothesis’, with
only minor modiﬁcations.
3.4
Bucket elimination
Most of this text is concerned with ﬁnding effective algorithmic solutions to probability
updating, most probable conﬁguration and maximum aposterior hypothesis. The ﬁrst
technique considered is an algorithm known as bucket elimination. It was introduced by
R. Dechter in [69].
Consider the DAG in Figure 3.5.
Here
pA,B,C,D,F,G = pG|F pF|C,BpD|B,ApB|ApC|ApA.
Assume there is a ﬁnding G = 1, and the updated probability
pA|e = pA|G(.|1)
is required. To compute this, both pA;e and p(e) are needed. Since e = {G = 1},
pA;e = pA,G(., 1) =

B,C,D,F
pG|F (1|.)pF|C,BpD|B,ApC|ApB|ApA.

90
EVIDENCE, SUFFICIENCY AND MONTE CARLO METHODS
A
B
C
D
F
G
Figure 3.5
Illustration for bucket elimination.
This may be manipulated symbolically using x(y + z) = xy + xz. This is the associative
law, which may seem mathematically trivial, but efﬁcient algorithms will exploit it as
much as possible. The left hand side requires two computations (y + z, followed by a
multiplication by x) while the right hand side requires three (xy, then xz, then add them
together).
pA;e = pA

C
pC|A

B
pB|A

F
pF|C,BpG|F (1|.)

D
pD|B,A.
Note that 
D pD|B,A = 1, so that
pA;e = pA

C
pC|A

B
pB|A

F
pF|C,BpG|F (1|.).
Set
λC,B;G=1(., .)
def
=

F
pF|C,BpG|F (1|.),
so that
pA;e = pA

C
pC|A

B
pB|AλC,B;G=1(., .)
and set
λC,A;G=1(., .)
def
=

B
pB|AλC,B;G=1(., .),
so that
pA;e = pA

C
pC|AλC,A;G=1(., .).
Set
λA;G=1(.)
def
=

C
pC|AλC,A;G=1(., .)

BUCKET ELIMINATION
91
so that ﬁnally
pA;e = pAλA;G=1(.).
Here all the variables except for A have been eliminated in a certain order. Let
U = {A, B, C, D, F, G},
then
pA;e =

U\{A,G}
pA,B,C,D,F,|G(., ., ., ., .|1).
The preceding can be described as follows. First, the conditional probability potentials
are put into buckets relative to the order to be used in elimination of variables. The
ﬁrst bucket (bucketG) contains all the potentials that have G in the domain. Bucket
bucketD contains all those, from the remaining potentials that contain D in the domain
and so on.
The buckets are therefore:
bucketG = pG|F (1|.),
bucketD = pD|B,A,
bucketF = pF|B,C,
bucketB = pB|A,
bucketC = pC|A,
bucketA = pA.
It is shown diagrammatically as follows. Each arrow corresponds to marginalization and
multiplication.
bucketG
pG|F (1|.)
↘
bucketD
pD|B,A
λ(.) = pG|F (1|.)
↘
↓
bucketF
pF|B,C
λ(.) = pG|F (1|.)
↘
↓
bucketB
pB|A
λC,B;G=1(., .)
↘
↓
bucketC
pC|A
λC,A;G=1(., .)
↘
↓
bucketA
pA
λA;G=1(.).
The algorithm is summarized as follows: start with a set of potentials V . Whenever a
variable X is to be removed, all potentials from V with X in the domain are taken, and
removed from V . Their product is calculated and the potential obtained by summing over
the X variable is computed from this product. The resulting potential λ is then added to
V . Repeat with the next variable.

92
EVIDENCE, SUFFICIENCY AND MONTE CARLO METHODS
3.5
Bayesian sufﬁcient statistics and prediction sufﬁciency
The conditional independence structure is central to the analysis of a Bayesian network.
Sufﬁcient statistics, a central concept in statistics, are random variables that help to estab-
lish conditional independence. Suppose that X is an n × d random matrix, representing
n independent copies of a random (row) vector X and  is a random vector. A function
t of X such that X and  are independent given t(X) is a Bayesian sufﬁcient statistic
for . The notation is suggestive; an instantiation of the random matrix X represents the
results of n independent replications of an experiment, where d attributes are measured,
while  is the random vector associated with the parameters. In this text, a random
vector that models the outcome of an experiment is, for the most part, discrete and the
random vector associated with the parameters is generally taken to be continuous. In
the analysis that follows, it will be assumed that X is discrete and  is continuous,
although the results are general and the proofs are easily modiﬁed to deal with the other
cases.
3.5.1
Bayesian sufﬁcient statistics
Let X be an n × d random matrix, where each row is an independent copy of a dis-
crete random vector and let  be a continuous random vector representing the unknown
parameters, from the Bayesian point of view. Suppose that, conditioned on  = θ, X has
conditional probability function pX|(.|θ). Suppose that  has prior density π

θ

and
suppose that t is a function or a statistic of X,
t = t (X) .
Deﬁnition 3.2 (Statistic) A statistic is the result of applying a function, or statistical algo-
rithm, to a set of data. More formally, a statistic is a function of a random sample (taken in
the sense of random variables), where the function itself is independent of the distribution
of the sampling distribution. The term ‘statistic’ is used both for the function and the value
of the function on a given sample.
Deﬁnition 3.3 (Bayesian Sufﬁciency) A statistic T deﬁned as T = t (X) such that for
every prior π within the space of prior distributions under consideration, there is a
function φ such that
π|X(θ|x) = pX|(x|θ)π(θ)
p X(x)
= φ(θ, t(x))
(3.5)
is called a Bayesian sufﬁcient statistic for .
This deﬁnition states that for learning about  based on X, the statistic T contains all
the relevant information, since the posterior distribution depends on X only through T .
The following result shows that conditional independence of X and  given t(X)
implies Bayesian sufﬁciency. If the families of probability measures have ﬁnite dimen-
sional parameter spaces, then the converse is also true. If there are an inﬁnite number of
parameters, counter examples may be obtained to the converse statement.

BAYESIAN SUFFICIENT STATISTICS AND PREDICTION SUFFICIENCY
93
Proposition 3.1 Let t denote a function and let T = t(X). If
X ⊥|T,
(3.6)
then T = t(X) is a Bayesian sufﬁcient statistic for .
Before proving this proposition, the following lemma is required.
Lemma 3.1 Let X be a discrete random matrix, T = t(X) where t is a function and let
 be a continuous random vector. Let X ⊥|T , then
pX|T (x|t) = pX|T,(x|t, θ).
(3.7)
Proof of Lemma 3.1 Let θ ⊂˜ and set Aθ,ϵ = {ψ ∈˜||ψ −θ| < ϵ}. Recall that, in
this text, if there are d parameters, then ˜ ⊆Rd, where ˜ is the parameter space. The
distance |ψ −θ| is deﬁned as the Euclidean distance:
|ψ −θ| :=




d

j=1
(ψj −θj)2.
Since X ⊥|T , it follows that
p({X = x}|{T = t}, { ∈Aθ,ϵ}) = p({X = x}, { ∈Aθ,ϵ}|{T = t})
p({ ∈Aθ,ϵ}|{T = t})
= p({X = x}|{T = t})p({ ∈Aθ,ϵ}|{T = t})
p({ ∈Aθ,ϵ}|{T = t})
= p({X = x}|{T = t}).
Since this holds for all ϵ > 0, the result follows by letting ϵ →0.
□
Proof of Proposition 3.1 In the set up considered here, X is considered to be a discrete
random n × d matrix (representing n replications of an experiment where d attributes
are recorded), while  is considered to be a continuous random vector. As usual, let
T = t(X).
An application of Bayes’ rule gives
π|X,T (θ|x, t) = pX,T |(x, t|θ)π(θ)
pX,T (x, t)
= pX|T,(x|t, θ)pT |(t|θ)π(θ)
pX,T (x, t)
,
(3.8)
and Equation (3.8), together with an application of Equation (3.7) gives
π|X,T (θ|x, t) = pX|T (x|t)pT |(t|θ)π(θ)
p X|T ( x|t)pT (t)
= pT |(t|θ)π(θ)
pT (t)
= π|T (θ|t).
(3.9)
The proposition is proved by setting φ(θ, t(x)) = π|T (θ|t(x)).
□

94
EVIDENCE, SUFFICIENCY AND MONTE CARLO METHODS
Example 3.6 (Tossing a Thumb-tack)
In the thumb-tack experiment described in
Section 1.8, there is a single parameter, θ. In this paragraph, a Bayesian sufﬁcient statistic
is derived for θ, for a suitable class of prior distributions. Let π denote the prior density
function for θ, and let  denote the random variable with this density function. In this
case, X is a n × 1 matrix, a column vector, which will be written as X = (X1, . . . , Xn)t,
a sequence of n independent Bernoulli trials, each with probability θ of success (that is
Xj ∼Be(θ), j = 1, . . . , n and Xi ⊥Xj| for i ̸= j). The sequence of outcomes will
be denoted by the vector
x = (x1, . . . , xn)t.
That is, for each j = 1, . . . , n, xj = 1 or 0. The statistic t is a function of n variables,
deﬁned as
t(x) =
n

j=1
xj.
That is, when t is applied to a sequence of n 0s and 1s, it returns the number of 1s in
the sequence. Here, T = t(X) = n
j=1 Xj and therefore T has a binomial distribution
with the parameters n and θ, since it is the sum of independent Bernoulli trials. The
probability function of T is given by
pT |(k|θ) =




n
k

θk(1 −θ)n−k
k = 0, 1, . . . , n
0
other
k.
Since t is a function of x, it follows that
pX,T |(x, k|θ) =

θk(1 −θ)n−k
k = 0, 1, . . . , n
0
other
k
from which
pX|T,(x|k, θ) = pX,T |(x, k|θ)
pT |(k|θ)
=
1
 n
k
.
The right hand side does not depend on θ, from which Equation (3.7) holds and hence
Equation (3.6) follows. Therefore, if x = (x1, . . . , xn) are n independent Bernoulli trials,
each with parameter θ, the function t such that t(x) = n
l=1 xl is a Bayesian sufﬁcient
statistic for the parameter θ. In the thumb-tack example, given in Section 1.8, the posterior
distribution, based on a uniform prior is an explicit function of the data x only through
the function t(x).
Now consider a random vector X and suppose now that t is a generic sufﬁcient
statistic. Since t is a function of X (i.e. t = t(X)), it follows, using the rules of conditional
probability and Equation (3.7), that
pX|(x|θ) = pX,T |(x, t(x)|θ) = pX|T,(x|t(x), θ)pT |(t(x)|θ)
= pX|T (x|t(x))pT |(t(x)|θ).

BAYESIAN SUFFICIENT STATISTICS AND PREDICTION SUFFICIENCY
95
In other words, there is a factorization of the form
pX|(x|θ) = g(t(x), θ)h(x),
(3.10)
where
h(x) = pX|T (x|t(x)) = pX|t(X)(x|t(x)).
In statistical literature, t(X) is often deﬁned to be a sufﬁcient statistic if there is a factor-
ization of the type given by Equation (3.10). Equation (3.10) is in fact a characterization
of sufﬁciency in the sense that the likelihood function for θ depends on data only through
t; the aspects of data that do not inﬂuence the value of t are not needed for inference
about θ, as long as pX|(x|θ) is the object of study. In the example above, and in many
other cases, this offers a data reduction. That is, for any n, a sample of size n can be
reduced to a quantity of ﬁxed dimension.
3.5.2
Prediction sufﬁciency
Let X be a discrete random vector, Y a discrete random variable or vector, t a function
and let T = t(X). Let  be a continuous random variable or vector. Suppose X, Y, 
and T satisfy
X ⊥

Y, 

|T.
(3.11)
That is, once t(X) is given, there is no additional statistical information in X about Y
or . The problem is to predict Y statistically using a function of X. The following
proposition is found in [70, 71].
Proposition 3.2
X ⊥

Y, 

|T ⇔

X ⊥|T
X ⊥Y|(, T ).
(3.12)
Proof of Proposition 3.2 Showing that X ⊥

Y, 

|T ⇒X ⊥|T
is a simple
marginalization. It is required to show that for any sets A and B and any value t such
that p({T = t}) ̸= 0,
p({X ∈A}, { ∈B}|{T = t}) = p({X ∈A}|{T = t})p({ ∈B}|{T = t}).
Assuming that X ⊥

Y, 

|T holds, then
p({X ∈A}, { ∈B}|{T = t}) =

y
p({X ∈A}, {Y = y}, { ∈B}|{T = t})
=

y
p({X ∈A}|{T = t})p({Y = y}, { ∈B}|{T = t})
= p({X ∈A}|{T = t})

y
p({Y = y}, { ∈B}|{T = t})
= p({X ∈A}|{T = t})p({ ∈B}|{T = t})
and hence X ⊥|T .

96
EVIDENCE, SUFFICIENCY AND MONTE CARLO METHODS
Next, it is proved that X ⊥

Y, 

|T ⇒X ⊥Y|(, T ). Let Aθ,ϵ = {z ∈˜||z −
θ| < ϵ}. It is required to show that
pX,Y|,T (x, y|θ, t) = pX|,T (x|θ, t)pY|,T (y|θ, t).
Assuming that X ⊥

Y, 

|T ,
pX,Y|,T (x, y|θ, t) = lim
ϵ→0
p({X = x}, {Y = y}, { ∈Aθ,ϵ}|{T = t})
p({ ∈Aθ,ϵ})
= lim
ϵ→0
p({X = x}|{T = t})p({Y = y}, { ∈Aθ,ϵ}|{T = t})
p({ ∈Aθ,ϵ})
= p({X = x}|{T = t})

lim
ϵ→0 p({Y = y}|{ ∈Aθ,ϵ}, {T = t})

= p({X = x}|{T = t})p({Y = y}|{ = θ}, {T = t})
= p({X = x}|{ = θ}, {T = t})p({Y = y}|{ = θ}, {T = t})
= pX|,T (x|θ, t),
so that
pX,Y|,T (x, y|θ, t) = pY|,T (y|θ, t).
By deﬁnition, therefore, X ⊥Y|(, T ). It has therefore been shown that X ⊥(Y, )|T
implies both X ⊥|T and X ⊥Y|(, T ).
Finally, the converse, that X ⊥|T and X ⊥Y|(, T ) imply X ⊥(Y, )|T , is rel-
atively straightforward. It is required to prove that, under the assumptions, for any sets
A, B, C and any value t such that p({T = t}) ̸= 0,
p({X ∈A}, {Y ∈B}, { ∈C}|{T = t})
= p({X ∈A}|{T = t})p({Y ∈B}, { ∈C}|{T = t}).
Using X ⊥Y|(, T ) for the second equality and X ⊥|T for the third,
p({X ∈A}, {Y ∈B}, { ∈C}|{T = t})
= p({X ∈A}, {Y ∈B}|{ ∈C}, {T = t})p({ ∈C}|{T = t})
= p({X ∈A}|{ ∈C}, {T = t})p({Y ∈B}|{ ∈C}, {T = t})p({ ∈C}|{T = t})
= p({X ∈A}|{T = t})p({Y ∈B}, { ∈C}|{T = t})
so that X ⊥(Y, )|T . The proof of Proposition 3.6 is complete.
□
The following result shows that if X ⊥(Y, )|T then, in a sense, (Y, T ) is Bayesian
sufﬁcient for .
Proposition 3.3 Let t denote a function and let T = t(X). If X, Y, T,  satisfy X ⊥
(Y, )|T , then
π|Y,X,T (θ | y, x, t) = π|Y,T (θ | y, t).
(3.13)

BAYESIAN SUFFICIENT STATISTICS AND PREDICTION SUFFICIENCY
97
Proof of Proposition 3.3 Firstly, X ⊥(Y, )|T implies (by a simple marginalization
over ) that X ⊥Y|T . The previous result also gives that X ⊥(Y, )|T implies X ⊥
|T and X ⊥Y|(T, ). It follows that
pX,Y|T (x, y|t) =

˜
pX,Y|,T (x, y|θ, t)π|T (θ|t)dθ
=

˜
pX|,T (x, y|θ, t)pY|,T (y|θ, t)π|T (θ|t)dθ
= pX|T (x|t)

˜
pY|,T (y|θ, t)π|T (θ|t)dθ
= pX|T (x|t)pY|T (y|t).
It follows that
pY|X,T (y|x, t) =
pX,Y|T (x, y|t)
pX|T (x|t)
= pY|T (y|t).
(3.14)
An application of Bayes’ rule gives
π|Y,X,T (θ|y, x, t) =
pY,X,T |(y, x, t|θ)π(θ)
pY,X,T (y, x, t)
=
pY,X|T,(y, x|t, θ)pT |(t|θ)π(θ)
pY,X,T (y, x, t)
=
pY|T,(y|t, θ)pX|T,(x|t, θ)pT |(t|θ)π(θ)
pY,X,T (y, x, t)
,
where the conditional independence X ⊥Y|(, T ) was used. Then, since X ⊥|T , it
follows that pX|T,(x|t, θ) = pX|T (x|t) and hence, using the Equation (3.14), that
π|Y,X,T (θ|y, x, t) =
pY|T,(y|t, θ)pX|T (x|t)pT |(t|θ)π(θ)
pY|X,T (y | x, t)pX|T (x | t)pT (t)
=
pY|T,(y|t, θ)pT |(t|θ)π(θ)
pY|T (y | t)pT (t)
.
It follows that
π|Y,X,T (θ|y, x, t) =
pY,T |(y, t | θ)π(θ)
pY|T (y, t)
= π|Y,T (θ|y, t),
as claimed.
□
3.5.3
Prediction sufﬁciency for a Bayesian network
Let G = (V, E) denote a DAG with V = {X1, . . . , Xd}, where the nodes are numbered,
for convenience such that for each j,
j ⊆{X1, . . . , Xj−1},

98
EVIDENCE, SUFFICIENCY AND MONTE CARLO METHODS
where j (as usual) denotes the parent set for Xj. Such a numbering is always possible
by Lemma 2.1.
Using a fully Bayesian approach to the problem, the parameter vector θ is considered
as an observation on a random vector  and for each j = 1 . . . , d the parameter vector
θj an observation on a random vector j.
Deﬁnition 3.4 (Parameter Modularity) A set of parameters  for a Bayesian network
satisﬁes parameter modularity
if it may be decomposed into d distinct parameter sets
1, . . . , d such that for j = 1, . . . , d, the parameters in vector j are directly linked
only to node Xj.
This deﬁnition was introduced in [72], and is a necessary condition for the sensitivity
analysis discussed in Chapter 7.
Under the assumption of parameter modularity, the DAG may be expanded by adding
the parameter nodes as parent variables in the graph, and directed links from each node
in the set j to the node Xj giving an extended graph that is directed and acyclic, where
pX1,...,Xd| has the decomposition
pX1,...,Xd| =
d
j=1
pXj |j ,j .
(3.15)
Furthermore, under the assumption of modularity, 1, . . . , d are independent random
vectors and the joint prior distribution is a product of individual priors; π = 	d
j=1 πj .
Following Proposition 3.2, the following notation is useful:
˜Xj :=

(X1, 1), . . . , (Xj−1, j−1)

,
j = 1, . . . , d
and, for j = 1, . . . , d, tj is used to denote the function such that
tj( ˜Xj) = j.
It follows directly from (3.15) that
˜Xj ⊥

Xj, j

|j.
In other words, the parent set j is a prediction sufﬁcient statistic for (Xj, j) in the
sense that there is no further information in

(X1, 1), . . . , (Xj−1, j−1)

relevant to
uncertainty about either j or Xj.
In a Bayesian network where the parameters satisfy the modularity assumption (Deﬁ-
nition 3.4), (j, Xj) are a Bayesian sufﬁcient statistic for j. The modularity assumption
is clearly satisﬁed when Equation (3.15) holds.
3.6
Time variables
This section considers Markov models.

TIME VARIABLES
99
X1
X2
X3
X4
X5
Figure 3.6
Illustration of a Markov model.
Deﬁnition 3.5 (Markov Chain) A sequence (Xn)n≥1 is a Markov Chain if for all n, N,
(Xn+N, . . . , Xn+1) ⊥(Xn−1, . . . , X1)|Xn.
In other words, the past is conditionally independent of the future given the present.
A Markov chain may be represented by the DAG given in Figure 3.6. The index n,
for n = 1, 2, 3, 4, 5, may be considered as a time variable. For a Markov chain, the
conditional probability potentials pXn+1|Xn are known as transition probabilities. The
Markov chain is said to be time homogeneous if pXn+1|Xn does not depend on n. For a
time homogeneous Markov chain on a binary space,
pXn+1|Xn =
Xn+1\Xn
0
1
0
p
1 −q
1
1 −p
q
where 0 ≤p ≤1 and 0 ≤q ≤1. The joint probability may be decomposed as
pX1,X2,X3,X4,X5 = pX1pX2|X1pX3|X2pX4|X3pX5|X4,
which is a product of transition probability potentials.
Consider the Bayesian network described by the graph in Figure 3.7 and suppose
that the variables (X1, X2, X3, . . .) cannot be observed, but that the values taken by
(Y1, Y2, Y3, . . .) can be observed. For example, Xn may be the state of an infection on
day n and Yn the test result on day n.
The model supposes that the past and future are independent given the current state
and therefore the sequence of variables (Xj)j≥1 in Figure 3.7 form a Markov chain.
Since only the variables (Yn)n≥1 may be observed, the sequence (Xn)n≥1 is referred to
as a hidden Markov chain. From the graph,
p(X1,Y1),...,(X5,Y5) = pY5|X5pX5|X4pY4|X4pX4|X3pY3|X3pX3|X2pY2|X2pX2|X1pY1|X1pX1.
It also follows from the graph that
Yn ⊥Yn−1|Xn
and
Yn+1 ⊥Yn|Xn.
X1
X3
X4
X5
Y1
Y2
Y3
Y4
Y5
X2
Figure 3.7
Illustration of a hidden Markov model.

100
EVIDENCE, SUFFICIENCY AND MONTE CARLO METHODS
In other words, the observable variables are conditionally independent given the state
sequence Xn.
Note that the observed variables Y1, . . . , Yn may be considered in the same way as
the virtual evidence node V in Figure 3.3.
Queries for Markov models
The queries for hidden Markov models formulated above
are given the following names:
• ﬁltering: Find pXn|Y1,...,Yn.
• prediction: Find pXn+1|Y1,...,Yn.
The maximum aposterior hypothesis problem is that of ﬁnding the most probable path;
i.e. ﬁnding the path X1, . . . , X5 which maximizes
pX1,...,X5|Y1,...,Y5.
Often, it is natural to consider discrete time variables when one considers a Markov
model. Associated with each time, one may have several variables, hidden or observable,
with causal relations between them. These variables, together with the associated DAG,
are known as a time slice. The DAG for the entire system is then decomposed into the
consecutive time slices, together with additional causal links between the time slices to
indicate the direct causal relationships. Time slices connected by temporal links constitute
dynamic Bayesian networks, as discussed by K. Murphy in [73].
3.7
A brief introduction to Markov chain
Monte Carlo methods
Since the problem of locating an appropriate graph structure of a Bayesian network is
NP-hard (Chickering [74]), exact methods are of limited use and Markov chain Monte
Carlo methods are essential for the analysis of larger networks. An McMC method for
locating the graph structure is described in Section 6.4 and the necessary preparatory
material on Markov chains and Monte Carlo methods is given here.
Since there are many good treatments of Markov chains available (see, for example,
[75]), only a sketch is presented here; the necessary results and features of Markov Chains
are recapitulated and the key ideas outlined.
For a random variable X taking values in a ﬁnite state space E = (s(1), . . . , s(k)),
the purpose of Markov chain Monte Carlo (McMC) is to ﬁnd an estimate ˆpE(.) of the
distribution of pE(.) for the random variable E. In the setting where it will be used, E is
the space of all possible graph structures. This may then used to ﬁnd an optimal ‘value’;
for example, the sj that maximizes ˆpE(sj). The McMC approach to the problem is to
develop a time homogeneous Markov chain X = (X0, X1, X2 . . .) (deﬁned below) with
state space E and has stationary distribution (deﬁned below) pE(.).
Deﬁnition 3.6 (Time Homogeneous Markov Chain, Stationary Distribution) A Markov
chain X with state space E = (s1, . . . , sk) is deﬁned as a sequence of random variables

A BRIEF INTRODUCTION TO MARKOV CHAIN MONTE CARLO METHODS
101
X = (X0, X1, X2, . . .) such that for any n ≥1 and any sequence (s0, s1, . . . , sn) ∈En,
pXn|Xn−1,...,X0(sn|sn−1, . . . , s1, s0) = pXn|Xn−1(sn|sn−1).
The Markov chain is time homogeneous if, in addition, there is a k × k matrix P with
entries Pij for all n ≥1 and all (s(i), s(j)) ∈E2,
pXn|Xn−1(s(j)|s(i)) = Pij.
The matrix P is known as the one step transition matrix, or transition matrix when the
‘one step’ is clear.
A stationary distribution for the time homogeneous Markov chain is a row vector
π = (π1, . . . , πk) such that πj ≥0 for all j ∈{1, . . . , k} and k
j=1 πj = 1 and such
that if pX0(sj) = πj for all j ∈{1, . . . , k}, then pXn(sj) = πj for all n ≥1 and all j ∈
{1, . . . , k}.
It follows directly from the deﬁnition of the Markov chain that the future is indepen-
dent of the past, conditioned on the present. That is,
(Xn+1, Xn+2, . . .) ⊥(X0, . . . , Xn−1)|Xn.
Since only time homogeneous Markov chains will be considered, the term ‘Markov
chain’ will be used for ‘time homogeneous Markov chain’.
Lemma 3.2 (Stationary Distribution) A distribution π = (π1, . . . , πk) (that is, a row vec-
tor such that 0 ≤πj ≤1 for each j ∈{1, . . . , k} and k
j=1 πj = 1) is a stationary dis-
tribution for the Markov chain X with transition matrix P if and only if
πP = π.
Proof of Lemma 3.2 Firstly, if pXn(sj) = πj for all j ∈{1, . . . , k} and all n ≥0, then
πj = pX1(sj) =
k

i=1
pX1|X0(sj|si)pX0(si) =
k

i=1
πiPij
so that a stationary distribution satisﬁes π = πP . Secondly, if pX0(sj) = πj for all
j ∈{1, . . . , k} and if πP = π, then
pXn(sj) =
k

i=1
pXn|Xn−1(sj|si)pXn−1(si).
Let ψ(n) = (pXn(s1), . . . , pXn(sk)) (the row vector). Then
ψ(n) = ψ(n−1)P = ψ(0)P n,
so if ψ(0) = π and π satisﬁes πP = π, then ψ(n) = π for all n ≥0. It follows that π is
a stationary distribution and the result is proved.
□

102
EVIDENCE, SUFFICIENCY AND MONTE CARLO METHODS
For each i ∈{1, . . . k} (where k is ﬁnite, but typically can be very large), the row Pi.
in the transition matrix should try to have as few j ∈{1, . . . , k} as possible such that
Pij is non-zero. Then, if the MC has been designed properly (the requirements will be
discussed below), and it can be simulated well enough, the empirical distribution from
the ﬁrst n steps, deﬁned by
ˆpE(si) = 1
n
n

j=1
Ksi(Xj),
si ∈E,
where Ks(e) = 1 if s = e and 0 otherwise, will be close to pE(.) for sufﬁciently large n.
Designed properly means that the MC satisﬁes certain conditions, as deﬁned below:
1. It has to be irreducible,
2. It has to be aperiodic,
3. pE (the target distribution) is a stationary distribution for the Markov chain.
With these conditions satisﬁed, it is possible to show that, for sufﬁciently n, the empirical
distribution will be close to the target distribution, no matter what initial value is chosen.
Let X = (X0, X1, X2, . . .) be a Markov chain with state space E = (s1, . . . , sk) A
realization of the process is a sequence of states. The state associated with time 0 is the
initial state, and it must be speciﬁed. After that the one step transition matrix P describes
how to select successive states.
Example 3.7
There is (in spring 2009) a coffee shop at every corner of the main square
(Stora torget) in the town of Link¨oping, Ostrogothia Province in Sweden. These are
ν1 = Cioccolata, ν2 = Coffee-house by George, ν3 = Linds, and ν4 = Santinis. Suppose
that an Ostrogothian is at corner νj at time n. He tosses a coin. If the coin comes up
heads, he will move to corner νj+1 at time n + 1, otherwise he will move to corner νj−1
at time n + 1, where ν5 ≡ν1 and ν0 ≡ν4. The states of this process and the transition
probabilities may be represented by the circle diagram in Figure 3.8.
Any time homogeneous Markov chain with a ﬁnite state space may be represented this
way. In this case, the transition matrix P with entries deﬁned by Pij = pXn+1|Xn(νj|νi)
is given by
P =


0
1
2
0
1
2
1
2
0
1
2
0
0
1
2
0
1
2
1
2
0
1
2
0


.
1
2
4
3
Figure 3.8
Diagram for state transitions.

A BRIEF INTRODUCTION TO MARKOV CHAIN MONTE CARLO METHODS
103
If an initial distribution µ = (µ1, . . . µk) is speciﬁed, such that µj ≥0 for j = 1, . . . , k,
k
j=1 µj = 1 and such that pX0(sj) = µj, then the distribution pX1 is determined by
pX1(sj) =
k

i=1
pX1,X0(sj, si) =
k

i=1
pX1|X0(sj|si)pX0(si) =
k

i=1
µiPij.
Let µ(n) = (pXn(s1), . . . , pXn(sk)), where the probability distribution is described as a
row vector with k components. Then it follows that
µ(1) = µ(0)P
and, in general, it is easy to see that
µ(n) = µ(0)P n.
3.7.1
Simulating a Markov chain
This discussion will assume that there is access to a source of uniformly distributed ran-
dom variables. In practice, this involves a pseudo random number generator of reasonable
quality. To simulate a time homogeneous Markov process X for n steps, the following
are required:
1. An initial distribution.
2. A procedure to determine the next state given the present state, so that
the input of a sequence of independent uniform variables of length n + 1,
(U0, U1, . . . , Un) produces the initial value and the ﬁrst n steps of the Markov
chain (X0, X1, . . . , Xn). For some algorithms, several independent uniformly
distributed variables are required to produce the next step of the chain.
3. It has to be possible verify that the simulation is a plausible trajectory of X.
Specifying the initial value
Suppose that an initial distribution µ = (µ1, . . . , µk) is
speciﬁed. Given u ∈[0, 1], an observation from a U(0, 1) distributed random variable,
an observation from this distribution can be simulated quite easily using a function
ψ : [0, 1] →E, deﬁned as
ψ(u) =
 s1
u ∈[0, µ1)
si
u ∈[i−1
j=0 µj, i
j=0 µj)
i = 1, . . . , k.
This function ψ is known as the initiation function.
Updating
Suppose Xn = sm, then Xn+1 may be determined from the function
φ(sm, u) by
φ(sm, u) =
 s1
u ∈[0, psm,s1)
ei
u ∈[i−1
j=0 psm,sj , i
j=0 psm,sj )
i = 1, . . . , k.

104
EVIDENCE, SUFFICIENCY AND MONTE CARLO METHODS
To simulate the chain now, given a sequence (u0, u1, . . . , un) of observations from inde-
pendent U(0, 1) random variables, set
X0 = ψ(u0),
Xj+1 = φ(Xj, uj+1),
j = 0, . . . , n −1.
3.7.2
Irreducibility, aperiodicity and time reversibility
This subsection clariﬁes the requirements on a Markov chain that is to be used for Monte
Carlo simulation. It has to be irreducible and aperiodic with a ﬁnite state space. Many
efﬁcient methods (such as the Metropolis-Hastings, considered below) also require that
the Markov chain is time reversible.
Deﬁnition 3.7 (Irreducible Markov Chain) An irreducible Markov chain is a chain in
which all the states communicate. Two states si and sj are said to communicate if there is
an n1 < +∞such that pXn1|X0(sj|si) > 0 and an n2 < +∞such that pXn2|X0(si|sj) > 0.
Aperiodic Markov chains
Let Pij(n) := pXn|X0(sj|si). The matrix P (n) is known as
the n step transition matrix. The greatest common divisor of a ﬁnite, or inﬁnite set of
positive integers A = {a1, a2, a3, . . .}, will be written GCD(A).
Deﬁnition 3.8 (Period of a State) Suppose E = {s1, . . . , sk}. For each j = 1, . . . , k, set
Aj = {n|Pjj(n) > 0}.
The period of state sj is deﬁned as d(sj) = GCD(Aj), the greatest common divisor of
the set of times that the chain starting at a state sj can return to state sj with positive
probability.
Deﬁnition 3.9 (Aperiodic Markov Chain) A state sj is said to be aperiodic if d(sj) = 1,
A Markov chain is said to be aperiodic if all of its states are aperiodic. Otherwise, the
chain is said to be periodic.
The following theorem describes a key property of aperiodic Markov chains, that is
necessary for Monte Carlo simulation.
Theorem 3.1 Suppose that X = (X0, X1, . . .) is a Markov chain with state space E =
(s1, . . . , sk) and one-step transition matrix P. Then there exists an N < ∞such for all
that for all j = 1, . . . , k and all n ≥N, Pjj(n) > 0.
The proof of this theorem requires the following standard lemma from number theory,
which is stated here without proof.
Lemma 3.3 Let A = {a1, a2, . . .} be a set of positive integers such that
1. GCD(A) = 1
2. For any ai, aj ∈A, ai + aj ∈A.
Then there exists an integer N < +∞such that n ∈A for all n ≥N.

A BRIEF INTRODUCTION TO MARKOV CHAIN MONTE CARLO METHODS
105
Proof of Lemma 3.3 Omitted: it may be found in any standard introduction to number
theory.
□
Proof of Theorem 3.1 For each sj ∈E, let Aj = {n ≥1|Pjj(n) > 0}, the set of possible
return times to the state sj for the chain E starting from site sj. Since the chain is
aperiodic, GDC(Aj) = 1. Suppose that a1, a2 ∈Aj, then
Pjj(a1 + a2) = pXa1+a2|X0(sj|sj) ≥pXa1,Xa1+a2|X0(sj, sj|sj)
= pXa1+a2|Xa1,X0(sj|sj, sj)pXa1|X0(sj|sj) = Pjj(a2)Pjj(a1) > 0,
so that the conditions of Lemma 3.3 are satisﬁed. The result follows directly from
Lemma 3.3.
□
Corollary 3.1 Suppose that X is an irreducible and aperiodic MC with state space E =
{s1, . . . , sk} with one-step transition matrix P. Then there exists a ﬁnite positive integer
M such that Pij(n) > 0 for all n ≥M and all 1 ≤i ≤k, 1 ≤j ≤k.
Proof of Corollary 3.1 Since X is aperiodic, there is an N < +∞such that Pjj(n) > 0
for all j = 1, . . . , k and all n ≥N. For any si, sj, there is, by irreducibility, an nij such
that Pij(nij) > 0. Let Mij = N + nij. Then, for all m ≥Mij,
pXm|X0(sj|si) ≥pXm,Xm−nij |X0(sj, si|si)
= pXm|Xm−nij (sj|si)pXm−nij |X0(si|si) = Pij(nij)Pii(m −nij) > 0.
Take M = maxij Mij.
□
Stationary distributions
When carrying out a Markov chain Monte Carlo simulation,
it is necessary that the distributions of X(n) converge as n →+∞to a distribution
that is independent of the initial condition and that they converge to the correct target
distribution. That is,
(pXn(s1), . . . , pXn(sk))
n→+∞
−→π
for any initial distribution µ.
The following results concern existence, uniqueness, and convergence to stationarity
for irreducible, aperiodic Markov chains.
Theorem 3.2 (Existence and Uniqueness) Let X be a time homogeneous irreducible ape-
riodic Markov chain with a ﬁnite state space E = (s1, . . . , sk). There exists a unique
stationary distribution π.
Proof of Theorem 3.2 This is found in any standard introduction to Markov chains and
is omitted.
□
When applying a Monte Carlo simulation, it is necessary that the process eventually visits
all sites in the state space, hence the ﬁrst hitting time and the expected return times for
a site are of interest.

106
EVIDENCE, SUFFICIENCY AND MONTE CARLO METHODS
Deﬁnition 3.10 (First Hitting Time, Mean Return Time) The ﬁrst hitting time of site sj
is deﬁned as
Tj = min{n : Xn = sj},
with Tj = +∞if the chain never visits site sj. The expected ﬁrst hitting time of site sj
for a process started at site si is deﬁned as
τij = E[Tj|X0 = si].
The mean return time for site si is deﬁned as τii for i = 1, . . . , k.
Lemma 3.4 Let X be an irreducible and aperiodic time homogeneous Markov chain with
state space E = (s1, . . . , sk) and one step transition matrix P and let si, sj ∈E. Then for
all i, j, p({Tj < +∞}|{X0 = si}) = 1. Furthermore, τij < +∞. That is, the mean hitting
time for any state is ﬁnite.
Proof of Lemma 3.4 By Corollary 3.1, there is an integer M < +∞such that Pij(n) > 0
for all i, j ∈{1, . . . , k} and all n ≥M. Let α = min1≤i≤k, 1≤j≤k{Pij(M)}. Note that α > 0.
For states si, sj,
p({Tj > M}|{X0 = si}) ≤p({XM ̸= sj}|{X0 = si}) ≤1 −α.
Also,
p({Tj > 2M}|{X0 = si}) =

r1̸=si,...,r2M̸=si
pX2M,X2M−1...,X1|X0(r2M, . . . , r1|si)
≤

r2M̸=si,rM̸=si
pX2M,XM|X0(r2M, rM|si)
=

r2M̸=si,rM̸=si
pX2M|XM (r2M|rM)pXM|X0(rM|si)
≤(1 −α) sup
q
p({X2M ̸= si}|{XM = sq}) ≤(1 −α)2.
By a similar argument, it is clear that for each l > 1,
p({Tj > lM}|{X0 = i}) ≤(1 −α)l l→+∞
−→0.
It follows that
lim
N→+∞p({Tj < N}|{X0 = si}) = 1.
Furthermore,
τij =E[Tj|{X0 = si}]=
∞

n=1
p({Tj ≥n}|{X0 = si}) ≤
∞

l=0
(l+1)M−1

n=lM
p({Tj ≥lM}|{X0 = si})
= M
∞

l=0
p({Tj > lM}|{X0 = si}) ≤M
∞

l=0
(1 −α)l = M
α < +∞.
The lemma is proved.
□

A BRIEF INTRODUCTION TO MARKOV CHAIN MONTE CARLO METHODS
107
Theorem 3.3 (The Markov Chain Convergence Theorem) Let X be an irreducible, ape-
riodic Markov chain with state space E = (s1, . . . , sk) and transition matrix P , with arbi-
trary initial distribution µ = (µ1, . . . , µk). Then for any distribution π = (π1, . . . , πk)
that is stationary for the transition matrix P,
lim
n→+∞
max
j∈{1,...,k} |µ(n)
j
−πj| = 0,
where µ(n) = µP n (where µ, π and µ(n) are, as usual, taken as row vectors).
Proof of Theorem 3.3 The proof uses a coupling argument. Let X = (X0, X1, X2, . . .)
and Y = (Y0, Y1, Y2, . . .) denote two independent copies of the Markov chain. Let τ =
min{n ≥1 : Xn = Yn}, with τ = +∞if the chains never meet. The aim is to show that
the random time τ satisﬁes p({τ < +∞}) = 1. Since the Markov chain is irreducible
and aperiodic, there exists an integer N < +∞by Corollary 3.17 such that Pij(M) > 0
for all i, j ∈{1, 2, . . . , k}. Let α = min{Pij(M), i, j ∈{1, . . . , k}} > 0. It follows that
p({τ ≤M}) =
M

j=1
p({Xj = Yj})
≥p({XM = YM}) ≥p({XM = YM = s1}) = p({XM = s1})p({YM = s1})
=
 k

i=1
Pi1(M)p({X0 = si})
  k

i=1
Pi1(M)p({Y0 = si})

≥α2.
It follows that p({τ > M}) ≤1 −α2. Similarly,
p({τ > lM}) = p({τ > M})
l
j=1
p({τ >(j + 1)M}|{τ > jM})
and a similar argument to the one that shows p({τ > M}) ≤1 −α2 also gives that
p({τ >(j + 1)M}|{τ > jM}) ≤1 −α2 for each j. From this, it follows directly that
p({τ > lM}) ≤(1 −α2)l
and hence that limN→+∞p(τ < N) = 1, so that limN→+∞p(τ > N) = 0. It is also
straightforward to show from the bounds that E[τ] < +∞.
For any j ∈{1, . . . , k}, note that p({Yn = sj}|{τ ≤n}) = p({Xn = sj}|{τ ≤n}). It
follows that, for each j = 1, . . . , k,
|pXn(sj) −pYn(sj)| ≤|(p({Xn = sj}|{τ ≤n}) −p({Yn = sj}|{τ ≤n})|p({τ ≤n})
+|p({Xn = sj}|{τ > n}) −p({Yn = sj}|{τ > n})|p({τ > n})
≤p({τ > n})
n→+∞
−→0.
The result follows directly.
□

108
EVIDENCE, SUFFICIENCY AND MONTE CARLO METHODS
Time reversible Markov chains
Several McMC algorithms require the Markov chain
to be time reversible. This is the case with the Metropolis-Hastings algorithm, discussed
in Section 3.7.3.
Deﬁnition 3.11 (Time Reversible Markov Chain) Let X = (X1, X2, . . .) be a time homo-
geneous Markov chain with state space E = (s1, . . . , sk). Let µ be the entrance law (that
is µj = pX0(sj)) and P be the transition matrix. The Markov chain is reversible if there
exists a distribution π (that is πj ≥0 for all j ∈{1, . . . , k} and k
j=1 πj = 1) such that
πiPij = πjPji
(3.16)
for all 1 ≤i ≤k, 1 ≤j ≤k. A distribution π with this property is said to be a reversible
distribution.
Proposition 3.4 Let X be a Markov chain, with ﬁnite state space E, one step transition
matrix P and entrance law µ. Let π be a reversible distribution. Then π is a stationary
distribution for the chain.
Proof of Proposition 3.4 Since k
j=1 Pij = 1, it follows by Equation (3.16) that
πi = πi
k

j=1
Pij =
k

j=1
πiPij =
k

j=1
πjPji
for all i ∈{1, . . . , k}. In other words,
π = πP,
where π is taken as a 1 × k row vector, so that π is a stationary distribution by
Lemma 3.2.
□
3.7.3
The Metropolis-Hastings algorithm
Since direct sampling from a posterior distribution may not be possible, the
Metropolis-Hastings algorithm starts by generating candidate draws from a so-called
proposal distribution. These draws are then corrected so that they behave asymptotically
as random observations from the desired invariant or target distribution.
The MC constructed by the algorithm at each stage is therefore built in two steps: a
proposal step and an acceptance step. These two steps are associated with the proposal
and acceptance distributions, respectively.
The Metropolis problem
Let f = (f1, . . . , fk) be an arbitrary probability function,
which is the target distribution, on a ﬁnite state space E = (s1, . . . , sk). That is,
• fj ≥0 for j ∈{1, . . . , k},
• k
j=1 fj = 1.

A BRIEF INTRODUCTION TO MARKOV CHAIN MONTE CARLO METHODS
109
The Metropolis problem is to construct a Markov chain with invariant distribution f . The
following discussion shows that it is always possible to construct an appropriate transition
matrix to solve the Metropolis problem. There are, in fact, inﬁnitely many solutions to
the stated problem.
A solution of the Metropolis problem
Let Q be a symmetric k × k matrix, that is
Qij = Qji,
∀(i, j) ∈{1, . . . , k}2
(3.17)
such that Qij ≥0 for each (i, j) and k
j=1 Qij = 1 for each i ∈{1, . . . , k}.
The aim is to construct a Markov chain X with the state space E and the invariant
distribution f . Consider the following rules for transition:
For Xn = si, propose a value of Yn+1 = sj where sj is drawn with probability Qij,
for j = 1, . . . , k, independently of X0, . . . , Xn−1. Then accept j with probability
αij = min

1, fj
fi
"
.
(3.18)
Acceptance means that the chain moves to Xn+1 = sj. Reject the proposed value j with
probability
1 −αi,j.
(3.19)
Rejection means that the chain stays at si. That is, Xn+1 = si. The procedure is imple-
mented in terms of an independent random toss of coin the probability function
(p(heads), p(tails)) =

αi,j, 1 −αi,j

.
Heads means acceptance and tails means rejection.
The next task is to compute the transition probabilities Pij of the Markov chain X.
Let {ta} denote the event that the proposed transition is accepted, and let {ta}c denote
the complement. Then, for i ̸= j,
Pij = pXn+1|Xn(sj|si) = p

{Yn+1 = sj} ∩ta|{Xn = si}

from the deﬁnition of proposal and acceptance. It follows that
= P ({Yn+1 = j}|ta ∩{Xn = si}) P (ta|{Xn = si}) = p

{Yn+1 = sj}|{Xn = si}

αi,j.
since, conditioned on Xn proposal is generated independently of acceptance. In other
words,
Pij = Qij · min

1, fj
fi
"
= Qij · αi,j
i ̸= j.
(3.20)
For i = j,
Pii = pXn+1|Xn (si|si)
= p ({Yn+1 = si} ∩ta|{Xn = si}) + p

{Yn+1 ̸= si} ∩tac|{Xn = si}

= pYn+1|Xn (si|si) p ( ta|{Xn = si}) + pYn+1|Xn (si|si) p

tac|{Xn = si}


110
EVIDENCE, SUFFICIENCY AND MONTE CARLO METHODS
= pYn+1|Xn (si|si) αi,i +

j̸=i
pYn+1|Xn

sj|si
 
1 −αi,j

= Qiiαi,i +

j̸=i
pYn+1|Xn

sj|si
 
1 −αi,j

= Qii +

j̸=i
Qij

1 −αi,j

.
The matrix P thus deﬁned is a legitimate transition probability matrix; Pij ≥0 for each
(i, j) and k
j=1 Pij = 1 for each k ∈{1, . . . , k}.
It remains to show that f is an invariant distribution for Markov chain X with
transition matrix P and an arbitrary initial distribution µ.
The following argument shows that f satisﬁes the reversibility condition fiPij =
fjPji for all (i, j). Consider i ̸= j such that
fi < fj.
Then, by Equation (3.20), and the symmetry of Q (which is assumed in the construction),
fiPij = fiQij min

1, fj
fi
"
= fiQij
= Qij min

1, fi
fj
"
fj
= Qjiαj,ifj
= Pjifj.
Now consider i ̸= j such that
fj < fi,
Then
fjPji = Qji min

1, fi
fj
"
fj.
Continuing in the same way as before gives the result. Hence the reversibility condition
(Equation (3.16)) holds for all (i, j), and therefore, by the result of Proposition 3.4, the
Metropolis problem is solved.
Example 3.8
Let
f =
1
4, 1
4, 1
6, 1
3

,
and
Q =


1
6
1
6
1
6
1
2
1
6
1
2
1
6
1
6
1
6
1
6
2
3
0
1
2
1
6
0
1
3


.

A BRIEF INTRODUCTION TO MARKOV CHAIN MONTE CARLO METHODS
111
Then the acceptance probabilities are
α =


1
1
2
3
1
1
1
2
3
1
1
1
1
0
3
4
3
4
0
1


.
The transition matrix may be computed using MATLAB. It is
P =


0.2222
0.1667
0.1111
0.5
0.1667
0.5556
0.1111
0.1667
0.1667
0.1667
0.667
0
0.3750
0.125
0
0.5

.
3.7.4
The one-dimensional discrete Metropolis algorithm
The solution of the Metropolis problem, as established above, can be used to simulate
a Markov chain X with preassigned stationary distribution f . The pertinent simulation
algorithm is known as the Metropolis algorithm.
Deﬁnition 3.12 (Metropolis Algorithm) Let Q be a symmetric transition probability
matrix. Given that Xn = si,
1. Generate: Yn+1 = sj with probability Qij for j ∈{1, . . . , k}.
2. Take
Xn+1 =

Yn+1
with probability
αi,j
si
with probability
1 −αi,j
where
αi,j = min

1, fj
fi
"
.
The distributions Qi,. are called the proposal distributions.
In W.K. Hastings [76], the algorithm of Metropolis was generalized by relaxing the
requirement that the matrix of proposal distributions Q be symmetric. The more general
simulation algorithm is known as the Metropolis-Hastings algorithm.
Deﬁnition 3.13 (Metropolis-Hastings Algorithm) Let Q be a transition probability
matrix. Given that Xn = si
1. Generate Yn+1 = sj with probability Qij, j ∈{1, . . . , k}.

112
EVIDENCE, SUFFICIENCY AND MONTE CARLO METHODS
2. Take
Xn+1 =

Yn+1
with probability
αH
i,j
si
with probability
1 −αH
i,j,
where
αH
i,j = min

1, fjQji
fiQij
"
.
(3.21)
The distributions Qi. for i ∈{1, . . . , k} are called the proposal distributions.
Notes
The ‘Chest Clinic Problem’ example is found in A.P. Dawid [68] and in S. Lau-
ritzen [77]. Conditional independence and sufﬁciency is discussed in [70] and [71].
McMC is a major computational workhorse in modern Bayesian inference. A thorough
introduction and treatment of McMC and its applications to Bayesian inference is found
in [26]. The software WinBUGS and BUGS for Bayesian statistical inference [78] on
a number of statistical models using McMC has a source code which is analogous to a
Bayesian network, and represents an application of Bayesian networks to computer soft-
ware. Simulation methods like McMC can, of course, be used for many other purposes
than computing expectations or probabilities by empirical averages; an example is found
in P˜ena [49]. The work in [79] presents a non-reversible McMC technique which has
turned out to be useful in, for example, learning of structures, as discussed in Chapter 6.
A study of sufﬁciency in machine learning is found in [80].

EVIDENCE, SUFFICIENCY AND MONTE CARLO METHODS
113
3.8
Exercises: Evidence, sufﬁciency and Monte Carlo
methods
1. Jeffrey’s Rule In a certain country, people use only two car models, Volvo and Saab,
which come in two colours, red and blue. The sales statistics suggest p(Volvo) =
p(Saab) = 1/2. Furthermore, p(red|Volvo) = 0.7 and p(red|Saab) = 0.2. You are
on holiday in this region and you are standing outside a large underground garage,
which you may not enter. The attendant of the garage communicates his impression
that 40% of the cars in the garage are red. What is the probability that the ﬁrst car
leaving the garage is a Volvo?
2. Pearl’s Method Let A denote an event that gives uncertain information (or vir-
tual/soft evidence) about the partition (that is a collection of mutually exclusive and
exhaustive events) {Gj}n
j=1. Suppose that A satisﬁes
p 
A | Gj, B = p 
A | Gj
 ,
j = 1, 2, . . . , n
for every event B. This is an assumption of conditional independence; the event A is
independent of all other events given the partition Gj. Set λj = p(A|Gj) and show
that
p (C | A) =
n
j=1 λjp

C ∩Gj

n
j=1 λjp

Gj

.
Check that p(.|A) satisﬁes the deﬁnition of the Pearl update (Deﬁnition 1.8).
The following two exercises are about converting soft evidence in a format where
Jeffrey’s rule is applicable, to a format where Pearl’s rule is applicable and vice
versa. They are taken from [81].
3. Let p denote a probability distribution before evidence is obtained and suppose
that a piece of evidence gives uncertain information about the partition (that is, the
collection of mutually exclusive and exhaustive events) {Gj}n
j=1. Suppose that this
evidence is speciﬁed by the posterior probabilities
p∗
Gj

= qj,
j = 1, 2, . . . , n
and by
λj =
qj
p

Gj
,
j = 1, 2, . . . , n.
For any event C, compute the probability p (C | A) using Pearl’s method of virtual
evidence and show that this gives the same result as Jeffrey’s rule of update.
Hint: Use the formula in the preceding exercise.
4. Suppose that p is a probability distribution before any new information has been
received and that the virtual evidence A gives uncertain information about the

114
EXERCISES: EVIDENCE, SUFFICIENCY AND MONTE CARLO METHODS
partition (that is, the collection of mutually exclusive and exhaustive events) {Gj}n
j=1.
Suppose that this evidence is speciﬁed by likelihood ratios
p

A | Gj

p (A | G1) = λj,
j = 1, 2, . . . , n.
Assume that
p∗
Gj

= qj = p

Gj | A

,
j = 1, 2, . . . , n.
For any event C, compute the probability p∗(C) using Jeffrey’s rule and show that
this gives the same result as Pearl’s method of virtual evidence.
5. Let X1, X2, X3 be three binary random variables, each taking values in {0, 1}, such
that
pX1,X2,X3 (x1, x2, x3) = 1
8,
for (x1, x2, x3) ∈{0, 1}3.
Now let V be an additional binary random variable and let E = {V = 1}. Here V
stands for virtual information. Suppose that the conditional probability function of
V given X3 satisﬁes
pV |X3 (1 | 1) = λpV |X3 (1 | 0) .
Let G1 and G2 be the two events
G1 =
#
(x1, x2, x3) ∈{0, 1}3 | x3 = 0
$
and
G2 = #(x1, x2, x3) ∈{0, 1}3 | x3 = 1$ .
The events G1 and G2 are mutually exclusive and exhaustive. Use Pearl’s method
of virtual evidence to obtain the updated probability distribution
˜pX1,X2,X3 (x1, x2, x3) = pX1,X2,X3|V (x1, x2, x3|1)
(x1, x2, x3) ∈{0, 1}3.
Comment Virtual evidence is an event that is not accommodated by the statistical
model. In this case, the model is described by the probability function pX1,X2,X3 and
it does not accommodate V , in the sense that the event {V = 1} is not an event in the
event algebra generated by the model. Information of this type (that is, in terms of
events not in the event algebra generated by the model) can be expected to occur in
practice, since a model cannot be expected to consider all scenarios and all possible
sources of information. When virtual evidence arrives, some assessment has to be
made as to how it should be incorporated. Here, the additional modelling assumption
has been added that the likelihood of {V = 1} depends only on X3. That is, {V = 1}
is conditionally independent of X1, X2 given X3. The statistical model for V is not
complete, since only the ratio of the likelihoods has been speciﬁed. This adds to the
modelling capacities of Bayesian networks, as virtual nodes may be added, as shown
in Figure 3.3.

EVIDENCE, SUFFICIENCY AND MONTE CARLO METHODS
115
6. Let (X1, X2) be discrete random variables, with joint probability function
pX1,X2(x1, x2|θ1, θ2)
=

n!
x1!x2!(n−x1−x2)!θx1
1 θx2
2 (1 −θ1 −θ2)n−x1−x2
x1 ≥0, x2 ≥0, x1 + x2 ≤n
0
otherwise,
where θ1 ≥0, θ2 ≥0 and θ1 + θ2 ≤1. Suppose that the prior distribution over (θ1, θ2)
is taken from the class
π(θ1, θ2) = (α1 + α2 + α3)
	3
j=1 (αj)
θα1−1
1
θα2−1
2
(1 −θ1 −θ2)α3
θj ≥0,
j = 1, 2,
θ1 + θ2 ≤1
where αj > 0, j = 1, 2, 3.
Let ψ = θ1 + θ2. Show that T = X1 + X2 is a sufﬁcient statistic for ψ.
7. Let X = (X1, . . . , Xn) be a random sample from U(θ1, θ2) (uniformly distributed
between θ1 and θ2). That is, the density function is
π(x|θ1, θ2) =

1
θ2−θ1
θ1 ≤x ≤θ2
0
otherwise.
Let T (X) = (minj Xj, maxj Xj). Find the distribution of T and show that it is a
sufﬁcient statistic for θ = (θ1, θ2).
8. Bucket Elimination Suppose A, B, C, D, E, F and G are all binary variables (tak-
ing values 0 or 1) and that their joint probability may be factorized along the DAG
given in Figure 3.9.
A
E
C
D
F
B
G
Figure 3.9
Bucket elimination.
Suppose that pF,C(1|1) is to be computed.
(a) Suppose the elimination order E, A, B, G, D is chosen. Compute the sizes of
the tables that have to be manipulated at each stage in the computation. Compute
the sizes of the tables that have to be manipulated for an elimination order: D,
G, B, A, E.

116
EXERCISES: EVIDENCE, SUFFICIENCY AND MONTE CARLO METHODS
(b) Write a MATLAB code for the problem of ﬁnding an efﬁcient bucket elimination
to compute pF|C(1|1) where the variables A, B, C, D, E, F, G are all binary
variables, the probability distribution p (A, B, C, D, E, F,G) may be factorized
according to a directed acyclic graph, the input, for each variable A, B, C, D,
E, F, G is the set of parents and the programme returns the elimination order
that yields the smallest sum of table sizes.
9. Divorcing
The following example considers a simple principle known as
divorcing, for reducing the sizes of the tables required. If the parent set (X)
of a variable X contains variables A1, . . . , Am, B1, . . . , Bn, where (A1, . . . ,
Am) ⊥(B1, . . . , Bn)∥GX, then there are circumstances where it may be possible to
introduce two divorcing variables C1 and C2, where C1 ⊥C2∥GX, C1 and C2 are
parents of X, (C1) = {A1, . . . , Am}, (C2) = {B1, . . . , Bn} and the direct links
from A1, . . . , Am, B1, . . . , Bn to X are removed.
Consider a Bayesian network connected with a land search-and-rescue operation.
The purpose is to predict the condition of an individual when found. There are
a large number of variables that inﬂuence this: temperature, wind, precipitation,
mental and physical health, age (which inﬂuences mental and physical health). But
the most important feature about the weather is whether or not the combination of
wind, precipitation and temperature can cause a dangerous wind chill, which leads
to hypothermia. The various aspects of a person’s health may be summarized as his
condition when lost, and the extend to which a person’s health deteriorates depends
on the person’s age and condition.
Construct a full Bayesian network containing all the variables and, from this,
construct a Bayesian network with appropriate divorcing variables. Assuming all
variables are binary, compute the sizes of the tables for the full network and compute
the sizes of the tables for the network with the divorcing variables.
10. Hidden Variables
Consider a situation where H is a hidden variable and observa-
tions are made on the variables (Ij)n
j=1. Let p(H) denote the prior distribution of
H. Show that, given observations on the variables (Ij)n
j=1, the posterior is given by
p(H|(Ij)n
j=1) = µp(H)
n

j=1
p(Ij|H)
where µ is a normalization constant if and only if the variables satisfy the causal
relations shown in the diagram in Figure 3.10.
H
I1
I2
In
. . .
Figure 3.10
Hidden variables example.

EVIDENCE, SUFFICIENCY AND MONTE CARLO METHODS
117
11. Monte Carlo
Simulate the Markov chain X = (X0, X1, X2, . . .) with state space
{1, 2, 3, 4} and one-step transition matrix
P =


0
1
2
0
1
2
1
2
0
1
2
0
0
1
2
0
1
2
0
1
2
0
1
2


.
Simulate it for n = 10, 100 and 1000 steps, with initial conditions X0 = 1 and X0 =
2. What are ( ˆp(1), ˆp(2), ˆp(3), ˆp(4)) for each simulation? Is this chain aperiodic?
12. A Metropolis Algorithm for Probabilistic Inference in a Bayesian Network Let
X = (X1, . . . , Xd) be a random vector where each variable is binary. That is, X =
X1 × . . . × Xd where Xj = {0, 1} for each j ∈˜V = {1, . . . , d}. An instantiation of
X is therefore a vector x = (x1, . . . , xd), where xi ∈{0, 1} for i = 1, . . . , d.
Let ˜V = {1, . . . , d} and ˜U = {i1, . . . , im} ⊆˜V . Let X ˜U = (Xi1, . . . , Xim) and assume
that X ˜U = y; that is, the variables X ˜U are instantiated. as y. Let X ˜V \ ˜U denote the
uninstantiated variables and let x ˜V \ ˜U denote a generic element of X ˜V \ ˜U = {0, 1}t,
where t is the number of uninstantiated variables.
The aim is to compute the probabilities
pX ˜V \ ˜U |X ˜U

x ˜V \ ˜U | y

=
pX ˜V \ ˜U ,X ˜U

x ˜V \ ˜U, y

pX ˜U

y

.
As will be shown in the later chapters, there are deterministic algorithms for prob-
abilistic inference, which compute the desired conditional probabilities exactly, but
since probabilistic inference in a Bayesian network is NP-hard in the number of
variables (G.F. Cooper [82]), these exact algorithms are not expected to run in poly-
nomial time. It therefore makes sense to run an approximate algorithm, if it converges
rapidly.
This exercise designs a Markov chain Monte Carlo method for approximate proba-
bilistic inference with pX ˜V \ ˜U |X ˜U

x ˜V \ ˜U | y

as the target distribution, based on the
Metropolis algorithm. Firstly, deﬁne a random walk {Zn}n≥1 with values in {0, 1}t
as follows.
• Let x ˜V \ ˜U =

x ˜V \ ˜U,i
t
i=1 and assume that Zn = x ˜V \ ˜U.
• Proposal.
Choose at random and independently of Zn an integer l in {1, . . . , t}
(For, convenience of writing, ˜V \ ˜U = {1, . . . , t} has been renumbered), i.e. q(l) =
1
t . Then deﬁne a new vector x∗
˜V \ ˜U in {0, 1}t by
x∗
˜V \ ˜U,l = x ˜V \ ˜U,l ⊕1

118
EXERCISES: EVIDENCE, SUFFICIENCY AND MONTE CARLO METHODS
where 1 ⊕1 = 0 ⊕0 = 0, 1 ⊕0 = 0 ⊕1 = 1, and
x∗
˜V \ ˜U,i = x ˜V \ ˜U,i,
if i ̸= l
In other words, x∗
˜V \ ˜U differs from x ˜V \ ˜U only in the lth coordinate.
• Acceptance.
Take
Zn+1 =
 x∗
˜V \ ˜U
with probability αx ˜V \ ˜U ,x∗
˜V \ ˜U
x ˜V \ ˜U
with probability 1 −αx ˜V \ ˜U,x∗
˜V \ ˜U
where
αx ˜V \ ˜U,x∗
˜V \ ˜U = min

1, f ∗
f
"
,
where for a potential φ,
f ∗
f =
φ

X ˜V \ ˜U,l = x∗
˜V \ ˜U,l, Ml

φ

X ˜V \ ˜U,l = x ˜V \ ˜U,l, Ml

and where, ﬁnally, Ml is the Markov blanket of the variable X ˜V \ ˜U,l in the DAG.
Here, the variable X ˜V \ ˜U,l is the variable in X ˜V \ ˜U that was picked in the proposal
step of the algorithm and φ is a function of X ˜V \ ˜U,l and the variables in Ml. Note
that this algorithm requires no operations of marginalization (required by bucket
elimination).
• The questions are:
(a) Show that {Zn}n≥1 is an irreducible, aperiodic Markov chain in {0, 1}t.
(b) Find the expression for the potential φ.
(c) Show that pX ˜V \ ˜U |X ˜U

x ˜V \ ˜U | y

is the stationary distribution of {Zn}n≥1.
Hence {Zn}n≥1 is a sequence of samples from the target distribution, which is
pX ˜V \ ˜U |X ˜U

. | y

, and any value pX ˜V \ ˜U |X ˜U

x ˜V \ ˜U | y

may be computed by counting
the number of times x ˜V \ ˜U occurs in the samples.
This is a Metropolis algorithm based on Pearl [83].
The crucial thing to be investigated, but not requested here, is the run time of the
algorithm needed to approximate the target distribution with a given error bound.
13. The purpose of this exercise is to use the software technology from HUGIN Expert
A/S to make queries on the Bayesian network speciﬁed later, in Chapter 9 (The Wet
Pavement and the Sprinkler).
The Wet Pavement and the Sprinkler A simpliﬁed version of a standard example
of a Bayesian network is given by the DAG in Figure 3.11 and by the conditional
probability potentials given below. (In HUGIN, the term ‘conditional probability
table’, abbreviated CPT is used).
C = Cloudy, S = Sprinkler,
R = Rain, W = WetGrass.

EVIDENCE, SUFFICIENCY AND MONTE CARLO METHODS
119
The joint probability distribution is assumed to factorize according to the graph as
product of the following conditional probability tables:
pS|C =
S\C
0
1
0
0.5
0.9
1
0.5
0.1
pR|C =
R\C
0
1
0
0.8
0.2
1
0.2
0.8
pW|R,S(1|., .) =
S\R
0
1
0
0.00
0.90
1
0.9
0.99
The prior distribution is
pC(0) = pC(1) = 0.5.
(a) Find pC|W (1 | 1) using HUGIN.
(b) Find pC|S (1 | 1) and pR|S (1 | 1) using HUGIN.
(c) What is the state of maximum probability for C if W = 1 ? This query can be
handled by using the max propagation button in the tool bar of the run mode
of HUGIN.
Using HUGIN Lite for propagating evidence and making queries Start the pro-
gramme by clicking on the icon. This opens a window with a command tool bar.
You are now in the edit mode of HUGIN. You use the tool bar to introduce your
DAG in the window.
The package includes a help ﬁle (click on Help in the tool bar of the starting window)
with a manual that explains how to use the software. The steps needed are outlined
below.
• Open the menu under ‘Network’, click on Auto propagate and choose Do not
auto propagate (this is more instructive for our aims than auto propagation).
• Draw your DAG:
(a) By clicking with left mouse button in the command tool bar at the button
depicting an ellipse with a single boundary choose ﬁrst (discrete) nodes and
place them in the window. Then draw your DAG in the window by connecting
the nodes with arrows, press the left mouse button and move from the centre
of a parent node to the centre of the child node and release the mouse button.
(b) You may give names to nodes by double clicking on a node, thus opening an
appropriate window.

120
EXERCISES: EVIDENCE, SUFFICIENCY AND MONTE CARLO METHODS
C
R
S
W
Figure 3.11
Sprinkler, Rain and Wet Pavement.
• Introduce your CPTs (CPT–Conditional Probability Table in HUGIN).
(a) Click on a node in the DAG you created in the edit mode.
(b) Then use ctrl + left (?) mouse click. A window should now open and you
may write the probabilities in the window as text. Note the correct order of
probabilities, where a column is a conditional probability distribution.
(c) Repeat the same for all nodes.
(d) The CPT windows may be closed/opened by clicking on the table button in
the tool bar.
• Click the yellow ﬂash button in the tool bar, and you enter the run mode in
HUGIN. (You may get back to edit mode by clicking the yellow pen button.) The
network is now compiled.
• Introduce evidence:
(a) Check the initial marginal distributions on the nodes by using expand node
list in the tool bar. You will see the distribution in green coloured staples and
in digits.
(b) Collapse the node list using the button on the right hand side of the expand
button.
(c) Click on a node.
(d) Click then on the enter likelihood evidence button (three green bars) in the
tool bar.
(e) Write in the evidence (here 0 and 1).
(f) a tag with little red e inside a box should appear in the node where evidence
is introduced.
(g) repeat for all nodes where you want evidence to be inserted.
(h) expand the node list (a button in the tool bar). The nodes with evidence should
be shown and have red colour, and the other nodes have a grey colour.
• PROPAGATE (query: posterior probability): click in the tool bar (run mode) on
the sum propagate button (depicting a  under two arrows in each direction).
The posterior probabilities of the states will be emerge in green colour and will

EVIDENCE, SUFFICIENCY AND MONTE CARLO METHODS
121
also be shown as digits in the left hand part of the window (expand this part if
necessary).
• PROPAGATE (query: state of maximum probability): with evidence inserted as in
the procedure above, click on the max propagate button The states with maximum
probabilities will be emerge with number 100 assigned in the left hand part of the
window (extend this part if necessary).
• Use the print button to dump down the result, i.e. the screen, on a printer.
By using the refresh button (indicated by a loop arrow) you can always restart with
the DAG initially speciﬁed. In the edit mode (yellow pencil) you may modify the
graph and the CPTs.


4
Decomposable graphs and
chain graphs
Having discussed the way that information and uncertainty about variables are represented
by a Bayesian network, the next task is to develop the graph theory that is used for learning
the graphical structure and for constructing a junction tree to enable the probability
distribution to be updated efﬁciently in the light of new information.
The second of these topics will be considered ﬁrst. Once the graph structure has been
learned, there are many ways of updating the probability distribution in light of new
information, but attention is focused on the fact that Bayesian networks are ‘tree-like’,
or ‘almost trees’, in the sense that they exhibit many of the important properties of trees
that are useful for computations. This was the basic idea of Pearl, Lauritzen and Speed
in the 1980s, as pointed out by S. Arnborg in [84] and [85]. One key idea is that the
Bayesian network may be expressed as a junction tree, where the nodes are groups of
variables. Some selected edges are added in, and the edges made undirected, to form a
decomposable graph, from which the junction tree is constructed.
Next, the necessary structures for considering classes of Markov equivalent graphs
will be considered. Markov equivalent directed acyclic graphs were introduced in
Chapter 2 and it is convenient to develop a language to characterize them, in terms of
the essential graph, which will be introduced in this chapter and used in Chapter 6.
The essential graph is a graph that retains all the directed edges that are common to all
the Markov equivalent graphs, and removes the direction of the remaining edges, by
replacing them with undirected edges. The collection of essential graphs is important
when applying numerical methods to learn the graph structure, the topic of Chapter 6; if
a Markov chain Monte Carlo algorithm is to be efﬁcient, then it should move between
graph structures that are essentially different. A brief outline of the basic ideas of
Bayesian Networks: An Introduction
T. Koski, J. Noble
2009 John Wiley & Sons, Ltd

124
DECOMPOSABLE GRAPHS AND CHAIN GRAPHS
Markov chains is given in Chapter 3, the essential graph is introduced in this chapter
and a suitable Markov chain Monte Carlo algorithm for locating the essential graph is
discussed in Chapter 6.
4.1
Deﬁnitions and notations
Many of the basic notations and deﬁnitions necessary for the text have already been given
in Chapter 2, the remainder are given here.
Recall the deﬁnition of an induced sub-graph (Deﬁnition 2.6).
Deﬁnition 4.1 (Complete Graph, Complete Subset) A graph G is complete if every pair
of nodes is joined by an undirected edge. That is, for each (α, β) ∈V × V with α ̸=
β, (α, β) ∈E and (β, α) ∈E. In other words, ⟨α, β⟩∈U, where U denotes the set of
undirected edges.
A subset of nodes is called complete if it induces a complete sub-graph.
Deﬁnition 4.2 (Clique) A clique is a complete sub-graph that is maximal with respect to
⊆. In other words, a clique is not a sub-graph of any other complete graph.
Deﬁnition 4.3 (Simplicial Node) Recall the deﬁnition of family, found in Deﬁnition 2.3.
For an undirected graph, the family of a node β is F(β) = {β} ∪N(β), where N(β)
denotes the set of neighbours of β. A node β in an undirected graph is called simplicial
if its family F(β) is a clique.
Deﬁnition 4.4 (Connectedness, Strong Components) Let G = (V, E) be a simple graph,
where E = U ∪D. That is, E may contain both directed and undirected edges. Let α →β
denote that there is a path (Deﬁnition 2.8) from α to β. If there is both α →β and β →α
then α and β are said to be connected. This is written:
α ↔β.
This is clearly an equivalence relation. The equivalence class for α is denoted by [α]. In
other words, β ∈[α] if and only if β ↔α. These equivalence classes are called strong
components of G.
Note that a graph is connected if between any two nodes there exists a trail (Deﬁnition
2.7), but any two nodes α and β are only said to be connected if there is path from α to
β and a path between β and α, where the deﬁnition of a ‘path’ is given in Deﬁnition 2.8.
Deﬁnition 4.5 (Separator) A subset S ⊆V is called an (α, β) separator if every trail from
α to β intersects S. Let A ⊆V and B ⊆V , such that A, B and S are disjoint. The subset
S is said to separate A from B if it is an (α, β) separator for every α ∈A and β ∈B.
Deﬁnition 4.6 (Minimal Separator) Let A ⊆V , B ⊆V and S ⊆V be three disjoint
subsets of V . Let S separate A and B. The separator S is said to be a minimal
separator of A and B if no proper subset of S is itself an (α, β) separator for any
(α, β) ∈A × B.

DEFINITIONS AND NOTATIONS
125
root
g
d
leaf
leaf
leaf
Figure 4.1
Illustration of a rooted tree.
Deﬁnition 4.7 (Rooted Tree) A rooted tree T is a tree graph with a designated node ρ
called the root. A leaf of a tree is a node that is joined to at most one other node. Figure 4.1
gives an illustration of a rooted tree.
Deﬁnition 4.8 (Diameter) The diameter of a tree is the length of the longest trail between
two leaf nodes.
Deﬁnition 4.9 (Moral Graph) Let G be a DAG. Then G is said to be moralized if all
undirected edges between all pairs of parents of each node which are not already joined
are added and then all edges are made undirected.
An example of a DAG is given in Figure 4.2. The result of moralizing is given in
Figure 4.3. The cliques of the moral graph are illustrated in Figure 4.4.
Deﬁnition 4.10 (Chord) Let G = (V, E) be an undirected graph. Let σ be an n cycle in
G. A chord of this cycle is a pair (αi, αj) of non-consecutive nodes in σ such that αi ∼αj
in G.
Deﬁnition 4.11 (Triangulated)
An undirected graph G = (V, E) is triangulated if every
one of its cycles of length ≥4 possesses a chord.
a1
a2
a3
a5
a4
a7
a6
Figure 4.2
A directed acyclic graph.
a1
a2
a3
a5
a4
a7
a6
Figure 4.3
The graph in Figure 4.2, after moralizing.

126
DECOMPOSABLE GRAPHS AND CHAIN GRAPHS
a1
a2
a3
a5
a3
a4
a7
a5
a6
Figure 4.4
Cliques of the graph in Figure 4.3.
Lemma 4.1 If G = (V, E) is triangulated, then the induced graph GA is also triangulated.
Proof of Lemma 4.1 Consider any cycle of length ≥4 in the restricted graph. All the
edges connecting these nodes remain. If the cycle possessed a chord in the original graph,
the chord remains in the restricted graph.
□
Deﬁnition 4.12 (Decomposition) A triple (A, B, S) of disjoint subsets of the node set V
of an undirected graph is said to form a decomposition of G or to decompose G if
V = A ∪B ∪S
and
• S separates A from B,
• S is a complete subset of V .
A, B or S may be the empty set. If both A and B are non-empty, then the decomposition
is proper.
Clearly, every graph can be decomposed to its connected components (Deﬁnition 2.7).
If the graph is undirected, then the connected components are the strong components
(Deﬁnition 4.4).
Deﬁnition 4.13 (Decomposable Graph) An undirected graph G is decomposable if either
1. it is complete, or
2. it possesses a proper decomposition (A, B, S) such that both sub graphs GA∪S and
GB∪S are decomposable.
This is a recursive deﬁnition, which is permissible, since the decomposition (A, B, S)
is required to be proper, so that GA∪S and GB∪S have fewer nodes than the original
graph G.
Example 4.1 A decomposable graph
Consider the graph in Figure 4.5. In the ﬁrst
stage, set S = {α3}, with A = {α1, α2} and B = {α4, α5, α6}. Then S is a clique and S
separates A from B. Then A ∪S = {α1, α2, α3} and GA∪S is a clique. B ∪S = {α3, α4,
α5, α6}.
The graph GB∪S is decomposable; take S2 = {α3, α5}, A2 = {α4} and B2 = {α6}. Then
GA2∪S2 and GB2∪S2 are cliques.
□

DECOMPOSABLE GRAPHS AND TRIANGULATION OF GRAPHS
127
a1
a2
a3
a5
a4
a6
Figure 4.5
Example of a decomposable graph.
4.2
Decomposable graphs and triangulation of graphs
Decomposable graphs provide the basis for one of the key methods for updating a prob-
ability distribution described in terms of a Bayesian network. The DAG is moralized and
then triangulated using the most efﬁcient triangulation algorithms available. The triangu-
lated graph is then decomposed and organized to form a junction tree, which supports
effective algorithms, one of which will be described in Chapter 10.
Theorem 4.1 Let G = (V, E) be an undirected graph. The following conditions 1), 2)
and 3) are equivalent.
1. G is decomposable.
2. G is triangulated.
3. For every pair of nodes (α, β) ∈V × V , their minimal separator is complete.
Proof of Theorem 4.1 The proof below follows the lines of Cowell, Dawid, Lauritzen
and Spiegelhalter in [67].
Proof of 1) ⇒2), Theorem 4.1
Inductive hypothesis: All undirected decomposable graphs with n nodes or less are
triangulated. This is true for one node.
Let G be a decomposable graph with n + 1 nodes. There are two alternatives:
Either G is complete, in which case it is triangulated,
Or: by the deﬁnition of decomposable, there are three disjoint subsets A, B, S such that
S is a complete subset, S separates A from B, V = A ∪B ∪S and GA∪S and GB∪S are
decomposable. The decomposition is proper, hence GA∪S and GB∪S have less than or
equal to n nodes. Therefore, by the inductive hypothesis GA∪S and GB∪S are triangulated.
Therefore, a cycle of length ≥4 without a chord, will be a cycle from A which passes
through B. By decomposability, S separates A from B and therefore any such cycle must
pass S at least twice. But then this cycle has a chord, since S is a complete subset.
□
Proof of 2) ⇒3), Theorem 4.1 Assume that G = (V, E) is an undirected, triangulated
graph. Let S be a minimal separator for two nodes α and β. Let A denote the set such that
α ∈A and GA is the largest connected sub-graph of GV \S such that α is in the node set.
Let B = V \(A ∪S). For every node γ ∈S, there is a node τ ∈A such that ⟨γ, τ⟩∈E
and there is a node σ ∈B such that ⟨γ, σ⟩∈E. Otherwise S\{γ } would be a separator
for α and β, contradicting the minimality of S. Hence, for any pair (γ, δ) ∈S × S, there

128
DECOMPOSABLE GRAPHS AND CHAIN GRAPHS
exist paths γ, τ1, . . . , τm, δ and γ, σ1, . . . , σn, δ where all the nodes {τ1, . . . , τm} are in A
and all the nodes {σ1, . . . σn} are in B. Then γ, τ1, . . . , τm, δ, σn, . . . , σ1, γ is a cycle of
length ≥4 and therefore has a chord. Assume that τ1, . . . , τm and σ1, . . . , σn have been
chosen so that the paths are as short as possible (that is, there is no shorter path from γ
to δ with all intervening nodes in A and no shorter path from γ to δ with all intervening
nodes in B).
The chord cannot be of the form ⟨τi, τj⟩for some (i, j) or ⟨σk, σl⟩for any (k, l)
because of the minimality of the lengths of the chosen paths. Therefore, γ and δ are
adjacent for every pair (γ, δ) ∈S × S. It follows that S is a clique.
□
Proof of 3) ⇒2), Theorem 4.1 (This part is unnecessary for the argument, but the
direct proof is reasonably straightforward). Assume that for every pair of nodes (α, β),
their minimal separator is complete. Let α, γ, β, τ1, . . . , τr, α be a cycle of length ≥4
in G. If ⟨α, β⟩is not a chord of the cycle, then denote by S the minimal separator that
puts α and β in different components of GV \S. Then S clearly contains γ and τj for
some j ∈{1, . . . , r}. By hypothesis, S is a clique, so that ⟨γ, τj⟩∈E and ⟨γ, τj⟩is
a chord of the cycle. Therefore, any cycle of length ≥4 has a chord, therefore G is
triangulated.
Proof of 3) ⇒1), Theorem 4.1 If G is complete, then the result is clear. If G is
not complete, then choose two distinct nodes (α, β) ∈V × V that are not adjacent. Let
S ⊆V \{α, β} denote the minimal separator for the pair (α, β). Let A denote the node set
of the maximal connected component of GV \S and let B = V \(A ∪S). Then (A, B, S) pro-
vides a decomposition. This procedure can be repeated on GA∪S and GB∪S, and repeated
recursively, hence the graph is decomposable.
□
Deﬁnition 4.14 (Perfect Node Elimination Sequence) Let V = {α1, . . . , αd} denote the
node set of a graph G. A perfect node elimination sequence of a graph G is an ordering of
the node set {α1, . . . , αd} such that for each j in 1 ≤j ≤d −1, αj is a simplicial node
of the sub-graph of G induced by {αj, αj+1, . . . , αd}
Lemma 4.2 Every triangulated graph G has a simplicial node. Moreover, if G is not
complete, then it has two non-adjacent simplicial nodes.
Proof of Lemma 4.2 The lemma is trivial if either G is complete, or else G has two or
three nodes. Assume that G is not complete. Suppose the result is true for all graphs with
fewer nodes than G. Consider two non-adjacent nodes α and β. Let S denote the minimal
separator of α and β. Let GA denote the largest connected component of GV \S such that
α ∈A and let B = V \(A ∪S), so that β ∈B.
By induction, either GA∪S is complete, or else it has two non-adjacent simplicial
nodes. Since GS is complete, it follows that at least one of the two simplicial nodes is in
A. Such a node is therefore also simplicial in G, because none of its neighbours is in B.
If GA∪S is complete, then any node of A is a simplicial node of G.
In all cases, there is a simplicial node of G in A. Similarly, there is a simplicial node
in B. These two nodes are then non-adjacent simplicial nodes of G.
□
Theorem 4.2 A graph G is triangulated if and only if it has a perfect node elimination
sequence.

DECOMPOSABLE GRAPHS AND TRIANGULATION OF GRAPHS
129
Proof of Theorem 4.2 Suppose that G is triangulated. Assume that every triangulated
graph with fewer nodes than G has a perfect elimination sequence. By the previous
lemma, G has a simplicial node α. Removing α returns a triangulated graph. (Consider
any cycle of length ≥4 with a chord. If the cycle remains after the node is removed,
then the chord is not removed). By proceeding inductively, it follows that G has a perfect
elimination sequence.
Conversely, assume that G has a perfect sequence, say {α1, . . . , αd}. Consider any
cycle of length ≥4. Let j be the ﬁrst index such that αj is in the cycle. Let V (C)
denote the node set of the cycle and let Vj = {αj, . . . , αd}. Then V (C) ⊆Vj. Since αj
is simplicial in GVj+1, the neighbours of αj in the cycle are adjacent, hence the cycle has
a chord. Therefore G is triangulated.
□
Deﬁnition 4.15 (Eliminating a Node) Let G = (V, E) be an undirected graph. A node α
is eliminated from an undirected graph G in the following way:
1. For all pairs of neighbours (β, γ ) of α add a link if G does not already contain
one. The added links are called ﬁll ins.
2. Remove α.
The resulting graph is denoted by G−α.
Example 4.2
Consider the graph in Figure 4.6.
This graph is already triangulated. But suppose one did not notice this and decided to
eliminate node α3 from the graph in Figure 4.6. The resulting graph is given in Figure 4.7.
Deﬁnition 4.16 (Elimination Sequence) An elimination sequence of G is a linear ordering
of its nodes.
a1
a2
a3
a4
a5
a6
Figure 4.6
Example for Deﬁnition 4.5, eliminating a node.
a1
a2
a5
a6
a4
Figure 4.7
Graph 4.6 with α3 eliminated.

130
DECOMPOSABLE GRAPHS AND CHAIN GRAPHS
a1
a2
a3
a5
a4
a6
Figure 4.8
Gσ. Elimination sequence (α3, α2, α4, α1, α5, α6).
Let σ be an elimination sequence and let  denote the ﬁll-ins produced by eliminating
a node of G in the order σ. Denote by Gσ the graph G extended by .
Example 4.3
Consider the graph in Figure 4.6. Suppose the elimination sequence α3, α2,
α4, α5, α6 is employed. Then the ﬁll-ins, for each stage, will be ⟨α1, α6⟩, ⟨α1, α5⟩, ⟨α2, α6⟩
for α3, then ⟨α1, α4⟩, ⟨α4, α6⟩for α2. No further ﬁll-ins are required. The graph Gσ is
given in Figure 4.8.
Deﬁnition 4.17 (Elimination Domains) Consider an elimination sequence σ; that is a
linear ordering of the nodes, such that for any node α, σ(α) denotes the number assigned
to α. A node β is said to be of higher elimination order than α if σ(β) > σ(α). The
elimination domain of a node α is the set of neighbours of α of higher elimination order.
In Gσ, any node α together with its neighbours of higher elimination order form a complete
subset. The neighbours of α of higher elimination order are denoted by Nσ(α). The sets
Nσ(α) are the elimination domains corresponding to the elimination sequence σ.
An efﬁcient algorithm clearly tries to minimize the number of ﬁll-ins. If possible, one
should ﬁnd an elimination sequence that does not introduce ﬁll-ins.
Proposition 4.1 All cliques in a Gσ are a Nσ(α) for some α ∈V .
Proof Let C be a clique in Gσ and let α be a variable in C of the lowest elimination
order. Then C = Nσ(α).
□
An efﬁcient algorithm ought to ﬁnd an elimination sequence for the domain graph that
yields cliques of minimal total size.
The following proposition is clear.
Proposition 4.2 Any Gσ is a triangulation of G.
□
It is known (proof omitted) that the algorithms for triangulating a graph are NP - complete;
i.e. a graph can be triangulated in a number of steps that is polynomial in the number of
nodes.
Recall that a graph is triangulated if and only if it has an elimination sequence without
ﬁll-ins. This is equivalent to the statement that an undirected graph is triangulated if and
only if all nodes can be eliminated by successively eliminating a node α such that the
family Fα = {α} ∪Nα is complete. From the deﬁnition, such a node α is a simplicial node.

JUNCTION TREES
131
4.3
Junction trees
The purpose of this section is to deﬁne junction trees and to show how to construct them.
They provide a key tool for updating a Bayesian network.
Deﬁnition 4.18 (Junction Trees) Let C be a collection of subsets of a ﬁnite set V and T
be a tree with C as its node set. Then T is said to be a junction tree (or join tree) if any
intersection C1 ∩C2 of a pair C1, C2 of sets in C is contained in every node on the unique
path in T between C1 and C2. Let G be an undirected graph and C the family of its cliques.
If T is a junction tree with C as its node set, then T is known as junction tree for the
graph G.
Theorem 4.3 There exists a junction tree T of cliques for the graph G if and only if G is
decomposable.
Proof of Theorem 4.3 The proof is by construction; a sequence is established in the
following way. Firstly, a simplicial node α is chosen; Fα is therefore a clique. The
algorithm continues by choosing nodes from Fα that only have neighbours in Fα. Let i
be the number of nodes in Fα that only have neighbours in Fα. The set of nodes Fα is
labelled Vi and the set of those nodes in Fα that have neighbours not in Fα is labelled
Si. This set is a separator.
Now remove the nodes in Fα that do not have neighbours outside Fα and name the
new graph G′. Choose a new node α in the graph G′ such that Fα is a clique. Repeat the
process, with the index j, where j is the previous index, plus the number of nodes in
the current Fα that only have neighbours in Fα.
When the parts have been established (as indicated in the diagram below), each
separator Si is then connected to a clique Vj with j > i and such that Si ⊂Vj. This is
always possible, because Si is a complete set and, in the elimination sequence described
above, the ﬁrst point of Si is eliminated when dealing with a clique of index greater
than i.
It is necessary to prove that the structure constructed is a tree and that it has the
junction tree property.
Firstly, each clique has at most one parent, so there are not multiple paths. The
structure is therefore a tree.
To prove the junction tree condition, consider two cliques, Vi and Vj with i > j and
let α be a member of both. There is a unique path between Vi and Vj.
Because α is not eliminated when dealing with Vi, it is a member of Si. It is also a
member of the parent of Vi, say Vk and is a member of the parent of Vk and, by induction
it is also a member of Vj and, of course, all the separators in between.
□
Example 4.4
Consider the directed acyclic graph in Figure 4.9. The corresponding
moral graph is given in Figure 4.10.
An appropriate elimination sequence for this moral graph is
(α8, α7, α4, α9, α2, α3, α1, α5, α6).
There are two ﬁll-ins; these are ⟨α1, α5⟩corresponding to the elimination of α2 and
⟨α1, α6⟩, corresponding to the elimination of α3. The corresponding triangulated graph is
given in Figure 4.11.

132
DECOMPOSABLE GRAPHS AND CHAIN GRAPHS
a1
a2
a5
a3
a9
a4
a7
a8
a6
Figure 4.9
A directed acyclic graph for Example 4.3.
a1
a2
a3
a5
a9
a4
a7
a8
a6
Figure 4.10
Moral graph corresponding to Figure 4.9.
a1
a2
a3
a5
a4
a7
a8
a6
a9
Figure 4.11
The triangulated graph corresponding to Figure 4.10.

MARKOV EQUIVALENCE
133
{a4, a7, a8}
V2
{a4, a7}
S2
{a3, a4, a6}
V4
{a3, a6}
S4
{a1, a2, a5}
V6
{a1, a5}
S6
{a1, a5, a6}
V9
{a4, a6, a7}
V3
{a4, a6}
S3
{a5, a6, a9}
V5
{a5, a6}
S5
{a1, a3, a6}
V7
{a1, a6}
S7
Figure 4.12
The Cliques and Separators from Figure 4.11.
V4
{a3, a4, a6}
S4 = {a3, a6}
S3 = {a4, a6}
S2 = {a4, a7}
S7 = {a1, a6}
S6 = {a1, a5}
S5 = {a5, a6}
V7
{a1, a3, a6}
V9
{a1, a5, a6}
V3
{a4, a6, a7}
V6
{a1, a2, a5}
V2
{a4, a7, a8}
V5
{a5, a6, a9}
Figure 4.13
A junction tree (or join tree) constructed from the triangulated graph in
Figure 4.11.
The junction tree construction may be applied. The cliques and separators, with the
labels resulting from the diagram, are shown in Figure 4.12 and put together to form the
junction tree, or join tree, shown in Figure 4.13.
4.4
Markov equivalence
Section 2.10 discussed the concept of Markov equivalence for directed acyclic graphs.
That is, two different directed acyclic graphs over the same set of variables V are said to

134
DECOMPOSABLE GRAPHS AND CHAIN GRAPHS
X2
X3
X1
X2
X3
X1
X2
X3
X1
Figure 4.14
Three DAGs, each with the same independence structure.
X2
X1
X3
Figure 4.15
Not Markov equivalent to the graphs in Figure 4.14.
be Markov equivalent if they have exactly the same d-separation properties. The formal
deﬁnition of Markov equivalence for directed acyclic graphs is given by Deﬁnition 2.32.
When trying to ﬁt a graphical model to data, the subject of Chapter 6, all the DAGs
in a Markov equivalence class will ﬁt the data equally well and efﬁcient algorithms for
ﬁnding the structure will therefore only examine the different equivalence classes, rather
than all the different possible DAGs. If there are direct causal dependencies between the
variables, additional information about the causal structure is needed to choose the DAG
from the equivalence class that has an appropriate causal interpretation.
Figure 4.14 shows three directed acyclic graphs, each with the same independence
structure; for any probability distribution that factorizes according to these DAGs, X1 ⊥
X3|X2. Markov equivalence is considered in Exercise 7. In that exercise, it is shown that
a distribution factorized along the DAG given in Figure 4.15 does not have the same
independence structure as those in Figure 4.14.
One key result in this section is Theorem 4.4, which states that two features of the
directed acyclic graph are necessary and sufﬁcient to determine its Markov structure; its
immoralities and its skeleton. These are deﬁned below.
Deﬁnition 4.19 (Immorality) Let G = (V, E) be a graph. Let E = D ∪U, where D con-
tains directed edges, U contains undirected edges and D ∩U = φ. An immorality in a
graph is a triple (α, β, γ ) such that (α, β) ∈D and (γ, β) ∈D, but (α, γ ) ̸∈D, (γ, α) ̸∈
D and ⟨α, γ ⟩̸∈U.
Deﬁnition 4.20 (Skeleton) The skeleton of a graph G = (V, E) is the graph obtained by
making the graph undirected. That is, the skeleton of G is the graph ˜G = (V, ˜E) where
⟨α, β⟩∈˜E ⇔(α, β) ∈D or (β, α) ∈D or ⟨α, β⟩∈U.
Theorem 4.4 states simply that two DAGs are Markov equivalent if and only if they have
the same skeleton and the same immoralities. The key to establishing this criteria will be
to consider the active trails (Deﬁnition 2.16) in the graph. The following two deﬁnitions
are also required.

MARKOV EQUIVALENCE
135
Deﬁnition 4.21 (S-active node) Let G = (V, E) be a directed acyclic graph and let S ⊂
V . Recall the deﬁnition of a trail (Deﬁnition 2.5) and the deﬁnition of an active trail
(Deﬁnition 2.16). A node α ∈V is said to be S-active if either α ∈S or there is a directed
path from the node α to a node β ∈S.
Deﬁnition 4.22 (Minimal S-active trail) Let G = (V, E) be a directed acyclic graph and
let S ⊂V . An S-active trail τ in G between two nodes α and β is said to be a minimal
S-active trail if it satisﬁes the following two properties:
1. If k is the number of nodes in the trail, the ﬁrst node is α and the kth node is β,
then there does not exist an S-active trail between α and β with fewer than k nodes
and
2. There does not exist a different S-active trail ρ between α and β with exactly
k nodes such that for all 1 < j < k either ρj = τj or ρj is a descendant
of τj.
Theorem 4.4 was proved by P. Verma and J. Pearl; Corollary 3.2 in [62].
Theorem 4.4 Two DAGs are Markov equivalent if and only if they have the same skeleton
and the same immoralities.
The proof of Theorem 4.4 follows directly from Lemma 4.3.
Lemma 4.3 Let G1 = (V, E1) and G2 = (V, E2) be two directed acyclic graphs with the
same skeletons and the same immoralities. Then for all S ⊂V , a trail is S-active trail in
G1 if and only if it is S-active in G2.
Proof of Lemma 4.3 Recall the notation from Deﬁnition 2.3: α ∼β denotes that two
nodes (α, β) ∈V × V are neighbours. That is, either (α, β) ∈E or (β, α) ∈E. Since G1
and G2 have the same skeletons, any trail τ in G1 is also a trail in G2. Let S ⊂V . Assume
that τ is an S-active trail in G1. It is now proved, by induction on the number of collider
nodes along the path, that τ is also an S-active trail in G2. By deﬁnition, a single node
will be considered an S-active trail, for any S ⊂V . The proof is in three parts: Let τ be
a minimal S-active trail in G1. Then
1. If τ contains no colliders in G1, then it is S-active in G2.
2. If τ contains at least one collider connection centred at node τj, then τ is S-active
in G2 if and only if τj is S-active in G2.
3. If τ contains at least one collider centred at node τj, then τj is an S-active node
in G2.
Part 1:
If τ is an S-active trail in G1 and does not contain any collider connections in
G1, then none of the nodes on τ are in S. This can be seen by considering the Bayes ball
algorithm, which characterizes d-separation. It follows that the path is S-active in G2 if
and only if it does not contain a collider connection in G2.

136
DECOMPOSABLE GRAPHS AND CHAIN GRAPHS
Let τ be a minimal S-active trail in G1 with k nodes and no collider connections in
G1. Suppose that a node τi is a collider node in G2, so that τi−1 and τi+1 are parents of
τi in G2. Then, so that no new immoralities are introduced, it follows that τi−1 ∼τi+1.
Since τi is either a chain or a fork in G1, it follows that in G1, the connections between
nodes τi−2, τi−1, τi, τi+1, τi+2 take one of the forms shown in Figure 4.16 when τi a
chain node or those in Figure 4.17 when τi a fork node.
It is clear from Figure 4.16 and 4.17 that the trail of length k −1 in G1, obtained
by removing τi and using the direct link from τi−1 to τi+1 is also an S-active trail in
G1, contradicting the assumption that τ was a minimal S-active trail. Hence τi is a chain
node or a fork node in G2.
It follows that there are no collider connections along the trail τ taken in G2 and
hence, since it does not contain any nodes that are in S, it is an S- active trail in G2.
ti
ti−2
ti−1
ti+1
ti+2
ti
ti−2
ti−1
ti+1
ti+2
ti
ti−2
ti−1
ti+1
ti+2
ti
ti−2
ti−1
ti+1
ti+2
Figure 4.16
Possible connections between the nodes when τi is chain node.
ti
ti−2
ti−1
ti+1
ti+2
ti
ti−2
ti−1
ti+1
ti+2
Figure 4.17
Possible connections between the nodes if τi is a fork node.

MARKOV EQUIVALENCE
137
Part 2:
Assume that any minimal S-active trail in G1 containing n collider connections
is also S-active in G2. This is true for n = 0 by part 1. Let τ be a trail with k nodes
that is minimal S-active in G1 and with n + 1 collider connections in G1. Consider one
of the collider connections centred at τj, with parents τj−1 and τj+1. Let ˜τ (0,j−1) =
(τ0, τ1, . . . , τj−2, τj−1) and let ˜τ (j+1,k) = (τj+1, . . . , τk). Both ˜τ (0,j−1) and ˜τ (j+1,k) are
minimal S-active in G1 and they both have a number of collider connections less than or
equal to n. By the inductive hypothesis, they are therefore both S-active in G2.
Because the trail τ is minimal S-active in G1, it follows that τj−1 ̸∼τj+1. This is
because both τj−1 and τj+1 are S- active nodes in G1 (they have a common descendant
in S to make the trail active), and neither is in S (neither is the centre of a collider along τ)
it follows that if τj−1 ∼τj+1, then the trail on k −1 nodes obtained by removing the node
τj would be S-active in G1, for the following reason: any chain or fork (τj−2, τj−1, τj+1)
or (τj−1, τj+1, τj+2) would be active because both τj−1 and τj+1 are uninstantiated. Any
collider (τj−2, τj−1, τj+1) or (τj−1, τj+1, τj+2) would be active because both τj−1 and
τj+1 have a descendant in S. It follows that τj−1 ̸∼τj+1. This holds in both G1 and G2,
since the skeletons are the same.
Since ˜τ (1,i−1) and ˜τ (i+1,k) are both active, and τj−1 →τj ←τj+1 is a collider, the
trail τ is active if and only if τj is an active node. That is, it is either in S or has a
descendant in S.
Part 3:
Let τ be a minimal S-active trail in G1 and let τj ∈τ be a collider node in
G1. Since the trail τ is a minimal S-active trail in G1, it follows either that τj ∈S or τj,
considered in G1, has a descendant in S. That is, considered in G1, there is a directed
path from τj to a node w ∈S. Let ρ denote the shortest such path. If τj ∈S, then the
length of the path is 0 and τj is also an S-active node in G2.
Assume there is a directed edge from τj to w ∈S in G1. If there are links from τj−1
to w or τj+1 to w, then these links are τj−1 →w or τj+1 →w respectively, otherwise
the DAG would have cycles. If both are present, then the trail τ violates the second
assumption of the minimality requirement. This is seen by considering the trail formed
by taking w instead of τj in τ. It follows that either τj−1 ̸∼w or τj+1 ̸∼w or neither of
the edges are present. Without loss of generality, assume τj−1 ̸∼w (since the argument
proceeds in the same way if τj+1 ̸∼w). The diagram in Figure 4.18 may be useful.
Since neither τj−1 nor w are parents of τj in G1, they cannot both be parents of τj
in G2, since both graphs have the same immoralities. Furthermore, τj−1 ̸∼τj+1 (since
they are both uninstantiated, and, in G1 both have a common descendant in S, so that if
τj−1 ∼τj+1 then the trail with τj removed would be active whether the connections at
τj−1 and τj+1 are chain, fork or collider, contradicting the minimality assumption). Since
both graphs have the same immoralities and τj−1 ̸∼τj+1, it follows that (τj−1, τj, τj+1)
is an immorality in both G1 and G2 and hence that τj−1 is a parent of τj in G2. Therefore,
τj is a parent of w in G2 and is therefore w is an S- active node in G2.
w
tj
tj−1
tj+1
Figure 4.18
Illustration where τj is an uninstantiated collider node.

138
DECOMPOSABLE GRAPHS AND CHAIN GRAPHS
Assume that for the shortest directed path ρ from τj to w in G1, the ﬁrst l links have
the same directed edges in G2. Now suppose that the shortest directed path is ρ, where
τj = ρ0, . . . , ρl+p = w and consider the links ρl ∼ρl+1 and ρl+1 ∼ρl+2. If ρl ∼ρl+2,
then in G1, the directed edge ρl →ρl+2 is present, otherwise there is a cycle. If the
directed edge ρl →ρl+2 is present in G1, then the path ρ is not minimal. Therefore,
ρl ̸∼ρl+2. This holds in both G1 and G2, because both graphs have the same skeletons.
By a similar argument, ρl−1 ̸∼ρl+1. (there would be a cycle in G1 if (ρl+1, ρl−1) were
present; ρ would not be minimal in G1 if (ρl−1, ρl+1) were present. Since the skeletons
are the same, ρl−1 ̸∼ρl+1 in either G1 or G2). Since ρl ̸∼ρl+2, it follows that ρl and ρl+2
are not both parents of ρl+1 in G2; otherwise G2 would contain an immorality not present
in G1. Similarly, since ρl−1 ̸∼ρl+1, the edge ρl →ρl+1 is present in G2, otherwise G2
would have either the immorality (ρl−1, ρl, ρl+1), since the edge (ρl−1, ρl) is present in
G2 by assumption. It follows that the directed edges (ρl, ρl+1) and (ρl+1, ρl+2) are both
present in G2. By induction, therefore, the whole directed path ρ is also present in G2
and hence τj is an S-active in both G1 and G2.
□
Proof of Theorem 4.4 This follows directly: let G1 and G2 denote two DAGs with the
same skeleton and the same immoralities. For any set S and any two nodes Xi and Xj,
it follows from the lemma, together with the deﬁnition of d-separation, that
Xi ⊥Xj∥G1S ⇔Xi ⊥Xj∥G2S;
(4.1)
if there is an S-active trail between the two variables in one of the graphs, then there
is an S-active trail between the two variables in the other. If there is no S-active trail
between the two variables in one of the graphs then there is no S-active trail between
the two variables in the other. By deﬁnition, two variables are d-separated by a set of
variables S if and only if there is no S-active trail between the two variables. Two graphs
are Markov equivalent, or I-equivalent (Deﬁnition 2.32), if and only if Equation (4.1)
holds for all (Xi, Xj, S) ∈V × V × V.
□
4.5
Markov equivalence, the essential graph
and chain graphs
Consider the DAG in Figure 4.19. Using the characterization given by Theorem 4.4, the
DAG in Figure 4.19 is equivalent to the DAGs in Figure 4.20.
a2
a1
a4
a3
Figure 4.19
A DAG on four nodes.

MARKOV EQUIVALENCE, THE ESSENTIAL GRAPH AND CHAIN GRAPHS
139
a2
a1
a4
a3
a2
a1
a4
a3
Figure 4.20
The equivalent DAGs.
For the DAG in Figure 4.19, all the DAGs with the same skeleton can be enumerated,
and it is clear that those in Figure 4.20 are the only two that satisfy the criteria. To ﬁnd
the DAGs equivalent to the one in Figure 4.19, the immorality (α2, α4, α3) has to be
preserved and no new immoralities may be added. The directed edges (α1, α4), (α2, α4)
and (α3, α4) are therefore essential, the directed edges (α2, α4) and (α3, α4) to form the
immorality, the directed edge (α1, α4) because the connection (α2, α1, α3) is either a fork
or chain, forcing (α1, α4) to prevent a cycle. These three directed edges will be present
in any equivalent DAG. The other three edges may be oriented in 23 different ways, but
only ﬁve of these lead to DAGs (the other graphs contain cycles) and of these ﬁve, only
the three shown in Figures 4.19 and 4.20 have the same immoralities.
A useful starting point for locating all the DAGs that are Markov equivalent to a
given DAG is to locate the essential graph, given in the following deﬁnition.
Deﬁnition 4.23 (Essential Graph) Let G be a directed acyclic graph. The essential graph
G∗associated with G is the graph with the same skeleton as G, but where an edge is
directed in G∗if and only if it occurs as a directed edge with the same orientation in every
DAG that is Markov equivalent to G. The directed edges of G∗are the essential edges of G.
Once the essential graph is located, if it has k undirected edges, then there are 2k directed
graphs to be checked for possible Markov equivalence to G. They are equivalent if they
are acyclic, with the same immoralities. The essential graph contains both directed and
undirected edges, and is an example of a chain graph. The following material gives the
deﬁnition of a chain graph and deals mainly with the properties that will be used when
considering the essential graph, with some extensions.
Deﬁnition 4.24 (Chain Graph) A chain graph is a graph G = (V, E) containing both
directed and undirected edges, where the node set V can be partitioned into n disjoint
subsets V = V1 ∪. . . ∪Vn such that
1. GVj is an undirected graph for all j = 1, . . . , n
2. For any i ̸= j, and any α ∈Vi, β ∈Vj, there is no cycle in G = (V, E) (Deﬁnition
2.10) containing both α and β.
The chain graph consists of components where the edges are undirected, which are
connected by directed edges. The components with undirected edges are known as chain
components, which are deﬁned below.

140
DECOMPOSABLE GRAPHS AND CHAIN GRAPHS
Deﬁnition 4.25 (Chain Component) Let G = (V, E) be a chain graph. Let ˆG = (V, ˆE)
denote the graph obtained by removing all the directed edges from E. Each connected
component of ˜G is known as a chain component.
The chain components (Vj, Fj), j = 1, . . . , n of G therefore satisfy the following con-
ditions:
1. Vj ⊆V and Fj is the edge set obtained by retaining all undirected edges ⟨α, β⟩∈
E such that α ∈Vj and β ∈Vj.
2. There is no undirected edge in E from any node in V \Vj to any node in Vj.
Theorem 4.5 states any essential graph is necessarily a chain graph and presents the addi-
tional features required to ensure that a chain graph is an essential graph corresponding
to a directed acyclic graph. One of the features is that the directed edges in a chain graph
have to be strongly protected.
Deﬁnition 4.26 (Strongly Protected) Let G = (V, E) be a chain graph, where E = D ∪
U. A directed edge (α, β) ∈D is said to be strongly protected if it occurs in at least one
of the conﬁgurations in Figure 4.21.
The following theorem, stated here without proof, characterizes essential graphs. The
statement has been included, because it is a vital step in the Monte Carlo algorithm
presented later, in Section 6.4 for locating the graph structure.
Theorem 4.5 Let G = (V, E) be a graph, where E = D ∪U. There exists a directed
acyclic graph G∗for which G is the corresponding essential graph if and only if G satisﬁes
the following conditions:
1. G is a chain graph,
2. Each chain component of G is triangulated,
a
b
g1
g1
a
b
g2
g1
b
a
a
g1
b
Figure 4.21
The directed edge (α, β) is strongly protected (Deﬁnition 4.26).

MARKOV EQUIVALENCE, THE ESSENTIAL GRAPH AND CHAIN GRAPHS
141
a1
a3
a2
Figure 4.22
A forbidden sub-graph on {α1, α2, α3} in an essential graph.
3. The conﬁguration shown in Figure 4.22 does not occur in any induced sub-graph of
a three variable set {α1, α2, α3} ⊂V (that is, a directed edge (α1, α2), an undirected
edge ⟨α2, α1⟩and no edge between α1 and α3), and
4. Every directed edge (α1, α2) ∈D is strongly protected in G.
Proof of Theorem 4.5 Omitted.
□
It is useful to extend the deﬁnition of faithfulness to the essential graph, since all the
DAGs with the same immoralities and skeleton as the essential graph preserve the same
independence structure. The following discussion extends the deﬁnition of faithfulness
to the chain graph which, in view of Theorem 4.5 covers the essential graph.
Recall deﬁnition of parents, directed and undirected neighbours, from Deﬁnition 2.3.
Since the chain graph contains both directed and undirected edges, the notion of the
‘parents’ of a node has to be extended to ‘ancestral boundary’, which will play a similar
role. It is simply the collection of parents together with the undirected neighbours.
Deﬁnition 4.27 (Ancestral Boundary) The ancestral boundary of a node α in a graph G
is deﬁned as
G(α) = 
(α) ∪N(u)(α)
(4.2)
and the ancestral boundary of a subset A ⊆V is deﬁned as
G(A) = 
(A) ∪N(u)(A).
(4.3)
The closure of a subset A ⊆V in G is deﬁned as
G(A) = A ∪G(A).
(4.4)
Crucial to the extension of the deﬁnition of faithfulness is the deﬁnition of ancestral set.
For a directed acyclic graph, the minimal ancestral set of a node α is simply the set of
all nodes β such that there is a directed path from β to α.
Deﬁnition 4.28 (Ancestral Set, Minimal Ancestral Set) A subset A ⊆V is said to be
ancestral if G(α) ⊆A for each α ∈A, where G(α) is deﬁned in Equation (4.2). For any
subset A ⊆V , the minimal ancestral set is deﬁned as the smallest ancestral set containing
A and is denoted An(A).
For a chain graph, in many situations, a complex, deﬁned below, plays the same role as
an immorality in a directed acyclic graph.

142
DECOMPOSABLE GRAPHS AND CHAIN GRAPHS
g
a2
b
a1
Figure 4.23
A simple chain graph; here S = {α1, α2} and (β, γ, S) is a minimal
complex.
Deﬁnition 4.29 (Complex, Minimal Complex) Recall that (β, α) denotes the directed
edge β →α. Let G = (V, E) be a chain graph. Let α, β ∈V and let S ⊆V . The triple
(α, β, S) is known as a complex in G if S is a connected subset of a chain component C of
G and α, β are two non-adjacent nodes in G(S) ∩G(C). The triple (α, β, S) is known
as a minimal complex in G if there is no strict subset S′ ⊂S such that (α, β, S′) forms a
complex in G.
It follows directly from the deﬁnition that if S contains a single node, then the complex
(α, β, S) is an immorality, where the node in S is a collider.
An example of a simple chain graph, where S = {α1, α2} and (β, γ, S) is a minimal
complex, is given in Figure 4.23.
The following concepts and results have been established. For directed acyclic graphs,
Markov equivalence is deﬁned in Deﬁnition 2.32. That is, two DAGs are Markov equiva-
lent if they have the same d-separation properties. Theorem 4.4 states that two DAGs are
Markov equivalent if and only if they have the same skeleton and the same immoralities.
The essential graph (Deﬁnition 4.23) is the partially directed graph that retains the same
skeleton as a given directed acyclic graph and where the set of directed edges is simply
the set of all those directed edges that are common to all the DAGs that are Markov
equivalent; all the other edges are undirected. The resulting essential graph is a chain
graph.
The notion of local Markov property, the moral graph, and global Markov property
(faithfulness) are now extended to chain graphs.
The deﬁnitions of descendants and ancestors are given in Deﬁnition 2.9; the deﬁnition
of ancestral boundary is given in Deﬁnition 4.27. The deﬁnition of the local Markov
condition (given in Deﬁnition 2.28) may be deﬁned for a chain graph:
Deﬁnition 4.30 (Local Markov Condition for a Chain Graph) Let G = (V, E) be a chain
graph. A probability distribution p over a set of variables V = {X1, . . . , Xd} is said to
be local G-Markovian, or satisﬁes the local Markov condition if for each j ∈{1, . . . , d}
Xj ⊥(V \(D(Xj) ∪G({Xj}))|G({Xj}),
where G and G are deﬁned by Equations (4.4) and (4.3) respectively.
In other words, p satisﬁes the local Markov condition for a graph G = (V, E) if for each
j, Xj is conditionally independent, conditioned on the ancestral boundary (which is the
generalization to chain graphs of the set of parents) of all the other variables that are
not either descendants or belonging to the ancestral boundary. It is left as an exercise
(Exercise 9) to see that this reduced to the local directed Markov condition of Deﬁnition
2.28 when the G is a directed acyclic graph.

MARKOV EQUIVALENCE, THE ESSENTIAL GRAPH AND CHAIN GRAPHS
143
g
a2
b
a1
Figure 4.24
The moralized graph for the chain graph in Figure 4.23.
The deﬁnition of faithfulness (Deﬁnition 2.30) may also be extended to chain graphs.
The following deﬁnitions are necessary to make the extension.
Deﬁnition 4.31 (Moral Graph for a Chain Graph) Let G = (V, E) be a chain graph. The
moral graph Gm is deﬁned as the graph (V, Em), where Em contains the edge ⟨β, γ ⟩if
and only if either
1. (β, γ ) ∈E or (γ, β) ∈E or ⟨α, β⟩∈E, or
2. There exist nodes α1 and α2 (not necessarily distinct) in the same chain component
such that (β, α1) ∈E and (γ, α2) ∈E.
For example, the moral graph for the chain graph of Figure 4.23 is shown in Figure 4.24.
The global Markov property (deﬁned below) generalizes the deﬁnition of faithfulness
to probability distributions represented by chain graphs.
Deﬁnition 4.32 (Global Markov Property) Recall the deﬁnition An(C), the minimal
ancestral set of a subset C ⊆V , given in Deﬁnition 4.28 and recall the Deﬁnition 4.5,
where a separator is deﬁned. Let G = (V, E) be a chain graph, where V = {X1, . . . , Xd}
and let p be a probability function over the variable set V , then p is said to be globally
G-Markovian if for any sets A, B, S ⊂V such that S separates A and B in Gm
An(A∪B∪S)
(the moralized graph restricted to the set An(A ∪B ∪S)), it holds that A ⊥B|S.
It is left as an exercise (Exercise 10) to show that this deﬁnition is equivalent to the
deﬁnition of faithfulness when G = (V, E) is a directed acyclic graph.
The deﬁnition of Markov equivalence (Deﬁnition 2.32) for directed acyclic graphs,
may be extended to chain graphs.
Deﬁnition 4.33 (Markov Equivalence for Chain Graphs) Two chain graphs G1 = (V, E1)
and G2 = (V, E2) are Markov equivalent if for any three sets A, B and S, S separates A
from B in Gm
1 if and only if S separates A from B in Gm
2 .
Deﬁnition 4.34 (Graphical Equivalence for Chain Graphs) Two chain graphs are said to
be graphically equivalent if they have the same skeleton and the same minimal complexes.
The following basic result about Markov equivalence of chain graphs was ﬁrst proved by
M. Frydenberg [86] in a special case (Theorem 5.6 in that article) and then by Madigan,
Andersson and Perlman [87] for the general case.
Theorem 4.6 Two chain graphs G1 = (V, E1) and G2 = (V, E2) are Markov equivalent
if and only if they have the same skeleton and the same minimal complexes. That is, G1
and G2 are Markov equivalent if and only if they are graphically equivalent.
This result extends Theorem 4.4. The proof is omitted.

144
DECOMPOSABLE GRAPHS AND CHAIN GRAPHS
Notes
This chapter presents the basic graph theory necessary for the treatment of
Bayesian networks covered in this text. The theory of decomposable graphs is stan-
dard material from algorithmic graph theory [88]. Some specialized additional material
on the algorithms for Bayesian networks is found in Chapter 4 of [67]. A rigorous and
up-to-date research treatise covering chain graphs is [89] (M. Studen´y).

DECOMPOSABLE GRAPHS AND CHAIN GRAPHS
145
4.6
Exercises: Decomposable graphs and chain graphs
1. Determine the simplicial nodes of the graph in Figure 4.25. Is the graph triangulated?
a2
a1
a4
a3
a5
a7
a6
Figure 4.25
Graph for Example 1.
2. Determine the simplicial nodes for the graph in Figure 4.26. Is the graph triangulated?
If not, state a single edge which, if added, would triangulate the graph.
a2
a1
a4
a3
a5
a6
Figure 4.26
Graph for Exercises 2 and 3.
3. Consider the graph in Figure 4.26. Determine the cliques and construct a junction
tree for the graph.
4. Consider the graph in Figure 4.27. Is it decomposable? Justify your answer.
5. Consider the Bayesian network shown in Figure 4.28.
(a) Moralize the graph.
(b) By ﬁnding an appropriate elimination sequence, show that the moral graph tri-
angulated.
(c) Find a junction tree.
a1
a2
a4
a3
a5
a6
Figure 4.27
Graph for Exercise 4.

146
EXERCISES: DECOMPOSABLE GRAPHS AND CHAIN GRAPHS
X4
X5
X7
X9
X8
X1
X2
X3
X6
X13
X10
X11
X12
Figure 4.28
Bayesian network for Exercise 5.
6. A directed acyclic graph is singly connected if the graph obtained by dropping the
directions of the links is a tree. For example, the Bayesian network in Figure 4.28
is singly connected.
(a) Prove that the moral graph of a singly connected graph is triangulated.
(b) Prove that the separators in a junction tree for a singly connected graph consist
of exactly one node.
7. Let G = (V, E) be a triangulated graph, let A ⊂V and let GA = (A, EA), where
EA = E ∩A × A is the graph restricted to A. Prove (formally) that GA is triangulated.
8. Consider the Bayesian network given in Figure 4.29.
(a) Find the moral graph G(m)
(b) Find a triangulation of the moral graph, adding in as few edges as possible.
(c) Find a junction tree for the triangulated graph in (b).
a1
a2
a4
a6
a3
a5
Figure 4.29
Graph for Exercise 8.
9. Prove that Deﬁnition 4.30 is equivalent to Deﬁnition 2.28 if G = (V, E) is a directed
acyclic graph.

DECOMPOSABLE GRAPHS AND CHAIN GRAPHS
147
10. Let G = (V, E) be a directed acyclic graph. Prove that if a probability distribution
over the variables V is Globally G-Markovian (Deﬁnition 4.32), then p and G are
faithful (Deﬁnition 2.32).
11. Let T be a junction tree of cliques constructed from an undirected triangulated graph
G = (V, E). Let α be a node in G. Show that if all nodes not containing α are
removed from T, then the remaining tree is connected.


5
Learning the conditional
probability potentials
In most applications of artiﬁcial learning, a machine is expected, using a database of
instantiations, to ‘learn’ (in other words estimate) both the structure of the DAG (in
other words, which directed edges should be used and which should be omitted) and,
once the structure of the DAG has been established, the conditional probability potentials
(CPPs). Let V = {X1, . . . , Xd} then, provided the graph structure has been established,
the probability potentials are {(pXv|v)d
v=1} where v (as usual) denotes the parent set
of variable Xv.
Chapter 5 considers the second of these problems; how to learn the CPPs given a DAG
structure, while Chapter 6 considers the problem of learning a suitable DAG structure
from a set of instantiations.
In the analysis given below, it is assumed that the prior distribution over the condi-
tional probability potentials may be expressed as a product of Dirichlet distributions. The
analysis in [90] illustrates that under rather broad modelling assumptions, the Dirichlet
prior is inevitable.
5.1
Initial illustration: maximum likelihood estimate
for a fork connection
Consider a network with three variables, U = (X, Y, Z), where the distribution factorizes
as pU = pX|ZpY|ZpZ. Suppose that the variables X, Y and Z are binary, and sup-
pose there are four complete and independent instantiations of U; namely, (u(i))i=1,2,3,4,
Bayesian Networks: An Introduction
T. Koski, J. Noble
2009 John Wiley & Sons, Ltd

150
LEARNING THE CONDITIONAL PROBABILITY POTENTIALS
given by
x
y
z
u(1)
0
1
0
u(2)
1
1
0
u(3)
0
0
1
u(4)
0
1
0
Deﬁnition 5.1 (Complete Instantiation) An instantiation of a set of variables U =
(X1, . . . , Xd) is said to be complete if there are no missing values.
The probability of the four instantiations listed above from independent observations is
pU(u(1))pU(u(2))pU(u(3))pU(u(4)),
where
pU(u(i)) = pX|Z(x(i)|z(i))pY|Z(y(i)|z(i))pZ(z(i)).
The probabilities of success, conditioned on the state of the parents, are given by
θx|0 = pX|Z(1|0)
θx|1 = pX|Z(1|1),
θy|0 = pY|Z(1|0)
θy|1 = pY|Z(1|1),
θz = pZ(1).
The CPPs are fully speciﬁed by these ﬁve parameters in this example.
Expressed in these parameters, the probabilities of getting the four instantiations
u(1), u(2), u(3), u(4) are:
pU(u(1)) = (1 −θx|0)θy|0(1 −θz)
pU(u(2)) = θx|0θy|0(1 −θz)
pU(u(3)) = (1 −θx|1)(1 −θy|1)θz
pU(u(4)) = (1 −θx|0)(1 −θy|0)(1 −θz).
This gives
pU(u(1))pU(u(2))pU(u(3))pU(u(4)) = θx|0(1 −θx|0)2(1 −θx|1)θ3
y|0(1 −θy|1)θz(1 −θz)3.
This is to be maximized as a function of the ﬁve parameters in the expression. The
maximization splits into the maximization of ﬁve separate likelihoods, which have already
been considered in the thumb-tack example in Section (1.8). These are
L(θx|0) = θx|0(1 −θx|0)2,
L(θx|1) = 1 −θx|1,
L(θy|0) = θ3
y|0,
L(θy|1) = 1 −θy|1,
L(θz) = (1 −θz)3θz.

THE MAXIMUM LIKELIHOOD ESTIMATOR FOR MULTINOMIAL SAMPLING
151
Maximizing these gives
ˆθx|0 = 1
3, ˆθx|1 = 0, ˆθy|0 = 1, ˆθy|1 = 0, ˆθz = 1
4.
The following terminology is used: (X, Z) is called a family (child, parent). An instan-
tiation (x, z) is a family conﬁguration. z is called a parent conﬁguration. Note, in the
above example, that z = 0 appears three times and (x, z) = (1, 0) appears once. Note
that
ˆθx|z=0,MLE = 1
3 = frequency of family conﬁguration (x, z) = (1, 0)
frequency of parent conﬁguration z = 0
.
5.2
The maximum likelihood estimator for multinomial
sampling
Consider a DAG over a set of variables V = {X1, . . . , Xd}, where j denotes the set of
parent variables of Xj. Suppose that for each j = 1, . . . , d, the variables Xj takes values
in the set Xj := {x(1)
j , . . . , x
(kj )
j
}, and that the parent conﬁgurations j take values in
the set {π(q1)
j
, . . . , π
(qj )
j
} for j = 1, . . . , d, for j such that j is non-empty. Let
θijl := p({Xj = x(i)
j }|{j = π(l)
j }).
In Section 5.3, it is shown that in any DAG with a set of complete instantiations, for all
(i, j, l), the maximum likelihood estimate of θijl will be of the form
ˆθMLE;ijl =
frequency of the family conﬁguration (x(i)
j , π(l)
j )
frequency of the parent conﬁguration π(l)
j
.
To prepare the way, Section 5.2 considers multinomial sampling. Let X1, . . . , Xn be
independent, identically distributed random variables, each with the same distribution as
X, where X takes values in a set X = {x(1), . . . , x(k)}. (Consider, for example, an urn
containing 25 balls of which six are red, six are blue and 13 are green. The random
variable X denotes the resulting colour if one pulls out a ball at random, notes its colour
and then puts it back. Let x(1) denote a red selection, x(2) a blue selection and x(3) a
green selection, so here k = 3). Let
θi = pX(x(i)),
so that θ1 + · · · + θk = 1. Set
θ = (θ1, . . . , θk).
Let X = (X1, . . . , Xn), and let x = (x(i1), . . . , x(in)) denote the sequence drawn in n
independent trials that are conditionally independent, given θ. Let
nl = number of times x(l) appears in x,
l = 1, . . . , k
so that n = n1 + · · · + nk, then
pX(x|θ) = θn1
1 . . . , θnk
k .

152
LEARNING THE CONDITIONAL PROBABILITY POTENTIALS
Maximum likelihood
Let
˜k =


(θ1, θ2, . . . , θk) | θj ≥0, j = 1, . . . , k,
k

j=1
θj = 1



denote the parameter space.
Deﬁnition 5.2 (Likelihood function, Likelihood Estimate, Log Likelihood Function)
The likelihood function of the parameters θ is deﬁned as
L(θ|x) = pX(x|θ).
The maximum likelihood estimate(θME is deﬁned as the value of θ that maximizes L(θ|x).
The log likelihood function l is deﬁned as:
l (θ1, θ2, . . . , θk) = log pX

x | θ

,
where log is used to denote the natural logarithm.
The computation of (θME is a maximization problem with constraints. Equivalently, the
maximum of l (θ1, θ2, . . . , θk) may be computed, subject to the constraint θ1 + θ2 + · · · +
θk = 1. This is best achieved by introducing the auxiliary function in the k −1 free
variables
)l (θ1, θ2, . . . , θk−1) = l (θ1, θ2, . . . , 1 −(θ1 + θ2 + · · · + θk−1)) .
This gives
)l(θ1, θ2, . . . , θk−1) = n1 · log θ1 + n2 · log θ2 + · · · + nk · log(1 −(θ1 + θ2 + · · · + θk−1)).
The partial derivatives of )l (θ1, θ2, . . . , θk−1) are taken with respect to each of θ1,
θ2, . . . , θk−1 and the critical points occur at the values for which the partial derivatives
are all equal to zero. This gives the system of equations
∂
∂θ1
)l (θ1, θ2, . . . , θk−1) = n1
θ1
−
nk
1 −(θ1 + θ2 + . . . + θk−1) = 0,
...
∂
∂θk−1
)l (θ1, θ2, . . . , θk−1) = nk−1
θk−1
−
nk
1 −(θ1 + θ2 + . . . + θk−1) = 0.
These equations are equivalent to the equalities
n1
θ1
= n2
θ2
= . . . =
nk
1 −(θ1 + θ2 + · · · + θk−1).
To simplify notation, the common value of these ratios is denoted as λ. This gives
θ1 = n1
λ ,
θ2 = n2
λ , . . . , θk = nk
λ .

THE MAXIMUM LIKELIHOOD ESTIMATOR FOR MULTINOMIAL SAMPLING
153
The constraint θ1 + θ2 + · · · + θk = 1 is now employed, giving
1 = θ1 + θ2 + · · · + θk = n1
λ + n2
λ + · · · + nk
λ ,
and
λ = n1 + n2 + · · · + nk = n.
This gives the solution
(θi = ni
n ,
i = 1, . . . , k.
It remains to show that this critical point yields a maximum. This could be achieved by
checking the matrix of second order partial derivatives of )l. There is a simpler way to
prove that the estimate found above maximizes the likelihood function, which requires
the following two deﬁnitions and a lemma.
Deﬁnition 5.3 (Shannon Entropy) The Shannon entropy, or entropy of a probability dis-
tribution θ = (θ1, . . . , θk), where θj ≥0, j = 1, . . . , k and θ1 + · · · + θk = 1 is deﬁned as
H(θ) = −
k

j=1
θj log θj.
Natural logarithms are used. In the deﬁnition of H(θ), the deﬁnition 0 log 0 = 0 is used,
obtained by continuous extension of the function x log x, x > 0.
Note that H(θ) ≥0.
Deﬁnition 5.4 (Kullback-Leibler Divergence) The Kullback-Leibler divergence between
two discrete probability distributions f and g with the same state space X is deﬁned as
D(f |g) =

x∈X
f ({x}) log f ({x})
g({x}) .
If g({x}) = 0 for f ({x}) ̸= 0, then f ({x}) · log 0 = +∞. If D(f |g) = +∞, then there
is at least one outcome x such that f and g may be distinguished without error.
Lemma 5.1 For any two discrete probability distributions f and g, it holds that
D(f |g) ≥0.
Proof of Lemma 5.1 The proof uses Jensen’s inequality,1 namely, that for any
convex function φ, E[φ(X)] ≥φ(E[X]). Note that f ({x}) ≥0 for all x ∈X and
that 
x∈X f ({x}) = 
x∈X g({x}) = 1. Using this, together with the fact that −log is
convex, yields
D(f |g) = −

x∈X
f ({x}) log
 g({x})
f ({x})

≥−log

x∈X
f ({x}) g({x})
f ({x})

= −log 1 = 0.
□
1 J.L. Jensen (1859–1925) published this in Acta Mathematica 1906 30(1).

154
LEARNING THE CONDITIONAL PROBABILITY POTENTIALS
It is now proved that the candidate estimate given above maximizes the likelihood
function.
Proposition 5.1 The maximum likelihood estimate (θML of θ is
(θML =
n1
n , n2
n , . . . , nk
n

.
Proof of Proposition 5.1 The candidate solution (θML belongs to ˜k and is therefore
feasible. Since pX

x(n) | θ

= 	k
i=1 θni
i , the following identity follows directly:
H

(θML

= −1
n log pX

x | (θML

,
(5.1)
where
H 
(θML
 = −
k

i=1
(θi log(θi
(5.2)
is the Shannon entropy (for an empirical distribution) in natural logarithm, given in
Deﬁnition 5.3.
For arbitrary θ ∈˜k, the following identity follows directly from Equation (5.2):
pX

x | θ =
k
i=1
θni
i
= exp
 k

i=1
ni log θi
*
= exp

n
k

i=1
ˆθi log θi
*
(5.3)
= exp

n
k

i=1
ˆθi log ˆθi −n
k

i=1
ˆθi log
ˆθi
θi
*
= exp
#
−n

H

(θML

+ D

(θML|θ
$
.
(5.4)
Thus, from Lemma 5.1, together with the fact that D(f |f ) = 0 for any admissible f , it
follows that
L(θ) := pX

x | θ = exp #−n 
H 
(θML
 + D 
(θML|θ$ ≤exp #−nH 
(θML
$
= pX

x | (θML
 = L((θML)
for every θ ∈˜k and Proposition 5.1 is proved.
□
Mean posterior estimate
The approach that has just been presented is the maximum
likelihood method. An estimator derived from the Bayesian posterior distribution is now
considered. First, a prior distribution is put over the parameter space. The Dirichlet
distribution is useful here, because the integral can be computed explicitly to give a

MLE FOR THE PARAMETERS IN A DAG: THE GENERAL SETTING
155
posterior density which is again a Dirichlet distribution. The deﬁnition of the Dirichlet
distribution was given in Deﬁnition 1.13.
The maximum posterior estimate, the value of the parameter value which gives the
maximum value for the posterior distribution, has already been discussed. The mean
posterior estimate is the expected value of the posterior distribution. Here,
ˆθi,MEP =

θiπ(θ1, . . . , θk|x, α)dθ1 . . . dθk =
ni + αi
k
j=1 nj + k
j=1 αj
.
This computation is left as an exercise.
Remarks
The following remarks are applicable to learning CPPs for any DAG G =
(V, E) where V = {X1, . . . , Xd} and the distribution of p(Xi|i) (i,e, of Xi conditioned
on the parent set) is multinomial for each i = 1, . . . , d.
1. The aim of statistics is to predict future outcomes based on past information.
The parameters are only a tool to help this. In a fully Bayesian approach to
statistics, the only estimate of the parameter, given data x, is the entire posterior
distribution π|X(.|x), which is then used to compute the predictive distribution.
But this approach, although intellectually satisfying, is not always practical, and
with Bayesian networks it is often useful to have a point estimator. There are
some situations where the MEP turns out to be more useful than the MLE. One
example may be the situation of learning using information from a very large data
warehouse. Even though the warehouse is large, it may happen that the size of
the entire set of possible cases is very much larger (i.e. data is sparse). In other
words, it is suspected that there may be positive cases, even if the data warehouse
does not contain any examples. In such a situation, the sample yields ni = 0. This
would yield ˆθi,MLE = 0, but if the a priori supposition that there may exist positive
cases is modelled into the prior, then ˆθi,MEP > 0. The strict positivity of the MEP
sometimes turns out to be a valuable property in the mining of very large data
sets, which are usually sparse. This makes it preferable to the MLE.
2. Comparing with ˆθi,MLE = ni
n , note that
lim
n→+∞
ˆθiMEP
ˆθiMLE
= 1.
This is an asymptotic result and in the situation envisaged above, n may not be
sufﬁciently large for the ratio to be close to 1.
5.3
MLE for the parameters in a DAG: the general setting
Notations
Firstly, the necessary notation is developed. V = {X1, . . . , Xd} denotes the
collection of random variables under consideration and X denotes the set of all possible
outcomes for the experiment. ˜ denotes the parameter space and  = ˜ × X denotes the
context of the experiment. G = (V, E) denotes the directed acyclic graph along which
the probability function pX1,...,Xd is factorized.

156
LEARNING THE CONDITIONAL PROBABILITY POTENTIALS
In the situation considered here, the sample space for variable Xj is denoted
Xj = {x(1)
j , . . . , x
(kj )
j
},
so that
X = X1 × · · · × Xd.
That is,
X = {(x(j1)
1
, . . . , x(jd)
d
) |j1 ∈(1, . . . , k1), . . . , jd ∈(1, . . . , kd)}.
A complete instantiation is an outcome x = (x(i1)
1
, . . . , x(id)
d
). Note that here the vectors
are taken as rows. This is because in a database of instantiation, it is usual that each
column represents a variable and each row represents an instantiation. A sample of cases
(also called records in the data mining literature) is given by a matrix
x =


x(1)
...
x(n)

,
where x(i) = (x(j1)
i,1 , . . . , x(jd)
i,d ) denotes the ith instantiation. Usually, when data is stored,
each column represents the outcomes for one particular variable. The CPPs of a Bayesian
network are to be estimated on the basis of this data.
For each variable Xj, consider all possible instantiations of the parent set j and
label them (π(l)
j )
qj
l=1. That is, π(l)
j
denotes that the parent conﬁguration of variable Xj is
in state π(l)
j
and there are qj possible conﬁgurations of j.
Example 5.1
For the DAG shown in Figure 5.1, where A, B, C are all binary variables,
(B) = (A, C) has four possible sets of instantiations:
π(1)
B
= (0, 0), π(2)
B
= (0, 1), π(3)
B
= (1, 0), π(4)
B
= (1, 1).
Set
nk(x(i)
j |π(l)
j ) =

1
(x(i)
j , π(l)
j ) is found in x(k)
0
otherwise,
where (x(i)
j , π(l)
j ) is a conﬁguration of the family (Xj, j). Let θ denote the set of
parameters deﬁned as
θjil = p({Xj = x(i)
j }|{j = π(l)
j }).
A
B
C
Figure 5.1
A collider.

MLE FOR THE PARAMETERS IN A DAG: THE GENERAL SETTING
157
for j = 1, . . . , d, i = 1, . . . , kj, l = 1, . . . , qj, with given graph structure G = (V, E).
Then the joint probability of a case x(k) occurring may be written as
pX|(x(k)|θ, E) =
d
j=1
qj

l=1
kj

i=1
θ
nk(x(i)
j |π(l)
j )
jil
.
The following is a summary of the notation:
• d is the number of variables ( = the number of nodes).
• kj is the number of possible states that the variable Xj can take.
• qj is the number of possible parent conﬁgurations for variable Xj.
• θjil := p({Xj = x(i)
j }|{j = π(l)
j }).
(5.5)
That is, θjil is the conditional probability that variable Xj is in state i, given
that the parent conﬁguration is conﬁguration l. The notation π(l)
j
denotes that the
parents of variable j are in state l.
Let X =


X(1)
...
X(n)

. The cases x =


x(1)
...
x(n)

are considered to be independent obser-
vations, giving
pX|(x|θ, G)
=
n

k=1
pX|(x(k)|θ, G) =
d
j=1
qj

l=1
kj

i=1
n

k=1
θ
nk(x(i)
j |π(l)
j )
jil
=
d
j=1
qj

l=1
kj

i=1
θ
n
k=1 nk(x(i)
j |π(l)
j )
jil
.
Set
n(x(i)
j |π(l)
j ) =
n

k=1
nk(x(i)
j |π(l)
j ).
This is simply the number of times the conﬁguration (x(i)
j , π(l)
j ) appears in x =


x(1)
...
x(n)

.
The likelihood is therefore
L(θ) =
n

k=1
pX|(x(k)|θ, G) =
d
j=1
qj

l=1
kj

i=1
θ
n(x(i)
j |π(l)
j )
jil
.
The likelihood factorizes into local parent child factors and additionally to d × qj separate
maximum likelihood estimations all of the basic form treated in the preceding section. It

158
LEARNING THE CONDITIONAL PROBABILITY POTENTIALS
follows that
ˆθMLE,jil =
n(x(i)
j |π(l)
j )
n(π(l)
j )
,
where
n(π(l)
j ) =
kj

i=1
n(x(i)
j |π(l)
j )
is the frequency of conﬁguration π(l)
j
in X. The maximum likelihood estimate of θjil is
therefore as advertized:
ˆθjil = frequency of the family conﬁguration
frequency of the parent conﬁguration .
Posterior distribution of the CPPs
The approach of computing the maximum like-
lihood estimator is now contrasted with the Bayesian approach. For convenience of
writing,
θj.l =

θj1l, . . . , θjkj l

is used to denote the probability distribution over the states of Xj, given that π(l)
j
is the
parent conﬁguration. The prior distribution over θj.l is taken to be
Dir(αj1l, . . . , αjkj l).
A standard computation using the Dirichlet integral yields that the posterior density is
the Dirichlet density
θj.l|(x(1), . . . , x(n)) ∼Dir(n(x(1)
j |π(1)
j ) + αj1l, . . . , n(x
(kj )
j
|π(1)
j ) + αjkj l).
The tables of counts of family conﬁgurations at node j, e.g.
n(x(1)
j |π(l)
j ), . . . , n(x
(kj )
j
|π(l)
j )
is stored as a memory of past experience. The posterior distribution of θj.l depends only
on counts of family conﬁgurations at node j and not on conﬁgurations at any other node.
Therefore it follows from the discussion in Section 3.5.1 that

n(x(1)
j |π(l)
j ), . . . , n(x
(kj )
j
|π(l)
j )
qj
l=1
is a predictive sufﬁcient statistic for (θj1l, . . . , θjkj l).

MLE FOR THE PARAMETERS IN A DAG: THE GENERAL SETTING
159
Predictive distribution
The predictive distribution of a new case x(n+1) may be com-
puted using the posterior density. The basic idea was given in the thumb-tack modelling
of Section 1.8. In the presence of a set of instantiations x, θjil, deﬁned in Equation (5.5),
will be estimated by:
˜θjil = p({Xn+1,j = x(i)
j }|{j = π(l)
j }, {X = x}).
(5.6)
This is the predictive conditional probability that variable Xj attains value x(i)
j , given
the parent conﬁguration π(l)
j
and the cases stored in x. Then, by computations as before,
with ˜θjil deﬁned by Equation (5.6),
˜θjil = p({Xn+1,j = x(i)
j }|{jπ(l)
j }, {X = x})
=

Sjl
p({Xn+1,j = x(i)
j }|{ = θ})π(θ|{j = π(l)
j }, {X = x})dθj.l
=

Sjl
θjilπj.l|X(θj.l|x; αj.l)dθj.l
=

Sjl
θjil
(n(π(l)
j ) + αj.l)
	kj
m=1 (n(x(m)
j
|π(l)
j ) + αjml)
kj

i=1
θ
n(x(i)
j |π(l)
j )+αjil
jil
dθj.l
=
(n(π(l)
j ) + αj.l)
	kj
m=1 (n(x(m)
j
|π(l)
j ) + αjml)

Sjl

m̸=l
θ
n(x(m)
j
|π(l)
j )
jml
θ
n(x(i)
j |π(l)
j )+1
jil
dθj.l
=
(n(π(l)
j ) + αj.l)
	kj
m=1 (n(x(m)
j
|π(l)
j ) + αjml)
×
(n(x(i)
j |π(l)
j ) + αjil + 1) 	
m̸=i (n(x(i)
j |π(l)
j ) + αjil)
(n(π(l)
j ) + αj.l + 1)
=
n(x(i)
j |π(l)
j ) + αjil
n(π(l)
j ) + kj
i=1 αjil
,
where Sjl is deﬁned as
Sjl =


(θjil)
kj
i=1|θjil ≥0, i = 1, . . . , kj,
kj

i=1
θjil = 1


.

160
LEARNING THE CONDITIONAL PROBABILITY POTENTIALS
5.4
Updating, missing data, fractional updating
Updating
Suppose the cases x(1), . . . , x(n) are complete. Suppose next that x(i)
j
and
π(l)
j
are observed in x(n+1). Then, by Bayes’ rule,
θj.l|(x(1), . . . , x(n), (x(i)
n+1,j, π(l)
n+1,j))
∼Dir(n∗(x(1)
j |π(l)
j ) + αj1l, . . . , n∗(x
(kj )
j
|π(l)
j ) + αjkj l),
where
n∗(x(r)
j |π(l)
j ) =

n(x(r)
j |π(l)
j )
r ̸= i
n∗(x(i)
j |π(l)
j ) = n(x(i)
j |π(l)
j ) + 1
r = i.
The virtual sample size is updated as
s∗= n(π(l)
j ) + 1 +
kj

i=1
αjil.
A missing instantiation
Suppose the instantiation at node j is missing in the new case;
the parent conﬁguration π(l)
j
is present. Set
e∗= (x(1), . . . , x(n), x(n+1)).
The distribution of the random variable θj.l|e∗is expressed as the mixture of distributions
kj

i=1
wiDir(n(x(i)
j |π(l)
j ) + αj1l, . . . , n(x(i)
j |π(l)
j ) + 1 + αjil, . . . , n(x
(kj )
j
|π(l)
j ) + αjkj l),
where wi = pX({(Xj, j) = (x(i)
j , π(l)
j )}|e∗).
Updating: parent conﬁguration and the state at node j are missing
Consider a new
case x(n+1) where both the state and the parent conﬁguration of node j are missing. Then
the distribution of θj.l|e∗is given as the mixture of distributions
kj

i=1
viDir(n(x(1)
j |π(l)
j ) + αj1l, . . . , n(x(i)
j |π(l)
j ) + 1 + αjil, . . . , n(x
(kj )
j
|π(l)
j ) + αjkj l)
+ Dir(n(x(1)
j |π(l)
j ) + αj1l, . . . , n(x
(kj )
j
|π(l)
j ) + αjkj l)v∗,
where
vi = p({(Xj, j) = (x(i)
j , π(l)
j )}|e∗),
i = 1, . . . , kj
and
v∗= 1 −p({j = π(l)
j }|e∗).

UPDATING, MISSING DATA, FRACTIONAL UPDATING
161
Fractional updating
The preceding shows that adding new cases with missing values
results in dealing with increasingly messy mixtures, with increasing numbers of compo-
nents. One, perhaps naive, way of approximating this is to use only one Dirichlet density,
with the parameters updated by
n∗(x(i)
j |π(l)
j ) = n(x(i)
j |π(l)
j ) + p({(Xj, j) = (x(i)
j , π(l)
j )}|e∗),
i = 1, . . . , kj.
This is known as fractional updating.
Fading
If the parameters change with time, then information learnt a long time ago
may not be so useful. A way to make the old cases less relevant is to have the sample
size discounted by a fading factor qF , a positive number less than one.
The fading update is
n →qF n(x(j)
r |π(j)
l
) = n∗(x(r)
j |π(l)
j ),
r ̸= i
and
n →qF n(x(i)
j |π(l)
j ) + 1 = n∗(x(i)
j |π(l)
j ).
The virtual sample size is updated as
sn−1 →qF

n(π(l)
j ) +
kj

i=1
αjil

+ 1 = sn.
The above may be written as
sn = qFsn−1 + 1, s0 = s.
Iteration gives
sn = qn
F s +
n

i=0
qi
F = qn
Fs + 1 −qn+1
F
1 −qF
.
The limiting effective maximal sample size is therefore
s∗=
1
1 −qF
.
Notes
Chapter 5 on learning of parameters (or estimation of parameters) is largely based
on ‘A Tutorial on Learning with Bayesian Networks’ by D. Heckerman [25] and also
[72]. Learning from incomplete data (Section 5.4) is discussed in [91]. Another treatment
of learning is found in R.E. Neapolitan [34].

162
EXERCISES: LEARNING THE CONDITIONAL PROBABILITY POTENTIALS
5.5
Exercises: Learning the conditional probability
potentials
1. Kullback-Leibler divergence Let X = {x1, · · · , xL} denote a space with L elements
and let
f := (f (x1), · · · , f (xL))
and
g := (g(x1), · · · , g(xL))
be two probability distributions deﬁned on X. Recall that the Kullback-Leibler diver-
gence between f and g is deﬁned by
D (f | g) =
L

i=1
f (xi) log f (xi)
g(xi) ,
(5.7)
with the conventions 0 · log
0
g(xi) = 0 and f (xi) log f (xi)
0
= ∞. The logarithm is the
natural logarithm unless otherwise stated.
Let X = {0, 1} and 0 ≤p ≤1 and 0 ≤g ≤1. Let f = (1 −p, p) and g = (1 −
g, g) be the two Bernoulli distributions Be(p) and Be(g), respectively. Find the
Kullback-Leibler divergence between them.
2. Jensen’s inequality Let φ(x) be a convex function and X ﬁnite discrete real valued
random variable, deﬁned on a ﬁnite space X. Prove, by induction, that
E [φ (X)] ≥φ (E [X]) .
3. Calibration of Kullback-Leibler divergence Let D (f | g) = k be the value of
the Kullback distance between any two probability distributions deﬁned on X =
{x1, . . . , xn}. Let h(x) solve
D (Be (1/2) | Be (h(x))) = x.
(5.8)
The function h is known as the calibration. The Kullback distance between f and g is
the same as between a fair Bernoulli distribution Be (1/2) and a Bernoulli distribution
Be (h(k)). Prove that
h(k) = 1
2

1 ±
+
1 −e−2k

.
(5.9)
4. Let
π(θ) =

j,l
πj.l(θj.l)
denote the prior distribution over a Bayesian network, where
πj.l = Dir(αj1l, . . . , αjkj l).

LEARNING THE CONDITIONAL PROBABILITY POTENTIALS
163
Let X =


X(1)
...
X(n)

denote the random matrix corresponding to n independent instan-
tiations and let Xn+1 denote a random vector corresponding to an instantiation,
independent of X.
Prove that
p

{Xn+1,j = xi
j}|{j = πl
j}, {X = x}

=

Sj.l
θjilπ| X

θj.l|x; αj,l

dθj.l
=
n(xi
j|πl
j) + αijl
n(πl
j) + kj
i=1 αijl
.
5. This exercise is taken from [92]. The idea of using Kullback-Leibler in this way
for a database is due to Jensen. Suppose one has a database C with n cases of
conﬁgurations over a collection of variables V . Let Sp(V ) denote the set of possible
conﬁgurations over V and let #(v) denote the number of cases of conﬁguration v.
Deﬁne P C(v) = #(v)
n . Let P M denote a probability distribution over Sp(V ). Assume
that P C(v) = 0 if and only if P M(v) = 0 and discount these conﬁgurations. Deﬁne
SM(C) = −
c∈C log P M(c).
Let dK denote the Kullback-Leibler distance (namely, dK(f|g) = 
i fi log fi
gi ). Show
that
SM(C) −SC(C) = ndK(P C|P M).
6. Suppose that p(A, B, C, D) factorizes along the DAG in Figure 5.2, where A, B, C
and D are each binary variables, taking the values 1 or 0. Suppose there are 10
instantiations:
U (1)
U (2)
U (3)
U (4)
U (5)
U (6)
U (7)
U (8)
U (9)
U (10)
A
1
1
0
1
0
0
0
0
1
1
B
1
1
0
0
1
0
1
1
1
1
C
0
0
1
0
0
1
0
0
1
0
D
0
1
1
0
0
1
1
0
1
1
C
A
D
B
Figure 5.2
DAG for A, B, C, D.

164
EXERCISES: LEARNING THE CONDITIONAL PROBABILITY POTENTIALS
(a) Find the maximum likelihood estimates for θB := pB|A(1|0) and θD :=
pD|B,C(1|1, 0).
(b) Suppose that θD has prior distribution Be( 1
2, 1) (Beta distribution with parameters
( 1
2, 1)). That is,
π(θD) =

(1.5)
(1)(0.5)θ−0.5
D
0 ≤θD ≤1
0
θD ̸∈[0, 1]
Show that the posterior density π(θD|U) is given by
π(θD|U) =

(6.5).(5.5).(4.5).(3.5)
3!
θ2.5
D (1 −θD)3
0 ≤θD ≤1
0
θD ̸∈[0, 1].
7. Consider the following model for the two variables A and B, where both A and B
are binary variables, each taking values y/n (yes or no).
Let θa = pA(y), θb|y = pB|A(y|y) and θb|n = pB|A(y|n). Suppose the parameters
have prior distributions
π(θa) = 11!
7!3!θ7
a(1 −θa)3
π(θb|y) = 9!
6!2!θ6
b|y(1 −θb|y)2
π(θb|n) = 6!
2!3!θ2
b|n(1 −θb|n)3
Now suppose that a sequence of 20 instantiations is observations, with results
A
y
y
y
y
n
n
n
y
y
n
y
y
y
y
y
n
n
y
y
y
B
n
y
n
y
n
n
y
y
n
n
n
y
y
y
y
y
n
n
y
y
(a) Find the posterior distributions over the parameters where the updating is carried
out without fading.
(b) Find the posterior distributions over the parameters for the same sequence, using
a fading factor of 0.9.
8. Consider a Bayesian network over two binary variables A and B, where the directed
acyclic graph is given in Figure 5.3 and A and B each take the values 0 or 1.
A
B
Figure 5.3
Directed acyclic graph on two variables.

LEARNING THE CONDITIONAL PROBABILITY POTENTIALS
165
Let θa = pA(1), θb|y = pB|A(1|1), pB|A(1|0). Let the prior distributions over the
parameters be
πa(θ) =

3θ2
0 ≤θa ≤1
0
θ ̸∈[0, 1],
πb|y(θ) =

12θ2(1 −θ)
0 ≤θ ≤1
0
θ ̸∈[0, 1],
πb|n(θ) =

12θ(1 −θ)2
0 ≤θ ≤1
0
θ ̸∈[0, 1],
Suppose that there is a single instantiation, where B = 1 is observed, but A is
unknown. Perform the approximate updating.
9. Consider a Bayesian network, with variables X1, X2, X3, X4, X5, where all the vari-
ables are binary; variable Xj has state space Xj = {0, 1} for j = 1, 2, 3, 4, 5. Suppose
the conditional probability tables are
pX1(1) = 0.4
pX2|X1 =
X2\X1
1
0
1
0.3
0.8
0
0.7
0.2
pX3|X1 =
X3\X1
1
0
1
0.7
0.4
0
0.3
0.6
pX4|X2 =
X4\X2
y
n
y
0.5
0.1
n
0.5
0.9
pX5|X3,X4(1|., .) =
X4\X3
1
0
1
0.9
0.999
0
0.999
0.999
Suppose 100 trials are made on a network which is thought to have these probability
tables. The numbers of times the corresponding conﬁgurations occurred is given
below. Calculate the marginals from the observed probabilities below and compare
with the ‘theoretical’ probabilities.
X1X2\X3X4X5
111
110
101
100
011
010
001
000
11
4
0
5
0
1
0
2
0
10
2
0
16
0
1
0
8
0
01
9
1
10
0
14
0
16
0
00
0
0
4
0
0
0
7
0

166
EXERCISES: LEARNING THE CONDITIONAL PROBABILITY POTENTIALS
10. Let the likelihood for θ = (θ1, . . . , θL) with data x be given by
L

θ; x

=
L

j=1
θ
nj
j ,
where nj is the number of times the symbol xj (in a ﬁnite alphabet with L symbols)
is present in x and L
j=1 θj = 1. For the prior distribution over θ, a ﬁnite Dirichlet
mixture is taken, given by
π (θ) =
k

i=1
λiDir

α(i)q(i)
1 , . . . , α(i)q(i)
L

,
where λi ≥0, k
i=1 λi = 1 (the mixture distribution), α(i) > 0, q(i)
j > 0, L
i=1 q(i)
j
=
1 for every i. Compute the mean posterior estimate (θj;MP for j = 1, . . . , L.

6
Learning the graph structure
As stated in the introduction to Chapter 5, there are two basic learning problems in
Bayesian networks: learning the structure of the Bayesian network from the data; and,
when the structure is established, learning the conditional probability potentials. Chapter 5
considered the second of these problems (under the assumption that the graph structure
was given); Chapter 6 considers the ﬁrst.
In real life applications, expert prior knowledge may often be incorporated when
learning the structure. For example, in many domains of study there is a huge journal
literature available in electronic form and an automated search of the literature may
be carried out to derive text-based priors for Bayesian network structures, as described
in [93].
This chapter restricts attention to methods of learning structures of networks known
as ‘tabula rasa’, or ‘blank slate’ methods. This means that only data samples are available
for estimating the structure. These methods fall generally into two categories: method
that employ a score function and methods that are constraint based. After a cursory
glance at possible prior distributions, the chapter considers a score function, the the data
likelihood for the possible graph structures. The straightforward approach of maximizing
the likelihood leads to a problem is, in general, not computationally feasible. There are
two reasons for this: ﬁrstly, the number of possible DAGs grows super-exponentially
in the number of nodes. Secondly, there are equivalence classes of network structures
where, without additional information, each DAG within the equivalence class represents
the data equally well. To tackle the ﬁrst problem, the Chow-Liu tree may be used, where
an effective solution for learning the best structure from a restricted class of DAGs is
found by Kruskal’s algorithm. The K2 algorithm presents another approach. The study
then moves on to constraint based methods, which are based on the notions of faithful
distributions, and testing for conditional independence.
Bayesian Networks: An Introduction
T. Koski, J. Noble
2009 John Wiley & Sons, Ltd

168
LEARNING THE GRAPH STRUCTURE
6.1
Assigning a probability distribution
to the graph structure
For a Bayesian network with a directed acyclic graph G = (V, E), the edge set E is often
referred to as the structure of the network. Let ˜E denote the set of all possible edge sets
that give a directed acyclic graph with node set V . In this chapter, it is assumed that E is
unknown and has to be inferred from data, where the individual cases are assumed to be
observations on random vectors that are independent, conditioned on the graph structure
E and the parameters θ. The prior distribution over the parameter vectors θj.l are taken
from the family Dir(αj1l, . . . , αjkj l) for all nodes and parent conﬁgurations (j, l). As
usual, X =


X(1)
...
X(n)

denotes the matrix where each row is an independent copy of X
and E denotes the structure of the DAG. This has a prior distribution pE, which is the
probability function for a random variable E which takes values in ˜E, the set of possible
graph structures.
The prior distribution for the graph structure
There are several possible ways of con-
structing a prior distribution pE, as discussed in [94]. If it is known a priori that the graph
structure lies within a subset A ⊆˜E, then an obvious choice is the uniform prior over A:
pE(E) =

1
|A|
if E ∈A
0
otherwise
where |A| is the number of elements in a subset A ⊆˜E.
Another simple choice is to assign to each pair of nodes Xi and Xj a probability
distribution with three values such that
p

{

Xi, Xj

∈E}

+ p

{

Xj, Xi

∈E}

+ p

{

Xi, Xj

̸∈E}, {

Xj, Xi

̸∈E}

= 1.
Here again, uniform distribution over the three possibilities may be chosen. Then, for
a given structure E, the prior probability is obtained by multiplying the appropriate
probabilities over all the edges in E and normalizing to give pE(E).
The likelihood for the graph structure
The likelihood for the graph structure, given
data x, is given by
pX|E(x|E) =

pX|,E(x|θ, E)π|E(θ|E)dθ,
where π|E(.|E) denotes the prior distribution over the parameters θ, conditioned on the
graph structure, and using the structure of the prior as the product of Dirichlet distribu-
tions,
pX|E(x|E) =

n

k=1
pX|,E(x(k)|θ, E)
d
j=1
qj

l=1
φ(θj.l, αj.l)dθj.l

ASSIGNING A PROBABILITY DISTRIBUTION TO THE GRAPH STRUCTURE
169
where
φ(θj.l, αj.l)
is
a
compact
way
of
referring
to
the
Dirichlet
density
Dir(αj1l, . . . , αjkj l).
Because pX|,E(x|θ, E) has a convenient product form, computing the Dirichlet inte-
gral is straightforward and gives
pX|E(x|E) =
d
j=1
qj

l=1
(kj
i=1 αjil)
(n(πl
j) + kj
i=1 αjil)
kj

i=1
(n(xi
j|πl
j) + αjil)
(αjil)
.
(6.1)
This is the Cooper-Herskovitz likelihood for the graph structure. It was introduced and
discussed by G.F. Cooper and E. Herskovitz in [95].
The Bayesian selection rule for a graph G = (V, E) uses the graph which maximizes
the posterior probability
pE| X(E|x) = pX|E(x|E)pE(E)
pX(x)
,
(6.2)
where pE is the prior probability over the space of edge sets. The prior odds ratio for
two different edge sets E1 and E2 is deﬁned as pE(E1)
pE(E2) and the posterior odds ratio is
deﬁned as
pE|X(E1|x)
pE|X(E2|x). Equation (6.2) may then be expressed as
Posterior odds = Likelihood ratio × Prior odds.
At a ﬁrst glance, the learning procedure may appear fairly straightforward: there is only
a ﬁnite number of different possible DAGs G = (V, E) with d nodes and hence only
a ﬁnite number of values pE|X(E|x) have to be computed. The edge set E that yields
the maximum value is chosen. Let N(d) denote the number of possible directed acyclic
graphs on d nodes. Then, enumerating all possible graphs (Er)N(d)
r=1 gives
pX(x) =
N(d)

r=1
pX|E(x|Er)pE(Er).
In [96], R.W. Robinson gave the following recursive function for computing the number
N(d) of acyclic directed graphs with d nodes:
N(d) =
d

i=1
(−1)i+1
 d
i

2i(d−1)N(d −i).
(6.3)
This number grows super-exponentially. For d = 5 it is 29 000 and for d = 10 it is
approximately 4.2 × 1018. Here N(d) is a very large number, even for small values of
d. Therefore, it is clearly not feasible to compute this sum, even for modest values of
d. Computing the posterior distribution is an NP-hard problem; G.F. Cooper [82] proves
that the inference problem is NP-hard. That means, worse than an NP-problem. This is
discussed in [74]. Recently, M. Koivisto and K. Sood [97] constructed the ﬁrst algorithm
that had a complexity less than super-exponential for ﬁnding the posterior probability of
a network.

170
LEARNING THE GRAPH STRUCTURE
Aside: P-, NP- and NP-hard problems
A problem is assigned to the NP (non-
deterministic polynomial time) class if it is veriﬁable in polynomial time by a
non-deterministic Turing machine. (A non-deterministic Turing machine is a ‘parallel’
Turing machine which can take many computational paths simultaneously, with the
restriction that the parallel Turing machines cannot communicate.) A P-problem (whose
solution time is bounded by a polynomial) is always also NP. If a problem is known
to be NP, and a solution to the problem is somehow known, then demonstrating the
correctness of the solution can always be reduced to a single P (polynomial time)
veriﬁcation. A problem is NP-hard if an algorithm for solving it can be translated into
one for solving any other NP-problem (non-deterministic polynomial time problem).
NP-hard therefore means ‘at least as hard as any NP-problem’ although it might, in fact,
be harder.
The prediction problem
Given an n × d data matrix x representing n independent
instantiations of a random vector X = (X1, . . . , Xd), the prediction problem is to com-
pute the distribution pX(n+1)|X(x|x), the conditional probability distribution of the next
observation. The article [98] introduces the stochastic complexity distribution.
Firstly, the maximum likelihood estimate E(x) is deﬁned as the value of E that
maximizes
ˆpX|E(x|E) =
d
j=1
qj

l=1
kj

i=1
ˆθ
nk(x(i)
j |π(l)
j )
jil
,
where ˆθjil =
n(x(i)
j ,πl
j )
n(πl
j ) ; n(x(i)
j , πl
j) denotes the number of times that the conﬁguration
(x(i)
j , πl
j) appears in the data and n(πl
j) denotes the number of times that the ‘parent’
conﬁguration πl
j appears.
In principle, the likelihood has to be computed for all possible graph structures and
the graph that maximizes it chosen. As discussed, locating the graph structure that obtains
the maximum is an NP-hard problem.
The stochastic complexity predictive distribution PSC based on n observations is then
deﬁned as
P(n)
SC(x) =
ˆpX|E(x|E(x))

y∈X(n) ˆpX|E(y|E(y)) = 1
Fn
ˆpX|E(x|E(x)).
This motivates the deﬁnition of the stochastic complexity of x with respect to ˜E, where
˜E is the set of edge sets (or models) under consideration. It is deﬁned as
S(x|˜E) = −log P(n)
SC(x) = −log pX|E(x|E(x)) + kn,
where kn = log Fn is a constant. The aim is to predict the next observation xn+1 given the
previous n observations. The stochastic complexity predictive distribution is deﬁned as
P(n+1)
SC
(x|x) = C
d
j=1
qj

l=1
kj

i=1
˜θ
nk(x(i)
j |π(l)
j )
jil
,

MARKOV EQUIVALENCE AND CONSISTENCY
171
where n(x(i)
j , π(l)
j ), n(π(l)
j ) and ˜θjil are computed from the extended data set, the n +
1 × d data matrix
 x
x

.
6.2
Markov equivalence and consistency
Given a set of conditional independence relations, perhaps derived from data, this section
describes how to determine whether or not there is a directed acyclic graph that is faithful
(Deﬁnition 2.30) to the underlying probability distribution; that is, consistent with the
set of independence relations. The method is constructive, in the sense that if there is a
faithful directed acyclic graph, then the method constructs one particular faithful directed
acyclic graph and indicates how to determine all the other DAGs that are faithful to the
set of conditional independence statements. Different graphs that share exactly the same
d-separation properties are said to be Markov equivalent (Deﬁnition 2.32).
Let M denote the complete set of conditional independence statements. That is, let
V denote the set of all subsets of V (including φ and V ) and let
M = {(X, Y, S) ∈V × V × V | X ⊥Y|S,
X, Y ̸∈S}.
Suppose that a set of conditional independence relations M has been obtained empirically
from data. This section addresses two issues: ﬁrstly, whether or not there exists a directed
acyclic graph G that is consistent with M (Deﬁnition 2.31), that is, whether or not there
exists a directed acyclic graph G such that X ⊥Y∥GS ⇔(X, Y, S) ∈M; and secondly,
how to ﬁnd all the directed acyclic graphs that are consistent with a set of independence
statements M.
A set of conditional independence statements M for which there exists such a directed
acyclic graph is said to be DAG isomorphic.
Deﬁnition 6.1 (DAG Isomorphic Conditional Probability Statements) Let V = {X1, . . . ,
Xn} be a collection of random variables and let M denote the entire collection of
conditional independence statements: for all (Xi, Xj, S) ∈V × V × V : Xi, Xj ̸∈S,
(Xi, Xj, S) ∈M ⇔Xi ⊥Xj|S. The collection M is said to be DAG isomorphic if
there exists a DAG G = (V, E) such that G is consistent with M (Deﬁnition 2.31).
The second of these issues is considered ﬁrst; if a set of conditional independence state-
ments M is DAG isomorphic, the following theorem gives a criterion for establishing
whether or not M is consistent with a particular DAG G.
Theorem 6.1 Let V = {X1, . . . , Xn} be a collection of random variables and let M
denote the entire collection of conditional independence statements; for all (Xi, Xj, S) ∈
V × V × V : Xi, Xj ̸∈S, (Xi, Xj, S) ∈M if and only if Xi ⊥Xj∥GS. Assume that M
is DAG isomorphic (Deﬁnition 6.1). A DAG G = (V, E) is consistent with M (Deﬁnition
2.31) if and only if:
1. ∀S ∈V,
Xi ∼Xj ⇔(Xi, Xj, S) ̸∈M.

172
LEARNING THE GRAPH STRUCTURE
2. (Xi, Xj, Xk) forms an immorality in G if and only if (Xi ̸∼Xk, and Xi ⊥Xk|S ⇒
Xj ̸∈S).
If there is a probability distribution p, for which every conditional independence state-
ment in M holds for p and no others, then Theorem 6.1 gives necessary and sufﬁcient
conditions for a graph G to be faithful to p (Deﬁnition 2.30); the terms ‘faithful’ and
‘consistent’ are equivalent.
Proof of Theorem 6.1 The proof consists of three parts: Part 1 shows that the ﬁrst
condition is necessary, Part 2 shows that the second condition is necessary and Part 3
shows that both conditions taken together are sufﬁcient.
Part 1: If G is consistent with M, then
∀S ∈V : Xi, Xj ̸∈S, Xi ∼Xj ⇔(Xi,
Xj, S) ̸∈M. Deﬁnition 2.31 states that G is consistent with M if and only if
(Xi, Xj, S) ∈M ⇔Xi ⊥Xj∥GS. It is therefore sufﬁcient to show that for any two
distinct nodes Xi and Xj ∈G, Xi ∼Xj if and only if they cannot be d-separated by
any set of nodes S ⊂V .
Clearly, if Xi ∼Xj, then following Deﬁnitions 2.17 and 2.18, there does not exist
a set S that d-separates Xi from Xj. It remains to show that if (Xi, Xj, S) ̸∈M for all
S ⊂V : Xi, Xj ̸∈S, then Xi ∼Xj.
Let
S = {Y | Y is an ancestor of Xi or Xj}\{Xi, Xj}.
Since Xi, Xj are not d-separated by any set, it follows that (Xi, Xj, S) ̸∈M. Therefore,
there is an S-active trail τ (Deﬁnition 2.16) connecting Xi and Xj in G. Since the trail
τ is active, all the collider connections along τ are either in S or have a descendant in S
and all the fork or chain nodes are not in S. By the deﬁnition of S, every node that has
a descendant in S is itself in S. Thus every collider node on the trail τ is in S. Every
other node on τ (fork or chain) is an ancestor either of Xi or Xj or one of the collider
nodes of the path. Hence every node on τ is in S with the exception of Xi and Xj and
hence every node (other than Xi and Xj) is a collider node. It follows that the trail is
either Xi →Xj or Xj →Xi, or Xi →Y ←Xj, where Y ∈S. But, by construction of
S, the third possibility implies that the graph has a cycle – since Y is a child of both Xi
and Xj, there is a cycle if it is an ancestor of Xi and there is a cycle if it is an ancestor
of Xj. It follows that Xi ∼Xj.
Part 2: Assume that G is consistent with M, then (Xi, Xj, Xk) forms an immorality
in G if and only if
(Xi ̸∼Xk, and ∀S ⊂V : Xi, Xk ̸∈S, Xi ⊥Xk|S ⇒Xj ̸∈S)
Suppose (Xi, Xj, Xk) forms an immorality, then by deﬁnition Xi ̸∼Xk and, by the
basic separation properties of colliders, Xi ⊥Xk∥GS ⇔Xj ̸∈S.
For the other direction, suppose Xi ̸∼Xk and ∀S ⊂V, Xi ⊥Xk|S ⇒Xj ̸∈S. Con-
sider any trail τ between Xi and Xk containing Xj, where all the variables along the trail,
except for Xi and Xk are instantiated. Let S denote the set containing all the variables
along the trail except for Xi and Xk and no other variables, so that no other variables
are instantiated. Then, since Xi is not independent of Xk given S, it follows from basic
properties of instantiated connections that all connections along the trail are colliders and
hence that S contains exactly one variable, Xj. It follows that (Xi, Xj, Xk) forms an
immorality in G.

MARKOV EQUIVALENCE AND CONSISTENCY
173
Part 3: If the two conditions hold for a DAG G = (V, E), then it is consistent with
M. Since it is assumed that M is DAG isomorphic, there exists a DAG ˜G that is
consistent with M. By parts 1) and 2), ˜G satisﬁes both the conditions and hence has the
same immoralities and skeleton as G By Lemma 4.3, it follows that for any S, a trail τ
is S-active in G if and only if it is S-active in ˜G, so that G is consistent with M.
□
A criterion for determining a collection of DAGs that are Markov equivalent has been
established and this was used to determine conditions to characterize the set of directed
acyclic graphs that are consistent with a given set of conditional probability statements,
if such a directed acyclic graph exists.
6.2.1
Establishing the DAG isomorphic property
The last task of this section is to construct an algorithm that determines whether or
not a given set of conditional probability statements is DAG isomorphic. The method
is constructive; an algorithm is proposed that will return a DAG consistent with the
conditional probability statements if the statements are DAG isomorphic and will declare
that there is no such DAG otherwise. The algorithm is in three stages.
• Stage 1 examines the independence statements in M and tries to construct a con-
sistent graph. If this stage fails to ﬁnd a graph, then M is not DAG isomorphic.
• Stage 2 turns the graph into a directed acyclic graph, if this is possible.
• Stage 3 veriﬁes whether or not the resulting directed acyclic graph is consistent
with M. If it is, then M is clearly DAG isomorphic, but if it is not, then there
does not exist a consistent DAG.
Stage 1: Generate a graph from M if possible
This stage has three steps.
1. Consider all (Xi, Xj, S) ∈V × V × V, where V denotes the set of all subsets of V ,
such that Xi ̸= Xj and S ⊆V \{Xi, Xj}. Recall that (by deﬁnition) φ ∈V, where
φ denotes the empty set. Then set
Xi ∼Xj ⇎ ∃S ∈V | (Xi, Xj, S) ∈M.
Let G = (V, E) denote the undirected graph formed in this way. Let
S(Xi, Xj) = {Xk : Xi ⊥Xj|Xk}.
2. For every pair of nodes Xi and Xj such that Xi ̸∼Xj, test whether or not there
is a node Xk such that Xi ∼Xk and Xj ∼Xk and Xk ̸∈S(Xi, Xj).
If there is such a node, then direct the edges ⟨Xi, Xk⟩and ⟨Xj, Xk⟩by removing
them and replacing them with the directed edges (Xi, Xk) and (Xj, Xk) from E, so
that Xk is a collider node, UNLESS the algorithm has already modiﬁed sufﬁcient
edges so that this involves changing the direction of a directed edge. If this is
happens, then Stage 1 FAILS; the set of conditional independence statements is
not DAG isomorphic. The proof of this follows the statement of the algorithm.
3. If the orientation of Step 2 is completed, then phase 1 SUCCEEDS and returns a
graph ˜G, which is partially directed.

174
LEARNING THE GRAPH STRUCTURE
Stage 2: Turn G into a Directed Graph
The algorithm is as follows: Start with the
partially directed graph obtained from Stage 1, call it ˜G. Let G denote the current state
of the graph. While G contains undirected edges, repeat the following three steps.
1. Direct the graph G according to the following algorithm: run through all variables,
labelled {1, . . . , n}. For j = 1 to n, do:
(a) Rule 1: If variable Xj is part of a structure shown in Figure 6.1 (that is if
Xj takes the position of either A, B or C), where A ̸∼C; that is, if there is
a structure A →B ↔C and A ̸∼C, then replace B ↔C with the directed
edge B →C.
B
A
C
Figure 6.1
Structure for Rule 1.
(b) Rule 2: If the variable Xj is part of a structure between the three variables
(A, B, C) given in Figure 6.2, then replace A ↔C with the directed edge
A →C.
B
A
C
Figure 6.2
Structure for Rule 2.
(c) Rule 3: If the variable Xj is part of an edge structure given in Figure 6.3,
then replace B ↔D with B →D.
B
A
C
D
Figure 6.3
Structure for Rule 3.
(d) Rule 4: If Xj is part of an edge structure given in Figure 6.4, then replace
A ↔B with A →B, and replace C ↔B with C →B.
IF any of pair of directed edges form a collider not present from the directed
edges in ˜G, then the algorithm is terminated, and returns the value ‘FAIL’; the set
of conditional independence statements is not DAG isomorphic.

MARKOV EQUIVALENCE AND CONSISTENCY
175
B
A
C
D
Figure 6.4
Structure for Rule 4.
Repeat step 1 of Stage 2 until either a directed acyclic graph is formed, or until
no edges are directed during a full run, or until an additional collider connection
or cycle is formed.
If a directed acyclic graph is returned, then the algorithm SUCCEEDS. Go to
Stage 3.
2. If the resulting graph still has directed edges, then perform the following: Copy
the current graph G and call the copy ˜G∗. Select one of the undirected edges of
G and choose a direction for it that does not introduce any directed cycles or
new colliders. If this is possible, then TERMINATE with the resulting directed
acyclic graph; the algorithm SUCCEEDS; go to Stage 3. If this is not possible,
then discard ˜G∗and choose the opposite direction for the edge, then continue.
Stage 3: Check the Answer
Check that for every statement (Xi, Xj, S) ∈M, Xi ⊥
Xj∥GS holds for the resulting directed acyclic graph G. This may be carried out using
the algorithm to check for d-separation. If it does, then M is DAG isomorphic and G is
a consistent DAG; otherwise M is not DAG isomorphic.
Proof that the Algorithm Determines the DAG Isomorphism Property The following
additional lemma is needed.
Lemma 6.1 Let M be a DAG isomorphic set of conditional independence statements.
Suppose that Xi and Xk are two nodes such that Xi ̸∼Xk, but there is node Xj such that
(Xi, Xj, Xk) is a trail. If there is an S ⊂V such that Xj ̸∈S and (Xi, Xk, S) ∈M, then
for any T ⊂V , (Xi, Xk, T ) ∈M ⇒Xj ̸∈T .
Proof of Lemma 6.1 Suppose that Xj is a collider, chain or fork node between Xi and
Xk and suppose that Xi ⊥Xk∥GS, with Xj ̸∈S. It follows that the connection is Xi →
Xj ←Xk. It follows that any set that contains Xj will activate the trail (Xi, Xj, Xk)
between Xi and Xk. Hence, for any T ⊂V , if (Xi, Xk, T ) ∈M, then Xj ̸∈T .
□
The next task is to show that the algorithm given above determines whether or not a set
of conditional independence statements M is DAG isomorphic. That is, to show that if
the algorithm fails to return a consistent DAG, then M is not DAG isomorphic.
Stage 1:
Theorem 6.1 shows that if M is DAG isomorphic, then any directed acyclic
graph consistent with M will have the same skeleton and the same colliders as the graph
produced by Stage 1. The ﬁrst part of this stage produces the skeleton, the second part

176
LEARNING THE GRAPH STRUCTURE
identiﬁes the colliders that are present in all consistent graphs, if M is DAG isomorphic.
It identiﬁes a collider Xi →Xj ←Xk as soon as it ﬁnds a single S ⊂V such that
(Xi, Xk, S) ∈M with Xj ̸∈S. This decision is justiﬁed by Lemma 6.2.
The failure condition in Stage 1, Part 2 is necessary to prevent the algorithm from
switching edges that are essential for a DAG consistent with M; at each stage in the
algorithm, the directions of all the edges that have already been directed are necessary.
Stage 2:
The second stage is to locate a directed acyclic graph that retains the same
colliders and skeleton as the graph constructed in Stage 1. This is purely a task of graph
theory and does not involve M.
To show that the ﬁrst part this part of the process is sound, it is sufﬁcient to show
that edges directed by the four rules are a logical consequence of the requirement that
Stage 1 gives the skeleton and all the colliders.
• Rule 1 The alternative directed edge B ←C would create a new collider.
• Rule 2 The alternative directed edge A ←C would create a cycle.
• Rule 3 Since A ∼B and B ∼C, the edge B →D is permitted. If D →B were
chosen, it would be necessary to also have both A →B and C →B, to prevent
a cycle. But then (A, B, C) would form a new immorality, so D →B is not
permitted. Therefore the directed edge B →D is forced.
• Rule 4 Firstly, A →B is forced to prevent the formation of a new immorality
(D, A, B). If the edge B →C is used, this forces C →D to prevent formation of
a new immorality (B, C, D), which results in a directed cycle (A, B, C, D, A). It
follows that B →B ←C is forced.
Stage 3:
By the deﬁnition of ‘consistency’, any graph constructed from Stage 2 will
be consistent with M, if M is DAG isomorphic. Therefore, it is sufﬁcient to check the
statements in M. If there is a statement (Xi, Xj, S) ∈M such that Xi and Xj are not
d-separated by S, then M is not DAG isomorphic.
Any graph G satisfying Part 1 of Stage 3 is therefore a perfect I-map for any prob-
ability distribution p satisfying the set of conditional independence statements M. Part
2 of Stage 3 checks that the DAG does indeed satisfy the local directed Markov con-
dition (Deﬁnition 2.28); this is necessary and sufﬁcient to conclude that G is consistent
with M.
□
6.3
Reducing the size of the search
Finding the optimal structure from among all possible structures is (as stated earlier)
an NP-hard problem. Furthermore, not only does one have to estimate the underlying
probability distribution from a ﬁnite number of samples, one also has to store the resulting
probability potentials in a limited amount of machine memory. In general, it is therefore
not possible to consider all possible dependencies. Two methods of reducing of the
problem, the Chow-Liu tree and the K2 algorithm, are now considered. These seem to
have worked quite well in practice and applications of these approaches are discussed to
illustrate this.

REDUCING THE SIZE OF THE SEARCH
177
6.3.1
The Chow-Liu tree
The Chow-Liu tree assumes that each variable has at most one parent. Suppose there are
d variables and possible dependencies between the variables. The Chow-Liu approach is
to ﬁnd d −1 ﬁrst order dependencies between the variables. That is, the distribution p
over the random vector X = (X1, . . . Xd) is considered to be of the form
pX1,...,Xd =
d
j=1
pXmi |Xmi(j) ,
where (m1, . . . , md) is a permutation of (1, . . . , d) and 0 ≤i(j) ≤j. Each variable may
be conditioned on at most one of the variables; hence the term ﬁrst order tree dependence.
When estimating the tree, the Kullback-Leibler divergence is used to determine how
close two distributions are to each other. To compute the best ﬁtting Chow-Liu tree, the
potentials ˆpXi,Xj , ˆpXi and ˆpXj have to be computed, where ˆpXi,Xj , ˆpXi and ˆpXj denote
the empirical estimates of the potentials pXi,Xj , pXi and pXj from data. The deﬁnition
of mutual information is based on the Kullback-Leibler divergence.
Deﬁnition 6.2 (Mutual Information) The mutual information between two variables X
and Y is deﬁned as
I(X, Y) =

x,y
pX,Y(x, y) log pX,Y(x, y)
pX(x)pY (y).
This is, of course, the Kullback-Leibler divergence between pX,Y and pXpY:
I(X, Y) = DKL(pX,Y |pXpY).
Let ˆI denote the estimate of the mutual information from data; that is,
ˆI(X, Y) =

x,y
ˆpX,Y(x, y) log
ˆpX,Y(x, y)
ˆpX(x) ˆpY (y).
(6.4)
The idea is to ﬁnd the maximum weight dependence tree; that is, a tree σ such that for
any other tree σ ′,
d

j=1
ˆI(Xj, Xσ(j)) ≥
d

j=1
ˆI(Xj, Xσ ′(j)).
This uses Kruskal’s algorithm, described below. Here (j, σ(j))d
j=1 denotes the edge set
of the maximal weight tree; (j, σ ′(j))d
j=1 denotes the edge set of any other admissible
tree.
The optimization procedure
Kruskal’s algorithm runs as follows:
1. The d variables yield d(d −1)/2 edges. The edges are indexed in decreasing order,
according to their weights b1, b2, b3, . . . , bd(d−1)/2.
2. The edges b1 and b2 are selected. Then the edge b3 is added, if it does not form a
cycle.

178
LEARNING THE GRAPH STRUCTURE
3. This is repeated, through b4, . . . bd(d−1)/2, in that order, adding edges if they do
not form a cycle and discarding them if they form a cycle.
This procedure returns a unique tree if the weights are different. If two weights are equal,
one may impose an arbitrary ordering. From the d(d −1)/2 edges, exactly d −1 will be
chosen.
Lemma 6.2 Kruskal’s algorithm returns the tree with the maximum weight.
Proof of Lemma 6.2 The result may be proved by induction. It is clearly true for two
nodes. Assume that it is true for d nodes and consider a collection of d + 1 nodes,
labelled (X1, X2, . . . , Xd+1), where they are ordered so that for each j = 1, . . . , d + 1,
the maximal tree from (X1, . . . , Xj) gives the maximal tree from any selection of j
nodes from the full set of d + 1 nodes. Let b(i,j) denote the weight of edge (i, j) for
1 ≤i < j ≤d + 1. Edges will be considered to be undirected. Let T (d+1)
j
denote the
maximal tree obtained by selecting j nodes from the d + 1 and consider T (d+1)
d+1 .
Let Z denote the leaf node in T (d+1)
d+1
such that among all leaf nodes in T (d+1)
d+1
the
edge (Z, Y) in T (d+1)
d+1
has the smallest weight. Removing the node Z gives the maximal
tree on d nodes from the set of d + 1 nodes. This is seen as follows. Clearly, there is no
tree with larger weight that can be formed with these d nodes, otherwise the tree on d
nodes with larger weight, with the addition of the leaf (Z, Y) would be a tree on d + 1
nodes with greater weight than T d+1
d+1. It follows that Z = Xd+1 and hence that Xd+1 is
a leaf node of T (d+1)
d+1 .
By the inductive hypothesis, T (d+1)
d
may be obtained by applying Kruskal’s algorithm
to the weights (b(i,j))1≤i<j≤d. Now consider an application of Kruskal’s algorithm to the
weights (b(i,j))1≤i<j≤d+1 and note that for any (i, j) with i < j such that the undirected
edge (Xi, Xj) forms part of the tree T (d+1)
d
, b(i,d+1) < b(i,j) and b(j,d+1) < b(i,j). There-
fore, if the edges (b(i,j))1≤i<j≤d+1 are listed according to their weight and the Kruskal
algorithm applied, then all the edges used in T (d+1)
d
will appear further up the list than
any edge (b(k,d+1))d
k=1 and therefore all the edges of T (d+1)
d
will be included by the algo-
rithm before the edges (b(k,d+1))d
k=1 are considered. It follows that T (d+1)
d+1
is the graph
obtained by applying Kruskal’s algorithm to the nodes (X1, . . . , Xd+1).
□
Application to pattern recognition
The example given in the article [99] considers the
problem of machine recognition of handwritten numerals, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9. There
are c = 10 pattern classes. Let ai denote the numeral i. There is a prior distribution
p = (p0, p1, . . . , p9) over the numerals. The number is written on a 12 × 8 rectangle
and 96 binary measurements are used to represent the numeral; 1 if the cell contains
writing and 0 otherwise. In the example given in [99], 19 000 numerals produced by
four inventory clerks were scanned. 7000 of these were employed as training examples,
to ﬁnd the best ﬁtting trees and estimate the probabilities p0, . . . , p9. The optimal trees
for each of the 10 numerals were obtained. For the remaining numerals, the observation
x = (x1, . . . , x96) was considered. Using Bayes’ rule,
p(ak|x) = p(x|ak)p(ak)
p(x)
= p(x|ak)pk
p(x)
,

REDUCING THE SIZE OF THE SEARCH
179
the following classiﬁcation rule was used: the numeral was declared to be of class ak
if pkp(x|ak) ≥pip(x|ai) for all i ̸= k. Using the trees, the error rate was reduced from
0.09 to 0.04 compared with the model produced by assuming independence between the
contents of the 96 cells.
6.3.2
The Chow-Liu tree: A predictive approach
The following predictive approach (and the computations) in the section are due to M.
Gyllenberg and T.Koski [100], but the work turns out to be a special case of J. Suzuki
[101]. Recall the deﬁnition of the prior predictive distribution (Deﬁnition 1.9). The pre-
dictive approach considers the prior distribution and a set of parameters so that it may be
expressed in the form given in Equation (1.12), and uses this to construct the posterior
distribution, conditioned on the observations.
The prediction problem for a given Chow-Liu tree is now considered. This is inter-
preted as the likelihood function for the tree structure and is used as part of a predictive
technique for learning the optimal Chow-Liu tree.
Consider a Chow-Liu tree with d nodes, where X1 is the root variable and (Xm(2), . . . ,
Xm(d)) are the parent variables for (X2, . . . , Xd). That is, j = {Xm(j)}, j = 2, . . . , d.
The distribution along a tree T factorizes as follows:
pX(.|T ) = pX1
d
j=2
pXj |Xm(j).
Set
θj = pXj |Xm(j)(1|1)
j = 2, . . . , d
φj = pXj |Xm(j)(1|0)
j = 2, . . . , d
and
θ1 = pX1(1).
Now consider t complete, independent instantiations x1, . . . , xt of the variables in the
tree network, where for each j = 1, . . . , t, the row vector xj = (xj1, . . . , xjd) denotes
instantiation j. Let the matrix x =


x1...
xt

denote the complete set of independent
instantiations and let X denote the random matrix where each row Xj, j = 1, . . . , t is
an independent copy of X = (X1, . . . , Xd). Then
pX(x|T ) =
t
l=1
pX(xl|T )
= θn1
1 (1 −θ1)t−n1
d
j=2
θ
nj (1,1)
j
(1 −θj)nj (0,1)φ
nj (1,0)
j
(1 −φj)nj (0,0),

180
LEARNING THE GRAPH STRUCTURE
where, for j ≥2,
nj(1, 1) =
t
l=1
xljxl,m(j),
nj(1, 0) =
t
l=1
xlj(1 −xl,m(j))
nj(0, 1) =
t
l=1
(1 −xlj)xl,m(j),
nj(0, 0) =
t
l=1
(1 −xlj)(1 −xl,m(j)).
and
Nj(1) =
t
l=1
xlj,
Nj(0) = t −Nj(1),
j = 1, . . . , d.
Set
n = {(Nj(0), Nj(1))d
j=1, (nj(1, 1))d
j=2, (nj(1, 0))d
j=2, (nj(0, 1))d
j=2, (nj(0, 0))d
j=2}.
Note that
Nm(j)(0) = nj(0, 0) + nj(1, 0)
and
Nm(j)(1) = nj(1, 1) + nj(0, 1).
The interpretation of these quantities is clear; for example, nj(1, 1) counts the number of
rows of x in which the family conﬁguration (xkj, xk,m(j)) = (1, 1) appears. Regarded as
a function of (θ, φ), where θ = (θ1, . . . , θd) and φ = (φ2, . . . , φd), this is the likelihood
function:
L(θ, φ; x) =
t
l=1
pX(xl|T )
= θN1
1 (1 −θ1)t−N1
d
j=2
θ
nj (1,1)
j
(1 −θj)nj (0,1)φni(1,0)
i
(1 −φi)ni(0,0).
(6.5)
When the parameters (θ, φ) are included in the notation, the probability distribution may
be written as
pX|θ,φ(x) =
d
j=1
pXj |Xm(j),θj ,φj (xj|xm(j)).
To compute the predictive distribution for a ﬁxed tree, a prior distribution g(θ, φ) is
required over the parameter space. Then, using T to denote the tree structure, the prior
predictive distribution for X, deﬁned in Equation (1.12), is given by
pX(x|T ) =

×
t
l=1
pX|θ,φ(xl)g(θ, φ)dθdφ,
where  ×  denotes the parameter space for (θ, φ). It is convenient to choose
g(θ, φ) =
d
i=1
h(θi)
d
i=2
k(φi),
where the parameters are taken to be independent, the variables (θi)d
i=1 identically dis-
tributed and the variables (φi)d
i=2 are identically distributed. Such a model is known as

REDUCING THE SIZE OF THE SEARCH
181
local meta independence ([67] p. 191). For this model, it is clear that the probability
distribution factorizes as
pX(x|T ) = I1I2I3,
where
I1 =
 1
0
θN1
1 (1 −θ1)t−N1h(θ1)dθ1,
I2 =
d
j=2
 1
0
θn(1,1)
j
(1 −θj)n(0,1)h(θj)dθj
I3 =
d
j=2
 1
0
φ
nj (1,0)
j
(1 −φj)nj (0,0)k(φj)dφj.
If h = k, and h has a Beta density
h(θ) =

(α1+α2)
(α1)(α2)θα1−1(1 −θ)α2−1
θ ∈[0, 1]
0
θ ̸∈[0, 1],
the integrals may be computed in the usual way;
I1 = (α1 + α2)(N1 + α1)(t −N1 + α2)
(α1)(α2)(t + α1 + α2)
,
I2 =
d
j=2
(α1 + α2)(nj(1, 1) + α1)(nj(0, 1) + α2)
(α1)(α2)(α1 + α2 + nj(1, 1) + nj(0, 1)),
I3 =
d
j=2
(α1 + α2)(nj(1, 0) + α1)(nj(0, 0) + α2)
(α1)(α2)(α1 + α2 + nj(1, 0) + nj(0, 0)).
The learning of the tree structure T now involves ﬁnding the tree that maximizes pX(x|T )
or, equivalently, minimizes −log pX(x|T ). This is equivalent to minimizing
F(n) = −log (N1(1) + α1) −log (t −N1(1) + α2)
−
d

j=2
log (nj(1, 1) + α1) −
d

j=2
log (nj(0, 1) + α2)
+
d

j=2
log (α1 + α2 + Nm(j)(1))
−
d

j=2
log (nj(1, 0) + α1) −
d

j=2
log (nj(0, 0) + α2)
+
d

j=2
log (α1 + α2 + Nm(j)(0)).

182
LEARNING THE GRAPH STRUCTURE
Now consider, for example, the Jeffreys’ prior [24], where α1 = α2 = 1
2 and recall the
standard formula:
(n + 1) = (2π)1/2nn+ 1
2 e−nea(n)/12n,
where 0 ≤an ≤1. If n is a non-negative integer, this is simply Stirling’s formula. It
follows that, for any n ≥1,
log 

n + 1
2

= 1
2 log(2π) + n log

n −1
2

−

n −1
2

+ c1(n)
n
and
log (n + 1) =

n + 1
2

log n −n + c2(n)
n
,
where 0 ≤ci(n) ≤1. To good approximation, therefore, the optimal tree is found by
minimizing
˜F(n) = C(n) −N1(1) log

N1(1) −1
2

−N1(0) log

N1(0) −1
2

−
d

j=2
nj(1, 1) log

nj(1, 1) −1
2

−
d

j=2
nj(0, 1) log

nj(0, 1) −1
2

+
d

j=2

Nm(j)(1) + 1
2

log Nm(j)(1)
−
d

j=2
nj(1, 0) log

nj(1, 0) −1
2

−
d

j=2
nj(0, 0) log

nj(0, 0) −1
2

+
d

j=2

Nm(j)(0) + 1
2

log Nm(j)(0)
where C is bounded and the bound depends only on d. Now, for j = 1, . . . , d set ˆqj = Nj
t
and, for j ≥2, ˆpj(a, b) =
nj (a,b)
N(j)(b) for a = 0, 1 and b = 0, 1 and ˆpj(a) = Nj (a)
t
. Recall
the deﬁnition of the empirical mutual information, Equation (6.4). Let ˆIj,(j) denote the
empirical mutual information between the variable Xj and its parent, so that
ˆIj,(j) =
1

a=0
1

b=0
ˆpj(a, b) log
ˆpj(a, b)
ˆqj(a)ˆq(j)(b)
and let H denote the function
H(x) = −x log x −(1 −x) log(1 −x).
Then, after a little computation,
−log p(Xt|T ) = ˜C + t
d

j=1
H(ˆqj) −t
d

j=2
ˆIj,(j) + 1
2
d

j=2

log N(j)(1) + log N(j)(0)

,

REDUCING THE SIZE OF THE SEARCH
183
where ˜C is a bounded function. The problem therefore reduces to maximizing
t
d

j=2
ˆIj,(j) −1
2
d

j=2

log N(j)(1) + log N(j)(0) .
(6.6)
Recall that for the maximum likelihood approach, the problem was to maximize the
mutual information, d
j=2 ˆIj,(j), and this was carried out using Kruskal’s algorithm.
The problem of maximizing the Equation (6.6) may be tackled by a similar use of
Kruskal’s algorithm.
□
6.3.3
The K2 structural learning algorithm
This example is taken from the paper [102]. It shows an application to Bayesian network
learning techniques for task execution in mobile robots. The task here is for the robot to
locate an open door and travel through it.
The robot emits sonar pulses and is equipped with eight detectors, which detect the
echoes. From this information, it has to decide where the door is located.
An action has to be taken: step to left, right, or straight ahead. This is the class
variable and the class has to be determined by the signals received by the eight detectors.
Since the signals are not independent of each other (the echoes may be created by the
same object), the model is improved by incorporating a dependence structure.
In this experiment, the problem is to learn the structure of the Bayesian network and
to estimate the probability potentials from the training database.
The K2 algorithm is employed to establish a suitable structure. The algorithm has to
take into account that the number of parents of a node should not be high, since the size
of the associated probability potentials increases exponentially according to the number
of parents of a node. Therefore, the algorithm limits the number of parents a node can
take. For the robot learning example, the maximum number is set to four, which is a
value widely used. The size of the probability potentials cannot be too large, since the
robot is expected to ﬁnd the door and travel through it in real time.
The algorithm assumes that an order has been established for the d nodes X1, . . . , Xd
so that, for each i, the parent nodes i for variable Xi are established among the nodes
X1, . . . , Xi−1. For j = 1, . . . , i −1, the empirical Kullback-Leibler divergence between
the two empirical probability distributions of (X1, . . . , Xi), one determined by the graphs
with and the other determined by the graph without the directed edge (i, j), is measured
and the edge is retained if a) the divergence is sufﬁciently large and b) node i does not
already have four parents.
The resulting algorithm is a greedy algorithm and therefore it cannot ensure that the
net resulting from the learning process is the best ﬁt.
The intensity of an echo may be modelled as a continuous random variable, but the
variables are discretised for computational convenience. In general, it is not convenient
to use a variable with more than 20 different values. For this reason, the multinomial
distribution is by far the most useful in Bayesian networks.
When the K2 algorithm is used, the learnt structure depends completely on the
random order of the variables generated before the learning process starts. This may
have unfortunate consequences if an unfortunate order is used. In the Bayesian robotics

184
LEARNING THE GRAPH STRUCTURE
S4
S5
S3
S6
S2
S7
C
S1
S8
Figure 6.5
Network produced by the K2 algorithm. Here the nodes Sj represent the
signals received by the sensors and C denotes the class variable, the action to be per-
formed.
experiment therefore, in order to reduce the impact of the random order in the net struc-
tures learnt, the experiments were repeated 1000 times and nets with optimal values
selected.
The Bayesian network learnt using the K2 algorithm and entropy evaluation met-
rics (i.e. the Kullback-Leibler divergence) provided the best classiﬁcation accuracy. The
resulting network for the eight variables is shown in Figure 6.5.
6.3.4
The MMHC algorithm
The maximum minimum hill climbing (MMHC) algorithm was introduced in [103]. It
ﬁnds a directed acyclic graph structure for a probability distribution p, where there exists
a graph G, such that p and G are faithful to each other, following Deﬁnition 2.30. The
problem is that it requires that there exists a graph faithful to the distribution, so there
are situations where it will not work. The example of the trek (Section 2.10.1) gives an
example of a relatively simple distribution over four binary variables for which there
does not exist a faithful graph. The graph located will satisfy Deﬁnition 2.30, assuming
that there are sufﬁcient instantiations to determine that there is a signiﬁcant association
between two variables when an association exists. The key identity used is that, for
a faithful Bayesian network (G, p), X and Y d-separated by a set of variables Z is
equivalent to the statement that X and Y are conditionally independent given Z. The
algorithm then works by locating the conditional independence structure.
The maximum minimum parents and children algorithm
The MMPC algorithm,
when run on a target variable T , identiﬁes the existence of edges to and from T . It
cannot determine the direction of the edges; it determines the skeleton. The algorithm
works in three stages. Firstly, a forward stage starts with an empty graph, and adds
in all possible edges. There are possibly too many edges after this stage. Secondly, a
backward stage removes some of the edges. The resulting graph, after the second stage,

REDUCING THE SIZE OF THE SEARCH
185
will contain no false negatives, but may still contain some false positives. A third stage
is implemented to remove the false positives. The algorithm runs as follows:
Stage 1:
• Order the variables Xi ∈V \{T }, (Xi)d
i=1. Start with Z1 = φ, the empty set.
• For i = 1, . . . d, Xi ∈V , Xi ̸= T , check whether Xi ⊥T |Z. If it is not, let Zi+1 =
Zi ∪{Xi}. Otherwise, Zi+1 = Zi.
Set Z = Zd.
Stage 2:
Suppose that Z contains k variables. Label them X1, . . . , Xk. Let Zk = Z. For
i = 0, . . . , k −1, check whether there exists a set S ⊆Zk−i\{Xk−i} such that T ⊥Xk−i|S.
If there is, then Zk−i−1 = Z\{Xk−i}, otherwise Zk−i−1 = Zk−i.
Let ZT = Z1. This set contains all the variables which have an edge either to or from
the variable T .
The algorithm may return false positives. Suppose a probability distribution may be
represented by the DAG in Figure 6.6. Working from T , the node C may enter the output,
and remain in the output.
This is because C is dependent on T , conditioned on all subsets of T s parents and
children; namely, φ (the empty set) and {A}. Note that the collider connection T AB, is
opened when A is instantiated so that, when A is instantiated and B is uninstantiated, T
is d-connected with C. For φ (the empty set), T AC is a chain connection, where A is
uninstantiated, so that T is d-connected to C.
T and C are d-separated if and only if A and B are simultaneously instantiated; that
is, T ⊥C|{A, B}. But if B is independent from T given the empty set, so it will be
removed from Z. Therefore, the link T C will not be removed.
This is corrected by considering the parent/child sets of the other variables. When
working from C, both A and B will be in the parent/child set, and T ⊥C|{A, B}. This
leads to the third stage of the algorithm.
Stage 3:
Let (ZX)X∈V denote the parent/child sets for all the variables arrived at after
Stage 2. Let X1, . . . , Xj denote the set of variables in ZT , the parent child set for T
arrived at after Stage 2.
Set Y0 = ZT . For i = 1, . . . , j, set
Yi =
 Yi−1\{Xi}
T ̸∈ZXi
Yi−1
T ∈ZXi.
Now set YT = Yj. This returns the complete parent/child set for T .
T
A
C
B
Figure 6.6
Min max parent child: A false positive.

186
LEARNING THE GRAPH STRUCTURE
Testing for conditional independence
Testing for conditional independence is carried
out, quite simply, using the usual χ2 test. To test whether or not X ⊥Y|Z, let n(x, y, z)
denote the number of times (X, Y, Z) = (x, y, z) appears in the data, n(x, z), n(y, z),
n(z) the number of instances of (X, Z) = (x, z), (Y, Z) = (y, z), Z = z respectively.
The G2 statistic, which is standard, is deﬁned as
G2 = 2

x,y,z
n(x, y, z) log n(x, y, z)n(z)
n(x, z)n(y, z).
Asymptotically, this is distributed as a χ2 distribution on (jx −1)(jy −1)jz degrees of
freedom, where jx, jy and jz are the number of values that the random variables X, Y
and Z respectively can take. Note that G tests independence based on characterization 4
of conditional independence, listed in Theorem 2.1.
Maximum minimum Hill Climbing algorithm
Having located the parent/child sets,
the algorithm advocated by [103] works as follows: The nodes in V are labelled (Xi)d
i=1
and, for i = 1, . . . , d, ZXi denotes the edge set for node Xi obtained from the MMPC
algorithm. Let E = ∪d
i=1ZXi. The algorithm is as follows: denoting the current graph
by G,
• Start with the empty graph.
• At each stage, either add an edge in E to G, or delete an edge from G, or reverse an
edge in G, or leave the graph unaltered. From all the possibilities of ‘add an edge’,
‘delete an edge’, ‘reverse an edge’, ‘leave the graph unaltered’, choose the one that
gives the greatest score; that is, the operation that produces the greatest reduction in
the Kullback-Leibler divergence between the probability modelled along the graph
and the empirical probability.
• Repeat until the score is not changed.
The algorithm may be modiﬁed as follows: instead of the best change, make the best
change that results on a graph that has not already appeared. When 15 changes occur
without an increase in the best score ever encountered during the search, the algorithm
terminates. The DAG that produced the best score is then returned.
6.4
Monte Carlo methods for locating the graph structure
As usual, let V = {X1, . . . , Xd} denote the set of variables. This section describes a
Markov chain Monte Carlo method for locating the essential graph. If there is a large
number of variables, the graph returned by the Markov chain Monte Carlo method may
not necessarily be ‘optimal’, but it should represent the dependence structure to a good
approximation.
A Monte Carlo algorithm generates a stochastic process that moves through the graph
structures under consideration. Let x denote the data and let E∗denote the set of possible
edge sets for essential graphs, then the idea is to generate a time homogeneous Markov
chain {M(t) : t = 0, 1, 2, . . .}, where M(t) ∈E∗for each t ≥0, with an equilibrium dis-
tribution pE(.|x) : E∗→[0, 1], deﬁned by modifying Equation (6.2), to take into account

MONTE CARLO METHODS FOR LOCATING THE GRAPH STRUCTURE
187
the fact that E∗is the space of essential graphs. That is, a directed acyclic graph within
the equivalence class is chosen, the probability given by Equation (6.2) is computed and
then multiplied by the number of graphs in the equivalence class to give the probability
of the essential graph.
The necessary results about Markov chains and Monte Carlo methods were discussed
in Section 3.7. The Monte Carlo method ﬁnds constructs an irreducible aperiodic Markov
chain that has pE(.|x) (or a suitable approximation) as its invariant measure. By running
the chain, an approximation ˆpE(.|x) to this distribution may be computed and the edge
set E that maximizes ˆpE(.|x) is selected.
The Markov chain Monte Carlo model composition algorithm
The Markov chain
Monte Carlo model composition algorithm, known as MC3, and the augmented Markov
chain Monte Carlo model composition (AMC3) algorithm were introduced by Madigan
and York, and Madigan, Andersson, Perlman and Volinsky in 1995 and 1996 respec-
tively. They are described in [87]. The MC3 algorithm constructs an aperiodic irreducible
Markov chain {M(t), t = 1, 2, . . .} with state space E∗with an equilibrium distribution
that approximates pE(.|x), where x denotes the data matrix.
The difﬁculty with constructing Markov chains over the set of essential graphs is that
if only a single edge is modiﬁed at a time, the chain is not irreducible. This is seen rather
simply with the immorality A →B ←C. This is an essential graph on three variables.
Any alteration of a single edge (either by adding in one of (A, C), (C, A) or ⟨A, C⟩,
or un-directing one of the directed edges, or changing the direction of an edge) gives
a graph that is not an essential graph. It is therefore not possible to move in a single
step from the immorality (A, B, C) (where B is the collider node) to a different essential
graph on the variables (A, B, C). Filling in the details is left as an exercise (Exercise
2). Therefore, any Markov chain with state space the possible essential graphs for three
variables will not be irreducible if at most one edge is altered at each transition.
The (MC)3 algorithm therefore considers triples of nodes and works as follows. Let
M(0) be an edge set of an arbitrarily chosen essential graph. To move from M(t) to
M(t + 1), do the following:
• Choose three nodes (Xi, Xj, Xk) at random, where Xi ̸= Xj, Xj ̸= Xk, Xi ̸= Xk,
taking any possible triple of nodes each with equal probability.
• Let E denote the current edge set. As usual, E = D ∪U where D denotes the
directed edges and U denotes the undirected edges. ⟨α, β⟩∈U if and only if
both (α, β) ∈E and (β, α) ∈E. (α, β) ∈D if and only if both (α, β) ∈E and
(β, α) ̸∈E. For Fij and Fjk where Fpq is deﬁned below, consider the 16 possible
graphs generated by keeping all other edges the same and modifying any edges
between the two pairs [Xi, Xj] and [Xj, Xk] (where [α, β] simply denotes the
ordered pair of vertices) according to the four possibilities for each pair:
Fpq =



1
(Xp, Xq) ̸∈E
(Xq, Xp) ̸∈E
(i.e.(Xp, Xq) ̸∈D,
(Xq, Xp) ̸∈D, ⟨Xp, Xq⟩̸∈U)
2
(Xp, Xq) ̸∈E
(Xq, Xp) ∈E
(i.e.(Xq, Xp) ∈D)
3
(Xp, Xq) ∈E
(Xq, Xp) ̸∈E
(i.e.(Xp, Xq) ∈D)
4
(Xp, Xq) ∈E
(Xq, Xp) ∈E
(i.e.⟨Xp, Xq⟩∈U).

188
LEARNING THE GRAPH STRUCTURE
Suppose the current state is E0. Check each of the 16 graphs E0, E1, . . . , E15 (the
current graph will be one of the possibilities) generated by all the possibilities of
Fij and Fjk. For each graph, check whether it is an essential graph, using the
criteria of Theorem 4.5.
This part takes a fair amount of computation. It is necessary to keep track of
both U and D, the undirected and directed nodes respectively. By keeping track
of the chain components, it should be relatively easy to check
1. whether the graph is still a chain graph following the alteration of the two edges,
2. if it is, whether the chain components are still triangulated after alteration of the
two edges,
3. that the alteration of the two edges has not introduced any induced sub-graph
of three variables of the type shown in Figure 4.22 and
4. that every directed edge is strongly protected (the protection has not been
removed from unaltered edges and that new directed edges are strongly pro-
tected).
For graph Ek, k = 0, . . . , 15, assign a probability 0 if it is not a chain graph, other-
wise compute a quantity proportional to pE|X(Ek|x), or a suitable approximation to this
quantity.
One way to do this is to ﬁnd a directed acyclic graph within the equivalence class of
the essential graph with edge set Ek. If a uniform prior is considered over all possible
directed acyclic graphs, then, from Equation (6.2), it is sufﬁcient to use Equation (6.1)
and multiply by the number of directed acyclic graphs within the equivalence class. In
general, computing the exact number of directed acyclic graphs within an equivalence
class is not straightforward; either all possible equivalent DAGs have to be constructed,
or else a suitable approximation to this number has to be found. Let xk = pX|E(Dk|x)
using Equation (6.1), where Dk is a directed acyclic graph within the equivalence class.
Set
PE0,Ek =
xk
15
m=0 xm
k = 0, 1, . . . , 15.
These are the one-step transition probability values for moving from M(t) = E0 to M(t +
1).
At ﬁrst sight, this may look unpleasant. But the programming can be made more
efﬁcient by observing that graph M(t) is already essential and that only two edges
are being altered. This observation may be used both to help determine which of the
16 possibilities for M(t + 1) are essential graphs and also to reduce the number of
new quantities n(xi
j|πl
j) and n(πl
j) that have to be computed at each stage. Writing the
programme to run a Monte Carlo on directed acyclic graphs, changing a single edge at
a time, is easier, but the problem with this is that the algorithm may run through several
different DAGs that are all Markov equivalent. The selection procedure for the (MC)3
algorithm ensures that the process moves between graphs that have essentially different
Markov structures and hence improves ‘irreducibility’.
To keep track of the number of DAGs within an equivalence class, the following
modiﬁcation of the MC3 algorithm may be used:

WOMEN IN MATHEMATICS
189
1. Start with an empty graph.
2. It is required to store M(t) for each t = 1, . . . , N (where N is the length of the
Markov chain that is to be run) and at each stage it is required to store each DAG
for which M(t) is the essential graph.
3. For l = 1, . . . , 1
2d(d −1)(d −2) (where d is the number of nodes) do:
(a) Choose a triple of nodes (Xi, Xj, Xk) where i < k, i ̸= j, j ̸= k at random,
among the triples that have not been chosen before in the current cycle.
(b) Consider all 16 possibilities of (Fij, Fjk) (deﬁned above) when applied to the
essential graph and record those for which the new graph is an essential graph.
For each of those that is an essential graph, apply each of the possible directed
edge arrangements corresponding to (Fij, Fjk) (exactly one if both Fij and
Fjk contain either no edge or a directed edge, two if one of them contains
an undirected edge and the other either no edge or a directed edge and four
if both of them contain undirected edges) and retain those that are directed
acyclic graphs. This keeps track of the DAGs in the equivalence class.
(c) For E0, . . . , E16, set xk = 0 if Ek is not a chain graph. Otherwise, choose
a directed acyclic graph Dk within the equivalence class and compute xk =
n(k)pX|E(x|Dk), where n(k) is the number of DAGs in the equivalence class
of Ek and pX|E(Dk|x) is computed using Equation (6.1).
(d) Set
PE0,Ek =
xk
15
m=0 xk
,
k = 1, . . . , 15.
These are the one-step transition probabilities for moving from M(t) to M(t +
1).
4. M(t) is a Markov chain, but not time homogeneous, because the edges have not
been chosen in the same way at each stage. But Nn = M( n
2d(d −1)(d −2) + k)
is a time homogeneous Markov chain for any choice of k ∈{0, . . . , 1
2d(d −1)(d −
2) −1}. The whole sequence M(t) : t = 1, 2, 3, . . . , N may be taken to compute
the estimate for the graph structure.
The (MC)3 algorithm is computationally expensive, because it has to check whether each
graph is an essential graph. The (AMC)3 algorithm introduced by D. Madigan, S.A.
Andersson and M.D. Perlman [87] makes more use of the graph structure in an attempt
to reduce the computational complexity. Details are found in [87].
The number of DAGs per equivalence class was investigated by P˜ena in [104]. There
does not exist a general method for exact computation of this; P˜ena develops Markov
chain Monte Carlo techniques to estimate numbers of DAGs in an equivalence class in
some situations.
6.5
Women in mathematics
The following example is taken from [87], concerning the attitudes of New Jersey high
school students towards mathematics. The (MC)3 algorithm applied to the ‘Women in

190
LEARNING THE GRAPH STRUCTURE
B
E
C
A
D
F
Figure 6.7
Essential graph for the ‘Women in Mathematics’ data, produced by AMC3
algorithm.
Mathematics’ data given below returned the graph shown in Figure 6.7. It was well known
that female students of high school age tended to take fewer mathematics courses than
males. The Woman and Mathematics (WAM) Secondary School Lectureship Programme
was designed both to encourage more interest in mathematics by females and to show
‘positive role models’1 by presenting lectures, all given by women in the mathematical
sciences. A survey was carried out to evaluate one aspect of the WAM lectures. A total of
1190 students at eight high schools (four urban and four suburban) responded to a ques-
tionnaire inquiring about attitudes towards mathematics achievement and related topics.
Although care was taken both in the selection of schools and in the assignment of students
to either attendance or non attendance of lectures, this was not a formal experiment. The
variables were: A – lecture attendance, B – gender, C – school type (suburban or urban),
D – ‘I’ll need mathematics in my future work’ (agree/disagree), E – subject preference
(mathematical science/liberal arts), F – future plans (higher education/immediate job).
The data obtained is given in Table 6.1, which may be found in [105]; the data is taken
from [106].
This is an example with six variables, each binary, which may be implemented in
MATLAB, using the algorithms outlined in the chapter; the computational requirements
for a complete search are not too demanding.
The following results were obtained using a Markov chain Monte Carlo approaches.
The essential graph selected by the Augmented Markov chain Monte Carlo model com-
position scheme, known as AMC3, is given in Figure 6.7. Note that this graph has two
connected components; one component containing the variable A (whether or not the
Table 6.1
Data for ‘Women in Mathematics’. Source: C.B. Lacampagne (1979) [106].
suburban
urban
school gender
female
male
female
male
lecture
y
n
y
n
y
n
y
n
future
preference
‘need mathematics’
college
mathematical
y
37
27
51
48
51
55
109
86
n
16
11
10
19
24
28
21
25
arts
y
16
15
7
6
32
34
30
31
n
12
24
13
7
55
39
26
19
job
mathematical
y
10
8
12
15
2
1
9
5
n
9
4
8
9
8
9
4
5
arts
y
7
10
7
3
5
2
1
3
n
8
4
6
4
10
9
3
6
1 A person who serves as a model in a particular behavioural or social role for another person to emulate.

WOMEN IN MATHEMATICS
191
B
E
C
A
D
F
Figure 6.8
Moral graph for the ‘Women in Mathematics’ data, produced by an McMC
algorithm.
students heard a lecture) and the other component containing the variables B, C, D, E
and F.
This example illustrates that the optimal graph will not be connected if two sets of
variables are independent of each other. The algorithm gives a graph where the variable
A (lecture attendance) is independent of all the other variables in the model. The essential
graph indicates that C ⊥D, but that C ̸⊥D|F.
The Markov chain Monte Carlo algorithm, introduced by J. Corander and T. Koski
[107], returns the moral graph. It is computationally less expensive and therefore can be
applied to larger sets of variables. The graph returned using this algorithm is given in
Figure 6.8. It is the moral graph corresponding to the graph in Figure 6.7.
Notes
The Cooper-Herskovitz likelihood is taken from [95]. Statistical learning of
graphical models from databases has been extensively discussed both in the computer
science and statistics literature. Generally, the vast number of existing works agree on
the main challenges related to such tasks, ﬁrst of which is the super-exponential increase
in the number of potential model structures as a function of the number of nodes, and
the second obstacle being the equivalence of statistical models determined by different
networks. The equivalence of Bayesian networks creates a set of equivalence classes in
the space of DAGs. This constitutes a difﬁculty for efﬁciency of algorithms for structure
learning, as an algorithm may be wasting resources searching within an equivalence class.
For a study and references in this topic, the reader is referred to [107]. The Chow-Liu tree
is taken from [99] and the robotics experiment is taken from [102]. The maximum min-
imum hill climbing algorithm is found in [103]. The treatise [89] contains an advanced
treatment of the learning of graph structures and several new concepts for this.

192
EXERCISES: LEARNING THE GRAPH STRUCTURE
6.6
Exercises: Learning the graph structure
1. Consider the DAGs in Figure 6.9. For each DAG in the ﬁgure, ﬁnd all the equivalent
DAGs. and ﬁnd the essential graph for the equivalent graphs.
B
A
D
C
B
A
D
C
B
A
D
C
B
A
D
C
Figure 6.9
Four DAGs (see Exercise 1).
2. Consider a collider connection A →B ←C. Is this an essential graph? List all
the essential graphs on three variables. List all the graphs that may be obtained by
altering one edge of the graph A →B ←C through either adding or removing a
directed edge or an undirected edge, or from directing an undirected edge, or from
‘un-directing’ a directed edge. Which of these graphs are essential graphs?
3. a(a) Let φ(θj.l, αj.l) denote the Dirichlet density Dir(αj1l, . . . , αjkj l). By performing
the required integration, prove that the likelihood function for the graph structure,
deﬁned by
pX|E(x|E) =

n

k=1
pX(k)|,E(x(k)|θ, E)
d
j=1
qj

l=1
φ(θj.l, αj.l)dθj.l
is given by
pX|E(x|E) =
d
j=1
qj

l=1
(kj
i=1 αjil)
(n(π(l)
j ) + kj
i=1 αjil)
kj

i=1
(n(x(i)
j |π(l)
j ) + αjil)
(αjil)
.
In your answer, state clearly the meaning of all the notation. You may use the
identity: for any a1 > 0, . . . , an > 0,

LEARNING THE GRAPH STRUCTURE
193
 1
0
 1−θ1
0
. . .
 1−(θ1+...+θn−2)
0


n−1

j=1
θ
aj −1
j


×

1 −
n−1

j=1
θj


an−1
dθn−1 . . . dθ1 =
	n
j=1 (aj)
(n
j=1 aj).
(b) What parameters αj.l are used if a uniform prior is taken on every θj.l? What does
this give for the Cooper-Herskovitz likelihood? You may use (n) = (n −1)!.
4. Chow-Liu Tree: MATLAB exercise. Generate three columns, c1, c2 and c3, each
containing random samples of 50 Be(1/2) observations. Here Be(1/2) means
Bernoulli trials, returning 0 with probability 1/2 and 1 with probability 1/2.
Let c4 = c1 + c2 and let c5 = c3 + c4. Implement the Kruskal algorithm on the
variables c1, c2, c3, c4, c5 and see which edges are chosen.
5. Chow-Liu Tree: MATLAB exercise. Download the data set from the URL address
http://archive.ics.uci.edu/ml/machine-learning-databases/zoo/zoo.data
A description of the data is found at the address http://archive.ics.uci.edu/ml/datasets/
Zoo
The data set presents animal attributes: hair type, feather type, egg type, milk
type; whether it is airborne, aquatic, a predator; has teeth, has a backbone, breathes,
is venomous, has ﬁns, legs, tail; is domestic, or cat-size. The last variable is a
classiﬁcation of the type of animal.
Firstly, compute the estimated probability distribution for the 17 variables, assum-
ing that they are independent. What is the Kullback-Leibler distance between the
empirical distribution and the estimate using the independence model? Secondly,
use MATLAB to perform Kruskal’s algorithm, to determine the optimal Chow-Liu
tree. Calculate the estimated probability distribution, assuming that the distribution
factorizes according to the Chow-Liu tree. Calculate the Kullback-Leibler distance
between this estimate and the empirical probability distribution.
6. The K2 structural learning algorithm allows each variable to have at most four
parents. It starts with an ordering of the nodes and, working through each node,
chooses the best parents for that node, out of the nodes with a lower order, up to
maximum of four parents. Write a MATLAB code to implement this on the ‘zoo’
data set.
There are 17! ≃3.56 × 1014 ways to choose an ordering for the nodes. It is clearly
not feasible to try all of them. Choose 100 at random. Compute the estimates of the
probability distribution assuming that it may be factorized along the resulting trees.
Does the K2 algorithm give a signiﬁcantly better approximation than the Chow-Liu
tree?
7. Write a MATLAB code to implement an MMPC (Maximum Minimum Parents Chil-
dren Algorithm) on the zoo data set. Next, try to implement the Maximum Minimum

194
EXERCISES: LEARNING THE GRAPH STRUCTURE
Hill Climbing algorithm to ﬁnd the graph structure. Is the resulting factorization sig-
niﬁcantly better than the factorizations obtained using the K2 algorithm or Chow-Liu
algorithm? Use the Kullback-Leibler distance and an appropriate χ2 test.
8. Download the Bayes Nets Toolbox for MATLAB from http://www.cs.ubc.ca/
∼murphyk/Software/BNT/bnt.html and see what results it gives for structure
learning with the ‘zoo’ data set.
9. Women in Mathematics The ‘Women in Mathematics’ data provides an example
on six variables to which the algorithms presented in the chapter may be applied.
(a) Write a MATLAB code to establish the complete set of conditional independence
statements of the form X ⊥Y or X ⊥Y|Z that seem to be supported by the data.
The independence statements are obtained by computing for all variables
(X, Y, Z) ˆpX, ˆpX,Y, ˆpX,Y|Z. Since there are multiple hypothesis tests, the nom-
inal signiﬁcance level may be much lower than the true signiﬁcance level, but
we will ignore that problem in this example. Since all the variables are binary,
the data supports the assertion X ̸⊥Y if
ˆI(X, Y) :=

x,y
ˆpX,Y log
ˆpX,Y(x, y)
pX(x)pY (y) > 1
2χ1(0.95)
(where the nominal signiﬁcance level for each test is 5%) and the data supports
the assertion X ̸⊥Y|Z if
ˆI(X, Y|Z = z) :=

x,y
ˆpX,Y|Z log
ˆpX,Y|Z(x, y|z)
pX|Z(x|z)pY|Z(y|z) > 1
2χ1(0.95)
for either of z.
This is a ‘toy’ example, on only six binary variables. The conditional inde-
pendence relations from conditioning only on no variables or a single vari-
able, illustrates that the algorithm is unworkable in practice. There are already
6 × 5 + 6 × 5 × 4 × 2 = 270 tests required to establish only those conditional
independence relations that involve conditioning on zero or one variable.
(b) Write a MATLAB code that implements the following steps:
• For each pair (X, Y), add an undirected edge ⟨X, Y⟩if either X ̸⊥Y or if
there is a node Z ̸= X or Y such that X ̸⊥Y|Z.
• For each (X, Y), determine the sets
S(X, Y) = {Z | X ⊥Y|Z}.
• For each pair X, Y such that X ̸∼Y, determine whether there is a node Z
such that X ∼Z and Y ∼Z and Z ̸∈S(X, Y). If there is such a node, then
direct the edges X →Z and Y →Z, so that Z is a collider node.
(c) Does the algorithm SUCCEED or FAIL? If it succeeds, is the graph in Figure 6.7
obtained?

LEARNING THE GRAPH STRUCTURE
195
10. In MATLAB, implement the Minimum Maximum Parent Child and Minimum Max-
imum Hill Climbing algorithms on the ‘Women in Mathematics’ data.
11. In MATLAB, implement a Markov chain Monte Carlo algorithm on the ‘Women in
Mathematics’ data set, where the search space is the space of directed acyclic graphs.


7
Parameters and sensitivity
Having speciﬁed the structure of a Bayesian network and the conditional probability
potentials, the parameters of the network are the conditional probability values (θjil)
kj
i=1
deﬁned by Equation (5.5), or the update by Equation (5.6) if there is a data base of
instantiations. These satisfy the condition 0 ≤θjil ≤1 for all (j, i, l) and the constraints
kj
i=1 θjil = 1 for each (j, l), so that for each j, l variable/parent conﬁguration, there are
kj −1 free parameters.
In many practical applications, the full parameter set is too large for efﬁcient updating.
There are several ways to approach this problem. In Chapter 6, methods to ﬁnd a reduced
DAG (for example, the Chow-Liu tree) that describes the network adequately were dis-
cussed. In some situations there may be modelling constraints that make it inappropriate to
remove any of the directed edges to assist computation. Instead, it may be appropriate to
parametrize the conditional probabilities (θjil)
kj
i=1 by a set of parameters (t(jl)
1
, . . . , t(jl)
mjl )
where mjl ≤kj −1 which model the conditional probability distributions as functions of
these parameters; θjil(t(jl)
1
, . . . , t(jl)
mjl ). There are other situations where it may be appro-
priate to reduce the graph and parametrize the conditional probability potentials for the
reduced graph.
After a suitable parametrization, a set of queries (deﬁned in Section 3.3) may be
processed and the parameters adjusted until the response of the Bayesian network to the
test queries is in line with any constraints that are to be imposed.
Having parametrized the network in a suitable way, sensitivity analysis aims to
describe changes in the network associated with small changes in parameter values.
The sensitivity of the network to the parameters strongly inﬂuences the accuracy and
rates of convergence of numerical methods for estimating probability values associated
with a Bayesian network.
Bayesian Networks: An Introduction
T. Koski, J. Noble
2009 John Wiley & Sons, Ltd

198
PARAMETERS AND SENSITIVITY
7.1
Changing parameters in a network
There are several types of parameter change that can be made. The simplest is where
there is one free parameter under consideration. Alternatively, a single CPP may be
varied. That is, all probabilities in one of the CPPs (conditional probability potentials),
using the notation given by Equation (5.5) may be varied subject to the constraint that
kj
i=1 θjil = 1, while the other CPPs are kept ﬁxed. A query constraint may be satisﬁed
more efﬁciently if several of the CPPs are varied. The problem may be reduced using
proportional scaling techniques, where the entries of a CPP depend linearly on a single
parameter.
As usual, let j denote the parent set for variable j and let π(l)
j
denote conﬁguration
l of the parent set for variable j.
Deﬁnition 7.1 (Proportional Scaling Property) A Bayesian network satisﬁes the propor-
tional scaling property if for each conditional probability distribution θj.l, where θjil =
p(Xj = x(j)
i
|j = π(j)
l
), there is a parameter t(jl) such that
pXj |j (.|π(j)
l
) = (αj1l + βj1lt(jl), . . . , αjkj l + βjkj lt(jl)),
where kj
m=1 αjml = 1 and kj
m=1 βjml = 0.
Theorem 7.1 Let BN be a Bayesian network over a collection of variables U. Let t
be a single parameter and let e be a collection of hard evidence potentials (Deﬁnition
3.1) entered into the BN; p(e) the probability that this evidence is obtained. Assuming
proportional scaling in that single parameter, then
p(e)(t) = αt + β
for two constants α and β.
Proof of Theorem 7.1 Let X = {X1, . . . , Xd}. Let pXj |j (.|π(l)
j ) denote the conditional
probability distribution with parameter t. Let the evidence potentials be e = (e1, . . . , em).
Recall that the (ek)m
k=1 are potentials containing 1s and 0s and recall the notation: for
j = 1, . . . , d, variable Xj takes values in state space Xj = (x(1)
j , . . . x
(kj )
j
) and X takes
values in state space X = X1 × · · · × Xd. Recall the notation
pX;e = pX
m

k=1
ek,
where pX is the joint probability potential of X and multiplication is taken in the sense
of multiplication of potentials (Deﬁnitions 2.23, 2.25 and 2.26). Then
p(e) =

x∈X
pX;e =

x∈X
pXj |j

k̸=j
pXk|k
m

l=1
el.
(7.1)

CHANGING PARAMETERS IN A NETWORK
199
It is clear from the deﬁnition of proportional scaling, and from Equation (7.1), that t
enters linearly. It therefore follows that
p(e)(t) = αt + β.
□
Because the event ‘{A = a}’ can be treated as hard evidence, it follows that p({A = a}, e)
is also a linear function in t, say p({A = a}, e) = γ t + δ. It follows that
p({A = a}|e)(t) = p({A = a}, e)
p(e)
= γ t + δ
αt + β .
Example 7.1
Consider the Bayesian network called Fire.1 The model is shown in
Figure 7.1.
The network models the scenario of whether or not there is a ﬁre in the building. Let
F denote ‘ﬁre’, T denote ‘tampering’, S ‘smoke’, A ‘alarm’, L ‘leaving’ and R ‘report’.
Now consider the following evidence e = {report = true, smoke = false}. That is, the
ﬁre department receives a report that people are evacuating the building, but no smoke is
observed. This evidence should make it more likely that the ﬁre alarm has been tampered
with than that there is a real ﬁre. Let t denote ‘true’ and f denote ‘false’. Suppose that
the CPPs for this network are given by
pF =
t
f
0.01
0.99 ,
pT =
t
f
0.02
0.98 ,
pR|L =
L\R
t
f
t
0.75
0.25
f
0.01
0.99
pS|F =
F\S
t
f
t
0.9
0.1
f
0.01
0.99
,
pL|A =
A\L
t
f
t
0.88
0.12
f
0.001
0.999
Fire
tamper
smoke
alarm
leaving
report
Figure 7.1
The DAG for the Bayesian Network ‘Fire’.
1 This Bayesian network is distributed with the evaluation version of the commercial HUGIN Graphical
User Interface, by HUGIN Expert.

200
PARAMETERS AND SENSITIVITY
pA|F,T (t|., .) =
F\T
t
f
t
0.5
0.99
f
0.85
0.0001.
Here,
pT (t|e) = pT,R,S(t, t, f )
pR,S(t, f )
.
Using the notation XZ to denote the state space of a variable Z,
pT,R,S(t, t, f ) = pT (t)

XL
pR|L(t|.)

XA
pL|A

XF
pA|T,F(.|t, .)pS|F (f |.)pF
and
pR,S(t, f ) =

XT
pT

XL
pR|L(t|.)

XA
pL|A

XF
pA|T,FpS|F (f |.)pF.
Similarly,
pF (t|e) = pF,R,S(t, t, f )
pR,S(t, f )
,
and
pF,R,S(t, t, f ) = pF (t)pS|F (f |t)

XT
pT

XA
pA|T,F(.|., f )

XL
pL|ApR|L(t|.)
The computations are straightforward and give
pT (t|e) = 0.501,
pF (t|e) = 0.0294.
Suppose that it is known from experience that the probability that the alarm has been
tampered with should be no less than 0.65 given this evidence. The network should
therefore be adjusted to accommodate. It is simplest to try changing only one network
parameter. Suppose that the potential pT is to be adjusted. Let θ = pT (t). Let
α =

XL
pR|L(t|.)

XA
pL|A

F
pA|T,F(.|t, .)pS|F (f |.)pF
so that
p({T = t}, e) = pT,R,S(t, t, f ) = θα
and
β =

XL
pR|L(t|.)

XA
pL|A

XF
pA|T,F(.|f, .)pS|F (f |.)p(.),
so that
p(e) = pR,S(t, f ) = θα + (1 −θ)β.
Then α and β may be computed numerically and
pT (t|e) = p({T = t}, e)
p(e)
=
αθ
(α −β)θ + β .

MEASURES OF DIVERGENCE BETWEEN PROBABILITY DISTRIBUTIONS
201
The solution to the equation
αθ
(α −β)θ = 0.65
is θ = 0.0364.
Similarly, let ψ = pR|L(t|t). Keeping all other potentials ﬁxed, pT (t|e) may be com-
puted as a function of ψ and the equation pT (t|e)(ψ) = 0.65 has solution ψ = 0.00471.
For all other single parameter adjustments, the equation does not have a solution
in the interval [0, 1]. Therefore, if only one parameter is to be adjusted, the constraint
p({tampering = t}|e) = 0.65 can be dealt with in either of the following two ways:
1. Increase pT (t) from 0.02 to greater than 0.0364, or
2. Decrease the probability of a false report, given that there is an evacuation, from
0.01 to less than 0.00471.
It turns out for this example that it is not possible to enforce the desired constraint by
adjusting a single parameter in any of the CPPs of the variables ﬁre, smoke, alarm and
leaving.
7.2
Measures of divergence between probability
distributions
In the ‘ﬁre’ example, two suggestions were made for changing the parameters to accom-
modate the constraint. Firstly, it was proposed to raise θ from 0.02 to 0.0364, an increase
of 0.0164, or a factor of 1.82. The other possibility for a single parameter adjustment
was to reduce ψ from 0.01 to 0.00471. That is a difference of 0.00529, or a factor of
0.471 =
1
2.12. Clearly, the magnitude of the adjustment will depend on the way that the
divergence between two probability distributions is computed.
A distance is a more speciﬁc measure of divergence, which satisﬁes the properties
given in the following deﬁnition.
Deﬁnition 7.2 (Distance) A measure of divergence d between probability distributions is
a distance if it satisﬁes the following three properties: for any three probability distribu-
tions p1, p2 and p3 over the same space X = (x1, . . . , xk),
• Positivity: d(p1, p2) ≥0. Furthermore, d(p1, p2) = 0 ⇔p1 ≡p2
• Symmetry: d(p1, p2) = d(p2, p1)
• Triangle Inequality: d(p1, p3) ≤d(p1, p2) + d(p2, p3).
Consider two common measures of divergence between probability distributions. Let p
and q be two probability distributions over the same ﬁnite state space X = (x1, . . . , xk)
(Deﬁnition 1.2) and let pj = p({xj}) and qj = q({xj}) for j = 1, . . . , k. The familiar
quadratic or Euclidean distance is deﬁned as
d2(p, q) =




k

j=1
(pj −qj)2.

202
PARAMETERS AND SENSITIVITY
The Kullback-Leibler divergence between two probability distributions p and q over the
same state space X is deﬁned in Deﬁnition 5.4. In this case, it may be written as
dKL(p|q) =
k

j=1
pj log pj
qj
.
The Kullback-Leibler divergence is not a distance in the sense of Deﬁnition 7.2; it does
not, in general, satisfy dKL(p|q) = dKL(q|p). Let p(1) = (0.02, 0.98), q(1) = (0.0364,
0.9636), p(2) = (0.01, 0.99), q(2) = (0.00471, 0.99529). Then
d2(p(1), q(1)) =
+
(0.02 −0.0364)2 + (0.98 −0.9636)2 = 0.0232
d2(p(2), q(2)) =
+
(0.00471 −0.01)2 + (0.99529 −0.99)2 = 0.00748,
so the change represented by the second adjustment is less than one third of the change
represented by the ﬁrst if the change is measured using the quadratic distance measure.
dKL(p(1)|q(1)) = 0.02 log 0.02
0.0364 + 0.98 log 0.98
0.9636 = 0.004562,
dKL(p(2)|q(2)) = 0.01 log
0.01
0.00471 + 0.99 log
0.99
0.99529 = 0.00225,
so the change represented by the second adjustment is approximately one half of the
change represented by the ﬁrst. Clearly, different distance measures give different impres-
sions of the relative importance of parameter changes. Furthermore, it is important to have
a reference point; for example, if the Kullback-Leibler distance is being used, it is useful
to know what a particular value of the Kullback-Leibler distance means in terms of a
well known family of distributions.
7.3
The Chan-Darwiche distance measure
The problem with both the Kullback-Leibler and the quadratic distance measure is that
they do not emphasize the proportional difference between two probability values when
they are close to zero. In [66], the following distance measure is proposed.
Deﬁnition 7.3 (Chan-Darwiche Distance) Let p and q be two probability functions over
a ﬁnite state space X. That is, p : X →[0, 1] and q : X →[0, 1], 
x∈X p(x) = 1 and

x∈X q(x) = 1. The Chan-Darwiche distance is deﬁned as
DCD(p, q) = log max
x∈X
q(x)
p(x) −log min
x∈X
q(x)
p(x),
where, by deﬁnition, 0
0 = 1 and +∞
+∞= 1. If p and q are two probability distributions
(Deﬁnition 1.2) over a ﬁnite state space X, then the Chan-Darwiche distance is deﬁned
as DCD(p, q), where p and q are taken as the probability functions in Deﬁnition 1.3.
Unlike the Kullback-Leibler divergence, the Chan-Darwiche distance is a distance; it
satisﬁes the three requirements of Deﬁnition 7.2.

THE CHAN-DARWICHE DISTANCE MEASURE
203
The support of a probability function deﬁned on a ﬁnite state space; namely, those
points where it is strictly positive (relating to outcomes that can happen) is important
when comparing two different probability functions over the same state space.
Deﬁnition 7.4 (Support) Let p be a probability function over a countable state space X;
that is, p : X →[0, 1] and 
x∈X p(x) = 1. The support of p is deﬁned as the subset
Sp ⊆X such that
Sp = {x ∈X|p(x) > 0}.
(7.2)
Theorem 7.2 The Chan-Darwiche distance measure is a distance measure, in the sense
that for any three probability functions p1, p2, p3 over a state space X, the following three
properties hold:
• Positivity: DCD(p1, p2) ≥0 and DCD(p1, p2) = 0 ⇔p1 ≡p2.
• Symmetry: DCD(p1, p2) = DCD(p2, p1)
• Triangle Inequality: DCD(p1, p2) + DCD(p2, p3) ≥DCD(p1, p3).
Proof of Theorem 7.2 Positivity and symmetry are clear (Exercise 2). It only remains
to prove the triangle inequality. Since the state space is discrete and ﬁnite, it follows that
there exist y, z ∈X such that
DCD(p1, p3) = log max
x∈X
p3(x)
p1(x) −log min
x∈X
p3(x)
p1(x) = log p3(y)
p1(y) −log p3(z)
p1(z)
= log p3(y)
p2(y) + log p2(y)
p1(y) −log p3(z)
p2(z) −log p2(z)
p1(z)
=

log p3(y)
p2(y) −log p3(z)
p2(z)

+

log p2(y)
p1(y) −log p2(z)
p1(z)

≤

log max
x∈X
p3(x)
p2(x) −log min
x∈X
p3(x)
p2(x)

+

log max
x∈X
p2(x)
p1(x) −log min
x∈X
p2(x)
p1(x)

= DCD(p1, p2) + DCD(p2, p3).
□
This distance is relatively easy to compute. It has the advantage over the Kullback-Leibler
divergence (which is not a true distance measure) that it may be used to obtain bounds
on odds ratios.
Example 7.2: The Chan-Darwiche distance between two multivariate bernoulli
distributions
Consider d independent Bernoulli trials, X = (X1, . . . , Xd), where the
‘success’ probabilities for each trial may differ. The distribution of the random vector X
is known as a multivariate Bernoulli distribution. This example considers the distance
between two multivariate Bernoulli distributions where the ‘success’ probabilities for
the two distributions are given by the vectors p = (p1, . . . , pd) and q = (q1, . . . , qd)
respectively. From this, the Chan-Darwiche distance between two different models
for tossing the thumb-tack (Section 5), and the Chan-Darwiche distance between two
(univariate) Bernoulli distributions may be derived.

204
PARAMETERS AND SENSITIVITY
Let X be the binary hypercube; that is, X = {0, 1}d and let x ∈X denote an element
in X. Then x = (xi)d
i=1, where xi ∈{0, 1}. Let q and p be two multivariate Bernoulli
probability functions over X. That is, q : X →[0, 1] and p : X →[0, 1] are deﬁned such
that each x ∈X,
q

x

=
d
i=1
qxi
i (1 −qi)1−xi
and
p

x

=
d
i=1
pxi
i (1 −pi)1−xi
where, for this example, it is assumed that 0 < qi < 1 and 0 < pi < 1 (i.e. the inequalities
are strict) for all i ∈{1, . . . , d}.
Thus, the likelihood ratio between q and p is well deﬁned and is given by
LR

x

= q

x

p

x
 =
d
i=1
 qi
pi
xi  1 −qi
1 −pi
1−xi
.
(7.3)
For each i ∈{1, . . . , d}, let mi be deﬁned as
mi =

1
if qi
pi ≥1−qi
1−pi
0
otherwise.
(7.4)
Then m = (mi)d
i=1 ∈X and, by construction, it follows from Equation (7.3) that for all
x ∈X,
LR

x

≤LR

m

=
d
i=1
max
 qi
pi
, 1 −qi
1 −pi

.
(7.5)
Next let ¯m be the binary complement of m deﬁned by Equation (7.4). That is, for each
i ∈{1, . . . , d}, ¯mi = 1 −mi, giving ¯mi = 0, if mi = 1 and ¯mi = 1 if mi = 0. Then it
holds that
LR 
x ≥LR 
 ˜m =
d
i=1
min
 qi
pi
, 1 −qi
1 −pi

.
(7.6)
It now follows from the deﬁnition of the Chan-Darwiche distance measure (Deﬁnition
7.3) and Equations (7.5) and (7.6) that
DDC(p, q) = log LR

m

−log LR

˜m

=
d

i=1
log
max

qi
pi , 1−qi
1−pi

min

qi
pi , 1−qi
1−pi
 .
(7.7)

THE CHAN-DARWICHE DISTANCE MEASURE
205
For i such that mi = 1 it clearly holds that
max

qi
pi , 1−qi
1−pi

min

qi
pi , 1−qi
1−pi
 =
qi
pi
1−qi
1−pi
= Oq,i
Op,i
,
where O denotes the odds;
Oq,i =
qi
1 −qi
, Op,i =
pi
1 −pi
.
Similarly for i such that mi = 0 it holds that
max

qi
pi , 1−qi
1−pi

min

qi
pi , 1−qi
1−pi
 = Op,i
Oq,i
,
from which it follows that
DDC(p, q) =
d

i=1
log max
Oq,i
Op,i
, Op,i
Oq,i

.
(7.8)
The expression in Equation (7.8) gives two interesting special cases.
1. Let qi = q and pi = p for all i, and 0 < q < 1 and 0 < p < 1. This corresponds,
for example, to d tosses of a thumb-tack when considering two different probabil-
ities of head for tosses that are conditionally independent given this parameter. In
this case,
q

x

= qk (1 −q)d−k ,
p

x

= pk (1 −p)d−k ,
where k is the number of digital ones in x. By Equation (7.8)
DDC(p, q) = d log max
Oq
Op
, Op
Oq

,
(7.9)
with obvious deﬁnitions of the odds. If, say, Oq
Op > Op
Oq , then
DDC(p, q) = d

log Oq −log Op

,
which is a fairly neat formula.
2. Furthermore, taking d = 1 in the preceding gives the special case
DDC(p, q) = log max
Oq
Op
, Op
Oq

,
(7.10)
This is the Chan-Darwiche distance between the Bernoulli distributions Be (q) and
Be (p).
□

206
PARAMETERS AND SENSITIVITY
Example 7.3: Chan-Darwiche distance for a fork with binary variables
Let
(X, Y, Z) be three binary variables, each with state space {0, 1}, that satisfy X ⊥Y|Z,
so that their probability distribution may be factorized along a fork:
pX,Y,Z = pZpX|ZpY|Z.
(7.11)
Five parameters are required to specify the joint distribution:
θX|1 = pX|Z(1|1), θX|0 = pX|Z(1|0), θY|1 = pY|Z(1|1), θY|0 = pY|Z(1|0), θZ = pZ(1).
Assume that all of these probabilities lie strictly between zero and one. Then the factor-
ization in Equation (7.11) may be written
pX,Y,Z (x, y, z) = pa · pb,
where
pa =
,
θx
X|1(1 −θX|1)1−x 
θy
Y|1(1 −θY|1)1−y
θZ
-z
∀(x, y, z) ∈{0, 1}3
and
pb =
,
θx
X|0(1 −θX|0)1−x 
θy
Y|0(1 −θY|0)1−y
(1 −θZ)
-1−z
∀(x, y, z) ∈{0, 1}3.
Let qX,Y,Z be another joint probability function where X ⊥Y|Z, that may be factorized
qX,Y,Z = qZqX|ZqY|Z. Then qX,Y,Z may be written as
qX,Y,Z (x, y, z) = qa · qb,
where, analogously
qa =
,
ψx
X|1(1 −ψX|1)1−x 
ψy
Y|1(1 −ψY|1)1−y
ψZ
-z
∀(x, y, z) ∈{0, 1}3
and
qb =
,
ψx
X|0(1 −ψX|0)1−x 
ψy
Y|0(1 −ψY|0)1−y
(1 −ψZ)
-1−z
.
Assume that all these probabilities lie strictly between zero and one, so that p and q have
the same support. Then
LR (x, y, z) = qa · qb
pa · pb
(7.12)
For i = 0, 1, set
OX|Z=i
q
=
ψX|i
1 −ψX|i
,
OX|Z=i
p
=
θX|i
1 −θX|i
and
OY|Z=i
q
=
ψY|i
1 −ψY|i
,
OY|Z=i
p
=
θY|i
1 −θY|i
.

THE CHAN-DARWICHE DISTANCE MEASURE
207
Then, following the same procedure as in the example above,
LR (x, y, z) ≤Az · B1−z ≤max (A, B) .
(7.13)
where
A = max

OX|Z=1
q
OX|Z=1
p
, OX|Z=1
p
OX|Z=1
q

· max

OY|Z=1
q
OY|Z=1
p
, OY|Z=1
p
OY|Z=1
q

· ψZ
θZ
B = max

OX|Z=0
q
OX|Z=0
p
, OX|Z=0
p
OX|Z=0
q

· max

OY|Z=0
q
OY|Z=0
p
, OY|Z=0
p
OY|Z=0
q

·
1 −ψZ
1 −θZ

.
Note that these inequalities determine one (or several) of the eight conﬁgurations (x, y, z)
to maximize LR (x, y, z). Similarly, using the method of the example above,
LR (x, y, z) ≥Cz · D1−z ≥min (C, D) ,
(7.14)
where
C = min

OX|Z=1
q
OX|Z=1
p
, OX|Z=1
p
OX|Z=1
q

· min

OY|Z=1
q
OY|Z=1
p
, OY|Z=1
p
OY|Z=1
q

· ψZ
θZ
,
D = min

OX|Z=0
q
OX|Z=0
p
, OX|Z=0
p
OX|Z=0
q

· min

OY|Z=0
q
OY|Z=0
p
, OY|Z=0
p
OY|Z=0
q

·
1 −ψZ
1 −θZ

.
In other words, the Chan-Darwiche distance between the two distributions factorized
along the fork is
DDC(p, q) = log max (A, B) −log min (C, D) .
□
Theorem 7.3 Let p and q be two probability distributions (Deﬁnition 1.2) over the same
ﬁnite state space X and let A and B be two subsets of X. Let Ac = X\A and Bc = X\B.
Let Op(A|B) = p(A|B)
p(Ac|B) and Oq(A|B) = q(A|B)
q(Ac|B). Then
e−DCD(p,q) ≤Oq(A|B)
Op(A|B) ≤eDCD(p,q).
The bound is sharp in the sense that for any pair of distributions (p, q) there are subsets
A and B of X such that
Oq(A|B)
Op(A|B) = exp{DCD(p, q)},
Oq(Ac|B)
Op(Ac|B) = exp{−DCD(p, q)}.
Proof of Theorem 7.3 Recall Deﬁnitions 1.2 and 1.3. Without loss of generality, it
may be assumed that p and q have the same support; that is, p(x) > 0 ⇔q(x) > 0.
Otherwise DCD(p, q) = +∞and the statement is trivially true; for any A, B ⊆X, 0 ≤

208
PARAMETERS AND SENSITIVITY
Oq(A|B)
Op(A|B) ≤+ ∞. For p and q such that p and q have the same support, let r(x) = q(x)
p(x).
For any two subsets A, B ⊆X,
Oq(A|B)
Op(A|B) =
q(A|B)
1 −q(A|B)
1 −p(A|B)
p(A|B)
= q(AB)
q(AcB)
p(AcB)
p(AB) =

x∈AB q(x)

x∈AcB q(x)

x∈AcB p(x)

x∈AB p(x)
=

x∈AB r(x)p(x)

x∈AcB r(x)p(x)

x∈AcB p(x)

x∈AB p(x) ≤maxz∈X r(z) 
x∈AB p(x)
minz∈X r(z) 
x∈AcB p(x)

x∈AcB p(x)

x∈AB p(x)
= maxz∈X r(z)
minz∈X r(z) .
Similarly,
Oq(A|B)
Op(A|B) ≥minz∈X r(z)
maxz∈X r(z).
From the deﬁnition of DCD(p, q), it follows directly that
eDCD(p,q) = maxz∈X r(z)
minz∈X r(z) ,
hence
e−DCD(p,q) ≤Oq(A|B)
Op(A|B) ≤eDCD(p,q),
as required.
□
To prove that the bound is tight, consider x such that r(x) = maxz∈X r(z) and y such
that r(y) = minz∈X r(z). Set A = {x} and B = {x, y}. Then
Oq(A|B) = r(x)p(x)
r(y)p(y).
Since Op(A|B) = p(x)
p(y) and eDCD(p,q) = maxz∈X r(z)
minz∈X r(z) , it follows that
Oq(A|B)
Op(A|B) = eDCD(p,q).
Similarly, let C = {y}, then
Oq(C|B)
Op(C|B) = e−DCD(p,q).
□
Theorem 7.3 may be used to obtain bounds on arbitrary queries q(A|B) for the measure
q in terms of p(A|B).
Corollary 7.1 Set d = DCD(p, q), then
p(A|B)e−d
1 + (e−d −1)p(A|B) ≤q(A|B) ≤
p(A|B)ed
1 + (ed −1)p(A|B).
(7.15)
Proof of Corrollary 7.1 This is straightforward and the details are left to the
reader.
□

THE CHAN-DARWICHE DISTANCE MEASURE
209
7.3.1
Comparison with the Kullback-Leibler divergence
and euclidean distance
For binary variables, divergences are entirely characterized by the Kullback-Leibler mea-
sure. Consider two probability distributions p and q over {1, 2, 3} deﬁned by
p(1) = a,
p(2) = b −a,
p(3) = 1 −b
q(1) = ka,
q(2) = b −ka,
q(3) = 1 −b
Then
DKL(p|q) = a log 1
k + (b −a) log b −a
b −ka = −a log k −(b −a) log b −ka
b −a .
Consider the events A = {1}, B = {1, 2}, then Op(A|B) =
a
b−a and Oq(A|B) =
ka
b−ka
and the odds ratio is given by
Oq(A|B)
Op(A|B) = k(b −a)
b −ka .
As a →0, DKL(p|q) →0, while
Oq(A|B)
Op(A|B) →k. It is therefore not possible to ﬁnd a
bound on the odds ratio in terms of the Kullback-Leibler divergence.
Similarly, in this example, the Euclidean distance is
d2(p, q) =
√
2a(1 −k)
a→0
−→0.
Neither the Kullback-Leibler divergence nor the Euclidean distance can be used to provide
uniform bounds on the odds ratios; even if there is a large relative difference between
pairs of probability values for p and q, they will be ignored if the absolute values of
these probabilities are small.
The Chan-Darwiche distance in terms of the bayes factor
The Chan-Darwiche dis-
tance may be interpreted in terms of the Bayes factor, given in Deﬁnition 1.6. Recall that
for two probability distributions p and q over a state space X and two events A, B ⊆X,
the Bayes factor is deﬁned as
Fq,p(A; B) := q(A)/q(B)
p(A)/p(B).
(7.16)
Recall the notation given in Deﬁnition 1.3. Note that
DCD(p, q) = log max
x,y∈X Fp,q({x}, {y}).
Theorem 7.4 Let p and q be two probability distributions over the same state space X.
Let A and B be two events, then
e−DCD(p,q) ≤Fq,p(A, B) ≤eDCD(p,q).

210
PARAMETERS AND SENSITIVITY
Proof of Theorem 7.4 This is similar to the proof of Theorem 7.3. Note that
Fq,p(A, B) = q(A)
q(B)
p(B)
p(A) =

x∈A q(x)

x∈B q(x)

x∈B p(x)

x∈A p(x)
=

x∈A r(x)p(x)

x∈B r(x)p(x)

x∈B p(x)

x∈A p(x) ≤maxz∈X r(z) 
x∈A p(x)
minz∈X r(z) 
x∈B p(x)

x∈B p(x)

x∈A p(x)
= maxz∈X r(z)
minz∈X r(z) .
Similarly, F(A, B) ≥minz∈X r(z)
maxz∈X r(z) and the result follows.
□
7.3.2
Global bounds for queries
A key issue is to ﬁnd bounds on the global effect of changing a parameter θjil; namely,
bounds on an arbitrary query p(A|B) where A and B are two events. The Chan-Darwiche
distance satisﬁes the following important property.
Theorem 7.5 Consider a Bayesian network, with probability distribution p. Suppose that
the DAG remains the same and the only change to the CPPs is that θj.l is changed to
˜θj.l for variable j, parent conﬁguration π(l)
j , resulting in a new probability distribution q.
Then
dCD(p, q) = dCD(θj.l, ˜θj.l).
Proof of Theorem 7.5 This is straightforward from the construction.
□
Corollary 7.2 Consider a DAG G and consider two probability distributions p and q
which factorize according to G. Suppose that q is obtained from p by changing θj.l to ˜θj.l
for one ﬁxed (j, l), all other conditional probabilities remaining the same. Let π(l)
j
denote
parent conﬁguration l for variable j and suppose that p({j = π(l)
j }) > 0. Then for any
two subsets A and B of X,
e−dCD(θj.l, ˜θj.l) ≤Oq(A|B)
Op(A|B) ≤e−dCD(θj.l, ˜θj.l).
Proof of Corollary 7.2 This follows directly from Theorem 7.5 and Theorem 7.3.
□
One feature of the Chan-Darwiche distance measure is that it can be used to bound odds
ratios. Another important feature, following from Theorem 7.5 and Corollary 7.2 is that
if the probability measure is changed locally, only local computations are required to
obtain the Chan-Darwiche measure and to use this to bound changes in the values of
queries. This is not the case with the Kullback-Leibler divergence. If the same change is
applied to p to obtain q, then
dKL(p, q) = p({j = π(l)
j })dKL(θj.l, ˜θj.l).
This follows almost directly from the deﬁnition and is left as an exercise (Exercise 1).

THE CHAN-DARWICHE DISTANCE MEASURE
211
The optimality of proportional scaling
Consider one of the conditional probability
distributions (θj1l, . . . , θjkj l) and suppose that θj1l is to be altered to a different value,
denoted by ˜θjl1. Under proportional scaling, the probabilities of the other states are
given by
˜θjil = 1 −˜θj1l
1 −θj1l
θjil.
Proportional scaling turns out to be optimal under the Chan-Darwiche distance measure.
Theorem 7.6 Consider a probability distribution p factorized according to a DAG G.
Suppose the value θj1l is changed to ˜θj1l. Among the class of probability distributions Q
factorized along G with q({Xj = x(l)
j }|{j = π(l)
j }) = ˜θj1l, minq∈Q DCD(p, q) is obtained
for q such that ˜θa.b = θa.b for all (a, b) ̸= (j, l) and
˜θjil = 1 −˜θj1l
1 −θj1l
θjil.
Under proportional scaling, the Chan-Darwiche distance is then given by
DCD(p, q) = | log ˜θj1l −log θj1l| + | log(1 −˜θj1l) −log(1 −θj1l)|.
Proof of Theorem 7.6 Let p be a distribution that factorizes along a DAG G, with
conditional probabilities θaib = p(Xa = x(a)
i
|a = π(a)
b ). Let q denote the distribution
that factorizes along G, with conditional probabilities
˜θaib = q({Xa = x(i)
a }|{a = π(b)
a }),
where ˜θj1l is given,
˜θaib = θaib
(a, b) ̸= (j, l)
and
˜θjil = 1 −˜θj1l
1 −θj1l
θjil
i = 2, . . . , kj.
This is the distribution generated by the proportional scheme. Let r denote any other
distribution that factorizes along G with r({Xj = x(1)
j }|{j = π(b)
j }) = ˜θj1b. The aim is
to prove that DCD(p, r) ≥DCD(p, q).
If θj1l = 1 and ˜θj1l < 1, then there is a ˜θjkl > 0 with θjkl = 0 and it follows that
DCD(p, q) = DCD(p, r) = +∞.
If θj1l = 0 and ˜θj1l > 0, then it follows directly that DCD(p, q) = DCD(p, r) = +∞.
Consider 0 < θj1l < 1. Firstly, consider ˜θj1l > θj1l. Then
max
x∈X
q(x)
p(x) = max
 ˜θj1l
θj1l
, 1 −˜θj1l
1 −θj1l

=
˜θj1l
θj1l
and
min
x∈X
q(x)
p(x) = 1 −˜θj1l
1 −θj1l
.

212
PARAMETERS AND SENSITIVITY
Similarly, if ˜θj1l < θj1l, then maxx∈X
q(x)
p(x) =
1−˜θj1l
1−θj1l and minx∈X
q(x)
p(x) =
˜θj1l
θj1l , so
DCD(p, q) = | log ˜θj1l −log θj1l| + | log(1 −˜θj1l) −log(1 −θj1l)|.
Now consider any other distribution r with r({Xj = x(1)
j }|{j = π(b)
j }) = ˜θj1b. It is clear,
from the factorization along the DAG that if ˜θj1l ≥θj1l then maxx∈X
r(x)
p(x) ≥
˜θj1l
θj1l and
minx∈X
r(x)
p(x) ≤
1−˜θj1l
1−θj1l ; if ˜θj1l ≤θj1l then maxx∈X
r(x)
p(x) ≥
1−˜θj1l
1−θj1l and minx∈X
r(x)
p(x) ≤
˜θj1l
θj1l . In
all cases
DCD(p, q) ≤DCD(p, r).
□
7.3.3
Applications to updating
Sections 1.4.2 and 1.4.3 considered the problem of incorporating soft evidence and
proposed two methods, depending on the form in which the soft evidence was given:
‘Jeffrey’s rule’ and ‘Pearl’s method of virtual evidence’ respectively. The purpose of this
section is to obtain bounds on the distance between the original and updated measures
when these methods are applied.
Jeffrey’s rule
Recall Jeffrey’s rule, discussed in Section 3.2.1. Let p denote a probabil-
ity distribution over a countable state space X and let q denote the distribution obtained
by updating according to Jeffrey’s rule. Then Theorem 7.3 may be applied to give the
following bound.
Theorem 7.7 Let p be a probability distribution over a countable state space X and let
G1, . . . , Gn be a collection of mutually exclusive and exhaustive events. Let λj = p(Gj)
for j = 1, . . . , n. Let q denote the probability distribution such that q(Gj) = µj for j =
1, . . . , n and such that for any other event A,
q(A) =
n

j=1
µjp(A|Gj).
In other words, q is the Jeffrey’s update of p, deﬁned by q(Gj) = µj, j = 1, . . . , n. Then
dCD(p, q) = log max
j
λj
µj
−log min
j
µj
λj
.
□
Proof of Theorem 7.7 This is straightforward and the details are left to the reader (see
Exercise 3, Chapter 7).
□
This immediately gives the following bound.
Corollary 7.3 Let Op and Oq denote the odds function before and after applying Jeffrey’s
rule. Let
d = log max
j
λj
µj
−log min
j
µj
λj
.

THE CHAN-DARWICHE DISTANCE MEASURE
213
Then for any two events A and B,
e−d ≤Oq(A|B)
Op(A|B) ≤ed.
Under the Chan-Darwiche distance measure, Jeffrey’s rule may be considered optimal,
in the following sense.
Theorem 7.8 Let p denote a probability distribution over X and let G1, . . . , Gr denote a
collection of mutually exclusive and exhaustive events. Let µj = p(Gj), let λ1, . . . , λr be
a collection of non-negative numbers such that r
j=1 λj = 1 and let q be the probability
distribution over X deﬁned by
q(x) =
r

j=1
λj
µj
p(x)1Gj (x),
x ∈X,
where
1Gj (x) =

1
x ∈Gj
0
x ̸∈Gj.
Then dCD(p, q) minimizes dCD(p, r) subject to the constraint that r is a probability dis-
tribution over X such that r(Gi) = λi for i = 1, . . . , r.
Proof of Theorem 7.8 Let q denote the distribution generated by Jeffrey’s rule and
let r be any distribution that satisﬁes the constraint r(Gj) = q(Gj) = λj, j = 1, . . . , r.
If p and r do not have the same support (Deﬁnition 7.4), then +∞= DCD(p, r) ≥
DCD(p, q). If they have the same support, let j denote the value such that λj
µj = maxi
λi
µi
and let k denote the value such that λk
µk = mini
λi
µi . Let α = maxx∈X
r(x)
p(x). Then
αµj = α

x∈Gj
p(x) ≥

x∈Gj
r(x)
p(x)p(x) = r(Gj) = λj,
so that
α ≥λj
µj
.
Similarly, set β = minx∈X
r(x)
p(x), then a similar argument gives β ≤λk
µk . It follows that the
distance between p and r is
DCD(p, r) = log max
x∈X
r(x)
p(x) −log min
x∈X
r(x)
p(x) = log α −log β
≥log λj
µj
−log λj
µj
= log max
i
λi
µi
−log min
i
λi
µi
= DCD(p, q).
Therefore q gives the smallest distance.
□

214
PARAMETERS AND SENSITIVITY
Example 7.4
Consider Example 3.4, Section 3.2.1. It is taken from [65] and discussed
in [66]. The probability may be updated using Jeffrey’s rule to give, for example,
qS,C(s, cg) = λg
µg
pS,C(s, cg) = 0.7
0.3 × 0.12 = 0.28.
Updating the whole distribution in this way gives
qS,C =
S\C
cg
cb
cv
s
0.28
0.10
0.04
sc
0.42
0.15
0.01
.
Theorem 7.7 gives
dCD(p, q) = log max
i
λi
µi
−log min
i
λi
µi
= log 0.7
0.3 −log 0.05
0.4 = 2.93,
while Corollary 7.3 gives
0.05 ≤Oq(cg|s)
Op(cg|s) ≤18.73.
This suggests that the distributions have changed dramatically. Note that pC|S(cg|s) =
0.12
0.56 = 0.214, while qC|S(cg|s) = 0.28
0.42 = 0.667. The change of probability has led to a
dramatic change and
Oq(cg|s)
Op(cg|s) = 0.667/0.333
0.214/0.786 = 7.34.
If
the
new
distribution
over
colour
were
q∗
C = (0.25, 0.25, 0.50)
instead,
then
dCD(p, q∗) = 0.406 and
0.666 ≤Oq∗(cg|s)
Op(cg|s) ≤1.5.
The evidence is weaker and the bounds are therefore tighter.
□
Now consider the following problem: the probability that the piece of cloth is green,
given that it is sold tomorrow is, before updating, 0.214. What evidence would satisfy
the constraint that the updated probability that the cloth is green, given that it is sold
tomorrow, does not exceed 0.3?
By Corollary 7.1,
0.214e−d
1 + (e−d −1) × 0.214 ≤q(cg|s) ≤
0.214ed
1 + (ed −1) × 0.214.
The constraint qC|S(cg|s) ≤0.3 is satisﬁed if
0.214ed
1 + (ed −1) × 0.214 ≤0.3
giving d ≤0.454. The current distribution over colour is (µg, µb, µv) = (0.3, 0.3, 0.4).
The problem now reduces to ﬁnding (λg, λb, λv) such that qC|S(cg|s) = 0.3 and
log max
 λg
0.3, λb
0.3, λv
0.4

−log min
 λg
0.3, λb
0.3, λv
0.4

= 0.454.

THE CHAN-DARWICHE DISTANCE MEASURE
215
Since
qC,S(cj, s) = λj
µj
pC,S(cj, s),
j = g, b, v
it follows that
qC|S(cg|s) =
0.4λg
0.4λg + 0.4λb + 0.4λv
.
With pC|S(cg|s) = 0.3,
0.28λg −0.12λb −0.24λv = 0,
with constraint λg + λb + λv = 1, so that 10λg −3λv = 3.
If the maximum and minimum are then given by λg and λv respectively, then
0.454 = log λg
0.3 −log 10λg −3
1.2
giving
λg =
3e0.454
10e0.454 −4 = 0.402
λv = 0.34,
λb = 0.258.
Finally, to check that the solution is valid, 0.4
λv = 0.4
0.34 = 1.176 > 1.163 =
0.3
0.258 = 0.3
λb . Sim-
ilarly, λg
0.3 clearly gives the maximum in the ﬁrst term.
□
Pearl’s method of virtual evidence
Recall Pearl’s method of virtual evidence, dis-
cussed in Section 1.4.3 and developed in Section 3.2.2.
Again, dCD(p, q) may be computed using only local information.
Theorem 7.9 Let p be a probability distribution over a countable state space X and let
λ1 = 1 and λ2, . . . , λr positive numbers. Let q denote a set function, deﬁned over subsets
of X, such that q(φ) = 0 (where φ denotes the empty set) and for any ﬁnite collection of
disjoint subsets A1, . . . , An ⊂X, q(∪n
j=1Aj) = n
j=1 q(Aj), which satisﬁes
q(x) = p(x)
r

j=1
λj
r
k=1 p(Gk)λk
1Gj (x),
x ∈X.
Then q is a probability distribution over X and
dCD(p, q) = log max
i
λi −log min
i
λi.
Proof of Theorem 7.9 Firstly, it is clear from the construction that 
x∈X q(x) = 1
and that q(x) ≥0 for all x ∈X and (from the deﬁnition) that q satisﬁes the necessary
additivity properties, so that q is a probability. From the deﬁnition,
q(x)
p(x) =

j
λj

k p(Gk)λk
1Gj (x)
x ∈X.

216
PARAMETERS AND SENSITIVITY
It follows that
DCD(p, q) = log max
x∈X
q(x)
p(x) −log min
x∈X
q(x)
p(x)
= log max
j
λj

k p(Gk)λk
−log min
j
λj

k p(Gk)
= log max
j
λj −log min
j
λj
as required.
□
This immediately gives the following bound.
Corollary 7.4 Let Oq and Op denote the odds functions associated with the probability
measures deﬁned in Theorem 7.9 and let
d = DCD(p, q) = log max
i
λi −log min
i
λi.
Then for any events A, B ⊆X,
e−d ≤Oq(A|B)
Op(A|B) ≤ed.
Example 7.5
‘Burglary’ (Example 3.5, Section 3.2.2) may be developed to illustrate
these results. The discussion follows [66]. Let A denote the event that the alarm goes off,
B the event that a burglary takes place and let E denote the evidence of the telephone
call from Jemima. According to Pearl’s method, this evidence can be interpreted as
λ = p(E|A)
p(E|Ac) = 4.
Therefore, the distance between the original distribution p and the update q(.) = p(.|E)
derived according to Pearl’s method is DCD(p, q) = log 4 ≃1.386. This distance may
be used to bound q(B), the probability of a burglary, after the update to incorporate the
evidence. Using Equation (7.15),
p(B)e−d
1 + (e−d −1)p(B) ≤q(B) ≤
p(B)ed
1 + (ed −1)p(B),
so that 2.50 × 10−5 ≤q(B) ≤4.00 × 10−4. An application of Pearl’s virtual evidence
rule gives q(B) = 3.85 × 10−4.
□
7.4
Parameter changes to satisfy query constraints
The problem considered in this section is to decide whether an individual parameter is
relevant to a given query constraint and, if it is, to compute the minimum amount of
change needed to that parameter to enforce the constraint. The constraints considered are
of the following form: Let e denote an instantiation of a collection of variables E (so
that {E = e} is a piece of hard evidence, and may be expressed as e = (e1, . . . , em) as
in Deﬁnition 3.1) and let Y, Z denote two random variables such that Y ̸∈E and Z ̸∈E.

PARAMETER CHANGES TO SATISFY QUERY CONSTRAINTS
217
The query constraints considered are of the following type:
pY|E(y|e) −pZ|E(z|e) ≥ϵ,
(7.17)
pY|E(y|e)
pZ|E(z|e) ≥ϵ.
(7.18)
The notation will be abbreviated by writing: p(y|e) when the abbreviation is clear from
the context.
Let pX denote the probability function for a collection of variables X = (X1, . . . , Xd),
which may be factorized along a graph G = (V, E) (where V = {X1, . . . , Xd}), with
given conditional probability potentials, θjil = pXj |j (x(i)
j |π(l)
j ). Then
pX(x) =
d
j=1
qj

l=1
kj

i=1
θ
n(x(j)
i
|π(j)
l
)
jil
,
where n(x(j)
i
|π(j)
l
) = 1 if the child parent conﬁguration appears in x and 0 otherwise. Sup-
pose that the probabilities (θj1l, . . . , θj,kj ,l) are parametrized by (t(jl)
1
, . . . , t(jl)
mj ), where
mj ≤kj −1. The following result holds.
Theorem 7.10 Let X = (X1, . . . , Xd) denote a set of variables and let p be a probability
distribution that factorizes along a DAG G with node set V = {X1, . . . , Xd}. with corre-
sponding probability potentials θjil = pXj |j (x(i)
j |π(l)
j ). Suppose that for each (j, l) the
probabilities (θj1l, . . . , θj,kj ,l) are parametrized by (t(jl)
1
, . . . , t(jl)
mjl ) where mjl ≤kj −1.
Let E = (Xe1, . . . Xem) denote a subset of X and let e = (x(i1)
e1 , . . . , x(im)
(em)) denote an
instantiation of E. Then for all 1 ≤k ≤mjl,
∂
∂t(jl)
k
pE(e) =
kj

α=1
p({E = e}, {Xj = x(α)
j }, {j = π(l)
j })
θjαl
∂
∂t(jl)
k
θjαl.
Proof of Theorem 7.10 Firstly,
pE(e) =

il
pE|Xj ,j (e|x(i)
j , π(l)
j )pXj |j (x(i)
j |π(l)
j )pj (π(l)
j )
=

il
pE|Xj ,j (e|x(i)
j , π(l)
j )θjilpj (π(l)
j ).
It follows that
∂
∂t(jl)
k
pE(e) =
kj

i=1
pE|Xj ,j (e|x(i)
j , π(l)
j )pj (π(l)
j ) ∂θjil
∂t(jl)
k
=
kj

i=1
pXj ,j |E(x(i)
j , π(l)
j |e)pE(e)pj (π(l)
j )
pXj ,j (x(i)
j , π(l)
j )
∂θjil
∂t(jl)
k

218
PARAMETERS AND SENSITIVITY
=
kj

i=1
pXj ,j ,E(x(j)
i
, π(j)
l
, e)
pXj |j (x(i)
j |π(l)
j )
∂θjil
∂t(jl)
k
=
kj

i=1
pXj ,j ,E(x(i)
j , π(l)
j , e)
θjil
∂θjil
∂t(jl)
k
as required.
□
Proportional scaling
Again, the complete set of variables is X = (X1, . . . , Xd), with
a joint probability distribution p that may be factorized along a directed acyclic graph G.
Evidence is received on a subset of the variables E = (Xe1, . . . , Xem). Consider a pro-
portional scaling scheme, where each conditional probability distribution (θj1l, . . . , θjkj l)
has exactly one parameter. Under proportional scaling, this may be represented as θj1l =
t(jl) and there are non-negative numbers a(jl)
2
, . . . , a(jl)
kj
satisfying kj
α=2 a(jl)
α
= 1, such
that
θj1l = t(jl)
θjαl = a(jl)
α
(1 −t(jl)),
α = 2, . . . , kj.
Then, an application of Theorem 7.10 in the simpliﬁed setting of proportional scaling
immediately gives
∂
∂t(jl) pE(e) =
pE,Xj ,j (e, x(1)
j , π(l)
j )
θj1l
−
kj

α=2
pE,Xj ,j (e, x(α)
j , π(l)
j )
θjαl
a(jl)
α
.
(7.19)
When a proportional scaling scheme is used, Theorem 7.1 gives
pE(e) = α + βt(jl),
where α and β do not depend on t(jl). It follows that for any t(jl),
∂
∂t(jl) pE(e) = β, where
β is constant (i.e. it does not depend on t(jl)). This observation makes it straightforward,
under proportional scaling, to ﬁnd the necessary change in a single parameter t(jl) (if
such a parameter change is possible) to enforce a query constraint.
7.4.1
Binary variables
Assume that variable Xj is binary, with pXj |j (x(1)
j |π(l)
j ) = t(jl) and pXj |j (x(0)
j |π(l)
j ) =
1 −t(jl). Then Equation (7.19) reduces to:
∂
∂t(jl) pE(e) =
pE,Xj ,j (e, x(1)
j , π(l)
j )
t(jl)
−
pE,Xj ,j (e, x(0)
j , π(l)
j )
1 −t(jl)
.
(7.20)

PARAMETER CHANGES TO SATISFY QUERY CONSTRAINTS
219
The statement Y = y, E = e may be treated as hard evidence. By Theorem 7.1, it follows
that there are real numbers λ, λy and λz such that
λ =
∂
∂t(jl) pE(e) =
pE,Xj ,j (e, x(1)
j , π(l)
j )
t(jl)
−
pE,Xj ,j (e, x(0)
j , π(l)
j )
1 −t(jl)
,
λy =
∂
∂t(jl) pY,E(y, e) =
pY,E,Xj ,j (y, e, x(1)
j , π(l)
j )
t(jl)
−
pY,E,Xj (y, e, x(0)
j , π(l)
j )
1 −t(jl)
and
λz =
∂
∂t(jl) pZ,E(z, e) =
pZ,E,Xj ,j (z, e, x(1)
j , π(l)
j )
t(jl)
−
pZ,E,Xj ,j (z, e, x(0)
j , π(l)
j )
1 −t(jl)
.
The following is a corollary of Theorem 7.10, which reduces to Equation (7.20) for the
binary case.
Corollary 7.5 To satisfy the constraint given by Equation (7.17), the parameter t(jl) has
to be changed to t(jl) + δ,where δ satisﬁes
pY,E(y, e) −pZ,E(z, e) −ϵpE(e) ≥δ(−λy + λz + ϵλ).
(7.21)
To satisfy the constraint given by Equation (7.18), the parameter t(jl) has to be changed
to t(jl) + δ, where
pY,E(y, e) −ϵpZ,E(z, e) ≥δ(−λy + ϵλz).
(7.22)
Proof of Corollary 7.5 Since pY|E(y|e) =
pY,E(y,e)
pE(e) , it follows that pY|E(y|e) −
pZ|E(z|e) ≥ϵ is equivalent to pY,E(y, e) −pZ,E(z, e) ≥ϵpE(e). A change in the con-
straint changes pY,E(y, e), pZ,E(z, e) and pE(e) to pY,E(y, e) + δλy, pZ,E(z, e) + δλz
and pE(e) + δλ respectively. To enforce the difference constraint, it follows that δ
satisﬁes
(pY,E(y, e) + λyδ) −(pZ,E(z, e) + λzδ) ≥ϵ(pE(e) + λδ).
Equation (7.21) follows directly.
Similarly, to enforce the ratio constraint, the following inequality is required:
pY,E(y, e) + λyδ
pZ,E(z, e) + λzδ ≥ϵ.
Equation (7.22) now follows directly and the proof is complete.
□
Proportional scaling
If proportional scaling is being used, so that Equation (7.19)
holds, then it is clear that Corollary 7.5 can be extended, with minor adjustment to the
set of linear equations, to satisfy the constraints given by Equations (7.17) and (7.18).

220
PARAMETERS AND SENSITIVITY
7.5
The sensitivity of queries to parameter changes
In line with the Chan-Darwiche distance measure, the following deﬁnition will be used
for ‘sensitivity’.
Deﬁnition 7.5 (Sensitivity) Let p denote a parametrized family of probability distribu-
tions, over a ﬁnite, discrete state space X, parametrized by k parameters (θ1, . . . , θk) ∈˜,
where ˜ ⊆Rk denotes the parameter space. Let p(θ1,...,θk)(.) denote the probability distri-
bution when the parameters are ﬁxed at θ1, . . . , θk. Then the sensitivity of p to parameter
θj is deﬁned as
Sj(p)(θ1, . . . , θk) = max
x∈X
∂
∂θj
log p(θ1,...,θk)({x}) −min
x∈X
∂
∂θj
log p(θ1,...,θk)({x}).
Example 7.6
If p is a family of binary variables, with state space X = {x0, x1} and a
single parameter θ, then
S(p)(θ) =
....
∂
∂θ log p(θ)({x1})
p(θ)({x0})
.... .
□
This section restricts attention to a single parameter model. Consider a network with d
variables, X = (X1, . . . , Xd) where one particular variable Xj is a binary variable. The
other variables may be multivalued. Let
t(jl) = pXj |j (x(1)
j |π(l)
j ).
Let Y denote a collection of variables, taken from (X1, . . . , Xn) and let Y = y denote
an instantiation of these variables. Let y denote the event {Y = y} and let yc denote
the event {Y ̸= y}. Similarly, let e denote the event {E = e}, where E is a different
sub-collection of variables from X. From Deﬁnition 7.5, the sensitivity of a query p(y|e)
to the parameter t(jl) is deﬁned as
.....
∂
∂t(jl) log
p(y|e)
p(yc|e)
..... .
The following theorem provides a simple bound on the derivative in terms of p(y|e) and
t(jl) only.
Theorem 7.11 Suppose Xj is a binary variable taking values x(1)
j
or x(0)
j . Set
t(jl) = pXj |j (x(1)
j |π(l)
j ).
Then
....
∂
∂t(jl) p(y|e)
.... ≤
p(y|e)(1 −p(y|e))
t(jl)(1 −t(jl))
.
(7.23)
The example given after the proof shows that this bound is sharp; there are situations
where the derivative assumes the bound exactly.

THE SENSITIVITY OF QUERIES TO PARAMETER CHANGES
221
Proof of Theorem 7.11 Firstly, p(y|e) =
p(y,e)
p(e) , so that
∂
∂t(jl) p(y|e) =
1
p(e)
∂
∂t(jl) p(y, e) −
p(y|e)
p(e)
∂
∂t(jl) p(e).
Using this, Equation (7.20) gives
∂
∂t(jl) p(y|e)
=
/
(1 −t(jl))p(y, x(1)
j , π(l)
j |e) −t(jl)p(y, x(0)
j , π(l)
j |e)
0
t(jl)(1 −t(jl))
(7.24)
−
/
(1 −t(jl))p(y|e)p({Xj = x(1)
j }, {j = π(l)
j }|e) −t(jl)p(y|e)p({Xj = x(0)
j }, {j = π(l)
j }|e)
0
t(jl)(t −t(jl))
=
(1 −t(jl))(p(y, {Xj = x(1)
j }, {j = π(l)
j }|e) −p(y|e)p({Xj = x(1)
j }, {j = π(l)
j }|e))
t(jl)(1 −t(jl))
−
t(jl)(p(y, {Xj = x(0)
j }, {j = π(l)
j }|e) −p(y|e)p({Xj = x(0)
j }, {j = π(l)
j }|e))
t(jl)(t −t(jl))
.
(7.25)
With the shorthand notation yc to denote the event {Y ̸= y},
p({Xj = x(1)
j }, {j = π(l)
j }, y|e) −p(y|e)p({Xj = x(1)
j }, {j = π(l)
j }|e)
≤p({Xj = x(1)
j }, {j = π(l)
j }, y|e) −p(y|e)p({Xj = x(1)
j }, {j = π(l)
j }, y|e)
= p({Xj = x(1)
j }, {j = π(l)
j }, y|e)(1 −p(y|e)
≤p(y|e)(1 −p(y|e))
and
p(y|e)p({Xj = x(1)
j }, {j = π(l)
j }|e) −p({Xj = x(1)
j }, {j = π(l)
j }, y|e)
= (1 −p(yc|e))p({Xj = x(1)
j }, {j = π(l)
j }|e)
−p({Xj = x(1)
j }, {j = π(l)
j }|e) + p({Xj = x(1)
j }, {j = π(l)
j }, yc|e)
= p({Xj = x(1)
j }, {j = π(l)
j , yc|e) −p(yc|e)p({Xj = x(1)
j }, {j = π(l)
j |e)
= p({Xj = x(1)
j }, {j = π(l)
j }, yc|e)(1 −p(yc|e))
≤p(yc|e)(1 −p(yc|e))
= (1 −p(y|e))p(y|e)
From this, it follows directly from Equation (7.25) that
....
∂
∂t(jl) p(y|e)
.... ≤
p(y|e)(1 −p(y|e))
t(jl)(1 −t(jl))
.
The proof of Theorem 7.11 is complete.
□

222
PARAMETERS AND SENSITIVITY
Corollary 7.6 The sensitivity of p(y|e) to the parameter t(jl) is bounded by
.....
∂
∂t(jl) log
p(y|e)
p(yc|e)
..... ≤
1
t(jl)(1 −t(jl)).
(7.26)
Proof of Corollary 7.6 Immediate.
□
It is clear that the worst situation from a robustness point of view arises when the
parameter value t(jl) is close to either 0 or 1, while the query takes values that are close
to neither 0 nor 1.
Example 7.7
This example shows that the bounds given by inequalities (7.23) and (7.26)
are tight, in the sense that there are examples where it is attained. Consider the network
given in Figure 7.2, where X and Y are binary variables taking values from (x0, x1) and
(y0, y1) respectively. pX(x0) = θx and pY(y0) = θy. Suppose that E is a deterministic
binary variable; that is, p({E = e}|{X = Y}) = 1 and p({E = e}|{X ̸= Y}) = 0.
The probability potentials are
pX = x0
x1
θx
1 −θx
pY = y0
y1
θy
1 −θy
pE|X,Y(e|., .) =
X\Y
y0
y1
x0
1
0
x1
0
1
It follows that
pY|E(y0|e) = pY|E(y0, e)
pE(e)
= pY (y0) 
x pX(x)pE|X,Y (e|x, y0)

x,y pX(x)pY (y)pE|X,Y (e|x, y)
=
θyθx
θyθx + (1 −θy)(1 −θx).
It follows that
∂
∂θx
pY|E(y0|e) =
θy(1 −θy)
(θxθy + (1 −θx)(1 −θy))2
while
pY|E(y0|e)(1 −pY|E(y0|e))
θx(1 −θx)
=
θyθx(1 −θy)(1 −θx)
(θxθy + (1 −θx)(1 −θy))2θx(1 −θx)
=
θy(1 −θy)
(θxθy + (1 −θx)(1 −θy))2 ,
X
Y
E
Figure 7.2
The network for Example 7.7.

THE SENSITIVITY OF QUERIES TO PARAMETER CHANGES
223
so that
∂
∂θx
pY|E(y0|e) =
θy(1 −θy)
(θxθy + (1 −θx)(1 −θy))2
showing that the bound (7.23) is achieved.
□
For the bound (7.26), note from the above that
∂
∂θx
pY|E(y0|e) = pY|E(y0|e)pY|E(y1|e)
θx(1 −θx)
so that
∂
∂θx
log pY|E(y0|e) = pY|E(y1|e)
θx(1 −θx)
and, because pY|E(y0|e) + pY|E(y1|e) = 1,
∂
∂θx
pY|E(y1|e) = −∂
∂θx
pY|E(y0|e) = −pY|E(y0|e)pY|E(y1|e)
θx(1 −θx)
so that
∂
∂θx
log pY|E(y0|e)
pY|E(y1|e) =
1
θx(1 −θx),
so that equality is achieved in bound (7.26).
□
Although a small change in a parameter t(jl) can lead to a large change in a query p(y|e),
the change is not so large if instead the change in odds are considered.
Theorem 7.12 Let p be a parametrized family of probability distributions, factorized
along the same DAG, with a single parameter θ. Let Xj be a binary variable and let
θ = p(θ)
Xj |j (x(0)
j |π(l)
j ); all the other CPPs remain ﬁxed and let Oθ =
θ
1−θ . Consider a
parameter change from θ = t to θ = s. Note that Ot =
t
1−t and Os =
s
1−s . Let p(θ)(y|e)
denote the probability value of a query when θ is the parameter value. Let ˜Oθ(y|e) =
p(θ)(y|e)
1−p(θ)(y|e). Then
Ot
Os
≤
˜Os(y|e)
˜Ot(y|e) ≤Os
Ot
s ≥t
Os
Ot
≤
˜Os(y|e)
˜Ot(y|e) ≤Ot
Os
s ≤t.
This gives the bound
...log ˜Os(y|e) −log ˜Ot(y|e)
... ≤|log Os −log Ot| .
Proof of Theorem 7.12 Let x denote the probability of the query p(y|e) when the value
of the parameter t(jl) is z.
Note that, for 0 < a ≤b < 1,
 b
a
dx
x(1 −x) =
 b
a
dx
x +
 b
a
dx
1 −x = log b
a
1 −a
1 −b.

224
PARAMETERS AND SENSITIVITY
Then, for t(jl) ≤s(jl), Equation (7.23) gives
−
 s
t
dz
z(1 −z) ≤
 ps(y|e)
pt(y|e)
dx
x(1 −x) ≤
 s
t
dz
z(1 −z),
so that
−log s
t
1 −t
1 −s ≤log
ps(y|e)
pt(y|e)
1 −ps(y|e)
1 −pt(y|e) ≤log s
t
1 −t
1 −s
giving immediately that
Ot
Os
≤
˜Os(y|e)
˜Ot(y|e) ≤Os
Ot
.
For s ≤t the argument is similar and gives
Os
Ot
≤
˜Os(y|e)
˜Ot(y|e) ≤Ot
Os
.
In both cases
...log ˜Os(y|e) −log ˜Ot(y|e)
... ≤|log Os −log Ot|
and the result follows.
□
Notes
The observation that the probability of evidence is a linear function of any single
parameter in the model and hence that the conditional probability is the ratio of two
linear functions is due to E. Castillo, J.M. Guti´errez and A.S. Hadi [108] and [109]. See
also V.M. Coup´e and L.C. van der Gaag [110]. The most signiﬁcant developments in
sensitivity analysis, which comprise practically the whole chapter, were introduced by
H. Chan and A. Darwiche in the article [111] and developed in the article [66], where
Chan-Darwiche distance measure was introduced, and the article [81], which discusses
the application to Jeffrey’s update rule and Pearl’s method of virtual evidence.

PARAMETERS AND SENSITIVITY
225
7.6
Exercises: Parameters and sensitivity
1. Let G = (V, E) be a directed acyclic graph, where V = (X1, . . . , Xd), and let p
and q be two probability distribution factorized along G. Suppose that the con-
ditional probability tables for p and q are the same except for one single (j, l)
variable/parent conﬁguration, where p has table θj.l and q has table ˜θj.l. Let dKL
denote the Kullback-Leibler distance. Show that
dKL(p, q) = p({j = π(l)
j })dKL(θj.l, ˜θj.l).
2. Let DCD(p, q) denote the Chan-Darwiche distance between two probability distribu-
tions. Prove that for any two probability distributions p and q, deﬁned on the same
ﬁnite state space X,
• DCD(p, q) ≥0 with equality if and only if p(x) = q(x) for all x ∈X.
• DCD(p, q) = DCD(q, p).
3. Let p be a probability distribution over a countable state space X and let G1, . . . , Gn
be a collection of mutually exclusive and exhaustive events. Let λj = p(Gj) for
j = 1, . . . , n. Let q denote the probability distribution such that q(Gj) = µj for
j = 1, . . . , n and such that for any other event A,
q(A) =
n

j=1
µjp(A|Gj).
In other words, q is the Jeffrey’s update of p, deﬁned by q(Gj) = µj, j = 1, . . . , n.
Prove that
dCD(p, q) = log max
j
λj
µj
−log min
x∈X
µj
λj
,
where dCD denotes the Chan-Darwiche distance.
4. Consider a Bernoulli trial, with probability function pX(.|t) deﬁned by
pX(x|t) = tx(1 −t)1−x,
x = 0, 1,
t ∈[0, 1].
Recall the deﬁnition of sensitivity, Deﬁnition 7.5. Compute the sensitivity with
respect to the parameter t.
5. Find the calibration cd(k) of the Chan-Darwiche distance by suitably reformulating
the concept of calibration as found in Exercise 3, Chapter 5. You should obtain
cd(k) =
e±k
1 + e±k .
Why is this a reasonable calibration? Compare by plotting h(k), the calibration of the
Kullback-Leibler divergence in Example 3, Chapter 5, together with the calibration
cd(k) and comment on any differences you observe.

226
EXERCISES: PARAMETERS AND SENSITIVITY
A
C
D
B
Figure 7.3
DAG on four variables.
6. Consider the DAG given in Figure 7.3, where (A, B, C, D) are all binary variables
with probability tables
pA = 1
0
s
1 −s ,
pB = 1
0
t
1 −t
pC|A,B(1|., .) =
A\B
1
0
1
1
0.8
0
0.8
0
Compute the sensitivity, with respect to s and with respect to t, for pA|C(1|1).
7. A test to detect whether a person has the Green Monkey Disease virus is being
developed. Laboratory tests show that if a person has the condition, then the test
gives a positive result with probability 0.99. If a person does not have the condition,
then the test gives a positive result with probability 0.02. The proportion of the
population that has the condition is t, where t is unknown, but suspected to be around
1%. Compute the sensitivity of the probability that a randomly chosen person has
the disease given a positive test result with respect to this parameter. Compute the
probability that the person has the disease, given a positive test result, for t = 0.01
and t = 0.02.
8. Following the example (Section 7.3) of the Chan-Darwiche distance for two distri-
butions factorized along a fork, ﬁnd an expression for the Chan-Darwiche distance
between two distributions factorized along the same Chow-Liu tree.
9. Suppose that the probability distribution pA,B,C,D can be factorized along the the
DAG given in Figure 7.3. Suppose that pA(1) = pB(1) = 0.5,
pD|C =
C\D
0
1
0
0.7
0.3
1
0.3
0.7
and
pC|A,B(1|., .) =
A\B
0
1
0
0
t
1
1
1
Is it possible to ﬁnd a value of t such that pA|D = 0.8?

PARAMETERS AND SENSITIVITY
227
10. Suppose the probability distribution of (A, B, C, D) may be factorized along the
DAG given in Figure 7.3.
All variables are binary, taking values 0 or 1. Suppose, after training examples,
pA(1) = pB(1) = 0.5,
pD|C =
C\D
0
1
0
0.6
0.4
1
0.2
0.8
Suppose
pC|A,B(1|., .) =
A\B
0
1
0
0
1 −t
1
1 −s
1 −st
(a) Compute the range of values that pA|D(1|1) can take.
(b) The gradient descent method is a standard numerical procedure. Suppose the
parameters are t and a constraint (in the example below, pA|D(1|1) = 0.6) has
to be satisﬁed. If one wants to ﬁnd parameter values t as close as possible to
prescribed initial values t0 according to a certain distance measure, one starts by
choosing a step length δ. One then ﬁnds values of the parameters for which the
constraints are satisﬁed. (In the example below, this can be done by computing
s1 and t1 such that (s0, t1) and (s1, t0) satisfy the constraint and letting (s∗, t∗) =
(s0, t1) or (s1, t0), whichever gives the smallest value of d((s, t), (s0, t0)).
Next, if there are constraints, such as f (t) = c (where t is the set of parameters), the
gradient of the constraint ∇tf (t)|t∗is computed. From the unit vectors ν such that
(ν, ∇tf (t)|t∗) = 0 (that is |ν| = 1), the one that gives the lowest value (i.e. negative
number with largest absolute value) of (ν, dt(t, t0)|t=t∗) is chosen. The parameters
are then updated; t∗→t∗+ δν. Then, compute the appropriate vector ν and update
again. Continue until the change in the distance d(t∗, t0) is less than a prescribed
value.
Write a MATLAB code that employs a Gradient Descent Method, with step length
0.02, to compute the values of (s, t) as close as possible to prescribed initial values
(s0, t0), which satisﬁes the constraint pA|D(1|1) = 0.6, where the distance used is the
Chan-Darwiche distance.


8
Graphical models and
exponential families
This chapter introduces exponential families of distributions, focusing on links with con-
vex analysis and speciﬁcally with the theory of conjugate duality. This is then applied to
updating the probability distribution in a graphical model in the light of new information.
One of the key features of exponential families, with mean parameters, is the relative
ease with which the entropy and Kullback-Leibler distance between two members of the
family may be computed.
8.1
Introduction to exponential families
The notations are as before, except that continuous random variables will also be consid-
ered for modelling the outcome of an experiment. Let V = {X1, . . . , Xd} denote the ran-
dom variables. For j = 1, . . . , d, Xj will denote the state space for variable Xj. If Xj is
continuous, then Xj ⊆R (the real numbers). If Xj is discrete, then Xj = {x(1)
j , . . . , x
(kj )
j
},
where kj is possibly +∞. The notation X = (X1, . . . , Xd) denotes the row vector; when
data is presented in a matrix, it is usual that each column presents a different attribute
and each row represents an independent instantiation. An instantiation of X will be
denoted x ∈X1 × . . . Xd ≡X (when no subscript is employed, X denotes the product
space, which is the state space of the row vector X).
An exponential family is a family of probability distributions satisfying certain prop-
erties, listed in Deﬁnition 8.1 below. For the purposes of Bayesian networks, the emphasis
is on discrete variables.
Bayesian Networks: An Introduction
T. Koski, J. Noble
2009 John Wiley & Sons, Ltd

230
GRAPHICAL MODELS AND EXPONENTIAL FAMILIES
Deﬁnition 8.1 (Exponential Family) An exponential family is a family of probability
distributions deﬁned by a probability function pX(.|θ) if X are discrete variables, or a
probability density function πX(.|θ) for continuous variables, indexed by a parameter set
˜ ⊆Rp (where p is possibly inﬁnite), where there is a function  : X →Rp, a function
A : ˜ →R and a function h : X →R such that
pX(x|θ) = exp{⟨θ, (x)⟩−A(θ)}h(x)
if X is a discrete random vector and
πX(x|θ) = exp{⟨θ, (x)⟩−A(θ)}h(x)
if X is a continuous random vector.
It is convenient to use the notation I to denote the indexing set for the parameters;
θ = (θα)α∈I. Then  denotes a collection of functions  = (φα)α∈I, where φα : X →R.
The inner product notation is deﬁned as
⟨θ, (x)⟩=

α∈I
θαφα(x).
The parameters in the vector θ are known as the canonical parameters or exponential
parameters.
Attention will be restricted to distributions where |I| = p < +∞; namely, I has a ﬁnite
number, p, of elements.
Since 
X pX(x|θ) = 1 for discrete variables and

X πX(x|θ)dx = 1 for continuous
variables, it follows that the quantity A, known as the log partition function, is given by
the expression
A(θ) = log

X
exp{⟨θ, (x)⟩}h(x)dx
for continuous variables and
A(θ) = log

X
exp
#
⟨θ, (x)⟩
$
h(x)
for discrete variables. It is assumed that h, θ and  satisfy appropriate conditions so that
A is ﬁnite.
Set
P (x; θ) = pX(x|θ)
h(x)
.
(8.1)
With the set of functions  ﬁxed, each parameter vector θ indexes a particular probability
function pX(.|θ) belonging to the family. The exponential parameters of interest belong
to the parameter space, which is the set
˜ = {θ ∈Rp|A(θ) < +∞}.
(8.2)
It will be seen shortly that A is a convex function of θ.

STANDARD EXAMPLES OF EXPONENTIAL FAMILIES
231
Deﬁnition 8.2 (Regular Families) An exponential family for which the domain ˜ of
Equation (8.2) is an open set is known as a regular family.
Attention will be restricted to regular families.
Deﬁnition 8.3 (Minimal Representation) An exponential family, deﬁned using a collec-
tion of functions  for which there is no linear combination ⟨a, (x)⟩= 
α∈I aαφα(x)
equal to a constant is known as a minimal representation.
For a minimal representation, there is a unique parameter vector θ associated with each
distribution.
Deﬁnition 8.4 (Over-complete) An over-complete representation is a representation that
is not minimal; there is a linear combination of the elements of  which yields a constant.
When the representation is over-complete, there exists an afﬁne subset of parameter
vectors θ, each associated with the same distribution.
Recall the deﬁnition of sufﬁciency, given in Deﬁnition 3.3. The following lemma is
crucial. Its proof is left as an exercise.
Lemma 8.1 Let X = (X1, . . . , Xd) be a random vector with joint probability function
pX(x|θ) = exp{⟨θ, (x)⟩−A(θ)}h(x),
X ∈X
then (X), which will be denoted , is a Bayesian sufﬁcient statistic for θ. If the repre-
sentation is minimal, then (X) is a minimal sufﬁcient statistic for θ.
Proof of Lemma 8.1 See Exercise 2, Chapter 8.
□
8.2
Standard examples of exponential families
The purpose of this section is to take some basic distributions, which are well known,
and illustrate that they satisfy the deﬁnition of an exponential family.
Bernoulli
Consider the random variable X, taking values 0 or 1, with probability func-
tion pX(1) = p, pX(0) = 1 −p. This may be written as
pX(x) =

px(1 −p)1−x
x ∈{0, 1}
0
other x.
Then
pX(x) = exp

x log

p
1 −p

+ log(1 −p)
"
= exp {xθ + log(1 −p)}
= exp
#
xθ −log(1 + eθ)
$
,
where θ = log

p
1−p

.

232
GRAPHICAL MODELS AND EXPONENTIAL FAMILIES
Note: Change of notation
In Chapter 1.9, the quantity now denoted by p was denoted
by θ. The quantity θ no longer denotes a probability; it now denotes the canonical
parameter.
In the language of exponential families, X = {0, 1},  = {φ} where φ(x) = x, h(0) =
h(1) = 1,
pX(0|θ) = e−A(θ), pX(1|θ) = eθ−A(θ)
In other words
log pX(x|θ) = θx −A(θ),
which gives
1 = pX(0|θ) + pX(1|θ) = e−A(θ)(1 + eθ)
so that
A(θ) = log(1 + exp{θ}).
Gaussian
Recall that the one-dimensional Gaussian density is of the form
π(x|µ, σ) =
1
√
2πσ
exp

−(x −µ)2
2σ 2
"
.
This may be expressed in terms of an exponential family as follows: X = R, h(x) = 1,
 = {φ1, φ2} where φ1(x) = x and φ2(x) = −x2.
log π(x|θ) = θ1x −θ2x2 −A(θ)
where
1 = e−A(θ)
 ∞
−∞
eθ1x−θ2x2dx.
The partition function is therefore
A(θ) = 1
2 log π −1
2 log θ2 + θ2
1
4θ2
2
and the parameter space is
˜ = {(θ1, θ2) ∈R2|θ2 > 0}.
Note that in the ‘usual’ notation
θ1 = µ
σ 2 ,
θ2 = 1
σ 2 .
Exponential
Recall that an exponential density is of the form
π(x|λ) =

λe−λx
x ≥0
0
x < 0.
This is an exponential family, taking X = (0, +∞), h(x) = 1,  = φ, where φ(x) = −x,
θ = λ, so that e−A(θ) = θ, yielding A(θ) = −log θ,  = (0, +∞).

GRAPHICAL MODELS AND EXPONENTIAL FAMILIES
233
Poisson
Recall that the probability function p for a Poisson distribution with parameter
µ is given by
p(x|µ) = µx
x! e−µ,
x = 0, 1, 2, . . .
This is an exponential family with h(x) = 1
x!, θ = log µ so that p(x|µ) = P (x; θ)h(x),
where
P (x; θ) = exθ−eθ .
This gives A(θ) = exp{θ}. Since µ ≥0 and θ = log µ, it follows that ˜ = R.
Beta
Recall that the probability density function for a Beta distribution is given by
π(x|α, β) =

(α+β)
(α)(β)xα−1(1 −x)β−1
x ∈[0, 1]
0
other x.
This is an exponential family, with X = (0, 1), h ≡1, α −1 = θ1, β −1 = θ2,  =
{φ1, φ2} where φ1(x) = log x, φ2(x) = log(1 −x). Then
log π(x|θ) = θ1 log x + θ2 log(1 −x) −A(θ),
where the partition function A is given by
A(θ) = log (θ1 + 1) + log (θ2 + 1) −log (θ1 + θ2 + 2)
and the parameter space is ˜ = (−1, ∞)2.
8.3
Graphical models and exponential families
The scalar examples described in Section 8.2 serve as building blocks for the construction
of exponential families, which have an underlying graphical structure.
Example 8.1: Sigmoid belief network model
The sigmoid belief network model,
described below, was introduced by R. Neal in [112]. It is an exponential family, with
an underlying graphical structure.
Consider a directed acyclic graph G = (V, E), where V = {X1, . . . , Xd} is the set of
variables, along which the probability distribution of X = (X1, . . . , Xd) may be factor-
ized. Suppose that for each Xj ∈V , j = 1, . . . , d, the random variable Xj takes values
0 or 1, each with probability 1/2. For any two components Xs and Xt of the random
vector X, component Xs has a direct causal effect on Xt only if (Xs, Xt) ∈E.
The following notation will be used:
˜V = {1, . . . , d},
˜E = {(s, t)|(Xs, Xt) ∈E}.
The probability distribution over the possible conﬁgurations is modelled by an exponential
family with probability function pX(.|θ) of the form
pX(x|θ) = exp



d

s=1
θsxs +

(s,t)∈˜E
θ(s,t)xsxt −A(θ)


.

234
GRAPHICAL MODELS AND EXPONENTIAL FAMILIES
The notation i denotes the parent set of node Xi and πi(x) denotes the instantiation of
i corresponding to the instantiation {X = x}, this may be rewritten as
pX(x|θ) =
d
i=1
pXi|i(xi|πi(x), θ),
where (clearly)
pXi|i(xi|πi(x), θ) =
exp
/
xi

θi + 
xj ∈πi(x) θ(ij)xj
0
1 + exp
/
θi + 
xj ∈πi(x) θ(ij)xj
0 ,
where the notation xj ∈πi(x) is clear. The index set is I = ˜V ∪˜E. The domain  = Rn,
where n = |I|. Since the sum that deﬁnes A(θ) is ﬁnite for all θ ∈Rn, it follows that
the family is regular. It is minimal, since there is no linear combination of the functions
equal to a constant.
This model may be generalized. For example, one may consider higher order inter-
actions. To include coupling of triples (Xs, Xt, Xu), one would add a monomial xsxtxu
with corresponding exponential parameter θ(s,t,u). More generally, the set C of indices of
interacting variables may be considered, giving
pX(x|θ) = exp

C∈C
θ(C)

s∈C
Xs −A(θ)
*
.
8.4
Noisy ‘or’ as an exponential family
This section outlines some ideas of propositional logic in the presence of noise. The
noisy ‘or’ gate is a device that reduces the sizes of the conditional probability tables
and can hence lead to sharper probabilistic inference. The section ends with an example
illustrating that the basic structures of the noisy ‘or’ gate can be expressed as exponential
families.
Disjunction in propositional logic
In logic, the ‘or’ disjunction of two propositions p
and q is denoted by p ∨q and is deﬁned by the truth table
p
q
p ∨q
1
1
1
1
0
1
0
1
1
0
0
0
Here 1 = the proposition is true, 0 = the proposition is false. For example, if p and q
are the causes of some effect (e.g. a sore throat) and the presence of either or both of
them will make the effect occur.

NOISY ‘OR’ AS AN EXPONENTIAL FAMILY
235
B
A1
A2
An
. . .
Figure 8.1
A logical ‘or’ gate.
1–q1
1–q2
1–qn
B
A1
A2
An
. . .
Figure 8.2
Noisy ‘or’ junction.
Noisy ‘or’ as a causal network
Consider the DAG given in Figure 8.1 where B =
A1 ∨A2 ∨. . . ∨An. This is the logical ‘or’ and there is no noise.
The noise then enters, as in the DAG given in Figure 8.2, by considering that if
any of the variables Ai, i = 1, . . . , n is present, then B is present unless something has
inhibited it.
Noisy ‘or’: inhibitors
Consider the DAG in Figure 8.2, where qi denotes the probability
that the impact of Ai is inhibited.
All variables are binary, and take value 1 if the cause, or effect, is present and 0
otherwise. In other words, pB|Ai(0|1) = qi. The assumption from the DAG is that all the
inhibitors are independent. This implies that
pB|A1,...,An(0|a1, . . . , an) =

j∈Y
qj,
where Y = {j ∈{1, . . . , n}|aj = 1}. This may be described by a noisy ‘or’ gate.
Noisy ‘or’ gate
The noisy ‘or’ can be modelled directly, introducing the variables Bi
i = 1, . . . , n, where Bi takes the value 1 if the cause Ai is on and it is not inhibited and
0 otherwise. The corresponding DAG is given in Figure 8.3.
B
B1
B2
Bn
A2
An
A1
. . .
. . .
Figure 8.3
Noisy ‘Or’ Gate.
where
pB|B1,...,Bn(1|b1, . . . , bn) = b1 ∨. . . ∨bn.

236
GRAPHICAL MODELS AND EXPONENTIAL FAMILIES
C
B
A
Figure 8.4
A collider connection.
The B1, . . . , Bn are introduced as mutually independent inhibitors, and
pBi|Ai(0|1) = qi,
yielding the result given above.
Example 8.2
Consider the collider connection given in Figure 8.4.
Suppose that the variables A, B and C are binary, taking values 0 or 1 and that
the conditional probabilities of C given {A = 1} and {B = 1} are pC|A(1|1) = 0.7 and
pC|B(1|1) = 0.8. Suppose, furthermore, that they satisfy the assumptions of the ‘noisy or’
gate; namely, that {A = 1} causes {C = 1} unless an inhibitor prevents it; this inhibitor
functions with probability 0.3, that {B = 1} causes {C = 1} unless an inhibitor prevents
it; this inhibitor functions with probability 0.2. Then this network may be considered as
a ‘noisy or’ gate, with additional variables X and Y,
pC|X,Y (1|1, 1) = pC|X,Y (1|1, 0) = pC|X,Y (1|0, 1) = 1,
pC|X,Y (1|0, 0) = 0
and
pX|A(1|1) = 0.7,
pX|A(1|0) = 0,
pY|B(1|1) = 0.8,
pY|B(1|0) = 0.
Example 8.3: Noisy ‘or’ as an exponential family
The QMR–DT (Quick Medical
Reference–Decision Theoretic) database is a large scale probabilistic data base that is
intended to be used as a diagnostic aid in the domain of internal medicine. It is a bipartite
graphical model; that is, a graphical model where the nodes may be of one of two types.
The upper layer of nodes (the parents) represent diseases and the lower layer of nodes
represent symptoms. There are approximately 600 disease nodes and 4000 symptom
nodes in the database.
An evidence, or ﬁnding, will be a set of observed symptoms, denoted by a vector
of length 4000, each entry being a 1 or 0 depending upon whether or not the symptom
is present or absent. This will be denoted f , which is an instantiation of the random
vector F. The vector d will be used to represents the diseases; this is considered as an
instantiation of the random vector D. Let dj denote component j of vector d and let fj
denote component j of vector f . Then, if the occurrence of various diseases are taken
to be independent of each other, the following factorization holds:
pF,D(f , d) = pF|D(f |d)pD(d) =

i
pFi|D(fi|d)

j
pDj (dj).

PROPERTIES OF THE LOG PARTITION FUNCTION
237
This may be represented by a noisy ‘or’ model. Let qi0 denote the probability that
symptom i is present in the absence of any disease and qij the probability that disease
j induces symptom i, then the probability that symptom i is absent, given a vector of
diseases d is
pFi|D(0|d) = (1 −qi0)

j
(1 −qij)dj .
The noisy ‘or’ may then be rewritten in an exponential form:
pFi|D(0|d) = exp


−

j
θijdj −θi0


,
where θij ≡log(1 −qij) are the transformed parameters.
8.5
Properties of the log partition function
Firstly, some basic properties of the log partition function A(θ) are discussed, which
are then developed using convex analysis, discussed in [113]. Let Eθ[.] denote expecta-
tion with respect to p(.|θ) for discrete variables, or π(.|θ) for continuous variables. Of
particular importance is the idea that the vector µ, where µi := Eθ[φi(X)] provides an
alternative parametrisation of the exponential family. Here expectation is deﬁned as
Eθ[f (X)] =

X
πX(x|θ)f (x)dx
if X is a continuous random vector and
Eθ[f (X)] =

x∈X
pX(x|θ)f (x)
if X is a discrete random vector. Recall that, for discrete variables,
A(θ) = log

x∈X
e⟨θ,(x)⟩h(x).
(8.3)
Provided expectations and variances exist, it follows that
∂
∂θα
A(θ) =

x∈X
e⟨θ,(x)⟩−A(θ)φα(x)h(x) = Eθ[φα(X)].
(8.4)
Taking second derivatives yields
∂
∂θα∂θβ
A(θ) = Eθ[φα(X)φβ(X)] −Eθ[φα(X)]Eθ[φα(X)] = Covθ(φα(X), φβ(X)).
It is and easy to show, and a standard fact, that any covariance matrix is non-negative
deﬁnite. It now follows that, on ˜, A is a convex function.

238
GRAPHICAL MODELS AND EXPONENTIAL FAMILIES
Mapping to mean parameters
Given a vector of functions , set F(θ) = Eθ[(X)]
and let M = F( ˜). For an arbitrary exponential family deﬁned by
pX(x|θ) = exp
#
⟨θ, (x)⟩−A(θ)
$
h(x),
a mapping  : ˜ →M may be deﬁned as follows:
(θ) := Eθ[(X)].
To each θ ∈˜, the mapping  associates a vector of mean parameters µ = (θ) belong-
ing to the set M. Note that, by Equation (8.4),
(θ) = ∇A(θ).
The mapping  is one to one, and hence invertible on its image, when the representation
is minimal. The image of ˜ is the interior of M.
Example 8.4
Consider a Bernoulli random variable X with state space {0, 1}.
That is, pX(0) = 1 −p and pX(1) = p. Now consider an over-complete exponential
representation
pX(x|θ) = exp {θ0(1 −x) + θ1x −A(θ0, θ1)}
so that
A(θ0, θ1) = log

eθ0 + eθ1
.
Here ˜ = R2. φ0(x) = 1 −x and φ1(x) = x.
∂
∂θ0
A(θ) = eθ0−A(θ0,θ1) = 1 −p = µ0
∂
∂θ1
A(θ) = eθ1−A(θ0,θ1) = p = µ1.
The set M of mean parameters is the simplex {(µ0, µ1) ∈R+ × R+|µ0 + µ1 = 1}. For
any ﬁxed µ = (µ0, µ1) where µ0 ≥0, µ1 ≥0, µ0 + µ1 = 1, the inverse image is,
−1(µ) =

(θ0, θ1) ∈R2
....
eθ0
eθ0 + eθ1 = µ0
"
which may be rewritten as
−1(µ) =

(θ0, θ1) ∈R2
....θ1 −θ0 = log µ1
µ0
"
.
In an over-parametrized, or over-complete representation, there is no longer a bijection
between ˜ and ( ˜). Instead, there is a bijection between elements of ( ˜) and afﬁne
subsets of ˜. A pair (θ, µ) is said to be dually coupled if µ = (θ), and hence θ ∈
−1(µ).

FENCHEL LEGENDRE CONJUGATE
239
8.6
Fenchel Legendre conjugate
The Fenchel Legendre conjugate of the log partition function A is deﬁned as follows:
A∗(µ) := sup
θ∈˜
/
⟨µ, θ⟩−A(θ)
0
.
(8.5)
The choice of notation is deliberately suggestive; the variables in the Fenchel Legendre
dual turn out to have interpretation as the mean parameters. Recall the deﬁnition of
P given by Equation (8.1); namely, if pX(x|θ) is the probability function (or density
function), then
P (x; θ) = pX(x|θ)
h(x)
.
Deﬁnition 8.5 (Boltzmann-Shannon
Entropy) The
Boltzmann-Shannon
entropy
of
pX(x|θ) with respect to h is deﬁned as
H(pX(x|θ)) = −Eθ[log P (x; θ)].
The following is the main result of the chapter.
Theorem 8.1 For any µ ∈M, let θ(µ) ∈−1(µ). Then
A∗(µ) = −H(pX(x|θ(µ)).
In terms of this dual, for θ ∈˜, the log partition satisﬁes be expressed:
A(θ) = sup
µ∈M
{⟨θ, µ⟩−A∗(µ)}.
(8.6)
Proof of Theorem 8.7 From the deﬁnition µ = Eθ[(X)], it follows that
−H(pX(x|θ)) = Eθ[log P (X; θ)] = Eθ[⟨θ, (X)⟩] −A(θ) = ⟨θ, µ⟩−A(θ).
(8.7)
Consider the function
F(µ, θ) = ⟨µ, θ⟩−A(θ).
Let θ(µ) denote a value of θ that maximizes F(µ, θ) if such a value exists in ˜. The result
follows directly by using the deﬁnition given by Equation (8.5) together with Equation
(8.7). Otherwise, let θ(n)(µ) denote a sequence such that limn→+∞F(µ, θ(n)(µ)) =
A∗(µ). The ﬁrst statement of the theorem follows directly from this.
For the second part, choose θ ∈˜ and choose µ(θ) = ∇θA(θ). By the deﬁnition of
M, note that µ(θ) ∈M. Since A is convex, it follows that µ(θ) maximizes ⟨θ, µ⟩−
A(θ), so that
A(θ) = ⟨µ(θ), θ⟩−A∗(µ(θ)).
But, from the deﬁnition of A∗(µ), it follows that for all µ ∈M,
A(θ) ≥⟨µ, θ⟩−A∗(µ).

240
GRAPHICAL MODELS AND EXPONENTIAL FAMILIES
From this,
A(θ) = sup
µ∈M
{⟨µ, θ⟩−A∗(µ)}
and Theorem 8.1 is established.
□
The conjugate dual pair (A, A∗) for the families of exponential variables given before
are now given.
Bernoulli
Recall that A(θ) = log(1 + exp{θ}) for θ ∈R. It follows that
A∗(µ) = sup
θ∈R
{θµ −log(1 + eθ)}
The supremum is attained for θ(µ) satisfying
µ =
eθ(µ)
1 + eθ(µ) .
It follows that
eθ(µ) =
µ
1 −µ
and
θ(µ) = log µ −log(1 −µ)
so that
A∗(µ) = µ log µ −µ log(1 −µ) −log(1 +
µ
1 −µ),
which gives
A∗(µ) = µ log µ + (1 −µ) log(1 −µ).
Gaussian
Recall that ˜ = {(θ1, θ2)|θ2 > 0} and
A(θ) = 1
2 log π −1
2 log θ2 + θ2
1
4θ2
.
A∗(µ) = sup
θ∈˜
{θ1µ1 + θ2µ2 −1
2 log π + 1
2 ln θ2 −θ2
1
4θ2
}.
This is maximized when



µ1 −θ1(µ)
2θ2(µ) = 0
µ2 +
1
2θ2(µ) +
θ2
1 (µ)
4θ2
2 (µ) = 0,
which gives



θ2(µ1, µ2) = −
1
2(µ2
1+µ2)
θ1(µ) = −
µ1
µ2
1+µ2

KULLBACK-LEIBLER DIVERGENCE
241
and
A∗(µ1, µ2) = −1
2 −1
2 log π −1
2 log(−2(µ2
1 + µ2)).
Note that
M = {(µ1, µ2)|µ2
1 + µ2 < 0}.
Exponential distribution
Recall that ˜ = (0, +∞) and that A(θ) = −log(θ). By a
straightforward computation,
A∗(µ) = −1 −log(−µ)
and
M = (−∞, 0).
Poisson distribution
Recall that ˜ = R and that A(θ) = exp{θ}. It is a straightforward
computation to see that
A∗(µ) = µ log µ −µ
and that
M = (0, +∞).
8.7
Kullback-Leibler divergence
Recall Deﬁnition 5.4, the Kullback-Leibler distance between two probability distributions
p and q over a ﬁnite state space X;
DKL(q|p) =

x∈X
q({x}) log q({x})
p({x}).
When X = Zd and p and q are two probability functions, this may be written as
DKL(q|p) = Eq

log q(X)
p(X)

,
(8.8)
where X is a random vector taking values in Zd and Eq denotes expectation with respect
the probability distribution by q (i.e. q is the probability function of X). The deﬁnition
of Kullback-Leibler may be extended to continuous distributions using Equation (8.8),
where q and p denote the respective density functions. In this case, Equation (8.8) is
taken as
DKL(q|p) =

Rd q(x) log q(x)
p(x)dx.
When q and p are members of the same exponential family, the Kullback-Leibler diver-
gence may be computed in terms of the parameters. The key result, for expressing the

242
GRAPHICAL MODELS AND EXPONENTIAL FAMILIES
divergence in terms of the partition function, is Fenchel’s inequality given in Equation
(8.9), which can be seen directly from the deﬁnition of A∗(µ).
A(θ) + A∗(µ) ≥⟨µ, θ⟩,
(8.9)
with equality if and only if µ = (θ) and θ ∈−1(µ). That is, for µ = (θ) and
θ ∈−1(µ),
A(θ) + A∗(µ) = ⟨µ, θ⟩.
(8.10)
Consider an exponential family of distributions, and consider two exponential parameter
vectors, θ1 ∈˜ and θ2 ∈˜. When distributions are from the same exponential family,
the notation D(θ1|θ2) is used to denote DKL(p(.|θ1)|p(.|θ2)). Set µi = (θi). Using the
parameter to denote the distribution with respect to which the expectation is taken, note
that
D(θ1|θ2) = Eθ1

log p(X|θ1)
p(X|θ2)

= A(θ2) −A(θ1) −⟨µ1, θ2 −θ1⟩.
(8.11)
The representation of the Kullback-Leibler divergence given in Equation (8.11) is known
as the primal form of the Kullback-Leibler divergence.
Taking µ1 = (θ1) and applying Equation (8.10), the Kullback-Leibler divergence
may also be written
D(θ1|θ2) ≡˜D(µ1|θ2) = A(θ2) + A∗(µ1) −⟨µ1, θ2⟩.
(8.12)
The representation given in Equation (8.12) is known as the mixed form of the
Kullback-Leibler divergence. Recall the deﬁnition of A∗given by
A∗(µ) := sup
θ∈˜
{⟨µ, θ⟩−A(θ)}
and recall Equation (8.6) from theorem 8.1,
A(θ) = sup
µ∈M
{⟨θ, µ⟩−A∗(µ)}.
Equation (8.6) may be rewritten as
inf
µ∈M{A(θ) + A∗(µ) −⟨θ, µ⟩} = 0.
It follows that infµ∈M ˜D(µ|θ) = 0.
Finally, taking µ2 = (θ2) and applying Equation (8.10) once again to Equation
(8.12) yields the so-called dual form of the Kullback-Leibler divergence:
˜˜D(µ1|µ2) ≡D(θ1|θ2) = A∗(µ1) −A∗(µ2) −⟨θ2, µ1 −µ2⟩.
(8.13)

MEAN FIELD THEORY
243
8.8
Mean ﬁeld theory
In this section, probability distributions of the form
pX(x|θ) = exp

α
θαφα(x) −A(θ)
*
h(x)
are considered. Mean ﬁeld theory techniques are discussed and it is shown how they
may be used to obtain estimates of the log partition function A(θ). This is equivalent
to the problem of ﬁnding an appropriate normalizing constant to make a function into a
probability density, a problem that often arises when updating using Bayes’ rule.
Mean ﬁeld theory is based on the variational principle of Equation (8.6). The two
fundamental difﬁculties associated with the variational problem are the nature of the
constraint set M and the lack of an explicit form for the dual function A∗. Mean ﬁeld
theory entails limiting the optimization to a subset of distributions for which A∗is
relatively easy to characterize.
More speciﬁcally, the discussion in this chapter is restricted to the case where the
functions φα are either linear or quadratic. The problem therefore reduces to considering
a graph G = (V, E), where the node set V denotes the variables and the edge set E
denotes a direct association between the variables. For this discussion, the edges in E are
assumed to be undirected. As usual, V = {X1, . . . , Xd}. Let ˜V = {1, . . . , d} denote the
indexing set and let ˜E = {(s, t)|(Xs, Xt) ∈E}. Speciﬁcally, the probability distributions
under consideration are of the form
pX(x|θ) = exp




s∈˜V
θsxs +

(s,t)∈˜E
θ(s,t)xsxt −A(θ)


.
Let H denote a sub-graph of G over which it is feasible to perform exact calculations. In
an exponential formulation, the set of all distributions that respect the structure of H can
be represented by a linear subspace of the exponential parameters. Let I(H) denote the
subset of indices associated with cliques in H. Then the set of exponential parameters
corresponding to distributions structured according to H is given by
E(H) :=
#
θ ∈˜ | θα = 0, α ∈I\I(H)
$
.
The simplest example is to consider the completely disconnected graph H = (V, φ).
Then
E(H) =
#
θ ∈˜ | θ(s, t) = 0, (s, t) ∈E
$
.
The associated distributions are of the product form
pX(x|θ) =

s∈˜V
pXs(xs|θs).
Optimization and lower bounds
Let pX(x|θ) denote the target distribution that is
to be approximated. The basis of mean ﬁeld approximation is the following: any valid

244
GRAPHICAL MODELS AND EXPONENTIAL FAMILIES
mean parameter speciﬁes a lower bound on the log partition function, established using
Jensen’s inequality.
Proposition 8.1 (Mean Field Lower Bound)
A(θ) ≥sup
µ∈M
/
⟨θ, µ⟩−A∗(µ)
0
Proof of Proposition 8.1 The proof is given for discrete variables; the proof for contin-
uous variables is exactly the same, replacing the sum with an integral.
A(θ) = log

x∈X
exp{⟨θ, (x)⟩}
= log

x∈X
pX(x|θ) exp{⟨θ, (X)⟩−log pX(x|θ)}
= log Eθ[exp{⟨θ, (X)⟩−log pX(X|θ)}]
(a)
≥⟨θ, Eθ[(X)]⟩−Eθ[log pX(X|θ)}]
= ⟨θ, µ⟩−A∗(µ).
The inequality (a) follows from Jensen’s inequality; the last line follows from
Theorem 8.1.
□
There are difﬁculties in computing the lower bound in cases where there is not an explicit
form for A∗(µ). The mean ﬁeld approach circumvents this difﬁculty by restricting to
M(G; H) :=
/
µ ∈Rd | µ = Eθ[(X)], θ ∈E(H)
0
.
Note that M(G; H) ⊂M, hence
A(θ) ≥sup
µ∈M
/
⟨θ, µ⟩−A∗(µ)
0
≥
sup
µ∈M(G;H)
/
⟨θ, µ⟩−A∗(µ)
0
.
This lower bound is the best that can be obtained by restricting to H.
Let µ(n) denote a sequence such that for each n, µ(n) ∈M(G, H), such that
µ(n) n→+∞
−→µ and such that
⟨θ, µ(n)⟩−A∗(µ(n))
n→+∞
−→
sup
µ∈M(G;H)
/
⟨θ, µ⟩−A∗(µ)
0
.
Note that µ ∈M(G; H). Since θ ∈˜, it follows that µ ∈M. The distribution associated
with µ minimizes the Kullback-Leibler divergence between the approximating distribu-
tion and the target distribution, subject to the constraint that µ ∈M(G; H). Recall the
mixed form of the Kullback-Leibler divergence; namely, Equation (8.12).
˜D(µ|θ) = A(θ) −A∗(µ) −⟨µ, θ⟩.

MEAN FIELD THEORY
245
Naive mean ﬁeld updates
In the naive mean ﬁeld approach, a fully factorised distri-
bution is chosen. This is equivalent to the approximation obtained by taking an empty
edge set to approximate the original distribution. The naive mean ﬁeld updates are a set
of recursions for ﬁnding a stationary point of the resulting optimization problem.
Example 8.5
Consider the sigmoid belief network model. Here X = (X1, . . . , Xd) and
X = {0, 1}d. Suppose that the distribution may be factorized along a graph G = (V, E).
Let ˜E = {(i, j)|(Xi, Xj) ∈E}. The probability function is given by
pX(x|θ) = exp



d

j=1
θjxj +

(i,j)∈˜E
θ(i,j)xixj −A(θ)


.
The naive mean ﬁeld approach involves considering the graph with no edges. In this
restricted class,
pX(x|θ) = exp



d

j=1
θjxj −A(θ(H))


,
where θ(H) is the collection of parameters θ(H)
s
= θs, s = 1, . . . , d and θ(H)(s, t) ≡0.
Note that
µs = Eθ[φs(X)] = Eθ[Xs]
and
µ(s,t) = Eθ[φs,t(X)] = Eθ[XsXt].
When θ ∈H, it follows that (Xs)d
s=1 are independent, so that
µ(s,t) = Eθ[XsXt] = µsµt.
The optimization is therefore restricted to the set of parameters
M(G; H) = {(µs)d
s=1, (µ(s,t))(s,t)∈{1,...,d}2|0 ≤µs ≤1, µ(s,t) = µsµt.}
With the restriction to product form distributions, (Xs)d
s=1 are independent Bernoulli
variables and hence
A∗
H(µ) =
d

s=1
{µs log µs + (1 −µs) log(1 −µs).
Set
F(µ; θ) =
d

s=1
θsµs +

(s,t)∈˜E
θ(s,t)µsµt −
d

s=1
(µs log µs + (1 −µs) log(1 −µs)),
then the lower bound is given by
A(θ) ≥
sup
(µs)d
s=1∈[0,1]d
F(µ; θ).

246
GRAPHICAL MODELS AND EXPONENTIAL FAMILIES
Note that, for each µs, the function F is strictly convex. It is easy to see that the maximum
is attained when, for all 1 ≤s ≤t, (µt)d
t=1 satisﬁes
θs +

t:(s,t)∈˜E
θ(s,t)µt −log
µs
1 −µs
= 0,
or
log
µs
1 −µs
= θs +

t∈N(s)
θ(s,t)µt.
Note that if
log
y
1 −y = x,
then
y = σ(x),
where
σ(x) =
1
1 + e−x .
The algorithm then proceeds by setting
µ(j+1)
s
= σ(θs +

t∈N(s)
θ(s,t)µ(j)
t ).
As discussed in [114] (p. 222), the lower bound thus computed seems to provide a good
approximation to the true value.
8.9
Conditional Gaussian distributions
One very important family of distributions is the family of conditional Gaussian distribu-
tions. These are not of themselves of exponential type, but the conditional distributions
are of exponential type. An example of the situation in view here is found in Section 10.9,
where some of the variables are continuous and, conditioned on the discrete variables,
have Gaussian distribution.
Let V be a set of variables, where V = D ∪C; D is a set of discrete variables and C
is a set of continuous variables. Let ˜V denote the indexing set, where ˜V is decomposed
into ˜V = △∪; △is the indexing set for the discrete variables and  is the indexing
set for the continuous variables. Let |△| denote the number of variables in △and let ||
denote the number of variables in . The random vector X of variables in V will be
written
X = (X△, X),
where X△denotes the vector of discrete variables and X denotes the vector of continuous
variables. Random vectors will be row vectors. The state space is
X = X1 × . . . × X|△| × X|△|+1 × . . . × X|△|+||,

CONDITIONAL GAUSSIAN DISTRIBUTIONS
247
where Xj denotes the state space for variables j. The following notation will also be
used:
X△= X1 × . . . × X|△|,
X = X|△|+1 × . . . × X|△|+||,
X = X△× X.
Attention is restricted to the case where the continuous variables have Gaussian distribu-
tion, conditional on the discrete variables, so X = R||. For the discrete variables,
Xj = {i(1)
j , . . . , i
(kj )
j
}.
A particular conﬁguration i ∈X△is called a cell.
The following notation will be used to indicate that a random vector X1 conditioned
on X2 = x2 has distribution F:
X1 | X2 = x2 ∼F.
The moment generating function is necessary to deﬁne a multivariate normal distribution.
Deﬁnition 8.6 (Moment Generating Function) Let X = (X1, . . . , Xd) be a random vec-
tor. Its moment generating function is the function MX : Rd →R, where R ∪{+∞} ∪
{−∞}, is deﬁned as
MX(p1, . . . , pd) = E

exp



d

j=1
pjXj




.
The moment generating function is useful, because it uniquely determines the distribution
of a random vector X. That is, a joint probability determines a unique moment generating
function, and the moment generating function uniquely determines a corresponding joint
probability. The moment generating function is essentially a Laplace transform.
A multivariate normal distribution is deﬁned as follows:
Deﬁnition 8.7 (Multivariate Normal Distribution) A random vector X = (X1, . . . , Xd)
is said to have a multivariate normal distribution, written X ∼N(µ, C), if its moment
generating function is of the form
φ(p1, . . . , pd) = exp



d

j=1
pjµj + 1
2

jk
pjpkCjk


,
p ∈Rd.
If a random vector X ∼N(µ, C), then E[Xi] = µi
for each i = 1, . . . , d
and
Cov(Xi, Xj) = Cij for each (i, j). If C is positive deﬁnite, then the joint density
function of X = (X1, . . . , Xd) is given by
πX1,...,Xd(x1, . . . , xd) =
1
(2π)d/2|C|1/2 exp

−1
2(x −µ)C−1(x −µ)
"
,
x ∈Rd,

248
GRAPHICAL MODELS AND EXPONENTIAL FAMILIES
where x = (x1, . . . , xd) and µ = (µ1, . . . , µd) are row vectors and |C| denotes the deter-
minant of C.
The conditional Gaussian distribution, or CG distribution, may now be deﬁned.
Deﬁnition 8.8 (CG Distribution) A collection of random variables X =

X△, X

is said
to follow a CG distribution if for each i ∈X△,
X|X△= i ∼N

µ(i), C(i)

.
(8.14)
The notation for such a conditional Gaussian distribution is
X ∼CG(|△|, ||).
If the numbers of discrete and continuous random variables are, respectively, |△| = p
and || = q, then X ∼CG(p, q).
If C−1 is well deﬁned, then the conditional density function of X conditioned on
X△= i is
πX|X△

x|i

=
1
(2π)q/2√detC(i)e−1
2 (x−µ(i))C(i)−1(x−µ(i))t ,
(8.15)
for all i ∈X△such that
pX△(i) > 0.
For this discussion, it is assumed that pX△(i) > 0 for each i ∈X△.
Directly from Equation (8.15),
pX△(i)πX|X△

x|i

= χ(i)eg(i)+xh(i)−1
2 xK(i)xt
(8.16)
where χ(i) = 1 if pX△(i) > 0 and 0 if pX△(i) = 0, h(i) = C(i)−1µ(i), K(i) = C(i)−1
and
g(i) = log pX△(i) + 1
2

log detK(i) −|| log 2π −xK(i)xt
.
The family of joint distributions is not an exponential family, but from Equation (8.15),
it is clear that conditioning on the discrete variables gives a family of multivariate normal
distributions, which is an exponential family. The canonical parameters of this exponen-
tial family are

h(i), K(i)

and it is easy to see that the mean parameters of the CG
distribution are

µ(i), C(i)

since, conditioned on X△= i, it follows that E[X] = µ(i)
and E[Xt
X] = C(i) (recall that random vectors are taken to be row vectors).
Parametrization of the CG distribution
The CG distribution may be parametrized in
terms of the exponential family to which the conditional distributions belong. The canoni-
cal parameters for the joint distribution, deﬁned by the pair of functions

pX△, πX|X△

are deﬁned as (g, h, K), where the parameters (h(i), K(i)) are the canonical parameters
of the conditional distribution and g(i) is the log partition function of the conditional
distribution, conditioned on X△= i.

CONDITIONAL GAUSSIAN DISTRIBUTIONS
249
Similarly, the mean parameters are deﬁned as (p, µ, C), where (µ(i), C(i)) are the
mean parameters of the conditional distribution and p(i) is the appropriate multiplier
obtained from Equation (8.15).
Proposition 8.2 Let X have a conditional Gaussian distribution. Let V denote the set of
variables and ˜V the indexing set. Let A and B be two disjoint sets such that ˜V = A ∪B,
then the conditional distribution of XA given XB = xB is conditional Gaussian.
Proof of Proposition 8.2 The following calculation shows that XA∩ | {XB = xB} ∪
{XA∩△= xA∩△} has a multivariate Gaussian distribution. Firstly, it is clear that
πXA∩|XA∩△,XB

xA∩ | xA∩△, xB

= πXA∩|X△,XB∩

xA∩ | x△, xB∩

;
this is obtained simply by reorganising the sets of variables.
The conditional density function on the right hand side is obtained by conditioning
the distribution of XA∩ | X△= x△on XB∩ = xB∩. Since (XA∩, XB∩) | X△= x△
has a multivariate Gaussian distribution, and the conditional distribution of a multivariate
Gaussian, conditioning on some of its component variables is again multivariate Gaussian,
it follows that the conditional distribution is multivariate. The proof is complete.
□
8.9.1
CG potentials
Deﬁnition 8.9 (Conditional Gaussian Potential) A CG potential φ

x

, for x = (i, x) ∈
I × Rq, for a discrete space I (where x is a row vector) is any function of the form
φ

x

= χ(i)eg(i)+xh(i)−1
2 xK(i)xt
,
where χ takes the values 1 or 0 and K(i) is symmetric for each i ∈I.
A CG potential is said to have canonical parameters (g, h, K).
If , furthermore, K(i) is positive deﬁnite for each i ∼I, then the mean parameters are
deﬁned by
µ(i) = K(i)−1h(i),
C(i) = K(i)−1
and
pX△(i) = χ(i) exp

g(i) −1
2

log detK(i) −q log 2π −xK(i)xt

"
,
where q = ||.
The margins of a CG potential with respect to a subset of the continuous variables may be
computed in the standard way for multivariate Gaussian distributions, but for a potential,
the integral may not be 1.
8.9.2
Some results on marginalization
If the variables to be integrated out are discrete, then complicated mixture distributions
arise. The following proposition gives a special case.

250
GRAPHICAL MODELS AND EXPONENTIAL FAMILIES
Proposition 8.3 Let A ⊆˜V denote a subset of the indexing set for the variables. If X is
CG and B = ˜V \ A (namely, B is the set of all indices in ˜V that are not in A) and B ⊆△
and
XB ⊥X | X△\B,
then XA ∼CG.
Proof of Proposition 8.3 Clearly, from the deﬁnition of a CG distribution, it is necessary
and sufﬁcient to show that
XA∩ | X△\B ∼N|A∩|.
(multivariate normal, with dimension | A ∩ |). The proof requires the following identity:
If XB ⊥X | X△\B, then
πX|X△(x|X△) = πX|XB,X△\B(x|xB, x△\B) = πX|X△\B(x|x△\B).
This follows almost directly from the ﬁrst characterization of conditional independence
from Theorem 2.1. The result in Theorem 2.1 was stated for discrete variables; it is
straightforward to verify that it holds for conditional density functions. Recall that, from
the deﬁnition of a CG distribution, πX|X△

x | x△

is a multivariate normal distribution.
Therefore the conditional distribution of X conditioned on X△\B is multivariate Gaus-
sian, therefore the conditional distribution of X∩A conditioned on X△\B is multivariate
Gaussian. The proof is complete.
□
8.9.3
CG regression
Deﬁnition 8.10 (CG Regression) Let Z = (Z1, . . . , Zs) be a continuous random (row)
vector and let I be a discrete random (row) vector with probability function pI. Let I
denote the state space for I. If a random (row) vector Y = (Y1, . . . , Yr) has the property
that
Y | (I = i, Z = z) ∼Nr

A(i) + zB(i), C(i)

,
where for each i ∈I
• A(i) is a 1 × r row vector for each i ∈I,
• B(i) is an s × r matrix,
• C(i) is a positive semi-deﬁnite symmetric matrix,
then Y is said to follow a CG regression.
Let V denote a set of variables, containing both discrete and continuous variables, which
have been ordered so that the probability distribution may be factorized along a directed
acyclic graph G = (V, E). Let Xγ be a continuous variable, with parent set (γ ). Suppose
that X has a conditional Gaussian distribution. Then the conditional distribution for Xγ ,
conditioned on its parent nodes (γ ) is the CG regression
Xγ |  (γ ) ∼N

α(i) + zβ, σ(i)

,

CONDITIONAL GAUSSIAN DISTRIBUTIONS
251
where the discrete variables of (γ ) take values i and the continuous variables of (γ )
take values z. Here α(i) is a number, σ(i) > 0 and β is a column vector with dimension
equal to the dimension of the continuous component z so that zβ is a well deﬁned inner
product. Thus, the conditional density corresponds to a CG potential φ(i, z, xγ ), equal to
φ(i, z, xγ ) =
1
+
2πσ(i)
exp

−
(xγ −α(i) + zβ)2
2σ(i)
*
(8.17)
The canonical characteristics of this potential are easily found manipulating the expression
in Equation (8.17).
Notes
The material for Chapter 8 is taken mostly from M.J. Wainright and M.I. Jordan
[115]. It is developed further in [114]. Possible improvements to the lower bound
are proposed by K. Humphreys and D.M. Titterington in [116]. The book by
O. Barndorff-Nielsen [113] is the standard treatise of exponential families and the
required convex analysis. Conditional Gaussian distributions and their applications to
Bayesian networks are discussed in [67].

252
EXERCISES: GRAPHICAL MODELS AND EXPONENTIAL FAMILIES
8.10
Exercises: Graphical models and exponential families
1. Which of the following families of distributions are exponential families? Obtain the
minimal sufﬁcient statistics for those which are.
(a)
p(x|θ) =

1
10
x = 0.1n + θ,
n ∈{0, 1, . . . , 9}
0
otherwise
(b) The family of N(µ, µ) distributions, where µ > 0 (that is mean and variance
both µ)
(c) The family X|X ̸= 0, where X ∼Bi(n, p).
2. Prove Lemma 8.1.
3. Consider a one parameter exponential family, in canonical parameters, with proba-
bility function, or density function,
p(x|θ) = exp{θφ(x) −A(θ)}h(x).
Show that
E[φ(X)] = A′(θ)
and
Var(φ(X)) = A′′(θ).
4. Let (X1, X2, X3) be random variables, with joint probability function
p(x1, x2, x3|η) =
n!
x1!x2!x3!
3

j=1
pxi
i ,
x1 + x2 + x3 = n,
where p1 = η2, p2 = 2η(1 −η) and p3 = (1 −η)2 and 0 ≤η ≤1.
(a) Is this an exponential family?
(b) Obtain the minimal sufﬁcient statistic for θ.
(c) Compute the mean parameter in terms of η.
(d) Compute the Fenchel Legendre conjugate of the log partition function.
(e) Prove that the Kullback-Leibler divergence is given by
D(θ1|θ2) = A(θ2) −A(θ1) −⟨µ1, θ2 −θ1⟩.
˜D(µ1|θ2) = A(θ2) + A∗(µ1) −⟨µ1, θ2⟩
˜˜D(µ1|µ2) = A∗(µ1) −A∗(µ2) −⟨θ2, µ1 −µ2⟩.
State the deﬁnitions of the terms used in this equation.

GRAPHICAL MODELS AND EXPONENTIAL FAMILIES
253
(f) Compute the primal form of the Kullback-Leibler divergence D(θ1|θ2), where θ1
and θ2 are the canonical parameters. Compute the dual form, expressed in terms
of the mean parameters.
5. (Propositional Logic)
Consider a treatment for high blood pressure which, under
‘normal’ circumstances is effective in 9 cases out of 10. The patient may have
additional conditions, which cause the treatment to fail. If the patient has condition
R, it causes the treatment to fail with probability 1
7. If the patient has condition W, it
causes the treatment to fail with probability 1
4. If the patient has condition C, it causes
the treatment to fail with probability 1
3. If the patient has condition B, it causes the
treatment to fail with probability 1
2. Assume that all these factors act independently
on the probability that the treatment fails. Compute the probabilities for success and
failure for all possible combinations of the factors listed above.
6. (Mean Field Update)
Consider a probability function, given by
pX(x|θ) = exp



n

j=1
θ(j)x(j) +

(i,j)∈E
θ(i, j)x(j) −A(θ)


,
where θ = {(θ(j))n
j=1, (θ(j, k)), (j, k) ∈E}, E denotes the edge set and x ∈{0, 1}n.
Let q denote the probability function
qX(x|θ) = exp



n

j=1
θ(j)x(j) −AH(θ)


.
Let
A∗
H(µ) = sup
θ
{⟨µ, θ⟩−AH(θ).
(a) Prove that
A∗
H(µ) =
n

j=1
{µ(j) log µ(j) + (1 −µ(j)) log µ(j)} .
(b) Prove that
A(θ) ≥sup
µ



n

j=1
θ(j)µ(j) +

(j,k)∈E
θ(j, k)µ(j)µ(k) −A∗
H(µ)


.
(c) Consider the probability distribution
p(x1, x2, x3; θ) = exp



3

j=1
θ(j)xj + θ(1, 2)x1x2 + θ(1, 3)x1x3 −A(θ)


.

254
EXERCISES: GRAPHICAL MODELS AND EXPONENTIAL FAMILIES
Show that the expression in the previous part is maximized for (µ(1), µ(2), µ(3))
that satisfy
log
µ(1)
1 −µ(1) = θ(1) + θ(1, 2)µ(2) + θ(1, 3)µ(3)
log
µ(2)
1 −µ(2) = θ(2) + θ(1, 2)µ(1)
log
µ(3)
1 −µ(3) = θ(3) + θ(1, 3)µ(1).
(d) Write a MATLAB code to compute numerical approximations to the values
(µ(1), µ(2), µ(3)) that give the naive mean ﬁeld approximation to the log par-
tition function A(θ).
7. Let
X =

X△, X

∼CG(|△|, 1).
Let I denote the state space for X△and let p denote the probability function for the
random vector X△. Prove that
E [X] =

i∈I
p(i)µ(i)
and
Var (X) =

i∈I
p(i)σ(i)2 +

i∈I
p(i) (µ(i) −E [X])2 ,
stating clearly any results about multivariate normal random variables that you are
using.
8. Let X ∼CG(2, 2) and let I1 and I2 be binary variables. Find the canonical parameters
for the distribution.
9. Prove that if a conditional Gaussian distribution is marginalized over a subset of the
continuous variables, the resulting distribution is again a CG distribution. Find the
canonical characteristics of the marginal distribution in terms of the original canonical
characteristics, stating clearly any results about multivariate normal random variables
that you are using.
10. Suppose that hard evidence is entered into a subset of the continuous variables of
a CG distribution. Show that the updated distribution is again a CG distribution
and express the canonical characteristics of the updated distribution in terms of the
canonical characteristics of the original distribution, stating clearly any results about
multivariate normal random variables that you are using.

9
Causality and intervention
calculus
9.1
Introduction
Causality is a notion with a manipulative component. Wold R.H. Strotz and H.O.A. [117]
state:
‘. . . in common scientiﬁc usage, (causality) has the following general mean-
ing: z is the cause of y if, by the hypothesis that it is, or would be, possible,
by controlling z indirectly, to control y, at least stochastically’.
Hence, causal inference in this chapter is meant to answer to predictive queries about
the effect of a hypothetical or pondered manipulation or intervention.1 Causal predictive
inference requires a machinery to signify intervention, i.e. when one actively changes
the value of one or more of the variables. Examples of manipulations are medical treat-
ments and manual interceptions in an automatic controller. These change the states in
a data-generating mechanism by upsetting the normal forces working on it. This is the
basic principle of a ‘controlled experiment’. In order to assess whether or not a particular
variable has a causal effect on another, the values of that variable are assigned purely at
random, by the controller, without reference to any other factors.
1 There are also counter-factual causal inferences of the form of an explanation If and event A had not
occurred, then C would not have occurred, which are not explicitly covered here.
Bayesian Networks: An Introduction
T. Koski, J. Noble
2009 John Wiley & Sons, Ltd

256
CAUSALITY AND INTERVENTION CALCULUS
One argument against making causal inferences has been that statistics lacks the
language to model effects of intervention. In order to discuss effects of intervention in
the language of DAGs, an edge between two nodes in a Bayesian network is expressly
interpreted as a causal link. In other words, the states of a parent variable are said to
be direct causes, stochastically, of the states of a child variable. This is the sense in
which the discussion about causal Bayesian networks is to be understood. This does not
perhaps contribute to a deeper understanding of the generic concept of causality, but it is
sufﬁcient for predictive causal inference, and in this setting, the modelling assumptions
are transparent.
The present chapter introduces the ‘do-calculus’ of probability updating by interven-
tion: some of the variables in a causal Bayesian network are forced to take certain values,
rather than simply being observed to have these values. If the variable is forced into a
certain value, and then the conditional probabilities computed, this is known as ‘do’ con-
ditioning; while if the variable is simply observed, this is known as ‘see’ conditioning.
The ‘do-calculus’ is due to Judea Pearl in [118] and [1]. It enables conclusions to be
drawn about the effects of active interventions, based on passive observations.
The updating of probabilities based on ‘see’ conditioning (Bayes’ rule, Jeffrey’s rule)
are deﬁned without reference to any causal structure. The conditional probabilities are
estimated directly from observations and it is important that the variables are merely
observed and that they have not been forced. The updating associated with ‘do’ condi-
tioning depends upon the causal structure. It is the ‘do’ conditional probabilities that are
estimated in a controlled randomized experiment. The calculus described in this chapter
enables one to compute intervention probabilities from the conditional probabilities in
causal Bayesian networks.
Pearl’s ‘do-calculus’ describes how to treat a perfect intervention; namely, an inter-
vention which essentially ‘cuts off’ the inﬂuence of the parents to a node. Some real
world interventions may be modelled in this way; for example, gene knock outs. Many
interventions, however, are not so precise in their effects and Pearl’s calculus has been
extended by Eaton and Murphy in [119] to take this into account.
In times past, only a few expositions of statistics considered causal inference. A
notable exception is H.O.A. Wold [120]. Nowadays, there are discussions of causality in
statistical textbooks; for example, in D. Edwards [121], Chapter 9. Even though prob-
abilistic reasoning and statistical studies avoid claims that they have established causal
links and restrict claims to ‘correlation’ and ‘association’, statistical methods are rou-
tinely used incorrectly to justify causal inference from data, see D. Freedman [122]. D.V.
Lindley [123] quotes a statement by Terry Speed: ‘considerations of causality should
be treated as they have always been treated in statistics: preferably not at all (but if
necessary, then with great care).’ It should thus come as no surprise that the method-
ologies in Pearl [1] and Spirtes et al. [48] have met with critical reviews, for example,
[20] (Freedman and Humphreys), which should be taken seriously by every statistician
studying ‘do-calculus’.
One practical result of the theory of ‘do-calculus’ is that sets of confounding variables
(any common ancestor to two nodes in a causal DAG is a confounder) may be charac-
terized. That is, one may locate a sufﬁcient set of variables that, if they were known, one
could establish the correct causal effect between variables of interest.
It will be shown that a sufﬁcient set for estimating the causal effect of X on Y is
any set of non-descendants of X that d-separate X from Y after all the arrows emanating

CONDITIONING BY OBSERVATION AND BY INTERVENTION
257
from X have been removed. This criterion, known as the ‘back door criterion’, provides a
mathematical deﬁnition of confounding and helps to identify accessible sets of variables
that ought to be investigated.
Another effect of the theory of ‘do-calculus’ is to give a precise meaning to the well
known phrase learning by doing in the sense that there can be perfect interventions that
can resolve the question of which structure to use from a Markov equivalence class. In
practise, a controlled experiment provides a perfect intervention.
D.V. Lindley [123] notes that the do-calculus is well adapted for Bayesian use; the
separate assessments of p(Y | see(x)) and p(Y | do(x)) do not violate coherence. Lind-
ley also states that ‘the “do”-calculus is an extremely sensible method of developing
a calculus for controlled experiments.’ ‘Do’-calculus also has applications to situations
other than controlled experiments.
9.2
Conditioning by observation and by intervention
Suppose that X = x is observed. Then the conditional probability of Y = y can be
expressed, using Bayes’ rule, as
pY|X(y|x) = pX|Y(x|y)pY (y)
pX(x)
.
This formula describes the way that the probability distribution of the random variable
Y changes after X = x is observed. This is denoted by
p(Y = y|see(x)).
The causal probability calculus, also describes how to modify the probability distribution
of the random variable Y in the presence of an active intervention to force the random
variable X to take the value x, where the intervention is independent of the state of the
system. This is denoted by X ←x. The conditional distribution of Y, after an intervention
has forced the value x on the variable X, is denoted by
p(Y = y|do(x)) = p(y∥x) = p(Y = x|X ←x).
There is a distinction between these two types of conditioning. As P. Spirtes et al. [48]
put it:
‘How can an observed distribution p be used to obtain reliable predictions
of the effects of alternative policies that would impose a new marginal dis-
tribution on some set of variables? The very idea of imposing a policy that
would directly change the distribution of some variable . . . necessitates that
the resulting distribution pMAN will be different from p. p alone cannot be
used to predict pMAN, but p and a causal structure can be.’
Here ‘MAN’ is an abbreviation of ‘manipulate’.
This chapter shows how the causal structure of the Bayesian network may be used
to construct an intervention calculus.

258
CAUSALITY AND INTERVENTION CALCULUS
9.3
The intervention calculus for a Bayesian network
Notations
Consider a directed acyclic graph (DAG) G = (V, E) where the node set
V is ﬁnite. Let ˜V denote the indexing set for the nodes. For each v ∈˜V = {1, . . . , d},
the random variable Xv takes its values in a ﬁnite state space Xv = {x(1)
v , . . . , x(kv)
v
}. As
usual, j will denote the set of parent variables of variable Xj. The notation ˜j will be
used to denote the indexing set of the variables in j.
For a set of indices A, the space XA = ×v∈AXv.
The edges E in the graph represent the causal relationships between the variables,
so that a parent of Xv is a direct cause of Xv.
X ˜V = ×v∈˜V Xv = (X1, . . . , Xd),
while x ˜V will denote a value in X1 × . . . × Xd. That is,
x ˜V = ×d
v=1x(iv)
v
= (x(i1)
1
, . . . , x(id)
d
);
that is,
X ˜V = x ˜V ⇔(X1 = x(i1)
1
, . . . , Xd = x(id)
d
).
For any A ⊂˜V , XA is deﬁned as
XA = ×v∈AXv,
while xA will denote a value in ×v∈AXv. That is,
xA = ×v∈Ax(iv)
v
.
Since the superscripts are implied, they will often be omitted in the text.
The set difference, written
˜V \A,
is deﬁned as all the indices in ˜V which are not included in A. The following notation
will be used in situations where the ordering of the variables is not important:
x ˜V = ×d
v=1xv = ×v∈˜V \Axv ×v∈A xv = x ˜V \A.xA.
Let φ be a function deﬁned on X1 × . . . × Xd. Then the quantity 
˜V \A φ is deﬁned as


˜V \A
φ

(x ˜V ) =

x ˜V \A
φ(x ˜V \A.xA).
Deﬁnition 9.1 (The Intervention Formula) The conditional probability of X ˜V \A = x ˜V \A,
given that the variables XA were forced to take the values xA independently of all else, is
written
pX ˜V ∥XA(x ˜V |XA ←xA)
or
pX ˜V \A∥XA(x ˜V ∥xA)

THE INTERVENTION CALCULUS FOR A BAYESIAN NETWORK
259
and deﬁned as
pX ˜V \A∥XA(x ˜V |XA ←xA) = pX ˜V \A∥XA(x ˜V ∥xA)
=
pX ˜V (x ˜V )
	
v∈A pXv|v(x(iv)
v
|x ˜v)
=

v∈˜V \A
pXv|v(x(iv)
v
|x ˜v).
(9.1)
The function pX ˜V \A∥XA(.∥xA) deﬁnes a probability distribution over the space X ˜V \A, the
space where the variables on which no intervention has been made take their values.
This means instantiation of the variables indexed by the set A and elimination of those
edges in E which lead from the parents of the nodes indexed by A to the nodes indexed
by A. The terminology ‘local surgery’ is used to describe such an elimination. A local
surgery is performed and the conditional probabilities on the remaining edges are mul-
tiplied. This yields a factorization along a mutilated graph. The direct causes of the
manipulated variable are put out of effect.
The idea of deletion of connections (in terms of wiping out equations in a multivariate
model) is found in R.H. Strotz and H.O.A. Wold [117].
The quantity pX ˜V \A∥XA(.∥xA) from Deﬁnition 9.1 deﬁnes a family of probability
measures over X ˜V \A, which depending on the parameter xA, the values forced on the
variables indexed by A. This family includes original probability measure; if A = φ,
then
pX ˜V \A∥XA(.∥xA) = pX ˜V (.).
The family of probability measures deﬁned in Deﬁnition 9.1 is the intervention measure.
In addition, the ﬁnal expression on the right hand side of Equation (9.1) is called the
intervention formula. This formula is due to Pearl, but is also given independently in the
ﬁrst edition of P. Spirtes et al. [48]. See also C. Meek and C. Glymour [124], p. 1010.
Intervention
An ‘intervention’ is an action taken to force a variable into a certain state,
without reference to its own current state, or the states of any of the other variables. It
may be thought of as choosing the values x∗
A for the variables XA by using a random
generator independent of the variables X ˜V .
Remark
In the same style of notation, conditioning by observation is
pX ˜V \A|XA(x ˜V \A|see(xA)) = pX ˜V \A|XA(x ˜V \A|xA).
(9.2)
where, by the standard deﬁnition of conditional probability,
pX ˜V \A|XA(x ˜V \A|xA) =
pX ˜V (x ˜V )

y ˜V :yA=xA p(X1 = yi1, . . . Xd = yin).
(9.3)
Comparing Equation (9.1) with Equation (9.3) gives the following corollary.

260
CAUSALITY AND INTERVENTION CALCULUS
X
Y
Figure 9.1
A DAG for X having causal effect on Y.
Corollary 9.1 If Xi has no parents, i.e. if i = φ, then for all x ˜V \{i} ∈×j∈˜V \{i}Xj and
all y ∈Xi,
pX ˜V \{i}∥Xi(x ˜V \{i}∥y) = pX ˜V |Xi(x ˜V |y).
Example 9.1
Consider the DAG given in Figure 9.1, for ‘X having causal effect on Y’.
The factorization of pX,Y along the DAG in Figure 9.1 is
pX,Y (x, y) = pY|X(y|x)pX(x)
and the intervention formula gives
pY∥X(y∥x) = pY|X(y|x).
Since X is a parent of Y, intervening to force X = x transforms the distribution over Y
in exactly the same way as observing X = x. But if instead Y is forced, the intervention
formula yields
pX∥Y(x∥y) = pX(x).
Clearly, pX∥Y(x∥y) ̸= pX|Y(x|y) as functions unless X and Y are independent.
The causal graph in Figure 9.1 may be used to illustrate the following situation, where
X denotes ‘rain’, with values ‘yes’ or ‘no’, while Y denotes ‘barometer reading’, with
values ‘high’ or ‘low’.
• The reading in a barometer is useful to predict rain:
p (rain | barometer reading = high) > P (rain | barometer reading = low)
• But forcing the barometer will not cause rain:
P (rain | barometer ←high) = P (rain | barometer ←low)
Example 9.2: The DAG for a wet pavement
Consider the DAG given in Figure 9.2,
which represents a causal model for a wet pavement.
The season A has four states: spring, summer autumn, winter. Rain B has two states:
yes/no. Sprinkler C has two states: on/off. Wet pavement D has two states: yes/no.
Slippery pavement has two states: yes/no.
The joint probability distribution is factorized as
pA,B,C,D,E = pApB|ApC|ApD|B,CpE|D.

THE INTERVENTION CALCULUS FOR A BAYESIAN NETWORK
261
A
B
C
D
E
Figure 9.2
DAG for wet pavement, no intervention.
Suppose, without reference to the values of any of the other variables and without refer-
ence to the current state of the sprinkler, ‘sprinkler on’ is now enforced. Then
pA,B,C,D,E(.|C ←1) = pA,B,C,D,E(., ., 1, ., .)
pC|A(1|.)
= pApB|ApD|B,C(.|., 1)pE|D.
After observing that the sprinkler is on, it may be inferred that the season is dry and that
it probably did not rain and so on. If ‘sprinkler on’ is enforced, without reference to the
state of the system when the action is taken, then no such inference should be drawn in
evaluating the effects of the intervention. The resulting DAG is given in Figure 9.3. It is
the same as before, except that C = 1 is ﬁxed and the edge between C and A disappears.
The deletion of the factor pC|A represents the understanding that whatever relation-
ships existed between sprinklers and seasons prior to the action, found from
pA,B,D,E|C(., ., ., .|1)
are no longer in effect when the state of the variable is forced, as in a controlled exper-
iment, without reference to the state of the system. This is an example of the difference
between seeing and doing. After observing that the sprinkler is on, it may be inferred
A
B
C = 1
D
E
Figure 9.3
Sprinkler ‘on’ is forced.

262
CAUSALITY AND INTERVENTION CALCULUS
X
Z
Y
Z
Z
Y
X
Y
X
X
Z
Y
Z
Z
Y
X
Y
Figure 9.4
Three Markov equivalent graphs.
X
Z = z
Z = z
Z = z
Y
X
Y
X
Y
Figure 9.5
Graphs from Figure 9.4 with intervention Z ←z applied.
that the season is dry, that it probably did not rain and so on. No such inferences may
be drawn in evaluating the effects of the intervention ‘ensure that the sprinkler is on’.
9.3.1
Establishing the model via a controlled experiment
The three graphs in Figure 9.4 are Markov equivalent. The chains X →Z →Y and
X ←Z ←Y and the fork X ←Z →Y are all Markov equivalent, with conditional
independence structure X ⊥Y|Z. If there are causal relations between the variables,
then it is not possible to distinguish which of the models is appropriate from the data
alone.
If one of the graphs in Figure 9.4 represents an appropriate causal structure between
the variables, and if it is possible to intervene by controlling the variables Z, then it is
possible to distinguish which of the models is appropriate. Figure 9.5 shows the associated
structural model when the control Z ←z has been applied, forcing Z to be independent
of its ancestors. A controlled experiment, where the direct causal links between Z and its
parent variables have been eliminated, will exhibit independence structure X ⊥(Y, Z) in
the ﬁrst case, X ⊥Y|Z in the second (X, Z) ⊥Y in the third. The original experiment
determines the equivalence; the additional controlled experiment, if it is possible to carry
it out, will determine which graph within the equivalence class is appropriate.
9.4
Properties of intervention calculus
The following propositions summarize some basic properties of intervention calculus.
Proposition 9.1 If Xi has no parents, then for all x ˜V ∈X ˜V
pX ˜V \{i}∥Xi(x ˜V \{i}∥xi) = pX ˜V \{i}|Xi(x ˜V \{i}|xi).
□
Proposition 9.2 introduces the term exogeneity. A variable is exogenous to a model if it
is not determined by other parameters and variables in the model, but is set externally
and any changes to it come from external forces. In this context, it simply means that

PROPERTIES OF INTERVENTION CALCULUS
263
‘do’ conditioning on a variable affects the offspring in the same way that it would if the
variable took that value without the external intervention.
Proposition 9.2 (Exogeneity and Invariance) For each j = 1, . . . , d, let j denote the
set of parent variables for variable Xj, ˜j the indexing set for the variables in j,
so that X ˜j denotes the state space for j. For each variable in the variable set V =
{X1, . . . , Xd}, exogeneity holds, where exogeneity for variable Xj is deﬁned as follows:
for each (x, π) ∈Xj × X ˜j ,
pXj ∥j (x∥π) = pXj |j (x|π).
(9.4)
For all j = 1, . . . , d and each S ⊆˜V
such that S ∩({j} ∪{ ˜j}) = φ, modular-
ity/invariance holds. This is deﬁned as follows for each (x, π, xS) ∈Xj × X ˜j × XS,
pXj ∥j ,XS(x∥π, xS) = pXj |j (x|π).
(9.5)
Proof of Proposition 9.2 Equation (9.4) is established ﬁrst. pXj ∥j (.∥π) is a marginal
distribution which depends on the enforced value j ←π. For all (x, π) ∈Xj × X ˜j ,
pXj ∥j (x∥π) =

x ˜V \ ˜j |xj =x
pX ˜V \ ˜j ∥j (x ˜V \ ˜j ∥π).
An application of the intervention formula (9.1) yields
pXj ∥j (x∥π) =

x ˜V \ ˜j |xj =x


v∈˜V \ ˜j
pXv|v(xv|x ˜v)


......
(xj ,x ˜j )=(x,π)
.
Successive application of the distributive law, together with

x∈Xv
pXv|v(x|πv) = 1
for any πv ∈X ˜v gives
pXj ∥j (x∥π) = pXj |j (x|π)
for all (x, π) ∈Xj × X ˜j as required. The proof of Equation (9.5) may be carried out
by a similar marginalization.
□
The property described by Equation (9.5) expresses the notion of invariance, or modularity
found in Woodward [21]. Once all the direct causes of a variable Xj are controlled, no
other interventions will affect the probability of Xj.
The following property is another straightforward consequence of the deﬁnition.
Proposition 9.3 For any (x, π) ∈Xj × X ˜j ,
pj ∥Xj (π∥x) = pj (π).

264
CAUSALITY AND INTERVENTION CALCULUS
Proof of Proposition 9.3 By marginalization, followed by an application of the inter-
vention formula (9.1), for each (x, π) ∈Xj × X ˜j ,
pj ∥Xj (π∥x) =

x ˜V \({j}∪{j })
pX ˜V \({j}∪˜j ),∥X(x ˜V \({j}∪˜j , π∥x)
=

x ˜V \({j}∪˜j )
pX ˜V (xV )
pXj |j (x|π)
.......
(xj ,x ˜j )=(x,π)
=

x ˜V \({j}∪˜j ) pX ˜V (x ˜V )
pXj |j (x|π)
......
(xj ,x ˜j )=(x,π)
=
pXj ,j (x, π)
pXj |j (x|π)
= pj (π).
□
The probability measure after intervention is factorized along the mutilated graph. The
following proposition determines the probabilities on the mutilated graph.
Proposition 9.4 Let A ⊂V and let ˜A denote the indexing set for the variables in A. Then,
for Xj ̸∈A and any (x, x ˜j \ ˜A, xA) ∈Xj × X ˜j × XA,
pXj |j \A∥A(x|x ˜j \ ˜A∥x ˜A) = pXj |j \A,j ∩A(x|x ˜j \ ˜A, x ˜j ∩˜A),
where the conditioning is taken in the sense of: ﬁrst the ‘do’ conditioning X ˜A ←x ˜A is
applied and then the set of variables j\A is observed.
The causality calculus means here that the conditional speciﬁcations are unchanged for
variables which are not used for the intervention.
Proof of Proposition 9.4 By deﬁnition of conditional probability,
pXj |j \A∥A(x|x ˜j \ ˜A∥x ˜A) =
pXj ,j \A∥A(x, x ˜j \ ˜A∥xA)
pj \A∥A(x ˜j \ ˜A∥xA)
.
An application of the intervention formula to the numerator gives
pXj ,j \A∥A(x, x ˜j \ ˜A∥x ˜A) =

v∈{j}∪˜j\ ˜A
pXv|v(xv|x ˜v)
and to the denominator gives
pj \A∥A(x ˜j \ ˜A∥x ˜A) =

v∈˜j \ ˜A
pXv|v(xv|x ˜v).

TRANSFORMATIONS OF PROBABILITY
265
Putting these together clearly gives
pXj |j \A∥A(x|x ˜j \ ˜A∥x ˜A) = pXj |j \A,j ∩A(x|x ˜j \ ˜A, x ˜j ∩˜A),
as claimed. The proof is complete.
□
As Lauritzen [125] states, the intervention calculus is shown to have the property that
conditional speciﬁcations are unchanged for variables that are not used in the intervention.
Example
9.3:
Wet
pavement
revisited
Consider
the
conditional
probability
pD|B∥C(.|.∥1). Here B and C are parents of D, and in the notation of the preceding
proposition (using A in the sense of the previous proposition), D = Xj, j\A = B,
X ˜A = C and x ˜A = x ˜j ∩˜A = 1. Plugging into the formula in the preceding proposition,
pD|B∥C(.|.∥1) = pD|B,C(.|., 1).
The right hand side may be thought of as a pre-intervention probability, which can be
estimated from the data before the intervention C ←1 is made. In this case, an estimate of
the pre-intervention probability pD|B,C(.||., 1) is also an estimate of the post-intervention
probability pD|B∥C(.|.∥1).
9.5
Transformations of probability
The following proposition is almost a direct consequence of the deﬁnition. It presents a
simple rearrangement of the intervention formula in a special case.
Proposition 9.5
pX ˜V \{j}∥Xj (x ˜V \{j}∥x) = pX ˜V \({j}∪˜j )|Xj ,j (x ˜V \({j}∪˜j )|x, x ˜j )pj (x ˜j )
Proof of Proposition 9.5 An application of the deﬁnition gives
pX ˜V \{j}∥Xj (x ˜V \{j}∥x) =

v∈˜V \{j}
pXv|v(xv|x ˜j ).
One term has been removed in the product, namely, pXj |j (x|x ˜j ), so that (with xj = x)

v∈˜V \{j}
pXv|v(xv|x ˜j ) =
pX ˜V (x ˜V )
pXj |j (x|x ˜j )
=
pX ˜V (x ˜V )pj (x ˜j )
pXj ,j (x, x ˜j )
= pX ˜V \({j}∪˜j )|Xj ,j (x ˜V \({j}∪˜j )|x, x ˜j )pj (x ˜j )
as required.
□

266
CAUSALITY AND INTERVENTION CALCULUS
The effect of the intervention may be viewed as follows: all the conditional probability
potentials remain the same except for pXj |j . After the intervention Xj ←x, this is
replaced by the potential:
pXj |j (x|π) = 1
∀π ∈X ˜j ,
and
pXj |j (y|π) = 0
∀y ∈Xj\{x}, π ∈X ˜j .
Consider the probability over the remaining variables in V \{Xj} after conditioning on
{Xj = x}. The following two equations illustrate the differences in the way the probability
mass is distributed following ‘see’ conditioning on the one hand and ‘do’ conditioning
on the other.
pX ˜V \A|XA(x ˜V \A|see(xA)) = pX ˜V \A|XA(x ˜V \A|xA) =
pX ˜V (x ˜V )
pXA(xA) .
It follows that for any set of variables
pX ˜V \A∥XA(x ˜V \A∥do(xA)) =

v∈˜V \A
pXv|v(xv|x ˜v).
Proposition 9.6 (Adjustment for Direct Causes) Let B ⊂V be a set of random variables
in a DAG that are disjoint from {Xj} ∪j and let ˜B denote the indexing set for the
variables in B. Then for any (x, x ˜B) ∈Xj × X ˜B,
pB∥Xj (x ˜B∥x) =

X ˜j
pB|Xj,j (x ˜B|x, x ˜j )pj (x ˜j ).
(9.6)
Proof of Proposition 9.6 Firstly,
pB∥Xj (x ˜B∥x) =

X ˜V \( ˜B∪{j})
pV \{j}(x ˜V \{j}|x).
By Proposition 9.5, this may be written as
pB∥Xj (x ˜B∥x) =

X ˜V \( ˜B∪{j})
pV \({Xj }∪j )(x ˜V \({j}∪˜j )|x, x ˜j )pj (x ˜j ).
A marginalization over X ˜V \( ˜B∪{j}∪˜j ) gives
pB|Xj (X ˜B|x) =

X ˜j
pB|Xj ,j (x ˜B|x, x ˜j )pj (x ˜j )
as required. The proof is complete.
□
In Proposition 9.6, the ‘do’ probability is computed by ﬁrst ‘see’ conditioning on the
direct causes j of Xj and then averaging over them.

A NOTE ON THE ORDER OF ‘SEE’ AND ‘DO’ CONDITIONING
267
9.6
A note on the order of ‘see’ and ‘do’ conditioning
The causal calculus is well deﬁned for a causal network if ﬁrstly some of the variables
are forced, irrespective of the state of the network, and then some other variables are
observed. Consider a set of variables V = {X1, . . . , Xd}. Let A, B and C denote three
disjoint subsets of variables and ˜A, ˜B, ˜C their indexing sets. Conditioning is to be read
from right to left. The quantity
pA|B∥C(x ˜A|x ˜B∥x ˜C)
may be computed quite easily using the calculus developed so far by ﬁrst modifying the
network according to the ‘do’ conditioning C ←x ˜C, and then applying Bayes’ rule to
condition on {B = x ˜B} in the usual way.
In terms of the ‘causal’ interpretation deﬁned above, conditioning the other way
round, ﬁrst see B = x ˜B and then force C = x ˜C does not appear to be so well deﬁned. It
is difﬁcult to give it a ‘causal’ interpretation, particularly if there are variables in C that
are ancestors of variables in B. Mathematically, the ‘see’ followed by ‘do’ conditioning,
for a causal network, can be deﬁned, although the result may not have a practical value. If
the ‘see’ conditioning is carried out ﬁrst, then these variables can no longer be inﬂuenced
by the ‘do’ conditioned variables, even though the ‘do’ conditioned variables may be
ancestors.
If B = x ˜B is observed, these variables are now ﬁxed, so the states of the variables
in C can no longer have any inﬂuence. More precisely, to deﬁne ‘see’ followed by
‘do’ conditioning, one has to assume that the variables V = {X1, . . . , Xd} have a causal
order, (Xσ(1), . . . , Xσ(d)), where Xσ(1) has no ancestors and, for each j ∈{2, . . . , d},
the parent set is either empty, or is chosen from (Xσ(1), . . . , Xσ(j−1)). Furthermore, it
is assumed that the same causal order holds for the remaining random variables after a
‘see’ conditioning of the network. After the ‘see’ conditioning C = x ˜C, the conditional
probability potentials pXσ(j)|σ(j) are replaced, for σ(j) ̸∈˜C, by
˜pXσ(j)|σ(j)\C = pXσ(j)|σ(j)\C,C(.|., x ˜C).
The ‘do’ conditioning is then applied to this new network.
Example 9.4
Consider three variables (X1, X2, X3), where the probability distribution
may be factorized along the DAG given in Figure 9.6. Consider pX1|X3∥X2(x1|x3∥x2) and
pX1∥X2|X3(x1∥x2|x3), where the conditioning is taken from right to left.
X1
X2
X3
Figure 9.6
DAG for ‘see’ and ‘do’ example.

268
CAUSALITY AND INTERVENTION CALCULUS
X1
X2
Figure 9.7
DAG for ‘see’ and ‘do’ example, after ‘see’ conditioning.
In terms of the original tables,
pX1|X3∥X2(x1|x3∥x2) =
pX1(x1)pX3|X1,X2(x3|x1, x2)

y pX1(y)pX3|X1,X2(x3|y, x2).
(9.7)
If the ‘see’ conditioning X3 = x3 is applied ﬁrst, the network after this conditioning is
given by Figure 9.7. The conditional probability potentials, after conditioning on X3 = x3,
are given by ˜pX1 and ˜pX2|X1, which are given in terms of the original potentials in
Equations (9.8) and (9.9).
˜pX1(.) = pX1|X3(.|x3) =

z pX1(.)pX3|X1,X2(x3|., z)pX2|X1(z|.)

y,z pX1(y)pX3|X1,X2(.|y, z)pX2|X1(z|y)
(9.8)
˜p(X2|X1) = p(X2|X1, X3 = x3) =
p(X1)p(X2|X1)p(X3 = x3|X1, X2)

y p(X1)p(X2 = y|X1)p(X3 = x3|X1, X2 = y).
(9.9)
Now, the application of the ‘do X2 ←x2’ conditioning breaks the causal link between
X1 and X2, so that
pX1∥X2|X3(x1∥x2|x3) = ˜pX1(x1) =

z pX1(x1)pX3|X1,X2(x3|x1, z)pX2|X1(z|x1)

y,z pX1(y)pX3|X1,X2(x3|y, z)pX2|X1(z|y) . (9.10)
The formula is clearly different from the formula for pX1|X3∥X2(x1|x3∥x2), which is given
by Equation (9.7).
Although a mathematical deﬁnition may be given to applying a ‘see’ conditioning
ﬁrst, it is difﬁcult to see how to make sense of this in terms of causality.
9.7
The ‘Sure Thing’ principle
The following result is taken from [1], p. 181, where Pearl refers to it as the ‘Sure Thing’
principle.
Proposition 9.7 Consider three binary variables A, B, C with the network given in Figure
9.10. If
pB|C∥A(1|1∥1) < pB|C∥A(1|1∥0)
and
pB|C∥A(1|0∥1) < pB|C∥A(1|0∥0)
then
pB∥A(1∥1) < pB∥A(1∥0).
The notation means: ﬁrst A is forced, then C is observed.

THE ‘SURE THING’ PRINCIPLE
269
Proof of Proposition 9.7 Firstly,
pB∥A(1∥1) = pB|C∥A(1|1∥1)pC∥A(1∥1) + pB|C∥A(1|0∥1)pC∥A(0∥1).
Since C is a parent of A,
pC∥A(.∥1) = pC(.).
It follows that
pB∥A(1∥1) =
1

x=0
pB|C∥A(1|x∥1)pC∥A(x∥1) =
1

x=0
pB|C∥A(1|x∥1)pC(x).
Similarly,
pB∥A(1∥A0) =
1

x=0
pB|C∥A(1|x∥0)pC(x).
It now follows directly from the assumptions that
pB∥A(1∥1) < pB∥A(1∥0),
as advertized.
□
Simpson’s paradox resolved by a controlled experiment
Consider three binary vari-
ables, A, B and C. Simpson’s paradox is the observation that there are situations where
pB|C,A(1|1, 1)/pB|C,A(0|1, 1)
pB|C,A(1|1, 0)/pB|C,A(0|1, 0) > 1
and
pB|C,A(1|0, 1)/pB|C,A(0|0, 1)
pB|C,A(1|0, 0)/pB|C,A(0|0, 0) > 1,
but pB|A(1|1)/pB|A(0|1)
pB|A(1|0)pB|A(0|0) < 1. For example, suppose that A denotes ‘treatment’, B denotes
‘recovery’ and C denotes ‘gender’. With no information about the causal relations
between the variables, Simpson’s paradox states that even if the ‘treatment’ may improve
the chances of recovery for both men and women, it may nevertheless be bad for the
population as a whole.
If the model is that shown in Figure 9.8, where A denotes ‘treatment’, (1 = applied,
0 = no treatment), C ‘gender’ (male/female), B recovery (yes/no) and the value of A is
randomly assigned to each individual so that a proper controlled experiment is carried
out, then the causal link between C and A is broken. The ‘sure thing’ principle may then
be applied, which states that if the treatment improves the chances of recovery for both
men and women, it is good for the population as a whole.
C
A
B
Figure 9.8
A = treatment, B = recovery, C = gender.

270
CAUSALITY AND INTERVENTION CALCULUS
C
A
B
Figure 9.9
A = treatment/B = recovery/C = blood pressure.
Now consider the DAG given in Figure 9.9, where A denotes treatment, B recovery
and C blood pressure.
Suppose that ‘treatment’ is randomly assigned to individuals in an appropriate manner
so that a controlled experiment is carried out. In this model, blood pressure is on the
causal pathway inﬂuencing recovery. The controlled experiment has not removed the
direct causal link between A and B and hence the possibility of Simpson’s paradox
cannot be excluded. It could be that although the treatment is comparatively good within
the group where high blood pressure is observed after treatment and also comparatively
good within the group where low blood pressure is observed after treatment, it may
be bad for the population as a whole. This could happen if ‘treatment’ increases blood
pressure and increased blood pressure reduces the chances of recovery.
9.8
Back door criterion, confounding and identiﬁability
Given a causal Bayesian network and observational data, where the values of the vari-
ables in a set A have been forced (due, for example, to a controlled experiment), the
task is to estimate the conditional probability distribution over the remaining variables;
pV \A∥A

x ˜V \ ˜A∥x ˜A

. The problem is simpliﬁed if pV \A∥A

x ˜V \ ˜A∥x ˜A

can be modiﬁed until
no ‘do’ operations appear, so that the required conditional probability potentials may be
estimated using observational data, thus reducing a causal query to a probabilistic query.
Confounding
Consider the DAG given in Figure 9.10.
The factorization is
pA,B,C = pB|A,CpA|CpC.
Consider pB∥A(.∥a). Note that
pB∥A(.∥a) =

c∈XC
pB,C∥A(., c∥a)
and that
pB,C∥A(., .∥a) = pB|C∥A(.|.∥a)pC∥A(.∥a) = pB|A,C(.|a, .)pC,
C
A
B
Figure 9.10
Illustration for Confounding.

BACK DOOR CRITERION, CONFOUNDING AND IDENTIFIABILITY
271
where in the second term, the ‘do’ conditioning of A ←a is applied ﬁrst, and then C is
observed. It follows that
pB∥A(.∥a) =

c∈XC
pB|A,C(.|a, c)pC(c).
This shows that to estimate pB∥A(.∥a) from data, it is necessary to be able to estimate
the potentials pB|A,C and pC. If C is observable, then the effect on the probability
potentials of B of manipulating A may be estimated. But if C is a hidden random
variable (sometimes the term latent is used) in the sense that no direct sample of the
outcomes of C may be obtained, it will not be possible to estimate the probabilities used
on the right hand side and hence it will not be possible to predict the effect on B of
manipulating A. This is known as confounding.
Semi-Markovian model
The model described above is an example of a semi-
Markovian model. Let V = {X1, . . . , Xd} and suppose that the probability distribution
over the variables V may be factorized along a directed acyclic graph G = (V, E). Now
suppose that V = U ∪Z, where U is a set of unobserved variables, while Z is the set of
observed variables. Assume that in G, no variable in U is a descendant of any variable
in Z. Such a model is known as a semi-Markovian model, following [126]. Let ˜Z and ˜U
denote the indexing sets for the variables in Z and U respectively. The joint distribution
of the observed variables becomes a mixture of products of conditional probabilities,
pZ

x ˜Z

=

X ˜U

v∈˜Z
pXv|v

xv | x ˜v

pU

x ˜U

.
(9.11)
Now consider an intervention ZT ←xZT , where ZT ⊂Z. Then the ‘do’ conditional
probability for the remaining variables in Z is
pZ

x ˜Z
 =

X ˜U

v∈˜Z\ ˜ZT
pXv|v

xv | x ˜v
 pU

x ˜U
 .
(9.12)
The question of identiﬁability, considered next, is whether it is possible to express
pZ\ZT ∥ZT (x ˜Z\ ˜ZT ∥x ˜ZT )
uniquely as a function of the observed distribution pZ

x ˜Z

; that is, without involving
either the unknown conditional probability tables PZv|v for v ∈{1, . . . , d} such that
v ∪U ̸= φ or the unknown distribution pU

x ˜U

.
Back door criterion
Recall that there are three basic types of connection in a DAG:
chain, collider and fork. Any sequence of nodes with edges between successive nodes,
regardless of direction, is known as a trail. Two subsets of nodes A and B are d-separated
by a set of nodes C if on all trails between a node in A and B there is an intermediate
node X such that
• either the connection is a chain or a fork and X ∈C
• or the connection is a collider and neither X nor any of its descendants are in C.

272
CAUSALITY AND INTERVENTION CALCULUS
The notation
A ⊥B∥GC
denotes that A and B are d-separated by C. That is, C blocks every trail from a node
in A to a node in B; if all the nodes of C are instantiated, then there are no active
trails between A and B. Theorem 2.2 states that A ⊥B∥GC implies that A ⊥B|C. That
is, if the probability distribution factorizes along the graph G, then d-separation implies
conditional independence.
Deﬁnition 9.2 (Back Door Criterion) A set of nodes C satisﬁes the back door criterion
relative to an ordered pair of nodes (Xi, Xj) ∈V × V if
1. no node of C is a descendant of Xi and
2. C blocks every trail (in the sense of d-separation) between Xi and Xj which con-
tains an edge pointing to Xi.
If A and B are two disjoint subsets of nodes, C is said to satisfy the back door criterion
relative to (A, B) if it satisﬁes the back door criterion relative to any pair (Xi, Xj) ∈
A × B.
The name ‘back door criterion’ reﬂects the fact that the second condition requires that
only trails with nodes pointing at Xi be blocked. The remaining trails can be seen as
entering Xi through a back door.
Example 9.5
Consider the back door criterion DAG, given in Figure 9.11. The sets
of variables C1 = {X3, X4} and C2 = {X4, X5} satisfy the back door criterion relative
to the ordered pair of nodes (Xi, Xj), whereas C3 = {X4} does not satisfy the criterion
relative to the ordered pair of nodes (Xi, Xj); if X4 is instantiated, the Bayes ball may
pass through the collider connection from X1 to X2.
Identiﬁability
Suppose that X1, . . . , Xn and Z are sets of variables in a Bayesian
network and that Z satisﬁes the back door criterion with respect to (Xi, Xj). The aim is
to show that the set of variables Z plays a similar role to the variable C in the discussion
on confounding. Firstly, since no variables of Z are descendants of Xi, it follows that
X1
X2
X3
X4
X5
Xi
X6
Xj
Figure 9.11
Back door criterion.

BACK DOOR CRITERION, CONFOUNDING AND IDENTIFIABILITY
273
pZ∥Xi(.|xi) = pZ(.). This is seen as follows: ﬁrst, marginalise over all variables that are
descendants of Xi. Now consider the resulting reduced DAG, where all descendants of
Xi have been eliminated. The result follows directly, by noting that since no variables of
the set Z are descendants of Xi, and the conditional probability tables for the Bayesian
network remain unchanged except that the causal links between the set of variables
i ∪{Xi} removed. It follows that
pXj ∥Xi(.∥xi) =

z∈XZ
pXj ,Z∥Xi(., z∥xi)
=

z∈XZ
pXj |Z∥Xi(.|z∥xi)pZ∥Xi(z∥xi)
where the conditioning is to be taken that ﬁrst Xi →xi is imposed and then Z is
observed. Since Z d-separates Xi from Xj via any trail with arrow pointing into Xi, it
follows that the same probability tables are used for the computation of pXj |Z∥Xi(.|.∥xi)
and pXj |Z,Xi(.|., xi), from which
pXj |Z∥Xi(.|.∥xi) = p(Xj|Z, Xi = xi),
so that
pXj ∥Xi(.∥xi) =

z∈XZ
pXj |Z,Xi(.|z, xi)pZ(z).
(9.13)
If a set of variables Z satisfying the back door criterion with respect to (Xi, Xj) can
be chosen such that pZ and pXj |Z,Xi can be estimated from the observed data, then the
distribution pXj ∥Xi can also be estimated from the observed data.
Deﬁnition 9.3 (Identiﬁability) If a set of variables Z satisﬁes the back door criterion
relative to (X, Y), then the causal effect of X to Y is given by the formula
pY∥X(.∥x) =

z∈XZ
pY|X,Z(.|x, z)pZ(z)
(9.14)
and the causal effect of X on Y is said to be identiﬁable.
The formula given in Equation (9.14) is named adjustment for concomitants. The word
identiﬁability refers to the fact that the existence of the concomitants Z satisfying the
back door criterion makes it possible to compute, or identify, pY∥X(y∥x) uniquely from
any p which is strictly positive over V .
Notes
In the main, Chapter 9 presents material found in [125], [1] and [121]. The paper
[127] summarizes the recent developments in the problem of identiﬁability and presents
an algorithmic solution. The identiﬁability question asks whether it is possible to compute
the probability of some set of effect variables given an intervention on another set of

274
CAUSALITY AND INTERVENTION CALCULUS
variables in the presence of non-observable variables, using data that is not obtained
from a controlled experiment. The results by Y. Huang, M. Valtorta in [127] show that
the do-calculus rules of J. Pearl [118] and [128] are complete in the sense that if a
causal effect is identiﬁable, then there exists a sequence of applications of the ‘do’ rules
that transforms the causal effect formula to a formula that only contains observational
quantities. The philosophical paper [129] argues for Bayesian networks as the proper
representation of stochastic causality.

CAUSALITY AND INTERVENTION CALCULUS
275
9.9
Exercises: Causality and intervention calculus
1. Consider a Bayesian network with the following DAG
A
B
C
Figure 9.12
Directed acyclic graph.
where A, B and C are binary variables (i.e. taking values either 0 or 1), together with
probabilities
pA =
1
2, 1
2

pB|A =
A\B
1
0
1
0.75
0.25
0
0.25
0.75
pC|B =
B\C
1
0
1
0.125
0.875
0
0.675
0.375
(a) Compute pB∥C(1∥1).
(b) Compute pA∥C(1∥1).
(c) Compute pA∥B(1∥1)
(d) Compute pC∥B(1∥1)
2. Consider the joint probability table for the three variables A, B and C.
pA,B,C =
C
A
B
1
0
1
1
0.15
0.22
0
0.04
0.09
0
1
0.1
0.03
0
0.26
0.11
Compute pA,B, pB, pB,C and pC|A,B(1|., .). Compare your answers with pC|B(1|.).
Show that this is an example of Simpson’s paradox.
Suppose that the values in the table for pA,B,C are the ‘empirical’ probabilities
obtained from 400 observations. The following model is considered to be appropriate.
C
A
B
Figure 9.13
Directed acyclic graph.

276
EXERCISES: CAUSALITY AND INTERVENTION CALCULUS
(a) Which conditional probabilities do we need for this Bayesian network?
(b) What are the estimates of pA,B,C obtained using these estimates for the conditional
probability?
(c) How many parameters does this model have?
(d) Let x denote the empirical probabilities and y denote the ﬁtted probabilities.
Calculate
dK(x, y).
If the ﬁtted model holds, then one would expect
400dK(x, y) ∼χ2
1.
The reason: the empirical distribution has seven parameters, while the ﬁtted model
has six parameters. For large numbers of observations, the Kullback-Leibler
distance is approximately 
j
(xj −yj )2
xj
. Multiply by the number of observations
to obtain the χ2 statistic. Degrees of freedom is the difference in number of
parameters.
(e) Assuming that the probabilities given in the table for p(A, B, C) are exact, and
the distribution factorizes according to the DAG given in Figure 9.14, compute
pC|A∥B(1|1∥1)
pC|A∥B(1|1∥0)
pC|A∥B(1|0∥1)
pC|A∥B(1|0∥0)
and compare with pC∥B(1∥1) and pC∥B(1∥0).
A
B
C
Figure 9.14
DAG for A, B, C.
3. Let A, B, C, W be disjoint sets of nodes in a Bayesian network. Let G denote the
directed acyclic graph describing the causal network, and let G−C denote the graph
with all edges between C and parents of C removed.
Prove that if A and B are d-separated by (C, W) on the graph G−C, then
pA|W,B∥C(xA|xW, xB, ∥xC) = pA|W∥C(xA|xW∥xC),
where the conditioning is performed from right to left.

CAUSALITY AND INTERVENTION CALCULUS
277
4. Let G be a directed acyclic graph, and suppose that a probability distribution p may be
factorized along G. Let G−X denote the graph obtained by deleting from G all arrows
pointing towards X (that is, all links between X and its parents are deleted). Prove
that if Y and Z are d-separated in G−X by X, then
pY|Z∥X(.|.∥x) = pY∥X(.∥x),
where the conditioning is taken from right to left.
5. Suppose the causal relations between the variables (X1, X2, X3, X4, X5, X6, Y, Z) may
be expressed by the DAG given in Figure 9.15. Prove that C1 = {X1, X2} and C2 =
{X4, X5} satisfy the back door criterion relative to the ordered pair of nodes (Y, Z),
while C3 = {X4} does not. Which sets of nodes satisfy the back door criterion with
respect to the ordered set of nodes (Z, Y)?
X1
X2
X3
X4
X5
Y
X6
Z
Figure 9.15
Causal relations between variables.
6. Let a set of variables C satisfy the back door criterion relative to (X, Y). Prove that
pY∥X(y∥x) =

c
pY|C∥X(y|c∥x)pC∥X(c∥x).
7. Let C be a set of variables in a Bayesian network and let X be a variable such that
C contains no descendants of X. Prove, from the deﬁnition, that
pC∥X(c∥x) = pC(c).
8. Let V = {X1, . . . , Xd} denote a set of variables. Let V = Z ∪U, where the variables
in Z are observable and the variables in U are unobservable. Assume that the proba-
bility distribution over the variables in V may be factorized along a directed acyclic
graph G = (V, E), where no variable in U is a descendant of any variable in Z; that
is, the model is semi-Markovian. Consider a single variable, say Xj ∈Z. Assume that
there exists no fork of the form {(Xi, Xj), (Xi, Xk)}, where Xk ∈Z and j ̸= k, and
Xi ∈U is an unobservable variable. That is, there are no confounders between Xj
and the rest of the observable variables. Then show that
pZ\{Xj}∥Xj (x ˜Z\{j}∥xj) = pZ\({Xj}∪j )|Xj ,j

x ˜Z\({j}∪˜j )|xj, x ˜j

pj (x ˜j ).


10
The junction tree and probability
updating
A Bayesian network presents a factorization of a probability distribution according to
a directed acyclic graph. Chapter 4 introduced the basic material about decomposable
graphs that enabled the construction of a junction tree from a directed acyclic graph.
Trees often provide the basis of efﬁcient algorithms and in this chapter, the junction
tree is used as the basis for developing a method of updating the probability distribu-
tion.
The probability updating task is the following: let X = (X1, . . . , Xd) denote a set of
variables, with a probability function pX that factorizes along a directed acyclic graph to
form a Bayesian network. The task is to compute the conditional probability distribution
of X given some evidence (Deﬁnition 3.1) that is entered into the network. Chapter 10
develops a method based on the junction tree and applies it for hard evidence received
in a network where all the variables are discrete. Section 10.9 extends the technique to
hard evidence received for a conditional Gaussian (CG) distribution (deﬁned in Section
8.9) and Section 10.10 to virtual and soft evidence.
10.1
Probability updating using a junction tree
Let e = (e1, . . . , em) denote hard evidence potential (Deﬁnition 3.1); that is, a collection
of hard ﬁndings. A hard ﬁnding is deﬁned as an instantiation of a variable in the network;
for each j ∈{1, . . . , m}, ej is a potential containing 0s and 1s corresponding to the
instantiation. The task is to compute the conditional distribution pX|e given the hard
Bayesian Networks: An Introduction
T. Koski, J. Noble
2009 John Wiley & Sons, Ltd

280
THE JUNCTION TREE AND PROBABILITY UPDATING
evidence e. Following Equation (3.3), the conditional distribution is deﬁned as
pX|e = pX;e
p(e),
where
p(e) =

x∈X
pX;e(x)
and
pX;e = pX
m

j=1
ej.
The main object of this chapter is to illustrate how this may be carried out effectively
using the following procedure:
1. Moralize the DAG of the Bayesian network.
2. Triangulate the moralized graph.
3. Let the cliques of the triangulated graph be the nodes of a tree, which is a junction
tree.
4. Use the Markov properties of the probability distribution to associate potentials to
the separators and nodes of the junction tree.
5. Propagate (i.e. send messages to update the potentials on the separators and nodes
of the junction tree) through the junction tree.
For producing a tree that is computationally efﬁcient, the most important step of the
construction is the triangulation of the moralized graph. There are many ways to add
edges to triangulate a graph and it is important, with large networks, to ﬁnd a method
that is optimal. An efﬁcient triangulation will lead to a junction tree that produces the
smallest possible largest clique size, and should ﬁnd the triangulation within polynomial
time.
An algorithm for constructing a triangulation that is close to optimal, when the state
spaces (Xv)d
v=1 are the same size for each variable in (X1, . . . , Xd) is found in [130].
In general, probabilistic inference is an NP-hard problem. The complexity of the
inference techniques are not discussed in this text; an analysis of the complexity is found
in [82].
10.2
Potentials and the distributive law
Notations
The following paragraph repeats various notations. Let
˜V = {1, . . . , d}
denote the indexing set for the d nodes for a graph G = (V, E). To each node is
associated a random variable. To each node j ∈˜V , is associated a ﬁnite state space
Xj = (x(1)
j , . . . , x
(kj )
j
), the set of possible states for the random variable Xj. The state
space of X = (X1, . . . , Xd) is denoted by X = ×d
j=1Xj. Let D ⊂˜V denote a subset of
the nodes. The notation
xD = ×v∈Dxv

POTENTIALS AND THE DISTRIBUTIVE LAW
281
is used to denote a conﬁguration (or a collection of outcomes) on the nodes in D. The
state space of the variables in D is denoted by
XD = ×v∈DXv.
Suppose D ⊆W ⊆˜V and that xW ∈XW. That is, xW = ×v∈Wxv. Then, ordering the
variables of W so that XW = XD × XW\D, the projection of xW onto D is deﬁned as the
variable xD that satisﬁes
xW = (xD, xW\D),
where the meaning of the notation ‘(, )’ is clear from the context. Here A\B denotes the
set difference, i.e. the elements in the set A not included in B.
Deﬁnition of a potential and charge
Let
 = {φ1, . . . , φm}
be a set of non-negative real valued functions on X. The functions φj ∈ are called
potentials. The set of potentials  is known as a charge. For each j = 1, . . . , m, XDj
will denote the state space for potential φj, while the set Dj ⊂˜V denotes the set of
indices for the argument variables of φj. The set XDj is called the domain of φj.
The joint probability function pX1,...,Xd is itself a potential, with domain X. If the
joint probability function may be factorized according to a DAG G = (V, E), the decom-
position is written as
pX1,...,Xd =
d
j=1
pXj |j .
Then for each j = 1, . . . , d, φj deﬁned by φj(xv, x ˜j ) = pXj |j (xj|x ˜j ) is a potential
with domain XDj = Xj × X ˜j and Dj = {j} ∪˜j.
Example 10.1
Consider a probability function over six variables that may be factorized
along the directed acyclic graph in Figure 10.1.
The potentials corresponding to the conditional probabilities are
φ1 = pX1, φ2 = pX2|X1, φ3 = pX3|X1,
φ4 = pX4|X2, φ5 = pX5|X2,X3, φ6 = pX6|X3.
X1
X2
X3
X4
X5
X6
Figure 10.1
A Bayesian network on six variables.

282
THE JUNCTION TREE AND PROBABILITY UPDATING
The corresponding domains are
XD1 = X1
XD2 = X2 × X1
XD3 = X3 × X1
XD4 = X4 × X2
XD5 = X5 × X2 × X3
XD6 = X6 × X3.
Deﬁnition 10.1 (Contraction) Recall Deﬁnition 2.26 for multiplication of potentials. A
contraction of a charge, or set of potentials is an operation of multiplication and division
of potentials, after extending them to X, that returns a function over X.
For example,
(x) =
m

j=1
φj(x)
is a contraction, when the φj have ﬁrst been expanded to the domain XD = X∪m
j=1Dj .
In other settings, the charge may contain potentials of two types;
 = {φ1, . . . , φm1, ψ1, . . . , ψm2}
and
(x) =
	m1
j=1 φj(x)
	m2
j=1 ψj(x)
is a contraction, where the domains of all the potentials have been extended to X before
the operations of multiplication and division were applied.
The same notation is often used to denote the contraction of a charge and of the set
of potentials (the charge). The context makes it clear which is intended.
Evidence potentials
Suppose that a random variable Xv is known to be instantiated
with the value y. This is a piece of hard evidence and may be expressed as an evidence
potential, which is a table containing 1s and 0s. Using notation x = (x1, . . . , xd), this
evidence may be expressed as a potential e(y)
v
over the domain X deﬁned as
e(y)
v (x) =

1
xv = y
0
xv ̸= y.
(10.1)
Let U ⊆V and let ˜U denote the indexing set for the variables in U. An evidence potential,
denoted by eU, is deﬁned for y ∈XU, x ∈X, where the components of y are indexed by
˜U, as
e
(y)
U (x) =

v∈˜U
e(yv)
v
(x),

POTENTIALS AND THE DISTRIBUTIVE LAW
283
where ev is deﬁned by Equation (10.1). The interpretation is that eU is a ﬁnding in the
sense of seeing; not in the sense of doing.
For a Bayesian network with the nodes indexed by ˜V = {1, . . . , d} and joint proba-
bility distribution factorized recursively as
pX =
d
v=1
pXv|v,
the quantity pX;e
(y)
U
= pX.e
(y)
U , is obtained by multiplication of tables to give
pX;e
(y)
U
(x) =
d
v=1
pXv|v(xv|x ˜v)

v∈U
e(yv)
v
(x).
The product is zero for all x ∈X such that xU ̸= y; that is, if xv ̸= yv for some v ∈˜U.
It follows that, for ﬁxed y ∈XU, pX;e
(y)
U
may be considered as a potential with domain
XV \U.
10.2.1
Marginalization and the distributive law
Recall the discussion of marginalization in Section 2.5. The distributive law is used when
marginalizing a product of potentials. It can be written as follows: let φ1 be a potential
with domain XD1 and let φ2 be a potential with domain XD2. Suppose that A ⊂D1 ∪D2
and the product φ1φ2 (Deﬁnition 2.26) is to be marginalized over XA. If A ∩D1 = φ
(the empty set), then

XA
φ1φ2 = φ1

XA
φ2.
More particularly, suppose that φ1 has domain XD1∪D3 and φ2 has domain XD2∪D3∪D4,
where D1, D2, D3 and D4 are disjoint. In coordinates, the distributive law may be
written as

x2∈XD2
φ1(x1, x3)φ2(x2, x3, x4) = φ1(x1, x3)

x2∈XD2
φ2(x2, x3, x4).
The effect of the distributive law is that the potential over XD1 × XD3 × XD4 is ﬁrst
marginalized down to a function over XD3 × XD4. The function is transmitted to the
function over XD2 × XD3, to which it is multiplied. The domains of the two functions to
be multiplied have to be extended to XD1 × XD3 × XD4. Using X1, X2, X3, X4 to denote
the associated domains XD1, XD2, XD3 and XD4, the domains under consideration for the
operations are illustrated in Figure 10.2. First, the potential φ2, deﬁned over (X2, X3, X4)
is considered. This is marginalized to a potential over (X3, X4) and is then extended, by
multiplying with φ1, to a potential over (X1, X3, X4).
Example 10.2: A marginalization
Consider the computation for marginalizing a con-
traction of a charge  deﬁned over a state space X = X1 × X2 × X3 × X4 × X5 where
(x) = φ1(x1, x3, x5)φ2(x1, x2)φ3(x3, x4)φ4(x5, x6).

284
THE JUNCTION TREE AND PROBABILITY UPDATING
(X2, X3, X4)
(X3, X4)
(X1, X3, X4)
Figure 10.2
The distributive law.
X2
X6
X1
X5
X3
X4
Figure 10.3
Associations of variables.
More particularly, consider the computation of
↓0 =

x∈X
(x),
where the notation ↓U is deﬁned in the discussion of marginalization in Section 2.5.
The bucket elimination method may be used, with (for example) the order of summation:
x2, x4, x6, x5, x3, x1. The sum may be written as

x1∈X1

x3∈X3

x5∈X5
φ1(x1, x3, x5)

x6∈X6
φ4(x5, x6)

x4∈X4
φ3(x3, x4)

x2∈X2
φ2(x1, x2).
The computation, carried out in this order, may be represented by the graph in Figure 10.3.
A computational tree, according to the distributive law, is given in Figure 10.4.
10.3
Elimination and domain graphs
The notations described at the beginning of Section 2.5 will be used. Let U denote a
subset of V and set W = V \U. Let ˜U, ˜W and ˜V denote the indexing sets of U, W and
V respectively. Consider the computation of
↓U(x) =



xV \U ∈XV \U
(xV \U, xU)


for a contraction
(x) =
m

j=1
φj(xDj ).

ELIMINATION AND DOMAIN GRAPHS
285
X1, X2
X5, X6
X1
X5
X1, X3, X5
X3
X3, X5
Figure 10.4
A computational tree for the marginalization.
Recall that the operation ↓U(x) means marginalizing  over all variables not
in the set U. The variable xv, with index v ∈˜W = ˜V \ ˜U
is eliminated
from

xV \U ∈XV \U (xV \U, xU)
by
the
following
procedure,
where
contraction
means
multiplying together all the potentials in the charge.
1. Let v (or Xv) denote the contraction of the potentials in  that have Xv in
their domain; that is,
v =

j|v∈Dj
φj.
2. Let φ(v) (or φ(Xv)) denote the function 
xv∈Xv v.
3. Find a new set of potentials −v (or −Xv) by setting
−v = ( ∪{φ(v)})\v.
This is the deﬁnition of −v, also denoted by −Xv. Those potentials that do not contain
Xv in their domain have been retained; the others have been multiplied together and then
marginalized over Xv (thus eliminating the variable) to give φ(v). This potential has been
added to the collection, and all those containing Xv (other than φ(v)) have been removed.
Note that the notation −Xv has two meanings: it is used to denote the collection
of potentials, and it is also used to denote the contraction of the charge obtained by
multiplying together the potentials in the collection. The meaning is determined by the
context. Having removed Xv, it remains to compute

xW\{Xv}
−Xv(xU, xW\{Xv}).
Proposition 10.1 Let  be a contraction over a domain XW and let U ⊂W. The quantity
↓U(xU) =

xW\U ∈XW\U
(xW\U, xU)

286
THE JUNCTION TREE AND PROBABILITY UPDATING
can be computed through successive elimination of the variables
Xv ∈W\U.
Proof of Proposition 10.1 By the commutative laws for multiplication and adding,
together with the distributive law, the elimination of Xv gives
↓U(xU) =

xW\{Xv}∈XW\{Xv}
−Xv(xU, xW\{Xv}).
By the argument given above, −Xv is the contraction of a potential over XW\{Xv}
and it is clear, by induction, that the marginal can be computed through successive
elimination.
□
The problem, of course, is to ﬁnd an elimination sequence which gives as small
elimination domains as possible. Elimination domains were deﬁned in Deﬁnition 4.17
and, in this context, the elimination domain is the union of the domains of potentials in
 having Xv in their domain. This is facilitated by considering the domain graph.
Deﬁnition 10.2 (Domain Graph) The domain graph for the set of potentials in  is an
undirected graph with the variables as nodes and the links between any pair of variables
which are members of the same domain.
Figure 10.5 illustrates the domain graph associated with Figure 10.1. Figure 10.3 illus-
trates the domain graph associated with Figure 10.4. Note here that the domain graph is
the moral graph. It is clear that the domain graph for any Bayesian network is the moral
graph, since by deﬁnition all the parents are connected to each other and to the variable.
Eliminating a node
Let G = (V, E) be an undirected graph, where V = {X1, . . . , Xd}.
Recall Deﬁnition 4.15; eliminating a node. When a node Xv is eliminated from the graph
G, the resulting graph is denoted by G−Xv. If G is the domain graph for a set of potentials
, then it is clear from Deﬁnition 4.15 that the graph G−Xv is the domain graph for the
set of potentials −Xv.
Junction trees
Let
 = {φ1, . . . , φm}
X1
X2
X3
X4
X5
X6
Figure 10.5
Domain graph of Bayesian network in Figure 10.1.

ELIMINATION AND DOMAIN GRAPHS
287
be a set of potentials on X. Let G be a triangulated graph. A junction tree for  is a
junction tree for G such that
• Each φj is associated with a clique Cj = {Xj1, . . . Xjk} such that XCj , the domain
of φj is the state space of the variables {Xj1, . . . , Xjk}.
• Each edge is labelled by a separator consisting of the nodes remaining after elimi-
nation in a clique by an elimination sequence.
Figure 4.9 shows a directed acyclic graph, Figure 4.10 shows its moralized version,
Figure 4.11 shows a triangulation of the moral graph and ﬁnally, Figure 4.13 shows a
junction tree constructed from the directed acyclic graph in Figure 4.9.
Summary: Constructing an inference algorithm1
• Take a Bayesian network and ﬁnd its domain graph G.
• Triangulate the domain graph: G →G′.
• Find an elimination sequence in G′.
• The elimination sequence determines the elimination domains.
• The elimination domains are cliques.
• Organize the cliques into a junction tree (which is possible, following the results
of Chapter 4).
• Associate the potentials to the junction tree.
It remains to describe a scheme of message passing (propagation) for the task of marginal-
ization to compute
pX|eU (x)↓A =



x ˜V \A∈X ˜V \A
pX|eU (xA, x ˜V \A)

.
The task may be performed computationally using ‘HUGIN’.2 HUGIN propagation for
the task is based on representing joint distribution of a Bayesian network using the
so-called Aalborg formula
pX(x1, . . . , xd) =
	
C∈C φC(xC)
	
S∈S φS(xS) ,
where
C = cliques of the triangulated moral graph
1 This algorithm is sometimes referred to as an inference engine.
2 http://www.hugin.com/
HUGIN is a product from the HUGIN EXPERT A/S, a software company, with head ofﬁce in the town of
Aalborg in Denmark, developing intelligent solutions in areas such as information management, data mining,
decision analysis, troubleshooting, decision support, prediction, diagnosis, risk management, safety assessment,
control systems, all based on Bayesian networks.

288
THE JUNCTION TREE AND PROBABILITY UPDATING
and
S = separators of the junction tree
and each φC and φS is the potential over the respective clique C and separator S. The
propagation presented is the approach of Lauritzen and Spiegelhalter, discussed in [131];
the technicalities differ slightly from the implementation in HUGIN.
10.4
Factorization along an undirected graph
Let G = (V, E) be an undirected graph, where V = {X1, . . . Xd} is a set of discrete
variables.
Deﬁnition 10.3 A joint probability pX over a random vector X = (X1, . . . , Xd) is said
to be factorized according to G if there exist potentials or factors, φA deﬁned on ×v∈˜AXv
where A is a complete set of nodes in G, and ˜A is the index set for A, such that
pX(x) =

A
φA(xA)
where the notation is clear (see Section 2.5); the product is over all the potentials.
Joint probability distributions factorized along a graph are known as Markov probability
distributions and a corresponding Markov property will be established later on. If the
factorization holds, then clearly, for any W ⊂V ,
pXW (xW) =

xV \W ∈XV \W

A
φA(xW, xV \W),
where, for each A, the domain of φA has ﬁrst been extended to X.
Recall Deﬁnition 4.5 of a separator and Deﬁnition 4.12 of a decomposition. In the
deﬁnition, A, B or S may be the empty set, φ.
Proposition 10.2 Let G be a decomposable undirected graph and let (A, B, S) decompose
G. Then the following two statements are equivalent:
1. p factorizes along G and
2. both pA∪S and pB∪S factorize along GA∪S and GB∪S respectively and
p(x) = pA∪S(xA∪S)pB∪S(xB∪S)
pS(xS)
.
Proof of Proposition 10.2, 1) ⇒2) Since the graph is decomposable, its cliques can
be organized as a junction tree. Hence, without loss of generality, the factorization can
be taken to be of the form
p(x) =

K∈C
φK(xK),
where the product is over the cliques of G. Since (A, B, S) decomposes G, any clique of
G can either be taken as a subset of A ∪S or as a subset of B ∪S. Furthermore, S is a

FACTORIZATION ALONG AN UNDIRECTED GRAPH
289
strict subset of any clique of A ∪S containing S and S is a strict subset of any clique
of B ∪S containing S. Letting K denote a clique, it follows that
p(x) =

K⊆A∪S
φK(xK)

K⊆B∪S
φK(xK).
Since S is itself complete, it is a subset of any clique containing S, so that no clique in
the decomposition will appear in both A ∪S and B ∪S. Set
h(xA∪S) =

K⊆A∪S
φK(xK)
and
k(xB∪S) =

K⊆B∪S
φK(xK),
then
p(x) = h(xA∪S)k(xB∪S)
and the marginal distribution is given by
pA∪S(xA∪S) =

XB
h(xA∪S)k(xB∪S).
The distributive law now yields
pA∪S(xA∪S) = h(xA∪S)

XB
k(xB∪S) = h(xA∪C)k(xS),
where
k(xS) :=

XB
k(xB∪S).
Similarly,
pB∪S(xB∪S) = k(xB∪S)h(xS),
where
h(xS) :=

XA
h(xA∪S).
It follows that
p(x) = h(xA∪S)k(xB∪S) = p(xA∪S)p(xB∪S)
k(xS)h(xS)
.
Furthermore,
pS(xS) =

XA∪S
h(xA∪S)k(xB∪S) =

XA
h(xA∪S)

XB
k(xB∪S) = h(xS)k(xS).
It follows that
p(x) = pA∪S(xA∪S)pB∪S(xB∪S)
pS(xS)
.

290
THE JUNCTION TREE AND PROBABILITY UPDATING
In the course of the proof, it has also been shown that pA∪S and pB∪S are factorizable
along the corresponding graphs. This establishes the proof of 1) ⇒2).
□
Proof of Proposition 10.2, 2) ⇒1) This is clear.
□
The Markov property
By a recursive application of the proposition, together with
p(x) =

C⊆A∪S
φC(xC)

C⊆B∪S
φC(xC),
it follows that
p(x) =
	
C∈C pC(xC)
	
S∈S pS(xS) ,
where C denotes the set of cliques and S denotes the set of separators, and thus the
desired Markov property has been established.
□
This result may be extended to more general undirected graphs and the following proposi-
tion for factorization of probability distributions on undirected graphs has been proved by
Lauritzen in [132]. The generality is unnecessary for the scope of this text and therefore
the proof is omitted.
Proposition 10.3 Let G = (V, E) be an undirected graph and let p be a probability dis-
tribution over the variable set V . If p(x) > 0 for all x ∈X, then p may be factorized along
G if and only if for any sets of variables A, B, S ⊂V such that A and B are separated
by S,
A ⊥B|S
(that for any sets A and B separated by S, the sets of variables A and B are conditionally
independent given S).
□
This property is known as the undirected global Markov property.
10.5
Factorizing along a junction tree
Let p be a probability distribution that factorizes along a directed acyclic graph G =
(V, E). The factorization is given by
pX(x) =
d
v=1
pXv|v(xv|x ˜v),
where ˜j denotes the indexing set for the parent set j. It is clear that this may be
expressed as a factorization according to the moralized graph Gmor, which is undirected:
pX(x) =
d
v=1
φAv(xAv)

FACTORIZING ALONG A JUNCTION TREE
291
where A = {Xv} ∪v and
φAv(xAv) = pXv|v(xv|x ˜v).
Hence a probability distribution factorized along the DAG is also factorized along the
moral graph Gmor and the global Markov property is seen to hold on Gmor. For implement-
ing algorithms, the problem is that it may not be possible to represent the sets (Av)d
v=1
on a tree. To enable this, Gmor is triangulated to give (Gmor)t. Recall that (Gmor)t is
decomposable and its cliques can be organized into a junction tree T. Then
pX(x) =

C∈C
φC(xC),
where φC(xC) is the product of all those p(xv|x ˜v), all of whose arguments belong to
C. By moralization, there is always one such clique. Note that this factorization is not
necessarily unique. It follows that
pX(x) =
	
C∈C pC(xC)
	
S∈S pS(xS) ,
(10.2)
where C denotes the set of cliques and S denotes the set of separators of (Gmor)t. Fur-
thermore, the cliques of the expression in Equation (10.2) may be organized according
to a junction tree. This is the deﬁnition of a factorization along a junction tree.
Deﬁnition 10.4 (Factorization along a Junction Tree, Marginal Charge) Let pX be a prob-
ability distribution over a random vector X = (X1, . . . , Xd). Suppose that the variables
can be organized as a junction tree, with cliques C and separators S such that pX has
representation given in Equation (10.2), where pC and pS denote the marginal probability
functions over the clique variables C ∈C and separator variables S ∈S respectively. The
representation in Equation (10.2) is known as the factorization along the junction tree,
and the charge
 = {pS : S ∈S, pC : C ∈C}
is known as the marginal charge.
From the foregoing discussion, it is clear that Deﬁnition 10.4 is a special case of Deﬁnition
10.3, where the potentials are appropriately deﬁned.
Entering evidence
Equation (10.2) expresses the prior distribution in terms of potentials
over the cliques and separators of (Gmor)t, or the junction tree. Suppose that new hard
evidence eU is obtained on the variables U; namely, that for U ⊆V , {XU = yU} and
the probability over the variables V \U has to be updated accordingly. Then
pX;eU (x) =

	
C∈C pC(xC)
	
S∈S pS(xS)
xU = yU
0
xU ̸= yU.
Now, set φC(xC) = pC(xC\U, yC∩U) and φS(xS) = pS(xS\U, yS∩U). Then
pXV \U ,XU (xV \U, yU) =
	
C∈C φC(xC)
	
S∈S φS(xS) .
(10.3)

292
THE JUNCTION TREE AND PROBABILITY UPDATING
The posterior distribution
pX ˜V \U |XU (.|yU) =
pX ˜V \U ,XU (., yU)
pXU (yU)
may then be computed by ﬁnding a representation of the function pXV \U ,XU (., yU) over
domain XV \U using the algorithm deﬁned below. The algorithm such that for any function
f : X →R+ (not necessarily a probability function) that is expressed as
f (x) =
	
C∈C φC(xC)
	
S∈S φS(xS) ,
(10.4)
for a collection of potentials  = {φC, C ∈C, φS, S ∈S} where C and S are the cliques
and separators of a junction tree, the algorithm updates  to a collection of potentials
∗= {φ∗
C, C ∈C, φ∗
S, S ∈S} that satisfy
φ∗
C(xC) =

z∈XV \C
f (z, xC)
and
φ∗
S(xS) =

z∈XV \S
f (z, xS)
for each C ∈C and each S ∈S. It follows that the probability of the evidence is
pXU (yU) =

z∈XC\(U∩C)
φ∗
C(z, yU∩C) =

z∈XS\(U∩S)
φ∗
S(z, yU∩S)
for all S ∈S and all C ∈C. The conditional probability distribution over the remaining
variables V \U may therefore be computed by marginalizing the clique or separator with
the smallest domain, giving a representation of the conditional distribution in terms of
marginal distributions over the cliques and separators.
10.5.1
Flow of messages initial illustration
Consider a non-negative function with domain X × Y × Z, F : X × Y × Z →R+, which
may be written as
F(x, y, z) = f (x, z)g(y, z)
h(z)
,
(10.5)
for potentials f : X × Z →R+, g : Y × Z →R+ and h : Z →R+.
The decomposition shown in Equation (10.5) for the function F is of the form given
in Equation (10.4). The graph illustrating the associations between variables is given
in Figure 10.6, with cliques C1 = {X, Z}, C2 = {Z, Y} and separator S = {Z} arranged
according to the junction tree in Figure 10.7.
The following algorithm returns a representation F(x, y, z) = F1(x,z)F2(y,z)
F3(z)
, where
F1(x, z) =

y∈Y
F(x, y, z),
F2(y, z) =

x∈X
F(x, y, z),
F3(z) =

(x,y)∈X×Y
F(x, y, z).

FACTORIZING ALONG A JUNCTION TREE
293
X
Z
Y
Figure 10.6
Undirected graph for the three variables.
XZ
Z
ZY
Figure 10.7
Junction tree for message passing.
Firstly,
F1(x, z) =

y∈Y
F(x, y, z) =

y
f (x, z)g(y, z)
h(z)
= f (x, z)
h(z)

y∈Y
g(y, z).
Deﬁne
the
auxiliary
function
h∗(z) = 
y g(y, z),
and
the
update
f ∗(x, y) =
f (x, y) h∗(z)
h(z) , then clearly
f ∗(x, z) = f (x, z)h∗(z)
h(z) = F1(x, z).
The calculation of the marginal function F1(x, z) by means of the auxiliary function
h∗(z) may be described as being done by passing a local message ﬂow from ZY to XZ
through their separator Z. The factor
h∗(z)
h(z)
is called the update ratio. It follows that
F(x, y, z) = f (x, z)g(y, z)
h(z)
= f (x, z)g(y, z)h∗(z)
h∗(z)h(z)
= F1(x, z)
1
h∗(z)g(y, z).
The passage of the ﬂow has resulted in a new representation of F(x, y, z) similar to the
original, but where one of the factors is a marginal function.
Similarly, a message can be passed in the other direction, i.e. from XZ to ZY Using
the same procedure, set
˜h(z) =

x∈X
F1(x, z) =

(x,y)∈X×Y
F(x, y, z) = F3(z).
Next, set
˜g(y, z) = g(y, z)
˜h(z)
h∗(z).
It then follows that ˜g(y, z) = F2(y, z).
This follows because
F(x, y, z) = F1(x, z) 1
˜h(z) ˜g(y, z) = F1(x, z)
1
F3(z) ˜g(y, z)

294
THE JUNCTION TREE AND PROBABILITY UPDATING
and hence, since F3(z) = 
x∈X F1(x, z), it follows that
F2(y, z) =

x∈X
F(x, y, z) = ˜g(y, z)

x∈X
F1(x, z)
1
F3(z) = ˜g(y, z).
□
Passing messages in both directions results in a new overall representation of the function
F(x, y, z);.
F(x, y, z) = f ∗(x, z)
1
h∗(z)g(y, z) = f ∗(x, z)
1
h∗(z)
h∗(z)
˜h(z) ˜g(y, z)
= f ∗(x, z) 1
˜h(z) ˜g(y, z)
= F1(x, z)
1
F3(z)F2(y, z).
The original representation using potentials has been transformed into a new representa-
tion where all the potentials are marginal functions.
The idea is now extended to arbitrary non-negative functions represented on junction
trees.
10.6
Local computation on junction trees
Consider a junction tree T with nodes C and separators S and let  be a charge
 = {φC : C ∈C, φS : S ∈S},
(10.6)
that is, a collection of potentials such that φC : XC →R+ and φS : XS →R+ for each
C ∈C and each S ∈S.
Deﬁnition 10.5 (Contraction of a Charge on a Junction Tree) The contraction of a charge
(Equation (10.6)) over a junction tree is deﬁned as
f (x) =
	
C∈C φC(xC)
	
S∈S φS(xS) .
(10.7)
Local message passing
Let C1 and C2 be two adjacent neighbouring nodes in T sepa-
rated by S0. Set
φ∗
S0(xS0) =

z∈XC1\S0
φC1(z, xS0)
(10.8)
and set
λS0 =
φ∗
S0(xS0)
φS0(xS0)
(10.9)

LOCAL COMPUTATION ON JUNCTION TREES
295
C1
S0
C2
lS 0
f*
C 2 = lS 0fC2
→
Figure 10.8
Flow from C1 to C2.
Note that, directly from the deﬁnition of division of potentials (Deﬁnition 2.24), λS0 = 0
for φS0 = 0. The update ratio is deﬁned as the quantity λS0. The ‘message passing’ is
deﬁned as the operation of updating φS0 to φ∗
S0 and φC2 to
φ∗
C2 = λS0φC2.
(10.10)
All other potentials remain unchanged. The scheme of local message passing is illustrated
in Figure 10.8.
Lemma 10.1 Let f : X →R+ be the contraction of a a charge  = {φS, S ∈
S, φC, C ∈C} on a junction tree (Deﬁnition 10.5), where C is the collection of cliques
and S the collection of separators.
A ﬂow does not change the contraction of the charge.
Proof of Lemma 10.1 The initial contraction is given by
f (x) =
	
C∈C φC(xC)
	
S∈S φS(xS) .
(10.11)
Firstly, recall Deﬁnition 2.24. By the deﬁnition of division of potentials, f (x) = 0 for all
x such that φS(xS) = 0 for some S ∈S. This is part of the deﬁnition in the hypothesis
that f has a representation of the form given in Equation (10.11). Let the charge, after
the ﬂow from C1 to C2, be denoted by
∗= {φ∗
C : C ∈C, φ∗
S : S ∈S}
and the contraction
f ∗(x) :=
	
C∈C φ∗
C(xC)
	
S∈S φ∗
S(xS) .
(10.12)
Note that
f ∗(x) =
φ∗
C2(xC2) 	
C∈C,C̸=C2 φC(xC)
φ∗
S0(xS0) 	
S∈S,S̸=S0 φS(xS) .
(10.13)
There are three cases to consider.
• For x such that φS0(xS0) > 0 and φ∗
S0(xS0) > 0,
φ∗
C2
φ∗
S0
= φC2λS0
φ∗
S0
=
φC2

φ∗
S0
φS0

φ∗
S0
= φC2
φS0
and the result is proved.

296
THE JUNCTION TREE AND PROBABILITY UPDATING
• For second case and the third case, the results follow from the convenient arrange-
ment of the deﬁnitions for division by zero in the sense of tables. For x such that
φS0(xS0) = 0, f (x) = 0, from the deﬁnition. Furthermore, from Equation (10.9),
λS0 = 0 (this follows from division by zero in the sense of division of potentials)
and hence, by the deﬁnition of φ∗
C2, it follows that that φ∗
C2 = 0. It therefore follows
from Equation (10.12) that f ∗(x) = 0, so that 0 = f ∗(x) = f (x).
• For x such that φS0(xS0) > 0, but φ∗
S0(xS0) = 0, it follows directly from Equation
(10.13) using the deﬁnition of division by zero in the sense of potentials) that
f ∗(x) = 0. It remains to show that f (x) = 0. From the deﬁnition,
0 = φ∗
S0(xS0) =

z∈XC1\S0
φC1(z, xS0).
Since φC1(xC1) ≥0 for all xC1 ∈XC1, it follows that φC1(z, xS0) = 0 for all z ∈
XC1\S0. Since
f (x) = φC1(xC1)
φC2(xC2)
φS0(xS0)
	
C∈C,C̸=C1,C2 φC(xC)
	
S∈S,S̸=S0 φS(xS)
,
it follows directly from the facts that the domains of the cliques other than C1 and
C2 and separators other than S0 do not include XS0, and that
φC2(xC2)
φS0(xS0) < +∞that
f (x) = 0, hence f (x) = f ∗(x).
In all cases, it follows that a ﬂow does not change the contraction of a charge.
□
Having shown how to update for a simple example, and having shown that message
passing does not alter the contraction of a charge, it remains to schedule the message
transmissions in an efﬁcient way for a general junction tree to update the potentials over
the cliques and separators.
10.7
Schedules
The aim of this section is to describe how to construct a series of transmissions between
the various cliques of a junction tree, to update a set of potentials, whose contraction is
a probability distribution, to the posterior probability distributions over the cliques and
separators. First, some deﬁnitions and notations are established.
Deﬁnition 10.6 (Sub-tree, Neighbouring Clique) A sub-tree T ′ of a junction tree T is a
connected set of nodes of T together with the edges in T between them.
A clique C of a junction tree T is a neighbour of a sub-tree T if the corresponding
node of T is not a node of T ′ but is connected to T ′ by an edge of T.
The following deﬁnition gives the technical terms that will be used.
Deﬁnition 10.7 (Schedule, Active Flow, Fully Active Schedule) A schedule is an ordered
list of directed edges of T specifying which ﬂows are to be passed and in which order.

SCHEDULES
297
A ﬂow is said to be active relative to a schedule if before it is sent the source has
already received active ﬂows from all its neighbours in T, with the exception of the sink;
namely, the node to which it is sending its ﬂow. It follows that the ﬁrst active ﬂow must
originate in a leaf of T. This leaf serves as a root for the junction tree. A schedule is full
if it contains an active ﬂow in each direction along every edge of the tree T. A schedule
is active if it contains only active ﬂows. It is fully active if it is both full and active.
Example 10.3
Figures 10.9 and 10.10 depict a DAG and the corresponding junction
tree.
A fully active schedule for the junction tree given in Figure 10.10 would be:
AT →ELT, BLS →BEL, BDE →BEL, EK →ELT, ELT →BEL
BEL →ELT, ELT →EK, ELT →AT, BEL →BLS, BEL →BDE.
Deﬁnition 10.8 (Lazy Propagation) The method of updating a distribution that factorizes
over a junction tree is known as lazy propagation.
Proposition 10.4 For any tree T, there exists a fully active schedule.
Proof of Proposition 10.4 If there is only one clique, the proposition is clear; no trans-
missions are necessary. Assume that there is more than one clique. Let C0 denote a leaf
in T. Let T0 be a sub-tree of T obtained by removing C0 and the corresponding edge S0.
Assume that the proposition is true for T0. Adding the edge
C0 →S0 →T0
A
S
T
E
L
B
K
D
Figure 10.9
Example of a DAG.
AT
T
BLS
BL
ELT
EL
E
BEL
BE
EK
BDE
Figure 10.10
Corresponding junction tree.

298
THE JUNCTION TREE AND PROBABILITY UPDATING
to the beginning of the schedule and
C0 ←S0 ←T0
to the end of the schedule provides a fully active schedule for T.
□
The aim of a substantial part of the remainder of the chapter is to show that if the con-
traction of a charge  on a junction tree (Deﬁnition 10.5) is a probability distribution,
then after the passage of a fully active schedule of ﬂows over a junction tree, the resulting
charge is the marginal charge. That is, all the potentials of the charge are probability
functions over the respective cliques and separators. Furthermore, there is global consis-
tency after the passage of a fully active schedule of ﬂows over a junction tree. This will
be deﬁned later, but loosely speaking, it means that if there are several apparent ways
to compute a probability distribution over a set of variables using the potentials of the
marginal charge, they will all give the same answer.
Deﬁnition 10.9 (The Base of a Sub-tree, Restriction of a Charge, Live Sub-tree) Let T ′
be a sub-tree of T, with nodes C′ ⊆C and edges S′ ⊆S. The base of T ′ is deﬁned as the
set of variables
U ′ := ∪C∈C′C.
Let
 = {φC : C ∈C, φS : S ∈S}
be a charge for T. Its restriction to T ′ is deﬁned as
T ′ = {φC : C ∈C′, φS : S ∈S′}.
Recall Deﬁnition 10.5. The contraction of T ′ is deﬁned as
	
C∈C′ φC(xC)
	
S∈S′ φS(xS) .
A sub-tree T ′ is said to be live with respect to the schedule of ﬂows if it has already
received active ﬂows from all its neighbours.
Proposition 10.5 Let
0 = {φ0
C : C ∈C, φ0
S : S ∈S}
denote an initial charge for a function f that has factorization
f (x) =
	
C∈C φ0
C(xC)
	
S∈S φ0
S(xS)
where C and S are the sets of cliques and separators for a junction tree T. Suppose that
0 is modiﬁed by a sequence of ﬂows according to some schedule. Then, whenever T ′ is
live, the contraction of the charge for T ′ is the margin of the contraction f of the charge
for T on U ′.

SCHEDULES
299
Proof of Proposition 10.5 Assume that T ′ ⊂T and that T ′ is live. Let C∗denote that
last neighbour to have passed a ﬂow into T ′. Let T ∗be the sub-tree obtained by adding
C∗and the associated edge S∗to T ′. Let C∗, S∗and U ∗be the cliques, separators and
the base of T ∗. By the junction tree property of T, the separator associated with the edge
S∗joining C∗to T ′ is
S∗= C∗∩U ′.
Furthermore,
C∗= C′ ∪{C∗}
and
S∗= S′ ∪{S∗}.
and the set of base variables for T ∗is
U ∗= U ′ ∪C∗.
The induction hypothesis
The assertion holds for the contraction of the charge on T ∗.
Set
fU∗(xU∗) =

U\U∗
f (x).
Then the inductive hypothesis states that
fU∗(xU∗) =
	
C∈C∗φC(xC)
	
S∈S∗φS(xS) .
(10.14)
Let
 = {φC : C ∈C, φS : S ∈S}
denote the charge just before the last ﬂow from C∗into T ′. Lemma 10.1 states that a ﬂow
does not change the contraction of a charge. This is applied to the contraction of the
charge restricted to T ∗which, from Equation (10.14) is given by
fU∗(xU∗) = φC∗(xC∗)
φS∗(xS∗)
	
C∈C′ φC(xC)
	
S∈S′ φS(xS) .
(10.15)
Set
αU′ =
	
C∈C′ φC(xC)
	
S∈S′ φS(xS) ,
then Equation (10.15) may be rewritten as
fU∗(xU∗) = φC∗(xC∗)
φS∗(xS∗) αU′.
(10.16)
The aim is to ﬁnd the margin fU′ of f on U ′ and to show that after the ﬂow,
fU′(xU′) =
	
C∈C′ φC(xC)
	
S∈S′ φS(xS) .
Note that
U ′ ⊂U ∗⊂U,

300
THE JUNCTION TREE AND PROBABILITY UPDATING
so that
U\U ′ = (U\U ∗) ∪(U ∗\U ′).
Since the variables may be summed in any order,

XU\U′
f =

X(U\U∗)

X(U∗\U′)
f =

X(U∗\U′)


X(U\U∗)
f

=

X(U∗\U′)
fU∗.
(10.17)
It follows that

XU∗\U′
fU∗= αU′

XU∗\U′
φC∗(xC∗)
φS∗(xS∗) .
(10.18)
Since S∗= C∗∩U ′ and U ∗= U ′ ∪C∗, it follows that
U ∗\U ′ = C∗\S∗,
so that

XU∗\U′
φC∗(xC∗)
φS∗(xS∗) =

C∗\S∗
φC∗(xC∗)
φS∗(xS∗) =
1
φS∗(xS∗)

C∗\S∗
φC∗(xC∗) = φ∗
S∗(xS∗)
φS∗(xS∗)
(def )
= λS∗.
Recall Equation (10.9); λS∗is the update ratio. This, together with Equation (10.18),
may be applied to Equation (10.17) to give

XU\U′
f = αU′.λS∗.
But after the ﬂow into T ′, αU′ is updated as
α∗
U′ = λS∗αU′,
because the potential φC∗over the nearest neighbour C∗in T ′ is updated to λS∗φC∗(the
update deﬁned in Equation (10.10)). Hence, using φ∗
C∗to denote the update of φC∗and
φ∗
S∗to denote the update of φS∗and using the inductive hypothesis that a ﬂow does not
alter the contraction of a charge on T ∗, it follows that
fU∗(xU∗) = φC∗(xC∗)
φS∗(xS∗) αU′ = φ∗
C∗(xC∗)
φ∗
S∗(xS∗) α∗
U′.
Recall Equation (10.8); the potential over the separator φS∗(xS∗) is updated to
φ∗
S∗(xS∗) =

z∈XC∗\S∗
φC∗(z, xS∗).
Since the ﬂow is from C∗to T ′, φ∗
C∗= φC∗. It follows that

XU∗\U′
fU∗(xU∗) =

z∈XC∗\S∗
φC∗(xC∗)
φ∗
S∗(xS∗) α∗
U′ = α∗
U′,

SCHEDULES
301
from which it follows that after the ﬂow,

U\U′
f =
	
C∈C′ φC(xS)
	
S∈S′ φS(xS) ,
which is the deﬁnition of the contraction of the charge on T ′. It follows that after the
ﬂow, the contraction of the charge on T ′ is 
U\U′ f , as required. The proof is complete.
Corollary 10.1 Let {φC, C ∈C, φS, S ∈S} denote the current potentials over the cliques
and separators. For any set A ⊆V , let fA = 
XV \A f ; the marginal over A. Whenever
a clique C is live, its potential is φC = fC = 
XV \C f .
Proof of Corollary 10.1 A single clique is a sub-tree. The result is immediate from
Proposition 10.5.
□
Corollary 10.2 Using the notation of Corollary 10.1, whenever active ﬂows have passed
in both directions across an edge in T, the potential for the associated separator is φS =
fS = 
XV \S f .
Proof of Corollary 10.2 The potential φS for the associated separator is, by deﬁnition
of the update,
φS =

XC\S
φC,
so that

XC\S
φC =

XC\S
fC = fS,
because φC is fC by the previous corollary.
□
Proposition 10.6 (The Main Result) After passage of a fully active schedule of ﬂows, the
resulting charge is the charge consisting of the marginals over the cliques and separators
and its contraction represents f . In other words, the following formula, known as the
Aalborg formula (see [52]);
f (x) =
	
C∈C fC(xC)
	
S∈S fS(xS) .
Proof of Proposition 10.6 This follows from the previous two corollaries and Lemma
10.1, stating that the contraction is unaltered by the ﬂows.
□
Thin junction trees
Sometimes the cliques in a junction tree may contain rather many
variables, which may be problematic if one is using numerical methods to compute
marginal distributions. This problem is addressed by R.G. Cowell, A.P. Dawid, S.L.
Lauritzen and D.J. Spiegelhalter in [67], who discuss methods of breaking the probability
distributions of the cliques into smaller marginal distributions. The paper [133] develops
an algorithm that tries to keep the maximum size of cliques below a prescribed bound.
Such junction trees are known as thin junction trees. The thin junction trees cannot be
created after the graph is ﬁxed; the restriction on the graph width has to be enforced
during the learning process.

302
THE JUNCTION TREE AND PROBABILITY UPDATING
10.8
Local and global consistency
Recall that T denotes the junction tree, the set of cliques which form the nodes of T is
denoted C and the intersection of neighbours in the tree T are the separators, denoted by
S. Recall that the potentials associated with C ∈C and S ∈S are denoted by φC and φS
respectively, and that the charge on T,  is deﬁned as:
 = {φC : C ∈C, φS : S ∈S}.
Deﬁnition 10.10 (Local Consistency) A junction tree T is said to be locally consistent if
whenever C1 ∈C and C2 ∈C are two neighbours with separator S, then

XC1\(C1∩C2)
φC1 = φS =

XC2\(C1∩C2)
φC2.
Deﬁnition 10.11 (Global Consistency) A junction tree T (or its charge) is said to be
globally consistent if for every C1 ∈C and C2 ∈C it holds that

XC1\(C1∩C2)
φC1 =

XC2\(C1∩C2)
φC2.
Global consistency means that the marginalization to C1 ∩C2 of φC1 and φC2 coincide
for every C1 and C2 in C. The following results show that, for a junction tree, local
consistency implies global consistency.
Proposition 10.7 After a passage of a fully active schedule of ﬂows, a junction tree T is
locally consistent.
Proof of Proposition 10.7 The two corollaries of the main result give that for any two
neighbouring C1 and C2,

C1\S
fC1 = fS =

C2\S
fC2.
□
An equilibrium, or ﬁxed point has been reached, in the sense that any new ﬂows passed
after passage of a fully active schedule do not alter the potentials. The update ratio for
another message from C1 to C2 becomes
λS =

C1\S fC1
fS
= 1.
Global consistency of junction trees
Here it is shown that for junction trees, local
consistency implies global consistency.
By deﬁnition, a junction tree is a tree such that the intersection C1 ∩C2 of any pair
C1 and C2 in C is contained in every node on the unique trail in T between C1 and C2.
The set C1 ∩C2 can be empty and, in this case it is therefore (by convention) a subset
of every other set.
The following example is instructive.

LOCAL AND GLOBAL CONSISTENCY
303
Example 10.4
Consider the junction tree given in Figure 10.10. Let C1 = EK and
C2 = BDE. Then C1 ∩C2 = E. There is a unique trail
EK ↔ELT ↔BEL ↔BDE
from EK to BDE. Clearly E = C1 ∩C2 is a subset of every separator on the path.
The potentials will be denoted (for example) φEK(xe, xk) =: φEK(ek). The following
abbreviated notation will be used for a marginalization:

xe∈Xe
φEK(xe, xk) =

e
φEK(ek).
Now assume that the junction tree is locally consistent. Then, an application of the result
that

XC1\(C2∩C2)
φC1 = φS =

XC2\(C1∩C2)
φC2
gives

k
φEK(ek) =

lt
φELT (elt),

t
φELT (elt) =

b
φBEL(bel),

l
φBEL(bel) =

d
φBDE(bde).
Using these, it follows that

k
φEK(ek) =

l

t
φELT (elt)

=

l

b
φBEL(bel)

=

b

l
φBEL(bel)

=

b

d
φBDE(bde),
so that

k
φEK(ek) =

b

d
φBDE(bde).
The potential on the right hand side is the marginalization of φBDE to E.
The property of local consistency has therefore been extended to the nodes C1 = EK
and C2 = BDE. The rest of the conditions can be checked; the details are left to the
reader.
Proposition 10.8 A locally consistent junction tree is globally consistent.
Proof of Proposition 10.8 In a junction tree the intersection C1 ∩C2 of any pair C1 and
C2 in C is contained in every node on the unique path in T between C1 and C2. Assume
that C1 ∩C2 is non empty. Consider the unique path from C1 to C2. Let the nodes on

304
THE JUNCTION TREE AND PROBABILITY UPDATING
the path be denoted by {C(i)}n
i=0 with C(0) = C1 and C(n) = C2, so that C(i) and C(i+1)
are neighbours. Denote the separator between C(i) and C(i+1) by
S(i) = C(i) ∩C(i+1).
Then, for all i,
C1 ∩C2 ⊆S(i).
For a set of variables C, let 
C denote 
XC. The assumption of local consistency means
that for any two neighbours

C(i)\(C(i+1)∩C(i))
φC(i) =

C(i+1)\(C(i)∩C(i+1))
φC(i+1).
Since C(i)\(C(i+1) ∩C(i)) = C(i)\S(i) and C(i+1)\(C(i+1) ∩C(i)) = C(i+1)\S(i), local con-
sistency may be written equivalently as

C(i)\S(i)
φC(i) =

C(i+1)\S(i)
φC(i+1).
For any two neighbours C1 and C2, their associated potentials have to be marginalized
to C1 ∩C2. This is equivalent to computing the sum

C(i)\(C1∩C2∩C(i))
φC(i)
and

C(i+1)\(C1∩C2∩C(i+1))
φC(i+1).
Starting with the leftmost marginalization,

C(i)\(C1∩C2∩C(i))
φC(i) =

C(i)\S(i)

S(i)\(C1∩C2∩S(i))
φC(i),
since the nodes (variables) in C(i)\(C1 ∩C2 ∩C(i)) can be split into two disjoint sets,
namely S(i)\(C1 ∩C2 ∩S(i)) (which can be empty: those outside C1 ∩C2 but inside the
separator S(i)) and C(i)\S(i) (those outside the separator).
Next, the order of summation may be exchanged so that

C(i)\S(i)

S(i)\(C1∩C2∩S(i))
φC(i) =

S(i)\(C1∩C2∩S(i))

C(i)\S(i)
φC(i).
Now, using local consistency,

S(i)\(C1∩C2∩S(i))


C(i)\S(i)
φC(i)

=

S(i)\(C1∩C2∩S(i))



C(i+1)\S(i)
φC(i+1)

.

MESSAGE PASSING FOR CONDITIONAL GAUSSIAN DISTRIBUTIONS
305
This gives

S(i)\(C1∩C2∩S(i))



C(i+1)\S(i)
φC(i+1)

=

C(i+1)\S(i)



S(i)\(C1∩C2∩S(i))
φC(i+1)


=

C(i+1)\(C1∩C2∩C(i+1))
φC(i+1).
It follows that

C(i)\(C1∩C2∩C(i))
φC(i) =

C(i+1)\(C1∩C2∩C(i+1))
φC(i+1).
This operation can be repeated using C(i+1) and C(i+2). Therefore, the ﬁrst step is to
marginalize the potential φC1 to C1 ∩C2 and the next step is to move over to the next
node C(1) on the unique path between C1 and C2. The marginalization of φC1 and φC(1)
coincide. This procedure is continued along the path until the node C2 is reached. The
result is proved.
□
Corollary 10.3 After the passage of a fully active schedule of ﬂows, a junction tree is
globally consistent.
Proof of Corollary 10.3 This follows from the proposition stating that after passage
of a fully active schedule of ﬂows a junction tree T is locally consistent, together with
Proposition 10.4.
□
The algorithm for updating considered the cliques of a junction tree, which sent and
received messages locally; the global update is performed entirely by a series of local
computations. By organizing the variables into cliques and separators on a junction tree
and determining a schedule, there is no need for global computations in the inference
problem; the global update is achieved entirely by passing messages between neighbours
in the tree according to a schedule and the algorithm terminates automatically when the
update is completed.
10.9
Message passing for conditional Gaussian
distributions
This section uses the junction tree approach for ﬁnding a suitable conditional Gaussian
approximation for the update of a conditional Gaussian distribution. The problem here
is that CG distributions are convenient to use, but while marginalizing a CG distribution
over one of its continuous variables gives another CG distribution, marginalizing a CG
distribution over one of its discrete variables does not necessarily give a CG distribution.
Therefore, in the message passing algorithm, approximating CG distributions are used,
which return the true CG distribution after the fully active schedule has been completed.
To ensure that the result is the correct CG distribution, some restrictions have to be
made on the variables that are permitted in the cliques and separators. It is therefore
convenient to modify the junction tree construction a little, using a marked graph to
describe the dependence structure. For this section, marked graphs are graphs with two
types of nodes, corresponding to the discrete and continuous variables.

306
THE JUNCTION TREE AND PROBABILITY UPDATING
W
Min
Mout
F
E
D
B
C
L
Figure 10.11
Marked graph.
Example 10.5
Consider the following example, taken from [77]. The emissions from a
waste incinerator differ because of compositional differences in incoming waste. Another
important factor is the way in which the waste is burnt, which can be monitored by
measuring the concentration of carbon dioxide in the emissions. The efﬁciency of the
ﬁlter depends on its technical state and also on the amount and composition of the
waste. The emission of heavy metals depends both on the concentration of metals in the
incoming waste and the emission of dust particles in general. The emission of dust is
monitored by measuring the penetration of light.
This example may be modelled using the directed acyclic marked graph (DAMG) in
Figure 10.11. The categorical variables are F: ﬁlter state, W: waste type, B method of
burning. The continuous variables are Min: metals in the waste, Mout: metals emitted,
E: ﬁlter efﬁciency, D: Dust emission, C: carbon dioxide concentration in emission and
L: light penetration.
Recall the notation introduced for CG distributions in Section 8.9:  is the set of
discrete variables, while  is the set of continuous variables. For marked graphs, the
notion of a decomposition has to be extended:
Deﬁnition 10.12 (Strong Decomposition) A triple (A, B, S) of disjoint subsets of the
node set V of an undirected marked graph G is said to form a strong decomposition
of G if V = A ∪B ∪S and the following three conditions hold:
1. S separates A from B,
2. S is a complete subset of V ,
3. Either S ⊆, or B ⊆, or both.
When this holds, (A, B, S) is said to decompose G into the components GA∪S and GB∪S.
If only the ﬁrst two conditions hold, then (A, B, S) is said to form a weak decompo-
sition. Thus, a weak decomposition ignores the markings of the graph.
Deﬁnition 10.13 (Strongly Decomposable) An undirected marked graph is said to be
strongly decomposable if it is complete, or if there exists a strong decomposition (A, B, S),
where both A and B are non empty, into strongly decomposable sub-graphs GA∪S, and
GB∪S.
Decomposable unmarked graphs are triangulated; any cycle of length four or more
has a chord. Strongly decomposable marked graphs are further characterized by not

MESSAGE PASSING FOR CONDITIONAL GAUSSIAN DISTRIBUTIONS
307
having any path between two discrete variables that contains two adjacent continuous
variables.
Proposition 10.9 For an undirected marked graph G, the following are equivalent:
1. G is strongly decomposable.
2. G is triangulated, and for any path (δ1, α1, . . . , αn, δ2) between two discrete nodes
(δ1, δ2) where (α1, . . . , αn) are all continuous, δ1 and δ2 are neighbours.
3. For any α and β in G, every minimal (α, β) separator is complete. If both α and β
are discrete, then their minimal separator contains only discrete nodes.
Proof of Proposition 10.9, 1
⇒
2 The proof, as before for unmarked graphs,
is by induction. The inductive hypothesis is: All undirected strongly decompos-
able graphs with n or fewer nodes are triangulated and satisfy the conditions of
Statement 2.
This is clearly true for a graph on one node.
Let G be a strongly decomposable graph on n + 1 nodes.
Either G is complete, in which case the properties of statement 2 clearly
follow,
Or There exist sets A, B, S, where V = A ∪B ∪S, where either B ⊆ or S ⊆
or both, and such that GA∪S and GB∪S are strongly decomposable. Then any cycle
of length 4 without a chord must pass through both A and B. By decomposability,
S separates A from B. Therefore the cycle must pass through S at least twice.
Since S is complete, the cycle will therefore have a chord. Since GA∪S and GB∪S
are triangulated, it follows that G is also triangulated. If the nodes of S are discrete,
it follows that any path between two discrete variable passing through S satisﬁes
the condition of Statement 2. If B ⊆, then since all paths in GA∪S and all paths
in GB∪S satisfy the condition of Statement 2, it is clear that all paths passing
through C will also satisfy the condition of Statement 2. It follows that G is strongly
decomposable.
□
Proof of Proposition 10.9, 2 ⇒3 Assume that G is triangulated, with the additional
property in Statement 2. Consider two nodes α and β and let S be their minimal sep-
arator. Let A denote the set of all nodes that may be connected to α by a trail that
does not contain nodes in S and let B denote all nodes that may be connected to β
by a trail that does not contain nodes in S. Every node γ ∈S must be adjacent to
some node in A and some node in B, otherwise GV \(S\{γ }) would not be connected. This
would contradict the minimality of S, since S\{γ } would separate α from β. Suppose that
the condition in Statement 2 holds and consider the minimal separator for two discrete
nodes α, β, which are not neighbours. The separator is complete. Denote the separator
by S. Consider ˆS, which is S with the continuous nodes removed. Then ˆS separates α
and β on the sub graph induced by the discrete variables. But the condition of state-
ment 2 implies that α and β are also separated on G. Therefore, ˆS separates α and
β. It follows that the minimal separator for two discrete nodes contains only discrete
nodes.
□
Proof of Proposition 10.9, 3 ⇒1 If G is complete, it follows that every node
is discrete and the result is clear. Let α and β be two discrete nodes that are not

308
THE JUNCTION TREE AND PROBABILITY UPDATING
contained within their minimal separator. Let S denote their minimal separator.
Let A denote the maximal connected component of V \S and let B = V \(A ∪S).
Then (A, B, S) provides a decomposition, with S ⊆. Suppose that two such
discrete nodes cannot be found. Let α and β be two nodes that are not contained
within their minimal separator, where β is continuous. Let S denote the minimal
separator. Let B denote the largest connected component of V \S containing β.
Suppose that B contains a discrete node γ . Then S separates γ
from α and
therefore consists entirely of discrete nodes. Therefore, either S ⊆, or B ⊆, as
required.
□
The construction of the junction tree has to be modiﬁed. Starting from the directed
acyclic graph, the graph is ﬁrst moralized by adding in the links between all the
parents of each variable and then making all the edges undirected, as before. Then,
sufﬁcient edges are added in to ensure that the graph is a decomposable marked
graph.
Next, a junction tree is constructed. As before, this is an organization of a collection
of subsets of the variables V into a tree, such that if A and B are two nodes on the
junction tree, then the variables in A ∩B appear in each node on the path between A
and B.
Deﬁnition 10.14 (Strong Root) A node R on a junction tree is a strong root if any pair
of neighbours A, B, such that A lies on the path between R and B (so that A is closer to
R than B) satisﬁes
B\A ⊆
or
B ∩A ⊆
or both.
This condition is equivalent to the statement that the triple (A\(A ∩B), B\(A ∩B), A ∩
B) forms a strong decomposition of GA∪B. This means that when a separator between
two neighbouring cliques is not purely discrete, the clique furthest away from the root
has only continuous nodes beyond the separator.
Theorem 10.1 The cliques of a strongly decomposable marked graph can be organized
into a junction tree with at least one strong root.
Proof of Theorem 10.1 As before, start with a simplicial discrete node X1. Then FX1
is a clique. Continue choosing nodes from FX1 that only have neighbours in FX1. Set i1
the number of nodes in FX1 that only have neighbours in FX1. Name the set of nodes in
FX1 Vi1 and the set of nodes in FX1 that have neighbours not in FX1 Si1.
Now remove the nodes of FX1 that do not have neighbours outside FX1. Choose a
new discrete simplicial node X2, such that FX1 ∩FX2 ̸= φ, the empty set, and repeat the
process with index i2, where i2 is i1 plus the number of nodes in FX2 that only have
neighbours in FX2.
Continue this process, by choosing Xj such that FXj ∩(∪j−1
k=1FXk) ̸= φ, until there
are no discrete simplicial nodes left with which to continue the process. Let Xn denote
the last discrete simplicial node, following this procedure.
Now continue the procedure, for, j = n + 1, . . . , N, choosing Xj such that FXj ∩
(∪j−1
k=1FXk) ̸= φ, until there are no nodes left.

MESSAGE PASSING FOR CONDITIONAL GAUSSIAN DISTRIBUTIONS
309
Since the minimal separator of any two discrete nodes contains only discrete nodes,
it follows that if there is a discrete node in V \(∪n
j=1Vij ), then there is a discrete simpli-
cial node among those remaining, such that FXn+1 ∩(∪n
j=1FXn) ̸= φ, contradicting the
assertion. It follows that all the nodes in V \(∪n
j=1Vij ) are continuous.
By construction, the cliques Vi1, . . . , Vin may be organised as a junction tree. If cliques
Vik and Vil are adjacent in the tree, it is clear that their separator is the minimal separator
between Xk and Xl. These are both discrete. Because the minimal separator between two
discrete nodes contains only discrete variables, it follows that all the separators in this
junction tree are discrete.
Now continue the construction of the junction tree as in Theorem 4.3, where the
remaining cliques have index greater than in. The resulting construction will have the
desired properties; any of the cliques Vi1, . . . , Vin may be chosen as the strong root.
□
To exploit the properties of CG distributions, the following further assumption needs to
be made:
Hypothesis 10.1
No continuous nodes have discrete children.
This is because, conditioned on the discrete variables, the distribution is Gaussian, which
makes certain aspects of the computation rather easy.
The prior conditional probability distributions corresponding to the directed acyclic
marked graph need to be speciﬁed. The assumption is that for a continuous variable X,
with parents (X) = (d(X), c(X)), where d(X) are the discrete parents and c(X)
are the continuous parents,
X|(d(X) = y, c(X) = z) ∼N(α(y) + β(y)tz, γ (y)).
Following Deﬁnition 1.4, the random vectors are taken as row vectors when they are
several attributes measured on a single run of an experiment. Here, α is a function, β is
a (row) vector of the same length as z and γ is the conditional variance. The assumption
is that the variance is only affected by the discrete parents; the continuous parents only
enter linearly through the mean. The conditional density is then a CG potential,
φX(y, z, x) =
1
(2πγ (y))1/2 exp

(x −α(y) −β(y)zt)2
2γ (y)
*
.
From this, expanding the parentheses, taking logarithms and identifying terms gives the
canonical parameters (gX, hX, KX). The log partition function is
gX(y) = −
α(y)2
2γ (y) −1
2 log(2πγ (y)),
and the other parameters are given by
hX(y) =
α(y)
γ (y)

 1
−β(y) 
and
KX(y) =
1
γ (y)

1
−β(y)
−β(y)t
β(y)tβ(y)

.

310
THE JUNCTION TREE AND PROBABILITY UPDATING
Marginalization: Continuous variables
Suppose φY,X1,X2 is a CG potential, where Y
are discrete variables and X1 and X2 are continuous variables. That is, φ is given by
φY,X1,X2(y, x1, x2)
= χ(y) exp

g(y) + h1(y)xt
1 + h2(y)xt
2 −1
2(x1, x2)

K11
K12
Kt
12
K22
 
xt
1
xt
2
*
,
where χ(y) is a function returning the value 1 if pY(y) > 0 and 0 if pY(y) = 0, K is
symmetric and the triple (g, h, K) represents the canonical characteristics. Recall the
standard result that, taking z ∈Rp as a row vector, and K a positive deﬁnite p × p
symmetric matrix,
1
(2π)p/2

Rp exp

−1
2zKzt
"
dz =
1
√det(K)
and hence that for a ∈Rp and K a positive deﬁnite p × p symmetric matrix

Rp exp

(a, z) −1
2ztKz
"
dz = exp
1
2atK−1a
" (2π)p/2
√det(K).
From this, it follows, after some routine calculation, that if X1 is a random p-vector with
positive deﬁnite covariance matrix, then

Rp φY,X1,X2(y, x1, x2)dx1 = χ(y) exp

˜g(y) + ˜h(y)xt
2 −1
2x2 ˜Kxt
2
"
,
where
˜g(y) = g(y) + 1
2

p log(2π) −log det(K11(y)) + h1(y)K11(y)−1h1(y)t
,
˜h(y) = h2(y),
˜K = −K21(y)K11(y)−1K12(y).
Marginalization: Discrete variables
Consider a CG potential φY 1,Y 2,X, where Y 1 and
Y 2 denote sets of discrete variables and X a set of continuous variables. Consider
marginalization over Y 2. Firstly, if h(y1, y2) = ˜h(y1) and K(y1y2) = ˜K(y1) for some
functions ˜h and ˜K (i.e. they do not depend on y2), then ˜φ, the marginal of φY 1,Y 2,X is
simply
˜φ(y1, x) = exp

˜h(y1)tx −1
2xt ˜K(y1)x
" 
y2
χ(y1, y2) exp
/
g(y1, y2)
0
.
The potential ˜φ is therefore CG with canonical characteristics
˜g(y1) = log 
y2 exp
/
g(y1, y2)
0
and ˜h, ˜K as before.
If either h or K depends on y2, then a marginalization will not produce a CG dis-
tribution, so an approximation is used. For this, it is convenient to consider the mean
parameters, (p, C, µ), where p(y1, y2) = p((Y 1, Y 2) = (y1, y2)) and
X|{(Y 1, Y 2) = (y1, y2)} = N(µ(y1, y2), C(y1, y2)).

USING A JUNCTION TREE WITH VIRTUAL EVIDENCE AND SOFT EVIDENCE
311
The approximation is as following: ˜φ is deﬁned as the CG potential with mean parameters
( ˜p, ˜C, ˜µ) deﬁned as:
˜p(y1) =

y2
p(y1, y2),
˜µ(y1) =
1
˜p(y1)

y2
p(y1, y2)µ(y1, y2),
˜C(y1) =
1
˜p(y1)

y2
p(y1, y2)

C(y1, y2) + (µ(y1, y2) −˜µ(y1))t(µ(y1, y2) −˜µ(y1))

.
It is relatively straightforward to compute that this approximate marginalization has the
correct expected value and second moments.
Marginalizing over both discrete and continuous
When marginalizing over both types
of variables, ﬁrst the continuous variables are marginalized, and then the discrete.
The fully active schedule may now be applied. Firstly, the evidence is inserted. This
is hard evidence, that certain states of the discrete variables are impossible, or that the
continuous variables take certain ﬁxed values. The information then has to be propagated.
Start at the leaves, send all messages to a strong root, then propagate back out to the
leaves. Since messages are propagated to and from a strong root, all marginalizations are
proper marginalizations. This is clear: marginalizing continuous variables gives a proper
marginalization. If the separator is purely discrete, then once the continuous variables have
been marginalized, the remaining discrete marginalizations are proper marginalizations.
The directed acyclic marked graph, and its strong decomposition, have ensured that the
propagation is exact.
Having inserted hard evidence and run the schedule, the resulting potentials are not
necessarily probability distributions, but after the schedule, the same constant is required
to normalize each of them. The updating is ﬁnished, therefore, by ﬁnding the constant
that normalizes potential over one of the cliques or separators to make it a probability
distribution.
If the graph is not strongly decomposable, then the approximate marginalization may
be used, to obtain an approximate update.
10.10
Using a junction tree with virtual evidence
and soft evidence
The methods discussed so far in this chapter may be extended to the problem of updating
in the light of virtual evidence and soft evidence.
Dealing with virtual evidence is straightforward; for each virtual ﬁnding, one adds
in a virtual node, as illustrated in Figure 3.3, which will be instantiated according to the
virtual ﬁnding. This simply adds the virtual ﬁnding node to the clique containing the
variable for which there is a virtual ﬁnding.
Incorporating soft evidence cannot be carried out in such a straightforward manner,
because when there is a soft ﬁnding on a variable, the DAG is altered by removing the

312
THE JUNCTION TREE AND PROBABILITY UPDATING
directed arrows from the parent nodes to the variable. One method for incorporating soft
evidence is discussed in [134]. The input is a Bayesian network with a collection of soft
and hard ﬁndings. The method returns a joint probability distribution with two properties:
1. The ﬁndings are the marginal distributions for the updated distribution.
2. The updated distribution is the closest to the original distribution (where the
Kullback-Leibler divergence is used) that satisﬁes this constraint (that the ﬁndings
are the marginals of the updated distribution).
The lazy big Clique algorithm
The method described in chapter 10 is modiﬁed to
incorporate soft evidence in the following way.
1. Construct a junction tree, in which all the variables that have soft evidence are in
the same clique–the big clique C1.
2. Let C1 (the big clique) be the root node, apply the hard evidence and run the ﬁrst
half of the fully active schedule; that is, propagating from the leaves to the root
node.
3. Once the big clique C1 has been updated with the information from all the other
cliques, absorb all the soft evidence into C1. This is described below.
4. Distribute the evidence according to the method described in Section 10.7 for
sending messages from the updated root out to the leaves.
If the big clique is updated to provide a probability function (namely a potential that
sums to 1), then the distribution of evidence will update the potentials over the cliques
and separators to probability distributions over the respective cliques and separators.
Absorbing the soft evidence
Suppose the big clique C1 has soft evidence on the vari-
ables (Y1, . . . , Yk). Suppose soft evidence is received that Y1, . . . , Yk have distributions
qY1, . . . , qYk respectively. Let qC1 denote the probability function over the variables in C1
after the soft evidence has been absorbed. Then it is required that, for each j ∈{1, . . . , k},
qYj = 
XC1\{Yk} qC1. That is, the marginal of qC1 over all variables other than Yk is qYk.
The important feature of soft evidence (Deﬁnition 3.1) is that after soft evidence has
been received, the variable has no parent variables. The Iterative Proportional Fitting
Procedure (IPFP), therefore, may be employed. It goes in cycles of length k. Firstly,
normalize the potential over C1 (after the hard evidence has been received) so that it is
a probability distribution pC1. Then
p(0)
C1 = pC1
for j = 1, . . . , k, set p(mk+j−1)
Yj
= 
XC1\{Yj } p(mk+j−1)
C1
, and
p(mk+j)
C1
=
p(mk+j−1)
C1
qYj
p(mk+j−1)
Yj
.

USING A JUNCTION TREE WITH VIRTUAL EVIDENCE AND SOFT EVIDENCE
313
This is repeated until the desired accuracy is obtained. It has been well established that,
for discrete distributions with ﬁnite state space, the IPFP algorithm converges to the
distribution that minimises the Kullback-Leibler distance from the original distribution
(see [135]).
□
Notes
The original paper describing the use of junction trees for updating a Bayesian
network is by S.L. Lauritzen and D.J. Spiegelhalter [131]. The terminology Aalborg
formula is found in [52]. Much of the material is taken from ‘Probabilistic Networks and
Expert Systems’ by R.G. Cowell, A.P. David, S.L. Lauritzen and D.J Spiegelhalter [67].
The proofs or the main results were originally presented in [68]. The reference [136] gives
a mathematically rigorous presentation of an alternative message passing scheme known
as Schachter’s method. This method is also valid for more general inﬂuence diagrams.
The application of junction tree methods to conditional Gaussian distributions was taken
from S.L. Lauritzen [77]. The Iterative Proportion Fitting Procedure dates back to W.E.
Deming and F.F. Stephan (1940) [137]; this is the basis for updating a junction tree in
the light of soft evidence. The basic technique is taken from [134].

314
EXERCISES: THE JUNCTION TREE AND PROBABILITY UPDATING
10.11
Exercises: The junction tree and probability updating
1. Let V = {X1, X2, X3, X4, X5, X6} be a set of variables, where for j = 1, . . . , 6,
variable Xj has state state space Xj. Consider the potentials φ1 : X1 × X2 × X3 →
R+, φ2 : X2 × X3 × X5 →R+, φ3 : X1 × X3 × X4 →R+, φ4 : X5 × X6 →R+.
(a) Determine the domain graph.
(b) Eliminate X3 and determine the resulting set of potentials and their domain graph.
(c) For the original domain graph, determine a perfect elimination sequence ending
with X1.
2. Let V = {X1, X2, X3, X4, X5, X6, X7, X8} where, for j = 1, . . . , 8, variable Xj has
state space Xj. Consider the potentials φ1 : X1 × X2 × X3 →R+, φ2 : X2 × X4 ×
X5 →R+, φ3 : X4 × X6 × X7 →R+ and φ4 : X1 × X6 × X8 →R+.
(a) Determine the domain graph.
(b) Eliminate X1 and determine the resulting set of potentials and their domain graph.
(c) For the original domain graph, is there a perfect elimination sequence?
3. Consider the Bayesian network in Figure 10.12.
(a) Determine the domain graph.
(b) Does the domain graph have a perfect elimination sequence?
(c) Triangulate the graph, adding as few ﬁll ins as possible.
X6
X2
X4
X1
X7
X3
X5
X8
Figure 10.12
Bayesian network for exercises 3 and 4.
4. Consider again the Bayesian network in Figure 10.12.
(a) Write down the elimination sequence corresponding to your triangulation in the
previous exercise.

THE JUNCTION TREE AND PROBABILITY UPDATING
315
(b) Suppose each variable has three states and let Xj = (x(1)
j , x(2)
j , x(3)
j ) for j =
1, . . . , 8. Calculate the total number of entries in all the tables needed to deﬁne
pX1,X2,X3,X4,X5,X6,X7,X8(., ., ., ., ., ., x(1)
7 , x(1)
8 )
(c) Consider the calculation of
pX1|X7,X8(.|x(1)
7 , x(1)
8 )
where the variables are marginalized in the following order: X2, X6, X4, X5, X3.
Calculate the size of each table to be marginalized in the process.
(d) Try to ﬁnd an elimination sequence resulting in smaller tables to be marginal-
ized.
5. Let G = (V, E) be an undirected graph, with node set V = {α1, . . . , αd} Recall that
an elimination sequence of a graph G is a linear ordering of its nodes. Let σ be an
elimination sequence and let  denote the ﬁll-ins produced by eliminating the nodes
of G in the order σ. Let Gσ denote the graph G extended by . Note that in Gσ any
node α ∈V together with its neighbours of higher elimination order form a complete
subset in the sense that they are all pairwize linked. Let Nσ(α) denote the set that
contains α and its neighbours of a higher elimination order.
(a) Let G−αv denote the graph G, after the node αv has been eliminated. Prove
that G−αv is the domain graph for −αv, the potential  with the variable αv
eliminated.
(b) Prove that the sets Nσ(α) are the elimination domains corresponding to the elim-
ination sequence σ.
6. Construct a junction tree for the Bayesian network shown in Figure 10.13.
(a) Construct a fully active schedule.
(b) Assume,
now,
that
you
have
hard
evidence
that
{(X4, X5, X8) =
(x(1)
4 , x(3)
5 , x(2)
8 )}.
Which
communications
are
necessary
to
update
the
network?
X1
X2
X3
X9
X4
X5
X6
X7
X8
Figure 10.13
Bayesian network.

316
EXERCISES: THE JUNCTION TREE AND PROBABILITY UPDATING
7. a(a) Consider the Bayesian network shown in Figure 10.14, where for each j, the
number inside the node j indicates the number of possible states that variable
Xj can take. Construct a junction tree using the following ‘greedy’ algorithm:
take a simplicial node, if possible. Otherwise, take the node that requires as few
ﬁll-ins as possible. Once the number of ﬁll-ins has been determined, take the node
associated with the potential with the smallest table. Once the number of ﬁll-ins
and table size has been determined, take the node, satisfying these conditions,
with the largest number of states.
(b) Write a MATLAB code to implement the procedure outlined above, when there
are six variables. The input should be the parents of each node. The algorithm
should then determine the edges required for the moral graph. The output should
be the elimination sequence.
X13, 2
X1, 2
X7, 2
X2, 2
X3, 2
X8, 3
X4, 3
X5, 3
X11, 4
X9, 2
X12, 2
X6, 5
X10, 5
Figure 10.14
Bayesian network for Exercise 7.
8. Consider the Bayesian network in Figure 4.28. Suppose the network receives hard
evidence: {(X5, X6, X9) = (x(1)
5 , x(3)
6 , x(2)
9 )}. From the junction tree computed for
Exercise 5 in Chapter 4, describe an active schedule to update the distribution of the
other variables, conditioned on these instantiations.
9. Let C denote the set of cliques from a triangulated graph. A pre-I-tree is a tree
over C with separators S = C1 ∩C2 for adjacent cliques C1 and C2. The weight of
a pre-I-tree is the sum of the number of variables in the separators.
(a) Prove that a junction tree is a pre-I-tree of maximal weight.
(b) Prove that any pre-I-tree of maximal weight is a junction tree.
10. Consider the DAG given in the Bayesian network in Figure 10.15.
(a) Determine the minimal set of conditioning variables for the DAG to reduce it to
a singly connected DAG.
(b) The numbers attached to the variables indicate the number of states. Determine
a conditioning resulting in a minimal number of singly connected DAGs.

THE JUNCTION TREE AND PROBABILITY UPDATING
317
X1, 2
X2, 2
X3, 4
X9, 2
X4, 3
X5, 5
X6, 2
X8, 2
X7, 2
Figure 10.15
Bayesian network for Exercise 10.
11. This exercise is taken from S. Lauritzen [77]. It is a ﬁctitious problem connected
with controlling the emission of heavy metals from a waste incinerator. The type
of incoming waste W affects the metals in the waste Min, the dust emission D and
the ﬁlter efﬁciency E. The quantity of metals in the waste Min affects the metals
emission Mout. Another important factor is the waste burning regimen B, which
is monitored via the carbon dioxide concentration in the emission C. The burning
regimen, the waste type and the ﬁlter efﬁciency E affect the dust emission D. The
dust emission affects the metals emission and it is monitored by recording the light
penetration L. The state of the ﬁlter F (whether it is intact or defective) affects E.
The variables F, W, B are qualitative variables with states (the ﬁlter is either
intact or defective, the waste is either industrial or household, the burning regimen is
either stable or unstable). The variables E, C, D, L, Min and Mout are continuous.
The directed acyclic marked graph is given in Figure 10.11.
• Moralize the graph.
• By adding in as few links as possible, construct a strongly decomposable graph.
• Construct a junction tree. What are the possible strong roots for the junction tree?


11
Factor graphs and the sum
product algorithm
The last few chapters treated the algorithmic problem of marginalizing a global potential
which is expressed as a product of local potentials, each with a domain that is a subset of
the global domain. This chapter addresses the same problem, but introduces a different
algorithm is, which does not require (at least explicitly) the construction of junction trees.
The algorithm is known as the sum product algorithm and it operates on factor graphs.
11.1
Factorization and local potentials
As usual, let ˜V = {1, . . . , d}, and for each j ∈˜V let Xj = (x(1)
j , . . . , x
(kj )
j
) denote a ﬁnite
state space. Let X = ×d
j=1Xj. The space X is the conﬁguration space. Let φ denote a
potential deﬁned on X.
Let x = (x1, . . . , xd) ∈X denote a conﬁguration and, for a subset D ⊆{1, . . . , d},
where D = {j1, . . . , jm}, let xD = (xj1, . . . , xjm) and XD = ×v∈DXv.
A domain XD for D ⊂{1, . . . , d} (where the subset is strict) is called a local domain.
Deﬁnition 11.1 (Factorizability) The potential φ is said to be factorizable if it factors into
a product of several local potentials γj each deﬁned on local domains, such that
φ(x) =

j∈J
γj(xSj )
(11.1)
for a collection of local domains XSj , j ∈J where J = {1, 2, . . . , q} and q ≤d.
Bayesian Networks: An Introduction
T. Koski, J. Noble
2009 John Wiley & Sons, Ltd

320
FACTOR GRAPHS AND THE SUM PRODUCT ALGORITHM
For a factorizable potential φ, consider the problem of computing the marginal
φi(xi) =

z∈X ˜V \{i}

j∈J
γj(z, xi),
(11.2)
where the domains of the potentials have been extended to X (Deﬁnition 2.25). This is
also known as the ‘one i (eye) problem’. The aim of this section is to develop an efﬁcient
procedure for computing the marginalization, which exploits the way in which the global
potential is factorized and uses the current values to update the values assigned to each
variable. The method involves a factor graph, which is an example of a bipartite graph.
Deﬁnition 11.2 (Bipartite Graph) A graph G is bipartite if its node set can be partitioned
into two sets W and U in such a way that every edge in G has one node in W and another
in U.
Bipartite graphs can be (roughly) characterized as graphs that have no cycles of odd
length, but this property is not used here. A factor graph is a bipartite graph that expresses
the structure of the factorization given by Equation (11.1). The graph is constructed as
follows.
• there is a variable node (an element of U) for each variable. A capital letter X will
be used to denote the variable node, a small letter the value x in the state space
XX associated with the variable.
• there is a function node (an element of W) for each potential γj. γj will be used
to denote both the potential and the node.
• an undirected edge connecting variable node Xi to factor node γj if and only if Xi
is in the local domain of γj.
In other words, a factor graph is a representation of the relation ‘is an argument of’.
11.1.1
Examples of factor graphs
Example 11.1: Error correcting codes
The following example is taken from N. Wiberg
[138]. Consider the system of equations:
x1 + x2 + x3 = 0
x3 + x4 + x5 = 0
x1 + x5 + x6 = 0
x2 + x4 + x6 = 0
(11.3)
The variables are binary (taking values 0 or 1) and addition is binary (modulo 2). This
system of equations may be expressed as the factor graph given in Figure 11.1. The
boxes correspond to the operation + and the circles correspond to the variables of the
system. The notation Xj will be used to denote both the variable and the corresponding
variable node in the graph for the variable that takes its values in state space Xj. Similar
notation will be used throughout.
□

FACTORIZATION AND LOCAL POTENTIALS
321
X1
X3
X2
X6
X4
X5
Figure 11.1
A Factor Graph for Equations (11.3).
X1
X2
X3
X4
Figure 11.2
A directed acyclic graph.
p(X1)
X1
p(X2|X1)
X2
p(X3|X1,X2)
X3
p(X4|X3)
X4
Figure 11.3
The factor graph corresponding to the directed acyclic graph in Figure 11.2.
Example 11.2: Bayesian networks as factor graphs
A Bayesian network has a joint
probability distribution that factorizes according to a DAG. This joint distribution can be
converted into a factor graph. Each function is the local potential pXi|i and edges are
drawn from this node to Xi and to its parents i. The DAG is shown in Figure 11.2 and
the corresponding factor graph in Figure 11.3.
Example 11.3: A factor graph of a probability distribution
1. A general joint probability distribution p(X1, X2, X3, X4) over four variables has
the trivial factor graph shown in Figure 11.4.
2. The deﬁnition of conditional probability yields
pX1,X2,X3,X4 = pX1pX2|X1pX3|X1,X2pX4|X1,X2,X3.
The factor graph corresponding to this chain rule is given in Figure 11.5.

322
FACTOR GRAPHS AND THE SUM PRODUCT ALGORITHM
X1
X2
X3
X4
Figure 11.4
Trivial factor graph for a probability distribution.
p(X1)
X4
X1
p(X4|X1,X2,X3)
p(X2|X1)
X2
p(X3|X1,X2)
X3
Figure 11.5
The chain rule as a factor graph.
p(X1)
X1
p(X2|X1)
X2
p(X3|X2)
X3
Figure 11.6
A Markov chain as a factor graph.
3. A hidden Markov model (HMM) has
pX1,X2,X3,Y1,Y2,Y3
= pX1pX2|X1pX3|X2pX4|X3pY1|X1pY2|X2pY3|X3
and the corresponding factor graph is given in Figure 11.7. The factor graph
corresponding to a Markov chain is illustrated in Figure 11.6.
p(X1)
X1
p(X2|X1)
X2
p(X3|X2)
X3
p(Y1|X1)
p(Y2|X2)
p(Y3|X3)
Y1
Y2
Y3
Figure 11.7
A hidden Markov model as a factor graph.

THE SUM PRODUCT ALGORITHM
323
11.2
The sum product algorithm
The following algorithm computes the marginalization for the one eye problem of
Equation (11.2) without the introduction of junction trees. Consider Figure 11.8.
The following notation is introduced:
µXk→γj (x)
x ∈Xk :
Variable to local potential.
This is the message sent from node X to node γj in the sum product algorithm and
µγj →Xk(x)
x ∈Xk :
Local potential to variable.
This is the message sent from the function node γj to the variable node X.
Recall the deﬁnition of neighbour (Deﬁnition 2.3). Nv will be used to denote the set
of neighbours of a node v. A factor graph is undirected. By the deﬁnition of a factor
graph, all the neighbours of a node will be of the opposite type to the node itself.
The message sent from node v on edge e is the product of the local potential at v (or
the unit function if v is a variable node) with all messages received at v on edges other
than e and then marginalized to the variable associated with e. The messages are deﬁned
recursively as follows.
Deﬁnition 11.3 (Sum Product Update Rule) For x ∈Xk, and for each Xk ∈Nγj ,
µXk→γj (x) =
 	
h∈NXk \{γj } µh→Xk(x)
∀x ∈Xk
NXk ̸= φ
1
NXk = φ.
(11.4)
and for each γj ∈NXk,
µγj →Xk(x) =

y∈X ˜V \{k}
γj(y, x)

Y∈Nγj \{Xk}
µY→γj (yj)
∀x ∈Xk
(11.5)
where φ denotes the empty set, and where the domain of γj has been extended to X and
variable Xk takes the last position; yj is the value taken by variable Xj (j ̸= k).
The ﬂow of computation in a factor graph is illustrated in Figure 11.9.
Deﬁnition 11.4 (Initialization) The initialization is
µXk→γj (x) = 1
∀x ∈Xj
X
gj
mgj→X(x)
mX→gj(x)
Figure 11.8
Updates in a factor graph.

324
FACTOR GRAPHS AND THE SUM PRODUCT ALGORITHM
m
m
γ
γ
Nx\ γ
y3
y2
y1
h3
h2
h3
h1
m
Nγ\ x 
x
γ
γ
(x)
x
x(x)
(x)
x
y1
m
Figure 11.9
Updates in a fragment of a factor graph.
for each Xk ∈Nγj and
µγj →Xk(x) = 1
∀x ∈Xj
for each γj ∈NXk, for each variable node Xk and each function node γj.
Deﬁnition 11.5 (Termination) The termination at a node is the product of all messages
directed towards that node.
µXk(x) =

γj ∈NXk
µγj →Xk(x),
x ∈Xk
(11.6)
and
µγj (xDj ) =

Xk∈Nj
µXk→γj (xk)
∀xDj ∈XDj .
Note that the function node receives communications from precisely those variables that
are in the domain of the function.
After sending sufﬁciently many messages according to a suitable schedule, the ter-
mination at the variable node yields the marginalization, or a suitable approximation to
the marginalization, over that variable. That is,
µXi(x) =

y∈X ˜V \{i}
φ(y, x)
∀x ∈Xi,
where the arguments of φ have been rearranged, so that variable Xi appears last.

THE SUM PRODUCT ALGORITHM
325
The schedule
One node is arbitrarily chosen as a root and, for the purposes of con-
structing a schedule, the edges are directed to form a directed acyclic graph, where the
root has no parents. If the graph is a tree, then the choice of directed acyclic graph is
uniquely deﬁned by the choice of the root node. Computation begins at the leaves of the
factor graph.
• Each leaf variable node sends the trivial identity function to its parents.
• Each leaf function node sends a description of γ to its parents.
• Each node waits for the message from all its children before computing the message
to be sent to its parents.
• Once the root has received messages from all its children, it sends messages to all
its children.
• Each node waits for messages from all its parents before computing the message
to be sent to its children.
This is repeated from root to leaves and is iterated a suitable number of times. No
iterations are needed if the factor graph is cycle free. This is known as a generalized
forward and backward algorithm.
The following result was proved by N. Wiberg [138].
Theorem 11.1 (Wiberg).
Let
φ(x) =

j
γj(xDj )
and let G be a factor graph with no cycles, representing φ. Then, for any variable node
Xk, the marginal of φ at x ∈Xk is
µXk(x) =

y∈X ˜V \{k}
φ(y, x),
where the arguments of φ have been rearranged so that the kth variable appears last and
µXk(x) is given in Equation (11.6).
Example 11.4
Before giving a proof of Wiberg’s theorem, the following example may
be instructive. Consider
φ(x1, x2, x3) = γ1(x1, x2)γ2(x2, x3).
The factor graph is then a tree given in Figure 11.10.
X1
g1(x1, x2)
X2
g2(x2, x3)
X3
Figure 11.10
An example on three variables and two functions.
In this case, the messages are:
µX1→γ1(x1) = µX3→γ2(x3) = 1.

326
FACTOR GRAPHS AND THE SUM PRODUCT ALGORITHM
µγ1→X2(x2) =

x1∈X1
γ1(x1, x2)µX1→γ1(x1) =

x1∈X1
γ1(x1, x2)
µγ2→X2(x2) =

x3∈X3
γ2(x2, x3)µX3→γ2(x3) =

x3∈X3
γ2(x2, x3)
µX2→γ2(x2) = µγ1→X2(x2) =

x1∈X1
γ1(x1, x2)
µX2→γ1(x2) = µγ2→X2(x2) =

x3∈X3
γ2(x2, x3)
µγ1→X1(x1) =

x2∈X2
γ1(x1, x2)µX2→γ1(x2) =

(x2,x3)∈X2×X3
γ1(x1, x2)γ2(x2, x3)
µγ2→X3(x3) =

(x1,x2)∈X1×X2
γ1(x1, x2)γ2(x2, x3).
Note that the variable terminations are
µX1(x1) = µγ1→X1(x1) =

(x2,x3)∈X2×X3
γ1(x1, x2)γ2(x2, x3)
µX2(x2) = µγ1→X2(x2)µγ2→X2(x2) =

(x1,x3)∈X1×X3
γ1(x1, x2)γ2(x2, x3)
µX3(x3) = µγ2→X3(x3) =

(x1,x2)∈X1×X2
γ1(x1, x2)γ2(x2, x3),
which are the required marginalization. The Wiberg’s theorem states that if the factor
graph is a tree, then after a full schedule, the terminations give the required marginaliza-
tion.
Proof of Theorem 11.1 Consider Figure 11.11. Suppose that a full schedule has been
performed on a tree. The proof proceeds in three steps.
□
Step 1:
Decompose the factor graph into n components, R1, . . . , Rn Choose a variable
Xi and suppose that n edges enter the variable node Xi. Since there are no cycles, the
margin 
y∈X ˜V \{i} φ(y, xi) (where the arguments of φ have been suitably rearranged) may
be written as

y∈X ˜V \{i}
φ(y, xi) =

y∈X|yi=xi

j∈R1
γj(yDj )

j∈R2
γj(yDj ) . . .

j∈Rn
γj(yDn)
=
n

k=1

yRk ∈XRk |yi=xi

j∈Rk
γj(yDj )
=
n

k=1
νRk(xi),

THE SUM PRODUCT ALGORITHM
327
x1
R1
R2
g1ο
g2
mγ 
ο
x1
x1
1
mγ 2
Figure 11.11
Step 1.
where the notation is clear. The last expression has the same form as the termination
formula. Therefore the assertion is proved if it can be established that
νRk(xi) = µγ 0
k →Xi(xi),
k = 1, . . . , n,
where γ 0
1 , . . . , γ 0
n are the n function nodes that are neighbours of Xi. Due to the clear
symmetry, it is only necessary to consider one of these.
Step 2:
Consider the decomposition of R1 shown in Figure 11.12, where the function
node γ 0
1 has three edges. In the three variable case shown in Figure 11.12, X1 is the
node under consideration and γ 0
1 is outside R3 and R4. For this case,
νR1(x1) =

y∈XR1|y1=x1

j∈R1
γj(yDj )
=

(y3,y4)∈X3×X4
γ 0
1 (x1, y3, y4)

zR3∈XR3|z3=y3

j∈R3
γj(zDj )

zR4∈XR4|z4=y4

j∈R4
γj(zDj )
=

(x3,x4)∈X3×X4
γ 0
1 (x1, x3, x4)˜νR3(x3)˜νR4(x4)
where XR1 denotes all the variable nodes that are neighbours of function nodes with R1,
retaining the same indices as the full set of variables, and similar. It is straightforward
to derive a similar expression when the function node has m neighbours.
This expression has the same form as the update rule given for µγj →X in Equation
(11.5). In other words, if ˜νR3(x3) = µX3→γ 0
1 (x3) and ˜νR4(x4) = µX4→γ 0
1 (x4), then the
result is proved. The algorithm proceeds to the leaf nodes of the factor graph.
Step 3:
There are two cases. If the leaf node is a function node (as in Step 1, going
from a variable to functions), then (clearly from the graph) this is a function (h say) of

328
FACTOR GRAPHS AND THE SUM PRODUCT ALGORITHM
x1
g ο
1
R1
g4
g3
R4
R3
x3
x4
x
x
mγ
ο
x
g1
ο
g1
ο
x1
3
4
1
m
m
Figure 11.12
Step 2.
a single variable (say Y) and (from Formula (11.5)),
ν(y) = h(y) = µh→Y(y).
If the leaf node is a variable node X (as in Step 2, going from functions to variables),
then the leaf variable is adjacent to a single function h (or else it is not a leaf), which
has neighbours (Y1, . . . , Ym, X), say, then
˜ν(x) = 1 = µX→h(x),
since if X is a leaf, then h is the only neighbour of X and hence µX→h(x) ≡1 from
Equation (11.4).
By tracing backward from the leaf nodes, it is now clear, by induction, that

y∈X ˜V \{i}
φ(y, xi) =
n

j=1
µγ 0
j →Xi(xi),
where (γ 0
1 , . . . , γ 0
n ) are the neighbours of node Xi.
Termination
Consider the termination formula
µX(x) =

γj ∈NX
µγj →X(x),

DETAILED ILLUSTRATION OF THE ALGORITHM
329
together with the formula for the message from a variable node to a function node:
µX→γj (x) =

h∈NX\{γj }
µh→X(x).
Suppose the factor graph is a tree. Then, since any variable to function message is the
product of all but one of the factors in the termination formula, it is clear that µX(x) may
be computed as the product of the two messages that were passed in opposite directions,
a) from the variable X to one of the functions and b) from the function to the variable X.
11.3
Detailed illustration of the algorithm
The example in this section is taken from taken from Frey [139]. The following ﬁgure
shows the ﬂow of messages that would be generated by the sum product algorithm applied
to the factor graph of the product
g(x1, x2, x3, x4, x5) = fA(x1)fB(x2)fC(x1, x2, x3)fD(x3, x4)fE(x3, x5).
shown in Figure 11.13.
The messages are generated in ﬁve steps, indicated with circles in the ﬁgure. The
same ﬂow is detailed below.
Step 1:
µfA→x1(x1) =

\x1
fA(x1) = fA(x1).
µfB→x2(x2) =

\x2
fB(x2) = fB(x2).
µx4→fD(x4) = 1
µx5→fE(x5) = 1
E
f
D
f
x1
x3
x2
x5
x4
C
f
B
f
A
f
1
4
4
5
5
4
3
3
4
2
5
1
5
2
2
2
1
1
Figure 11.13
Flows of messages.

330
FACTOR GRAPHS AND THE SUM PRODUCT ALGORITHM
Step 2:
µx1→fC(x1) = µfA→x1(x1) = fA(x1)
µx2→fC(x2) = µfB→x2(x2) = fB(x2)
µfD→x3(x3) =

\x3
µx4→fD(x4)fD(x3, x4) =

x4
fD(x3, x4)
µfE→x3(x3) =

\x3
µx5→fE(x5)fE(x3, x5) =

x5
fE(x3, x5)
Step 3:
µfC→x3(x3) =

\x3
µx1→fC(x1)µx2→fC(x2)fC(x1, x2, x3)
=

x1,x2
fA(x1)fB(x2)fC(x1, x2, x3)
µx3→fC(x3) = µfD→x3(x3)µfE→x3(x3) =

x4,x5
fD(x3, x4)fE(x4, x5)
Step 4:
µfC→x1(x1) =

\x1
µx3→fC(x3)µx2→fC(x2)fC(x1, x2, x3)
=

x2,x3,x4,x5
fD(x3, x4)fE(x4, x5)fB(x2)fC(x1, x2, x3)
µfC→x2(x2) =

\x2
µx3→fC(x3)µx1→fC(x1)fC(x1, x2, x3)
=

x1,x3,x4,x5
fD(x3, x4)fE(x4, x5)fA(x1)fC(x1, x2, x3)
µx3→fD(x3) = µfC→x3(x3)µfE→x3(x3)
=

x1,x2,x4,x5
fA(x1)fB(x2)fC(x1, x2, x3)fE(x3, x5)
µx3→fE(x3) = µfC→x3(x3)µfD→x3(x3)
=

x1,x2,x4,x5
fA(x1)fB(x2)fC(x1, x2, x3)fD(x3, x4).

DETAILED ILLUSTRATION OF THE ALGORITHM
331
Step 5:
µx1→fA(x1) = µfC→x1(x1) =

x2,x3,x4,x5
fB(x2)fC(x1, x2, x3)fD(x3, x4)fE(x4, x5)
µx2→fB(x2) = µfC→x2(x2) =

x1,x3,x4,x5
fA(x1)fC(x1, x2, x3)fD(x3, x4)fE(x4, x5)
µfD→x4(x4) =

\x4
µx3→fD(x3)fD(x3, x4)
=

x1,x2,x3,x5
fA(x1)fB(x2)fC(x1, x2, x3)fD(x3, x4)fE(x4, x5)
µfE→x5(x5) =

\x5
µx3→fE(x3)fE(x3, x5)
=

x1,x2,x3,x4
fA(x1)fB(x2)fC(x1, x2, x3)fD(x3, x4)fE(x3, x5).
Termination
g1(x1) = µfA→x1(x1)µfC→x1(x1) =

x2,x3,x4,x5
g(x1, x2, x3, x4, x5)
g2(x2) = µfB→x2(x2)µfC→x2(x2) =

x1,x3,x4,x5
g(x1, x2, x3, x4, x5)
g3(x3) = µfC→x3(x3)µfD→x3(x3)µfE→x3(x3) =

x1,x2,x4,x5
g(x1, x2, x3, x4, x5)
g4(x4) = µfD→x4(x4) =

x1,x2,x3,x5
g(x1, x2, x3, x4, x5)
g5(x5) = µfD→x5(x5) =

x1,x2,x3,x4
g(x1, x2, x3, x4, x5).
In the termination step, gi(xi) is computed as the product of all messages directed towards
xi. Equivalently, gi(xi) may be computed as the product of the two messages that were
passed in opposite directions over any single edge incident on xi, because the message
passed on any given edge is equal to the product of all but one of these messages.
Therefore (for example), there are three ways to compute g3(x3) by multiplying only two
messages together:
g3(x3) = µfC→x3(x3)µx3→fC(x3) = µfD→x3(x3)µx3→fD(x3) = µfE→x3(x3)µx3→fE(x3).

332
FACTOR GRAPHS AND THE SUM PRODUCT ALGORITHM
Notes
Chapter 11 presents a brief outline of some aspects of S.M. Aji and R.J. McEliece
(2000) [140], F.R. Ksischang, B.J. Frey and H.A. Loeliger [141], B.J. Frey [139] and N.
Wiberg [138], as related to Bayesian networks. Most of the ideas are originally developed
in N. Wiberg [138]. A well written treatise on bipartite graphs is found in [142].

FACTOR GRAPHS AND THE SUM PRODUCT ALGORITHM
333
11.4
Exercise: Factor graphs and the sum product
algorithm
Consider the directed acyclic graph below.
B
E
A
R
Figure 11.14
Burglary, earthquake and radio.
The variables are B – Burglary, A – Alarm, E – Earthquake and R – news broad-
cast.
These are random variables with the states (0 – no (false), 1 – yes (true)). The
alarm is reliable for detecting burglary, but also responds to minor earthquakes.
Radio broadcasts tell about occurrences of such earthquakes, but are not always
correct. The conditional probability distributions for this problem are given below.
PR|E =
R\E
0
1
0
0.99
0.05
1
0.01
0.95
PA|B,E(0|., .) =
E\B
0
1
0
0.97
0.05
1
0.05
0.02
PB(1) = 0.01, PE(1) = 0.999
Assume that the joint distribution PA,B,E,R factorizes recursively according to the
Bayesian network shown in Figure 11.14. Using the sum-product algorithm, compute
1. The conditional probability pB|A(1|1)
2. the conditional probability pB|A,R(1|1, 1).


References
1. Pearl, J. (2000) Causality, Cambridge University Press, Cambridge.
2. Pearson, K. (1892) The Grammar of Science. Elibron Classics, 2006.
3. Hume, D. (1731) An Enquiry Concerning Human Understanding, reprinted in Dover Philo-
sophical Classics, 2004.
4. Williamson, J. (2005) Bayesian Networks and Causality: Philosophical and Computational
Foundations, Oxford University Press, Oxford.
5. Langseth, H. and Portinale, L. (2007) Bayesian networks in reliability. Reliability Engineering
and System Safety, 92, 92–108.
6. Woof, D., Goldstein, M. and Coolen, F. (2002) Bayesian graphical models for software testing
IEEE Transactions in Software Engineering, 28(5), 510–525.
7. Markowetz, F. and Spang, R. (2007) Inferring cellular networks–a review. BMC Bioinfor-
matics, 8 (suppl. 6): S5, available from http://www.biomedcentral.com/1471-2105/8/S6/S5/
8. Gowadia, V. Farkas, C. and Valtorta, M. (2005) PAID: a probabilistic agent-based intrusion
detection system. Computers and Security, 24, 529–545.
9. Pearl, J. (1990) Probabilistic Reasoning in Intelligent Systems, 2nd edn revised, Morgan and
Kaufman Publishers Inc., San Francisco.
10. Halpern, J. (2003) Reasoning about Uncertainty, MIT Press, Cambridge, MA, 71–73.
11. Teller, P. and Fine, A. (1975) A Characterisation of conditional probability. Mathematics
Magazine, 48(5), 494–498.
12. Diaconis, P. and Zabell, S. L. (1982) Updating subjective probability. Journal of the American
Statistical Association, 77(380), 822–830.
13. Cheeseman, P. (1988) In defence of an enquiry into computer understanding. Computational
Intelligence, 4, 129–142.
14. Russell, S. and Norvig, P. (1995) Artiﬁcial Intelligence: A Modern Approach, Prentice Hall,
New Jersey.
15. Jeffrey, R. C. (2004) Subjective Probability: The Real Thing, Cambridge University Press,
Cambridge. UK.
Bayesian Networks: An Introduction
T. Koski, J. Noble
2009 John Wiley & Sons, Ltd

336
REFERENCES
16. Cheeseman, P. (1988) An enquiry into computer understanding. Computational Intelligence,
4, 58–66.
17. Lewis, D. (1986) A subjectivist’s guide to objective chance, in Philosophical Papers, vol. 2,
83–132.
18. Aumann, R. J. (1976) Agreeing to disagree. The Annals of Statistics, 4, 1236–1239.
19. Suppes, P. (1970) A Probabilistic Theory of Causation, Acta Philosophica Fennica, 24,
North-Holland Publishing Company, Amsterdam.
20. Freedman, D. and Humphreys, P. (1999) Are there algorithms that discover causal structure?
Synthese, 121, 29–54.
21. Woodward, J. (2001) Probabilistic causality, direct causes and counterfactual dependence
Stochastic Causality (eds M. Galavotti, P. Suppes and D. Constantini) CSLI Publications,
Stanford, California, 39–63.
22. Pearson, K. (1920) The fundamental problem of practical statistics. Biometrika, 13, 1–16.
23. De Finetti, B. (1937) La pr´evision: ses lois logiques, ses sources subjectives. Annales de
1’Institut Henri Poincar´e, 7, 1–68.
24. Jeffreys, H. (1939) Theory of Probability, Clarendon Press, 3rd edn, 1998.
25. Heckerman,
D.
(1998)
A tutorial on learning with Bayesian networks.
Report
#
MSR-TR-95-06, Microsoft Research, Redmont, Washington. http://research.microsoft.com/∼
heckerman/
26. Robert, C.R. (2001) The Bayesian Choice From Decision-Theoretic Foundations to Compu-
tational Implementation, Springer.
27. Chung, K.L. (2002) Will the sun rise again? Mathematical Medley, 29, 67–73.
Reprinted
2004, in Chance and Choice: Memorabilia, World Scientiﬁc, New Jersey, 1–8.
28. Lidstone, G.J. (1920) Note on the general case of the Bayes-Laplace formula for the inductive
or aposterior probabilities. Transactions of the Faculty of Actuaries. Faculty of Actuaries in
Scotland, 8, 182–192.
29. Perks, W. (1947) Some observations on inverse probability including a new indifference rule.
Journal of the Institute of Actuaries, 72, 285–334.
30. Good, I.J. (1968) Estimation of Probabilities, MIT Press, Cambridge MA.
31. Bernardo, J.M. and Smith, A.F.M. (1994) Bayesian Theory. John Wiley & Sons, Ltd.
32. Skyrms, B. (2000) Choice Chance: An Introduction to Inductive Logic, 4th edn, Wadsworth,
Belmont, California.
33. Hacking, I. (2001) An Introduction to Probability and Inductive Logic, Cambridge University
Press, Cambridge.
34. Neapolitan, R.E. (2004) Learning Bayesian Networks, Pearson Prentice Hall, New Jersey.
35. Korb, K.B. and Nicholson, A.E. (2004) Bayesian Artiﬁcial Intelligence, Chapman and Hall.
36. Bayes, T., and Price, R. (1763) An essay towards solving a problem in the doctrine of
chance. By the late Rev. Mr. Bayes, F.R.S. Communicated by Mr. Price, in a letter to John
Canton, A.M.F.R.S. Philosophical Transactions of the Royal Society of London, 53, 370–418.
Reprinted in Biometrika (1958) 45, 296–315.
37. Gillies, D. (1987) Was Bayes a Bayesian? Historia Mathematica, 14, 325–346.
38. Molina, E.C. (1931) Bayes theorem: an expository presentation. The Annals of Mathematical
Statistics, 2, 23–37.
39. Stigler, S.M. (1982) Thomas Bayes’s Bayesian inference. Journal of the Royal Statistical
Society, Series A, 145, 250–258.
40. Stigler, S.M. (1983) Who discovered Bayes’s theorem? American Statistician, 37, 290–296.
41. Bellhouse, D.R. (2004) The reverend Thomas Bayes, FRS: a biography to celebrate the
tercentenary of his birth. Statistical Science, 19(1), 3–43.
42. Murray, F.H. (1930) Note on the scholium of Bayes. Bulletin of the American Mathematical
Society, 36, 129–132.
43. Gillies, D. (2000) Philosophical Theories of Probability, Routledge, London.

REFERENCES
337
44. Pearl, J. (1982) Reverend Bayes on inference engines: a distributed hierarchical approach.
AAAI-82 Proceedings, 133–136.
45. Good, I.J. (1994) Causal tendency, necessitivity, and sufﬁcientivity: an updated review, in
P. Humphreys (ed.) Patrick Suppes: Scientiﬁc Philosopher, Kluwer Academic Publ. Dor-
drecht, vol. 1, 293–316.
46. Savage, J.L. (1966) Foundations of Statistics, John Wiley and Sons, Inc., New York.
47. Dickey, J.M. (1983) Multiple hypergeometric functions: probabilistic interpretations and sta-
tistical uses. Journal of the American Statistical Association, 78(383), 628–637.
48. Spirtes, P. Glymour, C. and Scheines, R. (2000) Causation, Prediction and Search, 2nd edn,
MIT Press.
49. P˜ena, J.M., Nilsson, R., Bj¨orkegren, J., and Tegner, J. (2007) Towards scalable and data
efﬁcient learning of Markov boundaries International Journal of Approximate Reasoning, 45,
211–232.
50. Schachter, R.D. (1998) Bayes ball: the rational pass time for determining irrelevance and
requisite information in belief networks and inﬂuence diagrams. Proceedings of the 14th
Annual Conference on Uncertainty in Artiﬁcial Intelligence (eds G.F. Cooper and S. Moral)
Morgan Kaufmann, San Francisco. 480–487.
51. Weidl, G. Madsen, A.L. and Israelson, S. (2005) Applications of object-oriented root cause
analysis and decision support on operation of complex continuous processes. Computers and
Chemical Engineering, 29, 1996–2009.
52. Shafer, G. (1996) Probabilistic Expert Systems, SIAM, Philadelphia.
53. Vorobev, N. (1963) Markov measures and Markov extensions. Theory of Probability and
Applications, 8, 420–429.
54. Langseth, H. and Bangsø, O. (2001) Parameter learning in object oriented Bayesian networks.
Annals of Mathematics and Artiﬁcial Intelligence, 32, 221–243.
55. Andersson, S.A., Madigan, D., Perlman, M.D. and Triggs, C.M. (1997) A graphical charac-
terisation of lattice conditional independence models. Annals of Mathematics and Artiﬁcial
Intelligence, 21, 27–50.
56. Pearl, J., Geiger, D. and Verma, T. (1989) Conditional independence and its representations.
Kybernetica, 25(2), pp. 33–44.
57. Wright, S. (1921) Correlation and causation, Journal of Agricultural Research, 20, 557–585.
58. Shipley, B. (2000) Cause and Correlation in Biology: A User’s Guide to Path Analysis,
Structural Equations and Causal Inference, Cambridge University Press.
59. Kiiveri, H., Speed, T.P., and Carlin, J.B. (1984) Recursive causal models. Journal of the
Australian Mathematical Society (Series A), 36, 30–52.
60. Pearl, J. and Verma, T. (1987) The logic of representing dependencies by directed acyclic
graphs. Proceedings of the AAAI , Seattle, 374–379.
61. Geiger, D. Verma, T. and Pearl, J. (1990) Identifying independence in Bayesian networks.
Networks, 20, 507–534.
62. Verma, P. and Pearl, J. (1992) An Algorithm for Deciding if a Set of Observed Independencies
has a Causal Explanation in Uncertainty in Artiﬁcial Intelligence, Proceedings of the Eighth
Conference (eds D. Dubois, M.P. Welman, B. D’Ambrosio and P. Smets), Morgan Kaufman,
San Francisco, 323–330.
63. Murphy, K. (1998) A brief introduction to graphical models and Bayesian Networks.
http://www.cs.ubc.ca/∼murphyk/Bayes/bnintro.html
64. Garcia, L.D., Stillman, M. and Sturmfels, B. (2005) Algebraic geometry of Bayesian networks
Journal of Symbolic Computation, 39, 331–355.
65. Jeffrey, R.C. (1965) The Logic of Decision, McGraw-Hill, New York; University of Chicago
Press, Chicago, 2nd edn revised, 1990.
66. Chan, H. and Darwiche, A. (2005) A distance measure for bounding probabilistic belief
change. International Journal of Approximate Reasoning, 38, 149–174.

338
REFERENCES
67. Cowell, R.G., Dawid, A.P., Lauritzen, S.L. and Spiegelhalter, D.J. (1999) Probabilistic Net-
works and Expert Systems, Springer, New York.
68. Dawid, A.P. (1992) Applications of a general propagation algorithm for probabilistic expert
systems. Statistics and Computing 2, 25–36.
69. Dechter, R. (1999) Bucket elimination: a unifying framework for reasoning. Artiﬁcial Intel-
ligence 113, 41–85.
70. Dawid, A.P. (1997) Conditional independence for statistics and AI. Tutorial at AISTATS–97.
citeseer.ist.psu.edu/article/dawid97conditional.html.
71. Dawid, A.P. (1997) Conditional independence, in Encyclopedia of Statistical Sciences. Update
Volume 2, eds S. Kotz, C.B. Read and D.L. Banks, Wiley-Interscience, pp. 146–155.
72. Heckerman, D. Geiger, D. and Chickering, D.M. (1995) learning Bayesian networks: the
combination of knowledge and statistical data. Machine Learning, 20, 197–243.
73. Murphy, K. (2002) Dynamic Bayesian networks. http://www.cs.ubc.ca/∼murphyk/Papers/
dbnchapter.pdf
74. Chickering, D.M., Heckerman, D. and Meek, C. (2004) Large sample learning of Bayesian
networks is NP-Hard. Journal of Machine Learning Research, 5, 1287–1330.
75. H¨aggstr¨om, O. (2002) Finite Markov Chains and Algorithmic Applications, Cambridge Uni-
versity Press.
76. Hastings, W.K. (1970) Monte Carlo sampling methods using Markov chains and their appli-
cations, Biometrika, 57, 97–109.
77. Lauritzen, S. (1992) Propagation of probabilities, means and variances in mixed graphical
association models. Journal of the American Statistical Association, 87(420), 1098–1108.
78. Lunn, D.J., Thomas, A., Best, N. and Spiegelhalter, D. (2000) WinBUGS–A Bayesian
modelling framework: concepts, structure, and extensibility. Statistics and Computing, 10,
325–337.
79. Corander, J., Koski, T. and Gyllenberg, M. (2006) Bayesian model learning based on a
parallel MCMC Strategy. Statistics and Computing, 16, 355–362.
80. Moore, A. and Soon Lee, M. (1998) Cached sufﬁcient statistics for efﬁcient machine learning
with large data sets. Journal of Artiﬁcial Intelligence Research, 8, 67–91.
81. Chan, H. and Darwiche, A. (2005) On the revision of probabilistic beliefs using uncertain
evidence. Artiﬁcial Intelligence, 163, 67–90.
82. Cooper, G.F. (1990) The computational complexity of probabilistic inference using Bayesian
belief Networks. Artiﬁcial Intelligence, 42, 393–405.
83. Pearl, J. (1987) Evidential reasoning using stochastic simulation of causal models. Artiﬁcal
Intelligence, 32, 245–257.
84. Arnborg, S. (1985) Efﬁcient algorithms for combinatorial problems with bounded decompos-
ability: a survey. BIT, 25, 2–23.
85. Arnborg, S. (1993) Graph decompositions and tree automata in reasoning with uncertainty
Journal of Experimental and Theoretical Artiﬁcial Intelligence, 5, 335–357.
86. Frydenberg, M. (1990) The chain graph Markov property. Scandinavian Journal of Statistics,
17, 333–353.
87. Madigan, D., Andersson, S A., Perlman, M.D. and Volinsky, C.T. (1996) Bayesian model
averaging and model selection for markov equivalence classes of acyclic digraphs. Commu-
nications in Statistics: Theory and Methods, 25(11), 2493–2519.
88. Golumbic, M.C. (2004) Algorithmic Graph Theory and Perfect Graphs, Elsevier.
89. Studen´y, M. (2005) Probabilistic Conditional Independence Structures, Springer.
90. Geiger, D. and Heckerman, D. (1997) A characterization of the Dirichlet distribution through
global and local parameter independence. The Annals of Statistics, 25 (3), 1344–1369.
91. Ramoni, M. and Sebastiani, P. (1997) Parameter estimation in Bayesian networks from incom-
plete databases, KMI-TR-97-22, Knowledge Media Institute, The Open University.
92. Jensen, F.V. (2001) Bayesian Networks and Decision Graphs, Springer

REFERENCES
339
93. Antal, P., Fannes, G., Timmerman, D., et al. (2004) Using literature and data to learn
Bayesian networks as clinical models of ovarian tumors. Artiﬁcial Intelligence in Medicine,
30, 257–281.
94. Castelo, R. and Siebes, A. (2000) Priors on network structures. Biasing the search for Bayesian
networks. International Journal of Approximate Reasoning, 24, 39–57.
95. Cooper, G.F. and Herskovitz, E. (1992) A Bayesian method for the induction of probabilistic
networks from data. Machine Learning, 9, 309–347.
96. Robinson, R.W. (1977) Counting unlabelled acyclic digraphs, in Springer Lecture Notes in
Mathematics: Combinatorial Mathematics V , C.H.C. Little (ed.) 28–43.
97. Koivisto, M., and Sood, K. (2004) Exact Bayesian structure discovery in Bayesian networks
Journal of Machine Learning Research, 5, 549–573.
98. Kontkanen, P. Myllym¨aki, P. et al. (2000) On predictive distributions and Bayesian networks.
Statistics and Computing, 10, 39–54.
99. Chow, C.K. and Liu, C.N. (1968) Approximating discrete probability distributions with depen-
dence trees. IEEE Transactions on Information Theory, IT-14, (3), 462–467.
100. Gyllenberg, M. and Koski, T. (2002) Tree augmented classiﬁcation of binary data minimizing
stochastic complexity. Technical Report, Matematiska institutionen, University of Link¨oping,
LiTH-MAT-R-2002-4.
101. Suzuki, J. (1999) Learning Bayesian belief networks based on the minimum description length
Principle: Basic Properties. IEICE Transactions on Fundamentals, E82, (10) 2237–2245.
102. Lazkano, E. Sierra, B. et al. (2007) On the use of Bayesian networks to develop behaviours
for mobile robots. Robots and Autonomous Systems, 55, 253–265.
103. Tsamardinos, I., Brown, L.E. and Aliferis, C.F. (2006) The max-min hill-Climbing Bayesian
network structure learning algorithm Machine Learning, 65, 31–78.
104. P˜ena, J.M. (2007) Approximate counting of graphical models via MCMC. Proceedings of the
11th Conference in Artiﬁcial Intelligence, 352–359.
105. Fowlkes, E.B., Freeny, A.E. and Ladwehr, J.M. (1988) Evaluating logistic models for large
contingency tables. Journal of the American Statistical Association, 83(403), 611–622.
106. Lacampagne, C.B. (1979) An evaluation of the Women and Mathematics (WAM) program and
associated sex-related differences in the teaching, learning and counseling of mathematics.
Unpublished Ed. D. thesis, Columbia University Teacher’s College.
107. Corander, J., Ekdahl, M. and Koski, T. (2008) Parallel interacting MCMC for learning of
Bayesian network topologies. Data Mining and Knowledge Discovery, 17 (3), 431–456.
108. Castillo, E., Guti´errez, J.M. and Hadi, A.S. (1996) A new method for efﬁcient symbolic
propagation in discrete Bayesian networks Networks, 28 (1), 31–43.
109. Castillo, E., Guti´errez, J.M. and Hadi, A.S. (1997) Sensitivity analysis in discrete Bayesian
networks. IEEE Transactions on Systems, Man and Cybernetics. Part A: Systems and Humans,
27(4).
110. Coup´e, V.M. and van der Gaag, L.C. (1998) Practicable sensitivity analysis of Bayesian
belief networks. Prague Stochastics ’98: Proceedings of the Joint Session of the 6th Prague
Symposium of Asymptotic Statistics and the 13th Prague Conference on Information Theory,
Statistical Decision Functions and Random Processes, 81–86.
111. Chan, H. and Darwiche, A. (2002) When do numbers really matter? Journal of Artiﬁcial
Intelligence Research, 17, 265–287.
112. Neal, R. (1992) Correctionist learning of belief networks. Artiﬁcial Intelligence, 56, 71–113.
113. Barndorff-Nielsen, O. (1978) Information and Exponential Families in Statistical Theory,
John Wiley & Sons, Ltd.
114. Jordan, M.I., Ghahramani, Z., Jaakkola, T.S. and Saul, L.K. (1999) An Introduction to Vari-
ational Methods for Graphical Models. Machine Learning, 37, 183–233.
115. Wainright, M.J. and Jordan, M.I. (2003) Graphical models, exponential families and vari-
ational Inference. Technical report 649, Department of Statistics, University of California,
Berkeley.

340
REFERENCES
116. Humphreys, K. and Titterington, D.M. (2000) Improving the mean-ﬁeld approximation in
belief networks using Bahadur’s reparameterisation of the multivariate binary distribution
Neural Processing Letters, 12, 183–197.
117. Strotz, R.H. and Wold, H.O.A. (1960) Recursive versus nonrecursive systems: an attempt at
synthesis. Econometrica 28, 417–427.
118. Pearl, J. (1995) Causal diagrams for empirical research. Biometrika, 82, 669–710.
119. Eaton, D., and Murphy, K. (2007) Exact Bayesian structure learning from uncertain inter-
ventions AI & Statistics. http://www.cs.ubc.ca/∼murphyk/my papers.html
120. Wold, H.O.A. (1963) Orientering i det statistiska arbetsf¨altet, biblioteksf¨orlaget, Stockholm.
121. Edwards, D. (2000) Introduction to Graphical Modelling, Chapter 9: Causal inference.
Springer.
122. Freedman, D. (1999) From association to causation: some remarks on the history of statistics.
Statistical Science, 14(3), 243–258.
123. Lindley, D.V. (2002) Seeing and doing: the concept of causation. International Statistical
Review/Revenue International de Statistique, 70, 191–197.
124. Meek, C. and Glymour, C. (1994) Conditioning and intervening. British Journal of Philosophy
of Science, 45, 1001–1021.
125. Lauritzen, S. (2001) Causal Inference from Graphical Models, in Complex Stochastic Systems,
Chapman and Hall, 63–108.
126. Tian, J. and Pearl, J. (2002) A General Identiﬁcation Condition for Causal Effects. Proceed-
ings of the Eighteenth National Conference on Artiﬁcal Intelligence, AAAI Press, Menlo Park,
California 567–573.
127. Huang, Y. Valtorta, M. (2006) Pearl’s calculus of intervention is complete. Proceedings of
the 22nd Conference on Uncertainty in Artiﬁcal Intelligence, UAI Press, 217–224.
128. Pearl, J. (1995) Causal inference from indirect experiments. Artiﬁcial Intelligence in Medicine,
7, 561–582.
129. Spohn, W. (2001) Bayesian nets are all there is to causal dependence. Stochastic Causality
(eds Galavotti, M., Suppes, P. and Constantini, D.), CLSI Publications, Stanford, California,
157–172.
130. Becker, A., and Geiger, D. (2001) A sufﬁciently fast algorithm for ﬁnding close to optimal
clique trees. Artiﬁcial Intelligence, 125, 3–17.
131. Lauritzen, S. and Spiegelhalter, D. (1988) Local computations with probabilities on graphical
structures and their application to expert systems. Journal of the Royal Statistical Society,
Series B, 50, 157–224.
132. Lauritzen, S. (1996) Graphical Models, Clarendon Press, Oxford.
133. Bach, F.R. and Jordan, M.I. (2002) Thin junction trees. Advances in Neural Information
Processing Systems (NIPS) 14.
134. Valtorta, M. Kim, Y.G. and Vomlel, J. (2002) Soft evidential update for probabilistic multi-
agent systems. International Journal of Approximate Reasoning, 29(1), 71–106.
135. Brown, D.T. (1959) A note on approximations to discrete probability distributions. Informa-
tion and Control, 2, 386–392.
136. H´ajek, P. Havranek, T. and Jirousek, R. (2000) Uncertain Information Processing in Expert
Systems, CRC Press, Boca Raton, Florida.
137. Deming, W.E., and Stephan, F.F. (1940) On a least squares adjustment of a sampled frequency
table when the expected marginal totals are known. Annals of Mathematical Statistics, 11,
427–444.
138. Wiberg, N. (1996) Codes and decoding on general graphs. Link¨oping Studies in Science and
Technology. Dissertation 440, Link¨opings Universitet, Link¨oping.
139. Frey, B.J. (1998) Graphical Models for Machine Learning and Digital Communication, MIT
Press, Cambridge MA.

REFERENCES
341
140. Aji, S.M., and McEliece, R.J. (2000) The generalised distributive law. IEEE Transactions on
Information Theory, 46, 325–343.
141. Ksischang, F.R., Frey, B.J., Loeliger, H.A. (2001) Factor graphs and the sum product algo-
rithm IEEE Transactions on Information Theory, 47, 498–519.
142. Asratian, A.S., Denley, T.M.J. and H¨aggkvist, R. (1998) Bipartite Graphs and their Applica-
tions, Cambridge University Press, Cambridge.


Index
I-equivalence, 70
I-map, 69, 70
perfect, 69
d-connected, 50
d-separation, 41, 47, 49, 66–67
d-separation implies conditional
independence, 66
G-Markovian
locally, chain graph, 142
locally, DAG, 68
active ﬂow, 296
ancestral
boundary, 141
set, 141
set (minimal), 141
association (Yule), 31
associative law, 55
Aumann, R.J., 15
agreeing to disagree, 24
back door criterion, 257, 270, 271, 272
Bayes
ball, 50
factor, 10, 209
rule, 1, 9–11
update, 9
Bayes, Reverend Thomas, 28
Bayesian network, 1, 4, 58, 58–63
object oriented, 63–66
Bayesian Networks: An Introduction
T. Koski, J. Noble
2009 John Wiley & Sons, Ltd
Bayesian sufﬁcient statistics, 92
Bernoulli, 231
multivariate, 203
Beta
density, 21, 22, 233
integral, 21
billiards, 28
bipartite graph, 320
bipartite graphical model, 236
Boltzmann-Shannon entropy, 239
bucket elimination, 89
canonical parameters, 230, 248
causal predictive inference, 255
causal relations, 2
causality, 255
chain
connection, 47
node, 47
chain component, 140
Chan-Darwiche distance, 202, 202–216
charge, 281
contraction, 282
restriction, 298
chest clinic problem, 89
cholera, 31
chord, 125, 306
Chow-Liu tree, 177–183
predictive approach, 179
clique, 124, 131

344
INDEX
collider
connection, 48
node, 48
commutative law, 55, 57
complete, 307
complex, 142
minimal, 142
conditional Gaussian distribution, 248,
246–251
mean parameters, 249
parametrization, 248
update using a junction
tree, 305–311
conditional Gaussian potential, 249
canonical parameters, 249
conditional Gaussian regression, 250
conditional independence, 38–41, 66
characterizations, 39
conditioning
‘do’, 257
‘see’, 257
conﬁguration, 54, 281
confounding, 256, 270
conjugate dual, 240
connected
component, 44, 45
graph, 44
two nodes, 124
consistency, 69, 171–176, 302–305
global, 302
local, 302
contraction of a charge on a junction
tree, 294
controlled experiment, 2, 3, 255
to establish the model within the
equivalence class, 262
Cooper-Herskovitz likelihood, 169
cross product difference, 78
cycle, 44, 306
DAG Isomorphic, 171
Dickey, J.M., 36
directed acyclic graphs, 41
transitive, 79
Dirichlet
density, 25
integral, 24
distance, 201
distributive law, 57, 280
marginalization, 283
divorcing, 116
do-calculus, 256
domain graphs, 284
elimination, 284
domain, 130, 286
order, 130
sequence, 129
Engstr¨om, A., 47
entropy, 229
error correcting code, 320
Euler Gamma function, 21
evidence, 81
hard, 81
soft, 81, 85–88
virtual, 81, 85–88
weight of, 32
evidence potentials, 282
exogeneity, 263
explaining away, 51, 53, 63
earthquake, burglar, alarm, 75
exponential distribution, 232
exponential family, 229, 230
exponential parameters, 230
factor graph, 319
factorizability, 319
factorization of a probability function,
46
along a directed acyclic graph,
46
along an undirected graph, 288
fading, 161
faithful, 69, 143, 172
Fenchel inequality, 242
Fenchel Legendre conjugate, 239
ﬁnding
hard, 81
soft, 81
ﬁre, 199
Fisher, R.A., 2
ﬂow of messages, 292, 294–301
CG distribution, 305
forest, 45

INDEX
345
fork
connection, 47
node, 47
fractional updating, 161
frequentist, 17
function node, 320
Gates, W.H., 4
Gaussian, 232
global Markov property, 143
Good, I.J., 33
graph, 41
chain, 139, 138–143, 188
complete, 124
decomposable, 123, 126, 279
decomposition, 126
directed, 41, 43
directed acyclic, 45
directed acyclic marked graph, 306
domain graph, 286
essential, 139, 138–143, 187
family, 42
marked graph, 305
moral graph for a DAG, 125, 286
moral graph for chain graph, 143
simple, 41
strong decomposition, 306
strongly decomposable, 306
undirected, 41, 43
weak decomposition, 306
graphical equivalence, 143
graphical models, 1
greedy algorithm, 183
hidden Markov model, 322
hidden variables, 3
HUGIN, 75, 118, 287
Hume, David, 2, 29
identiﬁability, 4, 270, 271, 273
immorality, 134
independence, 37
inductive learning, 11–14
inference algorithm, engine, 287
instantiated, 6, 46
instantiation
complete, 150
intervention, 2, 3, 255
formula, 258
measure, 259
invariance, 263
iterative proportional ﬁtting
procedure, 312
Jeffrey’s rule, 13, 86, 113, 212
Jensen, J.L., 153
junction tree, 131–133, 279
construction, 131
factorization along, 291
probability updating, 279
soft evidence, 311
thin, 301
K2 structural learning
algorithm, 183–184
Kruskal’s algorithm, 177
Kullback-Leibler divergence, 177, 184,
202, 229, 153, 241–242
dual form, 242
mixed form, 242
primal form, 242
Laplace rule of succession, 24, 35
lattice conditional independence
property, 79
lazy big clique algorithm, 312
lazy propagation, 297
leaf, 125
learning, 149
likelihood
estimate, 152
function, 152
likelihood ratio, 19, 204
local directed Markov condition, 68
chain graph, 142
local potential, 319
local surgery, 259
log likelihood function, 152
log partition function, 230, 237–238
marginal charge, 291
marginal distribution, 8
marginalization, 56
computational tree, 284

346
INDEX
Markov blanket, 50, 77
Markov chain
aperiodic, 104
irreducible, 104
simulation, 103
stationary distribution, 101
time homogeneous, 100
time reversible, 108
transition matrix, 101
Markov chain Monte Carlo, 21, 100
learning the graph structure, 186
Model Composition Algorithm, 187
Markov equivalence, 69, 70, 133–138,
171–176
chain graph, 143
Markov model, 68
Markov probability distribution, 288
Markov property, 290
undirected global, 290
maximum minimum hill climbing
algorithm, 184–186
maximum minimum parents children
algorithm, 184
maximum posterior estimate, 23, 155
mean ﬁeld lower bound, 244
mean ﬁeld theory, 243–246
mean parameters, 238, 249
mean posterior estimate, 154
Metropolis-Hastings algorithm, 108–112,
117
minimal representation, 231
missing data, 160
modularity, 1, 98, 263
moment generating function, 247
multinomial
coefﬁcient, 25
distribution, 25
sampling, 24
multinomial sampling
maximum likelihood estimator, 151
multivariate normal distribution, 247
mutilated graph, 259, 264
mutual information, 177
naive mean ﬁeld update, 245
necessitivity, 33
negative causation, 32
node
ancestor, 44
child, 42
descendant, 44
neighbour, 42
parent, 42
simplicial, 124, 131
node elimination, 129, 286
ﬁll ins, 129
perfect sequence, 128
noisy ‘or’, 234
causal network, 235
gate, 235
inhibitor, 235
NP-hard, 170, 176
object oriented assumption, 65
odds, 11, 32, 223
one eye problem, 320
optimization
constraint based, 167
score function, 167
over-complete representation, 231
parameters, 197
path, 44
directed path, 44
pattern recognition, 178
Pearl’s method, 13, 87, 113, 215
Pearson, K., 2
Poisson distribution, 233
potential, 53–57, 280
addition, 54
division, 55
domain, 54
extending the domain, 55
multiplication, 54
prediction sufﬁciency, 92, 95
for a Bayesian network, 97
predictive distribution, 159
predictive probability, 15, 23
Price, Richard, 28
prima facie cause, 31
probabilistic contraposition, 33
probability function, 6
proportional scaling, 198, 218
optimality of, 211

INDEX
347
propositional logic, 234
disjunction, 234
QMR-DT database, 236
query, 88, 197
maximum aposteriori hypothesis,
88
most probable conﬁguration, 88
probability updating, 88
query constraints, 216
random
variables, 6
vectors, 6
regular family, 231
role model, 190
root, 125
strong, 308
root cause analysis, 53, 79
Savage, J.L., 35
schedule, 296, 296
fully active, 296
semi-Markovian model, 271
Semmelweis, I., 2
sensitivity, 197
separator, 124, 131, 307
minimal, 124, 307
Shachter, R., 50
Shafer, 58
Shannon, 153
Shannon entropy, 153
sigmoid belief network model, 233
Simpson’s paradox, 269
skeleton, 134
statistic, 92
Bayesian sufﬁcient, 92, 231
minimal sufﬁcient, 231
stochastic complexity distribution, 170
strong component, 124
strongly protected, 140, 188
structure, 168
likelihood, 168
prior distribution, 168
Sturmfels, B., 78
sub-graph, 43
induced, 43
sub-tree
base, 298
live, 298
sufﬁciency
Bayesian, 231
sufﬁcientivity, 33
sum product algorithm, 319
sum product rule
initialization, 323
schedule, 325
termination, 324
sum product update rule, 323
Suppes, P., 31
support, 203
sure-thing principle, 268
thumb-tack, 20–24, 94, 203, 205
Tirri, H., 170
trail, 43
active, 49
active node, 135
blocked, 49
minimal S-active, 135
tree, 45
diameter, 125
rooted, 125
trek, 72
triangulated, 125, 307
Turing machine, 170
update ratio, 293
variable node, 320
variables
hidden, 116
variational principle, 239
Vorobev, N., 60
weight of evidence, 32
Wiberg, N., 320
Women in Mathematics, 189–191
Yule, G.U., 31

WILEY SERIES IN PROBABILITY AND STATISTICS
Established by WALTER A. SHEWHART AND SAMUEL S. WILKS
Editors: David J. Balding, Noel A. C. Cressie, Garrett M. Fitzmaurice, Harvey
Goldstein, Geert Molenberghs, David W. Scott, Adrian F.M. Smith, Ruey S. Tsay,
Sanford Weisberg
Editors Emeriti: Vic Barnett, J. Stuart Hunter, David G. Kendall, Jozef L. Teugels
The Wiley Series in Probability and Statistics is well established and authoritative. It covers many
topics of current research interest in both pure and applied statistics and probability theory. Written
by leading statisticians and institutions, the titles span both state-of-the-art developments in the ﬁeld
and classical methods.
Reﬂecting the wide range of current research in statistics, the series encompasses applied, method-
ological and theoretical statistics, ranging from applications and new techniques made possible by
advances in computerized practice to rigorous treatment of theoretical approaches.
This series provides essential and invaluable reading for all statisticians, whether in academia,
industry, government, or research.
ABRAHAM and LEDOLTER · Statistical Methods for Forecasting
AGRESTI · Analysis of Ordinal Categorical Data
AGRESTI · An Introduction to Categorical Data Analysis
AGRESTI · Categorical Data Analysis, Second Edition
ALTMAN, GILL and McDONALD · Numerical Issues in Statistical Computing for the
Social Scientist
AMARATUNGA and CABRERA · Exploration and Analysis of DNA Microarray and
Protein Array Data
ANDˇEL · Mathematics of Chance
ANDERSON · An Introduction to Multivariate Statistical Analysis, Third Edition
* ANDERSON · The Statistical Analysis of Time Series
ANDERSON, AUQUIER, HAUCK, OAKES, VANDAELE and WEISBERG · Statistical
Methods for Comparative Studies
ANDERSON and LOYNES · The Teaching of Practical Statistics
ARMITAGE and DAVID (editors) · Advances in Biometry
ARNOLD, BALAKRISHNAN and NAGARAJA · Records
* ARTHANARI and DODGE · Mathematical Programming in Statistics
* BAILEY · The Elements of Stochastic Processes with Applications to the Natural Sciences
BALAKRISHNAN and KOUTRAS · Runs and Scans with Applications
BALAKRISHNAN and NG · Precedence-Type Tests and Applications
BARNETT · Comparative Statistical Inference, Third Edition
BARNETT · Environmental Statistics: Methods & Applications
BARNETT and LEWIS · Outliers in Statistical Data, Third Edition
BARTOSZYNSKI and NIEWIADOMSKA-BUGAJ · Probability and Statistical Inference
BASILEVSKY · Statistical Factor Analysis and Related Methods: Theory and Applications
BASU and RIGDON · Statistical Methods for the Reliability of Repairable Systems
BATES and WATTS · Nonlinear Regression Analysis and Its Applications
BECHHOFER, SANTNER and GOLDSMAN · Design and Analysis of Experiments for
Statistical Selection, Screening and Multiple Comparisons
BEIRLANT, GOEGEBEUR, SEGERS, TEUGELS and DE WAAL · Statistics of Extremes:
Theory and Applications
BELSLEY · Conditioning Diagnostics: Collinearity and Weak Data in Regression
BELSLEY, KUH and WELSCH · Regression Diagnostics: Identifying Inﬂuential Data and
Sources of Collinearity
*Now available in a lower priced paperback edition in the Wiley Classics Library.

BENDAT and PIERSOL · Random Data: Analysis and Measurement Procedures, Third
Edition
BERNARDO and SMITH · Bayesian Theory
BERRY, CHALONER and GEWEKE · Bayesian Analysis in Statistics and Econometrics:
Essays in Honor of Arnold Zellner
BHAT and MILLER · Elements of Applied Stochastic Processes, Third Edition
BHATTACHARYA and JOHNSON · Statistical Concepts and Methods
BHATTACHARYA and WAYMIRE · Stochastic Processes with Applications
BIEMER, GROVES, LYBERG, MATHIOWETZ and SUDMAN · Measurement Errors in
Surveys
BILLINGSLEY · Convergence of Probability Measures, Second Edition
BILLINGSLEY · Probability and Measure, Third Edition
BIRKES and DODGE · Alternative Methods of Regression
BISWAS, DATTA, FINE and SEGAL · Statistical Advances in the Biomedical Sciences:
Clinical Trials, Epidemiology, Survival Analysis, and Bioinformatics
BLISCHKE and MURTHY (editors) · Case Studies in Reliability and Maintenance
BLISCHKE and MURTHY · Reliability: Modeling, Prediction and Optimization
BLOOMFIELD · Fourier Analysis of Time Series: An Introduction, Second Edition
BOLLEN · Structural Equations with Latent Variables
BOLLEN and CURRAN · Latent Curve Models: A Structural Equation Perspective
BOROVKOV · Ergodicity and Stability of Stochastic Processes
BOSQ and BLANKE · Inference and Prediction in Large Dimensions
BOULEAU · Numerical Methods for Stochastic Processes
BOX · Bayesian Inference in Statistical Analysis
BOX · R. A. Fisher, the Life of a Scientist
BOX and DRAPER · Empirical Model-Building and Response Surfaces
* BOX and DRAPER · Evolutionary Operation: A Statistical Method for Process
Improvement
BOX · Improving Almost Anything Revised Edition
BOX, HUNTER and HUNTER · Statistics for Experimenters: An Introduction to Design,
Data Analysis and Model Building
BOX, HUNTER and HUNTER · Statistics for Experimenters: Design, Innovation and
Discovery, Second Edition
BOX and LUCE ˜NO · Statistical Control by Monitoring and Feedback Adjustment
BRANDIMARTE · Numerical Methods in Finance: A MATLAB-Based Introduction
BROWN and HOLLANDER · Statistics: A Biomedical Introduction
BRUNNER, DOMHOF and LANGER · Nonparametric Analysis of Longitudinal Data in
Factorial Experiments
BUCKLEW · Large Deviation Techniques in Decision, Simulation and Estimation
CAIROLI and DALANG · Sequential Stochastic Optimization
CASTILLO, HADI, BALAKRISHNAN and SARABIA · Extreme Value and Related
Models with Applications in Engineering and Science
CHAN · Time Series: Applications to Finance
CHARALAMBIDES · Combinatorial Methods in Discrete Distributions
CHATTERJEE and HADI · Regression Analysis by Example, Fourth Edition
CHATTERJEE and HADI · Sensitivity Analysis in Linear Regression
CHERNICK · Bootstrap Methods: A Practitioner’s Guide
CHERNICK and FRIIS · Introductory Biostatistics for the Health Sciences
CHIL´ES and DELFINER · Geostatistics: Modeling Spatial Uncertainty
CHOW and LIU · Design and Analysis of Clinical Trials: Concepts and Methodologies,
Second Edition
CLARKE · Linear Models: The Theory and Application of Analysis of Variance
CLARKE and DISNEY · Probability and Random Processes: A First Course with
Applications, Second Edition
* COCHRAN and COX · Experimental Designs, Second Edition
CONGDON · Applied Bayesian Modelling
*Now available in a lower priced paperback edition in the Wiley Classics Library.

CONGDON · Bayesian Models for Categorical Data
CONGDON · Bayesian Statistical Modelling, Second Edition
CONOVER · Practical Nonparametric Statistics, Second Edition
COOK · Regression Graphics
COOK and WEISBERG · An Introduction to Regression Graphics
COOK and WEISBERG · Applied Regression Including Computing and Graphics
CORNELL · Experiments with Mixtures, Designs, Models and the Analysis of Mixture
Data, Third Edition
COVER and THOMAS · Elements of Information Theory
COX · A Handbook of Introductory Statistical Methods
* COX · Planning of Experiments
CRESSIE · Statistics for Spatial Data, Revised Edition
CS ¨ORG ¨O and HORV ´ATH · Limit Theorems in Change Point Analysis
DANIEL · Applications of Statistics to Industrial Experimentation
DANIEL · Biostatistics: A Foundation for Analysis in the Health Sciences, Sixth Edition
* DANIEL · Fitting Equations to Data: Computer Analysis of Multifactor Data, Second
Edition
DASU and JOHNSON · Exploratory Data Mining and Data Cleaning
DAVID and NAGARAJA · Order Statistics, Third Edition
* DEGROOT, FIENBERG and KADANE · Statistics and the Law
DEL CASTILLO · Statistical Process Adjustment for Quality Control
DEMARIS · Regression with Social Data: Modeling Continuous and Limited Response
Variables
DEMIDENKO · Mixed Models: Theory and Applications
DENISON, HOLMES, MALLICK and SMITH · Bayesian Methods for Nonlinear
Classiﬁcation and Regression
DETTE and STUDDEN · The Theory of Canonical Moments with Applications in
Statistics, Probability and Analysis
DEY and MUKERJEE · Fractional Factorial Plans
DILLON and GOLDSTEIN · Multivariate Analysis: Methods and Applications
DODGE · Alternative Methods of Regression
* DODGE and ROMIG · Sampling Inspection Tables, Second Edition
* DOOB · Stochastic Processes
DOWDY, WEARDEN and CHILKO · Statistics for Research, Third Edition
DRAPER and SMITH · Applied Regression Analysis, Third Edition
DRYDEN and MARDIA · Statistical Shape Analysis
DUDEWICZ and MISHRA · Modern Mathematical Statistics
DUNN and CLARK · Applied Statistics: Analysis of Variance and Regression, Second
Edition
DUNN and CLARK · Basic Statistics: A Primer for the Biomedical Sciences, Third Edition
DUPUIS and ELLIS · A Weak Convergence Approach to the Theory of Large Deviations
EDLER and KITSOS (editors) · Recent Advances in Quantitative Methods in Cancer and
Human Health Risk Assessment
* ELANDT-JOHNSON and JOHNSON · Survival Models and Data Analysis
ENDERS · Applied Econometric Time Series
ETHIER and KURTZ · Markov Processes: Characterization and Convergence
EVANS, HASTINGS and PEACOCK · Statistical Distribution, Third Edition
FELLER · An Introduction to Probability Theory and Its Applications, Volume I, Third
Edition, Revised; Volume II, Second Edition
FISHER and VAN BELLE · Biostatistics: A Methodology for the Health Sciences
FITZMAURICE, LAIRD and WARE · Applied Longitudinal Analysis
* FLEISS · The Design and Analysis of Clinical Experiments
FLEISS · Statistical Methods for Rates and Proportions, Second Edition
FLEMING and HARRINGTON · Counting Processes and Survival Analysis
FULLER · Introduction to Statistical Time Series, Second Edition
FULLER · Measurement Error Models
*Now available in a lower priced paperback edition in the Wiley Classics Library.

GALLANT · Nonlinear Statistical Models.
GEISSER · Modes of Parametric Statistical Inference
GELMAN and MENG (editors) · Applied Bayesian Modeling and Casual Inference from
Incomplete-data Perspectives
GEWEKE · Contemporary Bayesian Econometrics and Statistics
GHOSH, MUKHOPADHYAY and SEN · Sequential Estimation
GIESBRECHT and GUMPERTZ · Planning, Construction and Statistical Analysis of
Comparative Experiments
GIFI · Nonlinear Multivariate Analysis
GIVENS and HOETING · Computational Statistics
GLASSERMAN and YAO · Monotone Structure in Discrete-Event Systems
GNANADESIKAN · Methods for Statistical Data Analysis of Multivariate Observations,
Second Edition
GOLDSTEIN and WOLFF · Bayes Linear Statistics, Theory & Methods
GOLDSTEIN and LEWIS · Assessment: Problems, Development and Statistical Issues
GREENWOOD and NIKULIN · A Guide to Chi-Squared Testing
GROSS, SHORTLE, THOMPSON and HARRIS · Fundamentals of Queueing Theory,
fourth Edition
GROSS, SHORTLE, THOMPSON and HARRIS · Solutions Manual to Accompany
Fundamentals of Queueing Theory, fourth Edition
* HAHN and SHAPIRO · Statistical Models in Engineering
HAHN and MEEKER · Statistical Intervals: A Guide for Practitioners
HALD · A History of Probability and Statistics and their Applications Before 1750
HALD · A History of Mathematical Statistics from 1750 to 1930
HAMPEL · Robust Statistics: The Approach Based on Inﬂuence Functions
HANNAN and DEISTLER · The Statistical Theory of Linear Systems
HARTUNG, KNAPP and SINHA · Statistical Meta-Analysis with Applications
HEIBERGER · Computation for the Analysis of Designed Experiments
HEDAYAT and SINHA · Design and Inference in Finite Population Sampling
HEDEKER and GIBBONS · Longitudinal Data Analysis
HELLER · MACSYMA for Statisticians
HERITIER, CANTONI, COPT and VICTORIA-FESER · Robust Methods in Biostatistics
HINKELMANN and KEMPTHORNE · Design and Analysis of Experiments, Volume 1:
Introduction to Experimental Design
HINKELMANN and KEMPTHORNE · Design and analysis of experiments, Volume 2:
Advanced Experimental Design
HOAGLIN, MOSTELLER and TUKEY · Exploratory Approach to Analysis of Variance
HOAGLIN, MOSTELLER and TUKEY · Exploring Data Tables, Trends and Shapes
* HOAGLIN, MOSTELLER and TUKEY · Understanding Robust and Exploratory Data
Analysis
HOCHBERG and TAMHANE · Multiple Comparison Procedures
HOCKING · Methods and Applications of Linear Models: Regression and the Analysis of
Variance, Second Edition
HOEL · Introduction to Mathematical Statistics, Fifth Edition
HOGG and KLUGMAN · Loss Distributions
HOLLANDER and WOLFE · Nonparametric Statistical Methods, Second Edition
HOSMER and LEMESHOW · Applied Logistic Regression, Second Edition
HOSMER and LEMESHOW · Applied Survival Analysis: Regression Modeling of Time to
Event Data
HUBER · Robust Statistics
HUBERTY · Applied Discriminant Analysis
HUBERY and OLEJNKI · Applied MANOVA and Discriminant Analysis, 2nd Edition
HUNT and KENNEDY · Financial Derivatives in Theory and Practice, Revised Edition
HURD and MIAMEE · Periodically Correlated Random Sequences: Spectral Theory and
Practice
HUSKOVA, BERAN and DUPAC · Collected Works of Jaroslav Hajek – with Commentary
*Now available in a lower priced paperback edition in the Wiley Classics Library.

HUZURBAZAR · Flowgraph Models for Multistate Time-to-Event Data
IMAN and CONOVER · A Modern Approach to Statistics
JACKSON · A User’s Guide to Principle Components
JOHN · Statistical Methods in Engineering and Quality Assurance
JOHNSON · Multivariate Statistical Simulation
JOHNSON and BALAKRISHNAN · Advances in the Theory and Practice of Statistics: A
Volume in Honor of Samuel Kotz
JOHNSON and BHATTACHARYYA · Statistics: Principles and Methods, Fifth Edition
JOHNSON and KOTZ · Distributions in Statistics
JOHNSON and KOTZ (editors) · Leading Personalities in Statistical Sciences: From the
Seventeenth Century to the Present
JOHNSON, KOTZ and BALAKRISHNAN · Continuous Univariate Distributions, Volume
1, Second Edition
JOHNSON, KOTZ and BALAKRISHNAN · Continuous Univariate Distributions, Volume
2, Second Edition
JOHNSON, KOTZ and BALAKRISHNAN · Discrete Multivariate Distributions
JOHNSON, KOTZ and KEMP · Univariate Discrete Distributions, Second Edition
JUDGE, GRIFFITHS, HILL, LU TKEPOHL and LEE · The Theory and Practice of
Econometrics, Second Edition
JURE ˇCKOV ´A and SEN · Robust Statistical Procedures: Asymptotics and Interrelations
JUREK and MASON · Operator-Limit Distributions in Probability Theory
KADANE · Bayesian Methods and Ethics in a Clinical Trial Design
KADANE and SCHUM · A Probabilistic Analysis of the Sacco and Vanzetti Evidence
KALBFLEISCH and PRENTICE · The Statistical Analysis of Failure Time Data, Second
Edition
KARIYA and KURATA · Generalized Least Squares
KASS and VOS · Geometrical Foundations of Asymptotic Inference
KAUFMAN and ROUSSEEUW · Finding Groups in Data: An Introduction to Cluster
Analysis
KEDEM and FOKIANOS · Regression Models for Time Series Analysis
KENDALL, BARDEN, CARNE and LE · Shape and Shape Theory
KHURI · Advanced Calculus with Applications in Statistics, Second Edition
KHURI, MATHEW and SINHA · Statistical Tests for Mixed Linear Models
* KISH · Statistical Design for Research
KLEIBER and KOTZ · Statistical Size Distributions in Economics and Actuarial Sciences
KLUGMAN, PANJER and WILLMOT · Loss Models: From Data to Decisions
KLUGMAN, PANJER and WILLMOT · Solutions Manual to Accompany Loss Models:
From Data to Decisions
KOTZ, BALAKRISHNAN and JOHNSON · Continuous Multivariate Distributions, Volume
1, Second Edition
KOTZ and JOHNSON (editors) · Encyclopedia of Statistical Sciences: Volumes 1 to 9 with
Index
KOTZ and JOHNSON (editors) · Encyclopedia of Statistical Sciences: Supplement Volume
KOTZ, READ and BANKS (editors) · Encyclopedia of Statistical Sciences: Update Volume
1
KOTZ, READ and BANKS (editors) · Encyclopedia of Statistical Sciences: Update Volume
2
KOVALENKO, KUZNETZOV and PEGG · Mathematical Theory of Reliability of
Time-Dependent Systems with Practical Applications
KOWALSI and TU · Modern Applied U-Statistics
KROONENBERG · Applied Multiway Data Analysis
KULINSKAYA, MORGENTHALER and STAUDTE · Meta Analysis: A Guide to
Calibrating and Combining Statistical Evidence
KUROWICKA and COOKE · Uncertainty Analysis with High Dimensional Dependence
Modelling
*Now available in a lower priced paperback edition in the Wiley – Interscience Paperback Series.

KVAM and VIDAKOVIC · Nonparametric Statistics with Applications to Science and
Engineering
LACHIN · Biostatistical Methods: The Assessment of Relative Risks
LAD · Operational Subjective Statistical Methods: A Mathematical, Philosophical and
Historical Introduction
LAMPERTI · Probability: A Survey of the Mathematical Theory, Second Edition
LANGE, RYAN, BILLARD, BRILLINGER, CONQUEST and GREENHOUSE · Case
Studies in Biometry
LARSON · Introduction to Probability Theory and Statistical Inference, Third Edition
LAWLESS · Statistical Models and Methods for Lifetime Data, Second Edition
LAWSON · Statistical Methods in Spatial Epidemiology, Second Edition
LE · Applied Categorical Data Analysis
LE · Applied Survival Analysis
LEE · Structural Equation Modelling: A Bayesian Approach
LEE and WANG · Statistical Methods for Survival Data Analysis, Third Edition
LEPAGE and BILLARD · Exploring the Limits of Bootstrap
LEYLAND and GOLDSTEIN (editors) · Multilevel Modelling of Health Statistics
LIAO · Statistical Group Comparison
LINDVALL · Lectures on the Coupling Method
LIN · Introductory Stochastic Analysis for Finance and Insurance
LINHART and ZUCCHINI · Model Selection
LITTLE and RUBIN · Statistical Analysis with Missing Data, Second Edition
LLOYD · The Statistical Analysis of Categorical Data
LOWEN and TEICH · Fractal-Based Point Processes
MAGNUS and NEUDECKER · Matrix Differential Calculus with Applications in Statistics
and Econometrics, Revised Edition
MALLER and ZHOU · Survival Analysis with Long Term Survivors
MALLOWS · Design, Data and Analysis by Some Friends of Cuthbert Daniel
MANN, SCHAFER and SINGPURWALLA · Methods for Statistical Analysis of Reliability
and Life Data
MANTON, WOODBURY and TOLLEY · Statistical Applications Using Fuzzy Sets
MARCHETTE · Random Graphs for Statistical Pattern Recognition
MARKOVICH · Nonparametric Analysis of Univariate Heavy-Tailed Data: Research and
practice
MARDIA and JUPP · Directional Statistics
MARKOVICH · Nonparametric Analysis of Univariate Heavy-Tailed Data: Research and
Practice
MARONNA, MARTIN and YOHAI · Robust Statistics: Theory and Methods
MASON, GUNST and HESS · Statistical Design and Analysis of Experiments with
Applications to Engineering and Science, Second Edition
MCCULLOCH and SERLE · Generalized, Linear and Mixed Models
MCFADDEN · Management of Data in Clinical Trials
MCLACHLAN · Discriminant Analysis and Statistical Pattern Recognition
MCLACHLAN, DO and AMBROISE · Analyzing Microarray Gene Expression Data
MCLACHLAN and KRISHNAN · The EM Algorithm and Extensions
MCLACHLAN and PEEL · Finite Mixture Models
MCNEIL · Epidemiological Research Methods
MEEKER and ESCOBAR · Statistical Methods for Reliability Data
MEERSCHAERT and SCHEFFLER · Limit Distributions for Sums of Independent Random
Vectors: Heavy Tails in Theory and Practice
MICKEY, DUNN and CLARK · Applied Statistics: Analysis of Variance and Regression,
Third Edition
* MILLER · Survival Analysis, Second Edition
MONTGOMERY, JENNINGS and KULAHCI · Introduction to Time Series Analysis and
Forecasting Solutions Set
*Now available in a lower priced paperback edition in the Wiley Classics Library.

MONTGOMERY, PECK and VINING · Introduction to Linear Regression Analysis, Fourth
Edition
MORGENTHALER and TUKEY · Conﬁgural Polysampling: A Route to Practical
Robustness
MUIRHEAD · Aspects of Multivariate Statistical Theory
MULLER and STEWART · Linear Model Theory: Univariate, Multivariate and Mixed
Models
MURRAY · X-STAT 2.0 Statistical Experimentation, Design Data Analysis and Nonlinear
Optimization
MURTHY, XIE and JIANG · Weibull Models
MYERS and MONTGOMERY · Response Surface Methodology: Process and Product
Optimization Using Designed Experiments, Second Edition
MYERS, MONTGOMERY and VINING · Generalized Linear Models. With Applications
in Engineering and the Sciences
† NELSON · Accelerated Testing, Statistical Models, Test Plans and Data Analysis
† NELSON · Applied Life Data Analysis
NEWMAN · Biostatistical Methods in Epidemiology
OCHI · Applied Probability and Stochastic Processes in Engineering and Physical Sciences
OKABE, BOOTS, SUGIHARA and CHIU · Spatial Tesselations: Concepts and
Applications of Voronoi Diagrams, Second Edition
OLIVER and SMITH · Inﬂuence Diagrams, Belief Nets and Decision Analysis
PALTA · Quantitative Methods in Population Health: Extentions of Ordinary Regression
PANJER · Operational Risks: Modeling Analytics
PANKRATZ · Forecasting with Dynamic Regression Models
PANKRATZ · Forecasting with Univariate Box-Jenkins Models: Concepts and Cases
PARDOUX · Markov Processes and Applications: Algorithms, Networks, Genome and
Finance
PARMIGIANI and INOUE · Decision Theory: Principles and Approaches
* PARZEN · Modern Probability Theory and Its Applications
PE ˜NA, TIAO and TSAY · A Course in Time Series Analysis
PIANTADOSI · Clinical Trials: A Methodologic Perspective
PORT · Theoretical Probability for Applications
POURAHMADI · Foundations of Time Series Analysis and Prediction Theory
POWELL · Approximate Dynamic Programming: Solving the Curses of Dimensionality
PRESS · Bayesian Statistics: Principles, Models and Applications
PRESS · Subjective and Objective Bayesian Statistics, Second Edition
PRESS and TANUR · The Subjectivity of Scientists and the Bayesian Approach
PUKELSHEIM · Optimal Experimental Design
PURI, VILAPLANA and WERTZ · New Perspectives in Theoretical and Applied Statistics
PUTERMAN · Markov Decision Processes: Discrete Stochastic Dynamic Programming
QIU · Image Processing and Jump Regression Analysis
RAO · Linear Statistical Inference and its Applications, Second Edition
RAUSAND and HØYLAND · System Reliability Theory: Models, Statistical Methods and
Applications, Second Edition
RENCHER · Linear Models in Statistics
RENCHER · Methods of Multivariate Analysis, Second Edition
RENCHER · Multivariate Statistical Inference with Applications
RIPLEY · Spatial Statistics
RIPLEY · Stochastic Simulation
ROBINSON · Practical Strategies for Experimenting
ROHATGI and SALEH · An Introduction to Probability and Statistics, Second Edition
ROLSKI, SCHMIDLI, SCHMIDT and TEUGELS · Stochastic Processes for Insurance and
Finance
ROSENBERGER and LACHIN · Randomization in Clinical Trials: Theory and Practice
†Now available in a lower priced paperback edition in the Wiley – Interscience Paperback Series.
*Now available in a lower priced paperback edition in the Wiley Classics Library.

ROSS · Introduction to Probability and Statistics for Engineers and Scientists
ROSSI, ALLENBY and MCCULLOCH · Bayesian Statistics and Marketing
ROUSSEEUW and LEROY · Robust Regression and Outline Detection
ROYSTON and SAUERBREI · Multivariable Model - Building: A Pragmatic Approach to
Regression Anaylsis based on Fractional Polynomials for Modelling Continuous Variables
RUBIN · Multiple Imputation for Nonresponse in Surveys
RUBINSTEIN · Simulation and the Monte Carlo Method, Second Edition
RUBINSTEIN and MELAMED · Modern Simulation and Modeling
RYAN · Modern Engineering Statistics
RYAN · Modern Experimental Design
RYAN · Modern Regression Methods
RYAN · Statistical Methods for Quality Improvement, Second Edition
SALEH · Theory of Preliminary Test and Stein-Type Estimation with Applications
SALTELLI, CHAN and SCOTT (editors) · Sensitivity Analysis
* SCHEFFE · The Analysis of Variance
SCHIMEK · Smoothing and Regression: Approaches, Computation and Application
SCHOTT · Matrix Analysis for Statistics
SCHOUTENS · Levy Processes in Finance: Pricing Financial Derivatives
SCHUSS · Theory and Applications of Stochastic Differential Equations
SCOTT · Multivariate Density Estimation: Theory, Practice and Visualization
* SEARLE · Linear Models
SEARLE · Linear Models for Unbalanced Data
SEARLE · Matrix Algebra Useful for Statistics
SEARLE and WILLETT · Matrix Algebra for Applied Economics
SEBER · Multivariate Observations
SEBER and LEE · Linear Regression Analysis, Second Edition
SEBER and WILD · Nonlinear Regression
SENNOTT · Stochastic Dynamic Programming and the Control of Queueing Systems
* SERFLING · Approximation Theorems of Mathematical Statistics
SHAFER and VOVK · Probability and Finance: Its Only a Game!
SILVAPULLE and SEN · Constrained Statistical Inference: Inequality, Order and Shape
Restrictions
SINGPURWALLA · Reliability and Risk: A Bayesian Perspective
SMALL and MCLEISH · Hilbert Space Methods in Probability and Statistical Inference
SRIVASTAVA · Methods of Multivariate Statistics
STAPLETON · Linear Statistical Models
STAUDTE and SHEATHER · Robust Estimation and Testing
STOYAN, KENDALL and MECKE · Stochastic Geometry and Its Applications, Second
Edition
STOYAN and STOYAN · Fractals, Random and Point Fields: Methods of Geometrical
Statistics
STREET and BURGESS · The Construction of Optimal Stated Choice Experiments: Theory
and Methods
STYAN · The Collected Papers of T. W. Anderson: 1943–1985
SUTTON, ABRAMS, JONES, SHELDON and SONG · Methods for Meta-Analysis in
Medical Research
TAKEZAWA · Introduction to Nonparametric Regression
TAMHANE · Statistical Analysis of Designed Experiments: Theory and Applications
TANAKA · Time Series Analysis: Nonstationary and Noninvertible Distribution Theory
THOMPSON · Empirical Model Building
THOMPSON · Sampling, Second Edition
THOMPSON · Simulation: A Modeler’s Approach
THOMPSON and SEBER · Adaptive Sampling
THOMPSON, WILLIAMS and FINDLAY · Models for Investors in Real World Markets
TIAO, BISGAARD, HILL, PE ˜NA and STIGLER (editors) · Box on Quality and Discovery:
with Design, Control and Robustness
*Now available in a lower priced paperback edition in the Wiley Classics Library.

TIERNEY · LISP-STAT: An Object-Oriented Environment for Statistical Computing and
Dynamic Graphics
TSAY · Analysis of Financial Time Series
UPTON and FINGLETON · Spatial Data Analysis by Example, Volume II: Categorical and
Directional Data
VAN BELLE · Statistical Rules of Thumb
VAN BELLE, FISHER, HEAGERTY and LUMLEY · Biostatistics: A Methodology for the
Health Sciences, Second Edition
VESTRUP · The Theory of Measures and Integration
VIDAKOVIC · Statistical Modeling by Wavelets
VINOD and REAGLE · Preparing for the Worst: Incorporating Downside Risk in Stock
Market Investments
WALLER and GOTWAY · Applied Spatial Statistics for Public Health Data
WEERAHANDI · Generalized Inference in Repeated Measures: Exact Methods in
MANOVA and Mixed Models
WEISBERG · Applied Linear Regression, Second Edition
WELSH · Aspects of Statistical Inference
WESTFALL and YOUNG · Resampling-Based Multiple Testing: Examples and Methods
for p-Value Adjustment
WHITTAKER · Graphical Models in Applied Multivariate Statistics
WINKER · Optimization Heuristics in Economics: Applications of Threshold Accepting
WONNACOTT and WONNACOTT · Econometrics, Second Edition
WOODING · Planning Pharmaceutical Clinical Trials: Basic Statistical Principles
WOODWORTH · Biostatistics: A Bayesian Introduction
WOOLSON and CLARKE · Statistical Methods for the Analysis of Biomedical Data,
Second Edition
WU and HAMADA · Experiments: Planning, Analysis and Parameter Design Optimization
WU and ZHANG · Nonparametric Regression Methods for Longitudinal Data Analysis:
Mixed-Effects Modeling Approaches
YANG · The Construction Theory of Denumerable Markov Processes
YOUNG, VALERO-MORA and FRIENDLY · Visual Statistics: Seeing Data with Dynamic
Interactive Graphics
ZACKS · Stage-Wise Adaptive Designs
* ZELLNER · An Introduction to Bayesian Inference in Econometrics
ZELTERMAN · Discrete Distributions: Applications in the Health Sciences
ZHOU, OBUCHOWSKI and McCLISH · Statistical Methods in Diagnostic Medicine
*Now available in a lower priced paperback edition in the Wiley Classics Library.

