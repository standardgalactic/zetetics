Entailment as Robust Self-Learner
Jiaxin Ge1∗and Hongyin Luo2∗and Yoon Kim2 and James Glass2
1 Peking University, Beijing, China
2 MIT Computer Science and Artificial Intelligence Lab, Cambridge MA, US
aomaru@stu.pku.edu.cn, {hyluo, yoonkim, glass}@mit.edu
Abstract
Entailment has been recognized as an impor-
tant metric for evaluating natural language un-
derstanding (NLU) models, and recent studies
have found that entailment pretraining benefits
weakly supervised fine-tuning. In this work,
we design a prompting strategy that formulates
a number of different NLU tasks as contex-
tual entailment. This approach improves the
zero-shot adaptation of pretrained entailment
models. Secondly, we notice that self-training
entailment-based models with unlabeled data
can significantly improve the adaptation perfor-
mance on downstream tasks. To achieve more
stable improvement, we propose the Simple
Pseudo-Label Editing (SimPLE) algorithm for
better pseudo-labeling quality in self-training.
We also found that both pretrained entailment-
based models and the self-trained models are
robust against adversarial evaluation data. Ex-
periments on binary and multi-class classifi-
cation tasks show that SimPLE leads to more
robust self-training results, indicating that the
self-trained entailment models are more effi-
cient and trustworthy than large language mod-
els on language understanding tasks.
1
Introduction
Although achieving state-of-the-art performance
in different natural language understanding (NLU)
tasks (Devlin et al., 2018; Liu et al., 2019; Yang
et al., 2019; Clark et al., 2020; He et al., 2020; Joshi
et al., 2020), large-scale pretrained language mod-
els still highly depend on human-annotated, task-
specific training corpora for fine-tuning because
the self-supervised pretraining objective does not
incorporate explicit task-related knowledge. As
a result, state-of-the-art language models are still
challenged by the lack of adequate fine-tuning data
and difficult evaluation examples crafted by adver-
∗Equal contribution. Correspondence to Hongyin Luo
at hyluo@mit.edu. Code and processed data are available at
https://github.com/luohongyin/EntST.
sarial attacks or model-in-loop adversarial data an-
notations (Wang et al., 2021a; Bartolo et al., 2020;
Zang et al., 2019; Garg and Ramakrishnan, 2020;
Li et al., 2020).
On the other hand, entailment is recognized
as a minimal requirement for NLU (Condoravdi
et al., 2003). Recent studies have found that en-
tailment learning improves sentence representation
(Reimers and Gurevych, 2019a; Gao et al., 2021).
However, these models still need fine-tuning with
human-annotated training data to handle down-
stream NLU tasks. The authors of Wang et al.
(2021b) found that entailment-based models are
also few-shot learners that outperform recent ef-
forts on few-shot NLU. For example, LM-BFF
(Gao et al., 2020) proves that entailment learning
can significantly improve the data efficiency and
adaptation ability of language models.
In this work, we further explore the zero-shot
and unsupervised adaptation abilities of entailment-
based models without any human-labeled training
corpora on downstream tasks. We first study the
zero-shot and unsupervised adaptation abilities of
the entailment-based language models. Inspired
by recent progress on prompt tuning, we formulate
different NLU tasks as contextual entailment (Rout-
ley and Meyer, 1973) by constructing task-specific
suppositions. The language models are trained to
predict the truth value of the constructed suppo-
sitions. In zero-shot adaptation experiments, we
find this approach significantly outperforms naively
concatenating different inputs and labels, proving
that the supposition construction method mitigates
the distribution gap among different NLU tasks.
We further explore the potential of the unsuper-
vised adaptation ability of entailment-based models.
We use the pretrained entailment models to predict
the pseudo-labels of unlabeled, task-specific lan-
guage data. We find that the entailment-based mod-
els can be improved with self-training (Blum and
Mitchell, 1998) with the automatically annotated
arXiv:2305.17197v1  [cs.CL]  26 May 2023

pseudo-labels (He et al., 2019). While the self-
training strategy has been proven effective on dif-
ferent tasks and modalities (Zou et al., 2019; Zoph
et al., 2020; Meng et al., 2020; Xie et al., 2020b),
a major challenge for self-training is the unstable
performance caused by the noisy pseudo-labels. A
number of solutions have been proposed to mit-
igate this issue. The most popular methods are
training data selection (Li and Zhou, 2005; Lang
et al., 2022) and pseudo-label editing (Shin et al.,
2020; Mandal et al., 2020). Recent work also found
that simple Dropout (Srivastava et al., 2014) ap-
proaches improve contrastive learning (Gao et al.,
2021) and speech recognition (Khurana et al., 2021;
Dawalatabad et al., 2022).
To combine the benefits of data selection and la-
bel editing methods, we propose SimPLE, a simple
pseudo-label editting algorithm with simple text
augmentation, uncertainty-based data filtering, and
majority-based pseudo-labeling. Experiments with
different backbone models on binary, multi-class,
regular, and adversarial NLU tasks show that our
approach makes the following contributions,
• Supposition-based task formulation improves
the zero-shot adaptation and robustness
against adversarial evaluation data of entail-
ment models across different NLU tasks.
• SimPLE improves the pseudo-labeling accu-
racy on confident and uncertain training sam-
ples, leading to significant improvement over
all self-training and pretrained baselines.
• Self-trained,
350M-parameter
entailment
models without human-generated labels out-
perform supervised language models with
137B parameters, proving the data and com-
putation efficiency of entailment self-training.
2
Related Work
Language modeling. Task-agnostic, large-scale
language models can solve a number of natural
language understanding (NLU) tasks (Brown et al.,
2020; Raffel et al., 2020; Lewis et al., 2019; Wei
et al., 2022a,b). On the other hand, pretraining
with annotated training corpora of different natural
language tasks also benefits the generalize ability
and zero-shot adaptation performance (Sanh et al.,
2021). Recent studies have found that textual entail-
ment (Bowman et al., 2015; Williams et al., 2018)
is a powerful pretraining task. Entailment mod-
els are applied for sentence representation learning
(Reimers and Gurevych, 2019b; Gao et al., 2021),
relation extraction (Obamuyide and Vlachos, 2018;
Yin et al., 2019), and fact-checking (Thorne and
Vlachos, 2018). The authors of Wang et al. (2021b)
showed that entailment models can benefit the few-
shot learning performance of pretrained language
models on NLU tasks.
Robustness in Self-training. While most self-
training studies are under the computer vision con-
text (Zoph et al., 2020; Zou et al., 2019), efforts
also exist for self-training the latest neural language
models, including back translation (He et al., 2019),
text augmentation (Xie et al., 2020a; Chen et al.,
2020), question-answer synthesis (Bartolo et al.,
2021; Luo et al., 2022), and co-training (Lang
et al., 2022). However, self-training methods suf-
fer from noisy pseudo-labels. In computer vision,
a straightforward solution is obtaining confident
pseudo-labels by augmenting input images (Shin
et al., 2020; Mandal et al., 2020; Sohn et al., 2020),
including shifting, rotating, or adding noise to pix-
els. However, data augmentation is not as straight-
forward for natural language if no additional model
is used. Instead, some model-level methods can be
applied. Zou et al. (2019) proposed regularizing
over pseudo-label confidence to avoid overfitting
to simple cases, Gao et al. (2021); Khurana et al.
(2021) applied dropout to improve the quality of
training corpora. Li and Zhou (2005); Lang et al.
(2022) applied a graph-based confidence estima-
tion method for removing training samples with
uncertain pseudo labels.
Difference with previous work. Without any ad-
ditional language model for text augmentation, we
propose a model-level, augmented pseudo-labeling
method that improves self-training performance
for entailment models. Our method avoids drop-
ping training data and performs more stably than
dropout-based methods. Different from previous
work on weakly-supervised language understand-
ing with entailment models (Wang et al., 2021b),
we do not use any human-generated labels. Our
models contain 1/500 trainable parameters com-
pared to the models used in Lang et al. (2022);
Sanh et al. (2021).
3
Entailment Self-training
Pretraining.
Recent studies have found that
entailment-based language models can efficiently
adapt to different natural language understanding
(NLU) tasks with a limited number of human-

labeled training samples (Wang et al., 2021b; Luo
and Glass, 2023). In this work, we find that en-
tailment models can be self-improved without any
human-generated labels by constructing supposi-
tions (prompts) that describe the given tasks. Most
NLU tasks can be formulated as predicting the truth
value of the constructed suppositions that wrap in-
puts and label descriptions, as shown in Table 1.
Task
Inputs
Supposition
MNLI
{p, h}
h is entailed by p.
RTE
{p, h}
h is entailed by p.
QNLI
{t, q}
The answer to q is entailed by t.
QQP
{q1, q2}
q1’s answer is entailed by q2’s answer.
SST2
{x}
The movie is good is entailed by x.
Table 1: The suppositions constructed based on the
definitions of different GLUE tasks (Wang et al., 2018).
By training the entailment model using the
MNLI corpus given with the constructed suppo-
sitions, the model can be directly adapted to other
tasks with relatively high accuracy. We will show
that without entailment pretraining, similar perfor-
mance can only be achieved by 400 times bigger
language models. The entailment-based models
can be further fine-tuned on unlabeled texts via
self-training. We apply different adaptation strate-
gies for binary and multi-class classification tasks.
Binary classification. Supposition-based entail-
ment models predict True, Neutral, and False
scores for each supposition, corresponding to entail,
neutral, and contradictory labels of the MNLI cor-
pus. For binary classification, we ignore the neutral
score and calculate only True and False probabil-
ities, and the True/False predicted can be linked
to corresponding labels according to the supposi-
tion. For example, the SST2 supposition in Table
1 being true means that {x} is a positive movie
review. The predicted True/False values are used
as pseudo-labels for self-training,
Multi-class classification. In binary classification,
the model is presented with a single supposition
and asked to decide whether it’s true or not. In
multi-class classification, the model is presented
with a context sentence and multiple labels and is
asked to choose the correct label.
To predict the correct answer from multiple op-
tions, we propose an entailment score ranking
method. First, for each sentence to be classified, we
construct a supposition for each label. For example,
in an emotion classification task, given the sentence
S, we construct the following suppositions: "I am
happy is entailed by S", "I am sad is entailed by
S", and "I am shocked is entailed by S". We calcu-
late the entailment probability of each supposition
with the entailment model and predict the label
associated with the most entailed supposition.
We propose a max-confidence tuning method
for self-training. We select the class with the high-
est entailment score and then record its predicted
pseudo-label for further self-training, and ignore
other classes. The model does not need to classify
each class correctly but merely learns to predict the
truth value of its most confident supposition.
4
Simple Pseudo-label Editing
We propose the simple pseudo-label editing (Sim-
PLE) method, a three-step pipeline for generating
robust pseudo labels, including augmented pseudo-
labeling with dropout, uncertain data filtering, and
majority-based relabeling. We introduce the details
of each step in this section.
4.1
Simple Augmentation for Pseudo-labeling
Because of languages’ discrete and sequential na-
ture, changing a token in a sentence might com-
pletely invert its meaning.
As a result, unlike
straightforward and effective image augmentation
processes like FixMatch (Sohn et al., 2020), addi-
tional augmentation models are usually needed for
text augmentation. Recent studies have found that
instead of data-level augmentation, the Dropout
mechanism leads to decent embedding-level aug-
mentation. Gao et al. (2021) applied dropout for
contrastive sentence representation learning, and
Khurana et al. (2021) selected confident pseudo-
labels by measuring the consistency of a model
with the same input data and random dropouts.
As the first step of generating augmented pseudo
labels, we run N independent evaluations with ran-
dom dropout (dropout rate = 0.1) for each input
training sample xi and obtain a set of N noisy
pseudo-labels.
Yi = {yj = M∗
j (xi) | j ∈[0, N)}
(1)
where j stands for the j-th independent evalu-
ation with a dropout model M∗.
Meanwhile,
we store a set of sequence representations Ei =
{e0, e1, . . . , eN−1} of xi collected in each feed-
forward process. After finishing this step, we col-
lect a set of data, pseudo-label, and embeddings.
C = {(xi, yj
i , ej
i) | i ∈[0, M), j ∈[0, N)}
(2)

A
C
B
Uncertain region
Samples with Label 1
Samples with Label 2
A * N Dropouts
C * N Dropouts
B * N Dropouts
Figure 1: Visualization of the SimPLE method. The figure shows the embedding space of natural sentences, and
different colors represent different predicted labels. Each data sample is labeled with multiple random dropouts, and
we use the SETRED algorithm to detect the uncertain pseudo-labels. The final label is voted by confident inferences.
where M stands for the number of unlabeled train-
ing samples, each associated with N pseudo-labels
and corresponding hidden states. In total, the aug-
mented method outputs M ∗N label-embedding
pairs for further processing.
4.2
Uncertainty Estimation
Following Li and Zhou (2005) and Lang et al.
(2022), we estimate the confidence of all pseudo-
labels using the SETRED algorithm. The motiva-
tion of this algorithm is that training samples with
similar embeddings are likely to have the same
pseudo-labels. On the other hand, if a training sam-
ple is located near samples with different pseudo-
labels in the embedding space, its own pseudo-
label is likely to be uncertain. Using the output
data-embedding-label set shown in Equation 2, we
can calculate the nearest neighbors of each training
sample and estimate the labeling consistency.
To estimate the uncertainty of yu, the pseudo-
label of training sample xu, we calculate the Eu-
clidean distances between xu and all other M ∗N −
1 samples using the calculated text embeddings.
We construct a set of the top k nearest neighbors of
xu, namely N(u). With the nearest neighbor set,
an uncertain score of (xu, yu) can be calculated as
follows,
Ju =
X
v∈N(u)
I(yu ̸= yj) / (1 + ∥eu −ev∥2) (3)
where I is a binary indicator function, whose value
is 1 when yu ̸= yv and 0 otherwise. ∥eu −ev∥2
stands for the Euclidean distance between the em-
beddings of xu and xv. As a result, Ju would have
a higher value when more near neighbors are asso-
ciated with different pseudo-labels.
To estimate the uncertainty of yu, we compare
Ju with a null hypothesis where all pseudo-labels in
C except yu are randomly shuffled. After the shuf-
fling, the entire data-label mapping set becomes
uncertain. The expectation and variance of Ju after
shuffling is
Ev[Ju] = (1 −ˆPyu)
X
v∈N(u)
1/(1 + ∥eu −ev∥2)
σ(Ju)2 = ˆPyu(1−ˆPyu)
X
v∈N(u)
1/(1+∥eu−ev∥2)2
The uncertainty can be estimated by verifying the
significance of the difference between Ju and the
null hypothesis. An uncertainty score can be calcu-
lated as
s(u) = Ju −Ev[Ju]
σ(Ju)
(4)
With this method, we calculate uncertainty
scores for all M ∗N training samples in C for
further processing.
4.3
Filtering and Relabeling
After finishing estimating the uncertainty of each
training sample, we sort all training samples in C
by their uncertainty scores and remove the 20%
most uncertain training samples. The remaining
samples are used for relabeling based on major-
ity voting. For example, a training sample xi has

N pseudo-labels [yi
0, yi
1, . . . , yi
N−1] after the aug-
mented labeling step, and n labels are removed
based on the uncertainty scores.
The final pseudo-label of xi is decided by the
voting result of the N −n remaining labels. If all
generated pseudo-labels of a training sample are
removed or there is a tie in the voting, we re-run
the labeling process without dropout to get the final
pseudo-label. Following this approach, we keep
all training samples and, meanwhile, obtain a more
robust pseudo-label set.
5
Experiments
Benchmarks. We conduct experiments on pop-
ular natural language understanding tasks in the
GLUE (Wang et al., 2018) benchmark, including
RTE (Dagan et al., 2005), QNLI (Rajpurkar et al.,
2016), QQP, SST-2 (Socher et al., 2013), and CoLA
(Warstadt et al., 2019). We also assess the robust-
ness of the proposed method against adversarial
evaluation sets in the AdvGLUE corpus (Wang
et al., 2021a), including Adv-QNLI, Adv-QQP,
Adv-RTE, and Adv-SST2. The data in AdvGLUE
is created by adding word-level and sentence-level
perturbations to the GLUE data, as well as human-
crafted examples. For Multi-Classification, we
use Copa (Wang et al., 2019) (which consists of
questions paired with two answer choices), Emo-
tion Classification (Saravia et al., 2018), Amazon
Review (Keung et al., 2020) and Ag-News (Xi-
ang Zhang, 2015). More details are shown in Ap-
pendix A.
Hyper-parameters. We train 350M RoBERTa
(Devlin et al., 2018) and DeBERTa (He et al.,
2020) models for the language understanding
tasks, without using larger language models like
GPT-3 (Brown et al., 2020) or T0 (Sanh et al.,
2021) that are used for generating pseudo-labels in
(Lang et al., 2022). We also use the same hyper-
parameters across all tasks, attempting to avoid
the problems mentioned in Perez et al. (2021). In
the entailment pretraining on the MNLI dataset
(Williams et al., 2018), we optimize both RoBERTa
and DeBERTa models with the AdamW optimizer
(Loshchilov and Hutter, 2018). For all tasks and
both models, we set ε = 10−6. In the entailment
pretraining, we set the weight decay weight to
10−5, and the learning rate for both models is 3e-
6. During the self-training step, the learning rate
of both models on all binary classification tasks is
4e-6 and is 1e-6 on multi-classification tasks, and
the weight decay is constantly 10−2. We run the
entailment pretraining for 2 epochs and the self-
training for 6 epochs. In confidence-based labeling,
we drop 1/8 data with the lowest confidence.
Self-training details. For each binary classifica-
tion task, we randomly select N = 2000 unlabeled
data examples. For each multi-classification task,
we randomly select N = 50 unlabeled data exam-
ples. To estimate the uncertainty of the pseudo-
labels in SETRED and SimPLE algorithms, we use
the hidden states of the 4th layer from the top of
both RoBERTa and DeBERTa language models as
the supposition embeddings and measure the un-
certainty with 9 neighbors. In SimPLE, we run 7
inferences for each training sample with different
dropouts. We train and evaluate the models for
each task with 10 independent runs on 2 V100 32G
GPUs. Each experiment takes less than an hour.
Assessment. We evaluate the performance of our
algorithm by comparing the average classification
accuracy against baseline methods and the robust-
ness.
We describe the term Robustness as fol-
lows: in multiple independent experiments, a ro-
bust method should achieve high maximum, mini-
mum, and average accuracy against with different
backbone model and training data, on different nat-
ural language understanding tasks.
5.1
GLUE and AdvGLUE Tasks
The experiment results are shown in Table 2. We
compare the adaptation performance of entailment-
based language models and the improvement of
different self-training approaches.
Compare with supervised baselines. We compare
our entailment self-training methods with few-shot
fine-tuning baselines. The few-shot baselines, in-
cluding PET (Schick and Schütze, 2021), LM-BFF
(Gao et al., 2020), P-tuning (Liu et al., 2021), PPT
(Gu et al., 2021), and UPT (Wang et al., 2022),
are based on 350M BERT or RoBERTa backbones.
Our pretrained DeBERTa entailment model out-
performs the best few-shot baseline (LM-BFF) by
4.5%, and the RoBERTa entailment model outper-
forms LM-BFF by 1.5%. With self-training, our
SimPLE method further improves the model’s per-
formance by a large margin. The RoBERTa per-
formance is boosted by nearly 5% and the average
performance of DeBERTa is over 86%, outperform-
ing the best few-shot supervised baselines by 6.9%.
On the other hand, we compare our model with
fully supervised RoBERTa/DeBERTa models and

Method
GLUE
Method
AdvGLUE
QNLI
QQP
RTE
SST2
Avg.
QNLI
QQP
RTE
SST2
Avg.
Few-shot (left) and fully-supervised (right) medium LMs (350M) with human-generated labels
PET
61.3
67.6
65.7
91.8
71.6
R3F
47.5
40.6
50.1
38.5
44.2
LM-BFF
69.2
69.8
83.9
90.3
78.3
CTT
49.6
40.7
46.2
39.2
43.9
P-tuning
58.8
67.6
70.8
92.6
72.5
MT
47.5
41.5
52.5
51.3
48.2
PPT
68.8
67.2
67.9
92.3
74.1
BERT
39.8
37.9
40.5
33.0
37.8
UPT
70.1
72.1
68.9
92.9
76.0
RoBERTa
52.5
45.4
62.8
58.5
54.8
EFL
68.0
67.3
85.8
90.8
78.0
DeBERTa
57.9
60.4
79.0
57.8
63.8
Few-shot large LMs (137B) with human-generated labels
LaMDA
55.7
58.9
70.8
92.3
69.4
\
-
-
-
-
-
FLAN
63.3
75.9
84.5
94.6
79.6
\
-
-
-
-
-
Zero-shot adaptation of entailment classifiers based on medium LMs (350M)
DeBERTa-Cat
71.6
70.5
74.0
84.6
75.2
\
60.8
47.4
50.6
56.1
53.7
RoBERTa-Sup
71.5
78.6
81.2
87.7
79.8
\
62.1
52.6
61.7
59.9
59.1
DeBERTa-Sup
77.3
79.9
84.5
90.1
82.9
\
61.5
64.1
66.7
42.6
58.7
Self-trained RoBERTa-large (350M) without human-generated labels
Baseline-ST
74.1
80.1
81.5
88.3
81.0
Baseline-ST
64.9
60.6
60.9
56.6
60.8
Dropout
78.5
80.5
80.9
88.8
82.2
Dropout
69.2
57.8
61.9
57.3
61.6
SETRED
80.5
80.5
80.8
88.3
82.5
SETRED
68.0
56.5
62.6
58.9
61.5
SimPLE (ours)
83.1
80.7
83.1
91.6
84.6
SimPLE (ours)
69.6
54.4
62.3
58.8
61.3
Self-trained DEBERTa-large (350M) without human-generated labels
Baseline-ST
79.0
80.2
83.4
92.1
83.7
Baseline-ST
65.8
70.4
68.4
50.9
63.9
Dropout
81.1
80.5
84.1
91.8
84.4
Dropout
70.1
63.3
70.9
49.9
63.6
SETRED
83.4
80.5
83.9
92.0
84.9
SETRED
69.8
69.5
69.9
50.9
65.0
SimPLE (ours)
85.2
81.0
85.5
92.8
86.1
SimPLE (ours)
70.1
68.1
73.8
51.6
65.9
Table 2: Experimental results on binary classification tasks with 10 independent experiments. Cat stands for
concatenation-based pretraining and Sup stands for supposition classification.
robust training methods, including R3F (Agha-
janyan et al., 2020), child tuning (CT) (Xu et al.,
2021), and match tuning (MT) (Tong et al., 2022)
models, on the AdvGLUE benchmark. We found
that the fully-supervised DeBERTa model is the
best baseline on the AdvGLUE benchmark. How-
ever, our RoBERTa entailment model outperforms
all robust training baselines with the same pre-
trained backbone by over 10%. With SimPLE self-
training, the DeBERTa entailment model achieves
the best performance on AdvGLUE, outperforming
the fully-supervised DeBERTa model by 2.1% as
well as all other baselines.
We found that our pretrained entailment models
outperform EFL, the few-shot fine-tuned entail-
ment model based on RoBERTa-large proposed
by Wang et al. (2021b). The self-trained models
further outperform EFL with larger margins. This
indicates the strong adaptation ability introduced
by the supposition-based NLU strategy.
Compare with large language models. We found
that both zero-shot pretrained and semi-supervised
self-trained entailment models outperform the few-
shot large language models on QNLI, QQP, and
RTE tasks, and achieve significantly higher average
accuracy on GLUE. This suggests that our method
is computation-efficient - the models use 1/400 pa-
rameters, without human-generated task-specific
labels, but achieve better performance than expen-
sive large-scale language models on NLU tasks.
Compare with self-training baselines. By averag-
ing 10 independent evaluations across GLUE and
AdvGLUE benchmarks and backbone models, we
found that Dropout and SETRED improve baseline
self-training performance on a similar level. On
average, SETRED outperforms Dropout by 0.5%
on 4 experiment settings. On the GLUE bench-
mark, the SimPLE method improves the model’s
performance by 1.5 to 2% on average. The highest
improvement boost is on the QNLI tasks, where
the SimPLE self-training method outperforms the
baseline self-training by 9% and 6% on RoBERTa
and DeBERTa respectively. Although the average
improvement is not very high, we will show that
SimPLE is significantly more robust. The results
show that augmenting the pseudo-labels without
removing uncertain training samples benefits self-
training, which aligns with our hypothesis.
In general, the experiments on binary classifica-
tion NLU tasks proved the data and computation
efficiency of entailment self-training over different
strong baseline models. Furthermore, the SimPLE

1
2
3
4
0.75
0.80
0.85
QNLI
RoBERTa Reg.
1
2
3
4
0.60
0.65
0.70
0.75
RoBERTa Adv.
1
2
3
4
0.75
0.80
0.85
DeBERTa Reg.
1
2
3
4
0.60
0.65
0.70
0.75
0.80
DeBERTa Adv.
1
2
3
4
0.800
0.805
0.810
0.815
QQP
1
2
3
4
0.50
0.55
0.60
0.65
1
2
3
4
0.795
0.800
0.805
0.810
0.815
1
2
3
4
0.60
0.65
0.70
0.75
0.80
1
2
3
4
0.78
0.80
0.82
0.84
RTE
1
2
3
4
0.55
0.60
0.65
0.70
1
2
3
4
0.82
0.84
0.86
1
2
3
4
0.65
0.70
0.75
0.80
BaseST
Dropout
SETRED
SimPLE
0.86
0.88
0.90
0.92
SST2
BaseST
Dropout
SETRED
SimPLE
0.56
0.58
0.60
0.62
BaseST
Dropout
SETRED
SimPLE
0.90
0.92
0.94
BaseST
Dropout
SETRED
SimPLE
0.45
0.50
0.55
Figure 2: The results of 10 independent experiments with self-trained RoBERTa and DeBERTa models on GLUE
(*BERTa Reg.) and AdvGLUE (*BERTa Adv.) benchmarks.
algorithm we propose in this work achieves the best
average performance, significantly outperforms all
baselines on some of the tasks, and meanwhile pre-
serves the robustness of entailment models against
adversarial benchmarks.
5.2
Multi-class NLU Tasks
The experiment results on Copa, Emotion, Ama-
zon Review, and Ag News are shown in Table
3. In multi-classification tasks, we present the
comparative results of the pretrained entailment-
based language models and the 4 self-training ap-
proaches compared in the previous section with
binary NLU tasks, including standard self-training,
dropout-based re-labeling, SETRED, and SimPLE.
The effect of dropout-based augmentation. By
merely using dropout, the augmented self-training
outperforms the standard normal self-training base-
line which keeps all the pseudo-labels in general.
This further validates the previous finding that by
adding dropout, the models adopt noises that bene-
fit the inference, generate augmented pseudo-labels
and mitigate the overfitting problem.
The effect of SETRED. By merely using SETRED,
the self-training does not see a consistent improve-
ment in performance and even falls behind the pre-
trained and standard self-trained models that pre-
serve all pseudo labels in some tasks (like Amazon-
Review). This fact suggests that removing uncer-
tain pseudo-labels can lead the model to overfit
confident training samples, thus hurting the self-
fine-tuning performance.
The effect of SimPLE. Table 3 shows that the
SimpLE algorithm constantly outperforms all pre-
trained and self-trained baselines on both backbone
models across all multi-class benchmarks, which
aligns with the result on the binary NLU tasks. This
fact further validates our hypothesis that augment-
ing the pseudo-labels of uncertain training samples
can improve the performance of self-training.
Compare with Large Language Models. We no-
tice that our self-trained methods can outperform
several large language models. On Emotion and
AG News tasks, the pretrained entailment model
without self-training can achieve a significant im-
provement over the GPT-3-175b model, which is
500 times large than the entailment model. This
indicates that the entailment-based model is a more
efficient and trustworthy option for many natural
language understanding tasks.
6
Analysis
Robustness. Besides the mean accuracy of all
experiments, we also visualize the results of all
independent evaluations of different self-training
strategies in Figure 2. We found that SimPLE con-
stantly outperforms other self-training baselines
on the regular GLUE benchmark by comparing

Method
Multi-Classification
Copa
EM
AR
News
Avg
DEBERTa-large (350M)
Pretrain
77.0
51.93
37.01
73.40
59.84
BaseST
78.75
51.24
38.80
73.10
60.47
Dropout
78.25
53.69
38.19
73.16
60.82
SETRED
78.0
52.42
37.61
73.33
60.34
SimPLE
79.75
54.58
39.05
73.57
61.74
RoBERTa-large (350M)
Pretrain
76.0
49.21
33.31
63.18
55.43
BaseST
76.67
50.94
37.38
64.64
57.41
Dropout
78.67
50.99
42.87
61.05
58.40
SETRED
78.0
50.53
27.16
63.24
54.73
SimPLE
79.0
51.79
44.06
65.60
60.11
Large Language Models
Zero-shot
70.0♢
42.7‡
-
43.9‡
-
Few-shot
77.0†
-
-
61.0‡
-
Class Num
2
6
5
4
-
Table 3: Multi-class NLU results with 3 independent
runs. The Copa model selects from two candidate sen-
tences, which is different from the previous binary NLU
tasks. ♢: T5-11b, †: GPT-Neo-6b, ‡: GPT-3-175b. The
performance of large language models are cited from
Zhao et al. (2021) and Wang et al. (2023).
mean, maximum, and minimum accuracy.
Al-
though DeBERTa performs similarly under differ-
ent self-training strategies on QQP in terms of aver-
age accuracy, there exists a significant gap between
the minimal performance of baseline and SimPLE.
This indicates that SimPLE is more robust and safer
compared with the regular self-training algorithm.
The only exception is the DeBERTa model on SST2
- the mean performance of SimPLE is better, but it
has a lower minimal performance than the baseline
self-training method.
Most models overfit to the training corpora and
achieve high accuracy on regular evaluation sets,
but perform poorly on adversarial benchmarks
(Wang et al., 2021a). As a result, fully supervised
models achieve less than 60% accuracy on Ad-
vGLUE. We also investigate if SimPLE hurts the
model’s robustness against adversarial evaluation
data. We found that, except RoBERTa on AdvQQP,
other settings show that the entailment-based mod-
els are still robust after SimPLE self-training. As
we compared in Table 2, all these results signifi-
cantly outperform fully-supervised baselines.
Pseudo-labeling Accuracy. We show the pseudo-
labeling accuracy of RoBERTa and DeBERTa-
based entailment models with different strategies
in Figure 3 with 10 independent experiments. The
results indicate that the DeBERTa models predict
more accurate pseudo-labels in general. On the
BaseST
Dropout
SETRED
SimPLE
0.68
0.70
0.72
0.74
0.76
0.78
0.80
RoBERTa
DeBERTa
Figure 3: Pseudo-labeling accuracy of entailment mod-
els with standard (ST), dropout, SETRED, and SimPLE
strategies. SETRED achieves higher accuracy because
uncertain data samples are dropped.
Uncertain region
+ : True
- : False
Num sample: 1762
Num True : 562
Num False :  1182
Figure 4: Visualization of the embeddings, pseudo-
labels, and uncertainty of QNLI suppositions calculated
by the pretrained DeBERTa entailment model. Each
data sample has 7 embeddings calculated with different
dropouts. Black stands for uncertain points and other
colors stand for different training examples.
other hand, the pseudo-label sets produced by Sim-
PLE with both models are significantly less noisy
than the standard and dropout-based labeling meth-
ods without removing any uncertain data samples.
SETRED achieves the highest labeling accuracy be-
cause it drops uncertain samples. The comparison
suggests that SimPLE achieves the highest perfor-
mance because it achieves high pseudo-labeling
accuracy on uncertain training samples.
Case study. We visualize the hidden states, pseudo-
labels, and confidence of the training samples in the
QNLI tasks calculated by the pretrained DeBERTa
entailment model with the SimPLE algorithm in
Figure 4. The embedding space is calculated with t-
SNE (Van der Maaten and Hinton, 2008) using 252
training samples with 252*7=1764 embeddings.
Half of them are plotted in the figure. Each training
sample is evaluated with 7 different dropouts, and
the uncertainty is estimated with 9 neighbors. In

Figure 4, different embeddings of the same training
sample are labeled with the same color, while the
uncertain cases are marked in black. + and - stand
for the truth value of the suppositions. As shown
in the figure, most uncertain cases appear around
the uncertain circle. We also highlight two training
samples with uncertain representations. This phe-
nomenon indicates that the SimPLE algorithm can
drop most embeddings of a data sample and edit the
voting results of the dropout-based pseudo-labeling
method, improving the pseudo-labeling accuracy
from 76.5% to 79.2% in this experiment.
We also show that the original pseudo-label set is
unbalanced, with 67.1% of all predicted labels be-
ing “False”. Although we do not provide any prior
knowledge about the label distribution of the task
(unknown without human annotation), the SimPLE
method mitigates the bias through the uncertain
candidate removal process. Figure 4 shows that
most uncertain pseudo-labels estimated by Sim-
PLE are “False”, thus the remaining pseudo-labels
are more balanced.
7
Conclusion
We show that entailment-based language models
can be adapted to different NLU tasks without su-
pervision and achieve robust performance against
noisy pseudo-labels and adversarial texts. We de-
sign a supposition-based prompting strategy to
improve the zero-shot adaptation performance of
entailment-based models. To improve the stability
of self-training, we propose the SimPLE algorithm
for augmented pseudo-labeling. Experiments on
binary, multi-class, regular, and adversarial NLU
tasks show that the SimPLE self-training strategy
significantly outperforms a number of strong base-
lines, including 400 and 500 times larger language
models on both zero-shot and weakly supervised
settings, proving the effectivenss of entailment self-
training for efficient and trustworthy natural lan-
guage understanding systems.
Limitations
Our method utilized pretrained entailed models
and adapted them to other domains under zero-
shot and self-training settings. There are two lim-
itations that we would like to improve in future
work. Firstly, we use human-designed suppositions
for each task, which is less automatic than a di-
rect, zero-shot adaptation of the models. Secondly,
the self-training on some multi-class classification
tasks is not as high as on binary NLU tasks, indi-
cating the challenge of applying entailment models
to multi-choice tasks. We would like to overcome
this in the next step.
Ethics Statement
We propose a method that can significantly reduce
the financial and environmental cost of language
model learning. By reducing the need for data
collection and human labeling, our method can ef-
fectively protect user and data privacy by avoiding
leaking any information while building the training
corpora. We found that a medium-sized language
model can achieve similar performance as the state-
of-the-art large-scale language models, suggesting
that we can cost less financially and environmen-
tally during model training and evaluation for com-
parable performance. However, since we reduced
the need for human-labeling efforts, the deploy-
ment of the system might decrease the number of
data annotation jobs.
References
Armen Aghajanyan, Akshat Shrivastava, Anchit Gupta,
Naman Goyal, Luke Zettlemoyer, and Sonal Gupta.
2020. Better fine-tuning by reducing representational
collapse. In International Conference on Learning
Representations.
Max Bartolo, Alastair Roberts, Johannes Welbl, Sebas-
tian Riedel, and Pontus Stenetorp. 2020. Beat the ai:
Investigating adversarial human annotation for read-
ing comprehension. Transactions of the Association
for Computational Linguistics, 8:662–678.
Max Bartolo, Tristan Thrush, Robin Jia, Sebastian
Riedel, Pontus Stenetorp, and Douwe Kiela. 2021.
Improving question answering model robustness with
synthetic adversarial data generation. In Proceedings
of the 2021 Conference on Empirical Methods in Nat-
ural Language Processing, pages 8830–8848, Online
and Punta Cana, Dominican Republic. Association
for Computational Linguistics.
Avrim Blum and Tom Mitchell. 1998. Combining la-
beled and unlabeled data with co-training. In Pro-
ceedings of the eleventh annual conference on Com-
putational learning theory, pages 92–100.
Samuel R Bowman, Gabor Angeli, Christopher Potts,
and Christopher D Manning. 2015. A large annotated
corpus for learning natural language inference. arXiv
preprint arXiv:1508.05326.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, et al. 2020. Language models are few-shot

learners. Advances in neural information processing
systems, 33:1877–1901.
Jiaao Chen, Zichao Yang, and Diyi Yang. 2020. Mix-
Text: Linguistically-informed interpolation of hid-
den space for semi-supervised text classification. In
Proceedings of the 58th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 2147–
2157, Online. Association for Computational Lin-
guistics.
Kevin Clark, Minh-Thang Luong, Quoc V Le, and
Christopher D Manning. 2020. Electra: Pre-training
text encoders as discriminators rather than generators.
arXiv preprint arXiv:2003.10555.
Cleo Condoravdi, Dick Crouch, Valeria De Paiva, Rein-
hard Stolle, and Daniel Bobrow. 2003. Entailment,
intensionality and text understanding. In Proceedings
of the HLT-NAACL 2003 workshop on Text meaning,
pages 38–45.
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2005. The pascal recognising textual entailment chal-
lenge. In Machine learning challenges workshop,
pages 177–190. Springer.
Nauman Dawalatabad, Sameer Khurana, Antoine Lau-
rent, and James Glass. 2022.
On unsupervised
uncertainty-driven speech pseudo-label filtering and
model calibration. arXiv preprint arXiv:2211.07795.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2018. Bert: Pre-training of deep
bidirectional transformers for language understand-
ing. arXiv preprint arXiv:1810.04805.
Tianyu Gao, Adam Fisch, and Danqi Chen. 2020. Mak-
ing pre-trained language models better -shot learners.
arXiv preprint arXiv:2012.15723.
Tianyu Gao, Xingcheng Yao, and Danqi Chen. 2021.
Simcse: Simple contrastive learning of sentence em-
beddings. arXiv preprint arXiv:2104.08821.
Siddhant Garg and Goutham Ramakrishnan. 2020. Bae:
Bert-based adversarial examples for text classifica-
tion. arXiv preprint arXiv:2004.01970.
Yuxian Gu, Xu Han, Zhiyuan Liu, and Minlie Huang.
2021. Ppt: Pre-trained prompt tuning for -shot learn-
ing. arXiv preprint arXiv:2109.04332.
Junxian He, Jiatao Gu, Jiajun Shen, and Marc’Aurelio
Ranzato.
2019.
Revisiting
self-training
for
neural sequence generation.
arXiv preprint
arXiv:1909.13788.
Pengcheng He, Xiaodong Liu, Jianfeng Gao, and
Weizhu Chen. 2020. Deberta: Decoding-enhanced
bert with disentangled attention.
arXiv preprint
arXiv:2006.03654.
Mandar Joshi, Danqi Chen, Yinhan Liu, Daniel S Weld,
Luke Zettlemoyer, and Omer Levy. 2020. Spanbert:
Improving pre-training by representing and predict-
ing spans. Transactions of the Association for Com-
putational Linguistics, 8:64–77.
Phillip Keung, Yichao Lu, György Szarvas, and Noah A.
Smith. 2020. The multilingual amazon reviews cor-
pus. arxiv preprint: arXiv:2010.02573.
Sameer Khurana, Niko Moritz, Takaaki Hori, and
Jonathan Le Roux. 2021. Unsupervised domain adap-
tation for speech recognition via uncertainty driven
self-training. In ICASSP 2021-2021 IEEE Interna-
tional Conference on Acoustics, Speech and Signal
Processing (ICASSP), pages 6553–6557. IEEE.
Hunter Lang, Monica Agrawal, Yoon Kim, and David
Sontag. 2022. Co-training improves prompt-based
learning for large language models. arXiv preprint
arXiv:2202.00828.
Mike Lewis, Yinhan Liu, Naman Goyal, Marjan
Ghazvininejad, Abdelrahman Mohamed, Omer Levy,
Ves Stoyanov, and Luke Zettlemoyer. 2019. Bart: De-
noising sequence-to-sequence pre-training for natural
language generation, translation, and comprehension.
arXiv preprint arXiv:1910.13461.
Linyang Li, Ruotian Ma, Qipeng Guo, Xiangyang
Xue, and Xipeng Qiu. 2020. Bert-attack: Adver-
sarial attack against bert using bert. arXiv preprint
arXiv:2004.09984.
Ming Li and Zhi-Hua Zhou. 2005. Setred: Self-training
with editing. In Pacific-Asia Conference on Knowl-
edge Discovery and Data Mining, pages 611–621.
Springer.
Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding,
Yujie Qian, Zhilin Yang, and Jie Tang. 2021. Gpt
understands, too. arXiv preprint arXiv:2103.10385.
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-
dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,
Luke Zettlemoyer, and Veselin Stoyanov. 2019.
Roberta: A robustly optimized bert pretraining ap-
proach. arXiv preprint arXiv:1907.11692.
Ilya Loshchilov and Frank Hutter. 2018. Decoupled
weight decay regularization. In International Confer-
ence on Learning Representations.
Hongyin Luo and James Glass. 2023. Logic against
bias: Textual entailment mitigates stereotypical sen-
tence reasoning. In Proceedings of the 17th Confer-
ence of the European Chapter of the Association
for Computational Linguistics, pages 1243–1254,
Dubrovnik, Croatia. Association for Computational
Linguistics.
Hongyin Luo, Shang-Wen Li, Mingye Gao, Seunghak
Yu, and James Glass. 2022. Cooperative self-training
of machine reading comprehension. In Proceedings
of the 2022 Conference of the North American Chap-
ter of the Association for Computational Linguis-
tics: Human Language Technologies, pages 244–257,
Seattle, United States. Association for Computational
Linguistics.

Devraj Mandal, Shrisha Bharadwaj, and Soma Biswas.
2020. A novel self-supervised re-labeling approach
for training with noisy labels. In Proceedings of
the IEEE/CVF Winter Conference on Applications of
Computer Vision, pages 1381–1390.
Yu Meng, Yunyi Zhang, Jiaxin Huang, Chenyan Xiong,
Heng Ji, Chao Zhang, and Jiawei Han. 2020. Text
classification using label names only: A language
model self-training approach. In Proceedings of the
2020 Conference on Empirical Methods in Natural
Language Processing (EMNLP), pages 9006–9017.
Abiola Obamuyide and Andreas Vlachos. 2018. Zero-
shot relation classification as textual entailment.
EMNLP 2018, page 72.
Ethan Perez, Douwe Kiela, and Kyunghyun Cho. 2021.
True few-shot learning with language models. Ad-
vances in Neural Information Processing Systems,
34:11054–11070.
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine
Lee, Sharan Narang, Michael Matena, Yanqi Zhou,
Wei Li, Peter J Liu, et al. 2020. Exploring the limits
of transfer learning with a unified text-to-text trans-
former. J. Mach. Learn. Res., 21(140):1–67.
Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and
Percy Liang. 2016.
Squad: 100,000+ questions
for machine comprehension of text. arXiv preprint
arXiv:1606.05250.
Nils Reimers and Iryna Gurevych. 2019a. Sentence-
bert:
Sentence embeddings using siamese bert-
networks. arXiv preprint arXiv:1908.10084.
Nils Reimers and Iryna Gurevych. 2019b. Sentence-
BERT: Sentence embeddings using Siamese BERT-
networks. In Proceedings of the 2019 Conference on
Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natu-
ral Language Processing (EMNLP-IJCNLP), pages
3982–3992, Hong Kong, China. Association for Com-
putational Linguistics.
Richard Routley and Robertk Meyer. 1973. The se-
mantics of entailment. In Studies in Logic and the
Foundations of Mathematics, volume 68, pages 199–
243. Elsevier.
Victor Sanh, Albert Webson, Colin Raffel, Stephen H
Bach, Lintang Sutawika, Zaid Alyafeai, Antoine
Chaffin, Arnaud Stiegler, Teven Le Scao, Arun
Raja, et al. 2021. Multitask prompted training en-
ables zero-shot task generalization. arXiv preprint
arXiv:2110.08207.
Elvis Saravia, Hsien-Chi Toby Liu, Yen-Hao Huang,
Junlin Wu, and Yi-Shin Chen. 2018. Carer: Contextu-
alized affect representations for emotion recognition.
EMNLP 2018.
Timo Schick and Hinrich Schütze. 2021. Exploiting
cloze-questions for -shot text classification and nat-
ural language inference. In Proceedings of the 16th
Conference of the European Chapter of the Associ-
ation for Computational Linguistics: Main Volume,
pages 255–269.
Inkyu Shin, Sanghyun Woo, Fei Pan, and In So Kweon.
2020. Two-phase pseudo label densification for self-
training based domain adaptation. In European con-
ference on computer vision, pages 532–548. Springer.
Richard Socher, Alex Perelygin, Jean Wu, Jason
Chuang, Christopher D Manning, Andrew Y Ng, and
Christopher Potts. 2013. Recursive deep models for
semantic compositionality over a sentiment treebank.
In Proceedings of the 2013 conference on empiri-
cal methods in natural language processing, pages
1631–1642.
Kihyuk Sohn, David Berthelot, Nicholas Carlini, Zizhao
Zhang, Han Zhang, Colin A Raffel, Ekin Dogus
Cubuk, Alexey Kurakin, and Chun-Liang Li. 2020.
Fixmatch:
Simplifying semi-supervised learning
with consistency and confidence. Advances in neural
information processing systems, 33:596–608.
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,
Ilya Sutskever, and Ruslan Salakhutdinov. 2014.
Dropout: a simple way to prevent neural networks
from overfitting. The journal of machine learning
research, 15(1):1929–1958.
James Thorne and Andreas Vlachos. 2018. Automated
fact checking: Task formulations, methods and future
directions. arXiv preprint arXiv:1806.07687.
Shoujie Tong, Qingxiu Dong, Damai Dai, Tianyu Liu,
Baobao Chang, Zhifang Sui, et al. 2022. Robust
fine-tuning via perturbation and interpolation from
in-batch instances. arXiv preprint arXiv:2205.00633.
Laurens Van der Maaten and Geoffrey Hinton. 2008.
Visualizing data using t-sne. Journal of machine
learning research, 9(11).
Alex Wang, Yada Pruksachatkun, Nikita Nangia, Aman-
preet Singh, Julian Michael, Felix Hill, Omer Levy,
and Samuel R. Bowman. 2019. Superglue: A stickier
benchmark for general-purpose language understand-
ing systems. arxiv preprint: arXiv:1905.00537.
Alex Wang, Amanpreet Singh, Julian Michael, Felix
Hill, Omer Levy, and Samuel R Bowman. 2018.
Glue: A multi-task benchmark and analysis platform
for natural language understanding. arXiv preprint
arXiv:1804.07461.
Boxin Wang, Chejian Xu, Shuohang Wang, Zhe Gan,
Yu Cheng, Jianfeng Gao, Ahmed Hassan Awadal-
lah, and Bo Li. 2021a. Adversarial glue: A multi-
task benchmark for robustness evaluation of language
models. arXiv preprint arXiv:2111.02840.
Jianing Wang, Chengyu Wang, Fuli Luo, Chuanqi Tan,
Minghui Qiu, Fei Yang, Qiuhui Shi, Songfang Huang,
and Ming Gao. 2022. Towards unified prompt tun-
ing for -shot text classification.
arXiv preprint
arXiv:2205.05313.

Sinong Wang, Han Fang, Madian Khabsa, Hanzi Mao,
and Hao Ma. 2021b. Entailment as few-shot learner.
arXiv preprint arXiv:2104.14690.
Xinyi Wang, Wanrong Zhu, and William Yang Wang.
2023. Large language models are implicitly topic
models:
Explaining and finding good demon-
strations for in-context learning.
arXiv preprint
arXiv:2301.11916.
Alex Warstadt, Amanpreet Singh, and Samuel Bow-
man. 2019. Neural network acceptability judgments.
Transactions of the Association for Computational
Linguistics, 7:625–641.
Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel,
Barret Zoph, Sebastian Borgeaud, Dani Yogatama,
Maarten Bosma, Denny Zhou, Donald Metzler, et al.
2022a. Emergent abilities of large language models.
arXiv preprint arXiv:2206.07682.
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten
Bosma, Ed Chi, Quoc Le, and Denny Zhou. 2022b.
Chain of thought prompting elicits reasoning in large
language models. arXiv preprint arXiv:2201.11903.
Adina Williams, Nikita Nangia, and Samuel Bowman.
2018. A broad-coverage challenge corpus for sen-
tence understanding through inference. In Proceed-
ings of the 2018 Conference of the North American
Chapter of the Association for Computational Lin-
guistics: Human Language Technologies, Volume 1
(Long Papers), pages 1112–1122. Association for
Computational Linguistics.
Yann LeCun Xiang Zhang,
Junbo Zhao. 2015.
Character-level convolutional networks for text clas-
sification. arxiv preprint: arXiv:1509.01626.
Qizhe Xie, Zihang Dai, Eduard Hovy, Thang Luong,
and Quoc Le. 2020a. Unsupervised data augmen-
tation for consistency training. Advances in Neural
Information Processing Systems, 33:6256–6268.
Qizhe Xie, Minh-Thang Luong, Eduard Hovy, and
Quoc V Le. 2020b. Self-training with noisy student
improves imagenet classification. In Proceedings of
the IEEE/CVF Conference on Computer Vision and
Pattern Recognition, pages 10687–10698.
Runxin Xu, Fuli Luo, Zhiyuan Zhang, Chuanqi Tan,
Baobao Chang, Songfang Huang, and Fei Huang.
2021. Raise a child in large language model: To-
wards effective and generalizable fine-tuning. In Pro-
ceedings of the 2021 Conference on Empirical Meth-
ods in Natural Language Processing, pages 9514–
9528.
Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-
bonell, Russ R Salakhutdinov, and Quoc V Le. 2019.
Xlnet: Generalized autoregressive pretraining for lan-
guage understanding. Advances in neural informa-
tion processing systems, 32.
Wenpeng Yin, Jamaal Hay, and Dan Roth. 2019. Bench-
marking zero-shot text classification: Datasets, eval-
uation and entailment approach. In Proceedings of
the 2019 Conference on Empirical Methods in Natu-
ral Language Processing and the 9th International
Joint Conference on Natural Language Processing
(EMNLP-IJCNLP), pages 3914–3923, Hong Kong,
China. Association for Computational Linguistics.
Yuan Zang, Fanchao Qi, Chenghao Yang, Zhiyuan
Liu, Meng Zhang, Qun Liu, and Maosong Sun.
2019.
Word-level textual adversarial attacking
as combinatorial optimization.
arXiv preprint
arXiv:1910.12196.
Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and
Sameer Singh. 2021. Calibrate before use: Improv-
ing few-shot performance of language models. In In-
ternational Conference on Machine Learning, pages
12697–12706. PMLR.
Barret Zoph, Golnaz Ghiasi, Tsung-Yi Lin, Yin Cui,
Hanxiao Liu, Ekin Dogus Cubuk, and Quoc Le. 2020.
Rethinking pre-training and self-training. Advances
in neural information processing systems, 33:3833–
3845.
Yang Zou, Zhiding Yu, Xiaofeng Liu, BVK Kumar,
and Jinsong Wang. 2019. Confidence regularized
self-training. In Proceedings of the IEEE/CVF In-
ternational Conference on Computer Vision, pages
5982–5991.
A
Data Details
GLUE/AdvGLUE In this work, we evaluate our
method with the GLUE1 and AdvGLUE2 bench-
marks. We pretrain our models on MNLI, and
evaluate on all other AdvGLUE tasks, AdvQNLI,
AdvQQP, AdvRTE, and AdvSST2. we also eval-
uate the models on the regular versions of these
tasks in GLUE. The statistics of the GLUE and
AdvGLUE benchmarks are shown in Table 4.
Corpus
|Train|
|Test|
|Adv-Test|
MNLI
393k
20k
1.8k
QNLI
105k
5.4k
0.9k
QQP
364k
391k
0.4k
RTE
2.5k
3k
0.3k
SST2
67k
1.8k
1.4k
Table 4: Statistics of the corpora used in this work
Multi-Classification In multi-class classification
tasks, we evaluate our method with SuperGlue
1https://gluebenchmark.com/
2https://adversarialglue.github.io/

Copa(Wang et al., 2019), Emotion Classifica-
tion(Saravia et al., 2018), and Amazon Re-
view(Keung et al., 2020). The statistics of these
corpora are shown in Table 5.
Corpus
|Train|
|Test|
|Class Num|
Copa
400
100
2
Emotion
16k
2k
6
AR
200k
5k
5
News
120k
7.6k
4
Table 5: Statistics of the corpora used in multi-class
classification
Reproducibility
Data. We introduce the tasks and corpora we used
for training and evaluation in Section 5 and Ap-
pendix A.
Method. We introduce the difference between our
method and previous work in Section 2, the details
of our method in Section 3 and 4.
Hyper-parameter. We describe the key hyper-
parameters of self-training in Section 5.
Experiments. We describe the experiment results,
and number of independent runs in Section 5, and
Section 6 to prove the statistical significance.

