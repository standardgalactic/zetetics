Simple steps are all you need: Frank-Wolfe
and generalized self-concordant functions
Alejandro Carderera
ALEJANDRO.CARDERERA@GATECH.EDU
Department of Industrial and Systems Engineering
Georgia Institute of Technology
Atlanta, USA
Mathieu BesanÃ§on
BESANCON@ZIB.DE
Zuse Institute Berlin
Berlin, Germany
Sebastian Pokutta
POKUTTA@ZIB.DE
Institute of Mathematics
Zuse Institute Berlin and Technische UniversitÃ¤t Berlin
Berlin, Germany
Abstract
Generalized self-concordance is a key property present in the objective function of many important
learning problems. We establish the convergence rate of a simple Frank-Wolfe variant that uses the
open-loop step size strategy ğ›¾ğ‘¡= 2/(ğ‘¡+ 2), obtaining a O(1/ğ‘¡) convergence rate for this class of
functions in terms of primal gap and Frank-Wolfe gap, where ğ‘¡is the iteration count. This avoids the
use of second-order information or the need to estimate local smoothness parameters of previous
work. We also show improved convergence rates for various common cases, e.g., when the feasible
region under consideration is uniformly convex or polyhedral.
1. Introduction
Constrained convex optimization is the cornerstone of many machine learning problems. We consider
such problems, formulated as:
min
xâˆˆX ğ‘“(x),
(1.1)
where ğ‘“: â„ğ‘›â†’â„âˆª{+âˆ} is a generalized self-concordant function and X âŠ†â„ğ‘›is a compact convex
set. When computing projections onto the feasible regions as required in, e.g., projected gradient
descent, is prohibitive, Frank-Wolfe (FW) [Frank & Wolfe, 1956] algorithms (a.k.a. Conditional Gradi-
ents (CG) [Levitin & Polyak, 1966]) are often the algorithm of choice, relying on Linear Minimization
Oracles (LMO) at each iteration to solve Problem (1.1). The analysis of their convergence often relies
on the assumption that the gradient is Lipschitz-continuous. This assumption does not necessarily
hold for generalized self-concordant functions, an important class of functions for which the growth
can be unbounded.
1.1 Related work
In the classical analysis of Newtonâ€™s method, when the Hessian of ğ‘“is assumed to be Lipschitz
continuous and the function is strongly convex, one arrives at a convergence rate for the algorithm
that depends on the Euclidean structure of â„ğ‘›, despite the fact that the algorithm is afï¬ne-invariant.
This motivated the introduction of self-concordant functions in Nesterov & Nemirovskii [1994],
functions for which the third derivative is bounded by the second-order derivative, with which one
1
arXiv:2105.13913v1  [math.OC]  28 May 2021

can obtain an afï¬ne-invariant convergence rate for the aforementioned algorithm. More importantly,
many of the barrier functions used in interior-point methods are self-concordant, which extended the
use of polynomial-time interior point methods to many settings of interest.
Self-concordant functions have received strong interest in recent years due to the attractive properties
that they allow to prove for many statistical estimation settings [Marteau-Ferey et al., 2019, Ostrovskii
& Bach, 2021]. The original deï¬nition of self-concordance has been expanded and generalized since
its inception, as many objective functions of interest have self-concordant-like properties without
satisfying the strict deï¬nition of self-concordance. For example, the logistic loss function used in
logistic regression is not strictly self-concordant, but it ï¬ts into a class of pseudo-self-concordant
functions, which allows one to obtain similar properties and bounds as those obtained for self-
concordant functions [Bach et al., 2010]. This was also the case in Ostrovskii & Bach [2021] and
Tran-Dinh et al. [2015], in which more general properties of these pseudo-self-concordant functions
were established. This was fully formalized in Sun & Tran-Dinh [2019], in which the concept of
generalized-self concordant functions was introduced, along with key bounds, properties, and variants
of Newton methods for the unconstrained setting which make use of this property.
Most algorithms that aim to solve Problem (1.1) assume access to second-order information, as
this often allows the algorithms to make monotonous progress, remain inside the domain of ğ‘“,
and often, converge quadratically when close enough to the optimum. Recently, several lines of
work have focused on using Frank-Wolfe algorithm variants to solve these types of problems in the
projection-free setting, for example constructing second-order approximations to a self-concordant
ğ‘“using ï¬rst and second order information, and minimizing these approximations over X using
the Frank-Wolfe algorithm [Liu et al., 2020]. Other approaches, such as the ones presented in
Dvurechensky et al. [2020a] (later extended in Dvurechensky et al. [2020b]), apply the Frank-Wolfe
algorithm to a generalized self-concordant ğ‘“, using ï¬rst and second-order information about the
function to guarantee that the step sizes are so that the iterates do not leave the domain of ğ‘“, and
monotonous progress is made. An additional FW variant in that work, in the spirit of Garber & Hazan
[2016], utilizes ï¬rst and second order information about ğ‘“, along with a Local Linear Optimization
Oracle for X, to obtain a linear convergence rate in primal gap over polytopes given in inequality
description. The last algorithm presented in Dvurechensky et al. [2020b], the only one that does not
use second-order information, uses the FW algorithm with the backtracking line search of Pedregosa
et al. [2020] to estimate local smoothness parameters at a given iterate. Other specialized FW
algorithms have been developed for speciï¬c problems involving generalized self-concordant functions,
such as the FW variant developed for marginal inference with concave maximization [Krishnan et al.,
2015], or the variant developed in Zhao & Freund [2020] for ğœƒ-homogeneous barrier functions, a
subset of standard self-concordant functions.
1.2 Contribution (see also Table 1)
Simple FW for generalized self-concordant functions.
We show that a small variation of the
original Frank-Wolfe algorithm [Frank & Wolfe, 1956] with an open-loop step size of the form
ğ›¾ğ‘¡= 2/(ğ‘¡+ 2), where ğ‘¡is the iteration count is all that is needed to achieve a convergence rate of
O(1/ğ‘¡) in primal gap; this also answers an open question posed in Dvurechensky et al. [2020b]. Our
variation ensures monotonous progress while employing an open-loop strategy which, together with
the iterates being convex combinations, ensures that we do not leave the domain of ğ‘“. In contrast
to other methods that depend on either a line search or second-order information, our variant uses
only ï¬rst-order information and a domain oracle for ğ‘“(x). The assumption of the latter oracle is very
mild and was also implicitly assumed in the ï¬rst-order algorithm in Dvurechensky et al. [2020b].
As such, our iterations are much cheaper than those in previous work, while essentially achieving
the same convergence rates in the general case of Problem (1.1). Moreover, our variant relying on
2

the open-loop step size ğ›¾ğ‘¡= 2/(ğ‘¡+ 2) is adaptive, i.e., does not need to estimate local smoothness
parameters and it allows to establish a convergence rate of O(1/ğ‘¡) for the Frank-Wolfe gap as well.
Algorithm
Convergence
Reference
1st-order /
Requirements
Primal gap
FW gap
LS free?
FW-GSC
O(1/ğœ€)
-
[1, Alg.2]
 / 
-
B-FW
O(1/ğœ€)
-
[1, Alg.3]
 / 
DO
FW-LLOO
O(log 1/ğœ€)
-
[1, Alg.5]
 / 
polyh. X, LLOO
M-FW
O(1/ğœ€)
O(1/ğœ€)
This work
 / 
DO
B-AFW
O(log 1/ğœ€)
O(log 1/ğœ€)
This work
 / 
polyh. X, DO
Table 1: Convergence results for Problem 1.1 in the literature to achieve an ğœ€-optimal solution, in
terms of number of iterations. We denote Dvurechensky et al. [2020b] using [1], line search
by LS, domain oracle by DO, and local linear optimization oracle by LLOO.
Faster rates in common special cases.
We also obtain improved convergence rates when the
optimum is contained in the interior of X âˆ©dom( ğ‘“), or when the set X is uniformly or strongly
convex, using the backtracking line search of Pedregosa et al. [2020]. We also show that the Away-
Step Frank-Wolfe algorithm [Lacoste-Julien & Jaggi, 2015, Wolfe, 1970] can use the aforementioned
line search to achieve linear rates over polytopes.
Numerical experiments.
We provide numerical experiments that showcase the performance of
the algorithms on generalized self-concordant objectives to complement the theoretical results. In
particular, they highlight that the simple step size strategy we propose is competitive with and
sometimes outperforms other variants on many instances.
1.3 Preliminaries and Notation
We denote the (potentially non-unique) minimizer of Problem (1.1) by xâˆ—and we denote the primal
gap and the Frank-Wolfe gap at x âˆˆX as â„(x)
def= ğ‘“(x) âˆ’ğ‘“(xâˆ—) and ğ‘”(x)
def= maxvâˆˆX âŸ¨âˆ‡ğ‘“(x), x âˆ’vâŸ©,
respectively. We use âˆ¥Â·âˆ¥, âˆ¥Â·âˆ¥ğ», and âŸ¨Â·, Â·âŸ©to denote the Euclidean norm, the matrix norm induced by a
symmetric positive deï¬nite matrix ğ»âˆˆâ„ğ‘›Ã—ğ‘›, and the Euclidean inner product, respectively. We denote
the diameter of X as ğ·
def= maxx,yâˆˆX âˆ¥x âˆ’yâˆ¥. Given a non-empty set X âŠ‚â„ğ‘›we refer to its boundary
as Bd(X) and to its interior as Int (X). We denote the probability simplex of dimension ğ‘›by Î”ğ‘›and
the domain of ğ‘“, as dom( ğ‘“)
def= {x âˆˆâ„ğ‘›, ğ‘“(x) < +âˆ}. Given a compact convex set C âŠ†dom( ğ‘“) we
denote ğ¿C
ğ‘“=
max
uâˆˆC,dâˆˆâ„ğ‘›âˆ¥dâˆ¥2
âˆ‡2 ğ‘“(u) /âˆ¥dâˆ¥2
2 and ğœ‡C
ğ‘“=
min
uâˆˆC,dâˆˆâ„ğ‘›âˆ¥dâˆ¥2
âˆ‡2 ğ‘“(u) /âˆ¥dâˆ¥2
2. We assume access to:
1. Domain Oracle (DO): Given x âˆˆX, return true if x âˆˆdom( ğ‘“), false otherwise.
2. First-Order Oracle (FOO): Given x âˆˆdom( ğ‘“), return âˆ‡ğ‘“(x).
3. Linear Minimization Oracle (LMO): Given d âˆˆâ„ğ‘›, return argminxâˆˆX âŸ¨x, dâŸ©.
The FOO and LMO oracles are standard in the FW literature and the DO oracle is motivated by the
properties of generalized self-concordant functions. It is reasonable to assume the availability of
such oracles: following the deï¬nition of the function codomain, one could simply evaluate ğ‘“at x
and assert ğ‘“(x) < +âˆ. In many cases, testing the membership of x âˆˆdom( ğ‘“) is computationally less
demanding than the function evaluation.
3

Remark 1.1. Access to a domain oracle is a mild assumption, that was also implicitly assumed in
one of the three FW-variants presented in Dvurechensky et al. [2020b] when computing the step size
according to the strategy from Pedregosa et al. [2020]; see Line 3 in Algorithm 3. The remaining two
variants ensure that x âˆˆdom( ğ‘“) by using second-order information about ğ‘“, which we explicitly do
not rely on.
The following example motivates the use of Frank-Wolfe algorithms in the context of generalized
self-concordant functions. We present more examples in the computational results.
Example 1.2 (Intersection of a convex set with a polytope). Consider Problem (1.1) where X = Pâˆ©C,
P is a polytope over which we can minimize a linear function efï¬ciently, and C is a convex compact
set for which one can easily build a barrier function.
-1.5
0.0
1.5
3.0
4.5
4.5
6.0
7.5
xâˆ—
P
C
(a) Plot of ğ‘“(x).
2.0
4.0
6.0
8.0
xâˆ—
P
C
(b) Plot of ğ‘“(x) + ğœ‡â€²Î¦C(x).
-0.5
0.0
0.5
1.0
2.0
4.0
6.0
8.0
xâˆ—
P
C
(c) Plot of ğ‘“(x) + ğœ‡Î¦C(x).
Figure 1: Minimizing ğ‘“(x) over P âˆ©C, versus minimizing the sum of ğ‘“(x) and Î¦C(x) over P for two
different penalty values ğœ‡â€² and ğœ‡such that ğœ‡â€² â‰«ğœ‡.
Solving a linear optimization problem over X may be extremely expensive. In light of this, we can
incorporate C into the optimization problem through the use of a barrier penalty in the objective
function, minimizing instead ğ‘“(x) + ğœ‡Î¦C(x) where Î¦C(x) is a log-barrier function for C and ğœ‡is a
parameter controlling the penalization for points closer to Bd(C). The reformulation of the problem
is illustrated in Figure 1. Note that if the original objective function is generalized self-concordant, so
is the new objective function. We assume that computing the gradient of ğ‘“(x) + ğœ‡Î¦C(x) is roughly
as expensive as computing the gradient for ğ‘“(x) and solving an LP over P is inexpensive relative
to solving an LP over P âˆ©C. The ğœ‡parameter can be driven down to 0 after a solution converges
in a warm-starting procedure similar to interior-point methods, ensuring convergence to the true
optimum.
An additional advantage of this transformation of the problem is the solution structure. Running
Frank-Wolfe on the set P âˆ©C could potentially select a large number of extremal points from Bd(C)
if C is non-polyhedral. In contrast, P has a ï¬nite number of vertices, a small subset of which will be
selected throughout the optimization procedure. The same solution as that of the original problem
can thus be constructed as a convex combination of a small number of vertices of P, improving
sparsity and interpretability in many applications.
The following deï¬nition formalizes the setting of Problem (1.1).
Deï¬nition 1.3 (Generalized self-concordant function). Let ğ‘“âˆˆğ¶3 (dom( ğ‘“)) be a closed convex
function with dom( ğ‘“) âŠ†â„ğ‘›open. Then ğ‘“is (ğ‘€, ğœˆ) generalized self-concordant if:
|

ğ·3 ğ‘“(x)[w]u, u

| â‰¤ğ‘€âˆ¥uâˆ¥2
âˆ‡2 ğ‘“(x) âˆ¥wâˆ¥ğœˆâˆ’2
âˆ‡2 ğ‘“(x) âˆ¥wâˆ¥3âˆ’ğœˆ
2
,
for any x âˆˆdom( ğ‘“) and u, w âˆˆâ„ğ‘›, where ğ·3 ğ‘“(x)[w] = lim
ğ›¼â†’0 ğ›¼âˆ’1  âˆ‡2 ğ‘“(x + ğ›¼w) âˆ’âˆ‡2 ğ‘“(x).
4

2. Frank-Wolfe Convergence Guarantees
Algorithm 1 Monotonous Frank-Wolfe (M-FW)
Input: Point x0 âˆˆX âˆ©dom( ğ‘“), function ğ‘“
Output: Iterates x1, . . . âˆˆX
1: for ğ‘¡= 0 to . . . do
2:
vğ‘¡â†argminvâˆˆX âŸ¨âˆ‡ğ‘“(xğ‘¡), vâŸ©
3:
ğ›¾ğ‘¡â†2/(ğ‘¡+ 2)
4:
xğ‘¡+1 â†xğ‘¡+ ğ›¾ğ‘¡(vğ‘¡âˆ’xğ‘¡)
5:
if xğ‘¡+1 âˆ‰dom( ğ‘“) or ğ‘“(xğ‘¡+1) > ğ‘“(xğ‘¡) then
6:
xğ‘¡+1 â†xğ‘¡
We establish convergence rates for a Frank-
Wolfe variant with an open-loop step size
strategy on generalized self-concordant
functions. The Monotonous Frank-Wolfe (M-
FW) algorithm presented in Algorithm 1 is a
rather simple, but powerful modiï¬cation of
the standard Frank-Wolfe algorithm, with
the only difference that before taking a step,
we verify if xğ‘¡+ ğ›¾ğ‘¡(vğ‘¡âˆ’xğ‘¡) âˆˆdom( ğ‘“), and
if so, we check whether moving to the next
iterate provides primal progress. Note, that
the open-loop step size rule 2/(ğ‘¡+ 2) does
not guarantee monotonous primal progress for the vanilla Frank-Wolfe algorithm in general. If either
of these two checks fails, we simply do not move: the algorithm sets xğ‘¡+1 = xğ‘¡in Line 6 of Algorithm 1.
As customary, we assume short-circuit evaluation of the logical conditions in Algorithm 1, i.e., if the
ï¬rst condition in Line 5 is true, then the second condition is not even checked, and the algorithm
directly goes to Line 6. This minor modiï¬cation of the vanilla Frank-Wolfe algorithm enables us to use
the monotonicity of the iterates in the proofs to come, at the expense of one extra function evaluation
per iteration. In order to lower bound the progress per iteration we use Proposition 2.1.
Proposition 2.1. (C.f., [Sun & Tran-Dinh, 2019, Proposition 10]) Given a (ğ‘€, ğœˆ) generalized self-
concordant function, then for ğœˆâ‰¥2, we have that:
ğ‘“(y) âˆ’ğ‘“(x) âˆ’âŸ¨âˆ‡ğ‘“(x), y âˆ’xâŸ©â‰¤ğœ”ğœˆ(ğ‘‘ğœˆ(x âˆ’y)) âˆ¥y âˆ’xâˆ¥2
âˆ‡2 ğ‘“(x) ,
(2.1)
where the inequality holds if and only if ğ‘‘ğœˆ(x, y) < 1 for ğœˆ> 2, and we have that,
ğ‘‘ğœˆ(x, y)
def=
(
ğ‘€âˆ¥y âˆ’xâˆ¥
if ğœˆ= 2
( ğœˆ
2 âˆ’1)ğ‘€âˆ¥y âˆ’xâˆ¥3âˆ’ğœˆâˆ¥y âˆ’xâˆ¥ğœˆâˆ’2
âˆ‡2 ğ‘“(x)
if ğœˆ> 2,
where:
ğœ”ğœˆ(ğœ)
def=
ï£±ï£´ï£´ï£´ï£´ï£´ï£´ï£²
ï£´ï£´ï£´ï£´ï£´ï£´ï£³
ğ‘’ğœâˆ’ğœâˆ’1
ğœ2
if ğœˆ= 2
âˆ’ğœâˆ’ln(1âˆ’ğœ)
ğœ2
if ğœˆ= 3
(1âˆ’ğœ)ln(1âˆ’ğœ)+ğœ
ğœ2
if ğœˆ= 4

ğœˆâˆ’2
4âˆ’ğœˆ

1
ğœ
h
ğœˆâˆ’2
2(3âˆ’ğœˆ)ğœ

(1 âˆ’ğœ)
2(3âˆ’ğœˆ)
2âˆ’ğœˆ
âˆ’1

âˆ’1
i
otherwise.
The inequality shown in Equation (2.1) is very similar to the one that we would obtain if the gradient
of ğ‘“were Lipschitz continuous, however, while the Lipschitz continuity of the gradient leads to an
inequality that holds globally for all x, y âˆˆdom( ğ‘“), the inequality in Equation (2.1) only holds for
ğ‘‘ğœˆ(x, y) < 1. Moreover, there are two other important differences, the norm used in Equation (2.1) is
now the norm deï¬ned by the Hessian at xğ‘¡instead of the â„“2 norm, and the term multiplying the norm
is ğœ”ğœˆ(ğ‘‘ğœˆ(x, y)) instead of 1/2. We deal with the latter issue by bounding ğœ”ğœˆ(ğ‘‘ğœˆ(x, y)) with a constant
that depends on ğœˆfor any x, y âˆˆdom( ğ‘“) such that ğ‘‘ğœˆ(x, y) â‰¤1/2, as shown in Remark 2.2.
Remark 2.2. As ğ‘‘ğœ”ğœˆ(ğœ)/ğ‘‘ğœ> 0 for ğœ< 1 and ğœˆâ‰¥2, then ğœ”ğœˆ(ğœ) â‰¤ğœ”ğœˆ(1/2) for ğœâ‰¤1/2.
Due to the fact that we use a simple step size ğ›¾ğ‘¡= 2/(ğ‘¡+ 2), that we make monotonous progress, and
we ensure that the iterates are inside dom( ğ‘“), careful accounting allows us to bound the number of
iterations until ğ‘‘ğœˆ(xğ‘¡, xğ‘¡+ ğ›¾ğ‘¡(vğ‘¡âˆ’xğ‘¡)) â‰¤1/2. Before formalizing the convergence rate we ï¬rst review
a lemma that we will need in the proof.
5

Lemma 2.3. (C.f., [Sun & Tran-Dinh, 2019, Proposition 7]) Let ğ‘“be a generalized self concordant
function with ğœˆ> 2. If ğ‘‘ğœˆ(x, y) < 1 and x âˆˆdom( ğ‘“) then y âˆˆdom( ğ‘“). For the case ğœˆ= 2 we have that
dom( ğ‘“) = â„ğ‘›.
Putting all these things together allows us to obtain a convergence rate for Algorithm 1.
Theorem 2.4. Suppose X is a compact convex set and ğ‘“is a (ğ‘€, ğœˆ) generalized self-concordant function
with ğœˆâ‰¥2. Then the Monotonous Frank-Wolfe algorithm (Algorithm 1) satisï¬es:
â„(xğ‘¡) â‰¤4(ğ‘‡ğœˆ+ 1)
ğ‘¡+ 1
max
n
â„(x0), ğ¿L0
ğ‘“ğ·2ğœ”ğœˆ(1/2)
o
.
(2.2)
for ğ‘¡â‰¥ğ‘‡ğœˆ, where ğ¿L0
ğ‘“
=
max
uâˆˆL0,dâˆˆâ„ğ‘›âˆ¥dâˆ¥2
âˆ‡2 ğ‘“(u) /âˆ¥dâˆ¥2
2 and ğ‘‡ğœˆis deï¬ned as:
ğ‘‡ğœˆ
def=
(
âŒˆ4ğ‘€ğ·âŒ‰âˆ’2
if ğœˆ= 2
l
2ğ‘€ğ·(ğ¿L0
ğ‘“)ğœˆ/2âˆ’1(ğœˆâˆ’2)
m
âˆ’2
otherwise.
(2.3)
Otherwise it holds that â„(xğ‘¡) â‰¤â„(x0) for ğ‘¡< ğ‘‡ğœˆ.
Proof. Consider the compact set L0
def= {x âˆˆdom( ğ‘“) âˆ©X | ğ‘“(x) â‰¤ğ‘“(x0)}. As the algorithm makes
monotonous progress and moves towards points such that xğ‘¡âˆˆdom( ğ‘“), then xğ‘¡âˆˆL0 for ğ‘¡â‰¥
0. This allows us to claim, in a similar fashion as is done in Dvurechensky et al. [2020b], that
âˆ¥xğ‘¡âˆ’vğ‘¡âˆ¥2
âˆ‡2 ğ‘“(xğ‘¡) â‰¤ğ¿L0
ğ‘“ğ·2. We then deï¬ne ğ‘‡ğœˆas in Equation (2.3). Note that for ğ‘¡â‰¥ğ‘‡ğœˆwe have that
ğ‘‘(xğ‘¡, xğ‘¡+ ğ›¾ğ‘¡(vğ‘¡âˆ’xğ‘¡)) â‰¤1/2, and so as xğ‘¡âˆˆdom( ğ‘“) we will have xğ‘¡+ ğ›¾ğ‘¡(vğ‘¡âˆ’xğ‘¡) âˆˆdom( ğ‘“) for ğ‘¡â‰¥ğ‘‡ğœˆ,
by application of Lemma 2.3. This means that the non-zero step size ğ›¾ğ‘¡will automatically ensure that
xğ‘¡+ ğ›¾ğ‘¡(vğ‘¡âˆ’xğ‘¡) âˆˆdom( ğ‘“) in Line 5 of Algorithm 1. Moreover, it allows us to use the upper bound on
the Bregman divergence between points xğ‘¡and xğ‘¡+ ğ›¾ğ‘¡(vğ‘¡âˆ’xğ‘¡) in Proposition 2.1, which holds for
ğ‘‘(xğ‘¡, xğ‘¡+ ğ›¾ğ‘¡(vğ‘¡âˆ’xğ‘¡)) < 1. With this we can estimate the primal progress we can guarantee for ğ‘¡â‰¥ğ‘‡ğœˆ
if we move from xğ‘¡to xğ‘¡+ ğ›¾ğ‘¡(vğ‘¡âˆ’xğ‘¡):
â„(xğ‘¡+ ğ›¾ğ‘¡(vğ‘¡âˆ’xğ‘¡)) â‰¤â„(xğ‘¡) âˆ’ğ›¾ğ‘¡ğ‘”(xğ‘¡) + ğ›¾2
ğ‘¡ğœ”ğœˆ(ğ‘‘ğœˆ(xğ‘¡, xğ‘¡+ ğ›¾ğ‘¡(vğ‘¡âˆ’xğ‘¡))) âˆ¥vğ‘¡âˆ’xğ‘¡âˆ¥2
âˆ‡2 ğ‘“(xğ‘¡)
â‰¤â„(xğ‘¡) (1 âˆ’ğ›¾ğ‘¡) + ğ›¾2
ğ‘¡ğ¿L0
ğ‘“ğ·2ğœ”ğœˆ(1/2),
where the second inequality follows from the upper bound on the primal gap via the Frank-Wolfe
gap ğ‘”(xğ‘¡), the application of Remark 2.2 as for ğ‘¡â‰¥ğ‘‡ğœˆwe have that ğ‘‘ğœˆ(xğ‘¡, xğ‘¡+ ğ›¾ğ‘¡(vğ‘¡âˆ’xğ‘¡)) â‰¤1/2, and
from the fact that xğ‘¡âˆˆL0 for all ğ‘¡â‰¥0. With the previous chain of inequalities we can bound the
primal progress for ğ‘¡â‰¥ğ‘‡ğœˆas
â„(xğ‘¡) âˆ’â„(xğ‘¡+ ğ›¾ğ‘¡(vğ‘¡âˆ’xğ‘¡)) â‰¥ğ›¾ğ‘¡â„(xğ‘¡) âˆ’ğ›¾2
ğ‘¡ğ¿L0
ğ‘“ğ·2ğœ”ğœˆ(1/2).
(2.4)
From these facts we can prove the convergence rate shown in Equation (2.2) by induction. The base
case ğ‘¡= ğ‘‡ğœˆholds trivially by the fact that using monotonicity we have that â„(xğ‘‡ğœˆ) â‰¤â„(x0). Assuming
the claim is true for some ğ‘¡â‰¥ğ‘‡ğœˆwe distinguish two cases.
Case ğ›¾ğ‘¡â„(xğ‘¡) âˆ’ğ›¾2
ğ‘¡ğ¿L0
ğ‘“ğ·2ğœ”ğœˆ(1/2) > 0: Focusing on the ï¬rst case, we can plug the previous inequality
into Equation (2.4) to ï¬nd that ğ›¾ğ‘¡guarantees primal progress, that is, â„(xğ‘¡) > â„(xğ‘¡+ ğ›¾ğ‘¡(vğ‘¡âˆ’xğ‘¡))
with the step size ğ›¾ğ‘¡, and so we know that we will not go into Line 6 of Algorithm 1, and we have
that â„(xğ‘¡+1) = â„(xğ‘¡+ ğ›¾ğ‘¡(vğ‘¡âˆ’xğ‘¡)). Thus using the induction hypothesis and plugging in the expression
for ğ›¾ğ‘¡= 2/(ğ‘¡+ 2) into Equation (2.4) we have:
â„(xğ‘¡+1) â‰¤4 max
n
â„(x0), ğ¿L0
ğ‘“ğ·2ğœ”ğœˆ(1/2)
o 
(ğ‘‡ğœˆ+ 1)ğ‘¡
(ğ‘¡+ 1)(ğ‘¡+ 2) +
1
(ğ‘¡+ 2)2

â‰¤4(ğ‘‡ğœˆ+ 1)
ğ‘¡+ 2
max
n
â„(x0), ğ¿L0
ğ‘“ğ·2ğœ”ğœˆ(1/2)
o
,
6

where we use that (ğ‘‡ğœˆ+ 1)ğ‘¡/(ğ‘¡+ 1) + 1/(ğ‘¡+ 2) â‰¤ğ‘‡ğœˆ+ 1 for all ğ‘¡â‰¥0 and any ğ‘¡â‰¥ğ‘‡ğœˆ.
Case ğ›¾ğ‘¡â„(xğ‘¡) âˆ’ğ›¾2
ğ‘¡ğ¿L0
ğ‘“ğ·2ğœ”ğœˆ(1/2) â‰¤0:
In this case, we cannot guarantee that the step size ğ›¾ğ‘¡
provides primal progress by plugging into Equation (2.4), and so we cannot guarantee if a step size
of ğ›¾ğ‘¡will be accepted and we will have xğ‘¡+1 = xğ‘¡+ ğ›¾ğ‘¡(vğ‘¡âˆ’xğ‘¡), or we will simply have xğ‘¡+1 = xğ‘¡,
that is, we may go into Line 6 of Algorithm 1. Nevertheless, if we reorganize the expression
ğ›¾ğ‘¡â„(xğ‘¡) âˆ’ğ›¾2
ğ‘¡ğ¿L0
ğ‘“ğ·2ğœ”ğœˆ(1/2) â‰¤0, by monotonicity we will have that:
â„(xğ‘¡+1) â‰¤â„(xğ‘¡) â‰¤
2
ğ‘¡+ 2 ğ¿L0
ğ‘“ğ·2ğœ”ğœˆ(1/2) â‰¤4(ğ‘‡ğœˆ+ 1)
ğ‘¡+ 2
max
n
â„(x0), ğ¿L0
ğ‘“ğ·2ğœ”ğœˆ(1/2)
o
.
Where the last inequality holds as 2 â‰¤4(ğ‘‡ğœˆ+ 1) for any ğ‘‡ğœˆâ‰¥0.
â–¡
Remark 2.5. In the case where ğœˆ= 2 we can easily bound the primal gap â„(x1), as in this setting
dom( ğ‘“) = â„ğ‘›, which leads to â„(x1) â‰¤ğ¿X
ğ‘“ğ·2 from Equation (2.4), regardless of if we set x1 = x0 or
x1 = v0. Moreover, as the upper bound on the Bregman divergence holds for ğœˆ= 2 regardless of the
value of ğ‘‘2(x, y), we can modify the proof of Theorem 2.4 to obtain a convergence rate of the form
â„(xğ‘¡) â‰¤2/(ğ‘¡+ 1)ğ¿X
ğ‘“ğ·2ğ‘¤2(ğ‘€ğ·) for ğ‘¡â‰¥1, which is reminiscient of the O(ğ¿X
ğ‘“ğ·2/ğ‘¡) rate of the original
Frank-Wolfe algorithm for the smooth and convex case.
Note that in the proof of Theorem 2.4 we explicitly use the progress bound from generalized
self-concordance as opposed to the progress bound that arises from ğ¿L0
ğ‘“-smoothness, as there is
no straightforward way to bound the number of iterations until the latter progress bound holds
indeï¬nitely for all xğ‘¡+ ğ›¾ğ‘¡(vğ‘¡âˆ’xğ‘¡), while there is a straightforward criterion on ğ›¾ğ‘¡that allows us to
ensure that the former holds from some point onward (see Remark A.1 for more details). Furthermore,
with this simple step size we can also prove a convergence rate for the Frank-Wolfe gap, as shown in
Theorem 2.6 (see Theorem A.2 in Appendix for the proof).
Theorem 2.6. Suppose X is a compact convex set and ğ‘“is a (ğ‘€, ğœˆ) generalized self-concordant function
with ğœˆâ‰¥2. Then if the Monotonous Frank-Wolfe algorithm (Algorithm 1) is run for ğ‘‡â‰¥ğ‘‡ğœˆ+ 6 iterations,
we will have that min
1â‰¤ğ‘¡â‰¤ğ‘‡ğ‘”(xğ‘¡) â‰¤O(1/ğ‘‡).
2.1 Improved convergence guarantees
Algorithm
2
(Monotonous)
Frank-Wolfe
with
Backtrack of Pedregosa et al. [2020]
Input: x0 âˆˆX âˆ©dom( ğ‘“), function ğ‘“, estimate ğ¿âˆ’1
Output: Iterates x1, . . . âˆˆX
1: for ğ‘¡= 0 to . . . do
2:
vğ‘¡â†argminvâˆˆX âŸ¨âˆ‡ğ‘“(xğ‘¡), vâŸ©
3:
ğ›¾ğ‘¡, ğ¿ğ‘¡â†Backtrack( ğ‘“, xğ‘¡, vğ‘¡âˆ’xğ‘¡, ğ¿ğ‘¡âˆ’1, 1)
4:
xğ‘¡+1 â†xğ‘¡+ ğ›¾ğ‘¡(vğ‘¡âˆ’xğ‘¡)
We will now establish improved con-
vergence rates for various special cases.
We ï¬rst focus on the assumption that
xâˆ—âˆˆInt (X âˆ©dom( ğ‘“)), obtaining improved
rates when we use the FW algorithm cou-
pled with the adaptive step size strategy
from Pedregosa et al. [2020] (see Algo-
rithm 3).
The analysis in this case is reminiscent of
the analysis of GuÃ©lat & Marcotte [1986],
and is a reasonable assumption if for exam-
ple Bd(X) âŠˆdom( ğ‘“), and Int (X) âŠ†dom( ğ‘“). We can upper bound the value of ğ¿ğ‘¡for ğ‘¡â‰¥0 by
Ëœğ¿
def= max{ğœğ¿L0
ğ‘“, ğ¿âˆ’1}, where ğœ> 1 is the backtracking parameter and ğ¿âˆ’1 is the initial smoothness
estimate in Algorithm 3.
7

Algorithm 3 Backtrack( ğ‘“, x, d, âˆ‡ğ‘“(x), ğ¿ğ‘¡âˆ’1, ğ›¾max) (line search of Pedregosa et al. [2020])
Input: Point x âˆˆX âˆ©dom( ğ‘“), d âˆˆâ„ğ‘›, function ğ‘“, gradient âˆ‡ğ‘“(x), estimate ğ¿ğ‘¡âˆ’1, step ğ›¾max
Output: ğ›¾, ğ‘€
1: Choose ğœ> 1, ğœ‚â‰¤1 and ğ‘€âˆˆ[ğœ‚ğ¿ğ‘¡âˆ’1, ğ¿ğ‘¡âˆ’1]
2: ğ›¾= min{âˆ’âŸ¨âˆ‡ğ‘“(x), dâŸ©/(ğ‘€âˆ¥dâˆ¥2), ğ›¾max}
3: while ğ‘“(x + ğ›¾d) âˆ’ğ‘“(x) > ğ‘€ğ›¾2
2
âˆ¥dâˆ¥2 + ğ›¾âŸ¨âˆ‡ğ‘“(x), dâŸ©and x + ğ›¾d âˆˆdom( ğ‘“) do
4:
ğ‘€= ğœğ‘€
5:
ğ›¾= min{âˆ’âŸ¨âˆ‡ğ‘“(x), dâŸ©/(ğ‘€âˆ¥dâˆ¥2), ğ›¾max}
Theorem 2.7. Let ğ‘“be a (ğ‘€, ğœˆ) generalized self-concordant function with ğœˆâ‰¥2 and let dom( ğ‘“) not
contain straight lines. Furthermore, we denote by ğ‘Ÿ> 0 the largest value such that B(xâˆ—, ğ‘Ÿ) âŠ†Xâˆ©dom( ğ‘“).
Then the Frank-Wolfe algorithm with Backtrack (Algorithm 2) achieves a convergence rate for ğ‘¡â‰¥1 of:
â„(xğ‘¡) â‰¤â„(x0) Â©Â­
Â«
1 âˆ’
ğœ‡L0
ğ‘“
2Ëœğ¿
 ğ‘Ÿ
ğ·
2ÂªÂ®
Â¬
ğ‘¡
.
The assumption that dom( ğ‘“) does not contain straight lines in Theorem 2.7 is related to the Hessian
being positive deï¬nite over dom( ğ‘“) (see the proof in the Appendix in Theorem A.5). Note that this
is a very mild assumption as we can simply modify the function with a very small â„“2 regularizer, as
e.g., in Nesterov [2012]. Next, we recall the deï¬nition of uniformly convex sets, used in Kerdreux
et al. [2021], which will allow us to to obtain improved convergence rates for the FW algorithm over
uniformly convex feasible regions.
Deï¬nition 2.8 ((ğœ…, ğ‘)-uniformly convex set). Given two positive numbers ğœ…and ğ‘, we say the set
X âŠ†â„ğ‘›is (ğœ…, ğ‘)-uniformly convex with respect to a norm âˆ¥Â·âˆ¥if for any x, y âˆˆX, 0 â‰¤ğ›¾â‰¤1, and z âˆˆâ„ğ‘›
with âˆ¥zâˆ¥= 1 we have that y + ğ›¾(x âˆ’y) + ğ›¾(1 âˆ’ğ›¾) Â· ğœ…âˆ¥x âˆ’yâˆ¥ğ‘z âˆˆX.
Theorem 2.9. Suppose X is a compact (ğœ…, ğ‘)-strongly convex set and ğ‘“is a (ğ‘€, ğœˆ) generalized self-
concordant function with ğœˆâ‰¥2. Furthermore, assume that minxâˆˆX âˆ¥âˆ‡ğ‘“(x)âˆ¥â‰¥ğ¶> 0. Then the
Frank-Wolfe algorithm with Backtrack (Algorithm 2) achieves a convergence rate of:
â„ğ‘¡â‰¤
ï£±ï£´ï£´ï£´ï£´ï£²
ï£´ï£´ï£´ï£´ï£³
â„(x0)

1 âˆ’1
2 min
n
1, ğœ…ğ¶
Ëœğ¿
oğ‘¡
if ğ‘= 2
â„(x0)
2ğ‘¡
if ğ‘> 2, 1 â‰¤ğ‘¡â‰¤ğ‘¡0
Ëœğ¿ğ‘/(ğ‘âˆ’2)/(ğœ…ğ¶)2/(ğ‘âˆ’2)
(1+(ğ‘âˆ’2) (ğ‘¡âˆ’ğ‘¡0)/(2ğ‘))ğ‘/(ğ‘âˆ’2) = O  ğ‘¡âˆ’ğ‘/(ğ‘âˆ’2)
if ğ‘> 2, ğ‘¡> ğ‘¡0,
for ğ‘¡â‰¥1, where ğ‘¡0 = max
n
1,
j
log1/2

( Ëœğ¿ğ‘/(ğœ…ğ¶)2)1/(ğ‘âˆ’2)
â„(x0)
ko
.
However, in the general case we cannot assume that the norm of the gradient is bounded away from
zero over X. We deal with the general case in Theorem 2.10
Theorem 2.10. Suppose X is a compact (ğœ…, ğ‘)-strongly convex set and ğ‘“is a (ğ‘€, ğœˆ) generalized self-
concordant function with ğœˆâ‰¥2 for which the domain does not contain straight lines. Then the
Frank-Wolfe algorithm with Backtrack (Algorithm 2) results in a convergence rate:
â„ğ‘¡â‰¤
ï£±ï£´ï£´ï£²
ï£´ï£´ï£³
â„(x0)
2ğ‘¡
if 1 â‰¤ğ‘¡â‰¤ğ‘¡0
( Ëœğ¿ğ‘/(ğœ…2ğœ‡
L0
ğ‘“))1/(ğ‘âˆ’1)
(1+(ğ‘âˆ’1) (ğ‘¡âˆ’ğ‘¡0)/(2ğ‘))ğ‘/(ğ‘âˆ’1) = O  ğ‘¡âˆ’ğ‘/(ğ‘âˆ’1)
if ğ‘¡> ğ‘¡0,
for ğ‘¡â‰¥1, where ğ‘¡0 = max

1,

log1/2

( Ëœğ¿ğ‘/(ğœ…2ğœ‡
L0
ğ‘“))1/(ğ‘âˆ’1)
â„(x0)

.
8

Remark 2.11. Contrary to previous claims, there is no obstacle for the Away-step Frank-Wolfe (AFW)
algorithm [GuÃ©lat & Marcotte, 1986, Lacoste-Julien & Jaggi, 2015] together with the step size
strategy in Algorithm 3 to obtain a linear convergence rate in primal and Frank-Wolfe gap when X is
a polytope and ğ‘“is generalized self-concordant. This is not surprising, as ğ‘“is strongly convex and
smooth over L0 if dom( ğ‘“) does not contain straight lines, and monotonicity ensures the feasibility of
the iterates. We leave the analysis for this case to Appendix B, and the formal convergence statement
to Theorem B.2 and B.3.
3. Computational experiments
We showcase the performance of the Monotonous Frank-Wolfe algorithm (M-FW), the second-order
step size and the LLOO algorithm from Dvurechensky et al. [2020b] (GSC-FW and LLOO) and the
Frank-Wolfe and the Away-Step Frank-Wolfe algorithm with the backtracking stepsize of Pedregosa
et al. [2020], denoted by B-FW and B-AFW respectively. All experiments are carried out in Julia
using the FrankWolfe.jl package [BesanÃ§on et al., 2021], available under the MIT license and the
examples considered extend the ones presented in Dvurechensky et al. [2020b] and Liu et al. [2020].
We also use the vanilla FW algorithm denoted by FW, which is simply Algorithm 1 without Lines 5
and 6 using the traditional ğ›¾ğ‘¡= 2/(ğ‘¡+ 2) open-loop step size rule. Note that there are no formal
convergence guarantees for this algorithm when applied to Problem (1.1). Details on the experiments
setup, data and remarks on the considered problems are provided in Appendix C. All ï¬gures show
the evolution of the â„(xğ‘¡) and ğ‘”(xğ‘¡) against time and number of iterations with a log-log scale.
As in Dvurechensky et al. [2020b] we implemented the LLOO based variant only for the portfolio
optimization instance over the probability simplex; for the other examples the oracle implementation
was less straightforward due to the estimation of parameters.
As can be seen in all experiments, the Monotonous Frank-Wolfe algorithm is very competitive,
outperforming previously proposed variants in both in progress per iteration and time. The only other
algorithm that is sometimes faster is the Away-Step Frank-Wolfe variant as detailed in Remark 2.11,
which however depends on an active set, and can induce up to a quadratic overhead, making iterations
progressively more expensive; this can be also observed in our experiments as the advantage in time
is much less pronounced than in iterations.
Portfolio optimization. We consider ğ‘“(x) = âˆ’Ãğ‘
ğ‘¡=1 log(âŸ¨rğ‘¡, xâŸ©), where ğ‘denotes the number of
periods and X = Î”ğ‘›. The results are shown in Figure 2.
Signal recovery with KL divergence. We apply the aforementioned algorithms to the recovery
of a sparse signal from a noisy linear image using the Kullback-Leibler divergence, expressed as
ğ‘“(x) = ğ·(ğ‘Šx, y) = Ãğ‘
ğ‘–=1
n
âŸ¨wğ‘–, xâŸ©log

âŸ¨wğ‘–,xâŸ©
ğ‘¦ğ‘–

âˆ’âŸ¨wğ‘–, xâŸ©+ ğ‘¦ğ‘–
o
, where wğ‘–is the ğ‘–th row of a matrix ğ‘Š. In
order to promote sparsity and enforce nonnegativity of the solution, we use the unit simplex of radius
ğ‘…as the feasible set X = {x âˆˆâ„ğ‘‘
+, âˆ¥xâˆ¥1 â‰¤ğ‘…}. The results are shown in Figure 3. We used the same
ğ‘€= 1 choice for the second-order method as in Dvurechensky et al. [2020b] for comparison; its
admissibility is unknown (see Remark C.1).
Logistic regression. We consider a logistic regression task with a design matrix with rows ağ‘–âˆˆâ„ğ‘›
with 1 â‰¤ğ‘–â‰¤ğ‘and a vector y âˆˆ{âˆ’1, 1}ğ‘and formulate the problem with elastic net regularization,
in a similar fashion as is done in Liu et al. [2020], with ğ‘“(x) = 1/ğ‘Ãğ‘
ğ‘–=1 log(1 + exp(âˆ’ğ‘¦ğ‘–âŸ¨x, ağ‘–âŸ©)) +
ğœ‡/2 âˆ¥xâˆ¥2, where ğœ‡is a regularization parameter and X is the â„“1 ball of radius ğœŒ. The results can be
seen in Figure 4 and Appendix C.
Birkhoff polytope. All previously considered applications have in common a feasible region possess-
ing computationally inexpensive LMOs (probability/unit simplex and â„“1 norm ball). Additionally, each
vertex returned from the LMO is highly sparse with at most one non-zero element. To complement
9

the results, we consider the logistic regression problem over the Birkhoff polytope, where the LMO
call uses the Hungarian method and is not as inexpensive as in the other examples. The results are
shown in Figure 5.
100
101
102
103
Iteration
10âˆ’10
10âˆ’6
10âˆ’2
â„(xí‘¡)
10âˆ’1
101
103
Time [s]
100
101
102
103
Iteration
10âˆ’5
10âˆ’3
10âˆ’1
101
í‘”(xí‘¡)
10âˆ’1
101
103
Time [s]
M-FW
FW
B-FW
B-AFW
GSC-FW
LLOO
Figure 2: Portfolio Optimization: LLOO and GSC-FW perform similarly to FW on a per-iteration basis
but the iterations are computationally more expensive. B-AFW is the fastest method both in
terms iteration and runtime, followed by M-FW which is the only other method to terminate
with the speciï¬ed dual gap tolerance.
100
101
102
103
Iteration
10âˆ’3
100
103
â„(xí‘¡)
10âˆ’1
101
103
Time [s]
100
101
102
103
Iteration
101
103
105
í‘”(xí‘¡)
10âˆ’1
101
103
Time [s]
M-FW
FW
B-FW
B-AFW
GSC-FW
Figure 3: Signal Recovery: B-AFW signiï¬cantly outperforms all other methods. FW and B-FW perform
similarly in dual gap progress and converge slower than M-FW. In terms of primal gap
progress, M-FW and FW perform similarly and outperform B-FW.
100
102
104
Iteration
10âˆ’11
10âˆ’8
10âˆ’5
10âˆ’2
â„(xí‘¡)
10âˆ’1
101
103
Time [s]
100
102
104
Iteration
10âˆ’6
10âˆ’4
10âˆ’2
100
í‘”(xí‘¡)
M-FW
FW
B-FW
B-AFW
GSC-FW
10âˆ’1
101
103
Time [s]
Figure 4: Logistic Regression: This instance shows that although simple in essence, M-FW can
outperform other methods including B-AFW in terms of convergence. The primal and dual
gaps for B-FW and GSC-FW converge at similar rates against iteration count.
10

100
102
104
Iteration
10âˆ’11
10âˆ’7
10âˆ’3
101
â„(xí‘¡)
10âˆ’1
101
Time [s]
100
102
104
Iteration
10âˆ’6
10âˆ’4
10âˆ’2
100
í‘”(xí‘¡)
10âˆ’1
101
Time [s]
M-FW
FW
B-FW
B-AFW
GSC-FW
Figure 5: Birkhoff Polytope: B-AFW is the fastest-converging method for all measures. However, the
dual gap reaches a plateau due to numerical issues above the termination threshold, unlike
M-FW which reaches the dual gap tolerance. GSC-FW is run for 1000 iterations only given
the longer runtime. Its slow progress is likely due to numerical instabilities in the Hessian
computation which do not occur in ï¬rst-order methods.
Acknowledgements
Research reported in this paper was partially supported through the Research Campus Modal funded
by the German Federal Ministry of Education and Research (fund numbers 05M14ZAM,05M20ZBM)
and the Deutsche Forschungsgemeinschaft (DFG) through the DFG Cluster of Excellence MATH+.
11

References
Bach, F. et al. Self-concordant analysis for logistic regression. Electronic Journal of Statistics, 4:
384â€“414, 2010.
BesanÃ§on, M., Carderera, A., and Pokutta, S. FrankWolfe.jl: a high-performance and ï¬‚exible toolbox
for Frank-Wolfe algorithms and conditional gradients. arXiv preprint arXiv:2104.06675, 2021.
Csiszar, I. et al. Why least squares and maximum entropy? An axiomatic approach to inference for
linear inverse problems. The annals of statistics, 19(4):2032â€“2066, 1991.
Dvurechensky, P., Ostroukhov, P., Saï¬n, K., Shtern, S., and Staudigl, M. Self-concordant analysis of
Frank-Wolfe algorithms. In Proceedings of the 37th International Conference on Machine Learning,
pp. 2814â€“2824. PMLR, 2020a.
Dvurechensky, P., Saï¬n, K., Shtern, S., and Staudigl, M. Generalized self-concordant analysis of
Frank-Wolfe algorithms. arXiv preprint arXiv:2010.01009, 2020b.
Frank, M. and Wolfe, P. An algorithm for quadratic programming. Naval research logistics quarterly, 3
(1-2):95â€“110, 1956.
Garber, D. and Hazan, E.
A linearly convergent variant of the conditional gradient algorithm
under strong convexity, with applications to online and stochastic optimization. SIAM Journal on
Optimization, 26(3):1493â€“1528, 2016.
GuÃ©lat, J. and Marcotte, P. Some comments on Wolfeâ€™s â€˜away stepâ€™. Mathematical Programming, 35
(1):110â€“119, 1986.
Jaggi, M. Revisiting Frank-Wolfe: Projection-free sparse convex optimization. In Proceedings of the
30th International Conference on Machine Learning, pp. 427â€“435. PMLR, 2013.
Kerdreux, T., dâ€™Aspremont, A., and Pokutta, S. Restarting Frank-Wolfe. In Proceedings of the 22nd
International Conference on Artiï¬cial Intelligence and Statistics, pp. 1275â€“1283. PMLR, 2019.
Kerdreux, T., dâ€™Aspremont, A., and Pokutta, S. Projection-free optimization on uniformly convex sets.
In Proceedings of the 24th International Conference on Artiï¬cial Intelligence and Statistics, pp. 19â€“27.
PMLR, 2021.
Krishnan, R. G., Lacoste-Julien, S., and Sontag, D. Barrier Frank-Wolfe for Marginal Inference. In
Proceedings of the 28th Conference in Neural Information Processing Systems. PMLR, 2015.
Lacoste-Julien, S. and Jaggi, M. On the global linear convergence of Frank-Wolfe optimization
variants. In Proceedings of the 29th Conference on Neural Information Processing Systems, pp.
566â€“575. PMLR, 2015.
Levitin, E. S. and Polyak, B. T. Constrained minimization methods. USSR Computational Mathematics
and Mathematical Physics, 6(5):1â€“50, 1966.
Liu, D., Cevher, V., and Tran-Dinh, Q. A Newton Frank-Wolfe method for constrained self-concordant
minimization. arXiv preprint arXiv:2002.07003, 2020.
Marron, J. S., Todd, M. J., and Ahn, J. Distance-weighted discrimination. Journal of the American
Statistical Association, 102(480):1267â€“1271, 2007.
Marteau-Ferey, U., Ostrovskii, D., Bach, F., and Rudi, A.
Beyond least-squares: Fast rates for
regularized empirical risk minimization through self-concordance. In Proceedings of the 32nd
Conference on Learning Theory, pp. 2294â€“2340. PMLR, 2019.
12

Nesterov, Y. How to make the gradients small. Optima. Mathematical Optimization Society Newsletter,
(88):10â€“11, 2012.
Nesterov, Y. and Nemirovskii, A. Interior-point polynomial algorithms in convex programming. SIAM,
1994.
Nesterov, Y. et al. Lectures on convex optimization, volume 137. Springer, 2018.
Ostrovskii, D. M. and Bach, F. Finite-sample analysis of M-estimators using self-concordance. Electronic
Journal of Statistics, 15(1):326â€“391, 2021.
Pedregosa, F., Negiar, G., Askari, A., and Jaggi, M. Linearly convergent Frankâ€“Wolfe with backtracking
line-search. In Proceedings of the 23rd International Conference on Artiï¬cial Intelligence and Statistics.
PMLR, 2020.
Sun, T. and Tran-Dinh, Q. Generalized self-concordant functions: a recipe for Newton-type methods.
Mathematical Programming, 178(1):145â€“213, 2019.
Temlyakov, V. Greedy approximation in convex optimization. Constructive Approximation, 41(2):
269â€“296, 2015.
Tran-Dinh, Q., Li, Y.-H., and Cevher, V. Composite convex minimization involving self-concordant-like
cost functions. In Modelling, Computation and Optimization in Information Systems and Management
Sciences, pp. 155â€“168. Springer, 2015.
Wolfe, P. Convergence theory in nonlinear programming. In Integer and Nonlinear Programming, pp.
1â€“36. North-Holland, Amsterdam, 1970.
Zhao, R. and Freund, R. M. Analysis of the Frank-Wolfe method for logarithmically-homogeneous
barriers, with an extension. arXiv preprint arXiv:2010.08999, 2020.
13

Simple steps are all you need: Frank-Wolfe
and generalized self-concordant functions
Supplementary material
Outline.
The appendix of the paper is organized as follows:
â€¢ Section A presents the full convergence proof of the Frank-Wolfe gap for the Monotonous Frank-
Wolfe algorithm, and improved convergence bounds when using the Frank-Wolfe algorithm with
the step size strategy of Pedregosa et al. [2020] when the optimum is contained in the interior of
X âˆ©dom( ğ‘“), or when the feasible region is strongly convex.
â€¢ Section B reviews the Away-step Frank-Wolfe algorithm [GuÃ©lat & Marcotte, 1986, Lacoste-Julien
& Jaggi, 2015], and shows how using the step size strategy of Pedregosa et al. [2020] one can
show a linear convergence in primal gap and in Frank-Wolfe gap when the feasible region is a
polytope.
â€¢ Section C presents additional information about the experimental section of the paper.
Appendix A. Monotonous Frank-Wolfe
This appendix contains the theoretical proofs that have not been included in the main body of the
paper due to space constraints, as well as several remarks of interest. We start off with a remark
regarding the convergence proof in Theorem 2.4, and continue by showing that the Monotonous
Frank-Wolfe algorithm (restated for convenience in Algorithm 4) not only contracts the primal gap at
a rate of O(1/ğ‘¡) where ğ‘¡is the iteration count, but also ensures that the minimum of the Frank-Wolfe
gap over the run of the algorithm is bounded by O(1/ğ‘¡) from above.
Algorithm 4 Monotonous Frank-Wolfe (M-FW)
Input: Point x0 âˆˆX âˆ©dom( ğ‘“), function ğ‘“
Output: Iterates x1, . . . âˆˆX
1: for ğ‘¡= 0 to . . . do
2:
vğ‘¡â†argminvâˆˆX âŸ¨âˆ‡ğ‘“(xğ‘¡), vâŸ©
3:
ğ›¾ğ‘¡â†2/(ğ‘¡+ 2)
4:
xğ‘¡+1 â†xğ‘¡+ ğ›¾ğ‘¡(vğ‘¡âˆ’xğ‘¡)
5:
if xğ‘¡+1 âˆ‰dom( ğ‘“) or ğ‘“(xğ‘¡+1) > ğ‘“(xğ‘¡) then
6:
xğ‘¡+1 â†xğ‘¡
Remark A.1 (Regarding the proof of Theorem 2.4). One of the quantities that we have used in the
proof of Theorem 2.4 is ğ¿L0
ğ‘“. Note that the function ğ‘“is ğ¿L0
ğ‘“-smooth over L0. One could wonder then
why we have bothered to use the bounds on the Bregman divergence in Proposition 2.1 for a (ğ‘€, ğœˆ)-
generalized self-concordant function, instead of simply using the bounds from the ğ¿L0
ğ‘“-smoothness
of ğ‘“over L0. The reason is that the upper bound on the Bregman divergence in Proposition 2.1
applies for any x, y âˆˆdom( ğ‘“) such that ğ‘‘ğœˆ(x, y) < 1, and we can easily bound the number of
iterations ğ‘‡ğœˆit takes for the step size ğ›¾ğ‘¡= 2/(ğ‘¡+ 2) to verify both xğ‘¡, xğ‘¡+ ğ›¾ğ‘¡(vğ‘¡âˆ’xğ‘¡) âˆˆdom( ğ‘“)
and ğ‘‘ğœˆ(xğ‘¡, xğ‘¡+ ğ›¾ğ‘¡(vğ‘¡âˆ’xğ‘¡)) < 1 for ğ‘¡â‰¥ğ‘‡ğœˆ. However, in order to apply the bounds on the Bregman
divergence we need xğ‘¡, xğ‘¡+ğ›¾ğ‘¡(vğ‘¡âˆ’xğ‘¡) âˆˆL0, and while it is easy to show by monotonicity that xğ‘¡âˆˆL0,
14

there is no straightforward way to prove that for some Ëœğ‘‡ğœˆwe have that xğ‘¡+ ğ›¾ğ‘¡(vğ‘¡âˆ’xğ‘¡) âˆˆL0 for all
ğ‘¡â‰¥Ëœğ‘‡ğœˆ.
A.1 Convergence of the Frank-Wolfe gap for the Monotonous Frank-Wolfe
algorithm
In this section of the appendix we show that when running the Monotonous Frank-Wolfe algorithm
(Algorithm 1) the minimum of the Frank-Wolfe gap over the run of the algorithm converges at a
rate of O(1/ğ‘¡). The idea of the proof is very similar to the one in Jaggi [2013]. In a nutshell, as the
primal progress per iteration is directly related to the step size times the Frank-Wolfe gap, we know
that the Frank-Wolfe gap cannot remain indeï¬nitely above a given value, as otherwise we would
obtain a large amount of primal progress, which would make the primal gap become negative. This
is formalized in Theorem A.2.
Theorem A.2. Suppose X is a compact convex set and ğ‘“is a (ğ‘€, ğœˆ) generalized self-concordant function
with ğœˆâ‰¥2. Then if the Monotonous Frank-Wolfe algorithm (Algorithm 1) is run for ğ‘‡â‰¥ğ‘‡ğœˆ+ 6 iterations,
we will have that:
min
1â‰¤ğ‘¡â‰¤ğ‘‡ğ‘”(xğ‘¡) â‰¤O(1/ğ‘‡),
where ğ‘‡ğœˆis deï¬ned as:
ğ‘‡ğœˆ
def=
(
âŒˆ4ğ‘€ğ·âŒ‰âˆ’2
if ğœˆ= 2
l
2ğ‘€ğ·(ğ¿L0
ğ‘“)ğœˆ/2âˆ’1(ğœˆâˆ’2)
m
âˆ’2
otherwise.
(A.1)
Proof. In order to prove the claim, we focus on the iterations ğ‘‡ğœˆ+ âŒˆ(ğ‘‡âˆ’ğ‘‡ğœˆ)/3âŒ‰âˆ’2 â‰¤ğ‘¡â‰¤ğ‘‡âˆ’2,
where ğ‘‡ğœˆis deï¬ned in Equation (A.1). Note that as we assume that ğ‘‡â‰¥ğ‘‡ğœˆ+ 6, we know that
ğ‘‡ğœˆâ‰¤ğ‘‡ğœˆ+ âŒˆ(ğ‘‡âˆ’ğ‘‡ğœˆ)/3âŒ‰âˆ’2, and so for iterations ğ‘‡ğœˆ+ âŒˆ(ğ‘‡âˆ’ğ‘‡ğœˆ)/3âŒ‰âˆ’2 â‰¤ğ‘¡â‰¤ğ‘‡âˆ’2 we know that
ğ‘‘ğœˆ(xğ‘¡, xğ‘¡+1) â‰¤1/2, and so:
â„(xğ‘¡+1) â‰¤â„(xğ‘¡) âˆ’ğ›¾ğ‘¡ğ‘”(xğ‘¡) + ğ›¾2
ğ‘¡ğ¿L0
ğ‘“ğ·2ğœ”ğœˆ(1/2).
(A.2)
In a very similar fashion as was done in the proof of Theorem 2.4, we divide the proof into two
different cases.
Case âˆ’ğ›¾ğ‘¡ğ‘”(xğ‘¡) + ğ›¾2
ğ‘¡ğ¿L0
ğ‘“ğ·2ğœ”ğœˆ(1/2) â‰¥0 for some ğ‘‡ğœˆ+ âŒˆ(ğ‘‡âˆ’ğ‘‡ğœˆ)/3âŒ‰âˆ’2 â‰¤ğ‘¡â‰¤ğ‘‡âˆ’2: Reordering the
inequality above we therefore know that there exists a ğ‘‡ğœˆ+ âŒˆ(ğ‘‡âˆ’ğ‘‡ğœˆ)/3âŒ‰âˆ’2 â‰¤ğ¾â‰¤ğ‘‡âˆ’2 such that:
ğ‘”(xğ¾) â‰¤
2
2 + ğ¾ğ¿L0
ğ‘“ğ·2ğœ”ğœˆ(1/2)
â‰¤
2
ğ‘‡ğœˆ+ âŒˆ(ğ‘‡âˆ’ğ‘‡ğœˆ)/3âŒ‰ğ¿L0
ğ‘“ğ·2ğœ”ğœˆ(1/2)
=
6
2ğ‘‡ğœˆ+ ğ‘‡ğ¿L0
ğ‘“ğ·2ğœ”ğœˆ(1/2),
where the second inequality follows from the fact that ğ‘‡ğœˆ+ âŒˆ(ğ‘‡âˆ’ğ‘‡ğœˆ)/3âŒ‰âˆ’2 â‰¤ğ¾. This leads to
min
1â‰¤ğ‘¡â‰¤ğ‘‡ğ‘”(xğ‘¡) â‰¤ğ‘”(xğ¾) â‰¤
6
2ğ‘‡ğœˆ+ğ‘‡ğ¿L0
ğ‘“ğ·2ğœ”ğœˆ(1/2).
Case âˆ’ğ›¾ğ‘¡ğ‘”(xğ‘¡)+ğ›¾2
ğ‘¡ğ¿L0
ğ‘“ğ·2ğœ”ğœˆ(1/2) < 0 for all ğ‘‡ğœˆ+ğ‘‡ğœˆ+âŒˆ(ğ‘‡âˆ’ğ‘‡ğœˆ)/3âŒ‰âˆ’2 â‰¤ğ‘¡â‰¤ğ‘‡âˆ’2: Using the inequality
above and plugging into Equation (A.2) allows us to conclude that all steps ğ‘‡ğœˆ+ğ‘‡ğœˆ+ âŒˆ(ğ‘‡âˆ’ğ‘‡ğœˆ)/3âŒ‰âˆ’2 â‰¤
ğ‘¡â‰¤ğ‘‡âˆ’2 will produce primal progress using the step size ğ›¾ğ‘¡, and so as we know that xğ‘¡+1 âˆˆdom( ğ‘“)
by Lemma 2.3, then for all ğ‘‡ğœˆ+ âŒˆ(ğ‘‡âˆ’ğ‘‡ğœˆ)/3âŒ‰âˆ’2 â‰¤ğ‘¡â‰¤ğ‘‡âˆ’2 we will take a non-zero step size
determined by ğ›¾ğ‘¡, as xğ‘¡+ ğ›¾ğ‘¡(vğ‘¡âˆ’xğ‘¡) âˆˆdom( ğ‘“) and ğ‘“(xğ‘¡+ ğ›¾ğ‘¡(vğ‘¡âˆ’xğ‘¡)) < ğ‘“(xğ‘¡) in Line 5 of Algorithm 1.
15

Consequently, summing up Equation (A.2) from ğ‘¡min
def= ğ‘‡ğœˆ+ âŒˆ(ğ‘‡âˆ’ğ‘‡ğœˆ)/3âŒ‰âˆ’2 to ğ‘¡max
def= ğ‘‡âˆ’2 we have
that:
â„(xğ‘¡max+1) â‰¤â„ xğ‘¡min
 âˆ’
ğ‘¡max
âˆ‘ï¸
ğ‘¡=ğ‘¡min
ğ›¾ğ‘¡ğ‘”(xğ‘¡) + ğ¿L0
ğ‘“ğ·2ğœ”ğœˆ(1/2)
ğ‘¡max
âˆ‘ï¸
ğ‘¡=ğ‘¡min
ğ›¾2
ğ‘¡
(A.3)
â‰¤â„ xğ‘¡min
 âˆ’2
min
ğ‘¡min â‰¤ğ‘¡â‰¤ğ‘¡max ğ‘”(xğ‘¡)
ğ‘¡max
âˆ‘ï¸
ğ‘¡=ğ‘¡min
1
2 + ğ‘–+ 4ğ¿L0
ğ‘“ğ·2ğœ”ğœˆ(1/2)
ğ‘¡max
âˆ‘ï¸
ğ‘¡=ğ‘¡min
1
(2 + ğ‘¡)2
(A.4)
â‰¤â„ xğ‘¡min
 âˆ’2 min
1â‰¤ğ‘¡â‰¤ğ‘‡ğ‘”(xğ‘¡) ğ‘¡max âˆ’ğ‘¡min + 1
2 + ğ‘¡max
+ 4ğ¿L0
ğ‘“ğ·2ğœ”ğœˆ(1/2) ğ‘¡max âˆ’ğ‘¡min + 1
(2 + ğ‘¡min)2
(A.5)
â‰¤4
 ğ‘‡ğœˆ+ 1
ğ‘¡min + 1 + ğ‘¡max âˆ’ğ‘¡min + 1
(2 + ğ‘¡min)2

max
n
â„(x0), ğ¿L0
ğ‘“ğ·2ğœ”ğœˆ(1/2)
o
(A.6)
âˆ’2 min
1â‰¤ğ‘¡â‰¤ğ‘‡ğ‘”(xğ‘¡) ğ‘¡max âˆ’ğ‘¡min + 1
2 + ğ‘¡max
.
(A.7)
Note that Equation A.4 stems from the fact that minğ‘¡min â‰¤ğ‘¡â‰¤ğ‘¡max ğ‘”(xğ‘¡) â‰¤ğ‘”(xğ‘¡) for any ğ‘¡min â‰¤ğ‘¡â‰¤ğ‘¡max,
and from plugging ğ›¾ğ‘¡= 2/(2+ğ‘¡), and Equation A.5 follows from the fact that âˆ’1/(2+ğ‘¡) â‰¤âˆ’1/(2+ğ‘¡max)
and 1/(2 + ğ‘¡) â‰¤1/(2 + ğ‘¡min) for all ğ‘¡min â‰¤ğ‘¡â‰¤ğ‘¡max. The last inequality, Equation (A.6) and (A.7) arises
from plugging in the upper bound on the primal gap â„(xğ‘¡min) from Theorem 2.4 and collecting terms.
If we plug in the speciï¬c values of ğ‘¡max and ğ‘¡min this leads to:
â„(xğ‘‡âˆ’2) â‰¤12

ğ‘‡ğœˆ+ 1
2ğ‘‡ğœˆ+ ğ‘‡âˆ’3 + 2ğ‘‡âˆ’2ğ‘‡ğœˆ+ 3
(2ğ‘‡ğœˆ+ ğ‘‡)2

max
n
â„(x0), ğ¿L0
ğ‘“ğ·2ğœ”ğœˆ(1/2)
o
(A.8)
âˆ’2
3 min
1â‰¤ğ‘¡â‰¤ğ‘‡ğ‘”(xğ‘¡)ğ‘‡âˆ’ğ‘‡ğœˆ
ğ‘‡
.
(A.9)
We establish our claim using proof by contradiction. Assume that:
min
1â‰¤ğ‘¡â‰¤ğ‘‡ğ‘”(xğ‘¡) >
18ğ‘‡
ğ‘‡âˆ’ğ‘‡ğœˆ

ğ‘‡ğœˆ+ 1
2ğ‘‡ğœˆ+ ğ‘‡âˆ’3 + 2ğ‘‡âˆ’2ğ‘‡ğœˆ+ 3
(2ğ‘‡ğœˆ+ ğ‘‡)2

max
n
â„(x0), ğ¿L0
ğ‘“ğ·2ğœ”ğœˆ(1/2)
o
.
Then by plugging into the bound in Equation (A.9) we have that â„(xğ‘‡âˆ’2) < 0, which is the desired
contradiction, as the primal gap cannot be negative. Therefore we must have that:
min
1â‰¤ğ‘–â‰¤ğ‘‡ğ‘”(xğ‘–) â‰¤
18ğ‘‡
ğ‘‡âˆ’ğ‘‡ğœˆ

ğ‘‡ğœˆ+ 1
2ğ‘‡ğœˆ+ ğ‘‡âˆ’3 + 2ğ‘‡âˆ’2ğ‘‡ğœˆ+ 3
(2ğ‘‡ğœˆ+ ğ‘‡)2

max
n
â„(x0), ğ¿L0
ğ‘“ğ·2ğœ”ğœˆ(1/2)
o
= O(1/ğ‘‡).
This completes the proof.
â–¡
A.2 Improved convergence bounds
We focus on two different settings to obtain improved convergence rates, in the ï¬rst we assume that
xâˆ—âˆˆInt (X âˆ©dom( ğ‘“)) (Section A.2.1), and in the second we assume that X is strongly or uniformly
convex (Section A.2.2). In this section we focus on the combination of a slightly modiï¬ed Frank-Wolfe
algorithm with the adaptive line search technique of Pedregosa et al. [2020] (shown for reference in
Algorithm 5 and 6). This is the same algorithm used in Dvurechensky et al. [2020b], however we
show improved convergence rates in several settings of interest.
16

Algorithm 5 Monotonous Frank-Wolfe with the adaptive step size of Pedregosa et al. [2020]
Input: Point x0 âˆˆX âˆ©dom( ğ‘“), function ğ‘“, initial smoothness estimate ğ¿âˆ’1
Output: Iterates x1, . . . âˆˆX
1: for ğ‘¡= 0 to . . . do
2:
vğ‘¡â†argminvâˆˆX âŸ¨âˆ‡ğ‘“(xğ‘¡), vâŸ©
3:
ğ›¾ğ‘¡, ğ¿ğ‘¡â†Backtrack( ğ‘“, xğ‘¡, vğ‘¡âˆ’xğ‘¡, ğ¿ğ‘¡âˆ’1, 1)
4:
xğ‘¡+1 â†xğ‘¡+ ğ›¾ğ‘¡(vğ‘¡âˆ’xğ‘¡)
Algorithm 6 Backtrack( ğ‘“, x, d, ğ¿ğ‘¡âˆ’1, ğ›¾max) (line search of Pedregosa et al. [2020])
Input: Point x âˆˆX âˆ©dom( ğ‘“), v âˆˆâ„ğ‘›, function ğ‘“, estimate ğ¿ğ‘¡âˆ’1, step size ğ›¾max
Output: ğ›¾, ğ‘€
1: Choose ğœ> 1, ğœ‚â‰¤1 and ğ‘€âˆˆ[ğœ‚ğ¿ğ‘¡âˆ’1, ğ¿ğ‘¡âˆ’1]
2: ğ›¾= min{âˆ’âŸ¨âˆ‡ğ‘“(x), dâŸ©/(ğ‘€âˆ¥dâˆ¥2), ğ›¾max}
3: while ğ‘“(x + ğ›¾d) âˆ’ğ‘“(x) > ğ‘€ğ›¾2
2
âˆ¥dâˆ¥2 + ğ›¾âŸ¨âˆ‡ğ‘“(x), dâŸ©and x + ğ›¾d âˆˆdom( ğ‘“) do
4:
ğ‘€= ğœğ‘€
5:
ğ›¾= min{âˆ’âŸ¨âˆ‡ğ‘“(x), dâŸ©/(ğ‘€âˆ¥dâˆ¥2), ğ›¾max}
A.2.1 OPTIMUM CONTAINED IN THE INTERIOR
Before proving the main theoretical results of this section we ï¬rst review some auxiliary results that
allow us to prove the linear convergence in this setting.
Theorem A.3. (C.f., [Nesterov et al., 2018, Theorem 5.1.6]) Let ğ‘“be generalized self-concordant and
dom( ğ‘“) not contain straight lines, then the Hessian âˆ‡2 ğ‘“(x) is non-degenerate at all points x âˆˆdom( ğ‘“).
Note that the assumption that dom( ğ‘“) not contain straight lines is without loss of generality as we
can simply modify the function outside of our compact convex feasible region so that it holds.
Proposition A.4. (C.f., GuÃ©lat & Marcotte [1986]) If there exists an ğ‘Ÿ> 0 such that B(xâˆ—, ğ‘Ÿ) âŠ†
X âˆ©dom( ğ‘“), then for all x âˆˆX âˆ©dom( ğ‘“) we have that:
ğ‘”(x)
âˆ¥x âˆ’vâˆ¥â‰¥ğ‘Ÿ
ğ·âˆ¥âˆ‡ğ‘“(x)âˆ¥â‰¥ğ‘Ÿ
ğ·
âŸ¨âˆ‡ğ‘“(x), x âˆ’xâˆ—âŸ©
âˆ¥x âˆ’xâˆ—âˆ¥
,
where v = argmin
yâˆˆX
âŸ¨âˆ‡ğ‘“(x), yâŸ©and ğ‘”(x) is the Frank-Wolfe gap.
With these tools at hand, we have that the Frank-Wolfe algorithm with the backtracking step size
strategy converges at a linear rate.
Theorem A.5. Let ğ‘“be a (ğ‘€, ğœˆ) generalized self-concordant function with ğœˆâ‰¥2 and let dom( ğ‘“) not
contain straight lines. Furthermore, we denote by ğ‘Ÿ> 0 the largest value such that B(xâˆ—, ğ‘Ÿ) âŠ†Xâˆ©dom( ğ‘“).
Then the Frank-Wolfe algorithm (Algorithm 5) with the backtracking strategy of Pedregosa et al. [2020]
results in a convergence:
â„(xğ‘¡) â‰¤â„(x0) Â©Â­
Â«
1 âˆ’
ğœ‡L0
ğ‘“
2Ëœğ¿
 ğ‘Ÿ
ğ·
2ÂªÂ®
Â¬
ğ‘¡
,
for ğ‘¡â‰¥1, where Ëœğ¿
def= max{ğœğ¿L0
ğ‘“, ğ¿âˆ’1}, ğœ> 1 is the backtracking parameter, ğ¿âˆ’1 is the initial smoothness
estimate in Algorithm 6, ğœ‡L0
ğ‘“
=
min
uâˆˆL0,dâˆˆâ„ğ‘›âˆ¥dâˆ¥2
âˆ‡2 ğ‘“(u) /âˆ¥dâˆ¥2
2 and ğ¿L0
ğ‘“
=
max
uâˆˆL0,dâˆˆâ„ğ‘›âˆ¥dâˆ¥2
âˆ‡2 ğ‘“(u) /âˆ¥dâˆ¥2
2.
17

Proof. Consider the compact set L0
def= {x âˆˆdom( ğ‘“) âˆ©X | ğ‘“(x) â‰¤ğ‘“(x0)}. As the backtracking line
search makes monotonous primal progress, we know that for ğ‘¡â‰¥0 we will have that xğ‘¡âˆˆL0.
Consequently we can deï¬ne ğœ‡L0
ğ‘“
= minuâˆˆL0,dâˆˆâ„ğ‘›âˆ¥dâˆ¥2
âˆ‡2 ğ‘“(u) /âˆ¥dâˆ¥2
2. As by our assumption dom( ğ‘“) does
not contain any straight lines we know that for all x âˆˆdom( ğ‘“) the Hessian is non-degenerate, and
therefore ğœ‡L0
ğ‘“
> 0. This allows us to claim that for any x, y âˆˆL0 we have that:
ğ‘“(x) âˆ’ğ‘“(y) âˆ’âŸ¨âˆ‡ğ‘“(y), x âˆ’yâŸ©â‰¥
ğœ‡L0
ğ‘“
2
âˆ¥x âˆ’yâˆ¥2 .
(A.10)
The backtracking line search in Algorithm 6 will either output a point ğ›¾ğ‘¡= 1 or ğ›¾ğ‘¡< 1. In any
case, Algorithm 6 will ï¬nd and output a smoothness estimate ğ¿ğ‘¡and a step size ğ›¾ğ‘¡such that for
xğ‘¡+1 = xğ‘¡+ ğ›¾ğ‘¡(vğ‘¡âˆ’xğ‘¡) we have that:
ğ‘“(xğ‘¡+1) âˆ’ğ‘“(xğ‘¡) â‰¤ğ¿ğ‘¡ğ›¾2
ğ‘¡
2
âˆ¥xğ‘¡âˆ’vğ‘¡âˆ¥2 âˆ’ğ›¾ğ‘¡ğ‘”(xğ‘¡).
(A.11)
In the case where ğ›¾ğ‘¡= 1 we know by observing Line 5 of Algorithm 6 that ğ‘”(xğ‘¡) â‰¥ğ¿ğ‘¡âˆ¥xğ‘¡âˆ’vğ‘¡âˆ¥2,
and so plugging into Equation (A.11) we arrive at â„(xğ‘¡+1) â‰¤â„(xğ‘¡)/2. In the case where ğ›¾ğ‘¡=
ğ‘”(xğ‘¡)/(ğ¿ğ‘¡âˆ¥xğ‘¡âˆ’vğ‘¡âˆ¥2) < 1, we have that ğ‘”(xğ‘¡) < ğ¿ğ‘¡âˆ¥xğ‘¡âˆ’vğ‘¡âˆ¥2, which leads to â„(xğ‘¡+1) â‰¤â„(xğ‘˜) âˆ’
ğ‘”(xğ‘¡)2/(2ğ¿ğ‘¡âˆ¥xğ‘¡âˆ’vğ‘¡âˆ¥2), when plugging the expression for the step size in the progress bound in
Equation A.11. In this last case where ğ›¾ğ‘¡< 1 we have the following contraction for the primal gap:
â„(xğ‘¡) âˆ’â„(xğ‘¡+1) â‰¥
ğ‘”(xğ‘¡)2
2ğ¿ğ‘¡âˆ¥xğ‘¡âˆ’vğ‘¡âˆ¥2
â‰¥ğ‘Ÿ2
ğ·2
âˆ¥âˆ‡ğ‘“(xğ‘¡)âˆ¥2
2ğ¿ğ‘¡
â‰¥
ğœ‡L0
ğ‘“
Ëœğ¿
ğ‘Ÿ2
ğ·2 â„(xğ‘¡),
where we have used the inequality that involves the central term and the leftmost term in Proposi-
tion A.4, and the last inequality stems from the bound â„(xğ‘¡) â‰¤âˆ¥âˆ‡ğ‘“(xğ‘¡)âˆ¥2 /(2ğœ‡L0
ğ‘“) for ğœ‡L0
ğ‘“-strongly
convex functions. Putting the above bounds together we have that:
â„(xğ‘¡+1) â‰¤â„(xğ‘¡) Â©Â­
Â«
1 âˆ’1
2 min
ï£±ï£´ï£²
ï£´ï£³
1,
ğœ‡L0
ğ‘“
Ëœğ¿
 ğ‘Ÿ
ğ·
2ï£¼ï£´ï£½
ï£´ï£¾
ÂªÂ®
Â¬
â‰¤â„(xğ‘¡) Â©Â­
Â«
1 âˆ’
ğœ‡L0
ğ‘“
2Ëœğ¿
 ğ‘Ÿ
ğ·
2ÂªÂ®
Â¬
,
which completes the proof.
â–¡
The previous bound depends on the largest positive ğ‘Ÿsuch that B(xâˆ—, ğ‘Ÿ) âŠ†X âˆ©dom( ğ‘“), which can be
arbitrarily small. Note also that the previous proof uses the lower bound of the Bregman divergence
from the ğœ‡L0
ğ‘“-strong convexity of the function over L0 to obtain linear convergence. Note that
this bound is local, and is only of use because the step size strategy of Algorithm 6 automatically
ensures that if xğ‘¡âˆˆL0 and dğ‘¡is a direction of descent, then xğ‘¡+ ğ›¾ğ‘¡dğ‘¡âˆˆL0. This is in contrast with
Algorithm 4, in which the step size ğ›¾ğ‘¡= 2/(2 + 1) did not automatically ensure monotonicity in primal
gap, and this had to be enforced by setting xğ‘¡+1 = xğ‘¡if ğ‘“(xğ‘¡+ ğ›¾ğ‘¡dğ‘¡) > ğ‘“(xğ‘¡), where dğ‘¡= vğ‘¡âˆ’xğ‘¡. If
we were to have used the lower bound on the Bregman divergence from Sun & Tran-Dinh [2019,
Proposition 10] in the proof, which states that:
ğ‘“(y) âˆ’ğ‘“(x) âˆ’âŸ¨âˆ‡ğ‘“(x), y âˆ’xâŸ©â‰¥ğœ”ğœˆ(âˆ’ğ‘‘ğœˆ(x âˆ’y)) âˆ¥y âˆ’xâˆ¥2
âˆ‡2 ğ‘“(x) ,
18

for any x, y âˆˆdom( ğ‘“) and any ğœˆâ‰¥2, we would have arrived at a bound that holds over all dom( ğ‘“).
However, in order to arrive at a usable bound, and armed only with the knowledge that the Hessian
is non-degenerate if dom( ğ‘“) does not contain straight lines, and that x, y âˆˆL0, we would have had
to write:
ğœ”ğœˆ(âˆ’ğ‘‘ğœˆ(x âˆ’y)) âˆ¥x âˆ’yâˆ¥2
âˆ‡2 ğ‘“(y) â‰¥ğœ‡L0
ğ‘“ğœ”ğœˆ(âˆ’ğ‘‘ğœˆ(x âˆ’y)) âˆ¥x âˆ’yâˆ¥2 ,
where the inequality follows from the deï¬nition of ğœ‡L0
ğ‘“. It is easy to see that as ğ‘‘ğœ”ğœˆ(ğœ)/ğ‘‘ğœ> 0 by
Remark 2.2, we have that 1/2 = ğœ”ğœˆ(0) â‰¥ğœ”ğœˆ(âˆ’ğ‘‘ğœˆ(x âˆ’y)). This results in a bound:
ğ‘“(y) âˆ’ğ‘“(x) âˆ’âŸ¨âˆ‡ğ‘“(x), y âˆ’xâŸ©â‰¥ğœ‡L0
ğ‘“ğœ”ğœˆ(âˆ’ğ‘‘ğœˆ(x âˆ’y)) âˆ¥x âˆ’yâˆ¥2 .
(A.12)
When we compare the bounds on Equation (A.10) and (A.12), we can see that the bound from ğœ‡L0
ğ‘“-
strong convexity is tighter than the bound from the properties of (ğ‘€, ğœˆ)-generalized self-concordant
functions, albeit local. This is the reason why we have used the former bound in the proof of
Theorem A.5.
A.2.2 STRONGLY CONVEX OR UNIFORMLY CONVEX SETS
In order to prove convergence rate results for the case where the feasible region is (ğœ…, ğ‘)-strongly
convex, we ï¬rst review the deï¬nition of the (ğœ…, ğ‘)-strong convexity of a set (see Deï¬nition A.6), as
well as a useful lemma that allows us to go from contractions to convergence rates.
Deï¬nition A.6 ((ğœ…, ğ‘)-uniformly convex set). Given two positive numbers ğœ…and ğ‘, we say the set
X âŠ†â„ğ‘›is (ğœ…, ğ‘)-uniformly convex with respect to a norm âˆ¥Â·âˆ¥if for any x, y âˆˆX, 0 â‰¤ğ›¾â‰¤1, and z âˆˆâ„ğ‘›
with âˆ¥zâˆ¥= 1 we have that:
y + ğ›¾(x âˆ’y) + ğ›¾(1 âˆ’ğ›¾) Â· ğœ…âˆ¥x âˆ’yâˆ¥ğ‘z âˆˆX.
The previous deï¬nition allows us to obtain a scaling inequality very similar to the one shown in
Theorem A.4, which is key to proving the following convergence rates, and can be implicitly found in
Kerdreux et al. [2021] and Garber & Hazan [2016].
Proposition A.7. Let X âŠ†â„ğ‘›be (ğœ…, ğ‘)-uniformly convex, then for all x âˆˆX:
ğ‘”(x)
âˆ¥x âˆ’vâˆ¥ğ‘â‰¥ğœ…âˆ¥âˆ‡ğ‘“(x)âˆ¥,
where v = argminuâˆˆX âŸ¨âˆ‡ğ‘“(x), uâŸ©, and ğ‘”(x) is the Frank-Wolfe gap.
The next lemma that will be presented is an extension of the one used in Kerdreux et al. [2021,
Lemma A.1] (see also Temlyakov [2015]), and allows us to go from per iteration contractions to
convergence rates.
Lemma A.8. We denote a sequence of nonnegative numbers by {â„ğ‘¡}ğ‘¡. Let ğ‘0, ğ‘1, ğ‘2 and ğ›¼be positive
numbers such that ğ‘1 < 1, â„1 â‰¤ğ‘0 and â„ğ‘¡âˆ’â„ğ‘¡+1 â‰¥â„ğ‘¡min{ğ‘1, ğ‘2â„ğ›¼
ğ‘¡} for ğ‘¡â‰¥1, then:
â„ğ‘¡â‰¤
(
ğ‘0 (1 âˆ’ğ‘1)ğ‘¡âˆ’1
if 1 â‰¤ğ‘¡â‰¤ğ‘¡0
(ğ‘1/ğ‘2)1/ğ›¼
(1+ğ‘1 ğ›¼(ğ‘¡âˆ’ğ‘¡0))1/ğ›¼= O  ğ‘¡âˆ’1/ğ›¼
otherwise.
where
ğ‘¡0
def= max

1,

log1âˆ’ğ‘1
 (ğ‘1/ğ‘2)1/ğ›¼
ğ‘0

.
19

This allows us to conveniently transform the per iteration contractions to convergence rates. Moving
on to the proof of the convergence rate.
Theorem A.9. Suppose X is a compact (ğœ…, ğ‘)-strongly convex set and ğ‘“is a (ğ‘€, ğœˆ) generalized self-
concordant function with ğœˆâ‰¥2. Furthermore, assume that minxâˆˆX âˆ¥âˆ‡ğ‘“(x)âˆ¥â‰¥ğ¶. Then the Frank-Wolfe
algorithm with Backtrack (Algorithm 5) results in a convergence:
â„ğ‘¡â‰¤
ï£±ï£´ï£´ï£´ï£´ï£²
ï£´ï£´ï£´ï£´ï£³
â„(x0)

1 âˆ’1
2 min
n
1, ğœ…ğ¶
Ëœğ¿
oğ‘¡
if ğ‘= 2
â„(x0)
2ğ‘¡
if ğ‘> 2, 1 â‰¤ğ‘¡â‰¤ğ‘¡0
( Ëœğ¿ğ‘/(ğœ…ğ¶)2)1/(ğ‘âˆ’2)
(1+(ğ‘âˆ’2) (ğ‘¡âˆ’ğ‘¡0)/(2ğ‘))ğ‘/(ğ‘âˆ’2) = O  ğ‘¡âˆ’ğ‘/(ğ‘âˆ’2)
if ğ‘> 2, ğ‘¡> ğ‘¡0,
for ğ‘¡â‰¥1, where:
ğ‘¡0 = max

1,

log1/2
 (Ëœğ¿ğ‘/(ğœ…ğ¶)2)1/(ğ‘âˆ’2)
â„(x0)

.
and Ëœğ¿
def= max{ğœğ¿L0
ğ‘“, ğ¿âˆ’1}, where ğœ> 1 is the backtracking parameter, ğ¿âˆ’1 is the initial smoothness
estimate in Algorithm 6, and ğ¿L0
ğ‘“
=
max
uâˆˆL0,dâˆˆâ„ğ‘›âˆ¥dâˆ¥2
âˆ‡2 ğ‘“(u) /âˆ¥dâˆ¥2
2.
Proof. At iteration ğ‘¡the backtracking line search strategy ï¬nds through successive function evaluations
a ğ¿ğ‘¡> 0 such that:
â„(xğ‘¡+1) â‰¤â„(xğ‘¡) âˆ’ğ›¾ğ‘¡ğ‘”(xğ‘¡) + ğ¿ğ‘¡ğ›¾2
ğ‘¡
2
âˆ¥xğ‘¡âˆ’vğ‘¡âˆ¥2 .
Finding the ğ›¾ğ‘¡that maximizes the right-hand side of the previous inequality leads to ğ›¾ğ‘¡= min{1, ğ‘”(xğ‘¡)/(ğ¿ğ‘¡âˆ¥xğ‘¡âˆ’vğ‘¡âˆ¥2)},
which is the step size ultimately taken by the algorithm at iteration ğ‘¡. Note that if ğ›¾ğ‘¡= 1 this means
that ğ‘”(xğ‘¡) â‰¥ğ¿ğ‘¡âˆ¥xğ‘¡âˆ’vğ‘¡âˆ¥2, which when plugged into the inequality above leads to â„(xğ‘¡+1) â‰¤â„(xğ‘¡)/2.
Conversely, for ğ›¾ğ‘¡< 1 we have that â„(xğ‘¡+1) â‰¤â„(xğ‘¡) âˆ’ğ‘”(xğ‘¡)2/(2ğ¿ğ‘¡âˆ¥xğ‘¡âˆ’vğ‘¡âˆ¥2). Focusing on this case
and using the bounds ğ‘”(xğ‘¡) â‰¥â„(xğ‘¡) and ğ‘”(xğ‘¡) â‰¥ğœ…âˆ¥âˆ‡ğ‘“(xğ‘¡)âˆ¥âˆ¥xğ‘¡âˆ’vğ‘¡âˆ¥ğ‘from Proposition A.7 leads to:
â„(xğ‘¡+1) â‰¤â„(xğ‘¡) âˆ’â„(xğ‘¡)2âˆ’2/ğ‘(ğœ…âˆ¥âˆ‡ğ‘“(xğ‘¡)âˆ¥)2/ğ‘
2ğ¿ğ‘¡
(A.13)
â‰¤â„(xğ‘¡) âˆ’â„(xğ‘¡)2âˆ’2/ğ‘(ğœ…ğ¶)2/ğ‘
2Ëœğ¿
,
(A.14)
where the last inequality simply comes from the bound on the gradient norm, and the fact that ğ¿ğ‘¡â‰¤Ëœğ¿,
for Ëœğ¿
def= max{ğœğ¿L0
ğ‘“, ğ¿âˆ’1}, where ğœ> 1 is the backtracking parameter and ğ¿âˆ’1 is the initial smoothness
estimate in Algorithm 6. Reordering this expression and putting together the two cases we have that:
â„(xğ‘¡) âˆ’â„(xğ‘¡+1) â‰¥â„(xğ‘¡) min
(
1
2, (ğœ…ğ¶)2/ğ‘
2Ëœğ¿
â„(xğ‘¡)1âˆ’2/ğ‘
)
.
For the case where ğ‘= 2 we get a linear contraction in primal gap. Using Lemma A.8 to go from a
contraction to a convergence rate for ğ‘> 2 we have that:
â„ğ‘¡â‰¤
ï£±ï£´ï£´ï£´ï£´ï£²
ï£´ï£´ï£´ï£´ï£³
â„(x0)

1 âˆ’1
2 min
n
1, ğœ…ğ¶
Ëœğ¿
oğ‘¡
if ğ‘= 2
â„(x0)
2ğ‘¡
if ğ‘> 2, 1 â‰¤ğ‘¡â‰¤ğ‘¡0
( Ëœğ¿ğ‘/(ğœ…ğ¶)2)1/(ğ‘âˆ’2)
(1+(ğ‘âˆ’2) (ğ‘¡âˆ’ğ‘¡0)/(2ğ‘))ğ‘/(ğ‘âˆ’2) = O  ğ‘¡âˆ’ğ‘/(ğ‘âˆ’2)
if ğ‘> 2, ğ‘¡> ğ‘¡0,
20

for ğ‘¡â‰¥1, where:
ğ‘¡0 = max

1,

log1/2
 (Ëœğ¿ğ‘/(ğœ…ğ¶)2)1/(ğ‘âˆ’2)
â„(x0)

,
which completes the proof.
â–¡
Lastly, we deal with the general case in which the norm of the gradient is not bounded away from
zero in X.
Theorem A.10. Suppose X is a compact (ğœ…, ğ‘)-strongly convex set and ğ‘“is a (ğ‘€, ğœˆ) generalized self-
concordant function with ğœˆâ‰¥2 for which domain does not contain straight lines. Then the Frank-Wolfe
algorithm with Backtrack (Algorithm 5) results in a convergence:
â„ğ‘¡â‰¤
ï£±ï£´ï£´ï£²
ï£´ï£´ï£³
â„(x0)
2ğ‘¡
if 1 â‰¤ğ‘¡â‰¤ğ‘¡0
( Ëœğ¿ğ‘/(ğœ…2ğœ‡
L0
ğ‘“))1/(ğ‘âˆ’1)
(1+(ğ‘âˆ’1) (ğ‘¡âˆ’ğ‘¡0)/(2ğ‘))ğ‘/(ğ‘âˆ’1) = O  ğ‘¡âˆ’ğ‘/(ğ‘âˆ’1)
if ğ‘¡> ğ‘¡0,
for ğ‘¡â‰¥1, where:
ğ‘¡0 = max
ï£±ï£´ï£´ï£²
ï£´ï£´ï£³
1,
ï£¯ï£¯ï£¯ï£¯ï£¯ï£°
log1/2
Â©Â­
Â«
(Ëœğ¿ğ‘/(ğœ…2ğœ‡L0
ğ‘“))1/(ğ‘âˆ’1)
â„(x0)
ÂªÂ®
Â¬
ï£ºï£ºï£ºï£ºï£ºï£»
ï£¼ï£´ï£´ï£½
ï£´ï£´ï£¾
.
and Ëœğ¿
def= max{ğœğ¿L0
ğ‘“, ğ¿âˆ’1}, where ğœ> 1 is the backtracking parameter, ğ¿âˆ’1 is the initial smoothness esti-
mate in Algorithm 6, ğœ‡L0
ğ‘“
= minuâˆˆL0,dâˆˆâ„ğ‘›âˆ¥dâˆ¥2
âˆ‡2 ğ‘“(u) /âˆ¥dâˆ¥2
2 and ğ¿L0
ğ‘“
= maxuâˆˆL0,dâˆˆâ„ğ‘›âˆ¥dâˆ¥2
âˆ‡2 ğ‘“(u) /âˆ¥dâˆ¥2
2.
Proof. Consider the compact set L0
def= {x âˆˆdom( ğ‘“) âˆ©X | ğ‘“(x) â‰¤ğ‘“(x0)}. As the algorithm makes
monotonous primal progress we have that xğ‘¡âˆˆL0 for ğ‘¡â‰¥0. The proof proceeds very similarly as
before, except for the fact that now we have to bound âˆ¥âˆ‡ğ‘“(xğ‘¡)âˆ¥using ğœ‡L0
ğ‘“-strong convexity for points
xğ‘¡, xğ‘¡+ ğ›¾ğ‘¡(vğ‘¡âˆ’xğ‘¡) âˆˆL0. Continuing from Equation (A.13) for the case where ğ›¾ğ‘¡< 1 and using the
fact that â„(xğ‘¡) â‰¤âˆ¥âˆ‡ğ‘“(xğ‘¡)âˆ¥2 /(2ğœ‡L0
ğ‘“) we have that:
â„(xğ‘¡+1) â‰¤â„(xğ‘¡) âˆ’â„(xğ‘¡)2âˆ’2/ğ‘(ğœ…âˆ¥âˆ‡ğ‘“(xğ‘¡)âˆ¥)2/ğ‘
2ğ¿ğ‘¡
â‰¤â„(xğ‘¡) âˆ’â„(xğ‘¡)2âˆ’1/ğ‘ğœ…2/ğ‘(ğœ‡L0
ğ‘“)1/ğ‘21/ğ‘âˆ’1
Ëœğ¿
,
where we have also used the bound ğ¿ğ‘¡â‰¤Ëœğ¿in the last equation. This leads us to a contraction,
together with the case where ğ›¾ğ‘¡= 1, which is unchanged from the previous proofs, of the form:
â„(xğ‘¡) âˆ’â„(xğ‘¡+1) â‰¥â„(xğ‘¡) min
ï£±ï£´ï£²
ï£´ï£³
1
2,
ğœ…2/ğ‘(ğœ‡L0
ğ‘“)1/ğ‘21/ğ‘âˆ’1
Ëœğ¿
â„(xğ‘¡)1âˆ’1/ğ‘ï£¼ï£´ï£½
ï£´ï£¾
.
Using again Lemma A.8 to go from a contraction to a convergence rate for ğ‘> 2 we have that:
â„ğ‘¡â‰¤
ï£±ï£´ï£´ï£²
ï£´ï£´ï£³
â„(x0)
2ğ‘¡
if 1 â‰¤ğ‘¡â‰¤ğ‘¡0
( Ëœğ¿ğ‘/(ğœ…2ğœ‡
L0
ğ‘“))1/(ğ‘âˆ’1)
(1+(ğ‘âˆ’1) (ğ‘¡âˆ’ğ‘¡0)/(2ğ‘))ğ‘/(ğ‘âˆ’1) = O  ğ‘¡âˆ’ğ‘/(ğ‘âˆ’1)
if ğ‘¡> ğ‘¡0,
for ğ‘¡â‰¥1, where:
ğ‘¡0 = max
ï£±ï£´ï£´ï£²
ï£´ï£´ï£³
1,
ï£¯ï£¯ï£¯ï£¯ï£¯ï£°
log1/2
Â©Â­
Â«
(Ëœğ¿ğ‘/(ğœ…2ğœ‡L0
ğ‘“))1/(ğ‘âˆ’1)
â„(x0)
ÂªÂ®
Â¬
ï£ºï£ºï£ºï£ºï£ºï£»
ï£¼ï£´ï£´ï£½
ï£´ï£´ï£¾
,
which completes the proof.
â–¡
21

Appendix B. Away-step Frank-Wolfe
When the domain X is a polytope, one can obtain linear convergence in primal gap for a generalized-
self concordant function using the well known Away-step Frank-Wolfe (AFW) algorithm [GuÃ©lat &
Marcotte, 1986, Lacoste-Julien & Jaggi, 2015] with the adaptive step size of Pedregosa et al. [2020],
shown in Algorithm 7. We use Sğ‘¡to denote the active set at iteration ğ‘¡, that is, the set of vertices of
the polytope that gives rise to xğ‘¡as a convex combination with positive weights. We can see that the
algorithm either chooses to perform what is know as a Frank-Wolfe step in Line 6 of Algorithm 7 if
the Frank-Wolfe gap ğ‘”(x) is greater than the away gap âŸ¨âˆ‡ğ‘“(xğ‘¡), ağ‘¡âˆ’xğ‘¡âŸ©or an Away-step in Line 8 of
Algorithm 7 otherwise.
Algorithm 7 Away-step Frank-Wolfe with the step size of Pedregosa et al. [2020]
Input: Point x0 âˆˆX âˆ©dom( ğ‘“), function ğ‘“, initial smoothness estimate ğ¿âˆ’1
Output: Iterates x1, . . . âˆˆX
1: S0 â†{x0}, Î»0 â†{1}
2: for ğ‘¡= 0 to . . . do
3:
vğ‘¡â†argminvâˆˆX âŸ¨âˆ‡ğ‘“(xğ‘¡) , vâŸ©
4:
ağ‘¡â†argmaxvâˆˆSğ‘¡âŸ¨âˆ‡ğ‘“(xğ‘¡) , vâŸ©
5:
if âŸ¨âˆ‡ğ‘“(xğ‘¡), xğ‘¡âˆ’vğ‘¡âŸ©â‰¥âŸ¨âˆ‡ğ‘“(xğ‘¡), ağ‘¡âˆ’xğ‘¡âŸ©then
6:
dğ‘¡â†vğ‘¡âˆ’xğ‘¡, ğ›¾max â†1
7:
else
8:
dğ‘¡â†xğ‘¡âˆ’ağ‘¡, ğ›¾max â†Î»ğ‘¡(ağ‘¡)/(1 âˆ’Î»ğ‘¡(ağ‘¡))
9:
ğ›¾ğ‘¡, ğ¿ğ‘¡â†Backtrack( ğ‘“, xğ‘¡, dğ‘¡, âˆ‡ğ‘“(xğ‘¡), ğ¿ğ‘¡âˆ’1, ğ›¾max)
10:
xğ‘¡+1 â†xğ‘¡+ ğ›¾ğ‘¡dğ‘¡
11:
Update Sğ‘¡and Î»ğ‘¡to Sğ‘¡+1 and Î»ğ‘¡+1
The proof of linear convergence follows closely from Pedregosa et al. [2020] and Lacoste-Julien &
Jaggi [2015], with the only difference that we need to take into consideration that the function is
generalized self-concordant as opposed to smooth and strongly convex. One of the key inequalities
used in the proof is a scaling inequality from Lacoste-Julien & Jaggi [2015] very similar to the one
shown in Proposition A.4 and Proposition A.7, which we state next:
Proposition B.1. Let X âŠ†â„ğ‘›be a polytope, and denote by S the set of vertices of the polytope X that
gives rise to x âˆˆX as a convex combination with positive weights, then for all y âˆˆX:
âŸ¨âˆ‡ğ‘“(x), a âˆ’vâŸ©â‰¥ğ›¿âŸ¨âˆ‡ğ‘“(x), x âˆ’yâŸ©
âˆ¥x âˆ’yâˆ¥
,
where v = argminuâˆˆX âŸ¨âˆ‡ğ‘“(x), uâŸ©, a = argmaxuâˆˆS âŸ¨âˆ‡ğ‘“(x), uâŸ©, and ğ›¿> 0 is the pyramidal width of X.
Theorem B.2. Suppose X is a polytope and ğ‘“is a (ğ‘€, ğœˆ) generalized self-concordant function with ğœˆâ‰¥2
for which the domain does not contain straight lines. Then the Away-step Frank-Wolfe (AFW) algorithm
with Backtrack (Algorithm 7) results in a convergence:
â„(xğ‘¡) â‰¤â„(x0) Â©Â­
Â«
1 âˆ’
ğœ‡L0
ğ‘“
4Ëœğ¿
 ğ›¿
ğ·
2ÂªÂ®
Â¬
âŒˆ(ğ‘¡âˆ’1)/2âŒ‰
,
where ğ›¿is the pyramidal width of the polytope X, Ëœğ¿
def= max{ğœğ¿L0
ğ‘“, ğ¿âˆ’1}, ğœ> 1 is the backtracking
parameter, ğ¿âˆ’1 is the initial smoothness estimate in Algorithm 6, ğœ‡L0
ğ‘“
=
min
uâˆˆL0,dâˆˆâ„ğ‘›âˆ¥dâˆ¥2
âˆ‡2 ğ‘“(u) /âˆ¥dâˆ¥2
2 and
ğ¿L0
ğ‘“
=
max
uâˆˆL0,dâˆˆâ„ğ‘›âˆ¥dâˆ¥2
âˆ‡2 ğ‘“(u) /âˆ¥dâˆ¥2
2.
22

Proof. Proceeding very similarly as in the proof of Theorem A.5, we have that as the backtracking
line search makes monotonous primal progress, we know that for ğ‘¡â‰¥0 we will have that xğ‘¡âˆˆL0.
As the function is ğœ‡L0
ğ‘“-strongly convex over L0, we can use the appropriate inequalities from strong
convexity in the progress bounds. Using this aforementioned property, together with the scaling
inequality of Proposition B.1 results in:
ğ‘“(xğ‘¡) âˆ’ğ‘“(xâˆ—) â‰¤âŸ¨âˆ‡ğ‘“(xğ‘¡), xğ‘¡âˆ’xâˆ—âŸ©
2ğœ‡L0
ğ‘“
âˆ¥xğ‘¡âˆ’xâˆ—âˆ¥2
(B.1)
â‰¤âŸ¨âˆ‡ğ‘“(xğ‘¡), ağ‘¡âˆ’vğ‘¡âŸ©2
2ğœ‡L0
ğ‘“ğ›¿2
(B.2)
= (âŸ¨âˆ‡ğ‘“(xğ‘¡), ağ‘¡âˆ’xğ‘¡âŸ©+ âŸ¨âˆ‡ğ‘“(xğ‘¡), xğ‘¡âˆ’vğ‘¡âŸ©)2
2ğœ‡L0
ğ‘“ğ›¿2
,
(B.3)
where the ï¬rst inequality comes from the ğœ‡L0
ğ‘“-strong convexity over L0, and the second inequality
comes from applying Proposition B.1 with y = xâˆ—. Note that if the Frank-Wolfe step is chosen
in Line 6, then âˆ’âŸ¨âˆ‡ğ‘“(xğ‘¡), dğ‘¡âŸ©= âŸ¨âˆ‡ğ‘“(xğ‘¡), xğ‘¡âˆ’vğ‘¡âŸ©â‰¥âŸ¨âˆ‡ğ‘“(xğ‘¡), ağ‘¡âˆ’xğ‘¡âŸ©, otherwise, if an away step is
chosen in Line 8, then âŸ¨âˆ‡ğ‘“(xğ‘¡), xğ‘¡âˆ’vğ‘¡âŸ©< âŸ¨âˆ‡ğ‘“(xğ‘¡), ağ‘¡âˆ’xğ‘¡âŸ©= âˆ’âŸ¨âˆ‡ğ‘“(xğ‘¡), dğ‘¡âŸ©, in any case, plugging
into Equation (B.3) we have that:
â„(xğ‘¡) = ğ‘“(xğ‘¡) âˆ’ğ‘“(xâˆ—) â‰¤2 âŸ¨âˆ‡ğ‘“(xğ‘¡), dğ‘¡âŸ©2
ğœ‡L0
ğ‘“ğ›¿2
.
(B.4)
Note that using a similar reasoning, as âŸ¨âˆ‡ğ‘“(xğ‘¡), xğ‘¡âˆ’vğ‘¡âŸ©= ğ‘”(xğ‘¡), in both cases it holds that:
â„(xğ‘¡) â‰¤ğ‘”(xğ‘¡) â‰¤âˆ’âŸ¨âˆ‡ğ‘“(xğ‘¡), dğ‘¡âŸ©.
(B.5)
As in the preceding proofs, the backtracking line search in Algorithm 6 will either output a point
ğ›¾ğ‘¡= ğ›¾max or ğ›¾ğ‘¡< ğ›¾max. In any case, and regardless of if a Frank-Wolfe step (Line 6) or an away step
(Line 8) is chosen, Algorithm 6 will ï¬nd and output a smoothness estimate ğ¿ğ‘¡and a step size ğ›¾ğ‘¡such
that:
â„(xğ‘¡+1) âˆ’â„(xğ‘¡) â‰¤ğ¿ğ‘¡ğ›¾2
ğ‘¡
2
âˆ¥dğ‘¡âˆ¥2 + ğ›¾ğ‘¡âŸ¨âˆ‡ğ‘“(xğ‘¡), dğ‘¡âŸ©.
(B.6)
As before, we will have two different cases. If ğ›¾ğ‘¡= ğ›¾max we know by observing Line 5 of Algorithm 6
that âˆ’âŸ¨âˆ‡ğ‘“(xğ‘¡), dğ‘¡âŸ©â‰¥ğ›¾maxğ¿ğ‘¡âˆ¥dğ‘¡âˆ¥2, and so plugging into Equation (B.6) we arrive at â„(xğ‘¡+1) âˆ’â„(xğ‘¡) â‰¤
âŸ¨âˆ‡ğ‘“(xğ‘¡), dğ‘¡âŸ©ğ›¾max/2. In the case where ğ›¾ğ‘¡< ğ›¾max, we have that âˆ’âŸ¨âˆ‡ğ‘“(xğ‘¡), dğ‘¡âŸ©< ğ›¾maxğ¿ğ‘¡âˆ¥dğ‘¡âˆ¥2 and
ğ›¾ğ‘¡= âˆ’âŸ¨âˆ‡ğ‘“(xğ‘¡), dğ‘¡âŸ©/(ğ¿ğ‘¡âˆ¥dğ‘¡âˆ¥2), and so plugging into Equation (B.6) we arrive at â„(xğ‘¡+1) âˆ’â„(xğ‘¡) â‰¤
âˆ’âŸ¨âˆ‡ğ‘“(xğ‘¡), dğ‘¡âŸ©2 /(2ğ¿ğ‘¡âˆ¥dğ‘¡âˆ¥2). In any case, we can rewrite Equation (B.6) as:
â„(xğ‘¡) âˆ’â„(xğ‘¡+1) â‰¥min
(
âˆ’âŸ¨âˆ‡ğ‘“(xğ‘¡), dğ‘¡âŸ©ğ›¾max
2
, âŸ¨âˆ‡ğ‘“(xğ‘¡), dğ‘¡âŸ©2
2ğ¿ğ‘¡âˆ¥dğ‘¡âˆ¥2
)
.
(B.7)
We can now use the inequality in Equation (B.4) to bound the second term in the minimization
component of Equation (B.7), and Equation (B.5) to bound the ï¬rst term. This leads to:
â„(xğ‘¡) âˆ’â„(xğ‘¡+1) â‰¥â„(xğ‘¡) min
ï£±ï£´ï£²
ï£´ï£³
ğ›¾max
2
,
ğœ‡L0
ğ‘“ğ›¿2
4ğ¿ğ‘¡âˆ¥dğ‘¡âˆ¥2
ï£¼ï£´ï£½
ï£´ï£¾
(B.8)
â‰¥â„(xğ‘¡) min
ï£±ï£´ï£²
ï£´ï£³
ğ›¾max
2
,
ğœ‡L0
ğ‘“ğ›¿2
4Ëœğ¿ğ·2
ï£¼ï£´ï£½
ï£´ï£¾
.
(B.9)
23

where in the last inequality we have used âˆ¥ğ‘‘ğ‘¡âˆ¥â‰¤ğ·and ğ¿ğ‘¡â‰¤Ëœğ¿for all ğ‘¡. It remains to bound ğ›¾max
away from zero to obtain the linear convergence bound. For Frank-Wolfe steps we immediately have
ğ›¾max = 1, but for away steps there is no straightforward way of bounding ğ›¾max away from zero. One
of the key insights from Lacoste-Julien & Jaggi [2015] is that instead of bounding ğ›¾max away from
zero for all steps up to iteration ğ‘¡, we can instead bound the number of away steps with a step size
ğ›¾ğ‘¡= ğ›¾max up to iteration ğ‘¡, which are steps that reduce the cardinality of the active set Sğ‘¡and satisfy
â„(xğ‘¡) â‰¤â„(xğ‘¡+1). This leads us to consider only the progress provided by the remaining steps, which
are away steps with ğ›¾ğ‘¡< ğ›¾max, and Frank-Wolfe steps. For a number of steps ğ‘¡, only at most half of
these steps could have been away steps with ğ›¾ğ‘¡= ğ›¾max, as we cannot drop more vertices from the
active set than the number of vertices we could have potentially picked up with Frank-Wolfe steps.
For the remaining âŒˆ(ğ‘¡âˆ’1)/2âŒ‰steps we know that â„(xğ‘¡) âˆ’â„(xğ‘¡+1) â‰¥â„(xğ‘¡)ğœ‡L0
ğ‘“ğ›¿2/(4Ëœğ¿ğ·2). Therefore
we have that the primal gap satisï¬es:
â„(xğ‘¡) â‰¤â„(x0) Â©Â­
Â«
1 âˆ’
ğœ‡L0
ğ‘“ğ›¿2
4Ëœğ¿ğ·2
ÂªÂ®
Â¬
âŒˆ(ğ‘¡âˆ’1)/2âŒ‰
.
This completes the proof.
â–¡
We can make use of the proof of convergence in primal gap to prove linear convergence in Frank-Wolfe
gap. In order to do so, we recall a quantity formally deï¬ned in Kerdreux et al. [2019] but already
implicitly used earlier in Lacoste-Julien & Jaggi [2015] as:
ğ‘¤(xğ‘¡, Sğ‘¡)
def=
max
uâˆˆSğ‘¡,vâˆˆX âŸ¨âˆ‡ğ‘“(xğ‘¡), u âˆ’vâŸ©
= max
uâˆˆSğ‘¡
âŸ¨âˆ‡ğ‘“(xğ‘¡), u âˆ’xğ‘¡âŸ©+ max
vâˆˆX âŸ¨âˆ‡ğ‘“(xğ‘¡), xğ‘¡âˆ’vâŸ©
= max
uâˆˆSğ‘¡
âŸ¨âˆ‡ğ‘“(xğ‘¡), u âˆ’xğ‘¡âŸ©+ ğ‘”(xğ‘¡).
Note that as the ï¬rst term, the so-called away gap in the previous equation is positive and hence
ğ‘¤(xğ‘¡, Sğ‘¡) provides an upper bound on the Frank-Wolfe gap.
Theorem B.3. Suppose X is a polytope and ğ‘“is a (ğ‘€, ğœˆ) generalized self-concordant function with
ğœˆâ‰¥2 for which the domain does not contain straight lines. Then the Away-step Frank-Wolfe (AFW)
algorithm with Backtrack (Algorithm 7) contracts the Frank-Wolfe gap linearly, i.e., min1â‰¤ğ‘¡â‰¤ğ‘‡ğ‘”(xğ‘¡) â‰¤ğœ€
after ğ‘‡= O(log 1/ğœ€) iterations.
Proof. Note that the condition in Line 5 of Algorithm 7 means that regardless of if we chose to
perform an away step of a Frank-Wolfe step, we have that âˆ’2 âŸ¨âˆ‡ğ‘“(xğ‘¡), dğ‘¡âŸ©â‰¥âŸ¨âˆ‡ğ‘“(xğ‘¡), xğ‘¡âˆ’vğ‘¡âŸ©+
âŸ¨âˆ‡ğ‘“(xğ‘¡), ağ‘¡âˆ’xğ‘¡âŸ©= ğ‘¤(xğ‘¡, Sğ‘¡). On the other hand, we also have that â„(xğ‘¡) âˆ’â„(xğ‘¡+1) â‰¤â„(xğ‘¡). Plugging
these bounds into the right-hand side and the left hand side of Equation B.7 in Theorem B.2, and
using the fact that âˆ¥dğ‘¡âˆ¥â‰¤ğ·we have that:
min
 ğ‘¤(xğ‘¡, Sğ‘¡)ğ›¾max
4
, ğ‘¤(xğ‘¡, Sğ‘¡)2
8ğ¿ğ‘¡ğ·2

â‰¤â„(xğ‘¡) â‰¤â„(x0) Â©Â­
Â«
1 âˆ’
ğœ‡L0
ğ‘“
4Ëœğ¿
 ğ›¿
ğ·
2ÂªÂ®
Â¬
âŒˆ(ğ‘¡âˆ’1)/2âŒ‰
,
where the second inequality follows from the convergence bound on the primal gap from Theorem B.2.
Considering the steps that are not away steps with ğ›¾ğ‘¡= ğ›¾max as in the proof of Theorem B.2, leads us
to:
ğ‘”(xğ‘¡) â‰¤ğ‘¤(xğ‘¡, Sğ‘¡) â‰¤4â„(x0) max
ï£±ï£´ï£´ï£²
ï£´ï£´ï£³
1,
âˆšï¸„
Ëœğ¿ğ·2
2â„(x0)
ï£¼ï£´ï£´ï£½
ï£´ï£´ï£¾
Â©Â­
Â«
1 âˆ’
ğœ‡L0
ğ‘“
4Ëœğ¿
 ğ›¿
ğ·
2ÂªÂ®
Â¬
âŒŠ(ğ‘¡âˆ’1)/4âŒ‹
.
â–¡
24

Appendix C. Remarks and supplementary information on the experimental
section
We ran all experiments on a server with 8 Intel Xeon 3.50GHz CPUs and 31GB RAM. All computations
are run in single-threaded mode using Julia 1.6.0 with the FrankWolfe.jl package. We provide the
full details of the experiments carried out in the paper:
Portfolio optimization.
We consider ğ‘“(x) = âˆ’Ãğ‘
ğ‘¡=1 log(âŸ¨rğ‘¡, xâŸ©), where ğ‘denotes the number of
periods and X = Î”ğ‘›. The results are shown in Figure 2. We use the revenue data rğ‘¡from Dvurechensky
et al. [2020b] and add instances generated in a similar fashion from independent Normal random
entries with 1000, 2000, and 5000 dimensions, and from a Log-normal distribution with (ğœ‡= 0.0, ğœ=
0.5).
Signal recovery with KL divergence.
We apply the aforementioned algorithms to the recovery
of a sparse signal from a noisy linear image using the Kullback-Leibler divergence. Given a linear
map ğ‘Š, we assume a signal y is generated by y = ğ‘Šx0 + ğœ€, where x0 is assumed to be a sparse
unknown input signal and ğœ€is a random error. Assuming ğ‘Šand y are entrywise positive, and that the
signal to recover should also be entrywise positive, the minimizer of the KL divergence (or Kullbackâ€™s
I-divergence [Csiszar et al., 1991]) can be used as an estimator for x0. The KL divergence between the
resulting output signals is expressed as ğ‘“(x) = ğ·(ğ‘Šx, y) = Ãğ‘
ğ‘–=1
n
âŸ¨wğ‘–, xâŸ©log

âŸ¨wğ‘–,xâŸ©
ğ‘¦ğ‘–

âˆ’âŸ¨wğ‘–, xâŸ©+ ğ‘¦ğ‘–
o
,
where wğ‘–is the ğ‘–th row of ğ‘Š. In order to promote sparsity and enforce nonnegativity of the solution,
we use the unit simplex of radius ğ‘…as the feasible set X = {x âˆˆâ„ğ‘‘
+, âˆ¥xâˆ¥1 â‰¤ğ‘…}. The results are
shown in Figure 3. We used the same ğ‘€= 1 choice for the second-order method as in Dvurechensky
et al. [2020b] for comparison; whether this choice is admissible is unknown (see Remark C.1). We
generate input signals x0 with 30% non-zeros elements following an exponential distribution of mean
ğœ†= 1. The entries of ğ‘Šare generated from a folded Normal distribution built from absolute values of
Gaussian random numbers with standard deviation 5 and mean 0. The additive noise is generated
from a Gaussian centered distribution with a standard deviation equal to a fraction of the standard
deviation of ğ‘Šx0.
Logistic regression.
One of the motivating examples for the development of a theory of generalized
self-concordant function is the logistic regression problem, as it does not match the deï¬nition of a
standard self-concordant function but shares many of its characteristics. We consider a design matrix
with rows ağ‘–âˆˆâ„ğ‘›with 1 â‰¤ğ‘–â‰¤ğ‘and a vector y âˆˆ{âˆ’1, 1}ğ‘and formulate a logistic regression
problem with elastic net regularization, in a similar fashion as is done in Liu et al. [2020], with
ğ‘“(x) = 1/ğ‘Ãğ‘
ğ‘–=1 log(1 + exp(âˆ’ğ‘¦ğ‘–âŸ¨x, ağ‘–âŸ©)) + ğœ‡/2 âˆ¥xâˆ¥2, and X is the â„“1 ball of radius ğœŒ, where ğœ‡and ğœŒ
are two regularization parameters. The logistic regression loss is generalized self-concordant with
ğœˆ= 2. The results can be seen in Figure 4 and expanded in Appendix C. We use the a1a-a9a datasets
from the LIBSVM classiï¬cation data.
Birkhoff polytope.
All applications previously considered all have in common a constraint set
possessing computationally inexpensive LMOs (probability or unit simplex and â„“1 norm ball). Addi-
tionally, each vertex returned from the LMO is highly sparse with at most one non-zero element. To
complement the results we consider a problem over the Birkhoff polytope, the polytope of doubly
stochastic matrices, where the LMO is implemented through the Hungarian algorithm, and is not
as inexpensive as in the other examples considered. We use a quadratic regularization parameter
ğœ‡= 100/âˆšğ‘›where ğ‘›is the number of samples.
Remark C.1. Note that Proposition 2 in Sun & Tran-Dinh [2019], which deals with the composition of
generalized self-concordant functions with afï¬ne maps, does not apply to the KL divergence objective
25

function, reproduced here for reference:
ğ‘“(x) = ğ·(ğ‘Šx, y) =
ğ‘
âˆ‘ï¸
ğ‘–=1

âŸ¨wğ‘–, xâŸ©log
 âŸ¨wğ‘–, xâŸ©
ğ‘¦ğ‘–

âˆ’âŸ¨wğ‘–, xâŸ©+ ğ‘¦ğ‘–

.
Furthermore, the objective function is strongly convex if and only if rank(ğ‘Š) â‰¥ğ‘›, where ğ‘›is the
dimension of the problem.
Proof. [Sun & Tran-Dinh, 2019, Proposition 2] establishes certain conditions under which the
composition of a generalized self-concordant function with an afï¬ne map results in a generalized
self-concordant function. The objective is of the form:
ğ‘
âˆ‘ï¸
ğ‘–=1
ğœ™ğ‘–(âŸ¨wğ‘–, xâŸ©)
with
ğœ™ğ‘–(ğ‘¡) = ğ‘¡log
 ğ‘¡
ğ‘¦ğ‘–

âˆ’ğ‘¡+ ğ‘¦ğ‘–= ğ‘¡log ğ‘¡âˆ’ğ‘¡log ğ‘¦ğ‘–âˆ’ğ‘¡+ ğ‘¦ğ‘–.
Note that generalized self-concordant functions are closed under addition, and so we only focus on
the individual terms in the sum. As ï¬rst-order terms are (0, ğœˆ)-generalized self-concordant for any
ğœˆ> 0, then we know that the composition of these ï¬rst-order terms with an afï¬ne map results in a
generalized self-concordant function Sun & Tran-Dinh [2019, Proposition 2]. We therefore focus on
the entropy function ğ‘¡log ğ‘¡which is (1, 4) generalized self-concordant. The conditions which ensure
that the composition of a (ğ‘€, ğœˆ)-generalized self-concordant function with an afï¬ne map ğ‘¥â†¦â†’ğ´ğ‘¥
results in a generalized self-concordant function requires in the case ğœˆ> 3 that ğœ†ğ‘šğ‘–ğ‘›(ğ´ğ‘‡ğ´) > 0 [Sun &
Tran-Dinh, 2019, Proposition 2]. In the case of the KL divergence objective, ğ´= wğ‘‡
ğ‘–and ğ´ğ‘‡ğ´= wğ‘–wğ‘‡
ğ‘–
is an outer product with only one positive eigenvalue, and 0 of multiplicity ğ‘›âˆ’1. Therefore we cannot
guarantee that the function ğœ™ğ‘–(âŸ¨wğ‘–, xâŸ©) is generalized self-concordant by application of Proposition 2
in Sun & Tran-Dinh [2019].
Alternatively, in order to try to show that the function is generalized self-concordant we could consider
ğ‘“(x) := ğ‘”(ğ‘Šx). Assuming rank(ğ‘Š) â‰¥ğ‘›, then ğ‘Šğ‘‡ğ‘Šis positive deï¬nite, and only the generalized
self-concordance of ğ‘”is left to prove.
ğ‘”(z) =
ğ‘
âˆ‘ï¸
ğ‘–=1
ğœ™ğ‘–(zğ‘–).
Each term z â†¦â†’ğœ™ğ‘–(zğ‘–) = ğœ™ğ‘–(eğ‘‡
ğ‘–z) with eğ‘–the ğ‘–th standard basis vector is the composition of a
generalized self-concordant function composed with a rank-one afï¬ne transformation, this raises the
same issues encountered in the paragraph above.
Regarding the strong-convexity of the objective function, we can express the gradient and the Hessian
of the function as:
âˆ‡ğ‘“(x) =
ğ‘
âˆ‘ï¸
ğ‘–=1
wğ‘–(log âŸ¨wğ‘–, xâŸ©âˆ’log ğ‘¦ğ‘–)
âˆ‡2 ğ‘“(x) =
ğ‘
âˆ‘ï¸
ğ‘–=1
wğ‘–wğ‘‡
ğ‘–
âŸ¨wğ‘–, xâŸ©,
which is the sum of ğ‘outer products, each corresponding to a single eigenvector wğ‘–. If rank(ğ‘Š) â‰¥ğ‘›,
the Hessian is deï¬nite positive and the objective is strongly convex. Otherwise, it possesses zero as
an eigenvalue regardless of x, and the function Hessian is positive semi-deï¬nite.
â–¡
26

Strong convexity parameter for the LLOO.
The LLOO procedure explicitly requires a strong
convexity parameter ğœğ‘“of the objective function, an underestimator of ğœ†ğ‘šğ‘–ğ‘›(âˆ‡2 ğ‘“(x)) over X. For the
portfolio optimization problem, the Hessian is a sum of rank-one terms:
âˆ‡2 ğ‘“(x) =
âˆ‘ï¸
ğ‘¡
rğ‘¡rğ‘‡
ğ‘¡
âŸ¨rğ‘¡, xâŸ©.
The only non-zero eigenvalue associated with each ğ‘¡term is bounded below over X by:
âˆ¥rğ‘¡âˆ¥2
maxxâˆˆX âŸ¨rğ‘¡, xâŸ©2 =
âˆ¥rğ‘¡âˆ¥2
max{maxxâˆˆX âŸ¨rğ‘¡, xâŸ©, âˆ’minxâˆˆX âŸ¨rğ‘¡, xâŸ©}2 .
The denominator can be solved by two calls to the LMO, and we will denote it by ğ›½ğ‘¡for the ğ‘¡th term.
Each summation term contributes positively to one of the eigenvalues of the Hessian matrix, an
underestimator of the the strong convexity parameter is then given by:
ğœ†ğ‘šğ‘–ğ‘›
 âˆ‘ï¸
ğ‘¡
rğ‘¡rğ‘‡
ğ‘¡
ğ›½ğ‘¡
!
.
The second-order method GSC-FW has been implemented with an in-place Hessian matrix updated
at each iteration, following the implementation of Dvurechensky et al. [2020b]. The Hessian
computation nonetheless adds signiï¬cant cost in the runtime of each iteration, even if the local
norm and other quadratic expressions

âˆ‡2 ğ‘“(x)y, z
 can be computed allocation-free. A potential
improvement for future work would be to represent Hessian matrices as functional linear operators
mapping any y to âˆ‡2 ğ‘“(x)y.
Monotonous step size: the numerical case.
The computational experiments highlighted that the
Monotonous Frank-Wolfe performs well in terms of iteration count and time against other Frank-Wolfe
and Away-step Frank-Wolfe variants. Another advantage of a simple step size computation procedure
is its numerical stability. On some instances, an ill-conditioned gradient can lead to a plateau of the
primal and/or dual progress. Even worse, some step-size strategies do not guarantee monotonicity
and can result in the primal value increasing over some iterations. The numerical issue that causes
this phenomenon is illustrated by running the methods of the FrankWolfe.jl package over the same
instance using 64-bits ï¬‚oating-point numbers and Julia BigFloat types (which support arithmetic in
arbitrary precision to remove numerical issues).
100
101
102
103
Iteration
10âˆ’11
10âˆ’7
10âˆ’3
101
â„(xí‘¡)
10âˆ’2
100
Time [s]
100
101
102
103
Iteration
10âˆ’5
10âˆ’3
10âˆ’1
101
í‘”(xí‘¡)
10âˆ’2
100
Time [s]
M-FW
FW
B-FW
B-AFW
Figure 6: Ill-conditioned portfolio optimization problem solved using ï¬‚oating-point arithmetic.
On Figure 6, we observe a plateau of the dual gap for both M-FW and B-AFW. The primal value however
worsens after the iteration where B-AFW reaches its dual gap plateau. In contrast, M-FW reaches a
plateau in both primal and dual gap at a certain iteration. Note that the primal value at the point
where the plateau is hit is already below âˆšğœ€ï¬‚oat64, the square root of the machine precision. The same
instance and methods operating in arbitrary precision arithmetic are presented Figure 7. Instead
of reaching a plateau or deteriorating, B-AFW closes the dual gap tolerance and terminates before
27

100
101
102
103
Iteration
10âˆ’11
10âˆ’7
10âˆ’3
101
â„(xí‘¡)
100
101
102
103
Time [s]
100
101
102
103
Iteration
10âˆ’6
10âˆ’4
10âˆ’2
100
í‘”(xí‘¡)
100
101
102
103
Time [s]
M-FW
FW
B-FW
B-AFW
Figure 7: Ill-conditioned portfolio optimization problem solved using arbitrary precision.
other methods. Although this observation (made on several instances of the portfolio optimization
problem) only impacts ill-conditioned problems, it suggests M-FW may be a good candidate for a
numerically robust default implementation of Frank-Wolfe algorithms.
Function domain in the constraints
One of the arguments used to motivate the construction of FW
algorithms for standard and generalized self-concordant minimization in prior work is the difï¬culty
of handling objective functions with implicitly deï¬ned domains. We make several observations that
highlight the relevance of this issue and justify the assumption of the availability of a Domain Oracle
for dom( ğ‘“). In the portfolio optimization example, all revenue vectors are assumed positive, rğ‘¡âˆˆâ„ğ‘›
++
and x âˆˆÎ”ğ‘›, it follows that all feasible points lie in dom( ğ‘“). More generally ,for the logarithm of an
afï¬ne function, verifying that a candidate x lies in dom( ğ‘“) consists of a single afï¬ne transformation
and element-wise comparison ğ´x + b > â„ğ‘š
++.
In the inverse covariance estimation problem, the information on dom( ğ‘“) can be added to the
constraints by imposing mat(x) âˆˆğ•Šğ‘›
+, yielding a semi-deï¬nite optimization problem. The domain
oracle consists of the computation of the smallest eigenvalue, which needs to be positive.
We can also modify the feasible region of the signal retrieval application using the KL divergence,
resulting in a new feasible region Xâ€² so that Int (Xâ€²) âŠ‚dom( ğ‘“). The objective is of the form:
ğ‘“(ğœƒ) =
âˆ‘ï¸
ğ‘–
âŸ¨wğ‘–, ğœƒâŸ©log
 âŸ¨wğ‘–, ğœƒâŸ©
ğ‘¦ğ‘–

âˆ’âŸ¨wğ‘–, ğœƒâŸ©
where the data ğ‘Šand ğ‘¦are assumed to be entrywise positive, thus dom( ğ‘“) = {ğ‘¥âˆˆâ„ğ‘›
+, ğ‘¥â‰ 0}.
Therefore we can deï¬ne the set Xâ€² as the unit simplex. The domain of each function involved in the
sum in ğ‘“has an open domain (0, +âˆ).
However, the positivity assumption on all these components could be relaxed. Without the positivity
assumption on ğ‘Š, the Domain Oracle would consist of verifying:
âŸ¨wğ‘–, ğœƒâŸ©> 0 âˆ€ğ‘–.
(C.1)
This veriï¬cation can however be simpliï¬ed by a preprocessing step if the number of data points is
large by ï¬nding the minimal set of supporting hyperplanes in the polyhedral cone (C.1), which we
can ï¬nd by solving the following linear problem:
max
ğœ,ğœƒğœ
(C.2a)
s.t. ğœâ‰¤âŸ¨wğ‘–, ğœƒâŸ©âˆ€ğ‘–(ğœ†ğ‘–)
(C.2b)
âˆ¥ğœƒâˆ¥1 â‰¤ğ‘…,
(C.2c)
where ğœ†ğ‘–is the dual variable associated with the ğ‘–th inner product constraint. If the optimal solution
of Problem (C.2a) is 0, the original problem is infeasible and the cone deï¬ned by (C.1) is empty.
Otherwise the optimal ğœƒwill lie in the intersection of the closure of the polyhedral cone and the
28

â„“1 norm ball. Furthermore, the support of ğœ†provides us with the non-redundant inequalities of
the cone. Let Ë†ğ‘Šbe the matrix formed with the rows wğ‘–such that ğœ†ğ‘–> 0, then the Domain Oracle
can be simpliï¬ed to the veriï¬cation that Ë†ğ‘Šğœƒâˆˆâ„ğ‘›
++. The Distance Weighted Discrimination (DWT)
model also considered in Dvurechensky et al. [2020b] was initially presented in Marron et al. [2007],
the denominator of each sum element ğœ‰ğ‘–is initially constrained to be nonnegative, which makes
Int (X) âŠ†dom( ğ‘“) hold. Even without this additional constraint, the nonnegativity of all ğœ‰ğ‘–can be
ensured with a minimum set of linear constraints in a fashion similar to the signal retrieval application,
thus simplifying the Domain Oracle.
29

100
101
102
103
Iteration
10âˆ’10
10âˆ’6
10âˆ’2
â„(xí‘¡)
10âˆ’1
101
103
Time [s]
100
101
102
103
Iteration
10âˆ’5
10âˆ’3
10âˆ’1
101
í‘”(xí‘¡)
10âˆ’1
101
103
Time [s]
M-FW
FW
B-FW
B-AFW
GSC-FW
LLOO
Figure 8: Portfolio Optimization: Convergence of â„(xğ‘¡) and ğ‘”(xğ‘¡) vs. ğ‘¡and wall-clock time.
30

100
101
102
103
Iteration
10âˆ’11
10âˆ’7
10âˆ’3
101
â„(xí‘¡)
10âˆ’2
100
Time [s]
100
101
102
103
Iteration
10âˆ’5
10âˆ’3
10âˆ’1
101
í‘”(xí‘¡)
10âˆ’2
100
Time [s]
M-FW
FW
B-FW
B-AFW
Figure 9: Portfolio Optimization: Convergence of â„(xğ‘¡) and ğ‘”(xğ‘¡) vs. ğ‘¡and wall-clock time.
31

100
101
102
103
Iteration
10âˆ’11
10âˆ’7
10âˆ’3
101
â„(xí‘¡)
100
101
102
103
Time [s]
100
101
102
103
Iteration
10âˆ’6
10âˆ’4
10âˆ’2
100
í‘”(xí‘¡)
100
101
102
103
Time [s]
M-FW
FW
B-FW
B-AFW
Figure 10: Portfolio Optimization: Convergence of â„(xğ‘¡) and ğ‘”(xğ‘¡) vs. ğ‘¡and wall-clock time.
32

100
101
102
103
Iteration
10âˆ’3
100
103
â„(xí‘¡)
10âˆ’1
101
103
Time [s]
100
101
102
103
Iteration
101
103
105
í‘”(xí‘¡)
10âˆ’1
101
103
Time [s]
M-FW
FW
B-FW
B-AFW
GSC-FW
Figure 11: Signal Recovery: Convergence of â„(xğ‘¡) and ğ‘”(xğ‘¡) vs. ğ‘¡and wall-clock time.
33

100
102
104
Iteration
10âˆ’11
10âˆ’8
10âˆ’5
10âˆ’2
â„(xí‘¡)
10âˆ’1
101
103
Time [s]
100
102
104
Iteration
10âˆ’6
10âˆ’4
10âˆ’2
100
í‘”(xí‘¡)
M-FW
FW
B-FW
B-AFW
GSC-FW
10âˆ’1
101
103
Time [s]
Figure 12: Logistic Regression: Convergence of â„(xğ‘¡) and ğ‘”(xğ‘¡) vs. ğ‘¡and wall-clock time for the
a4a LIBSVM dataset.
34

100
102
104
Iteration
10âˆ’11
10âˆ’7
10âˆ’3
â„(xí‘¡)
10âˆ’1
100
101
102
Time [s]
100
102
104
Iteration
10âˆ’6
10âˆ’4
10âˆ’2
100
í‘”(xí‘¡)
10âˆ’1
100
101
102
Time [s]
FW
M-FW
B-FW
B-AFW
Figure 13: Logistic Regression: Convergence of â„(xğ‘¡) and ğ‘”(xğ‘¡) vs. ğ‘¡and wall-clock time for the
a8a LIBSVM dataset.
35

100
102
104
Iteration
10âˆ’11
10âˆ’7
10âˆ’3
101
â„(xí‘¡)
10âˆ’1
101
Time [s]
100
102
104
Iteration
10âˆ’6
10âˆ’4
10âˆ’2
100
í‘”(xí‘¡)
10âˆ’1
101
Time [s]
M-FW
FW
B-FW
B-AFW
GSC-FW
Figure 14: Birkhoff Polytope: Convergence of â„(xğ‘¡) and ğ‘”(xğ‘¡) vs. ğ‘¡and wall-clock time.
36

