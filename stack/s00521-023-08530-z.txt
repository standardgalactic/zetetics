ORIGINAL ARTICLE
InvolutionGAN: lightweight GAN with involution for unsupervised
image-to-image translation
Haipeng Deng1 • Qiuxia Wu1
• Han Huang1 • Xiaowei Yang1 • Zhiyong Wang2
Received: 17 August 2022 / Accepted: 21 March 2023
 The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature 2023
Abstract
The unsupervised image-to-image translation aims to learn a mapping that translates images from one domain to the target
domain. Current state-of-the-art generative adversarial network (GAN) models utilize time and space-costly operators to
produce impressive translated images. However, further research and model deployment are under restrictions due to the
high computational costs of the models. In order to resolve the problem, we enhance the GAN structure by employing a
lightweight operator named involution that facilitates extracting both local features and long-range dependencies across
channels. Besides, we also notice that previous works attach less importance to feature-level reconstruction discrepancy
between original and reconstructed images. Nevertheless, such information is crucial in improving the quality of the
synthesized images. Thus, we develop a novel loss term that evaluates the learned perceptual similarity distance to regulate
the training process. The qualitative and quantitative experiment results on several prevailing benchmarks demonstrate that
our model, dubbed InvolutionGAN, could produce competitive image results while saving computational costs up to
91.9%. In addition, extensive ablation studies are conducted to search for the best model structure and verify that each
component we introduced is effective.
Keywords Unsupervised image-to-image translation  Image style transfer  Generative adversarial networks 
Deep learning
1 Introduction
Image-to-image (I2I) translation is the task of mapping
images from a source domain to the target domain. A wide
range of applications could be found in the ﬁeld of
computer vision, including colourization [5, 16, 47], image
completion [44], and image conversion [35, 43]. Given that
paired images are offered, several earlier supervised
approaches [17, 40, 42] successfully generate faithful
output in a supervised manner. On the contrary, some prior
work, such as CycleGAN [50], proposed an unsupervised
network architecture with cycle-consistency loss, over-
coming the problem of paired samples that are either
expensive to obtain or do not exist.
Despite these advances, previous I2I translation GANs
fail to address a few limitations. The ﬁrst one is the deﬁ-
ciency in feature extraction. Since convolution is proven
successful in many computer vision tasks, most former
models [23, 46, 50] are built only based on convolution
blocks. Though convolution enjoys remarkable spatial-ag-
nostic and channel-speciﬁc properties, the operator could
not comprehensively capture long-range spatial features.
To cover this shortage, multiple latest I2I translation
methods combine more powerful operators in their GAN
models. For instance, the attention mechanism is utilized in
& Qiuxia Wu
qxwu@scut.edu.cn
Haipeng Deng
mehpdeng@mail.scut.edu.cn
Han Huang
hhan@scut.edu.cn
Xiaowei Yang
xwyang@scut.edu.cn
Zhiyong Wang
zhiyong.wang@sydney.edu.au
1
School of Software Engineering, South China University of
Technology, Guangzhou, Guangdong Province, China
2
School of Computer Science, The University of Sydney,
Camperdown, NSW, Australia
123
Neural Computing and Applications
https://doi.org/10.1007/s00521-023-08530-z
(0123456789().,-volV)(0123456789().,-volV)

several approaches [10, 21, 33] to enforce the network
concentrating on spatial information. However, we start to
rethink whether there are better choices when the size of
GAN is much larger than modern recognition CNNs after
attention is introduced.
Due to the above-mentioned problem, the second limi-
tation comes as the extra training expense introduced by
attention and other operators. The growing computational
cost of GAN’s components brings challenges to relative
researchers and restrains further application of the model.
After investigating some recent unsupervised I2I transla-
tion models, Fig. 1 shows that the number of trainable
parameters is relatively huge since attention is introduced
[6, 22]. In order to reduce parameters for GAN, Li et al.
[29] and Aguinaldo et al. [1] leverage knowledge distilla-
tion. Their strategies signiﬁcantly reduce trainable param-
eters; nonetheless, signiﬁcant performance degradation
inevitably occurs.
To search for approaches that could tackle the two
restrictions, we could utilize some lightweight operators
that could capture long-range spatial dependencies. In [28],
a novel atomic operation for deep neural networks coined
as involution is proposed. The involution-based model
outperforms its convolution-based counterparts in several
computer vision tasks for its excellence in global feature
extraction. Moreover, the operator could practically com-
press computational costs. Thus, we are inspired to further
explore the effectiveness of involution in unsupervised I2I
translation GAN.
The third issue is that prior works pay less attention to
the perceptual discrepancy in the training process. Most
current image-transferring GAN model leverage pixel-level
image reconstruction loss, namely cycle-consistency loss
and identity reconstruction loss [50]. However, imagine if
the reconstructed image is the same as the original one but
offsets just one pixel, pixel-wise reconstruction loss would
not faithfully represent the difference between the two
images. Encouraging generated and original images to have
similar feature representations is proven useful in other
tasks based on GAN [19]. Therefore, introducing percep-
tual optimization for unsupervised I2I translation is nec-
essary and practicable to obtain better results. Perceptual-
DualGAN [37] is the ﬁrst to leverage perceptual loss in an
unsupervised I2I model to enhance the perceptual quality
of output images. However, there are still some inade-
quacies as it utilizes the naive perceptual loss function to
evaluate the feature-level difference and discards pixel-
wise losses in training. Hence, investigating the validity of
reconstruction loss is another practical way to improve the
performance of the unsupervised I2I translation model.
Upon these motivations, we present a lightweight GAN
generator model which exploits the beneﬁts of the invo-
lution operator in this paper. In particular, we improve the
bottleneck module of the autoencoder model, enabling it to
process both local and long-range features of the source
images. After such transformation, our simple architecture
reduces trainable parameters in terms of the size of the
network, allowing devices with smaller memory to run our
model. Furthermore, we propose a new loss term, LPIPS
perceptual loss, which calculates the learned perceptual
image patch similarity (LPIPS) distance between original
and cycle-reconstructed and identity-reconstructed images.
In consideration of the vanishing gradient and unsta-
ble training problems, we make use of the least-square
adversarial loss [32]. Complementary to our proposed
LPIPS perceptual loss, we still adopt the traditional pixel-
wise reconstruction loss to regulate training. The main
contributions of our work are summarized as follows:
•
We enhance the generative adversarial network model
for unsupervised I2I translation. Unlike conventional
convolution-based GAN methods, we also utilize
involution operators to reduce computational costs
while facilitating extracting of both local features and
long-range dependencies simultaneously.
•
We introduce an objective function for unsupervised I2I
translation, which could calculate feature reconstruction
loss. The proposed loss term is complementary to
traditional pixel-level losses, encouraging the networks
to synthesize images of higher quality.
•
Compared with existing state-of-the-art counterparts,
our qualitative and quantitative experiments reveal
competitive performance while reducing a high ratio
of computational costs. In addition, ablation studies are
conducted to verify that our enhanced structure and
proposed loss are efﬁcacious.
The remaining sections of this paper are organized as fol-
lows: Sect. 2 gives a brief review of the literature sur-
rounding
generative
adversarial
network
and
I2I
Fig. 1 State-of-the-art unsupervised I2I translation models are
requiring more computational cost, making them more challenging
to be trained
Neural Computing and Applications
123

translation. In section 3, we introduce our proposed Invo-
lutionGAN in detail. Section 4 contains qualitative and
quantitative comparisons with the state-of-the-art models
and includes extensive ablation studies. Finally, we con-
clude our work in Sect. 5.
2 Related Work
With the development of generative adversarial networks, a
number of GAN-based methods have been proposed for
unsupervised I2I translation. In this section, we ﬁrst brieﬂy
review GAN and then introduce the literature surrounding
I2I translation.
2.1 Generative Adversarial Networks
As one of the most imaginative neuron network models,
GANs [11] offer an effective way to synthesize vivid
images. A classical GAN consists of two fundamental
parts: the generator and the discriminator. The core of the
GAN algorithm is like a min-max game: while the gener-
ator tries its best to map random noise vectors to realistic
images, the discriminator learns to distinguish whether the
input image is from the training dataset or fabricated by the
generator.
A problem with primal GAN is that it could only gen-
erate low-quality images. To enhance the quality of the
generated images, Deep Convolutional Generative Adver-
sarial Network (DCGAN) [38] ﬁrst replaces Multi-layer
Perceptron (MLP) with convolutional layers. After the
success of DCGAN, most current GANs are at least loosely
based on the DCGANs architecture. For example, Info-
GAN [7] further regulates image synthesis by decomposing
the input noise vector into two parts: an incompressible
noise and the latent code, which targets the signiﬁcant
structured semantic features of the real data distribution.
Another issue of the primordial GAN is how to generate
outputs of a speciﬁc label. In order to control image gen-
eration, Conditional Generative Adversarial Networks
(cGAN) [34] append some extra information to both the
discriminator and generator. Based on the given informa-
tion, cGANs could generate samples conditioning on class
labels, text, bounding box and key points.
In the training aspect, the training of GAN suffers from
instability. Wasserstein GAN (WGAN) [2, 12] and loss-
sensitive GAN [36] utilize novel loss functions to deal with
the problem and alleviate the mode collapse problem at the
same time. Additionally, the evolutionary multi-objective
cyclic GAN (EMOCGAN) [3] explores an evolutionary
setting for I2I translation.
In this paper, we leverage the GAN model to perform
the task of unsupervised I2I translation. In addition, we
propose a new feature-level objective function to improve
the quality of the synthesized images.
2.2 Image-to-Image Translation
The term I2I translation continuously gain popularity since
the invention of cGAN [34]. Pix2Pix [17] ﬁrst leverages a
conditional GAN framework to develop a uniﬁed model for
all problems. Some further works extend the model to solve
super-resolution [25, 39], face photo synthesis [45, 48] and
even fashion outﬁts synthesize [49] tasks. Nevertheless, the
need for ground-truth images from the source and target
domains limits the application of these approaches.
To break the restriction, Zhu et al. propose CycleGAN
[50] based on the idea of cycle-consistency loss. Different
from CycleGAN, DualGAN [46] and DiscoGAN [23]
develop other innovative loss functions to perform unsu-
pervised I2I translation. Furthermore, it is interesting that
Liu et al. introduce an unsupervised image-to-image
translation (UNIT) framework [30], which makes progress
by assuming images from different domains can be mapped
to shared latent space. For the purpose of obtaining multi-
modal outputs, the Multimodal Unsupervised Image-to-
Image Translation (MUNIT) framework [15] creatively
decompose the latent space of the source domain into a
content space and a style space, allowing the models
generate various images. As an extension of CycleGAN,
DRIT
[26]
introduces
a
disentangled
representation
framework and applies a content discriminator to facilitate
the factorization of domain-invariant content space and
domain-speciﬁc attribute space, which allows it to generate
diverse and high-quality results.
Another branch of unsupervised I2I translation is multi-
domain translation. StarGAN [8] and its enhanced model
[9] aim to simultaneously translate images into multiple
target domains. DRIT?? [27] enhances the naive model
(DRIT [26]), enabling the generator to produce diverse
outputs of different domains. Moreover, Mao et al. [31]
introduce an effective SAV-based framework which has
the capability to generate continuous and diverse images
across multiple domains.
More recently, Jeong et al. [18] develop an advanced
instance-level
unsupervised
I2I translation
framework
leveraging a key-value memory structure. Mejjati et al.
[33] integrate the attention mechanism into the GAN
model, while Kim et al. [22] design a delicate network by
using
Adaptive
Layer-Instance
Normalization.
Both
methods could produce exquisite images, but their models
are relatively large and, therefore, impossible for devices
with small GPU memory to run. NICE-GAN [6], proposed
by Chen et al., reduces the number of training parameters
by reusing discriminators in the encoding procedure of
generators. The performance of NICE-GAN surpasses its
Neural Computing and Applications
123

baseline [22]; however, the parameter reduction is not
encouraging enough.
In this paper, we develop a GAN structure to perform
the unsupervised I2I translation task. The new structure
enables the generative model to produce competitive
results compared with the attention-based models but
reduces computational costs dramatically.
3 InvolutionGAN
In this section, we present our proposed model Involu-
tionGAN. We ﬁrst describe the network architecture and
then introduce the detail of the involution block component
in our model. In the end, we explain the proposed LPIPS
perceptual loss functions and other objectives.
3.1 InvolutionGAN architecture
In this article, we focus on I2I translation between two
domains. We aim to learn a mapping function that could
translate images from the source domain X to the target
domain Y. Given unpaired training samples fxigM
i¼1 and
fyjgN
j¼1, where M and N denote the number of samples
belonging to X and Y, we are going to train two GAN
generators to model mappings of two opposite directions
(denoted by G : X ! Y and F : Y ! X). Furthermore, our
architecture also consists of two discriminators DX, DY to
distinguish real samples x  pdatafxg, y  pdatafyg from the
fake ones ^x ¼ FðyÞ, ^y ¼ GðxÞ. The overview of the trans-
lation network is shown in Fig. 2(a).
3.1.1 Generator
As shown in Fig. 2(b), our InvolutionGAN generator is a
typical instance of the autoencoder family, which evolves
from the one proposed by Johnson et al. [20]. Its frame-
work could be divided into three parts: encoder, bottleneck,
and decoder. First, input images are resized before being
fed into the encoder. Then, the following down-sampling
encoder takes the input and produces feature maps. After
that the feature maps go through the bottleneck, which
comprises residual blocks (ResBlocks) [13] (as shown in
Fig. 2(b)) and involution blocks [28] (described in 3.2). In
this module, local and long-range spatial features are
integrated, extracting more valuable information. Finally,
the decoder is used to perform up-sampling process, pro-
ducing plausible translated images.
3.1.2 Discriminator
In terms of the discriminator, we adopt 70  70 PatchGAN
[17] to discriminate whether the input is real or fake. In our
approach, we use PatchGAN for two beneﬁts: a) it could be
light and efﬁcient, and b) it could model the image as a
Markov random ﬁeld, producing generated images with
higher visual quality in content and style. As shown in
Fig. 3, the architecture of the discriminator contains four
4  4 Convolution-InstanceNorm-LeakyReLU layers with
stride 2. We note that for the ﬁrst layer, we do not employ
InstanceNorms. The discriminator applies a convolution at
the end of the last layer to output the 1-channel prediction
map.
3.2 Involution operator
Compared with conventional convolution, the involution
has an inverse characteristic in the spatial and channel
domain. Therefore, involution operators are spatial-speciﬁc
and channel-agnostic. Figure 4 shows the computation
process of a typical involution operator. Notably, an
involution kernel is described as Ii;j;:;:;g 2 RHWKKG,
where i, j indicating the pixel’s location shared across
channels and g ¼ 1; 2; ::: is the number of groups that share
the same involution kernel. Moreover, H, W, and C denote
the height, width and channel of the feature maps,
respectively. The variable K indicates the ﬁlter size, and we
also note that the group number G  C. To obtain the
output feature map Y, we apply Multiply-Add operations
on the input X.
Yi;j;k ¼
R
ðu;vÞ2DK Ii;j;uþbK
2c;vþbK
2c;dKG
C eX iþu;jþv;k
ð1Þ
Instead of using a ﬁxed shape weight matrix as the
kernel-like convolution, the shape of the involution kernel
is generated based on the input feature map X. The process
of kernel generation ensures that the input tensor size and
output kernels can be aligned in the spatial dimension. We
use U to denote the involution kernel generation function,
and for pixel location (i, j), the functional mapping is
described as:
I i;j ¼ UðX ui;jÞ;
ð2Þ
where
ui;j
represents
the
set
of
pixels
at
location
(i, j) across all channels.
In terms of utilizing the involution operator in our I2I
translation framework, we design an alternate bottleneck
structure (see Fig. 2(b)). We adopt 7  7 involution for the
involution blocks to extract features together with convo-
lution blocks. In our experiment, we apply six convolu-
tional
ResBlocks and
ﬁve
involution
blocks
to
the
bottleneck in order to attain the best performance.
Neural Computing and Applications
123

3.3 Loss Function
We adopt four kinds of losses to proceed with the training
process: adversarial loss, cycle-consistency loss, identity
reconstruction loss and our proposed LPIPS perceptual
loss. The adversarial loss is for tackling I2I translation,
while the remaining three are for generating outputs with
higher quality.
Adversarial loss. To pursue more stable training, we
exploit least-square adversarial loss by [32] instead of the
vanilla GAN objective. For the style transferring generator
G and the relative discriminator D, our adversarial loss is:
LlsganðGÞ ¼Ex  pdatafxg½ðDðGðxÞÞ  1Þ2;
ð3Þ
LlsganðDÞ ¼Ex  pdatafxg½DðGðxÞÞ2
þ Ey  pdatafyg½ðDðyÞ  1Þ2;
ð4Þ
where LlsganðGÞ encourages the generator G to produce
fake images G(x) that have a similar style as images from
the target domain, whereas LlsganðDÞ forces the discrimi-
nator D to distinguish whether the sample is real or fake.
Speciﬁcally, we use LGANðG; DY; X; YÞ to denote the
uniﬁed loss of mapping G : X ! Y and the corresponding
discriminator DY:
LGANðG; DY; X; YÞ ¼ LlsganðGÞ þ LlsganðDYÞ;
ð5Þ
where X and Y represent the source domain and target
domain, respectively, and G denotes the generator of
mapping X ! Y, DY indicates the discriminator that dis-
tinguishes input is really from Y domain or generated.
Similarly, the adversarial loss for the opposite mapping
F : Y ! X is denoted by LGANðF; DX; Y; XÞ.
Fig. 2 Illustration of the proposed method (a) the overall architecture
of the InvolutionGAN I2I translation network, (b) the structure of the
InvolutionGAN generator. Note that the ResBlock structure contains
a skip connection, two reﬂection paddings, two convolutions, two
instancenorms [41] and a ReLU operation in the middle. (Best view in
colour) (color ﬁgure online)
0
Generated
Real
LeakyReLU
InstanceNorm
Convolution Layers
Image Input
Matrix Output
Convolution
InvolutionGAN Discriminator
0
0
1
0
0
0
1
1
Label
Generated
Convolution Block
Fig. 3 Illustration of the InvolutionGAN discriminator. Our discrim-
inator shares the same implementation as [50]
Neural Computing and Applications
123

Cycle-consistency loss. By employing the cycle-con-
sistency constraint to the generators, we can alleviate the
mode collapse problem. Cycle-consistency loss is ﬁrst
proposed in [50]. For an image x 2 X, after translating x
from X to Y and then inversely from Y to X, the loss
measures the discrepancy between the original input x and
the double translated image F(G(x)). The whole process of
cycle-consistency loss is formulated as follows:
LcycleðG; FÞ ¼ Ex  pdatafxg½kFðGðxÞÞ  xk1
þ Ey  pdatafyg½kGðFðyÞÞ  yk1;
ð6Þ
where kFðGðxÞÞ  xk1 means to calculate the L1 distance
between cyclically transferred result F(G(x)) and input x,
and vice versa kGðFðyÞÞ  yk1 suggests the opposite cycle-
consistency procedure (i.e. Y ! X ! Y).
Identity loss. In I2I translation, another essential goal is
generating images with similar colour distribution to the
inputs. Here, we exploit the identity loss proposed by
CycleGAN [50] to ensure the generator is close to the
identity function. Given an input x 2 X, the loss assumes
that if we reconstruct x using F, F(x) should be the same as
x. The idea of identity reconstruction loss takes the fol-
lowing form:
LIdtðG; FÞ ¼ Ex  pdatafxg½kFðxÞ  xk1
þ Ey  pdatafyg½kGðyÞ  yk1;
ð7Þ
where we punish for discrepancy between F(x) and x, on
the contrary direction, between G(y) and y.
LPIPS
perceptual
loss.
Former
unsupervised
I2I
translation models only calculate cycle-consistency and
identity losses in pixel-level L1 distance. However, the
feature-level perceptual discrepancy caused by image
reconstruction should also be considered for the purpose of
generating superior results. To this end, we introduce the
LPIPS perceptual loss to minimize the feature reconstruc-
tion losses between original input images and both cycle-
consistency reconstructed as well as identity reconstructed
images. The proposed LPIPS perceptual loss could be
formulated as follows:
LLPIPSðG; FÞ ¼ Ex  pdatafxg R
k sk½CkðFðGðxÞÞÞ  CkðxÞ
þ Ey  pdatafyg R
k sk½CkðGðFðyÞÞÞ  CkðyÞ
þ Ex  pdatafxg R
k sk½CkðFðxÞÞ  CkðxÞ
þ Ey  pdatafyg R
k sk½CkðGðyÞÞ  CkðyÞ;
ð8Þ
where C is the network used to extract features, s generates
the LPIPS perceptual score with deep embedding produced
by C, and k denotes that the ﬁnal LPIPS score is averaged
from k layers (The process is shown in Fig. 5).
Full Objective. Finally, combining the losses men-
tioned above, we explain the full objective of Involu-
tionGAN as:
Fig. 4 Illustration of the involution operator. In this example, I
indicates the involution kernel Ii;j 2 RKK1 and the number of
groups G ¼ 3. b denotes multiplication broadcast across channels,
and a denotes summation aggregated within the K  K spatial
neighbourhood. (Best view in colour) (color ﬁgure online)
Neural Computing and Applications
123

LðG; F; DX; DYÞ ¼ kAdv½LGANðG; DY; X; YÞ
þ LGANðF; DX; Y; XÞ
þ kCycLcycleðG; FÞ þ kIdtLIdtðG; FÞ
þ kLPIPSLLPIPSðG; FÞ;
ð9Þ
where kAdv, kCyc, kIdt and kLPIPS control the relative dif-
ference of different terms. Our ﬁnal objective is to solve
the following min-max problem, formulated as follows:
G; F; D
X; D
Y ¼ arg min
G;F max
DX;DY LðG; F; DX; DYÞ:
ð10Þ
4 Experiment and Results
In this section, we introduce our experiments in three parts.
We ﬁrst brieﬂy introduce the baselines, datasets, evaluation
metrics, and implementation setting of our experiments.
After that, we compare InvolutionGAN with state-of-the-
art methods qualitatively and quantitatively. In the end,
extensive ablation studies are carried out to analyse the
validation of our proposed methods and to ﬁnd the best
bottleneck structure of our model.
4.1 Baselines, datasets and evaluation metrics
4.1.1 Baseline and datasets
We evaluate and compare the performance of Involu-
tionGAN with the following state-of-the-art image-to-im-
age translation methods: DualGAN [46], Cycle-GAN [50],
UNIT [30], MUNIT [15], U-GAT-IT-light [22] and NICE-
GAN-light [6]. Our experiments are carried out by using
the public codes of the above baselines.
The
experiments
are
conducted
on
three
popular
benchmarks of unpaired images to evaluate the perfor-
mance of InvolutionGAN: selﬁe2anime [22], horse2zebra
and vangogh2photo [50]. The train splits of the above
datasets are 3400/100 (selﬁe), 3400/100 (anime); 1,067/120
(horse), 1,334/140 (zebra); 400/400 (vangogh), 6,287/751
(photo), respectively. In addition, all images are resized to
256  256 for training and testing.
4.1.2 Evaluation metrics
For qualitative evaluation, we conducted a human per-
ceptual study to evaluate the user preference score. For
quantitative evaluation, we use FID [14] and KID [4] to
assess the performance of different I2I translation models,
as well as investigate the number of parameters and
inference time of the models.
User Preference In the human perceptual study, we
display the original image and two translated images gen-
erated by different methods. In the end, we collect all the
statistics to compute the preference score of every per-
ceptual test.
Number of Parameters and Inference Time In quan-
titative evaluation, we calculate the model sizes by adding
the number of trainable parameters of one generator and
one discriminator. We also compute the inference time by
averaging the time used in the testing phase. Lesser number
of parameters and lesser inference time cost indicate a
more lightweight model.
Fre´chet Inception Distance (FID) The FID is a
prevalent metric that calculates the Fre´chet distance
(Wasserstein-2 distance) between feature representations of
the real image and the generated image. The feature rep-
resentations are extracted from the last hidden layer of
InceptionNet for the input image set. Lower FID indicates
better performance.
Kernel Inception Distance (KID) The KID is proposed
more recently, which computes the squared Maximum
Mean Discrepancy (MMD) between the feature represen-
tations of actual and generated images. Compared to FID,
KID has an unbiased estimator, making it more reliable
with few test images. Lower KID indicates that the gen-
erated images are more similar to the real samples.
4.1.3 Implementation
We use the Adam [24] optimizer with learning rate 0.0002
and b1 ¼ 0.5, b2 ¼ 0.999 to train the networks. The
learning rate starts to decay linearly after 100 epochs. All
models are trained over 100K iterations with batch size 1.
The weight term kAdv is set to 1, while kCyc, kIdt and kLPIPS
are all set to 5 in Equation (9).
Original Image  
X
Reconstruted Image  
Xrec
Feature 
Extractor
(AlexNet)
Feature 
Extractor
(AlexNet)
Deep 
Embedding
Multiply 
L2 Norm 
Spatial Avg
LPIPS Loss 
(Feature-level
descrepancy)
Fig. 5 Illustration of the LPIPS loss calculation. The original and
cycle-reconstructed (or identity-reconstructed) images are fed into the
feature extractor. LPIPS score is derived after deep embedding and a
series of computations
Neural Computing and Applications
123

4.2 Qualitative evaluation
In our perceptual study, we conduct a user study using a
proportion of outputs generated by InvolutionGAN and
other state-of-the-art methods. 141 participants are asked to
complete a series of selections, and each participant is
given unlimited time to select the image which has a closer
appearance to the target domain. Every question contains
only two translated images with the original image to
ensure fair judgments. The preference score results are
exhibited in Table 1.
The InvolutionGAN model roughly shares the same
structure as CycleGAN, but our approach achieves better
preference scores in all image translation tasks. Particu-
larly, InvolutionGAN gets 9.6% (vangogh2photo) to 67.5%
(zebra2horse) higher scores in comparison with Cycle-
GAN. Furthermore, when compared with the state-of-the-
art approaches, InvolutionGAN outperforms them only
except for selﬁe2anime. The study proves that Involu-
tionGAN is also effective in I2I translation tasks. The User
preference results also reveal that by employing the invo-
lution operator and LPIPS perceptual loss, the model could
synthesize vivid images with better textures.
Figure 6 offers an intuitive visual comparison between
our results and other methods. The results demonstrate that
InvolutionGAN could produce images with ﬁner details.
To be speciﬁc, for all samples of DualGAN displayed in
Fig. 6, DualGAN does not perform well in the complex
unpaired datasets. As for CycleGAN (results shown in the
third column), it fails to deal with long-range spatial fea-
tures, causing the inferior quality of the outputs. Further-
more, UNIT and MUNIT do not generate images that share
similar colour distribution with the source domain since
they generate results with random style codes for output
diversity. For example, in the selﬁe2anime task (the ﬁrst
row), these two methods output images (as shown in the
fourth and ﬁfth columns) with a large colour discrepancy
with the input image. Although U-GAT-IT-light and
NICE-GAN-light could sometimes synthesize plausible
results, they suffer from some unstable performance
problems (see the third row for U-GAT-IT-light and the
second row for NICE-GAN-light).
Overall, InvolutionGAN results are more visually real-
istic, and their outputs are more consistent while handling
different I2I translation tasks.
4.3 Quantitative evaluation
For quantitative evaluation, we measure the number of
trainable parameters as well as the inference time of the
proposed and other unsupervised I2I translation models in
Table 2. The FID and KID scores displayed in Table 3 are
computed using the generated images and real samples.
The scores are considered crucial factors which represent
the performance of generative models.
As exhibited in Table 2, the proposed InvolutionGAN
has the least trainable parameters and inference time. By
applying the involution operator, the generators and dis-
criminators of InvolutionGAN have a size of 8.0 M and
2.8 M, having less trainable parameters from 23.9% to
91.9%. In addition, when employed in RTX1080 GPU, our
model spent the least inference time (3.82ms) compared
with other models under the same conditions.
Apart from the fact that InvolutionGAN has the smallest
model size, our model could preserve competitive perfor-
mance in various I2I translation datasets. For example, it is
notable that our generator model only contains 70% of
trainable parameters compared with CycleGAN but gen-
erates superior results quantitatively in all proposed data-
sets. We also found that our approach attains the second-
best FID score in selﬁe2anime (72.033), anime2selﬁe
(100.234) and zebra2horse (105.823). Despite the inﬂuence
of performance vs. computational cost trade-offs, our
model attain the lowest FID score in horse2zebra (62.929)
and photo2vangogh tasks (106.499). Also, it is interesting
to see that our model outperforms all state-of-the-art
methods having a KID score of 2.6214 ± 0.10 and 2.1019
± 0.12. The quantitative results show that the abilities to
extract and process long-range dependencies are crucial
factors inﬂuencing models’ performance. By constructing a
generator with the involution, we empirically discover a
Table 1 User study results. The
number represents the
preference score on that I2I
translation task
Model
selﬁe2anime
horse2zebra
zebra2horse
photo2vangogh
vangogh2photo
DualGAN [46]
4.019
5.807
10.886
9.594
16.320
CycleGAN [50]
16.851
17.269
20.297
15.174
16.970
UNIT [30]
10.243
9.759
5.100
19.059
9.174
MUNIT [15]
10.278
1.917
5.769
4.737
8.162
U-GAT-IT-light [22]
19.355
19.754
10.261
18.161
17.064
NICE-GAN-light [6]
20.493
16.569
21.545
11.429
13.710
Ours
18.761
28.925
26.142
21.846
18.601
* The bold texts indicate the best results
Neural Computing and Applications
123

better balance between the performance and computation
cost.
4.4 Ablation Study
4.4.1 Analysis of proposed components
We ﬁrst perform model ablation on the horse2zebra dataset
to evaluate the impact of our proposed components. In
Table 4, FID and KID scores are reported for different
conﬁgurations of InvolutionGAN. Firstly, we remove the
involution blocks in our model. In this case, our model is
reduced to a variant of CycleGAN architecture which
contains 11 residual blocks in the bottleneck modules. Our
results show that this led to a slightly better performance
than CycleGAN but worse than our full model. Next, we
remove the LPIPS perceptual loss of InvolutionGAN. The
results reveal that applying involution operators in the
generator could save computational costs and improve
synthesized images’ quality.
4.4.2 Analysis of k in the full objective
The k hyper-parameters mentioned in Equation (9) are
crucial to the training process. In order to train a satisfac-
tory model, we conduct experiments to search for the best k
settings. In the experiments, we ﬁx the kAdv to 1 and kCyc to
5 empirically, and change the value of kIdt and kLPIPS. The
results are shown in Table 5.
When the kIdt and kLPIPS are both set to 5, it is observed
that the identity loss and LPIPS perceptual loss are best
balanced. The results of the experiments prove that pixel-
level and perceptual-level reconstruction losses are of the
Input Image
DualGAN
CycleGAN
UNIT
MUNIT
U-GATIT-light
NICE-GAN-light
Ours 
Fig. 6 Visual comparison of the generated output. From top to bottom: selﬁe2anime, horse2zebra, zebra2horse, photo2vangogh, vangogh2photo.
Our approach successfully generates more plausible results on different datasets. (Best view in colour) (color ﬁgure online)
Table 2 Model parameters and inference time
Model
Parameters
Inference time
DualGAN [46]
34.5 M
24.82ms
CycleGAN [50]
14.2 M
5.07ms
UNIT [30]
19.3 M
250.64ms
MUNIT [15]
23.3 M
354.13ms
U-GAT-IT-light [22]
134.0 M
62.93ms
NICE-GAN-light [6]
108.9 M
22.65ms
Ours
10.8 M
3.82ms
 The bold ttexts indicate the least trainable parameters or inference
time
Neural Computing and Applications
123

same importance in training an I2I translation GAN model
of high quality.
4.4.3 Analysis of model structure design
We also conduct a series of extensive ablation studies in
order to ﬁnd out the best structure of the bottleneck. For
experiments, we alter the number of blocks in the
bottleneck of the InvolutionGAN generator. We use Res-
Block as the starting block in the bottleneck in all settings.
We report results for ﬁve varied bottleneck designs from 9
to 13 blocks. The visual comparison of generated outputs is
illustrated in Fig. 7, and the FID and KID scores of the
study are displayed in Table 6. We remove the LPIPS
perceptual loss to minimize the inﬂuence from other
components.
Table 3 Quantitative evaluation of different image translation models
Model task
selﬁe2anime
horse2zebra
photo2vangogh
FID#
KID100±std.100#
FID#
KID100±std.100#
FID#
KID100±std.100#
Real Image
64.532
0.3062 ± 0.13
26.159
0.0067 ± 0.02
0
0.1942 ± 2.89
DualGAN [46]
156.783
11.4526 ± 0.29
227.498
20.2053 ± 0.20
174.444
9.8027 ± 0.23
CycleGAN [50]
76.019
1.8005 ± 0.18
68.423
2.153 ± 0.04
123.190
3.0307 ± 0.11
UNIT [30]
89.046
3.4769 ± 0.23
109.975
5.7391 ± 0.12
138.021
5.4387 ± 0.14
MUNIT [15]
83.235
2.4908 ± 0.19
139.702
6.6238 ± 0.10
136.321
5.4972 ± 0.12
U-GAT-IT-light [22]
74.666
1.6554 ± 0.23
113.44
4.2154 ± 0.68
112.100
2:9564 	 0:12
NICE-GAN-light [6]
71.032
1.418–0.22
65.93
1:090 	 0:02
120.676
3.0174 ± 0.11
Ours
72.033
1:4274 	 0:16
62.929
1.0017–0.11
106.499
2.6214–0.10
Real Image
89.3234
0.3152 ± 0.09
79.741
0.0678 ± 0.07
34.030
0.0051 ± 0.01
DualGAN [46]
187.052
14.757 ± 0.50
185.685
11.4993 ± 0.20
162.417
9.0249 ± 0.21
CycleGAN [50]
104.8332
1.3935 ± 0.22
111.921
2.5706 ± 0.14
94.554
2.1313 ± 0.09
UNIT [30]
171.251
12.2188 ± 0.48
157.948
7.4077 ± 0.40
109.462
5.3293 ± 0.15
MUNIT [15]
113.049
3.2910 ± 0.19
169.922
7.0608 ± 0.39
110.959
5.0646 ± 0.12
U-GAT-IT-light [22]
95.112
1.3264 ± 0.18
145.47
3.3900 ± 0.30
81.551
2:1607 	 0:10
NICE-GAN-light [6]
100.67
1:2784 	 0:15
103.739
2.2430–0.15
85.014
2.2945 ± 0.09
Ours
100.234
1.2114±0.19
105.823
2:2880 	 0:18
85.789
2.1019±0.12
 The bold texts indicate the best results, and the underlined texts refer to the second best results
Table 4 Ablation study results.
w/o indicates removal of the
corresponding component
Model
horse2zebra
zebra2horse
FID#
KID100±std.100#
FID#
KID100±std.100#
Baseline(CycleGAN [50])
68.423
2.153 ± 0.04
111.921
2.5706 ± 0.14
Ours w/o Involution
66.843
1.9524 ± 0.16
108.324
2.3342 ± 0.15
Ours w/o LPIPS perceptual loss
67.274
1.9856 ± 0.17
108.634
2.3271 ± 0.17
InvolutionGAN
62.929
1.0017 – 0.11
105.823
2.2880 – 0.18
Table 5 Weight terms kIdt and
kLPIPS study results
Settings
horse2zebra
zebra2horse
FID#
KID100±std.100#
FID#
KID100±std.100#
kIdt ¼ 3; kLPIPS ¼ 7
67.292
1.0850 ± 0.29
119.521
3.0489 ± 0.19
kIdt ¼ 4; kLPIPS ¼ 6
66.984
1.0475 ± 0.32
116.488
3.0857 ± 0.28
kIdt ¼ 5; kLPIPS ¼ 5
62.929
1.0017 ± 0.11
105.823
2.2880±0.18
kIdt ¼ 6; kLPIPS ¼ 4
69.882
1.0320 ± 0.31
114.208
3.0349 ± 0.23
kIdt ¼ 7; kLPIPS ¼ 3
66.893
1.0311 ± 0.39
115.721
3.0995 ± 0.23
Neural Computing and Applications
123

Regarding the visual appearance of generated images
produced by different bottleneck structures, we can see a
noticeable performance enhancement when the block
number is added from 9 to 13. For example, the face and
hair textures are getting ﬁner in the synthesized anime
characters. On the contrary, the quality begins to decline
when we add more blocks to the bottleneck, as the output
images start to gain redundant lines in the anime portraits.
Our quantitative FID and KID result strongly support
the above ﬁndings. We have evaluated the FID and KID for
selﬁe2anime in both directions, and the result is shown in
Table 6. For the selﬁe2anime task, the FID and KID scores
dropped from 75.383, 1.866 ± 0.2 to 73.356, 1.711 ± 0.1
as the number of blocks increased. Subsequently, the scores
rise to 76.643 and 1.8207 ± 0.15 if we continue to add
involution blocks and residual blocks to the bottleneck. For
the opposite anime2selﬁe task, we could witness a similar
phenomenon. As a result, the 11-block bottleneck structure
could produce the most plausible results.
5 Conclusion
In this paper, we propose InvolutionGAN, a novel unsu-
pervised I2I translation model. We utilize both the invo-
lution
operator
and
ResBlock
to
construct
a
more
lightweight and powerful bottleneck, enabling the genera-
tor to extract the source images’ local and long-range
spatial features. Also, we introduce a new loss term named
LPIPS perceptual loss, which could enforce the generator
output more plausible results. Comparative qualitative and
quantitative evaluations on various prevalent datasets
reveal that InvolutionGAN achieves effectiveness while
compressing computation. We also conduct extensive
ablation studies to evaluate the impact of each component
and to help us ﬁnd the best bottleneck architecture. We
believe this work could foster the enthusiasm to properly
combine more powerful operators in future I2I translation
models and offer a new idea to improve other GAN-based
models.
Data Availability Statements All data and results included in this
study are available upon reasonable request by contact with the cor-
responding author.
Declaration
Conflict of interest Haipeng Deng, Qiuxia Wu, Han Huang, Xiaowei
Yang and Zhiyong Wang declare that no conflict of interest could
have appeared to influence the work reported in this paper.
Input Image
9 Blocks
10 Blocks
11 Blocks
12 Blocks
13 Blocks
Fig. 7 Visual comparison of outputs generated by different bottleneck structures (The examples are generated on selﬁe2anime dataset). (Best
View in colour)
Table 6 Structural study results.
The block number indicates the
amount of blocks in bottleneck
which consists of Resblocks and
involution blocks alternatively
Number of Blocks in Bottleneck
selﬁe2anime
anime2selﬁe
FID#
KID100±std.100#
FID#
KID100±std.100#
9
75.383
1.866 ± 0.20
104.221
1.5020 ± 0.20
10
75.295
1.923 ± 0.18
104.261
1.6290 ± 0.25
11
73.356
1:711 	 0:15
102.737
1:3580 	 0:19
12
76.355
2.215 ± 0.19
106.399
1.7653 ± 0.17
13
74.6435
1.8207 ± 0.15
103.423
1.4151 ± 0.19
Neural Computing and Applications
123

References
1. Aguinaldo A, Chiang PY, Gain A, et al (2019) Compressing gans
using knowledge distillation. arXiv preprint arXiv:1902.00159
2. Arjovsky M, Chintala S, Bottou L (2017) Wasserstein generative
adversarial networks. In: International conference on machine
learning, PMLR, pp 214–223
3. Bharti V, Biswas B, Shukla KK (2021) Emocgan: a novel evo-
lutionary multiobjective cyclic generative adversarial network
and its application to unpaired image translation. Neural Comput
Appl, pp 1–15
4. Bin´kowski M, Sutherland DJ, Arbel M, et al (2018) Demystify-
ing mmd gans. arXiv preprint arXiv:1801.01401
5. Cao Y, Zhou Z, Zhang W, et al (2017) Unsupervised diverse
colorization via generative adversarial networks. In: Joint Euro-
pean conference on machine learning and knowledge discovery in
databases, Springer, pp 151–166
6. Chen R, Huang W, Huang B, et al (2020) Reusing discriminators
for encoding: Towards unsupervised image-to-image translation.
In: Proceedings of the IEEE/CVF conference on computer vision
and pattern recognition, pp 8168–8177
7. Chen X, Duan Y, Houthooft R, et al (2016) Infogan: inter-
pretable representation learning by information maximizing
generative adversarial nets. In: Proceedings of the 30th interna-
tional conference on neural information processing systems,
pp 2180–2188
8. Choi Y, Choi M, Kim M, et al (2018) Stargan: Uniﬁed generative
adversarial networks for multi-domain image-to-image transla-
tion. In: Proceedings of the IEEE conference on computer vision
and pattern recognition, pp 8789–8797
9. Choi Y, Uh Y, Yoo J, et al (2020) Stargan v2: diverse image
synthesis for multiple domains. In: Proceedings of the IEEE/CVF
conference
on
computer
vision
and
pattern
recognition,
pp 8188–8197
10. Emami H, Aliabadi MM, Dong M et al (2020) Spa-gan: spatial
attention gan for image-to-image translation. IEEE Trans Mul-
timed 23:391–401
11. Goodfellow I, Pouget-Abadie J, Mirza M, et al (2014) Generative
adversarial nets. Adv Neural Inf Process Syst 27
12. Gulrajani I, Ahmed F, Arjovsky M, et al (2017) Improved
training of wasserstein gans. arXiv preprint arXiv:1704.00028
13. He K, Zhang X, Ren S, et al (2016) Deep residual learning for
image recognition. In: Proceedings of the IEEE conference on
computer vision and pattern recognition, pp 770–778
14. Heusel M, Ramsauer H, Unterthiner T, et al (2017) Gans trained
by a two time-scale update rule converge to a local nash equi-
librium. Adv Neural Inf Process Syst 30
15. Huang X, Liu MY, Belongie S, et al (2018) Multimodal unsu-
pervised image-to-image translation. In: Proceedings of the
European conference on computer vision (ECCV), pp 172–189
16. Iizuka S, Simo-Serra E, Ishikawa H (2016) Let there be color!
joint end-to-end learning of global and local image priors for
automatic image colorization with simultaneous classiﬁcation.
ACM Trans Graph 35(4):1–11
17. Isola P, Zhu JY, Zhou T, et al (2017) Image-to-image translation
with conditional adversarial networks. In: Proceedings of the
IEEE conference on computer vision and pattern recognition,
pp 1125–1134
18. Jeong S, Kim Y, Lee E, et al (2021) Memory-guided unsuper-
vised image-to-image translation. In: Proceedings of the IEEE/
CVF conference on computer vision and pattern recognition,
pp 6558–6567
19. Jo Y, Yang S, Kim SJ (2020) Investigating loss functions for
extreme super-resolution. In: Proceedings of the IEEE/CVF
conference on computer vision and pattern recognition work-
shops, pp 424–425
20. Johnson J, Alahi A, Fei-Fei L (2016) Perceptual losses for real-
time style transfer and super-resolution. In: European conference
on computer vision, Springer, pp 694–711
21. Kang T, Lee KH (2020) Unsupervised image-to-image translation
with self-attention networks. In: 2020 IEEE international con-
ference on big data and smart computing (BigComp), IEEE,
pp 102–108
22. Kim J, Kim M, Kang H, et al (2019) U-gat-it: unsupervised
generative attentional networks with adaptive layer-instance
normalization for image-to-image translation. arXiv preprint
arXiv:1907.10830
23. Kim T, Cha M, Kim H, et al (2017) Learning to discover cross-
domain relations with generative adversarial networks. In:
International
conference
on
machine
learning,
PMLR,
pp 1857–1865
24. Kingma DP, Ba J (2014) Adam: A method for stochastic opti-
mization. arXiv preprint arXiv:1412.6980
25. Ledig C, Theis L, Husza´r F, et al (2017) Photo-realistic single
image super-resolution using a generative adversarial network.
In: Proceedings of the IEEE conference on computer vision and
pattern recognition, pp 4681–4690
26. Lee HY, Tseng HY, Huang JB, et al (2018) Diverse image-to-
image translation via disentangled representations. In: Proceed-
ings of the European conference on computer vision (ECCV),
pp 35–51
27. Lee HY, Tseng HY, Mao Q et al (2020) Drit??: diverse image-
to-image translation via disentangled representations. Int J
Comput Vis 128(10):2402–2417
28. Li D, Hu J, Wang C, et al (2021) Involution: inverting the
inherence of convolution for visual recognition. In: Proceedings
of the IEEE/CVF conference on computer vision and pattern
recognition, pp 12321–12330
29. Li M, Lin J, Ding Y, et al (2020) Gan compression: Efﬁcient
architectures for interactive conditional gans. In: Proceedings of
the IEEE/CVF conference on computer vision and pattern
recognition, pp 5284–5294
30. Liu MY, Breuel T, Kautz J (2017) Unsupervised image-to-image
translation networks. In: Advances in neural information pro-
cessing systems, pp 700–708
31. Mao Q, Tseng HY, Lee HY et al (2022) Continuous and diverse
image-to-image translation via signed attribute vectors. Int J
Comput Vis 130(2):517–549
32. Mao X, Li Q, Xie H, et al (2017) Least squares generative
adversarial networks. In: Proceedings of the IEEE international
conference on computer vision, pp 2794–2802
33. Mejjati YA, Richardt C, Tompkin J, et al (2018) Unsupervised
attention-guided image to image translation. arXiv preprint
arXiv:1806.02311
34. Mirza M, Osindero S (2014) Conditional generative adversarial
nets. arXiv preprint arXiv:1411.1784
35. Peng X, Peng S, Hu Q, et al (2022) Contour-enhanced cyclegan
framework for style transfer from scenery photos to Chinese
landscape paintings. Neural Comput Appl, pp 1–22
36. Qi GJ (2020) Loss-sensitive generative adversarial networks on
lipschitz densities. Int J Comput Vis 128(5):1118–1140
37. Qu X, Wang X, Wang Z, et al (2018) Perceptual-dualgan: per-
ceptual losses for image to image translation with generative
adversarial nets. In: 2018 international joint conference on neural
networks (IJCNN), IEEE, pp 1–8
38. Radford A, Metz L, Chintala S (2015) Unsupervised represen-
tation learning with deep convolutional generative adversarial
networks. arXiv preprint arXiv:1511.06434
39. Shi W, Caballero J, Husza´r F, et al (2016) Real-time single image
and
video
super-resolution
using
an
efﬁcient
sub-pixel
Neural Computing and Applications
123

convolutional neural network. In: Proceedings of the IEEE con-
ference
on
computer
vision
and
pattern
recognition,
pp 1874–1883
40. Tang H, Xu D, Sebe N, et al (2019) Multi-channel attention
selection gan with cascaded semantic guidance for cross-view
image translation. In: Proceedings of the IEEE/CVF conference
on computer vision and pattern recognition, pp 2417–2426
41. Ulyanov D, Vedaldi A, Lempitsky V (2016) Instance normal-
ization: the missing ingredient for fast stylization. arXiv preprint
arXiv:1607.08022
42. Wang TC, Liu MY, Zhu JY, et al (2018) High-resolution image
synthesis and semantic manipulation with conditional gans. In:
Proceedings of the IEEE conference on computer vision and
pattern recognition, pp 8798–8807
43. Xiao J, Zhang S, Yao Y et al (2022) Generative adversarial
network with hybrid attention and compromised normalization
for
multi-scene
image
conversion.
Neural
Comput
Appl
34(9):7209–7225
44. Xu S, Zhu Q, Wang J (2020) Generative image completion with
image-to-image
translation.
Neural
Comput
Appl
32(11):7333–7345
45. Yadav NK, Singh SK, Dubey SR (2022) Csa-gan: cyclic syn-
thesized attention guided generative adversarial network for face
synthesis. Appl Intell, pp 1–20
46. Yi Z, Zhang H, Tan P, et al (2017) Dualgan: Unsupervised dual
learning for image-to-image translation. In: Proceedings of the
IEEE international conference on computer vision, pp 2849–2857
47. Zhang R, Isola P, Efros AA (2016) Colorful image colorization.
In:
European
conference
on
computer
vision,
Springer,
pp 649–666
48. Zhang Y, Yu L, Sun B, et al (2022) Eng-face: cross-domain
heterogeneous face synthesis with enhanced asymmetric cycle-
gan. Appl Intell pp 1–13
49. Zhou D, Zhang H, Li Q, et al (2022) Coutﬁtgan: learning to
synthesize compatible outﬁts supervised by silhouette masks and
fashion styles. IEEE Trans Multimed
50. Zhu JY, Park T, Isola P, et al (2017) Unpaired image-to-image
translation using cycle-consistent adversarial networks. In: Pro-
ceedings of the IEEE international conference on computer
vision, pp 2223–2232
Publisher’s Note Springer Nature remains neutral with regard to
jurisdictional claims in published maps and institutional afﬁliations.
Springer Nature or its licensor (e.g. a society or other partner) holds
exclusive rights to this article under a publishing agreement with the
author(s) or other rightsholder(s); author self-archiving of the
accepted manuscript version of this article is solely governed by the
terms of such publishing agreement and applicable law.
Neural Computing and Applications
123

