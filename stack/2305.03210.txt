AttentionViz: A Global View of Transformer Attention
Catherine Yeh, Yida Chen, Aoyu Wu, Cynthia Chen, Fernanda Viégas, and Martin Wattenberg
Fig. 1: AttentionViz, our interactive visualization tool, allows users to explore transformer self-attention at scale by creating a joint
embedding space for queries and keys. (a) In language transformers, these visualizations reveal striking visual traces that can be
linked to attention patterns. Each point in the scatterplot represents the query or key version of a word, as denoted by point color.
Users can explore individual attention heads (left) or zoom out for a “global” view of attention (right). (b) Our visualizations also
divulge interesting insights in vision transformers, such as attention heads that group image patches by hue and brightness. Border
color denotes query embeddings of a patch (green) or key embeddings (pink). (c) Sample input sentences (from [18]) and (d) images
(synthetic dataset) are provided for reference.
Abstract—Transformer models are revolutionizing machine learning, but their inner workings remain mysterious. In this work, we
present a new visualization technique designed to help researchers understand the self-attention mechanism in transformers that
allows these models to learn rich, contextual relationships between elements of a sequence. The main idea behind our method
is to visualize a joint embedding of the query and key vectors used by transformer models to compute attention. Unlike previous
attention visualization techniques, our approach enables the analysis of global patterns across multiple input sequences. We create an
interactive visualization tool, AttentionViz (demo: http://attentionviz.com), based on these joint query-key embeddings, and use it
to study attention mechanisms in both language and vision transformers. We demonstrate the utility of our approach in improving
model understanding and offering new insights about query-key interactions through several application scenarios and expert feedback.
Index Terms—Transformer, Attention, NLP, Computer Vision, Visual Analytics
1
INTRODUCTION
The transformer neural network architecture [45] is having a major im-
pact on ﬁelds ranging from natural language processing (NLP) [11,35]
to computer vision [12]. Indeed, transformers are now deployed in
large, real-world systems used by hundreds of millions of people (e.g.,
Stable Diffusion, ChatGPT, Microsoft Copilot). However, the mecha-
nisms behind this success remain somewhat mysterious, especially as
new capabilities continue to emerge with increasing model complexi-
ties and sizes [9,53]. A deeper understanding of transformer models
could help us build more reliable systems, troubleshoot problems, and
suggest avenues for improvement.
• Authors are with Harvard University. Viégas and Wattenberg are also with
Google, but this work was done at Harvard. Emails: {catherineyeh,
yidachen, aoyuwu, fernanda, wattenberg}@g.harvard.edu,
cynthiachen@college.harvard.edu
Manuscript received xx xxx. 201x; accepted xx xxx. 201x. Date of Publication
xx xxx. 201x; date of current version xx xxx. 201x. For information on
obtaining reprints of this article, please send e-mail to: reprints@ieee.org.
Digital Object Identiﬁer: xx.xxxx/TVCG.201x.xxxxxxx
In this work, we describe a new visualization technique aimed at
better comprehending how transformers operate. (We include a brief
introduction to transformers in Sec. 2.) The target of our analysis is
the characteristic transformer self-attention mechanism, which allows
these models to learn and use a rich set of relationships between input
elements. Although attention patterns have been intensively studied,
previous techniques generally visualize information related to just a
single input sequence (e.g., one sentence or image) at a time. Typ-
ical approaches create bipartite graph [44, 46] or heatmap [15, 25]
representations of attention weights for a given input sequence.
Our method offers a higher-level perspective, in which we can view
the self-attention patterns of many input sequences at once. One inspi-
ration for this approach is the success of tools such as the Activation
Atlas [4], which allows a researcher to “zoom out” to see an overview
of a neural network, then drill down for details. In our case, we seek
to build a kind of “attention atlas” that can provide researchers with a
rich and detailed view of how a transformer’s various attention heads
operate. The primary new technique is visualizing a joint embedding of
the query and key vectors used by transformers, which creates a visual
signature for an individual attention head.
To illustrate our technique, we implement AttentionViz, an inter-
arXiv:2305.03210v1  [cs.HC]  4 May 2023

active visualization tool that allows users to explore attention in both
language and vision transformers. AttentionViz affords exploration
through multiple levels of detail (Fig. 1), providing both a global view
to see all attention heads at once and the ability to zoom in on details
in a single attention head or input sequence.
We demonstrate the utility of our technique through several appli-
cation scenarios with AttentionViz and domain expert interviews. For
concreteness, we focus on what the visualization can reveal about a few
widely-used transformers: BERT [11], GPT-2 [34], and ViT [12]. We
ﬁnd several identiﬁable “visual traces” linked to attention patterns in
BERT, detect novel hue/frequency behavior in ViT’s visual attention
mechanism, and uncover potentially anomalous behavior in GPT-2.
User feedback also supports the wider applicability of our approach in
visualizing other embeddings at scale.
To summarize, the contributions of this work include:
• A visualization technique for exploring attention trends in trans-
former models based on joint query-key embeddings.
• AttentionViz, an interactive tool that applies our technique for
studying self-attention in vision and language transformers at
multiple scales.
• Application scenarios and expert feedback showing how Atten-
tionViz can reveal insights about transformer attention patterns.
2
BACKGROUND ON TRANSFORMER MODELS
The transformer, introduced in [45], is a neural network architecture
designed to operate on sequential input. A full description of transform-
ers is beyond the scope of this paper, but a few concepts are critical
for understanding our work. First, a transformer receives as input a
set of vectors (often called embeddings). Embeddings can represent a
variety of input types. In text-based transformers, they correspond to
words or pieces of words; in vision transformers, they encode patches
of pixels.
The network iteratively transforms these vectors via a series of
attention layers, each of which moves information between pairs of
embeddings. The name “attention” suggests that not all embeddings
will be equally related; certain pairs will interact more strongly–i.e.,
pay more “attention” to each other. Attention layers determine which
pairs should interact, and what information should ﬂow between them.
For example, in a transformer operating on the words of the sentence,
“The brown capybara is sleeping now,” one might expect high attention
(and information ﬂow) between embeddings for “capybara” and “is,”
but not between “brown” and “now.” The self-attention mechanism
allows transformers to learn and use a rich set of relationships between
elements of a sequence, yielding signiﬁcant performance improvements
across various NLP and computer vision tasks [11,12,34].
There may be different reasons for embedding pairs to attend to each
other. For instance, in our example sentence,“brown” and “capybara”
are linked by an adjective-noun relation, while “capybara” and “is” form
a subject-verb relation. To allow for several relation types, transformer
attention layers consist of multiple attention heads, each of which can
represent a different pattern of attention and information ﬂow.
Each attention head computes its own attention pattern using a bi-
linear form computed from a query weight matrix WQ and key weight
matrix WK. Concretely, for two embedding vectors x and y, attention
f(x,y) is determined by the inner product of a query vector, WQx, and
a key vector, WKy. Letting d be the dimension of WKy, we have:
f(x,y) = 1
√
d
⟨WQx,WKy⟩
Given embedding vectors {x1,x2,...,xn}, we compute the attention
between xi and the other vectors using the softmax function:
attn(xi,xj) = softmaxj(f(xi,x),..., f(xi,xn)) = ef(xi,x j)/∑
k
ef(xi,xk)
Critically, this formula shows that the greater the dot product between
the query and key vectors, the higher the ﬁnal attention value will be, a
fact that we rely upon in our joint embedding visualization.
There is much more to the transformer architecture than we cover
here. In particular, we have only described the attention weighting be-
tween pairs of embeddings, and not the speciﬁc information that ﬂows
between them. (As discussed later, this is an area ripe for further inves-
tigation.) One last technical point is worth mentioning, however, since
it will help interpret images later in the paper. The initial embeddings
given to a transformer typically incorporate a vector representation of
their ordering (for a 1D sequence) or spatial conﬁguration (for a grid,
as in vision transformers). For sequences, these position vectors are
deﬁned using trigonometric functions, and are located on a helix-like
curve in high-dimensional space (see [45]).
2.1
Models Studied in this Paper
We study three transformer models: BERT (language), GPT-2 (lan-
guage), and ViT (vision). Each has been an important object of study
in the machine learning community, and the three span a range of trans-
former architectures and applications. BERT, or Bidirectional Encoder
Representations from Transformers [11], is a multi-layer transformer
encoder. As a bidirectional model, BERT can attend to tokens (i.e.,
input elements) in either direction. GPT-2, or Generative Pre-trained
Transformer 2 [35], is a multi-layer transformer decoder. GPT-2 is a
unidirectional model, meaning it only attends to previous tokens. ViT,
or Vision Transformer [12], employs a self-attention-based transformer
architecture by splitting images into “patches” and treating them like
tokens in a sentence. Similar to BERT, ViT is a multi-layer, bidirec-
tional transformer encoder. In this work, we look at ViT performance
on 16x16 (ViT-16) and 32x32 (ViT-32) patch sizes.
3
RELATED WORK
Many researchers have attempted to investigate the inner workings
of transformers. [7,29] seek to understand the performance improve-
ment from transformer-based language models by exploring learned
linguistic representations, and [42] observed that BERT recapitulates
classic steps in natural language analysis, from part-of-speech tagging
to relation classiﬁcation. Attention, the backbone of transformers, has
also been studied intensively. For example, attention appears to relate
to syntactic structures in NLP systems [8,50] and gestalt-like grouping
in vision transformers [28]. Researchers have compared ViT’s visual
attention mechanism with convolutional ﬁlters as well, ﬁnding that
attention is more robust against image occlusion, corruption, and high-
frequency noise [30,33]. In our discussion of related work, we focus
on visual approaches for studying transformer attention.
3.1
Visualizing Attention in a Single Input Sequence
Attention patterns naturally lend themselves to visualization, in both
language and vision transformers [10,16,26,32]. These visualizations
are largely focused on visualizing attention weights between query and
key tokens in a single input sequence using bipartite graphs (e.g., [25,
40,44,46]) or heatmaps (e.g., [1,15,17,20,25,36]).
A few visualizations have been proposed that allow comparison
across multiple models or layers. For instance, Attention Flows [10]
supports users in comparing attention within and across layers of BERT,
as well as among attention heads given a single sentence. Dodrio [52]
uses a grid view, applied to single inputs, that enables direct com-
parison of attention heads. Another system, VisQA [16], visualizes
attention at different heads for visual question-answering tasks by
showing heatmaps of language self-attention, vision self-attention, and
language-vision cross-attention. Even in these model-comparison sys-
tems, however, an analyst must look at different inputs, one at a time,
to identify and verify patterns for a given attention head.
3.2
Beyond Single Inputs: Visualizing Embeddings and
Activation Maximization
It is natural to seek patterns that hold across multiple inputs. One tech-
nique that has proved effective toward this goal is visualizing collections
of embedding vectors from multiple input sequences [3,14,39,51]. For
example, [36] visualized BERT embeddings for the same word used
in many different contexts, and found clusters that corresponded to
word senses. In an exploration of syntax processing, [7] visualized

embeddings from a multilingual BERT model and once again found
meaningful clusters that helped with interpretation. LMFingerprints
[38] uses a tree-based radial layout to compare embedding vectors
across different language models.
A second technique, used for vision transformers in [13,54], aims to
ﬁnd images which maximize activations of particular units. Applied
to embedding vectors, this technique produces clearly interpretable
results. The authors note, however, that when applied to query and key
vectors the technique does not seem to produce useful results.
3.3
Gaps in the Literature
We note three gaps in the existing literature which motivate our work.
First, visualizing embedding vectors has been shown to be an ef-
fective technique for analyzing patterns across multiple inputs, but we
know of no systematic attempt to visualize query and key embeddings
in transformer models. [5] also argues that the intermediate artifacts
of self-attention, such as queries and keys, are underexplored. These
observations motivate our joint query-key embedding technique.
Second, although visualization techniques have been proposed to
compare multiple embeddings (e.g., [2,3,21]), these methods are often
limited to a few embeddings and cannot address our needs of comparing
embeddings at different transformer heads and layers. Thus, we design
a global matrix view to visualize query-key embeddings at scale.
Finally, bipartite graph representations have proven helpful in an-
alyzing NLP-based transformers, but we have not seen them applied
to vision tasks. We explore this direction by creating bipartite-style
visualizations to study image attention patterns in ViT.
4
GOALS & TASKS
The overarching aim of this work is to design a novel visualization tech-
nique that allows exploration of global attention trends in transformer
models. To collect some initial feedback about this idea and learn more
about user needs, we talked with 5 machine learning (ML) researchers
(4 Ph.D. students and 1 professor) interested in model interpretability.
During these individual interviews, we asked experts to describe their
current practices and challenges when working with transformers and
how attention visualizations could aid in their research objectives. We
will refer to these experts as E1-5.
Overall, experts emphasized the need for usability and simplicity
during attention exploration. As summarized by E2, “Many existing
visualization tools are too overwhelming to learn and use.” E5 also
reported often having to write custom code to investigate transformer
attention, which is a challenging, time-consuming task.
4.1
Goals
Ultimately, our conversations with experts yielded three main goals:
G1 - Understand how self-attention informs model behavior.
Overall, all 5 experts wanted to better understand the behavior of differ-
ent attention heads and what transformer models are learning through
their characteristic self-attention mechanism. Thus, they expressed the
desire to be able to quickly and easily explore attention patterns. E2
explained that “attention is still pretty closed-box and there’s lots of
mysteries,” so gaining a deeper understanding of transformer attention
patterns could provide insights into “why large language models fail at
reasoning tasks and math,” for example.
G2 - Compare & contrast attention heads. E5 mentioned that
visualizing differences in attention heads could help with hypothesis
generation, which is the ﬁrst step in their research process: “Visualiza-
tion can help formulate hypotheses to test and get an intuitive sense
of what transformers are doing.” Additionally, 3 experts (E1, E2, E5)
noted that attention head comparison would be useful for model prun-
ing and editing purposes. That is, if two attention heads appear to
behave similarly, perhaps one could be removed without signiﬁcantly
impacting model performance. In the words of E1, comparing heads
might allow us to “ﬁnd parts of the model that are actually useful.”
G3 - Identify attention anomalies. Four researchers (E2-5) wanted
to identify irregularities and potential behavioral issues with transform-
ers through attention pattern exploration. This information could then
Fig. 2: Creating a joint query-key embedding space for a single atten-
tion head. In the NLP case, given an input sentence, we ﬁrst transform
each token into its corresponding query and key vector. Then, we use t-
SNE/UMAP/PCA to project these 1×d vectors into 2D/3D scatterplot
coordinates. For the BERT, GPT-2, and ViT models used, d = 64.
be used for model debugging purposes. For instance, E4 said “visualiz-
ing attention could help you notice when the model is looking at the
wrong thing, even if the result is correct.” E3 agreed, reiterating the
importance of debugging especially in the context of model training:
“Training often fails and dies, but it’s hard to understand why it fails or
produces unexpected behavior.”
4.2
Tasks
Given these goals, we developed the following set of design tasks:
T1 - Visualize attention heads at scale. To help users quickly
explore model behavior [G1] and easily compare & contrast attention
patterns [G2], our tool simultaneously visualizes self-attention heads
across transformer layers.
T2 - Explore query-key interactions. E1 and E4 expressed the
desire to better understand query-key pairing information toward im-
proving their understanding of transformer self-attention. Thus, our
tool further supports attention pattern comparison [G2] and anomaly
detection [G3] through visualizing query-key interactions.
T3 - Probe attention at multiple levels. Our tool allows for local
and global comparisons of attention [G2] by providing visualizations at
the sentence/image, head, and model levels. The ﬂexibility of switching
between multiple views in a single interface also facilitates knowledge
discovery [G1] and helps users identify model irregularities [G3].
T4 - Customize model and data inputs. AttentionViz is easily
extendable to new transformers and datasets, affording quick visual
comparison [G2] and synthesis of attention patterns [G1] across differ-
ent models and modalities (language & vision).
5
QUERY/KEY EMBEDDINGS & DESIGN OF ATTENTIONVIZ
To address these goals and tasks, we build a tool called AttentionViz.
The primary technique used by our tool is a visualization of the joint
embedding of query and key vectors for each attention head. In this
section, we ﬁrst describe the motivation and mathematics that underlie
this technique, then discuss the design of the full application.
5.1
Visualizing Query/Key Embeddings
The technique behind AttentionViz is relatively simple, although as we
describe below, it requires two mathematical tricks to be effective. Re-
call that each transformer attention head transforms input embeddings
into query vectors and key vectors by applying matrices WQ and WK,
respectively (Sec. 2). These matrices project the original vector embed-
dings to a lower dimensional space, essentially selecting a particular
type of information from the higher-dimensional vector embeddings.
Therefore, by inspecting the query and key vectors, one might hope to
learn what information is selected by WQ and WK.

Fig. 3: Left: original queries and keys in joint embedding space. Right:
Increased overlap after translating keys to align query and key centroids.
A central observation is that the relative positions of query and key
vectors can offer clues about how attention will be distributed, since
attention coefﬁcients depend on the dot product between queries and
keys. To see why, consider a hypothetical situation where query and
key vectors always have the same norm. Then, closer distances would
directly relate to higher attention coefﬁcients. In practice, query and
key vectors vary in norm, so the relationship between dot product and
distance is not precise. However, as described in the following sections,
we can arrange for this relation to be surprisingly close.
Fig. 2 illustrates our technique with a synthetic example of a single
attention head in a language transformer. To create the joint embedding,
we ﬁrst obtain the query and key vector representation of each token in
a given sentence (Sec. 2). Then, we use one of three dimensionality-
reduction methods to project these high-dimensional vectors onto a
shared, lower-dimensional subspace: t-SNE [43], UMAP [27], or
PCA [19]. The output from these dimensionality-reduction algorithms
is a 2D/3D scatterplot, where each point represents a single query or
key token. The same process can be used to create joint embeddings for
ViT attention heads, where each token is an image patch. By default,
we visualize queries in green and keys in pink. However, there are other
color encodings users can choose from (see Sec. 5.2).
5.1.1
Vector Normalization
While designing AttentionViz, we noticed two “free parameters,” which
can be varied without losing any information. Tuning these parameters
creates a closer relationship between embedding distance and attention
weights, and greatly improves the readability of the visualization. These
normalizations are applied prior to dimensionality reduction (Fig. 2).
Key Translation: Query and key vectors are sometimes well sep-
arated in our visualizations (Fig. 3, left).
This separation makes
it difﬁcult to directly compare query and key embeddings.
How-
ever, a simple mathematical trick allows us to move these embed-
dings closer together, without affecting the attention computation
for any given input sequence. In particular, note that the softmax
function is translation invariant: i.e., for any constant a, we have
softmaxj(x1 + a,x2 + a,...) = softmax j(x1,x2,...). Now, consider a
query vector x and key vectors y1,...,yn. For any vector v, we have:
attentionj(x) = softmax j(⟨x,y1⟩,⟨x,y2⟩,...)
= softmax j(⟨x,y1⟩+⟨x,v⟩,⟨x,y2⟩+⟨x,v⟩,...)
= softmax j(⟨x,y1 +v⟩,⟨x,y2 +v⟩,...)
where the second step follows by translation invariance. This implies
that without changing attention patterns in any given input, we can
translate all key vectors such that the query and key distributions for
each attention head have identical centroids. This makes it much easier
to compare queries and keys (Fig. 3, right).
Scaling Queries and Keys: In some transformers, such as GPT-2,
we observed cases where the average query norm was very different
from the average key norm. This difference makes it hard to interpret
key-query relations in a joint embedding. Mathematically, it indicates a
poor relationship between dot product and distance; visually, it means
queries might be a tiny cluster, surrounded by a loose cloud of keys.
sky
is
the
the
sky
blue
is
blue
Big distance
dot product
Low
Small distance
dot product
High
(a)
(b)
BERT, layer 4 head 11
distance
dot product
Fig. 4: (a) Ideal distance-attention relationship, where query-key pairs
with higher dot products are closer in the joint embedding space. (b)
Example attention head with a strong, negative correlation (-0.983)
between query-key distance and dot product in BERT.
Luckily, scale is another “free parameter” of the system. Attention
levels depend only on dot products of query and key vectors, so if we
scale all query vectors by a factor of c ̸= 0, and all key vectors by a
factor of c−1, the attention values are unchanged. This allows high-
attention query-key pairs to be closer together in our joint visualizations
as depicted in Fig. 4a. (A subtle point: on its own, scaling leaves
cosine distance unchanged; however, in combination with translation
normalization it has a nontrivial effect.)
To determine the optimal value of c, we can deﬁne a weighted
correlation metric that places heavier weight on query-key pairs with
smaller distances, since we care most about nearby queries and keys in
the joint visualization. We can thus choose a scale factor c such that
the weighted correlation between query-key dot products and distances
is maximized. This scaling method allows for the distances in the
joint embedding space to most accurately represent the actual attention
values between queries and keys.
5.1.2
Distance as a Proxy for Attention
As explained above, ideally, if a query-key pair has a large, positive dot
product (corresponding to a high ﬁnal attention value), they should be
placed closer together in the embedding space, and vice versa (Fig. 4a).
Thus, we expect distance to be inversely correlated with attention in our
joint query-key embeddings. We study this potential link by computing
the Spearman rank correlation between cosine distance and dot product
for each attention head in BERT, GPT-2, and ViT. We also experimented
with using Euclidean distance as our distance metric when creating
t-SNE and UMAP projections of queries and keys, but this generally
led to weaker distance-dot product correlations.
Across multiple datasets and models, the relationship between dis-
tance and attention holds fairly well. For example, with Wiki-Auto
data [18], the mean correlation between query-key distances and dot
products is -0.938 for BERT and -0.792 for GPT. An example result
from BERT is shown in Fig. 4b. On the set of COCO images used [23],
the mean correlation is -0.873 for ViT-32 and -0.884 for ViT-16.
5.2
Color Encodings
To visualize different properties of queries and keys, AttentionViz offers
various color encodings. The default option colors points by token type,
i.e., query or key. For vision transformers, users can color by image
patch row or column to visualize positional patterns (Fig. 10). Since
images encode their own color information, we also allow users to view
the original patches without additional styling elements (Fig. 8).
For language transformers, we support two positional color schemes:
normalized and discrete. To compute normalized position, we divide
each token’s position in a sentence by the sentence length to produce
a continuous color scale. Lighter hues denote tokens closer to the
beginning of the sentence (Fig. 5b). Our discrete position encoding
takes each token’s position and applies the modulo operator to get its
remainder when divided by 5. Thus, the 1st and 6th tokens receive the
same color, the 2nd and 7th tokens receive the same color, etc.. We use

Fig. 5: Connecting form to function in BERT. (a) In Matrix View, there are several spiral-shaped plots in layer 3. (b) By zooming into one such
head (L3 H9) using Single View, we can see positional attention patterns by using a light-to-dark color scheme that encodes position in the input
sequence. (c) These patterns can be conﬁrmed by exploring sentence-level visualizations.
Fig. 6: Exploring attention patterns with global search. (a) Heads with
fewer clusters of search results often demonstrate more semantic be-
havior, while heads with dispersed results focus more on token position.
(b) Zooming into L2 H6, a head with one main result cluster, we indeed
see a large group of semantically related query and key tokens.
the same ﬁve colors to encode queries and keys at different positions,
using darker hues for the former. In joint embeddings such as Fig. 11
(left), a discrete coloring is helpful in seeing relationships based on
small offsets in position (e.g., queries paying attention to keys one step
away). Users can color by query/key norm as well (Fig. 12a).
5.3
Views
AttentionViz provides three main interactive views for attention explo-
ration: Matrix View, Single View, and Sentence/Image View.
5.3.1
Matrix View
The initial view in AttentionViz is Matrix View, which uses small
multiples to visualize all the attention heads in a transformer at once
(Fig. 5a), directly addressing [T1] and [T3]. Each row corresponds to
a model layer, moving from earlier layers at the top of the interface
to later layers at the bottom. With this “global” perspective, users
can more easily scan for patterns across different transformer layers
and heads, compared with single-plot (e.g., [46]) or instance-level
visualizations (e.g., [3,39]). All the models used in this work had the
same architecture: 12 layers x 12 heads per layer = 144 attention heads
in total, but our system scales to other dimensions.
In Matrix View, users can view the joint query-key embeddings
created with t-SNE, UMAP, or PCA. They can also switch between
model types (i.e., BERT, GPT-2, ViT-16/32) or datasets [T4], explore
different color schemes, and view the resultant plots in 2D or 3D.
Matrix View supports a global search feature (Fig. 6a), which helps
highlight patterns in token locations across different heads and offers
another way to analyze attention at scale (see Sec. 7).
5.3.2
Single View
Users can click on any plot in Matrix View to zoom into Single View
(Fig. 5b), which affords exploration of a single attention head in closer
detail [T3]. Like Matrix View, users can switch between colorings,
dimensions, projection modes, datasets, and models in Single View
[T4]; all graphical changes sync between views to facilitate comparison.
The user can click on a point to highlight all tokens in the corresponding
input sequence, spotlighting the relevant queries and keys in our joint
embedding space. Users also have the option to project attention lines
onto the scatterplots, which connect query and key tokens (Fig. 5c).
We only show the top 2 attention weights for each token to enhance
readability. Our attention lines feature supports [T2] and offers a new
way to visualize attention patterns in transformers at the head level.
In Single View, users can also search for tokens and use our labelling
feature to uncover semantic patterns in the data, similar to [39]. For
instance, in Fig. 6b, search reveals that query/key tokens with similar
meanings are placed together in the joint embedding for this BERT
head, indicating strong attention between them (Sec. 5.1.2).
5.3.3
Sentence/Image View
Sentence/Image View allows for exploration of the ﬁne-grained attention
patterns within a single sentence or image [T2, T3]. Both views are

Fig. 7: ViT Image View. (a) Original image. (b) Transparency attention
heatmap. We highlight the selected token (query) with a green border.
(c) Overlaid attention arrows. (d) Global attention ﬂow. The square
icon means an image patch has the strongest attention connection with
the <CLS> token, which is not in the original image.
synchronized with Single View, matching the attention lines overlaid
on each query/key scatterplot for a smooth user experience.
Sentence View. When using BERT or GPT-2, users can click on a
point in Single View to open Sentence View in the left sidebar, which
displays a BertViz-inspired visualization of sentence-level attention
with the clicked token highlighted [46] (Fig. 5c). We also considered
using heatmap visualizations (e.g., [32]), but it seemed that the bipartite
graph approach would offer greater readability and ease of pattern
exploration for longer sentences. The opacity of the lines connecting
query tokens in the left column and key tokens in the right column
signiﬁes their corresponding attention strength. Hovering on a token
highlights token-speciﬁc attention lines. To reduce the noise from
classiﬁcation tokens and separators in BERT, or the ﬁrst token in GPT-2
(Sec. 7), users can hide the attention lines from these special tokens.
Other query and key tokens can also be toggled on/off, and all attention
lines will be re-normalized accordingly. Users have the option of
viewing the aggregate attention pattern for each attention head as well,
to offer another layer of comparison (Fig. 11a).
Image View. For image-based input in ViT, when users click on
an image patch, the side panel displays its corresponding original
image and highlights the clicked token with a colored border (Fig. 7a).
Users also see an image overlaid with an attention heatmap, where the
transparency indicates the attention weight between the clicked image
patch and other regions of the image (Fig. 7b).
Beyond visualizing the attention of a single token, Image View
allow users to explore the overall attention pattern within an image by
showing arrowed attention lines between different image patches. We
provide users with two options when visualizing the attention arrows.
The ﬁrst option overlays arrows on the top of original image patches,
with each arrow representing the strongest attention connection between
a starting image patch and destination patch (Fig. 7c). This creates a
simpliﬁed bipartite attention graph for users to characterize the most
important patterns within a speciﬁc head. The second option shows all
strong attention connections (i.e., attn(xi,xj) > 0.1) beside the original
image, offering a more comprehensive view of attention (Fig. 7d). In
this visualization, both opacity and line thickness are used to encode the
strength of attention connections. We also tried visualizing all weights
between queries and keys to more closely mirror [46], but this often
produced overcrowded, inscrutable results.
6
SYSTEM IMPLEMENTATION
To process model inputs and compute attention information, we use the
Hugging Face Transformers library and PyTorch. We use pre-trained
implementations of BERT, GPT-2 (small), and ViT-16/32 with model
weights from Google and OpenAI. For each NLP dataset, we randomly
Fig. 8: In ViT-32 Matrix View, we ﬁnd two interesting visual attention
heads: one head orders the black-and-white image tokens according
to brightness, while the other aligns the colorful patches based on hue.
Attention patterns shown in Image View conﬁrm attention ﬂow between
patches with the same luminance.
sample 200 sentences (∼10k tokens per attention head, including both
queries and keys). Due to the increased computational size of image
attention data, we display 10 images per head (1000 tokens) for ViT-32
and 4 images per head (1576 tokens) for ViT-16. After extracting query
and key vector embeddings for each attention head, we generate the
corresponding 2D/3D t-SNE, UMAP, and PCA coordinates (Sec. 5.1).
To produce semantic labels (e.g., “dog” or “background”) for image
patches in ViT, we use the DeepLabv3 segmentation model [6].
Our ﬁnal AttentionViz prototype consists of a Python/Flask backend
that communicates with a frontend written in Vue and Typescript. The
demo system is available at: http://attentionviz.com. Due to
the large size of the data and browser memory constraints, we load
pre-computed attention/projection information via JSON ﬁles through
the backend. For ViT, the backend also performs image manipulation
(e.g., patch highlighting and transparency adjustments) to display in
the frontend. We use Deck.gl to visualize the resultant query-key joint
embeddings. AttentionViz is highly extensible and model-agnostic,
allowing users to add new transformers and datasets to the system.
7
FINDINGS & EVALUATION
We illustrate the utility of AttentionViz with three application scenarios,
as well as feedback from domain experts. Our scenarios target the goals
in Sec. 4, and show how AttentionViz can offer insights about global
self-attention trends in vision and language transformers.
Data.
For BERT/GPT-2, we experimented with various NLP
datasets but focus on two for our application scenarios. We use Wiki-
Auto [18] as a baseline to sample general input sentences and Super-
GLUE AXb [49] to explore task-speciﬁc attention patterns for textual
entailment. For ViT, we sample images from ImageNet Large Scale
Visual Recognition Challenge [37] and Microsoft COCO: Common
Objects in Context [23], as well as synthetic image data.
User Interviews. We invited E2 and E3 for a second round of in-
terviews, and include two new experts, E6 (interpretability researcher)
and E7 (vision science Ph.D. student). As in Sec. 4, all experts were
interviewed individually. We ﬁrst gave experts a quick demo of our
tool and shared some of our own ﬁndings, asking them to share any
thoughts or insights (Sec. 7.1-7.3). Then, we asked for more general
feedback about the main strengths, weaknesses, and novelties of Atten-
tionViz (Sec. 7.4). We also asked experts about possible extensions or
applications of this technique for visualizing embeddings at scale.
7.1
Goal: Understanding Machine Visual Attention
AttentionViz can be especially helpful in uncovering insights about
attention in vision transformers due to the inherently visual nature of

Fig. 9: Left: Dataset of spatial patterns with different frequencies and angles. Center: In Single View, we observe that one attention head of
ViT-32 arranges image tokens based on the frequencies and angles of their spatial pattern. Right: The attention heatmap in Image View further
conﬁrms these ﬁndings – spatial patterns with similar angles pay greater attention to each other.
image patch data [G1].
Hue/brightness specializations in visual attention. We were curi-
ous if any visual attention heads specialize in either color-based and
brightness-based patterns. To test this, we provided the pre-trained ViT-
32 model with synthetic color and brightness gradient images (Fig. 8),
loading the resultant query and key tokens into AttentionViz.
Browsing global patterns in Matrix View, we identiﬁed two attention
heads that resemble color and colorless vision. One head appears
to align black-and-white image tokens based on brightness, and the
other aligns colorful patches based on hue. Our dataset contains color
and brightness gradient images in all orientations, and we see similar
patches cluster together in the joint embedding space regardless of their
position in the original images. The attention heatmap in Image View
conﬁrms these ﬁndings; tokens pay the most attention to other tokens
with the same color or brightness. E7 was intrigued by these results,
having previously studied the color latent space of convolutional neural
networks (CNNs), and expressed interest in using our tool to further
explore the differences between CNN and ViT behavior.
Frequency ﬁltering and angle detection. Frequencies and angles
are low-level characteristics of image data. To investigate if the vision
transformer has an attention head that associates visual patterns based
on these features, we created images of sinusoidal signals with varying
frequencies and orientations, processing them using our pretrained
ViT-32 model. Examining the resultant query and key embeddings
in Matrix View, we identiﬁed an attention head that separates image
tokens based on their spatial pattern’s frequencies (x-axis) and angles
(y-axis) (Fig. 9). With Image View, we observed that tokens in the
images of concentric circles are paying attention to other tokens with
similar curvatures, further conﬁrming that this attention head associates
visual patterns based on their angles.
E7 said this result was interesting, but not too surprising given our
hue/brightness ﬁndings, and was more curious about heads that do not
exhibit this “attend to similar patches” behavior. One experiment they
proposed was to study attention modulation, e.g., if the same image
patch (e.g., vertical stripes) occurs in different contexts in two images
(e.g., zebra vs. umbrella), do we see unique attention patterns?
Increasing attention distance across model layers.
As noted
in [12], self-attention attends more broadly across images in deeper
layers of vision transformers. We conﬁrmed this ﬁnding using our
interactive, joint visualizations of query and key tokens in AttentionViz.
With Matrix View, we colored patches by image “row” and “column” to
ﬁnd four attention heads in layers 1 and 2 of ViT-32 that group tokens
with their nearest spatial neighbors: on their left, right, top, and bottom.
In layers 3 and 4, we saw similar positional attention patterns, but
image tokens pay attention to all the patches in the same row or column,
beyond their nearest neighbors (Fig. 10). This suggests that unlike
CNNs, which process images using a square ﬁlter, the self-attention
mechanism in transformers often processes images row by row and
column by column, analogous to an elongated ﬁlter.
Fig. 10: Coloring image patches by row highlights positional attention
patterns in ViT-32. In Layer 1, tokens in the same row and adjacent
columns form small clusters. Image View reveals a look at left pattern.
In Layer 4, large clusters of tokens form based on row positions. Using
the arrowed lines, we see a wider, bidirectional attention ﬂow.
7.2
Goal: Finding Global Attention Traces
To understand how self-attention patterns vary across different heads in
language transformers [G2], we used AttentionViz to explore BERT.
Positional attention signatures. We observed several attention
heads with unique shapes, e.g., the spiral-shaped plots in layer 3
(Fig. 5a). For example, coloring layer 3 head 9 by normalized po-
sition in Single View reveals that token position increases as we move
from the outside to the inside of the spiral (Fig. 5b). We used Sentence
View to examine this pattern more closely (Fig. 5c), conﬁrming that
there is a positional, “next-token” attention pattern. This “spiral” also
reﬂects the initial ordering vector given to transformers (Sec. 2).
We then noticed other identiﬁable “traces” in Matrix View, ﬁnding
that plots with small “clumps” also encode positional patterns (Fig. 11,
left), which we veriﬁed with our discrete position coloring. The dif-
ference between “spirals” and “clumps” appears to be whether tokens
attend selectively to others one position away, versus at several different
possible positions (Fig. 5c). Similarly, we learned that in heads with
high query-key overlap, tokens typically attend to themselves and other
instances of the same token, exhibiting a “look at self” pattern. Zoom-
ing into these heads, we see clear semantic clusters of nearby query-key
pairs as shown in Fig. 6b, further supporting this observation.

Fig. 11: Other visual traces of attention. Left: Heads with small “clumps” often have even tighter positional patterns than spirals. Our discrete
position encoding, which colors each token based on its position modulo 5, highlights a “next-token” attention trend. Right: Layered bands of
queries and keys only appear with SuperGLUE AXb data [49], indicating strong attention to text start, end, and midpoint.
[24] shows that earlier transformer layers have the most information
about linear word order, aligning with our ﬁndings and previous work
such as [8, 46]. During our interviews, E2, E6, and E7 immediately
noticed these interesting geometries, particularly spirals, and were curi-
ous about how much of the observed structure is purely due to position.
This inspired several follow-up experiment ideas from experts, e.g.,
manipulating or removing the positional embeddings in transformer
models and seeing how our query-key visualizations change.
Task-speciﬁc traces. After visualizing multiple datasets with At-
tentionViz, we found that the shapes of joint embeddings are highly
consistent across different NLP tasks. However, we did see one visual
trace that only arises in some later layers of BERT with the SuperGLUE
AXb data (Fig. 11, right). Clicking on on such head (layer 8 head 9) and
coloring by position, we observed a query-key “sandwich,” where keys
and queries at the beginning of the text are stacked on top, followed by
queries and keys at the end of the text in reverse order.
Sentence View reveals that the start, middle, and end of the text
receive the most attention. The overall plot shape and attention pat-
tern suggests that these heads can identify a text’s “midpoint” and
differentiate between sentences, mirroring how in entailment tasks, two
sentences are compared to see if they have similar meanings. Queries
also mostly attend to keys in the same sentence. [20,47] shows how syn-
tactic and task-speciﬁc information is most prominent in mid-to-later
model layers, perhaps explaining the uniqueness of this trace.
Global search patterns. The aggregate search feature in Matrix
View can also be used to quickly scan for and compare attention trends
across heads [G2]. We found that patterns in the search results reﬂect
the previously identiﬁed visual attention traces (Fig. 6a). For example,
heads that are spiral-shaped or have small clumps of queries/keys have
more dispersed search results, indicative of their underlying positional
attention patterns. On the other hand, heads with the “look at self”
attention pattern only have one cluster of search results, emphasizing
the strong interaction between queries and keys of the same token.
Even if a joint query-key embedding does not have a distinctive
shape, we see that if there are only a few search result clusters, the
head may display more semantic behavior; otherwise, there is likely
a positional attention pattern. [41] notes that semantic information is
spread across BERT’s layers, which we conﬁrmed with AttentionViz.
All of our experts were particularly excited by this feature of our tool
and its ability to facilitate attention pattern comparisons.
7.3
Goal: Identifying Anomalies and Unexpected Behavior
Through interacting with the joint query-key embeddings in Attention-
Viz, we discovered some irregular model behaviors [G3].
Norm disparities and null attention. While exploring GPT-2 in
Matrix View, we observed that in early model layers, some query and
key clusters were well-separated, even after key translation (Sec. 5.1.1).
Fig. 12: Anomalies in GPT-2. (a) In early model layers, we witness
a signiﬁcant disparity between query-key norms for many attention
heads (e.g., L1 H8 prior to norm scaling). (b) Example of the prevalent
“attend to ﬁrst” pattern in later layers. Sentence View reveals latent
attention behavior after hiding the ﬁrst token.
By coloring by norm (as measured before the norm scaling step), we
saw that in many heads, there is a signiﬁcant disparity between the
norms of query and key vectors (Fig. 12a). When query norms are
small (i.e., light green), key norms tend to be large (i.e., dark pink), and
vice versa. Computing the average norm difference between queries
and keys in GPT-2 vs. BERT, we found that in the former, the mean
query norm - key norm = -4.59 across attention heads, while in the
latter, the mean difference is only 0.41. None of our experts could
explain this ﬁnding: “It doesn’t really make sense why queries and keys
would have such different norms” (E6). Interestingly, a paper published
after we made this observation [9] points to out-of-control query and
key norms as a cause of serious training instability, indicating that this
phenomenon may be worth studying further. This observation inspired
our scaling approach from Sec. 5.1.1 as well.
We also noticed that in many GPT-2 heads, most attention is directed
to the ﬁrst token (Fig. 12b), especially in later layers.
[47] brieﬂy
mentions that the ﬁrst token is treated as a null position for attention-
receiving in GPT-2 “when the linguistic property captured by the atten-
tion head doesn’t appear in the input text.” However, this phenomenon
remains underexplored, proposing another open interpretability ques-
tion to consider. E2 and E6 both noticed this anomalous behavior on
their own with our tool, and all of our experts were surprised by this

(a)
(b)
(c)
(d)
Fig. 13: Identifying a “look at self” attention head in ViT-32. (a) Single View shows queries and keys are sparsely distributed in the joint
embedding space. (b) Zooming in, query and key vectors of the same image token are tightly overlapped. (c) Image View reveals tokens pay most
attention to themselves. (d) Comparing the learned parameters of query and key projection layers conﬁrms that they learn redundant projections.
ﬁnding. [48] shows that pruning the majority of attention heads in
transformers may not signiﬁcantly impact model performance, which
perhaps can be partially attributed to this dominant null attention pat-
tern. Regardless, AttentionViz allows users to ﬁlter out attention paid
to the ﬁrst token, uncovering hidden query-key interactions.
“Look at self” attention heads. AttentionViz can also reveal sur-
prising attention patterns in vision transformers. In Matrix View, we
identiﬁed several heads in early layers of ViT-32 with very diffused
key-query clusters (Fig. 13a). Looking at one such attention head
(layer 0 head 8), we discovered that the query and key embeddings of
the same token form a small but dense cluster, with each query-key
pair well-separated from the others (Fig. 13b). From the transparency
heatmap in Image View, we see that the patch is solely attending to
itself (Fig. 13c). Switching to the arrowed attention lines, we discover
that the overall attention pattern for this image is “look at self,” where
no information is ﬂowing between image tokens in this head.
After identifying this irregular attention pattern, we checked the
learned parameters of the query and key matrices with a correlation
test. We found a strong similarity score (linear correlation = 0.94),
indicating that the query and key layers in this ViT head are indeed
learning redundant projections (Fig. 13d). E3 noted that this knowledge
could be used to inform model pruning experiments.
7.4
Takeaways from User Feedback
Merits of Matrix View. Several experts found the “global” perspective
provided by Matrix View to be the most novel and valuable part of
AttentionViz. As E6 said, “It’s great for quick comparison and frees
you from tuning hyperparameters when you want to visualize multiple
embeddings at once.” E7 also mentioned that Matrix View is useful
because “for smaller visualizations, I can just code up something myself,
but it’s a lot harder at scale and with more data.” These comments
suggest that this idea of visualizing and comparing embeddings at scale
may be beneﬁcial in other ML settings as well.
Applications for joint query-key embeddings. Experts proposed
various use cases and extensions for our visualization technique, evi-
dencing its wider applicability. For example, E2 suggested visualizing
patterns in untrained or corrupt transformers, and both E3 and E7
wanted to visualize changes in attention during training for their own
models, aligning with our original goals (Sec. 4). E2 and E6 also
suggested adapting our tool to help with causal tracing, explaining
that “it might be useful to track attention ﬂow throughout the model
for hypothesis testing.” Similarly, E3 expressed interest in looking
into “how two attention patterns connect in different heads,” which
could certainly be applied to visualizing induction head pairs. E2 noted
that adding a way to “quantify similarity between two heads” could be
useful, while E6 proposed “measuring or visualizing randomness in
heads” for model pruning purposes.
Embedding projections – to trust or not to trust? E3 highlighted
the challenges of using projection methods. While they appreciated the
striking geometric patterns (e.g., spirals) we found, E3 expressed some
skepticism about interpreting these visualizations due to the distortion
from techniques such as t-SNE and UMAP: “How do I know if I can
trust what I see?” This emphasizes the importance of tying visual
insights to actionable interventions, perhaps through augmenting our
tool to support hypothesis testing in addition to exploration.
Flexibility-usability tradeoff.
E2 indicated that AttentionViz
“feels very usable and customizable,” addressing their earlier concerns
about existing visualization tools (Sec. 4). However, some experts like
E6 were still worried that “showing all the features and heads might be
overwhelming... Is there a way to summarize the information? Or focus
more on a speciﬁc task?” E7 added, “I wonder if there’s a quicker, more
digestible way to label heads,” suggesting an approach closer to feature
visualization [31]. We designed AttentionViz to be a ﬂexible tool (e.g.,
allowing attention analysis in different transformers and at different
granularities), but it seems that the ﬂexibility-usability tradeoff [22] of
our design could still be improved.
Additional interaction modes. Some experts suggested additional
interaction modes, e.g., on-the-ﬂy inference (E3) or further dimension-
ality reductions on circled clusters of queries and keys to reveal addi-
tional information and perform ﬁne-grained analyses (E2). E7 stressed
the importance of allowing users to directly upload new datasets to the
system: “The tool could be even more powerful... people are going to
want to explore more with it like adding their own images.”
8
CONCLUSIONS & FUTURE WORK
In this work, we introduce a new technique for visualizing transformer
self-attention based on a joint embedding space for queries and keys.
We show that with proper normalization, distance in this space is a
reasonable mathematical approximation for attention weights. Visu-
alizing collections of queries and keys for multiple inputs makes it
possible to see distinctive patterns in different attention heads, which is
not easily achieved through existing visualization methods. For vision
transformers, we also create a simple 2D graph visualization to help
understand patterns of attention for a single image, extending the idea
of bipartite attention representations to the image domain.
Applying our technique, we create AttentionViz (demo: http://
attentionviz.com), an interactive visualization tool for exploring
attention patterns at scale. Through multiple application scenarios and
expert interviews, we show how our method can reveal insights about
attention in both language and vision transformers by probing query-
key interactions at different levels. Expert feedback provides evidence
for the utility of this technique and points to several avenues for future
work. For example, ﬁnding ways to manage the complexity of multiple
embedding visualizations and focus users on features of interest would
certainly be helpful. Allowing users to add new inputs on the ﬂy might
prove fruitful as well.
Another natural direction for future research is exploring how to
incorporate information from value vectors in each attention head [45].
These value vectors are an essential part of the attention mechanism,
though it is not clear how to visualize them in the context of queries
and keys. Finding the right visualization approach might shed more
light on how attention heads function. Finally, although AttentionViz
is an exploratory tool, adapting it for hypothesis testing and/or causal
tracing might provide support for practical model debugging.

ACKNOWLEDGMENTS
We would like to thank all the participants in our user interviews for
their time and invaluable insights. We are also grateful for the thought-
ful feedback and support provided by the members of the Harvard
Insight + Interaction Lab throughout this project.
REFERENCES
[1] E. Aﬂalo, M. Du, S.-Y. Tseng, Y. Liu, C. Wu, N. Duan, and V. Lal. Vl-
interpret: An interactive visualization tool for interpreting vision-language
transformers. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition, pp. 21406–21415, 2022. 2
[2] D. L. Arendt, N. Nur, Z. Huang, G. Fair, and W. Dou. Parallel embed-
dings: a visualization technique for contrasting learned representations.
In Proceedings of the 25th International Conference on Intelligent User
Interfaces, pp. 259–274, 2020. 3
[3] A. Boggust, B. Carter, and A. Satyanarayan. Embedding comparator:
Visualizing differences in global structure and local neighborhoods via
small multiples. In 27th International Conference on Intelligent User
Interfaces, pp. 746–766, 2022. 2, 3, 5
[4] S. Carter, Z. Armstrong, L. Schubert, I. Johnson, and C. Olah. Activation
atlas. Distill, 4(3):e15, 2019. 1
[5] H. Chefer, S. Gur, and L. Wolf. Transformer interpretability beyond
attention visualization. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, pp. 782–791, 2021. 3
[6] L.-C. Chen, G. Papandreou, F. Schroff, and H. Adam.
Rethinking
atrous convolution for semantic image segmentation.
arXiv preprint
arXiv:1706.05587, 2017. 6
[7] E. A. Chi, J. Hewitt, and C. D. Manning. Finding universal grammatical
relations in multilingual bert. arXiv preprint arXiv:2005.04511, 2020. 2
[8] K. Clark, U. Khandelwal, O. Levy, and C. D. Manning. What does bert
look at? an analysis of bert’s attention. arXiv preprint arXiv:1906.04341,
2019. 2, 8
[9] M. Dehghani, J. Djolonga, B. Mustafa, P. Padlewski, J. Heek, J. Gilmer,
A. Steiner, M. Caron, R. Geirhos, I. Alabdulmohsin, et al. Scaling vision
transformers to 22 billion parameters. arXiv preprint arXiv:2302.05442,
2023. 1, 8
[10] J. F. DeRose, J. Wang, and M. Berger. Attention ﬂows: Analyzing and
comparing attention mechanisms in language models. IEEE Transactions
on Visualization and Computer Graphics, 27(2):1160–1170, 2020. 2
[11] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. Bert: Pre-training of
deep bidirectional transformers for language understanding. arXiv preprint
arXiv:1810.04805, 2018. 1, 2
[12] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Un-
terthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, et al. An image
is worth 16x16 words: Transformers for image recognition at scale. arXiv
preprint arXiv:2010.11929, 2020. 1, 2, 7
[13] A. Ghiasi, H. Kazemi, E. Borgnia, S. Reich, M. Shu, M. Goldblum, A. G.
Wilson, and T. Goldstein. What do vision transformers learn? a visual
exploration. arXiv preprint arXiv:2212.06727, 2022. 3
[14] F. Hohman, M. Kahng, R. Pienta, and D. H. Chau. Visual analytics in deep
learning: An interrogative survey for the next frontiers. IEEE transactions
on visualization and computer graphics, 25(8):2674–2693, 2018. 2
[15] B. Hoover, H. Strobelt, and S. Gehrmann. exbert: A visual analysis tool
to explore learned representations in transformers models. arXiv preprint
arXiv:1910.05276, 2019. 1, 2
[16] T. Jaunet, C. Kervadec, R. Vuillemot, G. Antipov, M. Baccouche, and
C. Wolf. Visqa: X-raying vision and language reasoning in transformers.
IEEE Transactions on Visualization and Computer Graphics, 28(1):976–
986, 2021. 2
[17] X. Ji, Y. Tu, W. He, J. Wang, H.-W. Shen, and P.-Y. Yen. Usevis: Visual
analytics of attention-based neural embedding in information retrieval.
Visual Informatics, 5(2):1–12, 2021. 2
[18] C. Jiang, M. Maddela, W. Lan, Y. Zhong, and W. Xu.
Neural crf
model for sentence alignment in text simpliﬁcation.
arXiv preprint
arXiv:2005.02324, 2020. 1, 4, 6
[19] I. T. Jolliffe. Principal Components in Regression Analysis, pp. 129–155.
Springer New York, New York, NY, 1986. doi: 10.1007/978-1-4757-1904
-8_8 4
[20] O. Kovaleva, A. Romanov, A. Rogers, and A. Rumshisky. Revealing the
dark secrets of bert. arXiv preprint arXiv:1908.08593, 2019. 2, 8
[21] Q. Li, K. S. Njotoprawiro, H. Haleem, Q. Chen, C. Yi, and X. Ma. Embed-
dingvis: A visual analytics approach to comparative network embedding
inspection. In 2018 IEEE Conference on Visual Analytics Science and
Technology (VAST), pp. 48–59. IEEE, 2018. 3
[22] W. Lidwell, K. Holden, and J. Butler. Universal principles of design,
revised and updated: 125 ways to enhance usability, inﬂuence perception,
increase appeal, make better design decisions, and teach through design.
Rockport Pub, 2010. 9
[23] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dollár,
and C. L. Zitnick. Microsoft coco: Common objects in context. In Com-
puter Vision–ECCV 2014: 13th European Conference, Zurich, Switzerland,
September 6-12, 2014, Proceedings, Part V 13, pp. 740–755. Springer,
2014. 4, 6
[24] Y. Lin, Y. C. Tan, and R. Frank. Open sesame: getting inside bert’s
linguistic knowledge. arXiv preprint arXiv:1906.01698, 2019. 8
[25] S. Liu, T. Li, Z. Li, V. Srikumar, V. Pascucci, and P.-T. Bremer. Visual
interrogation of attention-based models for natural language inference and
machine comprehension. Technical report, Lawrence Livermore National
Lab.(LLNL), Livermore, CA (United States), 2018. 1, 2
[26] J. Ma, Y. Bai, B. Zhong, W. Zhang, T. Yao, and T. Mei. Visualizing and
understanding patch interactions in vision transformer. arXiv preprint
arXiv:2203.05922, 2022. 2
[27] L. McInnes, J. Healy, and J. Melville. Umap: Uniform manifold ap-
proximation and projection for dimension reduction.
arXiv preprint
arXiv:1802.03426, 2018. 4
[28] P. Mehrani and J. K. Tsotsos. Self-attention in vision transformers per-
forms perceptual grouping, not attention. arXiv preprint arXiv:2303.01542,
2023. 2
[29] A. Miaschi, D. Brunato, F. Dell’Orletta, and G. Venturi. Linguistic proﬁl-
ing of a neural language model. arXiv preprint arXiv:2010.01869, 2020.
2
[30] M. M. Naseer, K. Ranasinghe, S. H. Khan, M. Hayat, F. Shahbaz Khan,
and M.-H. Yang. Intriguing properties of vision transformers. Advances
in Neural Information Processing Systems, 34:23296–23308, 2021. 2
[31] C. Olah, A. Mordvintsev, and L. Schubert. Feature visualization. Distill,
2(11):e7, 2017. 9
[32] C. Park, I. Na, Y. Jo, S. Shin, J. Yoo, B. C. Kwon, J. Zhao, H. Noh, Y. Lee,
and J. Choo. Sanvis: Visual analytics for understanding self-attention
networks. In 2019 IEEE Visualization Conference (VIS), pp. 146–150.
IEEE, 2019. 2, 6
[33] N. Park and S. Kim. How do vision transformers work? arXiv preprint
arXiv:2202.06709, 2022. 2
[34] A. Radford, K. Narasimhan, T. Salimans, I. Sutskever, et al. Improving
language understanding by generative pre-training. OpenAI blog, 2018. 2
[35] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever, et al.
Language models are unsupervised multitask learners. OpenAI blog,
1(8):9, 2019. 1, 2
[36] E. Reif, A. Yuan, M. Wattenberg, F. B. Viegas, A. Coenen, A. Pearce, and
B. Kim. Visualizing and measuring the geometry of bert. Advances in
Neural Information Processing Systems, 32, 2019. 2
[37] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang,
A. Karpathy, A. Khosla, M. Bernstein, et al. Imagenet large scale visual
recognition challenge. International journal of computer vision, 115:211–
252, 2015. 6
[38] R. Sevastjanova, E. Cakmak, S. Ravfogel, R. Cotterell, and M. El-Assady.
Visual comparison of language model adaptation. IEEE Transactions on
Visualization and Computer Graphics, 29(1):1178–1188, 2022. 3
[39] D. Smilkov, N. Thorat, C. Nicholson, E. Reif, F. B. Viégas, and M. Wat-
tenberg. Embedding projector: Interactive visualization and interpretation
of embeddings. arXiv preprint arXiv:1611.05469, 2016. 2, 5
[40] H. Strobelt, S. Gehrmann, M. Behrisch, A. Perer, H. Pﬁster, and A. M.
Rush. Seq2seq-vis: A visual debugging tool for sequence-to-sequence
models.
IEEE transactions on visualization and computer graphics,
25(1):353–363, 2018. 2
[41] I. Tenney, D. Das, and E. Pavlick. Bert rediscovers the classical nlp
pipeline. arXiv preprint arXiv:1905.05950, 2019. 8
[42] I. Tenney, J. Wexler, J. Bastings, T. Bolukbasi, A. Coenen, S. Gehrmann,
E. Jiang, M. Pushkarna, C. Radebaugh, E. Reif, et al. The language
interpretability tool: Extensible, interactive visualizations and analysis for
nlp models. arXiv preprint arXiv:2008.05122, 2020. 2
[43] L. Van der Maaten and G. Hinton. Visualizing data using t-sne. Journal of
machine learning research, 9(11), 2008. 4
[44] A. Vaswani, S. Bengio, E. Brevdo, F. Chollet, A. N. Gomez, S. Gouws,
L. Jones, Ł. Kaiser, N. Kalchbrenner, N. Parmar, et al. Tensor2tensor for
neural machine translation. arXiv preprint arXiv:1803.07416, 2018. 1, 2

[45] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
Ł. Kaiser, and I. Polosukhin. Attention is all you need. Advances in neural
information processing systems, 30, 2017. 1, 2, 9
[46] J. Vig. Bertviz: A tool for visualizing multihead self-attention in the bert
model. In ICLR Workshop: Debugging Machine Learning Models, 2019.
1, 2, 5, 6, 8
[47] J. Vig and Y. Belinkov. Analyzing the structure of attention in a transformer
language model. arXiv preprint arXiv:1906.04284, 2019. 8
[48] E. Voita, D. Talbot, F. Moiseev, R. Sennrich, and I. Titov. Analyzing
multi-head self-attention: Specialized heads do the heavy lifting, the rest
can be pruned. arXiv preprint arXiv:1905.09418, 2019. 9
[49] A. Wang, Y. Pruksachatkun, N. Nangia, A. Singh, J. Michael, F. Hill,
O. Levy, and S. Bowman. Superglue: A stickier benchmark for general-
purpose language understanding systems. Advances in neural information
processing systems, 32, 2019. 6, 8
[50] K. Wang, A. Variengien, A. Conmy, B. Shlegeris, and J. Steinhardt. Inter-
pretability in the wild: a circuit for indirect object identiﬁcation in gpt-2
small. arXiv preprint arXiv:2211.00593, 2022. 2
[51] Y. Wang, S. Liu, N. Afzal, M. Rastegar-Mojarad, L. Wang, F. Shen,
P. Kingsbury, and H. Liu. A comparison of word embeddings for the
biomedical natural language processing. Journal of biomedical informat-
ics, 87:12–20, 2018. 2
[52] Z. J. Wang, R. Turko, and D. H. Chau. Dodrio: Exploring transformer
models with interactive visualization. arXiv preprint arXiv:2103.14625,
2021. 2
[53] J. Wei, Y. Tay, R. Bommasani, C. Raffel, B. Zoph, S. Borgeaud, D. Yo-
gatama, M. Bosma, D. Zhou, D. Metzler, et al. Emergent abilities of large
language models. arXiv preprint arXiv:2206.07682, 2022. 1
[54] S. Yang, Z. Quan, M. Nie, and W. Yang. Transpose: Keypoint localization
via transformer. In Proceedings of the IEEE/CVF International Conference
on Computer Vision, pp. 11802–11812, 2021. 3

