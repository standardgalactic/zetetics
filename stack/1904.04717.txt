Label Propagation for Deep Semi-supervised Learning
Ahmet Iscen1
Giorgos Tolias1
Yannis Avrithis2
Ondˇrej Chum1
1VRG, FEE, Czech Technical University in Prague
2Univ Rennes, Inria, CNRS, IRISA
Abstract
Semi-supervised learning is becoming increasingly im-
portant because it can combine data carefully labeled by
humans with abundant unlabeled data to train deep neu-
ral networks. Classic methods on semi-supervised learn-
ing that have focused on transductive learning have not
been fully exploited in the inductive framework followed by
modern deep learning. The same holds for the manifold
assumption—that similar examples should get the same pre-
diction. In this work, we employ a transductive label prop-
agation method that is based on the manifold assumption to
make predictions on the entire dataset and use these predic-
tions to generate pseudo-labels for the unlabeled data and
train a deep neural network. At the core of the transductive
method lies a nearest neighbor graph of the dataset that
we create based on the embeddings of the same network.
Therefore our learning process iterates between these two
steps. We improve performance on several datasets espe-
cially in the few labels regime and show that our work is
complementary to current state of the art.
1. Introduction
Modern approaches to many computer vision problems
exploit deep neural networks. These are popular for being
very efﬁcient and providing great performance at test time.
The downside is a requirement of large amounts of training
examples, which are labeled either by humans or automati-
cally on proxy tasks.
Visual data are available in large quantities, however,
data reliably annotated by humans are still very scarce. Ob-
taining large amounts of annotated training data for every
single task is not only impractical, potentially costly, but it
also turns out to be error prone. The low quality of crowd-
sourced annotation is a common motivation to minimize the
need of annotation.
In the domain of metric learning, promising results have
been recently achieved by unsupervised methods for either
learning from scratch or ﬁne-tuning a supervised network
for domain adaptation, which devise proxy tasks for learn-
ing. These tasks exploit the distribution of data in the orig-
inal space, for instance pairwise relations of training ex-
Figure 1. Label propagation on manifolds toy example. Triangles
denote labeled, and circles un-labeled training data, respectively.
Top: color-coded ground truth for labeled points, and gray color
for unlabeled points. Bottom: color-coded pseudo-labels inferred
by diffusion that are used to train the CNN. The size reﬂects the
certainty of the pseudo-label prediction.
amples [42], relations between examples and cluster cen-
troids [1], or considering the manifold structure of data [19].
Alternatively, in self-supervised learning, one can take ad-
vantage of additional information like spatial layout in im-
ages [5, 12] or temporal relation in videos [40, 28]; or mine
for such information in unstructured data by algorithmic su-
pervision using conventional methods [13, 30]. However,
most such proxy tasks are inferior when directly compared
to laboriously annotated data by humans.
In classiﬁcation, semi-supervised methods attempt to re-
duce the number of labeled examples, whereby the fully su-
pervised performance on all data acts as an upper bound.
In transductive learning [43, 45], label inference restricted
to a given set of unlabeled examples is of interest. In in-
ductive learning, the goal is generalization to new unseen
data, while the original training data are discarded. This
is achieved e.g. by combining classiﬁcation loss on labeled
1
arXiv:1904.04717v1  [cs.CV]  9 Apr 2019

data with unsupervised objectives on all data, where the
latter act as regularization [41, 38]. Or, an existing clas-
siﬁer can be used to assign pseudo-labels [24, 35], which
is another form of algorithmic supervision. Using a pow-
erful classiﬁer trained on carefully annotated data can pro-
vide high-quality pseudo-labels, opening the door to learn-
ing from real unlabeled, large scale data. In such omni-
supervised learning [31], the fully supervised performance
on the labeled part is actually the lower bound. This only
refreshes the interest in inductive semi-supervised methods.
In this paper, we use efﬁcient transductive label propa-
gation [43] to infer pseudo-labels for unlabeled data, which
are used to train the classiﬁer. Label propagation is a graph-
based method, and in this work the graph is constructed ex-
ploiting the embeddings obtained by the classiﬁcation net-
work itself. Thus, the proposed method alternates between
two steps. First, the network is trained from labeled and
pseudo-labeled data. The second step uses the embeddings
of the network trained in the previous step to construct a
nearest neighbor graph. Label propagation is then used to
infer pseudo-labels for unlabeled images, as well as a cer-
tainty score per image and per class. Training is performed
on all data, using certainty-based weights.
We experimentally show on standard datasets that the
proposed method outperforms other semi-supervised ap-
proaches. The less labeled data is available, the more pro-
nounced the advantage of the proposed approach is.
2. Related work
The literature is rich in the problem of semi-supervised
learning (SSL). The reader is advised to see [3] for an ex-
tensive overview. The same holds for SSL in image classi-
ﬁcation [10, 16, 4, 37]. In this section, we mostly restrict
the discussion to approaches that use deep learning for SSL
and perform the training on a large image collection with
mini-batch optimization.
Prior work on semi-supervised deep learning for image
classiﬁcation is divided into two main categories. The ﬁrst
consists of methods, e.g. [15, 23, 34, 38], that add an un-
supervised loss term (often called a regularizer) into the
loss function. This term is applied to either all images or
only the unlabeled ones. Methods in the second category,
e.g. [24, 36], assign pseudo-labels to the unlabeled exam-
ples. The pseudo-labeled data are then used in training with
a supervised loss, such as cross entropy. Both categories use
a standard loss term that is trained with supervision from
labeled images. A thorough evaluation of SSL deep image
classiﬁcation can be found in Miyato et al. [27].
Our contribution belongs to the second category, and
is conceptually and implementation-wise orthogonal to the
ﬁrst. It is therefore straightforward to combine the proposed
method with any method from the ﬁrst category. We do
combine it with [38] as shown in Section 5.
Unsupervised loss in deep SSL. Assuming that every train-
ing image, labeled or not, belongs to a single category, a
natural requirement on the classiﬁer is to make a conﬁdent
prediction on the training set. This idea was formalized by
Sajjadi et al. [35], where the regularizer is designed to min-
imize the entropy of the network output. Such a loss term is
easily combined with other terms. A similar combination is
performed for denoising auto-encoders that are applied on
all images in an unsupervised manner [32].
A direction attracting a lot of attention is that of consis-
tency loss, where two related cases, e.g. coming from two
similar images, or made by two networks with related pa-
rameters, are encouraged to have similar network outputs.
Sajjadi et al. [34] is the ﬁrst, to our knowledge, to use a
consistency loss between the outputs of a network on ran-
dom perturbations of the same image. Laine and Aila [23]
rather apply consistency between the output of the current
network and the temporal average of outputs during train-
ing. The state-of-the-art mean teacher (MT) method [38]
replaces output averaging by averaging of network param-
eters. Consistency loss is commonly measured by squared
Euclidean distance. The Jensen-Shannon divergence is used
instead by Qiao et al. [29], while complementarity of the
two networks is enforced via adversarial examples. A simi-
lar idea is proposed by Miyato et al. [26].
Pseudo-labeling in deep SSL. Lee [24] uses the current
network to infer pseudo-labels of unlabeled examples, by
choosing the most conﬁdent class. These pseudo-labels are
treated like human-provided labels in the cross entropy loss.
Its impact is similar to that of entropy minimization [35]; in
both cases the network is forced to have more conﬁdent pre-
dictions. The same principle is adopted by Shi et al. [36],
where the authors further add contrastive loss to the con-
sistency loss. Our method is different from all such prior
work in that pseudo-labels are inferred by label propagation
rather than network predictions.
Label propagation has been extensively used in a transduc-
tive setup (see chapter 11 [3]). Recently, Douze et al. [7]
perform label propagation on a large image dataset with
CNN descriptors for few shot learning. Unseen images are
classiﬁed via online label propagation, which requires stor-
ing the entire dataset, while the network is trained in ad-
vance and descriptors are ﬁxed. Our work is different in
that we perform label propagation on the training set off-
line while training the network, such that inference is pos-
sible without accessing the original training set. Learning
by association [17] can been seen as two steps of propaga-
tion on a constrained bi-partite graph between labeled and
unlabeled examples. Graph transduction game (GTG) [9],
a form of label propagation, has been used for pseudo-
labels [8] as in our work, but in this case the network is
pre-trained, the graph remains ﬁxed and there is no weight-
ing mechanism. We compare to this approach in Section 5.

3. Preliminaries
In this section we formulate the semi-supervised learn-
ing problem and then we discuss the classiﬁer, different loss
functions that are commonly used in prior work, and ﬁnally
a transductive learning approach that our method is based
on. In our experiments we use a convolutional neural net-
work (CNN) to perform image classiﬁcation, but this for-
mulation applies to any network architecture in any domain.
Problem formulation. We assume a collection of n ex-
amples X := (x1, . . . , xl, xl+1, . . . , xn) with xi ∈X.
The ﬁrst l examples xi for i ∈L := {1, . . . , l}, denoted
by XL, are labeled according to YL := (y1, . . . , yl) with
yi ∈C, where C := {1, . . . , c} is a discrete label set
for c classes. The remaining u := n −l examples xi for
i ∈U := {l + 1, . . . , n}, denoted by XU, are unlabeled.
The goal in SSL is to use all examples X and labels YL
to train a classiﬁer that maps previously unseen samples to
class labels.
Classiﬁer. The network takes an input example from X and
produces a vector of class conﬁdence scores. We denote it
by fθ : X →Rc, where θ are the network parameters. It
is conceptually divided in two parts. The ﬁrst is a feature
extraction network φθ : X →Rd mapping the input to a
feature vector, or descriptor. We denote the descriptor of
the i-th example by vi := φθ(xi). The second typically
consists of a fully connected (FC) layer applied on top of
φθ and followed by softmax, producing a vector of conﬁ-
dence scores. Function fθ is the mapping from input space
directly to conﬁdence scores. The output of the network for
the i-th example is fθ(xi) and the prediction is the one of
maximum conﬁdence score
ˆyi := arg max
j
fθ(xi)j,
(1)
where subscript j denotes the j-th dimension of the vector.
Supervised loss.
In supervised learning, the network is
trained by minimizing a supervised loss term of the form
Ls(XL, YL; θ) :=
l
X
i=1
ℓs (fθ(xi), yi) ,
(2)
which applies only to labeled examples in XL. Such term
is part of the total loss when training a network in a semi-
supervised setup [36, 38, 29]. A standard choice for the
loss function ℓs in classiﬁcation is cross-entropy, given by
ℓs(s, y) := −log sy for s ∈Rc and y ∈C.
Pseudo-labeling is the process of assigning a pseudo-label
ˆyi to each example xi for i ∈U. Denoting by ˆYU :=
(ˆyl+1, . . . , ˆyn) the collection of pseudo-labels for XU, the
following additional pseudo-label loss term applies
Lp(XU, ˆYU; θ) :=
n
X
i=l+1
ℓs (fθ(xi), ˆyi) ,
(3)
where again ℓs is any supervised loss function like cross-
entropy. An example is the approach proposed by Lee [24],
who ﬁrst train network fθ with (2) and then assign pseudo-
labels according to (1) for i ∈U.
Unsupervised loss is another common alternative where
the loss function applies to both labeled and unlabeled ex-
amples and encourages consistency under different trans-
formations of the data or the network. The so-called consis-
tency loss [36, 38, 36] is deﬁned as
Lu(X; θ) :=
n
X
i=1
ℓu(fθ(xi), f˜θ(˜xi)),
(4)
where ˜xi refers to a different transformation of example xi.
Note that according to the standard practice of data augmen-
tation, every forward pass of xi during training is performed
under some random transformation. Parameter set ˜θ is ei-
ther equal to θ or any other transformation of it, such as a
moving average over the sequence of network updates [38].
A simple choice of ℓu is the squared Euclidean distance, i.e.
ℓu(s,˜s) := ||s −˜s)||2 for s,˜s ∈Rc, forcing the two outputs
to be as close as possible.
Transductive learning solves a more speciﬁc problem. In-
stead of training a generic classiﬁer able to classify new,
yet unseen, examples, the goal is to use X and YL to in-
fer labels for examples in XU. In this work, we adopt the
graph-based approach of Zhou et al. [43] for transductive
learning by diffusion1.
Diffusion for transductive learning [43].
Let V
=
(v1, . . . , vl, vl+1, . . . , vn) be the descriptor set, where vi
corresponds to xi as deﬁned earlier. A symmetric adjacency
matrix W ∈Rn×n with zero diagonal is constructed, whose
elements wij are non-negative pairwise similarities between
vi and vj.
Its symmetrically normalized counterpart is
given by W = D−1/2WD−1/2, where D := diag(W1n)
is the degree matrix and 1n is the all-ones n-vector. A n×c
label matrix Y is deﬁned with elements
Yij :=
 1,
if i ∈L ∧yi = j
0,
otherwise.
(5)
That is, the rows of Y corresponding to labeled examples
are one-hot encoded labels and the rest are zero. Diffusion
amounts to computing the n × c matrix
Z := (I −αW)−1Y,
(6)
where α ∈[0, 1) is a parameter. Finally, the class prediction
for an unlabeled example xi is
ˆyi := arg max
j
zij,
(7)
where zij is the (i, j) element of matrix Z.
1We ﬁrst present the original approach and discuss our design choices
in the following section.

It is interesting to observe that matrix Z as deﬁned by (6)
is the minimizer of the following quadratic cost function
J(Z) := α
2
n
X
i,j=1
wij

zi
√dii
−
zj
p
djj

2
+(1−α) ∥Y −Z∥2
F ,
(8)
where zi is the i-th row of matrix Z, dii is the i-th diago-
nal diagonal element of D and ∥·∥F is the Frobenius norm.
The ﬁrst term encourages smoothness such that nearby ex-
amples get the same predictions, while the second attempts
to maintain predictions for the labeled examples [43].
4. Method
In the following, we begin by providing an overview of
our approach. We then develop the main elements of our so-
lution, put everything together in a concrete algorithm, and
discuss how our approach is complementary to approaches
using unsupervised loss for SSL [38, 36, 36]. Finally, we
discuss the relation to prior work that encourages smooth-
ness in deep networks.
Overview. We introduce a new iterative process for semi-
supervised learning that can be summarized as follows.
First, we construct a nearest neighbor graph and perform
label propagation by transductive learning on the training
set. Then, we estimate of a weight reﬂecting the uncertainty
of label propagation for each unlabeled example. Finally,
we inject the obtained labels into the network training pro-
cess. These ideas are developed below, while a graphical
overview of the proposed approach is shown in Figure 2.
Nearest neighbor graph.
Given a network with pa-
rameters θ,
we construct the descriptor set V
=
(v1, . . . , vl, vl+1, . . . , vn), where vi := φθ(xi). A sparse
afﬁnity matrix A ∈Rn×n with elements
aij :=
(
[v⊤
i vj]γ
+,
if i ̸= j ∧vi ∈NNk(vj)
0,
otherwise
(9)
is constructed, where NNk denotes the set of k nearest
neighbors in X, and γ is a parameter following recent work
on manifold-based search [20]. Note that constructing the
afﬁnity matrix of the nearest neighbor graph is efﬁcient even
for large n [20], while constructing the full afﬁnity matrix
as in Zhou et al. is not tractable. Then, let W := A + A⊤,
which is indeed a symmetric nonnegative adjacency matrix
with zero diagonal.
Label propagation. Estimating matrix Z by (6) is imprac-
tical for large n because the inverse matrix (I −αW)−1 is
not sparse. We rather use the the conjugate gradient (CG)
method to solve linear system
(I −αW)Z = Y,
(10)
which applies because matrix (I −αW) is positive-deﬁnite.
This solution is known to be faster than the iterative solution
of Zhou et al. [43], and has been used in semi-supervised
learning [44], interactive image segmentation [14], image
retrieval [20] and semantic image segmentation [2]. Finally,
we infer the pseudo-labels ˆYU = (ˆyl+1, . . . , ˆyn), where ˆyi
is given by (7).
Pseudo-label certainty and class balancing.
Inferring
pseudo-labels from matrix Z by hard assignment has two
undesired effects: ﬁrst, we deﬁne pseudo-labels on all un-
labeled examples while clearly we do not have the same
certainty for each example. Second, pseudo-labels may not
be balanced over classes, which will impede learning.
To deal with the former issue we associate with each
pseudo-label a weight reﬂecting the certainty of the predic-
tion. We use entropy, as a measure of uncertainty, to assign
weight ωi to example xi, deﬁned by
ωi := 1 −H(ˆzi)
log(c),
(11)
where ˆZ is the row-wise normalized counterpart of Z, i.e.
ˆzij = zij/ P
k zik, and function H : Rc →R is the entropy
function. Weight ωi is normalized in [0, 1] because log(c)
is the maximum possible entropy in Rc.
To deal with the latter issue of class imbalance, we assign
weight ζj to class j that is inversely proportional to class
population, deﬁned as ζj := (|Lj| + |Uj|)−1, where Lj
(resp. Uj) are the examples labeled (resp. pseudo-labeled)
as class j.
Given the above deﬁnitions of per-example and per-class
weights, we associate the following weighted loss to the la-
beled and pseudo-labeled examples
Lw(X, YL, ˆYU; θ) :=
l
X
i=1
ζyiℓs (fθ(xi), yi)
+
n
X
i=l+1
ωiζˆyiℓs (fθ(xi), ˆyi) ,
(12)
which is the sum of weighted versions of Ls (2) and Lp (3).
In contrast to (3), pseudo-labels originate in diffusion rather
than network predictions.
A toy example showing the result of label propagation
and the estimated weights is shown in Figure 3.
Iterative training. Given the above deﬁnitions of nearest
neighbor graph deﬁnition, label propagation, example/class
weighting and pseudo-label loss, we plug those components
into an iterative learning process. We begin by randomly
initializing the network parameters θ and we train the net-
work for T epochs in a fully supervised manner on the l
labeled examples XL using the supervised loss term (2).
The trained network then provides the starting point for the

Feature extractor φθ
FC + softmax
Network fθ
Phase 1:
Train for T epochs with
Ls(XL, YL; θ)
(labeled examples only)
Train for 1 epoch with
Lw(X, YL, ˆYU; θ)
(all examples)
Extract descriptors V
Compute afﬁnity A (9)
W ←A + A⊤
W ←D−1/2W D−1/2
Use φθ
Solve (10)
Label propagation
Phase 2: Iterate T ′ times
: labels
: missing labels
: pseudo-labels (size proportional to certainty ωi)
Figure 2. Overview of the proposed approach. Starting from a randomly initialized network, we ﬁrst train it in a supervised fashion on
the labeled examples. Then we initiate an iterative process where at each iteration we compute a nearest neighbor graph of the entire
training set in the feature space of the current network, we propagate labels by transductive learning, and then we train the network on the
entire training set, with true labels or pseudo-labels on the labeled or unlabeled examples respectively. The pseudo-labels are weighted per
example and per class according to prediction certainty and inverse class population, respectively.
1 labeled example
3 labeled examples
10 labeled examples
Figure 3. Toy example with 300 examples demonstrating label propagation for different number of labeled examples. Triangle markers
correspond the labeled examples and circles to the unlabeled ones which are ﬁnally pseudo-labeled by label propagation. The class is
color-coded and the size of the circles corresponds to weight ωi. The true labels are the same as the example of Figure 1 (top).
following iterative process. First, we extract descriptors V
on the entire training set X and compute nearest neighbors
to construct the adjacency matrix W. Second, we perform
label propagation by solving linear system (10) and assign
pseudo-labels to unlabeled examples XU by (7). Finally,
we train the network for one epoch on the entire training
set X using the weighted loss Lw (12). We repeat this it-
erative process for T ′ epochs. The above is summarized in
Algorithm 1.
Procedure OPTIMIZE() refers to the mini-batch opti-
mization of the corresponding loss term for one epoch, i.e.
all examples are fed to the network once. More details about
batch construction are given in the implementation details.
Combination with other approaches.
Our contribution
falls in the case of pseudo-label loss in the form of (3). It is
orthogonal to approaches that use unsupervised loss, for in-
stance (4), applied to both labeled and unlabeled examples.
Combination of the two comes in a straightforward way by
adding term (4) to the total loss optimized in lines 4 and 16
of Algorithm 1. This is exactly the way we combine the
proposed approach with the state-of-the-art Mean-Teacher
approach [38] in our experiments.
Discussion. In an inductive framework, if zi/√dii is re-
placed by the network output fθ(xi) in the smoothness
term of (8), then this becomes an unsupervised loss term,
e.g. like (4), only now it encourages consistency between
nearby example predictions. And indeed such solution is
adopted e.g. by Weston et al. [41]. This is not very efﬁ-
cient because the adjacency matrix is typically sparse with
non-zero-elements only on nearest neighbors, and then the
gradient of the smoothness term will propagate from each
example to its neighbors only at each iteration.

Algorithm 1 Label propagation for deep SSL
1: procedure LPDSSL(Training examples X, labels YL)
2:
θ ←initialize randomly
3:
for epoch ∈[1, . . . , T] do
4:
θ ←OPTIMIZE(Ls(XL, YL; θ))
▷mini-batch optimization
5:
end for
6:
for epoch ∈[1, . . . , T ′] do
7:
for i ∈{1, . . . , n} do vi ←φθ(xi)
▷extract descriptors
8:
for (i, j) ∈{1, . . . , n}2 do aij ←afﬁnity values (9)
9:
W ←A + A⊤
▷symmetric afﬁnity
10:
W ←D−1/2WD−1/2
▷symmetrically normalized afﬁnity
11:
Z ←solve (10) with CG
▷diffusion
12:
for (i, j) ∈U × C do ˆzij ←zij/ P
k zik
▷normalize Z
13:
for i ∈U do ˆyi ←arg maxj ˆzij
▷pseudo-label
14:
for i ∈U do ωi ←certainty of ˆyi (11)
▷pseudo-label weight
15:
for j ∈C do ζj ←(|Lj| + |Uj|)−1
▷class weight/balancing
16:
θ ←OPTIMIZE(Lw(X, YL, ˆYU; θ))
▷mini-batch optimization
17:
end for
18: end procedure
Our main idea therefore is that instead of just encour-
aging nearby examples to get the same predictions, we en-
courage all examples to get predictions same as the ones
we would get by transductive learning according to the
quadratic cost (8) and its solution Z (6). Computing Z is
efﬁcient because it is performed outside our main optimiza-
tion process, i.e. it does not need iterating on mini-batches
of data and backpropagating through the network. Then,
given Z, the main optimization process drives all examples
directly to that solution, as if they were all labeled.
5. Experiments
We present the datasets used in our experiments and the
SSL setup that is followed. Then, we discuss the training
details of our method and the methods reproduced for fair
comparison. Finally, we perform experiments to show the
impact of different components involved in the proposed
method and to compare with the state of the art. All er-
ror rates reported are produced by our own implementation
unless otherwise stated.
5.1. Datasets
We use three image classiﬁcation datasets, namely
CIFAR-10 [22], CIFAR-100 [22] and Mini-ImageNet [39].
Each dataset is used in an SSL setup where part of the train-
ing images are labeled and the rest are unlabeled. We evalu-
ate the performance on an independent test set. Unless oth-
erwise speciﬁed, error rate is reported in our experiments.
CIFAR-10. The training set consists of 50k images com-
ing from 10 classes, while the test set consists of 10k im-
ages from the same 10 classes. All images have resolution
32 × 32. Evaluation is performed with 50, 100, 200, and
400 labeled images per classes, corresponding to l = 500,
1k, 2k, and 4k label images in total.
We use the same
random selection of labeled images that is used in Mean
Teacher [38] when available (1k, 2k and 4k labels). The se-
lection process is repeated 10 times, resulting in 10 different
dataset splits for SSL on CIFAR 10. We follow the common
practice which is to use each of them and report mean error
and standard deviation.
CIFAR-100. Similarly to CIFAR-10, CIFAR-100 has 50k
training and 10k test images of resolution 32 × 32, com-
ing from 100 classes. We follow a protocol equivalent to
the one of CIFAR-10. We evaluate with 40 and 100 labeled
images per class, corresponding to 4k and 10k labeled im-
ages in total. There are 3 such dataset splits, mean error and
standard deviation are reported.
Mini-ImageNet. We introduce an SSL evaluation setup for
Mini-ImageNet [39] which is a subset of the well-known
ImageNet [6] dataset and has been previously used for few-
shot learning [11]. We use the train/test splits created in the
work of Ravi and Larochelle [33]. It consists of 100 classes
with 600 images per class, of resolution 84 × 84. We ran-
domly assign 500 images from each class to the training set,
and 100 images to the test set. The result is a train and test
set of 50k and 10k images, respectively. We create three
dataset splits for the case of 40 and 100 labeled images per
class that correspond to 4k and 10k labeled images in to-
tal. Mean error and standard deviation over the three dataset
splits are reported.
5.2. Training
We list the reproduced baselines, and provide training
details per algorithm and dataset.
Implementation. We build our implementation on top
of the publicly available Pytorch code for the Mean Teacher
(MT) approach [38]2. The fully supervised baseline and MT
are reproduced identically as the original implementation.
In all our experiments SGD optimization is used.
Networks. Experiments on CIFAR-10 and CIFAR-100
are performed with the “13-layer” network that is used
in prior work [23, 38], while on Mini-ImageNet, Resnet-
18 [18] is engaged. Both networks consist of a feature ex-
tractor φθ followed by an FC layer and softmax. We add an
ℓ2-normalization layer right after φθ (before the FC layer)
providing unit-norm descriptors for the graph construction.
The same choice is also adopted in the fully supervised
baseline. One exception is all variants of MT as we ob-
served that the ℓ2-normalization layer slightly harms per-
formance. We normalize images to have channel-wise zero
mean and unit variance over the entire training set. Unlike
prior work [38], we do not normalize the input images with
ZCA, nor add Gaussian noise to the input layer, which result
in worse performance according to our experiments.
Hyper-parameters and training choices are adapted
from the MT method and implementation. These are ﬁxed
2https://github.com/CuriousAI/mean-teacher/tree/
master/pytorch

Pseudo-labeling
ωi
ζj
CIFAR-10
Diffusion (7)
36.53 ± 1.42

36.17 ± 1.98

33.32 ± 1.53


32.40 ± 1.80
GTG [8]


35.20 ± 2.23
Network (1)


35.17 ± 2.46
Table 1. Impact of weights ωi, class weights ζj, and pseudo-
labeling by diffusion prediction (7) or network prediction (1). Er-
ror rate is reported on CIFAR-10 with 500 labels.
0
50
100
150
0.5
0.55
0.6
0.65
0.7
Epochs
Prediction accuracy
Diffusion (7)
Network (1)
Figure 4.
Accuracy of predicted pseudo-labels according to
ground-truth on CIFAR-10 with 500 labeled images. Diffusion
predictions (7) are compared against network predictions (1).
0
0.2
0.4
0.6
0.8
1
Epoch 0, weight ωi
Number of images
0
0.2
0.4
0.6
0.8
1
Epoch 90, weight ωi
Figure 5. Distribution of weights ωi for unlabeled images at epoch
0 (left) and epoch 90 (right) during the training of CIFAR-10 with
500 labels. Correct pseudo-labels according to ground-truth are
shown in blue and incorrect in red.
500
1k
2k
4k
10
20
30
40
50
Number of labeled images
Error rate
Fully supervised
Ours
MT [38]
MT + ours
Figure 6. Error rate versus number of labeled images on CIFAR-
10 using different methods.
for all approaches (re)produced by this work. The training
is performed for 180 epochs in total. Initial learning rate l0
is decayed with cosine annealing [25] so that it would have
reached zero after 210 epochs, while l0 = 0.05 on CIFAR-
10, and l0 = 0.2 on CIFAR-100 and Mini-ImageNet. Ran-
dom data augmentation is performed by 4×4 random trans-
lations [38] followed by horizontal ﬂip in CIFAR-10 and
CIFAR-100. On Mini-ImageNet, each image is randomly
rotated by 10 degrees before random horizontal ﬂip. Batch
auto(0.82) auto(0.82) auto(0.82) auto(0.82) auto(0.81) ship(0.81)
ship(0.81) frog(0.80) auto(0.80) auto(0.80) frog(0.80) frog(0.80)
Figure 7. Examples of incorrectly pseudo-labeled images with
highest ωi in CIFAR-10. Predicted class and ωi are shown below
each image.
size is 100 for CIFAR-10 and 128 for CIFAR-100 and Mini-
ImageNet. All other learning parameters remain unchanged
from MT implementation.
The fully supervised approach corresponds to training
with (2) and labeled images only. MT uses the additional
dual output trick with coefﬁcient 0.01.
Both these ap-
proaches are reproduced.
Our approach is performed with mini-batch size B =
BU + BL, where BL images are labeled and BU images
are originally unlabeled. We set BL = 50 for CIFAR-10
and BL = 31 for CIFAR100 and Mini-ImageNet. Same
is also applied for MT. One epoch is deﬁned as one pass
through all originally unlabeled examples in the training
set, meaning that images in IL appear multiple times per
epoch. We follow the same diffusion parameters as Iscen et
al. [20]. We set k = 50 for graph construction, γ = 3
in (9), and α = 0.99 in (10). We solve (10) with at most
20 iterations of CG. Pairwise similarities for the graph are
computed with the publicly available FAISS library [21].
Conﬁdence weights ωi are normalized over all examples
s.t. maxi ωi = 1. Class weights ζj are normalized over c
classes such that the average class weight is 1. Pseudo-label
predictions, ωi, and ζj are updated after each epoch.
To assess the beneﬁt of diffusion, we ﬁnally evaluate
a variant of our approach where the pseudo-labels are not
provided by diffusion but derived from the network with
(1) or from GTG propagation [8] instead. Training is per-
formed with (12), as with our method. This is in the spirit
of pseudo-labeling in prior work [36, 24].
5.3. Ablation Study
We investigate the impact of different components of our
method. First, we study the effectiveness of weights intro-
duced in the loss function (12). Table 1 shows the classiﬁ-
cation performance on CIFAR-10 test set, when using only
500 labeled examples for training and the rest of the training
set is considered unlabeled. Different weighting schemes
are evaluated by setting all ωi to one, all ζi to one, or both
to one. It is shown that both weights have positive contribu-
tions. We also show the beneﬁt of predicting with diffusion
over predicting by the trained network or GTG propagation.
Pseudo-labeling by the network predictions uses examples

Dataset
CIFAR-10
Nb. labeled images
500
1000
2000
4000
Fully supervised
49.08 ± 0.83
40.03 ± 1.11
29.58 ± 0.93
21.63 ± 0.38
TDCNN [36]†
-
32.67 ± 1.93
22.99 ± 0.79
16.17 ± 0.37
Network prediction (1) + weights
35.17 ± 2.46
23.79 ± 1.31
16.64 ± 0.48
13.21 ± 0.61
Ours: Diffusion prediction (7) + weights
32.40 ± 1.80
22.02 ± 0.88
15.66 ± 0.35
12.69 ± 0.29
VAT [26]†
-
-
-
11.36
Π model [23]†
-
-
-
12.36 ± 0.31
Temporal Ensemble [23]†
-
-
-
12.16 ± 0.24
MT [38]†
-
27.36 ± 1.30
15.73 ± 0.31
12.31 ± 0.28
MT [38]
27.45 ± 2.64
19.04 ± 0.51
14.35 ± 0.31
11.41 ± 0.25
MT + Ours
24.02 ± 2.44
16.93 ± 0.70
13.22 ± 0.29
10.61 ± 0.28
Table 2. Comparison with the state of the art on CIFAR-10. Error rate is reported. “13-layer” network is used. The top part of the table
corresponds to training with pseudo-labels, while the bottom part of the table includes methods that are complementary to ours, as shown
by the combination of our method with MT. † denotes scores reported in prior work.
Dataset
CIFAR-100
Mini-ImageNet-top1
Mini-ImageNet-top5
Nb. labeled images
4000
10000
4000
10000
4000
10000
Fully supervised
55.43 ± 0.11
40.67 ± 0.49
74.78 ± 0.33
60.25 ± 0.29
53.07 ± 0.68
38.28 ± 0.38
Ours
46.20 ± 0.76
38.43 ± 1.88
70.29 ± 0.81
57.58 ± 1.47
47.58 ± 0.94
36.14 ± 2.19
MT [38]
45.36 ± 0.49
36.08 ± 0.51
72.51 ± 0.22
57.55 ± 1.11
49.35 ± 0.22
32.51 ± 1.31
MT + Ours
43.73 ± 0.20
35.92 ± 0.47
72.78 ± 0.15
57.35 ± 1.66
50.52 ± 0.39
31.99 ± 0.55
Table 3. Performance comparison on CIFAR-100 and Mini-ImageNet with 4k and 10k labeled images. Error rate is reported. “13-layer”
network is used for CIFAR-100 and Resnet-18 is used for Mini-ImageNet. All methods are reproduced by us.
that the network can already classify, while diffusion allows
for accurate predictions beyond those examples. In Fig-
ure 4, we report the progress of the pseudo-label accuracy
on unlabeled images XU throughout the training. Diffusion
predictions are consistently better than network predictions.
Figure 5 demonstrates how ωi accurately estimates the
certainty of the prediction. From the plots we observe that
predictions become more accurate as the training evolves,
while at the beginning most examples are misclassiﬁed.
The proposed weighting mechanism is robust to incorrect
pseudo-labels and prevents model collapse. Figure 7 shows
some of the incorrectly pseudo-labeled images with high
certainty ωi. Most of the incorrect labels come from trucks
labeled as automobiles or birds labeled as frogs.
5.4. Comparison with the state-of-the-art
We present a comparison with state-of-the-art on all 3
datasets in Tables 2 and 3. The comparison includes perfor-
mance reported in prior work and our reproduced results.
In the case of the work by Shi et al. [36], we only compare
with their TDCNN variant which refers to pseudo-labeling
for network training. The other loss terms in their work are
complementary to ours, similarly to MT. We additionally
compare with our implementation of pseudo-labeling with
network predictions combined with the proposed weights.
The proposed approach performs the best out of the
pseudo-label based approaches on CIFAR-10. Results in
Figure 6 show that our beneﬁt is larger when the num-
ber of labels is reduced. The results on CIFAR-10 show
that our approach is complementary to unsupervised loss,
such as the one used by MT. This combination achieves
the best performance on this dataset. The same holds for
CIFAR-100 and Mini-ImageNet for 10k available labels.
Our method also achieves a lower error rate than tempo-
ral ensemble (38.65±0.51) and Π-model (39.19±0.36) on
CIFAR-100 [23] with 10k labels. On Mini-ImageNet with
4k available labels, the best performance is achieved when
using our method without combining with Mean Teacher.
6. Conclusions
Most recent approaches for deep SSL rely on training
with unsupervised loss on both labeled and unlabeled im-
ages. We have proposed an approach that relies on graph-
based label propagation to infer pseudo-labels for the un-
labeled images. An additional training set is formed with
these pseudo-labels, which are shown to be more valuable
than the pseudo-labels inferred by the network itself. Our
method is in principle complementary to unsupervised loss
terms, which is experimentally shown in this work.
Acknowledgments This work is supported by the GA ˇCR
grant
19-23165S
and
the
OP
VVV
funded
project
CZ.02.1.01/0.0/0.0/16 019/0000765
“Research
Center
for
Informatics”.

References
[1] Mathilde Caron, Piotr Bojanowski, Armand Joulin, and
Matthijs Douze. Deep clustering for unsupervised learning
of visual features. ECCV, 2018. 1
[2] Siddhartha Chandra and Iasonas Kokkinos. Fast, exact and
multi-scale inference for semantic image segmentation with
deep Gaussian CRFs. In ECCV, 2016. 4
[3] Olivier Chapelle, Bernhard Scholkopf, and Alexander Zien.
Semi-Supervised Learning. MIT Press, 2006. 2
[4] Dengxin Dai and Luc Van Gool. Ensemble projection for
semi-supervised image classiﬁcation. In ICCV, 2013. 2
[5] Carl Doersch, Abhinav Gupta, and Alexei A. Efros. Unsu-
pervised visual representation learning by context prediction.
In ICCV, 2015. 1
[6] Wei Dong, Richard Socher, Li Li-Jia, Kai Li, and Li Fei-
Fei. Imagenet: A large-scale hierarchical image database. In
CVPR, June 2009. 6
[7] Matthijs Douze, Arthur Szlam, Bharath Hariharan, and
Herv´e J´egou. Low-shot learning with large-scale diffusion.
In CVPR, 2018. 2
[8] Ismail Elezi, Alessandro Torcinovich, Sebastiano Vascon,
and Marcello Pelillo.
Transductive label augmentation
for improved deep network learning.
arXiv preprint
arXiv:1805.10546, 2018. 2, 7
[9] Aykut Erdem and Marcello Pelillo. Graph transduction as a
noncooperative game. Neural Computation, 24, 2012. 2
[10] Rob Fergus, Yair Weiss, and Antonio Torralba.
Semi-
supervised learning in gigantic image collections. In NIPS,
2009. 2
[11] Spyros Gidaris and Nikos Komodakis. Dynamic few-shot
visual learning without forgetting. In CVPR, 2018. 6
[12] Spyros Gidaris, Praveer Singh, and Nikos Komodakis. Un-
supervised representation learning by predicting image rota-
tions. In ICLR, 2018. 1
[13] Albert Gordo, Jon Almazan, Jerome Revaud, and Diane Lar-
lus. End-to-end learning of deep visual representations for
image retrieval. IJCV, 124(2), 2017. 1
[14] Leo Grady. Random walks for image segmentation. IEEE
Trans. PAMI, 28(11):1768–1783, 2006. 4
[15] Yves Grandvalet and Yoshua Bengio.
Semi-supervised
learning by entropy minimization. In NIPS, 2005. 2
[16] Matthieu Guillaumin, Jakob Verbeek, and Cordelia Schmid.
Multimodal semi-supervised learning for image classiﬁca-
tion. In CVPR, 2010. 2
[17] Philip Haeusser, Alexander Mordvintsev, and Daniel Cre-
mers. Learning by association – a versatile semi-supervised
training method for neural networks. In CVPR, 2017. 2
[18] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition.
In CVPR,
2016. 6
[19] Ahmet Iscen, Giorgos Tolias, Yannis Avrithis, and Ondrej
Chum. Mining on manifolds: Metric learning without labels.
In CVPR, 2018. 1
[20] Ahmet Iscen, Giorgos Tolias, Yannis Avrithis, Teddy Furon,
and Ondrej Chum. Efﬁcient diffusion on region manifolds:
Recovering small objects with compact cnn representations.
In CVPR, 2017. 4, 7
[21] Jeff Johnson, Matthijs Douze, and Herv´e J´egou.
Billion-
scale
similarity
search
with
gpus.
arXiv
preprint
arXiv:1702.08734, 2017. 7
[22] Alex Krizhevsky and Geoffrey Hinton. Learning multiple
layers of features from tiny images. Technical report, Uni-
versity of Toronto, 2009. 6
[23] Samuli Laine and Timo Aila. Temporal ensembling for semi-
supervised learning. In ICLR, 2017. 2, 6, 8
[24] Dong-Hyun Lee.
Pseudo-label: The simple and efﬁcient
semi-supervised learning method for deep neural networks.
In ICMLW, 2013. 2, 3, 7
[25] Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient
descent with warm restarts. ICLR, 2017. 7
[26] Takeru Miyato, Shin-ichi Maeda, Shin Ishii, and Masanori
Koyama.
Virtual adversarial training:
a regularization
method for supervised and semi-supervised learning. IEEE
Trans. PAMI, 2018. 2, 8
[27] Avital Oliver, Augustus Odena, Colin Raffel, Ekin D Cubuk,
and Ian J Goodfellow.
Realistic evaluation of deep semi-
supervised learning algorithms. In ICLRW, 2018. 2
[28] Deepak Pathak, Ross B Girshick, Piotr Doll´ar, Trevor Dar-
rell, and Bharath Hariharan. Learning features by watching
objects move. In CVPR, 2017. 1
[29] Siyuan Qiao, Wei Shen, Zhishuai Zhang, Bo Wang, and Alan
Yuille. Deep co-training for semi-supervised image recogni-
tion. In ECCV, 2018. 2, 3
[30] Filip Radenovi´c, Giorgos Tolias, and Ondˇrej Chum. Fine-
tuning CNN image retrieval with no human annotation. IEEE
Trans. PAMI, 2018. 1
[31] Ilija Radosavovic, Piotr Dollar, Ross Girshick, Georgia
Gkioxari, and Kaiming He. Data distillation: Towards omni-
supervised learning. In CVPR, 2018. 2
[32] Antti Rasmus, Mathias Berglund, Mikko Honkala, Harri
Valpola, and Tapani Raiko. Semi-supervised learning with
ladder networks. In NIPS, 2015. 2
[33] Sachin Ravi and Hugo Larochelle. Optimization as a model
for few-shot learning. In ICLR, 2016. 6
[34] Mehdi Sajjadi, Mehran Javanmardi, and Tolga Tasdizen.
Mutual exclusivity loss for semi-supervised deep learning.
In ICIP, 2016. 2
[35] Mehdi Sajjadi, Mehran Javanmardi, and Tolga Tasdizen.
Regularization with stochastic transformations and perturba-
tions for deep semi-supervised learning. In NIPS, 2016. 2
[36] Weiwei Shi, Yihong Gong, Chris Ding, Zhiheng Ma, Xiaoyu
Tao, and Nanning Zheng.
Transductive semi-supervised
deep learning using min-max features. In ECCV, 2018. 2, 3,
4, 7, 8
[37] Abhinav Shrivastava, Saurabh Singh, and Abhinav Gupta.
Constrained semi-supervised learning using attributes and
comparative attributes. In ECCV, 2012. 2
[38] Antti Tarvainen and Harri Valpola. Mean teachers are better
role models: Weight-averaged consistency targets improve
semi-supervised deep learning results. In NIPS, 2017. 2, 3,
4, 5, 6, 7, 8
[39] Oriol Vinyals, Charles Blundell, Tim Lillicrap, Daan Wier-
stra, et al. Matching networks for one shot learning. In NIPS,
2016. 6

[40] Xiaolong Wang, Kaiming He, and Abhinav Gupta. Transi-
tive invariance for selfsupervised visual representation learn-
ing. In ICCV, 2017. 1
[41] Jason Weston, Fr´ed´eric Ratle, and Ronan Collobert. Deep
learning via semi-supervised embedding. In ICML, 2008. 2,
5
[42] Zhirong Wu, Yuanjun Xiong, Stella Yu, and Dahua Lin. Un-
supervised feature learning via non-parametric instance-level
discrimination. CVPR, 2018. 1
[43] Dengyong Zhou, Olivier Bousquet, Thomas Navin Lal, Ja-
son Weston, and Bernhard Sch¨olkopf. Learning with local
and global consistency. In NIPS, 2003. 1, 2, 3, 4
[44] Xiaojin Zhu, John Lafferty, and Ronald Rosenfeld. Semi-
Supervised Learning with Graphs.
PhD thesis, Carnegie
Mellon University, Language Technologies Institute, School
of Computer Science Pittsburgh, PA, 2005. 4
[45] Xiaojin Zhu, John D Lafferty, and Zoubin Ghahramani.
Semi-supervised learning: From Gaussian ﬁelds to Gaussian
processes. Technical report, 2003. 1

