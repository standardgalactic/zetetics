PUDLE: Implicit Acceleration of Dictionary
Learning by Backpropagation
Bahareh Tolooshams
School of Engineering and Applied Sciences
Harvard University
btolooshams@seas.harvard.edu
Demba Ba
School of Engineering and Applied Sciences
Harvard University
demba@seas.harvard.edu
Abstract
The dictionary learning problem, representing data as a combination of few atoms,
has long stood as a popular method for learning representations in statistics and sig-
nal processing. The most popular dictionary learning algorithm alternates between
sparse coding and dictionary update steps, and a rich literature has studied its theo-
retical convergence. The growing popularity of neurally plausible unfolded sparse
coding networks has led to the empirical ﬁnding that backpropagation through
such networks performs dictionary learning. This paper offers the ﬁrst theoretical
proof for these empirical results through PUDLE, a Provable Unfolded Dictionary
LEarning method. We highlight the impact of loss, unfolding, and backpropagation
on convergence. We discover an implicit acceleration: as a function of unfolding,
the backpropagated gradient converges faster and is more accurate than the gradient
from alternating minimization. We complement our ﬁndings through synthetic and
image denoising experiments. The ﬁndings support the use of accelerated deep
learning optimizers and unfolded networks for dictionary learning.
1
Introduction
This paper considers the dictionary learning problem, namely representing data x ∈X ⊂Rm as
linear combinations of a few atoms from a dictionary D ∈D ⊂Rm×p. Given x and D, the problem
of recovering the sparse (few non-zero elements) coefﬁcients z ∈Rp is referred to as sparse coding
which can be solved through the lasso [1] (also known as basis pursuit [2]):
ℓx(D) := minz∈Rp Lx(z, D) + h(z)
(1)
where Lx(z, D) = 1
2∥x −Dz∥2
2, and h(z) = λ∥z∥1. From the perspective of statistical learning, the
dictionary of interest comprises the atoms that minimize the expected risk, i.e.,
D∗∈arg minD∈D Ex∈X [ℓx(D)].
(2)
The signal processing literature typically assumes a dictionary ˜D that generates the data, i.e.,
x = ˜D˜z
(3)
where ˜z is sparse, and aims to recover ˜D. Olshausen and Field [3] introduced (3) in computational
neuroscience as a model for how early layers of the visual cortex process natural images. Sparse
coding has been widely studied and utilized in the statistics [4] and signal processing communities [5].
Practical examples are denoising [6], super-resolution [7], and classiﬁcation [8], where it enables
the extraction of sparse high-dimensional features representing data. Furthermore, sparse coding
has been utilized to construct neural architectures through approaches such as sparse energy-based
models [9, 10] or recurrent sparsifying encoders [11]. The latter has initiated a growing literature on
constructing deep networks based on an approach referred to as algorithm unfolding [12, 13]. Deep
Preprint. Under review.
arXiv:2106.00058v1  [cs.LG]  31 May 2021

(a) Unfolded network architecture with dictionary D.
(b) Gradients.
Figure 1: Provable unfolded dictionary learning (PUDLE).
unfolded neural networks have gained popularity in recent years due to their computational efﬁciency
and their performance in various domains such as image denoising [14, 15, 16], super-resolution [17],
medical imaging [18], deblurring [19, 20], and speech processing [12].
Prior to the advent of unfolded networks, gradient-based dictionary learning relied on analytic
gradients computed from the lasso given the sparse code. With unfolded networks, automatic
differentiation [21], referred to as backpropagation [22] in the reverse-mode, gained attention for
parameter estimation [23, 15, 16]. The automatic gradient is obtained by backpropagation through the
algorithm used to estimate the code. Automatic differentiation in reverse and forward-mode [24] is
used in other areas, e.g., hyperparameter selection [25], and in a more relevant context, in the seminal
work of LISTA [11]. Other works demonstrated empirically the convergence of dictionary learning
by backpropagation through unfolded networks [15, 23]. Despite the empirical evidence, there is no
known theoretical analysis of backpropagation in unfolded networks for dictionary learning.
This paper proposes a Provable Unfolded Dictionary LEarning (PUDLE) (Figure 1a). Given ﬁnite
computational power, sparse coding can be converted into an encoder obtained by unfolding T
iterations of ISTA [26, 27]. The decoder is a linear map reconstructing the data [23]. We adopt
the perspective of (3) and aim to recover ˜D by training the network using backpropagation with
a learning rate of η. Three different choices affect the gradient: the loss, the number of unfolded
iterations, and whether one backpropagates through decoder or through both the encoder and decoder.
This paper highlights the impact of such choices on the convergence of the training algorithm.
Backpropagation through the decoder results in the analytic gradient gdec
t
using the code estimate
zt. The gradients gae-lasso
t
and gae-ls
t
are computed by backpropagation through the autoencoder using
the lasso and least-squares objectives, respectively (Algorithm 2). We compare Algorithm 2 with
the classical gradient-based alternating-minimization algorithm for dictionary learning [28], which
cycles between sparse coding and dictionary update steps using the analytic gradient ˆg Algorithm 1.
Contributions
We study gradient estimations (Figure 1b) for dictionary learning in PUDLE.
• Implicit acceleration: We prove that gae-lasso
t
converges to ˆg faster than gdec
t . We refer to this as
implicit acceleration. Hence, given a reasonable ﬁnite number of unfoldings, gae-lasso
t
is a better
estimator than gdec
t , resulting in convergence to a closer neighbourhood of D∗(Theorem 3.3).
• Dictionary learning: Given ﬁxed λ, gdec
t
and gae-lasso
t
point towards D∗. gae-ls
t
is a better estimator
of the direction to recover ˜D than the other two gradients. Hence, gae-ls
t
converges to a closer
neighbourhood of ˜D than others (Theorem 3.4).
• Unbiased estimation: We show that the biased estimation of ˜D vanishes as λt = λβt (with
0 < β < 1) decays within the unfolded layers (Figure 6).
• Image denoising: In a supervised image denoising task, we show that the advantage of gae-ls
t
goes
beyond dictionary learning. Additionally, our network outperforms the sparse coding scheme in
NOODL, a state-of-the-art online dictionary learning algorithm [29] (Table 1).
• Neural plausibility: Prior works have mainly focused on the neural plausibility of sparse coding
and designed a dictionary gradient update independent of the coding step [30, 29]. PUDLE
highlights a link between the neural plausibility of sparse coding and the dictionary update.
Speciﬁcally, we show that backpropagating through the sparse coding step yields an implicit
gradient update rule and eliminates the need for explicit ones.
2

Related works
Implicit acceleration is termed super efﬁciency in [31], which compares analytic and
automatic gradient estimators of min-min or max-min optimizations with smooth and differentiable
functions. Unlike our work, where we evaluate the gradients for global model recovery, Ablin et al.
study the asymptotic error of the gradients locally in each step of an alternating minimization [31].
There is a vast literature on the theoretical convergence of dictionary learning. Spielman et al. pro-
posed a factorization method to recover the dictionary in the undercomplete setting (i.e., p ≤m) [32].
Barak et al. proposed to solve dictionary learning via sum-of-squares semideﬁnite program [33].
K-SVD [34] and MOD [35] are popular greedy approaches. Alternating-minimization-based methods
have been used extensively both in theory and in practice [36, 37, 38].
Recent work has incorporated gradient-based updates into alternating minimization [28, 30, 29].
Chatterji and Bartlett provided a ﬁnite sample analysis and convergence guarantees when updating
the dictionary using the analytic gradient [28]. Arora et al. proposed and studied neurally plausible
dictionary learning approaches with analytic gradients [30]. A follow-up work focused on online
dictionary learning [39] with an unbiased gradient for dictionary update [29]. Arora et al. discussed
methods to reduce the bias of dictionary estimation [30], and Rambhatla et al. showed how to reduce
bias in both code and dictionary estimations [29]. A common feature in the above-mentioned work is
the use of analytic gradients, instead of automatic ones. Indeed, the previous works explicitly design
tailored gradient updates independent of the sparse coding step and do not utilize backpropagation
with deep learning optimizers. A theoretical analysis of backpropagation for dictionary learning
exists only for shallow ReLU autencoders [40, 41].
The theoretical analysis of unfolded neural networks has mainly analyzed the convergence speed
of variants of LISTA [11], where the focus is on sparse coding (i.e., the encoder) not dictionary
learning [42, 43, 44, 45, 46, 47, 48]. Moreau and Bruna showed that upon successful factorization of
the Gram matrix of the dictionary within layers, the network achieves accelerated convergence [44].
Giryes et al. examined the tradeoffs between reconstruction accuracy and convergence speed of
LISTA [45]. Moreover, Chen et al. studied the learning dynamics of the weights and biases of
unfolded-ISTA and proved that it achieves linear convergence [46]. Follow-up works investigated the
dynamics of step-size in a recursive sparse coding encoder [47, 48]. Ablin et al. minimized the lasso
through backpropagation but still assumed the knowledge of the dictionary at the decoder [48].
Notation
Bold-lower-case and upper-case letters refer to vectors d and matrices D. We use dj
to denote the jth element of the vector d, and Dj is the jth column of the matrix D. λ > 0 is the
regularization (sparsity-enforcing) parameter. σmax(D) is the maximum singular value of D. When
taking the derivatives or norms w.r.t the matrix D, we assume that D is vectorized. ∇1L(z, D) and
∇2L(z, D) are the ﬁrst derivatives of the loss w.r.t z and D, respectively. ∇2
11L(z, D) is the second
derivative of the loss w.r.t z. ∇2
21L(z, D) is the derivative of ∇1L(z, D) w.r.t D. The support of z is
supp(z) ≜{j : zj ̸= 0}. We denote {1, . . . , p} by [1, p].
2
Preliminaries
Given n independent samples, dictionary learning aims to minimize the empirical risk, i.e.,
minD∈D Rn(D)
with
Rn(D) ≜1
n
Pn
i=1 ℓxi(D)
(4)
where limn→∞Rn(D) = Ex∈X [ℓx(D)] a.s. To prevent scaling ambiguity between the code z and
dictionary D, it is common to constrain the norm of the dictionary columns. Hence, we deﬁne the
set of feasible solutions for the dictionary as D ≜{D ∈Rm×p s.t. ∀j ∈[1, p], ∥Dj∥2
2 ≤1}. We
can project estimates of D onto the feasible set by performing Dj ←1/max(∥Dj∥2,1)Dj, either at
every update or at the end of training. We assume certain properties on the domain of the data and
dictionary (Assumption 2.1), as well as a bound on the energy of the data (Assumption 2.2).
Assumption 2.1 (Domain signals). X and D are both compact convex sets.
Assumption 2.2 (Bounded signals). ∃M > 0 s.t. ∥x∥2 < M ∀x ∈X.
The recurrent encoder and decoder, which perform the computations shown in Algorithm 2, use the
loss L and proximal operator Pαh(v) ≜sign(v) max(|v| −αλ) for the ℓ1 norm h: Rp →R. The
encoder implements ISTA [26, 27] with step-size α, assumed to be less than 1/σ2
max(D). With inﬁnite
encoder unfolding, the encoder’s output is the solution to the lasso (1), following the optimality
3

condition (Lemma A.3) where we denote fx(z, D) ≜Lx(z, D) + h(z). One immediate observation
is that λ ≥∥DTx∥∞⇔{0} ∈arg min fx(z, D). We assume λ < ∥DTx∥∞and the solution to (1)
is unique. Sufﬁcient conditions for uniqueness in the overcomplete case (i.e., p > m) are extensively
studied in the literature [49, 50, 51]. Tibshirani discussed that the solution is unique with probability
one if entries of D are drawn from a continuous probability distribution [51] (Assumption 2.3). We
argue that as long as the data x ∈X are sampled from a continuous distribution, this assumption holds
for the entire learning process. The assumption has been previously considered in analyses of unfolded
sparse coding networks [48] and can be extended to ℓ1 regularized optimization problems [51, 52].
Assumption 2.3 (Lasso uniqueness). The entries of the dictionary D are continuously distributed.
Hence, the minimizer of (1) is unique, i.e., {ˆz} ∈arg min fx(z, D) with probability one.
Lemma 2.1 states the ﬁxed-point property of the encoder recursion [53]. Given the deﬁnitions for
Lipschitz and Lipschitz differentiable functions, (Deﬁnitions A.1 and A.2), the loss L and function h
satisfy following Lipschitz properties, which will play an important role in our analysis.
Lemma 2.1 (Fixed-point property of lasso). Given Assumption 2.3, we have 0 ∈∇1L(ˆz, D)+∂h(ˆz).
The minimizer is a ﬁxed-point of the mapping, i.e., ˆz = Pαh(ˆz −α∇1L(ˆz, D)) = Φ(ˆz) [53].
Lemma 2.2 (Lipschitz differentiable least squares). Given Lx(z, D) = 1
2∥x −Dz∥2
2, D, and As-
sumption 2.2, the loss is Lipschitz differentiable. Let L1 and L2 denote the Lipschitz constants of the
ﬁrst derivatives ∇1Lx(z, D) and ∇2Lx(z, D), L11 and L21 the Lipschitz constants of the second
derivatives ∇2
11Lx(z, D) and ∇2
21Lx(z, D), all w.r.t z. Let ∇1Lx(z, D) be LD-Lipschitz w.r.t D.
Lemma 2.3 (Lipschitz proximal). Given h(z) = λ∥z∥1, its proximal operator has bounded sub-
derivative, i.e., ∥∂Ph(z)∥2 ≤cprox. In addition, ∥∂h(a) −∂h(b)∥2 ≤(Lh/2)∥a −b∥2
2 ∀a, b ∈Rp.
3
Main Results
The gradients deﬁned in PUDLE (Algorithm 2) can be compared against the local direction at each
update of classical alternating-minimization (Algorithm 1). Assuming there are inﬁnite samples, i.e.,
Best local direction :
ˆg ≜limn→∞1
n
Pn
i=1 ∇2Lxi(ˆzi, D) = Ex∈X [∇2Lx(ˆz, D)]
(5)
where ˆz = arg minz∈Rp Lx(z, D) + h(z). Additionally, to assess the estimators for model recovery,
hence dictionary learning, we compare them against gradients that point towards D∗and ˜D, namely
Desired gradient for D∗: g∗≜limn→∞1
n
Pn
i=1 ∇2Lxi(zi∗, D) = Ex∈X [∇2Lx(z∗, D)]
Desired gradient for ˜D :
˜g ≜limn→∞1
n
Pn
i=1 ∇2Lxi(˜zi, D) = Ex∈X [∇2Lx(˜z, D)]
(6)
where z∗= arg minz∈Rp Lx(z, D∗) + h(z). To see why the above are desired directions, we
highlight that (z∗, D∗) is a critical point of the expected risk. Hence, given the current D, to reach
the critical point by gradient descent, we move towards the direction minimizing Ex∈X [Lx(z∗, D)].
Similarly, (˜z, ˜D) is a critical point of the loss L which also reaches zero for data following the
model (3). Hence, to reach ˜D ∈arg minD∈D Ex∈X [Lx(˜z, D)], we move towards the direction
minimizing the loss in expectation. Given these directions, we analyze the error of the gradients gdec
t ,
gae-lasso
t
, and gae-ls
t
assuming inﬁnite samples. In this regard, we ﬁrst study the forward pass.
3.1
Forward pass
We show two convergence results in the forward pass, one for z and another for the Jacobian, i.e.,
Deﬁnition 3.1 (Code Jacobian). Given D, the Jacobian of zt is deﬁned as Jt ≜∂zt
∂D.
The forward pass analysis gives upper bounds on the error between zt and ˆz and the error between Jt
and ˆJ as a function of unfolded iterations t. We will require these errors in Section 3.2, where we
analyze the gradient estimation errors. Similar to [28], the error associated with gdec
t
depends on the
code convergence. Unlike gdec
t , the convergence of backpropagation with gradient estimates gae-lasso
t
and gae-ls
t
relies on the convergence properties of the code and the Jacobian [31]. Forward-pass
theories are based on studies by Gilbert on the convergence of variables and their derivatives in an
iterative process governed by a smooth operator [54]. Moreover, Hale et al. studied the convergence
analysis of ﬁxed point iterations for ℓ1 regularized optimization problems [55]. In Proposition 3.1,
we re-state a result from [55] on support selection.
4

Algorithm 1: Classical alternating-minimization-based dictionary learning using lasso (1).
Initialize: Samples {xi}n
i=1 ∈X, initial dictionary D(0)
Repeat: l = 0, 1, . . . , number of epochs
Sparse coding step:
zi(l) = arg minz Lxi(z, D(l)) + h(z),
(for i ∈[1, n])
Dictionary update:
D(l+1) = D(l) −ηˆg(l)
where
ˆg(l) ≜
1
n
Pn
i=1 ∇2Lxi(zi(l), D(l))
Algorithm 2: PUDLE: Provable unfolded dictionary learning framework.
Initialize: Samples {xi}n
i=1 ∈X, initial dictionary D(0), and z0 = 0.
Repeat: l = 0, 1, . . . , number of epochs
Forward pass: (for i ∈[1, n])
Encoder:
zi(l)
t+1 = Φ(zi(l)
t
, D(l)) = Pαh(zi(l)
t
−α∇1Lxi(zi(l)
t
, D(l))) (repeat for T)
Decoder:
ˆxi(l) = D(l)zi(l)
T
(7)
Backward pass: D(l+1) = D(l) −ηg(l)
T
where g(l)
T is either of
g(l) dec
t
≜
1
n
Pn
i=1 ∇2Lxi(zi(l)
t
, D(l))
g(l) ae-lasso
t
≜
1
n
Pn
i=1 ∇2Lxi(zi(l)
t
, D(l)) + ∂zi(l)
t
∂D(l)

∇1Lxi(zi(l)
t
, D(l)) + ∂h(zi(l)
t
)

g(l) ae-ls
t
≜
1
n
Pn
i=1 ∇2Lxi(zi(l)
t
, D(l)) + ∂zi(l)
t
∂D(l) ∇1Lxi(zi(l)
t
, D(l))
(8)
Proposition 3.1 (Finite-iteration support selection). Given Assumption 2.3, let ˆz = arg min fx(z, D)
with S ≜supp(ˆz). There exists a B > 0 such that supp(zB) = S, ∀t > B.
This means that the unfolded encoder identiﬁes the support in ﬁnite iterations. Support recovery
in ﬁnite iterations has been studied in the literature for LISTA [46], Step-LISTA [48], and shallow
autoencoders [30, 40, 41, 16]. We now state Theorem 3.1 on the rate of convergence of the encoder.
0
50
100
Number of unfolding [t]
10−2
10−1
100
||zt −ˆz||2
Figure 2: Results for code conver-
gence (Theorem 3.1) demonstrating
that as the PUDLE network unfolds,
zt converges to ˆz, the solution of lasso.
Theorem 3.1 (Forward pass code convergence). Given
the iterative encoder zt+1 = Φ(zt, D), Assumption 2.3,
and Lemmas 2.1, A.1 and A.2,
∃ρ < 1, B > 0
s.t.
∥zt −ˆz∥2 ≤O(ρt)
∀t > B (9)
where ˆz is the unique minimizer of lasso (1).
Proof sketch (full details in Appendix A.1). Given the ﬁnite-
iteration support selection property (Proposition 3.1), we
show the strong convexity of L w.r.t z restricted to the sup-
port for t > B (Lemma A.1). This property allows us to
bound the sub-derivative of the recursion zt+1 = Φ(zt, D)
for t > B (Lemma A.2). We then use the ﬁxed-point prop-
erty of the encoder (Lemma 2.1) to ﬁnd an upper bound on
∥zt −ˆz∥2 for t > B which goes to 0 as t →∞.
■
Remarks
Theorem 3.1 shows that in PUDLE, zt converges to ˆz at a linear rate eventually after a
certain number of unfoldings (Figure 2). The local linear convergence of ISTA and FISTA [56] in
the neighbourhood of a ﬁxed-point is studied in [57]. The speed of convergence depends on when
support selection happens (Proposition 3.1) [58, 59]. Overall, the lasso converges at the global rate of
O(1/t) and O(1/t2) for ISTA and FISTA, respectively [56]. Chen et al. has proved that it is possible
to attain linear convergence for all iterations given a LISTA encoder [46]. PUDLE is constrained for
unsupervised dictionary learning; hence it differs from LISTA as we assume no knowledge of ˆz.
Following properties similar to those used in Theorem 3.1, and assuming Jt is bounded (Assump-
tion 3.1), we show in Theorem 3.2 that, as the PUDLE unfolds, the code Jacobian Jt converges to ˆJ,
5

the Jacobian of the solution of lasso. The convergence of the Jacobian of proximal gradient descent is
also studied in [60] for hyperparameter selection through implicit differentiation [61]. In their case,
the Jacobian is taken w.r.t to the hyperparameter λ as opposed to D.
Assumption 3.1 (Bounded Jacobian). The Jacobian is bounded, i.e., ∃MJ > 0, s.t. ∥Jt∥2 ≤MJ ∀t.
Theorem 3.2 (Forward pass Jacobian convergence). Given the recursion zt+1 = Φ(zt, D), and ˆz
the unique minimizer of lasso with Jacobian ˆJ,
∃ρ < 1, B > 0
s.t.
∥Jt −ˆJ∥2 ≤O(tρt−1)
∀t > B.
(10)
Proof sketch (full details in Appendix A.2). Differentiating the recursion, we get the relation Jt+1 =
∇1Φ(zt, D)Jt + ∇2Φ(zt, D). Using Lemma 2.1, we have ˆJ = ∇1Φ(ˆz, D)ˆJ + ∇2Φ(ˆz, D). Then,
we ﬁnd an upper bound on ∥Jt −ˆJ∥2, given the Lipschitz properties and the bounded Jacobian.
■
3.2
Backward pass
We show two results for local (relating to ˆg) and global (relating to g∗and ˜g) gradient con-
vergence.
The goal is not to provide a ﬁnite sample analysis but to emphasize the rela-
tive differences between the gradients in Algorithm 2.
The impact of gradient error for pa-
rameter estimation has been studied by Devolder et al. indicating that the convergence to
the parameter’s neighbourhood is dictated by the gradient error [62, 63].
The intuition is
that the size of the gradient error dictates the size of the neighbourhood of the dictionary
within which one can guarantee convergence.
We argue that the method with lower gradi-
ent error recovers the dictionary better.
We drop the superscript (l) to simplify the notation.
0
50
100
Number of unfolding [t]
10−3
10−1
||g −ˆg||2
gdec
t
gae−lasso
t
gae−ls
t
Figure 3: Results for the convergence
rate of PUDLE gradients (Theorem 3.3).
gdec
t
converges to ˆg at a linear rate after
certain t, gae-lasso
t
converges faster, and
gae-ls
t
is its biased estimator.
Local gradient estimations
We highlight the effect of
ﬁnite computational capacity in the sparse coding step
of PUDLE on the gradient for parameter estimation [31].
Theorem 3.3 shows the convergence rate of gradients to ˆg,
determining the similarity of PUDLE and Algorithm 1.
Theorem 3.3 (Local convergence of gradients). Given the
convergence results from the forward pass (Theorems 3.1
and 3.2), ∃ρ < 1, B > 0 such that ∀t > B, the errors of
gradients deﬁned in Algorithm 2 w.r.t ˆg (5) satisfy
∥gdec
t
−ˆg∥2 ≤O(ρt)
∥gae-lasso
t
−ˆg∥2 ≤O(tρ2t−1)
∥gae-ls
t
−ˆg∥2 ≤O(tρ2t−1 + ˆCh)
(11)
where ˆCh is an upper bound on MJ∥∂h(ˆz)∥2.
Remarks
First, upper bounds on the errors related to gdec
t
and gae-lasso
t
go to zero as t increases.
Hence, both gradients converge to ˆg. This means that asymptotically as t increases, training PUDLE
with gdec
t
and gae-lasso
t
is equivalent to classical alternating-minimization (Algorithm 1). Second, as t
increases, gae-lasso
t
has faster convergence than gdec
t . Lastly, gae-ls
t
is a biased estimator of ˆg Figure 3.
Proof sketch (full details in Appendix A.3). The error for gdec
t
can be bounded using the constant L2.
Given inﬁnite samples, we replace the sample mean with expectation in their limit. We use the
property 0 ∈∇1Lx(ˆz, D) + ∂h(ˆz). For gae-lasso
t
and gae-ls
t
, we re-write the gradient errors as
gae-lasso
t
−ˆg = Ex∈X [Q(ˆz, Jt)(zt −ˆz)] + Ex∈X [Q21
t (ˆz)] + Ex∈X [JtQlasso-11
t
(ˆz)]
gae-ls
t
−ˆg = Ex∈X [Q(ˆz, Jt)(zt −ˆz)] + Ex∈X [Q21
t (ˆz)] + Ex∈X [JtQls-11
t
(ˆz)]
(12)
where
Q21
t (z) ≜∇2Lx(zt, D) −∇2Lx(z, D) −∇2
21Lx(z, D)(zt −z)
Qlasso-11
t
(z) ≜∇1Lx(zt, D) + ∂h(zt) −∇2
11Lx(z, D)(zt −z)
Qls-11
t
(z) ≜∇1Lx(zt, D) −∇2
11Lx(z, D)(zt −z)
Q(z, J) ≜J∇2
11Lx(z, D) + ∇2
21Lx(z, D).
(13)
6

Hence, it sufﬁces to bound the terms on the r.h.s using Assumption 3.1 and Lemma 3.1.
■
Lemma 3.1 (Local bounds). From local gradient errors in Theorem 3.3, the following are satisﬁed
∥Q21
t (ˆz)∥2 ≤(L21/2)∥zt −ˆz∥2
2,
∥Q(ˆz, Jt)∥2 ≤L1∥Jt −ˆJ∥2,
∥Qlasso-11
t
(ˆz)∥2 ≤(L11/2 + Lh/2)∥zt −ˆz∥2
2
∥Qls-11
t
(ˆz)∥2 ≤(L11/2)∥zt −ˆz∥2
2 + ∥∂h(ˆz)∥2.
(14)
Global gradient estimations
Theorem 3.4 shows the global gradient errors w.r.t g∗and ˜g from (6).
This result determines whether the PUDLE gradients facilitate the recovery of D∗or ˜D [62, 63].
Theorem 3.4 (Global convergence of gradients). Given the convergence results form the forward
pass, (Theorems 3.1 and 3.2), ∃ρ < 1, B > 0 such that ∀t > B, the errors of gradients deﬁned in
Algorithm 2 w.r.t global directions g∗and ˜g (deﬁned in (6)) satisfy
∥gdec
t
−g∗∥2 ≤O(ρt + C∗
z)
∥gae-lasso
t
−g∗∥2 ≤O(tρ2t−1 + C∗
ztρt−1 + ρtC∗
J + (C∗
z + 1)C∗
J + C∗
D)
∥gae-ls
t
−g∗∥2 ≤O(tρ2t−1 + C∗
ztρt−1 + ρtC∗
J + (C∗
z + 1)C∗
J + C∗
D + C∗
h)
(15)
∥gdec
t
−˜g∥2 ≤O(ρt + ˜Cz)
∥gae-lasso
t
−˜g∥2 ≤O(tρ2t−1 + ˜Cztρt−1 + ρt ˜CJ + ( ˜Cz + 1) ˜CJ + ˜CD + Ct
h)
∥gae-ls
t
−˜g∥2 ≤O(tρ2t−1 + ˜Cztρt−1 + ρt ˜CJ + ( ˜Cz + 1) ˜CJ + ˜CD)
(16)
where C∗
z ≜O(∥ˆz−z∗∥2), C∗
J ≜O(∥ˆJ−J∗∥2), C∗
D ≜O(∥D−D∗∥2), and C∗
h is an upper bound
on MJ∥∂h(z∗)∥2. Similarly, we have ˜Cz ≜O(∥ˆz−˜z∥2), C∗
J ≜O(∥ˆJ−˜J∥2), C∗
D ≜O(∥D−˜D∥2),
and Ct
h is an upper bound on MJ∥∂h(zt)∥2.
Remarks
Several factors affect the upper bounds in Theorem 3.4: the number of unfolded iterations,
the current estimate of the dictionary, code, Jacobian and, more importantly, the sub-derivative of ℓ1
norm. As we analyzed the effect of unfolding in Theorem 3.3, let us study the bounds as t →∞. In
this case, gdec
∞and gae-lasso
∞
are equivalent, so we compare only gae-lasso
∞
with gae-ls
∞. We have
∥gae-lasso
∞
−g∗∥2 ≤O(C∗),
∥gae-ls
∞
−g∗∥2 ≤O(C∗+ C∗
h),
∥gae-lasso
∞
−˜g∥2 ≤O( ˜C + Ct
h)
∥gae-ls
∞
−˜g∥2 ≤O( ˜C)
(17)
where C∗≜(C∗
z + 1)C∗
J + C∗
D, and ˜C ≜( ˜Cz + 1) ˜CJ + ˜CD. For errors w.r.t g∗, the upper bound
on gae-ls
∞
has an extra term (i.e., C∗
h) compared to gae-lasso
∞
. This highlights that gae-lasso
∞
is a better
estimator of g∗(Figure 4a). Similarly, we argue that gae-ls
∞
is a better estimator of ˜g (Figure 4b).
Implications of such gradient estimation are seen in dictionary learning where gae-ls
∞
recovers ˜D
better (Figures 4c and 4d). In Figure 4c, the encoder unfolds for T = 25, hence the phenomenon of
implicit acceleration is seen in faster and better dictionary learning performance of gae-lasso
∞
than gdec
∞.
In Figure 4d where T = 100, similar performance of gdec
∞and gae-lasso
∞
illustrates their asymptotic
equivalence as t →∞. Additionally, we observed that the learned dictionary by gdec
∞and gae-lasso
∞
is
closer to D∗than ˜D. This is reversed for gae-ls
∞, i.e., the learned dictionary is closer to ˜D.
0
50
100
Number of unfolding [t]
10−2
10−1
100
||g −g∗||2
(a) Convergence for g∗.
0
50
100
Number of unfolding [t]
||g −˜g||2
(b) Convergence for ˜g.
0
250
500
Number of epochs
||D−˜D||2
|| ˜D||2
(c) Learning (T = 25).
0
250
500
Number of epochs
||D−˜D||2
|| ˜D||2
gdec
t
gae−lasso
t
gae−ls
t
(d) Learning (T = 100).
Figure 4: Results for PUDLE’s global convergence (Theorem 3.4) and dictionary learning.
7

Proof sketch (full details in Appendix A.5). Given inﬁnite samples, we replace the sample mean with
expectation in their limit. The proof is similar to Theorem 3.3. The key difference is that we
ﬁrst upper bound the errors w.r.t ∥zt −z∗∥2 and ∥zt −˜z∥2. More importantly, in this case, 0 ∈
∇1Lx(z∗, D∗) + ∂h(z∗) and 0 = ∇1Lx(˜z, ˜D). We bound the error for gdec
t
using the constant L2.
We re-write the errors of gradients gae-lasso
t
and gae-ls
t
as following
gae-lasso
t
−g∗= Ex∈X [Q(z∗, Jt)(zt −z∗)] + Ex∈X [Q21
t (z∗)] + Ex∈X [JtQlasso-11
t
(z∗)]
gae-ls
t
−g∗= Ex∈X [Q(z∗, Jt)(zt −z∗)] + Ex∈X [Q21
t (z∗)] + Ex∈X [JtQls-11
t
(z∗)]
(18)
gae-lasso
t
−˜g = Ex∈X [Q(˜z, Jt)(zt −˜z)] + Ex∈X [Q21
t (˜z)] + Ex∈X [JtQlasso-11
t
(˜z)]
gae-ls
t
−˜g = Ex∈X [Q(˜z, Jt)(zt −˜z)] + Ex∈X [Q21
t (˜z)] + Ex∈X [JtQls-11
t
(˜z)]
(19)
where Q21
t (z), Qlasso-11
t
(z), Qls-11
t
(z), and Q(z, J) are deﬁned as in Theorem 3.3. Given Assump-
tion 3.1 and Lemma 3.2, we ﬁnd an upper bound on r.h.s.
■
Lemma 3.2 (Global bounds). From global gradient errors in Theorem 3.4, the following are satisﬁed
∥Q21
t (z∗)∥2 ≤(L21/2)∥zt −z∗∥2
2
∥Qlasso-11
t
(z∗)∥2 ≤(L11/2 + Lh/2)∥zt −z∗∥2
2 + LD∥D −D∗∥2
∥Qls-11
t
(z∗)∥2 ≤(L11/2)∥zt −z∗∥2
2 + LD∥D −D∗∥2 + ∥∂h(z∗)∥2
∥Q(z∗, Jt)∥2 ≤L1∥Jt −J∗∥2
(20)
∥Q21
t (˜z)∥2 ≤(L21/2)∥zt −˜z∥2
2
∥Qlasso-11
t
(˜z)∥2 ≤(L11/2)∥zt −˜z∥2
2 + LD∥D −˜D∥2 + ∥∂h(zt)∥2
∥Qls-11
t
(˜z)∥2 ≤(L11/2)∥zt −˜z∥2
2 + LD∥D −˜D∥2
∥Q(˜z, Jt)∥2 ≤L1∥Jt −˜J∥2.
(21)
Towards unbiased estimation
As long as λ is ﬁxed within the PUDLE architecture, all deﬁned
gradients remain biased estimators of ˜g, due to the biased estimation of the code ˜z through ℓ1 norm.
This bias exists as long as dictionary learning is performed strictly using lasso through Algorithm 1.
We empirically show in Section 4 that by decaying λ at each unfolded layer, this bias vanishes, and
the training converges to ˜D. We conjecture that in this case the sequence of D∗(λt) converges to ˜D.
We leave the theoretical analysis of this setting for future work.
Beyond dictionary learning
Our results are founded on three main properties: Lipschitz differ-
entiability of the loss relating to the parameter of interest, proximal gradient descent, and strong
convexity in ﬁnite-iteration. The ﬁndings can be applied to other min-min optimization problems,
e.g., ridge regression and logistic regression, following such properties. For example, our analysis
generalizes to the unfolded network in [16] for learning dictionaries using data from the natural
exponential family. In this case, the least-squares loss is replaced with negative log-likelihood, and
the dictionary models the expectation of the data [16].
Limitations
Finite-iteration support selection (Proposition 3.1) [55] and strong convexity may
seem stringent going beyond dictionary learning. Ablin et al. brieﬂy discuss generalization of local
gradient convergence by relaxing strong convexity to the p-Łojasiewicz property [31, 64]. Although
our analysis considered only the noiseless setting, we conjecture that the relative comparison of
the gradients in the presence of noise still holds, where the upper bounds will involve an additional
term related to the noise. This paper focused on inﬁnite sample convergence as the goal was to
highlight the relative differences between the gradients. With more assumptions on the code statistics,
ﬁnite-sample upper bounds can be derived similar to [28, 30].
4
Experiments
Dictionary learning
We focus on the performance of our best-performing gradient esti-
mators gae-ls
t
, and compare it with NOODL [29], a state-of-the-art online algorithm, and
SPORCO [65], an alternating-minimization algorithm that uses lasso.
NOODL, which
8

uses iterative hard-thresholding (HT) for sparse coding and a gradient update employ-
ing the code’s sign, has linear convergence upon proper initialization [29].
We train:
0
250
500
750
Number of iterations
10−7
10−5
10−3
10−1
||D−˜D||2
|| ˜D||2
gae−ls
t
gae−ls,decay
t
gae−ls,HT
t
NOODL
SPORCO
Figure 5: Dictionary convergence of
gae-ls
t
, NOODL [29] and SPORCO [65].
• gae-ls
t
: λ is ﬁxed across iterations.
• gae-ls, decay
t
: λ decays (i.e., λt = λβt, with 0 < β < 1)
where β decreases as training progresses.
• gae-ls, HT
t
: Pαh(v) is replaced with HTb(v) ≜v1|v|≥b.
The intuition behind decreasing λ across iterations is to
have an unbiased estimation of the code amplitudes. With
HT, the sparse coding step reduces to that from NOODL.
In this case, we highlight the difference between the gradi-
ent update of our method (backpropagation) with NOODL.
Figure 5 shows the convergence of D∈R1000×1500 to ˜D
for when the code is 20-sparse (for other sparsity levels
and details see Appendix B). A biased estimation of the
code amplitudes results in convergence only to a neighbourhood of the dictionary [29]. This is
observed in the convergence of SPORCO (ﬁnal error is shown) and gae-ls
t
. The convergence of gae-ls
t
to
a closer neighbourhood than SPORCO supports Theorem 3.4. Moreover, with decaying λ, the code
estimation bias vanishes, hence gae-ls, decay
t
and gae-ls, HT
t
converges to ˜D similar to NOODL. Here, we
focus on convergence, as η across methods is not comparable.
Table 1: Denoising of BSD68 [66].
Method
PSNR [dB]
λ
0.08
0.12
0.16
0.2
gdec
t
24.31 24.73 25.24 24.89
gae-ls
t
24.80 25.48 25.67 25.51
b
0.02
0.05
0.08
0.1
gae-ls, HT
t
22.87 25.40 24.68 23.77
Supervised image denoising
We focus on gae-ls
t
and
gdec
t
to highlight the impact of backpropagation, and also
consider gae-ls, HT
t
where the proximal operator is replaced
with HT. This is to compare with sparse coding scheme of
NOODL. We do not compare against NOODL’s dictionary
update, as this computation for two-dimensional convo-
lutions is not straightforward. Prior works have shown
that variants of PUDLE either rival or outperform state-of-
the-art architectures for image denoising [14, 16]. Thus,
we focus on a comparative analysis of the gradients. We
trained on 432 and tested on 68 images from BSD [66].
We used a convolutional dictionary and corrupted images
with zero-mean Gaussian noise of standard deviation of 25 (see Appendix B for details). Table 1
shows the denoising performance of soft-thresholding using λ, and HT with b in peak signal-to-
noise-ratio (PSNR). The result shows that the advantage of gae-ls
t
over gdec
t
is not limited to dictionary
learning and is seen in denoising. Additionally, the superior performance of gae-ls
t
compared to
gae-ls, HT
t
highlights the advantage of PUDLE (i.e., ℓ1-based unfolding) against HT used in NOODL.
5
Conclusions
This paper studied dictionary learning and analyzed the dynamics of unfolded sparse coding net-
works through a provable unfolded dictionary learning (PUDLE) framework. We provided the ﬁrst
theoretical results on network convergence when trained by backpropagation. The results highlight
the interplay between neurally plausibility of sparse coding and dictionary learning, speciﬁcally the
effect of learning the dictionary by backpropagation through the sparse coding step. We proved how
backpropagation exhibits an implicit acceleration of gradient convergence and dictionary learning as
a function of unfolded sparse coding iterations, compared to classical alternating-minimization-based
gradients. We showed how the loss function can ameliorate the gradient bias. We demonstrated that
this bias could be further reduced and eliminated by decaying the regularization parameter within
the unfolded layers. We showed that this result goes beyond dictionary learning where PUDLE
outperforms the NOODL sparse coding scheme in an image denoising task [29].
9

References
[1] R. Tibshirani, “Regression shrinkage and selection via the lasso,” Journal of the Royal Statistical Society.
Series B (Methodological), vol. 58, no. 1, pp. 267–288, 1996.
[2] S. S. Chen, D. L. Donoho, and M. A. Saunders, “Atomic decomposition by basis pursuit,” SIAM review,
vol. 43, no. 1, pp. 129–159, 2001.
[3] B. A. Olshausen and D. J. Field, “Sparse coding with an overcomplete basis set: A strategy employed by
v1?” Vision research, vol. 37, no. 23, pp. 3311–3325, 1997.
[4] T. Hastie, R. Tibshirani, and M. Wainwright, Statistical learning with sparsity: the lasso and generaliza-
tions.
CRC press, 2015.
[5] M. Elad, Sparse and redundant representations: from theory to applications in signal and image processing.
Springer Science & Business Media, 2010.
[6] M. Elad and M. Aharon, “Image denoising via sparse and redundant representations over learned dictionar-
ies,” IEEE Transactions on Image Processing, vol. 15, no. 12, pp. 3736–3745, 2006.
[7] J. Yang, J. Wright, T. S. Huang, and Y. Ma, “Image super-resolution via sparse representation,” IEEE
Transactions on Image Processing, vol. 19, no. 11, pp. 2861–2873, 2010.
[8] J. Mairal, J. Ponce, G. Sapiro, A. Zisserman, and F. Bach, “Supervised dictionary learning,” in Proc.
Advances in Neural Information Processing Systems, vol. 21, 2009, pp. 1–8.
[9] M. a. Ranzato, C. Poultney, S. Chopra, and Y. Cun, “Efﬁcient learning of sparse representations with an
energy-based model,” in Advances in Neural Information Processing Systems, vol. 19.
MIT Press, 2007.
[10] M. a. Ranzato, Y.-l. Boureau, and Y. Cun, “Sparse feature learning for deep belief networks,” in Proc.
Advances in Neural Information Processing Systems, J. Platt, D. Koller, Y. Singer, and S. Roweis, Eds.,
vol. 20, 2008.
[11] K. Gregor and Y. LeCun, “Learning fast approximations of sparse coding,” in Proc. international conference
on international conference on machine learning, 2010, pp. 399–406.
[12] J. R. Hershey, J. L. Roux, and F. Weninger, “Deep unfolding: Model-based inspiration of novel deep
architectures,” arXiv:1409.2574, pp. 1–27, 2014.
[13] V. Monga, Y. Li, and Y. C. Eldar, “Algorithm unrolling: Interpretable, efﬁcient deep learning for signal and
image processing,” arXiv:1912.10557, pp. 1–27, 2019.
[14] D. Simon and M. Elad, “Rethinking the csc model for natural images,” in Proc. Advances in Neural
Information Processing Systems, vol. 32, 2019, pp. 1–11.
[15] B. Tolooshams, S. Dey, and D. Ba, “Deep residual autoencoders for expectation maximization-inspired
dictionary learning,” IEEE Transactions on Neural Networks and Learning Systems, pp. 1–15, 2020.
[16] B. Tolooshams, A. H. Song, S. Temereanca, and D. Ba, “Convolutional dictionary learning based auto-
encoders for natural exponential-family distributions,” in Proc. International Conference on Machine
Learning, 2020, pp. 1–11.
[17] Z. Wang, D. Liu, J. Yang, W. Han, and T. Huang, “Deep networks for image super-resolution with sparse
prior,” in Proc. IEEE International Conference on Computer Vision, 2015, pp. 370–378.
[18] O. Solomon, R. Cohen, Y. Zhang, Y. Yang, Q. He, J. Luo, R. J. G. van Sloun, and Y. C. Eldar, “Deep
unfolded robust pca with application to clutter suppression in ultrasound,” IEEE Transactions on Medical
Imaging, vol. 39, no. 4, pp. 1051–1063, 2020.
[19] C. J. Schuler, M. Hirsch, S. Harmeling, and B. Schölkopf, “Learning to deblur,” IEEE Transactions on
Pattern Analysis and Machine Intelligence, vol. 38, no. 7, pp. 1439–1451, 2016.
[20] Y. Li, M. Toﬁghi, J. Geng, V. Monga, and Y. C. Eldar, “Efﬁcient and interpretable deep blind image
deblurring via algorithm unrolling,” IEEE Transactions on Computational Imaging, vol. 6, pp. 666–681,
2020.
[21] A. G. Baydin, B. A. Pearlmutter, A. A. Radul, and J. M. Siskind, “Automatic differentiation in machine
learning: a survey,” Journal of machine learning research, vol. 18, 2018.
[22] Y. A. LeCun, L. Bottou, G. B. Orr, and K.-R. Müller, “Efﬁcient backprop,” in Neural networks: Tricks of
the trade.
Springer, 2012, pp. 9–48.
[23] B. Tolooshams, S. Dey, and D. Ba, “Scalable convolutional dictionary learning with constrained recurrent
sparse auto-encoders,” in Proc. IEEE International Workshop on Machine Learning for Signal Processing,
2018, pp. 1–6.
[24] L. Franceschi, M. Donini, P. Frasconi, and M. Pontil, “Forward and reverse gradient-based hyperparameter
optimization,” in Proc. International Conference on Machine Learning, 2017, pp. 1165–1173.
10

[25] M. Feurer and F. Hutter, “Hyperparameter optimization,” in Automated Machine Learning.
Springer,
Cham, 2019, pp. 3–33.
[26] I. Daubechies, M. Defrise, and C. De Mol, “An iterative thresholding algorithm for linear inverse problems
with a sparsity constraint,” Communications on Pure and Applied Mathematics, vol. 57, no. 11, pp.
1413–1457, 2004.
[27] T. Blumensath and M. E. Davies, “Iterative thresholding for sparse approximations,” Journal of Fourier
analysis and Applications, vol. 14, no. 5-6, pp. 629–654, 2008.
[28] N. S. Chatterji and P. L. Bartlett, “Alternating minimization for dictionary learning: Local convergence
guarantees,” arXiv:1711.03634, pp. 1–26, 2017.
[29] S. Rambhatla, X. Li, and J. Haupt, “Noodl: Provable online dictionary learning and sparse coding,” in
Proc. International Conference on Learning Representations, 2018, pp. 1–11.
[30] S. Arora, R. Ge, T. Ma, and A. Moitra, “Simple, efﬁcient, and neural algorithms for sparse coding,” in
Proc. Conference on Learning Theory, ser. Proc. Machine Learning Research, P. Grünwald, E. Hazan, and
S. Kale, Eds., vol. 40.
Paris, France: PMLR, 03–06 Jul 2015, pp. 113–149.
[31] P. Ablin, G. Peyré, and T. Moreau, “Super-efﬁciency of automatic differentiation for functions deﬁned as a
minimum,” in Proc. International Conference on Machine Learning.
PMLR, 2020, pp. 32–41.
[32] D. A. Spielman, H. Wang, and J. Wright, “Exact recovery of sparsely-used dictionaries,” in Proc. Annual
Conference on Learning Theory, ser. PMRL, vol. 23, 2012, pp. 37.1–37.18.
[33] B. Barak, J. A. Kelner, and D. Steurer, “Dictionary learning and tensor decomposition via the sum-of-
squares method,” in Proc. Annual ACM Symposium on Theory of Computing, ser. STOC ’15, 2015, p.
143–151.
[34] M. Aharon, M. Elad, and A. Bruckstein, “K-svd: An algorithm for designing overcomplete dictionaries for
sparse representation,” IEEE Transactions on Signal Processing, vol. 54, no. 11, pp. 4311–4322, 2006.
[35] K. Engan, S. Aase, and J. Hakon Husoy, “Method of optimal directions for frame design,” in Proc. IEEE
International Conference on Acoustics, Speech, and Signal Processing, vol. 5, 1999, pp. 2443–2446 vol.5.
[36] P. Jain, P. Netrapalli, and S. Sanghavi, “Low-rank matrix completion using alternating minimization,” in
Proc. Annual ACM Symposium on Theory of Computing, 2013, p. 665–674.
[37] A. Agarwal, A. Anandkumar, P. Jain, P. Netrapalli, and R. Tandon, “Learning sparsely used overcomplete
dictionaries,” in Proc the 27th Conference on Learning Theory, ser. Proc. Machine Learning Research,
M. F. Balcan, V. Feldman, and C. Szepesvári, Eds., vol. 35.
Barcelona, Spain: PMLR, 13–15 Jun 2014,
pp. 123–137.
[38] S. Arora, R. Ge, and A. Moitra, “New algorithms for learning incoherent and overcomplete dictionaries,”
in Proc. the 27th Conference on Learning Theory, ser. Proc. Machine Learning Research, M. F. Balcan,
V. Feldman, and C. Szepesvári, Eds., vol. 35.
Barcelona, Spain: PMLR, 13–15 Jun 2014, pp. 779–806.
[39] J. Mairal, F. Bach, J. Ponce, and G. Sapiro, “Online dictionary learning for sparse coding,” in Proc. Annual
International Conference on Machine Learning, 2009, p. 689–696.
[40] A. Rangamani, A. Mukherjee, A. Basu, A. Arora, T. Ganapathi, S. Chin, and T. D. Tran, “Sparse coding
and autoencoders,” in Proc. IEEE International Symposium on Information Theory (ISIT), 2018, pp. 36–40.
[41] T. V. Nguyen, R. K. Wong, and C. Hegde, “On the dynamics of gradient descent for autoencoders,” in Proc.
International Conference on Artiﬁcial Intelligence and Statistics.
PMLR, 2019, pp. 2858–2867.
[42] P. Sprechmann, A. Bronstein, and G. Sapiro, “Learning efﬁcient structured sparse models,” in Proc.
International Coference on International Conference on Machine Learning, 2012, p. 219–226.
[43] B. Xin, Y. Wang, W. Gao, D. Wipf, and B. Wang, “Maximal sparsity with deep networks?” in Advances in
Neural Information Processing Systems, D. Lee, M. Sugiyama, U. Luxburg, I. Guyon, and R. Garnett, Eds.,
vol. 29, 2016, pp. 1–9.
[44] T. Moreau and J. Bruna, “Understanding trainable sparse coding via matrix factorization,” in Proc. 5th
International Conference on Learning Representations, 2017, pp. 1–13.
[45] R. Giryes, Y. C. Eldar, A. M. Bronstein, and G. Sapiro, “Tradeoffs between convergence speed and
reconstruction accuracy in inverse problems,” IEEE Transactions on Signal Processing, vol. 66, no. 7, pp.
1676–1690, 2018.
[46] X. Chen, J. Liu, Z. Wang, and W. Yin, “Theoretical linear convergence of unfolded ista and its practical
weights and thresholds,” in Proc. Advances in Neural Information Processing Systems, vol. 31, 2018, pp.
1–11.
[47] J. Liu and X. Chen, “Alista: Analytic weights are as good as learned weights in lista,” in Proc. International
Conference on Learning Representations, 2019.
11

[48] P. Ablin, T. Moreau, M. Massias, and A. Gramfort, “Learning step sizes for unfolded sparse coding,” in
Proc. Advances in Neural Information Processing Systems, vol. 32, 2019, pp. 1–11.
[49] M. J. Wainwright, “Sharp thresholds for high-dimensional and noisy sparsity recovery using ℓ1-constrained
quadratic programming (lasso),” IEEE transactions on information theory, vol. 55, no. 5, pp. 2183–2202,
2009.
[50] E. J. Candès and Y. Plan, “Near-ideal model selection by ℓ1 minimization,” The Annals of Statistics, vol. 37,
no. 5A, pp. 2145 – 2177, 2009.
[51] R. J. Tibshirani, “The lasso problem and uniqueness,” Electronic Journal of Statistics, vol. 7, no. none, pp.
1456 –1490, 2013.
[52] S. Rosset, J. Zhu, and T. Hastie, “Boosting as a regularized path to a maximum margin classiﬁer,” The
Journal of Machine Learning Research, vol. 5, pp. 941–973, 2004.
[53] N. Parikh and S. Boyd, “Proximal algorithms,” Foundations and Trends in optimization, vol. 1, no. 3, pp.
127–239, 2014.
[54] J. C. Gilbert, “Automatic differentiation and iterative processes,” Optimization methods and software,
vol. 1, no. 1, pp. 13–21, 1992.
[55] E. T. Hale, W. Yin, and Y. Zhang, “A ﬁxed-point continuation method for l1-regularized minimization with
applications to compressed sensing,” CAAM TR07-07, Rice University, vol. 43, p. 44, 2007.
[56] A. Beck and M. Teboulle, “A fast iterative shrinkage-thresholding algorithm for linear inverse problems,”
SIAM journal on imaging sciences, vol. 2, no. 1, pp. 183–202, 2009.
[57] S. Tao, D. Boley, and S. Zhang, “Local linear convergence of ista and ﬁsta on the lasso problem,” SIAM
Journal on Optimization, vol. 26, no. 1, pp. 313–336, 2016.
[58] K. Bredies and D. A. Lorenz, “Linear convergence of iterative soft-thresholding,” Journal of Fourier
Analysis and Applications, vol. 14, no. 5-6, pp. 813–837, 2008.
[59] L. Zhang, Y. Hu, C. Li, and J.-C. Yao, “A new linear convergence result for the iterative soft thresholding
algorithm,” Optimization, vol. 66, no. 7, pp. 1177–1189, 2017.
[60] Q. Bertrand, Q. Klopfenstein, M. Massias, M. Blondel, S. Vaiter, A. Gramfort, and J. Salmon, “Implicit
differentiation for fast hyperparameter selection in non-smooth convex learning,” arXiv:2105.01637, 2021.
[61] Y. Bengio, “Gradient-based optimization of hyperparameters,” Neural computation, vol. 12, no. 8, pp.
1889–1900, 2000.
[62] O. Devolder, F. Glineur, Y. Nesterov et al., “First-order methods with inexact oracle: the strongly convex
case,” UniversitÃ© catholique de Louvain, Center for Operations Research and . . . , Tech. Rep., 2013.
[63] O. Devolder, F. Glineur, and Y. Nesterov, “First-order methods of smooth convex optimization with inexact
oracle,” Mathematical Programming, vol. 146, no. 1, pp. 37–75, 2014.
[64] H. Attouch and J. Bolte, “On the convergence of the proximal algorithm for nonsmooth functions involving
analytic features,” Mathematical Programming, vol. 116, no. 1, pp. 5–16, 2009.
[65] B. Wohlberg, “Sporco: A python package for standard and convolutional sparse representations,” in Proc.
the 15th Python in Science Conference, Austin, TX, USA, 2017, pp. 1–8.
[66] D. Martin, C. Fowlkes, D. Tal, and J. Malik, “A database of human segmented natural images and its
application to evaluating segmentation algorithms and measuring ecological statistics,” in Proc. IEEE
International Conference on Computer Vision, vol. 2, 2001, pp. 416–423.
[67] A. Paszke, S. Gross, S. Chintala, G. Chanan, E. Yang, Z. DeVito, Z. Lin, A. Desmaison, L. Antiga, and
A. Lerer, “Automatic differentiation in pytorch,” 2017.
[68] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,” arXiv:1412.6980, 2014.
12

A
Appendix
Deﬁnition A.1 (Lipschitz function). A function f : Rm →Rp is L-Lipschitz w.r.t a norm ∥· ∥if
∃L > 0 s.t. ∥f(a) −f(b)∥≤L∥a −b∥∀a, b ∈Rm.
Deﬁnition A.2 (Lipschitz differentiable function). A twice differentiable function f : Rm →Rp is
L-Lipschitz differentiable w.r.t a norm ∥· ∥iff ∃L > 0 s.t. ∥∇2f(a)∥≤L ∀a ∈Rm.
Deﬁnition A.3 (Strong convexity). A twice differentiable function f : Rm →Rp is strongly convex
if ∃µ > 0 s.t. ∇2f(a) ⪰µI.
Lemma A.1 (Strong convexity of reconstruction loss). Given the support selection (Proposition 3.1),
DT
SDS is full-rank. Thus, ∀t > B, Lx(zt, D) = Lx(zt,S, DS) is strongly convex (Deﬁnition A.3) in
z.
Lemma A.2 (Lipschitz mapping). Given the recursion zt+1 = Φ(zt) = Pαh(zt −α∇1Lx(zt, D)),
from Lemma A.1, there exist B > 0 such that loss Lx(zt, D) is µ-strongly convex ∀t > B. Hence,
using Lemma 2.3,
∥∇1Φ(zt, D)∥2 = ∥(I −α∇2
11L(zt, D))∂Pαh(zt)∥2 ≤ρ
(22)
where ρ ≜cprox(1 −αµ) < 1.
Lemma A.3 (Lasso optimality). Lasso Karush-Kuhn-Tucker (KKT) optimality conditions are
ˆz ∈arg min
z∈Rp
fx(z, D) ⇔DT(x−Dˆz) ∈λ∂∥ˆz∥1, and |ˆzj| =
{sign(ˆzj)}
if ˆzj ̸= 0
[−1, 1]
if ˆzj = 0 , ∀j ∈[1, p].
(23)
A.1
Proof of Theorem 3.1
Theorem 3.1 (Forward pass code convergence). Given the iterative encoder zt+1 = Φ(zt, D), As-
sumption 2.3, and Lemmas 2.1, A.1 and A.2,
∃ρ < 1, B > 0
s.t.
∥zt −ˆz∥2 ≤O(ρt)
∀t > B
(9)
where ˆz is the unique minimizer of lasso (1).
Proof. Given the support selection at iteration B, from Lemma A.1, we have ∇2
11L(zt, D) ⪰µI
restricted to the support for t > B. Then, from Lemma A.2, we get
∥∇1Φ(zt, D)∥2 = ∥(I −α∇2
11L(zt, D))∂Pαh(zt)∥2 ≤ρ
where ρ ≜cprox(1 −αµ) < 1. Hence, using ﬁxed-point property (Lemma 2.1)
∃B > 0, s.t. ∥zt+1 −ˆz∥2 = ∥Φ(zt) −Φ(ˆz)∥2 ≤ρ∥zt −ˆz∥2 ∀t > B
where ˆz = arg min fx(z, D). Unrolling the recursion,
∥zt −ˆz∥2 ≤ρt−B∥zB −ˆz∥2.
■
A.2
Proof of Theorem 3.2
Theorem 3.2 (Forward pass Jacobian convergence). Given the recursion zt+1 = Φ(zt, D), and ˆz
the unique minimizer of lasso with Jacobian ˆJ,
∃ρ < 1, B > 0
s.t.
∥Jt −ˆJ∥2 ≤O(tρt−1)
∀t > B.
(10)
Proof. Differentiating the recursion,
Jt+1 = ∇1Φ(zt, D)Jt + ∇2Φ(zt, D).
Similarly,
ˆJ = ∇1Φ(ˆz, D)ˆJ + ∇2Φ(ˆz, D)
13

where ˆz is a minimizer of lasso and ﬁxed-point of the mapping (see Lemma 2.1). Subtract the terms
Jt+1 −ˆJ = ∇1Φ(zt, D)(Jt −ˆJ) + (∇1Φ(zt, D) −∇1Φ(ˆz, D))ˆJ + (∇2Φ(zt, D) −∇2Φ(ˆz, D))
Given the Lipschitz properties of L and h, we can further get the upper bounds on ∥∇1Φ(a, D) −
∇1Φ(b, D)∥2 ≤LΦ1 and ∥∇2Φ(a, D) −∇2Φ(b, D)∥2 ≤LΦ2. Hence, with upper bound on the
norm of Jacobian (Assumption 3.1), there exists B > 0 such that ∀t > B
∥Jt+1 −ˆJ∥2 ≤∥∇1Φ(zt, D)∥2∥Jt −ˆJ∥2 + ∥∇1Φ(zt, D) −∇1Φ(ˆz, D)∥2∥ˆJ∥2
+ ∥∇2Φ(zt, D) −∇2Φ(ˆz, D)∥2
≤ρ∥Jt −ˆJ∥2 + c∥zt −ˆz∥2
where c ≜MJLΦ1 + LΦ2. Hence,
∥Jt+1 −ˆJ∥2 ≤ρ∥Jt −ˆJ∥2 + O(ρt).
Unrolling the recursion,
∥Jt+1 −ˆJ∥2 ≤O((t + 1)ρt).
■
A.3
Proof of Theorem 3.3
Theorem 3.3 (Local convergence of gradients). Given the convergence results from the forward
pass (Theorems 3.1 and 3.2), ∃ρ < 1, B > 0 such that ∀t > B, the errors of gradients deﬁned in
Algorithm 2 w.r.t ˆg (5) satisfy
∥gdec
t
−ˆg∥2 ≤O(ρt)
∥gae-lasso
t
−ˆg∥2 ≤O(tρ2t−1)
∥gae-ls
t
−ˆg∥2 ≤O(tρ2t−1 + ˆCh)
(11)
where ˆCh is an upper bound on MJ∥∂h(ˆz)∥2.
Proof. For gdec
t , with the inﬁnite fresh samples, we have limn→∞1
n
Pn
i=1 ∇2Lxi(zi
t, D) =
Ex∈X [∇2Lx(zt, D)] a.s. Based on Lemma 2.2, we get
∥gdec
t
−ˆg∥2 = ∥Ex∈X [∇2Lx(zt, D)] −Ex∈X [∇2Lx(ˆz, D)]∥2
≤Ex∈X [∥∇2Lx(zt, D) −∇2Lx(ˆz, D)∥2] ≤Ex∈X [L2∥zt −ˆz∥2] ≤O(ρt).
(24)
Similarly, for gae-lasso
t
and gae-ls
t
, we replace the sample mean for gradient computations with expecta-
tion in their limit. We re-write the gradient estimation error as following
gae-lasso
t
−ˆg = Ex∈X [Q(ˆz, Jt)(zt −ˆz)] + Ex∈X [Q21
t (ˆz)] + Ex∈X [JtQlasso-11
t
(ˆz)]
gae-ls
t
−ˆg = Ex∈X [Q(ˆz, Jt)(zt −ˆz)] + Ex∈X [Q21
t (ˆz)] + Ex∈X [JtQls-11
t
(ˆz)]
(25)
where
Q21
t (z) ≜∇2Lx(zt, D) −∇2Lx(z, D) −∇2
21Lx(z, D)(zt −z)
Qlasso-11
t
(z) ≜∇1Lx(zt, D) + ∂h(zt) −∇2
11Lx(z, D)(zt −z)
Qls-11
t
(z) ≜∇1Lx(zt, D) −∇2
11Lx(z, D)(zt −z)
Q(z, J) ≜J∇2
11Lx(z, D) + ∇2
21Lx(z, D).
(26)
Hence, it sufﬁces to bound the terms on the r.h.s. Given Assumption 3.1 and Lemma 3.1,
∥gae-lasso
t
−ˆg∥2 ≤Ex∈X [L1∥Jt −ˆJ∥2∥zt −ˆz∥2 +(L21/2)∥zt −ˆz∥2
2 +MJ(L11/2+ Lh/2)∥zt −ˆz∥2
2].
(27)
Using the convergence errors from the forward pass (Theorems 3.1 and 3.2),
∥gae-lasso
t
−ˆg∥2 ≤L1O(tρ2t−1) + (L21/2 + MJ(L11/2 + Lh/2)) O(ρ2t) = O(tρ2t−1).
(28)
Similarly,
∥gae-ls
t
−ˆg∥2 ≤Ex∈X [L1∥Jt−ˆJ∥2∥zt−ˆz∥2+(L21/2)∥zt−ˆz∥2
2+MJ((L11/2)∥zt−ˆz∥2
2+∥∂h(ˆz)∥2)].
(29)
Using the convergence errors from the forward pass (Theorems 3.1 and 3.2),
∥gae-ls
t
−ˆg∥2 ≤L1O(tρ2t−1) + (L21/2 + MJL11/2) O(ρ2t) + MJ∥∂h(ˆz)∥2 = O(tρ2t−1 + ˆCh).
(30)
■
14

A.4
Proof of Lemma 3.1
Lemma 3.1 (Local bounds). From local gradient errors in Theorem 3.3, the following are satisﬁed
∥Q21
t (ˆz)∥2 ≤(L21/2)∥zt −ˆz∥2
2,
∥Q(ˆz, Jt)∥2 ≤L1∥Jt −ˆJ∥2,
∥Qlasso-11
t
(ˆz)∥2 ≤(L11/2 + Lh/2)∥zt −ˆz∥2
2
∥Qls-11
t
(ˆz)∥2 ≤(L11/2)∥zt −ˆz∥2
2 + ∥∂h(ˆz)∥2.
(14)
Proof. For Q21
t (ˆz), given convexity of ∇1Lx(z, D) and its domain (Assumption 2.1) and Lemma 2.2,
we achieve the quadratic upper bound. For Qlasso-11
t
(ˆz), we add and subtract ∇1Lx(ˆz, D), and then
use quadratic upper bound. At line four, given Lemma A.3, we use 0 ∈∇1Lx(ˆz, D) + ∂h(ˆz) and
also the quadratic upper bound on the function h (Lemma 2.3).
∥Qlasso-11
t
∥2 = ∥∇1Lx(zt, D) + ∂h(zt) −∇2
11Lx(ˆz, D)(zt −ˆz)∥
= ∥∇1Lx(zt, D) −∇1Lx(ˆz, D) + ∇1Lx(ˆz, D) + ∂h(zt) −∇2
11Lx(ˆz, D)(zt −ˆz)∥
≤(L11/2)∥zt −ˆz∥2
2 + ∥∂h(zt) + ∇1Lx(ˆz, D)∥2
∈(L11/2)∥zt −ˆz∥2
2 + ∥∂h(zt) −∂h(ˆz)∥2
≤(L11/2 + Lh/2)∥zt −ˆz∥2
2.
(31)
Similarly,
∥Qls-11
t
∥2 = ∥∇1Lx(zt, D) −∇2
11Lx(ˆz, D)(zt −ˆz)∥
= ∥∇1Lx(zt, D) −∇1Lx(ˆz, D) + ∇1Lx(ˆz, D) −∇2
11Lx(ˆz, D)(zt −ˆz)∥
≤(L11/2)∥zt −ˆz∥2
2 + ∥∇1Lx(ˆz, D)∥2 ∈(L11/2)∥zt −ˆz∥2
2 + ∥∂h(ˆz)∥2.
(32)
For Q(ˆz, Jt), from implicit function theorem, Q(ˆz, ˆJ) = 0. Hence, we can use ∇2
21Lx(ˆz, D) =
−ˆJ∇2
11Lx(ˆz, D) in the following
∥Q(ˆz, Jt)∥2 = ∥Jt∇2
11Lx(ˆz, D) + ∇2
21Lx(ˆz, D)∥2
= ∥Jt∇2
11Lx(ˆz, D) −ˆJ∇2
11Lx(ˆz, D)∥2
≤∥(Jt −ˆJ)∇2
11Lx(ˆz, D)∥2 ≤L1∥Jt −ˆJ∥2.
(33)
■
A.5
Proof of Theorem 3.4
Theorem 3.4 (Global convergence of gradients). Given the convergence results form the forward
pass, (Theorems 3.1 and 3.2), ∃ρ < 1, B > 0 such that ∀t > B, the errors of gradients deﬁned in
Algorithm 2 w.r.t global directions g∗and ˜g (deﬁned in (6)) satisfy
∥gdec
t
−g∗∥2 ≤O(ρt + C∗
z)
∥gae-lasso
t
−g∗∥2 ≤O(tρ2t−1 + C∗
ztρt−1 + ρtC∗
J + (C∗
z + 1)C∗
J + C∗
D)
∥gae-ls
t
−g∗∥2 ≤O(tρ2t−1 + C∗
ztρt−1 + ρtC∗
J + (C∗
z + 1)C∗
J + C∗
D + C∗
h)
(15)
∥gdec
t
−˜g∥2 ≤O(ρt + ˜Cz)
∥gae-lasso
t
−˜g∥2 ≤O(tρ2t−1 + ˜Cztρt−1 + ρt ˜CJ + ( ˜Cz + 1) ˜CJ + ˜CD + Ct
h)
∥gae-ls
t
−˜g∥2 ≤O(tρ2t−1 + ˜Cztρt−1 + ρt ˜CJ + ( ˜Cz + 1) ˜CJ + ˜CD)
(16)
where C∗
z ≜O(∥ˆz−z∗∥2), C∗
J ≜O(∥ˆJ−J∗∥2), C∗
D ≜O(∥D−D∗∥2), and C∗
h is an upper bound
on MJ∥∂h(z∗)∥2. Similarly, we have ˜Cz ≜O(∥ˆz−˜z∥2), C∗
J ≜O(∥ˆJ−˜J∥2), C∗
D ≜O(∥D−˜D∥2),
and Ct
h is an upper bound on MJ∥∂h(zt)∥2.
Proof. For gdec
t , we compute the gradient in their limit assuming inﬁnite fresh samples
limn→∞1
n
Pn
i=1 ∇2Lxi(zi
t, D) = Ex∈X [∇2Lx(zt, D)] a.s.. Based on Lemma 2.2,
∥gdec
t
−g∗∥2 = ∥Ex∈X [∇2Lx(zt, D)] −Ex∈X [∇2Lx(z∗, D)]∥2
≤Ex∈X [∥∇2Lx(zt, D) −∇2Lx(z∗, D)∥2]
≤Ex∈X [L2∥zt −z∗∥2]
≤O(ρt + C∗
z).
(34)
15

Similar to Theorem 3.3, we re-write the errors of gradients gae-lasso
t
and gae-ls
t
as following
gae-lasso
t
−g∗= Ex∈X [Q(z∗, Jt)(zt −z∗)] + Ex∈X [Q21
t (z∗)] + Ex∈X [JtQlasso-11
t
(z∗)]
gae-ls
t
−g∗= Ex∈X [Q(z∗, Jt)(zt −z∗)] + Ex∈X [Q21
t (z∗)] + Ex∈X [JtQls-11
t
(z∗)]
(35)
and
gae-lasso
t
−˜g = Ex∈X [Q(˜z, Jt)(zt −˜z)] + Ex∈X [Q21
t (˜z)] + Ex∈X [JtQlasso-11
t
(˜z)]
gae-ls
t
−˜g = Ex∈X [Q(˜z, Jt)(zt −˜z)] + Ex∈X [Q21
t (˜z)] + Ex∈X [JtQls-11
t
(˜z)].
(36)
where Q21
t (z), Qlasso-11
t
(z), Qls-11
t
(z), and Q(z, J) are deﬁned as in Theorem 3.3. Given Assump-
tion 3.1 and Lemma 3.2, we ﬁnd an upper bound on r.h.s.
∥gae-lasso
t
−g∗∥2 ≤Ex∈X [L1∥Jt −J∗∥2∥zt −z∗∥2 + (L21/2)∥zt −z∗∥2
2]
+ Ex∈X [MJ(L11/2 + Lh/2)∥zt −z∗∥2
2 + LD∥D −D∗∥2]
≤Ex∈X [L1(∥Jt −ˆJ∥2 + ∥ˆJ −J∗∥2)(∥zt −ˆz∥2 + ∥ˆz −z∗∥2)]
+ Ex∈X [(L21/2)(∥zt −ˆz∥2
2 + ∥ˆz −z∗∥2
2)]
+ Ex∈X [MJ(L11/2 + Lh/2)(∥zt −ˆz∥2
2 + ∥ˆz −z∗∥2
2) + LD∥D −D∗∥2].
(37)
Similarly,
∥gae-ls
t
−g∗∥2 ≤Ex∈X [L1∥Jt −J∗∥2∥zt −z∗∥2 + (L21/2)∥zt −z∗∥2
2]
+ Ex∈X [MJ(L11/2)∥zt −z∗∥2
2 + MJ∥∂h(z∗)∥2 + LD∥D −D∗∥2]
≤Ex∈X [L1(∥Jt −ˆJ∥2 + ∥ˆJ −J∗∥2)(∥zt −ˆz∥2 + ∥ˆz −z∗∥2)]
+ Ex∈X [(L21/2)(∥zt −ˆz∥2
2 + ∥ˆz −z∗∥2
2) + LD∥D −D∗∥2]
+ Ex∈X [MJ(L11/2)(∥zt −ˆz∥2
2 + ∥ˆz −z∗∥2
2) + MJ∥∂h(z∗)∥2].
(38)
Using the convergence errors from the forward pass (Theorems 3.1 and 3.2),
∥gae-lasso
t
−g∗∥2 ≤L1
 O(tρ2t−1 + C∗
ztρt−1 + ρtC∗
J + C∗
zC∗
J)

+ (L21/2 + MJ(L11/2 + Lh/2)) O(ρ2t + C∗
z) + C∗
D
= O(tρ2t−1 + C∗
ztρt−1 + ρtC∗
J + (C∗
z + 1)C∗
J + C∗
D)
(39)
and
∥gae-ls
t
−g∗∥2 ≤L1
 O(tρ2t−1 + C∗
ztρt−1 + ρtC∗
J + C∗
zC∗
J)

+ (L21/2 + MJL11/2) O(ρ2t + C∗
z) + C∗
D + MJ∥∂h(z∗)∥2
= O(tρ2t−1 + C∗
ztρt−1 + ρtC∗
J + (C∗
z + 1)C∗
J + C∗
D)
≤L1O(tρ2t−1) + (L21/2 + MJL11/2) O(ρ2t) + C∗
h
= O(tρ2t−1 + C∗
ztρt−1 + ρtC∗
J + (C∗
z + 1)C∗
J + C∗
D + C∗
h).
(40)
The steps to bound the errors w.r.t ˜g is similar. We have
∥gae-lasso
t
−˜g∥2 ≤Ex∈X [L1∥Jt −˜J∥2∥zt −˜z∥2 + (L21/2)∥zt −˜z∥2
2]
+ Ex∈X [MJ(L11/2)∥zt −˜z∥2
2 + MJ∥∂h(zt)∥2 + LD∥D −˜D∥2]
≤Ex∈X [L1(∥Jt −ˆJ∥2 + ∥ˆJ −J∗∥2)(∥zt −ˆz∥2 + ∥ˆz −˜z∥2)]
+ Ex∈X [(L21/2)(∥zt −ˆz∥2
2 + ∥ˆz −˜z∥2
2) + LD∥D −˜D∥2]
+ Ex∈X [MJ(L11/2)(∥zt −ˆz∥2
2 + ∥ˆz −˜z∥2
2) + MJ∥∂h(zt)∥2]
(41)
and
∥gae-ls
t
−˜g∥2 ≤Ex∈X [L1∥Jt −˜J∥2∥zt −˜z∥2 + (L21/2)∥zt −˜z∥2
2]
+ Ex∈X [MJ(L11/2)∥zt −˜z∥2
2 + LD∥D −˜D∥2]
≤Ex∈X [L1(∥Jt −ˆJ∥2 + ∥ˆJ −˜J∥2)(∥zt −ˆz∥2 + ∥ˆz −˜z∥2)]
+ Ex∈X [(L21/2)(∥zt −ˆz∥2
2 + ∥ˆz −˜z∥2
2)]
+ Ex∈X [MJ(L11/2)(∥zt −ˆz∥2
2 + ∥ˆz −˜z∥2
2) + LD∥D −˜D∥2].
(42)
16

From convergence rates in the forward pass (Theorems 3.1 and 3.2), we get
∥gae-lasso
t
−˜g∥2 ≤L1

O(tρ2t−1 + ˜Cztρt−1 + ρt ˜CJ + ˜Cz ˜CJ)

+ (L21/2 + MJL11/2) O(ρ2t + ˜Cz) + ˜CD + MJ∥∂h(zt)∥2
= O(tρ2t−1 + ˜Cztρt−1 + ρt ˜CJ + ( ˜Cz + 1) ˜CJ + ˜CD)
≤L1O(tρ2t−1) + (L21/2 + MJL11/2) O(ρ2t) + Ct
h
= O(tρ2t−1 + ˜Cztρt−1 + ρt ˜CJ + ( ˜Cz + 1) ˜CJ + ˜CD + Ct
h)
(43)
and
∥gae-ls
t
−˜g∥2 ≤L1

O(tρ2t−1 + ˜Cztρt−1 + ρt ˜CJ + ˜Cz ˜CJ)

+ (L21/2 + MJL11/2) O(ρ2t + ˜Cz) + ˜CD
= O(tρ2t−1 + ˜Cztρt−1 + ρt ˜CJ + ( ˜Cz + 1) ˜CJ + ˜CD).
(44)
■
A.6
Proof of Lemma 3.2
Lemma 3.2 (Global bounds). From global gradient errors in Theorem 3.4, the following are satisﬁed
∥Q21
t (z∗)∥2 ≤(L21/2)∥zt −z∗∥2
2
∥Qlasso-11
t
(z∗)∥2 ≤(L11/2 + Lh/2)∥zt −z∗∥2
2 + LD∥D −D∗∥2
∥Qls-11
t
(z∗)∥2 ≤(L11/2)∥zt −z∗∥2
2 + LD∥D −D∗∥2 + ∥∂h(z∗)∥2
∥Q(z∗, Jt)∥2 ≤L1∥Jt −J∗∥2
(20)
∥Q21
t (˜z)∥2 ≤(L21/2)∥zt −˜z∥2
2
∥Qlasso-11
t
(˜z)∥2 ≤(L11/2)∥zt −˜z∥2
2 + LD∥D −˜D∥2 + ∥∂h(zt)∥2
∥Qls-11
t
(˜z)∥2 ≤(L11/2)∥zt −˜z∥2
2 + LD∥D −˜D∥2
∥Q(˜z, Jt)∥2 ≤L1∥Jt −˜J∥2.
(21)
Proof. For Q21
t (z∗) and Q21
t (˜z), we achieve the quadratic bound using convexity of ∇1Lx(z, D) and
its domain (Assumption 2.1) and Lemma 2.2. For Qlasso-11
t
(z∗), we add and subtract ∇1Lx(z∗, D)
and ∇1Lx(z∗, D∗), and use quadratic upper bound similar to Lemma 3.1. At line ﬁve, we use
0 ∈∇1Lx(z∗, D∗) + ∂h(z∗) (Lemma A.3) and the quadratic upper bound on h (Lemma 2.3).
∥Qlasso-11
t
(z∗)∥2 = ∥∇1Lx(zt, D) + ∂h(zt) −∇2
11Lx(z∗, D)(zt −z∗)∥2
= ∥∇1Lx(zt, D) −∇1Lx(z∗, D) + ∇1Lx(z∗, D) + ∂h(zt) −∇2
11Lx(z∗, D)(zt −z∗)∥2
≤(L11/2)∥zt −z∗∥2
2 + ∥∂h(zt) + ∇1Lx(z∗, D)∥2
≤(L11/2)∥zt −z∗∥2
2 + ∥∂h(zt) + ∇1Lx(z∗, D) −∇1Lx(z∗, D∗) + ∇1Lx(z∗, D∗)∥2
≤(L11/2)∥zt −z∗∥2
2 + LD∥D −D∗∥2 + ∥∂h(zt) + ∇1Lx(z∗, D∗)∥2
∈(L11/2)∥zt −z∗∥2
2 + LD∥D −D∗∥2 + ∥∂h(zt) −∂h(z∗)∥2
≤(L11/2 + Lh/2)∥zt −z∗∥2
2 + LD∥D −D∗∥2.
(45)
Similarly,
∥Qls-11
t
(z∗)∥2 = ∥∇1Lx(zt, D) −∇2
11Lx(z∗, D)(zt −z∗)∥2
= ∥∇1Lx(zt, D) −∇1Lx(z∗, D) + ∇1Lx(z∗, D) −∇2
11Lx(z∗, D)(zt −z∗)∥2
≤(L11/2)∥zt −z∗∥2
2 + ∥∇1Lx(z∗, D)∥2
≤(L11/2)∥zt −z∗∥2
2 + ∥∇1Lx(z∗, D) −∇1Lx(z∗, D∗) + ∇1Lx(z∗, D∗)∥2
≤(L11/2)∥zt −z∗∥2
2 + LD∥D −D∗∥2 + ∥∇1Lx(z∗, D∗)∥2
∈(L11/2)∥zt −z∗∥2
2 + LD∥D −D∗∥2 + ∥∂h(z∗)∥2
≤(L11/2)∥zt −z∗∥2
2 + LD∥D −D∗∥2 + ∥∂h(z∗)∥2.
(46)
17

For Q(z∗, Jt), from implicit function theorem, Q(z∗, J∗) = 0. Hence, we can use ∇2
21Lx(z∗, D) =
−J∗∇2
11Lx(z∗, D) in the following
∥Q(z∗, Jt)∥2 = ∥Jt∇2
11Lx(z∗, D) + ∇2
21Lx(z∗, D)∥2
= ∥Jt∇2
11Lx(z∗, D) −J∗∇2
11Lx(z∗, D)∥2
≤∥(Jt −J∗)∇2
11Lx(z∗, D)∥2 ≤L1∥Jt −J∗∥2.
(47)
For (21), the proof procedure is similar, where we instead use the property 0 = ∇1Lx(˜z, ˜D).
∥Qlasso-11
t
(˜z)∥2 = ∥∇1Lx(zt, D) + ∂h(zt) −∇2
11Lx(˜z, D)(zt −˜z)∥2
= ∥∇1Lx(zt, D) −∇1Lx(˜z, D) + ∇1Lx(˜z, D) + ∂h(zt) −∇2
11Lx(˜z, D)(zt −˜z)∥2
≤(L11/2)∥zt −˜z∥2
2 + ∥∂h(zt) + ∇1Lx(˜z, D)∥2
≤(L11/2)∥zt −˜z∥2
2 + ∥∂h(zt) + ∇1Lx(˜z, D) −∇1Lx(˜z, ˜D)∥2
≤(L11/2)∥zt −˜z∥2
2 + LD∥D −˜D∥2 + ∥∂h(zt)∥2.
(48)
Similarly,
∥Qls-11
t
(˜z)∥2 = ∥∇1Lx(zt, D) −∇2
11Lx(˜z, D)(zt −˜z)∥2
= ∥∇1Lx(zt, D) −∇1Lx(˜z, D) + ∇1Lx(˜z, D) −∇2
11Lx(˜z, D)(zt −˜z)∥2
≤(L11/2)∥zt −˜z∥2
2 + ∥∇1Lx(˜z, D)∥2
≤(L11/2)∥zt −˜z∥2
2 + ∥∇1Lx(˜z, D) −∇1Lx(˜z, ˜D)∥2
≤(L11/2)∥zt −˜z∥2
2 + LD∥D −˜D∥2.
(49)
For Q(˜z, Jt), from implicit function theorem, Q(˜z, ˜J) = 0. Hence, we can use ∇2
21Lx(˜z, D) =
−˜J∇2
11Lx(˜z, D) in the following
∥Q(˜z, Jt)∥2 = ∥Jt∇2
11Lx(˜z, D) + ∇2
21Lx(˜z, D)∥2
= ∥Jt∇2
11Lx(˜z, D) −˜J∇2
11Lx(˜z, D)∥2
≤∥(Jt −˜J)∇2
11Lx(˜z, D)∥2 ≤L1∥Jt −˜J∥2.
(50)
■
B
Details of experiments
PUDLE is developed using PyTorch [67] on Python. We used one GeForce GTX 1080 Ti GPU.
B.1
Numerical experiments for theories
Dataset
We generated n = 10,000 samples following the model (3). We sampled ˜D ∈R50×100
from zero-mean Gaussian distribution, and then normalized the columns. The codes are 5-sparse
with their support uniformly chosen at random and their amplitudes are sampled from Uniform(1, 2).
Training
We let T = 200, and λ = 0.2 and α = 0.2. The dictionary is initialized to D =
˜D + τBB with B ∼N(0, 1
mI). For For Figures 2, 3, 4a and 4b, we set τB ≈0.55/log m. We
picked this closeness to highlight the gradient directions in a closer neighbourhood of the dictionary.
For Figures 4c and 4d, we chose much larger noise level, τB ≈2.8/log m. The network is trained for
600 epochs with full-batch gradient descent using Adam optimizer [68] with learning rate of 10−3
and ϵ = 10−8. The learned dictionary is evaluated based on the error ∥D−˜D∥2/∥˜D∥2. The results and
conclusion were consistent across various realizations of the dataset and across various optimizers.
Hence, in the main paper, the ﬁgures visualize results of one realization using Adam optimizer.
B.2
Dictionary learning
Dataset
We generated n=50,000 samples following the model (3). We let m=1000 and p=1500,
and sample ˜D from zero-mean Gaussian distribution, and then normalized the columns. The sparse
codes zi are 10, 20, 40-sparse, where their supports are chosen uniformly at random and their
amplitudes are sampled from Uniform(1, 2).
18

0
250
500
750
Number of iterations
10−7
10−5
10−3
10−1
||D−˜D||2
|| ˜D||2
gae−ls
t
gae−ls,decay
t
gae−ls,HT
t
NOODL
SPORCO
(a) 10-sparse code.
0
250
500
750
Number of iterations
10−7
10−5
10−3
10−1
||D−˜D||2
|| ˜D||2
gae−ls
t
gae−ls,decay
t
gae−ls,HT
t
NOODL
SPORCO
(b) 20-sparse code.
0
250
500
750
Number of iterations
10−7
10−5
10−3
10−1
||D−˜D||2
|| ˜D||2
gae−ls
t
gae−ls,decay
t
gae−ls,HT
t
NOODL
SPORCO
(c) 40-sparse code.
Figure 6: Dictionary learning convergence using gae-ls
t
compared to NOODL [29] and SPORCO [65].
Training
The dictionary is initialized to D = ˜D + τBB with B ∼N(0, 1
mI) where τB ≈1/log m.
We let λ = 0.2, and α = 0.2, and T = 100. The network is trained for 1,000 iterative updates with
batch-size of 50 using Adam [68] with learning rate of 10−3 and ϵ = 10−3. For decay method, β is
decreased in value by 0.005 every 100 update iterations. Each ﬁlter is normalized after every update.
The learned dictionary is evaluated based on the relative error ∥D−˜D∥2/∥˜D∥2.
B.3
Image denoising
Training
We trained PUDLE where the dictionary is convolutional with 64 ﬁlters of size 9 × 9
and strides of 4. The encoder unfolds for T = 15, and the step size is set to α = 0.1. Unlike the
theoretical analysis where full-batch gradient descent is studied, the network is trained stochastically
with Adam optimizer [68] with learning rate of 10−4 and ϵ = 10−3 for 250 epochs. At every training
iteration, a random 129 × 129 patch is cropped and a zero-mean Gaussian noise with standard
deviation of 25 is added. We report results in terms of the peak signal-to-noise-ratio (PSNR). The
standard deviation of the test set PSNR across multiple noise realizations was lower than 0.02 dB for
all the methods. Hence, we only reported the mean PSNR of the test set in the main paper.
C
Societal Impacts
The performance of deep learning, in its fully data-driven and model-agnostic form, is heavily
dependent on training data. Moreover, the computational burden of deep learning in both training and
inference tasks results in substantial carbon emissions. Unfolded networks (i.e., a sub-category of
model-based deep learning) can capture domain knowledge and data characteristics in its architecture.
Hence, it results in networks that are smaller in size and require less training data. Following
such approach in designing neural networks can ameliorate the carbon emission of deep learning.
Our paper studied the theoretical properties of a particular unfolded neural network for dictionary
learning. Understanding the properties and limits of such networks can encourage the ﬁeld to utilize
model-based deep learning.
Dictionary learning is a popular method in the signal processing community. As our work shows,
unfolding the iterations of the classical alternating-minimization algorithm gives rise to deep ReLU
networks for dictionary learning. Our theoretical analyses provide a strong basis for the principled
use, in the signal processing community, of deep learning for dictionary learning. Because they
originate from the sparse coding model, our unfolded networks are interpretable, and have orders of
magnitude fewer parameters than generic ReLU networks of the same depth, because the weights
at all layers are tied to the dictionary of interest. This makes the carbon footprint of training our
networks orders of magnitude smaller than that of training generic ReLU networks.
19

