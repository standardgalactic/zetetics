On the Low-Dimensional Structure of
Bayesian Inference
by
Alessio Spantini
B.Sc., Politecnico di Milano (2011)
S.M., Massachusetts Institute of Technology (2013)
Submitted to the Department of Aeronautics and Astronautics
in partial fulfillment of the requirements for the degree of
Doctor of Philosophy in Statistical Inference and Applied Probability
at the
MASSACHUSETTS INSTITUTE OF TECHNOLOGY
September 2017
Â© Massachusetts Institute of Technology 2017. All rights reserved.
Author . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Department of Aeronautics and Astronautics
August 24, 2017
Certified by. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Youssef M. Marzouk
Associate Professor of Aeronautics and Astronautics
Thesis Supervisor
Certified by. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Themistoklis Sapsis
Associate Professor of Mechanical Engineering
Thesis Committee Member
Certified by. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Alan S. Willsky
Professor of Electrical Engineering
Thesis Committee Member
Accepted by . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Hamsa Balakrishnan
Chairman, Graduate Program Committee

2

On the Low-Dimensional Structure of
Bayesian Inference
by
Alessio Spantini
Submitted to the Department of Aeronautics and Astronautics
on August 24, 2017, in partial fulfillment of the
requirements for the degree of
Doctor of Philosophy in Statistical Inference and Applied Probability
Abstract
The Bayesian approach to inference characterizes model parameters and predictions
through the exploration of their posterior distributions, i.e., their distributions con-
ditioned on available data. The Bayesian paradigm provides a flexible, principled
framework for quantifying uncertainty, wherein heterogeneous and incomplete sources
of information (e.g., prior knowledge, noisy observations, imperfect models) can be
properly rationalized. Yet a major obstacle to deploying Bayesian inference in realis-
tic applications is computational: characterizing the associated high-dimensional and
non-Gaussian posterior distributions remains a challenging task.
While the Bayesian formulation is quite general, essential features of a statistical
model can bring additional structure to the Bayesian update. For instance, the prior
distribution often encodes some kind of regularity in the parameters; observations
might be sparse and corrupted by noise; observations might also be indirect, related to
the parameters by a forward operator that filters out some information; the posterior
distribution might satisfy conditional independence assumptions that reflect local
probabilistic interactions; and in some cases we might be uninterested in the posterior
distribution per se, but rather in specific prediction goals.
In this thesis we: (1) provide a rigorous mathematical characterization of low-
dimensional structures that enable efficient Bayesian inference in high-dimensional
and continuous parameter spaces; and (2) exploit this characterization to devise new
structure-exploiting and computationally efficient inference algorithms.
Our contributions encompass multiple related topics. First we characterize optimal
low-rank approximations of linearâ€“Gaussian Bayesian inverse problems, and of their
goal-oriented extensions. Then we turn to inference in the nonlinear non-Gaussian
settingâ€”analyzing the sparsity, decomposability, and low-rank structure of determin-
istic couplings between distributions. These couplings facilitate efficient computation
of posterior expectations in generically non-Gaussian settings. Based on this anal-
ysis, we introduce a number of approaches for representing non-Gaussian Markov
random fields and for exploiting their conditional independence structure in compu-
tation by means of sparse nonlinear transport maps. We also develop new variational
3

algorithms for nonlinear smoothing and sequential parameter estimation. These algo-
rithms can be understood as the natural generalizationâ€”to the non-Gaussian caseâ€”of
the square-root Rauchâ€“Tungâ€“Striebel Gaussian smoother. Finally, we outline a new
class of nonlinear filters induced by local couplings, for inference in high-dimensional
spatiotemporal processes with chaotic dynamics.
Thesis Supervisor: Youssef M. Marzouk
Title: Associate Professor of Aeronautics and Astronautics
Thesis Committee Member: Themistoklis Sapsis
Title: Associate Professor of Mechanical Engineering
Thesis Committee Member: Alan S. Willsky
Title: Professor of Electrical Engineering
4

Contents
1
Introduction
17
1.1
Low-dimensional structure in Bayesian inference . . . . . . . . . . . .
17
1.2
Thesis contributions and roadmap . . . . . . . . . . . . . . . . . . . .
20
2
Notation
25
3
Optimal low-rank approximations of linear inverse problems
29
3.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
29
3.2
Optimal approximation of the posterior covariance matrix
. . . . . .
33
3.2.1
Defining the approximation class
. . . . . . . . . . . . . . . .
34
3.2.2
Optimality criteria: metrics between distributions . . . . . . .
34
3.2.3
Optimality results . . . . . . . . . . . . . . . . . . . . . . . . .
37
3.2.4
Computing eigenpairs of (ğ», Î“âˆ’1
pr ) . . . . . . . . . . . . . . . .
38
3.3
Properties of the optimal covariance approximation . . . . . . . . . .
39
3.3.1
Interpretation of the eigendirections . . . . . . . . . . . . . . .
40
3.3.2
Optimal projector . . . . . . . . . . . . . . . . . . . . . . . . .
41
3.3.3
Comparison with optimality in Frobenius norm
. . . . . . . .
42
3.3.4
Suboptimal posterior covariance approximations . . . . . . . .
44
3.4
Optimal approximation of the posterior mean
. . . . . . . . . . . . .
47
3.4.1
Optimality results . . . . . . . . . . . . . . . . . . . . . . . . .
48
3.4.2
Connection with â€œpriorconditionersâ€ . . . . . . . . . . . . . . .
52
3.5
Numerical examples . . . . . . . . . . . . . . . . . . . . . . . . . . . .
54
3.5.1
Example 1: Hessian and prior with controlled spectra . . . . .
54
5

3.5.2
Example 2: X-ray tomography . . . . . . . . . . . . . . . . . .
58
3.5.3
Example 3: Heat equation . . . . . . . . . . . . . . . . . . . .
66
3.6
Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
72
4
Goal-oriented optimal approximations of linear inverse problems
75
4.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
75
4.2
Theory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
78
4.2.1
Approximation of the posterior covariance of the QoI . . . . .
78
4.2.2
Approximation of the posterior mean of the QoI . . . . . . . .
88
4.3
Proof-of-concept example . . . . . . . . . . . . . . . . . . . . . . . . .
90
4.4
Numerical examples . . . . . . . . . . . . . . . . . . . . . . . . . . . .
92
4.4.1
Forward, observational and prior models . . . . . . . . . . . .
93
4.4.2
Goal-oriented linear inverse problem
. . . . . . . . . . . . . .
95
4.4.3
A nonlinear QoI . . . . . . . . . . . . . . . . . . . . . . . . . .
98
4.5
Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
99
5
The structure of low-dimensional couplings
107
5.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
107
5.2
Triangular transport maps: a building block . . . . . . . . . . . . . .
110
5.3
Markov networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
114
5.4
Sparsity of triangular transport maps . . . . . . . . . . . . . . . . . .
117
5.4.1
Sparsity bounds . . . . . . . . . . . . . . . . . . . . . . . . . .
117
5.4.2
Connection to Gaussian Markov random fields . . . . . . . . .
120
5.4.3
Ordering of triangular maps . . . . . . . . . . . . . . . . . . .
123
5.5
Decomposability of transport maps . . . . . . . . . . . . . . . . . . .
125
5.5.1
Preliminary notions . . . . . . . . . . . . . . . . . . . . . . . .
126
5.5.2
Decomposition and graph sparsification . . . . . . . . . . . . .
128
5.5.3
Recursive decompositions
. . . . . . . . . . . . . . . . . . . .
131
5.5.4
Computation of decomposable transports . . . . . . . . . . . .
135
5.6
Sequential inference on state-space models: variational algorithms . .
138
5.6.1
Smoothing and filtering: the full Bayesian solution
. . . . . .
139
6

5.6.2
The linear Gaussian case: connection with the RTS smoother
145
5.6.3
Transport maps in filtering: examples in the literature
. . . .
146
5.6.4
Sequential joint parameter and state estimation . . . . . . . .
148
5.6.5
Fixed-point smoothing . . . . . . . . . . . . . . . . . . . . . .
151
5.7
Low-rank transport maps . . . . . . . . . . . . . . . . . . . . . . . . .
152
5.7.1
General result . . . . . . . . . . . . . . . . . . . . . . . . . . .
152
5.7.2
Bayesian inference: local likelihood and the prior map . . . . .
155
5.7.3
Low-rank likelihood . . . . . . . . . . . . . . . . . . . . . . . .
158
5.8
Numerical examples . . . . . . . . . . . . . . . . . . . . . . . . . . . .
159
5.8.1
Stochastic volatility model with hyperparameters
. . . . . . .
159
5.8.2
Log-Gaussian Cox point process with sparse observations . . .
161
5.9
Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
169
6
A class of nonlinear filters induced by local couplings
175
6.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
175
6.1.1
Problem setting . . . . . . . . . . . . . . . . . . . . . . . . . .
175
6.1.2
Approach
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
176
6.2
Transport maps from samples:
conditional simulation
. . . . . . . . . . . . . . . . . . . . . . . . . .
178
6.3
Intuition for an abstract problem
. . . . . . . . . . . . . . . . . . . .
181
6.3.1
Parameterizing a high dimensional inverse map
. . . . . . . .
186
6.3.2
Introducing Markov assumptions
. . . . . . . . . . . . . . . .
187
6.4
Numerical example: Lorenz 96 . . . . . . . . . . . . . . . . . . . . . .
190
6.4.1
Configuration of the nonlinear filter . . . . . . . . . . . . . . .
192
6.4.2
Numerical results . . . . . . . . . . . . . . . . . . . . . . . . .
193
A Raoâ€™s metric between distributions
197
B Generalized Knothe-Rosenblatt rearrangement
199
C Proofs for Chapter 3
203
7

D Proofs for Chapter 4
213
E Proofs for Chapter 5
221
8

List of Figures
3-1
Randomized test case: variable hessian spectrum . . . . . . . . . . . .
58
3-2
Randomized test case: variable prior spectrum . . . . . . . . . . . . .
59
3-3
X-ray tomography problem: model setup . . . . . . . . . . . . . . . .
62
3-4
X-ray tomography problem: approximation of the posterior covariance
63
3-5
X-ray tomography problem: approximation of the posterior mean in
the limited-angle case . . . . . . . . . . . . . . . . . . . . . . . . . . .
66
3-6
X-ray tomography problem: error in the approximation of the posterior
mean for the low-rank and the low-rank update approximation . . . .
67
3-7
X-ray tomography problem: generalized eigenvaluse in the limited- and
full-angle configurations
. . . . . . . . . . . . . . . . . . . . . . . . .
67
3-8
X-ray tomography problem: approximation of the posterior mean in
the full-angle configuration . . . . . . . . . . . . . . . . . . . . . . . .
68
3-9
Heat equation problem: model setup
. . . . . . . . . . . . . . . . . .
71
3-10 Heat equation problem: approximation of the posterior mean . . . . .
71
3-11 Heat equation problem: comparison eigenvectors . . . . . . . . . . . .
72
4-1
Proof-of-concept example for goal-oriented approximations . . . . . .
92
4-2
CPU cooling problem: model setup . . . . . . . . . . . . . . . . . . .
100
4-3
CPU cooling problem: true temperature field and difference between
prior and posterior variance
. . . . . . . . . . . . . . . . . . . . . . .
100
4-4
CPU cooling problem: leading eigenvectors for the optimal approxi-
mation of the posterior covariance of the parameters . . . . . . . . . .
101
9

4-5
CPU cooling problem: leading eigenvectors for the optimal approxi-
mation of the posterior covariance of the QoI . . . . . . . . . . . . . .
101
4-6
CPU cooling problem: comparison error between different approxima-
tions of the posterior covariance of the QoI . . . . . . . . . . . . . . .
102
4-7
CPU cooling problem: approximation of the posterior mean of the QoI 103
4-8
CPU cooling problem: a nonlinear QoI . . . . . . . . . . . . . . . . .
104
5-1
Computation of transport maps . . . . . . . . . . . . . . . . . . . . .
115
5-2
Marginal graphs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
121
5-3
Predicted sparsity patterns for a triangular map . . . . . . . . . . . .
121
5-4
Stochastic volatility model: Markov structure and sparsity inverse map 122
5-5
Sparsity of a triangular map under different reorderings . . . . . . . .
125
5-6
Example of a decomposable map
. . . . . . . . . . . . . . . . . . . .
137
5-7
Markov structure of a typical state-space model . . . . . . . . . . . .
139
5-8
Markov structure of a typical state-space model with static parameters 148
5-9
Online fixed-point smoothing
. . . . . . . . . . . . . . . . . . . . . .
152
5-10 Stochastic volatility model: filtering and smoothing marginals
. . . .
162
5-11 Stochastic volatility model: posterior marginals of the static parameters162
5-12 Stochastic volatility model: reference MCMC solution for posterior
marginals of the state . . . . . . . . . . . . . . . . . . . . . . . . . . .
163
5-13 Stochastic volatility model: reference MCMC solution for posterior
marginals of the static parameters . . . . . . . . . . . . . . . . . . . .
163
5-14 Stochastic volatility model: posterior predicitve distribution
. . . . .
164
5-15 Log-Gaussian Cox problem: model setup . . . . . . . . . . . . . . . .
167
5-16 Log-Gaussian Cox: mean and standard deviation of the posterior latent
process computed via transport maps . . . . . . . . . . . . . . . . . .
168
5-17 Log-Gaussian Cox: mean and standard deviation of the posterior latent
process computed via MCMC . . . . . . . . . . . . . . . . . . . . . .
168
6-1
Cycle graph . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
190
6-2
Lorenz 96: sparse Markov structure . . . . . . . . . . . . . . . . . . .
194
10

6-3
Lorenz 96: dense Markov structure
. . . . . . . . . . . . . . . . . . .
195
6-4
Lorenz 96: accuracy filtering mean
. . . . . . . . . . . . . . . . . . .
196
11

Acknowledgments
I am indebted to my Ph.D. advisor Youssef Marzouk for his mentorship, friendship,
and guidance throughout these years. His passion and dedication will continue to be
a source of inspiration. He allowed me to pursue my academic interests and supported
me in every step of the way. He always believed in me and for that I will always be
grateful. I was fortunate to have crossed his path.
I want to thank the members of my thesis committee, Youssef Marzouk, Themis-
toklis Sapsis and Alan S. Willsky, and my readers, Pierre Jacob and Luis Tenorio,
for their support and countless suggestions that have greatly enhanced this thesis
project.
Luis Tenorio has been like a second advisor to me, an example to live by and a
friend. Thank you.
Research is never an individual activity.
It is a fruitful connection of minds,
thoughts and insights, and this work builds on this spirit. I am indebted to every
collaborator that took part in any project related to this work: Antti Solonen, Daniele
Bigoni, James Martin, Karen Willcox, Luis Tenorio, Olivier Zahm, Ricardo Baptista,
Tiangang Cui. This thesis would have not been possible without their help.
ACDL has been an exciting and fun place to grow, both as a person and as a
researcher. Many people crossed the lab during these years and every one of them
added something unique to the chemistry of this group.
I want to thank Antoni
Musolas, Alex Gorodetsky, Benjamin Zhang, Chad Lieberman, Chaitanya Talnikar,
Chi Feng, Daniele Bigoni, Elizabeth Qian, Ferran Vidal, Florian Augustin, Jayanth
Jagalur, Luca Tosatto, Lucio Di Ciaccio, Matthew Parno, Olivier Zahm, Pablo Fer-
nandez, Patrick Blonigan, Rebecca Morrison, RÃ©mi Lam, Ricardo Baptista, Sergio
Amaral, Tarek El Moselhy, Xun Huan, Zheng Wang, and so many others for such
memorable moments and for giving me a family away from home.
I want to thank the US Department of Energy, Office of Advanced Scientific
Computing Research (ASCR), for generously supporting my research.
Last, I want to dedicate this thesis to my friends, family, and loved ones. Their
12

unconditional support and encouragement kept me strong along the way, and motivate
me now, more than ever, to strive even harder to achieve my future goals.
13

14

Previously Published Material
â€¢ Chapter 3 revises a previous publication [261]: A. Spantini, A. Solonen, T. Cui,
J. Martin, L. Tenorio, and Y. Marzouk, Optimal low-rank approximations of
Bayesian linear inverse problems, SIAM Journal on Scientific Computing, 37
(2015), pp. A2451â€“A2487.
â€¢ Chapter 4 revises a previous publication [260]: A. Spantini, T. Cui, K. Will-
cox, L. Tenorio, and Y. M. Marzouk, Goal-oriented optimal approximations of
Bayesian linear inverse problems, SIAM Journal on Scientific Computing, in
press (2017).
Other work by the author, not presented in this thesis but nonetheless related to its
primary themes, includes:
â€¢ A. Spantini, D. Bigoni, and Y. Marzouk, Inference via low-dimensional cou-
plings, arXiv:1703.06131, (2017), [259]
â€¢ A. Spantini, D. Bigoni, and Y. Marzouk, Variational inference via decomposable
transports: algorithms for Bayesian filtering and smoothing, NIPS workshop on
Approximate Inference, (2016), [258]
â€¢ D. Bigoni, A. Spantini, and Y. Marzouk, Adaptive construction of measure
transports for Bayesian inference, NIPS workshop on Approximate Inference,
(2016), [29]
â€¢ Y. Marzouk, T. Moselhy, M. Parno, and A. Spantini, Sampling via measure
transport: An introduction, in Handbook of Uncertainty Quantification, R.
Ghanem, D. Higdon, and H. Owhadi, editors, Springer, 2016, [187]
15

â€¢ T. Cui, J. Martin, Y. Marzouk, A. Solonen, and A. Spantini, Likelihood-
informed dimension reduction for nonlinear inverse problems, Inverse Problems,
30 (2014), p. 114015, [74]
16

Chapter 1
Introduction
1.1
Low-dimensional structure in Bayesian inference
This thesis is concerned with the solution of Bayesian inference problems in high
dimensional and continuous parameter spaces. Many of these problems arise naturally
from a statistical reformulation of classical inverse problems, where a typical goal is to
characterize a parameter field of interest (e.g., the permeability of a porous medium)
from noisy observations, perhaps limited in number or resolution, that are indirectly
related to the parameters through the action of a forward model [268]. But we will
also consider Bayesian inference in much more general settings, ranging from spatial
statistics to sequential observations and dynamical models (e.g., filtering, smoothing,
and sequential parameter estimation in the nonlinear and non-Gaussian setting). A
common feature of the inference problems that we will investigate is the presence
of structureâ€”some notion of low dimensionalityâ€”that can be exploited for efficient
computation. A goal of this thesis is precisely to define, detect, and exploit such
low-dimensional structure.
Inference arises in virtually every aspect of science that needs to process data:
radar, optical imaging, tomography, astronomy, and atmospheric data assimilation,
to name a few.
In this thesis we consider several examples inspired by practical
applications.
For instance, in Chapter 3 we consider real-time X-ray imaging of
logs that enter a sawmill for automatic quality control [125, 261]. In Chapter 5, we
17

consider an online joint parameter and state estimation problem for the volatility
of the pound-dollar exchange rate [149, 237]. Also in Chapter 5 we perform infer-
ence with a log-Gaussian Cox process, frequently used to model spatially aggregated
point patterns in seismology and neuroimaging [193]. In Chapter 6, we consider the
nonlinear filtering of strongly chaotic and fully turbulent dynamical systems via the
Lorenz-96 model [180], which is intended to reproduce coarse features of the mid-
latitude atmosphere [182] and is frequently used as a testbed for numerical weather
prediction algorithms. Additionally, in related published work by the author [74], not
included in this thesis, we consider the the problem of estimating atmospheric trace
gas concentrations using measurements from the GOMOS (Global Ozone MOnitoring
System) satellite instrument [114, 74]. Also in [74], we infer the permeability field of
a groundwater system governed by elliptic PDEs. These examples hint at the broad
applicability of the ideas developed in this thesis.
In Bayesian inference, the parameters of interest are treated as random variables,
endowed with a prior distribution that encodes (possibly subjective) beliefs before
data are collected [232, 20]. The distribution of the data conditioned on any value of
the parameters is specified through the likelihood model. Bayesâ€™ rule then combines
prior and likelihood information to yield the posterior distribution, i.e., the distribu-
tion of the parameters conditioned on the data. The posterior distribution defines the
Bayesian solution to the inference problem. Thus, the Bayesian approach to inference
formalizes the characterization of the parameters through exploration of the posterior
distribution [268, 145, 265]. Computing expectations with respect to the posterior
distribution yields not only point estimates of the parameters (e.g., the posterior
mean), but a complete description of their uncertainty. From the posterior distribu-
tion one can extract the posterior covariance and higher moments, marginal distribu-
tions, quantiles, and event probabilities. Uncertainty in parameter-dependent predic-
tions can be quantified by integrating over the posterior distribution. The Bayesian
paradigm offers a coherent framework to account for all sources of uncertainty in
an inference problem, including stochastic forward models, noisy observations, and
model error. Not surprisingly, the laws of probability formalize intuitive â€œdesiderata
18

of rationalityâ€ and promote â€œplausible reasoningâ€ when dealing with uncertainty [138].
The major obstacle to deploying Bayesian inference in high-dimensional real world
applications is computational: characterizing a high-dimensional and generically non-
Gaussian posterior distribution can be a challenging task. In some sense, this is the
price to pay for demanding more than a point estimate of the parameters or of model
predictions (cf. Tikhonov regularization [269]). While the Bayesian formulation is
quite general, essential features of the statistical model can bring additional structure
to the Bayesian update. For instance, the prior distribution often encodes some kind
of smoothness or correlation among the inversion parameters; observations might be
finite, local, few in number, and corrupted by noise; observations might also be in-
direct, related to the parameters by the action of a forward operator that filters out
some information; the posterior distribution might satisfy conditional independence
assumptions that model local probabilistic interactions among the underlying random
variables; and in some cases we might not even be interested in the posterior distri-
bution per se, but rather in specific prediction goals. A key consequence of all this
structure in the inference model is that Bayesian computations in high dimensions
are far from being hopeless.
Clearly, not every problem has every feature mentioned above. But most prob-
lems do possess some combination thereof. This observation is not mere coincidence.
Inference relies ultimately on a statistical model. Each model strives to achieve a
compromise between predictive power and computational efficiency. Understanding
what types of structure enable efficient Bayesian computation can have a profound
impact on modeling choices: we will favor models that have such structure. And in
many cases we would be able to use rather complex models without sacrificing any
predictive power. This is not to say that we should always aim for the most complex
available statistical model: in many scenarios simple models are also the most appro-
priate [50, 104, 123]. But we should at least reach a point where no modeling effort
is limited in accuracy by a lack of computational tractability.
This thesis sets forward several goals: (1) to provide a rigorous mathematical
characterization of various notions of low-dimensional structure that enable efficient
19

Bayesian inference in high-dimensional and continuous parameter spaces; (2) to de-
velop techniques to test for such low-dimensional structure in a given inference prob-
lem; (3) to exploit this characterization of low-dimensional structure to devise new
computationally efficient and structure-exploiting algorithms for Bayesian computa-
tion.
1.2
Thesis contributions and roadmap
The contributions of this thesis range over a number of related topics.
Here we
preview these contributions while giving an outline of each chapter of the thesis.
â€¢ Optimal low-rank approximations of linear inverse problems. In Chap-
ter 3, we introduce statistically optimal approximations of linear Gaussian in-
verse problems.
We exploit how data are often informative, relative to the
prior, only about a handful of directions in the parameter space [261]. These
optimality results support computationally efficient and structure exploiting
algorithms for Bayesian inference in high-dimensional linear Gaussian models
[96] and are the building block for many approaches to the solution of nonlinear
Bayesian inverse problems [186, 40, 42, 74, 73]. We first investigate the ap-
proximation of the posterior covariance matrix as a low-rank negative update
of the prior covariance matrix. We prove optimality of a particular update,
based on the leading eigendirections of the matrix pencil defined by the Hessian
of the negative log-likelihood and the prior precision, for a broad class of loss
functions. This class includes the natural geodesic distance on the manifold of
symmetric and positive definite matrices, as well as the Kullback-Leibler diver-
gence and the Hellinger distance between the associated distributions. We also
propose two fast approximations of the posterior mean and prove their opti-
mality with respect to a weighted Bayes risk under squared-error loss. These
approximations are deployed in an offline-online manner, where a more costly
but data-independent offline calculation is followed by fast online evaluations.
As a result, these approximations are particularly useful when repeated poste-
20

rior mean evaluations are required for multiple data sets. We demonstrate our
theoretical results with several numerical examples, including high-dimensional
X-ray tomography and an inverse heat conduction problem. In both of these
examples, the low-dimensional structure of the inverse problem can be exploited
while producing results that are essentially indistinguishable from the exact so-
lution. For an extension of the material of this chapter to nonlinear problems
we refer the reader to [74].
â€¢ Goal-oriented optimal approximations of linear inverse problems. In
Chapter 4, we extend the statistically optimal approximations of Chapter 3 to
the case of goal oriented linear Gaussian inverse problems, where the quantity of
interest (QoI) is a function of the inversion parameters. We introduce new algo-
rithms for the efficient characterization of the posterior statistics of the quantity
of interest [260]. These optimal approximations avoid the explicit computation
of the full posterior distribution of the parameters and instead focus on direc-
tions in the parameter space that are well informed by the data and relevant to
the QoI. These directions stem from a balance among all the components of the
goalâ€“oriented inverse problem: prior information, forward model, measurement
noise, and ultimate goals. In particular, we show how including ultimate goals
in the formulation of the inverse problem reduces the dimension of the Bayesian
update. We illustrate the theory using a high-dimensional inverse problem in
heat transfer.
â€¢ The structure of low-dimensional couplings.
In Chapter 5, we switch
gears to general non-Gaussian problems and study the structure of determin-
istic couplings between distributions. A principled approach to posterior sam-
pling in the general non-Gaussian case is to seek a coupling between a tractable
â€œreferenceâ€ distribution (e.g., a standard Gaussian) and the posterior. Deter-
ministic couplings are induced by transport maps, which are multivariate (and
possibly nonlinear) transformations that enable direct simulation from the pos-
terior simply by evaluating the transport map at samples from the reference
21

distribution. Representing, computing and evaluating such a map, however,
grows challenging in high dimensions. The central contribution of this chapter
is to establish an explicit link between the Markov properties of the reference-
posterior pair and the existence of certain low-dimensional couplings, induced
by transport maps that are sparse, decomposable, and/or low-rank. Our analysis
not only facilitates the construction of couplings in high-dimensional settings,
but also suggests new inference methodologies. For instance, our analysis of
sparse triangular maps provides a general framework for describing continuous
and non-Gaussian Markov random fields, and for exploiting the conditional in-
dependence structure of these fields in computation. In particular, this analysis
shows that the inverse of the Knotheâ€“Rosenblatt (KR) rearrangementâ€”a par-
ticular couplingâ€”is the natural generalization to the non-Gaussian case of the
Cholesky factor of the precision matrix of a Gaussian Markov random fieldâ€”in
that both the inverse KR rearrangement (a potentially nonlinear map) and the
Cholesky factor (a linear map) have the same sparsity pattern given posterior
distributions with the same Markov structure. Thus the KR rearrangement
can be used to extend well-known modeling and sampling techniques for high-
dimensional Gaussian MRFs [236] to non-Gaussian fields. As another example,
in the context of nonlinear and non-Gaussian state space models, we describe
new variational algorithms for filtering, smoothing, and sequential parameter
estimation. These algorithms implicitly characterizeâ€”via a transport mapâ€”
the full posterior distribution of the sequential inference problem using local
operations only incrementally more complex than regular filtering, while avoid-
ing importance sampling or resampling. These algorithms can be understood
as the natural generalizationâ€”to the non-Gaussian caseâ€”of the square-root
Rauchâ€“Tungâ€“Striebel Gaussian smoother [226, 210].
â€¢ A class of nonlinear filters induced by local couplings.
In Chapter
6, we introduce a new class of structure-exploiting nonlinear filters for high-
dimensional state-space models. These filters can exploit the following struc-
22

ture: (1) locality in the observations (e.g., pointwise likelihoods), (2) decay
of correlations, and (3) approximate conditional independence in the filtering
distribution. Thus we focus on applications where the state is usually the dis-
cretization of a distributed process, and where the dynamic is typically governed
by a (chaotic) partial differential equation that we treat as a black box; that
is, we assume that no gradients of the forward model (or transition kernel)
are available.
The idea is to transform the forecast ensemble (i.e., samples
from the bootstrap proposal) into samples from the current filtering distribu-
tion (i.e., conditioned on the new observations), by means of a sequence of
local (in state-space) nonlinear couplings computed mostly via low-dimensional
convex optimization. This sequence of low-dimensional transformations implic-
itly approximate the projection of the filtering distribution onto a manifold of
sparse Markov random fields (not necessarily Gaussian) and can be carried out
with very limited ensemble sizes. Many variations of the ensemble Kalman fil-
ter (EnKF) can be interpreted as special instances of the proposed framework
when we restrict our attention exclusively to linear transformations, and when
we neglect approximately sparse Markov structure in the filtering distribution
[93]. A key feature of the proposed algorithms, however, is that depending on
the forecast ensemble size, we can inject arbitrary non-Gaussian structure into
the problem in a stable way by exploiting locality of the transformations, and
thus we can reduce the EnKF bias that results from restricting the Bayesian
update to linear functions of the forecast ensemble.
In this thesis we consider some, but certainly not all, sources of low-dimensional
structure within the context of Bayesian inference. Some structure is perhaps yet
to be discovered, but other notions of low dimensionality have already been studied
extensively in the literature. We will explore connections with the relevant literature
within each chapter of the thesis, once the appropriate methodologies and terminology
are introduced. There are some important topics, however, that we will not touch
upon, and we refer the reader to the original contributions for further details. For
instance, there exists a large body of work on how to exploit conditional independence
23

structure in discrete and/or Gaussian (graphical) models (e.g., [156, 161, 179, 141]),
on how to exploit multiscale/multiresolution models [52, 53], and on conditionally
Gaussian or low-rank structure in filtering (e.g., [183, 253, 217, 57, 94]). We note
that most of the techniques proposed in this thesis are in fact complementary to
existing efforts in dimensionality reduction for Bayesian inference, and could thus be
used in conjunction with well-established and successful methodologies.
Finally, we note that this thesis presents a natural progression along two fronts:
(1) from the study of linear-Gaussian inverse problems to the more general case of
nonlinear non-Gaussian inference, and (2) from stationary inverse problems to se-
quential inference (e.g., filtering, smoothing and joint parameter-state estimation).
The chapters are relatively self-contained and can be read independently: broadly
speaking they are divided between Gaussian (Chapters 3 and 4) and non-Gaussian
(Chapters 5 and 6) problems. A reader interested exclusively in the computational
aspects of sequential inference can focus mostly on Section 5.6 and Chapter 6. Chap-
ter 6 restricts its attention to high-dimensional filtering for chaotic spatiotemporal
processes [230]. Chapter 2 contains some relevant notation used throughout the thesis
(mostly needed for Chapter 5), whereas the proofs of the main results are collected in
dedicated appendices for each chapter (see Appendices C-D-E). Appendix A reviews
Raoâ€™s notion of a metric between distributions [224], whereas Appendix B focuses
on some background material for the KR rearrangement. Concluding remarks are
offered at the end of each chapter.
24

Chapter 2
Notation
Here, we collect some useful notation used throughout the thesis.
Notation for functions, sets, and graphs
For a pair of functions ğ‘“and ğ‘”, we
denote their composition by ğ‘“âˆ˜ğ‘”. We denote by ğœ•ğ‘˜ğ‘“the partial derivative of ğ‘“with
respect to its ğ‘˜th input variable. By ğœ•ğ‘˜ğ‘“= 0, we mean that the function ğ‘“does not
depend on its ğ‘˜th input variable. Depending on the context, we can identify a matrix
ğ‘„with its corresponding linear map, given by ğ‘¥â†¦â†’ğ‘„ğ‘¥.
For all ğ‘›> 0, we let Nğ‘›= {1, . . . , ğ‘›} denote the set of the first ğ‘›integers. For
any pair of sets, ğ’œâŠ‚â„¬means that ğ’œis a subset of â„¬(including the possibility of
ğ’œ= â„¬). We denote by |ğ’œ| the cardinality of ğ’œ.
Given a graph ğ’¢= (ğ’±, â„°) with vertices ğ’±and edges â„°, we denote by Nb(ğ‘˜, ğ’¢) the
neighborhood of a node ğ‘˜in ğ’¢, while for any set ğ’œâŠ‚ğ’±, we denote by ğ’¢ğ’œ= (ğ’±â€², â„°â€²)
the subgraph given by ğ’±â€² = ğ’œand â„°â€² = â„°âˆ©(ğ’œÃ— ğ’œ).
Notation for measures and densities
In this thesis, we mostly consider prob-
ability measures on Rğ‘›that are absolutely continuous with respect to the Lebesgue
measure, ğœ†, and that are fully supported. We denote the set of such measures by
M+(Rğ‘›). The density of a measure will always be intended with respect to ğœ†. For a
pair of measures ğœˆ1, ğœˆ2, ğœˆ1 â‰ªğœˆ2 means that ğœˆ1 is absolutely continuous with respect
to ğœˆ2.
25

For any measure ğœˆand measurable map ğ‘‡, we denote by ğ‘‡â™¯ğœˆthe pushforward
measure given by ğœˆâˆ˜ğ‘‡âˆ’1, where for any set â„¬, ğ‘‡âˆ’1(â„¬) is the set-valued preimage
of â„¬under ğ‘‡. Similarly, we denote by ğ‘‡â™¯ğœˆthe pullback measure given by ğœˆâˆ˜ğ‘‡.
Given a measure ğœˆwith density ğœ‹and a map ğ‘‡, we denote by ğ‘‡â™¯ğœ‹the density of
ğ‘‡â™¯ğœˆ, provided it exists (depending on ğ‘‡). We call ğ‘‡â™¯ğœ‹the pushforward density of ğœ‹
by ğ‘‡. Similarly, we define the pullback density ğ‘‡â™¯ğœ‹as the density of ğ‘‡â™¯ğœˆ, provided it
exists. Whether the map ğ‘‡preserves the absolute continuity of the measure depends
on the regularity of ğ‘‡. For instance, if ğ‘‡: Rğ‘›â†’Rğ‘›is a diffeomorphismâ€”i.e., a
differentiable bijection with differentiable inverseâ€”then one has:
ğ‘‡â™¯ğœ‹(ğ‘¥) = ğœ‹(ğ‘‡âˆ’1(ğ‘¥)) | det âˆ‡ğ‘‡âˆ’1(ğ‘¥)|,
ğ‘‡â™¯ğœ‹(ğ‘¥) = ğœ‹(ğ‘‡(ğ‘¥)) | det âˆ‡ğ‘‡(ğ‘¥)|,
(2.1)
where âˆ‡ğ‘‡(ğ‘¥) denotes the Jacobian of ğ‘‡at ğ‘¥. The regularity assumptions on ğ‘‡can be
substantially weakened as long as one modifies (2.1) appropriately [235, 262, 101]. We
will give one such example shortly when dealing with triangular maps (see Section 5.2
or Appendix B). We denote by
âˆ«ï¸€
ğ‘“(ğ‘¥) ğœˆ(dğ‘¥) the integration of a measurable function
ğ‘“: Rğ‘›â†’R with respect to a measure ğœˆ. For the Lebesgue measure, we simplify our
notation as
âˆ«ï¸€
ğ‘“(ğ‘¥) ğœ†(dğ‘¥) =
âˆ«ï¸€
ğ‘“(ğ‘¥) dğ‘¥. Given a pair ğœ‚, ğœ‹of probability densities and
a map ğ‘‡: Rğ‘›â†’Rğ‘›, we say that ğ‘‡pushes forward ğœ‚to ğœ‹if and only if ğ‘‡couples
the corresponding probability measures, i.e., ğ‘‡â™¯ğœˆğœ‚= ğœˆğœ‹, with ğœˆğœ‚(â„¬) =
âˆ«ï¸€
â„¬ğœ‚(ğ‘¥) dğ‘¥and
ğœˆğœ‹(â„¬) =
âˆ«ï¸€
â„¬ğœ‹(ğ‘¥) dğ‘¥for all measurable sets â„¬. (Notice that ğ‘‡â™¯ğœ‚need not be given by
(2.1) since we are not specifying any regularity on ğ‘‡.)
When it is clear from context, we will freely omit the qualifier a.e. to indicate a
property that holds up to a set of measure zero.
Notation for random variables
We use boldface capital letters, e.g., ğ‘‹, to denote
random variables on Rğ‘›with ğ‘›> 1, while we write scalar-valued random variables
as ğ‘‹. The law of a random variable ğ‘‹defined on a probability space (Î©, P) is given
by ğ‘‹â™¯P. For a measure ğœˆ, ğ‘‹âˆ¼ğœˆmeans that ğ‘‹has law ğœˆ. If ğ‘‹= (ğ‘‹1, . . . , ğ‘‹ğ‘)
is a collection of random variables and ğ’œâŠ‚Nğ‘, then ğ‘‹ğ’œ= (ğ‘‹ğ‘–, ğ‘–âˆˆğ’œ) denotes
26

a subcollection of ğ‘‹. In the same way, for ğ‘—< ğ‘˜, ğ‘‹ğ‘—:ğ‘˜= (ğ‘‹ğ‘—, ğ‘‹ğ‘—+1, . . . , ğ‘‹ğ‘˜). If
ğ‘‹= (ğ‘‹1, . . . , ğ‘‹ğ‘) has joint density ğœ‹and ğ’œâŠ‚Nğ‘, we denote by ğœ‹ğ‘‹ğ’œthe marginal
of ğœ‹along ğ‘‹ğ’œ, i.e., ğœ‹ğ‘‹ğ’œ(ğ‘¥ğ’œ) =
âˆ«ï¸€
ğœ‹(ğ‘¥) dğ‘¥Nğ‘âˆ–ğ’œ. If ğœ‹is the density of ğ‘= (ğ‘‹, ğ‘Œ),
we denote by ğœ‹ğ‘‹|ğ‘Œthe density of ğ‘‹given ğ‘Œ, where
ğœ‹ğ‘‹|ğ‘Œ(ğ‘¥|ğ‘¦) =
â§
âª
â¨
âª
â©
ğœ‹ğ‘‹,ğ‘Œ(ğ‘¥, ğ‘¦)/ğœ‹ğ‘Œ(ğ‘¦)
if ğœ‹ğ‘Œ(ğ‘¦) Ì¸= 0
0
otherwise.
(2.2)
We denote independence of a pair of random variables ğ‘‹, ğ‘Œby ğ‘‹âŠ¥âŠ¥ğ‘Œ. In the
same way, ğ‘‹âŠ¥âŠ¥ğ‘Œ|ğ‘…means that ğ‘‹and ğ‘Œare independent given a third random
variable ğ‘….
27

28

Chapter 3
Optimal low-rank approximations of
linear inverse problems
3.1
Introduction
In this chapter we investigate approximation methods for finite-dimensional Bayesian
linear inverse problems with Gaussian measurement and prior distributions. We char-
acterize approximations of the posterior distribution that are structure-exploiting and
that are optimal in a sense to be defined below. Since the posterior distribution is
Gaussian, it is completely determined by its mean and covariance. We therefore focus
on approximations of these posterior characteristics. Optimal approximations will re-
duce computation and storage requirements for high-dimensional inverse problems,
and will also enable fast computation of the posterior mean in a many-query setting.
We consider approximations of the posterior covariance matrix in the form of low-
rank negative updates of the prior covariance matrix. This class of approximations
exploits the structure of the prior-to-posterior update, and also arises naturally in
Kalman filtering techniques (e.g., [10, 11, 254]); the challenge is to find an optimal
update within this class, and to define in what sense it is optimal. We will argue
that a suitable loss function with which to define optimality is the natural geodesic
distance on the manifold of symmetric and positive definite matrices [99], and will
show that this metric generalizes to a broader class of loss functions that emphasize
29

relative differences in covariance. We will derive the optimal low-rank update for this
entire class of loss functions. In particular, we will show that the prior covariance
matrix should be updated along the leading generalized eigenvectors of the pencil
(ğ», Î“âˆ’1
pr ) defined by the Hessian of the negative log-likelihood and the prior precision
matrix. If we assume exact knowledge of the posterior mean, then our results extend
to optimality statements between distributions (e.g., optimality in Kullback-Leibler
divergence and in Hellinger distance). The form of this low-rank update of the prior
is not new [40, 43, 96, 186], but previous work has not shown whetherâ€”and if so,
in exactly what senseâ€”it yields optimal approximations of the posterior.
A key
contribution of this chapter is to establish and explain such optimality.
Properties of the generalized eigenpairs of (ğ», Î“âˆ’1
pr ) and related matrix pencils
have been studied previously in the literature, especially in the context of classical
regularization techniques for linear inverse problems1 [270, 206, 121, 120, 89]. The
joint action of the log-likelihood Hessian and the prior precision matrix has also
been used in related regularization methods [47, 44, 46, 131]. However, these efforts
have not been concerned with the posterior covariance matrix or with its optimal
approximation, since this matrix is a property of the Bayesian approach to inversion.
One often justifies the assumption that the posterior mean is exactly known by
arguing that it can easily be computed as the solution of a regularized least-squares
problem [128, 207, 3, 190, 17]; indeed, evaluation of the posterior mean to machine
precision is now feasible even for million-dimensional parameter spaces [40]. If, how-
ever, one needs multiple evaluations of the posterior mean for different realizations
of the data (e.g., in an online inference context), then solving a linear system to de-
termine the posterior mean may not be the most efficient strategy. A second goal of
this chapter is to address this problem. We will propose two computationally effi-
cient approximations of the posterior mean based on: (i) evaluating a low-rank affine
function of the data; or (ii) using a low-rank update of the prior covariance matrix
in the exact formula for the posterior mean. The optimal approximation in each case
1In the framework of Tikhonov regularization [269], the regularized estimate coincides with the
posterior mean of the Bayesian linear model we consider here, provided that the prior covariance
matrix is chosen appropriately.
30

is defined as the minimizer of the Bayes risk for a squared-error loss weighted by the
posterior precision matrix. We provide explicit formulas for these optimal approx-
imations and show that they can be computed by exploiting the optimal posterior
covariance approximation described above. Thus, given a new set of data, computing
an optimal approximation of the posterior mean becomes a computationally trivial
task.
Low-rank approximations of the posterior mean that minimize the Bayes risk for
squared-error loss have been proposed in [63, 66, 65, 64, 62] for a general non-Gaussian
case. Here, instead we develop analytical results for squared-error loss weighted by the
posterior precision matrix. This choice of norm reflects the idea that approximation
errors in directions of low posterior variance should be penalized more strongly than
errors in high-variance directions, as we do not want the approximate posterior mean
to fall outside the bulk of the posterior probability distribution. Remarkably, in this
case, the optimal approximation only requires the leading eigenvectors and eigenvalues
of a single eigenvalue problem.
This is the same eigenvalue problem we solve to
obtain an optimal approximation of the posterior covariance matrix, and thus we can
efficiently obtain both approximations at the same time.
While the efficient solution of large-scale linear-Gaussian Bayesian inverse prob-
lems is of standalone interest [96], optimal approximations of Gaussian posteriors
are also a building block for the solution of nonlinear Bayesian inverse problems.
For example, the stochastic Newton Markov chain Monte Carlo (MCMC) method
[186] uses Gaussian proposals derived from local linearizations of a nonlinear forward
model; the parameters of each Gaussian proposal are computed using the optimal
approximations analyzed in this chapter.
To tackle even larger nonlinear inverse
problems, [40] uses a Laplace approximation of the posterior distribution wherein
the Hessian at the mode of the log-posterior density is itself approximated using the
present approach. Similarly, approximations of local Gaussians can facilitate the con-
struction of a nonstationary Gaussian process whose mean directly approximates the
posterior density [42]. Alternatively, [74] combines data-informed directions derived
from local linearizations of the forward modelâ€”a direct extension of the posterior co-
31

variance approximations described in this chapterâ€”to create a global data-informed
subspace. A computationally efficient approximation of the posterior distribution is
then obtained by restricting MCMC to this subspace and treating complementary
directions analytically. Moving from the finite to the infinite-dimensional setting, the
same global data-informed subspace is used to drive efficient dimension-independent
posterior sampling for inverse problems in [73].
Earlier work on dimension reduction for Bayesian inverse problems used the Karhunen-
LoÃ¨ve expansion of the prior distribution [188, 166] to describe the parameters of
interest. To reduce dimension, this expansion is truncated; this step renders both
the prior and posterior distributions singularâ€”i.e., collapsed onto the prior meanâ€”in
the neglected directions. Avoiding large truncation errors then requires that the prior
distribution impose significant smoothness on the parameters, so that the spectrum
of the prior covariance kernel decays quickly. In practice, this requirement restricts
the choice of priors. Moreover, this approach relies entirely on properties of the prior
distribution and does not incorporate the influence of the forward operator or the
observational errors. Alternatively, [171] constructs a reduced basis for the param-
eter space via greedy model-constrained sampling, but this approach can also fail
to capture posterior variability in directions uninformed by the data. Both of these
earlier approaches seek reduction in the overall description of the parameters. This
notion differs fundamentally from the dimension reduction technique advocated in
this chapter, where low-dimensional structure is sought in the change from prior to
posterior.
The rest of this chapter is organized as follows. In Section 3.2 we introduce the
posterior covariance approximation problem and derive the optimal prior-to-posterior
update with respect to a broad class of loss functions. The structure of the optimal
posterior covariance matrix approximation is examined in Section 3.3. Several in-
terpretations are given in this section, including an equivalent reformulation of the
covariance approximation problem as an optimal projection of the likelihood function
onto a lower dimensional subspace. In Section 3.4 we characterize optimal approxi-
mations of the posterior mean. In Section 3.5 we provide several numerical examples.
32

Section 3.6 offers concluding remarks. Appendix C collects proofs of the theorems
stated throughout the chapter, along with additional technical results. The material
presented in this chapter can be also found in [261].
3.2
Optimal approximation of the posterior covari-
ance matrix
Consider the Bayesian linear model defined by a Gaussian likelihood and a Gaussian
prior with a non-singular covariance matrix Î“pr â‰»0 and, without loss of generality,
zero mean:2
ğ‘Œ| ğ‘‹âˆ¼ğ’©(ğºğ‘‹, Î“obs),
ğ‘‹âˆ¼ğ’©(0, Î“pr).
(3.1)
Here ğ‘‹represents the parameters to be inferred, ğºis the linear forward operator,
and ğ‘Œare the observations, with Î“obs â‰»0. The statistical model (3.1) also follows
from:
ğ‘Œ= ğºğ‘‹+ â„°
where â„°âˆ¼ğ’©(0, Î“obs) is independent of ğ‘‹.
It is easy to see that the posterior
distribution is again Gaussian (see, e.g., [49]):
ğ‘‹| ğ‘Œâˆ¼ğ’©(ğœ‡pos(ğ‘Œ), Î“pos), with
mean and covariance matrix given by
ğœ‡pos(ğ‘Œ) = Î“pos ğºâŠ¤Î“âˆ’1
obs ğ‘Œ
and
Î“pos =
(ï¸€
ğ»+ Î“âˆ’1
pr
)ï¸€âˆ’1 ,
(3.2)
where
ğ»= ğºâŠ¤Î“âˆ’1
obsğº
(3.3)
is the Hessian of the negative log-likelihood (i.e., the Fisher information matrix).
Since the posterior is Gaussian, the posterior mean coincides with the posterior mode:
ğœ‡pos(ğ‘Œ) = arg maxğ‘¥ğœ‹pos(ğ‘¥; ğ‘Œ), where ğœ‹pos is the posterior density. Note that the
posterior covariance matrix does not depend on the data.
2ğ‘Œ|ğ‘‹refers to a random variable distributed according to the measure of ğ‘Œconditioned on ğ‘‹.
33

3.2.1
Defining the approximation class
We will seek an approximation, Ì‚ï¸€Î“pos, of the posterior covariance matrix that is optimal
in a class of matrices to be defined shortly. As we can see from (3.2), the posterior
precision matrix Î“âˆ’1
pos is a non-negative update of the prior precision matrix Î“âˆ’1
pr :
Î“âˆ’1
pos = Î“âˆ’1
pr + ğ‘ğ‘âŠ¤, where ğ‘ğ‘âŠ¤= ğ».
Similarly, using Woodburyâ€™s identity we
can write Î“pos as a non-positive update of Î“pr: Î“pos = Î“pr âˆ’ğ¾ğ¾âŠ¤, where ğ¾ğ¾âŠ¤=
Î“pr ğºâŠ¤Î“âˆ’1
ğ‘ŒğºÎ“pr and Î“ğ‘Œ= Î“obs + ğºÎ“pr ğºâŠ¤is the covariance matrix of the marginal
distribution of ğ‘Œ[144]. This update of Î“pr is negative semidefinite because the data
add information: the posterior variance in any direction is always smaller than the
corresponding prior variance. Moreover, the update is usually low rank for exactly
the reasons described in the introduction: there are directions in the parameter space
along which the data are not very informative, relative to the prior. For instance,
ğ»might have a quickly decaying spectrum [41, 247]. Note, however, that Î“pos itself
might not be low-rank. Low-rank structure, if any, lies in the update of Î“pr that yields
Î“pos. Hence, a natural class of matrices for approximating Î“pos is the set of negative
semi-definite updates of Î“pr, with a fixed maximum rank, that lead to positive definite
matrices:
â„³ğ‘Ÿ=
{ï¸€
Î“pr âˆ’ğ¾ğ¾âŠ¤â‰»0 : rank(ğ¾) â‰¤ğ‘Ÿ
}ï¸€
.
(3.4)
This class of approximations of the posterior covariance matrix takes advantage of
the structure of the prior-to-posterior update.
3.2.2
Optimality criteria: metrics between distributions
To measure the quality of the approximation of a posterior distribution we employ
a metric first introduced by Rao in [223] based on the Fisher information. Raoâ€™s
approach to comparing distributions is rooted in differential geometry. The idea is to
turn a parametric family of distributions into a Riemannian manifold endowed with
a metric based on the geodesic distance [224]. The Riemannian structure is induced
by a quadratic form defined by the Fisher information matrix. (See Appendix A for
the explicit construction of Raoâ€™s distance.) For a Gaussian family of distributions
34

this metric can be written explicitly at least in two particular cases [9, 250]. If the
family consists of Gaussian distributions with the same covariance matrix, Î“, then
the metric reduces to the Mahalanobis distance between the means [181, 224]:
ğ‘‘â„›(ğœˆ1, ğœˆ2) = â€–ğœ‡1 âˆ’ğœ‡2â€–Î“âˆ’1,
ğœˆ1 = ğ’©(ğœ‡1, Î“), ğœˆ2 = ğ’©(ğœ‡2, Î“),
(3.5)
where â€–ğ‘§â€–2
Î“âˆ’1 := ğ‘§âŠ¤Î“âˆ’1ğ‘§. We will use this metric to define optimal approximations
of the posterior mean in Section 3.4. Notice that this metric emphasizes differences
in the mean along eigendirections of Î“ corresponding to low variance.
If, on the other hand, the family consists of Gaussian distributions with the same
mean, ğœ‡, then the metric reduces to:
ğ‘‘â„›(ğœˆ1, ğœˆ2) =
âˆšï¸ƒ
1
2
âˆ‘ï¸
ğ‘–
ln2(ğœğ‘–),
ğœˆ1 = ğ’©(ğœ‡, Î“1), ğœˆ2 = ğ’©(ğœ‡, Î“2),
(3.6)
where (ğœğ‘–) are the generalized eigenvalues of the pencil (Î“1, Î“2) [139]. That is, (ğœğ‘–)
are the roots of the characteristic polynomial det(Î“1 âˆ’ğœÎ“2) and satisfy the equation
Î“1 ğ‘£ğ‘–= Î“2 ğ‘£ğ‘–ğœğ‘–for a collection of generalized eigenvectors (ğ‘£ğ‘–) [110]. Since this family
of distributions can be identified with the cone of SPD matrices, Sym+, (3.6) can
also be used as a Riemannian metric on Sym+ [99, 24]. We will use this Riemannian
metric to define optimal approximations of the posterior covariance matrix. This
metric on Sym+ is also the unique geodesic distance that satisfies the following two
important invariance properties:
ğ‘‘â„›(ğ´, ğµ) = ğ‘‘â„›(ğ´âˆ’1, ğµâˆ’1)
and
ğ‘‘â„›(ğ´, ğµ) = ğ‘‘â„›(ğ‘€ğ´ğ‘€âŠ¤, ğ‘€ğµğ‘€âŠ¤),
(3.7)
for any nonsingular matrix ğ‘€and matrices ğ´, ğµâˆˆSym+ (e.g., [35]) making it an
ideal candidate to compare covariance matrices. Moreover, ğ‘‘â„›treats under- and over-
approximations similarly in the sense that ğ‘‘â„›(Î“pos, ğ›¼Ì‚ï¸€Î“pos) â†’âˆas ğ›¼â†’0 and as
ğ›¼â†’âˆ.3 This metric has been used successfully in a variety of applications (e.g.,
3This behavior is shared by Steinâ€™s loss function, which has been proposed to assess estimates
of a covariance matrix [137]. Steinâ€™s loss function is just the Kullback-Leibler distance between two
35

[252, 215, 192, 15, 132, 97]). Notice that the flat distance induced by the Frobenius
norm does not satisfy the invariance properties (3.7), and has often been shown to be
inadequate for comparing covariance matrices [97, 7, 255].
In the most general case of manifolds of Gaussian families parameterized by both
the mean and covariance, there seems to be no explicit form for the geodesic distance.
We will show that our posterior covariance matrix approximation is optimal not
only in terms of the Riemannian metric ğ‘‘â„›, but also in terms of the following more
general class, â„’, of loss functions for SPD matrices.
Definition 3.1 (Loss functions). The class â„’is defined as the collection of functions
of the form
ğ¿(ğ´, ğµ) =
ğ‘›
âˆ‘ï¸
ğ‘–=1
ğ‘“(ğœğ‘–),
(3.8)
where ğ´and ğµare SPD matrices, (ğœğ‘–) are the generalized eigenvalues of the pencil
(ğ´, ğµ), and
ğ‘“âˆˆğ’°= {ğ‘”âˆˆğ’1(R+) : ğ‘”â€²(ğ‘¥)(1 âˆ’ğ‘¥) < 0 for ğ‘¥Ì¸= 1, and lim
ğ‘¥â†’âˆğ‘”(ğ‘¥) = âˆ}.
(3.9)
Elements of ğ’°are differentiable real-valued functions defined on the positive axis
that decrease on ğ‘¥< 1, increase on ğ‘¥> 1, and tend to infinity as ğ‘¥â†’âˆ. The
squared metric ğ‘‘2
â„›belongs to the class of loss functions defined by (3.8), whereas the
distance induced by the Frobenius norm does not.
Lemma 3.1, whose proof can be found in Appendix C, justifies the importance of
the class â„’. In particular, it shows that optimality of the covariance matrix approxi-
mation with respect to any loss function in â„’leads to an optimal approximation of the
posterior distribution using a Gaussian (with the same mean) in terms of other famil-
iar criteria used to compare probability measures, such as the Hellinger distance and
the Kullback-Leibler (K-L) divergence [209]. More precisely, we have the following
result:
Gaussian distributions with the same mean, but it is not a metric for SPD matrices.
36

Lemma 3.1 (Equivalence of approximations). If ğ¿âˆˆâ„’, then a matrix Ì‚ï¸€Î“pos âˆˆâ„³ğ‘Ÿ
minimizes the Hellinger distance and the K-L divergence between ğ’©(ğœ‡pos(ğ‘Œ), Î“pos)
and the approximation ğ’©(ğœ‡pos(ğ‘Œ), Ì‚ï¸€Î“pos) iff it minimizes ğ¿( Î“pos, Ì‚ï¸€Î“pos ).
Remark 3.1. We note that neither the Hellinger distance nor the K-L divergence
between the distributions ğ’©(ğœ‡pos(ğ‘Œ), Î“pos) and ğ’©(ğœ‡pos(ğ‘Œ), Ì‚ï¸€Î“pos) depends on the data
ğ‘Œ. Optimality in distribution does not necessarily hold when the posterior means
are different.
3.2.3
Optimality results
We are now in a position to state one of the main results of this chapter. Henceforth
whenever we write that (ğ›¼, ğ‘£) are eigenpairs of (ğ´, ğµ) we mean that (ğ›¼, ğ‘£) are the
generalized eigenvalueâ€“eigenvector pairs of the matrix pencil (ğ´, ğµ).
Theorem 3.1 (Optimal posterior covariance approximation). Let (ğ›¿2
ğ‘–, Ì‚ï¸€ğ‘¤ğ‘–) be the
eigenpairs of the pencil:
(ğ», Î“âˆ’1
pr ),
(3.10)
with the ordering ğ›¿2
ğ‘–â‰¥ğ›¿2
ğ‘–+1, and ğ»= ğºâŠ¤Î“âˆ’1
obsğºas in (3.3). Let ğ¿be a loss function
in the class â„’defined in (3.8).
Then:
(i) A minimizer, Ì‚ï¸€Î“pos, of the loss, ğ¿, between Î“pos and an element of â„³ğ‘Ÿis given
by:
Ì‚ï¸€Î“pos = Î“pr âˆ’ğ¾ğ¾âŠ¤,
ğ¾ğ¾âŠ¤=
ğ‘Ÿ
âˆ‘ï¸
ğ‘–=1
ğ›¿2
ğ‘–
1 + ğ›¿2
ğ‘–
Ì‚ï¸€ğ‘¤ğ‘–Ì‚ï¸€ğ‘¤âŠ¤
ğ‘–.
(3.11)
The corresponding minimum loss is given by:
ğ¿(Ì‚ï¸€Î“pos, Î“pos) = ğ‘“(1) ğ‘Ÿ+
âˆ‘ï¸
ğ‘–>ğ‘Ÿ
ğ‘“
(ï¸‚
1
1 + ğ›¿2
ğ‘–
)ï¸‚
.
(3.12)
(ii) The minimizer (3.11) is unique if the first ğ‘Ÿeigenvalues of (ğ», Î“âˆ’1
pr ) are different.
Theorem 3.1 provides a way to compute the best approximation of Î“pos by matrices
in â„³ğ‘Ÿ: it is just a matter of computing the eigenpairs corresponding to the decreasing
37

sequence of eigenvalues of the pencil (ğ», Î“âˆ’1
pr ) until a stopping criterion is satisfied.
This criterion can be based on the minimum loss (3.12). Notice that the minimum
loss is a function of the generalized eigenvalues (ğ›¿2
ğ‘–)ğ‘–â‰¥ğ‘Ÿthat have not been computed.
This is quite common in numerical linear algebra (e.g., error in the truncated SVD
[90, 110]). However, since the eigenvalues (ğ›¿2
ğ‘–) are computed in a decreasing order,
the minimum loss can be easily bounded.
The generalized eigenvectors, Ì‚ï¸€ğ‘¤ğ‘–, are orthogonal with respect to the inner product
induced by the prior precision matrix, and they maximize the Rayleigh ratio,
Ì‚ï¸€â„›(ğ‘§) = ğ‘§âŠ¤ğ»ğ‘§
ğ‘§âŠ¤Î“âˆ’1
pr ğ‘§,
over subspaces of the form Ì‚ï¸
ğ’²ğ‘–= spanâŠ¥( Ì‚ï¸€ğ‘¤ğ‘—)ğ‘—<ğ‘–. Intuitively, the vectors Ì‚ï¸€ğ‘¤ğ‘–associated
with generalized eigenvalues greater than one correspond to directions in the param-
eter space (or subspaces thereof) where the curvature of the log-posterior density is
constrained more by the log-likelihood than by the prior.
3.2.4
Computing eigenpairs of (ğ», Î“âˆ’1
pr )
If a square root factorization of the prior covariance matrix Î“pr = ğ‘†prğ‘†âŠ¤
pr is available,
then the Hermitian generalized eigenvalue problem can be reduced to a standard one:
find the eigenpairs, (ğ›¿2
ğ‘–, ğ‘¤ğ‘–), of ğ‘†âŠ¤
prğ»ğ‘†pr, and transform the resulting eigenvectors ac-
cording to ğ‘¤ğ‘–â†¦â†’ğ‘†prğ‘¤ğ‘–[13, Section 5.2]. An analogous transformation is also possible
when a square root factorization of Î“âˆ’1
pr is available. Notice that only the actions of ğ‘†pr
and ğ‘†âŠ¤
pr on a vector are required. For instance, evaluating the action of ğ‘†pr might in-
volve the solution of an elliptic PDE [174]. There are numerous examples of priors for
which a decomposition Î“pr = ğ‘†prğ‘†âŠ¤
pr is readily available, e.g., [275, 85, 174, 281, 265].
Either direct methods or, more often, matrix-free algorithms (e.g., Lanczos iteration
or its block version [159, 204, 75, 111]) can be used to solve the standard Hermitian
eigenvalue problem [13, Section 4]. Reference implementations of these algorithms are
available in ARPACK [164]. We note that the Lanczos iteration comes with a rich
literature on error analysis (e.g., [147, 203, 205, 143, 239, 110]). Alternatively, one can
38

use randomized methods [116], which offer the advantage of parallelism (asynchronous
computations) and robustness over standard Lanczos methods [40]. If a square root
factorization of Î“pr is not available, but it is possible to solve linear systems with
Î“âˆ’1
pr , we can use a Lanczos method for generalized Hermitian eigenvalue problems
[13, Section 5.5] where a Krylov basis orthogonal with respect to the inner product
induced by Î“âˆ’1
pr is maintained. Again, ARPACK provides an efficient implementation
of these solvers. When accurately solving linear systems with Î“âˆ’1
pr is a difficult task,
we refer the reader to alternative algorithms proposed in [243] and [112].
Remark 3.2. If a factorization Î“pr = ğ‘†prğ‘†âŠ¤
pr is available, then it is straightforward to
obtain an expression for a non-symmetric square root of the optimal approximation
of Î“pos (3.11) as in [43]:
Ì‚ï¸€ğ‘†pos = ğ‘†pr
(ï¸ƒ
ğ‘Ÿ
âˆ‘ï¸
ğ‘–=1
[ï¸ƒ
1
âˆšï¸€
1 + ğ›¿2
ğ‘–
âˆ’1
]ï¸ƒ
ğ‘¤ğ‘–ğ‘¤âŠ¤
ğ‘–+ ğ¼
)ï¸ƒ
(3.13)
such that Ì‚ï¸€Î“pos = Ì‚ï¸€ğ‘†pos Ì‚ï¸€ğ‘†âŠ¤
pos and ğ‘¤ğ‘–= ğ‘†âˆ’1
pr Ì‚ï¸€ğ‘¤ğ‘–. This expression can be used to efficiently
sample from the approximate posterior distribution ğ’©(ğœ‡pos(ğ‘Œ), Ì‚ï¸€Î“pos) (e.g., [96, 186]).
Alternative techniques for sampling from high-dimensional Gaussian distributions can
be found, for instance, in [211, 100].
3.3
Properties of the optimal covariance approxima-
tion
Now we discuss several implications of the optimal approximation of Î“pos introduced
in the previous section. We start by describing the relationship between this ap-
proximation and the directions of greatest relative reduction of prior variance. Then
we interpret the covariance approximation as the result of projecting the likelihood
function onto a â€œdata-informedâ€ subspace. Finally, we contrast the present approach
with several other approximation strategies: using the Frobenius norm as a loss func-
tion for the covariance matrix approximation, or developing low-rank approximations
39

based on prior or Hessian information alone. We conclude by drawing the connections
with the BFGS Kalman filter update.
3.3.1
Interpretation of the eigendirections
Thanks to the particular structure of loss functions in â„’, the problem of approxi-
mating Î“pos is equivalent to that of approximating Î“âˆ’1
pos. Yet the form of the optimal
approximation of Î“âˆ’1
pos is important, as it explicitly describes the directions that con-
trol the ratio of posterior to prior variance. The following corollary to Theorem 3.1
characterizes these directions.
Corollary 3.1 (Optimal posterior precision approximation). Let (ğ›¿2
ğ‘–, Ì‚ï¸€ğ‘¤ğ‘–) and ğ¿âˆˆâ„’
be defined as in Theorem 3.1. Then:
(i) A minimizer of ğ¿(ğµ, Î“âˆ’1
pos) for
ğµâˆˆâ„³âˆ’1
ğ‘Ÿ
:=
{ï¸€
Î“âˆ’1
pr + ğ½ğ½âŠ¤: rank(ğ½) â‰¤ğ‘Ÿ
}ï¸€
(3.14)
is given by
Ì‚ï¸€Î“âˆ’1
pos = Î“âˆ’1
pr + ğ‘ˆğ‘ˆâŠ¤,
ğ‘ˆğ‘ˆâŠ¤=
ğ‘Ÿ
âˆ‘ï¸
ğ‘–=1
ğ›¿2
ğ‘–Ìƒï¸€ğ‘¤ğ‘–Ìƒï¸€ğ‘¤âŠ¤
ğ‘–,
Ìƒï¸€ğ‘¤ğ‘–= Î“âˆ’1
pr Ì‚ï¸€ğ‘¤ğ‘–.
(3.15)
The minimizer (3.15) is unique if the first ğ‘Ÿeigenvalues of (ğ», Î“âˆ’1
pr ) are different.
(ii) The optimal posterior precision matrix (3.15) is precisely the inverse of the
optimal posterior covariance matrix (3.11).
(iii) The vectors Ìƒï¸€ğ‘¤ğ‘–are generalized eigenvectors of the pencil (Î“pos, Î“pr):
Î“pos Ìƒï¸€ğ‘¤ğ‘–=
1
1 + ğ›¿2
ğ‘–
Î“pr Ìƒï¸€ğ‘¤ğ‘–.
(3.16)
Note that the definition of the class â„³âˆ’1
ğ‘Ÿ
is analogous to that of â„³ğ‘Ÿ. Indeed,
Lemma C.2 in Appendix C defines a bijection between these two classes.
40

The vectors Ìƒï¸€ğ‘¤ğ‘–= Î“âˆ’1
pr Ì‚ï¸€ğ‘¤ğ‘–are orthogonal with respect to the inner product defined
by Î“pr. By (3.16), we also know that Ìƒï¸€ğ‘¤ğ‘–minimizes the generalized Rayleigh quotient,
â„›(ğ‘§) = ğ‘§âŠ¤Î“posğ‘§
ğ‘§âŠ¤Î“pr ğ‘§= Var(ğ‘§âŠ¤ğ‘¥| ğ‘¦)
Var(ğ‘§âŠ¤ğ‘¥) ,
(3.17)
over subspaces of the form Ìƒï¸
ğ’²ğ‘–= spanâŠ¥( Ìƒï¸€ğ‘¤ğ‘—)ğ‘—<ğ‘–. This Rayleigh quotient is precisely
the ratio of posterior to prior variance along a particular direction, ğ‘§, in the param-
eter space. The smallest values that â„›can take over the subspaces Ìƒï¸
ğ’²ğ‘–are exactly
the smallest generalized eigenvalues of (Î“pos, Î“pr). In particular, the data are most
informative along the first ğ‘Ÿeigenvectors Ìƒï¸€ğ‘¤ğ‘–and, since
â„›( Ìƒï¸€ğ‘¤ğ‘–) = Var( Ìƒï¸€ğ‘¤âŠ¤
ğ‘–ğ‘¥| ğ‘¦)
Var( Ìƒï¸€ğ‘¤âŠ¤
ğ‘–ğ‘¥)
=
1
1 + ğ›¿2
ğ‘–
,
(3.18)
the posterior variance is smaller than the prior variance by a factor of (1 + ğ›¿2
ğ‘–)âˆ’1. In
the span of the other eigenvectors, ( Ìƒï¸€ğ‘¤ğ‘–)ğ‘–>ğ‘Ÿ, the data are not as informative. Hence,
( Ìƒï¸€ğ‘¤ğ‘–) are the directions along which the ratio of posterior to prior variance is mini-
mized. Furthermore, a simple computation shows that these directions also maximize
the relative difference between prior and posterior variance normalized by the prior
variance. Indeed, if the directions ( Ìƒï¸€ğ‘¤ğ‘–) minimize (3.17) then they must also maximize
1 âˆ’â„›(ğ‘§), leading to:
1 âˆ’â„›( Ìƒï¸€ğ‘¤ğ‘–) = Var( Ìƒï¸€ğ‘¤âŠ¤
ğ‘–ğ‘¥) âˆ’Var( Ìƒï¸€ğ‘¤âŠ¤
ğ‘–ğ‘¥| ğ‘¦)
Var( Ìƒï¸€ğ‘¤âŠ¤
ğ‘–ğ‘¥)
=
ğ›¿2
ğ‘–
1 + ğ›¿2
ğ‘–
.
(3.19)
3.3.2
Optimal projector
Since the data are most informative on a subspace of the parameter space, it should
be possible to reduce the effective dimension of the inference problem in a manner
that is consistent with the posterior approximation. This is essentially the content of
the following corollary, which follows by a simple computation.
Corollary 3.2 (Optimal projector). Let Ì‚ï¸€Î“pos and the vectors ( Ì‚ï¸€ğ‘¤ğ‘–, Ìƒï¸€ğ‘¤ğ‘–) be defined as
in Theorems 3.1 and 3.1. Consider the reduced forward operator Ì‚ï¸€ğºğ‘Ÿ= ğºâˆ˜ğ‘ƒğ‘Ÿ, where
41

ğ‘ƒğ‘Ÿis the oblique projector (i.e., ğ‘ƒ2
ğ‘Ÿ= ğ‘ƒğ‘Ÿbut ğ‘ƒâŠ¤
ğ‘¡Ì¸= ğ‘ƒğ‘Ÿ):
ğ‘ƒğ‘Ÿ=
ğ‘Ÿ
âˆ‘ï¸
ğ‘–=1
Ì‚ï¸€ğ‘¤ğ‘–Ìƒï¸€ğ‘¤âŠ¤
ğ‘–.
(3.20)
Then Ì‚ï¸€Î“pos is precisely the posterior covariance matrix corresponding to the Bayesian
linear model:
ğ‘Œ| ğ‘‹âˆ¼ğ’©( Ì‚ï¸€ğºğ‘Ÿğ‘‹, Î“obs),
ğ‘‹âˆ¼ğ’©(0, Î“pr).
(3.21)
The projected Gaussian linear model (3.21) reveals the intrinsic dimensionality of
the inference problem. The introduction of the optimal projector (3.20) is also useful
in the context of dimensionality reduction for nonlinear inverse problems. In this case
a particularly simple and effective approximation of the posterior density, ğœ‹pos(ğ‘¥|ğ‘¦), is
of the form Ì‚ï¸€ğœ‹pos(ğ‘¥|ğ‘¦) âˆğœ‹(ğ‘¦; ğ‘ƒğ‘Ÿğ‘¥) ğœ‹pr(ğ‘¥), where ğœ‹pr is the prior density and ğœ‹(ğ‘¦; ğ‘ƒğ‘Ÿğ‘¥)
is the density corresponding to the likelihood function with parameters constrained
by the projector. The range of the projector can be determined by combining locally
optimal data-informed subspaces from high-density regions in the support of the
posterior distribution. This approximation is the subject of a related work [74].
Returning to the linear inverse problem, notice also that the posterior mean of
the projected model (3.21) might be used as an efficient approximation of the exact
posterior mean. We will show in Section 3.4 that this posterior mean approximation
in fact minimizes the Bayes risk for a weighted squared-error loss among all low-rank
linear functions of the data.
3.3.3
Comparison with optimality in Frobenius norm
Thus far our optimality results for the approximation of Î“pos have been restricted to
the class of loss functions â„’given in Definition 3.1. However, it is also interesting to
investigate optimality in the metric defined by the Frobenius norm. Given any two
matrices ğ´and ğµof the same size, the Frobenius distance between them is defined
as â€–ğ´âˆ’ğµâ€–, where â€– Â·â€– is the Frobenius norm. Note that the Frobenius distance does
not exploit the structure of the positive definite cone of symmetric matrices. The
42

matrix Ì‚ï¸€Î“pos âˆˆâ„³ğ‘Ÿthat minimizes the Frobenius distance from the exact posterior
covariance matrix is given by:
Ì‚ï¸€Î“pos = Î“pr âˆ’ğ¾ğ¾âŠ¤,
ğ¾ğ¾âŠ¤=
ğ‘Ÿ
âˆ‘ï¸
ğ‘–=1
ğœ†ğ‘–ğ‘¢ğ‘–ğ‘¢âŠ¤
ğ‘–,
(3.22)
where (ğ‘¢ğ‘–) are the directions corresponding to the ğ‘Ÿlargest eigenvalues of Î“pr âˆ’Î“pos.
This result can be very different from the optimal approximation given in Theorem
3.1. In particular, the directions (ğ‘¢ğ‘–) are solutions of the eigenvalue problem
Î“pr ğºâŠ¤Î“âˆ’1
ğ‘ŒğºÎ“pr ğ‘¢= ğœ†ğ‘¢,
(3.23)
which maximize
ğ‘¢âŠ¤(Î“pr âˆ’Î“pos)ğ‘¢= Var(ğ‘¢âŠ¤ğ‘‹) âˆ’Var(ğ‘¢âŠ¤ğ‘‹| ğ‘Œ).
(3.24)
That is, while optimality in the Riemannian metric ğ‘‘â„›identifies directions that max-
imize the relative difference between prior and posterior variance, the Frobenius dis-
tance favors directions that maximize only the absolute value of this difference. There
are many reasons to prefer the former. For instance, data might be informative along
directions of low prior variance (perhaps due to inadequacies in prior modeling); a
covariance matrix approximation that is optimal in Frobenius distance may ignore
updates in these directions entirely. Also, if parameters of interest (i.e., components
of ğ‘‹) have differing units of measurement, relative variance reduction provides a
unit-independent way of judging the quality of a posterior approximation; this notion
follows naturally from the second invariance property of ğ‘‘â„›in (3.7). From a computa-
tional perspective, solving the eigenvalue problem (3.23) is quite expensive compared
to finding the generalized eigenpairs of the pencil (ğ», Î“âˆ’1
pr ). Finally, optimality in the
Frobenius distance for an approximation of Î“pos does not yield an optimality state-
ment for the corresponding approximation of the posterior distribution, as shown in
Lemma 3.1 for loss functions in â„’.
43

3.3.4
Suboptimal posterior covariance approximations
Hessian-based and prior-based reduction schemes
The posterior approximation described by Theorem 3.1 uses both Hessian and prior
information. It is instructive to consider approximations of the linear Bayesian inverse
problem that rely only on one or the other.
As we will illustrate numerically in
Section 3.5.1, these approximations can be viewed as natural limiting cases of our
approach. They are also closely related to previous efforts in dimensionality reduction
that propose only Hessian-based [168] or prior-based [188] reductions. In contrast
with these previous efforts, here we will consider versions of Hessian- and prior-based
reductions that do not discard prior information in the remaining directions. In other
words, we will discuss posterior covariance approximations that remain in the form
of (3.4)â€”i.e., updating the prior covariance only in ğ‘Ÿdirections.
A Hessian-based reduction scheme updates Î“pr in directions where the data have
greatest influence in an absolute sense (i.e., not relative to the prior). This involves
approximating the negative log-likelihood Hessian (3.3) with a low-rank decomposi-
tion as follows: let (ğ‘ 2
ğ‘–, ğ‘£ğ‘–) be the eigenvalue-eigenvector pairs of ğ»with the ordering
ğ‘ 2
ğ‘–â‰¥ğ‘ 2
ğ‘–+1. Then a best low-rank approximation of ğ»in the Frobenius norm is given
by:
ğ»â‰ˆ
ğ‘Ÿ
âˆ‘ï¸
ğ‘–=1
ğ‘ 2
ğ‘–ğ‘£ğ‘–ğ‘£âŠ¤
ğ‘–= ğ‘‰ğ‘Ÿğ‘†ğ‘Ÿğ‘‰âŠ¤
ğ‘Ÿ,
where ğ‘£ğ‘–is the ğ‘–th column of ğ‘‰ğ‘Ÿand ğ‘†ğ‘Ÿ= diag{ğ‘ 2
1, . . . , ğ‘ 2
ğ‘Ÿ}. Using Woodburyâ€™s identity
we then obtain an approximation of Î“pos as a low-rank negative semidefinite update
of Î“pr:
Î“pos â‰ˆ
(ï¸€
ğ‘‰ğ‘Ÿğ‘†ğ‘Ÿğ‘‰âŠ¤
ğ‘Ÿ+ Î“âˆ’1
pr
)ï¸€âˆ’1 = Î“pr âˆ’Î“prğ‘‰ğ‘Ÿ
(ï¸€
ğ‘†âˆ’1
ğ‘Ÿ
+ ğ‘‰âŠ¤
ğ‘ŸÎ“prğ‘‰ğ‘Ÿ
)ï¸€âˆ’1 ğ‘‰âŠ¤
ğ‘ŸÎ“pr.
(3.25)
This approximation of the posterior covariance matrix belongs to the class â„³ğ‘Ÿ. Thus,
Hessian-based reductions are in general suboptimal when compared to the optimal
approximations defined in Theorem 3.1. Note that an equivalent way to obtain (3.25)
is to use a reduced forward operator of the form Ì‚ï¸€ğº= ğºâˆ˜ğ‘‰ğ‘Ÿğ‘‰âŠ¤
ğ‘Ÿ, which is the compo-
44

sition of the original forward operator with a projector onto the leading eigenspace
of ğ». In general, the projector ğ‘ƒğ‘Ÿ= ğ‘‰ğ‘Ÿğ‘‰âŠ¤
ğ‘Ÿ
is different from the optimal projector
defined in Corollary 3.2 and is thus suboptimal.
To achieve prior-based reductions, on the other hand, we restrict the Bayesian
inference problem to directions in the parameter space that explain most of the prior
variance. More precisely, we look for a rank-ğ‘Ÿorthogonal projector, ğ‘ƒğ‘Ÿ, that minimizes
the mean squared-error defined as:
â„°(ğ‘ƒğ‘Ÿ) = E
(ï¸€
â€–ğ‘‹âˆ’ğ‘ƒğ‘Ÿğ‘‹â€–2)ï¸€
,
(3.26)
where the expectation is taken over the prior distribution (assumed to have zero
mean) and â€–Â·â€– is the standard Euclidean norm [133]. Let (ğ‘¡2
ğ‘–, ğ‘¢ğ‘–) be the eigenpairs of
Î“pr ordered as ğ‘¡2
ğ‘–â‰¥ğ‘¡2
ğ‘–+1. Then a minimizer of (3.26) is given by a projector, ğ‘ƒğ‘Ÿ, onto
the leading eigenspace of Î“pr defined as: ğ‘ƒğ‘Ÿ= âˆ‘ï¸€ğ‘Ÿ
ğ‘–=1 ğ‘¢ğ‘–ğ‘¢âŠ¤
ğ‘–= ğ‘ˆğ‘Ÿğ‘ˆâŠ¤
ğ‘Ÿ, where ğ‘¢ğ‘–is the
ğ‘–th column of ğ‘ˆğ‘Ÿ. The actual approximation of the linear inverse problem consists
of using the projected forward operator, Ì‚ï¸€ğº= ğºâˆ˜ğ‘ˆğ‘Ÿğ‘ˆâŠ¤
ğ‘Ÿ. By direct comparison with
the optimal projector defined in Corollary 3.2, we see that prior-based reductions are
suboptimal in general. Also in this case, the posterior covariance matrix with the
projected Gaussian model can be written as a negative semidefinite update of Î“pr:
Î“pos â‰ˆÎ“pr âˆ’ğ‘ˆğ‘Ÿğ‘‡ğ‘Ÿ[ ( ğ‘ˆâŠ¤
ğ‘Ÿğ»ğ‘ˆğ‘Ÿ)âˆ’1 + ğ‘‡ğ‘Ÿ]âˆ’1ğ‘‡ğ‘Ÿğ‘ˆâŠ¤
ğ‘Ÿ,
where ğ‘‡ğ‘Ÿ= diag{ğ‘¡2
1, . . . , ğ‘¡2
ğ‘Ÿ}. The double matrix inversion makes this low-rank up-
date computationally challenging to implement. It is also not optimal, as shown in
Theorem 3.1.
To summarize, the Hessian and prior-based dimensionality reduction techniques
are both suboptimal. These methods do not take into account the interactions be-
tween the dominant directions of ğ»and those of Î“pr, nor the relative importance of
these quantities. Accounting for such interaction is a key feature of the optimal covari-
ance approximation described in Theorem 3.1. Section 3.5.1 will illustrate conditions
under which these interactions become essential.
45

Connections with the BFGS Kalman filter
The linear Bayesian inverse problem analyzed in this chapter can be interpreted as
the analysis step of a linear Bayesian filtering problem [93]. If the prior distribution
corresponds to the forecast distribution at some time ğ‘¡, the posterior coincides with
the so-called analysis distribution. In the linear case, with Gaussian process noise
and observational errors, both of these distributions are Gaussian. The Kalman filter
is a Bayesian solution to this filtering problem [146]. In [10] the authors propose a
computationally feasible way to implement (and approximate) this solution in large-
scale systems. The key observation is that when solving an SPD linear system of
the form ğ´ğ‘¥= ğ‘by means of BFGS or limited memory BFGS (L-BFGS [176]),
one typically obtains an approximation of ğ´âˆ’1 for free. This approximation can be
written as a low-rank correction of an arbitrary positive definite initial approximation
matrix ğ´âˆ’1
0 . The matrix ğ´âˆ’1
0
can be, for instance, the scaled identity. Notice that
the approximation of ğ´âˆ’1 given by L-BFGS is full rank and positive definite. This
approximation is in principle convergent as the storage limit of L-BFGS increases
[198]. An L-BFGS approximation of ğ´is also possible [276].
There are many ways to exploit this property of the L-BFGS method. For example,
in [10] the posterior covariance is written as a low-rank update of the prior covariance
matrix: Î“pos = Î“pr âˆ’Î“prğºâŠ¤Î“âˆ’1
ğ‘ŒğºÎ“pr, where Î“ğ‘Œ= Î“obs + ğºÎ“prğºâŠ¤, and Î“âˆ’1
ğ‘Œ
itself
is approximated using the L-BFGS method. Since this approximation of Î“ğ‘Œis full
rank, however, this approach does not exploit potential low-dimensional structure of
the inverse problem. Alternatively, one can obtain an L-BFGS approximation of Î“pos
when solving the linear system Î“âˆ’1
pos ğ‘¥= ğºâŠ¤Î“âˆ’1
obsğ‘Œfor the posterior mean ğœ‡pos(ğ‘Œ) [11].
If one uses the prior covariance matrix as an initial approximation matrix, ğ´âˆ’1
0 , then
the resulting L-BFGS approximation of Î“pos can be written as a low-rank update of
Î“pr. This approximation format is similar to the one discussed in [96] and advocated
in this chapter. However, the approach of [11] (or its ensemble version [254]) does not
correspond to any known optimal approximation of the posterior covariance matrix,
nor does it lead to any optimality statement between the corresponding probability
46

distributions. This is an important contrast with the present approach, which we will
revisit numerically in Section 3.5.1.
3.4
Optimal approximation of the posterior mean
In this section, we develop and characterize fast approximations of the posterior mean
that can be used, for instance, to accelerate repeated inversion with multiple data
sets.
Note that we are not proposing alternatives to the efficient computation of
the posterior mean for a single realization of the data. This task can already be ac-
complished with current state-of-the-art iterative solvers for regularized least-squares
problems [128, 207, 3, 190, 17]. Instead, we are interested in constructing statistically
optimal approximations4 of the posterior mean as linear functions of the data. That
is, we seek a matrix ğ´, from an approximation class to be defined shortly, such that
the posterior mean can be approximated as ğœ‡pos(ğ‘Œ) â‰ˆğ´ğ‘Œ.
We will investigate
different approximation classes for ğ´; in particular, we will only consider approxima-
tion classes for which applying ğ´to a vector ğ‘Œis relatively inexpensive. Computing
such a matrix ğ´is more expensive than solving a single linear system associated with
the posterior precision matrix to determine the posterior mean. However, once ğ´is
computed, it can be applied inexpensively to any realization of the data.5 Our ap-
proach is therefore justified when the posterior mean must be evaluated for multiple
instances of the data. This approach can thus be viewed as an offlineâ€“online strategy,
where a more costly but data-independent offline calculation is followed by fast online
evaluations. Moreover, we will show that these approximations can be obtained from
an optimal approximation of the posterior covariance matrix (cf. Theorem 3.1) with
minimal additional cost. Hence, if one is interested in both the posterior mean and
covariance matrix (as is often the case in the Bayesian approach to inverse problems),
then the approximation formulas we propose can be more efficient than standard
approaches even for a single realization of the data.
4 We will precisely define this notion of optimality in Section 3.4.1.
5In particular, applying ğ´is much cheaper than solving a linear system.
47

3.4.1
Optimality results
For the Bayesian linear model defined in (3.1), the posterior mode is equal to the
posterior mean, ğœ‡pos(ğ‘Œ) = E(ğ‘‹|ğ‘Œ), which is in turn the minimizer of the Bayes risk
for squared-error loss [163, 175]. We first review this fact and establish some basic
notation. Let ğ‘†be an SPD matrix and let
ğ¿(ğ›¿(ğ‘Œ), ğ‘‹) = (ğ‘‹âˆ’ğ›¿(ğ‘Œ))âŠ¤ğ‘†(ğ‘‹âˆ’ğ›¿(ğ‘Œ)) = â€–ğ‘‹âˆ’ğ›¿(ğ‘Œ)â€–2
ğ‘†
be the loss incurred by the estimator ğ›¿(ğ‘Œ) of ğ‘‹. The Bayes risk, ğ‘…(ğ›¿(ğ‘Œ), ğ‘‹), of
ğ›¿(ğ‘Œ) is defined as the average loss over the joint distribution of ğ‘‹and ğ‘Œ[49, 163]:
ğ‘…(ğ›¿(ğ‘Œ), ğ‘‹) = E ( ğ¿(ğ›¿(ğ‘Œ), ğ‘‹) ). Since
ğ‘…(ğ›¿(ğ‘Œ), ğ‘‹) = E
(ï¸€
â€– ğ›¿(ğ‘Œ) âˆ’ğœ‡pos(ğ‘Œ) â€–2
ğ‘†
)ï¸€
+ E
(ï¸€
â€–ğœ‡pos(ğ‘Œ) âˆ’ğ‘‹â€–2
ğ‘†
)ï¸€
,
(3.27)
it follows that ğ›¿(ğ‘Œ) = ğœ‡pos(ğ‘Œ) minimizes the Bayes risk over all estimators of ğ‘‹.
To study approximations of ğœ‡pos(ğ‘Œ), we use the squared-error loss function defined
by the Mahalanobis distance [61] induced by Î“âˆ’1
pos: ğ¿(ğ›¿(ğ‘Œ), ğ‘‹) = â€–ğ›¿(ğ‘Œ) âˆ’ğ‘‹â€–2
Î“âˆ’1
pos .
This loss function accounts for the geometry induced by the posterior measure on the
parameter space, penalizing errors in the approximation of ğœ‡pos(ğ‘Œ) more strongly in
directions of lower posterior variance.
Under the assumption of zero prior mean, ğœ‡pos(ğ‘Œ) is a linear function of the data.
Hence we seek approximations of ğœ‡pos(ğ‘Œ) of the form ğ´ğ‘Œ, where ğ´is a matrix in
a class to be defined. Our goal is to obtain fast posterior mean approximations that
can be applied repeatedly to multiple realizations of ğ‘Œ. We consider two classes of
approximation matrices:
ğ’œğ‘Ÿ:= {ğ´: rank(ğ´) â‰¤ğ‘Ÿ}
and
Ì‚ï¸€
ğ’œğ‘Ÿ:=
{ï¸€
ğ´= (Î“pr âˆ’ğµ) ğºâŠ¤Î“âˆ’1
obs : rank(ğµ) â‰¤ğ‘Ÿ
}ï¸€
.
(3.28)
The class ğ’œğ‘Ÿconsists of low-rank matrices; it is standard in the statistics literature
[133]. The class Ì‚ï¸€
ğ’œğ‘Ÿ, on the other hand, can be understood via comparison with (3.2);
48

it simply replaces Î“pos with a low-rank negative semidefinite update of Î“pr. We shall
henceforth use ğ’œto denote either of the two classes above.
Let ğ‘…ğ’œ(ğ´ğ‘Œ, ğ‘‹) be the Bayes risk of ğ´ğ‘Œsubject to ğ´âˆˆğ’œ. We may now restate
our goal as: find a matrix, ğ´* âˆˆğ’œ, that minimizes the Bayes risk ğ‘…ğ’œ(ğ´ğ‘Œ, ğ‘‹). That
is, find ğ´* âˆˆğ’œsuch that
ğ‘…ğ’œ(ğ´*ğ‘Œ, ğ‘‹) = min
ğ´âˆˆğ’œE( â€–ğ´ğ‘Œâˆ’ğ‘‹â€–2
Î“âˆ’1
pos ).
(3.29)
The following two theorems show that for either class of approximation matrices, ğ’œğ‘Ÿ
or Ì‚ï¸€
ğ’œğ‘Ÿ, this problem admits a particularly simple analytical solution that exploits the
structure of the optimal approximation of Î“pos. The proofs of the theorems rely on a
result developed independently by Sondermann [256] and Friedland & Torokhti [102],
and are given in Appendix C. We also use the fact that E
(ï¸
â€–ğœ‡pos(ğ‘Œ) âˆ’ğ‘‹â€–2
Î“âˆ’1
pos
)ï¸
= â„“,
where â„“is the dimension of the parameter space.
Theorem 3.2. Let (ğ›¿2
ğ‘–, Ì‚ï¸€ğ‘¤ğ‘–) be defined as in Theorem 3.1 and let (Ì‚ï¸€ğ‘£ğ‘–) be generalized
eigenvectors of the pencil (ğºÎ“prğºâŠ¤, Î“obs) associated with a non-increasing sequence
of eigenvalues, with the normalization Ì‚ï¸€ğ‘£âŠ¤
ğ‘–Î“obs Ì‚ï¸€ğ‘£ğ‘–= 1. Then:
(i) A solution of (3.29) for ğ´âˆˆğ’œğ‘Ÿis given by:
ğ´* =
ğ‘Ÿ
âˆ‘ï¸
ğ‘–=1
ğ›¿ğ‘–
1 + ğ›¿2
ğ‘–
Ì‚ï¸€ğ‘¤ğ‘–Ì‚ï¸€ğ‘£âŠ¤
ğ‘–,
(3.30)
(ii) The corresponding minimum Bayes risk over ğ’œğ‘Ÿis given by:
ğ‘…ğ’œğ‘Ÿ(ğ´*ğ‘Œ, ğ‘‹) = E
(ï¸
â€–ğ´*ğ‘Œâˆ’ğœ‡pos(ğ‘Œ)â€–2
Î“âˆ’1
pos
)ï¸
+E
(ï¸
â€–ğœ‡pos(ğ‘Œ) âˆ’ğ‘‹â€–2
Î“âˆ’1
pos
)ï¸
=
âˆ‘ï¸
ğ‘–>ğ‘Ÿ
ğ›¿2
ğ‘–+â„“.
(3.31)
Notice that the rank-ğ‘Ÿposterior mean approximation given by Theorem 3.2 co-
incides with the posterior mean of the projected linear Gaussian model defined in
(3.21). Thus, applying this approximation to a new realization of the data requires
only a low-rank matrix-vector product, a computationally trivial task. We define the
49

quality of a posterior mean approximation as the minimum Bayes risk (3.31). No-
tice, however, that for a given rank ğ‘Ÿof the approximation, (3.31) depends on the
eigenvalues that have not yet been computed. Since (ğ›¿2
ğ‘–) are determined in order
of decreasing magnitude, (3.31) can be easily bounded (cf. discussion after Theorem
3.1). The forthcoming minimum Bayes risk (3.34) can be bounded analogously.
Remark 3.3. Equation (3.30) can be interpreted as the truncated GSVD solution
of a Tikhonov regularized linear inverse problem [121] (with unit regularization pa-
rameter). Hence, Theorem 3.2 also describes a Bayesian property of the (frequentist)
truncated GSVD estimator.
Remark 3.4. If factorizations of the form Î“pr = ğ‘†prğ‘†âŠ¤
pr and Î“obs = ğ‘†obsğ‘†âŠ¤
obs are read-
ily available, then we can characterize the triplets (ğ›¿ğ‘–, Ì‚ï¸€ğ‘¤ğ‘–, Ì‚ï¸€ğ‘£ğ‘–) from a singular value
decomposition, ğ‘†âˆ’1
obsğºğ‘†pr = âˆ‘ï¸€
ğ‘–â‰¥1 ğ›¿ğ‘–ğ‘£ğ‘–ğ‘¤âŠ¤
ğ‘–, of the matrix ğ‘†âˆ’1
obsğºğ‘†pr with the transfor-
mations Ì‚ï¸€ğ‘¤ğ‘–= ğ‘†prğ‘¤ğ‘–, Ì‚ï¸€ğ‘£ğ‘–= ğ‘†âˆ’âŠ¤
obs ğ‘£ğ‘–and the ordering ğ›¿ğ‘–â‰¥ğ›¿ğ‘–+1.
In particular, the
approximate posterior mean can be written as:
ğœ‡(ğ‘Ÿ)
pos(ğ‘Œ) = ğ‘†pr(ğ‘†âˆ’1
obsğºğ‘†pr)Tikh
ğ‘Ÿ
ğ‘†âˆ’1
obsğ‘Œ
(3.32)
where (ğ‘†âˆ’1
obsğºğ‘†pr)Tikh
ğ‘Ÿ
is the best rank-ğ‘Ÿapproximation to a Tikhonov regularized
inverse.6 That is, for any matrix ğ´, (ğ´)ğ‘Ÿis the best rank-ğ‘Ÿapproximation of ğ´(e.g.,
computed via SVD), whereas (ğ´)Tikh := (ğ´âŠ¤ğ´+ ğ¼)âˆ’1ğ´âŠ¤.
Theorem 3.3. Let Ì‚ï¸€Î“pos âˆˆâ„³ğ‘Ÿbe the optimal approximation of Î“pos defined in The-
orem 3.1. Then:
(i) A solution of (3.29) for ğ´âˆˆÌ‚ï¸€
ğ’œğ‘Ÿis given by:
Ì‚ï¸€ğ´* = Ì‚ï¸€Î“pos ğºâŠ¤Î“âˆ’1
obs.
(3.33)
6With unit regularization parameter and identity regularization operator [122].
50

(ii) The corresponding minimum Bayes risk over Ì‚ï¸€
ğ’œğ‘Ÿis given by:
ğ‘…Ì‚ï¸€
ğ’œğ‘Ÿ( Ì‚ï¸€ğ´*ğ‘Œ, ğ‘‹) = E
(ï¸‚âƒ¦âƒ¦âƒ¦Ì‚ï¸€ğ´*ğ‘Œâˆ’ğœ‡pos(ğ‘Œ)
âƒ¦âƒ¦âƒ¦
2
Î“âˆ’1
pos
)ï¸‚
+E
(ï¸
â€–ğœ‡pos(ğ‘Œ) âˆ’ğ‘‹â€–2
Î“âˆ’1
pos
)ï¸
=
âˆ‘ï¸
ğ‘–>ğ‘Ÿ
ğ›¿6
ğ‘–+â„“.
(3.34)
Once the optimal approximation of Î“pos described in Theorem 3.3 is computed, the
cost of approximating ğœ‡pos(ğ‘Œ) for a new realization of ğ‘Œis dominated by the adjoint
and prior solves needed to apply ğºâŠ¤and Î“pr, respectively. Combining the optimal
approximations of ğœ‡pos(ğ‘Œ) and Î“pos given by Theorems 3.3 and 3.1, respectively, yields
a complete approximation of the Gaussian posterior distribution. This is precisely the
approximation adopted by the stochastic Newton MCMC method [186] to describe
the Gaussian proposal distribution obtained from a local linearization of the forward
operator of a nonlinear Bayesian inverse problem. Our results support the algorithmic
choice of [186] with precise optimality statements.
It is worth noting that the two optimal Bayes risks, (3.31) and (3.34), depend
on the parameter, ğ‘Ÿ, that defines the dimension of the corresponding approximation
classes ğ’œğ‘Ÿand Ì‚ï¸€
ğ’œğ‘Ÿ. In the former case, ğ‘Ÿis the rank of the optimal matrix that de-
fines the approximation. In the latter case, ğ‘Ÿis the rank of a negative update of Î“pr
that yields the posterior covariance matrix approximation. We shall thus refer to the
estimator given by Theorem 4.2 as the low-rank approximation and to the estimator
given by Theorem 3.3 as the low-rank update approximation. In both cases, we shall
refer to ğ‘Ÿas the order of the approximation. A posterior mean approximation of order
ğ‘Ÿwill be called under-resolved if more than ğ‘Ÿgeneralized eigenvalues of the pencil
(ğ», Î“âˆ’1
pr ) are greater than one. If this is the case, then using the low-rank update ap-
proximation is not appropriate because the associated Bayes risk includes high-order
powers of eigenvalues of (ğ», Î“âˆ’1
pr ) that are greater than one. Thus, under-resolved
approximations tend to be more accurate when using the low-rank approximation.
As we will show in Section 4.4, this estimator is also less expensive to compute than
its counterpart in Theorem 3.3. If, on the other hand, fewer than ğ‘Ÿeigenvalues of
(ğ», Î“âˆ’1
pr ) are greater than one, then the optimal low-rank update estimator will have
51

better performance than the optimal low-rank estimator in the following sense:
0 < ğ‘…ğ’œğ‘Ÿ(ğ´*ğ‘Œ, ğ‘‹) âˆ’ğ‘…Ì‚ï¸€
ğ’œğ‘Ÿ( Ì‚ï¸€ğ´*ğ‘Œ, ğ‘‹) =
âˆ‘ï¸
ğ‘–>ğ‘Ÿ
ğ›¿2
ğ‘–
(ï¸€
1 + ğ›¿2
ğ‘–
)ï¸€(ï¸€
1 âˆ’ğ›¿2
ğ‘–
)ï¸€
.
3.4.2
Connection with â€œpriorconditionersâ€
In this subsection we draw connections between the low-rank approximation of the
posterior mean given in Theorem 3.2 and the regularized solution of a discrete ill-
posed inverse problem, ğ‘Œ= ğºğ‘‹+â„°(using the notation of this chapter), as presented
in [47, 44]. In [47, 44], the authors propose an early stopping regularization using
iterative solvers preconditioned by prior statistical information on the parameter of
interest, say ğ‘‹âˆ¼ğ’©(0, Î“pr), and on the noise, say â„°âˆ¼ğ’©(0, Î“obs).7
That is, if
factorizations Î“pr = ğ‘†prğ‘†âŠ¤
pr and Î“obs = ğ‘†obsğ‘†âŠ¤
obs are available, then [47] provides a
solution, ğ‘¥= ğ‘†pr ğ‘, to the inverse problem, where ğ‘comes from an early stopping
regularization applied to the preconditioned linear system:8
ğ‘†âˆ’1
obsğºğ‘†pr ğ‘= ğ‘†âˆ’1
obsğ‘¦.
(3.35)
The iterative method of choice in this case is the CGLS algorithm [47, 119] (or GMRES
for nonsymmetric square systems [45]) equipped with a proper stopping criterion
(e.g., the discrepancy principle [144]). Although the approach of [47] is not exactly
Bayesian, we can still use the optimality results of Theorem 3.2 to justify the observed
good performance of this particular form of regularization. By a property of the CGLS
algorithm, the ğ‘Ÿth iterate, ğ‘¥ğ‘Ÿ= ğ‘†prğ‘ğ‘Ÿ, satisfies:
ğ‘ğ‘Ÿ= argmin
ğ‘âˆˆğ’¦ğ‘Ÿ( Ì‚ï¸€
ğ»,Ì‚ï¸€ğ‘¦)
â€– ğ‘†âˆ’1
obsğ‘¦âˆ’ğ‘†âˆ’1
obsğºğ‘†prğ‘â€–.
(3.36)
where ğ’¦ğ‘Ÿ( Ì‚ï¸€ğ», Ì‚ï¸€ğ‘¦) is the ğ‘Ÿâ€“dimensional Krylov subspace associated with the matrix
Ì‚ï¸€ğ»= ğ‘†âŠ¤
prğ»ğ‘†pr and starting vector Ì‚ï¸€ğ‘¦= ğ‘†âŠ¤
prğºâŠ¤Î“âˆ’1
obsğ‘¦. It was shown in [127] that the
7It suffices to consider a Gaussian approximation to the distribution of ğ‘‹and â„°
8We can think of ğ‘¦as a particular realization of ğ‘Œ.
52

CGLS solution, at convergence, can be written as ğ‘¥* = ğ‘†pr(ğ‘†âˆ’1
obsğºğ‘†pr)â€ ğ‘†âˆ’1
obsğ‘¦, where
( Â· )â€  denotes the Moore-Penrose pseudoinverse [194, 216]. To highlight the differences
between the CGLS solution and (3.32), we assume that ğ’¦ğ‘Ÿ( Ì‚ï¸€ğ», ğ‘¦) â‰ˆran(ğ‘Šğ‘Ÿ) for all
ğ‘Ÿ, where ğ‘Šğ‘Ÿ= [ğ‘¤1 | Â· Â· Â· | ğ‘¤ğ‘Ÿ], ran(ğ´) denotes the range of a matrix ğ´, and Ì‚ï¸€ğ»=
âˆ‘ï¸€
ğ‘–ğ›¿2
ğ‘–ğ‘¤ğ‘–ğ‘¤âŠ¤
ğ‘–is an SVD of Ì‚ï¸€ğ». Notice that the condition ğ’¦ğ‘Ÿ( Ì‚ï¸€ğ», ğ‘¦) â‰ˆran(ğ‘Šğ‘Ÿ) is usually
quite reasonable for moderate values of ğ‘Ÿ. This practical observation is at the heart of
the Lanczos iteration for symmetric eigenvalue problems [159]. With simple algebraic
manipulations we conclude that:
ğ‘¥ğ‘Ÿâ‰ˆğ‘†pr(ğ‘†âˆ’1
obsğºğ‘†pr)â€ 
ğ‘Ÿğ‘†âˆ’1
obsğ‘¦.
(3.37)
Recall from (3.32) that the optimal rankâ€“ğ‘Ÿapproximation of the posterior mean
defined in Theorem 3.2 can be written as:
ğœ‡(ğ‘Ÿ)
pos(ğ‘Œ) = ğ‘†pr(ğ‘†âˆ’1
obsğºğ‘†pr)Tikh
ğ‘Ÿ
ğ‘†âˆ’1
obsğ‘Œ.
(3.38)
The only difference between (3.37) and (3.38) is the use of a Tikhonov-regularized in-
verse in (3.38) as opposed to a Moore-Penrose pseudoinverse. If ğ‘†âˆ’1
obsğºğ‘†pr = âˆ‘ï¸€
ğ‘–â‰¥1 ğ›¿ğ‘–ğ‘£ğ‘–ğ‘¤âŠ¤
ğ‘–
is a reduced SVD of the matrix ğ‘†âˆ’1
obsğºğ‘†pr, then:
(ğ‘†âˆ’1
obsğºğ‘†pr)â€ 
ğ‘Ÿ=
âˆ‘ï¸
ğ‘–â‰¤ğ‘Ÿ
1
ğ›¿ğ‘–
ğ‘¤ğ‘–ğ‘£âŠ¤
ğ‘–,
(ğ‘†âˆ’1
obsğºğ‘†pr)Tikh
ğ‘Ÿ
=
âˆ‘ï¸
ğ‘–â‰¤ğ‘Ÿ
ğ›¿ğ‘–
1 + ğ›¿2
ğ‘–
ğ‘¤ğ‘–ğ‘£âŠ¤
ğ‘–.
(3.39)
These two matrices are nearly identical for values of ğ‘Ÿcorresponding to ğ›¿2
ğ‘Ÿgreater than
one9 (assuming the ordering ğ›¿2
ğ‘–â‰¥ğ›¿2
ğ‘–+1). Beyond this regime, it might be convenient
to stop the CGLS solver to obtain (3.37) (i.e., early stopping regularization). The
similarity of these expressions is quite remarkable since (3.38) was derived as the
minimizer of the optimization problem (3.29) with ğ’œ= ğ’œğ‘Ÿ. This informal argument
may explain why priorconditioners perform so well in applications [46, 131].
Yet
9In Section 3.5 we show that by the time we start including generalized eigenvalues ğ›¿2
ğ‘–â‰ˆ1 in
(3.30), the approximation of the posterior mean is usually already satisfactory.
Intuitively, this
means that all the directions in parameter space where the data are more informative than the prior
have been considered.
53

we remark that the goals of Theorem 3.2 and [47] are still quite different; [47] is
concerned with preconditioning techniques for early stopping regularization of ill-
posed inverse problems, whereas Theorem 3.2 is concerned with statistically optimal
approximations of the posterior mean in the Bayesian framework.
Algorithm 1 Optimal low-rank approximation of the posterior mean
INPUT: forward and adjoint models ğº, ğºâŠ¤; prior and noise precisions Î“âˆ’1
pr , Î“âˆ’1
obs;
approximation order ğ‘ŸâˆˆN
OUTPUT: approximate posterior mean ğœ‡(ğ‘Ÿ)
pos(ğ‘Œ)
1: Find the ğ‘Ÿleading generalized eigenvalue-eigenvector pairs (ğ›¿2
ğ‘–, Ì‚ï¸€ğ‘¤ğ‘–) of the pencil
(ğºâŠ¤Î“âˆ’1
obsğº, Î“âˆ’1
pr )
2: Find the ğ‘Ÿleading generalized eigenvector pairs (Ì‚ï¸€ğ‘£ğ‘–) of the pencil (ğºÎ“prğºâŠ¤, Î“obs)
3: For each new realization of the data ğ‘Œ, compute ğœ‡(ğ‘Ÿ)
pos(ğ‘Œ) = âˆ‘ï¸€ğ‘Ÿ
ğ‘–=1 ğ›¿ğ‘–(1 + ğ›¿2
ğ‘–)âˆ’1 Ì‚ï¸€ğ‘¤ğ‘–Ì‚ï¸€ğ‘£âŠ¤
ğ‘–ğ‘Œ.
Algorithm 2 Optimal low-rank update approximation of the posterior mean
INPUT: forward and adjoint models ğº, ğºâŠ¤; prior and noise precisions Î“âˆ’1
pr , Î“âˆ’1
obs;
approximation order ğ‘ŸâˆˆN
OUTPUT: approximate posterior mean Ì‚ï¸€ğœ‡(ğ‘Ÿ)
pos(ğ‘Œ)
1: Obtain Ì‚ï¸€Î“pos as described in Theorem 3.1.
2: For each new realization of the data ğ‘Œ, compute Ì‚ï¸€ğœ‡(ğ‘Ÿ)
pos(ğ‘Œ) = Ì‚ï¸€Î“pos ğºâŠ¤Î“âˆ’1
obs ğ‘Œ.
3.5
Numerical examples
Now we provide several numerical examples to illustrate the theory developed in
the preceding sections. We start with a synthetic example to demonstrate various
posterior covariance matrix approximations, and continue with two more realistic
linear inverse problems where we also study posterior mean approximations.
3.5.1
Example 1: Hessian and prior with controlled spectra
We begin by investigating the approximation of Î“pos as a negative semidefinite update
of Î“pr. We compare the optimal approximation obtained in Theorem 3.1 with the
Hessian-, prior-, and BFGS-based reduction schemes discussed in Section 3.3.4. The
idea is to reveal differences between these approximations by exploring regimes where
54

the data have differing impacts on the prior information. Since the directions defining
the optimal update are the generalized eigenvectors of the pencil (ğ», Î“âˆ’1
pr ), we shall
also refer to this update as the generalized approximation.
To compare these approximation schemes, we start with a simple example with
diagonal Hessian and prior covariance matrices: ğº= ğ¼, Î“obs = diag{ğœ2
ğ‘–}, and Î“pr =
diag{ğœ†2
ğ‘–}. Since the forward operator ğºis the identity, this problem can (loosely) be
thought of as denoising a signal ğ‘‹. In this case, ğ»= Î“âˆ’1
obs and Î“pos = diag{ğœ†2
ğ‘–ğœ2
ğ‘–/(ğœ2
ğ‘–+
ğœ†2
ğ‘–)}. The ratios of posterior to prior variance in the canonical directions (ğ‘’ğ‘–) are
Var(ğ‘’âŠ¤
ğ‘–ğ‘‹| ğ‘Œ)
Var(ğ‘’âŠ¤
ğ‘–ğ‘‹)
=
1
1 + ğœ†2
ğ‘–/ğœ2
ğ‘–
.
We note that if the observation variances ğœ2
ğ‘–are constant, ğœğ‘–= ğœ, then the directions
of greatest variance reduction are those corresponding to the largest prior variance.
Hence the prior distribution alone determines the most informed directions, and the
prior-based reduction is as effective as the generalized one. On the other hand, if
the prior variances ğœ†2
ğ‘–are constant, ğœ†ğ‘–= ğœ†, but the ğœğ‘–vary, then the directions of
highest variance reduction are those corresponding to the smallest noise variance.
This time the noise distribution alone determines the most important directions, and
Hessian-based reduction is as effective as the generalized one. In the case of more
general spectra, the important directions depend on the ratios ğœ†2
ğ‘–/ğœ2
ğ‘–and thus one
has to use the information provided by both the noise and prior distributions. This
is done naturally by the generalized reduction.
We now generalize this simple case by moving to full matrices ğ»and Î“pr with
a variety of prescribed spectra. We assume that ğ»and Î“pr have SVDs of the form
ğ»= ğ‘ˆÎ›ğ‘ˆâŠ¤and Î“pr = ğ‘‰Ìƒï¸€Î›ğ‘‰âŠ¤, where Î› = diag{ğœ†1, . . . , ğœ†ğ‘›} and Ìƒï¸€Î› = diag{Ìƒï¸€ğœ†1, . . . , Ìƒï¸€ğœ†ğ‘›}
with
ğœ†ğ‘˜= ğœ†0/ğ‘˜ğ›¼+ ğœ
and
Ìƒï¸€ğœ†ğ‘˜= Ìƒï¸€ğœ†0/ğ‘˜Ìƒï¸€ğ›¼+ Ìƒï¸€ğœ.
To consider many different cases, the orthogonal matrices ğ‘ˆand ğ‘‰are randomly
and independently generated uniformly over the orthogonal group [264], leading to
different realizations of ğ»and Î“pr. In particular, ğ‘ˆand ğ‘‰are computed with a ğ‘„ğ‘…
55

decomposition of a square matrix of independent standard Gaussian entries using a
Gram-Schmidt orthogonalization. (In this case, the standard Householder reflections
cannot be used.)
Before discussing the results of the first experiment, we explain our implementa-
tion of BFGS-based reduction. We ran the BFGS optimizer with a dummy quadratic
optimization target ğ’¥(ğ‘¥) = 1
2 ğ‘¥âŠ¤Î“âˆ’1
posğ‘¥and used Î“pr as the initial approximation ma-
trix for Î“pos. Thus, the BFGS approximation of the posterior covariance matrix can
be written as Î“pos = Î“pr âˆ’ğ¾ğ¾âŠ¤for some rankâ€“ğ‘Ÿmatrix ğ¾. The rankâ€“ğ‘Ÿupdate is
constructed by running the BFGS optimizer for ğ‘Ÿsteps from random initial condi-
tions as shown in [11]. Note that in order to obtain results for sufficiently high-rank
updates, we use BFGS rather than L-BFGS in our numerical examples. While [10, 11]
in principle employ L-BFGS, the results in these papers use a number of optimization
steps roughly equal to the number of vectors stored in L-BFGS; our approach thus is
comparable to [10, 11]. Nonetheless, some results for the highest-rank BFGS updates
are not plotted in Figures 3-1 and 3-2, as the optimizer converged so close to the
optimum that taking further steps resulted in numerical instabilities.
Figure 3-1 summarizes the results of the first experiment. The top row shows
the prescribed spectra of ğ»âˆ’1 (red) and Î“pr (blue). The parameters describing the
eigenvalues of Î“pr are fixed to Ìƒï¸€ğœ†0 = 1, Ìƒï¸€ğ›¼= 2, and Ìƒï¸€ğœ= 10âˆ’6. The corresponding
parameters for ğ»are given by ğœ†0 = 500 and ğœ= 10âˆ’6 with ğ›¼= 0.345 (left), ğ›¼= 0.690
(middle), and ğ›¼= 1.724 (right). Thus, moving from the leftmost column to the
rightmost column, the data become increasingly less informative. The second row
in the figure shows the Riemannian distance ğ‘‘â„›between Î“pos and its approximation,
Ì‚ï¸€Î“pos = Î“prâˆ’ğ¾ğ¾âŠ¤, as a function of the rank of ğ¾ğ¾âŠ¤for 100 different realizations of ğ»
and Î“pr. The third row shows, for each realization of (ğ», Î“pr) and for each fixed rank of
ğ¾ğ¾âŠ¤, the difference between the distance obtained with a prior-, Hessian-, or BFGS-
based dimensionality reduction technique and the minimum distance obtained with
the generalized approximation. All of these differences are positiveâ€”a confirmation
of Theorem 3.1. But Figure 3-1 also shows interesting patterns consistent with the
observations made for the simple example above. When the spectrum of ğ»is basically
56

flat (left column), the directions along which the prior variance is reduced the most
are likely to be those corresponding to the largest prior variances, and thus a prior-
based reduction is almost as effective as the generalized one (as seen in the bottom
two rows on the left). As we move to the third column, eigenvalues of ğ»âˆ’1 increase
more quickly. The data provide significant information only on a lower-dimensional
subspace of the parameter space. In this case, it is crucial to combine this information
with the directions in the parameter space along which the prior variance is the
greatest. The generalized reduction technique successfully accomplishes this task,
whereas the prior and Hessian reductions fail as they focus either on Î“pr or ğ»alone;
the key is to combine the two. The BFGS update performs remarkably well across
all three configurations of the Hessian spectrum, although it is clearly suboptimal
compared to the generalized reduction.
In Figure 3-2 the situation is reversed and the results are symmetric to those of
Figure 3-1. The spectrum of ğ»(red) is now kept fixed with parameters ğœ†0 = 500,
ğ›¼= 1, and ğœ= 10âˆ’9, while the spectrum of Î“pr (blue) has parameters Ìƒï¸€ğœ†0 = 1 and
Ìƒï¸€ğœ= 10âˆ’9 with decay rates increasing from left to right: Ìƒï¸€ğ›¼= 0.552 (left), Ìƒï¸€ğ›¼= 1.103
(middle), and Ìƒï¸€ğ›¼= 2.759 (right). In the first column, the spectrum of the prior is
nearly flat. That is, the prior variance is almost equally spread along every direction in
the parameter space. In this case, the eigenstructure of ğ»determines the directions of
greatest variance reduction, and the Hessian-based reduction is almost as effective as
the generalized one. As we move towards the third column, the spectrum of Î“pr decays
more quickly. The prior variance is restricted to lower-dimensional subspaces of the
parameter space. Mismatch between prior- and Hessian-dominated directions then
leads to poor performance of both the prior- and Hessian-based reduction techniques.
However, the generalized reduction performs well also in this more challenging case.
The BFGS reduction is again empirically quite effective in most of the configurations
that we consider. It is not always better than the prior- or Hessian-based techniques
when the update rank is low, or when the prior spectrum decays slowly; for example,
Hessian-based reduction is more accurate than BFGS across all ranks in the first
column of Figure 3-2. But when either the prior covariance or the Hessian have quickly
57

decaying spectra, the BFGS approach performs almost as well as the generalized
reduction. Though this approach remains suboptimal, its approximation properties
deserve further theoretical study.
0
50
100
10
âˆ’4
10
âˆ’2
10
0
eigenvalue
index i
Î“ p r
H âˆ’1
0
50
100
10
âˆ’2
10
âˆ’1
10
0
10
1
d F
pr i o r
hes s i a n
B FG S
g ener a l i zed
20
40
60
80
10
âˆ’2
10
0
rank of update
diï¬€erence in d F
pr i o r - g ener a l i zed
hes s i a n - g ener a l i zed
B FG S - g ener a l i zed
0
50
100
10
âˆ’4
10
âˆ’2
10
0
index i
20
40
60
80
10
âˆ’2
10
0
20
40
60
80
10
âˆ’2
10
0
rank of update
0
50
100
10
âˆ’4
10
âˆ’2
10
0
index i
20
40
60
80
10
0
20
40
60
80
10
âˆ’2
10
0
rank of update
Figure 3-1: Top row: Eigenspectra of Î“pr (blue) and ğ»âˆ’1 (red) for three values for the
decay rate of the eigenvalues of ğ»: ğ›¼= 0.345 (left), ğ›¼= 0.690 (middle) and ğ›¼= 1.724
(right). Second row: Riemannian distance ğ‘‘â„›between Î“pos and its approximation
versus the rank of the update for 100 realizations of Î“pr and ğ»using prior-based
(blue), Hessian-based (green), BFGS-based (magenta) and optimal (red) updates.
Bottom row: Differences of posterior covariance approximation error (measured with
ğ‘‘â„›) between the prior-based and optimal updates (blue), between the Hessian-based
and optimal updates (green), and between the BFGS-based and optimal updates
(magenta).
3.5.2
Example 2: X-ray tomography
We consider a classical inverse problem of X-ray computed tomography (CT), where
X-rays travel from sources to detectors through an object of interest. The intensities
from multiple sources are measured at the detectors, the goal is to reconstruct the
density of the object. In this framework, we investigate the performance of the optimal
58

0
50
100
10
âˆ’6
10
âˆ’4
10
âˆ’2
10
0
eigenvalue
index i
Î“ p r
H âˆ’1
20
40
60
80
100
10
0
10
1
d F
pr i o r
hes s i a n
B FG S
g ener a l i zed
20
40
60
80
10
âˆ’2
10
0
rank of update
diï¬€erence in d F
pr i o r - g ener a l i zed
hes s i a n - g ener a l i zed
B FG S - g ener a l i zed
0
50
100
10
âˆ’6
10
âˆ’4
10
âˆ’2
10
0
index i
20
40
60
80
100
10
âˆ’1
10
0
20
40
60
80
10
âˆ’2
10
0
rank of update
0
50
100
10
âˆ’6
10
âˆ’4
10
âˆ’2
10
0
index i
20
40
60
80
10
0
20
40
60
80
10
âˆ’2
10
0
rank of update
Figure 3-2: Analogous to Figure 3-1 but this time the spectrum of ğ»is fixed, while
that of Î“pr has varying decay rates: Ìƒï¸€ğ›¼= 0.552 (left), Ìƒï¸€ğ›¼= 1.103 (middle) and Ìƒï¸€ğ›¼=
2.759 (right).
59

mean and covariance matrix approximations presented in Sections 3.2 and 3.4. This
synthetic example is motivated by a real application: real-time X-ray imaging of logs
that enter a saw mill for the purpose of automatic quality control. For instance, in the
system commercialized by Bintec (www.bintec.fi), logs enter the X-ray system on fast-
moving conveyer belt and fast reconstructions are needed. The imaging setting (e.g.,
X-ray source and detector locations) and the priors are fixed; only the data changes
from one log cross-section to another. The basis for our posterior mean approximation
can therefore be pre-computed, and repeated inversions can be carried out quickly
with direct matrix formulas.
We model the absorption of an X-ray along a line, â„“ğ‘–, using Beerâ€™s law:
ğ¼ğ‘‘= ğ¼ğ‘ exp
(ï¸‚
âˆ’
âˆ«ï¸
â„“ğ‘–
ğ‘¥(ğ‘ )ğ‘‘ğ‘ 
)ï¸‚
,
(3.40)
where ğ¼ğ‘‘and ğ¼ğ‘ are the intensities at the detector and at the source, respectively,
and ğ‘¥(ğ‘ ) is the density of the object at position ğ‘ on the line â„“ğ‘–. The computational
domain is discretized into a grid and the density is assumed to be constant within
each grid cell. The line integrals are approximated as
âˆ«ï¸
â„“ğ‘–
ğ‘¥(ğ‘ )ğ‘‘ğ‘ â‰ˆ
# of cells
âˆ‘ï¸
ğ‘—=1
ğ‘”ğ‘–ğ‘—ğ‘¥ğ‘—,
(3.41)
where ğ‘”ğ‘–ğ‘—is the length of the intersection between line â„“ğ‘–and cell ğ‘—, and ğ‘¥ğ‘—is the
unknown density in cell ğ‘—.
The vector of absorptions along ğ‘šlines can then be
approximated as
ğ¼ğ‘‘â‰ˆğ¼ğ‘ exp (âˆ’ğºğ‘¥) ,
(3.42)
where ğ¼ğ‘‘is the vector of ğ‘šintensities at the detectors and ğº= (ğ‘”ğ‘–ğ‘—) is the ğ‘šÃ— ğ‘›
matrix of intersection lengths for each of the ğ‘šlines.
Even though the forward
operator (3.42) is nonlinear, the inference problem can be recast in a linear fashion
by taking logarithm of both sides of (3.42). This leads to the following linear model
for the inversion: ğ‘Œ= ğºğ‘‹+ â„°, where the measurement vector is ğ‘Œ= âˆ’log(ğ¼ğ‘‘/ğ¼ğ‘ )
and the measurement errors are assumed to be iid Gaussian, â„°âˆ¼ğ’©(0, ğœ2 I).
60

The setup for the inference problem, borrowed from [?
], is as follows.
The
rectangular domain is discretized with an ğ‘›Ã—ğ‘›grid. The true object consists of three
circular inclusions, each of uniform density, inside an annulus. Ten X-ray sources
are positioned on one side of a circle, and each source sends a fan of 100 X-rays
that are measured by detectors on the opposite side of the object.
Here, the 10
sources are distributed evenly so that they form a total illumination angle of 90
degrees, resulting in a limited-angle CT problem.
We use the exponential model
(3.40) to generate synthetic data in a discretization-independent fashion by computing
the exact intersections between the rays and the circular inclusions in the domain.
Gaussian noise with standard deviation ğœ= 0.002 is added to the simulated data.
The imaging setup and data from one source are illustrated in Figure 3-3.
The unknown density is estimated on a 128Ã—128 grid. Thus the discretized vector,
ğ‘‹, has length 16384, and direct computation of the posterior mean and the posterior
covariance matrix, as well as generation of posterior samples, can be computationally
nontrivial. To define the prior distribution, ğ‘‹is modeled as a discretized solution of
a stochastic PDE of the form:
ğ›¾
(ï¸€
ğœ…2â„âˆ’â–³
)ï¸€
ğ‘¥(ğ‘ ) = ğ’²(ğ‘ ),
ğ‘ âˆˆÎ©,
(3.43)
where ğ’²is a white noise process, â–³is the Laplacian operator, and â„is the identity
operator. The solution of (3.43) is a Gaussian random field whose correlation length
and variance are controlled by the free parameters ğœ…and ğ›¾, respectively. A square
root of the prior precision matrix of ğ‘‹(which is positive definite) can then be easily
computed (see [174] for details). We use ğœ…= 10 and ğ›¾=
âˆš
800 in our simulations.
Our first task is to compute an optimal approximation of Î“pos as a low-rank
negative update of Î“pr (cf. Theorem 3.1). Figure 3-4 (top row) shows the convergence
of the approximate posterior variance as the rank of the update increases. The zero-
rank update corresponds to Î“pr (first column). For this formally 16384-dimensional
problem, a good approximation of the posterior variance is achieved with a rank 200
update; hence the data are informative only on a low-dimensional subspace. The
61

quality of the covariance matrix approximation is also reflected in the structure of
samples drawn from the approximate posterior distributions (bottom row). All five of
these samples are drawn using the same random seed and the exact posterior mean,
so that all the differences observed are due to the approximation of Î“pos. Already
with a rank 100 update, the small-scale features of the approximate posterior sample
match those of the exact posterior sample. In applications, agreement in this â€œeye-ball
normâ€ is important. Of course, Theorem 3.1 also provides an exact formula for the
error in the posterior covariance; this error is shown in the right panel of Figure 3-7
(blue curve).
0
20
40
60
80
100
0.9
0.92
0.94
0.96
0.98
1
1.02
detector pixel
intensity
Figure 3-3:
X-ray tomography problem.
Left:
Discretized domain, true object,
sources (red dots), and detectors corresponding to one source (black dots).
The
fan transmitted by one source is illustrated in gray. The density of the object is 0.006
in the outer ring and 0.004 in the three inclusions; the background density is zero.
Right: The true simulated intensity (black line) and noisy measurements (red dots)
for one source.
Our second task is to assess the performances of the two optimal posterior mean
approximations given in Section 3.4. We will use ğœ‡(ğ‘Ÿ)
pos(ğ‘Œ) to denote the low-rank
approximation and Ì‚ï¸€ğœ‡(ğ‘Ÿ)
pos(ğ‘Œ) to denote the low-rank update approximation. Recall that
both approximations are linear functions of the data ğ‘Œ, given by ğœ‡(ğ‘Ÿ)
pos(ğ‘Œ) = ğ´*ğ‘Œ
with ğ´* âˆˆğ’œğ‘Ÿand Ì‚ï¸€ğœ‡(ğ‘Ÿ)
pos(ğ‘Œ) = Ì‚ï¸€ğ´*ğ‘Œwith Ì‚ï¸€ğ´* âˆˆÌ‚ï¸€
ğ’œğ‘Ÿ, where the classes ğ’œğ‘Ÿand Ì‚ï¸€
ğ’œğ‘Ÿare
defined in (3.28). As in Section 3.4, we shall use ğ’œto denote either of the two classes.
Figure 3-5 shows the normalized error â€–ğœ‡(ğ‘Œ)âˆ’ğœ‡pos(ğ‘Œ)â€–Î“âˆ’1
pos/â€–ğœ‡pos(ğ‘Œ)â€–Î“âˆ’1
pos for dif-
ferent approximations ğœ‡(ğ‘Œ) of the true posterior mean ğœ‡pos(ğ‘Œ) and a fixed realization
62

prior
âˆ’5.8
âˆ’5.6
âˆ’5.4
rank = 50
rank = 100
rank = 200
posterior
âˆ’7
âˆ’6.8
âˆ’6.6
âˆ’6.4
âˆ’6.2
Figure 3-4: X-ray tomography problem. First column: Prior variance field, in log scale
(top), and a sample drawn from the prior distribution (bottom). Second through last
columns (left to right): Variance field, in log scale, of the approximate posterior as
the rank of the update increases (top); samples from the corresponding approximate
posterior distributions (bottom) assuming exact knowledge of the posterior mean.
{ğ‘Œ= ğ‘¦} of the data. The error is a function of the order ğ‘Ÿof the approximation
class ğ’œ.
Snapshots of ğœ‡(ğ‘¦) are shown along the two error curves.
For reference,
ğœ‡pos(ğ‘¦) is also shown at the top. We see that the errors decrease monotonically, but
that the low-rank approximation outperforms the low-rank update approximation for
lower values of ğ‘Ÿ. This is consistent with the discussion at the end of Section 3.4; the
crossing point of the error curves is also consistent with that analysis. In particular,
we expect the low-rank update approximation to outperform the low-rank approxi-
mation only when the approximation starts to include generalized eigenvalues of the
pencil (ğ», Î“âˆ’1
pr ) that are less than oneâ€”i.e., once the approximations are no longer
under-resolved. This can be confirmed by comparing Figure 3-5 with the decay of the
generalized eigenvalues of the pencil (ğ», Î“âˆ’1
pr ) in the right panel of Figure 3-7 (blue
curve).
On top of each snapshot in Figure 3-5, we show the relative CPU time required
to compute the corresponding posterior mean approximation for each new realization
of the data. The relative CPU time is defined as the time required to compute this
approximation10 divided by the time required to apply the posterior precision matrix
10This timing does not include the computation of (3.30) or (3.33), which should be regarded as
offline steps. Here we report the time necessary to apply the optimal linear function to any new
realization of the data, i.e., the online cost.
63

to a vector. This latter operation is essential to computing the posterior mean via
an iterative solver, such as a Krylov subspace method. These solvers are a standard
choice for computing the posterior mean in large-scale inverse problems. Evaluating
the ratio allows us to determine how many solver iterations could be performed with
a computational cost roughly equal to that of approximating the posterior mean
for a new realization of the data. Based on the reported times, a few observations
can be made. First of all, as anticipated in Section 3.4, computing ğœ‡(ğ‘Ÿ)
pos(ğ‘Œ) for any
new realization of the data is faster than computing Ì‚ï¸€ğœ‡(ğ‘Ÿ)
pos(ğ‘Œ). Second, obtaining an
accurate posterior mean approximation requires roughly ğ‘Ÿ= 200, and the relative
CPU times for this order of approximation are 7.3 for ğœ‡(ğ‘Ÿ)
pos(ğ‘Œ) and 29.0 for Ì‚ï¸€ğœ‡(ğ‘Ÿ)
pos(ğ‘Œ).
These are roughly the number of iterations of an iterative solver that one could
take for equivalent computational cost. That is, the speedup of the posterior mean
approximation compared to an iterative solver is not particularly dramatic in this
case, because the forward model ğ´is simply a sparse matrix that is cheap to apply.
This is different for the heat equation example discussed in Section 3.5.3.
Note that the above computational time estimates exclude other costs associated
with iterative solvers. For instance, preconditioners are often applied; these signifi-
cantly decrease the number of iterations needed for the solvers to converge but, on
the other hand, increase the cost per iteration. A popular approach for solving the
posterior mean efficiently is to use the prior covariance as the preconditioner [40].
In the limited-angle tomography problem, including the application of this precondi-
tioner in the reference CPU time would reduce the relative CPU time of our ğ‘Ÿ= 200
approximations to 0.48 for ğœ‡(ğ‘Ÿ)
pos(ğ‘Œ) and 1.9 for Ì‚ï¸€ğœ‡(ğ‘Ÿ)
pos(ğ‘Œ). That is, the cost of com-
puting our approximations is roughly equal to one iteration of a prior-preconditioned
iterative solver. The large difference compared to the case without preconditioning
is due to the fact that the cost of applying the prior here is computationally much
higher than applying the forward model.
64

Figure 3-6 (left panel) shows unnormalized errors in the approximation of ğœ‡pos(ğ‘¦),
â€–ğ‘’(ğ‘¦)â€–2
Î“âˆ’1
pos = â€–ğœ‡(ğ‘Ÿ)
pos(ğ‘¦) âˆ’ğœ‡pos(ğ‘¦)â€–2
Î“âˆ’1
pos
and
â€–Ì‚ï¸€ğ‘’(ğ‘¦)â€–2
Î“âˆ’1
pos = â€–Ì‚ï¸€ğœ‡(ğ‘Ÿ)
pos(ğ‘¦) âˆ’ğœ‡pos(ğ‘¦)â€–2
Î“âˆ’1
pos,
(3.44)
for the same realization of ğ‘Œused in Figure 3-5. In the same panel we also show the
expected values of these errors over the prior predictive distribution of ğ‘Œ, which is
exactly the ğ‘Ÿ-dependent component of the Bayes risk given in Theorems 4.2 and 3.3.
Both sets of errors decay with increasing ğ‘Ÿand show a similar crossover between the
two approximation classes. But the particular error â€–ğ‘’(ğ‘¦)â€–2
Î“âˆ’1
pos departs consistently
from its expectation; this is not unreasonable in general (the mean estimator has a
nonzero variance), but the offset may be accentuated in this case because the data
are generated from an image that is not drawn from the prior. (The right panel of
Figure 3-6, which comes from Example 3, represents a contrasting case.)
By design, the posterior approximations described in this chapter perform well
when the data inform a low-dimensional subspace of the parameter space. To better
understand this effect, we also consider a full-angle configuration of the tomography
problem, wherein the sources and detectors are evenly spread around the entire un-
known object. In this case, the data are more informative than in the limited-angle
configuration. This can be seen in the decay rate of the generalized eigenvalues of the
pencil (ğ», Î“âˆ’1
pr ) in the center panel of Figure 3-7 (blue and red curves); eigenvalues
for the full-angle configuration decay more slowly than for the limited-angle configu-
ration. Thus, according to the optimal loss given in (3.12) (Theorem 3.1), the prior-
to-posterior update in the full-angle case must be of greater rank than the update in
the limited-angle case for any given approximation error. Also, good approximation
of ğœ‡pos(ğ‘Œ) in the full-angle case requires higher order of the approximation class ğ’œ,
as is shown in Figure 3-8. But because the data are strongly informative, they allow
an almost perfect reconstruction of the underlying truth image. The relative CPU
times are similar to the limited angle case: roughly 8 for ğœ‡(ğ‘Ÿ)
pos(ğ‘Œ) and 14 for Ì‚ï¸€ğœ‡(ğ‘Ÿ)
pos(ğ‘Œ).
If preconditioning with the prior covariance is included in the reference CPU time
calculation, the relative CPU times drop to 1.5 for ğœ‡(ğ‘Ÿ)
pos(ğ‘Œ) and to 2.6 for Ì‚ï¸€ğœ‡(ğ‘Ÿ)
pos(ğ‘Œ).
65

We remark that in realistic applications of X-ray tomography, the limited angle setup
is extremely common as it is cheaper and more flexible (yielding smaller and lighter
devices) than a full-angle configuration.
0
50
100
150
200
250
300
350
400
10
âˆ’4
10
âˆ’2
10
0
10
2
10
4
normalized error
order of approximating class
19.0860
1.9139
22.0409
3.8533
28.9616
7.3060
34.9874
11.5087
Âµ p os
Figure 3-5: Limited-angle X-ray tomography: Comparison of the optimal posterior
mean approximations, ğœ‡(ğ‘Ÿ)
pos(ğ‘Œ) (blue) and Ì‚ï¸€ğœ‡(ğ‘Ÿ)
pos(ğ‘Œ) (black) of ğœ‡pos(ğ‘Œ) for a fixed
realization of the data {ğ‘Œ= ğ‘¦}, as a function of the order ğ‘Ÿof the approximating
classes ğ’œğ‘Ÿand Ì‚ï¸€
ğ’œğ‘Ÿ, respectively. The normalized error for an approximation ğœ‡(ğ‘¦)
is defined as â€–ğœ‡(ğ‘¦) âˆ’ğœ‡pos(ğ‘¦)â€–Î“âˆ’1
pos / â€–ğœ‡pos(ğ‘¦)â€–Î“âˆ’1
pos. The numbers above or below the
snapshots indicate the relative CPU time of the corresponding mean approximationâ€”
i.e., the time required to compute the approximation divided by the time required to
apply the posterior precision matrix to a vector.
3.5.3
Example 3: Heat equation
Our last example is the classic linear inverse problem of solving for the initial con-
ditions of an inhomogeneous heat equation. Let ğ‘¢(ğ‘ , ğ‘¡) be the time dependent state
of the heat equation on ğ‘ = (ğ‘ 1, ğ‘ 2) âˆˆÎ© = [0, 1]2, ğ‘¡â‰¥0, and let ğœ…(ğ‘ ) be the heat
conductivity field. Given initial conditions, ğ‘¢0(ğ‘ ) = ğ‘¢(ğ‘ , 0), the state evolves in time
66

50
100
150
200
250
300
350
400
10
0
10
2
10
4
10
6
10
8
10
10
10
12
order of approximating class
âˆ¥e(y)âˆ¥2
Î“âˆ’1
p o s
âˆ¥be(y)âˆ¥2
Î“âˆ’1
p o s
Ey
h
âˆ¥e(y)âˆ¥2
Î“âˆ’1
p o s
i
Ey
h
âˆ¥be (y)âˆ¥2
Î“âˆ’1
p o s
i
50
100
150
200
250
300
10
âˆ’5
10
0
10
5
10
10
10
15
10
20
order of approximating class
âˆ¥e(y)âˆ¥2
Î“âˆ’1
p o s
âˆ¥be(y)âˆ¥2
Î“âˆ’1
p o s
Ey
h
âˆ¥e(y)âˆ¥2
Î“âˆ’1
p o s
i
Ey
h
âˆ¥be (y)âˆ¥2
Î“âˆ’1
p o s
i
Figure 3-6: The errors â€–ğ‘’(ğ‘¦)â€–2
Î“âˆ’1
pos (blue) and â€–Ì‚ï¸€ğ‘’(ğ‘¦)â€–2
Î“âˆ’1
pos (black) defined by (3.44), and
their expected values in green and red, respectively; for Example 2 (left panel) and
Example 3 (right panel).
index i
0
100
200
300
400
500
eigenvalues
10-8
10-7
10-6
10-5
10-4
10-3
prior
limited angle
full angle
index i
100
200
300
400
500
generalized eigenvalues
10-2
10-1
100
101
102
103
104
105
limited angle
full angle
rank of update
100
200
300
400
500
dF
100
101
102
103
limited angle
full angle
Figure 3-7:
Left: Leading eigenvalues of Î“pr and ğ»âˆ’1 in the limited-angle and
full-angle X-ray tomography problems. Center: Leading generalized eigenvalues of
the pencil (ğ», Î“âˆ’1
pr ) in the limited-angle (blue) and full-angle (red) cases.
Right:
ğ‘‘â„±(Î“pos, Ì‚ï¸€Î“pos) as a function of the rank of the update ğ¾ğ¾âŠ¤, with Ì‚ï¸€Î“pos = Î“pr âˆ’ğ¾ğ¾âŠ¤,
in the limited-angle (blue) and full-angle (red) cases.
67

0
100
200
300
400
500
600
700
800
10
âˆ’5
10
âˆ’3
10
âˆ’1
10
1
10
3
10
5
normalized error
order of approximating class
6.8947
1.1752
10.1510
4.3366
14.0623
8.0357
18.2506
11.2724
Âµ p os
Figure 3-8: Same as Figure 3-5, but for full-angle X-ray tomography (sources and
receivers spread uniformly around the entire object).
according to the linear heat equation:
ğœ•ğ‘¢(ğ‘ , ğ‘¡)
ğœ•ğ‘¡
=
âˆ’âˆ‡Â· (ğœ…(ğ‘ )âˆ‡ğ‘¢(ğ‘ , ğ‘¡)),
ğ‘ âˆˆÎ©, ğ‘¡> 0,
ğœ…(ğ‘ )âˆ‡ğ‘¢(ğ‘ , ğ‘¡) Â· ğ‘›(ğ‘ )
=
0,
ğ‘ âˆˆğœ•Î©, ğ‘¡> 0,
(3.45)
where ğ‘›(ğ‘ ) denotes the outward-pointing unit normal at ğ‘ âˆˆğœ•Î©. We place ğ‘›ğ‘ = 81
sensors at the locations ğ‘ 1, . . . , ğ‘ ğ‘›ğ‘ , uniformly spaced within the lower left quadrant
of the spatial domain, as illustrated by the black dots in Figure 3-9. We use a finite-
dimensional discretization of the parameter space based on the finite element method
on a regular 100Ã—100 grid, {ğ‘ â€²
ğ‘–}. Our goal is to infer the vector ğ‘‹= (ğ‘¢0(ğ‘ â€²
ğ‘–)) of initial
conditions on the grid. Thus, the dimension of the parameter space for the inference
problem is ğ‘›= 104. We use data measured at 50 discrete times ğ‘¡= ğ‘¡1, ğ‘¡2, . . . , ğ‘¡50,
where ğ‘¡ğ‘–= ğ‘–â–³ğ‘¡, and â–³ğ‘¡= 2 Ã— 10âˆ’4. At each time ğ‘¡ğ‘–, pointwise observations of the
state ğ‘¢are taken at these sensors, i.e.,
ğ‘‘ğ‘–= ğ’ğ‘¢(ğ‘ , ğ‘¡ğ‘–),
(3.46)
68

where ğ’is the observation operator that maps the function ğ‘¢(ğ‘ , ğ‘¡ğ‘–) to
ğ‘‘= (ğ‘¢(ğ‘ 1, ğ‘¡ğ‘–), . . . , ğ‘¢(ğ‘ ğ‘›, ğ‘¡ğ‘–))âŠ¤.
(3.47)
The vector of observations is then ğ‘‘= [ğ‘‘1; ğ‘‘2; . . . ; ğ‘‘50]. The noisy data vector is
ğ‘Œ= ğ‘‘+ â„°, where â„°âˆ¼ğ’©(0, ğœ2ğ¼) and ğœ= 10âˆ’2. Note that the data are a linear
function of the initial conditions, perturbed by Gaussian noise. Thus the data can be
written as:
ğ‘Œ= ğºğ‘‹+ â„°,
â„°âˆ¼ğ’©(0, ğœ2ğ¼).
(3.48)
where ğºis a linear map defined by the composition of the forward model (3.45) with
the observation operator (3.46), both linear.
We generate synthetic data by evolving the initial conditions shown in Figure
3-9.
This â€œtrueâ€ value of the inversion parameters ğ‘‹is a discretized realization
of a Gaussian process satisfying an SPDE of the same form used in the previous
tomography example, but now with a non-stationary permeability field.
In other
words, the truth is a draw from the prior in this example (unlike in the previous
example), and the prior Gaussian process satisfies the following SPDE:
ğ›¾
(ï¸€
ğœ…2â„âˆ’âˆ‡Â· c(ğ‘ )âˆ‡
)ï¸€
ğ‘¥(ğ‘ ) = ğ’²(ğ‘ )
ğ‘ âˆˆÎ©,
(3.49)
where c(ğ‘ ) is the space-dependent permeability tensor.
Figure 3-10 and the right panel in Figure 3-6 show our numerical results. They
have the same interpretations as Figures 3-5 and 3-6 in the tomography example. The
trends in the figures are consistent with those encountered in the previous example
and confirm the good performance of the optimal low-rank approximation. Notice
that in Figures 3-10 and 3-6 the approximation of the posterior mean appears to be
nearly perfect (visually) once the error curves for the two approximations cross. This
is somewhat expected from the theory since we know that the crossing point should
occur when the approximations start to use eigenvalues of the pencil (ğ», Î“âˆ’1
pr ) that
are less than oneâ€”that is, once we have exhausted directions in the parameter space
69

where the data are more constraining than the prior.
Again, we report the relative CPU time for each posterior mean approximation
above/below the corresponding snapshot in Figure 3-10. The results differ signifi-
cantly from the tomography example. For instance, at order ğ‘Ÿ= 200, which yields
approximations that are visually indistinguishable from the true mean, the relative
CPU times are 0.001 for ğœ‡(ğ‘Ÿ)
pos(ğ‘Œ) and 0.53 for Ì‚ï¸€ğœ‡(ğ‘Ÿ)
pos(ğ‘Œ). Therefore we can compute
an accurate mean approximation for a new realization of the data much more quickly
than taking one iteration of an iterative solver. Recall that, consistent with the setting
described at the start of Section 3.4, this is a comparison of online times, after the
matrices (3.30) or (3.33) have been precomputed. The difference between this case
and tomography example of Section 3.5.2 is due to the higher CPU cost of apply-
ing the forward and adjoint models for the heat equationâ€”solving a time dependent
PDE versus applying a sparse matrix. Also, because the cost of applying the prior
covariance is negligible compared to that of the forward and adjoint solves in this
example, preconditioning the iterative solver with the prior would not strongly affect
the reported relative CPU times, unlike the tomography example.
Figure 3-11 illustrates some important directions characterizing the heat equation
inverse problem. The first two columns show the four leading eigenvectors of, re-
spectively, Î“pr and ğ». Notice that the support of the eigenvectors of ğ»concentrates
around the sensors. The third column shows the four leading directions ( Ì‚ï¸€ğ‘¤ğ‘–) defined
in Theorem 3.1.
These directions define the optimal prior-to-posterior covariance
matrix update (cf. (3.11)). This update of Î“pr is necessary to capture directions ( Ìƒï¸€ğ‘¤ğ‘–)
of greatest relative difference between prior and posterior variance (cf. Corollary 3.1).
The four leading directions ( Ìƒï¸€ğ‘¤ğ‘–) are shown in the fourth column. The support of
these modes is again concentrated around the sensors, which intuitively makes sense
as these are directions of greatest variance reduction.
70

conductivity ï¬eld
0
0.5
1
0
0.5
1
t = 0
0
0.5
1
0
0.5
1
t = 10â–³t
0
0.5
1
0
0.5
1
t = 20â–³t
0
0.5
1
0
0.5
1
t = 30â–³t
0
0.5
1
0
0.5
1
t = 40â–³t
0
0.5
1
0
0.5
1
t = 50â–³t
0
0.5
1
0
0.5
1
Figure 3-9: Heat equation problem. Initial condition (top left) and several snapshots
of the states at different times. Black dots indicate sensor locations.
0
50
100
150
200
250
300
350
400
10
âˆ’10
10
âˆ’5
10
0
10
5
normalized error
order of approximating class
0.5178
0.0014
0.5189
0.0040
0.5252
0.0098
0.5324
0.0150
Âµ p os
Figure 3-10: Same as Figure 3-5, but for Example 3.
71

prior
Hessian
bw i
ew i
Figure 3-11: Heat equation problem.
First column: Four leading eigenvectors of
Î“pr. Second column: Four leading eigenvectors of ğ». Third column: Four leading
directions ( Ì‚ï¸€ğ‘¤ğ‘–) (cf. (3.11)). Fourth column: Four leading directions ( Ìƒï¸€ğ‘¤ğ‘–) (cf. Corollary
3.1)
3.6
Discussion
This chapter has presented and characterized optimal approximations of the Bayesian
solution of linear inverse problems, with Gaussian prior and noise distributions defined
on finite-dimensional spaces. In a typical large-scale inverse problem, observations
may be informativeâ€”relative to the priorâ€”only on a low-dimensional subspace of the
parameter space. Our approximations therefore identify and exploit low-dimensional
structure in the update from prior to posterior.
We have developed two types of optimality results.
In the first, the posterior
covariance matrix is approximated as a low-rank negative semidefinite update of the
prior covariance matrix. We describe an update of this form that is optimal with re-
spect to a broad class of loss functions between covariance matrices, exemplified by the
natural geodesic distance on the manifold of symmetric positive definite matrices [99].
We argue that this is the appropriate class of loss functions with which to evaluate
approximations of the posterior covariance matrix, and show that optimality in such
metrics identifies directions in parameter space along which the posterior variance is
72

reduced the most, relative to the prior. Optimal low-rank updates are derived from
a generalized eigendecomposition of the pencil defined by the negative log-likelihood
Hessian and the prior precision matrix. These updates have been proposed in previous
work [96], but our work complements these efforts by characterizing the optimality
of the resulting approximations. Under the assumption of exact knowledge of the
posterior mean, our results extend to optimality statements between the associated
distributions (e.g., optimality in the Hellinger distance and in the Kullback-Leibler
divergence). Second, we have developed fast approximations of the posterior mean
that are useful when repeated evaluations thereof are required for multiple realizations
of the data (e.g., in an online inference setting). These approximations are optimal
in the sense that they minimize the Bayes risk for squared-error loss induced by the
posterior precision matrix. The most computationally efficient of these approxima-
tions expresses the posterior mean as the product of a single low-rank matrix with
the data. We have demonstrated the covariance and mean approximations numeri-
cally on a variety of inverse problems: synthetic problems constructed from random
Hessian and prior covariance matrices; an X-ray tomography problem with different
observation scenarios; and inversion for the initial condition of a heat equation, with
localized observations and a non-stationary prior.
The material in this chapter has several possible extensions of interest. First, it is
natural to generalize the present approach to infinite-dimensional parameter spaces
endowed with Gaussian priors. This setting is essential to understanding and for-
malizing Bayesian inference over function spaces [43, 265]. Here, by analogy with
the current results, one would expect the posterior covariance operator to be well
approximated by a finite-rank negative perturbation of the prior covariance opera-
tor. A further extension could allow the data to become infinite-dimensional as well.
Another important task is to generalize the present methodology to inverse problems
with nonlinear forward models. One approach for doing so is presented in [74] and will
not be described in this thesis; other approaches are certainly possible. See [282] for
a recent contribution that generalizes the approximations presented in this chapter to
the nonlinear case by means of logarithmic Sobolev inequalities. See also Chapter 5
73

for a related discussion on low-rank couplings. Yet another interesting research topic
is the study of analogous approximation techniques for sequential inference. We note
that the assimilation step in a linear (or linearized) data assimilation scheme can be
already tackled within the framework presented here. But the nonstationary setting,
where inference is interleaved with evolution of the state, should introduce the possi-
bility for even more tailored and structure-exploiting approximations. Lastly, we note
that an important extension of this work is to account for ultimate goals in inference
the task. We address this topic in the forthcoming Chapter 4.
74

Chapter 4
Goal-oriented optimal approximations
of linear inverse problems
4.1
Introduction
As we saw in Chapter 3, the posterior distribution defines the Bayesian solution to
the inverse problem. Characterizing this posterior distribution is of primary interest
in a variety of engineering and science applications. For instance, we might be inter-
ested in the posterior marginals, the posterior probability of some functionals of the
parameters, or the probability of rare events under the posterior measure. In all these
cases we may need to draw samples from the posterior distribution. This sampling
task tends to be extremely challenging in large-scale applications, especially when the
parameters represent a finite-dimensional approximation to a distributed stochastic
process like a permeability or a temperature field. In many applications, however, we
are only interested in a particular function of the parameters (e.g., the temperature
field over a subregion of the entire domain or the probability that the temperature
exceeds a critical value). In this chapter we exploit such ultimate goals to reduce the
cost of inversion.
We begin by considering our usual finite-dimensional linear-Gaussian inverse prob-
lem of the form
ğ‘Œ= ğºğ‘‹+ â„°,
(4.1)
75

where ğ‘‹âˆˆRğ‘›represents the unknown parameters, ğ‘ŒâˆˆRğ‘‘denotes the noisy ob-
servations, ğºâˆˆRğ‘‘Ã—ğ‘›is a deterministic linear forward operator, and â„°âˆ¼ğ’©(0, Î“obs)
is a zero-mean additive Gaussian noise, statistically independent of ğ‘‹and with co-
variance matrix Î“obs â‰»0.
We prescribe a Gaussian prior distribution, ğ’©(0, Î“pr),
on ğ‘‹and assume, without loss of generality, zero prior mean and Î“pr â‰»0.
In
Chapter 3, we were concerned with the posterior distribution of the parameters,
ğ‘‹|ğ‘Œâˆ¼ğ’©(ğœ‡pos(ğ‘Œ), Î“pos), which has mean and covariance given by
ğœ‡pos(ğ‘Œ) = Î“pos ğºâŠ¤Î“âˆ’1
obs ğ‘Œ,
Î“pos = (ğ»+ Î“âˆ’1
pr )âˆ’1,
(4.2)
where ğ»:= ğºâŠ¤Î“âˆ’1
obs ğºis the Hessian of the negative log-likelihood. In this chapter,
however, we are not interested in the parameters ğ‘‹per se, but rather in a quantity
of interest (QoI) ğ‘that is a function of the parameters,
ğ‘= ğ’ªğ‘‹,
(4.3)
for some linear and, without loss of generality, full row-rank operator ğ’ªâˆˆRğ‘Ã—ğ‘›with
ğ‘< ğ‘›. Our interests are thus goal-oriented, as we wish to characterize only ğ‘and
not the parameters ğ‘‹. Including such ultimate goals in the inference formulation is
an important modeling step in many applications of Bayesian inverse problems. This
additional step should reduce the computational complexity of inference by making
the ultimate goals explicit. Nevertheless, it is still not clear how to leverage ultimate
goals to yield more efficient Bayesian inference algorithms. The present chapter will
address this issue.
The full Bayesian solution to the goal-oriented inverse problem is the posterior
distribution of the QoI, i.e., ğ‘|ğ‘Œ. It is easy to see that ğ‘|ğ‘Œis once again Gaussian
with mean and covariance matrix given by
ğœ‡ğ‘|ğ‘Œ(ğ‘Œ) = ğ’ªğœ‡pos(ğ‘Œ),
Î“ğ‘|ğ‘Œ= ğ’ªÎ“pos ğ’ªâŠ¤.
(4.4)
The goal of this work is to characterize statistically optimal, computationally effi-
76

cient, and structureâ€“exploiting approximations of the statistics of ğ‘|ğ‘Œwhenever the
use of direct formulas such as (4.4) is challenging or impractical (perhaps due to high
computational complexity or excessive storage requirements). We will approximate
Î“ğ‘|ğ‘Œas a low-rank negative update of the prior covariance of the QoI. Optimality will
be defined with respect to the natural geodesic distance on the manifold of symmet-
ric and positive definite (SPD) matrices [99]. The posterior mean ğœ‡ğ‘|ğ‘Œ(ğ‘Œ) will be
approximated as a low-rank function of the data, where optimality is defined by the
minimization of the Bayes risk for squared-error loss weighted by Î“âˆ’1
ğ‘|ğ‘Œ. The essence
of these approximations is the restriction of the inference process to directions in the
parameter space that are informed by the data relative to the prior and that are
relevant to the QoI. These directions correspond to the leading generalized eigenpairs
of a suitable matrix pencil that is fundamentally different from the one analyzed in
Chpater 3.
This chapter extends, in several different ways, the work on goal-oriented inference
originally presented in [169]. First of all, we introduce the notion of optimal approx-
imation, rather than exact computation, for both the posterior covariance matrix
and the posterior mean of the QoI. We propose computationally efficient algorithms
to determine these optimal approximations. The complexity of our algorithms scales
with the intrinsic dimensionality of the goal-oriented problemâ€”which here reflects the
dimension of the parameter subspace that is simultaneously relevant to the QoI and
informed by the data, as noted above. In particular, the full posterior distribution of
the parameters need not be computed at any stage of the algorithms. This is a key
contrast with [169]. Moreover, we make it possible to handle high-dimensional QoIs
such as those arising from the discretization of a distributed stochastic process. This
class of problems is frequently encountered in applications (see, e.g., Section 4.4).
The ideas and algorithms presented in this chapter are primarily developed in the
linear-Gaussian case. On one hand, their application to goal-oriented linear inverse
problems is of standalone interest [169]. On the other hand, in the context of high-
dimensional nonlinear Bayesian inverse problems, the Gaussian approximation is often
the only tractable approximation of the posterior distribution [40, 135]. For example,
77

in [135], a linearization of the parameter-to-observable map and a linearization of the
parameter-to-prediction map are performed for a high-dimensional ice sheet model,
reducing the nonlinear inverse problem to a goal-oriented linear Gaussian inverse
problem. Moreover, the rigorous analysis of dimensionality reduction ideas in linear
inverse problems often leads to computationally efficient dimensionality reduction
strategies for nonlinear inverse problems (see, e.g., [261, 74] or [169, 170]).
The remainder of this chapter is organized as follows. In Section 4.2 we introduce
statistically optimal approximations of the posterior statistics of the QoI. Section
4.3 contains a simple proof-of-concept example, while in Section 4.4 we illustrate
the theory using a more realistic inverse problem in heat transfer. Section 4.5 offers
some concluding remarks. Appendix D contains the proofs of the main results. The
material presented in this chapter can be also found in [260].
4.2
Theory
In this section we introduce optimal approximations of the posterior mean ğœ‡ğ‘|ğ‘Œ(ğ‘Œ)
and posterior covariance Î“ğ‘|ğ‘Œof the QoI. Section 4.2.1 focuses on the approximation
of Î“ğ‘|ğ‘Œ, while the posterior mean approximation is addressed in Section 4.2.2. The
main results of this section are Theorem 4.1 and Theorem 4.2.
4.2.1
Approximation of the posterior covariance of the QoI
We first focus on approximating Î“ğ‘|ğ‘Œ. The cost of computing Î“ğ‘|ğ‘Œaccording to (4.4)
is dominated by the solution of ğ‘linear systems with coefficient matrix Î“âˆ’1
pos in order
to determine Î“pos ğ’ªâŠ¤. In large-scale inverse problems only the action of the precision
matrix Î“âˆ’1
pos on a vector is usually available; it is not reasonable to expect to have direct
factorizations of Î“âˆ’1
pos, such as Cholesky decomposition. Thus, the solution of linear
systems with coefficient matrix Î“âˆ’1
pos is often necessarily iterative (e.g., via Krylov
subspace methods for SPD matrices [128, 110]).
Moreover, the storage requirements
for Î“ğ‘|ğ‘Œscale as ğ‘‚(ğ‘2). If the dimension of the QoI is inherently low, e.g., ğ‘= ğ‘‚(1),
then the use of direct formulas like (4.4) can be remarkably efficient. For instance, if
78

we are only interested in the average of ğ‘‹, i.e., ğ‘:= 1
ğ‘›
âˆ‘ï¸€
ğ‘–ğ‘‹ğ‘–, then the QoI is only
one-dimensional and computing the posterior covariance of the QoI amounts to solving
essentially a single linear system. As the dimension of the QoI increases, however,
direct formulas like (4.4) become increasingly impractical due to high computational
complexity and storage requirements. In many cases of interest, the dimension of the
QoI can be arbitrarily large. Consider the following simple example. If ğ‘‹represents
a finite-dimensional approximation of a spatially distributed stochastic process (e.g.,
an unknown temperature field), then the QoI could be the restriction of this process to
a domain of interest. In this case, the QoI is also a finite-dimensional approximation
of a spatially distributed process, and its dimension can be arbitrarily high depending
on the chosen level of discretization. (We will revisit this example in Section 4.4.)
There is a clear need for new inference algorithms to efficiently tackle such problems.
Even though direct formulas like (4.4) can be intractable when the QoI is high-
dimensional, we saw in Chapter 3 that essential features of large-scale Bayesian inverse
problems bring additional structure to the Bayesian update: The prior distribution
might encode some kind of smoothness or correlation among the inversion parame-
ters. Observations are typically limited in number, indirect, corrupted by noise, and
related to the inversion parameters by the action of a forward operator that filters
out some information [261, 74]. As a result, data are usually informative, relative to
the prior, only about a low-dimensional subspace of the parameter space. That is,
the important differences between the prior and posterior distributions are confined
to a low-dimensional subspace. This source of low-dimensional structure is key to the
development of efficient Bayesian inversion algorithms [96, 74] and also plays a crucial
role when dealing with goal-oriented problems. The optimal approximation, Ì‚ï¸€Î“pos, of
the posterior covariance of the parameters introduced in Chapter 3 is the starting
point for our analysis of goal-oriented inverse problems.
A naÃ¯ve approximation
In this section we introduce an intuitive but suboptimal approximation of Î“ğ‘|ğ‘Œ,
which will provide insight and motivation for the structure of the forthcoming optimal
79

approximation. The reader interested exclusively in the optimal approximation may
jump directly to Section 4.2.1.
The combination of Theorem 3.1 with the direct formulas (4.4) suggests a first
approximation strategy for the posterior covariance of the QoI: just replace Î“pos in
(4.4) with the optimal approximation described by Theorem 3.1,
Î“ğ‘|ğ‘Œâ‰ˆÌ‚ï¸€Î“ğ‘|ğ‘Œ:= ğ’ªÌ‚ï¸€Î“pos ğ’ªâŠ¤= ğ’ªÎ“pr ğ’ªâŠ¤âˆ’ğ’ªğ¾ğ¾âŠ¤ğ’ªâŠ¤,
(4.5)
where the low-rank update ğ¾ğ¾âŠ¤is given by (3.11). Approximation (4.5) is already
a major computational improvement over the direct formulas (4.4). There is no need
to solve ğ‘linear systems; rather, we only need to compute the leading eigenpairs
of (ğ», Î“âˆ’1
pr ) with a matrix-free algorithm. The rank of the update depends on the
dimension of the parameter subspace that is most informed by the data.
Despite these favorable computational properties, the approximation (4.5) is still
not satisfactory as it does not explicitly account for the goal-oriented feature of the
problem: Ì‚ï¸€Î“pos in (4.5) is the optimal approximation of the posterior covariance of
the parameters, but is by no means tailored to the QoI. The pencil (ğ», Î“âˆ’1
pr ) used
to compute the approximation Ì‚ï¸€Î“pos does not include the goal-oriented operator ğ’ª.
In other words, the directions ( Ì‚ï¸€ğ‘¤ğ‘–) defining the optimal prior-to-posterior update in
(3.11), though strongly data-informed, need not be relevant to the QoI. For instance,
some of the ( Ì‚ï¸€ğ‘¤ğ‘–) could lie in the nullspace of the goal-oriented operator. Comput-
ing these eigenvectors would be an unnecessary waste of computational resources. Of
course, as the rank of the optimal prior-to-posterior update increases, the correspond-
ing approximation Ì‚ï¸€Î“ğ‘|ğ‘Œwill continue to improve until eventually Î“ğ‘|ğ‘Œ= Ì‚ï¸€Î“ğ‘|ğ‘Œ. In
the worst case scenario, however, Ì‚ï¸€Î“ğ‘|ğ‘Œwill be a good approximation of Î“ğ‘|ğ‘Œonly
as we start computing eigenpairs of (ğ», Î“âˆ’1
pr ) associated with the smallest nonzero
generalized eigenvalues. This is clearly unacceptable as the overall complexity of the
approximation algorithm would not depend on the nature of the goal-oriented op-
erator. Therefore, the approximation (4.5) cannot satisfy any reasonable optimality
statement in the spirit of Theorem 3.1 and calls for a proper modification.
80

An optimal approximation
The form of Ì‚ï¸€Î“ğ‘|ğ‘Œin (4.5) shows that the posterior covariance of the QoI can be written
as a low-rank update of the prior covariance. (Recall that the prior distribution of the
QoI is Gaussian, ğ‘âˆ¼ğ’©(0, Î“ğ‘) with Î“ğ‘= ğ’ªÎ“pr ğ’ªâŠ¤.) This is once again consistent
with our intuition about the Bayesian update: the data will only inform certain
aspects of the QoI. Thus a structure-exploiting approximation class for Î“ğ‘|ğ‘Œis given
by the set of positive definite matrices that can be written as rankâ€“ğ‘Ÿnegative-definite
updates of Î“ğ‘:
â„³ğ‘
ğ‘Ÿ= {Î“ğ‘âˆ’ğ¾ğ¾âŠ¤â‰»0 : rank(ğ¾) â‰¤ğ‘Ÿ}.
(4.6)
Before introducing one of the main results of this chapter, we observe that ğ‘Œand
ğ‘are related by a linear model similar to (4.1). The following lemma clarifies this
relationship.
Lemma 4.1. A linear Gaussian model consistent with (4.4) is given by:
ğ‘Œ= ğºğ’ªâ€  ğ‘+ Î”,
(4.7)
where ğ’ªâ€  := Î“prğ’ªâŠ¤Î“âˆ’1
ğ‘, ğ‘âˆ¼ğ’©(0, Î“ğ‘) and Î” âˆ¼ğ’©(0, Î“Î”) are independent, and
Î“Î” := Î“obs + ğº(Î“pr âˆ’Î“prğ’ªâŠ¤Î“âˆ’1
ğ‘ğ’ªÎ“pr)ğºâŠ¤.
The results of Chapter 3 can be extended to the goal-oriented case by applying
them to the reduced linear model in (4.7). The following theorem defines the optimal
approximation of Î“ğ‘|ğ‘Œand is one of the main results of this chapter.
Theorem 4.1 (Optimal approximation of the posterior covariance of the QoI). Let
(ğœ†ğ‘–, ğ‘ğ‘–) be the eigenpairs of:
(ğºÎ“pr ğ’ªâŠ¤Î“âˆ’1
ğ‘ğ’ªÎ“pr ğºâŠ¤, Î“ğ‘Œ)
(4.8)
with the ordering ğœ†ğ‘–â‰¥ğœ†ğ‘–+1 > 0 and normalization ğ‘âŠ¤
ğ‘–ğºÎ“pr ğ’ªâŠ¤Î“âˆ’1
ğ‘ğ’ªÎ“pr ğºâŠ¤ğ‘ğ‘–= 1,
where Î“ğ‘Œ:= Î“obs + ğºÎ“pr ğºâŠ¤is the covariance matrix of the marginal distribution
81

of ğ‘Œ. Then, a minimizer Ìƒï¸€Î“ğ‘|ğ‘Œof the Riemannian metric ğ‘‘â„›between Î“ğ‘|ğ‘Œand an
element of â„³ğ‘
ğ‘Ÿis given by:
Ìƒï¸€Î“ğ‘|ğ‘Œ= Î“ğ‘âˆ’ğ¾ğ¾âŠ¤,
ğ¾ğ¾âŠ¤=
ğ‘Ÿ
âˆ‘ï¸
ğ‘–=1
ğœ†ğ‘–Ì‚ï¸€ğ‘ğ‘–Ì‚ï¸€ğ‘âŠ¤
ğ‘–,
Ì‚ï¸€ğ‘ğ‘–:= ğ’ªÎ“pr ğºâŠ¤ğ‘ğ‘–,
(4.9)
where the corresponding minimum distance is:
ğ‘‘2
â„›(Î“ğ‘|ğ‘Œ, Ìƒï¸€Î“ğ‘|ğ‘Œ) = 1
2
âˆ‘ï¸
ğ‘–>ğ‘Ÿ
ln2( 1 âˆ’ğœ†ğ‘–).
(4.10)
The optimal approximation in Theorem 4.1 yields the best approximation for any
given rank of the prior-to-posterior update and, most importantly, never requires the
full posterior covariance of the parameters. (This should be contrasted with [169].)
The directions (ğ‘ğ‘–) that define the optimal update are the leading eigenvectors of
(ğºÎ“pr ğ’ªâŠ¤Î“âˆ’1
ğ‘ğ’ªÎ“pr ğºâŠ¤, Î“ğ‘Œ) and stem from a careful balance of all the ingredients
of the goal-oriented inverse problem: the forward model, measurement noise, prior
information, and ultimate goals. Incorporating ultimate goals reduces the intrinsic
dimensionality of the inverse problem: for any fixed approximation error, the rank
of the optimal update (4.9) can only be less than or equal to that of the suboptimal
approximation introduced in (4.5).
Computational remarks
If square roots of Î“pr and Î“obs are available, such that Î“pr = ğ‘†pr ğ‘†âŠ¤
pr and Î“obs =
ğ‘†obs ğ‘†âŠ¤
obs, then we can rewrite the pencil (4.8) in a more concise form as follows.
Corollary 4.1. Let Ì‚ï¸€ğº:= ğ‘†âˆ’1
obs ğºğ‘†pr, and let Î  be an orthogonal projector onto the
range of ğ‘†âŠ¤
pr ğ’ªâŠ¤. Then the eigenvalues of
( Ì‚ï¸€ğºÎ  Ì‚ï¸€ğºâŠ¤, ğ¼+ Ì‚ï¸€ğºÌ‚ï¸€ğºâŠ¤)
(4.11)
are the same as those of (4.8), and the eigenvectors of (4.11) can be mapped to the
eigenvectors of (4.8) with the transformation ğ‘¤â†¦â†’ğ‘†âˆ’âŠ¤
obs ğ‘¤.
82

The proof of the corollary is straightforward once we note that Î  can be written
as Î  = ğ‘†âŠ¤
pr ğ’ªâŠ¤(ğ’ªğ‘†prğ‘†âŠ¤
prğ’ªâŠ¤)âˆ’1 ğ’ªğ‘†pr = ğ‘†âŠ¤
pr ğ’ªâŠ¤Î“âˆ’1
ğ‘ğ’ªğ‘†pr. Moreover, the action of the
projector Î  on a vector ğ‘£can be computed efficiently since Î (ğ‘£) := ğ‘†âŠ¤
pr ğ’ªâŠ¤ğ‘¥ls, where
ğ‘¥ls is the least squares solution of the overdetermined linear system ğ‘†âŠ¤
pr ğ’ªâŠ¤ğ‘¥ls = ğ‘£.
There is a variety of techniques for the solution of large-scale matrix-free least squares
problems (e.g., [208, 98, 54, 128, 190]).
We now focus our computational remarks on the analysis of the pencil in (4.11),
which is well suited for practical implementations of the approximation. To simplify
notation, let us rewrite (4.11) as (ğ´, ğµ), where ğ´:= Ì‚ï¸€ğºÎ  Ì‚ï¸€ğºâŠ¤and ğµ:= ğ¼+ Ì‚ï¸€ğºÌ‚ï¸€ğºâŠ¤.
Finding the leading generalized eigenpairs of (4.11) requires the solution of a Her-
mitian generalized eigenvalue problem [13]. Unfortunately, it is not easy to reduce
(4.11) to a standard eigenvalue problem,1 as doing so would require the action of a
square root of ğµor of ğµâˆ’1. Nevertheless, there are a plethora of matrix-free algo-
rithms for large-scale generalized eigenvalue problems: generalized Lanczos iteration
[13, Section 5.5], randomized SVDâ€“type methods [241], manifold optimization algo-
rithms [1, 2, 14], the trace minimization algorithm [243, 153], and the inverseâ€“free
preconditioned Krylov subspace method [112], to name a few. These algorithms re-
quire the iterative solution of linear systems associated with ğµ, in some cases to low
accuracy [243, 112]. Applying ğµto a vector requires the evaluation of the forward
model, which may or may not be expensive (e.g., consider PDE based models [96]
versus image deblurring problems [145]). In practice, for a fixed dimension of the de-
sired eigenspace, algorithms for characterizing the eigenpairs of (ğ´, ğµ) lead to more
expensive computations than those for the pencil (ğ», Î“âˆ’1
pr ) used in the naÃ¯ve approx-
imation of Section 4.2.1. However, the key point is that the optimal approximation
of Theorem 4.1 requires the characterization of lower dimensional eigenspaces for a
given accuracy.
Moreover, if we solve the generalized eigenvalue problem using a block Lanczos
iteration or a randomized method, then we can also exploit block Krylov methods to
1In contrast, this is often possible in the non-goal-oriented case when dealing with the pencil
(ğ», Î“âˆ’1
pr ), as the action of a square root of Î“âˆ’1
pr , or of its inverse, is available in many cases of interest
(e.g., [174]).
83

solve the associated linear systemsâ€”comprised of ğµand multiple right-hand sidesâ€”
simultaneously [200, 240]. In particular, the convergence of Krylov methods for solv-
ing linear systems of the form ğµğ‘¥= ğ‘, such as the conjugate gradient algorithm,
depends not only on the spectrum of ğµbut also on the right-hand side ğ‘[172, 12].
This dependence is especially important when the right-hand side has some structure
and is not entirely random: in our case, ğ‘lies in the range of the possibly low-rank
operator ğ´.
For instance, if ğ‘is mostly contained in a low-dimensional invariant
subspace of ğµ(whether associated with small or large eigenvalues), then the Krylov
solver will likely converge to an accurate solution in few steps. Conversely, it should
be noted that if the range of the operator Î  in (4.11)â€”essentially the subspace of the
parameter space that is relevant to the QoIâ€”has non-negligible components along
every data-informed parameter direction (corresponding to the leading eigenspace of
(ğ», Î“âˆ’1
pr )), then it would be difficult to obtain an accurate approximation of Î“ğ‘|ğ‘Œ
without exploring the full data-informed subspace.
Even though the naÃ¯ve approximation is suboptimal, it may be interesting from
a practical standpoint to assess its performance, since it is cheaper to compute for
a given approximation rank. The following lemma provides useful guidelines in this
direction.
Lemma 4.2 (Relationship between approximations). Let Ìƒï¸€Î“ğ‘|ğ‘Œ, Ì‚ï¸€Î“ğ‘|ğ‘Œâˆˆâ„³ğ‘
ğ‘Ÿbe,
respectively, the optimal and suboptimal approximations of Î“ğ‘|ğ‘Œintroduced in (4.9)
and (4.5). Moreover, let Ì‚ï¸€Î“pos âˆˆâ„³ğ‘Ÿbe the optimal approximation of Î“pos defined in
(3.11). Then
ğ‘‘â„›( Î“ğ‘|ğ‘Œ, Ìƒï¸€Î“ğ‘|ğ‘Œ) â‰¤ğ‘‘â„›( Î“ğ‘|ğ‘Œ, Ì‚ï¸€Î“ğ‘|ğ‘Œ) â‰¤ğ‘‘â„›( Î“pos , Ì‚ï¸€Î“pos ).
(4.12)
Lemma 4.2 has several interesting consequences.
First of all, notice that it is
possible to bound the accuracy of the naÃ¯ve approximation, Ì‚ï¸€Î“ğ‘|ğ‘Œ= ğ’ªÌ‚ï¸€Î“pos ğ’ªâŠ¤, using
ğ‘‘â„›( Î“pos , Ì‚ï¸€Î“pos ). The latter distance can easily be bounded as a function of the gener-
alized eigenvalues of (ğ», Î“âˆ’1
pr ), as shown in (3.12). These are precisely the eigenvalues
computed by the naÃ¯ve approximation. Thus, if the eigenvalues of (ğ», Î“âˆ’1
pr ) decay
84

sharply or, equivalently, if the distance ğ‘‘â„›( Î“pos , Ì‚ï¸€Î“pos ) can be made small with only
a low-dimensional (small ğ‘Ÿ) prior-to-posterior update, then Lemma 4.2 says that the
naÃ¯ve approximation, albeit suboptimal, can yield a remarkably efficient approxima-
tion of Î“ğ‘|ğ‘Œâ€”with strong accuracy guarantees in terms of ğ‘‘â„›( Î“pos , Ì‚ï¸€Î“pos). Intuitively,
if Ì‚ï¸€Î“ğ‘|ğ‘Œalready accounts for most of the data-informed directions in the parameter
space, then there is no major loss of accuracy in neglecting further directions of the
prior-to-posterior update, even if these directions are relevant to the QoI. In this sit-
uation, these additional directions would provide very limited information relative to
the prior and can be safely neglected.
On the other hand, if the eigenvalues of (ğ», Î“âˆ’1
pr ) do not decay as quickly, then
the bound provided in (4.12) becomes useless. This is not to say that the naÃ¯ve ap-
proximation will necessarily perform poorly. (It is possible that the QoI depends only
on a few of the leading data-informed directions, such that the naÃ¯ve approximation
performs well even for a low rank prior-to-posterior update.) But we cannot quantify
the accuracy of the naÃ¯ve approximation unless we directly compute ğ‘‘â„›(Î“ğ‘|ğ‘Œ, Ì‚ï¸€Î“ğ‘|ğ‘Œ),
which in turn requires the solution of an expensive generalized eigenvalue problem
for each rank of the update. This is not feasible in practice. In such situations, we
should resort to the optimal approximation introduced in Theorem 4.1, which offers
a useful error bound as well as a concrete possibility for both computational and
storage savings.
Properties of the optimal covariance approximation
An important consequence of the optimal approximation of Î“ğ‘|ğ‘Œwith respect to
the metric ğ‘‘â„›is optimality in distribution whenever the posterior mean of the QoI
is known. It follows from Lemma 3.1 that the minimizer of the Hellinger distance
(or the Kullbackâ€“Leibler divergence) between the Gaussian posterior measure of the
QoI, ğœˆğ‘|ğ‘Œ:= ğ’©(ğœ‡ğ‘|ğ‘Œ(ğ‘Œ), Î“ğ‘|ğ‘Œ), and the approximation ğ’©(ğœ‡ğ‘|ğ‘Œ(ğ‘Œ), Î“) for a matrix
Î“ âˆˆâ„³ğ‘
ğ‘Ÿ, is given by the optimal approximation (4.9) defined in Theorem 4.1. In
particular, let Ëœğœˆğ‘|ğ‘Œ:= ğ’©(ğœ‡ğ‘|ğ‘Œ(ğ‘Œ), Ìƒï¸€Î“ğ‘|ğ‘Œ) be the measure that optimally approxi-
mates ğœˆğ‘|ğ‘Œ, where Ìƒï¸€Î“ğ‘|ğ‘Œis defined in (4.9). Then it is easy to show that the Hellinger
85

distance between ğœˆğ‘|ğ‘Œand the optimal approximation Ëœğœˆğ‘|ğ‘Œis given by:
ğ‘‘Hell(ğœˆğ‘|ğ‘Œ, Ëœğœˆğ‘|ğ‘Œ) =
âˆšï¸ƒ
1 âˆ’
âˆï¸
ğ‘–>ğ‘Ÿ
21/2(1 âˆ’ğœ†ğ‘–)1/4
(2 âˆ’ğœ†ğ‘–)1/2
(4.13)
where (ğœ†ğ‘–) are the generalized eigenvalues defined in Theorem 4.1 (e.g., [261, Appendix
A] or [209]).
The Hellinger distance can be used to bound the error of expectations of functions
of interest with respect to approximate measures [76].
That is, suppose that we
are interested in the posterior expectation Eğœˆğ‘|ğ‘Œ[ğ‘”] of some measurable function ğ‘”:
Rğ‘â†’R, with certain bounded moments with respect to the prior measure ğœˆğ‘:=
ğ’©(0, Î“ğ‘), and suppose further that we can only evaluate integrals with respect to
the approximate measure Ëœğœˆğ‘|ğ‘Œ. Then the error resulting from computing EËœğœˆğ‘|ğ‘Œ[ğ‘”], as
opposed to Eğœˆğ‘|ğ‘Œ[ğ‘”], for a fixed realization of the data ğ‘Œ, can be bounded in terms of
the Hellinger distance between the two Gaussian measures using the following lemma,
which follows easily from [76, Lemma 7.14].
Lemma 4.3 (Convergence in expectation). Let ğ‘”: Rğ‘â†’R be a measurable function
with ğ›½> 2 bounded moments with respect to the prior measure, i.e., Eğœˆğ‘[ |ğ‘”|ğ›½] < âˆ.
Then:
âƒ’âƒ’âƒ’Eğœˆğ‘|ğ‘Œ[ğ‘”] âˆ’EËœğœˆğ‘|ğ‘Œ[ğ‘”]
âƒ’âƒ’âƒ’â‰¤ğ’(ğ‘Œ, ğ‘”) ğ‘‘Hell(ğœˆğ‘|ğ‘Œ, Ëœğœˆğ‘|ğ‘Œ),
(4.14)
where ğ’(ğ‘Œ, ğ‘”) := 2
âˆš
2
|Î“ğ‘|1/4
|Î“ğ‘|ğ‘Œ|1/4 exp
(ï¸
1
2(ğ›½âˆ’2) â€–ğœ‡ğ‘|ğ‘Œ(ğ‘Œ)â€–2
Î“âˆ’1
ğ‘
)ï¸
Eğœˆğ‘[ |ğ‘”|ğ›½]1/ğ›½and where
|ğ´| denotes the determinant of the matrix ğ´.
Notice that the constant ğ’(ğ‘Œ, ğ‘”) in (4.14) is independent of the approximating
measure Ëœğœˆğ‘|ğ‘Œ. Convergence of the approximation in Hellinger distance thus implies
convergence of the expectation EËœğœˆğ‘|ğ‘Œ[ğ‘”] to Eğœˆğ‘|ğ‘Œ[ğ‘”].
It is interesting to note that the optimal approximation of the posterior covari-
ance matrix of the QoI in Theorem 4.1 is always associated with a corresponding
approximation of the posterior covariance of the parameters, Î“pos.
The following
result clarifies the nature of this approximation.
86

Lemma 4.4 (Goal-oriented approximation of Î“pos). Let Ìƒï¸€Î“ğ‘|ğ‘Œbe the minimizer of the
metric ğ‘‘â„›between Î“ğ‘|ğ‘Œand an element of â„³ğ‘
ğ‘Ÿas given by (4.9). Then Ìƒï¸€Î“ğ‘|ğ‘Œcan be
written as
Ìƒï¸€Î“ğ‘|ğ‘Œ= ğ’ªÌ‚ï¸€Î“*
pos ğ’ªâŠ¤,
Ì‚ï¸€Î“*
pos = Î“pr âˆ’
ğ‘Ÿ
âˆ‘ï¸
ğ‘–=1
ğœ†ğ‘–Ìƒï¸€ğ‘ğ‘–Ìƒï¸€ğ‘âŠ¤
ğ‘–,
Ìƒï¸€ğ‘ğ‘–:= ğ‘†pr Î  ğ‘†âŠ¤
pr ğºâŠ¤ğ‘ğ‘–,
(4.15)
where the vectors (ğ‘ğ‘–) are defined in Theorem 4.1, ğ‘†pr is a square root of the prior
covariance matrix such that Î“pr = ğ‘†pr ğ‘†âŠ¤
pr, Î  is the orthogonal projector onto the
range of ğ‘†âŠ¤
pr ğ’ªâŠ¤, while Ì‚ï¸€Î“*
pos satisfies
Ì‚ï¸€Î“*
pos
âˆˆ
arg min
Î“ ğ‘‘â„›( Î“ğ‘|ğ‘Œ, ğ’ªÎ“ ğ’ªâŠ¤)
(4.16)
s.t.
Î“ âˆˆâ„³ğ‘Ÿ:= {Î“ = Î“pr âˆ’ğ¾ğ¾âŠ¤â‰»0, rank(ğ¾) â‰¤ğ‘Ÿ}.
The matrix Ì‚ï¸€Î“*
pos is an optimal goal-oriented approximation of Î“pos. This notion
of optimality is quite different from that in Theorem 3.1.
The prior-to-posterior
update directions, (Ìƒï¸€ğ‘ğ‘–) in (4.15), have the intuitive interpretation of directions, in the
parameter space, that are most informed by the data, relative to the prior, and that
are relevant to the QoI. In particular, it is easy to see that the (Ìƒï¸€ğ‘ğ‘–) are orthogonal to
the nullspace of the goal-oriented operator with respect to the inner product induced
by the prior precision, i.e.,
â„âŠ¤Î“âˆ’1
pr Ìƒï¸€ğ‘ğ‘–= (ğ’ªâ„)âŠ¤Î“âˆ’1
ğ‘ğ’ªÎ“prğºâŠ¤ğ‘ğ‘–= 0,
âˆ€â„âˆˆNull(ğ’ª).
(4.17)
Note that even if Ìƒï¸€Î“ğ‘|ğ‘Œ= ğ’ªÌ‚ï¸€Î“*
pos ğ’ªâŠ¤is a good approximation of Î“ğ‘|ğ‘Œ, Ì‚ï¸€Î“*
pos need not
be a good approximation of Î“pos.
Now we introduce a particularly simple factorization of the optimal approximation
Ìƒï¸€Î“ğ‘|ğ‘Œfrom Theorem 4.1, as Ìƒï¸€Î“ğ‘|ğ‘Œ= Ìƒï¸€ğ‘†ğ‘|ğ‘ŒÌƒï¸€ğ‘†âŠ¤
ğ‘|ğ‘Œfor some matrix Ìƒï¸€ğ‘†ğ‘|ğ‘Œ. We can think
of Ìƒï¸€ğ‘†ğ‘|ğ‘Œas a square root of Ìƒï¸€Î“ğ‘|ğ‘Œ, even though Ìƒï¸€ğ‘†ğ‘|ğ‘Œneed not be a square matrix.
Obtaining the action of a square root of Ìƒï¸€Î“ğ‘|ğ‘Œon a vector is an essential task if
our goal is to sample the distribution ğ’©(ğœ‡ğ‘|ğ‘Œ(ğ‘Œ), Ìƒï¸€Î“ğ‘|ğ‘Œ) in truly high-dimensional
problems. The key requirement is that Ìƒï¸€ğ‘†ğ‘|ğ‘Œbe easy to compute once we have the
87

optimal approximation Ìƒï¸€Î“ğ‘|ğ‘Œ. We have deferred the discussion of this topic until now
in order to exploit the results of Lemma 4.4 to obtain an explicit characterization of
Ìƒï¸€ğ‘†ğ‘|ğ‘Œ.
Lemma 4.5. Let (ğœ†ğ‘–, ğ‘ğ‘–), ğ‘†pr, and Î  be defined as in Lemma 4.4. Then, a non-
symmetric square root, Ìƒï¸€ğ‘†ğ‘|ğ‘Œ, of Ìƒï¸€Î“ğ‘|ğ‘Œ, such that Ìƒï¸€Î“ğ‘|ğ‘Œ= Ìƒï¸€ğ‘†ğ‘|ğ‘ŒÌƒï¸€ğ‘†âŠ¤
ğ‘|ğ‘Œ, is given by
Ìƒï¸€ğ‘†ğ‘|ğ‘Œ= ğ’ªğ‘†pr
(ï¸ƒ
ğ‘Ÿ
âˆ‘ï¸
ğ‘–=1
(
âˆšï¸€
1 âˆ’ğœ†ğ‘–âˆ’1) Â¯ğ‘ğ‘–Â¯ğ‘âŠ¤
ğ‘–+ ğ¼
)ï¸ƒ
,
Â¯ğ‘ğ‘–:= Î  ğ‘†âŠ¤
pr ğºâŠ¤ğ‘ğ‘–,
(4.18)
where ğ¼is the identity matrix.
The virtue of this result is that it does not require an invertible square root of
Î“ğ‘= ğ’ªÎ“pr ğ’ªâŠ¤as one would expect from [261, Remark 2].
Note that it is easy
to apply Ìƒï¸€ğ‘†ğ‘|ğ‘Œto a vector, which allows efficient sampling from ğ’©(ğœ‡ğ‘|ğ‘Œ(ğ‘Œ), Ìƒï¸€Î“ğ‘|ğ‘Œ).
An interesting feature of Ìƒï¸€ğ‘†ğ‘|ğ‘ŒâˆˆRğ‘Ã—ğ‘›is that it is a nonsquare matrix with ğ‘< ğ‘›.
This is certainly not an issue as long as Ìƒï¸€ğ‘†ğ‘|ğ‘ŒÌƒï¸€ğ‘†âŠ¤
ğ‘|ğ‘Œ= Ìƒï¸€Î“ğ‘|ğ‘Œ. Notice also that (4.18)
contains a square root of the prior covariance matrix. The action of this matrix is
usually available in large-scale applications (e.g., [275, 85, 174, 265, 281]). However,
if the action of a square root of Î“pr is truly unavailable, then one can still sample
from ğ’©(ğœ‡ğ‘|ğ‘Œ(ğ‘Œ), Ìƒï¸€Î“ğ‘|ğ‘Œ) by resorting to the action of the matrix Ìƒï¸€Î“ğ‘|ğ‘Œalone (e.g.,
[59, 100, 211, 248]). It is straightforward to apply Ìƒï¸€Î“ğ‘|ğ‘Œto a vector (see (4.9)).
4.2.2
Approximation of the posterior mean of the QoI
We conclude this theory section by introducing an optimal approximation of the
posterior mean of the QoI. The cost of computing
ğœ‡ğ‘|ğ‘Œ(ğ‘Œ) := ğ’ªğœ‡pos(ğ‘Œ) = ğ’ªÎ“pos ğºâŠ¤Î“âˆ’1
obs ğ‘Œ
(4.19)
for a single realization of the data is usually dominated by the cost of solving a single
linear system associated with Î“âˆ’1
pos to determine ğœ‡pos(ğ‘Œ). This task can be efficiently
tackled with state-of-the-art matrix-free iterative solvers for symmetric linear systems
88

(e.g., [13, 128, 3]). If, however, one is interested in the fast computation of ğœ‡ğ‘|ğ‘Œ(ğ‘Œ) for
multiple realizations of the data, as we were in Chapter 3, then the situation is quite
different. Solving a linear system to compute ğœ‡ğ‘|ğ‘Œ(ğ‘Œ) each time a new measurement
is available might be infeasible in practical applications. If the dimension of the QoI
is small, say ğ‘= ğ‘‚(1), then there is an easy solution to this problem. One can just
precompute the matrix ğ‘€:= ğ’ªÎ“pos ğºâŠ¤Î“âˆ’1
obs in an offline stage and then compute the
posterior mean of the QoI as ğœ‡ğ‘|ğ‘Œ(ğ‘Œ) = ğ‘€ğ‘Œeach time a new realization of the data
becomes available. Yet the computational efficiency of this procedure breaks down
as the dimension of the QoI increasesâ€”for instance, if the QoI is a finite-dimensional
approximation of some underlying function. In this case, the matrix ğ‘€would be large
and dense, and storing it could be quite inefficient. Moreover, performing a dense
matrix-vector product to compute ğœ‡ğ‘|ğ‘Œ(ğ‘Œ) = ğ‘€ğ‘Œmight become more expensive
than solving, a single linear system associated with Î“âˆ’1
pos.
Our goal is thus to characterize computationally efficient and statistically optimal
approximations of ğœ‡ğ‘|ğ‘Œ(ğ‘Œ) as a low-rank linear function of the data, i.e., ğœ‡ğ‘|ğ‘Œ(ğ‘Œ) â‰ˆ
Ìƒï¸€ğœ‡ğ‘|ğ‘Œ(ğ‘Œ) := ğ´ğ‘Œfor some low-rank matrix ğ´. With such an ğ´, computing Ìƒï¸€ğœ‡ğ‘|ğ‘Œ(ğ‘Œ)
for each new realization of the data would be computationally efficient. We define
optimality of the approximation with respect to the Bayes risk for squared-error loss
weighted by the posterior precision matrix of the QoI, i.e.,
â„¬(ğ´) := E
[ï¸‚
â€–ğ´ğ‘Œâˆ’ğ‘â€–2
Î“âˆ’1
ğ‘|ğ‘Œ
]ï¸‚
,
(4.20)
where â„¬(ğ´) denotes the Bayes risk associated with the matrix ğ´, and where the
expectation is taken over the joint distribution of ğ‘and ğ‘Œ. Minimizing the Bayes
risk (4.20) is equivalent to minimizing
E
[ï¸‚
â€–ğœ‡ğ‘|ğ‘Œ(ğ‘Œ) âˆ’Ìƒï¸€ğœ‡ğ‘|ğ‘Œ(ğ‘Œ)â€–2
Î“âˆ’1
ğ‘|ğ‘Œ
]ï¸‚
(4.21)
over all approximations of the posterior mean of the form Ìƒï¸€ğœ‡ğ‘|ğ‘Œ(ğ‘Œ) = ğ´ğ‘Œfor some
low-rank matrix ğ´. The Mahalanobis distance in (4.21) is precisely a Riemannian
metric of the form described in Section 3.2.2 and thus it is a natural way to assess
89

the quality of a posterior mean approximation. Notice that (4.21) is an average of
the squared Riemannian distance between ğœ‡ğ‘|ğ‘Œ(ğ‘Œ) and its approximation Ìƒï¸€ğœ‡ğ‘|ğ‘Œ(ğ‘Œ)
over the distribution of the data ğ‘Œ.
The following theorem characterizes the optimal approximation of ğœ‡ğ‘|ğ‘Œ(ğ‘Œ).
Theorem 4.2 (Optimal approximation of ğœ‡ğ‘|ğ‘Œ(ğ‘Œ)). Let (ğœ†ğ‘–, ğ‘ğ‘–, Ì‚ï¸€ğ‘ğ‘–) be defined as in
Theorem 4.1 and consider the minimization of the following Bayes risk over the set
of low-rank matrices:
min
ğ´
E
[ï¸‚
â€–ğ´ğ‘Œâˆ’ğ‘â€–2
Î“âˆ’1
ğ‘|ğ‘Œ
]ï¸‚
,
s.t.
rank(ğ´) â‰¤ğ‘Ÿ.
(4.22)
Then a minimizer of (4.22) is given by:
ğ´* =
ğ‘Ÿ
âˆ‘ï¸
ğ‘–=1
ğœ†ğ‘–Ì‚ï¸€ğ‘ğ‘–ğ‘âŠ¤
ğ‘–,
(4.23)
with minimum Bayes risk:
â„¬(ğ´*) = E
[ï¸‚
â€– ğ´* ğ‘Œâˆ’ğ‘â€–2
Î“âˆ’1
ğ‘|ğ‘Œ
]ï¸‚
=
âˆ‘ï¸
ğ‘–>ğ‘Ÿ
ğœ†ğ‘–
1 âˆ’ğœ†ğ‘–
+ â„“,
(4.24)
where â„“is the dimension of the parameter space.
Note that (4.23) can be computed â€œfor freeâ€ from the optimal approximation of
Î“ğ‘|ğ‘Œintroduced in Theorem 4.1. Also, the optimal approximations of both the pos-
terior mean and the posterior covariance of the QoI become quite accurate as soon as
we start including generalized eigenvalues ğœ†â‰ª1 in the corresponding approximations
(see minimum loss (4.10) and Bayes risk (4.24)). This is perfectly consistent with our
previous analysis of Chapter 3.
4.3
Proof-of-concept example
Before investigating the numerical performance of our goal-oriented approximations,
we illustrate the theory with a simple proof-of-concept example.
We consider an
90

identity forward model ğº= ğ¼, a diagonal observational noise precision Î“âˆ’1
obs =
diag(â„1, . . . , â„ğ‘›), and a diagonal prior covariance Î“pr = diag(ğœ‡1, . . . , ğœ‡ğ‘›), with â„ğ‘–=
ğ‘›âˆ’ğ‘–and ğœ‡ğ‘–= ğ‘–for ğ‘–= 1, . . . , ğ‘›. We may think of this problem as denoising a signal
ğ‘‹[261]. Figure 4-1 shows the normalized eigenvalues of Î“âˆ’1
obs and Î“pr in blue and red,
respectively, for the case ğ‘›= 30. The eigenvectors of both matrices correspond to the
canonical vectors in Rğ‘›, i.e., ğ‘’1, . . . , ğ‘’ğ‘›. In this case, the data are most informativeâ€”
in absolute termsâ€”along directions ğ‘’ğ‘–with ğ‘–â‰ªğ‘›, since the observational noise
precision â„ğ‘–is a decreasing function of ğ‘–. On the other hand, the prior variance is
large along ğ‘’ğ‘–when ğ‘–â‰«1, since ğœ‡ğ‘–is an increasing function of ğ‘–. Thus the prior is
more constraining where the data are more informative. The eigenpairs (ğ›¿2
ğ‘–, Ì‚ï¸€ğ‘¤ğ‘–) of
the pencil (ğ», Î“âˆ’1
pr ), defined in Theorem 3.1, are given by ğ›¿2
ğ‘–= â„ğ‘–Â· ğœ‡ğ‘–= (ğ‘›âˆ’ğ‘–) Â· ğ‘–and
Ì‚ï¸€ğ‘¤ğ‘–âˆğ‘’ğ‘–for ğ‘–= 1, . . . , ğ‘›. (These ğ›¿2
ğ‘–are not sorted in decreasing order; for simplicity,
we retain the same index ğ‘–as in the problem definition.) From the relative magni-
tudes of (ğ›¿2
ğ‘–)â€”illustrated by the green parabola in Figure 4-1â€”we can identify the
parameter directions that are most informed by the data relative to the prior: they
correspond to ğ‘’ğ‘–with ğ‘–around ğ‘›/2 (the middle of the spectrum). These directions
define the optimal prior-to-posterior update of Theorem 3.1. Modes ğ‘’ğ‘–with ğ‘–â‰ªğ‘›/2
are strongly informed by the data in an absolute sense, but not relative to the prior;
thus their overall importance is limited. In the same way, modes ğ‘’ğ‘–with ğ‘–â‰«ğ‘›/2
are unimportant to the update since the posterior variance along these directions is
roughly equal to the prior variance (ğ›¿2
ğ‘–â‰ª1), even though both variances are relatively
large [261].
Now let the goal-oriented operator ğ’ª: Rğ‘›â†’Rğ‘be defined as follows: ğ’ªğ‘¥=
(ğ‘¥1, . . . , ğ‘¥ğ‘) for ğ‘¥= (ğ‘¥1, . . . , ğ‘¥ğ‘›) and ğ‘= ğ‘›/2. Simple algebra shows that the goal-
oriented eigenpairs (ğœ†ğ‘–, ğ‘ğ‘–) of Theorem 4.1 are given by ğ‘ğ‘–âˆğ‘’ğ‘–for ğ‘–= 1, . . . , ğ‘›, and
ğœ†ğ‘–= 1/(1 + 1/(â„ğ‘–ğœ‡ğ‘–)) for ğ‘–â‰¤ğ‘and ğœ†ğ‘–= 0 for ğ‘–> ğ‘. So that the eigenvalues ğ›¿2
ğ‘–
and ğœ†ğ‘–are comparable in terms of their associated covariance approximation errorsâ€”
see (3.12) and (4.10)â€”we plot a nonlinear function of each ğœ†ğ‘–in Figure 4-1, namely
^ğœ†ğ‘–= ğ‘“(ğœ†ğ‘–) for ğ‘“(ğ‘¥) = 1/(1 âˆ’ğ‘¥). (Since ğ‘“is strictly increasing on [0, 1), the relative
importance of the (^ğœ†ğ‘–) is the same as that of the original (ğœ†ğ‘–).) The introduction of a
91

goal-oriented operator reveals directions that can be strongly informed by the data,
relative to the prior, but that are irrelevant to the QoI. These modes correspond to
(ğ‘’ğ‘–) for ğ‘–> ğ‘, and can be safely neglected when computing the Bayesian update
relevant to the QoI.
Of course, in the general case of non-diagonal operators (ğº, Î“obs, Î“pr), the direc-
tions ( Ì‚ï¸€ğ‘¤ğ‘–) and (ğ‘ğ‘–) need not coincide. The following numerical example will illustrate
this general situation.
index eigenvalue
0
5
10
15
20
25
30
normalized eigenvalues
0
0.2
0.4
0.6
0.8
1
Figure 4-1:
Normalized eigenvalues defined in the proof-of-concept example of Sec-
tion 4.3: in blue we show (â„ğ‘–/â„max), in red (ğœ‡ğ‘–/ğœ‡max), in green (ğ›¿2
ğ‘–/ğ›¿2
max), and in
magenta (^ğœ†ğ‘–/^ğœ†max), for ğ‘–= 1, . . . , ğ‘›and ğ‘›= 30. For any finite collection of eigenval-
ues (ğœğ‘–), we define ğœmax to be the maximum value over that collection. Since for ğ‘–â‰¤ğ‘
and ğ‘= 15, we have ^ğœ†ğ‘–= ğ›¿2
ğ‘–in this example, we shifted the magenta curve slightly
upwards to distinguish it from the green one.
4.4
Numerical examples
Now we numerically illustrate the performance of our approximations using a goal-
oriented inverse problem in heat transfer. In particular, we study the cooling of a
CPU by means of a heat sink. Our goal is to infer the (spatially inhomogeneous)
temperature of the CPU from noisy pointwise observations of temperature on the
heat sink. Figure 4-2 shows the problem setup: the three different layers of material
correspond, respectively, to the CPU (ğ’Ÿ1), a thin silicone layer that connects the
CPU to the heat sink (ğ’Ÿ2), and an aluminum fin (ğ’Ÿ3). We denote by ğ’Ÿthe union
92

of these domains. Each ğ’Ÿğ‘–represents a two-dimensional cross section of material of
constant width ğ‘Šalong the horizontal direction and a height ğ¿ğ‘–. We assume that
no heat transfer happens along the third dimension; this is a common engineering
approximation [19]. Each material has a constant density ğœŒğ‘–, a constant specific heat
ğ‘ğ‘–and a constant thermal conductivity ğ‘˜ğ‘–. The corresponding thermal diffusivities
ğ›¼ğ‘–= ğ‘˜ğ‘–/ğœŒğ‘–ğ‘ğ‘–are shown in the table at the right of Figure 4-2. The time-dependent
temperature field in each domain is Î˜(ğ‘–) : ğ’Ÿğ‘–Ã— ğ‘‡â†’R, where ğ‘‡= (0, ğ‘¡end], for
ğ‘–= 1, 2, 3. Jointly, these temperature fields are simply Î˜ : ğ’ŸÃ— ğ‘‡â†’R.
4.4.1
Forward, observational and prior models
The time evolution of each temperature field Î˜(ğ‘–) is described by a linear time-
dependent PDE of the form
ğœŒğ‘–ğ‘ğ‘–ğœ•ğ‘¡Î˜(ğ‘–) = âˆ‡Â· (ğ‘˜ğ‘–âˆ‡Î˜(ğ‘–)),
ğ‘–= 1, . . . , 3,
(4.25)
where ğœ•ğ‘¡denotes partial differentiation with respect to time. We assume no volumet-
ric heat production and use Fourierâ€™s law for the heat flux [113]. Equations (4.25)
should be complemented with appropriate boundary and initial conditions to define
a well-posed forward problem. We use the independent variables ğ‘ 1 and ğ‘ 2 to denote,
respectively, the horizontal and vertical directions and let ğ‘ = (ğ‘ 1, ğ‘ 2). The point
ğ‘ = (0, 0) corresponds to the lower left corner of ğ’Ÿ. At the lower boundary of ğ’Ÿ1 we
impose a space- and time-dependent heat flux: ğ‘˜1 ğœ•âƒ—ğ‘›Î˜(1) = ğ‘(ğ‘ , ğ‘¡) for ğ‘ âˆˆğ’Ÿ1,bottom,
where âƒ—ğ‘›refers to the outward pointing normal and ğ‘is a given nonconstant scalar
function in ğ‘ . At the interface between domains ğ’Ÿğ‘–and ğ’Ÿğ‘–+1 we assume heat trans-
fer by conduction with no thermal contact resistance: ğ‘˜ğ‘–ğœ•âƒ—ğ‘›Î˜(ğ‘–) = ğ‘˜ğ‘–+1 ğœ•âƒ—ğ‘›Î˜(ğ‘–+1) and
Î˜(ğ‘–) = Î˜(ğ‘–+1) for ğ‘ âˆˆinterface(ğ’Ÿğ‘–, ğ’Ÿğ‘–+1) and ğ‘–= 1, 2. At the top, left, and right
boundaries of ğ’Ÿ3, we assume heat transfer by convection with a fluid at constant
temperature Î˜âˆ: âˆ’ğ‘˜3 ğœ•âƒ—ğ‘›Î˜(3) = â„ğ‘(Î˜(3) âˆ’Î˜âˆ) for ğ‘ âˆˆğ’Ÿ3,top âˆªğ’Ÿ3,left âˆªğ’Ÿ3,right, where
â„ğ‘is a convective heat transfer coefficient. Finally, we impose adiabatic conditions
(no heat exchange) on the left and right boundaries of ğ’Ÿ1 and ğ’Ÿ2: ğœ•âƒ—ğ‘›Î˜(ğ‘–) = 0 for
93

ğ‘ âˆˆğ’Ÿğ‘–,left âˆªğ’Ÿğ‘–,right and ğ‘–= 1, 2. The initial conditions are not specified here as they
are the objective of the forthcoming inverse problem.
We consider a finite element spatial approximation of the weak form of (4.25) by
means of linear elements on simplices [220]. We denote by Î˜â„(ğ‘¡) âˆˆRğ‘›the collection
of temperature values at the finite element nodes on ğ’Ÿat time ğ‘¡âˆˆğ‘‡. The function
Î˜â„satisfies a system of ODEs of the form ğ‘€ğœ•ğ‘¡Î˜â„(ğ‘¡) + ğ´Î˜â„(ğ‘¡) = ğ‘“(ğ‘¡), with ğ‘¡âˆˆğ‘‡,
for a suitable mass matrix ğ‘€, stiffness matrix ğ´, known time-dependent forcing term
ğ‘“and initial conditions Î˜0â„:= Î˜â„(ğ‘¡= 0).
The initial conditions Î˜0â„are unknown and must be estimated from local mea-
surements of the temperature field Î˜ at different locations in space and time. The
locations of the sensors ğ‘ 1, . . . , ğ‘ ğ‘are shown as black dots in Figure 4-2. Observa-
tions are collected every Î”ğ‘¡time units for ğ‘¡âˆˆğ‘‡. The first observation happens at
time ğ‘¡= Î”ğ‘¡and we assume that there are ğ‘€observation times in total. We denote
measurements at time ğ‘¡ğ‘–= ğ‘–Î”ğ‘¡as Ì‚ï¸€ğ‘Œğ‘–=
[ï¸€
Î˜(ğ‘ 1, ğ‘–Î”ğ‘¡), . . . , Î˜(ğ‘ ğ‘, ğ‘–Î”ğ‘¡)
]ï¸€
. We concatenate
the observations into a vector Ì‚ï¸€ğ‘Œ= ( Ì‚ï¸€ğ‘Œ1, . . . , Ì‚ï¸€ğ‘Œğ‘€) âˆˆRğ‘‘. The actual observations are
corrupted with additive Gaussian noise: ğ‘Œ= Ì‚ï¸€ğ‘Œ+ â„°, where â„°âˆ¼ğ’©(0, ğœ2
obs ğ¼) and ğ¼
is the identity matrix. Notice that Ì‚ï¸€ğ‘Œis an affine function of Î˜0â„. This relationship
can be made linear by a suitable redefinition of the data vector. Thus, we are lead to
a linear Gaussian inverse problem in standard form, ğ‘Œ= ğºÎ˜0â„+â„°, where ğºdefines
the forward operator, Î˜0â„â†¦â†’Ì‚ï¸€ğ‘Œ, that can be evaluated implicitly by solving a heat
equation with no forcing term and initial conditions Î˜0â„for a time interval necessary
to collect the corresponding observations Ì‚ï¸€ğ‘Œ.
We define a zero-mean Gaussian prior distribution2 on Î˜0â„by modeling Î˜0â„as a
discretized solution of a stochastic PDE of the form
ğ›¾
(ï¸€
ğœ…2â„âˆ’â–³
)ï¸€
Î˜(ğ‘ ) = ğ’²(ğ‘ ),
ğ‘ âˆˆğ’Ÿ,
(4.26)
where ğ’²is a white noise process, ğœ…is a positive scalar parameter, â–³is the Laplacian
2There is no loss of generality in assuming zero prior mean. If we are given a statistical model of
the form ğ‘Œ= ğºÎ˜0â„+ â„°, where Î˜0â„âˆ¼ğ’©(ğœ‡pr, Î“pr) has a nonzero prior mean, then we can trivially
rewrite this model as Ì‚ï¸€ğ‘Œ:= ğ‘Œâˆ’ğºğœ‡pr = ğº(Î˜0â„âˆ’ğœ‡pr) + â„°for a modified data vector Ì‚ï¸€ğ‘Œand infer,
equivalently, a zero-prior-mean process Î˜0â„âˆ’ğœ‡pr âˆ¼ğ’©(0, Î“pr).
94

operator and â„is the identity operator. In particular, we exploit the explicit link
between Gaussian Markov random fields with the MatÃ©rn covariance function and
solutions to stochastic PDEs as outlined in [174]. In this case, the action of a square
root of the prior covariance matrix on a vector is readily available as the solution of
an elliptic PDE on ğ’Ÿ, and thus it is scalable to very large inverse problems [174].
In this example we use â„ğ‘= 23.8 W/m2 K for the convective heat transfer co-
efficient between the aluminum fin and the external fluid (air), which has constant
temperature Î˜âˆ= 283 K. The width of the domain ğ’Ÿin Figure 4-2 is ğ»= 2Ã—10âˆ’2 m.
The heat flux ğ‘(ğ‘ , ğ‘¡) is time-independent and nonnegative and can be written as the
superposition of two square impulse functions with zero background: one centered at
6 Ã— 10âˆ’3 m with width 8 Ã— 10âˆ’3 m and intensity 0.6 W/m2; and the other centered at
15Ã—10âˆ’3 m with width 4Ã—10âˆ’3 m and intensity 0.3 W/m2. Observations are collected
every Î”ğ‘¡= 5 Ã— 10âˆ’4 s for a total of ğ‘€= 100 measurements. We use ğœobs = 1/2 as
the standard deviation of the observational noise. The prior parameters in (4.26) are
given by ğ›¾= 1 Ã— 104 and ğœ…=
âˆš
8/ğœŒpr with ğœŒpr = ğ»/10. This choice of ğœ…defines a
prior with correlation values near 1/10 at distance ğœŒpr [174]. The original prior mean
is set to ğœ‡pr = 318 K. However, we equivalently infer the zero-prior-mean process
Î˜0â„âˆ’ğœ‡pr as explained in the previous footnote.
4.4.2
Goal-oriented linear inverse problem
We now introduce the goal-oriented feature of the problem. As stated earlier, we are
only interested in the initial temperature distribution over the CPU (i.e., in ğ’Ÿ1). Let
ğ‘be the restriction of Î˜0â„to the domain of interest ğ’Ÿ1. Clearly, there is a linear
map between ğ‘and Î˜0â„, i.e., ğ‘= ğ’ªÎ˜0â„with ğ’ªâˆˆRğ‘Ã—ğ‘›and ğ‘â‰ªğ‘›. Thus, we have
a linear-Gaussian goal-oriented inverse problem as introduced in Section 4.2:
â§
âª
â¨
âª
â©
ğ‘Œ= ğºÎ˜0â„+ â„°
ğ‘= ğ’ªÎ˜0â„,
(4.27)
95

where both the marginal distribution of Î˜0â„and the likelihood ğ‘Œ|Î˜0â„are specified.
(In this example we denote the parameters by Î˜0â„rather than ğ‘‹.) We choose a finite
element discretization of the temperature field such that Î˜0â„âˆˆR2400 and ğ‘âˆˆR370.
Our goal is to characterize optimal approximations of the posterior statistics of the
QoI, ğ‘|ğ‘Œ, for a given set of observations (see Figure 4-3 (left)). In this case, comput-
ing the posterior distribution of the QoI using direct formulas like (4.4) is challenging
as the QoI is a finite-dimensional approximation to a distributed stochastic process,
Î˜(0)|ğ’Ÿ1, and can be arbitrarily high-dimensional depending on the chosen level of
discretization.
The configuration of this problem highlights a crucial aspect of dimensionality
reduction of goal-oriented inverse problems. Ideally we would place the sensors on
ğ’Ÿ1 since we are interested in inferring the temperature field on the CPU. However,
due to geometrical constraints, we are forced to place our sensors on the heat sink
(ğ’Ÿ3). As a result, observations are much more informative about the parameters
in ğ’Ÿ3 than in ğ’Ÿ1.
We see a hint of this in Figure 4-3 (right), which shows the
normalized difference between the prior and posterior variance of the parameters,
(Var(Î˜0â„) âˆ’Var(Î˜0â„|ğ‘Œ))/Var(Î˜0â„). The prior variance is reduced the most around
the sensor locations in ğ’Ÿ3, which makes intuitive sense as the data are increasingly
less informative as we move away from the sensors.
We first focus on the approximation of the posterior covariance of the QoI. If we use
the suboptimal approximation introduced in (4.5), then we have to pay a considerable
computational price as a result of the data being informative about directions in the
parameter space that are not relevant to the QoI. This issue is illustrated by the
numerical results in Figure 4-6.
To set the stage, we begin with the posterior covariance Î“pos of the parameters
Î˜0â„and construct the optimal approximation Ì‚ï¸€Î“pos = Î“pr âˆ’ğ¾ğ¾âŠ¤from Theorem
3.1.
Though this approximation is optimal for any given rank of the update, its
convergence in this problem is rather slowâ€”as shown by the dotted blue line in Figure
4-6â€”because there are many data-informed directions in the parameter space. (Notice
the multitude of sensors on the heat sink in Figure 4-2, each yielding observations
96

at ğ‘€successive times.) If we now use Ì‚ï¸€Î“pos to yield an approximation of the actual
posterior covariance of interest Î“ğ‘|ğ‘Œby means of Î“ğ‘|ğ‘Œâ‰ˆÌ‚ï¸€Î“ğ‘|ğ‘Œ= ğ’ªÌ‚ï¸€Î“pos ğ’ªâŠ¤(i.e.,
the naÃ¯ve approximation of (4.5)), then the convergence of this approximation is still
slow, as seen in green solid line of Figure 4-6. This slow convergence can be easily
explained. The optimal approximation Ì‚ï¸€Î“pos of Î“pos accounts first for those directions
that are most informed by the data.
These directions correspond to modes with
features near the sensors in ğ’Ÿ3 (see Figure 4-4), but they provide little information
about the parameters in the region of interest (ğ’Ÿ1).
On the other hand, if we use the optimal approximation of Î“ğ‘|ğ‘Œdefined in The-
orem 4.1, then convergence is remarkably fast, as illustrated via the red solid line
in Figure 4-6. Now we only need to update Î“ğ‘along a handful of directionsâ€”say
twentyâ€”to achieve a satisfactory approximation of Î“ğ‘|ğ‘Œ. The key to achieving such
fast convergence is to confine the inference to directions in the parameter space that
are most informed by the data, relative to the prior, and that are relevant to the
QoI. Moreover, these fundamental directions can be explicitly extracted from a goal-
oriented approximation of the posterior covariance of the parameters, as explained in
Lemma 4.4; three such directions are shown in Figure 4-5.
We note that Î“ğ‘|ğ‘Œis by no means a low-rank matrix. (See its spectrum in Figure 4-
8 (left)). This situation is fairly typical when dealing with large-scale inverse problems
with non-smoothing priors (e.g., Gaussian fields with correlation function of MatÃ©rn
type) and limited observations. In these situations, seeking an approximation of Î“ğ‘|ğ‘Œ
as a low-rank matrix would be inappropriate; that is, classic dimensionality reduction
techniques, e.g., Karhunenâ€“LoÃ¨ve reduction [188, 167], are quite inefficient. Instead,
low-dimensional structure lies in the change from prior to posterior, due to the data
being informative, relative to the prior, only about a low-dimensional subspace of Rğ‘.
This fact justifies the choice of the approximation class â„³ğ‘
ğ‘Ÿfor Î“ğ‘|ğ‘Œin (4.6). The
efficiency of the approximation class â„³ğ‘
ğ‘Ÿis also evident from the sharp decay of the
red curve in Figure 4-6: only a handful of directions in the prior-to-posterior update
are needed for a good approximation of Î“ğ‘|ğ‘Œ.
The optimal approximation of the posterior mean of the QoI as a low-rank linear
97

function of the data, as introduced in Theorem 4.2, also converges very quickly as a
function of the rank of the approximation, as shown in Figure 4-7. Once a low-rank
approximation of the form (4.23) is available, then one can compute an accurate ap-
proximation of ğœ‡ğ‘|ğ‘Œ(ğ‘Œ) for each new realization of the data ğ‘Œby simply performing
a low-rank (ğ‘Ÿ= 20 in this case) matrix-vector product.
4.4.3
A nonlinear QoI
We conclude this section by applying the approximation formulas introduced in this
chapter to a particular case of nonlinear goal-oriented inference. Suppose that we are
only interested in the posterior distribution of the maximum temperature over ğ’Ÿ1
(see Figure 4-2). This is a useful QoI because the material properties of a sensitive
component (e.g., the CPU) might deteriorate above a certain critical temperature
(e.g., [38]). In this case, the QoI Ì‚ï¸€ğ‘:= maxğ’Ÿ1 Î˜0â„is a low-dimensional (in fact scalar-
valued) nonlinear function of the parameters. In general, let us write Ì‚ï¸€ğ‘= J (Î˜0â„) for
some nonlinear function J : Rğ‘›â†’R. Then we can cast the nonlinear goal-oriented
Bayesian inverse problem as
â§
âª
â¨
âª
â©
ğ‘Œ= ğºÎ˜0â„+ â„°
Ì‚ï¸€ğ‘= J (Î˜0â„)
(4.28)
and try to characterize the posterior Ì‚ï¸€ğ‘|ğ‘Œfor a particular realization of the data.
This problem is nontrivial, however, as Ì‚ï¸€ğ‘|ğ‘Œis non-Gaussian and cannot easily be
characterized by just two moments. In the most general case, one needs to resort
to sampling techniques such as MCMC [129] to characterize Ì‚ï¸€ğ‘|ğ‘Œ, or perhaps some
deterministic alternative [196, 246, 83, 197]. Unfortunately, it is still not well under-
stood how to properly adapt these techniques to exploit ultimate goals and bypass
full inference of the parameters (see the offlineâ€“online strategy of [170] for a related
effort in the context of goal-oriented nonlinear Bayesian inference). Though devel-
oping computationally efficient techniques to tackle general problems like (4.28) is of
fundamental importance, in this particular example we can adopt a much simpler,
98

yet effective, approach. Using the notation of this section, notice that the nonlinear
QoI Ì‚ï¸€ğ‘can be written as Ì‚ï¸€ğ‘= ğ‘”(ğ‘), where ğ‘represents the inversion parameters in
the region of interest ğ’Ÿ1, and where ğ‘”(ğ‘¥) = maxğ‘–(ğ‘¥ğ‘–) for all ğ‘¥= (ğ‘¥1, . . . , ğ‘¥ğ‘). Thus
we can rewrite (4.28) as:
â§
âª
âª
âª
âª
âª
â¨
âª
âª
âª
âª
âª
â©
ğ‘Œ= ğºÎ˜0â„+ â„°
ğ‘= ğ’ªÎ˜0â„
Ì‚ï¸€ğ‘= ğ‘”(ğ‘).
(4.29)
Then we can approximate the Gaussian posterior distribution of ğ‘|ğ‘Œusing the goal-
oriented techniques presented in this chapter, and finally we can push forward the
latter distribution through the nonlinear operator ğ‘”to obtain a suitable approxima-
tion of Ì‚ï¸€ğ‘|ğ‘Œ. The nonlinear operator ğ‘”is never approximated in this process. That
is, we first compute the posterior mean and the optimal goal-oriented approximation
of the covariance of ğ‘|ğ‘Œusing the results of Theorem 4.1, then we sample from
the optimal approximating measure Ëœğœˆğ‘|ğ‘Œ:= ğ’©(ğœ‡ğ‘|ğ‘Œ(ğ‘Œ), Ìƒï¸€Î“ğ‘|ğ‘Œ) using the results of
Lemma 4.5, and finally we push forward these samples through ğ‘”: Rğ‘â†’R to ob-
tain approximate samples from the posterior distribution of the nonlinear QoI. We
can easily estimate the quality of these approximate posterior samples using bounds
like (4.14). Note, however, that a bound like (4.14) quantifies only the accuracy of
the posterior moments, and does not yield an explicit measure of distance between
the non-Gaussian posterior distribution of Ì‚ï¸€ğ‘|ğ‘Œand its corresponding approximation.
Nevertheless, the plot on the right of Figure 4-8 shows that the resulting approxima-
tion of the density of Ì‚ï¸€ğ‘|ğ‘Œis indeed quite good for this particular choice of nonlinear
operator ğ‘”.
4.5
Discussion
In this chapter, we developed statistically optimal and computationally efficient ap-
proximations of the posterior statistics of a quantity of interest (QoI) in a goalâ€“
oriented linearâ€“Gaussian inverse problem.
The posterior covariance of the QoI is
99

Î˜âˆ
D1
D3
âƒ—q(t)
âƒ—q = 0
Sensors
âƒ—q = 0
D2
Material
ğ›¼ğ‘–at 20 âˆ˜C
Domain
â€”
m/s2
â€”
Copper
1.11 Ã— 10âˆ’4
ğ’Ÿ1
Silicon
8.8 Ã— 10âˆ’5
ğ’Ÿ2
Aluminum
8.42 Ã— 10âˆ’5
ğ’Ÿ3
Figure 4-2: (left) CPU cooling problem. Inversion for the initial temperature field
on ğ’Ÿ1 given noisy temperature measurements on an aluminum heat sink (ğ’Ÿ3). The
figure shows the problem configuration, the locations of the sensors (black dots),
and the boundary conditions for the heat equation describing time evolution of the
temperature field on the domain ğ’Ÿ:= ğ’Ÿ1 âˆªğ’Ÿ2 âˆªğ’Ÿ3. (right) Material properties of
the different layers.
290
300
310
320
330
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
Figure 4-3: (left) Initial temperature field used to generate synthetic data according
to the problem configuration described in Section 4.4. This temperature field was
not drawn from the prior distribution of Î˜0â„; instead, it corresponds to a finer dis-
cretization of the continuous stochastic process Î˜ evaluated at the initial time. (right)
Normalized difference between the prior and posterior variance of the parameters, i.e.,
(Var(Î˜0â„)âˆ’Var(Î˜0â„|ğ‘Œ))/ Var(Î˜0â„). The regions of greatest relative decrease of the
variance are localized around the sensor locations (black dots).
100

Figure 4-4: Three eigenvectors ( Ì‚ï¸€ğ‘¤ğ‘–) of the matrix pencil (ğ», Î“âˆ’1
pr ) as defined in Theo-
rem 3.1: Ì‚ï¸€ğ‘¤1 (left), Ì‚ï¸€ğ‘¤6 (center), and Ì‚ï¸€ğ‘¤10 (right). These eigenvectors define the prior-
to-posterior update in the optimal approximation (3.11) of the posterior covariance
of the parameters Î“pos. Note that these leading eigenvectors have features near the
locations of the sensors in ğ’Ÿ3. This is the region where the data are most informative
for the parameters, but not necessarily for the QoI.
Figure 4-5: Three vectors (Ìƒï¸€ğ‘ğ‘–) defining the prior-to-posterior update in the optimal
goal-oriented approximation of Î“pos introduced in (4.15) (see Lemma 4.4). In partic-
ular, we show Ìƒï¸€ğ‘1 (left), Ìƒï¸€ğ‘3 (center), and Ìƒï¸€ğ‘5 (right). One can interpret these vectors as
directions in the parameter space that are informed by the data, relative to the prior,
and that are relevant to the QoI. The relevant features of the (Ìƒï¸€ğ‘ğ‘–) are concentrated
around the region of interest (ğ’Ÿ1). These directions should be contrasted with the
modes in Figure 4-4, which are strongly informed by the data but, at the same time,
nearly irrelevant to the QoI.
101

rank of update
0
200
400
600
800
1000
error
10-10
10-5
100
dR(Î“pos, bÎ“pos)
dR(Î“Z|Y, eÎ“Z|Y)
dR(Î“Z|Y, O bÎ“posOâŠ¤)
rank of update
0
50
100
150
200
error
10-15
10-10
10-5
100
105
dR(Î“pos, bÎ“pos)
dR(Î“Z|Y, eÎ“Z|Y)
dR(Î“Z|Y, O bÎ“posOâŠ¤)
Figure 4-6:
(left) Convergence of the covariance approximations in the natural
geodesic distance over the manifold of SPD matrices (see Section 3.2.2). The blue
dotted line shows the distance between the covariance of the parameters Î˜0â„|ğ‘Œ(i.e.,
Î“pos) and its optimal approximation Ì‚ï¸€Î“pos = Î“pr âˆ’ğ¾ğ¾âŠ¤, as a function of the rank of
ğ¾(see Theorem 3.1). The red line shows the distance between Î“ğ‘|ğ‘Œand its optimal
approximation introduced in Theorem 4.1, Ìƒï¸€Î“ğ‘|ğ‘Œ= Î“ğ‘âˆ’ğ¾ğ¾âŠ¤, as a function of the
rank of ğ¾. Finally, the green line shows the distance between Î“ğ‘|ğ‘Œand the subop-
timal approximation (4.5) obtained as ğ’ªÌ‚ï¸€Î“pos ğ’ªâŠ¤. (right) Detail of the figure on the
left, with both axes rescaled.
approximated as a low-rank negative update of the prior covariance of the QoI. Op-
timality holds with respect to the natural geodesic distance on the manifold of sym-
metric positive definite matrices. The posterior mean of the QoI is approximated as
a low-rank function of the data, and optimality follows from the minimization of the
Bayes risk for squared-error loss weighed by the posterior precision matrix of the QoI.
The minimization of this Bayes risk is associated with the minimization of a Rieman-
nian metric averaged over the distribution of the data. These optimal approximations
avoid computation of the full posterior distribution of the parameters and focus only
on directions in the parameter space that are informed by the data and that are rele-
vant to the QoI. These directions are obtained as the leading generalized eigenvectors
of a suitable matrix pencil, and reflect a balance among all the ingredients of the
goalâ€“oriented inverse problem: prior information, the forward model, measurement
noise, and the ultimate goals.
An important avenue for future work is the extension of these optimality results
to the case of nonlinear forward operators. Here, we expect that interpreting the
102

rank approximation
0
10
20
35
50
75
100
150
error
10-6
10-4
10-2
100
102
 Âµ  Z |  Y  
Figure 4-7: The solid curve shows the error associated with the optimal low-rank
approximation of the posterior mean of the QoI, ğœ‡ğ‘|ğ‘Œ(ğ‘Œ), given in Theorem 4.2. The
error is measured as the square root of E
[ï¸‚âƒ¦âƒ¦ğœ‡ğ‘|ğ‘Œ(ğ‘Œ) âˆ’ğ´*ğ‘Œ
âƒ¦âƒ¦2
Î“âˆ’1
ğ‘|ğ‘Œ
]ï¸‚
and is a function
of rank(ğ´*). The top right corner shows ğœ‡ğ‘|ğ‘Œ(ğ‘Œ) for a particular realization of ğ‘Œ
(see Figure 4-3 (left)). The snapshots along the solid curve show the corresponding
approximation ğœ‡ğ‘|ğ‘Œ(ğ‘Œ) â‰ˆğ´* ğ‘Œfor various ranks of ğ´* and for the same realization
of ğ‘Œ. Notice that the approximation of the posterior mean of the QoI is already good
with rank(ğ´*) = 20.
103

index i
0
50
100
150
200
250
300
350
eigenvalues
10-1
100
101
102
103
temperature
322
324
326
328
330
332
334
336
pdf
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
exact
approximate
Figure 4-8:
(left) Eigenvalues of Î“ğ‘|ğ‘Œ. For this problem configuration ğ‘âˆˆR370, so
the matrix Î“ğ‘|ğ‘Œis not low-rank. (right) The blue solid curve shows a kernel density
estimate (KDE) of the exact posterior density of the nonlinear QoI Ì‚ï¸€ğ‘:= max(ğ‘),
i.e., the density of Ì‚ï¸€ğ‘|ğ‘Œ, constructed from 1Ã—106 samples. Notice that this density is
non-Gaussian. The red dotted curve shows a KDE constructed with 1 Ã— 106 samples
from an approximation of Ì‚ï¸€ğ‘|ğ‘Œobtained as follows: First we sample the approximate
measure Ëœğœˆğ‘|ğ‘Œ:= ğ’©(ğœ‡ğ‘|ğ‘Œ(ğ‘Œ), Ìƒï¸€Î“ğ‘|ğ‘Œ) obtained from an optimal approximation, Ìƒï¸€Î“ğ‘|ğ‘Œ,
of Î“ğ‘|ğ‘Œas a 20â€“dimensional low rank update of Î“ğ‘(see Theorem 4.1). Then, we push
forward these samples through the nonlinear goal-oriented operator ğ‘”: Rğ‘â†’R. The
quality of the density approximation is already good for a rankâ€“20 update. These
results are consistent with the theoretical bounds (4.14). (See, in particular, the error
curves in Figure 4-6.)
104

QoI posterior approximation as the result of composing the forward model with a
carefully chosen projection operator, as in [74], may be quite helpful. Relaxing the
Gaussianity assumptions on both the prior distribution and the measurement noise
are also important generalizations of the present work.
Moving forward, we are now ready to investigate a variety of notions of low-
dimensional structure in more general nonlinear non-Gaussian problems. This will
be the subject of the forthcoming Chapter 5 which will undertake a slightly more
general approach and address the characterization of non-Gaussian distributions in
high dimensions, including, of course, the case of posterior distributions arising in the
Bayesian approach to inverse problems.
105

106

Chapter 5
The structure of low-dimensional
couplings
5.1
Introduction
In this chapter we investigate possible sources of low-dimensional structure in non-
linear non-Gaussian Bayesian inverse problems. Our discussion, however, will not be
restricted to inference problems, but will rather address the more general task of inte-
gration with respect to a computationally intractable probability measureâ€”certainly
one of the fundamental challenges of modern statistical inference, particularly in the
Bayesian setting. Many measures of interest are in fact computationally intractable,
in the sense that Monte Carlo integration with independent samples and/or deter-
ministic integration via quadrature rules are not feasible.
Couplingsâ€”in particular, deterministic couplingsâ€”are a natural way to address
this challenge. A deterministic coupling between a â€œtractableâ€ measure ğœˆğœ‚that we
can easily simulate (e.g., a standard Gaussian) and an arbitrary target measure ğœˆğœ‹
immediately renders the latter tractable. Such a coupling is induced by a map ğ‘‡,
called a transport map, that satisfies ğœˆğœ‚( ğ‘‡âˆ’1(â„¬)) = ğœˆğœ‹(â„¬) for all measurable sets â„¬
[271, 31]. The transport map allows any integral
âˆ«ï¸€
ğ‘”dğœˆğœ‹over the target measure to
107

be rewritten as an integral over the reference measure, i.e.,
âˆ«ï¸
ğ‘”dğœˆğœ‹=
âˆ«ï¸
ğ‘”âˆ˜ğ‘‡dğœˆğœ‚,
and thus enables the use of standard integration techniques for ğœˆğœ‚to provide an ele-
gant solution to the inference problem [191, 196, 272]. For instance, given a collection
of i.i.d. samples (ğ‘‹ğ‘–)ğ‘–from the reference measure, we can simply evaluate the trans-
port map to obtain i.i.d. samples (ğ‘‡(ğ‘‹ğ‘–))ğ‘–from the target. The transport map ğ‘‡
can be viewed as a transformation that moves particles: given a collection of samples
from ğœˆğœ‚, ğ‘‡rearranges them in accordance with the new distribution ğœˆğœ‹.
In this chapter we focus on absolutely continuous measures (ğœˆğœ‚, ğœˆğœ‹) on Rğ‘›, for
which the existence of a deterministic coupling, and thus of a transport map ğ‘‡:
Rğ‘›â†’Rğ‘›, is guaranteed [244]. Such a map, however, is seldom unique.
One of the key contributions of this chapter is to establish a link between the con-
ditional independence structure of the reference-target pairâ€”the so-called Markov
properties [161] of (ğœˆğœ‚, ğœˆğœ‹)â€”and the existence of special low-dimensional couplings.
These couplings are induced by transport maps that are (1) sparse or (2) decompos-
able. A sparse map consists of scalar-valued component functions that each depend
only on a few input variables, whereas a decomposable map factorizes as the exact
composition of finitely many functions of low effective dimension (i.e., ğ‘‡= ğ‘‡1âˆ˜Â· Â· Â·âˆ˜ğ‘‡â„“,
where each ğ‘‡ğ‘–differs from the identity map only along a subset of its components).
We also describe conditions on (ğœˆğœ‚, ğœˆğœ‹) that yield (3) low-rank maps, i.e., functions
that depart from the identity only on a linear subspace of the parameter space. These
properties, and their combinations, dramatically reduce the complexity of represent-
ing a transport map and can be deduced before the map is explicitly computed.
The utility of these results is twofold.
First, they make the construction of
couplingsâ€”and hence the complete characterization of complex probability distributionsâ€”
tractable for a large class of inference problems by leveraging several notions of low
dimensionality.
In fact, we will show how to systematically translate features of
the target distribution into low-dimensional properties of the mapâ€”thus providing a
108

powerful framework for harnessing structure of a typical inference problem in com-
putations. Second, they suggest new algorithmic approaches for important classes
of statistical models. For instance, our analysis of sparse triangular maps provides
a general framework for describing continuous and non-Gaussian Markov random
fields, and for exploiting the conditional independence structure of these fields in
computation. Our analysis of decomposable transport maps yields new variational
algorithms for sequential inference in nonlinear and non-Gaussian state space models.
These algorithms characterize the full Bayesian solution to the sequential inference
problemâ€”i.e., the joint posterior distribution of all the states and parameters up to
the current assimilation timeâ€”by means of a decomposable transport map, which is
constructed (recursively) in a single forward pass using local operations only slightly
more complex than regular filtering.
These algorithms can be understood as the
natural generalizationâ€”to the non-Gaussian caseâ€”of the square-root Rauch-Tung-
Striebel Gaussian smoother [226, 210].
This chapter is organized as follows. Section 5.2 reviews the Knothe-Rosenblatt
rearrangement, a key coupling for our analysis, while Section 5.3 briefly recalls some
standard terminology for Markov random fields and graphical models.
The main
results are in Sections 5.4â€“5.6: Section 5.4 addresses the sparsity of triangular trans-
ports, while Section 5.5 introduces and develops the concept of decomposable trans-
port maps for general Markov networks. These two sections can be read independently
(even though the proofs of Section 5.5 rely on some of the results of Section 5.4). Sec-
tion 5.6 specializes the theory of Section 5.5 to state-space models, introducing new
variational algorithms for filtering, smoothing, and sequential parameter inference.
This section is largely self-contained and could be the focus of a reader primarily
interested in algorithmic aspects of sequential inference or data assimilation. Section
5.7 characterizes the notion of low-rank transports. Section 5.8 illustrates aspects of
the theory on two numerical examples: a stochastic volatility model with hyperpa-
rameters and a high-dimensional log-Gaussian Cox model with sparse observations. A
final discussion is presented in Section 5.9. Appendix B collects some technical details
on the Knothe-Rosenblatt rearrangement and its generalizations, while Appendix E
109

contains the proofs of the main results. The material presented in this chapter can
be also found in [259].
5.2
Triangular transport maps: a building block
A foundational transport for our analysis is the Knothe-Rosenblatt (KR) rearrange-
ment on Rğ‘›[234, 154, 34]. For a pair of measures ğœˆğœ‚, ğœˆğœ‹âˆˆM+(Rğ‘›), with densities ğœ‚
and ğœ‹, respectively, the KR rearrangement is the unique monotone increasing lower
triangular measurable map that pushes forward ğœˆğœ‚to ğœˆğœ‹, i.e., ğ‘‡â™¯ğœˆğœ‚= ğœˆğœ‹[48, 34].
Here, monotonicity is with respect to the lexicographic order on Rğ‘›, while uniqueness
is up to ğœˆğœ‚-null sets [34]. A lower triangular map ğ‘‡: Rğ‘›â†’Rğ‘›is a multivariate
function whose ğ‘˜th component depends only on the first ğ‘˜input variables, i.e.,
ğ‘‡(ğ‘¥) =
â¡
â¢â¢â¢â¢â¢â¢â£
ğ‘‡1(ğ‘¥1)
ğ‘‡2(ğ‘¥1, ğ‘¥2)
...
ğ‘‡ğ‘›(ğ‘¥1, ğ‘¥2, . . . ğ‘¥ğ‘›)
â¤
â¥â¥â¥â¥â¥â¥â¦
(5.1)
for some collection of functions (ğ‘‡ğ‘˜) and for all ğ‘¥= (ğ‘¥1, . . . , ğ‘¥ğ‘›).
The distinction between lower, upper, or other more general forms of triangular
map is a matter of convention. We will revisit this important point in Section 5.5.
See Appendix B for a constructive definition of the KR rearrangement based on a
sequence of one-dimensional transports. In our hypothesis, the KR rearrangement is
always a bijection on Rğ‘›, while each map
ğœ‰â†¦â†’ğ‘‡ğ‘˜(ğ‘¥1, . . . , ğ‘¥ğ‘˜âˆ’1, ğœ‰)
(5.2)
is homeomorphic (continuous bijection with continuous inverse), strictly increasing,
and differentiable a.e. [244, 267]. Here, monotonicity with respect to the lexicographic
order is equivalent to each (5.2) being an increasing function. The resulting rearrange-
ment ğ‘‡is far from being a diffeomorphism but is still regular enough to define a useful
110

change of variables, as the following lemma proven in [34] shows.
Lemma 5.1. If ğ‘‡is a KR rearrangement pushing forward ğœˆğœ‚to ğœˆğœ‹, then ğœˆğœ‚-a.e.,
ğ‘‡â™¯ğœ‹(ğ‘¥) = ğœ‹(ğ‘‡(ğ‘¥)) det âˆ‡ğ‘‡(ğ‘¥) = ğœ‚(ğ‘¥),
(5.3)
where det âˆ‡ğ‘‡:= âˆï¸€ğ‘›
ğ‘–=1 ğœ•ğ‘˜ğ‘‡ğ‘˜exists a.e., and where ğ‘‡â™¯ğœ‹is the density of ğ‘‡â™¯ğœˆğœ‹.
In general, det âˆ‡ğ‘‡in (5.3) is not the determinant of the Jacobian of ğ‘‡since the
map may not be differentiable, in which case it would not be possible to define âˆ‡ğ‘‡
in the classical sense; this is why det âˆ‡ğ‘‡is redefined in the lemma. Nevertheless, it
is known that ğ‘‡inherits the same regularity as ğœ‚and ğœ‹, but not more [244, 34]. See
Appendix B for additional remarks on the regularity of the map.
An essential feature of the triangular transport map is its anisotropic dependence
on the input variables. That is, even though each component of the transport map
does not depend on all ğ‘›inputs, the map is still capable of coupling arbitrary prob-
ability distributions. Informally, we can think of the KR rearrangement as imposing
the sparsest possible structure that preserves generality of the couplingâ€”in that the
rearrangement is guaranteed to exist for any ğœˆğœ‚, ğœˆğœ‹âˆˆM+(Rğ‘›). (In fact, the transport
can be defined under much weaker conditions [244].) In Section 5.5, we will show that
the anisotropy of the KR rearrangement is crucial to proving that certain â€œcomplexâ€
(and generally non-triangular) transports can be factorized into compositions of a few
lower-dimensional triangular maps. Thus we can think of the KR rearrangement as
the fundamental building block of a more general class of non-triangular transports.
The KR rearrangement also enjoys many attractive computational features. As
shown in [196, 187], it can be characterized as the unique minimizer of the Kullbackâ€“
Leibler (KL) divergence ğ’ŸKL( ğ‘‡â™¯ğœˆğœ‚|| ğœˆğœ‹) over the cone ğ’¯â–³of monotone increasing
triangular maps. From the perspective of function approximation, parameterizing a
monotone triangular map is straightforward: it suffices to write each component of
the map as
ğ‘‡ğ‘˜(ğ‘¥) = ğ‘ğ‘˜(ğ‘¥1, . . . , ğ‘¥ğ‘˜âˆ’1) +
âˆ«ï¸ğ‘¥ğ‘˜
0
exp (ğ‘ğ‘˜(ğ‘¥1, . . . , ğ‘¥ğ‘˜âˆ’1, ğ‘¡)) dğ‘¡,
(5.4)
111

for some arbitrary functions ğ‘ğ‘˜: Rğ‘˜âˆ’1 â†’R and ğ‘ğ‘˜: Rğ‘˜â†’R [30, 221]. The resulting
transport map is always monotone and invertible. (In contrast, parameterizing gen-
eral classes of monotone non-triangular maps is a difficult task.) The minimization
of ğ’ŸKL( ğ‘‡â™¯ğœˆğœ‚|| ğœˆğœ‹) for a map in ğ’¯â–³and for a pair of nonvanishing target (ğœ‹) and
reference (ğœ‚) densities can be rewritten as [196, 187]:
min
ğ‘‡
âˆ’E
[ï¸ƒ
log ğœ‹(ğ‘‡(ğ‘‹)) +
âˆ‘ï¸
ğ‘˜
log ğœ•ğ‘˜ğ‘‡ğ‘˜(ğ‘‹) âˆ’log ğœ‚(ğ‘‹)
]ï¸ƒ
(5.5)
s.t.
ğ‘‡âˆˆğ’¯â–³,
where the expectation is taken with respect to the reference measureâ€”which is the
law of ğ‘‹.
Two aspects of (5.5) are particularly important. First, for the purpose of opti-
mization, the target density can be replaced with its unnormalized version Â¯ğœ‹. (This
replacement is essential in Bayesian inference, where the posterior normalizing con-
stant is usually unknown.) Second, (5.5) can be treated as a stochastic program and
solved by means of sample-average approximation (SAA) [249] or stochastic approxi-
mation [158, 257, 36]. Recall that the reference measure is a degree of freedom of the
problem and is chosen precisely to make the integration in (5.5) feasible using, for
instance, quadrature, Monte Carlo, or quasi-Monte Carlo methods [79, 233, 84, 83].
Assuming some additional regularity for ğœ‹(e.g., at least differentiability) and us-
ing the monotone parameterization of (5.4), then (5.5) becomes an unconstrained and
differentiable optimization problem. In particular, we can use the gradient of log ğœ‹
to obtain an unbiased estimator for the gradient of (5.5) [130, 107, 8]. Alternatively,
if âˆ‡log ğœ‹is unavailable, we can use the score method [108, 222] to produce an esti-
mator that is still unbiased, but with higher variance. For concreteness, consider the
realization of an i.i.d. sample (ğ‘¥ğ‘–)ğ‘€
ğ‘–=1 from ğœˆğœ‚. Then a SAA of (5.5) reads as:
min
ğ‘‡
âˆ’
ğ‘€
âˆ‘ï¸
ğ‘–=1
(ï¸ƒ
log Â¯ğœ‹(ğ‘‡(ğ‘¥ğ‘–)) +
âˆ‘ï¸
ğ‘˜
log ğœ•ğ‘˜ğ‘‡ğ‘˜(ğ‘¥ğ‘–) âˆ’log ğœ‚(ğ‘¥ğ‘–)
)ï¸ƒ
(5.6)
s.t.
ğ‘‡âˆˆğ’¯â–³,
112

which is now amenable to deterministic optimization techniques. The numerical solu-
tion of (5.6) by means of an iterative optimization method (e.g., BFGS [276]) produces
a sequence of maps Ìƒï¸€ğ‘‡1, Ìƒï¸€ğ‘‡2, . . . that are increasingly better approximations of the KR
rearrangement, in the sense defined by (5.6). In particular, we can interpret ( Ìƒï¸€ğ‘‡ğ‘˜)ğ‘˜as
a discrete time flow that pushes forward the collection of reference samples, (ğ‘¥ğ‘–)ğ‘€
ğ‘–=1,
to the target distribution.
See Figure 5-1 for a simple illustration.
As shown in
[196], the KL divergence ğ’ŸKL( Ìƒï¸€ğ‘‡â™¯ğœˆğœ‚|| ğœˆğœ‹) for an approximate map Ìƒï¸€ğ‘‡can be efficiently
estimated as:
ğ’ŸKL( Ìƒï¸€ğ‘‡â™¯ğœˆğœ‚|| ğœˆğœ‹) â‰ˆ1
2Var
[ï¸ƒ
log Â¯ğœ‹( Ìƒï¸€ğ‘‡(ğ‘‹)) +
âˆ‘ï¸
ğ‘˜
log ğœ•ğ‘˜Ìƒï¸€ğ‘‡ğ‘˜(ğ‘‹) âˆ’log ğœ‚(ğ‘‹)
]ï¸ƒ
,
(5.7)
up to second-order terms, in the limit of ğ’ŸKL( Ìƒï¸€ğ‘‡â™¯ğœˆğœ‚|| ğœˆğœ‹) â†’0, even if the normal-
izing constant of ğœ‹is unknown. This convergence criterion is rather useful for any
variational inference method, and is usually not available for techniques like MCMC.
In the same way, [196] constructs efficient estimators for the normalizing constant
ğ›½:= Â¯ğœ‹/ğœ‹as
^ğ›½= exp E
[ï¸ƒ
log Â¯ğœ‹( Ìƒï¸€ğ‘‡(ğ‘‹)) +
âˆ‘ï¸
ğ‘˜
log ğœ•ğ‘˜Ìƒï¸€ğ‘‡ğ‘˜(ğ‘‹) âˆ’log ğœ‚(ğ‘‹)
]ï¸ƒ
(5.8)
We refer the reader to [212, 187], and Section 6.2, for an alternative construction of
the transport map that is useful when only samples from the target measure are avail-
able. An interesting application of the latter construction is the problem of density
estimation [266] or Bayesian inference with intractable likelihoods [274, 184, 72]. In
this case, it turns out that the inverse transport ğ‘†= ğ‘‡âˆ’1 can be easily computed
via convex optimization [213]. (Notice that ğ‘†is just an ordinary triangular transport
map that pushes forward ğœˆğœ‹to ğœˆğœ‚. The â€œinverseâ€ descriptor will help distinguish ğ‘†
from the map ğ‘‡that pushes forward the reference to the target distribution. We
refer to ğ‘‡as the direct transport.) We can then invert ğ‘†at ğ‘¥âˆˆRğ‘›to obtain the
evaluation of the direct transport ğ‘‡(ğ‘¥). Inverting a monotone triangular function
is a computationally trivial task since it requires the solution of a sequence of one-
113

dimensional root finding problems [212, 187]. In practice, one just needs to invert
(5.2) for ğ‘˜= 1, . . . , ğ‘›. It is also possible to compute the inverse transport from the
unnormalized target density, rather than from samples; here, it suffices to minimize
ğ’ŸKL( ğœˆğœ‚|| ğ‘†â™¯ğœˆğœ‹) for ğ‘†âˆˆğ’¯â–³. The resulting variational problem is equivalent to (5.5)
with the identity ğ‘†= ğ‘‡âˆ’1. By symmetry of our formulation, ğ‘†has the same regu-
larity as ğ‘‡. In particular, Lemma 5.1 holds for ğ‘†as well, and gives a formula for the
pushforward density ğ‘‡â™¯ğœ‚as:
ğ‘‡â™¯ğœ‚(ğ‘§) = ğœ‚(ğ‘†(ğ‘§)) det âˆ‡ğ‘†(ğ‘§) = ğœ‹(ğ‘§),
(5.9)
where det âˆ‡ğ‘†:= âˆï¸€ğ‘›
ğ‘–=1 ğœ•ğ‘˜ğ‘†ğ‘˜exists a.e., and where ğ‘‡â™¯ğœ‚is the density of ğ‘‡â™¯ğœˆğœ‚. There
is a growing body of literature on the efficient numerical approximation of transport
maps (see, e.g., [30, 231, 178, 266, 196]). Essentially all of these approaches employ
numerical optimization to construct or realize the action of a map, and thus harness
optimization to enhance integration. Yet all these approaches face a fundamental
challenge: the transport map is a function from Rğ‘›onto itself, and in high dimensions
(i.e., for large ğ‘›) the representation and approximation of such functions becomes
increasingly intractable. In the ensuing sections, on the other hand, we will show
that a large class of transport maps are in fact only superficially high-dimensional;
that is, they possess some hidden low-dimensional structure that can facilite their fast
and reliable computation. This low-dimensional structure is linked to the Markov
properties of the target measure, which we briefly review in the next section.
5.3
Markov networks
Let ğ‘= (ğ‘1, . . . , ğ‘ğ‘›) be a collection of random variables with law ğœˆğœ‹and density
ğœ‹. We can represent a list of conditional independences satisfied by ğ‘â€”the so-called
Markov propertiesâ€”using a simple undirected graph ğ’¢= (ğ’±, â„°), where each node
ğ‘˜âˆˆğ’±is associated with a distinct random variable, ğ‘ğ‘˜, and where the edges in â„°
encode a specific notion of probabilistic interaction among these random variables
114

3
0
3
3
0
3
6
9
12
15
18
Ìƒï¸€ğ‘‡0
3
0
3
3
0
3
6
9
12
15
18
Ìƒï¸€ğ‘‡1
3
0
3
3
0
3
6
9
12
15
18
Ìƒï¸€ğ‘‡2
3
0
3
3
0
3
6
9
12
15
18
Ìƒï¸€ğ‘‡3
Figure 5-1: Computation of a simple transport map in two dimensions: The leftmost
figure shows contours of the reference density ğœ‚, which is a standard Gaussian, and
of the target density ğœ‹, which is a banana-shaped distribution in the tails of ğœ‚. The
target distribution has a nonlinear dependence structure. The orange dots in the
leftmost figure correspond to 100 samples (ğ‘¥ğ‘–) from ğœ‚and are used to make a sample-
average approximation of (5.5). We adopt the triangular monotone parameterization
of (5.4) for the candidate transport map, where the functions ğ‘ğ‘˜, ğ‘ğ‘˜are expanded
in a multivariate Hermite polynomial basis of total degree two [277]. The resulting
optimization problem is solved with a quasi-Newton method (BFGS). The ğ‘˜th figure
from the left shows the pushforward of the original reference samples through the
approximate transport map, Ìƒï¸€ğ‘‡ğ‘˜, after ğ‘˜iterations of BFGS. The initial map Ìƒï¸€ğ‘‡0 is
chosen to be the identity. The reference samples flow collectively towards the target
density and eventually settle on the support of ğœ‹, capturing its structure after just a
few iterations.
[156]. In particular, we say that ğ‘is a Markov networkâ€”or a Markov random field
(MRF)â€”with respect to ğ’¢if for any triplet ğ’œ, ğ’®, â„¬of disjoint subsets of ğ’±, where
ğ’®is a separator set for ğ’œand â„¬,1 the subcollections ğ‘ğ’œand ğ‘â„¬are conditionally
independent given ğ‘ğ’®, i.e.,
ğ‘ğ’œâŠ¥âŠ¥ğ‘â„¬| ğ‘ğ’®.
(5.10)
The measure ğœˆğœ‹is said to satisfy the global Markov property, relative to ğ’¢, if (5.10)
holds [161]. We can also say that ğœˆğœ‹is globally Markov with respect to ğ’¢. The
1 ğ’®is a separator set for ğ’œand â„¬if (1) ğ’®is disjoint from ğ’œand â„¬(2) Every path from ğ›¼âˆˆğ’œ
to ğ›½âˆˆâ„¬intersects ğ’®. If ğ’œand â„¬are disconnected components of ğ’¢, then ğ’®= âˆ…is a separator set
for ğ’œand â„¬.
115

corresponding graph is then called an independence map (I-map) for ğœˆğœ‹[156].
Intuitively, a sparse graph represents a family of distributions that enjoy many
conditional independence properties. I-maps are in general not unique. Of particular
interest are minimal I-maps, i.e., the sparsest graphs compatible with the conditional
independence structure of ğœˆğœ‹.
Conditional independence is associated with factorization properties of ğœ‹. For
instance, ğ‘ğ’œâŠ¥âŠ¥ğ‘â„¬| ğ‘ğ’®if and only if ğœ‹ğ‘ğ’œ,ğ‘â„¬|ğ‘ğ’®= ğœ‹ğ‘ğ’œ|ğ‘ğ’®ğœ‹ğ‘â„¬|ğ‘ğ’®a.e. [161].
We
then say that ğœˆğœ‹factorizes according to some graph ğ’¢if there exists a version of the
density of ğœˆğœ‹such that
ğœ‹(ğ‘§) = 1
c
âˆï¸
ğ’âˆˆğ’
ğœ“ğ’(ğ‘§ğ’),
(5.11)
for some nonnegative functions (ğœ“ğ’) called potentials, where ğ’is the set of maximal
cliques2 of ğ’¢and c is a normalizing constant. It is immediate to show that if ğœˆğœ‹
factorizes according to ğ’¢, then ğœˆğœ‹satisfies the global Markov property relative to ğ’¢
[161, Proposition 3.8]. The converse is true only under additional assumptions: for
instance, if ğœˆğœ‹admits a continuous and strictly positive density (see the Hammersley-
Clifford theorem [118, 161]).
A critical question then is how to characterize a suitable I-map for a given measure.
There are several answers. First of all, in many applications that involve probabilistic
modeling, the target distribution is defined in terms of its potentials, as in (5.11),
because this is just a more convenient way to specify a high-dimensional distribution
and to perform inference (or general probabilistic reasoning) with it [156]. Finding
a graph for which ğœˆğœ‹factorizes is then a trivial task. See Figure 5-4 (left) for an
example. Applications where this commonly holds range from spatial statistics and
image analysis to speech recognition [156, 236].
In Section 5.6, for example, we
focus exclusively on discrete-time Markov processes, where the Markov structure of
the problem is self-evident. In other settings, the graph is unknown and must be
estimated. When only samples from ğœˆğœ‹are available, this is a question of model
learning, as described in [156, Part III]; see also [134, 189, 280, 173] for various
2 A clique is a fully connected subset of the vertices, whereas a maximal clique is a clique that is
not a strict subset of another clique.
116

applications.
In case of a known and smooth target density, we can characterize
pairwise conditional independence in terms of mixed second-order partial derivatives,
as shown by the following lemma.
Lemma 5.2 (Pairwise conditional independence). If ğ‘âˆ¼ğœˆğœ‹for a measure ğœˆğœ‹with
smooth and strictly positive density ğœ‹, we have:
ğ‘ğ‘–âŠ¥âŠ¥ğ‘ğ‘—| ğ‘ğ’±âˆ–(ğ‘–,ğ‘—)
â‡â‡’
ğœ•2
ğ‘–,ğ‘—log ğœ‹= 0 oğ‘›Rğ‘›.
(5.12)
Thus, if we can evaluate ğœ‹and its derivatives (up to a normalizing constant),
we can use Lemma 5.2 to assess pairwise conditional independence and to define a
minimal I-map for ğœˆğœ‹as follows: add an edge between every pair of distinct nodes un-
less the corresponding random variables are conditionally independent [156, Theorem
4.5].
Regardless of the many ways to obtain an I-map, there is a fundamental connection
between Markov properties of a distribution and the existence of low-dimensional
transport maps. The rest of the chapter will elaborate precisely on this connection.
5.4
Sparsity of triangular transport maps
We begin our investigation of low dimensional structure by considering the notion
of sparse transport map. A sparse map is a multivariate function where each com-
ponent does not depend on all of its input variables. According to this definition, a
triangular transport is already sparse. In this section, however, we show that the KR
rearrangement can be even sparser, depending on the Markov structure of the target
distribution.
5.4.1
Sparsity bounds
Given a lower triangular function ğ‘‡, we define its sparsity pattern, Iğ‘‡, as the set of
all integer pairs (ğ‘—, ğ‘˜), with ğ‘—< ğ‘˜, such that the ğ‘˜th component of the map does
not depend on the ğ‘—th input variable, i.e., Iğ‘‡= {(ğ‘—, ğ‘˜) : ğ‘—< ğ‘˜, ğœ•ğ‘—ğ‘‡ğ‘˜= 0}. (We do
117

not include pairs ğ‘—> ğ‘˜in the definition of Iğ‘‡since, for a lower triangular function,
ğœ•ğ‘—ğ‘‡ğ‘˜= 0 for ğ‘—> ğ‘˜by construction.)
Knowing the sparsity pattern of the KR rearrangement before computing the
actual transport has important computational implications.
For instance, in the
variational characterization of the transport described in (5.5), we can restrict the
feasible domain to the set of triangular maps with sparsity pattern given by Iğ‘‡,
and still recover the desired KR rearrangement.
That is, if (ğ‘—, ğ‘˜) âˆˆIğ‘‡, we can
parameterize any candidate transport map by removing the dependence on the ğ‘—th
input variable from the ğ‘˜th component of the map. Thus, analyzing the Markov
structure of the target distribution enables the representation and computation of
maps in possibly higher-dimensional settings.
The following theorem, which is the main result of this section, characterizes
bounds on the sparsity patterns of triangular transport maps given an I-map for the
target measure. In the statement of the theorem, we denote the direct transport by
ğ‘‡and the inverse transport by ğ‘†= ğ‘‡âˆ’1 (see Section 5.2). The theorem suggests that
ğ‘†and ğ‘‡can have quite different sparsity patterns.3
Theorem 5.1 (Sparsity of Knotheâ€“Rosenblatt rearrangements). Let ğ‘‹âˆ¼ğœˆğœ‚, ğ‘âˆ¼
ğœˆğœ‹with ğœˆğœ‚, ğœˆğœ‹âˆˆM+(Rğ‘›) and ğœˆğœ‚tensor product measure. Moreover, assume that ğœˆğœ‹
is globally Markov with respect to ğ’¢, and define, recursively, the sequence of graphs
(ğ’¢ğ‘˜)ğ‘›
ğ‘˜=1 as: (1) ğ’¢ğ‘›:= ğ’¢and (2) for all 1 â‰¤ğ‘˜< ğ‘›, ğ’¢ğ‘˜âˆ’1 is obtained from ğ’¢ğ‘˜by
removing node ğ‘˜and by turning its neighborhood Nb(ğ‘˜, ğ’¢ğ‘˜) into a clique. Then the
following hold:
1. If Iğ‘†is the sparsity pattern of the inverse transport map ğ‘†, then
Ì‚ï¸€Iğ‘†âŠ‚Iğ‘†,
(5.13)
where Ì‚ï¸€Iğ‘†is the set of integer pairs (ğ‘—, ğ‘˜) such that ğ‘—/âˆˆNb(ğ‘˜, ğ’¢ğ‘˜).
3 A note: as we already saw, the KR rearrangement is unique up to a set of measure zero.
Theorem 5.1 characterizes the sparsity pattern of a particular version of the map, the one given by
Definition B.2 in Appendix B. We will implicitly make this assumption throughout the chapter.
118

2. If Iğ‘‡is the sparsity pattern of the direct transport map ğ‘‡, then
Ì‚ï¸€Iğ‘‡âŠ‚Iğ‘‡,
(5.14)
where Ì‚ï¸€Iğ‘‡is defined recursively as follows: for ğ‘˜= 2, . . . , ğ‘›the pair (ğ‘—, ğ‘˜) âˆˆÌ‚ï¸€Iğ‘‡
if and only if (ğ‘—, ğ‘–) âˆˆÌ‚ï¸€Iğ‘‡for all ğ‘–âˆˆNb(ğ‘˜, ğ’¢ğ‘˜).
3. The predicted sparsity pattern of ğ‘†is always greater than or equal to that of ğ‘‡,
i.e.,
Ì‚ï¸€Iğ‘‡âŠ‚Ì‚ï¸€Iğ‘†.
(5.15)
Several remarks are in order. First, we emphasize the fact that Theorem 5.1 char-
acterizes sparsity patterns using only an I-map for ğœˆğœ‹, without requiring any actual
computation of the transports. One only needs to perform simple graph operations
on ğ’¢to build the sequence of graphs (ğ’¢ğ‘˜). See Figure 5-2 for an illustration of this
procedure, with the corresponding sparsity patterns in Figure 5-3. We refer to (ğ’¢ğ‘˜)
as the marginal graphs. In fact, the sequence (ğ’¢ğ‘˜) is precisely the set of intermediate
graphs produced by the variable elimination algorithm [156, Ch. 9], when marginal-
izing with elimination ordering (ğ‘›, ğ‘›âˆ’1, . . . , 1). This should not be surprising as the
KR rearrangement is essentially a sequence of ordered marginalizations [271]. Also,
the hypothesis that ğœˆğœ‚is a tensor product measure is not restrictive since the reference
distribution is a degree of freedom of the problem.
We note that Theorem 5.1 does not provide the exact sparsity patterns of the
triangular transport maps; instead, (5.13) and (5.14) provide subsets of Iğ‘‡and Iğ‘†.
In other words, the actual transport maps might be sparser than predicted by the sets
Ì‚ï¸€Iğ‘†and Ì‚ï¸€Iğ‘‡â€”but, crucially, they cannot be less sparse. Thus, we can think of Theorem
5.1 as providing bounds on the sparsity of triangular transports. An important fact
is that, without additional information on ğœˆğœ‹, these bounds are sharp. That is, we
can always find a pair of measures (ğœˆğœ‚, ğœˆğœ‹) satisfying the hypotheses of Theorem 5.1
and such that the predicted and actual sparsity patterns coincide, i.e., Ì‚ï¸€Iğ‘‡= Iğ‘‡or
Ì‚ï¸€Iğ‘†= Iğ‘†.
119

Part 3 of Theorem 5.1 shows that the predicted sparsity pattern of the inverse
KR rearrangement is always larger than or equal to that of the direct transport, i.e.,
Ì‚ï¸€Iğ‘‡âŠ‚Ì‚ï¸€Iğ‘†. This does not mean that for every pair of measures (ğœˆğœ‚, ğœˆğœ‹), the inverse
triangular transport is always at least as sparse as the direct transport; in fact, it is
possible to provide simple counterexamples. However, this result does imply that if
we are only given an I-map for ğœˆğœ‹, then parameterizing candidate inverse triangular
transports allows the imposition of more sparsity constraints than parameterizing
candidate direct transports. In general, sparser transports are easier to represent.
See Figure 5-4 (right) for a nontrivial example of sparsity patterns for a stochastic
volatility model.
Indeed, (5.15) hints at a typical trend: inverse transport maps tend to be sparser
(in many practical cases, much sparser) than their direct counterparts. Intuitively, the
sparsity of a direct transport is associated with marginal independence in ğ‘, whereas
the inverse transport inherits sparsity from the conditional independence structure
of ğ‘. The latter is a weaker condition than mutual independence; for instance, the
correlation length of a process modeled by a Markov random field may be much larger
than the typical neighborhood size [236, 32]. Thus, given a sparse I-map for the target
measure, it can be computationally advantageous to characterize an inverse transport
rather than a direct one, because the inverse transport can inherit a larger sparsity
pattern. Given an inverse triangular transport ğ‘†, we can then easily evaluate the
direct transport ğ‘‡= ğ‘†âˆ’1 at any point ğ‘¥âˆˆRğ‘›by inverting ğ‘†pointwise, as described
in Section 5.2.
There is no need to have an explicit representation of the direct
transport as long as it can be implicitly defined through its inverse.
5.4.2
Connection to Gaussian Markov random fields
The reader familiar with Gaussian Markov random fields (GMRFs), might see links
between the preceding results and widespread approaches to the modeling of Gaussian
fields. In this section, we clarify the extent of these connections.
Many applications (e.g., image analysis, spatial statistics, time series) involve
modeling by means of high-dimensional Gaussian fields. Dealing with large and dense
120

3
2
5
1
4
ğ’¢5
3
2
5
1
4
ğ’¢4
3
2
5
1
4
ğ’¢3
5
2
4
1
3
ğ’¢2
Figure 5-2: Sequence of graphs (ğ’¢ğ‘˜) described in Theorem 5.1 for a target measure
in M+(R5) with I-map illustrated by the leftmost graph, ğ’¢5. Notice that to generate
the graph ğ’¢2, we remove node 3 from ğ’¢3 and turn its neighborhood into a clique by
adding the edge (1, 2).
Pğ‘—ğ‘˜= ğœ•ğ‘—ğ‘†ğ‘˜
Pğ‘—ğ‘˜= ğœ•ğ‘—ğ‘‡ğ‘˜
Figure 5-3: Sparsity patterns predicted by Theorem 5.1 for the target measure an-
alyzed in Figure 5-2. We represent the sparsity patterns using a symbolic matrix
notation: the (ğ‘—, ğ‘˜)-th entry of the matrix is not colored if the ğ‘˜th component of the
map (ğ‘†or ğ‘‡) does not depend on the ğ‘—th input variable, or, equivalently, if (ğ‘—, ğ‘˜) âˆˆÌ‚ï¸€Iğ‘†
(resp. Ì‚ï¸€Iğ‘‡) (5.13). (Since we are considering lower triangular transports, all entries
ğ‘—> ğ‘˜are uncolored. Note also that ğ‘†ğ‘˜and ğ‘‡ğ‘˜are always functions of their ğ‘˜th input
by strict monotonicity of the map.) The predicted sparsity pattern for the direct
transport in this example is Ì‚ï¸€Iğ‘‡= âˆ….
covariances, however, is often impractical; both storage and sampling of the Gaussian
field are problematic. The usual workaround is to replace or approximate the Gaussian
field with a sparse GMRFâ€”i.e., a Gaussian Markov network that enforces locality in
the probabilistic interactions among the underlying random variables. The minimal
I-map for the GMRF is thus sparse, and so is the precision matrix Î› of the field [236].
The covariance matrix is still in general dense, but dealing with the sparse precision
matrix is much easier. If ğ¿ğ¿âŠ¤is a (sparse) Cholesky decomposition of Î›, then ğ¿âŠ¤
represents a linear triangular transport that pushes forward the joint distribution of
the GMRF, ğœˆğœ‹= ğ’©(0, Î›âˆ’1), to a standard normal, ğœˆğœ‚= ğ’©(0, I). The key point
is that for many Markov structures of interest, the Cholesky factor inherits sparsity
from the underlying graph, so that sampling from ğœˆğœ‹can be achieved at low cost as
121

ğ‘0
ğ‘1
ğ‘2
ğ‘3
ğ‘ğ‘
ğœ‡
ğœ‘
Pğ‘—ğ‘˜= ğœ•ğ‘—ğ‘†ğ‘˜
Figure 5-4:
(left) Markov network for a stochastic volatility model in [149]. Blue
nodes represent the discrete-time latent log-volatility process (ğ‘ğ‘˜)ğ‘
ğ‘˜=0, which obeys a
simple autoregressive model with hyperparameters ğœ‡, ğœ‘. The graph above is a min-
imal I-map for the posterior density described in Section 5.8.1, ğœ‹ğœ‡,ğœ‘,ğ‘0:ğ‘|ğ‘¦0:ğ‘, where
ğ‘¦0:ğ‘are some (fixed) observations. (right) The predicted sparsity pattern Ì‚ï¸€Iğ‘†(only
the top 6 Ã— 6 block is shown) for the inverse transport corresponding to the model
on the left: the first column/row of the matrix refer jointly to all of the hyperpa-
rameters. Each component ğ‘†ğ‘˜of the inverse transport can depend at most on four
input variables, namely ğœ‡, ğœ‘, ğ‘ğ‘˜âˆ’1, ğ‘ğ‘˜, regardless of the overall dimension ğ‘of the
problem. In order to apply the results of Theorem 5.1, we must select an ordering of
the input variables; here, we used the ordering (ğœ‡, ğœ‘, ğ‘0, . . . , ğ‘ğ‘). Optimal orderings
are further discussed in Section 5.4.3.
follows: if ğ‘‹is a sample from ğœˆğœ‚, then we can obtain a sample ğ‘from ğœˆğœ‹simply by
solving the sparse triangular linear system ğ¿âŠ¤ğ‘= ğ‘‹. There is no need to explicitly
represent or store the dense factor ğ¿âˆ’âŠ¤, since we can implicitly represent its action
by inverting a sparse triangular function.
Now the connection with Section 5.4 is clear: ğ¿âŠ¤is an inverse triangular trans-
port,4 while ğ¿âˆ’âŠ¤is a direct one.
Moreover, solving a triangular linear system is
just a particular instance of inverting a nonlinear triangular function by performing
a sequence of one-dimensional root-findings. Thus the developments of the previous
section, which consider arbitrary nonlinear maps, are a natural generalizationâ€”to
the non-Gaussian caseâ€”of modeling and sampling techniques for high-dimensional
GMRFs [236].
4Actually, this transport is upper rather than lower triangular. This distinction plays no role
in the following discussion, and the fact that a KR rearrangement is a lower triangular function is
merely a matter of convention. We will return to this important point in Section 5.5.
122

5.4.3
Ordering of triangular maps
The results of Theorem 5.1 suggest that the sparsity of a triangular transport map
depends on the ordering of the input variables. See Figure 5-5 for a simple illustration.
Indeed, the triangular transport itself depends anisotropically on the input variables
and requires the definition of a proper ordering. A natural approach is then to seek
the ordering that promotes the sparsest transport map possible.
Consider a pair of measures (ğœˆğœ‚, ğœˆğœ‹) that satisfies the hypotheses of Theorem
5.1. We associate an ordering of the input variables with a permutation ğœon Nğ‘›=
{1, . . . , ğ‘›}, and define the reordered target measure ğœˆğœ
ğœ‹as the pushforward of ğœˆğœ‹by the
matrix ğ‘„ğœthat represents the permutation ğœ. In particular, (ğ‘„ğœ)ğ‘–ğ‘—= (ğ‘’ğœ(ğ‘–))ğ‘—, where
ğ‘’ğ‘–is the ğ‘–th standard basis vector on Rğ‘›. Moreover, if ğ’¢is an I-map for ğœˆğœ‹, then
we denote an I-map for ğœˆğœ
ğœ‹by ğ’¢ğœ. Notice that ğ’¢ğœcan be derived from ğ’¢simply by
relabeling its nodes according to the permutation ğœ. Then we can cast a variational
problem for the best ordering ğœ* as:
ğœ* âˆˆ
arg maxğœ
|Iğ‘†|
(5.16)
s.t.
ğ‘†â™¯ğœˆğœ
ğœ‹= ğœˆğœ‚
ğœâˆˆP(Nğ‘›),
where ğ‘†is the KR rearrangement that pushes forward the reordered target ğœˆğœ
ğœ‹to ğœˆğœ‚
and P(Nğ‘›) is the set of permutations of Nğ‘›. The goal is to maximize the cardinality
of the sparsity pattern of the inverse map, |Iğ‘†|. We restrict our attention to the
sparsity of the inverse transport, since we know from Section 5.4 that the direct
transport tends to be dense, even for the most trivial Markov structures.
Ideally, we would like to determine a good ordering for the map before computing
the actual transport, and to use the resulting information about the sparsity pattern to
simplify the optimization problem for ğ‘†. However, evaluating the objective function
of (5.16) requires computing a different inverse transport for each permutation ğœ.
One possible way to relax (5.16) is to replace Iğ‘†with the predicted sparsity pattern
Ì‚ï¸€Iğ‘†introduced in (5.13). The advantage of this approach is that the objective function
123

of the relaxed problem can now be evaluated in closed form without computing any
transport map, but rather by performing the simple sequence of graph operations on
ğ’¢ğœdescribed by Theorem 5.1. The caveat is that, in general, Ì‚ï¸€Iğ‘†âŠ‚Iğ‘†, and thus
maximizing |Ì‚ï¸€Iğ‘†| amounts to seeking the tightest lower bound on the sparsity pattern
of the inverse transport. From the definition of Ì‚ï¸€Iğ‘†, it follows that the best ordering
ğœ* for the relaxed problem is one that introduces the fewest edges in the construction
of the marginal graphs ğ’¢ğ‘›, . . . , ğ’¢1, whenever ğ’¢ğ‘›= ğ’¢ğœ*. Thus, for a given I-map ğ’¢,
we denote by F(ğœ; ğ’¢) the fill-in produced by the ordering ğœ. That is, F(ğœ; ğ’¢) is a set
containing all the edges introduced in the construction of the marginal graphs (ğ’¢ğ‘˜)
from ğ’¢ğœ. A computationally feasible relaxation of (5.16) is then given by:
ğœ* âˆˆ
arg minğœ
|F(ğœ; ğ’¢)|
(5.17)
s.t.
ğœâˆˆP(Nğ‘›) .
(5.17) is a standard problem in graph theory; it arises in a variety of practical settings,
including (most relatedly) finding the best elimination ordering for variable elimina-
tion in graphical models [156], or finding the permutation that minimizes the fill-in of
the Cholesky factor of a positive definite matrix [105, 240]. From an algorithmic point
of view, (5.17) is NP-complete [279]. This should not be surprising, as bestâ€“ordering
problems are typically combinatorial in nature. Nevertheless, given its widespread
applicability, a host of effective polynomial-time heuristics for (5.17) have been devel-
oped in past years (e.g., min-fill or weighted-min-fill [156]). Most importantly, (5.17)
can be solved without ever touching the target measure (assuming, of course, that
an I-map ğ’¢for ğœˆğœ‹is known). As a result, the cost of finding a good ordering is
often negligible compared to the cost of characterizing a nonlinear transport map via
optimization.
124

3
2
5
1
4
ğ’¢
Pğ‘–ğ‘—= ğœ•ğ‘–ğ‘†ğ‘—
5
2
3
1
4
ğ’¢â€²
Pâ€²
ğ‘–ğ‘—= ğœ•ğ‘–ğ‘†ğ‘—
Figure 5-5:
Illustration of how a simple re-ordering of the input variables can change
the (predicted) sparsity pattern of the inverse map. On the left, ğ’¢represents an I-
map for the target measure considered in Figure 5-2, with ordering (ğ‘1, ğ‘2, ğ‘3, ğ‘4, ğ‘5),
together with its sparsity pattern Ì‚ï¸€Iğ‘†. (See Figure 5-3 for details on the â€œmatrixâ€ rep-
resentation of sparsity patterns.) On the right, ğ’¢â€² is an I-map for the same target
measure but with the ordering (ğ‘1, ğ‘2, ğ‘5, ğ‘4, ğ‘3). The corresponding sparsity pat-
tern Ì‚ï¸€Iğ‘†â€² is now the empty set.
5.5
Decomposability of transport maps
Thus far, we have investigated the sparsity of triangular transport maps and found
that inverse transports tend to inherit sparsity from the underlying Markov structure
of the target measure. Though direct triangular transports also inherit some sparsity
according to Theorem 5.1, they tend to be more dense.
This section shows that direct transports enjoy a different form of low-dimensional
structure: decomposability. A decomposable transport map is a function that can be
written as the composition of a finite number of low-dimensional maps, e.g., ğ‘‡=
ğ‘‡1 âˆ˜Â· Â· Â· âˆ˜ğ‘‡â„“for some integer â„“â‰¥2. We use a very specific notion of low-dimensional
map, as follows.
Definition 5.1 (Low-dimensional map with respect to a set). A map ğ‘€: Rğ‘›â†’Rğ‘›
is low-dimensional with respect to a nonempty set ğ’âŠ‚ğ’±â‰ƒNğ‘›if
1. ğ‘€ğ‘˜(ğ‘¥) = ğ‘¥ğ‘˜for ğ‘˜âˆˆğ’
2. ğœ•ğ‘—ğ‘€ğ‘˜= 0 for ğ‘—âˆˆğ’and ğ‘˜âˆˆğ’±âˆ–ğ’.
The effective dimension of ğ‘€is the minimum cardinality |ğ’±âˆ–ğ’| over all sets ğ’with
respect to which ğ‘€is low-dimensional.
125

In particular, up to a permutation of its components, we can rewrite ğ‘€as:
ğ‘€(ğ‘¥) =
â¡
â£ğ‘€Â¯ğ’(ğ‘¥Â¯ğ’)
ğ‘¥ğ’
â¤
â¦,
(5.18)
where Â¯ğ’= ğ’±âˆ–ğ’denotes the complement of ğ’in ğ’±, and where for any map ğ‘€and set
ğ’œ= {ğ‘1, . . . , ğ‘ğ‘˜}, ğ‘€ğ’œdenotes the multivariate function ğ‘¥â†¦â†’(ğ‘€ğ‘1(ğ‘¥), . . . , ğ‘€ğ‘ğ‘˜(ğ‘¥))
obtained by stacking together the components of ğ‘€with index in ğ’œ. Thus ğ‘€is the
trivial embedding of a | Â¯ğ’|-dimensional function into the identity map and has effec-
tive dimension bounded by | Â¯ğ’| < ğ‘›. It is not surprising, then, that a decomposable
transport ğ‘‡= ğ‘‡1 âˆ˜Â· Â· Â· âˆ˜ğ‘‡â„“should be easier to represent than an ordinary map. A
perhaps less intuitive feature, however, is that the computation of a high-dimensional
decomposable transport can be broken down into multiple simpler steps, each associ-
ated with the computation of a low-dimensional map ğ‘‡ğ‘—that accounts only for local
features of the target measure.
The forthcoming analysis will consider general, and hence possibly non-triangular,
transports. Thus its scope is much broader than that of Section 5.4, where we only
focused on the sparsity of triangular transports. Yet, we will show that triangular
maps are the building block of decomposable transports. The cornerstone of our anal-
ysis is Theorem 5.2, which characterizes the existence and structure of decomposable
transports given only the Markov structure of the underlying target measure.
Our discussion will proceed in two stages: first, we show how to identify direct
transports that decompose into two maps, i.e., ğ‘‡= ğ‘‡1 âˆ˜ğ‘‡2, and then we explain
how to apply this result recursively to obtain a general decomposition of the form
ğ‘‡= ğ‘‡1 âˆ˜Â· Â· Â· âˆ˜ğ‘‡â„“.
5.5.1
Preliminary notions
Before addressing the decomposability of transport maps, we need to introduce two
useful concepts: proper graph decompositions and generalized triangular functions.
The decomposition of a graph is a standard notion [161].
126

Definition 5.2 (Proper graph decomposition). Given a graph ğ’¢= (ğ’±, â„°), a triple
(ğ’œ, ğ’®, â„¬) of disjoint subsets of the vertex set ğ’±forms a proper decomposition of ğ’¢if
(1) ğ’±= ğ’œâˆªğ’®âˆªâ„¬, (2) ğ’œand â„¬are nonempty, (3) ğ’®separates ğ’œfrom â„¬, and (4) ğ’®
is a clique.
See Figure 5-6 (top left) for an example of a decomposition. Clearly, not every
graph admits a proper decomposition; for instance, a fully connected graph does not
have a separator set for nonempty ğ’œand â„¬. The idea we will pursue here is that
graph decompositions lead to the existence of decomposable transports.
The notion of a generalized triangular function is perhaps less standard, but still
relatively straightforward:
Definition 5.3 (Generalized triangular function). A function ğ‘‡: Rğ‘›â†’Rğ‘›is said to
be generalized triangular, or simply ğœ-triangular, if there exists a permutation ğœof Nğ‘›
such that the ğœ(ğ‘˜)th component of ğ‘‡depends only on the variables ğ‘¥ğœ(1), . . . , ğ‘¥ğœ(ğ‘˜),
i.e., ğ‘‡ğœ(ğ‘˜)(ğ‘¥) = ğ‘‡ğœ(ğ‘˜)(ğ‘¥ğœ(1), . . . , ğ‘¥ğœ(ğ‘˜)) for all ğ‘¥= (ğ‘¥1, . . . , ğ‘¥ğ‘›) and for all ğ‘˜= 1, . . . , ğ‘›.
We can think of a generalized triangular function as a map that is lower triangular
up to a permutation. In particular, if ğœis the identity on Nğ‘›, then a ğœ-triangular
function is simply a lower triangular map (see Section 5.2). To represent the permu-
tation ğœ, we use the notation ğœ({ğ‘–1, . . . , ğ‘–ğ‘˜}) = {ğœ(ğ‘–1), . . . , ğœ(ğ‘–ğ‘˜)} to denote an ordered
set that collects the action of the permutation on the elements (ğ‘–ğ‘—). For example, if
ğ‘‡: R4 â†’R4 is a ğœ-triangular map with ğœdefined as ğœ(N4) = {1, 4, 2, 3}, then ğ‘‡will
be of the form:
ğ‘‡(ğ‘¥) =
â¡
â¢â¢â¢â¢â¢â¢â£
ğ‘‡1(ğ‘¥1)
ğ‘‡2(ğ‘¥1, ğ‘¥4, ğ‘¥2)
ğ‘‡3(ğ‘¥1, ğ‘¥4, ğ‘¥2, ğ‘¥3)
ğ‘‡4(ğ‘¥1, ğ‘¥4)
â¤
â¥â¥â¥â¥â¥â¥â¦
(5.19)
for some collection (ğ‘‡ğ‘˜). We regard each component ğ‘‡ğœ(ğ‘˜) as a map Rğ‘˜â†’R. We say
that a ğœ-triangular function ğ‘‡is monotone increasing if each component ğ‘‡ğ‘˜is a mono-
tone increasing function of the input ğ‘¥ğ‘˜. Moreover, for any ğœˆğœ‚, ğœˆğœ‹âˆˆM+(Rğ‘›) and for
any permutation ğœof Nğ‘›, there exists a (ğœˆğœ‚-unique) monotone increasing ğœ-triangular
127

mapâ€”which we call a ğœ-generalized KR rearrangementâ€”that pushes forward ğœˆğœ‚to
ğœˆğœ‹. We give a constructive definition for a generalized KR rearrangement in Appendix
B.
A key property of a ğœ-generalized KR rearrangement is that it allows different
sparsity patterns to be engineered, depending on ğœ, in a map that is otherwise fully
generalâ€”in the sense of being able to couple arbitrary measures in M+(Rğ‘›). This
feature will be essential to characterizing decomposable transport maps.
5.5.2
Decomposition and graph sparsification
We now characterize transports that decompose into a pair of low-dimensional maps,
as described in the following theorem. We formulate the theorem for a generic target
measure ğœˆğ‘–. Later we will apply the theorem recursively to a sequence (ğœˆğ‘–) of different
targets.
Theorem 5.2 (Decomposition of transport maps). Let ğ‘‹âˆ¼ğœˆğœ‚, ğ‘ğ‘–âˆ¼ğœˆğ‘–, with
ğœˆğœ‚, ğœˆğ‘–âˆˆM+(Rğ‘›) and ğœˆğœ‚tensor product measure. Denote by ğœ‚, ğœ‹ğ‘–a pair of nonvan-
ishing densities for ğœˆğœ‚and ğœˆğœ‹, respectively, and assume that ğœˆğ‘–factorizes according to
a graph ğ’¢ğ‘–, which admits a proper decomposition (ğ’œ, ğ’®, â„¬). Then the following hold:
1. There exists a factorization of ğœ‹ğ‘–of the form
ğœ‹ğ‘–(ğ‘§) = 1
c ğœ“ğ’œâˆªğ’®(ğ‘§ğ’œâˆªğ’®) ğœ“ğ’®âˆªâ„¬(ğ‘§ğ’®âˆªâ„¬),
(5.20)
where ğœ“ğ’œâˆªğ’®is strictly positive and integrable, with c =
âˆ«ï¸€
ğœ“ğ’œâˆªğ’®.
2. For any factorization (5.20) and for any permutation ğœof Nğ‘›with
ğœ(ğ‘˜) âˆˆ
â§
âª
âª
âª
âª
âª
â¨
âª
âª
âª
âª
âª
â©
ğ’®
if ğ‘˜= 1, . . . , |ğ’®|
ğ’œ
if ğ‘˜= |ğ’®| + 1, . . . , |ğ’œâˆªğ’®|
â„¬
otherwise,
(5.21)
there exists a nonempty family, Dğ‘–, of decomposable transport maps ğ‘‡= ğ¿ğ‘–âˆ˜ğ‘…
128

parameterized by ğ‘…âˆˆRğ‘–such that each ğ‘‡âˆˆDğ‘–pushes forward ğœˆğœ‚to ğœˆğ‘–and
where:
(a) ğ¿ğ‘–is a ğœ-generalized KR rearrangement that pushes forward ğœˆğœ‚to a mea-
sure with density ğœ“ğ’œâˆªğ’®(ğ‘§ğ’œâˆªğ’®) ğœ‚ğ‘‹â„¬(ğ‘§â„¬)/c and is low-dimensional with re-
spect to â„¬.
(b) Rğ‘–is the set of maps Rğ‘›â†’Rğ‘›that are low-dimensional with respect to ğ’œ
and that push forward ğœˆğœ‚to the pullback ğ¿â™¯
ğ‘–ğœˆğ‘–âˆˆM+(Rğ‘›).
(c) If ğ‘ğ‘–+1 âˆ¼ğ¿â™¯
ğ‘–ğœˆğ‘–, then ğ‘ğ‘–+1
ğ’œ
âŠ¥âŠ¥ğ‘ğ‘–+1
ğ’®âˆªâ„¬and ğ‘ğ‘–+1
ğ’œ
= ğ‘‹ğ’œin distribution.
(d) ğ¿â™¯
ğ‘–ğœˆğ‘–factorizes according to a graph ğ’¢ğ‘–+1 that can be derived from ğ’¢ğ‘–as
follows:
â€“ Remove any edge from ğ’¢ğ‘–that is incident to any node in ğ’œ.
â€“ For any maximal clique ğ’âŠ‚ğ’®âˆªâ„¬with nonempty intersection ğ’âˆ©ğ’®,
let ğ‘—ğ’be the maximum integer ğ‘—such that ğœ(ğ‘—) âˆˆğ’âˆ©ğ’®and turn
ğ’âˆª{ğœ(1), . . . , ğœ(ğ‘—ğ’)} into a clique.
We first look at the theorem for ğ‘–= 1 and let ğœˆ1 := ğœˆğœ‹and ğ’¢1 := ğ’¢, where
ğœˆğœ‹denotes our usual target measure with I-map ğ’¢and where (ğ’œ, ğ’®, â„¬) denotes a
decomposition of ğ’¢.
Among the infinitely many transport maps from ğœˆğœ‚to ğœˆğœ‹, Theorem 5.2 identifies
a family of decomposable ones. The existence of these maps relies exclusively on the
Markov structure of ğœˆğœ‹: we just require ğ’¢to admit a (proper) decomposition.5
Each transport ğ‘‡âˆˆD1 pushes forward ğœˆğœ‚to ğœˆğœ‹and is the composition of two
low-dimensional maps, i.e., ğ‘‡= ğ¿1 âˆ˜ğ‘…for a fixed ğ¿1 defined in Theorem 5.2[Part
2a] and for some ğ‘…âˆˆR1. (We also write D1 := ğ¿1 âˆ˜R1.6) The structure of these
low-dimensional maps is quite interesting. Up to a reordering of their components,
Theorem 5.2[Parts 2a and 2b] show that ğ¿1 and ğ‘…have an intuitive complementary
5 To obtain a proper decomposition of ğ’¢, one is free to add edges to ğ’¢in order to turn the
separator set ğ’®into a clique (see Definition 5.2); ğœˆğœ‹still factorizes according to any less sparse
version of ğ’¢.
6 The notation here is intuitive: for a given ğ‘”: Rğ‘›â†’Rğ‘›and for a given set of functions â„±from
Rğ‘›to Rğ‘›, ğ‘”âˆ˜â„±denotes the set of maps that can be written as ğ‘”âˆ˜ğ‘“for some ğ‘“âˆˆâ„±.
129

form:
ğ¿1(ğ‘¥) =
â¡
â¢â¢â¢â£
ğ¿ğ’œ
1 (ğ‘¥ğ’®, ğ‘¥ğ’œ)
ğ¿ğ’®
1 (ğ‘¥ğ’®)
ğ‘¥â„¬
â¤
â¥â¥â¥â¦,
ğ‘…(ğ‘¥) =
â¡
â¢â¢â¢â£
ğ‘¥ğ’œ
ğ‘…ğ’®(ğ‘¥ğ’®, ğ‘¥â„¬)
ğ‘…â„¬(ğ‘¥ğ’®, ğ‘¥â„¬)
â¤
â¥â¥â¥â¦.
(5.22)
(If ğ’®= âˆ…, one can just remove ğ¿ğ’®
1 and ğ‘…ğ’®from (5.22), and drop the dependence of
the remaining components on ğ‘¥ğ’®.) In particular, ğ¿1 and ğ‘…have effective dimensions
bounded by |ğ’œâˆªğ’®| and |ğ’®âˆªâ„¬| = |ğ’±âˆ–ğ’œ|, respectively (see Definition 5.1). Even
though ğ¿1 and ğ‘…are low-dimensional maps, their composition is quite denseâ€”in the
sense of Section 5.4â€”and is in general nontriangular:
ğ‘‡(ğ‘¥) = (ğ¿1 âˆ˜ğ‘…)(ğ‘¥) =
â¡
â¢â¢â¢â£
ğ¿ğ’œ
1 ( ğ‘…ğ’®(ğ‘¥ğ’®, ğ‘¥â„¬), ğ‘¥ğ’œ)
ğ¿ğ’®
1 ( ğ‘…ğ’®(ğ‘¥ğ’®, ğ‘¥â„¬) )
ğ‘…â„¬(ğ‘¥ğ’®, ğ‘¥â„¬)
â¤
â¥â¥â¥â¦,
(5.23)
and thus more difficult to represent and to work with. The key idea of decomposable
transports is that they can be represented implicitly through the composition of their
low-dimensional factors, similar to the way that direct transports can be represented
implicitly through their sparse inverses (Section 5.4).
The sparsity patterns of ğ¿1 and ğ‘…in (5.22) are needed for the theorem to hold.
In particular, ğ¿1 must be a ğœ-triangular function with ğœspecified by (5.21). Notice
that (5.21) does not prescribe an exact permutation, but just a few constraints on
a feasible ğœ. Intuitively, these constraints say that ğ¿1 should be a function whose
components with indices in ğ’®depend only on the variables in ğ’®(whenever ğ’®Ì¸= âˆ…),
and whose components with indices in ğ’œdepend only on the variables in ğ’œâˆªğ’®.
Thus, there is usually some freedom in the choice of ğœ. Different permutations lead
to different families of decomposable transports, and can induce different sparsity
patterns in an I-map, ğ’¢2, for ğ¿â™¯
1 ğœˆğœ‹(Theorem 5.2[Part 2d]).
Part 2d of the theorem shows how to derive a possible I-map ğ’¢2â€”not necessarily
minimalâ€”by performing a sequence of graph operations on ğ’¢. There are two steps:
one that does not depend on ğœand one that does. Let us focus first on the former:
130

the idea is to remove from ğ’¢any edge that is incident to any node in ğ’œ, effectively
disconnecting ğ’œfrom the rest of the graph. That is, if ğ‘2 âˆ¼ğ¿â™¯
1ğœˆğœ‹, then, regardless
of ğœ, ğ¿1 makes ğ‘2
ğ’œmarginally independent of ğ‘2
ğ’®âˆªâ„¬by acting locally on ğ’¢. And not
only that: ğ¿1 also ensures that the marginals of ğœˆğœ‚and ğ¿â™¯
1ğœˆğœ‹agree along ğ’œ(see
Theorem 5.2[Part 2c]). Thus we should really interpret ğ¿1 as the first step towards a
progressive transport of ğœˆğœ‚to ğœˆğœ‹. ğ¿1 is a local map: it can depend nontrivially only
upon variables in ğ‘¥ğ’œâˆªğ’®. Indeed, in the most general case, |ğ’œâˆªğ’®| is the minimum
effective dimension of a low-dimensional map necessary to decouple ğ’œfrom the rest
of the graph.
The more edges incident to ğ’œ, the higher-dimensional a transport
is needed.
This type of graph sparsification requires a peculiar â€œblock triangularâ€
structure for ğ¿1 as shown by (5.22): any ğœ-triangular function with ğœgiven by (5.21)
achieves this special structure. The second step of Part 2d shows that if ğ’®Ì¸= âˆ…, then
it might be necessary to add edges to the subgraph ğ’¢ğ’®âˆªâ„¬, depending on ğœ.7 The
relevant aspect of ğœfor this discussion is the definition of the permutation onto the
first |ğ’®| integers. In general, there are |ğ’®|! different permutations that could induce
different sparsity patterns in ğ’¢2. We shall see that permutations that add the fewest
edges possible are of particular relevance.
5.5.3
Recursive decompositions
The sparsity of ğ’¢2 is important because it affects the â€œcomplexityâ€ of the maps in
R1: each ğ‘…âˆˆR1 pushes forward ğœˆğœ‚to ğ¿â™¯
1 ğœˆğœ‹. More specifically, by the previous
discussion, we can see how the role of each ğ‘…âˆˆR1 is really only that of matching
the marginals of ğœˆğœ‚and ğ¿â™¯
1 ğœˆğœ‹along ğ’±âˆ–ğ’œ. A natural question then is whether we
can break this matching step into simpler tasks, or, in the language of this section,
whether R1 contains transports that are further decomposable. Intuitively, we are
seeking a finer-grained representation for some of the transports in R1. The following
lemma (for ğ‘–= 1) provides a positive answer to this question as long as ğ’±âˆ–ğ’œis not
fully connected in ğ’¢2. From now on, we denote (ğ’œ, ğ’®, â„¬) by (ğ’œ1, ğ’®1, â„¬1), since we
7 This is not always the case. For instance, if ğ’®is a subset of every maximal clique of ğ’¢in ğ’®âˆªâ„¬
that has nonempty intersection with ğ’®, then, by Theorem 5.2[Part 2d], no edges need to be added.
131

will be dealing with a sequence of different graph decompositions.
Lemma 5.3 (Recursive decompositions). Let ğœˆğœ‚, ğœˆğ‘–, ğ’¢ğ‘–be defined as in the assump-
tions of Theorem 5.2 for a proper decomposition (ğ’œğ‘–, ğ’®ğ‘–, â„¬ğ‘–) of ğ’¢ğ‘–, while let ğ’¢ğ‘–+1 and
Dğ‘–= ğ¿ğ‘–âˆ˜Rğ‘–be the resulting graph (Part 2d) and family of decomposable transports,8
respectively. Then there are two possibilities:
1. ğ’®ğ‘–âˆªâ„¬ğ‘–is not a clique in ğ’¢ğ‘–+1. In this case, it is possible to identify a proper
decomposition (ğ’œğ‘–+1, ğ’®ğ‘–+1, â„¬ğ‘–+1) of ğ’¢ğ‘–+1 for some ğ’œğ‘–+1 that is a strict superset
of ğ’œğ‘–by (possibly) adding edges to ğ’¢ğ‘–+1 in order to turn ğ’®ğ‘–+1 into a clique.
Let Dğ‘–+1 = ğ¿ğ‘–+1 âˆ˜Rğ‘–+1 be defined as in Theorem 5.2 for the pair of measures
ğœˆğœ‚, ğœˆğ‘–+1 := ğ¿â™¯
ğ‘–ğœˆğ‘–and (ğ’œğ‘–+1, ğ’®ğ‘–+1, â„¬ğ‘–+1). Then the following hold:
(a) Rğ‘–âŠƒDğ‘–+1 and ğ¿ğ‘–âˆ˜Rğ‘–âŠƒğ¿ğ‘–âˆ˜ğ¿ğ‘–+1 âˆ˜Rğ‘–+1.
(b) ğ¿ğ‘–+1 is low-dimensional with respect to ğ’œğ‘–âˆªâ„¬ğ‘–+1 and has effective dimen-
sion bounded by |(ğ’œğ‘–+1 âˆ–ğ’œğ‘–) âˆªğ’®ğ‘–+1|.
(c) Each ğ‘…âˆˆRğ‘–+1 has effective dimension bounded by |ğ’±âˆ–ğ’œğ‘–+1|.
2. ğ’®ğ‘–âˆªâ„¬ğ‘–is a clique in ğ’¢ğ‘–+1. In this case, the decomposition of Part 1 does not
exist.
Lemma 5.3[Part 1] shows that if ğ’®1 âˆªâ„¬1 is not fully connected in ğ’¢2, then there
exists a proper decomposition (ğ’œ2, ğ’®2, â„¬2) of ğ’¢2 (obtained, possibly, by adding edges
to ğ’¢2 in ğ’±âˆ–ğ’œ1) for which ğ’œ2 is a strict superset of ğ’œ1. One can then apply Theorem
5.2 for the pair ğœˆğœ‚, ğœˆ2 = ğ¿â™¯
1 ğœˆ1 and the decomposition (ğ’œ2, ğ’®2, â„¬2). As a result, Part
1a of the lemma shows that R1 contains a subset D2 = ğ¿2 âˆ˜R2 of decomposable
transport maps where both ğ¿2 and each ğ‘…âˆˆR2 are local transports on ğ’±âˆ–ğ’œ1, i.e.,
they are both low-dimensional with respect to ğ’œ1. In particular, ğ¿2 is responsible for
decoupling ğ’œ2 âˆ–ğ’œ1 from the rest of the graph and for matching the marginals of ğœˆğœ‚
and ğ¿â™¯
2 ğ¿â™¯
1 ğœˆğœ‹= (ğ¿1 âˆ˜ğ¿2)â™¯ğœˆğœ‹along ğ’œ2 âˆ–ğ’œ1. The effective dimension of ğ¿2 is bounded
above by the size of the separator set ğ’®2 plus the number of nodes in ğ’œ2 âˆ–ğ’œ1 (Part 1b
8 Whenever we do not specify a permutation ğœğ‘–or a factorization (5.20) in the definition of ğ¿ğ‘–,
it means that the claim holds true for any feasible choice of these parameters.
132

of the lemma). The effective dimension of each ğ‘…âˆˆR2 is bounded by the cardinality
of ğ’±âˆ–ğ’œ2 and is, in the most general case, lower than that of the maps in R1 (Part
1c). Moreover, by Part 1a, D1 = ğ¿1 âˆ˜R1 âŠƒğ¿1 âˆ˜ğ¿2 âˆ˜R2, which means that among the
infinitely many decomposable transports that push forward ğœˆğœ‚to ğœˆğœ‹, there exists at
least one that factorizes as the composition of three low-dimensional maps as opposed
to two, i.e., ğ‘‡= ğ¿1 âˆ˜ğ¿2 âˆ˜ğ‘…for some ğ‘…âˆˆR2. If, on the other hand, ğ’®1 âˆªâ„¬1 is fully
connected in ğ’¢2, then by Lemma 5.3[Part 2] we know that the decomposition of Part
1 does not exist. As a result, we cannot use Theorem 5.2 to prove the existence of
more finely decomposable transports in R1. In other words, if we want to match the
marginals of ğœˆğœ‚and ğ¿â™¯
1 ğœˆğœ‹along ğ’±âˆ–ğ’œ1 = ğ’®1 âˆªâ„¬1, then we must do so in one shot,
using a single transport map; no more intermediate steps are feasible.
The main idea, then, is to apply Lemma 5.3[Part 1], recursively, ğ‘˜times, where ğ‘˜
is the first integer (possibly zero) for which ğ’®ğ‘˜+1 âˆªâ„¬ğ‘˜+1 is a clique in ğ’¢ğ‘˜+2. After ğ‘˜
iterations, the following inclusion must hold:
D1 = ğ¿1 âˆ˜R1 âŠƒğ¿1 âˆ˜Â· Â· Â· âˆ˜ğ¿ğ‘˜+1 âˆ˜Rğ‘˜+1,
(5.24)
which shows that there exists a decomposable transport,
ğ‘‡= ğ¿1 âˆ˜Â· Â· Â· âˆ˜ğ¿ğ‘˜+1 âˆ˜ğ‘…,
(5.25)
for some ğ‘…âˆˆRğ‘˜+1, that pushes forward ğœˆğœ‚to ğœˆğœ‹. (Note that we can apply Lemma
5.3[Part 1] only finitely many times since |ğ’±âˆ–ğ’œğ‘–+1| is an integer function strictly
decreasing in ğ‘–and bounded away from zero.) Each ğ¿ğ‘–in (5.24) is a ğœğ‘–-triangular
map for some permutation ğœğ‘–that satisfies (5.25), and is low-dimensional with respect
to ğ’œğ‘–âˆ’1 âˆªâ„¬ğ‘–, i.e., for ğ‘–> 1 and up to a permutation of its components,
ğ¿ğ‘–(ğ‘¥) =
â¡
â¢â¢â¢â¢â¢â¢â¢â£
ğ‘¥ğ’œğ‘–âˆ’1
ğ¿ğ’œğ‘–âˆ–ğ’œğ‘–âˆ’1
ğ‘–
(ğ‘¥ğ’®ğ‘–, ğ‘¥ğ’œğ‘–âˆ–ğ’œğ‘–âˆ’1)
ğ¿ğ’®ğ‘–
ğ‘–(ğ‘¥ğ’®ğ‘–)
ğ‘¥â„¬ğ‘–
â¤
â¥â¥â¥â¥â¥â¥â¥â¦
.
(5.26)
133

The map ğ‘…is low-dimensional with respect to ğ’œğ‘˜+1 and can also be chosen as a
generalized triangular function. Intuitively, we can think of ğ¿ğ‘–as decoupling nodes
in ğ’œğ‘–âˆ–ğ’œğ‘–âˆ’1 from the rest of the graph in an I-map for (ğ¿1 âˆ˜Â· Â· Â· âˆ˜ğ¿ğ‘–âˆ’1)â™¯ğœˆğœ‹. (Recall
that by Lemma 5.3 all the sets (ğ’œğ‘–) are nested, i.e., ğ’œ1 âŠ‚Â· Â· Â· âŠ‚ğ’œğ‘˜+1.) Figure 5-6
illustrates the mechanics underlying the recursive application of Lemma 5.3.
We emphasize that the existence and structure of (5.25) follow from simple graph
operations on ğ’¢, and do not entail any actual computation with the target measure
ğœˆğœ‹. Notice also that even if each map in the decomposition (5.24) is ğœ-triangular, the
resulting transport map ğ‘‡need not be triangular at all. In other words, we obtain
factorizations of general and possibly non-triangular transport maps in terms of low-
dimensional generalized triangular functions. In this sense, we can regard triangular
maps as a fundamental â€œbuilding blockâ€ of a much larger class of transport maps.
Decomposable transports are clearly not unique.
In particular, there are two
factors that affect both the sparsity pattern and the number ğ‘˜of composed maps in
the family ğ¿1 âˆ˜Â· Â· Â· âˆ˜ğ¿ğ‘˜+1 âˆ˜Rğ‘˜+1: the sequence of decompositions (ğ’œğ‘–, ğ’®ğ‘–, â„¬ğ‘–) and the
sequence of permutations (ğœğ‘–). Usually, there is a certain freedom in the choice of these
parameters, and each configuration might lead to a different family of decomposable
transports. Of course some families might be more desirable than others: ideally, we
would like the low-dimensional maps in the composition to have the smallest effective
dimension possible. Recall that by Lemma 5.3 the effective dimension of each ğ¿ğ‘–
can be bounded above by |(ğ’œğ‘–âˆ–ğ’œğ‘–âˆ’1) âˆªğ’®ğ‘–| (with the convention ğ’œ0 = âˆ…). Thus
we should intuitively choose a decomposition (ğ’œğ‘–, ğ’®ğ‘–, â„¬ğ‘–) of ğ’¢ğ‘–and a permutation ğœğ‘–
for ğ¿ğ‘–that minimize the cardinality of (ğ’œğ‘–âˆ–ğ’œğ‘–âˆ’1) âˆªğ’®ğ‘–, and that, at the same time,
minimize the number of edges added from ğ’¢ğ‘–to ğ’¢ğ‘–+1. In principle, we should also
account for the dimensions of all future maps in the recursion. In the most general
case, this graph theoretic question could be addressed using dynamic programming
[21]. In practice, however, we will often consider graphs for which a good sequence of
decompositions and permutations is rather obvious (see Section 5.6). For instance,
if the target distribution ğœˆğœ‹factorizes according to a tree ğ’¢, then it is immediate
to show the existence of a decomposable transport ğ‘‡= ğ‘‡1 âˆ˜Â· Â· Â· âˆ˜ğ‘‡ğ‘›âˆ’1 that pushes
134

forward ğœˆğœ‚to ğœˆğœ‹and that factorizes as the composition of ğ‘›âˆ’1 low-dimensional
maps (ğ‘‡ğ‘–)ğ‘›âˆ’1
ğ‘–=1 , each associated to an edge of ğ’¢: it suffices to consider a sequence of
decompositions (ğ’œğ‘–, ğ’®ğ‘–, â„¬ğ‘–) with ğ’œ1 âŠ‚ğ’œ2 âŠ‚Â· Â· Â· , where, for a given rooted version of
ğ’¢, ğ’œğ‘–âˆ–ğ’œğ‘–âˆ’1 consists of a single node ğ‘ğ‘–with the largest depth in ğ’¢ğ’±âˆ–ğ’œğ‘–âˆ’1, and where
ğ’®ğ‘–contains the unique parent of that node. Remarkably, each map ğ‘‡ğ‘–has effective
dimension less than or equal to two, independent of ğ‘›â€”the size of the tree.
At this point, it might be natural to consider a junction tree decomposition of a
triangulation of ğ’¢[156] as a convenient graphical tool to schedule the sequence of
decompositions (ğ’œğ‘–, ğ’®ğ‘–, â„¬ğ‘–) needed to apply Lemma 5.3 recursively. Decomposable
graphs are in fact ultimately chordal [161]. However, the situation might not be as
straightforward. The problem is that the clique structure of ğ’¢ğ‘–, an I-map for ğœˆğ‘–, can
be very different than that of ğ’¢ğ‘–+1, an I-map for ğ¿â™¯
ğ‘–ğœˆğ‘–; Theorem 5.2[Part 2d] shows
that ğ’¢ğ‘–+1 might contain larger maximal cliques than those in ğ’¢ğ‘–, even if ğ’¢ğ‘–is chordal
(see Figure 5-6 for an example). Thus, working with a junction tree might require a
bit of extra care.
5.5.4
Computation of decomposable transports
Given the existence and structure of a decomposable transport like (5.25), what to
do with it? There are at least two possible ways of exploiting this type of informa-
tion. First, one could solve a variational problem like (5.5) and enforce an explicit
parameterization of the transport map as the composition ğ‘‡= ğ¿1 âˆ˜Â· Â· Â· âˆ˜ğ¿ğ‘˜+1 âˆ˜ğ‘….
In this scenario, one need only parameterize the low-dimensional maps (ğ¿ğ‘–, ğ‘…) and
optimize, jointly, over their composition. The advantage of this approach is that it
bypasses the parameterization of a single high-dimensional function, ğ‘‡, altogether.
See the literature on normalizing flows [231] for possible computational ideas in this
direction.
An alternativeâ€”and perhaps more intriguingâ€”possibility is to compute the maps
(ğ¿ğ‘–) sequentially by solving separate low-dimensional optimization problemsâ€”one for
each map ğ¿ğ‘–. By Theorem 5.2[Part 2a] and Lemma 5.3, there exists a factorization
(5.20) of ğœ‹ğ‘–â€”a density of ğ¿â™¯
ğ‘–âˆ’1 ğœˆğ‘–âˆ’1â€”for which ğ¿ğ‘–is a ğœğ‘–-generalized KR rearrangement
135

that pushes forward ğœˆğœ‚to a measure with density proportional to ğœ“ğ’œğ‘–âˆªğ’®ğ‘–ğœ‚ğ‘‹â„¬ğ‘–, where
(ğ’œğ‘–, ğ’®ğ‘–, â„¬ğ‘–) is a decomposition of ğ’¢ğ‘–and ğ’¢ğ‘–is an I-map for ğœˆğ‘–. In general ğœ“ğ’œğ‘–âˆªğ’®ğ‘–
depends on ğ¿ğ‘–âˆ’1, and so the maps (ğ¿ğ‘–) must be computed sequentially.9 In essence,
decomposable transports break the inference task into smaller and possibly easier
steps.
Note that we could define ğ¿ğ‘–with respect to any factorization (5.20) with ğœ“ğ’œğ‘–âˆªğ’®ğ‘–
integrable: these different factorizations would lead to a family of decomposable trans-
ports with the same low-dimensional structure and sparsity patterns (as predicted by
Theorem 5.2). Thus, as long as we have access to a sequence of integrable factors
(ğœ“ğ’œğ‘–âˆªğ’®ğ‘–), we can compute each map ğ¿ğ‘–individually by solving a low-dimensional
optimization problem. (See Appendix B for computational remarks on generalized
triangular functions.) Intuitively, since by Lemma 5.3[Part 1b] ğ¿ğ‘–is low-dimensional
with respect to ğ’œğ‘–âˆ’1 âˆªâ„¬ğ‘–, we really only need to optimize for a portion of the map,
namely ğ¿ğ’
ğ‘–for ğ’= (ğ’œğ‘–âˆ–ğ’œğ‘–âˆ’1)âˆªğ’®ğ‘–, which can be regarded effectively as a multivariate
map on R|ğ’|. In the same way, the map ğ‘…can be computed as any transport (pos-
sibly triangular) that pushes forward ğœˆğœ‚to ğ¿â™¯
ğ‘˜+1 ğœˆğ‘˜+1. Theorem 5.2[Part 2b] tells us
that once again we only need to optimize for a low-dimensional portion of the map,
namely, ğ‘…ğ’®ğ‘˜+1âˆªâ„¬ğ‘˜+1.
While it might be difficult to access a sequence of factorizations (5.20) for a general
problem, there are important applications, such as Bayesian filtering, smoothing, and
joint parameter/state estimation, where the sequential computation of the transports
(ğ¿ğ‘–, ğ‘…) is always possible by construction. We discuss these applications in the next
section.
9 This is not always the case. For instance, given a rooted version of ğ’¢and a pair of consecutive
depths (see the discussion at the end of Section 5.5.3), all the maps (ğ¿ğ‘–) associated with edges
connecting nodes at these two depths can be computed in parallel.
136

ğ’œ1
â„¬1
ğ’®1
2
3
1
6
4
5
ğ’œ1
â„¬1
ğ’®1
2
3
1
6
4
5
ğ’œ2
â„¬2
ğ’®2
2
3
1
6
4
5
ğ’œ2
â„¬2
ğ’®2
2
3
1
6
4
5
Figure 5-6:
Sequence of graph decompositions associated with the recursive applica-
tion of Lemma 5.3. At (top left) is an I-map, ğ’¢1, for ğœˆğœ‹, with ğœˆğœ‹âˆˆM+(R6). We first
decompose this graph into (ğ’œ1, ğ’®1, â„¬1) as indicated, and apply Theorem 5.2 to the
pair ğœˆğœ‚, ğœˆğœ‹. To do so, we first need to add edge (2, 3) to ğ’¢1 in order to turn (ğ’œ1, ğ’®1, â„¬1)
into a proper decomposition of ğ’¢1 with a fully connected ğ’®1.The resulting graph, ğ’¢1
â‹†,
is now chordal (in fact, a triangulation of ğ’¢1 [161]), but still an I-map for ğœˆğœ‹. The
first map ğ¿1 is ğœ1-triangular with ğœ1(N6) = {2, 3, 1, 4, 5, 6} and it is low-dimensional
with respect to â„¬1; The (top right) figure shows the I-map, ğ’¢2, for ğ¿â™¯
1 ğœˆğœ‹as given
by Theorem 5.2[Part 2d]: as expected, ğ’œ1 is disconnected from ğ’®1 âˆªâ„¬1; moreover,
a new maximal clique {2, 3, 4, 5} appears in ğ’¢2. This new clique is larger than any
of the maximal cliques in ğ’¢1
â‹†, even though ğ’¢1
â‹†is chordal. (Notice that ğœ1 is not the
permutation that adds the fewest edges possible in ğ’¢2. An example of such â€œbestâ€
permutation would be ğœ(N6) = {3, 2, 1, 4, 5, 6}.) Though Theorem 5.2 guarantees the
existence of a low-dimensional map ğ‘…âˆˆR1 that pushes forward ğœˆğœ‚to ğ¿â™¯
1 ğœˆğœ‹, we in-
stead proceed recursively by applying Lemma 5.3[Part 1] for a proper decomposition,
(ğ’œ2, ğ’®2, â„¬2), of ğ’¢2, where ğ’œ2 is a strict superset of ğ’œ1 (bottom left). The lemma
shows that R1 âŠƒğ¿2 âˆ˜R2 for some ğœ2-triangular map ğ¿2, which is low-dimensional
with respect to ğ’œ1 âˆªâ„¬2, and where each ğ‘…âˆˆR2 pushes forward ğœˆğœ‚to (ğ¿1 âˆ˜ğ¿2)â™¯ğœˆğœ‹.
Can we apply Lemma 5.3 one more time to characterize decomposable transports in
R2? The answer is no, as the I-map for (ğ¿1 âˆ˜ğ¿2)â™¯ğœˆğœ‹(bottom right) consists of a single
clique in ğ’®2 âˆªâ„¬2. Nevertheless, each ğ‘…âˆˆR2 is still low-dimensional with respect
to ğ’œ2. Overall, we showed the existence of a transport map ğ‘‡: R6 â†’R6 pushing
forward ğœˆğœ‚to ğœˆğœ‹that decomposes as ğ‘‡= ğ¿1 âˆ˜ğ¿2 âˆ˜ğ‘…, where ğ¿1, ğ¿2, ğ‘…are effectively
{3, 4, 3}-dimensional maps, respectively.
137

5.6
Sequential inference on state-space models: vari-
ational algorithms
In this section, we consider the problem of sequential Bayesian inference (or discrete-
time data assimilation [162, 230, 245]) for continuous, nonlinear, and non-Gaussian
state-space models.
Our goal is to specialize the theory developed in Section 5.5 to the solution of
Bayesian filtering and smoothing problems. The key result of this section is a new
variational algorithm for characterizing the full posterior distribution of the sequential
inference problemâ€”e.g., not just a few filtering or smoothing marginalsâ€”via recursive
lagâ€“1 smoothing with transport maps. The proposed algorithm builds a decomposable
high-dimensional transport map in a single forward pass by solving a sequence of local
small-dimensional problems, without resorting to any backward pass on the state
space model (see Theorem 5.3). These results extend naturally to the case of joint
parameter and state estimation (see Section 5.6.4 and Theorem 5.4).
A state-space model consists of a pair of discrete-time stochastic processes (ğ‘ğ‘˜, ğ‘Œğ‘˜)ğ‘˜â‰¥0
indexed by the time ğ‘˜, where (ğ‘ğ‘˜) is a latent Markov process of interest and where
(ğ‘Œğ‘˜) is the observed process. We can think of each ğ‘Œğ‘˜as a noisy and perhaps indi-
rect measurement of ğ‘ğ‘˜. The Markov structure corresponding to the joint process
(ğ‘ğ‘˜, ğ‘Œğ‘˜) is shown in Figure 5-7. The generalization of the results of this section to
the case of missing observations is straightforward and will not be addressed here (in
the interest of a concise formulation of our theorems).
We assume that we are given the transition densities ğœ‹ğ‘ğ‘˜+1|ğ‘ğ‘˜for all ğ‘˜â‰¥0,
sometimes referred to as the â€œprior dynamic,â€ together with the marginal density
of the initial conditions ğœ‹ğ‘0. (For instance, the prior dynamic could stem from the
discretization of a continuous time stochastic differential equation [199].) We denote
by ğœ‹ğ‘Œğ‘˜|ğ‘ğ‘˜the likelihood function, i.e., the density of ğ‘Œğ‘˜given ğ‘ğ‘˜, and assume that
ğ‘ğ‘˜and ğ‘Œğ‘˜are random variables taking values on Rğ‘›and Rğ‘‘, respectively. Moreover,
we denote by (ğ‘¦ğ‘˜)ğ‘˜â‰¥0 a sequence of realizations of the observed process (ğ‘Œğ‘˜) that will
define the posterior distribution over the unobserved (hidden) states of the model,
138

and make the following regularity assumption in our theorems: ğœ‹ğ‘0:ğ‘˜âˆ’1,ğ‘Œ0:ğ‘˜âˆ’1 > 0 for
all ğ‘˜â‰¥1. (The existence of underlying fully supported measures will be left implicit
throughout the section for notational convenience.)
ğ‘0
ğ‘1
ğ‘2
ğ‘3
ğ‘ğ‘
ğ‘Œ0
ğ‘Œ1
ğ‘Œ2
ğ‘Œ3
ğ‘Œğ‘
ğ‘‹0
ğ‘‹1
ğ‘‹2
ğ‘‹3
ğ‘‹ğ‘
Figure 5-7:
(above) I-map for the joint process (ğ‘ğ‘˜, ğ‘Œğ‘˜)ğ‘˜â‰¥0 defining the state-space
model. (below) I-map for the independent reference process (ğ‘‹ğ‘˜)ğ‘˜â‰¥0 used in Theorem
5.3.
5.6.1
Smoothing and filtering: the full Bayesian solution
In typical applications of state-space modeling, the process (ğ‘Œğ‘˜) is only observed
sequentially, and thus the goal of inference is to characterizeâ€”sequentially in time
and via a recursive algorithmâ€”the joint distribution of the current and past states
given currently available measurements, i.e.,
ğœ‹ğ‘0:ğ‘˜|ğ‘¦0:ğ‘˜(ğ‘§0:ğ‘˜) := ğœ‹ğ‘0:ğ‘˜|ğ‘Œ0:ğ‘˜(ğ‘§0:ğ‘˜|ğ‘¦0:ğ‘˜)
(5.27)
for all ğ‘˜â‰¥0. That is, we wish to characterize ğœ‹ğ‘0:ğ‘˜|ğ‘¦0:ğ‘˜based on our knowledge of the
posterior distribution at the previous timestep, ğœ‹ğ‘0:ğ‘˜âˆ’1|ğ‘¦0:ğ‘˜âˆ’1, and with an effort that
is constant over time. We regard (5.27) as the full Bayesian solution to the sequential
inference problem [245].
Usually, the task of updating ğœ‹ğ‘0:ğ‘˜âˆ’1|ğ‘¦0:ğ‘˜âˆ’1 to yield ğœ‹ğ‘0:ğ‘˜|ğ‘¦0:ğ‘˜becomes increasingly
challenging over time due to the widening inference horizon, making characterization
of the full Bayesian solution impractical for large ğ‘˜[245]. Thus, two simplifications of
the sequential inference problem are frequently considered: filtering and smoothing
[245].
In filtering, we characterize ğœ‹ğ‘ğ‘˜|ğ‘¦0:ğ‘˜for all ğ‘˜â‰¥0, while in smoothing we
139

recursively update ğœ‹ğ‘ğ‘—|ğ‘¦0:ğ‘˜for increasing ğ‘˜> ğ‘—, where ğ‘ğ‘—is some past state of the
unobserved process. Both filtering and smoothing deliver particular low-dimensional
marginals of the full Bayesian solution to the inference problem, and hence are often
considered good candidates for numerical approximation [87, 245, 70].
The following theorem shows that characterizing the full Bayesian solution to
the sequential inference problem via a decomposable transport map is essentially no
harder than performing lagâ€“1 smoothing, which, in turn, amounts to characterizing
ğœ‹ğ‘ğ‘˜âˆ’1,ğ‘ğ‘˜|ğ‘¦0:ğ‘˜for all ğ‘˜â‰¥0 (an operation only incrementally harder than regular filter-
ing). This result relies on the recursive application of the decomposition theorem for
couplings (Theorem 5.2) to the tree Markov structure of ğœ‹ğ‘0:ğ‘˜|ğ‘¦0:ğ‘˜. In what follows,
let (ğ‘‹ğ‘˜)ğ‘˜â‰¥0 be an independent (reference) process with nonvanishing marginal den-
sities (ğœ‚ğ‘‹ğ‘˜), with each ğ‘‹ğ‘˜taking values on Rğ‘›. See Figure 5-7 for the corresponding
Markov network.
Theorem 5.3 (Decomposition theorem for state-space models). Let (Mğ‘–)ğ‘–â‰¥0 be a
sequence of (ğœğ‘–)-generalized KR rearrangements on Rğ‘›Ã— Rğ‘›, which are of the form
Mğ‘–(ğ‘¥ğ‘–, ğ‘¥ğ‘–+1) =
â¡
â£M0
ğ‘–(ğ‘¥ğ‘–, ğ‘¥ğ‘–+1)
M1
ğ‘–(ğ‘¥ğ‘–+1)
â¤
â¦
(5.28)
for some ğœğ‘–, M0
ğ‘–: Rğ‘›Ã— Rğ‘›â†’Rğ‘›, M1
ğ‘–: Rğ‘›â†’Rğ‘›, and that are defined by the
recursion:
â€“ M0 pushes forward ğœ‚ğ‘‹0,ğ‘‹1 to ğœ‹0 = Ìƒï¸€ğœ‹0/c0,
â€“ Mğ‘–pushes forward ğœ‚ğ‘‹ğ‘–,ğ‘‹ğ‘–+1 to ğœ‹ğ‘–(ğ‘§ğ‘–, ğ‘§ğ‘–+1) = ğœ‚ğ‘‹ğ‘–(ğ‘§ğ‘–) Ìƒï¸€ğœ‹ğ‘–(M1
ğ‘–âˆ’1(ğ‘§ğ‘–), ğ‘§ğ‘–+1)/cğ‘–,
where cğ‘–is a normalizing constant and where (Ìƒï¸€ğœ‹ğ‘–)ğ‘–â‰¥0 are functions on Rğ‘›Ã— Rğ‘›given
by:
â€“ Ìƒï¸€ğœ‹0(ğ‘§0, ğ‘§1) = ğœ‹ğ‘0,ğ‘1(ğ‘§0, ğ‘§1) ğœ‹ğ‘Œ0|ğ‘0(ğ‘¦0|ğ‘§0) ğœ‹ğ‘Œ1|ğ‘1(ğ‘¦1|ğ‘§1),
â€“ Ìƒï¸€ğœ‹ğ‘–(ğ‘§ğ‘–, ğ‘§ğ‘–+1) = ğœ‹ğ‘ğ‘–+1|ğ‘ğ‘–(ğ‘§ğ‘–+1|ğ‘§ğ‘–) ğœ‹ğ‘Œğ‘–+1|ğ‘ğ‘–+1(ğ‘¦ğ‘–+1|ğ‘§ğ‘–+1) for ğ‘–â‰¥1.
Then, for all ğ‘˜â‰¥0, the following hold:
140

1. The map M1
ğ‘˜pushes forward ğœ‚ğ‘‹ğ‘˜+1 to ğœ‹ğ‘ğ‘˜+1|ğ‘¦0:ğ‘˜+1.
[filtering]
2. The map Mğ‘˜, defined as (M1
ğ‘˜âˆ’1(ğ‘¥) = ğ‘¥for ğ‘˜= 0)
Mğ‘˜(ğ‘¥ğ‘˜, ğ‘¥ğ‘˜+1) =
â¡
â£M1
ğ‘˜âˆ’1(M0
ğ‘˜(ğ‘¥ğ‘˜, ğ‘¥ğ‘˜+1))
M1
ğ‘˜(ğ‘¥ğ‘˜+1)
â¤
â¦,
(5.29)
pushes forward ğœ‚ğ‘‹ğ‘˜,ğ‘‹ğ‘˜+1 to ğœ‹ğ‘ğ‘˜,ğ‘ğ‘˜+1|ğ‘¦0:ğ‘˜+1.
[lagâ€“1 smoothing]
3. The composition of transport maps Tğ‘˜= ğ‘‡0 âˆ˜Â· Â· Â· âˆ˜ğ‘‡ğ‘˜, where each ğ‘‡ğ‘–is defined
as
ğ‘‡ğ‘–(ğ‘¥0, . . . , ğ‘¥ğ‘˜+1) =
â¡
â¢â¢â¢â¢â¢â¢â¢â¢â¢â¢â¢â¢â¢â¢â¢â¢â¢â¢â¢â£
ğ‘¥0
...
ğ‘¥ğ‘–âˆ’1
M0
ğ‘–(ğ‘¥ğ‘–, ğ‘¥ğ‘–+1)
M1
ğ‘–(ğ‘¥ğ‘–+1)
ğ‘¥ğ‘–+2
...
ğ‘¥ğ‘˜+1
â¤
â¥â¥â¥â¥â¥â¥â¥â¥â¥â¥â¥â¥â¥â¥â¥â¥â¥â¥â¥â¦
,
(5.30)
pushes forward ğœ‚ğ‘‹0:ğ‘˜+1 to ğœ‹ğ‘0:ğ‘˜+1|ğ‘¦0:ğ‘˜+1.
[full Bayesian solution]
4. The model evidence (marginal likelihood) is given by
ğœ‹ğ‘Œ0:ğ‘˜+1(ğ‘¦0:ğ‘˜+1) =
ğ‘˜
âˆï¸
ğ‘–=0
cğ‘–.
(5.31)
Theorem 5.3 suggests a variational algorithm for smoothing and filtering a contin-
uous state-space model: compute the sequence of maps (Mğ‘–), each of dimension 2ğ‘›;
embed them into higher-dimensional identity maps to form (ğ‘‡ğ‘–) according to (5.30);
then evaluate the composition Tğ‘˜= ğ‘‡0 âˆ˜Â· Â· Â· âˆ˜ğ‘‡ğ‘˜to sample directly from ğœ‹ğ‘0:ğ‘˜+1|ğ‘¦0:ğ‘˜+1
(i.e., the full Bayesian solution) and obtain information about any smoothing or fil-
tering distribution of interest.
Successive transports in the composition (Tğ‘˜)ğ‘˜â‰¥0 are nested and thus ideal for
141

sequential assimilation: given Tğ‘˜âˆ’1, we can obtain Tğ‘˜simply by computing an ad-
ditional map Mğ‘˜of dimension 2ğ‘›â€”with no need to recompute (Mğ‘–)ğ‘–<ğ‘˜. This step
converts a transport map that samples ğœ‹ğ‘0:ğ‘˜|ğ‘¦0:ğ‘˜into one that samples ğœ‹ğ‘0:ğ‘˜+1|ğ‘¦0:ğ‘˜+1.
This feature is quite remarkable since Mğ‘˜is always a 2ğ‘›-dimensional map, while
ğœ‹ğ‘0:ğ‘˜+1|ğ‘¦0:ğ‘˜+1 is a density on Rğ‘›(ğ‘˜+2)â€”a space whose dimension increases with time
ğ‘˜. In fact, from the perspective of Section 5.5, Theorem 5.3 simply shows that each
ğœ‹ğ‘0:ğ‘˜+1|ğ‘¦0:ğ‘˜+1 can be represented via a decomposable transport Tğ‘˜= ğ‘‡0 âˆ˜Â· Â· Â· âˆ˜ğ‘‡ğ‘˜. The
sparsity pattern of each map Mğ‘–, specified in (5.28), is necessary for Theorem 5.3 to
hold: Mğ‘–cannot be any transport map; it must be block upper triangular.
The proposed algorithm consists of a forward pass on the stace-space modelâ€”
wherein the sequence of transport maps (Mğ‘–) are computed and storedâ€”followed by
a backward pass where the composition Tğ‘˜= ğ‘‡0âˆ˜Â· Â· Â·âˆ˜ğ‘‡ğ‘˜is evaluated deterministically
to sample ğœ‹ğ‘0:ğ‘˜+1|ğ‘¦0:ğ‘˜+1. This backward pass does not re-evaluate the potentials of the
state-space model (e.g., transition kernels or likelihoods) at earlier times, nor does
it perform any additional computation other than evaluating the maps (Mğ‘–) in Tğ‘˜.
(See [151, 87, 109] for alternative approximations of the forwardâ€“filtering backwardâ€“
smoothing formulas.)
Though each map ğ‘‡ğ‘—is usually trivial to evaluateâ€”e.g., the map might be pa-
rameterized in terms of polynomials [187] and differ from the identity along only 2ğ‘›
componentsâ€”it is true that the cost of evaluating Tğ‘˜grows linearly with ğ‘˜. This is
hardly surprising since ğœ‹ğ‘0:ğ‘˜+1|ğ‘¦0:ğ‘˜+1 is a density over spaces of increasing dimension.
A direct approximation of Tğ‘˜is usually a bad idea since the map is high-dimensional
and dense (in the sense defined by Section 5.5); it is better to store Tğ‘˜implicitly
through the sequence of maps (Mğ‘–)ğ‘˜
ğ‘–â‰¥0, and sample smoothed trajectories by evalu-
ating Tğ‘˜only when it is needed. If we are only interested in a particular smoothing
marginal, e.g., ğœ‹ğ‘0|ğ‘¦0:ğ‘˜+1 for all ğ‘˜â‰¥0, then we can define a general forward recursion
to sample ğœ‹ğ‘0|ğ‘¦0:ğ‘˜+1 with a single transport map that is updated recursively over time,
rather than with a growing composition of mapsâ€”and thus with a cost independent
of ğ‘˜. This construction is given in Section 5.6.5.
Also, it is important to emphasize that in order to assimilate a new measurement,
142

say ğ‘¦ğ‘˜+1, we do not need to evaluate the full composition Tğ‘˜âˆ’1; we only need to com-
pute a low-dimensional map Mğ‘˜whose target density ğœ‹ğ‘˜depends only on Mğ‘˜âˆ’1. The
previous maps (Mğ‘–)ğ‘–<ğ‘˜âˆ’1 are unnecessary at this stage. Thus the effort of assimilating
a new piece of data is constant in timeâ€”modulo the complexity of each Mğ‘˜.
The distribution ğœ‹ğ‘0:ğ‘˜+1|ğ‘¦0:ğ‘˜+1 is not represented via a collection of particles as
ğ‘˜â‰¥0 increases, but rather via a growing composition of low-dimensional transport
maps that yields fully supported approximations of ğœ‹ğ‘0:ğ‘˜+1|ğ‘¦0:ğ‘˜+1.
The maps are com-
puted via deterministic optimization: there are no importance sampling or resampling
steps. Intuitively, the optimization step for Mğ‘˜moves the particles on which the map
is evaluated, rather than reweighing them.
(Think of the assimilation step of an
ensemble Kalman filter [92, 93] as a particular example with linear maps.)
Part 1 of Theorem 5.3 shows that the lower subcomponent M1
ğ‘˜: Rğ‘›â†’Rğ‘›of the
map Mğ‘˜characterizes the filtering distribution ğœ‹ğ‘ğ‘˜+1|ğ‘¦0:ğ‘˜+1 for all ğ‘˜â‰¥0, while Part 2
shows that each Mğ‘˜also characterizes the lagâ€“1 smoothing distribution ğœ‹ğ‘ğ‘˜,ğ‘ğ‘˜+1|ğ‘¦0:ğ‘˜+1
up to an invertible transformation of the marginal over ğ‘ğ‘˜.
Thus, Theorem 5.3
implicitly suggests a deterministic algorithm for lagâ€“1 smoothing that in fact fully
characterizes the posterior distribution of the nonlinear state-space modelâ€”much in
the same spirit as the Rauch-Tung-Striebel (RTS) smoothing algorithm for Gaussian
models. We clarify this connection in Section 5.6.2.
The maps (Mğ‘–) must be approximated numerically, in general (see Section 5.2).
As a result, Monte Carlo estimators associated with the evaluation of Tğ‘˜= ğ‘‡0âˆ˜Â· Â· Â·âˆ˜ğ‘‡ğ‘˜
are biased, although possibly with negligible variance, since it is trivial to evaluate the
map a large number of times. This bias is only due to the numerical approximation of
(Mğ‘–), and not to the particular factorization properties of Tğ‘˜. In practice, one might
either accept this bias or try to reduce it. The bias can be reduced in at least two
ways: (1) by enriching the parameterization of some (Mğ‘–), and thus by increasing the
accuracy of these maps via optimization, or (2) by using the map-induced proposal
density (Tğ‘˜)â™¯ğœ‚ğ‘‹0:ğ‘˜+1â€”i.e., the pushforward of a marginal of the reference process
through Tğ‘˜â€”within the context of importance sampling (IS) or MCMC (see Section
143

5.8 ). For instance, the weight function
ğ‘¤ğ‘˜+1(ğ‘¥) = ğœ‹ğ‘0:ğ‘˜+1|ğ‘¦0:ğ‘˜+1(ğ‘¥)
(Tğ‘˜)â™¯ğœ‚ğ‘‹0:ğ‘˜+1(ğ‘¥)
(5.32)
is readily available, and can be used to yield consistent estimators with respect to the
smoothing distribution. However, the resulting weights cannot be computed recur-
sively in time, because even though the small dimensional maps Mğ‘˜are computed
sequentially, the map-induced proposal (Tğ‘˜)â™¯ğœ‚ğ‘‹0:ğ‘˜+1 changes entirely at every step.
In particle filters, the complexity of an approximation of the underlying distribu-
tion is given by the number ğ‘of particles. In the proposed variational approach,
the complexity of the approximation depends on the parameterization of each map
Mğ‘–. There is no single parameter like ğ‘to describe the complexity of the latter,
though, broadly, it should depend on the number of degrees of freedom in the param-
eterization. In some cases, one might think of using the total order of a multivariate
polynomial expansion of each component of the map as a tuning parameter. But this
is far from general or practical in high dimensions. The virtue of a functional repre-
sentation of the transport map is the ability to carefully select the degrees of freedom
of the parameterization. For instance, we might model local interactions between dif-
ferent groups of input variables using different approximation orders or even different
sets of basis functions. This freedom should not be frightening, but rather should be
embraced as a great opportunity to exploit the structure of the particular problem at
hand. (See Chapter 6 for an example of this fact in the context of high-dimensional
filtering of spatio-temporal processes.)
As shown in (5.8), the computation of each Mğ‘–is also associated with an approx-
imation of the normalizing constant cğ‘–of its own target density, which then leads to
a one-pass approximation of the marginal likelihood using (5.31).
One last remark: the proof of Theorem 5.3 shows that the triangular structure
hypothesis for each Mğ‘–can be relaxed provided that the underlying densities are
regular enough. The following corollary clarifies this point.
Corollary 5.1. The results of Theorem 5.3 still hold if we replace every KR rear-
144

rangement Mğ‘–with a â€œblock triangularâ€ diffeomorphism of the form (5.28) that couples
the same distributions, provided that such regular transport maps exist.
Filtering and smoothing are of course very rich problems, and in this section we
have by no means attempted to be exhaustive. Rather, our goal was to highlight
some implications of decomposable transports on problems of sequential Bayesian
inference, in a general non-Gaussian setting. See Section 6 for additional topics in
high-dimensional filtering.
5.6.2
The linear Gaussian case: connection with the RTS smoother
In this section, we specialize the results of Theorem 5.3 to linear Gaussian state-space
models, and make explicit the connection with the RTS Gaussian smoother [226].
Consider a linear Gaussian state-space model defined by
ğ‘ğ‘˜+1
=
ğ¹ğ‘˜ğ‘ğ‘˜+ ğœ€ğ‘˜
(5.33)
ğ‘Œğ‘˜
=
ğ»ğ‘˜ğ‘ğ‘˜+ ğœ‰ğ‘˜
for all ğ‘˜â‰¥0, where ğœ€ğ‘˜âˆ¼ğ’©(0, ğ‘„ğ‘˜), ğœ‰ğ‘˜âˆ¼ğ’©(0, ğ‘…ğ‘˜), ğ¹ğ‘˜âˆˆRğ‘›Ã—ğ‘›, ğ»ğ‘˜âˆˆRğ‘‘Ã—ğ‘›, and
ğ‘0 âˆ¼ğ’©(ğœ‡0, Î“0). Both ğœ€ğ‘˜and ğœ‰ğ‘˜are independent of ğ‘ğ‘˜, while ğ‘„ğ‘˜, ğ‘…ğ‘˜, and Î“0 are
symmetric positive definite matrices for all ğ‘˜â‰¥0.
If we choose an independent reference process (ğ‘‹ğ‘˜) with standard normal marginals,
i.e., ğœ‚ğ‘‹ğ‘˜= ğ’©(0, I), then the maps (Mğ‘˜) of Theorem 5.3 can be chosen to be linear:
Mğ‘˜(ğ‘§ğ‘˜, ğ‘§ğ‘˜+1) =
â¡
â£ğ´ğ‘˜
ğµğ‘˜
0
ğ¶ğ‘˜
â¤
â¦
â§
â¨
â©
ğ‘§ğ‘˜
ğ‘§ğ‘˜+1
â«
â¬
â­+
â§
â¨
â©
ğ‘ğ‘˜
ğ‘ğ‘˜
â«
â¬
â­,
(5.34)
for some matrices ğ´ğ‘˜, ğµğ‘˜, ğ¶ğ‘˜âˆˆRğ‘›Ã—ğ‘›and ğ‘ğ‘˜, ğ‘ğ‘˜âˆˆRğ‘›. (Notice that in this case
Corollary 5.1 applies and the matrices ğ´ğ‘˜, ğµğ‘˜can be full and not necessarily trian-
gular.) The following lemma gives a closed form expression for the maps (Mğ‘˜) with
ğ‘˜â‰¥1. (M0 can be derived analogously with simple algebra.)
145

Lemma 5.4 (The linear Gaussian case). For ğ‘˜â‰¥1, the map Mğ‘˜in (5.34) can be
defined as follows: if (ğ‘ğ‘˜, ğ¶ğ‘˜) is the output of a square-root Kalman filter at time ğ‘˜
[28], i.e., if ğ‘ğ‘˜and ğ¶ğ‘˜are, respectively, the mean and square root of the covariance
of the filtering distribution ğœ‹ğ‘ğ‘˜+1|ğ‘¦0:ğ‘˜+1, then one can set:
ğ´ğ‘˜
=
ğ½âˆ’1/2
ğ‘˜
(5.35)
ğµğ‘˜
=
âˆ’ğ½âˆ’1
ğ‘˜
ğ‘ƒğ‘˜ğ¶ğ‘˜
ğ‘ğ‘˜
=
ğ½âˆ’1
ğ‘˜
ğ‘ƒğ‘˜(ğ¹ğ‘˜ğ‘ğ‘˜âˆ’1 âˆ’ğ‘ğ‘˜),
for ğ½ğ‘˜:= I + ğ¶âŠ¤
ğ‘˜âˆ’1 ğ¹âŠ¤
ğ‘˜ğ‘„âˆ’1
ğ‘˜ğ¹ğ‘˜ğ¶ğ‘˜âˆ’1 and ğ‘ƒğ‘˜= âˆ’ğ¶âŠ¤
ğ‘˜âˆ’1 ğ¹âŠ¤
ğ‘˜ğ‘„âˆ’1
ğ‘˜.
The formulas in Lemma 5.4 can be interpreted as one possible implementation
of a square-root RTS smoother for Gaussian models: at each step ğ‘˜of a forward
pass, the filtering estimates (ğ‘ğ‘˜, ğ¶ğ‘˜) are augmented with a collection (ğ‘ğ‘˜, ğ´ğ‘˜, ğµğ‘˜)
of stored quantities, which can then be reused to sample the full Bayesian solution
(or particular smoothing marginals) whenever needed, and without ever touching
the state-space model again. In this sense, the algorithm proposed in Section 5.6.1
can be understood as the natural generalizationâ€”to the non-Gaussian caseâ€”of the
square-root RTS smoother.
5.6.3
Transport maps in filtering: examples in the literature
In this section we briefly review few approaches to filtering that rely explicitly on the
construction of couplings.
One of the earliest contributions using optimal transport in filtering is [228]. In
[228], the author proposes to construct an optimal transport plan between the empir-
ical approximation of the forecast distribution given by simulating the prior dynamic
and a corresponding empirical approximation of the filtering distribution obtained
by reweighing the forecast ensemble according to the likelihood. The cost function
used to define the optimal plan is given by the squared Euclidean distance. Thus,
[228] solves a discrete Kantorovich optimal transport problem (e.g., via the auction
146

algorithm [22]) instead of a continuous one for a transport map (cf. Section 5.6.1),
and then derives a linear update for the forecast ensemble from the optimal plan.
The linear update is chosen so that the mean of the transformed samples matches
the mean of the empirical approximation of the filtering distribution. Higher-order
schemes are also possible [80]. The resulting algorithm moves forecast particles much
in the same way as the EnKF does, but with important differences: (1) the analysis
ensemble is constrained to the convex hull of the forecast ensemble, and (2) under cer-
tain assumptions, the resulting filtering estimates are consistent. In [228], the explicit
construction of couplings is only used to update the forecast distribution, instead of
the previous filtering marginal (cf. Section 5.6.1). In some sense, the approach of
[228] is closer in spirit to our treatment of high-dimensional filtering by means of
local couplings (as presented in Chapter 6).
In Section 5.6.1 we considered filtering by means of block triangular Knothe-
Rosenblatt (KR) rearrangements. A recent work that relies on the notion of KR
rearrangement for the purpose of sequential inference is [126]. In [126] the authors
define a proposal for SMC (or MCMC) methods using the solution map of a dis-
cretized ordinary differential equation (ODE) whose drift term depends only on the
full conditionals of the target distribution, hence the name Gibbs flow. The inte-
gration of the flow requires only the evaluation of one-dimensional integrals, and the
action of the solution map defines implicitly a transport map, without the need to
parameterize explicitly the transformation. The authors show that that the proposed
flow is an approximation of a more general ODE, which can be thought of as the flow
analogue of the KR rearrangement. The approximation is obtained by replacing the
conditionals that usually define the KR rearrangement with full conditionals. Ap-
proaches that rely on the solution of different ODEs inspired by mass transportation
ideas are also available in the literature, e.g., [78, 278].
Another related methodology is certainly implicit sampling for particle filters [58].
We discuss implicit sampling in Section 6 of [187], together with many other ap-
proaches to inference that leverage the notion of transportation of probability mass
and the explicit construction of couplings.
147

In the next section, we look at the role of decomposable couplings for a slightly
more general problem: sequential parameter-state estimation in non-Gaussian state-
space models.
5.6.4
Sequential joint parameter and state estimation
In defining a state-space model, it is common to parameterize the transition densities
of the unobserved process or the likelihoods of the observables in terms of some
hyperparameters Î˜.
The Markov structure of the resulting Bayesian hierarchical
model, conditioned on the data, is shown in Figure 5-8. The state-space model is
now fully specified in terms of the conditional densities (ğœ‹ğ‘Œğ‘˜|ğ‘ğ‘˜,Î˜)ğ‘˜â‰¥0, (ğœ‹ğ‘ğ‘˜+1|ğ‘ğ‘˜,Î˜)ğ‘˜â‰¥0,
ğœ‹ğ‘0|Î˜, and the marginal ğœ‹Î˜. We assume that the hyperparameters Î˜ take values
on Rğ‘, and that the following regularity conditions hold: ğœ‹Î˜,ğ‘0:ğ‘˜âˆ’1,ğ‘Œ0:ğ‘˜âˆ’1 > 0 for all
ğ‘˜â‰¥1.
Given such a parameterization, one often wishes to jointly infer the hidden states
and the hyperparameters of the model as observations of the process (ğ‘Œğ‘˜) become
available. That is, the goal of inference is to characterize, via a recursive algorithm,
the sequence of posterior distributions given by
ğœ‹Î˜,ğ‘0:ğ‘˜|ğ‘¦0:ğ‘˜(ğ‘§ğœƒ, ğ‘§0:ğ‘˜) := ğœ‹Î˜,ğ‘0:ğ‘˜|ğ‘Œ0:ğ‘˜(ğ‘§ğœƒ, ğ‘§0:ğ‘˜|ğ‘¦0:ğ‘˜)
(5.36)
for all ğ‘˜â‰¥0 and for a sequence (ğ‘¦ğ‘˜)ğ‘˜â‰¥0 of observations. The following theorem shows
that we can characterize (5.36) by computing a sequence of low-dimensional transport
maps in the same spirit as Theorem 5.3. In what follows, let (ğ‘‹ğ‘˜) be an independent
process with marginals (ğœ‚ğ‘‹ğ‘˜) as defined in Theorem 5.3 and let ğ‘‹Î˜ be a random
variable on Rğ‘that is independent of (ğ‘‹ğ‘˜) and with nonvanishing density ğœ‚ğ‘‹Î˜.
ğ‘0
ğ‘1
ğ‘2
ğ‘3
ğ‘ğ‘
Î˜
Figure 5-8:
I-map for ğœ‹Î˜,ğ‘0,...,ğ‘ğ‘|ğ‘¦0,...,ğ‘¦ğ‘, for any ğ‘> 0.
148

Theorem 5.4 (Decomposition theorem for joint parameter and state estimation).
Let (Mğ‘–)ğ‘–â‰¥0 be a sequence of (ğœğ‘–)-generalized KR rearrangements on Rğ‘Ã— Rğ‘›Ã— Rğ‘›,
which are of the form
Mğ‘–(ğ‘¥ğœƒ, ğ‘¥ğ‘–, ğ‘¥ğ‘–+1) =
â¡
â¢â¢â¢â£
MÎ˜
ğ‘–(ğ‘¥ğœƒ)
M0
ğ‘–(ğ‘¥ğœƒ, ğ‘¥ğ‘–, ğ‘¥ğ‘–+1)
M1
ğ‘–(ğ‘¥ğœƒ, ğ‘¥ğ‘–+1)
â¤
â¥â¥â¥â¦
(5.37)
for some ğœğ‘–, MÎ˜
ğ‘–: Rğ‘â†’Rğ‘, M0
ğ‘–: Rğ‘Ã— Rğ‘›Ã— Rğ‘›â†’Rğ‘›, M1
ğ‘–: Rğ‘Ã— Rğ‘›â†’Rğ‘›, and
that are defined by the recursion:
â€“ M0 pushes forward ğœ‚ğ‘‹Î˜,ğ‘‹0,ğ‘‹1 to ğœ‹0 = Ìƒï¸€ğœ‹0/c0,
â€“ Mğ‘–pushes forward ğœ‚ğ‘‹Î˜,ğ‘‹ğ‘–,ğ‘‹ğ‘–+1 to
ğœ‹ğ‘–(ğ‘§ğœƒ, ğ‘§ğ‘–, ğ‘§ğ‘–+1) = ğœ‚ğ‘‹Î˜,ğ‘‹ğ‘–(ğ‘§ğœƒ, ğ‘§ğ‘–) Ìƒï¸€ğœ‹ğ‘–(TÎ˜
ğ‘–âˆ’1(ğ‘§ğœƒ), M1
ğ‘–âˆ’1(ğ‘§ğœƒ, ğ‘§ğ‘–), ğ‘§ğ‘–+1)/cğ‘–,
(5.38)
where cğ‘–is a normalizing constant, the map TÎ˜
ğ‘—:= MÎ˜
0 âˆ˜Â· Â· Â· âˆ˜MÎ˜
ğ‘—for all ğ‘—â‰¥0, and
where (Ìƒï¸€ğœ‹ğ‘–)ğ‘–â‰¥0 are functions on Rğ‘Ã— Rğ‘›Ã— Rğ‘›given by:
â€“ Ìƒï¸€ğœ‹0(ğ‘§ğœƒ, ğ‘§0, ğ‘§1) = ğœ‹Î˜,ğ‘0,ğ‘1(ğ‘§ğœƒ, ğ‘§0, ğ‘§1) ğœ‹ğ‘Œ0|ğ‘0,Î˜(ğ‘¦0|ğ‘§0, ğ‘§ğœƒ) ğœ‹ğ‘Œ1|ğ‘1,Î˜(ğ‘¦1|ğ‘§1, ğ‘§ğœƒ),
â€“ Ìƒï¸€ğœ‹ğ‘–(ğ‘§ğœƒ, ğ‘§ğ‘–, ğ‘§ğ‘–+1) = ğœ‹ğ‘ğ‘–+1|ğ‘ğ‘–,Î˜(ğ‘§ğ‘–+1|ğ‘§ğ‘–, ğ‘§ğœƒ) ğœ‹ğ‘Œğ‘–+1|ğ‘ğ‘–+1,Î˜(ğ‘¦ğ‘–+1|ğ‘§ğ‘–+1, ğ‘§ğœƒ) for ğ‘–â‰¥1.
Then, for all ğ‘˜â‰¥0, the following hold:
1. The map Ìƒï¸
Mğ‘˜, defined as
Ìƒï¸
Mğ‘˜(ğ‘¥ğœƒ, ğ‘¥ğ‘˜+1) =
â¡
â£TÎ˜
ğ‘˜(ğ‘¥ğœƒ)
M1
ğ‘˜(ğ‘¥ğœƒ, ğ‘¥ğ‘˜+1)
â¤
â¦,
(5.39)
pushes forward ğœ‚ğ‘‹Î˜,ğ‘‹ğ‘˜+1 to ğœ‹Î˜,ğ‘ğ‘˜+1|ğ‘¦0,...,ğ‘¦ğ‘˜+1.
[filtering]
2. The composition of transport maps Tğ‘˜= ğ‘‡0 âˆ˜Â· Â· Â· âˆ˜ğ‘‡ğ‘˜, where each ğ‘‡ğ‘–is defined
149

as
ğ‘‡ğ‘–(ğ‘¥ğœƒ, ğ‘¥0, . . . , ğ‘¥ğ‘˜+1) =
â¡
â¢â¢â¢â¢â¢â¢â¢â¢â¢â¢â¢â¢â¢â¢â¢â¢â¢â¢â¢â¢â¢â¢â£
MÎ˜
ğ‘–(ğ‘¥ğœƒ)
ğ‘¥0
...
ğ‘¥ğ‘–âˆ’1
M0
ğ‘–(ğ‘¥ğœƒ, ğ‘¥ğ‘–, ğ‘¥ğ‘–+1)
M1
ğ‘–(ğ‘¥ğœƒ, ğ‘¥ğ‘–+1)
ğ‘¥ğ‘–+2
...
ğ‘¥ğ‘˜+1
â¤
â¥â¥â¥â¥â¥â¥â¥â¥â¥â¥â¥â¥â¥â¥â¥â¥â¥â¥â¥â¥â¥â¥â¦
,
(5.40)
pushes forward ğœ‚ğ‘‹Î˜,ğ‘‹0,...,ğ‘‹ğ‘˜+1 to ğœ‹Î˜,ğ‘0,...,ğ‘ğ‘˜+1|ğ‘¦0,...,ğ‘¦ğ‘˜+1.
[full Bayesian solution]
3. The model evidence (marginal likelihood) is given by (5.31).
Theorem 5.4 suggests a variational algorithm for the joint parameter and state
estimation problem that is similar to the one proposed in Theorem 5.3: compute the
sequence of maps (Mğ‘–), each of dimension 2ğ‘›+ğ‘; embed them into higher-dimensional
identity maps to form (ğ‘‡ğ‘–) according to (5.40); then evaluate the composition Tğ‘˜=
ğ‘‡0 âˆ˜Â· Â· Â· âˆ˜ğ‘‡ğ‘˜to sample directly from ğœ‹Î˜,ğ‘0:ğ‘˜+1|ğ‘¦0:ğ‘˜+1 (i.e., the full Bayesian solution).
Each map Mğ‘–is now of dimension twice that of the model state plus the dimension
of the hyperparameters.
This dimension is slightly higher than that of the maps
(Mğ‘–) considered in Theorem 5.3, and should be regarded as the price to pay for
introducing hyperparameters in the state-space model and having to deal with the
Markov structure of Figure 5-8 as opposed to the tree structure of Figure 5-7. By
Theorem 5.4[Part 1], the composition of maps TÎ˜
ğ‘˜= MÎ˜
0 âˆ˜Â· Â· Â·âˆ˜MÎ˜
ğ‘˜provides a recursive
characterization of the posterior distribution over the static parameters, ğœ‹Î˜|ğ‘¦0:ğ‘˜+1, for
all ğ‘˜â‰¥0. The latter is often the ultimate goal of inference [6]. In order to have
a sequential algorithm for parameter estimation, we also need to keep a running
approximation of TÎ˜
ğ‘˜using the recursion TÎ˜
ğ‘˜= TÎ˜
ğ‘˜âˆ’1 âˆ˜MÎ˜
ğ‘˜â€”e.g., via regressionâ€”so
that the cost of evaluating TÎ˜
ğ‘˜does not grow with ğ‘˜.
Even in the joint parameter and state estimation case, only a single forward pass
150

with local computations is necessary to gather all the information from the state-space
model needed to sample the collection of posteriors (ğœ‹Î˜, ğ‘0:ğ‘˜+1|ğ‘¦0:ğ‘˜+1). Notice that the
accuracy of the variational procedure is only limited by the accuracy of each computed
map, and that the proposed approach does not prescribe an artificial dynamic for the
parameters [152, 177], or an a priori fixed-lag smoothing approximation [218]. Yet,
there is no rigorous proof that the performance of the proposed sequential algorithm
for parameter estimation does not deteriorate with time. Indeed, developing exact,
sequential, and online algorithms for parameter estimation in general non-Gaussian
state-space models is among the chief research challenges in SMC methods [136]. See
[55, 71, 82] for recent contributions in this direction and [148] for a review of SMC
approaches to Bayesian parameter inference. See also [91] for a hybrid approach that
combines elements of variational inference with particle filters.
We refer the reader to Section 5.8.1 for a numerical illustration of parameter
estimation with transport maps involving a stochastic volatility model.
5.6.5
Fixed-point smoothing
Consider again the problem of sequential inference in a state-space model without
static parameters (see Figure 5-7), and suppose that we are interested only in the
smoothing marginal ğœ‹ğ‘0|ğ‘¦0:ğ‘˜for all ğ‘˜â‰¥0; this is the fixed-point smoothing problem
[245].
In Section 5.6.1 we showed that computing a sequence of maps (Mğ‘–)â€”each of
dimension 2ğ‘›â€”is sufficient to sample the joint distribution ğœ‹ğ‘0:ğ‘˜+1|ğ‘¦0:ğ‘˜+1 by evaluating
the composition Tğ‘˜= ğ‘‡0 âˆ˜Â· Â· Â· âˆ˜ğ‘‡ğ‘˜, where each ğ‘‡ğ‘–is a trivial embedding of Mğ‘–into an
identity map. If we can sample ğœ‹ğ‘0:ğ‘˜+1|ğ‘¦0:ğ‘˜+1, then it is easy to obtain samples from the
marginal ğœ‹ğ‘0|ğ‘¦0:ğ‘˜+1: in fact, it suffices to evaluate only the first ğ‘›components of Tğ‘˜,
which can be interpreted as a map from Rğ‘›Ã—(ğ‘˜+2) to Rğ‘›. To do so, however, we need
to evaluate ğ‘˜maps. A natural question then is whether it is possible to characterize
ğœ‹ğ‘0|ğ‘¦0:ğ‘˜+1 via a single transport map that is updated recursively in time, as opposed
to a growing composition of maps.
Here we propose a solutionâ€”certainly not the only possibilityâ€”based on the the-
151

ory of Section 5.6.4. The idea is to treat ğ‘0 as a static parameter, i.e., to set Î˜ := ğ‘0
and apply the results of Theorem 5.4 to the Markov structure of Figure 5-9. The
resulting algorithm computes a sequence of maps (Mğ‘–) of dimension 3ğ‘›, i.e., three
times the state dimension, and keeps a running approximation of TÎ˜
ğ‘˜via the recursion
TÎ˜
ğ‘˜= TÎ˜
ğ‘˜âˆ’1 âˆ˜MÎ˜
ğ‘˜, where each MÎ˜
ğ‘˜is just a subcomponent of Mğ‘˜. These maps (Mğ‘–)
are higher-dimensional than those considered in Section 5.6.1, but they do yield the
desired result: each TÎ˜
ğ‘˜: Rğ‘›â†’Rğ‘›characterizes the smoothing marginal ğœ‹ğ‘0|ğ‘¦0:ğ‘˜+1,
for all ğ‘˜â‰¥0, via a single transport map that is updated recursively in time with just
one forward pass (see Theorem 5.4[Part 1]).
ğ‘1
ğ‘2
ğ‘3
ğ‘4
ğ‘ğ‘
ğ‘0
Figure 5-9: I-map (certainly not minimal) for ğœ‹ğ‘0,ğ‘1:ğ‘|ğ‘¦0:ğ‘, for any ğ‘> 0. Orange
edges have been added compared to the tree structure of Figure 5-7.
5.7
Low-rank transport maps
In this section, we characterize yet another source of low-dimensional structure in
the representation of a transport map, based on the notion of low-rank functions.
Intuitively, a low-rank transport is a function that is low-dimensional up to a rotation
of the space, i.e., a function whose action is nontrivial only along a low-dimensional
subspace of the input space. We will see that this type of structure appears quite
naturally in the context of high-dimensional Bayesian inference problems (e.g., inverse
problems, spatial statistics) where the data may be informative only about a few
directions in the parameter space [261, 74].
5.7.1
General result
Let us start with a formal definition.
152

Definition 5.4 (Low-rank map). We say that ğ‘‡: Rğ‘›â†’Rğ‘›has rank (at most) ğœ…â‰¤ğ‘›,
if there exists a pair of rotation matrices (ğ‘„, ğ‘‰) on Rğ‘›such that for all ğ‘¥âˆˆRğ‘›,
ğ‘‡(ğ‘¥) = (ğ‘„âˆ˜Ì‚ï¸€ğ‘‡âˆ˜ğ‘‰)(ğ‘¥) = ğ‘ˆÂ· Ì‚ï¸€ğ‘‡( ğ‘‰ğ‘¥)
(5.41)
with Ì‚ï¸€ğ‘‡: Rğ‘›â†’Rğ‘›of the form
Ì‚ï¸€ğ‘‡(ğ‘¥) =
â¡
â¢â¢â¢â¢â¢â¢â£
M(ğ‘¥1, . . . , ğ‘¥ğœ…)
ğ‘¥ğœ…+1
...
ğ‘¥ğ‘›
â¤
â¥â¥â¥â¥â¥â¥â¦
(5.42)
for some M : Rğœ…â†’Rğœ….
There is no universal definition for a low-rank nonlinear multivariate function;
here, we have opted for a simple definition that suits our purposes. One can certainly
imagine more general characterizations of low-rank maps based, for instance, on tensor
decompositions [115, 202].
The following theorem identifies a set of conditions on the referenceâ€“target pair
(ğœˆğœ‚, ğœˆğœ‹) that guarantee the existence of at least one low-rank transport. In what
follows, let ğœ‚, ğœ‹denote the densities of ğœˆğœ‚and ğœˆğœ‹, respectively.
Theorem 5.5. Let ğ‘‹âˆ¼ğœ‚= ğ’©(0, I) and ğ‘âˆ¼ğœ‹. The following hold:
1. If there exists a rotation ğ‘„of Rğ‘›such that the pullback ğœ‹ğ‘„:= ğ‘„â™¯ğœ‹factorizes
as
ğœ‹ğ‘„(ğ‘¥) = ğ›¿(ğ‘¥1, . . . , ğ‘¥ğœ…) ğœ‚ğ‘‹ğœ…+1,...,ğ‘‹ğ‘›(ğ‘¥ğœ…+1, . . . , ğ‘¥ğ‘›)
(5.43)
for some positive density ğ›¿: Rğœ…â†’R with ğœ…> 0, then there is a low-rank
153

transport ğ‘‡of the form
ğ‘‡(ğ‘¥) = (ğ‘„âˆ˜Ì‚ï¸€ğ‘‡)(ğ‘¥) = ğ‘„Â·
â¡
â¢â¢â¢â¢â¢â¢â£
M(ğ‘¥1, . . . , ğ‘¥ğœ…)
ğ‘¥ğœ…+1
...
ğ‘¥ğ‘›
â¤
â¥â¥â¥â¥â¥â¥â¦
,
(5.44)
such that ğ‘‡â™¯ğœ‚= ğœ‹, where M can be any map that pushes forward ğœ‚ğ‘‹1,...,ğ‘‹ğœ…to ğ›¿.
2. If, additionally, ğœ‹is smooth and there exists any smooth density ğœŒwith finite
second central moment such that
âˆ«ï¸€
â€–âˆ‡log ğœ‹(ğ‘¥)â€–2 ğœŒ(ğ‘¥) dğ‘¥< âˆ, we can define
a matrix CğœŒâˆˆRğ‘›Ã—ğ‘›as
(CğœŒ)ğ‘–ğ‘—=
âˆ«ï¸
ğœ•ğ‘–r(ğ‘¥) ğœ•ğ‘—r(ğ‘¥) ğœŒ(ğ‘¥) dğ‘¥
(5.45)
for all ğ‘–, ğ‘—= 1, . . . , ğ‘›, with r := log(ğœ‹/ğœ‚). Then the rotation ğ‘„of Part 1 exists
if and only if rank(CğœŒ) â‰¤ğœ…. In this case, a valid choice for ğ‘„is ğ‘„= [ğ‘„1 | ğ‘„2],
where ğ‘„1 is a basis for the range of CğœŒ, while ğ‘„2 is a basis for Rğ‘›âˆ–span(ğ‘„1).
Theorem 5.5[Part 1] shows that if the â€œdifferenceâ€ between reference and tar-
get density lies in a ğœ…-dimensional subspace of the parameter spaceâ€”not necessarily
aligned with the coordinate axesâ€”then both densities can be coupled with a low-rank
map that is essentially ğœ…-dimensional. The potentially nonlinear map Ì‚ï¸€ğ‘‡in (5.44) is
low-dimensional with respect to {ğœ…+ 1, . . . , ğ‘›}, according to Definition 5.1, and thus
its effective dimension is upper bounded by ğœ…. An important question is then whether
there exists a rotation, ğ‘„, of Rğ‘›that guarantees the factorization (5.43) of the pull-
back density ğ‘„â™¯ğœ‹. There are multiple answers to this question. One the one hand,
the existence and form of ğ‘„might be clear from the context (see the forthcoming
section 5.7.2 for a relevant example). In some other cases, however, they are not,
and one can rely on Theorem 5.5[Part 2] for a necessary and sufficient condition
for the existence of such rotation, provided that ğœ‹satisfies some additional smooth-
ness assumptions. Part 2 of the theorem uses CğœŒâ€”a particular average derivative
154

functional [242, 238, 67]â€”to certify the existence of a low-rank transport, whenever
rank(CğœŒ) â‰¤ğœ….
From a computational point of view, Theorem 5.5 has important consequences:
the existence of a transport map that factorizes as (5.44) can be established before
computing the transport itself. For instance, we might just need to determine an
eigenvalue decomposition of CğœŒ[68]. Thus, when solving a variational problem for a
transport map ğ‘‡, we can impose the additional constraint ğ‘‡= ğ‘„âˆ˜Ì‚ï¸€ğ‘‡and optimize
over Ì‚ï¸€ğ‘‡. The advantage of this approach is that we only need to parameterize Mâ€”a
ğœ…-dimensional mapâ€”instead of looking for a coupling in the full space. Simple algebra
shows that Ì‚ï¸€ğ‘‡can be any transport map that pushes forward ğœ‚to the pullback density
ğœ‹ğ‘„= ğ‘„â™¯ğœ‹, including the KR rearrangement of Section 5.2. In particular, we can
formulate a variational problem like (5.5) for a map Ì‚ï¸€ğ‘‡such that Ì‚ï¸€ğ‘‡â™¯ğœ‚= ğœ‹ğ‘„. If we do
so, then (5.43) shows that we can approximate the integrand in the objective of (5.5)
as if it were a function on a ğœ…-dimensional space as opposed to a function on Rğ‘›.
Depending on the value of ğœ…, we can then consider the use of integration techniques
with faster convergence rates than regular Monte Carlo (e.g., quasi-Monte Carlo or
cubature rules [84]).
The reference distribution in Theorem 5.5 is a standard normal. This is the first
time we make an explicit distributional assumption on ğœ‚(other than independence).
In fact, this hypothesis can be relaxed, but not too much. For the theorem to be useful
in general settingsâ€”e.g., for a general CğœŒâ€”ğœ‚âˆ˜ğ‘„should factorize as the product of its
marginals for all rotations ğ‘„of Rğ‘›(see the proof of Theorem 5.5). A nice argument
by Kac shows that the class of such densities consists precisely of isotropic Gaussians,
i.e., ğœ‚= ğ’©(0, ğœ2I) for any ğœâˆˆR [142, 39]. In Theorem 5.5, we chose ğœ= 1.
5.7.2
Bayesian inference: local likelihood and the prior map
Given an arbitrary target density ğœ‹, there need not exist a rotation ğ‘„that satisfies
the hypothesis of Theorem 5.5[Part 1]. Nevertheless, in this section we show how, in
a rich class of Bayesian inference problems, it is possible to transform the posterior
distribution to fit the hypothesis of Theorem 5.5 with ğœ…< ğ‘›.
155

Let us first recall some notation for simple Bayesian inference problems.
We
denote by ğ‘the latent variables with prior density ğœ‹ğ‘and by ğ‘Œthe observables
with likelihood function ğœ‹ğ‘Œ|ğ‘. Our target is the posterior distribution of ğ‘given the
event {ğ‘Œ= ğ‘¦} for some fixed ğ‘¦, i.e., ğœ‹= ğœ‹ğ‘|ğ‘¦âˆğœ‹ğ‘¦|ğ‘ğœ‹ğ‘, where
ğœ‹ğ‘|ğ‘¦(ğ‘§) := ğœ‹ğ‘|ğ‘Œ(ğ‘§|ğ‘¦)
and
ğœ‹ğ‘¦|ğ‘(ğ‘§) := ğœ‹ğ‘Œ|ğ‘(ğ‘¦|ğ‘§).
(5.46)
Now assume that the likelihood function is localized, i.e., that ğœ‹ğ‘¦|ğ‘depends only on
ğœ…< ğ‘›latent variables. This is a common situation in, e.g., spatial statistics with
point processes (see Section 5.8.2 for an example). Thus, up to a reordering of the
input variables, we can assume that the likelihood function is of the form
ğœ‹ğ‘¦|ğ‘(ğ‘§) = ğ‘”(ğ‘§1, . . . , ğ‘§ğœ…)
(5.47)
for some ğ‘”: Rğœ…â†’R, so that ğœ‹ğ‘|ğ‘¦(ğ‘§) âˆğ‘”(ğ‘§1, . . . , ğ‘§ğœ…) ğœ‹ğ‘(ğ‘§). If ğœ‚= ğ’©(0, I), then the
pair (ğœ‚, ğœ‹) need not satisfy the hypothesis of Theorem 5.5, but consider for a moment
a simple transformation of the target density. Let ğ‘‡pr : Rğ‘›â†’Rğ‘›be a (block) lower
triangular map of the form
ğ‘‡pr(ğ‘¥) =
â¡
â£ğ‘‡0
pr(ğ‘¥1:ğœ…)
ğ‘‡1
pr(ğ‘¥1:ğœ…, ğ‘¥ğœ…+1:ğ‘›)
â¤
â¦,
(5.48)
for some ğ‘‡0
pr : Rğœ…â†’Rğœ…and ğ‘‡1
pr : Rğ‘›â†’Rğœ…, that pushes forward ğœ‚to ğœ‹ğ‘. Such a map
always exists, e.g., take a KR rearrangement, and notice that the pullback density
ğ‘‡â™¯
prğœ‹can be written as
ğ‘‡â™¯
prğœ‹(ğ‘§) = ğ‘”
(ï¸€
ğ‘‡0
pr(ğ‘§1, . . . , ğ‘§ğœ…)
)ï¸€
ğœ‚(ğ‘§).
(5.49)
The referenceâ€“target pair (ğœ‚, ğ‘‡â™¯
prğœ‹) now does satisfy the hypothesis of Theorem 5.5[Part
1] with ğ‘„equal to the identity matrix. Thus there exists a low-rank map ğ‘„âˆ˜Ì‚ï¸€ğ‘‡, of
the form (5.44), that pushes forward ğœ‚to ğ‘‡â™¯
prğœ‹. At the same time, there should also
156

exist a transport map ğ‘‡that pushes forward ğœ‚to ğœ‹and that decomposes10 as:
ğ‘‡= ğ‘‡pr âˆ˜ğ‘„âˆ˜Ì‚ï¸€ğ‘‡.
(5.50)
The maps ğ‘‡pr and ğ‘„have a simple structure: ğ‘„is a rotation, while ğ‘‡prâ€”which we call
the prior mapâ€”pushes forward ğœ‚to ğœ‹ğ‘. The prior map is usually simple to compute.
In many cases, it can be can be written down analytically [273]; alternatively, if we
can sample the prior distribution, then we can determine ğ‘‡pr as the solution of a
convex and separable optimization problem [212] that is independent of the data. In
fact, the only map in (5.50) which actually depends on ğ‘Œis Ì‚ï¸€ğ‘‡. But Ì‚ï¸€ğ‘‡is essentially
a ğœ…-dimensional map, and ğœ…does not depend on ğ‘›â€”the nominal dimension of the
inverse problemâ€”but only on the likelihood function ğœ‹ğ‘Œ|ğ‘.
A key ingredient of this result is the sparsity pattern of the prior map (5.48). In
this case, we could use a (block) lower triangular map because we assumed that the
observed variables had indices 1, . . . , ğœ…. This is always possible modulo a permutation
of the latent variables.
A different sparsity pattern of ğ‘‡pr precludes, in general,
the existence of a simple transport like (5.50) (see Example E.1 in Appendix E).
Nevertheless, there is a practically relevant scenario where the sparsity pattern of the
prior map does not matter. Consider the case of a linear ğ‘‡pr that pushes forward
ğœ‚= ğ’©(0, I) to a Gaussian prior ğœ‹ğ‘. Then, for a localized likelihood of the form
(5.47), it is immediate to verify the existence of a transport map that decomposes as
(5.50). This result is general, but it is particularly easy to grasp if we assume, for
a second, that the additional regularity assumptions of Theorem 5.5[Part 2] hold for
the density pair (ğœ‚, ğ‘‡â™¯
pr ğœ‹). In this case, rank(CğœŒ) â‰¤ğœ…, independent of the sparsity
pattern of ğ‘‡pr, since the gradient of a linear map ğ‘‡pr is constant over Rğ‘›.
10 The transport ğ‘‡in (5.50) is not decomposable in the sense of Section 5.5.
157

5.7.3
Low-rank likelihood
There is another example of low-dimensional structure in the likelihood that guaran-
tees the existence of a low-rank transport. Assume that
ğœ‹ğ‘¦|ğ‘(ğ‘§) = ğ‘”( Î ğœ…ğ‘§),
(5.51)
for some ğ‘”: Rğ‘›â†’R and a rank-ğœ…projector Î ğœ…, i.e., Î 2
ğœ…= Î ğœ….
(Notice that
we fall under the hypothesis of the previous section if we consider an orthogonal
projector onto the first ğœ…coordinate directions. The forthcoming analysis, however,
is more general.) It turns out that the likelihood of a broad class of high-dimensional
Bayesian inverse problems [265] can be well approximated by (5.51) for ğœ…â‰ªğ‘›. This
is precisely what we saw in Chapter 3 for linear Gaussian problems [261], and what
is shown in [74, 73, 69] for nonlinear inverse problems. The intuition is simple: for
problems with high-dimensional latent fields (as arising from the discretization of a
distributed stochastic process) and limited indirect observations, the data will only
inform, relative to the prior, few linear combinations of the latent variables. As a
result, the prior-to-posterior update can be confined to a low-dimensional subspace
of the latent space.
Now let ğ‘„be any orthonormal matrix on Rğ‘›with the following structure: ğ‘„=
[ğ‘„1 | ğ‘„2], where the columns of ğ‘„2 âˆˆRğ‘›Ã—(ğ‘›âˆ’ğœ…) form a basis for the nullspace of Î ğœ….
It is easy to verify that the pullback density ğ‘„â™¯ğœ‹can now be written as
ğ‘„â™¯ğœ‹(ğ‘§) = ^ğ‘”(ğ‘§1, . . . , ğ‘§ğœ…) ğœ‹ğ‘(ğ‘„ğ‘§),
(5.52)
for some ^ğ‘”: Rğœ…â†’R, and hence all the analysis of Section 5.7.2 for localized likeli-
hoods readily applies, if we interpret ğ‘„â™¯ğœ‹as the new posterior and ğœ‹ğ‘âˆ˜ğ‘„as the new
prior.
158

5.8
Numerical examples
We present two numerical examples: The former is a joint parameter and state es-
timation problem on a stochastic volatility model, which illustrates some aspects of
the theory developed in Section 5.6 for decomposable transports on non-Gaussian
state-space models. The latter example illustrates the role of low-rank transports in
a high-dimensional log-Gaussian Cox process with sparse observations, and demon-
strates some of the theory discussed in Section 5.7.
5.8.1
Stochastic volatility model with hyperparameters
Following [149, 237], we model the scalar log-volatility (ğ‘ğ‘˜) of the return of a financial
asset at time ğ‘˜= 0, . . . , ğ‘using an autoregressive process of order one, which is
fully specified by ğ‘ğ‘˜+1 = ğœ‡+ ğœ‘(ğ‘ğ‘˜âˆ’ğœ‡) + ğœ€ğ‘˜, for all ğ‘˜â‰¥0, where ğœ€ğ‘˜âˆ¼ğ’©(0, 1)
is independent of ğ‘ğ‘˜, ğ‘0|ğœ‡, ğœ‘âˆ¼ğ’©(ğœ‡,
1
1âˆ’ğœ‘2), and where ğœ‘and ğœ‡represent scalar
hyperparameters of the model. In particular, ğœ‡âˆ¼ğ’©(0, 1) and ğœ‘= 2 exp(ğœ‘â‹†)/(1 +
exp(ğœ‘â‹†)) âˆ’1 with ğœ‘â‹†âˆ¼ğ’©(3, 1). We define Î˜ := (ğœ‡, ğœ‘). The process (ğ‘ğ‘˜) and
parameters Î˜ are unobserved and must be estimated from an observed process (ğ‘Œğ‘˜),
which represents the mean return on holding the asset at time ğ‘˜, ğ‘Œğ‘˜= ğœ‰ğ‘˜exp(1
2ğ‘ğ‘˜),
where ğœ‰ğ‘˜is a standard normal random variable independent of ğ‘ğ‘˜. As a dataset
(ğ‘¦ğ‘˜)ğ‘
ğ‘˜=0, we use the ğ‘+1 daily differences of the pound/dollar exchange rate starting
on 1 October 1981 with ğ‘= 100 [237, 88].
Our goal is to sequentially characterize ğœ‹Î˜,ğ‘0:ğ‘˜|ğ‘¦0:ğ‘˜, for all ğ‘˜= 0, . . . , ğ‘, as ob-
servations of (ğ‘Œğ‘˜) become available. The Markov structure of ğœ‹Î˜,ğ‘0:ğ‘|ğ‘¦0:ğ‘matches
Figure 5-8. We solve the problem using the algorithm introduced in Section 5.6.4: we
compute a sequence, (Mğ‘—)ğ‘âˆ’1
ğ‘—=0 , of four-dimensional transport maps (ğ‘›= dim(ğ‘ğ‘—) = 1
and ğ‘= dim(Î˜) = 2) according to their definition in Theorem 5.4 and using the vari-
ational form (5.5). All reference densities are standard Gaussians. Then, by Theorem
5.4[part 1], for any ğ‘˜< ğ‘, we can easily sample the filtering marginal ğœ‹ğ‘ğ‘˜+1|ğ‘¦0:ğ‘˜+1 by
pushing forward a standard normal through the subcomponent M1
ğ‘˜of Mğ‘˜, and we can
also sample the posterior distribution over the static parameters ğœ‹Î˜|ğ‘¦0:ğ‘˜+1 by pushing
159

forward a standard normal through the map TÎ˜
ğ‘˜. The map TÎ˜
ğ‘˜= MÎ˜
0 âˆ˜Â· Â· Â· âˆ˜MÎ˜
ğ‘˜is
updated sequentially over time (via regression) using the recursion TÎ˜
ğ‘˜= TÎ˜
ğ‘˜âˆ’1 âˆ˜MÎ˜
ğ‘˜,
so that the cost of evaluating TÎ˜
ğ‘˜does not increase with ğ‘˜.
The resulting algo-
rithm for parameter estimation is thus sequential. Moreover if we want to sample
ğœ‹Î˜,ğ‘0:ğ‘˜+1|ğ‘Œ0:ğ‘˜+1â€”the full Bayesian solution at time ğ‘˜+ 1â€”we simply need to embed
each Mğ‘—into an identity map to form the transport ğ‘‡ğ‘—, for ğ‘—= 0, . . . , ğ‘˜, and push for-
ward reference samples through the composition Tğ‘˜= ğ‘‡0 âˆ˜Â· Â· Â· âˆ˜ğ‘‡ğ‘˜(Theorem 5.4[part
2]).
Figure 5-10 shows the resulting filtering and smoothing marginals of the states
over time. Figure 5-11 collects the corresponding posterior marginals over the static
parameters, while Figure 5-12 (left) shows some trajectories drawn from the smooth-
ing distribution.
Figure 5-14 (left) illustrates the marginals of the posterior data
predictive together with the observed data (ğ‘¦ğ‘˜), showing excellent coverage overall.
Our results rely on a numerical approximation of the desired transport maps.
Each component of Mğ‘˜is parameterized via the monotone representation (5.4) using
linear combinations of tensorized radial basis functions, while the expectation in (5.5)
is approximated using tensorized Gauss quadrature rules. Both the number of basis
in the parameterization of the map and the quadrature order are decided adaptively
following the algorithm in [30]. The resulting minimization problems are sequentially
solved using the Newton-CG method [276].
This test case was run using a dedi-
cated software publicly available at http://transportmaps.mit.edu. The website
contains also additional details about possible parameterizations of the maps.
There are several ways to investigate the quality of these approximations. Fig-
ures 5-12 (right) and 5-13 compare the numerical approximation (via a decomposable
transport map) of the smoothing marginals of the states and the static parameters
to a â€œreferenceâ€ solution obtained via MCMC. The MCMC chain is run until it yields
105 effectively independent samples. The two solutions agree remarkably well and are
almost indistinguishable in most places. (Of course, MCMC in this context is not
a sequential algorithm; it requires that all the data (ğ‘¦ğ‘˜)ğ‘
ğ‘˜=0 be available simultane-
ously.) An important fact is that the MCMC chain is generated using an independent
160

proposal [233] given by the pushforward of a standard Gaussian through the numer-
ical approximation of Tğ‘âˆ’1 (denoted as Ìƒï¸€Tğ‘âˆ’1). The resulting MCMC chain has an
acceptance rate slightly above 0.8, confirming the overall quality of the variational
approximation.
A second quality test can proceed as follows: since we use a standard Gaussian
reference distribution ğœˆğœ‚, we expect the pullback of ğœ‹Î˜,ğ‘0:ğ‘|ğ‘¦0:ğ‘through Ìƒï¸€Tğ‘âˆ’1 to be
close to a standard Gaussian. Figure 5-14 (right) supports this claim by showing
a collection of random two-dimensional conditionals of the approximate pullback:
these â€œslicesâ€ of the 103-dimensional (ğ‘+1 states plus two hyperparameters) pullback
distribution are identical to a two-dimensional standard normal, as expected. The
fact that we can evaluate the approximate pullback density is one of the key features
of this variational approach to inference. Even more, we can use this approximate
pullback density in (5.7) to estimate the KL divergence between our target ğœˆğœ‹(the full
Bayesian solution at time ğ‘) and the approximating measure (Ìƒï¸€Tğ‘âˆ’1)â™¯ğœˆğœ‚. A numerical
realization of (5.7) yields ğ’ŸKL( (Ìƒï¸€Tğ‘âˆ’1)â™¯ğœˆğœ‚|| ğœˆğœ‹) â‰ˆ6.2Ã—10âˆ’2, which confirms the good
numerical approximation of ğœˆğœ‹, a 103-dimensional target measure. For comparison,
we notice that the KL divergence from ğœˆğœ‹to its Laplace approximation (a Gaussian
approximation at the mode) is approximately â‰ˆ22.6â€”considerably worse than what
is achieved through optimization of a nonlinear transport map.
5.8.2
Log-Gaussian Cox point process with sparse observa-
tions
We consider an inference problem in spatial statistics for a log-Gaussian Cox point
process on a square domain ğ’Ÿ= [0, 1]2. This type of stochastic process is frequently
used to model spatially aggregated point patterns [193, 60, 237, 106].
Following
a configuration similar to [60, 193], we discretize ğ’Ÿinto a 64 Ã— 64 uniform grid,
and denote by ğ‘ ğ‘–âˆˆğ’Ÿthe center of the ğ‘–th cell, for ğ‘–= 1, . . . , ğ‘›, with ğ‘›= 642.
We consider a discrete stochastic process (ğ‘Œğ‘–)ğ‘›
ğ‘–=1, where ğ‘Œğ‘–denotes the number of
occurrences/points in the ğ‘–th cell.
(For instance, ğ‘Œğ‘–could denote the number of
161

0
20
40
60
80
100
time
2.5
2.0
1.5
1.0
0.5
0.0
0.5
1.0
1.5
0
20
40
60
80
100
time
2.5
2.0
1.5
1.0
0.5
0.0
Figure 5-10:
At each time ğ‘˜, we illustrate the {5, 25, 40, 60, 75, 95}â€“percentiles
(shaded regions) and the mean (solid line) of the numerical approximation of the
filtering distribution ğœ‹ğ‘ğ‘˜|ğ‘¦0:ğ‘˜(left) and of the marginals ğœ‹ğ‘ğ‘˜|ğ‘¦0:ğ‘of the full smoothing
distribution (right), for ğ‘˜= 0, . . . , ğ‘.
0
20
40
60
80
100
time
1.5
1.0
0.5
0.0
0.5
1.0
0
20
40
60
80
100
time
0.3
0.4
0.5
0.6
0.7
0.8
0.9
Figure 5-11:
At each time ğ‘˜, we illustrate the {5, 25, 40, 60, 75, 95}â€“percentiles
(shaded regions) and the mean (solid line) of the numerical approximation of the
posterior marginals over the static parameters: ğœ‹ğœ‡|ğ‘¦0:ğ‘˜(left) and ğœ‹ğœ‘|ğ‘¦0:ğ‘˜(right), for
ğ‘˜= 0, . . . , ğ‘.
162

0
20
40
60
80
100
time
2.5
2.0
1.5
1.0
0.5
0.0
0
20
40
60
80
100
time
2.5
2.0
1.5
1.0
0.5
0.0
Figure 5-12:
(left) The shaded regions and the red solid line have the same in-
terpretation as in Figure 5-10 (right), whereas the black solid lines correspond to
smoothed trajectories, i.e., independent realizations from the numerical approxima-
tion of ğœ‹ğ‘0:ğ‘|ğ‘¦0:ğ‘. (right) Comparison between the {5, 25, 75, 95}â€“percentiles (dashed
lines) and the mean (solid line) of the numerical approximation of the smoothing
marginals ğœ‹ğ‘ğ‘˜|ğ‘¦0:ğ‘via transport maps (red lines) versus a â€œreferenceâ€ solution obtained
via MCMC (black lines) with 105 effectively independent samples, for ğ‘˜= 0, . . . , ğ‘.
The two solutions are indistinguishable.
3
2
1
0
1
2
0.0
0.2
0.4
0.6
0.8
1.0
1.2
1.4
1.6
Ï€Âµ|Y0 : N
Transport map
MCMC
0.5
0.0
0.5
1.0
0
1
2
3
4
5
Ï€Ï†|Y0 : N
Transport Map
MCMC
Figure 5-13:
Comparison between the numerical approximation of the final-time
posterior marginals ğœ‹ğœ‡|ğ‘¦0:ğ‘(left) and ğœ‹ğœ‘|ğ‘¦0:ğ‘(right) via transport maps (solid lines)
versus a â€œreferenceâ€ solution obtained via MCMC (dashed lines) with 105 effectively
independent samples.
163

0
20
40
60
80
100
time
2.0
1.5
1.0
0.5
0.0
0.5
1.0
1.5
data
Figure 5-14:
(left) Shaded regions represent the {5, 25, 40, 60, 75, 95}â€“percentiles of
the marginals of the posterior predictive distribution (conditioning on all the data),
along with black dots that represent the observed data (ğ‘¦ğ‘˜)ğ‘
ğ‘˜=0. (right) Randomly
chosen two-dimensional conditionals of the pullback of ğœ‹Î˜,ğ‘0:ğ‘|ğ‘¦0:ğ‘through the numer-
ical approximation of Tğ‘âˆ’1. Since we use a standard normal reference distribution,
the numerical approximation of the transport map should be deemed satisfactory if
the corresponding pullback density is close to a standard normal, as it is here.
crimes in the ğ‘–th neighborhood during a certain period of time.) Each ğ‘Œğ‘–is modeled
as a Poisson random variable with mean exp(ğ‘ğ‘–)/ğ‘›, where (ğ‘ğ‘–) is a Gaussian process
with covariance
cov(ğ‘ğ‘–, ğ‘ğ‘—) = ğœ2 exp
(ï¸‚
âˆ’â€–ğ‘ ğ‘–âˆ’ğ‘ ğ‘—â€–2
ğ›½
)ï¸‚
,
(5.53)
and mean E[ğ‘ğ‘–] = ğœ‡, for all ğ‘–= 1, . . . , ğ‘›. We consider the following values for the
parameters: ğ›½= 7/10, ğœ= 1, and ğœ‡= 2 log 64. The (ğ‘Œğ‘–) are assumed conditionally
independent given the (latent) Gaussian field. For interpretability reasons, we also
define the intensity process (Î›ğ‘–)ğ‘›
ğ‘–=1 as Î›ğ‘–= exp(ğ‘ğ‘–), for ğ‘–= 1, . . . , ğ‘›[106].
The goal of this problem is to infer the posterior distribution of the latent process
ğ‘:= (ğ‘1, . . . , ğ‘ğ‘›) given few sparse realizations of (ğ‘Œğ‘–) at ğ‘‘= 30 spatial locations
ğ‘ ğ‘˜1, . . . , ğ‘ ğ‘˜ğ‘‘shown in Figure 5-15 (top left). Thus, we set ğ‘Œ:= (ğ‘Œğ‘˜1, . . . , ğ‘Œğ‘˜ğ‘‘) and
denote by ğ‘¦âˆˆRğ‘‘a realization of ğ‘Œobtained by sampling the latent Gaussian field
according to its marginal distribution (see Figure 5-15 (top left) for an illustration of
164

ğ‘¦). Our target distribution (ğœ‹) is then:
ğœ‹ğ‘|ğ‘¦(ğ‘§) := ğœ‹ğ‘|ğ‘Œ(ğ‘§|ğ‘¦).
(5.54)
We notice that the likelihood function is local, i.e., ğœ‹ğ‘¦|ğ‘(ğ‘§) = ğ‘”(ğ‘§ğ‘˜1, . . . , ğ‘§ğ‘˜ğ‘‘) for some
ğ‘”: Rğ‘‘â†’R, and thus all the discussion of Section 5.7.2 on low-rank transports
readily applies, provided that we perform a relabeling of the latent variables such
that ğ‘§ğ‘˜1, . . . , ğ‘§ğ‘˜ğ‘‘correspond to the first ğ‘‘inputs, i.e., we can assume, without loss of
generality, that ğœ‹ğ‘¦|ğ‘(ğ‘§) = ğ‘”(ğ‘§1, . . . , ğ‘§ğ‘‘), where ğ‘ğ‘–now refers to the latent process at
the ğ‘–th location ğ‘ ğ‘˜ğ‘–, for ğ‘–= 1, . . . , ğ‘‘. Section 5.7.2 shows that there exists a transport
map ğ‘‡that pushes forward a standard normal reference, ğœ‚, to the target ğœ‹ğ‘|ğ‘¦, which
can be written as ğ‘‡= ğ‘‡prâˆ˜Ì‚ï¸€ğ‘‡, where ğ‘‡pr is the so-called prior map, i.e., a transport that
pushes forward ğœ‚to ğœ‹ğ‘â€”the marginal distribution of ğ‘â€”with the sparsity pattern
in (5.48), and where Ì‚ï¸€ğ‘‡is essentially a ğ‘‘-dimensional function. Two remarks are in
order: First, the prior map can be chosen as the KR rearrangement (lower triangular)
that couples ğœ‚with ğœ‹ğ‘= ğ’©(ğœ‡, Î£), where ğœ‡= (ğœ‡, . . . , ğœ‡) and Î£ğ‘–ğ‘—= cov(ğ‘ğ‘–, ğ‘ğ‘—). In
particular, ğ‘‡pr is an affine function given by the Choleksy factor, ğ¿, of Î£, i.e.,
ğ‘‡pr(ğ‘¥) = ğœ‡+ ğ¿ğ‘¥=
â§
â¨
â©
ğœ‡1
ğœ‡2
â«
â¬
â­+
â¡
â£ğ¿11
0
ğ¿12
ğ¿22
â¤
â¦ğ‘¥,
(5.55)
where Î£ = ğ¿ğ¿âŠ¤and ğ¿11 âˆˆRğ‘‘Ã—ğ‘‘. Second, the map Ì‚ï¸€ğ‘‡pushes forward ğœ‚to ğ‘‡â™¯
pr ğœ‹ğ‘|ğ‘¦(ğ‘§) =
ğ‘”(ğœ‡1 + ğ¿11 ğ‘§1:ğ‘‘) ğœ‚(ğ‘§) and is of the form
Ì‚ï¸€ğ‘‡(ğ‘¥) =
â¡
â¢â¢â¢â¢â¢â¢â£
M(ğ‘¥1, . . . , ğ‘¥ğ‘‘)
ğ‘¥ğ‘‘+1
...
ğ‘¥ğ‘›
â¤
â¥â¥â¥â¥â¥â¥â¦
,
(5.56)
for some M : Rğ‘‘â†’Rğ‘‘.
The important point here is that Mâ€”really the only
nonlinear map involvedâ€”can be computed as the solution of a ğ‘‘â‰ªğ‘›dimensional
165

problem, regardless of ğ‘›, the nominal dimension of the latent field. That is, M can
be any map that pushes forward a standard normal to the density ğœ‰â†¦â†’ğ‘”(ğœ‡1 + ğ¿11 ğœ‰)
on Rğ‘‘â€”for instance a KR rearrangement. This remark has important implications
since it allows us to get away with computing a nonlinear map on R30 as opposed to
a map on R4096.
The numerical results in Figures 5-16 follow precisely this approach by character-
izing ğ‘‡= ğ‘‡prâˆ˜Ì‚ï¸€ğ‘‡via a numerical approximation of M (see Section 5.2). A comparison
versus a â€œreferenceâ€ solution obtained via MCMC shows almost identical low-order
posterior marginals (compare Figures 5-16 and 5-17). Lastly, a quantitative measure
of the quality of the approximation is given by (5.7): ğ’ŸKL( Ìƒï¸€ğ‘‡â™¯ğœˆğœ‚|| ğœˆğœ‹) â‰ˆ5.9 Ã— 10âˆ’2,
where Ìƒï¸€ğ‘‡denotes the numerical approximation of ğ‘‡, ultimately a transport map in
4096 dimensions.
166

6
7
8
9
10
0
1
2
3
4
5
6
0.00
0.32
0.64
0.96
1.28
1.60
1.92
2.24
2.56
Ã—104
0
1
2
3
4
5
6
Figure 5-15:
Realization of the latent Gaussian field (ğ‘ğ‘–) (top left), and of the cor-
responding intensity process (Î›ğ‘–) (top right), used to generate the Poisson data ğ‘Œ
for the inference problem. The centers of the circles represent the locations of the
observations, while the diameter of the circles is proportional to the magnitude of the
measurement ğ‘¦. (middle) Three different realizations from the numerical approxima-
tion (via transport maps) of the posterior distribution of the latent field, and of the
intensity process (bottom).
167

8
9
0.36
0.42
0.48
0.54
0.60
0.66
0.72
0.78
0.84
Figure 5-16: Mean (left) and standard deviation (right) of the numerical approxima-
tion (via transport maps) of the posterior distribution of the latent process.
8
9
0.36
0.42
0.48
0.54
0.60
0.66
0.72
0.78
0.84
Figure 5-17: Mean (left) and standard deviation (right) of a â€œreferenceâ€ approximation
of the posterior distribution of the latent process via MCMC with 105 effective sample
size. These posterior statistics are almost indistinguishable from those of Figure 5-16,
obtained via transport maps.
168

5.9
Discussion
This chapter has focused on the problem of coupling a pair (ğœˆğœ‚, ğœˆğœ‹) of absolutely
continuous measures on Rğ‘›, for the purpose of sampling or integration, e.g., in
the context of non-Gaussian Bayesian inference, by leveraging key sources of low-
dimensional structure in the underlying distributions. If ğœˆğœ‚is a tractable measure
(e.g., an isotropic Gaussian) and ğœˆğœ‹is an intractable measure of interest (e.g., a pos-
terior distribution), then a deterministic coupling enables principled approximations
of integrals via the identity
âˆ«ï¸€
ğ‘”dğœˆğœ‹=
âˆ«ï¸€
ğ‘”âˆ˜ğ‘‡dğœˆğœ‚. In other words, a deterministic
coupling provides a simple way to simulate ğœˆğœ‹by pushing forward samples from ğœˆğœ‚
through a transport map ğ‘‡. This idea, modulo some variations, has been exploited
in a variety of statistical applicationsâ€”some old, some newâ€”including random num-
ber generation [185], Bayesian inference [196, 247, 231, 178, 187], the computation of
model evidence ratios [191], model learning and density estimation [266, 160, 5, 263],
non-Gaussian proposals for MCMC or importance sampling [213, 16, 195, 126, 201],
multiscale methods [214], and filtering [77, 58, 229, 228, 126], to name a few. Indeed
there are infinitely many ways to transport one measure to another [271] and as many
ways to compute one.
This chapter establishes an explicit link between the condititional independence
structure of (ğœˆğœ‚, ğœˆğœ‹) and the existence of low-dimensional couplings induced by trans-
port maps that are sparse, decomposable, and/or low-rank. These results can enhance
a wide array of numerical approaches to the transportation of measures, including
[196, 266, 231, 178, 30], and thus facilitate integration with respect to complex dis-
tributions in high dimensions. We briefly discuss our main results below.
Sparse transports
A sparse transport is a map whose components do not de-
pend on all input variables. Section 5.4 derives tight bounds on the sparsity pattern
of the Knotheâ€“Rosenblatt (KR) rearrangement (a triangular transport map) based
solely on the Markov structure of ğœˆğœ‹, provided that ğœˆğœ‚is a tensor product reference
measure (Theorem 5.1). This analysis shows that the inverse of the KR rearrange-
ment is the natural generalization to the non-Gaussian case of the Cholesky factor
169

of the precision matrix of a Gaussian MRFâ€”in that both the inverse KR rearrange-
ment (a potentially nonlinear map) and the Cholesky factor (a linear map) have the
same sparsity pattern given target measures with the same Markov structure. Thus
the KR rearrangement can be used to extend well-known modeling and sampling
techniques for high-dimensional Gaussian MRFs [236] to non-Gaussian fields (Sec-
tion 5.4.2). Section 5.4 shows that sparsity is usually a feature of inverse transports,
while direct transports tend to be dense, even for the most trivial Markov struc-
tures. In fact, the sparsity of direct transports stems from marginal (rather than
conditional) independenceâ€”a property frequently exploited in localization schemes
for high-dimensional covariance estimation [103, 117].
Decomposable transports
A decomposable map is a function that can be written
as the composition of finitely many low-dimensional maps that are triangular up to
a permutationâ€”i.e., ğ‘‡= ğ‘‡1 âˆ˜Â· Â· Â· âˆ˜ğ‘‡â„“, where each ğ‘‡ğ‘–differs from the identity only
along a small subset of its components and is a generalized triangular function as
defined in Section 5.5. Theorem 5.2 shows that every target measure whose Markov
network admits a graph decomposition can be coupled with a tensor product (refer-
ence) measure via a decomposable map. Decomposable maps are important because
they are much easier to represent than arbitrary multivariate functions on Rğ‘›. In
general, these maps are non-triangular, even though each map in the composition is
generalized triangular. In fact, we can think of the generalized KR rearrangement in
Appendix B as the fundamental â€œbuilding blockâ€ of a much richer class of couplings:
the couplings induced by decomposable maps.
The notion of a decomposable map is different from the composition-of-maps
approaches advocated in the literature for the approximation of transport maps [266,
178, 231, 160].
In these approaches, very simple maps (ğ‘€ğ‘–)ğ‘–â‰¥1 are composed in
growing number to define a transport map of increasing complexity, ğ‘€= ğ‘€1âˆ˜Â· Â· Â·âˆ˜ğ‘€ğ‘˜.
The number of layers in ğ‘€depends on the desired accuracy of the transport and can
be arbitrarily large. On the other hand, a decomposable coupling is induced by a
special transport map that can be written exactly as the composition of finitely many
170

maps, ğ‘‡= ğ‘‡1 âˆ˜Â· Â· Â· âˆ˜ğ‘‡â„“, where each ğ‘‡ğ‘–has a specific sparsity pattern that makes it
low-dimensional. This definition does not specify a representation for ğ‘‡ğ‘–. In fact, each
ğ‘‡ğ‘–could itself be approximated by the composition of simple maps. The advantage of
targeting a decomposable transport, however, is the fact that the (ğ‘‡ğ‘–) are guaranteed
to be low-dimensional.
Approximate Markov properties
Sparsity and decomposability of certain trans-
port maps are induced by the Markov properties of the target measure. A natural
question is: what happens when ğœˆğœ‹satisfies some Markov properties only approxi-
mately? In particular, let ğœˆğœ‹be Markov with respect to ğ’¢, and assume that there
exists a measure ^ğœˆâˆˆM+(Rğ‘›) which is Markov with respect to a graph ^ğ’¢that is
sparser than ğ’¢and such that ğ’ŸKL( ^ğœˆ|| ğœˆğœ‹) < ğœ€, for some ğœ€> 0. For small ğœ€, we
would be tempted to use ^ğ’¢to characterize couplings of (ğœˆğœ‚, ğœˆğœ‹) that are possibly
sparser or more decomposable than those associated with ğ’¢. Concretely, if we are
interested in a triangular transport that pushes forward ğœˆğœ‚to ğœˆğœ‹, we could minimize
ğ’ŸKL( ğ‘‡â™¯ğœˆğœ‚|| ğœˆğœ‹) over the set of maps that have the same sparsity pattern as the KR
rearrangement between ğœˆğœ‚and ^ğœˆ. Bounds on this sparsity pattern are given by The-
orem 5.1 using only graph operations on ^ğ’¢; no explicit knowledge of ^ğœˆis required.
Alternatively, if we are interested in decomposable transports that push forward ğœˆğœ‚to
ğœˆğœ‹, we could minimize ğ’ŸKL( ğ‘‡â™¯ğœˆğœ‚|| ğœˆğœ‹) over the set of maps that factorize as any of
the decomposable transports between ğœˆğœ‚and ^ğœˆ. The shapes of these low-dimensional
factorizations are given by Theorem 5.2 using, once again, only graph operations on
^ğ’¢.
Now let ^ğ’¯denote the set of maps whose structure is constrained by ^ğ’¢in terms
of sparsity or decomposability. It is easy to show that
min
ğ‘‡âˆˆ^ğ’¯ğ’ŸKL( ğ‘‡â™¯ğœˆğœ‚|| ğœˆğœ‹) < ğœ€,
(5.57)
which means that the price of assuming that the coupling is either sparser or more
decomposable than it ought to be is just a small error in the approximation of ğœˆğœ‹.
171

Of course, the pending question is whether ğœˆğœ‹can be well approximated by a
measure that satisfies additional Markov properties.
There is some work on this
topic, e.g., [141, 140, 51]â€”especially in the case of Gaussian measuresâ€”but a more
thorough investigation of the problem remains an open and important direction for
future work. Interestingly, the transport map framework also allows one to adaptively
discover information about low-dimensional couplings. For instance, one might start
with a very sparse transport map and then incrementally decrease the sparsity level
of the map until the resulting approximation of ğœˆğœ‹becomes satisfactory. The same
can be done for decomposable transports. See [30] for some details on this idea.
Filtering and smoothing
Section 5.5.4 shows how not only the representation,
but also the computation, of a decomposable map, ğ‘‡= ğ‘‡1 âˆ˜Â· Â· Â· âˆ˜ğ‘‡â„“, can be broken
into a sequence of â„“simpler steps, each associated with a low-dimensional optimiza-
tion problem whose solution yields ğ‘‡ğ‘–. We give a concrete example of this idea for
filtering, smoothing, and sequential joint stateâ€“parameter inference in nonlinear and
non-Gaussian state-space models (Section 5.6). In this context, Theorems 5.3 and
5.4 introduce variational approaches for characterizing the full posterior distribution
of the sequential inference problem, essentially by performing only recursive lagâ€“1
smoothing with transport maps. The proposed approaches consist of a single forward
pass on the state-space model, and generalize the square-root Rauch-Tung-Striebel
smoother to non-Gaussian models (see Section 5.6.2). In practice, we should think of
Theorems 5.3 and 5.4 as providing â€œmeta-algorithmsâ€ within which all kinds of ap-
proximations can be introduced, e.g., linearizations of the forward model, restriction
to linear maps, and approximate flows [77, 178], to name a few. These approximations
are the workhorse of modern approaches to large-scale filtering, e.g., data assimilation
in geophysical applications [245, 93], and may play a key role in further instantiations
of the â€œmeta-algorithmsâ€ proposed in Section 5.6. Of course, it would be desirable to
complement such variational approximations with a rigorous error analysis like the
one that is available for SMC methods (e.g., [70, 81, 251]). It is also important to re-
mark that one can always use functionals like (5.7) to estimate the quality of a given
172

approximate map, or use the map itself to build advanced proposals for sampling
techniques like MCMC [213].
Low-rank transports
A low-rank transport is a map that is low-dimensional up
to a rotation of the space, i.e., maps whose action is nontrivial only along a low-
dimensional subspace (Definition 5.4). This type of structure appears quite naturally
in certain high-dimensional Bayesian inference problems (e.g., inverse problems [265]
and spatial statistics) where the data may be informative only about a few linear
combinations of the latent parameters [261, 74]. This is essentially the same type of
structure that we exploited in Chapter 3 in the context of Gaussian problems. Low-
rank structure can be detected via certain average derivative functionals (Theorem
5.5), but cannot be deduced, in general, from the Markov structure of (ğœˆğœ‚, ğœˆğœ‹), an
important contrast with sparse and decomposable maps. An important future direc-
tion is to extend these results in the context of approximately low-rank structure as
we did in Chapter 3 and, more generally, as in [67, 282].
Further extensions
We envision many additional ways to extend the present work.
For instance, it would be interesting to investigate the low-dimensional structure of
deterministic couplings between pair of measures (ğœˆğœ‚, ğœˆğœ‹) that are not absolutely
continuous and that need not be defined on the same space Rğ‘›. Such couplings are
usually induced by â€œrandomâ€ maps and can be particularly effective for approximating
multi-modal distributions; see the warp bridge transformations in [191, 272] for some
examples. Moreover, it would be nice to account for ultimate goals in the construction
of a nonlinear map (a natural generalization of what we did in Chapter 4).
173

174

Chapter 6
A class of nonlinear filters induced by
local couplings
6.1
Introduction
6.1.1
Problem setting
In this chapter we propose new nonlinear filtering algorithms that rely on our previous
study of low-dimensional deterministic couplings (see Chapter 5). We would like these
filtering algorithms to tackle models with:
1. High-dimensional and continuous state-space, which usually represents the dis-
cretization of a (spatially) distributed process
2. Challenging nonlinear dynamic (e.g., chaotic) governed by a possibly expensive
forward model, like a partial differential equation (PDE)
3. Intractable transition kernel, i.e., we assume that we can only simulate the prior
dynamic. We do not rely on any gradient information of the forward model.1
1 This assumption greatly extends the applicability of our techniques since both tangent and
adjoint codes are often times challenging to develop and maintain, and some forward models might
even fail to be differentiable in the first place. Of course, if the forward model were differentiable,
and if cheap gradients were available, then we would use this information and construct algorithms
that might look different than those proposed here.
175

4. Sparse measurement configuration, both in space and time, with likelihoods
that are mostly local and possibly non-Gaussian.
The algorithms that we propose can work in a much more general setting, but
they are specifically designed to exploit the structure mentioned above, which is fre-
quently encountered when filtering high-dimensional spatiotemporal processes [230]â€”
particularly in geophysical applications, such as atmosphere/ocean data assimilation
[33]. In addition, we are interested in situations where the ensemble sizeâ€”and thus
the number of times we can simulate the model dynamicâ€”must remain relatively
small, perhaps much smaller than the dimension of the model state. State-of-the-art
resultsâ€”in terms of trackingâ€”for these scenarios are currently obtained with local-
ized versions of the ensemble Kalman filter (EnKF) [93], even though recent advances
in particle filters are also promising [23, 227, 219]. We should remark that assess-
ing the accuracy of filtering algorithms in high dimensions is a challenging and open
problem. Ideally, we would compare filtering algorithms on their ability to approxi-
mate the filtering distribution. But the latter is often unavailable, because currently
there is no golden standard algorithm that can approximate the filtering distribution
arbitrarily well over long assimilation windows and with a reasonable computational
budget. The usual fallback is to measure filter accuracy in terms of tracking, e.g.,
using the mean square root error (RMSE) [182].
In the next paragraph, we describe our new filtering algorithm.
6.1.2
Approach
The idea that we pursue here is a generalization of the EnKF philosophy, and can
be synthesized as follows: we seek to transform the forecast ensemble into samples
from the filtering distribution by means of a sequence of local, low-dimensional, and
possibly nonlinear deterministic couplings (i.e., transport maps). The maps are local
in that they only act on a few components of the forecast ensemble at a time. Many
of these maps can be computed efficiently and in parallel via convex optimization be-
cause they are learned â€œfrom samplesâ€ (see Section 6.2). Only few of these maps are
176

computed â€œfrom densitiesâ€â€”following the construction outlined in Section 5.2â€”and
can be extremely low-dimensional, depending on the local nature of each likelihood
function, i.e., regardless of the dimension of the parameter space (see Section 6.3).
Particles from the forecast distribution are progressively transformed into particles
from the filtering distribution, under the action of these maps. The forecast distri-
bution is only learned locally, whenever needed. In some sense, the tasks of first
estimating the forecast distribution and then sampling from the resulting filtering
distribution (a procedure common to many, but not all, EnKF algorithms), are now
entangled in a single process.
The use of local transformations implicitly approximates the projection of the
filtering distribution onto a manifold of sparse Markov random fields, not necessarily
Gaussian. The Markov assumption is a natural one: several spatially distributed
processes can be well approximated by sparse Markov random fields (MRFs) [236].
We envision the sparsity pattern of the MRFs in the manifold as a degree of freedom of
the problemâ€”a regularization parameterâ€”, but it could also be learned from samples
[156]. Each choice of the sparsity pattern corresponds to a sequence of local maps
that are needed to approximate the corresponding projection. As an intuitive rule
of thumb, the sparser the Markov structure, the lower dimensional the maps needed.
We will discuss this point in Section 6.3.2.
The projection step that we advocate has several benefits:
1. It regularizes the estimation of the filtering distribution. This is an important
step when dealing with high-dimensional problems and limited ensemble sizes.
Regularization in high-dimensions is needed, even when estimating a simple
covariance [26, 27], and allows to trade bias for variance of the estimators.
2. It enables the transformation from forecast to filtering distribution to be broken
into simpler, smaller dimensional steps that correspond to the computation of
local, low-dimensional, transport maps.
Since each map is low-dimensional,
one can inject local nonlinearities in the parameterization of each map without
worrying too much about the variance of an estimator for the coefficients of the
177

transformation. In general, this is very different from learning a single, high-
dimensional, and unstructured transport map. Of course, how much nonlinear
structure one can actually learn depends primarily on the size of the forecast
ensemble.
With very few particles, it is very hard to learn anything other
than a linear map. But the key point is that if one is willing to increase the
ensemble size, then the proposed framework offers a principled way to increase
the complexity of the forecast-to-filtering transformation, and thus to reduce
bias in the filtering estimation.
EnKF algorithms cannot do this: they are
limited by linear transformations. Larger ensemble sizes in EnKF algorithms
are not guaranteed to yield better inference results, and in fact, in most cases,
they simply will not.
An important point: one can recover many EnKF algorithms from the proposed
framework by restricting the attention exclusively to linear transformations, and by
neglecting approximately sparse Markov structure in the filtering distribution as a
form of regularization.
The rest of this chapter is organized as follows. In Section 6.2, we review the
construction of transport maps from samples via convex optimization. This will be
an essential construction for the proposed algorithm. In Section 6.3, we illustrate the
main idea and the mechanics of the new filtering algorithm on an abstract problem.
In Section 6.4, we investigate the performances of the proposed algorithm on the
Lorenz 96 model [180].
6.2
Transport maps from samples:
conditional simulation
We recall a construction first proposed by [213]. Assume that we have ğ‘€samples
(ğ‘§ğ‘–)ğ‘€
ğ‘–=1 from a target distribution ğœ‹= ğœ‹ğ‘1,...,ğ‘ğ‘›on Rğ‘›. Given a reference density
178

ğœ‚= ğœ‚ğ‘‹1,...,ğ‘‹ğ‘›on Rğ‘›, our goal is to compute a triangular transport map ğ‘†: Rğ‘›â†’Rğ‘›,
ğ‘†(ğ‘¥) =
â¡
â¢â¢â¢â¢â¢â¢â£
ğ‘†1(ğ‘¥1)
ğ‘†2(ğ‘¥1, ğ‘¥2)
...
ğ‘†ğ‘›(ğ‘¥1, ğ‘¥2, . . . ğ‘¥ğ‘›)
â¤
â¥â¥â¥â¥â¥â¥â¦
,
(6.1)
that pushes forward the target to the reference density, i.e., ğ‘†â™¯ğœ‹= ğœ‚. As shown in
[213], one possibility is to minimize the Kâ€“L divergence between ğ‘†â™¯ğœ‹and ğœ‚over the
cone of monotone maps. The resulting optimization problem is convex and reads as
min
ğ‘†
âˆ’E
[ï¸ƒ
log ğœ‚(ğ‘†(ğ‘)) +
âˆ‘ï¸
ğ‘˜
log ğœ•ğ‘˜ğ‘†ğ‘˜(ğ‘)
]ï¸ƒ
(6.2)
s.t.
ğœ•ğ‘˜ğ‘†ğ‘˜> 0 on Rğ‘›,
ğ‘˜= 1, . . . , ğ‘›,
where the expectation is taken over the marginal distribution of ğ‘âˆ¼ğœ‹. See Section
5.2 for possible parameterizations of a monotone triangular map.
Given a map ğ‘†, we can easily sample from ğœ‹by inverting the map at samples
from ğœ‚. Inverting a triangular map is a computationally trivial task as it amounts
to a sequence of one-dimensional root findings (if the map were linear, then invert-
ing it would be equivalent to solve a triangular linear system by means of forward
substitution [110]).
If we choose a reference density that factorizes according to the product of its
marginals, i.e., if ğœ‚ğ‘‹1,...,ğ‘‹ğ‘›= âˆï¸€
ğ‘–ğœ‚ğ‘‹ğ‘–, then the optimization problem in (6.2) decouples
over the components of ğ‘†. That is, we can compute each component, ğ‘†ğ‘˜, of the map
independently as follows:
min
ğ‘†ğ‘˜
âˆ’E
[ï¸ƒ
log ğœ‚ğ‘‹ğ‘˜(ğ‘†ğ‘˜(ğ‘)) +
âˆ‘ï¸
ğ‘˜
log ğœ•ğ‘˜ğ‘†ğ‘˜(ğ‘)
]ï¸ƒ
(6.3)
s.t.
ğœ•ğ‘˜ğ‘†ğ‘˜> 0 on Rğ‘›,
179

for ğ‘˜= 1, . . . , ğ‘›. For instance, if ğœ‚= ğ’©(0, I), then (6.3) reduces to
min
ğ‘†ğ‘˜
E
[ï¸ƒ
1
2 (ğ‘†ğ‘˜(ğ‘))2 âˆ’
âˆ‘ï¸
ğ‘˜
log ğœ•ğ‘˜ğ‘†ğ‘˜(ğ‘)
]ï¸ƒ
(6.4)
s.t.
ğœ•ğ‘˜ğ‘†ğ‘˜> 0 on Rğ‘›,
and if we further use the samples (ğ‘§ğ‘–)ğ‘€
ğ‘–=1 to approximate the expectation with respect
to ğœ‹, then the stochastic program can be written in a very simple form, i.e.,
min
ğ‘†ğ‘˜
1
ğ‘€
ğ‘€
âˆ‘ï¸
ğ‘–=1
[ï¸ƒ
1
2 (ğ‘†ğ‘˜(ğ‘§ğ‘–))2 âˆ’
âˆ‘ï¸
ğ‘˜
log ğœ•ğ‘˜ğ‘†ğ‘˜(ğ‘§ğ‘–)
]ï¸ƒ
(6.5)
s.t.
ğœ•ğ‘˜ğ‘†ğ‘˜> 0 on Rğ‘›.
Now we make an important observation for the purpose of this chapter. Assume
that we want to sample from the conditional ğœ‹ğ‘ğ‘˜+1,...,ğ‘ğ‘›|ğ‘1,...,ğ‘ğ‘˜( Â· |ğœ‰1, . . . , ğœ‰ğ‘˜) for some
ğ‘˜â‰¥1 and given ğœ‰1:ğ‘˜= (ğœ‰1, . . . , ğœ‰ğ‘˜) âˆˆRğ‘˜. By definition of the Knothe-Rosenblatt
rearrangement (see Appendix B), it suffices to consider the triangular map ğ‘†ğœ‰1:ğ‘˜given
by
ğ‘¥ğ‘˜+1, . . . , ğ‘¥ğ‘›â†¦â†’
â¡
â¢â¢â¢â£
ğ‘†ğ‘˜+1(ğœ‰1:ğ‘˜, ğ‘¥ğ‘˜+1)
...
ğ‘†ğ‘›(ğœ‰1:ğ‘˜, ğ‘¥ğ‘˜+1, . . . , ğ‘¥ğ‘›)
â¤
â¥â¥â¥â¦.
(6.6)
It is easy to see that if ğœ‚factorizes as ğœ‚= ğœ‚ğ‘‹1:ğ‘˜ğœ‚ğ‘‹ğ‘˜+1:ğ‘›, then ğ‘†ğœ‰1:ğ‘˜pushes forward
the desired conditional ğœ‹ğ‘ğ‘˜+1:ğ‘›|ğ‘1:ğ‘˜( Â· |ğœ‰1:ğ‘˜) to the reference marginal ğœ‚ğ‘‹ğ‘˜+1:ğ‘›. We thus
make a few remarks:
1. We can easily sample ğœ‹ğ‘ğ‘˜+1:ğ‘›|ğ‘1:ğ‘˜( Â· |ğœ‰1:ğ‘˜) by inverting the triangular map ğ‘†ğœ‰1:ğ‘˜
at samples from ğœ‚ğ‘‹ğ‘˜+1:ğ‘›
2. If we are only interested in ğ‘†ğœ‰1:ğ‘˜, then we really only need to compute (pos-
sibly in parallel) the components ğ‘†ğ‘˜+1, . . . , ğ‘†ğ‘›of the inverse map via convex
optimization.
3. If the target density ğœ‹ğ‘1:ğ‘›has a sparse Markov structure, then the map ğ‘†
inherits the sparsity pattern described in Section 5.4. A symmetric argument
180

also holds. By enforcing sparsity in the parameterization of each component of
ğ‘†in (6.2), we can effectively â€œprojectâ€ ğœ‹ğ‘1:ğ‘›, according to the KL divergence,
onto a manifold of sparse Markov Random Fields, not necessarily Gaussian. In
this case, the result of the so-called M-projection [156] would be the pullback
density ğ‘†â™¯ğœ‚.
We are going to use extensively this construction in the proposed filtering algorithms.
6.3
Intuition for an abstract problem
We first shape our intuition on an abstract problem. We have a collection of random
variables, ğ‘= (ğ‘1, . . . , ğ‘ğ‘›) and ğ‘Œ, with joint distribution ğœ‹ğ‘1:ğ‘›,ğ‘Œ, and local likeli-
hood function, i.e., ğœ‹ğ‘Œ|ğ‘1:ğ‘›= ğœ‹ğ‘Œ|ğ‘1, where we assume a local ordering of the state
variables so that the observed node is always the first one. (Here we only consider a
single measurement. We will generalize the procedure to multiple observations later
on in the section.)
We should think of ğ‘as a random variable on Rğ‘›distributed according to the
forecast distribution at a certain time ğ‘¡, whereas we can think of ğ‘Œas a random
variable corresponding to the observed data at the same assimilation time. For in-
stance, if the state ğ‘of the filtering problem corresponds to the spatial discretization
of a distributed process, then ğ‘ğ‘–might denote the state at the ğ‘–th spatial location.
Assume that we also have ğ‘€iid samples (ğ‘§ğ‘–)ğ‘€
ğ‘–=1 from the marginal ğœ‹ğ‘1:ğ‘›, i.e., each
ğ‘§ğ‘–is a vector in Rğ‘›with components
ğ‘§ğ‘–=
â¡
â¢â¢â¢â£
ğ‘§ğ‘–
1
...
ğ‘§ğ‘–
ğ‘›
â¤
â¥â¥â¥â¦
(6.7)
and also a sample from the forecast distribution. We assume that the only density
that we can evaluate in closed form is the local likelihood ğœ‹ğ‘Œ|ğ‘1. We remark that we
only have samples from ğœ‹ğ‘1:ğ‘›, rather than the density itself.
181

Our goal is to generate approximate samples from the conditional ğœ‹ğ‘1:ğ‘›|ğ‘Œgiven
some known realization of the data {ğ‘Œ= ğ‘¦}. We are thus in the usual framework
for the EnKF or the bootstrap particle filter.
A possible approach to this problem is to notice that the filtering distribution
ğœ‹ğ‘1:ğ‘›|ğ‘Œfactorizes as
ğœ‹ğ‘1:ğ‘›|ğ‘Œâˆğœ‹ğ‘1|ğ‘Œğœ‹ğ‘2:ğ‘›|ğ‘1,
(6.8)
where the latter term is independent of the data. This factorization is general as long
as the likelihood function is local and suggests the following sampling procedure:
1. Step 1 (local assimilation): sample from ğœ‹ğ‘1|ğ‘Œ.
2. Step 2 (propagation): given samples from ğœ‹ğ‘1|ğ‘Œ, sample from ğœ‹ğ‘2:ğ‘›|ğ‘1.
We address these two steps separately, since they are philosophically different.
Step 1 (local assimilation). First, we need to sample from ğœ‹ğ‘1|ğ‘Œ. One possibility
is certainly to assign likelihood weights to the forecast ensemble and then to perform a
resampling step, in the spirit of standard SMC methods [87]. This strategy, however,
could suffer when the marginal ğœ‹ğ‘1 and the likelihood ğœ‹ğ‘Œ|ğ‘1 are mutually singular
[56]. We propose to sample from an approximation of ğœ‹ğ‘1|ğ‘Œby means of a local and
low-dimensional transport map. There are plenty of ways to compute this transport
map: for instance, one could adopt the parametric construction outlined in Section
5.2 or opt for a nonparametric approach, as in [178]. Either way, we need to estimate
the marginal density ğœ‹ğ‘1 in order to evaluate the local posterior
ğœ‹ğ‘1|ğ‘Œâˆğœ‹ğ‘Œ|ğ‘1 ğœ‹ğ‘1
(6.9)
in closed form, a prerequisite for the construction of the map. It is not hard to ap-
proximate the marginal ğœ‹ğ‘1 in low-dimensions since we already have forecast samples
(ğ‘§ğ‘–) at our disposal. One can use any preferred method: from kernel density esti-
mation (KDE) to implicit density estimation via inverse transport maps [266]. In
this chapter we opt for the latter strategy and seek an inverse map ğ‘†1 : R â†’R
that pushes forward ğœ‹ğ‘1 to a one dimensional reference density ğœ‚ğ‘‹1, e.g., a standard
182

normal or any other simple distribution with a tail behavior similar to that of ğœ‹ğ‘1. In
particular, we adopt the construction outlined in Section 6.2 for a map from samples
and find a parametric approximation of a monotone ğ‘†1 via convex optimization. The
resulting approximation of ğœ‹ğ‘1 is given by the pullback density
ğœ‚ğ‘‹1(ğ‘†1(ğ‘¥)) dğ‘†1(ğ‘¥)
dğ‘¥
,
(6.10)
which is easy to evaluate and that yields a corresponding approximation of ğœ‹ğ‘1|ğ‘Œ. A
parametric approximation of ğœ‹ğ‘1 has the virtue of filling in the space (with probability
mass), and thus makes the ensuing conditioning on the data â€œrobustâ€ whenever ğœ‹ğ‘1
and ğœ‹ğ‘Œ|ğ‘1 are mutually singular.
Now we need to compute a transport map ğ‘‡1 : R â†’R that samples the approxi-
mation of ğœ‹ğ‘1|ğ‘Œdescribed above. For instance, we might characterize the increasing
rearrangement on R via optimization or direct numerical integration [271]. Also the
computation of ğ‘‡1 requires to specify a reference density. For simplicity, we consider
the same reference density ğœ‚ğ‘‹1 used to compute ğ‘†1, but other choices are certainly
possible.
Given the pair of maps ğ‘†1 and ğ‘‡1, we can easily generate approximate samples
(ğ‘ğ‘–
1)ğ‘€
ğ‘–=1 from ğœ‹ğ‘1|ğ‘Œby setting
ğ‘ğ‘–
1 = ğ‘‡1(ğ‘†1(ğ‘§ğ‘–
1)),
(6.11)
for ğ‘–= 1, . . . , ğ‘€. An important point: we do not sample ğœ‹ğ‘1|ğ‘Œby pushing forward
samples from ğœ‚ğ‘‹1 through ğ‘‡1, but rather update each forecast particle directly using
the composition ğ‘‡1 âˆ˜ğ‘†1. If the maps (ğ‘†1, ğ‘‡1) were exact, then these two sampling
procedures would be equivalent. But if we only have a numerical approximation of
(ğ‘†1, ğ‘‡1), then the sampling strategy that we advocate performs empirically better.
The underlying intuition is exactly the core philosophy in EnKF, which despite the
Gaussian approximation of the forecast distribution, generates approximate samples
from the filtering distributionâ€”with possibly non-Gaussian statisticsâ€”by pushing
forward forecast particles through a linear map.
183

Step 2 (propagation). We need to sample from ğœ‹ğ‘2:ğ‘›|ğ‘1 given the events {ğ‘1 = ğ‘ğ‘–
1}
for ğ‘–= 1, . . . , ğ‘€. We call this step propagation, as we effectively need to propagate
information from the local filtering marginal ğœ‹ğ‘1|ğ‘Œto every remaining state variable.
A first simple solution to this problem is to use the construction of Section 6.2 for
conditional simulation. That is, compute a triangular map2 ğ‘†: Rğ‘›â†’Rğ‘›,
ğ‘†(ğ‘¥) =
â¡
â¢â¢â¢â¢â¢â¢â£
ğ‘†1(ğ‘¥1)
ğ‘†2(ğ‘¥1, ğ‘¥2)
...
ğ‘†ğ‘›(ğ‘¥1, ğ‘¥2, . . . ğ‘¥ğ‘›)
â¤
â¥â¥â¥â¥â¥â¥â¦
,
(6.12)
that pushes forward ğœ‹ğ‘1:ğ‘›â€”available through samplesâ€”to a reference density that
factorizes as ğœ‚(ğ‘¥) = âˆï¸€
ğ‘–ğœ‚ğ‘‹ğ‘–(ğ‘¥ğ‘–), e.g., a standard normal. (The first component of
(6.12) coincides with the map ğ‘†1 computed in the local assimilation step, assuming
that the corresponding reference densities match along the first marginal, as we do
here.) Then, for all ğœ‰âˆˆR, consider the restriction ğ‘†ğœ‰: Rğ‘›âˆ’1 â†’Rğ‘›âˆ’1 given by
ğ‘¥ğ‘˜+1, . . . , ğ‘¥ğ‘›â†¦â†’
â¡
â¢â¢â¢â£
ğ‘†2(ğœ‰, ğ‘¥2)
...
ğ‘†ğ‘›(ğœ‰, ğ‘¥2, . . . , ğ‘¥ğ‘›)
â¤
â¥â¥â¥â¦,
(6.13)
and compute the â€œmarginalâ€ filtering samples (ğ‘ğ‘–
2:ğ‘›)ğ‘€
ğ‘–=1â€”i.e., approximate samples
from ğœ‹ğ‘2:ğ‘›|ğ‘Œâ€”as
ğ‘ğ‘–
2:ğ‘›= ğ‘†âˆ’1
ğ‘ğ‘–
1 ( ğ‘†ğ‘§ğ‘–
1( ğ‘§ğ‘–
2:ğ‘›) ),
(6.14)
for ğ‘–= 1, . . . , ğ‘€, where
ğ‘ğ‘–
2:ğ‘›=
â¡
â¢â¢â¢â£
ğ‘ğ‘–
2
...
ğ‘ğ‘–
ğ‘›
â¤
â¥â¥â¥â¦,
ğ‘§ğ‘–
2:ğ‘›=
â¡
â¢â¢â¢â£
ğ‘§ğ‘–
2
...
ğ‘§ğ‘–
ğ‘›
â¤
â¥â¥â¥â¦,
(6.15)
2 Notice that we use a lower triangular function because we assigned a local ordering so that the
observed node is also the first one in the ordering. This construction is always possible.
184

and where (ğ‘ğ‘–
1)ğ‘€
ğ‘–=1 were obtained in the local assimilation step. The inversion of ğ‘†ğ‘ğ‘–
1
in (6.14) is trivial since ğ‘†ğ‘ğ‘–
1 is a triangular map: we just need to perform a sequence
of ordered one-dimensional root-findings (see [212] for additional details). Moreover,
each component of ğ‘†can be computed independently and in parallel using only fore-
cast samples.
Finally, by concatenating ğ‘ğ‘–
1 and ğ‘ğ‘–
2:ğ‘›, we can obtain approximate
filtering samples as
ğ‘ğ‘–=
â¡
â¢â¢â¢â£
ğ‘ğ‘–
1
...
ğ‘ğ‘–
ğ‘›
â¤
â¥â¥â¥â¦,
(6.16)
for ğ‘–= 1, . . . , ğ‘€. The resulting filtering samples can be written as a (possibly non-
linear) function of the forecast ensemble. This transformation is given by the compo-
sition
ğ’¯(ğ‘¥) =
â¡
â£ğ‘‡1(ğ‘¥1)
ğ‘†âˆ’1
ğ‘‡1(ğ‘¥1)(ğ‘¥2, . . . , ğ‘¥ğ‘›)
â¤
â¦âˆ˜ğ‘†(ğ‘¥),
(6.17)
so that ğ‘ğ‘–= ğ’¯(ğ‘§ğ‘–) for all ğ‘–.
In practice, one can then iterate this very same construction by assimilating a new
local observation at the same time ğ‘¡, and by using (ğ‘ğ‘–)ğ‘€
ğ‘–=1 as the new â€œpriorâ€ samples.
In fact, one has, for instance,
ğœ‹ğ‘1:ğ‘›|ğ‘Œ1,ğ‘Œğ‘˜âˆğœ‹ğ‘Œğ‘˜|ğ‘ğ‘˜ğœ‹ğ‘1:ğ‘›|ğ‘Œ1,
(6.18)
where we assumed that the likelihood for ğ‘Œğ‘˜is again local and only a function of the
corresponding variable ğ‘ğ‘˜. When iterating the construction for ğ‘Œğ‘˜, it is important to
choose a new local ordering of the state variables so that ğ‘ğ‘˜corresponds to the first
element in the ordering. Once every measurement at time ğ‘¡is assimilated, we can
propagate the filtering samples through the system dynamic to the next assimilation
time, e.g., ğ‘¡+ Î”ğ‘¡obs.
In this section we presented an assimilation strategy that processes data sequen-
tially one at a time, but the proposed framework can be easily generalized to batches
of data. For instance, we can assimilate a pair of local measurements (ğ‘Œ1, ğ‘Œ2) follow-
185

ing the decomposition
ğœ‹ğ‘1:ğ‘›|ğ‘Œ1,ğ‘Œ2 âˆğœ‹ğ‘1,ğ‘2|ğ‘Œ1,ğ‘Œ2 ğœ‹ğ‘3:ğ‘›|ğ‘1,ğ‘2
(6.19)
of the filtering distribution and repeating the construction above with minor modifi-
cations. The price to pay for the batch strategy is that now we need to solve a local
assimilation problem in two, rather than one, dimensions: not a big deal as long as
the dimension of the batches is limited.
In the next section we address the parameterization of the (possibly high-dimensional)
map ğ‘†.
6.3.1
Parameterizing a high dimensional inverse map
ğ‘†is a multivariate function on Rğ‘›and so we must discuss the parameterization of its
components. A simple choice that would correspond to an EnKF algorithm is to let
each component ğ‘†ğ‘˜to be a linear function of its inputs, i.e.,
ğ‘†ğ‘˜(ğ‘¥1, . . . , ğ‘¥ğ‘˜) = ğ‘0 +
ğ‘˜
âˆ‘ï¸
ğ‘–=1
ğ‘ğ‘–ğ‘¥ğ‘–,
(6.20)
where (ğ‘0, . . . , ğ‘ğ‘˜) are ğ‘˜+ 1 parameters to be found via optimization. But the frame-
work that we investigate here is far more general, and it allows to consider any
monotone parameterization of the map. For instance, let ğ’œğ‘˜be a set of indeces (in-
cluding ğ‘˜) that correspond to nodes that are spatial neighbors of ğ‘ğ‘˜. Then we might
consider a parameterization of ğ‘†ğ‘˜,
ğ‘†ğ‘˜(ğ‘¥1, . . . , ğ‘¥ğ‘˜) = ğ‘0 +
âˆ‘ï¸
ğ‘–âˆˆğ’œğ‘
ğ‘˜
ğ‘ğ‘–ğ‘¥ğ‘–+ Î¨(ğ‘¥ğ’œğ‘˜),
(6.21)
where the nonlinearity is only restricted to variables in ğ’œğ‘˜. For instance, Î¨ could be
a nonlinear function given by a multivariate polynomial chaos expansion of a certain
order [277]. In Section 6.4 we will give yet another example of scalable parameteri-
zation for ğ‘†. This flexibility in the parameterization of the map allows to introduce
186

local nonlinearities in a way that scales well with dimensions: transport maps offer
an opportunity to depart from linear EnKF updates in a scalable and principled way.
There is plenty of structure in the filtering problem that can be translated into low-
dimensional parameterizations for ğ‘†. As an example consider the decay of correlation
in the forecast distribution, and let ğ‘Ÿ> 0 be a localization radius, i.e., we assume that
if ğ‘‘(ğ‘ğ‘–, ğ‘ğ‘—) < ğ‘Ÿ, then ğ‘ğ‘–and ğ‘ğ‘—are marginally independent, where ğ‘‘(Â·, Â·) denotes a
distance function on the physical space. This localization radius is frequently used
in EnKF algorithms to localize covariance computations, and is an empirically tuned
parameter [150, 93].
Marginal independence is also reflected into sparsity of the
inverse map (see Section 5.4).
That is, from each component ğ‘†ğ‘˜(ğ‘¥1, . . . , ğ‘¥ğ‘˜) we
can drop the explicit dependence on variables ğ‘¥ğ‘—with ğ‘‘(ğ‘ğ‘—, ğ‘ğ‘˜) < ğ‘Ÿ, and turn the
corresponding component into a function that is local in state space. Even more, if
ğ‘1 is the observed node, then we can set every component ğ‘†ğ‘—with ğ‘‘(ğ‘ğ‘—, ğ‘1) < ğ‘Ÿto
be the identity function,
ğ‘†ğ‘—(ğ‘¥1, . . . , ğ‘¥ğ‘—) = ğ‘¥ğ‘—,
(6.22)
since the local observation will not update â€œsignificantlyâ€ the forecast distribution
beyond the localization radius.
Thus, accounting for decay of correlations in the
forecast distribution can reduce dramatically the complexity of parameterizing ğ‘†.
An additional source of low-dimensional structure is related to conditional inde-
pendence. We address this point in the next section.
6.3.2
Introducing Markov assumptions
If the state of the filtering problem represents the discretization of a distributed pro-
cess (like the solution of a PDE), then in many cases it is true that the forecast
distribution can be well approximated by a sparse Markov Random Field (MRF).
For instance, this observation is at the core of modeling with sparse Gaussian MRFs
[236]. In this context, â€œapproximatedâ€ is a key word since the filtering distribution
will not satisfy, in general, any Markov property, strictly speaking [156]. But consid-
ering approximate Markov structure has several important benefits that we (briefly)
187

describe in this section. A pioneering work at the intersection of filtering and sparse
MRFs is certainly [155], on which we build upon.
Sometimes it is possible to learn the approximate Markov structure of the forecast
distribution from samples [189, 156, 124], but in most cases, especially when dealing
with a limited ensemble size, one can simply impose the sparse Markov structure
when computing the map ğ‘†, as a solution of (6.2), in the propagation step. This
constraint can be easily enforced in terms of sparsity of ğ‘†, as explained in Section 5.4.
In particular, the sparsity of ğ‘†is given by Theorem 5.1 by just looking at the graph
that represents the Markov structure. The resulting map implicitly approximates
the projection of the forecast distribution onto a manifold of sparse MRFs. If the
likelihoods are local (pointwise measurements), then the approximate forecast and
filtering distributions have the same Markov structure.
When dealing with spatially distributed processes, there are intuitive choices of
Markov structure. For instance, in 1-D we might want to consider chains or cycles,
in 2-D grids, and so on. In particular, we can regard the neighborhood size of the
graph as a (discrete) tuning parameter of the algorithm, in the same spirit as the
localization radius.
The sparsity of ğ‘†induced by approximate Markov properties should be added to
the sparsity of ğ‘†induced by the decay of correlations (and discussed in the previous
section). It is thus an additional form of regularization that increases the efficiency
of computations. As discussed in Section 5.4, the sparsity of ğ‘†induced by Markov
assumptions can be huge. For instance, if the underlying graphical model were a
cycle, then each component of ğ‘†would depend at most on three variables, regardless
of the dimension of the state.
So far, we showed how the components of ğ‘†can be sparse (due to correlation
and conditional independence), and how they can be computed independently and
in parallel via convex optimization.
In order to transform the forecast ensemble,
however, we need to invert ğ‘†, as explained in Section 6.3 (see equation 6.17). This
operationâ€”which amounts to a sequence of 1-D root findingsâ€”is cheap, but requires
to evaluate the components of ğ‘†in an orderly fashion (like in forward substitution).
188

A perhaps unintuitive consequence of the Markov assumption, is that the inversion of
ğ‘†can be parallelized, if we choose appropriate orderings for the triangular map. We
make this fact explicit for the simple Markov structure in Figure 6-1. The reader can
then simply extrapolate this strategy to more complex graphs. Given the â€œspecialâ€
ordering of Figure 6-1, the map ğ‘†has the following sparsity pattern (Theorem 5.1):
ğ‘†(ğ‘¥) =
â¡
â¢â¢â¢â¢â¢â¢â¢â¢â¢â¢â¢â¢â¢â¢â¢â£
ğ‘†1(ğ‘¥1)
ğ‘†2(ğ‘¥1, ğ‘¥2)
ğ‘†3(ğ‘¥1, ğ‘¥2, ğ‘¥3)
ğ‘†4(ğ‘¥1, ğ‘¥2,ğ‘¥3, ğ‘¥4)
ğ‘†5(ğ‘¥1, ğ‘¥2, ğ‘¥3, ğ‘¥4,ğ‘¥5)
ğ‘†6(ğ‘¥1,ğ‘¥2, ğ‘¥3, ğ‘¥4, ğ‘¥5,ğ‘¥6)
ğ‘†7(ğ‘¥1, ğ‘¥2, ğ‘¥3, ğ‘¥4, ğ‘¥5,ğ‘¥6, ğ‘¥7)
â¤
â¥â¥â¥â¥â¥â¥â¥â¥â¥â¥â¥â¥â¥â¥â¥â¦
,
(6.23)
which suggests a simple strategy to invert ğ‘†at a point ğ‘§.
Let ğœ‰= ğ‘†âˆ’1(ğ‘§) and
consider the following steps:
1. Determine ğœ‰1:3 = (ğœ‰1, ğœ‰2, ğœ‰3) by inverting the submap
(ğ‘¥1, ğ‘¥2, ğ‘¥3) â†¦â†’
â¡
â¢â¢â¢â£
ğ‘†1(ğ‘¥1)
ğ‘†2(ğ‘¥1, ğ‘¥2)
ğ‘†3(ğ‘¥1, ğ‘¥2, ğ‘¥3)
â¤
â¥â¥â¥â¦
(6.24)
at ğ‘§1:3 = (ğ‘§1, ğ‘§2, ğ‘§3).
2. Determine ğœ‰4:5 = (ğœ‰4, ğœ‰5) by inverting the submap
(ğ‘¥4, ğ‘¥5) â†¦â†’
â¡
â£ğ‘†4(ğœ‰1, ğ‘¥2,ğœ‰3, ğ‘¥4)
ğ‘†5(ğœ‰1, ğœ‰2, ğ‘¥3, ğ‘¥4,ğ‘¥5)
â¤
â¦
(6.25)
at ğ‘§4:5 = (ğ‘§4, ğ‘§5).
189

3. Determine ğœ‰6:7 = (ğœ‰6, ğœ‰7) by inverting the submap
(ğ‘¥6, ğ‘¥7) â†¦â†’
â¡
â£ğ‘†6(ğ‘¥1,ğœ‰2, ğœ‰3, ğ‘¥4, ğ‘¥5,ğ‘¥6)
ğ‘†7(ğ‘¥1, ğ‘¥2, ğ‘¥3, ğ‘¥4, ğ‘¥5,ğ‘¥6, ğ‘¥7)
â¤
â¦
(6.26)
at ğ‘§6:7 = (ğ‘§6, ğ‘§7).
The key observation is that steps 2 and 3 can now be executed in parallel. What
is special about the ordering in Figure 6-1 is that the first variables in the ordering
correspond to (1) the observed node, and (2) a separator set for the graph. Clearly
this strategy can be applied to more general graphs, even recursively. For instance,
when dealing with a grid graph (in 2-D), one can choose a ring containing the observed
node as a separator set.
ğ‘7
ğ‘3
ğ‘4
ğ‘1
ğ‘5
ğ‘2
ğ‘6
ğ‘Œ
Figure 6-1: Cycle graph used as a Markov structure for the filtering distribution in
the simple example of Section 6.3.2.
6.4
Numerical example: Lorenz 96
We demonstrate a particular version of the algorithm presented in Section 6.3 on a
challenging test case configuration for the Lorenz 96 model [180], which is intended to
reproduce coarse features of the mid-latitude atmosphere [182] and is frequently used
190

as a testbed for numerical weather prediction algorithms. The goal of this example is
to show how the use of local nonlinear couplings can outperform state-of-the-art EnKF
methodologies, even in terms of tracking (RMSE), which is a notoriously challenging
task [18]. We leave further numerical experiments to a forthcoming publication.
The test case configuration is taken from [18], which also develops its own nonlin-
ear filter based on Gaussian mixtures. The model dynamic is prescribed as a set of
deterministic nonlinear ODEs,
dğ‘ğ‘—
dğ‘¡= (ğ‘ğ‘—+1 âˆ’ğ‘ğ‘—âˆ’2) ğ‘ğ‘—âˆ’1 âˆ’ğ‘ğ‘—+ ğ¹,
(6.27)
for ğ‘—= 1, . . . , 40, periodic boundary conditions, and ğ¹= 8 (chaotic regime). The
equations in (6.27) represent the spatial discretization of a one dimensional time
dependent PDE [230]. The state ğ‘= (ğ‘1, . . . , ğ‘40) is 40-dimensional. We discretize
the dynamic in (6.27) using a Runge-Kutta 4 method with constant stepsize Î”ğ‘¡=
0.01. We observe the state locally in space and time according to the model
ğ‘Œğ‘—= ğ‘ğ‘—+ ğœ€ğ‘—,
(6.28)
for ğ‘—= 1, 3, 5, . . . , 39â€”i.e., we observe every other componentâ€”and ğœ€ğ‘—âˆ¼ğ’©(0, 0.5).
The (ğœ€ğ‘—)ğ‘—are mutually independent of each other and of the state. We make obser-
vations every Î”ğ‘¡obs = 0.4 time units, which corresponds to 40 discrete time steps,
and which is considered to be quite large for the purpose of estimation (e.g., highly
non-Gaussian statistics of the forecast distribution [18]).
We consider an estimation window of 2000 assimilation cycles. The reference solu-
tion and the observations are generated with the same model used for the assimilation,
following the usual â€œidentical twin experimentâ€ setting [230].
In the next section we describe the particular configuration of the filtering algo-
rithm used for the numerical experiments.
191

6.4.1
Configuration of the nonlinear filter
We use the same nonlinear filter induced by local couplings described in Section 6.3,
with the following choice of parameters, which is by no means optimal:
â€¢ At each assimilation time, we process measurements sequentially one at a time.
â€¢ We project the filtering distribution onto a manifold of non-Gaussian MRFs
with the Markov structure given by Figure 6-2, where each node is connected
with the 5 closest neighbors on the graph. See the discussion of Section 6.3.2
for more details. For comparison, Figure 6-3 shows the typically dense Markov
structure of the filtering distribution after a few assimilation cycles [156]. The
fact that the actual Markov structure is dense, does not mean that the filtering
distribution cannot be well-approximated by a sparse MRF [155]. The numerical
results in the next section support this claim.
â€¢ Each component ğ‘†ğ‘˜of the inverse map ğ‘†used in the propagation step follows
the sparsity pattern predicted by Theorem 5.1 for the Markov structure of
Figure 6-2. Moreover, each ğ‘†ğ‘˜is parameterized as follows:
ğ‘†ğ‘˜(ğ‘¥ğ‘—1, . . . , ğ‘¥ğ‘—â„“, ğ‘¥ğ‘˜) = ğœ“(ğ‘¥ğ‘—1) + Â· Â· Â· + ğœ“(ğ‘¥ğ‘—â„“) + ğœ“(ğ‘¥ğ‘˜),
(6.29)
with
ğœ“(ğ‘¥) = ğ‘0 + ğ‘1 ğ‘¥+
7
âˆ‘ï¸
ğ‘–=2
ğ‘ğ‘–exp(âˆ’1
2ğœ2(ğ‘¥âˆ’ğœ‰ğ‘–)2),
(6.30)
where (ğ‘0, . . . , ğ‘7) are optimization parametersâ€”these parameters are different
for each instance of ğœ“in (6.29)â€”, (ğœ‰2, . . . , ğœ‰7) are the abscissas of a sixth or-
der Hermite quadrature rule on R, and where ğœ= 1.35. The choice of these
parameters is somewhat arbitrary and could be improved.
â€¢ Each local optimization problem for ğ‘†ğ‘˜is solved via Newtonâ€™s method.
â€¢ The direct map ğ‘‡1 in the local assimilation step (Section 6.3) is the increasing
rearranegement on R, computed via accurate numerical integration.
192

â€¢ We use standard normal reference densities throughout the algorithm.
â€¢ We also employ standard multiplicative inflation [182] with parameter ğ‘Ÿinfl =
0.1.
We use the acronym LocNLF to denote the resulting nonlinear filtering algorithm.
If, in the LocNLF algorithm, we restrict every transport map to be linear, we obtain
an EnKF-type algorithm that we call LocLF. The only difference between these two
algorithms is the degree of nonlinearity of the local couplings.
6.4.2
Numerical results
Numerical results in terms of time-averaged RMSE (over 2000 assimilation cycles)
are shown in Table 6.1 for different filtering algorithms (EnKF, LocLF, LocNLF) and
different ensemble sizes (400 or 200). The values of RMSE for the EnKF algorithm
with 400 particles are those published in [18] for the same problem configuration as
the one considered here, but necessarily for a different data set. [18] uses a state-
of-the-art EnKF algorithmâ€”with an optimized tapering function [103]â€”in order to
establish a baseline measure of performance.
First of all, we notice that the values of RMSE for the EnKF in Tabe 6.1 are
almost identical to those of the LocLF algorithm (400 particles). Also the LocLF
can be thought of as a particular EnKF algorithm: one that uses a linear forecast-to-
filtering update, and that regularizes the covariance estimation by imposing sparsity
on its precision matrix [27], rather than tapering the covariance. Both the EnKF of
[18] and LocLF achieve similar performance on this challenging test case.
Second, and most importantly, we observe how the proposed nonlinear filtering
algorithm (LocNLF) is about 25% more accurate in every RMSE statistic (i.e.,
median, mean, variance) than any of its linear competitors, using either 400 or 200
particles. Figure 6-4 shows the excellent tracking performance of LocNLF over the
last 100 assimilation cycles.
The only difference between LocLF and LocNLF is in the nonlinearity of their
couplings. Thus, the 25% increase in accuracy of LocNLF over LocLF is entirely
193

due to the use of nonlinear transformations. This last observation reinforces the main
take-home message for this chapter: combining different sources of low-dimensional
structure (e.g., locality of observations, decay of correlations, approximately sparse
Markov structure) with local, low-dimensional, nonlinear couplings, produces nonlin-
ear filtering algorithms that can outperform state-of-the-art EnKF techniques, for the
same ensemble size, and at a marginal increase in computational cost.
Figure 6-2: Sparse Markov structure used by the nonlinear filter to project the filtering
distribution. Each node is connected with the 5 closest neighbors.
#particles: 400
#particles: 200
EnKF[18]
LocLF
LocNLF
LocLF
LocNLF
med RMSE
0.88
0.87
0.64
0.91
0.66
avg RMSE
0.97
0.99
0.74
1.02
0.79
var RMSE
0.12
0.1
0.06
0.1
0.09
Table 6.1: We report the median (med), the average (avg), and the variance (var) of
the time-averaged RMSE (over 2000 assimilation cycles) for different filtering algo-
rithms and number of particles. The proposed nonlinear filter (LocNLF) is â‰ˆ25%
more accurate in RMSE than a state-of-the-art EnKF algorithm.
194

Figure 6-3:
Actual dense Markov structure of a typical filtering distribution.
195

Figure 6-4: Comparison between the true signal (left) and the mean of the proposed
nonlinear filter (right), over the last 100 assimilation cycles. The overall agreement
between the two fields is excellent.
196

Appendix A
Raoâ€™s metric between distributions
Let M = {ğœ‹ğœƒ, ğœƒâˆˆÎ˜} be a parametric family of probability densities indexed by
ğœƒ= (ğœƒ1, . . . , ğœƒğ‘›) âˆˆÎ˜ [9]. Rao considered a quadratic differential form given by
dğ‘ 2 =
âˆ‘ï¸
ğ‘–,ğ‘—
ğ‘”ğ‘–ğ‘—(ğœƒ) dğœƒğ‘–dğœƒğ‘—,
(A.1)
where ğ‘”ğ‘–ğ‘—(ğœƒ) = Eğœ‹ğœƒ[ ğœ•ğœƒğ‘–ln ğœ‹ğœƒğœ•ğœƒğ‘—ln ğœ‹ğœƒ] are the entries of the Fisher information ma-
trix, with Eğœ‹ğœƒdenoting integration with respect to ğœ‹ğœƒ[95]. The Fisher information
matrix is a central object in mathematical statistics (e.g., the CramÃ©r-Rao inequality
[223]). Intuitively, we can interpret (A.1) as the variance of the function that de-
scribes the first order relative difference between ğœ‹ğœƒand a contiguous density, ğœ‹ğœƒ+dğœƒ,
on M [224]. The definition of a quadratic form like (A.1) allows us to measure curves
on M .
Given a smooth curve ğ›¾: [0, 1] â†’Î˜ â‰ƒM , we can define its length as
â„“(ğ›¾) :=
âˆ«ï¸€1
0 (âˆ‘ï¸€
ğ‘–,ğ‘—ğ‘”ğ‘–ğ‘—(ğ›¾(ğ‘¡)) dğ›¾ğ‘–dğ›¾ğ‘—)1/2 dğ‘¡[86]. Thus, Raoâ€™s distance between a pair of
distributions on M is simply their geodesic distance, i.e., the length of the minimum
length curve joining these distributions [224]. The quadratic form defined by the
Fisher information matrix is invariant under regular reparameterizations of M [223].
Thus, this fundamental invariance is also shared by Raoâ€™s distance which yields an
intrinsic way of comparing distributions on M . Of course, it is possible to consider
more general quadratic differential forms not based on the notion of Fisher infor-
mation. See [225] for various examples of differential metrics derived from entropy
197

functions or divergence measures between probability distributions. See [4] for a mod-
ern treatment of information geometry, the field at the intersection of statistics and
differential geometry.
198

Appendix B
Generalized Knothe-Rosenblatt
rearrangement
In this section we first review the classical notion of KR rearrangement [234, 154],
and then give a formal definition for a generalized KR rearrangement, i.e., a transport
map that is lower triangular up to a permutation. A disclaimer: these transports can
also be defined under weaker conditions than those considered here, at the expense,
however, of some useful regularity (e.g., see [34]).
The following definition introduces the one-dimensional version of the KR-rearrangement,
and it is key to extend the transport to higher dimensions.
Definition B.1 (Increasing rearrangement on R). Let ğœˆğœ‚, ğœˆğœ‹âˆˆM+(R), and let ğ¹, ğº
be their respective cumulative distribution functions, i.e., ğ¹(ğ‘¡) = ğœˆğœ‚((âˆ’âˆ, ğ‘¡)) and
ğº(ğ‘¡) = ğœˆğœ‹((âˆ’âˆ, ğ‘¡)). Then the increasing rearrangement on R is given by ğ‘‡= ğºâˆ’1âˆ˜ğ¹.
Under the hypothesis of Definition B.1, it is easy to see that both ğ¹and ğºare
homeomorphisms, and that ğ‘‡is a strictly increasing map that pushes forward ğœˆğœ‚to
ğœˆğœ‹[244].
Definition B.2 (Knothe-Rosenblatt rearrangement). Given ğ‘‹âˆ¼ğœˆğœ‚, ğ‘âˆ¼ğœˆğœ‹, with
ğœˆğœ‚, ğœˆğœ‹âˆˆM+(Rğ‘›), and a pair ğœ‚, ğœ‹of strictly positive densities for ğœˆğœ‚and ğœˆğœ‹, re-
spectively, the corresponding KR rearrangement is a triangular map ğ‘‡: Rğ‘›â†’Rğ‘›
199

defined, recursively, as follows. For all ğ‘¥1:ğ‘˜âˆ’1 âˆˆRğ‘˜âˆ’1, the map ğœ‰â†¦â†’ğ‘‡ğ‘˜(ğ‘¥1:ğ‘˜âˆ’1, ğœ‰)â€”
the restriction of the ğ‘˜th component of ğ‘‡onto its first ğ‘˜âˆ’1 inputsâ€”is defined as
the increasing rearrangement on R that pushes forward ğœ‰â†¦â†’ğœ‚ğ‘‹ğ‘˜|ğ‘‹1:ğ‘˜âˆ’1(ğœ‰|ğ‘¥1:ğ‘˜âˆ’1) to
ğœ‰â†¦â†’ğœ‹ğ‘ğ‘˜|ğ‘1:ğ‘˜âˆ’1(ğœ‰|ğ‘‡1(ğ‘¥1), . . . , ğ‘‡ğ‘˜âˆ’1(ğ‘¥1:ğ‘˜âˆ’1)), where ğœ‚ğ‘‹ğ‘˜|ğ‘‹1:ğ‘˜âˆ’1 and ğœ‹ğ‘ğ‘˜|ğ‘1:ğ‘˜âˆ’1 are condi-
tional densities defined as in (2.2).
Notice that for any measure ğœˆin M+(Rğ‘›) there always exists a strictly positive
version of its density. By considering such positive densities in Definition B.2, we can
define the KR rearrangement on the entire Rğ‘›[34]. In fact, we should really think of
Definition B.2 as providing a possible version of the KR rearrangement (recall that
the increasing triangular transport is unique up to sets of measure zero [244]). Since
in this case ğœˆğœ‹is equivalent to the Lebesgue measure (ğœˆğœ‹(ğ’œ) =
âˆ«ï¸€
ğ’œğœ‹(ğ‘¥) ğœ†(dğ‘¥) = 0 â‡’
ğœ†(ğ’œ) = 0 if ğœ‹> 0 a.e.), the component (5.2) is also absolutely continuous on all
compact intervals [34, Lemma 2.4]. As a result, the rearrangement can be used to
define general change of variables as well as pullbacks and pushforwards with respect
to arbitrary densities, as shown by the following lemma adapted from [34].
Lemma B.1. Let ğ‘‡be an increasing triangular bijection on Rğ‘›such that the functions
ğœ‰â†¦â†’ğ‘‡ğ‘˜(ğ‘¥1, . . . , ğ‘¥ğ‘˜âˆ’1, ğœ‰)
(B.1)
are absolutely continuous on all compact intervals for a.e. (ğ‘¥1, . . . , ğ‘¥ğ‘˜âˆ’1) âˆˆRğ‘˜âˆ’1.
Then for any integrable function ğœ™, it holds:
âˆ«ï¸
ğœ™(ğ‘¦) dğ‘¦=
âˆ«ï¸
ğœ™(ğ‘‡(ğ‘¥)) det âˆ‡ğ‘‡(ğ‘¥) dğ‘¥,
(B.2)
where det âˆ‡ğ‘‡:= âˆï¸€ğ‘›
ğ‘˜=1 ğœ•ğ‘˜ğ‘‡ğ‘˜. In particular, if ğœˆğœŒis a measure on Rğ‘›with density ğœŒ,
then we also have ğ‘‡â™¯ğœˆğœŒâ‰ªğœ†with density (a.e.):
ğ‘‡â™¯ğœŒ(ğ‘¥) = ğœŒ(ğ‘‡(ğ‘¥)) det âˆ‡ğ‘‡(ğ‘¥).
(B.3)
The lemma can also be applied to the inverse KR rearrangement ğ‘‡âˆ’1 to show that
ğ‘‡â™¯ğœˆğœŒâ‰ªğœ†, where the form of the corresponding pushforward density ğ‘‡â™¯ğœŒis given by
200

replacing ğ‘‡with ğ‘‡âˆ’1 in (B.3). We will use these results extensively in the proofs of
Appendix ??. Notice, however, that Lemma B.1 does not hold for a generic triangular
function: the map must be somewhat regular, in the sense specified by the lemma.
See [34] for an in depth discussion.
We now give a constructive definition for a generalized KR rearrangement.
Definition B.3 (Generalized Knothe-Rosenblatt rearrangement). Given ğ‘‹âˆ¼ğœˆğœ‚,
ğ‘âˆ¼ğœˆğœ‹, with ğœˆğœ‚, ğœˆğœ‹âˆˆM+(Rğ‘›), a pair ğœ‚, ğœ‹of strictly positive densities for ğœˆğœ‚and
ğœˆğœ‹, respectively, and a permutation ğœof Nğ‘›, the corresponding ğœ-generalized KR
rearrangement is a ğœ-triangular map1 ğ‘‡: Rğ‘›â†’Rğ‘›defined at any ğ‘¥âˆˆRğ‘›using the
following recursion in ğ‘˜. The map ğœ‰â†¦â†’ğ‘‡ğœ(ğ‘˜)(ğ‘¥ğœ(1), . . . , ğ‘¥ğœ(ğ‘˜âˆ’1), ğœ‰) is defined as the
increasing rearrangement on R that pushes forward ğœ‰â†¦â†’ğœ‚ğ‘‹ğœ(ğ‘˜)|ğ‘‹ğœ(1:ğ‘˜âˆ’1)(ğœ‰|ğ‘¥ğœ(1:ğ‘˜âˆ’1)) to
ğœ‰â†¦â†’ğœ‹ğ‘ğœ(ğ‘˜)|ğ‘ğœ(1:ğ‘˜âˆ’1)(ğœ‰|ğ‘‡ğœ(1)(ğ‘¥ğœ(1)), . . . , ğ‘‡ğœ(ğ‘˜âˆ’1)(ğ‘¥ğœ(1:ğ‘˜âˆ’1))),
(B.4)
where ğ‘¥ğœ(1:ğ‘˜âˆ’1) = ğ‘¥ğœ(1), . . . , ğ‘¥ğœ(ğ‘˜âˆ’1).
Existence of a generalized KR rearrangement follows trivially from its definition.
Moreover, the transport map satisfies all the regularity properties discussed for the
classic KR rearrangement, including Lemmas 5.1 and B.1. Thus we will often cite
these two results when dealing with generalized KR rearrangements in our proofs.
The following lemma shows that the computation of a generalized KR rearrangement
is also essentially no different than the computation of a lower triangular transport
(and thus all the discussion of Section 5.2 readily applies).
Lemma B.2. Given ğœˆğœ‚, ğœˆğœ‹âˆˆM+(Rğ‘›), let ğ‘‡be a ğœ-generalized KR rearrangement
that pushes forward ğœˆğœ‚to ğœˆğœ‹for some permutation ğœ. Then ğ‘‡= ğ‘„âŠ¤
ğœâˆ˜ğ‘‡â„“âˆ˜ğ‘„ğœa.e.,
where ğ‘„ğœâˆˆRğ‘›Ã—ğ‘›is a matrix representing the permutation, i.e., (ğ‘„ğœ)ğ‘–ğ‘—= (ğ‘’ğœ(ğ‘–))ğ‘—,
and where ğ‘‡â„“is a (lower triangular) KR rearrangement that pushes forward (ğ‘„ğœ)â™¯ğœˆğœ‚
to (ğ‘„ğœ)â™¯ğœˆğœ‹.
Proof. If ğ‘‡â„“pushes forward (ğ‘„ğœ)â™¯ğœˆğœ‚to (ğ‘„ğœ)â™¯ğœˆğœ‹, then ğœˆğœ‚âˆ˜ğ‘„âŠ¤
ğœâˆ˜ğ‘‡âˆ’1
â„“
= ğœˆğœ‹âˆ˜ğ‘„âŠ¤
ğœ, and
so ğ‘‡= ğ‘„âŠ¤
ğœâˆ˜ğ‘‡â„“âˆ˜ğ‘„ğœmust push forward ğœˆğœ‚to ğœˆğœ‹. Moreover, notice that ğ‘‡ğœ(ğ‘˜)(ğ‘¥) =
1 See Definition 5.3.
201

ğ‘‡ğ‘˜
â„“(ğ‘¥âŠ¤ğ‘’ğœ(1), . . . , ğ‘¥âŠ¤ğ‘’ğœ(ğ‘˜)), which shows that ğ‘‡is a monotone increasing ğœ-generalized
triangular function (see Definition 5.3). The lemma then follows by ğœˆğœ‚-uniqueness of
a KR rearrangement.
202

Appendix C
Proofs for Chapter 3
Here we collect the proofs and other technical results necessary to support the state-
ments made in Chapter 3.
We start with an auxiliary approximation result that plays an important role in our
analysis. Given a semi-positive definite diagonal matrix ğ·, we seek an approximation
of ğ·+ ğ¼by a rank ğ‘Ÿperturbation of the identity, ğ‘ˆğ‘ˆâŠ¤+ ğ¼, that minimizes a loss
function from the class â„’defined in (3.8).
The following lemma shows that the
optimal solution Ì‚ï¸€ğ‘ˆÌ‚ï¸€ğ‘ˆâŠ¤is simply the best rank ğ‘Ÿapproximation of the matrix ğ·in
the Frobenius norm.
Lemma C.1 (Approximation lemma). Let ğ·= diag{ğ‘‘2
1, . . . , ğ‘‘2
ğ‘›}, with ğ‘‘2
ğ‘–â‰¥ğ‘‘2
ğ‘–+1,
and ğ¿âˆˆâ„’. Define the functional ğ’¥: Rğ‘›Ã—ğ‘Ÿâ†’R, as: ğ’¥(ğ‘ˆ) = ğ¿(ğ‘ˆğ‘ˆâŠ¤+ ğ¼, ğ·+ ğ¼) =
âˆ‘ï¸€
ğ‘–ğ‘“(ğœğ‘–), where (ğœğ‘–) are the generalized eigenvalues of the pencil (ğ‘ˆğ‘ˆâŠ¤+ ğ¼, ğ·+ ğ¼)
and ğ‘“âˆˆğ’°. Then:
(i) There is a minimizer, Ì‚ï¸€ğ‘ˆ, of ğ’¥such that
Ì‚ï¸€ğ‘ˆÌ‚ï¸€ğ‘ˆâŠ¤=
ğ‘Ÿ
âˆ‘ï¸
ğ‘–=1
ğ‘‘2
ğ‘–ğ‘’ğ‘–ğ‘’âŠ¤
ğ‘–.
(C.1)
where (ğ‘’ğ‘–) are the columns of the identity matrix.
(ii) If the first ğ‘Ÿeigenvalues of ğ·are distinct, then any minimizer of ğ’¥satisfies
(C.1).
203

Proof. The idea is to apply [165, Theorem 1.1] to the functional ğ’¥. To this end,
we notice that ğ’¥can be equivalently written as: ğ’¥(ğ‘ˆ) = ğ¹âˆ˜ğœŒğ‘›âˆ˜ğ‘”(ğ‘ˆ), where:
ğ¹: Rğ‘›
+ â†’R is of the form ğ¹(ğ‘¥) = âˆ‘ï¸€ğ‘›
ğ‘–=1 ğ‘“(ğ‘¥ğ‘–); ğœŒğ‘›denotes a function that maps an
ğ‘›Ã— ğ‘›SPD matrix ğ´to its eigenvalues ğœ= (ğœğ‘–) (i.e., ğœŒğ‘›(ğ´) = ğœand since ğ¹is a
symmetric function, the order of the eigenvalues is irrelevant); and the mapping ğ‘”
is given by: ğ‘”(ğ‘ˆ) = (ğ·+ ğ¼)âˆ’1/2(ğ‘ˆğ‘ˆâŠ¤+ ğ¼)(ğ·+ ğ¼)âˆ’1/2, for all ğ‘ˆâˆˆRğ‘›Ã—ğ‘Ÿ. Since the
function ğ¹âˆ˜ğœŒğ‘›satisfies the hypotheses in [165, Theorem 1.1], ğ¹âˆ˜ğœŒğ‘›is differentiable
at the SPD matrix ğ‘‹if and only if ğ¹is differentiable at ğœŒğ‘›(ğ‘‹), in which case
(ğ¹âˆ˜ğœŒğ‘›)â€²(ğ‘‹) = ğ‘ğ‘†ğœğ‘âŠ¤, where
ğ‘†ğœ= diag[ ğ¹â€²(ğœŒğ‘›(ğ‘‹)) ] = diag{ğ‘“â€²(ğœ1), . . . , ğ‘“â€²(ğœğ‘›)},
and ğ‘is an orthogonal matrix such that ğ‘‹= ğ‘diag[ ğœŒğ‘›(ğ‘‹) ]ğ‘âŠ¤. Using the chain
rule, we obtain
ğœ•ğ’¥(ğ‘ˆ)
ğœ•ğ‘ˆğ‘–ğ‘—
= tr
(ï¸‚
ğ‘ğ‘†ğœğ‘âŠ¤ğœ•ğ‘”(ğ‘ˆ)
ğœ•ğ‘ˆğ‘–ğ‘—
)ï¸‚
,
which leads to the following gradient of ğ’¥at ğ‘ˆ:
ğ’¥â€²(ğ‘ˆ) = 2(ğ·+ ğ¼)âˆ’1/2ğ‘ğ‘†ğœ(ğ·+ ğ¼)âˆ’1/2ğ‘âŠ¤ğ‘ˆ= 2 ğ‘Šğ‘†ğœğ‘ŠâŠ¤ğ‘ˆ,
where the orthogonal matrix ğ‘is such that the matrix ğ‘Š= (ğ·+ ğ¼)âˆ’1/2ğ‘satisfies
(ğ‘ˆğ‘ˆâŠ¤+ ğ¼)ğ‘Š= (ğ·+ ğ¼)ğ‘ŠÏ’ğœ
(C.2)
with Ï’ğœ= diag(ğœ). Now we show that the functional ğ’¥is coercive. Let (ğ‘ˆğ‘˜) be a
sequence of matrices such that â€–ğ‘ˆğ‘˜â€–ğ¹â†’âˆ. Hence, ğœğ‘šğ‘ğ‘¥(ğ‘”(ğ‘ˆğ‘˜)) â†’âˆand so does
ğ’¥since:
ğ’¥(ğ‘ˆğ‘˜) â‰¥ğ‘“(ğœğ‘šğ‘ğ‘¥(ğ‘”(ğ‘ˆğ‘˜))) + (ğ‘›âˆ’1)ğ‘“(1)
and ğ‘“(ğ‘¥) â†’âˆas ğ‘¥â†’âˆ. Thus, ğ’¥is a differentiable coercive functional, and has a
204

global minimizer Ì‚ï¸€ğ‘ˆwith zero gradient:
ğ’¥â€²(Ì‚ï¸€ğ‘ˆ) = 2ğ‘Šğ‘†ğœğ‘ŠâŠ¤Ì‚ï¸€ğ‘ˆ= 0.
(C.3)
However, since ğ‘“âˆˆğ’°, ğ‘“â€²(ğ‘¥) = 0 iff ğ‘¥= 1. It follows that condition (C.3) is equivalent
to
(ğ¼âˆ’Ï’ğœ)ğ‘ŠâŠ¤Ì‚ï¸€ğ‘ˆ= 0.
(C.4)
(C.2) and (C.4) give Ì‚ï¸€ğ‘ˆÌ‚ï¸€ğ‘ˆâŠ¤âˆ’ğ·= ğ‘Šâˆ’âŠ¤Ï’âˆ’1(Ï’ âˆ’ğ¼)ğ‘ŠâŠ¤, and right-multiplication by
Ì‚ï¸€ğ‘ˆÌ‚ï¸€ğ‘ˆâŠ¤then yields:
ğ·(Ì‚ï¸€ğ‘ˆÌ‚ï¸€ğ‘ˆâŠ¤) = ( Ì‚ï¸€ğ‘ˆÌ‚ï¸€ğ‘ˆâŠ¤)2.
(C.5)
In particular, if ğ‘¢is an eigenvector of Ì‚ï¸€ğ‘ˆÌ‚ï¸€ğ‘ˆâŠ¤with nonzero eigenvalue ğ›¼, then ğ‘¢is an
eigenvector of ğ·, ğ·ğ‘¢= ğ›¼ğ‘¢, and thus ğ›¼= ğ‘‘2
ğ‘–> 0 for some ğ‘–. Thus, any solution of
(C.5) is such that:
Ì‚ï¸€ğ‘ˆÌ‚ï¸€ğ‘ˆâŠ¤=
ğ‘Ÿğ‘˜
âˆ‘ï¸
ğ‘–=1
ğ‘‘2
ğ‘˜ğ‘–ğ‘’ğ‘˜ğ‘–ğ‘’âŠ¤
ğ‘˜ğ‘–,
(C.6)
for some subsequence (ğ‘˜â„“) of {1, . . . , ğ‘›} and rank ğ‘Ÿğ‘˜â‰¤ğ‘Ÿ. Notice that any Ì‚ï¸€ğ‘ˆsatisfying
(C.5) is also a critical point according to (C.4). From (C.6) we also find that ğ‘”(Ì‚ï¸€ğ‘ˆ) is
a diagonal matrix,
ğ‘”(Ì‚ï¸€ğ‘ˆ) = (ğ·+ ğ¼)âˆ’1
(ï¸ƒğ‘Ÿğ‘˜
âˆ‘ï¸
ğ‘–=1
ğ‘‘2
ğ‘˜ğ‘–ğ‘’ğ‘˜ğ‘–ğ‘’âŠ¤
ğ‘˜ğ‘–+ ğ¼
)ï¸ƒ
.
The diagonal entries ğœğ‘–, which are the eigenvalues of ğ‘”(Ì‚ï¸€ğ‘ˆ), are given by ğœğ‘–= 1 if
ğ‘–= ğ‘˜â„“for some â„“â‰¤ğ‘Ÿğ‘˜, or ğœğ‘–= 1/(1 + ğ‘‘2
ğ‘–) otherwise. In either case, we have 0 <
ğœğ‘–â‰¤1 and the monotonicity of ğ‘“implies that ğ’¥(Ì‚ï¸€ğ‘ˆ) is minimized by the subsequence
ğ‘˜1 = 1, . . . , ğ‘˜ğ‘Ÿ= ğ‘Ÿ, and by the choice ğ‘Ÿğ‘˜= ğ‘Ÿ. This proves (C.1). It is clear that if the
first ğ‘Ÿeigenvalues of ğ·are distinct, then any minimizer of ğ’¥satisfies (C.1).
Most of the objective functions we consider have the same structure as the loss
function ğ’¥. Hence, the importance of Lemma C.1.
The next lemma shows that searching for a negative update of Î“pr is equivalent to
205

looking for a positive update of the prior precision matrix. In particular, the lemma
provides a bijection between the two approximation classes, â„³ğ‘Ÿand â„³âˆ’1
ğ‘Ÿ, defined
by (3.4) and (3.14). In what follows, ğ‘†pr is any square root of the prior covariance
matrix such that Î“pr = ğ‘†pr ğ‘†âŠ¤
pr.
Lemma C.2 (Prior updates). For any negative semidefinite update of Î“pr, Ì‚ï¸€Î“pos =
Î“pr âˆ’ğ¾ğ¾âŠ¤with Ì‚ï¸€Î“pos â‰»0, there is a matrix ğ‘ˆ(of the same rank as ğ¾) such that
Ì‚ï¸€Î“pos =
(ï¸€
Î“âˆ’1
pr + ğ‘ˆğ‘ˆâŠ¤)ï¸€âˆ’1 . The converse is also true.
Proof. Let ğ‘ğ·ğ‘âŠ¤= ğ‘†âˆ’1
pr ğ¾ğ¾âŠ¤ğ‘†âˆ’âŠ¤
pr , ğ·= diag{ğ‘‘2
ğ‘–}, be a reduced SVD of ğ‘†âˆ’1
pr ğ¾ğ¾âŠ¤ğ‘†âˆ’âŠ¤
pr .
Since Ì‚ï¸€Î“pos â‰»0 by assumption, we must have ğ‘‘2
ğ‘–< 1 for all ğ‘–, and we may thus define
ğ‘ˆ= ğ‘†âˆ’âŠ¤
pr ğ‘ğ·1/2(ğ¼âˆ’ğ·)âˆ’1/2. By Woodburyâ€™s identity:
(ï¸€
Î“âˆ’1
pr + ğ‘ˆğ‘ˆâŠ¤)ï¸€âˆ’1
=
Î“pr âˆ’Î“prğ‘ˆ
(ï¸€
ğ¼+ ğ‘ˆâŠ¤Î“âˆ’1
pr ğ‘ˆ
)ï¸€âˆ’1 ğ‘ˆâŠ¤Î“pr = Î“pr âˆ’ğ¾ğ¾âŠ¤= Ì‚ï¸€Î“pos.
Conversely, given a matrix ğ‘ˆ, we use again Woodburyâ€™s identity to write Ì‚ï¸€Î“pos as a
negative semidefinite update of Î“pr: Ì‚ï¸€Î“pos = Î“pr âˆ’ğ¾ğ¾âŠ¤â‰»0.
Now we prove our main result on approximations of the posterior covariance ma-
trix.
Proof of Theorem 3.1. Given a loss function ğ¿âˆˆâ„’, our goal is to minimize:
ğ¿(Î“pos, Ì‚ï¸€Î“pos) =
âˆ‘ï¸
ğ‘–
ğ‘“(ğœğ‘–)
(C.7)
over ğ¾âˆˆRğ‘›Ã—ğ‘Ÿsubject to the constraint Ì‚ï¸€Î“pos = Î“pr âˆ’ğ¾ğ¾âŠ¤â‰»0, where (ğœğ‘–) are
the generalized eigenvalues of the pencil (Î“pos, Ì‚ï¸€Î“pos) and ğ‘“belongs to the class ğ’°
defined by Eq. (3.9). We also write ğœğ‘–(Î“pos, Ì‚ï¸€Î“pos) to specify the pencil corresponding
to the eigenvalues. By Lemma C.2, the optimization problem is equivalent to finding
a matrix, ğ‘ˆâˆˆRğ‘›Ã—ğ‘Ÿ, that minimizes (C.7) subject to Ì‚ï¸€Î“âˆ’1
pos = Î“âˆ’1
pr + ğ‘ˆğ‘ˆâŠ¤. Observe
that (ğœğ‘–) are also the eigenvalues of the pencil (Ì‚ï¸€Î“âˆ’1
pos, Î“âˆ’1
pos).
Let ğ‘Šğ·ğ‘ŠâŠ¤= ğ‘†âŠ¤
prğ»ğ‘†pr with ğ·= diag{ğ›¿2
ğ‘–}, be an SVD of ğ‘†âŠ¤
prğ»ğ‘†pr. Then, by
206

the invariance properties of the generalized eigenvalues we have:
ğœğ‘–(Ì‚ï¸€Î“âˆ’1
pos, Î“âˆ’1
pos)
=
ğœğ‘–( ğ‘ŠâŠ¤ğ‘†âŠ¤
pr Ì‚ï¸€Î“âˆ’1
pos ğ‘†prğ‘Š, ğ‘ŠâŠ¤ğ‘†âŠ¤
pr Î“âˆ’1
pos ğ‘†prğ‘Š) = ğœğ‘–( ğ‘ğ‘âŠ¤+ ğ¼, ğ·+ ğ¼),
where ğ‘= ğ‘ŠâŠ¤ğ‘†âŠ¤
prğ‘ˆ. Therefore, our goal reduces to finding a matrix, ğ‘âˆˆRğ‘›Ã—ğ‘Ÿ, that
minimizes (C.7) with (ğœğ‘–) being the generalized eigenvalues of the pencil ( ğ‘ğ‘âŠ¤+
ğ¼, ğ·+ ğ¼). Applying Lemma C.1 leads to the simple solution: ğ‘ğ‘âŠ¤= âˆ‘ï¸€ğ‘Ÿ
ğ‘–=1 ğ›¿2
ğ‘–ğ‘’ğ‘–ğ‘’âŠ¤
ğ‘–,
where (ğ‘’ğ‘–) are the columns of the identity matrix. In particular, the solution is unique
if the first ğ‘Ÿeigenvalues of ğ‘†âŠ¤
prğ»ğ‘†pr are distinct. The corresponding approximation
ğ‘ˆğ‘ˆâŠ¤is then
ğ‘ˆğ‘ˆâŠ¤= ğ‘†âˆ’âŠ¤
pr ğ‘Šğ‘ğ‘âŠ¤ğ‘ŠâŠ¤ğ‘†âˆ’1
pr =
ğ‘Ÿ
âˆ‘ï¸
ğ‘–=1
ğ›¿2
ğ‘–Ìƒï¸€ğ‘¤ğ‘–Ìƒï¸€ğ‘¤âŠ¤
ğ‘–,
(C.8)
where Ìƒï¸€ğ‘¤ğ‘–= ğ‘†âˆ’âŠ¤
pr ğ‘¤ğ‘–and ğ‘¤ğ‘–is the ğ‘–th column of ğ‘Š. Woodburyâ€™s identity gives the
corresponding negative update of Î“pr as:
Ì‚ï¸€Î“pos = Î“pr âˆ’ğ¾ğ¾âŠ¤,
ğ¾ğ¾âŠ¤=
ğ‘Ÿ
âˆ‘ï¸
ğ‘–=1
ğ›¿2
ğ‘–
(ï¸€
1 + ğ›¿2
ğ‘–
)ï¸€âˆ’1 Ì‚ï¸€ğ‘¤ğ‘–Ì‚ï¸€ğ‘¤ğ‘–
âŠ¤
(C.9)
with Ì‚ï¸€ğ‘¤ğ‘–= ğ‘†prğ‘¤ğ‘–. Now, it suffices to note that the couples (ğ›¿2
ğ‘–, Ì‚ï¸€ğ‘¤ğ‘–) defined here are
precisely the generalized eigenpairs of the pencil (ğ», Î“âˆ’1
pr ).
At optimality, ğœğ‘–= 1 for
ğ‘–â‰¤ğ‘Ÿand ğœğ‘–= (1 + ğ›¿2
ğ‘–)âˆ’1 for ğ‘–> ğ‘Ÿ, proving (3.12). â–¡
Before proving Lemma 3.1, we recall that the Kullback-Leibler (K-L) divergence
and the Hellinger distance between two multivariate Gaussians, ğœˆ1 = ğ’©(ğœ‡, Î£1) and
ğœˆ2 = ğ’©(ğœ‡, Î£2), with the same mean and full rank covariance matrices are given,
respectively, by [209]:
ğ·KL (ğœˆ1â€– ğœˆ2) = 1
2
[ï¸‚
trace
(ï¸€
Î£âˆ’1
2 Î£1
)ï¸€
âˆ’rank(Î£1) âˆ’ln
(ï¸‚det(Î£1)
det(Î£2)
)ï¸‚]ï¸‚
(C.10)
ğ‘‘Hell (ğœˆ1, ğœˆ2) =
âˆšï¸ƒ
1 âˆ’|Î£1|1/4 |Î£2|1/4
|1
2Î£1 + 1
2Î£2|1/2.
(C.11)
Proof of Lemma 3.1. By (C.10), the K-L divergence between the posterior ğœˆpos(ğ‘Œ)
207

and the Gaussian approximation Ì‚ï¸€ğœˆpos(ğ‘Œ) can be written in terms of the generalized
eigenvalues of the pencil (Î“pos, Ì‚ï¸€Î“pos) as:
ğ·KL (ğœˆpos(ğ‘Œ)â€– Ì‚ï¸€ğœˆpos(ğ‘Œ)) =
âˆ‘ï¸
ğ‘–
( ğœğ‘–âˆ’ln ğœğ‘–âˆ’1 ) /2,
and since ğ‘“(ğ‘¥) = (ğ‘¥âˆ’ln ğ‘¥âˆ’1) /2 belongs to ğ’°, we see that the K-L divergence is
a loss function in the class â„’defined by (3.8). Hence, Theorem 3.1 applies and the
equivalence between the two approximations follows trivially. An analogous argument
holds for the Hellinger distance. The squared Hellinger distance between ğœˆpos(ğ‘Œ) and
Ì‚ï¸€ğœˆpos(ğ‘Œ) can be written in terms of the generalized eigenvalues, (ğœğ‘–), of the pencil
(Î“pos, Ì‚ï¸€Î“pos), as:
ğ‘‘Hell (ğœˆpos(ğ‘Œ), Ì‚ï¸€ğœˆpos(ğ‘Œ))2 = 1 âˆ’2â„“/2 âˆï¸
ğ‘–
ğœ1/4
ğ‘–
(1 + ğœğ‘–)âˆ’1/2 .
(C.12)
where â„“is the dimension of the parameter space. Minimizing (C.12) is equivalent
to maximizing âˆï¸€
ğ‘–ğœ1/4
ğ‘–
(1 + ğœğ‘–)âˆ’1/2, which in turn is equivalent to minimizing the
functional:
ğ¿(Î“pos, Ì‚ï¸€Î“pos) = âˆ’
âˆ‘ï¸
ğ‘–
ln( ğœ1/4
ğ‘–
(1 + ğœğ‘–)âˆ’1/2 ) =
âˆ‘ï¸
ğ‘–
ln( 2 + ğœğ‘–+ 1/ğœğ‘–)/4.
(C.13)
Since ğ‘“(ğ‘¥) = ln( 2+ğ‘¥+1/ğ‘¥)/4 belongs to ğ’°, Theorem 3.1 can be applied once again.
â–¡
Proof of Corollary 3.1. The proofs of parts (i) and (ii) were already given in the
proof of Theorem 3.1. Part (iii) holds because,
(1 + ğ›¿2
ğ‘–)Î“pos Ìƒï¸€ğ‘¤ğ‘–
=
(1 + ğ›¿2
ğ‘–)(ğ»+ Î“âˆ’1
pr )âˆ’1ğ‘†âˆ’âŠ¤
pr ğ‘¤ğ‘–
=
(1 + ğ›¿2
ğ‘–) ğ‘†pr(ğ‘†âŠ¤
prğ»ğ‘†pr + ğ¼)âˆ’1ğ‘¤ğ‘–= ğ‘†pr ğ‘¤ğ‘–= Î“pr Ìƒï¸€ğ‘¤ğ‘–,
because ğ‘¤ğ‘–is an eigenvector of ( ğ‘†âŠ¤
prğ»ğ‘†pr + ğ¼)âˆ’1 with eigenvalue (1 + ğ›¿2
ğ‘–)âˆ’1 as shown
in the proof of Theorem 3.1. â–¡
208

Now we turn to optimality results for approximations of the posterior mean. In
what follows, let ğ‘†pr, ğ‘†obs, ğ‘†pos, and ğ‘†ğ‘Œbe the matrix square roots of, respectively,
Î“pr, Î“obs, Î“pos, and Î“ğ‘Œ:= Î“obs + ğºÎ“pr ğºâŠ¤such that Î“ = ğ‘†ğ‘†âŠ¤(i.e., possibly non-
symmetric square roots).
Equation (3.27) shows that, to minimize E( â€–ğ´ğ‘Œâˆ’ğ‘‹â€–2
Î“âˆ’1
pos ) over ğ´âˆˆğ’œ, we
need only to minimize E( â€– ğ´ğ‘Œâˆ’ğœ‡pos(ğ‘Œ) â€–2
Î“âˆ’1
pos ).
Furthermore, since ğœ‡pos(ğ‘Œ) =
Î“pos ğºâŠ¤Î“âˆ’1
obs ğ‘Œ, it follows that
E( â€– ğ´ğ‘Œâˆ’ğœ‡pos(ğ‘Œ) â€–2
Î“âˆ’1
pos ) = â€– ğ‘†âˆ’1
pos (ğ´âˆ’Î“pos ğºâŠ¤Î“âˆ’1
obs) ğ‘†ğ‘Œâ€–2
ğ¹,
(C.14)
We are therefore led to the following optimization problem:
min
ğ´âˆˆğ’œâ€– ğ‘†âˆ’1
posğ´ğ‘†ğ‘Œâˆ’ğ‘†âŠ¤
posğºâŠ¤Î“âˆ’1
obs ğ‘†ğ‘Œâ€–ğ¹.
(C.15)
The following result shows that an SVD of the matrix ğ‘†Ì‚ï¸€
ğ»:= ğ‘†âŠ¤
pr ğºâŠ¤ğ‘†âˆ’âŠ¤
obs can be used
to obtain simple expressions for the square roots of Î“pos and Î“ğ‘Œ.
Lemma C.3 (Square roots). Let ğ‘Šğ·ğ‘‰âŠ¤be an SVD of ğ‘†Ì‚ï¸€
ğ»= ğ‘†âŠ¤
pr ğºâŠ¤ğ‘†âˆ’âŠ¤
obs . Then:
ğ‘†pos
=
ğ‘†pr ğ‘Š( ğ¼+ ğ·ğ·âŠ¤)âˆ’1/2 ğ‘ŠâŠ¤
(C.16)
ğ‘†ğ‘Œ
=
ğ‘†obs ğ‘‰( ğ¼+ ğ·âŠ¤ğ·)1/2 ğ‘‰âŠ¤
(C.17)
are square roots of Î“pos and Î“ğ‘Œ.
Proof. We can rewrite Î“pos = ( ğºâŠ¤Î“âˆ’1
obsğº+ Î“âˆ’1
pr )âˆ’1 as
Î“pos
=
ğ‘†pr ( ğ‘†Ì‚ï¸€
ğ»ğ‘†âŠ¤
Ì‚ï¸€
ğ»+ ğ¼)âˆ’1 ğ‘†âŠ¤
pr = ğ‘†pr ğ‘Š( ğ·ğ·âŠ¤+ ğ¼)âˆ’1ğ‘ŠâŠ¤ğ‘†âŠ¤
pr
=
[ ğ‘†pr ğ‘Š( ğ·ğ·âŠ¤+ ğ¼)âˆ’1/2 ğ‘ŠâŠ¤] [ ğ‘†pr ğ‘Š( ğ·ğ·âŠ¤+ ğ¼)âˆ’1/2 ğ‘ŠâŠ¤]âŠ¤,
which proves (C.16). The proof of (C.17) follows similarly using: ğ‘†âŠ¤
Ì‚ï¸€
ğ»ğ‘†Ì‚ï¸€
ğ»= ğ‘†âˆ’1
obsğºÎ“pr ğºâŠ¤Î“âˆ’âŠ¤
obs.
In the next two proofs we use (ğ¶)ğ‘Ÿto denote a rank ğ‘Ÿapproximation of the matrix
209

ğ¶in the Frobenius norm.
Proof of Theorem 4.2. By [102, Theorem 2.1], an optimal ğ´âˆˆğ’œğ‘Ÿis given by:
ğ´= ğ‘†pos
(ï¸€
ğ‘†âŠ¤
posğºâŠ¤Î“âˆ’1
obs ğ‘†ğ‘Œ
)ï¸€
ğ‘Ÿğ‘†âˆ’1
ğ‘Œ.
(C.18)
Now, we need some computations to show that (C.18) is equivalent to (3.30). Using
(C.16) and (C.17) we find ğ‘†âŠ¤
posğºâŠ¤Î“âˆ’1
obs ğ‘†ğ‘Œ= ğ‘Š(ğ¼+ğ·ğ·âŠ¤)âˆ’1/2ğ·(ğ¼+ğ·âŠ¤ğ·)1/2 ğ‘‰âŠ¤, and
therefore ( ğ‘†âŠ¤
posğºâŠ¤Î“âˆ’1
obs ğ‘†ğ‘Œ)ğ‘Ÿ= âˆ‘ï¸€ğ‘Ÿ
ğ‘–=1 ğ›¿ğ‘–ğ‘¤ğ‘–ğ‘£âŠ¤
ğ‘–, where ğ‘¤ğ‘–is the ğ‘–th column of ğ‘Š, ğ‘£ğ‘–is the
ğ‘–th column of ğ‘‰, and ğ›¿ğ‘–is the ğ‘–th diagonal entry of ğ·. Inserting this back into (C.18)
yields ğ´= âˆ‘ï¸€
ğ‘–â‰¤ğ‘Ÿğ›¿ğ‘–(1 + ğ›¿2
ğ‘–)âˆ’1ğ‘†prğ‘¤ğ‘–ğ‘£âŠ¤
ğ‘–ğ‘†âˆ’1
obs. Now it suffices to note that Ì‚ï¸€ğ‘¤ğ‘–:= ğ‘†prğ‘¤ğ‘–is
a generalized eigenvector of (ğ», Î“âˆ’1
pr ), that Ì‚ï¸€ğ‘£ğ‘–:= ğ‘†âˆ’âŠ¤
obs ğ‘£ğ‘–is a generalized eigenvector of
(ğºÎ“prğºâŠ¤, Î“obs), and that (ğ›¿2
ğ‘–) are also eigenvalues of (ğ», Î“âˆ’1
pr ).
The minimum Bayes
risk is a straightforward computation for the optimal estimator (3.30) using (C.14).
â–¡
Proof of Theorem 3.3. Given ğ´âˆˆÌ‚ï¸€
ğ’œğ‘Ÿ, we can restate (C.15) as the problem of
finding a matrix ğµ, of rank at most ğ‘Ÿ, that minimizes:
â€– ğ‘†âˆ’1
pos(Î“pr âˆ’Î“pos) ğºâŠ¤Î“âˆ’1
obs ğ‘†ğ‘Œâˆ’ğ‘†âˆ’1
posğµ
(ï¸€
ğºâŠ¤Î“âˆ’1
obs ğ‘†ğ‘Œ
)ï¸€
â€–ğ¹
(C.19)
such that ğ´= (Î“pr âˆ’ğµ) ğºâŠ¤Î“âˆ’1
obs. By [102, Theorem 2.1], an optimal ğµis given by:
ğµ= ğ‘†pos( ğ‘†âˆ’1
pos (Î“pr âˆ’Î“pos) ğºâŠ¤Î“âˆ’1
obs ğ‘†ğ‘Œ)ğ‘Ÿ(ğºâŠ¤Î“âˆ’1
obs ğ‘†ğ‘Œ)â€ 
(C.20)
where â€  denotes the pseudo-inverse operator. A closer look at [102, Theorem 2.1]
reveals that another minimizer of (C.19), itself not necessarily of minimum Frobenius
norm, is given by:
ğµ= ğ‘†pos( ğ‘†âˆ’1
pos (Î“pr âˆ’Î“pos) ğºâŠ¤Î“âˆ’1
obs ğ‘†ğ‘Œ)ğ‘Ÿ(ğ‘†âŠ¤
prğºâŠ¤Î“âˆ’1
obs ğ‘†ğ‘Œ)â€ ğ‘†âŠ¤
pr.
(C.21)
210

By Lemma C.3,
ğ‘†âŠ¤
prğºâŠ¤Î“âˆ’1
obs ğ‘†ğ‘Œ
=
ğ‘Š[ ğ·
(ï¸€
ğ¼+ ğ·âŠ¤ğ·
)ï¸€1/2 ]ğ‘‰âŠ¤
ğ‘†âˆ’1
pos Î“prğºâŠ¤Î“âˆ’1
obsğ‘†ğ‘Œ
=
ğ‘Š[ (ğ¼+ ğ·ğ·âŠ¤)1/2ğ·(ğ¼+ ğ·âŠ¤ğ·)1/2 ]ğ‘‰âŠ¤
ğ‘†âˆ’1
pos Î“posğºâŠ¤Î“âˆ’1
obsğ‘†ğ‘Œ
=
ğ‘Š[ (ğ¼+ ğ·ğ·âŠ¤)âˆ’1/2ğ·(ğ¼+ ğ·âŠ¤ğ·)1/2 ]ğ‘‰âŠ¤
and therefore (ğ‘†âŠ¤
prğºâŠ¤Î“âˆ’1
obs ğ‘†ğ‘Œ)â€  = âˆ‘ï¸€ğ‘
ğ‘–=1 ğ›¿âˆ’1
ğ‘–
(1 + ğ›¿2
ğ‘–)âˆ’1/2 ğ‘£ğ‘–ğ‘¤âŠ¤
ğ‘–for ğ‘= rank(ğ‘†Ì‚ï¸€
ğ»), whereas
( ğ‘†âˆ’1
pos (Î“pr âˆ’Î“pos)ğºâŠ¤Î“âˆ’1
obsğ‘†ğ‘Œ)ğ‘Ÿ=
ğ‘Ÿ
âˆ‘ï¸
ğ‘–=1
ğ›¿3
ğ‘–ğ‘¤ğ‘–ğ‘£âŠ¤
ğ‘–.
Inserting these expressions back into (C.21), we obtain:
ğµ= ğ‘†pr
(ï¸ƒ
ğ‘Ÿ
âˆ‘ï¸
ğ‘–=1
ğ›¿2
ğ‘–
1 + ğ›¿2
ğ‘–
ğ‘¤ğ‘–ğ‘¤âŠ¤
ğ‘–
)ï¸ƒ
ğ‘†âŠ¤
pr,
where ğ‘¤ğ‘–is the ğ‘–th column of ğ‘Š, ğ‘£ğ‘–is the ğ‘–th column of ğ‘‰, and ğ›¿ğ‘–is the ğ‘–th diagonal
entry of ğ·. Notice that (ğ›¿2
ğ‘–, Ì‚ï¸€ğ‘¤ğ‘–), with Ì‚ï¸€ğ‘¤ğ‘–= ğ‘†prğ‘¤ğ‘–, are the generalized eigenpairs
of (ğ», Î“âˆ’1
pr ) (cf. proof of Theorem 3.1). Hence, by Theorem 3.1, we recognize the
optimal approximation of Î“pos as Ì‚ï¸€Î“pos = Î“pr âˆ’ğµ. Plugging this expression back into
(C.21) gives (3.33). The minimum Bayes risk in (ii) follows readily using the optimal
estimator given by (3.33) in (C.14). â–¡
211

212

Appendix D
Proofs for Chapter 4
The following two Lemmas, D.1 and 4.1, will be used to prove Theorems 4.1 and 4.2.
We start with a result describing the relationships between different eigenpairs of the
Schur complements of a particular class of covariance matrices that arises in Bayesian
inverse problems.
Lemma D.1 (Eigenpairs of Schur complements). Let Î£ â‰»0 be a matrix partitioned
as
Î£ =
â›
âğ´
ğµ
ğµâŠ¤
ğ¶
â
â ,
(D.1)
where ğ´and ğ¶are square matrices and ğµÌ¸= 0. Then, ğ´,ğ¶, and the Schur comple-
ments, ğ’®(ğ´) := ğ¶âˆ’ğµâŠ¤ğ´âˆ’1ğµand ğ’®(ğ¶) := ğ´âˆ’ğµğ¶âˆ’1ğµâŠ¤, are also SPD matrices.
Moreover:
1. If (ğ›½, ğ‘¤) is an eigenpair of (ğµğ¶âˆ’1ğµâŠ¤, ğ´), then ğ›½< 1 and (1 âˆ’ğ›½, ğ‘¤) is an
eigenpair of (ğ’®(ğ¶), ğ´). Furthermore, if ğ›½Ì¸= 0, then ((1 âˆ’ğ›½)âˆ’1, ğµâŠ¤ğ‘¤) is an
eigenpair of (ğ’®(ğ´)âˆ’1, ğ¶âˆ’1).
2. If ğ›½Ì¸= 0 and (ğ›½, ğ‘¤) is an eigenpair of (ğµğ¶âˆ’1ğµâŠ¤, ğ´), then (ğ›½(1 âˆ’ğ›½)âˆ’1, ğµâŠ¤ğ‘¤) is
an eigenpair of (ğ¶âˆ’1ğµâŠ¤ğ’®(ğ¶)âˆ’1ğµğ¶âˆ’1, ğ¶âˆ’1).
3. If ğ‘¤1, . . . , ğ‘¤ğ‘˜are linearly independent eigenvectors of (ğµğ¶âˆ’1ğµâŠ¤, ğ´) with asso-
ciated eigenvalues ğ›½1 â‰¥ğ›½2 â‰¥Â· Â· Â· â‰¥ğ›½ğ‘˜> 0, then ğµâŠ¤ğ‘¤1, . . . , ğµâŠ¤ğ‘¤ğ‘˜are linearly
213

independent. Moreover, if ğ‘˜= rank(ğµğ¶âˆ’1ğµâŠ¤), then there can be at most ğ‘˜
linearly independent eigenvectors of (ğ¶âˆ’1ğµâŠ¤ğ’®(ğ¶)âˆ’1ğµğ¶âˆ’1, ğ¶âˆ’1) associated with
strictly positive eigenvalues.
Proof. The fact that ğ´, ğ¶, ğ’®(ğ´) and ğ’®(ğ¶) are SPD matrices follows from [37]. (1)
From ğµğ¶âˆ’1ğµâŠ¤ğ‘¤= ğ›½ğ´ğ‘¤we obtain ğ’®(ğ¶)ğ‘¤= (1 âˆ’ğ›½)ğ´ğ‘¤, which also implies that
ğ›½< 1 as ğ’®(ğ¶) â‰»0 and ğ´â‰»0. If ğ›½Ì¸= 0, then ğµâŠ¤ğ‘¤Ì¸= 0 and
ğ’®(ğ´)âˆ’1ğµâŠ¤ğ‘¤
=
[ ğ¶âˆ’1 + ğ¶âˆ’1ğµâŠ¤ğ’®(ğ¶)âˆ’1ğµğ¶âˆ’1 ] ğµâŠ¤ğ‘¤
=
[ ğ¶âˆ’1ğµâŠ¤+ ğ¶âˆ’1ğµâŠ¤ğ’®(ğ¶)âˆ’1(ğ´âˆ’ğ’®(ğ¶)) ]ğ‘¤
=
(1 âˆ’ğ›½)âˆ’1 ğ¶âˆ’1ğµâŠ¤ğ‘¤.
where we used the Woodbury identity to rewrite ğ’®(ğ´)âˆ’1. (2) It follows from (1)
that ((1 âˆ’ğ›½)âˆ’1, ğµâŠ¤ğ‘¤) is an eigenpair of (ğ’®(ğ´)âˆ’1, ğ¶âˆ’1) and by ğ’®(ğ´)âˆ’1 = ğ¶âˆ’1 +
ğ¶âˆ’1ğµâŠ¤ğ’®(ğ¶)âˆ’1ğµğ¶âˆ’1 that (ğ›½(1âˆ’ğ›½)âˆ’1, ğµâŠ¤ğ‘¤) is an eigenpair of (ğ¶âˆ’1ğµâŠ¤ğ’®(ğ¶)âˆ’1ğµğ¶âˆ’1, ğ¶âˆ’1).
(3) If âˆ‘ï¸€ğ‘˜
ğ‘—=1 ğ‘ğ‘—ğµâŠ¤ğ‘¤ğ‘—= 0, then ğ´âˆ‘ï¸€ğ‘˜
ğ‘—=1 ğ›½ğ‘—ğ‘ğ‘—ğ‘¤ğ‘—= 0, and therefore âˆ‘ï¸€ğ‘˜
ğ‘—=1 ğ›½ğ‘—ğ‘ğ‘—ğ‘¤ğ‘—= 0
since ğ´â‰»0, which leads to ğ›½ğ‘—ğ‘ğ‘—= 0 for ğ‘—= 1, . . . . , ğ‘˜since (ğ‘¤ğ‘—) are linearly in-
dependent, and thus ğ‘ğ‘—= 0 for ğ‘—= 1, . . . . , ğ‘˜since ğ›½ğ‘—> 0. Moreover, notice that
rank(ğ¶âˆ’1ğµâŠ¤ğ’®(ğ¶)âˆ’1ğµğ¶âˆ’1) = rank(ğµâŠ¤ğ’®(ğ¶)âˆ’1ğµ) = rank(ğµğ¶âˆ’1ğµâŠ¤). Thus, there can
be at most rank(ğµğ¶âˆ’1ğµâŠ¤) linearly independent eigenvectors of (ğ¶âˆ’1ğµâŠ¤ğ’®(ğ¶)âˆ’1ğµğ¶âˆ’1, ğ¶âˆ’1)
with nonzero eigenvalues.
Proof of Lemma 4.1. Consider the identity ğ‘Œ= ğºğ‘‹+â„°= ğºğ’ªâ€  ğ’ªğ‘‹+ğº(ğ¼âˆ’
ğ’ªâ€  ğ’ª) ğ‘‹+ â„°= ğºğ’ªâ€  ğ‘+ Î”, where ğ’ªâ€  := Î“prğ’ªâŠ¤Î“âˆ’1
ğ‘and Î” := ğº(ğ¼âˆ’ğ’ªâ€  ğ’ª) ğ‘‹+ â„°.
A simple computation shows that E[(ğ¼âˆ’ğ’ªâ€  ğ’ª)ğ‘‹ğ‘âŠ¤] = 0. Hence, (ğ¼âˆ’ğ’ªâ€  ğ’ª)ğ‘‹and
ğ‘are uncorrelated, and, more importantly, independent since they are also jointly
Gaussian. It follows that Î” and ğ‘are also independent since â„°was independent
of ğ‘‹and ğ‘= ğ’ªğ‘‹. In the hypothesis of zero prior mean, the mean of Î” is also
zero. Moreover, Î“Î” = Var[ğº(ğ¼âˆ’ğ’ªâ€  ğ’ª)ğ‘‹] + Var[â„°] since ğ‘‹and â„°are independent.
Simple algebra leads to the particular form of Î“Î”.
â–¡
214

We can now prove the main results of this chapter.
Proof of Theorem 4.1. By applying [261, Theorem 2.3] to the linear Gaussian
model defined in Lemma 4.1, we know that a minimizer, Ì‚ï¸€Î“ğ‘|ğ‘Œ, of the geodesic dis-
tance, ğ‘‘â„›, between Î“ğ‘|ğ‘Œand an element of â„³ğ‘
ğ‘Ÿis given by: Ì‚ï¸€Î“ğ‘|ğ‘Œ= Î“ğ‘âˆ’âˆ‘ï¸€ğ‘Ÿ
ğ‘–=1 ğœ‚2
ğ‘–(1+
ğœ‚2
ğ‘–)âˆ’1 Ì‚ï¸€ğ‘ğ‘–Ì‚ï¸€ğ‘âŠ¤
ğ‘–, where (ğœ‚2
ğ‘–, Ì‚ï¸€ğ‘ğ‘–) are the eigenpairs of (ğ»ğ‘, Î“âˆ’1
ğ‘), with the ordering ğœ‚2
ğ‘–â‰¥ğœ‚2
ğ‘–+1,
the normalization Ì‚ï¸€ğ‘âŠ¤
ğ‘–Î“âˆ’1
ğ‘Ì‚ï¸€ğ‘ğ‘–= 1 and where ğ»ğ‘:= ğ’ªâŠ¤
â€  ğºâŠ¤Î“âˆ’1
Î” ğºğ’ªâ€  is the Hessian of
the negative logâ€“likelihood ğ‘Œ|ğ‘âˆ¼ğ’©(ğºğ’ªâ€ , Î“Î”). Moreover, [261, Theorem 2.3] im-
plies that the distance, at optimality, is given by ğ‘‘2
â„›(Ì‚ï¸€Î“ğ‘|ğ‘Œ, Î“ğ‘|ğ‘Œ) = âˆ‘ï¸€
ğ‘–>ğ‘Ÿln2( 1 + ğœ‚2
ğ‘–)
and that the minimizer is unique if the first ğ‘Ÿeigenvalues of (ğ»ğ‘, Î“âˆ’1
ğ‘) are distinct.
Now let (ğœ†ğ‘–, ğ‘ğ‘–) be defined as in Theorem 4.1, with ğœ†ğ‘–> 0, and let Î£ â‰»0 be the
covariance matrix of the joint distribution of ğ‘Œand ğ‘, i.e.,
Î£ =
â›
â
Î“ğ‘Œ
ğºÎ“pr ğ’ªâŠ¤
ğ’ªÎ“pr ğºâŠ¤
Î“ğ‘
â
â .
(D.2)
By Lemma D.1[part 2] applied to (D.2), we know that ( ğœ†ğ‘–(1 âˆ’ğœ†ğ‘–)âˆ’1 , ğ’ªÎ“pr ğºâŠ¤ğ‘ğ‘–)
are eigenpairs of (ğ»ğ‘, Î“âˆ’1
ğ‘).
Moreover, by Lemma D.1[part 3] we know that we
can always write a maximal set of linearly independent eigenvectors of (ğ»ğ‘, Î“âˆ’1
ğ‘),
associated with nonzero eigenvalues, as (ğ’ªÎ“pr ğºâŠ¤ğ‘ğ‘–). Thus, since ğ‘“(ğœ†) = ğœ†(1 âˆ’ğœ†)âˆ’1
is a decreasing function of ğœ†as ğœ†â†“0, we must have ğœ‚2
ğ‘–= ğœ†ğ‘–(1 âˆ’ğœ†ğ‘–)âˆ’1 and we can
assume, without loss of generality, that Ì‚ï¸€ğ‘ğ‘–= ğ›¼ğ’ªÎ“pr ğºâŠ¤ğ‘ğ‘–for some real ğ›¼> 0. Given
the normalizations Ì‚ï¸€ğ‘âŠ¤Î“âˆ’1
ğ‘Ì‚ï¸€ğ‘= 1 and ğ‘âŠ¤
ğ‘–( ğºÎ“pr ğ’ªâŠ¤Î“âˆ’1
ğ‘ğ’ªÎ“pr ğºâŠ¤)ğ‘ğ‘–= 1, it follows that
ğ›¼= 1.
Simple algebra then leads to (4.9) and (4.10).
Notice, that ğœ†ğ‘–> 0 and
ğ‘“(ğœ†ğ‘–) > 0 imply ğœ†ğ‘–< 1. This property will be useful when proving Lemma 4.5.
â–¡
We now state a standard result that will be used in proving Lemma 4.2.
Theorem D.1 (Cauchy Interlacing Theorem (e.g., [157, 25])). Let ğ´, ğµâˆˆRğ‘›Ã—ğ‘›be
symmetric matrices with ğµâ‰»0 and let ğ›¾1 â‰¥ğ›¾2 â‰¥Â· Â· Â· â‰¥ğ›¾ğ‘›be the eigenvalues of
(ğ´, ğµ). For any ğ‘ƒâˆˆRğ‘›Ã—ğ‘, with ğ‘â‰¤ğ‘›and full column-rank, let ğœ‡1 â‰¥ğœ‡2 â‰¥Â· Â· Â· â‰¥ğœ‡ğ‘
215

be the eigenvalues of (ğ‘ƒâŠ¤ğ´ğ‘ƒ, ğ‘ƒâŠ¤ğµğ‘ƒ). Then:
ğ›¾ğ‘˜â‰¥ğœ‡ğ‘˜â‰¥ğ›¾ğ‘›âˆ’ğ‘+ğ‘˜,
ğ‘˜= 1, . . . , ğ‘.
(D.3)
Proof of Lemma 4.2. The first inequality in (4.12) follows from the optimality
statement of Theorem 4.1 since Ì‚ï¸€Î“ğ‘|ğ‘Œâˆˆâ„³ğ‘
ğ‘Ÿ. The second inequality in (4.12) follows
from the Cauchy interlacing theorem (see Theorem D.1). Let ğ›¾1 â‰¥ğ›¾2 â‰¥Â· Â· Â· â‰¥ğ›¾ğ‘›â‰¥1
be the eigenvalues of ( Ì‚ï¸€Î“pos , Î“pos ) and ğœ‡1 â‰¥ğœ‡2 â‰¥Â· Â· Â· â‰¥ğœ‡ğ‘be the eigenvalues of
( Ì‚ï¸€Î“ğ‘|ğ‘Œ, Î“ğ‘|ğ‘Œ) = ( ğ’ªÌ‚ï¸€Î“pos ğ’ªâŠ¤, ğ’ªÎ“pos ğ’ªâŠ¤), where ğ’ªis a full row-rank matrix. Then,
by Theorem D.1,
ğ›¾ğ‘˜â‰¥ğœ‡ğ‘˜â‰¥1,
ğ‘˜= 1, . . . , ğ‘.
(D.4)
In particular, since ln2(ğ‘¥) is monotone increasing on ğ‘¥> 1, we have:
ğ‘‘â„›( Î“ğ‘|ğ‘Œ, Ì‚ï¸€Î“ğ‘|ğ‘Œ) = 1
2
âˆ‘ï¸
ğ‘˜
ln2(ğœ‡ğ‘˜) â‰¤1
2
âˆ‘ï¸
ğ‘˜
ln2(ğ›¾ğ‘˜) â‰¤ğ‘‘â„›( Î“pos , Ì‚ï¸€Î“pos ),
(D.5)
where clearly âˆ‘ï¸€
ğ‘˜>ğ‘ln2(ğ›¾ğ‘˜) â‰¥0.
â–¡
The following two lemmas will be used in proving Lemma 4.3.
Lemma D.2. If Î“1 âª°Î“2 â‰»0, then |Î“1| â‰¥|Î“2|
Proof. If Î“1 âª°Î“2, then there exists a ğ‘†âª°0 such that Î“1 = Î“2+ğ‘†. Thus, |Î“1| |Î“2|âˆ’1 =
|ğ¼+ Î“âˆ’1/2
2
ğ‘†Î“âˆ’1/2
2
| â‰¥1.
Lemma D.3. Let ğ‘‹âˆ¼ğ’©(ğœ‡, Î£) and ğ‘Œâˆ¼ğ’©(0, Î“) with Î“ âª°Î£ â‰»0. Let ğ‘”be a
measurable real-valued function such that
E[|ğ‘”|2+ğ›¼(ğ‘Œ)] < âˆ
(D.6)
for some ğ›¼> 0. Then,
E[ğ‘”2(ğ‘‹)] â‰¤|Î“|1/2
|Î£|1/2 exp
(ï¸‚ğœ‡âŠ¤Î“âˆ’1ğœ‡
ğ›¼
)ï¸‚
E[|ğ‘”|2+ğ›¼(ğ‘Œ)]1/(1+ğ›¼/2)
(D.7)
216

Proof. Let ğ‘“ğ‘‹and ğ‘“ğ‘Œbe the densities of ğ‘‹and ğ‘Œ, respectively, and ğ‘€ğ‘Œthe moment
generating function of ğ‘Œ. Since we have Î£âˆ’1 = Î“âˆ’1 + ğ‘†for some ğ‘†âª°0, it follows
that for all ğ‘¥,
ğ‘“ğ‘‹(ğ‘¥) â‰¤ğ¾exp(ğœ‡âŠ¤Î“âˆ’1 ğ‘¥) ğ‘“ğ‘Œ(ğ‘¥),
(D.8)
where ğ¾:= |Î“|1/2 |Î£|âˆ’1/2 exp(âˆ’ğœ‡âŠ¤Î“âˆ’1 ğœ‡/2). Now we use HÃ¶lderâ€™s inequality, with
ğ‘= 1 + ğ›¼/2 and ğ‘= ğ‘/(ğ‘âˆ’1) so that 1/ğ‘+ 1/ğ‘= 1, to obtain:
E[ğ‘”2(ğ‘‹)]
â‰¤
ğ¾E[ğ‘”2(ğ‘Œ) exp(ğœ‡âŠ¤Î“âˆ’1 ğ‘Œ)]
â‰¤
ğ¾(E[|ğ‘”|2ğ‘(ğ‘Œ)])1/ğ‘(E[exp(ğ‘ğœ‡âŠ¤Î“âˆ’1 ğ‘Œ)])1/ğ‘
=
ğ¾(E[|ğ‘”|2ğ‘(ğ‘Œ)])1/ğ‘ğ‘€1/ğ‘
ğ‘Œ(ğ‘Î“âˆ’1ğœ‡)
=
ğ¾(E[|ğ‘”|2ğ‘(ğ‘Œ)])1/ğ‘exp(ğ‘ğœ‡âŠ¤Î“âˆ’1ğœ‡/2)
=
|Î“|1/2 |Î£|âˆ’1/2 (E[|ğ‘”|2ğ‘(ğ‘Œ)])1/ğ‘exp((ğ‘âˆ’1) ğœ‡âŠ¤Î“âˆ’1ğœ‡/2)
=
|Î“|1/2 |Î£|âˆ’1/2 exp(ğœ‡âŠ¤Î“âˆ’1ğœ‡/ğ›¼)(E[|ğ‘”|2+ğ›¼(ğ‘Œ)])1/(1+ğ›¼/2),
where we used the fact that ğ‘€ğ‘Œ(ğ‘¡) = exp(ğ‘¡âŠ¤Î“ ğ‘¡/2) since ğ‘Œâˆ¼ğ’©(0, Î“).
Proof of Lemma 4.3. By [76, Lemma 7.14] we have:
âƒ’âƒ’âƒ’Eğœˆğ‘|ğ‘Œ[ğ‘”] âˆ’EËœğœˆğ‘|ğ‘Œ[ğ‘”]
âƒ’âƒ’âƒ’â‰¤2
âˆšï¸ƒâˆ«ï¸
|ğ‘”|2 (ğœ‹ğ‘|ğ‘Œ+ Ëœğœ‹ğ‘|ğ‘Œ) ğ‘‘Hell(ğœˆğ‘|ğ‘Œ, Ëœğœˆğ‘|ğ‘Œ)
(D.9)
where ğœ‹ğ‘|ğ‘Œand Ëœğœ‹ğ‘|ğ‘Œare, respectively, the densitites of ğœˆğ‘|ğ‘Œand Ëœğœˆğ‘|ğ‘Œwith respect
to the Lebesgue measure. Now notice that Î“ğ‘|ğ‘Œâª¯Î“ğ‘as well as Ìƒï¸€Î“ğ‘|ğ‘Œâª¯Î“ğ‘. Thus,
by Lemma D.3, we have:
Eğœˆğ‘|ğ‘Œ[|ğ‘”|2] + EËœğœˆğ‘|ğ‘Œ[|ğ‘”|2] â‰¤2 |Î“ğ‘|1/2
|Î“ğ‘|ğ‘Œ|1/2 exp
(ï¸‚
1
ğ›½âˆ’2 â€–ğœ‡ğ‘|ğ‘Œ(ğ‘Œ)â€–2
Î“âˆ’1
ğ‘
)ï¸‚
Eğœˆğ‘[|ğ‘”|ğ›½]2/ğ›½
(D.10)
where we used the fact that |Ìƒï¸€Î“ğ‘|ğ‘Œ| â‰¥|Î“ğ‘|ğ‘Œ| since Ìƒï¸€Î“ğ‘|ğ‘Œâª°Î“ğ‘|ğ‘Œ(see Lemma D.2).
Thus, (4.14) follows from simple algebra.
â–¡
Lemma D.4. Let ğ‘€:= ğ´(ğ¼âˆ’ğµğµâŠ¤)ğ´âŠ¤â‰»0 for a pair of compatible matrices ğ´, ğµ,
217

and let ğ‘ƒbe the orthogonal projector onto the range of ğ´âŠ¤. Then ğ¶:= ğ¼âˆ’ğ‘ƒğµğµâŠ¤ğ‘ƒâ‰»
0 and ğ‘€= ğ´ğ¶ğ´âŠ¤.
Proof. Since ğ‘€â‰»0, ğ´âŠ¤must be full column rank.
Thus, by definition, ğ‘ƒ=
ğ´âŠ¤(ğ´ğ´âŠ¤)âˆ’1ğ´and ğ‘ƒğ´âŠ¤= ğ¼= ğ´ğ‘ƒ. Hence ğ‘€= ğ´ğ¶ğ´âŠ¤. Now let ğ‘„:= ğ¼âˆ’ğ‘ƒand
notice that ğ‘ƒğ‘„= 0 and ğ¶ğ‘„= ğ‘„. Thus, for ğ‘§Ì¸= 0, âŸ¨ğ¶ğ‘§, ğ‘§âŸ©= âŸ¨ğ¶ğ‘ƒğ‘§, ğ‘ƒğ‘§âŸ©+âŸ¨ğ‘„ğ‘§, ğ‘„ğ‘§âŸ©=
âŸ¨ğ¶ğ‘ƒğ‘§, ğ‘ƒğ‘§âŸ©+â€–ğ‘„ğ‘§â€–2. In particular, âŸ¨ğ¶ğ‘ƒğ‘§, ğ‘ƒğ‘§âŸ©= âŸ¨ğ‘ƒğ¶ğ‘ƒğ‘§, ğ‘§âŸ©= âŸ¨ğ‘€(ğ´ğ´âŠ¤)âˆ’1ğ´ğ‘§, (ğ´ğ´âŠ¤)âˆ’1ğ´ğ‘§âŸ©â‰¥
0 and it is zero only if ğ‘ƒğ‘§= 0, in which case ğ‘„ğ‘§Ì¸= 0 and â€–ğ‘„ğ‘§â€– > 0. Thus ğ¶â‰»0.
Proof of Lemma 4.4. We first need to show the equivalence Ìƒï¸€Î“ğ‘|ğ‘Œâ‰¡ğ’ªÌ‚ï¸€Î“*
pos ğ’ªâŠ¤,
where Ì‚ï¸€Î“*
pos is defined in (4.15).
Notice that ğ’ªÌ‚ï¸€Î“*
pos ğ’ªâŠ¤= Î“ğ‘âˆ’âˆ‘ï¸€
ğ‘–ğœ†ğ‘–ğ‘£ğ‘–ğ‘£âŠ¤
ğ‘–
with
ğ‘£ğ‘–:= ğ’ªğ‘†pr Î  ğ‘†âŠ¤
pr ğºâŠ¤ğ‘ğ‘–= ğ’ªÎ“pr ğºâŠ¤ğ‘ğ‘–since Î  is a projector onto the rowspace of ğ’ªğ‘†pr.
The desired equivalence follows by comparison with (4.9). In particular, it follows
that ğ’ªÌ‚ï¸€Î“*
pos ğ’ªâŠ¤â‰»0. Thus, in order to show that Ì‚ï¸€Î“*
pos is in the feasible set of (4.16) it
remains to prove that Ì‚ï¸€Î“*
pos âˆˆâ„³ğ‘Ÿ. Clearly, it just suffices to show that Ì‚ï¸€Î“*
pos â‰»0. Notice
that Ìƒï¸€Î“ğ‘|ğ‘Œ= ğ’ªğ‘†pr(ğ¼âˆ’Î ğµğµâŠ¤Î )ğ‘†âŠ¤
pr ğ’ªâŠ¤â‰»0 where ğµğµâŠ¤:= ğ‘†âŠ¤
pr ğºâŠ¤âˆ‘ï¸€ğ‘Ÿ
ğ‘–=1 ğ‘ğ‘–ğ‘âŠ¤
ğ‘–ğºğ‘†pr.
Thus, we can apply Lemma D.4 with ğ‘€:= Ìƒï¸€Î“ğ‘|ğ‘Œ, ğ´:= ğ’ªğ‘†pr, ğ¶:= ğ¼âˆ’Î ğµğµâŠ¤Î ,
and get ğ¶â‰»0. In particular, this shows that Ì‚ï¸€Î“*
pos = ğ‘†pr ğ¶ğ‘†âŠ¤
pr â‰»0 and thus Ì‚ï¸€Î“*
pos is
in the feasible set of (4.15). Optimality of Ì‚ï¸€Î“*
pos then follows almost immediately. By
Theorem 4.1:
ğ‘‘â„›(Î“ğ‘|ğ‘Œ, ğ’ªÌ‚ï¸€Î“*
pos ğ’ªâŠ¤) = ğ‘‘â„›(Î“ğ‘|ğ‘Œ, Ìƒï¸€Î“ğ‘|ğ‘Œ) â‰¤ğ‘‘â„›(Î“ğ‘|ğ‘Œ, Ìƒï¸€Î“)
âˆ€Ìƒï¸€Î“ âˆˆâ„³ğ‘
ğ‘Ÿ.
(D.11)
In particular, we can consider Ìƒï¸€Î“ of the form Ìƒï¸€Î“ = ğ’ªÎ“ ğ’ªâŠ¤for Î“ âˆˆâ„³ğ‘Ÿ. Notice that
Ìƒï¸€Î“ â‰»0 since ğ’ªis assumed to be full row-rank. This shows optimality of Ì‚ï¸€Î“*
pos according
to (4.16).
â–¡
Proof of Lemma 4.5. We first provide an explicit square root factorization of
Ì‚ï¸€Î“*
pos, defined in (4.15) (Lemma 4.4), as Ì‚ï¸€Î“*
pos = Ì‚ï¸€ğ‘†*
pos (Ì‚ï¸€ğ‘†*
pos)âŠ¤for some matrix Ì‚ï¸€ğ‘†*
pos. We
claim that
Ì‚ï¸€ğ‘†*
pos = ğ‘†pr
(ï¸ƒ
ğ‘Ÿ
âˆ‘ï¸
ğ‘–=1
(
âˆšï¸€
1 âˆ’ğœ†ğ‘–âˆ’1) Â¯ğ‘ğ‘–Â¯ğ‘âŠ¤
ğ‘–+ ğ¼
)ï¸ƒ
(D.12)
218

where ğ¼is the identity matrix. and the (Â¯ğ‘ğ‘–) are defined in (4.18). First of all, notice
that (D.12) is well defined since 1 > ğœ†ğ‘–> 0 for all ğ‘–= 1, . . . , ğ‘Ÿ(see the proof of
Theorem 4.1). One can verify that (D.12) is indeed a valid square root of Ì‚ï¸€Î“*
pos. They
key observation is that Â¯ğ‘ğ‘–= ğ‘†âˆ’1
pr Ìƒï¸€ğ‘ğ‘–and that the vectors (Ìƒï¸€ğ‘ğ‘–) are Î“âˆ’1
pr -orthogonal, i.e.,
Ìƒï¸€ğ‘âŠ¤
ğ‘–Î“âˆ’1
pr Ìƒï¸€ğ‘ğ‘—= ğ›¿ğ‘–ğ‘—. To see this, consider the following identities:
Ìƒï¸€ğ‘âŠ¤
ğ‘–Î“âˆ’1
pr Ìƒï¸€ğ‘ğ‘—= ğ‘âŠ¤
ğ‘–ğºğ‘†pr Î  ğ‘†âŠ¤
pr Î“âˆ’1
pr ğ‘†pr Î  ğ‘†âŠ¤
pr ğºâŠ¤ğ‘ğ‘—= ğ‘âŠ¤
ğ‘–ğºğ‘†pr Î  ğ‘†âŠ¤
pr ğºâŠ¤ğ‘ğ‘—.
(D.13)
Since Î  = ğ‘†âŠ¤
pr ğ’ªâŠ¤Î“âˆ’1
ğ‘ğ’ªğ‘†pr, it must be that Ìƒï¸€ğ‘âŠ¤
ğ‘–Î“âˆ’1
pr Ìƒï¸€ğ‘ğ‘—= ğ‘âŠ¤
ğ‘–ğºÎ“pr ğ’ªâŠ¤Î“âˆ’1
ğ‘ğ’ªÎ“pr ğºâŠ¤ğ‘ğ‘—=
ğ›¿ğ‘–ğ‘—, for the (ğ‘ğ‘–) are the generalized eigenvectors of the pencil (4.8), properly normal-
ized. Now notice that Ìƒï¸€ğ‘†ğ‘|ğ‘Œ= ğ’ªÌ‚ï¸€ğ‘†*
pos and thus (4.15) implies that Ìƒï¸€ğ‘†ğ‘|ğ‘ŒÌƒï¸€ğ‘†âŠ¤
ğ‘|ğ‘Œ= Ìƒï¸€Î“ğ‘|ğ‘Œ.
â–¡
Proof of Theorem 4.2. By applying [261, Theorem 4.1] to the linear Gaus-
sian model defined in Lemma 4.1, we know that a minimizer of (4.22) is given by:
ğ´* = âˆ‘ï¸€ğ‘Ÿ
ğ‘–=1 ğœ‚ğ‘–(1 + ğœ‚2
ğ‘–)âˆ’1 Ì‚ï¸€ğ‘ğ‘–Ì‚ï¸€ğ‘£âŠ¤
ğ‘–, where (ğœ‚2
ğ‘–, Ì‚ï¸€ğ‘ğ‘–) are eigenpairs of (ğ»ğ‘, Î“âˆ’1
ğ‘) with normal-
ization Ì‚ï¸€ğ‘âŠ¤
ğ‘–Î“âˆ’1
ğ‘Ì‚ï¸€ğ‘ğ‘–= 1, whereas (Ì‚ï¸€ğ‘£ğ‘–) are eigenvectors of (ğºğ’ªâ€  Î“ğ‘ğ’ªâŠ¤
â€  ğºâŠ¤, Î“Î”) with nor-
malization Ì‚ï¸€ğ‘£âŠ¤
ğ‘–Î“Î” Ì‚ï¸€ğ‘£ğ‘–= 1. Moreover, [261, Theorem 4.1] tells us that the Bayes risk as-
sociated with the minimizer ğ´* can be written as: E[ â€– ğ´* ğ‘Œâˆ’ğ‘â€–2
Î“âˆ’1
ğ‘|ğ‘Œ] = âˆ‘ï¸€
ğ‘–>ğ‘Ÿğœ‚2
ğ‘–+ğ‘›,
where ğ‘›is the dimension of the parameter space. The fact that the vectors (Ì‚ï¸€ğ‘ğ‘–) can
be written as Ì‚ï¸€ğ‘ğ‘–= ğ’ªÎ“pr ğºâŠ¤ğ‘ğ‘–for ğœ‚2
ğ‘–> 0 was proved in Theorem 4.1.
Further-
more, in the proof of Theorem 4.1 we showed that ğœ‚2
ğ‘–= ğœ†ğ‘–(1 âˆ’ğœ†ğ‘–)âˆ’1. Using the
latter expression we can rewrite the minimizer as ğ´* = âˆ‘ï¸€ğ‘Ÿ
ğ‘–=1
âˆšï¸€
ğœ†ğ‘–(1 âˆ’ğœ†ğ‘–) Ì‚ï¸€ğ‘ğ‘–Ì‚ï¸€ğ‘£âŠ¤
ğ‘–. If
(Ì‚ï¸€ğ‘£ğ‘–) are eigenvectors of (ğºğ’ªâ€  Î“ğ‘ğ’ªâŠ¤
â€  ğºâŠ¤, Î“Î”), then they must also be eigenvectors of
(ğºÎ“pr ğ’ªâŠ¤Î“âˆ’1
ğ‘ğ’ªÎ“pr ğºâŠ¤, Î“ğ‘Œ). In particular, we can set Ì‚ï¸€ğ‘£ğ‘–= ğ›¼ğ‘ğ‘–for some real ğ›¼> 0.
Given the normalizations ğ‘âŠ¤
ğ‘–ğºÎ“pr ğ’ªâŠ¤Î“âˆ’1
ğ‘ğ’ªÎ“pr ğºâŠ¤ğ‘ğ‘–= 1 and Ì‚ï¸€ğ‘£âŠ¤
ğ‘–Î“Î” Ì‚ï¸€ğ‘£ğ‘–= 1, it must
be ğ›¼= ğœ†1/2
ğ‘–
(1 âˆ’ğœ†ğ‘–)âˆ’1/2. Simple algebra then leads to (4.23).
â–¡
219

220

Appendix E
Proofs for Chapter 5
Proof of Lemma 5.2.
The general solution of ğœ•2
ğ‘–,ğ‘—log ğœ‹= 0 on Rğ‘›is given by
log ğœ‹(ğ‘§) = ğ‘”(ğ‘§1:ğ‘–âˆ’1, ğ‘§ğ‘–+1:ğ‘›) + â„(ğ‘§1:ğ‘—âˆ’1, ğ‘§ğ‘—+1:ğ‘›) for some functions ğ‘”, â„: Rğ‘›âˆ’1 â†’R.
Hence ğ‘ğ‘–âŠ¥âŠ¥ğ‘ğ‘—| ğ‘ğ’±âˆ–(ğ‘–,ğ‘—) [161]. Conversely, if ğ‘ğ‘–âŠ¥âŠ¥ğ‘ğ‘—| ğ‘ğ’±âˆ–(ğ‘–,ğ‘—), then ğœ‹must factor as
ğœ‹= ğœ‹ğ‘ğ‘–|ğ‘ğ’±âˆ–(ğ‘–,ğ‘—)ğœ‹ğ‘ğ‘—|ğ‘ğ’±âˆ–(ğ‘–,ğ‘—)ğœ‹ğ‘ğ’±âˆ–(ğ‘–,ğ‘—),
(E.1)
so that ğœ•2
ğ‘–,ğ‘—log ğœ‹= 0 on Rğ‘›.
â–¡
Proof of Theorem 5.1.
We begin with Part 1 of the theorem.
Let ğœ‚, ğœ‹be a
pair of strictly positive densities for ğœˆğœ‚and ğœˆğœ‹, respectively (these positive densities
exist since the measures are fully supported). Now consider a version of the KR
rearrangement, ğ‘†, that pushes forward ğœˆğœ‹to ğœˆğœ‚as given by Definition B.2 for the
pair ğœ‚, ğœ‹(Appendix B). By definition, and for all ğ‘§1:ğ‘˜âˆ’1 âˆˆRğ‘˜âˆ’1, the map ğœ‰â†¦â†’
ğ‘†ğ‘˜(ğ‘§1:ğ‘˜âˆ’1, ğœ‰) is the monotone increasing rearrangement that pushes forward ğœ‰â†¦â†’
ğœ‹ğ‘ğ‘˜|ğ‘1:ğ‘˜âˆ’1(ğœ‰|ğ‘§1:ğ‘˜âˆ’1) to the marginal ğœ‚ğ‘‹ğ‘˜(recall that ğœˆğœ‚is a tensor product measure).
Moreover, it follows easily from [161, Proposition 3.17], that each marginal ğœ‹ğ‘1:ğ‘˜â€”or
better yet, the corresponding measureâ€”is globally Markov with respect to ğ’¢ğ‘˜, and
that ğœ‹ğ‘1:ğ‘˜(ğ‘§1:ğ‘˜) ğœ‹ğ’(ğ‘§ğ’) = ğœ‹ğ‘ğ‘˜,ğ‘ğ’(ğ‘§ğ‘˜, ğ‘§ğ’) ğœ‹ğ‘1:ğ‘˜âˆ’1(ğ‘§1:ğ‘˜âˆ’1), where ğ’:= Nb(ğ‘˜, ğ’¢ğ‘˜), possibly
empty. Thus, the conditional ğœ‹ğ‘ğ‘˜|ğ‘1:ğ‘˜âˆ’1(ğ‘§ğ‘˜|ğ‘§1:ğ‘˜âˆ’1) is constant along any input ğ‘§ğ‘—with
ğ‘—/âˆˆNb(ğ‘˜, ğ’¢ğ‘˜). For any such ğ‘—, ğ‘†ğ‘˜must be constant along its ğ‘—th input, so that
(ğ‘—, ğ‘˜) âˆˆÌ‚ï¸€Iğ‘†.
221

Part 2 of the theorem follows similarly. Consider the KR rearrangement, ğ‘‡, that
pushes forward ğœˆğœ‚to ğœˆğœ‹as given by Definition B.2. For all ğ‘¥1:ğ‘˜âˆ’1 âˆˆRğ‘˜âˆ’1, the map
ğœ‰â†¦â†’ğ‘‡ğ‘˜(ğ‘¥1:ğ‘˜âˆ’1, ğœ‰) is the monotone increasing rearrangement that pushes forward ğœ‚ğ‘‹ğ‘˜
to
ğœ‰â†¦â†’ğœ‹ğ‘ğ‘˜|ğ‘1:ğ‘˜âˆ’1(ğœ‰|ğ‘‡1(ğ‘¥1), . . . , ğ‘‡ğ‘˜âˆ’1(ğ‘¥1:ğ‘˜âˆ’1)).
(E.2)
We already know that ğœ‹ğ‘ğ‘˜|ğ‘1:ğ‘˜âˆ’1(ğ‘§ğ‘˜|ğ‘§1:ğ‘˜âˆ’1) can only depend (nontrivially) on ğ‘§ğ‘˜and
on ğ‘§ğ‘—for ğ‘—âˆˆNb(ğ‘˜, ğ’¢ğ‘˜). Hence, if none of the components ğ‘‡ğ‘–, with ğ‘–âˆˆNb(ğ‘˜, ğ’¢ğ‘˜),
depends on the ğ‘—th input, then ğ‘‡ğ‘˜is constant along its ğ‘—th input as well, so that
(ğ‘—, ğ‘˜) âˆˆÌ‚ï¸€Iğ‘‡.
For Part 3, let (ğ‘—, ğ‘˜) âˆˆÌ‚ï¸€Iğ‘‡. Then, by definition, (ğ‘—, ğ‘–) âˆˆÌ‚ï¸€Iğ‘‡for all ğ‘–âˆˆNb(ğ‘˜, ğ’¢ğ‘˜),
which also implies that ğ‘—/âˆˆNb(ğ‘˜, ğ’¢ğ‘˜) since ğ‘—Ì¸= ğ‘–for all (ğ‘—, ğ‘–) âˆˆÌ‚ï¸€Iğ‘‡. Hence (ğ‘—, ğ‘˜) âˆˆÌ‚ï¸€Iğ‘†
and this shows the inclusion Ì‚ï¸€Iğ‘‡âŠ‚Ì‚ï¸€Iğ‘†.
These arguments show that there exists at least a version of the KR rearrangement
that is exactly at least as sparse as predicted by the theorem.
â–¡
The following lemma specializes the results of Theorem 5.1[Part 2] to the case of
I-maps ğ’¢with a disconnected component, and will be useful in the proofs of Section
5.5.
Lemma E.1. Let ğ‘‹âˆ¼ğœˆğœ‚, ğ‘âˆ¼ğœˆğœ‹with ğœˆğœ‚, ğœˆğœ‹âˆˆM+(Rğ‘›) and ğœˆğœ‚tensor product
measure, and let ğœbe any permutation of Nğ‘›. Moreover, assume that ğœˆğœ‹is globally
Markov with respect to ğ’¢= (ğ’±, â„°), and assume that there exists a nonempty set ğ’œâŠ‚
ğ’±â‰ƒNğ‘›such that ğ‘ğ’œâŠ¥âŠ¥ğ‘ğ’±âˆ–ğ’œand ğ‘ğ’œ= ğ‘‹ğ’œin distribution. Then the ğœ-generalized
KR rearrangement ğ‘‡given by Definition B.3 (for a pair ğœ‚, ğœ‹of nonvanishing densities
for ğœˆğœ‚and ğœˆğœ‹, respectively) is low-dimensional with respect to ğ’œ, i.e.,
1. ğ‘‡ğ‘˜(ğ‘¥) = ğ‘¥ğ‘˜for ğ‘˜âˆˆğ’œ
2. ğœ•ğ‘—ğ‘‡ğ‘˜= 0 for ğ‘—âˆˆğ’œand ğ‘˜âˆˆğ’±âˆ–ğ’œ.
Proof. It suffices to prove the lemma for a lower triangular KR rearrangement; the
result for an arbitrary ğœthen follows trivially. If ğ’œ= ğ’±, then ğ‘‡is simply the identity
map. Thus we assume that ğ’±âˆ–ğ’œis nonempty.
222

We begin with Part 1 of the lemma and use the results of Theorem 5.1[Part 2] to
characterize the sparsity of the rearrangement. Let ğ‘˜âˆˆğ’œand notice that Nb(ğ‘˜, ğ’¢ğ‘˜) =
âˆ…, where ğ’¢ğ‘˜is the marginal graph defined in Theorem 5.1. Thus (ğ‘—, ğ‘˜) âˆˆÌ‚ï¸€Iğ‘‡âŠ‚Iğ‘‡for
all ğ‘—= 1, . . . , ğ‘˜âˆ’1, so that ğ‘‡ğ‘˜(ğ‘¥) = ğ‘¥ğ‘˜for all ğ‘˜âˆˆğ’œ.
Now let us focus on Part 2 and prove that (ğ‘—, ğ‘˜) âˆˆÌ‚ï¸€Iğ‘‡for all ğ‘—âˆˆğ’œand ğ‘˜âˆˆğ’±âˆ–ğ’œ.
We proceed by contradiction. Assume that there exists some pair (ğ‘—, ğ‘˜) âˆˆğ’œÃ—(ğ’±âˆ–ğ’œ)
such that (ğ‘—, ğ‘˜) /âˆˆÌ‚ï¸€Iğ‘‡. In particular, let ğ’¦be the set of ğ‘˜âˆˆğ’±âˆ–ğ’œfor which there
exists at least a ğ‘—âˆˆğ’œsuch that (ğ‘—, ğ‘˜) /âˆˆÌ‚ï¸€Iğ‘‡. Clearly ğ’¦is nonempty and finite. Let
ğ‘ be the minimum integer in ğ’¦, and let ğ‘—âˆˆğ’œbe a corresponding index for which
(ğ‘—, ğ‘ ) /âˆˆÌ‚ï¸€Iğ‘‡. In this case, by Theorem 5.1[Part 2], there must exist an ğ‘–âˆˆNb(ğ‘ , ğ’¢ğ‘ )
such that (ğ‘—, ğ‘–) /âˆˆÌ‚ï¸€Iğ‘‡. Now there are two cases: either ğ‘–âˆˆğ’œ(for which we reach a
contradiction by part 1 of the lemma) or ğ‘–âˆˆğ’±âˆ–ğ’œ. In the latter case, we also reach a
contradiction since ğ‘–< ğ‘ and ğ‘ was defined as the smallest index for which (ğ‘—, ğ‘ ) /âˆˆÌ‚ï¸€Iğ‘‡
for some ğ‘—âˆˆğ’œ.
Proof of Theorem 5.2. For notational convenience, we drop the subscript and
superscript ğ‘–from ğœˆğ‘–, ğœ‹ğ‘–, ğ‘ğ‘–, and ğ’¢ğ‘–. Consider a factorization of ğœ‹of the form
ğœ‹(ğ‘§) = 1
c ğœ“ğ’œâˆªğ’®(ğ‘§ğ’œâˆªğ’®) ğœ“ğ’®âˆªâ„¬(ğ‘§ğ’®âˆªâ„¬),
(E.3)
where ğœ“ğ’œâˆªğ’®is strictly positive and integrable, with c =
âˆ«ï¸€
ğœ“ğ’œâˆªğ’®< âˆ. A factorization
like (E.3) always exist since ğœˆfactorizes according to ğ’¢â€”thus ğ’¢is an I-map for
ğœˆâ€”and since (ğ’œ, ğ’®, â„¬) is a proper decomposition of ğ’¢. For instance, one can set
ğœ“ğ’œâˆªğ’®= ğœ‹ğ‘ğ’œâˆªğ’®, c = 1, and ğœ“ğ’®âˆªâ„¬= ğœ‹ğ‘â„¬|ğ‘ğ’®since ğ‘ğ’œâŠ¥âŠ¥ğ‘â„¬|ğ‘ğ’®and since ğœ‹is a
nonvanishing density of ğœˆ. However, this is not the only possibility. See Section 5.6
for important examples where it is not convenient to assume that ğœ“ğ’œâˆªğ’®corresponds
to a marginal of ğœ‹. This proves Part 1 of the theorem.
By [161, Proposition 3.16], we can rewrite ğœ“ğ’®âˆªâ„¬as:
ğœ“ğ’®âˆªâ„¬(ğ‘§ğ’®âˆªâ„¬) =
âˆï¸
ğ’âˆˆğ’ğ’®âˆªâ„¬
ğœ“ğ’(ğ‘§ğ’)
(E.4)
223

for some nonvanishing functions (ğœ“ğ’), where ğ’ğ’®âˆªâ„¬denotes the set of maximal cliques
of the subgraph ğ’¢ğ’®âˆªâ„¬. Since ğ’®is a fully connected separator set (possibly empty) for
ğ’œand â„¬, the maximal cliques of ğ’¢ğ’®âˆªâ„¬are precisely the maximal cliques of ğ’¢that
are a subset of ğ’®âˆªâ„¬. We are going to use (E.4) shortly.
Define Ìƒï¸€ğœ‹: Rğ‘›â†’R as Ìƒï¸€ğœ‹(ğ‘§) = ğœ“ğ’œâˆªğ’®(ğ‘§ğ’œâˆªğ’®) ğœ‚ğ‘‹â„¬(ğ‘§â„¬)/c, and notice that Ìƒï¸€ğœ‹is a
nonvanishing probability density. Denote the corresponding measure by Ìƒï¸€ğœˆâˆˆM+(Rğ‘›).
For an arbitrary permutation ğœof Nğ‘›that satisfies (5.21), let ğ¿ğ‘–be the ğœ-generalized
KR rearrangement that pushes forward ğœˆğœ‚to Ìƒï¸€ğœˆas given by Definition B.3 in Appendix
B. By Lemma E.1, ğ¿ğ‘–is low-dimensional with respect to â„¬(Part 2a of the theorem).
To see this, let Ìƒï¸€ğ‘âˆ¼Ìƒï¸€ğœˆ, and notice that Ìƒï¸€ğ‘â„¬âŠ¥âŠ¥Ìƒï¸€ğ‘ğ’œâˆªğ’®and Ìƒï¸€ğ‘â„¬= ğ‘‹â„¬in distribution.
By Lemma B.1, we can write a density of the pullback measure ğ¿â™¯
ğ‘–ğœˆas:
ğ¿â™¯
ğ‘–ğœ‹
=
ğœ‹âˆ˜ğ¿ğ‘–| det âˆ‡ğ¿ğ‘–|
(E.5)
=
(ï¸
ğ¿â™¯
ğ‘–Ìƒï¸€ğœ‹
)ï¸âˆï¸€
ğ’âˆˆğ’ğ’®âˆªâ„¬ğœ“ğ’âˆ˜ğ¿ğ’
ğ‘–
ğœ‚ğ‘‹â„¬
=
ğœ‚ğ‘‹ğ’œâˆªğ’®
âˆï¸
ğ’âˆˆğ’ğ’®âˆªâ„¬
ğœ“ğ’âˆ˜ğ¿ğ’
ğ‘–,
where we used the identity ğœ‹= Ìƒï¸€ğœ‹ğœ“ğ’®âˆªâ„¬/ğœ‚ğ‘‹â„¬together with (E.4) and the fact that
ğ¿ğ‘˜
ğ‘–(ğ‘¥) = ğ‘¥ğ‘˜for ğ‘˜âˆˆâ„¬(Part 2a), and where, for any ğ’= {ğ‘1, . . . , ğ‘â„“} âˆˆğ’ğ’®âˆªâ„¬with
ğœ“ğ’(ğ‘§ğ’) = ğœ“ğ’(ğ‘§ğ‘1, . . . , ğ‘§ğ‘â„“), ğ¿ğ’
ğ‘–is a map Rğ‘›â†’Râ„“given by ğ‘¥â†¦â†’(ğ¿ğ‘1
ğ‘–(ğ‘¥), . . . , ğ¿ğ‘â„“
ğ‘–(ğ‘¥)).
If ğ‘â€² âˆ¼ğ¿â™¯
ğ‘–ğœˆ, then (E.5) shows that ğ‘â€²
ğ’œâŠ¥âŠ¥ğ‘â€²
ğ’®âˆªâ„¬and that ğ‘â€²
ğ’œ= ğ‘‹ğ’œin distribution
(Part 2c of the theorem). Moreover, from the factorization in (E.5), we can easily
construct a graph for which ğ¿â™¯
ğ‘–ğœˆfactorizes: it suffices to consider the scope of the
factors (ğœ“ğ’âˆ˜ğ¿ğ’
ğ‘–), i.e., the indices of the input variables that each ğœ“ğ’âˆ˜ğ¿ğ’
ğ‘–can depend
on. Recall that for a ğœ-triangular map, the ğœ(ğ‘˜)th component can only depend on
the variables ğ‘¥ğœ(1), . . . , ğ‘¥ğœ(ğ‘˜). For each ğ’âˆˆğ’ğ’®âˆªâ„¬there are two possibilites: Either
ğ’âˆ©ğ’®= âˆ…, in which case the scope of ğœ“ğ’âˆ˜ğ¿ğ’
ğ‘–is simply ğ’since ğ¿ğ‘˜
ğ‘–(ğ‘¥) = ğ‘¥ğ‘˜for
ğ‘˜âˆˆâ„¬. Or ğ’âˆ©ğ’®is nonempty, in which case let ğ‘—ğ’be the maximum integer ğ‘—such
that ğœ(ğ‘—) âˆˆğ’âˆ©ğ’®, and notice that the scope of ğœ“ğ’âˆ˜ğ¿ğ’
ğ‘–is simply ğ’âˆª{ğœ(1), . . . , ğœ(ğ‘—ğ’)}.
Thus, we can modify ğ’¢to obtain an I-map for ğ¿â™¯
ğ‘–ğœˆas follows: (1) Remove any edge
224

that is incident to any node in ğ’œbecause of Part 2c. (2) For every maximal clique
ğ’in ğ’¢that is a subset of ğ’®âˆªâ„¬and that has nonempty intersection with ğ’®, turn
ğ’âˆª{ğœ(1), . . . , ğœ(ğ‘—ğ’)} into a clique. This proves Part 2d of the theorem.
Now let Rğ‘–be the set of maps Rğ‘›â†’Rğ‘›that are low-dimensional with respect
to ğ’œand that push forward ğœˆğœ‚to ğ¿â™¯
ğ‘–ğœˆ. Rğ‘–is nonempty. To see this, let ğ‘…be the
ğœ-generalized KR rearrangement that pushes forward ğœˆğœ‚to ğ¿â™¯
ğ‘–ğœˆ, for an arbitrary
permutation ğœ, as given by Definition B.3 (for the pair of nonvanishing densities ğœ‚
and ğ¿â™¯
ğ‘–ğœ‹). By Part 2c and Lemma E.1, ğ‘…is low-dimensional with respect to ğ’œ. Thus
ğ‘…âˆˆRğ‘–(Part 2b of the theorem).
Let Dğ‘–:= ğ¿ğ‘–âˆ˜Rğ‘–be the set of maps that can be written as ğ¿ğ‘–âˆ˜ğ‘…for some ğ‘…âˆˆRğ‘–.
By construction, each ğ‘‡âˆˆDğ‘–pushes forward ğœˆğœ‚to ğœˆ(part 2 of the theorem).
â–¡
In the following corollary every symbol should be interpreted as in Theorem 5.2.
Corollary E.1. Given the hypothesis of Theorem 5.2, assume that there exists ğ’œâŠ¥âŠ‚
ğ’œsuch that ğ‘ğ‘–
ğ’œâŠ¥âŠ¥âŠ¥ğ‘ğ‘–
ğ’±âˆ–ğ’œâŠ¥and ğ‘ğ‘–
ğ’œâŠ¥= ğ‘‹ğ’œâŠ¥in distribution.
Then ğ¿ğ‘–is low-
dimensional with respect to ğ’œâŠ¥âˆªâ„¬, while each ğ‘‡âˆˆDğ‘–is low-dimensional with
respect to ğ’œâŠ¥.
Proof. By Theorem 5.2[Part 2a], ğ¿ğ‘–is low-dimensional with respect to â„¬, while
Lemma E.1 shows that ğ¿ğ‘–is also low-dimensional with respect to ğ’œâŠ¥. Moreover,
notice that if ğ’œâŠ¥is nonempty, then for all ğ‘‡= ğ¿ğ‘–âˆ˜ğ‘…in Dğ‘–, we have ğ‘‡ğ‘˜(ğ‘¥) = ğ‘¥ğ‘˜
for ğ‘˜âˆˆğ’œâŠ¥since ğ¿ğ‘˜
ğ‘–(ğ‘¥) = ğ‘¥ğ‘˜and ğ‘…ğ‘˜(ğ‘¥) = ğ‘¥ğ‘˜for ğ‘˜âˆˆğ’œâŠ¥(Theorem 5.2[Parts
2b]). Additionally, ğœ•ğ‘—ğ‘‡ğ‘˜= 0 for ğ‘—âˆˆğ’œâŠ¥and ğ‘˜âˆˆğ’±âˆ–ğ’œâŠ¥. To see this, notice that
ğ‘‡ğ‘˜(ğ‘¥) = ğ¿ğ‘˜
ğ‘–(ğ‘…(ğ‘¥)) and that the following two facts hold: (1) The component ğ¿ğ‘˜
ğ‘–,
for ğ‘˜âˆˆğ’±âˆ–ğ’œâŠ¥, does not depend on input variables whose index is in ğ’œâŠ¥since ğ¿ğ‘–
is low-dimensional with respect to ğ’œâŠ¥; (2) The â„“th component of ğ‘…with â„“/âˆˆğ’œâŠ¥
also does not depend on ğ‘¥ğ’œâŠ¥since ğ‘…is low-dimensional with respect to ğ’œ(Theorem
5.2[Parts 2b]). Hence, ğ‘‡must be a low-dimensional map with respect to ğ’œâŠ¥.
Proof of Lemma 5.3. Let ğœˆğœ‚, ğœˆğ‘–, ğœ‹ğ‘–, ğ’¢ğ‘–, Dğ‘–, ğ¿ğ‘–, Rğ‘–, and ğ’¢ğ‘–+1 be defined as in Theo-
rem 5.2 for a proper decomposition (ğ’œğ‘–, ğ’®ğ‘–, â„¬ğ‘–) of ğ’¢ğ‘–, a permutation ğœğ‘–that satisfies
(5.21), and for any factorization (5.20) of ğœ‹ğ‘–.
225

We first want to prove that ğ’®ğ‘–âˆªâ„¬ğ‘–is fully connected in ğ’¢ğ‘–+1 if and only if the
decomposition (ğ’œğ‘–+1, ğ’®ğ‘–+1, â„¬ğ‘–+1) of Part 1 does not exist.
Let us start with one
direction. Assume that a decomposition like the one in Part 1 does not exist, despite
the possibility to add edges to ğ’¢ğ‘–+1 in ğ’±âˆ–ğ’œğ‘–. We want to show that in this case
ğ’®ğ‘–âˆªâ„¬ğ‘–must be a clique in ğ’¢ğ‘–+1. Since â„¬ğ‘–is nonempty, there are two possibilities:
either |ğ’®ğ‘–âˆªâ„¬ğ‘–| = 1 or |ğ’®ğ‘–âˆªâ„¬ğ‘–| > 1. If |ğ’®ğ‘–âˆªâ„¬ğ‘–| = 1, then ğ’®ğ‘–âˆªâ„¬ğ‘–consists of a single
node and thus it is a trivial clique. If |ğ’®ğ‘–âˆªâ„¬ğ‘–| > 1, then ğ’®ğ‘–âˆªâ„¬ğ‘–contains at least two
nodes. In this case, let us proceed by contradiction and assume that ğ’®ğ‘–âˆªâ„¬ğ‘–is not
fully connected in ğ’¢ğ‘–+1 = (ğ’±, â„°ğ‘–+1), i.e., there exist a pair of nodes ğ›¼, ğ›½âˆˆğ’®ğ‘–âˆªâ„¬ğ‘–such
that (ğ›¼, ğ›½) /âˆˆâ„°ğ‘–+1. Let ğ’œğ‘–+1 = ğ’œğ‘–âˆª{ğ›¼}, â„¬ğ‘–+1 = {ğ›½}, and ğ’®ğ‘–+1 = (ğ’±âˆ–ğ’œğ‘–+1) âˆ–â„¬ğ‘–+1.
Notice that (ğ’œğ‘–+1, ğ’®ğ‘–+1, â„¬ğ‘–+1) forms a partition of ğ’±, with nonempty ğ’œğ‘–+1, â„¬ğ‘–+1 and
with ğ’œğ‘–+1 strict superset of ğ’œğ‘–. Moreover ğ’®ğ‘–+1 must be a separator set for ğ’œğ‘–+1 and
â„¬ğ‘–+1 since (ğ›¼, ğ›½) /âˆˆâ„°ğ‘–+1 and ğ’œğ‘–is disconnected from ğ’®ğ‘–âˆªâ„¬ğ‘–in ğ’¢ğ‘–+1 (Theorem 5.2[Part
2d]). Now there are two cases: If ğ’®ğ‘–+1 = âˆ…, then (ğ’œğ‘–+1, ğ’®ğ‘–+1, â„¬ğ‘–+1) is a decomposition
that satisfies Part 1 of the lemma (contradiction). If ğ’®ğ‘–+1 Ì¸= âˆ…, then we can always
add enough edges to ğ’¢ğ‘–+1 in ğ’®ğ‘–âˆªâ„¬ğ‘–âŠƒğ’®ğ‘–+1 in order to make ğ’®ğ‘–+1 fully connected.
Also in this case, the resulting decomposition (ğ’œğ‘–+1, ğ’®ğ‘–+1, â„¬ğ‘–+1) satisfies Part 1 of the
lemma and thus leads to a contradiction.
Now the reverse direction. Assume that ğ’®ğ‘–âˆªâ„¬ğ‘–is a clique in ğ’¢ğ‘–+1. If |ğ’®ğ‘–âˆªâ„¬ğ‘–| = 1,
then the decomposition of Part 1 cannot exist since both ğ’œğ‘–+1 âˆ–ğ’œğ‘–and â„¬ğ‘–+1 should
be nonempty. Hence, let |ğ’®ğ‘–âˆªâ„¬ğ‘–| > 1 and proceed by contradiction. That is, let
(ğ’œğ‘–+1, ğ’®ğ‘–+1, â„¬ğ‘–+1) be a proper decomposition that satisfies Part 1 of the lemma. Notice
that this decomposition must have been achieved without adding any edge to ğ’¢ğ‘–+1 in
ğ’®ğ‘–âˆªâ„¬ğ‘–since this set is already fully connected. By hypothesis, there must exist ğ›¼, ğ›½
such that ğ›¼âˆˆğ’œğ‘–+1 âˆ–ğ’œğ‘–and ğ›½âˆˆâ„¬ğ‘–+1. However, both ğ›¼and ğ›½are also in ğ’®ğ‘–âˆªâ„¬ğ‘–,
and so they must be connected by an edge in ğ’¢ğ‘–+1. Hence, ğ’®ğ‘–+1 is not a separator set
for ğ’œğ‘–+1 and â„¬ğ‘–+1 (contradiction).
The latter result proves directly Part 2 of the lemma. Moreover, it shows that if
ğ’®ğ‘–âˆªâ„¬ğ‘–is not a clique in ğ’¢ğ‘–+1, then there exists a proper decomposition (ğ’œğ‘–+1, ğ’®ğ‘–+1, â„¬ğ‘–+1)
of ğ’¢ğ‘–+1, where ğ’œğ‘–+1 is a strict superset of ğ’œğ‘–, obtained, possibly, by adding edges to
226

ğ’¢ğ‘–+1 in order to turn ğ’®ğ‘–+1 into a clique. Note that even if we add edges to ğ’¢ğ‘–+1, ğ¿â™¯
ğ‘–ğœˆğ‘–
still factorizes according to the resulting graph, which is then an I-map for ğ¿â™¯
ğ‘–ğœˆğ‘–.
Moreover we can really only add edges in ğ’±âˆ–ğ’œğ‘–since ğ’œğ‘–must be a strict subset of
ğ’œğ‘–+1, and thus ğ’œğ‘–remains disconnected from ğ’®ğ‘–âˆªâ„¬ğ‘–in ğ’¢ğ‘–+1. Let Dğ‘–+1, ğ¿ğ‘–+1, Rğ‘–+1
be defined as in Theorem 5.2 for the pair of measures ğœˆğœ‚, ğœˆğ‘–+1 = ğ¿â™¯
ğ‘–ğœˆğ‘–, the decom-
position (ğ’œğ‘–+1, ğ’®ğ‘–+1, â„¬ğ‘–+1) of ğ’¢ğ‘–+1, a permutation ğœğ‘–+1 that satisfies (5.21), and for
any factorization (5.20) (note that ğ¿â™¯
ğ‘–ğœˆğ‘–âˆˆM+(Rğ‘›) by Theorem 5.2[Part 2b]). Fix
ğ‘‡âˆˆDğ‘–+1. By Theorem 5.2[Part 2], ğ‘‡pushes forward ğœˆğœ‚to ğœˆğ‘–+1 = ğ¿â™¯
ğ‘–ğœˆğ‘–. Moreover, if
ğ‘ğ‘–+1 âˆ¼ğ¿â™¯
ğ‘–ğœˆğ‘–, then by Theorem 5.2[Part 2c] we have ğ‘ğ‘–+1
ğ’œğ‘–âŠ¥âŠ¥ğ‘ğ‘–+1
ğ’®ğ‘–âˆªâ„¬ğ‘–and ğ‘ğ‘–+1
ğ’œğ‘–= ğ‘‹ğ’œğ‘–
in distribution. Then by Corollary E.1 it must also be that ğ‘‡is low-dimensional with
respect to ğ’œğ‘–. Thus ğ‘‡âˆˆRğ‘–, and this proves the inclusion Rğ‘–âŠƒDğ‘–+1.
Now fix any ğ‘‡âˆˆğ¿ğ‘–âˆ˜ğ¿ğ‘–+1 âˆ˜Rğ‘–+1 = ğ¿ğ‘–âˆ˜Dğ‘–+1. It must be that ğ‘‡= ğ¿ğ‘–âˆ˜ğ‘”for
some ğ‘”âˆˆDğ‘–+1 âŠ‚Rğ‘–, so that ğ‘‡âˆˆğ¿ğ‘–âˆ˜Rğ‘–, which shows the inclusion ğ¿ğ‘–âˆ˜Rğ‘–âŠƒ
ğ¿ğ‘–âˆ˜ğ¿ğ‘–+1 âˆ˜Rğ‘–+1 (Part 1a of the lemma). By Corollary E.1, we have that ğ¿ğ‘–+1 is low-
dimensional with respect to ğ’œğ‘–âˆªâ„¬ğ‘–+1, and so its effective dimension is bounded above
by |ğ’±âˆ–(ğ’œğ‘–âˆªâ„¬ğ‘–+1)| = |(ğ’œğ‘–+1 âˆ–ğ’œğ‘–)âˆªğ’®ğ‘–+1| (Part 1b). Finally, by Theorem 5.2[Part 2b],
each ğ‘…âˆˆRğ‘–+1 is low-dimensional with respect to ğ’œğ‘–+1, and so its effective dimension
is bounded by |ğ’±âˆ–ğ’œğ‘–+1| (Part 1c).
â–¡
Proof of Theorem 5.3. For the sake of clarity, we divide the proof in two parts:
First, we show that the maps (Mğ‘–)ğ‘–â‰¥0 are well-defined. Then, we prove the remaining
claims of the theorem.
The maps (Mğ‘–)ğ‘–â‰¥0 are well-defined as long as, for instance, we show that ğœ‹ğ‘–is a
probability density for all ğ‘–â‰¥0, and as long as there exist permutations (ğœğ‘–) that
guarantee the block upper triangular structure of (5.28). As for the permutations, it
suffices to consider ğœ= ğœ1 = ğœ2 = Â· Â· Â· with ğœ(N2ğ‘›) = {2ğ‘›, 2ğ‘›âˆ’1, . . . , 1}, i.e., upper
triangular maps. (If ğ‘›> 1, then there is some freedom in the choice of ğœ.) As for the
targets (ğœ‹ğ‘–), we now show that ğœ‹ğ‘–is a nonvanishing density and that the marginal
âˆ«ï¸€
ğœ‹ğ‘–(ğ‘§ğ‘–, ğ‘§ğ‘–+1) dğ‘§ğ‘–= ğœ‹ğ‘ğ‘–+1|ğ‘¦0:ğ‘–+1, for all ğ‘–â‰¥0, using an induction argument over ğ‘–. For
227

the base case (ğ‘–= 0), just notice that
c0 =
âˆ«ï¸
Ìƒï¸€ğœ‹0(ğ‘§0, ğ‘§1) dğ‘§0:1 = ğœ‹ğ‘Œ0,ğ‘Œ1(ğ‘¦0, ğ‘¦1) < âˆ,
(E.6)
so that ğœ‹0 = Ìƒï¸€ğœ‹0/c0 > 0 is a valid density. Moreover, we have the desired marginal,
i.e.,
âˆ«ï¸
ğœ‹0(ğ‘§0, ğ‘§1) dğ‘§0 =
âˆ«ï¸
ğœ‹ğ‘0,ğ‘1|ğ‘Œ0,ğ‘Œ1(ğ‘§0, ğ‘§1|ğ‘¦0, ğ‘¦1) dğ‘§0 = ğœ‹ğ‘1|ğ‘Œ0,ğ‘Œ1(ğ‘§1|ğ‘¦0, ğ‘¦1).
(E.7)
Now assume that ğœ‹ğ‘–is a nonvanishing density and that the marginal
âˆ«ï¸€
ğœ‹ğ‘–(ğ‘§ğ‘–, ğ‘§ğ‘–+1) dğ‘§ğ‘–=
ğœ‹ğ‘ğ‘–+1|ğ‘¦0:ğ‘–+1 for some ğ‘–> 0. The map Mğ‘–is then well-defined. In particular, by def-
inition of KR rearrangement, the submap M1
ğ‘–pushes forward ğœ‚ğ‘‹ğ‘–+1 to the marginal
âˆ«ï¸€
ğœ‹ğ‘–(ğ‘§ğ‘–, ğ‘§ğ‘–+1) dğ‘§ğ‘–. Moreover, by Lemma B.1, we have:
cğ‘–+1
=
âˆ«ï¸
ğœ‚ğ‘‹ğ‘–+1(ğ‘§ğ‘–+1) Ìƒï¸€ğœ‹ğ‘–+1(M1
ğ‘–(ğ‘§ğ‘–+1), ğ‘§ğ‘–+2) dğ‘§ğ‘–+1:ğ‘–+2
(E.8)
=
âˆ«ï¸
ğœ‹ğ‘ğ‘–+2,ğ‘Œğ‘–+2|ğ‘Œ0:ğ‘–+1(ğ‘§ğ‘–+2, ğ‘¦ğ‘–+2|ğ‘¦0:ğ‘–+1) dğ‘§ğ‘–+2
=
ğœ‹ğ‘Œğ‘–+2|ğ‘Œ0:ğ‘–+1(ğ‘¦ğ‘–+2|ğ‘¦0:ğ‘–+1) < âˆ,
where we used the change of variables ğ‘¥ğ‘–+1 = M1
ğ‘–(ğ‘§ğ‘–+1) and the fact that (M1
ğ‘–)â™¯ğœ‚ğ‘‹ğ‘–+1 =
ğœ‹ğ‘ğ‘–+1|ğ‘¦0:ğ‘–+1 (induction hypothesis). Thus ğœ‹ğ‘–+1 is a nonvanishing density and by (E.8)
we can easily verify that ğœ‹ğ‘–+1 has the desired marginal, i.e.,
âˆ«ï¸€
ğœ‹ğ‘–+1(ğ‘§ğ‘–+1, ğ‘§ğ‘–+2) dğ‘§ğ‘–+1 =
ğœ‹ğ‘ğ‘–+2|ğ‘¦0:ğ‘–+2. This argument completes the induction step and shows that not only the
maps (Mğ‘–)ğ‘–â‰¥0 are well-definedâ€”together with the maps (ğ‘‡ğ‘–)ğ‘–â‰¥0 in (5.30)â€”but also
that (M1
ğ‘–)â™¯ğœ‚ğ‘‹ğ‘–+1 = ğœ‹ğ‘ğ‘–+1|ğ‘¦0:ğ‘–+1 for all ğ‘–â‰¥0 (Part 1 of the theorem).
Now we move to Part 3 of the theorem and use another induction argument over
ğ‘˜â‰¥0. For the base case (ğ‘˜= 0), notice that T0 = ğ‘‡0 = M0, and that, by definition,
M0 pushes forward ğœ‚ğ‘‹0,ğ‘‹1 to ğœ‹0 = ğœ‹ğ‘0,ğ‘1|ğ‘¦0,ğ‘¦1.
Assume that Tğ‘˜pushes forward ğœ‚ğ‘‹0:ğ‘˜+1 to ğœ‹ğ‘0:ğ‘˜+1|ğ‘¦0:ğ‘˜+1 for some ğ‘˜> 0 (Tğ‘˜is well-
defined for all ğ‘˜since the maps (ğ‘‡ğ‘–)ğ‘–â‰¥0 in (5.30) are also well-defined), and notice
228

that
ğœ‹ğ‘0:ğ‘˜+2|ğ‘¦0:ğ‘˜+2 = ğœ‹ğ‘0:ğ‘˜+1|ğ‘¦0:ğ‘˜+1
ğœ‹ğ‘¦ğ‘˜+2|ğ‘ğ‘˜+2 ğœ‹ğ‘ğ‘˜+2|ğ‘ğ‘˜+1
ğœ‹ğ‘¦ğ‘˜+2|ğ‘¦0:ğ‘˜+1
= ğœ‹ğ‘0:ğ‘˜+1|ğ‘¦0:ğ‘˜+1
Ìƒï¸€ğœ‹ğ‘˜+1
cğ‘˜+1
,
(E.9)
where we used (E.8) and the definition of the collection (Ìƒï¸€ğœ‹ğ‘–). Let Tğ‘˜+1 = ğ‘‡0âˆ˜Â· Â· Â·âˆ˜ğ‘‡ğ‘˜+1
be defined as in Part 3 of the theorem, and observe that Tğ‘˜+1 = ğ´ğ‘˜+1 âˆ˜ğ‘‡ğ‘˜+1 with
ğ´ğ‘˜+1(ğ‘¥0:ğ‘˜+2) =
â¡
â£Tğ‘˜(ğ‘¥0:ğ‘˜+1)
ğ‘¥ğ‘˜+2
â¤
â¦,
ğ‘‡ğ‘˜+1(ğ‘¥0:ğ‘˜+2) =
â¡
â¢â¢â¢â¢â¢â¢â¢â¢â¢â£
ğ‘¥0
...
ğ‘¥ğ‘˜
M0
ğ‘˜+1(ğ‘¥ğ‘˜+1, ğ‘¥ğ‘˜+2)
M1
ğ‘˜+1(ğ‘¥ğ‘˜+2)
â¤
â¥â¥â¥â¥â¥â¥â¥â¥â¥â¦
.
(E.10)
Thus the following hold:
Tâ™¯
ğ‘˜+1 ğœ‹ğ‘0:ğ‘˜+2|ğ‘Œ0:ğ‘˜+2
=
ğ‘‡â™¯
ğ‘˜+1
(ï¸‚(ï¸
Tâ™¯
ğ‘˜ğœ‹ğ‘0:ğ‘˜+1|ğ‘¦0:ğ‘˜+1
)ï¸ğœ‹ğ‘˜+1
ğœ‚ğ‘‹ğ‘˜+1
)ï¸‚
(E.11)
=
ğ‘‡â™¯
ğ‘˜+1
(ï¸€
ğœ‚ğ‘‹0:ğ‘˜ğœ‹ğ‘˜+1)ï¸€
=
ğœ‚ğ‘‹0:ğ‘˜Mâ™¯
ğ‘˜+1 ğœ‹ğ‘˜+1 = ğœ‚ğ‘‹0:ğ‘˜+2,
where we used the fact that by Lemma B.1 (applied iteratively) it must be that
(ğ´ğ‘˜+1 âˆ˜ğ‘‡ğ‘˜+1)â™¯ğœŒ= ğ‘‡â™¯
ğ‘˜+1 ğ´â™¯
ğ‘˜+1ğœŒfor all densities ğœŒ. (Notice that ğ´ğ‘˜+1 is the composition
of functions which are trivial embeddings into the identity map of KR rearrangements
that couple pair of measures in M+(Rğ‘›Ã— Rğ‘›), and thus each map in the composition
satisfies the hypothesis of Lemma B.1.) In particular, (Tğ‘˜+1)â™¯ğœ‚ğ‘‹0:ğ‘˜+2 = ğœ‹ğ‘0:ğ‘˜+2|ğ‘¦0:ğ‘˜+2
(Part 3 of the theorem).
Now notice that each Tğ‘˜can also be written as
Tğ‘˜(ğ‘¥0:ğ‘˜+1) =
â¡
â£ğµğ‘˜(ğ‘¥0:ğ‘˜+1)
Mğ‘˜(ğ‘¥ğ‘˜, ğ‘¥ğ‘˜+1)
â¤
â¦
(E.12)
for a multivariate function ğµğ‘˜â€”whose particular form is not relevant to this argumentâ€”
229

and for a map, Mğ‘˜, defined in (5.29) as a function on Rğ‘›Ã— Rğ‘›. Since (Tğ‘˜)â™¯ğœ‚ğ‘‹0:ğ‘˜+1 =
ğœ‹ğ‘0:ğ‘˜+1|ğ‘¦0:ğ‘˜+1, the map Mğ‘˜must also push forward ğœ‚ğ‘‹ğ‘˜,ğ‘‹ğ‘˜+1 to the lag-1 smoothing
marginal ğœ‹ğ‘ğ‘˜,ğ‘ğ‘˜+1|ğ‘¦0:ğ‘˜+1. This proves Part 2 of the theorem.
For Part 4, just notice that
ğœ‹ğ‘Œ0:ğ‘˜+1(ğ‘¦0:ğ‘˜+1) = ğœ‹ğ‘Œ0,ğ‘Œ1(ğ‘¦0, ğ‘¦1)
ğ‘˜
âˆï¸
ğ‘–=1
ğœ‹ğ‘Œğ‘–+1|ğ‘Œ0:ğ‘–(ğ‘¦ğ‘–+1|ğ‘¦0:ğ‘–) =
ğ‘˜
âˆï¸
ğ‘–=0
cğ‘–,
(E.13)
where we used both (E.6) and (E.8).
â–¡
Proof of Lemma 5.4. First a remark about notation: we denote by ğ’©(ğ‘¥; ğœ‡, Î£) the
density (as a function of ğ‘¥) of a Gaussian with mean ğœ‡and covariance Î£.
Now let ğ‘˜> 0 and notice that ğœ‹ğ‘ğ‘˜+1|ğ‘ğ‘˜(ğ‘§ğ‘˜+1|ğ‘§ğ‘˜) = ğ’©(ğ‘§ğ‘˜+1; ğ¹ğ‘˜ğ‘§ğ‘˜, ğ‘„ğ‘˜), ğœ‹ğ‘Œğ‘˜+1|ğ‘ğ‘˜+1(ğ‘¦ğ‘˜+1|ğ‘§ğ‘˜+1) =
ğ’©(ğ‘¦ğ‘˜+1; ğ»ğ‘˜+1 ğ‘§ğ‘˜+1, ğ‘…ğ‘˜+1) and ğœ‚ğ‘‹ğ‘˜(ğ‘§ğ‘˜) = ğ’©(ğ‘§ğ‘˜; 0, I). By definition of the target ğœ‹ğ‘˜
in Theorem 5.3, we have:
ğœ‹ğ‘˜(ğ‘§ğ‘˜, ğ‘§ğ‘˜+1)
=
ğœ‚ğ‘‹ğ‘˜(ğ‘§ğ‘˜) ğœ‹ğ‘Œğ‘˜+1|ğ‘ğ‘˜+1(ğ‘¦ğ‘˜+1|ğ‘§ğ‘˜+1) ğœ‹ğ‘ğ‘˜+1|ğ‘ğ‘˜(ğ‘§ğ‘˜+1|M1
ğ‘˜âˆ’1(ğ‘§ğ‘˜))
=
ğ’©(ğ‘§ğ‘˜; 0, I) ğ’©(ğ‘¦ğ‘˜+1; ğ»ğ‘˜+1 ğ‘§ğ‘˜+1, ğ‘…ğ‘˜+1)
ğ’©(ğ‘§ğ‘˜+1; ğ¹ğ‘˜(ğ¶ğ‘˜âˆ’1 ğ‘§ğ‘˜+ ğ‘ğ‘˜âˆ’1), ğ‘„ğ‘˜)
âˆ
exp(âˆ’1
2ğ‘§âŠ¤ğ½ğ‘§+ ğ‘§âŠ¤â„),
where ğ‘§= (ğ‘§ğ‘˜, ğ‘§ğ‘˜+1) âˆˆR2ğ‘›, and where ğ½âˆˆR2ğ‘›Ã—2ğ‘›, â„âˆˆR2ğ‘›are defined as
ğ½=
â¡
â£ğ½11
ğ½12
ğ½âŠ¤
12
ğ½22
â¤
â¦,
â„=
â¡
â£â„1
â„2
â¤
â¦,
(E.14)
with:
â§
âª
âª
âª
âª
âª
âª
âª
âª
âª
âª
âª
âª
â¨
âª
âª
âª
âª
âª
âª
âª
âª
âª
âª
âª
âª
â©
ğ½11 = I + ğ¶âŠ¤
ğ‘˜âˆ’1 ğ¹âŠ¤
ğ‘˜ğ‘„âˆ’1
ğ‘˜ğ¹ğ‘˜ğ¶ğ‘˜âˆ’1
ğ½12 = âˆ’ğ¶âŠ¤
ğ‘˜âˆ’1 ğ¹âŠ¤
ğ‘˜ğ‘„âˆ’1
ğ‘˜
ğ½22 = ğ‘„âˆ’1
ğ‘˜
+ ğ»âŠ¤
ğ‘˜+1 ğ‘…âˆ’1
ğ‘˜+1 ğ»ğ‘˜+1
â„1 = ğ½12 ğ¹ğ‘˜ğ‘ğ‘˜âˆ’1
â„2 = ğ‘„âˆ’1
ğ‘˜ğ¹ğ‘˜ğ‘ğ‘˜âˆ’1 + ğ»âŠ¤
ğ‘˜+1 ğ‘…âˆ’1
ğ‘˜+1 ğ‘¦ğ‘˜+1.
(E.15)
230

In particular, we can rewrite ğœ‹ğ‘˜in information form [156] as ğœ‹ğ‘˜(ğ‘§) = ğ’©âˆ’1(ğ‘§; â„, ğ½).
Moreover we know by Theorem 5.3[Part 1], that the submap M1
ğ‘˜(ğ‘§ğ‘˜+1) = ğ¶ğ‘˜ğ‘§ğ‘˜+1+ğ‘ğ‘˜
pushes forward ğœ‚ğ‘‹ğ‘˜+1 to the filtering marginal ğœ‹ğ‘ğ‘˜+1|ğ‘¦0:ğ‘˜+1. Hence (ğ‘ğ‘˜, ğ¶ğ‘˜) should
be, respectively, the mean and a square root of the covariance of ğœ‹ğ‘ğ‘˜+1|ğ‘¦0:ğ‘˜+1â€”thus
the output of any square-root Kalman filter at time ğ‘˜+ 1. Now we just need to
determine the submap M0
ğ‘˜(ğ‘§ğ‘˜, ğ‘§ğ‘˜+1) = ğ´ğ‘˜ğ‘§ğ‘˜+ ğµğ‘˜ğ‘§ğ‘˜+1 + ğ‘ğ‘˜. Given that Mğ‘˜is a
block upper triangular function, the map ğ‘§ğ‘˜â†¦â†’M0
ğ‘˜(ğ‘§ğ‘˜, ğ‘§ğ‘˜+1) should push forward
ğœ‚ğ‘‹ğ‘˜to ğ‘§ğ‘˜â†¦â†’ğœ‹ğ‘˜
ğ‘ğ‘˜|ğ‘ğ‘˜+1(ğ‘§ğ‘˜|M1
ğ‘˜(ğ‘§ğ‘˜+1)). Notice that ğœ‹ğ‘˜
ğ‘ğ‘˜|ğ‘ğ‘˜+1(ğ‘§ğ‘˜|ğ‘§ğ‘˜+1) = ğ’©âˆ’1(ğ‘§ğ‘˜; â„1 âˆ’
ğ½12 ğ‘§ğ‘˜+1, ğ½11) = ğ’©(ğ‘§ğ‘˜; ğ½âˆ’1
11 (â„1 âˆ’ğ½12 ğ‘§ğ‘˜+1), ğ½âˆ’1
11 ).
Hence ğœ‹ğ‘˜
ğ‘ğ‘˜|ğ‘ğ‘˜+1(ğ‘§ğ‘˜|M1
ğ‘˜(ğ‘§ğ‘˜+1)) =
ğ’©(ğ‘§ğ‘˜; ğ½âˆ’1
11 ğ½12(ğ¹ğ‘˜ğ‘ğ‘˜âˆ’1 âˆ’ğ¶ğ‘˜ğ‘§ğ‘˜+1 âˆ’ğ‘ğ‘˜), ğ½âˆ’1
11 ), and so:
M0
ğ‘˜(ğ‘§ğ‘˜, ğ‘§ğ‘˜+1) = ğ½âˆ’1
11 ğ½12(ğ¹ğ‘˜ğ‘ğ‘˜âˆ’1 âˆ’ğ¶ğ‘˜ğ‘§ğ‘˜+1 âˆ’ğ‘ğ‘˜) + ğ½âˆ’1/2
11
ğ‘§ğ‘˜.
(E.16)
Simple algebra then leads to (5.35).
â–¡
Proof of Theorem 5.4.
We use a very similar argument to Theorem 5.3.
We
first show that the maps (Mğ‘–)ğ‘–â‰¥0 are well-defined. These maps are well-defined as
long as, for instance, we show that ğœ‹ğ‘–is a probability density for all ğ‘–â‰¥0, and as
long as there exist permutations (ğœğ‘–) that guarantee the generalized block triangular
structure of (5.37). As for the permutations, it suffices to consider ğœ= ğœ1 = ğœ2 = Â· Â· Â·
with ğœ(Nğ‘+2ğ‘›) = {1, . . . , ğ‘, ğ‘+ 2ğ‘›, ğ‘+ 2ğ‘›âˆ’1, . . . , ğ‘+ 1}. As for the targets (ğœ‹ğ‘–), we
now use a (complete) induction argument over ğ‘–to show that, for all ğ‘–â‰¥0, ğœ‹ğ‘–is a
nonvanishing density and
âˆ«ï¸€
ğœ‹ğ‘–(ğ‘§ğœƒ, ğ‘§ğ‘–, ğ‘§ğ‘–+1) dğ‘§ğ‘–= ğ´â™¯
ğ‘–ğœ‹Î˜,ğ‘ğ‘–+1|ğ‘¦0:ğ‘–+1(ğ‘§ğœƒ, ğ‘§ğ‘–+1) for a map
ğ´ğ‘–defined on Rğ‘Ã— Rğ‘›as
ğ´ğ‘–(ğ‘¥ğœƒ, ğ‘¥ğ‘–+1) =
â¡
â£TÎ˜
ğ‘–âˆ’1(ğ‘¥ğœƒ)
ğ‘¥ğ‘–+1
â¤
â¦,
(E.17)
with TÎ˜
ğ‘–âˆ’1(ğ‘¥ğœƒ) = ğ‘¥ğœƒif ğ‘–= 0.
For the base case (ğ‘–= 0), just notice that c0 = ğœ‹ğ‘Œ0,ğ‘Œ1(ğ‘¦0, ğ‘¦1) < âˆ, so that
231

ğœ‹0 = Ìƒï¸€ğœ‹0/c0 > 0 is a valid density. Moreover, we have the desired marginal, i.e.,
âˆ«ï¸
ğœ‹0(ğ‘§ğœƒ, ğ‘§0, ğ‘§1) dğ‘§0 = ğœ‹Î˜,ğ‘1|ğ‘Œ0,ğ‘Œ1(ğ‘§ğœƒ, ğ‘§1|ğ‘¦0, ğ‘¦1) = ğ´â™¯
0 ğœ‹Î˜,ğ‘1|ğ‘¦0,ğ‘¦1(ğ‘§ğœƒ, ğ‘§1),
(E.18)
since ğ´0 is the identity map on Rğ‘Ã— Rğ‘›.
Now assume that ğœ‹ğ‘—is a nonvanish-
ing density for all ğ‘—â‰¤ğ‘–(complete induction) with ğ‘–> 0, and that the marginal
âˆ«ï¸€
ğœ‹ğ‘–(ğ‘§ğœƒ, ğ‘§ğ‘–, ğ‘§ğ‘–+1) dğ‘§ğ‘–= ğ´â™¯
ğ‘–ğœ‹Î˜,ğ‘ğ‘–+1|ğ‘¦0:ğ‘–+1(ğ‘§ğœƒ, ğ‘§ğ‘–+1).
Under this hypothesis, the maps
(Mğ‘—)ğ‘—â‰¤ğ‘–are well-defined, and so are ğ´ğ‘–, ğ´ğ‘–+1 since TÎ˜
ğ‘–
= MÎ˜
0 âˆ˜Â· Â· Â· âˆ˜MÎ˜
ğ‘–. Before
checking the integrability of ğœ‹ğ‘–+1, notice that by definition of Mğ‘–(a KR rearrange-
ment), the map ğµğ‘–, given by
ğµğ‘–(ğ‘¥ğœƒ, ğ‘¥ğ‘–+1) =
â¡
â£MÎ˜
ğ‘–(ğ‘¥ğœƒ)
M1
ğ‘–(ğ‘¥ğœƒ, ğ‘¥ğ‘–+1)
â¤
â¦,
(E.19)
pushes forward ğœ‚ğ‘‹Î˜,ğ‘‹ğ‘–+1 to the marginal
âˆ«ï¸€
ğœ‹ğ‘–(ğ‘§ğœƒ, ğ‘§ğ‘–, ğ‘§ğ‘–+1) dğ‘§ğ‘–, which equals ğ´â™¯
ğ‘–ğœ‹Î˜,ğ‘ğ‘–+1|ğ‘¦0:ğ‘–+1
(inductive hypothesis), i.e., (ğµğ‘–)â™¯ğœ‚ğ‘‹Î˜,ğ‘‹ğ‘–+1 = ğ´â™¯
ğ‘–ğœ‹Î˜,ğ‘ğ‘–+1|ğ‘¦0:ğ‘–+1. In particular, it must
also be that (ğ´ğ‘–âˆ˜ğµğ‘–)â™¯ğœ‚ğ‘‹Î˜,ğ‘‹ğ‘–+1 = ğœ‹Î˜,ğ‘ğ‘–+1|ğ‘¦0:ğ‘–+1, where ğ´ğ‘–âˆ˜ğµğ‘–corresponds precisely
to the map Ìƒï¸
Mğ‘–defined in (5.39), so that (Ìƒï¸
Mğ‘–)â™¯ğœ‚ğ‘‹Î˜,ğ‘‹ğ‘–+1 = ğœ‹Î˜,ğ‘ğ‘–+1|ğ‘¦0:ğ‘–+1. Now we can
prove that cğ‘–+1 < âˆusing the following identities:
cğ‘–+1
=
âˆ«ï¸
ğœ‚ğ‘‹Î˜,ğ‘‹ğ‘–+1(ğ‘§ğœƒ, ğ‘§ğ‘–+1)
(E.20)
Ìƒï¸€ğœ‹ğ‘–+1(TÎ˜
ğ‘–(ğ‘§ğœƒ), M1
ğ‘–(ğ‘§ğœƒ, ğ‘§ğ‘–+1), ğ‘§ğ‘–+2) dğ‘§ğœƒdğ‘§ğ‘–+1:ğ‘–+2
=
âˆ«ï¸
(Ìƒï¸
Mğ‘–)â™¯ğœ‚ğ‘‹Î˜,ğ‘‹ğ‘–+1(ğ‘¥ğœƒ, ğ‘¥ğ‘–+1) ğœ‹ğ‘ğ‘–+2|ğ‘ğ‘–+1,Î˜(ğ‘§ğ‘–+2|ğ‘¥ğ‘–+1, ğ‘¥ğœƒ)
ğœ‹ğ‘Œğ‘–+2|ğ‘ğ‘–+2,Î˜(ğ‘¦ğ‘–+2|ğ‘§ğ‘–+2, ğ‘¥ğœƒ) dğ‘¥ğœƒdğ‘¥ğ‘–+1 dğ‘§ğ‘–+2
=
âˆ«ï¸
ğœ‹Î˜,ğ‘ğ‘–+1|ğ‘¦0:ğ‘–+1(ğ‘¥ğœƒ, ğ‘¥ğ‘–+1)
(E.21)
ğœ‹ğ‘ğ‘–+2,ğ‘Œğ‘–+2|ğ‘ğ‘–+1,Î˜(ğ‘§ğ‘–+2, ğ‘¦ğ‘–+2|ğ‘¥ğ‘–+1, ğ‘¥ğœƒ) dğ‘¥ğœƒdğ‘¥ğ‘–+1 dğ‘§ğ‘–+2
=
ğœ‹ğ‘Œğ‘–+2|ğ‘Œ0:ğ‘–+1(ğ‘¦ğ‘–+2|ğ‘¦0:ğ‘–+1) < âˆ,
232

where we used the change of variables:
â¡
â£ğ‘¥ğœƒ
ğ‘¥ğ‘–+1
â¤
â¦=
â¡
â£TÎ˜
ğ‘–(ğ‘§ğœƒ)
M1
ğ‘–(ğ‘§ğœƒ, ğ‘§ğ‘–+1)
â¤
â¦= Ìƒï¸
Mğ‘–(ğ‘§ğœƒ, ğ‘§ğ‘–+1),
(E.22)
and the fact that (Ìƒï¸
Mğ‘–)â™¯ğœ‚ğ‘‹Î˜,ğ‘‹ğ‘–+1 = ğœ‹Î˜,ğ‘ğ‘–+1|ğ‘¦0:ğ‘–+1 (induction hypothesis). (The change
of variables in (E.22) is valid for the following reason: the map Ìƒï¸
Mğ‘–can be factorized as
the composition of ğ‘–+1 (generalized) triangular functions, all that fit the hypothesis of
Lemma B.1, so that (E.22) should really be interpreted as a sequence of ğ‘–+1 change of
variablesâ€”each associated with one map in the composition and justified by Lemma
B.1.) Therefore ğœ‹ğ‘–+1 is a nonvanishing density. Following the same derivations as in
(E.20), it is not hard to show that ğœ‹ğ‘–+1 has also the desired marginal, i.e.,
âˆ«ï¸
ğœ‹ğ‘–+1(ğ‘§ğœƒ, ğ‘§ğ‘–+1, ğ‘§ğ‘–+2) dğ‘§ğ‘–+1 = ğ´â™¯
ğ‘–+1 ğœ‹Î˜,ğ‘ğ‘–+2|ğ‘¦0:ğ‘–+2(ğ‘§ğœƒ, ğ‘§ğ‘–+2).
(E.23)
This argument completes the induction step and shows that not only the maps
(Mğ‘–)ğ‘–â‰¥0 are well-definedâ€”together with the maps (ğ‘‡ğ‘–)ğ‘–â‰¥0 in (5.40)â€”but also that
(Ìƒï¸
M1
ğ‘–)â™¯ğœ‚ğ‘‹Î˜,ğ‘‹ğ‘–+1 = ğœ‹Î˜,ğ‘ğ‘–+1|ğ‘¦0:ğ‘–+1 for all ğ‘–â‰¥0 (Part 1 of the theorem).
Now we prove Part 2 of the theorem using another induction argument on ğ‘˜â‰¥0.
For the base case (ğ‘˜= 0), notice that T0 = ğ‘‡0 = M0, and that, by definition, M0
pushes forward ğœ‚ğ‘‹Î˜,ğ‘‹0,ğ‘‹1 to ğœ‹0 = ğœ‹Î˜,ğ‘0,ğ‘1|ğ‘¦0,ğ‘¦1. Assume that Tğ‘˜pushes forward
ğœ‚ğ‘‹Î˜,ğ‘‹0:ğ‘˜+1 to ğœ‹Î˜,ğ‘0:ğ‘˜+1|ğ‘¦0:ğ‘˜+1 for some ğ‘˜> 0 (Tğ‘˜is well-defined for all ğ‘˜since the maps
(ğ‘‡ğ‘–)ğ‘–â‰¥0 in (5.40) are also well-defined), and notice that
ğœ‹Î˜,ğ‘0:ğ‘˜+2|ğ‘¦0:ğ‘˜+2 = ğœ‹Î˜,ğ‘0:ğ‘˜+1|ğ‘¦0:ğ‘˜+1
ğœ‹ğ‘¦ğ‘˜+2|ğ‘ğ‘˜+2,Î˜ ğœ‹ğ‘ğ‘˜+2|ğ‘ğ‘˜+1,Î˜
ğœ‹ğ‘¦ğ‘˜+2|ğ‘¦0:ğ‘˜+1
= ğœ‹Î˜,ğ‘0:ğ‘˜+1|ğ‘¦0:ğ‘˜+1
Ìƒï¸€ğœ‹ğ‘˜+1
cğ‘˜+1
,
where we used (E.20) and the definition of the collection (Ìƒï¸€ğœ‹ğ‘–). Let Tğ‘˜+1 = ğ‘‡0âˆ˜Â· Â· Â·âˆ˜ğ‘‡ğ‘˜+1
233

be defined as in Part 2 of the theorem, and observe that Tğ‘˜+1 = ğ¶ğ‘˜+1 âˆ˜ğ‘‡ğ‘˜+1 with
ğ¶ğ‘˜+1(ğ‘¥ğœƒ, ğ‘¥0:ğ‘˜+2) =
â¡
â£Tğ‘˜(ğ‘¥ğœƒ, ğ‘¥0:ğ‘˜+1)
ğ‘¥ğ‘˜+2
â¤
â¦,
ğ‘‡ğ‘˜+1(ğ‘¥ğœƒ, ğ‘¥0:ğ‘˜+2) =
â¡
â¢â¢â¢â¢â¢â¢â¢â¢â¢â¢â¢â¢â¢â£
MÎ˜
ğ‘˜+1(ğ‘¥ğœƒ)
ğ‘¥0
...
ğ‘¥ğ‘˜
M0
ğ‘˜+1(ğ‘¥ğœƒ, ğ‘¥ğ‘˜+1, ğ‘¥ğ‘˜+2)
M1
ğ‘˜+1(ğ‘¥ğœƒ, ğ‘¥ğ‘˜+2)
â¤
â¥â¥â¥â¥â¥â¥â¥â¥â¥â¥â¥â¥â¥â¦
.
Thus the following hold:
Tâ™¯
ğ‘˜+1 ğœ‹Î˜,ğ‘0:ğ‘˜+2|ğ‘¦0:ğ‘˜+2
=
ğ‘‡â™¯
ğ‘˜+1
(ï¸‚(ï¸
Tâ™¯
ğ‘˜ğœ‹Î˜,ğ‘0:ğ‘˜+1|ğ‘¦0:ğ‘˜+1
)ï¸
ğœ‹ğ‘˜+1
ğœ‚ğ‘‹Î˜,ğ‘‹ğ‘˜+1
)ï¸‚
(E.24)
=
ğ‘‡â™¯
ğ‘˜+1
(ï¸€
ğœ‚ğ‘‹0:ğ‘˜ğœ‹ğ‘˜+1)ï¸€
=
ğœ‚ğ‘‹0:ğ‘˜Mâ™¯
ğ‘˜+1 ğœ‹ğ‘˜+1 = ğœ‚ğ‘‹Î˜,ğ‘‹0:ğ‘˜+2,
where we used the fact that by Lemma B.1 (applied iteratively) it must be that
(ğ¶ğ‘˜+1 âˆ˜ğ‘‡ğ‘˜+1)â™¯ğœŒ= ğ‘‡â™¯
ğ‘˜+1 ğ¶â™¯
ğ‘˜+1ğœŒfor all densities ğœŒ. (Notice that ğ¶ğ‘˜+1 is the composition
of functions which are trivial embeddings into the identity map of KR rearrangements
that couple pair of measures in M+(Rğ‘Ã—Rğ‘›Ã—Rğ‘›), and thus each map in the composi-
tion satisfies the hypothesis of Lemma B.1.) Thus (Tğ‘˜+1)â™¯ğœ‚ğ‘‹Î˜,ğ‘‹0:ğ‘˜+2 = ğœ‹Î˜,ğ‘0:ğ‘˜+2|ğ‘¦0:ğ‘˜+2,
and this concludes the induction argument and the proof of Part 2 of the theorem.
The proof of Part 3 follows from c0 = ğœ‹ğ‘Œ0,ğ‘Œ1(ğ‘¦0, ğ‘¦1), (E.20), and (E.13).
â–¡
Proof of Theorem 5.5. Let us start with Part 1 of the Theorem. Let M be a map
that pushes forward ğœ‚ğ‘‹1,...,ğ‘‹ğœ…to ğ›¿(e.g., a KR rearrangement), and define a function
Ì‚ï¸€ğ‘‡: Rğ‘›â†’Rğ‘›as follows:
Ì‚ï¸€ğ‘‡(ğ‘¥) =
â¡
â¢â¢â¢â¢â¢â¢â£
M(ğ‘¥1, . . . , ğ‘¥ğœ…)
ğ‘¥ğœ…+1
...
ğ‘¥ğ‘›
â¤
â¥â¥â¥â¥â¥â¥â¦
.
(E.25)
234

Clearly Ì‚ï¸€ğ‘‡pushes forward ğœ‚to ğœ‹ğ‘„= ğ‘„â™¯ğœ‹, so that ğ‘‡= ğ‘„âˆ˜Ì‚ï¸€ğ‘‡must be a low-rank
transport that pushes forward ğœ‚to ğœ‹.
Now Part 2. Assume that rank(CğœŒ) â‰¤ğœ…. Let CğœŒ= ğ‘„1 Î£1 ğ‘„âŠ¤
1 be a (reduced)
eigendecomposition of CğœŒwith ğ‘„1 âˆˆRğ‘›Ã—ğœ…and Î£1 = diag(ğœ1, . . . , ğœğœ…) with ğœğ‘—â‰¥
0. Such a decomposition always exists since rank(CğœŒ) â‰¤ğœ…. By (for instance) [67,
Proposition 2.3], there exists a function ğ‘“: Rğœ…â†’R such that r(ğ‘¥) = ğ‘“(ğ‘„âŠ¤
1 ğ‘¥) for all
ğ‘¥âˆˆRğ‘›. Define a rotation ğ‘„as ğ‘„= [ğ‘„1|ğ‘„2], where the columns of ğ‘„2 âˆˆRğ‘›Ã—(ğ‘›âˆ’ğœ…)
are any orthonormal basis of Rğ‘›âˆ–span(ğ‘„1). Notice that the columns of ğ‘„1 span the
range of CğœŒ, and that r(ğ‘„ğ‘¥) = ğ‘”(ğ‘¥1, . . . , ğ‘¥ğœ…) for some ğ‘”: Rğœ…â†’R. In particular,
r âˆ˜ğ‘„= log ğœ‚âˆ˜ğ‘„âˆ’log ğœ‹âˆ˜ğ‘„, where ğœ‚âˆ˜ğ‘„= ğœ‚since ğœ‚= ğ’©(0, I), and where
ğœ‹âˆ˜ğ‘„= ğ‘„â™¯ğœ‹=: ğœ‹ğ‘„since | det ğ‘„| = 1. Simple algebra then shows that:
ğœ‹ğ‘„(ğ‘¥) = ğ›¿(ğ‘¥1, . . . , ğ‘¥ğœ…) ğœ‚ğ‘‹ğœ…+1:ğ‘›(ğ‘¥ğœ…+1:ğ‘›),
(E.26)
where ğ›¿= ğœ‚ğ‘‹1:ğœ…exp(âˆ’ğ‘”) = ğœ‹ğ‘„/ğœ‚ğ‘‹ğœ…+1:ğ‘›corresponds to a smooth marginal of ğœ‹ğ‘„. This
proves one direction. Now let us assume that there exists a rotation, ğ‘„, of Rğ‘›such
that the pullback ğœ‹ğ‘„= ğ‘„â™¯ğœ‹factorizes according to (5.44). In this case, ğœ‹ğ‘„/ğœ‚= ğ‘âˆ˜Î ğœ…,
where ğ‘is some real-valued function on Rğ‘›, and where Î ğœ…is an orthogonal projector
onto the first ğœ…coordinate directions. Hence ğœ‹/ğœ‚= ğ‘âˆ˜Î ğœ…âˆ˜ğ‘„âŠ¤, and
CğœŒ= ğ‘„Î ğœ…
(ï¸‚âˆ«ï¸
âˆ‡log ğ‘(Î ğœ…ğ‘„âŠ¤ğ‘¥)âˆ‡log ğ‘(Î ğœ…ğ‘„âŠ¤ğ‘¥)âŠ¤ğœŒ(ğ‘¥) dğ‘¥
)ï¸‚
Î ğœ…ğ‘„âŠ¤,
(E.27)
so that rank(CğœŒ) â‰¤rank(Î ğœ…) = ğœ….
â–¡
Example E.1. Let ğ‘= (ğ‘1, ğ‘2) âˆ¼ğœ‹ğ‘be a latent process on R2 with a non-Gaussian
prior density given by
ğœ‹ğ‘(ğ‘¥) âˆexp(âˆ’1
2(ğ‘¥2 âˆ’ğ‘¥2
1)2 âˆ’1
2ğ‘¥2
2),
(E.28)
and let ğ‘Œbe a real-valued random variable which represents the observed process,
modeled by ğ‘Œ= ğ‘1 + â„°, where â„°âˆ¼ğ’©(0, 1) is independent of ğ‘1. The likelihood
235

function, ğœ‹ğ‘Œ|ğ‘, is of the form:
ğœ‹ğ‘Œ|ğ‘(ğ‘¦|ğ‘¥) âˆexp(âˆ’1
2(ğ‘¦âˆ’ğ‘¥1)2),
(E.29)
whereas the posterior distribution for the event {ğ‘Œ= 0} is given by
ğœ‹ğ‘|ğ‘Œ(ğ‘¥|0) âˆğœ‹ğ‘Œ|ğ‘(0|ğ‘¥) ğœ‹ğ‘(ğ‘¥).
(E.30)
For the purpose of this example, we regard (E.30) as the target density ğœ‹and consider
ğœ‚= ğ’©(0, I) to be the reference density. If we apply Theorem 5.5[Part 2] to the density
pair (ğœ‚, ğœ‹) for ğœŒ= ğœ‚, we get
CğœŒ=
â¡
â£4(1 + 5!!)
0
0
1 + 3!!
â¤
â¦,
(E.31)
which is clearly a full-rank matrix, and so no dimensionality reduction is possible in
this case (ğœ…= ğ‘›= 2). However, since the likelihood function is localized (see Section
5.7.3), we know that there exists a transport map that decomposes as (5.50), where Ì‚ï¸€ğ‘‡
is essentially a one-dimensional map (ğœ…= 1), and where the prior map is the unique
monotone increasing and lower triangular rearrangement that pushes forward ğœ‚to ğœ‹ğ‘.
The prior map was certainly key to achieve this type of dimensionality reduction. But
what if we had used a prior map with a different sparsity pattern? For instance, an
upper rather than a lower triangular map? Let ğ‘‡pr be the monotone increasing upper
triangular transport that pushes forward ğœ‚to ğœ‹ğ‘. It is immediate to verify that ğ‘‡pr
is of the form
ğ‘‡pr(ğ‘¥) =
â¡
â£ğ‘¥1 + ğ‘¥2
2
ğ‘¥2
â¤
â¦,
(E.32)
and that
ğ‘‡â™¯
pr ğœ‹(ğ‘¥) = exp(âˆ’1
2(ğ‘¥1 + ğ‘¥2
2)2) ğœ‚(ğ‘¥).
(E.33)
236

If we now apply Theorem 5.5 to the density pair (ğœ‚, ğ‘‡â™¯
pr ğœ‹) for ğœŒ= ğœ‚, we find that
CğœŒ=
â¡
â£1 + 3!!
0
0
4(1 + 5!!)
â¤
â¦
(E.34)
is once again a full-rank matrix, so that no dimensionality reduction like (5.50) is
possible at this stage.
Thus, the sparsity pattern of the prior map can be really
important.
237

238

Bibliography
[1] P. A. Absil, C. G. Baker, and K. A. Gallivan, A truncated-CG style
method for symmetric generalized eigenvalue problems, J. Comput. Appl. Math,
189 (2006), pp. 274â€“285.
[2] P. A. Absil, C. G. Baker, K. A. Gallivan, and A. Sameh, Adaptive
model trust region methods for generalized eigenvalue problems, in International
Conference on Computational Science, Springer, 2005, pp. 33â€“41.
[3] V. AkÃ§elik, G. Biros, O. Ghattas, J. Hill, D. Keyes, and B. van
Bloemen Waanders, Parallel algorithms for PDE-constrained optimization,
Parallel Processing for Scientific Computing, 20 (2006), p. 291.
[4] S. Amari and H. Nagaoka, Methods of Information Geometry, vol. 191,
American Mathematical Soc., 2007.
[5] E. Anderes and M. Coram, A general spline representation for non-
parametric and semiparametric density estimates using diffeomorphisms,
arXiv:1205.5314, (2012).
[6] C. Andrieu, A. Doucet, and R. Holenstein, Particle markov chain monte
carlo methods, Journal of the Royal Statistical Society: Series B (Statistical
Methodology), 72 (2010), pp. 269â€“342.
[7] V. Arsigny, P. Fillard, X. Pennec, and N. Ayache, Geometric means
in a novel vector space structure on symmetric positive-definite matrices, SIAM
Journal on Matrix Analysis and Applications, 29 (2007), pp. 328â€“347.
239

[8] S. Asmussen and P. W. Glynn, Stochastic simulation: algorithms and anal-
ysis, vol. 57, Springer Science & Business Media, 2007.
[9] C. Atkinson and A. F. S. Mitchell, Raoâ€™s distance measure, SankhyÂ¯a:
The Indian Journal of Statistics, Series A, 43 (1981), pp. 345â€“365.
[10] H. Auvinen, J. M. Bardsley, H. Haario, and T. Kauranne, Large-scale
Kalman filtering using the limited memory BFGS method, Electronic Transac-
tions on Numerical Analysis, 35 (2009), pp. 217â€“233.
[11]
, The variational Kalman filter and an efficient implementation using lim-
ited memory BFGS, International Journal for Numerical Methods in Fluids, 64
(2010), pp. 314â€“335.
[12] O. Axelsson and I. Kaporin, On the sublinear and superlinear rate of con-
vergence of conjugate gradient methods, Numerical Algorithms, 25 (2000), pp. 1â€“
22.
[13] Z. Bai, J. Demmel, J. Dongarra, A. Ruhe, and H. van der Vorst,
Templates for the Solution of Algebraic Eigenvalue Problems:
A Practical
Guide, vol. 11, SIAM, 2000.
[14] C. G. Baker, P. A. Absil, and K. A. Gallivan, An implicit Riemannian
trust-region method for the symmetric generalized eigenproblem, in International
Conference on Computational Science, Springer, 2006, pp. 210â€“217.
[15] A. Barachant, S. Bonnet, M. Congedo, and C. Jutten, Classification
of covariance matrices using a Riemannian-based kernel for BCI applications,
Neurocomputing, 112 (2013), pp. 172â€“178.
[16] J. M. Bardsley, A. Solonen, H. Haario, and M. Laine, Randomize-
then-Optimize: A method for sampling from posterior distributions in nonlinear
inverse problems, SIAM Journal on Scientific Computing, 36 (2014), pp. A1895â€“
A1910.
240

[17] R. Barrett, M. W. Berry, T. F. Chan, J. Demmel, J. Donato, J. Don-
garra, V. Eijkhout, R. Pozo, C. Romine, and H. Van der Vorst,
Templates for the Solution of Linear Systems: Building Blocks for Iterative
Methods, vol. 43, SIAM, 1994.
[18] T. Bengtsson, C. Snyder, and D. Nychka, Toward a nonlinear ensemble
filter for high-dimensional systems, Journal of Geophysical Research: Atmo-
spheres, 108 (2003).
[19] T. L. Bergman, F. P. Incropera, and A. S. Lavine, Fundamentals of
Heat and Mass Transfer, John Wiley & Sons, 2011.
[20] J. M. Bernardo and A. F. Smith, Bayesian theory, 2001.
[21] D. P. Bertsekas, Dynamic programming and optimal control, vol. 1, Athena
Scientific Belmont, MA, 1995.
[22] D. P. Bertsekas and D. A. Castanon, The auction algorithm for the trans-
portation problem, Annals of Operations Research, 20 (1989), pp. 67â€“96.
[23] A. Beskos, D. Crisan, A. Jasra, K. Kamatani, and Y. Zhou, A stable
particle filter in high-dimensions, arXiv:1412.3501, (2014).
[24] R. Bhatia, Positive Definite Matrices, Princeton University Press, 2009.
[25]
, Matrix Analysis, vol. 169, Springer Science & Business Media, 2013.
[26] P. J. Bickel and E. Levina, Covariance regularization by thresholding, The
Annals of Statistics, (2008), pp. 2577â€“2604.
[27]
, Regularized estimation of large covariance matrices, The Annals of Statis-
tics, (2008), pp. 199â€“227.
[28] G. J. Bierman, Factorization methods for discrete sequential estimation,
Courier Corporation, 2006.
241

[29] D. Bigoni, A. Spantini, and Y. Marzouk, Adaptive construction of mea-
sure transports for Bayesian inference, NIPS workshop on Approximate Infer-
ence, (2016).
[30] D. Bigoni, A. Spantini, and Y. Marzouk, On the computation of monotone
transports, In preparation, (2016).
[31] P. Billingsley, Probability and measure, John Wiley & Sons, 2008.
[32] A. Blake, P. Kohli, and C. Rother, Markov random fields for vision and
image processing, MIT Press, 2011.
[33] E. Blayo, M. Bocquet, E. Cosme, and L. F. Cugliandolo, Advanced
data assimilation for geosciences, 2014.
[34] V. I. Bogachev, A. V. Kolesnikov, and K. V. Medvedev, Triangular
transformations of measures, Sbornik: Mathematics, 196 (2005), p. 309.
[35] S. Bonnabel and R. Sepulchre, Riemannian metric and geometric mean
for positive semidefinite matrices of fixed rank, SIAM Journal on Matrix Anal-
ysis and Applications, 31 (2009), pp. 1055â€“1070.
[36] L. Bottou, F. E. Curtis, and J. Nocedal, Optimization methods for large-
scale machine learning, arXiv:1606.04838, (2016).
[37] S. Boyd and L. Vandenberghe, Convex Optimization, Cambridge Univer-
sity Press, 2004.
[38] C. M. Branco, R. Ritchie, and V. Sklenicka, Mechanical behaviour of
materials at high temperature, vol. 15, Springer Science & Business Media, 1996.
[39] W. Bryc, The normal distribution:
characterizations with applications,
vol. 100, Springer Science & Business Media, 2012.
[40] T. Bui-Thanh, C. Burstedde, O. Ghattas, J. Martin, G. Stadler,
and L. Wilcox, Extreme-scale UQ for Bayesian inverse problems governed
242

by PDEs, in Proceedings of the International Conference on High Performance
Computing, Networking, Storage and Analysis, IEEE Computer Society Press,
2012, p. 3.
[41] T. Bui-Thanh and O. Ghattas, Analysis of the Hessian for inverse scatter-
ing problems: I. Inverse shape scattering of acoustic waves, Inverse Problems,
28 (2012), p. 055001.
[42] T. Bui-Thanh, O. Ghattas, and D. Higdon, Adaptive Hessian-based non-
stationary Gaussian process response surface method for probability density ap-
proximation with application to Bayesian solution of large-scale inverse prob-
lems, SIAM Journal on Scientific Computing, 34 (2012), pp. A2837â€“A2871.
[43] T. Bui-Thanh, O. Ghattas, J. Martin, and G. Stadler, A computa-
tional framework for infinite-dimensional Bayesian inverse problems part I: The
linearized case, with application to global seismic inversion, SIAM Journal on
Scientific Computing, 35 (2013), pp. A2494â€“A2523.
[44] D. Calvetti, Preconditioned iterative methods for linear discrete ill-posed
problems from a Bayesian inversion perspective, Journal of computational and
applied mathematics, 198 (2007), pp. 378â€“395.
[45] D. Calvetti, B. Lewis, and L. Reichel, On the regularizing properties of
the GMRES method, Numerische Mathematik, 91 (2002), pp. 605â€“625.
[46] D. Calvetti, D. McGivney, and E. Somersalo, Left and right precondi-
tioning for electrical impedance tomography with structural information, Inverse
Problems, 28 (2012), p. 055015.
[47] D. Calvetti and E. Somersalo, Priorconditioners for linear systems, In-
verse problems, 21 (2005), p. 1397.
[48] G. Carlier, A. Galichon, and F. Santambrogio, From Knotheâ€™s trans-
port to Brenierâ€™s map and a continuation method for optimal transport, SIAM
Journal on Mathematical Analysis, 41 (2010), pp. 2554â€“2576.
243

[49] B. Carlin and T. Louis, Bayesian Methods for Data Analysis, Chapman &
Hall/CRC, 3rd ed., 2009.
[50] M.-H. Chen, Q.-M. Shao, and J. G. Ibrahim, Monte Carlo methods in
Bayesian computation, Springer Science & Business Media, 2012.
[51] D. Cheng, Y. Cheng, Y. Liu, R. Peng, and S. Teng, Efficient sam-
pling for Gaussian graphical models via spectral sparsification., in Conference
on Learning Theory, 2015, pp. 364â€“390.
[52] M. J. Choi, V. Chandrasekaran, D. M. Malioutov, J. K. Johnson,
and A. S. Willsky, Multiscale stochastic modeling for tractable inference and
data assimilation, Computer Methods in Applied Mechanics and Engineering,
197 (2008), pp. 3492â€“3515.
[53] M. J. Choi, V. Chandrasekaran, and A. S. Willsky, Gaussian mul-
tiresolution models: Exploiting sparse Markov and covariance structure, IEEE
Transactions on Signal Processing, 58 (2010), pp. 1012â€“1024.
[54] S. T. Choi, Iterative Methods for Singular Linear Equations and Least-Squares
Problems, PhD thesis, Stanford University, 2006.
[55] N. Chopin, P. E. Jacob, and O. Papaspiliopoulos, SMC2: an efficient
algorithm for sequential analysis of state space models, Journal of the Royal
Statistical Society: Series B (Statistical Methodology), 75 (2013), pp. 397â€“426.
[56] A. Chorin, M. Morzfeld, and X. Tu, Implicit particle filters for data as-
similation, Communications in Applied Mathematics and Computational Sci-
ence, 5 (2010), pp. 221â€“240.
[57] A. J. Chorin and P. Krause, Dimensional reduction for a Bayesian fil-
ter, Proceedings of the National Academy of Sciences of the United States of
America, 101 (2004), pp. 15013â€“15017.
244

[58] A. J. Chorin and X. Tu, Implicit sampling for particle filters, Proceedings
of the National Academy of Sciences, 106 (2009), pp. 17249â€“17254.
[59] E. Chow and Y. Saad, Preconditioned Krylov subspace methods for sampling
multivariate Gaussian distributions, SIAM Journal on Scientific Computing, 36
(2014), pp. A588â€“A608.
[60] O. F. Christensen, G. O. Roberts, and J. S. Rosenthal, Scaling limits
for the transient phase of local Metropolis-Hastings algorithms, Journal of the
Royal Statistical Society. Series B: Statistical Methodology, 67 (2005), pp. 253â€“
268.
[61] R. Christensen, Plane Answers to Complex Questions: The Theory of Linear
Models, Springer-Verlag, 1987.
[62] J. Chung and M. Chung, Computing optimal low-rank matrix approxima-
tions for image processing, in Signals, Systems and Computers, 2013 Asilomar
Conference on, IEEE, 2013, pp. 670â€“674.
[63]
, An efficient approach for computing optimal low-rank regularized inverse
matrices, Inverse Problems, 30 (2014), p. 114009.
[64] J. Chung, M. Chung, and D. Oâ€™Leary, Designing optimal spectral filters for
inverse problems, SIAM Journal on Scientific Computing, 33 (2011), pp. 3132â€“
3152.
[65]
, Optimal filters from calibration data for image deconvolution with data
acquisition error, Journal of Mathematical Imaging and Vision, 44 (2012),
pp. 366â€“374.
[66] J. Chung, M. Chung, and D. P. Oâ€™Leary, Optimal regularized low rank
inverse approximation, Linear Algebra and its Applications, 468 (2015), pp. 260
â€“ 269.
245

[67] P. G. Constantine, E. Dow, and Q. Wang, Active subspace methods in
theory and practice: Applications to kriging surfaces, SIAM Journal on Scientific
Computing, 36 (2014), pp. A1500â€“A1524.
[68] P. G. Constantine and D. Gleich, Computing active subspaces with Monte
Carlo, arXiv:1408.0545, (2014).
[69] P. G. Constantine, C. Kent, and T. Bui-Thanh, Accelerating Markov
chain Monte Carlo with active subspaces, SIAM Journal on Scientific Comput-
ing, 38 (2016), pp. A2779â€“A2805.
[70] D. Crisan and A. Doucet, A survey of convergence results on particle fil-
tering methods for practitioners, IEEE Transactions on signal processing, 50
(2002), pp. 736â€“746.
[71] D. Crisan and J. Miguez, Nested particle filters for online parameter esti-
mation in discrete-time state-space Markov models, arXiv:1308.1883, (2013).
[72] K. CsillÃ©ry, M. G. B. Blum, O. E. Gaggiotti, and O. FranÃ§ois, Ap-
proximate Bayesian Computation (ABC) in practice, Trends in ecology & evo-
lution, 25 (2010), pp. 410â€“8.
[73] T. Cui, K. J. Law, and Y. M. Marzouk, Dimension-independent likelihood-
informed MCMC, Journal of Computational Physics, 304 (2016), pp. 109â€“137.
[74] T. Cui, J. Martin, Y. Marzouk, A. Solonen, and A. Spantini,
Likelihood-informed dimension reduction for nonlinear inverse problems, Inverse
Problems, 30 (2014), p. 114015.
[75] J. Cullum and W. Donath, A block Lanczos algorithm for computing the q
algebraically largest eigenvalues and a corresponding eigenspace of large, sparse,
real symmetric matrices, in Decision and Control including the 13th Symposium
on Adaptive Processes, 1974 IEEE Conference on, IEEE, 1974, pp. 505â€“509.
246

[76] M. Dashti and A. M. Stuart, The Bayesian approach to inverse problems,
arXiv:1302.6989, (2013).
[77] F. Daum and J. Huang, Particle flow for nonlinear filters with log-homotopy,
in SPIE Defense and Security Symposium, International Society for Optics and
Photonics, 2008, pp. 696918â€“696918.
[78] F. Daum and J. Huang, Particle flow and Monge-Kantorovich transport, in
Information Fusion (FUSION), 2012 15th International Conference on, IEEE,
2012, pp. 135â€“142.
[79] P. J. Davis and P. Rabinowitz, Methods of numerical integration, Courier
Corporation, 2007.
[80] J. de Wiljes, W. Acevedo, and S. Reich, Second-order accurate ensemble
transform particle filters, arXiv:1608.08179, (2016).
[81] P. Del Moral, Feynman-Kac formulae, in Feynman-Kac Formulae:
Ge-
nealogical and Interacting Particle Systems with Applications, Springer, 2004,
pp. 47â€“93.
[82] P. Del Moral, A. Jasra, and Y. Zhou, Biased online parameter inference
for state-space models, Methodology and Computing in Applied Probability, 19
(2017), pp. 727â€“749.
[83] J. Dick, R. N. Gantner, Q. T. L. Gia, and C. Schwab, Higher or-
der Quasi-Monte Carlo integration for Bayesian estimation, arXiv:1602.07363,
(2016).
[84] J. Dick, F. Y. Kuo, and I. H. Sloan, High-dimensional integration: the
Quasi-Monte Carlo way, Acta Numerica, 22 (2013), pp. 133â€“288.
[85] C. R. Dietrich and G. N. Newsam, Fast and exact simulation of stationary
Gaussian processes through circulant embedding of the covariance matrix, SIAM
Journal on Scientific Computing, 18 (1997), pp. 1088â€“1107.
247

[86] M. P. do Carmo, Riemannian Geometry, BirkhÃ¤user, 1992.
[87] A. Doucet and A. M. Johansen, A tutorial on particle filtering and smooth-
ing: Fifteen years later, Handbook of nonlinear filtering, 12 (2009), p. 3.
[88] J. Durbin and S. J. Koopman, Time series analysis of non-Gaussian obser-
vations based on state-space models from both classical and Bayesian perspec-
tives, Journal of the Royal Statistical Society: Series B, 62 (2000), pp. 3â€“56.
[89] L. Dykes and L. Reichel, Simplified GSVD computations for the solution
of linear discrete ill-posed problems, Journal of Computational and Applied
Mathematics, 255 (2014), pp. 15â€“27.
[90] C. Eckart and G. Young, The approximation of one matrix by another of
lower rank, Psychometrika, 1 (1936), pp. 211â€“218.
[91] Y. B. Erol, Y. Wu, L. Li, and S. J. Russell, A nearly-black-box online al-
gorithm for joint parameter and state estimation in temporal models., in AAAI,
2017, pp. 1861â€“1869.
[92] G. Evensen, The ensemble Kalman filter: Theoretical formulation and prac-
tical implementation, Ocean dynamics, 53 (2003), pp. 343â€“367.
[93] G. Evensen, Data Assimilation, Springer, 2007.
[94] M. Fisher, Development of a simplified Kalman filter, European Centre for
Medium-Range Weather Forecasts, 1998.
[95] R. A. Fisher, On the mathematical foundations of theoretical statistics, Philo-
sophical Transactions of the Royal Society of London. Series A, 222 (1922),
pp. 309â€“368.
[96] H. Flath, L. Wilcox, V. AkÃ§elik, J. Hill, B. van Bloemen Waanders,
and O. Ghattas, Fast algorithms for Bayesian uncertainty quantification in
large-scale linear inverse problems based on low-rank partial Hessian approxi-
mations, SIAM Journal on Scientific Computing, 33 (2011), pp. 407â€“432.
248

[97] P. T. Fletcher, C. Lu, S. M. Pizer, and S. Joshi, Principal geodesic
analysis for the study of nonlinear statistics of shape, Medical Imaging, IEEE
Transactions on, 23 (2004), pp. 995â€“1005.
[98] D. C. Fong and M. Saunders, LSMR: An iterative algorithm for sparse
least-squares problems, SIAM Journal on Scientific Computing, 33 (2011),
pp. 2950â€“2971.
[99] W. FÃ¶rstner and B. Moonen, A metric for covariance matrices, in
Geodesy-The Challenge of the 3rd Millennium, Springer, 2003, pp. 299â€“309.
[100] C. Fox and A. Parker, Convergence in variance of Chebyshev accelerated
Gibbs samplers, SIAM Journal on Scientific Computing, 36 (2014), pp. A124â€“
A147.
[101] D. H. Fremlin, Measure theory, vol. 4, Torres Fremlin, 2000.
[102] S. Friedland and A. Torokhti, Generalized rank-constrained matrix ap-
proximations, SIAM Journal on Matrix Analysis and Applications, 29 (2007),
pp. 656â€“659.
[103] G. Gaspari and S. E. Cohn, Construction of correlation functions in two
and three dimensions, Quarterly Journal of the Royal Meteorological Society,
125 (1999), pp. 723â€“757.
[104] A. Gelman, J. B. Carlin, H. S. Stern, and D. B. Rubin, Bayesian Data
Analysis, Chapman and Hall, 2 ed., 2003.
[105] A. George and J. W. Liu, The evolution of the minimum degree ordering
algorithm, SIAM Review, 31 (1989), pp. 1â€“19.
[106] M. Girolami and B. Calderhead, Riemann manifold Langevin and Hamil-
tonian Monte Carlo methods, Journal of the Royal Statistical Society: Series B
(Statistical Methodology), 73 (2011), pp. 123â€“214.
249

[107] P. Glasserman, Gradient estimation via perturbation analysis, Springer Sci-
ence & Business Media, 1991.
[108] P. W. Glynn, Likelihood ratio gradient estimation for stochastic systems, Com-
munications of the ACM, 33 (1990), pp. 75â€“84.
[109] S. J. Godsill, A. Doucet, and M. West, Monte Carlo smoothing for non-
linear time series, Journal of the American Statistical Association, 99 (2004),
pp. 156â€“168.
[110] G. Golub and C. Van Loan, Matrix Computations, vol. 3, JHU Press, 2012.
[111] G. H. Golub and R. Underwood, The block Lanczos method for computing
eigenvalues, Mathematical software, 3 (1977), pp. 361â€“377.
[112] G. H. Golub and Q. Ye, An inverse free preconditioned Krylov subspace
method for symmetric generalized eigenvalue problems, SIAM Journal on Scien-
tific Computing, 24 (2002), pp. 312â€“334.
[113] G. Guglielmini and C. Pisoni, Elementi di trasmissione del calore, Veschi,
1990.
[114] H. Haario, M. Laine, M. Lehtinen, E. Saksman, and J. Tamminen,
Markov chain Monte Carlo methods for high dimensional inversion in remote
sensing, Journal of the Royal Statistical Society: Series B (Statistical Method-
ology), 66 (2004), pp. 591â€“607.
[115] W. Hackbusch, Tensor spaces and numerical tensor calculus, vol. 42, Springer
Science & Business Media, 2012.
[116] N. Halko, P. Martinsson, and J. Tropp, Finding structure with random-
ness: Probabilistic algorithms for constructing approximate matrix decomposi-
tions, SIAM Review, 53 (2011), pp. 217â€“288.
250

[117] T. M. Hamill, J. S. Whitaker, and C. Snyder, Distance-dependent fil-
tering of background error covariance estimates in an ensemble Kalman filter,
Monthly Weather Review, 129 (2001), pp. 2776â€“2790.
[118] J. M. Hammersley and P. Clifford, Markov fields on finite graphs and
lattices, (1971).
[119] M. Hanke, Conjugate gradient type methods for ill-posed problems, vol. 327,
CRC Press, 1995.
[120] M. Hanke and P. C. Hansen, Regularization methods for large-scale prob-
lems, Surv. Math. Ind, 3 (1993), pp. 253â€“315.
[121] P. C. Hansen, Regularization, GSVD and truncated GSVD, BIT Numerical
Mathematics, 29 (1989), pp. 491â€“504.
[122]
, Rank-Deficient and Discrete Ill-Posed Problems: Numerical Aspects of
Linear Inversion, vol. 4, SIAM, 1998.
[123] T. Hastie, R. Tibshirani, J. Friedman, and J. Franklin, The elements
of statistical learning: data mining, inference and prediction, The Mathematical
Intelligencer, 27 (2005), pp. 83â€“85.
[124] T. Hastie, R. Tibshirani, and M. Wainwright, Statistical learning with
sparsity: the lasso and generalizations, CRC press, 2015.
[125] J. Heikkinen, Statistical inversion theory in X-ray tomography, Masterâ€™s the-
sis, Lappeenranta University of Technology, Finland, 2008.
[126] J. Heng, A. Doucet, and Y. Pokern, Gibbs flow for approximate transport
with applications to Bayesian computation, arXiv:1509.08787, (2015).
[127] M. R. Hestenes, Pseudoinversus and conjugate gradients, Communications
of the ACM, 18 (1975), pp. 40â€“43.
[128] M. R. Hestenes and E. Stiefel, Methods of conjugate gradients for solving
linear systems, vol. 49, National Bureau of Standards Washington, DC, 1952.
251

[129] D. Higdon, C. S. Reese, J. D. Moulton, J. A. Vrugt, and C. Fox,
Posterior exploration for computationally intensive forward models, Handbook
of Markov chain Monte Carlo, CHAPMAN & HALL/CRC, (2011), pp. 401â€“418.
[130] Y. C. Ho and X. Cao, Perturbation analysis and optimization of queueing
networks, Journal of Optimization Theory and Applications, 40 (1983), pp. 559â€“
582.
[131] L. Homa, D. Calvetti, A. Hoover, and E. Somersalo, Bayesian pre-
conditioned CGLS for source separation in MEG time series, SIAM Journal on
Scientific Computing, 35 (2013), pp. B778â€“B798.
[132] I. Horev, F. Yger, and M. Sugiyama, Geometry-aware principal compo-
nent analysis for symmetric positive definite matrices, in JMLR: Workshop and
Conference Proceedings, 2015.
[133] Y. Hua and W. Liu, Generalized Karhunen-LoÃ¨ve transform, IEEE Signal
Processing Letters, 5 (1998), pp. 141â€“142.
[134] A. HyvÃ¤rinen, Estimation of non-normalized statistical models by score
matching, Journal of Machine Learning Research, 6 (2005), pp. 695â€“709.
[135] T. Isaac, N. Petra, G. Stadler, and O. Ghattas, Scalable and efficient
algorithms for the propagation of uncertainty from data through inference to
prediction for large-scale problems, with application to flow of the antarctic ice
sheet, Journal of Computational Physics, 296 (2015), pp. 348â€“368.
[136] P. E. Jacob, Sequential Bayesian inference for implicit hidden Markov models
and current limitations, ESAIM: Proceedings and Surveys, 51 (2015), pp. 24â€“48.
[137] W. James and C. Stein, Estimation with quadratic loss, in Proceedings of
the Fourth Berkeley Symposium on Mathematical Statistics and Probability,
vol. 1, 1961, pp. 361â€“379.
252

[138] E. T. Jaynes, Probability theory: The logic of science, Cambridge University
Press, 2003.
[139] S. T. Jensen. Private communication, 1976. (As cited in Atkinson, Mitchell,
â€œRaoâ€™s Distance Measure,â€ SankhyÂ¯a: The Indian Journal of Statistics, Series A,
43 (1981), pp. 345â€“365.).
[140] V. Jog and P. Loh, On model misspecification and KL separation for Gaus-
sian graphical models, in IEEE International Symposium on Information The-
ory, 2015, pp. 1174â€“1178.
[141] J. K. Johnson and A. S. Willsky, A recursive model-reduction method for
approximate inference in Gaussian Markov random fields, IEEE Transactions
on Image Processing, 17 (2008), pp. 70â€“83.
[142] M. Kac, On a characterization of the normal distribution, American Journal
of Mathematics, 61 (1939), pp. 726â€“728.
[143] W. Kahan and B. N. Parlett, How far should you go with the Lanczos
process., tech. rep., DTIC Document, 1978.
[144] J. Kaipio and E. Somersalo, Statistical and Computational Inverse Prob-
lems, vol. 160, Springer, 2005.
[145] J. Kaipio and E. Somersalo, Statistical inverse problems: Discretization,
model reduction and inverse crimes, Journal of Computational and Applied
Mathematics, 198 (2007), pp. 493â€“504.
[146] R. E. Kalman, A new approach to linear filtering and prediction problems,
Journal of Fluids Engineering, 82 (1960), pp. 35â€“45.
[147] S. Kaniel, Estimates for some computational techniques in linear algebra,
Mathematics of Computation, (1966), pp. 369â€“378.
253

[148] N. Kantas, A. Doucet, S. S. Singh, J. Maciejowski, and N. Chopin,
On particle methods for parameter estimation in state-space models, Statistical
Science, 30 (2015), pp. 328â€“351.
[149] S. Kim, N. Shephard, and S. Chib, Stochastic volatility: likelihood infer-
ence and comparison with ARCH models, The Review of Economic Studies, 65
(1998), pp. 361â€“393.
[150] P. Kirchgessner, L. Nerger, and A. Bunse-Gerstner, On the choice
of an optimal localization radius in ensemble Kalman filter methods, Monthly
Weather Review, 142 (2014), pp. 2165â€“2175.
[151] G. Kitagawa, Non-Gaussian state-space modeling of nonstationary time se-
ries, Journal of the American Statistical Association, 82 (1987), pp. 1032â€“1041.
[152]
, A self-organizing state-space model, Journal of the American Statistical
Association, (1998), pp. 1203â€“1215.
[153] A. Klinvex, F. Saied, and A. Sameh, Parallel implementations of the trace
minimization scheme TraceMIN for the sparse symmetric eigenvalue problem,
Computers & Mathematics with Applications, 65 (2013), pp. 460â€“468.
[154] H. Knothe et al., Contributions to the theory of convex bodies., The Michigan
Mathematical Journal, 4 (1957), pp. 39â€“52.
[155] D. Koller and R. Fratkina, Using learning for approximation in stochastic
processes., in ICML, 1998, pp. 287â€“295.
[156] D. Koller and N. Friedman, Probabilistic graphical models: principles and
techniques, MIT press, 2009.
[157] D. Kressner, M. M. Pandur, and M. Shao, An indefinite variant of
LOBPCG for definite matrix pencils, Numerical Algorithms, 66 (2014), pp. 681â€“
703.
254

[158] H. Kushner and G. G. Yin, Stochastic approximation and recursive algo-
rithms and applications, vol. 35, Springer Science & Business Media, 2003.
[159] C. Lanczos, An iteration method for the solution of the eigenvalue problem of
linear differential and integral operators, United States Governm. Press Office,
1950.
[160] V. Laparra, G. Camps-Valls, and J. Malo, Iterative Gaussianization:
from ICA to random rotations, IEEE transactions on neural networks, 22
(2011), pp. 537â€“549.
[161] S. L. Lauritzen, Graphical models, Oxford University Press, 1996.
[162] K. Law, A. Stuart, and K. Zygalakis, Data Assimilation: A Mathematical
Introduction, vol. 62, Springer, 2015.
[163] E. Lehmann and G. Casella, Theory of Point Estimation, Springer-Verlag,
1998.
[164] R. B. Lehoucq, D. C. Sorensen, and C. Yang, ARPACK usersâ€™ guide: so-
lution of large-scale eigenvalue problems with implicitly restarted Arnoldi meth-
ods, vol. 6, SIAM, 1998.
[165] A. Lewis, Derivatives of spectral functions, Mathematics of Operations Re-
search, 21 (1996), pp. 576â€“588.
[166] W. Li and O. A. Cirpka, Efficient geostatistical inverse methods for struc-
tured and unstructured grids, Water Resources Research, 42 (2006), p. W06402.
[167]
, Efficient geostatistical inverse methods for structured and unstructured
grids, Water Resources Research, 42 (2006).
[168] C. Lieberman,
K. Fidkowski,
K. Willcox,
and B. van Bloe-
men Waanders, Hessian-based model reduction: large-scale inversion and
prediction, International Journal for Numerical Methods in Fluids, 71 (2013),
pp. 135â€“150.
255

[169] C. Lieberman and K. Willcox, Goal-oriented inference: approach, linear
theory, and application to advection diffusion, SIAM Journal on Scientific Com-
puting, 34 (2012), pp. A1880â€“A1904.
[170]
, Nonlinear goal-oriented Bayesian inference: application to carbon capture
and storage, SIAM Journal on Scientific Computing, 36 (2014), pp. B427â€“B449.
[171] C. Lieberman, K. Willcox, and O. Ghattas, Parameter and state model
reduction for large-scale statistical inverse problems, SIAM Journal on Scientific
Computing, 32 (2010), pp. 2523â€“2542.
[172] J. Liesen and P. Tich`y, Convergence analysis of Krylov subspace methods,
GAMM-Mitteilungen, 27 (2004), pp. 153â€“173.
[173] L. Lin, M. Drton, and A. Shojaie, High-dimensional inference of graphical
models using regularized score matching, arXiv:1507.00433, (2015).
[174] F. Lindgren, H. Rue, and J. LindstrÃ¶m, An explicit link between Gaussian
fields and Gaussian Markov random fields: the stochastic partial differential
equation approach, Journal of the Royal Statistical Society: Series B, 73 (2011),
pp. 423â€“498.
[175] D. Lindley and A. Smith, Bayes estimates for the linear model, Journal of
the Royal Statistical Society. Series B, (1972), pp. 1â€“41.
[176] D. C. Liu and J. Nocedal, On the limited memory BFGS method for large
scale optimization, Mathematical programming, 45 (1989), pp. 503â€“528.
[177] J. Liu and M. West, Combined parameter and state estimation in simulation-
based filtering, in Sequential Monte Carlo methods in practice, Springer, 2001,
pp. 197â€“223.
[178] Q. Liu and D. Wang, Stein Variational Gradient Descent: A general purpose
Bayesian inference algorithm, in Advances in Neural Information Processing
Systems, 2016, pp. 2370â€“2378.
256

[179] Y. Liu, V. Chandrasekaran, A. Anandkumar, and A. S. Willsky,
Feedback message passing for inference in Gaussian graphical models, IEEE
Transactions on Signal Processing, 60 (2012), pp. 4135â€“4150.
[180] E. N. Lorenz, Predictability: A problem partly solved, in Proc. Seminar on
predictability, vol. 1, 1996.
[181] P. C. Mahalanobis, On the generalized distance in statistics, Proceedings of
the National Institute of Sciences (Calcutta), 2 (1936), pp. 49â€“55.
[182] A. J. Majda and J. Harlim, Filtering complex turbulent systems, Cambridge
University Press, 2012.
[183] A. J. Majda, D. Qi, and T. P. Sapsis, Blended particle filters for large-
dimensional chaotic dynamical systems, Proceedings of the National Academy
of Sciences, 111 (2014), pp. 7511â€“7516.
[184] J. M. Marin, P. Pudlo, C. P. Robert, and R. J. Ryder, Approxi-
mate Bayesian computational methods, Statistics and Computing, 22 (2012),
pp. 1167â€“1180.
[185] G. Marsaglia and W. W. Tsang, The Ziggurat method for generating ran-
dom variables, Journal of Statistical Software, 5 (2000), pp. 1â€“7.
[186] J. Martin, L. Wilcox, C. Burstedde, and O. Ghattas, A stochastic
Newton MCMC method for large-scale statistical inverse problems with appli-
cation to seismic inversion, SIAM Journal on Scientific Computing, 34 (2012),
pp. A1460â€“A1487.
[187] Y. Marzouk, T. Moselhy, M. Parno, and A. Spantini, Sampling via
measure transport: An introduction, in Handbook of Uncertainty Quantifica-
tion, R. Ghanem, D. Higdon, and H. Owhadi, editors, Springer, 2016.
[188] Y. Marzouk and H. Najm, Dimensionality reduction and polynomial chaos
acceleration of Bayesian inference in inverse problems, Journal of Computa-
tional Physics, 228 (2009), pp. 1862â€“1902.
257

[189] N. Meinshausen and P. BÃ¼hlmann, High-dimensional graphs and variable
selection with the lasso, The annals of statistics, (2006), pp. 1436â€“1462.
[190] X. Meng, M. A. Saunders, and M. W. Mahoney, LSRN: A parallel it-
erative solver for strongly over-or underdetermined systems, SIAM Journal on
Scientific Computing, 36 (2014), pp. C95â€“C118.
[191] X. Meng and S. Schilling, Warp bridge sampling, Journal of Computational
and Graphical Statistics, 11 (2002), pp. 552â€“586.
[192] M. Moakher and M. ZÃ©raÃ¯, The Riemannian geometry of the space of
positive-definite matrices and its application to the regularization of positive-
definite matrix-valued data, Journal of Mathematical Imaging and Vision, 40
(2011), pp. 171â€“187.
[193] J. MÃ¸ller, A. R. Syversveen, and R. Waagepetersen, Log Gaussian
Cox Processes, Scandinavian Journal of Statistics, 25 (1998), pp. 451â€“482.
[194] E. H. Moore, On the reciprocal of the general algebraic matrix, Bulletin of the
American Mathematical Society, 26, pp. 394â€“395.
[195] M. Morzfeld, X. Tu, J. Wilkening, and A. Chorin, Parameter es-
timation by implicit sampling, Communications in Applied Mathematics and
Computational Science, 10 (2015), pp. 205â€“225.
[196] T. Moselhy and Y. Marzouk, Bayesian inference with optimal maps, Jour-
nal of Computational Physics, 231 (2012), pp. 7815â€“7850.
[197] J. B. Nagel and B. Sudret, Spectral likelihood expansions for Bayesian
inference, Journal of Computational Physics, 309 (2016), pp. 267â€“294.
[198] J. Nocedal, Updating quasi-Newton matrices with limited storage, Mathemat-
ics of computation, 35 (1980), pp. 773â€“782.
[199] B. Oksendal, Stochastic differential equations: an introduction with applica-
tions, Springer Science & Business Media, 2013.
258

[200] D. P. Oâ€™Leary, The block conjugate gradient algorithm and related methods,
Linear Algebra and its Applications, 29 (1980), pp. 293â€“322.
[201] D. S. Oliver, Metropolized randomized maximum likelihood for sampling from
multimodal distributions, arXiv:1507.08563, (2015).
[202] I. V. Oseledets, Tensor-train decomposition, SIAM Journal on Scientific
Computing, 33 (2011), pp. 2295â€“2317.
[203] C. C. Paige, The computation of eigenvalues and eigenvectors of very large
sparse matrices., PhD thesis, University of London, 1971.
[204]
, Computational variants of the Lanczos method for the eigenproblem, IMA
Journal of Applied Mathematics, 10 (1972), pp. 373â€“381.
[205]
, Error analysis of the Lanczos algorithm for tridiagonalizing a symmetric
matrix, IMA Journal of Applied Mathematics, 18 (1976), pp. 341â€“349.
[206] C. C. Paige and M. A. Saunders, Towards a generalized singular value
decomposition, SIAM Journal on Numerical Analysis, 18 (1981), pp. 398â€“405.
[207]
, Algorithm 583: LSQR: Sparse linear equations and least squares problems,
ACM Transactions on Mathematical Software (TOMS), 8 (1982), pp. 195â€“209.
[208]
, LSQR: An algorithm for sparse linear equations and sparse least squares,
ACM Transactions on Mathematical Software (TOMS), 8 (1982), pp. 43â€“71.
[209] L. Pardo, Statistical Inference Based on Divergence Measures, CRC Press,
2005.
[210] P. Park and T. Kailath, New square-root smoothing algorithms, IEEE
Transactions on Automatic Control, 41 (1996), pp. 727â€“732.
[211] A. Parker and C. Fox, Sampling Gaussian distributions in Krylov spaces
with conjugate gradients, SIAM Journal on Scientific Computing, 34 (2012),
pp. B312â€“B334.
259

[212] M. Parno, Transport maps for accelerated Bayesian computation, PhD thesis,
Massachusetts Institute of Technology, 2015.
[213] M. Parno and Y. Marzouk, Transport map accelerated Markov chain Monte
Carlo, arXiv:1412.5492, (2014).
[214] M. Parno, T. Moselhy, and Y. Marzouk, A multiscale strategy for
Bayesian inference using transport maps, SIAM/ASA Journal on Uncertainty
Quantification, 4 (2016), pp. 1160â€“1190.
[215] X. Pennec, P. Fillard, and N. Ayache, A Riemannian framework for
tensor computing, International Journal of Computer Vision, 66 (2006), pp. 41â€“
66.
[216] R. Penrose, A generalized inverse for matrices, in Mathematical Proceedings
of the Cambridge Philosophical Society, vol. 51, Cambridge Univ Press, 1955,
pp. 406â€“413.
[217] E. A. Pnevmatikakis, K. R. Rad, J. Huggins, and L. Paninski, Fast
Kalman filtering and forwardâ€“backward smoothing via a low-rank perturba-
tive approach, Journal of Computational and Graphical Statistics, 23 (2014),
pp. 316â€“339.
[218] N. G. Polson, J. R. Stroud, and P. MÃ¼ller, Practical filtering with
sequential parameter learning, Journal of the Royal Statistical Society: Series
B, 70 (2008), pp. 413â€“428.
[219] J. Poterjoy, A localized particle filter for high-dimensional nonlinear systems,
Monthly Weather Review, 144 (2016), pp. 59â€“76.
[220] A. Quarteroni and A. Valli, Numerical Approximation of Partial Differ-
ential Equations, vol. 23, Springer Science & Business Media, 2008.
[221] J. Ramsay, Estimating smooth monotone functions, Journal of the Royal Sta-
tistical Society. Series B, Statistical Methodology, (1998), pp. 365â€“375.
260

[222] R. Ranganath, S. Gerrish, and D. M. Blei, Black box variational infer-
ence, in Artificial Intelligence and Statistics, 2014, pp. 814â€“822.
[223] C. R. Rao, Information and the accuracy attainable in the estimation of sta-
tistical parameters, Bulletin of the Calcutta Mathematical Society, 37 (1945),
pp. 81â€“89.
[224]
, On the distance between two populations, Sankhya, 9 (1949), pp. 246â€“248.
[225]
, Differential metrics in probability spaces, Differential geometry in statis-
tical inference, 10 (1987), pp. 217â€“240.
[226] H. E. Rauch, C. Striebel, and F. Tung, Maximum likelihood estimates of
linear dynamic systems, AIAA Journal, 3 (1965), pp. 1445â€“1450.
[227] P. Rebeschini, R. Van Handel, et al., Can local particle filters beat
the curse of dimensionality?, The Annals of Applied Probability, 25 (2015),
pp. 2809â€“2866.
[228] S. Reich, A nonparametric ensemble transform method for Bayesian inference,
SIAM Journal on Scientific Computing, 35 (2013), pp. A2013â€“A2024.
[229] S. Reich and C. Cotter, Ensemble filter techniques for intermittent data
assimilation, Large Scale Inverse Problems. Computational Methods and Ap-
plications in the Earth Sciences, 13 (2013), pp. 91â€“134.
[230]
, Probabilistic Forecasting and Bayesian Data Assimilation, Cambridge
University Press, 2015.
[231] D. J. Rezende and S. Mohamed, Variational inference with normalizing
flows, arXiv:1505.05770, (2015).
[232] C. Robert, The Bayesian choice: from decision-theoretic foundations to com-
putational implementation, Springer Science & Business Media, 2007.
[233] C. Robert and G. Casella, Monte Carlo statistical methods, Springer Sci-
ence & Business Media, 2013.
261

[234] M. Rosenblatt, Remarks on a multivariate transformation, The annals of
mathematical statistics, (1952), pp. 470â€“472.
[235] W. Rudin, Real and complex analysis, Tata McGraw-Hill Education, 1987.
[236] H. Rue and L. Held, Gaussian Markov random fields: theory and applica-
tions, CRC Press, 2005.
[237] H. Rue, S. Martino, and N. Chopin, Approximate Bayesian inference for
latent Gaussian models by using integrated nested Laplace approximations, Jour-
nal of the Royal Statistical Society: Series B, 71 (2009), pp. 319â€“392.
[238] T. M. Russi, Uncertainty quantification with experimental data and complex
system models, PhD thesis, UC Berkeley, 2010.
[239] Y. Saad, On the rates of convergence of the Lanczos and the block-Lanczos
methods, SIAM Journal on Numerical Analysis, 17 (1980), pp. 687â€“706.
[240]
, Iterative Methods for Sparse Linear Systems, SIAM, 2003.
[241] A. K. Saibaba, J. Lee, and P. K. Kitanidis, Randomized algorithms
for generalized Hermitian eigenvalue problems with application to comput-
ing Karhunenâ€“LoÃ¨ve expansion, Numerical Linear Algebra with Applications,
(2015).
[242] A. M. Samarov, Exploring regression structure using nonparametric func-
tional estimation, Journal of the American Statistical Association, 88 (1993),
pp. 836â€“847.
[243] A. H. Sameh and J. A. Wisniewski, A trace minimization algorithm for
the generalized eigenvalue problem, SIAM Journal on Numerical Analysis, 19
(1982), pp. 1243â€“1259.
[244] F. Santambrogio, Optimal Transport for Applied Mathematicians, vol. 87,
Springer, 2015.
262

[245] S. SÃ¤rkkÃ¤, Bayesian filtering and smoothing, no. 3, Cambridge University
Press, 2013.
[246] C. Schillings and C. Schwab, Sparse, adaptive Smolyak quadratures for
Bayesian inverse problems, Inverse Problems, 29 (2013), p. 065011.
[247]
, Scaling limits in computational Bayesian inversion, Research Report No.
2014-26, ETH-ZÃ¼rich, (2014).
[248] M. K. Schneider and A. S. Willsky, A Krylov subspace method for co-
variance approximation and simulation of random processes and fields, Multi-
dimensional Systems and Signal Processing, 14 (2003), pp. 295â€“318.
[249] A. Shapiro, Sample Average Approximation, Springer US, Boston, MA, 2013,
pp. 1350â€“1355.
[250] L. T. Skovgaard, A Riemannian geometry of the multivariate normal model,
Scandinavian Journal of Statistics, (1984), pp. 211â€“223.
[251] A. Smith, A. Doucet, N. de Freitas, and N. Gordon, Sequential Monte
Carlo methods in practice, Springer Science & Business Media, 2013.
[252] S. T. Smith, Covariance, subspace, and intrinsic CramÃ©r-Rao bounds, Signal
Processing, IEEE Transactions on, 53 (2005), pp. 1610â€“1630.
[253] A. Solonen, T. Cui, J. Hakkarainen, and Y. Marzouk, On dimension
reduction in Gaussian filters, Inverse Problems, 32 (2016), p. 045003.
[254] A. Solonen, H. Haario, J. Hakkarainen, H. Auvinen, I. Amour, and
T. Kauranne, Variational ensemble Kalman filtering using limited memory
BFGS, Electronic Transactions on Numerical Analysis, 39 (2012), pp. 271â€“285.
[255] S. Sommer, F. Lauze, S. Hauberg, and M. Nielsen, Manifold valued
statistics, exact principal geodesic analysis and the effect of linear approxima-
tions, in Computer Visionâ€“ECCV 2010, Springer, 2010, pp. 43â€“56.
263

[256] D. Sondermann, Best approximate solutions to matrix equations under rank
restrictions, Statistical Papers, 27 (1986), pp. 57â€“66.
[257] J. C. Spall, Introduction to stochastic search and optimization: estimation,
simulation, and control, vol. 65, John Wiley & Sons, 2005.
[258] A. Spantini, D. Bigoni, and Y. Marzouk, Variational inference via de-
composable transports: algorithms for Bayesian filtering and smoothing, NIPS
workshop on Approximate Inference, (2016).
[259]
, Inference via low-dimensional couplings, arXiv:1703.06131, (2017).
[260] A. Spantini, T. Cui, K. Willcox, L. Tenorio, and Y. M. Mar-
zouk, Goal-oriented optimal approximations of Bayesian linear inverse prob-
lems, SIAM Journal on Scientific Computing, in press (2017).
[261] A. Spantini, A. Solonen, T. Cui, J. Martin, L. Tenorio, and Y. Mar-
zouk, Optimal low-rank approximations of Bayesian linear inverse problems,
SIAM Journal on Scientific Computing, 37 (2015), pp. A2451â€“A2487.
[262] M. Spivak, Calculus on manifolds, vol. 1, WA Benjamin New York, 1965.
[263] F. Stavropoulou and J. MÃ¼ller, Parametrization of random vectors in
polynomial chaos expansions via optimal transportation, SIAM Journal on Sci-
entific Computing, 37 (2015), pp. A2535â€“A2557.
[264] G. Stewart, The efficient generation of random orthogonal matrices with an
application to condition estimators, SIAM Journal on Numerical Analysis, 17
(1980), pp. 403â€“409.
[265] A. M. Stuart, Inverse problems: a Bayesian perspective, Acta Numerica, 19
(2010), pp. 451â€“559.
[266] E. Tabak and C. V. Turner, A family of nonparametric density estima-
tion algorithms, Communications on Pure and Applied Mathematics, 66 (2013),
pp. 145â€“164.
264

[267] T. Tao, An introduction to measure theory, vol. 126, American Mathematical
Soc., 2011.
[268] A. Tarantola, Inverse problem theory and methods for model parameter es-
timation, SIAM, 2005.
[269] A. Tikhonov, Solution of incorrectly formulated problems and the regulariza-
tion method, in Soviet Math. Dokl., vol. 5, 1963, pp. 1035â€“1038.
[270] C. F. Van Loan, Generalizing the singular value decomposition, SIAM Journal
on Numerical Analysis, 13 (1976), pp. 76â€“83.
[271] C. Villani, Optimal transport: old and new, vol. 338, Springer Science &
Business Media, 2008.
[272] L. Wang and X. Meng, Warp bridge sampling:
the next generation,
arXiv:1609.07690, (2016).
[273] Z. Wang, J. M. Bardsley, A. Solonen, T. Cui, and Y. M. Marzouk,
Bayesian inverse problems with ğ‘™1 priors: a Randomize-then-Optimize approach,
SIAM Journal on Scientific Computing, in press (2017).
[274] D. J. Wilkinson, Stochastic modelling for systems biology, CRC press, 2011.
[275] A. T. A. Wood and G. Chan, Simulation of stationary Gaussian processes
in [0, 1]ğ‘‘, Journal of Computational and Graphical Statistics, 3 (1994), pp. 409â€“
432.
[276] S. J. Wright and J. Nocedal, Numerical Optimization, vol. 2, Springer
New York, 1999.
[277] D. Xiu, Numerical methods for stochastic computations: a spectral method ap-
proach, Princeton University Press, 2010.
[278] T. Yang, P. G. Mehta, and S. P. Meyn, Feedback particle filter, IEEE
transactions on Automatic control, 58 (2013), pp. 2465â€“2480.
265

[279] M. Yannakakis, Computing the minimum fill-in is NP-complete, SIAM Jour-
nal on Algebraic Discrete Methods, 2 (1981), pp. 77â€“79.
[280] M. Yuan and Y. Lin, Model selection and estimation in the Gaussian graph-
ical model, Biometrika, 94 (2007), pp. 19â€“35.
[281] Y. Yue and P. L. Speckman, Nonstationary spatial Gaussian Markov ran-
dom fields, Journal of Computational and Graphical Statistics, 19 (2010).
[282] O. Zham, Dimension reduction of nonlinear Bayesian inverse problem, In
preparation, (2017).
266

