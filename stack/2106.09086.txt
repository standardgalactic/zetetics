Learned Belief Search:
Efﬁciently Improving Policies in Partially Observable Settings
Hengyuan Hu* 1 Adam Lerer* 1 Noam Brown 1 Jakob Foerster 1
Abstract
Search is an important tool for computing ef-
fective policies in single- and multi-agent en-
vironments, and has been crucial for achieving
superhuman performance in several benchmark
fully and partially observable games. However,
one major limitation of prior search approaches
for partially observable environments is that the
computational cost scales poorly with the amount
of hidden information. In this paper we present
Learned Belief Search (LBS), a computation-
ally efﬁcient search procedure for partially ob-
servable environments.
Rather than maintain-
ing an exact belief distribution, LBS uses an ap-
proximate auto-regressive counterfactual belief
that is learned as a supervised task. In multi-
agent settings, LBS uses a novel public-private
model architecture for underlying policies in or-
der to efﬁciently evaluate these policies during
rollouts. In the benchmark domain of Hanabi,
LBS can obtain 55% ∼91% of the beneﬁt of ex-
act search while reducing compute requirements
by 35.8× ∼4.6×, allowing it to scale to larger
settings that were inaccessible to previous search
methods.
1. Introduction
Search has been a vital component for achieving superhu-
man performance in a number of hard benchmark prob-
lems in AI, including Go (Silver et al., 2016; 2017; 2018),
Chess (Campbell et al., 2002), Poker (Moravˇc´ık et al.,
2017; Brown & Sandholm, 2017; 2019), and, more re-
cently, self-play Hanabi (Lerer et al., 2020).
Beyond
achieving impressive results, the work on Hanabi and Poker
are some of the few examples of search being applied in
large partially observable settings. In contrast, the work
on belief-space planning typically assumes a small belief
*Equal contribution
1Facebook AI Research. Correspondence
to: Hengyuan Hu <hengyuan@fb.com>.
space, since these methods scale poorly with the size of the
belief space.
Inspired by the recent success of the SPARTA search tech-
nique in Hanabi (Lerer et al., 2020), we propose Learned
Belief Search (LBS) a simpler and more scalable approach
for policy improvement in partially observable settings, ap-
plicable whenever a model of the environment and the poli-
cies of any other agents are available at test time. Like
SPARTA, the key idea is to obtain Monte Carlo estimates
of the expected return for every possible action in a given
action-observation history (AOH) by sampling from a be-
lief distribution over possible states of the environment.
However, LBS addresses one of the key limitations of
SPARTA. Rather than requiring a sufﬁciently small belief
space, in which we can compute and sample from an ex-
act belief distribution, LBS samples from a learned, auto-
regressive belief model which is trained via supervised
learning (SL). The auto-regressive parameterization of the
probabilities allows LBS to be scaled to high-dimensional
state spaces, whenever these are composed as a sequence
of features.
Another efﬁciency improvement over SPARTA is replac-
ing the full rollouts with partial rollouts that bootstrap from
a value function after a speciﬁc number of steps. While
in general this value function can be trained via SL in a
separate training process, this is not necessary when the
blueprint (BP) was trained via RL. In these cases the RL
training typically produces both a policy and an approx-
imate value function (either for variance reduction or for
value-based learning). In particular, it is common prac-
tice to train centralized value functions, which capture the
required dependency on the sampled state even when this
state cannot be observed by the agent during test time.
While LBS is a very general search method for Partially
Observable Markov Decision Processes (POMDPs), our
application is focused on single-agent policy improvement
in Decentralized POMDPs (Dec-POMDPs) (in our speciﬁc
case, Hanabi). One additional challenge of single-agent
policy improvement in Dec-POMDPs is that, unlike stan-
dard POMDPs, the Markov state s of the environment is no
longer sufﬁcient for estimating the future expected return
for a given AOH of the searching player. Instead, since the
arXiv:2106.09086v1  [cs.AI]  16 Jun 2021

Learned Belief Search: Efﬁciently Improving Policies in Partially Observable Settings
other players’ policies also depend on their entire action-
observation histories, e.g., via Recurrent-Neural Networks
(RNNs), only the union of Markov state s and all AOHs is
sufﬁcient.
This in general makes it challenging to apply LBS, since
it would require sampling entire AOHs, rather than states.
However, in many Dec-POMDPs, including Hanabi, infor-
mation can be split between the common-knowledge (CK)
trajectory and private observations.
Furthermore, com-
monly information is ‘revealing’, such that there is a map-
ping from the most recent private observation and the CK
trajectory to the AOH for each given player. In these set-
tings it is sufﬁcient to track a belief over the union of pri-
vate observations, rather than over trajectories, which was
also exploited in SPARTA. We adapt LBS to this setting
with a novel public-RNN architecture which makes replay-
ing games from the beginning, as was done in SPARTA,
unnecessary.
When applied to the benchmark problem of two player
Hanabi self-play, LBS obtains 55% ∼91% of the bene-
ﬁt of exact search while reducing compute requirements by
35.8× ∼4.6× depending on the trade-offs between speed
and performance. We also successfully apply LBS to a six-
and seven-card versions of Hanabi, where calculating the
exact belief distribution would be prohibitively expensive.
2. Related Work
2.1. Belief Modeling & Planning in POMDPs
Deep RL on POMDPs typically circumvents explicit belief
modeling by using a policy architecture such as an LSTM
that can condition its action on its history, allowing it to
implicitly operate on belief states (Hausknecht & Stone,
2015). ‘Blueprint’ policies used in this (and prior) work
take that approach, but this approach does not permit search
since search requires explicitly sampling from beliefs in or-
der to perform rollouts.
There has been extensive prior work on learning and plan-
ning in POMDPs. Since solving for optimal policies in
POMDPs is intractable for all but the smallest problems,
most work focuses on approximate solutions, including of-
ﬂine methods to compute approximate policies as well as
approximate search algorithms, although these are still typ-
ically restricted to small grid-world environments (Ross
et al., 2008).
One closely related approach is the Rollout algorithm
(Bertsekas & Castanon, 1999), which given an initial pol-
icy, computes Monte Carlo rollouts of the belief-space
MDP assuming that this policy is played going forward,
and plays the action with the highest expected value. In the
POMDP setting, rollouts occur in the MDP induced by the
belief states1.
There has been some prior work on search in large
POMDPs. Silver & Veness (2010) propose a method for
performing Monte Carlo Tree Search in large POMDPs
like Battleship and partially-observable PacMan. Instead
of maintaining exact beliefs, they approximate beliefs us-
ing a particle ﬁlter with Monte Carlo updates. Roy et al.
(2005) attempt to scale to large belief spaces by learning a
compressed representation of the beliefs and then perform-
ing Bayesian updates in this space.
Most recently MuZero combines RL and MCTS with a
learned implicit model of the environment (Schrittwieser
et al., 2019). Since recurrent models can implicitly op-
erate on belief states in partially-observed environments,
MuZero in effect performs search with implicit learned be-
liefs as well as a learned environment model.
2.2. Games & Hanabi
Search has been responsible for many breakthroughs on
benchmark games. Most of these successes were achieved
in fully observable games such as Backgammon (Tesauro,
1994), Chess (Campbell et al., 2002) and Go (Silver et al.,
2016; 2017; 2018).
More recently, belief-based search
techniques have been scaled to large games, leading to
breakthroughs in poker (Moravˇc´ık et al., 2017; Brown &
Sandholm, 2017; 2019) and the cooperative game Han-
abi (Lerer et al., 2020), as well as large improvements in
Bridge (Tian et al., 2020).
There has been a growing body of work developing agents
in the card game Hanabi.
While early hobbyist agents
codiﬁed human conventions with some search (O’Dwyer,
2019; Wu, 2018), more recent work has focused on Han-
abi as a challenge problem for learning in cooperative
partially-observed games (Bard et al., 2020). In the self-
play setting (two copies of the agent playing together),
the Bayesian Action Decoder (BAD) was the ﬁrst learn-
ing agent (Foerster et al., 2019), which was improved upon
by the Simpliﬁed Action Decoder (SAD) (Hu & Foerster,
2020). The state-of-the-art in Hanabi self-play is achieved
by the SPARTA Monte Carlo search algorithm applied to
the SAD blueprint policy (Lerer et al., 2020).
There has been recent work on ad-hoc team play and zero-
shot coordination, in which agents are paired with unknown
partners (Canaan et al., 2019; Walton-Rivers et al., 2017;
Hu et al., 2020).
1SPARTA’s single agent search uses a similar strategy in the
DEC-POMDP setting, but samples states from the beliefs rather
than doing rollouts directly in belief space.

Learned Belief Search: Efﬁciently Improving Policies in Partially Observable Settings
3. Setting and Background
In this paper we consider a Dec-POMDP, in which N
agents each take actions ai
t at each timestep, after which
the state st updates to st+1 based on the conditional proba-
bility P(st+1|st, at) and the agents receive a (joint) reward
r(st, at), where at is the joint action of all agents.
Since the environment is partially observed each agent only
obtains the observation oi
t = Z(st, i) from a deterministic
observation function Z. We denote the environment trajec-
tory as τt = {s0, a0, ...st, at} and the action-observation
history (AOH) of agent i as τ i
t = {oi
0, ai
0, ..., oi
t, ai
t}. The
total forward looking return from time t for a trajectory
τ is Rt(τ) = P
t′≥t γt′−tr(st, at), where γ is an op-
tional discount factor. Each agent chooses a policy πi(τ i)
conditioned on its AOHs, with the goal that the joint pol-
icy π = {πi} maximises the total expected return Jπ =
Eτ∼P (τ|π)R(τ).
Starting from a common knowledge blueprint (BP), i.e. a
predetermined policy that is known to all players, in or-
der to perform search in a partially observable setting,
agents will need to maintain beliefs about the state of the
world given their observations. We deﬁne beliefs Bi(τt) =
P((st, {τ j
t })|τ i
t), which is the probability distribution over
states and AOHs, given player i’s private trajectory. Note
that in Dec-POMDPs the beliefs must model other agents’
AOHs as well as the current state, since in general the poli-
cies of other players condition on these AOHs.
In general, the domain of Bi (the range) is extremely large,
but in Dec-POMDPs with a limited amount of hidden in-
formation there is often a more compact representation.
For example, in card games like Hanabi, the range con-
sists of the unobserved cards in players’ hands. SPARTA
(Lerer et al., 2020) assumed that the domain was small
enough to be explicitly enumerated. In our case, we as-
sume that the beliefs are over private features f i that can
be encoded as a sequence of tokens from some vocabulary:
f i = Q
j f i
j ∈V. Furthermore, it simpliﬁes the algorithm
if, as is typically the case, we can factor out these private
features from τ to produce a public trajectory τ pub; then
each τ i can be speciﬁed by the pair (τ pub, f i) and τ by
(τ pub, f 1, ..., f N).
LBS is applicable to general POMDPs, which are a natural
corner case of the Dec-POMDP formalism when we set the
number of players to 1.
4. Method
The starting point for our method is the single-agent search
version of SPARTA (Lerer et al., 2020): Given a set of
BP policies, πi, we estimate expected returns, Q(ai|τ i), by
sampling possible trajectories τ from a belief conditioned
on τ i:
Q(ai|τ i) = Eτ∼P (τ|τ i)Q(ai|τ)
(1)
Here Q(ai|τ) is the expected return for playing action ai
given the history τ:
Q(ai|τ) = Eτ ′∼P (τ ′|τ,ai)Rt(τ ′),
(2)
where Rt(τ ′) is the Monte Carlo forward looking return
from time t.
Whenever the argmax of Q(ai|τ i) exceeds the expected re-
turn of the BP, Q(aBP |τ i), by more than a threshold δ, the
search-player deviates from the BP and plays the argmax
instead. For more details, please see Figure 1 (LHS) and
the original paper.
Even though this is single-agent search, in SPARTA this
process is costly: First of all, the belief P(τ|τ i) is an ex-
act counterfactual belief, which requires evaluating the BP
at every timestep for every possible τ to identify which
of these are consistent with the actions taken by the other
agents. Secondly, to estimate Rt(τ ′) SPARTA plays full
episodes (rollouts) until the end of the game.
Lastly, since in Dec-POMDPs policies condition on full
AOHs (typically implemented via RNNs) the games have
to be replayed from the beginning for each of the sampled τ
to obtain the correct hidden state, h(τ i(τ)), for all players
i, for each of the sampled trajectories, τ.
Learned Belief Search addresses all of these issues: First of
all, rather than using costly exact beliefs, we use supervised
learning (SL) to train an auto-regressive belief model which
predicts P(τ|τ i). As described in Section 3, in our set-
ting this reduces to predicting the private observations for
all other players f −i, since the public trajectory is known.
We use an auto-regressive model to encode the private ob-
servations as it can be decomposed to sequence of tokens
f −i = Q
j f −i
j . For scalability, as illustrated in Figure 2
(RHS), the auto-regressive belief model is parameterized
by a neural network with weights φ:
Pexact(f −i|τ i) →Pφ(f −i|τ i) =
Y
j
P(f −i
j |f −i
<j, τ i).
(3)
Secondly, to avoid having to unroll episodes until the end
of the game we use a learned value function to bootstrap
the expected return after a predeﬁned number, N, of steps.
While this value function in general needs to be trained via
SL, this is not necessary in our setting: Since our BPs are
recurrent DQN agents that learn an approximate expected
return via value-decomposition networks (VDN), we can
directly use the BP to estimate the expected return, as illus-

Learned Belief Search: Efﬁciently Improving Policies in Partially Observable Settings
SPARTA
LBS
P1 private
observation history 
Sampled
P2 hand
ℎ!"#
3
1
ℎ$ (&)
!()*
ℎ+
!()*
3
1
P1 private
observation history 
Sampled
P2 hand & history
…
3
1
3
1
4
1
Possible Hidden States
ℎ$ (&)
!()*
ℎ$ (,)
!()*
ℎ$ (-)
!()*
33%
50%
16%
…
Play Card 1
Discard Card 1
Hint Red
Full rollout
24
22
25
Full rollout
Full rollout
Play Card 1
Discard Card 1
Hint Red
N-step rollout
N-step rollout
N-step rollout
23.8
22.4
24.6
Final Scores:
max
.
𝑄(𝑠, 𝑎):
Belief
Model
Figure 1: Comparison of SPARTA (Lerer et al., 2020) and LBS (ours). SPARTA maintains an explicit belief distribution with an
accompanying AOH hpriv
2
for each belief state. LBS uses an auto-regressive belief model to sample states from the belief distribution,
given the AOH. AOHs do not need to be maintained for each belief state in LBS since the model only relies on the public trajectory.
Additionally, LBS uses an N-step rollout followed by a bootstrap value estimate.
trated in Figure 1 (B) :
Rt(τ ′) ≃
t+N
X
t′=t
rt′ +
X
i
Qi
BP (ai
BP |f i
t+N, τ pub
t+N)|τ ′. (4)
This is a fairly general insight, since RL training in
POMDPs commonly involves centralized value functions
that correctly capture the dependency of the expected re-
turn on the central state.
Lastly, we address a challenge that is speciﬁc to Dec-
POMDPs: To avoid having to re-unroll the policies for
the other agents from the beginning of the game for each
of the sampled τ, LBS uses a speciﬁc RNN architecture.
Inspired by other public-private methods (Foerster et al.,
2019; Kovaˇr´ık et al., 2019; Hor´ak & Boˇsansk`y, 2019), we
exploit that the public trajectory in combination with the
current private observation contains the same information
as the entire private trajectory and only feed the public
information τ pub into the RNN. The public hidden state
h(τ pub
t
) is then combined with the private observation f i
t
through a feedforward neural network, as illustrated in Fig-
ure 2 (LHS).
π(τ i
t) →π(h(τ pub
t
), f i
t)
(5)
We note that whenever it is possible to factor out the public
trajectory, this architecture can be used. If not, LBS can
still be used, but instead for each sampled f i we would
need to reconstruct the entire game from scratch to obtain
the correct model state.
We also point out that in this paper we only consider sin-
gle player search where all others act according to the BP.
Carrying out search for more than one player would not be
theoretically sound because the trained belief model would
no longer be accurate. Crucially, the learned belief model
can only provide accurate implicit beliefs when the other
players are playing according to the blueprint policy, be-
cause the belief model is trained before any agent conducts
search. As a consequence, doing independent LBS for mul-
tiple players at the same time would lead to inaccurate be-
liefs. As we show later in the paper, it will lower the per-
formance. Further details, including speciﬁc architectures,
of our three innovations are included in Section 5.
5. Experimental Setup
We evaluate our methods in Hanabi, a partially observable
fully cooperative multi-step board game. In Hanabi, the
deck consists of ﬁve different colors and ﬁve ranks from
1 to 5. For each color. There are three 1s, two 2s, 3s, 4s
and one 5, totaling 50 cards. At beginning of a game, each
player draw 5 cards. Each player cannot observe their own
cards but instead can see other players’ cards. Players take

Learned Belief Search: Efﬁciently Improving Policies in Partially Observable Settings
LSTM
x
MLP
Dueling Net
Auto-Regressive Belief Model
(trained by SL on final agents)
Encoder LSTM
ℎ!
"
ℎ!#$
"
𝑥!
(history encoding)
Decoder 
LSTM
5
1
3
1
5
Decoder 
LSTM
Decoder 
LSTM
𝑥!
Action Q Values
Public Observations
(board cards, discards, 
common knowledge V0 
beliefs)
Private Observations
(Partner’s hand)
ℎ!
ℎ!#$
Policy Model
(trained by RL)
𝑥!
𝑥!
sample card
sample card
sample card
Player’s Public &
Private Observations
5
1
3
Sampled Hidden Info
(Player Hand)
Play Card 1
Discard Card 1
Hint Red
24.0
22.1
18.3
…
ℎ$
%
ℎ&
%
Auto-Regressive Decoder
(A)
(B)
Figure 2: (A): Illustration of the public-LSTM network used for the BP policy. (B): The auto-regressive network for modeling beliefs .
turn to move and the goal for the team is to play cards of
each color from 1 to 5 in the correct order to get points. At
each turn, the active player can either play a card, discard a
card, or select a partner and reveal information about their
cards. Playing a wrong card will cause the entire team to
lose 1 life token. The game will terminate early if all 3 life
tokens are exhausted and the team will get 0 point. To re-
veal information, the active player can either choose a color
or a rank and every card of that color/rank will be revealed
to the chosen player. The game starts with 8 information
tokens. Each reveal action costs 1 token and each discard
action regains 1 token. Players will draw a new card af-
ter play/discard action until the deck is ﬁnished. After the
entire deck is drawn, each player has one last turn before
the game ends. For simplicity we focus on 2-player Hanabi
for all our experiments and note that it is straightforward to
extend our method to any number of players.
5.1. Blueprint Training
As explained in Section 3 the public and private observa-
tions in Hanabi can be factorized into public and private
features. We modify the open-source Hanabi Learning En-
vironment (HLE) (Bard et al., 2020) to implement this.
Here, the only private observation is the partner’s hand,
while all other information is public. There are many dif-
ferent options for implementing the public-RNN concept.
We end up with the design shown in Figure 2 (A). Specif-
ically, the LSTM only takes public features as input while
an additional MLP takes in the concatenation of private
and public features. The outputs of the two streams are
fused through element-wise multiplication before feeding
into the dueling architecture (Wang et al., 2016) to produce
Q values. We have also experimented with other designs
such as using concatenation in place of the element-wise
multiplication, or feeding only private observation to the
MLP. Empirically we ﬁnd that the design chosen performs
the best, achieving the highest score in self-play.
We use the distributed training setting described in (Hu
et al., 2021). We follow their hyper-parameters but replace
the network with the one described above. The training
jobs run with 40 CPU cores and 3 GPUs.
5.2. Belief Training
The belief model is trained to predict the player’s own hand
given their action observation history. An overview of the
architecture is shown in the right panel of Fig 2. An en-
coder LSTM converts the sequence of observations to a
context vector. The model then predicts its own hand in
an auto-regressive fashion from oldest card to newest. The
input at each step is the concatenation of the context vector
and the embedding of the last predicted card. The model is
trained end-to-end with maximum likelihood:
L(c1:n|τ) = −
n
X
i=1
log p(ci|τ, c1:i−1),
(6)

Learned Belief Search: Efﬁciently Improving Policies in Partially Observable Settings
where n is the number of cards in hand and ci is the value
of the i-th card.
We use a setup similar to that of reinforcement learning
to train the belief model instead of a more traditional way
of creating a ﬁxed train, test and validation set. We use a
trained policy and a large number of parallel Hanabi sim-
ulators to continuously collect trajectories and write them
into a replay buffer. In parallel we sample from the replay
buffer to train the belief model using a supervised loss. This
helps us easily avoid over-ﬁtting without manually tuning
hyper-parameters and regularization. The RL policy used
to generate data is ﬁxed during the entire process. It takes
20 CPU cores and 2 GPUs to train the belief models.
5.3. LBS Implementation Details
Learned Belief Search is straightforward once we have
trained a BP and a belief model. The search player samples
hands from the belief model and ﬁlters out the ones that are
inconsistent with current game status based on their private
observation. In the extremely rare case where the belief
model fails to produce a sufﬁcient number of legal hands, it
reverts back to the BP. To understand the impact of various
design parameters, we experimented with both playing out
all trajectories until the end of the game as well as rolling
out for a ﬁxed number of steps and using a bootstrapped
value estimate at the ﬁnal state. Similar to SPARTA, the
search actor only deviates from the BP if the expected value
of the action chosen by search is δ = 0.05 higher than that
of the BP and a UCB-like pruning method is used to reduce
the number of samples required. All search experiments are
executed using 5 CPU cores, 1 GPU and 64GB of memory.
6. Results
In this section, we ﬁrst show the performance and run time
of our method compared with blueprint baseline and ex-
act search method, SPARTA. We then study the quality of
the learned belief by comparing it to two analytical bench-
marks. Finally, we study trade-offs between training and
testing under a ﬁxed budget.
6.1. Performance
Table 1 demonstrates the scalability of LBS compared to
SPARTA in environments involving different amount of
hidden information. We test them on modiﬁed variants of
Hanabi where each player holds 6 or 7 cards as well as the
ofﬁcial version of 5 cards. In these experiments we use
setting mentioned in Section 5.1 to train BP with RL and
then train belief model until convergence following Sec-
tion 5.2. It worth noting that it is easier to achieve a higher
score with bigger hand size as on average more informa-
tion will be revealed per hint. In fact, we need to reduce
Figure 3: Comparison of speed and average score of different
policies in 5-card (top) and 6-card (bottom) Hanabi. The number
next to each LBS data point is the rollout depth. LBS with differ-
ent rollout depths provide a tunable trade-off between speed and
performance. LBS provides most of the performance beneﬁts of
exact search (SPARTA) at a fraction of the compute cost, and the
speedup grows with the size of the belief space (5-card vs 6-card).
the maximum number of hints from 8 to 4 in the 7 card
variant to prevent the RL blueprint policy from being al-
most perfect.
For comparison, we run SPARTA on our
models which can be seen as a expensive upper bound for
LBS. For LBS, we roll out for 16 steps before bootstrap-
ping from the Q values. In the standard Hanabi, LBS de-
livers 90% of the performance boost compared to SPARTA
while being 4.6× faster. In the 6-card variant, the SPARTA
method runs 8× slower than it does on standard Hanabi
while LBS runs faster (likely due to shorter games), deliv-
ering 76% of the improvement with 42× less time. In the 7-
card variant, the SPARTA exhausts the 64GB of memory as
the storage required for all possible hands and correspond-
ing LSTM states grow exponentially w.r.t. hand size. LBS,
on the other hand, scales well and delivers strong perfor-
mance. Note that the run time of LBS on the 7-card variant
is slower partially due to a small change in the implemen-
tation to accommodate larger hand.
Figure 3 summarizes the speed/performance trade-offs of
LBS compared to running an RL policy directly or using
an exact search method (SPARTA). In 5-card Hanabi (left),
we start with a blueprint policy that achieves an average

Learned Belief Search: Efﬁciently Improving Policies in Partially Observable Settings
Variant
Blueprint
SPARTA
LBS
5-card
24.08 ± 0.01
(<1s)
24.52 ± 0.01
(215s)
24.48 ± 0.01
(47s)
6-card
24.57 ± 0.01
(<1s)
24.82 ± 0.01
(1747s)
24.76 ± 0.01
(42s)
7-card
23.67 ± 0.02
(<1s)
Out-of-Mem Error
24.26 ± 0.01
(58s)
Table 1: Result on different Hanabi variants. Each cell contains the mean and standard error of mean (s.e.m.) over 5000 games. It also
shows the average time it takes to run a game in the parentheses next to the performance. Note that for the 7-card variant, we reduce the
maximum number of hints to 4 in order to make the game harder. All the experiments are executed using 5 CPU cores, 1 Nvidia Quadro
GP100 GPU and 64GB of memory. SPARTA fails to run on the 7-card variant due to out-of-memory error since the entire belief space
is too large to ﬁt into the allocated memory.
Method
Depth
Time
Blueprint Strength
Weak
Medium
Strong
Best
Blueprint
<1s
15.38 ± 0.05
22.99 ± 0.03
24.08 ± 0.01
24.42 ± 0.01
SPARTA
215s
19.53 ± 0.03
24.16 ± 0.02
24.52 ± 0.01
24.66 ± 0.01
LBS
∞
121s
18.88 ± 0.03
23.95 ± 0.02
24.42 ± 0.01
24.59 ± 0.01
LBS
32
84s
19.05 ± 0.03
24.01 ± 0.02
24.45 ± 0.01
24.60 ± 0.01
LBS
16
47s
19.27 ± 0.03
24.04 ± 0.02
24.48 ± 0.01
24.62 ± 0.01
LBS
8
25s
19.14 ± 0.03
24.03 ± 0.02
24.43 ± 0.01
24.59 ± 0.01
LBS
4
14s
18.75 ± 0.03
23.95 ± 0.02
24.41 ± 0.01
24.55 ± 0.01
LBS
2
9s
18.26 ± 0.04
23.81 ± 0.02
24.35 ± 0.01
24.44 ± 0.02
LBS
1
6s
17.99 ± 0.04
23.69 ± 0.02
24.26 ± 0.02
24.43 ± 0.02
Table 2: Average scores in 2-player Hanabi with different search variants. Time column shows the average wall-clock time of each
method to play a game. Weak, Medium and Strong blueprints uses 512 hidden units per layer and are trained with RL method after 1
hour, 5 hours, and 20 hours respectively. The Best blueprint is a larger model with 1024 hidden units each layer trained for 72 hours.
Each cell contains the mean and standard error of mean averaged over 5000 games. LBS variants achieve a substantial fraction of the
policy improvements of SPARTA over the blueprint at a lower computational cost. Both overall best results (from SPARTA) and the best
LBS results are bold for better readability. Note that the Time column does not reﬂect the time for the more expensive Best blueprint.
score of 22.99 after 5 hours of training, and apply LBS with
different rollout lengths as well as an exact search. LBS
achieves most of the score improvement of exact search
while being much faster to execute.
Changing the roll-
out depth allows for tuning speed vs. performance. As
we move to a larger game (6-card Hanabi), SPARTA be-
comes 10× slower while LBS runs at approximately the
same speed and continues to provide a similar fraction of
the performance improvement.
As mentioned in Section 4, applying LBS independently
on multiple players will not lead to better performance due
to the inaccurate beliefs. To verify this, we run LBS on
both players in standard 5-card Hanabi. The result is 24.34
± 0.02, which is noticeably worse than the 24.48 ± 0.01
achieved by single agent LBS.
A more comprehensive analysis of these trade-offs is
shown in Table 2. We train the BP with RL for a total of 20
hours and use 3 snapshots to demonstrate the performance
of LBS given BPs of different strength. Weak, Medium and
Strong policies are snapshots after 1 hour, 5 hours and 20
hours respectively. The network uses 512 units for both
LSTM and feed-forward layers.
The belief models are
trained to convergence for each BP. All search methods run
10K rollouts per step. The best performing variant is LBS
with 16-step rollouts, delivering 91% of the improvement
on average comparing to the exact search while being 4.6×
faster. Even the cheapest method, LBS with 1-step roll-
outs, returns a decent improvement of 55% on average and
35.8× speedup. It also worth noting that LBS performs
strongly even on the weakest policy, demonstrating high
versatility of the method.
To further test the limit of self-play in Hanabi, we train a
larger model with 1024 units per layer for an extend period
of 72 hours and apply both LBS and SPARTA on top. The
model is dubbed Best in Table 2. The BP itself gets 24.42
and SPARTA further boosts the performance to 24.66, the
highest self-play score ever reported in 2 player Hanabi.

Learned Belief Search: Efﬁciently Improving Policies in Partially Observable Settings
Figure 4: Per-card cross entropy with the true hand for different
beliefs in games played by BP.
Remarkably, LBS achieves 24.62, delivering 83% fo the
beneﬁt of SPARTA while taking less than 1/4 of the time
(69s for LBS and 364s for SPARTA).
We note that the more expensive LBS that rolls out until the
end of the games (LBS-∞) is not the best performing one.
It consistently under-performs some of the LBS-k variants
by a small, in many cases signiﬁcant, amount. We hypoth-
esis that under LBS-∞it is more likely for the agents to
reach a state that is under-explored during training. There-
fore the approximate belief will be less accurate and the
estimate Q-value be wrong. The LBS-k methods where the
Q-value after k steps is used as a bootstrap may naturally
avoid those situations since the BP may also have low Q
values for under-explored states. One piece of evidence for
this theory is that in LBS-∞, 0.1% of the games end up
with a completely failed belief prediction and have to re-
vert back to BP while the same only happens to 0.0083%
of the games for LBS-k.
Clearly, this is a potential problem: part of the reason
search works well is that it can discover moves that were
underestimated and, consequently, under-explored by the
BP. The brittleness of the learned belief, in combination
with the difference in overall belief quality (Fig 4), help ex-
plain the difference in performance between LBS and exact
methods like SPARTA.
6.2. Belief Quality
We examine the quality of the learned belief model by look-
ing at its cross entropy loss for predicting hidden card val-
ues. We compare against two benchmarks: the exact beliefs
marginalized over each card, and an auto-regressive belief
based only on grounded information. The grounded belief
predicts a distribution over card values proportional to re-
maining card counts, for all card values consistent with past
hints.
Figure 5: Result of ﬁxed budget (24 hours) at training time. The
ticks “a|b”on x-axis means a hours to train BP and b hours to train
belief model.
Figure 6: Result of ﬁxed budget at test time. The ticks “a|b”
on x-axis means run search for a steps before bootstrapping with
Q function and run b search per move. Each data point on both
ﬁgures is evaluated on 5000 games and the shaded area is the
standard error of mean.
We generate 1000 games with the strong policy and com-
pute the two benchmarks together with the loss of the
learned belief (Eq. 6) from a fully trained belief model.
Figure 4 shows how these 3 values change over the course
of games. We see that our learned belief model performs
considerably better than the grounded belief. There is still
a gap between learned belief and exact belief, especially in
the later stage of the game. More powerful models such
as transformers (Vaswani et al., 2017) may further improve
the belief learning but we leave it for future work.
6.3. Fixed Budget Training & Testing
Since one of the motivations for LBS is speed, it would
be interesting to know how we could allocate resources
at training and test time to maximize performance given

Learned Belief Search: Efﬁciently Improving Policies in Partially Observable Settings
a ﬁxed computational budget. For ﬁxed training budget,
we train the BP for l hours and then train belief model for
24 −l hours. We evaluate these combinations with LBS-
∞as well as BPs themselves. As shown in Figure 5, with
longer RL training, the BP improves monotonically, but the
ﬁnal performance suffers due to a poor belief model. The
best combination is ∼18 hours for RL and ∼6 hours for
belief learning.
We then take the models from the best combination to study
how to allocate compute between the number of rollouts
and rollout depth. We start with LBS-1 and 128K search
per step, and then halve the number of searches as we dou-
ble the search depth. The result is shown in Figure 6. If we
compare the results here with those from Table 2, we see
that although LBS-16 still has the best performance, the rel-
ative strength between LBS-32 and LBS-8 ﬂips, indicating
that the trade-off may still matter in some cases.
7. Conclusion
We presented Learned Belief Search, a novel search algo-
rithms for POMDPs that can be used to improve upon the
performance of a blueprint policy at test time whenever a
simulator of the environment is available. We also pre-
sented extensions of LBS that make it applicable to fully
cooperative, partially observable multi-agent settings. The
heart of LBS is an auto-regressive model that can be used to
generate samples from an approximate belief for any given
AOH.
While LBS achieves strong performance on the benchmark
problem Hanabi, our work also clearly points to a num-
ber of future directions.
For a start, the search process
can bring the belief model to under-explored regions of the
state space. This could be addressed by retraining the belief
model on the data generated from LBS.
Another interesting direction for future work is to amortize
the search process, e.g. by integrating it into the training
process, and to extend LBS to multiplayer and multi-step
search. To enable these directions, and many others, we
plan to open-source all of the code for LBS.
References
Bard, N., Foerster, J. N., Chandar, S., Burch, N., Lanc-
tot, M., Song, H. F., Parisotto, E., Dumoulin, V., Moitra,
S., Hughes, E., Dunning, I., Mourad, S., Larochelle, H.,
Bellemare, M. G., and Bowling, M. The hanabi chal-
lenge: A new frontier for ai research. Artiﬁcial Intelli-
gence, 280:103216, 2020. ISSN 0004-3702.
Bertsekas, D. P. and Castanon, D. A. Rollout algorithms for
stochastic scheduling problems. Journal of Heuristics, 5
(1):89–108, 1999.
Brown, N. and Sandholm, T. Superhuman AI for heads-up
no-limit poker: Libratus beats top professionals.
Sci-
ence, pp. eaao1733, 2017.
Brown, N. and Sandholm, T. Superhuman AI for multi-
player poker. Science, pp. eaay2400, 2019.
Campbell, M., Hoane Jr, A. J., and Hsu, F.-h. Deep Blue.
Artiﬁcial intelligence, 134(1-2):57–83, 2002.
Canaan, R., Togelius, J., Nealen, A., and Menzel, S. Di-
verse agents for ad-hoc cooperation in hanabi. In IEEE
Conference on Games 2019, CoG 2019, IEEE Confer-
ence on Computatonal Intelligence and Games, CIG.
IEEE Computer Society, August 2019. doi: 10.1109/
CIG.2019.8847944. 2019 IEEE Conference on Games,
CoG 2019 ; Conference date: 20-08-2019 Through 23-
08-2019.
Foerster, J., Song, F., Hughes, E., Burch, N., Dunning, I.,
Whiteson, S., Botvinick, M., and Bowling, M. Bayesian
action decoder for deep multi-agent reinforcement learn-
ing. In International Conference on Machine Learning,
pp. 1942–1951, 2019.
Hausknecht, M. and Stone, P. Deep recurrent q-learning
for partially observable mdps. In 2015 AAAI Fall Sym-
posium Series, 2015.
Hor´ak, K. and Boˇsansk`y, B. Solving partially observable
stochastic games with public observations. In Proceed-
ings of the AAAI Conference on Artiﬁcial Intelligence,
volume 33, pp. 2029–2036, 2019.
Hu, H. and Foerster, J. N.
Simpliﬁed action decoder
for deep multi-agent reinforcement learning.
In In-
ternational Conference on Learning Representations,
2020. URL https://openreview.net/forum?
id=B1xm3RVtwB.
Hu, H., Peysakhovich, A., Lerer, A., and Foerster, J.
“other-play”for zero-shot coordination. In Proceedings
of Machine Learning and Systems 2020, pp. 9396–9407.
2020.
Hu, H., Lerer, A., Cui, B., Pineda, L., Wu, D., Brown,
N., and Foerster, J. N.
Off-belief learning.
CoRR,
abs/2103.04000, 2021. URL https://arxiv.org/
abs/2103.04000.
Kovaˇr´ık, V., Schmid, M., Burch, N., Bowling, M., and
Lis`y, V.
Rethinking formal models of partially ob-
servable multiagent decision making.
arXiv preprint
arXiv:1906.11110, 2019.
Lerer, A., Hu, H., Foerster, J. N., and Brown, N. Improv-
ing policies via search in cooperative partially observ-
able games. In AAAI, pp. 7187–7194, 2020.

Learned Belief Search: Efﬁciently Improving Policies in Partially Observable Settings
Moravˇc´ık, M., Schmid, M., Burch, N., Lis`y, V., Morrill,
D., Bard, N., Davis, T., Waugh, K., Johanson, M., and
Bowling, M. Deepstack: Expert-level artiﬁcial intelli-
gence in heads-up no-limit poker. Science, 356(6337):
508–513, 2017.
O’Dwyer, A.
Hanabi.
https://github.com/
Quuxplusone/Hanabi, 2019.
Ross, S., Pineau, J., Paquet, S., and Chaib-Draa, B. Online
planning algorithms for pomdps. Journal of Artiﬁcial
Intelligence Research, 32:663–704, 2008.
Roy, N., Gordon, G., and Thrun, S. Finding approximate
pomdp solutions through belief compression. Journal of
artiﬁcial intelligence research, 23:1–40, 2005.
Schrittwieser, J., Antonoglou, I., Hubert, T., Simonyan, K.,
Sifre, L., Schmitt, S., Guez, A., Lockhart, E., Hassabis,
D., Graepel, T., et al.
Mastering atari, go, chess and
shogi by planning with a learned model. arXiv preprint
arXiv:1911.08265, 2019.
Silver, D. and Veness, J.
Monte-carlo planning in large
pomdps. In Advances in neural information processing
systems, pp. 2164–2172, 2010.
Silver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L.,
Van Den Driessche, G., Schrittwieser, J., Antonoglou, I.,
Panneershelvam, V., Lanctot, M., et al. Mastering the
game of go with deep neural networks and tree search.
Nature, 529(7587):484, 2016.
Silver, D., Schrittwieser, J., Simonyan, K., Antonoglou, I.,
Huang, A., Guez, A., Hubert, T., Baker, L., Lai, M.,
Bolton, A., et al. Mastering the game of go without hu-
man knowledge. Nature, 550(7676):354, 2017.
Silver, D., Hubert, T., Schrittwieser, J., Antonoglou, I., Lai,
M., Guez, A., Lanctot, M., Sifre, L., Kumaran, D., Grae-
pel, T., et al. A general reinforcement learning algorithm
that masters chess, shogi, and go through self-play. Sci-
ence, 362(6419):1140–1144, 2018.
Tesauro, G.
TD-Gammon, a self-teaching backgammon
program, achieves master-level play. Neural computa-
tion, 6(2):215–219, 1994.
Tian, Y., Gong, Q., and Jiang, T.
Joint policy search
for multi-agent collaboration with imperfect informa-
tion. arXiv preprint arXiv:2008.06495, 2020.
Vaswani,
A.,
Shazeer,
N.,
Parmar,
N.,
Uszkoreit,
J., Jones, L., Gomez, A. N., Kaiser, L. u., and
Polosukhin,
I.
Attention is all you need.
In
Guyon, I., Luxburg, U. V., Bengio, S., Wallach,
H., Fergus, R., Vishwanathan, S., and Garnett, R.
(eds.), Advances in Neural Information Processing
Systems 30, pp. 5998–6008. Curran Associates, Inc.,
2017.
URL http://papers.nips.cc/paper/
7181-attention-is-all-you-need.pdf.
Walton-Rivers, J., Williams, P. R., Bartle, R., Perez-
Liebana, D., and Lucas, S. M. Evaluating and modelling
hanabi-playing agents. In IEEE Congress on Evolution-
ary Computation (CEC), pp. 1382–1389, 2017.
Wang, Z., Schaul, T., Hessel, M., Hasselt, H., Lanc-
tot, M., and Freitas, N.
Dueling network architec-
tures for deep reinforcement learning.
volume 48
of Proceedings of Machine Learning Research, pp.
1995–2003, New York, New York, USA, 20–22 Jun
2016. PMLR. URL http://proceedings.mlr.
press/v48/wangf16.html.
Wu, D.
A rewrite of hanabi-bot in scala.
https:
//github.com/lightvector/fireflower,
2018.

