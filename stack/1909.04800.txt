Probabilistic framework for solving Visual Dialog
Badri N. Patro1 , Anupriy2, Vinay P. Namboodiri2
1 Department of Electrical Engineering, Indian Institute of Technology Kanpur, India
2 Department of Computer Science and Engineering, Indian Institute of Technology Kanpur, India
Abstract
In this paper, we propose a probabilistic framework for solving the task of ‘Visual Di-
alog’. Solving this task requires reasoning and understanding of visual modality, lan-
guage modality, and common sense knowledge to answer. Various architectures have
been proposed to solve this task by variants of multi-modal deep learning techniques
that combine visual and language representations. However, we believe that it is crucial
to understand and analyze the sources of uncertainty for solving this task. Our approach
allows for estimating uncertainty and also aids a diverse generation of answers. The
proposed approach is obtained through a probabilistic representation module that pro-
vides us with representations for image, question and conversation history, a module
that ensures that diverse latent representations for candidate answers are obtained given
the probabilistic representations and an uncertainty representation module that chooses
the appropriate answer that minimizes uncertainty. We thoroughly evaluate the model
with a detailed ablation analysis, comparison with state of the art and visualization of
the uncertainty that aids in the understanding of the method. Using the proposed prob-
abilistic framework, we thus obtain an improved visual dialog system that is also more
explainable.
Keywords: CNN, LSTM, Uncertainty, Aleatoric uncertainty, Epistemic Uncertainty
Vision and Language, Visual Dialog, VQA, Answer Generation, Question Generation,
Bayesian Deep Learning.
Email address: (badri,anupriy,vinaypn)@iitk.ac.in (Badri N. Patro1 , Anupriy2,
Vinay P. Namboodiri2)
Preprint submitted to Journal
October 18, 2019
arXiv:1909.04800v2  [cs.CV]  17 Oct 2019

1. Introduction
Deep learning has aided signiﬁcant progress in solving various computer vision
tasks such as object classiﬁcation [1, 2] and object detection [3, 4]. The solution of
more semantic tasks such as visual question answering [5, 6] and image captioning
[7, 8] has also seen progress lately. A challenging problem that extends these is that
of maintaining a dialog with a user [9, 10] In this case, a system is required to main-
tain context concerning the history of the conversation while answering a question and
this is be more challenging. A speciﬁc task in the visual context is that of the ‘Visual
Dialog’ task [10]. The aim here is that given an image, we need to train an agent to
maintain a dialog. The motivation for this emerges from an interest in developing as-
sociative technologies for visually impaired persons or chat-bot based dialog agents.
Several methods have been proposed for solving the task, such as using various dis-
criminative and generative encoder-decoder frameworks that aim to solve the task of
generating dialog [10, 9]. In this paper we aim to extend the previous approaches by
formulating a probabilistic approach towards solving this task. This approach is illus-
trated in ﬁgure 1. Through our approach we can obtain a principled model that we
can train end-to-end while being able to have uncertainty estimates and the ability to
evaluate and explain the model. Such an ability to explain the model is crucial, espe-
cially, when we consider that the method could be used by visually impaired people.
At any point in the method, the model can be probed to ensure that it is certain about
the answers that it generates and more importantly any failure of the method can be
addressed by explaining the precise reason for failure as shown in 2. However, as the
task is challenging it is important to have an insight into obtaining estimates regarding
the uncertainty of the model. This would aid in knowing when the method is conﬁdent
about its prediction. In this paper, we consider the task of understanding the uncer-
tainty while solving the ‘Visual Dialog’ task, as shown in 2. This proposed method
addresses the limitations of the previous approaches as the previous approaches do not
have the ability to obtain uncertainty estimates or obtain diverse conversations. Our
method consists of the following parts :
• Probabilistic Representation Module: Through this module, we obtain proba-
2

Figure 1: Proposed Probabilistic Diversity and Uncertainty Network (PDUN) consists of three parts, viz. a)
Probabilistic Representation Module encodes image feature with a question and history feature in an attentive
manner. b) Diversity module captures the diversity, and diverse answer is generated using Variational Auto-
Encoder. c) Uncertainty module predicts uncertainty of the network.
bilistic representations for image, question, and history of the conversation using
Bayesian CNN and Bayesian RNN modules.
• Diverse Latent Answer Generation Module: In this module, we use a variational
autoencoder based latent representation that allows us to obtain latent represen-
tations from which we can sample answers.
• Uncertainty Representation Module: In this module, we propose a Reverse Un-
certainty based Attention Map (RUAM) method by using Bayesian deep learning
methods that allows us to minimize data uncertainty and model uncertainty.
To provide an overview of the technical contributions we make, the main idea is
to consider incorporating a Gaussian prior for generating samples of answers. We
minimize the KL divergence between the prior and the posterior distribution. The other
contribution is to explicitly incorporate a loss to ensure that the correlation between
different samples is minimal. We further use these losses along with a loss to minimize
the uncertainty. A similar loss has been considered in another context by Patro et
al. [11]. In this work we are interested in a principled framework for minimizing
uncertainty by sampling and generating diverse answers. Moreover, its use has not
been considered for the problem of visual dialog. We evaluate each of the contributions
3

in our work. The technical details mentioned here are discussed further in detail in the
following sections.
Figure 2: Results were showing the certainty of the correct class increases from baseline[9] to our proposed
uncertainty model (PDUN). In this ﬁgure, we show the top 2 class conﬁdence score of the question, ”Is this
a park?”. In the baseline model focus on woman, guitar and chair and predicts ”NO,” which is confused with
the correct prediction of the answer, whether it is a park or not. PDUN model minimizes the uncertainty and
predicts the correct answer ”Yes” with a high conﬁdence score.
To summarize, the main contributions of this paper are as follows :
1. We provide a module to obtain a probabilistic representation for image, question
and conversation history that are obtained as input.
2. The probabilistic representations are used to generate diverse latent representa-
tions for candidate answers
3. We propose a method for obtaining reverse uncertainty based on attention maps
(RUAM). These allow us to select an appropriate answer that minimizes uncer-
tainty.
4. We provide extensive analysis and comparison of our framework with previous
methods and evaluate the various ablations of the method. Our proposed frame-
work improves recall@10 score by 3.5%, mean reciprocal rank (MRR) by 1%
and NDGC score by 1%. Moreover, we also provide estimation & visualization
of the uncertainty of the output.
2. Related Work
A novel problem, ‘Visual dialog’ has been introduced recently [9]. In this problem,
we aim to answer a question given the context of a conversation about an image. This
is one of the latest challenging problems that has been posed in the ﬁeld of vision and
language. There have been various other related problems considered in the ﬁeld of
4

vision and language. One of the earliest such problems is that of image captioning
where we aim to generate a sentence describing an image [12, 13, 14, 15, 7, 8, 16,
17, 18]. Further, the community moved on to answer questions based on an image in
the visual question answering (VQA) task [19, 5, 20, 21, 22, 23, 24, 25, 26, 27, 28,
29, 30]. Another interesting problem that has been addressed is that of visual question
generation (VQG) [31, 32, 33, 34], where the aim is that given an image to generate
natural questions similar to that asked by humans. The solution of the visual dialog
problem builds up on the previous work conducted for solving the various problems
described above.
Visual dialog task requires the agents to have meaningful dialog conversation about
the visual content. This task was introduced by Das et al. [10]. The authors have pro-
posed three approaches, namely late fusion in which all the history rounds are concate-
nated, attention-based hierarchical LSTM which handles variable length history and
memory-based method for performing results best in terms of accuracy for solving this
task. Following up, [35, 36] have proposed generator and discriminator based archi-
tecture. Of these, Lu et al. [35] consider an attention based method to combine all
history rounds to get a single representation. Further works [37, 38, 9] have proposed
visual dialog as an image guessing game. The latest work on the visual dialog to obtain
state of the art results has been proposed by Jain et al. [39]. This work is based on
discriminative question generation and answering. In another work, Jain et al. [32]
have proposed a method to bring diversity in the question generation from an image
using Variational Auto-encoder (VAE). Wang et al. [40] have proposed a similar kind
of method to generate a caption from an image using VAE. In related works, [41, 42]
have captured diversity in the caption generation from an image using generative ad-
versary network. In contrast to these earlier works, in our framework we consider a
fully probabilistic framework for solving the task of visual dialog.
We use Bayesian CNN in our work for obtaining probabilistic image representa-
tions. Modeling distribution over CNN ﬁlters is still a difﬁcult task. Due to the large
number of parameters to be inferred, the posterior distribution becomes intractable. To
approximate this posterior distribution, the variational inference is one of the existing
approaches introduced by [43, 44, 45, 46]. Gaussian distribution is the simplest varia-
5

tional approximation used to ﬁt the model parameters to the true distribution of param-
eters, but it is computationally expensive [46]. This can be overcome using Bernoulli
approximation.
There has been some work done in terms of estimating uncertainty in the predic-
tions using deep learning the work by [47] estimates the predictive variance of the deep
network with the help of dropout [48]. [49] has proposed a method to capture model
uncertainty for image segmentation task. They observed that softmax probability func-
tion approximates relative probability between the class labels, but does not provide
information about the model’s uncertainty. Recently, [50] has decomposed predictive
uncertainty into two major types, namely aleatoric and epistemic uncertainty, which
capture uncertainty[51, 52] about the predicted model and uncertainty present in the
data itself. Here, our objective is to generate diverse answer, to analyze and minimize
the uncertainty in answer data, and to analyze the uncertainty of the model for the chal-
lenging visual dialog task. We build up on the techniques proposed in several such
works to obtain a fully probabilistic framework for solving the visual dialog problem.
In the next section we consider the background in terms of Bayesian modeling required
for obtaining our probabilistic framework.
3. Background: Bayesian Approach of Model
Consider the distribution p(x, y) over the input features x and labels y. For the
visual dialog classiﬁcation task, x corresponding to joint encoding feature of image,
history and query question and y answer class label. For the given observation, X
and its corresponding output Y . In a Bayesian framework, the predictive uncertainty
of the classiﬁcation model (p(y∗|x∗, D))1 is trained on a ﬁnite set of training data
D = {xi, yi}N
i=1. The predictive uncertainty will result in data(aleatoric) uncertainty
and the model(epistemic) uncertainty. The model estimates two kinds of uncertainty,
i.e., data uncertainty and model uncertainty. The posterior distribution describes the
data uncertainty over class labels, given set of model parameters w, and the model
1Standard shorthand notation for p(y = y∗|x∗, X, Y ) = p(y∗|x∗, D)
6

uncertainty is described by the posterior distribution over model parameters w, given
input data. The predictive uncertainty for new example point x∗by integrating over all
possible set of parameters w is given by
p(y∗|x∗, X, Y ) =
Z
p(y∗|x∗, w)
|
{z
}
Data
p(w|X, Y )
|
{z
}
Model
dw
(1)
Our main objective is to ﬁnd the best set of weights of our model that will generate
our data X, Y. One of the approaches to make Bayesian inference is to compute the
posterior distribution overweights, i.e., p(w|X, Y ). This distribution captures the best
plausible set of model parameters given our observed data.
p(w|X, Y ) = p(Y |X, W)p(W)/p(Y |X)
It is challenging to perform inference over the Bayesian network because the marginal
probability p(Y |X) of the posterior cannot be evaluated analytically. So, the posterior
distribution p(w|X, Y ) is intractable. To approximate the intractable posterior distri-
bution, various approximation approaches are proposed in [53, 47, 46]. Variational
inference is one of the approximating technique, where the posterior p(w|X, Y ) is
approximated by a simple distribution qθ(W), where θ is the parameterized by vari-
ational parameter p(w|X, Y ) ≈qθ(W). We thus minimize the KullbackLeibler(KL)
divergence between approximate distribution qθ(w) and the posterior p(w|X, Y ) w.r.t
θ, which is denoted by KL(qθ(w)||p(w|X, Y )).
KL(qθ(w)||p(w|X, Y )) ∝−
Z
qθ(w) log p(Y |X, w)dw + KL(qθ(w)||p(w))
= −
N
X
i=1
Z
qθ(w) log p(yi|f
ˆ
W (xi))dw + KL(qθ(w)||p(w))
Minimizing the KL divergence is equivalent to maximizing the log evidence lower
bound [53] with respect to the variational parameters deﬁning qθ(w),
LV I =
Z
qθ(w) log p(Y |X, w)dw −KL(qθ(w)||p(w))
(2)
The intractable posterior problem i.e., averaging over all the weight of the BNN, is re-
placed by the simple distribution function. Now we need to optimize the parameter of
7

simple distribution function instead of optimizing the original neural network’s param-
eters. Furthermore the integral in equation 1 (predictive posterior) is also intractable
for the neural network, which is approximated via sampling using Monte Carlo dropout
[47] or Langevin Dynamics [54] or explicit ensembling [55]. So we approximate the
integral with Monte Carlo sampling.
p(y∗= c|x∗, X, Y ) =
Z
p(y∗= c|x∗, w)p(w|X, Y )dw
≈
Z
p(y∗= c|x∗, w)qθ(w)dw
≈1
M
M
X
i=1
p(y∗= c|x∗, w(i))q(w(i))
(3)
where w(i) ∼q(w(i)), which is modeled by the dropout distribution and M samples
of w(i) is obtained. , each p(y∗|x∗, w(i)) in an ensemble p(y∗|x∗, w(i))
M
i=1 obtained
sampled from q(w(i)). In the following section, we have discussed Bayesian CNN and
Bayesian LSTM.
3.1. Bayesian CNN
One way to deﬁne a Bayesian neural network [47] is to place a prior distribu-
tion over neural network weights, w = (Wi)L
i=1. Given weight matrix Wi and bias
bi for ith layer, we use standard Gaussian prior distribution over the weight matrix
p0(Wi) = N(Wi; 0, 1). The variational Bayesian approximation in a Bayesian neu-
ral network can be interpreted as adding stochastic regularization in the deterministic
neural network. The stochastic regularization technique is equivalent to multiplying
random noise ϵi with neural network weight matrices Mi.
Wi = Mi · diag([ϵi,j]Ki
j=1)
ϵi,j ∼Bernoulli(pi), i = {1, ., L}, j = {1, ., Ki−1}
(4)
where, ϵi,j is a Bernoulli distributed random variable with probability pi. The diag(.)
operator maps vectors to diagonal matrices, whose diagonal elements are the elements
of the vectors. The set of variational parameters Mi is now the set of matrices θ =
{Mi}L
i=1. The binary variable ϵi,j = 0 indicates the corresponding element j in the
8

layer i −1 is dropped out as an input to layer i. In CNN with dropout [56], the forward
propagation is formulated as,
mi
k ∼Bernoulli(pi)
ˆai
k = ai
k ∗mi
k
zi+1
j
=
n(l)
X
k=1
Conv(W l+1
j
, ˆai
k)
(5)
Here ai
k denotes the activations of feature map k (k = 1, 2, , n(l)) at layer l. The
mask matrix ml
k consists of independent Bernoulli variables ml
k(i). This mask is sam-
pled and multiplied with activations in kth feature map at layer l, to produce dropout-
modiﬁed activations ˆai
k. These modiﬁed activations are convolved with ﬁlter W l+1
j
to produce convolved features zi+1
j
. The function f is applied element wise to the
convolved features to get the activations of convolutional layers.
3.2. Bayesian LSTM
The conventional LSTM is a neural network that maps LSTM state st (at time step
t) and input xt to a new LSTM state st+1, fl : (st, xt) →st+1. The state of LSTM is
given by st = (ct, ht), where ct is a memory state and ht is the output of the hidden
state. To train a input sequence of length T, x1, x2..., xT , the LSTM cell is unrolled T
times in to a feed forward network with initial state s0 and can be represented by
sj = fl(sj−1, xj)
In Bayesian LSTM, let p(y∗|w, x∗) be the likelihood of the neural network, then the
posterior of the network is approximated to q(w) by minimizing the variational free
energy L(w)[57, 58]. Minimizing the variational free energy is equivalent to maxi-
mizing the likelihood log p(y|x, w) subject to KL divergence, which regularizes the
parameters of the network.
L(w) = −Eq(w)[ log p(y∗
1:T |x∗
1:T , w)] + KL(q(w)||p(w))
Here log p(y1:T |x1:T , w) is the likelihood function of the sequence and the expec-
tation in the previous equation is approximated by the Monte Carlo sampling. The
9

predictive posterior for LSTM is calculated just as in equation 3 by,
p(y∗
1:T |x∗
1:T , X, Y ) =
Z
p(y∗
1:T |x∗
1:T , w)p(w|X, Y )dw
≈1
M
M
X
m=1
p(y∗
1:T |x∗
1:T , ˆwm)dw
(6)
with ˆwm ∼qθ(w), where qθ(w) is called the dropout distribution for LSTM.
4. Methods
Visual dialog task is introduced by [10]. The visual dialog task is deﬁned as, given
image I, a caption C, a dialog history till t−1 rounds, H = {C, (q1, a1), ....(qt−1, at−1)
and the following question qt at round t. The objective of the visual dialog agent is to
predict a natural language answer to the question qt. The visual dialog problem can
be solved into two possible ways; one is by using a generative model and the other
by using a discriminative model. In a generative model, given the embeddings of im-
age, history, and question(qt), a generative model is trained to maximize the likelihood
function to predict ground truth answer sequence. The discriminative model receives
embedding of an image, history, and question(qt) along with 100 candidate answers
At = {a1
t, ...a100
t
} and effectively learns to rank the list of candidate answers.
One aspect of previous approaches tends to be a lack of diverse generations of an-
swers; for instance, the tendency to correlate the animal ‘zebra’ with black and white
stripes. In contrast, during conversations, a conversation is interesting if an unexpected
or novel observation is raised. In our method, we hope to produce such insights. To
do that we need an ability to characterize the space of all possible answers. We do that
by using a Gaussian prior for the generation of answers. This allows us to generate
samples of plausible answers. We then further use a diversity loss that would penalize
correlations between the multiple samples. The ﬁnal task then lies to choose an appro-
priate retort or response. To do so, we rely on minimizing uncertainty while generating
the answer. We now consider the proposed approach in detail.
Our method consists of three parts viz. 1) Probabilistic Representation Module, 2)
Latent feature-based Diverse Answer Generation Module, and 3) Uncertainty Module
as illustrated in ﬁgure 3 :
10

Figure 3: Probabilistic Diversity Uncertainty Network(PDUN), Bayesian CNN/LSTM is used to obtain the
embeddings gi, fi, hi which is then fused using the Fusion Module to get ef. Then correlation is found
between fused embedding with answer option embedding. Finally, variance and logits output are obtained
using MLP, which is then used in Logits Reparameterization Trick(LRT) to get ﬁnal softmax output.
4.1. Probabilistic Representation Module
We adopt a probabilistic representation module that has been previously considered
in Patro et al. [11] for the VQA task. However, using this representation for visual di-
alog requires us to also consider the history of previous dialogs as a new input. By
using a probabilistic representation, we are able to investigate the uncertainty in any
part of the full proposed model. To obtain this, we use the methods of Bayesian CNN
and Bayesian LSTM that has been discussed in the background section. Given an input
image xi, we obtain an image embedding gi by using a Bayesian CNN that we parame-
terized through a function G(xi, Wi), where Wi are the weights of the Bayesian CNN.
We extract gi ∈Rw×h×c dimensional CONV-5 feature from Bayesian CNN network
as shown in ﬁgure 3. We obtain gq, gh encoding feature for given question and history,
after passing through an LSTM (Bayesian LSTM Network), which is parameterized
using the function Gq(XW E, θl), where θl are the weights of the LSTM as shown in
ﬁgure 3. Similarly, we obtain answer embedding Ga parameterised by G(Xa, θa).
After this, the question and answer embedding are combined to obtain a history em-
bedding. To model the Bayesian CNN [59], we use pre-trained CNN layers and put
dropout layer with dropout rate p, before each CNN layer. Similarly for Bayesian:
LSTM [58], we add dropout on each input and a hidden layer of the LSTM cell. These
are input to an attention network that combines question-answer pair with previous
history embedding using a weighted softmax function and produces a weighted output
11

attention vector gf. There are various ways of modeling the attention network. In this
paper, we have evaluated the network proposed in SAN [60]. In the last round, we
combine image embedding with the last history embedding to get a dialog context vec-
tor. At each round, we attend over the question representation with the previous history
(combined question-answer representation). In the ﬁrst round, the previous history is
an encoded caption feature. In the ﬁnal round, we attend to image representation with
the appropriate history representation to obtain an attentive encoder feature, gf. The
attention mechanism is illustrated as follows:
ga = tanh(Wcgi + Wq(gq||gh) + bc)
α = Softmax(Waga + ba)
(7)
where || means concatenation, Wa, Wc, Wq, bc, ba are the weights and bias of different
layers.
4.2. Latent feature based Diverse Answer Generation Module
This module mainly focuses on representing a latent representative vector from
attentive encoder module and generate a diverse answer using answer generator. We
use the VAE [61] based generative framework to generate diverse answer from the
attentive encoder. We obtain mean, µ = Wµgf and log variance, log σ2 = Wσgf,
where µ and σ are the parameters of a multivariate Gaussian distribution. We train
this network to learn a variational distribution which is close to a prior deﬁned by the
normal distribution with zero mean and unit variance i.e., N(0, 1). Then we obtain a
latent vector representation z by using the reparameterization trick z = µ + ϵ ⊙σ.
The major concern for answer generation is the spread of the variance in the latent
representation. Our main objective is to increase the spread in the Gaussian as much as
possible for generating diverse answers. If the spread of the Gaussian is too low (∼0),
then we have sampled similar answers, and if the variance is too high (∼∞), then we
have sampled from a uniform distribution. Hence, we want to put some constraints
on variance such that our sampled latent representations are as diverse as possible. A
diversity loss which minimizes the correlation between the latent representations is
introduced to ensure this. Let us deﬁne z1 and z2 as the two latent vectors randomly
12

sampled from the N(µ, σ), z1 = µ + ϵ1 ⊙σ, z2 = µ + ϵ2 ⊙σ, where ϵ1 and ϵ2 are
sampled from N(0, 1) and µ and σ are Gaussian parameters. The diversity loss is given
by
Ldiverse =
⟨(z1 −α), (z2 −α)⟩
max(||z1 −α||2 · ||z1 −α||2, γ)
(8)
Where γ = 10−8 is used to avoid division by zero, and α is the average of all the z sam-
ples. Similarly, we obtain an average loss for k sample points (in our experiments, we
choose k to be 100) randomly sampled from the latent distribution. This loss ensures
that these latent vectors are as far as possible.
Finally, the diverse latent feature is input to an LSTM based answer decoder module
to generate diverse answers. The softmax probability for the predicted answer token at
different time steps is given by the following equations:
h0 = Zi = N(µ, σ)
xt = We ∗at, ∀t ∈{0, 1, 2, ...T −1}
ht+1 = LSTM(xt, ht), ∀t ∈{0, 1, 2, ...T −1}
ˆyt+1 = softmax(Wo ∗ht+1)
LCE = −1
C
C
X
j=1
yj log p(ˆyj|fo)
where ˆyt+1 is the predicted answer class and fo is the context. Now, we classify the
generated diverse answer among 100 classes in order to rank them with 100 ground
truth answer options. We use a reasoning network to perform reasoning by predicting
an answer and comparing it with the ground truth answer to obtain the ﬁnal score. A
similar approach has been used by Das et al. [9].
4.3. Uncertainty Representation Module
Through the previous module, we obtain the ability to generate diverse answers.
The task then is to choose an appropriate answer that is correct. To do that, we use an
uncertainty representation module that characterizes the uncertainty among the diverse
set of candidate answers (i.e., the classes present in the answers). We want to be certain
about the response that is chosen. That is, we would like to minimize the uncertainty.
We do that by using an explicit loss for reducing uncertainty.
13

In this work, we also incorporate the attention regions, which speciﬁes the spatially
distributed weights given to a speciﬁc region embedding while generating the answer.
To obtain the best embedding, we consider the ground-truth answer and through at-
tention, consider the corresponding spatial location. This region is multiplied with the
uncertainty for generating the spatial attention weighted uncertainty corresponding to
the ground-truth answer. We want to increase the weight for the spatial attention cor-
responding to generating the ground-truth answer and minimize the uncertainty for the
same. At the same time, we would like to increase the uncertainty for all other an-
swers and minimize the weight given in terms of attention to all other regions. We
achieve this through a reverse uncertainty based attention map (RUAM) that is shown
in ﬁgure 4.
Figure 4: Reverse Uncertainty based Attention Map (RUAM): We obtain attention embedding fi from the
attention network Gf using image, question and history embeddings gi, gq, gh. Then we classify into
answer class and obtain the uncertainty present in the data. Then we obtain reverse uncertainty map with
will combine with attention map to get better conﬁdence on the attention map as shown in the ﬁgure.
Reverse Uncertainty based Attention Map (RUAM): Patro et al.[11] have pro-
posed a model to estimate aleatoric and predictive uncertainty for Visual Question An-
swering task, where the gradient of uncertainty loss and gradient of classiﬁcation is
multiplied to improve attention feature. Kurmi et al. [62] have also proposed a sim-
ilar kind of network in the domain adaption task, where they train the discriminator
network to reduce uncertainty in source and target domain. We follow a similar type
14

of network in a visual dialog task to reduce uncertainty in the attention mask with the
help of a predicted answer in the dialog turns. We stress more on those attention re-
gions whose uncertainty is less and vice-versa. The aleatoric uncertainty occurs due to
corruption in the feature or noise in the attention regions. These regions are the main
source of predicting the wrong answer in the visual dialog.
We adopt a Bayesian framework to predict answer classiﬁcation uncertainty efﬁ-
ciently. We make our answer classiﬁer as Bayesian and perform probabilistic inference
over the classiﬁer to obtain the ﬁnal answer score. We adopt a Bayesian classiﬁer as
considered in several works [57, 47, 58, 62, 11]. The Bayesian classiﬁer is obtained
by applying dropout after every fully connected (FC) layer in the classiﬁer and the
Bayesian classiﬁer predicts answer class logits yi,d and aleatoric uncertainties. These
are obtained as follows:
yi,d = Gy(Gd(fi)),
σi,d = Gv(Gd(fi))
(9)
where fi is the attention feature for input image sample xi, question sample xq and
history sample xh, which is obtained by the attention feature extractor Gf : fi =
Gf(Gi(xi), Gq(xq), Gh(xh), where Gy and Gv are the logits and aleatoric variance
predictor of the classiﬁer Gd respectively. The whole model is trained with the help of
uncertainty loss (More details are present in 4.4) and cross-entropy loss. The uncer-
tainty loss helps the classiﬁer to make the classiﬁer features more robust for prediction.
Finally, we measure the uncertainty for our answer prediction and found it to be lower.
We learn and estimate observational noise parameter σi,d to capture the uncertainty
present in the input data (Image, History and Question). This can be achieved by
corrupting the logit value (yi,d) with the Gaussian noise with variance σi,d (diagonal
matrix with one element for each logits value) before the softmax layer. We used a
Logit Reparameterization Trick (LRT) [63], which combines two outputs yi,d, σi,d and
then we obtain a loss with respect to ground truth. That is, after combining we get
N(yi,d, (σi,d)2) which is expressed as:
ˆyi,t,d = yi,d + ϵt,d ⊙σi,d,
where
ϵt,d ∼N(0, 1)
(10)
15

Lu =
X
i
log 1
T
X
t
exp (ˆyi,t,M −log
X
M ‘
exp ˆyi,t,M ‘)
(11)
where M
′ is a discrete word token present in each sample sentence. yi,t. M is a
discrete word token present in the ground truth sentence, Lv is minimized for the true
work token M, and t ∈T is the number of Monte Carlo simulations. σi,d is the standard
deviation, ( σi,d = √vi,d).
Now, we obtain uncertainty for attention map αatt ∈Ru×v of width u and height
v using following steps such as, we ﬁrst compute gradient of the predictive uncertainty
σ2
g of our generator with respect to the features fi. This gradient of the uncertainty loss
Lu with respect to the attention feature fi is given by ∂Lv
∂fi . Now we pass the uncertainty
gradient through a gradient reversal layer to reverse the gradient to get certainty mask
for the attention map. This is given by
∇u = −γ ∗∂Lu
∂fi
We perform an element-wise multiplication of the forward attention feature map and
reverse uncertainty gradients to get an enhanced attention feature map i.e.
α
′
u,v = −γ ∗∂Lu
∂fi
∗αu,v
(12)
The positive sign of the gradient γ indicates that the aleatoric certainty is activated on
these regions and vice-versa. We apply a ReLU activation function on the product of
gradients of the attention map and the gradients of aleatoric certainty as we are only
interested in attention regions that have a positive inﬂuence for a corresponding answer
class, i.e. attention pixels whose intensity should be increased in order to increase yc,
where negative values are multiplied by γ (large negative number). Negative attention
pixels are likely to belong to other categories in the image.
α
′′
u,v = ReLU(α
′
u,v) + γReLU(−α
′
u,v)
(13)
Images with higher aleatoric uncertainty correspond to lower certainty. Therefore the
certain regions of these images should have lower attention values. We use residual
connection to obtain the ﬁnal attention feature by combining original attention feature
16

with the reverse uncertainty map α
′′
u,v. This is given by:
αnew = αu,v + α
′′
u,v ∗αu,v
f
′
i =
X
u,v
gi ∗αnew
(14)
Where, gi ∈Gi(xi). The ﬁnal attention feature (f
′′
i ) can be obtained by combining
attention feature (fi) with RUAM based new attention feature (f
′
i).
f
′′
i = fi + f
′
i
(15)
We show here, that using reverse uncertainty based attention Map (RUAM) results in
an improved attention network and the attention conﬁdence also increases. The entropy
and predicted variance of the sampled logit’s probability can be calculated as:
H(ˆyi,t) = −
M
X
m=1
p(ˆyi,t = M) ∗log p(ˆyi,t = M)
(16)
The predictive uncertainty is the combination of entropy and variance of T sample
outputs (of randomly masked model weights).
σ2
p = 1
T
T
X
t=1
H(ˆyi,t) + 1
T
T
X
t=1
v2
i,t
(17)
Where H(ˆyi,t) is the entropy of the probability p(ˆyc
i,t), which depends on the spread
of the probabilities and the variance captures both the spread and the magnitude of
outcome values ˆyi,t. Algorithm-1 explains details about reverse uncertainty map for
attention mask.
4.4. Cost Function
Finally, we trained our complete PDUN model with the help of answer generation
loss and uncertainty loss. The answer generation loss Lgen is the combination of cross
entropy loss LCE, to generate each and every token in the answer sequence, KL diver-
gence loss LKL, to bring the approximate posterior closer to N(0, 1), and the diversity
loss Ldiv 8, to ensure diverse answer generation. The cost function used for obtaining
the parameters θf of the attention network, θc of the classiﬁcation network, θy of the
17

prediction network and θu for uncertainty network is as follows:
C(θf, θc, θy, θu) = 1
n
n
X
j=1
Lj
y(θf, θc, θy)+Lj
KL(θf, θc)+Lj
div(θf, θc)+ηLj
u(θf, θc, θu)
where n is the number of examples, and η is a hyper-parameter that is ﬁne-tuned using
validation set and Lc is standard cross entropy loss. We train the model with this cost
function till it converges so that the parameters (ˆθf, ˆθc, ˆθy, ˆθu) deliver a saddle point
function
(ˆθf, ˆθc, ˆθy, ˆθu) = arg
max
θf ,θc,θy,θu(C(θf, θc, θy, θu))
(18)
Figure 5: This shows the measurement of Entropy, Variation, Softmax scores and Bayesian loss for Bayesian
model with dropout value 0.5 and 0.1 in ﬁrst and second plot respectively for capturing Epistemic Uncertainty
5. Experiments
We evaluate the proposed method in the following steps: First, we evaluate our
proposed uncertainty model against other variants described in section 5.2. Second, we
have shown analysis results for epistemic uncertainty in ﬁgure-5 and aleatoric uncer-
tainty in ﬁgure-6 and in table 2. Third, we further analyze effect of noise in aleatoric
and epistemic uncertainty in table 3. Fourth, we compare diversity score for different
18

Algorithm 1 Reverse Uncertainty based Attention Map (RUAM)
1: procedure RUAM(I, Q, H)
2:
Input: Image XI, Question XQ, History XH
3:
Output: Answer y
4:
while loop do
5:
Attention features Gf(Gi(XI), Gq(XQ), GH(XH)) ←fi
6:
Answer Logit Gy(Gd(fi)) ←ˆy
7:
Data Uncertainty Gu(Gd(fi)) ←σ2
A
8:
σ2
W = σ2
A + H(ˆyi,t), (Ref: eq- 4)
9:
Ans cross entropy Ly ←loss(ˆy, y)
10:
Variance Equalizer [64] LV E := P ReLU(expσ2
w −expI),
11:
while t = 1 : #MC −Samples do
12:
Sample ϵw
t ∼N(0, σ2
W )
13:
Distorted Logits:ˆyi,t = ϵw
t + ˆyi
14:
Gaussian Cross Entropy [64] LGCE = −P y log p( ˆyd|F(.))
15:
Distorted Loss :LUDL = exp(Ly −LGCE)2
16:
Aleatoric uncertainty loss Lu = LGCE + LVE + LUDL
17:
Compute Reverse Gradients w.r.t fi, ∇u = −λ ∗∂LU
∂fi
18:
Certainty Activation for attention α
′
u,v = ∇u ∗αu,v
19:
Certainty Activation for attention α
′′
u,v = ReLU(α
′
u,v)+γ∗ReLU(−α
′
u,v)
20:
New Attention gradient αnew = αu,v + α
′′
u,v ∗αu,v
21:
New attended feature: f
′
i = P
u,v fi ∗αnew
22:
Final attended feature: f
′′
i = fi + f
′
i
23:
update θf ←θf −η∇
′
y
variants of our model in table 5. Fifth, we compare our network with state-of-the-art
methods such as ‘visdial’ [10] in table 4. Then, we have shown the Grad-CAM [65] vi-
sualization of activation due to aleatoric uncertainty and baseline model (late fusion).
We further compare our network with state-of-the-art methods such as visdial [10]
model. Finally, we have provided some qualitative results of our visual dialog model.
The quantitative evaluation is conducted using standard retrieval metrics, namely (1)
19

mean rank, (2) recall @k, (3) mean reciprocal rank (MRR) of the human response in
the returned sorted list.
Figure 6: Upper Graph: This shows how aleatoric uncertainty loss varies over different turns in visual
dialog. There is an eventual decreasing trend. Lower Graph: This shows how variance equalizer loss varies
over different turns in visual dialog. There is an eventual increasing trend.
20

Loss
R1
R5
R10
MRR
Mean
Baseline
40.9
72.4
82.8
0.550
05.95
VE
30.9
51.4
62.5
0.371
12.41
CE
41.1
72.6
82.9
0.556
05.98
GCE
43.8
77.2
88.0
0.587
04.79
CE+VE
42.3
74.6
84.7
0.551
05.50
VE+GCE
44.3
79.5
89.6
0.599
04.41
CE+GCE
45.4
80.6
91.2
0.610
03.95
ACE
47.0
82.4
92.3
0.622
03.81
Table 1: Aleatoric loss variant in VisDial-v1.0 in test-std dataset.
5.1. Dataset
We evaluate our proposed approach by conducting experiments on Visual Dialog
dataset [10], which contains human annotated questions based on images of MS-COCO
dataset. This dataset was developed by pairing two subjects on Amazon Mechanical
Turk to chat about an image. One person was assigned the job of a ‘questioner’ and the
other person act as an ‘answerer’. The questioner sees only the text description of an
image which is present in caption from MS-COCO dataset. The image remains hidden
to the questioner. Their task is to ask questions about this hidden image to “imagine
the scene better”. The answerer sees the image and caption and answers the questions
asked by the questioner. The two of them can continue the conversation by asking and
answering questions for 10 rounds at max. We have performed experiments on “Vis-
Dial 1.0” version of the dataset. “VisDial v1.0” contains 123k dialogs on COCO-train
and 2k on “VisualDialog val2018” images for val and 8k on “VisualDialog test2018”
for test-standard set. The caption is considered to be the ﬁrst round in the dialog history.
5.2. Ablation Analysis on Model Parameter for Uncertainty
Aleatoric Cross Entropy consists of distorted (Gaussian Cross Entropy (GCE)),
undistorted (Cross Entropy (CE)) loss, and Variance Equalizer (VE) loss.. The ﬁrst
block of the table 1 analyses individual loss function and its comparison is provided
in that table. We use these models as our baseline and compare other variations of
21

Type of Uncertainty
Mean
Std
Aleatoric (with CE)
0.0051
8.677
Aleatoric (with VE)
0.0044
7.353
Aleatoric (with GCE)
0.0039
3.431
Aleatoric (with ACE)
0.0032
2.119
Epistemic (50% training)
0.6680
66.9321
Epistemic (75% training)
0.6310
42.8923
Epistemic (100% training)
0.5520
36.8110
Table 2: Aleatoric & Epistemic uncertainty measurement score.
our model with the best single loss function. The GCE loss performs best among
all the 3 losses. This is reasonable as GCE can guide the loss function to minimize
the variance in the data. The second block of table 1 depicts the models which uses
combination of the loss function as variations of our method such as GCE, VE or CE.
We see an improvement of around 4% in R@1, 8% in R@5 score, 9% in R@10 score
and 5% in MRR score from the baseline score. The combination of GCE loss and
CE performs best among all the 3 cases. The third block takes into consideration all
the loss functions ACE (GCE+CE+VE) and we see an improvement of around 6% in
R@1, 10% in R@5 score, 10% in R@10 score and 7% in MRR score from the baseline
score. The behaviour of dialog turn for a particular example is shown in 6. The ﬁrst
part of the ﬁgure 6 shows, how aleatoric uncertainty loss varies over different turns in
visual dialog. As dialog progress the width of the dialog turn decreases. There is an
eventual decreasing trend. The second part of the ﬁgure shows how variance equalizer
loss varies over different turns in visual dialog. There is an eventual increasing trend.
We can observe the third and fourth turn is more uncertain. This indicates that to have
a successful dialog, it basically depend on the central part of the dialog not only start
and end tuns of the dialog.
5.2.1. Analysis of Epistemic Uncertainty
One of the main purposes of the Bayesian deep learning is that it improves both the
predictions and the uncertainty estimates of the model. We have measured uncertainty
22

score in terms of mean and variance for all the dialog prediction in “val-v1.0” dataset.
We have also measured uncertainty for a single dialog in the dataset. Here, we split our
training data into three parts. In ﬁrst part the model is trained with 50% of the training
data. Then, second part is trained by 75% of training data and third part is trained
by full dataset as shown in second block of the table 2. It is observed the epistemic
uncertainty decreases as training data increases.
Type of Uncertainty
Mean
Std
Aleatoric (original)
0.0067
8.956
Aleatoric (γ = 0.8)
0.0123
11.717
Aleatoric (γ = 1.2)
0.0034
6.353
Epistemic (original)
0.671
70.213
Epistemic (γ = 0.5)
0.701
71.893
Epistemic (γ = 0.8)
0.646
69.117
Table 3: Ablation study on Noise parameters.
Figure 7: Bayesian CNN experiment based on dropout, Max-pooling and average pooling. We found out
that Architecture 5 works best and we used it throughout our experiments.
23

5.2.2. Analysis of Aleatoric Uncertainty
Here, we have captured data uncertainty by checking contribution of each terms in
aleatoric uncertainty as shown in ﬁrst block of the table 2. From the measurements,
it can be easily seen that comparing aleatoric uncertainty of an image with epistemic
uncertainty of another image doesn’t make sense because of signiﬁcant difference in
their values. But both the uncertainties can be separately compared for different images
to see which answer is more uncertain.
5.2.3. Analysis of Noise in Aleatoric & Epistemic uncertainty
We have performed another ablation study for change in uncertainty based on noise
value. We estimated both aleatoric and epistemic uncertainty for visual dialog dataset.
We randomly selected 200 examples on val dataset and applied noise to the image and
question responses and observed that uncertainty value changes on seeing noise image
and noise question. So we applied noise value γ of 0.8 to decrease pixel value and
γ = 1.2 to increase pixel value i.e. there is inverse proportionality. Mean and standard
deviation of uncertainties are recorded in the table. From table 3, it can be observed that
aleatoric uncertainty is very small as compared to epistemic uncertainty. The aleatoric
uncertainty changes much rapidly as noise changes in comparison to that of epistemic
uncertainty.
(a)Distorted loss(GCE)
(b) Undistorted loss(CE)
(c) Variance Equalizer loss(VE)
Figure 8: This ﬁgure shows role of different types of Losses over Epochs. From the plot we observed that
variance is decreasing as it goes through more and more epochs.
5.2.4. Analysis of Epistemic Uncertainty : Dropout
We experimented with various dropout ratios and use the following values for the
same. For implementing Bayesian CNN, we used dropout ratio of (0.1, 0.2, 0.3, 0.4,
24

(a)Distorted Loss(GCE)
(b) Undistorted loss(CE)
(c) Variance Equalizer loss(VE)
Figure 9: We have shown the variance ﬂow plots over Epochs. This shows role of different type of loss over
epoch. from the plot we observed that variance is decreasing as it goes through more and more epochs.
0.5) for each stack of convolutional layers respectively and 0.5 for FC layers. As the
number of neurons increase in subsequent layers, we increase the dropout ratio for
better generalization. For Bayesian LSTM, we use dropout ratio 0.3 for input & hid-
den layers [66] and for output layer we have used 0.5 dropout ratio similar to [57].
We further experimented with different ways of placing the dropout layer in the CNN
architecture and observe that putting dropout after Max pooling layer works best.
5.2.5. Loss Visualization
We have analyzed the signiﬁcance of Distorted loss (Gaussian Cross Entropy (GCE)
Loss), Undistorted loss (Cross Entropy(CE) Loss) and Variance Equalizer (VE) loss as
shown in the Figure 8. It is clear form the ﬁgure that all the losses converges as epoch
progresses. Variance ﬂow in the various losses is shown in Figure 9. Also we have seen
same type of behavior in the variance plot. The variance decreases for all the losses as
training progresses.
5.3. Diversity
We used Singular value decomposition (SVD) based evaluation metric to demon-
strate diversity across various generated answers in a dialog. Here, we have randomly
selected 400 dialogs. For each dialog, we sampled m number of latent embedding
feature using the attentive encoder. Each one is having n-dimensional feature vector.
To measure the variance, all the answer embedding features can be concatenated into
a feature matrix A ∈Rm×n. σi can be obtained by SVD, L = U P V T of matrix
25

Models
R1
R5
R10
MRR
Mean
LF [10]
43.8
74.6
84.0
0.580
5.78
HRE [10]
44.8
74.8
84.3
0.586
5.65
MN [10]
45.5
76.2
85.3
0.596
5.46
HCIAE [35]
48.4
78.7
87.5
0.622
4.81
SF-1 [39]
48.1
78.6
87.5
0.620
4.79
AMEM [67]
48.5
78.6
87.4
0.622
4.85
NMN [68]
50.9
80.1
88.8
0.641
4.45
ECE (ours)
44.3
76.1
85.9
0.590
5.51
ACE (ours)
49.0
80.5
89.3
0.629
4.32
PDUN (ours)
49.2
81.0
90.5
0.634
4.03
Table 4: Results on dataset-v0.9 for Visual dialog
Model
Diversity(σ2
d)
VE
6.41
CE
22.231
GCE
27.845
ECE
24.980
ACE
32.12
PDUN
34.35
Table 5: Answer Diversity results for Visual dialog.
A, where P = diag(σ0, σ1....σn−1); U and V T are m × m and n × n unitary matri-
ces respectively. The overall variance in all dimensions is l1-norm, σo = Pn−1
i=0 |σi|.
A large variance indicates very less correlation among the generated answers, which
further implies large diversity among the answers as shown in table 5.
5.4. Comparison with state-of-the-art (SOTA)
The comparison of PDUN method with various state-of-the-art methods for visual
dialog dataset v0.9 and v1.0 are provided in table 4. The ﬁrst block of the table 4
consists of the state-of-the-art methods, second and third block consist of our methods.
We compared our results with baseline results of the model ‘Late-fusion-QIH’ [10].
26

Models Mean
R1
R5
R10
MRR
Mean
NDGC
LF [10]
40.9
72.4
82.8
0.55
5.95
0.45
HRE [10]
39.9
70.4
81.5
0.54
6.41
0.45
MN [10]
42.4
74.0
84.3
0.56
5.59
0.47
NMN [68]
47.5
78.1
88.8
0.61
4.40
0.54
ECE (ours)
43.6
75.4
85.3
0.58
5.36
0.49
ACE (ours)
47.0
82.4
92.3
0.62
3.81
0.53
PDUN (ours)
47.3
82.5
92.6
0.62
3.68
0.54
Table 6: Results on dataset v1.0 for Visual dialog
We observe that in terms of @R10 score, we obtain an improvement of around 10%
from the baseline & 3.5% from SOTA (NMN [68]) method. In terms of NDGC score
9% from base model & 0.5% from SOTA model and in term of MRR, 7% from the
base model & 1% from SOTA (NMN [68]) model using our proposed method. .
Figure 10: Figure shows the difference between aleatoric dialog results and baseline dialog results. In this
ﬁgure, the ﬁrst row refers to Grad-CAM visualization of ﬁrst example for baseline visual dialog model and
second row refers to Grad-CAM visualization of ﬁrst example for Aleatoric visual dialog model and same
scheme is followed for next 2 rows. The ﬁrst column indicates target Image and corresponding caption and
starting from second column is the visualization of rounds of dialog from round 1 to 10.
5.5. Qualitative Result
We provide qualitative results, which easily distinguishes between results of Base-
line dialog model with our aleatoric dialog model for two dialog generation example in
27

ﬁgure 10. We can clearly see that our proposed method are able to capture uncertainty
and minimize it, which further improves dialog results. For example, in the ﬁrst image,
the question is “Is this in a park ?”. The baseline model’s main focus is on the chair,
where the uncertainty is very high. But our model explains about ﬁeld, plant and back-
ground image, which provides the extra information about the query that eventually
decreases the uncertainty as shown in ﬁgure 10. We visualize the certainty activation
map of other two dialogs whose uncertainty score decrease over turns.
Figure 11: Difference Between Aleatoric dialog results and Baseline dialog results are shown in the ﬁgure.
In this ﬁgure,The ﬁrst row refer to Grad-CAM visualization of ﬁrst example for baseline visual dialog model
and second row refer to Grad-CAM visualization of ﬁrst example for aleatoric visual dialog model and so on..
The ﬁrst column indicates target Image and corresponding caption, second column indicates visualization of
dialog round 1, third column refer to visualization of dialog round 2 and so on.
Figure 12: This ﬁgure provide aleatoric and epidemic variance and visualize the aleatoric uncertainty using
Grad-CAM for a particular Dialog.
We provide qualitative results, which easily distinguishes between results of base-
line dialog model with our aleatoric dialog model for three dialog generation example
28

in ﬁgure 11. We can clearly see that our proposed method is able to capture uncertainty
and minimize it, which further improves dialog results. Also, we have measured epis-
temic and aleatoric uncertainty and showed how uncertainty decreases as dialog turns
in ﬁgure 12. We visualize the uncertainty by taking multiple samples and show how
does it change as per samples as shown in Figure 13. We also made the GIF version
of this visualization with name ‘aleatoric uncertainty ques gradcam 100.gif’ and other
visualisation ﬁle present in the following link2.
Figure 13: We visualize the multiple outputs from the Bayesian neural network. We took 100 sample from
the posterior distribution of dialog model for particular image, particular question. It shows how Grad-CAM
is ﬂowing for particular image, particular question.
5.6. Evaluation Protocol
We have followed the evaluation protocol mentioned in [10]. We use a retrieval set-
ting to evaluate the responses at each round in the dialog. Speciﬁcally, every question
2https://delta-lab-iitk.github.io/PDUN/
29

in VisDial is coupled with a list of 100 candidate answers, which the models are asked
to sort for evaluation purposes. Models are evaluated on standard retrieval metrics (1)
mean rank, (2) recall @k and (3) mean reciprocal rank (MRR) of the human response
in the returned sorted list.
5.7. Preprocessing
We truncate captions/questions/answers longer than 24/16/8 words respectively.
We then build a vocabulary of words that occur at least 5 times in train, resulting in
8964 words. In our experiments, all 3 Bayesian LSTMs are single layer with 512-
dimensional hidden state. For Bayesian CNN we use pretrained VGG-19 [1] with
dropout to get the representation of image. We ﬁrst re-scale the images to 448 × 448
pixels and take the output of FC7 layer which is 4096-dimensional as image feature.
We use the Adam optimizer with a base learning rate of 4e-4.
6. Conclusion
In this paper, we propose a novel probabilistic architecture that is termed as the
probabilistic diversity and uncertainty network (PDUN), for solving visual dialog. The
main parts in the proposed architecture are the modules that capture uncertainty and
diversity. We captured aleatoric and epistemic uncertainty that provide us with uncer-
tainty estimates and these are further reduced using appropriate loss functions. We have
particularly shown that the performance in the visual dialog is improved around 3.5%
by the proposed network. Further the use of the diversity module obtained through a
variational autoencoder allows us to generate diverse answers. We validate that indeed
the diversity of the proposed network is high as compared to variants of the method.
These two contributions enable us to obtain a signiﬁcantly improved model for solving
the challenging visual dialog task.
7. Acknowledgment
We acknowledge the help provided by our DelTA Lab members and our family
who have supported us in our research activity.
30

8. Reference
References
[1] K. Simonyan, A. Zisserman, Very deep convolutional networks for large-scale
image recognition, arXiv preprint arXiv:1409.1556.
[2] K. He, X. Zhang, S. Ren, J. Sun, Deep residual learning for image recognition, in:
Proceedings of the IEEE conference on computer vision and pattern recognition,
2016, pp. 770–778.
[3] S. Ren, K. He, R. Girshick, J. Sun, Faster r-cnn: Towards real-time object detec-
tion with region proposal networks, in: Advances in neural information process-
ing systems, 2015, pp. 91–99.
[4] Z.-Q. Zhao, P. Zheng, S.-t. Xu, X. Wu, Object detection with deep learning: A
review, IEEE transactions on neural networks and learning systems.
[5] S. Antol, A. Agrawal, J. Lu, M. Mitchell, D. Batra, C. L. Zitnick, D. Parikh,
VQA: Visual Question Answering, in: International Conference on Computer
Vision (ICCV), 2015.
[6] Y. Goyal, T. Khot, D. Summers-Stay, D. Batra, D. Parikh, Making the v in vqa
matter: Elevating the role of image understanding in visual question answering,
in: Proceedings of the IEEE Conference on Computer Vision and Pattern Recog-
nition, 2017, pp. 6904–6913.
[7] O. Vinyals, A. Toshev, S. Bengio, D. Erhan, Show and tell: A neural image cap-
tion generator, in: Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, 2015, pp. 3156–3164.
[8] A. Karpathy, L. Fei-Fei, Deep visual-semantic alignments for generating image
descriptions, in: Proceedings of the IEEE conference on computer vision and
pattern recognition, 2015, pp. 3128–3137.
31

[9] A. Das, S. Kottur, J. M. Moura, S. Lee, D. Batra, Learning cooperative visual di-
alog agents with deep reinforcement learning, in: IEEE International Conference
on Computer Vision (ICCV), 2017.
[10] A. Das, S. Kottur, K. Gupta, A. Singh, D. Yadav, J. M. Moura, D. Parikh, D. Batra,
Visual dialog, in: Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, 2017.
[11] B. N. Patro, M. Lunayach, S. Patel, V. P. Namboodiri, U-cam: Visual ex-
planation using uncertainty based class activation maps, in:
arXiv preprint
arXiv:1908.06306, 2019.
[12] K. Barnard, P. Duygulu, D. Forsyth, N. de freitas, d, Blei, and MI Jordan,” Match-
ing Words and Pictures”, submitted to JMLR.
[13] A. Farhadi, M. Hejrati, M. A. Sadeghi, P. Young, C. Rashtchian, J. Hockenmaier,
D. Forsyth, Every picture tells a story: Generating sentences from images, in:
European conference on computer vision, Springer, 2010, pp. 15–29.
[14] G. Kulkarni, V. Premraj, S. Dhar, S. Li, Y. Choi, A. C. Berg, T. L. Berg, Baby
talk: Understanding and generating image descriptions, in: Proceedings of the
24th CVPR, Citeseer, 2011.
[15] R. Socher, A. Karpathy, Q. V. Le, C. D. Manning, A. Y. Ng, Grounded composi-
tional semantics for ﬁnding and describing images with sentences, Transactions
of the Association of Computational Linguistics 2 (1) (2014) 207–218.
[16] H. Fang, S. Gupta, F. Iandola, R. Srivastava, L. Deng, P. Doll´ar, J. Gao, X. He,
M. Mitchell, J. Platt, et al., From captions to visual concepts and back, in: Pro-
ceedings of the IEEE conference on computer vision and pattern recognition,
2015.
[17] J. Johnson, A. Karpathy, L. Fei-Fei, Densecap: Fully convolutional localization
networks for dense captioning, in: Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition, 2016, pp. 4565–4574.
32

[18] X. Yan, J. Yang, K. Sohn, H. Lee, Attribute2image: Conditional image generation
from visual attributes, in: European Conference on Computer Vision, Springer,
2016, pp. 776–791.
[19] M. Malinowski, M. Fritz, A multi-world approach to question answering about
real-world scenes based on uncertain input, in: Advances in Neural Information
Processing Systems (NIPS), 2014.
[20] M. Ren, R. Kiros, R. Zemel, Exploring models and data for image question an-
swering, in: Advances in Neural Information Processing Systems (NIPS), 2015,
pp. 2953–2961.
[21] H. Noh, P. Hongsuck Seo, B. Han, Image question answering using convolutional
neural network with dynamic parameter prediction, in: Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition, 2016, pp. 30–38.
[22] H. Xu, K. Saenko, Ask, attend and answer: Exploring question-guided spatial
attention for visual question answering, in: European Conference on Computer
Vision, Springer, 2016, pp. 451–466.
[23] J. Lu, J. Yang, D. Batra, D. Parikh, Hierarchical question-image co-attention for
visual question answering, in: Advances In Neural Information Processing Sys-
tems, 2016, pp. 289–297.
[24] K. J. Shih, S. Singh, D. Hoiem, Where to look: Focus regions for visual question
answering, in: Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, 2016, pp. 4613–4621.
[25] B. Patro, V. P. Namboodiri, Differential attention for visual question answering,
in: The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
2018.
[26] J.-H. Kim, J. Jun, B.-T. Zhang, Bilinear attention networks, in: Advances in Neu-
ral Information Processing Systems, 2018, pp. 1571–1581.
33

[27] P. Anderson, X. He, C. Buehler, D. Teney, M. Johnson, S. Gould, L. Zhang,
Bottom-up and top-down attention for image captioning and visual question an-
swering, in: Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, 2018, pp. 6077–6086.
[28] Y. Bai, J. Fu, T. Zhao, T. Mei, Deep attention neural tensor network for visual
question answering, in: Proceedings of the European Conference on Computer
Vision (ECCV), 2018, pp. 20–35.
[29] H. Ben-younes, R. Cadene, M. Cord, N. Thome, Mutan: Multimodal tucker fu-
sion for visual question answering, in: The IEEE International Conference on
Computer Vision (ICCV), 2017.
[30] Y. Zhang, J. Hare, A. Pr¨ugel-Bennett, Learning to count objects in natural images
for visual question answering.
[31] N. Mostafazadeh, I. Misra, J. Devlin, M. Mitchell, X. He, L. Vanderwende, Gen-
erating natural questions about an image, arXiv preprint arXiv:1603.06059.
[32] U. Jain, Z. Zhang, A. Schwing, Creativity: Generating diverse questions using
variational autoencoders, arXiv preprint arXiv:1704.03493.
[33] B. N. Patro, S. Kumar, V. K. Kurmi, V. Namboodiri, Multimodal differential net-
work for visual question generation, in: Proceedings of the 2018 Conference on
Empirical Methods in Natural Language Processing, Association for Computa-
tional Linguistics, 2018, pp. 4002–4012.
URL http://aclweb.org/anthology/D18-1434
[34] B. N. Patro, V. K. Kurmi, S. Kumar, V. Namboodiri, Learning semantic sentence
embeddings using sequential pair-wise discriminator, in: Proceedings of the 27th
International Conference on Computational Linguistics, 2018, pp. 2715–2729.
[35] J. Lu, A. Kannan, J. Yang, D. Parikh, D. Batra, Best of both worlds: Transferring
knowledge from discriminative learning to a generative visual dialog model, in:
Advances in Neural Information Processing Systems, 2017, pp. 314–324.
34

[36] Q. Wu, P. Wang, C. Shen, I. Reid, A. van den Hengel, Are you talking to me?
reasoned visual dialog generation through adversarial learning.
[37] H. De Vries, F. Strub, S. Chandar, O. Pietquin, H. Larochelle, A. Courville,
Guesswhat?! visual object discovery through multi-modal dialogue, in: Proc.
of CVPR, 2017.
[38] F. Strub, H. De Vries, J. Mary, B. Piot, A. Courville, O. Pietquin, End-to-end op-
timization of goal-driven and visually grounded dialogue systems, arXiv preprint
arXiv:1703.05423.
[39] U. Jain, S. Lazebnik, A. G. Schwing, Two can play this game: Visual dialog with
discriminative question generation and answering, in: Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition, 2018.
[40] L. Wang, A. Schwing, S. Lazebnik, Diverse and accurate image description us-
ing a variational auto-encoder with an additive gaussian encoding space, in: Ad-
vances in Neural Information Processing Systems, 2017, pp. 5756–5766.
[41] B. Dai, S. Fidler, R. Urtasun, D. Lin, Towards diverse and natural image descrip-
tions via a conditional gan, in: 2017 IEEE International Conference on Computer
Vision (ICCV), IEEE, 2017, pp. 2989–2998.
[42] D. Li, X. He, Q. Huang, M.-T. Sun, L. Zhang, Generating diverse and
accurate visual captions by comparative adversarial learning, arXiv preprint
arXiv:1804.00861.
[43] G. E. Hinton, D. Van Camp, Keeping the neural networks simple by minimizing
the description length of the weights, in: Proc. of the Conference on Computa-
tional learning theory (COLT), ACM, 1993, pp. 5–13.
[44] D. Barber, C. M. Bishop, Ensemble learning in bayesian neural networks (1998)
215–238.
[45] A. Graves, Practical variational inference for neural networks, in: Advances in
Neural Information Processing Systems (NIPS), 2011, pp. 2348–2356.
35

[46] C. Blundell, J. Cornebise, K. Kavukcuoglu, D. Wierstra, Weight uncertainty in
neural networks, arXiv preprint arXiv:1505.05424.
[47] Y. Gal, Z. Ghahramani, Dropout as a bayesian approximation: Representing
model uncertainty in deep learning, in: International Conference on Machine
Learning (ICML), 2016, pp. 1050–1059.
[48] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, R. Salakhutdinov, Dropout:
a simple way to prevent neural networks from overﬁtting, The Journal of Machine
Learning Research 15 (1) (2014) 1929–1958.
[49] A. Kendall, V. Badrinarayanan, R. Cipolla, Bayesian segnet: Model uncertainty in
deep convolutional encoder-decoder architectures for scene understanding, arXiv
preprint arXiv:1511.02680.
[50] A. Kendall, Y. Gal, What uncertainties do we need in bayesian deep learning for
computer vision?, in: Advances in neural information processing systems, 2017,
pp. 5574–5584.
[51] L. Smith, Y. Gal, Understanding measures of uncertainty for adversarial example
detection, arXiv preprint arXiv:1803.08533.
[52] M. Teye, H. Azizpour, K. Smith, Bayesian uncertainty estimation for batch nor-
malized deep networks, arXiv preprint arXiv:1802.06455.
[53] C. M. Bishop, Pattern recognition and machine learning.
[54] M. Welling, Y. W. Teh, Bayesian learning via stochastic gradient langevin dynam-
ics, in: Proceedings of the 28th International Conference on Machine Learning
(ICML-11), 2011, pp. 681–688.
[55] B. Lakshminarayanan, A. Pritzel, C. Blundell, Simple and scalable predictive
uncertainty estimation using deep ensembles, in: Advances in Neural Information
Processing Systems, 2017, pp. 6402–6413.
[56] H. Wu, X. Gu, Towards dropout training for convolutional neural networks, Neu-
ral Networks 71 (2015) 1–10.
36

[57] Y. Gal, Z. Ghahramani, A theoretically grounded application of dropout in re-
current neural networks, in: Advances in neural information processing systems,
2016, pp. 1019–1027.
[58] M. Fortunato, C. Blundell, O. Vinyals, Bayesian recurrent neural networks, arXiv
preprint arXiv:1704.02798.
[59] Y. Gal, Z. Ghahramani, Bayesian convolutional neural networks with bernoulli
approximate variational inference, arXiv preprint arXiv:1506.02158.
[60] Z. Yang, X. He, J. Gao, L. Deng, A. Smola, Stacked attention networks for image
question answering, in: Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, 2016, pp. 21–29.
[61] D. P. Kingma, M. Welling, Auto-encoding variational bayes, stat 1050 (2014) 1.
[62] V. K. Kurmi, S. Kumar, V. P. Namboodiri, Attending to discriminative certainty
for domain adaptation, in: Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, 2019, pp. 491–500.
[63] Y. Gal, Uncertainty in deep learning, Ph.D. thesis, University of Cambridge
(2016).
[64] K.
Dorman,
Bayesian
neural
network
blogpost,
https://github.com/kyle-
dorman/bayesian-neural-network-blogpost.
[65] R. R. Selvaraju, M. Cogswell, A. Das, R. Vedantam, D. Parikh, D. Batra, Grad-
cam: Visual explanations from deep networks via gradient-based localization., in:
Proceedings of the IEEE International Conference on Computer Vision (ICCV),
2017.
[66] S. Hochreiter, J. Schmidhuber, Long short-term memory, Neural computation
9 (8) (1997) 1735–1780.
[67] P. H. Seo, A. Lehrmann, B. Han, L. Sigal, Visual reference resolution using at-
tention memory for visual dialog, in: Advances in neural information processing
systems, 2017, pp. 3719–3729.
37

[68] S. Kottur, J. M. Moura, D. Parikh, D. Batra, M. Rohrbach, Visual coreference
resolution in visual dialog using neural module networks, in: Proceedings of the
European Conference on Computer Vision (ECCV), 2018, pp. 153–169.
38

