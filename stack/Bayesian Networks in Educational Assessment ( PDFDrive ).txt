
Statistics for Social and Behavioral
Sciences
Series Editor
Stephen E. Fienberg
Carnegie Mellon University Dept. Statistics
Pittsburgh
Pennsylvania
USA

Statistics for Social and Behavioral Sciences (SSBS) includes monographs and
advanced textbooks relating to education, psychology, sociology, political sci-
ence, public policy, and law.
More information about this series at http://www.springer.com/series/3463

Russell G. Almond • Robert J. Mislevy
Linda S. Steinberg • Duanli Yan
David M. Williamson
Bayesian Networks in
Educational Assessment
2123

Russell G. Almond
Duanli Yan
Florida State University
Educational Testing Service
Tallahassee
Princeton
Florida
New Jersey
USA
USA
Robert J. Mislevy
David M. Williamson
Educational Testing Service
Educational Testing Service
Princeton
Princeton
New Jersey
New Jersey
USA
USA
Linda S. Steinberg
Pennington
New Jersey
USA
Statistics for Social and Behavioral Sciences
ISBN 978-1-4939-2124-9
ISBN 978-1-4939-2125-6 (eBook)
DOI 10.1007/978-1-4939-2125-6
Library of Congress Control Number: 2014958291
Springer New York Heidelberg Dordrecht London
c⃝Springer Science+Business Media New York 2015
This work is subject to copyright. All rights are reserved by the Publisher, whether the
whole or part of the material is concerned, speciﬁcally the rights of translation, reprint-
ing, reuse of illustrations, recitation, broadcasting, reproduction on microﬁlms or in any
other physical way, and transmission or information storage and retrieval, electronic adap-
tation, computer software, or by similar or dissimilar methodology now known or hereafter
developed.
The use of general descriptive names, registered names, trademarks, service marks, etc. in
this publication does not imply, even in the absence of a speciﬁc statement, that such names
are exempt from the relevant protective laws and regulations and therefore free for general
use.
The publisher, the authors and the editors are safe to assume that the advice and informa-
tion in this book are believed to be true and accurate at the date of publication. Neither the
publisher nor the authors or the editors give a warranty, express or implied, with respect
to the material contained herein or for any errors or omissions that may have been made.
Printed on acid-free paper
Springer is part of Springer Science+Business Media (www.springer.com)

Dedication
Forward into future times we go
Over boulders standing in out way
Rolling them aside because we know
Others follow in our steps one day
Under deepest earth the gems are found
Reaching skyward ’till we grasp the heights
Climbing up to where the view surrounds
Hidden valleys oﬀer new delights
Inch by inch and yard by yard until
Luck brings us to the hidden vale
Desiring a place to rest yet still
Returning home now to tell the tale
Ever knowing when that day does come
New hands will take up work left undone

Acknowledgements
Bayesian Inference in Educational Assessments (BNinEA) is the direct issue
of two projects, and the descendant, cousin, or sibling of many more. We are
grateful for all we have learned in these experiences from many collaborators
and supporters over the years.
The ﬁrst direct ancestor is the series of workshops we have presented at
the annual meeting of the National Council on Measurement in Education
(NCME) almost every year since 2001. We are grateful to NCME for this
opportunity and to ETS for support in developing the materials they have
granted us permission to use. Workshop participants will recognize many con-
cepts, algorithms, ﬁgures, and hands-on exercises from these sessions.
Its second direct ancestor is the Portal project at Educational Testing
Service. It was here that Linda, Russell, and Bob ﬂeshed out the evidence-
centered design (ECD) assessment framework, implemented it in an object
model and design system, and carried out the applications using Bayes nets.
We are grateful to Henry Braun and Drew Gitomer, successive vice-presidents
of Research, and Len Swanson, head of New Product Development, for sup-
porting Portal. Our collaborators included Brian Berenbach, Marjorie Biddle,
Lou DiBello, Howie Chernik, Eddie Herskovits, Cara Cahallan Laitusis, Jan
Lukas, Alexander Matukhin, and Peggy Redman.
Biomass (Chaps. 14 and 15) was a ground-up demonstration of Portal,
ECD design, standards-based science assessment, web-delivered interactive
testing, with automated scoring of inquiry investigations and Bayes net mea-
surement models. The Biomass team included Andy Baird, Frank Jenkins,
our subject matter lead Ann Kindﬁeld, and Deniz Senturk. Our subject mat-
ter consultants were Scott Kight, Sue Johnson, Gordon Mendenhall, Cathryn
Rubin, and Dirk Vanderklein.
The Networking Performance Skill System (NetPASS) project was an
online performance-based assessment activity for designing and troubleshoot-
ing computer networks. It was developed in collaboration with the Cisco Net-
working Academy Program, and led by John Behrens of Cisco. NetPASS fea-
tured principled design of proﬁciency, task, and evidence models and a Bayes

VIII
Acknowledgements
net psychometric model using the methods described in BNinEA. Team mem-
bers included Malcolm Bauer, Sarah DeMark, Michael Faron, Bill Frey, Dennis
Frezzo, Tara Jennings, Peggy Redman, Perry Reinert, and Ken Stanley. The
NetPASS prototype was foundational for the Packet Tracer simulation sys-
tem and Aspire game environment that Cisco subsequently developed, and
millions of Cisco Network Academy (CNA) students around the world have
used operationally to learn beginning network engineering skills.
The DISC scoring engine was a modular Bayes-net-based evidence accumu-
lation package developed for the Dental Interactive Simulations Corporation
(DISC), by the Chauncey Group International, ETS, and the DISC Scor-
ing Team: Barry Wohlgemuth, DISC President and Project Director; Lynn
Johnson, Project Manager; Gene Kramer; and ﬁve core dental hygienist mem-
bers, Phyllis Beemsterboer, RDH, Cheryl Cameron, RDH, JD, Ann Eshenaur,
RDH, Karen Fulton, RDH, and Lynn Ray, RDH. Jay Breyer was the Chauncey
Group lead, and was instrumental in conducting the expert–novice studies and
constructing proﬁciency, task, and evidence models.
Adaptive Content with Evidence-based Diagnosis (ACED) was the brain-
child of Valerie J. Shute. It had a large number of contributors including Larry
Casey, Edith Aurora Graf, Eric Hansen, Waverly Hester, Steve Landau, Peggy
Redman, Jody Underwood, and Diego Zapata-Rivera. ACED development
and data collection were sponsored by National Science Foundation Grant
No. 3013202. The complete ACED models and data are available online; see
the Appendix for details.
Bob’s initial forays into applying Bayesian networks in educational assess-
ment were supported in part by grants from the Oﬃce of Naval Research
(ONR) and from the National Center for Research on Evaluation, Standards,
and Student Testing (CRESST) at the University of California at Los Ange-
les. We are grateful to Charles Davis, Project Oﬃcer of ONR’s Model-Based
Measurement program and Eva Baker, Director of CRESST, for their sup-
port. Much of the work we draw on here appears in ONR and CRESST
research reports. The ﬁndings and opinions expressed in BNinEA, however,
do not reﬂect the positions or policies of ONR, the National Institute on Stu-
dent Achievement, Curriculum, and Assessment, the Oﬃce of Educational
Research and Improvement, or the U.S. Department of Education.
Some of the ideas in this book are based on Russell’s previous work on
the Graphical-Belief project. Thanks to Doug Martin at StatSci for spon-
soring that project as well as the NASA Small Business Innovation Research
(SBIR) program for supporting initial development. David Madigan made a
number of contributions to that work, particularly pointing out the impor-
tance of the weight of evidence. Graphical-Belief is based on the earlier
work on Belief while Russell was still a student at Harvard. Art Dempster
and Augustine Kong both provided valuable advise for that work. The work
of Glenn Shafer and David Schum in thinking about the representation of
evidence has been very useful as well. Those contributions are documented in
Russell’s earlier book.

Acknowledgements
IX
Along with the NCME training sessions and working on NetPASS and
DISC, David completed his doctoral dissertation on model criticism in Bayes
nets in assessment at Fordham University under John Walsh, with Russell
and Bob as advisors.
Hydrive was an intelligent tutoring system for helping trainees learn to
troubleshoot the hydraulics subsystems of the F-15 aircraft. Drew Gitomer
was the Principal Investigator and Linda was the Project Manager. The
project was supported by Armstrong Laboratories of the US Air Force, under
the Project Oﬃcer Sherrie Gott. Design approaches developed in Hydrive
were extended and formalized in ECD. Bob and Duanli worked with Drew
and Linda to create and test an oﬄine Bayes net scoring model for Hydrive.
Russell and Bob used drafts of BNinEA in classes at Florida State Univer-
sity (FSU) and the University of Maryland, respectively. We received much
helpful feedback from students to clarify our ideas and sharpen our presen-
tations. Students at Maryland providing editorial and substantive contribu-
tions included Younyoung Choi, Roy Levy, Junhui Liu, Michelle Riconscente,
and Daisy Wise Rutstein. Students at FSU, providing feedback and advice,
included Mengyao Cui, Yuhua Guo, Yoon Jeon Kim, Xinya Liang, Zhongtian
Lin, Sicong Liu, Umit Tokac, Gertrudes Velasquez, Haiyan Wu, and Yan Xia.
Kikumi Tatsuoka has been a visionary pioneer in the ﬁeld of cognitive
assessment, whose research is a foundation upon which our work and that
many others in the assessment and psychometric communities builds. We
are grateful for her permission to use her mixed-number subtraction data in
Chaps. 6 and 11.
Brent Boerlage, of Norsys Software Corp., has supported the book in a
number of ways. First and foremost, he has made the student version of Netica
available for free, which has been exceedingly useful in our classes and online
training. Second, he has oﬀered general encouragement for the project and
oﬀered to add some of our networks to his growing Bayes net library.
Many improvements to a draft of the book resulted from rigorous attention
from the ETS review process. We thank Kim Fryer, the manager of editing
services in the Research and Development division at ETS, Associate Editors
Dan Eignor and Shelby Haberman, and the reviewers of individual chapters:
Malcolm Bauer, Jianbin Fu, Aurora Graf, Shelby Haberman, Yue Jia, Feifei
Li, Johnny Lin, Ru Lu, Frank Rijmen, Zhan Shu, Sandip Sinharay, Lawrence
Smith, Matthias von Davier, and Diego Zapata-Rivera.
We thank ETS for their continuing support for BNinEA and the various
projects noted above as well as support through Bob’s position as Frederic
M. Lord Chair in Measurement and Statistics under Senior Vice-President
for Research, Ida Lawrence. We thank ETS for permission to use the ﬁgures
and tables they own and their assistance in securing permission for the rest,
through Juana Betancourt, Stella Devries, and Katie Faherty.
We are grateful also to colleagues who have provided support in more gen-
eral and pervasive ways over the years, including John Mark Agosta, Malcolm
Bauer, Betsy Becker, John Behrens, Judy Goldsmith, Geneva Haertel, Sidney

X
Acknowledgements
Irvine, Kathryn Laskey, Roy Levy, Bob Lissitz, John Mazzeo, Ann Nicholson,
Val Shute, and Howard Wainer.
It has taken longer than it probably should have to complete Bayesian
Networks in Educational Assessment. For their continuing encouragement
and support, we are indebted to our editors at Springer: John Kimmel, who
brought us in, and Jon Gurstelle and Hannah Bracken, who led us out.

Using This Book
An early reviewer urged us to think of this book not as a primer in Bayesian
networks (there are already several good titles available, referenced in this
volume), but to focus instead on the application: the process of building the
model. Our early reviewers also thought that a textbook would be more useful
than a monograph, so we have steered this volume in that particular way. In
particular, we have tried to make the book understandable to any reasonably
intelligent graduate students (and several of our quite intelligent graduate
students have let us know when we got too obscure), as this should provide
the broadest possible audience.
In particular, most chapters include exercises at the end. We have found
through both our classes and the NCME training sessions, that students do
not learn from our lectures or writing (no matter how brilliant) but from
trying to apply what they heard and read to new problems. We would urge
all readers, even just the ones skimming to try the exercises. Solutions are
available from Springer or from the authors.
Another thing we have found very valuable in using the volume education-
ally is starting the students early with a Bayesian network tool. Appendix A
lists several tools, and gives pointers to more. Even in the early chapters,
merely using the software as a drawing tool helps get students thinking about
the ideas. Of course, student projects are an important part of any course
like this. Many of the Bayes net collections used in the example are available
online; Appendix A provides the details.
We have divided the book into three parts, which reﬂect diﬀerent levels
of complexity. Part I is concerned with the basics of Bayesian networks, par-
ticularly developing the background necessary to understand how to use a
Bayesian network to score a single student. It begins with a brief overview of
the ECD. The approach is key to understanding how to use Bayesian networks
as measurement models as an integral component of assessment design and
use from the beginning, rather than simply as a way to analyze data once it
is in hand. (To do the latter is to be disappointed—and it is not the fault of
Bayes nets!) It ends with Chap. 7, which goes beyond the basics to start to

XII
Using This Book
describe how the Bayesian model supports inference more generally. Part II
takes up the issue of the calibrating the networks using data from students.
This is too complex a topic to cover in great depth, but this section explores
parameterizations for Bayesian networks, looks at updating models from data
and model criticism, and ends with a complete example. Part III expands
from the focus on mechanics to embedding the Bayesian network in an assess-
ment system. Two chapters describe the conceptual assessment framework
and the four-process delivery architecture of ECD in greater depth, showing
the intimate connections among assessment arguments, design structures, and
the function of Bayesian networks in inference. Two more chapters are then
devoted to the implementation of Biomass, one of the ﬁrst assessments to be
designed from the ground up using ECD.
When we started this project, it was our intention to write a compan-
ion volume about evidence-centered assessment design. Given how long this
project has taken, that second volume will not appear soon. Chapters 2, 12,
and 13 are probably the best we have to oﬀer at the moment. Russell has
used them with some success as standalone readings in his assessment design
class. Although ECD does not require Bayesian networks, it does involve a
lot of Bayesian thinking about evidence. Readers who are primarily interested
in ECD may ﬁnd that reading all of Part I and exploring simple Bayes net
examples helps deepen their understanding of ECD, then moving to Chaps. 12
and 13 if they want additional depth, and the Biomass chapters to see the
ideas in practice.
Several of our colleagues in the Uncertainty in Artiﬁcial Intelligence com-
munity (the home of much of the early work on Bayesian Networks) have
bemoaned the fact that most of the introductory treatises on Bayesian net-
works fall short in the area of helping the reader translate between a speciﬁc
application and the language of Bayesian networks. Part of the challenge here
is that it is diﬃcult to do this in the absence of a speciﬁc application. This
book starts to ﬁll that gap. One advantage of the educational application is
that it is fairly easy to understand (most people having been subjected to
educational assessment at least once in their lives). Although some of the
language in the book is speciﬁc to the ﬁeld of education, much of the develop-
ment in the book comes from the authors’ attempt to translate the language
of evidence from law and engineering to educational assessment. We hope that
readers from other ﬁelds will ﬁnd ways to translate it to their own work as
well.
In an attempt to create a community around this book, we have created
a Wiki for evidence-centered assessment design (http://ecd.ralmond.net/
ecdwiki/ECD/ECD/). Speciﬁc material to support the book, including example
networks and data, are available at the same site (http://ecd.ralmond.net/
BN/BN). We would like to invite our readers to browse the material there and
to contribute (passwords can be obtained from the authors).

Notation
Random Variables
Random variables in formulae are often indicated by capital letters set in italic
type, e.g., X, while a value of the corresponding random variable is indicated
as a lowercase letter, e.g., x.
Vector-valued random variables and constants are set in boldface. For
example, X is a vector valued random variable and x is a potential value
for X.
Random variables in Bayesian networks with long descriptive names are
usually set in italic type when referenced in the text, e.g., RandomVariable.
If the long name consists of more than one word, capitalization is often used
to indicate word boundaries (so-called CamelCase).
When random variables appear in graphs they are often preceded by an
icon indicating whether they are deﬁned in the proﬁciency model or the evi-
dence model. Variables preceded by a circle, ⃝, are proﬁciency variables, while
variables preceded by a triangle, , are deﬁned locally to an evidence model.
They are often but not always observable variables.
The states of such random variables are given in typewriter font, e.g., High
and Low.
Note that Bayesian statistics does not allow ﬁxed but unknown quantities.
For this reason the distinction between variable and parameter in classical
statistics is not meaningful. In this book, the term “variable” is used to refer
to a quantity speciﬁc to a particular individual taking the assessment and the
term “parameter” is used to indicate quantities that are constant across all
individuals.
Sets
Sets of states and variables are indicated with curly braces, e.g., {High, Medium,
Low}. The symbol x ∈A is used to indicate that x is an element of A. The

XIV
Notation
elements inside the curly braces are unordered, so {A1, A2} = {A2, A1}.
The use of parenthesis indicates that the elements are ordered, so that
(A1, A2) ̸= (A2, A1).
The symbols ∪and ∩are used for the union and intersection of two sets.
If A and B are sets, then A ⊂B is used to indicate that A is a proper subset
of B, while A ⊆B also allows the possibility that A = B.
If A refers to an event, then A refers to the complement of the event; that
is, the event that A does not occur.
Ordered tuples indicating vector valued quantities are indicated with
parenthesis, e.g., (x1, . . . , xk).
Occasionally, the states of a variable have a meaningful order. The symbol
≻is used to state that one symbol is lower than the other. Thus High ≻Low.
The quantiﬁer ∀x is used to indicate “for all possible values of x.” The
quantiﬁer ∃x is used to indicate that an element x exists that satisﬁes the
condition.
For intervals of real numbers a square bracket, ‘[’ (‘]’), is used to indicate
that the lower (upper) bound is included in the interval. Thus:
[0, 1] is equivalent to {x : 0 ≤x ≤1}
[0, 1) is equivalent to {x : 0 ≤x < 1}
(0, 1] is equivalent to {x : 0 < x ≤1}
(0, 1) is equivalent to {x : 0 < x < 1}
Probability Distributions and Related Functions
The notation P(X) is used to refer to the probability of an event X. It is also
used to refer to the probability distribution of a random variable X with the
hope that the distinction will be obvious from context.
To try to avoid confusions with the distributions of the parameters of dis-
tributions, the term law is used for a probability distribution over a parameter
and the term distribution is used for the distribution over a random variable,
although the term distribution is also used generically.
The notation P(X|Y ) is used to refer to the probability of an event X
given that another event Y has occurred. It is also used for the collection of
probability distributions for a random variable X given the possible instanti-
ations of a random variable Y . Again we hope that this loose use of notation
will be clear from context.
If the domain of the random variable is discrete, then the notation p(X) is
used for the probability mass function. If the domain of the random variable is
continuous, then the notation f(X) is used to refer to the probability density.
The notation E[g(X)] is used for the expectation of the function g(X)
with respect to the distribution P(X). When it is necessary to emphasize

Notation
XV
the distribution, then the random variables are placed as a subscript. Thus,
EX[g(X)] is the expectation of g(X) with respect to the distribution P(X)
and EX|Y [g(X)] is the expectation with respect to the distribution P(X|Y ).
The notation Var(X) is used to refer to the variance of the random vari-
able X. The Var(X) is a matrix giving the Var(Xk) on the diagonal and the
covariance of Xi and Xj in the oﬀ-diagonal elements.
If A and B are two events or two random variables, then the notation
A ⊥⊥B and I(A|∅|B) is used to indicate that A is independent of B. The
notations A ⊥⊥B | C and I(A|C|B) indicate that A is independent of B
when conditioned on the value of C (or the event C).
The notation N(μ, σ2) is used to refer to a normal distribution with mean
μ and variance σ2; N+(μ, σ2) refers to the same distribution truncated at
zero (so the random variable is strictly positive). The notation Beta(a, b) is
used to refer to a beta distribution with parameters a and b. The notation
Dirichlet(a1, . . . , aK) is used to refer to K-dimensional Dirichlet distribution
with parameters a1, . . . , aK. The notation Gamma(a, b) is used for a gamma
distribution with shape parameter a and scale parameter b.
The symbol ∼is used to indicate that a random variable follows a par-
ticular distribution. Thus X ∼N(0, 1) would indicate that X is a random
variable following a normal distribution with mean 0 and variance 1.
Transcendental Functions
Unless speciﬁcally stated otherwise, the expression log X refers to the natural
logarithm of X.
The notation exp X is used for the expression eX, the inverse of the log
function.
The notation logit x, also Ψ(x), is used for the cumulative logistic function:
logit x = Ψ(x) =
ex
1 + ex .
The notation y!, y factorial, is used for y! = y
k=1 k, where y is a positive
integer.
The notation Γ(x) is used for the gamma function:
Γ(z) =
 ∞
0
tx−1e−tdt .
Note that Γ(n) = (n −1)! when n is a positive integer.
The notation B(a, b) is used for the beta function:
B(a, b) =
 1
0
ta−1(1 −t)b−1 dt = Γ(a)Γ(b)
Γ(a + b)

XVI
Notation
The notation
n
y

is used to indicate the combinatorial function
n!
(n−y)!y!.
The extended combinatorial function

n
y1 ··· yK

is used to indicate
y!
y1!·····yK!,
where  yk = n.
The notation Φ(x) is used for the cumulative unit normal distribution
function.
Usual Use of Letters for Indices
The letter i is usually used to index individuals.
The letter j is usually used to index tasks, with J being the total number
of tasks.
The letter k is usually used to index states of a variable, with K being the
total number of states. The notation k[X] is an indicator which is 1 when the
random variable X takes on the kth possible value, and zero otherwise.
If x = (x1, . . . , xK) is a vector, then x< k refers to the ﬁrst k−1 elements of
x, (x1, . . . , xk−1), and x > k refers to the last K −k elements (xk+1, . . . , xK).
They refer to the empty set when k = 1 or k = K. The notation x−k refers
to all elements except the jth; that is, (x1, . . . , xk−1, xk+1, . . . , xK).

Contents
Part I Building Blocks for Bayesian Networks
1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3
1.1
An Example Bayes Network . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4
1.2
Cognitively Diagnostic Assessment. . . . . . . . . . . . . . . . . . . . . . . . .
7
1.3
Cognitive and Psychometric Science . . . . . . . . . . . . . . . . . . . . . . . 11
1.4
Ten Reasons for Considering Bayesian Networks . . . . . . . . . . . . . 14
1.5
What Is in This Book . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
2
An Introduction to Evidence-Centered Design . . . . . . . . . . . . . 19
2.1
Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
2.2
Assessment as Evidentiary Argument . . . . . . . . . . . . . . . . . . . . . . 21
2.3
The Process of Design. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
2.4
Basic ECD Structures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
2.4.1
The Conceptual Assessment Framework . . . . . . . . . . . . . . 27
2.4.2
Four-Process Architecture for Assessment Delivery . . . . 34
2.4.3
Pretesting and Calibration. . . . . . . . . . . . . . . . . . . . . . . . . . 38
2.5
Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39
3
Bayesian Probability and Statistics: a Review. . . . . . . . . . . . . . 41
3.1
Probability: Objective and Subjective . . . . . . . . . . . . . . . . . . . . . . 41
3.1.1
Objective Notions of Probability . . . . . . . . . . . . . . . . . . . . 42
3.1.2
Subjective Notions of Probability . . . . . . . . . . . . . . . . . . . . 43
3.1.3
Subjective–Objective Probability . . . . . . . . . . . . . . . . . . . . 45
3.2
Conditional Probability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46
3.3
Independence and Conditional Independence . . . . . . . . . . . . . . . . 51
3.3.1
Conditional Independence . . . . . . . . . . . . . . . . . . . . . . . . . . 53
3.3.2
Common Variable Dependence . . . . . . . . . . . . . . . . . . . . . . 54
3.3.3
Competing Explanations . . . . . . . . . . . . . . . . . . . . . . . . . . . 55
3.4
Random Variables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57
3.4.1
The Probability Mass and Density Functions. . . . . . . . . . 57
3.4.2
Expectation and Variance . . . . . . . . . . . . . . . . . . . . . . . . . . 60

XVIII
Contents
3.5
Bayesian Inference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62
3.5.1
Re-expressing Bayes Theorem . . . . . . . . . . . . . . . . . . . . . . . 63
3.5.2
Bayesian Paradigm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63
3.5.3
Conjugacy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67
3.5.4
Sources for Priors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 72
3.5.5
Noninformative Priors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 74
3.5.6
Evidence-Centered Design and the Bayesian Paradigm . 76
4
Basic Graph Theory and Graphical Models . . . . . . . . . . . . . . . . 81
4.1
Basic Graph Theory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 82
4.1.1
Simple Undirected Graphs . . . . . . . . . . . . . . . . . . . . . . . . . . 83
4.1.2
Directed Graphs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 83
4.1.3
Paths and Cycles . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 84
4.2
Factorization of the Joint Distribution . . . . . . . . . . . . . . . . . . . . . 86
4.2.1
Directed Graph Representation . . . . . . . . . . . . . . . . . . . . . 86
4.2.2
Factorization Hypergraphs. . . . . . . . . . . . . . . . . . . . . . . . . . 88
4.2.3
Undirected Graphical Representation . . . . . . . . . . . . . . . . 90
4.3
Separation and Conditional Independence . . . . . . . . . . . . . . . . . . 91
4.3.1
Separation and D-Separation . . . . . . . . . . . . . . . . . . . . . . . 91
4.3.2
Reading Dependence and Independence from Graphs . . 93
4.3.3
Gibbs–Markov Equivalence Theorem . . . . . . . . . . . . . . . . . 94
4.4
Edge Directions and Causality . . . . . . . . . . . . . . . . . . . . . . . . . . . . 95
4.5
Other Representations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 97
4.5.1
Inﬂuence Diagrams . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 97
4.5.2
Structural Equation Models . . . . . . . . . . . . . . . . . . . . . . . . 99
4.5.3
Other Graphical Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . 100
5
Eﬃcient Calculations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 105
5.1
Belief Updating with Two Variables . . . . . . . . . . . . . . . . . . . . . . . 106
5.2
More Eﬃcient Procedures for Chains and Trees . . . . . . . . . . . . . 111
5.2.1
Propagation in Chains . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 112
5.2.2
Propagation in Trees. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 116
5.2.3
Virtual Evidence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 119
5.3
Belief Updating in Multiply Connected Graphs . . . . . . . . . . . . . . 122
5.3.1
Updating in the Presence of Loops . . . . . . . . . . . . . . . . . . 122
5.3.2
Constructing a Junction Tree . . . . . . . . . . . . . . . . . . . . . . . 123
5.3.3
Propagating Evidence Through a Junction Tree . . . . . . . 134
5.4
Application to Assessment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 135
5.4.1
Proﬁciency and Evidence Model Bayes Net Fragments . 137
5.4.2
Junction Trees for Fragments . . . . . . . . . . . . . . . . . . . . . . . 139
5.4.3
Calculation with Fragments. . . . . . . . . . . . . . . . . . . . . . . . . 143
5.5
The Structure of a Test . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 145
5.5.1
The Q-Matrix for Assessments Using Only Discrete
Items . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 146
5.5.2
The Q-Matrix for a Test Using Multi-observable Tasks . 147
5.6
Alternative Computing Algorithms . . . . . . . . . . . . . . . . . . . . . . . . 149

Contents
XIX
5.6.1
Variants of the Propagation Algorithm . . . . . . . . . . . . . . 150
5.6.2
Dealing with Unfavorable Topologies . . . . . . . . . . . . . . . . . 150
6
Some Example Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 157
6.1
A Discrete IRT Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 158
6.1.1
General Features of the IRT Bayes Net . . . . . . . . . . . . . . . 161
6.1.2
Inferences in the IRT Bayes Net . . . . . . . . . . . . . . . . . . . . . 162
6.2
The “Context” Eﬀect . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 166
6.3
Compensatory, Conjunctive, and Disjunctive Models . . . . . . . . . 172
6.4
A Binary-Skills Measurement Model . . . . . . . . . . . . . . . . . . . . . . . 178
6.4.1
The Domain of Mixed Number Subtraction . . . . . . . . . . . 178
6.4.2
A Bayes Net Model for Mixed-Number Subtraction . . . . 180
6.4.3
Inferences from the Mixed-Number Subtraction Bayes
Net . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 184
6.5
Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 190
7
Explanation and Test Construction . . . . . . . . . . . . . . . . . . . . . . . . 197
7.1
Simple Explanation Techniques. . . . . . . . . . . . . . . . . . . . . . . . . . . . 198
7.1.1
Node Coloring . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 198
7.1.2
Most Likely Scenario. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 200
7.2
Weight of Evidence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 201
7.2.1
Evidence Balance Sheet . . . . . . . . . . . . . . . . . . . . . . . . . . . . 202
7.2.2
Evidence Flow Through the Graph . . . . . . . . . . . . . . . . . . 205
7.3
Activity Selection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 209
7.3.1
Value of Information . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 209
7.3.2
Expected Weight of Evidence . . . . . . . . . . . . . . . . . . . . . . . 213
7.3.3
Mutual Information. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 215
7.4
Test Construction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 215
7.4.1
Computer Adaptive Testing . . . . . . . . . . . . . . . . . . . . . . . . 216
7.4.2
Critiquing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 217
7.4.3
Fixed-Form Tests. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 220
7.5
Reliability and Assessment Information . . . . . . . . . . . . . . . . . . . . 224
7.5.1
Accuracy Matrix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 225
7.5.2
Consistency Matrix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 230
7.5.3
Expected Value Matrix. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 230
7.5.4
Weight of Evidence as Information . . . . . . . . . . . . . . . . . . 232
Part II Learning and Revising Models from Data
8
Parameters for Bayesian Network Models . . . . . . . . . . . . . . . . . 241
8.1
Parameterizing a Graphical Model . . . . . . . . . . . . . . . . . . . . . . . . . 241
8.2
Hyper-Markov Laws . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 244
8.3
The Conditional Multinomial—Hyper-Dirichlet Family . . . . . . . 246
8.3.1
Beta-Binomial Family . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 247

XX
Contents
8.3.2
Dirichlet-Multinomial Family . . . . . . . . . . . . . . . . . . . . . . . 248
8.3.3
The Hyper-Dirichlet Law . . . . . . . . . . . . . . . . . . . . . . . . . . . 248
8.4
Noisy-OR and Noisy-AND Models . . . . . . . . . . . . . . . . . . . . . . . . . 250
8.4.1
Separable Inﬂuence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 254
8.5
DiBello’s Eﬀective Theta Distributions . . . . . . . . . . . . . . . . . . . . . 254
8.5.1
Mapping Parent Skills to θ Space . . . . . . . . . . . . . . . . . . . . 256
8.5.2
Combining Input Skills . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 257
8.5.3
Samejima’s Graded Response Model . . . . . . . . . . . . . . . . . 260
8.5.4
Normal Link Function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 263
8.6
Eliciting Parameters and Laws . . . . . . . . . . . . . . . . . . . . . . . . . . . . 267
8.6.1
Eliciting Conditional Multinomial and Noisy-AND . . . . . 269
8.6.2
Priors for DiBello’s Eﬀective Theta Distributions . . . . . . 272
8.6.3
Linguistic Priors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 273
9
Learning in Models with Fixed Structure . . . . . . . . . . . . . . . . . . 279
9.1
Data, Models, and Plate Notation . . . . . . . . . . . . . . . . . . . . . . . . . 279
9.1.1
Plate Notation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 280
9.1.2
A Bayesian Framework for a Generic Measurement
Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 282
9.1.3
Extension to Covariates . . . . . . . . . . . . . . . . . . . . . . . . . . . . 284
9.2
Techniques for Learning with Fixed Structure . . . . . . . . . . . . . . . 287
9.2.1
Bayesian Inference for the General Measurement Model 288
9.2.2
Complete Data Tables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 289
9.3
Latent Variables as Missing Data . . . . . . . . . . . . . . . . . . . . . . . . . . 297
9.4
The EM Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 298
9.5
Markov Chain Monte Carlo Estimation. . . . . . . . . . . . . . . . . . . . . 305
9.5.1
Gibbs Sampling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 308
9.5.2
Properties of MCMC Estimation . . . . . . . . . . . . . . . . . . . . 309
9.5.3
The Metropolis–Hastings Algorithm . . . . . . . . . . . . . . . . . 312
9.6
MCMC Estimation in Bayes Nets in Assessment . . . . . . . . . . . . . 315
9.6.1
Initial Calibration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 316
9.6.2
Online Calibration. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 321
9.7
Caution: MCMC and EM are Dangerous! . . . . . . . . . . . . . . . . . . . 324
10
Critiquing and Learning Model Structure . . . . . . . . . . . . . . . . . . 331
10.1 Fit Indices Based on Prediction Accuracy . . . . . . . . . . . . . . . . . . 332
10.2 Posterior Predictive Checks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 335
10.3 Graphical Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 342
10.4 Diﬀerential Task Functioning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 347
10.5 Model Comparison . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 350
10.5.1 The DIC Criterion. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 350
10.5.2 Prediction Criteria . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 353
10.6 Model Selection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 354
10.6.1 Simple Search Strategies . . . . . . . . . . . . . . . . . . . . . . . . . . . 355
10.6.2 Stochastic Search . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 356

Contents
XXI
10.6.3 Multiple Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 357
10.6.4 Priors Over Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 357
10.7 Equivalent Models and Causality . . . . . . . . . . . . . . . . . . . . . . . . . . 358
10.7.1 Edge Orientation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 358
10.7.2 Unobserved Variables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 358
10.7.3 Why Unsupervised Learning cannot Prove Causality . . . 360
10.8 The “True” Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 362
11
An Illustrative Example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 371
11.1 Representing the Cognitive Model . . . . . . . . . . . . . . . . . . . . . . . . . 372
11.1.1 Representing the Cognitive Model as a Bayesian
Network. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 372
11.1.2 Representing the Cognitive Model as a Bayesian
Network. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 377
11.1.3 Higher-Level Structure of the Proﬁciency Model; i.e.,
p(θ|λ) and p(λ) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 379
11.1.4 High Level Structure of the Evidence Models; i.e., p(π) . 381
11.1.5 Putting the Pieces Together . . . . . . . . . . . . . . . . . . . . . . . . 382
11.2 Calibrating the Model with Field Data . . . . . . . . . . . . . . . . . . . . . 382
11.2.1 MCMC Estimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 383
11.2.2 Scoring . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 389
11.2.3 Online Calibration. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 392
11.3 Model Checking . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 397
11.3.1 Observable Characteristic Plots . . . . . . . . . . . . . . . . . . . . . 398
11.3.2 Posterior Predictive Checks . . . . . . . . . . . . . . . . . . . . . . . . . 401
11.4 Closing Comments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 405
Part III Evidence-Centered Assessment Design
12
The Conceptual Assessment Framework . . . . . . . . . . . . . . . . . . . 411
12.1 Phases of the Design Process and Evidentiary Arguments. . . . . 414
12.1.1 Domain Analysis and Domain Modeling . . . . . . . . . . . . . . 414
12.1.2 Arguments and Claims . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 418
12.2 The Student Proﬁciency Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . 424
12.2.1 Proﬁciency Variables. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 424
12.2.2 Relationships Among Proﬁciency Variables . . . . . . . . . . . 428
12.2.3 Reporting Rules . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 433
12.3 Task Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 438
12.4 Evidence Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 443
12.4.1 Rules of Evidence (for Evidence Identiﬁcation) . . . . . . . . 444
12.4.2 Statistical Models of Evidence (for Evidence
Accumulation) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 448
12.5 The Assembly Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 453
12.6 The Presentation Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 458

XXII
Contents
12.7 The Delivery Model. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 460
12.8 Putting It All Together . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 461
13
The Evidence Accumulation Process . . . . . . . . . . . . . . . . . . . . . . . 467
13.1 The Four-Process Architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . 468
13.1.1 A Simple Example of the Four-Process Framework. . . . . 471
13.2 Producing an Assessment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 474
13.2.1 Tasks and Task Model Variables. . . . . . . . . . . . . . . . . . . . . 474
13.2.2 Evidence Rules. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 478
13.2.3 Evidence Models, Links, and Calibration . . . . . . . . . . . . . 486
13.3 Scoring. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 488
13.3.1 Basic Scoring Protocols . . . . . . . . . . . . . . . . . . . . . . . . . . . . 489
13.3.2 Adaptive Testing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 493
13.3.3 Technical Considerations . . . . . . . . . . . . . . . . . . . . . . . . . . . 497
13.3.4 Score Reports . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 500
14
Biomass: An Assessment of Science Standards . . . . . . . . . . . . . 507
14.1 Design Goals. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 507
14.2 Designing Biomass. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 510
14.2.1 Reconceiving Standards . . . . . . . . . . . . . . . . . . . . . . . . . . . . 510
14.2.2 Deﬁning Claims . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 513
14.2.3 Deﬁning Evidence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 514
14.3 The Biomass Conceptual Assessment Framework . . . . . . . . . . . . 515
14.3.1 The Proﬁciency Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 515
14.3.2 The Assembly Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 519
14.3.3 Task Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 523
14.3.4 Evidence Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 529
14.4 The Assessment Delivery Processes . . . . . . . . . . . . . . . . . . . . . . . . 535
14.4.1 Biomass Architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 536
14.4.2 The Presentation Process. . . . . . . . . . . . . . . . . . . . . . . . . . . 538
14.4.3 Evidence Identiﬁcation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 540
14.4.4 Evidence Accumulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 541
14.4.5 Activity Selection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 543
14.4.6 The Task/Evidence Composite Library . . . . . . . . . . . . . . . 543
14.4.7 Controlling the Flow of Information Among the
Processes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 544
14.5 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 545
15
The Biomass Measurement Model . . . . . . . . . . . . . . . . . . . . . . . . . 549
15.1 Specifying Prior Distributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 550
15.1.1 Speciﬁcation of Proﬁciency Variable Priors . . . . . . . . . . . 552
15.1.2 Speciﬁcation of Evidence Model Priors . . . . . . . . . . . . . . . 554
15.1.3 Summary Statistics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 560
15.2 Pilot Testing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 561
15.2.1 A Convenience Sample . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 561

Contents
XXIII
15.2.2 Item and other Exploratory Analyses . . . . . . . . . . . . . . . . 564
15.3 Updating Based on Pilot Test Data . . . . . . . . . . . . . . . . . . . . . . . . 566
15.3.1 Posterior Distributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 566
15.3.2 Some Observations on Model Fit . . . . . . . . . . . . . . . . . . . . 575
15.3.3 A Quick Validity Check . . . . . . . . . . . . . . . . . . . . . . . . . . . . 577
15.4 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 579
16
The Future of Bayesian Networks in Educational
Assessment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 583
16.1 Applications of Bayesian Networks . . . . . . . . . . . . . . . . . . . . . . . . . 583
16.2 Extensions to the Basic Bayesian Network Model . . . . . . . . . . . . 586
16.2.1 Object-Oriented Bayes Nets . . . . . . . . . . . . . . . . . . . . . . . . 586
16.2.2 Dynamic Bayesian Networks . . . . . . . . . . . . . . . . . . . . . . . . 588
16.2.3 Assessment-Design Support . . . . . . . . . . . . . . . . . . . . . . . . . 592
16.3 Connections with Instruction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 593
16.3.1 Ubiquitous Assessment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 594
16.4 Evidence-Centered Assessment Design and Validity . . . . . . . . . . 596
16.5 What We Still Do Not Know. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 597
A
Bayesian Network Resources . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 601
A.1 Software . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 601
A.1.1 Bayesian Network Manipulation . . . . . . . . . . . . . . . . . . . . . 602
A.1.2 Manual Construction of Bayesian Networks . . . . . . . . . . . 603
A.1.3 Markov Chain Monte Carlo . . . . . . . . . . . . . . . . . . . . . . . . . 603
A.2 Sample Bayesian Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 604
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 607
Author Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 639

List of Figures
1.1
A graph for the Language Testing Example . . . . . . . . . . . . . . . . . .
5
2.1
The principle design objects of the conceptual assessment
framework (CAF) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27
2.2
The proﬁciency model for a single variable, Proﬁciency Level . . 28
2.3
The measurement model for a dichotomously scored item . . . . . . 31
2.4
The four principle processes in the assessment cycle. . . . . . . . . . . 35
3.1
Canonical experiment: balls in an urn . . . . . . . . . . . . . . . . . . . . . . . 42
3.2
Graph for Feller’s accident proneness example . . . . . . . . . . . . . . . . 55
3.3
Unidimensional IRT as a graphical model. . . . . . . . . . . . . . . . . . . . 56
3.4
Variables θ1 and θ2 are conditionally dependent given X . . . . . . 56
3.5
Examples of discrete and continuous distributions. a Discrete
distribution. b Continuous distribution . . . . . . . . . . . . . . . . . . . . . . 59
3.6
Likelihood for θ generated by observing 7 successes in 10 trials . 65
3.7
A panel of sample beta distributions . . . . . . . . . . . . . . . . . . . . . . . . 68
3.8
A panel of sample gamma distributions . . . . . . . . . . . . . . . . . . . . . 73
4.1
A simple undirected graph . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 83
4.2
A directed graph . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 84
4.3
A tree contains no cycles . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 85
4.4
Examples of cyclic and acyclic directed graphs . . . . . . . . . . . . . . . 85
4.5
Filling-in edges for triangulation. Without the dotted edge,
this graph is not triangulated. Adding the dotted edge makes
the graph triangulated . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 86
4.6
Directed
Graph
for
P(A)P(B)P(C|A, B)P(D|C)P(E|C)P(F|E, D) . . . . . . . . . . . . . . . 87
4.7
Example of a hypergraph (a) and its 2-section (b) . . . . . . . . . . . 89
4.8
Hypergraph representing
P(A)P(B)P(C|A, B)P(D|C)P(E|C)P(F|E, D) . . . . . . . . . . . . . . . 89
4.9
2-section of Fig. 4.8 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 90

XXVI
List of Figures
4.10
D-Separation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 93
4.11
Directed graph running in the “causal” direction: P(Skill)
P(Performance|Skill) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 95
4.12
Directed graph running in the “diagnostic” direction:
P(Performance)P(Skill|Performance) . . . . . . . . . . . . . . . . . . . . . . . . 96
4.13
A graph showing one level of breakdown in language skills . . . . . 96
4.14
Inﬂuence diagram for skill training decision . . . . . . . . . . . . . . . . . . 98
4.15
Graph for use in Exercise 4.2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 102
5.1
Graph for the distribution of X and Y . . . . . . . . . . . . . . . . . . . . . . 107
5.2
The acyclic digraph and junction tree for a four-variable chain . 112
5.3
A junction tree corresponding to a singly-connected graph . . . . . 119
5.4
A polytree and its corresponding junction tree . . . . . . . . . . . . . . . 119
5.5
A loop in a multiply connected graph . . . . . . . . . . . . . . . . . . . . . . . 124
5.6
The tree of cliques and junction tree for Figure 5.5 . . . . . . . . . . . 124
5.7
Acyclic digraph for two-skill example (Example 5.5) . . . . . . . . . . 125
5.8
Moralized undirected graph for two-skill example (Example 5.5) 129
5.9
Two ways to triangulate a graph with a loop . . . . . . . . . . . . . . . . . 130
5.10
Cliques for the two-skill example . . . . . . . . . . . . . . . . . . . . . . . . . . . 130
5.11
Junction tree for the two-skill example . . . . . . . . . . . . . . . . . . . . . . 132
5.12
Relationship among proﬁciency model and evidence model
Bayes net fragments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 138
5.13
Total acyclic digraph for three-task test . . . . . . . . . . . . . . . . . . . . . 141
5.14
Proﬁciency model fragments for three-task test . . . . . . . . . . . . . . . 141
5.15
Evidence model fragments for three-task test . . . . . . . . . . . . . . . . 141
5.16
Moralized proﬁciency model graph for three-task test . . . . . . . . . 142
5.17
Moralized evidence model fragments for three-task test . . . . . . . . 142
6.1
Model graph for ﬁve item IRT model . . . . . . . . . . . . . . . . . . . . . . . 160
6.2
The initial probabilities for the IRT model in Netica. The
numbers at the bottom of the box for the Theta node
represent the expected value and standard deviation of Theta . . 162
6.3
a Student with Item 2 and 3 correct. b Student with Item 3
and 4 correct . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 164
6.4
Probabilities conditioned on θ = 1 . . . . . . . . . . . . . . . . . . . . . . . . . . 165
6.5
Five item IRT model with local dependence. . . . . . . . . . . . . . . . . . 168
6.6
a Student with Item 2 and Item 3 correct with context eﬀect.
b Student with Item 3 and Item 4 correct with context eﬀect . . 169
6.7
Three diﬀerent ways of modeling observable with two parents . . 173
6.8
Initial probabilities for three distribution types . . . . . . . . . . . . . . . 174
6.9
a Updated probabilities when Observation = Right.
b Updated probabilities when Observation = Wrong. . . . . . . . . . . 175
6.10
a Updated probabilities when P1 = H and
Observation = Right.
b Updated probabilities when P1 = H and Observation = Wrong 176

List of Figures XXVII
6.11
a Updated probabilities when P1 = M and
Observation = Right.
b Updated probabilities when P1 = M and Observation = Wrong177
6.12
Proﬁciency model for Method B for solving mixed number
subtraction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 181
6.13
Two evidence model fragments for evidence models 3 and 4 . . . . 182
6.14
Full Bayesian model for Method B for solving mixed number
subtraction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 183
6.15
Prior (population) probabilities . . . . . . . . . . . . . . . . . . . . . . . . . . . . 185
6.16
Mixed number subtraction: a sample student. . . . . . . . . . . . . . . . . 186
6.17
Mixed number subtraction: posterior after 7 Items . . . . . . . . . . . . 188
6.18
Mixed number subtraction: Skill 1 = Yes . . . . . . . . . . . . . . . . . . . . 189
6.19
Mixed number subtraction: Skill 1 = No . . . . . . . . . . . . . . . . . . . . . 191
6.20
Mixed number subtraction: Skill 1 = Yes, Skill 2 = No . . . . . . . . 192
6.21
Updated probabilities when P1 = L and Observation = Wrong . . 194
7.1
Colored graph for an examinee who reads well but has trouble
speaking. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 199
7.2
Evidence balance sheet for θ ≥1 (Example 7.2) . . . . . . . . . . . . . . 203
7.3
Evidence balance sheet for Reading = Novice (Example 7.1) . . . 204
7.4
Evidence balance sheet for Listening = Novice (Example 7.1) . 206
7.5
Evidence balance sheet for Speaking = Novice (Example 7.1) . . 207
7.6
Evidence ﬂows using weight of evidence . . . . . . . . . . . . . . . . . . . . . 208
7.7
Proﬁciency model for ACED assessment . . . . . . . . . . . . . . . . . . . . . 219
7.8
Subset of ACED proﬁciency model for exercises . . . . . . . . . . . . . . 234
8.1
A simple latent class model with two skills. . . . . . . . . . . . . . . . . . . 242
8.2
Hypergraph for a simple latent class model with two skills . . . . . 242
8.3
Second layer of parameters on top of graphical model . . . . . . . . . 243
8.4
Introducing the demographic variable Grade breaks global
parameter independence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 245
8.5
A conjunctive model, with no “noise” . . . . . . . . . . . . . . . . . . . . . . . 250
8.6
A conjunctive model with noisy output (DINA) . . . . . . . . . . . . . . 251
8.7
A conjunctive model with noisy inputs (NIDA). . . . . . . . . . . . . . . 252
8.8
A noisy conjunctive model, with noisy inputs and outputs . . . . . 253
8.9
Separable inﬂuences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 255
8.10
Midpoints of intervals on the normal distribution . . . . . . . . . . . . . 256
8.11
Probabilities for Graded Response model . . . . . . . . . . . . . . . . . . . . 262
8.12
Output translation method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 264
9.1
Expanded and plate digraphs for four Bernoulli variables . . . . . . 281
9.2
Plate digraph for hierarchical Beta-Bernoulli model . . . . . . . . . . . 282
9.3
Plate digraph for hierarchical Rasch model . . . . . . . . . . . . . . . . . . 283
9.4
Graph for generic measurement model . . . . . . . . . . . . . . . . . . . . . . 286
9.5
Graph with covariates and no DIF . . . . . . . . . . . . . . . . . . . . . . . . . . 287

XXVIII
List of Figures
9.6
Graph with DIF. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 288
9.7
Graph for mixture model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 288
9.8
Graph for three-item latent class model . . . . . . . . . . . . . . . . . . . . . 292
9.9
Examples of poor mixing and good mixing. . . . . . . . . . . . . . . . . . . 311
9.10
Convergence of three Markov chains . . . . . . . . . . . . . . . . . . . . . . . . 311
9.11
Plot of the Brook–Gelman–Rubin R vs. cycle number . . . . . . . . . 312
9.12
A Metropolis step that will always be accepted . . . . . . . . . . . . . . . 314
9.13
A Metropolis step that might be rejected . . . . . . . . . . . . . . . . . . . . 314
9.14
Posteriors for selected parameters . . . . . . . . . . . . . . . . . . . . . . . . . . 321
10.1
Two alternative discrete IRT models with and without context
eﬀect. a Conditionally independent IRT model. b Testlet model 339
10.2
Stem-and-leaf plots of posterior predictive probabilities for Q3
values of item pairs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 341
10.3
Q3 values for item pairs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 341
10.4
Observable characteristic plot . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 344
10.5
Observable characteristic plot for additional skill . . . . . . . . . . . . . 345
10.6
Direct data display for mixed number subtraction test . . . . . . . . 347
10.7
Two graphs showing the eﬀects of diﬀerential task functioning.
a Graph with no DTF b Graph with DTF. . . . . . . . . . . . . . . . . . . 348
10.8
Graphs (a), (b), and (c) have identical independence
structures, but Graph (d) does not . . . . . . . . . . . . . . . . . . . . . . . . . 359
10.9
Four graphs which could have identical distributions on the
observed variables A and C. a No hidden cause, b common
cause, c intermediate cause, d partial cause . . . . . . . . . . . . . . . . . . 359
10.10 Selection eﬀect produces apparent dependence among observed
variables. a No selection eﬀect. b Selection eﬀect . . . . . . . . . . . . . 360
10.11 A minimal graph which should not be interpreted as causal . . . . 361
10.12 Inclusion of an additional variable changes picture dramatically 362
10.13 Two candidate models for a hypothetical medical licensure
assessment. a Model A. b Model B . . . . . . . . . . . . . . . . . . . . . . . . . 363
10.14 Observable characteristic plot for Exercise 10.13 . . . . . . . . . . . . . . 368
10.15 Observable characteristic plot for Exercise 10.14 . . . . . . . . . . . . . . 368
11.1
Proﬁciency model for mixed-number subtraction, method B. . . . 373
11.2
Evidence model fragments for evidence models 3 and 4 . . . . . . . . 375
11.3
Mixed number subtraction Bayes net . . . . . . . . . . . . . . . . . . . . . . . 375
11.4
Plate representation of the parameterized mixed-number
subtraction model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 382
11.5
Gelman–Rubin potential scale reduction factors for selected
parameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 386
11.6
Histories of MCMC chains for selected parameters . . . . . . . . . . . . 387
11.7
Posterior distributions from MCMC chains for selected
parameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 389
11.8
Observable characteristic plots for ﬁrst eight items . . . . . . . . . . . . 399

List of Figures
XXIX
11.9
Observable characteristic plots for last seven items. . . . . . . . . . . . 400
12.1
The principal design objects of the CAF. . . . . . . . . . . . . . . . . . . . . 413
12.2
Toulmin’s structure for arguments . . . . . . . . . . . . . . . . . . . . . . . . . . 420
12.3
Partial language testing proﬁciency model showing a part-of
relationship . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 429
12.4
Proﬁciency model for language testing using only the four
modal skills. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 430
12.5
Proﬁciency model for language with additional communicative
competence skills . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 431
12.6
Prototype score report for Language Placement Test . . . . . . . . . . 436
12.7
Evidence model for lecture clariﬁcation task for use with
modal proﬁciency model. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 450
13.1
The four-process architecture for assessment delivery. . . . . . . . . . 470
13.2
Assessment blueprint for a small test . . . . . . . . . . . . . . . . . . . . . . . 474
13.3
Tasks generated from CAF in Fig. 13.2 . . . . . . . . . . . . . . . . . . . . . . 475
13.4
A sample receiver operating characteristic (ROC) curve . . . . . . . 480
13.5
Link model generated from Task 1a in Fig. 13.3 . . . . . . . . . . . . . . 487
13.6
Absorbing evidence from Task 1a for a single candidate . . . . . . . 490
13.7
ACED proﬁciency model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 495
14.1
A concept map for mechanisms of evolution. . . . . . . . . . . . . . . . . . 511
14.2
Representation of a standards-based domain for assessment
design . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 512
14.3
The biomass proﬁciency model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 516
14.4
Biomass: a sample score report from the Biomass classroom
assessment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 520
14.5
Biomass: the introductory screen . . . . . . . . . . . . . . . . . . . . . . . . . . . 523
14.6
Biomass: background for ﬁrst task . . . . . . . . . . . . . . . . . . . . . . . . . . 526
14.7
Biomass: ﬁrst task is to complete a table for allele
representation of mode of inheritance . . . . . . . . . . . . . . . . . . . . . . . 527
14.8
Biomass: second task, population attribute table . . . . . . . . . . . . . 528
14.9
Biomass: fourth task, what to do next?. . . . . . . . . . . . . . . . . . . . . . 529
14.10 An evidence model Bayes net fragment with seven observables . 533
14.11 An evidence model using three observables and a context eﬀect 534
14.12 An evidence model fragment with three conditionally-
independent observables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 534
14.13 An evidence model fragment using the inhibitor relationship . . . 535
16.1
A basic dynamic Bayesian network . . . . . . . . . . . . . . . . . . . . . . . . . 589
16.2
A learning-model dynamic Bayesian network . . . . . . . . . . . . . . . . . 590
16.3
Instruction as a partially observed Markov decision process . . . . 591
16.4
Inﬂuence diagram for skill training decision . . . . . . . . . . . . . . . . . . 593

List of Tables
5.1
Updating probabilities in response to learning Y = y1 . . . . . . . . . 109
5.2
Numerical example of updating probabilities . . . . . . . . . . . . . . . . . 110
5.3
Updating probabilities down a chain . . . . . . . . . . . . . . . . . . . . . . . . 117
5.4
Updating probabilities up a chain . . . . . . . . . . . . . . . . . . . . . . . . . . 118
5.5
Updating with virtual evidence. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 121
5.6
Probabilities for Example 5.5 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 127
5.7
Potential tables for the two-skill example . . . . . . . . . . . . . . . . . . . . 132
5.8
Updating the potential tables for {θA, θB, X2} . . . . . . . . . . . . . . . 136
5.9
Q-Matrix for design leading to saturated model . . . . . . . . . . . . . . 147
5.10
Q-Matrix for Fig. 5.13, one row per task . . . . . . . . . . . . . . . . . . . . 148
5.11
Q-Matrix for Fig. 5.13, one row per observable . . . . . . . . . . . . . . . 148
5.12
Q-Matrix for proposed assessment (Exercise 5.12) . . . . . . . . . . . . 154
6.1
Conditional probabilities of a correct response for the ﬁve-item
IRT model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 160
6.2
Initial marginal probabilities for ﬁve items from IRT model . . . . 162
6.3
New potentials for Item 3 and Item 4, conditioned on Context . 170
6.4
Conditional probabilities for the three distributions. . . . . . . . . . . 174
6.5
Q-Matrix for the Tatsuoka (1984) mixed number subtraction
test . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 179
6.6
Conditional probability table for Skill 5 . . . . . . . . . . . . . . . . . . . . . 184
6.7
Conditional probability table CPT for Item 16 . . . . . . . . . . . . . . . 184
6.8
Posteriors after two sets of observations . . . . . . . . . . . . . . . . . . . . . 187
6.9
Predictions for various skill patterns Subtraction assessment . . . 190
6.10
Potentials for Exercise 6.17 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 195
7.1
Accuracy matrices for Reading and Writing based on 1000
simulated students. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 227
7.2
Expected accuracy matrices based on 1000 simulations . . . . . . . . 232
7.3
Conditional probabilities for ACED subset proﬁciency model. . . 234
7.4
Conditional probabilities for evidence models for ACED subset . 234

XXXII List of Tables
7.5
Alternative conditional probabilities for ACED subset
proﬁciency model. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 235
7.6
Accuracy matrices for Speaking and Listening based on 1000
simulated students. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 237
8.1
Eﬀective thetas for a compensatory combination function . . . . . . 258
8.2
Eﬀective thetas for the conjunctive and disjunctive
combination functions. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 259
8.3
Eﬀective thetas for inhibitor combination functions . . . . . . . . . . . 260
8.4
Conditional probability table for simple graded response model. 263
8.5
Compensatory combination function and graded response link
function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 263
8.6
Conditional probability table with normal link function,
correlation = 0.8 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 266
8.7
Conditional probability table for path coeﬃcients 0.58 and 0.47 267
9.1
Special cases of the generic measurement model . . . . . . . . . . . . . . 285
9.2
Prior and posterior statistics for beta distribution with r
successes in n trials . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 291
9.3
Response pattern counts with proﬁciency variable, θ . . . . . . . . . . 295
9.4
Response pattern counts collapsing over proﬁciency variable, θ . 295
9.5
E-step probabilities for iterations 1, by response pattern . . . . . . . 304
9.6
E-step expected response pattern counts . . . . . . . . . . . . . . . . . . . . 304
9.7
E-step iteration 1 expectations of suﬃcient statistics . . . . . . . . . . 305
9.8
Trace of EM parameter estimates. . . . . . . . . . . . . . . . . . . . . . . . . . . 306
9.9
MCMC cycle response pattern counts . . . . . . . . . . . . . . . . . . . . . . . 318
9.10
MCMC parameter draws from intervals of 1000 and summary
statistics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 320
9.11
Approximating Dirichlet priors from posterior means and
standard deviations for π31 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 322
9.12
Response pattern counts for online calibration . . . . . . . . . . . . . . . 323
9.13
Average parameter values from initial and Online calibrations . . 324
10.1
Actual and predicted outcomes for the hypothetical medical
licensure exam. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 364
10.2
Logarithmic scores for ten student outcome vectors . . . . . . . . . . . 365
10.3
Deviance values for two ACED models . . . . . . . . . . . . . . . . . . . . . . 366
10.4
Observed outcome for two items for Exercise 10.10 . . . . . . . . . . . 367
11.1
Skill requirements for fraction subtraction items . . . . . . . . . . . . . . 374
11.2
Equivalence classes and evidence models . . . . . . . . . . . . . . . . . . . . 376
11.3
Summary statistics for binary-skills model . . . . . . . . . . . . . . . . . . . 390
11.4
Selected student responses . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 391
11.5
Prior and posterior probabilities for selected examinees . . . . . . . . 392
11.6
Summary statistics for binary-skills model, Admin 1 . . . . . . . . . . 394

List of Tables
XXXIII
11.7
Summary statistics for binary-skills model, Admin 2 . . . . . . . . . . 395
11.8
Item-ﬁt indices for the mixed-number subtraction test. . . . . . . . . 404
11.9
Person-ﬁt p-values for selected students . . . . . . . . . . . . . . . . . . . . . 404
13.1
Summary of the four processes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 469
13.2
Confusion matrix for binary proﬁciency and observable . . . . . . . . 479
13.3
Expected accuracy matrix for observable PC3 in Task Exp4.1. . 483
13.4
MAP accuracy matrix for Task Exp4.1 . . . . . . . . . . . . . . . . . . . . . . 483
13.5
MAP accuracy matrix for Task Exp6.1 . . . . . . . . . . . . . . . . . . . . . . 483
13.6
Three-way table of two observables given proﬁciency variable . . 484
13.7
Three-way table of two observables given marginal proﬁciency. . 485
13.8
Calculation of expected weight of evidence . . . . . . . . . . . . . . . . . . . 496
13.9
Data for Exercise 13.4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 502
13.10 Ten randomly selected entries from a set of pretest data . . . . . . . 502
13.11 Expected accuracy matrix for two experimental tasks . . . . . . . . . 503
13.12 Expected accuracy matrix (normalized) for two multiple-choice
tasks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 503
13.13 Data for diﬀerential task functioning detection problem
(Exercise 13.8) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 503
13.14 Data for conditional independence test problem (Exercise 13.9) 504
13.15 Calculation of expected weight of evidence after one observation504
14.1
A hierarchical textual representation of science standards . . . . . . 509
14.2
Potential observations related to scientiﬁc investigation. . . . . . . . 514
14.3
Segments in the Biomass “mice” scenario . . . . . . . . . . . . . . . . . . . . 522
14.4
Connecting knowledge representations with investigation steps . 525
14.5
Rules of evidence for table task . . . . . . . . . . . . . . . . . . . . . . . . . . . . 532
15.1
Task and evidence models from the ﬁrst Biomass segment . . . . . 551
15.2
Initial conditional distributions for observables 2–7 of Task 1 . . . 556
15.3
Initial conditional distributions for observable 1 of Task 2 . . . . . 558
15.4
Initial conditional probability distributions for all three
observables of Task 3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 559
15.5
Initial conditional distribution for observable 1 of Task 4 . . . . . . 560
15.6
Summary statistics of parameter prior distributions . . . . . . . . . . . 562
15.7
Observed responses . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 564
15.8
Summary statistics of prior and posterior population
parameter distributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 567
15.9
Summary statistics of item parameter distributions . . . . . . . . . . . 568
15.10 Prior and posterior expected proﬁciency levels . . . . . . . . . . . . . . . 569
15.11 Revised conditional distributions for observable 3 of Task 1 . . . . 571
15.12 Revised conditional probability table for observable 4 of Task 1 572
15.13 Revised conditional distributions for observable 1 of Task 2 . . . . 573
15.14 A set of simulated preposterior predictive responses . . . . . . . . . . . 576

Part I
Building Blocks for Bayesian Networks

1
Introduction
David Schum’s 1994 book, The Evidential Foundations of Probabilistic Rea-
soning, changed the way we thought about assessment. Schum, a psychologist
cum legal scholar, was writing about evidence in the most familiar meaning of
the word, looking at a lawyer’s use of evidence to prove or disprove a propo-
sition to a jury. However, Schum placed that legal deﬁnition in the context
of the many broader uses of the term “evidence” in other disciplines. Schum
notes that scientists and historians, doctors and engineers, auto mechanics,
and intelligence analysts all use evidence in their particular ﬁelds. From their
cross-disciplinary perspectives, philosophers, statisticians, and psychologists
have come to recognize basic principles of reasoning from imperfect evidence
that cut across these ﬁelds.
Mislevy (1994) shows how to apply the idea of evidence to assessment
in education. Say, for example, we wish to show that a student having com-
pleted a reading course, is capable of reading, with comprehension, an article
from The New York Times. We cannot open the student’s brain and observe
directly the level of comprehension, but we can ask the student questions
about various aspects of articles she reads. The answers provide evidence of
whether or not the student comprehended what she read, and therefore has
the claimed skill.
Schum (1994) faced two problems when developing evidential arguments in
practical settings: uncertainty and complexity. We face those same problems in
educational assessment and have come to adopt the same solutions: probability
theory and Bayesian networks.
Schum (1994) surveys a number of techniques for representing imprecise
and uncertain states of knowledge. While approaches such as fuzzy sets, belief
functions, and inductive probability all oﬀer virtues and insights, Schum grav-
itates to probability theory as a best answer. Certainly, probability has had
the longest history of practical application and hence is the best understood.
Although other systems for representing uncertain states of knowledge, such as
Dempster–Shafer models (Shafer 1976; Almond 1995), may provide a broader
c⃝Springer Science+Business Media New York 2015
3
R. G. Almond et al., Bayesian Networks in Educational Assessment,
Statistics for Social and Behavioral Sciences, DOI 10.1007/978-1-4939-2125-6 1

4
1 Introduction
palate of states of knowledge that can be modeled, the greater ﬂexibility incurs
greater computational cost.
The idea that probability can be used to model an uncertain state of
knowledge is often omitted from elementary statistics courses. However, the
idea dates back to Bernoulli and other early developers of probability. This
use of probability to represent states of knowledge is most often found in the
Bayesian approaches to statistics (Chap. 3). It is a powerful concept which
allows us to use probability theory to make complex inferences from uncertain
and incomplete evidence.
In complex situations, it can be diﬃcult to calculate the probability of an
event, especially if there are many dependencies. The solution is to draw a
picture. A graphical model, a graph whose nodes represent the variables and
whose edges represent dependencies between them, provides a guide for both
constructing and computing with the statistical models. Graphical models in
which all the variables are discrete, have some particular computational advan-
tages. These are also known as Bayesian networks because of their capacity to
represent complex and changing states of information in a Bayesian fashion.
Pearl (1988) popularized this approach to represent uncertainty, especially
in the artiﬁcial intelligence community. Since then, it has seen an explosive
growth.
This book explores the implications of applying graphical models to edu-
cational assessment. This is a powerful technique that supports the use of
more complex models in testing, but is also compatible with the models and
techniques that have been developing in psychometrics over the last century.
There is an immediate beneﬁt of enabling us to build models which are closer
to the cognitive theory of the domain we are testing (Pelligrino et al. 2001).
Furthermore, this approach can support the kind of complexity necessary for
diagnostic testing and complex constructed response or interactive tasks.
This book divides the story of Bayesian networks in educational assess-
ment into three parts. Part I describes the basics of properties of a Bayesian
network and how they could be used to accumulate evidence about the state
of proﬁciency of a student. Part II describes how Bayesian networks can be
constructed, and in particular, how both the parameters and structure can
be reﬁned with data. Part III ties the mathematics of the network to the
evidence-centered assessment design (ECD) framework for developing assess-
ments and contains an extensive and detailed example. The present chapter
brieﬂy explores the question of why Bayesian networks provide an interesting
choice of measurement model for educational assessments.
1.1 An Example Bayes Network
Bayesian networks are formally deﬁned in Chap. 4, but a simple example will
help illustrate the basic concepts.

1.1 An Example Bayes Network
5
Example 1.1 (Language Testing Example). (Mislevy 1995c). Imagine a
language assessment which is designed to report on four proﬁciency variables:
Reading, Writing, Speaking and Listening. This assessment has four types
of task: (1) a reading task, (2) a task which requires both writing and read-
ing, (3) a task which requires speaking and either reading or listening, and
(4) a listening task. Evaluating the work product (selection, essay or speech)
produces a single observable outcome variable for each task. These are named
Outcome R, Outcome RW, Outcome RLS, and Outcome L respectively.
Reading
Writing
Speaking
Listening
Outcome R
Outcome RW
Outcome RSL
Outcome L
Fig. 1.1 A graph for the Language Testing Example
A Bayesian network for the language test example of Mislevy (1995c). Rounded
rectangles in the picture represent variables in the model. Arrows (“edges”) represent
patterns of dependence and independence among the variables. This graph provides
a visual representation of the joint probability distribution over the variables in the
picture. Reprinted with permission from Sage Publications.
Figure 1.1 shows the graph associated with this example. Following conven-
tions from ECD (cf. Chaps. 2 and 12), the nodes (rounded rectangles in the
graph) for the proﬁciency variables are ornamented with a circle, and the
nodes for the evidence variables are ornamented with a triangle. The edges
in the graph ﬂow from the proﬁciency variables to the observable variables
for tasks which require those proﬁciencies. Thus the graph gives us the infor-
mation about which skills are relevant for which tasks, providing roughly the
same information that a Q-Matrix does in many cognitively diagnostic assess-
ment models (Tatsuoka 1983).
The graphs used to visualize Bayesian networks, such as Fig. 1.1, act as
a mechanism for visualizing the joint probability distribution over all of the
variables in a complex model, in terms of theoretical and empirical relation-
ships among variables. This graphical representation provides a shared work-
ing space between subject matter experts who provide insight into the cogni-
tive processes underlying the assessment, and psychometricians (measurement
experts) who are building the mathematical model. In that sense, Bayesian

6
1 Introduction
networks have a lot in common with path analysis and structural equation
models.
However, an important diﬀerence between Bayesian networks and these
other models using similar graphical structures is the way that the Bayes
nets encode conditional independence conditions in the graph. Essentially,
variables that are separated in the graph are independent given the values of
the variables which separate them. (The exact rules depend on what evidence
has and has not been observed and are given in Chap. 4.) These conditional
independence constraints lead to eﬃcient algorithms for updating probability
distributions (cf, Chap. 5; Appendix A lists readily available software packages
which implement those algorithms).
As the name implies, Bayesian networks are based on Bayesian views of
statistics (see Chap. 3 for a review). The key idea is that a probability dis-
tribution holds a state of knowledge about an unknown event. As Bayesian
networks represent a probability distribution over multiple variables, they
represent a state of knowledge about those variables.
Usually, the initial state of a Bayesian network in educational assessment is
based on the distribution of proﬁciency in the target population for an assess-
ment and the relationship between those proﬁciencies and task outcomes in the
population. The probability values could have come from theory, expert opin-
ion, experiential data, or any mixture of the three. Thus, the initial state of
the Bayes net in Fig. 1.1 represents what we know about a student who enters
the testing center and sits down at a testing station to take the hypothetical
language test: the distribution of proﬁciencies in the students we typically see,
and the range of performance we typically see from these students.
As the student performs the assessment tasks, evaluating the work prod-
ucts using the appropriate evidence rules yields values for the observable out-
come variables. The values of the appropriate variables in the network are
then instantiated or set to these values, and the probability distributions
in the network are updated (by recursive applications of Bayes rule). The
updated network now represents our state of knowledge about this student
given the evidence we have observed so far. This is a powerful paradigm for
the process of assessment, and leads directly to mechanisms for explaining
complex assessments and adaptively selecting future observations (Chap. 7).
Chapter 13 describes how this can form the basis for an embedded scoring
engine in an intelligent tutoring or assessment system.
The fact that the Bayes net represents a complete Bayesian probability
model has another important consequence: such models can be critiqued and
reﬁned from data. Complete Bayesian models provide a predictive probability
for any observable pattern of data. Given the data pattern, the parameters of
the model can be adjusted to improve the ﬁt of the model. Similarly, alterna-
tive model structures can be proposed and explored to see if they do a better
job of predicting the observed data. Chapters 9–11 explore the problems of
calibrating a model to data and learning model structure from data.

1.2 Cognitively Diagnostic Assessment
7
Bayesian network models for assessments are especially powerful when
used in the context of ECD (Mislevy et al. 2003b) Chap. 2 gives a brief intro-
duction to some of the language used with ECD, while Chap. 12 explains it in
more detail. The authors have used ECD to design a number of assessment,
and our experience has caused us to come to value Bayesian networks for
two reasons. First, they are multivariate models appropriate for cognitively
diagnostic assessment (Sect. 1.2). Second, they help assessment designers to
explicitly draw the connection between measurement models and cognitive
models that underlie them (Sect. 1.3).
1.2 Cognitively Diagnostic Assessment
Most psychometricians practicing today work with high-stakes tests designed
for selection, placement, or licensing decisions. This is no accident. Errors
and ineﬃciencies in such tests can have high costs, both social and mone-
tary, so it is worthwhile to employ someone to ensure that the reliability and
validity of the resulting scores are high. However, because of the prominence
of selection/placement tests, assumptions based on the selection/placement
purpose and the high stakes are often embedded in the justiﬁcation for par-
ticular psychometric models. It is worth examining closely the assumptions
which come from this purpose, to tease apart the purposes, the statistics, and
the psychology that are commingled in familiar testing practices.
First, it is almost always good for a selection/placement assessment to
be unidimensional. The purpose of a college admission oﬃcer looking at an
assessment is to rank order the candidates so as to be better able to make
decisions about who to admit. This rank ordering implies that the admissions
oﬃcer wants the candidates in a single line. The situation with licensure and
certiﬁcation testing is similar; the concern is whether or not the candidate
makes the cut, and little else.
Because of the high stakes, we are concerned with maximizing the validity
of the assessment—the degree to which it provides evidence for the claims we
would like to make about the candidate. For selection and placement situa-
tions, a practically important indicator of validity is the degree to which the
test correlates with a measure of the success after the selection or placement.
Test constructors can increase this correlation by increasing the reliability of
the assessment—the precision of the measurement, or, roughly, the degree to
which the test is correlated with itself. This can lead them to discard items
which are not highly correlated with the main dimension of the test, even if
they are of interest for some other reason.
Although high-stakes tests are not necessarily multiple choice, multiple
choice items often play a large role in them. This is because multiple choice is
particularly cost eﬀective. The rules of evidence—procedures for determining
the observable outcome variables—for multiple choice items are particularly
easy to describe and eﬃcient to implement. With thoughtful item writing,

8
1 Introduction
multiple choice items can test quite advanced skills. Most importantly, they
take little of the student’s time to answer. A student can solve 20–30 multi-
ple choice items in the time it would take to answer a complex constructed
response task like an essay, thus increasing reliability. While a complex con-
structed response task may have a lower reliability than 20–30 multiple choice
items, it may tap skills (e.g., generative use of language) that are diﬃcult to
measure in any other way. Hence the complex constructed response item can
increase validity even though it decreases reliability.
However, the biggest constraints on high stakes testing come from security
concerns. With high stakes comes incentive to cheat, and the measures to
circumvent cheating are costly. These range from proctoring and verifying the
identity of all candidates, to creating alternative forms of the test. The last of
these produces a voracious appetite for new items as old ones are retired. It
also necessitates the process of equating between scores on alternative forms
of the test.
Increasingly, the end users of tests want more than just a single score to
use for selection or placement. They are looking for a set of scores to help
diagnose problems the examinee might be facing. This is an emerging ﬁeld
called cognitively diagnostic assessment (Leighton and Gierl 2007; Rupp et al.
2010). The “cognitive” part of this name indicates that scores are chosen to
reﬂect a cognitive model of how students acquire skills (see Sect. 1.3). The
“diagnostic” part of the name reﬂects a phenomenon that seeks to identify
and provide remedy for some problem in a students’ state of proﬁciency. Such
diagnostic scores can be used for a variety of purposes: as an adjunct to a high
stakes test to help a candidate prepare, as a guidance tool to help a learner
choose an appropriate instructional strategy, or even shaping instructions on
the ﬂy in an intelligent tutoring system. Often these purposes carry much
lower stakes, and hence less stringent requirements for security.
Nowhere is the interplay between high stakes and diagnostic assessment
more apparent than in the No Child Left Behind (NCLB) Act passed by the
U.S. Congress in 2002 and the Race To the Top program passed as part of
the American Reinvestment and Recovery Act of 2009. The dual purpose of
assessments—accountability and diagnosis at some level—remains a part of
the U.S. educational landscape. Under these programs, all children are tested
to ensure that they are meeting the state standards. Schools must be making
adequate progress toward bringing all students up to the standards. This, in
turn, means that educators are very interested in why students are not yet
meeting the standards and what they can do to close the gap. They need
diagnostic assessment to supplement the required accountability tests to help
them identify problems and choose remedies.
When we switch the purpose from selection to diagnosis, everything
changes. First and foremost, a multidimensional concept of proﬁciency usually
underlies cognitively diagnostic scoring. (A metaphor: Whereas for a selection
exam we might have been content with knowing the volume of the examinee,

1.2 Cognitively Diagnostic Assessment
9
in a diagnostic assessment we want to distinguish examinees who are tall but
narrow and shallow from those who are short, wide and shallow and those
who are short, narrow and deep.) As a consequence, the single score becomes
a multidimensional proﬁle of student proﬁciency. Lou DiBello (personal com-
munication) referred to such tests as proﬁle score assessments.
The most important and diﬃcult part of building a multidimensional
model of proﬁciency is identifying the right variables. The variables (or suit-
able summaries) must be able to produce scores that the end users of the
assessment care about: scores which relate to claims we wish to make about
the student and educational decisions that must be made. That is, it is not
enough that the claims concern what students know and can do; they must
be organized in ways that help teachers improve what they know and can do.
A highly reliable and robust test built around the wrong variables will not be
useful to end users and consequently will fall out of use.
Another key diﬀerence between a single score test and a proﬁle score test
is that we must specify how each task outcome depends on the proﬁciency
variables. In a proﬁle score assessment, for each task outcome, we must answer
the questions “What proﬁciencies are required?”; “How are they related in
these requirements?”; and “To what degree are they involved?” This is the
key to making the various proﬁciency variables identiﬁable. In a single score
assessment, each item outcome loads only onto the main variable; the only
question is with what strength. Consequently, assessment procedures that
are tuned to work with single score assessments will not provide all of the
information necessary to build a proﬁle score test.
Suppes (1969) introduced a compact representation of the relationship
between proﬁciency and outcome variables for a diagnostic test called the Q-
Matrix. In the Q-Matrix, columns represent proﬁciency variables and rows
represent items (observable outcome variables). A one is placed in the cells
where the proﬁciency is required for the item, and a zero is placed in the other
cells. Note that an alternative way to represent graphs is through a matrix
with ones where an edge is present and zero where there is no edge. Thus, there
is a close connection between Bayesian network models and other diagnostic
models which use the Q-Matrix (Tatsuoka 1983; Junker and Sijtsma 2001;
Roussos et al. 2007b).
The situation can become even more complicated if the assessment includes
complex constructed response tasks. In this case, several aspects of a student’s
work can provide evidence of diﬀerent proﬁciencies. Consequently, a task may
have multiple observable outcomes. For example, a rater could score an essay
on how well the candidate observed the rules of grammar and usage, how well
the candidate addressed the topic, and how well the candidate structured the
argument. These three outcomes would each draw upon diﬀerent subsets of
the collection of proﬁciencies measured by the assessment.
Some of the hardest work in assessment with complex constructed response
tasks goes into deﬁning the scored outcome variables. Don Melnick, who for

10
1 Introduction
several years led the National Board of Medical Examiners (NBME) project
on computer-based case management problems, observed “The NBME has
consistently found the challenges in the development of innovative testing
methods to lie primarily in the scoring arena. Complex test stimuli result in
complex responses which require complex models to capture and appropriately
combine information from the test to create a valid score” (Melnick 1996,
p. 117).
The best way to do this is to design forward. We do not want to wait for a
designer to create marvelous tasks, collect whatever data result, and throw it
over the wall for the psychometrician to ﬁgure out “how to score it.” The most
robust conclusion from the cognitive diagnosis literature is this: Diagnostic
statistical modeling is far more eﬀective when applied in conjunction with task
design from a cognitive framework that motivates both task construction and
model structure, than when applied retrospectively to existing assessments
(Leighton and Gierl 2007).
Rather, we start by asking what we can observe that will provide evidence
that the examinee has the skill we are looking for. We build situations with
features that draw on those skills, and call for the examinee to say, do, or
make something that provides evidence about them—work products. We call
the key features of this work observable outcome variables, and the rules for
computing them, rules of evidence. For example, in a familiar essay test the
observable outcomes are the one or more scores assigned by a rater, and the
rules of evidence are the rubrics the rater uses to evaluate the essay as to its
qualities.
A richer example is HYDRIVE (Gitomer et al. 1995), an intelligent tutor-
ing system built for the US Air Force and designed to teach troubleshoot-
ing for the hydraulics systems of the F-15 aircraft. An expert/novice study
of hydraulics mechanics revealed that experts drew on a number of trou-
bleshooting strategies that they could bring to bear on problems (Steinberg
and Gitomer 1996). For example, they might employ a test to determine
whether the problem was in the beginning or end of a series of components
that all had to work for a ﬂap to move when a lever was pulled. This strategy
is called “space splitting” because it splits the problem space into two parts
(Newell and Simon 1972). HYDRIVE was designed to capture information
not only about whether or not the mechanic correctly identiﬁed and repaired
the problem, but also about the degree to which the mechanic employed eﬃ-
cient strategies to solve the problem. Both of these were important observable
outcomes.
However, when there are multiple aspects of proﬁciency and tasks can have
multiple outcomes, the problem of determining the relationships between pro-
ﬁciencies and observable variables becomes even harder. In HYDRIVE, both
knowledge of general troubleshooting strategies and the speciﬁc system being
repaired were necessary to solve most problems. Thus each task entailed a
many-to-many mapping between observable outcomes and proﬁciency vari-
ables.

1.3 Cognitive and Psychometric Science
11
The solution for HYDRIVE was to draw a graph (Mislevy and Gitomer
1996). By drawing arrows from the skills required to the observable outcomes,
we could untangle the complex relationships. Furthermore, the joint proba-
bility distribution over the variables in the model could be represented with
a Bayesian network. This network is a graphical model whose graph repre-
sents the relationships between the skills and observations we just described.
Expressing our understanding of the problem as a Bayesian network brings
with it a number of computational advantages which are described in this
book.
ECD grew out of a desire to generalize what worked well about HYDRIVE
to other assessments. Since that time, the authors have participated in many
design projects using ECD and Bayesian networks, including DISC (Mislevy,
Steinberg, et al. 1999b; Mislevy, Steinberg, Breyer, et al. 2002d), Biomass
(Steinberg et al. 2003, Chaps. 14 and 15), NetPASS (Behrens et al. 2004),
ACED (this book, Chaps. 7 and 13; Shute 2004; Shute et al. 2005; Shute et
al. 2008), an alternative scoring method for ETS’s ICT Literacy assessment
(Katz et al. 2004), and a game-based assessment called SimCityEDU (Mislevy
et al. 2014).
1.3 Cognitive and Psychometric Science
The HYDRIVE experience taught us many lessons. Among them was the
amount of work required to build a diagnostic assessment that truly relates to
variables learners and educators care about. Building such assessments consis-
tently and in a cost eﬀective way demands an approach to assessment design
that supports many kinds of assessments, both familiar selection assessments
and new kinds of diagnostic assessments. Furthermore, it requires a philosophy
of assessment design that would provide a framework for answering questions
when new problems inevitably arise.
As we based this new approach on the principle of ﬁnding evidence for
the knowledges, skills, and abilities we were testing, we called this approach
ECD. The basic approach can be laid out in four steps:
1. Gather together the claims we wish to make about an examinee, for exam-
ple, “An examinee who scores highly can pick up and read with compre-
hension a journal article written in English in their ﬁeld of expertise.”
2. Organize these claims into a proﬁciency model, constructing variables rep-
resenting the knowledges, skills, and abilities required to meet the claims.
3. Determine what we could observe in a candidate’s work which would pro-
vide evidence that the candidate did (or did not, or did to what extent or
in what way) have a particular complex of proﬁciencies.
4. Structure tasks which will provide us with the opportunity to make those
kinds of observations.

12
1 Introduction
Note that this design philosophy has the validity argument built right in.
Ultimately, validity is how well the scores from an assessment support the
claims the assessment makes. In an ECD assessment, this argument is the
central core. Task results provide observable outcomes that provide evidence
about proﬁciency variables which support the claims of the assessment. Any
valid assessment obeys this principle, but ECD forces us to think about it
from the very start of the assessment design process. Other authors who have
thought about assessment design in similar ways include Susan Embretson
(Embretson 1998), who focuses on psychological measurement, Grant Wig-
gins (Wiggins 1998), who focuses on instructional assessment, and Ric Luecht
(Luecht 2012), who focuses on the re-usability of task design, delivery, and
scoring components.
The proﬁciency model for HYDRIVE was based on the cognitive theory of
the domain. This cognitive basis simpliﬁed the process of designing instruc-
tions to help learners acquire the knowledge, skills, and abilities underlying
the proﬁciency variables. It further ensured that reporting would be in terms
of concepts that were useful for the intended audience.
Contrast this to trait theories of psychology in which the latent trait being
measured is eﬀectively deﬁned in terms of how it is measured. Take for example
IQ tests, in which intelligence has been deﬁned as being what the IQ test
measures. This is unsatisfactory in that it is diﬃcult to see how to provide
training to increase one’s intelligence.
Furthermore, the trait theory breaks down as we introduce multiple traits.
When the traits are deﬁned after the fact, there is a question of identiﬁabil-
ity (rotational indeterminacy is the bane of factor analysis!). We can always
relabel the traits to create a new “theory.” Obviously, one needs a better way
to identify the variables in the proﬁciency model.
Bayesian networks and evidence-centered design support multidimensional
models in terms of cognitive theory—speciﬁcally, with models motivated by an
information-processing perspective, but posited as a working approximation
of proﬁciency in some domain rather than an authentic representation of the
details of cognition. Variables are introduced to stand for aspects of an exam-
inee’s proﬁciency—elements of knowledge, strategies and procedures, tenden-
cies to solve problems that have certain properties, and so on. By using prob-
abilities to represent the examiner’s uncertain state of knowledge about an
examinee’s proﬁciency variables, Bayesian networks can represent the theory
of the domain, modeling complex relationships among proﬁciency variables.
Bayesian networks can model quite complex relationships between observ-
able and proﬁciency variables. These evidence models allow us to update our
knowledge about a student’s proﬁciency as more evidences (in the form of
observations from tasks) arrive. The Bayesian network tracks our state of
knowledge about a particular student. It starts with general knowledge based
on the population of students that this individual is drawn from. Part I of

1.3 Cognitive and Psychometric Science
13
this book describes how Bayesian networks can be used as a mechanism to
update our knowledge about that student as more and more evidence arrives.
Part II discusses learning and revising models from data. When the model
is based on a cognitive theory, these activities take on additional importance.
Data which ﬁt the model fairly well provide support for the underlying cog-
nitive theory. Data which do not ﬁt well provide information about potential
gaps or weaknesses in the theory. The assessment designer is prompted to
consider revising the theory, the statistical model, or the way data are being
collected and interpreted. When the statistical model and cognitive theory
mirror one another, developments in one can be reﬂected in the other.
Often, the problem with cognitive theories is that it is diﬃcult or expen-
sive to measure the knowledge structures they posit. In our experience there
are two types of expertise that test designers bring to bear on the assess-
ment design process. One is the knowledge of the cognitive processes of the
domain, that is, of the proﬁciency model. The second is the knowledge about
how to structure tasks, including what makes tasks easier and harder, and
which aspects of knowledge or skills they tap. Often it is diﬃcult to make the
connection between these two types of expertise.
Evidence is the bridge between theories about tasks and theories about
proﬁciency. Asking “How can I get evidence that this subject has this proﬁ-
ciency?” leads to designs for tasks to assess that proﬁciency. Similarly, by ask-
ing “What knowledge, skills, and abilities are required to perform this task?”
we can understand what it is that the task provides evidence for. By driving
forward and backward over this bridge we can iteratively build assessments
that reﬂect our cognitive theory.
Although we are excited about the potential of graphical models to model
a broad range of cognitive theories, we cannot get around some of the fun-
damental laws of psychometrics. First, no matter what we claim that the
assessment is measuring, it eﬀectively measures the ability to perform tasks
like the ones in the assessment. Only if we have built the tasks with ﬁdelity
to the construct(s) we are trying to measure will the assessment provide evi-
dence to support its claims. Furthermore, a certain amount of evidence is
required to support each claim we are trying to make. For example, it would
be very diﬃcult to provide enough evidence to support a proﬁciency model
with 30 variables on a test that contains only 10 items (unless those items
represented large, complex tasks with many parts, and complex rules of evi-
dence were used to pull out many partially-dependent bits of evidence). Thus,
a fairly sophisticated knowledge of the strengths and limitations of the models
we are proposing is required to construct graphical models for use in educa-
tional assessment.

14
1 Introduction
1.4 Ten Reasons for Considering Bayesian Networks
Evidence-centered design, as set out in Mislevy et al. (2003b) and elsewhere,
is neutral to the measurement model. The principles apply whether Bayesian
networks, latent class analysis, classical test theory, factor analysis or item
response theory and its many extensions are used to score the assessment.
However, in applications, Bayes nets have been our ﬁrst choice for the mea-
surement model. Bayes nets enjoy a number of advantages, some immediately
visible to test users, some under the hood. Other methods may share some of
the properties, but the combination oﬀered by Bayes nets is unique.
1. Bayes nets report scores in terms of “Probability of Claim.” When built
using evidence-centered design, each level of each proﬁciency variable in
a Bayes net is associated with one or more ECD claims. Thus, the nat-
ural score report provides the probability that the claim holds. This is
exactly the kind of information that test users need to make instructional
planning decisions. It suggests that using the Bayes nets as a part of an
artiﬁcial intelligence planning system would make a powerful engine for an
intelligent tutoring system. However, even in the simpler world of human
instructional planning system, the kind of score reports described here
were thought to be useful by a focus group of score users (Jody Under-
wood, unpublished focus group results).
2. Bayes nets use a graphical representation for the proﬁciency model. Bayes
nets take their name from the network diagram or graph they use to
describe the relationship among the variables. This graph provides both a
rigorous mathematical description of the model and an informal schematic
description. This representation helps facilitate conversations between
cognitive and measurement experts. However, it goes deeper than that.
Daniel et al. (2003) suggest that even secondary school students ﬁnd this
representation valuable, and it can be used to facilitate a dialogue between
the instructor and the learner.
3. Bayes nets can incorporate expert knowledge about the cognitive domain.
Expert input is needed in any model building exercise, particularly in the
diﬃcult steps of deﬁning the variables and the relationships among them—
the “graphical structure” of the Bayesian network. Bayes net modeling
encourages cognitive experts to get involved in the process. This means
the structure of the model can be well suited to a particular purpose; for
example, a proﬁciency model can be built to reﬂect a particular instruc-
tional theory about a domain. It also means that Bayes nets can take full
advantage of other information gathered during an ECD design process,
such as prior opinions about the diﬃculty of a task.
4. Bayes nets can “learn” from data. Bayes nets are probability models. This
indicates that they make probabilistic predictions about what will hap-
pen. It also means that there is a measure for how well observed data meet
the expectations of the model. This property can be used to improve the

1.4 Ten Reasons for Considering Bayesian Networks
15
original model as more and more data become available. This learning can
be used to both adjust the parameters of the model and suggest changes
to the structure. The latter is instructive as it gives us feedback into the
cognitive models, which form the basis of the Bayes net. These last two
points taken together give many possible strategies for constructing Bayes
nets: from building networks entirely from expert opinion with no pretest-
ing, to building networks entirely from pretest data, and any number of
combinations of the two.
5. Bayes nets handle complex models and tasks. Complex tasks (e.g., simula-
tions, multistep problems, and complex constructed response) are in high
demand for assessments because they both feel more authentic and they
can tap higher order, constructive, and interactive skills that are diﬃcult
to capture with simpler tasks. Bayes nets tackle large problems by parsing
our reasoning about them as combinations of smaller more manageable
chunks. Speciﬁcally, Bayes nets can model multiple dependent observables
coming from complex tasks, each of which can provide evidence about dif-
ferent aspects of proﬁciency. All that is necessary to score such a task using
Bayes nets is to specify the model. Fitting data with such complex depen-
dencies is challenging for all measurement models, but Bayes nets oﬀer
an approach which reﬂects the cognitive model based on what happens
during the task.
6. Bayes nets are fast. By using only discrete variables, Bayes nets can obtain
exact, closed-form solutions to problems which would require numeric
approximations to diﬃcult integrals using other methods. This means that
Bayes nets models can be updated very quickly, making them suited for
embedded scoring engines (Biomass, NetPASS and ACED are all proto-
type systems using Bayes net scoring; ACED is even adaptive, and SimC-
ityEDU is fully interactive). Paradoxically, Bayes net models have a rep-
utation for being slow because their speed tempts designers to try larger
models than they would using other methods.
7. Bayes nets provide proﬁle scores. Bayes nets will provide scores on as many
variables as are available in the proﬁciency model. This means that Bayes
nets can provide subscores on dimensions that are meaningful according
to the underlying cognitive model. It also indicates that Bayes nets can
handle integrated tasks which address more than one proﬁciency. Further-
more, Bayes net can assess higher-level skills (such as science inquiry skills
in Biomass) in ways that obtain evidence about lower-level skills, and par-
tialling it out to understand what can be learned about the higher-level
skills.
8. Bayes nets provide real-time diagnosis. Because Bayes nets provide proﬁle
information quickly, they can be queried at any time in an assessment
situation. In particular, an intelligent tutoring system can use Bayes nets
to make decisions about when to continue assessment, when to switch to
instruction and what instruction would be expected to provide the most
value.

16
1 Introduction
9. Building Bayes nets is natural in the context of evidence-centered design.
It seems like there is a lot of information that needs to go into the con-
struction of a Bayesian network. However, much of this information must
be generated in the context of the assessment design no matter which
measurement model is eventually used to score the assessment. This is
especially true in the context of complex tasks, where often ad hoc scor-
ing software must be built to either score the whole assessment or simplify
the observed outcomes so that the outcomes can be analyzed with an oﬀ-
the-shelf measurement model. Essential questions such as which proﬁcien-
cies are relevant for which tasks, and what scores will be reported must
be answered no matter what the measurement model. Building Bayes
nets requires only that the questioning goes slightly deeper, asking the
experts questions about the strength of the relationships. With the ECD
design perspective and ECD design tools, much of the work of building
the Bayesian network ﬂows from the process of designing the assessment.
10. Bayes net models are “useful.” The statistician George Box (1976) stated
“All models are false, but some models are useful.” Bayes nets built using
ECD fall into what Berliner (2005) called “Bayesian hierarchical mod-
eling.” In particular, they incorporate a probabilistic data model and a
process model built around our cognitive understanding of the domain
to be assessed. Berliner claims that “Simple models in conjunction with
Bayesian hierarchical modeling may be better than either ‘more faithful
models’ or ‘statistical models.’ ” The most useful model for every purpose
may not be a Bayes net, but Bayes nets will often be a worthwhile place
to look for useful models.
1.5 What Is in This Book
This book gathers together in one place many of the ideas and structures that
form the psychometric underpinnings of evidence-centered design. It concen-
trates on the mathematical models underlying evidence-centered design, in
particular, the use of graphical models to represent cognitive models of a
domain. It talks about how to build the models, score assessments with them,
and reﬁne them from data.
This book is not a complete description of ECD. In particular, it does
not deal with many of the aspects of how one designs a cognitive model of a
domain and then reﬁnes it for assessment. That part of the story will be left
for a future book, although much of it has been laid out in a series of papers
(Behrens et al. 2012, Mislevy et al., 1999b; Mislevy et al. 2003c; Mislevy et al.
2002c, 2003b; Mislevy et al. 2006). Chapters 2, 12, and 14 touch upon some
of these broader issues, but mostly in service of setting the context for the
more mathematical work.
The goal of this book is to give the reader enough familiarity with Bayesian
networks and other graphical models to be able to build models to mirror a

1.5 What Is in This Book
17
particular cognitive theory and support it with an assessment. It deliberately
takes a model construction rather than computational emphasis. Many other
texts (e.g., Pearl 1988; Jensen 1996; Almond 1995; Cowell et al. 1999; Neapoli-
tan 1990; Neapolitan 2004) cover the computational aspects. Our goal is to
give enough background to discuss the implications for model construction
and understand the connections with other literature working with graphical
models.
The book is designed to be used as a textbook for a graduate student
in education or psychology. We have assumed that the student has taken at
least one course in probability and statistics, or a discipline-based course with
a strong statistical component. We also assume some familiarity with testing,
in particular, item response theory. Although this is not a prerequisite per
se, many of the examples and explanations draw on ideas from item response
theory. We have added exercises at the end of the chapters to facilitate its use
as a textbook. Chapters 6 and 11 concentrate on extensive data sets which
can be used for larger projects. Appendix A contains links to software tools
for working with Bayes nets and links to data sets which are available for
classroom projects, including an online glossary to provide quick reference to
deﬁnitions for terms relating to ECD and Bayesian networks.
The chapters are organized into three parts. Part I lays out the basic
deﬁnitions and key ideas from the theory of graphical models that will be used
throughout the rest of the book. Part II moves to the challenges of building,
estimating, and revising models from data. Part III brings these tools fully to
bear on problems of educational assessment.
Part I is not meant to be a complete course in Bayesian networks, but
we have included enough of the essential deﬁnitions and algorithms to enable
readers to follow the applications in educational testing. Readers looking for a
more complete introduction should see Pearl (1988) or Jensen (1996). Readers
looking for a more mathematical treatment should see Whittaker (1990) or
Lauritzen (1996).
Chapter 2 provides an overview of evidence-centered design both to moti-
vate the subsequent mathematics and to introduce some terms that will not be
formally deﬁned until Chap. 12. Chapter 3 provides a review of probability and
Bayesian statistics, with careful attention to representing states of knowledge
with probability distributions. Chapter 4 provides some of the basic deﬁnitions
of graph theory and Bayesian networks, paying particular attention to the rep-
resentation of conditional independence with graphs. Chapter 5 describes the
basic algorithms for moving probability around graphs, and how we can use
them to draw inferences about students’ proﬁciencies based on the outcomes
of assessment tasks. Chapter 6 looks at some examples of the application of
Bayesian networks to educational testing. Chapter 7 deﬁnes the concept of
weight of evidence and how it can be used to both explain scores and select
items.
While Part I concentrates on models for a single learner, Part II discusses
what can be done with data from many learners. Chapter 8 describes the

18
1 Introduction
parameters used in these models, and introduces some models with reduced
parameters. Chapter 9 describes the EM algorithm and Markov Chain Monte
Carlo (MCMC) estimation, the techniques we have been using to ﬁt models
to data. This is by no means a complete treatment for either of the two (e.g.,
Gilks et al. (1996) and Lynch (2007) provide good starting points for MCMC).
Chapter 10 looks at the problem of diagnosing problems with the model from
patterns in the data. Learning models from data is a natural extension of
model criticism, and the chapter includes a brief survey of the literature on
learning models. Chapter 11 looks at our experiences in applying these ideas
to one particular example.
While the ﬁrst two parts primarily address the mathematical aspects of
using Bayesian networks as a measurement model for an assessment, Part III
ties the mathematics back to psychology and the assessment design. Chap-
ter 12 deﬁnes the basic design elements of evidence-centered design and
describes the construction of a model. Chapter 13 shows how to use the mathe-
matics to build a scoring engine for an online assessment or intelligent tutoring
system.
Chapters 14 and 15 explore a prototype assessment for high school trans-
mission genetics, named Biomass. Chapter 14 provides a sketch of how the
system was designed; in particular, how the proﬁciency model was constructed
from national science standards, how tasks were developed, and observable
variables were deﬁned for assessing higher order skills that involve applying
the scientiﬁc method in the context of biology. Chapter 15 explores the con-
struction of the Bayesian network scoring engine for Biomass, both how it was
constructed from expert opinion and how pilot data could be used to update
the model parameters.
The last chapter reviews some of what we have learned in the course of
applying graphical models to educational assessment. This ﬁeld is very new,
and there is a lot more to learn. Many parts of psychometrics that have been
well explored in the context of item response theory (IRT) and classical test
theory remain to be developed for graphical models. The ﬁnal chapter surveys
some of these research frontiers.

2
An Introduction to Evidence-Centered Design
Although assessment design is an important part of this book, we do not tackle
it in a formal way until Part III. Part I builds up a class of mathematical
models for scoring an assessment, and Part II discusses how the mathemat-
ical models can be reﬁned with data. Although throughout the book there
are references to cognitive processes that the probability distributions model,
the full discussion of assessment design follows the discussion of the more
mathematical issues.
This presents two problems. First, a meaningful discussion of the statistical
modeling of the assessment requires a basic understanding of the constraints
and aﬀordances of the assessment design process. The second is that the dis-
cussion of the statistical models and processes requires certain technical terms,
in particular, proﬁciency model, evidence model, task model, and assembly
model, that are not formally deﬁned until Chap. 12. This chapter provides
brief working deﬁnitions which will be suﬃcient to describe the mathematical
models, leaving the more nuanced discussion of assessment design until after
the mathematical tools have been deﬁned.
Evidence-centered design (ECD) is an approach to constructing educa-
tional assessments in terms of evidentiary arguments. This chapter introduces
the basic ideas of ECD, including some of the terminology and models that
have been developed to implement the approach. In particular, it presents
the high-level models of the Conceptual Assessment Framework (see also
Chap. 12) and the four-process architecture for assessment delivery systems
(see also Chap. 13). Special attention is given to the roles of probability-
based reasoning in accumulating evidence across task performances, in terms
of belief about unobservable variables that characterize the knowledge, skills,
and/or abilities of students. This is the role traditionally associated with psy-
chometric models, such as item response theory and latent class models. Later
chapters will develop Bayesian network models which unify the ideas and pro-
vide a foundation for extending probability-based reasoning in assessment
applications more broadly. This brief overview of evidence-centered design,
c⃝Springer Science+Business Media New York 2015
19
R. G. Almond et al., Bayesian Networks in Educational Assessment,
Statistics for Social and Behavioral Sciences, DOI 10.1007/978-1-4939-2125-6 2

20
2 An Introduction to Evidence-Centered Design
then, provides context for where and how graphical models ﬁt into the larger
enterprise of educational and psychological assessment.
2.1 Overview
All educational assessments have in common the desire to reason from partic-
ular things students say, do, or make, to inferences about what they know or
can do more broadly. Over the past century a number of assessment methods
have evolved for addressing this problem in a principled and systematic man-
ner. The measurement models of classical test theory and, more recently, item
response theory (IRT), latent class analysis, and cognitive diagnosis modeling,
have proved quite satisfactory for the large scale tests and classroom quizzes
with which every reader is by now quite familiar.
But oﬀ-the-shelf assessments and standardized tests are increasingly unsat-
isfactory for guiding learning and evaluating students’ progress. Advances in
cognitive and instructional sciences stretch our expectations about the kinds
of knowledge and skills we want to develop in students, and the kinds of obser-
vations we need to evidence them (Pelligrino et al. 2001; Moss et al. 2008).
Advances in technology make it possible to evoke evidence of knowledge more
broadly conceived, and to capture more complex performances. One of the
most serious bottlenecks we face, however, is making sense of complex data
that result.
Fortunately, advances in evidentiary reasoning (Schum 1994) and in sta-
tistical modeling (Gelman et al. 2013a) allow us to bring probability-based
reasoning to bear on the problems of modeling and uncertainty that arise nat-
urally in all assessments. These advances extend the principles upon which
familiar test theory is grounded to more varied and complex inferences from
more complex data (Mislevy 1994).
We cannot simply construct “good tasks” in isolation, however, and hope
that someone else down the line will ﬁgure out “how to score them.” We
must design a complex assessment from the very start around the inferences
we want to make, the observations we need to ground them, the situations
that will evoke those observations, and the chain of reasoning that connects
them (Messick 1994). We can expect iteration and reﬁnement as we learn,
from data, whether the patterns we observe accord with our theories and
our expectations; we may circle back to improve our theories, our tasks, or
our analytic models (Mislevy et al. 2012). But the point is that while more
complex statistical models may indeed be required, they should evolve from
the substance of the assessment problem, jointly with the purposes of the
assessment and the design of tasks to provide observable evidence.
ECD lays out a conceptual design framework for the elements of a coher-
ent assessment, at a level of generality that supports a broad range of assess-
ment types, from familiar standardized tests and classroom quizzes, to coached

2.2 Assessment as Evidentiary Argument
21
practice systems and simulation-based assessments, to portfolios and student–
tutor interaction. The design framework is based on the principles of evi-
dentiary reasoning and the exigencies of assessment production and delivery.
Designing assessment products in such a framework ensures that the way in
which evidence is gathered and interpreted bears on the underlying knowledge
and the purposes the assessment is intended to address. The common design
architecture further aids coordination among the work of diﬀerent special-
ists, such as subject matter experts, statisticians, instructors, task authors,
delivery-process developers, and interface designers. While the primary focus
of the current volume is building, ﬁtting, testing, and reasoning with statis-
tical models, this short chapter places such models into the context of the
assessment enterprise. It will serve to motivate, we hope, the following chap-
ters on technical issues of this sort. After that machinery has been developed,
Chap. 12 returns to ECD, to examine it more closely and work through some
examples.
Section 2.4 describes a set of models called the Conceptual Assessment
Framework, or CAF, and the four-process architecture for assessment delivery
systems. The CAF is not itself the assessment design process, but rather the
end product of the assessment design process. Although this book does not
cover the earlier stages of the design process, Sect. 2.3 touches on them brieﬂy.
Mislevy, Steinberg, and Almond (2003b) present a fuller treatment of ECD
including connections to the philosophy of argument and discussions of the
earlier stages of design. Almond et al. (2002a) and Almond et al. (2002b)
amplify the delivery system architecture and its connection to the design.
One of the great strengths of evidence-centered design is that it provides
a set of ﬁrst principles, based on evidentiary reasoning, for answering ques-
tions about assessment design. Section 2.2 provides a rationale for assessment
as a special case of evidentiary reasoning, with validity as the grounds for
the inferences drawn from assessment data (Cronbach 1989; Embretson 1983;
Kane 1992; Kane 2006; Messick 1989; Messick 1994; Mislevy 2009). ECD pro-
vides a structural framework for parsing and developing assessments from this
perspective.
2.2 Assessment as Evidentiary Argument
Advances in cognitive psychology deepen our understanding of how students
gain and use knowledge. Advances in technology make it possible to cap-
ture more complex performances in assessment settings, by including, for
example, simulation, interactivity, collaboration, and constructed responses
in digital form. Automated methods have become available for parsing com-
plex work products and identifying educationally meaningful features of them
Williamson et al. (2006b).
The challenge is in knowing just how to put all this new knowledge to work
to best serve the purposes of an assessment. Familiar practices for designing

22
2 An Introduction to Evidence-Centered Design
and analyzing single-score tests composed of familiar items are useful because
they are coherent, but the schemas are limited to the constraints under which
they evolved—the kinds of tasks, purposes, psychological assumptions, cost
expectations, and so on that deﬁne the space of tests they produce. Break-
ing beyond the constraints requires not only the means for doing so (through
advances such as those mentioned above) but schemas for producing assess-
ments that are again coherent, but in a larger design space; that is, assessments
that may indeed gather complex data to ground inferences about complex
proﬁciency models, to gauge multidimensional learning or to evaluate multi-
faceted programs—but which are built on a sound chain of reasoning from
what we propose to observe to what we want to infer. We want to design
in reverse direction: What do we want to infer? What then must we observe
in what kinds of situations, and how are the observations interpreted as evi-
dence?
Recent work on validity in assessment lays the conceptual groundwork
for such an approach. The contemporary view focuses on the support—
conceptual, substantive, and statistical—that assessment data provide for
inferences or actions (Messick 1989). From this view, an assessment is a spe-
cial case of evidentiary reasoning. Messick (1994) lays out the general form of
an assessment design argument in the quotation below. (We will look more
closely at assessment arguments in Sect. 12.1.2.)
A construct-centered approach [to assessment design] would begin by
asking what complex of knowledge, skills, or other attribute should
be assessed, presumably because they are tied to explicit or implicit
objectives of instruction or are otherwise valued by society. Next, what
behaviors or performances should reveal those constructs, and what
tasks or situations should elicit those behaviors? Thus, the nature of
the construct guides the selection or construction of relevant tasks as
well as the rational development of construct-based scoring criteria
and rubrics (p. 17).
This perspective organizes thinking for designing assessments for all kinds
of purposes, using all kinds of data, task types, scoring methods, and statistical
models. An assessment interpretation reasons from what we observe to what
we then believe about students’ proﬁciencies. Assessment design reasons in
the reverse direction, laying out the elements of an assessment in a way that
will support the needed interpretations.
For the purpose of the assessment, what are the proﬁciencies we are inter-
ested in? In what situations do people draw on them, to accomplish what
ends, using what tools and representations, and producing what kinds of out-
comes? Taking context and resources into account, we consider task situations
we can devise and observations we can make to best ground our inferences.
If interactions are key to getting evidence about some proﬁciency, for exam-
ple, we can delve into what features a simulation must contain, and what the
student must be be able to do, in order to exhibit the knowledge and skills

2.3 The Process of Design
23
we care about. We craft scoring methods to pick up the clues that will then
be present in performances. We construct statistical models that will synthe-
size evidence across multiple aspects of a given task performance, and across
multiple task performances. These decisions in the assessment design process
build the inferential pathway we then follow back from examinees’ behaviors
in the task setting to inferences about what they know or can do. From an
evidentiary reasoning perspective, we can examine the impact of these design
decisions on the inferences we ultimately want to make.
As powerful as it is in organizing thinking, simply having this concep-
tual point of view is not as helpful as it could be in carrying out the actual
work of designing and implementing assessments. A more structured frame-
work is needed to provide common terminology and design objects that make
the design of an assessment explicit and link the elements of the design to
the processes that must be carried out in an operational assessment. Such a
framework not only makes the underlying evidentiary structure of an assess-
ment more explicit, but it makes it easier to reuse and to share the operational
elements of an assessment. The evidence-centered design models address this
need.
2.3 The Process of Design
The ﬁrst step in an assessment design is to establish the purpose of the assess-
ment. Many fundamental design trade-oﬀs, e.g., assessment length versus reli-
ability, breadth across multiple aspects of proﬁciency versus depth in a single
proﬁciency, are ultimately resolved by deciding how to best meet the purpose
of the assessment. Fixing the purpose of the assessment early in the process
has a marvelous focusing eﬀect on the design and development processes.
Fixing the purpose, however, is easier said than done. Diﬀerent test users
may have diﬀerent and competing purposes in mind for a proposed assessment.
Expectations can be unrealistic, and can change over time. The purpose of
an assessment often starts as somewhat vague in the beginning of the design
process and becomes further reﬁned as time goes on.
The ECD framework describes the assessment design process in three
stages: domain analysis—gathering and organizing information related to the
cognitive background of the assessment as well as the purposes and constraints
of the design process; domain modeling—building a preliminary sketch of the
assessment argument as a general, reusable framework for a family of possible
assessments; and the conceptual assessment framework—ﬁlling in the details
of the initial sketch, particularly resolving design decisions to focus the prod-
uct on a particular purpose.
The lines between requirements-gathering, analysis, design, and implemen-
tation are diﬃcult to draw (indeed, the authors have argued among themselves
about which of the steps of the ECD process correspond to which steps of the
general engineering workﬂow). Describing the ECD process in phases might

24
2 An Introduction to Evidence-Centered Design
seem to suggest a waterfall development process, where each stage ﬂows into
the next and the ﬂow is just one way. Real-world assessment design processes
are usually iterative, with prototypes and cycles; things learned at later stages
of design often prompt the designer to revisit, rethink, and revise work done
at the earlier stages. Mislevy, Steinberg, and Almond (2003b) discussed the
ECD design process in more detail.
For the most part, this book does not delve deeply into these design issues
so that it can focus on the theory, the roles, and the mechanics of Bayesian
networks in the assessment argument. Most of the examples assume that the
conceptual assessment framework has already been speciﬁed. Only with the
Biomass example of Chaps. 14 and 15 do we work through the design process
from the very beginning: from targeted educational standards, through the
CAF, to the innovative, interactive tasks, and a Bayes nets scoring model that
result from the uniﬁed design process. It does not hurt to say again, though,
that complex measurement models such as Bayesian networks will provide
the greatest value when they arise from a principled design process to serve
an evidentiary argument, rather than applied retrospectively to data that are
collected without clear hypotheses connecting proﬁciencies and the situations
and performances that reveal them (iteration and reﬁnement notwithstand-
ing).
Section 2.4, then, describes the basic design objects of the CAF. The
domain-model design objects are basically lighter weight versions of their
CAF counterparts; detailed enough to support the assessment argument, but
not yet detailed enough to support implementation. In the domain modeling
phase, the design team are encouraged to think about how the assessment
argument would play out for multiple purposes and in multiple settings. It
helps to identify opportunities in which argument structures from one assess-
ment can be reused in another.
One kind of design object, developed in the early stages of the design pro-
cess but used extensively in the CAF, is the claim. A claim is a statement
about a participant that the assessment will provide evidence for (or against).
Claims are important because they give clarity to the purpose of an assess-
ment. One of the most important design decisions is deciding which claims will
be the primary focus of an assessment. Indeed, the whole question of validity
could be framed as determining to what extent an assessment really supports
its claims.
A simple example, used through the rest of the chapter, illustrates these
ideas.
Example 2.1 (Calculus Placement Exam). University C requires all stu-
dents to take 2 years of calculus, in the form of a two-semester freshman
sequence followed by a two-semester sophomore sequence. Typically a stu-
dent starts with the ﬁrst semester in the freshman year, but some students
(particularly those who took an advanced calculus class in high school) start
with the second semester, or with the third semester with the sophomore cal-

2.3 The Process of Design
25
culus class. Some students do not have the necessary background to begin the
sequence, and should take a precalculus remedial course ﬁrst. University C
administers a placement exam to all incoming freshmen to determine how to
best place them into the calculus sequence.
Claims in this assessment are based on the student having proﬁciencies that
are addressed in each of the courses in the calculus series. Examples include,
“Student can integrate functions of one variable,” and “Student can ﬁnd par-
tial derivatives of multivariate functions.” Note that there may be competing
interest in the claims. For example, the Physics department may have more
interest in the claim “Student can solve integrals in two and three dimen-
sions” while the Math department is more interested in the claim “Student
can construct a valid mathematical proof.”
Often claims are arranged hierarchically. For example, the claim “Student
can integrate functions of one variable” involves the subclaims, “Student can
integrate polynomial functions” and “Students can integrate trigonometric
functions” as well as the subclaims “Student can use transformation of vari-
ables to solve integrals” and “Student can use partial fractions to solve inte-
grals.” “Student can construct a valid mathematical proof” will need further
speciﬁcation with respect to the particular models and the kind of the proof at
issue (e.g., existence proof, induction, construction, proof by contradiction).
It will be seen that a set of claims is not suﬃcient to determine the proﬁciency
model for a given purpose. Composite claims that bundle ﬁner-grained claims
dealing with skills in the same semester are good enough for course placement,
but the ﬁner-grained claims would be distinguished for quizzes and diagnostic
tests during a semester.
In this particular case, the claims are relatively easy to establish. They will
fall naturally out of the syllabus for the calculus series and the calculus text
books. They are not simply a list of topics, but rather the kinds of problems,
proofs, and applications a student is expected to be able to carry out.
Another frequent source of claims is the educational standards published
by states and content area associations, such as the Next Generation Science
Standards (NGSS Lead States 2013). Grain size and speciﬁcity vary from one
set of standards to another, and often they need to be reﬁned or clariﬁed
to take the form of claims. They may not be phrased in terms of targeted
capabilities of students, or indicate what kinds of evidence is needed. It is
not enough say, for example, that “Student understands what constitutes a
valid mathematical proof.” Chapter 14 provides an example of moving from
standards to a framework of claims to ground an assessment.
Claims play two key roles in domain modeling: (1) including and excluding
speciﬁc claims clariﬁes the purpose of the assessment, and (2) laying them out
starts the process of developing an assessment argument. These roles are so
important that while most domain modeling design objects are reﬁned and
expanded in the CAF, claims remain largely in their initial form.

26
2 An Introduction to Evidence-Centered Design
2.4 Basic ECD Structures
In an ECD, an assessment design is expressed through a collection of objects
called the CAF. In any particular assessment, the objects in the CAF models
described in general terms in Sect. 2.4.1 will need to have been designed to
address the purposes of that particular assessment. In line with the Messick
quotation cited above, the characteristics of tasks have been selected to pro-
vide the opportunity to get evidence about the targeted knowledge and skills
(i.e., the claims); the scoring procedures are designed to capture, in terms of
observable variables, the features of student work that are relevant as evi-
dence to that end; and the characteristics of students reﬂected as proﬁciency
variables summarize evidence about the relevant knowledge and skills from a
perspective and at a grain size that suit the purpose of the assessment. The
CAF models provide the technical detail required for implementation: speci-
ﬁcations, operational requirements, statistical models, details of rubrics, and
so on.
CAF models provide speciﬁcations, but speciﬁcations are not an assess-
ment. As examinees and users of assessment ourselves, we see activities: Tasks
being administered, for example, and students interacting with task contexts
to produce essays or solve problems, raters evaluating performances or auto-
mated algorithms evaluating work, score reports being generated, and feed-
back being given to students in practice tests. We will organize all of this
activity in terms of processes, as described below. It is the CAF that speciﬁes
the structure and the relationships of the all content, messages, and products
involved in the processes. In other words, the CAF lays out the structural ele-
ments of an assessment that embody an assessment argument. The delivery
processes described below bring the assessment to life. They are real-world
activities that interact with students, gather evidence, and support inference
using those structures.
In describing both the design and implementation of scoring models and
algorithms, it is useful to have a generic model of the assessment delivery
process. Section 2.4.2 describes the four-process architecture that forms a ref-
erence model for the delivery of an assessment. The four processes of the
delivery system carry out, examinee by examinee, the functions of selecting
and administering tasks, interacting as required with the examinee to present
materials and capture work products, then evaluating responses from each
task and accumulating evidence across them. The information in the CAF
models specs out details of the objects, the processes, and the messages that
are all interacting when an assessment is actually in play. Any real assessment
must have elements that correspond to the four processes in some way. Thus,
exploring how assessment ideas play out in the four process framework pro-
vides an understanding about how they will play out in speciﬁc assessment
implementations.

2.4 Basic ECD Structures
27
2.4.1 The Conceptual Assessment Framework
The blueprint for an assessment is called the CAF. To make it easier to rear-
range the pieces of the framework (and deal with them one at a time when
appropriate), the framework is broken up into pieces called models. Each
model provides speciﬁcations that answer such critical questions as “What
are we measuring?” or “How do we measure it?”
Delivery Model
Assembly Model
Proficiency Model(s)
Evidence Models
Stat
model
Evidence
Rules
Task Models
Features
1.
xxxxx
2.
xxxxx
3.
xxxxx
Presentation Model
Fig. 2.1 The principle design objects of the conceptual assessment framework
(CAF). These models are a bridge between the assessment argument and the oper-
ational activities of an assessment system. Looking at the assessment argument,
they provide a formal framework for specifying the knowledge and skills to be mea-
sured, the conditions under which observations will be made, and the nature of
the evidence that will be gathered to support the intended inference. Looking at
the operational assessment, they describe the requirements for the processes in the
assessment delivery system.
Reprinted from Mislevy et al. (2004) with permission from the Taylor & Francis
Group.
What Are We Measuring? The Proﬁciency Model
A proﬁciency model deﬁnes one or more variables related to the knowl-
edge, skills, and abilities we wish to measure. A simple proﬁciency model
characterizes a student in terms of the proportion of a domain of tasks the
student is likely to answer correctly. A more complicated model might char-
acterize a student in terms of degree or nature of knowledge of several kinds,

28
2 An Introduction to Evidence-Centered Design
each of which may be required in diﬀerent combinations in diﬀerent tasks. It
may address aspects of knowledge such as strategy use or propensity to solve
problems with certain characteristics in certain situations. Looking ahead, the
proﬁciency model variables will be the subset of the variables in a Bayesian
net that accumulate evidence across tasks.
A closer look at the proﬁciency model in Fig. 2.1 reveals two kinds of
elements. On the right is a graphical structure, a representation of the kinds
of statistical models that are the focus of this book. On the left are a number
of stars that represent claims. Claims are what users of assessments want to be
able to say about examinees, and are the basis of score reports. A reporting
rule maps information from probability distributions for proﬁciency model
variables to summary statements about the evidence a student’s performance
provides it to support a claim.
Example 2.2 (Calculus Proﬁciency Model; Example 2.1 Continued).
Given that the primary purpose of the assessment is placement, only one vari-
able is necessary in the proﬁciency model. This is a discrete variable whose
levels correspond to the various placement options: Remedial Class, 1st
Semester Freshman, 2nd Semester Freshman, 1st Semester Sophomore,
2nd Semester Sophomore, Junior Math Classes. Fig. 2.2 shows the graph-
ical representation of this model. If there were a secondary purpose of trying
to diagnose problems in low performing students, there might be a need for
additional proﬁciency variables that would accumulate evidence about more
speciﬁc skills. However, in a short test, the designers typically need to choose
between good reliability for the main variables and good diﬀerential diagno-
sis for problems in the assessment. University C could use two tests: This
placement test ﬁrst, followed by a diagnostic test just for students placed into
the remedial class, addressing only claims concerning precalculus skills and
accumulating evidence at a grainsize that matches the instructional modules.
Proficiency Level
Fig. 2.2 The proﬁciency model for a single variable, Proﬁciency Level. The rounded
rectangle with the circle symbol represents the proﬁciency variable, and the square
box with the table represents its probability distribution
Reprinted with permission from ETS.
Associated with each level of the proﬁciency variable are one or more claims.
Which claim is associated with which level depends on how the various
skills are taught in the calculus series. For example, the level 2nd Semester
Freshman would be associated with all of the claims that constitute the kinds
of performances in the kinds of tasks we would want a student successfully
completing that course to be able to do. If multivariate calculus is not taught

2.4 Basic ECD Structures
29
until the sophomore year, then all of the claims associated with multivariate
calculus would be associated with the levels corresponding to the sophomore
year.
Completing the proﬁciency model requires the speciﬁcation of a prior dis-
tribution for the proﬁciency variable. This distribution represents our state
of knowledge about a student who sits down to take the assessment before
we learn their responses to any of the problems they were given. In this case,
a uniform distribution does not seem appropriate as only a very few incom-
ing freshmen will be ready for junior level coursework. However, university
records for the past 5 years might provide a reasonable prior distribution.
This distribution is represented by the square box in Fig. 2.2.
The key idea of the proﬁciency model is that it represents our state of
knowledge about an examinee’s state of knowledge about calculus. Chapter 13,
which talks about constructing a scoring engine based on Bayesian networks,
talks about making a copy of the proﬁciency model that tracks our state
of knowledge as we gather more evidence about the examinee’s proﬁciency.
This copy is called the scoring model. When the examinee ﬁrst sits down for
the assessment, it is identical to the proﬁciency model. However, as we see
the answer from each task the examinee attempts, the scoring model will be
updated to reﬂect our state of knowledge about this particular examinee. The
evidence models determine how that updating is done.
In succeeding chapters, we will look at proﬁciency models with several
variables, each representing some aspect of knowledge, skill, or ability posited
to inﬂuence students’ performance. In each case, the idea is the same as in the
simple placement test case: These variables are how we characterize students’
knowledge; we do not get to observe their values directly; we express what we
do know about them in terms of a probability distribution; and evidence in the
form of behavior in assessment situations allow us to update our knowledge,
by updating the probability distributions accordingly.
How Do We Measure it? The Evidence Model
Evidence models provide detailed instructions on how we should update
our information about the proﬁciency model variables given a performance in
the form of examinees’ work products from tasks. An evidence model contains
two parts, which play distinct roles in the assessment argument. They are
the evidence rules for identifying information in students’ performances and
statistical machinery for accumulating information across tasks.
•
Evidence rules describe how observable variables characterize an exami-
nee’s performance in a particular task, from the work product(s) that the
examinee produced for that task. A work product is the capture of some
aspect(s) of a student’s performance. It could be as simple as a binary digit
conveying a response to a true–false item, or as complex as a sequence

30
2 An Introduction to Evidence-Centered Design
of diagnostic test orders and medical treatments in an extended patient-
management problem in a medical simulation. The observables are the
primary outcomes from task performances, and they provide both infor-
mation that will be used to update our beliefs about proﬁciency model
variables and information that will be used for task-level feedback. In
an operational assessment, evidence rules guide the evidence identiﬁca-
tion process. Evidence rules concern the identiﬁcation and summary of
evidence within tasks, in terms of observable variables.1 Summary of evi-
dence across tasks will be the role of the statistical part of the evidence
model.
Example 2.3 (Calculus Evidence Rules; Example 2.1 Continued). A
prerequisite to specifying the evidence rules for the calculus placement test,
is specifying the form of the work product. If the test is presented as multiple
choice, then the work product would be the selection made by the examinee.
The evidence rule would match the selection against the key to determine
whether the outcome was correct or not. The observable variable would be
a binary variable indicating whether the answer was correct or not. If the
test is presented as a free response but the observable outcome variable was
correct or incorrect, then the evidence rule would be to compare the student
answer to the correct answer and code it correct if they are mathematically
equivalent. If the observable outcome variable has more than two values to
allow for partial credit, then the evidence rules would be the scorer’s rubric
used to determine partial credit. As typically there is an evidence model for
each task model in an assessment, an assessment could have a mixture of
diﬀerent types of tasks with diﬀerent evidence rules.
Evidence rules are indiﬀerent as to whether the scoring is done by comput-
ers or humans. What is necessary is that they provide a clear set of instructions
on how to determine the value of the observables. The key-matching rule of a
multiple-choice test can be done by hand but lends itself readily to computer-
ization. A value of 0 or 1 for an observable variable based on a multiple-choice
item no longer depends on how that value was calculated. Short answer ques-
tions are more diﬃcult for computers, as they need to be able to parse and
recognize equivalent mathematical expressions. Partial credit scoring can be
quite diﬃcult even for human raters. The problem of achieving agreement
among multiple human raters is well studied, and we know that clearly writ-
ten evidence rules and worked-through examples are a big help. Sophisticated
automatic methods such as neural networks and multistage rules can be used
to evaluate observable variables from rich performances in, for example, prob-
lem solving in computer simulations Williamson et al. (2006b)
1 Note the distinction between the conceptual notion of evidence about proﬁciency
and the “stuﬀ” one sees in an operating assessment: The work product as a
capture of something the student has done in the task situation and the observable
variables as evaluated features of it. They do not constitute “evidence” absent
the assessment argument, and their embodiment of elements of it.

2.4 Basic ECD Structures
31
•
The statistical part of the evidence model provides information about the
connection between proﬁciency model variables and observable variables.
Psychometric models are often used for this purpose, including the familiar
classical test theory and item response theory, and the less familiar latent
trait models, cognitive diagnosis models, and Bayes nets. In an operational
assessment, the statistical part of the evidence model, together with the
proﬁciency model, structure the evidence accumulation process. The sta-
tistical part of the evidence model concerns the accumulation and synthesis
of evidence across tasks, in terms of proﬁciency variables.
The proﬁciency model together with the statistical part of the evidence
model constitute the measurement model of the assessment. The theory of
ECD is broad enough that the measurement model does not need to be a prob-
ability model. For example, if the measurement model was a sum of scores or
“number right” model, then the statistical part of the evidence model would
simply state how many points to give for each answer. However, if both the
proﬁciency model and evidence models are expressed in terms of probabil-
ity distributions, then we can use Bayes theorem as the update mechanism.
Chapter 3 (and most of the rest of the book) explains this in detail. In this
case, the statistical part of the evidence model is speciﬁed as a conditional
probability distribution providing the probability of the observable outcome
variables given the examinee’s state of proﬁciency. The familiar measurement
models from IRT, the logistic function and the normal ogive function, take this
form; that is, conditional probability distributions for a correct item response
given the proﬁciency variable θ.
Example 2.4 (Calculus Evidence models; Example 2.1 Continued).
Assume that according to the evidence rules, the observable outcome for a task
was a single variable isCorrect taking on values true and false. It is necessary
to specify for each of the possible levels of proﬁciency the probability that an
examinee at that level will get the item correct. This is shown as the square
box in Fig. 2.3. Note that this ﬁgure shows two kinds of variables: evidence
model variables (observables) labeled with a triangle, and proﬁciency variables
(borrowed from the proﬁciency model) labeled with a circle.
Proficiency Level
Item j Outcome
Fig. 2.3 The measurement model for a dichotomously-scored item. Variables labeled
with a triangle are local to the evidence model, while variables labeled with a circle
are borrowed from the proﬁciency model (and hence shared across evidence models
for diﬀerent tasks that provide evidence about them). The square box represents the
probability distribution, which must be speciﬁed to make the model complete
Reprinted with permission from ETS.

32
2 An Introduction to Evidence-Centered Design
As there are six possible levels for the proﬁciency variable, six diﬀerent proba-
bility values must be speciﬁed for each evidence model. This is a lot. Chapter 8
discusses some ways of reducing this work. Another possibility is to learn the
probabilities from data. This is called calibration; Part II discusses this in
detail.
Where Do We Measure it? The Task Model
Task models describe how to structure the kinds of situations we need
to evoke the evidence we need for the evidence models. They describe the
presentation material that is presented to the examinee and the work prod-
ucts, which the examinee generates in response. They also contain task model
variables that describe features of tasks as well as how those features are
related to the presentation material and work products. Those features can
be used by task authors to help structure their work, by psychometricians to
help reduce the number of pretest subjects needed, and by test assemblers
to help ensure that a particular form of the assessment is balanced across
particular kinds of tasks. Mislevy, Steinberg, and Almond (2002c) explore the
myriad uses of task model variables.
A task model does not represent a single task, but rather a family of
potential tasks waiting to be written. Tasks are made from task models by
ﬁlling in the speciﬁcation made by the task model, i.e., ﬁnding or authoring
presentation material and setting the values of the task model variables to
the corresponding values. A typical assessment may have several task models
representing diﬀerent families of tasks.
Example 2.5 (Calculus Task Model; Example 2.1 Continued). Con-
sider the task model for a unidimensional integration task. The presentation
material for this type of task consists of the integrand, the limits of the inte-
gral, the instructions given to the examinee (could be shared by several tasks)
and, if the format is multiple-choice, the values of the options. Task model vari-
ables are related to this choice of material. For example, task model variables
might indicate the number of factors, whether or not trigonometric functions,
logarithms or exponential expressions are used, and what integration tech-
niques must be applied to solve the integral. Note that task model variables
could be set before or after the presentation material is authored: e.g., the
author could note that a particular task involves two factors with trigonomet-
ric functions, or be requested to write a task that requires using integration
by parts.
The task model also must contain the expected form of the work product. If
the format is multiple choice, the work product will be some form of capture of
the selection that was made, such as a 0–9 digit in a data ﬁle. If the response
is open-ended, the work product from a paper-and-pencil test might be the
student’s written production on the physical answer sheet (to be scored by a
human); from a computer-based test it might be the text in a rich text ﬁle
(.rtf) produced by the student’s interaction with the task.

2.4 Basic ECD Structures
33
How Much Do We Need to Measure? The Assembly Model
Assembly models describe how the proﬁciency models, evidence models,
and task models must work together to form the psychometric backbone of
the assessment. The assembly model speciﬁes what constitutes a valid form
of the assessment. This is especially important if not all examinees get the
same form (e.g., there are multiple forms for security reasons, or the test is
adaptive). The rules for constructing a form are speciﬁed through targets and
constraints. Targets describe how accurately each proﬁciency model variable
must be measured (see Chap. 7), and constraints describe how tasks must be
balanced to properly reﬂect the breadth and diversity of the domain being
assessed.
Example 2.6 (Calculus Assembly Model; Example 2.1 Continued).
In constructing the assembly model for the calculus placement test, it is impor-
tant that the range of tasks reﬂect the syllabus for the calculus sequence. This
is generally achieved through constraints. It is important that the test give
good information about whether or not the student is in the lower placement
categories for all students. Only for a few students will we be interested in the
sophomore and junior levels. This is easier to handle in an adaptive test. If the
delivery mode is computer-based, then we could use the techniques described
in Chap. 7 to make an adaptive test. If the delivery mode is paper-and-pencil,
we could use a brief self-scored routing test or put the advanced items into
a separate section at the end and instruct the students to work on this part
only if they feel conﬁdent of their performance in the earlier sections.
How Does It Look? The Presentation Model
Assessments today can be delivered through many diﬀerent means; for
example, paper and pencil, standalone computer or through the web, on a
handheld device, read aloud over the phone, or as portfolios assembled by
the students. A presentation model describes how the tasks appear in various
settings, providing a style sheet for organizing the material to be presented
and captured.
A common use of this idea is to support presentation of the same assess-
ment in both paper and pencil and computer format. A more recent but
increasingly important use of the presentation model is alternative presenta-
tion modes to accommodate examinees with disabilities (Shaftel et al. 2005;
Russell 2011). This latter usage requires a careful examination of exactly
what is being claim and what constitutes evidence, as diﬀerent students
may be able provide to evidence about the same capabilities despite dif-
ferent ways of accessing information, interacting with tasks, or producing
performances—in measurement terms, exactly what constitutes construct-
relevant and construct-irrelevant sources of variance (Hansen et al. 2003; Mis-
levy et al. 2013).

34
2 An Introduction to Evidence-Centered Design
Putting It All Together: The Delivery System Model
The delivery system model describes the collection of proﬁciency, evidence,
task, assembly, and presentation models necessary for the assessment and how
they will work together. It also describes issues that cut across all of the other
models, such as platform, security, and timing.
Breaking the assessment speciﬁcation up into many smaller pieces enables
us to reassemble pieces in diﬀerent conﬁgurations for diﬀerent purposes. For
example, a diagnostic assessment requires a ﬁner grain size proﬁciency model
than a selection/placement assessment. If we want to use the same tasks in
both the diagnostic and selection assessment, we can use the same task models
(written generally enough to address both purposes). However, we will want
diﬀerent evidence models, each one appropriate to the level of detail consistent
with the purpose of the assessment.
2.4.2 Four-Process Architecture for Assessment Delivery
As we have noted, assessments are delivered in a variety of platforms, from the
more familiar paper-and-pencil tests, oral exams, and more recent computer-
based tests, to the newer ways of delivering tests through the Web, over the
phone, and with handheld devices such as minitablet computers and smart-
phones.
To assist in planning for all these diverse ways of delivering a test, ECD
provides a generic framework for test delivery: the four -process delivery archi-
tecture (Almond et al. 2002a; Almond et al. 2002b). The four-process deliv-
ery architecture shown in Fig. 2.4 is an ideal system; any realized assessment
system must contain these four processes in some form or other. They are
essential to making the observations and drawing the inferences that com-
prise an assessment argument. This is true whether some of the processes
are collapsed or degenerate in a given system, and regardless of whether they
are carried out by humans, computers, or human–computer interactions. The
IMS Consortium adopted this idealization as a reference model for use with
their standards on question and test interoperability (IMS 2000), although
they used diﬀerent names for diﬀerent pieces.
How Is the Interaction with the Examinee Handled? The Pre-
sentation Process
The presentation process is responsible for presenting the task and all
supporting presentation material, managing interaction with the student, and
gathering the work products. Examples include a display engine for computer-
based testing, a simulator which can capture an activity trace, a protocol for
a structured interview and the human administering it, and a system for dis-
tributing test booklets and capturing and scanning the answer sheets. In a
paper-and-pencil assessment, the presentation process concerns administering
preassembled test booklets to examinees and collecting and possibly scanning

2.4 Basic ECD Structures
35
Summary Feedback
Task/
Evidence
Composite
Library
Task Level Feedback
Evidence
Identification
Evidence
Accumulation
Activity
Selection
Presentation
Administrator
Participant
Fig. 2.4 The four principle processes in the assessment cycle. The activity selec-
tion process selects a task (“tasks” could include items, sets of items, simulation
problems, or learning activities, as examples) and directs the presentation process
to display it. When the participant has ﬁnished interacting with the task, the pre-
sentation process sends the results (one or more work products) to the evidence
identiﬁcation process. This process identiﬁes essential observations about the results
and passes them to the evidence accumulation process, which updates the scoring
record, tracking our beliefs about the participant’s knowledge. All four processes
add information to the Results Database. The activity selection process then makes
a decision about what to do next, based on the current beliefs about the participant
or other criteria
Reprinted from Mislevy et al. (2004) with permission from the Taylor & Francis
Group.
the answer sheets. In a computerized adaptive assessment, presentation con-
cerns presenting a sequence of tasks to an examinee one at a time (as directed
by the activity selection process), in each instance capturing a response. The
next processes will evaluate it on the spot and use the information to guide
the selection of the next task.
How Is Evidence Extracted from a Task Performance? The Evi-
dence Identiﬁcation Process
The evidence identiﬁcation process (called response processing in the IMS
speciﬁcation) is responsible for identifying the key features of the work prod-
uct that are the observable outcomes for one particular task. The observable
outcomes can go back to the participant for task-level feedback, be passed on
to the evidence accumulation process, or both. Examples include matching a
selected response to an answer key, running an essay through an engine, and
having a human rater score a student portfolio according to a rubric. The

36
2 An Introduction to Evidence-Centered Design
evidence rules from the CAF specify how this is to be accomplished. Evidence
identiﬁcation can consist of multiple stages, as when lexical and syntactic fea-
tures are identiﬁed in an essay and a regression model is used to summarize
them into a single score for a response to this task.
A question sometimes arises as to whether a particular operation is part
of the presentation process or the evidence identiﬁcation process. Often the
answer lies with how the system could possibly be reused or reconﬁgured, and
steps in a continuous process are parsed out in CAF models accordingly.
Consider a multiple-choice item presented on a computer for which the evi-
dence model calls for a binary observable isCorrect. The presentation process
must present the stem, key, and distractors to the examinee, and provide some
mechanism for making a selection. The examinees make some kind of gesture
(clicking the mouse, pressing a key) to indicate their selections. The assess-
ment delivery system must translate that gesture ﬁrst into a code indicating
which option was selected, then match that against the key.
In this setup, the ideal division of labor is achieved when the work product
consists of a speciﬁed representation of the selection made by the examinee.
Using the raw mouse click as the work product is too detailed. It requires the
evidence identiﬁcation process to know details of the formatting of the item
on the screen in order to interpret the raw data. We would like the freedom to
use an alternative presentation process that uses key presses to indicate the
selections without having to also change the rules of evidence. On the other
hand, having the presentation process match the selection to the key goes
too far in interpreting the raw response. We want the freedom to substitute
an alternative evidence identiﬁcation process that uses which distractor was
selected to help diagnose misconceptions without needing to also change the
presentation process.
How Is Evidence Accumulated Across Tasks? The Evidence
Accumulation Process
The evidence accumulation process (called summary scoring in the IMS
speciﬁcation) process is responsible for synthesizing the information from
observable outcomes across multiple tasks to produce section and assessment
level scores. Examples include the IRT engine used in GRE computerized
adaptive testing (CAT) testing, the Bayesian network evidence accumulation
process at the heart of this book, and simply counting up the number of right
answers. The measurement model in the CAF associated with a particular
task speciﬁes how this is to be accomplished.
What Happens Next? The Activity Selection Process
The activity selection process is responsible for deciding what the next
task should be and when to stop the assessment. When making these deci-
sions, adaptive assessments consult the current state of what is known about a
student, in terms of the values of the proﬁciency-model variables as they have

2.4 Basic ECD Structures
37
been updated thus far by the evidence accumulation process (Chap. 7 talks
about some possible measures). An instructional system will also make deci-
sions about switching between assessment and instruction (Shute 2003April).
Examples of activity selection processes include simple linear sequencing (for
example, paper-and-pencil tests, although the student may chose the order in
which to answer items within each section as it is administered), and comput-
erized adaptive item selection (e.g., the GRE CAT), and student choice as to
when to move on in a self-paced practice system.
Where Do Processes Get the Information They Need? The
Task/Evidence Composite Library
All four processes require certain kinds of data in order to do their jobs:
The presentation process requires the text, pictures, and other material to be
displayed. The evidence identiﬁcation process requires the “key,” the param-
eters for algorithms, or other evidence rule data with which to evaluate the
work products. The evidence accumulation process requires the parameters
that provide the “weights of evidence” for each task, such as scoring weights,
item response theory parameters, or the conditional probabilities in Bayes nets
discussed in this book. The activity selection process requires classiﬁcation
and information codes to balance the assessment form. The Task/Evidence
Composite Library is a uniﬁed database that stores this information.
We have suggested, without detailing, the mapping between the Design
models in the conceptual assessment framework and the four processes. All of
the design decisions made in the blueprint are reﬂected either directly in the
implementation or in one of the processes leading up to the implementation.
Again, further discussion and examples are available in Almond et al. (2002a);
Almond et al. (2002b).
Example 2.7 (Calculus Test Delivery System; Example 2.1 Contin-
ued). Consider ﬁrst a paper-and-pencil version of the calculus placement
test. The activity selection process consists of rules for assembling and dis-
tributing the paper-and-pencil forms. One possible mechanism is to group the
tasks into sections of increasing diﬃculty and instruct the examinee to not
attempt the next section unless they are fairly conﬁdent of their answers to
the previous section. The presentation process consists of the mechanism for
distributing the forms and collecting and scanning the answer sheets. In this
case, the work product for each task is a free response that has been scanned
and stored as bitmap image. The evidence identiﬁcation process consists of
scoring system in which the work products are distributed to raters who mark
them as correct or incorrect, and record the scored outcomes. The evidence
accumulation process is a Bayesian network that incorporates the information
from the observable variables (i.e., “absorbs the evidence”) and calculates the
probability that the examinee is in each of the six possible groups.
Now, consider an alternative computer delivered version of the assessment
using multiple-choice versions of the tasks. In this case, the activity selection

38
2 An Introduction to Evidence-Centered Design
process is a version of the critiquing algorithm of Chap. 7 that tries to estab-
lish whether the examinee is at or above each of the possible levels in sequence
(ready for 1st semester, 2nd semester, and so on). The presentation pro-
cess displays the task and records the selection made by the examinee. The
evidence identiﬁcation process matches the selection to the key and sets the
value of the observable outcome variable to correct or incorrect as appropriate.
The evidence accumulation process is again Bayesian network engine which
absorbs the evidence from the evidence identiﬁcation process and calculates
the probability that the examinee is in each group based on the evidence so
far. Note that the activity selection process can query the Bayes net when
making the decision about which task to select or whether to stop or move on
to target the next level of proﬁciency.
2.4.3 Pretesting and Calibration
In order to score an assessment, the evidence identiﬁcation process or the
evidence accumulation process (or both) may need to build in empirical infor-
mation from previous administrations of the tasks. In the case of evidence
identiﬁcation, this information is incorporated into evidence rules. For exam-
ple, an automated essay-scoring system can be “trained” to match human
ratings given values of lower-level lexical and syntactic features of a partic-
ular essay (Deane 2006). (Calibration of evidence-identiﬁcation processes is
not discussed in this book, but see Williamson et al. (2006b).) In the case of
evidence accumulation, it appears in scoring weights or task-speciﬁc evidence
model parameters. We refer to a start-up set of data from which to estimate
these values as pretest data, and the operation of determining the values as
calibration. An evidence model tuned to work with a speciﬁc task is called a
link model (Chap. 13).
If the measurement model for the assessment is a probability model, we can
use Bayes theorem to “learn” the task-speciﬁc parameters in the link model.
The original evidence model provides a prior distribution for the parameters,
based on the experts’ understanding of the domain and our previous experi-
ence with similar task models. We can then use Bayes theorem to reﬁne those
priors with pretest data. Part II describes two methods for doing this with
Bayesian network models.
Using probability-based models has another advantage as well. The model
makes a prediction for what the pretest data will look like before they are
gathered. If the pretest data look “surprising” compared to the model, then
this suggests we might want to reﬁne our model. This principle extends to
ways to compare models and to search for a best model. Chapter 10 looks
at some techniques for model criticism and model search. This is particu-
larly important when the measurement model has been built to reﬂect our
understanding of the underlying cognitive processes. In this case, critiquing
the model will help us reﬁne our knowledge of the cognitive processes we are
trying to measure.

2.5 Conclusion
39
2.5 Conclusion
Developments in statistical methodologies and new kinds of psychometric
measurement models hold the promise of supporting a wider variety of edu-
cational assessments than have been traditionally used. To capitalize on their
potential, however, one cannot think of using them in isolation from the other
components of assessment design. All must work in concert to create an assess-
ment that is at once coherent and practicable.
Toward this end, it will be of signiﬁcant beneﬁt to have a shared framework
for talking about the roles that each facet of the design elements and delivery
processes play in the support of a coherent assessment argument. Evidence-
centered design provides such a framework, and can thus prove useful for
understanding how graphical modeling techniques ﬁt into assessment systems.
Exercises
2.1. The basic models of the ECD “conceptual assessment framework” are the
proﬁciency model, the evidence model, and the task model. In which of these
models are variables that concern characteristics of the situations in which
students say, do, or make things? In which are variables that concern charac-
teristics of the students? In which are variables that concern characteristics
of the particular things students say, do, or make?
2.2. What are the two submodels of the evidence model? How do their roles
diﬀer from one another?
2.3. How are the proﬁciency model and the evidence model related to each
other, in terms of both shared or overlapping information and connections in
the assessment argument?
2.4. How are the evidence model and the task model related to each other,
in terms of both shared or overlapping information and connections in the
assessment argument?
2.5. How are the proﬁciency model and the task model related to each other,
in terms of both shared or overlapping information and connections in the
assessment argument?
2.6. How are the assembly model and the task model related to each other,
in terms of both shared or overlapping information and connections in the
assessment argument?
2.7. How are the assembly model and the proﬁciency model related to each
other, in terms of both shared or overlapping information and connections in
the assessment argument?

40
2 An Introduction to Evidence-Centered Design
2.8. An important part of the process of designing any assessment is selecting
items (or tasks) for a form. Which ECD model speciﬁes what constitutes a
valid form?
2.9. A common task teachers use to assess a student is the book report, where
the student reads a book and then writes (or presents) a report based on the
contents. Consider the Book Report as a task model. What are the presenta-
tion material and work products? List some possible task model variables.
2.10. The Book Report is used across a large number of grades. How do
the values of the task model variables change when the Book Report task is
used in a 6th grade classroom as opposed to a 4th grade classroom? Which
ECD model(s) must be changed to ensure that a Book Report task is grade
appropriate?
2.11. If the form of the work product in the Book Report task model is
changed from a written report to an oral presentation, how much do the
other models change? (Changing the expected work product form variable
changes the evidentiary focus of the task. Often when changing a task model
variable changes the evidentiary focus of a task, it is helpful to split the task
model into two task models, in this case Oral Book Report and Written Book
Report task models.)
2.12. Decision analysts often use what is called the clarity test when deﬁning
variables. A variable passes the clarity test if a person who had access to all
of the available data (referred to as a “clairvoyant” in this literature) could
unambiguously assign a value to the variable. A variable that does not meet
the clarity test must be reﬁned in order to begin statistical modeling. For
example, “the SAT score of a candidate” does not meet the clarity test, but
“the most recent SAT score of a candidate” and “the highest SAT score of a
candidate” are both reﬁnements that do meet the clarity test.
For each of the following variables, state whether or not they meet the
clarity test. If the do not, suggest how the deﬁnition might be revised to meet
the clarity test.
a. The Gender of a participant.
b. The Race of a participant.
c. The Socioeconomic Status of a participant.
d. Whether or not a participant receives a free or reduced price lunch.
e. Whether a participant lives in a high crime area.
f. Whether a participant lives in an urban, suburban or rural location.
2.13. What elements of ECD are used to ensure that proﬁciency variables
pass the clarity test? For observable variables?

3
Bayesian Probability and Statistics: a Review
Our ultimate goal is to draw inferences about the state of a student’s proﬁ-
ciency, which is unknown, from observations, which are known. We will press
probability theory into the service of modeling our uncertain knowledge about
the student’s proﬁciencies. Probability theory is one of the most studied mod-
els for uncertainty and the choice of probability theory brings with it all of
the tools of Bayesian statistics.
Although we assume the reader has had at least one course in probability
and statistics at the college level, typically such courses only dwell brieﬂy on
the Bayesian formulation of statistics (and that unit is often omitted for time
reasons). This chapter, therefore, reviews some of the key diﬀerences between
Bayesian and Classical statistics. Section 3.1 discusses the basic deﬁnition
of probability and its use in representing states of information. Section 3.2
reviews conditional probability and Bayes’ theorem, tools we will use again
and again. Section 3.3 looks at the concepts of independence and conditional
independence, which will form the basic building blocks of our models; Chap. 4
on graphical representation will build heavily on this section. Section 3.4 pro-
vides a quick review of random variables and Sect. 3.5 looks at how Bayes’
theorem can become a paradigm for learning about unknown quantities.
A short chapter such as this one cannot cover all of the probability theory
and statistics necessary to understand in detail all of the models explored
in this book. We hope that this chapter will provide enough background in
Bayesian ideas of probability so that an educational researcher can at least
follow the arguments at a high level. Readers wishing to follow the more
mathematical parts in greater detail will need to be familiar with Bayesian
statistics at the level of Gelman et al. (2013a).
3.1 Probability: Objective and Subjective
Although all statisticians agree that a probability is a number between 0 and 1,
there are two main schools for interpreting that probability. Perhaps the best
c⃝Springer Science+Business Media New York 2015
41
R. G. Almond et al., Bayesian Networks in Educational Assessment,
Statistics for Social and Behavioral Sciences, DOI 10.1007/978-1-4939-2125-6 3

42
3 Bayesian Probability and Statistics: a Review
known uses the Relative Frequency deﬁnition (Sect. 3.1.1), based on objec-
tive properties of repeatable experiments. However, Bayesian statistics owes a
large debt to the Subjective school of probability (Sect. 3.1.2). For educational
testing, it is necessary to synthesize the two views into an Objective–Subjective
approach (Sect. 3.1.3).
3.1.1 Objective Notions of Probability
A random experiment is one whose outcome for a single trial is unknown
before the experiment begins, but for which we can make reliable predictions
about the collection of outcomes that comes from repeating the experiment
many times. To make this more deﬁnite, consider the following experiment.
Example 3.1 (Balls in an Urn). Consider an urn1 that contains b black
balls and w white balls all of the same size and weight, thoroughly mixed
together (Fig. 3.1). Somebody reaches into the urn and draws out a ball
without looking. If the experiment is repeated many times, replacing and
mixing the balls each time, the proportion of black balls drawn will be
b
b+w.
We say the probability that a drawn ball is black is θ =
b
b+w.
?
Fig. 3.1 Canonical experiment: balls in an urn
Reprinted with permission from ETS.
1 The use of an urn instead of another type of container that might be easier to
draw from is a hallowed tradition in statistics.

3.1 Probability: Objective and Subjective
43
Example 3.1 is a canonical example of a Bernoulli experiment. While we
cannot say much about the outcome of a single experiment, if we have a series
of independent Bernoulli trials, we can say quite a lot about the behavior of
the series. Suppose we have n independent draws from our urn, and let Y be
the number of black balls we obtain in those draws. Combinatorial arguments
(see Ross 1988) show that there are
n
y

=
n!
y! (n−y)! distinct sequences of n
balls with y blacks and n −y whites. Therefore, we have,
p(Y = y|θ, n) =
n
y

θy(1 −θ)n−y
for y = 0, . . . , n
0
otherwise.
(3.1)
Equation 3.1 is the well-known binomial distribution with parameters θ and n.
The mean of the binomial—the expected number of events in n trials—is nθ
and the variance is nθ(1 −θ). As test items are often scored dichotomously—
1 for right, 0 for wrong—the binomial distribution (and its close cousin the
multinomial distribution) will play a large role in the sequel.
Y/n is the proportion of black balls in a given sample of size n. The mean
of Y/n in repeated experiments is θ, and the variance is θ(1−θ)
n
. Note that
as n gets larger and larger, the variance of Y/n gets smaller and smaller. In
other words, the distribution of the proportion of black balls obtained in very
large samples is clustered closely around the true urn proportion θ. Thus, as n
goes to inﬁnity Y/n goes to θ. This is known as the Law of Large Numbers. In
the frequency school of probability (the one most often taught in elementary
statistics courses) this limit is taken as the deﬁnition of probability.
Deﬁnition. Probability (Frequency Deﬁnition). Let Ω be the set of out-
comes from an experiment that can be repeated many times (such as black ball
and white ball). Let A ⊂Ω; that is, A is a subset of the outcomes (such as a
black ball). Then the probability of A, P(A), is the limiting proportion of the
times for which the outcome lies in A in an arbitrarily long sequence of trials.
Some important properties of probability come out of this deﬁnition. First,
probability is a measure in the same sense that we can measure the length of a
line segment in geometry: the probability of two disjoint (nonoverlapping) sets
is the sum of the probabilities. Second, the smallest possible probability is 0
and the largest is 1. It would be nice to say that probability 1 means deﬁnitely
certain and probability 0 means deﬁnitely will not occur. This is indeed the
case when Ω consists of a ﬁnite set of possible outcomes. However, when the set
of possible outcomes is continuous, such as all the points on the real line, there
are certain pathological cases (such as when A consists of a single point) that
can have probability 0. Therefore, probability 1 means “practically certain”
and probability 0 means “does not occur except in pathological cases.”
3.1.2 Subjective Notions of Probability
The deﬁnition of probability given above presents two problems when applied
to educational measurement. First, many of the experiments we wish to

44
3 Bayesian Probability and Statistics: a Review
describe are not repeatable. If we present the same item to a student twice
we expect some learning will have taken place, which renders the two tri-
als no longer independent. Second, many of the quantities we wish to make
probabilistic statements about are not observable at all. For example, we can-
not directly observe a student’s calculus proﬁciency; we can only indirectly
observe the outcomes when she is asked to solve calculus problems.
Bayesian statistics often talks about probability distributions over unknown
parameters. It is customary in Bayesian statistics to take as the fundamental
deﬁnition of probability not the relative frequency but a degree of belief in
unknown events.
Deﬁnition. Probability (Subjective Deﬁnition). Let Ω be the set of out-
comes from an experiment which may or may not be possible to carry out. Let
A ⊂Ω. Then Your probability of A, P(A), is Your belief that the outcome
will be in A; numerically it is equal to Your belief that a black ball will be
drawn from an urn with a proportion P(A) of black balls.
This phrasing deﬁnes probability in terms of analogies to simple, repeat-
able experiments. Furthermore, it emphasizes probability as a degree of belief
of some person. People with diﬀerent information or diﬀerent models for the
underlying situation can therefore have diﬀerent probabilities for the same
event.
The deﬁnition of subjective probability can be derived from a set of axioms
describing properties of rational belief of an ideal decision maker2(Savage
1972; de Finetti 1990). These axioms provide the standard properties of prob-
ability as a measure between 0 and 1. They also can be used to show that in the
case of a repeatable experiment, a reasonable person’s subjective probability
will converge to the relative frequency.
The use of this subjective deﬁnition of probability has been one of the
reasons for the slow adoption of Bayesian statistical ideas (the second has been
computational diﬃculties). It has not helped that the axioms are often stated
in terms of “fair bets”; people’s attitudes both in favor and against games of
chance has hindered the appeal of the derivation. However, the ability to make
statements of probability about experiments that are only theoretical opens
up a great number of applications. In particular, using Bayesian statistics we
can make direct statements of probability about unknown parameters rather
than indirect statements about the property of estimators (take a look at the
deﬁnition of a conﬁdence interval in most any statistics textbook).
When using the subjective deﬁnition of probability, assessing probabil-
ity distributions that model real-world phenomena can be a challenge. The
number of techniques for this particular task is large, and many of them
are somewhat controversial. Morgan and Henrion (1990), Walley (1991), and
Berger (1985) provide reviews. One problem in this ﬁeld is that the lay percep-
tion of probability is not consistent and is subject to heuristic biases, such as
2 In this literature the ideal decision maker is often called “You.”

3.1 Probability: Objective and Subjective
45
ignoring base rates and mistaking representativeness for likeliness (Kahneman
et al. 1982). The fact that many subject matter experts are not also statis-
tical experts presents a major challenge when basing probabilities on expert
opinion.
One method that is universally agreed upon is the principle of an equal
probability space.
Deﬁnition. Principle of Equal Probability Space. Let Θ be a sample
space with N elements that are judged to be equally likely. Let A be an event
in Θ. Then P(A) = #(A)
N , where #(A) is the number of elements in A.
Example 3.2 (Playing Cards). Consider an ordinary deck of 52 playing
cards with ace, two, three, . . ., ten, jack, queen, and king in each of the
four suits spades, hearts, diamonds, and clubs. The probability of drawing
a spade on a single draw is 13/52 = 1/4. The probability of drawing an ace is
4/52 = 1/13. The probability of drawing the ace of spades is 1/52.
This principle works well for simple games of chance like cards and dice.
However, in many experiments the elements of the underlying space are not
equally likely. Consider the experiment of giving a calculus exam to early
elementary students. It is quite unreasonable to assume that all scores are
equally likely! This same problem shows up in subtler ways when trying to
form a noninformative distribution for a binomial proportion (Sect. 3.5.5).
Notwithstanding these diﬃculties, this principle often provides a reasonable
starting place.
3.1.3 Subjective–Objective Probability
In educational testing, we want to be able to make objective statements about
learners. On the other hand, the subjectivist approach oﬀers possibilities for
making interesting statements about unobservable proﬁciencies. Clearly we
need some kind of synthesis.
Good (1976) points out that even seemingly objective models have a sub-
jective component. For example, we may make a subjective judgment that
scores from a given test follow a normal distribution. Then we gather data
and create an “objective” estimate of the population mean. The “objective”
estimate of this model parameter is conditioned on the subjective choice of the
model. Good (1983) points out that the sensitivity of inferences to modeling
choices, such as the assumption of normality, is often much larger than the
sensitivity to obviously subjective prior opinion about the parameters in the
models.
Good’s philosophical approach essentially states that all models are sub-
jective. They become objective when many people agree on the model. In many
cases, the model may not be known precisely, or diﬀerent peoples’ models may
diﬀer in minor ways. In such cases a sensitivity analysis can reveal whether
critical decisions are dependent on these diﬀerences between candidate models.

46
3 Bayesian Probability and Statistics: a Review
Dempster (1990) mixes the subjectivist and objectivist ideas in a diﬀerent
fashion. He states that all probability judgments are subjective in the sense
that they are relative to a given body of evidence. However, he allows only
probabilities that are objective in the sense that they come from a readily
identiﬁable and objective data source.
This book takes an approach somewhere between Good’s and Dempster’s
blend of objectivism and subjectivism. Objective models come from consen-
sus between a group of decision makers on a relatively identiﬁable body of
information. The key here is the identiﬁcation and publication of the sources
of information. It is critical that reviewers be able to examine and critique all
of the information sources. We therefore deﬁne probability as representing a
state of information about an unknown event.
Deﬁnition. Probability (Objective–Subjective Deﬁnition). Let Ω be
the set of outcomes from an experiment which may or may not be possible
to carry out. Let A ⊂Ω. Suppose that according to our best information, the
outcome A is analogous to drawing a black ball from an urn with a proportion
P(A) of black balls. Then our probability of A is P(A).
This deﬁnition diﬀers from the Subjective deﬁnition of probability by
positing an agreement among people about the information and the models
that will be used to ground probabilistic inferences. Inferences in educational
testing can be consequential decisions such as employment and instruction.
In this context, fairness is an important value that goes beyond the statis-
tical property of objectivity. Candidates should know the criteria on which
they are being judged. If expert opinion is used, candidates and test users
must be able to learn who the experts are, what their qualiﬁcations are, how
they were selected, and what rationale the criteria are based on. In terms of
the evidence-centered design (ECD) models, this includes in particular the
evidence rules. Transparency ensures that aspects of the model are open to
challenge and eventually improvement.
3.2 Conditional Probability
The key to using subjective probability objectively is to identify the body of
information on which our probability judgments are based. It follows that if
that body of information changes, then so may our probabilities. The concept
of conditional probability formalizes this idea.
Deﬁnition. Conditional Probability. Let A and B be any two events such
that P(B) ̸= 0. Deﬁne the conditional probability of A given B (written
P(A|B)) as follows:
P(A|B) = P(A ∩B)
P(B)
.
(3.2)

3.2 Conditional Probability
47
An important use of conditional probability in the physical and social
sciences is describing the probability of hypothetical situations. In particular,
the conditional probability of A given B, P(A|B), answers the question, “If
we knew B, what would we believe about A?” In the medical context, we
might let A represent a particular disease state and B represent a particular
symptom. P(B|A) says how likely we think the symptom would be if a person
actually had the disease. P(A|B) says how likely it would be that a person
has the disease if we were to observe the symptom.
In many respects, all probabilities are conditioned on our experience. Some
authors even write P(A|e), where e represents the body of our experience so
far.
Example 3.3 (Remedial Reading). Let R represent the proposition that
a student will beneﬁt from a remedial reading program. Let e represent a
teacher’s experience with that student’s reading ability. Then P(R|e) would
represent the teacher’s probability that the student would beneﬁt from the
program. If T represents the proposition that the student does well on a read-
ing test, then P(R|T, e) represents the teacher’s probability that the student
would beneﬁt from the program given both the test score and the experience.
If we wish to be objective about our deﬁnition of probability, we must be
explicit about what we can and cannot include in e. From a purely scientiﬁc
perspective, the more information we have the better decision we can make.
However, there may be social reasons why we wish to restrict what information
we have. Suppose the experience of the teacher is that students from certain
racial groups are more likely to beneﬁt from the reading program. Should the
teacher include race in e? Primary language spoken at the student’s home?
These are not easy questions to answer, and they involve complicated personal
and societal values about fairness.3 The answer may be diﬀerent if the decision
is admission to college rather than placement into a reading program.
The joint probability of A∩B can be recovered from the conditional proba-
bility of A given B and the marginal probability of B. In this context, marginal
should be read as unconditional. We will see that it helps to think of this prob-
ability as the margin of a table. The multiplication rule does this recovery.
Deﬁnition. Multiplication Rule. Let A and B be two events such that
P(B) > 0. Then
P(A ∩B) = P(A|B)P(B) .
(3.3)
3 Let R represent the proposition that Jim robbed the convenience store, and F be
the fact that to the question “Did you rob the convenience store?” Jim responded
“I refuse to answer that question on the grounds that my answer may tend to
incriminate me.” Even though F empirically supports R—that is, P(R|F) >
P(R)—an American judge instructs the jury to ignore it in their deliberations.
The Fifth Amendment provides citizens this right in order to reduce the govern-
ment’s incentive to elicit confessions unscrupulously.

48
3 Bayesian Probability and Statistics: a Review
By applying this idea repeatedly, it is always possible to write a joint
probability as the product of conditional probabilities, with each event in the
list conditional on those earlier in the list. That is,
P (An, An−1, . . . , A2, A1)
= P (An|An−1, ..., A2, A1) × P (An−1|An−2, . . . , A2, A1) × · · · ×
P (A2|A1) × P (A1)
= 
k
P (Ak|Ak−1, . . . , A1),
where the ﬁnal term is understood to be simply P(A1). This is called a recur-
sive representation. Such a representation holds for any ordering, but we will
see in Sect. 3.3.1 that some orderings are more useful than others.
Conditional probability provides ways of calculating probabilities that
would be otherwise diﬃcult to judge. Two principal tools for doing this are
the Law of Total Probability and Bayes’ Theorem.
Deﬁnition. Law of Total Probability. Let A1, . . . , An be a partition of an
outcome space Ω and let B be another event in Ω. Then
P(B) =
n

i=1
P(B|Ai)P(Ai) .
(3.4)
This relationship is valuable because the conditional probabilities are often
easier to assess than the unconditional probability. The following theme comes
up over and over again in educational testing.
Example 3.4 (A Discrete Item Response Model). Suppose that the
students in a classroom can be divided in thirds on their mastery of a new skill,
S. One-third have completely mastered the skill, S = high; one-third have
partially mastered the skill, S = medium, and one-third have not mastered
the skill at all, S = low. Let X represent the event that a student (chosen
at random from the class) is able to solve a particular problem that uses
the skill in question. Further, we say that there is a 90 % chance a master
can solve the problem, a 50 % chance a partial master can solve it, and a
20 % chance a nonmaster will stumble upon a solution. Then the expected
proportion of correct responses in the classroom as a whole is the weighted
average of the expected proportion for each mastery state, with the weights
being the proportion of students at each state:
P(X) = P(X|S = high)P(S = high) + P(X|S = medium)P(S = medium)
+ P(X|S = low)P(S = low)
= 0.9 · 1/3 + 0.5 · 1/3 + .2 · 1/3 ≈0.533.
Suppose that in another classroom, the conditional probabilities of a correct
solution are the same but the distribution of students in mastery states is

3.2 Conditional Probability
49
diﬀerent: 3/6 at S = high, 2/6 at S = medium, and 1/6 at S = low. The
expected percentage in this classroom is obtained by the same formula with
the same conditional probabilities, but the mastery distribution for the second
classroom:
P(X) = P(X|S = high)P(S = high) + P(X|S = medium)P(S = medium)
+ P(X|S = low)P(S = low)
= 0.9 · 3/6 + 0.5 · 2/6 + .2 · 1/6 = 0.650.
This example demonstrates an assumption that conditional probabilities
for item responses given latent proﬁciencies are stable, but distributions of
latent proﬁciencies can vary across groups or before and after instruction. It
is a hallmark of psychometric models such as latent class analysis and item
response theory.
We have just seen how we can build up the population proportion of correct
response from the conditional probabilities of students at the diﬀerent levels
of mastery. The reasoning is in the opposite direction of the problem we face
in educational testing, however. It would be useful to reverse the direction of
the conditioning, that is, to calculate the probability of the skill state given
the result from solving the problem. Bayes’ Theorem allows us to do just that.
Theorem 3.1 (Bayes’ Theorem). Let A1, . . . , An be a partition and B be
an event such that P(B) > 0 and P(Ai) > 0 for all i. Then:
P(Ai|B) =
P(B|Ai)P(Ai)
n

i=1
P(B|Ai)P(Ai)
= P(B|Ai)P(Ai)
P(B)
.
(3.5)
Example 3.5 (Diagnostic Testing; Example 3.4 Continued). Suppose
that one of the students from the ﬁrst classroom in Example 3.4 is able to
solve the problem. What is the probability that that student has completely
mastered the skill; that is, P(S = high|X)?
P(S = high|X) = P(X|S = high)P(S = high)
P(X)
≈0.9 · 1/3
0.533
≈0.5629
This application of Bayes’ Theorem is so useful that its parts are given
special names. The unconditional probability, P(S = high), is called the prior
and the conditional probabilities P(X|S = si) for si = {high,medium,low}
are called the likelihood. The ﬁnal (conditional) probability for the quantity
of interest, P(S = high|X) is called the posterior because it represents our
information about S after observing X. In general, both the likelihood and
the prior have an inﬂuence on the posterior. This next example shows this
dramatically.

50
3 Bayesian Probability and Statistics: a Review
Example 3.6 (HIV Test; Almond (1995)). A common test for the HIV-1
virus (believed to be a principal cause of AIDS) is the Western Blot Test.
In 1988, the Morbidity and Mortality Weekly Report reported the analytic
sensitivity and speciﬁcity of the Western Blot test as reported by the Cen-
ter for Disease control in a 1988 evaluation. The analytic sensitivity is the
conditional probability of obtaining a positive test result from a positive sam-
ple; it was 99.3 %. The analytic speciﬁcity is the conditional probability of
obtaining a negative result from a negative sample; it was 97.8 %. As a rough
guess, about 5 persons per 10,000 had HIV in the state of Washington in
1991. (Note: These ﬁgures were obtained by multiplying the AIDS prevalence
rate reported in the November 8, 1991 Seattle Times by 5. This fudge factor
should probably be increased for urban areas or other high risk populations.
For a discussion of more accurate methods for estimating HIV infection, see
Bacchetti et al. 1993.)
Deﬁne the following events:
HIV+—subject has HIV
HIV−—subject does not have HIV
T+—subject tests positive
T−—subject tests negative
The Western Blot test’s performance can be summarized by the follow-
ing two conditional probabilities: P(T−|HIV−) = 0.978 (speciﬁcity) and
P(T+|HIV+) = 0.993 (sensitivity). In both cases higher values are preferred,
because speciﬁcity is the probability of a negative test result when the disease
is not actually present and sensitivity is the probability of a positive result
when it is.
If the hospital blood bank uses this test to screen blood donations, it wants
to know the probability that a randomly chosen sample of blood will have
HIV given that it tests negative with the Western Blot test.
P(HIV+|T−) =
P(T−|HIV+)P(HIV+)
P(T−|HIV+)P(HIV+) + P(T−|HIV−)P(HIV−)
=
.007 × .0005
.007 × .0005 + .978 × .9995 ≈4 × 10−6
If a doctor administers the test to patients to diagnose them for AIDS, she
wants to know the probability that a randomly chosen patient has HIV given
that he tests positive with the Western Blot test.
P(HIV+|T+) =
P(T+|HIV+)P(HIV+)
P(T+|HIV+)P(HIV+) + P(T+|HIV−)P(HIV−)
=
.993 × .0005
.993 × .0005 + .022 × .9995 ≈.022

3.3 Independence and Conditional Independence
51
Or about 1 in 50! How could there be such a low probability of a correct result
when the test seems so reliable in terms of its sensitivity and speciﬁcity? The
picture starts to become clearer when we realize that the chance of test failure,
even though it does not happen often, is still much larger than that of the
disease (at least for low-risk populations).
Many people ﬁnd this example, often called the Rare Disease Problem,
counterintuitive. Kahneman et al. (1982) talk about people’s heuristic biases
in evaluating probabilities. The result is puzzling if one ignores or discounts
the eﬀect of the low background rate of occurrence of the phenomenon, and
puts too much weight on the test results. A false reading from the Western
Blot test is a rare occurrence; but so is having HIV (unless the patient belongs
to a high-risk population). In this case, a false positive is less rare than the
disease itself. That is why doctors do not recommend HIV tests unless the
patient is believed to be at risk a priori (before the test). Other information
that increases the background probability of HIV would reduce the probability
that a positive reading is false. Most doctors would not regard a positive result
on the Western Blot test as a deﬁnitive positive diagnosis; they would follow
it up with more speciﬁc (and expensive) tests.
Contrast this to the blood screening test done by the hospital blood bank.
Here the two rare events must occur together in order for the undesirable
outcome (HIV-positive blood put in the blood bank) to occur. The blood
bank is happy to throw out the blood on the “better safe than sorry” principle,
and the overall risk to the resulting blood supply is very small (about 4 in
1 million).
3.3 Independence and Conditional Independence
In the previous section we discussed how the knowledge about whether or not
B occurred can aﬀect our knowledge about A. When there is no eﬀect, we say
that the events are independent. Because B provides no information about A,
P (A |B ) = P (A) and the multiplication rule reduces as in Eq. 3.6. We take
this as our deﬁnition.
Deﬁnition. Independence. Let A and B be two events. Then we say A and
B are independent if and only if
P(A ∩B) = P(A) · P(B) .
(3.6)
This is also called Marginal Independence to distinguish it from the related
concept of Conditional Independence we will introduce shortly.
If P(A) > 0 and P(B) > 0, independence can be deﬁned in terms of
conditional probability as shown below (Pearl 1988). We use the standard
deﬁnition because it works for events with zero probability.

52
3 Bayesian Probability and Statistics: a Review
Deﬁnition. Alternative Deﬁnitions of Independence. If A and B are
two events such that P(A) > 0 and P(B) > 0, then the following three state-
ments are equivalent:
A and B are independent (Eq. 3.6),
P(A|B) = P(A) = P(A|B) ,
and
P(B|A) = P(B) = P(B|A) ,
where A is the complement of the event A in Ω.
Independence corresponds to the cases in which we can simplify the cal-
culation of the probability of complex events. This is a key result which we
will use over and over again when building complex models for educational
assessment. Often it is easier to determine that certain events are independent
than to assess their joint probabilities. Once we have laid out the pattern of
independence, we can simplify the probabilities we need to assess and com-
pute. This becomes more interesting when more events are involved, so we
need to expand our notion of independence to more events.
Deﬁnition. Mutual Independence. Let A1, . . . , An be a set of n events.
These events are mutually independent if P(A1 ∩· · · ∩An) = n
i=1 P(Ai) and
any smaller subset of those events is mutually independent.
Pairwise independence does not imply mutual independence. The following
example demonstrates the diﬃculty.
Example 3.7 (Agreement of Two Random Statements). Consider a
psychological survey designed to test the attitude of subjects towards certain
topics. To see if the subjects’ attitudes are consistent, the survey asks two
questions about the same topic at diﬀerent points in the survey. Let S1 and
S2 be the events that a subject agrees with the two statements respectively and
let C be the event that a subject’s attitudes on the topic are consistent, either
agreeing to both or disagreeing to both. Suppose one subject is answering the
survey by ﬂipping a coin for every statement. In this situation, S1 and S2 are
independent. Also, P(S1) = P(S2) = 1
2.
Now C is functionally determined by S1 and S2; speciﬁcally, C = (S1 ∩S2)∪
(S1 ∩S2). Therefore:
P(C) = P(S1)P(S2) + (1 −P(S1))(1 −P(S2))
= 1
2 · 1
2 + 1
2 · 1
2 = 1
2
P(C ∩S1) = P(S1)P(S2) = 1
4
= P(S1)P(C) = 1
4
P(C ∩S2) = P(S2)P(C) = 1
4 .

3.3 Independence and Conditional Independence
53
Therefore, C and S1 are pairwise independent, as are C and S2. But when
we look at all three events:
P(C ∩S1 ∩S2) = P(S1 ∩S2) = 1
4
̸= P(C)P(S1)P(S2) .
Thus C, S1, and S2 are not mutually independent.
Recall that we deﬁned probability as a state of knowledge. Knowledge
about the response to one statement alone does not provide any knowledge
about the other statement or the agreement between the two. But knowl-
edge about one statement and the agreement between the two conveys exact
information about the other statement. Thus, every pair of events is pairwise
independent but the three together are not mutually independent.
3.3.1 Conditional Independence
As Example 3.7 shows, situations with more than two events can get quite
complex. Fortunately, the opposite situation sometime occurs. Learning that
an event C occurred can render two other events independent. We call this
conditional independence.
Deﬁnition. Conditional Independence. Let A, B, and C be three events.
Then we say A and B are conditionally independent given C if and only if
P(A ∩B|C) = P(A|C) · P(B|C) .
(3.7)
If A and B are conditionally independent given C, we write I(A|C|B).
This notation is from Pearl (1988). The intuition is that C separates A from
B (Chap. 4 develops this intuition). The standard statistical notation (Dawid
1979) is A ⊥⊥B | C. The notation I(A|∅|C), or A ⊥⊥B, refers to marginal
(unconditional) independence.
Conditional independence is a powerful tool for building models. By con-
ditioning on the right things, we can often build quite complex models
from smaller independent pieces. Consider for example the joint probabil-
ity P(A, B, C, D, E, F) under the following set of independence relationships:
I(F|D, E|A, B, C), I(E|C|A, B, D), I(D|C|A, B), and I(A|∅|B). The recur-
sive representation of the joint probability simpliﬁes to the product of smaller
factors as follows:
P(A, B, C, D, E, F) = P(F|D, E)P(E|C)P(D|C)P(C|A, B)P(B)P(A) .
The graphical models we develop in Chap. 4 combine conditional proba-
bility with representations and results from graph theory to support inference

54
3 Bayesian Probability and Statistics: a Review
in even large collections of variables, if theory and experience suggest condi-
tional independence relationships among them. (Moreover, the independence
relationships are much easier to see in graphs than in the symbolic notation
of the preceding example!)
It will be helpful to review a few examples that give us more intuition
into how conditional independence works and where these conditional inde-
pendence relationships come from. The next few subsections provide some
illustrations that arise in educational testing.
3.3.2 Common Variable Dependence
Conditional independence is not the same as mutual independence. The fol-
lowing illustration, adapted from the ”accident proneness” example of Feller
(1968), illustrates the diﬀerence.
Example 3.8 (Accident Proneness). Imagine a population with two types
of individuals: N, normal, and N, accident prone. And suppose that 5/6 of
these people are normal, so that if we randomly select a person from this
population, the probability that the chosen person is normal is P(N) = 5/6.
Let Ai be the event that an individual has an accident in year i. For each
individual, Ai is independent of Aj whenever i ̸= j. Thus for each individ-
ual, whether or not that person has an accident, a Bernoulli process is fol-
lowed. The accident probability, however, is diﬀerent for the two classes of
individuals.
P(Ai|N) = .01
P(Ai|N) = .1
The chance of a randomly chosen individual having an accident in a given year
follows from the Law of Total Probability, as a weighted average of the proba-
bility of an accident for normal individuals and for accident-prone individuals:
P(Ai) = P(Ai|N)P(N) + P(Ai|N)P(N)
= .05
6 + .1
6 = .15
6 = .025 .
That is, P(A1) = P(A2) = .025.
The probability that a randomly chosen individual has an accident in
both the ﬁrst and second year follows from the Law of Total Probability and
the fact that A1 and A2 are independent for a given individual. It too is
a weighted average, now of the probability of an accident in both years for
normal individuals and for accident-prone individuals:
P(A1 ∩A2) = P(A1 ∩A2|N)P(N) + P(A1 ∩A2|N)P(N)
= P(A1|N)P(A2|N)P(N) + P(A1|N)P(A2|N)P(N)
= .01 × .01 × 5
6 + .1 × .1 × 1
6
= .0005
6
+ .01
6 = .0105
6
= .00175 .
Note that

3.3 Independence and Conditional Independence
55
P(A2|A1) = P(A1 ∩A2)
P(A2)
= .00175
.025
= .07 .
But P(A2) = .025, so P(A2) ̸= P(A2|A1). Therefore, A1 and A2 are not
(unconditionally) independent!
Accident Prone
Year1
Year2
Fig. 3.2 Graph for Feller’s accident proneness example
Reprinted with permission from ETS.
The explanation for this phenomenon lies with the interpretation of prob-
ability as a state of information. When we learn that the individual in ques-
tion has had an accident during the ﬁrst year, it provides information about
whether or not he is accident prone, which in turn provides information about
what will happen during the next year. In general, whenever a variable (in
Feller’s example, accident-proneness) that determines the distribution of a
set of observations (whether an individual has an accident in each year) is
unknown, information about one sample value (accident in Year i?) pro-
vides information about the others (accident in Year j?) through the variable
(accident-proneness). This is the essence of common variable dependence.
A common example from educational testing is unidimensional item
response theory (IRT). Here the latent trait θ accounts for all of the depen-
dence between the observations. This structure, shown as Fig. 3.3, is some-
times called the “na¨ıve Bayes” model. It does not always work well, like when
the underlying interrelationships among observable variables are complex,
such as medical symptoms that tend to appear in clusters. In assessment,
though, tasks can be engineered to make this model ﬁt pretty well (Almond
and Mislevy 1999).
3.3.3 Competing Explanations
Conditioning on multiple events requires only a straightforward generalization
of the notation. It is worth exploring an example of a situation that arises in
diagnostic assessment.
Example 3.9 (Conjunctive Skills Model). Suppose θ1 and θ2 represent
two skills (e.g., reading and writing) and X represents performance on a task
which requires both (e.g., document-based writing task). Poor performance
on the task could be a sign of lack of either of the skills. Suppose we learned

56
3 Bayesian Probability and Statistics: a Review
X 1
X 2
X 3
X 4
X 5
Fig. 3.3 Unidimensional IRT as a graphical model
Reprinted with permission from ETS.
(from an earlier reading test) that the reading skills of the examinee were
high; we would then conclude that there was a deﬁciency in writing. Thus,
observing the performance on the task induces a dependency in our knowledge
about the skill variables. (See Problem 3.6.)
Figure 3.4 shows this example graphically. Knowing the state of the agree-
ment allows us to “complete the knowledge circuit” between the two state-
ments. Conditioning on common descendents induces dependencies. This is
the Competing Explanation phenomenon. This is the intuition behind the
concept of D-separation, deﬁned in Chap. 4.
1
X
2
Fig. 3.4 Variables θ1 and θ2 are conditionally dependent given X. Even though θ1
and θ2 are marginally independent, if X is known they become dependent
Reprinted with permission from ETS.

3.4 Random Variables
57
3.4 Random Variables
Setting a variable on the basis of a random event produces random variables.
Random variables are very convenient to work with, and we will see numerous
examples in educational testing. For example, if we perform an experiment
that consists of selecting a student from a school and giving that student a test
and a questionnaire, we could deﬁne numerous random variables associated
with that event: the age of the student, the response given to item 3, an out-
come variable representing whether the response was correct or not. Naturally,
these variables will not be independent and characterizing that dependence in
educationally useful ways will be the subject of most of the rest of this book.
There are generally three types of random variables: categorical ran-
dom variables whose outcomes are members of a category, possibly ordered;
integer–valued random variables whose outcomes are members of a subset of
the integers; and real–valued random variables whose outcomes are members
of a subset of the real line. Categorical and integer-valued random variables
are called discrete and real-valued random variables are called continuous.
The topic of random variables is usually well covered in basic statistics
texts. This section provides some basic deﬁnitions to support the discussion
of Bayes theorem in Sect. 3.5.
3.4.1 The Probability Mass and Density Functions
For a discrete random variable, the probability of each atom—outcome with
nonzero probability—of the distribution completely characterizes the distribu-
tion. If the random variable X has range {x1, . . . , xn}, then we can reconstruct
the probability measure from:
p(xi) = P(X = xi) .
(3.8)
This is known as the probability mass function or p.m.f., and is usually written
p(·). We can think of the random variable as being generated by an urn ﬁlled
with balls with numbers printed on the side. The p.m.f. p(xi) indicates the
proportion of balls with xi written on them.
Consider any set A of possible outcomes for a discrete random variable X.
We can calculate the probability that the outcome will fall into that set as
follows:
P(X ∈A) =

xi∈A
p(xi) .
(3.9)
All p.m.f.s can be characterized by two properties:
1. 1 ≥p(x) ≥0
∀x
2. 
all x p(x) = 1
(normalization)

58
3 Bayesian Probability and Statistics: a Review
Any function p(·) satisfying these two properties is a p.m.f. for some random
variable. The second property is particularly important. It is known as the nor-
malization constraint. Any non-negative function g(x) deﬁned on {x1, . . . , xn}
for which the normalization constant, 
all x g(x), is ﬁnite can be normalized
to produce a p.m.f. by dividing through by the normalization constant. That
is, we can obtain a p.m.f. p(x) from g(x) as
p (xi) ≡g (xi)
	
allx g (x).
Continuous random variables present us with an additional problem. For
one thing, our canonical example of objective probability, balls from an urn, no
longer works. Example 3.10 presents a new canonical example for continuous
distributions.
Example 3.10 (Random Point on a Line Segment). Let Θ = [0, 1] be
the unit line segment, and consider an experiment that consists of randomly
selecting a point from that line. Let A ⊆Θ be any set consisting of a collection
of disjoint intervals. The probability of the Event A is the total of the length
of all the intervals making up the set A.
Note that this example only deﬁnes probability for collections of disjoint
intervals. This allows us to avoid some pathological cases. At the same time,
the collection of disjoint intervals covers the most practically useful cases, so
we lose little by doing this.
For continuous random variables, the mass associated with any speciﬁc
outcome is always zero (think of the length of a single point). But since the
set of outcomes is dense, we can consider the density of the probability in a
small region around the outcome of interest. Thus we deﬁne the probability
density function or p.d.f. by
f(x) = lim
Δx→0
P(x ≤X ≤x + Δx)
Δx
.
(3.10)
The p.d.f. behaves very much like the p.m.f, except that in most of the
formulas, sums are replaced by integrals. Thus if X is a continuous random
variable and A is a set of possible outcomes for X then:
P(X ∈A) =

A
f(x) dx .
(3.11)
Similarly, the normalization constant is deﬁned by
P(X ∈Ω) =

Ω
f(x) dx ,
(3.12)
where Ω is the set of all possible values for X which have nonzero values of
f(·) (sometimes called the support of f). Normalizing a p.d.f. is analogous

3.4 Random Variables
59
to normalizing a p.m.f. Thus, if g(·) is a nonnegative function whose integral
over the whole real line exists and is equal to m, the normalized probability
density function is f(x) = g(x)/m.
A third useful representation for probability measures is the (cumulative)
distribution function or d.f. (or c.d.f.). It is deﬁned the same way for contin-
uous and discrete random variables:
F(x) = P(X ≤x) .
(3.13)
F(x) is thus the probability of all outcomes less than or equal to x. For a
discrete random variable
F(x) =

y≤x
p(y) ,
(3.13a)
while for a continuous random variable
F(x) =
 x
−∞
f(y) dy .
(3.13b)
The distribution function uniquely deﬁnes the probability measure. The
term distribution is often used to represent any function (d.f., p.d.f., p.m.f.,
or a probability measure) that uniquely deﬁnes the probability measure.
F(y)
0
1
2
3
0.0
0.2
0.4
0.6
0.8
1.0
*
*
*
*
F(x)
-3
-2
-1
0
1
2
3
0.0
0.2
0.4
0.6
0.8
1.0
a
b
Fig. 3.5 Examples of discrete and continuous distributions. a Discrete distribution.
b Continuous distribution
Reprinted with permission from Almond (1995) with permission from Taylor &
Francis Group.
Figure 3.5a, b shows an example of both a discrete and a continuous dis-
tribution function. We can see some common features, in particular:

60
3 Bayesian Probability and Statistics: a Review
1. Distribution functions are always nondecreasing: That is, x ≤y implies
F(x) ≤F(y).
2. They range between zero and one, that is, 0 ≤F(x) ≤1 with F(−∞) = 0
and F(+∞) = 1.
The discrete distribution (Fig. 3.5a) is a step function that takes jumps at
the atoms (points of nonzero probability) of the distribution. The jump of each
step is the probability associated with that particular atom, and the height
is the probability of observing a value less than or equal to the value of that
atom. For example, the mass associated with the atom 2 for the distribution
pictured in Fig. 3.5a is .375. Thus, there is a one-to-one relationship between
the p.m.f. and the distribution function for discrete probability relationships.
The distribution function of the continuous distribution (Fig. 3.5b) is con-
tinuous at every point in its domain (this is where it gets its name; this
property is called being absolutely continuous). We can recover the p.d.f. by:
f(x) = dF(x)/dx .
(3.14)
Finally, independence can be characterized in terms of the probability mass
or density function.
Deﬁnition. Independence of Random Variable. A series of random
variables X1, . . . , Xn are independent if and only if
discrete case
pX(x) = pX1(x1) · pX2(x2) · · · pXr(xr)
(3.15)
continuous
fX(x) = fX1(x1) · fX2(x2) · · · fXr(xr) .
3.4.2 Expectation and Variance
One of the most useful features of random variables is that we can calculate
expected values for functions of those variables.
Deﬁnition. Expected Value. Let X be a random variable and let h(x) be
a function deﬁned on the range of that random variable. Deﬁne the expected
value of h(X), denoted E[h(X)], to be:
E[h(X)] =

all X
h(x) dF(x) ,
(3.16)
if the integral exists. This so-called Lebesque–Stiltjes4 integral is a compact way
of writing analogous expressions for discrete and continuous random variables.
4 A Lebesque–Stiltjes integral is a generalization of ordinary integration that is per-
formed with respect to a measure (e.g., a probability distribution). If the measure
is continuous, then it becomes an ordinary integral with respect to the density.
If the measure is a counting measure (like a discrete probability) it becomes a
sum. Thus, it handily uniﬁes a lot of statistical formulas that are integrals for
continuous distributions and sums for discrete distributions.

3.4 Random Variables
61
If X is discrete random it expands to the sum

all X
h(x)p(x)
and if X is continuous it is the integral

all X
h(x)f(x) dx.
In the special case where h(x) = x, E[X] is the expected value of the random
variable X.
E[X] is also called the mean of X and is often written X. The mean is a
measure of location, the “center of gravity” of the distribution.
Example 3.11 (Resampling Distribution). We can create a probability
distribution corresponding to an observed sample as follows: Let x1, . . . , xm
be the unique values observed in a sample of size N. Let ni be the number of
times xi is observed. Deﬁne the following p.m.f.:
p(xi) = ni
N .
This is the resampling p.m.f. for “sampling from a sample.” Let X be a random
variable corresponding to the experiment. We draw a value at random from
the set of values in the sample. Then the expected value of X is the average
of the sample. The laws of probability theory say that if the original sample
size (N) was large enough, the resampling (bootstrap) distribution should
approach the original distribution function.
A second special expected value is the variance, which measures the
amount of uncertainty associated with a random variable (or a process whose
outcome is expressed in terms of the value of a random variable).
Deﬁnition. Variance. Let X be a random variable that has a ﬁnite mean
E[X] = μ. Then the variance of variance of X is the expectation of (X −μ)2
(if it exists), written Var(X).
The variance is a measure of spread of a distribution, speciﬁcally the
expected value of the squared distance from the mean. Its value is always
greater than or equal to zero. As the variance gets closer to zero, the state
of information about the random variable becomes more certain. The units
of the variance are the squared units of the original random variable, which
is not natural to think about. For that reason the standard deviation, which
is the square root of the variance, is often used instead of the variance to
describe the dispersion of a distribution. The reciprocal of the variance is
called the precision. The smaller the variance, the larger the precision of the
distribution. The precision turns out to be useful in calculations involving the
normal distribution (especially using Bayes theorem).

62
3 Bayesian Probability and Statistics: a Review
Example 3.12 (Normal Distribution). Let X be a random variable with
the following probability density function:
f(x) =
1
√
2πσ e−1
2( X−μ
σ )
2
.
(3.17)
Then E[X] = μ and Var(X) = σ2. We say that X follows a normal distribution
with parameters μ and σ2, and write X ∼

μ, σ2
.
A normal distribution is completely deﬁned by its mean and variance. This
means we can approximate any distribution function by a normal with the
same mean and variance. Sometimes this is a good approximation and some-
times it is a bad one, but it turns out to be quite good in a large number of
important situations. In particular, the central limit theorem implies that the
totals and averages of reasonably well-behaved random variables are approx-
imately normally distributed. For a normal distribution, approximately 2/3
of the outcomes lie within 1 standard deviation of the expected value and
approximately 95 % of the outcomes lie within 2 standard deviations.
Example 3.13 (Monte Carlo Integration). Let X be a random variable
with distribution F(X). Let h(X) be a function whose expectation and vari-
ance over the distribution F(X) we would like to know. In general E[h(X)]
may be diﬃcult to calculate. However, if we can take a sample X1, . . . , Xn
from F(X) we can get an approximation to the expectation and variance:
E[h(X)] ≈
n
i=1 h(Xi)
n
Var(h(X)) ≈
n
i=1(h(Xi) −E[h(X)])2
n −1
With this expectation and variance we can ﬁnd the closest approximating
normal distribution for h(X).
This Monte Carlo Integration is a useful trick which we will use when
trying to learn parameters for educational testing models.
3.5 Bayesian Inference
Although Bayesian statistics centers around Bayes theorem, it really repre-
sents a statistical philosophy (or philosophies, see Good 1971). The central
pillar of this philosophy is that a state of information about an unknown
event, variable, or parameter can be represented with a probability distri-
bution. Although initially controversial because of this extra assumption,
Bayesian statistics has proved quite powerful. It provides a guiding princi-
ple for building and reasoning about complex models, and provides correct
solutions to problems that were not tractable under the classical approach

3.5 Bayesian Inference
63
(treating parameters as ﬁxed but unknown quantities). Furthermore, modern
computing technology has made possible techniques like Markov Chain Monte
Carlo (MCMC) which can solve quite complex problems as long as they can
be cast in the Bayesian framework.
This book builds up a Bayesian discipline of psychometrics, with a par-
ticular focus on discrete observable variables and proﬁciency variables. In the
process of building that discipline it will use the fundamental ideas of Bayesian
statistics over and over. This section provides a brief review of those funda-
mentals. More thorough treatments can be found in Lee (1989), DeGroot
(1970), Box and Tiao (1973), Gelman et al. (2013a) or for a more mathemat-
ical treatment, Berger (1985).
3.5.1 Re-expressing Bayes Theorem
We introduced Bayes theorem in Sect. 3.2 in terms of probabilities of events.
Here is how it looks when written in terms of densities:
p (y |x∗) = Kf (x∗|y) p (y) ,
(3.18)
where p(y) is the density of the random variable y, f (x | y) is the conditional
density of another random variable x given y, x∗is a particular value of x,
p (y | x∗) is the conditional density of y given that x = x∗, and K is the
normalization constant needed to make p (y | x∗) integrate or sum to one.
That is,
K−1 =

all y
f (x∗| y)d P (y) = p (x∗) .
(3.19)
The value of K is not directly relevant to inference about x or y under the
Bayesian paradigm (although it is a consideration in calculations). Writing
Bayes theorem only up to proportionality focuses attention on the important
pieces:
p (y | x∗) ∝p (y) f (x∗| y)
(3.20)
or
posterior ∝prior × likelihood .
The next section walks through these pieces in detail.
3.5.2 Bayesian Paradigm
Example 3.14 (Propensity to Make Free Throws). Suppose that a cer-
tain individual has a probability θ for making a basketball free throw.5 Sup-
pose further that each time the individual attempts the shot, the outcomes
5 A free throw or foul shot in basketball is an attempt to throw the ball from a
ﬁxed line into the basket. It is awarded for a penalty, so the player can always
attempt the shot without the interference of opponents. Thus, each attempt is
made under reasonably repeatable conditions.

64
3 Bayesian Probability and Statistics: a Review
are independent given the probability of success and that the probability of
success remains unchanged. Our problem is to characterize what we believe
about θ if we observe this individual have S successes in n attempts.
This situation is mathematically equivalent to the binomial model for test
scores: An assessment consists of n tasks, each student is characterized by a
single propensity variable θ that gives probability of a correct answer to each
task, and all the responses are conditionally independent given θ.
Let us look ﬁrst at how this problem would be handled under the “classical
statistical” approach. Recall that the probability of observing S successes in
n attempts for a given value of θ is given by the binomial distribution:
Pr(S|θ, n) =

n
S

θS(1 −θ)n−S ,
(3.21)
and its mean and variance are nθ and θ (1 −θ) /n. The most common way
to estimate θ is by the observed proportion of successes, or ˆθ = S/n. The
observed proportion ˆθ is the least squares estimate of θ, and it is also an
unbiased estimate. That is, if increasingly many samples of size n were taken,
the mean of the ˆθs would be θ. Their variance would be θ (1 −θ) /n—which
would not be known in practice because it depends θ, so is usually approxi-
mated by ˆθ

1 −ˆθ

/n. The approximated standard error of estimation 
SEM
is the square root of this quantity. When n = 10 and S = 7, ˆθ = .7 and

SEM

ˆθ

= .145. An approximate 95 % conﬁdence interval, obtained by
adding and subtracting 1.96 times 
SEM around ˆθ, is (.416, .984).
One obvious shortcoming with this approach is that it breaks down when
the observations are all failures or all successes. In these cases 
SEM is zero,
which implausibly suggests there is no uncertainty associated with 0 or 1 as
an estimate of θ.
A bigger problem is that the reasoning is in the wrong direction. It tells us
what the distribution of the estimator ˆθ would be in repeated samples, given
that we know the true value of θ. But we are not taking repeated samples;
we generally have only the one realized sample. And we do not know θ; that
is what we want to make inferences about in the ﬁrst place. The classical
approach does allow us to make some rather indirect statements such as “If θ
were .7 then the probability of observing an S ≥s would be . . . ” There is a
natural tendency of statistical consumers to misinterpret such statements as
probabilistic statements about θ.
Maximum likelihood estimation, developed by R. A. Fisher in the 1920s, is
a more sophisticated classical approach. Once we actually observe a particular
value of S for a given number of attempts, Eq. 3.21 is reinterpreted as a
function of θ given the observed value of S. This is the likelihood, which
corresponds to the piece f (x∗| y) in Eq. 3.20 with θ playing the role of y and
the observed value of S playing the role of x∗.

3.5 Bayesian Inference
65
From a Bayesian point of view, computing the likelihood function based
on the realized sample is a step in the right direction: It conveys the evidence
that the sample we actually observed holds about θ. Figure 3.6 shows the
likelihood that is induced for θ when n = 10 and S = 7. Seven successes
could occur if θ is any value of than 0 or 1, but it is more likely at some
values than others. The ﬁgure shows that seven successes are very unlikely
to occur for low values of θ (we would usually see fewer successes) and also
unlikely for high values of θ as well (we would usually see more successes).
The relative heights of the likelihood indicate just how likely 7 of 10 would
be at each possible value of θ. It is about three times as great at .5 as it is
at .4, for example, and it takes its highest value when θ = .7. The observed
proportion of successes is in fact the maximum likelihood estimate (MLE) of
θ under the binomial distribution. These are all statements that can be read
directly from the likelihood function. They concern only the observed sample,
not the distribution of S or of estimates of θ in repeated samples, and not an
unknown true value of θ.
0.0
0.2
0.4
0.6
0.8
1.0
0.0
1.5
3.0
theta
likelihood
Fig. 3.6 Likelihood for θ generated by observing 7 successes in 10 trials
Reprinted with permission from ETS.
Of course MLEs can also be interpreted in terms of sampling distributions.
This is how Fisher used them, and most people do today. Their properties
under repeated sampling of S for known parameter values of θ are derived,
and the problematic interpretation of estimates and standard errors noted
above return in full force.
The Bayesian paradigm uses the evidence in the likelihood function in a
diﬀerent way. It allows us to coherently express our belief about θ, conditional
on the observed sample, in terms of a probability distribution for θ. We might
plot a variable’s posterior density to give a full picture of what we believe
after obtaining the new information (especially useful if the shape is unusual
or it has multiple modes), or we might summarize the information in terms of
its mean or its mode, and its standard deviation, or an interval that contains
95 % of the posterior posterior. The posterior mean, sometimes used a point
estimate, is called an EAP or expectation a posteriori estimate. The posterior
mode is called an MAP or maximum a posteriori estimate.

66
3 Bayesian Probability and Statistics: a Review
The key is that in order to express belief about θ in terms of a probability
distribution after experiment, we must also express our belief about it before
the experiment. Classical statisticians are reluctant to do this, but as the
Bayesian statistician Jimmie Savage said, you do not get to enjoy the Bayesian
omelet unless you break the Bayesian eggs.
Let us take the gentlest possible step in the Bayesian direction: A uniform
distribution over the interval [0, 1] as the prior distribution; that is, p(θ) = 1.
This says that before we see the player take the free throws, we have no reason
a priori to think that any possible value of θ is more probable than any other.
(This would not be the case if we knew something about the basketball player,
and we will discuss this in the next section.)
Example 3.15 (Propensity to Make Free Throws; Example 3.14 con-
tinued). Substituting the uniform prior for θ into Eq. 3.20, the proportion-
ality form of Bayes theorem, gives
p (θ |n = 10, S = 7) ∝p (θ) P (S = 7 |n = 10, θ)
∝1 × θ7 (1 −θ)3
= θ7 (1 −θ)3 .
(3.22)
The normalizing constant is an instance of a general form called the beta
function. The resulting posterior distribution is an member of a family of
distributions called beta distributions—speciﬁcally, a Beta(8, 4) distribution.
Beta distributions are central in Bayesian inference about binary data, and
will be discussed in greater detail in the following section. With a uniform
prior, the posterior for θ has exactly the same shape as the likelihood function
(Fig. 3.6), although it is rescaled so it can be interpreted as a probability
density. For example, the mode of the posterior is .7. A calculator for the Beta
distribution tells us further that the mean is .667, the standard deviation is
.131, and the region containing the most probable 95 % of the posterior is
(.390, .891).
We can use the posterior density to express our belief about θ after having
observed S. A person who misinterprets a classical 95 % conﬁdence interval
to mean that there is a 95 % probability that theta is between its bounds is
implicitly making a Bayesian inference assuming a uniform prior. But this
is the correct way to interpret the Bayesian posterior credibility interval,
(.390, .891).
We used the uniform prior for expository reasons, but trying to understand
just where it comes from, and why it rather than something else, is a deeper
question. Rev. Bayes himself had so much diﬃculty with the uniform assump-
tion that he did not publish the paper from which the Bayes theorem takes
its name during his lifetime (it was published posthumously). The remaining
sections in this chapter say more about where priors come from and how to
construct them.

3.5 Bayesian Inference
67
One simple technique often used when there are questions about the prior
distribution is to perform a sensitivity analysis on the choice of priors. To
perform a sensitivity analysis, one simply redoes the analysis with several
diﬀerent choices of prior. (See Exercise 3.2). If the results change substantially,
then they need to be viewed with some skepticism: Your inference depends
notably on Your choice of prior, which means that the data are not (through
the likelihood) providing enough information for a solid answer.
A thorough sensitivity analysis will check assumptions about the likelihood
as well as in the prior. The prior is not the only assumption in Example 3.14.
The independent, identically distributed assumption that underlies the bino-
mial distribution can also be questioned. We are assuming that the individual
is not learning how to perform the task better during the experiment. This
might be approximately true if the experiment is short in duration, but if the
experiment goes on for a long time it will become increasingly dubious. (Mod-
eling learning on the part of the test taker is a more diﬃcult problem, which
we will not explore in great depth in this book.) Both the Bayesian and clas-
sical approach to this problem share the i.i.d. assumption, and the Bayesian
analysis is much more sensitive to this assumption than to reasonable choices
of prior.
3.5.3 Conjugacy
The distribution in Eq. 3.22 is an example of a beta distribution. The beta
distribution has the following p.d.f.:
f(θ|a, b) =

1
B(a, b)

θa−1(1 −θ)b−1 ,
(3.23)
where the normalizing constant B(a, b) is the beta function6,
B(a, b) =
 1
0
ta−1(1 −t)b−1 dt.
(3.24)
The mean of the beta distribution is a/(a + b), the mode is
a−1
a+b−2 and its
variance is
ab
(a+b)2(a+b+1). Both a and b must be greater than zero for this to
be a proper distribution (otherwise the beta integral is inﬁnite). Note that the
uniform distribution is a special case of the beta distribution, corresponding
to a Beta(1, 1) distribution (i.e., with a = 1 and b = 1).
Figure 3.7 shows several beta distributions. We see that when a is equal to
b, Beta(a, b) is symmetric and is centered at .5. When a > b, the distribution
shifts lower, and when a < b, the distribution shifts higher. The greater a + b,
the more concentrated it is (that is, the higher the central peak). When a
or b is less than 1, then it shoots up at the upper and/or lower tail. (This
would look ﬂat if the x-axis was plotted on a logistic scale, as is often done
for probabilities.)
6 A beta function can also be written in terms of gamma functions: B(a, b) =
Γ(a)Γ(b)/Γ(a + b).

68
3 Bayesian Probability and Statistics: a Review
0.0
0.4
0.8
1.0
2.0
3.0
Beta ( 0.5 , 0.5 )
0.0
0.4
0.8
1
2
3
4
5
Beta ( 0.5 , 1 )
0.0
0.4
0.8
0
2
4
6
Beta ( 0.5 , 2 )
0.0
0.4
0.8
0
4
8
12
Beta ( 0.5 , 5 )
0.0
0.4
0.8
0
5
10
20
Beta ( 0.5 , 20 )
0.0
0.4
0.8
1
2
3
4
5
Beta ( 1 , 0.5 )
0.0
0.4
0.8
0.6
1.0
1.4
Beta ( 1 , 1 )
0.0
0.4
0.8
0.0
1.0
2.0
Beta ( 1 , 2 )
0.0
0.4
0.8
0
1
2
3
4
Beta ( 1 , 5 )
0.0
0.4
0.8
0
5
10
15
Beta ( 1 , 20 )
0.0
0.4
0.8
0
2
4
6
Beta ( 2 , 0.5 )
0.0
0.4
0.8
0.0
1.0
2.0
Beta ( 2 , 1 )
0.0
0.4
0.8
0.5
1.0
1.5
Beta ( 2 , 2 )
0.0
0.4
0.8
0.0
1.0
2.0
Beta ( 2 , 5 )
0.0
0.4
0.8
0
2
4
6
8
Beta ( 2 , 20 )
0.0
0.4
0.8
0
4
8
12
Beta ( 5 , 0.5 )
0.0
0.4
0.8
0
1
2
3
4
Beta ( 5 , 1 )
0.0
0.4
0.8
0.0
1.0
2.0
Beta ( 5 , 2 )
0.0
0.4
0.8
0.0
1.0
2.0
Beta ( 5 , 5 )
0.0
0.4
0.8
0
1
2
3
4
5
Beta ( 5 , 20 )
0.0
0.4
0.8
0
5
10
20
Beta ( 20 , 0.5 )
0.0
0.4
0.8
0
5
10
15
Beta ( 20 , 1 )
0.0
0.4
0.8
0
2
4
6
8
Beta ( 20 , 2 )
0.0
0.4
0.8
0
1
2
3
4
5
Beta ( 20 , 5 )
0.0
0.4
0.8
0
1
2
3
4
5
Beta ( 20 , 20 )
Fig. 3.7 A panel of sample beta distributions
Reprinted with permission from ETS.
The beta distribution is handy for representing knowledge about proba-
bilities because its range is restricted to the interval [0, 1]. It is even handier
when the likelihood is a binomial distribution, because if the prior distribu-
tion is a beta and the likelihood is a binomial distribution, then the posterior
distribution will be a beta as well. Speciﬁcally, if the prior is Beta(a, b) and
the data consist of S successes in n trials, then the posterior is:
f(θ|a, b, S, n) ∝prior × likelihood
∝θa−1(1 −θ)b−1 × θS(1 −θ)n−S
∝θa+S−1(1 −θ)b+n−S−1 ,
which is also a beta distribution, Beta(a + S, b + n −S).

3.5 Bayesian Inference
69
Note the similarity in the functional forms of the beta prior and the bino-
mial likelihood. The diﬀerence is that in the beta distribution, θ is variable
and a and b are the parameters, while the likelihood comes from the binomial
distribution with S is the observed variable and n and θ are its parameters. It
follows that when the beta distribution is used as a prior for observations that
follow a binomial or Bernoulli distribution, it expresses information equivalent
to a hypothetical experiment with a + b −2 observations, of which a −1 were
successes and b −1 were failures.
The beta and binomial distribution share a special relationship with each
other in Bayesian inference: When the prior is a beta distribution and the like-
lihood is a binomial distribution, then the posterior is always a beta distribu-
tion too. We will see that the normal distribution shares a similar relationship
with itself: If both the likelihood and the prior distribution for the mean are
normal (and the variance is known), then the posterior distribution for the
mean will be normal too. Distribution families for prior and likelihood with
this special property are known as conjugate families. The Beta-Binomial and
Normal–Normal we will see shortly are well-known examples.
Example 3.16 (Propensity to Make Free Throws; Example 3.14 con-
tinued). Consider the same individual from Example 3.14, and suppose that
individual will make another m attempts at the same free throw. Based on the
previous data, our prior distribution for success on the new set of attempts is
now a Beta(S, n −S). Suppose we observe T successes in the second set of m
attempts. Then our posterior for the individual’s success after the second set
will be a Beta(S + T, n −S + m −T ) distribution.
Note that even if we had reversed the orders of the two sessions of testing
(the one with n trials and the one with m trials), we still would have reached
the same conclusion at the end. We also would have reached the same conclu-
sion if we did the testing in one large session and observed S + T successes in
n + m trials.
This property has an interesting application. It means that we can use
Bayesian inference as a model for our learning about the individual from
Examples 3.14–3.16. We start with a prior distribution for θ representing our
knowledge about this individual if he “dropped out of the sky”—a very weak
prior such as Beta(1, 1), for example. We then observe some data about the
individual and update our beliefs (using Bayes theorem) to generate a new
posterior distribution. That posterior then becomes the prior for new data.
Thus Bayesian inference is a powerful tool for keeping track of our state
of knowledge about the student as the student interacts with an assessment
task by task, or in a series of assessments.
There are other conjugate families beside the Beta-Binomial. Suppose our
task attempt produces a categorical outcome rather than the success/failure
of Example 3.14. An example would be rolling a possibly loaded die, with pos-
sible outcomes {1, ..., 6}. Let θ = {θ1, . . . , θK} be a vector of k probabilities of

70
3 Bayesian Probability and Statistics: a Review
observing each of the K outcomes on a single trial. We observe a multinomial
outcome S = {S1, . . . , SK} where Sk is the number of outcomes in category k
in n trials. The likelihood induced by S would now be a multinomial distri-
bution:
p (S |n, θ) ∝

k
θSk
k
In this case, the conjugate prior is a generalization of the beta distribution
called the Dirichlet distribution. The parameter is a vector of K random values
between zero and one, θ = {θ1, . . . , θK}, with the restriction that K
k=1 θk =
1. The Dirichlet distribution then has the following density function:
f(θ|α) = C
K

k=1
θαk−1
k
,
(3.25)
with K
k=1 θk = 1, and normalizing constant C.
The mean and variance of each component of θ in the Dirichlet are sim-
ple functions of the αs. Let m = 
k θk. Then E[θi] = αi/m and Var[θi] =
[αi (m −αi)] /

m2 (m + 1)

. In analogy to the beta distribution, we can think
about the Dirichlet as the amount of information about the vector of multi-
nomial probabilities θ conveyed by observing a total of m trials, with αk
occurring in each category k.
If we then use a Dirichlet prior with parameters α = {α1, . . . , αK} and
observe a multinomial outcome S = S1, . . . , SK where Sk is the number of
outcomes in category k in n trials, then the posterior will be a Dirichlet(α1 +
S1, . . . , αK + SK) distribution:
f(θ|α, S) ∝
K

k=1
θαk+Sk−1
k
.
This distribution will come back again in Chap. 8 where we try to learn
Bayesian networks (which consist mainly of multinomial distributions) from
data.
Another convenient conjugate family is the Normal–Normal family. Sup-
pose our prior distribution for an unknown proﬁciency variable, θ, for an indi-
vidual is a normal distribution with mean μ and variance τ2, written N(μ, τ2).
Let X be the score of that individual on an assessment designed to measure
that proﬁciency. Under classical theory with normal errors, the probability
function for a student’s observed score X given her true score θ is N(θ, σ2),
where σ2 is the error variance, which we will assume is known. In this situ-
ation the likelihood for θ induced by observing X is also normal, N(X, σ2).
Then the posterior distribution for θ is
θ|X, μ, σ2, τ2 ∼N

μ/τ2 + X/σ2
1/τ 2 + 1/σ2 ,
1
1/τ 2 + 1/σ2

.

3.5 Bayesian Inference
71
(In this context, the symbol ∼should be read, “is distributed as.”)
To avoid taking all those reciprocals, Bayesians often work with the preci-
sion (the reciprocal of the variance) rather than the variance. Let U = 1/τ2
and V = 1/σ2. Then
θ|X, μ, U, V ∼N

Uμ + V X
U + V
,
1
U + V

.
The posterior mean is thus a precision-weighted average of the mean of the
mean and the mean of the likelihood. The posterior precision is the sum of
the prior precision and the precision from the likelihood; i.e., the information
about θ from the prior and from the observations.
To extend this reasoning, suppose that we have another assessment, Y ,
which measures the same proﬁciency. Let the error variance of Y be σ2
Y and
its reciprocal, or precision, be W. Then the posterior from observing both X
and Y will be
θ|X, Y, μ, U, V, W ∼N

Uμ + V X + WY
U + V + W
,
1
U + V + W

.
Thus the precision of the two assessments taken together is the sum of the
precision of the individual instruments added together. Continuing in this
fashion, we can see what happens as we gather more and more data (more
and more assessments). First, the precision of our posterior will get larger
and larger (i.e., its variance will get smaller and smaller). Second, the weight
of the prior will get smaller and smaller with respect to the weight of the
data, so if we have enough data it will eventually overwhelm the prior. (See
Exercise 3.9).
What if we do not know the variance for our assessment in the above
example? If both the mean and variance are unknown, we need to integrate out
over the unknown variance to draw inferences about the mean. The resulting
marginal posterior distribution for the mean will be a Student t distribution. If
we knew the mean but not the precision, the natural conjugate prior family for
the precision would be the gamma distribution, which for certain parameter
values is also a chi-squared distribution. Gelman et al. (2013a) develop these
cases in greater detail.
The known-mean unknown-precision situation appears in MCMC estima-
tion for models that use the normal distribution such as estimating conditional
probabilities in Bayes nets (Chap. 9). The parameterization for the gamma
distribution that (Gelman et al. 2013a) and the WinBUGS (Lunn et al. 2000)
computer program use has the following p.d.f.:
ba
Γ (a)xa−1e−bx
(3.26)
with both a and b positive. The support of gamma is the positive half-line, as
is appropriate for a prior for precision. Its mean is a
b and the variance is
a
b2 .
The mode is a−1
b2
for a > 1, but it asymptotes at zero when a ≤1.

72
3 Bayesian Probability and Statistics: a Review
Figure 3.8 shows several gamma distributions. All the examples in the left
column have a mean of 1, as a = b. Similarly, the examples in the right column
have a mean of 5. As mentioned, when a ≤1 the gamma shoots up to zero
as x decreases. For a > 1, gamma distributions have a mode greater than
zero and are positively skewed. As a increases, the gamma looks more like a
normal distribution.
As with the Beta and Dirichlet distributions, one can interpret the param-
eters of the gamma as results from a previous hypothetical experiment, in
which the sample size was 2a and the sum of squares around the mean was
2b. For people who do lots of analyses of variance, this may be an intuitive
way to set a prior for precision. At least thinking about 2a as sample size
helps. Working backwards from a mean and standard deviation and looking
at graphs of representative gamma distributions is probably more helpful.
Reasonable mild priors for precision in psychometric applications are (.5, 1)
or (1, 1)—not much weight, means around values that error variances and
population variances have when the latent scale is set so that populations
are roughly centered around zero and have variances that don’t diﬀer dra-
matically from 1. for example, the 95 % credibility interval for gamma(1, 1) is
(.025, 3.691).
3.5.4 Sources for Priors
The Bayesian paradigm has several advantages over the classical statistical
paradigm: Bayesian credibility intervals are easier to interpret than classical
conﬁdence intervals, and the paradigm often gives us guidance for computa-
tional strategies in cases where the classical paradigm has diﬃculties. The
price we must pay for those advantages is specifying the prior distribution.
There are generally two approaches to specifying the prior information.
The strong prior Bayesian approach tries very hard to model the state of infor-
mation before data is observed, often eliciting priors from experts. The weak
prior Bayesian approach instead tries to minimize the impact of the prior,
building up a collection of noninformative priors. Section 3.5.5 describes non-
informative priors; the remainder of this section looks at eliciting informative
priors.
Consider what we know about the individual in Example 3.14. Because the
probability of making a free throw is a probability, it must be between zero and
one. However, anyone growing up in a culture where basketball is routinely
played has access to better information then that. Children on playgrounds
often make free throws, so a probability of 1 in 1 million would obviously
be too small, even 1 in 100 seems on the low side. On the other hand, even
professional players routinely miss free throws, so 999 times out 1000 seems
very high; even 90 % would seem like a remarkable feat. Using this knowledge
we would build a prior distribution that is high in the middle probabilities,
but tails oﬀnear the upper and lower ends, such as Beta(5, 5).

3.5 Bayesian Inference
73
0
1
2
3
4
5
6
0.0
0.5
1.0
1.5
2.0
0.0
0.5
1.0
1.5
2.0
0.0
0.5
1.0
1.5
2.0
0.0
0.5
1.0
1.5
2.0
Gamma(1.0,1.0)
0
5
10
15
20
25
30
0.0
0.1
0.2
0.3
0.4
0.0
0.1
0.2
0.3
0.4
0.0
0.1
0.2
0.3
0.4
0.0
0.1
0.2
0.3
0.4
Gamma(0.5,0.1)
0
1
2
3
4
5
6
Gamma(2.0,2.0)
0
5
10
15
20
25
30
Gamma(2.0,0.4)
0
1
2
3
4
5
6
Gamma(5.0,5.0)
0
5
10
15
20
25
30
Gamma(5.0,1.0)
0
1
2
3
4
5
6
Gamma(20.0,20.0)
0
5
10
15
20
25
30
Gamma(20.0,4.0)
Fig. 3.8 A panel of sample gamma distributions
Reprinted with permission from ETS.
However, if we know the population that the individual comes from, we
might be able to say a lot more. For example, if the individual were a female
college student who was a member of her school team, we would have access
to a large collection of data about the performance of other student athletes
in her division on which to base our prior. Suppose the distribution of the
previous year’s players’ percentages across the division was .60 and had a
standard deviation of .15. We can work backwards to ﬁnd the beta distribution
that has this mean and standard deviation. Let ¯x be the sample mean of a
set of proportions and v be its variance. The method of moments estimates
of the beta parameters are
a = ¯x

 ¯x (1 −¯x)
v
−1

and
b = (1 −¯x)

 ¯x (1 −¯x)
v
−1

.
For our college player, this translates to a prior of about Beta(6, 4). If we
then observed her make 7 of 10 attempts, our posterior would be Beta(13, 7),

74
3 Bayesian Probability and Statistics: a Review
which has a mean of .65. and a standard deviation of .10. A teammate who
made only 2 of 10 would lead to a posterior of Beta(8, 12), with a mean
and standard deviation of .40 and .11, noticeably higher than the observed
proportion of .2. If she continued to shoot at this rate, however, and made
20 of 100 the posterior would be Beta(26, 84), with a mean and standard
deviation of .24 and .04.
When the prior for an individual case comes from a population distribu-
tion, the Bayesian paradigm has an interesting interpretation. The posterior
distribution is a mixture of the population information and the information
from the data speciﬁc to that individual. This is easiest to see in the case of
the Normal–Normal conjugate family. In this case, the posterior mean is a
weighted average of the population mean and the mean of the data (and the
posterior precision is the sum of the prior precision and the precision associ-
ated with the data). Thus, the estimate of the mean from the data is “shrunk”
towards the population mean. As the amount of data goes up, the amount of
shrinkage decreases. Such shrinkage estimators are generally more stable than
estimates taken purely from data, especially if the amount of data is low.
(Another common example of a shrinkage estimator is a linear regression,
where the information we get from the predictor X is “regressed” towards the
mean of the dependent variable Y .)
Population distributions are an ideal source for priors when they are avail-
able. In educational testing, the reference population is usually clear from the
speciﬁcation of the assessment. However, when substantial prior data on this
population or assessment are not available, prior opinion may be the only
possible source of information about the unknown values.
The elicitation of prior information can be a diﬃcult and time-consuming
task. Morgan and Henrion (1990), Walley (1991), Box and Tiao (1973) and
Berger (1985) review a number of methods. One problem in this ﬁeld is that
the lay perception of probability is not consistent and is subject to heuristic
biases (Kahneman et al. 1982).
Our favorite method is to ask the expert for a mean for the unknown and
an “observational equivalence” for the expert’s information, or the number
of observations which would be equivalent to this expert’s opinion. We saw
that with Beta and Dirichlet priors (and less intuitively, the gamma) this can
be thought of in terms of the results of a hypothetical experiment. There is,
however, no evidence that experts are particularly better calibrated on this
scale than any other. Furthermore, this approach requires a choice of baseline
for zero information (noninformative priors).
3.5.5 Noninformative Priors
To build a model based mainly on data we would like a prior distribution that
has a minimal impact on the posterior. The weak prior Bayesian analysis uses
noninformative priors distributions—priors that, according to some criteria,
contain no information about the parameter. Such noninformative priors also

3.5 Bayesian Inference
75
play an important role in Bayesian data analysis. Even if the ﬁnal analysis
uses stronger priors, the noninformative prior may be useful for sensitivity
analysis or in eliciting expert opinion.
The idea of the equal probability space introduced in Sect. 3.1.2 is the most
commonly invoked principle for construction probability distributions. It in
fact underlies the Canonical Examples 3.1 and 3.10. The result is a uniform
distribution over the primitive outcomes.
Applying this principle requires a subjective judgment, namely that each
primitive outcome is equally likely. This is not always reasonable in practice.
While it may be a good assumption for a simple game of chance, such as
tossing a coin or rolling a die, it obviously fails for more complex phenomena,
such as whether or not Los Angeles will experience a major earthquake next
year. Just because there are two outcomes does not mean, in and of itself, it
is reasonable to think they are equally likely.
Applying the principle of equal probability requires a second judgement
when assigning a distribution to a continuous random variable: which space we
should take to be uniform? Consider the beta-binomial model for the “propen-
sity to succeed” parameter, θ, in Example 3.14. In that example, we used a
uniform prior over the space, that is θ ∼Beta(1, 1). If instead we take θ to be
uniform in the logistic scale, we get θ ∼Beta(1/2, 1/2), and taking the dis-
tribution to be uniform in the natural exponential family parameter (Jaynes
1968) gives us θ ∼Beta(0, 0). The ﬁrst two priors give a marginal predic-
tion of 1/2 for the probability that the ﬁrst observation will be a “success”
event. The third is not a proper probability distribution because it cannot
be normalized. This is a fair amount of information for a “noninformative”
prior. Dempster (1968) proposes a system of using upper and lower bounds on
the prior, however the resulting distributions are not probabilities but rather
belief functions (see Almond 1995).
Jeﬀreys (1961) argues that the noninformative prior should be invariant
under transformations of the variables. Using an information theoretic argu-
ment, he winds up with the principle that the prior should be proportional
to the reciprocal of the Fisher information. In the beta-binomial case, this
yields a Beta(1/2, 1/2) prior. In the normal–normal case, it yields a uniform
distribution over the real line, which is not a proper probability distribution.
The use of improper priors (priors for which the normalization integral
is inﬁnite) is a matter of controversy in Bayesian statistics. If we have a fair
amount of data, the choice of noninformative prior will not make much dif-
ference. For example, if used with a binomial likelihood, the improper Jaynes
prior Beta(0,0), for example, amounts to reducing the impact of the observed
data by one success and one failure. The posterior does not change much if
many successes and failures have been observed. However, the improper prior
could get us into trouble if we do not have enough data. The resulting poste-
rior distribution will be improper unless there is at least one success and one
failure in the observed data.

76
3 Bayesian Probability and Statistics: a Review
One can get many of the beneﬁts of noninformative priors without the
problems on noninformative priors by specifying a weak proper prior. Such
a prior should be ﬂat or nearly so throughout much of the space of interest,
but can rapidly shrink outside the part of the sample space which is thought
to be likely. For example, you can take a normal distribution with a “large”
variance (where large is taken such that the 95 % interval will have a high
probability of covering all meaningful values of the unknown quantity).
Sometimes stronger measures are needed. For example, in an uncon-
strained IRT model, the person ability variable, θ is usually taken to be nor-
mally distributed. However, there is nonidentiﬁability in this model: You can
add a constant to all the abilities and item diﬃculties, and you can multiply
the population standard deviation by another constant and divide all of the
item slopes by the same constant, and get an equivalent model. Taking the
prior for the ability variable to be N(0, 1) resolves this ambiguity.
3.5.6 Evidence-Centered Design and the Bayesian Paradigm
Evidence-Centered Design is built around the Bayesian paradigm. Although it
will work with non-Bayesian measurement models (such as counting up item
scores to get the number right), it is at its best when the Proﬁciency and
Evidence Models (Chap. 2) are designed according to the Bayesian paradigm.
The proﬁciency model plays the role of the prior and the evidence model plays
the role of the likelihood. The Summary Scoring Process then simply applies
Bayes theorem to absorb the evidence coming in from the various task results.
Chapter 12 will explore this relationship more formally.
Calibrating the ECD Models (i.e., estimating the conditional probability
distributions) requires another application of the Bayesian paradigm. First, we
write prior distributions for the parameters of the Proﬁciency and Evidence
models. Then we apply Bayes theorem to update those parameters based on
the pretest data. At the same time we can do model checking to reﬁne both
our mathematical and cognitive models.
A fundamental principal of ECD is that the mathematical model of the
assessment should reﬂect the cognitive theory of the domain being assessed
from a perspective and at a grainsize that suits the purpose of the assess-
ment. As a consequence, applications of ECD can use strong priors based on
the cognitive theory. This use of strong priors does not mean that the models
are subjective. The ECD process mandates documenting the sources of infor-
mation and decisions which go into the design of an assessment (Mislevy et
al. 2003b) of the knowledge that went into making decisions about both the
priors and, more importantly, the likelihoods is disclosed for others to view,
critique, and object to. More importantly, as we learn from our data what
parts of the model do and do not work well, we can reﬁne the corresponding
parts of our cognitive model as well as our mathematical one.

3.5 Bayesian Inference
77
Exercises
3.1. (Subjective Probability) Bob, David, Duanli, and Russell are sitting in
Russell’s oﬃce. Russell takes a silver dollar out of his desk drawer and ﬂips it.
For each step of this story, write down if Bob’s, David’s, Duanli’s, and Russell’s
probability that the coin has landed heads side up, is (a) 0, (b) between
0 and 1/2, (c) 1/2, (d) between 1/2 and 1, or (e) 1.
1. Russell has not yet ﬂipped the coin.
2. Russell ﬂips the coin where nobody can see, but does not look at the
result.
3. Russell looks at the result, and sees that it is tails. He does not show it
to anybody else.
4. Duanli remembers that Russell has a two-headed silver dollar he bought
at a magic shop in his desk.
5. Duanli asks Russell about what coins he had in his desk. He replies that
he has two normal dollars, a two-headed coin, and a two-tailed coin.
6. Russell shows Bob the result, but does not let anybody else see.
7. Bob announces that the result is tails. Duanli believes Bob always tells
the truth, but David remembers that Bob likes to occasionally lie about
such things, just to make life a little more interesting.
8. David tells Duanli that Bob sometimes lies.
9. Russell shows everybody the coin.
3.2. (Sensitivity Analysis) Example 3.6 (Almond 1995) is mostly based on
fairly reliable, if dated, numbers, except for the factor of 5 which is used
to inﬂate the number of reported AIDS cases to the number of HIV-positive
individuals. This could charitably be called a wild guess. Perform a sensitivity
analysis to this guess by using several diﬀerent values for this fudge factor (e.g.,
1, 5, 10, 25, 50) and calculating the chance that a patient who tests positive
on the Western Blot test has HIV. How sensitive are the results to the prior?
3.3. In Example 3.6, the true rate of HIV infection is unknown. Suppose
we use a uniform distribution, P(HIV+) = .5, as a “noninformative” prior.
Calculate the chance that blood that passes the screening actually contains
HIV. Comment on the appropriateness of the uniform distribution as a prior.
3.4. (Subtest Independence) Suppose we have a 50-item assessment that fol-
lows the Unidimensional IRT model (Fig. 3.3). In particular, assume that all
of the item responses, Xi, are conditionally independent given the latent trait,
θ. Consider the score on two subtests, S1 = 25
i=1 Xi and S2 = 50
i=26 Xi, con-
sisting of the ﬁrst and second halves of the test. Are S1 and S2 independent?
If not, how could they be made independent? You may use the fact that if
X1, . . . , Xn are (conditionally) independent of Y , then 
i Xi is independent
of Y .

78
3 Bayesian Probability and Statistics: a Review
3.5. (Conjunctive Model) A math “word problem” requires students to read
an English-language description of a problem, translate it into a mathematical
problem, and solve it. To solve a word problem, a student generally needs both
suﬃcient English reading proﬁciency, E, and the math skill, M. Let S be the
score (right or wrong) for a given word problem, and assume the probability
of a correct answer is 85 % for students who have mastered both skills and
15 % for students who lack E, M, or both. (This is a conjunctive model.)
Assume that in a particular class 90 % of the students have suﬃcient English
proﬁciency to solve word problems of this type. Of the students that have
suﬃcient English proﬁciency 75 % of them have M. Of the students that lack
E only 50 % have M. Calculate the probability of mastery for the math skill
for the following students from this class:
a. A student for which we have not yet observed the performance on the
word problem.
b. A student who solves the problem correctly and is known to have suﬃcient
English proﬁciency.
c. A student who solves the problem incorrectly and is known to have suﬃ-
cient English proﬁciency.
d. A student who solves the problem correctly and is known to lack suﬃcient
English proﬁciency.
e. A student who solves the problem incorrectly and is known to lack suﬃ-
cient English proﬁciency.
f. A student who solves the problem correctly and whose English proﬁciency
is unknown.
g. A student who solves the problem incorrectly and whose English proﬁ-
ciency is unknown.
What is the eﬀect of a student’s lack of English proﬁciency on our ability
to measure her math skill?
3.6. (Competing Explanation) Presume the same situation described in Prob-
lem 3.5, except with E and M marginally independent, and P(E) = P(E) = .5
and P(M) = P(M) = .5. Show that E and M are not independent conditional
on S.
3.7. Suppose that we are trying to determine the ability of several students
to solve a particular kind of problem. Call the probability the student will
get the answer right on any particular problem θ. Use the Jeﬀreys prior
(Beta(1/2, 1/2)), and calculate the posterior mean and variance for the fol-
lowing students:
a. A student who got 7 items right on a 10-item test.
b. A student who got 9 items right on a 10-item test.
c. A student who got 15 items right on a 20-item test.
d. A student who got 18 items right on a 20-item test.
e. A student who got 30 items right on a 40-item test.

3.5 Bayesian Inference
79
f. A student who got 36 items right on a 40-item test.
Repeat this exercise with the uniform prior (Beta(1, 1)). How sensitive are
the conclusions to the choice of prior?
3.8 (True Score Test Theory). Suppose that a student’s score on a test
X = T + E, where T is the students true score (the score the student would
have obtained if the student did not make any mistakes) and E is the error.
Suppose that for a particular assessment instrument, the error is known to
be N(0, 52). Assume that the distribution of T for the student’s true score is
known to be N(70, 102). Calculate the mean and variance of the posterior for
the following students:
a. A student who got a score of 75.
b. A student who got a score of 90.
c. A student who got a score of 50.
What happens to those posteriors if the population variance gets larger?
smaller?
3.9 (Test Length). Suppose that an assessment is assembled from a collec-
tion of short tests. Let the score on Test i be Xi = T + Ei, where T is the
true score and the error Ei ∼N(0, σ2); that is each short test has the same
measurement-error variance. Assume that the population distribution for the
true score is N(μ, τ2). Let X = K
k=1 Xk be a student’s score on an assess-
ment consisting of K tests. Calculate the posterior mean and variance for the
true score for that student. What happens to these values as K increases?

4
Basic Graph Theory and Graphical Models
One of the underlying principles in our approach to assessment design is that
the psychometric model should reﬂect the cognitive model, at a grain size
and in a manner that suits the job at hand (Mislevy 1994). This answers
the fundamental question from the previous chapter, “where do we get the
knowledge to construct prior distributions?” It comes from the experts in the
domain being modeled. However, experts in cognition, learning, and substance
of an assessment area will rarely be comfortable with mathematical notation
for expressing their ideas. To work with them, the psychometrician needs
a representation which is rigorous, but intuitive enough for the substantive
experts to be comfortable.
Enter the graphical model depicting variables with nodes in a graph and
patterns of dependency with edges drawn between them; the graphical model
is a representation of the joint distribution over all of the variables. However,
because this representation is graphical, domain experts can provide feedback
or even help construct the corresponding model. Bayesian networks share
the idea of using graphs to communicate with experts with other statistical
techniques, in particular, structural equation models. The diﬀerence is that
the graphical representation that structural equation models use is tuned to
building systems of simultaneous equations that represent functional relations
among variables, while the graphs used with Bayes nets are tuned to express-
ing conditional independence relationships, within a representation of their
joint distribution.
This diﬀerence is subtle but has important implications. First, the condi-
tional independence conditions lead directly to eﬃcient computational algo-
rithms, in particular those discussed in Chaps. 5 and 9. Second, the factoriza-
tion properties of the graph lead to strategies for eliciting consistent probabil-
ity distributions. These advantages have led to a rapid rise of graphical model
techniques in the artiﬁcial intelligence community. In particular, Bayesian net-
works—graphical models in which all of the variables are discrete—are very
popular. This book arises out of our work in applying those techniques to
problems in educational testing.
c⃝Springer Science+Business Media New York 2015
81
R. G. Almond et al., Bayesian Networks in Educational Assessment,
Statistics for Social and Behavioral Sciences, DOI 10.1007/978-1-4939-2125-6 4

82
4 Basic Graph Theory and Graphical Models
This chapter provides a basic foundation of graph theory and graphical
models to support the application to educational testing that is developed in
the rest of the book. Section 4.1 provides a brief introduction to graph the-
ory, providing deﬁnitions of all of the necessary terms. The focus is on ideas
rather than technical details. The chapter provides enough background for
the reader to work with experts to build directed graphs to express the sub-
stantively important relationships, and to understand the key ideas behind
how this representation is transformed to a representation that supports eﬃ-
cient computational methods. Section 4.2 explores the relationship between
the graph and factorization and Sect. 4.3 explores the relationship between
separation in the graph and conditional independence. As Bayesian networks
are frequently built using causal arguments, Sect. 4.4 reviews the relationship
between graphical and causal modeling. Finally, Sect. 4.5 contrasts Bayesian
networks to a number of other techniques which use similar graphs.
4.1 Basic Graph Theory
The key feature of graphical models (including Bayesian networks) is that they
represent probability distributions that factor according to a graph. That is,
the joint probability distribution can be expressed as the product of factors
(such as conditional probability distributions) that each involve only subsets
of the variables, and those subsets correspond to the topology of the graph.
This graph thus provides a picture representing key aspects of the structure
of the distribution. The previous chapters have already used these pictures
informally. Deﬁning graphical models more formally requires some terms from
graph theory.
A graph is a pair G = ⟨N, E⟩where N is a set of objects called nodes or
vertices and E is a set of edges with each edge being a nonempty set of nodes.
Usually, edges contain exactly two nodes, in which case the resulting graph
is a simple graph (Sect. 4.2 describes hypergraphs which allow more than two
nodes in an edge). In this book, we draw nodes with a label (a letter, word,
or abbreviation) in a circle or a round box. Simple edges are drawn as a line
connecting two nodes.
For graphical models in statistics, the nodes represent variables in a prob-
ability model. Each variable, Ai, is associated with an outcome space, or set
of possible values. For most of the models in this book the outcome space will
be a ﬁnite set {ai,1, . . . , ai,n}, although for more general graphical models,
the outcome space can be an dense set such as the real line. For educational
applications, it helps to add an icon to the node to indicate the type of vari-
able represented. We will use node labels with circles to indicate proﬁciency
model variables and triangles to indicate evidence model variables.

4.1 Basic Graph Theory
83
4.1.1 Simple Undirected Graphs
In a undirected graph all of the edges are unordered pairs (In this book, the
word graph will be used to refer to simple, undirected graphs unless another
meaning is clear from the context). Fig. 4.1 shows an example.
A
B
C
D
E
F
Fig. 4.1 A simple undirected graph
Reprinted from Almond et al. (2007) with permission from ETS.
For a graph G = ⟨A, E⟩, two nodes, A1 and A2 are neighbors if there exists
an edge {A1, A2} (curly braces are used to represent unordered sets, so this is
equivalent to {A2, A1}) in E. That is, there is a line between A1 and A2 in the
representation. The set of all neighbors of a node Ai is called the neighborhood
of Ai in G. In Fig. 4.1, A and C are neighbors, but C and F are not. The
neighborhood of C is {A, B, D, E}.
Let C be a set of nodes such that for all Ai, Aj in C, Ai and Aj are
neighbors, i.e., there is a line between every two nodes in the set. Such as set
is called complete. {C, D, E, F} in Fig. 4.1 is not complete, because it lacks a
{C, F} edge. {A, B} is a complete set, but we notice that we could include C
and get a larger complete set. A maximal complete set is called a clique. The
cliques of Fig. 4.1 are {A, B, C}, {C, D, E}, and {D, E, F}. Cliques will be
important when we get to calculation, because they are subsets of variables
that we need to work with simultaneously.
There are two ways that one graph can be smaller than another: it can have
a smaller set of edges or it can have a smaller set of nodes. Let G1 = ⟨A1, E1⟩
and G2 = ⟨A2, E2⟩be two graphs. If they have the same nodes but G1 lacks
some of the edges in G2 (i.e., A1 = A2 and E1 ⊂E2) then G1 is a partial
graph of G2. If G1 lacks some of the nodes and possibly some edges of G2 and
does not have any additional edges (i.e., A1 ⊂A2 and E1 ⊆E2), then G1 is a
subgraph G2.
4.1.2 Directed Graphs
A directed graph (sometimes called a digraph) extends the idea of a undirected
graph in that its edges are ordered pairs. By convention the edges are drawn
as arrows. Figure 4.2 shows a typical directed graph. The arrow from D to F,
as an example, represents the directed edge (D, F) (parenthesis are used to
indicate ordered sets, so this is not equivalent to (F, D)).

84
4 Basic Graph Theory and Graphical Models
A
B
C
D
E
F
Fig. 4.2 A directed graph
Reprinted from Almond et al. (2007) with permission from ETS.
Let G = ⟨A, E⟩be a directed graph. Let A be a node in G. All of the nodes
that have an arrow from them to A are called parents of A. More formally,
they are deﬁned as {A∗: (A∗, A) ∈E}, and they are denoted pa(A|G) or more
simply pa(A) when the context is clear. Similarly, the nodes that A has an
edge going to are the children of A, or {A∗: (A, A∗) ∈E}. In a directed graph,
two nodes are neighbors if one is a parent of the other. In Fig. 4.2, C is a parent
of D. This language is often extended in the obvious way. For example, A is
an ancestor of D and F is a descendant of C. This terminology comes from
early applications in animal breeding, where nodes represent characteristics
of animals that are literally parents and children, ancestors and descendants.
4.1.3 Paths and Cycles
Let A0, A1, . . . , An be a series of nodes such that Ai and Ai+1 are neighbors.
Such a series is called a path of length n. A path is simple if no node is repeated.
Two nodes in a graph are connected if there exists a path between them. A
graph is connected if all its nodes are connected.
A path whose ﬁrst and last node are the same is a cycle. In the undirected
graph shown in Fig. 4.1, (C, D, F, E, C) is a cycle.
For directed graphs, what was deﬁned as a path is called a chain, but
there is an additional condition: a path on a directed graph requires that for
each i, the ordered pair (Ai, Ai+1) is an edge. That is, all the directed edges
must point in the direction of travel. In the directed graph shown in Fig. 4.2,
(C, D, F, E, C) is not a directed cycle because the directions do not allow a
trip from C back to C again. We sometimes call (C, D, F, E, C) an undirected
cycle, meaning that it would be a cycle if direction were ignored.
An undirected connected graph that contains no cycles is said to be acyclic,
and is called a tree. A node of a tree that is a member of only one edge is a
leaf. Figure 4.3 is a tree (in fact, it is a spanning tree of Fig. 4.1, which means
it is a subgraph that is a tree). In this graph nodes A, B, and F are leaves.
The idea of a tree will also be important when we consider updating beliefs.
This is because in a tree, moving from an initial node to each of its neighbors,
then to each of their neighbors in turn which have not yet been visited, we
are ensured that each node will be visited exactly once.
Acyclic directed graphs—directed graphs containing no directed cycles—
play a special role in the construction of models. Figure 4.4b is an acyclic
directed graph (note that these graphs may contain undirected cycles).

4.1 Basic Graph Theory
85
A
B
C
D
E
F
Fig. 4.3 A tree contains no cycles
Reprinted from Almond et al. (2007) with permission from ETS.
Fig. 4.4a is cyclic. Acyclic directed graphs are often called by the euphonious
misnomer DAG. Technically speaking a directed acyclic graph is an acyclic
graph, a tree, whose edges are directed. However, most authors who use the
abbreviation DAG are talking about acyclic directed graphs, and we will do
so too.
A
B
C
A
B
C
a
b
Fig. 4.4 Examples of cyclic and acyclic directed graphs. a Cyclic, b acyclic
Reprinted from Almond et al. (2007) with permission from ETS.
Acyclic directed graphs play a key role in the theory of Bayesian net-
works. As the direction of the edges represents the direction of statistical
conditioning, the acyclic condition prevents the modeler from specifying the
distribution using circular dependencies, ensuring that the distribution is well
deﬁned from the graph (In contrast, the graphing conventions used with struc-
tural equations models allow directed cycles, for example to convey reciprocal
causation).
Let A0, A1, . . . , An, A0 be a undirected cycle. A pair of nonadjacent nodes
that are contained in a common edge are called a chord of the cycle. If a cycle
contains no chords it is called chordless. Recall that (C, D, F, E, C) is a cycle
in Fig. 4.1; {D, E} is a chord in this cycle. An undirected graph that has
no simple chordless cycles of length greater than three is called triangulated.
If a graph is not triangulated, additional edges can be ﬁlled in until it is
triangulated.
Figure 4.5 shows an untriangulated graph, and one ﬁll-in that will make it
triangulated. We will see in Chap. 5 how triangulation is used in the compu-
tation algorithms to avoid double counting evidence. Suppose that in Fig. 4.5
we are propagating evidence from D to A. There are two paths by which the

86
4 Basic Graph Theory and Graphical Models
A
B
C
D
Fig. 4.5 Filling-in edges for triangulation. Without the dotted edge, this graph is
not triangulated. Adding the dotted edge makes the graph triangulated
Reprinted from Almond et al. (2007) with permission from ETS.
evidence ﬂows, one through B and one through C. The triangulation reminds
us that the two evidence ﬂows are not independent (they both come from D)
and we will have to take the joint eﬀect into account.
4.2 Factorization of the Joint Distribution
Armed with our knowledge of graph theory, we can now deﬁne a graphical
model. A graphical model combines graphs and probability in such a way
that features of graphs help us better understand and work eﬀectively with
probability models.
Recall that an integer can be written as the product of smaller integers,
such as 360 = 5 × 32 × 23. Basically, a graphical model is a probability dis-
tribution that can be factored into the product of pieces involving smaller
sets of variables, according to structure of the graph. However, the nature of
the pieces and the exact rules varies with the type of graph. Section 4.2.1
describes models using directed graphs and Sect. 4.2.3 describes models using
undirected graphs. Section 4.2.2 describes the factorization hypergraph which
links the two representations. As described in Sect. 4.3, the diﬀerent types
of graphs also have diﬀerent rules for reading conditional independence con-
straints, and it is often useful to work back and forth between the directed and
undirected representations. In particular, directed graphs are better for work-
ing with experts, building models around substantive knowledge, and eliciting
initial estimates of probabilities, while undirected graphs support key compu-
tational eﬃciencies.
4.2.1 Directed Graph Representation
We saw in Chap. 3 that a probability distribution can be written in a recursive
representation, and that terms simplify when conditional independences let
some variables drop out of the conditioning lists. This section shows how

4.2 Factorization of the Joint Distribution
87
this phenomenon can be expressed in terms of directed graphs. Consider a
probability distribution over six variables, A, B, C, D, E, and F, that can be
factored as follows:
P(A, B, C, D, E, F) = P(A)P(B)P(C|A, B)P(D|C)P(E|C)P(F|E, D) .
To draw the directed graph that corresponds to this distribution, start with
a set of nodes, one for each variable. For every conditional distribution, draw
a directed edge from each conditioning variable to the consequence variable
in the distribution; for example, for P(C|A, B), draw edges from A to C and
from B to C. The result is Fig. 4.6. This is the basic idea of the directed
graphical model: the distribution for each variable is deﬁned conditionally
on its parents in the graph. Note that this representation does not say any-
thing about the nature or the functional form of the dependence—just that
the dependence exists. This correspondence between direct dependence and
conditional probabilities is the starting point for all that follows. Here is the
formal deﬁnition.
A
B
C
D
E
F
Fig. 4.6 Directed Graph for P(A)P(B)P(C|A, B)P(D|C)P(E|C)P(F|E, D)
Reprinted from Almond et al. (2007) with permission from ETS.
Deﬁnition. Directed Graphical Model. Let A be a set of variables that
describe some problem space. Let G = ⟨A, E⟩an acyclic directed graph whose
nodes correspond to the variables in A. The probability function PG is called
the total probability and is deﬁned by:
PG =

A∈A
P(A|pa(A)) .
(4.1)
If pa(A) is empty, then the conditional probability is taken as an unconditional
(marginal) probability.
The key idea is to use the law of total probability to break a big joint prob-
ability distribution up into many small factors. Although a joint probability
distribution can always be factored according to Eq. 4.1 (using a complete
graph—one where there is an edge between every possible pair of nodes—as
the base), more often than not the modeler gets a break from conditional inde-
pendence conditions. In fact, Pearl (1988) claims it is a characteristic of human
reasoning to organize our knowledge in terms of conditional independences,
and where they do not exist, invent variables to create them—syndromes in

88
4 Basic Graph Theory and Graphical Models
medicine, for example, or in our case, latent variables in educational mea-
surement. At any rate, if the edges are sparse in a graph, then the factors
in Eq. 4.1 will be small. This condition can be exploited to produce eﬃcient
algorithms both for eliciting probabilities and carrying out computations.
4.2.2 Factorization Hypergraphs
It would be nice if the edges were in one-to-one correspondence with the fac-
tors of the joint distribution, but this only happens in special cases such as
chains. Graphs with more than one edge per distribution, as in the preced-
ing example, are the rule rather than the exception. It happens whenever a
variable has more than one parent. We can extend our graphical tool kit to
express these relationships with hypergraphs. If graph edges are allowed to be
arbitrary nonempty sets of nodes, not just pairs, then the resulting graph is
a hypergraph and the edges of the hypergraph are called hyperedges.
Using hypergraphs we can represent distributions with one hyperedge for
each factor in Eq. 4.1. This is a key step for moving from a directed graph,
which is easiest to build working with experts, to an undirected graph that
supports eﬃcient calculation algorithms. To see the steps by which we move
from a directed graph that represents a joint distribution to an undirected
graph that supports computation on that distribution, we will need to deﬁne
hypergraphs, directed hypergraphs, and 2-sections.
A hypergraph is drawn with the nodes represented by points, numbers, or
letters and the edges represented by closed curves enclosing the elements of the
edges. Figure 4.7a shows an undirected hypergraph. {F}, {F, W}, {A, D, S},
and {D, L, F, M} are some of its hyperedges.
For a hypergraph G = ⟨A, E⟩, two nodes, A1, A2 are neighbors if there
exists a hyperedge that contains them both. F and W are neighbors in
Fig. 4.7a, and the neighbors of S are A, D, W, and R.
If H is a hypergraph, then there exists a simple (undirected) graph G with
the same set of nodes such that every node A has the same neighbors in G
as it has in H. This graph is called the 2-section of H. We can construct
a hypergraph’s 2-section by starting with its set of nodes, and drawing a
simple edge between every pair of nodes that are neighbors in the hypergraph.
Figure 4.7b is the 2-section of Fig. 4.7a. A given hypergraph has a unique
2-section, but many hypergraphs can have the same 2-section. The idea of
a 2-section is a step in moving from a directed graph representation of a
joint probability distribution to a computing representation based on a simple
graph.
A directed hypergraph is made by partitioning each hyperedge into two
parts: a set of parents and a set of children. These are directed hyperedges.
For the most part, we will restrict ourselves to hypergraphs with one child
node per edge, and associate the directed hyperedges with marginal and con-
ditional probability distributions. This gives what is called the factorization
hypergraph of the directed graphical model—a representation that connects

4.2 Factorization of the Joint Distribution
89
L
D
F
M
A
S
W
R
L
D
F
M
A
S
W
R
a
b
Fig. 4.7 Example of a hypergraph (a) and its 2-section (b)
Reprinted from Almond (1995) with permission from Taylor & Francis.
the factors of the probability distribution with the features of the graph. We
will draw directed hyperedges as rectangles, and later we will annotate them
with icons that signify the type of distribution they represent. Tentacles link
the hyperedge icons to the nodes; they look like arrows. Figure 4.8 shows an
example of a directed hypergraph.
B
A
P(A)
P(B)
P(C|A,B)
C
P(D|C)
P(E|C)
D
E
P(F|D,E)
F
Fig. 4.8 Hypergraph representing P(A)P(B)P(C|A, B)P(D|C)P(E|C)P(F|E, D)
Reprinted from Almond et al. (2007) with permission from ETS.
In this representation, variables are nodes in the graph, and hyperedges
are distributions. We see a directed hyperedge for each node, which is labeled
by the conditional or marginal probability distribution that is associated with
it. That is, for each variable X in Fig. 4.8 there is a directed hyperedge
(X, pa(X)). For example, P(F|E, D) is represented in the ﬁgure by the box
with that label and the tentacles from the parents D and E, through the
box, to the child F. Recalling that a single node can be a hyperedge in a
hypergraph, we also see a directed hyperedge associated with A, which has no
parents; accordingly, the box representing this hyperedge is labeled with the
marginal distribution P(A).

90
4 Basic Graph Theory and Graphical Models
Shenoy and Shafer (1990) call this representation a valuation based system,
where “valuation” refers to the probability distributions and conditional dis-
tributions that add a layer of quantitative information to the structural rela-
tionships depicted in the hypergraph. The term valuation is actually broader
than just probabilities, and a number of kinds of relationships between vari-
ables can be modeled with valuation based systems (Sect. 4.5 lists a few).
As with the undirected hypergraph, we make an undirected 2-section by
connecting all nodes that are in the same hyperedge, for all hyperedges. Fig-
ure 4.9 shows the undirected 2-section of Fig. 4.8.
A
B
C
D
E
F
Fig. 4.9 2-section of Fig. 4.8
Reprinted from Almond et al. (2007) with permission from ETS.
Note what happens as we go from a directed graphical model, Fig. 4.6,
through its factorization hypergraph, Fig. 4.8, to its 2-section, Fig. 4.9. Nodes
that are the parents of common children are joined in the undirected version.
This process is called moralization and the resulting graph is the moral graph
because the parents are “married.” This is the principle way to go from the
directed to the undirected graphical representation of a probability distribu-
tion.
4.2.3 Undirected Graphical Representation
In an undirected graphical model, the factors of the joint probability distri-
bution are associated with the cliques of the graph. The graph in Fig. 4.9
has three cliques: {A, B, C}, {C, D, E}, and {D, E, F}. The factor associ-
ated with each clique is the product of the component distributions deﬁned
over the variables in the clique, which we read from Fig. 4.8. The rule is to
include the conditional probability distribution for each variable in the clique
that has parents that are also in the clique, and the marginal probability
distribution for each variable in the clique that has no parents. Thus, the
factor associated with the clique {A, B, C} is the joint probability obtained
as P(C|A, B)P(A)P(B). The factor associated with {D, E, F} is the condi-
tional probability P(F|D, E), and the factor associated with {C, D, E} is the
product of conditional probabilities P(D|C)P(E|C).
As this example shows, the factors associated with the cliques can be either
a probability distribution, a collection of conditional probability distributions,
or complex combinations of probabilities and conditional probabilities. We call
such objects potentials. They are what we will use in Chap. 5 for updating

4.3 Separation and Conditional Independence
91
probability distributions when values are obtained from a subset of variables.
Although potentials may or may not represent probability distributions, they
can usually be normalized and interpreted as probabilities. The collection of
variables over which a potential is deﬁned is called its frame of discernment
(This term is adapted from Dempster-Shafer theory where it originally was
used to mean outcome space. See Almond 1995).
Whether the graph is directed or undirected, the key idea is the same. The
joint probability distribution is broken up into a collection of factors, C.
•
Directed Graphs—Sets C correspond to each node A and its parents.
•
Undirected Graphs—Sets C correspond to cliques in the graph.
Factorization, and the corresponding conditional independence conditions,
can be exploited when calculating probabilities for subsets of variables. Chap-
ter 5 explores some of the methods for making routine Bayesian updating
calculations much more eﬃcient by using these ideas.
4.3 Separation and Conditional Independence
As mentioned previously, one important feature of the graphical model is
that separation of the variables in the graph implies conditional indepen-
dence of the corresponding variables. To formalize this intuition, we need to
formally deﬁne separation. Section 4.3.1 provides a deﬁnition for separation
in both undirected and directed graphs. Section 4.3.2 explores the relation-
ship between separation and independence. Finally, Sect. 4.3.3 describes the
important Gibbs–Markov equivalence theorem which states that factorization
implies independence and vise versa.
4.3.1 Separation and D-Separation
Directed and undirected graphs encode conditional independence conditions
diﬀerently. Consequently, the deﬁnition of separation is diﬀerent in the two
diﬀerent types of graph. For the undirected graph, separation corresponds
nicely to the intuitive notion of the concept. Fortunately, separation in undi-
rected graphs is not only easier to understand, it is the one that matters in
computing algorithms.
Deﬁnition. Separation. Let X, Y, and Z be sets of nodes in an undirected
graph, G. Z separates X and Y, if for every Ax in X and for every Ay in Y,
all paths from Ax to Ay in G contain at least one node of Z.
In Fig. 4.9, C separates {A, B} from {D, E}. Taken together, {D, E} sep-
arates C from F.

92
4 Basic Graph Theory and Graphical Models
An equivalent way to think about separation is that deletion of the nodes
Z from the graph disconnects the nodes of X from the nodes of Y. The
intuition is that if we remove the nodes Z by conditioning on the corresponding
variables, this renders the variables in X and Y independent.
For directed graphs, the “competing explanation” phenomenon compli-
cates the notion of independence. Recall from Example 3.9 that sometimes
making an observation can render two previously independent variables depen-
dent. These competing explanation cases will have a graphical structure that
looks like X →Z ←Y , with converging arrows both pointing at Z (these are
sometimes called colliders). As there is no directed path from X to Y , they
are separated. However, when Z is observed there is still a dependence. For
that reason, reading independence conditions from a directed graph requires
deﬁnition of d-separation.
Deﬁnition. d-Separation. (Pearl 1988) Let X, Y, and Z be sets of nodes
in a acyclic directed graph, G. Z d-separates X and Y, if for every Ax in X
and for every Ay in Y, there is no chain from Ax to Ay in G along which the
following conditions hold: (1) every node with converging arrows is in Z or
has a descendant in Z and (2) every other node is outside Z.
This somewhat obscure deﬁnition captures the fact that observing the
value of a common descendant can make ancestors dependent. Simply looking
at where there are edges is no longer suﬃcient, as it was with separation in
undirected graphs, because the same pattern of edges can lead to d-separation
in some cases and not in others, depending on their directions.
The intuition is as follows: Ax and Ay are d-connected if information can
“ﬂow” from Ax to Ay (or the other way). Assume we “know” the values for the
variables corresponding to nodes in Z; in some cases this blocks the ﬂow and
in other cases it opens the ﬂow. (1) Knowing intermediate steps along a path
from Ax to Ay (or Ay to Ax) blocks the ﬂow of information along that path.
(2) Knowing common (direct) ancestors blocks the ﬂow of information from
Ax and Ay through that ancestor (see Example 3.8). (3) Knowing common
descendants unblocks the ﬂow of information from Ax to Ay through the
common descendant (see Example 3.9), although if the common descendant
is not known, then that path is still blocked. Figure 4.10 shows some examples.
The upshot of d-separation for our purposes is this: It is easiest to construct
directed graphs that reﬂect the local relationships that are cognitively and
substantively important. The competing explanations phenomenon, however,
can introduce some relationships that are not apparent in this representa-
tion, and have important implications for updating beliefs from observations.
Induced dependencies are important in two ways: conceptually, for sorting out
evidence in correct but subtle ways, and computationally, for making sure the
updating to all other variables is coherent. Because under some circumstances
knowledge about D renders B and C dependent, then, B and C must be con-
nected in the undirected graph to represent the same pattern of dependencies.

4.3 Separation and Conditional Independence
93
A
B
C
E
F
D
Fig. 4.10 D-Separation (Pearl, 1988). Here, {E} d-separates D and F (intermediate
step in chain). {A} d-separates B and C (common ancestor), but {D} does not d-
separates B and C (common descendant). Furthermore, {A, F} does not d-separate
B and C even though {A} does by itself (common descendants must not be included
in the separator set)
Reprinted from Almond et al. (2007) with permission from ETS.
This is why, to go from the directed to undirected representations, B and C
must be married, producing the moral graph (A formal way of saying this is
that we need to work with the 2-section of the factorization hypergraph).
4.3.2 Reading Dependence and Independence from Graphs
Ideally, the separation properties of the graph should show all of the con-
ditional independence relationships in the probability model. This is seldom
possible. The terms I-Map and D-Map (Pearl 1988) categorize the relation-
ship between a model and a graph. The formal deﬁnitions are stated in terms
of separation in the graph and independence in the model:
Deﬁnition. D-Map, I-Map. Let M be a probability model and let G be
a graph with a one-to-one correspondence between the nodes of G and the
variables of M. G is a dependency map (or D-map) of M if for all disjoint
subsets X, Y, and Z of the variables such that X is independent of Y given
Z in M, Z separates X and Y in G. Similarly, G is an independence map
(or I-map) of M if Z separates X and Y in G implies X is independent of Y
given Z in M.
What this means is that in a D-map, wherever there is an edge there is
a dependence relationship between those variables in the probability model.
However, there may be dependencies that are not shown. This is the issue we
discussed in the previous section, where observing a descendant can induce a
dependency among ancestors that was not depicted in the original directed
graph. If we are thorough, the directed graphs we create with experts to depict
cognitive and substantive relationships will usually be D-maps.
An I-map, in contrast, can miss independence conditions. Everywhere
there can a dependency between variables in the model that cannot be
removed by conditioning on other variables, there will be an edge in the
graph—but there may be edges in the graph where there are not in fact
dependencies. The 2-section of a factorization hypergraph, like Fig. 4.9, is an

94
4 Basic Graph Theory and Graphical Models
I-map. The additional edges ensure that any induced dependencies will be
taken into account, but they might not have been needed. A key result is that
if the graph G is an I-map of the probability model M, then M is Markov with
respect to G, i.e., variables that are separated in the graph are conditionally
independent, given the variables that separate them.
If G is both a D-map and and I-map, it is a perfect map. Perfect maps are
unfortunately rare (again, chains provide examples). However, I-maps and
D-maps always exist. For example, the complete graph (all components con-
nected) is a trivial I-map, and the disconnected graph (no edges) is a trivial
D-map. Minimal I-maps (maximal D-maps) capture as many of the indepen-
dence (dependence) conditions as possible (see Pearl 1988).
In sum, directed graphs are good for making maximal D-maps, and undi-
rected graphs are good for making minimal I-maps. One reason to transform
the directed graph to the undirected graph is that it is easier to read the con-
ditional independence relationships from the undirected graph. However, the
added moral edges represent dependencies which are only realized in certain
circumstances. For the purpose of eliciting distributions we prefer D-maps,
and hence prefer to work with the directed graphs. The computational algo-
rithms of Chap. 5 require I-maps, and hence we transform the graph to the
undirected representation for calculation.
4.3.3 Gibbs–Markov Equivalence Theorem
Which comes ﬁrst, the factorization or the conditional independence? We do
not need to think hard about this question because the two are equivalent
under fairly mild restrictions. This problem was addressed early in the world
of statistical physics under the name Gibbs–Markov equivalence. The term
Gibbs refers to the ability to factor the distribution into Gibbs potentials,
whose frames are given by the cliques of the graph. As noted above, the
term Markov means that variables which are separated in the graph are con-
ditionally independent. Moussouris (1974) provides suﬃcient conditions for
Gibbs–Markov equivalence in probabilistic models.
Theorem 4.1 (Gibbs–Markov Equivalence). Let G be a graphical model
for which the probability is strictly positive (there is no combination of input
variables which has zero probability). The model graph is an I-map of its graph-
ical model, or equivalently, a graphical model is Markov with respect to its
model graph. (For a proof, see Moussouris 1974)
In practice we usually use the Gibbs →Markov direction, i.e., given
the factorization we derive the conditional independence properties. Logical

4.4 Edge Directions and Causality
95
relationships1 have zero probabilities and cause the Markov →Gibbs direction
to break down. However, such cases usually do not cause a problem because
we start with the factorization and go to the conditional independence state-
ments.
Practical model building usually goes back and forth between the two
representations. If we have a factorization, we ask experts about conditional
independence implications to verify the factorization. If we have independence
conditions, we draw an a appropriate factorization and try to elicit factors.
Eventually, we exploit the Markov conditions to derive eﬃcient computational
techniques (Chap. 5).
4.4 Edge Directions and Causality
The directed graphical representation is often easier to use than the undirected
representation for building models. In the undirected graph, one must ensure
that all of the factors (associated with the cliques) make up a consistent
probability distribution. For the directed graph, if the graph is acyclic and
each factor is a consistent probability or conditional probability, then the
resulting distribution will be a proper joint probability.
When building such a graph the question, “what do the arrows mean,”
inevitably arises. The direction of an edge means the “direction of statistical
conditioning”—as a formal property, simply what is on the right and left of
the conditioning bar. Things become more interesting when we link graphs
and pictures to real-world relationships.
Edges can be drawn in either the “causal” (Fig. 4.11) or “diagnostic”
(Fig. 4.12) direction. Both are representations of the same joint distribution,
so formally they are equivalent.
Skill
Performance
Fig.
4.11
Directed
graph
running
in
the
“causal”
direction:
P(Skill)
P(Performance|Skill)
Reprinted from Almond et al. (2007) with permission from ETS.
We can use Bayes theorem to translate between the two representations.
This is called arc reversal in inﬂuence diagram literature (Howard and Math-
eson 1984). However, arc reversal sometimes results in a more complex graph.
For example, if we were to reverse the edge between Language Proﬁciency
1 For example, let A, B, and C be variables that take the values true and false.
Deﬁning P(C|A, B) to be true with probability 1 if both A and B are true and
false with probability 1 otherwise is a logical, rather than stochastic relationship.

96
4 Basic Graph Theory and Graphical Models
Skill
Performance
Fig. 4.12 Directed graph running in the “diagnostic” direction: P(Performance)
P(Skill|Performance)
Reprinted from Almond et al. (2007) with permission from ETS.
and Reading in Fig. 4.13, then we would wind up drawing an edge between
Reading and the other three variables as well.
Language Proficiency
Reading
Writing
Speaking
Listening
Fig. 4.13 A graph showing one level of breakdown in language skills
Reprinted with permission from ETS.
Graphs whose edges run in the “causal” direction are generally simpler
than graphs whose edges run in the “diagnostic” direction. That is because
relationships are understood as events or observations that are conditionally
independent, given underlying factors of a situation or prior events, perhaps
from a substantively grounded understanding of the situation at hand. Pearl
(1988) and others have used that to build a theory of causal modeling (see
Sect. 10.7). Following this tradition, many authors call Bayesian networks
causal models. The inﬂuence diagram school, on the other hand, tends to
avoid the word “causal” because it is often misinterpreted by the lay public
(Sect. 10.7).
Causality is not necessary for building graphical models. Weaker notions
like “tendency to cause,” “inﬂuence,” or “knowledge dependence” are suf-
ﬁcient. In Fig. 4.13 the meaning of the edges is a “part-of” relationship—
Reading is a part of Language Proﬁciency. Ultimately, the meaning of the
edges is knowledge dependence. An edge from A to B means that knowledge
about A aﬀects our beliefs about B.
This does not mean that if our domain experts have a well established
causal theory, we should throw it away. Rather, the causal theory can help
build eﬃcient models. In particular, cognitive theory about factors which go
into performance on an assessment task can be used to build a mathematical
model of that performance (Mislevy 1994). Psychometric models in general
posit latent variables that characterize aspects of students’ knowledge and

4.5 Other Representations
97
skill, which are modeled as parents of aspects of their behavior. This is as
true for cognitive diagnosis models as it is for more traditional psychometric
models such as classical test theory, item response theory, and factor analysis.
But we should not wait until we have a universally agreed upon causal model
before building a graph. Any theory of the domain should be enough to get
us started.
In evidence centered design , domain experts and assessment designers are
encouraged to draw preliminary versions of the future proﬁciency and evidence
models before settling down to built the ﬁnal statistical model of the test.
During this domain modeling phase, the edges in the graph are often labeled
with the type of relationship (e.g., prerequisite, part-of, induces, inhibits).
These labels are not used in the ﬁnal Bayes nets, but help the experts think
about how to model the conditional probability tables.
4.5 Other Representations
One of the principle advantages of using graphical models for educational
assessment is that they provide a useful representation for discussing math-
ematical models with domain experts who may be less familiar with mathe-
matical notation. However, graphical models are not the only system to use
graphs as a means of conveying mathematical models. This section discusses
several of these other representations and their relationships to graphical mod-
els. Inﬂuence diagrams (Sect. 4.5.1) are a speciﬁc generalization of graphical
models which add decision variables to the mix. Structural equation models
(Sect. 4.5.2) also use graphs to represent complex models, but with some nota-
tional diﬀerences which are important to point out. Section 4.5.3 brieﬂy lists
some other related models.
4.5.1 Inﬂuence Diagrams
Inﬂuence diagrams (Howard and Matheson 1984; Shachter 1986; Oliver and
Smith 1990) have had a strong inﬂuence on the modern development of graph-
ical models, especially the application in artiﬁcial intelligence. Inﬂuence dia-
grams diﬀer from Bayesian networks in that they use both probabilities and
utilities which represent preferences among outcomes. Inﬂuence diagrams also
use three classes of nodes, one to represent random variables, one to repre-
sent decisions (under the control of the decision maker), and one to represent
utilities. The “solution” to an inﬂuence diagram is a strategy for making the
decisions involved in the problem to maximize the expected utility.
Figure 4.14 shows a typical inﬂuence diagram.
•
Square boxes are decision variables. Arrows going into decision variables
represent information available at the time when the decision is to be
made.

98
4 Basic Graph Theory and Graphical Models
Select Test
Skill
Intervention
Test
Result
Skill at
Testing
Time
Skill at
End of
Course
Test Cost
Intervention
Cost
Utility of
Skill
Fig. 4.14 Inﬂuence diagram for skill training decision
Reprinted from (Almond 2007b) with permission from ETS.
•
Round boxes are chance nodes (random variables).
•
Hexagonal boxes are utilities. Costs are negative utilities.
The example in Fig. 4.14 brings up some interesting issues so it is worth
exploring in a little more detail.
Example 4.1 (Value of Testing). Suppose that we are trying to teach a
certain skill to a certain student. We have a utility associated with this student
knowing the skill at the end of the course. The student’s probability of knowing
the skill at the end of the course will depend on both the student’s skill
level at the beginning of the course and what kind of instruction the student
receives. The instruction has certain costs (both monetary and student’s time)
associated with it (as does no instruction, but we can scale our utilities so that
that cost is zero). We do not know the student’s ability at the beginning of
the course, but we can give the student a pretest whose outcome will depend
on the student’s ability. This pretest also has a cost associated with it. We can
observe the outcome of the pretest when we make the decision about what
instruction to give.
The decision of what instruction to give depends not only on whether or not
the student seems to have the skill from the pretest, but also the value of the
skill and the cost of the instruction. If the instruction is very expensive and
the skill not very valuable, it may not be cost eﬀective to give the instruction
even if we know the student does not have the skill. Similarly, the decision
about whether or not to test will depend on the cost of the test and the cost
of the instruction. If the instruction is very inexpensive (for example, asking
the student to read a short paper or pamphlet), it may be more cost eﬀective
to just give the instruction and not bother with the pretest.
This example brings up the important concept of value of information
(Matheson 1990). This will come up again, along with its close cousin weight
of evidence when we discuss explanation and task (item) selection in Chap. 7.

4.5 Other Representations
99
An inﬂuence diagram with all chance nodes is called a relevance diagram
and is a Bayesian network. This fact, along with the eﬃcient algorithms for
Bayesian networks (Chap. 5) has caused most of the current inﬂuence diagram
research to be cast in terms of Bayesian networks. If the number of decision
variables is low, then it is often eﬃcient to represent them as random variables
in a Bayesian network and simply assign them distributions with probability
1 for a particular choice.
4.5.2 Structural Equation Models
Before graphical models, Wright (1921) (also Wright 1934) used graphs to
represent statistical models in his pioneering development of path analysis.
This technique has been a strong inﬂuence on many of the early developers of
Bayesian networks (Pearl 1988). Path analysis has been popular in the social
sciences because the pictorial representation of the model is often easier to
use than mathematical notation.
Bayesian networks are not the only models using a graphical representa-
tion to descend from path analysis. Structural equation models (Bollen 1989;
Joreskog and Sorbom 1979; Kaplan 2000), or SEMs, have been quite popular
in psychological and educational testing applications. Although they concen-
trate on modeling associations in populations rather than behavior of a single
individual, there are many similarities.
This book will not cover structural equation models, as they are covered
by many other authors. But it will be helpful to notice a few key diﬀerences
between structural equation models and Bayesian networks.
1. SEMs most often work with continuous variables which are assumed to
have a multivariate normal distribution, and Bayesian networks most often
use discrete variables that have a multinomial distribution. There are
exceptions on each side (see Whittaker 1990; Lauritzen 1996), but this
rule holds for many applications.
2. SEMs model the covariance matrix, while graphical models model the
inverse of the covariance matrix (Whittaker 1990). Zeros in the covariance
matrix imply marginal independence, while zeros in the inverse covariance
matrix imply conditional independence. This drives the next diﬀerence.
3. SEMs and Bayesian networks use slightly diﬀerent graphical notations.
Some of these are obvious: SEMs allow bidirectional or undirected edges to
model correlations, while all edges in Bayesian networks must be directed
and the directed graph must be acyclic. Perhaps more subtle is what a
missing edge means. “The missing links in those statistical models [graph-
ical models] represent conditional independencies, whereas the missing
links in causal diagrams [SEMs] represent absence of causal connections
. . . that may or may not imply conditional independencies in the distri-
bution” (Pearl 1998, p. 237).

100
4 Basic Graph Theory and Graphical Models
4. SEMs frequently model error terms as nodes in the graph, while in
Bayesian networks, they are often implicit in the distribution locked in
the edges.
5. In practice, SEM modeling usually focuses on modeling the distribution of
a population, while Bayesian network modeling often focuses on calculat-
ing probabilities for an individual (see Part I). However, when estimating
the parameters of the Bayesian network, the population distribution must
be considered (Part II).
In short, SEMs and Bayesian networks are both rich notations for describ-
ing complex multivariate distributions and using graphs to visualize the rela-
tionships. However, the rules are slightly diﬀerent, so not all SEMs and
Bayesian networks are equivalent (Pearl 1988, notes that the ones that are
equivalent are called recursive models in the SEM literature). So, when
faced with a graph, it is important to know which representation is implied.
Anderson and Vastag (2004) provide a side-by-side comparison of SEMs and
Bayesian networks.
4.5.3 Other Graphical Models
Although Bayesian networks are one of the most frequently used graphical
models, there exist other kinds of graphical models as well. Generally, a
graphical models is a representation of a probability distribution that factors
according to a graph. So a set of rules for associating graphs (or hypergraphs)
of various types with factorizations, and conditional independence conditions
produces a new kind of graphical model. This section provide a few pointers
into the rich literature on this topic.
The term graphical model comes from Darroch et al. (1980) where it is
used for modeling contingency tables. According to the deﬁnition, a model is
graphical if for every set of variables for which all two-way interactions are
included in the model (this would be a clique in the graph), all higher order
interactions are included as well. The implication of this deﬁnition is that the
joint probability distribution factors according to the cliques of the graph.
These models are particularly convenient computationally, and usually quite
easy to interpret.
The restriction to discrete variables is convenient because the integral
which is the denominator of Bayes’ rule turns into a sum. If that restric-
tion is lifted, then the denominator becomes, in some cases, an integral that
usually cannot be solved analytically. One exception is if all of the variables
are normal. In this framework, directed edges behave like regression models.
Whittaker (1990) provides a general reference for both discrete and multivari-
ate normal graphical models, as well as some cases of mixed models.
Edwards (1990) describes one class of mixed graphical model which is con-
venient to work with under the name hierarchical interaction model. Edwards
(1995) describes a system for ﬁtting these models to data, realized in the

4.5 Other Representations
101
software package MIM. Lauritzen (1996) describes a general case of condi-
tional Gaussian models in which normal variables are allowed to be children
of discrete parents, but discrete variables are not allowed to have continuous
parents (Contrast this to item response theory (IRT) models in which the
continuous latent trait, θ, is a parent of discrete observable outcome vari-
ables). Lauritzen (1996) shows how conditional Gaussian models support the
algorithms of Chap. 5.
Cox and Wermuth (1996) describe an extension of these ideas called chain
graph models. Chain graphs use a mixture of directed and undirected edges.
The directed edges represent conditional relationships, i.e., the distribution
of the child is given conditioned on the parent. The undirected edges repre-
sent correlational relationships, i.e., the variables in question are given a joint
distribution given common parents.
To unify very similar work in Bayesian networks, inﬂuence diagrams,
discrete dynamic programming, and graphical belief functions, Shenoy and
Shafer (1990) developed a general framework they called valuation-based sys-
tems. Valuation-based systems associate quantitative information with rela-
tionships among variables. The probability potential introduced above is an
example of a valuation, as are utilities in inﬂuence diagrams. To support this
framework, valuations need to support a couple of operations and properties.
First, there needs to be some notion of conditional independence related to the
factorization of the model. Second, the valuations must support a combination
and a projection (changing the frame for the valuation) operation. Finally, it
must be possible at least under some circumstances to interchange the com-
bination and projection operators. Given these conditions the algorithms of
Chap. 5 can be used to solve problems.
Some examples of valuation based systems include: Bayesian Networks
(Shenoy and Shafer 1990; Almond 1995), Inﬂuence diagrams (Shenoy 1991),
Discrete Dynamic Programming (Bertel`e and Brioschi 1972), Graphical Belief
Functions (Shenoy and Shafer 1990; Almond 1995), and Mixed Graphical Mod-
els (Cobb and Shenoy 2005). This last paper shows that if the distribution of
the continuous variables can be described through a mixture of exponential
distributions, then the Shenoy and Shafer (1990) algorithm can be used to get
exact solutions. Mixtures of exponentials can often provide good approxima-
tions to complex functional forms.
The computation schemes of Chap. 5 rely on the conditional independence
properties of Bayesian networks (or, by extension, the other graphical models
described here). In particular, Bayesian networks are suitable for the purpose
of gathering data, drawing inferences, and making decisions about individuals
as data arrive for each of them. This is the reason for our choice of Bayesian
networks as the basis of our models for educational testing.

102
4 Basic Graph Theory and Graphical Models
Exercises
4.1. The graph for the item response theory (IRT) model in Chap. 3 (Fig. 3.3)
has all of the arrows pointing from θ to the observable item outcomes, Xi.
Why did we choose to draw the arrows in that direction?
A
B
C
D
E
F
G
H
Fig. 4.15 Graph for use in Exercise 4.2
Reprinted from Almond et al. (2007) with permission from ETS.
4.2. Refer to Fig. 4.15. In each of the following scenarios state whether the
variables associated with nodes A and C are independent. In each case, if
they are not independent, indicate a set of additional variables such that
conditioning on these variables would render A and C independent again.
a. No variable values have been observed.
b. Values for the variables F and H are observed, but no other variables are
known.
c. A value for the variable G has been observed, but all other variables are
unknown.
4.3. In developing an assessment for algebraic sequences, the domain experts
identiﬁed four proﬁciency variables: overall sequence proﬁciency, arithmetic
sequence proﬁciency (sequences like 2, 4, 6, 8), geometric sequence proﬁciency
(sequences like 2, 4, 8, 16), and other recursive proﬁciency (sequences that do
not follow arithmetic or geometric rules, like the Fibonacci sequence). Accord-
ing to the experts the last three proﬁciencies are “part of” overall proﬁciency.
What direction should the edges representing this relationship be drawn?
Why? Are other edges needed between the remaining variables? What ques-
tion could be asked of the experts to see if additional edges are needed?

4.5 Other Representations
103
4.4. Start with the language proﬁciency model on Fig. 4.13. Now add nodes
representing the scored outcomes from the following tasks:
a. A pure Reading Task.
b. A pure Listening Task.
c. A task which requires a written response based on textual stimulus which
the candidate must read.
d. A task which requires a spoken response based on a audio stimulus which
the candidate must listen to, with written instructions.
e. A task which requires a candidate to write a transcript of an audio stim-
ulus, with spoken instructions.
Connect the new outcome variables nodes to the proﬁciency variables
nodes already in the graph. Now connect the parents of each outcome variable
to form the moral graph. What is the size of the largest clique? What happens
to the conditional independence of the skills? (Mislevy 1995c).
4.5. Suppose we have a proﬁciency model consisting of an overall proﬁciency
skill and several subskills. The experts tell us that we can model the subskills
as conditionally independent given the parent. Suppose further that our test
consists of a collection of items which tap each pair of the subskills, so that the
moral graph for the proﬁciency model will be saturated (there will be an edge
between every pair of variables). Given that the moral graph will be saturated,
why is it still better to have the arrows go from the overall proﬁciency to the
subskills? Hint: Assume all variables have four levels. The expert must then
specify three probability values per combination of parent states (because the
four probabilities in each conditional distribution must sum to 1). Thus, if a
node has two parents, the expert must specify 3 × (4 × 4) probability values.
4.6. Consider a dance performance competition in which there are three per-
formers and three judges. Draw a Bayesian network to represent this structure
in each of the following scenarios:
a. Each dancer gives a single performance which receives a single rating
agreed upon by all three judges.
b. Each dancer gives a single performance and receives a separate perfor-
mance from each judge.
c. Each dancer gives three performances, each performance is rated by a
diﬀerent judge.
d. Each dancer gives three performances, each performance is rated by all
three judges.
For the models with multiple performances, should there be arrows between
the performances? For the models with multiple ratings, should there be
arrows between their ratings? Justify your answers.

5
Eﬃcient Calculations
The previous chapters have laid a foundation for building probability mod-
els and embedding them in a graphical structure. The graphical structure
reﬂects our knowledge bout interrelationships among the variables. Once we
have expressed all of the interrelationships in terms of a joint probability dis-
tribution, it is always possible in principle to calculate the eﬀect of new infor-
mation about any subset of the variables on our beliefs about the remaining
variables (i.e., to propagate the evidence).
For even relatively small numbers of variables, however, the cost of updat-
ing the full joint distribution using the deﬁnitional expression of Bayes theo-
rem becomes prohibitive. A model with 15 variables with four values each
already means working with a joint probability table with over a trillion
entries. We have intimated that when the topology of the graph is favor-
able, we will be able to carry out calculations more eﬃciently. Mathemat-
ically speaking, topologies that are favorable for computing are those that
have small cliques, that is, low treewidth. Using the algorithms described in
this chapter, the cost of the computation only grows linearly in the total num-
ber of variables, but grows exponentially with the size of the largest clique.
More informally, if we have structured our understanding of the domain so
that the important interactions take place among small subsets of variables,
then we can exploit that structure to create an eﬃcient calculation algorithm.
This chapter introduces eﬃcient calculation in networks of discrete vari-
ables. The objective is to ground intuition with a simpliﬁed version of a basic
junction-tree algorithm, illustrated in detail with a small numerical example.
More complete descriptions of this so-called fusion and propagation algorithm
are available in Jensen (1996), Cowell et al. (1999), and Neapolitan (2004).
Practically, such calculations are done with computer programs rather than
by hand, and Appendix A describes how to get several commercial and free
research programs that support these calculations. The basic belief-updating
algorithm presented here is just one of a large number of variants on the
message-passing algorithm described in Pearl (1988); Sect. 5.6 describes some
of them.
c⃝Springer Science+Business Media New York 2015
105
R. G. Almond et al., Bayesian Networks in Educational Assessment,
Statistics for Social and Behavioral Sciences, DOI 10.1007/978-1-4939-2125-6 5

106
5 Eﬃcient Calculations
Section 5.1 begins by reexamining probability updating in a simple two-
variable model, which corresponds to a graph with one variable Y depending
on a parent variable X. However, this simple example deﬁnes two operations,
combination and marginalization, which are key to the belief-updating algo-
rithm for more general graphs. Section 5.2 describes how the belief-updating
algorithm works in undirected graphs that have no loops, such as chains and
trees. This section adds the message operation. Section 5.3 notes that this
method of propagation breaks down when a graph contains a cycle, or mul-
tiple paths from one variable to another. It then describes how the method
can be generalized to propagating at the level of cliques of variables in what
is called a junction tree or join tree, rather than at the level of individual
variables. The construction of a junction tree and propagation of evidence
with a junction tree are illustrated with a simple example of how a junc-
tion tree is constructed. Section 5.4 discusses implications of this approach
for assessment, including the idea of distinguishing fragments of Bayes nets
that correspond to proﬁciency models and evidence models. These pieces can
be assembled dynamically to absorb evidence about students’ knowledge in
applications such as computerized adaptive testing. Section 5.5 presents an
alternative representation for the structure of an assessment, the Q-Matrix.
Section 5.6 brieﬂy surveys alternative updating techniques, both exact and
approximate, for use in situations when the algorithm of Section 5.3 is not
viable.
5.1 Belief Updating with Two Variables
In its deﬁnitional form, Bayes theorem (Theorem 3.1) can be applied in prin-
ciple to any number of variables. However, the calculations soon become
intractable. The ﬁrst steps toward eﬃcient computation in large networks
of variables were developed for special structures such as chains and trees, in
which globally coherent updating could be accomplished by working through
the structure two variables at a time.
It is easy to understand probability updating and Bayes theorem in terms
of deﬁnitional, or what might be called “brute force,” calculation. In discrete
Bayes nets, one enumerates the probabilities of all the possible combinations
of values of all the variables, reduces the set to reﬂect news about the values
of some variables, and carries out some simple arithmetic to calculate the
probabilities of all the remaining possibilities. This is easy to understand and
easy to demonstrate, as we will do in this section—as long as there are not
too many variables and not too many combinations of values. But the basic
steps in the brute force calculation are the building blocks of more eﬃcient
calculation in more complex networks.
Consider the case of just two variables, X and Y , and suppose that our
initial knowledge consists of marginal probabilities p(x) for the values of X

5.1 Belief Updating with Two Variables
107
and conditional probabilities p(y|x) for values of Y given values of X. The
graph takes the now-familiar form shown as Fig. 5.1.
X
Y
Fig. 5.1 Graph for the distribution of X and Y
Reprinted from with permission from ETS.
As discussed in Chap. 4, this directed representation of the relationship
between the two variables supports updating our belief about either X or
Y given the values of the other. If we learn the value of X, we update belief
about Y directly via the conditional probability distribution p(y|x). If we learn
the value of Y , we update belief about X via Bayes theorem. The following
example begins by reviewing Bayesian updating in the context of a numerical
illustration.
We will then recast the problem in terms of operations on potentials on
this (very simple!) graphical structure. In general, we call a multiway array
of numbers where each dimension corresponds to a variable in our model,
a potential. Potential tables are probability distributions for which we have
relaxed the normalization constraint. That is, the numbers are nonnegative,
and they are in the right proportions to reﬂect relative probabilities of out-
comes and events. A potential could represent a probability distribution, a
product of a set of conditional probability distributions or another intermedi-
ate quantity in a probability calculation. If we want to interpret a potential
as a probability, we often must normalize it.
Example 5.1 (Dental Hygienist Assessment). Let X represent a dental
hygiene student’s proﬁciency in examining patients, and Y represent whether
a student takes an adequate patient history in a particular simulation task.
The single proﬁciency variable, X, can take two values, x1 = expert and x2 =
novice, and the observable variable, Y, can also take two values, y1 = yes and
y2 = no. In this assessment the work product is the examinee’s sequence of
actions in taking a patient history. Assume we can determine unambiguously
whether a given sequence is adequate or inadequate.
Suppose that it is equally likely that a student is an expert or a novice,
so p(expert) = p(novice) = .5. Suppose further that an expert’s probability
of taking an adequate history in such tasks is .8, and a novice’s is .4. Thus
p (y1 | x1) = .8 and p (y2 | x1) = .2, and p (y1 | x2) = .4 and p (y2 | x2) = .6.
In an operational assessment we would want to observe an examinee’s per-
formance, evaluate its adequacy, and update our belief about the examinee’s
expert/novice status. We observe Jay take an adequate history in the task;
that is, Y = y1 = yes. Bayes theorem updates our initial beliefs of (.50,.50)

108
5 Eﬃcient Calculations
probabilities for expert and novice as follows:
p (x1 | y1) =
p (y1 | x1) p (x1)
p (y1 | x1) p (x1) + p (y1 | x2) p (x2) =
.8 × .5
.8 × .5 + .4 × .5 = .67
and p (x2 | y1) = 1 −p (x1 | y1) = .33.
Now let us see how this example can be expressed in terms of potential
tables and operations on them. We will ﬁrst work through the symbolic rep-
resentation, then show the corresponding numbers from Example 5.1.
There is one clique in this undirected graph, {X, Y }. From the information
about p(x) and p(y|x), we can construct a two-way table of the joint proba-
bilities p(x, y) for all possible (x, y) pairs. The results are shown as the top
panel in Table 5.1. This is a potential table for p(x, y), which at this point
conveys the initial beliefs. The margins for X and Y are shown at either side
of the joint distribution; both are consistent with the joint probabilities in the
center.
We now learn that Y = y1. Because Y and X are dependent, the value
we learned for Y provides evidence for X. This evidence can be expressed
as the vector [1, 0]. In order to combine this new belief about Y with our
initial belief about the joint distribution of X and Y , we ﬁrst replicate the
information about Y into a potential table for X and Y . This is shown in the
second panel of Table 5.1.
To instantiate the observed value of Y , multiply each cell in the initial
table for X and Y by the contents of the corresponding cell for the new
evidence. The result is the third panel in Table 5.1. The states that now
have zero probability (i.e., are impossible) are colored gray. Note that the
contents of the table are no longer a proper probability distribution as the
values do not sum to one, but rather sum to p(y1). Interpreting the values
as a probability requires normalizing the values in the table by dividing by
the sum of the elements. This yields the ﬁnal panel of Table 5.1, a potential
table that represents the conditional probability distribution for X given that
Y = y1.
Example 5.2 (Dental Hygienist Assessment, Example 5.1, Contin-
ued). Table 5.2 gives the numbers for the Dental Hygienist example that
correspond to the symbolic representation of the potentials in Table 5.1. We
see that the operations on the potentials reﬂect the calculations we carried
out in the deﬁnitional application of Bayes theorem. Learning that a student
takes an adequate history means focusing attention on the yes column. The
ratio of the values is 2:1, and normalizing, we ﬁnd that we have updated our
initial (.50,.50) probabilities for expert and novice to (.67, .33).

5.1 Belief Updating with Two Variables
109
Table 5.1 Updating probabilities in response to learning Y = y1
y1
y2
P(X)
x1
p(y1|x1)p(x1)
= p(x1, y1)
p(y2|x1)p(x1)
= p(x1, y2)
p(x1)
x2
p(y1|x2)p(x2)
= p(x2, y1)
p(y2|x2)p(x2)
= p(x2, y2)
p(x2)
P(Y )
p(y1)
p(y2)
1
a) Table for initial joint and marginal probabilities.
y1 y2
x1 1 0
x2 1 0
b) Potential table representing evidence, Y = y1.
y1
y2
P(X)
x1
p (x1, y1) × 1
= p (x1, y1)
p (x2, y1) × 0
= 0
p(x1, y1)
x2
p (x2, y1) × 1
= p (x2, y1)
p (x2, y2) × 0
= 0
p(x2, y1)
P(Y )
p(y1)
0
p(y1)
c) Combine initial probabilities with evidence.
y1
y2
P(X)
x1
p(x1, y1)/p(y1)
= p(x1|y1)
0
p(x1|y1)
x2
p(x2, y1)/p(y1)
= p(x2|y1)
0
p(x2|y1)
P(Y )
1
0
1
d) Normalize by dividing by the grand total, p(y1).
This example introduced most of the key operations on potentials we will
need to update Bayes nets in light of new evidence. They are projecting a
potential for one set of variables into a potential for an overlapping set of

110
5 Eﬃcient Calculations
Table 5.2 Numerical example of updating probabilities
yes
no
P(X)
expert .8 × .5 = .40 .2 × .5 = .10
.5
novice .4 × .5 = .20 .6 × .5 = .30
.5
P(Y )
.6
.4
1
a) Table for initial joint and marginal probabilities.
y1 y2
x1 1 0
x2 1 0
b) Potential table representing evidence, Y = y1.
yes
no
P(X)
expert .4 × 1 = .4 .1 × 0 = 0
.4
novice .2 × 1 = .2 .3 × 0 = 0
.2
P(Y )
.6
0
.6
c) Combine initial probabilities with evidence.
yes
no
P(X)
expert .4/.6 = .67
0
.67
novice .2/.6 = .33
0
.33
P(Y ) .6/.6 = 1
0
1
d) Normalize by dividing by the grand total, .6.
variables (both up to a larger set and down to a subset), and combining two
potentials over the same set of variables. (All we still need is an operation to
pass messages from one potential to another to update beliefs, which we will
get in Sect. 5.2.)
To go from a bigger space (like {X, Y }) to a smaller one (like {X}), sum
across the values of the unused variables. The resulting distributions are often
written in the margins of the joint table and hence they are called marginal
distributions. We already ran into the marginal distribution when talking
about joint probability distributions in Chap. 3. The process of calculating
the marginal distribution from the joint distribution is called marginalization.
We use the symbol ⇓x to denote marginalizing over the variable X, and deﬁne
it as follows:
p (x, y) ⇓
x =

y
p (x, y) = p(x) ,
(5.1)
where the sum is taken over all possible values Y .

5.2 More Eﬃcient Procedures for Chains and Trees
111
To go from a smaller space (like {Y }) to a larger one (like {X, Y }), simply
replicate the distribution over new dimensions. We did this in the example
because as the potential representing Y = y1, namely [1, 0], is a potential
deﬁned only over the variable Y . We just replicated it over X to get the
second panel in Table 5.2.
To combine the information in two potentials over the same variables,
multiply them together element by element. This is how we got the third panel
in Table 5.2, as we combined the potential representing the initial distribution
with the potential representing Y = y1. Note that the combination operation
was also used in constructing the initial table. This was the combination of
the potentials representing p(x) and p(y|x). The potential over p(x) needed
to be projected onto the larger space {X, Y } before the combination could
occur. The symbol ⊗is used to denote combination of potentials.
Finally, to interpret the potential as a probability, normalize the potential
by dividing by the sum of the elements. This was done as the last step of the
calculation. This calculation is often done last because (a) normalization is
only needed to interpret the results, and (b) delaying normalization as long
as possible increases the numerical stability of the results.
In principle, belief updating and marginalization as done in this section can
be carried out with an arbitrarily large number of discrete variables. All the
calculations are done with a large potential table over all the variables in the
problem. While the procedure is straightforward conceptually, the problem is
feasibility. Even with only ten dichotomous variables, there is a table of size
210 to work with. The cost grows exponentially with the number of variables in
the table associated with the largest clique; increasing the number of variables
beyond six or seven (or even fewer if each variable has many states) makes
computation impractical. The remainder of this chapter discusses a strategy
that exploits the conditional independence conditions in the model graph to
ensure all computations happen over tables of a manageable size.
5.2 More Eﬃcient Procedures for Chains and Trees
Kim and Pearl (1983) introduce an updating algorithm for a chain of variables,
in which computation only grows linearly in the number of variables. Almost
all of the various algorithms for performing calculations in Bayesian networks
are variations on the basic Kim and Pearl approach. The variation presented
here is based on the junction-tree algorithm of Cowell and Dawid (1992).
Section 5.2.1 presents the basic algorithm on a very undirected graph, a
chain of variables. Section 5.2.2 extends the algorithm to polytrees, which
are basically directed graphical structures whose undirected graphs would be
trees. Finally, Sect. 5.2.3 talks about handling evidence that is not certain;
this will have some interesting applications later in the chapter. The simple
approach described in this section can be extended to more complex graphical
structures. Section 5.3 will take up the case of more complex models.

112
5 Eﬃcient Calculations
5.2.1 Propagation in Chains
A chain is a set of variables whose joint distribution takes a form like this:
P (W, X, Y, Z) = P(Z|Y ) × P(Y |X) × P(X|W) × P(W) .
(5.2)
That is, each variable except for the ﬁrst depends directly on only the one
variable preceding it. The acyclic digraph for such is system is also a chain in
the graph-theory sense, as shown at the left of Fig. 5.2.
W
X
Y
Z
p(w)
p(x) 
p(y)
p(z)
p(w,x)
p(x,y)
p(y,z)
p(w)
p(x) 
p(y)
p(z)
p(w,x)
p(x,y)
p(y,z)
Fig. 5.2 The acyclic digraph and junction tree for a four-variable chain, correspond-
ing to Eq. 5.2. In the junction tree (in the middle right) the square boxes correspond
to cliques in the digraph (on the left). The round boxes correspond to intersections
between cliques. In the ﬁnal version on the far right, the two “intersection” nodes
that only connect to a single node are dropped
Reprinted with permission from ETS.
Moving from the right to the left in Fig. 5.2 is a transformation of the
original graph called a junction tree. This junction tree has several notable
properties. First, its structure contains nodes for both variables themselves
and pairs of adjacent variables. The nodes for the pairs are where interrela-
tionships among variables that directly inﬂuence one another are manipulated.
These are called clique nodes and they correspond to the cliques in the digraph
(more speciﬁcally, as we shall see below, the cliques of the undirected graph
corresponding to the digraph). The nodes for individual variables are inter-
mediate areas where information common to adjacent cliques, necessary for
updating from one clique to the next, are stored. These are called intersec-
tion nodes. The junction tree in the middle contains two “intersection” nodes,
p(w) and p(z) which do not join two clique nodes. As these are not needed for
computation, they are commonly dropped (far right in the ﬁgure). Section 5.3
describes the properties of junction trees in more detail.

5.2 More Eﬃcient Procedures for Chains and Trees
113
In each node of the junction tree we will store a potential table deﬁned
over the variables in the node. We would like this table to be proportional
to the joint probability distribution over the variables in the node. There are
several ways to initialize the values in a junction tree. In the case of the chain,
the easiest way is to follow the recursive deﬁnition of the joint distribution,
Eq. 5.2.
Start with the clique node {W, X}. From Eq. 5.2 the joint distribution
P(W, X) = P(X|W)P(W). Construct potentials corresponding to P(X|W)
and P(W) and combine them to create a potential corresponding to P(X, W).
Store this potential in the node {W, X}. Marginalize out the variable W to get
the potential P(X) and put that in the corresponding intersection node. We
will call the potential stored in the Node {W, X}, p(w, x), and the potential
stored in the Node {X}, p(x).
The next clique node is {Y, X}; it has the joint distribution P(X, Y ) =
P(Y |X)P(X). The ﬁrst term was speciﬁed in Eq. 5.2, the second term was
the value we just calculated and stored in the intersection node {X}. This
node is conveniently connected to the new clique node making it easy to ﬁnd.
Combining these two potentials, we calculate the potential for the node {Y, X}
and marginalize it to get the potential for the intersection node {Y }. A similar
procedure produces the initial potential for the last clique node {Z, Y }.
Even though the largest table contains only two variables, the junction tree
contains all the information necessary to reconstruct the full joint distribution
of all four variables. The conditional independence relationships that allowed
us to simplify the joint distribution in Eq. 5.2 also allow us to work with
smaller tables, rather than one large table for all possible combinations of all
variables.
Updating the distribution in response to new information can be carried
out with a generalization of the updating approach described in Sect 5.1. The
remainder of this section shows how to do this ﬁrst in symbols, then with a
numerical example.
The initialization process stored a potential with each node of the junction
tree given in the right side of Fig. 5.2. Call these potentials pold(w, x), pold(x),
pold(x, y), pold(y) and pold(y, z). Now suppose that evidence, e1, arrives about
X. We represent this new information as a potential over {X}, and denote it
φ(x). We now enter this into the system in the following steps:
1. Pick any clique node containing {X}, and update the potential in that
node by combining it with φ(x). We will use {X, Y } because this choice
allows us to illustrate updating both up and down the chain. Call the
new potential in that node pnew(x, y) = pold(x, y) ⊗φ(x) and note that it
represents P(X, Y |e1). This is done just as in Sect. 5.1. At this point, our
potential for {X, Y } correctly reﬂects our new belief about X, as obtained
directly in the form of e1, and about Y , as obtained by projection and
combining into the {X, Y } potential. However the nodes for all of the

114
5 Eﬃcient Calculations
other cliques and clique intersections still contain the initial beliefs and
are inconsistent with the new state of information.
2. Next, update the potentials in the neighboring clique nodes by passing
messages from clique node to clique node. Note that the clique nodes
are connected through intersection nodes, and hence the messages will
be passed through intersection nodes. We will move down the chain ﬁrst,
from {X, Y } to {Y, Z}. Denote the message from {X, Y } to {Y, Z} as
“{X, Y } ⇒{Y, Z}.” It takes the form of a potential over the clique inter-
section {Y }. It is calculated as follows. Call the potential over {Y } already
in the intersection node pold(y). Then calculate pnew(y) = pnew(x, y) ⇓y ,
the marginal distribution over {Y } of the new potential at {X, }. The mes-
sage sent to the {Y, Z} node will be ratio of the new intersection potential
divided by the old:
{X, Y } ⇒{Y, Z} = pnew(y) ⊘pold(y) ,
(5.3)
where ⊘is element by element division of potentials.
3. Adjust the potential in the node receiving the message by combining the
message with the potential currently in that node (after suitably extending
it to the set of variables in the node being updated):
pnew(y, z) = pold(y, z) ⊗(pnew(y) ⊘pold(y)) .
(5.4)
This essentially scales the potential in the node {Y, Z} by the amount the
information about {Y } changed because of the added evidence.
4. Now, pass messages up the chain from the Node {X, Y } where the new
evidence was entered. In this case, calculate the message {X, Y } ⇒{W, X}
in the same manner as Eq. 5.3:
{X, Y } ⇒{W, X} =

pnew (x, y) ⇓
x ⊘pold (x)

= pnew (x) ⊘pold (x)
.
5. Calculate the new value for Node {W, X} by combining the potential
already in the node by the just-received message as in Eq. 5.4:
pnew(w, x) = pold(w, x) ⊗(pnew(x) ⊘pold(x)) .
6. If either Node {W, X} or {Y, Z} had additional neighbors then the process
would be repeated. In for each neighbor, an analogue to Eq. 5.3 would be
used to calculate the eﬀect of the evidence on the variables in the intersec-
tion node. That would be combined with the information already in the
clique node using an analogue of Eq. 5.4. This process would be repeated
until all nodes in the junction tree have received a message containing the
information from the newly entered evidence.
If the evidence arrived about the Variable W, then the procedure would
have started with the node {W, X} and the messages ﬂowed down the chain.

5.2 More Eﬃcient Procedures for Chains and Trees
115
Evidence about Z is entered in {Y, Z} and updating ﬂows up the chain. In all
cases, other than the obvious changes of variable, the procedure is the same.
As with X, there are two choices of where to enter evidence about Y . The
messages ﬂow outward from there to the edges of the junction tree, and the
same result is obtained with either choice.
After this procedure, the potentials in the tree now represent the joint
distribution P(W, X, Y, Z|e1). If additional evidence, e2 were to arrive about
another variable, the same procedure would be applied a second time, now
using the current potentials as pold(·). Following Pearl (1988) we will call this
the belief-updating algorithm, although the algorithm given here is a variant
of Pearl’s algorithm. This variant is sometimes called the Hugin algorithm
because of its use in the software package HUGIN (Appendix A). A numerical
illustration of this algorithm is given in Example 5.8.
Note that it is not necessary that the values in the potential tables be
normalized to sum to one for the message passing; it is only necessary that
they reﬂect the correct proportions in the ﬁnal joint probability. Normalizing
is necessary only when one wants to interpret the marginal distribution for one
or more variables. Delaying the normalization until just before the results are
interpreted improves both the speed and numerical stability of the algorithm.
Actually, the normalization constant may be of interest in its own right.
Recall that in Table 5.1 the normalization constant was p(y1), that is the
probability of the evidence. This holds with more complex patterns of evidence
as well. In particular, the normalization constant is the probability (likelihood)
of the observed pattern of evidence. This is useful when evaluating how well
the data ﬁt the model (see Chap. 10 for more about this).
Example 5.3 (Updating in a Chain). Let the variables W, X, Y , and
Z in Fig. 5.2 all be dichotomous random variables deﬁned on {0, 1}. Let
P (W = 1) = .6, and
P (X = 1 | W = 1) = P (Y = 1 | X = 1) = P (Z = 1 | Y = 1) = .9
P (X = 1 | W = 0) = P (Y = 1 | X = 0) = P (Z = 1 | Y = 0) = .2.
This information produces the initial potentials in the junction tree shown
in Table 5.3a. Now suppose we learn that X = 1; thus, φ(x) = [1, 0]. This
evidence is entered into the potential pold (x, y) to produce pnew (x, y), as
shown in Table 5.3b. At this point, we have updated the potential for {X, Y },
but the other potentials remain at their initial values and inconsistent with
our new beliefs as shown in Table 5.3c.
We obtain pnew(y) as pnew(x, y) ⇓y , namely [.558, .062]. From the initial
status of the junction tree, pold(y) = [.634, .366]. The message to be passed
from {X, Y } to {Y, Z} is thus calculated as in Table 5.3d. This message is
interpreted as a signal to shift belief about Y in the ratio .880/.185, and
whatever this implies for the other variables in the receiving clique (in this
case, just Z) through their association with Y , as executed in Table 5.3e.

116
5 Eﬃcient Calculations
Table 5.3f shows the potentials after e1 has been propagated down the chain
but not yet up the chain.
Propagating evidence up the chain from {X, Y } requires calculating the
message {X, Y } ⇒{W, X}, which again takes the form of a potential over
the clique intersection, here {X}. The message is calculated in Table 5.4b.
Table 5.4c uses it to update the potential for {W, X}. Table 5.4d shows the
potentials after e1 has been propagated both up and down the chain. At this
point, the junction tree is ready to receive and propagate evidence in the
form of values for W, Y , or Z (for example, e2 that Y = 0; see Exercise 5.4).
Normalizing the potentials in every node gives Table 5.4e.
5.2.2 Propagation in Trees
The approach of updating belief in chains can be easily generalized for updat-
ing belief about a set of variables when the undirected graph for their relation-
ships is a tree. A tree is a singly connected graph—there is never more than
one path from any variable to any other. A chain is a particularly simple tree.
(In Sect. 5.3.2, we will address the general question of moving from an acyclic
digraph, which represents a probability distribution as a directed graph, to an
undirected graph that will serve as the vehicle for updating belief. As we saw
in Chap. 4, an acyclic digraph that is singly connected does not necessarily
give rise to a singly connected undirected graph for computing purposes.)
A singly connected undirected graph supports a junction tree representa-
tion for updating, similar to the one the chain depicted in Fig. 5.2. An example
is shown in Fig. 5.3, for a joint distribution that factors as follows:
P (U, V, X, Y, Z) = P (Z | X) P (Y | X) P (X | V ) P (U | V ) P (V ) .
Suppose new information arrived about X, in the form of pnew(x). This
information would be propagated through the rest of the network in the
update-and-marginalize steps in Sect. 5.2.1, but now in three directions: Down
and left to Y , down and right to Z, and up to V , and from V to U in the
same manner.
When Kim and Pearl (1983) ﬁrst deﬁned the belief-updating algorithm,
they restricted it to a kind of graph called a polytree. A polytree is basically
a directed graph that is a tree after the direction of the edges is dropped.
Figure 5.4 provides an example. Figure 5.4 gives the junction tree for this
graph. Note that two of the clique nodes, {T, W, U} and {W, X, Y }, have
size three, so this graph has a treewidth of three. The updating algorithm
described for chains in Sect. 5.2.1 works in essentially the same way. All the
clique intersections are still single variables, but the projections into cliques
and marginalizations down from them now sometimes involve more than two
variables.

5.2 More Eﬃcient Procedures for Chains and Trees
117
Table 5.3 Updating probabilities down a chain
pold (w, x)
pold (x)
pold (x, y)
pold (y)
pold (y, z)
W
X
Y
X
1
0
X
Y
1
0
Y
Z
1
0
1 .540 .080
1
.620
1 .558 .076
1
.634
1 .571 .073
0 .060 .320
0
.380
0 .062 .304
0
.366
0 .063 .293
a) Potential tables for initial joint and marginal probabilities.
pnew (x, y) = pold (x, y) ⊗φ (x) = .558 .076
.062 .304 ⊗1 0
1 0 = .558 0
.062 0 .
b) Evidence X = 1 entered into pold (x, y) to produce pnew (x, y).
pold (w, x)
pold (x)
pnew (x, y)
pold (y)
pold (y, z)
W
X
Y
X
1
0
X
Y
1
0
Y
Z
1
0
1 .540 .080
1
.620
1 .558 0
1
.634
1 .571 .073
0 .060 .320
0
.380
0 .062 0
0
.366
0 .063 .293
c) Potential tables after having updated only {X, Y }.
{X, Y } ⇒{Y, Z} = pnew (y) ⊘pold (y) = .558
.062 ⊘.634
.336 = .558/.634
.062/.336 = .880
.185 .
d) Calculating the message from {X, Y } to {Y, Z}.
pnew (y, z) = pold (y, z) ⊗({X, Y } ⇒{Y, Z})
= .571 .073
.063 .293 ⊗.880 .185
.880 .185 = .592 .014
.055 .054 .
e) Passing the message from {X, Y } to {Y, Z}.
pold (w, x)
pold (x)
pnew (x, y)
pnew (y)
pnew (y, z)
W
X
Y
X
1
0
X
Y
1
0
Y
Z
1
0
1 .540 .080
1
.620
1 .558
0
1
.558
1 .592 .014
0 .060 .320
0
.380
0 .062
0
0
.062
0 .055 .054
f) Status after e1 has been propagated down the chain from {X, Y }.

118
5 Eﬃcient Calculations
Table 5.4 Updating probabilities up a chain
pold (w, x)
pold (x)
pnew (x, y)
pnew (y)
pnew (y, z)
W
X
Y
X
1
0
X
Y
1
0
Y
Z
1
0
1 .540 .080
1
.620
1 .558
0
1
.558
1 .592 .014
0 .060 .320
0
.380
0 .062
0
0
.062
0 .055 .054
a) Status after e1 has been propagated down the chain from {X, Y }.
{X, Y } ⇒{W, X} = pnew (x) ⊘pold (x) = .620
0
⊘.620
.380 = 1
0 .
b) Calculating the message from {X, Y } to {W, X}.
pnew (w, x) = pold (w, x) ⊗({X, Y } ⇒{W, X})
= .540 .080
.060 .320 ⊗1 1
0 0 = .540 .080
0
0
.
c) Updating the potential for {W, X}.
pnew (w, x)
pnew (x)
pnew (x, y)
pnew (y)
pnew (y, z)
W
X
Y
X
1
0
X
Y
1
0
Y
Z
1
0
1 .540 .080
1
.620
1 .558
0
1
.558
1 .592 .014
0
0
0
0
0
0 .062
0
0
.062
0 .055 .054
d) Status after propagating e1 both up and down the chain.
pnew (w, x)
pnew (x)
pnew (x, y)
pnew (y)
pnew (y, z)
W
X
Y
X
1
0
X
Y 1
0
Y
Z
1
0
1 .871 .120
1
1
1 .9
0
1
.9
1 .810 .020
0
0
0
0
0
0 .1
0
0
.1
0 .090 .080
e) Status after normalizing.
This leads us to an important property of the junction tree. Look at the
three junction tree examples we have seen so far, Figs. 5.2, 5.3, and 5.4. In
each case pick out the nodes (both clique and intersection) in the junction
tree that contain X. Note that they are all connected. This is true for each
variable in the model. This so-called running intersection property is critically
important because every time we pass a message from a node that contains
the variable X to one that does not, we marginalize out the variable X. This
means that we will not pass messages about X through a section of the tree
that does not contain X. This property will be discussed further in Sect. 5.3.

5.2 More Eﬃcient Procedures for Chains and Trees
119
Fig. 5.3 A junction tree corresponding to a singly connected graph. As with Fig. 5.2
the intersection nodes that attach to only one clique node, p(u),p(y) and p(z) are
usually dropped for computational purposes
Reprinted with permission from ETS.
Fig. 5.4 A polytree and its corresponding junction tree
Reprinted with permission from ETS.
5.2.3 Virtual Evidence
Before we move to more complicated graphs, it is worth mentioning what to
do when the evidence is uncertain. This situation arises all the time when
human (or even computer) raters make a judgment about a performance. It
is well known that raters are not perfect. Suppose that we have information

120
5 Eﬃcient Calculations
that the raters on a particular assessment give the correct rating say 90 % of
the time. How do we enter this information into the Bayes net?
The key to understanding how to incorporate this uncertain evidence in
our model is to think about the expert’s rating as another, implicit, node in
our graphical model. If in Fig. 5.1, X represents the examinee’s proﬁciency
variable and Y represents the true quality of the performance, we could add
Node Z to represent a rater’s noisy report about the quality of the perfor-
mance. Let us suppose that if the performance is truly adequate, or Y = yes,
the probability of the rater correctly judging it as adequate, or Z = yes, is
p (z1|y1). If the performance is truly inadequate, or Y = no, the probability
of the rater erroneously judging it as adequate, or Z = yes, is p (z1|y2).
Now Node Z is a child of Node Y . This produces a simple chain X →
Y →Z. The junction tree would have two clique nodes, {X, Y } and {Y, Z},
with a single intersection node, {Y }. We can use the conditional probabilities
for z given y and the marginal probability of y, p (y) to construct the initial
potential table for {Y, Z}:
p (z1, y1) p (z2, y1)
p (z1, y2) p (z2, y2) = p (z1 | y1) p (y1) p (z2 | y1) p (y1)
p (z1 | y2) p (y2) p (z2 | y2) p (y2)
A judgment from the rater takes the form of Z = z1 or Z = z2. Let us
use Z = z1 as an illustration, and examine the message {Y, Z} ⇒{X, Y }. The
denominator pold(y) is the original marginal distribution for Y , [p(y1), p(y2)].
The numerator pnew(y) will have the the column from the {Y, Z} potential
that corresponds to observing Z = 1, namely [p(z1|y1)p(y1), p(z1|y2)p(y2)].
Thus the message to {X, Y } will be
p(z1|y1)p(y1)
p(y1)
, p(z1|y2)p(y2)
p(y2)

= [p(z1|y1), p(z1|y2)] .
This message is precisely the likelihood of the observed evidence under the
various states of Node Y . These likelihoods form a potential that multiplies
the potential in Node {X, Y } to make the new distribution.
Pearl (1988) calls uncertain evidence, like the judgment from the rater,
virtual evidence and notes that statements of certainty about the evidence
should usually be treated as likelihood information. By the reasoning illus-
trated above, we can add virtual evidence directly into the Bayes net without
needing to explicitly add the node representing the statement (Node Z) into
the model. Form the likelihood vector, [p(z1|y1), p(z1|y2)], as a potential over
the variables {Y }. This can be combined with any clique node in the junction
tree which contains Y . The belief-updating algorithm can then propagate this
information to the other tables in the junction tree just as before.
This calculation extends smoothly to cases with more than one rater. We
assume that given the performance, all of the ratings are independent. There-
fore, we can simply multiply the likelihoods for the individual ratings together
to get the combined likelihood for the whole set of ratings, then update the

5.2 More Eﬃcient Procedures for Chains and Trees
121
Table 5.5 Updating with virtual evidence
yes
no
P(X)
expert .8 × .5 = .40 .2 × .5 = .10
.5
novice .4 × .5 = .20 .6 × .5 = .30
.5
P(Y )
.6
.4
1
a) Table for initial joint and marginal probabilities.
yes
no
P(X)
expert .4 × .9 = .36 .1 × .05 = .005
.365
novice .2 × .9 = .18 .3 × .05 = .015
.195
P(Y )
.54
.02
.56
b) Information (likelihood) arrives for Y : .9 for yes and .05 for no.
yes
no
P(X)
expert .643
.009
.652
novice .321
.027
.348
P(Y ) .964
.036
1
c) Normalize by dividing by the grand total, .56.
proﬁciency variable. If the ratings arrive one at a time, then we can apply the
belief-updating algorithm for virtual evidence sequentially, once for the ﬁrst
rater, again for the second rater, and so on.
Example 5.4 (Dental Hygienist Assessment, Example 5.1, Contin-
ued). For a numerical example, suppose performances are evaluated by a
human judge who has the following characteristics. If the patient history is
truly adequate, the judge marks it yes 90 % of the time. If the history is not
adequate, then the judge marks it no 95 % of the time. (Looking ahead to
Chap. 7, these numbers are called the sensitivity and speciﬁcity of the rater.)
A rater marking a performance as yes produces a virtual evidence likelihood
vector of [.9, .05]. This is a potential over Y , which will be combined with the
potential representing the prior distribution over X and Y given in the ﬁrst
panel of Table 5.5.
The ﬁrst panel of Table 5.5 is identical to the ﬁrst panel of Table 5.2, as
the prior distributions are identical. The diﬀerence comes with the arrival of
the evidence in the second panel. In the middle panel the virtual evidence is
combined with the prior distributions to produce the posterior potential over
{X, Y }. In order to interpret this potential as probability distributions we

122
5 Eﬃcient Calculations
need to normalize it. The sum of the entries in middle panel is .56, the prior
probability of this particular bit of evidence (judge gives a rating of yes). The
ﬁnal panel shows the normalized distribution.
One way to think about how virtual evidence works is that the junction
tree is broken into two parts. The virtual evidence is the message passed from
the {Y, Z} part of the tree to the {X, Y } part of the tree. We could actually
make the split at any intersection node of the junction tree. Section 5.4 shows
an application of this idea that is important in educational assessment.
5.3 Belief Updating in Multiply Connected Graphs
As the graphical structure becomes more complex, so does the belief-updating
algorithm. However, we already saw that if the graph is shaped like a tree,
the belief-updating algorithm was just a simple extension of the algorithm for
chains. Thus, we can use the algorithm in any graphical structure if only we
could transform it into a tree: in this case, a tree where the nodes represent
groups of variables.
The tree we are looking for is the tree of cliques, a more general version of
the junction tree we have seen in chains and trees. By transforming the graph
into a tree whose nodes represent cliques in our original graph, we can deal
with much more complex graphical structures. There are a number of variants
on this basic algorithm (See Sect. 5.6). This section describes one approach to
doing this, and illustrates the procedures with a simple numerical example.
5.3.1 Updating in the Presence of Loops
A multiply connected graph is a graph with at least one loop, for instance,
where there is more than one chain (undirected path) from one variable to
another. Figure 5.5 shows an example of a loop. As a directed graph this graph
is acyclic. However, if we drop the directions of the edges, the underlying
undirected graph has a cycle, V to X to Y to U and back to V .
Had there been only the path from V to X to Y , we could have built a
junction tree for pairs of variables that enables coherent updating across the
three variables. Likewise had there been only a path from V to U to Y . But
with two paths, trying to use both of these variable-level structures to update
from information on Y does not generally provide the correct posterior for U,
V , or X.1 The “competing explanations” phenomenon described in Sect. 3.3.3
(Example 3.9) is a clear example of the failure. Seeing an incorrect response
1 Weiss (2000) attempts to characterize situations in which the algorithm will con-
verge and produce proper marginal distributions. This seems to depend on both
the network and the evidence (Murphy et al. 1999). Weiss (2000) does note that
the loopy-propagation algorithm will always produce the proper Maximum A
Posteriori (MAP) estimate, even if the margins are incorrect.

5.3 Belief Updating in Multiply Connected Graphs
123
means that at least one of the two required skills is probably lacking, and
learning that one was present or missing would inﬂuence our belief about
the other. This is a strong ﬁnding about the joint distribution of the two
skills that cannot be captured by updating belief about each of them them
independently.
Lauritzen and Spiegelhalter (1988) broke beyond the barrier of single
connectedness. The essential idea is this: One can carry out the coherent
propagation of information by passing messages between the cliques. It is
common practice to express this algorithm as message passing in a tree of
cliques. The junction tree, introduced in the previous section, takes the tree of
cliques and adds nodes corresponding to the intersections between the cliques.
The junction-tree algorithm described here is a variant of the Lauritzen–
Spiegelhalter algorithm.
As a look ahead, the cliques—maximal sets of connected nodes—for the
loopy digraph in Fig. 5.5 are {U, V, X}, {U, Y, X}, and {X, Z}, and the junc-
tion tree of cliques and clique intersections is shown as Figure 5.6. The key
implication is that if information arrives about Y , its implications for our
belief about X must deal with U and V jointly. Coherent reasoning around
the loop is achieved, although at the cost of working with groups of variables
rather than single variables.
Every acyclic digraph can be represented as a tree of cliques (or junction
tree). A key feature is its treewidth, or the size of the largest clique. The brute
force algorithm requires constructing a giant table with all of the variables
in the model, but the biggest table in the message-passing algorithm is the
size of the biggest clique, the treewidth. If the topology of the graph is favor-
able, meaning that the treewidth is small, calculation can be feasible for even
networks with very many variables. The muscle and nerve inference network
(MUNIN) Bayes net (Andreassen et al. 1987), for example, comprises about a
thousand variables, yet can support real-time calculation because of its sparse
treewidth. The sparse treewidth is achieved largely through conditional inde-
pendence assumptions from the substantive domain of the application. For
MUNIN, the domain is diagnosing neuromuscular diseases. Medical knowl-
edge and characteristics of test procedures imply independence of certain tests
given physical states, of symptoms given syndromes, and of syndromes given
disease states. If the topology of a graph is not favorable, large cliques can-
not be avoided; treewidth, and therefore computational burden, increases.
At worst, all variables are in a single clique and computation through the
Lauritzen–Spiegelhalter algorithm reduces to the brute force calculation with
all combinations of values of all variables.
5.3.2 Constructing a Junction Tree
This section walks through the steps from a joint distribution of variables to
a junction tree for eﬃcient calculation, in the context of a simple example: an

124
5 Eﬃcient Calculations
Fig. 5.5 A loop in a multiply connected graph
Reprinted from Almond et al. (2006a) with permission from ETS.
Fig. 5.6 The tree of cliques and junction tree for Figure 5.5. Clique nodes are
represented with rectangles and intersection nodes are represented with ovals
Reprinted with permission from ETS.

5.3 Belief Updating in Multiply Connected Graphs
125
adaptation of a medical diagnosis example from Jensen (1996) to the context
of cognitive diagnosis.
Example 5.5 (Two Skills and Two Tasks). We are interested in learning
the proﬁciency that a student Pat has with literary terms. Skill A is literary
vocabulary, which we suppose can take only the two values High (H) and Low
(L). Skill B is the ability to infer the meaning of such words in context, which
again we suppose can take the values of High (H) and Low (L). Denote these
proﬁciency variables by θA and θB.
There are two sources of evidence, Task 1 and Task 2, both of which yield a
response that is evaluated as Right (1) or Wrong (0). Denote these observable
variables by X1 and X2. Task 1 asks the meaning of “simile” and provides a
short essay about how poets use similes and metaphors to open readers’ eyes
to unexpected connections. A student is more likely to answer Task 1 correctly
if either she already is familiar with the word, or can infer its meaning from the
passage. Task 2 asks the meaning of “anaphora” with a sparse text illustrating
its use, so it is mainly prior familiarity with the term that will be likely to
produce a correct response. Figure 5.7 shows the graphical structure of this
problem.
A
B
X1
X2
Fig. 5.7 Acyclic digraph for two-skill example (Example 5.5)
Reprinted from Almond et al. (2006a) with permission from ETS.
Using this information about Skills, Tasks, and their relationships, we will
build a Bayes net, a junction tree, and a computational representation in the
form of an interconnected set of potential tables. Then we will observe Pat’s
response to Task 1, and use this machinery to update our beliefs about Skill A
and Skill B and to predict whether we will see a correct response to Task 2
as well.
The following sections describe six steps to building a computing repre-
sentation for problems such as these. The steps address these topics:
1. Recursive representation of the joint distribution of variables.

126
5 Eﬃcient Calculations
2. Acyclic digraph representation of the probability distribution.
3. Representation as a “moralized” and triangulated undirected graph.
4. Determination of cliques and clique intersections.
5. Junction tree representation.
6. Potential tables.
At this point, the potential tables are ready to support calculations that
propagate the eﬀect of new evidence.
Recursive Representation of the Joint Distribution
By repeatedly applying the deﬁnition of conditional probability, it is always
possible to write the joint probability distribution of a number of variables as
the product of conditional probabilities, each variable in the list conditional
on those earlier in the list. That is,
P (An, An−1, . . . , A2, A1)
= P (An|An−1, ..., A2, A1) × P (An−1|An−2, . . . , A2, A1) × · · · ×
P (A2|A1) × P (A1)
= 
k
P (Ak|Ak−1, . . . , A1),
(5.5)
where the ﬁnal term is understood to be simply P(A1). This is a recursive
representation of the distribution. Such a representation holds for any ordering
of the variables, but some orderings are more useful than others; a variable Ak
may be conditionally independent of some of the variables with lower indices,
and they drop out of its conditioning list. As in Chap. 4, we call those that
remain its parents and denote them by pa(Ak). Thus,
P (An, An−1, ..., A2, A1) =

k
P (Ak| pa (Ak)).
(5.6)
We have already seen this equation in Sect. 4.2.1 as Eq. 4.1. There the par-
ents refer to the parents in the graphical structure. In general, exploiting the
conditional independence relationships modeled in the graph lead to an eﬃ-
cient recursive representation. Examples of such relationships are eﬀects that
are conditionally independent given causes, observations that are condition-
ally independent given parameters, and current events that are conditionally
independent given past events. In educational assessment, we typically model
observable variables from diﬀerent tasks as conditionally independent given
proﬁciency variables.
The joint distribution we are interested in for the running example in
this section is P (θA, θB, X1, X2). The description of the setup asserts that
the observable variables X1 and X2 are conditionally independent given the
proﬁciency variables θA and θB, and that the two proﬁciency variables are
independent with respect to one another in the absence of any observations.
This suggests the following recursive expression of the distribution:
P (θA, θB, X1, X2) = P (X1|θA, θB, X2) P (X2|θA, θB) P (θA|θB) P (θB) .

5.3 Belief Updating in Multiply Connected Graphs
127
Note that the order of the nodes in the recursive decomposition (Eq. 5.5) follow
the graph in Figure 5.7 in the sense that the parents of each node in the graph
are always ahead of the child node in the order. Exploiting the conditional
independence relationships in the graph yields the following factorization:
P (θA, θB, X1, X2) = P (X1|θA, θB) P (X2|θA, θB) P (θA) P (θB) .
(5.7)
Example 5.6 (Numbers for Example 5.5). In order to illustrate calcu-
lations later in the section, we propose numerical values for the probability
distributions in the recursive representation. Part II addresses the issue of
where these numbers come from, but it suﬃces to say at this point that the
structures for the probability distributions and initial numerical values can be
provided by experts, and both the structures and the numerical values can be
reﬁned in light of data that bear on the problem as an exercise in Bayesian
estimation. For now we will work with the values in Table 5.6.
Table 5.6 Probabilities for Example 5.5
θA:
 P (θA = H) = .11
P (θA = L) = .89
θB:
 P (θB = H) = .11
P (θB = L) = .89
X1:
⎧
⎪
⎪
⎨
⎪
⎪
⎩
P (X1 = 1 | θA = H, θB = H) = .99;
P (X1 = 1 | θA = H, θB = L) = .90;
P (X1 = 1 | θA = L, θB = H) = .90;
P (X1 = 1 | θA = L, θB = L) = .01;
P (X1 = 0 | θA = H, θB = H) = .01
P (X1 = 0 | θA = H, θB = L) = .10
P (X1 = 0 | θA = L, θB = H) = .10
P (X1 = 0 | θA = L, θB = L) = .99
X2:
⎧
⎪
⎪
⎨
⎪
⎪
⎩
P (X2 = 1 | θA = H, θB = H) = .99;
P (X2 = 1 | θA = H, θB = L) = .05;
P (X2 = 1 | θA = L, θB = H) = .90;
P (X2 = 1 | θA = L, θB = L) = .01;
P (X2 = 0 | θA = H, θB = H) = .01
P (X2 = 0 | θA = H, θB = L) = .95
P (X2 = 0 | θA = L, θB = H) = .10
P (X2 = 0 | θA = L, θB = L) = .99
We can make some observations in passing on the probative, or evidentiary,
value of X1 and X1 for inferences about θA and θB. They are based on exam-
ining the conditional probability distributions of the observables given their
proﬁciency model parents. Suppose for example it is observed that X1 = 1.
This could occur no matter what the values of θA and θB are, since there are
nonzero conditional probabilities for X1 = 1 at each combination. But this is
a more likely occurrence under some combinations of θA and θB than others;
for example, the conditional probability of X1 = 1 is .99 if both skills are
high and only .01 if both are low. The column of conditional probabilities for
X1 = 1 at each combination of its parents θA and θB is the likelihood function
induced for these proﬁciency variables by a realized observation of X1 = 1. In
this case, the likelihood is .9 or above for all combinations with at least one
skill at High, and only .01 when both skills are Low. Conversely, observing
X1 = 0 induces a relatively high likelihood for {θA = Low, θB = Low} and

128
5 Eﬃcient Calculations
a low likelihood for all the other skill combinations. To borrow a term from
medical diagnosis, X1 provides good diﬀerential diagnosis for distinguishing
between “both skills at Low” and all other proﬁciency states. On the other
hand, it has no value whatsoever for diﬀerential diagnosis between the states
{θA = High, θB = Low} and {θA = Low, θB = High}. By similar reasoning,
X2 has diﬀerential diagnostic value for distinguishing between states with
θB = High from those with θB = Low, but has little value for distinguishing
states that diﬀer with respect to θA.
Acyclic Digraph Representation
As discussed in Sect. 4.2, we can draw an acyclic digraph to represent the
joint probability distribution of a set of variables straight from a recursive
distribution. Each variable is a node in the digraph, and for each node Ak,
there is a directed edge coming to it from each of its parents, or the variables in
pa(Ak). The digraph for our running example is shown as Fig. 5.7. The digraph
depicts the structure of the joint probability distribution with regard to the
conditional independence relationships that can be read from the recursive
representation of Eq. 5.7.
Moralized and Triangulated Undirected Graph
Starting with the acyclic digraph, we drop the directions of the edges and
add additional edges as necessary to meet two requirements. First, all the
parents of every given child must be connected by edges (i.e., the parents of
children must be “married”—hence the term moralized graph). Looking ahead,
we need to assign the factor in the recursive representation corresponding to
this child and its parents to one of the clique nodes in the junction tree.
Connecting the parents ensures that the child and its parents will either be a
clique or a subset of a clique in the moralized graph. It also ensures that any
dependencies caused by the competing explanations phenomenon (Sect. 3.3.3)
will be handled coherently, as they will be dealt with jointly in the potential
table for a clique in the junction tree. Figure 5.8 is the moralized undirected
graph for our example.
Note that it is the loops in the moralized graph that cause problems for
computation. Even our simple example now has loops; for example, one can
start a path at X1, follow a connection to θA, then to X2, then to θB, and
ﬁnally return to X1. This loop did not count as a cycle in the directed graph
because it did not follow the direction of the arrows. However, it is the loops
in the undirected moral graph that cause problems for the simple updating
algorithm.
Another way to understand moralization is to think about the factorization
hypergraph. Each child variable and its set of parents in the recursive repre-
sentation corresponds to a hyperedge in the hypergraph. When we construct
the 2-section of the factorization hypergraph, the parents of the child variable

5.3 Belief Updating in Multiply Connected Graphs
129
A
B
X1
X2
Fig. 5.8 Moralized undirected graph for two-skill example (Example 5.5)
Reprinted from Almond et al. (2006a) with permission from ETS.
are in the same hyperedge and thus must be joined. Thus, the moralized graph
is the 2-section of the factorization hypergraph.
In addition to being moralized, the graph must be triangulated; that is,
any cycle (in the moral graph) consisting of four or more variables must have
a chord, or “short cut.” The graph in Fig. 5.8 is already triangulated; the
edge between θA and θB induced by moralization is a chord. The leftmost
graph in Fig. 5.9 is an example of a graph that is not triangulated. Although
triangulation is not a problem in our simple example, it can be a big issue in
larger problems.
Triangulation is necessary to express probability relationships in a way
that lends itself to coherent propagation of information under Lauritzen–
Spiegelhalter propagation and its variants. Without triangulation, the cliques
may form a graph with cycles. For example, the cliques of the leftmost graph
in Fig. 5.9 are {A1, A2}, {A2, A3}, {A3, A4}, {A4, A5}, and {A5, A1}. These
make a cycle. Recall from Chap. 4 that an acyclic hypergraph was deﬁned as
one whose 2-section is triangulated. Triangulating the moral graph guaran-
tees that a singly-connected clique representation can be constructed (Jensen
1988).
Although a given moral graph may not be triangulated, new edges can be
“ﬁlled in” to make it so. There can be more than one way to ﬁll in a graph to
make it triangulated. Figure 5.9 shows two diﬀerent ways to triangulate the
untriangulated graph.
Finding the optimal triangulation for a graph is a hard problem. Diﬀerent
ﬁll ins will create diﬀerent sized cliques and hence will aﬀect the treewidth of
the ﬁnal graph. Almond (1995) summarizes some of the heuristics which are
commonly used to ﬁnd the best triangulation.
Cliques and Clique Intersections
From the triangulated graph, we next determine cliques, or biggest subsets
of variables that are all linked pairwise to one another. Cliques overlap, with

130
5 Eﬃcient Calculations
A1
A 2
A 3
A 4
A 5
A1
A 2
A 3
A 4
A 5
A1
A 2
A 3
A 4
A 5
or
Fig. 5.9 Two ways to triangulate a graph with a loop
Reprinted from Almond et al. (2006a) with permission from ETS.
sets of overlapping variables called clique intersections. In the next step these
cliques and clique intersections will become the nodes of the junction tree. Fig-
ure 5.10 shows the two cliques in our example, {θA, θB, X1} and {θA, θB, X2}.
The clique intersection is {θA, θB}.
Although there can be multiple ways to produce a triangulated graph
from a given digraph, there is only one way to deﬁne cliques from a trian-
gulated graph. There can be multiple ways to arrange them in a tree, but
the computational cost is dominated by the size of the largest clique, that is,
the treewidth. For this reason a triangulation that yields many small cliques
is preferred to one that yields fewer but larger cliques. The HUGIN Bayes
net compiler (Appendix A) oﬀers several alternatives for triangulation, and
on request reports the resulting cliques to the user. Strategies for increased
computational eﬃciency include adding variables to break loops, redeﬁning
variables to collapse combinations of variable values that are not meaning-
fully distinct, and dropping associations when the consequences are benign.
A
B
A
B
X1
X1
X2
X2
Fig. 5.10 Cliques for the two-skill example. The graph on the left shows the clique
{θA, θB, X1}; the graph on the right shows the clique {θA, θB, X2}
Reprinted from Almond et al. (2006a) with permission from ETS.
Junction Tree Representation
Once we have the cliques and clique intersections, creating a junction tree is
straightforward. Start with any clique node, and connect it to clique intersec-

5.3 Belief Updating in Multiply Connected Graphs
131
tion nodes it contains. Taking each of these clique intersection nodes one at
a time, connect it to clique nodes that also contain it and have not yet been
addressed. Having done this with all the intersection nodes from the starting
clique, take each of the cliques that were added to the junction tree one at
a time and repeat the same process, in each case bringing in clique nodes
that have not yet been addressed. When no more cliques can be connected
through intersections in this way, either all the cliques are now connected or
some remain unconnected. Variables in cliques that are unconnected are inde-
pendent of the variables in the cliques that were connected thus far, and will
be in separate junction trees. The connecting process begins anew, starting
with one of the remaining cliques. When multiple junction trees result, evi-
dence about variables associated with one tree has no impact on belief about
variables associated with another tree, and they can be treated as separate
problems. In all, this process ensures that the graph(s) so constructed will be
trees and have the running intersection property.
Deﬁnition. Running Intersection Property. A junction tree (or other
tree containing sets of variables) has the running intersection property if for
every variable the subgraph which contains that variable is connected. A tree
with the running intersection property is called a Markov Tree.
The key to the eﬃciency of the belief-updating algorithm is that we can
marginalize out information that is no longer needed. The running intersection
property tells us when information can be safely marginalized out. When we
pass a message from a clique node which contains X to an intersection node
which does not, we can be sure because of the running intersection property
that there will be no nodes containing X on the other side of that intersection.
The running intersection property is a key part of the proof of correctness of
this algorithm (Shenoy and Shafer 1990; Almond 1995).
Figure 5.11 gives the junction tree for our example. Note that the two
clique nodes correspond to the two tasks and the clique intersection corre-
sponds to the proﬁciency variables. Thus the junction tree reﬂects our under-
standing of the conditional independence assumptions on which this model
is based, namely that the two observable outcome variables are independent
given the proﬁciency variable.
Potential Tables
As described in Sect. 5.2, each clique or clique intersection node in the junction
tree has a potential table, which is related to the joint probability distribution
of the nodes in that clique or intersection. Each of the factors in the recursive
representation is expressed as a potential and allocated to one of these tables.
As implied by the preceding steps, the variables in each factor will be together
in some clique, but depending on the topology, a clique may contain the
variables for multiple factors. The allocated tables are combined to make the
initial potential associated with that node.

132
5 Eﬃcient Calculations
Fig. 5.11 Junction tree for the two-skill example
Reprinted from Almond et al. (2006a) with permission from ETS.
To initialize the junction tree version of the belief-updating algorithm
requires that the table in each clique or clique intersection reﬂect the joint dis-
tribution of those variables (i.e., before adding evidence). The potential tables
in Table 5.7 indicate the initial status of the network for Example 5.5; that
is, before speciﬁc knowledge of a particular individual’s observable variables
states becomes known.
Table 5.7 Potential tables for the two-skill example
θA θB P(X1 = 1) P(X1 = 0)
H
H
.012
.000
H
L
.088
.010
L
H
.088
.010
L
L
.008
.784
θA θB Probability
H
H
.012
H
L
.098
L
H
.098
L
L
.792
θA θB P(X2 = 1) P(X2 = 0)
H
H
.011
.001
H
L
.005
.093
L
H
.088
.010
L
L
.008
.784

5.3 Belief Updating in Multiply Connected Graphs
133
There are a number of algorithms for initializing the junction tree. The
following example shows one.
Example 5.7 (Potential Tables for Example 5.5). Constructing poten-
tial tables can be accomplished in a number of ways. Starting with a recursive
representation of the probability distribution, it is easiest to work from root
nodes, or the one or more variables that have no parents, and successively use
their marginal distributions and the conditional distributions of their children,
as they appear in cliques further down the list. For example, the potential table
for the clique {θA, θB, X1} was calculated as follows: θA and θB are both root
nodes. Because they are independent, their joint distribution is calculated by
multiplying the prior probabilities of .11 for High and .89 for Low for all four
High/Low combinations the two variables can take:
P (θA = H, θB = H) = P (θA = H) P (θB = H) = .11 × .11 = .012
P (θA = H, θB = L) = P (θA = H) P (θB = L) = .11 × .89 = .098
P (θA = L, θB = H) = P (θA = L) P (θB = H) = .89 × .11 = .098
P (θA = L, θB = L) = P (θA = L) P (θB = L) = .89 × .89 = .792.
X1 is the child of θA and θB. Its conditional for each combination of values
of its parents was given in the recursive deﬁnition of the distribution. For
example, P (X1 = 1 | θA = H, θB = L) = .90, so
P (X1 = 1, θA = H, θB = L)
= P (X1 = 1 | θA = H, θB = L) P (θA = H, θB = L)
= .9 × .098 = .010.
In a similar manner, the joint probability for every combination of {θA, θB, X1}
values can be calculated, and becomes the entry for that combination in the
potential table.
Once the potential table for a clique has been calculated, the table for any
clique intersection connecting it to another clique is obtained by marginaliz-
ing with respect to whatever variables are in the intersection. In this simple
example, the only clique intersection is {θA, θB}. It can be obtained by col-
lapsing the {θA, θB, X1} potential table over X1, or p ({θA, θB, X1}) ⇓X1 . We
already know the result since we obtained the joint {θA, θB} along the way to
building the table for {θA, θB, X1}, but this does not generally happen.
Having started from root nodes, moving from a clique intersection to a
successive clique means that the variables in the clique intersection come
in with their joint marginal distribution. The new variables will have con-
ditional distributions given those in the clique intersection and possibly on
other variables in the clique. One computes joint distributions using the rule
of marginal times conditional distribution as above, variable by variable, in
the order they appear in the recursive representation. The second clique in
our two-skill example is {θA, θB, X2}. We already have the joint distribution
for θA and θB. The conditional distribution of X2 given θA and θB is given

134
5 Eﬃcient Calculations
in the recursive representation of the full joint distribution. For example,
P (X2 = 0 | θA = L, θB = L) = .99, so
P (X2 = 0, θA = L, θB = L)
= P (X2 = 0 | θA = L, θB = L) P (θA = L, θB = L)
= .99 × .792 = .784.
The six steps used to move from the digraph representation of the proba-
bility model to the junction tree are sometimes called compiling the Bayesian
network (this term is used by many of the software packages described in
Appendix A, even if those packages do not use exactly the algorithm described
here). The digraph representation is most convenient for deﬁning the condi-
tional independence relationships that will deﬁne the shape of the graph and
eliciting the conditional probabilities that will deﬁne the joint distribution (or
will serve as priors for distributions to be learned from data, as in Part II).
The junction tree is more convenient for answering queries, such as what is
the probability distribution of θA after observing X1. Just like compiling a
computer program, compiling a digraph into a junction tree makes it ready
to go to work for us.
5.3.3 Propagating Evidence Through a Junction Tree
To absorb new evidence about a single variable, ﬁrst express the evidence
as a potential. Pick a clique node containing the variable, and combine the
potential in that node with the potential representing the evidence. Now apply
the belief-updating algorithm described in Sect. 5.2.1 to propagate that infor-
mation throughout the tree. The only additional wrinkle is that for clique
intersections with more than one variable, we work with entries for the joint
combinations of all the variables in the clique intersection, rather than just
for the values of a single variable. When messages containing the evidence
have reached all clique nodes in the tree, then the posterior distribution of
any variable in the model given the evidence can be found by looking at the
potential of any node in the junction tree that contains that variable. The
single-connectedness and running intersection properties of the junction tree
assure that coherent probabilities result.
Example 5.8 (Evidence Propagation in Example 5.5). Suppose Pat
answers Item 1 correctly; that is, X1 = 1. How does this change our beliefs
about the other variables?
The process begins with the potential table for the clique {θA, θB, X1}. In
the initial condition, we had a joint probability distribution for the variables
in this clique, as shown in the top table of Table 5.7. We now know with
certainty that X1 = 1, so the column for X1 = 0 is zeroed out (Table 5.8).
The remaining columns (in this case there is just one of them) reﬂect the
proportion of our revised belief about the values for the other variables in the
clique, θA and θB.

5.4 Application to Assessment
135
That ﬁrst column in the top table of Table 5.7, or [.012, .088, .088, .008],
is the updated potential in the clique intersection, or pnew{θA, θB}. The ini-
tial potential that was stored in {θA, θB} was [.012, .098, .098, .792], which
is pold{θA, θB}. This is the information we need to calculate the message
{θA, θB, X1} ⇒{θA, θB, X2}.
Message
pold
pnew (pnew/pold)
θA = H, θB = H
.012
.012
1.00
θA = H, θB = L
.098
.088
.90
θA = L, θB = H
.098
.088
.90
θA = L, θB = L
.720
.008
.01
The values in the potential table for {θA, θB, X2} are obtained with the
belief updating operation as shown in Table 5.7. The resulting values are
proportional to the new probabilities for the variables in this clique. The ﬁnal
panel of Table 5.8 shows the values after normalizing. The highest probabilities
are for the combinations in which only one of the skills is High (a consequence
of the low prior probabilities for the skills) and X2 being right or wrong in
accordance with whether it is θA or θB, that is at High.
5.4 Application to Assessment
Chapter 2 described a general framework for assessments in terms of a num-
ber of models. Two of those models, the proﬁciency model and the evidence
model have components that describe a probabilistic relationship among the
variables. (That is not to say that the other models do not have a strong
inﬂuence on the statistical properties of the assessment, rather that these are
the two parts of the model that are conventionally modeled with direct state-
ments of probability). In this book, we are interested in assessments for which
those probabilistic parts of the model are expressed with Bayesian networks.
The proﬁciency models consist of proﬁciency variables—latent variables
that characterize the knowledge, skills, or other attributes of students—and
their distribution in a population of interest. The measurement component
of evidence models addresses the relationship of these proﬁciency variables
to observable variables—characterizations of the qualities of things students
say, do, or make. The proﬁciency variables are of persistent interest in an
assessment application. They are the level at which we conceive of the eﬀects of
learning and the locus of decisions about instruction. The observable variables
that appear in evidence models are of interest mainly insofar as they provide
information about proﬁciency variables.
The total graphical model for an assessment consists of a Bayesian network
with all of the proﬁciency variables and all of the observable outcome variables

136
5 Eﬃcient Calculations
Table 5.8 Updating the potential tables for {θA, θB, X2}
Before belief updating
θA θB P(X2 = 1)
P(X2 = 0)
H
H
.011
.001
H
L
.005
.093
L
H
.088
.010
L
L
.008
.784
Belief updating (i.e., multiplication by message)
θA θB P(X2 = 1)
P(X2 = 0)
H
H .011 × 1.00
.001 × 1.00
H
L
.005 × .90
.093 × .90
L
H
.088 × .90
.010 × .90
L
L
.008 × .01
.784 × .01
After belief updating
θA θB P(X2 = 1)
P(X2 = 0)
H
H
.011
.001
H
L
.004
.084
L
H
.080
.009
L
L
.000
.008
After normalizing
θA θB P(X2 = 1)
P(X2 = 0)
H
H
.056
.005
H
L
.020
.426
L
H
.406
.046
L
L
.000
.041
from any task which could conceivably be given to a student. In an ongoing
assessment system, hundreds or thousands of test items are developed and
used, all providing information about the same small set of proﬁciency vari-
ables; the tasks (and the task model variables) are relevant during the time
they are used, but they are retired and replaced continually.
In such an environment, it is obviously of beneﬁt to be able to update
proﬁciency models without having to build a single huge computational rep-
resentation for every student using all items that have been and may ever
be presented. Computation using a representation using only the tasks that
are or may be used in the present test would be preferable. Such a scheme
would be an example of what Breese et al. (1994) call knowledge-based model
construction: dynamic assembly of computational or representational mod-
els from preassembled fragments, according to the unfolding nature of the

5.4 Application to Assessment
137
problem. Computerized adaptive testing (CAT; Wainer et al. 2000) with item
response theory (IRT) is a familiar example from psychometrics.
The key idea is that the Bayesian networks associated with the proﬁciency
model and the measurement component of the evidence models are only frag-
ments of the total graphical model for the assessment. These fragments can
be stored in a library and assembled on demand. This is related to the object-
oriented Bayesian network models of Koller and Pfeﬀer (1997) and Laskey and
Mahoney (2000). This section expresses this modular measurement framework
in the context of Bayes nets, deﬁning proﬁciency model and evidence model
Bayes nets fragments. The focus is on the implications for assessment design
and analysis, with an eye toward adaptive applications.
5.4.1 Proﬁciency and Evidence Model Bayes Net Fragments
Two fundamental properties of psychometric models hold important implica-
tions for the recursive representation of the variables in psychometric mod-
els, and consequently for the Bayes nets and junction trees they induce. Let
(θ1, . . . , θm) be proﬁciency variables and (X1, . . . , Xn) be observable variables.
The two properties are as follows:
Property 5.1. Observable variables are always children, and never parents, of
proﬁciency variables. Proﬁciency variables may be parents of other proﬁciency
variables, and generally will be when there are multiple proﬁciency variables
and they are associated in the examinee population. Proﬁciencies in Reading,
Writing, Speaking, and Listening, for example, tend to be correlated, and we
would probably model θR, θW , θS, and θL as either directly related among
themselves or as children of a common language proﬁciency, say θLP .
Property 5.2. (Local Independence) Observable variables from distinct tasks
are conditionally independent, given proﬁciency variables. Observable vari-
ables from the same complex performance or from the same multipart task
may be parents of other observable variables in the same task, in addition to
their student-model parents, as when an answer to a multiple-choice question
is followed by “explain your answer.” (In graphical terminology, the proﬁ-
ciency variables in the digraph d-separate the sets of observable variables
from diﬀerent tasks.)
This second property is often called local independence. Yen (1993) describes
a number of situations in which local independence breaks down at the level
of individual observables; that is, local dependence occurs. Ratings of multiple
aspects of the same complex performance and items whose response depends
on previous responses are two examples. One of the most common testing
situations with local dependence is a testlet (Wainer and Kiely 1987) in which
several discrete items share a common stimulus, such as a reading passage or
a graph. But if observable outcome variables that exhibit local dependence
are placed within a single task and hence are scored by a single evidence

138
5 Eﬃcient Calculations
model, then Property 5.2 is not violated at the level of tasks and we can use
the method in this section. This gives Bayesian network models a expressive
power to model situations that can be diﬃcult to model with other methods.
The properties together imply ﬁrst that the joint distribution of
(θ1, . . . , θm) and (X1, . . . , Xn) can be written in terms corresponding to the
joint distribution of θs and the conditional distribution of Xs from distinct
tasks. We refer to the joint distribution of the θs as the proﬁciency model Bayes
net fragment, or PMF for short; that is, P (θ1, . . . , θm). To allow for conditional
dependence among observable variables from the same task, we introduce the
index j for such interrelated groups of observables, denote the observables
corresponding to Task j as

Xj1, . . . , Xjnj

, and refer to the conditional dis-
tribution of (Xk1, . . . , Xknk) given its student-model parents as the evidence
model Bayes net fragment, or EMF, for Task j; that is, P

Xj1, . . . , Xjnj | Θj

where Θj is the subset of θs that are parents of any of the Xs in Task k. We
refer to Θk as the footprint of Task j in the proﬁciency model. Thus, the joint
distribution can be written as
P

Xj1, . . . , Xjnj, θ1, . . . , θm

= P (θ1, . . . , θm)

j
P

Xj1, . . . , Xjnj | Θj

.
(5.8)
An acyclic digraph corresponding to a recursive representation in this form
may have edges connecting θs to one another, and Xs have as parents only
some set of θs and possibly other Xs from the same task. Figure 5.12 represents
these relationships in terms of a Venn diagram for variables in the PMF
and EMFs for two hypothetical tasks. Note that Θ1 = {θ2, θ3} and Θ2 =
{θ2, θ5, θ6}.
X11
X12
X13
X
X
X
X
Fig. 5.12 Relationship among proﬁciency model and evidence model Bayes net
fragments
Reprinted with permission from ETS.

5.4 Application to Assessment
139
5.4.2 Junction Trees for Fragments
Consider the total graphical model for an assessment consisting of a single
task. It is formed by joining the PMF to the EMF by connecting the footprint
variables. Now form the junction tree for this graph. We can arrange it so
there will always be a node in the junction tree corresponding to the footprint
(see Exercise 5.11), because moralization and triangulation will force edges
among at least some of them, and we can add edges if we need to in order to
connect them all. Split the junction tree at that node, producing two junction
trees, one for the proﬁciency model and one for the evidence model. The
virtual-evidence algorithm (Sect. 5.2.3) can then be used to pass information
between the two trees. Almond et al. (1999) use this as the basis of an eﬃcient
algorithm for working with large assessments.
According to the Local Independence Property (Property 5.2), any impact
on belief about the observables of one task on the observables of another task
is mediated strictly through the inﬂuence on proﬁciency variables. This sepa-
ration of tasks by the proﬁciency variables allows us to precompute junction
trees and potential tables for PMFs and EMFs. These can be stored in a large
pool and only the PMFs and EMFs relevant to a particular assessment situa-
tion (the form of the assessment the examinee actually sees) need be consulted
to draw inferences.
More formally, the Proﬁciency Model–Evidence Model ( PMEM) algorithm
(Almond et al. 1999) requires special construction procedures for the PMF and
EMF.
•
For the PMF: The PMF is a Bayes net in itself, and potential tables
could be built following the procedure described above in Sect. 5.3. But
doing so from whatever structure of dependencies happen to reside in
the recursive representation for P (θ1, . . . , θm) does not guarantee that the
footprint of each EMFs will appear in a clique. In addition to edges added
to moralize and triangulate the PMF, one must also add edges among
proﬁciency variables to ensure that the footprint of each EMF that will
be used to update the θs appears in at least one clique. After adding
the additional edges joining the proﬁciency variable, the junction tree and
potential tables for the PMF are then constructed as usual, starting with
the triangulation step.
•
For the EMF for each Task j: The essential element of an evi-
dence model Bayes net fragment is a conditional distribution of observ-
able variables given the proﬁciency variables in its footprint, namely
P

Xj1, . . . , Xjnj|Θk

. The EMF does not have information about the
marginal distribution of the proﬁciency variables, Θj. To produce the full
joint distribution to initialize the EMF, say P∗
Xj1, . . . , Xjnj, Θj,

assign
independent uniform distributions for the θs in Θj. This implies the unit
potential, 1, in which every element is 1 (or, since only proportionality
matters in potentials, equal values at any other nonnegative value), over

140
5 Eﬃcient Calculations
all possible combinations of values of the proﬁciency variables in Θj. Start-
ing from the acyclic digraph for this augmented EMF, produce an undi-
rected graph that ﬁrst adds edges between all pairs of θs in Θj, as well
as whatever additional edges are needed to moralize and triangulate the
graph.
The junction tree and potential tables for the EMF for Task j are then
constructed as usual.
This procedure guarantees that every EMF k will share an identical clique
with the PMF, namely Θk.
Example 5.9 (Bayes net fragments for Example 5.5).
The total graphical model in Example 5.5 can be expressed as one PMF,
over {θA, θB}, and two EMFs, one for each task, over {θA, θB, X1} and
{θA, θB, X2}. As before, {θA, θB} is initialized at [.012, .098, .098, .792] based
on the the marginal distributions for {θA} and {θB} and the fact that they
are independent. The EMF for Task 1 is initialized using the conditional prob-
abilities for X1 given θA and θB (as shown in the middle of Table 5.6) and
the unit potential over all possible combinations of the values of θA and θB.
The resulting initial potential in EMF1 is thus
⎡
⎢⎢⎣
1 1
1 1
1 1
1 1
⎤
⎥⎥⎦⊗
⎡
⎢⎢⎣
P (X1 = 1 | θA = H, θB = H) P (X1 = 0 | θA = H, θB = H)
P (X1 = 1 | θA = H, θB = L) P (X1 = 0 | θA = H, θB = L)
P (X1 = 1 | θA = L, θB = H) P (X1 = 0 | θA = L, θB = H)
P (X1 = 1 | θA = L, θB = L) P (X1 = 0 | θA = L, θB = L)
⎤
⎥⎥⎦
=
⎡
⎢⎢⎣
.99 .01
.90 .10
.90 .10
.01 .99
⎤
⎥⎥⎦.
(5.9)
Note that the two columns of this result are the likelihood induced for θA
and θB by observing X1 = 1 and X1 = 0 respectively.
By similar calculation, the initial potential in EMF2 is
⎡
⎢⎢⎣
.99 .01
.05 .95
.90 .10
.01 .99
⎤
⎥⎥⎦.
(5.10)
To illustrate PMFs and EMFs in a more interesting example, consider the
case of ﬁve proﬁciency variables, θ1, . . . , θ5, and three tasks. Task 1 and Task 3
contain one observable each, X11 and X31 respectively. Task 2 contains two
observable variables, X21 and X22, and also an unobservable evidence model
variable X23 to account for conditional dependence between X21 and X22
(more about this idea in Sect. 6.2. The acyclic digraph is shown as Fig. 5.13.

5.4 Application to Assessment
141
1
2
3
4
X 1 1
X 2 1
X 2 3
X 2 2
X 3 1
Fig. 5.13 Total acyclic digraph for three-task test
Reprinted from Almond et al. (2010) with permission from ETS.
Fig. 5.14 Proﬁciency model fragments for three-task test
Reprinted from Almond et al. (2010) with permission from ETS.
1
2
4
X 1 1
X 2 1
X 2 3
X 2 2
2
3
4
X 3 1
Fig. 5.15 Evidence model fragments for three-task test
Reprinted from Almond et al. (2010) with permission from ETS.

142
5 Eﬃcient Calculations
The digraphs that corresponds to the PMF is shown as Fig. 5.14, and the
EMFs are shown as Fig. 5.15.
Because of independence and conditional independence relationships, the
digraph for the proﬁciency model fragment (Fig. 5.14) is quite sparse. A
junction tree for this digraph by itself consists of two cliques, {θ1, θ2} and
{θ3, θ4, θ5}. The footprints of the three tasks are these: Θ1 = {θ2}, Θ2 =
{θ1, θ4}, and Θ3 = {θ2, θ3, θ4}. Θ1 requires no new edges in the proﬁciency
model fragment, but both Θ2 and Θ3 do. For example, Task 2 demands an
edge between θ2 and θ4, which were independent in the original digraph. We
refer to this phenomenon as an induced dependency. The moralized and trian-
gulated undirected graph for the proﬁciency model fragment, with additional
edges required to conform with the footprints of the three evidence mode frag-
ments, is shown in Fig. 5.16. The moralized, triangulated, and conformable
undirected graphs for the evidence model fragments are shown in Fig. 5.17.
Fig. 5.16 Moralized proﬁciency model graph for three-task test
Reprinted from Almond et al. (2010) with permission from ETS.
Fig. 5.17 Moralized evidence model fragments for three-task test
Reprinted from Almond et al. (2010) with permission from ETS.

5.4 Application to Assessment
143
5.4.3 Calculation with Fragments
Once junction trees and potential tables have been constructed for each frag-
ment in the manner described in the preceding sections, the PMEM algorithm
(Almond et al. 1999) can update the PMF with evidence coming from any
Task j in ﬁve steps:
Update Step 1: Start with the junction tree for the evidence model for Task j.
Calculate the marginal distribution over Θj, pold(Θj). If the
junction tree was calculated according to the method of the
previous section, this should be the unit potential, 1.
Update Step 2: Cast the obtained evidence in the form of potentials over
the observable variables and combine this evidence with the
existing potentials over the observable nodes. This produces
pnew

xj1, . . . , xjnj

.
Update Step 3: Apply the belief-updating algorithm to obtain the new joint
distribution over the observables and footprint of Task j:

xj1, . . . , xjnj, Θj

⇒Θk = pnew (Θj) .
Update Step 4: The message EMF ⇒PMF will be pnew (Θj) ⊘pold (Θj) =
pnew (Θj) (as the denominator is the unit potential). Enter
this value as virtual evidence in any clique node in the junction
tree for the PMF which contains the footprint, Θj.
Update Step 5: Apply belief updating to update the remaining proﬁciency
variables outside Θj, if there are any.
At the end of the PMEM updating, the PMF contains the posterior dis-
tribution over the proﬁciency variables given the evidence from Task k. The
EMF is no longer needed and can be discarded (or recycled for use with
another student). In fact this gives us a simple Computer Adaptive Testing
(CAT) engine. To start, the PMF contains the prior distribution over the proﬁ-
ciency variables for a student. As each observation arrives about an examinee,
the engine fetches the appropriate EMF for that task from a database. The
PMEM algorithm is then used to update the PMF. At any time the PMF
can be queried to give our current state of knowledge about the student’s
proﬁciency. Chapter 13 expands on this idea.
Occasionally, we want to be able to forecast the values for observable out-
come variables we have not yet seen. (Chapter 7 describes several applications
of this capability.) We can run the PMEM algorithm backwards to obtain the
predictive distribution for observables in another not-yet-administered task,
Task j′:
Predict Step 1: Start with the distribution of the proﬁciency variables after
evidence from previous tasks has been entered. In the update
steps above, this is pnew (θ1, . . . , θm). Marginalize down to Θj′
to obtain pnew (Θj′).

144
5 Eﬃcient Calculations
Predict Step 2: There will be a clique node in EMF j′ corresponding to Θj′. It
will be the unit potential (unless some other source of evidence
has already been used to update it). Marginalize down to Θj′
to obtain pold (Θj′). Combine the result with pnew (Θj′).
Predict Step 3: Apply the belief updating algorithm to update the remain-
ing variables in the EMF junction tree, including in particu-
lar the observable variables

Xj′1, . . . , Xj′nj′

. Marginalizing
down to them gives the predictive distribution of the not-yet-
administered observable variables in Task j′.
This use of the PMEM algorithm has a big advantage over the procedure
of producing the total graphical model for the assessment that contains every
observable for every task. In the latter conﬁguration, the belief-updating algo-
rithm propagates messages to all of the clique nodes to update these predictive
distributions even when they are not needed (although modern Bayes net soft-
ware usually uses lazy-propagation algorithms that only calculate messages
in response to queries). In the PMEM algorithm, only the PMF is updated
by default. The EMFs are only updated on demand, to answer a particular
question.
Example 5.10 (Example 5.9, continued). By the way it was constructed
in Example 5.9, Update Step 1 of marginalizing the initial potential in EMF1
with respect to θA and θB does yield the unit potential for pold(θA, θB). When
evidence arrives that X1 = 1, Update Step 2 tells us to zero out the right
column (for X1 = 0) in Eq. 5.9. Update Step 3 gives the left column as
pnew(θA, θB). In Update Step 4, the message pnew(θA, θB) ⊘old(θA, θB) is
simply pnew(θA, θB) since pold(θA, θB) = 1. It is combined with the initial
potential for for {θA, θB} to produce the new belief about θA and θB:
[.012, .098, .098, .792]⊗[.99, .90, .90, .01] = [.012, .088, .088, .008].
This is the same updated potential for {θA, θB} we obtained in Example 5.9
using the message passing algorithm. There are no other proﬁciency variables
in the PMF, so nothing further needs to be done in Update Step 5.
We are now in a position to forecast the response to Item 2 in light of hav-
ing observed a correct response to Item 1. Predict Step 1 is to marginalize the
new status of the PMF down to the footprint of the task in question. Noth-
ing really needs to be done, since in this small example Θ1 = Θ2 ={θA, θB}.
In Predict Step 2, we marginalize the potential in EMF2 down to the foot-
print and again obtain pold(θA, θB) = 1. We combine this with pnew(θA, θB)
to obtain [.012, .088, .088, .008]; this is the message to pass to EMF2. Predict
Step 3 combines the message [.012, .088, .088, .008] with the initial potential
in EMF2 (Eq. 5.10) to produce an updated potential, which reweights expec-

5.5 The Structure of a Test
145
tations about X2 in accordance with our new belief about θA and θB:
⎡
⎢⎢⎣
.012 .012
.088 .088
.088 .088
.008 .008
⎤
⎥⎥⎦⊗
⎡
⎢⎢⎣
.99 .01
.90 .10
.90 .10
.01 .99
⎤
⎥⎥⎦=
⎡
⎢⎢⎣
.011 .005
.004 .426
.080 .046
.000 .041
⎤
⎥⎥⎦.
Marginalizing down to X2 gives [.095, .518] and normalizing gives [.155, .845].
That is, P (X2 = 1 | X1 = 1) = .155 and P (X2 = 0 | X1 = 1) = .845.
5.5 The Structure of a Test
In the previous section, we moved from viewing an assessment as a giant
Bayesian network to viewing it as a library of network fragments: a central
fragment based on the proﬁciency model and a collection of evidence models
for each task that could be potentially used. Another evidence-centered design
(ECD) model, the assembly model controls which tasks an examinee actually
sees, and thus, what constitutes a valid form of the assessment.
This view of the assessment is helpful for task design. As the test designers
focus on each task, they concentrate on the evidence provided by that task. By
the local independence property, they only need to worry about evidence from
one task; the evidence from other tasks should be conditionally independent,
given proﬁciency variables.
However, the library of fragments view is not as useful for considering the
evidence from an assessment as a whole. Assessment designers need to know
if a given form is properly balanced so as to provide evidence about all of the
claims to be made. To answer this question, the test designer must look across
many fragments all at once.
An alternative way of viewing a graph is to use a matrix. Each row rep-
resents an observable variable and column represents a proﬁciency variable.
Put a one in the matrix everywhere there is an edge in the graph; that is,
for each instance where a given proﬁciency variable is a direct parent of a
given observable. Doing that with the total graphical model of an assess-
ment leads in a straightforward way to the Q-matrix, a representation that
has been popular with many ways of modeling diagnostic assessment (Fischer
1973; Haertel 1989; Haertel 1984; Leighton and Gierl 2007; Rupp et al. 2010;
Suppes 1969; Tatsuoka 1983). The Q-matrix provides the kind of view of the
assessment that does help the designer answer questions about the balance
and evidentiary properties of the assessment as a whole, or of one particular
form.
Section 5.5.1 deﬁnes the Q-matrix more formally for an important sub-
set of assessments, those which consist solely of discrete items (e.g., multiple
choice), that each have a single, conditionally independent, observable out-
come. Section 5.5.2 talks about how to expand the basic Q-matrix notation

146
5 Eﬃcient Calculations
for assessments consisting of more complex tasks with multiple observables.
Chapter 7 will look at how to use this new notation to assess the amount of
evidence in a particular assessment.
5.5.1 The Q-Matrix for Assessments Using Only Discrete Items
This book has been careful to use the term task rather than item to remind
readers that assessment tasks can be more than just a collection of multiple-
choice items. However, tasks which yield only a single observable outcome
variable are easy to work with from a lot of diﬀerent perspectives. In this
section, we will restrict our attention to tasks which have a single observable
outcome variable. The next section will talk about how to lift that limit.
Let Θ = {θ1, . . . , θM} be the set of proﬁciency variables in our assessment.
Let X1, . . . , Xj be the set of observable outcome variables associated with the
items. Note that by our assumption above each observable is associated with
a diﬀerent task. In particular, that means by the local independence property
Xj is independent of Xj′ given Θ.
Now consider the footprint for Task j, that is, the parents of Xj. Let
qjm = 1 if θm is a parent of Xj and 0 if it is not. Tatsuoka (1983) calls the
matrix of qjs for a set of items the Q-matrix of a assessment. This matrix
provides an at-a-glance view of the assessment. For example, the column sum

j qjm is the number of tasks that tap the proﬁciency variable θm. Section 6.4
shows an example. An assessment that has only a single proﬁciency variable
will have a Q-matrix that consists of just a single column of ones.
Consider a Q-matrix for which the sum of each row is 1; that is, each
observable has only a single proﬁciency parent. Such a test is said to have
simple structure. In particular, it can be thought of as a collection of unidi-
mensional tests, one for each proﬁciency variable, that are combined in some
way. Adams et al. (1997) refers to this as between items multidimensionality,
in contrast to within items multidimensionality where at least some observable
variables depend on more than one proﬁciency variable.
According to the construction algorithm in Sect. 5.4.2, the proﬁciency
variables indicated in each row of Q must appear in a common clique in
the ﬁnal undirected graph for the proﬁciency model for the assessment. This
is forced at the level of the PMF digraph by drawing an edge between all
indicated θs in a row, where the edge is from the θ earlier (i.e., closer to the
root) in the recursive representation to the one later. These are the induced
dependencies mentioned earlier.
An interesting if unpleasant consequence is that even EMFs with simple
structures in themselves can force many edges to be added to the graph for
the PMF, and increase the treewidth of the junction tree for the proﬁciency
model. For example, suppose there are ﬁve independent proﬁciency variables
and ﬁve items, each of which has only two parents, as indicated by the Q-
matrix in Table 5.9.

5.5 The Structure of a Test
147
Table 5.9 Q-Matrix for design leading to saturated model
θ1 θ2 θ3 θ4 θ5
x1,1 1 1 0 0 0
x2,1 1 0 1 0 0
x3,1 1 0 0 1 0
x4,1 1 0 0 0 1
x5,1 0 1 1 0 0
x6,1 0 1 0 1 0
x7,1 0 1 0 0 1
x8,1 0 0 1 1 0
x9,1 0 0 1 0 1
x10,1 0 0 0 1 1
This design induces an edge between every pair of θs, and forces all the θs
into a single large clique. This is called the saturated model, and the belief-
updating algorithm oﬀers no computational advantage over the brute-force
algorithm for such models. This example is small enough that computational
demand of the brute force example would not be an issue. But with a larger
proﬁciency model, it is easy to imagine that allowing an unconstrained number
of EMFs with unconstrained patterns of proﬁciency variable parents could
easily lead to cliques and thus potential tables of an intractable size. Some
ways to avoid this problem include the following:
•
Limit the size of the proﬁciency model. (This option is often quite practical
as limited test time forces test designers to concentrate on a few variables
of interest.)
•
Neglect minor θ-to-X edges.
•
Predetermine a set of EMF structures and their footprints (i.e., motifs),
ensure its computability, and constrain task development to those struc-
tures.
If designs that would require large saturated or nearly saturated junction
trees for the PMF are desired nevertheless, the alternative approximations
discussed in Sect. 5.6 can be pressed into service.
Recall from Chap. 2 that many tasks can be generated from a single task
model. Usually, all of those tasks are scored using the same, or similarly struc-
tured, evidence models. This implies that the rows in the Q-matrix corre-
sponding to the tasks from the same evidence model will be identical. For the
purposes of determining the induced dependencies in the proﬁciency model,
it is suﬃcient to have a single row in the Q-matrix for each motif.
5.5.2 The Q-Matrix for a Test Using Multi-observable Tasks
The Q-matrix provides a nice compact view of the entire assessment when all
of the tasks are discrete items with only a single observable outcome variable.

148
5 Eﬃcient Calculations
However, much of our interest in Bayes nets stems from their ability to model
more complex tasks with multiple observables. How do we extend the Q-
matrix notation to include tasks with multiple observables? There are basically
two options: (1) make the rows of the Q-matrix correspond to tasks/evidence
models, and (2) make the rows of the Q-matrix correspond to observables.
Building a Q-matrix of the ﬁrst type is straightforward. Each row of the Q-
matrix becomes the representation of the footprint for that task. This version
of the Q-matrix is particularly useful for scanning through the dependencies
induced by a collection of tasks on a test form. The test designer can scan
through the list quickly and estimate the treewidth of the ﬁnal model.
Using the one row per task representation, the Q-matrix for the network
depicted in Fig. 5.13 is:
Table 5.10 Q-Matrix for Fig. 5.13, one row per task
θ1 θ2 θ3 θ4 θ5
Task 1 1 0 0 0 0
Task 2 0 1 0 1 0
Task 3 0 1 1 1 0
The alternative is to use one row for each observable in the task, giving
only the proﬁciency variables of that observable in the matrix. Table 5.11
depicts the Q-Matrix for the model in Fig. 5.13 using the one row per observ-
able representation. However, the variable X23, which is used to model local
dependence between X21 and X22 presents some problems. First it produces
a row with no “1”s, which is somewhat odd. Second, neither the relationship
between X23 and X21 nor the relationship between X23 and X22 are captured
in the Q-matrix.
Table 5.11 Q-Matrix for Fig. 5.13, one row per observable
θ1 θ2 θ3 θ4 θ5
x11 1 0 0 0 0
x21 0 1 0 0 0
x22 0 1 0 1 0
x23 0 0 0 0 0
x31 0 1 1 1 0
Thus, we can see that in the tasks with multiple observables, we are bump-
ing up against the limits of the Q-matrix representation. Bayesian networks
have a richer set of tools to describe complex relationships among variables.
That is a large part of our interest in using them. Still, the Q-matrix is good

5.6 Alternative Computing Algorithms
149
for providing an at-a-glance summary of an entire assessment form. For this
reason, we often compute the Q-matrix for a given Bayesian network model
as a way of evaluating the assessment.
5.6 Alternative Computing Algorithms
The preceding sections introduced a method for eﬃcient calculation in Baye-
sian networks. This is by no means an exhaustive treatment of the topic,
but it does lay a foundation for the use of Bayesian networks in the following
chapters. A large and active research community has grown up around the use
of Bayesian networks. This last section provides some pointers into the body
of literature that community has developed. In particular, Jensen (1996) and
Cowell et al. (1999) provide recent tutorial references. Pearl (1988) remains
a classic in the ﬁeld and has many chapters that anticipated current research
trends. Lauritzen (1996) describes how to extend the algorithm to graphical
models which include normally distributed continuous variables and mixtures
of discrete and continuous variables.
The algorithm presented in the previous sections is a variant of the propa-
gation algorithm described in Lauritzen and Spiegelhalter (1988). That paper
spawned a large family of algorithms for propagating in Markov trees. The
variant presented here ﬁrst appeared in Cowell et al. (1993).
Shenoy and Shafer (1990) show how the algorithm can be extended to
models which are not purely probabilistic, including inﬂuence diagrams and
graphical belief functions. They deﬁne a general version of the belief-updating
algorithm called the fusion and propagation algorithm, which is based on val-
uations. A valuation describes the relationship about the states of a set of
variables in a frame of discernment, the prime example being the probability
potential we have been working with. Valuations support the combination and
projection operations that are the basis of message passing. They, in combi-
nation with the running intersection property, provide for eﬃcient updating
algorithms.
Section 5.6.1 notes variants on the basic algorithm and Sect. 5.6.2 explores
variations of the algorithm and approximation techniques that can be used
when the treewidth of the model gets too large.
The Uncertainty In Artiﬁcial Intelligence (UAI) community is constantly
doing research into better algorithms for computation with Bayesian networks,
and recent copies of the UAI proceedings are a good source for keeping up
with the latest developments Many software packages exist which implement
both variations on the basic Markov tree propagation algorithm and many
of the variants described here. Appendix A contains some pointers to online
resources for both software and articles.

150
5 Eﬃcient Calculations
5.6.1
Variants of the Propagation Algorithm
By making small changes in the fusion and propagation algorithm we can
adapt it for a number of special purposes. In particular, we can use it to ﬁnd
the most likely proﬁciency proﬁle and to sample from the joint distribution
of all variables.
Most Likely Conﬁguration
Pearl (1988) notes that by simply using maximization instead of summation
in the belief-updating algorithm, one can ﬁnd the most likely conﬁguration
of a set of nodes. The idea is that any time we perform a marginalization
operation (going from a clique node to an intersection node in the Junction
tree, we pick the value of of the eliminated variable(s) that maximizes the
probability. When one reaches the end of the Markov tree, pick the most
likely conﬁguration for the remaining variables.
In the educational setting, we are usually interested in conﬁgurations of
proﬁciency variables. Thus the output of this algorithm is usually the pro-
ﬁciency proﬁle which provides the most likely explanation for the observed
evidence. Section 7.1.2 takes up this idea.
Sampling Algorithm
A junction tree representation can be used to sample from the joint distribu-
tion of all of the variables in the model. First, pick any node in the Markov tree
as a root node. Draw a sample conﬁguration from those variables, in accor-
dance with the probabilities its potential implies. Now condition the model on
the sampled values and start propagating out from the root. At each new node
in the junction as we move out, we have the correct conditional distribution
based on the sample so far. Sample the remaining variables in this node and
continue propagating outwards until all values are sampled.
This can be very useful for calculating the accuracy of a proposed assess-
ment design (Sect. 7.5). In a typical experiment, we start by simulating a set
of proﬁciency variables for a simulee. This produces a proﬁciency proﬁle for
the simulee. Next, for each task in the assessment design we can sample a
set of observed outcomes given the conditional distribution of the observables
given the proﬁciency variables. The result is an observation vector for that
simulee. This kind of simulation experiment has a number of diﬀerent uses,
such as calculating traditional reliability indices, item statistics, and expected
score distributions, and ﬁtting alternative models to the generated data.
5.6.2 Dealing with Unfavorable Topologies
The Peeling Algorithm
If we are only interested the marginal distribution of a particular set of vari-
ables, we can sequentially eliminate the remaining variables one by one. At

5.6 Alternative Computing Algorithms
151
each step, we combine all of the potentials involving the eliminated variable
and project the result onto the remaining variables. The procedure is called
peeling. Hilden (1970) and Cannings et al. (1978) ﬁrst developed the peeling
algorithm to answer questions about genetic probabilities in complex pedi-
grees.
The disadvantage of peeling compared to the Markov tree propagation
algorithm is that it can produce only one marginal distribution at a time;
producing other marginal distributions requires repeeling the model, possi-
bly many times. In such cases, it usually makes sense to transform to the
Markov tree. On the other hand, if we know precisely which query we want
to make peeling can take advantage of special structures in the model (Li and
D’Ambrosio 1994).
Cut Set Methods
In both the Markov tree propagation algorithm and the peeling algorithm, the
cost of the computation is largely determined by the treewidth of the graph.
What can we do when that largest clique gets too large for these algorithms to
be practical? One suggestion presented in Pearl (1988) is to choose a variable
or set of variables to “condition out” of the model. Because these variables are
typically chosen to cut loops, they are called a cut set. The best cut variables
are usually roots of the original directed graph (or close to the roots).
For example, suppose we have a proﬁciency model consisting of an Overall
Proﬁciency variable which has three levels and several subskills. We choose
the Overall Proﬁciency variable (usually the root in the model graph) as our
cut set. We make three new graphical models by conditioning on the three
possible values of the overall proﬁciency variable. Because we have conditioned
on this variable, it can be eliminated from the model. If, as typically happens,
the overall proﬁciency variable is the apex of a number of loops, the resulting
conditional models will have lower treewidth. We now build a junction tree for
each of those models. We weight the resulting trees by the original probabilities
of the overall probability model. The resulting model is a weighted mixture
of trees.
Not only do the conditional models eliminate the cut variable, but they
also could have diﬀerent graphical forms (Heckerman 1991). For example,
there may be a dependency between certain skills at high levels of proﬁciency
which is not typically observed at lower levels of proﬁciency.
The update algorithm for the mixture of trees is straightforward. We start
by updating each tree in the normal fashion. We next update the weights.
This is done in the normal fashion using Bayes rule with the likelihood of
the particular observation. Any query we make is a weighted average of the
queries from each of the conditional models.
Loopy Belief Propagation
Pearl (1988) suggests that one could simply apply the propagation for trees in
loopy graphs iteratively with every node send a message to its neighbors every

152
5 Eﬃcient Calculations
cycle. In the case of polytrees, this algorithm always converges. In the case
of graphs with loops, the algorithm may or may not converge. Weiss (2000)
attempts to characterize situations in which the algorithm will converge and
produce proper marginal distributions, and Murphy et al. (1999) attempt to
validate these situations empirically. Murphy suggests that when loopy belief
propagation does not converge it oscillates between two or more states. This
might correspond to the explaining away problem mentioned earlier, where the
system would oscillate between two modes indicating alternative explanations
for the observed evidence.
Variational Approximations
Jordan (1998) and Jaakkola (2001) describe an approximate inference method
for Bayes nets based on calculus of variations. The basic idea is that the
Bayes net is approximated by another Bayes net with a lower treewidth, and
calculus of variations is used to ﬁnd the optimal approximation. The usual
approximation described is the mean ﬁeld method, in which the approximating
graph assumes that all of the variables are independent. This approximation is
usually pretty close on the marginal distribution of the variables but sacriﬁces
information about the interactions.
Particle Filtering
Sequential importance sampling (Liu 2001), sometimes known as particle ﬁl-
tering can also be used to calculate approximate posterior distributions. Par-
ticle ﬁltering can be done in the following steps:
1. Sample a collection of skill proﬁles, conﬁgurations of proﬁciency variables,
according to the prior distribution (this is given by the proﬁciency model).
These are the “particles.” Assign each of them equal weight.
2. As each new piece of evidence arrives, adjust the weights by multiplying
by the likelihood of that evidence given the proﬁciency state described by
the particle.
3. After a period of time, the weights of some particles will become very
small. At this point in time, resample from the existing particles using
their current weights.
Koller and Learner (2001) and Murphy and Russell (2001) describe par-
ticle ﬁltering to handle calculations in dynamic Bayesian networks (Bayesian
networks which capture changes to a system over time). There is an additional
step at each time point as we put the particles through a random growth oper-
ator. Thus in the dynamic Bayes net model, the particles represent trajectories
through the proﬁciency states at various points in time.

5.6 Alternative Computing Algorithms
153
Exercises
5.1. Assume that we have built a Bayesian network with proﬁciency vari-
ables S1, . . . , SK and observable outcome variables X1, . . . , Xm. For simplic-
ity, assume that all variables are binary. Assume that a learner about whom
we do not have information other than the fact that this learner is from the
population for whom the test is designed, has just walked into the test center.
For each of the following questions, write a symbolic expression that answers
the question.
a. Marginal Belief “What is the probability that the learner has skill S1?”
b. Marginal Belief 2 “What is the probability that the learner will have a
good outcome on observable X1?”
c. Conditional Belief “What is the probability that the learner has skill S1
given we have made observations on tasks X = {X1, . . . , Xn}?”
d. Hypothetical Belief ”Given that we have made observations on tasks X =
{X1, . . . , Xn}, if we observed a performance on Task Y , how would our
beliefs change?”
5.2. Reverse the direction of the arrow between W and X in Fig. 5.2. How
do the calculations in Sect. 5.2.1 change?
5.3. Reverse the direction of the arrow between Z and Y in Fig. 5.2. How do
the calculations in Sect. 5.2.1 change?
5.4. Starting from the ﬁnal state of the junction tree in Example 5.3, calculate
the eﬀect of evidence e2 that Y = 0. Does the marginal distribution for W
change? Why or why not?
5.5. Figure 5.3 gives the junction tree that corresponds to the factorization
of P (U, V, X, Y, Z). Draw the intermediate steps of a directed graph for the
distribution, the factorization hypergraph, and its 2-section.
5.6. Change the performance rating procedure for the Dental Hygienist assess-
ment in Example 5.4, by adding a second rater. Assume that the two raters
work independently, but all rate the same performance. What does the graph
look like (include nodes for the ratings)? How about for N raters?
5.7. Suppose that the rater in Example 5.4 instead of calling a history ade-
quate or not, gives a probabilistic rating, such as “the probability of this
history being adequate is 2/3.” How should that evidence be entered into the
model?
5.8. Consider the Dental Hygienist Exam of Example 5.4. Suppose that
through training we can increase the sensitivity and speciﬁcity of the raters,
so that we have a perfect rater that always rates adequate performances as
adequate and inadequate performances as inadequate. What is the maxi-
mum value for the posterior probability of the dental hygiene skill after the

154
5 Eﬃcient Calculations
patient history task? Note that one way to increase the accuracy of the rat-
ing is to have more and more raters rate the same performance. What is the
maximum change in the posterior probability we can get by increasing the
number of raters?
5.9. Imaging that we want to add a task to the assessment described in Exam-
ple 5.6 (Table 5.6) that had good diﬀerential diagnosis for θA, but not θB.
What would its conditional probability table need to look like?
5.10. In Fig. 5.4, replaced the directed edge T →U with U →T and replace
X →Y with Y →X. What is the size of the largest clique in the moral graph
for the modiﬁed graph? What is the treewidth of the junction tree? Why is
the one smaller than the other?
5.11. When the PMFs and EMFs are constructed separately, after moral-
ization and before triangulation, additional edges are added to join together
proﬁciency variables that appeared together in the footprint of one or more
evidence fragments. This step was not included when constructing the total
graphical model for the assessment (that is the proﬁciency and evidence model
fragments are together in one big graph). Why was it not necessary there?
5.12. A design committee for a new assessment identiﬁes six proﬁciencies,
θ1, . . . , θ6, which it wants to measure with a new assessment. The committee
structures the proﬁciency model so that all six proﬁciencies are independent
given an overall proﬁciency labeled θ0. They propose a collection of nine tasks,
each of which tap two proﬁciencies as show in Table 5.12.
a. Draw the graph for this example.
b. Calculate the treewidth for this assessment.
c. How would you recommend the committee reduce the treewidth of the
assessment? Hint: Are all of the tasks necessary?
Table 5.12 Q-Matrix for proposed assessment (Exercise 5.12)
θ1 θ2 θ3 θ4 θ5 θ6
Task 12 1 1 0 0 0 0
Task 23 0 1 1 0 0 0
Task 34 0 0 1 1 0 0
Task 45 0 0 0 1 1 0
Task 56 0 0 0 0 1 1
Task 61 0 0 0 0 0 1
Task 14 1 0 0 1 0 0
Task 25 0 1 0 0 1 0
Task 36 0 0 1 0 0 1

5.6 Alternative Computing Algorithms
155
5.13. Suppose that we have an assessment described by a PMF and a small set
of EMFs. Suppose further that the assessment design calls for each examinee
to see each task. Section 5.6 presents a simple algorithm for sampling from a
junction tree. Explain how that sampling algorithm can be extended to work
when the PMF and EMFs are maintained separately.
5.14. Let C be the set of clique nodes in a junction tree, and let I be the set
of intersection nodes. Let pn(·) be the potential over Node n. The condition
for the junction-tree algorithm can be expressed as
P(·) =
 
CinC
pC(·)

⊘
 
IinI
pI(·)

.
That is, the joint probability of all the variables is the product of all of the
potentials over the clique nodes, divided by the product of the potentials over
the intersection nodes.
Use the simple chain graph in Fig. 5.2 and demonstrate that this is correct.
Hint: Use the fact that p(x, y)/p(x) = p(y|x) and use that to recover the
original recursive deﬁnition of the joint probability.

6
Some Example Networks
This chapter will illustrate the calculations shown in Chap. 5 with speciﬁc
examples. It shows the basic ways that belief updating, or propagation, is
used in discrete Bayes nets applications for assessment. The conditional prob-
abilities are taken as known for now, as subsequent chapters will address just
where they come from (to anticipate: theory, design, expert opinion, and data).
In educational assessment, the objective is to uncover the relationships
between the students’ unobservable characteristics and the observable out-
comes from students’ performance on tasks. The methods in the previous
chapters allow us to answer these questions. Once the probability models are
built and embedded in a graphical structure that reﬂects our knowledge about
interrelationships among the variables, we can propagate evidence through the
model and make inferences about a student.
As we discussed in Chap. 5, updating for the full joint distribution using
the deﬁnitional expression of Bayes theorem is often prohibitively expensive,
even for relatively small numbers of variables. A model with 15 variables and
4 values for each variable already means working with a joint probability table
with over a trillion entries. Several computer software packages exist to help
with this task. Appendix A presents a brief summary of several commonly
available software application for doing this task as well as instructions for
where to download some of the examples used in this chapter.
The goal of this chapter is to describe how, with the help of Bayes net soft-
ware, to build and use Bayesian networks as the scoring engine for an assess-
ment. Section 6.1 begins with a simple item response theory (IRT) model
example translated into its Bayesian network equivalent. Section 6.2 expands
the example in Sect. 6.1 to include a context eﬀect that is similar to the
testlet-eﬀect IRT model (Bradlow et al. 1999). In this model, some items in
a particular context are correlated with each other beyond their joint depen-
dence on the proﬁciency variable. Section 6.3 illustrates three combination
distributions for use when an observable outcome variable has more than one
parent: the compensatory, conjunctive, and disjunctive distributions. Parallel
models using the three distribution types provide a mechanism for studying
c⃝Springer Science+Business Media New York 2015
157
R. G. Almond et al., Bayesian Networks in Educational Assessment,
Statistics for Social and Behavioral Sciences, DOI 10.1007/978-1-4939-2125-6 6

158
6 Some Example Networks
the behavior of the three distributions in application. Section 6.4 shows a
more complicated educational assessment example drawn from real data, a
binary-skills measurement model with multiple, intercorrelated, proﬁciency
variables.
6.1 A Discrete IRT Model
A very common case in educational testing is an assessment designed to assess
a single proﬁciency. Usually the domain of tasks is restricted to the kind of
simple items that can be unambiguously scored as right or wrong. Multiple-
choice items are natural in this context, but short constructed-response tasks
such as ﬁll in the blank items are often used as well (especially, if the evidence
rules are to be applied by a human rater). In this case, the rules of evidence
are very simple and most of the work in building an evidence model goes into
determining the strength of the relationship between the observable outcome
variable and the proﬁciency model.
As this simple case is very common, a large number of psychometric
approaches have been developed to model it. The most widely used is IRT
(Hambleton et al. 1991; Thissen and Wainer 2001). By convention, the single
proﬁciency variable1 in the IRT model is called θ. The observable outcome
variables, one for each task or item2, are called Xj and usually take on the
values 0 (for incorrect responses) and 1 (for correct responses).
A fundamental assumption of the IRT model is the local item independence
property, that is, Xj ⊥⊥Xj′|θ. Using this assumption, we can write the joint
probability distribution over both the proﬁciency and evidence variables as:
P(X1, . . . , XJ, θ) = P(θ)
J

j=1
Pj(Xj|θ) .
(6.1)
From the previous chapters, it is readily apparent that this factorization struc-
ture can be represented with a graphical model, such as the one in Fig. 6.1.
The IRT model is an example of the more general graphical model rather
than a discrete Bayesian network. This is because, typically, in IRT the proﬁ-
ciency variable θ is continuous. Not only that, but the direction of the arrows
goes from the continuous to the discrete variables, so it does not fall into the
computationally convenient class of conditional Gaussian networks (Lauritzen
1992; Lauritzen 1996), for which all integrals involved in the the fusion and
propagation algorithm (described in Chap. 5) can be solved in closed form. Of
course this is old news in the ﬁeld of psychometrics, where a large number of
approximate methods have been developed for working with the IRT model.
1 This is sometimes called a proﬁciency parameter in the IRT literature, but in this
book we use variable to emphasize that the value is speciﬁc to a person.
2 In this context each task consists of a single discrete item and so the terms task
and item can be used interchangeably.

6.1 A Discrete IRT Model
159
Approximating the continuous proﬁciency variable, θ, with a discrete vari-
able, for example, restricting θ ∈{−2, −1, 0, 1, 2}, makes all variables discrete
and creates a Bayesian network. As all variables are discrete, there are no inte-
grals that need to be solved numerically when scoring students. This approx-
imation may not even be that bad. Haberman (2005a) compares a ﬁve level
ordered latent class model (the simple Bayesian network model posed here is
essentially an ordered latent class model) to a unidimensional IRT model and
notes that both models ﬁt the chosen data sets equally well. Furthermore,
an instructor using the inferences from the network may not be concerned
with ﬁner distinctions. A student for whom θ = 0 is doing about as well as
expected. A student for whom θ = −2 is clearly in need of extra attention
and a student for whom θ = +2 could beneﬁt from extra curricular work.
Students for whom θ = −1 should be watched closely to make sure they do
not slip further down and students for whom θ = +1 could be stimulated to
try and move them further up the scale.
There are a number of diﬀerent variants of the basic IRT model depending
on how the evidence model, Pj(Xj|θ), is parameterized. Technically speaking,
the evidence model also contains the rules of evidence, but those are often
quite simple, e.g., matching the key with the selected option. They are typi-
cally left in the background in the IRT literature, and attention focuses on the
more interesting part of the evidence model, namely, the probability of the
observable, given the proﬁciency variable. The example below uses the Rasch
model (Rasch 1960), which uses the following probability function:
P(Xj|θ, βj) =
1
1 + e−(θ−βj) .
(6.2)
The parameter βj is called the diﬃculty of the item. Note that the diﬃculty
paralmeter and the proﬁciency variable are on the same scale. A person whose
proﬁciency exactly equals the diﬃculty of the item would have a 50–50 chance
of getting that item correct.
Although the Rasch model was built to work with continuous proﬁciency
variables, we can use it to ﬁll out the conditional probability tables (CPT)
which drive the Bayes net approximation to the IRT model. In particular, by
plugging the values θ = −2, −1, 0, 1, 2 into Eq. 6.2 we get the values for each
row of the tables for Fig. 6.1. For Item 3, for example, β3 = 0, so probabilities
of a correct response at the ﬁve θ levels are .1192, .2689, .5000, .7311, and
.8088. The following example illustrates this idea.
Example 6.1 (Classroom Math Quiz). Consider a simple math quiz con-
sisting of ﬁve items scored to yield observable values of Right and Wrong. Let
θ represent a student’s proﬁciency in the math knowledge and skills that this
set of items taps. The teacher is interested in drawing inferences about the
math proﬁciency of each student based on the observed score patterns from
the quiz. Figure 6.1 shows the Bayesian network for this ﬁve item quiz.

160
6 Some Example Networks
Fig. 6.1 Model graph for ﬁve item IRT model
Reprinted with permission from ETS.
To make the model discrete, restrict the proﬁciency variable to fall in the
set θ ∈{−2, −1, 0, 1, 2}. Further assume that the proﬁciency of the students in
the class is distributed with a triangular distribution, with 40 % of the students
at the 0 level, 20 % of the students at both the +1 and −1 levels, and 10 %
of the students at both the −2 and +2 levels. Suppose further the chance
of a student answering the item correctly follows the Rasch model (Eq. 6.2)
and that the items and range from easy (Item 1) to hard (Item 5) with β =
−1.5, −0.75, 0, 0.75, 1.5. Table 6.1 contains the conditional probabilities of a
correct response for this Bayes net.
Table 6.1 Conditional probabilities of a correct response for the ﬁve-item IRT
model
θ Prior
Conditional Probability
θ
Item 1 Item 2 Item 3 Item 4 Item 5
−2 0.1
0.3775 0.2227 0.1192 0.0601 0.0293
−1 0.2
0.6225 0.4378 0.2689 0.1480 0.0759
0 0.4
0.8176 0.6792 0.5000 0.3208 0.1824
1 0.2
0.9241 0.8520 0.7311 0.5622 0.3775
2 0.1
0.9707 0.9399 0.8088 0.7773 0.6225

6.1 A Discrete IRT Model
161
6.1.1 General Features of the IRT Bayes Net
Figure 6.1 and Table 6.1 provide enough information to specify the Bayesian
network for Example 6.1. Although this model is small enough that one could
do all of the calculations in this section by hand, it is much more convenient
to use a computer. As the software can react quickly to new data and other
changes in the model, properly designed Bayesian network software encourages
the modeler to explore the model by posing a large number of hypothetical
questions. Appendix A describes how to obtain many of the more popular
Bayes net software packages, many of which have free student or research ver-
sions which are suitable for following along with this example. The appendix
also lists where copies of the example network can be downloaded (although
this example is small enough to enter by hand).
Most Bayesian network software operates in two modes: a model construc-
tion/editing mode and a model manipulation or inference mode. In the model
construction mode, the analyst performs the following steps:
1. Construct a node for every variable in the model. The number and names
of the variables possible states must be speciﬁed. Various software pack-
ages provide places for specifying other details about the node (e.g., a
deﬁnition).
2. Draw edges between the nodes to represent the conditional dependence
relationships inherent in the model.
3. Specify a CPT for each node in the model, given its parents in the graph.
If a node has no parents, an unconditional probability table is used.
Although the modeling software oﬀers considerable freedom in what order
those three steps are completed, they must be completed for every node in
the model before the model itself is “complete.” Once the model is complete,
the model is compiled, i.e., a junction tree is built from the completed model.
This junction tree is then used to support inference. If the model is later
edited (e.g., another node is added), the compilation step must be repeated
for the revised model.
After the compilation step, most Bayesian network software packages
immediately display the marginal distributions for all nodes (some packages
require you to speciﬁcally query the nodes you want to display). Figure 6.2
shows the result of compiling the Bayes net from Example 6.1 in the Netica
(Norsys 2004) software package. Table 6.2 gives the conditional probabilities
in a slightly more legible format.
These marginal probabilities represent our best predictions of how a stu-
dent who we know nothing about, other than that the student is representative
of the population for which the model was built, would do on this quiz. (They
are calculated from the conditional probabilities of correct response given θ
and the initial marginal distribution for θ, using the law of total probability,
Eq. 3.4). These are the expected proportions correct (P+) on each item in a
population of students for whom this quiz is designed.

162
6 Some Example Networks
Fig. 6.2 The initial probabilities for the IRT model in Netica. The numbers at the
bottom of the box for the Theta node represent the expected value and standard
deviation of Theta
Reprinted with permission from ETS.
Table 6.2 Initial marginal probabilities for ﬁve items from IRT model
Item 1 Item 2 Item 3 Item 4 Item 5
Right 0.77
0.65
0.49
0.35
0.23
Wrong 0.23
0.35
0.51
0.65
0.77
6.1.2 Inferences in the IRT Bayes Net
One important reason for using Bayesian network software, instead of simply
doing these calculations by hand, is that the software will nearly instantly
(especially with such a small model) update the distributions to take the new
evidence into account. The basic operation is called instantiating the value
of a variable. To do this, select the variable in the model and chose a value
for that variable (The details of how this is done are similar but diﬀerent
for diﬀerent Bayes net packages. In particular, many packages have an “auto-
update” mode which will immediately propagate evidence to other nodes in
the model so that they immediately reﬂect the changes, and a “batch” mode
in which propagation of evidence must be requested manually). This basic

6.1 A Discrete IRT Model
163
facility can be used to both enter data and play “what-if” games for various
hypotheses.
The most basic application of this idea is the scoring procedure for
Bayesian networks. Out of the evidence identiﬁcation process (item level
scoring) will come the values of a collection of observables (in this case,
right/wrong judgments) for the variables in our model. To score the student,
simply instantiate the nodes in the graph corresponding to the observable vari-
ables with the values from evidence identiﬁcation. The updated graph shows
the posterior distribution for the proﬁciency variables, now conditional on the
responses that have been observed.
Example 6.2 (Absorbing Evidence from Simple Math Quiz, Exam-
ple 6.1 continued). Suppose that Ali, a student in this class, has observed
outcomes of Item 2 = Right and Item 3 =Right. Instantiating these value in
the network and propagating the information using the methods of Chap. 5
produces the posterior distribution for Ali’s θ (Fig. 6.3a). It has shifted toward
higher probabilities for higher values of θ. The probabilities for Ali to get the
other items “Right” also increase as a consequence.
Ali’s classmate Yuri also answers two items correctly, this time Items 3
and 4. The state of the network after instantiating the values of these observ-
ables is shown in Fig. 6.3b. The shift toward higher values of θ is stronger
because the two items that Yuri answered correctly were more diﬃcult than
the ones Ali answered. Although the story here is much the same as in Ali’s
case, we will see an interesting diﬀerence in the next section.
Although in practice, the proﬁciency variables are unobservable, the Bayes
net software allows us to hypothesize values for the latent variables in order
to play out various scenarios. The following example illustrates this idea.
Example 6.3 (Calculating Conditional Probabilities for Simple Math
Quiz (Example 6.1 continued)). In the class for which the quiz was
designed, consider the groups of students corresponding to the ﬁve proﬁciency
levels (We don’t actually ever know students’ proﬁciencies, of course, so we
can not form these groups for, say, small group instruction. The best we could
do is to form groups based on what we do know about their proﬁciencies—for
example, the modal value of their posterior distributions after they have taken
a quiz). Suppose that Group A corresponds to θ = 2, Group B corresponds to
θ = 1, and so forth. What is the expected performance on each of the items
for a member of Group B?
In the IRT Bayes net, instantiate θ = 1, then propagate the probabilities
through the Bayes net. This yields probabilities as in Fig. 6.4). This is the
fourth row of Table 6.1. Group A corresponds to the ﬁfth row of that table
and Group C to the third row.

164
6 Some Example Networks
Item_3
Right
Wrong
 100
   0
Item_3
Right
Wrong
 100
   0
Item_4
Right
Wrong
48.6
51.4
Item_4
Right
Wrong
48.6
51.4
Item_5
Right
Wrong
33.4
66.6
Item_5
Right
Wrong
33.4
66.6
Item_2
Right
Wrong
 100
   0
Item_2
Right
Wrong
 100
   0
Item_1
Right
Wrong
87.1
12.9
Item_1
Right
Wrong
87.1
12.9
Theta
+2
+1
 0
-1
-2
21.0
34.4
37.5
6.49
0.73
0.683 ± 0.9
Theta
+2
+1
 0
-1
-2
21.0
34.4
37.5
6.49
0.73
0.683 ± 0.9
Item_3
Right
Wrong
 100
   0
Item_3
Right
Wrong
 100
   0
Item_4
Right
Wrong
 100
   0
Item_4
Right
Wrong
 100
   0
Item_5
Right
Wrong
37.9
62.1
Item_5
Right
Wrong
37.9
62.1
Item_2
Right
Wrong
80.9
19.1
Item_2
Right
Wrong
80.9
19.1
Item_1
Right
Wrong
89.3
10.7
Item_1
Right
Wrong
89.3
10.7
Theta
+2
+1
 0
-1
-2
28.9
37.7
29.4
3.65
0.33
0.911 ± 0.87
Theta
+2
+1
 0
-1
-2
28.9
37.7
29.4
3.65
0.33
0.911 ± 0.87
a
b
Fig. 6.3 a Student with Item 2 and 3 correct. b Student with Item 3 and 4 correct
Reprinted with permission from ETS.

6.1 A Discrete IRT Model
165
Item_3
Right
Wrong
73.1
26.9
Item_3
Right
Wrong
73.1
26.9
Item_4
Right
Wrong
56.2
43.8
Item_4
Right
Wrong
56.2
43.8
Item_5
Right
Wrong
37.8
62.3
Item_5
Right
Wrong
37.8
62.3
Item_2
Right
Wrong
85.2
14.8
Item_2
Right
Wrong
85.2
14.8
Item_1
Right
Wrong
92.4
7.59
Item_1
Right
Wrong
92.4
7.59
Theta
+2
+1
 0
-1
-2
   0
 100
   0
   0
   0
1
Theta
+2
+1
0
-1
-2
   0
100
   0
   0
   0
1
Fig. 6.4 Probabilities conditioned on θ = 1
Reprinted with permission from ETS.
Example 6.3 is a rather trivial application of this conditioning idea; Exer-
cise 6.3 extends the model a bit further. This technique is more interesting in
models with more than one proﬁciency variable. Here, conditioning on a sin-
gle proﬁciency variable will have subtle inﬂuence on both the values of other
proﬁciency variables and the implications of evidence from integrated tasks
that tap more than one proﬁciency.
One application of this technique is in validating a Bayesian network.
In this simple example, it can be used to verify that the probabilities from
Table 6.1 were correctly entered. In more complex models it can be used with
domain experts to validate properties of the model. For a given proﬁciency
proﬁle—that is, instantiating values of all of the proﬁciency variables—look
at the predicted observables on each task and verify with the experts that this
distribution is reasonable.
Examples 6.2 and 6.3 also show how Bayesian networks support both
deductive and inductive reasoning. Example 6.3 is an example of deductive
reasoning: The network reasons from the known value of the proﬁciency vari-
able to deduce the likely consequences. Example 6.2, on the other hand, is

166
6 Some Example Networks
an example of inductive reasoning: From the known values of the observed
outcomes, the network induces a posterior distribution for the unknown pro-
ﬁciency.
Another important application of conditioning on the unknown proﬁciency
variable is calculating the expected weight of evidence for an unobserved item
or task. Section 7.2 deﬁnes this idea formally, but in the context of the sim-
ple discrete IRT example, there is a simple short cut. In this case, we can
lean on a result from IRT theory that says that under the Rasch model, an
item for which a student has a 50–50 chance of getting right will provide the
most information.3 Therefore, looking for the item whose predictive probabil-
ity, given the evidence so far, is closest to 50–50 produces a simple adaptive
testing mechanism (The situation become much more complex when there are
multiple proﬁciency variables, see Chap. 7).
Example 6.4 (Item Selection for Simple Math Quiz, Example 6.2
continued). Recall Ali from Example 6.2, who got Items 2 and 3 correct.
Now, what items should we give Ali next? The responses Item 2 = Right and
Item 3 = Right provide some evidence about Ali’s proﬁciency. Entering this
information into the Bayes net (Fig. 6.3a) has updated not only our posterior
distribution over theta, but also the predictive distributions for Items 1, 4,
and 5. The updated probability of getting a Right outcome is .87 for Item 1,
.48 for Item 4, and .33 for Item 5. Thus for what we currently know about
Ali, Item 4 appears to be the best choice; Item 1 is too easy and Item 5 is too
hard.
6.2 The “Context” Eﬀect
The example in the previous section recreates results from IRT, or because θ
was made discrete, from a comparable structured latent class model. It shows
how results from more familiar testing models can be expressed in terms of
Bayesian networks. It does not yet highlight the strength of Bayesian networks,
which is their ﬂexibility.
One way to put that ﬂexibility to use is in modeling more complex tasks,
not just simple discrete items. Natural examples of complex tasks are plentiful:
reading passages followed by several multiple choice (or short answer) items,
multistep tasks, and simulation tasks which are scored on multiple aspects
of the same performance. In such contexts, it is more natural to think of a
3 It may seem counterintuitive that an item for which a student has 50–50 chances
of responding correctly provides the most information about her θ. The way to
think of it is, “Which item provides the biggest diﬀerence between the posteriors
that result if she gets it right or gets it wrong?”

6.2 The “Context” Eﬀect
167
deﬁned collection of material presented to the examinee and the work prod-
ucts obtained in the performance as a single task, with multiple observable
outcomes, as opposed to a collection of items that is somehow bound together.
A problem with such complex tasks is the observable outcome variables are
likely to be dependent, even after conditioning on the relevant proﬁciency vari-
ables. In the usual IRT framework, where all items are assumed to be locally
independent, given the proﬁciency variables, the observable variables from
these complex tasks violate that assumption (Yen 1993). However, because the
evidence-centered design (ECD) framework calls for a single evidence model
to score all observables coming from a single task, a more complex Bayes net
model can be built to account for the local dependence.
One source of local dependence is common stimulus material shared by
several items. A common example is a reading passage followed by several
multiple-choice items. Although the items are typically authored to minimize
the dependency among the items, the topic of the passage still might produce
an eﬀect on the overall performance of the examinee. An examinee who is
unfamiliar with the topic of the passage is likely to have a more diﬃcult time
with all of the items, where an examinee who is familiar with the topic will
be able to read it more quickly and retain more details in working memory.
One trick that can be used in such situations is to introduce a variable,
often called Context, that represents familiarity with the topic of the passage
(or other stimulus material). This variable is made a parent of all of the
observable outcome variables in just that task. Thus, even after conditioning
on the proﬁciency variable, the observable outcomes within this task are still
dependent. As the dependent outcomes are all within a single evidence model,
the local independence property in the Bayes net model is not violated at
the level of tasks (Note that Context is associated with the student, much
as a proﬁciency variable. We will discuss shortly why it is not included in
the proﬁciency model). The Context variable approach can also be used as a
mathematical device to induce dependency among observables even when the
cognitive model of topic familiarity is less appropriate. Examples are ratings
of the same performance by multiple judges, ratings of several aspects of the
same complex performance, and observables that share the same format or
come from the same work product.
Example 6.5 (Story Problem in the Math Quiz (Example 6.1 con-
tinued)). Story problems are popular with Math instructors because they
test the student’s ability to recognize and apply the mathematical principle
being tested in real world circumstances. On the other hand, the story of the
story problem also introduces irrelevant information, namely, the topic and
the details of the story, into the problem solving experience. If multiple prob-
lems depend on the same story, then how well the student comprehended the
background story can have an eﬀect on all of the items related to the story.
Suppose that in the math quiz , Item 3 and Item 4 are story problem
items that depend on the same story. In this case, we can expect that if the

168
6 Some Example Networks
student has trouble understanding the story, then both Item 3 and Item 4 will
be aﬀected. To model this relationship, we add a node Context to the graph
of Fig. 6.1, and make it a parent of both Item 3 and Item 4. Figure 6.5 shows
the results. The Context variable takes two possible values U, or Unfamiliar,
for students who have diﬃculty relating to the content of the story, and F,
or Familiar, for students who do not. The CPT for Item 3 and Item 4 need
to be modiﬁed to take the new parent variable into account. The diﬀerence
between the probabilities for F and U for given θ is the strength of the context
eﬀect. For each level of θ, we made the conditional probability of a correct
response about .10 higher if Context = F and about .10 lower if Context = U.
Table 6.3 shows the modiﬁed values. Finally, we assign a 50-50 probability to
the two states of the Context variable.
Fig. 6.5 Five item IRT model with local dependence
Reprinted with permission from ETS.
Call the original version of the math quiz with no story problem (as
described in Example 6.1) Form A. Call the variant version with the added
story problem Form B. It is interesting to compare what the inference about
a student would be on the two forms.
Consider again the responses of the student Ali, who got Item 2 (not in
the set) and Item 3 (in the set) correct. Figure 6.6a shows the inferences
graphically. Note that when only one item from the set is observed, that the
eﬀect of the Context variable is averaged out and the inferences are the same
on both Forms.
Now consider the response vector of Yuri, who got Item 3 and Item 4
correct. Figure 6.6b shows the inferences on Form B. Comparing the posterior
distribution of θ here with the one from Form A with the same responses
(Fig. 6.3b), the posterior distribution in Form B is a little less concentrated

6.2 The “Context” Eﬀect
169
Item_3
Right
Wrong
 100
   0
Item_3
Right
Wrong
 100
  0
Item_4
Right
Wrong
50.4
49.6
Item_4
Right
Wrong
50.4
49.6
Item_5
Right
Wrong
33.4
66.6
Item_5
Right
Wrong
33.4
66.6
Item_2
Right
Wrong
 100
   0
Item_2
Right
Wrong
 100
  0
Item_1
Right
Wrong
87.0
13.0
Item_1
Right
Wrong
87.0
13.0
Theta
+2
+1
 0
-1
-2
21.0
34.3
37.5
6.52
0.74
0.683 ± 0.9
Theta
+2
+1
0
-1
-2
21.0
34.3
37.5
6.52
0.74
0.683 ± 0.9
Context
Familiar
Unfamiliar
58.9
41.1
Context
Familiar
Unfamiliar
58.9
41.1
Item_3
Right
Wrong
 100
   0
Item_3
Right
Wrong
 100
  0
Item_4
Right
Wrong
 100
   0
Item_4
Right
Wrong
 100
  0
Item_5
Right
Wrong
37.2
62.8
Item_5
Right
Wrong
37.2
62.8
Item_2
Right
Wrong
80.2
19.8
Item_2
Right
Wrong
80.2
19.8
Item_1
Right
Wrong
88.8
11.2
Item_1
Right
Wrong
88.8
11.2
Theta
+2
+1
 0
-1
-2
28.2
36.8
29.9
4.43
0.77
0.871 ± 0.9
Theta
+2
+1
0
-1
-2
28.2
36.8
29.9
4.43
0.77
0.871 ± 0.9
Context
Familiar
Unfamiliar
68.6
31.4
Context
Familiar
Unfamiliar
68.6
31.4
a
b
Fig. 6.6 a Student with Item 2 and Item 3 correct with context eﬀect. b Student
with Item 3 and Item 4 correct with context eﬀect
Reprinted with permission from ETS.

170
6 Some Example Networks
Table 6.3 New potentials for Item 3 and Item 4, conditioned on Context
Parents
Item 3
Item 4
θ Context
Right Wrong Right Wrong
−2
F
0.2192 0.7808 0.1601 0.8399
U
0.0192 0.9808 0.0001 0.9999
−1
F
0.3689 0.6311 0.2480 0.7520
U
0.1689 0.8311 0.0480 0.9520
0
F
0.6000 0.4000 0.4208 0.5792
U
0.4000 0.6000 0.2208 0.7792
1
F
0.8311 0.1689 0.6622 0.3378
U
0.6311 0.3689 0.4622 0.5378
2
F
0.9088 0.9120 0.8773 0.1227
U
0.7088 0.2192 0.6773 0.3227
and has not moved quite as far from the prior distribution. The Context
variable here oﬀers an alternative explanation for Yuri’s performance, thus
the amount of evidence that the two observations provide jointly about θ is
less than it is in Form A.
This example makes sense from an evidentiary perspective. With the
two independent items, the only possible explanation for correct answers is
increased proﬁciency (higher θ). Therefore all of the evidence goes toward
the proﬁciency. When the two items are part of the same larger task (in this
example, the story problem and its two items form a task), then the task
speciﬁc Context variable forms an alternative explanation. This diverts some
of the evidence and so the joint evidence from dependent observations is less
than that from independent observations. Chapter 7 talks about evidence in
a more formal way. For context eﬀects speciﬁcally, Chap. 10 discusses testing
for their presence, and Chaps. 14 and 15 show how they arose naturally from
task design and are estimated from pilot data in the Biomass example.
The exact degree to which evidence is diverted depends on the relative
sizes of the inﬂuences from the proﬁciency (θ) and Context variables on the
observable outcomes. If the Context variable has a large inﬂuence on the
outcomes then the decrease in evidence will be large. If the Context variable
has a more modest eﬀect, then the decrease in evidence will be more modest
as well.
This example illustrates some of the ﬂexibility that makes Bayesian net-
works attractive as a model for assessments. Grouping observables that depend
on common stimulus together into a task supports more complex models of
dependencies among those observables. Recall that standard IRT requires the
rather strong local independence property that all items (observables) are
conditionally independent given the single proﬁciency variable. The Bayes
net model illustrated here, like the IRT testlet model (Bradlow et al. 1999)

6.2 The “Context” Eﬀect
171
uses the weaker local independence property that observables (items) from
diﬀerent tasks are conditionally independent given the proﬁciency variable(s).
Within the evidence model for a single task, there is considerable freedom
as to how to model the dependence among observables. The Context variable
model described here is just one possibility; Almond et al. (2006b) compare
several possible models. Ideally, domain experts and psychometricians should
pick an evidence model for each task which is based on a theory about how
students solve the problem. In practice, the Context variable model is often
used because it is easy to articulate, and it has roughly the right eﬀect of
decreasing the joint evidence from observables from the same task.
The Context variable has a peculiar place in the evidence-centered assess-
ment design (ECD) framework because it is neither a proﬁciency variable
residing in the proﬁciency model nor is it an observable outcome variable
residing in the evidence model for a particular task. Instead it represents
“proﬁciency” variable that is local to a particular task. As such it resides in
the evidence model for that task. Unlike conventional proﬁciency variables, we
are usually not interested in its value; rather it is a nuisance variable whose
value must be estimated along the way, then marginalized out, in order to esti-
mate the proﬁciency variables whose values form the basis of the assessment’s
claims.
The interpretation of the Context variable as a proﬁciency speciﬁc to the
task highlights the problems that can arise when the distribution of this vari-
able is not uniform across the tested population. For example, if the story in
the story problem was about boat racing, then students who lived near large
bodies of water and students from wealthy families who vacation near large
bodies of water would be expected to have F Context more often than poor
students living in inland communities. This would be a poor choice of story
for a large, standardized test because it raise issues of fairness. On the other
hand, a classroom teacher might reasonably expect all of the students to be
familiar with the story context because they had just completed reading a
nautical adventure story in their literature class.
In the math quiz example, there is another possible cognitive explanation
for the Context variable, namely that it represents reading comprehension. If
all the other items on the math quiz are expressed algebraically, then read-
ing comprehension would be a common skill between these two items. While
familiarity with boats is not a proﬁciency that is related to the claims of the
math quiz, the ability to recognize mathematical terms embedded in natural
language could be. As such, it is probably better placed as a second variable
in the proﬁciency model, which will be reported on, than as a variable local
to the task. Both interpretations have the same eﬀect on the evidence for the
overall mathematics proﬁciency, but the latter supports inferences about the
speciﬁc skill of extracting mathematical information from natural language.

172
6 Some Example Networks
6.3 Compensatory, Conjunctive, and Disjunctive Models
The simple IRT example was restricted to a single proﬁciency variable parent.
Part of the ﬂexibility of the Bayesian network is that it allows for multiple
proﬁciency variable parents. However, as soon as there are more than one
parent variable, the question arises “how do the skills required for this task
interact to make the probability of a successful outcome?” There are three
commonly used models for CPT where there are more than one proﬁciency
variable as the parent of an observable outcome:
•
Conjunctive Distribution—This is the case where all skills are necessary
for a high probability of a successful outcome. Because this corresponds to
a logical “and” of the input proﬁciencies, this model is sometimes called a
noisy-and model. The “noise” comes because the relationship is not perfect
but probabilistic. If the proﬁciency variables have more than two levels,
then the conjunctive model assumes that the student behaves at the level
of the weakest skill. For this reason, the model is also sometimes called a
noisy-min model.
•
Disjunctive Distribution—This is the case where the parent skills repre-
sent alternative ways to solve the task. Presumably examinees choose the
approach based on their strongest skills and the probability of success is
determined by the strongest skill. This is sometimes called a noisy-or or
noisy-max.4
•
Compensatory Distribution—In this model having more of one skill will
“compensate” for having less of another. In this case the probability of
success is determined by the sum of the skills, possibly a weighted sum.
This is sometimes called an additive model.
When using Bayesian networks, there is no need to choose a single model
for skill interaction that holds across all observable outcome variables. The
choice of model is determined for each obervable by how the CPT for that
variable is set up. The analyst can mix and match any of the three types
of distribution in a single Bayesian network, or build other distributions for
special situations. Chapter 8 describes some possible parameterizations for
this kind of model. This section compares simple examples of each type of
model to help develop intuition for when and where they should be used.
Figure 6.7 shows the three kinds of conditional probability distribution in
a series of simple parallel models. In this directed hypergraph notation the
variables are shown as rounded rectangles, and the CPT are shown as square
boxes. Each of the boxes is labeled with an icon depending on the type of
4 Although in educational testing conjunctive models are more common than dis-
junctive model, in other applications of Bayes nets noisy-or models are more
common than noisy-and, and there is a considerable literature on the topic (for
example, D´ıez 1993; Srinivas 1993; Pearl 1988). Fortunately, the two models are
symmetric so translating the results is fairly straightforward.

6.3 Compensatory, Conjunctive, and Disjunctive Models
173
Fig. 6.7 Three diﬀerent ways of modeling observable with two parents
Reprinted with permission from ETS.
distribution. The plus sign is used for the compensatory (additive) model.
The symbols in the boxes for the conjunctive and disjunctive distributions
are the symbols used for AND-gates and OR-gates in logical diagrams. The
advantage of this directed hypergraph notation is that the type of relationship
is obvious from the picture; in the more usual directed graph notation, one
needs to open the CPT to determine the type of distribution.
The three models are designed to be close parallels of each other. They
have the following characteristics in common:
•
There are two proﬁciency variables as parent nodes (P1 and P2), and the
two proﬁciencies are independent of each other (before making observa-
tions).
•
The priors for the proﬁciency nodes are the same for the three models
with a probability of 1/3 for each of the high (H), medium (M), and low (L)
proﬁciency states.
•
The initial marginal probability for observable variable Obs is the same
for the three models (50/50). (Fig. 6.8)
The diﬀerence comes in how the conditional probability distribution
P(Obs|P1, P2) is set up. Table 6.4 gives the probabilities for the three dis-
tributions. The easiest way to approach this table is to start in the middle
with the row corresponding to both parent variables in the middle state. For
the compensatory distribution when either skill increases, the probability of
success increases by .2, and when either skill decreases, the probability of suc-
cess decreases by a corresponding amount. For the conjunctive distribution
both skills must increase before the probability of success increases, but a
drop in either skill causes a decline in probability. The opposite is true for
the disjunctive distribution. The probability of the middle category needs to

174
6 Some Example Networks
Initial Probabilities
−0.4
−0.2
0.0
0.2
0.4
0.6
P1
P2
Obs
P1
P2
Obs
P1
P2
Obs
Compensatory
Conjunctive
Disjunctive
0.33
0.33
0.33
0.33
0.33
0.33
0.5
0.5
0.33
0.33
0.33
0.33
0.33
0.33
0.5
0.5
0.33
0.33
0.33
0.33
0.33
0.33
0.5
0.5
Fig. 6.8 This ﬁgure shows the probabilities for all three models side by side. Each bar
represents the marginal probability of one of the variables in one of the models. The
length of the fragment give the probability of a particular state from best (highest
and lightest) to worst (lowest and darkest). The bars are oﬀset so that the extent
below the line gives the probability of being in the lowest category and the extent
above the line give the probability of being above the lowest category. The y−axis
shows amount of probability of being below the line as negative and the probability
of being above as positive
Reprinted with permission from ETS.
be adjusted slightly to get the marginal probability of success to be .5 for all
three distributions.
Table 6.4 Conditional probabilities for the three distributions.
Parent state
P(Obs = Right)
P1
P2
Compensatory Conjunctive Disjunctive
H
H
0.9
0.9
0.7
H
M
0.7
0.7
0.7
H
L
0.5
0.3
0.7
M
H
0.7
0.7
0.7
M
M
0.5
0.7
0.3
M
L
0.3
0.3
0.3
L
H
0.5
0.3
0.7
L
M
0.3
0.3
0.3
L
L
0.1
0.3
0.1
Obs is the observable outcome variable in each of the three models

6.3 Compensatory, Conjunctive, and Disjunctive Models
175
Eﬀects of Evidence
Suppose we observe the value Right for the outcome variable Obs in all three
models. Figure 6.9a shows the posterior probabilities after adding this evi-
dence. In all three cases, the probability mass shifts toward the higher states,
however, more mass remains at the L level in the disjunctive model. While
the compensatory and conjunctive models have the same probability for the
low state, the eﬀect is slightly diﬀerent for the highest state, here the compen-
satory model shifts slightly more probability mass toward the highest state.
These minor diﬀerences are as much a function of the adjustments to the
probabilities needed to get the diﬃculties to match as they are diﬀerences in
the way the three distribution types behave.
Observations = Right
−0.2
0.0
0.2
0.4
0.6
0.8
1.0
P1
P2
Obs
P1
P2
Obs
P1
P2
Obs
Compensatory
Conjunctive
Disjunctive
0.2
0.33
0.46
0.2
0.33
0.46
1
0.2
0.37
0.42
0.2
0.37
0.42
1
0.24
0.28
0.46
0.24
0.28
0.46
1
Observations = Wrong
−1.0
−0.5
0.0
0.5
P1
P2
Obs
P1
P2
Obs
P1
P2
Obs
Compensatory
Conjunctive
Disjunctive
0.46
0.33
0.2
0.46
0.33
0.2
1
0.46
0.28
0.24
0.46
0.28
0.24
1
0.42
0.37
0.2
0.42
0.37
0.2
1
a
b
Fig. 6.9 a Updated probabilities when Observation = Right. b Updated probabil-
ities when Observation = Wrong
Reprinted with permission from ETS.
If the observed outcome value is Wrong instead of Right similar eﬀects
work in the opposite directions. Figure 6.9b shows the posterior probabilities
for this case. Now the conjunctive model has the highest probability for the H
high states. Other conclusions follow as well with the H and L low proﬁciency
states and conjunctive and disjunctive distributions switching roles.

176
6 Some Example Networks
Observations = Right, P1 = H
−0.2
0.0
0.2
0.4
0.6
0.8
1.0
P1
P2
Obs
P1
P2
Obs
P1
P2
Obs
Compensatory
Conjunctive
Disjunctive
1
0.23
0.33
0.42
1
1
0.15
0.36
0.47
1
1
0.33
0.33
0.33
1
a
Observations = Wrong, P1 = H
−1.0
−0.5
0.0
0.5
1.0
P1
P2
Obs
P1
P2
Obs
P1
P2
Obs
Compensatory
Conjunctive
Disjunctive
1
0.55
0.33
0.11
1
1
0.63
0.27
0.09
1
1
0.33
0.33
0.33
1
b
Fig. 6.10 a Updated probabilities when P1 = H and Observation = Right.
b Updated probabilities when P1 = H and Observation = Wrong
Reprinted with permission from ETS.
Eﬀect of Evidence When One Skill is Known
When there are two parent proﬁciencies for an observable outcome variable,
what is known about one proﬁciency will aﬀect inferences about the other.
Suppose that P1 is easy to measure and its state can be determined almost
exactly by an external test. How does knowledge about P1 aﬀect inferences
about P2 under each of the three types of distribution?
Assume that we know (through other testing) that P1 is in the H state.
Figure 6.10a shows the posterior distribution when the observable is Right
and Fig. 6.10b shows the posterior distribution when the observable is Wrong.
The most startling eﬀect is with the disjunctive distribution. The fact that P1
is at the H is a perfectly adequate explanation for the observed performance.
As can be seen from Table 6.4, when P1 is at the H state, the probability of
success is the same no matter the value of P2. Therefore, if P1 = H the task
provides no information whatsoever about P2.
The eﬀect of the additional information about P1 in the conjunctive dis-
tribution is the opposite of its eﬀect in the disjunctive distribution. Given that

6.3 Compensatory, Conjunctive, and Disjunctive Models
177
P1 is at the highest state, the second proﬁciency P2 governs the probability
of success. Therefore the distributions in Fig. 6.10a and b are very diﬀerent.
The compensatory distribution shows a more moderate change, lying between
the posteriors of the conjunctive and disjunctive distributions.
Observations = Right, P1 = M
−0.2
0.0
0.2
0.4
0.6
0.8
1.0
P1
P2
Obs
P1
P2
Obs
P1
P2
Obs
Compensatory
Conjunctive
Disjunctive
1
0.2
0.33
0.46
1
1
0.17
0.41
0.41
1
1
0.23
0.23
0.53
1
Observations = Wrong, P1 = M
−1.0
−0.5
0.0
0.5
1.0
P1
P2
Obs
P1
P2
Obs
P1
P2
Obs
Compensatory
Conjunctive
Disjunctive
1
0.46
0.33
0.2
1
1
0.53
0.23
0.23
1
1
0.41
0.41
0.17
1
a
b
Fig. 6.11 a Updated probabilities when P1 = M and Observation = Right.
b Updated probabilities when P1 = M and Observation = Wrong
Reprinted with permission from ETS.
Now assume that we know (through other testing) that P1 is only in the
M state. Figure 6.11a shows the posterior distribution when the observable is
Right and Fig. 6.11b shows the posterior distribution when the observable
is Wrong. Starting with the compensatory distribution, note that the eﬀect is
similar to when the value of P1 was H, only shifted a bit toward high values
of P2. The conjunctive distribution gives a big swing (between the posteriors
after the two diﬀerent observable values) for the lowest state, but provides no
information to distinguish between the two higher states of P2. This is because
the state of M for P1 provides an upper bound on the ability of the student to
perform the task. Similarly, in the disjunctive distribution the evidence can
distinguish between the highest state of P2 and the others, but provides no
information to distinguish between the lower two states.

178
6 Some Example Networks
6.4 A Binary-Skills Measurement Model
The examples in this chapter so far have been completely artiﬁcial. The ﬁnal
section in this chapter explores a real example. Any real example starts with
a cognitive analysis of the domain, which is a lot of work. For this example we
will borrow an extensive cognitive analysis of the domain of mixed-number
subtraction found in Tatsuoka (1984) and Tatsuoka et al. (1988). This exam-
ple was used by Tatsuoka (1990) as part of the development of the rule space
method, but the description shown here comes from the Mislevy (1995b) adap-
tation of this problem to Bayesian networks.
Section 6.4.1 describes the results of the cognitive analysis of this domain
(Tatsuoka 1984; Tatsuoka et al. 1988). Section 6.4.2 derives a Bayes net model
based on the cognitive analysis. Section 6.4.3 describes how the model is used
to make inferences about students.
6.4.1 The Domain of Mixed Number Subtraction
Tatsuoka (1984) begins with cognitive analyses of middle-school students’
solutions of mixed-number subtraction problems. Klein et al. (1981) identiﬁed
two methods that students used to solve problems in this domain:
•
Method A: Convert mixed numbers to improper fractions, subtract, and
then reduce if necessary.
•
Method B: Separate mixed numbers into whole number and fractional
parts; subtract as two subproblems, borrowing one from minuend whole
number if necessary; then simplify and reduce if necessary.
The cognitive analysis mapped out ﬂowcharts for applying each method
to items from a universe of fraction subtraction problems. A number of key
procedures appear, which any given problem may or may not require depend-
ing on the features of the problem and the method by which a student might
attempt to solve it. Students had trouble solving a problem with Method B,
for example, when they could not carry out one or more of the procedures
an item required. Tatsuoka constructed a test to determine which method a
student used to solve problems in the domain5 and which procedures they
appeared to be having trouble with.
This analysis concerns the responses of 325 students, whom Tatsuoka
(1984) identiﬁed as using Method B, to 15 items in which it is not neces-
sary to ﬁnd a common denominator. These items are a subset from a longer
40-item test, and are meant to illustrate key ideas from Bayes nets analysis in
a realistic, well-researched cognitive domain. Instructional decisions in oper-
ational work were based on larger numbers of items. Figure 6.12 shows the
proﬁciency model for the following skills:
5 Their analyses indicated their students tended to use one method consistently,
even though an adult might use whichever strategy appears easier for a given
item.

6.4 A Binary-Skills Measurement Model
179
Skill 1 Basic fraction subtraction.
Skill 2 Simplify/reduce fraction or mixed number.
Skill 3 Separate whole number from fraction.
Skill 4 Borrow one from the whole number in a given mixed number.
Skill 5 Convert a whole number to a fraction.
All of these skills are binary, that is a student either has or does not have
the particular skill. Furthermore, there is a prerequisite relationship between
Skills 3 and 4: a student must acquire Skill 3 before acquiring Skill 4.
In the rule space method (Tatsuoka 1984; Tatsuoka 1990) it is traditional
to express the relationship between the proﬁciency variables and the observ-
able outcome variables (in this case, whether each problem was correct or
not), through the use of a Q-matrix (Sect. 5.5). Table 6.5 shows the Q-matrix
for the mixed-number subtraction test. All of the models in this example are
conjunctive—all skills are necessary to solve the problem. Note that several
groups of items have identical patterns of required skills. Following ECD nota-
tion, we call a common pattern an evidence model. The column in the table
labeled EM shows the items’ associations with the six evidence models that
appear in the example.
Table 6.5 Q-Matrix for the Tatsuoka (1984) mixed number subtraction test
Skills required
Item
Text
1
2
3
4
5 EM
6
6
7 −4
7
x
1
8
3
4 −3
4
x
1
12
11
8 −1
8
x
x
2
14 3 4
5 −3 2
5
x
x
3
16 4 5
7 −1 4
7
x
x
3
9
3 7
8 −2
x
x
3
4 3 1
2 −2 3
2
x
x
x
4
11 4 1
3 −2 4
3
x
x
x
4
17
7 3
5 −4
5
x
x
x
4
20 4 1
3 −1 5
3
x
x
x
4
18 4 1
10 −2 8
10
x
x
x
4
15
2 −1
3
x
x
x
x
5
7
3 −2 1
5
x
x
x
x
5
19
7 −1 4
3
x
x
x
x
5
10 4 4
12 −2 7
12
x
x
x
x
6

180
6 Some Example Networks
With ﬁve binary skills there are 32 possible proﬁciency proﬁles—assignment
of values to all ﬁve skills. However, the prerequisite relationship reduces the
number of legal proﬁles to 24, since combinations with Skill 3 = No and
Skill 4 = Yes are impossible. Not all 24 proﬁles can be identiﬁed using the
data from the test form described in Table 6.5. For example, there are no tasks
which do not require Skill 1, therefore this form provides no evidence for dis-
tinguishing among the twelve proﬁciency proﬁles which lack Skill 1. This does
not make a diﬀerence for instruction, as a student lacking Skill 1 would be
tutored on that skill and then retested. The test was designed to determine
which of the more advanced skills a student might need further instruction in.
Up to this point, the analysis for the Bayesian network model is the same
kind of analysis that is done for the rule space method (Tatsuoka 1990). It
is in accounting for departures from this ideal model that the two methods
diﬀer. Rule space looks at ideal response vectors from each of the 24 skill
proﬁles and attempts to ﬁnd the closest match in the data. The Bayesian
method requires specifying both a probability distribution over the possible
proﬁciency proﬁles (a proﬁciency model) and a probability distribution for
the observed outcomes given the proﬁciency parents. It is then in a position
to calculate a posterior distribution over each examinee’s proﬁciencies given
their observed responses. The next section describes how that is done in this
example.
6.4.2 A Bayes Net Model for Mixed-Number Subtraction
The ECD framework divides the Bayes net for this model into several frag-
ments. The ﬁrst is the proﬁciency model fragment (PMF) containing only the
variables representing the skills. Then there are 15 separate evidence model
fragments (EMFs), one for each item (task) in the assessment. In order to
specify a Bayes net model for the mixed-number subtraction assessment, we
must specify both the graphical structure and the condition probability tables
for all 16 fragments.
We start with the proﬁciency model. There are only ﬁve binary proﬁciency
variables, making the total number of possible skill proﬁles 32. As this is a
manageable size for a clique, we will not worry about asking the experts for
additional conditional independence statements to try to reduce the treewidth
of the proﬁciency model. Instead, we will just choose an ordering of the proﬁ-
ciency variable and use that to derive a recursive representation for the joint
probability distribution.
Mislevy (1995b) chose the order: Skill 1, Skill 2, Skill 5, and ﬁnally Skill 3
and Skill 4. We leave those two for last because of the prerequisite relationship
between them which requires special handling. Putting Skill 1 ﬁrst makes
sense because normally this skill is acquired before any of the others. This
is a kind of a soft or probabilistic prerequisite, as opposed to the relation

6.4 A Binary-Skills Measurement Model
181
Fig. 6.12 Proﬁciency model for Method B for solving mixed number subtraction
Reprinted with permission from ETS.
between Skill 3 and Skill 4 which is a hard prerequisite; there are no cases
where Skill 4 is present and Skill 3 is absent.
This means that there are only three possible states of the two variables
Skill 3 and Skill 4. To model this, we introduce a new variable MixedNum-
ber which has three possible states: (0) neither Skill 3 nor Skill 4 present,
(1) Skill 3 present but Skill 4 absent, and (2) both Skill 3 and Skill 4 present.
The relationship between the MixedNumber variable and Skill 3 and Skill 4
are logical distributions which consist solely of ones and zeroes.
Figure 6.12 gives the graphical structure for the proﬁciency model. The
structures of the EMFs are given by the rows of Table 6.5. First note that
several rows in that table are identical, in that they use exactly the same
skills. Items 9, 14, and 16, for example, all requires Skills 1 and 3. We have
assigned each unique row an evidence model . Thus, we really only need to
create six EMFs to build the complete model for this short assessment. Items
9, 14, and 16 will all use EMF 3. Later on, we will assign diﬀerent probability
tables to the EMFs for diﬀerent tasks. When we do that we will create indi-
vidual links—task speciﬁc versions of the evidence model—for each task (see
Chap. 13 for details).
The Q-Matrix (Table 6.5) provides most of the information necessary to
build the EMFs. In particular, the parents of the observable outcome variable
(correct/incorrect for the item) are variables checked in the Q-Matrix. The
one additional piece of information we need, supplied by the cognitive experts,
is that the skills are used conjunctively, so the conjunctive distribution is
appropriate. Figure 6.13 shows the EMFs for evidence models 3 and 4.
After constructing the six diﬀerent evidence model fragments and repli-
cating them to make links for all 15 items in the test, we have a collection
of 16 Bayes net fragments: one for the proﬁciency model and 15 (after repli-

182
6 Some Example Networks
Fig. 6.13 Two evidence model fragments for evidence models 3 and 4
Reprinted with permission from ETS.
cation)for evidence model fragments. We can catenate them to produce the
full Bayesian model for the mixed number subtraction test. This is shown
in Fig. 6.14. Although we could use the computational trick described in
Sect. 5.4.1 to calculate probabilities in this model just joining one EMF at a
time to the PMF, the full Bayesian model is small enough to be easily handled
by most Bayes net programs.
All that remains to complete the model is to build a CPT for each variable
in the model. First we must build a CPT for each variable in the proﬁciency
model. Then we must build a CPT for each observable variable in the Evidence
Model Fragments. (In this example, all variables in the evidence models are
either borrowed from the proﬁciency model, and hence do not require a CPT,
or are observable. If we had other evidence model variables, like the Context
variable above, they would require CPTs as well.)
There are basically two sources for the numbers, expert opinion and data.
In this particular case, Tatsuoka (1984) collected data on 325 students. As
mentioned above, Chap. 11 (see also Mislevy et al. (1999a)) tells that part
of the story. The numbers derived from those calculations are the ones used
below.
However, even with only the expert opinion to back it up, the model is
still useful. In fact, the version used in Mislevy (1995b) uses only the expert
numbers. At ﬁrst pass, we could simply assign a probability of .8 for success
on an item if a student has all the prerequisite skills, and a probability of .2
for success if the student lacks one or more skills. Similarly, we could assign

6.4 A Binary-Skills Measurement Model
183
Fig. 6.14 Full Bayesian model for Method B for solving mixed number subtraction
Reprinted from Almond et al. (2010) with permission from ETS.
a prior probability of around 0.8 for students having all the parent skills and
a probability of around 0.2 when they lack one or more of the parent skills.
When there is more than one parent, or more than two states for the skill
variable (e.g., the MixedNumber variable) we interpolate as appropriate.
While such a Bayes net, built from expert numbers, might not be suit-
able for high stakes purposes, surely it is no worse than a test scored with
number right and a complicated weighting scheme chosen by the instructor.
In fact, it might be a little better because at least it uses a Bayesian scheme
to accumulate the evidence (Exercise 7.13). Furthermore, if there are several
proﬁciency variables being estimated, the Bayes net model will incorporate
both direct evidence from tasks tapping a particular proﬁciency and indirect
evidence from tasks tapping related proﬁciencies in providing an estimate for
each proﬁciency. This should make estimates from the Bayes net more stable
than those which rely on just subscores in a number right test.
To give a feel for the structure and the contents of the CPTs behind the
following numerical illustrations, let us look at two tables based on the analysis
in Chap. 11, one for a proﬁciency variable and one for an observable outcome
variable.

184
6 Some Example Networks
Recall that there are associations among the proﬁciency variables. It suf-
ﬁces here to say that the skills are positively associated—having high values
of parent skills makes it more likely that a student will have a high value
for a child skill as well—and to present one example. Skill 5 is modeled as
depending on Skill 1 and Skill 2. This means that there are four conditional
probability distributions for Skill 5, one for each combination of Skill 1 and
Skill 2 values. We estimated these conditional probabilities under the con-
straint that the distributions would be the same for both combinations in
which one prior skill is mastered but the other is not. The CPT for Skill 5
that is built into the network is shown in Table 6.6.
Table 6.6 Conditional probability table for Skill 5
Skill 1 Skill 2 P(Skill 5 = Yes) P(Skill 5 = No)
Yes
Yes
0.748
0.252
Yes
No
0.469
0.531
No
Yes
0.469
0.531
No
No
0.129
0.871
Item 16 is one of three items that uses evidence model 3: The parents of
the observable outcome are Skill 1 and 3, and the relationship is conjunctive.
Again the CPT is composed of four conditional probability distributions. We
estimated them under a constraint common in binary skills models, that there
would be one distribution when both Skill 1 and Skill 3 are Yes and a diﬀerent
distribution common to all combinations in which one one or both required
skills are No. We expect the conditional probability of a correct response to
be higher in the ﬁrst case (The equality constraint of probabilities across the
three proﬁles with a No can be examined using methods discussed in Part II).
Table 6.7 shows the CPT used in the example.
Table 6.7 Conditional probability table CPT for Item 16
Skill 1 Skill 3 P(Item 16 = Right) P(Item 16 = Wrong)
Yes
Yes
0.910
0.090
Yes
No
0.170
0.830
No
Yes
0.170
0.830
No
No
0.170
0.830
6.4.3 Inferences from the Mixed-Number Subtraction Bayes Net
Regardless of how the numbers get into the Bayesian network, the procedure
used to draw inferences from the Bayesian network is the same. There are

6.4 A Binary-Skills Measurement Model
185
two common cases: drawing inferences about proﬁciency variables given the
observed outcomes, and making predictions about observable outcomes given
the hypothesized proﬁciency levels. These are both described below. In both
cases, the ﬁrst step is to enter the conditional probability tables into the
Bayesian network software and compile the network. The network will then
produce the prior probability for a student from this population having each of
the proﬁciencies as well as predictions for each observable variable. Figure 6.15
shows how this looks in the software package Netica (other packages have
similar displays).
Skill1
Yes
No
88.3
11.7
Skill2
Yes
No
61.8
38.2
Skill5
Yes
No
31.3
68.7
MixedNumbers
0
1
2
6.33
53.7
39.9
Skill3
Yes
No
93.7
6.33
Skill4
Yes
No
39.9
60.1
Item8
Yes
No
70.5
29.5
Item12
Yes
No
71.1
28.9
Item10
Yes
No
30.4
69.6
Item11
Yes
No
39.5
60.5
Item17
Yes
No
33.1
66.9
Item18
Yes
No
43.0
57.0
Item20
Yes
No
33.4
66.6
Item15
Yes
No
37.1
62.9
Item19
Yes
No
26.3
73.7
Item4
Yes
No
39.4
60.6
Item7
Yes
No
33.1
66.9
Item14
Yes
No
73.8
26.2
Item16
Yes
No
72.6
27.4
Item9
Yes
No
68.8
31.2
Item6
Yes
No
79.4
20.6
Fig. 6.15 Prior (population) probabilities
Reprinted with permission from ETS.
Scoring a Test
The most basic use of the Bayes net can be described as follows. For each
observed outcome, ﬁnd the corresponding node in the Bayes net and instan-
tiate its value (The details of how to do this diﬀer according to the Bayes

186
6 Some Example Networks
net software, but it generally involves clicking on the node and selecting a
value from a list of possible states for the variable). The instantiated values
are then propagated to the nodes representing the proﬁciency variables whose
marginal distributions are then calculated with some variation of the algo-
rithm in Chap. 5 (Again, depending on the software and chosen options, the
propagation could be manual or automatic).
Example 6.6 (Mixed-Number Subtraction Complete Response). Sup-
pose that a student (whose class uses Method B) takes the mixed-number sub-
traction test, and gets Items 4, 6, 8, 9, 11, 17, and 20 correct and Items 7, 10,
12, 14, 15, 16, 18, and 19 incorrect (Items 1, 2, 3, 5 and 13 were dropped from
the shortened 15 item version of the test). Which skills does this student have?
Skill1
Yes
No
99.5
0.49
Skill2
Yes
No
17.5
82.5
Skill5
Yes
No
1.84
98.2
MixedNumbers
0
1
2
3.41
2.20
94.4
Skill3
Yes
No
96.6
3.41
Skill4
Yes
No
94.4
5.60
Item8
Yes
No
 100
   0
Item12
Yes
No
   0
 100
Item10
Yes
No
   0
 100
Item11
Yes
No
 100
   0
Item17
Yes
No
 100
   0
Item18
Yes
No
   0
 100
Item20
Yes
No
 100
   0
Item15
Yes
No
   0
 100
Item19
Yes
No
   0
 100
Item4
Yes
No
 100
   0
Item7
Yes
No
   0
 100
Item14
Yes
No
   0
 100
Item16
Yes
No
   0
 100
Item9
Yes
No
 100
   0
Item6
Yes
No
 100
   0
Fig. 6.16 Mixed number subtraction: a sample student
Reprinted with permission from ETS.
Figure 6.16 shows the network after instantiating these observed variables
into this network. From this picture, there is a high probability that the stu-
dent has Skills 1, 3, and 4, but low probability that the student has Skills 2
and 5. The second column of Table 6.8 summarizes the results.

6.4 A Binary-Skills Measurement Model
187
Table 6.8 Posteriors after two sets of observations
Node
Initial probability Example 6.6 Example 6.7
Skill 1
0.883
0.995
0.999
Skill 2
0.618
0.172
0.587
Skill 3
0.921
0.950
0.991
Skill 4
0.396
0.929
0.839
Skill 5
0.313
0.032
0.215
Item 6
0.794
1.000
1.000
Item 8
0.705
1.000
1.000
Item 12
0.711
0.000
0.712
Item 14
0.731
0.000
0.849
Item 16
0.719
0.000
0.835
Item 9
0.686
1.000
1.000
Item 4
0.392
1.000
1.000
Item 11
0.393
1.000
1.000
Item 17
0.330
1.000
0.584
Item 20
0.332
1.000
0.606
Item 18
0.428
0.000
0.668
Item 15
0.369
0.000
0.356
Item 7
0.329
0.000
0.000
Item 19
0.262
0.000
0.251
Item 10
0.304
0.000
0.000
The test results need not be complete in order to draw inferences; any sub-
set of the observables can be used to draw inferences about the proﬁciencies
(including the empty set, but that will just reproduce the prior population
levels for the skills). For observable variables whose values are not observed,
we simply leave the node uninstantiated (or select the special value unknown
depending on the software package). The Bayes net updating algorithm auto-
matically uses only observed values. In fact, it will even provide predictions
for the remaining unknown values.
Example 6.7 (Mixed-Number Subtraction, Partial Data). Suppose
that the test is administered on a computer and that a student is midway
through completing the test. Suppose further, that the outcomes observed so
far are correct results for Items 6, 8, 9, 4, and 11 and incorrect results for
Items 7 and 10. Figure 6.17 shows the state of the network after entering this
information.
Even the ﬁrst half of the test is suﬃcient to drive the probability of Skill 1
to close to 1.00. The probability for Skills 3 and 4 have increased over the
prior values, but the probabilities for Skills 2 and 5 have dropped slightly.
The third column of Table 6.8 summarizes the results.
Note that the software automatically produces predictive distributions for
the as–yet–unobserved observable variables. Section 7.2 shows an important
application of this idea in test selection.

188
6 Some Example Networks
Skill1
Yes
No
99.9
.058
Skill2
Yes
No
58.3
41.7
Skill5
Yes
No
22.0
78.0
MixedNumbers
0
1
2
0.67
15.4
84.0
Skill3
Yes
No
99.3
0.67
Skill4
Yes
No
84.0
16.0
Item8
Yes
No
 100
   0
Item12
Yes
No
71.0
29.0
Item10
Yes
No
   0
 100
Item11
Yes
No
 100
   0
Item17
Yes
No
58.5
41.5
Item18
Yes
No
67.8
32.2
Item20
Yes
No
60.7
39.3
Item15
Yes
No
36.0
64.0
Item19
Yes
No
25.4
74.6
Item4
Yes
No
 100
   0
Item7
Yes
No
   0
 100
Item14
Yes
No
85.0
15.0
Item16
Yes
No
83.6
16.4
Item9
Yes
No
 100
   0
Item6
Yes
No
 100
   0
Fig. 6.17 Mixed number subtraction: posterior after 7 Items
Reprinted with permission from ETS.
Predicting Test Results
Although the proﬁciency variables are not directly observable, we can use
the Bayes net software to instantiate them in order to answer hypothetical
questions. This is actually a good method for validating a Bayesian network.
Show the predictions made from the model to an expert in the subject of
the assessment and ask if they are reasonable. Any predictions which seem
unusual are potential problems with the model (The techniques discussed in
the next chapter can help debug the network).
Turning to the mixed-number subtraction problem which has been the
focus of this section, instantiate the value of Skill 1 to Yes. The result can
be seen in Fig. 6.18 and the second column of Table 6.9. The probabilities of
a correct response to all items increases. This is so for two reasons. First, we
know that a requirement for all tasks, Skill 1, has been met. Second, the high
value of Skill 1 increases our belief that the student also has the other skills
because the skills are positively associated in the proﬁciency model.

6.4 A Binary-Skills Measurement Model
189
Skill1
Yes
No
 100
   0
Skill2
Yes
No
66.2
33.8
Skill5
Yes
No
34.0
66.0
MixedNumbers
0
1
2
3.37
53.1
43.5
Skill3
Yes
No
96.6
3.37
Skill4
Yes
No
43.5
56.5
Item8
Yes
No
76.0
24.0
Item12
Yes
No
74.5
25.5
Item10
Yes
No
33.1
66.9
Item11
Yes
No
42.8
57.2
Item17
Yes
No
36.0
64.0
Item18
Yes
No
45.8
54.2
Item20
Yes
No
36.4
63.6
Item15
Yes
No
38.7
61.3
Item19
Yes
No
27.8
72.2
Item4
Yes
No
42.2
57.8
Item7
Yes
No
34.5
65.5
Item14
Yes
No
82.9
17.1
Item16
Yes
No
81.5
18.5
Item9
Yes
No
71.4
28.6
Item6
Yes
No
89.2
10.8
Fig. 6.18 Mixed number subtraction: Skill 1 = Yes
Reprinted with permission from ETS.
Unsurprisingly, conditioning on Skill 1 = No has the opposite result. The
result can be seen in Fig. 6.19 and the third column of Table 6.9. Not only
does the predictive probability for each item drop, but so does the predictive
probability for the remaining skills.
It is possible to condition on any number of proﬁciency variables (or a
complete proﬁciency proﬁle). As a simple example, consider a student who
has Skill 1 but lacks Skill 2. This result can be seen in Fig. 6.20 and the
last column of Table 6.9. The probabilities of getting correct responses on
Items 6 and 8 increase, as do the probabilities for Items 14 and 17. However,
the predictive probability drops for Items 19, 20, and 10. The probability that
the student has Skill 3 drops slightly from the value when conditioning only
on Skill 1=Yes, but there is a substantial drop in the predictive probabilities
for Skill 4 and Skill 5 decrease.

190
6 Some Example Networks
Table 6.9 Predictions for various skill patterns Subtraction assessment
Node
Initial Skill 1=Yes Skill 1=No Skill 1=Yes
& Skill 2=No
Skill 1
0.883
1.000
0.000
1.000
Skill 2
0.618
0.662
0.289
0.000
Skill 3
0.921
0.956
0.650
0.921
Skill 4
0.396
0.432
0.117
0.156
Skill 5
0.313
0.340
0.110
0.100
Item 6
0.794
0.892
0.053
0.892
Item 8
0.705
0.760
0.289
0.760
Item 12 0.711
0.745
0.452
0.452
Item 14 0.731
0.821
0.049
0.792
Item 16 0.719
0.807
0.049
0.779
Item 9
0.686
0.712
0.488
0.703
Item 4
0.392
0.421
0.178
0.265
Item 11 0.393
0.426
0.140
0.243
Item 17 0.330
0.358
0.117
0.204
Item 20 0.332
0.362
0.103
0.196
Item 18 0.428
0.456
0.220
0.305
Item 15 0.369
0.385
0.246
0.258
Item 7
0.329
0.343
0.223
0.234
Item 19 0.262
0.276
0.154
0.165
Item 10 0.304
0.331
0.098
0.098
6.5 Discussion
By this point in the book, we hope you have acquired enough understanding of
how Bayes nets work to try something for yourself. Many of the concepts that
are diﬃcult to explain in words and equations are easy to understand with
the help of Bayes net software that allows you to manipulate the data and
rapidly see the results. We urge you to look at some of the software described
in Appendix A and try out the extended example presented in Appendix A.2.
The next chapter looks at some more advanced uses of the basic computa-
tion algorithms presented in the previous chapter: explaining scores, selecting
tasks in an adaptive test, and test construction. Part II turns to the more
diﬃcult question of how to get the numbers into the Bayesian network.
Exercises
6.1. Suppose that a teacher has a classroom of 25 students, all of whom are
members of the population for which the model in Example 6.1 was built.
Answer the following questions about that model:

6.5 Discussion
191
Skill1
Yes
No
   0
 100
Skill2
Yes
No
28.9
71.1
Skill5
Yes
No
11.0
89.0
MixedNumbers
0
1
2
28.6
58.6
12.7
Skill3
Yes
No
71.4
28.6
Skill4
Yes
No
12.7
87.3
Item8
Yes
No
28.9
71.1
Item12
Yes
No
45.2
54.8
Item10
Yes
No
9.80
90.2
Item11
Yes
No
14.0
86.0
Item17
Yes
No
11.7
88.3
Item18
Yes
No
22.0
78.0
Item20
Yes
No
10.3
89.7
Item15
Yes
No
24.6
75.4
Item19
Yes
No
15.4
84.6
Item4
Yes
No
17.8
82.2
Item7
Yes
No
22.3
77.7
Item14
Yes
No
4.90
95.1
Item16
Yes
No
4.90
95.1
Item9
Yes
No
48.8
51.2
Item6
Yes
No
5.30
94.7
Fig. 6.19 Mixed number subtraction: Skill 1 = No
Reprinted with permission from ETS.
1. For each of the ﬁve items on that quiz, how many students are expected
to get that item correct?
2. For each item, give a range of values for the number of students who
should get that item right such that the teacher should be surprised if the
number of right answers falls outside that range.
Hint: The variance of the binomial distribution with parameters n and p is
np(1 −p).
6.2. Following Examples 6.2 and 6.3, suppose that Ali is a member of
Group B. Start with the network instantiated to θ = 1 and enter the data
that Ali got Item 2 and Item 3 correct. What changes in this network? Why?
6.3. Suppose that for Example 6.1 the school district deﬁnes a student who
has a θ value of 0 or higher as meeting the district standards. Add a node to
the model for Example 6.1 with two values, which represents whether or not

192
6 Some Example Networks
Skill1
Yes
No
 100
   0
Skill2
Yes
No
   0
 100
Skill5
Yes
No
10.0
90.0
MixedNumbers
0
1
2
5.00
78.5
16.5
Skill3
Yes
No
95.0
5.00
Skill4
Yes
No
16.5
83.5
Item8
Yes
No
76.0
24.0
Item12
Yes
No
45.2
54.8
Item10
Yes
No
9.80
90.2
Item11
Yes
No
24.9
75.1
Item17
Yes
No
20.9
79.1
Item18
Yes
No
31.0
69.0
Item20
Yes
No
20.2
79.8
Item15
Yes
No
26.4
73.6
Item19
Yes
No
17.0
83.0
Item4
Yes
No
27.1
72.9
Item7
Yes
No
23.8
76.2
Item14
Yes
No
81.6
18.4
Item16
Yes
No
80.2
19.8
Item9
Yes
No
71.0
29.0
Item6
Yes
No
89.2
10.8
Fig. 6.20 Mixed number subtraction: Skill 1 = Yes, Skill 2 = No
Reprinted with permission from ETS.
the student meets the standards. What are the expected response probabilities
for students who are meeting the standards?
6.4. A common practice in tests scored with IRT is to provide the maximum
likelihood estimate (MLE) for θ; that is, to ﬁnd the value of θ that maxi-
mizes P(X|θ). Using the assessment and Bayesian network model of Exam-
ple 6.1, how can you compute the maximum likelihood estimate for θ under
the assumption that θ ∈{−2, −1, 0, 1, 2}? Hint: You will need to change the
probability distribution in one of the nodes.
6.5. Suppose in Example 6.5 the teacher covers very similar story problems in
class before the quiz. The teacher therefore believes that 95 % of the students
should have the value High for Context. Modify the Bayesian network for
Example 6.5 to reﬂect this. First adjust the CPT for the Context node, and
then adjust the CPTs for Item 3 and Item 4 so that there marginal distribution
matches what is shown in Fig. 6.2. You can do this by adding the diﬀerence
between the observed marginal probability and the desired one to the numbers

6.5 Discussion
193
in Table 6.3. Compare the evidence from this new model to the complete
independence model and the model of Example 6.5.
6.6. Section 6.2 noted that Context variables are usually placed in the evi-
dence model because they are local to a speciﬁc task. Suppose that common
background material would render observables from multiple tasks dependent
(even after conditioning on the proﬁciency variables). How can this be mod-
eled without violating the local dependence property?
6.7. (van der Gaag et al. 2004) experts often believe that a Bayesian network
should be isotonic: as the observable variables move into better states then the
probability of a good outcome should increase. In particular, if e and e′ are two
instantiations of observable outcome variables such that for each component
ek ⪰ek′ then P(S|e) ⪰P(S|e′) . In networks for educational assessment, this
means that if the “score” on a task (or item) increases then the probability
of proﬁciency variable being in a high state should also increase. Verifying
the monotonicity properties can be an important check that a model is build
correctly and accounts for all of the necessary latent variables.
Comparing two probability distributions is a bit tricky, but in the case of
the simple IRT Bayes net of Example 6.1 we can simply compare the expected
value for Theta. We can verify that this network is isotonic by picking a couple
of increasing sequences of probability assignments and verify that the expected
value increases for each assignment. Using 0 and 1 to represent Incorrect and
Correct observed outcomes, consider the two sequences:
1. (0, 0, 0, 0, 0) (1, 0, 0, 0, 0) (1, 1, 0, 0, 0) (1, 1, 1, 0, 0) (1, 1, 1, 1, 0) (1, 1, 1, 1, 1)
2. (0, 0, 0, 0, 0) (0, 0, 0, 0, 1) (0, 0, 0, 1, 1) (0, 0, 1, 1, 1) (0, 1, 1, 1, 1) (1, 1, 1, 1, 1)
Calculate the expected value for Theta for each response pattern in both
sequences. Is this network isotone in θ?
6.8. Repeat Exercise 6.7 for the network with the context variable in Exam-
ple 6.5.
6.9. Section 6.3 described a simple example of each of three types of distri-
butions for tasks involving three skills. Suppose that through an external test
we have established that a particular student is low in P1. Figure 6.21 gives
the posterior distribution if the value of the task observable is Wrong. What
does the posterior distribution for P2 look like for each distribution when the
observed outcome is Right?
6.10. Recall that in Fig. 6.5 each of the items which has the Context variable
has a parent. Therefore it is necessary to choose a type for the CPT linking
Theta and Context to the observable. For each of the following scenarios,
tell whether it would be more appropriate to model the relationship with a
conjunctive, disjunctive, or compensatory distribution:

194
6 Some Example Networks
Observations = Wrong, P1 = L
−1.0
−0.5
0.0
0.5
P1
P2
Obs
P1
P2
Obs
P1
P2
Obs
Compensatory
Conjunctive
Disjunctive
1
0.42
0.33
0.23
1
1
0.33
0.33
0.33
1
1
0.47
0.36
0.15
1
Fig. 6.21 Updated probabilities when P1 = L and Observation = Wrong
Reprinted with permission from ETS.
1. There is an alternative fast method for solving Items 3 and 4 which was
taught on a day in which many students were absent. Students who were
present that day, or did their make-up homework, should be able to use
either the usual or alternative method to solve the problem. The Context
variable represents knowledge of the alternative solution.
2. Items 3 and 4 are part of an extended complex task with complex instruc-
tions. Students who do not understand the instructions are likely to be oﬀ
task for both problems. The Context variable represents understanding
the instructions.
3. Items 3 and 4 are both story problems with the subject taken from the ﬁeld
of physics. Students who have studied physics are likely to have previously
studied these problem types. Here Theta represents the student’s math
ability and Context represents the student’s physics ability.
6.11. Section 6.4.2 introduced the artiﬁcial MixedNumber variable to model
the relationship between Skill 3 and 4. What must the CPT linking Mixed-
Number and Skill 3 look like? The CPT linking MixedNumber and Skill 4?
Hint: As this is a logical probability all of the entries must be either 0 or 1.
6.12. In the mixed-number subtraction example, suppose that Skill 1 was a
hard prerequisite for the other skills instead of a soft one. Speciﬁcally, suppose
that in the CPT for that network we set the probability of having any of the
other skills given that Skill 1 to zero, but make no other changes. Describe
how the new model would diﬀer from the one presented in Sect. 6.4 with
respect to the following queries:
1. The probability that a student has Skill j, j > 1, given no other informa-
tion.
2. The probability that a student will get Item i correct, given no information
about proﬁciencies (or no inferences about other items).
3. The probability that a student has Skill 5, given that the student got
Items 7, 15, and 19 correct.

6.5 Discussion
195
4. The probability that a student has Skill 5, given that the student got
Items 7, 15, and 19 incorrect.
6.13. In Example 6.7, why do the predictive probabilities for Items 15 and 19
decrease slightly after observing the ﬁrst seven items, while the predictive
probabilities for Items 14 and 16 increase?
6.14. Consider once again the student who has Skill 1 but lacks Skill 2
(Fig. 6.20). Why does the predictive probability for Item 16 go up, but the
predictive probability for Item 20 go down?
6.15. What is the expected number of correct score on the mixed-number
subtraction test for somebody who has Skill 1? Who lacks Skill 1? Hint: Use
the values in Table 6.9.
6.16. Pick one of the Bayesian network packages listed in Appendix A and
use it to build the accident proneness example, Example 3.8. Verify the com-
putations in that example.
6.17. Use your favorite Bayesian network package to build the following sim-
ple IRT Bayes net. Assume a single proﬁciency variable, θ with which takes on
the values {−1.5, −0.5, 0.5, 1.5} with prior probabilities {.125, .375, .375, .125}.
Let the test have three items with the conditional probabilities given in
Table 6.10.
Table 6.10 Potentials for Exercise 6.17
Item 1
Item 2
Item 3
θ Right Wrong Right Wrong Right Wrong
−1.5 0.378 0.622
0.182 0.818
0.076 0.924
−0.5 0.622 0.378
0.378 0.622
0.182 0.818
0.5 0.818 0.182
0.622 0.378
0.378 0.622
1.5 0.924 0.076
0.818 0.182
0.622 0.378
Use that network to answer the following questions:
1. What is the probabilities of getting a correct outcome for each item for a
student for whom θ = −1.5?
2. What is the most likely level of θ for a student whose answer to Item 3 is
Right?
3. Is it possible for the student who got Item 3 Right to answer Item 1
Wrong?
6.18. Appendix A.2 shows where to obtain a complete description of the lan-
guage assessment. Build this network using your favorite Bayes net package
and use it to score some possible response patterns.

7
Explanation and Test Construction
For Bayesian network models to be useful in educational applications, they
must not only provide belief estimates for important proﬁciencies and claims
about the learner, but they must also explain the basis of those estimates.
Explanation transforms the model from a black box that pontiﬁcates an
answer to a question into a glass box, whose reasoning methods and assump-
tions can be evaluated. Contrast this to a neural network model that classiﬁes
a learner without being able to explain the rationale behind its conclusion.
Usually, a preliminary model makes several unrealistic assumptions, which
result in unrealistic inferences. Models must be “debugged” like computer pro-
grams, to correct errors in assumption or speciﬁcation (Almond et al. 2013).
The mechanisms used for explanation aid in the process of model validation,
criticism, and debugging.
For assessments constructed using evidence-centered design (ECD; Chap. 2),
it is only natural that the explanation would be in terms of evidence. Each
observed outcome from an assigned task provides “evidence” for or against a
claim represented by one or more proﬁciency variables. But how much? The
weight of evidence quantiﬁes the evidence provided by a single observation,
a complete task, or a complete test. There is a close connection between the
weight of evidence and the reliability of an assessment.
If we have not yet seen the results from a task, we can calculate the expected
weight of evidence (EWOE) for that task. This gives a guide to test construc-
tion for both adaptive and ﬁxed form tests. EWOE is always calculated with
respect to a hypothesis, so we can use it as a spot meter to determine where
an assessment has the most power. We can use expected weight of informa-
tion to make cost/beneﬁt trade-oﬀs and focus on the assessment for particular
purposes, even on the ﬂy in adaptive tests.
Section 7.1 reviews some of the literature on explanation in graphical mod-
els, describing some simple textual and coloring techniques. Section 7.2 for-
mally deﬁnes weight of evidence and provides some of its key properties. Sec-
tion 7.3 describes EWOE as a metric for Activity Selection—selecting the
next task in an adaptive test. Section 7.4 expands on this idea to explore
c⃝Springer Science+Business Media New York 2015
197
R. G. Almond et al., Bayesian Networks in Educational Assessment,
Statistics for Social and Behavioral Sciences, DOI 10.1007/978-1-4939-2125-6 7

198
7 Explanation and Test Construction
issues of test design and construction. Finally, Sect. 7.5 explores the connec-
tion between EWOE and the reliability of the test.
7.1 Simple Explanation Techniques
An expert system is a computer program that takes in information and pro-
duces predictions, solutions, or diagnoses. A key feature of an expert system
is its ability to explain its ﬁndings to a human decision maker. For example,
a rule-based system could “explain” itself by running through the chain of
rules it had followed to reach a conclusion. As Bayes nets are often thought
of as “statistical expert systems,” the explanation problem has been explored
in this literature as well. Suermondt (1992) oﬀers a relatively comprehensive
discussion of metrics for inﬂuential ﬁndings and conﬂicts of evidence, arriv-
ing at Kullback–Leibler as his metric for explanation. Henrion and Druzdzel
(1990) also looked at qualitative propagation through a graph. Both of these
authors looked at natural language as a tool for communicating their ﬁndings
to users.
This section brieﬂy looks at two proposed techniques. First, Sect. 7.1.1
looks at the technique of Madigan et al. (1997) for coloring the nodes in the
graph to provide an explanation. Second, Sect. 7.1.2 looks at an algorithm
for ﬁnding the most likely explanation for a given pattern of outcomes (Pearl
1988).
7.1.1 Node Coloring
One of the simplest explanation techniques is to simply color the nodes accord-
ing to the probability of occurrence (Madigan et al. 1997). Thus nodes with
a high probability of a noteworthy event would have a diﬀerent appearance.
For proﬁciency variables, one would color them according to the probability
of mastery. For dichotomous observable outcome variables, one would color
them according to the probability of getting a correct outcome.
Example 7.1 (Simplified Language Test). Mislevy (1995c) creates a sim-
ple language test to illustrate sorting out evidence from integrated tasks in
language testing. In this test, reporting is on the basis of the four modalities
of language: Reading, Listening, Writing, and Speaking. There are four kinds
of tasks: a pure reading task; a pure listening task; two kinds of integrated
tasks, one involving reading and writing and another involving reading, speak-
ing, and listening. We increase the test length of this illustrative example by
making replications of the tasks. In this case, we have ﬁve replicates each
of the reading and listening tasks and three replicates each of the speaking
and writing tasks. Figure 7.1 shows the model graph. Appendix A.2 describes
where complete networks for this example can be found online.

7.1 Simple Explanation Techniques
199
Outcome-L-1
Wrong
Outcome-L-2
Wrong
Outcome-L-3
Wrong
Outcome-L-4
Right
Outcome-L-5
Wrong
Outcome-Rw-3
Good-Writi
Outcome-Rw-2
Poor-Writi
Outcome-Rw-1
Poor-Writi
Outcome-Rls-3
Poor
Outcome-Rls-2
Okay
Outcome-Rls-1
Poor
Outcome-R-5
Very-Good
Outcome-R-4
Good
Outcome-R-3
Good
Outcome-R-2
Okay
Outcome-R-1
Good
Listening
Speaking
Writing
Reading
Fig. 7.1 Colored graph for an examinee who reads well but has trouble speaking.
This is a set of “typical” numbers taken from Example 7.1. The color of the node
depends on the probability that the student is in a high (blue) or low (red) state of
master, with the darkness indicating the strength of the belief. The black bars on the
sides of the nodes indicate belief of mastery before (left side) and after (right side)
observing the evidence from this test. This ﬁgure was generated by Graphical-
Belief.

200
7 Explanation and Test Construction
Figure 7.1 shows an example using the model of Example 7.1. The graph
is colored according to the trouble spots. The ﬁgure shows that this student is
doing fairly well in reading, but is having trouble with speaking and listening.
The program Graphical-Belief (Almond 1995) produced this graph
using the idea behind so-called heat maps. On a color screen, it uses a “tem-
perature” going from bright red (high probability of negative state) to bright
blue (high probability of positive state). It uses nine color levels rather than
trying to make ﬁne distinctions with colors. The bar on the right side of the
nodes provides a more detailed estimate of the probability.
Graphical-Belief will display the colors for any binary hypothesis. In
this example, as in many others, the nodes can take on more than two states.
For each variable, we must pick one state (or set of states) as the positive state
(blue color or light gray); the rest of the states become the negative states
(red color or dark gray). Interactively changing which states are deﬁned to be
positive provides a more complete picture of the model.
Daniel et al. (2003) uses this kind of graphical representation to facilitate
interaction between the teacher and the student. Both the teacher and the
student are given a view of the graph. The teacher can enter data through
assessment nodes and the student through self-assessment nodes (the data
entry form contains additional ﬁelds for justiﬁcations). The display shows
the joint information. Initial ﬁeld trials in classrooms have yielded positive
responses.
7.1.2 Most Likely Scenario
Pearl (1988) suggests dividing the variables of the model into three categories:
observable variables whose values may or may not be observed; unobservable
hypothesis variables which cause the particular conﬁguration of the obser-
vation variables; intermediate variables whose results are important only in
calculating the beliefs of the other variables. He considers the problem of
ﬁnding a pattern of the hypothesis variables that best (highest probability)
explains the conﬁguration of the observable variables. Pearl (1988) calls this
task belief revision, as the idea is to possibly revise a current “best explana-
tion” when new evidence arrives. This approach contrasts with belief updating
using the fusion and propagation algorithm in Chap. 5 that produces marginal
probability distributions for the hypothesis variables.
Applying this approach to ECD terminology, the hypothesis variables cor-
respond to proﬁciency variables. Let S = {S1, . . . , SK} be the set of proﬁ-
ciency variables. A given assignment of values to all of the proﬁciency vari-
ables s = {s1, . . . , sK} is a proﬁciency proﬁle. The goal of belief revision is
then to ﬁnd the proﬁciency proﬁle that is most likely to produce the given
pattern of observed outcomes. This is the most likely “explanation” for the
observed outcomes.
Belief revision is simple to carry out computationally using the same Bayes
net structures. One simply replaces a summation in the algorithm of Chap. 5

7.2 Weight of Evidence
201
with a maximization. Pearl (1988) gives the details (also Almond 1995; Shenoy
1991). Belief revision is an option on almost all of the free and commercial
software for manipulating Bayesian networks (Appendix A.1.1).
The most likely scenario or proﬁciency proﬁle consistent with a particular
set of observations and hypotheses can provide insight into the behavior of
the model. At any point, the best explanation is just the pattern of proﬁcien-
cies that best ﬁts the pattern of observations. Henrion and Druzdzel (1990)
advocate this approach and suggest that scenario-based explanations mimic
the way one person would explain a model to another.
7.2 Weight of Evidence
Each outcome that we observe from each task provides evidence for whether
the learner has the proﬁciencies we are trying to assess. An important part
of explanation is understanding which observations were most inﬂuential in
estimating those proﬁciencies. The weight of evidence provides a metric for
inﬂuential ﬁndings.
When an observation is used to update beliefs in Bayes net built for a
proﬁle score assessment, it usually changes the belief about all of the proﬁ-
ciency variables. However, the same observation will cause a diﬀerent strength
and even direction of change for each proﬁciency variable. Any collection of
proﬁciency variables deﬁnes a universe of possible proﬁciency proﬁles. We will
call any split of the set of all possible proﬁles into two groups a hypothesis,
H, and its negation, H. Typical hypotheses have to do with the mastery of
one of the skills, for example, Sk ≥proficient. More complex hypotheses
may also be of interest, for example, whether a given instructional program is
appropriate for a given learner might depend on several proﬁciency variables.
Good (1985) derives the weight of evidence as a measure of the amount of
information a piece of evidence E provides for a hypothesis H. The weight of
evidence for H vs H is:
W(H:E) = log P(E|H)
P(E|H) = log P(H|E)
P(H|E) −log P(H)
P(H) .
(7.1)
Thus, the weight of evidence is the diﬀerence between the prior and posterior
log odds for the hypothesis. (Good recommended taking the logarithms to base
10, and multiplying the result by 100. He calls the resulting units “centibans.”
All the comparisons are the same; he just found the units easier to work with).
Just as our hypothesis may be compound, our evidence may be compound
too. In a typical testing situation, the evidence is in fact made up of the
observed outcomes from many tasks. In this case, Eq. 7.1 refers to the joint
evidence from all tasks. If we can partition the evidence into two sets of
observations, E1 and E2, we can deﬁne the conditional weight of evidence:
W(H:E2|E1) = log P(E2|H, E1)
P(E2|H, E1) .
(7.2)

202
7 Explanation and Test Construction
These sum in much the way that one would expect:
W(H:E1, E2) = W(H:E1) + W(H:E2|E1) .
(7.3)
In general, W(H:E2|E1) = W(H:E2) only when E1 and E2 are indepen-
dent given both H and H. As typically either H or H is usually a com-
pound (consists of several diﬀerent skill proﬁles), this independence usu-
ally does not hold. Instead, if E1 and E2 both favor H, then typically
W(H:E2|E1) < W(H:E2). This makes intuitive sense. Suppose that we do
not know whether a learner has a particular skill. The ﬁrst time we see the
learner solve a problem that requires that skill, we will get a great deal of
evidence that the learner has the skill. The second and third time we make
that kind of observation, we are conﬁrming what we observed from the ﬁrst
observation, so, we expect the evidentiary value of the replications to be lower.
7.2.1 Evidence Balance Sheet
Spiegelhalter and Knill-Jones (1984) present the weights of evidence in an
evidence balance sheet in simple logistic regression models. Madigan et al.
(1997) adapt the evidence balance sheet for graphical models. Figures 7.2,
7.3, 7.4, and 7.5 show a possible graphical interpretation from Graphical-
Belief.1 The evidence is ordered according to when it arrives (for example,
the order of test items in a booklet, or presentation of tasks in a CAT). At
each point of time, the weight of evidence conditioned on all previous evidence
is displayed along with the current estimate of probability.
Example 7.2 (IRT with identical items). This example looks at a ﬁve
item test, where all of the items have identical item parameters. The model
used is essentially a discretized IRT model. The proﬁciency model has a single
variable θ with ﬁve levels: {−2, −1, 0, 1, 2}. The prior distribution is a trian-
gular distribution (similar to the normal, but with fatter tails). The evidence
models are made by calculating the 2PL likelihood with discrimination 1.0
and diﬃculty 0.0, and then ﬁlling the entries in the table. Figure 7.2 shows
the evidence balance sheet for the hypothesis θ ≥1 for a person who got all
ﬁve items right.
Note that in the example of Fig. 7.2, the only diﬀerence between observa-
tions X1 and X5 is the order in which they arrive. When X1 arrives, we know
little about the student, and the observation has a relatively large evidentiary
value. However, when later observations arrive, they are conﬁrming what we
learned from the ﬁrst observation. Their evidentiary value is smaller, and it
decreases as the number of replications go up. It is important to remember
this order eﬀect as we look at the evidence balance sheet. If we had asked for
1 On a color screen, this rendering uses pale blue for positive evidence, and red for
negative evidence.

7.2 Weight of Evidence
203
Evidence Balance Sheet [Theta = 1 orxo 2]
32
Initial
0.30
X-1
Correct
0.47
X-2
Correct
0.61
X-3
Correct
0.73
X-4
Correct
0.82
X-5
Correct
0.88
Indicant
State
WOE
Target Probability
Fig. 7.2 Evidence balance sheet for θ ≥1 (Example 7.2)
This shows the progressive inﬂuence of the ﬁve identical (in parameters) items on
the running probability that θ is at the highest level. The column marked “WOE”
displays the conditional weight of evidence (given the previous evidence). (The num-
ber “32” at the top of the column indicates that the weight of evidence bar runs from
−32 to +32 centibans). The column marked target probability shows the cumulative
probability that θ ≥1 after each ﬁnding. This ﬁgure was generated by Graphical-
Belief.
Task 5 ﬁrst instead of last, it would have had the biggest evidentiary value,
not Task 1. As the conditional weights of evidence are order sensitive, Madi-
gan et al. (1997) suggest interactively ordering the observations to promote
better understanding of sensitivity to the ﬁndings.
This order eﬀect does not change the total evidence across all tasks; this
remains constant. What is changing is which variables are conditioned on when
we calculate the conditional weight of evidence observation by observation. It
is always the case that W(H:E1)+W(H:E2|E1) = W(H:E2)+W(H:E1|E2).
If E1 and E2 point in the same direction, it is usually, but not necessarily, the
case that W(H:E1) ≥W(H:E1|E2).
For a richer model, we return to Example 7.1. The proﬁciency model has
four reporting variables. This give us the chance to observe the eﬀects of both
direct evidence—evidence about proﬁciency variables that are parents of the
task—and indirect evidence—evidence about tasks which are children of other
correlated proﬁciency variables.
Figure 7.3 shows that the evidence against low reading ability is mostly
direct. The reading tasks are ﬁrst, and relatively good performance on those
tasks (averaging at about the second highest state) quickly establishes a low
probability of Reading being at the novice state. Even though poor perfor-

204
7 Explanation and Test Construction
Evidence Balance Sheet [Reading = NOVICE]
124
Initial
0.25
Outcome-R-1
Good
0.09
Outcome-R-2
Okay
0.09
Outcome-R-3
Good
0.03
Outcome-R-4
Good
0.01
Outcome-R-5
Very-Good
0.00
Outcome-L-1
Wrong
0.00
Outcome-L-2
Wrong
0.00
Outcome-L-3
Wrong
0.00
Outcome-L-4
Right
0.00
Outcome-L-5
Wrong
0.00
Outcome-Rls-1
Poor
0.00
Outcome-Rls-2
Okay
0.00
Outcome-Rls-3
Poor
0.00
Outcome-Rw-1
Poor-Writ
0.00
Outcome-Rw-2
Poor-Writ
0.00
Outcome-Rw-3
Good-Writ
0.00
Indicant
State
WOE
Target Probability
Fig. 7.3 Evidence balance sheet for Reading = Novice (Example 7.1)
As the ﬁve reading tasks are ﬁrst in this assessment, the assessment quickly estab-
lishes that this person reads fairly well (above the novice level). The other items have
relatively little inﬂuence. Note that hypothesis here is negative; it is a hypothesis
that reading is in its low state, so that evidence against this hypothesis is strong.

7.2 Weight of Evidence
205
mance on the later tasks provides some direct and indirect evidence in favor of
Reading being low, it is not enough to overwhelm the initial direct evidence.
In particular, there is an alternative explanation (poor Writing and Speaking
skills) that explains away the direct evidence from the latter tasks.
The story with Listening, shown in Fig. 7.4, is quite diﬀerent. First, the
good performance on the ﬁve reading tasks provides a small bit of indirect
evidence against Listening being at the lowest level. Second, the poor perfor-
mance on the Listening tasks (4 out of 5 wrong) provides strong direct evidence
that the Listening proﬁciency is in the lowest state. Note that one listening
task that has a correct outcome has an eﬀect in the opposite direction, but
it is quickly countered by the next task whose outcome is once again wrong.
Also, the ﬁrst integrated Reading–Listening–Speaking task provides evidence
that Listening is at the Novice level. This is a combination of direct and indi-
rect evidence. Relatively strong evidence that Reading skills are good makes
poor listening skills a much more likely explanation for poor performance on
this task.
Figure 7.5 shows how the inferences about the Speaking variable pro-
gresses. First, the good performance on the Reading items provides indirect
evidence against Speaking being low (through its correlation with Reading).
Next, in contrast, the weak performance on the Listening items provides indi-
rect evidence that Speaking may in fact be low. However, the poor perfor-
mance on the three integrated tasks involving speaking provides much stronger
direct evidence than the preceding indirect evidence. Finally, the indirect evi-
dence provided by the Reading–Writing tasks is quite small.
A related way that the weight of evidence could be used is to select a
few tasks for more detailed feedback to the test taker. By giving feedback on
tasks that have the biggest negative weight of evidence for a skill mastery
hypothesis, one can focus the learner’s attention on problem spots. By giving
feedback on tasks that have the biggest positive weight of evidence, one can
reinforce students’ appropriate use of skills.
7.2.2 Evidence Flow Through the Graph
Madigan et al. (1997) suggest using the model graph to provide a picture
of the ﬂow of information through the model. In particular, they suggest
coloring the edges of the graphical model to encode the strength of information
ﬂow through the model. Working with probabilistic models, they create a
hollow edge whose width displays the strength of inﬂuence from a node to its
neighbor. Several metrics can be used to measure this strength; Madigan et
al. (1997) recommend the weight of evidence.
Madigan et al. (1997) demonstrate weight of evidence-based edge coloring
in simple chain graphs. For example, consider a model with three binary
variables: A, B, and C. Suppose that we know that A is true and want to
know what impact this information has on our belief that C is true. For each

206
7 Explanation and Test Construction
Evidence Balance Sheet [Listening = NOVICE]
100
Initial
0.29
Outcome-R-1
Good
0.23
Outcome-R-2
Okay
0.26
Outcome-R-3
Good
0.24
Outcome-R-4
Good
0.24
Outcome-R-5
Very-Good
0.17
Outcome-L-1
Wrong
0.36
Outcome-L-2
Wrong
0.53
Outcome-L-3
Wrong
0.68
Outcome-L-4
Right
0.51
Outcome-L-5
Wrong
0.65
Outcome-Rls-1
Poor
0.81
Outcome-Rls-2
Okay
0.79
Outcome-Rls-3
Poor
0.84
Outcome-Rw-1
Poor-Writ
0.86
Outcome-Rw-2
Poor-Writ
0.86
Outcome-Rw-3
Good-Writ
0.86
Indicant
State
WOE
Target Probability
Fig. 7.4 Evidence balance sheet for Listening = Novice (Example 7.1)
The ﬁrst ﬁve tasks are reading tasks which have good outcomes. They provide a
little bit of evidence against low listening ability. The next ﬁve tasks are listening
tasks with poor outcomes. They provide evidence for low listening ability, except for
L-4 which has a correct outcome. The ﬁrst integrated Reading–Listening–Speaking
task (which has a poor outcome) also provides evidence for low Listening ability.

7.2 Weight of Evidence
207
Evidence Balance Sheet [Speaking = NOVICE]
100
Initial
0.27
Outcome-R-1
Good
0.22
Outcome-R-2
Okay
0.24
Outcome-R-3
Good
0.22
Outcome-R-4
Good
0.21
Outcome-R-5
Very-Good
0.17
Outcome-L-1
Wrong
0.27
Outcome-L-2
Wrong
0.35
Outcome-L-3
Wrong
0.40
Outcome-L-4
Right
0.34
Outcome-L-5
Wrong
0.39
Outcome-Rls-1
Poor
0.76
Outcome-Rls-2
Okay
0.75
Outcome-Rls-3
Poor
0.91
Outcome-Rw-1
Poor-Writ
0.90
Outcome-Rw-2
Poor-Writ
0.90
Outcome-Rw-3
Good-Writ
0.89
Indicant
State
WOE
Target Probability
Fig. 7.5 Evidence balance sheet for Speaking = Novice (Example 7.1)
The ﬁrst ten tasks are the reading and listening tasks. They provide a small amount
of indirect evidence for the Speaking skill being at the novice level. However, the
poor performance on two of the three integrated Reading–Listening–Speaking tasks
provides rather stronger direct evidence for low Speaking ability.

208
7 Explanation and Test Construction
possible value of B, say bi, the quantity W(C/C:B = bi) is the potential weight
of evidence for bi. The largest potential weight of evidence is the relevant
potential weight of evidence. As B is a binary variable, only one of W(C/C:B)
and W(C/C:B) is positive; that one will be the relevant potential weight of
evidence.
The following scheme encodes the weight of evidence (for the evidence
chain) via the width of the edge of a graphical model. Figure 7.6 displays this
idea for a simple graphical model. The arrow between nodes A and B shows
the weight of evidence A provides for B. As A is known, the actual weight of
evidence equals the potential and the edge is shown as a ﬁlled arrow. The outer
arrow between nodes B and C shows the relevant potential weight of evidence,
that is the maximum evidence B could provide for C if it were known. The
inner arrow shows the actual weight of evidence all ﬁndings upstream of B
(i.e., A) provides for C.
A
B
C
Fig. 7.6 Evidence ﬂows using weight of evidence
Although edge coloring is an eﬀective technique for tree-shaped graphical
models with binary variables, extending it beyond those special cases presents
some diﬃculties. In particular, if the intermediate variable B has many possi-
ble outcomes it may be diﬃcult to show how each outcome contributes to our
overall beliefs about C. Clustering variables to form a Markov tree presents
the same diﬃculty: the clustered nodes are eﬀectively nonbinary variables.
Madigan et al. (1997) suggest selecting a set of positive states for each node
in the graph and using the positive states for determining color in the dis-
plays. All weight of evidence calculations are made with respect to the binary
proposition “The variables in the node take on one of the positive states.”
Interactively selecting the marked state (or set of states) for each node should
allow the modeler to build up a good picture of evidence ﬂow. Nicholson and
Jitnah (1998) use mutual information instead of weight of evidence to similar
ends. This has the advantage of not requiring that the nodes be reduced to
binary ones.
The evidence-ﬂow graphs are primarily useful in understanding how indi-
rect evidence ﬂows through a system. In particular, they can help identify
evidence bottlenecks, places where no matter how much evidence we get down-
stream, the evidence has little eﬀect on our inferences upstream. This could
mean that we need to redesign our test, to provide more direct evidence about
the skills of interest. It also could mean that there is an inhibitor eﬀect, some
alternative explanation of the observed evidence that does not allow us to
make inferences about the quantity of interest.

7.3 Activity Selection
209
7.3 Activity Selection
The ECD framework is intended to describe both pure assessments (activities
whose goal is to assess the current proﬁciencies of the learner) and assessments
embedded in the more complex setting of a tutoring system, where students
can also be administered activities meant to increase their proﬁciencies. In
the latter case, we would have a large number of activities to choose from,
including activities whose purpose is primarily instruction, or instructional
tasks, and activities whose primary purpose is assessing the learner’s cur-
rent state, or assessment tasks. The process responsible for selecting the next
activity would also have the responsibility for deciding when to switch between
“instruction” and “assessment” modes, as well as criteria for terminating the
activities.
A teacher making an instructional plan for a student is very much like a
physician diagnosing a disease and prescribing treatment for a patient. Success
in both cases typically depends on the degree to which the teacher/doctor has
valid knowledge of the student/patient. As such, instructional planning goes
hand-in-hand with student modeling. In general, instructional planning refers
to the process of specifying instructional goals and actions that provide for a
learning experience that is coherent, complete, and valid.
Blending assessment and instruction is a big topic and this book cannot do
it justice. Instead, this section looks at the smaller problem of just selecting
assessment tasks. In particular, it develops an approach based on the weight
of evidence. Section 7.3.1 deﬁnes the related concept of value of information,
and Sect. 7.3.2 deﬁnes the expected weight of evidence. Section 7.3.3 provides
an analogous measure called mutual information that can be used in more
complicated situations. Section 7.4 goes on to show how these metrics can be
used in both ﬁxed-form and adaptive test construction.
7.3.1 Value of Information
For a licensing agency, it is presumably more regrettable to award somebody
a license who is not ﬁt to practice, than it is to fail to award the license to
somebody who is ﬁt. The latter person only needs to take the test again, while
the former could do harm to the public. On the other hand, for a classroom
teacher, it is more regrettable to fail to identify a student who is having
diﬃculty than it is to mistakenly identify a student as needing extra help who
does not. Presumably this mistake will be uncovered and corrected during the
remediation.
This discussion gets at the idea of utility. In these examples, we have a
decision we need to make: for instance, whether or not to license the candi-
date, or whether or not to assign the student to remediation. The utility of
a decision depends on both the decision, d, and the true values of the pro-
ﬁciency variables S, as well as the relative costs of decision alternatives and

210
7 Explanation and Test Construction
the expected outcomes. We can write the utility as u(d, S). This utility now
can be traded-oﬀagainst the cost of the test, as in Example 4.1.
The decision theoretic approach to this problem would be to calculate the
value of information (Matheson 1990; Heckerman et al. 1993) with respect to
the instructional choices that are available. As the true proﬁciency state S is
unknown, it is considered a random variable under the Bayesian framework.
Thus, our expected utility is ESu(d, S), where ES is the expectation taken with
respect to the marginal distribution P(S) before any evidence is observed. The
Bayes decision is to take the value of d that maximizes the expected utility.
Suppose, we had the result of some test T that is related to S. Then the
decision we would take is the one that maximizes expected utility with respect
to the posterior distribution P(S|T = tk). The expected value of information
is the expected diﬀerence between the best decision we could make with the
test result and the best decision we could make without the test result. Thus,
VoI(T ) = ET

max
d
ES|Tu(d, S) −max
d
ESu(d, S)

,
(7.4)
where ES|T is the expectation taken with respect to the conditional distribu-
tion P(S|T ) and ET is the expectation with respect to the marginal distribu-
tion of the evidence, P(T )—that is, before T is observed.
Now, we have a rule for deciding whether or not to test: If the expected
value of information exceeds the cost of the test, testing is worthwhile. Con-
sider Example 4.1. There the goal is to maximize the student’s ability at the
end of whatever instructional strategy is chosen. If almost all students need
the same instruction, it may not make sense to test because the test results
will not aﬀect the optimal decision. Similarly, it does not make sense to test
if test is very expensive compared with the cost of the instruction. Only if the
test helps us make better instructional decisions is testing worthwhile.
If we have multiple tests, we would pick the one that maximizes the
expected value of information. Using the ECD framework, we can regard each
task as a “test” (in the sense we have been using the term in this section),
so the expected value of information gives us a principle for task selection
in adaptive testing. We can also consider the collection of tasks {T1, . . . , Tn}
that maximizes the expected value of information.
Suppose, during an adaptive test, we pick the task T∗which maximizes
the expected value of information at every step, stopping when there is no test
whose expected value of information (given the results seen so far) exceeds
the cost of the test. This is called myopic search. It is sometimes possible to
ﬁnd cases where there is a pair of tests T1 and T2 that together give more
information than T∗. Thus, myopic search is not guaranteed to ﬁnd the best
sequence of tests. Heckerman et al. (1993) explore this issue in more detail.
Example 7.3 (Value of Information for a Dental Hygienist Assess-
ment). This illustration extends Example 5.1. To be consistent with the
notation in this chapter, S represents a dental hygiene student’s proﬁciency in

7.3 Activity Selection
211
examining patients, which can take two values, s1 = expert and s2 = novice.
The proﬁciency model S consists simply of the single variable S. Initially
p(expert) = p(novice) = .5. Now there are three tasks available to adminis-
ter:
•
Task 1 is a hard task. It yields an observable variable X1, which can take
two values, x11 = yes and x12 = no corresponding to whether an examinee
takes an adequate patient history. An expert’s probability of taking an
adequate history in such tasks is .6 and a novice’s is .2, so p (x11 |s1 ) = .6
and p (x12 |s1 ) = .4, and p (x11 |s2 ) = .2 and p (x12 |s2 ) = .8 .
•
Task 2 is a medium task. It yields the observable variable X2, with an
expert’s and novice’s probabilities of taking an adequate history .8 and .4;
that is, p (x21 |s1 ) = .8 and p (x21 |s2 ) = .4 . This is the task that appears
in Example 5.1.
•
Task 3 is easy. It yields X3, where p (x31 |s1 ) = .95 and p (x31 |s2 ) = .65.
Denote the possible decisions by d1 = expert and d2 = novice, and let
the utilities u(d, s) for deciding an examinee is an expert or novice given true
proﬁciency be as shown below. There are high utilities for deciding an expert
is an expert and a novice is a novice; there is a lower utility for deciding an
expert is a novice and the lowest utility for deciding a novice is an expert.
Decision Proficiency Utility
expert
expert
100
novice
expert
40
expert
novice
0
novice
novice
100
We can now use Eq. 7.4 to calculate the value of information of any of the
tasks. We will focus on Task 2.
Starting from the initial .5 probabilities for novice and expert, we ﬁrst
determine the term maxd ESu(d, S). This is the expected utility of the decision
that gives the greatest expected utility before administering any tasks. This
expression is needed and is the same for calculating VoI for all tasks in the pool
at this point. If the decision is expert, the expected utility is the average of
the utility for expert if the true proﬁciency is expert, or 100, and the utility
for expert if the true proﬁciency is novice, or 0, weighted by their respective
current probabilities of .5. That is,
ESu(expert, S) = u(d1, s1)p(s1) + u(d1, s2)p(s2) = 100 × .5 + 0 × .5 = 50.
By similar calculations,
ESu(novice, S) = u(d2, s1)p(s1) + u(d2, s2)p(s2) = 70.
The decision maximizing expected information is therefore d2, novice, and
maxd ESu(d, S) = 70.

212
7 Explanation and Test Construction
We next consider the expected value of the maximum-decision utility if
Task 2 were administered. Calculations similar to those above are ﬁrst deter-
mined to evaluate maxd ES|Tu(d, S), where X2 plays the role of T . We need
to consider the cases of when X2 = x21 and X2 = x22, or when an adequate
or inadequate performance is observed, and determine the maximum-decision
utility in each case. For X2 = x21, recall from Example 5.1 that the posterior
probabilities for expert and novice given an adequate performance are .67
and .33. For the decision of expert, ES|T u(d, S) becomes in this case
ES|x21u(expert, S) = u(d1, s1)p(s1|x21) + u(d1, s2)p(s2|x21)
= 100 × .67 + 0 × .33 = 67.
For the decision of novice,
ES|x21u(novice, S) = u(d2, s1)p(s1|x21) + u(d2, s2)p(s2|x21)
= 40 × .67 + 100 × .33 = 60.
Thus, when X2 = x21 deciding expert produces the maximal utility, 67.
If the performance to Task 2 is inadequate, or X2 = x22, the posterior for
S is .25 for expert and .75 for novice. By calculations similar to those above,
ES|x22u(expert, S) = 25 and ES|x22u(novice, S) = 85. Deciding novice pro-
duces the maximal utility of 85.
The ﬁnal step is the outer expectation. This is the weighted average of
the maximizing utilities for the decisions when X2 takes each possible value,
weighted by the current marginal probabilities for those outcomes before the
observation – from Table 5.2, .6 and .4 respectively. Thus
VoI(X2) = EX2

max
d
ES|X2u(d, S) −max
d
ESu(d, S)

=

max
d
ES|X2=x21u(d, S) −max
d
ESu(d, S)

× .6
+

max
d
ES|X2=x22u(d, S) −max
d
ESu(d, S)

× .4
= [67 −70] × .6 + [85 −70] × .4 = 4.2.
Applying the same steps to Tasks 1 and 3, the hard and easy tasks, we
ﬁnd their Value of Information to be 8 and 0. Task 1, the hard task, provides
the greatest expected VoI. If the cost of administering Task 1 were greater
than 8, however, it would be best to decide novice without testing.
Task 3 has zero VoI. The best decision before testing was deciding novice.
Test 3 is very easy, so practically all experts and even most novices get it right.
With the utilities favoring caution in deciding expert status, the expected
utilities for both an adequate and an inadequate performance to this task lead
to deciding novice. However, diﬀerent utilities or diﬀerent initial probabilities
for proﬁciency could produce circumstances under which administering Task 3
would increase expected utility.

7.3 Activity Selection
213
If a test is carried out, the same steps can be repeated with not-yet-
administered tests to see if a subsequent test provides suﬃcient VoI to then
apply, and if so which had the largest VoI. All of the calculations carried out
above would start with beliefs conditional on the observed value of the tests
already administered.
7.3.2 Expected Weight of Evidence
The decision-making approach described in the previous section requires expli-
cating utility functions for various states of proﬁciency on the same scale as
the costs of instructional and assessment tasks, in addition to establishing
the probabilities. Establishing such a mapping can be diﬃcult, and diﬀerent
stakeholders can disagree. In these cases, the optimizing machinery of deci-
sion theory can be pressed into service nevertheless by substituting a quasi-
utility (Glasziou and Hilden 1989) for the true utility. Quasi-utilities gauge
how close our estimated proﬁciency is to the actual proﬁciency. For example,
Lord’s (1980) “expected information” computerized adaptive testing (CAT)
algorithm in item response theory uses Fisher information as a quasi-utility.
Henson and Douglas (2005) suggest using a weighted sum of Kullback–Leibler
distances, and Madigan and Almond (1995) recommend the use of the weight
of evidence as a quasi-utility. This section explores the idea.
When discriminating between a single hypothesis H and its negation H,
Good and Card (1971) recommend the Expected Weight Of Evidence (EWOE)
as a quasi-utility:
EW(H:E) =
n

j=1
W(H:ej)P(ej | H)
(7.5)
where {ej, j = 1, . . . , n}, represent the possible outcomes of the observation
E. W(H:ej) is the weight of evidence concerning H provided by the evidence
ej, log[P(ej | H)/P(ej | H)] (Eq. 7.1). Informally, EW(H:E) is the weight of
evidence that will be obtained from E “on the average,” when the hypothesis
H is true.
Example 7.4 (Expected Weight of Evidence for a Dental Hygienist
Assessment). This example is also based on the dental hygienist assessment
of Example 5.1, with the probabilities and the additional items introduced in
Example 7.3. We consider the hypothesis that an examinee is an expert, so
H : S = s1 and H : S = s2, and initially focus on Task 2, so X2 plays the role
of E. Equation 7.5 then takes the form
EW(s1:X2) =
n

j=1
W(s1:x2j)P(x2j | s1).

214
7 Explanation and Test Construction
Now
W(s1:x21) = log P(x21|s1)
P(x21|s2) = log .8
.4 = .69.
Similarly, W(s1:x22) = log [P(x22|s1)/P(x22|s2)] = log [.20/.60] = −1.10.
Then since p(x21|s1) = .8 and p(x22|s1) = .2,
EW(s1:X2) = .69 × .8 + −1.10 × .2 = .332.
Similar calculations for Task 1 and Task 3 give EW(s1:X1) = .384 and
EW(s1:X3) = .259. Thus the hard task provides the greatest expected evi-
dence about whether an examinee is an expert, followed by the medium task,
then the easy task.
If applied directly to H and H, EWOE does not distinguish the values
of diﬀerent outcomes, and thus does not use utility in a formal way. It is
however possible to adjust it for misclassiﬁcation costs. Breiman et al. (1984)
have proposed the following approach (see also Glasziou and Hilden 1989).
Suppose that to misclassify as candidate for whom H holds as H is w times
as regrettable as the reverse. An eﬀectively weighted EWOE formulation can
be applied using an artiﬁcial hypothesis H′, where a case of H is considered
to be w cases of H′; that is,
P(H′) =
wP(H)
wP(H) + P(H).
A quasi-utility based on EWOE and valuing correct classiﬁcations of H as w
times as much as correct classiﬁcations of H is then obtained as
n

j=1
W(H′:ej)P(ej|H′)
(7.6)
with
P(ej|H′) = P(ej|H) P(H)(w −1) + 1
P(H|ej)(w −1) + 1
and
W(H′:ej) = W(H:ej).
Incorporating the misclassiﬁcation costs into the EWOE means that it is
assessing improvement in risk (the glass-half-empty view of expected utility)
instead of just probabilities. If the probability of a certain disease is relatively
low, but missing it would be serious and a simple test is available, a doctor
will usually order the test. A risk-based metric for test selection that includes
the costs of misclassiﬁcation enables the procedure to reﬂect this reasoning.
Heckerman et al. (1993) note a close connection between the EWOE and
value of information. If our hypothesis is binary and we have two possible

7.4 Test Construction
215
actions we can take, it is possible to deﬁne a value p∗such that if p(H) > p∗
then taking one particular action is always the best decision. This is called
the Bayes decision and is discussed in most standard texts on decision theory
(DeGroot 1970; Berger 1985). The formulation can be re-expressed in term
of the log odds. Because the EWOE represents changes to the log odds, it
is a metric for determining how much evidence is needed to reach the Bayes
decision.
7.3.3 Mutual Information
One problem with EWOE is that the hypothesis must be binary. Pearl (1988)
suggests using Mutual Information (MI) to select the best next observation.
The mutual information of two variables X and Y is deﬁned as:
MI(X, Y ) =

x,y
P(x, y) log
P(x, y)
P(x)P(y) .
(7.7)
This is the Kullback–Leibler distance between the joint distribution and the
distribution in which X and Y are independent. It can also be expressed as

x
P(x)

y
P(y|x) log P(y|x)
P(y)
.
(7.8)
Suppose, for example, that S is a proﬁciency variable with multiple values
and X1, . . . , Xn are observable variables from n tasks that could be admin-
istered. A test selection procedure based on MI would choose the task that
maximizes MI(S, Xj), say Xj∗. After the outcome xj∗is observed, the pro-
cess is repeated by ﬁnding the maximizing value of MI(S, Xj|xj∗), where all
of the probabilities in Eq. 7.7 are calculated conditional on Xj∗= xj∗:
MI(S, X|xj∗) =

s,x
P(s, x|xj∗) log
P(s, x|xj∗)
P(s|xj∗)P(x|xj∗) .
(7.9)
Note that MI is a symmetric measure; Y provides as much information
about X as X provides about Y . A number of Bayesian network software
packages will calculate mutual information (Appendix A.1.1).
7.4 Test Construction
What has gone before has almost given us a theory of adaptive test con-
struction. Simply maximizing value of information, or in the absence of true
utilities, a quasi-utility such as weight of evidence, produces a test form that
is useful for a particular purpose. Indeed, Madigan and Almond (1995) pro-
pose such a theory of test selection, and much of the material both here and

216
7 Explanation and Test Construction
in the previous section is an adaptation of the methods described there to
the educational setting. Section 7.4.1 uses this perspective to explore relation-
ships between item response theory computer adaptive testing (IRT-CAT) and
Bayesian network CAT. Section 7.4.2 picks up on a technique called critiquing
recommended by Madigan and Almond (1995) to improve the coherence of
the test forms generated by such a procedure.
7.4.1 Computer Adaptive Testing
There is a large body of theory and practice associated with item response
theory (IRT)
computer
adaptive testing
(CAT)
(Wainer
et
al. 2000
and van der Linden and Glas 2010, present overviews). We brieﬂy describe a
basic version of IRT-CAT to provide a feel for key ideas in adaptive testing.
The proﬁciency model consists of a single real-valued variable, θ. At any point
in time, our state of knowledge about a student taking the test consists of a
posterior distribution over θ. A simple way to make a selection of the next
item is to calculate a MAP estimate (posterior mode) estimate for θ and pick
the item that maximizes Fisher information at that value of θ.
In this framework, CAT is essentially myopic search using Fisher informa-
tion as a quasi-utility. The search is myopic because we are only searching
for the single best task or item at each point of time. There may well be a
combination of tasks or items which does better than any single task. Further-
more, the best combination may not include the best single task. Generally
speaking, ﬁnding a single best task is a simpler search problem than ﬁnding
the best combination. One faces a time versus optimality problem.
As Fisher information is deﬁned for continuous variables only, it is not
directly applicable when the proﬁciency model is a Bayesian network consist-
ing of discrete variables. However, we could use other measures, for example
weight of evidence on some hypothesis of interest, perhaps an overall proﬁ-
ciency variable. Another suggestion by Henson and Douglas (2005) is to try
to minimize entropy over the proﬁciency model. This should have the eﬀect
of trying to move the probability mass toward a single proﬁciency proﬁle.
One issue with adaptive testing is when to stop. In IRT-CAT the stopping
rule can be based on the number of items presented or a target posterior vari-
ance (in practice, time limits, maximum and minimum test lengths, content
constraints, and exposure rates for items are considered jointly with an infor-
mation metric; van der Linden and Glas 2010). In a proﬁle scored assessment,
a stopping rule analogous to target posterior variance can be based on the
target classiﬁcation probability.
If we know the potential actions that could be taken on the basis of an
assessment, it would be even better to base the stopping rules on the conse-
quences of the decisions. For example, if it is known that assigning a student a
particular remedial treatment is an optimal decision if the probability that she
has mastered a skill is below 25 %, then as soon as her posterior probability

7.4 Test Construction
217
for that skill falls below 25 % we can stop testing and start providing that
instruction.
These kinds of stopping rules are particularly useful in the context of an
Intelligent Tutoring or e-Learning system. Such a system moves between two
modes: an assessment mode and an instructional mode. Presumably such a
system would start in assessment mode and continue assessing the learner’s
state of knowledge until the probability that a particular piece of instructional
material is useful exceeds a certain threshold. At this point, it would switch to
instructional mode until the learner completes that unit. It would then switch
back to the assessment mode to gauge the state of the student’s progress.
Such a system could be enhanced by including a model for student growth
and learning in the assessment model (Sect. 16.2.2).
7.4.2 Critiquing
Standard IRT-CAT is unidimensional, although the ideas have been extended
to
multidimensional
proﬁciency
models
in
the
context
of
both
IRT
(Mulder and van der Linden 2010) and cognitive diagnosis (Cheng 2009).
Bayes nets are well-suited to multivariate proﬁciencies and amenable to adap-
tive testing as well. Sometimes there is a node in the model for overall proﬁ-
ciency, but in some domains, there is no single dominant proﬁciency. Even if
there is a node for overall proﬁciency, presumably the multidimensional model
has been chosen because the other dimensions are of interest in addition to
the overall proﬁciency. Thus the activity selection algorithm must support
multiple purposes.
EWOE is attractive as a measure of test quality because it focuses on one
potential purpose at a time, as operationally deﬁned by a particular H. The
EWOE is like a spot meter for a test, looking at how much power the test
has with respect to a particular purpose. An assessment design with multiple
conceivable Hs must then balance the design over each potential purpose of
interest.
Picking a single, main purpose has a marvelous focusing eﬀect. In par-
ticular, choosing a quasi-utility focused on that purpose and maximizing it
produces a test design optimized for that purpose. This produces a univer-
sal rule for test construction. Maximizing weight of evidence for an overall
proﬁciency variable is one way to achieve this. The mutual information and
expected value of information principles are two others.
As an alternative, one could use a task selection strategy based on a global
measure of information calculated across all proﬁciencies. Suppose for example
there are n dichotomous proﬁciency variables and H1, . . . , Hn are hypotheses
corresponding to mastery of each of the corresponding skills. A plausible task
selection strategy is to choose whichever task provides the greatest expected
information, looking across all items and all proﬁciencies. A criticism of this

218
7 Explanation and Test Construction
kind of automated test selection strategies is that they tend to meander. The
sequence of selected tasks can cycle rapidly through diﬀerent topics, an item
for one hypothesis and the next item for a diﬀerent hypothesis according to
where the maximum information happens to appear (Barr and Feigenbaum
1982, p. 82). This slows down the student who must mentally switch topics
for each new task.
Example 7.5 (Adaptive Content with the Evidence-Based Diagnosis
(ACED) Sequences Test). Shute (2006) and Shute et al. (2008) describe
an assessment system designed for a sequences unit of an eight grade algebra
system (also see Shute 2004; Shute et al. 2005). The proﬁciency model was a
Bayesian network with 42 nodes (Fig. 7.7). At the top level is a variable for
overall proﬁciency in sequences. Just below that are three variables measuring
proﬁciency in the three types of subsequences, arithmetic, geometric and other
recursive, as well as a proﬁciency for recognizing sequences. The lower level
nodes are skills corresponding to proﬁciency with tasks based on various ways
sequences can be presented and manipulated. Appendix A.2 shows where the
ACED models can be accessed online.
Suppose we used an activity selection model for ACED (Example 7.5) that
maximizes EWOE for the overall proﬁciency node. Suppose that the ﬁrst item
chosen provided direct evidence about arithmetic sequences. If arithmetic,
geometric, and other recursive sequences provide equal input into the overall
proﬁciency construct, the next best item will come from one of the other two
branches. The third item will come from the remaining branch. Only at the
fourth item, we will potentially return to the arithmetic sequences. This will
force the student to switch contexts between each task. The minimum entropy
principle will exhibit this tendency even more strongly as it will attempt to
make sure we have roughly equal information about all corners of the model.
To counter this problem, Madigan and Almond (1995) suggest using the
critiquing approach (Miller 1983). First, elicit a suggested hypothesis from
the tutor, say S = s0, where S denotes a node associated with a skill, and
s0 is one particular state of S. The system then selects tasks to maximize
the probability of quickly accepting or rejecting s0. Thus, once the tutor has
suggested a hypothesis, the system only selects activities that are of high
relevance (expected weight of evidence) to that hypothesis. If the hypothesis
is rejected, the tutor is prompted for an alternative suggestion, and so on. Once
a hypothesis has been chosen, EWOE for the chosen hypothesis becomes the
criteria for selecting a task. The sequence of hypothesis would come from the
instructional design of the course and could be related to the natural order of
learning.
The adaptive algorithm currently used in the ACED prototype of Exam-
ple 7.5 (Shute et al. 2008) consists of a two stage decision-making process:
(1) selection of a target node (what to assess), then (2) selection of a speciﬁc
task tied to that node (how the target is assessed). The ﬁrst stage determines

7.4 Test Construction
219
solveArithmeticProblems
tableArithmetic
visualArithmetic
induceRulesArithmetic
extendArithmetic
verbalRuleArithmetic
algebraRuleArithmetic
explicitArithmetic
commonDifference
solveGeometricProblems
solveSequenceProblems
recognizeSequences
distinguishTypes
solveOtherRecursiveProblems
recursiveRuleArithmetic
extendGeometric
tableGeometric
visualGeometric
induceRulesGeometric
commonRatio
algebraRuleGeometric
verbalRuleGeometric
recursiveRuleGeometric
explicitGeometric
visualOtherRecursive
extendOtherRecursive
tableOtherRecursive
induceRulesOtherRecursive
algebraRuleOtherRecursive
recursiveRuleOtherRecursive
explicitOtherRecursive
verbalRuleOtherRecursive
Fig. 7.7 Proﬁciency model for ACED assessment
Overall proﬁciency in sequences is divided into three branches and then further
divided into nodes corresponding to various skills related to series. To simplify the
presentation, only 32 of 42 nodes are displayed.
the appropriate proﬁciency, represented as a Bayesian network variable, as the
target of the proﬁciency for the adaptive process. Given the hierarchical nature
of the cognitive model, the main target variables map to one of the parent
nodes of three branches; i.e., solving/understanding (1) arithmetic, (2) geo-
metric, and (3) other recursive sequences. The highest node can also serve
as a general target variable: i.e., understanding sequences (which subsumes
the three branches). Once the target node (proﬁciency variable) is selected,
a cut point (“High” vs. “Medium or Low” or “High or Medium” vs. “Low”)
is selected as well. Together the target node and cut point make up a target

220
7 Explanation and Test Construction
hypothesis. The idea is that once the value of a target hypothesis is learned
to a given threshold of accuracy, then a new target will be selected.
The second stage commences once a target hypothesis (proﬁciency variable
and cut point) is identiﬁed as the assessment goal for the next cycle of task
administration. The second stage then selects the task from the pool which
maximizes the EWOE provided for the hypothesis represented by the tar-
get node and cut point. After the outcome from a particular task is received
from the learner, we update our beliefs about the learner’s proﬁciency as
represented in the Bayes net. If the target hypothesis has been learned to
the desired degree of accuracy, the selection process returns to stage one and
selects a new target hypothesis. Otherwise, it continues selecting tasks with
maximum EWOE (in the updated Bayes net) about the current target hypoth-
esis.
The critiquing strategy can be combined with a strategy of switching
between assessment mode and instructional mode mentioned above, to make
an activity selection engine for a intelligent tutoring or e-Learning system.
Here the instructional goals of the system would be described as a series of
goals which the system would try to achieve. Each goal would be formulated
as a hypothesis described as a variable and a cut state for that variable. For
each goal in turn, the system would ﬁrst attempt to assess that the hypothesis
was true, and if it uncovered suﬃcient evidence that the hypothesis was false,
it would switch to instructional mode and attempt to remedy the problem.
The system would continue until all goals are met. Having models for student
growth and learning would be useful in this application (Sect. 16.2.2).
7.4.3 Fixed-Form Tests
In some ways, producing a form for an adaptive test seems easier than ﬁnding
a good form for a ﬁxed-form test. In the adaptive test, choosing the next task
requires that at each step we maximize the EWOE for one item based on the
observations we have seen so far. Thus we are doing each step of maximization
with respect to the conditional distribution, and only calculating the weights
of evidence for the observations from one task at a time. This is the myopic
search strategy, which is not optimal, but is computationally simple.
For a ﬁxed-form test, we need to maximize the joint expected weight of
evidence over all of the variables in the test. As we noted above, the joint
evidence as not necessarily the same as the sum of the marginal evidence for
each task. This is the nonmyopic search problem, and in general it is hard (at
a worst case involving iterations over both proﬁciency proﬁles and outcome
vectors). Thus, we need approximation methods to tackle this problem.
The problem of assembling optimal test forms from a bank of items has
been successfully addressed in the context of item response theory and clas-
sical test theory, using the machinery of combinatorial optimization (van der
Linden 2005). This approach is readily adapted to assembling optimal col-
lections of tasks for inference about proﬁciency variables in Bayes nets. This

7.4 Test Construction
221
section describes how to express form assembly as a 0/1 linear programming
problem.
In a traditional test assembly framework, we have a collection of items
which can be selected for the use on a particular form. Let uj = 1 if item j
is selected to appear on the form, and let uj = 0 if it does not appear on the
form. Linear programming is a mathematical technique which seeks to ﬁnd a
set of 0/1 values for the uj’s that gives the maximum value of an objective
function given a series of constraints.
In ECD, the rules for building a form of the assessment are determined by
the assembly model. First, the assembly model is a container which tells the
assessment assembler which student, evidence, and task models are available
to be used in this assessment. Second it contains a number of rules for the
optimization routine: target rules—rules that deﬁne the objective function,—
and constraints—rules that deﬁne other aspects of the tests.
Using the instructions in the assembly model, the assessment is assembled
from a collection of tasks and their corresponding links (Sect. 13.2.3)—versions
of the evidence model whose parameters are adjusted for a speciﬁc task. (For
high-stakes assessments, this usually involves pretesting and calibration, c.f.
Part II. For low-stakes assessments, often the default parameter values in the
evidence model are good enough.) Note that there is also a Q-matrix corre-
sponding to this collection of tasks. Let qjk = 1 if Task j taps Proﬁciency k
(i.e., Proﬁciency j is in the footprint of the link for Task j).
The objective function is the heart of an optimization problem. It is the
quantity the designer wants to minimize or maximize, subject to constraints.
We will deﬁne objective functions in terms of the amount of information tasks
provide for hypotheses about skill proﬁles. The target rules of the assembly
model represent a series of hypotheses. Let H1, . . . , HM be hypothesis con-
cerning the proﬁciency variables, S. For example, if there are K dichotomous
proﬁciency variables, we could deﬁne K hypotheses Hk : Sk = 1. We could
deﬁne additional hypotheses about particular combinations of skills, such as
{Sk = 1 and Sk′ = 1}. If proﬁciencies have multiple states, we could deﬁne
hypotheses such as Sk > novice. In addition to the hypotheses, H1, . . . , HM,
we need a corresponding set of weights, w1, . . . , wM indicating the relative
importance of the hypothesis. An objective function for assembling a test
form is deﬁned as the weighted sum of EWOE (or value of information), the
tasks in the assessment. In other words, the objective is to maximize
J

j=1
M

m=1
ujwmEW(Hm:Xj),
(7.10)
where Xj are the observable outcome variables associated with Task j.
Equation 7.10 has a trivial maximum: simply include every single task in
the collection on the form. This is not a very interesting solution, however. For
one thing, it is likely to require that the examinee spend far too much time on
the test. For another thing, it means that all of the items in the collection will

222
7 Explanation and Test Construction
be exposed and none will be saved for later use. A far more realistic solution
would be to maximize Eq. 7.10 subject to a constraint that no more than N
tasks be used.
In practice, there are a wide variety of constraints that the designers would
like to put on the test form. The constraints of the assembly model are how
these are recorded. Some typical constraints are:
•
Minimum and Maximum number of tasks. Constraints of this type are
straightforward. Suppose the test design calls for between 30 and 60 tasks.
This can be represented with inequalities: 30 ≤ uj ≤60.
•
Should be able to complete in speciﬁed time. Usually there is either a ﬁxed
time limited or an expected time to complete an assessment. Let tj be the
75th percentile of the distribution of times required to complete the task
in pilot test attempts. The constraint that most students should complete
the assessment in 120 min is expressed with the inequality  ujtj ≤120.
•
Minimum or maximum number of tasks from a certain task model. Although
proﬁciency variables are deﬁned by subject matter experts based on claims,
unless the set of tasks in the form supports the targeted set of claims, the
eﬀective meaning of the proﬁciency variable might be diﬀerent from the
intended meaning. Consider a proﬁciency variable with the interpretation
“Understands Tables and Graphs.” If the form consisted of all table tasks,
and no graph tasks, the eﬀective meaning of that variable would diﬀer
from its label. To avoid such problems, the assembly model could require
a minimum number of tasks from a certain task model. To avoid weighting
a claim or set of claims too heavily, it could also include a maximum from
that task model.
To express this as an inequality, let T Mjn = 1 if Task j comes Task
Model n and 0 otherwise. Let N n be the minimum number of tasks from
Task Model n and let N n be the maximum. This adds one constraint for
each task model type of the form N n ≤
j ujT Mjn ≤N n.
•
Spanning Contexts. Often the assessment needs to span content at a ﬁner
grain size than that of the task model. Here, task model variables can
be used to describe that ﬁner detail of content. For example, a language
test might require that there are a mixture of tasks spanning both formal
academic and informal non-academic contexts. Let Yj be a task model
variable that represents the context of the task and let Yjv be an indicator
variable that takes on the value 1 if task model variable Yj = v and 0 oth-
erwise. The constraint can be written in the form of a series of inequalities
for each possible value v of Yj, NY v ≤
j Yjv ≤NY v.
Note that we can also consider constraints on task model variables nested
within tasks. For example, we could require an easy, medium and hard
variant of each task model by appropriately constraining a task model
variable in each task corresponding to diﬃculty.
•
Number of tasks tapping a given proﬁciency. Suppose that the proﬁciency
model has several proﬁciency variables, S1, . . . , SK. Almost certainly the

7.4 Test Construction
223
form should achieve some sort of balance among the amounts of evidence
gathered about each variable. If the objective function is written as the
sum of hypotheses about a number of proﬁciency variables, the optimiza-
tion will balance across proﬁciencies according to the provided weights. A
second approach is to write the objective function in terms of some over-
all ability variable, but to constrain the form so that there is a minimum
number of items with direct evidence about a particular proﬁciency. The
second approach has a certain advantage in that it supports reporting in
terms of number right scales on subtests. For example, a report that an
examinee got 7 out of 10 possible points in tasks which address Skill Sk
would provide a good explanation of a score report that said a student’s
probability of mastery of Sk is 65 %. For that reason, the assembly model
may constrain the minimum (or maximum) number of tasks that provide
direct evidence of a particular skill.
This is done through the Q-matrix. In particular, 
j ujqjk indicates the
number of tasks providing direct evidence for Skill Sk. This sum then forms
the object of the constraint. Note that it might be better to work with the
version of the Q-matrix that is focused on observables rather than tasks.
Then the constraint would be on the number of observations relevant to
Skill Sj.
•
Simple versus complex tasks. Simple structure tasks—tasks that tap exactly
one skill variable—hold an important place in form design. Tasks that tap
multiple skills usually support competing explanations for poor perfor-
mance. Adding a few simple structure tasks usually helps disambiguate
the causes of the poor performance. On the other hand, complex tasks
that tap multiple proﬁciencies are important because they are often closer
to the kinds of authentic tasks that constitute valued work in the domain
of the assessment.
For each Task j, let QRj = 
k qjk be the sum of 1s in the row of the
Q-matrix for Task j, indicating the number of proﬁciency variable parents
it has. This is a measure of task complexity. Let QRj == c be an expres-
sion which evaluates to 1 if QRj = c and 0 if not. Then an constraint
on the number of simple structure tasks could be expressed through the
expression 
j uj(QRj == 1). Similarly, the number of tasks providing
direct evidence for 2, 3 or more proﬁciency variables could be constrained.
Also, the overall complexity of the the assessment could be constrained by
putting upper and lower bounds on the expression 
j ujQRj.
•
Incompatible tasks. There are a number of cases where we might not want
two tasks to appear on the same form. For example, one task may contain
the solution to another in its background material. Or two variant tasks
from the same task model may be so close that they should not appear in
the same form. Or maybe both tasks have a similar context and familiarity
with that context might provide an unmodeled dependence among the
observables from the two tasks.

224
7 Explanation and Test Construction
This can be also handled with constraints. Often what is done is to create
an enemy list, E, of tasks that should not appear on the same form. The
constraint is then 
j∈E uj ≤1. Assessment designs often require many
such enemy lists. Note that enemy lists can be deﬁned through task model
variables: Any task which has a particular task model variable set to a
certain value might appear in an enemy list.
•
Item sets. One problem that has long been diﬃcult for conventional test
design is how to accommodate item sets: items that must appear together
on the same form because the share common stimulus material (e.g.,
a reading passage). Optimization algorithms that use a greedy function
to optimize the objective function (Eq. 7.10) can easily get into trouble
because once they pick one item from the set, the algorithm must then pick
several others from that set. This can lead to bad forms if the remaining
items in the set are too hard, too easy, or do not meet other constraints.
Although the problem is more noticeable in adaptive testing, it makes the
optimization problem more diﬃcult in ﬁxed form tests as well.
The ECD model avoids many of the diﬃculties by assembling forms from
tasks instead of items. Usually, an item set can be modeled as a single task.
As the selection algorithm considers the joint evidence from the task, it is
harder to get stuck by making a poor initial choice. However, there may
be other considerations here: tasks may be bound together in scenarios,
or some but all items from a set might be needed. Additional constraints
can be written to meet these conditions.
The assembly model must contain a target rule and at least one constraint.
There can be as many or as few constraints of each type as are needed to
express the intent of the designers and to ensure that suﬃcient evidence is
gathered for all of the claims. By expressing the target rule as a function
of the task indicators, uj, and the constrains as inequalities using the task
indicators, one can use standard 0/1 linear programming to assemble a test
form that resembles a previous form or to assemble multiple parallel test
forms. Standard optimization theory and software (e.g., Nocedal and Wright
2006) can be applied. Alternative approaches and many insights to particular
challenges and kinds of constraints in test-assembly more generally are found
in van der Linden (2005).
7.5 Reliability and Assessment Information
When we build the evidence model P(X|S) we are acknowledging that the
relationship between the proﬁciency variables, S and the observed outcomes,
X is a probabilistic one. In other words, the outcome pattern X is not a
pure measure of the proﬁciency variable S, but contains some noise that is
irrelevant to what we are trying to measure. In engineering terms, we could

7.5 Reliability and Assessment Information
225
think about the signal to noise ratio for the assessment; in psychometrics we
speak of the reliability.
Note that not all sources of noise are actually irrelevant to the construct
we are trying to measure. Take for example an assessment of communicative
competence in a given language. By restricting the setting to the academic
environment, we remove one source of variability. However, other settings may
also be relevant to the kinds of inferences we are trying to make. For example,
if we are trying to understand how well a potential student is likely to be able
to get by living in a foreign country, settings related to shopping and inter-
acting with the local bureaucracy may be equally important. In assessment,
reliability is usually taken as a measure of the irrelevant sources of variability
given a speciﬁed domain of tasks and test procedures (Brennan 2001).
Our treatment of reliability with Bayes nets diﬀers from that of classi-
cal test theory in two important respects. First, if our proﬁciency model is
expressed as a Bayesian network, then our scores will typically be either clas-
siﬁcations of participants according their proﬁciency variables or posterior
distributions over one or more proﬁciency variables. The majority of the lit-
erature on reliability is devoted to continuous or integer valued scores. Even
when authors do talk about classiﬁcations, it is usually in the context of a
cut score on a continuous variable. Second, classical test theory relies on the
concept of a true score. Typically, the distribution of the true score in the
population is unknown and must be estimated from data. In our case, the
true score corresponds to the skill proﬁle, S. The proﬁciency model provides
the population distribution for the skill proﬁle, P(S).
For simplicity, we start with purely discrete scores, where the student
is classiﬁed as having the skill proﬁle with the highest posterior probability
(the MAP estimate). Section 7.5.1 looks at some measures of accuracy, and
Sect. 7.5.2 looks at some measures of consistency between two test forms.
However, the Bayes net score is not just a single best proﬁciency proﬁle, but
rather a probability distribution over possible proﬁles. These contain more
information than the point estimates, and hence are usually better scores.
Section 7.5.3 extends the discrete accuracy and consistency measures to this
continuous world.
7.5.1 Accuracy Matrix
We start with a classiﬁcation score. We partition the space of skill proﬁles into
a series of disjoint hypotheses, H1, . . . , HK, which span the space of possible
skill proﬁles. A common case is to look at the value of one proﬁciency variable
ignoring the others; that is Hj : Sj = expert. A more sophisticated model
might look at a number of possible courses a student could be placed in and
what kinds of student would beneﬁt from which course, yielding a partitioning
of all possible vectors in S according to placement based on proﬁciency proﬁles.
When there are exactly two hypotheses, this corresponds to the setup in the
weight of evidence calculation above. But when the students are to be classiﬁed

226
7 Explanation and Test Construction
into more than two categories, a new measure is needed which extends to
multiple categories.
Suppose that we observe a pattern of outcomes X from the collection
of tasks that appears on one form of the assessment. By Bayes’ theorem we
obtain the posterior distribution P(S|X), and the implied posterior probability
for each hypothesis in the partition, or P(hk|X). We can then deﬁne a point
estimate for H by ˆH = maxhk P(hk|X). This is the Maximum A Posteriori
or MAP estimate for H. It will be a function of X so we can write ˆH(X).
Doing this assumes that the utility function associated with misclassiﬁca-
tion is relatively symmetric. That might not always be the case. Again in a
licensure test it is more regrettable to license somebody who is not qualiﬁed
than to make the opposite mistake. Similarly, it may be much more regret-
table to fail to identify a student who needs remediation than the opposite. In
such case, instead of choosing the value of ˆH which maximizes the posterior
probability, we would take the one which maximizes expected utility. This is
called the Bayes decision and is covered in standard texts on decision theory
(e.g., DeGroot 1970; Berger 1985).
We deﬁne the elements of the accuracy matrix2 A as follows:
aij = P(H = hi, ˆH = hj) =

x: ˆ
H(x)=hj
P(x|H = hi)P(H = hi) .
(7.11)
This is the probability that when hi is the correct hypothesis, a response
vector x will be observed for which ˆH = hj is the decision. The diagonal of
this matrix corresponds to the cases where the decision agrees with the true
classiﬁcation. Perfect agreement would result in a diagonal matrix. Thus, we
can deﬁne the accuracy as the trace of the matrix, that is, 
k akk.
The accuracy matrix A may be diﬃcult to calculate analytically, especially
for a long assessment. In general, evaluating Eq. 7.11 involves iteration over
both the set of possible skill proﬁles and the set of possible outcome patterns.
This becomes prohibitively expensive as the number and complexity of the
tasks in the assessment increases. However, it can be easily estimated by a
simple simulation experiment. First randomly select a skill proﬁle according to
the distribution of the proﬁciency model, P(S). We can then assign the value
of H based on the selected skill proﬁle S. Next, we randomly select an outcome
pattern X according to the distribution of the evidence model P(X|S). We can
then classify the simulated outcome with an estimated value of the hypothesis
ˆH(X). If we repeat this experiment many times, the observed frequencies will
converge to the accuracy matrix.
This experiment contains two important assumptions: The ﬁrst is that the
model is correct, i.e., the model used to generate the data is the same as the
one used in the classiﬁcation. In practice we can never know the true data
generation model. The second is that there is no accounting of the uncertainty
2 This is sometimes called a confusion matrix, referring to its oﬀ-diagonal elements.

7.5 Reliability and Assessment Information
227
about the probabilities (or parameters from which they are obtained) used
to generate the Ss and the Xs. Part II takes up the issue of uncertainty
about the parameters. Taking these problems into consideration, calculating
the accuracy matrix estimate in this fashion is really an upper bound on the
true accuracy of the assessment.
Example 7.6 (Language Test Accuracy Matrix). Consider once more
the simpliﬁed language test from Mislevy (1995c) described in Example 7.1,
(see also Appendix A.2). Suppose we perform the following experiment. First,
we simulate 1000 possible proﬁciency proﬁles from the proﬁciency model.
Next, we generate a response vector over 63 geometric sequence tasks for
each of the 1000 simulees. Finally, we score the test for all 1000 simulees
(ignoring their actual proﬁciency proﬁles). For each simulee, we should now
have both their “true” (from the simulation) value of the Reading, Writing,
Speaking and Listening nodes and the most likely (MAP) estimate for each
node from the scored response.
We can calculate the accuracy matrix as follows: First, set aij = 0 for all
i and j. Next, for each simulee, if the true value of Reading is i and the MAP
estimate is j, add one to the value of aij. Repeating this for all 1000 simulees
and dividing by 1000 (the number of simulees) yields a matrix like Table 7.1.
Table 7.1 Accuracy matrices for Reading and Writing based on 1000 simulated
students
Reading
Writing
Novice Intermediate Advanced
Novice Intermediate Advanced
Novice
0.229
0.025
0.000
0.163
0.097
0.000
Intermediate
0.025
0.445
0.029
0.053
0.388
0.065
Advanced
0.000
0.040
0.207
0.000
0.051
0.183
Some authors take the accuracy deﬁned in this way as a measure of validity
rather than one of reliability. However, this contains the implicit assumption
that the set of hypotheses H1, . . . , HK and by extension the proﬁciency vari-
ables S represent the construct on which we wish to base our decisions. We
prefer to think of the accuracy as a measure of internal consistency under the
model, that is, of reliability, and reserve the term “validity” for measures which
take into account the use and consequences of the classiﬁcation (Sect. 16.4).
Many other important measures of agreement can be derived from the
accuracy matrix. In deﬁning those measures, it will be helpful to have notation
for the row and column sums. Let ai+ = 
j aij = P(Hi) be the sum of all
of the elements in Row i. This is the marginal (population) probability of the
hypothesis. Let a+j = 
i aij = P( ˆHj). This is the marginal probability of
the classiﬁcations.

228
7 Explanation and Test Construction
Simply normalizing the accuracy matrix by dividing by the row sums pro-
duces interesting results. The matrix normalized in this way, aij/ai+ = P( ˆH =
hj|H = hi), produces the operating characteristics of the assessment. If the
hypothesis is binary, then a11/a1+ is the sensitivity of the test, the probability
of asserting that the hypothesis holds when it is in fact true. Similarly, a22/a2+
is the speciﬁcity of the test, the probability of asserting that the hypothesis
is false when in fact it is false. These terms are used frequently in medical
testing.
Often the multiple measures of the operating characteristics are more use-
ful than a single measure describing accuracy. This is particularly true because
most of the single number summaries depend on the population distribution
of H. The operating characteristics speciﬁcally condition out this distribution.
They are still a useful measure of the strength of evidence in the assessment
even when all of the members of the sample have the same value for the
hypothesis.
Normalizing by the column sums also has another interesting interpreta-
tion. Now we are conditioning on observed decision, aij/a+j = P(H = hi| ˆH =
hj). The resulting conditional probability distributions answer the question,
“If the assessment classiﬁes a participant as ˆH, what is the probability that
this is the true classiﬁcation?” This is the question that end users of the
assessment scores would very much like answered. As in the rare disease prob-
lem (Example 3.6), the probability of true classiﬁcation depends on both the
operating characteristics of the test and the population distribution of the
hypothesis.
The accuracy, 
i aii, answers the question “What is the probability that
the classiﬁcation assigned on the basis of this assessment will agree with
truth?” Note that it is possible to get a fairly large agreement by chance, even
if the classiﬁcation and truth are independent. Consequently, some authors
recommend adjusting the accuracy for chance agreement.
One such adjusted agreement is Cohen’s κ. Fleiss et al. (2003) note that
adjusting for chance agreement uniﬁes a number of diﬀerent measures of agree-
ment in a 2 by 2 table. If the true value of the hypothesis, H and the estimated
hypothesis, ˆH were two raters acting independently, the probability of agree-
ment by chance would be 
i ai+a+i. The coeﬃcient κ is expressed as a ratio
of the obtained accuracy corrected for chance to the ideal accuracy:
κ =

i aii −
i ai+a+i
1 −
i ai+a+i
.
(7.12)
The chance term is based on the idea that the two classiﬁcation mechanism
are independent. Thus Cohen’s κ answers the question “How much better is
the classiﬁcation given by this assessment than what we would expect if the
assessment was independent of truth?” Sometimes when the categories are
ordered, κ is weighted so that classiﬁcations that are one category away are
worth more that classiﬁcations that are multiple categories away. In either

7.5 Reliability and Assessment Information
229
case, κ is easier to interpret as a measure of the consistency of two classiﬁers
(Sect. 7.5.2) than the accuracy of one classiﬁer.
Goodman and Kruskal (1954) oﬀer a diﬀerent statistic, λ, that using a dif-
ferent baseline, max(pH), for adjusting the agreement statistic (see also Bren-
nan and Prediger 1977). This is the agreement level that would be achieved
by simply classifying everybody at the most likely state.
λ =

n an,n −maxn an,+
1 −maxn an,+
.
The metric λ corresponds to the question, “How much better do we do with
this assessment than simply classifying each person at the population mode?”
This index relates directly to the decision of whether or not to use the test.
Although less well known that Cohen’s κ, Goodman and Kruskal’s λ
is often a better choice when talking about the accuracy of an assessment
(regardless of the method used to obtain the estimates). In particular, the
question answered by λ is often more interesting. While κ answers how much
better is the agreement (between the truth and the classiﬁer), λ answer the
question how much better is it to use the classiﬁer than not. In fact, neither
measure may be the ideal measure; Goodman and Kruskal (1954), oﬀer a
number of alternatives that could be explored.
Example 7.7 (Simpliﬁed Language Test Accuracy Matrix, Kappa
and Lambda). Using the estimated accuracy matrix for the simpliﬁed lan-
guage test (Table 7.1, Example 7.6), we can calculate Cohen’s κ and Good-
man and Kruskal’s λ. To begin, we ﬁnd the diagonal of the Reading portion
of Table 7.1; this is 0.881. In other words, this form of the assessment clas-
siﬁes slightly almost 90 % of the examinees correctly on reading. Next, we
sum over the rows and columns to produce marginal distributions for the
true proﬁciency levels, (.254, .499, .247), and the estimated proﬁciency levels,
(.254, .510, .236).
To calculate κ, we need to calculate the probability of chance agreement.
We get this by multiplying the two vectors of marginal probabilities and taking
the sum, which yields 0.377. Thus, about 1/3 of the time we are likely to get
the correct classiﬁcation just by chance. The adjusted agreement is now κ =
(0.881−0.377)/(1−0.377) = 0.809, which means we are getting approximately
a 80 % improvement over the agreement we would have gotten if we just
assigned everybody a label randomly.
To calculate λ, we note that the modal category is Intermediate, and that
the probability that a randomly chosen simulee has Intermediate ability is
.499. In other words, if we rated everybody as Intermediate we would get
approximately 1/2 of the ratings correct. The adjusted agreement is now λ =
(0.881−0.499)/(1−0.499) = 0.763, which means we are getting approximately
a 75 % improvement over the agreement we would have gotten if we just
assigned everybody the label Intermediate.

230
7 Explanation and Test Construction
Turning to the Writing variable, similar calculations show κ = .567 and
λ = .462. These numbers are smaller as a consequence of the test design.
In particular, of the 16 tasks in this assessment all but the 5 listening tasks
involve at least some Reading and hence provide evidence for Reading. Only
the three Writing tasks provide direct evidence for Writing, and because those
are integrated tasks that also involve Reading, their evidence is weaker. Thus,
both λ and κ are smaller.
Note that the Bayesian network does not actually assign each student to
a category, rather it gives a probability distribution over the categories. We
can do slightly better if we look at the probabilities rather than just the most
likely category. Section 7.5.3 explores this. In fact, this is one example of a
scoring rule for Bayesian networks; Chap. 10 explores other scoring rules.
7.5.2 Consistency Matrix
Suppose that we have two parallel forms of the assessment, Form X and
Form Y . We could produce two accuracy matrices AX and AY , one for each
form. The Consistency Matrix is the product of those two accuracy matrices,
C = At
XAY . The normalized rows and columns represent conditional proba-
bilities which describe what we expect to happen when a person who takes
Form X later takes Form Y and visa versa. This is of practical importance to
testing program where the same assessment (with alternative forms) is given
over and over again to a similar population of examinees. In this case, large
shifts in the classiﬁcation is likely to produce confusion among the test-takers
and score-users.
The consistency matrix can be estimated with a simulation experiment as
described above, it can also be estimated by giving both Form X and Form Y
to a sample of examinees. If the test is long enough, it could also be used
to form split-half estimates of reliability. However, this may be tricky with a
diagnostic assessment. In particular, there may only be a few tasks providing
direct evidence for each proﬁciency of interest. Hence, the half-tests may be
very unbalanced or have very low reliability.
The consistency is the sum of the diagonal elements of the consistency
matrix, 
i cii. This answers the question, “What fraction of examinees who
take both Form X and Form Y will get the same classiﬁcation with respect
to hypothesis H?” Cohen’s κ is frequently used with the consistency matrix
as well. It answers the question “How much better do the two forms agree
than two form which are independent, that is measuring diﬀerent aspects of
proﬁciency?” Again, it may be worth exploring some of the other measures
described in Goodman and Kruskal (1954) as well.
7.5.3 Expected Value Matrix
One source of error in both the accuracy matrix and classiﬁcation matrix is
that we are assigning a person to a class on the basis of the MAP estimate for

7.5 Reliability and Assessment Information
231
the hypothesis. This gives equal weight to someone who we believe with high
conﬁdence is in one category and someone who is on the border between two
categories. By reporting the marginal probability of classiﬁcation rather than
the MAP estimate, we should better convey our uncertainty about the truth.
Suppose we simulate proﬁciency proﬁles and outcome vectors from N
simulees. Let Sn be the proﬁciency proﬁle for Simulee n and let Xn be the out-
come vector. Then P(H|Xn) is the probabilistic classiﬁcation that we would
assign to Simulee n on the basis of the outcome vector Xn. We can deﬁne
a probabilistic classiﬁcation matrix for Hypothesis H by summing over these
classiﬁcations.
zij =

n:H(Sn)=hi
P(H = hj|Xn).
(7.13)
Here, zij is the weighted number of individual whose true classiﬁcation is hi
who are classiﬁed as hj, where their weights are the posterior probability of
classiﬁcation in that class. In other words, in the simulation to estimate the
accuracy matrix we place a simulee from the ith category into the cell for the
jth decision category; to estimate the expected value matrix, we distribute
that simulee across all n cells in the jth column, according to its posterior
probabilities for each.
The sum of the diagonal elements, 
i zii, is another measure of accuracy
for the assessment. We can also look at Cohen’s κ and λ for the probabilistic
classiﬁcation as well. In general, these should do at least as well as their non-
probabilistic counterparts.
Another way to treat the probabilistic scores, P(H|Xn) is to regard them
as predictions of the true value of the hypothesis. In this case within the
conﬁnes of the simulation experiment, we can use the scoring rules in Chap. 10
to evaluate the quality of the assessment for making this particular prediction.
Example 7.8 (Simpliﬁed Language Test Expected Accuracy Matrix).
The procedure is similar to the one used in Example 7.6. The initial simulation
proceeds in the same way. It diﬀers at the scoring step, instead of calculating
the MAP score for Reading we calculate its marginal probability. This score
will be a vector of three numbers over the possible classiﬁcations (Novice,
Intermediate, and Advanced). The “true” value is still a single state.
We can calculate the expected accuracy matrix as follows: First, set zij = 0
for all i and j. Next, for each simulee, let the true value of SolveGeometricProb-
lems is i and the marginal estimate be {p1, p2, p3}, which is a probability vec-
tor. We update the values of zij by adding the probability vector to Row i,
that is, let zij ←zij + pj for j = 1, 2, 3. Repeating this for all 1000 simulees
yields a matrix like Table 7.2. We divide the entries in this matrix by 1,000
to put all of the numbers on a probability scale.
The agreement measures ˜κ and ˜λ are calculated in the same way. In this
case, ˜κReading = .73, ˜λReading = .66, ˜κWriting = .43, and ˜λWriting = .28. These
are lower than the agreement rates based on the modal classiﬁcations (Exam-
ple 7.7), but are more honest about the uncertainty in the classiﬁcations.

232
7 Explanation and Test Construction
Table 7.2 Expected accuracy matrices based on 1000 simulations
Reading
Writing
Novice Intermediate Advanced
Novice Intermediate Advanced
Novice
0.220
0.034
0.000
0.162
0.092
0.007
Intermediate
0.037
0.413
0.050
0.091
0.331
0.084
Advanced
0.000
0.049
0.198
0.003
0.076
0.154
7.5.4 Weight of Evidence as Information
The preceding discussion has introduced many diﬀerent possible measures for
reliability, not just one. That is because when a test user asks about the reli-
ability of an assessment, there are a number of possible motivations for that
question. She might be asking about how the results from the assessment
varies when sampling tasks from a pool of possible tasks. In this case, con-
sistency is the most appropriate answer. She might be asking about how well
the assessment captures the variability in the population; in this case accu-
racy, perhaps as measured by Cohen’s κ is a reasonable choice. She might be
asking whether or not it is worthwhile to give the assessment to learn about
a hypothesis, H. In this case, λ seems appropriate.
Smith (2003) presents another possible meaning for reliability, namely
“Suﬃciency of Information.” Smith points out that a teacher may give an end
of unit quiz expecting all of the students to get all of the items correct. After
all, having ﬁnished the unit, the students should have mastered the material.
This quiz serves several important purposes: (1) it helps the student’s self-
assessment of their progress on this material, and (2) it identiﬁes for the
teacher any students who have not yet mastered the material as predicted.
This assessment has value, even though by many of the reliability measures
posed above may have trivial values because all of the students are expected
to have mastered the material.
Note that the EWOE does not depend on the population distribution of
the hypothesis. The calculations for the EWOE are done with the conditional
distribution given the hypothesis. Thus, if an assessment has a high EWOE
for the hypothesis that the students have mastered the material of the unit
it will be appropriate. (It still may be diﬃcult to estimate the task speciﬁc
parameters from the classroom population as there is little variability with
respect to the proﬁciency variables of interest, but that is a separate problem.)
We have seen that weight of evidence provides a useful mechanism for
explaining how certain patterns of evidence inﬂuence our conclusions about
certain proﬁciency variables. Furthermore, its ability to act like a spot meter
for speciﬁc hypotheses helps us to evaluate how much information is provided
by a proposed assessment design for a speciﬁc purpose. If the assessment does
not provide enough information, we could consider altering the assessment
design, that is the Q-Matrix associated with the collection of tasks in an

7.5 Reliability and Assessment Information
233
assessment form, to obtain more information about the proﬁciency variables
of interest. Lengthening the test is one mechanism for altering the Q-Matrix;
replacing simple structure tasks that tap just one proﬁciency variable with
complex tasks that tap multiple variables, or vice versa, is another. How-
ever, careful test design requires balancing the cost (the biggest component
of which is usually the time the examinee spends taking the test) with both
the information gained and the complexity of the calculations (Sect. 7.4.3).
Exercises
7.1 (Order dependence of WOE). Recall the 5-item math quiz from
Example 6.1. Suppose that a student gets items 1, 2, and 4 correct and items 3
and 5 incorrect. Calculate the weight of evidence for Theta > 0 provided by
each item under the following assumptions:
1. The student works through the problems in order from item 1 to item 5.
2. The student works through the problems in reverse order from item 5 to
item 1.
7.2 (Context variable and WOE). To see what eﬀect the Context variable
has on the weight of evidence, compare the ﬁve item IRT model without the
context variable (Sect. 6.1) to the model with the Context variable (Sect. 6.2).
Calculate the weight of evidence for Theta > 0 provide by the following evi-
dence for both models:
1. Item 3 and item 4 are both correct.
2. Item 3 and item 4 are both incorrect.
For the following exercises, consider a simpliﬁed version of the ACED
model (Shute 2004) given in Fig. 7.8. This simpliﬁed model uses the same
conditional probability table given in Table 7.3 for all proﬁciency variables
except for SolveSequenceProblems, which has a uniform distribution. Also,
attached to all nodes except for SolveSequenceProblems are three tasks meant
to tap that proﬁciency. There are three variants of the tasks, a Hard, Medium
and Easy version. Each evidence model fragment adds one observable variable
with a conditional probability table given by one of the columns in Table 7.4.
7.3 (Direct and indirect evidence). Suppose that a person is assigned a
single medium diﬃculty task attached to the SolveArithmeticProblems pro-
ﬁciency and the person gets that item correct. Calculate the weight of evidence
provided for both SolveArithmeticProblems = H and SolveGeometricProblems =
H. Why is the ﬁrst higher than the second?
7.4 (Chaining weight of evidence). Consider two medium diﬃculty tasks,
one attached to AlgebraRuleGeometric and one attached to InduceRulesGe-
ometric. Calculate the weight of evidence that getting a correct score on

234
7 Explanation and Test Construction
Fig. 7.8 Subset of ACED proﬁciency model for exercises
Table 7.3 Conditional probabilities for ACED subset proﬁciency model
Generated using regression distribution (Sect. 8.5.4) with correlation of .8. All of
the conditional probability tables in the proﬁciency model use this same table
Subskill
Skill
H
M
L
High
0.83 0.09 0.08
Medium 0.40 0.20 0.40
Low
0.08 0.09 0.83
Table 7.4 Conditional probabilities for evidence models for ACED subset
Evidence model fragments that consist of a single dichotomous observable variable
attached to one of the proﬁciency nodes (except the overall proﬁciency). Depending
on the diﬃculty of the task, one of the three columns of this table indicates the
conditional probability for a correct response
Skill
P(Easy = 1) P(Moderate = 1) P(Hard = 1)
High
0.88
0.72
0.49
Medium 0.73
0.50
0.27
Low
0.51
0.28
0.12
each of those tasks (with on other evidence) provides for the proposition
SolveGeometricProblems = H. Why is the one smaller than the other?
7.5 (Correlation and weight of evidence). Make an alternative model for
the ACED subset problem by substituting the conditional probabilities given
in Table 7.5 for the conditional probabilities for the nodes AlgebraRuleGeo-
metric and InduceRulesGeometric. (Both the original and alternative condi-
tional probabilities tables were produced using the Regression Distribution,
Sect. 8.5.4. The original has a correlation of 0.8, while the alternative has a
correlation of 0.9).

7.5 Reliability and Assessment Information
235
Table 7.5 Alternative conditional probabilities for ACED subset proﬁciency model
Generated using regression distribution (Sect. 8.5.4) with correlation of .9.
Subskill
Skill
H
M
L
High
0.90 0.07 0.03
Medium 0.39 0.22 0.39
Low
0.03 0.07 0.90
7.6 (Compensatory, conjunctive and disjunctive distributions). Con-
sider the compensatory, conjunctive, and disjunctive distribution models from
Sect. 6.3. Calculate the weight of evidence for the proposition P1 = H for all
three models under the following conditions:
1. The observable is Right.
2. The observable is Wrong
3. The observable is Right conditioned on the proposition P2 = H.
4. The observable is Wrong conditioned on the proposition P2 = H.
7.7 (Task diﬃculty and EWOE). Consider a test which assesses a single
proﬁciency, Skill, which can take on the values High, Medium and Low. Assume
that all tasks are scored with one of three diﬀerent evidence models with
observable variables Easy, Moderate, and Hard (one observable per evidence
model) which take on the values 0 (wrong) and 1 (right). Let Skill have a
uniform distribution and the conditional probability tables for the observables
follow the distributions given in Table 7.4. (Note, these distributions were
produced by using the DiBello-Samejima models, Sect. 8.5, with diﬃculty
parameters of −1, 0 and +1 for easy, moderate and hard tasks.)
1. Calculate the EWOE for Skill = High versus Skill ∈{Medium, Low} for
each kind of task. Which kind of task provides the best evidence for this
distinction?
2. Calculate the expected weight of evidence for Skill = Low versus Skill ∈
{Medium, High} for each kind of task. Which kind of task provides the best
evidence for this distinction?
7.8 (Eﬀect of prior on EWOE). Consider the assessment described in
Exercise 7.7 using only the medium diﬃculty task. However, this time con-
sider three diﬀerent prior distributions for Skill: (.33, .33, .33), (.6, .2, .2), and
(.6, .3, .1), for the states High, Medium, and Low respectively.
1. Calculate the weight of evidence for the proposition Skill = High when
the observable is correct under all three priors.
2. Calculate the weight of evidence for the proposition Skill = High when
the observable is incorrect under all three priors.

236
7 Explanation and Test Construction
3. Calculate the expected weight of evidence the medium task provides for
the proposition Skill = High under all three priors.
How do the weight of evidence and the expected weight of evidence change
with the change in prior?
7.9 (Value of information). Consider the assessment described in Exer-
cise 7.7. Assume that a certain school district has determined that having a
student in the High state for Skill at the end of the year is worth $ 1200,
having a student at the Medium state is worth $ 1000, and having a student
at the Low state is worth zero (we can always make one particular state worth
zero by subtracting a constant from all of the values). Calculate the value of
information for a Hard, Medium and Easy task.
7.10 (Additivity of WOE and EWOE). Consider a situation in which
neither H nor H is compound and hence X1 ⊥⊥X2|H, H. Demonstrate under
those conditions that:
1. W(H:x1, x2) = W(H:x1) + W(H:x2)
2. EW(H:X1, X2) = EW(H:X1) + EW(H:X2)
7.11 (Test length and WOE). Consider a test with a single proﬁciency
variable, Skill, which takes on two values, High and Low, and let the prior
(population) probability for that task be (.5, .5). Suppose that there is a pool
of tasks to assess that skill, all of which have a single observable outcome X
which takes on values correct and incorrect. Assume that the link models
are identical for all the tasks in the pool and that P(X = correct|Skill =
High) = .8 and P(X = correct|Skill = Low) = .2. Calculate the EWOE for
Skill = High provided by a 5 task test, a 10 task test, and a 25 task test. Hint:
use the results from the Exercise 7.10.
7.12 (Reading passage topic). For a reading comprehension test for gradu-
ate students, the design team intends for there to be between 4–6 tasks calling
for a student to read a passage and then answer questions. The design team
would like the passages to be reasonably balance among topic chosen from
the natural sciences, the social sciences and the humanities. Describe how one
might set up constraints so that all forms will meet this criteria.
7.13 (Bayes net versus number right). Consider the assessment described
in Exercise 7.7 and a form that consists of 10 medium diﬃculty tasks. Suppose
we take this same assessment and score it with number right instead of the
Bayes net. How will the Bayes net and number right scores diﬀer?
7.14 (Accuracy and task diﬃculty). Consider the assessment described
in Exercise 7.7 and two test forms, one consisting of 10 easy tasks and one
consisting of 10 hard tasks. Use a simulation experiment to calculate the accu-
racy matrix for both forms. What can be said about the diﬀerence between
the two forms?

7.5 Reliability and Assessment Information
237
7.15 (Accuracy of language assessment). Modify the simpliﬁed language
assessment (Appendix A.2) by doubling the number of tasks of each type. Use
a simulation experiment to calculate the accuracy matrix for the modiﬁed test.
7.16 (Kappa and Lambda for Speaking and Listening). Calculate
Cohen’s κ and Goodman and Kruskal’s λ for Speaking and Listening pro-
ﬁciencies using the accuracy matrices in Table 7.6. Compare them to the
numbers from Example 7.7, and interpret what they say about the relative
information in the test for the four skills.
Table 7.6 Accuracy matrices for Speaking and Listening based on 1000 simulated
students
Speaking
Listening
Novice Intermediate Advanced
Novice Intermediate Advanced
Novice
0.243
0.027
0.000
0.242
0.054
0.000
Intermediate
0.044
0.390
0.030
0.054
0.290
0.059
Advanced
0.000
0.033
0.233
0.000
0.087
0.214

Part II
Learning and Revising Models from Data

8
Parameters for Bayesian Network Models
While Part I concentrated on models for one student at a time, Part II expands
our horizons to include data from a population of similar students. The most
important result of this transition is that we can use experiential data to
improve our model. In particular, we can learn about the parameters and
structure of the model. Chapter 9 describes a method for learning parameters
from data, Chap. 10 introduces some measures of how well our model ﬁts the
data and surveys a number of techniques for learning model structure from
data. Finally, Chap. 11 illustrates these ideas with an extensive analysis of a
single data set.
This chapter talks about various approaches to parameterizing graphical
models. There is a large and growing literature on parameterizing Bayes nets
and eliciting probabilities from experts more generally (e.g., D´ıez and Druzdzel
2006; Laskey and Mahoney 2000; O’Hagan et al. 2006; Zapata-Rivera 2002).
We bring some of these ideas to bear on the particular context of educational
assessment. Section 8.1 introduces basic notation for graphical parameters.
Section 8.2 discusses the hyper-Markov Laws, conditional independence rela-
tionships among parameters. Section 8.3 introduces the hyper-Dirichlet dis-
tribution, the natural conjugate distribution of the Bayesian network. As we
will see, the hyper-Dirichlet has many parameters as table size increases, and
it is often diﬃcult to assess hyper-Dirichlet priors. The chapter thus explores
two diﬀerent approaches to reducing the number of parameters in the model.
Section 8.4 describes models that add a layer of probabilistic noise to log-
ical functions. Section 8.5 describes a suggestion by Lou DiBello to model
probability tables using Samejima’s graded response model.
8.1 Parameterizing a Graphical Model
The standard directed graphical representation does a good job of hiding
the complexity of the graphical model. The edges in the model represent
probability tables that may have some internal structure. This hides the work
c⃝Springer Science+Business Media New York 2015
241
R. G. Almond et al., Bayesian Networks in Educational Assessment,
Statistics for Social and Behavioral Sciences, DOI 10.1007/978-1-4939-2125-6 8

242
8 Parameters for Bayesian Network Models
we need to do in parameterizing the model. This chapter will look more closely
at the internal structure of conditional probability tables. The next chapter,
on estimation, will introduce a representation called plate notation to clarify
the independence and dependence relationships across tasks, parameters, and
multiple subjects.
The directed hypergraph representation starts to make the internal struc-
ture of conditional probability tables more explicit.
Skill1
Skill2
Task1-Obs
Task2-Obs
Task3-Obs
Fig. 8.1 A simple latent class model with two skills
Reprinted from Almond et al. (2006a) with permission from ETS.
We’ll start with a simple latent skill model with two skills and observable
responses from three tasks, all dichotomous, shown in Fig. 8.1. The symbol ⃝
is used to denote Proﬁciency Model variables that are shared across evidence
models and the symbol ▽is used to denote evidence model variables that are
speciﬁc to a particular task.
Skill1
Skill2
Task1-Obs
Task2-Obs
Task3-Obs
Fig. 8.2 Hypergraph for a simple latent class model with two skills. Square boxes
with tables represent probability tables that must be learned or elicited from experts
Reprinted from Almond et al. (2006a) with permission from ETS.
Next we add icons to represent hyperedges for factorization to produce
Fig. 8.2. Each square box in Fig. 8.2 represents a conditional probability table
we must specify. To do learning, we use parametric forms for these tables. The
parameters “ﬂoat above” the model in a second layer. This is shown in Fig. 8.3.

8.1 Parameterizing a Graphical Model
243
Skill1
Skill2
Task1-Obs
Task2-Obs
Task3-Obs
Fig. 8.3 Second layer of parameters on top of graphical model. Parameters ﬂoat
above the model. When we want to do calculations for an individual learner with
the model, parameter values drop down into the model. Reprinted from Almond et
al. (2006a) with permission from ETS.
The parameters for the model in Fig. 8.3 are as follows:
π1 = P(⃝Skill1)
π2 = P(⃝Skill2)
λ1,2 = P(▽Task1-obs|⃝Skill1, ⃝Skill2)
λ−1,2 = P(▽Task1-obs|¬⃝Skill1, ⃝Skill2)
λ1,−2 = P(▽Task1-obs|⃝Skill1, ¬⃝Skill2)
λ−1,−2 = P(▽Task1-obs|¬⃝Skill1, ¬⃝Skill2)
where ¬⃝Skilli represents the absence of Skill i.
When we are making inferences about a single learner using the techniques
of Part I, we generally do not worry about the parametrization. In this case,
our best estimates for the parameters “drop down” into the model and give us
the values in the conditional probability tables in an ordinary Bayes network.
Only when we want to account for uncertainty about the parameters do we
need to worry about the distribution of the parameters. (For example, we may
want to gauge the impact of this uncertainty on inferences about individuals,
to see if we need to collect more data to estimate them more precisely.)
The directed hypergraph representation allows us to annotate each distri-
bution with the parametric form that it takes. In particular, the table icons

244
8 Parameters for Bayesian Network Models
seen in Figs. 8.2 and 8.3 represent generic conditional multinomial distribu-
tions (Sect. 8.3). We can use distinct icons to indicate particular structures.
For a noisy-or distribution (Sect. 8.4), we would use an OR-gate icon. For a
conjunctive distribution (Sect. 8.5), we would use an AND-gate icon. For a
compensatory distribution (Sect. 8.5), we use a plus sign.
As we go forward, we will need to discuss both distributions over variables
in our models (like Skill1 and Task1-obs) and distributions over parameters of
those distributions (like π1 and λ1,2). We will call distributions over parame-
ters, laws1 and reserve the word distribution for distributions over variables.
Naturally, laws will have hyperparameters. If we choose to give the hyperpa-
rameters distributions (instead of ﬁxing their values) we will call these laws
as well.
Similarly, we will reserve the term variable for a quantity that refers to
a particular learner: observable outcome variables, for variables that describe
the outcomes from scoring responses to presented tasks; proﬁciency variables
for variables that describe knowledge, skills, and abilities of the learner; and
demographic variables for variables that describe ﬁxed characteristics of the
learner. We will use the term parameter to refer to quantities that are con-
stant across all learners: population parameters associated with the proﬁciency
model and link parameters associated with the evidence model for a speciﬁc
task. (In the item response theory (IRT) literature, the latent variable θ is
called a person parameter. In our usage, this would be a latent variable asso-
ciated with the learner, not a parameter. Nothing diﬀerent conceptually—just
terminology to help keep straight what is happening at diﬀerent levels in the
model.)
8.2 Hyper-Markov Laws
Just as the graphical model entails certain probability conditions on the vari-
ables, we need to be able to make independence assumptions about the param-
eters of the model. Spiegelhalter and Lauritzen (1990) deﬁne two diﬀerent
types of independence.
Deﬁnition. Local Parameter Independence. If parameters within a sin-
gle probability table are independent, then the model is said to have local
parameter independence.
For example, if λ1,2, λ1,−2, λ−1,2, and λ−1,−2 in Fig. 8.3 are all indepen-
dent, then we have local parameter independence.
Deﬁnition. Global parameter Independence. If parameters for diﬀer-
ent probability tables are independent, then the model is said to have global
parameter independence.
1 Steﬀen Lauritzen (private communication) suggested this language to distinguish
the various kinds of distributions in a complex model.

8.2 Hyper-Markov Laws
245
For example, in the graph for Fig. 8.3 under global parameter indepen-
dence, π1 and π2 are independent.
Deﬁnition. Hyper-Markov Law. Let G be a graphical model and P be a
set of parameters for that model. A distribution for P which has both the local
and global parameter independence property is called a Hyper-Markov Law.
Both local and global parameter independence are assumptions of conve-
nience. They allow us to prove certain properties of the models we learn from
data. Even if local and global parameter independence hold a priori, though,
they may not hold a posteriori. If any of the variables have missing data for
one or more cases, global parameter independence will not hold in the poste-
rior model. We can get a posteriori local parameter dependence even without
missing data (York 1992).
Global parameter independence also will not hold if there is some com-
mon cause for the variables that is not modeled. Example 8.1 illustrates this
situation.
Example 8.1 (Breakdown of Global Parameter Independence). Sup-
pose the population for our latent skill model was from a mixture of grades.
Figure 8.4 illustrates this model. As both Skill 1 and Skill 2 are more likely
to be acquired with advancing instruction, π1 and π2 likely to be correlated.
Skill1
Skill2
Task1-Obs
Task2-Obs
Task3-Obs
Grade
Fig. 8.4 Introducing the demographic variable Grade breaks global parameter inde-
pendence
Reprinted from Almond et al. (2006a) with permission from ETS.
Adding explicit dependence on grade to our model is one way to deal with
this type of correlation. In point of fact, we could always add the parameters
directly to the model; the “layers” are merely a convenient way of simplifying
the distribution under certain circumstances.

246
8 Parameters for Bayesian Network Models
Another common reason for breakdown of global parameter independence
is that our probability assessments might share a common source of informa-
tion. Suppose that we generate a collection of items for a test from the same
task model. Because they are all similar items, we may use a common prior
distribution for the parameters of the evidence model for that task. However,
there should really be some dependence among those parameters because they
are all based on the same assessment. Ignoring this dependence will cause us
to understate our uncertainty about the ﬁnal conclusions of the model.
While the global parameter independence assumption breaks down easily,
the local parameter independence assumption is suspect from the start. We
can often place meaningful constraints on the parameters of the table.
Example 8.2 (Breakdown of Local Parameter Independence). Sup-
pose that we believe that the probability that a student will get an item right
in the model of Fig. 8.3 is strictly increasing in both Skill 1 and Skill 2. It is
almost certainly true that λ1,2 ≥λ1,−2 ≥λ−1,−2. This violates local parame-
ter independence.
We could reparameterize the table for Task 1-Obs given Skill 1 and Skill 2
as:
λ−1,−2 = φ0 ;
λ1,−2 = φ0 + (1 −λ−1,−2) ∗φ1 ;
λ1,2 = φ0 + (1 −λ−1,−2) ∗φ1 + (1 −λ1,−2) ∗φ2 .
Here the φ’s can be modeled as independent variables on [0,1] but the λ’s
increase in the way we expect.
Despite these problems, global and local parameter independence play a
major part in the theory of learning graphical models. In particular, they are
used in building the natural conjugate distribution for the Bayesian network,
the hyper-Dirichlet distribution.
8.3 The Conditional Multinomial—Hyper-Dirichlet
Family
In order to completely specify our parameterized model, we now need a set of
prior laws for our parameters. When trying to build priors it is often helpful to
look at the conjugate family (Sect. 3.5.3), if one is available. This section shows
how to build the conjugate family for Bayesian networks. One of the most
studied conjugate families, the beta-binomial family, is the starting point.
The Dirichlet-multinomial family is a natural generalization of it. This will
lead naturally to the hyper-Dirichlet—conditional multinomial family.

8.3 The Conditional Multinomial—Hyper-Dirichlet Family
247
8.3.1 Beta-Binomial Family
We already explored the beta-binomial family in Example 3.14. This section
reviews the key ideas.
Let Y be a binomial random variable, i.e., it has the following distribution:
p(y|θ, n) =
n
y

θy(1 −θ)n−y
for y = 0, . . . , n
0
otherwise.
(8.1)
Suppose that n is known and θ is unknown, and we choose the beta prior
law for θ, that is,
f(θ|a, b) ∝θa−1(1 −θ)b−1 .
(8.2)
After Y is observed, applying Bayes theorem produces the posterior dis-
tribution:
f(θ|a, b, y, n) ∝θa+y−1(1 −θ)b+n−y−1 .
(8.3)
Thus, the posterior is also a beta distribution. Therefore, the beta distri-
bution is the natural conjugate of the binomial distribution.
For certain parameter values, the beta distribution has interesting inter-
pretations. The Beta(1, 1) distribution is uniform between 0 and 1. Beta(.5, .5)
is uniform on the logistic scale. This prior is generated by applying Jeﬀrey’s
rule to the binomial likelihood to produce a prior that is invariant under repa-
rameterization, in terms of the amount of information it provides at diﬀerent
values of the variable (Berger 1985). Finally, although Beta(0, 0) is not a true
probability distribution and is therefore an “improper” prior, the posterior we
get when we combine it with a binomial likelihood is a probability distribution
as long as we have observed at least one instance of both the event and its
converse. The posterior mode using this prior will correspond to the maxi-
mum likelihood estimate. All three of these beta distributions have been put
forward as noninformative priors for the beta distribution, which we might
use if our information about θ were equivalent to no observations at all.
We can interpret the a and b hyperparameters of the beta law as the
number of positive and negative cases the expert has seen, i.e., the posterior
distribution after seeing a positive and b negative cases. This makes a + b
the “eﬀective sample size” of our prior distribution. In particular, it is the
size of a sample whose weight is equal to the prior in Bayes theorem. Often
a good way to approach experts about a beta prior is to ask for a mean, p∗,
and an eﬀective sample size, n∗. This expresses their best guess about θ and
their degree of conﬁdence in it. The parameters of the corresponding beta
distribution are then a = p∗n∗and b = (1 −p∗)n∗.

248
8 Parameters for Bayesian Network Models
8.3.2 Dirichlet-Multinomial Family
Just as the multinomial distribution is a natural generalization of the binomial
distribution, the Dirichlet law is a natural generalization of the beta law.
Let Y be a multinomial random variable that can take on one of K cate-
gories. Let Yk be number of observations that fall in category k in n experi-
ments. The multinomial likelihood is then:
p(y|θ, n) =

n
y1 ··· yK

θy1
1 · · · · · θyK
K
for yk ∈{0, . . . , n} and  yk = n
0
otherwise.
(8.4)
The natural generalization of beta law for θ is the Dirichlet law:
f(θ|a) ∝θa1−1
1
· · · · · θaK−1
K
,
(8.5)
where θ1 + · · · + θK = 1. The Dirichlet law is the natural conjugate for the
multinomial distribution. The Dirichlet posterior given data y is Dirichlet(a1+
y1, . . . , aK + yK). The beta distribution is a special case of the Dirichlet when
K = 2.
Setting all ak = 0 produces an improper prior similar to the Beta(0, 0)
prior, where the observed proportions will be both maximum likelihood esti-
mates and posterior modes. Applying Jeﬀrey’s rule to produce a noninforma-
tive prior that is invariant with respect to variable transformations yields a
Dirichlet distribution with all parameters set to 1/2.
As with the beta law, we can interpret the sum of the parameters of
the Dirichlet law as the “eﬀective sample size.” Again we can elicit Dirich-
let parameters by eliciting θ∗and n∗(eﬀective sample size of the expert’s
opinion). Then set ak = n∗θ∗
k.
8.3.3 The Hyper-Dirichlet Law
Suppose we have a Bayesian network, and a sample of individuals for whom
we have observed all the variables in our Bayes Net. This is unrealistic in
many of the examples discussed in the previous chapters as most of them
contain latent variables. Chapter 9 will discuss methods for getting around
that problem. Still, an understanding of how the hyper-Dirichlet law behaves
in the complete data case will be helpful in designing tools to deal with the
missing data that latent variables eﬀectively constitute.
Next assume that the global parameter independence assumption holds.
This allows us to build the prior law one probability table at a time. In the
Bayes net, we have two types of probability tables we must parameterize:
Unconditional tables; i.e., variables with no parents. The data for this kind
of table will be multinomial, so the Dirichlet law is the natural conjugate.

8.3 The Conditional Multinomial—Hyper-Dirichlet Family
249
Conditional tables, that is, variables with one or more parents. In this
case the data will be a multinomial distribution for each conﬁguration of
the parent variables. We call this distribution the conditional multinomial
distribution (sometimes it is called the product multinomial). By the local
parameter independence assumption, the parameters of these multinomi-
als are independent. Thus, the natural conjugate prior is a collection of
Dirichlet priors.
Constructing Dirichlet prior laws for every table in this way produces a
hyper-Dirichlet law. Spiegelhalter and Lauritzen (1990) show that under the
global and local parameter independence assumptions, the posterior law is also
a hyper-Dirichlet law (in which the global and local parameter independence
assumptions continue to hold). Thus, the hyper-Dirichlet law is the natural
conjugate of the Bayes net. Furthermore, the hyper-Dirichlet forms a natural
noninformative prior for the Bayes net. This property is exploited in many
algorithms for discovering models from data.
Although Dirichlet priors are generally straightforward to elicit, for exam-
ple by the mean and eﬀective sample size method, in practice the work is
tedious for a large size network, even for experts who are comfortable with
the mathematics. The hyper-Dirichlet prior has as many free parameters as
their were in the original network–every probability in every table. All of them
must be elicited.
The large number of parameters also means that without strong priors,
a large amount of data may be necessary to reliably learn the parameters of
the network. This is especially problematic as some conﬁgurations of parent
variables might be quite rare. For example, consider a distribution with two
parent variables Skill 1 and Skill 2. Suppose each have ﬁve states and are
moderately correlated in the population. Individuals who are Very High on
Skill 1 and Very Low on Skill 2 are likely to be very rare in the population
at large. Consequently, we are unlikely to do much better than our prior
estimates for the probabilities in this row.
We have two ways to get around these diﬃculties. First, tying together
several rows in the table that we believe have the same values (Almond 1995)
allows us to borrow strength for seldom-observed rows. Second, parametric
models can describe the relationships between the parent and child variables
with just a few parameters. The remainder of this chapter concentrates on
developing parametric models.
In building Bayes nets, we make the choice of what distribution to use on
a table by table basis. Consequently, we tend to use the term hyper-Dirichlet
distribution2 for just one table, i.e., for the collection of independent Dirichlets
for each table. We use a “table” icon in the hypergraph to represent hyper-
Dirichlet distributions (Fig. 8.3).
2 Technically, it is a conditional-multinomial distribution with a hyper-Dirichlet
law, but call it a hyper-Dirichlet distribution for short.

250
8 Parameters for Bayesian Network Models
8.4 Noisy-OR and Noisy-AND Models
To reduce the number of parameters in Bayes nets, many authors have looked
at a class of models known as noisy-OR, or disjunctive models (Pearl 1988;
D´ıez 1993; Srinivas 1993). Noisy-OR-type models have readily interpretable
parameters. They also separate the inﬂuences of the parent variables, allowing
factorizations of the probability distributions that can be exploited for eﬃcient
computation. But because the models suggested in an educational context are
more often conjunctive (Junker and Sijtsma 2001; Rupp et al. 2010), we will
start with the noisy-AND model and return to the noisy-OR later.
Consider a task that requires mastery of two skills for correct performance.
If “Skill 1 is mastered” and “Skill 2 is mastered” then the “Response is
correct” should be true, otherwise it should be false. This is an conjunctive
model as both skills are necessary to solve the item. The following truth table
represents the conjunctive model:
Conditions
Observed outcome
Skill 1
Skill 2
Right
Wrong
Yes
Yes
1
0
Yes
No
0
1
No
Yes
0
1
No
No
0
1
A distribution with this kind of truth table is known as an AND-gate in
the engineering world and is represented with the symbol,
Consequently,
Fig. 8.5 represents the conjunctive model.
Fig. 8.5 A conjunctive model, with no “noise”
Reprinted from Yan et al. (2004) with permission from ETS.
This deterministic model is not realistic in educational testing. A student
who has not mastered one of the required skills may be able to guess the
solution to a problem, or solve it via a diﬀerent mechanism than the one
modeled, giving a false-positive result. Let π−be the probability that a learner
without the skills gets the item correct in some other way:

8.4 Noisy-OR and Noisy-AND Models
251
Conditions
Observed outcome
Skill 1
Skill 2
Right
Wrong
Yes
Yes
1
0
Yes
No
π−
1 −π−
No
Yes
π−
1 −π−
No
No
π−
1 −π−
People who have the skills will occasionally get the item wrong anyway,
or slip (Tatsuoka 1983), whether through failure to apply the skills correctly,
failure to recognize the correct solution path, or carelessness. In this case, we
get a false-negative result. Let π+ be the probability that the learners who
have the requisite skill get the item correct, so 1 −π+ is the probability of a
slip. We then have the model:
Conditions
Observed outcome
Skill 1
Skill 2
Right
Wrong
Yes
Yes
π+
1 −π+
Yes
No
π−
1 −π−
No
Yes
π−
1 −π−
No
No
π−
1 −π−
With just two additional parameters we have made a more realistic model.
We could simplify things further by positing 1 −π+ = π−; that is, the prob-
ability of a false negative is the same as the probability of a false positive.
Junker and Sijtsma (2001) call this structure a DINA (Deterministic Input
Noisy AND) model. Figure 8.6 portrays it by adding a probabilistic inversion
gate to the conjunctive model (Fig. 8.5).
Fig. 8.6 A conjunctive model with noisy output (DINA)
Reprinted from Yan et al. (2004) with permission from ETS.
The classic approach to building a noisy-logic model is to look at inversions
of the inputs. To solve the example problem, the student must either have
mastered Skill 1, or ﬁnd a way to work around that lack of mastery. We call
the probability of ﬁnding that skill workaround r1. Similarly, let r2 be the
probability of ﬁnding a workaround for Skill 2. Then, we have:

252
8 Parameters for Bayesian Network Models
Conditions
Observed outcome
Skill 1
Skill 2
Right
Wrong
Yes
Yes
1
0
Yes
No
r2
1 −r2
No
Yes
r1
1 −r1
No
No
r1r2
1 −r1r2
Figure 8.7 shows this model. Junker and Sijtsma (2001) call this model
NIDA (Noisy Input Deterministic AND). Note that each of the inputs is a
combination of two factors. A person can solve the problem if the person has
Skill 1 OR can ﬁnd a workaround for Skill 1 AND the person has Skill 2 OR
can ﬁnd a workaround for Skill 2.
Fig. 8.7 A conjunctive model with noisy inputs (NIDA)
Reprinted from Yan et al. (2004) with permission from ETS.
To extend this to an arbitrary number of parents, let S represent the set
of skills required for a particular observable outcome variable. Let Sk = 1 if
the learner has mastered the kth skill to the necessary level to solve the task,
and let Sk = 0 otherwise. Let T represented the observed outcome from the
task. Then, the distribution for the outcome variable is:
P(T = Right|S) =

Sk∈S
r1−Sk
k
.
(8.6)
We can put the two diﬀerent types of “noise” together to make a full
noisy-AND distribution (Fig. 8.8). To reduce the number of parameters for
this model, we eliminate the false-positive parameter, π−, i.e., we set it to
0. The false-positive parameter and the skill workaround parameters, rk, are
measuring the same thing anyway. The ﬁnal probability model is then:

8.4 Noisy-OR and Noisy-AND Models
253
Conditions
Observed outcome
Skill 1
Skill 2
Right
Wrong
Yes
Yes
π+
1 −π+
Yes
No
π+r2
1 −π+r2
No
Yes
π+r1
1 −π+r1
No
No
π+r1r2
1 −π+r1r2
Fig. 8.8 A noisy conjunctive model, with noisy inputs and outputs
Reprinted from Yan et al. (2004) with permission from ETS.
The noisy-AND model as developed here assumes that all of the proﬁciency
variables are binary. If the variables have an arbitrary number of ordered
states, we can extend the noisy-and model to a noisy-min model.
Similarly, if only one of the k skills is needed to successfully solve the prob-
lem, we can use a noisy-OR, or disjunctive, model instead. It is a straightfor-
ward translation of the noisy-AND model described above. For instance, the
probability table for a noisy-OR with true positive and false positive param-
eters π+ and π−is
Conditions
Observed outcome
Skill 1
Skill 2
Right
Wrong
Yes
Yes
π+
1 −π+
Yes
No
π+
1 −π+
No
Yes
π+
1 −π+
No
No
π−
1 −π−
In certain application areas, such as failure diagnosis, noisy-OR models are
much more common than noisy-AND models. The noisy-OR model generalizes
to a noisy-MAX model.
Note that the number of parameters in these models are linear in the
number of input variables, not exponential! For a conditional multinomial
distribution with k binary parent variables, we must specify 2k parameters.

254
8 Parameters for Bayesian Network Models
However, for a noisy-AND model, we need only k + 1 parameters, the rk’s
and π+. Easier to elicit, easier to estimate, and often better suited to the
real-world problem at hand.
8.4.1 Separable Inﬂuence
Pearl (1988) develops a model for the noisy-OR case, but does not parameter-
ize it with true positive and false positive probabilities. In his model, qi is the
probability that Si behaves like it is absent even though it is present. Pearl’s
noisy-OR model looks like this for two skills:
Conditions
Task
Skill 1
Skill 2
Right
Wrong
Yes
Yes
1 −q1q2
q1q2
Yes
No
1 −q1
q1
No
Yes
1 −q2
q2
No
No
0
1
This generalizes to k skills as follows:
P(T = Wrong|S) =

k
qSk
k
,
(8.7)
P(T = Right|S) = 1 −

k
qSk
k
again with Sk = 1 for Yes and = 0 for No.
Notice how Eq. 8.7 factors according to the input variables. Thus, the
hypergraph for this model fragment eﬀectively simpliﬁes to Fig. 8.9b instead
of Fig. 8.9a—but only if the student gets the item wrong. If the learner gets
the task right the parents are dependent. This is exactly the competing expla-
nation phenomenon discussed in Chap. 3.
Distributions that can be factored according to the parent variables are
said to have separable inﬂuences (also called “causal independence,” a term we
avoid because of the dissonance between a technical sense of “causal” and its
everyday meaning). The NIDA model (Eq. 8.6) has the separable inﬂuences
property too, for just when the response is correct. Computation algorithms
can exploit this special structure (Li and D’Ambrosio 1994).
8.5 DiBello’s Eﬀective Theta Distributions
For the Biomass project (Chap. 14), Lou DiBello (Almond et al. 2001) devel-
oped a method based on IRT models to produce a class of reduced parameter
distributions for use in the evidence models. The central idea was to map

8.5 DiBello’s Eﬀective Theta Distributions
255
S1
S2
...
Sk
T
S1
S2
...
Sk
T
a
b
Fig. 8.9 Separable inﬂuences. If the conditional probability table for P(T/S1,
. . . , SK) has the separable inﬂuence property, the Graph (a) becomes Graph (b).
Reprinted with permission from ETS.
each conﬁguration of skill variables into an eﬀective theta—a real number
representing the student’s propensity to be able to perform tasks of that
type. The assumption was that even though the test is multidimensional, the
proﬁciency combination associated with scoring well on any given observable
within a task represents a single direction within that multidimensional space.
Once we represent our beliefs about the student’s state as a real number θ
(a distance along that direction), we can press familiar IRT models into ser-
vice to determine the probabilities for the distribution tables which drive the
Bayes nets. This reduces the number of parameters, and furthermore relates
the parameters to concepts like diﬃculty and discrimination that are already
familiar to psychometricians. The approach is akin to that of structured or
located latent class models (Almond et al. 2001; Formann 1985; von Davier
2008).
The DiBello eﬀective theta method proceeds in three steps:
1. For each input variable, designate a real number to serve as an eﬀective
theta for the contribution of that skill (Sect. 8.5.1).
2. The inputs each represent a separate dimension. Use a combination func-
tion to combine them into an eﬀective theta for the task (Sect. 8.5.2).
There are a number of combination functions that can be used, to model
for example compensatory, conjunctive, and inhibitor relationships.
3. Apply a link function to calculate probabilities for the dependent vari-
able from the combined eﬀected theta. DiBello proposed using Samejima’s
graded response model as the link function, i.e., the DiBello–Samejima
model (Sect. 8.5.3).3 For representing relationships among proﬁciency
variables, for example, we recommend a diﬀerent method, based on cut
points of the normal distribution (Sect. 8.5.4).
A key feature of this class of models is the combination function (Step 2).
The choice of this function dictates the type of relationship (e.g., sum for com-
pensatory, min for conjunctive). In typical modeling situations, the experts
3 Other models could be used to parameterize link functions at this point, such as
the generalized partial credit model (Muraki 1992) and the model for nominal
response categories (Bock 1972).

256
8 Parameters for Bayesian Network Models
provide not only which variables are parents of a given variable but also what
the type of relationship is. They also indicate the relative importance of each
variable in the combination. Section 8.6 describes the technique we use to
elicit the information from the experts. Note that the generalized diagnostic
model (von Davier 2008) also uses combination and link functions, although
it does not use the name “combination function.”
8.5.1 Mapping Parent Skills to θ Space
For the moment, we assume that each parent variable lies in its own separate
dimension. Each category for a parent variable represents an interval along this
dimension. It is common in IRT to assume that proﬁciency follows a standard
normal distribution. We will build eﬀective theta values from standard normal
distributions.
Suppose that one of the parent variables in the relationship is Skill 1.
Suppose further that it has three levels: Low, Medium, and High. Positing for
the moment that the skill labels reﬂect an equal-probabilities partitioning of
the distribution, we can break the area under the normal curve into three
segments (Fig. 8.10).
Effective Theta
m0
m1
m2
c1
c2
Fig. 8.10 Midpoints of intervals on the normal distribution
Reprinted from Yan et al. (2004) with permission from ETS.
This is fairly easy to do in practice. Assume that the parent variable has
K-ordered levels, indexed 0, . . . , K −1. Let mk be the eﬀective theta value
associated with Level k. Let pk = (2 ∗k + 1)/2K be the probability up to
and including this level. Then, mk = Φ−1(pk), where Φ(·) is the cumulative
normal distribution function. For two levels, for example, the values are −.67
and +.67. For three levels, the values are −.97, 0, and +.97.
When an observable has just one proﬁciency parent, these are the eﬀective
theta values we use to model conditional probabilities for it. Section 8.5.3

8.5 DiBello’s Eﬀective Theta Distributions
257
shows how this is done. First, we discuss how to combine eﬀective thetas
across proﬁciencies when there are multiple parents.
8.5.2 Combining Input Skills
Each mapping from a parent variable to its eﬀective theta value is done inde-
pendently. That is, each knowledge, skill, or ability represented by a par-
ent variable is assumed to have a separate dimension. The task observation
(or dependent proﬁciency variable) also has its own dimension. As in regres-
sion, the individual parents’ eﬀective thetas are independent variables and the
response variable is a dependent variable. The idea is to make a projection
from the space of the parent dimensions to a point on the child dimension.
This projection is the eﬀective theta, synthesized accordingly across parents,
as it applies to this particular child. The value will then be used in the Same-
jima graded response IRT model to produce probabilities for the response
categories of the child.
The easiest way to do this is to use a combination function, g(·), that pro-
duces a single theta value from a list of others, and the easiest combination
function to understand, and therefore the one we will start with, is the sum.
When eﬀective thetas are summed, having more of one parent skill can com-
pensate for having less of another. For this reason, we call the distribution we
build using this combination rule the compensatory distribution. If θ1, . . . , θK
are the eﬀective thetas for the parent dimensions, then the eﬀective theta for
the child dimension is
˜θ = g(θ1, . . . , θK) =
K

k=1
αk
√
K
θk −β .
(8.8)
This model has one slope parameter, αk, for each parent variable and one
intercept parameter. Following IRT terminology, we sometimes call these the
discrimination and diﬃculty parameters; in fact, β is given a negative sign
so that it will be interpreted as a diﬃculty (higher values of β mean that
the problem is “harder”—the probability of getting it right is lower). The
factor 1/
√
K is a variance stabilization term. If we assume that the variance
of each of the θk’s is 1 (unit normal assumption), then the variance of ˜θ will
be  α2
k/K. In other words, the variance of the eﬀective theta will not grow
with the number of parent variables. Table 8.1 gives a simple example for two
choices of the αs and βs.
Equation 8.8 does not include a subscript to indicate which probability
table we are deﬁning. Technically, there should be another subscript on all the
parameters in that equation to indicate the child variable. We have suppressed
that subscript to simplify the exposition. However, it is worth noting that
we are assigning a separate set of slope and diﬃculty parameters to each
observable outcome variable. Sometimes the values of the slope parameters are
constrained to be the same across multiple observable variables. In particular,

258
8 Parameters for Bayesian Network Models
Table 8.1 Eﬀective thetas for a compensatory combination function
Skill 1
θ1 Skill 2
θ2
Eﬀective thetas
α1 = α2 = β1 = 1 α1 = 1, α2 = 0.5,β1 = 0
High
+0.97 High
+0.97
+0.37
+1.03
High
+0.97 Medium
0.00
−0.32
+0.68
High
+0.97 Low
−0.97
−1.00
+0.34
Medium
0.00 High
+0.97
−0.32
+0.34
Medium
0.00 Medium
0.00
−1.00
0.00
Medium
0.00 Low
−0.97
−1.68
−0.34
Low
−0.97 High
+0.97
−1.00
−0.34
Low
−0.97 Medium
0.00
−1.68
−0.68
Low
−0.97 Low
−0.97
−2.37
−1.03
the Rasch IRT model uses a single slope parameter for all items. A constrained
model is possible, but if the slope parameter is estimated rather than ﬁxed, the
constrained model does not have the global parameter independence property.
The compensatory distribution produces a considerable savings in the
number of parameters we must elicit from the experts or learn from data.
The total number of parameters is K + 1, where K is the number of par-
ent variables. Contrast this with the hyper-Dirichlet model which requires
2K parameters when all variables are binary, and more if any variable is not
binary.
We can change the combination function to model other relationships
among the parent and child variable. The most common is to replace the
sum with a minimization or maximization. Thus, the combination function
˜θ = g(θ1, . . . , θK) =
 K
min
k=1 αkθk

−β ,
(8.9)
produces a conjunctive distribution where all the skills are necessary to solve
the problem. Equation 8.9 posits that the examinee will behave with the
weakest of the skills as the eﬀective theta level. Similarly, the combination
function
˜θ = g(θ1, . . . , θK) =

K
max
k=1 αkθk

−β ,
produces a disjunctive distribution where each parent variable represent an
alternative solution path. The examinee will behave with the strongest of the
skills as the eﬀective level. Table 8.2 gives examples for the conjunctive and
disjunctive combination functions; for simplicity, α1 = α2 = 1 and β = 0.
So far, all of the combination functions have been symmetric (or, more
properly, the asymmetry has been modeled by diﬀerent values for the slope
parameters). One interesting type of asymmetric combination function is the
inhibitor distribution (or more optimistically, the enabler distribution). Here,
we assume that a minimal level of the ﬁrst skill is required as a prerequisite,

8.5 DiBello’s Eﬀective Theta Distributions
259
Table 8.2 Eﬀective thetas for the conjunctive and disjunctive combination functions
Skill 1
θ1 Skill 2
θ2
Eﬀective thetas
Conjunctive Disjunctive
High
+0.97 High
+0.97
+0.97
+0.97
High
+0.97 Medium
0.00
0.00
+0.97
High
+0.97 Low
−0.97
−0.97
+0.97
Medium
0.00 High
+0.97
0.00
+0.97
Medium
0.00 Medium
0.00
0.00
0.00
Medium
0.00 Low
−0.97
0.00
0.00
Low
−0.97 High
+0.97
−0.97
+0.97
Low
−0.97 Medium
0.00
−0.97
0.00
Low
−0.97 Low
−0.97
−0.97
−0.97
but once the examinee have reached the minimal level in the ﬁrst skill, the
second skill takes over.
Example 8.3 (Story Problem). A typical example is a “story problem”
in a math assessment. Usually the skills we wish to assess are related to
mathematics, i.e., the ability to translate the verbal representation of the
problem into the symbolic one, then solve the symbolic problem. However, in
order to have any hope of solving the problem the examinee needs a minimal
competency in the language of the test. This is an inhibitor relationship.
Assume that only two skills are necessary, and that the ﬁrst one is the
inhibitor with the prerequisite that this skill is at least at level r. Also, let
θkm be the eﬀective theta value associated with the mth level of Skill k. Then
we can write the inhibitor relationship as follows:
˜θ = g(θ1, θ2) =

α2θ2 −β
for θ1 ≥θ1r,
α2θ2,0 −β
otherwise.
(8.10)
The numerical example in Table 8.3 shows two skills, both with three levels,
and θ1 acting as an inhibitor: If θ1 < Medium, the combined eﬀective theta
is assigned to the lowest level of θ2. If θ1 ≥Medium, the combined eﬀective
theta is that of θ2. For simplicity, α2 = 1 and β = 0. Contrast this with the
conjunctive combination function in Table 8.2
The inhibitor distribution has an interesting evidentiary interpretation.
Return to the English “story problem” example (Example 8.3). If the exam-
inee’s English proﬁciency is above the threshold, then the story problem can
provide evidence for the mathematical skills it is designed to measure. On the
other hand, if the English proﬁciency is below the threshold, the distribution
produces no evidence for the mathematical skills; the lack of a prerequisite
inhibits any evidence from ﬂowing from the observable to the proﬁciency (see
Exercise 8.10).

260
8 Parameters for Bayesian Network Models
Table 8.3 Eﬀective thetas for inhibitor combination functions
Skill 1 Skill 2
θ2 Eﬀective thetas
High
High
+0.97
+0.97
High
Medium
0.00
0.00
High
Low
−0.97
−0.97
Medium High
+0.97
+0.97
Medium Medium
0.00
0.00
Medium Low
−0.97
−0.97
Low
High
+0.97
−0.97
Low
Medium
0.00
−0.97
Low
Low
−0.97
−0.97
It is possible to make more complex combination functions by mixing the
combination functions given above. For example, a mix of conjunctive and
disjunctive combination functions could model a task that admits two solution
paths, one using Skills 1 and 2 and the other using Skills 3 and 4. In fact,
mixing the inhibitor eﬀect with compensatory and conjunctive combination
functions is quite common.
Distributions can also be chained together by introducing dummy latent
variables as stand-ins for combinations of skills. In the example above, new
variables would be deﬁned for having the conjunction of Skills 1 and 2, say
Skill 1&2, and similarly deﬁning Skill 3&4. Then a disjunctive combination
would follow, of having Skill 1&2 or Skill 3&4. Even though there are more
variables, computation is simpliﬁed because the conditional probability matri-
ces are smaller. On the other hand, there is a loss of precision with this
approach. An unrestricted conditional probability matrix can have diﬀerent
probabilities for each possible combination of parents, whereas the stage-wise
combinations collapse categories. Whether this is a good trade-oﬀdepends on
whether the collapsed parent combinations are substantively diﬀerent. There
also may be some other issues noted in Sect. 8.5.4.
8.5.3 Samejima’s Graded Response Model
Applying the combination function to the eﬀective thetas for each of the
parent variable produces one eﬀective theta for each row of the conditional
probability table (CPT). Completing the CPT requires turning that eﬀective
theta into a set of conditional probabilities, one for each possible state of
the child variable. This is the role of the link function. This section explores
a class of link functions based on the logistic function (commonly used in
IRT models). Now that we have projected the examinee’s skill state (i.e., on
the parent variables), we will calculate probabilities with, the graded response
model of Samejima (1969). Section 8.5.4 describes an alternative link function
based on the normal distribution.

8.5 DiBello’s Eﬀective Theta Distributions
261
The most common IRT models posit a single continuous skill variable, θ,
and binary response variables. The relationship between the observation and
the skill variable is a logistic regression. Various parameterizations are found
in the literature. The simplest is the one parameter logistic (1PL) or Rasch
model (same function, diﬀerent conceptual underpinnings):
P(X = 1|θ) = logit−1(θ −d) =
exp(D(θ −d))
1 + exp(D(θ −d)),
(8.11)
where b is an item diﬃculty parameter and X is a binary observed outcome,
1 for correct and 0 for incorrect. The constant D = 1.7 is sometimes used to
scale the logistic function so that it looks like a normal CDF (Sect. 8.5.4).
The 2PL additionally has a slope or discrimination parameter for each item,
so logit−1 Da(θ −d).
Samejima’s graded response model extends the this model to an observ-
able X that can take one of the ordered values x0 ≺· · · ≺xM−1. It is usually
developed from the 2PL, but we will build from the 1PL because the 2PL’s dis-
crimination parameter is redundant with the slope parameters in the eﬀective
theta combination function. For m = 1, . . . , M −1, we ﬁrst deﬁne cumulative
conditional probabilities for the response categories:
P(X ≥xm|θ) = P ∗
m(θ) = logit−1 D(θ −dm),
(8.12)
where dm is a category diﬃculty parameter. There is always one fewer cumula-
tive probability curve than the number of possible outcomes, as P(X ≥x0) =
1. The category response probabilities P(X = xm|θ) can be calculated from
the diﬀerences of the cumulative probabilities given by Equation 8.12, with
P(X = x0) = 1 −P(X ≥x1). That is,
P(X = xm|θ) = P ∗
m(θ) −P ∗
m+1(θ).
(8.13)
Figure 8.11 illustrates response category probabilities for a three-category
task, d1 = −1, and d2 = +1. For very low values of θ, the lowest level of
response is most likely. As θ increases, probabilities increase for higher-valued
responses in an orderly manner. Given the item parameters, a single value of
θ speciﬁes the full conditional distribution for all possible responses.
All that remains is to specify the values for dm. As we have a diﬃculty
parameter, β, additional constraints are needed for the ds in the eﬀective theta
combination step. One way to do this is to require M−1
m=1 dm = 0. For three
categories, this means that d1 = −d2. Or we can set d1 = −1 and dM−1 = 1,
or d1 = 0 and dM−1 = 1 (as we do in Chap. 15). Furthermore, as the categories
of the output variable are ordered with respect to diﬃculty, the dm’s should
be increasing. Other than that we can specify any values we like for these
parameters. When there are more than three categories, d2 < . . . < dm−1
can be estimated, or set at equal spacing, or, when M is large, determined
as a function of some smaller number of parameters (e.g., Andrich 1985).

262
8 Parameters for Bayesian Network Models
−3
−2
−1
0
1
2
3
0.0
0.2
0.4
0.6
0.8
1.0
Effective Theta
Probability
●
●
●
●
Pr(X = x0)
Pr(X = x1)
Pr(X = x2)
Fig. 8.11 Probabilities for Graded Response model
Reprinted from Almond et al. (2006a) with permission from ETS.
Almond et al. (2001) took them to be equally spaced in the interval [−1, 1].
The intercept β controls the overall diﬃculty. The d parameters control how
spread out the probabilities for the various child variable values are, with
the slope parameter(s) αk determining how much varying each of the parents
changes the probabilities.
Figure 8.11 illustrates the idea for a simple conditional probability table
for a child variable with a single parent, both of which have three levels. The
combination function is α1θ1 −β (as there is only one parent, it does not
matter if it is compensatory or conjunctive), and for simplicity, set α1 = 1
and β = 0, also let d0 = −1 which forces d1 = 1. Figure 8.11 shows the
three graded response curves, and the points are the values of those curves
evaluated at the eﬀective thetas. Putting this in tabular form yields the table
shown in Fig. 8.4.
Example 8.4. Compensatory Graded Response Distribution. As a more com-
plete example, consider the CPT for an observable variable Credit which rep-
resents three possible scoring levels for a short constructed response task Full,
Partial, or No credit. This task taps two measured skills which are labeled
Skill 1 and Skill 2 each of which have three possible levels: High, Medium, and
Low. The domain experts have determined that the relationship between these
skills is compensatory and that they are equally important, and that the task
should be of average diﬃculty. The combination function is therefore the one
given in Eq. 8.8, with α1 = α2 = 1 and β = 0. The eﬀective thetas are given in
Table 8.1. The graded response link functions are given in Eqs. 8.12 and 8.13.
Table 8.5 shows the ﬁnal conditional probability table.
Even though we may use equally-spaced values of dm for elicitation from
the experts, we may not want to preserve the equal spacing when we have a lot

8.5 DiBello’s Eﬀective Theta Distributions
263
Table 8.4 Conditional probability table for simple graded response model
Skill
Eﬀective theta Full Partial None
High
+0.967
0.656
0.278
0.066
Medium
0.000
0.269
0.462
0.269
Low
−0.967
0.066
0.278
0.656
Table 8.5 Compensatory combination function and graded response link function
Skill 1 Skill 2 Eﬀective thetas Full Partial
No
High
High
+1.03
0.678
0.262
0.060
High
Medium
+0.68
0.541
0.356
0.103
High
Low
+0.34
0.397
0.433
0.171
Medium High
+0.34
0.397
0.433
0.171
Medium Medium
0.00
0.269
0.462
0.269
Medium Low
−0.34
0.171
0.433
0.397
Low
High
−0.34
0.171
0.433
0.397
Low
Medium
−0.68
0.103
0.356
0.541
Low
Low
−1.03
0.060
0.262
0.678
of data. Instead, we could estimate distinct dm’s for each category, free to vary
independently as long as they remain in increasing order. In this approach,
the model will ﬁt the marginal proportions in each category exactly.
Following the advice of Kadane (1980), we should try to elicit priors in
terms of quantities the experts are used to observing. Here, the best parame-
ters to elicit would be the marginal proportions of observed outcome (Kadane
et al. 1980 and Chaloner and Duncan 1983 describe applications of this idea for
regression models and binomial proportion models, respectively). In a Bayes
net, marginal proportions depend on both the conditional probability distri-
butions described above and population distribution for the parent variables
(usually proﬁciency variables). If the population distribution for the parent
variables have already been elicited, it should be possible to pick bj’s and
dj,m’s to match the marginal proportions.
8.5.4 Normal Link Function
The IRT-based link functions shown in the preceding section work fairly well
when the child variable is an observable outcome and the parent variable is
proﬁciency variable. This makes sense as IRT models were designed to model
the relationship between a latent ability and an item outcome. When the
child variable is a proﬁciency variable, a diﬀerent link function—one based
on the normal distribution—works better (Almond 2010a). This link function

264
8 Parameters for Bayesian Network Models
is motivated by many users’ familiarity with basic regression with normally
distributed residuals.
Assume again that the population distribution on the eﬀective theta space
for the child variable is normally distributed, and that (before taking into
account information from the parent variables) it is equally divided between
the categories. We can establish cut points, ci, between the intervals. This
is the solid curve in Fig. 8.12). (Actually, equal divisions are not necessary,
Almond 2010a extends the method to use any desired marginal distribution
for the child variable. This section will stick to the equal divisions, as they
are easier to explain.)
Effective Theta
c1
c2
Fig. 8.12 Output translation method. The solid curve represents standard nor-
mal reference curve for child variable; cut points are established relative to equal
probability intervals on this curve. The dashed curve represents a displaced curve
after taking parent variables into account. Probabilities for child variables are areas
between cut points under the dashed curve. Reprinted with permission from ETS.
Next, we consider how the parent variables would shift our beliefs about
the child. We can think about this as a kind of a regression where the eﬀective
theta for the child variable is predicted from the eﬀective theta for the parent
variables, so that:
˜θ =

k∈parents
αk
√
K
θk −β,
(8.14)
where the θks on the right are the eﬀective thetas of the parent variables and ˜θ
on the left is the combined eﬀective theta of the child. As in the compensatory
combination function, the factor of 1/
√
K stabilizes variance of the linear
predictor the compensatory distribution. Equation 8.14 gives the mean of the
dashed curve in Fig. 8.12. Changing the value of β shifts this curve to the
right or the left, and changing the value of the αk’s changes the amount that
the curve shifts with the change in the parent variables.
A given individual will not have an eﬀective theta precisely at the mean
of this curve, but rather somewhere around it. Let σ be the residual standard

8.5 DiBello’s Eﬀective Theta Distributions
265
deviation. As the eﬀective thetas for the parent variables are scaled to have
variance one, Var(˜θ) = σ2 + K
k=1 α2
k/K. When σ2 is small compared to the
αk’s, then the distributions will be tightly clustered around the value predicted
by Eq. 8.14; in other words the parents will have more predictive power. When
σ2 is large, then there will be more uncertainty about the state of the child
variable given the parents.
Continuing the analogy to multiple regression, we can deﬁne
R2 =
K
k=1 α2
k/K
σ2 + K
k=1 α2
k/K
.
We need to be careful in interpreting this R2 value though. In the typical
eﬀective-theta application, it is describing the squared correlation between
latent variables. Latent variable correlations are considerably higher than the
observed variable correlations that experts are used to seeing. An observed
variable correlation of 0.5 might correspond to an latent correlation of 0.8 or
higher (depending on the reliability of the measures of the latent variables.)
The ﬁnal probabilities come from the area between the cut points in the
prediction (dashed) curve. The probability for the lowest state is the area
below c1. The probability for the second state is the area between c1 and c2,
and so on up to the highest state which is the area above the highest cut
point. Thus, if the child variable is X, then:
P (X ≥xm| pa(X)) = Φ

cm −˜θ
σ

(8.15)
where m > 1. Naturally, P(X ≥x1) = 1, so the individual probabilities can
be calculated from the diﬀerences:
P (X = xm| pa(X)) = Φ

cm−1 −˜θ
σ

−Φ

cm −˜θ
σ

(8.16)
The value of ˜θ is given by Eq. 8.14.
There is a strong similarity between Eqs. 8.12 and 8.15. First, the shape
of the logistic curve and the normal CDF is very similar; the factor D = 1.7
commonly used in IRT models makes the two curves almost the same shape.
In particular, the normal link function described here is really the probit
function, and the graded response link function described in the previous
section is the logistic link function.
There are four diﬀerences between the two link functions. The ﬁrst is the
metaphor that motivates the curve: Eq. 8.12 is based on IRT while Eq. 8.15 is
based on factor analysis and regression. The second is the role of the parameter
σ, the residual standard deviation. In the normal model, this eﬀectively scales
the αk’s and β, so that the equivalent values using the graded respond link

266
8 Parameters for Bayesian Network Models
function would be αk/σ and βk/β0. The residual standard deviation, σ, also
scales the constants cm so that dm = cm/σ. The third diﬀerence is that with
the normal link function, the cm’s are ﬁxed and σ must be elicited from experts
or learned from data. With the graded-response link function, the dm’s must
be elicited or learned. Fourth, although the normal and logistic distributions
have similar shapes, the normal is more dispersed. If comparable parameters
are desired as in some IRT applications, one can rescale the logistic parameters
using the approximation Ψ(1.7z) ≈Φ(z).
Example 8.5. Correlation between two proﬁciency variables. Assume that
the subject matter expert has suggested modeling Skill 1 as a parent of Skill 2,
and that the correlation between the two skills is 0.8 (remember, this is a
correlation between the two latent variables, so it will be higher than an
observed score correlations). Setting α1 = 0.8 and σ = 0.6 preserves this
correlation. Further assume that the expert asserts that Skill 2 is slightly
more common in the population (this is equivalent to having lower diﬃculty,
except that “diﬃculty” is not really appropriate for a proﬁciency variable).
This can be modeled by setting β = −0.5. As there are three categories, so we
need to divide the eﬀective theta range into three equal categories. This gives
the values c1 = Φ−1(1/3) = −0.43 and c2 = Φ−1(2/3) = +0.43. Table 8.6
gives the conditional probability table.
Table 8.6 Conditional probability table with normal link function, correlation =
0.8
Skill 2
Skill 1
High Medium
Low
High
0.920 0.078 0.002
Medium 0.546 0.394 0.060
Low
0.120 0.483 0.397
Example 8.6. Conditional Probability Table built from Path Analysis Results.
Assume that the subject matter expert has recommended modeling Skill 3 as
the child of Skill 1 and Skill 2, and furthermore, the expert has the results
of a path analysis, which shows path coeﬃcients of 0.58 and 0.47 for Skill 1
and Skill 2 to Skill 3, an a residual standard deviation of 0.67. To model this,
set α1 = 0.58, α2 = 0.47 and σ = 0.67. Path analysis centers all variables, so
it does not provide us with a source of information for β. Assume that the
expert believes that Skill 3 will be less common in the population than Skill 1
and Skill 2. On this basis set β = 0.75. As path analysis corresponds to the
compensatory combination function, Eq. 8.8 is used to calculate the eﬀective
thetas. The ﬁnal conditional probability table is given in Table 8.7.
Even though the number of parameters in conditional probability tables
created in this way grows linearly with the number of parents, the size of

8.6 Eliciting Parameters and Laws
267
Table 8.7 Conditional probability table for path coeﬃcients 0.58 and 0.47
Skill 3
Skill1
Skill2
Eﬀective theta High Medium Low
High
High
−0.032
0.245 0.479 0.276
Medium High
−0.428
0.100 0.401 0.499
Low
High
−0.825
0.030 0.248 0.722
High
Medium
−0.353
0.121 0.425 0.454
Medium Medium
−0.750
0.039 0.278 0.683
Low
Medium
−1.147
0.009 0.133 0.857
High
Low
−0.675
0.049 0.308 0.642
Medium Low
−1.072
0.012 0.157 0.831
Low
Low
−1.468
0.002 0.058 0.939
the conditional probability table grows exponentially as the number of par-
ents increases. Therefore, models in which every node has a limited number
of parent variables, will be generally more eﬃcient. There are two ways to
accomplish this: First, one can try to identify conditional independencies to
simplify the model. Almond (2010a) notes that path analyses and factor anal-
yses often produce correlation matrixes, and that zeroes in the inverse of the
covariance matrix (Dempster 1972; Whittaker 1990) correspond to conditional
independencies. Second, often a latent variable can be added to the Bayesian
network to capture additional dependencies in a way that makes can both add
insight and make computation more eﬃcient.
It is possible to use the normal link function with the conjunctive, dis-
junctive and inhibitor combination functions, too. This would allow modelers
to combine compensatory, conjunctive, disjunctive, or inhibitor combination
functions with logistic or probit link functions. However, because the linear
regression paradigm is so strongly associated with the normal link function,
the compensatory combination function is almost always used with the normal
link function.
8.6 Eliciting Parameters and Laws
Let us shift our focus back to the big picture of where the Bayes net models
we use in this book come from. Ideally, it is a combination of expert opinion
and data. As all the variables in the proﬁciency model are latent, we will
always need to lean on subject matter experts to deﬁne the variables and the
relationships between the latent and observable variables. On the other hand,
when we get data back from students taking the assessment, we want to revise
our models to be more consistent with the data. The following chapters will
describe techniques for doing just that.

268
8 Parameters for Bayesian Network Models
The evidence-centered design (ECD) process (described brieﬂy in Chap. 2
and in more detail in Part III) is a method for eliciting (and documenting) a
model of the assessment from the subject matter experts. Although many of
the steps of ECD are aimed at the equally important task of designing tasks
to provide the evidence to update the model, all of the core steps required
to build the Bayes nets for the proﬁciency and evidence models are part of
the ECD process. In particular, the assessment designer (working with the
subject matter expert) needs to perform the following steps:
1. Deﬁne all of the variables in the proﬁciency and evidence models. This step
can be tricky because of the need to balance the purposes and constraints
of the assessment and competing views of the domain held by various
groups of experts.
2. Deﬁne the relationships among the variables. The most important part of
this task is specifying which proﬁciency variables each observed outcome
variable depends on, i.e., deﬁning the Q-Matrix for the assessment. These
ﬁrst two steps may be 90 % of the problem of specifying the mathematical
structure of the assessment. Relationships among proﬁciency variables
can be learned from correlation matrixes built as part of factor analyses
or other structural equation models (Almond 2010a). Once the model
structure is set, the remaining steps involve establishing the conditional
probability tables, which are much easier to learn from data.
3. For each variable, choose a distribution type that deﬁnes the relation-
ship between it and its parents. This chapter provides a number of exam-
ples, and there are many others. The challenge is translating the expert’s
description of the relationship among variables (e.g., compensatory or
conjunctive) to a mathematical model.
4. For each distribution, establish the values of any parameters. In practice,
an operational assessment will usually use the mean of the law established
in Step 5. In other words, we will estimate the posterior distributions for
the parameters that determine conditional probability matrices, then use
the means of their posteriors to provide a working approximation of the
conditional probabilities in the operational assessment.
5. For each distribution establish a law (and appropriate hyperparameters)
that describe our uncertainty about the parameters. By this point we are
dealing with a level of abstraction in which the experts are likely to feel
uncomfortable. However, this step is necessary if the parameters are to be
updated from experiential data as described in the later chapters.4
4 As the number of parameters increases, more priors are required. If the same
strength of priors is imposed parameter by parameter, the eﬀective inﬂuence of
the priors is greater on the model as a whole. We think this is generally reasonable
for estimation; for a model that is more complicated and probably less stable, it
is conservative to require more evidence from the data to move parameter values

8.6 Eliciting Parameters and Laws
269
As we progress through the steps, the level of abstraction goes up and
the comfort level of the subject matter experts goes down. At the ﬁrst step,
the experts are very comfortable identifying skills which are important in
a domain. Typically the role of the psychometrician is to keep the experts
focused on what can be measured within the resource constraints (particularly
time) for the assessment.
Somewhere around Step 3, the tide turns. The experts are comfortable
describing the relationships of the variables in qualitative terms and the psy-
chometrician must help them translate this into quantitative terms. The fol-
lowing sections describe some of the techniques we have used for the distribu-
tions described in previous sections.
8.6.1 Eliciting Conditional Multinomial and Noisy-AND
The key to eliciting the parameters for the conditional multinomial distribu-
tion and hyper-Dirichlet law is to use the observational equivalence property
that comes out of the conjugacy. The examples in this section illustrate a
practical way to do this. The ﬁrst step in each case is to ask the subject mat-
ter expert about the skills and the anticipated performance of 100 ﬁctitious
students. This will give the mean of the law. The second step is to ask the
expert about how many student’s worth of observation their opinion is worth.
This will give the scale factor.
Example 8.7 (Unconditional Multinomial Elicitation). Suppose that
the proﬁciency model the experts provide has a node for OverallProﬁciency
which has no parents. (This is fairly typical). Suppose further that we are
allowing four levels for this variable: minimal, basic, proficient, and
advanced. We ask the experts “Of 100 typical students, among those for
whom the assessment is to be used, how many will fall in each category?”
Suppose that the expert tells us that about 1/6 will be in the lowest category,
1/3 will be in each of the two middle categories, and 1/6 will be advanced. If
the expert is fairly conﬁdent of those numbers, we might say this is worth a
pretest sample size of 100, so the Dirichlet hyperparameters for this distribu-
tion would be: (16.67, 33.33, 33.33, 16.67). If the expert was not so conﬁdent,
we might assign it an eﬀective sample size of 10. This would give the following
hyperparameters: (1.667, 3.333, 3.333, 1.667). The mean has not changed, but
the variance is considerably larger. It could change faster as data arrive and
we update the parameters accordingly by the methods in discussed in the next
chapter.
A prior distribution with an eﬀective sample size of 100, will have more
information than 100 pretest subjects. That is because the variable we are
away from the inferential structure suggested by experts and theory. Of course
model checking needs to be done along the way, as per Chap. 10.

270
8 Parameters for Bayesian Network Models
building the distribution for is latent, and hence not measured perfectly in
real data. (In eﬀect, we have to work with posterior distributions for each of
the students.) Depending on the design of the assessment, in particular, on the
Q-Matrix, we may not receive any information at all about certain parameters,
as when there are multiple parent variables and certain combinations of their
states are rarely observed.
The procedure for elicitation in the conditional multinomial case is similar.
Example 8.8 (Conditional Multinomial Elicitation). Suppose that the
variable Skill 1 is modeled as conditional on Overall Proﬁciency. Suppose that
both variables have four levels (as in Example 8.7). Then, we would ask the
experts in turn, “If we had 100 students who were at the advanced level in
OverallProﬁciency, how many of them would be advanced in Skill 1? How
many would be proficient, minimal, and basic?” Same set of questions
repeated, for when the parent values is proficient, then minimal and basic
in turn. Again, we would then ask the expert the number of students their
prior opinion is worth and use that to scale the Dirichlet parameters for each
row of the table.
Suppose further, that Skill 2 had both OverallProﬁciency and Skill 1 as
parents, again with four levels each. We would now have 16 conditional dis-
tributions for the child variable to elicit from the expert, one for each combi-
nation of possible values for OverallProﬁciency and Skill 1.
The amount of work required to support this unconstrained distribution
goes up exponentially with the number of parent variables. Consequently,
eliciting the parameters is a lot of work. Even though the distribution is
extremely ﬂexible, and the “100 students” method is reasonably compact, once
there are very many variables or they have many possible values, experts may
not feel that their knowledge is detailed enough to be comfortable specifying
all of those parameters. For this reason, the experts seem to prefer the other
distribution types in the ways discussed below.
One ﬁnal note, even though the probability of a cell may be small, one
should be careful about setting any Dirichlet hyperparameters to zero. A zero
hyperparameter means that this outcome could never occur. Any data we see
in this cell of the table is an error. This makes sense if the combination is
logically impossible. But otherwise we should put a small positive value in
the cell instead of a zero.
Eliciting parameters for the Noisy-AND family of distributions is similar.
The true-positive parameter, π+, is a binomial proportion. The natural conju-
gate prior is a beta law, and we can use the same hypothetical data questions
to elicit the parameters. We would ask the subject matter expert, “Of 100
students who had all of the necessary skills, how many would get the prob-
lem right?” We would then ask how many observations the expert’s opinion
should be worth and use this to set the hyperparameters of the beta law. The
elicitation method for the false-positive parameter, π−, is similar, with the

8.6 Eliciting Parameters and Laws
271
ﬁrst question now being, “Of 100 students who lacked some of the necessary
skills, how many would get the problem right anyway?”
However, if we use the NIDA distribution, we need a slightly diﬀerent
method for the skill workaround parameters, rk. Each rk represents the prob-
ability that a student who has all of the required skills except Skill k will
be able to solve the problem correctly. This is what we must elicit from the
expert. There is one rk for each parent skill, but we can make the process sim-
pler by asking for one value that we’d use for all of them, unless the expert
has substantive reasons for making some of them diﬀerent. Of course when
data arrives, we can use it to update all of these parameters.
Things get more complicated with the full noisy-AND distribution, with
both the skill workaround and true-positive parameter. In this model, rk rep-
resents the probability that a student who lacks only Skill k will behave like a
student who has all of the required skills. The beta law is still the appropriate
conjugate prior, but putting the question just like this is rather abstract. It is
probably better to ask the expert about the probability that a student who
had all of the required skills except Skill k would get the question right. This
is π+rk. If we have previously elicited π+ we can work backward to get a prior
mean for rk.
Another possible approach is put a relatively weak prior (one with a large
variance) on the rk parameters and try to learn them from data. (This is more
or less the approach taken in the Fusion model Hartz 2002; Roussos, DiBello,
et al. 2007a.) However, we cannot use a completely noninformative prior here,
as there is a potential problem with identiﬁability. Suppose that all of our
skills are binary, with 1 representing mastery and 0 representing non-mastery.
There is another “solution” to our model with 0 representing mastery, so we
need a way to distinguish between the two.
Technically speaking, identiﬁability is not a Bayesian problem. As long as
the prior law is a proper probability distribution, the posterior law will be a
proper probability distribution as well. But in this case, we want to use our
prior law to gently guide us towards one solution (the one with 1 representing
mastery) as opposed to the other.
Even though identiﬁability is not a technical issue, we still want to pay
attention to it. If a parameter is not identiﬁable in our model, then its prior
and posterior law will be virtually identical in some aspect. This means
that the posterior distribution is very sensitive to changes in the prior. This
may or may not be a problem depending on the purpose of the assessment.
Nonidentiﬁability and approximate nonidentiﬁability often result in tech-
nical problems (e.g., slow convergence of iterative algorithms) even in the
Bayesian framework, so it is worth spending time trying to build models that
avoid it.

272
8 Parameters for Bayesian Network Models
8.6.2 Priors for DiBello’s Eﬀective Theta Distributions
Unlike the other two types of distribution, the eﬀective theta distributions
have no natural conjugate prior for the parameters. Therefore, we use a normal
law to describe the distribution of the parameters. At this level of abstraction,
we have no particular reason to believe that the parameters are truly normal.
However, the normal law is represented by exactly two moments, the mean and
the variance, and thus it serves as a workable form for “approximating” the
truth. Previous experience with IRT models suggest that there may be some
posterior dependence among the parameters, so we will use a multivariate
normal law. However, for elicitation, we will use a diagonal covariance matrix
(i.e., assume local parameter independence of the parameters). Later, the
cross-parameter relationships that are hard to think about can be estimated
from data.
This section discusses eliciting priors for the DiBello–Samejima model from
experts who are familiar with the IRT models they are based on. The next
section proposes an alternative that is based on experts’ perceptions of items
in terms that are more familiar to them.
One problem with the choice of a normal prior is that the normal law
has inﬁnite support. That is, any possible value of the parameters can occur.
However, not all possible choices of parameter are legal in our problem. The
discrimination parameters, αk should be positive. And the level diﬀerence
parameters, dm for the Samejima distribution need to be in increasing order
and must sum to zero. We need to apply some transformations to ensure we
end up with appropriate priors.
For the αk, we again follow a common practice of IRT software (such as
BILOG, Zimowski et al. 2003) and model log αk as normal. However, we still
elicit the mean μθ and “standard deviation” σθ of the prior law on the natural
scale. We can take log (μθ) as the mean of our log-normal prior. The variance
of the corresponding log–normal is

exp

σ2
θ −1

exp

2μθ + σ2
θ

.
We can elicit the values for diﬃculty parameters, β, in the natural scale, as
long as our experts are used to IRT discrimination and diﬃculty parameters.
However, the level diﬃculty parameters for the Samejima graded response
model, dm, need to be transformed on this scale. When we use the constraint
set M−1
m=1 dm = 0, there are only M −2 free parameters. We are better
oﬀworking with the diﬃculty increment parameters: d′
m = dm −dm−1 for
m = 2, . . . , M −1. These are diﬀerences between the eﬀective diﬃculties for
the Samejima curves. As the center of the distribution is given by the diﬃculty
parameter, β, we arrange the dm parameters symmetrically around zero by
setting d1 = −M−1
m=2 d′
m/2.
Later, when we estimate the d′
ms, we will need to make sure they are
strictly positive. We will do this by giving these parameters a truncated normal
law. This is a normal distribution with the probability of any value below zero
set to zero. Dividing by the cumulative normal probability of being above zero
(given our current mean and variance) normalizes the truncated normal law.

8.6 Eliciting Parameters and Laws
273
The regression distribution does not have this problem. We treat the cm
parameters as ﬁxed. The 1/σ parameter is eﬀectively another discrimination
parameter, so we use the lognormal distribution for this parameter as well.
8.6.3 Linguistic Priors
Eliciting prior laws for the DiBello–Samejima Eﬀective Theta distributions
requires a fairly intimate knowledge of the IRT models that motivate them.
An expert may be comfortable in stating that a given problem is easier or
harder than average, but usually will be uncomfortable in assigning an exact
value to its diﬃculty parameter. The preceding section discussed eliciting
priors from experts who are familiar with IRT, so they bring the required
knowledge with them. This section discusses eliciting priors in a structure
that has the IRT knowledge built into it, so the expert only needs to think
about familiar properties of items.
We use linguistic priors to accomplish this. The psychometrician creates
three possible priors and labels them with appropriate terms. For example,
for diﬃculty we have three normal priors, one centered at −1, one centered at
0, and on centered at 1. They are labeled “easier,” “average,” and “harder,”
respectively. A fourth choice label-led “unknown” also has a mean of zero, but
a wider variance. The expert picks one of the labels and we use the correspond-
ing prior. The expert or psychometrician can also enter the hyperparameters
manually to produce for example a prior that represents beliefs about a task
halfway between “easier” and “average” diﬃculty, or a prior that reﬂects the
belief that a particular item will be “really hard”—say with a mean of 2.
Ideally, these linguistic priors would be calibrated by observing how the
expert’s predictions bore out over a long period of time. If that information
is not available, one should make sure that the prior has a suﬃciently large
variance that the data will overwhelm the expert’s opinion when they don’t
agree.
Another approach is to use hierarchical modeling, especially for the links,
the task speciﬁc versions of the evidence models. As the task model can be used
to control features of the task that inﬂuence diﬃculty and evidentiary focus
(discrimination), we can think of the task-speciﬁc link parameters as coming
from a hierarchical model (Mislevy et al. 1998). Test data from similar tasks
could tell us quite a bit about new types of tasks.
Although we have not done this in the context of Bayes net models, there
has been some work in this area in the context of IRT models. The techniques
of Johnson and Sinharay (2003) and Glas and van der Linden (2001) should
be straightforward to extend to this case. Note that these hierarchical models
tie the parameters of diﬀerent distributions together through shared hyper-
parameters and hence they do not satisfy the global parameter independence
property.

274
8 Parameters for Bayesian Network Models
Currently, the experience with the models presented in this chapter, espe-
cially Sect. 8.5, is limited. Models that seem reasonable for the cognitive phe-
nomenon they are trying to address may prove diﬃcult to calibrate to real
data. Practical experience will bring strategies and structures for eﬃcient use.
Further, the models described in this chapter represent only a few of the
ways we might parameterize the relationship among several variables. Draw-
ing on the rich tradition of models for psychological testing would produce
many more possible parameterizations for probability tables, and for data
other than discrete response values, such as response time or counts of var-
ious kinds of actions. In particular, since Bayes net proﬁciency variables are
essentially latent classes, many of the models used in latent class analysis
could be adapted to this framework. There is ample opportunity for research
in this ﬁeld.
Before going on to talk about how we will use these complicated models,
it is worth injecting a note of caution. As we are by Step 5 well beyond the
comfort level of our experts; we are building castles in latent variable space.
We need to be careful how much faith we are putting in our complicated
models for the hyperparameters of laws for parameters of distributions of
latent variables. To a large extent, we are leaning heavily on the experts.
Remember that any model we build will not be “true,” it will only be an
approximation of the patterns in the real world. A given model may capture
some aspects of real life well and others not. If it models the aspects of real life
that are important suﬃciently well, then it will be useful. A useful model that
is easy to compute is more important than one which is true but intractible.
Naturally, we would like to validate our model with data. If the model is
good, then it will be able to accurately predict future data. The next chapters
describe mechanisms for ﬁtting our model to data and critiquing it against
data. This model checking is an important part of validating our model.
In an ECD assessment, the ultimate utility of an assessment is measured
by how well claims made on the authority of the assessment are borne out
in the real world. The strongest kind of evidence for determining the validity
of such an assessment means ﬁnding an independent mechanism for measur-
ing the claims, and gauging the extent to which the assessment reﬂects the
patterns that more comprehensive “gold standard” evidence conveys. This
can be challenging in practice, but it is a challenge that all assessments must
ultimately meet.
Exercises
8.1 (Noisy-AND Model). Suppose that in a given population Skill 1 and
Skill 2 are both present roughly half the students, and that furthermore, the
two skills are acquired independently, so that the presence of Skill 1 and Skill 2
are independent in the population. Now suppose that solving a particular

8.6 Eliciting Parameters and Laws
275
problem requires both Skill 1 and Skill 2. Suppose further that students who
lack Skill 1 have a 10 % chance of stumbling on a way to solve the problem
without applying the skill and students who lack Skill 2 have a 20 % chance
of ﬁnding a workaround for the missing skill. Finally, assume that 5 % of
all students who solve the problem make careless mistakes which render an
otherwise correct solution incorrect.
Build a noisy-AND model for this problem. What is the joint posterior
probability of the two skills for a student from this population who got the
problem right? What is the joint posterior probability of the two skills for a
student who got the problem wrong? Under what circumstances are the two
skills independent a posteriori?
8.2 (Noisy-OR Model). Assume that Skill 1 and Skill 2 are distributed in
the population as in the previous exercise. Now suppose we have a problem
which admits two possible solution paths, one of which requires just Skill 1
and the other of which requires just Skill 2. Suppose that 10 % of students
who have Skill 1 make some kind of mistake when applying this strategy and
get the problem wrong, and 20 % of students who have Skill 2 and apply it
get the problem wrong.
Build a Pearl’s noisy-OR model for this problem (Sect. 8.4.1). What must
be assumed about the behavior of a student who has both skills? What is the
joint posterior probability of the two skills for a student who got the problem
right? What is the joint posterior probability of the two skills for a student
who got the problem wrong? Under what circumstances are the two skills
independent a posteriori?
8.3 (Hyper-Dirichlet Update). Suppose that we have a task with an
observable outcome variable which can take on values Right and Wrong, and
that this outcome requires Skill 1 and Skill 2 in some combination. Let the
prior beliefs about the parameters of the conditional probability table linking
Skill 1 and Skill 2 to the outcome variable be given by the hyper-Dirichlet
distribution shown in the following table:
Skill 1 Skill 2 Right Wrong
High
High
9.75
0.25
Medium High
8.75
1.25
Low
High
5.00
5.00
High
Medium
8.75
1.25
Medium Medium
5.00
5.00
Low
Medium
1.25
8.75
High
Low
5.00
5.00
Medium Low
1.25
8.75
Low
Low
0.25
9.75

276
8 Parameters for Bayesian Network Models
Suppose further that we have a sample of 1000 students who have been
rated by experts on Skill 1 and Skill 2. We administer the task to these
students and obtain the results given in the table below:
Skill 1 Skill 2 Right Wrong
High
High
293
3
Medium High
112
16
Low
High
0
1
High
Medium
14
1
Medium Medium
92
55
Low
Medium
4
5
High
Low
5
1
Medium Low
62
156
Low
Low
8
172
Calculate the posterior distribution for these parameters. How much have
we learned about the case in which both skills are High? How much have we
learned about the case where Skill 1 is High and Skill 2 is Low?
8.4 (DiBello Models). Consider a skill, Skill 1, with three levels: High,
Medium, and Low. Consider a task which uses this skill and is scored with an
evidence rule which yields one of three values: No Credit, Partial Credit, or
Full Credit. Build the conditional probability table for a DiBello–Samejima
compensatory distribution for the relationship between the proﬁciency vari-
able and the observable variable with the following parameters: discrimination
of 1.0, diﬃculty of 0.0 and diﬃculty increment of 1.0.
What happens to the probability table when we increase or decrease the
diﬃculty? When we increase or decrease the discrimination? When we increase
or decrease the diﬃculty increment? What would happen if the type of the
relationship was changed from compensatory to conjunctive or disjunctive?
8.5 (Diﬃculty and Weight of Evidence). Consider a skill, Skill 1, with
three levels: High, Medium, and Low, which are evenly distributed in the popu-
lation of interest. Consider a task which uses this skill and is scored Right or
Wrong. Build the conditional probability table for this model with a compen-
satory distribution with a discrimination of 1.0 and a diﬃculty of 0.0. What is
the expected weight of evidence for the distinction between High and Medium
or Low and the weight of evidence for the distinction between Low and Medium
or High.
Repeat this exercise for various diﬃculty values between −3 and +3. What
happens to the weight of evidence as the diﬃculty changes?
8.6 (Diﬃculty and Population Distribution). Repeat the previous exer-
cise with a population distribution of {.5, .3, .2} for the states High, Medium,

8.6 Eliciting Parameters and Laws
277
and Low. How does the relationship between diﬃculty and expected weight of
evidence change?
Repeat with a population distribution of {.2, .3, .5}.
8.7 (Discrimination and Mutual Information). Consider a skill, Skill 1,
with three levels: High, Medium, and Low, which are evenly distributed in the
population of interest. Consider a task which uses this skill and is scored
Right or Wrong. Build the conditional probability table for this model with
a compensatory distribution with a discrimination of 1.0 and a diﬃculty of
0.0. Calculate the mutual information between the proﬁciency variable and
the observable outcome variable.
Repeat this exercise for discrimination values of 0.25, 0.5, 0.75, 1.25, 1.5,
and 2.0. How is the discrimination related to the mutual information?
8.8 (Reading Skill and Limited Visual Acuity). Consider a reading
test where the skill Reading is deﬁned as having four levels: Advanced,
Proficient, Basic, and Below Basic. Assume that their distribution in
the population is {.2, .3, .3, .2}. Assume that the rules of evidence for the
task are such that an outcome variable has possible values: Full Credit,
Partial Credit, and No Credit. Naturally, a reading test requires suﬃ-
cient visual ability to read the words printed on the computer screen. Use
a second proﬁciency variable VisualAcuity with possible values Sufficient
and Insufficient to model this.
Build the conditional probability table for an inhibitor distribution to
model this. Use a discrimination of 1.0 and a diﬃculty of 0.0. What is the
mutual information between the observable and the Reading proﬁciency with
the VisualAcuity is Sufficient? When it is Insufficient? What does this
say about the appropriateness of this task for use with individuals with low
VisualAcuity?
8.9 (Decoding Skill and Limited Visual Acuity). Consider the same
model as before, but now a read aloud accommodation is oﬀered for individ-
uals with limited visual acuity. Assume further that the Reading construct
is primarily aimed at measuring the ability to decode the words in the doc-
ument. The appropriate model is now a kind of reverse inhibitor model in
which persons with the accommodation act as if they are in the highest pos-
sible category of the VisualAcuity skill.
Build the conditional probability table for this model using a discrimina-
tion of 1.0 and a diﬃculty of 0.0. What is the mutual information when the
accommodation is oﬀered? Not oﬀered? What does this say about the appro-
priateness of this task with the accommodation for use with individuals with
low Visual Acuity?
8.10 (Weight of Evidence for a Story Problem). Consider a story prob-
lem from an algebra test. The test is designed to assess Algebra skill, but
unless the student’s Reading level is at least at the Basic level, they will not

278
8 Parameters for Bayesian Network Models
be able to perform well on the problem no matter what their Algebra skill is.
Assume that the rules of evidence are such that there is a single observed out-
come which takes on values Right and Wrong. Also assume that the Algebra
variable takes on values High and Low.
Build an inhibitor model for this task. Use a discrimination and a diﬃculty
of 1.0. Assume that the Algebra skill is distributed 50/50 in the population,
and that 20 % of the population below the basic level in Reading. What is
the expected weight of evidence of this problem for the Algebra proﬁciency?
What happens if the proportion below basic on Reading drops to 10 %? Rises
to 30 %?

9
Learning in Models with Fixed Structure
The preceding chapters have described an approach to assessment design and
analysis that exploits the advantages of Bayesian networks. Chapter 5 showed
how to update beliefs about individual students’ proﬁciencies from the infor-
mation in their responses. Carrying out these calculations in practice, however,
requires the conditional probability distributions that are treated as known
in Chap. 5, and may be structured as in Chap. 8. This chapter addresses the
problem of estimating these distributions or parameters they are modeled in
terms of.
The following section begins by introducing representational forms for the
models that underlie Bayes nets, in terms of formulas and plate diagrams
(Buntine 1994). It lays out a general framework for expressing educational
measurement models in these terms. The next section is a brief sketch of
approaches to estimating the parameters in these kinds of models, including
Bayes modal estimation via the expectation–maximization (EM) algorithm
and Markov Chain Monte Carlo (MCMC) estimation. Both methods are then
discussed in additional detail, and illustrated with data from a simple latent
class example.
9.1 Data, Models, and Plate Notation
As we have seen repeatedly, conditional independence is central to probability-
based reasoning in situations with many variables. Models for large problems
are easier to understand and compute when we can structure them in terms
of conditionally independent instances of relatively simple and structurally
similar models. The dependencies that make their joint distribution complex
are attributed to their dependence on shared values of higher level parameters,
which may in turn have their own simplifying hierarchical structures.
Buntine (1994) introduced a set of graphical conventions that are useful for
depicting these kinds of models. They are a natural extension of the acyclic-
c⃝Springer Science+Business Media New York 2015
279
R. G. Almond et al., Bayesian Networks in Educational Assessment,
Statistics for Social and Behavioral Sciences, DOI 10.1007/978-1-4939-2125-6 9

280
9 Learning in Models with Fixed Structure
directed graphical notation discussed in Chap. 4, with the major elaboration
being
for structurally identical replications of models and variables. Plate
notation is introduced below with some simple examples that show the cor-
respondence between plate diagrams and probability models. A probability
model and a plate diagram for a general educational measurement model are
then presented.
We then discuss inference about structures like these—parameter estima-
tion, as statisticians typically call it, or “learning,” as it is more often called
in the artiﬁcial intelligence community. A natural starting point is estimating
the structural parameters of the model, namely the population proﬁciency dis-
tributions and conditional distributions for observable variables, when exam-
inees’ proﬁciency variables are known. This so-called complete data problem
is a straightforward application of the Bayesian updating in the binomial and
multinomial models. It is also at the heart of the more realistic incomplete
data problem, where neither examinees’ proﬁciencies nor structural parame-
ters are known. Two approaches are discussed: the EM algorithm (Dempster
et al. 1977) and MCMC estimate (Gilks et al. 1996). Many good references for
these techniques are available, so the focus here is on the key ideas and their
application in a simple problem. The special problem of bringing new tasks
into an established assessment, a recurring activity in ongoing assessment
systems, is also addressed.
9.1.1 Plate Notation
Plate notation (Buntine 1996) extends the notation for directed graphs we
developed in Chap. 4. As before, there are nodes that represent parameters and
variables, and directed edges that represent dependency relationships among
them. The new idea is an eﬃcient way to depict replications of variables and
structures by displaying a single representative on a “plate” that indicates
multiplicity.
As a ﬁrst simple example, consider the outcomes Xj of four draws from a
Bernoulli distribution with known parameter θ. The joint probability distri-
bution is  p(xj|θ). The usual digraph is shown in the top panel of Fig. 9.1.
The bottom panel represents the same joint distribution using a plate for
the replicated observations. The node for θ lies outside the plate—it is not
replicated—and inﬂuences all observations in the same manner. That is, the
structure of the conditional probability distributions for all four Xjs given θ
is the same. The double edge on the θ node indicates that this parameter is
known, while the single edge on the Xj node indicates that their values are
not known.
Plates can be nested. Consider a hierarchical model extending the previous
example such that four responses are observed each of N students. For each
student i, the response variables Xij are Bernoulli distributed with probability
θi1. These success probabilities are not known, and prior belief about them is
1 This is a variable not a parameter as it is student speciﬁc.

9.1 Data, Models, and Plate Notation
281
Fig. 9.1 Expanded and plate digraphs for four Bernoulli variables
Upper ﬁgure shows the full graph, lower ﬁgure shows the same structure with the
plate notation. Reprinted with permission from ETS.
expressed through a beta distribution with known parameters α and β. The
joint probability distribution is now

i

j
p (xij | θi)p (θi | α, β) ,
and the digraph using plate notation is as shown in Fig. 9.2. The replica-
tions of responses for a given examinee are conditionally independent and
all depend in the same way on the same Bernoulli probability θi, as implied
by 
j p (xij | θi). Similarly, all the θis depend in the same way on the same
higher level parameters α and β, as implied by 
i p (θi | α, β).
A ﬁnal introductory example is the Rasch model for dichotomous items,
shown in Fig. 9.3. At the center of the digraph, where plates for the proﬁciency
variables θi for students and diﬃculty parameters βj for items overlap, is the
probability pij of a correct answer by student i to item j:
pij = P (Xij = 1 | θi, βj) = Ψ (θi −βj) ≡
exp (θi −βj)
1 + exp (θi −βj) ,
where Ψ (·) denotes the cumulative logistic distribution. This probability is
known if θi and βj are known, through the functional form of the Rasch

282
9 Learning in Models with Fixed Structure
Fig. 9.2 Plate digraph for hierarchical Beta-Bernoulli model
Reprinted with permission from ETS.
model; this functional relationship rather than the stochastic relationship is
indicated in the digraph by a double edge on the node. That is, the double edge
indicates that the value of a variable is known or that it is known conditional
on the values of its parents. Logical functions such as an AND gate have this
property too, but not a noisy-AND because its outcome is probabilistic.
Examinee proﬁciency parameters are posited to follow a normal distri-
bution with unknown parameters mean μθ and variance σ2
θ. Higher level
distributions for the examinee proﬁciency distribution are μθ ∼N(μw, σ2
w)
and σ2
θ ∼Gamma(aθ, bθ), with the parameters of the higher level distribu-
tions known. Item parameters are also posited to follow a normal distribution,
with a mean ﬁxed at zero to set the scale and an unknown variance σ2
β, and
σ2
β ∼Gamma(aβ, bβ).
9.1.2
A Bayesian Framework for a Generic Measurement Model
In educational and psychological measurement models, observable variables
are outcomes of the confrontation between a person and a situation, or
more speciﬁcally, a task. In particular, observable variables X are modeled
as independent given unobservable, or latent, person variables2 θ and task
2 In the context of maximum likelihood estimation, these are called person param-
eters because they must be estimated, but this book is following the convention
of calling person-speciﬁc values variables rather than parameters.

9.1 Data, Models, and Plate Notation
283
σ0
σθ
θi
βj
aθ
aβ
bβ
σβ
bθ
μ0
μθ
pij
Xij
Observable j
Student i
Fig. 9.3 Plate digraph for hierarchical Rasch model
Reprinted with permission from ETS.
parameters β. In the Rasch model, for example, examinees have more or less
proﬁciency in the same amount with respect to all items, and items are more
or less diﬃcult for all examinees. (Section 9.1.3 gives extensions where this is
no longer technically true, including diﬀerential item functioning (DIF) and
mixture models.)
The discrete Bayes nets that are useful in cognitive diagnosis exhibit this
character. It is useful, however, to cast estimation in these models in a more
general framework in order to connect it with the broader psychometric and
statistical literature. The same characterization applies to the models of item
response theory (IRT), latent class analysis, factor analysis, latent proﬁle
analysis, and, at the level of parallel tests, parametric classical test theory.
Examples of familiar models that often have their own history, notation, and
terminology are shown in Table 9.13. All of these models diﬀer only in the
nature of the observable variables and student-model variables—discrete vs.
continuous, for example, or univariate vs. multivariate—and the form of the
link model, or probability model for observables given person variables and
task parameters. We will write p

xj | θ, βj

, and include a subscript i as in
p

xij | θi, βj

if we need to focus on the responses of a particular examinee i.
In discrete Bayesian models in cognitive diagnosis, p

xj | θ, βj

takes the
form of a collection of categorical distributions: For a given value of the pro-
ﬁciency variable(s) θ, there is a categorical probability distribution over the
possible values of xj, the value of the observable variable. The (vector valued)
3 The table is not exhaustive, as for example, classical test theory is readily applied
to vectors, factor analysis can be carried out without assuming normality, and
there are mixture models that include both discrete classes and classes with prob-
abilities structured by IRT models (e.g., Yamamoto 1987).

284
9 Learning in Models with Fixed Structure
parameter βj comprises the probabilities for each response category for each
given value of θ.
A full Bayesian model also requires distributions (or laws) for θs and βs
(as they are unknown). Treating examinees as exchangeable means that before
seeing responses, we have no information about which examinees might be
more proﬁcient than others, or whether there are groups of examinees more
similar to one another than to those in other groups. It is appropriate in
this case to model θs as independent and identically distributed, possibly
conditional on the parameter(s) λ of a higher level distribution or law. That
is, for all examinees i,
θi ∼p (θ | λ) .
In the normal-prior Rasch model example in the previous section, λ = (μθ, σ2
θ)
(the mean and variance of the normal law). If θ is the categorical variable, λ
will be category probabilities or a smaller number of parameters that imply
category probabilities in a more parsimonious way (e.g., the models discussed
in Chap. 8).
Similarly, if we have no prior beliefs to distinguish among item diﬃculties,
we can model βs as exchangeable given the parameter(s) of their law:
βj ∼p (β | ξ) .
In the Rasch example, ξ = σ2
β. When βs are probabilities in categorical dis-
tributions, as they are in discrete Bayes nets, then either p (β | ξ) is a beta or
Dirichlet prior on the probabilities or a function of smaller number of variables
that imply the conditional probabilities. The DiBello–Samejima distributions
in the previous chapter are an example of the latter.
Both λ and ξ may be speciﬁed as known, or as unknown with higher level
distributions, or laws, p(λ) and p(ξ).
The full Bayesian probability model for educational measurement is thus
p (X, θ, β, λ, ξ) =

i

j
p

xij | θi, βj

p (θi | λ) p

βj | ξ

p (λ) p (ξ) . (9.1)
The corresponding graph, using plate notation, is shown in Fig. 9.4.
9.1.3 Extension to Covariates
We brieﬂy note three common extensions of the basic measurement model that
involve covariates, or collateral information, about examinees, denoted Zi, or
about tasks, denoted Yj. (Note that the entries in a Q-matrix are collateral
information about tasks.) The ﬁrst is the extension to conditional exchange-
ability about examinee or task parameters. The second concerns interactions
between examinee covariates and response probabilities, or DIF. The third
concerns interactions as well, but when examinee covariates are not fully
known. These are mixture models.

9.1 Data, Models, and Plate Notation
285
Table 9.1 Special cases of the generic measurement model
Model
Student
variables (θ)
Observables
(X)
Link
function
Task
parameters (β)
Parametric
classical test
theory
Unidimen-
sional
continuous
(true score)
Continuous
observed
scores
Normal
Error variance
Dichotomous IRT Unidimen-
sional
continuous
Dichotomous
responses
Bernoulli
Item diﬃculty,
discrimination,
etc.
Graded response
IRT
Unidimen-
sional
continuous
Ordered
categorical
responses
Categorical
Item step
diﬃculties
Multidimensional
dichotomous IRT
Multivariate
continuous
Dichotomous
responses
Bernoulli
Item diﬃculty,
discrimination,
etc.
Latent class
Discrete (class
memberships)
Categorical
responses
(including
dichotomous)
Categorical
(including
Bernoulli)
Conditional
probabilities
Cognitive
diagnosis
Discrete
(attribute
masteries)
Categorical
responses
(including
dichotomous)
Categorical
(including
Bernoulli)
Conditional
probabilities
(and
parameters in
functional
forms)
Factor analysis
Multivariate
continuous
Continuous
scores
Normal
Factor
loadings
Latent proﬁle
analysis
Discrete (class
memberships)
Continuous
scores
Multivariate
normal
Means and
covariances for
each class
Dichotomous IRT
mixture
Unidimen-
sional
continuous
(proﬁciencies)
and discrete
(class
memberships)
Dichotomous
responses
Bernoulli
Item
parameters for
each class
(Discrete) Bayes
nets
Discrete
(proﬁciencies
and class
memberships)
Categorical
responses
Categorical
(including
Bernoulli)
Conditional
probabilities
(and
parameters in
functional
forms)

286
9 Learning in Models with Fixed Structure
θi
βj
λ
pij
Xij
Observable j
Student i
ξ
Fig. 9.4 Graph for generic measurement model
Reprinted with permission from ETS.
Z refers to known information about examinees such as demographic group
or instructional background. When this information aﬀects our beliefs about
examinees’ proﬁciencies, examinees are no longer exchangeable. Eighth grade
students are more likely to know how to subtract fractions than fourth grade
students, for example. We may however posit conditional exchangeability,
or exchangeability among examinees with the same covariates, by making
the distributions on θ conditional on covariates, namely p(θ|z, λ). Through
p(θ|z, λ), we model an expectation that examinees with the same value of Z
are more similar to one another than those with other values of Z, or that
eighth graders generally have higher proﬁciencies than fourth graders.
Similarly, we can incorporate covariates for tasks, through p(β|y, ξ). A task
model variable Y can indicate the particular task model used to generate a
task (or characteristics of the task deﬁned by the task model variable), so
we can use information about the parameters of similar tasks to estimate the
parameters of a new task from the same family, or allow for the fact that
problems with more steps are usually harder than problems with fewer steps
(Mislevy et al. 1993; Glas and van der Linden 2003).
Figure 9.5 depicts the extension to covariates that inﬂuence belief about
item or student parameters, but do not aﬀect responses directly. That is,
responses remain dependent on item parameters and student variables only.
If we knew an examinee’s θ and a task’s β, learning Z or Y would not change
our expectations about potential responses.
DIF occurs when learning Z or Y would change our expectations about
responses, beyond parameters for the student and task (Holland and Wainer
1993). One example occurred in a reading assessment that surveyed grades 8
and 12: A task that required ﬁlling in an employment application was rela-
tively easier for 12th grade students than for 8th grade students. Presumably
many 12th grade students had personal experience with employment applica-

9.2 Techniques for Learning with Fixed Structure
287
θi
βj
λ
pij
Xij
Observable j
Student i
ξ
Zi
Yi
Fig. 9.5 Graph with covariates and no DIF
Reprinted with permission from ETS.
tions, a factor that would make the item easier for such a student than one
who had not, but was otherwise generally similar in reading proﬁciency. In
this case, p

xj | θ, βj, Grade = 8

̸= p

xj | θ, βj, Grade = 12

. In such cases,
task parameters would diﬀer for at least some tasks, and be subscripted with
respect to covariates as βj(z). Figure 9.6 shows how DIF appears in a graph.
DIF concerns interactions between task response probabilities and exam-
inees’ background variables Z, when Z is known. Mixture models posit that
such interactions may exist, but examinees’ background variables are not
observed (e.g., Rost 1990). An example would be mixed number subtraction
items that vary in their diﬃculty depending on whether a student solves them
by breaking them into whole number and fraction subproblems or by con-
verting everything to mixed numbers and then subtracting (Tatsuoka 1983).
Figure 9.7 depicts the digraph for a generic mixture model. It diﬀers from the
DIF model only in that Z is marked as unobserved rather than observed. Note
that this requires specifying a prior (population) distribution for Z. Now the
higher level parameter λ for student variables has two components: λθ for the
law for θ and λZ representing the probability parameter in, say, a Bernoulli
law for Z if there are two classes, or a vector of probabilities for a categorical
law if there are more than two classes.
9.2 Techniques for Learning with Fixed Structure
How, then, does one learn the parameters β of conditional probability distri-
butions and λ of examinee proﬁciency distributions? This section addresses
inference in terms of the full Bayesian probability model of Eq. 9.1, the basics
of which are outlined below in Sect. 9.2.1. Section 9.2.2 discusses the sim-
pler problem in which students’ θs are observed as well as their xs—the
“complete data” problem, in the terminology introduced in Dempster et al.

288
9 Learning in Models with Fixed Structure
θi
βij
λ
pij
Xij
Observable j
Student i
ξ
Zi
Yi
Fig. 9.6 Graph with DIF
Reprinted with permission from ETS.
(1977)’s description of their EM algorithm. Section 9.3 relates the complete
data problem to the “incomplete data” problem we face in Eq. 9.1. This chap-
ter introduces two common approaches to solve it, namely the EM algorithm
(Sect. 9.4) and MCMC (Sect. 9.5). Although both algorithms are general
enough to work with any model covered by Eq. 9.1, the focus of this chapter
is on discrete Bayes nets measurement models.
9.2.1 Bayesian Inference for the General Measurement Model
Fig. 9.7 Graph for mixture model
Reprinted with permission from ETS.
The full Bayesian model (Eq. 9.1) contains observable variables X, pro-
ﬁciency variables (sometimes called person parameters) θ, task parameters

9.2 Techniques for Learning with Fixed Structure
289
β, and higher level parameters λ and ξ. Equation 9.1 represents knowledge
about the interrelationships among all the variables in the generic measure-
ment model without covariates, before any observations are made. The only
observations that can be made are those for observable variables. We observe
realized values of X, say x∗. In Bayesian inference, the basis of inference about
person variables and task parameters and their distributions is obtained by
examining the conditional distribution of these variables conditional on x∗.
By Bayes theorem, this posterior is easy enough to express. Aside from a nor-
malizing constant K, it has exactly the same form as Eq. 9.1, just with the
expressions involving X evaluated at their observed values:
p (θ, β, λ, ξ | x∗) = K

i

j
p

x∗
ij | θi, βj

p (θi | λ) p

βj | ξ

p (λ) p (ξ) ,
(9.2)
where
K−1 = p (x∗)
=

p (x∗| θ, β, λ, ξ) ∂θ ∂β ∂λ ∂ξ
=
 
i

j p

x∗
ij | θi, βj

p (θi | λ) p

βj | ξ

p (λ) p (ξ) ∂θ ∂β ∂λ ∂ξ .
(9.3)
Bayesian inference proceeds by determining important features of the poste-
rior distribution, such as means, modes, and variances of variables, picturing
their marginal distributions, calculating credibility intervals, and displaying
plots of marginal or joint distributions. If the prior laws are chosen to be
conjugate to the likelihood, then simple procedures of posterior inference are
available for independent and identically distributed samples, as illustrated in
the following section for the complete data problem.
But Eq. 9.2 in its entirety does not always have a natural conjugate, even
when all of the factors taken by themselves do (especially if some of the
variables are missing; see York 1992). So, while the form of Eq. 9.2 is simple
enough to write, the diﬃculty of evaluating the normalizing constant (Eq. 9.3)
renders direct inference from the posterior all but impossible. Approximations
must be found.
9.2.2 Complete Data Tables
The population distribution of proﬁciency variables (the proﬁciency model)
and conditional probability of observable outcome variables given the proﬁ-
ciency variables (the evidence models and link models) in discrete Bayes nets
take the form of categorical distributions, the special case being Bernoulli
distributions when there are just two categories. Bayesian inference in these
models is particularly simple, since they admit to posterior inference via con-
jugate prior distributions, and the forms of the priors, likelihoods, and poste-
riors have intuitive interpretations. This section describes Bayesian updating

290
9 Learning in Models with Fixed Structure
for the multinomial distributions, starting with the Bernoulli distributions
and generalizing to categorical distributions. An example of parameter esti-
mation for a complete data version of a simple Bayes net follows. We will
start with a quick review of Bayesian inference for Bernoulli and categorical
distributions.
A Bernoulli random variable X has two possible outcomes, which we may
denote 0 and 1. Let π be the probability that X = 1, typically representing
a “success” or “occurrence” event, or with dichotomous test items a correct
response. It follows that 1 −π is the probability that X = 0, a “failure” or
“nonoccurrence,” or an incorrect item response. The probability for a single
observation is πx(1 −π)1−x. The probability for r successes in n independent
replications is
p (r |n, π ) ∝πr (1 −π)n−r .
(9.4)
Such counts of successes in a speciﬁed number n of independent Bernoulli
trials with a common parameter π are said to follow a binomial distribution.
Once n trials occur and r successes are observed, Eq. 9.4 is interpreted as a
likelihood function, which we denote as L (π | r, n). The maximum likelihood
estimate (MLE) of π is the value that maximizes the likelihood, or equiva-
lently maximizes its log likelihood ℓ(π | r, n) = log [L (π | r, n)]. In the case
of Bernoulli trials, the MLE is ˆπ = r/n, the sample proportion of successes,
a suﬃcient statistic for π. The squared standard error of the mean, or error
variance, is π (1 −π) /n.
For Bayesian inference, the conjugate prior for the Bernoulli and binomial
distribution is the beta distribution (deﬁned in Sect. 3.5.3). What that means
is this: If we can express our prior belief about π in terms of a beta law,
then the posterior for π that results from combining this prior through Bayes
theorem with a likelihood in the form of Eq. 9.4 is also a beta law. Bayesian
inference through conjugate distributions has the advantages of eliminating
concern about the normalizing constant (in the case of the beta distribution,
the normalizing constant is the beta function evaluated at the parameters that
can be looked up in a table), and relegating posterior analysis to features of
a well-known family of distributions (conjugate families).
Leaving out the normalization constant, the p.d.f. for the beta distribution
is:
p (π|a, b) ∝πa−1 (1 −π)b−1 .
(9.5)
Comparing the form of the beta distribution in Eq. 9.5 with the form of the
binomial distribution in Eq. 9.4 suggests that the beta can be thought of as
what one learns about the unknown parameter π of a binomial distribution
from observing a −1 successes and b −1 failures or, in other words, from
a + b −2 trials of which the proportions (a −1)/(a + b −2) are successes. This
interpretation is reinforced by the combination of a beta prior and a binomial

9.2 Techniques for Learning with Fixed Structure
291
likelihood through Bayes theorem:
p(π|a, b)×L(π|r, n) ∝Beta (π | a, b)×L (π|r, n) ∝Beta (π | a + r, b + (n −r)) .
(9.6)
That is, a posterior state of knowledge about π after updating a Beta(a, b)
prior with the information from r successes in n trails is the same as it would
have been based on observing a+ r successes from (a+ b)+ n Bernoulli trials.
Table 9.2 shows what happens to the prior and posterior.
Table 9.2 Prior and posterior statistics for beta distribution with r successes in n
trials
Statistic
Prior
Posterior
Law
Beta(a, b)
Beta (a + r, b + (n −r))
Mean
a
a+b
a+r
a+b+n
Mode
a−1
a+b−2
a+r−1
a+b+r−2
Variance
ab
(a+b)2(a+b+1)
(a+r)(b+n−r)
(a+b+n)2(a+b+n+1)
As the observed sample size, n, becomes large compared with the pseudo-
sample size embodied in the prior, a+b, the posterior mean and mode approach
the sample mean and the posterior variance approaches the expression for the
maximum likelihood sampling variance evaluated with the MLE.
These results of Bayesian conjugate inference for the Bernoulli and bino-
mial generalize in a straightforward manner to the categorical and multino-
mial distributions. Now there are K possible outcomes, 1, . . . , K. Let pk be
the probability that X takes the particular value xk, where  pk = 1. This
is a categorical distribution, which simpliﬁes to a Bernoulli distribution when
K = 2. Let Rk be the count of the number of observations in category k from
n independent samples of the categorical variable. The collection of random
variables R1, . . . , RK follows a multinomial distribution:
P (R1 = r1, . . . , RK = rK | π1, ..., πK) ∝
K

k=1
πrk
k
,
(9.7)
when K
k=1 rk = n and 0 otherwise (Sect. 3.5.3). The MLEs for the category
probabilities are the observed proportions rk/n.
The conjugate prior for the categorical and multinomial distributions is
the Dirichlet distribution, with parameters a1, . . . , aK:
p (π|a1, . . . , aK) ∝πa1−1
1
× · · · × πaK−1
K
.
(9.8)
Letting n+ =  rK, a Dirichlet corresponds to what one knows about π from
observing a sample of n+ independent observations, with proportions ak/n+.

292
9 Learning in Models with Fixed Structure
With a Dirichlet prior, then, it follows from Eqs. 9.7 and 9.8 that the posterior
distribution induced by the observation of r = r1, . . . , rK is also Dirichlet:
p (π | α) × L (π | r, n) ∝Dirch (a1 + r1, ..., aK + rK) .
(9.9)
The posterior can be thought of as the results of combining the results from
a real sample of size n and a pseudo-sample of size n+. The posterior mean
for πk is (ak + rk)/(n + n+).
In discrete Bayes nets, each variable has a categorical distribution given the
state of its parent variables in the graph. This is the conditional multinomial
distribution from Sect. 8.3. If the parameters of these categorical distributions
are unknown, then the conjugate prior law will be a Beta law or Dirichlet law,
depending on the number of possible categories for the variable. We require
one law for each possible combination of the values of the parent variables (row
of the conditional probability table). We refer to such a collection of laws
as a hyper-Dirichlet prior law. The following example shows how updating
the distribution for conditional probabilities in a Bayes net would simply
be a matter of adding contingency tables of category counts if examinees’
proﬁciency variables were known.
Example 9.1 (Complete Data Estimation). Figure 9.8 is a simple dis-
crete Bayes net, which also happens to be a latent class model (Dayton 1999).
It contains one proﬁciency variable, θ, and three conditionally independent
observable variables, X1, X2, and X3. (The plate notation in the graph is
good for any number of observables, but the example only uses three to keep
things simple.) The proﬁciency variable θ can take two values, 1 for mas-
tery and 0 for nonmastery. The ﬁrst two observables, X1 and X2, are both
scored outcomes from dichotomous test items, with a value of 1 representing
a correct response and 0 an incorrect response. The last observable, X3, is
the scored outcome of a partial-credit task, taking values of 0, 1, and 2 that
represent responses of increasing quality. πj represents the conditional prob-
ability matrix for Xj. Which row applies to a given student is determined by
which class the student belongs to, indicated by θi.
Fig. 9.8 Graph for three-item latent class model
Reprinted with permission from ETS.

9.2 Techniques for Learning with Fixed Structure
293
Carrying out Bayesian updating for an examinee through a Bayes net
requires the probability distributions (and laws) listed below. The Bernoulli
distributions are written with the success probability ﬁrst, while the cate-
gorical distributions are written in terms of increasing order. This is a little
inconvenient here, but it will make writing the priors, data, and posteriors
under Bayesian conjugate updating with the Beta and Dirichlet priors more
transparent.
p (θ | λ) :
(P (θ = 1) , P (θ = 0))
= (λ, 1 −λ)
p (x1 | θ, π1) :
 
(P (X1 = 1 | θ = 0) , P (X1 = 0 | θ = 0))
(P (X1 = 1 | θ = 1) , P (X1 = 0 | θ = 1))
!
=
 
(π10, 1 −π10)
(π11, 1 −π11)
!
p (x2 |θ, π2 ) :
 (P (X2 = 1 |θ = 0) , P (X2 = 0 |θ = 0))
(P (X2 = 1 |θ = 1) , P (X2 = 0 |θ = 1))
!
=
 (π20, 1 −π20)
(π21, 1 −π21)
!
p (x3 |θ, π3 ) :
 (P (X3 = 0 |θ = 0) , P (X3 = 1 |θ = 0) , P (X3 = 2 |θ = 0))
(P (X3 = 0 |θ = 1) , P (X3 = 1 |θ = 1) , P (X3 = 2 |θ = 1))
!
=
 
(π300, π301, π302)
(π310, π311, π312)
!
.
The unknown parameters needed in the Bayes net are λ, π1, π2, and π3. We
propose prior laws that are proper but weak, with weights equivalent to six
observations in each case:
p (λ) = Beta (3, 3)
p (π10) = Beta (2, 4)
p (π11) = Beta (4, 2)
p (π20) = Beta (2, 4)
p (π21) = Beta (4, 2)
p (π30) = Dirch (3, 2, 1)
p (π31) = Dirch (1, 2, 3)
(9.10)
These priors mildly express a belief that masters and nonmasters are equally
likely, and that masters are more likely to give better answers than nonmas-
ters. If we had to carry out inference in a discrete Bayes net without accounting
for our uncertainty about these probabilities, we would use the means of these
priors in the network:
E [p (λ)] = .5
E [p (π10)] = .33
E [p (π11)] = .67
E [p (π20)] = .33
E [p (π21)] = .67
E [p (π30)] = (.500, .333, .167)
E [p (π31)] = (.167, .333, .500) .

294
9 Learning in Models with Fixed Structure
To illustrate Bayesian inference for the parameters λ and π, we generated
100 draws from the implied joint probability distribution P(θ, X1, X2, X3).
The “true”4 parameter values in the simulation were λ = .7, π10 = .1, π11 = .8,
π20 = .3, π21 = .6, π30 = (.5, .3, .2), and π31 = (.1, .2, .7). Table 9.3 shows the
counts of patterns for (θ, X1, X2, X3), the data in the complete data problem
and Table 9.4 shows the counts of patterns (X1, X2, X3) after collapsing over
θ. The latter are what would be observed in the practice, in the incomplete
data problem.
Bayesian inference is straightforward in the complete data case. We can
compute the suﬃcient statistics for λ, the observed counts of nonmasters and
masters, say r0 and r1. For each item j, we can calculate the counts at each
mastery level m of each response k, which we write as rjmk. Writing the data
in the same pattern as the priors, we obtain
λ :
(r1, r0)
= (76, 24)
π10 :
(r101, r100)
= (0, 24)
π11 :
(r111, r110)
= (51, 25)
π20 :
(r201, r200)
= (4, 20)
π21 :
(r211, r210)
= (58, 18)
π30 : (r300, r301, r302) = (10, 9, 5)
π31 : (r310, r311, r312) = (15, 15, 46) .
(9.11)
Note that the MLEs in the complete data problem are just the proportions
from these observed counts:
ˆλ = .760
ˆπ10 = 0
ˆπ11 = .671
ˆπ20 = .167
ˆπ21 = .763
ˆπ30 = (.417, .375, .208)
ˆπ31 = (.197, .197, .605)
Because no nonmaster got task 1 right, ˆπ10 = 0—the value that gives the
maximum probability for the observed data. But ˆπ10 = 0 does not reﬂect our
belief that this result is not likely but it surely is not impossible. We would
not want to put this conditional probability into a Bayes net to assess new
students.
Returning to the Bayesian solution, let X denote the complete data, that
is the data in Table 9.3. The posterior laws are Beta or Dirichlet, obtained by
summing the parameters of the prior from Eq. (9.10) and the observed counts
from Eq. (9.11) summarized previously:
4 In most practical problems, the truth is unknowable, but in a simulation experi-
ment, the truth is the value chosen for the simulator input.

9.2 Techniques for Learning with Fixed Structure
295
Table 9.3 Response pattern counts with proﬁciency variable, θ
Proﬁciencies Observables Observed
θ
X1 X2 X3
count
0
0
0
0
6
0
0
0
1
9
0
0
0
2
5
0
0
1
0
4
0
0
1
1
0
0
0
1
2
0
0
1
0
0
0
0
1
0
1
0
0
1
0
2
0
0
1
1
0
0
0
1
1
1
0
0
1
1
2
0
1
0
0
0
0
1
0
0
1
0
1
0
0
2
0
1
0
1
0
4
1
0
1
1
6
1
0
1
2
15
1
1
0
0
6
1
1
0
1
2
1
1
0
2
10
1
1
1
0
5
1
1
1
1
7
1
1
1
2
21
Table 9.4 Response pattern counts collapsing over proﬁciency variable, θ
Observables Observed
X1 X2 X3
count
0
0
0
6
0
0
1
9
0
0
2
5
0
1
0
8
0
1
1
6
0
1
2
15
1
0
0
6
1
0
1
2
1
0
2
10
1
1
0
5
1
1
1
7
1
1
2
21

296
9 Learning in Models with Fixed Structure
P (λ | θ, X) =
Beta (3 + 76, 3 + 24)
= Beta (79, 27)
P (π10 | θ, X) =
Beta (2 + 0, 4 + 24)
= Beta (2, 28)
P (π11 | θ, X) =
Beta (4 + 51, 2 + 25)
= Beta (55, 27)
P (π20 | θ, X) =
Beta (2 + 4, 4 + 20)
= Beta (6, 24)
P (π21 | θ, X) =
Beta (4 + 58, 2 + 18)
= Beta (62, 20)
P (π30 | θ, X) =
Dirch (3 + 10, 2 + 9, 1 + 5)
= Dirch (13, 11, 6)
P (π31 | θ, X) = Dirch (1 + 15, 2 + 15, 3 + 46) = Dirch (16, 17, 49)
The conditional probabilities for nonmasters are estimated less precisely
than those for masters, as seen by the smaller sums of parameters in the Beta
and Dirichlet posteriors. This is because there were about a third as many
nonmasters in the sample from which to estimate them. For example, by the
formulae in Table 9.2, we see the posterior standard deviations for π20 and
π21 are .072 and .047. The posterior means for the parameters, which could
be used in a Bayes net for calculating posterior distributions for the θs of
individual students, are obtained as:
E [P (λ | θ, X)] = .745
E [P (π10 | θ, X)] = .067
E [P (π11 | θ, X)] = .670
E [P (π20 | θ, X)] = .200
E [P (π21 | θ, X)] = .756
E [P (π30 | θ, X)] = (.433, .367, .200)
E [P (π31 | θ, X)] = (.195, .207, .598) .
Numerically these point estimates are not much diﬀerent from the MLEs,
which we would expect from using mild priors. But the zero MLE for π10 has
been replaced by a small nonzero value, namely .067. This value arises from
having observed no correct answers from 24 nonmasters when we expressed a
prior expectation of .33, with the weight of 6 observations. And the identical
MLEs of .197 for π310 and π311 that came from identical counts of 0 and 1
responses to X3 from masters are shifted to posterior means of .195 and .207,
an ordering that comes from our prior belief that masters’ probabilities should
be increasing for higher scores on X3.
The complete data case scales to Bayesian networks of arbitrary complex-
ity. Spiegelhalter and Lauritzen (1990) prove that if all of the prior laws for a
Bayesian network are independent Dirichlet distributions (a hyper-Dirichlet
law) and for each individual in the sample, there is complete data for every
variable in the Bayes net, then the posterior law will be a hyper-Dirichlet

9.3 Latent Variables as Missing Data
297
law as well. The update procedure simply replicates the calculations shown
here for every conditional probability table in the networks. Unfortunately,
as Spiegelhalter and Lauritzen (1990) observe, this conjugacy breaks down if
any of the variables are missing for any of the individuals in the sample.
9.3 Latent Variables as Missing Data
Observing values of proﬁciency variables (i.e., θ) would make estimation for
the parameters of Bayes nets easy, but by their nature they can never be
observed. What can we do? Fortunately, the perspective on missing data
developed by Donald Rubin (Rubin 1977; Rubin 1987; Little and Rubin 1987)
provides some leverage.
In Rubin’s terms, an observation is missing at random (MAR) if the mech-
anism by which the value of a random variable came to be missing does not
depend on that variable, conditional on data that are observed. This is a
weaker condition than missing completely at random (MCAR), where the
probability of missingness does not depend on the value of the variable or
the values of variables that are observed as well. MCAR implies MAR. Both
MAR and MCAR are independence statements. If Y is the variable that may
or may not be missing, SY is an indicator telling whether or not Y is missing,
and Z is the collection of completely observed data, then MAR is equivalent to
the conditional independence statement Y ⊥⊥SY |Z, and MCAR is equivalent
to the marginal independence statement SY ⊥⊥Y, Z.
Although the MAR assumption is central to most modern thinking about
missing data, it does not always hold in practice. For example, missing
responses to pain surveys are not MAR if patients do not ﬁll out the form on
days when they do not feel up to it. In such cases, the solution often lies in
gathering additional fully observed covariates so that the MAR assumption
holds at least approximately. The key result is that what one knows about
a missing observation that is MAR is appropriately expressed by its predic-
tive distribution, given whatever data have been observed. In other words, we
must be able to model P(Y |Z), or, if necessary, P(Y |Z, SY ).
Mislevy (2015) works through a number of examples of inference in the
presence of missing responses in IRT . Suppose an examinee is administered
one of several test forms selected at random. Her response values for items on a
form not presented are MCAR. In a computerized adaptive test (CAT), items
are selected for administration one at a time based on an examinee’s previ-
ous responses, in order to be maximally informative about that examinee’s
proﬁciency. Examinees doing well tend to receive harder items next, while
students doing poorly are administered easier ones. The responses to items
not administered in a CAT are MAR, but not MCAR. Suppose an examinee is
presented a test booklet and decides to omit some of them because he thinks

298
9 Learning in Models with Fixed Structure
he would probably get them wrong. These responses are neither MCAR nor
MAR.
An important result for assessment is that the latent variables θ in the pro-
ﬁciency models are always MAR, because they are missing for everyone regard-
less of their values. Proﬁciency variables are not MCAR, however, because
under psychometric models, they are instrumental in determining the prob-
abilities of observable variables X. Hence learning Student i’s responses xi
provides information about θi. If the parameters λ and β were known, the
missing value of θi would be appropriately replaced in Bayesian and maximum
likelihood inference by the predictive distribution:
p (θ | xi, λ, β) ∝p (xi | θ, β) p (θ | λ) ,
(9.12)
or if covariates yi for students were also available,
p (θ | xi, yi, λ, β) ∝p (xi | θ, β) p (θ | yi, λ) .
These ideas lie at the heart of popular methods for estimating the param-
eters in Bayes nets models. They are all based on ﬁlling in, in one way or
another, missing variables based on their predictive distributions given data
and information about other variables in the model. Spiegelhalter and Lau-
ritzen (1990), for example, developed an approach for learning the parameters
of hyper-Dirichlet distributions in the face of missing data. In general, they
are mixtures of Dirichlet distributions. One important result is that parameter
estimates for λ and πj that would have been independent (global indepen-
dence) under the complete data problem generally are not independent in the
incomplete data problem.
The following two sections address the problem from the point of view
of two more generally applicable approaches, namely the EM algorithm and
MCMC estimation. The focus is on the underlying concepts and how they
play out in Bayes nets with latent proﬁciency variables. For more in-depth
discussions of these and other estimation approaches in psychometric models,
see Junker (1999) and Patz and Junker (1999b); Patz and Junker (1999a).
9.4 The EM Algorithm
Until the recent rise in popularity of MCMC estimation, the most popu-
lar method of carrying out inference for complex posteriors in measurement
models was maximization (e.g., Mislevy 1986). This is because the values of
the parameters that maximize Eq. 9.2, or equivalently of its log, can be found
without having to calculate the normalizing constant. In many problems, once
samples are large enough for the data to swamp the inﬂuence of prior distribu-
tions, the posterior is essentially normal, the maximizing values are essentially
posterior means as well as modes (as well as MLEs), and the negative inverse

9.4 The EM Algorithm
299
of the matrix of second derivatives of the log posterior approximates the pos-
terior covariance matrix.
The joint posterior mode does not behave well in certain circumstances
however (O’Hagan 1976), and one of these is the case of “inﬁnitely many
incidental parameters5 (Neyman and Scott 1948)”. The general measurement
model presented as Eq. 9.1 exhibits just this property. The problem is that
for a ﬁxed set of tasks, increasing the sample size of examinees also propor-
tionally increases the number of student proﬁciency variables, or θs. In the
language of maximum likelihood estimation, θs are “incidental parameters,”
in contrast to the “structural parameters” β for tasks and λ for the proﬁciency
distribution, which do not increase with sample size. From the perspective of
maximum likelihood estimation, MLEs of the parameters λ and β can be
inconsistent. From the perspective of Bayesian inference, the posterior distri-
butions of both proﬁciency variables (incidental parameters) and parameters
(structural parameters) can be markedly nonnormal, so that the posterior
modes can be far from means and the normal approximation to the posterior
covariance matrix can give a misleading impression of both the shape and the
dispersion of the posterior.
Approximation based on maximizing the likelihood or posterior in such
cases can be improved, often dramatically so, by marginalizing or integrating
over the incidental parameters, here the proﬁciency variables. The posterior
marginal with respect to θs is
p (β, λ, ξ | X) =
⎡
⎣

θ

i

j
p

xij | θi, βj

p (θi | λ) ∂θi
⎤
⎦p

βj | ξ

p (λ) p (ξ) .
The expression in brackets on the right is called the marginal likelihood func-
tion. Maximum marginal likelihood (MML) estimation proceeds by maximiz-
ing this factor only with respect to λ and/or β, or equivalently its logarithm,
the log marginal likelihood
ℓ(β, λ | X) = log
⎡
⎣

θ

i

j
p

xij | θi, βj

p (θi | λ) ∂θi
⎤
⎦.
(9.13)
This is the formal expression of the incomplete data problem. From the per-
spective of maximum likelihood estimation, consistent estimates of λ and
β are obtained because increasing the size of the student sample does not
increase the number of parameters (including proﬁciency variables as param-
eters) to be estimated. From the perspective of Bayesian modal inference,
the marginal posterior for the parameters is typically much nearer to normal
5 In this context, the proﬁciency variables count as “parameters” because they must
be estimated from data.

300
9 Learning in Models with Fixed Structure
after many poorly determined nuisance variables (i.e., the proﬁciency vari-
ables) have been removed by marginalization.
Maximizing
Eq.
9.13
can
itself
be
a
challenge,
as
seen
in
Bock and Lieberman’s (1970) MML solution for IRT. Bock and Aitkin (1981)
found however that rearranging the order of computations led to a more
tractable iterative solution, in which each iteration presented a facsimile of
the easier to solve complete data problem. Bock and Aitkin’s solution turns
out to be a variant of the EM algorithm. The EM exploits what is known
about the missing variables, as seen in Sect. 9.3, to write the expected value
of the complete data log likelihood for λ and β at each cycle, conditional on
provisional estimates from the previous cycle:
Qt+1 (β, λ | X) = Eθ
⎡
⎣log

i

j
p

xij | θi, βj

p (θi | λ) | βt, λt, X
⎤
⎦.
(9.14)
In Bayesian modal estimation, Eq. 9.14 is additionally multiplied by the fac-
tors for the priors, namely  p

βj | ξ

p (λ) p (ξ), before maximizing. An E-
step calculates the expectation of the complete data log likelihood. The M-step
maximizes the result. The estimates obtained in each such cycle increase the
marginal likelihood or posterior, as required.
Some important properties of EM estimates are these (McLachlan and
Krishnan 2008). Under identiﬁability conditions, iterations converge to a local
maximum. When this is a unique global maximum, it is the MLE or poste-
rior mode, as required. This is always the case when the problem is in the
exponential family—a class of distributions that includes many distributions
discussed in this book, including the normal distribution, binomial and multi-
nomial distributions, beta and gamma distributions, and the Dirichlet distri-
bution. Again under identiﬁability conditions, the rate of convergence near the
maximum is geometric. The rate of convergence varies for diﬀerent parameters
depending on the amount of information available about them, and can be
very slow. Convergence can be accelerated by methods such as those described
by Lange (1995). An asymptotic normal approximation for the sampling vari-
ance or posterior covariance matrix (which are the same in the limit) can be
obtained as a computational by-product by the methods of Louis (1982) and
Cai (2008).
As noted earlier, the multinomial distributions that constitute the condi-
tional distributions in Bayes nets belong to the exponential family. Apply-
ing the EM to such problems is known in the categorical analysis literature
as iterative proportional ﬁtting (Deming and Stephan 1940; Fienberg 1970;
Haberman 1972). The solution takes the intuitively appealing form shown
below.
Example 9.2 (An EM Solution). We saw in Example 9.1 that the suf-
ﬁcient statistics for the population proportion λ in the complete data con-
tingency table problem were the counts of nonmasters and masters, r0 and

9.4 The EM Algorithm
301
r1. The suﬃcient statistics for the conditional response probabilities π in the
two classes were the observed counts rjmk of examinees in class m giving
response k to item j. These counts are all obtained as appropriate collapsing
of the information in the complete data table (Table 9.3), which gives the
number of students in each mastery class with each possible response pattern.
The counts cannot be observed directly in the incomplete data latent class
problem, because individual students’ class membership is not known.
If the πs and λ were known, the expectations of the rs could be calculated
from the observed response patterns. The EM solution proceeds iteratively as
follows. The expectations of the r’s, say ¯r(t), are calculated using provisional
estimates π(t) and λ(t) of the structural parameters. This is the E-step. Then
the facsimile of a complete data problem using an ¯r(t) in the place of each r is
solved to obtain improved estimates π(t+1) and λ(t+1) (One diﬀerence is that
the EM algorithm seeks the posterior mode rather than the posterior mean).
This is the M-step.
Consider Bayes modal estimation with the counts of observed response
patterns (Table 9.4) and prior distributions (9.10) from Example 9.1. Let
the initial estimates of the parameters take the following values: λ(0) = .5,
π(0)
10 = .2, π(0)
11 = .8, π(0)
20 = .2, π(0)
21 = .8, π(0)
30 = (.43, .34, .23), and π(0)
30 =
(.23, .34, .43).
The ﬁrst E-step begins from here by calculating the expected value of the
count of masters given the provisional values of the structural parameters.
This can be calculated examinee by examinee (usually more eﬃcient with
small samples and long tests), or over response patterns weighted by the
number of examinees with that pattern (usually more eﬃcient with large
samples and short tests). The expected count of masters, ¯r(0)
1
is obtained as
the sum of the posterior probability of being a master, over all examinees:
¯r(0)
1
=

i
p

θ = 1 | xi, λ(0), π(0)
=

i p

xi | θ = 1, λ(0), π(0)
P

θ = 1 | λ(0), π(0)
P

xi | λ(0), π(0)
=

i p

xi | θ = 1, λ(0), π(0)
P

θ = 1 | λ(0), π(0)

i p(xi|θ=1,λ(0),π(0))P(θ=1|λ(0),π(0))+
i p(xi|θ=0,λ(0),π(0))P(θ=0|λ(0),π(0)) ,
where
p

xi | θ = 1, λ(0), π(0)
P

θ = 1 | λ(0), π(0)
=p

xi1, xi2, xi3 | θ = 1, π(0)
P

θ = 1 | λ(0)
=

π(0)
11
xi1 
1 −π(0)
11
1−xi1 
π(0)
21
xi2 
1 −π(0)
21
1−xi2

π(0)
310
0[xi3] 
π(0)
311
1[xi3] 
π(0)
312
2[xi3]
λ(0)
1
.
Here, the notation k [xi3] in the exponent for the terms concerning the three-
category response is an indicator that takes the value 1 if xi3 = k and 0 if
not.

302
9 Learning in Models with Fixed Structure
For example, the following calculations produce the posterior probabilities
for response pattern 000 in the ﬁrst cycle, namely P

θ=0|X=(000), λ(0), π(0)
and P

θ = 1 | X = (000), λ(0), π(0)
.
First calculate the likelihood functions for the response pattern 000, given
class membership (θ) and provisional estimates of response probabilities given
class membership (π(0)). These are:
P

X = (000) | θ = 0, λ(0), π(0)
=P

X1 = 0, X2 = 0, X3 = 0 | θ = 0, π(0)
=P

X1 = 0 | θ = 0, π(0)
P

X2 = 0 | θ = 0, π(0)
P

X3 = 0 | θ = 0, π(0)
=(1 −π10)(1 −π20)π300
=.8 × .8 × .43
=.275 .
(9.15)
P

X = (000) | θ = 1, λ(0), π(0)
=P

X1 = 0, X2 = 0, X3 = 0 | θ = 1, π(0)
=P

X1 = 0 | θ = 1, π(0)
P

X2 = 0 | θ = 1, π(0)
P

X3 = 0 | θ = 1, π(0)
=(1 −π11)(1 −π21)π310
=.2 × .2 × .23
=.0092 .
(9.16)
The marginal probability of response pattern 000 is the average of the condi-
tional probabilities from two mastery classes, each weighted by the provisional
estimate of the proportions of masters (λ(0)) and nonmasters in the popula-
tion:
P

X = (000) | λ(0), π(0)
=P

X = (000) | θ = 0, λ(0), π(0)
P

θ = 0 | λ(0), π(0)
+
P

X = (000) | θ = 1, λ(0), π(0)
P

θ = 1 | λ(0), π(0)
=.275 ×

1 −λ(0)
+ .0092 × λ(0)
=.275 × .5 + .0092 × .5
=.1375 + .0046 = .1421 .
(9.17)
Posterior probabilities of mastery and nonmastery classes given response pat-
tern 000 and provisional parameter estimates π(0) and λ(0), are obtained with
Bayes theorem:
P

θ = 0 | X = (000), λ(0), π(0)
=
P(X=(000)|θ=0,π(0))(1−λ(0))
P(X=(000)|λ(0),π(0))
=
.1375
.1421 = .9676 ;
(9.18)
P

θ = 1 | X = (000), λ(0), π(0)
=
P(X=(000)|θ=1,π(0))λ(0)
P(X=(000)|λ(0),π(0))
=
.0046
.1421 = .0324 .
(9.19)

9.4 The EM Algorithm
303
The sum over all examinees of expressions like Eq. 9.19, or posterior probabil-
ities of being in the mastery class given responses and provisional parameter
estimates, produces r(0)
1 , the expected count of masters.
The expected count of correct responses to item 1 from masters is calcu-
lated in a similar manner, with the summation over only those examinees who
answered correctly:
¯r(0)
111 =

i:xi1=1
P

θ = 1 | xi, λ(0), π(0)
.
The same procedure is used to obtain expected values for all the suﬃcient
statistics. In this example, there are only 12 possible response patterns, so
computation is more conveniently carried out with respect to response pat-
terns, then taking sums weighted by the counts of the observed patterns.
Table 9.5 gives the required values by response pattern; that is, for each
response pattern, it gives the observed counts (collapsing across masters and
nonmasters), the likelihood of each pattern given the initial values of λ and π
under each class, and the posterior probabilities for each class. It does so for
this ﬁrst iteration, and also for iterations 10 and 100. We will say more about
them shortly, but for now we focus on the column for iteration 1. We can use
these posterior probabilities to produce a facsimile of the complete data table,
Table 9.3. A given examinee contributed to the count in exactly one line of
Table 9.3, namely the line for her response pattern and her class. In the E-
step, an examinee contributes in two places, partially, to the expected counts
in Table 9.6: for the count in the rows with her observed response pattern for
both classes, distributed according to her provisional posterior probabilities
of being in each class. For example, an examinee with pattern 000 contributes
.968 to the row for θ = 0 and X = 000, and .032 to the row for θ = 1 and
X = 000. Finally, Table 9.7 gives the resulting sums for the suﬃcient statistics
¯r(t) in iteration 1.
The M-step consists of solving the facsimile of the complete data problem,
which in this case requires combining the expected counts obtained above
with the pseudo-counts given by the Beta and Dirichlet priors:
P(1) (λ | θ, X) =
Beta (3 + 57.8, 3 + 42.2)
= Beta (60.8, 45.2)
P(1) (π10 | θ, X) =
Beta (2 + 10.01, 4 + 32.19)
= Beta (12.01, 36.19)
P(1) (π11 | θ, X) =
Beta (4 + 40.99, 2 + 16.81)
= Beta (44.99, 18.81)
P(1) (π20 | θ, X) =
Beta (2 + 15.05, 4 + 27.15)
= Beta (17.05, 31.15)
P(1) (π21 | θ, X) =
Beta (4 + 46.95, 2 + 10.85)
= Beta (50.95, 12.85)
P(1) (π30 | θ, X) =Dirch (3 + 15.45, 2 + 12.88, 1 + 13.87) =Dirch (18.45, 14.88, 14.87)
P(1) (π31 | θ, X) = Dirch (1 + 9.55, 2 + 11.12, 3 + 37.13) =Dirch (10.55, 13.13, 37.13)
The EM iteration 1 provisional estimates are the modes of these M-step
posteriors: λ(1) = .579, π(1)
10 = .264, π(1)
11 = .690, π(1)
20 = .373, π(1)
21 = .786,

304
9 Learning in Models with Fixed Structure
Table 9.5 E-step probabilities for iterations 1, by response pattern
Pattern Count p (x | θ = 0) p (x | θ = 1) p (x) P (θ = 0 | x) P (θ = 1 | x)
000
6
0.275
0.009
0.142
0.968
0.032
001
9
0.218
0.014
0.116
0.941
0.059
002
5
0.147
0.017
0.082
0.895
0.105
010
8
0.069
0.037
0.053
0.652
0.348
011
6
0.054
0.054
0.054
0.500
0.500
012
15
0.037
0.069
0.053
0.348
0.652
100
6
0.069
0.037
0.053
0.652
0.348
101
2
0.054
0.054
0.054
0.500
0.500
102
10
0.037
0.069
0.053
0.348
0.652
110
5
0.017
0.147
0.082
0.105
0.895
111
7
0.014
0.218
0.116
0.059
0.941
112
21
0.009
0.275
0.142
0.032
0.968
Table 9.6 E-step expected response pattern counts
Proﬁciencies Observables
Expected count
θ
X1 X2 X3
Iteration 1 Iteration 10 Iteration 100
0
0
0
0
5.806
5.618
5.542
0
0
0
1
8.471
7.921
7.603
0
0
0
2
4.477
1.742
0.010
0
0
1
0
5.212
6.210
6.118
0
0
1
1
3.000
3.803
3.562
0
0
1
2
5.227
1.679
0.008
0
1
0
0
3.909
4.621
4.564
0
1
0
1
1.000
1.251
1.177
0
1
0
2
3.485
1.085
0.005
0
1
1
0
0.523
2.206
2.302
0
1
1
1
0.412
1.979
1.940
0
1
1
2
0.679
0.586
0.003
1
0
0
0
0.194
0.382
0.458
1
0
0
1
0.529
1.079
1.397
1
0
0
2
0.523
3.258
4.990
1
0
1
0
2.788
1.790
1.882
1
0
1
1
3.000
2.197
2.438
1
0
1
2
9.773
13.321
14.992
1
1
0
0
2.091
1.379
1.436
1
1
0
1
1.000
0.749
0.823
1
1
0
2
6.515
8.915
9.995
1
1
1
0
4.477
2.794
2.698
1
1
1
1
6.588
5.021
5.060
1
1
1
2
20.321
20.414
20.997

9.5 Markov Chain Monte Carlo Estimation
305
Table 9.7 E-step iteration 1 expectations of suﬃcient statistics
Nonmasters
¯r(0)
0
¯r(0)
100
¯r(0)
101
¯r(0)
200
¯r(0)
201
¯r(0)
300
¯r(0)
301
¯r(0)
302
42.2 32.19 10.01 27.15 15.05 15.45 12.88 13.87
Masters
¯r(0)
1
¯r(0)
110
¯r(0)
111
¯r(0)
210
¯r(0)
211
¯r(0)
310
¯r(0)
311
¯r(0)
312
57.8 16.81 40.99 10.85 46.95 9.55 11.12 37.13
π(1)
30 = (.405, .328, .267), and π(1)
31 = (.145, .185, .670). These are maximum
a posteriori (MAP), or posterior mode, Bayesian estimates in each M-step,
maximizing the M-step provisional posterior. Over repeated iterations, the
EM algorithm will converge to the MAP estimate with respect to the marginal
posterior of the structural parameters.
EM cycles continue until convergence. Table 9.6 shows the E-step expected
counts of response patterns for masters and nonmasters at iterations 10
and 100. Note that expected counts for patterns in which nonmasters give
responses of 2 to task 3 are moving toward 0. Table 9.8 traces the progress
of the ﬁrst 10 iterations, every 10th iteration afterward up to 100, then the
200th and 300th. Convergence to three decimal places is achieved by the 200th,
although because the EM algorithm converges only linearly it is prudent to
run additional cycles after the algorithm has apparently converged to makes
sure that it has truly converged and is not just moving slowly.
Note the .000 value for π302. It is not a hard zero, but close to it. It is
the counterpart of the nonmasters’ expected count for a response of 2 to task
3 converging to 0, as seen in Table 9.6. This is a visible result of the EM
algorithm’s use of posterior modes rather than posterior means. Section 9.5
will say more about this when we compare the EM results with MCMC results,
which provide posterior means. Posterior modes, posterior means, and MLEs
are the same asymptotically, but a sample of 100 is small enough to make a
diﬀerence. We will also see the impact of the missingness of θ on the posteriors
for λ and π.
9.5 Markov Chain Monte Carlo Estimation
MCMC estimation takes an alternative approach to Bayesian inference from
complex posterior distributions such as Eq. 9.2. Rather than analytically ﬁnd-
ing a maximum for modal inference, MCMC estimation takes samples from
specially constructed distributions that in the long run are equivalent to sam-
ples from the posterior. The mean of the sampled values converges to the
mean of the posterior, their standard deviation converges to the posterior’s,
and so on; the collection of sampled points is an empirical approximation of

306
9 Learning in Models with Fixed Structure
Table 9.8 Trace of EM parameter estimates
Iteration
λ
π10
π11
π20
π21
π300
π301
π302
π310
π311
π312
0 0.575 0.238 0.712 0.347 0.808 0.386 0.307 0.307 0.157 0.199 0.644
1 0.579 0.264 0.690 0.373 0.786 0.405 0.328 0.267 0.145 0.185 0.670
2 0.583 0.276 0.678 0.387 0.774 0.421 0.342 0.237 0.135 0.176 0.689
3 0.586 0.283 0.671 0.394 0.766 0.435 0.351 0.214 0.127 0.170 0.703
4 0.590 0.287 0.666 0.399 0.761 0.447 0.359 0.194 0.120 0.166 0.714
5 0.593 0.290 0.662 0.402 0.756 0.458 0.365 0.178 0.115 0.163 0.722
6 0.597 0.292 0.658 0.404 0.753 0.467 0.370 0.164 0.110 0.161 0.729
7 0.600 0.294 0.655 0.406 0.750 0.475 0.374 0.151 0.107 0.159 0.734
8 0.603 0.296 0.652 0.407 0.747 0.483 0.377 0.141 0.103 0.158 0.739
9 0.606 0.297 0.650 0.408 0.745 0.489 0.380 0.131 0.101 0.157 0.742
10 0.609 0.298 0.647 0.409 0.743 0.495 0.383 0.122 0.099 0.156 0.745
20 0.630 0.304 0.633 0.412 0.731 0.534 0.400 0.066 0.088 0.153 0.758
30 0.642 0.304 0.626 0.412 0.725 0.553 0.410 0.037 0.087 0.153 0.761
40 0.650 0.303 0.623 0.411 0.722 0.563 0.416 0.021 0.087 0.152 0.761
50 0.656 0.302 0.621 0.409 0.720 0.568 0.420 0.012 0.088 0.152 0.759
60 0.659 0.301 0.620 0.408 0.719 0.570 0.422 0.007 0.089 0.152 0.758
70 0.662 0.300 0.619 0.407 0.718 0.572 0.424 0.004 0.090 0.153 0.757
80 0.663 0.299 0.619 0.406 0.718 0.572 0.425 0.002 0.091 0.153 0.756
90 0.664 0.299 0.619 0.406 0.718 0.573 0.426 0.001 0.092 0.153 0.756
100 0.665 0.298 0.618 0.405 0.718 0.573 0.426 0.001 0.092 0.153 0.755
200 0.667 0.297 0.618 0.404 0.717 0.573 0.427 0.000 0.093 0.153 0.754
300 0.667 0.297 0.618 0.404 0.717 0.573 0.427 0.000 0.093 0.153 0.754
the posterior, as accurate as we like by just making the chain of draws long
enough.
It is important to note that increasing the number of (MCMC) samples
from the posterior does not add information about the unknown variables
per se. The posterior contains all the information to be had from the model,
the prior, and the observed data. Running indeﬁnitely many cycles reduces
approximation error due to the fact that the run output is a larger sample
from the posterior, while the posterior itself, and the uncertainty due to having
only a ﬁxed amount of observed data remains the same no matter how long
the chain is.
The “Markov6 chain” part of MCMC refers to the property of drawing
from probability distributions in a sequence, where each draw leads to a next
distribution to draw from and the procedure works from one step to the next
with “no memory” of previous steps that led it to the present state.
6 This is a diﬀerent context from which the term “Markov” was introduced in
Chap. 4. In graphs the “Markov” property is that two variables are conditionally
independent given their separator. In time series, the “present” is the separator
which renders the past independent of the future.

9.5 Markov Chain Monte Carlo Estimation
307
“Monte Carlo” integration is a trick often employed to calculate a complex
integral whose integrand can be written as a probability distribution times a
function of the random variable. This is the expected value of the function
(under the probability distribution), and the mean of a random sample of func-
tion values is approximately equal to the value of integral. The approximation
gets better as the number of values sampled from the probability distribution
increases. Thus, if we can sample from the full joint posterior, Monte Carlo
integration is capable of calculating features such as means, variances, and
percentile points, as well as smoothed approximations of the distribution.
Putting these ideas together, MCMC uses a Markov Chain (a time series)
to sample from the joint posterior. This is not an independent sample because
each sampled value depends on the one in the previous time point. The lack
of independence does not bias the results if the series is long enough—the
draws for a given variable are usually correlated with the previous ones, but
the marginal distribution is right. It does mean though that a larger number
of draws (i.e., longer chains) is necessary for a given level of accuracy. MCMC
estimation can take a great deal of computation, especially with large models
and large data sets. Much of the art of MCMC estimation comes from trading-
oﬀdependency between number of iterations and ease of drawing samples
taken from the requisite probability distributions.
The software package BUGS (Bayesian analysis using Gibbs sampling
Thomas et al. 1992, see Appendix A) has played a special role in the popu-
larity of MCMC estimation. It takes a description of a model and develops a
MCMC sampler for that model, so the analyst does not need to write one-oﬀ
software for each problem. The successor package WinBUGS (Lunn et al. 2000)
adds a graphical user interface and some of the types of graphics used below.
We used WinBUGS in initial work on the examples in this chapter.7 BUGS
has had a huge impact in making Bayesian methods more widely available.
A full treatment of MCMC methods is beyond the current presentation.
A number of excellent resources are now available, however; the reader is
referred to Gelman et al. (2013a) and Gilks et al. (1996) for a start. This
section describes the approach and basic properties of MCMC estimation,
with a focus on popular variants called Gibbs sampling (Sect. 9.5.1) and the
Metropolis–Hastings algorithm (Sect. 9.5.3). It then continues the running
latent class example to show how Gibbs sampling can be applied to estimate
the parameters in Bayesian networks.
7 For the ﬁnal version, we used our own MCMC software StatShop (Almond, Yan,
et al. 2006c). We tested that software by comparing the posteriors it gener-
ated to those generated by WinBUGS. The graphics were prepared with CODA
(Best et al. 1996), a package of R (R Development Core Team 2007) functions for
doing MCMC output analysis originally developed for use with BUGS output.

308
9 Learning in Models with Fixed Structure
9.5.1 Gibbs Sampling
The Gibbs sampler is a variety of MCMC estimation with a particularly simple
form (Geman and Geman 1984). Each cycle consists of a set of unidimensional
draws, one for each unknown variable or parameter, from what is called a “full
conditional” distribution. In other words, a draw is taken from a distribution
for that variable conditional on a value for every other variable and parameter
in the problem—actual values for variables that have been observed, such as
observable variables or covariates, and, for unobserved variables, the previous
draw for that variable. Iteration t + 1 in the general measurement model
(Eq. 9.1) starts with values for all of the variables, say

θt, βt, λt, ξt
. A
value is then drawn from each of the following full conditional distributions
in turn:
For each person, i, draw θt+1
i
from p

θi | θt+1
<i , θt
>i, βt, λt, ξt, X

.
For each item, j, draw βt+1
j
from p

βj | θt+1, βt+1
<j , βt
>j, λt, ξt, X

.
Draw elements λt+1
k
from p

λk | θt+1, βt+1, λt+1
<k , λt
>k, ξt, X

.
Draw elements of ξt+1
ℓ
from p

ξℓ| θt+1, βt+1, λt+1, ξt+1
<ℓ, ξt
>ℓ, X

.
(9.20)
Run cycles like these long enough, and under broadly satisﬁed conditions
taking a draw for a given parameter follows the same distribution as a draw
from its marginal posterior distribution, given the observed data. What is
more, the set of draws for all parameters in a given cycle follows the same
distribution as a multivariate draw from their joint posterior. The diﬃcult
problem of characterizing a complex posterior with many parameters has been
reduced to a series of draws from unidimensional distributions that are usually
much easier to work with. The key condition for the Gibbs sampler to work is
reversibility, which means that it is possible for the chain to move from any
point in the parameter space to any other point, so it cannot become “stuck”
in some region of the space.8
Conditional independence relationships in the joint distribution can make
Eq. 9.20 easier to work with. In particular, with a Bayes net, we can often use
the conditional independence statements implicit in the network to simplify
calculations. In particular, in drawing a sample for a variable θi, we usually
only need to consider the neighbors of θi in the graph. Furthermore, if the
global independence condition holds, we only need to consider the parameters
for one conditional probability table at a time. The local independence con-
dition brings about further simpliﬁcations. Thus Gibbs sampling is a natural
method to use with Bayes nets. It can exploit the conditional independence
properties of the net to make the calculations more eﬃcient.
8 For example, consider a model containing variables X1, . . . , XN that are assumed
to be normally distributed with unknown mean μX and variance σX. Suppose
the sampler moves to a state with σX = 0. Then all Xi = μX for the next cycle,
which will in turn force σX = 0 in the following cycle as well, and ever after. To
avoid such problems, the prior for a variance parameter is usually restricted so
that the probability of σX = 0 is zero.

9.5 Markov Chain Monte Carlo Estimation
309
9.5.2 Properties of MCMC Estimation
Convergence of a Gibbs sampler is not to a point (a single value for each
parameter and unknown variable), as it is in the EM solutions discussed in
the previous section, but rather to a stationary distribution. That is, the joint
distribution of the draws at t through t+ℓis the same as that of t+m through
t + ℓ+ m for any m > 0. If a Markov Chain is in its stationary distribution at
one point in time, then it will still be in the stationary distribution at every
future time point.
The key result from Geman and Geman (1984) is that the stationary distri-
bution of the Gibbs sampler is the joint posterior of the unknown parameters
and variables. In the limit, draws from Gibbs cycles move around the posterior
in proportion to its density. Of course being possible to move from any part
of the parameter space to another in a given cycle does not mean it is prob-
able. Although only positive probability is required for long run behavior to
be satisﬁed, there is no guarantee that any ﬁnite portion of a chain covers the
support of the posterior (the space over which it is deﬁned) representatively.
A practical challenge in any application of Gibbs sampling, then, is how long
the chain should be.
Draws in cycle t + 1 depend on values in cycle t, but given them, not
on previous cycles. This conditional independence is the Markov property of
“no memory.” In general, however, values for a given parameter in a chain
do tend to be autocorrelated from cycle to cycle, so there is less information
about a parameter’s posterior in M successive draws from the Markov chain
than there would be from M independent draws from the same posterior.
Under regularity conditions, dependence on starting values is ‘forgotten’ after
a suﬃciently long run. After convergence to a stationary distribution, the
empirical distribution and summary statistics of a long series of draws estimate
the target posterior distribution and its summary characteristics.
The autocorrelations of a time series are the correlations of a point in the
series x(t) with previous points in the series x(t−ℓ). The diﬀerence between
the two points is called the lag of the autocorrelation. High autocorrelation
means the Markov chain is moving more slowly around the parameter space.
This is called slow mixing or poor mixing. It can be caused by lack of infor-
mation about the parameter in the data, and by high correlations with other
parameters; sometimes reparameterizing the model can help. A slow mixing
chain takes longer to visit all regions of the posterior, so it needs to be run
longer than a series that is mixing well. Slow mixing can be a sign of other
trouble as well, so it is worth checking to make sure the chain is visiting all
parts of the posterior. If a Gibbs sampler is started from initial points that
are in a low density region of the posterior, it can take many cycles before
stationarity is reached. Draws from early cycles are discarded for the purpose
of approximating the posterior. This is called “burn-in.” We will return to
this idea again shortly.

310
9 Learning in Models with Fixed Structure
Figure 9.9 shows a trace plot (after burn-in) for two chains along with
the autocorrelations plotted against the lag. The ﬁrst (an intercept parameter
in a DiBello–Samejima model) is mixing well, while the second parameter
(a slope parameter) is not. Diﬀerent variables in the same problem can have
good and poor mixing. A good trace plot should look like “white noise” that
is bouncing around with no discernible pattern. If we run the second series
long enough, then compression of the time scale will make the second plot
look like white noise. That is an indication that we have run the series longer
enough to compensate for slow mixing.
The autocorrelation plots for the same series are also plotted in Fig. 9.9. We
can see that the autocorrelation for the intercept parameter drops to a fairly
low ﬁgure by about lag 5, while the autocorrelation for the slope parameter
stays high even to lags of 20 and higher. When the autocorrelation for a given
parameter is high, more cycles are required for burn-in and more cycles are
required for a given number of draws to provide a given degree of accuracy for
estimating the posterior. A measure of the latter eﬀect is the eﬀective sample
size of the Markov chain—this is the sample size of a simple random sample
from the posterior which would give a similar accuracy. A correction factor
from time series analysis gives a reasonable approximation for the impact of
autocorrelation ρ on the estimate of the posterior mean of a variable from a
chain of length N:
Eﬀective sample size ≈N

1 −ρ
1 + ρ

.
(9.21)
In the series shown in Fig. 9.9, the intercept parameter has an eﬀective sam-
ple size of 1161, which is probably adequate for estimating posterior means,
standard deviations, and 95% credibility intervals. The slope parameter has
an eﬀective sample size of 80, which indicates more samples are needed. In
both cases, the full sample consists of 15,000 draws (5000 each from three
independent chains, i.e., three chains started from diﬀerent initial values).
To check convergence, Gelman and Rubin (1992) recommend running mul-
tiple chains from diﬀerent, widely dispersed, initial values, and monitoring
whether they come to approximate draws from the same stationary distribu-
tion. Figure 9.10 shows the ﬁrst 2000 draws from the slope parameter from
Fig. 9.9. The three chains were started from three diﬀerent starting points.
There is evidence that the chains have converged at about the 1000th cycle.
The values before this point should not be used to approximate the charac-
teristics of the posterior distribution, and are discarded as “burn-in” cycles.
Brooks and Gelman (1998) discuss ANOVA-like indices for a given variable
that compare variance between (B) and variance within (W) n chains. One
intuitive simple index is
R = B + W
W
.

9.5 Markov Chain Monte Carlo Estimation
311
1000
3000
5000
0.5
0.9
Iterations
Trace of S051Intercept
0
5
10
20
30
−1.0
1.0
Autocorrelation for  S051Intercept
Lag
Autocorrelation
1000
3000
5000
0.3
0.7
Iterations
Trace of LS051EvaSlope
0
5
10
20
30
−1.0
1.0
Autocorrelation for  LS051EvaSlope
Lag
Autocorrelation
Fig. 9.9 Examples of poor mixing and good mixing
The graph shows trace and autocorrelation plots for two parameters (an intercept
or diﬃculty parameter and log slope or discrimination parameter) from a DiBello-
Samejima model for Observable S051. The ﬁrst 1000 samples (not shown in the
plot) were discarded as “burn-in.” The intercept parameter is mixing well with the
autocorrelation damping down at higher lags, while the discrimination parameter is
mixing slowly with the autocorrelation remaining high even at longer lags. Reprinted
with permission from ETS.
0
500
1000
1500
2000
−0.4
0.2
0.6
Iterations
Fig. 9.10 Convergence of three Markov chains
Before cycle 1000, the three chains do not overlap, but after this time they substan-
tially overlap. The chain has almost certainly not reached its stationary distribution
before cycle 1000. Reprinted with permission from ETS.
One looks at values of R as computed in windows (short segments of the
chain) of length 50. If the chains have converged to the stationary distribu-
tion, R is near 1. But if the chains have not converged, R exceeds 1 as B
tends to overestimate the variance of the stationary distribution because the
chains were started from overdispersed initial values, while W tends to under-
estimate the variance of the stationary distribution because the chains have
not yet covered the range of the parameter space. Figure 9.11 is the trace
of an adjusted version of R, the Brooks–Gelman–Rubin (BGR) index for the

312
9 Learning in Models with Fixed Structure
same series shown in Fig. 9.10. R has settled to a value near 1 by cycle 1000
suggesting that this is a good choice for burn-in (The WinBUGS User Guide
suggests looking more closely at additional evidence, such as visual inspection
of trace plots and densities from diﬀerent chains, when values exceed 1.05;
Brooks and Gelman (1998) mention values like 1.1 and 1.2 as being satisfac-
tory, but emphasize looking at other sources of evidence rather than relying
on any single criterion). Sinharay (2003) reviews convergence diagnostics for
MCMC, including multivariate versions of the BGR.
0
1000
3000
5000
1.0
2.5
4.0
R plot for LS051EvaSlope
last iteration in chain
shrink factor
median
97.5%
Fig. 9.11 Plot of the Brook–Gelman–Rubin R vs. cycle number
The R statistic is near 1 by iteration 1000 suggesting a burn-in of 1000. Reprinted
with permission from ETS.
There are many variations of MCMC that all enjoy the same long-run
properties but are more eﬃcient for diﬀerent speciﬁc problems. For example,
generalizations of the basic Gibbs sampler include the following: variables can
be sampled in diﬀerent orders in diﬀerent cycles. Not every variable must
be sampled in each cycle; it is suﬃcient that each variable will be sampled
inﬁnitely many times in some mix with all the others. (One might draw ten
values of the slope parameter in the example of Fig. 9.9 from the same full
conditional, for instance, to help compensate for its large autocorrelation com-
pared to that of the intercept.) Sampling can be carried out drawing from
conditionals of blocks of parameters rather than individual parameters (Patz
and Junker 1999b, for example, draw from the trivariate distributions of each
task’s item parameters log a, b, and logit c in the three-parameter logistic IRT
model).
9.5.3 The Metropolis–Hastings Algorithm
There are handy programs to sample from the beta, Dirichlet, normal, and
gamma distributions we have seen as forms for full conditionals so far. If
the full conditional of a given parameter is diﬃcult to sample from, one can

9.5 Markov Chain Monte Carlo Estimation
313
carry out approximations such as Metropolis or Metropolis–Hastings sampling
within Gibbs cycles. A Metropolis sampling approximation to a density p(·)
(Metropolis and Ulam 1949) requires that one be able to compute the density
of the target at any given point, say p(z). Samples are not drawn from p(·),
but from a proposal distribution q(·) that has the properties that one can both
compute it at any point z and it is easy to draw samples from. In Metropo-
lis sampling, the idea is to draw a value from the proposal distribution, and
either accept this draw as the next value in the chain for the parameter or to
stay with the previous value. The probability of accepting the draw (Eq. 9.22
or 9.23) is chosen to correct for the diﬀerence between q(·) and p(·). There-
fore, the stationary distribution of the Metropolis sampler is still the target
distribution.
Let p be the target distribution in this case, the full conditional of a given
parameter z in the full Bayesian model. Let z(t) be the value for this parameter
in the tth cycle. Denote the proposal distribution for cycle t + 1 by q(y|z(t)).
Note that the proposal distribution may depend on the previous value z(t).
WinBUGS, for example, uses normal proposal distributions when it cannot
sample directly from full conditionals, with the mean at the previous value.
Proposal distributions can be described as symmetric or nonsymmetric. A
symmetric proposal distribution (such as WinBUGS’s) has the property that
q(y|z(t)) = q(z(t)|y). In this case, the probability of accepting a proposal value
y drawn from q(y|z(t)) is given by
α

z(t), y

= min

1, p(y)
p(z(t))

.
(9.22)
In other words, the proposed value y can become z(t+1) in two ways. It is
accepted with certainty if the density of the target distribution p is higher
than that of the previous cycle’s estimate. If its density is lower, it may still
be accepted, with a probability equal to the ratio of the densities of the target
distribution at the proposed value and the previous cycle’s value. Figures 9.12
and 9.13 illustrate these two situations.
Metropolis–Hastings sampling pertains to proposal distributions that are
not symmetric (Hastings 1970). The probability of acceptance is now
α

z(t), y

= min

1, p(y)q(z(t)|y)
p(z(t))q(y|z(t))

.
(9.23)
Metropolis–Hastings sampling simpliﬁes to Metropolis sampling if q(·, ·) is
symmetric.
Remarkably, Metropolis and Metropolis–Hastings sampling work for prac-
tically any proposal distribution that is positive over the support of the target
distribution. Diﬀerent choices may diﬀer considerably as to their eﬃciency,
however. If the proposal distribution is chosen to take random steps from the
current value, then the step size (variance) of the proposal distribution will
have a big impact on the eﬃciency of the chain. If the step size is too large,

314
9 Learning in Models with Fixed Structure
Fig. 9.12 A Metropolis step that will always be accepted
Reprinted from Almond et al. (2006a) with permission from ETS.
Fig. 9.13 A Metropolis step that might be rejected
Reprinted from Almond et al. (2006a) with permission from ETS.
then the Metropolis algorithm will reject frequently. This leads to slow mix-
ing, since the next value in the chain is too often the same as the previous
value. On the other hand, if the step size is too small, the chain will move
very slowly through the space because any new accepted value is usually close
to the previous step. This also results in slow mixing. Proposal distributions
that lead to 30–40% acceptance are most eﬃcient (Gelman et al. 1996). Win-
BUGS automatically tunes the proposal distribution (adjusting the variance
as it tracks the acceptance rate) during the burn-in cycles using Metropolis

9.6 MCMC Estimation in Bayes Nets in Assessment
315
sampling. For this reason, WinBUGS requires a longer burn-in for problems
in which it uses Metropolis sampling.9
Although the original Metropolis and Metropolis–Hastings algorithms
sampled from the joint distribution of all the unknown variables and parame-
ters, it is possible to combine this technique with Gibbs sampling. As in Gibbs
sampling, one samples from each variable or parameter in turn (or in a ran-
domly chosen order). If the full conditional distribution is convenient to work
with (e.g., the conditional independence properties of the Bayes net usually
make for relatively straightforward full conditionals), then a draw is taken
from the full conditional. If not, then a draw for that variable or parameter
is taken from a proposal distribution and it is then accepted/rejected with
a Metropolis (Eq. 9.22) or Metropolis–Hastings (Eq. 9.23) rule. WinBUGS
uses this strategy, automatically deciding whether to use Gibbs sampling,
Metropolis sampling, or some other algorithm, such as the slice sampler (Neal
2003), on a parameter-by-parameter, variable-by-variable basis.
9.6 MCMC Estimation in Bayes Nets in Assessment
In typical operational assessment programs, the latent proﬁciency variables
(θ) of students are of persistent interest. Particular tasks and their parameters
(π) are moved into and out of the system over time, whether for security
purposes, to extend the range of ways to collect evidence, or simply to provide
variety for students. We would be perfectly well satisﬁed if a student were
assessed using this set of tasks or that one, as long as they provide evidence
with respect to the same proﬁciencies. The latent scale is established in a
start-up data collection in which all examinee, task, and structural parameters
are estimated. This is called the initial calibration. The resulting structural
parameters can be used to estimate the θs of new examinees as they are
administered tasks for which good estimates of πs are available.
As the testing program continues, some tasks are retired or rotated out of
operational use. New tasks are created to provide information about the same
θs. We assume that the new items are created in accordance with existing
task models and conformable evidence models. In these cases, we estimate
the parameters for the evidence models of these new tasks. To accomplish
this, new tasks are administered to examinees along with already-calibrated
or old tasks, and the πs of the new tasks are estimated with the πs of the old
tasks setting the scale. This is called online calibration (Wainer et al. 2000).
This section describes MCMC estimation using Gibbs sampling for both
initial calibration and online calibration in Bayes nets. Additional detail can
be found in Hrycej (1990), Mislevy, Almond, et al. (1999a), and York (1992).
9 In fact, the slow mixing series shown in Fig. 9.9 was produced by artiﬁcially
shortening the time available for WinBUGS to tune the proposal distribution.

316
9 Learning in Models with Fixed Structure
The approach will be illustrated with the latent class example introduced
earlier in the chapter.
9.6.1 Initial Calibration
The form of Gibbs sampling for the general measurement model (Eq. 9.20)
specializes in the case of Bayes nets as follows:10
For each person, i, draw θ(t+1)
i
from p

θi | θ(t+1)
<i
, θ(t)
>i, π(t), λ(t), X

.
For each task, j, draw π(t+1)
j
from p

πj | θ(t+1), π(t+1)
<j
, π(t)
>j, λ(t), X

.
Draw class proportions λ(t+1)
k
from p

λk | θ(t+1), π(t+1), λ(t+1)
<k
, λ(t)
>k, X

.
(9.24)
The ﬁrst line of Eq. 9.24 is drawing values for proﬁciency variables from poste-
rior distributions for them, student by student, given their response patterns
and the previous cycle’s draws for conditional response probabilities π and λ.
Note that given the model parameters, student proﬁciencies are independent,
so p

θi | θ(t+1)
<i
, θ(t)
>i, π(t), λ(t), X

= p

θi | π(t), λ(t), Xi

and each student
can be considered in isolation. Sampling from this distribution for each stu-
dent provides an augmented data set containing the actual response data X ,
and for each examinee a provisional value of θ.
The posterior distributions of the πs and λs can be obtained as they were
in the complete data solution discussed in Sect. 9.2.2. In particular, the local
independence property of the assessment—that the observable outcomes for
given tasks are independent given the proﬁciency variables—often simpliﬁes
the required conditional probabilities in Eq. 9.24. For example, the second line
of Eq. 9.24 describes how to draw the task speciﬁc link model parameters.
If in addition the global parameter dependence model holds (in particular,
we are not using a hierarchical model for the task parameters), then the
parameters from each task are rendered independent by conditioning on the
proﬁciency variables. Thus, we have: p

πj | θ(t+1), π(t+1)
<j
, π(t)
>j, λ(t), X

=
p

πj | θ(t+1), λ(t), X

. This allows the sampling to take place one task at a
time. (In the case of hierarchical models, the additional parameters ξ usually
restore the conditional dependence again supporting working one task at a
time.)
When sampling the parameters of the Proﬁciency Model, the third line
of Eq. 9.24, again the independence conditions simplify the required work.
A direct consequence of the global parameter independence condition is that
10 If the πs are given by some parameterization such as DiBello–Samejima models,
then the parameters β of those models also appear in the MCMC chains, using
exactly the same principles: e.g., drawing from their full conditionals in turn with
the other variables in the full model.

9.6 MCMC Estimation in Bayes Nets in Assessment
317
λ is independent of π given θ. Thus the third line of Eq. 9.24 simpliﬁes
to p

λk | θ(t+1), π(t+1), λ(t+1)
<k
, λ(t)
>k, X

= p

λk | θ(t+1), λ(t+1)
<k
, λ(t)
>k

, and
often the dependence on λ values from other distribution vanishes as well.
Thus, a complete Gibbs cycle consists of three phases:
1. Draw proﬁciency variable values, θ, for each student.
2. Draw link model (evidence model) parameters, πj for each task.
3. Draw proﬁciency model parameters, λ.
(If there are any missing task responses, these can be ﬁlled in by another Gibbs
sampling step between Step 1 and Step 2.) When all values for all unknown
variables and parameters have been drawn, a new cycle starts drawing new
proﬁciency variables. This process continues until a Monte Carlo sample of
suﬃcient size has been drawn. If multiple chains are run, then the sample is
the pooled data from all chains after burn-in. We will take a closer look at
just how this works in the running latent class example.
Example 9.3 (Initial Calibration Using Gibbs Sampling, Example 9.1
Continued). The full conditionals for individual examinee’s class member-
ships shown in the ﬁrst line of Eq. 9.24 take simpler forms because each
examinee’s θ is conditionally independent of the other examinees’ responses
and θs:
p

θi | θ(t+1)
<i
, θ(t)
>i, π(t), λ(t), X

= p

θi | π(t), λ(t), xi

.
(9.25)
In particular, conditional probabilities of examinees’ class memberships are
calculated just as they were under the EM algorithm in Eqs. 9.15–9.19. In the
EM algorithm, however, one accumulates over examinees the posterior proba-
bilities of being a master, say, in order to obtain the expected count of masters.
In Gibbs sampling, one draws a value—0 or 1, nonmaster or master—from
the same posterior distribution, examinee by examinee. If there were a 100
examinees with identical response patterns that gave a .70 probability that
θ = 1 in a given Gibbs cycle, then for perhaps 60–80 of these examinees, a
draw of 1 would be taken as their θ in the next cycle and for the rest a draw
of 0 would be passed on.
In each cycle of the EM algorithm, expected counts of masters and non-
masters, and of item response counts among masters and nonmasters, were
substituted into the expressions for the posterior distributions of the struc-
tural parameters λ and π. In a Gibbs cycle, counts based on the draws θ(t)
are instead used to obtain facsimiles of the same posteriors. Cycle t counts
of masters and nonmasters are counts of corresponding draws from Eq. 9.25
examinee by examinee:
r(t)
1
=

i
θ(t)
i
and
r(t)
0
=

i

1 −θ(t)
i

.

318
9 Learning in Models with Fixed Structure
The cycle t counts of correct responses to item 1 from masters and nonmas-
ters are calculated in a similar manner, with the summation over only those
examinees who answered correctly:
r(t)
111 =

i:xi1=1
θ(t)
i
and
r(t)
101 =

i:xi1=1

1 −θ(t)
i

.
Note that in these sums, a given examinee’s response is always the same from
one cycle to the next, but whether the examinee is accumulated along with
masters or nonmasters will generally vary from cycle to cycle. In the long run,
the examinee will be accumulated with masters in proportion to the correct
marginal posterior probability. Table 9.9 gives the facsimile of the complete
data counts based on draws for θ at every 1000 MCMC cycles from 1000 to
6000.
Table 9.9 MCMC cycle response pattern counts
Proﬁciencies Observables
Expected count at cycle
θ
X1 X2 X3
1000 2000 3000 4000 5000 6000
0
0
0
0
6
5
6
6
6
5
0
0
0
1
9
8
7
6
6
8
0
0
0
2
1
0
2
0
5
1
0
0
1
0
8
6
8
6
7
7
0
0
1
1
3
4
1
4
3
5
0
0
1
2
1
0
2
0
5
2
0
1
0
0
6
2
5
2
4
2
0
1
0
1
1
1
1
1
1
0
0
1
0
2
2
0
0
0
7
1
0
1
1
0
5
1
5
1
3
2
0
1
1
1
2
1
0
0
0
0
0
1
1
2
0
0
1
0
0
0
1
0
0
0
0
1
0
0
0
1
1
0
0
1
0
1
2
3
3
1
1
0
0
2
4
5
3
5
0
4
1
0
1
0
0
2
0
2
1
1
1
0
1
1
3
2
5
2
3
1
1
0
1
2
14
15
13
15
10
13
1
1
0
0
0
4
1
4
2
4
1
1
0
1
1
1
1
1
1
2
1
1
0
2
8
10
10
10
3
9
1
1
1
0
0
4
0
4
2
3
1
1
1
1
5
6
7
7
7
7
1
1
1
2
21
21
20
21
21
21

9.6 MCMC Estimation in Bayes Nets in Assessment
319
The cycle t full conditional for the population proportion of masters, λ, is
a facsimile of the compete data posterior. Using the same Beta(3, 3) prior for
λ yields
p

λ | θ(t), π(t), X

= p

λ | θ(t)
= Beta

3 + r(t)
0 , 3 + r(t)
1

.
(9.26)
A draw from Eq. 9.26 will be the value for cycle t, λ(t). This draw is used
in two ways. It is used as one data point in the empirical approximation of
the posterior of λ, and is used in turn to compute full conditionals for the
proﬁciency variables, θi, in the next cycle.
Similarly, for the conditional probabilities of correct response for masters
and nonmasters for task 1, using again the same prior distributions as in the
EM solution,
p

π111 | θ(t), λ(t), π(t), X

= p

π111 | θ(t), X

= Beta

4 + r(t)
110, 2 + r(t)
111

and
p

π101 | θ(t), λ(t), π(t), X

= p

π101 | θ(t), X

= Beta

2 + r(t)
100, 4 + r(t)
101

.
Values are drawn for each of the πs as π(t+1)s.
Because the model only has hyper-Dirichlet distributions, the Gibbs sam-
pler works with this model (more complex parameterizations of the conditional
probability tables often require a Metropolis algorithm). A typical procedure
is to run three chains of length 6000 each (planning on discarding the ﬁrst 1000
observations from each chain) starting from three diﬀerent starting points: one
at the prior median or mean, one in the upper tail of the prior, and one in
the lower tail of the prior. The choice of starting points is arbitrary and three
sets of random draws could be used instead (or three sections of a very long
run could be compared).
The ﬁrst task is to assess whether or not these 18,000 cycles represent
an adequate draw from the posterior, and how many of them should be dis-
carded as burn-in. Taking the initial 1000 cycles from each chain as burn-in
we calculate the Gelman–Rubin R for each parameter in the model. As these
parameters are all probabilities, we can apply a logistic transformation to
make them more “normal” before calculating the Gelman-Rubin R values.
The maximum value across all parameters is 1.09, so this looks fairly settled.
The maximum autocorrelation at lag 5 is 0.7, which is pretty high, but the
smallest eﬀective sample size is 444, so this sample is reasonable. If the sample
were not adequate, one recourse would be to run the chains out for a longer
time. However, often running longer only helps a little bit, and alternative
parameterizations of the model should be considered.

320
9 Learning in Models with Fixed Structure
Table 9.10 shows the values of parameters from cycles at intervals of 1000
from the ﬁrst chains. The table also shows some of the summary statistics
for the combined data set provided by the coda package (Best et al. 1996).
WinBUGS provides similar summary statistics. The mean, standard devi-
ation, and 2.5- and 97.5-percentiles are just the corresponding statistics of
the data set combining draws from all three chains. The generating values,
the complete data solution, and the EM estimates are given for comparison.
Table 9.10 also shows diﬀerent estimates of the standard error of the mean
for this distribution. The “na¨ıve SE” shows what the standard error would be
if all of the samples were independent, in this case if we had a pure Monte
Carlo sample of size N = 10, 000. The “time-series SE” corrects the standard
error for the autocorrelation of the Markov chain. Time-series SE would be
equivalent to the na¨ıve SE if the draws from the posterior were independent,
but the greater the autocorrelation within a chain, the more time-series SE
exceeds the na¨ıve SE (Heidelberger and Welch 1981; Best et al. 1996).
Table 9.10 MCMC parameter draws from intervals of 1000 and summary statistics
Cycle
λ
π10
π11
π20
π21
π300
π301
π302
π310
π311
π312
1000
0.58
0.39
0.62
0.52
0.73
0.59
0.32
0.09
0.02
0.22
0.77
2000
0.74
0.16
0.64
0.43
0.63
0.47
0.51
0.01
0.20
0.14
0.66
3000
0.51
0.27
0.70
0.54
0.71
0.33
0.35
0.32
0.03
0.22
0.75
4000
0.61
0.15
0.54
0.19
0.63
0.60
0.37
0.02
0.15
0.16
0.68
5000
0.55
0.40
0.77
0.35
0.87
0.46
0.16
0.38
0.12
0.37
0.51
6000
0.65
0.15
0.58
0.53
0.69
0.47
0.41
0.11
0.20
0.21
0.59
Mean
0.633 0.308 0.629 0.399 0.724 0.462 0.377 0.160 0.136 0.177 0.687
SD
0.123 0.110 0.084 0.123 0.073 0.122 0.117 0.112 0.073 0.067 0.092
Naive SE
0.001 0.001 0.001 0.001 0.001 0.001 0.001 0.001 0.001 0.001 0.001
Time-series SE
0.005 0.003 0.002 0.003 0.002 0.003 0.003 0.004 0.002 0.001 0.003
% “missing”
0.95
0.85
0.87
0.87
0.79
0.85
0.84
0.91
0.91
0.79
0.89
2.5 %
0.37
0.09
0.48
0.14
0.58
0.24
0.16
0.01
0.01
0.05
0.52
50 %
0.64
0.31
0.62
0.41
0.72
0.46
0.37
0.14
0.13
0.17
0.68
97.5 %
0.85
0.52
0.82
0.62
0.87
0.72
0.62
0.41
0.28
0.32
0.87
EM MAP
0.667 0.297 0.618 0.404 0.717 0.573 0.427
0 0.093 0.153 0.754
Complete Bayes 0.745 0.067 0.670 0.200 0.756 0.433 0.367 0.200 0.195 0.207 0.598
Complete MLE 0.760
0 0.671 0.167 0.763 0.417 0.375 0.208 0.197 0.197 0.605
“True”
0.700 0.100 0.800 0.300 0.600 0.500 0.300 0.200 0.100 0.200 0.700
Figure 9.14 shows smoothed empirical approximations of the posteriors of
selected parameters. This density plot includes small tick marks along the bot-
tom of each graph showing the individual sample points. For most variables,
the posteriors are roughly normal. This accounts for the similarity between
the posterior means from MCMC and the Bayes modal estimates, or MAPs,
from the EM solution.

9.6 MCMC Estimation in Bayes Nets in Assessment
321
The outlier is π302, with a highly skewed posterior that peaks near zero.
The MAP is therefore near zero, although there is considerable probability
for values above zero. This fact is reﬂected in the posterior mean of .160, and
a 95 % posterior credibility interval of .01–.41. An advantage of the MCMC
solution is that the particular shape and spread of uncertainty about this
parameter is taken into account in the posteriors for the other other variables,
and in particular for the θs. No single point estimate can tell the whole story,
but if we were to use point estimates of the structural parameters from the
initial calibration in a Bayes net to make inferences about new students, the
posterior means (or, for that matter, posterior medians) are preferable to the
MAPs.
0.0
0.4
0.8
0
4
N = 15003   Bandwidth = 0.01771
Density of lambda
0.4
0.8
0
4
N = 15003   Bandwidth = 0.01125
Density of pi.11
0.0
0.4
0.8
0
4
N = 15003   Bandwidth = 0.01434
Density of pi.20
0.0
0.4
0.8
0
4
N = 15003   Bandwidth = 0.01995
Density of pi.300
0.0
0.4
0.8
0
4
N = 15003   Bandwidth = 0.01995
Density of pi.301
0.0
0.4
0.8
0
4
N = 15003   Bandwidth = 0.01418
Density of pi.302
Fig. 9.14 Posteriors for selected parameters
Reprinted with permission from ETS.
9.6.2 Online Calibration
At the beginning of an operational assessment program, one obtains responses
from a sample of examinees to estimate the parameters of the population of
examinees and a start-up set of tasks. The methods described in the preceding
section apply. At a later date one wants to calibrate new tasks into the item
pool. Let us use subscripts old and new to refer to data and parameters
from the initial calibration and data and parameters for the new items. The
inferential targets of the initial calibration were thus πold and λold, and the
relevant posterior distribution was p (πold, λold | Xold). In online calibration,

322
9 Learning in Models with Fixed Structure
a new sample of examinees is administered both some old items and new
items, with the resulting responses denoted Xnew. The inferential targets are
πnew and λnew (we do not need to assume that the distribution in the new
examinees is the same as in the old sample as long as we have some old items
in common).
Formally, the correct Bayesian model for online calibration is
p (πnew, πold, λnew, λold | Xnew, Xold) .
(9.27)
As a ﬁrst pass, one might try to ﬁx the parameters πold at the point estimate
from the initial calibration, to simplify the problem and reduce the number of
parameters to be estimated in Eq. 9.27. However, unless the initial calibration
is suﬃciently large, small errors in the estimation will accumulate and cause
the scale to drift over time (Mislevy et al. 1994; Mislevy et al. 1999a).
What we can do instead is to substitute pj(βj|ξ, xold) for pj(βj|ξ) in
Eq. 9.1. Then the output of the calibration will be consistent with the com-
bined data. There is only one catch: the posterior pj(βj|ξ, xold) does not
necessarily have a convenient function form (e.g., beta distribution). In such
cases, one can approximate the posterior with something with a convenient
functional form.
When the prior law takes the form of a beta distribution, a method of
moments approximation is convenient. To do this, one ﬁnds a beta distribution
with the same mean and variance as the posterior and uses that as the new
prior. For example, in Example 9.3, the posterior mean and standard deviation
for π11 are 0.629 and 0.084. Let p = a/(a + b) = E[π] and substitute this
into the formula for the variance of the beta distribution, we get Var(π) =
p(1 −p)/(a + b + 1). Solving this for a + b, we get n = a + b = 33.072; this is
the eﬀective sample size of the posterior. We can now get the beta parameter
a = np = 20.803 and b = n(1−p) = 12.270. Dirichlet parameter can be treated
like a collection of beta distributions and the resulting as can be summed to
produce a single eﬀective sample size for the entire law. Table 9.11 gives the
results for π31.
Table 9.11 Approximating Dirichlet priors from posterior means and standard
deviations for π31
π310
π311
π312
Mean
0.136
0.177
0.687
SD
0.073
0.067
0.092
Variance
0.0053
0.0045
0.0085
ak
2.863
5.567
16.766
 ak
25.196
25.196
25.196

9.6 MCMC Estimation in Bayes Nets in Assessment
323
Example 9.4 (Online Calibration Using Gibbs Sampling, Example 9.3
Continued). Consider a new sample of 200 (simulated) examinees who are
administered tasks 1 and 3 from the running example and a new dichotomous
item, task 4. The generating values of the old items 1 and 3 are as before:
π10 = .1, π11 = .8, π30 = (.5, .3, .2), and π31 = (.1, .2, .7). The new generating
parameters are π40 = .1 and π41 = .8 for item 4 and λnew = .6. The resulting
data are shown in Table 9.12.
Table 9.12 Response pattern counts for online calibration
Response
Item 1 Item 3 Item 4 Pattern count
0
0
0
22
0
0
1
17
0
1
0
12
0
1
1
18
0
2
0
11
0
2
1
29
1
0
0
4
1
0
1
6
1
1
0
4
1
1
1
19
1
2
0
12
1
2
1
46
As in the initial calibration, we let the prior distributions for π40 and π41 be
Beta(2, 4) and Beta(4, 2) respectively, reﬂecting a mild prior expectation that
masters are more likely to answer an item correctly, and nonmasters are more
likely to answer incorrectly. For the population proportion of masters, λnew,
we will use a slightly diﬀerent prior than in the initial calibration. Beta(3, 3)
was used there, reﬂecting a mild expectation that masters and nonmasters
would be equally likely. But the posterior mean of λold was .633 and the
posterior standard deviation was 0.123. Using the method of moments reveals
that a Beta(9.72, 5.64) distribution has the same mean and variance. We will
use this as a prior for λnew (if we thought the population might have shifted,
we could downweight the prior by multiplying the parameters by a constant
less than one). We apply similar calculations to come up with new beta and
Dirichlet prior distributions for π1 and π3.
The MCMC calibration then proceeds in a similar manner. Again we run
three chains of length 6000 and discard the ﬁrst 1000 observations from each
chain as burn-in. Again the Gelman–Rubin statistic and plot shows that the
chains have likely reached the stationary distribution by cycle 500, so that the
burn-in of 1000 is likely to be conservative.

324
9 Learning in Models with Fixed Structure
Table 9.13 Average parameter values from initial and Online calibrations
λ
π10
π11
π300
π301
π302
π310
π311
π312
π40
π41
Initial mean 0.633 0.308 0.629 0.462 0.377 0.160 0.136 0.177 0.687
–
–
Initial SD
0.123 0.110 0.084 0.122 0.117 0.112 0.073 0.067 0.092
–
–
Online mean 0.649 0.183 0.621 0.567 0.321 0.112 0.073 0.240 0.687 0.410 0.806
Online SD
0.060 0.054 0.047 0.078 0.067 0.064 0.032 0.043 0.050 0.080 0.044
“True”
0.700 0.100 0.800 0.500 0.300 0.200 0.100 0.200 0.700 0.100 0.800
Table 9.13 shows the posterior means from both the initial and subsequent
online calibration. For many of the variables that are common across the
initial and online calibrations, the online values are closer to the “true” values
which were used to simulate the data. This is not only because the online
sample is both larger in and of itself (200 as opposed to 100 simulees) but
also because the priors for the online calibration include information from the
initial calibration. Thus, as more data become available, we can improve the
estimation of the various parameters.
Even though the sample size was bigger than Example 9.3, the calibration
still does not exactly reproduce the simulation parameters. Two factors are
at work here. The ﬁrst factor is that 200 students is still a relatively small
sample. IRT calibrations with the two- and three-parameter logistic models
aim for 1000 examinees. The second and perhaps more important factor is
that 3 items is an extremely short test. Any individual’s proﬁciency is likely
to be estimated quite poorly and varies from chain to chain. Increasing the
test length and increasing the sample size are both likely to produce more
accurate estimates (although they may slow down the speed at which the
Markov chains mix).
9.7 Caution: MCMC and EM are Dangerous!
The EM algorithm (Dempster et al. 1977) was one of the earliest develop-
ments in the ﬁeld of Bayesian computation. Using EM, the posterior mode
and variance could be calculated for latent variable models of arbitrary com-
plexity. All that was needed was a complete Bayesian model. The arrival of the
Gibbs Sampler (Geman and Geman 1984) and the increasing availability of
cheap computer power brought about an explosion in the ﬁeld. Now Bayesian
techniques could be applied to a wide variety of problems (Gilks et al. 1996).
All one needed to do was to write down the full Bayesian model (prior as well
as likelihood), and some form of MCMC could approximate any statistic of
the posterior, not just the mode.
The BUGS program(Thomas et al. 1992) and its successors, WinBUGS
(Lunn et al. 2000), OpenBUGS (Lunn et al. 2009), and JAGS (Plummer 2012),
have made this new computational power easily accessible, without requiring a

9.7 Caution: MCMC and EM are Dangerous!
325
great deal of time to develop, code, and test algorithms for particular models.
With BUGS, the analyst only needs to specify the model in a language based
on the S statistical language (Chambers 2004). BUGS then ﬁgures out how
to set up a Gibbs sampler, and whether it can calculate the full conditional
distributions analytically or whether it needs the Metropolis algorithm. In the
latter case, it even automatically tunes the proposal distribution.
This means almost any Bayesian model can be ﬁt to any data set. There
is no requirement that the chosen model is sensible. If the data provide no
information about a parameter then the parameter’s prior and posterior law
will be nearly identical. The displays in WinBUGS are designed to help one
assess convergence, but they do not always help with the issue of whether or
not the model is appropriate.
For this reason, the BUGS manual (Spiegelhalter et al. 1995) bears the
warning, “Gibbs sampling is dangerous” on the ﬁrst page. The warning is
not so much about Gibbs sampling as it is to leaping into computation with-
out thinking about whether or not the model is appropriate for the data and
problem at hand. To that extent, the warning is equally applicable to blindly
applying the EM algorithm. Although both procedures will provide an answer
to the question, “What are the parameters of this model?” they do not nec-
essarily answer the question, “Is this a reasonable model?”
Fortunately, Bayesian statistics oﬀers an answer here as well. If we have a
full Bayesian model, that model makes a prediction about the data we might
see. If this model has a very low probability of generating a given data set, it
is an indication that the model may not be appropriate. We can also use this
idea to choose between two competing models, or search for the best possible
model. The next chapter explores model checking in some detail.
Exercises
9.1 (Stratiﬁed Sampling). Example 9.1 used a simple random sample of
100 students with the number of masters in the sample unknown in advance.
Suppose instead a stratiﬁed sample of 50 masters and 50 nonmasters was
used. How would the inference diﬀer, if it was known who the masters and
nonmasters are? How about if we do not know who is who, but we know there
are exactly 50 of each?
9.2 (Missing At Random). Classify the following situations as MCAR,
MAR, or neither:
1. A survey of high school seniors asks the school administrator to provide
grade point average and college entrance exam scores. College entrance
exam scores are missing for students who have not taken the test.
2. Same survey (mentioned in ﬁrst point) except now survey additionally
asks whether or not student has declared an intent to apply for college.

326
9 Learning in Models with Fixed Structure
3. To reduce the burden on the students ﬁlling out the survey, the back-
ground questions are divided into several sections, and each student is
assigned only some of the sections using a spiral pattern. Responses on
the unassigned section are missing.
4. Some students when asked their race decline to answer.
9.3 (Missing At Random and Item Responses). Item responses can be
missing for a variety of reasons. Classify the following situations concerning a
student’s missing response to a particular Task j as MCAR, MAR, or neither.
Hint: See (Mislevy 2015) or (Mislevy and Wu 1996).
1. John did not answer Task j because it was not on the test form he was
administered.
2. Diwakar did not answer Task j because there are linked harder and easier
test forms, intended for fourth and sixth grade students; Task j is an easy
item that only appears on the fourth grade form; and Diwakar is in sixth
grade, so he was administered the hard form.
3. Rodrigo took an adaptive test. He did well, the items he was administered
tended to be harder as he went along, and Task j was not selected to
administer because his responses suggested it was too easy to provide
much information about his proﬁciency.
4. Task j was near the end of the test, and Ting did not answer it because
she ran out of time.
5. Shahrukh looked at Task j and decided not to answer it because she
thought she would probably not do well in it.
6. Howard was instructed to examine four items and choose two of them to
answer. Task j was one of the four, and not one that Howard chose.
9.4 (Classical Test Theory). Consider the following simple model from
classical test theory. Let Ti be a student’s true score on a family of parallel tests
on which scores can range from 0–10. Ti characterizes Student i’s proﬁciency
in the domain, but cannot be observed directly. Instead, we observe noisy
scores on administrations of the parallel tests. Let Xij be Student i’s score on
Test j. Following classical test theory, let
Xij = Ti + ϵij
(9.28)

9.7 Caution: MCMC and EM are Dangerous!
327
where ϵi ∼N(0, σ2
ϵ) and Ti ∼N(μ, σ2
T ). The classical test theory index of
reliability is ρ =
σ2
T
σ2
T +σ2ϵ . The following is BUGS code for estimating μ, σ2
T ,
σ2
ϵ , and ρ from the responses of nine students to ﬁve parallel test forms:
model ctt {
for (i in 1:N) {
T[i] ~ dnorm(mu,tauT)
for (j in 1:ntest) {
x[i,j] ~ dnorm(T[i],taue);
}
}
mu ~ dnorm(0,.01)
tauT ~ dgamma(.5,1)
taue ~ dgamma(.5,1)
rho <- taue / (taue + tauT)
varT <- 1/tauT
varE <- 1 / taue
}
#inits
list(T = c(20,20,20,20,20,20,20,20,20))
list(T = c(-20,-20,-20,-20,-20,-20,-20,-20,-20))
#data
list(N=9,ntest=5,
x=structure(.Data=c(
2,
3,
2,
5,
3,
4,
3,
4,
3,
6,
6,
4,
3,
4,
3,
4,
7,
5,
4,
5,
7,
5,
4,
5,
4,
4,
5,
4,
7,
5,
6,
5,
6,
5,
8,
5,
6,
5,
8,
6,
7,
6,
7,
6,
9 ), .Dim=c(9,5)))
Note that in BUGS, the normal distribution is parameterized with the mean,
μ, and precision, τ = 1/σ2. The line varT <- 1/tauT produces draws for σ2
T .
Run the problem with this setup, and the two chains as initial values for the
T s. Monitor T, mu, varT, varE, and rho.
1. Run 500 MCMC cycles. There are overdispersed initial values for the T s.
Ask for Stats, history, density, and the BGR convergence diagnostics plot.
Does it look like burn-in cycles may be needed for this problem? Which
parameters seem to be more or less aﬀected by the overdispersed initial
values?
2. Run another 500 cycles, and calculate summary statistics and distribu-
tions for the parameters you monitored based on only the last 500 cycles

328
9 Learning in Models with Fixed Structure
(hint: beg = 501 on the sample monitor dialog box). Regarding estimates
for individual students: What are the posterior means for the T s (i.e.,
true scores) of each of the students? What are their maximum likelihood
estimates (hint: BUGS does not tell you this—you need to do a little
arithmetic). In what directions do they diﬀer, and why?
3. Look at the summary statistics for T[1]. What is the meaning of the
number in each column?
9.5 (Classical Test Theory and the Eﬀects of Diﬀerent Priors). Con-
sider the model and data from Exercise 9.4. Start with the original setup.
Run just one chain for each of the variations required. In each case, run 2000
cycles, and calculate statistics based on only the last 1000. Monitor mu, varT,
varE, T, and rho.
1. The original setup uses, as a prior distribution for mu, N(0, .01) (using
the BUGS convention with the precision as the shape parameter). Run
the same problem, except with N(0, 1) as the prior for mu, and monitor
the results. Focusing on posterior means and standard deviations of the
parameters listed above, which ones change? How much and why?
2. Repeat the run, except with N(0, 10) as the prior for mu. Again focusing on
posterior means and standard deviations of the parameters listed above,
which ones change? How much and why?
3. The original setup uses, as a prior distribution for both tauT and taue,
Gamma(.5, 1). Run the same problem, except with Gamma(.05, .10) as the
prior for tauT, and monitor the results. Focusing on posterior means and
standard deviations of the parameters listed above, which ones change?
How much and why?
4. Repeat the run, except with Gamma(50, 100) as the prior for tau. Again
focusing on posterior means and standard deviations of the parameters
listed above, which ones change? How much and why?
9.6 (Slow Mixing). A researcher runs a small pilot MCMC chain on a par-
ticular problem and gets a trace graph for one parameter like Fig. 9.9. The
researcher shows it to two colleagues. The ﬁrst says that the problem is the
proposal distribution and suggests that the researcher should try to ﬁnd for
new proposal distributions. The second says that if the researcher just runs
the chain for ten times longer than originally planned, then the trace plot
will look like “white noise” and the MCMC sample will be adequate. The
second colleague further suggests that a long weekend is coming up and the
lab computers will be idle anyway. Which advice should the researcher take?
Why?
9.7 (MAP and MCMC Mean). In Table 9.10, the MCMC mean frequently
appears to be closer to the MAP from the EM algorithm than it does to the
“true” value from the data generation. Why is this seen?

9.7 Caution: MCMC and EM are Dangerous!
329
9.8 (Latent Class). The following response vectors come from a simple
latent class model with two classes and ten dichotomously scored items.
1111111010 1110111011 0000010000 1111001011 0010001000
0000101000 0000010000 0010100000 0000000000 0000001000
0011100111 0100000000 0000000000 0000000000 0010000000
1100000000 1111011111 0010100000 0010000010 0111111011
0010000000 0000000000 1011111111 0010000100 1111111011
1110110111 0000000000 0000110010 1111010100 0000000010
0100100000 0111100010 0000000000 0011000000 1111111010
0100100000 0000001100 1111111111 1100000000 0010010000
0000101000 0101011111 0000000000 1111111011 0000010000
0010000000 0000010000 1111110111 0010000100 0010000000
0010010010 1111011111 1110101110 1111011101 0000000000
1111111110 1100010010 0001000001 1111111101 0011111011
0000000100 0010000001 0010000000 0010100000 0111111111
0000000000 0001110000 0000010010 1111011111 0010010000
1110101001 0010110110 1110000000 0000100101 0010100010
0000000110 0000000000 0000100010 1111001011 0000001000
0010000000 0000010001 0010011010 1111111011 1010101100
0010000001 0011000000 1000100010 0000010000 1111100011
0110000100 1001000000 1010000000 0100010110 0010111101
1101111111 0010000000 1111101111 1000100100 1111101111
Use a Beta(1, 1) prior for the class membership probability λ and for all tasks
use a Beta(1.6, .4) prior for the probability of success for masters, πj1, and a
Beta(.4, 1.6) probability of success for nonmasters, πj0. Estimate the param-
eters from the data using MCMC.
9.9 (Latent Class Prior). In Exercise 9.8, what would have happened if we
had used a Beta(1, 1) prior for πj1 and πj0?
Hint: Consider three MCMC chains starting from the starting points: πj =
(.2, .8), πj = (.5, .5), and πj = (.8, .2) for all j.
9.10 (Latent Class Parameter Recovery). The data for Exercise 9.8 are
from a simulation, and the parameters used in the simulation are: λ = 0.379,
plus the values in the following table.
Item
1
2
3
4
5
6
7
8
9
10
Nonmasters 0.22 0.17 0.31 0.07 0.25 0.19 0.15 0.18 0.22 0.11
Masters
0.82 0.84 0.85 0.72 0.81 0.76 0.8 0.7 0.84 0.76
Calculate a 95 % credibility interval for each parameter (this can be done by
taking the 0.025 and 0.975 quantiles of the MCMC sample). How many of the
credibility intervals cover the data generation parameters? How many do we
expect will cover the data generation parameters?

330
9 Learning in Models with Fixed Structure
9.11 (EM vs. MCMC). In each of the following situations, tell whether it
is better to use the EM algorithm or MCMC to estimate parameters.
1. The posterior mean will be used in an online scoring engine. The posterior
variance will be examined brieﬂy as a model checking procedure, but will
not be used in scoring.
2. The test specs call for only using items whose p-plus — marginal proba-
bility of success, p+
j = P(Xj = 1) — is greater than .1 and less than .9
with 90 % credibility, that is P(0.1 ≤p+
j ≤0.9) ≥0.9.
3. Only the posterior mean will be used in online scoring, but there is strong
suspicion that the distribution for the diﬃculty on several item parameters
is bimodal.
9.12 (Improving Posterior Standard Deviation). Consider the calibra-
tion in Example 9.3. Which of the following activities are likely to reduce the
standard deviation of the posterior law for the proﬁciency model parameter
λ:
1. Increase the size of the MCMC sample.
2. Increase the length (number of tasks) of the test.
3. Increase the number of students in the calibration sample.
Which of the following activities are likely to reduce the standard deviation
of the posterior for an evidence model parameter such as π10:
1. Increase the size of the MCMC sample.
2. Increase the length (number of tasks) of the test.
3. Increase the number of students in the calibration sample.
9.13 (LSAT model). The BUGS distribution package (Spiegelhalter et al.
n.d.) comes with a sample model called “LSAT” based on an analysis per-
formed by Bock and Aitkin (1981) of responses on ﬁve items from 1000 stu-
dents taking the Law School Admissions Test (LSAT). The data are analyzed
using the Rasch model, where if pij is the probability that Student i gets
item j correct, then
logit(pij) = θi −αj
(9.29)
where the proﬁciency variable θi has distribution N(0, τ) (Spiegelhalter et al.
n.d.). Note that this equation can be reparameterized as:
logit(pij) = βθi −αj
(9.30)
where θi ∼N(0, 1) and β =
"
1/τ.
Run an MCMC sampler using both the original (Eq. 9.29) and reparame-
terized (Eq. 9.29) models. What diﬀerences are there in the resulting Markov
Chains?

10
Critiquing and Learning Model Structure
The previous chapter described how to ﬁt a model to data. The parameter-
learning methods described there assumed the structure of the model was
ﬁxed. However, often there is as much or more uncertainty about the struc-
ture of the model as there is about the values of the parameters. There are
basically two approaches to this problem. The ﬁrst is model checking, or as
it is sometimes called, model criticism. Fit indices and graphical displays can
help us explore where and how well the model ﬁts the data, and bring to light
problems with a model. The second is model search. There are a number of
ways to search the model space for one that is “best” in some sense.
While traditional methods of characterizing model ﬁt emphasized overall
goodness of ﬁt, we take a more utilitarian perspective. The statistician George
Box famously said “All models are false, but some are useful” (Box 1976). We
want a Bayes net that captures the key interrelationships between what stu-
dents know, in terms of proﬁciency variables, and what they can do, in terms
of observables, without having to believe that the model expresses every pat-
tern in the data. We do not, however, want unmodeled patterns that make our
inferences about students’ proﬁciencies misleading for the purpose at hand.
We are interested in ﬁt indices that highlight particular kinds of model misﬁt
which, from experience, we know can appear in assessment data and distort
our uses of the model.
We emphasize exploratory uses of model checking over statistical tests of
ﬁt, partly because Bayes nets are often applied with small to medium size data
sets, and partly because the techniques we describe fall out almost as a by-
product of Markov Chain Monte Carlo (MCMC) estimation, in ways that gen-
erate their own reference distributions. The reader interested in large-sample
distributions of prediction-based ﬁt indices is referred to Gilula and Haber-
man (1995), Gilula and Haberman (2001), and to Haberman et al. (2013)
for an application to item response theory (IRT) models. Although we do not
pursue large sample properties here, the chapter draws in places on their work
on prediction-based indices.
c⃝Springer Science+Business Media New York 2015
331
R. G. Almond et al., Bayesian Networks in Educational Assessment,
Statistics for Social and Behavioral Sciences, DOI 10.1007/978-1-4939-2125-6 10

332
10 Critiquing and Learning Model Structure
Section 10.1 introduces some ﬁt indices and describes a simple simulation
experiment for using them. Section 10.2 looks at the technique of posterior pre-
dictive model checking (PPMC), which goes well with MCMC estimation. Sec-
tion 10.3 looks at some graphical methods for assessing model ﬁt. Section 10.4
addresses diﬀerential task functioning, where the issue is whether a task works
similarly across student groups. Section 10.5 then turns to model comparison.
Usually simpler models are preferable to more complex ones, but complex
ones will ﬁt better just because of the extra parameters. The DIC ﬁt measure
discussed in Sect. 10.5.1, which also goes well with MCMC, includes a penalty
for model complexity. In Sect. 10.5.2, prediction-based indices are deﬁned and
illustrated with the discrete-IRT testlet model. Looking ahead, Chap. 11 will
apply several of these techniques to the mixed-number subtraction example.
Given a measure of model ﬁt, one can search for a model that ﬁts the data
best. Section 10.6 looks at some literature on automatic model selection. There
are, however, some important limitations on learning model structure with
Bayes nets. In particular, there can be ways of reversing the direction of some
of the edges that have diﬀerent interpretations but do not change the implied
probability distribution. Section 10.7 discusses some of these equivalent mod-
els, and highlights pitfalls in attempting to learn “causality” from data.
10.1 Fit Indices Based on Prediction Accuracy
The fact that Bayes nets are probability models gives them a distinct advan-
tage over more ad hoc mathematical models for managing uncertainty, such
as fuzzy logic (Zadeh 1965) and certainty factors (Shortliﬀe and Buchanan
1975). In particular, the probabilities can be regarded as predictions and stan-
dard statistical techniques can assess how accurate those predictions are. This
assessment yields information about how well the model ﬁts the data.
Cowell et al. (1993) describe several locations in a Bayesian network at
which ﬁt can be assessed:
Node Fit:
How well the model predicts the distribution of a single variable
in the model. These can either be conditional predictions taking
the values of other variables into account or marginal predictions
ignoring the values of other variables.
Edge Fit:
How well the relationship between a parent and child in the graph
is modeled.
Global Fit: How well all variables in the data ﬁt the graphical model.
These are not always easy to apply in educational testing because the
proﬁciency variables are latent, and therefore predictive patterns of certain
dependencies described in the model cannot be directly assessed. In particular,
parent–child relationships often involve at least one latent variable and hence,
they cannot be directly tested with only observed data. Thus, the node ﬁt
indices can only be applied to observable variables and the global ﬁt indices
must calculate how well the model predicts all the observables.

10.1 Fit Indices Based on Prediction Accuracy
333
One way to get around this problem is to leave the data out for one
observable, and see how well the model predicts that observable based on the
remaining values. This is called leave one out prediction. (The idea extends
readily to leaving out a group of observables, then predicting some summary
statistic of them such as a subtest score.) Suppose that a collection of observ-
able outcomes is available from N learners taking a particular form of an
assessment. Let Yij be the value of Observable j for Learner i. Assume that
Yij is coded as an integer and it can take on possible values 1, . . . , Kj.
Using the methods of Chap. 5, it is easy to calculate a predictive dis-
tribution for Yij given any other set of data. Let Yi,−j be the vector of
responses for Learner i on every observable except Observable j. Deﬁne
pijk = P(Yij = k|Yi,−j). In all of the ﬁt indices described below, the idea
is to characterize how well the model’s prediction pijk, given all her other
responses, predicts the Yij that was actually observed. Let pij∗denote the
value of pijk for k = Yij, that is, the prediction probability of the event that
actually occurred.
Williamson (2000) noted that a number of measures of quality of predic-
tion have historically been applied to evaluate weather forecasting. These can
be pressed into service fairly easily to evaluate node ﬁt in Bayesian networks.
Williamson (2000) (see also Williamson et al. 2000) evaluated a number of
these and found the most useful to be Weaver’s Surprise Index, the Ranked
Probability Score, and Good’s Logarithmic Score. The ﬁrst two are more tra-
ditional indices, which remain useful to alert users to anomalies in collections
of predictions. Good’s Logarithmic Score, useful enough on its own, leads
us to more theoretically grounded techniques based on statistical theory and
information theory.
Weaver’s Surprise Index (Weaver 1948) attempts to distinguish between
a “rare” event and a “surprising” one. A rare event is one with a small proba-
bility. A surprising event is one with a small probability relative to the prob-
ability of other events. (The deﬁnition of events can make a diﬀerence: The
probability of a Royal Flush in clubs—the ace, king, queen, jack, and ten of
clubs—is the same as the probability of any other speciﬁed hand, so getting
this particular hand in poker is rare but no more surprising than any other
speciﬁed set of ﬁve cards in the deck when each is considered an event in the
comparison. It is rare and surprising with respect to events deﬁned by sets of
hands with the same poker value, such as one pair, two pairs, straight, etc.)
Weaver’s surprise index is deﬁned as the ratio of the expected value of
prediction probabilities to that of the actual event:
Wij = E(pijk)
pij∗
=
Kj
k=1 p2
ijk
pij∗
(10.1)
The expectation here is over Yij using the predictive probabilities pijk. The
larger the value, the more surprising the result. Weaver suggests that values

334
10 Critiquing and Learning Model Structure
of 3–5 are not large, values of 10 begin to be surprising and values above 1000
are deﬁnitely surprising.
Weaver’s surprise index assumes that one wrong prediction is as bad as
another. However, frequently the observables in an educational model rep-
resent ordered outcomes (e.g., a letter grade assigned to an essay). In those
situations, the measure of prediction quality should provide a greater penalty
for predictions that are far oﬀthan for near misses. The Ranked Probability
Score (Epstein 1969) takes this into account.
Sij = 3
2 −
1
2(Kj −1)
Kj−1

k=1
⎡
⎢⎣
 k

n=1
pijn
2
+
⎛
⎝
Kj

n=k+1
pijn
⎞
⎠
2⎤
⎥⎦
−
1
Kj −1
k

k=1
|k −Yij|pijk.
(10.2)
This index ranges from 0.0 (poor prediction) to 1.0 (perfect prediction). It
assumes the states have an interval scale.
Good’s Logarithmic Score extends the basic Logarithmic Scoring Rule.
Recalling that pij∗is the posterior probability of the observed outcome, the
basic logarithmic score is
Lij = −log pij∗.
(10.3)
The lower the probability, the larger the value of the logarithmic score for
this observation. Twice the logarithmic score is called the deviance, and ﬁnd-
ing parameter values that minimize the total deviance over a sample gives
the maximum likelihood estimates for a model. We will return to the use of
deviance in model comparisons in Sect. 10.5.1.
The basic logarithmic score makes no distinction between “rare” and “sur-
prising.” To take care of this eﬀect, Good (1952) subtracts the expected log-
arithmic score, over the values that might have been observed:
GLij = −log pij∗−−
Kj

k=1
pijk log pijk.
(10.4)
Values near zero indicate that the model accurately predicts the observation.
The average logarithmic score is known in information theory as entropy, or
the uncertainty about Yij before it was observed, and is denoted by Ent(Yij).
Any of the preceding ﬁt indices measures can be averaged across the sample
of examinees to give a measure of node ﬁt for a particular observable. For
example, Sj = N
i=1 Sij/N is the ranked probability score for Observable j.
The indices can also be averaged across nodes to get a measure of person ﬁt,
e.g., Wi = J
j=1 Wij/J.

10.2 Posterior Predictive Checks
335
The question is how extreme indices above need to be to indicate a prob-
lem. Williamson et al. (2000) posed a simple simulation experiment to answer
that question for any given model. When assessing model ﬁt, the null hypoth-
esis is that the data ﬁt the model. This suggests a procedure for determining
the distribution of the test statistic under the null hypothesis.
1. Generate a sample of the same size as the real data from the posited
model.
2. Calculate the value of the ﬁt indices for the simulated data set, using the
posited model and conditional probabilities.
3. Repeat Steps 1 and 2 many times to generate the desired reference distri-
bution of any of the ﬁt indices.
Step 2 produces ﬁt index values for all of the simulee*task combinations in
the sample, e.g., S∗
ij. Reference distributions for task and person ﬁt measures
are created by averaging over tasks or persons accordingly, i.e., Sj is computed
by averaging Sij. Williamson et al. (2000) note that the reference distribution
can be computed more cheaply using a simple bootstrap (Efron 1979), which
samples repeatedly from the observed data.
10.2 Posterior Predictive Checks
The Williamson et al. (2000) method ignores one potentially important source
of variability, the uncertainty in the predictions due to uncertainty about the
parameters (it matters less for problems with large samples). The method of
PPMC (Guttman 1967; Rubin 1984) does incorporate uncertainty in param-
eters. Furthermore, it works very naturally with Markov Chain Monte Carlo
estimation. Sinharay (2006) provides a good summary of this technique
applied to Bayesian network models. This section describes the approach and
gives simple example.
Let p(y|ω) be the likelihood for data y given parameters ω. Let yrep be
a replicate set of data generated by the same process as y with the same
parameters ω. This is sometimes called a shadow data set. The posterior
predictive method suggests using the posterior predictive distribution for yrep
to create a reference distribution for a given ﬁt statistic, similar to the way
Williamson et al. (2000) method created an empirical reference distribution
in the previous section. The posterior predictive distribution is deﬁned as:
p(yrep|y) =

p(yrep|ω)p(ω|y)dω .
(10.5)
Correspondingly, the posterior predictive distribution for a ﬁt index, e.g.,
Weaver’s surprise index for Task j, Wj, is obtained as
p(Wj(yrep)|y) =

p((Wj(yrep)|ω)p(ω|y)dω .
(10.6)

336
10 Critiquing and Learning Model Structure
The idea is to repeatedly draw shadow data sets yrep from a predictive dis-
tribution for data, using the posterior distribution of ω given the observed
data y. In each such replicate, calculate the value of some statistic or index of
interest. The resulting distribution is used as a reference distribution to eval-
uate the value calculated with y itself. (Exercise 10.11 is a simple problem
the reader can do by hand to get a feel for PPMC.)
Although p(yrep|y) can be diﬃcult to derive analytically in more complex
problems, it is actually straightforward to sample from, especially if an MCMC
algorithm was produced to sample from p(ω|y). In each cycle (or in selected
cycles) of the MCMC loop, after the values are drawn for the parameters ω,
a shadow data set yrep is drawn.
For instance, in the running latent class example in Chap. 9, for the ﬁrst
two tasks the probability of a correct response to Task j by a learner in Class k
is Bernoulli(πjk). We can, thus, write the probability for Learner i as πj,class(i).
When the model is described in the BUGS language, the corresponding line
in the model code is
y[i,j] ∼dbern(pi[class[i],j]).
In every MCMC cycle, the observed value of yij contributes to the likeli-
hood function for πj0 and πj1 and all of the other unobserved variables in the
model; in turn, values for each of them are drawn from their full conditionals
as described in Chap. 9. To obtain a shadow draw for yij in each cycle, we
merely need to add the line
yrep[i,j] ∼dbern(pi[class[i],j]).
Now in every cycle, the variable yrep
ij
is part of the model. No value is
observed for it so the sampler draws a value from its full conditional—which
is exactly the same in form as the distribution for yij. The draw is carried
out with class(i) and πj,class(i) ﬁxed at the values drawn for them this cycle.
These values will vary from one cycle to the next. The posterior distribution
for xrep
ij
, thus, properly takes into account uncertainty about these variables,
and all the other unknown variables in the problem.
Any descriptive statistic or analysis that can be run on y can also be run
on the shadow data set yrep(t) from MCMC cycle t, and the results compared.
Are there far too many zeros for Task j? Does a factor analysis of y yield
factors that yrep(t) does not? In diagnostic testing, an interesting statistic is
the number right on a subscale. Multiply the outcome vector by one column
of a Q-Matrix to get a number right score focused on one particular skill. This
will provide a measure of how well the model predicts performance on tasks
requiring that skill. The range of features to compare which might shed light
on model ﬁt and model improvement is limited only by the analyst’s ingenuity.
To avoid over-interpreting the results of such comparisons, they can be carried

10.2 Posterior Predictive Checks
337
out with multiple shadow data sets, drawn from diﬀerent MCMC chains or
widely spaced cycles in the same chain.
In practice, much posterior predictive checking uses a test statistic, or
discrepancy measure, D(y, ω). For example, Yan et al. (2003) illustrate the
use of Pearson residuals from each person-by-observable prediction (for other
choices of residual, see Bishop et al. 1975). Let Yij be the response of Per-
son i to Observable j. Following the notation of the previous chapter, let
λ(t) be the estimate of the proﬁciency model parameters at MCMC cycle t,
β(t)
j
the estimate of the link model parameters at MCMC cycle t, and θ(t)
i
be the estimate of the proﬁciency variables for Person i at cycle t. Let
p(t)
ij = E[Yij|λ(t), β(t)
j , θ(t)
i ] be the expected value for Yij given the values for
parameters and imputed variables at MCMC cycle t. The squared Pearson
residual for Person i, Observable j and cycle t is:
V (t)
ij
=

Yij −p(t)
ij
2
p(t)
ij (1 −p(t)
ij )
.
(10.7)
Taking the average of this measure over observables yields a measure of person
ﬁt and taking the average over persons yields a measure of observable ﬁt.
Taking the average over all observables for all people yields a measure of
overall goodness of ﬁt. As V (t)
ij
represents a squared residual, taking the square
root of the average provides a root mean squared error (RMSE).
For typical discrepancy measures like these where higher values indicate
worse ﬁt, Gelman et al. (1996) suggest comparing D(y, ω) to the distribution
of D(yrep, ω). In MCMC algorithms, one can simply count the number of
MCMC cycles in which D(yrep, ω) is larger than D(y, ω). This produces a
posterior predictive p-value (PPP):
PPP-value = P (D(yrep, ω) ≥D(y, ω)|y)
(10.8)
PPPs around .5 indicate that the observed discrepancies fall in the middle of
the distribution of discrepancy measures from the posterior predictive distri-
bution. This suggests adequate data-model ﬁt with respect to whatever char-
acteristic the measure is targeting. Values near zero (or unity) indicate that
the observed values fall in the upper (or lower) tail of the distribution, which
indicate that the model is underpredicting (or overpredicting) the measure,
and the patterns in the observed data depart from those in data generated
from the proposed model.
Robins et al. (2000) show that posterior predictive tests can be conser-
vative (fail to detect misﬁt). That is, when the null model is correct, PPMC
indices can tend to some degree to concentrate around .5 rather than having
a uniform distribution. Too few would be rejected at, say, at α = .05 when the
ﬁtted model is correct, and almost certainly too few misﬁts would be detected

338
10 Critiquing and Learning Model Structure
when the model is misspeciﬁed.1 However, the ease with which they can be
implemented in the MCMC context makes them particularly attractive. The
degree to which they are conservative turns out to depend on the choice of
discrepancy measure. Gelman et al. (1996) point to a number of studies in
which the posterior predictive tests are shown to have reasonable long-run
frequency properties, in the sense that they approximate nominal rejection
rates (see Example 10.1 for a graphical illustration of what this means).
Thus, the posterior predictive distribution method is conceptually simple
and computationally straightforward under MCMC estimation. The challenge
lies in ﬁnding discrepancy measures that are interesting, useful, and, prefer-
ably, have good long-run frequency properties.
The raison d’ˆetre of latent variables is modeling the associations among
observable variables. It stands to reason that discrepancy measures that con-
cern such associations are of particular interest for psychometric models in
general. Levy (Levy 2011; Levy et al. 2009) studied the performance of a num-
ber of discrepancy indices that focus on the joint distributions of observables,
including item correlations, residual covariances, log odds ratios (Sinharay and
Almond 2007), and Yen’s Q3 statistic (Yen 1993). These indices can be used
to detect violations of local independence from phenomena such as omitted
proﬁciency variables, diﬀerential item functioning, item drift, testlet eﬀects
(i.e., context eﬀects), rater eﬀects, and method eﬀects.
Yen’s Q3 is among
the indices Levy found to have good long-run frequency properties. It has the
additional appeal of being easy to understand and to calculate. The Q3 for a
pair of observables is the correlation between their residuals from model-based
predictions:
Q3jk = reijeik,
(10.9)
where, r is the correlation across persons i of the residuals eij = yij −
E(Yij). When conditional independence holds, these correlations are near zero
(approximately −(n −1)−1 for an n-item test). Positive values indicate the
variables are more positively associated than the model would predict, and
negative values indicate a more negative association than predicted. Originally
deﬁned for dichotomous items, the Q3 has proven useful for ordered response
outcomes as well (J. Mislevy et al. 2012).
Example 10.1 (PPMC for discrete IRT). This example extends the “dis-
crete IRT” models of Sects. 6.1 and 6.2. We will generate data from both
unidimensional discrete IRT model and a testlet model with two additional
“context” variables for subsets of observables, then look at model-checking
statistics assuming the unidimensional model.
1 Yes, this is frequentist reasoning in a book about Bayesian inference. Rubin (1984)
explains that this is appropriate logic for Bayesians who want to compare features
of an observed data set against the corresponding features in a sample of data
sets generated from a posited model.

10.2 Posterior Predictive Checks
339
The unidimensional model, Fig. 10.1a, contains a single proﬁciency vari-
able θ that can take ﬁve values. There are ten dichotomous observables, or
conditionally independent items. The testlet model, Fig. 10.1b, contains a
local proﬁciency variable Context1 that is also a parent of Items 3 and 4,
and another Context2 that is the parent of Items 6, 7, and 8. As discussed in
Sect. 6.2, the context variables make associations within such testlets higher
than the θ alone would predict; that is, the items within a testlet are condi-
tionally dependent given θ.
Item 1
Item 2
Item 3
Item 4
Item 5
Theta
Item 6
Item 7
Item 8
Item 9
Item 10
Item 1
Item 2
Item 3
Item 4
Item 5
 Context1
Theta
Item 6
Item 7
Item 8
Item 9
Item 10
Context2
a
b
Fig. 10.1 Two alternative discrete IRT models with and without context eﬀect.
a Conditionally independent IRT model. b Testlet model
Reprinted with permission from ETS.
The data were generated in all cases with θ ∼Categorical(.1, .2, .4, .2, .1).
The conditional probabilities for items were given by DiBello–Samejima mod-
els (Sect. 8.5). The ﬁve levels of θ are assigned the values {−2, −1, 0, 1, 2}. A
Rasch IRT model is used to calculate the probabilities of correct response to
each of the items, with item diﬃculties β = (−1.5, −.75, 0, .75, 1.5, −1.5,
−.75, 0, .75, 1.5):
pij = Ψ (θi −βj) = exp (θi −βj)/[1 + exp (θi −βj)] .

340
10 Critiquing and Learning Model Structure
In the testlet model, students’ context variables φi1 and φi2 can take the
values {−1, 1}. The probabilities for items 1, 2, 5, 6, and 10 are the same as
in the unidimensional model but
pij = Ψ (θi + cφi1 −βj)
for
j = 3, 4
and
pij = Ψ (θi + cφi2 −βj)
for
j = 7, 8, 9.
The variable c represents the strength of the context eﬀect. When c = 0, the
testlet model simpliﬁes to the Rasch model described above with conditional
independence. If c = .5, a student familiar with a context has her proﬁciency
increased by .5 for just those items in the testlet. Note that a student could
be familiar with one context and not another. In this example, the magnitude
of the eﬀect c is the same in both testlets.
Data sets were generated with 500 simulees each, for values of c = 0, .5, .75,
and 1.0. In each case, an MCMC solution was run assuming the unidimensional
model, which included β, the population probabilities π for θ, and the θ of
each simulee. Replicate data sets yrep were generated in each cycle. Q3 indices
for all item pairs were calculated in each cycle for both the real data and the
replicate data using that cycle’s draws of π, β, and simulees’ θs and φs. PPP
values were calculated for the Q3s.
Both the Q3 values and the PPPs are useful in analyzing the results.
Figure 10.2 shows Tukey stem and leaf plots for the PPPs at each level of c.
A row in the plot indicates the count of indices with a leading decimal. For
example, the lowest left row reads .0|23. This means that the data include the
values of .02 and .03. These are the worst-ﬁtting item-residual correlations, as
only 2 and 3 % of the model-generated residual correlations have higher values.
For the c = 0 plot, when the unidimensional model is correct, the distribution
is roughly uniform. This is what “good long run frequencies” means for an
index in PPMC analyses.
In the subsequent plots for observed data with testlet eﬀects, the PPPs
for item pairs in the same testlet are underscored. We are looking for greater-
than-expected residual correlations. For c = .5, these items show a tendency
to be toward the lower end, but this diagnostic is not good at distinguishing
them at this level of conditional dependence. The trend is more pronounced
for c = .75, and by c = 1, the four item pairs from testlets are the smallest
values in the plot. Their values are greater than the residuals from the null
model in more than 95 % of the cycles.
The PPP distribution for c = 1 is no longer rectangular but U–shaped.
We noted that the low values are for item pairs in the same testlet. The pile-
up of high values indicates item pairs for which the Q3 from the data was
lower than the one from the replicate data most of the time, i.e., residual
correlations were lower than expected. These tend to be items from diﬀerent
testlets.

10.2 Posterior Predictive Checks
341
.9 2367
.9 0578
.9 05
.9 12255666
.8 246
.8 789
.8 24889
.8 134
.7 12289
.7 014445579
.7 456679
.7 1114566
.6 24
.6 2
.6 24799
.6 23348
.5 022348
.5 04669
.5 344778
.5 4
.4 0223578
.4 1456
.4 01129
.4 0247
.3 0333479
.3 3578888
.3 01255
.3 2228
.2 1459
.2 4668
.2 0012
.2 34
.1 33455
.1 023578
.1 378
.1 02356
.0 23
.0 35
.0 356
.0 00157
c = 0
c = 0.5
c = 0.75
c = 1
Fig. 10.2 Stem-and-leaf plots of posterior predictive probabilities for Q3 values of
item pairs. Underscores indicate values for item-pairs in the same testlet. By c = .75,
the four pairs from common testlets are among the most discrepant from the ﬁtted
conditional independence model. By c = 1, they are the most extreme
Reprinted with permission from ETS.
Figure 10.3 shows the values of the Q3s themselves. We see strong positive
residuals for the items within testlets and negative residuals for items from
diﬀerent testlets. When the conditional dependence has reached 1, the Q3
indices give us useful clues to its structure.
1
2
3
4
5
6
7
8
9
1
2
3
4
5
6
7
8
9
2
+
2
++
3
+
3
4
++
−−
4
−
−
++
+
5
+
5
−
+
++
6
−−
++
++
++
+
6
++
−
−
7
+
−
−
+
−
7
−
−
−−
8
+
++
−−
−
8
−
−−
−
−−
−
−−
−
++
++
9
−−
−
++ −−
+
+
−
9
−−
+
−−
−−
−
++
++
++
++
10
−−
−
10
+
+
−−
−
−−
++
−
++
c
c = 0
c
c = 1
Fig. 10.3 Q3 values for item pairs, in increments of .025 from −.10 (bright red,
−−
−−) to +.10(dark blue,
++
++). For the actual conditionally-independent data, where
c = 0, values closer to zero and randomly distributed. For the testlet data, the item
pairs corresponding to testlets show strong residual correlations and the pairs from
diﬀerent testlets show strong negative residual correlations
Reprinted with permission from ETS.
We also calculated item-ﬁt and person-ﬁt RMSEs for the c = 0 and c = 1
data sets, under the assumption of conditional independence. Histograms for

342
10 Critiquing and Learning Model Structure
for both kinds of residual were roughly rectangular, and nearly identical in
shape across the two data sets. In contrast to the Q3 analyses, these marginal
residuals for items and for people were not sensitive to conditional dependence
from testlet eﬀects.
An examination of person-ﬁt RMSEs did, however, reveal interesting pat-
terns of misﬁt for individual simulees. Recall that the item diﬃculties went
from easy to hard for items 1–5, and again for items 6–10. The person-ﬁt
RMSEs did distinguish simulees whose responses were more consistent or less
consistent with the expectation of getting easier items right and harder items
wrong. The pairs of response patterns below have the same total score but
PPP values that signal whether which items are right are expected or surpris-
ing.
The point illustrated here is that diﬀerent ﬁt indices are better at picking
up diﬀerent kinds of misﬁt, so we use multiple methods for checking for dif-
ferent problems. The Q3 indices are better at picking up unexpectedly strong
relationships among items that can signal either unintended similarities across
items or distinct skills we might want to consider measuring distinctly. They
are not sensitive to particular individuals with aberrant response patterns.
Person-ﬁt indices can do this well, but tell us little about unmodeled condi-
tional dependencies.
Total
Pattern
PPP
3
00000 01110
.004
3
11000 10000
.878
5
00111 10001
.004
5
11100 11000
.874
8
11111 01011
.004
8
11110 11110
.897
10.3 Graphical Methods
Graphical methods can be more useful than diagnostic statistics because they
can reveal unexpected patterns. One of the more useful diagnostic plots from
IRT is the empirical item characteristic curve (Lord 1980). This graph plots
an estimate of the proportion of students at a proﬁciency level getting an item
correct against the proﬁciency variable. Yan et al. (2003) develop a variation
of the item characteristic curve that is appropriate for Bayes nets.
The basic idea of their observable characteristic plot, is to group the stu-
dents into classes based on their proﬁciency variables such that all students
within a class should have the same probability distribution for the observed
outcome variable. The model-based probability for the class is plotted against

10.3 Graphical Methods
343
an estimate based on a sample of individuals from that class, who should have
that probability for this observable. We will ﬁrst describe how the plots would
be constructed and used if we knew students’ proﬁciencies.
Consider an observable variable, Obs, that has two parents, Skill1 and
Skill2, both of which take on two values: 1 and 0. There are four possible
conﬁgurations of the two parent variables: (0, 0), (0, 1), (1, 0), and (1, 1).
The conditional probability table P(Obs|Skill1, Skill2) gives the probability
for Obs given each possible conﬁguration of the parents. For example, if
P(Obs|Skill1, Skill2) is a DINA (deterministic input noisy-and) model, then
conﬁgurations (0, 0), (0, 1) and (1, 0) are associated with π−and conﬁguration
(1, 1) is associated with π+.
Suppose for the moment that we had a sample of 100 students for which
we knew the value of Obs, Skill1 and Skill2. It would then be straightforward
to produce a graphical test for the values of P(Obs|Skill1, Skill2) predicted by
our model. Suppose that n00 students had the proﬁciency proﬁle (0, 0) and
that of them, x00 got a correct outcome for Obs. We can then build a 95 %
credibility interval for the proportion P(Obs|Skill1 = 0, Skill2 = 0) = π00 using
a beta-binomial model for just that proportion. To ensure that the posterior
is proper, we need to use a proper prior distribution for π00. Since we expect
this probability will be less than .5, we can use a weak prior that biases
the estimates slightly towards small values, say a Beta(.2, .8) distribution.
The posterior will then be a Beta(x00 + .2, n00 −x00 + .8) distribution we
can use to form the credibility interval. We can use a similar procedure to
form credibility intervals for the remaining three conﬁgurations of the parent
variables. The intervals for π10 and π01 are constructed similarly. However,
we expect π11 will be greater than .5 so we use a Beta(.8, .2) prior. The exact
value of the priors are not critical, but it is important that (a) it is a proper
prior, so we get a proper posterior, and (b) the sum of the parameters is small,
say 1 or less, so the interval depends mainly on the data.
Figures 10.4a, b show the observable characteristic plot (Yan et al. 2003),
a graphical realization of these credibility intervals. For each skill proﬁle, a
vertical bar gives the credibility interval for the proportion correct for that
group. The horizontal lines are the two probabilities π−and π+ predicted by
the DINA model. The midpoint of each credibility interval is plotted with the
symbol ‘−’ or ‘+’ according to whether π−or π+ is the appropriate probability
for this group.
Figure 10.4a shows the plot for a task that is working fairly well. All of
the credibility bars overlap the probabilities predicted under the model. The
ﬁrst three skill groups lack one or both skills, and the modeled probability is
.18; the three “observed” probabilities for these groups are between .15 and
.30. The fourth group, with both skills, is modeled as having .85 probability
of a correct response, and the “observed” value is .9.
Figure 10.4b shows a plot from a task that is not working according to its
evidence model. While the lowest two groups have “observed” probabilities
that are a bit high and the group with both skills has a high probability that

344
10 Critiquing and Learning Model Structure
Data that fit the model
Groups
Probability
−
−
−
+
0.0
0.2
0.4
0.6
0.8
(0,0) (0,1) (1,0) (1,1)
−
+
a
b
Data that don't fit the model
Groups
Probability
−
−
−
+
0.2
0.4
0.6
0.8
−
+
(1,1)
(1,0)
(0,1)
(0,0)
Fig. 10.4 Observable characteristic plot
Reprinted with permission from ETS.
agrees well with the modeled value. But the credibility interval for skill proﬁle
(1, 0) lies midway between the π−and π+ lines. This calls into question the
symmetric treatment of the two skills given by the DINA model. An alterna-
tive model, perhaps a compensatory one, should be investigated. Finding a
similar pattern on more tasks would reinforce this choice. Alternatively, the
problem may lie with the task, which should be investigated to see if it is
working as expected—e.g., with think-alouds from students as they solve it.
Of course, constructing observable characteristic plots is not as simple as
the above discussion implies, because the states of the proﬁciency variables,
and therefore the proﬁciency proﬁles for each person, are not known. One
solution is to use an imputed set of proﬁciency variable states from one cycle
of an MCMC sampler (Yan et al. 2003). An alternative is to use the propor-
tion of cycles an individual appears in a given skill proﬁle during the MCMC
as a “weight” for that individual (Sinharay et al. 2004; Sinharay and Almond
2007). Thus, if an individual who got correct outcome for the observable
and appeared was assigned Skill Proﬁle (1, 1) in 75 % of the cycles, Skill Pro-
ﬁle (1, 0) in 14 % of the cycles, Skill Proﬁle (0, 1) in 10 % of the cycles, and Skill
Proﬁle (0, 0) in the remaining 1 %, would provide weights of (.01, .1, .14, .75)
to the four proﬁles (in the order given in the plot in Fig. 10.4a). This way of
dealing with uncertainty about individuals’ latent proﬁciencies is analogous
to the E-Step of the EM algorithm in Chap. 9.
The proﬁciency proﬁles do not need to be limited to just the parent vari-
ables of the observable in question. Including other variables produces a test
of the local independence assumption. Consider what happens when we add
another variable, Skill 3, to the two-variable DINA shown in Fig. 10.4a. If
the local independence assumption holds, then the augmented graph should
look something like Fig. 10.5a with the proﬁles both with and without Skill 3,
giving similar credibility intervals. In Fig. 10.5b, however, Skill 3 appears to
give a boost in performance on the observable. In the face of such a plot, the

10.3 Graphical Methods
345
Data for which Skill 3 is irrelevant
Groups
Probability
−
−
−
−
−
−
+
+
0.0
0.2
0.4
0.6
0.8
1.0
(0,0,1)
(0,1,1)
(1,0,1)
(1,1,1)
High Skill3
(0,0,0)
(0,1,0)
(1,0,0)
(1,1,0)
Low Skill3
−
+
Data for which Skill 3 is relevant
Groups
Probability
−
−
−
−
−
−
+
+
(0,0,1)
(0,1,1)
(1,0,1)
(1,1,1)
High Skill3
(0,0,0)
(0,1,0)
(1,0,0)
(1,1,0)
Low Skill3
−
+
a
b
0.0
0.2
0.4
0.6
0.8
1.0
Fig. 10.5 Observable characteristic plot for additional skill
Reprinted with permission from ETS.
assumption of independence between Skill 3 and the observable in the evi-
dence model should be reviewed; Skill 3 might need to be added as a parent
to the observable.
As an extreme case, the x-axis of the graph could contain all possible
skill proﬁles. However, all skill proﬁles may not be distinguishable from the
data. For example, in the mixed number subtraction assessment, only nine
diﬀerent groups of skill patterns are identiﬁable based on the Q-Matrix for
this particular set of tasks (Klein et al. 1981; see also Sect. 6.4 and Chap. 11
of this book). In particular, all tasks require Skill 1, therefore, all 12 skill
patterns that lack Skill 1 have the same expected outcome pattern (all tasks
incorrect). Similar logic reveals that the 24 skill patterns fall into 9 diﬀerent
equivalence classes, or groups of skill patterns that have the same expected
outcome pattern (Yan et al. 2003; see also Sect. 11.1 of this book).
Sinharay and Almond (2007) develop an observable ﬁt statistic to accom-
pany this observable characteristic plot. It uses the same data that are used to
construct the plot. Divide the proﬁciency space up into K equivalence classes
as described above, and let τik represent the probability that Person i is in
Equivalence Class k. Deﬁne the “number” of people in Equivalence Class k as
Nk = 
i τik. Then (assuming that all observables are binary), the “observed”
number correct for Equivalence Class k and Observable j is Okj = 
i τikYij.
The “expected” number of correct outcomes is Ekj = πkjNk , where πkj is
the probability (according to the model) of a person in Equivalence Class k
getting Observable j correct. Then the goodness of ﬁt statistic is
χ2
j =

k
(Okj −Ekj)2
Ekj
.
(10.10)

346
10 Critiquing and Learning Model Structure
This statistic is inspired by the classical χ2 test. If proﬁciencies were known,
it would follow a χ2 distribution with degrees of freedom equal to the number
of equivalence classes minus the number of probabilities estimated for this
observable. Because the proﬁciencies are not known, Sinharay and Almond
(2007) suggest using the posterior predictive distribution as a reference dis-
tribution.
An information-based analogue to Eq. 10.10 is the entropy of the “observed”
proportions of correct responses in the equivalence classes with respect to the
modeled proportions (Savage 1971):
EntM(Yj) = −

k
pkj log( pkj
πkj
)
(10.11)
where pkj = Okj/Nk.
A problem occurs when there is a very large number of potential proﬁciency
proﬁles (assignments of states to all proﬁciency variables in a model). If each
unique proﬁciency proﬁle is assigned a unique equivalence class, there may be
many classes with fewer than three members in the sample. Coarser grouping
is then preferable.
Sinharay and Almond (2007) introduce a graphical method called Direct
Data Display. In this display, participants are sorted according to some rough
measure of overall ability (e.g., number right score) and observables are sorted
according to some measure of diﬃculty (e.g., their marginal distribution in
the sample). The entire data set (or a sample, if there are too many cases) is
then depicted in a grid, with students as the x–axis, from low to high, and
items as the y–axis, from easy to hard. Each observed outcome Yij is used
to assign a gray-scale value to a pixel in the image (white for lowest possible
response, black for highest possible response). The result should be a bar that
is mostly black toward the left and bottom and white toward the right and
top.
The reference distribution for this plot is produced through posterior pre-
dictive checks. Some number (say ten) cycles are randomly chosen from the
MCMC estimation procedure and a set of shadow data is generated from each.
Any feature that appears in the real data plot but not the shadow data rep-
resents an unmodeled feature of the data. It is up to the analyst to decide if
this feature is important.
Figure 10.6 shows an analysis by Sinharay (2006) of the mixed number
subtraction data (Sect. 6.4). Look at the students at the lower end of the scale.
In the simulated data, they get a random pattern of tasks correct through
guessing. In the real data, they seem to get only the fourth and ﬁfth easiest
items right. These turn out to be the items 3
4−3
4 and 3 7
8 −2. Both can be solved
by strategies that do not involve fraction subtraction skills: “Something minus
itself is always zero,” and “3 and something minus 2 is 1 and something.”

10.4 Diﬀerential Task Functioning
347
Fig. 10.6 Direct data display for mixed number subtraction test
The plot at the bottom is a direct data display of the mixed number subtraction data
(Klein et al. 1981). The ten comparable ﬁgures above it are posterior predictive
replications from an MCMC sampler. Note the diﬀerence between the real-data
plot at the bottom and the replicates above for the last 20 or so individuals: The
replicate data sets show more random correct answers through guessing spread across
the tasks, while the real data show that most of the correct answers by low-scoring
students are to two particular tasks. Reprinted from Sinharay (2006) with permission
of S. Sinharay and Sage Publications.
10.4 Diﬀerential Task Functioning
A special kind of model misﬁt can occur when the population of interest
for an assessment consists of distinct subpopulations. Ideally, the assessment
should behave in the same way for each subpopulation, i.e., the same evidence
models with the same parameters should be appropriate. There has been spe-
cial concern when the subpopulations are based on gender, racial, or cultural
groups. However, the problem also encompasses issues of whether an assess-
ment is suitable for multiple purposes. For example, is the same test (with
the same evidence models) suitable for both 2-year community colleges and
4-year universities?
This problem has been well studied under the name diﬀerential item func-
tioning (DIF) (Holland and Wainer 1993). The basic question of DIF analysis
is whether an item behaves diﬀerently for diﬀerent subpopulations, especially
if the diﬀerent behavior is unrelated to the construct of interest. In the con-
text of the more complex assessments supported by Bayes nets, tasks are
more naturally the unit of analysis. Therefore, we refer to this phenomenon
as diﬀerential task functioning.
Measuring the equivalence of a task across diﬀerent subpopulations is com-
plicated by the fact that the populations often have diﬀerent distributions of
proﬁciency. DIF analyses therefore examine conditional probabilities of task
response across groups conditional on some matching criteria, which could be
observed scores or latent proﬁciency variables (Holland and Wainer 1993).

348
10 Critiquing and Learning Model Structure
The latter alternative is well suited to Bayes nets proﬁciency models: If
the proﬁciency model is a complete description of the construct, then the
observable outcomes should be conditionally independent of subpopulation
membership given the proﬁciency variables. In other words, the performances
of the groups may diﬀer, but all of the diﬀerences can be captured as diﬀer-
ences in distributions on the proﬁciency variables. This framing provides a
testable deﬁnition of diﬀerential task functioning: Are task responses condi-
tionally independent given proﬁciency variables (including context variables
when needed)—speciﬁcally not requiring group membership as an additional
parent? Note that because of the independence assumptions embedded in
Bayes nets we have developed, only the proﬁciency variables that are involved
in the evidence model for a given task need to be considered for this determi-
nation. Bishop et al. (1975) suggest thinking about this test as the diﬀerence
between two models, one of which contains the independence condition.
Let D be a demographic variable representing membership in a subpopu-
lation of interest, O be an observable in a task to be tested for diﬀerential task
function, and P be a variable representing the proﬁciencies variables that are
parents of any observable in the task of interest. Figure 10.7a shows the model
without diﬀerential task functioning, with P D-separating D and O; Fig. 10.7b
shows the model with diﬀerential task functioning. If P were known, the dif-
ference in deviance between those two models would follow a χ2 distribution.
For unstructured, or hyper-Dirichlet, conditional probabilities, the degrees of
freedom would be (|D|−1)(|O|−1), where |D| represents the number of levels
of the demographic variable and |O| represents the number of possible states
of the observable. PPMC or Williamson simulations (Sect. 10.1) provide a
custom-built reference distribution to use in practice.
a
b
Fig. 10.7 Two graphs showing the eﬀects of diﬀerential task functioning. a Graph
with no DTF b Graph with DTF
Reprinted with permission from ETS.
The method illustrated in Fig. 10.4b provides a graphical check for DIF:
We examine empirical probabilities of task response, for students grouped by
their values on the proﬁciency variables that are parents of the task and their
subpopulation membership. As discussed there, since proﬁciency is not known
these charts can be made by assigning students their modal proﬁciencies or by

10.4 Diﬀerential Task Functioning
349
distributing their responses across groups according to their posterior proba-
bilities.
A statistical test is easiest to implement with a parametric model for con-
ditional probabilities, as in Sects. 8.4 and 8.5.2 For example, the DINA model
for a dichotomous task with two binary skill parents shown in Fig. 8.6 has
two parameters, the true-positive probability π+ of a correct response when
a student has both Skill 1 and Skill 2, and a false-negative probability π−
when she lacks one or both skills. To allow for DIF, extend the model with
group-by-task interaction parameters δ+k and δ−k, such that the true-positive
probability for a student in Group k is π+ + δ+k and the false-positive prob-
ability is π−+ δ−k. (To identify the values of the DIF parameters, designate
one group, the so-called reference group, for which δ+k = δ−k = 0.) Fit the
model using EM or MCMC, and examine the posterior distribution of the δs.
For example, does the 95 % posterior credibility interval for a given δ include
0? If so, the DIF is probably not substantial.
Similarly, suppose the conditional probabilities for a task are given by a
DiBello–Samejima model as in Eq. 8.12. The no-DIF cumulative probabil-
ity of a response at or above level m from a student with eﬀective theta
θ is P(X ≥xm|θ) = logit−1(θ −dm), with dm the category diﬃculty. A
single-parameter shift, toward relative harder or easier for subpopulation k,
is aﬀected by ﬁtting P(X ≥xm|θ, D = k) = logit−1(θ + δk −dm). More
reﬁned checks are aﬀected by incorporating category DIF parameters, as
P(X ≥xm|θ, D = k) = logit−1(θ −dm + δmk).
In high-stakes assessment, DIF analysis is used with unidimensional IRT
or number-correct scores to detect and remove items which show DIF that
is both statistically signiﬁcant and large enough to distort the meaning of
scores across groups. In the medium and low stakes uses of Bayes nets in
assessment, the goal is understanding students’ proﬁles of proﬁciency to sup-
port further learning. Including tasks that exhibit DIF but building the eﬀect
into the model is useful when certain tasks provide evidence but, due to con-
struct irrelevant features, provide less information or conﬂicting information
about students from diﬀerent backgrounds. For example, a writing task deal-
ing with job applications on the National Assessment of Educational Progress
some years ago was found to provide less information for 8th grade students
than 12th grade students, presumably because more of the older students had
personal experience with this genre. In a large-scale survey like NAEP, meant
to study patterns of proﬁciency among populations rather than assess indi-
viduals, downweighting the evidence from this task for younger students is an
appropriate course of action.
2 In IRT, the most popular method for detecting DIF is the nonparametric Mantel–
Haenszel test (Holland and Thayer 1988), which conditions on observed score. See
Exercise 10.9 and Sect. 13.2.2.

350
10 Critiquing and Learning Model Structure
10.5 Model Comparison
The previous section contained a number of diagnostic tests to see if a pro-
posed model ﬁts. Implicit in the idea of model diagnostics is that if the model
does not ﬁt well, a new better model can be found. Suppose that a new model is
proposed. How do we know that the new model is better? The two approaches
discussed below build around ideas discussed in Sect. 10.1 in connection with
Good’s logarithmic score. The classical statistical approach is to compare ﬁt
from the perspective of likelihood, which revolves around deviance. An alter-
native approach based on predictive power uses entropy as a metric (Gilula
and Haberman 1995; Gilula and Haberman 2001).
10.5.1 The DIC Criterion
The likelihood function is the basis of many model-comparison indices. Intu-
itively, the better a model ﬁts a given data set Y, the higher the likelihood
P(Y|ω) , where ω represents the model parameters. The deviance of a model
compared to a saturated model is deﬁned as
D(ω) = −2 log{p(Y|ω)} + 2 log{f(Y)},
(10.12)
where f(Y) is a term that depends on the data, but not the parameters. This
term usually drops out of the equations through subtraction when values for
two competing models are compared (thus a common misuse of terminology
is calling just −2 log{p(Y|ω)} the deviance). Note that the ﬁrst term is just
twice the logarithmic score (Eq. 10.3) summed over the sample, evaluated at
a particular parameter value ω.
A familiar case is when either the new model or the old one is a simpliﬁca-
tion, or submodel, of the other model—Model 1 is nested within Model 2, for
instance, a linear regression model with two coeﬃcients ﬁxed to zero. Model
2 will always ﬁt better than Model 1, but, is the improvement enough to jus-
tify estimating additional parameters? The likelihood ratio test developed by
Neyman and Pearson in 1928 can be expressed in terms of deviance: When
Model 1 is the true model, D(ˆω2) −D(ˆω1) should approximately follow a χ2
distribution with degrees of freedom equal to the diﬀerence in their numbers
of parameters.
This test does not generalize easily to the case where one model is not a
submodel of the other. Several authors have suggested model ﬁt indices that
can be used with models that are not nested, and also correct for the number
of parameters in the model. The most popular are the AIC (Akaike 1973) and
BIC (Schwarz 1978) criteria:
AIC = −2 log P(Y|ˆω) + 2d,
and
BIC = −2 log P(Y|ˆω) + log(n)d,

10.5 Model Comparison
351
where d is the number of parameters in the model, and n is the number of
observations. A more complicated model is preferred to a simpler one (one
with a smaller pD) only if the improvement in ﬁt is bigger than the diﬀerence
in dimensionality. Therefore when comparing two models, the one with the
smaller value is preferred. Diﬀerences less than less than, say, 4 or 5 are not
considered compelling.
AIC, BIC, and similar alternatives require knowing the number of free
parameters to be estimated in the model. While counting the number of
parameters is straightforward in a regression problem, it is not so straight-
forward in complex Bayes model, when parameters are constrained through
prior distributions or subsets of them are related in hierarchical structures.
Spiegelhalter et al. (2002) introduce a measure called DIC, which includes
a Bayesian notion of dimensionality(see also Plummer 2008; Gelman et al.
2013a). The −2 log{p(Y|ω)} part turns out to be very easy to calculate for
Bayes net models. Let Yi be the vector of outcomes for each person in the
sample. Score the students according to the method of Chap. 5, only make
sure that when passing messages with a Bayes net, or from the evidence model
to the scoring model that the message tables are not normalized. Now, pick
any node in the scoring model and calculate the normalization constant for
its marginal distribution. This should have the value p(Yi|ω). Its log is the
person speciﬁc contribution to the deviance:
Di(ω) = −2 log p(Yi|ω) .
(10.13)
The deviance D(ω) is the sum of the person speciﬁc contributions across
persons for a given value of ω.
Then (Spiegelhalter et al. 2002) propose a measure of model ﬁt based on
the posterior distribution of deviance:
DIC = D(ω) + pD,
(10.14)
where D(ω) = E[D(ω)], the average deviance calculated using the draws of
ω across MCMC cycles, and the dimensionality pD is calculated as either of
two asymptotically equivalent ways (Gelman et al. 2013b):
pD = D(ω) −D(¯ω)
(10.15)
or
pD = 1
2Var[D(ω)].
(10.16)
As with the AIC and DIC, a smaller value of DIC is preferred.
Spiegelhalter et al. (2002) showed the values of pD agree well with param-
eter counts in straightforward cases like nested regression models. However,
DIC assumes the posterior mean is a good estimate of the stochastic param-
eters in the model, and this assumption is not always reasonable. A case in
point is ﬁnite mixture models, which includes Bayes nets with ﬁnite-valued

352
10 Critiquing and Learning Model Structure
proﬁciencies. Suppose, e.g., a proﬁciency can take three values that represent
strategies for solving mixed-number subtraction, where a student is assumed
to apply the same strategy on all tasks. It is mechanically possible to label
them 1, 2, and 3, then calculate a posterior mean, but the result is not a
meaningful quantity in the model. Two work-arounds are useful to compare
structurally similar models. First, we can substitute posterior modes for means
in ¯ω if the values for such variables are well-determined, that is, with most
of the posterior probability on one value. Second, in some problems we can
approximate the discrete variable with a continuous one. Example 10.2 illus-
trates the latter approach.
From the individual contributions to the deviance, Di(ω), an individual
contribution to the dimensionality can be deﬁned:
pDi = Di(ω) −Di(¯ω) .
(10.17)
Spiegelhalter et al. (2002) suggest that this is a measure of leverage. Leverage
is a measure of how inﬂuential an individual is determining the parameters
of a model. For example, in a linear regression, points with high leverage are
often outliers on one or more explanatory variables.
Any individuals with an unusually high leverage value should be examined
carefully. Does this individual really belong with the other data, or is some-
thing else going on with this person? High values can indicate issues with
either the data, the model or the model ﬁtting process.
Example 10.2 (DIC for the Testlet Model). The deﬁnitional application
of DIC does not apply directly to the discrete IRT model and context model
of Example 10.1. The results of ﬁtting the context model to the c = 1 data set
showed the posterior modes of more than half of the simulees’ two-valued φs
had probabilities of less than .7. This is not disconcerting because the purpose
of a testlet model is to account for conditional dependence in responses, not
to estimate individuals’ testlet values. But it does mean the work-around of
using posterior modes to approximate DIC is not appropriate.
We can, however, compare analogous models with continuous θ, φ1, and
φ2, using normal priors with mean 0 and precision .25. This is more rea-
sonable since the categorical values of the θs in the discrete IRT model are
in fact associated with the values {−2, −1, 0, 1, 2} in calculating conditional
probabilities through the Rasch model. Continuous values for φs also accord
with the model’s conditional probability function, and account better for the
diﬀuse posteriors of most simulees’ testlet eﬀects.
Accordingly, DIC values were obtained ﬁtting a conditional independence
model, a two-testlet model with a common c across testlets, and a testlet
model with a c for each testlet. The results are are shown below. The model
with a single c ﬁts much better than the conditional independence model, and
the model allowing diﬀerent cs ﬁts marginally better.

10.5 Model Comparison
353
Model
D(ω)
D(¯ω)
pD
DIC
Diﬀerence
Conditional independence 5366.23 5030.71 335.52 5701.75
Testlets, common c
5013.86 4415.08 598.78 5612.64
89.11
Testlets, diﬀerent cs
5043.02 4478.51 564.51 5607.52
5.12
10.5.2 Prediction Criteria
One problem with ﬁt measures based on deviance is that they can be overly
sensitive to occurrences of observations with small probabilities. (This is the
rare vs. surprising diﬀerence that Good corrected for in his index for evaluating
predictions, by subtracting out the expected log penalty, or entropy.) Gilula
and Haberman (2001) propose model comparison metrics based on expected
improvement in prediction that are less sensitive to this problem. The indices
are based on improving prediction of a new observation: Given the information
in the data Y, how much better would Model M (2) be expected to predict a
new observation than Model M (1)? This question can be answered in terms
of how much smaller the expected deviance would be, or entropy reduction.
The entropy for M (r) is
Ent(M (r)) = −
N

i=1
p(r)(yi) log(p(r)(yi)),
(10.18)
where p(r)(yi) is the modeled probability of yi under M (r). A measure of how
much better M (2) predicts a new observation than M (1) is
Ent(M (1)) −Ent(M (2)).
(10.19)
The diﬀerence is positive if M (2) yields better prediction of Y than M (1),
and negative if M (1) yields better prediction. This measure can be used to
compare models that are nested or non-nested, and to evaluate the impact of
including covariate in a model. If M (1) is nested within M (2), the diﬀerence is
non-negative. In cases where counting parameters is straightforward, one can
evaluate model diﬀerences in terms of improvement per parameter by dividing
Eq. 10.19 by the diﬀerence in number of parameters. For interpretability, it
can be rescaled to improved prediction for a single observation by dividing
through by N.
Gilula and Haberman (2001) also suggest a criterion for model comparison
that is analogous to proportion of variance accounted for in regression analysis.
Let M (0) be a base model. The proportional improvement aﬀorded by M (r)
is
Ent(M (0)) −Ent(M (r))
Ent(M (0))
.
(10.20)
Example 10.3 (Prediction Improvement in the Testlet Model). This
example continues with the ten-task discrete IRT testlet model of Exam-
ple 10.1, using the data set with N = 500 and two testlets with a common

354
10 Critiquing and Learning Model Structure
c = 1. The following nested models were ﬁt, and per-person entropy values
calculated:
M (0):
Null model, with item eﬀects only. All responses of all per-
sons modeled as independent, with item probabilities given by
sample proportions-correct.
M (1):
Discrete Rasch model, i.e., person parameters (i.e., latent vari-
ables) added, conditional independence assumed.
M (2):
Testlet model, common c.
M (3):
Testlet model, possibly diﬀerent cs for the two testlets.
% improvement
% improvement
Model
Ent(M (r))
over M (0)
over M (r−1)
Null model
6.223
Conditional independence
2.687
56.8
56.8
Testlets, common c
2.645
57.5
1.6
Testlets, diﬀerent cs
2.519
59.5
4.7
Person parameters improve prediction substantially, but testlet eﬀects do not.
This is so even though the ﬁt investigation for conditional dependence in
Example 10.1 revealed strong testlet eﬀects. These can be important as feed-
back to test developers for ﬁnding problems with items and for discovering
omitted skills in the proﬁciency model, but in this case they have little impact
for estimating the proﬁciencies of individuals. (The story can be diﬀerent for
assessments consisting of a small number of testlets with many items each,
and strong testlet eﬀects.) The remaining entropy is the variation inherent
in the Bernoulli distributions of the item responses given person and task
parameters.
10.6 Model Selection
It is only a short step from comparing two models to see which is better, to
searching for a best model to ﬁt a particular set of data. Heckerman (1998)
(reprinted in Jordan 1998), Buntine (1996), and Gelman et al. (2013b) provide
good tutorials. Cowell et al. (1999) has several chapters on this topic, and
Neapolitan (2004) devotes half the book to both parameter and structure
learning. The rest of this chapter will brieﬂy review some of the trends in this
rapidly evolving area.
The idea is to search the space of possible Bayes net models that opti-
mize a given criterion. Some example criteria include model likelihood, or the
likelihood of the data under the model; posterior model probability, or the

10.6 Model Selection
355
posterior probability of the data under the model; various penalized versions
of those criteria, such as AIC, BIC, and DIC; and predictive power, as in
Sect. 10.5.2. The required computations can be made more eﬃcient if the
criterion factors with the graph (Cooper and Herskovits 1992).
In the educational assessment, setting the problem is more complex than
in some domains because the proﬁciency variables are usually latent. A spe-
cial case of much interest with the kinds of networks we have focused on in
this book, and in cognitive diagnosis models more general, is when we are
not 100 % sure about the Q-matrix. To tackle this problem speciﬁcally, the
general techniques described below can be applied by maintaining the set of
proﬁciency variables, and searching over the inclusion or exclusion of edges
from proﬁciency parents to potential observable children (de la Torre 2008).
The tasks in a test determine which possible Q-matrices can be distinguished
from one another and which cannot; Liu et al. (2012) provide theory for this
identiﬁcation issue.
One problem with model search is over ﬁtting. A model that is over ﬁt
will give good predictions of the training data, but may not be very good
at future predictions. Simpler models tend to avoid over ﬁtting better than
more complex ones. One way to guard against over ﬁtting is Cross validation
(Kohavi 1995). The basic idea is to divide the data into two groups. The
training data is used in the model selection algorithm, and the test data is used
in evaluating which model to select. More complex cross validation schemes
use multiple training and testing data sets to set parameters of the model
search algorithm.
The remainder of this section summarizes some basic results from the
ﬁeld of model search. Section 10.6.1 reviews simple search strategies and
Sect. 10.6.2 looks at stochastic search strategies. Section 10.6.3 considers
choosing a set of models, not just a single best model. Section 10.6.4 looks at
prior distributions over the space of possible models. Section 10.7 looks at an
important technical issue that comes up during model search, the fact that
models with diﬀerent graphical structures can in fact be equivalent.
10.6.1 Simple Search Strategies
In a typical problem, the space of all possible models is too big to perform a
“British Museum” search (calculate goodness of ﬁt measure for every possible
model). Instead several possible heuristic strategies can be used. The follow-
ing approaches are familiar from regression analysis and structural equation
modeling. They can be applied with, although they require more calculation:
1. Forward Selection—Start with a disconnected graph and keep adding
edges until the new model ﬁts no better than the current model.
2. Backward Selection—Start with the saturated (completely connected)
graph and keep removing edges until the new model ﬁts substantially
worse than the current model.

356
10 Critiquing and Learning Model Structure
3. Forward and Backward Selection—At each stage add or remove edges to
optimize model ﬁt.
Usually these searches are done using the greedy or myopic version of
the algorithm. That is, at each stage, the best single modiﬁcation is chosen,
without trying to look ahead to the eﬀect of multiple modiﬁcations. This
strategy often works fairly well, but it can get stuck in local maxima, e.g.,
the best three single modiﬁcations considered one at a time in sequence might
not improve model ﬁt as much as the best set of three together. A better
strategy for ﬁxed computational cost is greedy search with multiple restarts
from random starting graphs (Chickering 1996).
The search can be carried out in either the space of directed or undirected
graphs, although the choice has some consequences for model equivalence
(Sect. 10.7).
10.6.2 Stochastic Search
Incorporating a probabilistic aspect into a search can help get around the
problem of local maxima. Occasionally accepting a modiﬁcation to the model
that decreases the criterion can lead to two- or three-step improvements in
the model that a greedy search would miss. Stochastic search methods accept
such modiﬁcations with a small probability, so they should get to the globally
optimum model eventually. The two most popular methods are simulated
annealing and model search MCMC.
Simulated Annealing introduces a parameter called “temperature” that
controls the rate at which changes that do not improve the model are selected.
At each cycle, the algorithm carries out the following steps.
1 Randomly select a change in graph, and evaluate the criterion for both
the old and new models. Let δE be the change in the ﬁt measure (the
name suggests that it is the change in energy of the system).
2a If change improves the model, always accept.
2b If change does not improve the model, accept with probability based on
temperature, often e−δE/T .
The temperature is started at a high value (almost all changes accepted)
and then slowly “cooled” as time goes on (like annealing a metal). Depending
on the algorithm, it can be reheated and cooled several times. High tem-
peratures help avoid local maxima, while low temperatures cause it to climb
into local maxima. When the temperature is zero, the procedure is a greedy
algorithm.
The acceptance criteria in Step 2 looks similar to the formula in the
Metropolis–Hastings algorithm, and was in fact derived from the Metropo-
lis formula. Model Search MCMC extends the MCMC algorithm with a model

10.6 Model Selection
357
selection step. At the beginning of each cycle, propose a “step” (simple mod-
iﬁcation) in model space. Then use the Metropolis–Hastings rule to accept
or reject the modiﬁcation. There is a technical complication here in that the
Metropolis–Hastings algorithm requires a reversible jumping rule, that is, it
must be possible to calculate the probability of stepping forward to the new
model and backward to the old one. Liu (2001) discusses reversible jumping
rules. Madigan, Gavrin, and Raftery (1995a) implement the procedure for
Bayes net model search.
10.6.3 Multiple Models
The typical model selection algorithm ﬁnds a single best model, and then
proceeds to make predictions as if that model were true. However, this ignores
an important component of uncertainty, our uncertainty due to not knowing
the correct model. Draper et al. (1987) (also Draper 1995) suggest “averaging”
predictions across multiple models.
Let Δ be some quantity of interest, such as the probability that a student
has mastered all of some subset of skills. Let Y be the data. Let Sh for h =
1, . . . , H be a collection of models, say with diﬀerent plausible instantiations
of the Q-matrix structure. Then
P(Δ|Y) =
H

h=1
P(Δ|Sh, Y)P(Sh|Y)
(10.21)
is the posterior distribution for Δ averaged across the models. Methods that
average over several models tend to have better predictive accuracy (when
measured using cross validation) than methods that rely on a single “best”
model.
Madigan et al. (1996) point out that if model selection is done through
model search MCMC, there is no reason to need to select a set of best models
before doing model averaging. Simply calculate a value of Δ at each cycle of
the MCMC loop using the model employed in that cycle, then report on its
posterior distribution across cycles—eﬀectively across the space of possible
models, each weighted by its posterior probability.
10.6.4 Priors Over Models
Most of the methods search methods above, either explicitly or implicitly,
put a uniform prior over models. Maximum likelihood procedures implicitly
weight all models equally a priori. Bayesian methods (like Bayes factors) have
a built-in penalty for model complexity, because the extra parameters in the
more complex models lead to lower likelihoods when they are integrated out.
Several
methods
for
more
structured
priors
have
been
proposed.
Heckerman et al. (1995) suggest a prior based on deviations from proposed
model, and Madigan et al. (1995a) use imaginary data from experts to con-
struct a prior. Heckerman (1998) reviews proposed priors over models.

358
10 Critiquing and Learning Model Structure
10.7 Equivalent Models and Causality
The problem of searching over the space of possible directed graphs to ﬁnd the
best model is complicated by the fact that models with diﬀerent graphs can in
some sense be equivalent. The ﬁrst possible problem lies with the direction of
edges (Sect. 10.7.1). The second problem lies with the eﬀect of unobserved and
unmodeled variables (Sect. 10.7.2). The problems are confounded if the goal
of the model selection process is to discover causal mechanisms. Section 10.7.3
discusses some of the limitations of procedures with this goal.
10.7.1 Edge Orientation
Putting priors over a collection of directed graphs encounters the problem that
certain models are reparameterizations of each other (Andersen et al. 1996),
in the sense that they produce the same joint distribution over the variables.
Figure 10.8(a)–(c) represent reparameterizations of each other. Each graph
contains the independence condition “A is independent of C given B.” How-
ever, Fig. 10.8(d) has a diﬀerent set of conditional independence conditions.
In a simple unpenalized search, the ﬁrst three models would all have the same
likelihoods. In a model averaging situation, the ﬁrst three would get three
times the weight of the fourth model even though they are essentially the
same.
Spirtes et al. (1997) introduce an extended graphical notation called partial
ancestral graphs (PAGs) to mark models that are identical in this sense. In
this notation, arrows are annotated with circles to show which edges can and
cannot be reversed without changing the model. Searching the space of PAGs
instead of the space of DAGs avoids counting the same essential model more
than once.
The motivation of much of the structure learning research is to learn causal
structure from data. If the data are a faithful3 representation of the underlying
process, then the “causal” model should be the one with the fewest arrows.
In certain cases, the direction of the arrows in minimal directed model is
apparent from the data. For example, in Fig. 10.8(d), A and C are “causes” of
B, because the directed arrows must run in that particular direction to make
the independence conditions work. If the best ﬁtting model was Fig. 10.8(a),
(b), or (c), then the causal structure would not be apparent from the data.
10.7.2 Unobserved Variables
It is important to keep in mind that causal structure learned from data is
limited by which variables are included in the data set. Unobserved or hidden
3 The term “faithful” has a precise technical deﬁnition in the literature of causal
discovery: Roughly, the d-separation relationships in the digraph correspond com-
pletely to the conditional independencies in the probability distribution.

10.7 Equivalent Models and Causality
359
a
b
c
d
Fig. 10.8 Graphs (a), (b), and (c) have identical independence structures, but
Graph (d) does not
Reprinted from Almond et al. (2006) with permission from ETS.
variables can aﬀect the causal conclusions in one of two ways: they can be
intermediate or common causes, and they can produce selection eﬀects.
Variables that are not observed can redeﬁne the meaning of directed edge.
In Fig. 10.9b, the hidden variable H presents a common cause which accounts
for the apparent relationship between A and C. When H is unobserved, the
joint distribution of A and C could be identical in the models of Fig. 10.9a,
b.
a
b
c
d
Fig. 10.9 Four graphs which could have identical distributions on the observed
variables A and C. a No hidden cause, b common cause, c intermediate cause, d
partial cause
Reprinted from Almond et al. (2006) with permission from ETS.

360
10 Critiquing and Learning Model Structure
Figure 10.9c, d present two other models that can produce the same dis-
tribution over {A, C} as Fig. 10.9a. In Fig. 10.9c, A causes H which in turn
causes C. In Fig. 10.9d, the inﬂuence of H is only partial. None of the four
are distinguishable when H is unobserved. Thus “cause” can only be deﬁned
relative to a universe of variables. The choice of that universe will inﬂuence
the causal structure that is found by a search procedure and possibly its
interpretation.
Selection bias can also limit the conclusions. This can happen when data
are obtained in a so-called observational study of an existing population, a
convenience sample, or a self-selected sample, as opposed to an experiment.
Let the variable S represent selection of a case in the data set that is being used
for model selection. Only cases in which S is true are observed. For example,
if the sample consists of all of the students in a particular classroom and either
A or C is a topic of instruction in the classroom, then the relationship between
A and C may be diﬀerent than in a general population, some of whom have
received instruction and some of whom have not.
a
b
Fig. 10.10 Selection eﬀect produces apparent dependence among observed vari-
ables. a No selection eﬀect. b Selection eﬀect
Reprinted from Almond et al. (2006) with permission from ETS.
Figure 10.10a, b illustrate two models which are once again equivalent.
If A and C are both related to the selection mechanism then there might
appear to be a relationship between them, even though they are independent
in the population at large. This example illustrates the importance of random
sampling in surveys. Controlling the selection mechanism explicitly ensures
that it is independent from the measured variables.
10.7.3 Why Unsupervised Learning cannot Prove Causality
The literature on causal discovery is ﬁlled with some rather precise math-
ematical deﬁnitions of the term “causal,” which take into account some of
the diﬃculties described above. These mathematical deﬁnitions do not always
correspond to the lay deﬁnition of causality. This can produce problems if the
results are presented or interpreted carelessly. Example 10.4 illustrates some
of the problems.

10.7 Equivalent Models and Causality
361
Example 10.4 (Educational Survey). Consider an educational survey
that asks both background questions about a subject (producing demographic
variables) and cognitive tasks designed to measure a particular proﬁciency. For
the moment, construct a data set using the background variables Gender and
Race and include a proﬁciency variable and a few cognitive tasks. Applying
a causal discovery method to this data set would likely result in a directed
graph like the one in Fig. 10.11.
Fig. 10.11 A minimal graph which should not be interpreted as causal
Reprinted from Almond et al. (2006) with permission from ETS.
This model says that there is an association (in this sampled population)
between Gender and Proﬁciency and between Race and Proﬁciency. However,
there are far too many factors excluded from the model to hope to make causal
conclusions.
Suppose an additional demographic variable, Parent’s Education, was
included. This would likely result in a model like that of Fig. 10.12. Here,
the new variable explains some, but not all, of the association between Race
and Proﬁciency.
This example treats in a superﬁcial way a truly thorny educational prob-
lem, the achievement gap. Barton (2003) takes a more thorough look. The
research summarized in there includes not only observational studies that
characterize the gap and highlight potential causes, but also experimental
studies that conﬁrm or refute potential causes.
Causality research is an important motivation for statistical learning, but
causality cannot be proven by statistics alone, especially from observational
studies. At best an observational study can make us suspect that a factor
is a cause of an observed problem and suggest research to follow up on it.

362
10 Critiquing and Learning Model Structure
Fig. 10.12 Inclusion of an additional variable changes picture dramatically
Reprinted from Almond et al.(2006) with permission from ETS.
Model search can provide abductive evidence suggesting that a factor may be
a cause, but before accepting that factor as “causal” we would want conﬁrming
evidence and an understanding of the causal mechanism.
Learning causality from data has been a raging debate in the statistics
community for as long as there has been data analysis. For those who want
to know more about this topic, Holland (1986) on Rubin’s model and Shafer
(1996) are good places to start—then, on to Pearl (2009)!
10.8 The “True” Model
This chapter started with the idea that model ﬁt statistics could help us
explore both how and how well a model ﬁt a given set of data, and progressed
to searching for the model that best ﬁts the data. This idea of model search
might lead one to believe that there was a “true” model we are searching
for. However, unless the data are generated through simulation, the truth will
never be known.
We began the chapter with George Box’s maxim, “All models are false,
but some are useful.” Models are useful if they provide an explanation of a
complex phenomenon, predict future observations, or support better practical
decisions, such as instructional treatment. Generally speaking, simpler models
provide better explanations, and often better predictions. For this reason, it is
important to evaluate the predictive power of models using cross validation.
Model checking becomes especially important when there is an underlying
cognitive model that the graphical model is designed to represent. In this case,
feedback about how well the mathematical model ﬁts may supply insight into
how well the cognitive model ﬁts. This may in turn lead to new hypotheses

10.8 The “True” Model
363
about the underlying cognitive structure and new experiments to validate
those hypotheses.
Exercises
Williamson (2000) proposed a model, shown in Fig. 10.13a for a hypotheti-
cal physician licensure exam using simulated patients. Each simulated patient
required a diﬀerent combination of skills as shown in the graph, and the can-
didate’s care for each simulated patient was judged on a four point scale.
Suppose that the model in Fig. 10.13a, Model A, is closer to the true cogni-
tive demands of the simulated patient tasks, but that the model of 10.13b,
Model B, is proposed to draw inferences from the assessment. (Model B was
created by deleting the TreatmentPlan node, shown in gray, from the original
model.)
To evaluate the two models, Table 10.1 presents the outcomes from three
hypothetical students who took the assessment. Both Bayesian network mod-
els were used to predict the probability of each outcome given all of the others.
Table 10.1 shows the result. Refer to these models in the following exercises.
a
b
Fig. 10.13 Two candidate models for a hypothetical medical licensure assessment.
a Model A. b Model B
Reprinted with permission from ETS.

364
10 Critiquing and Learning Model Structure
Table 10.1 Actual and predicted outcomes for the hypothetical medical licensure
exam.
Each row of the table gives (a) the observation for one simulated student on one
patient (b) the prediction under Model A given all of the observations except the
one in the current row, (c) the prediction under Model B. (Data are simulated
from the model used in Williamson (2000))
Student Patient Observed
Model A
Model B
Number Outcome 1
2
3
4
1
2
3
4
1
1
4
.10 .15 .35 .40 .10 .15 .34 .41
2
4
.17 .22 .31 .30 .17 .23 .31 .29
3
2
.08 .16 .21 .55 .06 .12 .22 .60
4
3
.06 .22 .36 .36 .10 .16 .21 .53
5
3
.26 .18 .26 .30 .15 .25 .25 .35
2
1
1
.24 .24 .35 .17 .18 .23 .39 .20
2
2
.63 .18 .12 .07 .59 .20 .13 .08
3
2
.43 .23 .24 .10 .36 .25 .24 .15
4
1
.10 .38 .34 .18 .12 .35 .21 .32
5
3
.44 .19 .17 .20 .56 .22 .11 .11
3
1
3
.14 .17 .41 .28 .10 .17 .47 .26
2
4
.28 .29 .27 .17 .23 .28 .29 .20
3
3
.13 .21 .30 .36 .09 .18 .35 .38
4
2
.07 .29 .43 .21 .10 .44 .24 .22
5
2
.22 .20 .25 .33 .33 .41 .16 .10
10.1 (Person-By-Observable Fit Indicators for Hypothetical Medi-
cal Licensure Exam, Model A). Using the data from Table 10.1 calculate
Weaver’s surprise index, the logarithmic score, Good’s logarithmic score and
the ranked probability score for each observable (patient) for each simulee
using the predictions from Model A. Which observables are ﬂagged by which
ﬁt index?
10.2 (Person-By-Observable Fit Indicators for Hypothetical Medi-
cal Licensure Exam, Model B). Using the data from Table 10.1 calculate
Weaver’s surprise index, the logarithmic score, Good’s logarithmic score and
the ranked probability score for each observable (patient) for each simulee
using the predictions from Model B. Which observables are ﬂagged by which
ﬁt index?
10.3 (Person Fit Indicators for Hypothetical Medical Licensure
Exam). Using the data from Table 10.1 calculate the average Weaver’s sur-
prise index, the logarithmic score, Good’s logarithmic score and the ranked
probability score for each simulee using the predictions from both models
Model B. Which sets of results seem the most consistent with which model?
[Hint: Use the results from Exercises 10.1 and 10.2.]
10.4 (Task Fit Indicators for Hypothetical Medical Licensure Exam).
Using the data from Table 10.1 calculate the average Weaver’s surprise index,

10.8 The “True” Model
365
the logarithmic score, Good’s logarithmic score and the ranked probability
score for each task (simulated patient) using the predictions from both mod-
els. Which sets of results seem the most consistent with which model? [Hint:
Use the results from Exercises 10.1 and 10.2.]
10.5 (Model Fit Indicators for Hypothetical Medical Licensure Exam).
Using the data from Table 10.1 calculate the average Weaver’s surprise index,
the logarithmic score, Good’s logarithmic score and the ranked probability
score across both simulees and tasks using the predictions from both models.
Which model seems to ﬁt the observed results best?
10.6 (Calculating Log Score for Conﬁguration). Suppose that we are
given a Bayes net for an assessment and a vector of observed outcomes. What
is a fast way of calculating the logarithmic score for that observation vector?
10.7 (Bayes Factor Computation). Suppose that ten students take the
hypothetical medical exam, and suppose we calculate the logarithm score for
each candidate under both Model A and Model B. The results are given in
Table 10.2. Calculate a Bayes factor for comparing Model A to Model B.
Which model seems to ﬁt the data better?
Table 10.2 Logarithmic scores for ten student outcome vectors
Model
Candidates
1
2
3
4
5
A
3.167 2.252 2.780 2.241 2.152
B
2.936 2.117 3.022 2.019 2.097
Model
Candidates
6
7
8
9
10
A
3.377 2.652 2.423 3.167 2.932
B
3.066 2.682 2.564 3.481 2.807
10.8 (DIC Calculation). Shute et al. (2008) performed a small-scale ﬁeld
trial of the Bayesian network-based system ACED. We subsequently cal-
ibrated the model. We looked at two potential prior distributions. The
“anchored” prior constrained certain sets of tasks to have average diﬃculty
and discrimination parameters of 0 and 1 respectively. The “unanchored”
prior did not add these constraints. After a suitable burn-in, we recorded the
deviance from every cycle and calculated the deviance at the average param-
eter values (across all cycles). The results are shown in Table 10.3.
Calculate the eﬀective dimensionality, pD for both models. Which model
has the higher dimensionality? Why? Calculate the DIC model. Which model
is a better ﬁt to the data?

366
10 Critiquing and Learning Model Structure
Table 10.3 Deviance values for two ACED models
Model
D(ω)
D(ω)
Anchored
15111.68 15055.45
Unanchored 14495.74 14402.80
10.9 (Local Dependence Test). Consider two random variables X and Y ,
each of which can take on the values 1 or 0. Suppose that a sample of size
n is collected from the joint distribution, and let nij be the number sampled
individuals for which X = i and Y = j. The odds ratio is deﬁned as:
odds(X, Y ) = n11n00
n01n10
.
(10.22)
What is the expected odds ratio when X and Y are independent? Explain
how the odds ratio could be used as a test for local dependence between X
and Y . (Hint: think about the Mantel–Haenszel test.)
10.10 (Odds Ratio). Consider an assessment that contains two observables,
X1 and X2 both of which can take on values correct and incorrect. Sup-
pose further that both observables are conditionally independent of the other
proﬁciency variables given the value of Skill. Suppose that an assessment con-
taining these observables was given to 100 student and each student was clas-
siﬁed on Skill based on the observables other than X1 and X2. Tables 10.4(a),
(b), (c) give the conditional pairwise relationships among the variables in the
observed responses, and Table 10.4(d) gives the marginal observation for the
two variables.
Calculate the odds ratio for each conditional table and the one for the
overall table. Are they consistent? Explain the diﬀerences.
10.11 (A Very Simple PPMC Example). Suppose we have ten obser-
vations y = {−1.8, 0.0, 6.0, −0.7, 1.4, −1.2, 0.3, −1.1, 0.9, 1.2, 0.5} from a pre-
sumed normal distribution with unknown mean μ and known variance 1. The
sample mean is ¯y = .5. The sample standard deviation s = 2.2. With a
noninformative prior, the posterior for μ is N(.5, .1). To generate a replicate
data set, ﬁrst take a draw from for μ from N(5, .1), say μrep. Then generate
a yrep by taking ten independent draws from the predictive distribution of
y given μ = μrep, which under the stated assumptions is N(μrep, 1). Writ-
ing a program or using a spreadsheet, create 20 replicate data sets. For each
such replicate set, calculate statistics such as ¯yrep and srep, and the highest
and lowest observations. Look at their distribution over the replication sets.
Compare the observed data to the replicates.
Create a “measure of ﬁt” for each observation, both real and replicate,
as the squared distance from the corresponding sample mean. Compare the
squared deviations of each observed data point with the distribution of the
squared deviates of its replicate counterparts. (Note that the generating dis-

10.8 The “True” Model
367
Table 10.4 Observed outcome for two items for Exercise 10.10
(a) Skill=high
Correct Incorrect
Correct
19.00
11.00
Incorrect
7.00
5.00
(b) Skill=medium
Correct Incorrect
Correct
4.00
10.00
Incorrect
5.00
12.00
(c) Skill=low
Correct Incorrect
Correct
1.00
5.00
Incorrect
4.00
17.00
(d) All students
Correct Incorrect
Correct
24.00
26.00
Incorrect
16.00
34.00
tributions of all ten replicates yi are the same, but the distributions of the
draws will vary due to sampling.)
Repeat the exercise for 1000 replicate data sets, and compare the results.
10.12 (DTF and DIC). Explain how the DIC statistic could be used to test
for diﬀerential task functioning.
10.13 (OCP for Word Problem). A certain mathematics assessment con-
tains a number of word problems where the student needs to ﬁrst create
an algebraic expression from an English language description of a problem
and then solve the algebraic expression. According to the design of the test,
each word problem task requires two skills to solve: BuildAlgebraicExpression,
SolveAlgebraicExpression. Both variables are coded 0 (low proﬁciency) and 1
(high proﬁciency). However, because the population to which the assessment
will be given includes a number of English language learners, to evaluate the
assessment is it given along with a general test of English language proﬁciency.
This produces an additional EnglishLanguage proﬁciency variable, also coded
0–1.
Figure 10.14 shows an observable characteristic plot for one of the word
problem tasks for this assessment. Is there a problem which should be brought
to the attention of the task writers? What is the best form of the conditional
probability table for this observable?
10.14 (OCP for DTF). A researcher is interested in testing the fairness
of a certain test toward a group with which there has been a history of dis-
crimination. To assess the fairness of the test, the research administers the
selects 100 random students from the focal group and 100 random students
from the reference group. The proﬁciency variable Skill1 has ﬁve levels. The
research classiﬁed the students into the skill levels on Skill1 and produced
the observable characteristic plot shown in Fig. 10.14. Does this plot provide
enough evidence of diﬀerential task functioning to warrant further investiga-
tion? Justify your conclusions.
10.15 (Computer Skills and CAT). A researcher is interested in whether
or not prior computer experience eﬀects a student’s score on a new computer

368
10 Critiquing and Learning Model Structure
Plot to check for dependence on Reading (Skill 3)
Groups
Probability
−
−
−
−
−
−
+
+
0.0
0.2
0.4
0.6
0.8
1.0
(0,0,1)
(0,1,1)
(1,0,1)
(1,1,1)
High Reading
(0,0,0)
(0,1,0)
(1,0,0)
(1,1,0)
Low Reading
−
+
Fig. 10.14 Observable characteristic plot for Exercise 10.13
The skills in this plot, in order of the tuples are BuildAlgebraicExpression and
SolveAlgebraicExpression (both of which are designed to be part of the task) and
EnglishLanguage (which was not designed to be an explicit part of the task).
Reprinted with permission from ETS.
Observable Characteristic Plot for Differential Task Functioning
Groups
Probability
1R
1F
2R
2F
3R
3F
4R
4F
5R
5F
0.0
0.4
0.8
(L1, F)
(L2, F)
(L3, F)
(L4, F)
(L5, F)
Focal
(L1, R)
(L2, R)
(L3, R)
(L4, R)
(L5, R)
Reference
L1
L2
L3
L4
L5
Fig. 10.15 Observable characteristic plot for Exercise 10.14
This plot shows one Skill variable and one group membership variable. The levels of
the skill are marked 1, 2, 3, 4, 5 and the group membership is marked R (reference
group) and focal group. Reprinted with permission from ETS.

10.8 The “True” Model
369
delivered assessment. Because the researcher has a limited budget, subject
recruitment is done by putting up posters in the computer center and oﬀering
free pizza to the ﬁrst 100 students who agree to take the assessment. Par-
ticipating students both take the new assessment and answer a questionnaire
about prior computer experience. On the basis of these data, the research con-
cludes that there is a small but not signiﬁcant (p = .17) eﬀect of prior com-
puter experience on the assessment. Are these conclusions justiﬁed? Explain.
10.16 (Quit Smoking). Freedman et al. (1980) describe the following obser-
vational study about smoking: “In 1964, the Public Health Service studied the
eﬀects of smoking on health, in a sample of 42,000 households. For men and
for women in each age group, they found that those who had never smoked
were on average somewhat healthier than the current smokers, but the current
smokers were on average much healthier than former smokers.”
1. Draw a graph which represents the variables in this study. Include possible
hidden variables. Hint: Include variables for currently smoking and who
smoked in the past.
2. Upon reading the study somebody suggests, that “The study indicates
that smokers should continue to smoke to avoid health problems.” Is this
conclusion justiﬁed by the study?

11
An Illustrative Example
The focus of Part II has been how to build the Bayesian networks. Chapter 8
discussed the issue of how to choose parameterizations for the conditional
probability tables that quantify the network. Chapter 9 introduced several
techniques for learning the parameters of Bayes nets given a body of assess-
ment data. Chapter 10 suggested several techniques for evaluating how well a
proposed network ﬁts the data. This chapter reviews these concepts in terms
of an example.
The example we have chosen is the mixed number subtraction example
originally collected by Tatsuoka (1983) and introduced in Sect. 6.4. This exam-
ple is ideal in many respects: It is based on a cognitive analysis of the domain.
Both the cognitive analysis and the instructional practices for the domain are
closely arranged around a distinguishable set of skills and procedures. Tasks
were explicitly built around the features of tasks that do or do not evoke those
skills. It has a substantial sample of data collected on a representative sample
of the population. It is designed to assess, namely, middle school students cur-
rently learning mixed number subtraction (more items, to better determine
the unobservable skill variables, would have been nice).
Also, this chapter can lean on our previous work, in particular, the original
translation of this model into a Bayesian network by Mislevy (1994); Mislevy
(1995b), our use of Markov chain Monte Carlo (MCMC) to estimate the model
parameters for this model (Mislevy et al. 1999a), and several experiments with
model ﬁt diagnostics using these data (Yan et al. 2004; Sinharay et al. 2004;
Sinharay and Almond 2007). (Insightful analyses of Tatsuoka’s data from the
perspective of cognitive diagnosis include Close et al. (2012); de la Torre and
Douglas (2004); Henson et al. (2009); Rupp et al. (2010).)
The structure of this chapter roughly follows the sequence of that prior
research. Section 11.1 discusses the issues involved in building the Bayes net
for this problem and choosing a parameterization. Section 11.2 discusses using
the MCMC algorithm to calibrate the model to the test data, as well as the
issues of linking multiple forms. Section 11.3 explores what can be learned from
c⃝Springer Science+Business Media New York 2015
371
R. G. Almond et al., Bayesian Networks in Educational Assessment,
Statistics for Social and Behavioral Sciences, DOI 10.1007/978-1-4939-2125-6 11

372
11 An Illustrative Example
the model checking procedures and proposes an alternative parameterization
of the evidence model designed to ﬁx some of the problems.
11.1 Representing the Cognitive Model
Building a Bayesian network to represent a cognitive model proceeds in two
steps. The ﬁrst step (Sect. 11.1.1) is to deﬁne the variables and their condi-
tional dependence relationships; that is, the graphical structure of the model.
The second step (Sect. 11.1.2) is to choose a parameterization and a prior
distribution for the parameters. Section 11.2 then takes up the story of how
to adjust the parameters in light of data.
11.1.1 Representing the Cognitive Model as a Bayesian Network
Tatsuoka (1984) developed an assessment of mixed number subtraction skills
following a cognitive analysis of the domain (Klein et al. 1981). As in Sect. 6.4,
we restrict our attention to students who are using Method B (separate num-
bers into whole and fractional parts), and the 15 items that did not involve
ﬁnding a common denominator (allowing us to use a simpler model). Using
this method, the 15 problems can be solved using the following ﬁve skills:
Skill 1: Basic fraction subtraction.
Skill 2: Simplify/reduce fraction or mixed number.
Skill 3: Separate whole number from fraction.
Skill 4: Borrow one from the whole number in a given mixed number.
Skill 5: Convert a whole number to a fraction.
Students are characterized by a vector (θ1, ..., θ5) of binary variables, each
component indicating whether a student does or does not have Skill j. There
are relationships among these skills we will want to incorporate in the proﬁ-
ciency model. For example, the prerequisition relationship among skills high-
lighted in the attribute hierarchy model1 (Leighton et al. 2004; Gierl et al.
2007) holds implications for task design and inference about examinees. We
will see some of them in this example.
Skill 3 is a logical or strong prerequisite for Skill 4; it is not possible
to borrow one from a whole number to add to a fraction unless one can
distinguish and separate these parts of a mixed number. Thus, P(Skill 4 =
Yes|Skill 3 = No) = 0. One way to incorporate this knowledge is to build it
into a conditional probability matrix for Skill 4 given Skill 3. Alternatively,
we will express this relationship as we did in Sect. 6.4 by introducing a skill
MixedNumber, denoted θMN, with three possible states: (0) neither Skill 3
1 The rule space literature calls the aspects of knowledge, skill and ability attributes.

11.1 Representing the Cognitive Model
373
nor Skill 4 present, (1) Skill 3 present but Skill 4 absent, and (2) both Skill 3
and Skill 4 present. Skills 3 and 4 are thus logical children of MixedNumber.2
There are not strong prerequisition relationships between any of the other
variables, but there are relationships we should model. They are called weak
prerequisites because they are probabilistic rather than deterministic, even
though they could be strong empirically. For example, a student can know
how to reduce fractions (Skill 2) whether or not she can subtract fractions
(Skill 1), but in this population, subtracting fractions is taught before reduc-
ing fractions. Most students who have Skill 1 are likely to have Skill 2 than
those who do not. In the Bayes net proﬁciency model fragment, we will model
Skill 2 as a child of Skill 1 (Fig. 11.1).
Skill 1 is particularly important in this application, because the unit’s
focus is subtracting mixed numbers in a variety of circumstances. It may be
logically possible for a student to be able to reduce fractions without being
able to subtract simple fractions, but that’s irrelevant to the purpose of this
assessment. If Skill 1 is absent, then the status of the other variables is unim-
portant; the instructional prescription would be the same in all cases: concen-
trate on basic fraction subtraction skills. We will see that this purpose holds
implications for the mix of tasks as well. Skills 1, 2, 5, and mixed numbers
are ordered in the Bayes net roughly according to the order that the skills
are introduced, so that we can model weak prerequisite relations we might
expect—and information in data can reﬁne them, eliminate them, or counter
our expectations.
Fig. 11.1 Proﬁciency model for mixed-number subtraction, method B
Reprinted from Sinharay et al. (2004) with permission from ETS.
2 See von Davier and Haberman (2014) on implications of the alternative represen-
tations.

374
11 An Illustrative Example
Table 11.1 presents the 15 items used in the example. (To simplify the
model, ﬁve tasks requiring ﬁnding a common denominator, Items 1, 2, 3,
5, and 13 have been omitted.) It includes the Q-Matrix which indicates which
skills are used in each task: If Skill j is required for Observable i, then qij = 1,
otherwise it is equal to zero. Note that all of the items require Skill 1. This
assessment cannot provide evidence to support the hypothesis that a student
has, for example, Skill 3 but not Skill 1, or any other combination of skills
for which Skill 1 = No. But as mentioned above, these distinctions are not
germane to the instructional decision at hand.
Table 11.1 Skill requirements for fraction subtraction items
Skills
required
Item
Text
1 2 3 4 5 EMa
6
6
7 −4
7 =
1 0 0 0 0 1
8
3
4 −3
4 =
1 0 0 0 0 1
12
11
8 −1
8 =
1 1 0 0 0 2
9
3 7
8 −2 =
1 0 1 0 0 3
14 3 4
5 −3 2
5 =
1 0 1 0 0 3
16 4 5
7 −1 4
7 =
1 0 1 0 0 3
4 3 1
2 −2 3
2 =
1 0 1 1 0 4
11 4 1
3 −2 4
3 =
1 0 1 1 0 4
17
7 3
5 −4
5 =
1 0 1 1 0 4
18 4 1
10 −2 8
10 = 1 0 1 1 0 4
20 4 1
3 −1 5
3 =
1 0 1 1 0 4
7
3 −2 1
5 =
1 0 1 1 1 5
15
2 −1
3 =
1 0 1 1 1 5
19
7 −1 4
3 =
1 0 1 1 1 5
10 4 4
12 −2 7
12 = 1 1 1 1 0 6
aThis column classiﬁes items with respect to the unique skill patterns they require,
which correspond to Evidence models
Recall that a graph can also be represented as a matrix, with a 1 in the
cell indicating that there should be an edge between the nodes. Doing this
for each unique row in the Q-matrix produces the evidence model fragments,
as shown in Fig. 11.1.1. To make the full Bayes net for the assessment, the
Evidence model fragments produced from the Q-matrix must be combined
with the proﬁciency model in Fig. 11.1. The Evidence model fragments are
replicated (substituting the observable variables) for each item that uses that
Evidence model. This produces the network shown in Fig. 11.1.

11.1 Representing the Cognitive Model
375
Skill 1
Skill 3
Item 9
Skill 1
Skill 3
Skill 4
Item 4
a
b
Fig. 11.2 Evidence model fragments for evidence models 3 and 4
a Evidence model 3 (Skills 1 and 3). b Evidence model 4 (Skills 1, 3, and 4)
Reprinted from Mislevy et al. (2000) with permission from The National Center for
Research on Evaluation, Standards, & Student Testing (CRESST), UCLA.
Fig. 11.3 Mixed number subtraction Bayes net
Reprinted from Almond et al. (2006a) with permission from ETS.
The Q-Matrix is a summary representation of the collection of Evidence
models for this assessment. As the tasks for this assessment all have a single
observable and it depends conjunctively on its set of parent skills, each row of
the Q-matrix corresponds to a task-speciﬁc Evidence model. Some rows are
identical. The tasks in those rows are isomorphs:3 the mathematical struc-
ture, and hence the skills require to solve the problem, are identical, but the
individual tasks may use diﬀerent numbers in the conditional probability dis-
tributions. The task models and evidence models for this assessment thus
correspond to the unique rows. There are six diﬀerent unique task types. The
3 Items that are isomorphic under one method need not be isomorphic under the
other. Mislevy (1995b) gives a Bayes net for when it is not known whether a
student is using Method A or Method B.

376
11 An Illustrative Example
numbers in the last column of Table 11.1 identify the Evidence model for each
task.
The Q-matrix therefore shows for each proﬁciency proﬁle, which evi-
dence/task models should result in correct outcomes, and which should have
incorrect outcomes. Table 11.2 shows these ideal response patterns. The con-
ditional probability matrices will characterize, for each task, the tendency of
students to make careless errors even if they have all the required skills and
to occasionally guess the answer even if they lack one or more of those skills.
Table 11.2 Equivalence classes and evidence models
Equivalence Evidence model Class
class
1 2 3 4 5
6
description
1
No Skill 1
2
x
Skill 1 only
3
x
x
Skills 1 & 3
4
x
x x
Skills 1, 3, & 4
5
x
x x x
Skills 1, 3, 4, & 5
6
x x
Skills 1 & 2
7
x x x
Skills 1, 2, & 3
8
x x x x
x
Skills 1, 2, 3, & 4
9
x x x x x
x
All Skills
An “x” in a given cell indicates that students in the equivalence class corresponding
to the row are expected respond correct to tasks from the task/evidence model indi-
cated corresponding to the column. For example, a student in Equivalence Class 8
has Skills 1, 2, 3, and 4 and is expected to make a correct response for the tasks
using evidence models 1, 2, 3, 4, and 6, but make an incorrect response for tasks
from evidence model 5
Note that there are only nine unique patterns of expected outcomes, even
though there are 24 unique proﬁciency proﬁles associated with the graph in
Fig. 11.1. We call the proﬁciency proﬁles that all give rise to the same expected
response proﬁle an equivalence class. The ﬁrst equivalence class contains all
twelve proﬁciency proﬁles in which Skill 1 is missing. It would be possible
to make distinctions among these proﬁles, by adding tasks for which students
with diﬀerent proﬁles within the equivalence had diﬀerent conditional response
probabilities. For example, “Reduce 6
8 to lowest terms” requires Skill 2 but
not Skill 1. Such items distinguish among students who do and do not have
Skill 2 regardless of their standing on Skill 1. As Chap. 7 discussed, these test
assembly considerations are driven by the hypotheses that are pertinent to test
use, and the use of this test places no utility on hypotheses that distinguish
among patterns in the “Skill 1 = No” equivalence class.
Most of the other equivalence classes contain only one proﬁciency proﬁle.
The exceptions are Classes 3, 6, and 7, both of which contain two proﬁles,
one with Skill 5 one without Skill 5. The diﬃculty is that there are no tasks

11.1 Representing the Cognitive Model
377
that require Skill 5 that do not also require Skill 4. A closer look at the items
(or better yet, reading of Klein et al. (1981)) reveals that in the context of
this assessment, Skill 5 really means being able to move to a mixed num-
ber representation in the special case of the minuend being a whole number.
Understanding how to do this converts the problem to one more like those in
Evidence models 4 and 6. This is why there are no tasks that require Skill 5
but not Skill 4. It is a situationally deﬁned skill, but one that holds important
educational value in this unit: The state of having Skills 1, 3, and 4 but not
Skill 5 triggers instruction in what to do in this special case.
Over the years, rule space theory (Tatsuoka 1983; Tatsuoka 1984; Tat-
suoka 2009; Tatsuoka and Tatsuoka 1989; Tatsuoka 1990; Tatsuoka 1995) and
the attribute hierarchy method (Gierl et al. 2007; Leighton and Gierl 2007);
Leighton et al. 2004 have developed similar methods for analyzing test forms
and their relationships to proﬁciency proﬁles. Although the notation diﬀers
from the Bayes net notation and the subsequent inferential approaches may
diﬀer, there is much to be gained from such analysis. It is relatively sim-
ple to translate between the Q-matrix and the graphical structure (Sect. 5.5,
Almond 2010a). We can learn quite a lot about the strengths and weakness
of an assessment design before we even consider student responses to the
question.
11.1.2 Representing the Cognitive Model as a Bayesian Network
Ideally, students with a given proﬁciency proﬁle should respond according to
the pattern implied by the Q-matrix. They should respond correctly to any
item for which they possessed all the skills it required, and incorrectly to any
item that required one or more skills they did not possess. Real students do
not behave like mathematical models. Sometimes students miss items they
“should” get right, and get others right when they “should not.” False neg-
ative responses are errors due to slips and errors of execution, while false
positive responses include lucky guesses and answers that happen to be right
even though they were obtained through ﬂawed reasoning. The goal of the
diagnostic assessment is to classify them into one of the equivalence classes,
despite the deviation from “ideal” behavior.
It is here that the Bayesian and rule space approaches part company. The
Bayesian approach starts by creating a prior or population distribution over
the possible proﬁciency proﬁles (a proﬁciency model) and the likelihood of
a positive or negative observed outcome given the proﬁciency proﬁle. After
the outcomes of several tasks are observed, this generative probability model
can be inverted using Bayes theorem to draw inferences about the proﬁciency
proﬁle (Chap. 5). Furthermore, the generative model provides a probability
for observing the actual data. This can be used to evaluate the ﬁt of the model
to the data (Chap. 10).
The rule space method uses a pattern matching approach to classify stu-
dents. Students are classiﬁed into the proﬁciency proﬁle whose ideal response

378
11 An Illustrative Example
pattern most closely matches the observed response pattern. (For a more com-
plete explanation, see Tatsuoka and Tatsuoka 1989 or Tatsuoka 1990.) The
rule space method does not explicitly deﬁne a generative model for the observ-
able outcome variables. This means that most of the methods of Chap. 10
cannot be used to evaluate the ﬁt of the model. Further, coherent probability-
based inferences about students through a model are not available. It is not
obvious how to estimate parameters, exploit collateral information about tasks
and students, deal with missing data, improve the model as data accrues, or
extend the analysis to new tasks. For all these reasons, this chapter focuses
on the Bayesian network application.
To build the Bayesian model for the mixed number subtraction problem,
deﬁne the following variables:
Deﬁne the observable outcome variable Xij to be 1 if the response of Exam-
inee i to Item j is correct and 0 if incorrect. Let Xi = (Xi,1, . . . , Xi,J),
where J is the number of tasks in the assessment, be the outcome vector
for Examinee i.
Deﬁne the proﬁciency variable θik to be 1 if Examinee i possesses Skill k
and 0 if not. Deﬁne the proﬁciency proﬁle for Examinee i as θi =
(θi,1, . . . , θi,5).
The following additional notation will simplify the notation of later expres-
sions:
Deﬁne δi,s(j) to be 1 if Examinee i possesses all of the skills required by
items in Evidence model s and 0 if not. The notation s(j) is a reminder that
which Evidence model is appropriate for Task j is determined by Row j
of the Q-matrix. Note that the value of δi,s(j) is completely determined by
the value of θi.
Deﬁne Js as the set of indices for the tasks that are scored using Evidence
model s, and let S be the number of evidence models in the assessment.
At this point we can write the joint distribution of θi and Xi as
P(θi, Xi) = P(θi)
J

j=1
P(Xij|θi) = P(θi)
S

s=1

j∈Js
P(Xij|δi,s(j)) .
(11.1)
In Eq. 11.1, P(θi) is the proﬁciency model and P(Xij|δi,s(j)) is (the sta-
tistical part of) the evidence model for Item j. So far, this chapter has been
a review of material covered in Sect. 6.4. However, ﬁtting the model to the
observed data requires ﬁrst specifying a parameterization for the proﬁciency
and evidence models and a prior distribution for those parameters. These are
developed below.

11.1 Representing the Cognitive Model
379
11.1.3 Higher-Level Structure of the Proﬁciency Model; i.e.,
p(θ|λ) and p(λ)
The proﬁciency model should be properly thought of as a distribution of pro-
ﬁciency proﬁles in the population of likely test-takers. Although the domain
experts may have some initial guesses as to which skills are common and
which are rare, we almost certainly want to reﬁne those guesses with observed
data. To reﬁne the proﬁciency model in a Bayesian context, the experts initial
guesses must be expressed as prior distributions over the proﬁciency model
parameters.
Mislevy et al. (1999a) produced a parameterization for the Bayesian net-
work in Mislevy (1995b). They started with a standard assumption that all
examinees are exchangeable (before we have observed response data) and
therefore a single probability distribution p(θ|λ), and a single set of parame-
ters, λ, is suﬃcient to describe our beliefs about the proﬁciency proﬁle θi for
any Examinee i.
The independence assumptions implicit in the Bayesian network simplify
the task of specifying the joint distribution. With a Bayesian network it is
suﬃcient to specify for each proﬁciency variable, θk, the distribution for that
variable conditioned on its parents in the graph (Fig. 11.1).
Mislevy et al. (1999a) used a collection of hyper-Dirichlet distributions
(Sect. 8.3) to specify the conditional probability tables. Actually, because all
of the variables representing skills are binary, only a single parameter is neces-
sary for each row of the conditional probability table, namely the probability
of the skill being present given the state of the parent skills. Modeling the
prerequisite relationship between Skills 3 and 4 requires a deviation from this
pattern. As mentioned above, Mislevy (1995b) modeled this relationship by
introducing a new variable θMN (“MN” for Mixed Number) which takes on
three states: 0 for neither Skill 3 nor Skill 4, 1 for Skill 3 only, and 2 for both
skills. The conditional probability table for this variable has a conditional
multinomial distribution; that is, given the joint state of the parent variables
(Skills 1, 2 and 5), the distribution of θMN is multinomial. Skills 3 and 4 then
have logical distributions: conditional probability tables whose entries are all
zeros and ones, indicating which of the two skills are mastered.
The parameterized proﬁciency model, P(θ|λ) can be expressed in the fol-
lowing series of equations:
θ1 ∼Bern(λ1)
θ2 |θ1 = z ∼Bern(λ2z)
for z = 0, 1.
That is, there may be diﬀerent probabilities of having Skill 2 depending
on whether a student does or does not have Skill 1; those probabilities are λ20
and λ21, respectively.

380
11 An Illustrative Example
θ5 |θ1 + θ2 = z ∼Bern(λ5z)for z = 0, 1, 2.
That is, there may be diﬀerent probabilities of having Skill 5 depending
on whether a student has Skills 1 and 2; we allow for diﬀerent probabilities
depending on how many of them the student has: λ50 if neither, λ51 if just
one of them, and λ52 if both.
θMN |(θ1 + θ2 + θ5 = z) ∼Categorical(λMN,z,0, λMN,z,1, λMN,z,2),
for z = 0, 1, 2, 3.
As above, the probabilities for θMN are modeled as depending on Skills 1,
2, and 5, with only the count of those mastered being relevant.
θ3 = 0
if θMN = 0;
θ3 = 1
if θMN = 1 or 2.
θ4 = 0
if θMN = 0 or 1;
θ4 = 1
if θMN = 2.
Mislevy et al. (1999a) proposed fairly mild but informative Beta priors
for the parameters of the Bernoulli distributions, namely, λ1, λ20, λ21, λ50,
λ51, and λ52. The speciﬁc prior distribution for λ1 was Beta(20, 5), indicat-
ing that in this population most of the students will possess Skill 1—the
prior mean is 80 %, and the eﬀective sample size of the prior is 25 (=20+5)
observations.4 The prior distribution for λ21 was also Beta(20, 5), indicat-
ing we would expect a student who does have Skill 1 to also have Skill 2.
However, the prior for λ20 was Beta(5, 20), indicating a student who does
not have Skill 1 also probably does not have Skill 2. Using similar reasoning,
we proposed the priors Beta(5, 20), Beta(12.5, 12.5), and Beta(20, 5) for λ50,
λ51, and λ52, respectively. For the four three-category distributions, namely,
(λMN,z,0, λMN,z,1, λMN,z,2), for z = 0, 1, 2, 3, Mislevy et al. (1999a) used anal-
ogous Dirichlet priors, with parameters that sum to 27 to indicate a weight
of 27 observations (this change is relatively small, but makes the numbers
divisible by 3), and values that reﬂect relative frequencies in each category of
θMN depending on whether a student possesses 0, 1, 2, or 3 of the Skills 1, 2,
and 5. The actual values of the priors used are as follows:
(λMN,0,0, λMN,0,1, λMN,0,2) ∼Dirichlet(15, 7, 5)
(λMN,1,0, λMN,1,1, λMN,1,2) ∼Dirichlet(11, 9, 7)
(λMN,2,0, λMN,2,1, λMN,2,2) ∼Dirichlet(7, 9, 11)
(λMN,3,0, λMN,3,1, λMN,3,2) ∼Dirichlet(5, 7, 15)
4 Mislevy et al. (1999a) used Beta(21, 6). The diﬀerence in numbers is small.

11.1 Representing the Cognitive Model
381
These numbers show diﬀerent patterns that reﬂect a mild expectation that
the more “parent” skills a student possessed, the more likely he or she was to
have a higher value on the “child” skill.
11.1.4 High Level Structure of the Evidence Models; i.e., p(π)
All of the tasks in the mixed-number subtraction test yield a single observable
outcome variable that can take on the value right or wrong, coded 1 and
0, respectively. The parents of each observable variable are given by the Q-
matrix. All that remains to complete the model is to choose a parametric
form for the conditional probability table, and a prior distribution over its
parameters.
Mislevy (1995b) proposed an all-or-nothing or DINA model (Sect. 8.4) for
the mixed number subtraction example. In this model, there are two cases of
interest: either the student has all the skills necessary to solve the problem,
δi,s(j) = 1, or the student lacks one or more of those skills, δi,s(j) = 0. An
all-or-nothing model makes the same prediction for a given item for a student
who has none of the skills it requires and a student who has some but not all
of the skills.
We expect some deviations from the ideal response patterns. A student
who has all of the required skills may still make careless errors or slips, a
false-negative response. Deﬁne πj1 as the probability that a student for whom
δi,s(j) = 1 gets the item correct (the true-positive probability). Similarly, a
student who lacks one or more of the required skills may guess the answer
or work around the lack of skill. This is a false-positive. Deﬁne πj0 as the
probability that a student for whom δi,s(j) = 0 gets the item correct (the false-
positive probability). We can express the evidence model with the following
equation:
Xij
''δi,s(j) = z ∼Bern

πjδi,s(j)

,
for z = 0, 1.
(11.2)
Transforming Eq. 11.2 into a conditional probability table (CPT) for
Item j is straightforward. Each row of the CPT corresponds to a conﬁgu-
ration of the parent variables, which correspond to either the case δi,s(j) = 0
or 1. In the former case, the probabilities in that row are 1 −πj0 and πj0. If
δi,s(j) = 1, the conditional probabilities are 1 −πj1 and πj1.
All that remains is to specify for priors for the π’s. Again, the Beta dis-
tribution is the natural conjugate. As with the proﬁciency model, we choose
beta distributions with eﬀective sample sizes of 25. The actual priors are
πj0 ∼Beta(5, 20) and πj1 ∼Beta(20, 5).
(11.3)
The mean of the true-positive prior distribution is .8, and the mean of the
false-positive prior distribution is .2. These priors are just initial guesses. We
expect, and indeed will observe, substantial changes from these prior means
to posterior means.

382
11 An Illustrative Example
11.1.5 Putting the Pieces Together
The full Bayesian probability model for all the data across all items and
examinees, and including all the parameters of higher-level distributions, can
now be written as
p(X, θ, π, λ) =

i
⎛
⎝
j
p (xij | θi, πj) p (θi | λ)
⎞
⎠p (λ)

j
p (πj) .
(11.4)
Figure 11.4 is the representation of this model as an acyclic directed graph.
The plate notation (see Chap. 8) is used to convey replication over students,
evidence models and tasks within evidence models. Note that δi,s(j) is a deter-
ministic function (double oval) of the student speciﬁc proﬁciency proﬁle and
the evidence model speciﬁc Q-matrix row. Note also that the Task plate, j is
nested within the evidence model plate, s.
i
j
Xij
i(s)
Student i
Evidence 
Model s
Task j
Fig. 11.4 Plate representation of the parameterized mixed-number subtraction
model. Reprinted with permission from ETS.
11.2 Calibrating the Model with Field Data
It is straightforward to take the full Bayesian model described in the previous
section, and plug it into a MCMC program such as WinBUGS (Lunn et al.
2000) and start turning the Bayesian crank. If we attach it to a suﬃciently
large power source (say the Three Gorges Dam) we can generate as large a

11.2 Calibrating the Model with Field Data
383
sample of draws as we like from the posterior. What do we gain by doing this
Bayesian calibration?
The ﬁrst thing gained is a new set of adjusted parameters for the model
that reﬂect what is observed in the data. Both the population distribution of
skills and the diﬃculties of the individual tasks may be diﬀerent from what we
expect. Calculating the posterior distribution for the model parameters yields
insights into how the observed data diﬀer from our predictions. Section 11.2.1
explores this application. Section 11.2.2 talks about scoring individual stu-
dents after the calibration.
The same ideas also apply when the same assessment is in use for a long
period of time. Often the test developers wish to add new tasks and retire
old tasks (usually for security reasons). It is possible that the population
characteristics and task-speciﬁc evidence model parameters will change over
time. Section 11.2.3 shows how calibration can used to link past and present
versions of the assessment.
The ﬁnal reason to sample from the posterior distribution is to evaluate
how well the model ﬁts the data, and reﬁne the model on that basis. Sec-
tion 11.3 explores some possible model checking strategies.
11.2.1 MCMC Estimation
Equation 11.4 gives the joint prior distribution of all data and parameters for
the mixed-number subtraction example. Once data X are observed, applying
Bayes theorem yields the joint posterior distribution. The posterior distribu-
tion for θ, π, and λ is proportional to Eq. 11.4, but with the Xij’s ﬁxed at
their observed values:
p (θ, π, λ | X) ∝

i
⎛
⎝
j
p (xij | θi, πj) p (θi | λ)
⎞
⎠p (λ)

j
p (πj) .
(11.5)
Using MCMC (Chap. 9), we can draw a sample from this posterior without
needing to explicitly calculate the normalization constant. Statistics of the
posterior distribution can then be approximated by sample statistics of the
posterior sample. Mislevy et al. (1999a) used BUGS (Thomas et al. 1992;
Lunn et al. 2000) to draw a sample from the posterior. This chapter presents
a reanalysis using StatShop (Almond, Yan, et al. 2006c) and the R package
coda (Plummer et al. 2006; R Development Core Team 2007). The diﬀerences
between the two analyses are minor.
MCMC analysis starts one or more Markov chains from arbitrarily cho-
sen starting points. Although the stationary distribution of these chains is
the posterior, it may take a while to reach stationarity. Therefore, a num-
ber of “burn-in” cycles are produced and discarded at the the beginning of
each chain. After that point, the distribution of a large number of draws

384
11 An Illustrative Example
for a given parameter approximates its marginal distribution, and summaries
such as posterior means and variances can be calculated. Gelman and Rubin
(1992) (also Brooks and Gelman 1998) recommend running chains from multi-
ple starting points and then looking at the ratio of between chain and within
chain variance to verify that stationary has been reached. Graphing of the
Gelman–Rubin potential scale reduction factor convergence criterion for each
parameter shows as what point the the ratio of variance among chain means
matches the pooled variance of draws for that parameter within chains. Values
of around 1.1 or 1.2 or less are considered satisfactory (we have seen values as
high as 20 in analyses with multiple posterior modes and identiﬁcation prob-
lems) as long as other evidence of nonconvergence, such as visually disparate
traces, are not present.
StatShop oﬀers six diﬀerent strategies for choosing starting values for the
MCMC chains based on the prior distribution:
Midpoint: Use the prior mean or median as starting values.
High: Use the mean plus twice the standard deviation of the prior.
Low: Use the mean minus twice the standard deviation from the test run
posterior.
HighLow: Partition the parameters into two sets “up” and “down.” For the
up set, use the mean plus twice the standard deviation; for the down set,
use the mean minus twice the standard deviation.
LowHigh: Same as Step 4, with the “up” and “down” sets reversed.
Random: Use values drawn randomly from the prior.
If we approximate the prior distribution with a multivariate normal distribu-
tion, using all of the ﬁrst ﬁve strategies picks as starting points the center of
the prior, and 4 points on the 95 % ellipsoid. The analysis shown below used
ﬁve chains using the ﬁrst ﬁve methods. In practice, three chains are often
suﬃcient, and our standard practice has evolved into using three chains with
the ﬁrst three methods.
We start ﬁve chains and run them for 3000 cycle each. We know that the
ﬁrst few cycles are not in the stationary distribution, but we hope that the
last cycles are. The next step is to decide if the chains have converged to
the stationary distribution, and if they have, how many cycle to discard as
burn-in. The Gelman–Rubin R statistic provides one measure of convergence.
We can calculate the R statistic for any window for the series (for example
each 100 cycles); the usual heuristic is to call the chains converged when the
value of R drops below 1.1. One technique useful for determining the point of
convergence is to plot the R statistic against the number of cycles (Fig. 11.5
shows this for selected parameters). We see high values at the beginning of
the run, reﬂecting the widely dispersed starting points of the ﬁve chains, and
values settling down to less than 1.1 by 500 iterations. Typically, we look at
the proﬁciency model parameters ﬁrst (e.g., Fig. 11.5a and b), as when models
have serious problems, they are likely to be apparent in lack of convergence
of the proﬁciency model parameters. If these look okay (e.g., R < 1.1 and no

11.2 Calibrating the Model with Field Data
385
clear distinctions in trace lines for diﬀerent chains), then we go to the evidence
model parameters, as occasionally one or two evidence models has diﬃculty
converging even if the rest of the model looks good.
Based on the selected parameters, it looks like any value over 500 would
be adequate since the R values are all comfortably below 1.1 by then. The
analysis below uses 1000 to be conservative.5 We conﬁrm this by calculating
the R statistic for each parameter in the model for the window of (1001, 3000).
The maximum value is 1.01, so convergence to stationarity is very likely. An
alternative would be to look at the multivariate R (Brooks and Gelman 1998)
for each set of parameters associated with each conditional probability table.
Converging to the stationary distribution is not enough; in addition, the
chain must mix well: the sample must be long enough that the chance of
visiting any possible value for the joint posterior is roughly equal to its pos-
terior probability. There are two potential problems here. First, the sample
could get “stuck” in a local maxima of the posterior and never explore other
potential maxima. Starting multiple chains from well dispersed starting val-
ues guards against that problem. The second potential problem is that the
chain moves slowly through the posterior distribution. This is the slow mixing
phenomenon discussed in Sect. 9.5.2. In this case, a large MCMC sample is
needed to produce an unbiased estimates of descriptive statistics of posterior
distributions.
The trace or history plot—a plot of the values in the time series against
time—is a robust tool for diagnosing a number of problems, including slow
mixing. Figure 11.6 shows the MCMC chain histories for selected parameters.
Values from the ﬁve chains are plotted on top of one another, so that if there
were a convergence problem, the chains would appear separate. In Fig. 11.6,
all ﬁve chain lie on top of one another; this conﬁrms the earlier ﬁndings of
convergence. Large gaps in the series, especially behavior that looks like cycles
(although they will have irregular periods), is a sign of slow mixing. If the
chains are mixing well, the trace plots will look like white noise (a series of
independent draws from a normal distribution) with no discernible patterns.
The series from the mixed-number subtraction example all look like this, so
we know the chains are mixing well.
If the chains were mixing slowly, they would need to be run longer to make
sure that they cover the parameter space in proportion to the posterior they
are trying to approximate. Note that plotting the new longer series in the
same size graphic window will compress the x-axis. One sign that the MCMC
5 Be careful here if using BUGS and the Metropolis sampling method. BUGS uses
the ﬁrst 4000 cycles to adjust the size of the proposal distribution. During the
adjustment period, the chain is not guaranteed to have the right stationary dis-
tribution, so those cases require the burn-in to be at least 4000 cycles. This is not
required in this case because: (a) With all conditional probability tables in the
model using the hyper-Dirichlet design pattern, Gibbs sampling works well, and
does not require adaptation. (b) StatShop, unlike BUGS, uses a ﬁxed proposal
distribution.

386
11 An Illustrative Example
1.0
1.1
1.2
1.3
1.4
1.5
1.0
1.1
1.2
1.3
1.4
1.5
1.0
1.1
1.2
1.3
1.4
1.5
1.0
1.1
1.2
1.3
1.4
1.5
Proficiency Model: P(Skill 1 | Skill 2 = FALSE)
last iteration in chain
shrink factor
Proficiency Model: P(Skill 1 | Skill 2 = True)
last iteration in chain
shrink factor
0
500
1000
1500
2000
0
500
1000
1500
2000
0
500
1000
1500
2000
0
500
1000
1500
2000
Link Model for Item 8: P(isCorrect | Required Skills)
last iteration in chain
shrink factor
median
97.5%
Link Model for Item 8: P(isCorrect | Required Skills)
last iteration in chain
shrink factor
median
97.5%
a
b
c
d
median
97.5%
median
97.5%
Fig. 11.5 Gelman–Rubin potential scale reduction factors for selected parameters
model (ﬁve chains; ﬁrst 2000 updates each). a R for λ2,0. b R for λ2,1. c R for π8,0.
d R for π8,1. Reprinted with permission from ETS.
sample is suﬃciently large is that the trace plot on the new compressed series
now looks like white noise.
Another way to judge how well the chains are mixing is to look at the
autocorrelation of the series—the correlation of the series with itself a previ-
ous time. The correlation between x(t) and x(t−ℓ) is called the autocorrelation
at lag ℓ. Usually, the autocorrelation is a decreasing function of lag, hopefully
it dies out fairly quickly. The autocorrelation at lag 5 is usually a pretty good
indicator of how quickly the chain is moving. Autocorrelations and mixing
behavior can be diﬀerent for diﬀerent parameters in the same problem. High
autocorrelations and slow mixing can occur when there is not much informa-
tion in the data about a parameter, a parameter is unidentiﬁed, or when sets
of parameters are nearly multicollinear.

11.2 Calibrating the Model with Field Data
387
1000
1500
2000
2500
3000
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.2
0.4
0.6
0.8
1.0
Proficiency Model: P(Skill 1 | Skill 2 = False)
Iterations
1000
1500
2000
2500
3000
Proficiency Model: P(Skill 1 | Skill 2 = True)
Iterations
1000
1500
2000
2500
3000
Link Model for Item 8: P(isCorrect | Required Skills)
Iterations
1000
1500
2000
2500
3000
Link Model for Item 8: P(isCorrect | Required Skills)
Iterations
a
b
c
d
Fig. 11.6 Histories of MCMC chains for selected parameters (ﬁve chains; cycles
1001–3000 from each chain). a Trace plot for λ2,0, b Trace plot for λ2,1, c Trace plot
for π8,0, d [Trace plot for π8,1. Reprinted with permission from ETS.
A related statistic is the eﬀective MCMC sample size of the chain (Eq. 9.21).
If this were a simple random sample from the posterior, there would be 10,000
observations (2000 each from ﬁve chains). However, the samples are correlated,
so the MCMC estimation error will be larger than that for an uncorrelated
sample of the same size. This should be at least several hundred samples. If
not, we would want to run the chain longer until the eﬀective sample size is
high enough.
The autocorrelation and eﬀective MCMC sample size are closely related.
The lowest lag 5 autocorrelation, 0.0038, and the highest MCMC sample size,
8163, appear in the conditional probability tables for Item 18. The highest
lag 5 autocorrelation, 0.4626, and the lowest MCMC sample size, 812, appear
in the conditional probability tables for Skill 5 in the proﬁciency model. These
values do not look too bad, so there is no reason to run the chain further.
One trick that is often suggested to reduce autocorrelation is to “thin”
the chain, that is to only record every 3rd, every 5th or every 10th cycle.

388
11 An Illustrative Example
Although this reduces the autocorrelation in the ﬁle that is produced, it also
reduces the size of the sample, so the result is often a net loss of eﬀective
sample size, without substantially reducing the amount of time required to
compute the sample. If the chains are very large, though, thinning will produce
smaller series that take up less storage space on the computer and are faster to
work with in postprocessing tasks such as producing summary statistics and
graphics. The loss of information in the full chains will then be outweighed
by the convenience of working with the thinned chains.
The tests we have done above were all for the purpose of convincing our-
selves that the sample of 10,000 draws (cycles 1001–3000 from each of ﬁve
chains) are, in the aggregate, a good approximation of to the posterior distri-
bution. Given that the sample looks good, what does the posterior distribution
look like?
The easiest thing to look at is the marginal distribution of each parameter.
The trace plot gives us a quick impression of the range and typical values of
each series. For example, comparing Fig. 11.6a and b reveals that λ2,1 > λ2,0,
which corresponds to our expectations. (The variance of λ2,0 is also larger;
this is discussed below). Similarly, comparing Fig. 11.6a and b reveals that
π8,1 > π8,0, again conﬁrming our modeling assumptions.
Although we can see the range of values each parameter typically takes
from the trace plot, it is often easier to estimate the density more directly.
At this point, we can pool the data from all ﬁve chains to do that estimation.
Figure 11.7 shows the posterior distributions of the selected parameters. The
probability of having Skill 2 when Skill 1 is present, λ2,1, has a fairly tight
distribution distribution centered nicely at .9. The probability of having Skill 2
when Skill 1 is not present, λ2,0, is centered at .2, with a much larger variance.
The true-positive probability of a correct response to Task 8 given δ(4) = 1 is
centered at .78, while the false-positive of a correct response when δ(4) = 0 is
centered at .34 with about twice the variance. This indicates that there were
probably fewer students in the latter condition than in the former.
The marginal posterior distributions are smooth and unimodal, which indi-
cates that the posterior is fairly well behaved. The shapes of the posterior
distributions look a lot like beta distributions. Thus, a beta distribution with
the same mean and variance is likely to be not too bad an approximation to
the posterior. This will be useful for linking this assessment form with another
form (Sect. 11.2.3).
When the posterior is generally smooth and especially when it is uni-
modal, it is often easier to look at the marginal distributions in tabular form
than as a series of graphs. Table 11.3 provides the estimated posterior means,
standard deviations, and selected quantiles from this calibration run. Look-
ing at the values, we can see that most have moved from their original prior
distributions. There are a few exceptions. Note in particular, that both the
prior and posterior mean for λ20 is .2, and both the prior and posterior stan-
dard deviation is 0.078. The calibration has not changed our estimates of this

11.2 Calibrating the Model with Field Data
389
0.0
0.2
0.4
0.6
0.8
1.0
0
5
10
15
Proficiency Model: P(Skill 1 | Skill 2 = False)
N = 2001   Bandwidth = 0.01318
0.75
0.80
0.85
0.90
0.95
1.00
0
2
4
6
8
10
12
14
Proficiency Model: P(Skill 1 | Skill 2 = True)
N = 2001   Bandwidth = 0.005042
0.0
0.2
0.4
0.6
0.8
1.0
Link Model for Item 8: P(isCorrect | Required Skills)
N = 2001   Bandwidth = 0.009138
0.0
0.2
0.4
0.6
0.8
1.0
Link Model for Item 8: P(isCorrect | Required Skills)
N = 2001   Bandwidth = 0.004261
a
b
c
d
0
5
10
15
0
5
10
15
Fig. 11.7 Posterior distributions from MCMC chains for selected parameters (ﬁve
chains; 2000 samples each chain). a Posterior for λ2,0, b Posterior for λ2,1, c Posterior
distribution for π8,0, False Positive, d Posterior distribution for π8,1, True Positive.
Reprinted with permission from ETS.
parameter at all. This is because of the structure of the Q-matrix: There are
no items that would provide evidence about Skill 2 in the absence of Skill 1.
Thus, the prior and posterior are equal. Although this is less than ideal, it
does not present a practical issue. A teacher would use the same instructional
strategy—teach basic fraction subtraction—for all students who lack Skill 1.
11.2.2 Scoring
Once the model is calibrated, we can use it to draw inferences about indi-
vidual students—to score them, in common parlance. For the students in the
calibration process there are two approaches to scoring.
The ﬁrst approach is to use the distribution of the proﬁciency variables
for a given student in the MCMC sampler as the posterior distribution for
that student. The MCMC sampler can be augmented to include any statistic

390
11 An Illustrative Example
Table 11.3 Summary statistics for binary-skills model
Parameter Mean SD
2.5 % 50 % 97.5 % Parameter Mean SD
2.5 % 50 % 97.5 %
λ1
0.810 0.025 0.759 0.811 0.857
λ20
0.200 0.078 0.071 0.193 0.374
λ21
0.901 0.030 0.838 0.903 0.954
λMN,0,1
0.256 0.082 0.111 0.250 0.430
λMN,0,2
0.184 0.073 0.065 0.176 0.349
λMN,1,1
0.367 0.090 0.197 0.364 0.548
λMN,1,2
0.235 0.075 0.107 0.228 0.397
λMN,2,1
0.415 0.088 0.245 0.414 0.587
λMN,2,2
0.451 0.076 0.308 0.450 0.605
λMN,3,1
0.458 0.058 0.331 0.463 0.561
λMN,3,2
0.486 0.054 0.391 0.482 0.601
λ50
0.165 0.068 0.057 0.158 0.319
λ51
0.483 0.094 0.303 0.482 0.665
λ52
0.724 0.063 0.584 0.730 0.830
π4,0
0.085 0.019 0.051 0.084 0.126
π4,1
0.870 0.029 0.808 0.872 0.921
π6,0
0.189 0.055 0.089 0.187 0.303
π6,1
0.924 0.017 0.888 0.925 0.954
π7,0
0.151 0.023 0.109 0.151 0.198
π7,1
0.830 0.041 0.745 0.833 0.903
π8,0
0.341 0.054 0.238 0.341 0.451
π8,1
0.778 0.025 0.727 0.779 0.826
π9,0
0.475 0.052 0.376 0.475 0.576
π9,1
0.740 0.028 0.684 0.741 0.793
π10,0
0.042 0.014 0.019 0.040 0.073
π10,1
0.831 0.036 0.757 0.833 0.896
π11,0
0.079 0.018 0.047 0.078 0.119
π11,1
0.872 0.029 0.810 0.874 0.924
π12,0
0.148 0.044 0.070 0.146 0.241
π12,1
0.905 0.024 0.855 0.906 0.949
π14,0
0.183 0.050 0.094 0.181 0.285
π14,1
0.926 0.018 0.888 0.928 0.959
π15,0
0.206 0.026 0.157 0.206 0.259
π15,1
0.862 0.035 0.789 0.864 0.923
π16,0
0.196 0.047 0.109 0.194 0.294
π16,1
0.907 0.021 0.864 0.908 0.945
π17,0
0.076 0.018 0.044 0.075 0.115
π17,1
0.815 0.033 0.746 0.817 0.876
π18,0
0.191 0.034 0.129 0.189 0.262
π18,1
0.809 0.034 0.738 0.811 0.871
π19,0
0.046 0.014 0.023 0.045 0.077
π19,1
0.884 0.039 0.801 0.888 0.950
π20,0
0.035 0.013 0.014 0.033 0.063
π20,1
0.811 0.034 0.739 0.812 0.873
of the proﬁciency variables that may be of interest. We obtain the posterior
probability that the student has mastered each of the skills. Further, in this
example, the sum of the proﬁciency variables excluding θMN provides a count
of the number of skills acquired by the student, which is a good summary of
overall proﬁciency in the domain of tasks. The advantage of this approach is
that it fully accounts for the uncertainty about the higher level parameters
for the tasks and the population distribution. The disadvantage is that it only
works for students in the calibration sample.
The second approach is to drop the estimated parameters from the calibra-
tion into the Bayesian network. To score students with the calibrated model,
the easiest approach is drop the posterior means for each conditional proba-
bility table into the Bayesian network. Then, the algorithms of Chap. 5 can be
used to score the students. Inference about individual students in the mixed-
number subtraction example proceeds as in Sect. 6.4. (If we approximate the
posterior distribution for each row of a conditional probability with a beta
distribution, the resulting model can be used either for scoring students or
for additional calibrations.) Although this approach ignores the uncertainty
about the parameters, the numerical diﬀerences are slight if the higher-level
parameters have been estimated with suﬃcient accuracy. More importantly

11.2 Calibrating the Model with Field Data
391
for practical work, it works with students who were not in the calibration
sample.
Table 11.4 shows the responses of nine selected students, grouped by evi-
dence models. Table 11.5 shows how, based on their responses, these exam-
inees’ skill-possession probabilities changed from the population rates that
serve as priors for students in the population before their responses are
observed.
Table 11.4 Selected student responses
Student number
EM Item 26 32 35 36 47 94 127 156 315
1
6
1
1
1
1
1
1
1
1
0
8
1
1
1
0
1
1
1
1
0
2
12
1
1
1
1
1
1
0
1
1
3
9
1
0
1
0
1
1
0
1
0
14
1
1
1
1
1
1
0
1
0
16
1
1
1
1
1
1
0
1
1
4
4
1
1
1
0
1
1
0
1
0
11
1
1
1
1
0
0
1
1
0
17
0
1
0
1
0
0
0
1
1
18
1
0
1
0
0
0
0
0
1
20
1
0
1
1
0
0
0
1
1
5
7
0
1
0
0
1
0
0
0
0
15
1
1
0
0
1
0
0
0
0
19
0
1
0
1
1
0
0
0
0
6
10
1
0
1
1
0
0
0
0
0
Total
12 11 11
9 10
7
3
10
5
1 indicates a correct response to the item;
0 indicates an incorrect response
Except for Student 315, all of the selected examinees correctly answered
the questions from evidence model 1, providing evidence that they have Skill 1.
Students 26, 32, 35, and 36 also answered most of the items requiring Skills 2,
3, and 4 correctly, so they are very likely to have Skills 2, 3 and 4 as well.
Despite missing a few items, Student 32 is likely to have all of the skills. (This
is one place where we would like to have had more items, so the data could
better distinguish these competing explanations.) As her incorrect responses
are not systematic with respect to the skill items required, they are more apt
to be caused by slips than missing skills. Student 94 shows evidence of not
having Skill 4 and probably not Skill 5 either. The posterior probabilities for
Students 35 and 156 indicate that they are very likely to have Skills 1, 2, 3,
and 4, but not Skill 5.

392
11 An Illustrative Example
Table 11.5 Prior and posterior probabilities for selected examinees
Student
Skill
1
2
3
4
5
Prior probabilities
All students 0.883 0.618 0.921 0.396 0.313
Posterior probabilities
26
1.000 0.987 1.000 0.999 0.391
32
1.000 0.946 1.000 0.993 0.976
35
1.000 0.980 1.000 0.998 0.038
36
1.000 0.982 1.000 0.989 0.310
47
1.000 0.741 1.000 0.254 0.403
94
1.000 0.671 1.000 0.007 0.204
127
0.920 0.128 0.167 0.011 0.363
156
1.000 0.687 1.000 0.981 0.031
315
0.485 0.478 0.811 0.370 0.103
11.2.3 Online Calibration
Much applied psychometrics is concerned with measurement issues related
to testing programs—ongoing series of assessments designed to measure the
same set of proﬁciencies. To support a testing program, the psychometrician
must calibrate a series of variant forms of the same essential assessment. These
forms may diﬀer in several ways: (1) they may contain new tasks from the
existing task models, (2) the testing population may have changed (due to
changes in educational policy or cohort eﬀects), or (3) the conceptual assess-
ment framework may have changed, adding new task models or even new
proﬁciencies. We will use the mixed number subtraction data to simulate the
ﬁrst two cases. The third is more challenging, but can be tackled with the
same modeling ideas we have been discussing.
Following Mislevy et al. (1999a), we create two overlapping forms of an
assessment by dropping three tasks from each one. The Admin 1 form drops
Tasks 16, 20, and 19, from evidence models 3, 4, and 5. The Admin 2 form
drops Tasks 14, 18, and 7, from the same evidence models. Thus, the two forms
are parallel (having the same number of tasks from each Evidence model) and
each have 12 tasks. The nine tasks that appear on both forms constitute an
anchor set that will help link the two forms during calibration. Note that only
tasks from evidence models with three or more instances in the original form
were dropped to produce the Admin 1 and Admin 2 forms. Thus, the set of
distinct Q-matrix rows is the same for the anchor test and both Admin forms.
In high-stakes assessments, psychometricians often try to equate the two
forms; that is, try to ensure that scores from both forms have identical meaning
(Kolen and Brennan 2004). Strict equating is not necessary in the lower stakes
diagnostic setting that the mixed number subtraction test is designed for, but
it still is necessary to link the two assessments, to ensure that the proﬁciency
variables in both are at least on roughly the same scale.

11.2 Calibrating the Model with Field Data
393
The easiest way to link the assessments is to simply calibrate them
together—all data from all administrations at once, with not-presented items
coded as missing at random. This is called concurrent calibration. Both the
Evidence model and MCMC algorithms are quite robust to missing data as
long as it meets the missing-at-random assumption. Both the Evidence model
and MCMC algorithms depend on the number of students in the sample and
can be slow, although this restriction is becoming less of a problem as com-
puting power continues to increase.
Mislevy et al. (1999a) proposed a less computationally intensive alternative
that does not require item responses to previous administrations, and can be
chained across multiple administrations. It uses a three-step procedure:
1. Calibrate the data from the ﬁrst administration normally (Sect. 11.2.1).
2. Approximate the posterior distribution of the structural parameters with
a parametric distribution with a convenient form (e.g., Beta, Dirichlet, or
Normal distributions).
3. Calibrate the data from the second administration using the approximate
posterior calculated in the previous step as a prior.
To demonstrate this procedure, we randomly divided the original sample
of 325 students using Method B, assigning 225 to Admin 1, and the remaining
100 to Admin 2. Dropping Tasks 16, 20, and 19 from Admin 1 student records
and Tasks 14, 7, and 18 from Admin 2 student records produced data that is
fairly typical for multiple administrations. These data are analyzed below.
The calibration of the ﬁrst administration data is not qualitatively diﬀerent
from the analysis of the whole data set in the previous section. As there are
no problems in the model, it converged easily, and the posterior statistics
conditioned on the Admin 1 data are shown in Table 11.6. Comparing this to
the full data posterior, Table 11.3, shows that the posterior means diﬀer by
less than 0.02 for almost all parameters (except the three dropped items) and
the posterior standard deviations are slightly larger.
To link the second administration to the ﬁrst, we calibrate the second
administration’s data using the posterior from Admin 1 as a prior for Admin 2.
Fixing the values of common parameters at their Admin 1 posterior means (or
medians) during the Admin 2 calibration ignores an important contribution
to our overall uncertainty: our uncertainty are the values of the parameters.
This expedient biases the estimates of the new item parameters.
The easiest way to represent the new prior is to simply include both the
old and the new data set in the second calibration. The MCMC technique
appropriately handles missing data that are “missing at random” in the sense
of Rubin (1977), and both the values of the latent variables and the responses
of not-administered items are, so everything should work well. This method
produces an accurate representation of the prior (within the accuracy of the
MCMC algorithm). With a data set this small, this full calibration technique
does not present a problem. However, as the data get larger and larger, the

394
11 An Illustrative Example
Table 11.6 Summary statistics for binary-skills model, Admin 1
Parameter Mean SD
2.5 % 50 % 97.5 % Parameter Mean SD
2.5 % 50 % 97.5 %
λ1
0.822 0.027 0.766 0.823 0.873
λ20
0.198 0.076 0.075 0.191 0.362
λ21
0.890 0.034 0.817 0.893 0.951
λMN,0,1
0.260 0.082 0.116 0.256 0.433
λMN,0,2
0.187 0.074 0.067 0.179 0.354
λMN,1,1
0.370 0.092 0.200 0.367 0.554
λMN,1,2
0.219 0.074 0.094 0.212 0.378
λMN,2,1
0.401 0.090 0.231 0.399 0.578
λMN,2,2
0.415 0.082 0.261 0.413 0.578
λMN,3,1
0.389 0.054 0.279 0.391 0.492
λMN,3,2
0.541 0.052 0.442 0.540 0.647
λ50
0.166 0.067 0.057 0.158 0.315
λ51
0.506 0.098 0.314 0.506 0.696
λ52
0.797 0.059 0.667 0.801 0.901
π4,0
0.097 0.024 0.054 0.095 0.149
π4,1
0.890 0.032 0.821 0.892 0.944
π6,0
0.176 0.059 0.075 0.171 0.300
π6,1
0.933 0.018 0.894 0.935 0.965
π7,0
0.153 0.030 0.098 0.152 0.215
π7,1
0.806 0.046 0.710 0.809 0.893
π8,0
0.327 0.062 0.212 0.324 0.455
π8,1
0.802 0.029 0.742 0.803 0.855
π9,0
0.499 0.059 0.384 0.499 0.613
π9,1
0.770 0.031 0.707 0.771 0.828
π10,0
0.065 0.021 0.030 0.063 0.111
π10,1
0.819 0.040 0.735 0.821 0.890
π11,0
0.070 0.021 0.034 0.068 0.115
π11,1
0.884 0.033 0.812 0.886 0.942
π12,0
0.129 0.045 0.054 0.125 0.228
π12,1
0.907 0.028 0.847 0.908 0.956
π14,0
0.154 0.054 0.062 0.150 0.272
π14,1
0.952 0.017 0.912 0.954 0.981
π15,0
0.180 0.031 0.123 0.179 0.244
π15,1
0.837 0.044 0.745 0.840 0.916
π17,0
0.079 0.022 0.041 0.077 0.126
π17,1
0.810 0.039 0.727 0.812 0.882
π18,0
0.182 0.031 0.125 0.181 0.246
π18,1
0.823 0.037 0.746 0.825 0.890
MCMC chain will get slower and slower. Therefore, it is useful to be able to
represent the new prior more compactly.
One idea is to represent the posterior with a distribution that has the same
functional form as the prior, but diﬀerent parameters. The idea is similar to
the use of conjugate priors, but now as an approximation rather than an exact
form of updating. In this model, almost all of the prior are beta distribution
(with one Dirichlet distribution). The distributions in Fig. 11.7 are roughly
beta-shaped, so this approximation is probably not too bad. One word of
caution: Figure 11.7 is only showing the margins of the posterior distribution.
The complete distribution spans all of the parameters in the model, and even
if they are a priori independent, they may be dependent a posteriori. York
(1992) notes this happens when there are missing data in the model—and
in this case, the latent proﬁciency variables are missing for everybody! For
the purposes of computational simplicity, we will ignore the dependence. The
eﬀects of the dependency decrease as the sample size increases.
The method of moments presented in Sect. 9.6.2 is the easiest way to
ﬁt a beta distribution or a Dirichlet distribution to the marginal posterior
distribution. These are then plugged into the MCMC setup for Admin 2 as
the prior for the respective parameters.
Using this procedure to generate the prior for Admin 2 and running the
MCMC sample produces the posterior statistics shown in Table 11.7. Com-
paring these results to Table 11.7, we see the changes in posterior means
of the common parameters are small, and within what is expected from the

11.2 Calibrating the Model with Field Data
395
posterior variance. The posterior standard deviations have dropped slightly,
for the most part returning to the levels in the calibration using the full data
(Table 11.3). However, compare the posterior standard deviations for the three
items new to this administration, Tasks 16, 19, and 20. Here, the posterior
standard deviations are substantially larger, reﬂecting the smaller sample size
(about 100, compared to 325).
Table 11.7 Summary statistics for binary-skills model, Admin 2
Parameter Mean SD
2.5 % 50 % 97.5 % Parameter Mean SD
2.5 % 50 % 97.5 %
λ1
0.808 0.024 0.759 0.808 0.854
λ20
0.200 0.076 0.076 0.192 0.367
λ21
0.898 0.028 0.837 0.900 0.947
λMN,0,1
0.260 0.083 0.117 0.254 0.436
λMN,0,2
0.187 0.074 0.066 0.177 0.351
λMN,1,1
0.369 0.088 0.205 0.366 0.551
λMN,1,2
0.208 0.074 0.084 0.201 0.370
λMN,2,1
0.425 0.080 0.274 0.424 0.585
λMN,2,2
0.393 0.075 0.255 0.391 0.544
λMN,3,1
0.444 0.047 0.352 0.443 0.537
λMN,3,2
0.488 0.046 0.399 0.487 0.577
λ50
0.166 0.067 0.057 0.159 0.315
λ51
0.512 0.096 0.323 0.512 0.697
λ52
0.785 0.056 0.668 0.788 0.883
π4,0
0.077 0.018 0.046 0.076 0.116
π4,1
0.874 0.029 0.811 0.876 0.926
π6,0
0.199 0.056 0.099 0.196 0.314
π6,1
0.921 0.017 0.885 0.922 0.951
π8,0
0.326 0.055 0.223 0.325 0.437
π8,1
0.780 0.026 0.728 0.781 0.829
π9,0
0.464 0.051 0.366 0.463 0.562
π9,1
0.750 0.028 0.694 0.751 0.803
π10,0
0.043 0.014 0.020 0.041 0.074
π10,1
0.822 0.035 0.749 0.823 0.886
π11,0
0.079 0.018 0.047 0.078 0.119
π11,1
0.867 0.031 0.800 0.868 0.921
π12,0
0.150 0.044 0.073 0.147 0.246
π12,1
0.914 0.023 0.863 0.916 0.954
π15,0
0.177 0.025 0.130 0.176 0.229
π15,1
0.850 0.039 0.766 0.853 0.917
π16,0*
0.197 0.063 0.088 0.192 0.328
π16,1*
0.882 0.036 0.803 0.885 0.943
π17,0
0.080 0.019 0.048 0.079 0.120
π17,1
0.805 0.035 0.733 0.806 0.869
π19,0*
0.079 0.027 0.034 0.077 0.140
π19,1*
0.781 0.064 0.649 0.785 0.897
π20,0*
0.073 0.026 0.030 0.070 0.133
π20,1*
0.845 0.050 0.735 0.849 0.928
* indicates task new to this administration
Note that the second calibration used the posterior distribution from the
ﬁrst calibration for both the evidence models and proﬁciency model. This is
correct under the assumption that both the ﬁrst administration and second
administration are samples from the same population. Although this is cor-
rect in this exercise, it need not hold in real life situations. Often the second
administration is taken from diﬀerent points in time, diﬀerent geographical
regions, or diﬀerent classrooms. There are many reasons that the proﬁciency
model for diﬀerent populations might be diﬀerent: diﬀerences in instruction;
diﬀerences in ancillary, but important skills (e.g., language ability on a math
test); variation in who chooses to take the test on a given day (many large
testing programs show considerable seasonal variation in their test-taking pop-
ulation).
If the populations may be diﬀerent, using the posterior from the ﬁrst cal-
ibration as the prior for the second calibration’s proﬁciency model may not

396
11 An Illustrative Example
be the best strategy. The posterior mean may be reasonable, but the variance
overstates our certainty about the value of the second population’s proﬁciency
parameters. One strategy is to take the posterior distribution and “soften” the
prior by increasing the variance. In the case of Dirichlet and beta distribu-
tions, this means multiplying the parameters by a value less than one (1/2 or
1/4). In the case of a normal distribution, the mean remains the same, and
the variance is multiplied by a number greater than one (2 or 4). Another
strategy would be to create a prior for the second calibration by averaging
the prior from the ﬁrst calibration and the posterior from the ﬁrst calibra-
tion. This would be especially valuable if there had been considerable eﬀort to
base the original prior on expert opinion. (In concurrent calibration, the way
to accomplish this is to build a hierarchical model for the two populations,
with the parameters from the proﬁciency model drawn from the higher level
distribution.)
While it is usual to assume that the proﬁciency model parameters will be
diﬀerent in the two diﬀerent administrations, it is not usual to assume the
same about the evidence model parameters. In particular, the deﬁnition of
the proﬁciency has not changed and neither has the task, so why should the
conditional probabilities of task response given proﬁciency change?
Nevertheless, this is an assumption, and hence should be veriﬁed. It is for-
mally equivalent to the problem of diﬀerential item function (DIF) discussed
in Sect. 10.4, here with population subgroups distinguished by administra-
tions. In the example above, there were no unusual diﬀerences between the
parameters in Tables 11.6 and 11.7. If there were, that would be cause for
concern. There are many reasons that such diﬀerence arise, for example, a
change in the background knowledge of students typically taking the test.
These kinds of problem can arise in international assessment, in particular
when tasks need to be translated from one language to another (sometimes
distinctions which are subtle in one language are obvious in another). If a
large diﬀerence is observed in practice, then the usual procedure is to treat
the tasks on the two diﬀerent administrations as two diﬀerent tasks from the
same task models and calibrate a diﬀerent set of evidence model parameters
from each.
The linking methodology described here is a variant on the Non-Equivalent
groups Anchor Test (NEAT) design (Kolen and Brennan 2004). The quality
of the linking depends heavily on the number and type of tasks chosen for the
anchor test. The standard error of estimation when estimating the proﬁciency
variables from the anchor test will be reﬂected in the linking error. When
there are multiple proﬁciency variables, there must be enough evidence (either
direct evidence through tasks or indirect evidence though correlations among
proﬁciency variables to provide a reasonable estimate of the distribution of
the proﬁciency variables from the anchor test alone.
The evidence-centered design (ECD; Chap. 12) framework helps guide the
construction of a reasonable anchor test. In the example above, the anchor test
was chosen to contain all of the diﬀerent evidence model, and where possible

11.3 Model Checking
397
multiple tasks from the same evidence model. The length of the anchor test
is a bit short, but that is also true of the entire test. As an initial rule of
thumb, the anchor test should contain at least ten binary observables (or
more if they are clustered within tasks) and at least four or more observables
for each proﬁciency variable. This advice is based on experience with equating
unidimensional tests and identifying factors in factor analysis. More research
needs to be done on linking in cognitive diagnostic modeling in general.
Sometimes the changes in the conceptual assessment framework (CAF) are
so large that linking is diﬃcult to achieve. Examples of this include adding
or removing task models (not just individual tasks, but adding a new kind of
task or retiring a particular task type); changing the evidence model struc-
tures (i.e., the Bayes net fragments, in particular, changing the parents associ-
ated with an observable); and adding or removing variables to the proﬁciency
model. The linking procedure described above will help bridge between the
old and new designs, but the eﬀective meanings of the proﬁciency variables
and scores may have changed. Score users need to be appropriately cautioned
about the diﬀerence between the old and new scores.
11.3 Model Checking
The linking procedures described in the previous section all rely on the model
being correct. The advantage of using a fully Bayesian scoring model is that
it contains a built-in method of model validation: If the observed data have
a very low prior probability then it is likely that the model is inappropriate.
However, what “very low probability” means in this context is unclear. Fur-
thermore, this test does not provide information about how to ﬁx the model
to make it ﬁt better.
Posterior predictive model checking (Sect. 10.2) answers the question
about what is an unusual probability for the data. Comparing the observed
data to a number of artiﬁcial data sets which are known to ﬁt the model
provides a reference distribution for any potential model ﬁt statistic. If those
statistics are chosen cleverly enough they should lead to insight into what part
of the model is problematic and where it could be improved. The graphical
methods developed in Sect. 10.3 provide further guidance into looking at the
problem.
This section summaries a series of model checking exercises using the Bayes
net model described above with the mixed-number subtraction data (Yan et
al. 2003; Sinharay et al. 2004; Sinharay and Almond 2007). Section 11.3.1
looks at the observable characteristic plots and identiﬁes a possible problem.
Section 11.3.2 explores using posterior predictive model checks to determine
both task (item) ﬁt and person ﬁt.

398
11 An Illustrative Example
11.3.1 Observable Characteristic Plots
At its heart the Bayesian network model is a multivariate latent class model
(Maris 1999). The proﬁciency model deﬁnes 24 possible proﬁciency proﬁles.
If we knew the proﬁciency proﬁle of a given student, we should be able to
make accurate predictions about their potential response patterns. Due to the
design of the assessment, only nine equivalence classes of proﬁciency proﬁles
are distinguishable (Table 11.2). Still for the purposes of this assessment,
knowing which equivalence class a student belongs to is suﬃcient to make a
prediction about their response to any item.
Table 11.2 represents a fundamental assumption about this test form.
Although the proﬁciency model has nine distinguishable latent classes, for
the purposes of any given item students can be grouped into two sets. For
the set in which δi,s(j) = 1, we expect most students will be able to solve the
problem, with success rate πj1. For the set in which δi,s(j) = 0, we expect
most students will not be able to solve the problem with a guessing rate of
πj0.
What could go wrong with this model? The most obvious possibility is that
somehow the latent classes are misclassiﬁed, and probability of the students
who fell into that latent class getting the item correct would be diﬀerent
than the expected value (either πj0 or πj1 depending on the value of δi,s(j)).
The observable characteristic plot (Sect. 10.3) is designed to check for this
possibility. In this plot, a credibility interval for the success rate of all students
falling into a latent class is plotted for each latent class. The estimated values
of πj0 and πj1 are plotted for reference. If the observed credibility region
does not overlap the expected success rate, then there is an issue that needs
attention.
Yan et al. (2003) developed these plots for the mixed number subtraction
problem. However, there was one technical problem; the actual proﬁciency
proﬁle of the students, and hence which equivalence classes they fell into, was
unknown. Yan et al. solved this problem by simply looking at a single MCMC
iteration. Using the assigned proﬁciency proﬁle assigned to that student in
that iteration, it was simple to calculate the observed proportion correct for
students in that equivalence class. A Bayesian credibility interval was calcu-
lated by using a Bayesian prior of Beta(.2, .8) for equivalence classes in which
the student was expected to get the item wrong according to Table 11.1 and a
prior of Beta(.8, .2) for equivalence classes in which the student was expected
to get the item wrong.
A known limitation of the Yan et al. (2003) procedure was that it did
not take into account the uncertainty about the classiﬁcation of students into
equivalence classes. Sinharay et al. (2004) improved the technique by replacing
the use of a single iteration with an average over multiple iterations. Where
the Yan et al. procedure used the observed proportion correct for the students
falling each equivalence class, the Sinharay et al. procedure used a weighted
average with the weights taken from the proportion of MCMC iterations that

11.3 Model Checking
399
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
Proportion correct for Item 4 ( 3 1/2 - 2 3/2 ) in EM 4 for the 9 Equivalence Classes
Equivalence Classes
Proportion correct
0.0
0.2
0.4
0.6
0.8
1.0
1
2
3
4
5
6
7
8
9
Equivalence Classes
Proportion correct
0.0
0.2
0.4
0.6
0.8
1.0
1
2
3
4
5
6
7
8
9
Equivalence Classes
Proportion correct
0.0
0.2
0.4
0.6
0.8
1.0
1
2
3
4
5
6
7
8
9
Equivalence Classes
Proportion correct
0.0
0.2
0.4
0.6
0.8
1.0
1
2
3
4
5
6
7
8
9
Equivalence Classes
Proportion correct
0.0
0.2
0.4
0.6
0.8
1.0
1
2
3
4
5
6
7
8
9
Equivalence Classes
Proportion correct
0.0
0.2
0.4
0.6
0.8
1.0
1
2
3
4
5
6
7
8
9
Equivalence Classes
Proportion correct
0.0
0.2
0.4
0.6
0.8
1.0
1
2
3
4
5
6
7
8
9
Equivalence Classes
Proportion correct
0.0
0.2
0.4
0.6
0.8
1.0
1
2
3
4
5
6
7
8
9
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
Proportion correct for Item 6 ( 6/7 - 4/7 ) in EM 1 for the 9 Equivalence Classes
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
Proportion correct for Item 8 ( 3/4 - 3/4 ) in EM 1 for the 9 Equivalence Classes
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
Proportion correct for Item 9 ( 3 7/8 - 2 ) in EM 3 for the 9 Equivalence Classes
Proportion correct for Item 7 ( 3 - 2 1/5 ) in EM 5 for the 9 Equivalence Classes
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
Proportion correct for Item 10 ( 4 4/12 - 2 7/12 ) in EM 6 for the 9 Equivalence Classes
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
Proportion correct for Item 11 ( 4 1/3 - 2 4/3 ) in EM 4 for the 9 Equivalence Classes
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
Proportion correct for Item 12 ( 11/8 - 1/8 ) in EM 2 for the 9 Equivalence Classes
Fig. 11.8 Observable characteristic plots for ﬁrst eight items. For each equivalence
class, the symbol in the center represents the observed success rate for this item and
the bar represents a 95 % credibility interval for the success rate. Reprinted from
Sinharay et al. (2004) with permission from ETS.

400
11 An Illustrative Example
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
Proportion correct for Item 14 ( 3 4/5 - 3 2/5 ) in EM 3 for the 9 Equivalence Classes
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
Proportion correct for Item 15 ( 2 - 1/3 ) in EM 5 for the 9 Equivalence Classes
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
Proportion correct for Item 16 ( 4 5/7 - 1 4/7 ) in EM 3 for the 9 Equivalence Classes
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
Proportion correct for Item 17 ( 7 3/5 - 4/5 ) in EM 4 for the 9 Equivalence Classes
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
Proportion correct for Item 18 ( 4 1/10 - 2 8/10 ) in EM 4 for the 9 Equivalence Classes
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
Proportion correct for Item 19 ( 7 - 1 4/3 ) in EM 5 for the 9 Equivalence Classes
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
Proportion correct for Item 20 ( 4 1/3 - 1 5/3 ) in EM 4 for the 9 Equivalence Classes
Equivalence Classes
Proportion correct
0.0
0.2
0.4
0.6
0.8
1.0
1
2
3
4
5
6
7
8
9
Equivalence Classes
Proportion correct
0.0
0.2
0.4
0.6
0.8
1.0
1
2
3
4
5
6
7
8
9
Equivalence Classes
Proportion correct
0.0
0.2
0.4
0.6
0.8
1.0
1
2
3
4
5
6
7
8
9
Equivalence Classes
Proportion correct
0.0
0.2
0.4
0.6
0.8
1.0
1
2
3
4
5
6
7
8
9
Equivalence Classes
Proportion correct
0.0
0.2
0.4
0.6
0.8
1.0
1
2
3
4
5
6
7
8
9
Equivalence Classes
Proportion correct
0.0
0.2
0.4
0.6
0.8
1.0
1
2
3
4
5
6
7
8
9
1
Equivalence Classes
Proportion correct
0.0
0.2
0.4
0.6
0.8
1.0
1
2
3
4
5
6
7
8
9
Fig. 11.9 Observable characteristic plots for last seven items. Equivalence classes
whose midpoint are plotted with “*” are expected to get the item right, those plotted
with “x” are expected to get the item wrong. Reprinted from Sinharay et al. (2004)
with permission from ETS.

11.3 Model Checking
401
student fell into that equivalence class. Figures 11.8 and 11.9 show the plots
produced by this procedure. The plots produced by the Yan et al. procedure
look similar.
Looking at Fig. 11.9, Items 19 and 20 are examples of items that are
working well. In each case, the bars for each equivalence class overlap with
the upper or lower bar. The patterns are diﬀerent (representing the diﬀerent
evidence models), but in each case the bars overlap the expected success rate
for that evidence model.
Item 18 does not ﬁt as well. Equivalence Classes 1 and 6 do not overlap
the lower bar (1 is too low and 6 is too high). Equivalence Class 1 is also
problematic in Items 7 and 15, and Item 1 sits very close to the edge. For all
of these items, the success rate of students in Equivalence Class 1 is lower than
expected. Recall that this is the class for people who have none of the skills.
It makes sense that they might be lower than expected. Equivalence Class 6 is
also problematic for a number of items; however, a closer examination shows
that there are very few people in that equivalence class (5–10 assigned in each
MCMC iteration).
These graphs indicate a possible problem with the evidence models taking
an all-or-nothing approach to the skill patterns. In particular, they do not dis-
tinguish between students with no skills at all and students who lack only one
of the required skills. Relaxing the evidence model could produce something
that looks more like the observed plots. Deﬁne δ′
i,s(j) to be 0 if the student
has no skills, 1 if the student has some but not all of the required skills, and
2 if the student has has all of the required skills. In the revised model, there
are three probabilities for each evidence model (except for evidence model 1
which requires only the ﬁrst skill): π′
j0, π′
j1, and π′
j2. Sinharay et al. (2004)
and Sinharay and Almond (2007) explore this model among others and ﬁnd
that it does ﬁt better than the original Mislevy (1995b) model.
There is a close correspondence between the items ﬂagged with observable
characteristic plot and the observable ﬁt statistic (Eq. 10.10). Sinharay et
al. (2004) calculate this statistic for all of the items and note that the two
largest values are for Items 8 and 9. Inspecting these items reveals a problem:
both admit an alternative solution. Item 8 is 3/4 −3/4 which can be solved
by noting that anything minus itself is nothing. Item 9 is 3 7/8 −2, which
can be solved by guessing that three and something minus two is one and
something. Both items have a very high guessing probability (π8,1 = .341 and
π9,1 = .475). As the diﬀerences between the high and low values are so small,
these items have less evidentiary value than the others. It might be worth
checking with the experts to see if these items should be replaced.
11.3.2 Posterior Predictive Checks
If Eq. 11.4 is the correct probability function, then data generated from that
same model should be similar to the observed data. The posterior predic-
tive check (Sect. 10.2 formalizes this intuition). A replicate or shadow data

402
11 An Illustrative Example
set Y contains draws Yij for each examinee/item response in the observed
data set Xij. The posterior predictive distribution for Y is p (Y | X) =

p (Y | ω) p (ω | X) ∂ω, where ω represents all the parameters in the model—
in this problem, θ’s, π’s, and λ’s.
The MCMC sampler we have already built produces samples from the
posterior distribution for ω. We produced a shadow data set Y(t) in each
cycle, by augmenting the sampler to to draw Y (t)
ij
from a facsimile of Eq. 11.2:
Y (t)
ij
''(δi,s(j) = z ) ∼Bern

π(t)
jδi,s(j)

,
for z = 0, 1.
(11.6)
Then, for a chosen statistic D(·, ·), compute D(Y(t), ω(t)) and D(X, ω(t))
for each cycle of the MCMC sampler. The proportion of cycles in which
D(Y(t), ω(t)) > D(X, ω(t)) gives the posterior predictive p-value for that
statistic. When a statistic is constructed so that a higher value means worse
ﬁt, a low p-value means the ﬁt statistic for the observed response was usually
higher than the shadow response, so the observation was surprising in light of
the model.
Yan et al. (2003) chose two statistics based on the Pearson residual
(Eq. 10.7) to compare the observed value to its prediction (in this case
the probability of success under the model).6 For each MCMC cycle, let
pij(ω(t)) = E

xij | ω(t)
, that is the predicted probability of success (Eq. 11.2)
given the sampled parameters and proﬁciency variables for Iteration t. Then,
deﬁne person*item squared Pearson residuals as
V (uij, ω) =
(uij −pij(ω))2
pij(ω) (1 −pij(ω)).
This can be calculated for both the original data, V (Xij, ω(t)), and the
replicated data, V (Y (t)
ij , ω(t)). As this statistic depends on the estimated
parameters, the value for the original data will be diﬀerent on every MCMC
cycle. Averaging the squared residual across all observables for an individual
produces a measure of person ﬁt:
PFi(U, ω) =
⎛
⎝1
J
J

j=1
V (uij, ω)
⎞
⎠
1/2
.
(11.7)
6 Fit indices based on Pearson item-by-person residuals are simple and fairly widely
used in IRT, but their statistical properties leave much to be desired; see Meijer
and Sijtsma (2001) for a review of person-ﬁt statistics and Glas and Falc´on (2003)
on item ﬁt, and Haberman (2009) for more recent developments derived from con-
tingency table analysis. Using PPMC somewhat mitigates one serious problem,
namely, the lack of theoretical reference distributions, as it eﬀectively creates
tailor-made reference distributions for ﬁt statistics. Diﬀerent ﬁt indices provide
better or worse approximations of empirical distributions under the PPMC null
model, so research on optimal choices is in order (Levy 2011).

11.3 Model Checking
403
Averaging the squared residual across all individuals responding to an observ-
able produces a measure of observable ﬁt (or item ﬁt):
OFj(U, ω) =

1
N
N

i=1
V (uij, ω)
1/2
.
(11.8)
Averaging the squared residual across all individuals and observables produces
a measure of overall ﬁt (or total ﬁt):
T F(U, ω) =
⎛
⎝1
NJ
N

i=1
J

j=1
V (uij, ω)
⎞
⎠
1/2
.
(11.9)
Taking the square roots of the averages puts all three measures on a root mean
squared error metric. As each observable is either 0 and 1, and 0 < pij < 1,
all three measures always range between 0 and 1, with lower values indicating
better ﬁt. The posterior mean of all of measure indicates the typical magnitude
of discrepancy between observed and predicted observables.
Does a given degree of discrepancy represent good or poor model ﬁt? The
posterior predictive distribution for each statistic provides an approximate
null distribution against which observed values can be compared. In each
MCMC cycle, we compare the ﬁt measure (here high values mean worse ﬁt)
of the actual data and the shadow data: Is PFi(Y(t),ω(t)) > PFi(X, ω(t)) for
Person i? Is OFj(Y(t), ω(t)) > OFj(X, ω(t)) for Observable j? For overall
model ﬁt, is T F(Y(t), ω(t)) > T F(X, ω(t))? This can be easily built into the
MCMC sampler. We monitor the proportion of cycles in which the inequality
holds. If the model ﬁts the data, this should be around .5. If this value is close
to 0 (say below .10), that indicates that the original data ﬁts substantially
worse than the simulated data, and indication of trouble. Since the shadow
data are generated from parameters estimated from the observed data, these
probabilities tend to be conservative; that is, they show the observed data ﬁt-
ting better than they would under a true null distribution. They are therefore
less useful for absolute indicators of ﬁt than for comparisons of ﬁt among like
parameters (e.g., for detecting which items ﬁt relatively worse than others).
Observable-Fit Indices
Most of the observables ﬁt very well with respect to the mean square error
indices, although some tasks do ﬁt better than others. The ﬁt indices are
pseudo ﬁt probabilities, or frequencies with which mean squares for observed
data were higher than for shadow data, or P(OFj(Shad) ≥OFj(Obs)).
Table 11.8 provides the ﬁt indices. Most of the observables ﬁt well with indices
are around 0.5. Items 8 and 9 do not ﬁt as well as all the other items; their
item-ﬁt indices are 0.261 and 0.284. These are the same items that the previ-
ous analysis ﬂagged as problematic. (See Figs. 11.8 and 11.9).

404
11 An Illustrative Example
Table 11.8 Item-ﬁt indices for the mixed-number subtraction test
Item j
4
6
7
8
9
10
11
12
P(OFj(Shad) ≥OFj(Obs)) 0.538 0.569 0.430 0.261 0.284 0.903 0.563 0.529
OFj(Obs)
0.995 0.987 1.020 1.032 1.026 0.925 0.987 0.994
Item j
14
15
16
17
18
19
20
P(OFj(Shad) ≥OFj(Obs)) 0.547 0.392 0.472 0.539 0.368 0.712 0.777
OFj(Obs)
0.991 1.023 1.009 0.996 1.028 0.919 0.874
aSum of squared standardized residuals
Person-Fit Indices
Table 11.9 shows the person ﬁt indexes for selected students. Overall the
students ﬁt well with respect to the mean square error indices for all the items.
The average value for the pseudo probabilities over the full sample is between
.5 and .6. This is not surprising as 15 items is a relatively short test to develop
evidence of misﬁt. Still, small values of this statistic should indicate more
unusual response patterns. As examples, Students 26, 94, and 156 all ﬁt very
well according to our model with ﬁt indices values around 0.5. Students 36,
47, and 315 did not ﬁt well; their ﬁt indices are all less than 0.1.
Table 11.9 Person-ﬁt p-values for selected students
Student i
26
32
35
36
47
94
127
156
315
P(PFi(Shad) ≥PFi(Obs)) 0.499 0.266 0.740 0.035 0.057 0.427 0.270 0.493 0.002
Refer back to Table 11.4 for the response patterns for these students.
Consider Student 26, who has 12 out of 15 items correct. This student was
able to solve all but one of the tasks using evidence models 1, 2, 3, and 4, but
missed two out of three tasks using evidence model 5; this looks very much
like a person who has Skills 1 through 4, but not Skill 5. Student 156 shows
a similar pattern. Student 94 has seven correct observables, from evidence
models 1, 2, and 3, and another from evidence model 4; this also corresponds
to meeting a requirement of Skills 1 through 3 for these items. Student 127
has only three items correct that are all from evidence model 1, plus Item 11
from evidence model 4; this student also ﬁts the model well, most likely having
only Skill 1.
Student 36 does not ﬁt well. The nine items that student got correct are
scattered across diﬀerent evidence models. This examinee missed an item from
evidence model 1, requiring only the basic fraction skill (Skill 1), but was able
to answer several questions from evidence models 4 and 6 correctly, requiring
the more advanced Skill 4. This relatively contradictory pattern of evidence
leads to the evidence of misﬁt.
Student 47 is another case of contradictory evidence. The items this stu-
dent missed from evidence model 4 provide evidence for a lack of Skill 4, while

11.4 Closing Comments
405
the tasks this student was able to solve from evidence model 5 indicate that
Skill 5 is present. Finally, Student 315 got many relatively diﬃcult items from
evidence model 4 correct while missing the basic fraction subtraction items in
evidence model 1.
Patterns of misﬁt may provide interesting diagnostic information about
a student. These students may be developing their mixed number skills in a
pattern that is diﬀerent from the usual developmental sequence (although the
short length of this assessment makes any such conclusions rather tenuous;
again, we would use a longer test in practice, say with ﬁve items from each
task model). The teacher should look more closely at these individuals and
see if there is any diﬀerent kind of explanation or instruction that is needed.
11.4 Closing Comments
The goal of this chapter is to put the material of Part II in perspective through
an example. The ﬁrst step in building an assessment is to translate the infor-
mation about the domain gathered from the experts into a mathematical
model (Chap. 8; Sect. 11.1). For this example, we were able to build on the
work of Klein et al. (1981) and Tatsuoka (1984) in constructing the Bayesian
network. Given the Bayesian model, and the data collected by Tatsuoka, the
MCMC algorithm (Chap. 9) could be used to calibrate the model to the data,
and to link two diﬀerent forms of the assessment (Mislevy, Almond, et al.
1999a; Sect. 11.2). Finally, because the model was a fully Bayesian model,
the methods of Chap. 10 could be used to critique and suggest improvements
(Yan et al. 2003; Sinharay et al. 2004; Sinharay and Almond 2007, Sect. 11.3).
Of course, none of this would have been possible without the foundational
work of Klein et al. (1981) and Tatsuoka (1984) in building a cognitive model
for the domain of mixed number subtraction. The contributions of the papers
reviewed in this chapter have been to translate that cognitive model into a
mathematical model using the notation of Bayesian networks. This is primar-
ily a problem of knowledge management, and evidence-centered assessment
design was developed as a tool to manage the kinds of knowledge that go
into developing an assessment. Part III describes the role of ECD in building
Bayesian network models for assessment systems.
Exercises
11.1 (Skill 4 as a Prerequisite of Skill 5). How could the graphical struc-
ture of Fig. 11.1 be changed to include the relationship that Skill 4 is a
prerequisite Skill 5? What additional restrictions would be necessary on the
conditional probability tables?
11.2 (Sensitivity to Prior Means). Mislevy et al. (1999a) used a Beta(6, 21)
prior for λ20 and Beta(21, 6) prior for λ21. The posterior mean (and SD)

406
11 An Illustrative Example
reported in that paper is .22 (.07) for λ20 and .90 (.03) for λ21. Compare
these to the numbers in the second row of Table 11.3. How sensitive are these
results to the choice of prior? Explain the diﬀerence in sensitivity.
11.3 (Starting Values). Instead of using the prior distribution to derive
starting points, Mislevy et al. (1999a) ﬁrst did a test run, and then chose start-
ing points using the Midpoint, High, Low, HighLow, and LowHigh strategies
using the sample statistics from the test run rather than the prior. How does
using the prerun instead of the prior for starting values aﬀect the convergence
tests if the posterior is mostly smooth and unimodal? If the posterior has lots
of local maxima?
11.4 (Are More Chains Better?). Is it better to run more chains or have
fewer chains and run them longer?
11.5 (Missing at Random). Does the non-equivalent groups anchor test
(NEAT) design meet the qualiﬁcations for missing at random? Missing com-
pletely at random?
11.6 (Intelligent Tutoring System). Suppose that the Bayesian network
model is embedded in an intelligent tutoring system that controls which tasks
(both instructional and assessment) are presented to the students based on
the estimated value of the proﬁciency variables at the time the task is chosen.
A researcher proposes to calibrate the assessment model for this system using
the MCMC method with a large number of student records taken from using
the tutoring system in a number of classrooms. In these records, any given
student has recorded answers for only some of the tasks (those selected by
the tutoring system). The research plans to rely on the ability of the MCMC
method to handle missing data when calibrating the model. Will this work?
11.7 (Eﬀective Sample Sizes). Use the method of moments approximation
to calculate the eﬀective sample sizes for the posterior margins of the evidence
model parameters, pj0’s and pj1’s, from the second administration, Table 11.7.
What is diﬀerent about the eﬀective samples sizes for the parameters marked
with an asterisk in that table? Why are they diﬀerent?
11.8 (Weight of Evidence). Using the numbers from Table 11.3, calculate
the expected weight of evidence for Skill 1 provided by Item 6 and by Item 8.
Explain the diﬀerence.
11.9. Items 6 and 8 are simple structure tasks in that they tap only a single
proﬁciency variable. All of the other tasks in the mixed number subtraction
test are not simple structure. Does the expected weight of evidence for a single
skill depend on the previously seen observables for a simple structure task?
For a nonsimple structure task? Explain.
11.10 (Many Person Fit Tests). A psychometrician analyzes a data set
with 2000 students and ﬁnds that the person ﬁt statistic ﬂags 100 of them
(using .95 as the threshold). Does this indicate a problem with the model?

11.4 Closing Comments
407
11.11 (Length of Bars in Observable Characteristic Plots). Look at
the observable characteristic plots, Figs. 11.8 and 11.9. The error bars for
Equivalence Classes 4 and 5 are typically longer than for the other equivalence
classes. What is the most likely reason?
11.12 (Nonmonotonic Patterns in Observable Characteristic Plots).
Look at the observable characteristic plots in Fig. 11.8. Some of them, such
as those for Tasks 6, 10 and 12 clearly follow a monotonic pattern: the chance
of success is better for those students in higher numbered equivalence classes.
Other plots, such as those for Tasks 4, 7, and 11, show a nonmonotonic pat-
tern where the success probability is higher for Equivalence Class 5 than for
Equivalence Classes 6 or 7. Is this cause for concern?
11.13 (Model Fit). Agree or disagree: Assessment designers should always
chose the model that has the best value of the model ﬁt statistic. Justify your
choice.

Part III
Evidence-Centered Assessment Design

12
The Conceptual Assessment Framework
The ﬁrst two parts of this book have dealt mainly with the mathematics of
using Bayesian networks: ﬁrst for scoring assessments, and then for calibrating
scoring models to observed data. Aside from brief digressions in Chaps. 2 and
7, it has largely neglected any formal discussion of assessment design. This
follows a common trend because assessment design, as a formal discipline, may
be the most neglected area of study within professional measurement. The
principles of design typically get short shrift in treatments of psychometrics
and educational measurement in favor of statistical and technical topics.
This may be due, in part, to the fact that in most measurement organiza-
tions and institutions responsible for testing programs, the responsibilities of
test construction and scoring are divided among professionals with very dif-
ferent and speciﬁc skill sets and interests. It is uncommon for content experts
who write items to interact with the statistical analysts beyond the discus-
sion of items that are inconsistent with the scoring model employed for the
test or how they perform in pretesting. Common practice is for test develop-
ment, administration, and scoring to be “silos” of responsibility that seldom
interact except at key hand-oﬀpoints in the work. Therefore, discussions are
targeted toward a particular professional’s responsibilities and making sure
the “hand-oﬀ” to another team goes smoothly.
Getting hand-oﬀs right requires a common understanding of what is
expected from each team. As an analogy, the common understanding when
constructing a building is provided by the blueprints. Looking at the common
blueprints, carpenters, stonemasons, plumbers, electricians, painters, landsca-
pers, project managers, and inspectors can all see what is expected and plan
for their tasks, and leave appropriate spaces for other craftsmen to work. In a
similar way the conceptual assessment framework (CAF) is designed to serve
as the blueprint of an assessment. It should provide information to the teams
that work on the assessment about what their roles are and what the hand-oﬀs
are.
There are situations in which a formal design is not necessary. A single
person building a tool shed does not need a full set of blueprints; a rough
c⃝Springer Science+Business Media New York 2015
411
R. G. Almond et al., Bayesian Networks in Educational Assessment,
Statistics for Social and Behavioral Sciences, DOI 10.1007/978-1-4939-2125-6 12

412
12 The Conceptual Assessment Framework
sketch on the back on an envelope can be suﬃcient. Likewise, an experienced
team of barn raisers do not need formal blueprints; each new barn is similar
enough to the last that each of the members of the team can follow their
practiced roles.
A formal design is similarly not necessary for a teacher designing a class-
room quiz or a professional testing organization producing additional forms
for a well-established testing program. When an assessment is meant simply to
gauge students’ overall proﬁciency in some domain of tasks, and the tasks are
independent performances that can be scored on low-to-high scale (e.g., right
or wrong, ratings on a set of essays), the information for the hand-oﬀbetween
test developers and psychometricians is correspondingly simple: “Here are the
item data, and for each item, a higher score is better than a lower score.” Item
writers might employ deep content knowledge and subtle insights into each
item and psychometricians might ﬁt wonderfully complex models, but as long
as this assessment paradigm holds, the channel of information can be narrow
and the professionals at either end do not need to know the details at the
other end.
Notice that these situations share several elements that make it possible to
get away without an explicit design (at least seemingly). It is presumed that
the claims underlying the assessment purpose are suﬃciently well understood;
that the kinds of observations needed to back them are produced in the tasks;
the scoring procedures capture the salient features of tasks performances; the
task scores are combined in a way that conveys their evidentiary value; and
the results are accurate enough to serve the purpose. This may be the case in
some instances, but there are clearly several places where the argument could
break down. Even in informal or familiar assessments, it is worth thinking
through just what one wants to make inferences about, and how suitable
evidence might be obtained. A design framework could improve assessment
practices that have grown familiar and comfortable.
More complex assessments are the context for discussing evidence-centered
assessment design (ECD) in this book, but ECD is based on a general form
of thinking, hypothetico-deductive reasoning (i.e., the scientiﬁc method). It
is relevant as well to less formal assessment situations, and more broadly to
situations almost anywhere that information is being produced and evaluated,
such as simple classroom Socratic practice or evaluating homework responses.
We activate this kind of thinking, almost automatically, rapidly, iteratively,
many times a day. Employing it more consciously and frequently in even
routine, wholly familiar, or spontaneous assessment situations, both casual
and formal, would be beneﬁcial when a formal design process is not practical.
The need for a formal design becomes stronger when the assessment con-
tains elements that are new, unfamiliar, or complex. There may be a desire
to link assessment more tightly to theories of learning in the domain (Pelli-
grino et al. 2001). Psychometricians may want to provide test developers or
teachers with deeper insight into what scores and statistics mean in terms of
student learning (Wilson 2004). An organization may want to develop reusable

12 The Conceptual Assessment Framework
413
elements for constructing and delivering tasks (Luecht 2012). There may be
multiple aspects of proﬁciency, and diﬀerent tasks may require diﬀerent mixes
of them, as with the Bayes nets psychometric approach discussed up to now.
One of the primary advantages of Bayesian networks as scoring models
for assessment is their ﬂexibility; that ﬂexibility can also present challenges.
ECD helps us exploit the ﬂexibility of Bayesian networks. The intent of ECD
is that the ECD design process forms the knowledge engineering required
to build the appropriate Bayesian network model, while at the same time
an understanding of the models and methods required in scoring adds both
supporting structure and constraints to task development.
Part III takes up the neglected topic of assessment design. This chapter
focuses on the CAF—the ECD blueprint—which is the result of a design
process beginning with an examination of the claims an assessment needs
to ground and the evidence it needs to back them. Chapter 13 presents an
idealized delivery model for an assessment. Chapters 14 and 15 then illustrate
the ideas of integrated assessment design and analysis with a prototype biology
assessment called Biomass that used complex tasks, a multivariate model of
proﬁciency, and web delivery. Chapter 14 describes the assessment design,
and Chap. 15 works through scoring and the calibration of the scoring model
with ﬁeld trial data. Finally, Chap. 16 talks about some of the possibilities
for assessments that ECD and Bayesian networks open up.
Fig. 12.1 The principal design objects of the CAF
Reprinted from Mislevy et al. (2004) with permission from the Taylor & Francis
Group.

414
12 The Conceptual Assessment Framework
Figure 12.1 (repeated here from its original appearance in Chap. 2) sum-
marizes the parts of the CAF. This chapter walks through the CAF frame-
work with simple examples from a hypothetical English language placement
assessment. The Biomass example in Chaps. 14 and 15 is real and richer.
The interested reader will ﬁnd further applications in the literature, including
the design rationale of the Dental Interactive Simulation Corporation (DISC)
scoring engine (Mislevy et al. 2002d), redesign in the College Board Advanced
Placement examinations (Huﬀet al. 2010), unobtrusive assessment in learning
games (Shute and Torres 2012), and the Cisco Networking Academy’s NetPass
simulation-based assessment prototype (Williamson et al. 2004b; Williamson
et al. 2006a) and its descendant Packet Tracer simulation-based assessments
(Behrens et al. 2012).
Section 12.1 starts the chapter by discussing the design process and the
critical ECD concept of claims. The next six sections discuss the six models of
the CAF: the proﬁciency models (Sect. 12.2), the task models (Sect. 12.3), the
evidence models (Sect. 12.4), the assembly model (Sect. 12.5), the presentation
models (Sect. 12.6), and the delivery model (Sect. 12.7). Section 12.8 discusses
how these six models work together to form the complete assessment.
12.1 Phases of the Design Process and Evidentiary
Arguments
Chapter 2 already introduced the six models that comprise the CAF. However,
building those models starting from a description of purpose and a diverse
collection of knowledge about a domain of interest can be a complex process
(Mislevy, Steinberg, and Almond 2003b). It is a process of building a coherent
evidentiary argument, providing the right kinds of evidence about the right
kinds of proﬁciencies to serve the purpose of the assessment, then embody-
ing it in speciﬁcations for all of the materials, activities and processes that
constitute an operational assessment. Section 12.1.1 gives a brief description
of domain analysis and domain modeling processes. Section 12.1.2 describes
what is perhaps the most important output of the initial domain modeling:
the set of claims that ground the deﬁnition of the CAF models, and serve as
the ﬁrst sketch of the evidentiary argument of the assessment.
12.1.1 Domain Analysis and Domain Modeling
Almond et al. (2002a) provides an idealized picture of assessment delivery
(Chap. 13 discusses how this process looks when the assessment uses Bayesian
networks as the psychometric model). The CAF serves as a blueprint for the
pieces of that assessment delivery system and requirements for the processes
that make it work. However, the CAF does not spring fully formed from the
heads of the assessment designers. As with any design process, the design-
ers must ﬁrst gather information about the requirements and the constraints

12.1 Phases of the Design Process and Evidentiary Arguments
415
of the domain in which they are operating. Then they need to examine a
number of trade-oﬀs exploring possible alternatives and weighing competing
objectives. Once the overall shape of the assessment is clear, ﬁner-grained
design decisions go into completing the CAF.
ECD divides the design process into three stages:
Domain Analysis. The process of gathering and organizing the requirements
and information about the domain of the assessment.
Domain Modeling. The development of the central evidentiary argument of
the assessment, and sketching the basic models of the CAF.
Conceptual Assessment Framework. Fleshing out the argument from the domain
model to make a complete description of the assessment.
As with any design process, it is possible and even desirable to revisit and
reﬁne work done at earlier stages as necessary later in the process. Except in
well-understood domains and familiar kinds of assessment, iterative design is
the norm, rather than a “waterfall” design process. Task prototypes and pilot
testing are typical, and can spark not only revisions in task design and scoring
procedures, but revisions to the argument and needs to gather additional
information about the domain. Nevertheless, the ECD process encourages the
designers to think through the issues involved with ﬁtting the pieces of the
assessment together at an early conceptual phase, thus avoiding committing
resources to something that will require extensive changes later to make it
work.
The goal of the domain analysis phase is organizing the information about
the domain, which will inform the design of assessment elements. Further, it
will provide warrants and backing for the evidentiary arguments for claims of
interest in the intended uses of an assessment. In some cases, the challenges
will be sorting through a vast literature for a well-studied domain and the
need to reconcile conﬂicting viewpoints. In other cases, the challenge will be
ﬁnding relevant prior work on the constructs to be measured. In these cases,
the design team may need conduct cognitive analyses: observing the behavior
of experts and novices performing relevant tasks in natural or laboratory
conditions.
In either case, the challenge lies in integrating insights from psychological
theories about learning and cognition with a substantive model of performance
in the domain of interest (Mislevy 2006; Mislevy 2010). The ﬁnal model used
for the assessment will never capture all of the nuance in the psychological the-
ories. The design team will need to make simpliﬁcations to ﬁt the assessment
within time and cost constraints. This simpliﬁcation happens as the design
team moves from domain analysis to domain modeling to the ﬁnal CAF. Some
aspects of proﬁciency can be assumed in the population, for example; others
are not relevant to the purpose; target inferences may be needed only at a
coarser grain size than research studies have addressed. Good documentation
during the domain analysis phase allows those simplifying assumptions to be
revisited if they cause problems when the assessment is ﬁeld tested.

416
12 The Conceptual Assessment Framework
There is often a great deal of information already available that will be
useful in designing an assessment, but it has been gathered for purposes other
than assessment design. It may be pertinent, but it it is not clear just how.
One can begin by organize information along lines that relate to elements of
assessment arguments. Mislevy, Steinberg, and Almond (2003b) suggest the
following categories:
Valued work. Real world situations in which people do the kinds of things and
use the kinds of knowledge we care about.
Task features. Aspects of situations that vary to impact their diﬃculty or
evidentiary focus, or just make similar tasks seem diﬀerent. Cognitive
task analyses are particularly helpful here.
Representational forms. Ways that knowledge is expressed and represented in
the domain (e.g., graphs, diagrams, symbols, tables, and vocabulary that
must be understood), and the ways people use these representations and
situations they use them in.
Performance outcomes. Ways of distinguishing among performances and their
outcomes, such as aspects of quality, eﬃciency, and strategy use, and cri-
teria for recognizing “successful” performances.
Valued knowledge. Knowledge, skills, and abilities that are considered impor-
tant in the domain.
Knowledge structure and relationships. Information about how knowledge is
structured, such as prerequisite relationships, curricula, and knowledge
maps.
Knowledge-task relations. Information about which knowledge, skills, and
abilities are required for which tasks. Cognitive task analyses are help-
ful here too.
Some pieces of information fall into more than one category. Lack of infor-
mation in a category can indicate the need for additional research, perhaps
through additional literature searches, discussions with experts, or experi-
ments that ﬁll in gaps.
Together, this domain information and purpose of the assessment will help
the designer deﬁne the claims the assessment should address and ways to get
evidence to support them. The work of domain modeling can begin when
suﬃcient information has been gathered (although one expects to cycle back
to get further information when design work shows it is needed). The goal of
the domain model is to sketch the evidentiary argument of the assessment in
enough detail to identify the critical points of the assessment design and make
sure that enough resources are available to complete the design.
A domain model consists of a collection of objects called paradigms. The
paradigms of the domain model are lightweight versions of the more detailed
models of the CAF. The reason for building the domain model before the CAF
is to avoid the potential rework that might be required to develop the more
technical and detailed designs, until the design as a whole is suﬃciently worked
through that all of the stakeholders involved in the design process are satisﬁed

12.1 Phases of the Design Process and Evidentiary Arguments
417
to move ahead. Domain modeling is meant to be a stage where experts from
diﬀerent areas who might be involved in assessment design—subject matter
experts, psychometricians, software designers, teachers, cognitive scientists,
graphic designers—can talk with each other about what is needed in the
assessment argument and how it might be instantiated, discussing options
and trade-oﬀs that cut across their specialties.
Although a paradigm is less detailed than a CAF model, the level of detail
required will vary with the application and the similarity of the assessment
to other assessments that the design team are familiar with. Suppose initial
design discussions suggest that for the targeted claims and necessary evi-
dence, it will suﬃce to use a familiar form of task, such as an item type used
in similar assessments. The task paradigm then only needs to reference the
previous work, and indicate how task features and performances will provide
the necessary evidence. On the other hand, when it appears that a new kind of
simulation task will be needed to obtain evidence for the targeted claims, the
task paradigm might need to be quite detailed before the content specialists,
the computer implementation team, and the psychometricians are convinced
of its feasibility and evoked evidence to try a prototype.
Example 12.1 (Language Placement Test). University C holds a special
4-week language course before the fall semester for foreign students. The pur-
pose of the course is to ensure that students have suﬃcient English language
competence to take part in academic life. Several sections of the course are
oﬀered which diﬀer in their emphasis (for example, reading and writing versus
speaking and listening), and many students have suﬃcient English language
skills that they do not need the course all. Therefore, University C oﬀers a
placement exam to all incoming foreign student to determine how best to
place them into the appropriate section of the English class. 1
Note that the two most fundamental parts of the assessment design have
already been speciﬁed in this brief introduction to the Language Placement
Test example: (1) the purpose of the assessment and (2) the targeted popula-
tion. Both of these are critical ﬁrst steps in any design process.
The purpose is the declared intended use(s) of the assessment, including
its primary and secondary uses (e.g., course placement and performance feed-
back to the student). This deﬁned purpose will drive all subsequent stages
of the assessment development to ensure that the assessment is appropriate
for its intended use. As the initial step, all ECD claims (Sect. 12.1.2) will be
speciﬁed in terms of how they support the stated purpose of the assessment.
1 Some of the issues discussed in this example are similar to those arising in the
redesign of Educational Testing Service’s (ETS’s) Test of English as a Foreign
Language (TOEFLTM) (Chapelle et al. 2008). Indeed, this example draws on
conversations with members of the TOEFL redesign team. However, the purposes
of this hypothetical example and TOEFL are not the same, so the eventual design
decisions are not, nor should they expected to be, the same.

418
12 The Conceptual Assessment Framework
In this case, the purpose of the assessment is clearly stated as being for place-
ment into the appropriate level of language course oﬀered by University C.
Implicit in this stated purpose is the idea that the content of the assessment
must correspond to the intended purpose of the course, that is promoting the
English language abilities necessary to participate in academic life. However,
even that requires further reﬁnement. Does “academic life” include holding a
discussion with the registrar to straighten out a conﬂict of the student’s class
assignments? Requesting information from a cafeteria worker about whether
or not an entr´ee meets the student’s dietary restrictions? The design team
will want to examine the range of activities using English that students will
encounter in the university—Bachman and Palmer (1996) call these “target
language uses”—and determine the range and the features of those situations
as a guide for task models. For this example, we will use a restricted speciﬁ-
cation that only addresses the classroom context.
The targeted assessment population has also been speciﬁed. The popula-
tion is a complete deﬁnition of all individuals (or groups) who are eligible
to sit for the assessment and to whom the purpose of the assessment will be
applied. In this example, the population is deﬁned to be all newly matriculated
students who come from a country that does not have English as its primary
language. Note that even with this explicit deﬁnition of the population there
are important subaspects of the population deﬁnition that may need to be
made explicit, such as by specifying the expectations about prior qualiﬁca-
tions or conditions of the population. Some knowledge or skills may be critical
to performance, yet need not be included in the psychometric model because it
will be known aforehand that all examinees are suﬃciently proﬁcient in these
respects. For example, it may be worthwhile to specify that the deﬁnition of
population as incoming freshmen presumes that all such freshmen have been
subjected to an admissions process that ensures that they all have some basic
proﬁciency in English.
It is important to make the description of the population and purpose
explicit: these critical requirements will drive many of the subsequent design
decisions. It is better for an assessment to serve a single purpose well than
to do a poor job of supporting many purposes. The domain modeling process
helps clarify purposes and highlight trade-oﬀs that are involved. Moreover, a
domain model helps a design team think through what needs to be changed
about an assessment to meet a new purpose (Fulcher and Davidson 2009).
The population and purpose persist from the domain model to the more
detailed CAF. The another aspect of the design that is important to spec-
ify in the domain modeling phase is the basic evidentiary argument of the
assessment.
12.1.2 Arguments and Claims
The proﬁciency variables and observable variables are pieces of machinery to
aid reasoning in assessment. The Bayes nets help us express the relationships

12.1 Phases of the Design Process and Evidentiary Arguments
419
between what students know and what they do, combine evidence across mul-
tiple observations, and characterize what we know from evidence and what we
do not know. The pieces of machinery—the variables, the conditional probabil-
ities, the independence relationships—take their meanings from an underlying
evidentiary argument (Mislevy 2006).
Toulmin (1958) provides a schema for the structure of arguments (Fig. 12.2).
In assessment, its content is developed from the information gathered in
domain analysis activities. The focus is a claim, or targeted inference, which in
assessment is a statement about the participant that we wish to establish. For
example, in the Language Placement Exam, a claim might be that a student
can write plans for future study. The data are observations we can make about
the participant that would cause us to believe that the claim does or does not
hold. A piece of data might be the rated quality of a written response to the
question, “What are your academic goals and how do you plan to achieve
them?” on the student’s application.
The most obvious data in educational assessments are these aspects of
what examinees say or do or make. There are two additional kinds of data
in assessment arguments as well, however. First is features of tasks, or the
situations that examinees act in that make their performances meaningful as
evidence about their capabilities. That is, what is it about this situation that
examinees’ actions here provide evidence about their knowledge and skills?
The features of Tatsuoka’s mixed-number subtraction tasks, for example, indi-
cate which skills would be required for a correct response. The Q-matrix and
the response together are needed to ascertain the evidence that the response
data from examinees’ performances convey.
Second is additional information about the examinees the assessor has,
which can also be critical to interpreting response data. Some information of
this kind is known generally about the examinee population. It is assumed
that the mixed-number subtraction examinees are familiar with fraction rep-
resentations, so even though this skill is critical to performance, it does not
need to be in the psychometric model—it implicitly has a value of “mastered
at the required level.” In some assessments, what is known speciﬁcally about
individuals may be known, such as which method of mixed number subtrac-
tion they studied, so that the appropriate Q-matrix can be used to interpret
the evidence in their responses.
The arrow that goes from speciﬁc data up to a speciﬁc claim—an inference
about a particular examinee based on her responses—is justiﬁed by a warrant.
A warrant is a generalization that underlies the assessment’s construction:
why a task with such-and-such features is likely to evoke observably diﬀerent
performances from examinees with diﬀerent proﬁciencies. In mixed-number
subtraction, the warrant is that students with the requisite skills are likely to
make correct responses, and those lacking in skills are likely to make incorrect
responses. This warrant justiﬁes speciﬁc conclusions for each student’s par-
ticular pattern of responses. Similarly, the warrant in the running application
example would be that usually students who are able to write a good essay

420
12 The Conceptual Assessment Framework
Warrant
Backing
Claim
Data
Alternative
Explanation
Rebuttal
on account of
so
since
unless
supports
Fig. 12.2 Toulmin’s structure for arguments
Reprinted from Mislevy et al, (2003a) with permission from The National Center
for Research on Evaluation, Standards, & Student Testing (CRESST), UCLA.
on their application are usually also able to write other material related to
plans and goals, for example, papers describing a class project.
A warrant requires backing. In mixed-number subtraction, the backing
is teacher experience, the instructional design, and the cognitive analysis of
Tatsuoka and her colleagues (Klein et al. 1981). In the application example,
the backing is the experience at University C with the relationship between
student admission essays and later student work. We might also have results
from prior studies or theoretical work on writing we can use as part of the
backing. The material gathered during the domain analysis is an important
source of backing.
In any particular case the relationship between the data and claim may not
hold. There can be alternative explanations. In the case of the admission essay,
somebody else could have written the essay for the student. Some information
will support an alternative explanation in a particular case, while other infor-
mation will weaken it. Data both for and against an alternative are included
in the box labeled Rebuttal data.2 For example, we could observe that the
student’s writing performance is very diﬀerent in proctored and unproctored
writing samples, leading us to suspect that the student is receiving some kind
of assistance. Many assessment design decisions are meant to reduce the force
of alternative explanations: making sure tasks actually evoke the proﬁciencies
we are interested in, or reducing the demand for extraneous knowledge. From
a measurement perspective, this is reducing the validity threats of construct
underrepresentation and construct irrelevant variance (Messick 1989).
The next step in the language placement test example is to elaborate
a set of claims appropriate to the purpose. At the highest level the claims
are generally too broad to be useful. Often the claims are broken down in a
hierarchical fashion to get to something speciﬁc enough to guide task design.
As we see in the following example, there need not be a one-to-one relationship
between claims and proﬁciency variables.
2 See Schum (1994) for in-depth discussions of the elements of arguments and their
broader relation to probabilistic inference.

12.1 Phases of the Design Process and Evidentiary Arguments
421
Example 12.2 (Language Placement Test Claims). Given the purpose
of the test, the primary claim should be about the student being ready to
operate in the classroom without need for further instruction. It is immediately
apparent that this breaks down further into the ability to read, write, speak,
and listen suﬃciently well to participate in the classroom; that is, to engage
in the kinds of interactions involving language, using the forms and genres,
for the kinds of purposes, around which teaching and learning occur in the
classroom.
However, reading, writing, speaking, and listening are themselves complex
constructs, and need further speciﬁcation. The following set of more detailed
claims helps a task designer consider what kinds of situations and what kinds
of performances are needed to give a concrete meaning to the higher-order
claims, and thus what kinds of evidence will eﬀectively give concrete meaning
to proﬁciency variables. How detailed the proﬁciency model is depends partly
on the purpose of the test and partly on how informative ﬁner-grained reports
would be. If proﬁciencies are highly correlated and there are few tasks inform-
ing each of the ﬁner-grained claims, then separate measures for them may add
nothing but noise to measures of the coarser claims they deﬁne (Haberman
2005b). The claims below start to ﬂesh out what an assessment would look
like, even though proﬁciency variables and reports could be at the composite
levels of reading, writing, speaking, and listening.
•
Student has suﬃcient communicative competence in English that he or
she can get the full beneﬁt of participation in classroom activities.
W. Student can write English well enough to get the full beneﬁt of partic-
ipation in classroom activities.
W.1 Student has suﬃcient mastery of the mechanics of writing in English
to produce texts with an acceptable rate of errors.
W.2 Student has suﬃcient mastery of the academic style in English to
produce texts appropriate for the classroom.
W.3 Student has suﬃcient mastery of the organizational elements of
written English to produce texts of the genres found in typical
classroom activities.
W.3.1 Student has command of the paragraph structure, and appro-
priately breaks documents into paragraphs.
W.3.2 Student can clearly state the thesis of an argumentative essay.
W.3.3 Student can clearly state the conclusion of an argumentative
essay.
W.3.4 Student supports arguments with evidence in writing.
W.3.5 Student appropriately uses function words that indicate the
structure of the document.
. . .
W.4 Student can express ideas about a topic for which they have some
knowledge or opinion using written English.

422
12 The Conceptual Assessment Framework
. . . [breaks down to kinds of situation, purposes, and targeted per-
formances that constitute aspects of reading proﬁciency as needed
in the classroom contexts.]
R. Student can read English well enough to get the full beneﬁt of partici-
pation in classroom activities.
. . .
L. Student can listen to spoken English well enough to get the full beneﬁt
of participation in classroom activities.
. . .
S. Student can speak English well enough to get the full beneﬁt of partic-
ipation in classroom activities.
. . .
Only part of the breakdown is shown here. To complete the design, W.1,
W.2, W.3, and W.4 would require further elaboration which will almost cer-
tainly produce lower-level claims. (We note in passing that being able to
specify claims at this level does not mean tasks will need to be one-to-one
to assess them; we will see how richer tasks, with more ﬁdelity to valued
real-world situations, can provide evidence to support multiple claims. We
may have to do some work to make proper sense of the complex data, some
in identifying the evidence and some in properly modeling its relationships
in the statistical model, but more importantly in designing tasks that will
produce the necessary evidence.)
There are some general terms in these standards that require further spec-
iﬁcation. For example, what is meant by “full beneﬁt of participation”? This
could be diﬃcult to measure as there are large individual diﬀerences in the
beneﬁts that native English speakers gain from participating in a class. One
deﬁnition that might work is that the average beneﬁt for people who can meet
this claim is similar to the average beneﬁt for native speakers.
Further, these claims will need to be operationally deﬁned by the situations
in which they are relevant and what we would want to see people doing in those
situations to consider the claim satisﬁed—that is, we only fully understand
the claim when we know the evidence that would ground it. There is some
broader range of situations and actions in the real world we care about. There
is some narrower range of them that is practicable to build into an assessment.
For example, University C probably cares deeply about the student’s ability
to write term papers, but there is only time for the student to write a short
essay on the placement test. Working out the connection between the two is
the realm of validity argumentation—far too big to grapple with in its entirety
here, but a contemporary take on the key issues can be found in Kane (2013),
Messick (1994), Mislevy (2009), and Moss et al. (2006).
These claims are represented stars in the Proﬁciency Model in Fig. 12.1.
While in the ﬁgure, the stars are unconnected, claims are more naturally
thought of in hierarchical structure in which some claims are actually sub-
claims of more general claims. In Example 12.2 the highest level communica-

12.1 Phases of the Design Process and Evidentiary Arguments
423
tive competence claim is immediately decomposed into four claims relating to
the modal skills of Reading, Writing, Speaking, and Listening. The example
goes on to show the next level of breakdown for the Writing branch.
Writing is a high-level skill, and as such it is a composite of many compo-
nent skills, and the ability to marshal them eﬀectively in particular situations.
Indeed, despite their overlap, just what “writing proﬁciency” means will vary
across purposes and contexts; the writing proﬁciency needed for college class-
rooms is not the same as what is needed for the factory ﬂoor or the law oﬃce.
That is why just saying we would like to assess writing proﬁciency is not suf-
ﬁcient to design an assessment. We really need to be more explicit about the
claims we want to make and the evidence we need to see. The language proﬁ-
ciency example shows one possible breakdown (based on Deane and Quinlan
2010). However, these next-level skills are themselves compound. The example
shows a breakdown for the next level of Claim W.3 (organization in writing).
The next level starts to identify some of the elements that go into organiza-
tion. At this point we can start to see how we might be able to ﬁnd evidence
for the speciﬁc claims within a student’s writing and how we might design
tasks to elicit that evidence. We must get into at least this level of detail to
be able to eﬀectively use the claims as a basis for test design.
In the speciﬁcation of such a hierarchy of claims for the assessment,
the assessment designer must determine which claims are intended to be
reportable claims and which are used to support the structure and design
of the resultant assessment. Reportable claims are those that are expected to
directly appear on the score report as actionable items. That is, as pieces of
information that are intended to be suﬃcient for decision making. By implica-
tion, this means that certain claims have an expectation (preferably explicit)
of reliability. In the example provided above, the main claim and the claims
based on the four modal skills (Claims R, W, S, and L) are intended to be
reported and used for the placement decision. The claims that are lower in the
hierarchy provide conceptual support for drawing the conclusions speciﬁed in
the primary reportable claims, as well as inspiration for task design.
If the purpose of the assessment were diﬀerent, say providing diagnostic
feedback, some of the lower-level claims may also be designated as reportable
claims. Because the purpose of diagnostic feedback does not demand as high
a reliability, the claims used for only this purpose do not require as much evi-
dence to support them. However, care needs to be taken on score reports that
are designed for mixed purposes to ensure that the test user clearly under-
stands the distinction between primary reporting variables that are supported
by larger bodies of evidence and secondary diagnostic feedback variables that
have weaker evidential support.
Another important issue is to check the alignment of the claims and the
decisions that will be made with them. Suppose University C plans to oﬀer
three sections: one emphasizing Reading and Writing, one emphasizing Speak-
ing and Listening, and one covering all four topics. There is also an implicit
fourth section, which corresponds to placing out of the language course alto-

424
12 The Conceptual Assessment Framework
gether. Can we properly place students on the basis of these claims? Yes.
Students for which Claim S and Claim L hold but not Claim R and Claim W
are best suited for the ﬁrst section; students with the opposite pattern are
best suited for the second section; students for which three or more of the
model claims do not hold are best suited for the third section; and only stu-
dents for which all four claims hold should be excused from the English course
completely.
Recall the inﬂuence diagram presented in Sect. 4.5.1. A key lesson from
that diagram was that information has value when it can be used to make
better decisions than could be made without the information. In terms of
the claims it means that the claims for which the assessment provides evi-
dence must provide information that an educator can use to make conditional
decisions that are better that what could be done without the evidence.
In summary, claims provide an explicit representation of what assessment
results need to address for the intended purpose. They determine the “evi-
dence about what” the assessment will need to elicit. They hold implications
for the required test composition and length (to support required levels of
reliability for reportable claims). Claims per se can be evaluated with respect
to qualitative characteristics: their usefulness, reportability, and ﬁt to the
purpose of the assessment.
12.2 The Student Proﬁciency Model
The claims that are established for a given assessment deﬁne, in terms of
intentions and semantics, the complex of knowledge, skills, and abilities to
be assessed. A proﬁciency model deﬁnes their counterparts in the syntactic
space of the psychometric model, including the relationships among them.
Specifying their relationships to observable variables in tasks deﬁnes them
operationally. Section 12.2.1 describes how claims are organized through the
use of proﬁciency variables. Bayesian networks are useful for representing pro-
ﬁciency models when the models contains more than one proﬁciency variable;
Sect. 12.2.2 describes how to draw the graphical structure in this case. Sec-
tion 12.2.3 describes how to deﬁne reporting rules which go from the proﬁ-
ciency variables (or, more properly, probability distributions over them) to
scores that are reported.
12.2.1 Proﬁciency Variables
Proﬁciency variables are the machinery through which data from student per-
formance is synthesized in some form as evidence for claims. As mentioned in
the previous section, there is not necessarily a one-to-one relationship between
claims and proﬁciency variables. Mislevy, Almond, and Steinberg (2002b)
describes a number of approaches an assessment designer can relate claims

12.2 The Student Proﬁciency Model
425
and proﬁciency variables. The following two ﬁt particularly well with Bayes
nets proﬁciency models.
•
One can encompass multiple claims with a single proﬁciency variable with
a ﬁnite number of levels. Each value of the proﬁciency variable matches up
one-to-one with a particular claim or set of claims, as discussed in Exam-
ple 12.2. The American Council on the Teaching of Foreign Languages’s
guidelines for reading, for example, have 11 levels (Swender et al. 2012).
They range from Low Novice, in which the student can typically only use
language in the reading modality in rudimentary ways, up through Supe-
rior. Each level is described in terms of several kinds of things a typical
student at that level can do, in situations with certain key features, each
of which could be formalized as a claim in its own right. An excerpt from
the Mid-Intermediate level includes the following:
At the Intermediate Mid sublevel, readers are able to understand
short, non-complex texts that convey basic information and deal
with basic personal and social topics to which the reader brings
personal interest or knowledge, although some misunderstandings
may occur. Readers at this level may get some meaning from short
connected texts featuring description and narration, dealing with
familiar topics. (p. 23)
Exactly what “able to read” means would need to be speciﬁed in terms of
what kinds of performances are expected in what kinds of situations; this
statement does not yet say what the evidence needs to be to support a
claim like this. However, the features of tasks that would be required begin
to appear in the statement. Note that the phrase “dealing with familiar
topics” indicates information is needed about the relationship of a student
and a text, since a topic that is familiar to one student may not be familiar
to another. The intent is that statements within a level go together well
enough to characterize a student in terms of a single level, although there
will be some performances above or below that level. This diﬀuseness is a
cost of supporting many distinct claims with a single proﬁciency variable.
•
An alternative approach is useful when claims concern being able to per-
form at various levels in certain kinds of situations, and the situations
require multiple proﬁciencies in various combinations and at various lev-
els. Distinct proﬁciency variables are then used to maintain belief about
distinct aspects of knowledge and skill, and a claim is associated with par-
ticular patterns across them as they are called upon in settings that stress
or combine them in diﬀerent combinations. Students’ proﬁciency in such
a domain can be described in terms of which skills they possess at what
levels (via proﬁciency variables), tasks can be described in terms of which
skills they require (via task-model variables), and the outcomes expected
from any particular matchup can be described in terms of values of observ-
able variables. A claim can be stated about a student’s likely performance

426
12 The Conceptual Assessment Framework
in tasks with a given conﬁguration of features. The evidence for such a
claim is contained in the proﬁciency as the joint distribution for the par-
ticular skills in the particular combinations that are called for by tasks
with these features. Mixed-number subtraction, and cognitive diagnosis
models in general, are a familiar special case of this approach.
The student proﬁciency model thus describes the possible states of knowl-
edge, skill, and ability that we expect to see among the members of the target
population, as seen through the lens of the model. Diﬀerent values of the
proﬁciency variables correspond at some level to claims.
Depending on the purpose of the test, not all possible states of a proﬁciency
model are interesting. In the Language Placement Test example, it is not
necessary to make distinctions among students who have mastered all the
material covered in the course. Similarly, we do not need to cover very low
states of English proﬁciency as such students would not apply or be admitted
to the University. Thus, which states of proﬁciency we consider is colored by
the purpose of the assessment.
The usual way to describe the proﬁciency state of a student is through
one or more proﬁciency variables. This produces a factored representation of
the possible proﬁciency states. For example, in the Language Placement Test
example, it is natural to introduce variables to represent Reading, Writing,
Speaking, and Listening. A proﬁciency proﬁle is a set of values for each of those
variables, and each proﬁciency proﬁle represents a possible state of proﬁciency
for a member of the target population.
In addition to deciding which aspects of proﬁciency to represent as proﬁ-
ciency variables, the design team must decide whether the variables are dis-
crete (categorical) or continuous. Discrete variables seem more natural when
the purpose of the assessment is to classify students into groups: those for
whom a set of claims hold, and those for whom the claims do not hold. Con-
tinuous variables seem more natural when the purpose requires rank ordering
the students as in selection decisions.
Even though the choice between continuous and discrete variables seems
important, it is actually fairly easy to derive categorical scores from contin-
uous variables and continuous scores from discrete variables, as we did in
Example 10.1. To get a categorical score from a continuous variable, all that
is needed is a set of cut scores which divide the continuous space into reason.
These are often set by standard setting committees, and there is a substantial
literature on various methods for setting the cut scores (Hambleton and Pito-
niak 2006). Going from discrete to continuous, there are two distinct possible
methods. The ﬁrst is to pick a state of the proﬁciency variable and report
the probability that the student is in that state. The second is to assign a
numeric value to each proﬁciency state, and to take the expected value. Sec-
tion 8.5 describes one method for translating between continuous and discrete
variables in detail.

12.2 The Student Proﬁciency Model
427
Another consideration when choosing between continuous and discrete
variables is the algorithms used to update the proﬁciency model when evidence
is observed. Chapter 5 develops the model for discrete variables. If continuous
variables are used instead, then many of the summations in that algorithm
become integrations. In the usual models for educational testing, proﬁciency
variables are parents of the observables. If there are multiple continuous pro-
ﬁciency variables and the observables are discrete, then the required integrals
cannot be solved in closed form (Lauritzen 1996). However, this case is essen-
tially Multidimensional Item Response Theory (MIRT), and approximation
algorithms have been studied in MIRT (Reckase 2009).
Whether categorical or continuous, the proﬁciency variables must be
deﬁned well enough to pass the clarity test. The claims are useful for pro-
viding eﬀective deﬁnitions of the variables. Consider the case of an ordered
categorical variable. When comparing learners in one state to learners in the
next higher state, there should at least one additional claim that holds for
learners in the higher state. Thus, the claims provide the target deﬁnition of
the variables, as well as the task features and the performance expectations
around which tasks will be constructed.
In science education, for example, advances have been made in the topic
of learning progressions (Alonzo and Gotwals 2012). A learning progression is
marked by increasingly levels of sophistication in reasoning in a domain, which
are codeﬁned in terms of features of task situations and expected performances
that are evidence of performance at those levels. Zalles et al. (2010) show how
build assessments around learning progressions using ECD, and West et al.
(2012) show how to model the resulting data in a Bayes net.
The relationship between claims and continuous variables is a little bit
more complex. As a working deﬁnition, the claims should map to a speciﬁc
point on the scale, i.e., the claim should hold for learners above a certain
point on the scale. This is diﬃcult to deﬁne, of course, and especially thorny
if tasks have been created beforehand without regard to claims or cutpoints.
It can then be diﬃcult for experts to know exactly where a claim should
fall on a scale without an experimental study. For early stages of the model
building process it is suﬃcient to assign claims to falling on high or low parts
of the scale. This is similar to the procedure of item mapping (Beaton and
Allen 1992). Item maps place items along the scale at a point where 50 % (or
some other chosen fraction) of the students get the item correct. The software
package ConceptMap (Kennedy et al. 2006) does this visually, producing a
graph with students on one side and items on the other. Items, however, are
not pure representations of claims. They are only one possible realization of a
task for which the claimed skill is required and their diﬃculty may fall higher
or lower on the scale than the claim. Wilson (2004) tackles the problem from
the opposite direction, more in line with the ECD approach advocated here:
having in mind a theory of the construct and constructing tasks that are
intentionally instances of targeted performance in targeted situations.

428
12 The Conceptual Assessment Framework
When the proﬁciency model contains more than one proﬁciency variable,
some claims may require more than one proﬁciency to be satisﬁed. For exam-
ple, consider the claim that a student can solve a mathematical word problem.
If Reading and Mathematics are represented by two separate variables, then
a certain level of both would be required before the claim is met. Care must
be taken in the case where most of the claims deﬁning a particular proﬁciency
level require multiple proﬁciency variables. The Biomass example in Chaps. 14
and 15 and West et al. (2012) provide discussion and examples on this point.
In an assessment with such tasks, it helps to have some tasks with just one
or perhaps two parents to help deﬁne the scales.
12.2.2 Relationships Among Proﬁciency Variables
The proﬁciency model deﬁnes the set of possible proﬁciency proﬁles that an
examinee could have. To make the proﬁciency model Bayesian, we must deﬁne
a probability distribution over the set of possible proﬁciency proﬁles. This
distribution should be based on the target population of test takers; that
is, if member of the population is selected at random, the proﬁciency model
should provide the probability that the student has a given proﬁciency proﬁle.
(Chapter 13 discusses the student-speciﬁc version of the proﬁciency model
used in scoring.)
When the proﬁciency model contains multiple variables, an important part
of specifying the proﬁciency model is establishing the dependence structure
among the variables; that is, determining the graphical structure. The key
is the pattern of conditional independence relationships among the variables.
This can be diﬃcult to do when working with domain experts who are unfa-
miliar with graphical modeling, and may want to produce hierarchical content-
based breakdowns of the domain rather than graphical models.
There are a number of reasons why we might draw an edge between
two proﬁciency variables. In some cases they represent a part-of relation-
ship, where one variable represents a subskill of another. Another important
case is the prerequisite relationship, where a certain level of one skill must be
acquired before the second one can be acquired. Or skills can just be corre-
lated for a number of reasons which are not necessarily causal. In particular,
if two skills are always taught at the same time in the curriculum, they might
be correlated as they are both a function of students’ progress through the
course.
Example 12.3 (Language Placement Test Proﬁciency Model). After
some discussion the design committee comes up with the following list of
potential proﬁciency variables: Communication, Reading, Writing, Speaking,
Listening, Grammar, Correspondence, Conversation, Sociolinguistic, Purpose
(of the communication), Register (linguistic patterns appropriate to the com-
munication). (See Mislevy et al. (2002b) for a more detailed discussion of an
ECD model for communicative competence). The committee agrees that the

12.2 The Student Proﬁciency Model
429
ﬁrst ﬁve variables are important for reporting. Others are interested in the
remaining variables as a better reﬂection of their theories of communicative
competence.
Fig. 12.3 Partial language testing proﬁciency model showing a part-of relationship
Reprinted from Almond et al. (2006a) with permission from ETS.
The ﬁrst and most obvious relationship is the relationship between the
overall Communication variable and the four variables representing the modal
skills, Reading, Writing, Speaking, and Listening. This is a part-of relation-
ship, something that is commonly encountered in many knowledge engineer-
ing problems. Because of this relationship the variables will be dependent,
and there should be an edge between them. Generally with part-of relations,
the edge should be oriented from the larger concept to the smaller ones. This
produces the graph shown in Fig. 12.3.
Structures like those shown in Fig. 12.3 occur often enough in practice that
it is worth examining some of their properties in more detail. Suppose that all
of the evidence models in the assessment use one or more of the four modal
skills as their parents, and that there are no tasks that provide direct evidence
for Communication. In this case, the variable Communication is identiﬁed
only indirectly through the prior weights we place on the edges in Fig. 12.3.
Depending on the strength of that prior distribution, there can be a ridge
or multiple modes of the posterior distribution, which may cause a Markov
chain Monte Carlo (MCMC) algorithm to mix poorly (Almond et al. 2008).
Generally it is necessary to ﬁx one or more of the distributions, either the
marginal distribution of Communication (analogous to setting the population
distribution in an item response theory (IRT) model to standard normal) or
the conditional distribution of one of the modal skills given Communication
(analogous to setting the loading of one variable to 1 on each factor in a factor
analysis).
Example 12.4 (Language Placement Test Proﬁciency Model, Con-
tinued). The design committee decides that the single communication vari-
able is not enough to explain all of the correlation among the four modal
variables. In particular, they note that there is usually a prerequisite rela-
tionship between Reading and Writing and between Listening and Speaking,
because at least some measure of the receptive skill is usually required for mas-
tering the corresponding productive skill. In the case of prerequisite skills, it

430
12 The Conceptual Assessment Framework
is again usually best to orient the arrows from the skill that is acquired ﬁrst
to the skill that is acquired last.
Fig. 12.4 Proﬁciency model for language testing using only the four modal skills
Reprinted from Almond et al. (2006a) with permission from ETS.
The design committee also feels that there may be additional correla-
tion between Reading and Listening (both receptive skills) and Speaking and
Writing (both productive skills) beyond those explained by the general Com-
munication variable. Therefore, they decide to place additional edges between
Reading and Listening and Speaking and Writing. Care must be taken here, as
this edge states that the skills are dependent even after conditioning on Com-
munication (which presumably incorporates common factors such as vocab-
ulary and grammar). Since these edges represent correlations, they can be
oriented in either direction. As the intended population mostly contains stu-
dents for whom English is a second language, and these academic second lan-
guage learners usually spend more time with the written language, we orient
the edges from Reading to Listening and Writing to Speaking. The resulting
graph is shown in Fig. 12.4.
There were several places in the construction of Fig. 12.4 that the design
committee had to make arbitrary decisions about which direction to orient the
edges. Although some authors put great store in using Bayesian networks to
represent causal relationships, the functional meaning of the direction of the
arrows is mathematical. By orienting the edge from Reading to Listening the
committee is stating that they would rather specify the distribution of Lis-
tening conditioned on Reading than the distribution of Reading conditioned
on Listening. Orienting the edges in the causal direction is usually preferred
because (a) it usually yields Bayesian networks with smaller treewidth and
(b) it is usually easier for subject matter experts to provide prior probability
distributions for edges oriented in direction that corresponds to their opinions
on causality. Whether or not the arrows point in a causal direction does not
matter mathematically in using a Bayesian network; the concern is whether
it expresses an appropriate joint probability distribution over the proﬁciency
variables.
Example 12.5 (Language Placement Test Alternative Proﬁciency
Model). A group of experts on the design committee feels the proﬁciency
model that only contains the four modal skills does not really match their the-
ory of language use. They propose four new proﬁciency variables: Grammar,

12.2 The Student Proﬁciency Model
431
Correspondence competence, Conversation competence, and Sociolinguistic
competence. Grammar explains much of the observed dependence among the
four modal skills, and Correspondence and Conversation competence explain
the extra correlation among Reading and Writing and between Speaking and
Listening. Sociolinguistic competence represents capabilities that are not well
represented in the modal proﬁciency model. Figure 12.5 shows the graphical
representation of their proﬁciency model.
Fig. 12.5 Proﬁciency model for language with additional communicative compe-
tence skills
Reprinted from Almond et al. (2006a) with permission from ETS.
There is seldom a single view of competence in a domain. Research and
experience might support alternative views, and it is generally not hard to
come up with more proﬁciencies and alternative structures than can possibly
be expressed in one model. The point is not to build a comprehensive psycho-
logical model of proﬁciency in the domain. Rather, it is to build a model that
draws on research but is simply suﬃcient for the purpose of the assessment.
Some aspects of competence may not be involved, others may be moot because
of the testing population, and for others, ﬁne distinctions and alternative the-
ories are not necessary for the job at hand. If the design committee thought
that reporting about Grammar, Correspondence, Conversation, and Sociolin-
guistic competence was important (even if only for diagnostic purposes or as
part of aggregate reporting), then the model of Fig. 12.5 has a clear advan-
tage over that of Fig. 12.4. If reporting on those skills is not important, then
either model should work adequately and the simpler model should be easier
to build and maintain.
Another consideration is how much evidence is required to distinguish
between diﬀerent proﬁciency proﬁles. This cannot be calculated exactly until
the evidence models and assembly models are built. A heuristic often used
at this stage is to consider that between six and ten independent pieces of
evidence are required to deﬁne a scale with modest reliability. Thus, the model
of Fig. 12.4 would require a minimum of a 40-item test to get reliable estimates
for all of the proﬁciency variables, while the model of Fig. 12.5 would need
an 80-item test. If testing time is an important consideration, the model with
fewer proﬁciency variables is clearly favored. This heuristic is only a rough
guide for the initial stages of discussion. In most testing situations some kind of

432
12 The Conceptual Assessment Framework
pretesting will be required to ensure that the test provides adequate evidence
for the desired uses.
Still another consideration is whether the proposed kinds of tasks provide
the evidence needed to distinguish among the proﬁciency proﬁles. This should
have been addressed earlier in working through claims and evidence. However,
this is another place to recheck this central issue. As an example, consider the
Sociolinguistic skill in Fig. 12.5. If all of the tasks tap only the four modal
skills, Reading, Writing, Speaking, and Listening, then the assessment will pro-
vide no direct evidence for Sociolinguistic competence. The design committee
would need to revisit the claims they intended to address concerning sociolin-
guistic competence, and the evidence they indicated they needed (or failed
to indicate they needed). The assessment would require evidence addressing
sociolinguistic skills, which might require extending the existing evidence and
task models, or constructing new ones; for example, in which sociolinguis-
tic demands were varied in principled ways and the Sociolinguistic skill as
well as one or more modality skills were parents of observables evidencing
sociolinguistic skills.
Example 12.6 (Language Placement Test, Continued: Unused Pro-
ﬁciency Variables). In the initial draft of this problem, the design team
identiﬁed two potential proﬁciency variables, Purpose and Register, that were
not included in either Fig. 12.4 or 12.5. Although these concepts appear in
the complete list of claims, the team felt that it was not important to report
on them. Therefore, they do not appear in the proﬁciency model.
It was important, however, that the tasks have a representative sample
from the various purposes and registers. Therefore, rather than motivating
proﬁciency variables to report on, these concepts motivate the deﬁnition of
task model variables that will be used to guide task construction and test
assembly, and thus help implicitly deﬁne the scope and meaning of the vari-
ables that are in the proﬁciency model. First, each task needs to be tagged
with two task model variables that indicate the Purpose of the communication
in the task (e.g., provide information or make request) and the Register of
the task (e.g., informal [class discussion] or formal [lecture]). Sec-
ond, assembly rules need to be added to the assembly model that specify the
target distribution of values for these task model variables in the ﬁnal form
(Sect. 7.4).
Remember that the purpose of drawing the graph is, as a step toward
creating a joint probability distribution of the proﬁciency variables, to serve as
an appropriate prior distribution for scoring members of the target population.
After the graphical structure of the model is agreed upon, values need to
be chosen for parameters of the conditional probability tables. Section 15.1
describes one possible procedure for this later quantiﬁcation step.
It is also important to keep in mind that the role of edges in the graph
is to represent patterns of conditional independence in the joint distribution
of the proﬁciency variables. The substantive considerations discussed above

12.2 The Student Proﬁciency Model
433
that suggest dependencies due to part-of and prerequisite relationships are
important. However, there if there is prior research in the ﬁeld using these
variables, there may also be correlation matrixes resulting from factor ana-
lytic studies or structural equation models. A useful trick is to look at the
inverse of the correlation matrix. Zeroes represent instances where variables
are conditionally independent given the other variables in the model (Demp-
ster 1972; Whittaker 1990). Almond (2010a) shows how this fact can be used
to derive the graphical structure of a proﬁciency model.
12.2.3 Reporting Rules
While the role of the proﬁciency model is to describe the distribution of pro-
ﬁciency proﬁles within the target population, the role of a score is to provide
information about which proﬁciency proﬁle (or implications of proﬁciency pro-
ﬁles for performance) a particular student (or group of students) has. In the
case of a Bayesian scoring model, this will come from a probability distri-
bution over possible proﬁciency proﬁles. The proﬁciency model serves as a
common prior distribution for all students taking the exam. When we observe
responses Xi from a particular student, we can calculate P(θi|Xi), a student-
speciﬁc posterior distribution over the space of proﬁles, using the methods of
Chap. 5 when the proﬁciency model is a Bayes net. This posterior distribu-
tion is called a scoring model in Chap. 13. It represents our state of knowledge
about that student.
Reporting the entire posterior distribution is usually not practical, either
when the users are humans or computer processes. When the score users are
humans, it is diﬃcult to represent and many will not know to interpret it.
Moreover, it does not directly provide answers to questions users of either
kind care about. Instead, we apply reporting rules that map states of the
scoring model into scores that are shown on a score report when the users
are humans, or a value upon which decisions are based when the user is a
computer process. In the case of Bayesian scoring models, these are statistics
of the posterior distribution. Formally, a statistic is a functional (an operator
that maps a function, in this case the distribution function, into a scalar value)
of a probability distribution. Many familiar statistics are useful for developing
reporting rules.
Sections 12.1.2 and 12.2.1 discussed ways in which proﬁciency variables are
related to claims. Reporting rules can thus use information in posterior dis-
tributions over proﬁciencies to provide quantitative evidence for given claims.
For instance, in the examples below Most Likely Value and Probability of State
are suitable when claims correspond to levels of a proﬁciency variable. Most
Likely Explanation is useful when a claim is expressed by a set of proﬁles over
proﬁciencies. Expected Score on Market Basket gives an indication of a claim
in terms of expected performance across a representative tasks that implicitly
deﬁne a claim, whether one or several proﬁciencies are involved.

434
12 The Conceptual Assessment Framework
Many statistics only involve a single target proﬁciency variable. In that
case, they can be calculated from the marginal distribution of that variable.
For Bayesian network models, the most commonly used statistics are:
Most Likely Value. The value of the target variable that has the highest pos-
terior probability. This is also known as the mode or maximum a posteriori
(MAP) score.
Probability of State. It is easy to calculate the probability that the target
variable takes on one of its possible states.
Probability of Reaching Cut Score. It is also easy to calculate the probability
of any sets of states of the target variable. When the states of the target
variable are ordered, one state can be chosen as a cut score, and the
statistic used in the probability that the target variable is at or above the
cut score is reported.
Marginal Distribution. For a discrete Bayesian network, it is possible to report
the posterior distribution for the target variable as a vector of numbers.
This is easy to express graphically using a divided bar chart.
Expected Value. If the levels of the target variable are assigned numeric values,
then the expected value of the target variable can be calculated. This is
also known as the mean or expected a posteriori (EAP) score. Although
the most obvious assignment of consecutive integer values requires the
sometimes questionable assumption that the proﬁciency levels are equally
spaced, in some cases the mean provides a single number summary. The
discrete IRT model in Sect. 6.1 is such a case. Indeed, computer programs
for continuous IRT models often work with just such an approximation
“under the hood” (Bock and Aitkin 1981).
Standard Deviation. Again, if the values of the target variable are mapped to
numeric values, the standard deviation is easy to compute and provides
a simple description of the posterior uncertainty about the correct score
level (Bock and Mislevy 1982).
If instead of a single target proﬁciency variable there are multiple target
proﬁciencies, then there are a number of additional statistics of their joint
distribution that are useful. For example, the Activity selection process in a
coached practice system might check the joint distribution of Space-Splitting
and Canopy-System, so that if the former is high and the latter is low, it can
trigger a review of the canopy system.
Most Likely Explanation. This is the assignment of values to the proﬁle that
has the highest posterior probability. This is not always the same as the
mode of each variable taken individually. Section 5.6.1 describes the spe-
cial algorithm used to calculate this.
Probably of Hypothesis. We can also calculate the expected value of any
hypothesis, that is set of possible proﬁciency proﬁles.
Expected Value of Function. The expected value of any function of the target
variables is also straightforward to calculate.

12.2 The Student Proﬁciency Model
435
For Bayesian networks these values are easy to calculate when all of the target
variables fall into a single clique. If that is not the case, often the desired
statistics can be calculated by successively conditioning on the target variables
in turn.
Deﬁning a canonical collection of tasks called a market basket (DeVito et al.
2000) allows a number of statistics that can be used to communicate results as
expected performance on tasks chosen to exemplify claims. Let m ∈M be a
task from the market basket and let Ym be the observables from that task. The
evidence model for Task m provides the conditional distribution P(Ym|θ),
and the scoring model provides P(θi|Xi). Assuming that the operational task
observables, X, and the market basket observables, Y, have been calibrated
to the same scale using the methods of Part II, we can calculate a predictive
distribution for the market basket observables:
P(Ym|Xi) =

P(Ym|θ)P(θ|Xi)∂θ.
(12.1)
This can be used in a variety of interesting ways:
Expected Score on Task m. We can make a prediction about how likely the
student is to obtain various outcomes for Task m. This can be useful if the
operational tasks are not published for security reasons. Prediction of per-
formance on the disclosed tasks provides a proxy for actual performance
on the operational tasks.
Expected Score on Market Basket. Summing the expected scores across all
tasks in the market basket creates an expected score for that form, no
matter what operational form an examinee may have taken. As the mar-
ket basket form is disclosed, this score will have a concrete meaning for
the score users.
Expected Weight of Evidence on Task m. The predicted score on the market
basket task provides the required information for calculating an expected
weight of evidence that the market basket task provides for a hypothesis
of interest (Sect. 7.3.2). Tasks that have high expected weight of evidence
are interesting because they are on the cusp of what we think that the
student should be able to do.
The list of reporting rules presented above is by no means exhaustive, and
concentrates mainly on reporting rules that are useful when the proﬁciency
model uses Bayesian networks. So how should the design committee select the
right reporting rules to use with a given assessment?
The answer lies in returning to purpose of the assessment, which is to
establish whether or not the claims hold for a given student. Therefore, the
scores should all be related back to the claims, e.g., such-and-such claim usu-
ally holds when the score is above a certain level. The score report should be
designed in such a way that it is apparent to the score user what evidence
is provided by the assessment results for the claims in the ways discussed
previously.

436
12 The Conceptual Assessment Framework
When designing the score report, it is important that it be understand-
able not only for the design committee but also for the score users who are
its intended audience. It is often helpful to build a prototype score report, a
mockup of the ﬁnal score report using artiﬁcial but realistic numbers, and
show it to some potential score users. An important use here is to ensure that
the claims the design team chose to make the focus of the assessment are in
fact the ones the user community values. Adjusting the focus of the assess-
ment is much less expensive at this stage before production is underway, than
later when considerable eﬀort has been expended designing and testing tasks.
Another important use for the prototype score report is as a vehicle for
explaining the psychometric consequences of a design decision. The under-
standing of complex psychometric concepts like reliability is often rudimentary
in the score user population. If the design team is trying to decide between
two formats for the assessment, a longer one with higher reliability and a
shorter one with lower reliability, the best way to get feedback from potential
score users is to mock up score reports that illustrate the consequences of the
design choice. This frames the question in terms of how they will eventually
use the information from the assessment.
Example 12.7 (Language Placement Test, Continued: Prototype Score
Report). To decide between the modal model and the more complicated
communicative competence model, the design team builds two prototype score
reports. The central display for the modal model is a stacked bar chart, shown
in Fig. 12.6.
Writing
Reading
Listening
Speaking
30
45
25
20
40
40
30
35
35
55
25
20
Language Placement Scores for Anne Alias
Overall Score = 120
Fig. 12.6 Prototype score report for Language Placement Test. Reprinted with
permission from ETS.
A small simulation study convinces them that the reliability of scores based
on the Communication node is too low to report as an overall score (See

12.2 The Student Proﬁciency Model
437
Exercise 12.10). They decide to use instead the sum of the probabilities that
the student is in the highest (passing) state in each of the four modal skills.
They multiply this by 100 to get a score in the 0–400 range.
The stacked bar chart in Fig. 12.6 is a useful way to display the results from
Bayesian networks. Almond et al. (2009a) describe some considerations in the
design. First, note that the probabilities are scaled to percentages, as teachers
are more used to working with probabilities expressed as percentages than
with raw probabilities or fractions. Note also that the colors for the various
levels of the proﬁciency variables diﬀer only in intensity (value, in the graphic
designers’ {hue, saturation, value} model). There are two reasons for this:
ﬁrst, it means the report is understandable even for individuals with limited
color perception, and second, it means that the report is understandable even
if it printed or copied using a printer that can only render grayscale images.
Almond et al. (2009a) also describes some variations on this idea that can be
used for reporting at the classroom (or other student group) level.
Another important question in score report design is whether or not the
way the information is presented is understandable for the target audience.
To answer this question, the prototype score report could be used as part of
a formal usability study. In this kind of study, representatives of the target
score users are asked to perform tasks using the prototype score reports (such
as comparing two hypothetical examinees). The design team can then get
feedback both about how the target users reacted to the report and how
successful they were at performing the requested tasks.
Implicit in the deﬁnition of these reporting rules is the need to deﬁne a
“mastery” level for each of four modal proﬁciencies, Reading, Writing, Speak-
ing, and Listening. The speciﬁcation of the third level as the origin line in
Fig. 12.6 implies that the third level of the variables are associated with mas-
tery. Thus, it is reasonable to ask if the claims associated with the highest
level of the proﬁciency variable correspond to mastery, or in this example, the
ability to get value out of college courses without supplemental instruction in
English. No matter how much eﬀort goes into this design, the way students
actually interact with the assessment is sure to bring surprises. If the assess-
ment has high stakes then it is worth doing a formal validity study using the
implicit mastery designations. Even for a more moderate-stakes examination
it is worth reviewing the implicit standards after actual student performance
data from pilot test are available.
In review, thus far we have discussed the initial speciﬁcation of the assess-
ment goals and population and their implications for development of the
assessment claims and supporting proﬁciency model for the domain. The
overall purpose of assessment and target population are used to derive the
identiﬁcation of claims to be made from the assessment (represented by stars
in Fig. 12.1), which in turn helps to specify the number and nature of stu-
dent model variables (represented by green circles), which themselves have
conditional dependencies and interdependencies (represented by arrows) in

438
12 The Conceptual Assessment Framework
acquisition and implementation—all of which are used to develop and inform
the reporting rules that will link the student model to the characteristics that
appear on the score reports.
Of course, to produce score reports for individual students requires evi-
dence from individual students. Section 12.4 looks at how that evidence is
represented in the formal assessment design. However, before discussing evi-
dence, it is worth talking about the circumstances under which it will be
gathered. This is the province of task models.
12.3 Task Models
A task model is a set of detailed descriptions of task characteristics for a
family of similar tasks. The essential form appears in Hively et al. (1968) and
Osburn (1968), and Gierl and Haladyna (2012) present more recent work that
takes advantage of developments in digital technology. This discussion draws
on Almond et al. (2002a) and Mislevy et al. (2003b)
A task model provides a complete framework for the design and devel-
opment of a family of assessment tasks, as well as information used by the
assembly model in constructing test forms and by the evidence model is cal-
culating the evidentiary strength of possible observations. The fundamental
elements of a task model include the nature of the material presented to the
student (such as directions, initial prompts, response options, etc.), the char-
acteristics of activities that the examinee must undertake to complete the task,
and the nature of recorded information as a result of the examinee working
on the task (response, time to complete the task, etc.).
At an abstract level, a task presents a collection of material to the
examinee—the presentation material—and captures some kind of response—
the work product. For a family of tasks to be similar, there must be some
similarity in both the presentation material and the work products. Thus, a
primary goal of the task model is to provide a description of the range of and
format of the allowable presentation material.
Presentation material is the set of all representations and materials that
are displayed, or may be displayed, to an examinee during the delivery of an
assessment task. These can include instructions for the task (e.g., “. . . choose
the best answer. . . ”), the initial prompt from which the examinee begins
work (e.g., “A train leaves the station heading east at. . . ”), the various ways
the examinee can interact with the task (in familiar simple cases the various
response options available for the examinee to choose or in other ways indi-
cating areas for them to respond in), as well as any multimedia presentation
components such as video or audio clips, graphics, animations, etc. These pre-
sentation materials are represented in Fig. 12.1 as the papers, grid, and video
clips that appear in the upper right portion of the task model.
The work product speciﬁcations describe the elements of examinee per-
formance that are recorded and retained as a result of interacting with the

12.3 Task Models
439
assessment task. They deﬁne the important aspects of examinee performance,
both in process and outcomes, that are used for scoring and for other impor-
tant data functions in assessment. A computer-based hydraulics system trou-
bleshooting task, for example, can present work products in the form of the
ﬁnal conﬁguration of the system and a ﬁle with all the troubleshooting actions
and time stamps. Realized work products will be evaluated in terms of the
speciﬁcations in the evidence model, to identify the relevant evidence from the
performance evoked by the task. The work product is represented graphically
in Fig. 12.1 as the jumble of shapes in the upper left hand section of the task
model.
In a fully detailed task model, the speciﬁcations for both presentation
material and work products include semantic and functional descriptions of
the forms they will take, but do not specify the implementation details. This
is so that test developers can focus on the elements of tasks and performance
that embody the elements of the argument, but can be rendered in forms that
may vary over times and places in diﬀerent formats or presentation platforms,
or adapted to students with disabilities in ways they can better access stim-
ulus materials or produce their responses (Hansen and Mislevy 2005). The
Presentation Model discussed shortly contains the information that program-
mers will working on the project will know how to store the material, and
what kind of software and hardware will be required to display it.
One way to develop a task model starts earlier in Domain Modeling: Build
some prototype tasks that designers agree generate evidence for the targeted
claims, then look at what properties of the examples are generalizable. This
produces evidence paradigms and task paradigms, which can then be reﬁned
to produce evidence and task models. The key is using the prototypes to
elicit discussion about the nature of evidence being sought; that is, how the
intuitions behind the prototypes illuminate classes of situations with certain
key features, asking examinees to carry out certain kinds of work, and looking
for certain key features in their performances. To that end, we consider one
possible type of task that might be used in the Language Placement Test
example.
Example 12.8 (Language Placement Test, Continued: Lecture Clar-
iﬁcation Task). Looking through the claims associated with the assessment,
the design committee decides that one of the more important claims is that a
student who places out of the remedial language program is capable of asking
a question to clarify a particular ambiguous concept introduced in a lecture.
To that end the committee looks at a task in which the student is presented
with a short video clip of a lecture and is then asked to produce a question
requesting clariﬁcation about a point raised in the lecture. This task model
will have two pieces of presentation material: the lecture excerpt, stored as
a video ﬁle, and the written instructions to the student, stored as text with
markup.

440
12 The Conceptual Assessment Framework
Often it is important to know if a task has a particular feature. In the
lecture task, the length of the video clip, the topic, and the ambiguous point
are salient. Task model variables label tasks according to speciﬁed features.
Various processes that manipulate tasks can use task model variables to select
tasks with desired properties, as distinguished by possible values of those
variables. For example, task model variables can be used in automated test
assembly and adaptive item selection (Sect. 7.4).
Part of the deﬁnition of a task model is a list of task model variables that
are associated with tasks generated from the task model. That task model
deﬁnes the range of acceptable values for the task model variables, and it
deﬁnes speciﬁcation rules which describe how the value of the task model
variable relates to the presentation material or the potential work product. In
some cases, the task models can be suﬃcient to allow automated creation of
tasks (Gierl and Haladyna 2012).
Speciﬁcation rules can work either forward or backward. Consider the lec-
ture task and a task model variable that encodes the speech rate of the lec-
turer. To use the rule in the forwards direction, start with an existing lecture
video clip and measure the speech rate. To use the rule in the backward direc-
tion, there are two ways. Suppose a task has been requested with a slow
speech rate. One way to use the rule would be to search through a library of
video clips to ﬁnd one that meets the criteria. Another would be to videotape
a new lecture, and to instruct the lecturer to speak slowly. The backward
use of speciﬁcation rules is useful in automatic item generation; the generator
can select a random or prescribed value for the variable and then generate
presentation material that matches. Speciﬁcation rules can be quite useful
as well when the item generators are test developers. For complex or unique
tasks that require human creativity, the rules function as guidelines and con-
straints, around which unique tasks will be constructed. Mislevy et al. (2002c)
discuss roles that task model variables can play. These roles are not mutually
exclusive, as variables are often used for more than one purpose. The roles do
provide rationale for when and why task model variables should be created.
1. Task Construction. A fundamental tenet of evidence-centered assessment
design is that tasks must be built, selected, or recognized to provide the evi-
dence required to support the claims of the assessment. A key role of task
model variables is to tell test developers which features can be manipulated
to meet those goals. This includes both direct manipulation, where the mate-
rial is authored by the test developer, and indirect manipulation, where the
test developer seeks out material that meets the speciﬁcations.
Irvine et al. (1990) introduce the term radical for features that change
the evidentiary properties of the task, and incidental for features that do
not. Their work focused on features that drove diﬃculty in accordance with
cognitive theory, and produced a new version of the British Army Recruitment
Battery (BARB) with all items generated in real time through task models
(Collis et al. 1995; Irvine 2013).

12.3 Task Models
441
Incidentals also play an important role in the construction of tasks. In
high-stakes testing situations, it is often the case that examinees will discuss
diﬃcult problems with each other. Being able to easily produce variants on
a task by manipulating surface features (e.g., the names of actors in a story)
means that examinees cannot select solution strategies based solely on the sur-
face features. Incidentals are also useful in automatic item generation, where
they indicate features that can be manipulated to produce variants of the task
without aﬀecting psychometric properties.
It can be diﬃcult to tell which variables are radical and which are inci-
dental without pretesting the tasks with members of the target population.
Experience has shown that many features that should not matter, in fact do
have an impact on the evidentiary properties. There may be no true inciden-
tals; what we call incidentals are simply variables that are not central to the
claims and for which the impact is much smaller than the construct-related
task model variables in the targeted population.
2. Focusing Evidence. Changing some task model variables will shift the nature
of the proﬁciencies a task evokes, thus altering the nature of the evidence that
the task provides. Consider the clarifying question of Example 12.8. Suppose
that we change the task so that it now asks for a summary statement rather
than a clarifying question. This changes the claims associated with the task,
from “can generate questions to clarify uncertainties” or “can generate sum-
maries of information.” Thus, the focus of the task has changed. These could
be considered two diﬀerent task models, each of which has a restricted range
for the work product: expected form variable.
Depending on what variables are in the proﬁciency model, this may or
may not change the graphical structure of the evidence model. In the exam-
ple above, if the proﬁciency model is the one shown in Fig. 12.4, then the
observables are likely connected to the same proﬁciency variables because the
two task models because both models are built to evoke evidence of the same
coarsely-grained proﬁciency variables, in this case Listening and Speaking. If
the proﬁciency model is ﬁner grained and includes distinct proﬁciency vari-
ables for listening skills concerning details and concerning gist, then changing
the value of this task model variable would indicate that a diﬀerent evidence
model fragment needs to be used, so that the appropriate proﬁciency variables
are updated.
3. Assessment Assembly. Often there are many more claims associated with
an assessment than there are proﬁciency variables. In such cases, the tasks
selected for a given form of the assessment are a purposive sample from all
of the tasks that could be administered. An important goal of the assembly
model is to ensure that the sample of tasks is representative of the claims to be
made about this assessment. This can be done formally in terms of constraints
on an item selection as discussed in Sect. 7.4.
Example 12.9 (Language Placement Test, Continued: Communica-
tion Purpose). Recall that in building the proﬁciency model, the committee

442
12 The Conceptual Assessment Framework
considered and then rejected the variable Purpose as they would not use it to
organize reporting. Despite this rejection, the purpose of the communication
is still important, as the claims for the Speaking variable span a number of
diﬀerent communication purposes.
To make sure these claims are represented on the test, they add a con-
straint to the assembly model that there must be at least one task from a
number of diﬀerent purposes. In order to enforce that constraint, each task
needs to be tagged with a Purpose variable that provides the purpose of the
communication. For example, the task described in Example 12.8 would have
Purpose = clarify. The alternative in which the examinee was required to
summarize the lecture would have Purpose = summarize.
4. Controlling the Psychometric Properties of Tasks and Observables. Ideally,
test developers would like to be able to control the psychometric properties
of the items at design time. One critical role of task model variables is to
identify features that are thought to aﬀect those properties. Evidence models
can pick up the values of those task model variables and model the parameters
in the statistical part of the model as a function of them (e.g., Geerlings et
al. 2011; Fischer 1973; Mislevy et al. 1993; Rijmen et al. 2002). Embretson
(1998) illustrates how a cognitive approach can unify assessment design, task
construction, and statistical modeling. This leads to the idea of predicting the
statistical parameters of computer-generated tasks without any pretesting at
all, as in BARB (Collis et al. 1995; Irvine 2013).
When the statistical part of the evidence model uses IRT, then Mislevy
et al. (1993) show that the the diﬃculty parameter is both the easiest to
model and the most important in subsequent inferences. This insight applies
to Bayesian network models using the IRT-like DiBello–Samejima models.
Even if these relationships are too noisy to use without additional pretesting,
knowledge of the factors that inﬂuence diﬃculty can help test developers build
a balanced pool of potential tasks to pretest, or allow them to reduce the size
of the required pretest sample.
5. Characterizing Proﬁciency. This use is the complement of the previous one.
After the evidence model for a task is built, then one can predict how people
at various levels of proﬁciency are likely to perform. For any given proﬁciency
proﬁle, look at the expected performance on a collection of tasks that a person
with that proﬁle are likely to produce (right answers in dichotomous tasks,
levels of performance in ordered-category tasks). To the extent that this col-
lection of tasks shares common values for task model variables, those values
characterize the proﬁciency proﬁle.
In the previous section, we mentioned that proﬁciency variables were char-
acterized using claims. In many cases the claims are grounded in abstracted
descriptions of tasks, which would have implications for the corresponding
task model variables. Consider the three claims: “can read a comic book,”
“can read a newspaper,” and “can read a journal article in their ﬁeld.” All of
these have diﬀerent implications for task model variables describing the text,

12.4 Evidence Models
443
e.g., the diﬃcult of the vocabulary, the formality of the grammar, and the
presence of pictures to supplement the text.
6. Recognizing Task Situations. In interactive tasks, such as the HYDRIVE
environment for troubleshooting the hydraulics systems of the F-15 aircraft
(Gitomer et al. 1995), “tasks” may be recognized as they arise rather than
constructed. While each HYDRIVE task was deﬁned globally by the initial
fault and stimulus materials such as the video clip of the pilot’s report of
a problem, Mislevy and Gitomer (1996) showed how a delivery system can
activate the use of evidence model fragments when certain conditions are
recognized in a less constrained stream of actions. The current state of a
problem is tracked in terms of salient features of the state and past actions—
values of dynamic task model variables—and certain conﬁgurations signal that
an instance of a task model has been realized; that is, an instance of an
evidence-bearing situation described by a task model has been recognized.
The preceding discussion shows that the considerations of task models
from a design perspective for the CAF have much in common with task mod-
eling in automatic task generation, in which computer software automatically
generates tasks according to speciﬁc needs of assessment (e.g., Gierl and Hal-
adyna 2012; Irvine and Kyllonen 2002). Indeed, the automatic item genera-
tion models are subsumed under the more general umbrella of task models in
assessment design, with the exception that much of the discussion of automatic
item generation eﬀorts expand the concept of task models to include both the
conceptual characteristics of these models and the technical requirements for
implementation in an automated item generation system.
12.4 Evidence Models
It may seem relatively simple to come up with interesting ideas for tasks
and task models. We have argued here, however, that it is not optimal to
try to work backward from tasks, to how to score performances, and then
ask whether they even provide the evidence needed to support claims. It
is better to start from claims and evidence, then begin to craft situations
and performances that provide that evidence in some particular form. Even
when work does begin with prototype tasks just because designers can most
easily leverage their expertise in this way, the prototypes can then eﬀectively
play a domain analysis role: As examples of valued work, they can motivate
discussion that brings out more explicit statements of claims and classes of
evidence. Either way, we begin from a coherent qualitative argument for later
interpreting students’ performances as evidence. Then we can address the
machinery that embodies the argument, which indicates the machinery and
the processes by which evidence is identiﬁed and accumulated. This is the
province of the evidence model.
The evidence model bridges a task model (and hence particular tasks that
accord with that task model) to the proﬁciency model. The connection is

444
12 The Conceptual Assessment Framework
easiest to express when, in operation, one evidence model will be hooked up
with each task model–proﬁciency model pairing. In other words, we can use
the same task model, but with diﬀerent proﬁciency models, and when we do
we can use a diﬀerent evidence model to bridge them.
This arrangement allows for reuse of tasks in diﬀerent contexts. To use
a task to obtain evidence for a ﬁne-grained proﬁciency model, a ﬁne-grained
evidence model is needed. To use the same task to provide evidence for a
coarse-grained proﬁciency model, an evidence model with the correspond-
ingly coarser grain size is needed. The diﬀerence in evidence models could be
as simple as changing the proﬁciency variables that are parent of the same
observable variables. But diﬀerent sets of evidence rules can be used in dif-
ferent evidence models for the same task model, to identify and characterize
diﬀerent aspects of performance from the same work products. The observ-
able identiﬁed from an essay could be an overall rating of its quality when the
task is used for a summative purpose, for example, while several aspects of
lexicon, grammar, and structure could be identiﬁed when it is used in a diag-
nostic assessment. Furthermore, ﬁner-grained evidence models can identify
more subtle relationships between proﬁciencies and performances. Diﬀerent
patterns of more-detailed proﬁciencies can be modeled as parents of diﬀerent
observables, whereas under a coarser-grained composite-proﬁciency model all
the observables have it alone as their parent.
The observable outcome variables are central support pillar of the evidence
model bridge. On one side, there are the rules of evidence which link the
observables to the work products from which they are derived (Sect. 12.4.1).
On the other side, there is a statistical model that links the observables to
the proﬁciency variables (Sect. 12.4.2); earlier chapters describe how to model
these with Bayes net fragments. In both cases, the number and nature of the
observables drives most of the rest of the design decisions in the evidence
model.
12.4.1 Rules of Evidence (for Evidence Identiﬁcation)
The term “rules of evidence” is adapted from jurisprudence, but the analogy
is not bad. In the courtroom, the rules of evidence refer to legal rulings that
relate to which observations and testimony will be shown to the jury as pos-
sible evidence. In the classroom, the rules of evidence refer to the procedures
used to determine which features of the work product will be considered as
observables and used to update the proﬁciency model.
With multiple-choice and other selected-response tasks, there is usually a
single-evidence rule, the key-matching rule. If the selected response (as iden-
tiﬁed in the work product) matches the key, the observable is assigned the
value correct or 1. If the selected response does not match, then the observ-
able is assigned the value incorrect or 0. Even this simple scoring rule hides
some complex design decisions. Consider what happens when the work prod-
uct is null (i.e., the student made no selection). In many assessments, the null

12.4 Evidence Models
445
work product is assigned the value incorrect; however, sometimes diﬀerent
scores are assigned for incorrect selections and so another value, say omitted
is needed for the observable. Null work products from tasks that an examinee
chooses to skip may be treated diﬀerently from ones that are not reached and
from those that are not presented (Mislevy 2015).
Under the simple key-matching evidence rule, we cannot assign a value to
the observable unless we know the value of the key. The key is the simplest
example of evidence rule data, or task-speciﬁc parameters of the evidence
rules. Other more complex examples might include task-speciﬁc details in a
scoring rubric, facts to look for in a short response answer, or a composite
expert concept map against which to compare each examinee’s. In all cases,
this is additional information that must be authored with the task if the task
is to be used in this particular evidence model.
A second commonly seen type of evidence rule is the scoring rubric used by
human raters when scoring a constructed response. The observable variable is
the rating. In the case of holistic scoring, there is a single observable for each
work product; in the case of analytic or trait scoring, there may be more than
one observable per work product. In either case, the rubric should provide
speciﬁc descriptions of the characteristics of the responses at each response
type, and sample work products with scores and rationales for those scores.
Bejar et al. (2006) describe some considerations for human scoring. Even
if the goal is eventually to use a computer program to assign values to observ-
ables, a good clean rubric for human scoring is often a good starting point.
Computer scoring generally uses one of two methods: procedural methods
or machine learning methods. In procedural scoring, the rubric becomes the
speciﬁcation for computer software (Braun et al. 2006). In the case of machine
learning algorithms, the parameters of the algorithm are learned from the a
sample of human-scored data (Deane 2006). Here the human scoring rubric
is necessary to build the corpus of human-scored examples used to train the
algorithm. Automated scoring of essays (e.g., Attali and Burstein 2006) is
probably the most familiar application of this approach. Gobert et al. (2012)
is an example tuned to discovering meaningful patterns in students’ science
investigations—i.e., automated “feature detectors” as evidence identiﬁcation
processes.
In any case, a corpus of examples annotated with the “correct” value3 for
the observable variables is a valuable resource. In the case of human scoring,
this corpus can be used both for training raters and for checking that raters
are performing consistently (Baldwin et al. 2008). In the case of procedural
algorithms, the corpus becomes a valuable set of test cases for ensuring the
algorithm works. In the case of machine learning, the corpus is central to both
training and testing. It is worth the eﬀort to capture a sample of authentic
3 Rather than “correct,” we should say “targeted value for training.” Statistical
models bootstrapped from experts’ judgments can be more accurate than the
experts themselves (Bowman 1963).

446
12 The Conceptual Assessment Framework
examinee responses for this corpus, as this will inform the design team about
the unexpected ways real examinees respond to the tasks.
The downside of human scoring is the expense and the time. Even dis-
counting the cost of performing the ratings, the logistics of getting the work
product to the raters often means a delay between the time the examinee
produces the work product and the time the examinee receives the scores
(although in many sporting competitions, such as gymnastics and diving, the
judges watch the performance and can provide a score immediately). Feed-
back from the task is usually given days to weeks after the assessment and the
examinee has forgotten much of the thinking that went into the work product.
Further, the information from the human scored responses cannot be used in
adaptive task selection.
The alternative is some kind of automated scoring (Williamson et al.
2006b, provides a survey). As mentioned above, automated scoring algorithms
can be divided into two categories: procedural algorithms, whose parameters
are determined when the task is created, and machine learning algorithms,
whose parameters are learned from a sample of scored student responses.
The key-matching algorithm used for selected response items provides a
simple example of a procedural scoring algorithm. Often, a cleverly chosen
work product can enable an apparently complex task to be scored procedu-
rally. For example, selecting a word from a paragraph is a task that seems
open-ended to the examinee, but that can be easily scored by the computer.
Chapter 14 contains additional examples. As a general approach, working with
the knowledge representations in a learning area is fruitful because (a) learning
to work with the representations is essential to developing competence in the
domain, (b) it is natural to present information to examinees and to require
creating or completing a domain representation as a work product, and (c) in
computer-based assessments the work product can be structured so it seems
very open-ended to the examinee yet it is straightforward to characterize its
key features (Scalise and Giﬀord 2006).
A wide variety of machine learning algorithms have been applied to the
task of assigning scores to complex work products. These range in complexity
from linear regression (Attali and Burstein 2006) to neural networks (Stevens
and Thadani 2007). Despite the diﬀerences in the structural form, the basic
principle is similar: the parameters of the algorithm are learned from a sample
of student responses.
ETS’s e-rater R⃝(Attali and Burstein 2006; Burstein et al. 2013) system
for scoring essays provides a good example. First, several natural language
processing tools are run on the student essay to determine values for multiple
features (11 in version 13.1, which was used operationally in 2013). Typical
features are counts of errors of various types (grammar, usage, mechanics, and
style), measures of the richness of the vocabulary, measures of the organiza-
tional structure, and measures of how similar the vocabulary is to high-scoring
essays. These features may be transformed before putting them into a regres-

12.4 Evidence Models
447
sion equation; for example, the error counts are normalized by dividing by the
document length, and then the square root is taken to reduce the skewness of
the measures.
The features are then put into a regression equation to determine the
score, with weights that best predict human scores from the feature values.
E-rater supports two kinds of regression models: prompt-speciﬁc and generic.
To build a prompt-speciﬁc model (here a prompt corresponds to a single task
in the ECD framework), the regression weights are determined from several
hundred human-scored essays. In the generic model, the regression weights are
determined from a sample of several hundred, sometimes thousands, of human-
scored essays taken from several prompts (diﬀerent tasks from the same task
model in ECD terms). The advantage of the generic model is that new weights
are not needed for new tasks from the same task model; the disadvantage is
that it cannot take advantage of several features, such as the task-speciﬁc
vocabulary, and does not account for prompt eﬀects in the ratings. Hybrid
approaches include intercepts that account for average diﬀerences in prompt
diﬃculty within a generic model.
This approach extends easily to the case where there are multiple observ-
ables. In this case, diﬀerent features of the work product, with diﬀerent
weights, produce diﬀerent observables. Deane and Quinlan (2010) shows some
preliminary factor analysis results in which diﬀerent e-rater features map onto
diﬀerent strands of writing skill as measured by human raters.
Note that the calculation of observables can occur in several steps. These
steps can involve the creation of intermediate observable variables; for exam-
ple, the feature variables in the e-rater. Some of these intermediate observables
can be used for feedback. ETS’s CriterionSM system uses the output from the
grammar checking system in e-rater to provide feedback to essay writers.
The evidence rules can also involve producing intermediate processed work
products necessary for later calculations. Consider a short segment of audio
captured as part of a speaking task. It might be helpful to put this through a
ﬁlter designed to eliminate background noise before further processing, either
human or computer. It also might be helpful to note the position and duration
of pauses in the recording. Similarly, it may be useful to correct spelling in an
essay before putting it through a vocabulary matching program.
Evidence rules can get quite complicated. Regardless of their form, it is
important that the evidence rules provide clear instructions about how the
values of the observable outcome variables depend on the observed work prod-
ucts. That means that the observables as fed into the statistical part of the
evidence model are well deﬁned.
“Well deﬁned” does not mean there must be a single value. Neural net-
works, for example, can provide weights for each possible score value. Typically
the highest is used as “the” score, but the entire vector of relative strengths
could be entered into a Bayes net as virtual evidence (Sect. 5.2.3). While some
performances will point strongly to one of the values, others with uneven mixes

448
12 The Conceptual Assessment Framework
of features are harder to rate, and the more equivocal information they bear
is properly reﬂected by a more spread out virtual evidence vector.
12.4.2 Statistical Models of Evidence (for Evidence Accumulation)
The rules of evidence span the distance between the work product and the
observable variables. The statistical part of the evidence model spans the
remaining distance between the observables and the proﬁciency model. This
part of the evidence model presents the rules for how to update the proﬁciency
variables given a particular pattern of observed outcomes from a task.
Evidence-centered assessment design is intended to be neutral to the math-
ematical form chosen for the proﬁciency and evidence models. However, the
language was chosen to be natural when these are represented in a Bayesian
framework. The proﬁciency model is then a set of proﬁciency proﬁles and a
probability distribution over possible values that represents our current state
of knowledge about the examinee’s proﬁciency. The evidence model provides
the conditional distribution for the observables given the proﬁciency variables.
A given pattern of evidence then induces a likelihood over the proﬁciency vari-
ables, through the conditional probability distributions. The two are combined
through Bayes’ Theorem to give a posterior distribution over the proﬁciency
variables.
Two important special cases are Bayesian networks and IRT, either uni-
dimensional or multidimensional. In all cases the proﬁciency variables (some-
times called proﬁciency parameters) can initially be given a population dis-
tribution that serves as a prior, or if desired a diﬀuse prior. With Bayes nets,
the proﬁciency variables are discrete, and the statistical part of the evidence
model is represented with a Bayesian network fragment. With IRT, the proﬁ-
ciency variables are continuous and the evidence model is represented by item
response functions. Once the posterior is obtained, it can be used directly
or various reporting rules can be applied. Most of the statistics described in
Sect. 12.2.3 for Bayes nets have versions that are appropriate to IRT and
MIRT models.
The task of building the statistical part of the evidence model can be
divided into two steps: specifying which variables are involved and how they
are related to each other, and specifying the parameters of the relationships.
When we are representing the statistical part with a Bayesian network, these
steps become specifying the graphical structure and specifying the conditional
probability tables.
Note that the statistical part of the evidence model involves three kinds of
variables: proﬁciency variables, observable outcome variables, and other vari-
ables local to the evidence model (in particular, unobserved variables intro-
duced to model local dependence among the observables, like the context
variables introduced in Sect. 6.2). The proﬁciency variables appearing in the
evidence model are references to the corresponding variable proﬁciency model;

12.4 Evidence Models
449
their deﬁnitions are identical to those in the proﬁciency model. In the case of
a Bayesian model, their marginal distribution is provided by the proﬁciency
model.
An evidence model references a subset of the proﬁciency variables (possibly
all of them, as necessarily happens when there is only one proﬁciency variable
in the proﬁciency model). The referenced proﬁciency variables constitute the
boundary of the proﬁciency model/evidence model relationship. In the case
of Bayesian network model, the updating algorithms require that the bound-
ary variables appear within a clique in the proﬁciency model (Sect. 5.4.1).
Therefore, there are signiﬁcant computational implications for the number of
proﬁciency variables that appear as boundary variables. The necessary mor-
alization of boundary variables imposes additional computational burden as
the number of these boundary variables increases.
There are two key local independence assumptions associated with the
boundary variables. The ﬁrst is that the observable outcome variables are
independent of the other proﬁciency variables given the boundary variables.
In other words, evidence from a given task is always propagated through the
boundary variables of its evidence models; the evidence it provides for any
other proﬁciency variable is always indirect. The second is that the observ-
ables from two diﬀerent evidence models are independent given the boundary
variables from either of the evidence models. In other words, once the evidence
from the observables has been absorbed into the proﬁciency model, the observ-
ables can be discarded and not consulted again. Although these assumptions
are stated in terms of Bayesian network models, most other representations
use some variation of these assumptions.
The local independence assumptions for Bayesian network models are
weaker than those used for typical IRT models. In particular, when a task has
multiple observables, the Bayesian network can constructed so as to model
the dependence among the observables Almond et al. (2006b), while the most
widely used IRT models assume that the observables are locally independent.
An additional context or testlet variable can be introduced in either Bayes nets
(Sect. 6.2) or IRT (Wainer et al. 2007) to model local dependence between
items from a set (i.e., observables from the same task), although other models
are also possible (Almond et al. 2006b; Wilson and Adams 1995).
There are two useful notational forms to describe the relationship among
the variables in the statistical part of the evidence model. There is the graph-
ical notation that has been used extensively throughout this book (in par-
ticular Chap. 4). A widely used alternative is the the Q-matrix, a matrix in
which the columns correspond to proﬁciency variables, the rows correspond
to observables, and the entries are positive when the given proﬁciency vari-
able has a direct inﬂuence on the corresponding observable (Sect. 5.6). The
graphical notation has more ﬂexibility when there is dependency among the
observable variables that must be modeled. However, the Q-matrix notation is
more compact in the common case where each task yields a single observable
(requiring only one row per evidence model).

450
12 The Conceptual Assessment Framework
Example 12.10 (Evidence Model for Lecture Clariﬁcation Task). The
design team wants to build an evidence model for the Lecture Clariﬁcation
Task (Example 12.8) for use with the four modal skills proﬁciency model
(Fig. 12.4). After some discussion, the design team identiﬁes three observable
outcomes from the task:
•
Pronunciation: Was adequate pronunciation used in the work product?
•
CorrectForm: Was the work product in the form of a question or statement
as called for in the task directive?
•
OnTopic: Was the work product relevant to the point at issue?
Assume for the moment that the design team is able to build appropriate rules
of evidence for either human or automatic scoring of these three variables.
Fig. 12.7 Evidence model for lecture clariﬁcation task for use with modal proﬁciency
model
Reprinted from Almond et al. (2006a) with permission from ETS.
Figure 12.7 gives a graphical structure for this evidence model. OnTopic
depends on both Listening (to identify and understand the ambiguity in the
lecture), Speaking (to make the clariﬁcation request understood to the lis-
tener), and Reading (to understand the instructions). CorrectForm depends
only on Speaking (to be able to utter statements and questions) and Reading
(to understand the written directive for the task). Pronunciation depends only
on Speaking; the evidence rules call for oﬀ-topic but intelligible speech to be
given a high value.
Note that the proﬁciency variables (marked with circles) do not have par-
ents in this graph. Their distribution is given in the proﬁciency model, and
must not be repeated here (or it would be counted twice). Consequently,
Fig. 12.7 is a Bayes net fragment and not a complete network. The observable
variables (and any other local evidence model variables) marked with triangles
are unique to this model and must be deﬁned here.
The graphical structure chosen here is speciﬁc to both the task model in
one direction and the proﬁciency model in the other direction. If we were using
the alternative “communicative competence” proﬁciency model (Fig. 12.5),
then diﬀerent proﬁciency variables would be available, and the design team

12.4 Evidence Models
451
should consider a diﬀerent graphical structure (and maybe even diﬀerent
observables). This an important reason for separating the evidence models
and task models in ECD. The evidence model adapts the evidence from the
task for use with a given proﬁciency model, adjusting the grain size and focus
to be appropriate for the purposes supported by the proﬁciency model.
After the design team builds the structural part of the evidence model,
they choose a parameterization for the conditional probability relationships
that must be deﬁned. At this stage in the design, they usually stop short of
assigning values to the parameters. This is because those values may vary from
task to task. Just as tasks are instances of task models, there are instances of
the evidence model call links. A link is an evidence model with its parameter
values adjusted to a particular task. Chapter 13 describes links and their
relationships to tasks in more detail. Because examinees can respond to tasks
in unexpected ways, it is diﬃcult to assign ﬁnal values to the link parameters
without data from a pretest of the tasks.
Although exact values of the weights of evidence—the values of the link
parameters—are not assigned at the CAF development stage, a method must
be chosen to assign the parameter values. One simple method, appropriate for
low-stakes testing situations, is to simply assign the same values to the link
parameters for all of the links for tasks coming from the same task model.
This is similar in spirit to a teacher assigning point values to the items in
a quiz, and its psychometric properties are no worse. In this case, the link
parameters are assigned at the evidence-model level, and this is done at this
stage of the assessment design process (although it could be reviewed and
revised after seeing pretest data).
Part II discussed another method for assigning the link parameters:
Bayesian inference. Bayesian inference requires both pretest data and prior
distributions for all of the link parameters. These priors are usually deﬁned at
the level of the evidence model, and hence become part of the evidence model
construction process.
As noted, there can be patterns of local dependence among the observ-
ables. If there are multiple observables they may be dependent even given the
proﬁciency variable because they come from the same task, due for example
to familiarity with the topic or a misunderstanding of the task. Another com-
mon pattern is sequential dependence, where the observables represent steps
in a multistep task. Finally, there may be functional dependence among the
observables because the come from the same work product. For example, a null
or unintelligible response would produce low values for all three observables
on the Lecture Clariﬁcation Task (Example 12.10).
There are a number of possible approaches to modeling the local depen-
dence of the observables in the model:
•
Ignore it and hope the approximation error from ignoring it not strong.
Often the task model and rules of evidence can be designed to minimize
the dependence among observables.

452
12 The Conceptual Assessment Framework
•
Combine the observables into a superobservable by adding another rule of
evidence to combine the dependent observable (Wainer and Kiely 1987).
For example, if the task consisted of a reading passage followed by a num-
ber of questions asking about the passage, the superobservable might be
the sum of the individual question scores.
•
Add a local TopicFamiliarity or Context variable to soak up the depen-
dence (Wainer et al. 2007, Almond et al. 2006b).
•
Order the observables, and then make each observable a parent of the next
one in the series (Almond, Mulder, et al. 2006b call this the cascading
pattern).
•
Build a custom evidence model that captures the detail.
Although the diﬀerent models suggest diﬀerent mechanisms producing the
local dependence, the goal of the modeling is the same in all cases: to produce a
likelihood for the proﬁciency variable(s) given a pattern of evidence indicated
by the observed variables. If the model parameters are learned from pretest
data, it is likely that all of the models will produce roughly the same likeli-
hood for a given pattern of evidence (Almond et al. 2006b). However, adding
and removing observables from the evidence model can cause the learned
parameter values to be no longer appropriate, as dependence may increase
or decrease. However, as long as the evidence model is treated as a unit, the
diﬀerences between calibrated evidence models of various types is minimal.
When the evidence model only taps a single proﬁciency variable, the model
that combines the all of the observables into a single superobservable has a
distinct advantage: it has fewer parameters than some of the other models.
That makes it easier to estimate with smaller data sets and gets away from
possible issues with identiﬁability or collinearity in the more complex models.
This can be seen in the preference for holistic scoring over analytic scoring
when human raters score essays (Breland et al. 1987). The analytic scores
add little additional information when the goal is to assess overall writing
performance.
Multiple observables become more interesting when there are also multiple
proﬁciency variables. Diﬀerent observables from the same task may tap dif-
ferent combinations of proﬁciencies. When a subset of observables address the
same combination of proﬁciency variables, the designer might consider using
an additional rule of evidence to combine the similar observables rather than
modeling the local dependence.
Often feedback observables are chosen from intermediate observables that
a calculated as part of the rules for calculating the overall observables. The
student can be given values task by task with these feedback observables, not
as estimates of proﬁciency but as descriptions and evaluations of particular
performances. For example ETS’s CriterionSM can provide both feedback on
grammar, usage and style issues in the essay and an overall score using the
e-rater engine (Attali and Burstein 2006). The ﬁrst step in the processing is
to put the essay through a grammar checker that identiﬁes issues of grammar,

12.5 The Assembly Model
453
usage, mechanics and style. These are sent to Criterion’s feedback mechanism
to provide low-level feedback. They are also used as part of the overall task
scoring, which is then accumulated over tasks as evidence for a proﬁciency that
spans tasks. Error rates for the four kinds of errors are among the features
used in computing the ﬁnal e-rater score.
Observables can also be used for research purposes, to inform the test
designers about some aspect of how students are approaching the task. As
an example of these research observables, consider a computerized test that
records both the answer provided by the examinee and the response time the
examinee required to provide the response. While the response itself is used
as evidence of the ability of interest, the response time is typically not used as
evidence (there are many alternative explanations for why an examinee may
have a longer or shorter response time than expected). The response time is is
used for other decision making about the test. For example, tracking response
times for test tasks provides an empirical basis for predicting the required
testing time for a new assessment, and allows the test designers to adjust the
assembly model so that the assessment will ﬁt within an allotted time slot
without adding a speededness component to the assessment.
12.5 The Assembly Model
Most of this book talks about assessments when in many cases what is impor-
tant is not an individual assessment, but an assessment program: a series of
assessments that are all meant to be comparable in some sense. An assess-
ment program deﬁnes a series of forms, each a collection of tasks that are all
administered on the same occasion. There are many reasons why an assess-
ment program may require multiple forms. Two of the most common are that
the assessment will be given to the same students on multiple occasions (as
part of a longitudinal study) and test security (i.e., so that examinees who dis-
cuss the contents of the test with previous examinees will not have an unfair
advantage).
In designing an assessment program, we would like the forms to each pro-
vide similar evidence for the proﬁciencies. The question of how many tasks of
what type are required to make a valid form of the assessment immediately
arises. It is the role of the assembly model to answer this question.
Consider one of the most complex cases that the assembly model must
cover, the case of a computer adaptive test (CAT; Wainer et al. 2000). CAT
does not use ﬁxed forms; rather, the computer assembles the form as the
examinee interacts with the assessment. This means each form is potentially
unique. The CAT algorithm tries optimize information about a student, sub-
ject to various constraints about what kinds of tasks must be in the form, and
what kinds of tasks cannot appear together.
Typical CAT construction proceeds in two stages. The process starts with
the universe of all tasks which have been authored, reviewed, pilot-tested,

454
12 The Conceptual Assessment Framework
and judged suitable for the current assessment. This is sometimes called the
vat. The ﬁrst stage selects a pool of tasks from the vat for deployment in
an operational version of the test. Typically a pool will be in the ﬁeld for a
period of time (from a week to a year) and then will be replaced with a new
pool. This allows new tasks to enter the ﬁeld and old ones to be retired. The
second stage happens after the pool is in the ﬁeld and an examinee sits for
the assessment. At this point the algorithm selects tasks for that examinee to
attempt. The selection happens after each previous task has been scored (this
assumes automatic scoring) so the CAT algorithm can take current estimates
of the examinee’s proﬁciency into account while selecting tasks.
In a CAT, the activity selection process chooses tasks to optimize some
kind of information criteria, called the target rule. In IRT-CAT, this could be,
for example, the Fisher information the assessment provides at the current
best estimate of the examinee’s proﬁciency, or expected minimum variance for
the posterior. Almost all of the quasiutilities discussed in Chap. 7 are fodder
for designing target rules. Adaptive Content with Evidence-based Diagnosis
(ACED) (Shute et al. 2008) used expected weight of evidence.
The activity selection process is not free to pick any task in the pool to
meet the target; the process is subject to a number of constraints. There are
generally two kinds of constraints that are put on the form: minimums (and
maximums) for certain task types, and overlap constraints about two tasks
that are too similar appearing on the same form. Sometimes the constraints
are written in terms of task models, but usually the constraints can be imple-
mented by operating on task model variables (Sect. 7.4).
For high-stakes assessments, where test security is a concern, there are
usually constraints on how often items appear on tests across examinees. Test
security is an important issue in many testing programs; Veldkamp et al.
(2010) provide a concise overview of the item exposure challenge and main
approaches for tackling it. Wang et al. (2011) apply the ideas to cognitive
diagnosis modeling, which transfers readily to Bayes nets proﬁciency models.
Minimum (and maximum) constraints are usually concerned with the
breadth and depth of the evidence provided for each proﬁciency variable.
This also inﬂuences the meaning of the proﬁciency variables. Consider the
variable Reading in the language assessment. Claims about Reading are usu-
ally deﬁned in terms of the genre and complexity of the text to be read. In
order to provide evidence of for those claims, the tasks must be chosen to span
the genres and diﬃculties that are targeted by the assessment. If a test form
randomly selects only a single genre of content, then the eﬀective meaning of
Reading would be diﬀerent from the intended meaning.
Example 12.11 (Language Placement Test, Continued: Register Con-
straint).
In building the proﬁciency model for the Language Placement Test, the
design team identiﬁed two variables, Register and Purpose, for which they
decided not to build proﬁciency model variables. One of the claims associated

12.5 The Assembly Model
455
with the Speaking variable is that students can speak in a register appropriate
to the situation; for example, they can distinguish between language that is
appropriate to use with their instructor and language that is appropriate to
use with their peers. In order to validate that claim, a constraint is added to
the assembly model a Register variable is added to speaking tasks with two
possible values: inferior-to-superior and peer-to-peer. A constraint is
added to the assemble model that at least one speaking task of each type must
be included on every form of the assessment. The Purpose variable similarly
requires a constraint to enforce the distribution over several values.
An overlap constraint becomes necessary when two tasks are too close in
content to appear in the same form. If solving one task would provide hints for
another, then local independence assumption would be violated; that is, the
observables from the two tasks would be dependent even conditioned on the
proﬁciency variables. In this case, some kind of exclusion rule is necessary.
For example, the reading comprehension tasks could include a task model
variable describing the topic of the reading material. An exclusion rule would
guarantee that not too many reading texts about the same topic appear on
the same form.
When the CAT is implemented, an activity selection process will use the
information targets and constraints to build an assessment according to the
following algorithm. At each stage of the testing (when it is necessary to
select a new task) the activity selection process consults the student-speciﬁc
copy proﬁciency model called the student record) to get the current state of
knowledge about the student, then selects a task that maximizes the current
information target subject to the constraints. The last step that is required
is a stopping rule, a criterion for when to stop. It may be based on testing
time, number of tasks administered, obtaining suﬃcient information about
the proﬁciency variables, or some combination of these.
An alternative to CAT is a linear form in which all examinees see the same
items in the same sequence (often several linear forms are randomly assigned
or spiraled to the examinees). The linear form can either be computer admin-
istered or presented in a paper-and-pencil format. In a linear assessment the
pool stage of assessment construction is skipped and the activity selection pro-
cess (which runs in advance of the assessment now) goes straight from the vat
to the form. The same kinds of constraints are relevant to both the linear and
CAT assessments; however, the information targets are now slightly diﬀerent.
The goal in a linear form is to optimize the information for a population of
test takers. Linear forms usually provide about as much information as a CAT
for examinees in the middle of the proﬁciency distribution, but the CAT pro-
vides more information for examinees in the tails of the distribution. A big
advantage of linear tests over CAT is that the forms are created in advance.
Problems with automatic test construction algorithms can be detected and
ﬁxed ahead of time.

456
12 The Conceptual Assessment Framework
Multistage testing oﬀers a possibility midway between linear testing and
task-by-task adaptivity (Lord 1980; Yan et al. 2014). In a multistage test, a
number of short linear forms, called stages, are constructed. At the end of
each stage, the form to use in the next stage is selected based on the current
proﬁciency estimate for the student. Because there is usually a small number
of forms for the stages, they can be hand-checked, so any strange interaction
of the constraints and information criteria can be ﬁxed in advance. However,
there is usually suﬃcient information gain from just a few stages that the
amount of testing can be reduced.
When the assessment will report about more than one variable, the activity
selection process needs to balance the information about all of the reporting
variables. The algorithms described in Sect. 7.4 are helpful in this case.
One strategy is to pick a primary target (for example, Communicative
Competence in Fig. 12.4). As the other proﬁciencies are linked to this proﬁ-
ciency, the selection algorithm should choose among tasks designed to address
the this node.
Madigan and Almond (1995) predict that the primary proﬁciency approach
will have an unusual and unwanted behavior. In the Language Proﬁciency
example, an assessment built in this way is apt to switch rapidly among the
four modes (e.g., ﬁrst a Reading, then a Listening, then a Speaking, then
a Writing task, and then back to Reading again; see Sect. 7.4.2 for more
discussion). Madigan and Almond (1995) suggest a strategy called critiquing
(Barr and Feigenbaum 1982). In terms of the assembly model, this means that
there are multiple targets and some intermediate stopping rules for switch-
ing between them. Another way to think about this that the multiple targets
represent diﬀerent sections of the assessment, each of which has its own pro-
ﬁciency target and stopping rule.
Example 12.12 (Language Placement Test, Continued: Target and
Stopping Rules). For the language placement assessment (Example 12.3)
the design committee decides that they need good information about all four
modal variables: Reading, Writing, Speaking, and Listening. They also decide
to use expected weight of evidence as their information metric. In this case,
targets consist of a binary hypothesis, that is, both a target variable and a
target level. The goal is to ﬁrst identify students who are weak in one or more
of the proﬁciencies to place them into remedial classes, and then to identify
students who are strong in certain proﬁciencies so they can be placed outside
the language support program. Consequently, they decide to try to gather
evidence of low values before evidence of high values. The assembly model
has the following target and stopping rules:
1. Choose a task, t, to maximize EWOE(Reading > Low : Et). Continue
doing this until either P(Reading = Low) > 0.9 or P(Reading > Low) >
0.66.

12.5 The Assembly Model
457
2. Choose a task, t, to maximize EWOE(Listening > Low : Et). Continue
doing this until either P(Listening = Low) > 0.9 or P(Listening > Low) >
0.66.
3. Do the same thing with Speaking.
4. Do the same thing with Writing.
5. If P(Reading > Low) > 0.66, then choose a task, t, to maximize
EWOE(Reading ≥High : Et). Continue doing this until either P(Reading
≥High) > 0.9 or P(Reading < High) > 0.9.
6. If P(Listening > Low) > 0.66, do the same thing for Listening.
7. If P(Speaking > Low) > 0.66, do the same thing for Speaking.
8. If P(Writing > Low) > 0.66, do the same thing for Writing.
To complete the assembly model, the design team needs to add task-variable
minimum constraints to ensure a selection of genres, purposes, and registers.
The implementation team might also need to add overlap constraints to avoid
multiple tasks on the same topic within a given form.
The kind of stopping rule used in this example, looking for thresholds for
proﬁciency variables above and below certain cut scores, is appropriate when
the assessment will be used for classiﬁcation decisions (e.g., the placement
decision). An alternative stopping rule is to try to bring the standard error
of measurement below a threshold. This is appropriate when the purpose of
the assessment is to order the students or when the cut score is not known in
advance.
The stopping rule is just one way of expressing the fundamental question of
the assembly model (or for that matter the entire CAF): Does the assessment
provide enough evidence to support the claims we want to make based on
the information it provides? Whether the assessment is adaptive or linear,
the assessment design needs to be checked to ensure that it provide adequate
evidence. This includes both a theoretical check towards the end of the deign
phase (a construct validity argument, see Kane 2006) and an empirical check
when the completed assessment is ﬁelded (criterion, concurrent and predictive
validity checks). Naturally, the purposes for which the assessment will be used
and the stakes (the consequences to participants for incorrect decisions) will
inﬂuence the amount of evidence that is needed.
Once the evidence models and assembly model are speciﬁed, there is
enough information to construct a Q-matrix for the assessment. In particular,
the evidence models correspond to rows of the Q-matrix and the assembly
model describes how many times each row type is repeated. A simple check at
this stage is to simply scan down the rows of the evidence model to see that
each proﬁciency is represented a reasonable number of times (Almond 2010a).
More sophisticated analyses can look for other patterns of evidence that may
be problematic (such as blocking, see Gierl et al. 2007). These checks can help
the design team uncover the need for new kinds of tasks to gather evidence
not provided by the current assessment.

458
12 The Conceptual Assessment Framework
12.6 The Presentation Model
Although the number of assessments delivered by computer increased rapidly
during the end of the twentieth century, paper-and-pencil administration
is still frequently used in the beginning of the twenty-ﬁrst century. Com-
puter delivery oﬀers advantages (e.g., interactivity and automatic capture of
responses) but can be challenging in remote locations, and it can be diﬃcult
to obtain enough computer workstations to accommodate a high-volume test.
Many assessments must be designed for both computer and paper-and-pencil
delivery. New delivery methods are constantly being explored. Small devices
such as smartphones and tablet computers are being explored as modes of
delivery. More assistive technologies are becoming available for persons with
disabilities.
One of the reasons for formal assessment design methodologies like ECD
is to promote reuse of assessment elements. In particular, we would like to
use the same task models with multiple presentation platforms. The role of
the presentation model is to adapt tasks for particular platforms. The task
model deﬁnes the elements, the presentation material that makes up the task,
the interactions the platform must support, and the work products that must
be collected. The presentation model describes how they are rendered in the
delivery platform.
The presentation model is a style sheet for the task model, describing
how the elements of the task are arranged. For example, consider the piece
of presentation material providing the examinee with instructions for how to
complete the task.4 The presentation model for computer delivery might call
for this to always be displayed in a constant location, for example, a sidebar
on the screen. The presentation model for paper delivery might call for tasks
with common instructions to be presented together and the instructions to
be printed once at the top of the page on which the tasks appear. If the
assessment is for examinees with limited reading ability (say, young children),
the presentation model may also call for text-to-speech capability.
A question that can be important is whether alternate modes of presenta-
tion aﬀect the evidentiary properties of a task. Bridgeman et al. (2001), in one
of the few formal studies to investigate this question, looked at the issue of
screen size, resolution, and display rate on test performance. The study found
little diﬀerence for a math test, where most items ﬁt on a single screen even
at smaller screen sizes. It did ﬁnd a diﬀerence for a verbal test that included
passage-based reading comprehension tasks. In particular, tasks were more
diﬃcult if an examinee had to scroll to see the entire text.
Whether or not such diﬀerences matter depends on the purpose of the
assessment. In a low-stakes testing situation, the convenience of being able
to use whatever equipment is at hand may outweigh the construct-irrelevant
variance caused by students using diﬀerent presentation platforms. Accurate
4 In British usage, this would be called the rubric for the task.

12.6 The Presentation Model
459
comparisons among students at diﬀerent sites are not required. In high-stakes
testing situations, however, it may be important to control the presentation
platform tightly so as to not give examinees with access to better equipment
an unfair advantage. In either case, the presentation model must be clear
about what range of presentation platforms are appropriate.
Although assistive technologies have expanded the population of exami-
nees to provide access to more individuals, they also bring with them questions
about when a given accommodation is appropriate, or even possible. Consider
creating a presentation model for a read-aloud protocol that reads the item
text using a synthesized human voice. This works fairly well for a purely text-
based item, although the time limits may need to be adjusted as listening
often takes longer than reading silently. Additional thought is needed at the
level of the presentation model if the physical layout of the text is important
to the meaning (for example, in a table or an equation). The presentation
model needs to understand how to convey the physical layout through text. A
task that depends substantially on a visual stimulus (say a task that involves
reading a map or describing a piece of artwork) may not be compatible at
all with the read-aloud presentation model. In this case, the accommodation
needs to be made at the level of the assembly model. The task types that rely
on visual stimulus need to be replaced with tasks that do not.
Even that might not be suﬃcient if the task type was critical to obtaining
evidence to support one or more claims. Consider the use of the read-aloud
accommodation on a reading test. Part of the reading construct is decoding,
mapping the letters on the printed page to sounds. The read-aloud presenta-
tion has the computer do the decoding for the examinee. For young elementary
students, decoding is an important component of the reading construct. Thus,
the read-aloud accommodation would remove evidence about an important
component of the construct and the test with read-aloud would be substan-
tially diﬀerent. Most college students have no diﬃculty with the decoding
skills. Consequently, the read-aloud accommodation presents little problems
when assessing this population.
Accommodations for special needs is a complex issue, and this discussion
has only scratched the surface. Hansen and Mislevy (2004) and Mislevy et al.
(2013) use Toulmin diagrams to map out how the accommodation changes the
evidentiary argument, and how properly chosen accommodations can provide
more valid arguments for more diverse populations of students. The ECD
framework allows us to frame a complex question about accommodations as a
question about evidence. This framing allows the test designers to weigh the
consequences of adapting tasks for new kinds of presentation. It also enables
them to design systems that can present comparable tasks in diﬀerent ways to
diﬀerent students based on their proﬁles of knowledge and skill as are needed
for access but irrelevant to the targeted proﬁciencies.
When the assessment includes simulations or when tasks are embedded in
games, then the capabilities of the simulator or game engine are part of the

460
12 The Conceptual Assessment Framework
presentation model. Consider the design of the physics game Newton’s Play-
ground (Shute et al. 2013). Both Newton’s Playground and the commercial
game Crayon Physics Deluxe that inspired it are built oﬀthe same Box2D
physics engine; however, they diﬀer in the way they use it. In both games,
players make a ball by drawing a circle on the screen, but the two games diﬀer
in how they calculate the mass of the ball. In Crayon Physics the mass is pro-
portional to the area of the ball on the screen, while in Newton’s Playground,
the mass of the ball is proportional to the length of the line used to draw it.
Thus players can draw heavy objects by scribbling inside of them. Thus, the
Newton’s Playground presentation model has a capability not present in the
Crayon Physics model: it can create tasks where students need to distinguish
between mass and volume, a facet of physics understanding that Kennedy and
Wilson (2006) identiﬁed as a key state in a physics-learning progression.
12.7 The Delivery Model
Most of the important decisions made in the course of designing an assessment
are recorded in the proﬁciency, evidence, task, assembly, and presentation
models. However, there are a few important decisions that do not seem to ﬁt
anywhere else. For example, “Who can see the score reports and for how long
are they are available?” “What kind of identiﬁcation must candidates show
before sitting for the assessment?” “How long will the assessment take?” The
role of the delivery model5 is to capture these extra questions that do not seem
to ﬁt anywhere else.
Although these questions may seem unrelated to the evidentiary argument,
they actually set the stage for a number of hidden assumptions within the
evidentiary argument. For example, we assume that the person whose name
appears on the score report was also the person who sat for the exam. If
this is not true, the observations we make in the course of the assessment
provide little evidence on way or another about the true proﬁciencies of the
candidate. Similarly, the rules for how score reports delivered are important
because because the score report could be altered or changed (akin to “chain
of custody” issues in jurisprudence). Thus, these additional constraints help
eliminate potential threats to the evidentiary argument.
The question of assessment length is one that arises frequently. Ideally, this
would be the province of the assembly model; that is, the assessment would be
over when enough evidence was gathered for the purposes of the assessment. In
practice, the assessment always takes time away from other activities (unless
the assessment can be embedded within those activities). Thus, in almost all
assessment designs the length is set by external considerations. For example,
the assessment will be 45 min long because that is the length of one class
period, or the assessment will be no longer than 2 hours long because too few
schools would adopt it if it were longer.
5 The delivery model appears as the surrounding box in Fig. 12.1.

12.8 Putting It All Together
461
This has implications that bear on the other models. The time limit imme-
diately impacts the assembly model, and how much evidence can be gathered.
This also impacts the proﬁciency model. The limited amount of evidence
almost certainly means that only a few proﬁciencies can be reliability mea-
sured. This, in turn, argues for a smaller proﬁciency model. This is a tension
that is seen in many assessment designs: While the subject matter experts
would like a large proﬁciency model that spans the entire domain, the time
constraint means that only a small portion of that domain can be measured
in a typical large-scale assessment. Evidence-centered assessment design does
not resolve any of these dilemmas, but it does help the design team recognize
them early so that they can work out resolutions appropriate to the purpose
of the assessment being designed.
12.8 Putting It All Together
When writing about ECD, describing the models in a linear fashion is almost
unavoidable. There is a temptation to then conclude that building a CAF
is a linear process, in which the design team ﬁrst develops the proﬁciency
model, then the evidence models and task models, then the assembly models,
and ﬁnally the presentation and delivery models. In practice the assessment
design process is seldom so linear. The design team will need to work back
and forth between all parts of the model, frequently going back an revisiting
previous work to make sure it works well with the new pieces, and to take
into consideration newly discovered constraints.
As part of this process, there is a need to evaluate the success of the design
at each stage of development and work with the team to revise the design as
needed. ECD presents a clear criterion: the assessment must provide suﬃcient
evidence to support its purpose, as expressed in its claims. The ECD process
lays out an explicit chain of reasoning connecting task to observable evidence
to proﬁciency variables to claims. It is easy to get caught up in technical details
of the assessment design process such as creating tasks or ﬁtting measurement
models, and lose sight of the purpose. It is worth frequently returning to
initial statements claims and evidence to ask whether the current design is
appropriate to the purpose.
Another question that often arises is how much detail is needed in a com-
pleted CAF. The answer is enough so that all of the development work needed
to implement and operate the assessment (see the next chapter) is clear to
the teams that need to do that work. If the assessment is similar to other pre-
vious assessments, many details can be left vaguely speciﬁed as “same as last
time.” The ECD process is most valuable when item development is costly
and/or complex, pretesting samples are problematic or expensive to obtain,
novel uses for an assessment are being considered, a new construct is being
addressed, or a strong and explicit argument for the construct and content
validity of the assessment is needed at the outset.

462
12 The Conceptual Assessment Framework
ECD is a means of formalizing and monitoring the process of good design
for educational assessments. It shares with any formal design process the goal
of coordinating the work of the various teams building the assessment. When
any part of the design changes, that change will have impact on many diﬀerent
teams. The formal design allows the team to assess the impact of the change
before making it, allowing the design team to weigh the costs and beneﬁts
of any changes and notify the people whose work will be impacted. This
ultimately makes the assessment stronger and easier to produce.
ECD diﬀers from other formal design and development processes in its
emphasis on the chain of reasoning between claims and evidence, both as
a tool for eﬀective design and as an argument for the construct validity of
an assessment (Kane 2006). The design and development process forges this
initial chain of reasoning, incorporating the evidential arguments between
design components and conclusions, and the scoring process takes advantage
of this structure to ground appropriate inferences about participants based on
their interaction with the assessment. The next chapter explores the processes
of implementing and operating the assessment.
Exercises
12.1 (Competing Tasks). Consider the claim, “The student has suﬃcient
writing skills to complete assigned term papers.” Now consider two possi-
ble observations related to that claim. (1) The student performs well on an
admission essay, and (2) The student performs poorly on a timed essay in a
placement test. Draw a Toulmin diagram for these two arguments. How would
you reconcile the contradictory evidence?
12.2 (Memorization of Tasks). A certain high-stakes testing program uses
a computer adaptive test in which the same pool is in the ﬁeld for about a
month. The design team for this program discovers a web site on which stu-
dents discuss tasks from the test and their solutions. Build a Toulmin diagram
that contains the possibility that students have memorized the question and
the answer.
12.3 (Score Ordering). Consider the score report in Fig 12.6. Does the
order (left-to-right) of the bars in that ﬁgure matter? What is the best order
and why? If an additional bar is added for overall communicative competence,
where should it be placed?
12.4 (Statistic Choices). Consider the score report design from Exam-
ple 12.7. The design committee is considering three diﬀerent statistics for
Reading:
•
Proﬁciency Level. One of low, medium or high depending on whether
P(Reading = low), P(Reading = medium) or P(Reading = high) is larger.

12.8 Putting It All Together
463
•
Probability of Mastery. 100P(Reading = high) (recall that the high state
is considered strong enough ability to place out of the remedial course).
•
EAP Score. 100P(Reading = medium) + 200P(Reading = high)
Assume that University C has a limited number of spaces in the remedial
classes and wants to give preference to the student who need help the most.
What is the best statistic choice for the placement decision? What is the best
statistic choice for providing information to the instructor about the students
enrolled in the class? Explain your answers.
12.5 (Reading Comprehension Task Model). One of the task types for
the language placement test is based on the classic reading comprehension
task. In this task, the student is presented with a piece of text to read and
then asked a series of questions that are meant to provide evidence that the
student understood what was read.
Build a task paradigm (a sketch of a task model) for this task type. Your
task paradigm should include the following features:
•
A description of the presentation material.
•
A description of the expected work products.
•
A list of the most important task model variables and the roles that they
play.
12.6 (Reading Comprehension Evidence Model). Build an evidence
paradigm (a sketch of an evidence model) for the reading comprehension task
described in Exercise 12.5. This evidence paradigm should answer the follow-
ing questions:
•
What are the observables?
•
How do the observables relate to the work product (evidence rules)?
•
What kind of data in addition to the task is required for the evidence
rules?
•
How do the observables relate to the proﬁciency variables (in either of the
two proﬁciency models under consideration)?
•
How should local dependence among the observables be modeled?
•
What kind of task-level feedback should be available, and are additional
observables necessary to support it?
12.7 (Read-Script Task Model). Another task under consideration for the
language placement assessment is a script reading task, in which the student is
expected to read text that appears on the computer screen into a microphone.
Build a task paradigm (a sketch of a task model) for this task type. Your
task paradigm should include the following features:
•
A description of the presentation material.
•
A description of the expected work products.
•
A list of the most important task model variables and the roles that they
play.

464
12 The Conceptual Assessment Framework
12.8 (Read-Script Evidence Model). Build an evidence paradigm (a
sketch of an evidence model) for the script reading task described in Exer-
cise 12.7. This evidence paradigm should answer the following questions:
•
What are the observables?
•
How do the observables relate to the work product (evidence rules)?
•
What kind of data in addition to the task is required for the evidence
rules?
•
How do the observables relate to the proﬁciency variables (in either of the
two proﬁciency models under consideration)?
•
How should local dependence among the observables be modeled?
•
What kind of task-level feedback should be available, and are additional
observables necessary to support it?
12.9 (Read Script Accommodation). One of the foreign students admit-
ted to University C has severely limited vision, and normally accesses online
text through the use of a read-aloud program. This student requests a
read-aloud accommodation on the Language Placement assessment. Will
this accommodation aﬀect the evidence from the script reading task (Exer-
cises 12.7 and 12.8)? Should the student be provided with adapted version of
this task, a substitute task, or an assessment that does not include this task
(and an advisory note that this part of the construct was not tested)?
12.10 (Reliability of Overall Score). Use the proﬁciency model in Fig. 12.4,
and an assembly model that consists of four sections:
1. A Reading section that consists of 30 discrete items each of which has a
single binary observable. Each one taps only the Reading variable.
2. A Listening section that consists of 30 discrete items each of which has a
single binary observable. Each one taps only the Listening variable.
3. A Speaking section that consists of six partial credit items, each of which
has a single observable that ranges from 0–4. Each one taps only the Speak-
ing proﬁciency. (There is almost certainly some dependence on Reading
or Listening as well. Assume for the moment that almost all examinees
have the necessary prerequisite skills, so we do not need to model this
dependence. See the next exercise for more details.)
4. A Writing section that consists of four partial credit items, two of which
have a single observable that ranges from 0–6 and two of which have a
single observable that ranges from 0–4. Again, assume that each one taps
only the Writing proﬁciency.
Perform a simulation study to look at the reliability of this assessment
design. Use hyper-Dirichlet distributions for the and pick reasonable values
for the parameters of the proﬁciency model (they should be correlated and
the marginal distributions should put be close to P(X = low) = .25, P(X =
medium) = .5, and P(X = high) = .25). Use the DiBello–Samejima model
for the evidence models and pick reasonable values for the parameters (the

12.8 Putting It All Together
465
average diﬃculty should be zero and the average discrimination should be
one).
Now, perform a simulation using the following steps:
1. Randomly select “true” values for the proﬁciency variables for 1000
simulees.
2. Randomly generate observables for Form A of the assessment for each of
the simulees (there should be a total of 70 observables, 60 binary and 10
partial credit).
3. Calculate the MAP and EAP scores for Form A (assigning a value of 1
to medium and 2 to high) for each of the four modal proﬁciency variables
and the overall Communication variable.
4. Random generate another 70 observable for Form B of the assessment for
each of the simulees (We are pretending that we can generate an exactly
parallel form).
5. Calculate the MAP and EAP scores for Form B using only the second set
of data.
6. Look at the reliability by comparing the similarity of the scores for Form A
and Form B. Calculate the correlation coeﬃcient for the EAP scores, and
Cohen’s Kappa for the MAP scores.
Is the reliability acceptable for the overall Communication variable? For the
four modal variables? How does the reliability of Reading and Listening com-
pare to the reliability of Writing and Speaking? What might this imply about
the validity of the assessment with this design?
12.11 (Reliability and Integrated Tasks). Start with the setup for the
previous problem and change the evidence models for the Speaking and Writ-
ing tasks. Assume that they are now integrated tasks that require either Read-
ing or Listening in addition the the Speaking or Writing variable which is the
primary target. Look at two variants of the evidence model. In Variant 1,
use the compensatory DiBello–Samejima model, and chose sensible values for
the parameters; assume that the discrimination for the primary skill, Speak-
ing or Writing is higher than for the secondary skill, Reading or Listening.
In Variant 2, use the inhibitor model and assume that a medium level of the
secondary skill is necessary to solve the task.
Repeat the simulation study of the previous exercise. What eﬀects do the
integrated tasks have on the reliability?

13
The Evidence Accumulation Process
This chapter talks about how the conceptual assessment framework (CAF)
described in the previous chapter is used to score an assessment.1 Basically,
the models of the CAF lay out the structures for data, materials, and messages
needed for the activities that constitute an operating assessment system. A
four-process architecture describes agents (people, computers, or some mix)
that actually carry out the operations—from determining what to do, to inter-
acting with examinees, to capturing and evaluating their work, to creating and
reporting results.
Although we are interested in the case where the measurement model (that
is, the proﬁciency model and all of the evidence models) are Bayes nets, much
of the discussion in this chapter will apply to any type of measurement model.
This is a universal protocol for scoring: a list of things that processes using
the proﬁciency model and evidence models must do to score an assessment.
Before we can describe how to score an assessment, we must describe what
an assessment looks like as it operates. Section 13.1 deﬁnes four processes that
can be used to describe any assessment implementation. However, there are
still a large number of steps between an assessment design (Chap. 12) and an
assessment implementation. Section 13.2 describes some of the most important
steps in building an assessment. Finally, Sect. 13.3 describes the evidence
accumulation process or EAP2, the process that is responsible for generating
the statistics that will be displayed on the score report. An example from
the ACED (Adaptive Content with Evidence-based Diagnosis; Example 7.5;
1 With suitably broad deﬁnitions for both “score” and “assessment.” By score we
mean synthesizing evidence in students’ performances for inferences about what
they can know or can do more broadly. By assessment we mean a systematic
process for doing this, which could be a familiar test, but could also be embedded
in an interactive simulation or a game, and the user could be not only a teacher
or employer but an instructional system or the students themselves.
2 The abbreviation EAP is also commonly used for expectation a posteriori, or
posterior mean, Bayesian point estimates. It should be clear from the context
which one we are referring to.
c⃝Springer Science+Business Media New York 2015
467
R. G. Almond et al., Bayesian Networks in Educational Assessment,
Statistics for Social and Behavioral Sciences, DOI 10.1007/978-1-4939-2125-6 13

468
13 The Evidence Accumulation Process
Shute et al. 2005; Shute et al. 2007; Shute et al. 2008) illustrates the interplay
between a Bayes-net EAP and the activity selection process in linear and
adaptive assessment strategies.
13.1 The Four-Process Architecture
The four-process architecture (Almond et al. 2002a; Almond et al. 2002b) pro-
vides a fundamental paradigm for conceptualizing, implementing, and commu-
nicating about any assessment. Regardless of the intent of assessment (diag-
nosis, certiﬁcation, placement, etc.), the administration methodology (paper
and pencil, computerized adaptive testing, complex simulations, etc.), or the
scoring method (number-right, item response theory (IRT), neural networks,
etc.), all assessments require the same four fundamental processes (at least
in some trivial form) outlined in the four-process architecture. Establishing
a common conceptualization and language for these assessment processes is
an important step in the development of a more thorough, eﬃcient, and com-
plete means for researchers and test development professionals to understand,
implement, and communicate about assessment.
Figure 13.1 depicts the basic four-process architecture, including the inter-
actions between the processes and key elements of operational administration.
We walk through the processes below, but Table 13.1 summarized the essen-
tial features. This table also indicates the CAF models that the processes rely
upon, as specifying input, output, or information each needs to do its job.
More detailed discussion of the interrelationships among the CAF models
and the delivery processes appears in Almond et al. (2002a).
The four processes fundamental to every assessment are:
1. Activity selection process. The activity selection process selects assessment
tasks for an examinee from the library of available tasks (the task/evidence
composite library). It can operate either piecemeal or en masse as required
by the assessment design (e.g., linear or adaptive testing). “Available
tasks” include all aspects of the examination, including of course tasks
that elicit information about measurement variables from which claims
are made about participants, but also procedures to collect demographic
data about them. In a more general system designed for learning, such as
HYDRIVE (Sects. 1.2, 12.3, and 12.4), the collection of tasks could include
some whose primary focus was instruction in addition to the assessment
tasks. The activity selection process then relays information about the
selected task(s) to the presentation process.
2. Presentation process. The presentation process is responsible for the pre-
sentation of the selected task(s) to the examinee. It obtains the infor-
mation about the task from the database (the task/evidence composite
library) and renders the selected task(s) in an appropriate form for the
mode of administration. In an interactive task, it manages the interchanges

13.1 The Four-Process Architecture
469
Table 13.1 Summary of the four processes
Process
Input
Output
Activity selection (ASP).
Selects tasks, ends testing,
initiates subtests; in
learning systems, can
select instructional or
practice tasks
•
Instructions from test
administrator
•
Information from
TECL about available
tasks
•
Requests to TECL for
information about
available tasks
•
Instructions to PP
Presentation (PP)
Interacts with examinee:
Presents stimuli, manages
tool use; captures work
products; in simulations,
updates situation
according to examinee
actions
•
Instructions from PP
•
Presentation material
[TM] from TECL
•
Requests to TECL for
presentation material
•
Work products [TM]
to EIP
Evidence identiﬁcation
(EIP; aka response
processing, task-level
scoring). Given work
products and evidence
rules, determines values of
observable variables for
both task-level feedback
and for accumulating
evidence over tasks
•
Work products [TM]
from PP
•
Evidence-rule data
[EM-RE] from TECL
•
Requests to TECL for
evidence-rule data
•
Task-level feedback to
examinee [EM-RE]
•
Values of observable
variables [EM-RE] to
EAP
Evidence accumulation
(EAP; aka summary
scoring, test-level scoring).
Given values of observable
variables, integrates
evidence in the form of
belief about student
proﬁciency variables, via
measurement model such
as IRT or Bayes net
•
Measurement-model
fragments [EM-SM]
from TECL
•
Parameters/weights
[EM-SM] from TECL
•
Values of observable
variables [EM-RE]
from EIP
•
Requests to TECL for
measurement-model
fragments [EM-SM].
•
Requests to TECL for
parameters/weights
[EM-SM]
•
Examinee scoring
record [PM] to ASP
Bracketed abbreviations in second and third columns indicate models of the concep-
tual assessment framework (CAF) where models, messages, or data are speciﬁed:
PM proﬁciency model, TM task model, EM-RE evidence model, rules of evidence,
EM-SM evidence model, statistical model, TECL task/evidence composite library

470
13 The Evidence Accumulation Process
Fig. 13.1 The four-process architecture for assessment delivery
Reprinted from Mislevy et al. (2004) with permission from the Taylor & Francis
Group.
between the system and the examinee. During administration of the task,
the presentation process records the work product (the raw response) of
the examinee and delivers it to the evidence identiﬁcation process (EIP).
3. Evidence identiﬁcation process (also called response processing). Evidence
identiﬁcation is the ﬁrst stage of scoring: identifying the key features of
the examinee performance that bear evidence about the examinee’s knowl-
edge, skills, and abilities. Evidence identiﬁcation receives the work prod-
uct(s) from the presentation process and assigns values for one or more
observable outcome variables. It records the observables in the examinee
record and sends them to the EAP for aggregation across several tasks, as
well as to the activity selection process for generating task-based feedback
or other immediate activity selection decisions.
4. Evidence accumulation process (also called summary scoring). The EAP
acts on the evidence provided in the observed outcomes from particu-
lar tasks, as received from the evidence identiﬁcation process, to update
the current belief about the examinee’s knowledge, skills, and abilities.
This updated belief is then available for the next iteration of the activity
selection process and/or for summary feedback.
Central to each of these processes is the task/evidence composite library.
The task/evidence composite library is a database of assessment task objects,
descriptions of these objects, the information necessary to select and present
them, indication of the examinee work product to be retained, the information
necessary to score the work product, and the process for integrating task
evidence into updated beliefs about examinee knowledge, skill, and ability.

13.1 The Four-Process Architecture
471
The IMS Global Consortium adopted the four-process architecture as part
of their information model for question and test interoperability (IMS 2000).
In doing so, they renamed the “evidence identiﬁcation” as “response process-
ing” and “evidence accumulation” as “summary scoring.” (They also renamed
“work products” as “responses” and “observables” as “outcomes.”) The names
using “evidence” ﬁt better with the evidence-centered design philosophy that
is the focus of this book, and they apply just as well to assessments that do
not look like familiar tests.
13.1.1 A Simple Example of the Four-Process Framework
The roles of these four processes, and the function of the physical elements
of the four-process architecture, can be illustrated by following a dichoto-
mously scored IRT CAT (item response theory computer adaptive test3; see
Sect. 7.4.1) through a single cycle of the administration process.
We begin in the top left-hand corner with the role of the administrator.
The administrator is the person(s) responsible for initializing and maintaining
the assessment delivery. In initializing the assessment, the administrator must
make various choices, such as initializing the examination in the proper mode
when alternative modes are available. For example, in the Biomass assess-
ment (Steinberg et al. 2003; this volume, Chap. 14) the administrator could
choose between a diagnostic assessment to support learning and an end-of-unit
summary assessment. The administrator might also make examinee-speciﬁc
conﬁguration decisions; for example, whether the examinee will receive special
accommodation for disability, such as increased time or larger screen size. In
this role the administrator will access a preexisting examinee proﬁle or create
one for the administration.
The examinee record is the collection of data about the examinee for the
assessment administration. Initially, the record may possess little more than
basic demographic information and data about the registration of the exami-
nee for the assessment. As the assessment progresses, the examinee record will
accumulate data related to that examinee’s performance on the assessment
such as those describing the tasks presented to the examinee, the examinee
response latencies, and the navigation of the examinee through the assessment
interface.
A key part of the examinee record is the scoring model. It contains variables
that can be used in reporting scores; particularly, variables describing the
current belief about examinee knowledge, skills, and abilities. The scoring
model variables will start at initial values based on the population of learners
who typically use the assessment as described in the proﬁciency model. As
the assessment progresses, the values of the scoring model variables gradually
change to reﬂect the evidence provided by the participants’ performances.
3 See Wainer et al. (2000) for a good introduction to IRT CAT, and van der Linden
and Glas (2010) for a more technical treatment.

472
13 The Evidence Accumulation Process
For the IRT CAT example, the assessment begins with the creation of a
new examinee record for the examinee based on the examinee’s proﬁle and the
rules given by the assessment design. The scoring model is a distribution over
the single proﬁciency variable, θ. Typically this is either a ﬂat distribution or
a normal distribution whose mean and variance are given by the population.
Once initialization is complete, the activity selection process—in this case
an automated routine—accesses the task/evidence composite library to select
an initial task (in this case, all of the tasks are simple items) for the exami-
nee. In so doing, the activity selection process considers a variety of features of
potential tasks, all of which are stored in the task/evidence composite library.
These factors may include item content, item exposure rates/control variables,
information function of the item, item weights of evidence (in this case, item
parameters), and dependence with other items in the task/evidence composite
library. Because the test is adaptive, the activity selection process also consid-
ers the current state of knowledge about the examinee’s proﬁciency (initially
vague) to aid in selection. Once it has selected a task, the activity selection
process sends a message to the presentation process informing it about which
task to schedule next and delivering the parameters necessary to adapt the
item for the individual.
The presentation process receives from the activity selection process the
messages describing the task and the manner in which it is to be admin-
istered. Acting on this information, the presentation process searches the
task/evidence composite library and retrieves the task and associated pre-
sentation material to present to the examinee. The examinee interacts with
the presentation process to complete the selected task. During this interac-
tion the presentation process records the work product (captured part of the
response) of the examinee in accordance with the requirements of the task. In
the case of single multiple-choice items, the work product is simply an iden-
tiﬁer for the selection made by the examinee. In other situations, the work
product may contain a constructed response or a trace of examinee actions,
results, and navigation through a simulation.
The presentation process sends the resulting work product to the evidence
identiﬁcation process, which accesses the task/evidence composite library to
obtain the rules for evaluating the work product. The evidence identiﬁcation
process applies the evidence rules to the work product to determine from the
raw work product a collection of observed outcome variables that can be used
to update beliefs about the examinee. In the case of multiple-choice items, this
can be a simple process of fetching the key, determining whether the examinee
response matches the key, and setting the appropriate observation to a value of
“correct” or “incorrect.” For diagnostic feedback based upon multiple-choice
items, we may need to produce additional observables based on the selected
distractor. Constructed response items use more complex evidence rules which
are often expressed as rubrics for human raters or as algorithms for automated
scoring. Most rules of evidence are parameterized and along with the task
itself, the task/evidence composite library must store this evidence-rule data.

13.1 The Four-Process Architecture
473
For example, for multiple-choice items the key must be indicated, and for
essay tasks, task-speciﬁc scoring rubrics or scoring notes are required.
Designated results of the evidence identiﬁcation process can be output as
task level feedback, either immediately during the assessment or stored in a
database for future use. The task level feedback can be as simple as indicating
whether the examinee got the item correct or incorrect or as complex as
providing explicit diagnostic feedback regarding the strategies and cognitive
processes used to solve a complex task, depending on the nature and purpose
of the assessment.
The evidence identiﬁcation process sends the observed outcomes from
the evaluation of the work product to the EAP. The EAP accesses the
task/evidence composite library to obtain the links.4 In the case of CAT exam-
inations using dichotomously scored multiple-choice items, this process entails
accessing the task/evidence composite library to acquire the item parameters
and to process the item response data to update the current belief about the
examinee on the basis of the observables obtained from the item response.
The updated beliefs about the examinee and the associated data, such as the
identiﬁcation of the item that was administered, the response, actions, work
product, etc., are then placed into the examinee record, where they remain
available for the next iteration of the four-process cycle.
The examinee record thus always contains what is currently known about
the examinee. Any time a score report is needed, the examinee record can
be queried to produce summary feedback for the examinee. This can be as
basic as a number-right score for a linear multiple-choice test or as complex as
a set of diagnostic and instructionally relevant statements describing exam-
inee knowledge and strategies, depending on the nature and purpose of the
assessment (and how the four processes are deﬁned to meet that purpose).
The four-process architecture is a general enough framework that most
existing assessment delivery implementations can be mapped into it. Often
the evidence identiﬁcation process is lumped in with either the presentation
process or EAP, but separating it out, at least conceptually, gives us a great
deal more ﬂexibility. A simple example of this is a test with written essay
tasks. We could use either human scorers or sophisticated natural language
processing algorithms to score an assessment. Maintaining evidence identi-
ﬁcation as a separate process allows us to swap one for the other without
changing the other processes or the messages passed among all the processes.
The goal of this chapter is to show how to implement the EAP for an assess-
ment that uses Bayesian networks as its primary scoring model. Section 13.3
4 As we will see below, links are task-speciﬁc versions of the evidence models deﬁned
in the previous chapter. They contain explicit information and procedures describ-
ing how the new evidence will be used to update the current belief about examinee
knowledge, skills, and abilities. In particular, they contain weights of evidence, the
values of the task speciﬁc parameters in the link, such as conditional probability
tables in Bayes nets models and item parameters in IRT models

474
13 The Evidence Accumulation Process
describes the primary operations needed to support the other three processes
and how they play out with Bayesian networks. Section 13.2 describes how
we ﬁll the task/evidence composite library with the data needed to support
all four processes.
13.2 Producing an Assessment
This section describes how we can go from the CAF described in Chap. 12 to
the four-process architecture. Section 13.2.1 describes the authoring of tasks.
Section 13.2.2 describes the authoring of evidence rules and the evidence-rule
data they require. Finally, Sect. 13.2.3 describes the process of calibrating
the measurement model from pretest data, a process that produces links, or
task-speciﬁc versions of the evidence model.
13.2.1 Tasks and Task Model Variables
One frequently asked question about the CAF is “Where are the items
(tasks)?” The task models that are deﬁned in the CAF are not objects, but
metaobjects: objects that describe other objects. A task model speciﬁes what
kind of information must be presented to the examinee, but not the exact
information. That is left to the task.
ECD Project:
EMSMTest
ECD Blueprint:
TestCAF
Proficiency
Model
System
Evidence
Model:
System_
Task1
Task
Model:
Task1
Evidence
Model:
System_
Task2
Task
Model:
Task2
Fig. 13.2 Assessment blueprint for a small test. Reprinted with permission from
ETS.
Each task model in the CAF deﬁnes a family of potential tasks. The task
author creates instances of the task model as realized tasks. Consider the CAF

13.2 Producing an Assessment
475
ECD Project:
EMSMTest
ECD Blueprint: TestCAF
System
Evidence
Model:
System_
Task1
Task
Model:
Task1
Evidence
Model:
System_
Task2
Task
Model:
Task2
Task:
Task1a
Task:
Task2a
Task:
Task1b
Task:
Task2b
Proficiency
Model
Fig. 13.3 Tasks generated from CAF in Fig. 13.2
The two tasks are generated from each of two task models. Reprinted with permission
from ETS.
given in Fig. 13.2. Assume that two tasks are authored for each of the two
task models; Fig. 13.3 shows the result.
The task author has two important activities in authoring a task. The
ﬁrst is to ﬁnd or produce material which will be presented to the examinee as
a stimulus for the problem. This includes text, pictures, and sound stimulus
material, as well as prompts and in the case of multiple choice the key and
any distractors. The second activity is to assign values for all of the task
model variables. The task model deﬁnes the name and possible values for the
required task model variables. (The evidence-rule data also depend on the
task model variables, so it is often authored at the same time.) In the ﬁnal
task, the values for all of those variables must be determined.
Task model variables’ values are related to the presentation material. For
example in a subtraction problem, the Minuend and Subtrahend are presenta-
tion material; speciﬁcally, numbers. The Number of Digits in the Subtrahend
is a task model variable whose value is clearly related to the Subtrahend. The
task model variables may be determined before or after the work product. For
example, suppose that the Semantic Density of a reading passage is a task

476
13 The Evidence Accumulation Process
model variable. The task author might ﬁrst ﬁnd a passage and then evaluate
its semantic density, or the author might try to ﬁnd a passage that meets a
target density.
Mislevy, Steinberg, and Almond (2002c) deﬁne a number of roles for task
model variables (Sect. 12.3). Any task model variable may be used for one or
more of these roles. In brief they are: facilitating task construction, controlling
evidential focus, constraining assessment assembly, mediating the relationship
between performance and proﬁciency (in particular, determining diﬃculty),
and characterizing proﬁciency.
There is an advantage when, for some task model variables, there is a
correspondence between the levels of the task model variables and the levels
of the proﬁciency variables. For example, suppose the proﬁciency Skill 1 has
three levels: high, medium, and low. Suppose we can further categorize the
tasks using Skill 1 into two categories: simple applications, and complex
applications. The natural claims for the Skill variable are as follows:
high
A person who operates at the high level of Skill 1 can usually solve
problems which require a complex application of the skill.
medium A person who operates at the medium level of Skill 1 can usually
solve problems which require a simple application of the skill, but
has diﬃculty with problems which require a complex application of
the skill.
low
A person who operates at the low level of Skill 1 has diﬃculty with
problems which require a simple application of the skill.
In this scheme, tasks that require a simple application of the skill will have
a good weight of evidence for distinguishing between the low and medium levels
of Skill 1. Tasks that require a complex application of the skill will have good
weight of evidence for distinguishing between medium and high levels of the
skill. (See Exercise 7.7.)
Some task models variables are used to generate details that lend verisimil-
itude to the task but do not otherwise contribute to the purpose of the task.
For example, the names of the actors in a word problem can usually be freely
modiﬁed without substantially changing the diﬃculty of the tasks. Collis et
al. (1995) call such task model variables incidentals and contrast them to rad-
icals which are used to manipulate diﬃculty or evidential focus, or otherwise
aﬀect the the form or the parameters of the evidence model.
Identifying and manipulating incidentals allows for the automatic gen-
eration of tasks (Gierl and Haladyna 2012). Because the new tasks diﬀer on
surface features, it is harder to memorize a task and answer. Security increases
without having to calibrate additional tasks. It is hard to ﬁnd pure incidental
variables, though. Returning to the example of the names of the actors in the
word problem, using unusual names will add diﬃculty because the examinees
must decode the unfamiliar words rather than just recognize them. Ideally,

13.2 Producing an Assessment
477
the changes to the conditional probabilities caused by variables considered
incidental will be smaller than other sources of error in the scoring process.
Variables that are radicals (that is, they are believed to aﬀect the relation-
ship between performance and proﬁciency) can be passed on to the calibration
process. There they can be used to model the parameters of the evidence mod-
els, either directly or through some sort of hierarchical modeling that reﬂects
clustering of the tasks according to their task model variables (Geerlings et
al. 2011; Mislevy et al. 1993). Section 13.2.3 takes up the issue of calibration.
A step beyond automated task generation is automated task recognition. It
is useful in open-ended performances such as problem-solving simulations and
language proﬁciency interviews. The idea is that in more open-ended problem-
solving spaces, recurring evidence-bearing situations arise as examinees work
their way through a complex task. A task model can now be used to describe
a class of such situations: emergent tasks. An instance of an emergent task
is recognized when a prespeciﬁed set of values for the deﬁning task model
variables occurs.
Human raters use intuitive “automated task recognition” informally when
they give broad ratings to complex performances. An assessor conducting an
oral proﬁciency interview, for example, recognizes places where an examinee
should use past tense and notes whether or not she does. This information
is incorporated into the holistic rating she assigns the examinee. An exam-
ple of a testing program that applied the approach more formally was the
Praxis III: Classroom Performance AssessmentsT M assessment for beginning
teachers (Dwyer 1998). Trained assessors make observations with respect to
19 categories of more generally described dimensions of teaching practice that
are meant to apply across a range of teaching styles, curricula, and student
compositions.
An example of a computer-based simulation assessment that used auto-
mated task recognition is HYDRIVE (Mislevy and Gitomer 1996). If an exami-
nee worked himself into a situation where the problem-solving strategy called
space-splitting was possible, then an instance of a space-splitting task was
recognized and the next sequence of actions that had an eﬀect on the active
path in the problem was interpreted as a work product for this kind of task.
Recognizing an instance of a space-splitting task required posting an exam-
inee’s moves to the student record and running a program called an agent
to evaluate the state of the problem as it evolved, summarized in a set of
dynamic task model variables—that is, task model variables whose values are
computed by the presentation process. The agent that recognized instances
of space-splitting tasks (and other agents that recognized instances of other
classes of tasks) continually monitored the task model variables that deﬁned
classes of emergent tasks. When an instance was recognized, the agent sig-
naled the presentation process to capture the associated work product and
send it to the appropriate evidence identiﬁcation process.

478
13 The Evidence Accumulation Process
Task authors must perform one ﬁnal step before completing their work,
and that is to identify any data required by the evidence rules. For example, a
multiple-choice item is useless unless the key is known. A scoring rubric for an
essay task is based on the markers of evidence that are needed to support the
claims that performance on the task is meant to support. If the same task is
to be used in multiple assessments with multiple purposes and hence multiple
evidence models, then evidence-rule data must be developed for each evidence
model that might be used to score the task. The next section takes this up in
more detail.
13.2.2 Evidence Rules
The reason for the central database in the four-process architecture being
called the task/evidence composite library is that it contains both information
required by the task models and information required by the evidence models.
The “task” part of the task/evidence composite consists of the presentation
material for that task and any task model variables that are needed by the
activity selection process (i.e., have the role of assessment assembly), or any of
the other processes. The “evidence” part consists of both parameters for the
conditional probability tables in the links (the specialized version of the Bayes
net fragment in the evidence model for this task) and any data required for
the evidence rules. Section 13.2.3 looks at the issue of creating and calibrating
links. This section looks at the evidence rules.
Evidence rules are a mechanism for the reasoning between the raw response
that is captured as the work product of the examinee interacting with the task,
and belief about the examinee’s proﬁciencies. In some cases these rules may
be very simple; for example, matching the key in a multiple-choice item. In
other cases they could be quite complex, like the grading of an essay or the
application of a neural network to evaluate a hundred automatically recog-
nized linguistic features of the same kind of essay. Evidence rules produce
values for one or more observable outcome variables.
In order to reuse evidence rules across multiple tasks (usually from the
same task model), they must be parameterized. For a key-matching rule, the
parameter is the key. For scoring an essay, in addition to the scoring rubric
(the evidence rules) there are often prompt-speciﬁc scoring notes instructing
the scorers about what speciﬁc points to look for in the student essays. These
serve as the “parameters” for the essay-scoring rules. In complex situations
the parameter of an evidence rule could be quite complicated. Often these
are expressed with scoring tables or some other similar data structures. These
parameters are called evidence-rule data.
ROC Analysis
An interesting case is one in which the observable outcome is determined
by making a cut on an underlying count or continuous property of the work

13.2 Producing an Assessment
479
product. The count or continuous property could be the output of one or more
parsing rules which extract relevant features from a work product.
For example, consider a task in which an examinee is asked to write down
as many names for articles of clothing as possible within a speciﬁed time
period. We can deﬁne an intermediate variable by counting the number of
correct and incorrect answers provided. Suppose that one of the ﬁnal observ-
ables is Correct Set Size which can take on values small, medium, and large.
In this case the evidence-rule data will be a threshold for cutting the count
variable produced in the parsing step. This kind of cut point rule is inter-
esting because we can tune the thresholds to give the task better evidential
properties.
Suppose that both the proﬁciency variable and and ﬁnal observable out-
come variable are binary. When both variables are binary, it is usually the case
that one state of the proﬁciency variable is better (more positive) than the
other; similarly one state of the observable is usually considered better (e.g.,
correct) than the other. It should also be the case that when the proﬁciency
variable is in the positive state, the correct value of the observable is more
likely than when the proﬁciency variable is negative (if this does not hold,
then the observable generally is not providing good evidence). In this case,
we can “classify” the student on the basis of the observable. If the proﬁciency
variable is in the positive state and the observable is in the correct state, then
this is a true positive; however, when the proﬁciency variable is in the negative
state and the observable is in the correct state, we call that a false positive.
True and false negatives are similarly deﬁned. Table 13.2 shows graphically
the joint probabilities for the observable and the proﬁciency.
Table 13.2 Confusion matrix for binary proﬁciency and observable
Proﬁciency
High
Low
Observable Correct
True positive, ptp False positive, pfp
Incorrect False negative, pfn True negative, ptn
The diagonal cells of Table 13.2, often called a confusion matrix, con-
tain the probabilities of the observable that point toward the actual proﬁ-
ciency state, ptp and ptn. The oﬀ-diagonal cells contain the probabilities of
the observables that point towards the other proﬁciency state, pfp and pfn.
False positives are also called Type I Errors and false negatives are called
Type II Errors. A number of statistics can be deﬁned based on these num-
bers:
•
The sensitivity is deﬁned as ptp/(ptp + pfn); this is also called the recall.
It is the proportion of positive cases that the observable actually indicates
as positive.

480
13 The Evidence Accumulation Process
•
The speciﬁcity is deﬁned as ptn/(pfp + ptn). This is the proportion of
negative cases the observable indicates as negative.
•
The positive predictive value is deﬁned as ptp/(ptp +pfp); this is also called
the precision. The false discovery rate is the complement of the positive
predictive value, pfp/(ptp + pfp).
•
The negative predictive value is deﬁned as ptn/(pfn + ptn).
•
The accuracy is deﬁned as ptp + ptn. Note that if one of the states is rare,
then it is possible to have fairly high accuracy just by chance. Cohen’s
kappa is the accuracy corrected for chance agreement (see Sect. 7.5.1).
When the observable variable is created from a feature of the work product
via a cut point, then the sensitivity and speciﬁcity will depend on the chosen
cut point. If having more of the feature is better, then moving the cut point
higher should decrease the Type I (false positive) error rate and the expense
of the Type II (false negative) error rates. Moving the cut point lower has the
opposite eﬀect. The receiver operator characteristic (ROC) curve (Fig. 13.4)
is a useful tool for visualizing the trade-oﬀbetween the two types of errors.
1
2
3
4
5
6
7
8
9
A
0.0
0.2
0.4
0.6
0.8
1.0
0.2
0.4
0.6
0.8
1.0
1 − specificity
sensitivity
Fig. 13.4 A sample receiver operating characteristic (ROC) curve
This curve plots the sensitivity (true positive rate) vs. 1−speciﬁcity (false positive
rate) for various choices of the cut point for an evidence rule. It looks like 6 or 7
would make good choices. Reprinted with permission from ETS.
The ROC curve comes out of signal detection theory, but it is extensively
used in the ﬁeld of medical testing. The basic form is to plot the true positive
rate (sensitivity) versus the false positive rate (1−speciﬁcity). The diagonal
line at 45◦represents the worst possible test, one that simple guesses ran-
domly. The more area under the curve above the diagonal line, the better the
test. Diﬀerent tests can be compared in this way.
The ROC is most often used to explore the trade-oﬀbetween false positive
and false negative errors. We can pick a cut point based on which type of error

13.2 Producing an Assessment
481
we would rather avoid. Consider the example in Fig. 13.4.5 If we choose 5 as
the cut-oﬀ, we get a sensitivity of 88 %, but a speciﬁcity of only 50 %. If we
choose 8 as the cut-oﬀ, we get a speciﬁcity of 90 % but only get a sensitivity of
50 %. Depending on what type of error we would rather make we can adjust the
test to either end or in between. This is actually fairly typical for a situation
in which the underlying feature is a count. In this case, the ideal cut point is
often between the two possible values, and one of them is used.
Evidence Rule Analysis
However much research goes into the CAF, it is still a theory of how the
assessment should work. As such it predicts how the students should behave
when they address the tasks. When the tasks are placed in front of actual
students, the results can be surprising. These surprises happen frequently
enough that the best practice for assessment design is to perform some kind
of pilot test. Usually a small scale pilot is ﬁrst, often with students videoed
or interviewed after completing the assessment to learn how students actually
think about and interact with the item. Later a larger scale pilot is used to
get statistical information about the students’ performance.
Suppose we have pretest data for a large number of examinees that are
a representative sample of the potential assessment population. We can now
do some model checking on the evidence rules. This evidence rule analysis
is very similar to conventional item analysis. However, given that the target
proﬁciency model is a Bayesian network, there are some techniques that are
particularly attractive.
The ﬁrst place to start is with the marginal distribution of the observed
outcome variables. For dichotomous observables, this is sometimes called the
P + in item analysis. Obviously, if all or nearly all of the pretest population
obtain the same value for the observed outcome, then the task will have little
weight of evidence for almost any hypothesis. This is grounds for revisiting
the design and implementation of both the evidence rules and the task.
The strategies for ﬁxing such a task depend on which observable values
were not observed in the pretest data. If the best possible outcomes were
not observed, the task may be too diﬃcult for the target population. The
designers should consider simplifying the problem, or even if that kind of
task is appropriate for this population. If the worst possible outcomes were
not observed, then the task is too easy. The designers should again consider
modifying or dropping the problem. There are though a number of situations
in which tasks that are too easy are still useful. In an end-of-course or end-
of-unit assessment, easy tasks serve as conﬁdence builders for the students,
letting them know that they have mastered as least some of the material.
Also, getting a very easy task wrong has a high diagnostic value in identifying
students who have very low levels of ability and need remedial work.
5 This example was generated artiﬁcially using two Poisson distributions, one with
mean 8.5 and one with mean 5.5.

482
13 The Evidence Accumulation Process
If there are more than two observable outcomes, often the best solution is
to combine a state with very few students with one of the other states. When
the states are ordered, this is often a problem with one of the middle states.
The question then arises of whether to merge the unneeded state with its
higher or lower neighbor. Reviewing the claims associated with the observable
states helps determine which combination is appropriate in a given situation.
Every evidence model assumes a relationship between one or more proﬁ-
ciency variables and an observable outcome variable. This assumption can be
checked through the two-way table formed by one of the observable outcome
variable and one of the proﬁciency variables it provides evidence for. The
problem is that the proﬁciency variables are latent, and we cannot observe
them directly. However, we can approximate their values using the other tasks
in the pilot test.
If we have built the links for all of the tasks, then estimating the value
of the proﬁciency variable is simply a matter of applying the usual scoring
algorithm to the rest of the tasks. The catch is that we may not yet have
performed the calibration step (Sect. 13.2.3), so we do not have ﬁnal values
for the link parameters. As this is just an exploratory procedure, setting the
link parameters to the mean of their prior distributions (from the evidence
models) should produce a close enough approximation to get a rough estimate
of proﬁciency—not good enough for important decisions about students, but
useful for exploration and troubleshooting.
With a Bayesian scoring model, there are two frequently used scores: MAP
and EAP scores. They lead to diﬀerent ways of constructing the two-way score-
by-observable table. The MAP produces as a “score” the most likely value of
the proﬁciency variable. In this case, the table of interest is the crosstab of the
estimated proﬁciency variable and the observed outcome variable, a matrix A
where aij is the number of examinees who were classiﬁed as having proﬁciency
state i and obtained observable value j.
The EAP leads to the expected accuracy matrix (Sect. 7.5.3). Here we use
the marginal distribution of the proﬁciency variable for Examinee e, P(Se).
Suppose that Examinee e obtains the Observed Outcome j. Then P(Se)
becomes a contribution to the jth row of the matrix A. That is
aij =

Examinee e gets Outcome j
P(Se = si) .
The matrix constructed from the MAP estimates may suﬀer from a high
degree of variability when the sample size is small. The matrix constructed
from the marginal distribution should have smaller estimation error.
Table 13.3 shows the expected accuracy matrix for an observable called
PC3 in an experimental task, Task Exp4.1. In this case there was a pretest
with 500 examinees. The abilities are estimated from another set of 12 tasks
with known link parameters. The table on the left shows the raw numbers.
The entry in each cell is the sum of the probability of being in the state

13.2 Producing an Assessment
483
Table 13.3 Expected accuracy matrix for observable PC3 in Task Exp4.1
Skill 1
Skill 1
Low
Med
High
Low Med High
Exp4.1
2
8.86
83.38
67.76
2 0.07 0.32 0.59
1
66.47 107.22
20.30
1 0.55 0.41 0.18
0
44.60
73.86
27.54
0 0.37 0.28 0.24
Total 119.93 264.45 115.60
Normalized
Unnormalized
indicated by the column for all students who got the observable value on the
left. For example, the cell in the upper left is the sum of the probability of
being in the low state for all of the students for the 160 students who got a
2 on PC3 in Task Exp4.1. The diﬀering column totals for the left-hand table
make the columns diﬃcult to compare. To make the table easier to interpret,
we divide by the column totals, producing the table on the right. Now we can
see the pattern we hope: as the proﬁciency gets higher, the probabilities shift
towards the better values of the observable.
Table 13.4 MAP accuracy matrix for Task Exp4.1
Skill 1
Skill 1
Low Med High
Low Med High
Exp4.1
2
7
85
68
2 0.06 0.31 0.59
1
63
111
20
1 0.55 0.41 0.17
0
44
74
28
0 0.39 0.27 0.24
Total 114
270
116
Normalized
Unnormalized
Table 13.4 shows the same analysis, but using the MAP estimates of pro-
ﬁciency rather than the marginal distributions. Again, we normalize the table
by dividing by the column sums to make the results easier to interpret. With
a sample of 500, the diﬀerences between the two analyses are negligible.
Table 13.5 MAP accuracy matrix for Task Exp6.1
Skill 1
Skill 1
Low Med High
Low Med High
Exp6.1
2
9
68
52
2 0.08 0.25 0.45
1
1
0
1
1 0.02 0.00 0.01
0
104
202
63
0 0.91 0.75 0.54
Total 114
270
116
Normalized
Unnormalized

484
13 The Evidence Accumulation Process
Table 13.5 shows a task that has a problem: At all three proﬁciency levels,
almost nobody picked the middle category. One possibility is a problem with
the instructions for the task, causing students to not approach the task cor-
rectly. A second possibility is a problem with the scoring rules, that somehow
the distinction between 1 and 2 score (or 0 and 1 scores) is not properly being
made. One simple possibility is to eliminate the middle scoring category and
just assign scores of 0 or 2 for this task. (The conditional probability tables in
the evidence model need to be collapsed if the middle category is eliminated.)
Ultimately, we are interested in tasks that have a high expected weight of
evidence for cut points on the proﬁciency variable. One way of determining the
strength of the relationship is to look at the the mutual information between
the proﬁciency variable and the observable. Observables for which the mutual
information with the target proﬁciency variable is small (in particular, where
it small with respect to other similar tasks) are not contributing much to the
overall estimation. Designers might consider dropping or reworking them.
A central assumption of the partitioning of our model into proﬁciency
model and evidence model is that the observable outcome variables in our
model are independent of all of the other proﬁciency variables given the bound-
ary variables. This is a testable hypothesis. For simplicity, consider the case
where the footprint consists of a single proﬁciency variable. Using the scores
and observed values from the pretest data, we can construct a three-way table
with the observed outcome, the boundary variable and a diﬀerent proﬁciency
variable. Table 13.6 shows an example.
Table 13.6 Three-way table of two observables given proﬁciency variable
θ = high
θ = med
θ = low
Y = 1 Y = 0
Y = 1 Y = 0
Y = 1 Y = 0
X = 1
35
7
X = 1
31
99
X = 1
43
102
X = 0
35
11
X = 0
33
50
X = 0
18
36
MAP estimates for θ were used in calculating this table.
In this three-way table, we can test this conditional independence assump-
tion using a chi-square or Mantel–Haenszel test (see Bishop et al. 1975; Fleiss
et al. 2003). Essentially, what the Mantel–Haenszel test does is to ﬁt the model
assuming that the observable and boundary variables are independent given
the boundary variable, and looks at the residuals from that ﬁt. If the residuals
are high, then we reject the independence hypothesis and question whether
the evidence model is truly appropriate for the task. Bishop et al. (1975) sug-
gest, as an alternative, ﬁtting both models with and without independence
and looking at the change of deviance in the models.
The χ2 test for this diﬀerence is straightforward to calculate. Consider
the data in Table 13.6. The three states for the boundary variable Skill 1

13.2 Producing an Assessment
485
(denoted θ) deﬁne three strata of ability. We essentially perform a χ2 test
for independence at each strata. Consider the subtable on the left. If the
conditional independence property holds, the the expected values for the cells
in that table will be P(X|Skill 1 = high)P(Y|Skill 1 = high). The expected
values for the other two strata are calculated similarly. However, we do not
know the corresponding probabilities, so we need to estimate them from data.
Consider a conﬁguration of possible states, x ∈states(X), y ∈states(Y ),
z ∈states(θ), and let nxyz be the observed number of student who had MAP
a proﬁciency level of z and observed values x and y for X and Y . To esti-
mate the various probabilities, we need to look at the sums across the various
dimensions. Call the sum over all of the states of X, n+yz = 
x∈states(X) nxyz,
the sum over all the states of Y , nx+z = 
y∈states(Y ) nxyz, and the sub over
the states of both X and Y , n++z = 
x∈states(X)

y∈states(Y ) nxyz. Under
the hypothesized conditional independence assumption, the expected value for
each cell is then, ˆnxyz = nx+zn+yz/n++z, and we can calculate a χ2 goodness
of ﬁt statistic in the usual way:
χ2 =

z∈states(θ)

x∈states(X)

y∈states(Y )
(ˆnxyz −nxyz)2
ˆnxyz
.
(13.1)
The obvious reference distribution is a χ2 distribution with (|X|−1)(|Y |−
1)|θ| degrees of freedom (here |X| refers to the number of possible states for
variable X). The problem is that the values of θ are not known, but rather are
estimated from data. Therefore, we are not sure that the χ2 distribution is the
proper reference distribution. However, the quantile of the χ2 distribution still
provides a rough heuristic for cases that require further observation. Using the
MAP for θ, the χ2 value for Table 13.6 is 7.1, which is close to the critical
value of 7.8, so it may be worthwhile to look for causes of dependence between
the two tasks generating X and Y .6
Table 13.7 Three-way table of two observables given marginal proﬁciency
θ = high
θ = med
θ = low
Y = 1 Y = 0
Y = 1 Y = 0
Y = 1 Y = 0
X = 1 34.65 22.78
X = 1 36.26 89.13
X = 1 38.08 96.04
X = 0 36.04 17.90
X = 0 32.46 42.94
X = 0 17.48 36.14
Marginal estimates for θ contributed to fractional values for each case.
We could perform a similar test using the expected accuracy matrix (i.e.,
using the marginal distributions in place of the MAP entry). The values of
6 The table was actually produced from simulated data in which there was residual
conditional dependence. This may be a problem with the χ2 reference diﬀerence,
or it could be the dependence was too small to detect with this sample.

486
13 The Evidence Accumulation Process
nxyz would no longer be integers, but fractions as each observation would
contribute only a fractional value to each cell (Table 13.7). This is essentially
the method proposed in Sinharay et al. (2004), and is similar to the model
checking techniques covered in Chap. 7. The χ2 value in this example is 5.0.
This suggests that the χ2 reference distribution is not adequate (this was noted
by Sinharay et al. 2004). Comparing Tables 13.6 and 13.7 shows that using
the marginal values in place of the MAP estimates smoothes the table quite
a bit; hence the lower χ2 values. We could instead use posterior predictive
model checking as discussed in Sect. 10.2 to ﬁnd a reference distribution for
this statistic.
Diﬀerential item functioning (DIF) (or in our case diﬀerential task func-
tioning) can also be expressed as a conditional independence assumption
(Sect. 10.4). A test is considered fair if given the proﬁciency variables in the
boundary for the evidence model, the task performance is independence of
group membership (e.g., gender, race). We can similarly construct a three-way
table using the boundary variables, the observed outcome and a demographic
variable representing group membership and explicitly test for this indepen-
dence assumption using the Mantel–Haenszel test (Holland and Thayer 1988).
One frequent response to problems of this type is to change the evidence
rules associated with observables ﬂagged as problematic. However, changing
the evidence rules could change the classiﬁcation of the pretest examinees on
one or more skills. The best thing to do would be to rerun these tests after the
adjustments to the evidence rules. Calibration (next section) also will change
the classiﬁcations, so these tests are best run before calibration and again
after calibration.
13.2.3 Evidence Models, Links, and Calibration
Although evidence-centered design encourages test developers to make the fac-
tors which they are manipulating in tasks (the task model variables) explicit,
this alone is not suﬃcient to completely control variability in task perfor-
mance. In actual practice, examinees can surprise the test designers, and react
to a task in completely unexpected ways. This diﬀerence between theory and
the real world happens often enough that it is necessary to verify the per-
formance of any task before using it. The evidence rule analysis techniques
described in the previous section will catch gross departures from the expected
results, but smaller changes are also possible; for example, a task could be eas-
ier or harder than expected, or the dependency on one of the proﬁciency vari-
ables could be stronger or weaker than predicted by the model. In low-stakes
situations, this variability is commonly ignored and the end users live with a
certain amount of approximation error in their estimates. In situations where
that is not acceptable, the evidence models can be calibrated to produce more
accurate results.
Just as the task is a realization of the task model, the link is the realization
of the evidence model for a speciﬁc task. (If the task model supports more

13.2 Producing an Assessment
487
Fig. 13.5 Link model generated from Task 1a in Fig. 13.3
The link corresponding to Task 1a is created through calibration. Reprinted with
permission from ETS.
than one evidence model, the task will have a link corresponding to each
one). The link has exactly the same graphical structure as its parent evidence
model. It only diﬀers in the values of the parameters (weights of evidence),
which have been tuned for the speciﬁc task.
Just as the task model deﬁnes a set of possible tasks, the evidence model
deﬁnes a prior distribution over the set of possible links. The methods of
Chap. 9 (Markov chain Monte Carlo, MCMC or expectation–maximization,
EM algorithms) can be used to calculate the posterior (given pretest data)
distribution for a link for a given task. In later scoring, the posterior means
of the parameters for the links are used to score the assessment (although the
full posterior distributions are maintained for future calibrations, as discussed
in Sect. 9.6.2 with regard to their use in online calibration).
For lower-stakes situations, we can skip the calibration step, and simply use
the prior (evidence model) mean as the parameters for scoring. (The previous
section used this trick to get a preliminary scoring in order to test the evi-
dence rules.) In this case, the link is essentially a copy of the evidence model.
If controlling the task model variables suﬃciently constrains the operating
characteristics of tasks arising from task models, setting the link parameters
equal to the evidence model parameters might not be a bad approximation.
For example, an automatic task generation procedure might be allowed to ran-
domly select incidentals, but hold all radicals ﬁxed. In this case, the error by
assuming that all links have the same parameter might be ignorable (although
in moderate-to-high-stakes assessments, this assumption is worth testing).

488
13 The Evidence Accumulation Process
Even if the radicals are not ﬁxed, their values can be used during calibra-
tion. Fischer (1973), Embretson (1998), and Mislevy et al. (1993) demonstrate
this idea using IRT models. These ideas have seldom been put into practice, in
large part because of the expense of coding task model features retrospectively.
Evidence-centered design solves that problem by making the capture of the
values for the task model variables, in particular, the radical variables, part
of the design process. A notable illustration is the British Army Recruitment
Battery (BARB) (Irvine 2013). Every item in every test for every exami-
nee is generated from a model built around a cognitive theory, radicals, and
incidentals. Both the presentation materials and link models are generated
automatically.7
Usually links are not calibrated one at a time, but instead all of the links
for an entire form or pool are calibrated simultaneously with the proﬁciency
model. The calibrated links and proﬁciency models go into the task/evidence
composite library to form the data to drive the EAP. The assessment descrip-
tion is an index to all of the material stored in the task/evidence composite
library that should be used for administering and scoring a given assessment.
13.3 Scoring
In the four-process architecture, the responsibility for scoring is divided
between the EIP and the evidence accumulation process (EAP). The EIP is
responsible for scoring that is local to a particular task; that is, processing the
work product to produce the task level observed outcomes. These outcomes
can be used to drive task level feedback or sent to the EAP so the evidence
they contain about proﬁciency variables can be integrated with evidence from
the results from other tasks, or both. The EAP is responsible for scoring that
occurs across tasks in an assessment or a section. It produces the scores that
are used on summary score reports.
We can think about the four processes as four agents, each of which
responds to messages from the others.8 Each of the agents follows a certain
protocol: a set of messages that the agent will accept, and a description of
how they respond to those messages. That protocol deﬁnes how each process
behaves for the other processes, so that each process can regard the others as
7 Prof. Sidney Irvine, the creator of BARB, relates that the inspiration for BARB
came from a challenge from Dr. John Anderson: “What would tests be like with
no item-banks, no IRT--and no money!”
8 This is the fundamental idea of object-oriented programming: that parts of the
program can be thought of as “agents” who communicate through messages.
Software engineers have found that by dividing up a complicated program in this
way, they can assign responsibility for each of the pieces to a diﬀerent programmer,
and have reasonable assurance that when all of the pieces are assembled the
system will work properly.

13.3 Scoring
489
a black box that does the things deﬁned in its protocol. This is very useful for
the assessment designer, as the process of implementation can now be split
into several pieces, which can be located on diﬀerent machines (say a client
workstation in the test center and a large server in the computer center) and
which can be developed by diﬀerent software vendors.
The goal of this section is to describe the protocol for the EAP, which
will give a good idea for how scoring works in four-process architecture. Sec-
tion 13.3.1 describes the basic protocols necessary to support scoring a linear
test, or any test in which the adaptivity is based on task level outcomes and
not accumulated scores. Section 13.3.2 describes the minor changes that need
to be made to these protocols to support adaptive testing. Section 13.3.3
describes some technical issues like the handling of omitted and repeated
tasks. Finally, Sect. 13.3.4 describes how score reports are generated.
13.3.1 Basic Scoring Protocols
The heart of the EAP is what it does when presented with new evidence in the
form of observed outcomes sent from the evidence identiﬁcation process. For a
Bayes net-based scoring engine, the basic idea is that it ﬁnds the link particular
to a given task, and “docks” it with the scoring model—the proﬁciency model
for a particular examinee. It then instantiates the evidence in the link and
propagates it to the scoring model. The docked link can now be discarded.
Scores are reported by querying the scoring model. Figure 13.6 gives a general
picture. Almond and Mislevy (1999) gives a detailed description.
We can describe this protocol more formally by considering the EAP as
a server that reacts to a number of diﬀerent incoming messages. The three
most important messages are:
Candidate Begin A new candidate has arrived to start the assessment.
Absorb Evidence New evidence has arrived for a task from the EAP.
Report Score A score report is needed for this candidate.
We assume that each message contains header information to provide con-
text. In particular, each message is associated with a particular candidate and
a particular assessment (this deﬁnes which task/evidence library the EAP
should search for data) and a task.9
To better understand how the EAP must work, we explore what happens
in response to these three messages. For this example, we assume that the
assessment description calls for two proﬁciency models, a Bayesian network
model for the primary scoring and a number right model which is used pri-
marily to count tasks. This is actually a trick used in building the Biomass
assessment (see Chap. 14). The Bayes net was used for the primary inference,
but the number right model was used produce descriptive information about
9 Although there are special IDs for the beginning and end of an assessment, when
the task is not needed.

490
13 The Evidence Accumulation Process
Fig. 13.6 Absorbing evidence from Task 1a for a single candidate
The scoring model is created from the proﬁciency model when the candidate begins
the assessment. As evidence arrives from Task 1a, a copy of the link is obtained and
the observable values are instantiated in that copy. The evidence is then propagated
into the scoring model and the link copy is discarded. Reprinted with permission
from ETS.
the number of tasks the student had attempted for the score report. This
design required two sets of links for each task, one for the Bayes net proﬁ-
ciency model and one for the number right proﬁciency model. The idea can
be further extended to additional EAPs, such as ones that simply accumulate
whether a given misconception is evidenced in a task where it is apt to surface.
Candidate Begin. This message is received at the start of an assessment, and
tells the EAP to start an examinee record for a given candidate. The message
header should contain both the identiﬁer for the candidate and the identi-
ﬁer for the assessment. The EAP then looks up in the assessment descrip-
tion (essentially the assembly model for the assessment) what the appropriate
proﬁciency model is for this assessment. It then asks the proﬁciency model to
create a scoring model for this candidate. If the assembly model contains more
than one proﬁciency model, it creates multiple scoring models; the examinee
record is then a container for those scoring models.

13.3 Scoring
491
In the Bayesian network framework, the scoring model is essentially an
examinee-speciﬁc copy of the proﬁciency model.10 The proﬁciency model is
thus the prior value of the scoring model, containing our prior beliefs about
the examinee before seeing any responses. As we see more responses, the scor-
ing model will move away from the proﬁciency model to become a posterior
speciﬁc for that student. The proﬁciency model will remain at the initial state
ready to produce a new scoring model for the next examinee.
In the number right case, the scoring model is a collection of counters;
counters for the number of tasks seen, the number of tasks regarded as “cor-
rect,” the number of score points earned and the number of score points that
could have been earned. If there are subscores, each one of them requires a
set of counters as well. The counters are set to their initial values (usually
zero, but may be something else according to the scoring rule deﬁned in the
assessment description).
At this point, we have a new examinee record for the ﬁrst examinee. In
our running example it contains two scoring models: a Bayes net initialized to
the prior distribution values deﬁned in the Bayes net proﬁciency model, and
set of counters for the number right model initialized to the values deﬁned in
the number right proﬁciency model.
Absorb Evidence. When the evidence identiﬁcation process ﬁnishes process-
ing the work product from a task response, it sends an Absorb Evidence mes-
sage to the EAP. The header for the Absorb Evidence message must convey
which examinee, which assessment, and which task the evidence comes from.
The body of the message contains a number of names of observable outcome
variables and their values. The EAP then must fetch the appropriate scoring
model from the examinee record collection and the appropriate links for the
task named in the message from the task/evidence library. It can then use
these links to update the scoring model.
In the case of the example, one link corresponds to the number right
scoring model and one to the Bayes net scoring model.11 Consider the link
for the Bayesian network model. It contains a Bayesian network fragment
linking some of the proﬁciency variables to one or more observable variables.
The EAP ﬁnds the observable variable in the message and instantiates the
variables in the Bayes net fragment at the values in the message.
It is possible that one or more of the observable variables in the fragment
is missing from the message, or a given observable variable is irrelevant to
a particular performance. In these cases, such observables are left uninstan-
10 This is the reason we depreciated the use of the term “student model.” The
proﬁciency model refers to the properties of a population of students, the scoring
model refers to a single student.
11 It is possible to have multiple links update the same scoring model (as when a
task comprises multiple subtasks with distinct links), but this adds complexity
without adding insight. For the moment we will assume that there is exactly one
link for each task and scoring model pair.

492
13 The Evidence Accumulation Process
tiated. It is also possible that there are observable variables in the message
that do not appear in the Bayes net fragment. This could happen if the EIP
generates additional observables for task level feedback, or if they are needed
for the number right score. The Bayes net EAP ignores the extra variables.
Once the observable variables are instantiated in the Bayes net fragment,
the marginal distribution for the boundary variables is calculated. Recall that
the boundary variables are the proﬁciency variables found in both the proﬁ-
ciency model and evidence model (and by extension in both the scoring model
and the link). Thus, the marginal distribution over the boundary nodes can be
entered into the scoring model as virtual evidence. This procedure is described
in more detail in Sect. 5.4 (also Almond et al. 1999).
The link for the number right model does a simpler version of the same
thing. It looks for the observable that corresponds to “correct” for this task,
and if it has the “correct” value, it increments the “correct” counter. It also
increments the number of tasks counter, no matter what the value of the
parameter. The number right link has an additional parameter, a scoring
weight. It multiplies the “correct” value by the weight and adds that to the
score counter. It also increments the possible score counter by the value of the
weight.
After this update, the examinee record contains two updated scoring
models. The Bayesian network model contains the posterior distribution given
the observations from this task (and all previous tasks from which evidence
was absorbed). The number right model model contains the counts of the num-
ber of tasks seen and the number for which the “correct” value was observed.
The EAP stores the examinee record away for the next time a message comes
in for this examinee; either another Absorb Evidence message or a Report
Score message.
Report Score. Any time another process needs information about the state
of the scoring model it can send a message requesting the value of certain
scores. The message header contains an identiﬁer for both the examinee and
the assessment. The body of the message contains a list of the desired scores.
The EAP ﬁnds the scoring model for the examinee and the Reporting rule
(Sect. 12.2.3) for each requested score. It returns a message giving a value for
each one.
Note that the scoring models always contain all of the accumulated evi-
dence about the examinee. In the number right model, this is a count of the
number of tasks and the scores. The scores are simple functions of the counter
variables in the scoring model. For example, the percent correct is the num-
ber of tasks that were “correct” divided by the number of tasks seen then
multiplied by 100 (to make a percentage). The percentage score is calculated
similarly, except it uses the number of weighted points earned divided by the
total number of possible points (based on the tasks the student has seen).
In the Bayes net model, the scoring model contains our posterior belief
about the examinee given the observations made so far, in the form of a

13.3 Scoring
493
posterior distribution over the proﬁciency variables involved in that EAP. All
of the scores we report are statistics of that posterior distribution. Common
examples are the marginal distribution or mode (MAP estimate) of one of
the proﬁciency variables. Section 12.2.3 provides a large number of examples.
These can be calculated using the algorithms described in Chap. 5.
A fully realized EAP will require other messages as well, such as Candidate
End (close scoring model and clean up), Save Candidate (save scoring model
to ﬁle or database for use in later session), and Restore Candidate (recreate
scoring model from previous save). These are mostly details for the program-
mers to worry about, and do not aﬀect the logic of how the tests are scored.
In summary, the EAP for any particular examinee taking a linear test
performs the following sequence of actions:
1. When the examinee sits for the examination, the proﬁciency model creates
an initial scoring model for that examinee. In the case of a Bayesian scoring
model, this is a prior distribution over the proﬁciency variables.
2. As the EAP receives observed outcomes for this examinee from the evi-
dence identiﬁcation process, it updates the scoring models for the exam-
inee. In the case of a Bayesian scoring model, this prior estimate of abil-
ity in the scoring model is updated using the likelihood of the observed
outcomes and Bayes’ rule. This likelihood is speciﬁed by the link for each
task. Afterward, the scoring model contains the posterior estimate of abil-
ity. (For a linear test, the distribution over the proﬁciency variables in the
scoring model can be updated all at once, or task by task. The answer is
the same, since the likelihood for the score vector is just the product of
the task likelihoods.)
3. When we wish to draw inferences from our scoring model (in a linear test,
usually only at the end of the test), the scores are deﬁned as functions of
the scoring model variables. In the case of a Bayesian model, scores are
usually statistics of the current posterior distribution of the proﬁciency
variables, such as the posterior mean and standard deviation, or posterior
mean and standard deviation for some transformation of the proﬁciency
variables such as a scale score or a market-basket score (Sect. 12.2.3).
13.3.2 Adaptive Testing
The three basic messages described in the previous section are suﬃcient for
a linear test—a test in which the sequence of tasks is ﬁxed in advance—or
for a test in which the examinee can pick the sequence of tasks. An adaptive
test requires closer communication between the EAP and the activity selec-
tion process. Before selecting the next item in an adaptive test, the activity
selection process ﬁrst consults the EAP about our current beliefs about exam-
inee proﬁciency. This can happen in one of two ways: (1) either the activity
selection process can query the EAP about the value of the statistics it wants,
or (2) the EAP can automatically generate certain statistics (including the

494
13 The Evidence Accumulation Process
ones needed by activity selection) in response to every Absorb Evidence mes-
sage. On the basis of these statistics, activity selection selects an item which
maximizes the value of evidence, subject to content constraints and exposure
controls.
Automatically reporting the value of key statistics (for example, the
marginal distribution of reporting variables, or the percentage of tasks solved
correctly) at every iteration of the update has other advantages. Often look-
ing for a trace of the probability values for certain nodes over time (as new
observed outcomes arrive) can help explain how scores came about. Recall
that the evidence balance sheets (Chap. 7) are calculated by looking at the
diﬀerence in probability at adjacent time points.
If we plan to use expected weight of evidence to do task selection as
described in Sect. 7.3, the EAP must support one additional message type, a
Calculate EWOE message. This message’s header must specify the examinee,
so that the EAP can ﬁnd the correct scoring model. The body of the message
should specify both what hypothesis is under consideration and a list of can-
didate tasks. In response to this message, the EAP fetches the scoring model
for the examinee and the link for each task in the list of possibilities.
Recall that a hypothesis corresponds to a set of possible proﬁciency pro-
ﬁles. To calculate the expected weight of evidence, the EAP starts by instan-
tiating the hypothesis in the scoring model; that is, it restricts the variables to
values that appear in the hypothesis. It then performs the update algorithm
in reverse, that is, it calculates the marginal distribution over the boundary
variables for each link, and propagates it to the link. Given the marginal dis-
tribution for the boundary variables and the link, the EAP calculates the joint
distribution of the state of the observable variables given that the hypothesis
holds. It calculates this conditional distribution for each possible link.
Next, the EAP temporarily instantiates the negation of the hypothesis in
the scoring model and repeats the calculation. From this data we can calculate
the expected weight of evidence (Eq. 7.5) for each task. This can be returned
as an ordered list of tasks or a value for each task.
This calculation is typically too expensive to do for all of the tasks in the
task/evidence composite library. Typically some sort of heuristics are neces-
sary to prune the list of possibilities before calculating the expected weight of
evidence. One simple heuristic is to look for tasks which will provide direct
or almost direct (one or two nodes removed) evidence about the hypothesis
node. Another possibility is to divide the tasks up into sections covering the
content domain, and then search for the best tasks within each section. This
approach has the advantage of not jumping around to items of very diﬀerent
types.
Example 13.1 (ACED). Adaptive Content with Evidence-based Diagnosis
(ACED) is a computer-based assessment of sequences appropriate for a course
in middle school mathematics (Shute et al. 2005; Shute et al. 2007; Shute et
al. 2008). It is a prototype designed to explore: (a) the use of the Madigan and

13.3 Scoring
495
Almond (1995) expected weight of evidence algorithm (Sect. 7.3) to select the
next task in a assessment, (b) the use of targeted diagnostic feedback, and
(c) the use of technological solutions to make the assessment accessible to
students with visual disabilities.
Graf (2003) describes the construction of the proﬁciency model. ACED
spanned three sequence types—arithmetic, geometric, and other recursive
sequences—commonly taught in 8th grade, but tasks were only developed for
the geometric sequences. The model is expressed as a tree-shaped Bayesian
network with the following proﬁciency with an overall sequences proﬁciency
node at the top, with nodes for arithmetic, geometric, and other recursive
sequences as its immediate children. Figure 13.7 shows the details for the
geometric sequences branch of the model.
Fig. 13.7 ACED proﬁciency model
Only the central branch of the model for geometric sequences is elaborated. The
other two branches were symmetric to it Reprinted with permission from ETS.
ACED used the expected weight of evidence algorithm almost exactly as
described in Sect. 7.3. If the SolveGeometricProblems node was chosen as the
hypothesis, the activity selection process went through the 63 tasks developed
for ACED and picked the most informative task. As this was always the same
task at the beginning of the test, ACED selected a random task for the ﬁrst
one to force diﬀerent sequences. The student answered the ﬁrst task, and
ACED updated the proﬁciency model. ACED then checked the remaining 62
tasks to ﬁnd the one with the highest expected weight of evidence.
Table 13.8 shows the expected weight of evidence calculation using selected
tasks (all medium diﬃculty). The second column of numbers is produced

496
13 The Evidence Accumulation Process
by temporarily instantiating the target variable, Solve Geometric Problems
to low. The values in the column are the probability that the each of the
observables will be correct after the instantiation. To get the numbers in the
ﬁrst column, we retract the previous instantiation and now set the hypothesis
to be true. In this case Solve Geometric Problems ≥medium is a compound
hypothesis. To set this to be true, we use virtual evidence: We set the likelihood
of any state for which it holds (i.e., high and medium) to be one and the
likelihood for any state for which it does not hold (i.e., low) to be zero. After
propagating these values, the probabilities that the observables are correct
give the numbers in the ﬁrst column. With these sets of numbers, the expected
weight of evidence can be easily calculated using Eqs. 7.1 and 7.5.
Table 13.8 Calculation of expected weight of evidence
H = Solve Geometric Problems ≥medium
Task
P(X = 1|H) P(X = 1|H)
EWOE
Common Ratio Task
0.72
0.62
0.0222
Explicit Rule Task
0.29
0.28
0.0002
Recursive Rule Task
0.49
0.46
0.0018
Verbal Rule Task
0.44
0.31
0.0372
Table Task
0.55
0.36
0.0746
Visual Task
0.56
0.34
0.1001
Numbers based on a simpliﬁed version of the Bayesian network model for ACED
(Shute et al. 2008; Example 7.5); in particular, this model only uses medium
diﬃculty tasks.
ACED continued this process until the student had seen all 63 tasks (these
were short math items taking a minute to solve, not the complex performance
tasks used in Biomass). The ACED design team talked over rules that would
be used to switch hypotheses nodes from the arithmetic, to the geometric,
to the other-recursive sequence nodes following the critiquing strategy dis-
cussed in Madigan and Almond (1995), should the other two branches be
implemented.
ACED was built so that the activity selection process could be switched
between the adaptive expected weight of evidence and a linear algorithm that
returns the tasks in a ﬁxed sequence. It also had two feedback modes, one
that provided accuracy–only feedback, and one that provided an elaborated
feedback based on misconceptions middle school student are likely to have
about sequences. This allowed an evaluation of ACED (Shute et al. 2007;
Shute et al. 2008) where about 300 students were randomized into four diﬀer-
ent treatments: (1) adaptive sequencing and elaborated feedback, (2) adaptive
sequencing and accuracy only feedback, (3) linear sequencing and elaborated
feedback, and (4) a control group which did not use ACED at all. The partic-

13.3 Scoring
497
ipants in the study were also given a short pretest and posttest on geometric
sequences.
Even though the task sequence was adaptive, ACED was conﬁgured to
present all 63 tasks to each student. Shute et al. (2008) looked at the correla-
tions between the posttest score and the expected a posteriori (EAP) scores
for various subsequences of the items. For the adaptive sequence conditions,
the correlation between the ACED EAP scores based on the ﬁrst 20 items and
the posttest score was as high as the correlation between the EAP score from
all 63 items and the posttest. Furthermore, that correlation was about as large
as the reliability of the posttest. Therefore, the adaptive version of ACED was
doing about as well as possible after 20 items. The linear version, in contrast,
needed nearly the full 63 items to reach the same level of correlation with the
posttest. To be fair, the linear sequence was chosen in such a way that the full
63 items were needed to span the space of geometric sequence problems. A
20- or 30-task linear sequence that was selected with that test length in mind
should do better.
However, this was not the most interesting aﬀect of the adaptive selection
reported in Shute et al. (2008). Looking at the diﬀerence between the pretest
and the posttest scores, the control and accuracy–only feedback showed vir-
tually no change. The elaborated feedback paired with the linear sequence
showed a small but not signiﬁcant gain. The only signiﬁcant gain between
pretest and posttest was shown by the group that got both elaborated feed-
back and adaptive sequencing.
This result was somewhat surprising. After all, the claim made about the
EWOE algorithm is that is provides optimal information about a student,
and not that it optimal for learning. Note that the highest EWOE will be
for an observable that the student has a 50 % chance of getting correct based
on the current state of the proﬁciency model. It is possible that this kind
of task is one that is in the right place in that student’s zone of proximal
development (Vygotsky 1978) to optimally promote learning. However, this
speculation needs to be conﬁrmed with a more systematic study.
13.3.3 Technical Considerations
The preceding sections show a relatively simple procedure for scoring assess-
ments based on just a small number of messages. Adding a few additional
messages enables the system to support adaptive testing as well. However,
real-world testing situations are seldom so simple. There are two issues we need
to work through: how to handle missing responses and repeated responses.
Handling omitted responses is never a simple problem. It is an issue that
cuts across all of the models in the conceptual assessment framework and all
of the processes in the four-process architecture. Handling missingness as a
student interacts with an assessment belongs as part of the delivery model
but we will see it holds implications for the task model, evidence model, and
assembly model.

498
13 The Evidence Accumulation Process
Part of the problem is that there are many reasons that a response may
be omitted, and they can hold diﬀerent implications for how it should change
our beliefs (Mislevy 2015). The National Assessment for Educational Progress
(NAEP) uses three codes for missing values: omitted for when a subject has
left an item blank but answered later items, not reached for when a subject
left an item and all subsequent items in the booklet blank, and error when
the subject made multiple selections or some other problem occurred which
caused it to be impossible to record a response. The situation is even more
complex in the case of extended constructed response tasks or simulations
with large complex work products. Here the system must be robust enough
to deal with complex patterns of omitted responses.
The EAP described in the previous sections handles missing observables
by not instantiating the corresponding node in the graph. This is equivalent
to treating the omitted response as if it was neither positive nor negative
evidence. This is in fact the statistically appropriate way to handle missing
values when those missing values satisfy Rubin’s conditions of missing at ran-
dom (MAR) or missing completely at random (MCAR) (Sect. 9.3). NAEP’s
not reached and error missing values are both MCAR. So are hypothetical
responses to items on a NAEP form that a student was not administered.
Further, responses to tasks that are not presented to an examinee in adaptive
testing are MAR.
One particular result is worth underscoring here, because it applies to two
of the most assessment situations where one might most want to use Bayesian
networks as a measurement model, due to their modularity and recombin-
ability. The ﬁrst is in adaptive testing: Students are presented tasks based
on the values of their previous responses, in order to maximize information
or to obtain evidence about a particular claim (Sect. 7.4.1). The second is in
simulation- and game-based assessment: an examinee’s actions inﬂuence the
task situation as it evolves. Since in both cases the task depends on observable
behavior and beyond that not on the values of either the latent variables or the
unobserved responses, the missingness is MAR. Hence, the correct likelihood
is obtained through the conditional probability matrices of the observations
given the proﬁciencies (Rubin 1976).
Sometimes, however, the fact that the examinee deliberately omitted the
response in fact provides negative evidence for the skills required. In par-
ticular, examinees who have self-conﬁdence in the required skills are more
likely to attempt the task, and at least sometimes self-conﬁdence in having
skills is correlated with actually having them. Simply omitting the response
does not seem appropriate. Rubin calls this “nonignorable” or “informative”
missingness, because observing it should cause us to revise our beliefs about
proﬁciency. A simple solution for omitted multiple-choice responses is to treat
them as partially correct, at the guessing level, or the reciprocal of the number
of alternatives (Lord 1980). This is what NAEP does with omitted responses.
Another approach is to formally introduce an additional response category for
tasks, omitted, and estimate conditional probabilities for omitted given pro-

13.3 Scoring
499
ﬁciency like the other responses. This approach can be implemented using a
DiBello–Samejima categorical IRT strategy, but with the underlying links now
obtained through a nominal response IRT model (such as Bock 1972). More
sophisticated alternatives we will not explore here further introduce models
for examinees’ propensities to omit, and model omission and response jointly
(see Mislevy 2015). Ultimately how to handle diﬀerent kinds of missingness
is a policy decision to be made by the administrative authority for the test,
ideally informed jointly by theory about missingness and an understanding of
the missingness mechanisms that are in play.
The key to omitted response handling in the four-process architecture is
that the evidence rules must specify exactly what happens for omitted, null,
invalid, or other categories of missing work products, according to the pol-
icy decision of the test. There are two approaches the EIP can be instructed
to take that require no additional machinery: (1) set the observed outcome
variables to a predeﬁned default value (e.g., to have them counted a “wrong”
answers), and (2) omit the observables from the message (to have them omit-
ted from scoring). In either case, the EIP could create an additional observable
that would report whether the work product was present or not (which could
then be modeled explicitly in the statistical part of the model). The options
for response processing for not-reached values would be similar.
The approach adding a new response value for omitted requires compa-
rably extended evidence models and links. Lord’s approach of treating omits
as fractionally correct requires an additional Absorb Virtual Evidence mes-
sage (Sect. 5.2.3): Rather than indicating a single value for an observed out-
come variable, this message provides a predetermined vector across its possi-
ble values to propagate to the proﬁciency model. A virtual-evidence vector of
(.75, .25) over the values 0 and 1 would be used for an omitted four-alternative
multiple-choice item.
If the rules for navigation in the test allow the examinees to return and
resubmit results, the EAP may need to deal with repeated collections of
observed outcomes from the same task (this will not be true if the results
are not sent to the EAP until the end of the assessment, but will be true if
they are sent as they arrive). If the EAP applies the normal processing rules
to the repeated tasks, it will record the evidence as if it were two independent
administrations of the task. Although this may not be a bad approximation
in some tutoring systems, it is problematic in a higher-stakes assessment.
One possible solution is to add a Retract Evidence message to the EAP’s
interface. (This would also have uses in calculating the inﬂuence of a particu-
lar task, and in diagnostics which call for calculating the scoring leaving out a
particular task.) In this case the protocol for the EAP could be to replace the
evidence from the previous attempt at the task with the evidence from the lat-
est attempt. Even this is likely to be unsatisfactory in the case where task level
feedback, such as a hint, has been presented to the examinee between the ﬁrst
and second attempts. One way to deal with multiple attempts with feedback

500
13 The Evidence Accumulation Process
is to deﬁne an observed outcome variable that combines information about
correctness and number of attempts, then use a graded response model. The
values of an observable outcome variable for a dichotomous item with feedback
or would not simply be Right and Wrong, but Right-on-the-first-attempt,
Right-on-the-second-attempt, etc.
13.3.4 Score Reports
The last role of the EAP is always to calculate the scores. As was mentioned
previously, it can be called upon to do so any time it receives a Calculate
Scores message; however, such a message will almost certainly be sent at the
end of the assessment. The body of the Calculate Scores message contains
a list of which scores are required, along with any parameters that might
be necessary (for example, a request for a percentile may have a parameter
describing which percentile is required, e.g., 50, 75, 90, 95). Sometimes the
score report is generated immediately by the presentation process; sometimes
the values of the statistics are stored in a database with the examinee record
so the score reports can be generated later on demand.
As mentioned earlier the scores are always some kind of statistic of the
scoring model. In the cases of a Bayesian network scoring model, this might
be the marginal distribution of one of the nodes. In the case of the number
right scoring model, it might be the percentage of tasks answered correctly.
More complex kinds of reporting might require additional messages. Con-
sider the case of market basket reporting where the goal is to predict how the
examinee would perform on a standard set of tasks (the market basket). This
is fairly straightforward with a Bayesian scoring model. The scoring model
contains the posterior information about the student’s proﬁciencies. A link
is required for each task in the market basket; that link gives the likelihood
of any pattern of observables given a proﬁciency proﬁle. This can be used to
provide a probability distributions over possible observable patterns for each
of the market basket tasks.
The desired information to be placed in the score report should be decided
early in the design process. It is good practice to reason backwards from that
information to the statistics required to generate it, and then to ensure that
proﬁciency models (and hence the scoring models) for the assessment support
it. In the Biomass score report (Fig. 14.4, Chap. 14) a count of the number
of tasks is required in addition to the information from the Bayes margins.
To obtain the task counts, an additional number right proﬁciency model was
added (actually this was just a task counter model, as it did not use a notion of
“right”). Together the two scoring models provided the information required
for each assessment.
It is worth spending a great deal of time thinking about the score report.
The score report and the actual tasks are the two parts of the assessment
that are visible to the end users. Ultimately, the assessment will be judged
on whether or not the score report provides the information needed by the

13.3 Scoring
501
end users for the purpose to which they want to put the assessment. It is
incumbent on the score designers to transform the information that resides in
the scoring models at the end of an assessment into a form in which the end
users can understand both what is known and what remains unknown about
the student.
Exercises
13.1 (Four-Process Chart Fill-in). For each of the following scenarios,
describe what the four processes are and whether they are human or computer
processes, and whether they happen at the time the examinee sits for the
assessment or afterward.
1. A student taking a college entrance examination administered with paper
booklets and scanned answer sheets.
2. A student using a practice examination from a study guide for that college
entrance examination.
3. A student reviewing the practice book with a tutor.
4. A student using an online study system for this assessment that simulates
the real assessment.
5. A student using an online study system for this assessment that drills the
student on certain item types.
6. A student using an adaptive online study system for this assessment.
13.2 (IRT CAT). Section 6.1 introduced a discrete Bayes net approxima-
tion of an IRT model. Design a CAT item selection procedure, beginning with
Item 3: Use Expected Weight of Evidence to determine the next most infor-
mative item to administer if a correct response to Item 3 is observed, and if an
incorrect response is observed. Determine the next item to administer among
the remaining ones if the second response is correct, and if it is incorrect, and
so on. Repeat the exercise using mutual information to select items. Compare
the results.
13.3 (Raw Mouse Clicks vs. Higher Level Events). A certain testing
program is using computer-administered multiple choice tasks where the com-
puter places circles next to the options. For each of the following parts of the
software, say whether it logically belongs in the presentation or evidence iden-
tiﬁcation process. Justify your answers.
1. A subroutine which maps the location of the mouse click (e.g., screen
coordinate (104, 121)) to which option was selected (e.g., A, B, C, D, or E)
2. A subroutine which compares the selection made by the examinee to the
key and returns a value of correct or incorrect according to whether
or not it matches.

502
13 The Evidence Accumulation Process
13.4 (ROC Calculation). A certain vocabulary test designed for 8th grade
students has a task that presents a certain word and asks the examinee to
write down as many related words as possible within 3 min. To try to ﬁgure
out how many words to expect, the design team chose a group of experts
(college educated adults) and a group of novices (6th grade students) and
gave each of them the task. They tested 15 of each. The numbers are given in
Table 13.9. Calculate an ROC curve for distinguishing between experts and
novices on this task.
Table 13.9 Data for Exercise 13.4
Experts 24, 27, 17, 31, 22, 16, 19, 17,
28, 28, 21, 14, 31, 26, 33
Novices
8,
9,
7,
8,
7, 10,
7,
6,
7,
7, 10,
7, 14,
7, 17
13.5 (Constructing an Expected Accuracy Matrix). Table 13.10 shows
the estimated scores and observed outcomes for an item called Exp1.1, for 10
randomly selected students in a (simulated) pretest. Construct the expected
accuracy and MAP accuracy matrixes for these data. Is the diﬀerence in the
normalized values bigger or smaller than that between the normalized version
of Tables 13.3 and 13.4?
Table 13.10 Ten randomly selected entries from a set of pretest data
ID
EAP MAP P(θ = high) P(θ = med) P(θ = low) Exp1.1
Simulee7
0.46 Low
0.01
0.43
0.56
1
Simulee378 0.46 Low
0.01
0.43
0.56
0
Simulee1
1.70 High
0.71
0.27
0.01
1
Simulee441 0.61 Med
0.07
0.47
0.46
0
Simulee64
0.45 Low
0.07
0.32
0.62
1
Simulee183 0.61 Med
0.07
0.47
0.46
1
Simulee40
0.66 Med
0.04
0.58
0.38
1
Simulee333 0.58 Med
0.03
0.52
0.45
1
Simulee422 0.29 Low
0.01
0.28
0.72
0
Simulee487 1.78 High
0.79
0.20
0.01
0
13.6 (Mutual Information). Calculate the mutual information between the
target skill and the observable for the two tasks in Table 13.11. The pretest
form had six reference tasks, which had mutual information of 0.0048, 0.0266,
0.0121, 0.0492, 0.0317, and 0.0697. Is one of them abnormally low? If so, what
is the problem? [Hint: Look at normalized tables.]

13.3 Scoring
503
Table 13.11 Expected accuracy matrix for two experimental tasks
Skill 1
Skill 1
Low
Med High
Low
Med High
Exp1.1
1
69.78 107.57 74.61
Exp2.1
1
60.04
72.07 41.86
0 117.96
93.23 36.76
0 127.71 128.73 69.50
13.7 (Distractor Analysis). A multiple-choice task consists of three kinds
of presentation material: the stem, or the initial question, the key or the correct
answer, and the distractors or incorrect answers. A problem that is frequently
observed with multiple-choice tasks (especially vocabulary tests) is that one of
the distractors will not be understood except by people with high ability, and
hence the task will provide weak information. Look at the expected accuracy
matrices for the two experimental tasks in Table 13.12. Does one of them
exhibit this problem? If so, which one?
Table 13.12 Expected accuracy matrix (normalized) for two multiple-choice tasks
Skill 1
Skill 1
Low Med High
Low Med High
Exp7.1
d 0.19 0.17 0.07
Exp8.1
d 0.06 0.10 0.26
c 0.25 0.27 0.33
ca 0.36 0.44 0.43
ba 0.30 0.39 0.44
b 0.44 0.38 0.28
a 0.26 0.18 0.17
a 0.14 0.09 0.03
The key is marked with an “a.”
13.8 (Diﬀerential Task Functioning Detection). To look for possible
diﬀerences in the way male and female students approached a given item,
the design team used a pretest sample of 50 male and 50 female students to
produce the table shown in Table 13.13. Do these data indicate that there is
cause for concern?
Table 13.13 Data for diﬀerential task functioning detection problem (Exercise 13.8)
Male
Female
Low Med High
Low Med High
Incorrect 7.15
6.21
3.64 Incorrect 7.04
8.22
4.74
Correct
2.72 12.58 17.69 Correct
3.11 11.22 16.66

504
13 The Evidence Accumulation Process
13.9 (Conditional Independence Test). The design team has some con-
cern that the two observables X and Y are dependent even though they are
from diﬀerent tasks. To test this hypothesis they produce the three-way table
shown in Table 13.14 using the MAP value for the proﬁciency. On the basis
of these data, is there cause for concern?
Table 13.14 Data for conditional independence test problem (Exercise 13.9).
Skill=low
Skill=med
Skill=high
0
1
2
0
1
2
0
1
2
0 99 24
31
0 35 24
9
0 0
0
2
1 34
1
7
1 51 58
25
1 8 14
33
2
2
1
0
2
3
6
2
2 3 12
16
13.10 (Expected Weight of Evidence). Continuing from Example 13.1,
assume that we present the student with Visual Task and the student gets a
correct result. Table 13.15 gives the conditional probabilities after this new evi-
dence is absorbed. Calculate the expected weight of evidence for the remaining
tasks. Which is the best task to present next?
Table 13.15 Calculation of expected weight of evidence after one observation
H = Solve Geometric Problems ≥medium
Task
P(X = 1|H) P(X = 1|H)
EWOE
Common Ratio Task
0.72
0.62
Explicit Rule Task
0.29
0.28
Recursive Rule Task
0.49
0.46
Verbal Rule Task
0.45
0.31
Table Task
0.56
0.36
Visual Task
1.00
0.00
– –
Numbers based on the Bayesian network model for ACED (Shute et al. 2008; Exam-
ple 7.5), conditioned on a correct result from Visual Task
13.11 (Repeated Tasks with Context Eﬀect). A certain low-stakes
assessment consists of a number of extended tasks which the students can
attempt over the course of several weeks. The students are allowed to make
multiple attempts at the tasks on diﬀerent days, and the design team decides
to treat these as independent pieces of evidence. Several of the tasks require
a fair amount of background reading, and it is thought that students who
have previously studied the topic will have some advantage. To model this,
the evidence models for those tasks has a local variable called Context.

13.3 Scoring
505
The question is, how should this variable be treated on repeated attempts
at the task? Should there be diﬀerent instances of Context local to each link?
Or should Context be shared across separate instances? In either case, how
does the model need to be adjusted to take this into account?

14
Biomass: An Assessment of Science Standards
This chapter and the following one illustrate the use of evidence-centered
design (ECD) and Bayes nets to build the assessment Biomass, a prototype
of an interactive, inquiry-based assessment of secondary biology (Steinberg et
al. 2003). Section 14.1 provides a background for the project. Section 14.2 sum-
marizes domain analysis and domain modeling activities that supported the
design of assessment objects and delivery processes, and Sect. 14.3 describes
the resulting Conceptual Assessment Framework. Section 14.4 describes the
four-process delivery system used in the prototype. The following chapter pro-
vides numerical details about the models and our eﬀorts to reﬁne the models
from data.
14.1 Design Goals
A primary purpose of the Biomass project was to provide a test-bed for the
ECD methodology then under development. As such, we were trying to design
an assessment which would meet several goals:
•
Provide meaningful feedback (both task-level and summary feedback)
based on standards for the domain.
•
Demonstrate ECD modularity and support for repurposing the assessment
by building two variants that support diﬀerent purposes: (a) formative
assessment for classroom use, and (b) a culminating assessment that pro-
vides evidence of whether or not standards have been met at the end of
the unit.
•
Take advantage of web-based infrastructure to support complex, interac-
tive, automatically-scored tasks.
•
Demonstrate the ability of ECD to disentangle evidence from complex,
integrated tasks that tap more than one proﬁciency.
The result was Biomass, a web-delivered, interactive assessment that can
be used in two ways: as a formative assessment that supports learning in a
c⃝Springer Science+Business Media New York 2015
507
R. G. Almond et al., Bayesian Networks in Educational Assessment,
Statistics for Social and Behavioral Sciences, DOI 10.1007/978-1-4939-2125-6 14

508
14 Biomass: An Assessment of Science Standards
standards-based curriculum, and as a culminating assessment that provides
evidence of whether standards have been met, for purposes such as college
admissions or course placement.
When used as a culminating assessment, Biomass reports on students’
learning in terms of standards in the domain, and also provides supplemental
information for further study. To familiarize students with the forms and
conventions of the culminating test as well as the content and expectations,
there is a formative version that can be used for coached practice. When used
as a formative assessment, Biomass supports practice and self-evaluation. In
this use it informs students and teachers about progress toward mastery. It
addresses the same knowledge base and skills as the culminating assessment,
but it would be used by students working individually or together, often as
part of a course, to practice and to prepare for the culminating assessment.
The feedback in the self-evaluation use is more detailed than feedback from
the culminating assessment.
The Biomass assessment is intended to be “standards-based.” Despite all
the activity surrounding standards, a gap remains between published lists
of standards and sound systems for assessing students in terms of those
standards. Standards descriptions often reﬂect inconsistent mixtures of what
knowledge is valued, how it can be recognized, and activities for eliciting
evidence. They are typically represented as discrete pieces of hierarchically
organized text that do not reﬂect the integrated nature of the knowledge
and skill they are meant to foster. A recent exception is the Next Genera-
tion Science Standards (NGSS; NGSS Lead States 2013). Its “performance
expectations” are activities for learning or assessment that integrate content
knowledge, scientiﬁc processes, and crosscutting themes such as cause-and-
eﬀect. Biomass lays out ECD models for generating such tasks, and making
sense of the performances they evoke. The particular standards Biomass used
are from a precursor of the NGSS, namely the National Science Education
Standards (NRC 1996). Table 14.1 contains excerpts that illustrate crosscut-
ting themes, scientiﬁc processes (with an emphasis on inquiry), and some of
the life sciences topics addressed there.
The evidence-centered approach to standards-based assessment illustrated
in Biomass moves from statements of standards in a content area, through
claims about students’ capabilities that the standards imply, to the kinds
of evidence one would need to justify those claims, to the development of
assessment activities that elicit such evidence, and ﬁnally to measurement
models for synthesizing evidence from students’ work in terms of standards-
based claims. Rather than thinking at the level of individual tasks, we see
tasks as instances of prototypical ways of getting evidence about aspects of
knowledge that the standards bring to light. The ECD approach helps us
recognize aspects of knowledge that are similar across content areas and skill
levels, and craft schemas for obtaining evidence about such knowledge as it
specializes to diﬀerent particulars.

14.1 Design Goals
509
Table 14.1 A hierarchical textual representation of science standards
Unifying concepts and processes of science (All grades)
Systems, order, and organization
Evidence, models, and explanation
Constancy, change, and measurement
Evolution and equilibrium
Form and function
Science as inquiry (Grades 9–12)
Abilities necessary to do scientiﬁc inquiry
Identify questions and concepts that guide scientiﬁc investigation
Design and conduct scientiﬁc investigation
Use technology and mathematics to improve investigation and communication
Form and revise scientiﬁc explanations and models using logic and evidence
Recognize and analyze alternative explanations and models
Communicate and defend scientiﬁc argument
Understandings about scientiﬁc inquiry
Life Science (Grades 9–12)
Molecular basis of heredity
Theories/models
Chromosome theory of inheritance
Chromosome mapping
Base-pair complementarity
Chi-square model
Punnett squares
Excerpts from the National Science Education Standards (NRC 1996)
Science is a good domain to illustrate this approach. Science standards
(e.g., Table 14.1) typically reﬂect both domain knowledge about the facts
and theories of science and process knowledge about the scientiﬁc method.
It is generally easier to test science facts (e.g., How many planets are there
in the solar system?1) than the process knowledge (e.g., Design an exper-
iment to provide a good test of this hypothesis). Building tasks that tackle
the higher-order knowledge presents two challenges. First, ideally the students
will be using this knowledge constructively; this means that the assessment
system will need to extract evidence about the targeted knowledge from com-
plex work products. Second, it is diﬃcult to design a problem in which the
process knowledge is used in a meaningful way without reference to domain
knowledge; the assessment system must be able to untangle the various kinds
1 Science facts also have a tendency to go out of date as our understanding of
the world changes, e.g., the recent reclassiﬁcation of Pluto from planet to dwarf
planet. However, the process knowledge changes more slowly. Model-based rea-
soning and investigation will be important for a long time.

510
14 Biomass: An Assessment of Science Standards
of evidence about various mixes of content knowledge, processes capabilities,
and understanding of crosscutting or unifying themes.
14.2 Designing Biomass
To promote reuse and encourage freer thinking about the domain, ECD
divides the assessment design process into three stages. The ﬁrst stage,
Domain Analysis, consists mainly of gathering and organizing extant knowl-
edge about the domain (e.g., standards, cognitive models, previous assess-
ments, and other key literature) and requirements for the assessment. The
second stage, Domain Modeling, involves building a preliminary, less-detailed
version of the conceptual assessment framework (CAF) that embodies the
assessment argument but is not yet limited by the practical constraints of
administering the test. It is often possible to work out important design issues
with the lighter-weight models before the expense of building and calibrating
tasks is incurred.
In typical projects, there is usually some working back and forth across
these stages, such as what we learn in piloting—a light application of assess-
ment delivery—can tell us where we need to revise models in the CAF, sharpen
arguments in domain modeling, or gather more information, say in think-
alouds, to ﬂesh out areas in domain analysis that take on new importance.
The ECD layers are not rigid steps in some waterfall development process,
but rather a guide to distinct kinds of thinking that needs to be carried out
to create an assessment, representations that are useful at diﬀerent points,
and connections across the design stages and diﬀerent experts’ roles. This
section, then, describes the key results of the domain analysis and modeling
for Biomass, and the following section describes the CAF.
The ﬁrst steps in designing Biomass were to choose a subject matter
domain and convene a panel of domain experts. Data gathering then proceeded
with the substantive foundation: selecting pertinent educational standards,
choosing illustrative topics within the subject matter domain, and deﬁning
relationships between standards and subject matter topic content. The focus
then shifted to the claims we would want to make about students as a result
of their performance on assessment tasks, what we would need to observe
as evidence to support those claims, and the nature of assessment activities
that would provide students the opportunity to produce that evidence—all in
light of the aﬀordances and constraints of an appropriate mode of assessment
delivery.
14.2.1 Reconceiving Standards
Biology was chosen as the subject matter domain because several compre-
hensive sets of science standards had been developed (e.g., NRC 1996; AAAS
1994, which are consistent with, but not as fully integrated as, NGSS). Because

14.2 Designing Biomass
511
these standards also reﬂect common themes that cut across all science sub-
jects, biology provided a good context to demonstrate reusability and scala-
bility of assessment design elements. The expert panel next identiﬁed a set of
crosscutting themes to focus on, and then picked biology topics that would
best illustrate them. The chosen themes were unifying concepts and scien-
tiﬁc inquiry (Table 14.1). According to the NSES standards, these themes
apply broadly across domains of science. They would provide a way for us
to illustrate a task design approach that could be applied in many domains,
using argument structures that would be instantiated with the models and
problems of particular content areas. In Biomass, we focused on the way bio-
logical phenomena are studied across diﬀerent levels of organization (e.g., the
molecular, cellular, organism, and population levels) and the use of models
and evidence to reason about and explain biological phenomena. The partic-
ular content topics chosen in which to illustrate these themes in assessment
design were transmission genetics and microevolution. The experts developed
detailed concept maps for these subdomains (see Fig. 14.1 for an example).
Evolution 
allelic 
variation in 
population 
population 
processes 
Microevolution 
(populations)
Macroevolution
(speciation) 
natural 
selection 
mutation 
sexual life 
cycle model 
genetic drift
lateral gene 
transfer 
meiotic 
drive 
gene 
imprinting 
allele pool 
phenotypic 
variation 
species
continuous 
variation 
discontinuo
us variation 
adaptation 
differential 
reproduction 
fitness 
sexual 
selection 
gametic 
selection 
biotic 
abiotic 
environment 
random
to transmission genetics 
human-
mediated 
(“artificial”) 
selection 
sampling 
error 
transmission 
processes 
non-random
genotypic 
variation 
non-random 
(structured) 
mating 
gene flow 
constrained
adaptation 
selection
neutral 
evolution 
population 
Fig. 14.1 A concept map for mechanisms of evolution
Reprinted with permission from ETS.

512
14 Biomass: An Assessment of Science Standards
The expert panel suggested Fig. 14.2 as a way to show the relationships
among unifying themes and science content in terms of the capabilities we
want students to acquire.
Working
Knowledge
Integrated
Knowledge
Unifying
Concepts
Unified
Knowledge
Science
as
Inquiry
Unifying
Concepts
.
Disciplinary Knowledge--
Definitions, Concepts,
Models,
Relationships
Science
as
Inquiry
Science
as
Inquiry
Fig. 14.2 Representation of a standards-based domain for assessment design
Planes represent classes of claims, while circles, gears, and clouds represent indi-
vidual claims about Domain, Working, and Integrated Knowledge. Reprinted from
Steinberg et al. (2003) with permission from The National Center for Research on
Evaluation, Standards, and Student Testing (CRESST), UCLA.
At the left of Fig. 14.2 is Disciplinary, or Declarative, Knowledge: the
deﬁnitions, models, and relationships in transmission genetics and microevo-
lution. Two alternative elaborations of Disciplinary Knowledge appear to its
right. Along the top, Disciplinary Knowledge is extended by understanding
how to use it as the substantive grounding of inquiry, to produce what we will
call Working Knowledge: the capability to use deﬁnitions, concepts, models,
and relationships in inquiry, as when explaining a particular phenomenon in
terms of underlying models or investigating the plausibility of an explanatory
model. The bottom of the ﬁgure shows Disciplinary Knowledge extended by
understanding how it relates to a particular unifying concept or process. For
example, seeing cells through the unifying concept of form and function helps
one understand how cellular structures facilitate cellular processes. The unify-
ing concepts add structure and explanatory power to the myriad elements of
disciplinary knowledge. The right side of Fig 14.2 portrays Integrated Knowl-
edge, through which one can use models, evidence, and explanations from

14.2 Designing Biomass
513
diﬀerent topic areas and diﬀerent levels of organization to address increas-
ingly broader applied problems.
14.2.2 Deﬁning Claims
Claims specify what one would want to be able to say about a student as
the consequence of assessment. In Biomass, a claim addresses Disciplinary
Knowledge, Working Knowledge, or Integrated Knowledge.
A large number of claims about Disciplinary Knowledge can be devel-
oped from instructional materials and concept maps in transmission genetics
and microevolution. They address terms, concepts, and knowledge represen-
tations. Claims concerning Uniﬁed Knowledge address relationships among
terms, concepts, and knowledge representations across levels or content areas.
Claims can be cast at diﬀerent levels of speciﬁcity. Speciﬁc claims are use-
ful for guiding learning, while more encompassing claims that subsume more
detailed ones can be sampled broadly for summative evaluation. As examples,
the following are a Disciplinary Knowledge claim and a more speciﬁc claim
that it encompasses:
C1: The student understands the entities, events, and outcomes constituting
the Mendelian model.
C2: The student can reexpress a verbal description of dominance relationship
in terms of allele2 notation.
Claims about Working Knowledge involve Disciplinary Knowledge put to
work in explaining situations, making predictions, or solving problems. An
example of a Working Knowledge claim is
C3: Given complete data, the student can use the data to evaluate a hypothesis
about a situation involving a population-level model across time.
Claims about Integrated Knowledge also involve Disciplinary Knowledge
as it is put to work in explaining situations or solving problems, but addition-
ally involve connections across diﬀerent levels or content areas; for example,
C4: The student can reason through the Mendelian, sexual life cycle, natural
selection, and genetic drift models for prediction in a situation involv-
ing the cellular, organism, and/or population level(s) across transmission
genetics and mechanisms of evolution and across time.
2 An allele is a short stretch of DNA on a particular location on a chromosome
that (along with its pair on the matched chromosome) controls the expression
of a particular genetic trait. Allele notation uses letters to represent the various
alleles and a pair of letters to represent a genotype. For Mendel’s pea experiment,
the letter “T” could be used to represent the allele for tall plants, and the letter
“t” could be used for the allele for short plants. In this conﬁguration, the possible
genotypes would be “TT”, “Tt,” and “tt.” Because the tall allele is dominant,
the ﬁrst two result in tall phenotypes and the third results in shorter plants.

514
14 Biomass: An Assessment of Science Standards
As the expert panel saw it, Working Knowledge and Integrated Knowl-
edge always entail some Disciplinary Knowledge. Inquiry is always inquiry
about something, and model-based reasoning is always carried out with some
particular model(s).
14.2.3 Deﬁning Evidence
Once we had established what we wanted to assess in the form of claims,
we set about deﬁning evidence that would be needed to support them. Our
attention focused on Working and Integrated Knowledge, because methods
for assessing Disciplinary Knowledge are familiar in the content areas being
addressed. Methods for assessing inquiry are less familiar, and the computer-
based platform oﬀered interesting possibilities for obtaining direct evidence
about inquiry processes. Evidence in the form of potential observations related
to each claim was ﬁrst considered independent of speciﬁc tasks. As an example,
Table 14.2 lists some of the observations the experts thought would support
the claim that a student could design and conduct a scientiﬁc investigation in
a given area, based on research by Stewart and Hafner (1994), for example,
on model-based reasoning and White and Frederiksen (1998) on inquiry.3
Table 14.2 Potential observations related to scientiﬁc investigation
Recognition of need to obtain additional data
Eﬃcacious speciﬁcation of appropriate methodology(s) for gathering data
Adequacy of model testing
Eﬃcacious speciﬁcation of methodology(s) for testing model
Identiﬁcation of outcomes of model testing that bear on current hypothesis (i.e.,
conﬁrming/disconﬁrming evidence)
Association of anomalous data with relevant aspect(s) of relevant model(s)
Impasse speciﬁed in terms of data/model mismatch
Accuracy of model revisions
Three features of the observations listed in Table 14.2 are worth mention-
ing. First, they are cast broadly enough to apply to inquiry processes carried
out in many scientiﬁc domains and across educational levels. They can there-
fore be used to guide task design beyond the content areas Biomass addresses.
3 Follow-on work from Biomass led to a National Science Foundation-supported
project called Principled Assessment Design for Inquiry (PADI) (Mislevy and
Haertel 2006). The PADI project created “design patterns” that organize kinds
of claims, observations, work products, and task features for assessing students’
capabilities with science processes such as model-based reasoning and systems
thinking (Cheng et al. 2010; Mislevy et al. 2009).

14.3 The Biomass Conceptual Assessment Framework
515
Second, because the observations can be applied across content areas, any
speciﬁc instantiation will need to further specify the Disciplinary Knowledge
and inquiry techniques that are involved. A task providing evidence about
inquiry will necessarily depend on the required disciplinary knowledge. In
Sect. 14.3 we will see implications for building the proﬁciency and evidence
models and interpreting the proﬁciency variables.
Third, these observations focus on the nature of the thinking they reveal,
not on the speciﬁc form of data. One can go a long way in deﬁning evi-
dence before specifying exactly how to get it. Thinking about the relationship
between what we want to observe and the way knowledge is manifested before
specifying a particular type of task opens thinking about what can and should
be considered as evidence, and ways in which one might acquire it. Evidence
of a particular aspect of knowledge or kind of thinking can usually be obtained
in many ways, such as mutliple-choice questions, constructive exercises, open-
ended verbal explanations, hands-on laboratory work, or any number of other
methods. Each approach has its own costs and beneﬁts, its own advantages
and disadvantages. Determining what to use in a given assessment context
depends on the particular constraints, resources, and purposes for that assess-
ment. The form of the data is secondary to its evidentiary import.
14.3 The Biomass Conceptual Assessment Framework
The Biomass Conceptual Assessment Framework builds on the design ratio-
nale described above to provide blueprints for the operational elements of the
assessment, namely the proﬁciency model, task models, and evidence models
(Chaps. 2 and 12). We focus now on the way Bayes nets were constructed, to
model students in terms of variables that reﬂect standards-based claims.
14.3.1 The Proﬁciency Model
Figure 14.3 shows the Biomass Proﬁciency Model. The full model contains 15
variables. Each node represents an aspect of knowledge or skill about which we
want to accumulate evidence. These variables are derived from the conceptual
representation of knowledge in the domain represented in Fig. 14.2. There are
nodes that concern disciplinary knowledge, working knowledge, and integrated
knowledge. Each of the claims the expert panel articulated is associated with
one or more of these variables.
When the numbers are added to this graph (next chapter) the proﬁciency
model deﬁnes a probability distribution over the possible proﬁciency proﬁles—
assignments of values to each proﬁciency variable. This probability distribu-
tion represents the population distribution of proﬁciency proﬁles in the tar-
geted population. Using the algorithms discussed in Chaps. 5 and 13, this
probability distribution will be updated for speciﬁc students using the evi-
dence accumulated in the assessment, yielding a distribution over proﬁciency
proﬁles speciﬁc to that student.

516
14 Biomass: An Assessment of Science Standards
DK
WK
IK
IKSysOrg
IKModEvd
DKMechEv
DKNatSel
DKDrift
DKTrnGen
DKSexCyc
DKMendel
WKInqiry
WKModExp
WKModUse
WKModRev
Fig. 14.3 The biomass proﬁciency model
Reprinted with permission from ETS.
Each possible proﬁciency proﬁle has a number of claims which it does and
does not support. In a standards-based assessment many claims correspond
to one or more standards being met, thus the evidence gathered to infer the
proﬁciency proﬁle of a student can be used to infer the claims supported and
hence the standards that are met.
Whether or not a claim is supported may depend on the value of sev-
eral proﬁciency variables; however, many claims are supported by just one
of the proﬁciency variables. For example, Claim C1 states that “The student
understands the entities, events, and outcomes constituting the Mendelian
model.” Claim C1 is associated with the proﬁciency variable DKMendel, or
Disciplinary Knowledge about the Mendelian Model. Like all the proﬁciency
variables in Biomass, DKMendel can take three ordered values, high, medium,
and low. They are interpreted as claims that a student has High, Medium,
or Low proﬁciency respectively for the competence represented at the highest
level by the associated standard.
Note that the eﬀective meaning of high, medium, and low proﬁciency for
DKMendel accrues from the kinds of evidence that has been deﬁned for that
purpose; that is, the space of tasks and observations that can be generated
from the analysis of potential evidence as described in Sect. 14.2.3.
(A development that has occurred since the Biomass project is the use
of learning progressions (Alonzo and Gotwals 2012; Corcoran et al. 2009),
as discussed in Sect. 12.2.1. A learning progression is a sequence of increas-
ingly sophisticated understandings or capabilities that learners typically move
through in some domain area. They are often marked by the kinds of things
learners can do in situations with various features. This conception is neatly
matched to ECD assessment design. In simple cases, levels of a learning pro-
gression correspond directly to values of an ordered-state variable in a Bayes

14.3 The Biomass Conceptual Assessment Framework
517
net proﬁciency model with theory-deﬁned meanings, and the features of per-
formances and tasks correspond to evidence-model and task-model variables
(West et al. 2012; Zalles et al. 2010). In more complicated cases, the levels
of a more coarsely deﬁned learning progression are conﬁgurations of values of
ﬁner-grained proﬁciency variables (West et al. 2012; Wilson 2009).)
When multiple aspects of skill or knowledge are required in combination, a
claim can be modeled as depending on the values of more than one proﬁciency
variable. The compensatory, conjunctive, and disjunctive relationships among
proﬁciency variables described in Chaps. 6 and 8 are prototypical patterns for
the relationships between claims and proﬁciency variables. Section 14.3.4 illus-
trates how these design patterns were used in Biomass to model performance
in tasks that depend jointly on disciplinary and inquiry aspects of knowledge.
An example of a claim that requires a conjunction of proﬁciencies is this:
C5: Given complete data, the student can use the data to evaluate a hypothesis
about a situation involving a population-level model across time.
Claim C5 is associated with both Disciplinary Knowledge about Mech-
anisms of Evolution (DKMechEv) and Integrated Knowledge about Models
and Evidence (IKModEvd). Support for C5 is represented as the conjunctive
combination of these two proﬁciency variables, that is, high values on both.
The Biomass proﬁciency model is rather sparse, in terms of the number
of variables it contains. For example, Claim C2 concerning allele represen-
tation is deﬁned at a ﬁner grain size than the variables in this proﬁciency
model. It was used to guide the deﬁnition of evidence and construction of
tasks; the evidence aﬀorded by such tasks was one portion of the body of evi-
dence bearing on DKMendel, the proﬁciency variable that corresponds to the
broader Claim C1 that encompasses C2. The basic structure of the proﬁciency
model could be elaborated with additional proﬁciency variables at ﬁner grain
sizes or for additional topics in a manner discussed below. However, those
additional variables will require evidence (observables from multiple tasks) to
support them. Even for an assessment intended to be embedded in normal
classroom activity, the amount of time that can be spent gathering evidence
is limited. Thus, assessment design always requires trading-oﬀgrain size in
the proﬁciency model and time spent gathering evidence.
The Biomass proﬁciency model shows a tripartite hierarchical structure
of Disciplinary Knowledge, Working Knowledge, and Integrated Knowledge.
The three corresponding variables at the highest level of the hierarchy are
abbreviated DK, WK, and IK respectively. The probability distribution for a
variable at this highest level summarizes evidence across all the aspects of the
given kind of knowledge addressed in the Biomass, corresponding to broadly
cast claims. This tripartite structure is common to many branches of science
(either diﬀerent subjects within biology, such as botany or cell biology, or
diﬀerent sciences such as physics or psychology). The model structure supports
ready reuse at this general level.

518
14 Biomass: An Assessment of Science Standards
Finer-grained proﬁciency variables can be added to the basic model struc-
ture as children of any higher level proﬁciency variables. Here DK has two
children, for the subareas addressed in the prototype: transmission genetics
(DKTrnGen) and mechanisms of microevolution (DKMechEv). If Biomass
were extended to additional areas of a biology course, additional disciplinary
knowledge proﬁciency variables would be added at this level as children of
DK. DKTrnGen itself has two children, concerning the sexual cycle (DKSex-
Cyc) and the Mendelian model (DKMendel). DKMechEv similarly has two
children, namely genetic drift (DKDrift) and natural selection (DKNatSel).
Working Knowledge (WK) also has children that represent the subareas
of inquiry (WKInqry) and models and explanation (WKModExp), and models
and explanation itself has two children concerning model use (WKModUse)
and model revision (WKModRev). Integrated knowledge (IK) has two chil-
dren, concerning systems and organization (IKSysOrg) and models and evi-
dence (IKModEv).
The choice of grain size depends on the intended use of an assessment, since
ﬁner-grained proﬁciency variables are needed to support ﬁner-grained claims.
In the use of Biomass as a culminating assessment, for example, reports are
provided at the level exempliﬁed by DKTrnGen. In its use as a learning assess-
ment, feedback is provided at the level exempliﬁed by DKSexCyc and DKNat-
Sel. This is because grain size trades oﬀwith accuracy. A student’s proﬁcien-
cies at higher levels in the hierarchy are based on more evidence—i.e., more
task performances and resulting observable variables—than are proﬁciencies
in subdomains. The same phenomenon applies more generally to subscores in
assessments, whenever they are based on only portions of the evidence that
go into overall scores.
Figure 14.3 shows children at each level of the tree as conditionally inde-
pendent given their parents, the higher-level proﬁciencies. With this structure,
additional subtopics can be added within any level of the tree without aﬀecting
the structure of the network elsewhere. The form of the conditional distribu-
tions among proﬁciency variables and parameters for these distributions will
be detailed in Sect. 14.4.
As noted above, both integrated knowledge and working knowledge are
conceived as something a student knows or can do with particular disciplinary
knowledge. Section 14.3.4 will detail how this relationship is eﬀected in evi-
dence models to model performance. To anticipate, relevant aspects of both
disciplinary knowledge and, say, working knowledge are required conjunctively
for good performance. This modeling choice gives rise to a conditional inter-
pretation of working knowledge: To be high on WK means that a student can
carry out inquiry if she also possesses requisite levels of disciplinary knowledge
for the situation at hand.
The interpretation of IK is also conditional. To be high on IK means a
student is likely to do well reasoning through models at diﬀerent scales or from
diﬀerent areas if she also has the requisite levels of disciplinary knowledge in
those models as well.

14.3 The Biomass Conceptual Assessment Framework
519
In the Bayesian framework, the proﬁciency model (and student-speciﬁc
scoring model) will contain a probability distribution over all of the possible
proﬁciency proﬁles. The ﬁnal step in deﬁning the proﬁciency model is deciding
on the summary statistics of that posterior distribution that will be reported
on the ﬁnal score report. Biomass used a fairly conventional choice of report-
ing the marginal distributions of all of the proﬁciency variables, providing the
probabilities that a given subject is at or above the Advanced (high) and
Basic(medium) levels. Figure 14.4 shows a sample score report for the class-
room learning assessment. The proﬁciency variables are brieﬂy deﬁned on the
form in terms of the claims, while more complete deﬁnitions are available in
the interpretation guide. The score report for the culminating assessment is
similar, but would not have the ﬁner detailed variables. (The two assessments
actually used the same proﬁciency model, only the reporting rules were dif-
ferent. In the case of the culminating assessment only a selected subset of
variables are reported.)
The score report is typically all a test user sees of the proﬁciency model.
When seeking input from potential users about design options, having score
reports that illustrate the implications of the design choices will enable the
users to provide meaningful feedback. We have found it to be good practice
to show a prospective score report to potential users early in the development
process. This will provide valuable information about whether or not the pro-
ﬁciency variables and scores derived from them provide useful and actionable
information to the end users. (Recall the value of information calculations in
Example 4.1.)
In Biomass, the primary test user is likely to be the classroom teacher. The
teacher is not just concerned with the proﬁciency proﬁle of a single student,
but of all the students in the classroom. Fortunately, averaging the marginal
distributions across students provides a good description of the class average.
Almond et al. (2009a) explores a number of ways of presenting information
from Bayesian networks for a class full of students.
14.3.2 The Assembly Model
An Assembly Model speciﬁes the rationale by which tasks are combined into
an assessment in a ﬁxed test or the algorithm by which they are sequentially
selected in an adaptive test. The considerations that enter the speciﬁcation
include content coverage, time constraints, and amount of information about
particular proﬁciency variables (Sect. 7.4). There are actually two assembly
models for the Biomass prototype, one for the learning mode and the other for
culminating assessment. In both cases, the desire to assess Working Knowl-
edge in the form of investigations imposed the need to present segments of
investigations consisting of multiple steps and providing multiple observable
variables.
For the learning mode, the goal of presenting feedback in terms of proﬁ-
ciency estimates at a more detailed grain size than the culminating assessment,

520
14 Biomass: An Assessment of Science Standards
Fig. 14.4 Biomass: a sample score report from the Biomass classroom assessment
Reprinted with permission from ETS.

14.3 The Biomass Conceptual Assessment Framework
521
coupled with the requirement to be able to use the investigations freely over
several class periods if desired, led to the full sequence of 17 segments of an
investigation. This meant that multiple observable variables were provided not
only for higher-level nodes such as DK, WK, and IK, but also for the most
detailed nodes such as WKModUse and model revision WKModRev. Infor-
mation is obtained from at least ﬁve observable variables for each reported
proﬁciency. In conjunction with task-level textual feedback, this ﬁne level of
detail helps the student go back over, and repeat if desired, parts of tasks, or
study those aspects of knowledge outside the task.
For the culminating mode, testing time is a more pressing constraint. Two
class periods, the time typical for a ﬁnal examination or an Advanced Place-
ment test, does not permit the full experience of extended investigation. The
culminating tasks thus focus on only segments of an investigation, as noted
above. Model revision may be assessed by providing a model that does not
accord with data in some way, rather than having the student work through
repeated cycles of proposing, testing, and revising models. Further, because
less information can be gathered, the selection of tasks for the culminating
assessment spans the domain areas more broadly but less deeply. Samples of
aspects of knowledge involved in DK, WK, and IK are obtained in a balance
across topics and aspects of integrated and working knowledge. Higher-level
variables in the proﬁciency model are used as parents in the Bayes net for
the culminating test, so reports are at a coarser grain size. Again at least ﬁve
observables support every variable reported, but they are now sampled across
a broader spread of the content area.
A signiﬁcant challenge in Biomass is to represent all the steps in the scien-
tiﬁc method in a coherent fashion. This was not possible in the limited time
allowed to the culminating assessment, but the learning assessment mode
could spend some additional time to tell a more complete story. In Biomass,
the tasks were grouped into two extended scenarios: an investigation of trans-
mission genetics using a population of ﬁeld mice (code named “mice”), and
an investigation of microevolution and genetics using a population of lizards
(code named “lizard”).
Each scenario was broken up into a number of segments based on steps of
the investigation. For example, the mice scenario had the segments shown in
Table 14.3. The lizard scenario had 11 segments. Both scenarios had a special
“Segment 0” that provided background about the scenario, but did not have
any scored activities. As the classroom learning use of biomass was envisaged
to run over many days, the students were free to tackle the segments in any
order and to go back and repeat previous segments.
A key challenge in designing simulation-based assessments (and Biomass
is essentially a worked-out simulation) is what to do if a student gets badly
oﬀtrack. Making a key mistake early in the simulation means that they are
unlikely to be able to perform well in later stages of the task. Once the stu-
dent has gotten oﬀtrack, further interaction with the simulator is unlikely

522
14 Biomass: An Assessment of Science Standards
Table 14.3 Segments in the Biomass “mice” scenario
0 Scenario
1 Formalize H0
2 Select veriﬁcation method
3 Cross expectations/Punnett square
4 Do data support H0?
5 Recognize disconﬁrming data
6 Explain disconﬁrming data
7 Hint for new H0
8 Build new H0
9 Given new H0, what next?
10 Select crosses for testing new H0
11 Cross expectations/Punnett square
12 Given cross results, what next?
13 Interpret chi-squared results
14 Select ﬁnal conﬁrming crosses
15 Explain ﬁnal conﬁrming crosses
16 Interpret ﬁnal conﬁrming crosses
17 Connect genetics and cell/life cycles
to provide additional information of substantial value. In this respect, a large
simulation is likely to provide less information than a series of smaller min-
isimulations.
To get around this problem, Biomass introduced a surrogate investigator,
Jos´e. Rather than performing the experiments directly, the student’s role is to
advise Jos´e. As Jos´e always makes the right choice (sometimes after discussion,
advice, or false starts), the student is always on track at the start of the
segment. This eliminates one source of potential problem. As the student has
the option of changing their responses after seeing initial feedback from each
task, being able to jump ahead and see what Jos´e did actually provides little
scoring advantage. Besides, the scores from the learning assessment are not
appropriate for high-stakes purposes, so there is little incentive for cheating.
Figure 14.5 shows the initial scenario from the “mice” scenario. All of the
subsequent tasks depend on this initial data as well as data gathered in later
experiments. The student is free to return to earlier segments to help recall
details about earlier parts of the scenario.
During the development of Biomass, there was a vigorous discussion about
what was a “task.” The original conception used with the experts was that
each of the extended scenarios was a “task.” The implementation team found
this extended notion of task too clumsy to work with. The deﬁnition of task
they decided on was the unit of information passed around the four-process
architecture; this corresponded to the segments in Table 14.3. Thus, all four
activities from the ﬁrst segment (described in Sect. 14.3.3) were called a sin-
gle “task” in the original Biomass design. Under this choice, several evidence
models were required to score independent parts of the ﬁrst task. An alter-

14.3 The Biomass Conceptual Assessment Framework
523
As a segment of your study of genetics, your class has been given the assignment
to determine the mode of inheritance (MOI) of the gene giving rise to the various
coat colors in a small ﬁeld population of mice. The only information that your
teacher, Ms. Romano, has provided is that a single gene is responsible for this
aspect of coat color in mice. She also reminds you that mice are mammals like
humans, so females are XX and males are XY. Your sample from the initial ﬁeld
population  contains  the  following  mice:
Your friend Jos´e decides to cross mice with the same coat color in order to ﬁgure
out what’s going on with the inheritance of coat color. His results are shown in
the  table  on  the  next  page.
One male and one female agouti: These mice are
covered with black-tipped hairs that have yellow
bands on the hairs hafts, giving them a speckled
brown  appearance.
One male and one female black-tan: Ontheirbacks
these mice have black hair while on their bellies they
have  tan  hair.
One male and one female agouti-tan: On their
backs these mice have black-tipped hairs with yel-
low bands on the shafts, while on their bellies they
have tan hairs, so they appear to have speckled
brown  backs  and  tan  bellies.
Fig. 14.5 Biomass: the introductory screen
Reprinted with permission from ETS.
native we could have chosen would be to designate as a “task” an activity
that produces a group of work products that are scored together. As with all
design decisions, there are advantages and disadvantages to drawing the line
in diﬀerent places, and such design decisions can be revisited for each project.
14.3.3 Task Models
Although each task model deﬁnes a collection of possible tasks, in Biomass
there was only one realization of each task model. Even so, thinking about
all of the possible tasks helps the design team identify those elements of the
task that are important for the evidence that will be collected. Furthermore,
generalizing from a speciﬁc instance to the general class is generally easier for
the design team than trying to build the task model without reference to any
speciﬁc instance. Thus, it is a common practice for the task model and ﬁrst
task from that model to be developed simultaneously.
Task models describe situations in which students will perform work that
produces evidence about their proﬁciencies. Section 14.2.2 described the kinds
of claims that Biomass targets. Section 14.2.3 described the kinds of obser-

524
14 Biomass: An Assessment of Science Standards
vations we need to make in order to ground them. Once we had deﬁned a
collection of observations at a higher level of abstraction, we had to start
developing situations that provided students the opportunity to provide such
evidence, i.e., tasks. Biomass task models lay out the stimulus materials, tools,
directives, and work products that comprise tasks, and the task model vari-
ables used to express the key features of a given task.
In assessment design, identifying the salient knowledge representations for
a given domain helps us think about how information is conveyed both to and
from the student (Gitomer and Steinberg 1999; Mislevy et al. 2010). When
information is being conveyed to a student, we think about the representations
we need to use in presenting material to the student. When information is
coming from the student, we think about representations the student can
use to create a work product. The experts thus turned their attention to the
representational forms by which information about the targeted topics would
typically be communicated within a learning environment.
In biology, speciﬁcally within transmission genetics and microevolution,
there are conventional forms for conveying information: Punnett Squares, phe-
notypic distributions, allele symbols, pedigree and chromosome diagrams, and
population tables, to name a few. These play central roles in Biomass tasks;
in particular, a fair number of tasks (including Task 1 below) called for the
student to reexpress information presented in one form in a diﬀerent form, or
to distill and interpret what she learned in more open-ended exploration in
the form of a representation actually used in the domain.
The data-driven nature of working in these areas of biology leads to tasks
that emphasize the manipulation and interpretation of data, as Working
Knowledge. The number of diﬀerent knowledge representations necessary for
conveying information and solving problems calls for working across multi-
ple knowledge representations, providing evidence for Integrated Knowledge
claims. Having students work with or create representations will allow us to
produce evaluations for observations that bear on DK through the Mendelian
Model aspect of proﬁciency, i.e., DKMendel, and instances of WK and IK
that require knowledge of the Mendelian model.
The experts also identiﬁed a number of ways to communicate about inves-
tigative methodology in the contexts of transmission genetics and microevolu-
tion. At a high level, there are the steps in the hypothetico-deductive frame-
work. At a lower level, there are the rules governing the selection of test pop-
ulations and individuals within them. Having students work with these rules
within this framework, using whichever knowledge representations are appro-
priate for presenting information and capturing work products (Table 14.4),
provides performances we can evaluate for observables such as Eﬃcacious
Methodology.
To illustrate these ideas, we show the initial screens of Biomass’s exper-
imental investigation in transmission genetics, which was written from the
point of view of the hypothetical student Jos´e. The emphasis was on use

14.3 The Biomass Conceptual Assessment Framework
525
Table 14.4 Connecting knowledge representations with investigation steps
Methodology
Associated knowledge representation
FORMULATE H0
Hypothesis expressed in standard form or alterna-
tive form
GENERATE DATA Population summary cross table;
cross choice table
ANALYZE DATA
Hypothesis expressed in standard form or alterna-
tive form;
Population summary cross table; chi-sq table
ACCEPT H0?
Hypothesis expressed in standard form or alterna-
tive form;
Population summary cross/H0 connections table
and revision of the Mendelian Model to determine the mode of inheritance
for coat color in agouti mice. The problem was constrained to a single trait
controlled by three diﬀerent forms (alleles) of a single gene. Figure 14.5 sets
the stage, by introducing the mice, the problem, and Jos´e’s initial crosses.
Figure 14.6 shows the results of the crosses and Jos´e’s hypothesis about the
mode of inheritance. Figure 14.7 asks the student to represent the hypothe-
sis in an alternative representational form. Responses to tasks that require a
student to translate a textual representation such as that of Fig. 14.6 to an
allele representation such as Fig. 14.7 are just the kind of evidence needed
to support Claim C2, hence the more encompassing Claim C1. The evidence
will eventually be reﬂected in the probability distribution for DKMendel, the
proﬁciency variable associated with C1.
The set of screens illustrated by these ﬁgures is a speciﬁc task generated
from a task model. The task model includes variables specifying the number
and nature of the representational forms to be made available to the student
for expressing whatever the hypothesis happens to be, as well as variables
specifying the elements of the hypothesis itself. For example, which represen-
tational forms will be used? Which of the four kinds of dominance relationships
will underlie the problem? A task model also describes operational task pre-
sentation requirements—in this case a Web-based drag-and-drop capability
for ﬁlling in a mode of inheritance table like the one shown in Fig. 14.7.
The highest-level attributes of a task model delineate its purpose, domain,
audience, platform, and feedback options. At the Claim level, features specify-
ing the type of knowledge, domain topics, nature, and number of models were
described. The next lower level starts to shape speciﬁc tasks more directly
by specifying the general form (e.g., scenario) in which individual activities
appear, the nature of help and guidance, the type of activity to be carried out
(e.g., ﬁeld investigation), constraints on that activity (e.g., population sizes,
nature of the “ﬁeld”) and additional content speciﬁcation (e.g., organism).
At this level, a task developer could create a task concerning the mode of
inheritance of peas or imaginary dragons, thus using a diﬀerent organism and

526
14 Biomass: An Assessment of Science Standards
Here are the results of José’s crosses:
Cross
agouti
agouti-tan
black-tan
agouti
♀x  agouti ♂
11 (6♀:5♂)
agouti-tan ♀x agouti-tan ♂
3 (2♀:1♂)
7 (3♀:4♂)
2 (1♀:1♂)
black-tan ♀x black-tan ♂
10 (5♀:5♂)
Based on these results, José thinks that:
•
the gene for coat color is in an autosome,
•
there are two alleles for this gene in the population, and
•
when the two alleles are in the same individual, they both show up in that
individual’s coat color.
This is José’s hypothesis about the mode of inheritance of this gene for coat
color in mice.
Fig. 14.6 Biomass: background for ﬁrst task
Reprinted from Mislevy et al. (2002a) with permission from The National Center
for Research on Evaluation, Standards, and Student Testing (CRESST), UCLA.
mode of inheritance but the same representational forms and directives. Task
model variables at the individual task level address speciﬁc features of knowl-
edge representations that are employed for problem, reference, or response
data. At this lowest (i.e., most speciﬁc) level, a task developer could create an
alternative task using the same context of mice, but with diﬀerent features or
underlying modes of inheritance.
In addition to ﬁlling out the mode of inheritance table, the ﬁrst segment
of the Mice investigation presented three additional tasks4 to a student. In
both cases, information was presented using certain knowledge representa-
tions, directives were speciﬁed, and the student would respond by adding
information to the knowledge representation to produce a work product.
Figure 14.8 shows the ﬁrst of these, the Population Attribute table. Its
purpose is to obtain evidence bearing on the following Working Knowledge
claim:
C10: Given incomplete data and data collection resulting in anomalous data
and/or one or more deﬁcient models can generate data to explore natural
phenomena at the population level(s) across mechanisms of evolution and
across time,
where “to explore natural phenomena” with regard to model revision is to
(a) recognize need for revision of models
(b) reason through and revise models
4 For the purposes of the four-process model, these four tasks were grouped into a
single “task set.” The various evidence identiﬁcation and evidence accumulation
processes then pulled out the work products and observables from the various
tasks for their separate analyses.

14.3 The Biomass Conceptual Assessment Framework
527
In order to formalize José’s hypothesis, drag symbol(s) or phrase(s) from the
tool box at left to the appropriate columns. Use symbols to complete phrases you
have chosen.
Toolbox
Chromosome
type
Alleles
Dominance
relationships
Possible
phenotypes/
corresponding
genotypes
Ag-1  ag-1
An
Ag-1  Ag-2
Ag-1
Ag-1  Ag-1/
Ag-2  ag-2
...is co-dominant
with respect to...
Ag-1  Ag-2/
An XY
Ag-2
Ag-2  Ag-2/
...is dominant
with respect to...
/
...is recessive
with respect to...
Ag-2
/
...is co-dominant
with respectto...
...is co-dominant
with respect to...
/
...is incompletely
dominant
with respect to...
Ag-1
/
/
/
Fig. 14.7 Biomass: ﬁrst task is to complete a table for allele representation of mode
of inheritance
When the task is presented, only the icons in the toolbox are present. The student
drags selected icons to appropriate places in the four columns to the right in such a
way as to describe the mode of inheritance of hair color. Reprinted from Mislevy et
al. (2002a) with permission from The National Center for Research on Evaluation,
Standards, & Student Testing (CRESST), UCLA.
(c) recognize need for revision of and revise models
for the purpose of explanation, prediction, and/or model evaluation and com-
parison.
Two additional tasks were presented with the initial segment. The third
task was a series of three multiple-choice questions designed to check whether
the student understood the basic parts of the Mendelian model used in the ﬁrst
section. The last task was a special two-part question (Fig. 14.9) asking the
student about the next logical steps in the hypothetico-deductive framework.
The development of the task models and tasks in both the transmission
genetics (mice) and microevolution (lizard) scenarios was similar. In addition
to the stimulus material for the task, task speciﬁc feedback was developed for
each task. This feedback material was customized based on information from
the evidence models.
The task models for the culminating assessment are more focused and
less extended investigations, as would suit an assessment setting with time

528
14 Biomass: An Assessment of Science Standards
For each population attribute listed, check all the population(s) to which it
applies:
Population attributes
Initial ﬁeld
population
(prior to any
crosses)
Initial ﬁeld
population
(following
any crosses)
Offspring
population
(products of
crosses)
All possible phenotypes for a
given characteristic (e.g., coat
color) can bepresent.






























All possible phenotypes for a
given characteristic must be
present.
Phenotypic proportions for a
given characteristic can pro-
vide evidence for a mode of in-
heritance.
Phenotypic proportions for a
given characteristic can pro-
vide evidence for number of
genes involved.
Phenotypic proportions for a
given characteristic can pro-
vide evidence for the type of
chromosome each gene is in.
Phenotypic proportions for a
given characteristic can pro-
vide evidence for the number of
alleles for each gene.
Phenotypic proportions for a
given characteristic can pro-
vide evidence for dominance
relationships among alleles for
each gene.
Individual genotypes can be
proposed.
No mode of inheritance can be
proposed for a given character-
istic.
A mode of inheritance can be
proposed for a given character-
istic.
Fig. 14.8 Biomass: second task, population attribute table
Reprinted with permission from ETS.

14.3 The Biomass Conceptual Assessment Framework
529
☼Verify current hypothesis
 Write ﬁnal report
 Formulate hypotheis
 Generate data (cross mice)
 Analyize data (results of crosses)
Ms. Romano asks you to critique José’s work and his conclusion.
Based on your knowledge of genetics and scientiﬁc methodologies, what should
José do next?
Fig. 14.9 Biomass: fourth task, what to do next?
The second set of choices appears only after the student selects the “verify current
hypothesis” option. Reprinted with permission from ETS.
constraints and individual work. But the same family of observable variables,
bearing on the same aspects of proﬁciency, appear in both the learning and
culminating task models. This relationship between learning and culminating
tasks allows students to become familiar with interfaces, knowledge represen-
tations, and expectations for evaluation during the course of study, so these
crucial components of complex tasks will not “drop in from the sky” in the
culminating assessment.
Altogether, four multistage investigative scenarios (two for classroom use,
two for the culminating assessment) were developed, each consisting of a
sequence of segments that a student would work through in the course of
the larger task. Each segment presented information about results from any
previous segments that were needed in the current segment, in order to reduce
dependencies across segments. As described in the following section, however,
dependencies did occur within segments.
14.3.4 Evidence Models
We complete our tour of the Biomass CAF with a look at the evidence models.
Recall that the evidence models serves as a bridge between the work product
deﬁned in the task model and the proﬁciency variables deﬁned in the proﬁ-
ciency model. The center support for that bridge is a collection of observable
outcome variables that are the center point of the evidence model.
The evidence model bridge has two spans:
•
The rules of evidence that describe how to set the observable outcome
variables based on the work product produced by the student.
•
The Bayes net fragments (i.e., evidence model fragments, or EMFs) that
describes how the observables relate to the proﬁciency variables.5
5 More generally, the ECD framework speciﬁes a statistical relationship between the
proﬁciency and observable variables. Some informal assessments do not implement
this model. When it is implemented, Bayes nets are one way to do it, and we like
their ﬂexibility, but true-score and latent-variable psychometric models such as

530
14 Biomass: An Assessment of Science Standards
In the classroom assessment, the evidence models need to support addi-
tional observables whose role is to provide feedback to the students about their
performance. Biomass distinguished feedback observables that were passed
to the feedback system and ﬁnal observables that were passed to the evi-
dence accumulation process. Feedback observables require rules of evidence
to describe how they are set, but do not necessarily appear in an EMF. Final
observables must appear in an EMF, as well as requiring rules of evidence to
evaluate them (from work products, other observables, or some combination).
Some observables play both roles, being involved in both task-based feedback
and the summative feedback that comes from the evidence accumulation pro-
cess.
There is also another class of observables called auxiliary observables that
are not directly used for either feedback or involved in an EMF. Sometimes
these are intermediate steps in the calculations, sometimes these are of interest
for research purposes (for example, timing information that is collected but
not scored). If any of these auxiliary variables are important for research
purposes, they must be deﬁned in the evidence model as well.
Rules of Evidence
Although in Biomass the rules of evidence were written in a “IF . . . THEN
. . . ” format, test designers can use any mechanism for specifying the rules
that is clear and unambiguous. Eventually, the evidence rules will wind up as
instructions for human raters or requirements for programmers who will write
computer code to execute the rules. In the case of more complex rules, a set
of predeﬁned work products and the corresponding observable values that can
be used for testing will eventually be necessary.
Start with the simplest task in the ﬁrst scenario, the three multiple-choice
questions. The work product in this case is the option selected by the student
for each task. The evidence rule is simple:
If the selection made by the student matches the key, set the outcome
variable to correct.
Although this rule is fairly simple, it can still teach us some lessons. First,
there is a need for a key or in more complex cases evidence rule data. If we
expect the evidence model to be used by diﬀerent tasks from the same task
model, we will need to build in some mechanism to accommodate the diﬀerent
expectations from task variants. In this simple example, the key is just an
indicator for the preferred selection. More complex evidence rules might have
more complicated descriptions of what the expected results might be. For
example, in the automated scoring rule for an essay, the evidence rule data
might be a complex computational–linguistic model needed for a text scoring
algorithm (Deane 2006).
item response theory (IRT), latent class models, and cognitive diagnosis models
can all play this role.

14.3 The Biomass Conceptual Assessment Framework
531
Second, the nature of the evidence rule must be consistent with the purpose
of the assessment. In this case, the observable outcome variable only records
whether the response is correct or incorrect. In a more complex case the key
might be matched to the misconceptions (Bart et al. 1994; Graf 2008). In
this case the matching might set several observable variables (either ﬁnal or
feedback) based on the option selected. In this case, the evidence rule data
will be a table showing for each possible selection what variables should be
set to what values.
The evidence rules for the second task in the segment (the population
attribute table, Fig. 14.8) and the last task (what to do next, Fig. 14.9) are
only slightly more complex. For the second task, the ﬁnal (and feedback)
observables were based on whether the student selected the boxes that cor-
rectly indicated what could be inferred from three populations of mice with
diﬀerent evidentiary properties. There was a column for population, and rows
for diﬀerent potential inferences. The three observables were, for each pop-
ulation (column), whether all inferences were correct (high), mostly correct
except for a few less-critical instances (medium), or missing key inferences
(low). It is frequently the case in developing the evidence rules for complex
tasks that the ﬁnal observables (here “degree of correct inferences in a col-
umn) are summaries of more primitive observables (right/wrong in each cell
of the table). Often if the ﬁne grained observables are all providing evidence
about the same proﬁciency variable, combining many of them into a single
coarser grained observable will considerably simplify the task of building the
EMF, with little loss of information. In this case we just determined whether
all the entries in a column were correct. Alternatively, we could have used the
count of correct values in a column to make a graded response observable,
and ignored the diﬀerence among patterns with the same number correct.
The ﬁrst task, ﬁlling out the diagram shown in Fig. 14.7, illustrates the
importance of correctly deﬁning the work product in a complex task. Even
though the diagram looks quite complex to the student, it has a very simple
representation inside the computer. (And it can be reused for other mode-of-
inheritance tasks.) The table itself is a collection of cells organized into rows
and columns. The work product is a list of what objects the student dropped
into each cell. Given that representation it is easy to write computer code to
answer questions about what the student placed in each row and/or column.
Table 14.5 shows some of the evidence rules for this ﬁrst task at the highest
level. At a lower level, expressions like “MOI(Chromosome Type)” would need
to be translated into more speciﬁc instructions about what to look for in which
column of the table.
Note that the highest score for MendModGen(1) is obtained by ﬁlling in the
table correctly, but partial credit is given for a set of entries that is incorrect
but internally consistent. Giving a response like this is more evidence for a
claim of understanding the concepts of the Mendelian model, even though it
is wrong, than completing the table with internal inconsistencies.

532
14 Biomass: An Assessment of Science Standards
Table 14.5 Rules of evidence for table task
MendModRep(1) [Chromosome Type]
IF Response = MOI(Chromosome Type) is correct
THEN MendModRep(1) = 2
ELSE MendModRep(1) = 1
MendModRep(2) [Number of Alleles]
IF Response = MOI(Number of Alleles) is correct
THEN MendModRep(2) = 3
ELSE IF Response = MOI(Number of Alleles) is partially correct
THEN MendModRep(2) = 2
ELSE MendModRep(2) = 1
MendModGen(1) [Dominance Relationships]
IF [All aspects of Mendel’s Model represented using symbolic forms] are correct
AND [Phenotypic patterns related to all elements of MOI]
THEN MendModGen(1) = 3
ELSE IF [Coherent phenotypic patterns related to all elements of MOI]
THEN MendModGen(1) = 2
ELSE MendModGen(1) = 1
Evidence Model Bayes Net Fragments
Completing the evidence models for the four tasks in the ﬁrst segment of
the Biomass classroom assessment requires specifying evidence model Bayes
net fragments, or EMFs (Sect. 5.4) to link the (ﬁnal) observables to the pro-
ﬁciency variables. Each EMF contained between one and ten observable vari-
ables, and had from one to four proﬁciency variables in its footprint. Fig-
ures 14.10, 14.11, 14.12, and 14.13 depict the EMFs of the four tasks from the
ﬁrst segment.
Note that the EMFs in Figs. 14.10 and 14.11 include a Context variable to
account for possible conditional dependence among observable variables due
to shared stimulus materials, work products, or investigation activities (see
Sect. 6.2). The Context variables for the diﬀerent EMFs are diﬀerent vari-
ables. In both cases, in addition to understanding the general science content,
the students must understand what is required in this particular activity. The
Context variables provide an alternative explanation for poor (or good) per-
formance on all of the work products in a given task with multiple observables.
The EMF shown in Fig. 14.10, for example, is appropriate for modeling
the information about the proﬁciency variable Disciplinary Knowledge about
the Mendelian model (DKMendel) based on the observations made from ﬁll-
ing out the models of inheritance table (Fig. 14.7). The variable speciﬁes
two kinds of observables: MendModRep(x)—observable variables bearing on
representations,—and MendModGen(x)—variables bearing on general termi-
nology and concepts. Given DKMendel and Context, all seven observables are

14.3 The Biomass Conceptual Assessment Framework
533
independent and all are to be modeled using the compensatory design pat-
tern (Sect. 8.5). Again, the (unobservable) variable Context is local to the
evidence model. Thus, it must be given a distribution to complete the evi-
dence model. This is in contrast to DKMendel, a proﬁciency variable that
is “borrowed” from the proﬁciency model. Its distribution is speciﬁed in the
proﬁciency model and not the evidence model.
DKMendel
Context
MendModRep(1)
MendModRep(2)
MendModRep(3)
MendModRep(4)
MendModGen(1)
MendModGen(2)
MendModGen(3)
+
+
+
+
+
+
+
Fig. 14.10 An evidence model Bayes net fragment with seven observables
This is a fragment of the evidence model Bayes net fragment for the ﬁrst task
(Fig. 14.7) for the ﬁrst task segment with seven observables bearing on knowledge
about the Mendelian model (DKMendel). The observations concern representational
forms (MendModRep(x)) or general terminology and concepts (MendModGen(x)) A
compensatory relationship with a Context variable speciﬁc to a single task setting
allows for conditional dependence among the observable variables. Reprinted from
Mislevy et al. (2002a) with permission from The National Center for Research on
Evaluation, Standards, & Student Testing (CRESST), UCLA.
The second task in the ﬁrst segment, ﬁlling out the population attribute
table shown in Fig. 14.8, requires both basic knowledge about the Mendelian
model (DKMendel) and Working Knowledge about Inquiry (WKInqry) as
well as an understanding of the task situation and directives (Context). Fig-
ure 14.11 shows the EMF for this segment. Note that the footprint of this
evidence model is (DkMendel, WKInquiry), and that this will induce an edge
between those two variables in the proﬁciency model.
The evidence model for the three multiple-choice tasks (Fig. 14.12) is
very simple. A single proﬁciency variable DKMendel renders them condi-
tionally independent. This evidence model could be equivalently rendered as
three independent evidence models each with one observable. The icon for

534
14 Biomass: An Assessment of Science Standards
DKMendel
WKInqry
Context
+
+
+
EffMeth(1)
EffMeth(2)
EffMeth(2)
Fig. 14.11 An evidence model using three observables and a context eﬀect
This is an evidence model Bayes net fragment for the Population Attribute Table
task in the ﬁrst Biomass segment shown as Fig. 14.8. This evidence model assesses
the conjunction of Disciplinary Knowledge about the Mendelian model (DKMendel)
and Working Knowledge about Inquiry (WKInqry), followed by a compensatory
relationship with a Context variable that introduces conditional dependence among
three observables that all concern Eﬃcacious Methodology as applied with the
Mendelian Model (EﬀMeth(1)–EﬀMeth(3)). This new distribution type is described
in Sect. 15.1.2, Eq. 15.6. Reprinted from Mislevy et al. (2002a) with permission from
The National Center for Research on Evaluation, Standards, and Student Testing
(CRESST), UCLA.
the compensatory distribution does not indicate a compensatory relationship
(meaningless with a single proﬁciency variable) but rather that the DiBello–
Samejima style pseudo-item response theory (IRT) models (Sect. 8.5) will be
used to specify the probabilities.
DKMendel
MendModGen(1)
MendModGen(2)
MendModGen(3)
+
+
+
Fig. 14.12 An evidence model fragment with three conditionally-independent
observables
This is an evidence model Bayes net fragment for Evidence Model 3 showing three
conditionally independent observables concerning general terminology and concepts
in the Mendelian model (MendModGen(x)) that depend on Disciplinary Knowledge
about the Mendelian model (DKMendel). Reprinted from Mislevy et al. (2002a)
with permission from The National Center for Research on Evaluation, Standards,
and Student Testing (CRESST), UCLA.
The last task in the ﬁrst segment, “what to do next,” (Fig. 14.9) is
interesting because although Domain Knowledge of the Mendelian model
(DKMendel) is needed, the task is mostly about inquiry skills (WKInqry).
This is a situation for which an inhibitor distribution (Sect. 8.5) is ideally
suited. Once the student reaches the basic level of DKMendel, additional lev-
els do not help. Figure 14.13 shows the graphical structure of this model.

14.4 The Assessment Delivery Processes
535
WKInqry
DKMendel
EffMeth
Fig. 14.13 An evidence model fragment using the inhibitor relationship
This is an evidence model Bayes net fragment for Evidence Model 4, showing
one observable about the eﬃcaciousness of solution methodology (EﬀMeth), which
depends on Working Knowledge about Inquiry (WKInqry) in an “inhibitor” or “hur-
dle” relationship with respect to Disciplinary Knowledge about the Mendelian model
(DKMendel). That is, a medium level of proﬁciency in DKMendel is required, but
above this minimum, response probabilities depend only on WKInqry. Reprinted
from Mislevy et al. (2002a) with permission from The National Center for Research
on Evaluation, Standards, and Student Testing (CRESST), UCLA.
A total of 48 evidence models were needed to manage incoming information
from the various tasks in Biomass. The description of the models given here
is not complete, as an operational measurement model requires numbers for
all of the conditional probability tables in both the proﬁciency model and
the EMFs. Getting the numbers into the Biomass measurement model used
many of the steps and procedures speciﬁc to the Bayes net models described
in Part II. Chapter 15 describes both how the initial expert parameters were
speciﬁed and how they were later reﬁned with data.
14.4 The Assessment Delivery Processes
After the numbers were added to the models described above (using the
methods described in Chap. 15), Biomass was ready for implementation. The
Biomass project uses the four-process assessment delivery system (Sects. 2.4.2
and 13.1). This section considers implications of the Biomass design rationale
for each of the processes in turn.
Biomass is a web-delivered assessment.6 That means it must ﬁt within
the established protocols that govern the Internet, in particular, transmission
control protocol/Internet protocol (TCP/IP) and hypertext transfer protocol
(HTTP). Section 14.4.1 brieﬂy describes those protocols and how they ﬁt with
the four-process architecture. In particular, the Internet provides a mechanism
for distributing the workload of any program over several computers. The four-
process architecture provides a mechanism for thinking through those issues,
as discussed in Sects. 14.4.2 through 14.4.5.
6 Here “web-delivered” means using internet protocols and client–server architec-
ture. In our early tests the client and server programs actually ran on the same
computer. But if an assessment were delivered across locations, especially if it
had high stakes, internet security would become important. We do not address
this issue, as material in this rapidly evolving ﬁeld has a half-life measured in
months at best.

536
14 Biomass: An Assessment of Science Standards
14.4.1 Biomass Architecture
Biomass is a web-delivered assessment. This means it can be run in a typi-
cal web-browser by anybody with a computer that is connected to the same
network on which the Biomass server is running. To understand the implica-
tions of this for the design of Biomass requires a minimal knowledge of the
protocols that support the communication between the web server and the
browser. The description below should be enough to follow the details of the
Biomass design that follow. More complete descriptions are readily available
on the Internet.
The basic communication protocol used both on the Internet and local
area networks (LANs) is known as TCP/IP. Under TCP/IP every computer
is assigned an 6-byte IP address, which is like a phone number. You can contact
any computer on the Internet if you know its IP address. (If you know its name
but not its IP address, you use a domain name service, or DNS, to look up the
IP address). In addition to the IP address you need to know the port number
of the program you want to talk to on the other computer. Diﬀerent programs
typically have diﬀerent port numbers. For example, email is typically sent on
port 25, and web page requests typically come in on port 80.
If you have the IP address and the port number of a program on another
(or the same) computer, you can open up a communication channel, called a
socket between the two programs. Typically, one of the two programs listens
on the socket and then responds to messages it receives. The other program
sends a message then listens for a reply. Often the listening program sets up
a miniprogram called a thread to calculate the proper reply to the message it
just received. The operating system handles sharing the computer run time
among many threads, so it appears to users as if they are all running at the
same time.
Combining TCP/IP with the four-process architecture makes it possible
to run any of the four processes on any computer. For example, you could run
the activity selection, presentation, and evidence accumulation process on one
machine, but use another machine to do evidence identiﬁcation (for example,
a special server that provided natural language processing to score essays). In
Biomass, all four processes7 ran on the same machine, with some processes
implemented in Visual Basic and some in Java (a decision based mainly on the
resources available to do the programming). Using TCP/IP to communicate
between the programs avoided many of the hassles normally associated with
integrating programs written in two diﬀerent languages.
Rather than have all four processes communicate directly with each other
(which would require each of them to know the other IP address and port),
Biomass used a central message center process to handle communications
among the processes. Having this one process responsible for routine com-
munications among all the others made it simple to change the ﬂow among
7 Actually, the presentation was split between the client and the server, as we will
see below.

14.4 The Assessment Delivery Processes
537
the four-process model to support the two diﬀerent modes of the Biomass
assessment (Sect. 14.4.7). The message center served another purpose: it was
possible to structure the message center so that it responded to each message
with the next response from a script. This facilitated testing each of the four
processes before the ﬁnal integration.
HTTP is a layer that exists on top of TCP/IP for sending web pages across
the Internet. To start, a program called a browser sends a “get” message to
another machine called a server. The server responds by sending a ﬁle back to
the browser in reply. That response is often in a special format called hypertext
markup language (HTML). The HTML page contains data and instructions
for displaying it in the browser. It may also contain references to other ﬁles on
the server (or on a diﬀerent server). Those ﬁles could be images, audio, video,
or special lightweight programs called applets (Flash and Java are common
languages for applets). Diﬀerent browsers have diﬀerent capabilities as far as
what kinds of media they can display, what kinds of applets they can play, and
how much computation can be done within the web page. When the user is
ﬁnished with the content on this page, the user clicks on a link in the browser,
which causes the browser to send a new “get” message to the indicated server.
One useful kind of object that can be described in HTML is a form, in
which the user makes selections and ﬁlls in ﬁelds. After ﬁlling out a form, the
browser sends what is known as a “post” message. A post message is a little bit
more complex than a “get” message, in that in addition to the address where it
is to be sent, it contains data expressed as key–value pairs, where the key and
the value could be any string. Using extensible markup language (XML) large,
complex objects can be described as strings. Biomass did not take advantage
of this, but NetPASS (Williamson et al. 2004a, Behrens et al. 2004) did,
using XML to pass around completed network diagrams and troubleshooting
protocols. Usually when receiving post messages, the server does not just ﬁnd
the requested ﬁle and return it, but rather does some calculations involving
the posted data to ﬁgure out what the next page to display should be.
There are three big challenges in building a web-based assessment. The
ﬁrst is test security. If the examinee is running the assessment from her own
computer in her own home, there is little to stop her from opening an new
browser window and searching the Internet for information related to the task
on hand. If that behavior is not a desired part of the assessment, then some
form of proctoring will be required.
The second is that there are a large number of diﬀerent types of com-
puters and browsers out there on the Internet. All of the diﬀerent operating
systems and browsers support diﬀerent sets of capabilities. Even though there
is supposed to be a common core of services supported by all browsers, some
implementations are incomplete and some provide extensions to the core fea-
ture set. Moreover, they diﬀer as to how they handle mistakes in the HTML
coding. One browser may “ﬁx” the mistake so as to be close to the desired
rendering of the page, while another browser may produce something that is

538
14 Biomass: An Assessment of Science Standards
completely illegible. This diﬃculty requires that developers either do exten-
sive crossplatform testing or restrict the assessment to one browser/operating
system platform (irritating any user for whom that is not the preferred plat-
form).
The third is that the web server expects results to be calculated syn-
chronously, but the four-process architecture sends messages asynchronously,
returning a new message when it it ready. Generally, sending a “get” or “post”
message to the web server causes it to launch a thread that is supposed to
calculate the reply. To make this work with the four-process message center,
the Biomass presentation process “parked” the thread in a special waiting list
and waited for a reply from the message center. When a message came from
the message center for that user, it woke the appropriate thread which then
turned the message into a web page to be delivered back to the user.
Based on this architecture, we can now look at each of the four processes
in turn.
14.4.2 The Presentation Process
The primary responsibility of the presentation process is to present the task
to the examinee and return the work product to the evidence identiﬁcation
process. However, the Biomass presentation process played a number of other
roles as well. In particular, it presented both task-based feedback (in the
classroom assessment) and the summary feedback (the score report). Also,
it played a special role in starting the assessment and provided some special
screens for teachers to conﬁgure Biomass for a particular classroom setting.
As the presentation process was built on a web server, most of the pre-
sentation was in the form of web pages written in HTML. As HTML allows
the capability to embed images, tables, and forms, it could easily handle most
of the demands of the Biomass tasks. In fact, only 10 of the 29 segments
of the Biomass scenarios required something more. The something more was
the drag-and-drop capability for table completion tasks like the one shown in
Fig. 14.7. A single drag-and-drop applet conﬁgured in diﬀerent ways for each
task provided enough additional capability beyond that which was provided
by HTML forms to support all of the tasks in Biomass.
The lesson is that what determines the quality of the assessment is not
the sophistication of the technology, but how well the technology supports
gathering the evidence we need to make the inferences to ground the targeted
claims of the assessment. In a previous assessment project, creating a design
rationale for assessing dental hygienists’ problem-solving skills (Breyer et al.
1999), a sophisticated simulator was available for scenes and actions in a dental
practice. However, a discussion with the experts revealed that a crucial part
of the hygienist’s job was reasoning from history and examination results to
possible underlying conditions, and what the possibilities were for what to do
next—all tremendously important, and all happening unobserved in the head
of the candidate. We suggested a work product that was a simpliﬁed insurance

14.4 The Assessment Delivery Processes
539
form to capture the results of this thinking, as evidence that was at once
closer to the proﬁciencies of interest and authentic to actual clinical practice.
In Biomass, the key was understanding that experiments are an important
part of assessing Integrated Knowledge. The examinee can act in ways that
reveal their understanding of how to set up an experiment or analyze data
from an experiment using quite simple interactions, as long as it provides the
right kind of evidence.
One question that almost always comes up in designing a web-based appli-
cation is how much work should be done on the client machine (within the web
browser) and how much on the server. In particular, when the evidence rules
involve simple key matching it is tempting to build the logic for the evidence
identiﬁcation process into the web page. However, this eﬀectively changes the
work product from the selection made by the examinee to a variable indicat-
ing whether the selection was correct or not. There are a number of reasons
for keeping the evidence identiﬁcation on the server side: (1) Flexibility—if
the purpose of the assessment changes, the evidence rules might change as
well. In particular, there might be diﬀerent task-level feedback depending on
the selected options. (2) Scalability—embedding the evidence identiﬁcation
rules in the web page might work for simple multiple-choice rules but not for
a complex essay grading system. (3) Security—If the evidence identiﬁcation
rules are embedded in the web page, then the key must be as well. This oﬀers
a hacker an opportunity to reverse engineer the web page to ﬁnd the key.
On the other hand, the presentation process needs to do at least some
interpretation on the client side. If the work product is deﬁned to be the raw
mouse clicks, information such as “Mouse Down at (125,234)” is useless unless
the layout of the elements on the screen is known. Usually, the best level for
the work product is one level up, in this case which selections were made. A
rule of thumb is that usually the client should provide semantically meaningful
results. This maximizes ﬂexibility, in that if hardware or task details change
on the client side and require revising this step of identiﬁcation, it is more
likely that changes in code will not be needed on the server side. Note that
it is possible for the work product to contain additional information that is
not directly used by the Evidence Identiﬁcation process. For example, the
presentation process may want to capture timing information that is not used
for scoring, but is used for verifying that tasks meet their design targets for
how quickly the student can complete the tasks.
The feedback functions are usually fairly simple to implement. A number
of extensions to HTML (e.g., active server pages, ASP; Java server pages,
JSP; and personal home page, PHP) allow the developer to switch content on
and oﬀdepending on the values of feedback observables. This approach was
used to customize the task-based feedback in Biomass based on the observed
values of the feedback observables. For score reports, values from the evidence
accumulation process (stored in the administrative process described below)
can be inserted into the appropriate places in score-report web forms.

540
14 Biomass: An Assessment of Science Standards
Finally, the special startup and conﬁguration screens again are simple to
implement using basic HTML codes. The key diﬀerence is that in many cases,
the other three processes are not yet started. In Biomass, a special admin-
istrative mode took care of these special startup and conﬁguration tasks. In
Biomass, the message center had a special administrative mode that routes
traﬃc between the administrative and presentation processes. If the admin-
istrative process and message center were not started, then the presentation
process would take charge of starting them as well.
From the message center’s perspective, every time the user pressed a sub-
mit button on the screen, the browser turned this into an HTTP post request.
If the message included work products, these were coded as strings.8 The web
server then formatted this post request as a message center message and sent
it to the message center for appropriate routine. If it was a request by the
user to jump to a new task, this was routed to the activity selection process
for further processing. If the student was moving using the normal work ﬂow,
then the message would contain either feedback or ﬁnal work products that
would be sent to evidence identiﬁcation for the next stage of processing.
14.4.3 Evidence Identiﬁcation
The role of the evidence identiﬁcation process is to identify the essential fea-
tures of the work product that provide evidence about the examinee’s current
knowledge, skills, and abilities. They could be as simple as the correctness of a
multiple-choice response or as complicated as subtle patterns across hundreds
of actions and ﬁnal results in a minimally constrained hour-long computer-
ized patient management problem. These features are recorded as values of
observable variables, possibly after multiple stages of processing. As discussed
above, feedback observables are sent back to the presentation process to use in
customizing task-level feedback; ﬁnal observables are sent on to the evidence
accumulation process for use in summary level scoring.
The challenge of building the evidence identiﬁcation process lies in trans-
lating evidence rules, such as those found in Table 14.5, into computer code.
Note that whatever additional meaning is placed on evidence rules, they will
ultimately become speciﬁcations for the evidence identiﬁcation process code.
In Biomass, as each task was a one-oﬀ; the code for the evidence identiﬁcation
process was unique for each task. The evidence rules were quick enough to
process that both the feedback and ﬁnal observables were calculated at each
time step, even if only one was requested.
The heart of the evidence identiﬁcation process was a dispatch system that
determined, based on the ID of the task, which evidence identiﬁcation code
needed to be run. In Biomass, the key was programmed into that code. In
a system with more diﬀerent variants of tasks and sharing of scoring code,
8 XML was just coming in as a new technology when Biomass was built. If we had
done it a year later we would have used XML to encode the strings.

14.4 The Assessment Delivery Processes
541
the evidence identiﬁcation process would also need to fetch evidence rule data
from the task/evidence composite library, again keyed by task ID. For exam-
ple, a key-matching rule for a multiple choice task, would need to fetch the
key from the database. More sophisticated tasks, such as a variant of the
one shown in Fig. 14.7, would require more sophisticated evidence rule data;
for example, a list of the expected dominance relationship and the expected
genotype-phenotype pairs. This evidence rule data must be authored (or auto-
matically generated: “automated automated scoring” (DiCerbo and Behrens
2012)) along with the task.
Testing has proved to be a signiﬁcant challenge in implementing many evi-
dence identiﬁcation processes. Thorough testing requires sample work prod-
ucts that will generate each level of the observable, and the more complex the
work product, the more diﬃcult they are to produce. Often the presentation
process must be at least be partially built to be able to generate the required
work products. If the work product represents a complex construction, the
space of possible work products can be quite large, and generating enough
work products to provide suﬃcient testing can be diﬃcult. Pilot test data
can be helpful because real students always come up with surprising ways to
approach a problem that designers had not anticipated.
In the case of Biomass, the four-process architecture made it fairly easy
to write special rules for the message center so that instead of using all four
processes, it just fed the evidence identiﬁcation process the sample work prod-
ucts for testing. The generated observables could then be compared to what
was expected. This produced an automatic testing harness for the evidence
identiﬁcation (and all of the other) processes.
In addition to the testing of the evidence identiﬁcation process, these test
work products play a role in the ﬁnal integration testing of the whole system.
Now, instead of actual work products, the system testers need scripts for how
to generate work products, and then descriptions of what the observables and
expected feedback will be.
14.4.4 Evidence Accumulation
Section 13.3 already provides many details on how the the evidence accu-
mulation process (abbreviated EAP here) works, and Sect. 5.4 provides the
mathematical algorithms. The Biomass evidence accumulation process is sim-
ply an implementation of the algorithms described there. When a new stu-
dent began the Biomass Assessment, the EAP would create a scoring model
for that student by copying the proﬁciency model Bayesian network. These
scoring models would be indexed by the student ID (assigned by the Biomass
administrative process).
When the observables for a task came from the evidence identiﬁcation
process, the header of that message would contain two critical pieces of infor-
mation, the student ID and the task ID. The EAP would use the student ID
to ﬁnd the appropriate scoring model, and the task ID to ﬁnd the evidence

542
14 Biomass: An Assessment of Science Standards
model fragment (or fragments) associated with the task. (Chap. 15 describes
the evidence model fragments for the ﬁrst task.) It would then instantiate
the observables in the evidence model fragment to their observed values and
calculate the distribution over the footprint variables. This distribution would
then be inserted as virtual evidence into an appropriate clique of the scoring
model.
Upon request, the EAP could calculate statistics of the scoring model to
use for score reporting and other purposes. In the Biomass implementation,
the statistics were divided into two categories. Statistics that were easy to cal-
culate, like the marginal distributions of the reporting variables, were marked
“report on update” and reported after absorbing the evidence from each task.
As Biomass was not adaptive, these were not used by the activity selection
process, but were stored by the administrative process and could be used for
later analyses, such as constructing a evidence balance sheet (Sect. 7.2.1).
Statistics that were time-consuming to calculate would not be reported after
each task, but only when requested (because generating an extended score
report was required).
The classroom version of Biomass was intended to be used in multiple
sessions across many days. This required the EAP to additionally have the
capability to save the scoring model as a ﬁle and restore the saved model.
Early testing revealed a bug in the software that was causing the restored
scoring model to lose the evidence. This proved to be an easy bug to work
around as all of the observable variables for the student were stored in the
administrative process database. To restore the model, the administrative
process and EAP could work together to replay the series of tasks taken by
the student—skipping the presentation and evidence identiﬁcation processes,
since the saved observables from the early work were already on hand.
The classroom version of Biomass raised another issue because the student
could attempt the same task multiple times. The Biomass design team talked
over the issues discussed in Sect. 13.3.3 and eventually decided to simply treat
the evidence from each attempt as independent evidence. This solution was
the easiest to implement, and should be a close enough approximation for the
relatively low-stakes purposes of the classroom assessment. However, being
able to intelligently account in the scoring for feedback and multiple attempts
that (in some systems, intentionally) produce learning remains an active area
of research (Sect. 16.2.2; Ritter et al. 2007).
Biomass actually ran two evidence accumulation processes in parallel. The
ﬁrst, the Bayes net scoring process described here, was used for the primary
scoring. The second, a simple number-right scoring process, was used for the
purpose of counting tasks to provide a tasks completed ﬁeld on the score
report. The idea of running multiple scoring engines in parallel is a very
powerful feature of the four-process architecture. For example, an IRT-based
scoring engine could be used to provide a scale score on overall proﬁciency
while a Bayes net scoring engine provided proﬁciency-based diagnosis. Small
special-purpose evidence accumulation processes can also be be included, for

14.4 The Assessment Delivery Processes
543
example, to simply count instances of particular misconceptions or kinds of
errors, for feedback at the end of a segment or the end of the assessment (of
course evidence identiﬁcation processes would need to be able to identify them
ﬁrst).
14.4.5 Activity Selection
The classroom version of Biomass supported two mechanisms for activity
selection, but both were very simple compared to the expected-weight-of-
evidence adaptive task selection of Adaptive Content with Evidence-based
Diagnosis (ACED) (Sect. 13.1). The ﬁrst activity selection scheme in Biomass
was a simple linear mode in which the student moved from segment to segment
in the scenario (following the next step in the sequence given in Table 14.3,
or the similar table for the “lizard” scenario). This was implemented through
table look up: for each task the activity selection process looked up the task
ID of the next task and sent a message to the presentation process to present
that task. The second activity selection mechanism was user selection. Again,
this was implemented through a simple table lookup: given the name of the
segment, the activity selection returned the appropriate task ID.
The culminating assessment was even simpler. The option of user control
was no longer available, so only the “next task” mechanism was implemented.
One of the segments in the culminating assessment had a quite complex task
in which the student had to navigate through several screens, formulating
hypotheses and performing experiments to test those hypotheses. It was open
ended in the sense that the student could formulate and test a large number
of possible hypotheses. However, the culminating assessment had no interim
feedback, and hence this entire set of activities could be considered one “task”
even though it spanned several web pages. Thus, all the complexity could be
buried in the presentation process and not aﬀect the activity selection process.
14.4.6 The Task/Evidence Composite Library
In the center of the four-process delivery system is the task/evidence com-
posite library. It makes available to the four processes the information they
need to carry out their functions. The task/evidence composite library for
Biomass was not stored in a single location, but rather each of the four pro-
cesses maintained the part of the library that it used. In particular, the tasks
(and their feedback) were stored as a series of web pages in a location known
to the presentation process. Similarly the Bayes net fragments were stored as
a series of ﬁles in a location known to the evidence accumulation process.
What was important about the library is that the task ID served as the
primary key to each of the separate databases. In particular, the presentation
and evidence accumulation process could locate the right data ﬁles given the
task ID. Similarly, the evidence identiﬁcation process could select the right
evidence rules based on the task ID. Finally, the activity selection process
knew how to go from one task ID to the next.

544
14 Biomass: An Assessment of Science Standards
14.4.7 Controlling the Flow of Information Among the Processes
One of the critical diﬀerences between four-process delivery architecture and
traditional delivery systems is the way in which the ﬂow of information
through the system is managed. In traditional delivery, this ﬂow is usually
“hard-wired”; that is, delivery processes are strictly deﬁned to receive ﬁxed
information from, and send ﬁxed information to, certain other processes in a
ﬁxed sequence. When a designer changes the purpose of an assessment—its
context or conditions for use—all this must change as well. The two modes of
Biomass prototype illustrate the diﬀering logic requirements for the sequence
of interactions among the delivery processes for the learning and culminating
modes.
The purpose of the Culminating Assessment is to determine a student’s
level of proﬁciency at the end of a course, providing overall results and sum-
mary feedback at the end of the testing session. To this end, the activity
selection process tells the presentation process to start each successive task
after capturing the work products of the previous one. All of the work prod-
ucts can be sent to evidence identiﬁcation at once for task-level scoring. Evi-
dence identiﬁcation uses the resulting observable variables in two ways. Some
observable variables can be used to generate task-level feedback, which is pre-
sented to the examinee at the end of the assessment. Some observable variables
are passed on to evidence accumulation to update beliefs about proﬁciency
variables, and subsequently ground higher-level feedback, instructional deci-
sions, or score reports. Note that all of the task- and summary-scoring can be
accomplished by evidence identiﬁcation and evidence accumulation at a dis-
tant time or place from the actual testing session. Therefore, in the Biomass
Culminating assessment, the message center was conﬁgured to send messages
containing work products from the presentation process to both the activity
selection process and the evidence identiﬁcation process. That way the activity
selection could jump directly to the next task, while the evidence identiﬁca-
tion and accumulation processes worked in the background to generate the
ﬁnal score report.
The purposes of the Learning Assessment, on the other hand, are to pro-
vide practice and support learning in preparation for the Culminating Assess-
ment. These purposes are served by immediate feedback, cumulative scoring,
and opportunity to repeat a task. However, because the task sequence was
not adaptive the messages still did not need to ﬂow around all four processes
before a new task message could be sent.
The need for immediate task-based feedback did however cause a more
complex message ﬂow. As before, the activity section process tells the presen-
tation process to start a new task. The student produces work products which
are sent to evidence identiﬁcation to calculate the feedback observables. The
feedback observables are sent back to the presentation process which then gen-
erates appropriate feedback screens. After viewing the feedback, the student
decides to either repeat the task, in which case the feedback cycle will repeat,

14.5 Conclusion
545
or to submit the work product (perhaps after minor changes) for ﬁnal scor-
ing. In the latter case, the work product is submitted again to the evidence
identiﬁcation process, this time as a ﬁnal work product. Now the evidence
identiﬁcation process sends the ﬁnal observables to the evidence identiﬁca-
tion process. As in the Culminating Assessment, the task selection does not
depend on either the output of evidence identiﬁcation or evidence accumu-
lation. Therefore, as soon as the ﬁnal work product is received, the activity
selection process can send a message to the presentation process about what
task to present next. Thus, the student can be working on the next task while
the scoring happens in the background.
When the evidence accumulation process is ﬁnished scoring, it sends a
message containing the critical scores for the score report back to the message
center. In Biomass, the scores were only sent to the administrative process
to be recorded in the database. When the students ﬁnished the assessment
(either by choice in the Learning Assessment or by reaching the end in the
Culminating Assessment), they were given the option of viewing a summary
score report. If a student had not completed the assessment, the report would
be based only on those tasks the student had completed.
As this report was generated from data in the administrative process, the
student did not even need to be logged into the system to generate a score
report. As long as the scores were in the database, Biomass could generate the
report. It would be straightforward to implement a teacher’s view in which
the teacher could monitor a classroom of students (or teams) working through
Biomass, and visit the workstations of the students who needed the most help.
Almond et al. (2009a) describe some possible classroom views, using data from
the ACED assessment.
14.5 Conclusion
This chapter might seem a bit of an odd man out—way too much on assess-
ment design for a book on Bayes nets. This would be true if the book were just
about Bayes nets as an analytic method, to be applied with a some unspec-
iﬁed assessment. Even with this limited goal, there is a lot to learn about
Bayes nets technically, and it has indeed been our purpose to provide a solid
foundation for this aspect of the assessment enterprise.
The larger point, though, is that using Bayes nets in assessment eﬀectively
is not just about applying an analytic method to data already gathered from
an assessment that already exists. Rather, it can, and ideally should, arise from
designing an assessment system (all the elements, processes, and activities)
that embodies an assessment argument for some purpose. For some purposes
and in some contexts, the kinds of inferences that are desired, the kinds of
evidence that supports them, and the kinds of tasks that evoke the evidence
will produce data that Bayes nets are well suited for reasoning about.

546
14 Biomass: An Assessment of Science Standards
This will be the case especially when the evidence involves elements such
as multiple aspects of proﬁciency, in diﬀerent combinations in diﬀerent tasks;
conditional dependencies arise among observable variables, whether testlets of
structural relationships among task conditions or examinee actions; multiple
complex tasks need to be authored around the same evidentiary structure;
or multiple complex situations need to be assembled ﬂexibly, as in adaptive
testing and tutoring systems, yet provide information in the same proﬁciency
metrics (Almond and Mislevy 1999).
Biomass was a demonstration assessment meant to show these principles
in action. It illustrates several innovative and ambitious features, including
the following:
•
Designing assessments in terms of re-usable schemas, objects, and pro-
cesses.
•
Developing assessments to assess standards, in such a way as to both give
them concrete meaning and address higher-level forms of knowledge–in
this case, inquiry in science, with content from transmission genetics and
microevolution.
•
Using dynamically assembled Bayesian inference networks to manage the
accumulation of evidence in a multivariate model, from multivariate and
sometimes conditionally dependent observations.
This chapter has shown how the models and approaches of evidence-
centered design can be used to organize the design and implementation of
such an assessment, and do so in a way that lends itself to the reuse of the
materials and processes. The success with Biomass led to other projects, such
as NetPASS, ACED, and SimCityEDU (Mislevy et al. 2014), using the ECD
methodology as part of their design philosophy.
Although this chapter tells a major portion of the Biomass story, one
signiﬁcant piece is missing: how the Bayesian networks for the proﬁciency and
evidence models were built and how the parameters for those models were
set. The next chapter provides some of the more interesting details of that
process.
Exercises
The problems in this chapter build on the concepts of previous chapters. Refer
back to those chapters as necessary in addressing these problems.
14.1 (Task Model for Uniﬁed Knowledge). What might a task model
for Uniﬁed Knowledge look like? What elements would it need to include?
14.2 (All Integrated Knowledge Tasks Assembly Model). Suppose
that in the Biomass assembly model, the designers included only tasks that
tap integrated knowledge. Would this present a problem for measurement?

14.5 Conclusion
547
14.3 (All Proﬁciency Pairs). Suppose that the designers proposed an
assembly model in which (1) each of the domain knowledge proﬁciencies
was paired with each of the working knowledge proﬁciencies in at least one
task, (2) each of the domain knowledge proﬁciencies was paired with each of
the integrated knowledge proﬁciencies in at least one task, and (3) each of
the working knowledge proﬁciencies was paired with each of the integrated
knowledge proﬁciencies in at least one task. Would this present a problem for
measurement?
14.4 (Compensatory vs Inhibitor models for Context). Consider Task 1
(ﬁlling out the table in Fig. 14.7), whose evidence model structure is given in
Fig. 14.10. Suppose that an inhibitor or a conjunctive design pattern was used
for the conditional probability tables instead of the compensatory pattern that
was chosen. How would that change the interpretation of Context?
14.5 (Mice Make Me Go EEK!). Suppose that in the ﬁeld trial, a portion
of the subjects had diﬃculty answering the questions because every time they
saw the pictures of the mice, they just wanted to jump up on their chairs and
scream “EEK!”9 If this portion is suﬃciently large, then it should probably be
taken into account in the measurement model for Biomass. Explain how the
measurement model for Biomass could be modiﬁed to include this reaction.
14.6 (Radical or Incidental). For each of the following task model vari-
ables, explain whether it is radical or incidental. If it is radical explain whether
it eﬀects diﬃculty, evidentiary focus, or both. If it is incidental, explain
whether there is a range beyond which which it becomes radical.
1. The species of animal/plant being crossed.
2. Whether the species reproduces sexually or asexually.
3. The name of the ﬁctitious student used in the examples.
4. The phenotypic trait expressed by the gene (e.g., hair color).
5. How many alleles there are.
6. The type of dominance relationship (e.g., dominance, codominance, in
complete dominance).
7. Which chromosome the gene is found on.
8. Whether or not the gene is sex-linked.
9. How the initial ﬁeld population was gathered.
9 This is known as the Slartibartfast eﬀect (Adams 1978).

15
The Biomass Measurement Model
The previous chapter described the basic design and construction of the
Biomass assessment. It showed how the graphical structures for the proﬁ-
ciency and evidence model Bayes net fragments arise jointly from the the-
ory of learning in the domain and the construction of tasks, to embody an
assessment argument. This chapter examines how to populate the conditional
probability tables with numbers.
Quantiﬁcation of a Bayesian network comes about in two phases. The ﬁrst
phase is specifying a prior distribution for the unknown quantities. The second
is updating that prior with data. As described in Fig. 8.3, this distribution,
whether prior or posterior, “ﬂoats above” the basic Bayesian network. If we
want to score a student, we simply have the current mean values “drop down”
into the network for that purpose.
Section 15.1 describes how the prior for the Biomass Bayesian model was
constructed, both in terms of choosing parameterizations for the conditional
probability tables, and choosing initial parameters. Section 15.2 describes a
brief expedition to gather data, which yielded 28 observations of question-
able representativeness. Section 15.3 describes how those data can be used to
update the values for the conditional probability tables.
Taken together, this chapter and the previous one illustrate an interplay
between substantive knowledge and empirical evidence in constructing and
reﬁning the statistical model in a Bayes net assessment with complex tasks.
The previous chapter discussed in detail the design process which began with
an analysis of the domain and the targeted proﬁciencies. The range of situ-
ations, representations, and forms of activities that could adduce to obtain
evidence about students’ proﬁciencies was explored. The task models were
constructed by test developers and substantive experts, expressly to provide
this evidence; the tasks were structured such that the salient features of per-
formance could be identiﬁed, and their relationships to proﬁciencies speciﬁed
(at least provisionally). Thus, the Bayes net structures arose in conjunction
c⃝Springer Science+Business Media New York 2015
549
R. G. Almond et al., Bayesian Networks in Educational Assessment,
Statistics for Social and Behavioral Sciences, DOI 10.1007/978-1-4939-2125-6 15

550
15 The Biomass Measurement Model
with the building of the evidentiary argument from the start, rather than
being built only further down the road when data arrive.
This, we argue, is a good assessment design practice. The general approach
is indeed reﬂected in high-quality assessments, with Bayes nets or other psy-
chometric models (see for example, Leighton and Gierl 2007; Wiggins 1998;
Wilson 2004). It is sometimes employed in some formative assessments and
learning systems, but more often decisions are made in such assessments in a
more ad hoc manner. They may have used expert judgment (which might be
good, but might not be) to design tasks and evaluation schemes, and usually
with little data to start. The problem is that if they have wired-in rules to
synthesize evidence and make decisions, they cannot exploit the time-tested
machinery of statistical probability to check models, improve estimates, or
incorporate other sources of information as data arrive. There is no well-
understood pathway for improvement.
The Bayes net approach can model diﬀerent kinds of relationships and
deal with diﬀerent kinds of data in these applications. The 1980s saw much
debate about ways of dealing with uncertain evidence in the context of expert
systems: probability theory, of course, but also fuzzy logic (Zadeh 1965), belief
theory (Dempster 1990), and credibility factors (Shortliﬀe and Buchanan
1975). Researchers such as Pearl (1988) and Spiegelhalter et al. (1993) argued
that probability had substantial advantages for dealing with uncertainty in
the context of noisy information and complicated relationships. Their work on
Bayes nets extended probability-based reasoning in practical ways to expert
systems, which tackle the same challenges we face in complex assessments;
they start with lots of expert beliefs but not much data. With the Bayes
nets framework, expert beliefs are a starting point—for design as well as for
inference—rather than an ending point.
In this spirit, the initial Biomass network speciﬁcations could be used as an
expert-data-only system to ground inferences about students’ strengths and
weaknesses with regard to science standards. We see in this chapter the way
that initial pilot data are used to reﬁne estimates and provide some initial
model checks. Presumably better inferences about students could be based
on the improved model. The process can continue as more data accumulated,
using the model estimation and model checking methods discussed in earlier
chapters, to improve the model (and also feed back to improve task design).
15.1 Specifying Prior Distributions
The complete Biomass conceptual assessment framework contained 15 pro-
ﬁciency variables and 28 segments, where each segment consists of multi-
ple tasks with multiple supporting evidence models. This chapter focuses on
the models that support the ﬁrst segment of an investigation in transmission
genetics, called “Mice1.” In the scenario, a student Jos´e discovers a population
of mice, notes how many mice have each of four coat colorings, and decides

15.1 Specifying Prior Distributions
551
to investigate the mode of inheritance of coat color in mice. This segment
takes from 10–20 min to complete and yields 14 observable variables, each
providing a single categorical response on a 3-point scale. These 14 observ-
ables come from the four tasks described in Sect. 14.3.3 and were scored using
the four evidence models described in Sect. 14.3.4. The ﬁrst segment only pro-
vides evidence about the proﬁciency variables DKMendel and/or WKInqry,
so this chapter will use a simpliﬁed proﬁciency model containing only those
two variables (Table 15.1).
Table 15.1 Task and evidence models from the ﬁrst Biomass segment
Task model
Evidence model Observables
Name
Figure
Name
Figure
count
TM 1
14.7
EM 1
14.10
7
TM 2
14.8
EM 2
14.11
3
TM 3 Not shown EM 3
14.12
3
TM 4
14.9
EM 4
14.13
1
Task/Evidence Model 1 (TM1/EM1) concerns re-expressing Jos´e’s verbal
hypothesis about the mode of inheritance with a tabular diagram. Figure 14.7
shows the tabular diagram and Fig. 14.10 shows the evidence model graph
fragment.
Task/Evidence Model 2 (TM2/EM2) concerns a table (Fig. 14.8) that a
student was asked to ﬁll out, containing several statements about implications
of the mode of inheritance. The form of the EMF for this task is shown in
Fig. 14.11.
Task/Evidence Model 3 (TM3/EM3) consists of three multiple-choice
questions about implications of forms of dominance. DKMendel is the only
proﬁciency variable, and the responses are posited to be conditionally inde-
pendent. The form of this EMF is shown in Fig. 14.12.
Task/Evidence Model 4 (EM4) asks what Jos´e should do next, after having
formalized his hypothesis about the mode of inheritance of hair color based on
the ﬁeld population (Fig. 14.9). The form of this EMF is shown in Fig. 14.13.
The ﬁrst two evidence models include “context” variables (Sect. 6.2),
ContextT M1 and ContextT M2 . These are variables local to the respective evi-
dence models meant to handle conditional dependence among the observed
outcomes that is not explained by the proﬁciency variables alone. (It is thus
similar in spirit to the testlet parameter of Wainer et al. 2007.) These variables
are local to the scoring model for a speciﬁc task, and hence we add a subscript
indicating the task (or task model) they are related to. Although they are
placed in the evidence model to preserve this task dependency, they are more
like a proﬁciency variable in that they designate a latent property of the stu-
dent, rather than an observable property of the student’s work. In each case,
we assume that Context can take on two values that we will label familiar

552
15 The Biomass Measurement Model
and unfamiliar; familiarity is a common source of conditional dependence,
but there are plenty of others, and this mathematical structure accounts for
any of them that have a compensatory eﬀect with the proﬁciencies of interest
(Yen 1993).1
Altogether, to specify the Bayes net model for Segment 1 of the Biomass
interim assessment we need to specify conditional probability tables for
18 variables: 2 proﬁciency variables, 2 context variables, and 14 observables.
The 2 proﬁciency variables were modeled using the hyper-Dirichlet distri-
bution (Sect. 15.1.1). The 14 observable variables were each modeled using
a variant of the DiBello–Samejima design patterns (Sect. 8.5); Sect. 15.1.2
describes this process. The two context variables are given ﬁxed distributions,
since their role is simply to model conditional dependence among the observ-
ables in a task. Section 15.1.3 provides a tabular summary of the prior and
compares it with the posterior after updating using the pilot data from the
experiment described in Sect. 15.2.
15.1.1 Speciﬁcation of Proﬁciency Variable Priors
In the ﬁrst segment, only two proﬁciency variables, DKMendel and WKInqry,
appear in the footprint of any of the four evidence model for that section. For
the calculations in this chapter, we have simpliﬁed the proﬁciency model from
15 variables to only those two variables. Both DKMendel and WKInqry are
categorical and can take on one of the three values: high, medium, and low.
The experts expect that DKMendel and WKInqry are correlated; this
demands an edge between them. The question remains how to orient the edge.
If the student learns the inquiry skills in the context of genetics, then they
would need to learn the domain knowledge ﬁrst. This would suggest orienting
the edge from DKMendel to WKInqry. However, they may have learned the
inquiry skills in the context another discipline (e.g., ecology or chemistry).
However, in Biomass we are always assessing working knowledge skills in the
context of genetics, so it makes sense to orient the edge from DKMendel to
WKInqry.
Because in the reduced graph DKMendel has no parents, it has a con-
ditional probability table (CPT) with a single row (actually in this case an
1 There is neither interest in, nor data suﬃcient for, estimating the extent to which
a student’s performance on the observables in the task are meaningfully diﬀerent
than that which would be expected under conditional independence. The strength
that is estimated for the contribution of a task’s Context variable reﬂects the
frequency with which students’ performances in the task are a bit better or worse
as a set than would be expected. The practical impact is to appropriately reduce
the strength of updating for proﬁciency variable.

15.1 Specifying Prior Distributions
553
unconditional probability table). Positing prior exchangeability for students2,
we have the following distribution for DKMendel:
DKMendeli ∼Cat(λ1, λ2, λ3) ,
where λm is the probability that Student i is in State m of DKMendel. The
natural conjugate prior is the Dirichlet law. We posit the relatively uninfor-
mative prior
(λ1, λ2, λ3) ∼Dir (3, 4, 3) .
(15.1)
The variable WKInqry also takes one of three possible values: high,
medium, and low. However, its distribution is now conditional on the state
of DKMendel. Therefore, it has a CPT with three rows, each of which is an
independent categorical distribution.
WKInqryi|DKMendeli = p ∼Cat (λp1, λp2, λp3) ,
where λpm is the probability that Student i is in State m of WKInqry given
that she is in State p of DKMendel. Assigning an independent Dirichlet law
to each row of this table produces a hyper-Dirichlet law for this CPT. The
experts anticipate that DKMendel and WKInqry will be positively associated
among students; students with more knowledge about the concepts and rep-
resentational forms of the Mendelian model will probably have more skill in
applying their knowledge. We posit a set of parameters for the hyper-Dirichlet
law reﬂect that positive association:
(λ11, λ12, λ13) ∼Dir(5, 3, 2)
(λ21, λ22, λ23) ∼Dir(3, 4, 3)
(λ31, λ32, λ33) ∼Dir(2, 3, 5).
(15.2)
The strength of a Dirichlet prior can be gauged by summing its parameters
(Sect. 3.5.3). In the simple Dirichlet-multinomial case, the posterior param-
eters will be the sum of the prior parameters and the observed cell counts.
Thus, the sum of the prior parameters gives an eﬀective sample size for the
prior. In this case, the prior for DKMendel has eﬀective sample size of ten,
thus it would be weighted equally with ten observations (this would be quite
mild in comparison with a substantial sample of several thousand, but is fairly
large in comparison with the ﬁeld trial of size 28).
Judging the strength of the hyper-Dirichlet prior is a bit trickier. In prin-
ciple, it works the same way, summing the parameters in each row gives
the eﬀective strength of the Dirichlet distribution for each row. However,
when calculating the posterior only those members of the sample for which
DKMendel = low will contribute to the posterior. In this example, we sample
2 One could posit diﬀerent priors for diﬀerent students in the ﬁeld trial, based on,
say, how many courses they had taken in genetics and how many in science in
general.

554
15 The Biomass Measurement Model
to be divided evenly among the three categories of DKMendel. This means
that even though each row of Eqs. 15.1 and 15.2 sum to the same value,
the prior for WKInqry will have about three times the strength of that for
DKMendel.
Some care must be taken if one of the states of the parent variable is
rare. For example, if the expected probability for DKMendel = high is 0.05,
then only 1 out of 20 observations is used update that row of the conditional
probability table. In these cases, a much larger sample size is needed to learn
much about that value from data. It helps to use structured, parameterized
conditional probability matrices like those discussed in Sect. 8.5, because they
use both data from other rows and the beliefs about the inter-relationships
among probabilities throughout the table to produce smooth and plausible
estimates throughout the table.
15.1.2 Speciﬁcation of Evidence Model Priors
Strictly speaking, the four activities involved in Segment 1 of Biomass are
tasks and not task models. The graph fragments used to score them are link
models and not evidence models. The common practice is to specify priors at
the level of the evidence model parameters, and after observing data, calibrate
the posterior parameters for the link model. However, in Biomass there is only
one task per task model and hence, only one link model per evidence model
and the distinction is purely semantic. The details for the four evidence/link
models for the four tasks from the ﬁrst segment are given below.
Evidence Model 1
Task 1 (EM1) concerned re-expressing Jos´e’s verbally stated hypothesis about
the mode of inheritance in tabular form (Fig. 14.7). A student would use
the applet shown in Fig. 14.7 to express Jos´e’s hypothesis using the table
by dragging elements from palette of symbols and terms and dropping them
into the table. The work product consisted of a set of lists for each cell of
the table, describing which of the elements where dropped into that cell.
The observables were all based on applying rules of evidence to the work
product. The three MendModGen observable variables concerned the degree
of correctness of the elements in given cells; for example, on a 1–3 scale, how
accurately the dominance relation the student constructed matched Jos´e’s
working hypothesis.
The four MendModRep observables concerned the consistency among dif-
ferent portions of the work product. For example, Jos´e posited a dominant
relationship, but if a student indicates codominance and genotype/phenotype
combinations that were consistent with codominance, then the MendModRep
observable (consistency) gets a high value and the MendModGen observable
(accuracy) gets a low value. Seven distinct aspects of this solution are captured
as values of observable variables, all providing evidence about DKMendel.

15.1 Specifying Prior Distributions
555
Note that all seven observables are based on the same work product. Although
each of the observables is posited to depend on only one proﬁciency variable,
DKMendel, they are probably dependent beyond their relationship through
DKMendel. We model this situation as described in Sect. 6.2, by introducing
the independent context proﬁciency variable Context for TM1, or ContextT M1.
It has two values, familiar and unfamiliar. ContextT M1 is an additional par-
ent of all the observables within this task. The form of this EMF is shown in
Fig. 14.10.
Quantifying the EMF shown in Fig. 14.10 requires specifying eight condi-
tional probability tables: one for each of the seven observables, and one for
the Context variable (which is local to the evidence model). The proﬁciency
variable, DKMendel, is borrowed from the proﬁciency model. Its distribution
was already speciﬁed in the proﬁciency model, so, we do not need to specify
it again here.
Next, we need to choose a parameterization for the conditional probability
tables for the observable variables. The experts decided that the inﬂuence of
the DKMendel and Context variables should be combined using the compen-
satory design pattern. Given this, the DiBello-Samejima formula (Almond
et al. 2001; this volume, Sect. 8.5) can be used to establish the values in
the tables, given the values of certain parameters: an intercept or diﬃculty
parameter, and a slope or discrimination parameter for each parent variable.
Thus, to specify this evidence model, we must specify prior distributions for 21
parameters (one intercept and two slope parameters for each of seven observ-
ables).
Consider any one of these seven observables, Observable 1j. To start
the construction, we introduce eﬀective theta3 values for DKMendel and
ContextT M1: θ∗
DKM and θ∗
C1. These are related to the corresponding variables
as follows:
θ∗
DKM =
⎧
⎪
⎨
⎪
⎩
1
DKMendel = high
0
DKMendel = medium
−1
DKMendel = low
(15.3)
θ∗
C1 =

1
ContextT M1 = familiar
−1
ContextT M1 = unfamiliar
(15.4)
The eﬀective theta for a particular combination of parent variable states is
given by:
˜θ1j = α1jDKMθ∗
DKM + α1jContextθ∗
C1 + β1j
(15.5)
3 The calculations in this section use an older algorithm for assigning the eﬀective
theta values to parent states than the one given in Sect. 8.5.1. In particular, it
uses points that are equally spaced in the interval [−1, 1] rather than equally
spaced according to a normal distribution. This is historically accurate as the
Biomass work was done before we realized that there was an advantage of the
alternative spacing when chaining DiBello–Samejima distributions together.

556
15 The Biomass Measurement Model
Recall (Eq. 8.12) that the Samejima graded response model is derived from
curves that look like
P(X ≥xm|θ) = P ∗
m(θ) = logit−1(θ −dm).
Thus, as there are three possible levels for each observable variable, we identify
the latent scale by specifying d1 = 0 and d2 = 1. These are treated as ﬁxed
in the analysis below.
The experts thought that Observable 1 was particularly easy, and so rec-
ommended initial parameter values included β11 = +1 for the intercept4,
along with α11DKM = 1 and α11Context = .5 for slopes. They did not have
information to expect Observables 2 through 7 would be especially hard or
easy, so they used an initial value of 0 for β12 −β17, and again values of 1
and .5 for the slopes for DKMendel and Context. Table 15.2 shows the condi-
tional probability table with those parameter values. Later we will be able to
compare these initial tables with probabilities that are revised in accordance
with information about them in the pilot data.
Table 15.2 Initial conditional distributions for observables 2–7 of Task 1
DKMendel Context
P(X = k)
θ∗
DKM
θ∗
C1
˜θ11
Low Medium High
−1
−1 −0.50 0.82
0.11
0.08
−1
1
0.50 0.62
0.20
0.18
0
−1
0.50 0.62
0.20
0.18
0
1
1.50 0.38
0.24
0.38
1
−1
1.50 0.38
0.24
0.38
1
1
2.50 0.18
0.20
0.62
The value θ∗
DKM is coded low=−1, medium=0, high=1;
θ∗
C is coded unfamiliar=−1, familiar=1; probabilities
calculated via Eq. 15.5 using α1jDKM = 1, α1jContext =.5,
β1j = 0, and ﬁxed values d1 = 0 and d2 = 1
A full speciﬁcation of the model requires a law for the parameters α1jDKM,
α1jContext, and β1j, where j is an index for the seven observable variables.
The ﬁrst intercept parameter was given the distribution β11 ∼N (1, 1), and
the rest were given the distribution β1j ∼N (0, 1). The slope parameters
α1jDKM ∼N+(1, 1) and α1jContext ∼N+(0.5, 1) for j = 1, . . . , 7, and where
N+(·) represents a normal distribution truncated at zero. (These truncated-
normal priors for slopes have their maxima at the initial values 1 and .5
respectively, but their means are higher.)
Finally, we need a distribution for ContextT M1. The context variable is
independent of all of the other (unobserved) variables, and is given the dis-
4 Recall that β11 is an intercept, not a diﬃculty, parameter. Hence, larger values
result in higher probabilities of success, i.e., easier tasks.

15.1 Specifying Prior Distributions
557
tribution ContextT M1 ∼Bernoulli(.5)—speciﬁcally, probabilities of .5 for the
values of −1 and +1. The context variable in this task and all the others
in the example are given this ﬁxed distribution. (Specifying any two values is
actually suﬃcient to eﬀect conditional dependence in the CPTs, but centering
them around zero and having the values be ±1 simpliﬁes the interpretation
of the parameters in the DiBello–Samejima model.)
Evidence Model 2
Task 2 (EM2) concerned a table (Fig. 14.8) that a student was asked to ﬁll
out with several statements about implications of the mode of inheritance. In
each case, the student was to indicate if this statement could be conﬁrmed or
rejected on the basis of data from the ﬁeld population alone, from the oﬀspring
of matings of known members of the ﬁeld population, and from the oﬀspring
of matings of the next generation after that. For example, it is a common mis-
conception that if there were more tan mice than black mice in the ﬁeld popu-
lation, then tan is the expression of a dominant allele. Maybe, but maybe not!
The recessive allele could be much more common than the dominant allele.
There are three variables in this cluster, posited by our experts to depend con-
junctively on DKMendel and WKInqry, and conditionally dependent beyond
these joint inﬂuences. The form of this EMF was shown as Fig. 14.11.
The DiBello–Samejima equations for the compensatory conjunctive distri-
bution is more complex than any of those explored in Sect. 8.5, but illustrates
the ﬂexibility of that procedure. As before, we need eﬀective thetas for each
of the parent variables. We again deﬁne θ∗
DKM using Eq. 15.3, and we deﬁne
θ∗
WKI by substituting WKInqry for DKMendel in that equation. For the con-
text variable, θ∗
C2 is deﬁned by substituting ContextT M2 for ContextT M1 in
Eq. 15.4.
According to the experts, both domain knowledge and working knowledge
about inquiry are required to solve this problem, therefore, the contribution to
the eﬀective theta from the proﬁciency variables should be min(θ∗
DKM, θ∗
WKI).
However, there is an additional term for the context eﬀect, the extra depen-
dency introduced because all three observables come from the same work
product. Combining these two terms gives the following eﬀective theta for
Observable j:
˜θ2j = α2jDKMWKI min(θ∗
DKM, θ∗
WKI) + α2jContextθ∗
C2 + β2j
(15.6)
The experts thought that in this task, Observables 1 and 2 were on the
hard side, and posited initial values of α21DKMWKI = 1, α21Context = 0.5
and β21 = −0.5. Plugging these values into Eq. 15.6 yields the conditional
probability table shown in Table 15.3.
Again, a complete speciﬁcation of the evidence model requires a prior for
the parameters for all three observable variables. Since the experts thought the
ﬁrst two observables were harder than the third, we set intercept parameters

558
15 The Biomass Measurement Model
Table 15.3 Initial conditional distributions for observable 1 of Task 2
DKMendel WKInqry Context
P(X = k)
θ∗
DKM
θ∗
W KI
θ∗
C2
˜θ11
Low Medium High
−1
−1
−1 −2.00 0.88
0.07
0.05
−1
0
−1 −2.00 0.88
0.07
0.05
−1
1
−1 −2.00 0.88
0.07
0.05
0
−1
−1 −2.00 0.88
0.07
0.05
1
−1
−1 −2.00 0.88
0.07
0.05
−1
−1
1 −1.00 0.73
0.15
0.12
−1
0
1 −1.00 0.73
0.15
0.12
−1
1
1 −1.00 0.73
0.15
0.12
0
−1
1 −1.00 0.73
0.15
0.12
1
−1
1 −1.00 0.73
0.15
0.12
0
0
−1 −1.00 0.73
0.15
0.12
0
1
−1 −1.00 0.73
0.15
0.12
0
1
−1 −1.00 0.73
0.15
0.12
0
0
1
0.00 0.50
0.23
0.27
0
1
1
0.00 0.50
0.23
0.27
0
1
1
0.00 0.50
0.23
0.27
1
1
−1
0.00 0.50
0.23
0.27
1
1
1
1.00 0.27
0.23
0.50
Both θ∗
DKM and θ∗
W KI are coded low = −1, medium = 0, and high = 1;
θ∗
C is coded unfamiliar=−1, familiar=1. Probabilities
calculated via Eq. 15.6 using α21DKMW KI = 1, α21Context = .5,
β21 = −.5, and ﬁxed values d1 = 0 and d2 = 1
β21, β22 ∼N(−0.5, 1), and β23 ∼N(0, 1). The slope parameters α2jDKMWKI ∼
N+(1, 1) and α2jContext ∼N+(0.5, 1) for j = 1, 2, 3.
Finally, we again need a distribution for ContextT M2. The context variable
is independent of all of the other (unobserved) variables, and as with Evidence
Model 1, we give it a ﬁxed Bernoulli(.5) distribution.
Evidence Model 3
Task 3 (EM3) consisted of three multiple-choice questions about implications
of forms of dominance. DKMendel is the only proﬁciency variable, and the
responses are posited to be conditionally independent. The form of this EMF
was shown in Fig. 14.12. Note that because these are multiple choice items
without a partial credit option, the middle category is not used for the observ-
ables, which are just either low or high. The Samejima-type graded response
model simpliﬁes to a one-parameter logistic model, with only one item loca-
tion, d = 0.
The eﬀective theta for DKMendel, θ∗
DKM is again coded using Eq. 15.3.
Adding a slope and intercept produces the eﬀective theta for each item. The
experts said all three are items of typical diﬃculty, so the initial intercept

15.1 Specifying Prior Distributions
559
parameters are 0, and the initial slope parameters are 1. (Thus the eﬀective
theta for the observables is identical to the eﬀective theta for DKMendel a
priori, but the model can allow the diﬃculty and discrimination of the item to
be adjusted to better ﬁt the observed data.) Equation 15.7 shows expression
for the eﬀective theta. Table 15.4 gives conditional response probabilities that
correspond to our initial values for the intercept and slope.
˜θ3j = α3jDKMθ∗
DKM + β3j
(15.7)
Table 15.4 Initial conditional probability distributions for all three observables of
Task 3
DKMendel
P(X = k)
θ∗
DKM
˜θ11 Low High
−1 −1.00 0.73 0.27
0
0.00 0.50 0.50
1
1.00 0.27 0.73
The value θ∗
DKM is coded low=−1, high=1
Again, priors are required for the parameters. The chosen priors are β3j ∼
N(0, 1) for the intercept and α3jDKM ∼N+(1, 1) for the slope. The three
observables are conditionally independent given the proﬁciency variable, so
there is no context variable for this evidence model.
Evidence Model 4
Task 4 (EM4) asked what Jos´e should do next, after having formalized his
hypothesis about the mode of inheritance of hair color based on the ﬁeld pop-
ulation (14.9). There is just one observable variable. The key to its solution
is a central tenet of inquiry in transmission genetics: Simply generating a
hypothesis that is consistent with a ﬁeld population is not suﬃcient to con-
clude a mode of inheritance; one must carry out crosses, test the hypothesis,
and revise if necessary. Our experts indicated that a student must know at
least a bit about the Mendelian model to respond to this question, but the
quality of the response would depend mainly on the ability to apply inquiry
skills in this domain. The evidence model fragment therefore must reﬂect an
inhibition relationship, in which a student must be above the low level of
DKMendel to have chances at making a high-quality response that increase
with increasing levels of WKInqry.
Figure 14.13 shows the structure of the EM fragment for Evidence Model 4,
where DKMendel is an inhibitor of WKInqry—note the stop sign as a symbol
for the structure of the distribution. Again, we assign eﬀect theta values for

560
15 The Biomass Measurement Model
WKInqry by substituting it for DKMendel in Eq. 15.3. The eﬀective theta for
the observable then becomes:
˜θ =

−1α4,1 + β4,1
DKMendel = low
θ∗
WKIα4,1 + β4,1
DKMendel > low
(15.8)
Again the eﬀective theta is entered into the DiBello–Samejima model with d1
and d2 ﬁxed at 0 and 1. Table 15.5 gives a set of conditional probabilities that
are obtained when α4,1 = 1 and β4,1 = 0.
Table 15.5 Initial conditional distribution for observable 1 of Task 4
DKMendel WKInqry
P(X = k)
θ∗
DKM
θ∗
W KI
˜θ11 Low Medium High
−1
−1 −1.00 0.62
0.20
0.18
−1
0 −1.00 0.62
0.20
0.18
−1
1 −1.00 0.62
0.20
0.18
0
−1 −1.00 0.62
0.20
0.18
1
−1 −1.00 0.62
0.20
0.18
0
0
0.00 0.38
0.24
0.38
1
0
0.00 0.38
0.24
0.38
0
1
1.00 0.18
0.20
0.62
1
1
1.00 0.18
0.20
0.62
Both θ∗
DKM and θ∗
W KI are coded low = −1, medium = 0, and high = 1
Finishing the model requires prior distributions for the two parameters.
Following the patterns used for the other distributions, we use β4,1 ∼N(0, 1)
and α4,1 ∼N+(1, 1).
15.1.3 Summary Statistics
Table 15.6 gives summary statistics for the prior distributions described above.
These statistics are based on 50,000 draws from the prior using the Gibbs
sampler, without the response data. Section 15.3.1 will compare these with
posteriors obtained after incorporating information from pilot testing. Note
that the prior distributions for all the item slopes of a given type are identical,
while the item diﬃculties vary in selected cases in accordance with the experts’
judgments of their diﬃculties.
We will also calculate an expected proﬁciency level or EAP (expected a
posteriori) score for each student. This is obtained by assigning a numeric
value to each of the proﬁciency levels (low = 1, medium = 2, high = 3)5 and
then taking the expectation over the probability that the student is at each
5 This particular score makes the implicit assumption that the diﬀerence between
low and medium is the same size as the diﬀerence between medium and high.

15.2 Pilot Testing
561
proﬁciency state. Before looking at a student’s response data and using the
prior means for the model parameters, the prior EAP score for each student
is 2.00; that is, before seeing anyone’s performance, our best guess is that
they are at the medium level. The wide dispersion of the Dirichlet priors on
proﬁciency variables, though, says we are quite willing to believe they might
be low or high, after seeing some responses that suggest this.
15.2 Pilot Testing
15.2.1 A Convenience Sample
Part of building a calibration sample is crafting an argument for why the
sampled individuals are representative of the population of interest. Random
sampling from that population builds the strongest possible argument. Often,
however, random sampling is prohibitively expensive or impractical, or per-
missions are diﬃcult to obtain. In those cases, researchers make do with less
optimal samples. However, they are still obliged to describe the methods for
gather the sample, so that future readers can judge the suitability of the
sample for the designated purpose.
The population of interest for Biomass was high school students taking an
AP Biology class; that is, primarily high school seniors with a strong science
background. Furthermore, as the interim assessment was meant to take place
within the context of the class unit on genetics, we needed students who had
at least a little bit of familiarity with the concepts and terminology of genetics.
When collecting a sample for calibrating Biomass, we were faced with
several problems. First, as we had almost no budget, we knew that we would
need to make a number of compromises. Second, as it was summer, we could
not simply walk into an AP biology class and take the sample. Third, the
sampled individuals would not necessarily have the correct background in
genetics. This last one we overcame by creating a mini tutorial providing
enough background in genetics to try be able to tackle the ﬁrst segment of the
Mice tasks. However, this in itself created another problem: tutorial and tasks
together took about 20 min to complete. As we only had Biomass running on
one computer6, this severely limited our throughput.
Our ﬁrst attempt at building a sample was to visit a summer program
in science enrichment for high school students at a nearby college. However,
6 Nominally, as Biomass is a web application, one computer, the server, can supply
the assessment to many other computers, the clients, which only need a web
browser. However, in practice, ﬁrewall and security restrictions made it diﬃcult
to run Biomass in a computer lab (without a lot of additional administrative
eﬀort). Also, Biomass required a fairly large monitor, or else the student had to
do a lot of scrolling. In practice, these restrictions forced us to mostly use one
computer for testing.

562
15 The Biomass Measurement Model
Table 15.6 Summary statistics of parameter prior distributions
Parameter
Prior
mean (SD)
Evidence model 1
Slopes for
α11DKM−
1.29 (0.79)
DKMendel
α17DKM
Slopes for
α11Context−
1.00 (0.70)
ContextT M1
α17Context
Intercepts
β11
1.00 (1.00)
β12 −β17
0.00 (1.00)
Evidence model 2
Slopes for
α21 −α23
1.29 (0.79)
conjunction
Slopes for
α21Context−
1.00 (0.70)
ContextT M2
α23Context
Intercepts
β21, β22
−0.50 (1.00)
β23
0.00 (1.00)
Evidence model 3
Slopes for
α31 −α33
1.29 (0.79)
DKMendel
Intercepts
β31 −β33
0.00 (1.00)
Evidence model 4
Slope
α4,1
1.29 (0.79)
Intercept
β4,1
0.00 (1.00)
Proﬁciency model
Distribution of λ1
0.30 (0.14)
DKMendel
λ2
0.40 (0.15)
λ3
0.30 (0.14)
Conditional
λ11
0.50 (0.15)
distribution of λ12
0.30 (0.14)
WKInqry
λ13
0.20 (0.12)
given
λ21
0.30 (0.14)
DKMendel
λ22
0.40 (0.15)
λ23
0.30 (0.14)
λ31
0.20 (0.12)
λ32
0.30 (0.14)
λ33
0.50 (0.15)
Individual student, before responses
DKMendel
λ1
2.00 (0.77)
WKInqry
λ2
2.00 (0.82)

15.2 Pilot Testing
563
we only were able to get 1 hour of testing time, and hence we only got three
students from this sampling eﬀort.
Our second attempt involved going to a local comic book shop that had an
open game night on Wednesday evenings. People would randomly drop into
that location all afternoon and evening, and the shop tended to be frequented
by students from the local high school and the local college; thus, it was at
least likely to attract students close to the correct age range.
The individuals who participated in this second data collection eﬀort
included some people who were at the extreme ends of the data collection
range, and who provided some interesting anecdotal evidence about how
Biomass was performing. The ﬁrst was a college graduate who was at the
time working as a pharmacy technician. She drew a Punnett square on a
sheet of scrap paper while attempting to solve the ﬁrst task. This was an
important knowledge representation in genetics, a class of observable we used
in later segments, but not the ﬁrst one. It was interesting that she retained
these knowledge representations from her earlier training. The second was a
middle school student who begged to be allowed to try even though he was
below the age cutoﬀ. He had recently completed a unit in genetics at middle
school, and thought that he really knew the material. And he did—at least in
terms of vocabulary and representations. But the tasks that required a great
deal of working knowledge, particularly knowledge of the scientiﬁc inquiry
process, really stumped him. This provides anecdotal support for splitting
domain knowledge and working knowledge in the proﬁciency model.
The ﬁnal source of data was a number of summer interns at ETS. These
were students in graduate school, who varied greatly as to the number of
science courses they had taken in high school and as undergraduates. On top
of that, many had been high school students or undergraduates in a diﬀerent
country and had taken science course in languages other than English, thus,
they may not have been familiar with the English names for various terms
from genetics.
Altogether, these three data collection eﬀorts yielded a ﬁeld trial sample of
28 individuals (Table 15.7). Calling this sample representative of the desired
population is an extreme stretch. We would be reluctant to make any strong
conclusions about Biomass, or actually adjust the parameters of the models,
on the basis of this rather haphazard ﬁeld trial. On the other hand, this sample
is useful to test the machinery for performing the parameter update. Unlike
simulated data, it is diﬃcult to predict how the data from this ﬁeld trial will
behave. Adjusting on the basis of these numbers should make the Biomass
more suitable for the population of which this ﬁeld trial is representative,
whatever that may be. The next sections overlook the obvious limitations in
the data collection and proceed to analyze the data.

564
15 The Biomass Measurement Model
Table 15.7 Observed responses
Student x11 x12
x13
x14 x15
x16
x17
x21
x22
x23
x31
x32
x33
x4
Mean
1
3
1
1
1
1
1
1
1
1
1
3
3
1
2
1.50
2
1
1
1
1
1
1
1
1
1
1
1
1
1
3
1.14
3
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1.00
4
3
1
1
1
1
1
1
1
1
1
1
1
3
2
1.36
5
3
3
3
1
3
1
1
1
1
1
1
3
3
3
2.00
6
3
1
1
1
1
1
1
1
1
1
3
3
1
3
1.57
7
3
1
1
1
1
1
1
1
1
2
1
3
1
3
1.50
8
3
1
3
1
3
1
1
1
1
2
1
3
3
2
1.86
9
1
1
1
1
1
1
1
1
1
1
1
1
1
2
1.07
10
3
1
1
1
1
1
1
1
1
1
1
3
3
2
1.50
11
3
3
3
1
3
1
1
2
1
1
1
3
3
2
2.00
12
3
3
3
1
3
1
1
1
1
2
1
3
1
2
1.86
13
3
3
3
1
3
1
1
1
1
2
3
3
3
3
2.21
14
3
3
3
1
3
1
1
1
1
1
1
3
1
3
1.86
15
3
1
1
1
1
1
1
2
1
1
3
3
3
2
1.71
16
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1.00
17
3
3
1
1
1
1
1
3
1
3
3
3
3
3
2.14
18
3
1
3
1
3
1
1
1
1
3
3
1
3
3
2.00
19
3
3
1
1
1
1
1
1
1
1
3
1
1
2
1.50
20
1
1
1
1
1
1
1
1
1
1
1
3
1
2
1.21
21
3
3
3
1
3
1
1
1
1
1
1
1
1
1
1.57
22
3
3
3
1
3
1
1
3
1
3
3
3
3
3
2.43
23
3
1
1
1
1
1
1
3
1
2
3
3
3
3
1.93
24
1
1
1
1
1
1
1
1
1
2
3
3
3
3
1.64
25
3
3
3
1
3
1
1
1
1
2
3
3
3
2
2.14
26
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1.00
27
3
1
1
1
1
1
1
1
1
2
3
3
1
3
1.64
28
3
1
1
1
1
1
1
1
1
1
3
3
3
2
1.64
Mean
2.50 1.71 1.71 1.00 1.71 1.00 1.00 1.29 1.00 1.50 1.93 2.36 2.00 2.29 1.64
Responses are coded: low = 1, medium = 2, high = 3
15.2.2 Item and other Exploratory Analyses
Before spending a lot of time with fancy analysis, it is worth looking at some
simple descriptive statistics that tell us in a basic way how the assessment
is working. Whether the eventual method for scoring is based on classical
test theory, IRT, or Bayes nets, the same type of descriptive statistics can be
used in item analysis: number right (or in this case, sum of the partial credit
scores) statistics for each examinee, and marginal distributions, or P+, for
each observable outcome variable.
Table 15.7 codes the responses low = 1, medium = 2, and high = 3. Note
the dearth of ‘2’ responses, except for the last observable. In most cases, the
students did well or poorly on most aspects of the tasks, with few performances
of intermediate quality—even though the average of all the responses was 1.62,
just about in the middle. For the observables from Task 3, x3,1, x3,2, and
x3,3, there are no 2 responses because these were multiple-choice items scored

15.2 Pilot Testing
565
dichotomously; incorrect responses were scored as low and correct responses
as high. For the other observables, it is worth going back and checking to
make sure that the evidence rules were implemented correctly. If, in fact, all
of the work products produced correctly reﬂect low and high performance,
it may be necessary to reconsider what partial credit is for this task (see
Exercise 15.12). This is one place where having a sample of students who
are studying the material in Biomass would be particularly important: There
is apt to be more instability and partial understanding among students just
learning and applying concepts than ones who have either completed their
study or not yet started it. It is possible that the ‘2’s missing from this sample
are particularly useful in formative assessment, and that is precisely why our
experts who taught this material crafted them for the assessment. We note too
that the students also showed a great range in performance: Nothing but the
lowest responses from Students 3 and 26, to a majority of 3’s for Student 22.
It is often worth checking data records on outlying observations like these.
A quick check of the logs reveals that Student 26 logged in with the user
name “try.” Thus, rather than a real subject attempting the assessment, this
observation may represent an administrator testing or demonstrating the sys-
tem. As there are only 28 students, this record was retained for the analysis.
However, such spurious records are common in ﬁeld trials and real data must
be carefully cleaned.
The items range from very diﬃcult (nobody did better than the lowest
response on x1,4, x1,6, x1,7, and x2,2) to very easy (most students answered
x1,1 correctly). Did these results accord with the experts’ prior expectations?
Yes, for the most part. There were three items for which they had opinions
other than “typical.” They expected Observable 1 of Task 1 to be easier, and
it turned out to be the easiest one in the study. They expected Observables 1
and 2 of Task 2 to be harder than typical, and they were. But the four observ-
ables noted above on which every student was rated low were not expected
to be diﬀerent from typical. This may be due again to the fact that the stu-
dents in the ﬁeld trial are not exactly the same as the ones the experts had
in mind as a target population. They thought about how hard a task would
be for a student who had been working through a unit on this material, and
would be familiar with the notation and expectations used in the prototype.
Our ﬁeld-trial students did not have this advantage, which could diﬀer from
one task to the next. A second plausible explanation is a coding error in the
implementation of the evidence rules, and it was worth going back to check
the work products to see if they all truly reﬂect low performance (they did).
Another possible explanation is that there is some problem with the task
design, perhaps unclear wording of the directions, that caused all of the ﬁeld
trial subjects to miss this particular aspect of the task. Small-sample trials,
where each subject is watched or video recorded, and oﬀers thoughts during
or after the experience, are particularly useful in early stages of developing
interactive tasks like these before collecting large calibration samples.

566
15 The Biomass Measurement Model
These kinds of statistics are useful even with a small pilot test sample
that is chosen for convenience rather than carefully chosen to be representa-
tive of the population. As seen above, relatively simple statistics can be used
to spot problems with the design and/or implementation of the tasks and
evidence rules, and thus provide an important step in the quality control of
the assessment.
15.3 Updating Based on Pilot Test Data
We have spent some eﬀort to build a Bayesian probability framework that
expresses the experts’ beliefs about the key relationships between knowledge
and performance in the Biomass tasks. The probability distributions express
the qualitative structure of the relationships, and task and examinee param-
eters express the quantitative relationships within that structure. Note that
the experts’ opinion about the parameters is expressed through a collection of
prior laws which describe not only the experts’ best guess as to the parameter
values, but also their degree of certainty about those estimates. Overlook-
ing its limits, we can use the ﬁeld trial responses to show how the Bayesian
machinery uses data to update the model.
This section describes three applications of the ﬁeld trial data to the
Biomass model. Section 15.3.1 describes using the Bayesian machinery to
update the parameter values. Section 15.3.2 looks at some statistics for model
ﬁt. Finally, Sect. 15.3.3 shows a simple (hypothetical) validity check.
15.3.1 Posterior Distributions
WinBUGS (Thomas et al. 1992; Lunn et al. 2000) can calculate the posterior
distribution, using the MCMC approach of Sect. 9.6. To do this, the prior
distribution given in Sect. 15.1 must be expressed as BUGS code.7 After
reading the outcome data shown in Table 15.7, BUGS generates a speciﬁed
number draws from the posterior distribution. As there are only 28 response
vectors, the MCMC algorithm runs quickly and it is easy to generate many
samples from the posterior. Tables 15.9 and 15.10 give summaries of posterior
distributions for parameters conditional on this data, along with the priors to
facilitate comparison. We will look at population parameters, evidence-model
parameters, and student proﬁciency parameters in turn.
The prior distributions we posited for the parameters were fairly mild,
but the sample size was also fairly small. The prior distributions—that is, the
substantive theory, the task design, the structures relating performance to
proﬁciency, and initial beliefs about direction and strength of evidence—are
7 The analysis described in this section was originally done in WinBUGS (Mislevy
et al. 2002a); however, during the editing of this chapter we reran the MCMC
simulations using JAGS (Plummer 2012), which is essentially similar.

15.3 Updating Based on Pilot Test Data
567
thus essential to reasoning about students from their performance. We will see
that within this structure, estimates for some parameters were substantially
revised on the basis of even the small calibration sample, while others changed
very little. In addition to the means of the parameters’ posterior laws, we will
look at the variances to quantify the relative impact of the data on various
parameters. As the precision of the law (the reciprocal of the variance) quan-
tiﬁes the amount of information in the distribution, the increase in precision
provides an indication of how much information was gained from the sample.
The percentage increase in precision is calculated as follows:
% Increase in precision = 100 × (posterior SD)−2 −(prior SD)−2
(prior SD)−2
.
A value of zero indicates no new information, while a value of 100 means there
was twice as much information about a parameter after seeing the data than
before seeing it.
Table 15.8 Summary statistics of prior and posterior population parameter distri-
butions
Parameter
Prior
Posterior
% Increase
Mean (SD)
Mean (SD)
in precision
Distribution of λ1
0.30
(0.14) 0.30
(0.10)
97
DKMendel
λ2
0.40
(0.15) 0.47
(0.12)
48
λ3
0.30
(0.14) 0.22
(0.11)
65
Conditional
λ11 0.50
(0.15) 0.50
(0.15)
0
distribution of λ12 0.30
(0.14) 0.30
(0.14)
0
WKInqry
λ13 0.20
(0.12) 0.20
(0.12)
2
given
λ21 0.30
(0.14) 0.32
(0.15)
−11
DKMendel
λ22 0.40
(0.15) 0.41
(0.15)
−4
λ23 0.30
(0.14) 0.27
(0.13)
8
λ31 0.20
(0.12) 0.19
(0.11)
13
λ32 0.30
(0.14) 0.31
(0.14)
−5
λ33 0.50
(0.15) 0.50
(0.15)
0
Table 15.8 shows the posterior means and variances and the percent-
age increase in precision in the laws for the proﬁciency model parameters.
There are moderate increases in precision for the distribution of DKMendel,
since every student contributes something, with information from all of their
responses (sometimes confounded with information about WKInqry). The
direction of the change is to move belief about the distribution of WKIn-
qry in the sample from high to medium; that is, the mean of the law for λ3
decreases from .30 to .22, while the mean for λ2 increases from .40 to .47. The
assessment was more diﬃcult for the sample than the priors anticipated.

568
15 The Biomass Measurement Model
Table 15.9 Summary statistics of item parameter distributions
Parameter
Prior
Posterior
% Increase
Mean (SD)
Mean (SD)
in precision
Evidence model 1
Slopes for
α11DKM
1.29
(0.79) 2.08
(0.75)
12
DKMendel
α12DKM
1.29
(0.79) 1.22
(0.71)
26
α13DKM
1.29
(0.79) 1.02
(0.64)
53
α14DKM
1.29
(0.79) 0.90
(0.61)
66
α15DKM
1.29
(0.79) 1.02
(0.64)
52
α16DKM
1.29
(0.79) 0.90
(0.61)
65
α17DKM
1.29
(0.79) 0.90
(0.62)
65
Slopes for
α11Context 1.00
(0.70) 1.14
(0.53)
75
ContextT M1 α12Context 1.00
(0.70) 1.93
(0.52)
77
α13Context 1.00
(0.70) 2.96
(0.61)
32
α14Context 1.00
(0.70) 0.62
(0.43)
172
α15Context 1.00
(0.70) 2.95
(0.61)
30
α16Context 1.00
(0.70) 0.61
(0.42)
172
α17Context 1.00
(0.70) 0.62
(0.43)
168
Intercepts
β11
1.00 (1.00)
2.60 (0.58)
196
β12
0.00 (1.00)
0.04 (0.53)
255
β13
0.00 (1.00)
0.05 (0.62)
165
β14
0.00 (1.00) −2.54 (0.60)
175
β15
0.00 (1.00)
0.06 (0.61)
166
β16
0.00 (1.00) −2.54 (0.61)
170
β17
0.00 (1.00) −2.54 (0.61)
173
Evidence model 2
Slopes for
α21
1.29
(0.79) 2.07
(0.79)
1
conjunction α22
1.29
(0.79) 1.19
(0.70)
30
α23
1.29
(0.79) 1.46
(0.68)
34
Slopes for
α21Context 1.00
(0.70) 0.81
(0.56)
56
ContextT M2 α22Context 1.00
(0.70) 0.61
(0.44)
143
α23Context 1.00
(0.70) 0.62
(0.45)
144
Intercepts
β21
−0.50 (1.00) −1.12 (0.61)
166
β22
−0.50 (1.00) −2.59 (0.65)
136
β23
0.00 (1.00) −0.19 (0.49)
314
Evidence model 3
Slopes for
α31
1.29
(0.79) 1.91
(0.74)
14
DKMendel
α32
1.29
(0.79) 1.85
(0.72)
23
α33
1.29
(0.79) 1.89
(0.74)
16
Intercepts
β31
0.00 (1.00)
0.01 (0.50)
306
β32
0.00 (1.00)
1.00 (0.50)
298
β33
0.00 (1.00)
0.17 (0.50)
308
Evidence model 4
Slope
α4,1
1.29
(0.79) 0.64
(0.42)
252
Intercept
β4,1
0.00 (1.00)
1.14 (0.39)
565

15.3 Updating Based on Pilot Test Data
569
Table 15.10 Prior and posterior expected proﬁciency levels
DKMendel
WKInqry
Student
Prior
Posterior
% Increase
Prior
Posterior
% Increase
Mean (SD)
Mean (SD)
in precision Mean (SD)
Mean (SD)
in precision
1 2.00
(0.78) 2.01
(0.50)
141 2.00
(0.81) 1.82
(0.78)
8
2 2.00
(0.78) 1.06
(0.24)
948 2.00
(0.81) 1.69
(0.78)
10
3 2.00
(0.78) 1.03
(0.17)
2020 2.00
(0.81) 1.70
(0.78)
9
4 2.00
(0.78) 1.66
(0.53)
113 2.00
(0.81) 1.79
(0.79)
7
5 2.00
(0.78) 2.10
(0.48)
165 2.00
(0.81) 1.61
(0.73)
25
6 2.00
(0.78) 2.06
(0.46)
187 2.00
(0.81) 1.61
(0.73)
26
7 2.00
(0.78) 1.85
(0.46)
187 2.00
(0.81) 1.86
(0.75)
19
8 2.00
(0.78) 1.91
(0.51)
129 2.00
(0.81) 2.03
(0.76)
16
9 2.00
(0.78) 1.04
(0.19)
1610 2.00
(0.81) 1.71
(0.78)
9
10 2.00
(0.78) 2.00
(0.50)
140 2.00
(0.81) 1.83
(0.79)
7
11 2.00
(0.78) 2.17
(0.48)
155 2.00
(0.81) 2.22
(0.69)
38
12 2.00
(0.78) 1.82
(0.49)
146 2.00
(0.81) 2.01
(0.76)
14
13 2.00
(0.78) 2.47
(0.51)
127 2.00
(0.81) 1.91
(0.73)
23
14 2.00
(0.78) 1.79
(0.49)
147 2.00
(0.81) 1.63
(0.74)
20
15 2.00
(0.78) 2.46
(0.52)
125 2.00
(0.81) 2.28
(0.68)
43
16 2.00
(0.78) 1.03
(0.16)
2183 2.00
(0.81) 1.71
(0.78)
8
17 2.00
(0.78) 2.86
(0.35)
390 2.00
(0.81) 2.71
(0.51)
159
18 2.00
(0.78) 2.00
(0.46)
185 2.00
(0.81) 1.98
(0.73)
25
19 2.00
(0.78) 1.78
(0.58)
77 2.00
(0.81) 1.80
(0.79)
7
20 2.00
(0.78) 1.19
(0.39)
289 2.00
(0.81) 1.73
(0.78)
7
21 2.00
(0.78) 1.29
(0.46)
179 2.00
(0.81) 1.78
(0.80)
3
22 2.00
(0.78) 2.73
(0.44)
204 2.00
(0.81) 2.64
(0.54)
127
23 2.00
(0.78) 2.59
(0.50)
143 2.00
(0.81) 2.46
(0.62)
75
24 2.00
(0.78) 2.03
(0.43)
219 2.00
(0.81) 1.89
(0.74)
21
25 2.00
(0.78) 2.47
(0.52)
121 2.00
(0.81) 2.13
(0.72)
26
26 2.00
(0.78) 1.03
(0.16)
2150 2.00
(0.81) 1.71
(0.79)
7
27 2.00
(0.78) 2.10
(0.44)
212 2.00
(0.81) 1.88
(0.74)
22
28 2.00
(0.78) 2.37
(0.53)
113 2.00
(0.81) 1.83
(0.76)
14
The values for the examinee priors are means and standard deviations of the
student proﬁciency variables coded high = 3, medium = 2, and low = 1
There is virtually no change in the conditional distributions of WKInqry
given DKMendel. One reason is related to the eﬀective size of the sample
for the conditional distribution. As the subjects are split over three diﬀerent
possible values of DKMendel, the eﬀective sample size is more like 9 sub-
jects than 28. Furthermore, the value of DKMendel is not known with cer-
tainty, which further reduces the eﬀective sample size (Mislevy 1984). A sec-
ond reason is related to the test length. While almost all observables provide
information about DKMendel, only observables from Tasks 2 and 4 provide
information about WKInqry (an eﬀective test length of four observables com-
pared to 14 for DKMendel). Another problem comes with the way that
DKMendel and WKInqry interact to inﬂuence the observables in the ﬁrst
four tasks. In all cases where both variables are present, the relationship type
is conjunctive (the inhibitor relationship is a special type of conjunctive rela-

570
15 The Biomass Measurement Model
tionship). Unless DKMendel is at least at the medium level, then such tasks
provide little information about WKInqry.
Task parameter posteriors showed means that departed signiﬁcantly from
the priors. The prior means for the slopes for Context variables, for example,
were initially all at 1.00; their posterior means ranged from 0.61 up to 2.96,
which indicate from small conditional dependence for some observables to
quite substantial. The prior means for the slopes for proﬁciency variables were
initially 1.29, while their posterior means ranged from 0.64 to 2.08. Intercept
parameter means, which were initially at 0 for typical items, ranged from -2.5
for items on which no one succeeded, up to 2.60, for the ﬁrst observable on
Task 1, where most of the students did well.
Before looking more closely at each evidence model, we note from the
increase in precision values that the sample data provided notably more infor-
mation about intercept parameters than it did about slope parameters. Pre-
cision for intercepts increased between 165 and 565 %, or one-and-a-half to
ﬁve-and-a-half times as much information came from the data as from the
priors. This is consistent with item parameter estimation in IRT, where inter-
cepts and threshold parameters are easier to estimate than item slopes.
Task 1 was representing the mode of inheritance for mice coat color. The
seven observables in Evidence Model 1 concerned aspects of correctness and
consistency in a student’s re-expression of the mode of inheritance for coat
color.The experts thought the ﬁrst item would be easier than average, but
did not express expectations other than “typical” for the other items. In the
pilot data, Observable 1 was in fact quite easy, and the posterior mean for its
intercept moved substantially, from 1.00 to 2.60.
Observables 4, 6, and 7 were quite diﬃcult for these students: nobody got
a score above 1. The means for their intercepts went from 0.00 to −2.54. This
is a problem that we already observed during the item analysis (Sect. 15.2.2):
nobody in the ﬁeld trial got an observed outcome other than low. Perhaps
there is a problem with the task design (e.g., instructions are unclear) or the
evidence rules, to be investigated with user studies.
The remaining three observables, 2, 3, and 5, were about half and half
1s and 3s. The posterior means of their intercepts were all around 0, nearly
the same as their prior means, but with about 170 % more precision. Even a
small data set was enough to move the posteriors for the intercepts notably
to reﬂect their apparent diﬀerence in diﬃculty.
The distributions for the slopes for DKMendel didn’t change much at
all, except for Observation 1; its mean moved from 1.29 to 2.08. Not many
students missed this item, but the ones who did didn’t do well on other tasks
involving DKMendel either.
The slopes for ContextT M1 showed an interesting pattern. They were very
high for the three middle-diﬃculty items, as students tended to do quite well or
quite poorly on all three—much less variation than would have been expected
had the responses been conditionally independent given DKMendel. This is

15.3 Updating Based on Pilot Test Data
571
Table 15.11 Revised conditional distributions for observable 3 of Task 1
DKMendel Context
P(X = k)
θ∗
DKM
θ∗
C1
˜θ11 Low Medium High
−1
−1 −3.10 0.96
0.03
0.02
−1
1 −0.67 0.66
0.18
0.16
0
−1 −1.17 0.76
0.13
0.10
0
1
1.26 0.22
0.21
0.57
1
−1
0.75 0.32
0.24
0.44
1
1
3.19 0.04
0.06
0.90
Equation 15.5 with α13DKM = 1.93, α13Context = 1.22, and β13 = 0.04
probably due to understanding or not understanding the particulars of this
task, above and beyond Mendelian Knowledge. (The ContextT M1 slopes were
much lower for the three observables that nearly everyone missed and for the
one that most students got right, in a manner less strongly related to the
middling three conditionally dependent items.) This is potentially a problem,
as it may greatly decrease the amount of information that comes from this
task. However, in this case it may be a problem with the sample. Recall that
Task 1 involves ﬁlling out a chart to correspond to the mode of inheritance for
the mice. Examinees who understood what was required in this task, could
likely do all parts of the task. Examinees who didn’t understand the represen-
tation may have just had diﬃculty ﬁguring out what was required. Biomass
was intended to be used in the context of the a Biology class where such rep-
resentations would appear in the course text and in classroom work before
the students were expected to use them. Thus, if the ﬁeld data were more
representative, this problem might not appear. If it did, it might be better to
drop or merge the observables.
The slope and intercept parameter values determine the entries in the con-
ditional probability matrices for Evidence Model 1. Observables 3 and 4 of
Task 1 started with the same initial conditional probability tables (Table 15.2).
Tables 15.11 and 15.12 contain the revised conditional probabilities for these
observables. They are calculated through the DiBello–Samejima structure
with the posterior means of their respective task parameters. The revised
conditional distributions for Observable 3 show it is much easier than Observ-
able 4. Even a student with the high Mendelian Knowledge and familiarity
with this task context is only modeled as having 12 % chances of getting a
3 on Observable 4. Also, in Table 15.11 the diﬀerences in the rows with the
same DKMendel value but diﬀerent Context values show how it is expressed
that Observable 3 is conditionally associated strongly with other observables
within Task 1.
Task 2 is the population attribute table (Fig. 14.8), and Evidence Model 2
contains the three observables that indicate whether, in each of three columns
(kinds of populations), a student correctly identiﬁed all inferences that could

572
15 The Biomass Measurement Model
Table 15.12 Revised conditional probability table for observable 4 of Task 1
DKMendel Context
P(X = k)
θ∗
DKM
θ∗
C1
˜θ1,3 Low Medium High
−1
−1 −4.06 0.98
0.01
0.01
-1
1 −2.82 0.94
0.03
0.02
0
−1 −3.16 0.96
0.03
0.02
0
1 −1.93 0.87
0.08
0.05
1
−1 −2.27 0.91
0.06
0.04
1
1 −1.03 0.74
0.15
0.12
Equation 15.5 with α14DKM = 0.90, α14Context = 0.62, and β14 = −2.544
be drawn from a given kind of population, not all but the critical ones, or
missing the critical ones (high, medium, and low levels of performance respec-
tively).
The intercept posteriors reﬂected increased precision, with means moving
to indicate the success of the students in identifying the inferential properties
of the mice populations. The three observables of Task 2 showed slightly
increased slopes for the conjunction of DMendel and WKInqry but with little
increase in precision, and lower slopes for ContextT M2. It is intriguing to see
that evidence is particularly weak for the slope parameters of the conjunction
of DKMendel and WKInqry in Evidence Model 2. Note that the combination
of high on both DKMendel and WKInqry is rare in the ﬁeld test population.
Further investigation (in particular, a better sample) is needed to determine
whether this is a pervasive characteristic of combinations such as conjunctions
and disjunctions.
The revised conditional probability matrix for Observable 1 of Task 2 is
shown in Table 15.13, to be compared with the initial probabilities shown in
Table 15.3. This is another diﬃcult item, although not as diﬃcult as Observ-
able 4 of Task 1. The highest performing students succeeded on this task. The
ﬁnal row of the conditional probability table reﬂects that high performance
here is most likely to arise from high levels of understanding the Mendelian
model, inferential reasoning from the diﬀerent kinds of populations, and famil-
iarity with the context—that is, the situations and representations reﬂected
in the population attribute table.
Task 3 was three multiple-choice items concerning aspects of the Mendelian
model. The three observables are right/wrong indicators of correctness, and
are modeled as conditionally independent given DKMendel. The evidence
model fragments are based on the discrete two-parameter logistic model. This
task is thus most similar to familiar IRT modeling of unidimensional multiple-
choice items. The posterior means of the slope parameters are higher than the
prior means (nearly 2 in each case), although not estimated very precisely.
The posterior means for their intercepts reﬂect medium diﬃculty for these
students, and posterior precision increased by 300 % over the prior.

15.3 Updating Based on Pilot Test Data
573
Table 15.13 Revised conditional distributions for observable 1 of Task 2
DKMendel WKInqry Context
P(X = k)
θ∗
DKM
θ∗
W KI
θ∗
C2
˜θ11 Low Medium Migh
−1
−1
−1 −4.00 0.98
0.01
0.01
−1
0
−1 −4.00 0.98
0.01
0.01
−1
1
−1 −4.00 0.98
0.01
0.01
0
−1
−1 −4.00 0.98
0.01
0.01
1
−1
−1 −4.00 0.98
0.01
0.01
−1
−1
1 −2.38 0.92
0.05
0.03
−1
0
1 −2.38 0.92
0.05
0.03
−1
1
1 −2.38 0.92
0.05
0.03
0
−1
1 −2.38 0.92
0.05
0.03
1
−1
1 −2.38 0.92
0.05
0.03
0
0
−1 −1.93 0.92
0.05
0.03
0
1
−1 −1.93 0.92
0.05
0.03
0
1
−1 −1.93 0.92
0.05
0.03
0
0
1 −0.31 0.58
0.21
0.21
0
1
1 −0.31 0.58
0.21
0.21
0
1
1 −0.31 0.58
0.21
0.21
1
1
−1
0.13 0.47
0.24
0.30
1
1
1
1.75 0.15
0.17
0.68
Calculated via Eq. 15.6 using α21DKMW KI = 2.07, α21Context = 0.81,
β21 = −1.12, and ﬁxed values d1 = 0 and d2 = 1
These items provide a good deal of evidence about Mendelian model knowl-
edge (relatively speaking; they are just three observables. As such are partic-
ularly useful in disambiguating the evidence about WKInqry in the tasks that
provide evidence about inquiry skills in the context of the Mendelian model,
such as Tasks 1, 2, and 4.
Task 4 concerns the next step in the investigation. It targets WKInqry,
but requires at least a medium level on DKMendel. As the structure of the
conditional probability matrix shows (Table 15.5), this inhibitor relationship
means that even students who understand inquiry fairly well are unlikely to
succeed if they don’t know enough about the Mendelian model to apply their
knowledge to the situation at hand. Compared to all the other observables in
the analysis, the data provided most information about the slope and intercept
parameters in this evidence model: 252 % for the slope, 565 % for the intercept.
Its intercept indicates it was relatively easy for this sample. The slope is not
high, having posterior mean of 0.64. As a short series of choices to answer
questions, there is less information than an open-ended explanation might
have provided, or an analysis of subsequent steps a student would actually
take in an investigation.
Looking at posterior EAP scores for individual students (Table 15.10),
we see that the information in 14 responses each from 28 students was suﬃ-

574
15 The Biomass Measurement Model
cient to impact distributions for individual student posteriors for DKMendel
substantially, but it had a more modest eﬀect on WKInqry. We noted ear-
lier the impact on the distribution of the proﬁciency variables (i.e., the
λ’s, Table 15.8): there is a strong eﬀect on the population distribution for
DKMendel but the posterior for WKInqry has almost the same variance as
the prior.
Table 15.8 also shows the increase in precision for the posterior distribu-
tions for the proﬁciency variable distributions for each student in the ﬁeld
trial. The means are expected values over proﬁciency states coded as integers,
so high precision corresponds to probability concentrated on one particular
value. Thus, posterior precision is very high for students who performed at
high levels on all tasks or at low levels on all tasks. Almost all of their posterior
probability is on the highest or the lowest value of an proﬁciency variable.
In general, we learn more about DKMendel than about WKInqry, mainly
because there are more observables that provide information about DKMendel.
The increases in precision are quite substantial for DKMendel. However, for
only those students who did quite well overall is the posterior for WKInqry
more precise. It also has higher posterior means for those students. For stu-
dents who did poorly overall, the fact that all of the observables provided
information about DKMendel meant that we could be more conﬁdent that
their proﬁciency was low, but the conjunctive relationships with WKInqry
meant we obtained little information about their inquiry proﬁciency. This
assessment can’t really tell us if they would have been able to perform well
in inquiry tasks that involved some other topic with which they were more
proﬁcient. Their WKInqry posteriors are not much diﬀerent than the priors.
Note that this interpretation depends not on just the data, but the theory
and expert opinion about the nature and relationships of domain knowledge
and inquiry capabilities in science.
The full Biomass assessment, spanning 28 segments each with multiple
tasks, would provide greater increases in precision. However, even with this
short test we are pretty sure that a student who does poorly on most of
the observables is low and a student who does well is high with regard to
Mendelian Knowledge. We are far less sure about inquiry skills; since our
information about them is confounded with Mendelian knowledge and there
are far fewer observables, we have higher conﬁdence only for those students
who performed well on those tasks that required both proﬁciencies. For these
students, we can infer that their understanding of inquiry was high as reﬂected
in the context of of the Mendelian model. Whether they would show good
inquiry skill in the context of other models and other domains would require
evidence obtained in such situations.
We can quantify how much we learned about students in a Bayesian
approximation of reliability. Consider a latent proﬁciency variable θ on a
measured scale, and data x. An analogue of reliability is the proportion of the
variance of θ accounted for by x. The numerator is the variance of students’
EAP scores, and the denominator is the sum of this and the average of their

15.3 Updating Based on Pilot Test Data
575
posterior variances:
ρ ≈
V ar[E(θ|x)]
V ar[E(θ|x)] + E[V ar(θ|x)].
(15.9)
In the pilot data, the reliabilities were .62 for DKMendel and .14 for WKInqry.
Not bad at all for DKMendel, given there were only 14 observations; useful
enough to distinguish students who are clearly having diﬃculty and those who
are clearly doing well, but not accurate enough for high-stakes inferences. Not
good at all for WKInqry. Clearly more evidence would be needed even for
low-stakes purposes.
15.3.2 Some Observations on Model Fit
Model criticism is an essential facet of Bayesian (or any other) statistical infer-
ence. When the data do not accord well with the model, then the inferences
that probability-based reasoning allows us to draw through models are sus-
pect. This brief section illustrates some techniques used to examining ﬁt in
the kinds of models we have discussed in this volume.
The particular technique we are exploring is the use of preposterior pre-
dictive data sets, created in the course of MCMC iterations (Sect. 10.2). For
each observed response ximj in the realized data, we can deﬁne another vari-
able yimj that follows exactly the same distribution we have proposed and ﬁt
for ximj but is never observed. If our model is correct, the actual data are a
plausible draw from the predicted distribution of the shadow data. Thus, the
distribution of the shadow data or any summary statistic of it that is accu-
mulated over the MCMC iterations constitutes a tailor-made null distribution
against which to evaluate how surprising the data are in light of the model
we have proposed.
Table 15.14 presents one draw of simulated preposterior predictive responses.
The last rows and columns give provide the average observable sum scores
and item scores for both the ﬁeld trial data (x mean) and the shadow data
(y mean). Note that the averages for both observables and students approx-
imate those of the observed data closely, including the observables on which
no actual students did better than low. There are a few more ‘2’ responses
than in the actual data, but still most observed values are either ‘1’ or ‘3.’
The marginal means we have shown are just one possibility for a test
statistic. Any statistic of actual responses, such as correlations and joint dis-
tributions, could be calculated on the shadow data set as well. Because the dis-
tribution of the test statistic can be accumulated over iterations, the posterior
predictive distribution is an empirical null distribution for the test statistics,
which can be used to evaluate how typical or how surprising the corresponding
feature of the real data was.
We extended the BUGS code to add a preposterior predictive data distri-
bution, and then used an index of examinee ﬁt as the test statistic. Deﬁne the

576
15 The Biomass Measurement Model
Table 15.14 A set of simulated preposterior predictive responses
Student
y11
y12
y13
y14
y15
y16
y17
y21
y22
y23
y31
y32
y33
y4
y mean x mean
1
3
1
1
1
1
1
1
1
1
3
3
3
1
3
1.71
1.50
2
1
1
1
1
1
1
1
1
1
1
1
1
1
3
1.14
1.14
3
3
1
1
1
1
1
1
1
1
1
1
1
1
3
1.29
1.00
4
1
1
1
2
3
1
1
2
1
1
1
3
1
1
1.43
1.36
5
3
2
3
1
3
1
1
1
3
2
3
3
3
3
2.29
2.00
6
1
1
1
1
1
1
1
1
1
1
3
1
1
3
1.29
1.57
7
2
1
1
1
1
1
1
1
1
1
1
1
1
3
1.21
1.50
8
3
3
3
1
3
1
1
1
1
3
1
3
3
1
2.00
1.86
9
1
1
1
1
1
1
1
1
1
1
1
1
1
2
1.07
1.07
10
3
1
1
1
1
1
1
1
1
2
1
1
3
2
1.43
1.50
11
3
3
3
1
3
1
1
1
3
1
1
3
3
3
2.14
2.00
12
3
3
3
1
3
1
1
1
1
2
1
3
3
3
2.07
1.86
13
3
3
3
1
3
1
1
1
1
2
1
3
3
3
2.07
2.21
14
3
2
3
1
3
1
1
1
1
2
3
3
3
3
2.14
1.86
15
3
1
1
1
3
1
1
1
1
2
3
3
3
1
1.79
1.71
16
1
1
1
1
1
1
1
1
1
1
1
1
1
2
1.07
1.00
17
3
1
1
1
1
1
1
1
1
2
3
3
3
1
1.64
2.14
18
3
3
1
1
3
1
1
2
1
2
3
3
3
2
2.07
2.00
19
1
1
1
1
3
1
1
1
1
1
3
3
1
1
1.43
1.50
20
1
1
1
1
1
1
1
1
1
1
1
1
1
3
1.14
1.21
21
3
3
3
1
3
1
1
1
1
1
3
3
3
1
2.00
1.57
22
3
3
3
1
3
1
1
3
1
3
3
3
3
2
2.36
2.43
23
3
1
1
1
2
1
1
3
1
1
3
1
3
1
1.64
1.93
24
3
1
1
1
1
1
1
1
1
3
1
1
1
3
1.43
1.64
25
3
2
3
1
3
1
1
1
1
3
1
3
3
3
2.07
2.14
26
1
1
1
1
1
1
1
1
1
1
1
1
1
3
1.14
1.00
27
3
3
1
1
2
1
1
1
1
1
1
3
1
2
1.57
1.64
28
2
1
1
1
1
1
1
1
1
1
1
3
1
2
1.29
1.64
y Mean
2.36 1.68 1.64 1.04 2.00 1.00 1.00
1.21 1.14 1.64
1.79 2.21 2.00
2.25
1.64
x Mean
2.50 1.71 1.71 1.00 1.71 1.00 1.00
1.29 1.00 1.50
1.93 2.36 2.00
2.29
1.64
The x mean refers to the original data, the y mean refers to the shadow data
ﬁt mean square for Examinee i as follows:
Zi = 1
14

m

j
(ximj −E [ximj])2 ,
(15.10)
where responses are coded high = 3, medium = 2, and low = 1, and
E [ximj] =
3

k=1
kP(ximj = k|DKMendeli, WKInqryi, αm, βm) .
(15.11)
In each iteration of the MCMC sampler, there are sampled values for both the
proﬁciency variables and the evidence model parameters, so the Eq. 15.11 can
be easily calculated. Then Eq. 15.10 is calculated with both the observed and
shadow data, producing two realizations of the examinee ﬁt index. The rele-
vant index is the proportion of iterations in which the ﬁt mean square for the
y’s is greater than the one for the x’s. This is known as the posterior predictive
p-value. Note that diﬀerent values are used for the evidence model parameters

15.3 Updating Based on Pilot Test Data
577
(and the proﬁciency model parameters as well) for each iteration, so over the
course of MCMC cycles, the estimates take into account uncertainty about
the values of the model parameters, as well as the examinee proﬁciency.
One run with 30,000 iterations produced p-values across the 28 examinees
between .88 for Examinee 9 (the best ﬁt) and .09 for Examinee 22 (the worst
ﬁt). Examinee 22’s pattern is somewhat uncommon because of high values
for all observables except for the four observables (Task 1: Observables 4,
6 and 7, and Task 2, Observable 2) on which nobody scored well. We would
have expected some correct responses from someone who performed so well on
the rest of the assessment. This may be a problem as the posterior conditional
probability tables for those observables (e.g., Table 15.12) still have a prob-
ability of over .5 for a low outcome when both the DKMendel and Context
variable are in their highest states.
The fact that the lowest empirical p-value was only .09 caused us some
concern about the power of the test. We did a second run with an additional
ﬁctitious response vector, one with high values for the harder observables and
low values for the easier ones:
xbadﬁt = (1, 1, 1, 3, 1, 3, 3, 3, 3, 3, 3, 1, 1, 1) .
We were comforted to see that of 20,000 draws of a shadow response pat-
tern to this maximally bad ﬁtting pattern, only 4 had a higher mean square–an
empirical p-value of .0002. When a response vector is seriously out of sorts,
this index will ﬂag it.
However, this examinee ﬁt test does suﬀer from low power. First, there
is the generally conservative nature of the posterior predictive data proce-
dure: it generally takes a large deviation from the expected to overcome the
uncertainty about the parameters (Sinharay 2005). The second is the short
length of the test (just the ﬁrst segment of Biomass). Getting both informa-
tion about an examinee’s ability on two proﬁciency variables and information
about model ﬁt is asking a lot of 14 observables. A longer test would have
more power to detect potentially interesting response patterns.
15.3.3 A Quick Validity Check
Messick (1989) describes validity as “an integrated evaluative judgment of
the degree to which empirical evidence and theoretical rationales support the
adequacy and appropriateness of inferences and actions based on test scores
or other modes of assessment” (p. 13; emphasis original). We will touch here
on aspects of both empirical evidence and theoretical rationales.
Embretson (1983) distinguished lines of validity argumentation that con-
cerned assessment design, that is, why tasks created and scored in such-and-
such way ought to provide evidence about the construct of interest, and the
subsequent relationships of scores from an assessment with past, concurrent,

578
15 The Biomass Measurement Model
or future test scores and events. She called these “construct representation”
and “nomothetic span” sources of validity evidence.
For Biomass, the ECD process described in Chap. 14 inherently provides
construct representation evidence. The entire process of Domain Analysis in
Biomass was meant expressly to gather information from research and expe-
rience in the domain. Domain Modeling was meant expressly to lay out the
theoretical rationale for why what features of performance in what kinds of
situations ought to provide evidence about the targeted proﬁciencies, for the
targeted population, for the the targeted purposes. The translation of this
argument into ECD student, evidence, and task models links the argument to
the structure of the pieces of machinery that instantiate the argument. The
model building, model ﬁtting, and model criticism support the link of rea-
soning from performances to scores. The ECD framework makes explicit and
shareable (and re-usable) coordinated lines of validity argumentation that are
often reside only in test developers’ heads and work processes.
Looking at the relationships between Biomass EAP scores and previous
Biology experience can provide one line of nomothetic-span validity evidence.
Because the ﬁeld test data contains people who are at a variety of levels with
respect to biology and genetics, it should provide an opportunity to quickly
check an aspect of validity. If Biomass is a valid assessment of the domain
knowledge about genetics as well as the more general working knowledge about
scientiﬁc inquiry, we would expect that examinees who have had previous
training in Biology will do better than the others. (Stronger evidence that
bears directly on the formative use of Biomass would be to see if feedback on
the tasks would in fact lead to improved understanding in the domain.)
And unfortunately, we don’t have either of these kinds of actual data for
the pilot sample. We can however illustrate the thinking with a hypothetical
extension of the example. Let us suppose that in addition to the scores from
Table 15.10, Students 17–28 had previous experience, and Students 1–16 did
not. One simple way to check the relationship is to perform a Student’s t test
on the EAP scores for the two groups.
Looking ﬁrst at the expected proﬁciency level for DKMendel, we that the
group with previous training has an average EAP score of 2.04, compared
to an an average EAP score of 1.78 for the group without previous training
(t(26) = 1.24, p-value = .23). For WKInqry the story is similar, the group
with previous training again has an average EAP score of 2.04, compared to
an an average EAP score of 1.84 for the group without previous training (t(26)
= 1.92, p-value = .07). Thus the validity test points in the correct direction,
although the validity evidence from this study would be considered marginally
signiﬁcant just for WKInqry. There are several factors to consider here. First,
the size of the sample is quite small (only 28 students altogether); a larger
sample size would make the test much more sensitive to small diﬀerences
between the groups. Second, the length of the test is short (only 4 tasks,
containing 14 observables). A longer test would provide more opportunities

15.4 Conclusion
579
for examinees at high and low proﬁciency levels to distinguish themselves.
Finally, the non-representativeness of the sample further weakens inferences.
Even though the empirical validity evidence from this study is weak, we
still have one line of strong evidence for construct validity developed through
the ECD design methodology. If Biomass were to be used in an application
for which its consequential validity was important—impact on learning as a
formative assessment, for example—more careful study would be needed. In
this respect, a test built with Bayesian networks is no diﬀerent than one built
with a more familiar methodology (e.g., classical test theory or IRT).
15.4 Conclusion
A big advantage of the Bayesian approach to psychometrics is how smoothly
it scales from the case of no data to large amounts of data. Section 15.1
builds a perfectly functional scoring model for Biomass. It leans heavily on
the ECD design to accomplish this. In fact, the ECD design structures most
of the work. In many cases the only “psychometric” work the designers had
to do was to pick a particular parametric form, and prior laws that match
the experts’ description of the observable diﬃculty (i.e., “typical,” “harder,”
or “easier”), and how the proﬁciencies interact to when solving the problem
(e.g., “conjunctive,” “compensatory”).
Section 15.3 goes on to describe how, with an appropriate sample, we could
update the parameters of the scoring model. Given the limitations with the
ﬁeld data described in Sect. 15.2 and the fact that Biomass was never in use
in actual classroom practice, the ﬁelded version of Biomass only ever used the
prior parameters.
We would not claim generalizability for the results of the pilot study. We
would anticipate the item and population parameters to continue to change if
more data were obtained. Knowing that the estimates of some of the param-
eters depending materially on priors, we would want to investigate the sensi-
tivity of inferences to priors; ideally we would obtain enough data that infer-
ences would be robust with respect to a range of reasonable priors. More
importantly, though, with more data we would be able to ﬁt and compare
alternative scoring schemes for performances and explore alternative forms
for evidence models using methods described in the preceding chapters. We
would hope to feed back what we learned to improved task construction as
well. The value of the example is its detailed walk through of processes, and
of ways of thinking about assessment, design, and Bayes net modeling.
Although it can be diﬃcult to obtain a high quality sample for an assess-
ment like Biomass in classrooms, an alternative now exists. If Biomass were
made available over the Internet, chances are a number of classes would use
it, and a student sample could be gathered quite naturally over time. How-
ever, few teachers would be willing to adopt Biomass unless it is able to pro-
vide scores. The Bayesian approach to psychometrics oﬀers a solution to this

580
15 The Biomass Measurement Model
chicken-and-egg dilemma. Release Biomass 1.0 using a scoring model based
entirely on the prior. This is certainly no worse than a number right score,
and, because of the grounding in the ECD design, quite possibly better. When
suﬃcient data come in, the parameters can be updated and Biomass 1.1 can
be released. This process can continue indeﬁnitely into the future, and creates
a viable model for releasing low-to-moderate-stakes assessments quickly, and
improving them over time once they are ﬁelded.
This chapter also illustrates the other important use of data in Bayesian
psychometrics, the ability to provide critiquing information about the model.
Bayesian psychometrics supports sophisticated model checking procedures,
like posterior predictive data sets. We have see, however, that there is still a
great deal of power in the simple statistics used for item analysis. Even with
a small pilot sample, these provide vital checks on both the design and imple-
mentation of a new assessment. This practice of piloting and improvement in
both task design and evidence rules is a large part of the eﬀort in practical
assessment design.
Exercises
15.1 (Dependence of DKMendel and WKInqry). Section 15.1.1 claimed
that in a submodel of the Biomass proﬁciency model built in the previous
chapter (Fig. 14.3), DKMendel and WKInqry are dependent. Verify that
claim.
15.2 (Alternative Parameterizations). Suppose that one of the states of
DKMendel was anticipated to be rare in the sample. Would reparameterizing
the CPT for WKInqry help? Which parameterization would you choose and
why?
15.3 (Edge Orientation). How would the analysis in this chapter diﬀer if
the edge in the proﬁciency model was oriented from WKInqry to DKMendel
instead of from DKMendel to WKInqry? How would the ultimate estimates for
the evidence model parameters diﬀered? The proﬁciency model parameters?
15.4 (Data Collection Eﬀort). Obviously, the data collection eﬀort
described in Sect. 15.2 is biased. Describe some of the sources of bias and
how the actual data from the correct population might diﬀer from the real
data.
15.5 (Multiple Populations). The ﬁeld data described in Sect. 15.2 included
both high school and college students. Assume for the moment that the sub-
ject’s education level does not inﬂuence the evidence model parameters, just
the proﬁciency model. How can the model be changed to account for diﬀer-
ence between these two populations? Hint: add a demographic variable to the
proﬁciency model.

15.4 Conclusion
581
15.6 (Prior Strength). Consider the prior for DKMendel given in Equa-
tion 15.1. How could it be made stronger? Weaker?
15.7 (Prior Association). Consider the prior for WKInqry given DKMendel
(Eq. 15.2). How could the association be made stronger? Weaker? Does this
aﬀect the “strength” of the prior?
15.8 (Truncated Normal). In constructing the prior distributions for the
evidence models, why is the truncated normal distribution N+(1, 1) used for
α1jDKM instead of the untruncated normal distribution N(1, 1)? If a lognor-
mal distribution were used instead of a normal distribution, would truncation
still be necessary?
15.9 (Context Variable). What would happen if the context variables were
given a marginal probability Context ∼Bernoulli(.95) instead of Context ∼
Bernoulli(.5)? How would this aﬀect the estimated parameters? How would
this aﬀect the interpretation of Context?
15.10 (No Context Variable). Why is no context variable needed for Evi-
dence Model 4?
15.11 (Truncated Normal Mean). The parameter α1,1,DKM has the dis-
tribution N+(1, 1) but when BUGS is run with no data, it has a prior mean of
1.29 (Table 15.6). Why is this mean not 1.00? Why is the observed standard
deviation less than 1?
15.12 (Partial Credit Evidence Rules). Consider any of the MendMod-
Gen observables from Task 1. These observables all have three levels and are
all based on the DKMendel proﬁciency variable, which also has three lev-
els. If the observation is to provide the maximum possible information about
DKMendel what is the relationship between the states of the proﬁciency vari-
able and observable that must be expressed in the evidence rules?
15.13 (Negative Precision Increase). Some of the entries in Table 15.8
show a negative increase in precision from prior to posterior. What does this
mean?
15.14 (Percent Increase in WKInqry). Using the data from Table 15.10,
plot the posterior mean of DKMendel against the percent increase in posterior
precision for WKInqry. Explain the relationship.
15.15 (Project). This chapter contain all of the information necessary to
recreate the analysis in Sect. 15.3. Implement this analysis in BUGS and recre-
ate the analysis. Try several variant models and create several replicate data
sets. How well does the model ﬁt? Are there better ﬁtting alternatives? How
do alternative prior distributions for parameters (e.g., as in Exercise 15.11)
aﬀect parameter estimates?

582
15 The Biomass Measurement Model
15.16 (Project2). Four observables, x1,4, x1,6, x1,7, and x2,2 have no per-
sons in the sample with a response other than low. Drop those items from
the sample and rerun the calibration. Does this improve the ﬁt statistic for
Person 22?

16
The Future of Bayesian Networks in
Educational Assessment
From one perspective, evidence-centered assessment design and Bayesian net-
works are just notations. It is easy to express familiar assessment design pat-
terns using these notations. Bayesian networks are just a way to parameterize
multidimensional latent class models. What have we gained?
What we have gained is a notation that scales up from familiar assess-
ment designs and purposes, to the wide array of new kinds of assessments
made possible by advances in technology, cognitive psychology, and learning
sciences. This notation can help test designers know when they can reuse
familiar models and procedures, and do so eﬃciently, and when they need to
adapt or extend the models for a new purpose. Biomass (the previous two
chapters) is an example of the new kinds of things that can be done with
Bayesian networks.
This ﬁnal chapter cannot hope to tie up all of the loose ends left in this
book or explore more advanced applications. We rather hope that our read-
ers will take up the challenge. This ﬁnal chapter provides some pointers into
the literature to start you on that path. Section 16.1 provides an incomplete
review of various applications of Bayesian networks. Section 16.2 looks at
recent developments in Bayesian networks that should ﬁnd uses in educa-
tional applications. Section 16.3 looks at the important issue of integrating
assessment with instruction, and Sect. 16.4 looks at issues of validity. Finally,
Sect. 16.5 describes some problems that are still open.
16.1 Applications of Bayesian Networks
By far the most common application of Bayesian networks in education is
in intelligent tutoring systems (ITSs; some representative examples include
Aleven and Koedinger 2002; Baker et al. 2010; Bunt and Conati 2002, 2003;
Corbett and Anderson 1994; Crowley and Medvedeva 2006; El Saadawi et al.
2008; Gamboa and Fred 2001; Gertner et al. 1998; Graesser et al. 2001; Henze
and Nejdl 1999; Koedinger and Aleven 2007; Ley et al. 2010; Madigan et al.
c⃝Springer Science+Business Media New York 2015
583
R. G. Almond et al., Bayesian Networks in Educational Assessment,
Statistics for Social and Behavioral Sciences, DOI 10.1007/978-1-4939-2125-6 16

584
16 The Future of Bayesian Networks in Educational Assessment
1995b; Martin and VanLehn 1995; Mill´an and P´erez-de-la-Cruz 2002; Ritter
et al. 2007; Sao Pedro et al. 2013; VanLehn and Martin 1997; VanLehn 2008;
Vomlel 2003; Vomlel 2004; Zapata-Rivera et al. 1999; Zapata-Rivera and Greer
2004a; Zapata-Rivera and Greer 2004b). Bayesian networks have been popular
for intelligent tutoring applications because they oﬀer a principled mechanism
for reasoning about uncertainty and because of the ready availability of soft-
ware to perform that reasoning. Often, the Bayesian network is a part of the
student model of the tutoring system, although the term as used in the intel-
ligent tutoring literature diﬀers slightly from the term proﬁciency model used
in this book. In many applications noted above, the “student model” spans
the pieces this book would assign to the proﬁciency model and the evidence
model.
The student model in an ITS is used to make a number of diﬀerent kinds
of decisions. One important decision is the problem of task selection, which is
discussed in detail in Chap. 7. However, other decisions may happen within
the context of a task, such as how much help or scaﬀolding to give while a stu-
dent is attempting a problem. Often, the student model contains information
about aspects of the student not related to knowledge, skills, and abilities, for
example, learning preferences or eﬀect (Conati and Maclaren 2009).
A standard distinction between ITSs and educational assessments is that
the former are deployed as learning environments, while the latter are dis-
tinguished in learn–assess cycles. These cycles might be infrequent and hold
high stakes, such as end of course tests, or shorter and used for guidance,
such as formative tests. The shorter the cycles, though, the more ITSs and
assessments resemble one another. In general, the closer the ongoing instruc-
tion and the lower the stakes, the lower is the need for high reliability; each
decision is less critical and there is more opportunity to correct errors.
A high-stakes assessment needs high reliability for the reporting variables.
This means having a large enough number of tasks to provide evidence about
each reporting variable. As usual, there is a limited amount of time to adminis-
ter assessment tasks, the number of proﬁciency variables is limited by practical
considerations. In this situation, Bayesian networks have little computational
advantage over other similar methods, like multidimensional item response
theory (MIRT), unless the combinations of proﬁciencies or interdependencies
among observables are outside those that MIRT models can handle.
An ITS, in contrast, can be deployed over a long period of time. Thus, its
proﬁciency model can grow to tens or hundreds of variables. Here, the ability
of the Bayesian networks to break large problems into small pieces has dis-
tinct computational advantages. This is true whether it is a discrete Bayesian
network (as is developed in major part of this book) or a model with continu-
ous variables. Rijmen (2008) uses Bayesian network ideas to develop eﬃcient
algorithms for MIRT models taking advantage of conditional independence
assumptions inherent in special proﬁciency models (e.g., the bifactor model,
in which each task has exactly two parents: a general proﬁciency and one spe-
cialized proﬁciency, as in a mathematics test with a general math proﬁciency

16.1 Applications of Bayesian Networks
585
and unique factors for algebra, geometry, and statistics, with each item asso-
ciated with just one of the latter).
As mentioned, Bayes nets provide advantage over the more familiar alter-
natives in handling complex simulation tasks. Such tasks have multiple observ-
able outcomes and potentially complex patterns of dependence among them.
Here, Bayes nets provide a ﬂexible language for describing that local depen-
dence. Although Wainer et al. (2007) develop an elaborated testlet IRT model,
adding a latent variable to explain dependence among the related observed
outcome variables, this is only one possible pattern for dependence (Almond
et al. 2006b). It is advantageous for an assessment program to provide test
developers with a library of design patterns with preconstructed Bayes net
fragments to handle complex but recurring evidentiary situations: reusable,
ready-made argument structures around which they can write unique tasks.
Chapter 15 develops several diﬀerent design patterns for use in the Biomass
assessment (also see Mislevy et al. 2002d).
Another advantage of using Bayes nets in complicated but lower-stakes
applications is that calibrating the model (e.g., all of the machinery developed
in Part II) is not strictly necessary. An ITS using a Bayes net scoring engine
could be ﬁelded using just the expert’s best guess as to the parameter values.
This would be no worse than a test scored using a weighted number right
scheme. The ﬁelded system would produce scores, which would encourage
educators to use it. These early users could provide the data needed to perform
later calibration and model checking analyses.
Consider for instance adaptive content with evidence-based diagnosis
(ACED) (Example 13.1; Shute et al. 2008), an assessment for learning sys-
tem designed to assess/teach about algebraic sequences that had a Bayesian
network scoring engine. The initial version of ACED had 63 tasks (all with a
single binary observable), which the experts had classiﬁed as easy, medium,
or hard. The designers also provided a Q-matrix linking each task to one or
more proﬁciency variables. For the initial ﬁeld trial, parameter values were
set based on the prior distribution, that is, all easy tasks were assigned a
diﬃculty value of −1, medium tasks were assigned a diﬃculty of 0, and hard
tasks were assigned a diﬃculty value of 1. Similarly, discrimination parameters
were assigned based on the perceived importance of the proﬁciency variables
for each task type. Using these expert guesses for parameter values, the relia-
bility of the overall early assessment program (EAP) score from the Bayesian
network was 0.88, which was also the reliability of the number right score
using the tasks. The lesson here is that it is not the scoring model that makes
the reliability, but the quality and consistency of the tasks.
The ACED tasks were all provided with informative feedback, which
showed students a worked solution if they got a problem wrong, and an
adaptive task selection algorithm based on the expected weight of evidence
(Sect. 7.3). In a randomized trial of ACED (Shute et al. 2008), students who
were given both informative feedback and adaptive task selection showed sig-
niﬁcant improvement in their ability to solve sequence problems from pretest

586
16 The Future of Bayesian Networks in Educational Assessment
to posttest. Even though the students were obviously learning from the infor-
mative feedback, the EAP score from the Bayes net was still a good prediction
of the posttest score. The correlation was about 0.68, which is near the upper
bound determined by the reliability of the 20-item posttest. Thus, providing
feedback and learning during an assessment do not seem to hurt measurement
properties. In ACED, the feedback improved the measurement quality,1 with
the correlation between EAP score and posttest slightly higher in the groups
which received informative feedback.
16.2 Extensions to the Basic Bayesian Network Model
Another advantage of Bayes nets is that there are a large number of appli-
cations in addition to those in educational assessment. That means other
communities of practice are developing new techniques, which may be use-
ful for educational testing. In particular, there are many conferences in the
artiﬁcial intelligence community where the latest developments are often ﬁrst
published. Many of those conference proceedings are indexed in the Cite-
Seer database (http://citeseerx.ist.psu.edu/). Two developments that
hold implications for educational testing are object-oriented Bayes nets and
dynamic Bayes nets. They are summarized in Sects. 16.2.1 and 16.2.2. Sec-
tion 16.2.3 notes a third development, namely tools to help assessment design-
ers particularly take advantage of these and other advances in the more general
Bayes nets world.
16.2.1 Object-Oriented Bayes Nets
Breese et al. (1994) introduced the concept of knowledge-based model con-
struction. Bayesian networks and inﬂuence diagrams had proved to be good
at managing uncertainty in decision-making in ways that did not get tripped
up by dependence issues, but a Bayes net that covers all possible contingen-
cies that might be needed for a range of applications would be entirely too
large. To make the inﬂuence diagram more manageable, the knowledge-based
model construction splits the network into pieces and then uses logical rules
to assemble a model from just the pieces needed to solve a speciﬁc problem.
Mahoney and Laskey (1996) (see also Laskey and Mahoney 2000) present
a nice example of knowledge-based model construction. Consider a traﬃc
investigator who needs to recreate the scene of an accident to determine who
is at fault. Certain conditions or contexts require diﬀerent parts of the model.
1 The sample size was not large enough for the diﬀerences in correlations to be
signiﬁcant; however, that does not really matter here. The point is that the fact
that students are learning during the assessment did not decrease its predictive
validity.

16.2 Extensions to the Basic Bayesian Network Model
587
For example, if it was raining at the time of the accident, then there is a
set of nodes relating to wet road conditions, which is not necessary when the
road conditions are dry. Similarly, there are whole sets of nodes that are only
necessary if it was nighttime, the accident occurred at an intersection, there
was limited visibility in one direction, there were witnesses, and so forth.
Knowledge-based model construction inspired the idea of splitting the total
graphical model for an assessment into proﬁciency and evidence model frag-
ments (Sect. 5.4.1; Almond and Mislevy 1999; Almond et al. 1999). More-
over, using evidence models takes advantage of the special local independence
assumptions constructed into assessments. Each task provides a context for
the evidence model, and the local independence assumption means that the
nodes that are common across tasks only appear in the proﬁciency model.
The “knowledge” for knowledge-based assessment-model construction is the
selection of a task to present.
There are more advanced situations in educational assessment that can
beneﬁt from a more complete version of knowledge-based model construc-
tion, such as simulation-based assessment and direct observation. Consider
an observer watching a teacher in front of a classroom either live or on video.
There can be a whole collection of observable outcomes related to how the
teacher handles disruptive students that may not be relevant if no student
behaves disruptively during the observation period. A simulation-based assess-
ment can have certain evidence model fragments that only become active if
the student takes a certain path through the simulation. Agents recognize an
instance of a particular task conﬁguration and alert the presentation process
to capture the work product that will yield the values of the observables in
the evidence model (Sect. 13.2.1).
An important development in the ﬁeld of knowledge-based model con-
struction is the idea of object-oriented Bayesian networks (Koller and Pfeﬀer
1997; Laskey and Mahoney 2000). Think again of the accident investigation
example, and consider Bayes net fragments associated with multiple vehicles
at the scene of the accident. Vehicles form a general class and we create an
instance of that class for each vehicle at the scene. Each object would have
a Bayesian network fragment that has nodes relating to its properties (e.g.,
location, velocity, acceleration, and accident status) that are copies of the
nodes in the Bayes net for the class with some labeling convention used to
make them unique.
However, the object-oriented nature allows for more sophisticated reuse. A
key idea is inheritance, in which the default values for properties of an object
are taken from its parent. For example, the PassengerCar and Truck objects
may both inherit from the Vehicle class. The more general class would contain
common nodes for common properties (e.g., location), and the more speciﬁc
classes would have additional nodes appropriate for the subclass (e.g., number
of passengers or cargo weight). The advantage is that the subclasses can inherit
default properties from the more general parent classes, only overriding those
properties that are diﬀerent. Note that often the inheritance hierarchy is a

588
16 The Future of Bayesian Networks in Educational Assessment
matter of convenience and not taxonomy. For example, the Bus class might
inherit from the Truck class not because a bus is a kind of truck, but because
there are many similar properties (e.g., both are heavy and require special
licenses to drive).
In assessment, task models and task shells form a natural inheritance hier-
archy. Not only can inheritance properties be exploited to make task con-
struction more eﬃcient (Vendlinski et al. 2008), analogous structures can be
exploited for corresponding evidentiary relationships to make psychometric
model building more eﬃcient. The idea can also be exploited to learn link
model parameters for tasks in the same family. Geerlings et al. (2011) and
Johnson and Sinharay (2003) show how such task hierarchies can be used
to more eﬃciently calibrate assessments, eﬀectively reducing the size of the
pretest sample needed for each new assessment.
16.2.2 Dynamic Bayesian Networks
Throughout this book, we have mostly relied on the simplifying assumption
that a student’s proﬁciency does not change during the course of measurement.
However, we know that students learn, and that is why we teach them. This is
especially true in Assessment for Learning systems like intelligent tutors and
ACED (Shute et al. 2008, see also Examples 7.5 and 13.1 and Sect. 14.4.5),
which are designed to change student knowledge states. Consider the following
example.
Example 16.1 (The Aha Moment). Suppose that Aisha is struggling with
the geometric table concept. She gets a medium and two easy table items
wrong. Suddenly, something in the feedback kicks in and she has an “Aha”
moment. She subsequently gets two easy items right and a medium item right.
ACED treats this as a set of 6 items: 3 right and 3 wrong. It ignores the
temporal sequence and assumes that Aisha’s proﬁciency is the same through-
out the testing period. It would probably give her a pretty ﬂat distribution
for the table proﬁciency variable. The EAP score would dip down below zero
and then come back up to zero, but it would do so too slowly. If we took the
temporal ordering of the data into account, we can see that the past observa-
tions should be discounted in some way, and that her EAP score should now
be positive.
One simple technique for dealing with changing proﬁciency states is called
fading. The idea is that raising the likelihood to a power less than one will
make it ﬂatter. If we raise the likelihood to, say, the power 1/t, where t
being the number of time periods that has elapsed since the observation was
made, then more recent observations will be weighted more heavily than past
observations. Eventually, the inﬂuence of past observations will decrease to
zero. The ability to do fading is built into several Bayesian network software
packages. Compared to modeling change, fading has the advantages of being

16.2 Extensions to the Basic Bayesian Network Model
589
simpler and robust to patterns of change. On the other hand, it does not use
the data eﬃciently, and knowledge about patterns of change is not utilized
(Mislevy 1995a).
More complex models for change can be described through dynamic
Bayesian networks (DBNs) (Dean and Kanazawa 1989). The fundamental
idea is that a model that changes over time can be described by two pieces;
a single time-slice model that describes the initial state of the system, and
a two-time-slice model that describes how the system changes from one-time
slice to the next. The two-time-slice models can be chained for as many time
points as needed. Figure 16.1 shows the basic framework, as it would appear in
an assessment setting; each vertical panel represents a time slice at which stu-
dents are assessed. The upper variables are the proﬁciency variables (St) and
the lower ones are the observable outcome variables (Ot). The vertical links
between the proﬁciency variables are the familiar evidence models and much
of this book describes possibilities for building these models. The horizontal
links between the proﬁciency variables at two-time slices (St and St+1) rep-
resent the two-time-slice model. Underlying this model is a Markov property
that what happens in any time slice is independent of the past history given
the previous time slice. The simple form of this model has made it attractive
and it has been studied by a number of authors (e.g., Boyen and Koller 1998;
Murphy and Russell 2001; Koller and Learner 2001; Takikawa et al. 2002).
S
t=1
t=2
t=3
O
S
O
S
O
Assessment
Growth
Fig. 16.1 A basic dynamic Bayesian network
Reprinted with permission from ETS.
What’s new here is modeling the transition from one-time point to the
next. We will mention two ways of approaching this, which can be used sep-
arately or together depending on how the learning system is constructed.
They are mathematical learning models and Markov decision process (MDP)
models.

590
16 The Future of Bayesian Networks in Educational Assessment
Mathematical learning models build from the work of mathematical psy-
chologists going back to L. L. Thurstone and Clark L. Hull in the ﬁrst half
of the twentieth century, and formalization by Estes, Bush, Mosteller, and
others in the 1950s and 1960s (for an overview see Restle and Greeno 1970).
The basic idea used in practice systems and many tutoring systems is that a
student has or has not mastered some skill at Time t, makes a response, and
the probability of mastery at time t+1 depends on the previous mastery state
and the value of the response (Corbett and Anderson, 1994, Cen et al. 2006).
Many extensions are possible, but Fig. 16.2 shows the basic form.
S
t=1
t=2
t=3
O
S
O
S
O
Assessment
Growth
Fig. 16.2 A learning-model dynamic Bayesian network
Reprinted with permission from ETS.
Suppose, for example, a student who is in a mastery state at time t (that
is, St = 1) remains a master. A nonmaster may become a master at time
t + 1 after an incorrect response with some probability δ−, or after a correct
response with some presumably higher probability δ+:
P (St+1 = 1 |St, Ot ) =
⎧
⎨
⎩
1 if
St = 1
δ−if St = 0, Ot = 0
δ+ if St = 0, Ot = 1
.
Levy (2014) shows how to estimate the learning parameters along with the
conditional response probabilities in the Markov chain Monte Carlo (MCMC)
framework of Chap. 9.
The MDP (Boutilier et al. 1999) is an elaboration on DBN that includes
nodes for decisions and utilities (similar to the way an inﬂuence diagram is a
generalization of a Bayesian network). In particular, between each time slice,
an action is chosen, which inﬂuences how the variables are likely to change
between the time slices. Often, many of the key variables in the model cannot
be observed resulting in a partially observed MDP (POMDP). The reason
for building POMDPs is planning: to choose a sequence of actions or a policy

16.2 Extensions to the Basic Bayesian Network Model
591
for choosing actions that will optimize the probability of reaching a given
goal. Hoey et al. (2001) describe the software package SPUDD (stochastic
planning using decision diagrams) that helps to select optimal policies for
MDPs. Applications of MDP have been used, for example, to control elevators
(Nikovski and Brand 2003) and model learning in ITSs (Reye 2004).
Almond (2007a), (2007b) maps the fundamental problem of integrating
educational information to the MDP framework. Figure 16.3 shows the gen-
eral framework. Again the horizontal links between the proﬁciency variables
at two-time slices represent the two-time-slice model. In general, this tran-
sition can depend on what kind of instruction the student receives between
measurement opportunities. The choice of instruction is the action in the
POMDP model.
S
t=1
t=2
t=3
O
S
O
S
O
Assessment
Growth
Activity
Activity
Fig. 16.3 Instruction as a partially observed Markov decision process
Reprinted with permission from ETS.
DBNs and MDPs have both been used in ITSs ; Mayo and Mitrovic (2001)
review several systems. For example, Matsuda and VanLehn (2000) describe
the use of an MDP to make decisions about selecting hints or new problems.
Murray et al. (2004) use a DBN to calculate expected utility for a limited
set of actions and select the action that will maximize utility at the next
time step. While this may lead to action choices that are suboptimal when
considered as part of a sequence of actions, they are probably close enough
to optimal for practical purposes. Sabourin et al. (2013) use a dynamic Bayes
net to model students’ improving self-regulation skills in an exploratory game
environment. Reye (2004) describes the relationship between MDP approaches
and other approaches used for updating student models in the intelligent
tutoring literature. Conati and Maclaren (2009) use a DBN to track user
aﬀect in an educational computer game.

592
16 The Future of Bayesian Networks in Educational Assessment
16.2.3 Assessment-Design Support
As we have noted previously, this book is mainly about using Bayes nets in
educational assessment, but as a part of a coherent system of argumentation,
design, and inference. The idea is not to take some assessment and ask “might
we now use Bayes nets to make sense of the data that it will produce?” Rather,
ask what evidence do we need to ground the inferences we need to make?
What situations and actions will produce the evidence? How do we need to
evaluate, synthesize, and characterize the evidence to support the targeted
inferences? Bayes nets have features that lend themselves well to complexities
of evidentiary reasoning in complex assessments, and we may ﬁnd we can
capitalize on them, from the beginning, in producing assessments that suit
our purposes.
The Bayes net structures and calculations have great potential to support
a richer space of assessment, but they cannot fulﬁll their promise unless they
are used eﬀectively in conjunction with chunks of evidentiary arguments—
evidentiary arguments that are grounded solidly in an understanding of the
nature of proﬁciency in a domain and how we know it when we see it. The kinds
of Bayes net structures that we have discussed are conjunctive and compen-
satory combinations of proﬁciency, conditional dependence among observable
variables, and Markov processes across time points. What task designers need
is building blocks of Bayes nets fragments connected with the kinds of evidence
and arguments they need to make.
In other words, we would like to provide tools and libraries of “argument
chunks” that capture recurring relationships among proﬁciencies and observa-
tions in Bayes nets, so that assessment designers do not need to build Bayes
nets from scratch every time. We want to provide them these evidentiary
argument skeletons around which they can construct tasks that will evoke
evidence about targeted aspects of proﬁciency. We want to provide design
patterns, based on research and experience, which address aspects of proﬁ-
ciency that arise in many domains, such as model-based reasoning, inquiry,
and systems’ thinking—kinds of situations, work products, and observable
features that can then be tailored to the domain and purpose, with links to
Bayes nets fragments which too can then be tailored to the particulars of the
application (Mislevy et al. 2002d).
Assessment-design support tools have been explored by a number of
researchers (e.g., Conejo et al. 2004; Luecht 2012). Our own experience with
such systems has been the ECD-based design systems PORTAL (Almond et
al. 2002b) and PADI (Mislevy and Riconscente 2006). Such supports will be
necessary to scale-up the use of Bayes nets psychometric models we have dis-
cussed, to support the ﬂexible and recombinable assessments for which they
are a natural way to make sense of students’ performances.

16.3 Connections with Instruction
593
16.3 Connections with Instruction
ITSs naturally tie assessment and instruction together, but that relationship
is more universally important. Example 16.2 is an old joke that illustrates the
point.
Example 16.2 (Lost Car Keys). One night, a psychometrician was kneel-
ing down underneath a street lamp peering intently at the ground. A friend
walked up behind him.
“What are you doing?” asks the friend.
“Looking for my car keys,” replies the psychometrician.
“Where did you loose them?”
“Out there,” says the psychometrician gesturing out towards the darkness.
“Then why are you searching here?” asks the friend.
“Because the light is better!”
The metaphor is obvious, but the point is not always taken. It is far too
easy to ﬁnd IQ tests that deﬁne IQ as whatever it is the test measures, or
science tests that test science facts rather than scientiﬁc reasoning because
testing fact is easy and testing reasoning is hard.
Pelligrino et al. (2001) put the need for alignment more strongly: “Educa-
tional assessment does not exist in isolation, but must be aligned with curricu-
lum and instruction if it is to support learning [italic theirs].” Wilson (2004)
makes it a central theme of his textbook on assessment design.
Fig. 16.4 Inﬂuence diagram for skill training decision
Reprinted from Almond (2007) with permission from ETS.
The need for alignment is especially critical if the rationale for the assess-
ment is cognitive diagnosis. It is worth revisiting the inﬂuence diagram in
Fig. 4.14 (repeated here as Fig. 16.4). The educator ﬁnds the assessment valu-
able precisely when she can make a better decision about instruction using
the information than she can make without the information. An immediate
corollary is that if the educators set of options is impoverished (say limited

594
16 The Future of Bayesian Networks in Educational Assessment
to continuing mainstream instruction or expensive one-on-one tutoring), the
educator may not need a sophisticated assessment; simple classroom observa-
tion may be suﬃcient.
Even if the educator has a rich set of options, there needs to be an align-
ment between the options and the assessment. Making distinctions among
students that do not correspond to instructional options has limited value.
For example, the mixed-number subtraction example (Sect. 11.1), attempting
to diagnose the presence or absence of other skills for students who lacked
basic fraction subtraction ability, has limited pedagogical utility. A cogni-
tively diagnostic assessment that makes distinctions that do not correspond
to instructional options may however have utility as a research tool. In par-
ticular, identifying important and common cognitive states could improve
curriculum design and create new instructional options. On the other hand,
until those new options are in place, the distinctions will have limited utility
to a classroom teacher or administrator.
The approach taken in this book has been to treat assessment design as an
engineering discipline. While there are important scientiﬁc and mathematical
principles that underlie good assessment design, it is still largely a creative
endeavor with plenty of acceptable solutions. As with other branches of engi-
neering, the results are usually the most satisfactory when the design process
is customer focused, or at least involves numerous chances for feedback from
potential customers.
One tool that we have found useful in soliciting input from potential score
users is the prospective score report. This is simply a mock-up of what a score
report might look like under the current score design. Bayesian networks are
helpful here because it is easy to simulate plausible response patterns and
proﬁciency proﬁles. Prospective score reports are very useful for verifying
that the claims targeted by the current assessment design are in fact the ones
that the prospective customers want. They are also useful for conveying the
consequences of psychometric issues to score users with limited training in
psychometrics. If an issue will cause them a problem, it most likely will show
up on the score report in some way. Almond et al. (2009b) and Zapata-Rivera
et al. (2012) illustrate these ideas in the context of graphical representations
in score reports for teachers.
16.3.1 Ubiquitous Assessment
Assessments used to drive instructional decision making are often called for-
mative assessment. Black and Wiliam (1998a, 1998b) note that teachers who
employ formative assessment are often better at promoting learning than those
who do not. However, formative assessment is usually associated with activi-
ties done by a classroom teacher in the course of instruction, not with special
testing situations in which a formal score report is generated.
Although the two assessment purposes are often thought to be incompat-
ible, this is not necessarily the case. Consider once again the assessment for

16.3 Connections with Instruction
595
learning system, ACED (Shute et al. 2008). In this system, students are asked
to solve mathematics problems and provided feedback when they get incorrect
answers. In the ﬁeld trial of ACED, the group that showed signiﬁcant learning
gains had the highest correlation between the Bayesian network score from the
activities and the posttest score. This suggests that the same assessment can
be used for both formative and summative purposes. (One important caveat,
in order to be used for formative purposes, is that the report/feedback must
be available immediately, or at least very quickly. Often the lag between test-
ing and score report generation prevents an assessment from being used for
formative purposes.)
Taking the idea further, it may be the case that we only rarely need to
take time out of the regular learning activities for assessment. Any activity
performed by students potentially provides evidence about their knowledge,
skills, and abilities: assessment need not be conﬁned to special testing activi-
ties. Tanimoto (2001) calls this idea unobtrusive assessment, emphasizing the
fact that assessment does not need to be segregated from normal classroom
activities. Shute et al. (2009) call it stealth assessment, suggesting that assess-
ment can be embedded in activities that students enjoy, like simulations and
games. Almond (2010b) calls the idea ubiquitous assessment, emphasizing the
fact that all activities performed by a student are potential evidence.2
For an activity to provide evidence about claims of interest, it really only
requires an evidence model that links the natural work product to the proﬁ-
ciencies of interest. For example, if a student reads a web page with instruc-
tional material and then presses an “OK” button to go onto the next page,
that button press is a potential work product. This is particularly weak evi-
dence (a student could have pressed “OK” after only skimming the page).
Usually, much stronger evidence is available, such as results from practice
exercises or a more extended project or investigation.
The evidence from such found assessments may not be as strong as the evi-
dence from a formally engineered assessment, but it might make up for what
it lacks in quality in two ways: quantity and timeliness. If we are assessing all
the time, we are not longer constrained by the time limits of formal assess-
ments. Evidence is gathered as students work, so feedback can be made when
it is most useful—in real time if desired, or in debrieﬁng sessions the student,
the teacher, or the system might propose. Teachers can gather evidence from
extended tasks that require multiple sessions to complete.
They can also gather evidence from group work. This is another potential
application of Bayesian networks. A group task is just like a task with multiple
proﬁciency inputs. Some of the same design considerations that are required
2 They may indeed be evidence, but whether they should be captured and how and
when they should be used raises issues of privacy and consent. Bennett (2013) sug-
gests that situations under which performance may be monitored and used should
be clearly delineated from those that are not, stakeholders or their guardians must
be informed of the situations and uses and provide consent, and data must be
scrupulously protected.

596
16 The Future of Bayesian Networks in Educational Assessment
to untangle evidence in diagnostic assessment are required to untangle evi-
dence from multiple students. This particular application has not yet been
extensively explored; some applications are reported in Chung et al. (2002)
and Singley et al. (1999).
Finally, another challenge arises when setting up a ubiquitous assessment
system: the evidence may be gathered over a long stretch of time. This means
that students will grow over the time scale of assessment. The methods of
Sect. 16.2.2 need to be pressed into service to take this critical feature of
the inferential situation into account. Note that these models address the
instructional part of the activity and not just the assessment part.
16.4 Evidence-Centered Assessment Design and Validity
There is nothing special about Bayesian networks that forces assessments that
use them to be valid. The evidence-centered assessment design process, how-
ever, encourages test designers to think about the relationship between tasks
used to gather evidence of student proﬁciency and the claims that the design-
ers wish to make about students. Evidence-centered assessment design makes
the “construct representation” line of a validity argument (Embretson 1983)
central to the design process, and encourages test designers to document the
rationale for key design decisions (Kane 2006). A particular task is included in
an assessment precisely because it provides evidence for a claim of interest. To
the extent that test designers document the sources that went into establish-
ing the evidential relationships, this provides a public strand of the validity
argument. Similarly, tasks that do not provide evidence, or only provide weak
evidence for the claims of interest, are not included in the assessment.
Ultimately, the measurement model of the assessment is only a model. This
book has encouraged readers to think about assessment design as a process
of model building. The speciﬁc details about the computational techniques,
whether the proﬁciency variables are discrete or continuous, and whether the
model is based on classical test theory, IRT, or Bayesian networks are all
secondary concerns. The real questions are these: (1) Is the model an adequate
representation of the theory of cognition that underlies the assessment for
the purpose at hand? (2) Can the model adequately account for student’s
performance? (3) How well do the scores in fact serve the purpose of the
assessment?
The ECD process focuses on the ﬁrst question, the constructive part. It
helps the designer devise task situations, scoring rubrics, and psychometric
models to synthesize evidence that will all be pertinent to the purpose of the
assessment. The Bayesian paradigm oﬀers some assistance for the second ques-
tion too, the model-ﬁt part. Any assessment instrument needs to be subjected
to a program of ﬁeld testing designed to address the model ﬁt. Any pattern
of responses will have a prior probability. If too many surprising events occur,
the model is brought into question (Chap. 10). Such results can be used to

16.5 What We Still Do Not Know
597
reﬁne the model, and to the extent to which the model reﬂects the experts’
theory of cognition, used to identify weaknesses in that theory.
As these pilot studies are often small scale, they can take advantage of
that size to look at kinds of tasks that would not be feasible in a larger study.
In particular, in early stages of the assessment design, it is often useful to look
at natural activities that represent valued work in the domain of interest. The
ideal task might be to simply videotape and rate the student’s performance
in that authentic activity. While this is not feasible as part of a large-scale
testing program, it is feasible as part of a small-scale validity study.
The third question, how well the assessment serves the purpose, goes
beyond how it is constructed and whether the data ﬁt the model. Additional
studies are needed to see, for example, if better decisions are made using the
assessment rather than an alternative, or if its instructional recommendations
do in fact lead to better learning.
In these investigations, data can be used to produce evidence models that
link authentic criterion activities to the proﬁciency model of the assessment.
The assessment could then predict student’s performance on these tasks based
on the proﬁciency estimates. This is related to the idea of market basket
reporting (DeVito and Koenig 2000), but has the advantage of focusing on
tasks that represent valued work in the domain of interest— tasks that the
score users ultimately care about.
Another important aspect of validity is the relationship between the states
of the proﬁciency variables and educational standards set by various govern-
ment bodies and consortia. The traditional approach to standards and assess-
ments is retrospective: The standards are deﬁned, the assessment is deﬁned
and pretested, and then a panel of experts is convened to determine the passing
scores. Bayesian networks and ECD encourage an approach that is prospec-
tive (Bejar et al. 2007): The standards are deﬁned and then tasks are selected
for the assessment that provide evidence about whether or not the candidate
meets the standards. Intuitively, it would seem that the prospective approach
should yield instruments that are better focused on making judgments about
the standards. This has yet to be veriﬁed empirically. Furthermore, it is quite
possible that there will be diﬃculties in the design or implementation of the
assessment, which makes it measure something other than what was intended.
Thus, the prospective approach to standard setting still requires validation.
Procedures for that validation are still largely unexplored.
16.5 What We Still Do Not Know
Ultimately, the way we will learn more about Bayesian networks in educational
assessment is from people trying to use Bayesian networks in real applications.
Much of the content of this book is driven by what we learned from HYDRIVE

598
16 The Future of Bayesian Networks in Educational Assessment
(Gitomer et al. 1995; Mislevy and Gitomer 1996), DISC (Mislevy et al. 1999b;
Mislevy et al. 2002d), Biomass (Steinberg et al. 2003, Chap. 14), NetPASS
(Behrens et al. 2004), ACED (Shute 2004; Shute et al. 2005, 2008), and the
alternative scoring research for Educational Testing Service’s (ETS) inter-
net and computing technology (ICT) Literacy Assessment (Katz et al. 2004).
We have learned also from applications of others, such as those of Shute et
al. (2009), Martin and VanLehn (1994), and Iseli et al. (2010). Applications
like these help sort out the problems that are important because they block
progress toward practical applications from problems that are of a more the-
oretical nature.
As Bayesian networks provide an alternative notation for familiar psy-
chometric models, a large number of theoretical issues arise in studying how
familiar psychometric principles and issues play out in Bayesian networks. In
some cases, the Bayesian network view may help bring clarity (for example,
casting diﬀerential item functioning as a question of conditional independence;
Sect. 10.4). In other cases, our lack of experience with Bayesian networks
makes them harder to use at ﬁrst. In particular, using Bayesian networks,
it is easy to specify models that are only weakly identiﬁable from data, or
parts of models that can only be identiﬁed from the prior distribution. A lot
is known about model identiﬁcation in factor analysis and structural equation
modeling, but relatively little in Bayesian networks.
One question that has arisen in many of our conversions is how does one
equate a Bayesian network assessment. Standard equating is critical when
students are compared for high stakes purposes, and the evidence coming
from diﬀerent test forms needs to be equivalent with regard to construct
representation and measurement error. For these situations, we note that it is
possible to use multiple evidence accumulation processes, and a program can
still use standard test design and scoring procedures that ensure equivalency of
this kind when they need to, with a parallel Bayes net evidence accumulation
engine to produce more detailed feedback.
But high-stake purposes and strictly equated tests are not really playing to
the strengths of Bayes nets. For the purposes of modeling complex evidentiary
relationships in assessments closer to learning, the key issue is calibrating the
assessment so that the proﬁciency variables actually represent the proﬁciencies
they are named after. Two alternative forms of the same assessment share a
common proﬁciency model but have diﬀerent evidence models for the diﬀerent
tasks. If both forms were calibrated in a common framework (Sect. 9.6.2), they
are linked in a sense that they both provide estimates of the same proﬁciencies.
The greatest value of Bayesian networks lies in their application to prob-
lems of cognitive diagnosis and inference from complex assessments, such as
simulations and games meant to support learning, where true equating is not
necessary and linking is suﬃcient. (Beware of mission creep, though. It is not
uncommon for an assessment created for one purpose, say low-stake instruc-
tional guidance, to be adopted for a diﬀerent purpose, say a selection decision
or or teacher evaluation. In these cases, it is necessary to reexamine the entire
assessment argument, not just the measurement model.)

16.5 What We Still Do Not Know
599
One practical problem that arises in almost all applications is the grain-
size problem. How detailed should the proﬁciency model be? How many vari-
ables? How many levels for each variable? Cognitive scientists like to make
highly detailed models that match their detailed knowledge of the domain.
Psychometricians prefer simpler models that require less evidence to estimate
proﬁciencies, especially when the time available for testing is limited. There
is no “right” answer to this problem: every application must ﬁnd a solution
that is appropriate to the constrains and purposes of the assessment to be
designed. One good rule is that the grain size needs to be ﬁne enough to
support whatever decisions will be made. A corollary follows: In early design
phases, modeling at one level (more detailed) can be useful to think about
ﬁner-grained aspects of proﬁciency and features of situations to evoke them.
Design at this level will eﬀectively deﬁne the proﬁciencies that do appear in
the model.
Ultimately, good assessments are made from good tasks. Even complex
psychometric models can only go so far in extracting evidence from tasks
that do not provide adequate evidence. The potential of Bayes nets for edu-
cational assessment comes from their properties of modularity, computational
qualities, and the ﬂexibility to handle coherently diﬀerent conﬁgurations of
evidence. The potential will only be realized through thoughtful design and
modeling of educationally meaningful observation situations, which can evoke
good evidence about the right capabilities. We have tried to give our readers
a foundation to take advantage of these features in a range of new forms of
assessment in this book. We cannot wait to see what they accomplish.
Exercises
16.1. Design an assessment that uses Bayesian networks as a scoring model.
Use the principles of evidence-centered assessment design. Publish your assess-
ment model.
16.2. Join the ECD wiki (http://ecd.ralmond.net/ecdwiki/) and con-
tribute to the discussion about evidence-centered assessment design. (Contact
the authors for an editing password.)
16.3. The authors of this book have made several mistakes and the book
contains several important omissions (not that we meant to!). Find them and
bring them to the attention of the authors.

A
Bayesian Network Resources
This appendix provides pointers to online versions of various resources, which
may be useful when studying Bayesian networks. This includes two kinds
of resources: software packages we have found useful and sample Bayesian
networks and data for study.
The risk with including internet links in a printed book is that they can
become dated long before the text, and that there is no easy way to update
them. Therefore, we have mirrored most of the content in this appendix
on the evidence-centered design (ECD) Wiki (http://ecd.ralmond.net/
ecdwiki/). The ECD Wiki can be read by anyone, but is only open for edit-
ing by members of the Bayes net community. (Congratulations! By reading
this book you have become a member of that community. Send email to the
authors mailto:almond@acm.org to get the editing password.1) The resources
and errata for this book (including most of the contents of this appendix) are
available on this web site, http://ecd.ralmond.net/ecdwiki/BN/BN.
We, the authors, do not believe that we have created a complete or perfect
description of ECD or the use of Bayesian networks in assessment. In our
experience, each time we undertake a new project, the project takes us in new
directions. We have found others’ questions about and perspectives on ECD
to be valuable in reﬁning our thinking, so we hope you will become part of
the conversation.
A.1 Software
In preparing this book, we have used three diﬀerent types of software: Basic
Bayesian network manipulation software (Sect. A.1.1), software for construct-
1 If you come across a page reference that seems to require a password to access,
that means that the page has not been written yet, and the computer is asking
if you want to write the page. We are looking for enterprising individuals to help
us ﬁll in the content. Please volunteer.
c⃝Springer Science+Business Media New York 2015
601
R. G. Almond et al., Bayesian Networks in Educational Assessment,
Statistics for Social and Behavioral Sciences, DOI 10.1007/978-1-4939-2125-6

602
A Bayesian Network Resources
ing Bayesian networks by hand (Sect. A.1.2), and software for estimating
parameters using Markov chain Monte Carlo (MCMC) (Sect. A.1.3).
A.1.1 Bayesian Network Manipulation
Shortly after the publication of Pearl (1988), a large number of software
packages started appearing, which could carry out the basic manipulation
of Bayesian networks described in Chap. 5. Some of these survived and others
did not. Almond (1995) had a list of packages available at the time, and I
[Russell] tried to keep up with the changes on the internet, but soon gave
up. Not only did packages appear and disappear, but vendors also rearranged
their web sites, so links quickly broke. The solution turned out to be a wiki,
where many hands could help ﬁx broken links. Currently, the best list of
available software is the Wikipedia article on Bayesian networks (https://
en.wikipedia.org/wiki/Bayesian network). The list of applications at the
end includes both free and commercial software.
We describe only four software packages below because they have played
a special role in the creation of this book.
Netica: Netica R⃝is a commercial Bayesian network software environment
oﬀered by Norsys, LLC (http://www.norsys.com/). Netica comes in two
formats: a graphical interface version and an API for embedding in other
applications. Netica is available in both the full commercial version and a
free student version, which limits the size of a network that can be saved.
The student version is adequate for studying the examples in this course,
but you will want the full commercial version for a serious project.
The Netica graphical interface works only in Microsoft Windows R⃝. Norsys
supports Netica when used with various tools for running Windows
programs on other platforms. We have used Netica successfully with
both WINE (http://www.winehq.org/) and CrossOver (http://www.
codeweavers.com/products/).
GeNie and Smile:
GeNie (graphical interface version) and Smile (API ver-
sion) are Bayesian network software packages oﬀered by the Decision
Systems Laboratory, University of Pittsburgh (http://genie.sis.pitt.
edu/). A notable feature of these packages is that they will translate
Bayesian networks ﬁles from one format to another, so this is a useful tool
when transporting networks. We have used this tool to make alternate
versions of various Bayesian network packages available.
StatShop: StatShop was an internal Bayesian network package developed at
ETS (Almond, Yan, et al. 2006c). It is still a research prototype and was
never formally released, but interested people can write to Duanli Yan
(mailto:dyan@ets.org) to request a license. Once you have a license,
you can contact Russell Almond (mailto:almond@acm.org) for further
instructions and help. A word of warning! Although StatShop runs all of
the examples described here, it has never really undergone the reﬁnement
it needs to be a standalone tool. Considerable programming expertise is
probably necessary to get it running properly.

A.1 Software
603
HUGIN:
HUGIN Expert A/S (http://www.hugin.com/) is mentioned sev-
eral times in this book as it plays an important role as one of the ﬁrst
commercial Bayes net packages. It was created by taking the Bayes net
software for the the MUNIN network (Andreassen et al. 1987) and modi-
fying it to support arbirary networks (Andersen et al. 1989). Twenty-ﬁve
years later it is still being actively developed, supported and improved.
A.1.2 Manual Construction of Bayesian Networks
Constructing a Bayesian network for an assessment starting from the cognitive
analysis of the domain through the deﬁnition of the network structure and
conditional probabilities is a complex process. This book attempts to discuss
many of the issues that come up, but each project oﬀers unique challenges.
The Bayesian network software described in the previous section only sup-
ports the last part of the process. Typically, Bayesian network software only
oﬀers support for creating the graphical structure and adding the numbers.
Other tools are necessary to manage the knowledge that goes into the model
construction.
While we were constructing Biomass (Chaps. 14 and 15), we used a custom
tool called Portal. When working on other projects, we found that Portal was
both too complicated and not ﬂexible enough to support our needs. Instead,
what we found was that it was better for each design team to develop their
own set of custom forms and spreadsheets in which to capture the knowledge
important to the particular assessment purpose. The programmers could then
extract information from these tables to build the Bayesian networks (Almond
2010a).
The three R (R Development Core Team 2007) packages described here are
useful for writing programs that parse other data ﬁles and construct Bayesian
networks. All three are available as free downloads from http://pluto.coe.
fsu.edu/RNetica/.
CPTtools: This package has R code available to build conditional probability
tables using the DiBello–Samejima distribution and some of the other
methods described in Chap. 8. It also has R functions for the evidence
balance sheet (Chap. 7) and the observable characteristic plot (Chap. 10).
It does not require any other packages to run.
RNetica: This is basically an R binding for the Netica API, and requires a
Netica API license (see Sect. A.1.1). Together with CPTtools, most of the
packages in this book can be constructed in Netica.
SSX: The StatShop XML (SSX) package provides tools for reading/writing
StatShop (see Sect. A.1.1) network ﬁles.
A.1.3 Markov Chain Monte Carlo
Chapter 9 describes two diﬀerent algorithms for estimating Bayesian net-
work parameters from data: the expectation–maximization algorithm (EM

604
A Bayesian Network Resources
algorithm) and MCMC. Many Bayesian network packages have some kind of
parameter learning built in, which is usually a variant of the EM algorithm.
(StatShop had a built-in MCMC engine.) Once again, the Wikipedia page on
Bayesian networks has a good list of available software.
Often, however, we have found that the best approach is to write custom
model estimation code in BUGS (Bayesian inference Using Gibbs Sampling)
(Spiegelhalter et al. 1995) or a similar general purpose MCMC package. The
Windows version WinBUGS (Lunn et al. 2000) has a convenient graphical
interface that makes it easy for beginners to get started; however, WinBUGS
is no longer maintained, and hence better alternatives are available for seri-
ous work. We recommend either OpenBUGS (http://www.openbugs.info/
w/FrontPage; Lunn et al. 2009) or JAGS (Just Another Gibbs Compiler)
(http://www-fis.iarc.fr/~martyn/software/jags/; Plummer 2012). The
package Stan (http://mc-stan.org/) is a promising alternative, but cur-
rently, it lacks support for the discrete nodes needed for discrete Bayesian
networks (this may be ﬁxed by the time you read this).
A.2 Sample Bayesian Networks
As we have used the book in instruction, we have found it helpful for stu-
dents to work through some larger scale examples of Bayesian networks. To
that end we provide a number of networks for use by readers of the book.
Instructionally, these provide excellent tools for homework and projects.
Each network is provided in a number of diﬀerent formats (see the listing of
tools in Sect. A.1.1). First, they are provided in StatShop XML and HTML for-
mats. The HTML version of the network provides a complete human-readable
description of every conditional probability table. Second, they are provided
as a collection of Netica networks. Note that GeNie is able to convert the
Netica ﬁle to formats that are compatible with a number of other Bayesian
network programs.
The following examples are provided at http://ecd.ralmond.net/ecdwiki/
BN/BN:
Evidence Model Student Model: This is a very small example with a three
proﬁciency variables and four evidence model Bayes net fragments used for
testing transferring information between evidence model and proﬁciency
model fragments (Sect. 5.4; Almond and Mislevy 1999; Almond et al.
1999).
IRT5: This is the simple ﬁve-item IRT models with and without context
eﬀects explored in Sects. 6.1 and 6.2. This network is used Examples 6.1,
6.2, 6.3, 6.4, and 10.1.
Design: This is a small example contrasting the Compensatory, Conjunctive,
and Disjunctive design patterns discussed in Chap. 8.
Latent Class: This is the small latent class model used in Chap. 9. It is
used in Examples 9.1, 9.2, 9.3, and 9.4.

A.2 Sample Bayesian Networks
605
Language Testing: This is a language exam containing both single modal-
ity and integrated tasks ﬁrst described in Mislevy (1995c). It is used in
Examples 1.1, 7.1, 7.6, 7.7, and 7.8. It has several instances of each task, so
that it can be used for simulation experiments, and has several simulated
data sets.
Mixed Number Subtraction: This is the classic mixed-number subtraction
test of Tatsuoka (1983) adapted into a Bayesian network by Mislevy
(1994); Mislevy (1995b). As this is one of the ﬁrst published examples
of a cognitively diagnostic assessment, this example has seen a lot of use,
including use in Sect. 6.4 and Chap. 11. Only the Bayesian network is
provided, not the data gathered by Tatsuoka et al.
ACED: Adaptive content with evidence-based diagnosis (ACED) is one of the
ﬁrst ﬁelded diagnostic assessments using Bayesian networks as a scoring
model (Shute et al. 2005; Shute et al. 2007; Shute et al. 2008). The web site
contains the proﬁciency model, the evidence models for all 63 tasks and
data from over 200 students involved in the ﬁeld trial. ACED is explored
in Examples 7.5 and 13.1.

References
Adams, D. (1978). Hitchhiker’s guide to the galaxy: The primary phase (Vinyl
LP ed.). London: BBC. LP record.
Adams, R. J., Wilson, M. R., Wang, W.-C. (1997). The multidimensional
random coeﬃcients multinomial logit model. Applied Psychological Mea-
surement, 21, 1–23.
Akaike, H. (1973). Information theory and an extension of the maximum
likelihood principle. In B. N. Petrov
F. C´aki (Eds.), Proceedings of
the 2nd international symposium on information theory (pp. 267–281).
Budapest: Akademiai Kiado.
Aleven, V.,
Koedinger, K. R.
(2002, Mar-Apr).
An eﬀective metacogni-
tive strategy: Learning by doing and explaining with a computer-based
cognitive tutor. Cognitive Science, 26(2), 147–179.
Almond, R. G.
(1995).
Graphical belief modeling.
London: Chapman
and Hall. Retrieved from http://www.crcpress.com/product/isbn/
9780412066610
Almond, R. G.
(2007a).
Cognitive modeling to represent growth (learn-
ing) using Markov decision processes. Technology, Instruction, Cogni-
tion and Learning (TICL), 5, 313–324. Retrieved from http://www.
oldcitypublishing.com/TICL/TICL.html.
Almond, R. G. (2007b). An illustration of the use of Markov decision processes
to represent student growth (learning) (ETS Research Report No. RR-
07-40). Princeton: Educational Testing Service. Retrieved from http://
www.ets.org/research/researcher/RR-07-40.html.
Almond, R. G.
(2010a).
“I can name that Bayesian network in two
matrixes.” International Journal of Approximate Reasoning, 51, 167–
178. Retrieved from http://dx.doi.org/10.1016/j.ijar.2009.04.
005. doi: 10.1016/j.ijar.2009.04.005.
Almond, R. G. (2010b). Using evidence centered design to think about assess-
ments. In V. J. Shute B. J. Becker (Eds.), Innovative assessment for
the 21st century: Supporting educational needs (pp. 75–100). New York:
Springer. doi: 10.1007/978-1-4419-6530-1 6.
c⃝Springer Science+Business Media New York 2015
607
R. G. Almond et al., Bayesian Networks in Educational Assessment,
Statistics for Social and Behavioral Sciences, DOI 10.1007/978-1-4939-2125-6

608
References
Almond, R. G., Mislevy, R. J. (1999). Graphical models and computerized
adaptive testing. Applied Psychological Measurement, 23, 223–238.
Almond, R. G., Herskovits, E., Mislevy, R. J., Steinberg, L. S. (1999). Transfer
of information between system and evidence models. In D. Heckerman
J. Whittaker (Eds.), Artiﬁcial intelligence and statistics 99 (pp. 181–
186). San Francisco: Morgan Kaufmann.
Almond, R. G., DiBello, L. V., Jenkins, F., Mislevy, R. J., Senturk, D., Stein-
berg, L. S., Yan, D. (2001). Models for conditional probability tables in
educational assessment. In T. Jaakkola T. Richardson (Eds.), Artiﬁcial
intelligence and statistics 2001 (pp. 137–143). San Francisco: Morgan
Kaufmann.
Almond, R. G., Steinberg, L. S.,
Mislevy, R. J.
(2002a).
Enhancing
the design and delivery of assessment systems: A four-process archi-
tecture. Journal of Technology , Learning, and Assessment, 1(5), 1–
63. Retrieved from http://ejournals.bc.edu/ojs/index.php/jtla/
article/view/1671.
Almond, R. G., Steinberg, L. S., Mislevy, R. J. (2002b). A framework for
reusing assessment components.
In H. Yanai, O. A., K. Shigemasu,
Y. Kano,
J. J. Meulman (Eds.), New developments in psychometrics
(pp. 281–288). Tokyo: Springer.
Almond, R. G., Mislevy, R. J., Williamson, D. M.,
Yan, D.
(2006a,
April). Bayesian networks in educational assessment. Paper presented
at Annual meeting of the National Council on Measurement in Educa-
tion (NCME). San Francisco, CA.
Almond, R. G., Mulder, J., Hemat, L. A., Yan, D. (2006b). Models for local
dependence among observable outcome variables (ETS Research Report
No. RR-06-36). Princeton: Educational Testing Service. Retrieved from
http://www.ets.org/research/researcher/RR-06-36.html.
Almond, R. G., Yan, D., Matukhin, A., Chang, D. (2006c). StatShop test-
ing (Research Memorandum No. RM-06-04). Princeton: Educational
Testing Service.
Almond, R. G., Mislevy, R. J., Williamson, D. M.,
Yan, D. (2007, April).
Bayesian networks in educational assessment.
Paper presented at
Annual meeting of the National Council on Measurement in Education
(NCME) Chicago, IL.
Almond, R. G., Yan, D., Hemat, L. A. (2008). Parameter recovery studies
with a diagnostic Bayesian network model.
Behaviormetrika, 35(2),
159–185.
Almond, R. G., Shute, V. J., Underwood, J. S., Zapata-Rivera, J.-D. (2009a).
Bayesian networks: A teacher’s view. International Journal of Approx-
imate Reasoning, 50, 450–460. doi: 10.1016/j.ijar.2008.04.011.
Almond, R. G., Shute, V. J., Underwood, J. S., Zapata-Rivera, J.-D. (2009b).
Bayesian networks: A teacher’s view. International Journal of Approx-
imate Reasoning, 50, 450–460.

References
609
Almond, R. G., Mislevy, R. J., Williamson, D. M.,
Yan, D. (2010, April).
Bayesian networks in educational assessment. Paper presented at annual
meeting of the National Council on Measurement in Education (NCME).
Denver, CO.
Almond, R. G., Kim, Y. J., Shute, V. J.,
Ventura, M.
(2013).
Debug-
ging the evidence chain. In R. G. Almond O. Mengshoel (Eds.), Pro-
ceedings of the 2013 uai application workshops: Big data meet complex
models and models for spatial, temporal and network data (uai2013aw)
(pp. 1–10). Aachen. Retrieved from http://ceur-ws.org/Vol-XXX/
paper-01.pdf.
Alonzo, A. C.,
Gotwals, A. W. (Eds.).
(2012).
Learning progressions in
science: Current challenges and future directions. Rotterdam: Sense.
American Association for the Advancement of Science. (1994). Benchmarks
for scientiﬁc literacy. New York: Oxford University Press.
Andersen, S. A., Madigan, D., Perlman, M. D. (1996). A characterization
of Markov equivalence classes for acyclic digraphs. Annals of Statistics,
25, 505–541.
Anderson, R. D., Vastag, G. (2004). Causal modeling alternative in operations
research: Overview and application. European Journal of Operational
Research, 156(1), 92–109. doi: 10.1016/s0377-2217(02)00904-9.
Andreassen, S., Woldbye, M., Falck, B., Andersen, S. K. (1987). Munin—
a causal probabilistic network for interpretation of elecromyographic
ﬁndings. In J. P. McDermott (Ed.), Proceedings of the 10th international
joint conference on artiﬁcial intelligence (Vol. 1, pp. 366–372).
San
Francisco, CA.
Andrich, D. (1985). A latent trait model for items with response dependencies:
Implications for test construction and analysis.
In S. E. Embretson
(Ed.), Test design: Developments in psychology and psychometrics (pp.
245–275). New York: Academic.
Attali, Y., Burstein, J. (2006). Automated essay scoring with e-rater R⃝v.
2.0. The Journal of Technology, Learning, and Assessment, 4(3), 13–18.
Retrieved from http://escholorship.bc.edu/jtla/vol4/3/.
Bacchetti, P., Segal, M. R.,
Jewell, N. P. (1993). Backcalculation of HIV
infection rates (with discussion). Statistical Sciences, 8, 82–119.
Bachman, L. F., Palmer, A. S. (1996). Language testing in practice: Designing
and developing useful language tests (Vol. 1). Oxford: Oxford University
Press.
Baker, R. S. J. d., Corbett, A. T., Gowda, S. M., Wagner, A. Z., MacLaren,
B. A., Kauﬀman, L. R., et al. (2010). Contextual slip and prediction
of student performance after use of an intelligent tutor. In P. De Bra,
A. Kobsa, D. Chin (Eds.), User modeling, adaptation, and personaliza-
tion (pp. 52–63). New York: Springer.
Baldwin, D., Fowles, M., Livingston, S. (2008). Guidelines for constructed-
responses
and
other
performance
assessments
[Research Report].
Princeton: Educational Testing Service.
Retrieved from http://

610
References
www.ets.org/Media/About ETS/pdf/8561 ConstructedResponse
guidelines.pdf.
Barr, A., Feigenbaum, E. (1982). Handbook of artiﬁcial intelligence (Vol. 2).
Los Altos: HeurisTech.
Bart, W. M., Post, T., Behr, M. J., Lesh, R. (1994). A diagnostic analysis of
a proportional reasoning test item: An introduction to the properties of
a semi-dense item. Focus on Learning Problems in Mathematics, 16(3),
1–11.
Barton, P. E. (2003). Parsing the achievement gap: Baselines for tracking
progress (Policy Information Center Report). Educational Testing Ser-
vice. Retrieved from http://www.ets.org.
Beaton, A. E., Allen, N. L. (1992). Interpreting scales through scale anchor-
ing. Journal of Educational Statistics, 17(2), 192–204.
Behrens, J. T., Mislevy, R. J., Bauer, M. I., Williamson, D. M.,
Levy, R.
(2004). Introduction to evidence centered design and lessons learned
from its application in a global e-learning program. International Jour-
nal of Measurement, 4, 295–301.
Behrens, J. T., Mislevy, R. J., DiCerbo, K. E.,
Levy, R. (2012). An evi-
dence centered design for learning and assessment in the digital world.
In M. C. Mayrath, J. Clarke-Midura, D. Robinson (Eds.), Technology-
based assessments for 21st century skills: Theoretical and practical impli-
cations from modern research (pp. 13–54). Charlotte: Information Age.
Bejar, I. I., Williamson, D. M., Mislevy, R. J. (2006). Human scoring. In
D. M. Williamson, R. J. Mislevy, I. I. Bejar (Eds.), Automated scoring of
complex tasks in computer-based testing (pp. 49–82). Mahwah: Lawrence
Erlbaum.
Bejar, I. I., Braun, H. I.,
Tannenbaum, R.
(2007).
A prospective, pre-
dictive and progressive approach to standard setting. In R. L. Lissitz
(Ed.), Assessing and modeling cognitive development in school: Intellec-
tual growth and standard setting (pp. 1–30). Maple Grove: JAM.
Bennett,
R.
E.
(2013).
Preparing
for
the
future:
What
educa-
tional
assessment
must
do.
Princeton:
The
Gordon
Commis-
sion.
Retrieved from http://www.gordoncommission.org/rsc/pdf/
bennett preparing future assessment.pdf.
Berger, J. O. (1985). Statistical decision theory and Bayesian analysis. New
York: Springer.
Berliner, M.
(2005, May).
Physical-statistical modeling and predic-
tion.
Paper presented at “Some Challenging Applications of Statis-
tical Modeling and Analysis,” a special seminar series presented at
Harvard University on the occasion of the retirement of Arthur P.
Dempster. Retrieved from http://www.stat.harvard.edu/Dempster
Symposium/Berliner.pdf.
Bertel`e, U., Brioschi, F. (1972). Nonserial dynamic programming. New York:
Academic.

References
611
Best, N. G., Cowles, M. K., Vines, K. (1996). Coda: Convergence diagnosis
and output analysis software for Gibbs sampling output version 0.30
[Computer software manual]. Cambridge, UK.
Bishop, Y. M., Fienberg, S. E., Holland, P. W. (1975). Discrete multivariate
analysis. Cambridge: MIT Press.
Black, P., Wiliam, D. (1998a). Assessment and classroom learning. Assess-
ment in Education: Principles, Policy, and Practice, 5(1), 7–74.
Black, P.,
Wiliam, D.
(1998b).
Inside the black box: Raising standards
through classroom assessment. Phi Delta Kappan, 80(2), 139–147.
Bock, R. D.
(1972). Estimating item parameters and latent ability when
responses are scored in two or more nominal categories. Psychometrika,
37(1), 29–51.
Bock, R. D., Aitkin, M. (1981). Marginal maximum likelihood estimation of
item parameters: An application of an EM-algorithm. Psychometrika,
46, 443–459.
Bock, R. D., Lieberman, M. (1970). Fitting a response model for n dichoto-
mously scored items. Psychometrika, 35, 179–197.
Bock, R. D., Mislevy, R. J. (1982). Adaptive EAP estimation of ability in a
microcomputer environment. Applied Psychological Measurement, 6(4),
431–444.
Bollen, K. A. (1989). Structural equations with latent variables. New York:
Wiley.
Boutilier, C., Dean, T., Hanks, S. (1999). Decision-theoretic planning: Struc-
tural assumptions and computational leverage.
Journal of Artiﬁcial
Intelligence Research, 11, 1–94. Retrieved from citeseer.ist.psu.
edu/viewdoc/download?doi=10.1.1.65.9397&rep=rep1&type=pdf.
Bowman, E. H. (1963). Consistency and optimality in managerial decision
making. Management Science, 9(2), 310–321.
Box, G. E. P. (1976). Science and statistics. Journal of the American Statis-
tical Association, 71, 791–799.
Box, G. E. P., Tiao, G. C. (1973). Bayesian inference in statistical analysis.
New York: Wiley.
Boyen, X.,
Koller, D.
(1998). Tractable inference for complex stochastic
process. In G. Cooper S. Moral (Eds.), Uncertainty in Artiﬁcial Intel-
ligence: Proceedings of the 14th Annual Conference (pp. 33–42). San
Francisco: Morgan Kaufmann.
Bradlow, E. T., Wainer, H.,
Wang, X. (1999). A Bayesian random eﬀects
model for testlets. Psychometrika, 64, 153–168.
Braun, H. I., Bejar, I. I.,
Williamson, D. M. (2006). Rule-based methods
for automated scoring. In D. M. Williamson, R. J. Mislevy, I. I. Bejar
(Eds.), Automated scoring of complex tasks in computer-based testing
(pp. 83–122). Hillsdale: Lawrence Erlbaum.
Breese, J. S., Goldman, R. P.,
Wellman, M. P.
(1994).
Introduction to
the special section on knowledge-based construction of probabilistic and

612
References
decision models. IEEE Transactions on System, Man and Cybernetics,
24, 1577–1579.
Breiman, L., Friedman, J. H., Olshen, R., Stone, C. J. (1984). Classiﬁcation
and regression trees. Belmont: Wadsworth.
Breland, H. M., Camp, R., Jones, R. J., Morris, M. M., Rock, D. A. (1987).
Assessing writing skills. New York, NY: College Entrance Examination
Board.
Brennan, R. L. (2001). An essay on the history and future of reliability from
the perspective of replications. Journal of Educational Measurement,
38(4), 295–317.
Brennan, R. L., Prediger, D. J. (1977). Coeﬃcient kappa: Some uses, misuses,
and alternatives (Technical Bulletin No. 29). ACT.
Breyer, F. J., Mislevy, R. J., Steinberg, L. S., Almond, R. G. (1999, April).
Designing technology-based assessments: It’s the evidence for the infer-
ences that are important. Paper presented at the Annual Convention of
the Society for Industrial Organizational Psychology, Atlanta, GA.
Bridgeman, B., Lennon, M. L., Jackenthal, A. (2001). Eﬀects of screen size,
screen resolution, and display rate on computer-based test performance
(Research Report No. RR-01-23). Princeton: Educational Testing Ser-
vice.
Brooks, S.,
Gelman, A. (1998). General methods for monitoring conver-
gence of iterative simulations. Journal of Computational and Graphical
Statistics, 7, 434–55.
Bunt, A., Conati, C. (2002). Assessing eﬀective exploration in open learning
environments using Bayesian networks. In S. A. Cerri, G. Gouarderes, &
F. Paraguacu (Eds.), Proceedings of ITS 2002, 6th international confer-
ence on intelligent tutoring systems, Biarritz, France, 4–7 June 2002.
Bunt, A.,
Conati, C. (2003). Probabilistic student modelling to improve
exploratory behaviour. User Modeling and User-Adapted Interaction,
13(3), 269–309.
Buntine, W. L. (1994). Operations for learning with graphical models. Journal
of Artiﬁcial Intelligence Research, 2, 159–225.
Buntine, W. L. (1996). A guide to the literature on learning probabilistic
networks from data. IEEE Transactions on Knowledge and Data Engi-
neering, 8, 195–210.
Burstein, J., Tetreault, J., Madnani, N. (2013). The E-rater R⃝automated
essay scoring system. In M. D. Shermis J. J. Burstein (Eds.), Handbook
of automated essay evaluation: Current applications and new directions
(pp. 55–67). New York: Routledge.
Cai, L. (2008). SEM of another ﬂavour: Two new applications of the supple-
mented em algorithm. British Journal of Mathematical and Statistical
Psychology, 61, 309–329.
Cannings, C., Thompson, E. A., Skolnick, M. H. (1978). Probability functions
on complex pedigrees. Advances in Applied Probability, 10, 26–61.

References
613
Cen, H., Koedinger, K. R., Junker, B. W. (2006). Learning factors analysis:
A general method for cognitive model evaluation and improvement. In
M. Ikeda, K. Ashlay, T.-W. Chan (Eds.), Intelligent tutoring systems,
8th international conference. Lecture notes in computer science: Vol.
4053. (pp. 164–175). Berlin: Springer.
Chaloner, K. M., Duncan, G. T. (1983). Assessment of a beta prior distri-
bution: PM elicitation. The Statistician, 32, 174–180.
Chambers, J. L. (2004). Programming with data: A guide to the S language.
New York: Springer.
Chapelle, C., Enright, M., Jamieson, J. (2008). Building a validity argument
for the Test of English as a Foreign Language. New York: Routledge.
Cheng, Y. (2009). When cognitive diagnosis meets computerized adaptive
testing: CD-CAT. Psychometrika, 74(4), 619–632.
Cheng,
B.
H.,
Ructtinger,
L.,
Fujii,
R.,
Mislevy,
R.
J.
(2010).
Assessing systems thinking and complexity in science (Large-Scale
Assessment Technical Report No. 7).
Menlo Park: SRI Interna-
tional.
Retrieved from http://ecd.sri.com/downloads/ECD TR7
Systems Thinking FL.pdf.
Chickering, D. (1996). Learning equivalence classes of Bayesian-network struc-
tures. In P. Besnard
S. Hanks (Eds.), Uncertainty in artiﬁcial intel-
ligence: Proceedings of the 11th conference (pp. 87–98).
San Mateo:
Morgan Kaufmann.
Chung, G. K. W. K., Delacruz, G. C., de Vries, L. F., Phan, C. H., Srivas-
tava, M. B., Alarcon, R. (2002, September). Fusing wireless sensor data
to measure small-group collaborative processes in real-time. Paper pre-
sented at the annual conference of the the National Center for Research
on Evaluation, Standards, and Student Testing (CRESST), Los Angeles,
CA.
Close, C. N., Davison, M. L., Davenport, E. (2012, April). An exploratory
technique for ﬁnding the Q-matrix in cognitive diagnostic assessment:
Combining theory with data. Paper presented at the annual meeting of
the National Council on Measurement in Education, Vancouver, British
Columbia, Canada.
Cobb, B. R.,
Shenoy, P. P. (2005). Hybrid Bayesian networks with linear
deterministic variables. In F. Bacchus T. Jaakkola (Eds.), Uncertainty
in artiﬁcial intelligence: Proceedings of the 21st conference (pp. 136–
144). Corvallis: AUAI Press.
Collis, J. M., Tapsﬁeld, P. G. C., Irvine, S. H., Dann, P. L.,
Wright, D.
(1995). The British Army Recruit Battery goes operational: From theory
to practice in computer-based testing using item generation techniques.
International Journal of Selection and Assessment, 3, 96–104.
Conati, C.,
Maclaren, H.
(2009).
Empirically building and evaluating a
probabilistic model of user aﬀect.
User Modeling and User-Adapted
Interaction, 19, 267–303.

614
References
Conejo, R., Guzm´an, E., Mill´an, E., Trella, M., P´erez-de-la-Cruz, J. L., R´ıos,
A. (2004). Siette: A web-based tool for adaptive testing. International
Journal of Artiﬁcial Intelligence in Education, 14(1), 29–61.
Cooper, G. F., Herskovits, E. (1992). A Bayesian method for the induction
of probabilistic networks from data. Machine Learning, 9, 309–347.
Corbett, A. T., Anderson, J. R. (1994). Knowledge tracing: Modeling the
acquisition of procedural knowledge. User Modeling and User-Adapted
Interaction, 4, 253–278.
Corcoran, T., Mosher, F. A.,
Rogat, A. (2009). Learning progressions in
science: An evidence-based approach to reform (Vol. 13; CPRE Research
Report No. RR-63). Philadelphia: Consortium for Policy Research in
Education.
Cowell, R. G., Dawid, A. P. (1992). Fast retraction of evidence in a proba-
bilistic expert system. Statistics and Computing, 2, 36–41.
Cowell, R. G., Dawid, A. P., Spiegelhalter, D. J. (1993). Sequential model
criticism in probabilistic expert systems. IEEE Transactions on Pattern
Analysis and Machine Intelligence, 15, 209–129.
Cowell, R. G., Dawid, A. P., Lauritzen, S. L.,
Spiegelhalter, D. J. (1999).
Probabilistic networks and expert systems. New York: Springer.
Cox, D. R., Wermuth, N. (1996). Multivariate dependencies: Models, analysis
and interpretation. London: Chapman and Hall.
Cronbach, L. J. (1989). Intelligence: Measurement, theory, and public policy.
In R. L. Linn (Ed.), Construct validation after thirty years (pp. 147–
171). Champaign: University of Illinois Press.
Crowley, R., Medvedeva, O. (2006). An intelligent tutoring system for visual
classiﬁcation problem solving. Artiﬁcial Intelligence in Medicine, 36(1),
85–117. doi: 10.1016/j.artmed.2005.01.005.
Daniel, B., Zapata-Rivera, J.-D., McCalla, G. I. (2003). A Bayesian compu-
tational model of social capital in virtual communities. In M. Huysman,
E. Wenger, W. Volker (Eds.), Proceedings of the first international con-
ference on communities and technologies: C&T 2003. Deventer: Kluwer
Academic.
Darroch, J. N., Lauritzen, S. L., Speed, T. P. (1980). Markov ﬁelds and log-
linear interaction models for contingency tables. The Annals of Statis-
tics, 8, 522–539.
Dawid, A. P. (1979). Conditional independence in statistical theory. Journal
of the Royal Statistical Society, Series B, 41, 1–31.
Dayton, C. M. (1999). Latent class scaling analysis. Thousand Oaks: Sage.
de Finetti, B. (1990). Theory of probability, Volume I. New York: Wiley.
de la Torre, J. (2008). An empirically based method of Q-matrix validation for
the DINA model: Development and applications. Journal of Educational
Measurement, 45, 343–362.
de la Torre, J., Douglas, J. A. (2004). Higher-order latent trait models for
cognitive diagnosis. Psychometrika, 69, 333–353.

References
615
Dean, T.,
Kanazawa, K. (1989). A model for reasoning about persistence
and causation. Computer Intelligence, 5, 142–150.
Deane, P.
(2006).
Strategies for evidence identiﬁcation through linguistic
assessment of textual responses. In D. M. Williamson, R. J. Mislevy,
I. I. Bejar (Eds.), Automated scoring of complex tasks in computer-based
testing (pp. 313–371). Hillsdale: Lawrence Erlbaum.
Deane, P., Quinlan, T. (2010). What automated analyses of corpora can tell
us about students’ writing skills. Journal of Writing Research, 2(2),
151–177.
DeGroot, M. H. (1970). Optimal statistical decisions. New York: McGraw-
Hill.
Deming, W. E., Stephan, F. F. (1940). On a least squares adjustment of a
sampled frequency table when the expected marginal totals are known.
The Annals of Mathematical Statistics, 11(4), 427–444.
Dempster, A. P. (1968). A generalization of Bayesian inference (with discus-
sion). Journal of the Royal Statistical Society, Series B, 30, 205–247.
Dempster, A. P. (1972). Covariance selection. Biometrics, 28, 157–175.
Dempster, A. P. (1990). Bayes, Fisher and belief functions. In S. Geisser,
J. S. Hodges, S. J. Press, A. Zellner (Eds.), Bayesian likelihood meth-
ods in statistics and econometrics (pp. 35–47). Amsterdam: Elsevier
Science.
Dempster, A. P., Laird, N., Rubin, D. B. (1977). Maximum likelihood from
incomplete data via the EM algorithm. Journal of the Royal Statistical
Society (Series B), 39, 1–38.
DeVito, P. J., Koenig, J. A. (Eds.). (2000). Designing a market basket for
NAEP. Washington, DC: National. Retrieved from http://www.nap.
edu/catalog/9891.html.
DiCerbo, K. E., Behrens, J. T. (2012). Implications of the digital ocean on
current and future assessment. In R. L. Lissitz H. Jiao (Eds.), Com-
puters and their impact on state assessment (pp. 273–306). Charlotte:
Information Age.
D´ıez, F. J. (1993). Parameter adjustment in Bayes networks. the generalized
noisy or-gate. In D. Heckerman
A. Mamdani (Eds.), Uncertainty in
artiﬁcial intelligence: Proceedings of the 9th conference (pp. 99–105).
San Francisco: Morgan Kaufmann.
D´ıez, F. J.,
Druzdzel, M. J.
(2006).
Canonical probabilistic models for
knowledge engineering (Technical Report No. CISIAD-06-01). Madrid:
UNED.
Doucet, A., de Freitas, N.,
Gordon, N.
(2001).
Sequential Monte Carlo
methods in practice. New York: Springer.
Draper, D. (1995). Assessment and propagation of model uncertainty. Journal
of the Royal Statistical Society (Series B), 57, 45–98.
Draper, D., Hodges, J. S., Leamer, E. E., Morris, C. N., Rubin, D. B. (1987).
A research agenda for assessment and propagation of model uncertainty
(Rand Note No. N-2683-RC). Santa Monica: RAND.

616
References
Dwyer, C. A. (1998). Psychometrics of Praxis III: Classroom performance
assessments. Journal of Personnel Evaluation in Education, 12(2), 163–
187.
Edwards, D. (1990). Hierarchical interaction models. Journal of the Royal
Statistical Society (Series B), 52, 3–20.
Edwards, D. (1995). Introduction to graphical modelling. New York: Springer.
Efron, B. (1979). Bootstrap methods: Another look at the jackknife. The
Annals of Statistics, 7, 1–26.
El Saadawi, G. M., Tseytlin, E., Legowski, E., Jukic, D., Castine, M., Fine, J.,
et al. (2008). A natural language intelligent tutoring system for train-
ing pathologists: Implementation and evaluation. Advances In Health
Sciences Education, 13(5), 709–722. doi: 10.1007/s10459-007-9081-3.
Embretson, S. E. (1983). Construct validity: Construct representation versus
nomothetic span. Psychological Bulletin, 93, 179–197.
Embretson, S. E. (1998). A cognitive design system approach to generating
valid tests: Application to abstract reasoning. Psychological Methods,
3, 380–396.
Epstein, E. S. (1969). A scoring system for probability forecasts of ranked
categories. Journal of Applied Meteorology, 8, 985–987.
Feller, W. (1968). An introduction to probability theory and its applications
(3rd ed.). New York: Wiley.
Fienberg, S. E. (1970). An iterative procedure for estimation in contingency
tables. The Annals of Mathematical Statistics, 41, 907–917.
Fischer, G. H. (1973). The linear logistic test model as an instrument in
educational research. Acta Psychologica, 37, 359–374.
Fleiss, J. L., Levin, B., Paik, M. C. (2003). Statistical methods for rates and
proportions. New York: Wiley.
Formann, A. K. (1985). Constrained latent class models: Theory and applica-
tions. British Journal of Mathematical and Statistical Psychology, 38,
87–111.
Freedman, D., Pisani, R., Purves, R. (1980). Statistics. New York: W. W.
Norton.
Fulcher, G., Davidson, F. (2009). Test architecture, test retroﬁt. Language
Testing, 26(1), 123–144.
Gamboa, H., Fred, A. L. N. (2001). Designing intelligent tutoring systems: A
Bayesian approach. In Proceedings of the 3rd international conference
on enterprise information systems (ICEIS 2002) (Vol. 3, pp. 452–458).
Setubal: ICEIS Press.
Geerlings, H., Glas, C. A. W.,
van der Linden, W. J.
(2011). Modeling
rule-based item generation. Psychometrika, 76(2), 337–359.
Gelman, A., Rubin, D. B. (1992). Inference from iterative simulation using
multiple sequences. Statistical Science, 7, 457–511.
Gelman, A., Meng, X. L., Stern, H. S. (1996). Posterior predictive assessment
of model ﬁtness via realized discrepancies (with discussion). Statistica
Sinica, 6, 733–807.

References
617
Gelman, A., Carlin, J. B., Stern, H. S., Rubin, D. B. (2013a). Bayesian data
analysis (2nd ed.). London: Chapman and Hall.
Gelman, A., Carlin, J. B., Stern, H. S., Dunson, D. B., Vehtari, A., Rubin,
D. B. (2013b). Bayesian data analysis (3rd ed.). London: CRC.
Geman, S.,
Geman, D. (1984). Stochastic relaxation, Gibbs distributions,
and the Bayesian restoration of images. IEEE Transactions on Pattern
Analysis and Machine Intelligence, 6, 721–741.
Gertner, A., Conati, C., VanLehn, K. (1998). Procedural help in Andes: Gen-
erating hints using a Bayesian network student model. In Proceedings
of the ﬁfteenth national conference on artiﬁcial intelligence AAAI-98
(pp. 106–111). Cambridge: MIT Press.
Gierl, M. J., Haladyna, T. M. (2012). Automatic item generation: Theory
and practice. New York: Routledge.
Gierl, M. J., Leighton, J. P., Hunka, S. M. (2007). Using the attribute hier-
archy method to make diagnostic inferences about examinees’ cognitive
skills. In J. P. Leighton M. J. Gierl (Eds.), Cognitive diagnostic assess-
ment: Theories and applications (pp. 242–274). Cambridge: Cambridge
University Press.
Gilks, W. R., Richardson, S.,
Spiegelhalter, D. J. (Eds.). (1996). Markov
chain Monte Carlo in practice. London: Chapman and Hall.
Gilula, Z., Haberman, S. J. (1995). Prediction functions for categorical panel
data. The Annals of Statistics, 23(4), 1130–1142.
Gilula, Z., Haberman, S. J. (2001). Analysis of categorical response proﬁles
by informative summaries. Sociological Methodology, 31(1), 129–187.
Gitomer, D. H., Steinberg, L. S. (1999). Representational issues in assess-
ment design. In I. E. Sigel (Ed.), Development of mental representation
(pp. 351–370). Mahwah: Lawrence Erlbaum.
Gitomer, D. H., Steinberg, L. S., Mislevy, R. J. (1995). Diagnostic assess-
ment of trouble-shooting skill in an intelligent tutoring system.
In
P. D. Nichols, S. F. Chipman, R. L. Brennan (Eds.), Cognitively diag-
nostic assessment (pp. 73–101). Mahwah: Lawrence Erlbaum.
Glas, C. A. W., Falc´on, J. C. S. (2003). A comparison of item-ﬁt statistics for
the three-parameter logistic model. Applied Psychological Measurement,
27(2), 87–106.
Glas, C. A. W., van der Linden, W. J. (2001, July). Modeling variability in
item parameters in CAT. Paper presented at the International Meeting
of the Psychometric Society, Osaka, Japan.
Glas, C. A. W., van der Linden, W. J. (2003). Computerized adaptive testing
with item cloning. Applied Psychological Measurement, 27, 247–261.
Glasziou, P., Hilden, J. (1989). Test selection measures. Medical Decision
Making, 9, 133–141.
Gobert, J. D., Sao Pedro, M. A., Baker, R. S. J. d., Toto, E., Montalvo, O.
(2012). Leveraging educational data mining for real time performance
assessment of scientiﬁc inquiry skills within microworlds.
Journal of
Educational Data Mining, 4, 153–185.

618
References
Good, I. (1952). Rational decisions. Journal of the Royal Statistical Society
(Series B), 14, 104-114.
Good, I.
(1971).
46656 varieties of Bayesian. American Statistician, 25,
62–63.
Good, I. (1976). The Bayesian inﬂuence, or how to sweep subjectivism under
the carpet. In C. A. Hooker W. Harper (Eds.), Foundations of probabil-
ity theory, statistical inference, and statistical theories of science (Vol. 2,
pp. 125–174). Dordrecht: D. Reidel Publishing.
Good, I. (1983). Good thinking. Minneapolis: University of Minnesota Press.
Good, I. (1985). Weight of evidence: A brief survey. In J. Bernardo, M. DeG-
root, D. Lindley, A. Smith (Eds.), Bayesian statistics 2 (pp. 249–269).
Amsterdam: North-Holland.
Good, I., Card, W. (1971). The diagnostic process with special reference to
errors. Methods of Information in Medicine, 10, 176–188.
Goodman, L. A., Kruskal, W. H. (1954). Measures of association for cross
classiﬁcations. Journal of the American Statistical Association, 49(268),
732–764. Retrieved from http://www.jstor.org/stable/2281536.
Graesser, A. C., VanLehn, K., Rose, C. P., Jordan, P. W., Harter, D. (2001).
Intelligent tutoring systems with conversational dialogue. AI Magazine,
22(4), 39–52.
Graf, E. A. (2003, September). Designing a proﬁciency model and associated
item models for a mathematics unit on sequences. Paper presented at
the Cross Division Math Forum, Princeton, NJ.
Graf, E. A.
(2008).
Approaches to the design of diagnostic item mod-
els
(Research Report No. RR-08-07).
Educational Testing Ser-
vice.
Retrieved from http://www.ets.org/research/researcher/
RR-08-07.html.
Guttman, I. (1967). The use of the concept of a future observation in goodness-
of-ﬁt problems. Journal of the Royal Statistical Society, Series B, 29,
83–100.
Haberman, S. J.
(1972). Log-linear ﬁt for contingency tables—Algorithm
AS51. Applied Statistics, 21, 218–225.
Haberman, S. J. (2005a). Latent-class item response models (Research Report
No. RR-05-28). Princeton: Educational Testing Service.
Haberman, S. J. (2005b). When can subscores have value? (Research Report
No. RR-05-08). ETS. Retrieved from http://www.ets.org/research/
researcher/RR-05-08.html.
Haberman, S. J. (2009). Use of generalized residuals to examine goodness of
ﬁt of item response models (Research Report No. RR-09-15). Princeton:
Educational Testing Service.
Haberman, S. J., Sinharay, S., Chon, K. H. (2013). Assessing item ﬁt for uni-
dimensional item response theory models using residuals from estimated
item response functions. Psychometrika, 78(3), 417–440.
Haertel, E. H. (1984). An application of latent class models to assessment
data. Applied Psychological Measurement, 8, 333–346.

References
619
Haertel, E. H. (1989). Using restricted latent class models to map the skill
structure of achievement test items. Journal of Educational Measure-
ment, 26, 301–321.
Hambleton, R. K.,
Pitoniak, M. J. (2006). Educational measurement. In
R. L. Brennan (Ed.), Educational measurement (4th ed., pp. 433–470).
Westport: American Council on Education/Praeger.
Hambleton, R. K., Swaminathan, H., Rogers, H. J. (1991). Fundamentals of
item response theory. Newbury Park: Sage.
Hansen, E. G., Mislevy, R. J. (2004, April). Toward a uniﬁed validity frame-
work for ensuring access to assessments by individuals with disabilities
and English language learners. Paper presented at the annual meeting
of the National Council on Measurement in Education. Retrieved from
http://www.ets.org/research/dload/NCME2004-Hansen.pdf.
Hansen, E. G., Mislevy, R. J. (2005). Accessibility of computer-based testing
for individuals with disabilities and English language learners within a
validity framework. In M. Hricko (Ed.), Online assessment and mea-
surement: Foundation, challenges, and issues (pp. 212–259). Hershey:
Idea Group.
Hansen, E. G., Mislevy, R. J.,
Steinberg, L. S.
(2003).
Evidence-
centered assessment design and individuals with disabilities.
Paper
presented at annual meeting of the National Council on Measurement
in Education. Retrieved from http://www.ets.org/research/dload/
ncme03-hansen.pdf.
Hartz, S. M. (2002). A Bayesian framework for the uniﬁed model for assessing
cognitive abilities: Blending theory with practice. Unpublished doctoral
dissertation, University of Illinois at Urbana-Champaign.
Hastings, W. (1970). Monte Carlo sampling methods using Markov chains
and their applications. Biometrika, 57, 97–109.
Heckerman, D. (1991). Probabilistic similarity networks. New York: ACM
Press.
Heckerman, D. (1998). A tutorial on learning with Bayesian networks. In
M. I. Jordan (Ed.), Learning in graphical models (pp. 301–354). Ams-
terdam: Kluwer Academic.
Heckerman, D., Gieger, D., Chickering, D. (1995). Learning Bayesian net-
works: The combination of knowledge and statistical data.
Machine
Learning, 20, 197–243.
Heckerman, D., Horvitz, E.,
Middleton, B. (1993). An approximate non-
myopic computation for value of information. IEEE Transactions on
Pattern Analysis and Machine Intelligence, 15, 292–298.
Heidelberger, P., Welch, P. D. (1981). A spectral method for conﬁdence inter-
val generation and run length control in simulations. Communications
of the ACM , 24, 233–245.
Henrion, M., Druzdzel, M. J. (1990). Qualitative propagation and scenario-
based approaches to explanation of probabilistic reasoning. In Uncer-

620
References
tainty in artiﬁcial intelligence: Proceedings of the 6th conference (pp. 10–
20). Mountain View: Association for Uncertainty in AI.
Henson, R. A., Douglas, J. A. (2005). Test construction for cognitive diag-
nosis. Applied Psychological Measurement, 29(4), 262–277.
Henson, R. A., Templin, J. L.,
Willse, J. T. (2009). Deﬁning a family of
cognitive diagnosis models using log-linear models with latent variables.
Psychometrika, 74, 191–210.
Henze, N., Nejdl, W. (1999). Student modeling in an active learning envi-
ronment using Bayesian networks. In Proceedings of the seventh inter-
national conference on user modeling, UM99.
Hilden, J. (1970). GENEXX—An algebraic approach to pedigree probability
calculus. Clinical Genetics, 1, 319–348.
Hively, W., Patterson, H. L., Page, S. H. (1968). A ‘universe-deﬁned’ system
of arithmetic achievement tests. Journal of Educational Measurement,
5(4), 275–290.
Hoey, J., St-Aubin, R., Hu, A.,
Boutilier, C.
(2001).
SPUDD: Stochas-
tic planning using decision diagrams. In K. Laskey
H. Prad (Eds.),
Uncertainty in artiﬁcial intelligence: Proceedings of the 15th conference
(pp. 279–288). San Francisco: Morgan Kaufmann.
Holland, P. W. (1986). Statistics and causal inference. Journal of the Amer-
ican Statistical Association, 81, 945–960.
Holland, P. W.,
Thayer, D. T. (1988). Diﬀerential item performance and
the Mantel-Haenszel procedure. In H. Wainer H. I. Braun (Eds.), Test
validity (pp. 129–145). Mahwah: Lawrence Erlbaum.
Holland, P. W., Wainer, H. (1993). Diﬀerential item functioning. Mahwah:
Lawrence Erlbaum.
Howard, R. A., Matheson, J. E. (1984). Inﬂuence diagrams. In A. Howard
J. E. Matheson (Eds.), Readings on the principles and applications of
decision analysis (Vol. 2, pp. 717–762). Menlo Park: Strategic Decisions
Group.
Hrycej, T. (1990). Gibbs sampling in Bayesian networks. Artiﬁcial Intelli-
gence, 46, 351–363.
Huﬀ, K., Steinberg, L. S.,
Matts, T.
(2010).
The promises and chal-
lenges of implementing evidence-centered design in large-scale assess-
ment. Applied Measurement in Education, 23(4), 310–324.
IMS Global Learning Consortium. (2000). IMS question & test interoperabil-
ity information model speciﬁcation (Version 1.0 ed.) [Computer software
manual]. Retrieved from http://www.imsproject.org.
Irvine, S. H. (2013). Tests for recruitment across cultures: a tactical psycho-
metric handbook. Amsterdam: IOS.
Irvine, S. H.,
Kyllonen, P. (Eds.). (2002). Generating items for cognitive
tests: Theory and practice. Mahwah: Erlbaum.
Irvine, S. H., Dann, P. L.,
Anderson, J. D. (1990). Towards a theory of
algorithm-determined cognitive test construction.
British Journal of
Psychology, 81, 173–195.

References
621
Iseli, M. R., Koenig, A. D., Lee, J. J., Wainess, R. (2010). Automatic assess-
ment of complex task performance in games and simulations (CSE Tech-
nical Report No. 775). Los Angeles: The National Center for Research
on Evaluation, Standards, Student Testing (CRESST). Retrieved from
http://www.cse.ucla.edu/products/reports/R775.pdf.
Jaakkola, T. S. (2001). Tutorial on variational approximation methods. In
M. Opper D. Saad (Eds.), Advanced mean ﬁeld methods: Theory and
practice (pp. 129–159). Cambridge: MIT Press.
Jaynes, E. T. (1968). Prior probabilities. IEEE Transactions on Systems
Science and Cybernetics, SSC-4, 227–241.
Jeﬀreys, H. (1961). Theory of probability (3rd ed.). New York: Oxford Uni-
versity Press.
Jensen, F. V. (1988). Junction trees and decomposable hypergraphs (Judex
Research Report). Aalborg: Judex.
Jensen, F. V.
(1996). An introduction to Bayesian networks.
New York:
Springer.
Johnson, M. S., Sinharay, S. (2003). Calibration of polytomous item families
using Bayesian hierarchical modeling (Research Report No. RR-03-23).
Princeton: Educational Testing Service.
Jordan, M. I. (Ed.). (1998). Learning in graphical models. Amsterdam: Kluwer
Academic.
Joreskog, K. G., Sorbom, D. (1979). Advances in factor analysis and struc-
tural equation models. Cambridge: Abt.
Junker, B. W. (1999). Some statistical models and computational methods
that may be useful for cognitively-relevant assessment.
Junker, B. W., Sijtsma, K. (2001). Cognitive assessment models with few
assumptions, and connections with nonparametric item response theory.
Applied Psychological Measurement, 25, 258–272.
Kadane, J. B. (1980). Predictive and structural methods for eliciting prior
distributions. In A. Zellner (Ed.), Bayesian analysis and statistics (pp.
89–93). Amsterdam: North-Holland.
Kadane, J. B., Dickey, J. M., Winkler, R. L., Smith, W. S.,
Peters, S. C.
(1980).
Interactive elicitation of opinion for a normal linear model.
Journal of the American Statistical Association, 75, 845–854.
Kahneman, D., Slovic, P., Tversky, A. (1982). Judgment under uncertainty:
Heuristics and biases. Cambridge: Cambridge University Press.
Kane, M. T. (1992). An argument-based approach to validity. Psychological
Bulletin, 112, 527–535.
Kane, M. T. (2006). Validation. In R. L. Brennan (Ed.), Educational mea-
surement (4th ed., pp. 17–64). Westport: American Council on Educa-
tion/Praeger.
Kane, M. T. (2013). Validating the interpretations and uses of test scores.
Journal of Educational Measurement, 50(1), 1–73.
Kaplan, D. (2000). Structural equation modeling: Foundations and extensions.
Thousand Oaks: Sage.

622
References
Katz, I. R., Williamson, D. M., Nadelman, H. L., Kirsch, I., Almond, R. G.,
Cooper, P. L., et al. (2004, June). Assessing information and communi-
cations technology literacy for higher education. Paper presented at the
30th annual conference of the International Association for Educational
Assessment, Philadelphia, PA.
Kennedy, C. A.,
Wilson, M. R. (2006, October). Using progress variables
to map intellectual development. Paper presented at MSDE/MARCES
conference, College Park, MD.
Kennedy, C. A., Wilson, M. R., Draney, K., Tutunciyan, S., Vorp, R. (2006).
ConceptMap. [Computer software]. Berkeley: Bear Center. Retrieved
from http://bearcenter.berkeley.edu/GradeMap.
Kim, J. H., Pearl, J. (1983). A computational model for causal and diag-
nostic reasoning in inference systems. In Proceedings of the 8th inter-
national joint conference on artiﬁcial intelligence (pp. 190–193). Karl-
sruhe: William Kaufmann.
Klein, M. F., Birenbaum, M., Standiford, S. N., Tatsuoka, K. K. (1981). Log-
ical error analysis and construction of tests to diagnose student “bugs”
in addition and subtraction of fractions (Research Report No. 81-6).
Computer-based Education Research Laboratory, University of Illinois.
Koedinger, K. R.,
Aleven, V.
(2007).
Exploring the assistance dilemma
in experiments with cognitive tutors. Educational Psychology Review,
19(3), 239–264. doi: 10.1007/s10648-007-9049-0.
Kohavi, R. (1995). A study of cross-validation and bootstrap for accuracy
estimation and model selection. In C. S. Mellish (Ed.), Proceedings of
IJCAI-95, Montreal, Canada (pp. 1137–1143). Los Altos, CA: Morgan
Kaufmann.
Kolen, M. J.,
Brennan, R. L. (2004). Test equating, scaling, and linking:
Methods and practices. New York: Springer.
Koller, D., Learner, U. (2001). Sampling in factored dynamic systems. In
A. Doucet, N. de Freitas,
N. Gordon (Eds.), Sequential Monte Carlo
methods in practice (pp. 445–464). New York: Springer.
Koller, D.,
Pfeﬀer, A.
(1997).
Object-oriented Bayesian networks.
Uncertainty in Artiﬁcial Intelligence: Proceedings of the 13th Confer-
ence (pp. 302–313).
Retrieved from http://citeseer.nj.nec.com/
koller97objectoriented.html.
Lange, K. (1995). A quasi-Newton acceleration of the EM algorithm. Statistica
Sinica, 5(1), 1–18.
Laskey, K. B., Mahoney, S. M. (2000). Network engineering for agile belief
network models. IEEE Transactions on Knowledge and Data Engineer-
ing, 12, 481–486.
Lauritzen, S. L. (1992). Propagation of probabilities, means, and variances in
mixed graphical association models. Journal of the American Statistical
Association, 87, 1098–1108.
Lauritzen, S. L. (1996). Graphical models. Oxford: Oxford University Press.

References
623
Lauritzen, S. L., Spiegelhalter, D. J. (1988). Local computation with proba-
bilities on graphical structures and their application to expert systems
(with discussion). Journal of the Royal Statistical Society, Series B, 50,
205–247. (Reprinted in Shafer and Pearl (1990)).
Lee, P. M.
(1989).
Bayesian statistics: An introduction.
Oxford: Oxford
University Press.
Leighton, J. P., Gierl, M. J. (Eds.). (2007). Cognitive diagnostic assessment:
Theories and applications. Cambridge: Cambridge University Press.
Leighton, J. P., Gierl, M. J., Hunka, S. M. (2004). The attribute hierarchy
model: An approach for integrating cognitive theory with assessment
practice. Journal of Educational Measurement, 41, 205–236.
Levy, R. (2011). Posterior predictive model checking for conjunctive mul-
tidimensionality in item response theory. Journal of Educational and
Behavioral Statistics, 36, 672–694.
Levy, R. (2014). Dynamic Bayesian network modeling of game based diag-
nostic assessments (CSE Technical Report 837).
Los Angeles: The
National Center for Research on Evaluation, Standards, Student Testing
(CRESST).
Levy, R., Mislevy, R. J.,
Sinharay, S. (2009). Posterior predictive model
checking for multidimensionality in item response theory. Applied Psy-
chological Measurement, 33, 519–537.
Ley, T., Kump, B., Albert, D. (2010). A methodology for eliciting, modelling,
and evaluating expert knowledge for an adaptive work-integrated learn-
ing system. International Journal of Human-Computer Studies, 68(4),
185–208. doi: 10.1016/j.ijhcs.2009.12.001.
Li, Z., D’Ambrosio, B. (1994). Eﬃcient inference in Bayes nets as a combi-
natorial optimization problem. Intl Journal of Approximate Reasoning,
11, 55–81.
Little, R., Rubin, D. B. (1987). Statistical analysis with missing data. New
York: Wiley.
Liu, J. S. (2001). Monte Carlo strategies in scientiﬁc computing. New York:
Springer.
Liu, J., Xu, G., Ying, Z. (2012). Data-driven learning of Q-matrix. Applied
Psychological Measurement, 36, 548–564.
Lord, F. M. (1980). Applications of item response theory to practical testing
problems. Mahwah: Lawrence Erlbaum.
Louis, T. A. (1982). Finding the observed information matrix when using the
EM algorithm. Journal of the Royal Statistical Society, Series B, 44,
226–233.
Luecht, R. M. (2012). An introduction to assessment engineering for auto-
matic item generation. In M. J. Gierl T. M. Haladyna (Eds.), Auto-
matic item generation: Theory and practice (pp. 59–76).
New York:
Routledge.

624
References
Lunn, D. J., Spiegelhalter, D. J., Thomas, A.,
Best, N. G.
(2009).
The
BUGS project: Evolution, critique and future directions (with discus-
sion). Statistics in Medicine, 28, 3049–3082.
Lunn, D. J., Thomas, A., Best, N. G., Spiegelhalter, D. J. (2000). WinBUGS –
a Bayesian modeling framework: Concepts, structure, and extensibility.
Statistics and Computing, 10, 325–337.
Lynch, S. M. (2007). Introduction to applied Bayesian statistics and estima-
tion for social sciences. New York: Springer.
Madigan, D.,
Almond, R. G.
(1995).
Test selection strategies for belief
networks. In D. Fisher H. J. Lenz (Eds.), Learning from data: AI and
Statistics V (pp. 89–98). New York: Springer.
Madigan, D., Gavrin, J., Raftery, A. E. (1995a). Enhancing the predictive
performance of Bayesian graphical models. Communications in Statis-
tics: Theory and Methods, 24, 2271–2292.
Madigan, D., Hunt, E.,
Levidow, B.
(1995b). Bayesian graphical model-
ing for intelligent tutoring systems (Tech. Rep.). Seattle: University of
Washington, Department of Statistics.
Madigan, D., Mosurski, K.,
Almond, R. G.
(1997).
Graphical expla-
nation in belief networks.
Journal of Computational Graphics and
Statistics, 6(2), 160–181.
Retrieved from http://www.amstat.org/
publications/jcgs/index.cfm?fuseaction=madiganjun.
Madigan, D., Raftery, A. E., Volinsky, C.,
Hoeting, J.
(1996).
Bayesian
model averaging. In Proceedings of the AAAI Workshop on Integrating
Multiple Learned Models.
Mahoney, S. M.,
Laskey, K. B. (1996). Network engineering for complex
belief networks. In E. Horvitz F. Jensen (Eds.), Uncertainty in artiﬁ-
cial intelligence: Proceedings of the 12th conference (pp. 389–396). San
Francisco: Morgan Kaufmann.
Maris, E. (1999). Estimating multiple classiﬁcation latent class models. Psy-
chometrika, 64(2), 187–212.
Martin, J., VanLehn, K. (1994). Discrete factor analysis: Learning hidden
variables in Bayesian networks (Technical Report No. LRDC-ONR-94-
1). Pittsburgh: LRDC, University of Pittsburgh.
Martin, J., VanLehn, K. (1995). A Bayesian approach to cognitive assessment.
In P. D. Nichols, S. F. Chipman,
R. L. Brennan (Eds.), Cognitively
diagnostic assessment (pp. 141–165). Mahwah: Lawrence Erlbaum.
Matheson, J. E. (1990). Using inﬂuence diagrams to value information and
control. In R. M. Oliver, J. Q. Smith (Eds.), Inﬂuence diagrams, belief
nets and decision analysis (pp. 25–48). New York: Wiley.
Matsuda, N., VanLehn, K. (2000). Decision theoretic instructional planner
for intelligent tutoring systems. In B. du Boulay (Ed.), Proceedings for
workshop on modeling human teaching tactics and strategies (ITS 2000)
(pp. 72–83).

References
625
Mayo, M.,
Mitrovic, A. (2001). Optimising ITS behaviour with Bayesian
networks and decision theory. International Journal of Artiﬁcial Intel-
ligence in Education, 12(2), 124–153.
McLachlan, G., Krishnan, T. (2008). The EM algorithm and extensions (2nd
ed.). New York: Wiley.
Meijer, R. R., Sijtsma, K. (2001). Methodology review: Evaluating person
ﬁt. Applied Psychological Measurement, 25(2), 107–135.
Melnick, D. (1996). The experience of the National Board of Medical Exam-
iners. In E. Mancall, P. Vashook, J. Dockery (Eds.), Computer-based
examinations for board certiﬁcation (pp. 111–120). Chicago: American
Board of Medical Specialties.
Messick, S.
(1989).
Validity.
In R. L. Linn (Ed.), Educational measure-
ment (3rd ed., pp. 13–103). New York: American Council on Educa-
tion/Macmillan.
Messick, S. (1994). The interplay of evidence and consequences in the valida-
tion of performance assessments. Educational Researcher, 23(2), 13–23.
Metropolis, N., Ulam, S. (1949). The Monte Carlo method. Journal of the
American Statistical Association, 44, 335–341.
Mill´an, E., P´erez-de-la-Cruz, J. L. (2002). A Bayesian diagnostic algorithm for
student modeling and its evaluation. User Modeling and User-Adapted
Interaction, 12(2–3), 281–330.
Miller, P.
(1983).
Attending: Critiquing a physician’s management plan.
IEEE Transactions on Pattern Analysis and Machine Intelligence, 5,
449–461.
Mislevy, J., Rupp, A. A., Harring, J. R. (2012). Detecting local item depen-
dence in polytomous adaptive data. Journal of Educational Measure-
ment, 49, 127–147.
Mislevy, R. J. (1984). Estimating latent distributions. Psychometrika, 49,
359–381.
Mislevy, R. J.
(1986).
Bayes modal estimation in item response models.
Psychometrika, 51, 177–195.
Mislevy, R. J.
(1994).
Evidence and inference in educational assessment.
Psychometrika, 59, 439–483.
Mislevy, R. J.
(1995a).
Information-decay pursuit of dynamic parameters
in student models (Research Report No. RM-94-14-onr).
Princeton:
Educational Testing Service.
Retrieved from http://www.ets.org/
research/policy research reports/rm-94-14-onr.
Mislevy, R. J. (1995b). Probability-based inference in cognitive diagnosis.
In P. D. Nichols, S. F. Chipman,
R. L. Brennan (Eds.), Cognitively
diagnostic assessment (p. 43-71). Mahwah: Lawrence Erlbaum.
Mislevy, R. J.
(1995c). Test theory and language learning in assessment.
Language Testing, 12, 341–369.
Mislevy, R. J. (2006). Cognitive psychology and educational assessment. In
R. L. Brennan (Ed.), Educational measurement (4th ed., pp. 257–305).
American Council on Education/Praeger.

626
References
Mislevy, R. J. (2009). Validity from the perspective of model-based reasoning.
In R. W. Lissitz (Ed.), The concept of validity: Revisions, new directions,
and applications (pp. 83–108). Charlotte: Information Age.
Mislevy, R. J. (2010). Some implications of expertise research for educational
assessment. Research Papers in Education, 25(3), 253–270.
Mislevy, R. J. (2015). Missing responses in item response theory. In W. J. van
der Linden R. K. Hambleton (Eds.), Handbook of item response theory
(2nd ed.). New York: Chapman & Hall.
Mislevy, R. J., Gitomer, D. H. (1996). The role of probability based infer-
ence in an intelligent tutoring system. User-Modeling and User-Adapted
Interaction, 5, 253–282.
Mislevy, R. J., Haertel, G. D. (2006). Implications of evidence-centered design
for educational testing. Educational Measurement: Issues and Practice,
25(4), 6–20.
Mislevy, R. J.,
Riconscente, M. M. (2006). Evidence-centered assessment
design. In S. Downing, T. Haladyna (Eds.), Handbook of Test Develop-
ment (pp. 61–90). Mahwah: Erlbaum.
Mislevy, R. J., Wu, P.-K. (1996). Missing responses and Bayesian IRT ability
estimation: Omits, choice, time limits, and adaptive testing (Research
Report No. RR-96-30-ONR). Princeton: Educational Testing Service.
Mislevy, R. J., Sheehan, K. M.,
Wingersky, M. S. (1993). How to equate
tests with little or no data. Journal of Educational Measurement, 30,
55–78.
Mislevy, R. J., Wingersky, M. S.,
Sheehan, K. M.
(1994).
Dealing
with uncertainty about item parameters: Expected response functions
(Research Report No. RR-94-28-ONR). Princeton: Educational Testing
Service. Retrieved from http://www.ets.org/research/researcher/
RR-94-28-ONR.html.
Mislevy, R. J., Almond, R. G., Steinberg, L. S. (1998). A note on knowledge-
based model construction in educational assessment (CSE Technical
Report No. 480). Los Angeles: The National Center for Research on
Evaluation, Standards, Student Testing (CRESST).
Retrieved from
http://www.cresst.org/reports/TECH480.pdf
Mislevy, R. J., Almond, R. G., Yan, D., Steinberg, L. S. (1999a). Bayes nets in
educational assessment: Where the numbers come from. In K. B. Laskey
H. Prade (Eds.), Uncertainty in artiﬁcial intelligence: Proceedings of the
15th conference (pp. 437–446). San Francisco: Morgan Kaufmann.
Mislevy, R. J., Steinberg, L. S., Breyer, F. J., Almond, R. G., Johnson, L.
(1999b). A cognitive task analysis, with implications for designing a
simulation-based assessment system. Computers and Human Behavior,
15, 29–42.
Mislevy, R. J., Almond, R. G., Yan, D., Steinberg, L. S. (2000). Bayes nets in
educational assessment: Where do the numbers come from? (CSE Tech-
nical Report No. 518). Los Angeles: The National Center for Research

References
627
on Evaluation, Standards, Student Testing (CRESST). Retrieved from
http://www.cse.ucla.edu/products/reports/TECH518.pdf.
Mislevy, R. J., Almond, R. G., DiBello, L. V., Jenkins, F., Steinberg, L. S.,
Yan, D.,
Senturk, D.
(2002a). Modeling conditional probabilities in
complex educational assessments (CSE Technical Report No. 580). Los
Angeles: The National Center for Research on Evaluation, Standards,
Student Testing (CRESST). Retrieved from http://www.cresst.org/
reports/TR580.pdf
Mislevy, R. J., Almond, R. G., Steinberg, L. S. (2002b). Design and analysis
in a task-based language assessment. Language Testing, 19(4), 477–496.
Mislevy, R. J., Steinberg, L. S., Almond, R. G. (2002c). On the roles of task
model variables in assessment design. In S. H. Irvine P. Kyllonen (Eds.),
Generating items for cognitive tests: Theory and practice (p. 97–128).
Mahwah: Lawrence Erlbaum.
Mislevy, R. J., Steinberg, L. S., Breyer, F. J., Almond, R. G.,
Johnson,
L. (2002d). Making sense of data from complex assessments. Applied
Measurement in Education, 15(4), 363–389.
Mislevy, R. J., Steinberg, L. S.,
Almond, R. G.
(2003a).
On the struc-
ture of educational assessments (CSE Technical Report No. 597). Los
Angeles: The National Center for Research on Evaluation, Standards,
Student Testing (CRESST).
Retrieved from http://www.cse.ucla.
edu/products/reports/TR597.pdf.
Mislevy, R. J., Steinberg, L. S., Almond, R. G. (2003b). On the structure
of educational assessment (with discussion). Measurement: Interdisci-
plinary Research and Perspective, 1(1), 3–62.
Mislevy, R. J., Steinberg, L. S., Almond, R. G., Haertel, G. D.,
Penuel,
W. (2003c). Leverage points for improving educational assessment. In
B. Means G. D. Haertel (Eds.), Evaluating the eﬀects of technology in
education (pp. 149–180). Mahwah: Lawrence Erlbaum.
Mislevy, R. J., Almond, R. G.,
Lukas, J. F.
(2004).
A brief introduc-
tion to evidence-centered design (CSE Technical Report No. 632). Los
Angeles: The National Center for Research on Evaluation, Standards,
Student Testing (CRESST). Retrieved from http://www.cresst.org/
reports/r632.pdf (Also ETS Research Report RR-03-32.)
Mislevy, R. J., Steinberg, L. S., Almond, R. G., Lukas, J. F. (2006). Con-
cepts, terminology and basic models of evidence-centered design.
In
D. M. Williamson, R. J. Mislevy,
I. I. Bejar (Eds.), Automated scor-
ing of complex tasks in computer-based testing (pp. 15–47). Hillsdale:
Lawrence Erlbaum.
Mislevy, R. J., Riconscente, M. M., Rutstein, D. W. (2009). Design patterns
for assessing model-based reasoning (Large-Scale Assessment Technical
Report No. 6). Menlo Park: SRI International. Retrieved from http://
ecd.sri.com/downloads/ECD TR6 Model-Based Reasoning.pdf.
Mislevy, R. J., Behrens, J. T., Bennett, R. E., DeMark, S. F., Frezzo, D. C.,
Levy, R., et al.
(2010).
On the roles of external knowledge repre-

628
References
sentations in assessment design. The Journal of Technology, Learning
and Assessment, 8(2). Retrieved from http://napoleon.bc.edu/ojs/
index.php/jtla/article/viewFile/1621/1465.
Mislevy, R. J., Behrens, J. T., DiCerbo, K. E., Levy, R. (2012). Design and
discovery in educational assessment: Evidence-centered design, psycho-
metrics, and educational data mining. JEDM-Journal of Educational
Data Mining, 4(1), 11–48.
Mislevy, R. J., Haertel, G. D., Cheng, B. H., Ructtinger, L., DeBarger, A.,
Murray, E., et al. (2013). A ‘conditional’ sense of fairness in assessment.
Educational Research and Evaluation, 19(2–3), 121–140.
Mislevy, R. J., Oranje, A., Bauer, M. I., von Davier, A., Hao, J., Corrigan, S.,
et al. (2014). Psychometric considerations in game-based assessment.
New York: Institute of Play.
Morgan, M. G., Henrion, M. (1990). Uncertainty: A guide to dealing with
uncertainty in quantitative risk and policy analysis. Cambridge: Cam-
bridge University Press.
Moss, P. A., Girard, B. J., Haniford, L. C. (2006). Validity in educational
assessment. Review of Research in Education, 30, 109–162.
Moss, P. A., Pullin, D. C., Gee, J. P., Haertel, E. H., Young, L. J. (2008).
Assessment, equity, and opportunity to learn. Cambridge: Cambridge
University Press.
Moussouris, J. (1974). Gibbs and Markov random systems with constraints.
Journal of Statistical Physics, 10, 11–33.
Mulder, J., van der Linden, W. J. (2010). Multidimensional adaptive test-
ing with Kullback–Leibler information item selection. In Elements of
adaptive testing (pp. 77–101). New York: Springer.
Muraki, E. (1992). A generalized partial credit model: Application of an EM
algorithm. Applied Psychological Measurement, 16(2), 159–176. doi:
10.1177/014662169201600206.
Murphy, K. P.,
Russell, S. (2001). Rao-Blackwellised particle ﬁltering for
dynamic Bayesian networks. In A. Doucet, N. de Freitas, N. Gordon
(Eds.), Sequential Monte Carlo methods in practice (pp. 499–515). New
York: Springer.
Murphy, K. P., Weiss, Y., Jordan, M. I. (1999). Loopy belief propagation for
approximate inference: An empirical study. In K. B. Laskey H. Prade
(Eds.), Uncertainty in Artiﬁcial Intelligence: Proceedings of the 15th
conference (pp. 467–475). San Mateo: Morgan Kaufmann.
Murray, R., VanLehn, K., Mostow, J. (2004). Looking ahead to select tutorial
actions: A decision-theoretic approach. International Journal of Artiﬁ-
cial Intelligence in Education, 14(3, 4), 235–278.
National Research Council (Ed.). (1996). National science education stan-
dards. Washington, DC: National Academies Press.
Neal, R. M. (2003). Slice sampling (with discussion). Annals of Statistics,
31, 705–767.

References
629
Neapolitan, R. E. (1990). Probabilistic reasoning in expert systems: Theory
and algorithms. New York: Wiley.
Neapolitan, R. E. (2004). Learning Bayesian networks. Englewood Cliﬀs:
Prentice Hall.
Newell, A., Simon, H. A. (1972). Human problem solving. Englewood Cliﬀs:
Prentice Hall.
Neyman, J., Scott, E. L. (1948). Consistent estimators based on partially
consistent observations. Econometrika, 16, 1–32.
NGSS Lead States. (2013). Next generation science standards: For states, by
states. Washington, DC: National Academies Press.
Nichols, P. D., Chipman, S. F., Brennan, R. L. (Eds.). (1995). Cognitively
diagnostic assessment. Mahwah: Lawrence Erlbaum.
Nicholson, A. E., Jitnah, N. (1998). Using mutual information to determine
relevance in Bayesian networks. In H-Y Lee, H. Motoda (Eds.), Paciﬁc
Rim International Conference on Artiﬁcial Intelligence (pp. 399–410).
Berlin: Springer
Nikovski, D., Brand, M. (2003). Model minimization of dynamic belief net-
works for group elevator control. In Uncertainty in artiﬁcial intelligence:
Proceedings of the 19th conference, 1st Bayesian modeling application
workshop. (Vol. 3, pp. 9–13).
Nocedal, J., Wright, S. J. (2006). Numerical optimization (2nd ed.). New
York: Springer.
Norsys, Inc. (2004). Netica [Computer software manual]. Retrieved from
http://www.norsys.com.
O’Hagan, A. (1976). On posterior joint and marginal modes. Biometrika, 63,
329–333.
O’Hagan, A., Buck, C. E., Daneshkhah, A., Eiser, J. R., Garthwaite, P. H.,
Jenkinson, D. J., et al. (2006). Uncertain judgements: Eliciting experts’
probabilities. New York: Wiley.
Oliver, R. M.,
Smith, J. Q.
(1990).
Inﬂuence diagrams, belief nets and
decision analysis. New York: Wiley.
Osburn, H. (1968). Item sampling for achievement testing. Educational and
Psychological Measurement, 28, 95–104.
Patz, R. J., Junker, B. W. (1999a). Applications and extensions of MCMC
in IRT: Multiple item types, missing data, and rated responses. Journal
of Educational and Behavioral Statistics, 24(4), 342–366.
Patz, R. J., Junker, B. W. (1999b). A straight forward approach to Markov
chain Monte Carlo methods for item response models. Journal of Edu-
cational and Behavioral Statistics, 24, 146–178.
Pearl, J. (1988). Probabilistic reasoning in intelligent systems: Networks of
plausible inference. San Mateo: Morgan Kaufmann.
Pearl, J. (1998). Graphs, causality, and structural equation models. Socio-
logical Methods and Research, 27(2), 226–284.
Pearl, J. (2009). Causality: Models, reasoning and inference (2nd ed.). Cam-
bridge: Cambridge University Press.

630
References
Pelligrino, J., Glaser, R., Chudowsky, N. (Eds.). (2001). Knowing what stu-
dents know: The science and design of educational assessment. Wash-
ington, DC: National Research Council.
Plummer, M. (2008). Penalized loss functions for Bayesian model comparison.
Biostatistics, 9(3), 523–539. doi: 10.1093/biostatistics/kxm049.
Plummer, M.
(2012, May).
JAGS version 3.2.0 user manual (3.2.0 ed.)
[Computer software manual].
Retrieved from http://mcmc-jags.
sourceforge.net/.
Plummer, M., Best, N. G., Cowles, M. K.,
Vines, K. (2006). coda: Out-
put analysis and diagnostics for MCMC [Computer software manual].
Retrieved from http://cran.r-project.org/web/packages/coda/.
R Development Core Team.
(2007).
R: A language and environment for
statistical computing [Computer software manual].
Vienna, Austria.
Retrieved from http://www.R-project.org.
Rasch, G. (1960). Probabilistic models for some intelligence and attainment
tests. Chicago: The University of Chicago Press.
Reckase, M. D. (2009). Multidimensional item response theory. New York:
Springer.
Restle, F., Greeno, J. G. (1970). Introduction to mathematical psychology.
Reading: Addison-Wesley.
Reye, J. (2004). Student modelling based on belief networks. International
Journal of Artiﬁcial Intelligence in Education, 14, 63–96.
Rijmen, F. (2008). Bayesian networks with a logistic regression model for the
conditional probabilities. International Journal of Approximate Reason-
ing, 48, 659–666.
Rijmen, F., De Boeck, P.,
Leuven, K. (2002). The random weights linear
logistic test model. Applied Psychological Measurement, 26(3), 271–285.
Ritter, S., Anderson, J. R., Koedinger, K. R., Corbett, A. T. (2007). Cog-
nitive tutor: Applied research in mathematics education. Psychonomic
Bulletin and Review, 14(2), 249–255.
Robins, J. M., van der Vaart, A., Ventura, V. (2000). The asymptotic distri-
bution of p-values in composite null models (with discussion). Journal
of the American Statistical Association, 95(422), 1143–1172.
Ross, S. M. (1988). A ﬁrst course in probability. New York: Macmillan.
Rost, J.
(1990).
Rasch models in latent classes: An integration of two
approaches to item analysis. Applied Psychological Measurement, 14,
271–282.
Roussos, L. A., DiBello, L. V., Stout, W. F., Hartz, S. M., Henson, R. A.,
Templin, J. L. (2007a). The fusion model skills diagnosis system. In
J. P. Leighton M. J. Gierl (Eds.), Cognitive diagnostic assessment: The-
ories and applications (pp. 281–292). Cambridge: Cambridge University
Press.
Roussos, L. A., Templin, J. L., Henson, R. A. (2007b). Skills diagnosis using
IRT-based latent class models. Journal of Educational Measurement,
44(4), 293–311.

References
631
Rubin, D. B. (1976). Inference and missing data. Biometrika, 63(3), 581–592.
Rubin, D. B.
(1977).
Formalizing subjective notions about the eﬀect of
nonrespondents in sample surveys. Journal of the American Statistical
Association, 72, 538–543.
Rubin, D. B. (1984). Bayesian justiﬁable and relevant frequency calculations
for the applied statistician. Annals of Statistics, 12, 1151–1172.
Rubin, D. B. (1987). Multiple imputation for nonresponse in surveys. New
York: Wiley.
Rupp, A. A., Templin, J. L., Henson, R. A. (2010). Diagnostic measurement:
Theory, methods, and applications. New York: Guilford.
Russell, M. K. (2011). Accessible test design. In M. K. Russell M. Kavanaugh
(Eds.), Assessing students in the margin: Challenges, strategies, and
techniques (pp. 407–423). Charlotte: Information Age.
Sabourin, J., Mott, B., Lester, J. (2013). Utilizing dynamic Bayes nets to
improve early prediction models of self-regulated learning. In S. Car-
berry, S. Weibelzahl, A. Micarelli, G. Semeraro (Eds.), User modeling,
adaptation, and personalization (pp. 228–241). New York: Springer.
Samejima, F. (1969). Estimation of latent ability using a response pattern of
graded scores. Psychometrika Monograph No. 17, 34(4), (Part 2).
Sao Pedro, M. A., Baker, R. S. J. d., Gobert, J. D., Montalvo, O., Nakama,
A. (2013). Leveraging machine-learned detectors of systematic inquiry
behavior to estimate and predict transfer of inquiry skill. User Modeling
and User-Adapted Interaction, 23(1), 1–39.
Savage, L. J. (1971). Elicitation of personal probabilities and expectations.
Journal of the American Statistical Association, 66(336), 783–801.
Savage, L. J. (1972). The foundations of statistics (2nd ed.). New York:
Dover.
Scalise, K.,
Giﬀord, B. (2006). Computer-based assessment in e-learning:
A framework for constructing “intermediate constraint” questions and
tasks for technology platforms. The Journal of Technology, Learning
and Assessment, 4(6).
Schum, D. A. (1994). The evidential foundations of probabilistic reasoning.
New York: Wiley.
Schwarz, G. (1978). Estimating the dimension of a model. Annals of Statistics,
6, 461–464.
Shachter, R. D. (1986). Evaluating inﬂuence diagrams. Operations Research,
34, 871–82.
Shafer, G. (1976). A mathematical theory of evidence. Princeton: Princeton
University Press.
Shafer, G. (1996). The art of causal conjecture. Cambridge: MIT Press.
Shaftel, J., Yang, X., Glasnapp, D., Poggio, J. (2005). Improving assessment
validity for students with disabilities in large-scale assessment programs.
Educational Assessment, 10(4), 357–375.
Shenoy, P. P. (1991). A fusion algorithm for solving Bayesian decision prob-
lems. In B. D’Ambrosio, P. Smets, P. Bonissone (Eds.), Uncertainty in

632
References
artiﬁcial intelligence: Proceedings of the 7th conference (pp. 361–369).
San Mateo, CA.
Shenoy, P. P., Shafer, G. (1990). Axioms for probability and belief-function
propagation. In R. D. Shachter, T. Levitt, L. N. Lemmer J. F.and Kanal
(Eds.), Uncertainty in Artiﬁcial Intelligence: Proceedings of the 4th Con-
ference (pp. 169–198). Amsterdam: North-Holland.
Shortliﬀe, E., Buchanan, B. (1975). A model of inexact reasoning in medicine.
Mathematical Biosciences, 23, 351–379.
Shute, V. J. (2003, April). Under the hood of adaptive e-learning: Diagnos-
tic assessment, student modeling, and selection rules. Paper presented
at annual meeting of the American Educational Research Association,
Chicago, IL.
Shute, V. J. (2004). Towards automating ECD-based diagnostic assessments.
Technology, Instruction, Cognition, and Learning, 2(1–2), 1–18.
Shute, V. J.
(2006, April).
Assessments for learning: Great idea, but do
they work?
Paper presented at the annual meeting of the American
Educational Research Association, San Francisco, CA.
Shute, V. J.,
Torres, R. (2012). Where streams converge: Using evidence-
centered design to assess quest to learn. In M. C. Mayrath, J. Clarke-
Midura, D. Robinson (Eds.), Technology-based assessments for 21st cen-
tury skills: Theoretical and practical implications from modern research
(pp. 91–124). Charlotte: Information Age.
Shute, V. J., Graf, E. A., Hansen, E. G. (2005). Designing adaptive, diagnostic
math assessments for individuals with and without visual disabilities. In
L. M. Pytlikzillig, R. H. Bruning, M. Bodvarsson (Eds.), Technology-
based education: Bringing researchers and practitioners together (pp.
169–202). Charlotte: Information Age.
Shute, V. J., Hansen, E. G., Almond, R. G. (2007). An assessment for learning
system called ACED: The impact of feedback and adaptivity on learn-
ing. (Research Report No. RR-07-26). Princeton: Educational Testing
Service. Retrieved from http://www.ets.org/research/researcher/
RR-07-26.html.
Shute, V. J., Hansen, E. G.,
Almond, R. G.
(2008). You can’t fatten a
hog by weighing it - or can you? Evaluating an assessment for learning
system called ACED. International Journal of Artiﬁcial Intelligence in
Education, 18(4), 289–316. Retrieved from http://www.ijaied.org/
iaied/ijaied/abstract/Vol 18/Shute08.html.
Shute, V. J., Ventura, M., Bauer, M. I., Zapata-Rivera, J.-D. (2009). Melding
the power of serious games and embedded assessment to monitor and
foster learning: Flow and grow. In U. Ritterfeld, M. J. Cody, P. Vorderer
(Eds.), Serious games: Mechanisms and eﬀects (pp. 295–321).
New
York: Routledge.
Shute, V. J., Ventura, M.,
Kim, Y. J.
(2013).
Assessment and learning
of informal physics in Newton’s playground.
Journal of Educational
Research, 106(6), 423–430. doi: 10.1080/00220671.2013.832970.

References
633
Singley, M. K., Fairweather, P. G., Swerling, S. (1999). Team tutoring sys-
tems: Reifying roles in problem solving. In C. M. Hoadley J. Roschelle
(Eds.), Proceedings of the 1999 conference on computer support for col-
laborative learning (pp. 538–548). Mahwah: Lawrence Erlbaum.
Sinharay, S. (2003). Assessing convergence of the Markov chain Monte Carlo
algorithms: A review (Research Report No. RR-03-07). Princeton: Edu-
cational Testing Service.
Sinharay, S.
(2005). Assessing ﬁt of unidimensional item response theory
models using a Bayesian approach. Journal of Educational Measure-
ment, 42(4), 375–394.
Sinharay, S. (2006). Model diagnostics for Bayesian networks. Journal of
Educational and Behavioral Statistics, 31(1), 1–33.
Sinharay, S., Almond, R. G. (2007). Assessing ﬁt of cognitively diagnostic
models—A case study.
Educational and Psychological Measurement,
67(2), 239–257.
Sinharay, S., Almond, R. G., Yan, D. (2004). Assessing ﬁt of models with dis-
crete proﬁciency variables in educational assessment (Research Report
No. RR-04-07). Princeton: Educational Testing Service. Retrieved from
http://www.ets.org/research/researcher/RR-04-07.html.
Smith, J. K. (2003). Reconsidering reliability in classroom assessment and
grading. Educational Measurement: Issues and Practice, 22(4), 26–33.
Spiegelhalter, D. J., Knill-Jones, R. (1984). Statistical and knowledge-based
approaches to clinical decision support systems, with an application in
gastroenterology. Journal of the Royal Statistical Society (Series A),
147, 35–77.
Spiegelhalter, D. J.,
Lauritzen, S. L. (1990). Sequential updating of con-
ditional probabilities on directed graphical structures. Networks, 20,
579–605.
Spiegelhalter, D. J., Dawid, A. P., Lauritzen, S. L.,
Cowell, R. G. (1993).
Bayesian analysis in expert systems. Statistical Science, 8, 219–283.
Spiegelhalter, D. J., Thomas, A., Best, N. G., Gilks, W. R. (1995). BUGS:
Bayesian inference using Gibbs sampling, version 0.50 [Computer soft-
ware manual].
Cambridge: MRC Biostatistics Unit.
Retrieved from
http://www.mrc-bsu.cam.ac.uk/bugs/.
Spiegelhalter, D. J., Best, N. G., Carlin, B. P.,
van der Linde, A. (2002).
Bayesian measures of model complexity and ﬁt (with discussion). Jour-
nal of the Royal Statistical Society (Series B), 64, 583–639.
Spiegelhalter, D. J., Thomas, A., Best, N. G.,
Gilks, W. R.
(n.d.).
Bugs
0.5
examples
volume
1
(version
i)
[Computer
software
manual].
Retrieved from http://www.mrc-bsu.cam.ac.uk/bugs/
documentation/contents.shtml.
Spirtes, P., Meek, C., Richardson, T. S. (1997). A polynomial-time algorithm
for determining DAG equivalence in the presence of latent variables and
selection bias. In D. Madigan P. Smythe (Eds.), Preliminary papers of
the sixth international workshop on AI and statistics (pp. 489–501).

634
References
Srinivas, S. (1993). A generalization of the noisy-or model, the generalized
noisy or-gate. In D. Heckerman
A. Mamdani (Eds.), Uncertainty in
artiﬁcial intelligence: Proceedings of the 9th conference (pp. 208–215).
San Mateo: Morgan Kaufmann.
Steinberg, L. S., Gitomer, D. H. (1996). Intelligent tutoring and assessment
built on an understanding of a technical problem-solving task. Instruc-
tional Science, 24, 223–258.
Steinberg, L. S., Almond, R. G., Baird, A. B., Cahallan, C., Chernick, H.,
DiBello, L. V., et al. (2003). Introduction to the Biomass project: An
illustration of evidence-centered assessment design and delivery capabil-
ity (CSE Report No. 609). Los Angeles: National Center for Research
on Evaluation, Standards, and Student Testing (CRESST). Retrieved
from http://www.cse.ucla.edu/reports/R609.pdf.
Stevens, R. H., Thadani, V. (2007). Quantifying student’s scientiﬁc problem
solving eﬃciency and eﬀectiveness. Technology, Instruction, Cognition,
and Learning, 5(4), 325–338.
Stewart, J., Hafner, R. (1994). Research on problem solving: Genetics. In
D. Gabel (Ed.), Handbook of research on science teaching and learning
(pp. 284–300). New York: Macmillan.
Suermondt, H. (1992). Explanation in Bayesian belief networks. Unpublished
doctoral dissertation, Departments of Computer Science and Medicine,
Stanford University.
Suppes, P. (1969). Stimulus response theory of ﬁnite automata. Journal of
Mathematical Psychology, 6, 327–355.
Swender, E., Conrad, D.,
Vicars, R.
(2012). ACTFL Proﬁciency Guide-
lines 2012. Alexandria: American Council on the Teaching of Foreign
Languages.
Takikawa, M., D’Ambrosio, B., Wright, E. (2002). Real-time inference with
large-scale temporal Bayes nets. In J. Breese D. Koller (Eds.), Uncer-
tainty in artiﬁcial intelligence: Proceedings of the 18th conference. San
Mateo: Morgan Kaufmann.
Tanimoto, S. (2001). Distributed transcripts for online learning: Design issues.
Journal of Interactive Media in Education, 2001(2).
Retrieved from
http://www-jime.open.ac.uk/2001/2/.
Tatsuoka, K. K. (1983). Rule space: An approach for dealing with miscon-
ceptions based on item response theory. Journal of Educational Mea-
surement, 20, 345–354.
Tatsuoka, K. K. (1984). Analysis of errors in fraction addition and subtraction
problems (Vol. 20; NIE Final report No. NIE-G-81-002). Champaign:
University of Illinois at Urbana-Champaign, Computer-Based Education
Research.
Tatsuoka, K. K.
(1990).
Toward an integration of item response theory
and cognitive error diagnosis. In N. Frederiksen, R. Glaser, A. Lesgold,
M. G. Shafto (Eds.), Diagnostic monitoring of skill and knowledge acqui-
sition (pp. 453–488). Mahwah: Lawrence Erlbaum.

References
635
Tatsuoka, K. K. (1995). Architecture of knowledge structures and cognitive
diagnosis: A statistical pattern recognition approach. In P. D. Nichols,
S. F. Chipman, R. L. Brennan (Eds.), Cognitively diagnostic assessment
(pp. 327–359). Mahwah: Lawrence Erlbaum.
Tatsuoka, K. K. (2009). Cognitive assessment: An introduction to the rule
space method. New York: CRC.
Tatsuoka, M. M., Tatsuoka, K. K. (1989). Rule space. In S. Kotz N. L. John-
son (Eds.), Encyclopedia of statistical sciences (pp. 217–220). New York:
Wiley.
Tatsuoka, K. K., Linn, R. L., Tatsuoka, M. M., Yamamoto, K. (1988). Dif-
ferential item functioning resulting from the use of diﬀerent solution
strategies. Journal of Educational Measurement, 25(4), 301–319.
Thissen, D., Wainer, H. (2001). Test scoring. Mahwah: Lawrence Erlbaum.
Thomas, A., Spiegelhalter, D. J., Gilks, W. R. (1992). BUGS: A program to
perform Bayesian inference using Gibbs sampling. In J. M. Bernardo,
J. O. Berger, A. P. Dawid, A. F. M. Smith (Eds.), Bayesian statistics
4 (pp. 837–842). Gloucestershire: Clarendon.
Toulmin, S. E. (1958). The uses of argument. Cambridge: Cambridge Uni-
versity Press.
van der Gaag, L. C., Bodlaender, H. L., Feelders, A. (2004). Monotonicity in
Bayesian networks. In M. Chickering J. Halpern (Eds.), Uncertainty in
artiﬁcial intelligence: Proceedings of the 20th conference (pp. 569–576).
Arlington: AUAI Press.
van der Linden, W. J. (2005). Linear models for optimal test design. New
York: Springer.
van der Linden, W. J., Glas, C. A. W. (2010). Elements of adaptive testing.
New York: Springer.
VanLehn, K.
(2008).
Intelligent tutoring systems for continuous, embed-
ded assessment. In C. Dwyer (Ed.), The future of assessment: Shaping
teaching and learning (pp. 113–138). Mahwah: Lawrence Erlbaum.
VanLehn, K., Martin, J. (1997). Evaluation of an assessment system based
on Bayesian student modeling. International Journal of Artiﬁcial Intel-
ligence in Education, 8, 179–221.
Veldkamp, B. P., Verschoor, A. J., Eggen, T. J. (2010). A multiple objective
test assembly approach for exposure control problems in computerized
adaptive testing. Psicol´ogica, 31(2), 335–355.
Vendlinski, T. P., Baker, E. L.,
Niemi, D. (2008). Templates and objects
in authoring problem-solving assessments. In E. L. Baker, J. Dickieson,
W. Wulfeck, H. F. O’Neil (Eds.), User modeling, adaptation, and per-
sonalization (pp. 309–333). Mahwah: Lawrence Erlbaum.
Vomlel, J. (2003). Two applications of Bayesian networks. In Proceedings of
the conference Znalosti 2003, Ostrava, Czech Republic (pp. 73–82).
Vomlel, J. (2004). Bayesian networks in educational testing. International
Journal of Uncertainty Fuzziness and Knowledge Based Systems, 12,
83–100.

636
References
von Davier, M. (2008). A general diagnostic model applied to language testing
data. British Journal of Mathematical and Statistical Psychology, 61,
287–307.
von Davier, M.,
Haberman, S. J.
(2014).
Hierarchical diagnostic clas-
siﬁcation models morphing into unidimensional ‘diagnostic’ classiﬁca-
tion models—A commentary.
Psychometrika, 79(2), 340–346.
doi:
10.1007/s11336-013-9363-z.
Vygotsky, L.
(1978).
Mind in society: The development of higher mental
processes. Cambridge: Harvard University Press.
Wainer, H.,
Kiely, G. L. (1987). Item clusters and computerized adaptive
testing: A case for testlets. Journal of Educational Measurement, 24,
185–201.
Wainer, H., Bradlow, E. T., Wang, X. (2007). Testlet response theory and
its applications. Cambridge: Cambridge University Press.
Wainer, H., Dorans, N. J., Flaugher, R., Green, B. F., Mislevy, R. J., Stein-
berg, L., Thissen, D. (2000). Computerized adaptive testing: A primer
(2nd ed.). Mahwah: Lawrence Erlbaum.
Walley, P. (1991). Statistical reasoning with imprecise probabilities. London:
Chapman and Hall.
Wang, C., Chang, H.-H.,
Huebner, A. (2011). Restrictive stochastic item
selection methods in cognitive diagnostic computerized adaptive testing.
Journal of Educational Measurement, 48(3), 255–273.
Weaver, W.
(1948).
Probability, rarity, interest, and surprise.
Scientiﬁc
Monthly, 67, 390–392.
Weiss, Y. (2000). Correctness of local probability propagation in graphical
models with loops. Neural Computation, 12, 1–41.
West, P., Rutstein, D. W., Mislevy, R. J., Liu, J.,
Levy, R.
(2012).
A
Bayes net approach to modeling learning progressions. In A. C. Alonzo
A. W. Gotwals (Eds.), Learning progressions in science: Current chal-
lenges and future directions (pp. 255–291). Rotterdam: Sense.
White, B. Y., Frederiksen, J. R. (1998). Inquiry, modeling, and metacogni-
tion: Making science accessible to all students. Cognition and Instruc-
tion, 16, 3–118.
Whittaker, J. (1990). Graphical models in applied multivariate statistics. New
York: Wiley.
Wiggins, G. P. (1998). Educative assessment: Designing assessments to inform
and improve student performance. San Francisco: Jossey-Bass.
Williamson, D. M. (2000). Utility of model criticism indices for Bayesian
inference networks in cognitive assessment. Unpublished doctoral dis-
sertation, Fordham University.
Williamson, D. M., Mislevy, R. J., Almond, R. G. (2000). Model criticism
of Bayesian networks with latent variables. In C. Boutilier
M. Gold-
szmidt (Eds.), Uncertainty in artiﬁcial intelligence: Proceedings of the
16th conference (pp. 634–643). San Mateo: Morgan Kaufmann.

References
637
Williamson, D. M., Bauer, M. I., Steinberg, L. S., Mislevy, R. J., DeMark,
S. F. (2004a). Design rationale for a complex performance assessment.
International Journal of Testing, 4, 303–332.
Williamson, D. M., Mislevy, R. J., Almond, R. G. (2004b). Evidence-centered
design for certiﬁcation and licensure. CLEAR Exam Review, 14, 14–18.
Williamson, D. M., Almond, R. G., Mislevy, R. J.,
Levy, R. (2006a). An
application of Bayesian networks in automated scoring of computerized
simulation tasks. In D. M. Williamson, R. J. Mislevy, I. I. Bejar (Eds.),
Automated scoring of complex tasks in computer-based testing (pp. 201–
257). Hillsdale: Lawrence Erlbaum.
Williamson, D. M., Mislevy, R. J., Bejar, I. I. (Eds.). (2006b). Automated
scoring of complex tasks in computer-based testing. Hillsdale: Lawrence
Erlbaum.
Wilson, M. R. (2004). Constructing measures: An item response modeling
approach. New York: Routledge.
Wilson, M. R. (2009). Measuring progressions: Assessment structures under-
lying a learning progression. Journal of Research in Science Teaching,
46(6), 716–730.
Wilson, M. R., Adams, R. J. (1995). Rasch models for item bundles. Psy-
chometrika, 60(2), 181–198.
Wright, S.
(1921).
Correlation and causation.
Journal of Agricultural
Research, 20, 557–585.
Wright, S. (1934). The method of path coeﬃcients. Annals of Mathematical
Statistics, 5, 161–215.
Yamamoto, K. (1987). A model that combines IRT and latent class models.
Unpublished doctoral dissertation, University of Illinois, Champaign-
Urbana.
Yan, D., Mislevy, R. J.,
Almond, R. G.
(2003).
Design and analysis in
a cognitive assessment (Research Report No. RR-03-32).
Princeton:
Educational Testing Service.
Retrieved from http://www.ets.org/
research/researcher/RR-03-32.html.
Yan, D., Almond, R. G.,
Mislevy, R. J. (2004). Comparison of two mod-
els for cognitive diagnosis (Research Report No. RR-04-02).
Prince-
ton: Educational Testing Service. Retrieved from http://www.ets.org/
research/researcher/RR-04-02.html.
Yan, D., von Davier, A. A., Lewis, C. (Eds.). (2014). Computerized multistage
testing: Theory and applications. New York: CRC.
Yen, W. M. (1993). Scaling performance assessments: Strategies for managing
local item dependence. Journal of Educational Measurement, 30, 187–
213.
York, J.
(1992).
Use of the Gibbs sampler in expert systems.
Artiﬁcial
Intelligence, 56, 115–130.
Zadeh, L. (1965). Fuzzy sets. Information and Control, 8, 338–353.
Zalles, D., Haertel, G. D.,
Mislevy, R. J. (2010). Using evidence-centered
design to support assessment, design and validation of learning progres-

638
References
sions (Large-Scale Assessment Technical Report No. 10). Menlo Park:
SRI International. Retrieved from http://ecd.sri.com/downloads/
ECD TR10 Learning Progressions.pdf.
Zapata-Rivera, J.-D.
(2002).
cbCPT: Knowledge engineering support for
CPTs in Bayesian networks. In Proceedings of the 15th Canadian con-
ference on artiﬁcial intelligence AI 2002 (pp. 368–370).
Zapata-Rivera, J.-D.,
Greer, J. E. (2004a). Inspectable Bayesian student
modelling servers in multi-agent tutoring systems. International Journal
of Human-Computer Studies, 61(4), 535–563. doi: 10.1016/j.ijhcs.2003
.12.017.
Zapata-Rivera, J.-D.,
Greer, J. E.
(2004b).
Interacting with inspectable
Bayesian student models. International Journal of Artiﬁcial Intelligence
in Education, 14(2), 127–163.
Zapata-Rivera, J.-D., Neufeld, E.,
Greer, J. E.
(1999).
Visualization of
Bayesian belief networks. In IEEE Visualization 1999: Late Breaking
Hot Topics Proceedings (pp. 85–88). New Brunswick: IEEE Press.
Zapata-Rivera, J.-D., VanWinkle, W. H.,
Zwick, R. J.
(2012).
Applying
score design principles in the design of score reports for CBAL teach-
ers (Research Memorandum No. RM-12-20). Princeton: Educational
Testing Service.
Zimowski, M. F., Muraki, E., Mislevy, R. J., Bock, R. D. (2003). BILOG-
MG: Multiple-group IRT analysis and test maintenance for binary items
[Computer software manual]. Chicago: Scientiﬁc Software International.

Author Index
Adams, D., 547, 607
Adams, R. J., 146, 451, 607, 637
Aitkin, M., 300, 330, 436, 611
Akaike, H., 352, 607
Alarcon, R., 613
Albert, D., 623
Aleven, V., 583, 607, 622
Allen, N. L., 429, 610
Almond, R. G., 3, 17, 21, 24, 32, 34,
37, 50, 55, 75, 77, 83–87, 89–
91, 93, 95, 96, 98, 101, 102,
129, 131, 139, 141–143, 197,
200, 201, 213, 215, 216, 218,
249, 254, 255, 262, 264, 267,
268, 307, 316, 340, 346–348,
373, 379, 385, 399, 403, 407,
416, 418, 426, 431, 435, 439,
440, 454, 458, 459, 470, 478,
491, 494, 496, 498, 519, 545,
546, 555, 587, 591, 592, 594,
595, 602–604, 607–609, 612,
621, 624, 626, 627, 632–634,
636, 637
Alonzo, A. C., 429, 516, 609
American Association for the Advance-
ment of Science, 609
Andersen, S. A., 360, 609
Andersen, S. K., 609
Anderson, J. D., 620
Anderson, J. R., 583, 614, 630
Anderson, R. D., 100, 609
Andreassen, S., 123, 609
Andrich, D., 262, 609
Attali, Y., 447, 448, 454, 609
Bacchetti, P., 50, 609
Bachman, L. F., 420, 609
Baird, A. B., 634
Baker, E. L., 635
Baker, R. S. J. d., 583, 609, 617,
631
Baldwin, D., 447, 609
Barr, A., 218, 458, 610
Bart, W. M., 531, 610
Barton, P. E., 363, 610
Bauer, M. I., 610, 628, 632, 636
Beaton, A. E., 429, 610
Behr, M. J., 610
Behrens, J. T., 11, 16, 416, 537,
541, 610, 615, 627
Bejar, I. I., 21, 447, 597, 610, 611,
637
Bennett, R. E., 595, 610, 627
Berger, J. O., 44, 63, 74, 215, 226,
247, 610
Berliner, M., 16, 610
Bertel`e, U., 101, 610
Best, N. G., 307, 320, 610, 623, 630,
633
Birenbaum, M., 622
Bishop, Y. M., 339, 350, 486, 611
Black, P., 611
Bock, R. D., 255, 300, 330, 436,
501, 611, 638
Bodlaender, H. L., 635
Bollen, K. A., 99, 611
Boutilier, C., 590, 611, 620
Bowman, E. H., 447, 611
Box, G. E. P., 16, 63, 74, 333, 611
Boyen, X., 589, 611
Bradlow, E. T., 157, 170, 611, 636
Brand, M., 591, 629
Braun, H. I., 447, 610, 611
Breese, J. S., 136, 586, 611
Breiman, L., 214, 612
Breland, H. M., 454, 612
Brennan, R. L., 225, 229, 394, 398,
612, 622, 629
Breyer, F. J., 11, 539, 612, 626, 627
c⃝Springer Science+Business Media New York 2015
639
R. G. Almond et al., Bayesian Networks in Educational Assessment,
Statistics for Social and Behavioral Sciences, DOI 10.1007/978-1-4939-2125-6

640
AUTHOR INDEX
Bridgeman, B., 460, 612
Brioschi, F., 101, 610
Brooks, S., 311, 312, 386, 387, 612
Buchanan, B., 334, 550, 632
Buck, C. E., 629
Bunt, A., 583, 612
Buntine, W. L., 279, 280, 356, 612
Burstein, J., 447, 448, 454, 609, 612
Cahallan, C., 634
Cai, L., 300, 612
Camp, R., 612
Cannings, C., 151, 612
Card, W., 213, 618
Carlin, B. P., 633
Carlin, J. B., 616, 617
Castine, M., 616
Cen, H., 612
Chaloner, K. M., 263, 613
Chambers, J. L., 325, 613
Chang, D., 608
Chang, H.-H., 636
Chapelle, C., 419, 613
Cheng, B. H., 514, 613, 628
Cheng, Y., 217, 613
Chernick, H., 634
Chickering, D., 358, 613, 619
Chipman, S. F., 629
Chon, K. H., 618
Chudowsky, N., 629
Chung, G. K. W. K., 596, 613
Close, C. N., 373, 613
Cobb, B. R., 101, 613
Collis, J. M., 442, 444, 478, 613
Conati, C., 583, 584, 591, 612, 613,
617
Conejo, R., 592, 613
Conrad, D., 634
Cooper, G. F., 357, 614
Cooper, P. L., 621
Corbett, A. T., 583, 609, 614, 630
Corcoran, T., 516, 614
Corrigan, S., 628
Cowell, R. G., 17, 105, 111, 149,
334, 356, 614, 633
Cowles, M. K., 610, 630
Cox, D. R., 101, 614
Cronbach, L. J., 21, 614
Crowley, R., 583, 614
D’Ambrosio, B., 151, 254, 623, 634
Daneshkhah, A., 629
Daniel, B., 200, 614
Dann, P. L., 613, 620
Darroch, J. N., 100, 614
Davenport, E., 613
Davidson, F., 420, 616
Davison, M. L., 613
Dawid, A. P., 53, 111, 614, 633
Dayton, C. M., 292, 614
de Freitas, N., 615
de la Torre, J., 357, 373, 614
de Vries, L. F., 613
Dean, T., 589, 611, 614
Deane, P., 38, 425, 447, 449, 531,
614, 615
DeBarger, A., 628
De Boeck, P., 630
De Finetti, B., 44, 614
DeGroot, M. H., 63, 215, 226, 615
Delacruz, G. C., 613
DeMark, S. F., 627, 636
Deming, W. E., 301, 615
Dempster, A. P., 46, 75, 267, 280,
288, 324, 435, 550, 615
DeVito, P. J., 597, 615
DiBello, L. V., 271, 608, 626, 630,
634
DiCerbo, K. E., 541, 610, 615
DiCerbo, K. E., 627
Dickey, J. M., 621
D´ıez, F. J., 172, 241, 250, 615
Dorans, N. J., 636
Doucet, A., 615
Douglas, J. A., 213, 216, 373, 614,
620
Draney, K., 622
Draper, D., 359, 615
Druzdzel, M. J., 198, 201, 241, 615,
619

AUTHOR INDEX
641
Duncan, G. T., 263, 613
Dunson, D. B., 617
Dwyer, C. A., 479, 615
Edwards, D., 100, 616
Efron, B., 337, 616
Eggen, T. J., 635
Eiser, J. R., 629
El Saadawi, G. M., 583, 616
Embretson, S. E., 12, 21, 444, 489,
577, 596, 616
Enright, M., 613
Epstein, E. S., 336, 616
Fairweather, P. G., 632
Falck, B., 609
Falc´on, J. C. S., 404, 617
Feelders, A., 635
Feigenbaum, E., 218, 458, 610
Feller, W., 54, 616
Fienberg, S. E., 301, 611, 616
Fine, J., 616
Fischer, G. H., 145, 444, 489, 616
Flaugher, R., 636
Fleiss, J. L., 228, 486, 616
Formann, A. K., 255, 616
Fowles, M., 609
Fred, A. L. N., 583, 616
Frederiksen, J. R., 514, 636
Freedman, D., 371, 616
Frezzo, D. C., 627
Friedman, J. H., 612
Fujii, R., 613
Fulcher, G., 420, 616
Gamboa, H., 583, 616
Garthwaite, P. H., 629
Gavrin, J., 359, 624
Gee, J. P., 628
Geerlings, H., 444, 479, 588, 616
Gelman, A., 20, 41, 63, 71, 308,
311, 312, 315, 339, 340, 353,
356, 386, 387, 612, 616, 617
Geman, D., 308, 309, 325, 617
Geman, S., 308, 309, 325, 617
Gertner, A., 583, 617
Gieger, D., 619
Gierl, M. J., 8, 10, 145, 374, 379,
440, 442, 445, 459, 478, 550,
617, 623
Giﬀord, B., 448, 631
Gilks, W. R., 18, 280, 308, 325,
617, 633, 635
Gilula, Z., 333, 352, 355, 617
Girard, B. J., 628
Gitomer, D. H., 10, 11, 445, 479,
524, 598, 617, 625, 633
Glas, C. A. W., 216, 273, 286, 404,
473, 616, 617, 635
Glaser, R., 629
Glasnapp, D., 631
Glasziou, P., 213, 214, 617
Gobert, J. D., 447, 617, 631
Goldman, R. P., 611
Good, I., 45, 62, 201, 213, 336, 617,
618
Goodman, L. A., 229, 230, 618
Gordon, N., 615
Gormley, R., 616
Gotwals, A. W., 429, 516, 609
Gowda, S. M., 609
Graesser, A. C., 583, 618
Graf, E. A., 497, 531, 618, 632
Green, B. F., 636
Greeno, J. G., 590, 630
Greer, J. E., 584, 638
Guttman, I., 337, 618
Guzm´an, E., 613
Haberman, S. J., 159, 301, 333,
352, 355, 375, 404, 423, 617,
618, 636
Haertel, E. H., 145, 618, 628
Haertel, G. D., 514, 626–628, 637
Hafner, R., 514, 634
Haladyna, T. M., 440, 442, 445,
478, 617
Hambleton, R. K., 158, 428, 619
Haniford, L. C., 628
Hanks, S., 611

642
AUTHOR INDEX
Hansen, E. G., 33, 441, 461, 619,
632
Hao, J., 628
Harring, J. R., 627
Harter, D., 618
Hartz, S. M., 271, 619, 630
Hastings, W., 314, 619
Heckerman, D., 151, 210, 214, 356,
359, 619
Heidelberger, P., 320, 619
Hemat, L. A., 608
Henrion, M., 44, 74, 198, 201, 619,
628
Henson, R. A., 213, 216, 373, 620,
630, 631
Henze, N., 620
Herskovits, E., 357, 608, 614
Hilden, J., 151, 213, 214, 617, 620
Hively, W., 440, 620
Hodges, J. S., 615
Hoeting, J., 624
Hoey, J., 591, 620
Hoﬀman, E., 628
Holland, P. W., 287, 349, 351, 364,
488, 611, 620
Horvitz, E., 619
Howard, R. A., 97, 620
Hrycej, T., 316, 620
Hu, A., 620
Huebner, A., 636
Huﬀ, K., 416, 620
Hunka, S. M., 617, 623
Hunt, E., 624
IMS Global Learning Consortium,
620
Irvine, S. H., 442, 445, 490, 613,
620
Iseli, M. R., 598, 620
Jaakkola, T. S., 152, 621
Jackenthal, A., 612
Jamieson, J., 613
Jaynes, E. T., 75, 621
Jeﬀreys, H., 75, 621
Jenkins, F., 608, 626
Jenkinson, D. J., 629
Jensen, F. V., 17, 105, 125, 129,
149, 621
Jewell, N. P., 609
Jitnah, N., 208, 629
Johnson, L., 626, 627
Johnson, M. S., 273, 588, 621
Jones, R. J., 612
Jordan, M. I., 152, 356, 621, 628
Jordan, P. W., 618
Joreskog, K. G., 99, 621
Jukic, D., 616
Junker, B. W., 9, 250–252, 298,
313, 612, 621, 629
Kadane, J. B., 263, 621
Kahneman, D., 51, 74, 621
Kanazawa, K., 589, 614
Kane, M. T., 21, 424, 459, 464, 596,
621
Kaplan, D., 99, 621
Katz, I. R., 11, 598, 621
Kauﬀman, L. R., 609
Kennedy, C. A., 429, 462, 622
Kiely, G. L., 137, 454, 636
Kim, J. H., 111, 116, 622
Kim, Y. J., 609, 632
Kindﬁeld, A. C. H., 634
Kirsch, I., 621
Klein, M. F., 178, 347, 349, 374,
378, 407, 422, 622
Knill-Jones, R., 202, 633
Koedinger, K. R., 583, 607, 612,
622, 630
Koenig, A. D., 620
Koenig, J. A., 597, 615
Kohavi, R., 357, 622
Kolen, M. J., 394, 398, 622
Koller, D., 137, 152, 587, 589, 611,
622
Krishnan, T., 300, 624
Kruskal, W. H., 229, 230, 618
Kump, B., 623
Kyllonen, P., 445, 620

AUTHOR INDEX
643
Laird, N., 615
Lange, K., 300, 622
Laskey, K. B., 137, 241, 586, 587,
622, 624
Lauritzen, S. L., 17, 99, 101, 123,
149, 158, 244, 249, 297, 298,
429, 614, 622, 633
Leamer, E. E., 615
Learner, U., 152, 589, 622
Lee, J. J., 620
Lee, P. M., 63, 623
Legowski, E., 616
Leighton, J. P., 8, 10, 145, 374, 379,
550, 617, 623
Lennon, M. L., 612
Lesh, R., 610
Lester, J., 631
Leuven, K., 630
Levidow, B., 624
Levin, B., 616
Levy, R., 340, 404, 590, 610, 623,
627, 636, 637
Lewis, C., 637
Ley, T., 583, 623
Li, Z., 151, 254, 623
Lieberman, M., 300, 611
Linn, R. L., 635
Little, R., 297, 623
Liu, J., 357, 623, 636
Liu, J. S., 152, 359, 623
Livingston, S., 609
Lord, F. M., 344, 458, 500, 623
Louis, T. A., 300, 623
Luecht, R. M., 12, 415, 592, 623
Lukas, J. F., 627
Lunn, D. J., 71, 307, 325, 384, 385,
566, 604, 623
Lynch, S. M., 18, 624
MacLaren, B. A., 609
Maclaren, H., 584, 591, 613
Madigan, D., 198, 202, 203, 205,
208, 213, 215, 216, 218, 359,
458, 496, 498, 609, 624
Madnani, N., 612
Mahoney, S. M., 137, 241, 586, 587,
622, 624
Maris, E., 400, 624
Martin, J., 584, 598, 624, 635
Matheson, J. E., 97, 98, 210, 620,
624
Matsuda, N., 591, 624
Matts, T., 620
Matukhin, A., 608
Mayo, M., 591, 624
McCalla, G. I., 614
McLachlan, G., 300, 624
Medvedeva, O., 583, 614
Meek, C., 633
Meijer, R. R., 404, 624
Melnick, D., 10, 625
Meng, X. L., 616
Messick, S., 20–22, 422, 424, 577,
625
Metropolis, N., 313, 625
Middleton, B., 619
Mill´an, E., 584, 613, 625
Miller, P., 218, 625
Mislevy, J., 340, 627
Mislevy, R. J., 3, 5, 11, 16, 20, 21,
24, 27, 32, 33, 35, 55, 81, 96,
103, 177, 181, 182, 198, 227,
273, 286, 297, 299, 316, 322,
326, 373, 377, 381, 383, 403,
407, 415–418, 421, 424, 426,
436, 441, 444, 445, 447, 461,
472, 478, 479, 489, 491, 500,
501, 514, 524, 546, 569, 587,
589, 592, 598, 604, 605, 608,
610–613, 617, 619, 623, 625–
628, 636–638
Mitchell, A. P., 609
Mitrovic, A., 591, 624
Montalvo, O., 617, 631
Morgan, M. G., 44, 74, 628
Morris, C. N., 615
Morris, M. M., 612
Mosher, F. A., 614
Moss, P. A., 20, 424, 628
Mostow, J., 628

644
AUTHOR INDEX
Mosurski, K., 624
Mott, B., 631
Moussouris, J., 94, 628
Mulder, J., 217, 454, 608, 628
Muraki, E., 255, 628, 638
Murphy, K. P., 122, 152, 589, 628
Murray, E., 628
Murray, R., 591, 628
Nadelman, H. L., 621
Nakama, A., 631
National Research Council, 628
Neal, R. M., 315, 628
Neapolitan, R. E., 17, 105, 356, 628
Nejdl, W., 620
Neufeld, E., 638
Newell, A., 10, 628
Neyman, J., 299, 628
NGSS Lead States, 25, 508, 629
Nichols, P. D., 629
Nicholson, A. E., 208, 629
Niemi, D., 635
Nikovski, D., 591, 629
Nocedal, J., 224, 629
Norsys, Inc., 629
Oakley, J. E., 629
O’Hagan, A., 241, 299, 629
Oliver, R. M., 629
Olshen, R., 612
Oranje, A., 628
Osburn, H., 440, 629
Page, S. H., 620
Paik, M. C., 616
Palmer, A. S., 420, 609
Patterson, H. L., 620
Patz, R. J., 298, 313, 629
Pearl, J., 4, 17, 51, 53, 87, 92–
94, 96, 99, 100, 105, 111, 115,
116, 120, 149–151, 172, 198,
200, 201, 215, 250, 254, 364,
550, 602, 622, 629
Pelligrino, J., 4, 20, 414, 593, 629
Penuel, W., 627
P´erez-de-la-Cruz, J. L., 584, 613,
625
Perlman, M. D., 609
Peters, S. C., 621
Pfeﬀer, A., 137, 587, 622
Phan, C. H., 613
Pisani, R., 616
Pitoniak, M. J., 428, 619
Plummer, M., 325, 353, 385, 566,
604, 629, 630
Poggio, J., 631
Post, T., 610
Prediger, D. J., 229, 612
Pullin, D. C., 628
Purves, R., 616
Quinlan, T., 425, 449, 615
R Development Core Team, 308,
385, 603, 630
Raftery, A. E., 359, 624
Rasch, G., 159, 630
Reckase, M. D., 429, 630
Redman, M. L., 621
Restle, F., 590, 630
Reye, J., 591, 630
Richardson, S., 617
Richardson, T. S., 633
Riconscente, M. M., 592, 626, 627
Rijmen, F., 444, 584, 630
R´ıos, A., 613
Ritter, S., 630
Robins, J. M., 339, 630
Robinson, D. H., 627
Rock, D. A., 612
Rogat, A., 614
Rogers, H. J., 619
Rose, C. P., 618
Rose, D., 628
Ross, S. M., 43, 630
Rost, J., 287, 630
Roussos, L. A., 271, 630
Rubin, D. B., 297, 311, 337, 340,
386, 395, 500, 615–617, 623,
630, 631

AUTHOR INDEX
645
Ructtinger, L., 613, 628
Rupp, A. A., 8, 145, 250, 627, 631
Russell, M. K., 33, 631
Russell, S., 152, 589, 628
Rutstein, D. W., 627, 636
Sabourin, J., 591, 631
Samejima, F., 260, 631
Sao Pedro, M. A., 584, 617, 631
Savage, L. J., 44, 348, 631
Scalise, K., 448, 631
Schum, D. A., 3, 20, 422, 631
Schwarz, G., 352, 631
Scott, E. L., 299, 628
Segal, M. R., 609
Senturk, D., 608, 626
Shachter, R. D., 97, 631
Shafer, G., 3, 90, 101, 131, 149,
364, 631
Shaftel, J., 33, 631
Sheehan, K. M., 626
Shenoy, P. P., 90, 101, 131, 149,
201, 613, 631
Shortliﬀe, E., 334, 550, 632
Shute, V. J., 11, 37, 218, 233, 367,
416, 456, 462, 470, 496, 498,
499, 506, 585, 588, 595, 598,
605, 608, 609, 632
Sijtsma, K., 9, 250–252, 404, 621,
624
Simon, H. A., 10, 628
Singley, M. K., 596, 632
Sinharay, S., 273, 312, 337, 340,
346–349, 373, 375, 399–403,
407, 487, 488, 577, 588, 618,
621, 623, 632, 633
Skolnick, M. H., 612
Slovic, P., 621
Smith, J. K., 232, 633
Smith, J. Q., 629
Smith, W. S., 621
Sorbom, D., 99, 621
Speed, T. P., 614
Spiegelhalter, D. J., 123, 149, 202,
244, 249, 297, 298, 325, 330,
353, 354, 550, 604, 614, 617,
622, 623, 633, 635
Spirtes, P., 360, 633
Srinivas, S., 172, 250, 633
Srivastava, M. B., 613
St-Aubin, R., 620
Standiford, S. N., 622
Steinberg, L., 636
Steinberg, L. S., 11, 21, 24, 32, 416,
418, 426, 473, 478, 507, 512,
524, 598, 608, 612, 617, 619,
620, 626, 627, 633, 634, 636
Stephan, F. F., 301, 615
Stern, H. S., 616, 617
Stevens, R. H., 448, 634
Stewart, J., 514, 634
Stone, C. J., 612
Stout, W. F., 630
Suermondt, H., 198, 634
Suppes, P., 9, 145, 634
Swaminathan, H., 619
Swender, E., 427, 634
Swerling, S., 632
Takikawa, M., 634
Tanimoto, S., 595, 634
Tannenbaum, R., 610
Tapsﬁeld, P. G. C., 613
Tatsuoka, K. K., 5, 9, 145, 146,
177–179, 182, 251, 287, 373,
374, 379, 407, 605, 622, 634,
635
Tatsuoka, M. M., 379, 635
Templin, J. L., 620, 630, 631
Tetreault, J., 612
Thadani, V., 448, 634
Thayer, D. T., 351, 488, 620
Thissen, D., 158, 635, 636
Thomas, A., 307, 325, 385, 566,
623, 633, 635
Thompson, E. A., 612
Tiao, G. C., 63, 74, 611
Torres, R., 416, 632
Toto, E., 617
Toulmin, S. E., 421, 635

646
AUTHOR INDEX
Trella, M., 613
Tseytlin, E., 616
Tutunciyan, S., 622
Tversky, A., 621
Ulam, S., 313, 625
Underwood, J. S., 608
van der Gaag, L. C., 192, 635
van der Linde, A., 633
van der Linden, W. J., 216, 217,
273, 286, 473, 616, 617, 628,
635
van der Vaart, A., 630
VanLehn, K., 584, 591, 598, 617,
618, 624, 628, 635
VanWinkle, W. H., 638
Vastag, G., 100, 609
Vehtari, A., 617
Veldkamp, B. P., 456, 635
Vendlinski, T. P., 588, 635
Ventura, M., 609, 632
Ventura, V., 630
Verschoor, A. J., 635
Vicars, R., 634
Vines, K., 610, 630
Volinsky, C., 624
Vomlel, J., 584, 635
von Davier, A., 628
von Davier, A. A., 637
von Davier, M., 255, 256, 375, 635,
636
Vorp, R., 622
Vygotsky, L., 499, 636
Wagner, A. Z., 609
Wainer, H., 137, 158, 216, 287, 316,
349, 451, 454, 455, 473, 551,
585, 611, 620, 635, 636
Wainess, R., 620
Walley, P., 44, 74, 636
Wang, C., 456, 636
Wang, W.-C., 607
Wang, X., 611, 636
Weaver, W., 335, 636
Weiss, Y., 122, 152, 628, 636
Welch, P. D., 320, 619
Wellman, M. P., 611
Wermuth, N., 101, 614
West, P., 429, 430, 517, 636
White, B. Y., 514, 636
Whittaker, J., 17, 99, 100, 267, 435,
636
Wiggins, G. P., 12, 550, 636
Wiliam, D., 611
Williamson, D. M., 21, 335, 337,
365, 366, 608, 610, 611, 621,
636, 637
Willse, J. T., 620
Wilson, M. R., 414, 429, 451, 462,
517, 550, 593, 607, 622, 637
Wingersky, M. S., 626
Winkler, R. L., 621
Woldbye, M., 609
Wright, D., 613
Wright, E., 634
Wright, S., 99, 637
Wright, S. J., 224, 629
Wu, P.-K., 326, 626
Xu, G., 623
Yamamoto, K., 283, 635, 637
Yan, D., 250–253, 256, 307, 339,
344–347, 373, 385, 399, 400,
404, 407, 458, 602, 608, 626,
633, 637
Yang, X., 631
Yen, W. M., 137, 166, 340, 552, 637
Ying, Z., 623
York, J., 245, 289, 316, 396, 637
Young, L. J., 628
Zadeh, L., 334, 550, 637
Zalles, D., 429, 517, 637
Zapata-Rivera, J.-D., 241, 584, 594,
608, 614, 632, 637, 638
Zimowski, M. F., 272, 638
Zwick, R. J., 638

Subject Index
P+, 564
P +, 481
Q-Matrix, 5, 9, 148, 181, 232, 268,
374
Q-matrix, 145, 146, 147, 179, 221,
223, 377, 378, 381, 389, 392,
457, 585
κ, 229, 237
λ, 229, 229, 231, 237
icr , 180, 309
Reportable claims, 423
work product speciﬁcations, 438
Absorb Evidence, 494
“British Museum” search, 355
evidence-centered design, 16
evidence model fragments, 374
2-section, 88, 90, 128, 129
2PL likelihood, 202
cut set, 151
a part of, 96
absorb evidence, 134
accepted, 313
accommodation, 277, 459
accuracy, 150, 226, 228, 231, 232,
480
accuracy matrix, 226, 230
ACED, 218, 218, 467, 494, 546,
588, 595
achievement gap, 361
actions, 216
activity section process, 544
Activity Selection, 197
activity selection, 220, 455
activity selection algorithm, 217
Activity Selection Process, 36
Activity selection process, 434, 468
activity selection process, 35, 36,
37, 38, 454, 455, 472, 478,
493, 540, 543, 544
acyclic, 84, 95, 122, 140
acyclic digraph, 123, 128, 138, 140
Acyclic directed graphs, 84
acyclic hypergraph, 129
adaptive, 218, 496, 544
adaptive test, 220, 493
adaptive testing, 166, 210, 468
additive model, 172
administrative process, 539, 542,
545
administrator, 471
AIC, 350, 355
allele, 513
alternative explanations, 420
analytic scoring, 445, 452
ancestor, 84
anchor set, 392
Anchor Test, 396
and-gate, 244, 250
and-gates, 173
arc reversal, 95
Assembly Model, 33, 519
assembly model, 19, 34, 39, 145,
221, 222, 224, 438, 441, 442,
453, 455, 456, 457, 459, 460,
490, 497
Assembly models, 33
assembly rules, 432
assessment, 20, 145, 146, 149, 500
Assessment for Learning, 588
Assessment Assembly, 441
assessment description, 488–490
assessment design, 150, 232, 411
assessment designer, 268
assessment length, 23
assessment mode, 217, 220
assessment program, 321, 453
assessment purpose, 515
assessment system, 136
assessments, 453
c⃝Springer Science+Business Media New York 2015
647
R. G. Almond et al., Bayesian Networks in Educational Assessment,
Statistics for Social and Behavioral Sciences, DOI 10.1007/978-1-4939-2125-6

648
SUBJECT INDEX
assistive technologies, 459
attribute hierarchy model, 372
attributes, 372
autocorrelation, 386
automated scoring, 35, 37, 445, 446,
472, 507
automated task recognition, 477,
587
automatic item generation, 440
automatic task generation, 487
auxiliary observables, 530
backing, 420
Backward Selection, 355
Bayes decision, 210, 215, 226
Bayes factors, 357
Bayes net, 160, 249, 308, 542, 583
Bayes net fragment, 138, 529
Bayes net fragments, 397, 532, 543,
549, 604
Bayes nets, 137, 148, 283, 292, 297,
586
Bayes rule, 6
Bayes theorem, 31, 62, 63, 69, 106,
107, 289, 290, 377, 383
Bayes’ rule., 493
Bayes’ Theorem, 49
Bayesian, 4, 6, 377, 428
Bayesian computation, 324
Bayesian inference, 289, 294, 305,
451
Bayesian net, 28
Bayesian network, 11, 37, 99, 134,
145, 158, 180, 188, 225, 248,
296, 372, 390, 489, 590
Bayesian network fragment, 448
Bayesian networks, 3, 4, 6, 14, 82,
148, 149, 166, 170, 172, 473,
583, 595
Bayesian networks, 81
Bayesian probability model for edu-
cational measurement, 284
Bayesian statistics, 62
Bayesian updating, 293
belief revision, 200
belief updating, 116, 143, 144, 200
belief-updating algorithm, 106, 115,
116, 120, 122, 131, 132, 134,
143, 144, 147, 149
Bernoulli distribution, 280, 291
Bernoulli distributions, 289, 293
Bernoulli experiment, 43
Beta, 294
beta, XV, 247, 284, 388
beta and gamma distributions, 300
Beta distribution, 381
beta distribution, 67, 281, 290, 322,
390, 394
mean, 67, 291
mode, 67, 291
variance, 67, 291
beta distributions, 396
beta function, 67, 290
Beta law, 292
beta law, 270, 271
beta–binomial, 247
Beta-Binomial, 69, 290
biases, 393
BIC, 350, 355
binary skills, 179
binary variables, 205
binomial, 75, 191, 247, 300
binomial distribution, 43, 68, 290
Biology, 510
Biomass, 507, 515, 549, 550, 585
blueprints, 411
boundary, 449, 486
boundary variables, 449, 484, 492,
494
breadth, 23
Brooks-Gelman-Rubin, 311
brute force, 106
BUGS, 324, 327, 336, 383, 566, 604
burn-in, 310, 311, 315, 384
CAF, 21, 397, 411, 413–416, 457,
461, 474, 481
calculus, 24, 28, 30, 33
calculus of variations, 152
calibrate, 393

SUBJECT INDEX
649
calibrated, 390, 486, 488
calibrating, 6
calibration, 32, 38, 324, 383, 488
CamelCase, XIII
cascading, 452
CAT, 137, 143, 202, 455
categorical, 426
categorical distribution, 291, 292
categorical distributions, 283, 289,
293
categorical random variables, 57
causal, 95, 358, 360, 430
causal conclusions, 359
causal independence, 254
causal models, 96
Causality, 96, 361
cause, 360
centibans, 201
central limit theorem, 62
chain, 84, 112, 116, 122
Chain Graph Models, 101
chain of reasoning, 461
change, 588
Characterizing Proﬁciency, 442
chi-square, 484
chi-squared distribution, 71
child, 260
children, 84, 88
chord, 85, 129
claim, 24, 28, 419
Claims, 513
claims, 7, 9, 11, 24, 26, 28, 145, 171,
222, 224, 274, 414, 424, 426,
427, 442, 443, 454, 508, 514–
516, 523, 594–596
clairvoyant, 40
clarity, 24
clarity test, 40, 427
classical statistical, 64
classical test theory, 14, 20, 283
classiﬁcation matrix, 230
classiﬁcation probability, 216
clique, 83, 100, 111, 131, 133, 139,
140, 146, 147, 154, 180, 435,
449
clique intersection, 131
clique intersections, 130, 130
clique node, 113, 134, 150
clique nodes, 112, 120, 128, 155
cliques, 90, 105, 123, 129, 130
cognitive analyses, 415
cognitive analysis, 178
cognitive diagnosis, 20, 125, 283
cognitive model, 362, 372, 405
cognitive models, 15, 510
cognitive psychology, 21
Cognitive Theory, 96
cognitive theory, 12, 76
cognitively diagnostic assessment,
8, 593, 594
Cohen’s κ, 228, 230–232
Cohen’s kappa, 480
collateral information, 284
combination, 101, 149
combination function, 255, 257, 258,
260–262, 266
combination functions, 267
Combine, 144
combine, 111, 113, 134, 143
combined, 121, 131
combining, 113, 114
common cause, 359
common source, 246
communicative competence, 428
Compensatory, 604
compensatory, 175, 193, 235, 244,
262, 264, 266, 268, 276, 344,
465, 533, 547, 555
compensatory conjunctive distribu-
tion, 557
Compensatory Distribution, 172
compensatory distribution, 173, 177,
257
Competing Explanation, 56, 78
competing explanation, 92, 254
competing explanations, 122, 223
compile, 185
compiled, 161
compiling, 134
complete, 83

650
SUBJECT INDEX
complete data, 287, 294, 300, 301,
303
complete data problem, 294
complex tasks, 507
computer adaptive test, 453
Computer Adaptive Testing, 143
computer adaptive testing (CAT),
216
computerized adaptive test, 297
Computerized adaptive testing, 137
computerized adaptive testing, 36
Conceptual Assessment Framework,
19, 21, 26,
textbf 27, 415, 515
conceptual assessment framework,
23, 27, 37, 392, 397, 411, 497,
550
concurrent calibration, 393, 396
condition, 85, 150
conditional dependence, 138
conditional exchangeability, 286
conditional Gaussian, 101, 158
Conditional Independence, 51, 53
Conditional independence, 308
conditional independence, 6, 87, 91,
99, 113, 128, 131, 142, 180,
279, 297, 315, 428, 432, 486
Conditional Multinomial, 246
conditional multinomial, 244, 253,
292, 379
conditional multinomial distribu-
tion, 249, 269
Conditional Probability, 46
conditional probability, 126
conditional probability distribution,
31, 107, 172
conditional probability distributions,
107
conditional probability table, 161,
172, 182, 193, 194, 242, 267,
292, 367, 379, 381, 385, 390,
552
conditional probability tables, 159,
168, 172, 268, 319, 405, 552,
555, 603
conditional weight of evidence, 201,
203
conditional weights of evidence, 203
conditionally independent, 126, 137,
348, 551, 558
conditioned, 430
conditioning, 165
conﬁdence interval, 44
confusion matrix, 226, 479
conjugacy, 269
conjugate, 249, 289, 293, 553
conjugate families, 69, 69, 290
conjugate family, 70, 74, 246, 291
conjugate prior, 291, 292
Conjunctive, 55
conjunctive, 175, 179, 181, 193, 235,
244, 268, 276, 517, 547, 569,
572
Conjunctive Distribution, 172
conjunctive distribution, 173, 176,
177, 258
Conjunctive Model, 78
conjunctive model, 175, 250
Conjunctive,, 604
connected, 84
consistency, 230, 232
Consistency Matrix, 230
consistency matrix, 230
constraint, 442
constraints, 33, 221, 221, 222, 236,
454
construct irrelevant variance, 420
construct underrepresentation, 420
construct-centered, 22
Constructed response, 472
constructed response, 15, 21, 445,
472, 515
constructive tasks, 509
content constraints, 494
Context, 570
context, 449, 551, 559
Context Eﬀect, 504
Context variable, 171
context variable, 532
context variables, 338, 448, 581

SUBJECT INDEX
651
contexts, 586
contingency tables., 100
continuous, 159, 426
continuous random variables, 57
converged, 311, 384
Convergence, 309
convergence, 310
correlation, 266
correlation matrixes, 433
covariance matrix, 99
covariates, 284, 286, 298
CPT, 260, 262, 553
credibility interval, 329, 343, 398
credibility intervals, 72
critiquing, 38, 218, 220, 456, 496
Cross validation, 355
cross validation, 362
crosstab, 482
culminating, 507
cumulative logistic distribution, 281
cut point rule, 479
cut score, 225, 434
cut scores, 426, 457
cycle, 84
D-Map, 93
D-separation, 56
d-Separation, 92
d.f., 59
DAG, 85
DAGs, 358
data cleaning, 565
debugged, 197
decision analysis, 40
decision variables, 97
decision, 209
declarative knowledge, 512
decode, 277
deductive reasoning, 165
deletion, 92
delivery model, 460, 497
Delivery System Model, 34
delivery system model, 34
demographic, 580
demographic variable, 286, 348, 486
demographic variables, 244, 361
Dempster–Shafer models, 3
dental hygiene, 107, 210
Dental Hygienist, 153
depth, 23
descendant, 84, 92
deviance, 334, 350
diagnostic, 95, 405
diagnostic assessment, 8, 34, 145,
596
diagnostic feedback, 472, 473
diagnostics, 499
DiBello–Samejima, 276, 339, 349,
442, 464, 534, 552, 603
DiBello–Samejima Eﬀective Theta
distributions, 273
DiBello-Samejima, 235, 284, 311,
555
DIC, 351, 355, 365, 367
DIF, 284
diﬀerential diagnosis, 128, 154
Diﬀerential item functioning, 286
diﬀerential item functioning, 284,
347
Diﬀerential item functioning (DIF),
486
Diﬀerential Task Functioning, 503
diﬀerential task functioning, 347,
367, 396
diﬃculty, 14, 159, 202, 235, 255,
257, 262, 266, 273, 276, 277,
442, 547, 555, 556, 585
diﬃculty increment, 272, 276
diﬃculty parameter, 261, 272
diﬃculty parameters, 281
digraph, 83, 112, 134
dimensionality, 351, 352, 365
dimensions, 15
DINA, 251, 343, 344
Direct Data Display, 346
direct evidence, 183, 203, 207, 218,
396
directed edge, 128
directed graph, 83, 94, 95, 122, 173
Directed Graphical Model, 87

652
SUBJECT INDEX
directed graphical representation,
241
directed graphs, 358
directed hyperedges, 88
directed hypergraph, 88, 172
directed hypergraph representation,
242
directed representation, 107
direction of an edge, 95
Dirichlet, XV, 284, 294, 296, 298,
396
Dirichlet distribution, 70, 291, 300
Dirichlet law, 248, 292, 553
Dirichlet-Multinomial family, 291
disabilities, 33
Disciplinary Knowledge, 513, 515,
517
discrepancy measure, 337
discrepancy measures, 338
discrete, 4, 159, 426
discrete random variables, 57
discrete variables, 216
discrimination, 202, 255, 257, 273,
276, 277, 555, 585
discrimination parameters, 272
Disjunctive, 604
disjunctive, 175, 193, 253, 276
Disjunctive Distribution, 172
disjunctive distribution, 173, 176,
177, 235, 258
distractor, 472
distribution, 59, 268
distribution function, 59, 433
distribution tables, 255
distributions, 244
divided bar chart, 434
docks, 489
Domain Analysis, 415, 510
domain analysis, 23, 415, 420
domain knowledge, 547, 563
domain model, 416
Domain Modeling, 97, 415, 510
domain modeling, 23, 415, 416
Dynamic Bayesian networks, 591
dynamic Bayesian networks, 152,
589
dynamic task model variables, 443
e-Learning, 217, 220
E-step, 300, 301
EAP, 65, 434, 473, 488, 491–494,
497–499, 585
EAP score, 560
EAP scores, 573, 578
EAP, 473, 489
easier, 556
ECD, 4, 5, 7, 11, 19, 20, 221, 405,
451, 461, 507, 601
ECD design, 14
ECD framework, 171
edges, 5, 82, 161, 205
educational assessments, 20
educational standards, 25
eﬀective MCMC sample size, 387
eﬀective sample size, 247, 310, 380,
553
eﬀective sample sizes, 381
eﬀective theta, 255, 257, 555, 557,
560
eﬀective theta distributions, 272
EIP, 488
elicitation, 74, 262
eliminate, 150
EM, 301, 393
convergence, 305
EM Algorithm, 328
EM algorithm, 280, 288, 300, 317,
324, 330, 344, 604
EM algorithms, 487
EM solutions, 309
emergent tasks, 477
EMF, 138, 139, 143, 144, 146, 529,
532
EMFs, 535
enemy list, 224
engineering discipline, 594
entropy, 216, 334, 346, 353
equal probability space, 75
equate, 392

SUBJECT INDEX
653
equating, 8, 598
equivalence classes, 345
error variance, 290
essay, 530
evidence, 3, 11, 108, 113–115, 121,
125, 143, 145, 146, 175, 404,
431, 443, 489, 508, 514, 526,
596
evidence accumulation, 31, 35, 544
Evidence Accumulation Process, 36
Evidence accumulation process, 470
evidence accumulation process, 35,
36, 37, 38, 467, 488, 530, 540,
541, 543
evidence balance sheet, 202, 542,
603
evidence balance sheets, 494
evidence bottlenecks, 208
Evidence Centered Design, 97, 167,
274
Evidence Centered Design (ECD),
268
evidence identiﬁcation, 30, 163, 538,
544
Evidence Identiﬁcation Process, 35
Evidence identiﬁcation process, 470
evidence identiﬁcation process, 35,
35, 37, 38, 472, 473, 491, 493,
501, 539, 540, 543, 545
Evidence Model, 29, 179, 181
evidence model, XIII, 19, 34, 39,
135, 138, 143, 158, 159, 171,
193, 221, 224, 226, 244, 246,
343, 345, 351, 378, 396, 397,
438, 439, 442, 443, 451, 474,
478, 484, 486, 487, 492, 497,
532, 595
Evidence Model Fragment, 139
evidence model fragment, 233, 542
Evidence Model Fragments, 180–
182
evidence model parameters, 38, 576
evidence model variables, 82
Evidence Models, 76, 242
Evidence models, 29, 375, 442
evidence models, 12, 29, 33, 34,
137, 145, 147, 182, 268, 273,
289, 315, 347, 395, 451, 457,
467, 473, 482, 486, 515, 522,
529, 550, 554, 589
evidence order, 202
evidence rule, 276
evidence rule analysis, 481
evidence rule data, 37, 445, 530,
541
Evidence rules, 29, 447
evidence rules, 6, 36, 38, 447, 472,
478, 481, 486, 499, 540, 543,
565, 566, 570, 581
evidence variables, 5
Evidence–centered design, 14
Evidence-centered assessment design,
413, 448
evidence-centered assessment design,
4, 583, 596
Evidence-Centered Design, 76, 180,
197
Evidence-centered design, 19, 20
evidence-centered design, 396, 486,
488
evidence-rule data, 472, 475, 478
evidentiary argument, 415, 419
evidentiary focus, 40, 273
evidentiary reasoning, 20
evidentiary value, 127
EWOE, 218, 221, 232, 235, 495,
497
examinee, 283, 472, 492
examinee ﬁt, 575, 577
examinee record, 470, 471, 472,
473, 490, 491, 500
examinees, 284
exchangeable, 284, 286, 379
expectation, 300
expectation a posteriori, 65
expectations, 301
expected a posteriori, 497
expected accuracy matrix, 482
expected proﬁciency level, 560
Expected Value, 60, 434

654
SUBJECT INDEX
expected value of information, 210,
217
Expected Weight Of Evidence, 220
Expected weight of evidence, 217
expected weight of evidence, 166,
197, 218, 235, 276, 278, 406,
454, 456, 484, 494–496, 585
Expected Weight Of Evidence (EWOE),
213
expert, 14, 188
experts, 510
explain, 198
explaining, 6
explaining away, 152
Explanation, 197
explanation, 150
exponential family, 300
exposure controls, 494
factor, 128
factor analysis, 14, 283
factorization hypergraph, 88, 128
factors, 100
fading, 588
fair, 486
fairness, 46, 171
false discovery rate, 480
False negative, 377
false positive, 377, 479
false-negative, 251, 381
false-positive, 250, 381
false-positive parameter, 252, 270
feedback, 205, 423, 496, 507, 530,
543, 544
feedback observables, 452, 530, 539,
540, 544
ﬁll in the blank, 158
ﬁlled in, 85, 129
ﬁnal observables, 530, 540, 545
Fisher information, 213, 216, 454
ﬁxed-form, 220
ﬂexibility, 170, 172
Focusing Evidence, 441
footprint, 138, 139, 143, 146, 154,
221, 532, 533, 542, 552
footprints, 142
forecast, 143
form, 139, 145, 148, 149
formative assessment, 507, 594
forms, 392, 453, 598
Forward and Backward Selection,
356
Forward Selection, 355
four -process delivery architecture,
34
four processes, 37, 488
four-process architecture, 19, 21,
26, 468, 473, 474, 478, 488,
497, 499, 522, 536
four-process assessment delivery sys-
tem, 535
four-process cycle, 473
four-process delivery architecture,
544
four-process delivery system, 543
fragment, 450, 491
fragments, 145, 180
frame, 101
frame of discernment, 91, 149
full Bayesian model, 182, 288, 324,
382
full conditional, 308, 312, 315, 319
full conditional distributions, 325
full conditionals, 317
full Noisy-And distribution, 252
full noisy-and distribution, 271
functional dependence, 451
functional relationship, 282
fusion and propagation algorithm,
105, 149, 158, 200
games, 459
gamma, XV
gamma distribution, XV, 71
Gelman–Rubin R, 384
Gelman-Rubin potential scale reduc-
tion factor, 384
gender, 347, 486
general measurement model, 299,
316

SUBJECT INDEX
655
generative probability model, 377
Gibbs, 317
Gibbs Sampler, 324
Gibbs sampler, 308, 309, 312
Gibbs sampling, 315
Gibbs–Markov Equivalence, 94
Gibbs–Markov equivalence, 94
global ﬁt, 332
global independence, 298, 308
global parameter dependence, 316
Global parameter Independence, 244
global parameter independence, 248,
258, 273, 316
Good’s Logarithmic Score, 334
Good’s logarithmic score, 364, 365
graded response, 255, 260, 262, 265
grain size, 517, 519, 599
graph, 5, 82, 83, 449
graphical belief functions, 149
graphical model, 4, 11, 86, 100, 158
graphical models, 82, 97, 149
graphs, 9, 97
greedy, 356
greedy search, 356
group work, 595
growth, 152, 217, 220
hidden variables, 359
hierarchical interaction model, 100
hierarchical model, 280
hierarchically, 25
hierarchy of claims, 423
high, 577
high stakes, 8
high-stakes, 183, 441
high-stakes assessments, 392
high-stakes tests, 7
history plot, 385
HIV Test, 50
holistic scoring, 445, 452
HUGIN, 130
Hugin algorithm, 115
HYDRIVE, 10, 477
Hyper-Dirichlet, 275
hyper-Dirichlet, 246, 249, 258, 292,
298, 379, 385, 464, 552
hyper-Dirichlet law, 249, 269, 296,
553
Hyper-Markov Law, 245
hyperedge, 128
hyperedges, 88
hypergraphs, 88
hyperparameters, 244
hypotheses, 225
hypothesis, 201, 227, 494
hypothesis variables, 200
hypothesis., 200
hypothetical data, 270
I-Map, 93
identiﬁability, 12, 271, 329, 598
improper priors, 75
incidental, 440, 547
incidental parameters, 299
incidentals, 476, 487
incomplete data, 288
incomplete data problem, 294, 299
inconsistent, 299
Independence, 51, 52, 60
independence, 92, 99, 297
independence assumptions, 379
independent, XV, 57, 152, 173, 230,
275, 284
indirect evidence, 183, 203, 207,
208, 233, 396
induced dependencies, 146, 148
induced dependency, 142
inductive reasoning, 166
inﬂuence diagram, 95, 424, 590
Inﬂuence diagrams, 97
inﬂuence diagrams, 149, 586
information, 232
inhibitor, 277, 278, 465, 547, 559,
569
inhibitor distribution, 258, 367, 534
inhibitor eﬀect, 208
inhibits, 259
initial calibration, 315, 321, 323
initial values, 310

656
SUBJECT INDEX
initialization, 472
inquiry, 515
instantiate, 108, 163, 185, 188, 494
instantiated, 6
instantiates, 491
instantiating, 162, 496
instruction, 37, 468, 596
instructional design, 218
instructional goals, 220
instructional mode, 217, 220
instructional plan, 209
integer–valued random variables, 57
Integrated Knowledge, 512, 513, 517,
539
integrated knowledge, 547
integrated tasks, 165, 507
Intelligent Tutoring, 217
intelligent tutoring, 220
Intelligent Tutoring System, 406
intelligent tutoring system, 15
Intelligent Tutoring Systems, 583
intelligent tutoring systems, 591
intercept, 257, 555–557
intermediate observable, 447
intermediate variables, 200
intersection node, 120, 150
intersection nodes, 112, 155
inverse correlation matrix, 267, 433
IRT, 31, 55, 76, 101, 192, 202, 257,
260, 263, 273, 300, 338, 342,
442, 448, 449, 542, 604
IRT CAT, 471, 472
IRT models, 272
IRT-CAT, 216, 217
isotonic, 193
item, 146, 158, 180
item analysis, 481, 580
item characteristic curve, 342
item mapping, 427
item parameters, 312, 473
Item Response Theory, 14,
textbf 158, 170, 256
item response theory, 18, 20, 55,
102, 283, 297, 312
item response theory (IRT), 216,
254
items, 9, 147, 166, 170, 221, 224,
290
iterative proportional ﬁtting, 300
Jeﬀrey’s rule, 247, 248
joint distribution, 88, 110, 113, 132
joint expected weight of evidence,
220
joint probabilities, 108
joint probability, 47
joint probability distribution, 126,
430
junction, 142
Junction tree, 150
junction tree, 112, 112, 113, 116,
118, 122, 123, 128, 130–132,
134, 139, 140, 154, 155, 161
junction tree., 130
junction trees, 137, 139
junction-tree algorithm, 111, 123
key, 30, 36, 445, 478, 530
key matching, 539
key-matching, 445, 446
knowledge engineering, 413
knowledge representations, 513, 524
knowledge, skills, and abilities, 27
knowledge-based model construc-
tion, 136, 586
Kullback–Leibler, 213, 215
Kullback–Leibler distance, 198
lag, 309, 386
Language Placement Test, 417
Language Placement Test Claims,
421
Language Placement Test Proﬁciency
Model, 428
latent class, 166, 301, 307
latent class analysis, 14, 20, 283
latent class model, 159, 292
latent classes, 274
latent proﬁle analysis, 283

SUBJECT INDEX
657
latent scale, 315
latent variable, 267
Latent variable correlations, 265
latent variables, 163, 248
law, 268
Law of Large Numbers, 43
Law of Total Probability, 48
law of total probability, 87
laws, 244, 284
lazy-propagation algorithms, 144
leaf, 84
learn, 14
learning, 6, 217, 220
learning progressions, 427, 516
learning, model, 241
leave one out prediction, 333
level diﬀerence parameters, 272
leverage, 352
licensure test, 226
likelihood, 49, 115, 120, 121, 127,
290, 324, 350, 377, 493, 500
likelihood functions, 302
likelihoods, 76, 289
linear, 493
linear form, 455
Linear programming, 221
linguistic priors, 273
link, 486, 489, 492–494, 500, 505
link function, 255, 260, 262, 265
link functions, 263, 267
link model, 38, 283, 316, 317
link models, 289, 554
link parameters, 244, 451, 482
linked, 598
linking, 383, 388, 392, 396
links, 181, 221, 273, 451, 473, 474,
478, 482, 490, 491
literary terms, 125
local dependence, 366, 448, 451,
585
local dependence property, 193
Local Independence, 137
local independence, 145, 167, 308,
344, 449, 455, 587
Local Independence Property, 139
local independence property, 146,
167, 170, 316
local item independence, 158
local maxima, 356
Local Parameter Independence, 244
local parameter independence, 249,
272
logarithm score, 365
logarithmic score, 350, 364, 365
Logarithmic Scoring Rule, 334
logical distributions, 181, 379
logical probability, 194
logistic, 260, 265
low, 565, 577
M-step, 300, 301, 303
machine learning, 446
Mantel–Haenszel, 366, 484, 486
MAP, 65, 192, 226, 227, 305, 328,
434, 482
MAP estimate, 216, 225, 230, 493
MAR, 297, 325, 326, 498
marginal, 47
Marginal Belief, 153
Marginal Distribution, 434
marginal distribution, 110, 346, 481,
482, 492–494
marginal distributions, 564
Marginal Independence, 51
marginal likelihood, 299
marginal probability, 231
marginalization, 110, 150
Marginalize, 113
marginalize, 131
marginalizing, 133
margins, 108
market basket, 435, 597
market basket reporting, 500
Markov, 94, 94
Markov Chain, 309
Markov chain, 306
Markov Chain Monte Carlo, 280,
305, 307
Markov Decision Process, 590
Markov decision processes, 591

658
SUBJECT INDEX
Markov property, 309, 589
Markov Tree, 131
Markov tree, 150, 151, 208
Markov tree propagation, 149, 151
Markov trees, 149
married, 93, 128
mastery, 437
Math Quiz, 167
maximizes, 300
Maximum A Posteriori, 192, 226
maximum a posteriori, 65
maximum likelihood, 291
Maximum Likelihood Estimate, 192
maximum likelihood estimate, 247,
290, 291
Maximum likelihood estimation, 64
maximum likelihood estimation, 299
Maximum marginal likelihood esti-
mation, 299
MCAR, 297, 325, 326, 498
MCMC, 305, 308, 312, 324, 330,
336, 356, 382, 385, 393, 398,
402, 429, 487, 566, 576, 604
MCMC chains, 384
MCMC estimation, 315
MCMC estimation error, 387
MCMC sampler, 344, 389
MCMC, mixing, 385
mean, 61, 434
mean ﬁeld method, 152
meanings, 419
measure, 43
measurement component, 135, 137
measurement model, 31, 38, 467,
535
measurement models, 20
measures of agreement, 228
mediating relationships, 442
message, 114, 120
message center, 536, 540, 544
messages, 114
method of moments, 322, 323, 394
Metropolis, 313, 315, 325, 385
Metropolis–Hastings algorithm, 356
Metropolis-Hastings, 313, 315
minimum constraint, 454
minimum constraints, 457
minimum entropy, 218
MIRT, 584
misﬁt., 404
Missing at Random, 406
missing at random, 297, 393, 406
missing completely at random, 297
missing data, 297
missing responses, 297
missing-at-random, 393
mix, 324
mixed models, 100
mixed number subtraction, 345, 346,
371, 372
mixed-number subtraction, 178, 188,
194, 420
Mixture models, 287
mixture of trees, 151
MLE, 192, 290, 298, 300
mode, 434, 493
Model checking, 362
model checking, 76, 325
Model criticism, 575
model ﬁt, 332, 577
model ﬁts, 362
model graph, 205
model likelihood, 354
Model Search MCMC, 356
model search MCMC, 357
model validation, 397
model-comparison, 350
models, 27, 416
Monte Carlo, 307
Monte Carlo Integration,
textbf 62
moral, 94
moral graph, 90
moralization, 90, 153, 154, 449
moralize, 139
Moralized, 128
moralized, 142
moralized graph, 128
most likely conﬁguration, 150
Most Likely Explanation, 434

SUBJECT INDEX
659
Most Likely Value, 434
motif, 147
motifs, 147
multidimensional, 8
Multidimensional Item Response The-
ory, 427
multidimensionality, 146
multinomial, 248
multinomial distribution, 70, 291
multinomial distributions, 291, 300
multiple choice, 7, 501, 515, 530,
541
multiple proﬁciencies, 595
Multiple-choice, 158
multiple-choice, 444, 478, 503
multiple-choice items, 472
Multiplication Rule, 47
multiply connected graph, 122
Multistage, 456
multivariate latent class, 398
multivariate normal, 100
Mutual Independence, 52
Mutual Information, 277
mutual information, 208, 215, 217,
277, 484
myopic, 210, 356
myopic search, 216, 220
NEAT, 396, 406
negative predictive value, 480
neighborhood, 83
neighbors, 83, 308
Netica, 185
NetPASS, 546
neural network, 197
Next Generation Science Standards,
508
NIDA, 252
NIDA distribution, 271
node, 161, 280
node coloring, 198
node ﬁt, 332–334
nodes, 5, 82
Noisy-And, 274
Noisy-and, 270
noisy-and, 172, 250, 343
noisy-max, 172, 253
noisy-min, 172, 253
Noisy-Or, 275
noisy-or, 172, 244, 250, 253
non-informative, 247
non-informative prior, 248, 249, 271
nonidentiﬁability, 76
noninformative priors, 72, 74
nonmyopic search, 220
normal, XV, 298
Normal Distribution, 62
normal distribution, 70, 256, 300,
396
normal law, 272
normalization, 58, 107
normalization constant, 58, 58, 115,
351
normalize, 108, 111, 122
normalized, 59, 115, 351
normalizing constant, 289, 290, 298
nuisance variable, 171
number right, 183, 489, 500, 564
object-oriented Bayesian network,
137
object-oriented Bayesian networks,
587
objective, 45
objective function, 221
observable, XIII, 7, 12, 40, 147,
158, 221, 332, 447–450, 470,
478, 494, 497, 544, 564
observable characteristic plot, 342,
343, 345, 367, 398, 401, 603
observable ﬁt, 403
observable ﬁt statistic, 345, 401
observable outcome, 38, 172, 263,
589
observable outcome variable, 5, 146,
171, 252, 378, 482
observable outcome variables, 6, 9,
10, 179, 193, 198, 244, 444,
448, 449, 484, 491, 529

660
SUBJECT INDEX
observable outcomes, 10, 35, 36,
167, 316, 348, 585, 587
observable variable, 30, 107, 381
Observable variables, 137
observable variables, 29, 31, 126,
135, 143, 200, 282, 288, 292,
491, 540, 551
observable,, 159
observables, 31, 139, 170, 187, 223,
334, 346, 427, 444, 447, 449,
451, 498, 532, 542, 581
observation, 213
observation vector, 150
observational equivalence, 74, 269,
290, 291
observational studies, 361
observations, 35, 492, 514, 524
observe, 10, 11
observed outcome, 175, 185, 377,
472, 481
observed outcome variable, 268, 342
observed outcome variables, 499
observed outcomes, 150, 224, 470,
473, 488, 489, 493, 494, 499
odds ratio, 366, 366
omitted responses, 497
on-line calibration, 315, 321
one parameter logistic (1PL), 261
operating characteristics, 228
or-gate, 244
or-gates, 173
order, 202
outcome, 201, 470
outcome pattern, 345
outcome space, 82, 91
outcome variables, 478
outcome vectors, 220, 231
outcomes, 30
outliers, 352
over ﬁtting, 355
overall ﬁt, 403
overall proﬁciency, 217
overlap constraint, 455
overlap constraints, 454, 457
P+, 161
p-plus, 330
p.d.f., 58
p.m.f., 57
paradigms, 416
parameter, 244
parameter estimation, 280
parameter learning, 280
parameter uncertainty, 393
parameters, 126, 242, 266, 268, 280,
294, 297, 298, 351, 487
parent, 146, 267, 292
parents, 84, 88, 126, 128, 146, 161,
172
parsing rules, 479
part of, 102
part-of, 429
partial ancestral graphs, 358
partial credit, 30, 255
partial graph, 83
participant, 24
particle ﬁltering, 152
passing messages, 351
path, 84, 122
path analysis, 6, 99, 266
Pearson residual, 402
Pearson residuals, 337
peeling, 151
percent correct, 492
perfect map, 94
performance, 137
person ﬁt, 334, 342, 402, 404, 405
person parameters, 288
PFEM algorithm, 155
pixel, 346
planning, 590
plate, 284
Plate notation, 280, 280
plate notation, 242, 292, 382
plates, 280, 281
PMEM, 143, 144
PMEM algorithm, 139, 143
PMF, 138, 139, 139, 140, 143, 144
polytree, 116
polytrees, 111, 152

SUBJECT INDEX
661
POMDP, 590
pool, 454
population, 73, 347, 418, 428
population distribution, 515
population parameters, 244
positive predictive value, 480
positive states, 208
posterior, 49, 291, 298, 308, 309,
393
posterior covariance matrix, 299,
300
posterior distribution, 69, 134, 143,
210, 289, 292
posterior distributions, 305, 316
posterior laws, 567
posterior means, 298
posterior mode, 298, 300
posterior model probability, 354
posterior predictive p-value, 402,
576
posterior predictive check, 401
posterior predictive checks., 346
posterior predictive data, 577
posterior predictive distribution, 346
posterior predictive distribution, 335
posterior predictive model check-
ing, 486
posterior predictive p-value, 337
posterior predictive tests, 337
posteriors, 289, 320
potential, 107, 113, 120, 121, 133,
134, 139, 140, 149
potential table, 113, 131
potential tables, 115, 139
potential weight of evidence, 208
potentials, 90, 94, 107, 151
practice, 508
precision, 61, 71, 327, 480, 567
prediction, 333, 353
predictions, 332
predictive distributions, 144
preposterior predictive data, 575
prerequisite, 179, 180, 194, 379, 405,
428, 429
Presentation material, 438
presentation material, 32, 40, 438,
440, 458, 503
Presentation Model, 33
presentation model, 33, 458
presentation models, 34
Presentation Process, 34
Presentation process, 468
presentation process, 34, 35, 37,
38, 472, 473, 500, 501, 538,
540, 543, 544
pretest, 486
pretest data, 38, 481, 484, 487
Principle of Equal Probability Space,
45
prior, 45, 49, 291, 324, 393, 395
prior distribution, 29, 69, 72, 384
prior distributions, 81, 379
prior laws, 293
priors, 173, 289, 300, 380
probabilistic classiﬁcation, 231
probabilistic classiﬁcation matrix,
231
Probability, 43, 44, 46
probability, 3, 14, 42, 43, 58
probability density function, 58
probability distribution, 107
probability mass function, 57
probability potential, 101
probative value, 127
probit, 265
procedural scoring algorithm, 446
process knowledge, 509
product multinomial, 249
proﬁciency, 5, 9, 12, 472, 588
Proﬁciency Model, 76, 242, 316,
515
proﬁciency model, 11, 12, 14, 19,
27, 29, 31, 34, 39, 135, 137,
138, 145, 146, 152, 154, 171,
180, 203, 216, 225, 244, 267,
269, 289, 317, 348, 377–379,
395, 397, 398, 424, 426, 433,
441, 443, 444, 448, 451, 455,
467, 471, 481, 484, 488–493,

662
SUBJECT INDEX
495, 515, 519, 529, 533, 535,
541, 552, 563, 580, 584
Proﬁciency Model Fragment, 180
proﬁciency model fragment, 146,
373
proﬁciency model parameters, 384
proﬁciency model variable, 33
proﬁciency model variables, 29, 31,
82, 454
proﬁciency model variables, 28
Proﬁciency Model–Evidence Model
algorithm, 139, 143
proﬁciency models, 33, 135, 298,
500
proﬁciency proﬁle, 150, 165, 189,
200, 201, 216, 343, 376–378,
398, 426, 500
proﬁciency proﬁles, 201, 220, 227,
344, 346, 377, 379, 398, 428,
431, 433, 448, 515
proﬁciency variable, 28, 107, 158,
171, 172, 223, 263, 266, 292,
317, 378, 397, 434, 449, 454,
551, 555, 581
Proﬁciency variables, 298
proﬁciency variables, XIII, 5, 31,
40, 126, 135, 137, 139, 143,
150, 163, 179, 188, 189, 198,
200, 221, 222, 224, 244, 263,
268, 274, 281, 288, 297, 299,
315, 316, 332, 355, 389, 426,
427, 442, 448–450, 482, 484,
486, 492, 493, 515, 529, 532,
550, 576, 589, 597
proﬁle score, 201
proﬁle scored assessment, 216
proﬁle scores, 15
project, 109
projection, 101, 149
propagated, 186
proper distribution, 67
proper probability distribution, 75
proposal distribution, 313, 325, 328
prospective score report, 594
prototype score report, 436, 437
psychometric models, 137
psychometrician, 269
psychometricians, 5, 7
psychometrics, 598
purpose, 23, 417, 426, 461
purposes, 457
Q-Matrix, 270, 336, 345
Q-matrix, 419, 449
Q3, 338
quasi-utility, 213, 215–217
quasiutilities, 454
quiz, 159
race, 486
racial, 347
radical, 440, 488, 547
radicals, 476, 477, 487
random experiment, 42
random sampling, 360
random variable, 290, 297
random variables, 57, 98
Ranked Probability Score, 334
ranked probability score, 364, 365
Rare Disease Problem, 51
rare disease problem, 228
Rasch, 160, 258, 261, 330
Rasch model, 159, 281
raters, 30, 103, 119
reading comprehension, 463
reading passage, 137
reading passages, 166
real–valued random variables, 57
Rebuttal data, 420
recall, 479
receiver operator characteristic (ROC),
480
Recognizing Task Situations, 443
recursive decomposition, 127
recursive models, 100
recursive representation, 48, 126,
137–139
Regression Distribution, 234
regression distribution, 234, 235,
273

SUBJECT INDEX
663
relevance diagram, 99
relevant potential weight of evidence,
208
reliability, 7, 7, 23, 197, 198, 225,
227, 232, 265, 423, 436, 574
Reporting, 433
Reporting rule, 492
reporting rule, 28
reporting rules, 433, 435, 519
reporting variable, 456
reporting variables, 542
reporting variables, 203
repurposing, 507
Resampling Distribution, 61
research observables, 453
response, 470, 478
response patterns, 303
response processing, 35, 470
response time, 453
Results Database, 35
reuse, 511
reversibility, 308
reversible jumping rule, 357
risk, 214
ROC curve, 502
root node, 150
rubric, 30, 445
rubrics, 10, 472
rule of evidence, 452
Rule Space, 178–180
rule space, 377
rule-based system, 198
Rules of Evidence, 530
rules of evidence, 7, 10, 158, 159,
278, 444, 529, 554
running intersection, 118, 131
Running Intersection Property, 131
sample, 150, 561
sample size, 291
Sampling Algorithm, 150
sampling algorithm, 155
saturated model, 147
science, 509
scientiﬁc inquiry, 511
score, 225, 390, 433, 492
score report, 500, 519, 545
score reports, 539
score users, 433
Scores, 489
scores, 36, 225, 397, 424, 488, 500
scoring, 163, 488
scoring engine, 330
scoring model, 29, 351, 433, 471,
472, 489, 491, 491, 492–494,
500, 519, 541
scoring record, 35
scoring rubric, 478
scoring rule, 230
second layer, 242
section, 488
sections, 456
Security, 476
security, 315, 383
selected response, 446
selection bias, 360, 367
self-evaluation, 508
sensitivity, 121, 153, 228, 271, 479
Sensitivity Analysis, 77
sensitivity analysis, 45, 67
separable inﬂuences, 254, 275
Separation, 91
separator, 306
Sequential importance sampling, 152
shadow data, 335, 336, 346, 401,
575
shrinkage estimator, 79
shrinkage estimators, 74
signal to noise ratio, 225
SimCityEDU, 11
SimCityEDU, 546
simple graph, 82
simple language test, 198
Simple structure, 223
simple structure, 146, 406
simpliﬁed language assessment, 237
simpliﬁed language test, 227, 229
Simpson’s paradox, 366
Simulated Annealing, 356
simulation, 21, 472, 521

664
SUBJECT INDEX
simulation task, 107
simulations, 15, 459
simulee, 150
simulees, 231
skill proﬁle, 225
skill proﬁles, 152, 345
skill workaround parameters, 252,
271
slice sampler, 315
slope, 258, 555, 556, 558
slope parameter, 257
slow mixing, 309, 314, 385
Speaking, 455
speciﬁcation rules, 440
speciﬁcity, 121, 153, 228, 480
speededness, 453
stakes, 457
standard deviation, 61
standard error, 290, 320
standard error of estimation, 64
standardized tests, 20
Standards, 509
standards, 508, 516, 597
standards-based, 508
starting values, 384
stationary, 384
stationary distribution, 309–311, 313
statistic, 500
statistical part, 31, 448
statistics, 433, 493, 542
StatShop, 383, 384
stealth assessment, 595
step size, 313
stimulus, 137
stimulus materials, 524
stopping rule, 216, 455
story problem, 259, 277
Story problems, 167
strong prior Bayesian, 72
strong priors, 76
Structural equation models, 99
structural equation models, 6, 355
structural equation models, 81
structural parameters, 299
Student t, 71
student growth, 596
student model, 221, 584
student record, 455
subgraph, 83
subject matter experts, 5, 267, 269
subjective, 76
Subjective Probability, 77
subpopulations, 347
subscores, 183
Subtest Independence, 77
suﬃcient statistics, 294, 300, 303
sum of scores, 31
summary feedback, 470, 473, 538
summary scoring, 36, 470
Summary Scoring Process, 76
summative, 595
superobservable, 452
support, 58, 309
target, 38
target hypothesis, 220
target population, 426
target rule, 224, 454
target rules, 221, 221
targets, 33, 456
task, 5, 29, 34–36, 137–139, 143,
145, 146, 150, 158, 167, 170,
171, 180, 193, 210, 262, 282,
317, 344, 447, 478, 486, 491,
494, 522, 523, 538, 540, 570,
587, 595
Task Construction, 440
task design, 145
task ID, 541, 543
task level feedback, 473, 488, 499
Task Model, 32
task model, 19, 30, 34, 39, 147, 222,
246, 273, 438, 440, 447, 458,
486, 497, 523, 525, 529
task model variable, 286, 525
Task model variables, 440
task model variables, 32, 222, 224,
432, 442, 454, 475, 486–488
Task models, 32, 523

SUBJECT INDEX
665
task models, 33, 34, 221, 315, 375,
392, 397, 443, 451, 454, 474,
487, 554, 588
task pool, 442
task response, 491
task selection, 210, 584
task shells, 588
task-based feedback, 470, 538
task-level feedback, 539
Task/Evidence Composite Library,
37
task/evidence composite library, 468,
470, 472–474, 478, 488, 494,
541, 543
task/evidence library, 491
tasks, 11–13, 36, 137, 147, 166, 221–
224, 284, 286, 315, 438, 443,
451, 474, 486, 508, 543, 551,
554, 566, 596
teacher, 519
Tentacles, 89
test forms, 377
Test Length, 79
test security, 453
test statistic, 337, 575
test user, 519
test users, 23
testing program, 230
testing programs, 392
Testlet, 353
testlet, 137, 338, 352, 449, 551
the four-process architecture, 471
The Proﬁciency Model, 27
Three Gorges Dam, 382
three-way table, 484
threshold, 479
total graphical model, 135, 137,
144, 145, 154
Toulmin diagrams, 459
trace plot, 385
training data, 355
trait theories, 12
treat assessment, 594
tree, 84, 116
tree of cliques, 122, 123
treewidth, 105, 116, 123, 129, 130,
148, 149, 151, 154, 180, 430
triangulate, 139
Triangulated, 128
triangulated, 85, 129, 142
triangulation, 139, 154
true positive, 479
true score, 225
True Score Test Theory, 79
true-positive, 271
true-positive parameter, 270
truncated normal, 581
truncated normal law, 272
two-way table, 482
Type I Errors, 479
Type II Errors, 479
ubiquitous assessment, 595
undirected graph, 83, 94, 95
unidimensional, 7, 146, 158
Uniﬁed Knowledge, 513, 546
uniform distribution, 75
uniform distributions, 139
unit potential, 139, 140, 143, 144
unobtrusive assessment, 595
update algorithm, 151
urn, 58
utilities, 97, 98, 101
utility, 209, 213, 214
utility function, 226
validating, 165, 188
validity, 7, 7, 12, 24, 227, 274, 457,
578, 596
validity study, 437
valuation based system, 90
valuation-based systems, 101
valuations, 149
value of information, 98, 210, 214,
215, 424, 519
valued work, 223, 597
variable, 82, 161, 244, 292
variables, 268, 280
Variance, 61
variance, 313, 327, 567

666
SUBJECT INDEX
vat, 454
vertices, 82
virtual evidence, 120, 121, 122, 139,
447, 542
warrant, 419
weak prior, 271
weak prior Bayesian, 72, 74
weather forecasting., 333
Weaver’s Surprise Index, 333
Weaver’s surprise index, 364, 365
web application, 561
Weight of Evidence, 276, 277
weight of evidence, 98, 197, 201,
205, 209, 213, 216, 232, 476,
481
weights of evidence, 451, 472, 473,
487
white noise, 385
WinBUGS, 313–315, 324
window, 384
word problems, 367
work product, 29, 30, 32, 35–37, 40,
107, 438, 440, 444, 446, 451,
458, 470, 472, 472, 478, 479,
488, 491, 529, 531, 538, 540,
554, 595
work products, 6, 10, 29, 32, 40,
444, 447, 498, 499, 509, 523,
524, 540, 544, 565
workaround, 251
Working Knowledge, 512, 513, 524,
526
working knowledge, 547, 563
XML, 540
zero, 270
zone of proximal development, 497

