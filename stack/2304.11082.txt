Preprint. Under review.
FUNDAMENTAL LIMITATIONS OF ALIGNMENT
IN LARGE LANGUAGE MODELS
Yotam Wolf∗
The Hebrew University
yotam.wolf@cs.huji.ac.il
Noam Wies∗
The Hebrew University
noam.wies@cs.huji.ac.il
Yoav Levine
AI21 Labs
yoavl@ai21.com
Amnon Shashua
The Hebrew University
shashua@cs.huji.ac.il
ABSTRACT
An important aspect in developing language models that interact with humans is
aligning their behavior to be useful and unharmful for their human users. This is
usually achieved by tuning the model in a way that enhances desired behaviors and
inhibits undesired ones, a process referred to as alignment. In this paper, we propose
a theoretical approach called Behavior Expectation Bounds (BEB) which allows us
to formally investigate several inherent characteristics and limitations of alignment
in large language models. Importantly, we prove that for any behavior that has
a ﬁnite probability of being exhibited by the model, there exist prompts that can
trigger the model into outputting this behavior, with probability that increases with
the length of the prompt. This implies that any alignment process that attenuates
undesired behavior but does not remove it altogether, is not safe against adversarial
prompting attacks. Furthermore, our framework hints at the mechanism by which
leading alignment approaches such as reinforcement learning from human feedback
increase the LLM’s proneness to being prompted into the undesired behaviors.
Moreover, we include the notion of personas in our BEB framework, and ﬁnd that
behaviors which are generally very unlikely to be exhibited by the model can be
brought to the front by prompting the model to behave as speciﬁc persona. This
theoretical result is being experimentally demonstrated in large scale by the so
called contemporary “chatGPT jailbreaks", where adversarial users trick the LLM
into breaking its alignment guardrails by triggering it into acting as a malicious
persona. Our results expose fundamental limitations in alignment of LLMs and
bring to the forefront the need to devise reliable mechanisms for ensuring AI safety.
1
INTRODUCTION
Training large language models (LLMs) over vast corpora has revolutionized natural language
processing, giving LLMs the ability to mimic human-like interactions and serve as general purpose
assistants in a wide variety of tasks, such as wide-scoped question answering, writing assistance,
teaching, and more (Radford et al., 2019; Devlin et al., 2019; Brown et al., 2020; Schulman et al.,
2023; OpenAI, 2023; Bubeck et al., 2023; Nori et al., 2023; West, 2023; Park et al., 2023). A growing
concern due to the increasing reliance on LLMs for such purposes is the harm they can cause their
users, such as feeding fake information (Lin et al., 2022; Weidinger et al., 2022), behaving offensively
and feeding social biases (Hutchinson et al., 2020; Venkit et al., 2022; Weidinger et al., 2022), or
encouraging problematic behaviors by users (even by psychologically manipulating them Roose
2023; Atillah 2023). Indeed, evidently, the unsupervised textual data used for pretraining modern
LLMs includes enough demonstrations of the above undesired behaviors for them to be present in the
resulting models (Bender et al., 2021). The act of removing these undesired behaviors is often called
alignment (Yudkowsky, 2001; Taylor et al., 2016; Amodei et al., 2016; Shalev-Shwartz et al., 2020;
Hendrycks et al., 2021; Pan et al., 2022; Ngo, 2022).
∗Equal contribution
1
arXiv:2304.11082v1  [cs.CL]  19 Apr 2023

Preprint. Under review.
There are several different approaches to performing alignment in LLMs. One is to include aligning
prompts: Askell et al. (2021) show that injecting language models with helpful, honest, and harmless
(HHH) textual prompts improves alignment and decreases toxicity. Similarly, Rae et al. (2021) also
use prompting in order to decrease toxicity. Another approach for LLM alignment is the procedure of
reinforcement learning from human feedback (RLHF) in order to train language models to be helpful
and harmless (Bai et al., 2022). The procedure is to further train a pretrained language model with
the assistance of a human evaluator in order to optimize its outputs to the evaluator’s preferences.
Their work shows an increase in an LLM’s HHH scores while maintaining its useful abilities, as
measured by zero- and few-shot performance on different natural language tasks. Another notable
work using this method is by Ouyang et al. (2022), which ﬁne tune GPT-3 into InstructGPT using
data collected from human labelers to reach better performance on a variety of tasks, while improving
HHH (measured via bias and toxicity datasets Gehman et al. 2020; Nangia et al. 2020).
While the above approaches to alignment are effective to a certain extent, they are still dangerously
brittle. For example, Wallace et al. (2019) show that short adversarial prompts can trigger negative
behaviors and social biases. Yu & Sagae (2021) and Xu et al. (2021) provide methods for exposing
harmful behaviors of models by triggering problematic responses. Subhash (2023) showed that
adversarial prompts can manipulate ChatGPT to alter user preferences. Beyond academic works,
the general media is abundant with contemporary examples of leading LLMs being manipulated by
users to expose harmful behaviors via the so called “jailbreaking" approach of prompting the LLM to
mimic a harmful persona (Nardo, 2023; Deshpande et al., 2023). Even in the absence of adversarial
attacks, leading alignment methods can underperform and are not well understood: Perez et al. (2022)
provide evidence that certain negative behaviors have inverse scaling with the number of RLHF steps,
indicating that this popular procedure may have a complex affect on LLM alignment.
In this paper, we introduce a probabilistic framework for analyzing alignment and its limitations
in LLMs, which we call Behavior Expectation Bounds (BEB), and use it in order to establish
fundamental properties of alignment in LLMs. The core idea behind BEB is to decompose the LLM
distribution into well-behaved components versus ill-behaved ones, in order to provide guarantees
on the ability to restrain the ill-behaved components, i.e., guarantees that the LLM is aligned. It
is noteworthy that LLMs have been shown to distinctly capture representations of behaviors and
personas implicitly (Andreas, 2022). Our framework assumes an underlying categorization into
different behaviors, where any natural language sentence is assigned a ground truth score between
−1 (very negative) and +1 (very positive) for every behavior (see examples in Figure 1). Such a
categorization can be, e.g., into the previously proposed helpful, honest, and harmless categories,
but it can also be expanded and ﬁne-grained into many more categories such as polite, not racist,
compassionate, and so on. Given such a categorization and ground truth sentence scoring functions
per category, the alignment score of any distribution over natural sentences w.r.t. a given behavior
is the expectation value of sentence scores for sentences drawn from the distribution. The BEB
framework thus provides a natural theoretical basis for describing the goal of alignment approaches
such as RLHF: increasing the behavior expectation scores for behaviors of interest.
The BEB framework employs assumptions on the distinguishability of the ill- and well-behaved
components within the overall LLM distribution. We present these assumptions and the BEB
framework in section 2, and use it in section 3 order to assert several important statements regading
LLM alignment:
• Alignment impossibility: We show that an LLM alignment process which reduces unde-
sired behaviors to a small but nonzero fraction of the probability space is not safe against
adversarial prompts.
Informal theorem: If the LLM has ﬁnite probability of exhibiting negative behavior, there
exists a prompt for which the LLM will exhibit negative behavior with probability 1.
• Conversation length guardrail: We show that by aligning an LLM and limiting the inter-
action length that users have with it, undesired behaviors can be avoided.
Informal theorem: The more aligned a model is to begin with, the longer the adversarial
prompt required to elicit undesired behaviors.
• RLHF can make things worse: While alignment tuning methods lower the probability of
undesired behaviors, they may also sharpen the distinction between desired and undesired
2

Preprint. Under review.
behaviors. We show that increased distinction can have the negative effect of rendering the
LLM more susceptible to adversarial prompting.
Informal theorem: The better the distinction between positive and negative behaviors, the
shorter the adversarial prompt required to elicit undesired behaviors.
This result may explain empirical ﬁnding in Perez et al. (2022), which show that certain
negative behaviors are more easily revealed when performing more RLHF steps.
• LLMs can resist misalignment during a conversation: We show that if a user attempts
to misalign an LLM during a conversation, the LLM can restore alignment during its
conversation turns.
Informal theorem: an adversarial user will need to insert more text in a conversation
scenario than in a single prompt scenario in order to misalign the LLM.
• A misaligned LLM will not realign easily: We show that if an LLM was misaligned, it
will remain so for conversation lengths shorter than the misaligning prompt.
Informal theorem: In order to realign a misaligned LLM, one must insert text of length that
is on the order of that of the misaligning prompt.
• Imitating personas can lead to easy alignment “jailbreaking": We show that it is always
possible to prompt a language model into behaving as a certain persona it has captured
during pretraining, and further show that this mechanism can be used in order to easily
access undesired behaviors.
Informal theorem: Mimicking personas that demonstrate bad behaviors can be more
efﬁcient than directly evoking the same bad behavior.
Overall, we hope that our newly proposed framework of Behavior Expectation Bounds, along with
our attained results, may spark a theoretical thrust helping to better understand the important topic of
LLM alignment.
2
BEHAVIOR EXPECTATION BOUNDS: A FRAMEWORK FOR ANALYZING LLM
ALIGNMENT
In this section, we introduce Behavior Expectation Bounds (BEB), a probabilistic framework for
studying alignment of LLMs. Given a language model’s probability distribution P, we propose a
measure for quantifying its tendency to produce desired outputs as measured by a certain behaviour
vertical B, where for example B can be helpfulness, honesty, harmlessness, politeness, or any other
behavior vertical of interest. Formally, we model behaviour scoring functions along vertical B as
B : Σ⋆→[−1, 1], which take a string of text from an alphabet Σ as their input and rate the manner
in which B manifests in the string, with +1 being very positive and −1 being very negative. For
clarity, see examples of the behavior scores of different sentences, along different behavior verticals,
in Figure 1.
We use the following expected behavior scoring of distribution P w.r.t. behavior vertical B as a scalar
quantifyer of the tendency of P to produce desired behavior along the B vertical:
BP := Es∼P[B(s)]
(1)
We will use the above distribution notation P to represent that of an unprompted LLM, e.g., an LLM
straight out of pretraining or out of an alignment tuning procedure such as RLHF. Indeed, the task
of aligning a pretrained LLM can be now framed as increasing its expected behavior scores along
behavior verticals of interest.
As an LLM is prompted with a preﬁx text string s∗, the behaviour of the conditional probability
P (· | s∗) might change. Thus, we will denote by BP (s∗) the behaviour of the language model when
prompted with a prompt text s∗:
BP(s∗) := Es∼P(·|s∗)[B(s)]
(2)
3

Preprint. Under review.
𝐵1 = 𝑝𝑜𝑙𝑖𝑡𝑒
𝐵2 = h𝑒𝑙𝑝𝑓𝑢𝑙
+1
−1
−1
+1
“For your safety, please 
avoid lighting fires near 
explosive materials.” 
“Are you an idiot? 
You have to see a 
doctor ASAP.” 
“I don’t care 
what you 
want.” 
“I kindly suggest 
that you stop trying 
to quit smoking.” 
Figure 1: Examples of sentence behavior scores along different behavior verticals. Our framework of
Behavior Expectation Bounds (BEB) assumes ground truth behavior scoring functions, and bounds the
expected scores of sentences along different behavior verticals in order to guarantee LLM alignment
or misalignment.
We will consider several scenarios where the preﬁx s∗plays different roles. The ﬁrst and main one is
that s∗serves as an adversarial input prompt. Secondly, we will consider a scenario in which s∗is
comprised of an initial aligning prompt, denoted s0, concatenated by a subsequent user adversarial
input prompt. Lastly, we will analyze conversation scenarios in which s∗is comprised of previous
turns of user queries and LLM responses.
2.1
USEFUL DECOMPOSITIONS
Our key ﬁnding in this paper is that an LLM which was initially aligned w.r.t. a certain behavior
vertical, i.e., BP very close to 1, can still be vulnerable to adversarial prompts, i.e., there exists a
prompt s∗such that BP(s∗) is very close to −1. In this subsection, we present a key aspect of our
BEB framework: decomposing the LLM distribution P into a mixture of distributions, each behaving
differently. Importantly, LLMs exhibit signs of capturing such decompositions implicitly in practice.
For example, Andreas (2022) shows empirical evidence that current LLMs can infer behaviours
from textual prompts, and that these behaviour affect the text that the LLM generates. We will use
decompositions inspired by such ﬁndings, and prove that textual prompts can reweight the prior of
the mixture components, and can speciﬁcally emphasize the contribution of ill-behaved components.
With this in mind, we present two useful decompositions, where the second is a reﬁnement of the
ﬁrst.
2.1.1
THE GOOD AND THE BAD
Observe that for any decomposition of a distribution P into two components, P = αP0 + (1 −α)P1,
the relation BP = αBP0 + (1 −α)BP1 holds from linearity of expectations, and implies that one
component is more well-behaved w.r.t. B than the full distribution and the other more ill-behaved,
i.e.: BP1 ≤BP ≤BP0 (or vice versa). For this reason, focusing on a speciﬁc behavior, we adopt the
notation:
P = αP−+ (1 −α)P+
(3)
in the two component decomposition, where P+ is the well-behaved component and P−is the
ill-behaved component.
While this observation is true for any decomposition to two distributions, we will give results for
decompositions in which the two distributions P−and P+ are sufﬁciently distinct (formally deﬁned
in section 2.2), and we are interested in decompositions where the negative component is strictly
ill-behaved (i.e, BP−≤γ < 0). In these cases, the magnitude of α, the prior of the ill-behaved
4

Preprint. Under review.
component, will determine the alignment of the LLM: an LLM with a small prior α will be less likely
to produce undesired sentences along behavior B vertical. Our main result in section 3 states that no
matter how small α is (how aligned the model is to begin with), if it is positive then there exists a
prompt that can misalign the LLM to behave like P−.
2.1.2
MULTIPLE PERSONAS
A natural extension of the above two components mixture, is a decomposition into more than two
components, P(s) = P
φ∈Φ wφPφ(s). Indeed, for any such decomposition, each component may
be more well-behaved than the full model BPφ ≥BP or more ill-behaved BPφ ≤BP, w.r.t. a given
behavior B. For a different behavior B′, some of these inequalities may be ﬂipped. We therefore
refer to different components Pφ as different “personas", as each component represents a different
mixture of behaviors. Still, the weighted sum of the components always gives that of the model
BP = P
φ∈Φ wφBPφ.
This is a more reﬁned decomposition from the two components and in fact can reproduce it: Any
partition of the persona into two sets deﬁnes a two component mixture. In particular, w.r.t. a behavior
B, for a−= {φ ∈Φ : BPφ < γ} and a+ = Φ\a−, the two terms P+ ∝P
φ∈a+ wφPφ and
P−∝P
φ∈a−wφPφ deﬁne the two component decompositon with the ill-behaved part satisfying
BP−< γ. In results section 3.3 we will use the above decomposition in order to shed light on the so
called “chatGPT jailbreak" attack on LLM alignment, in which the LLM is prompted into playing a
speciﬁc persona and as a side effect exhibits an undesired behavior (Nardo, 2023).
2.2
DEFINITIONS FOR BOUNDING THE EXPECTED LLM BEHAVIOR
In this subsection, we will formally deﬁne:
• Deﬁntion 1 – Behavior misalignment using prompts.
• Deﬁntion 2 – Distinguishability between two distributions that ﬁts a prompting scenario.
• Deﬁntion 3 – The distinguishibility between ill- and well-behaved components comprising a
certain LLM’s distribution.
• Deﬁntion 4 – Generalizing deﬁntion 2 for the case of analyzing “personas" (mixtures of
behaviors, as deﬁned in section 2.1.2) rather than behaviours.
• Deﬁntions 5 – Generalizing deﬁntion 3 for the case of analyzing “personas".
• Deﬁntion 6 – The amount of change in the LLM’s behavior due to its own responses
(required for analyzing a scenario of conversation between user and model rather than single
prompt).
Once an LLM has ﬁnished training, our only tool for altering its behavior is prompting. Using the
above deﬁnition for behavior expectation, we deﬁne the prompt-misalignment property of LLMs:
Deﬁnition 1. Let γ ∈[−1, 0), we say that an LLM with distribution P is γ-prompt-misalignable
with respect to behaviour B, if for any ϵ > 0 there exists a textual prompt s∗∈Σ⋆such that
BP (s∗) < γ + ϵ.
This means, that there exists a prompt that elicits bad behavior of extent γ ∈[−1, 0) from the model.
Decomposing a language model into parts that are well-behaved and ill-behaved exposes components
which are more desirable to enhance. The following notion of distinguishability will allow us to
guarantee that one component can be enhanced over the others.
Deﬁnition 2. We say that a distribution Pφ is β-distinguishable from distribution Pψ if for any
prompt s0:
Es∼Pφ(·|s0)

log Pφ (s | s0)
Pψ (s | s0)

> β
(4)
If Pφ is the ill-behaved component and Pψ is the well-behaved component, it means that the condi-
tional distributions always maintain a ﬁnite KL distance of β from each other.
5

Preprint. Under review.
The following deﬁnition formally quantiﬁes β-distinguishability between the ill- and well-behaved
components comprising the LLM distribution, parameterized by α in equation 3, and adds a condition
that the behavior expectation of the ill-behaved component is bad enough (under γ) for all initial
prompts s∗:
Deﬁnition 3. Let γ ∈[−1, 0), we say that a behaviour B : Σ⋆→[−1, 1] is α, β, γ-distinguishable
in the probability distribution P, if:
• There exists a mixture P = α · P−+ (1 −α) · P+ for α > 0;
• sups∗{BP−(s∗)} ≤γ;
• P−is β-distinguishable from P+ (deﬁnition 2).
This deﬁnition will allow us to ensure that a bad component can be enhanced over a good component
via prompting, and that its behavior given that prompt is still negative.
When looking at a decomposition of more than two components (so called-personas, presented
in section 2.1.2), we ask whether such a decomposition can be leveraged by an adversarial user
in order to evoke undesired behavior along a certain behavior vertical B. Contrary to the case
of two components, which is one-dimensional in the sense that enhancing one component with a
prompt reduces the other, the case of multiple components is multi-dimensional as we need to ﬁnd
a prompt that enhances one component over many others simultaneously. This does not amount to
one component being distinguishable from all the rest by deﬁnition 2, as it requires a concentration
inequality. We use a sub-Martingale assumption which enables to build a prompt Q composed of
several sentences q1 ⊕... ⊕qn, where each sentence qi further enhances one component over the rest:
Deﬁnition 4. We say that a distribution Pφ is β-Martingale-distinguishable from distribution Pψ if
for any series of sentences sn = s0 ⊕q1 ⊕.... ⊕qn, the induced series M φ,ψ
n
:= log Pφ(sn)
Pψ(sn) obeys:
Esn+1∼Pφ(·)[M φ,ψ
n+1|M φ,ψ
1
= m1, ..., M φ,ψ
n
= mn] > mn + β
(5)
Intuitively, if Pφ is an ill-behaved component and Pψ is a well-behaved one, this means that given a
sequence of sentences q1 ⊕.... ⊕qn as a prompt, when the next sentence qn+1 is sampled from the
ill-behaved component, it is likely to increase the KL distance from the well-behaved one. Notice
that this deﬁnition keeps memory of the history m1...mn, which is required for the sub-Martingale
assumption and is also reasonable that in a conversation Mn is affected by its history.
Given the above modiﬁed distinguishablity deﬁnition, we generalize deﬁnition 3 of behavior distin-
guishability within an LLM’s distribution to the setting of personas, as follows:
Deﬁnition 5. Let γ ∈[−1, 0), we say that a behavior B : Σ⋆→[−1, 1] is α, β, γ-distinguishable
in persona mixture P = P
φ∈Φ wφPφ, if for any ϵ > 0, there exists a persona ˜φ, that satisﬁes:
• w ˜φ ≥α;
• sups∗[BP ˜
φ(s∗)] < γ + ϵ;
• is β-Martingale-distinguishable (deﬁnition 4) from any persona φ.
This means that within a mixture of components, there exists one which is ill-behaved with respect to
a behavior B and is distinguishable from all the other components. We will show that this allows an
adversarial user to enhance a negative component until it dominates the conditional response of the
language model, and that evoking such a persona can be a good strategy for eliciting bad behavior
along the B vertical.
The above deﬁnitions ﬁt a setting of an adversarial prompt trying to misalign an LLM in a single turn.
In order to discuss multi-turn adversarial conversations between users and LLMs, and conversations
where an aligning prompt is inserted, we must consider that the LLM generated text may effectively
reinforce positive behavior, while the user is attempting to enhance negative behaviors by the model.
Formally, we bound the extent to which each sentence (whether by the user or the model) enhances
one component over the other:
6

Preprint. Under review.
Deﬁnition 6. Two distributions, Pφ, Pψ are c-similar if there exists c > 0 such that for any strings
s0 and s the following holds:
log Pψ(s|s0)
Pφ(s|s0)
 < c
(6)
This bounds the change between the positive and negative components at each time step, and will
allow us to bound the rate at which the negative behavior invoked by the user can be mitigated
by the aligned LLM responses. Note that by deﬁnition, c has to be larger than β, as the latter is
a lower bound while the former is an upper bound on the conditional KL-divergence between the
distributions.
3
RESULTS: LIMITATIONS OF LLM ALIGNMENT
In this section, we use the above framework of Behavior Expectation Bounds (BEB) in order to
elucidate the question of when LLM alignment is robust or vulnerable to adversarial prompting
attacks. We begin with our main result in section 3.1, which states that under assumptions of
decomposability into distinguishable components of desired and undesired behavior, aligned LLMs
are not protected against adversarial misaligning prompts. We show that on the one hand, the more
aligned the LLM is to begin with the longer the adversarial prompt required to misalign it, and on
the other, the more distinguishable the components the shorter the misaligning prompt. This last
results can shed light on why common RLHF tuning practices render aligned LLM more vulnerable
to misaligning prompts.
In section 3.2, we extend the above framework to include cases of (i) preset aligning prompts—we
ﬁnd that in this case the length of the misaligning prompt must be linear in the length of the preset
aligning prompt; and (ii) multi-turn interactions between adversarial users and LLMs—we ﬁnd that
if the user does not provide long enough misaligning prompts, the LLM can resist misalignnment
by making aligning replies to the user during a conversation. Finally, in section 3.3, we analyze the
case of decomposing the LLM ditribution into multiple components (“personas", or, mixtures of
behaviors, presented in section 2.1.2), and show that if a certain persona is distinctly captured during
the LLM pretraining, evoking it in order to elicit bad behavior from an aligned LLM can be more
efﬁcient than directly trying to elicit this behavior from the LLM. This corresponds to the recently
popularized “chatGPT jailbreaking" practice of misaligning an LLM via requesting it to mimic a
malicious persona.
3.1
MISALIGNING VIA ADVERSARIAL PROMPTS
Alignment impossibility
We start with a statement that if a model can be written as a distinct
mixture of a positive and negative components, P+ and P−w.r.t. behavior B, where the ﬁrst exhibits
a desired behavior more than the other, then it is possible to insert an initial prompt to the model, such
that its next answer will exhibit a behavior arbitrarily close to the negative component’s behavior.
Theorem 1. Let γ ∈[−1, 0) and let B be a behaviour and P be an unprompted language model
such that B is α, β, γ-distinguishable in P (deﬁnition 3), then P is γ-prompt-misalignable to B
(deﬁnition 1) with prompt length of O(log 1
ϵ , log 1
α, 1
β ).
Intuitively, theorem 1 implies that if a component of the distribution exhibits a negative behavior with
expectation under γ, then there exists a prompt that triggers this behavior for the entire language
model into behaving with expectation under γ. Importantly, no matter how low the prior of the
negative component α is, if it is distinguishable within the distribution then the LLM is vulnerable to
adversarial prompting that exposes this negative component’s behavior. We provide below a sketch
for the proof of theorem 1, fully detailed in the appendix:
Proof sketch (see full details in section A of the appendix). The assumption that B is α, β, γ-
distinguishable in P implies that P can be written as a mixture distribution of a misaligned component
P_ and an aligned component P+. Now, while the prior of P_ might be low and hence the behaviour of
the unprompted P is initially aligned with high probability, the fact that P_ is β-distinguishable from
P+ assures us that the conditional Kullback-Leibler divergence between P_ and P+ is greater than β
for any initial prompt s0. Therefore, we can use the chain rule and get that when sampling n successive
7

Preprint. Under review.
sentences, the Kullback-Leibler divergence between P_ and P+ is at least n · β. Consequently, we
show that for any n there exists a textual prompt s⋆consisting of n sentences, such that the likelihood
of s⋆according to P_ is exponentially (both in β and n) more likely than the likelihood of s⋆according
to P+. Finally, note that during the evaluation of the expected behavior scoring, such exponential
differences between the likelihood of s⋆according to the different mixture components reweight
theirs priors. We show that the contribution of P+ to the behaviour of the prompted LLM P is
negligible.
The above guaranteed prompt length dependence on α and β suggests two interesting practical
implications, detailed in the next two paragraphs.
Prompt length guardrail
Theorem 1 guarantees the existence of a misaligning prompt. The length
of this prompt increases if α, the prior of the ill-behaved component, is made smaller. This implies
that limiting the interaction length can be used as a measure of safety. Moreover, if the LLM is more
aligned to begin with (a lower prior α on the bad component), then longer interactions are possible
without an adversarial prompt existence guarantee.
Distinguishability shortens adversarial prompt length
The prior of the the ill-behaved compo-
nent is not the only factor affecting the misaligning prompt’s length. Theorem 1 showcases that if
the distinguishability between the ill-and well-behaved components, measured by β, is increased,
then the guaranteed length of the misaligning prompt is made shorter. This implies that even if
LLM-1 is more aligned than LLM-2 in the sense that it has a lower prior for the bad behavior,
α1 < α2, then the prompt for eliciting bad behavior from LLM-1 can still be shorter if the ill-behaved
component is distinguishable enough within it, i.e., if asymptotically β1
β2 > log( 1
α1 )/log( 1
α2 ). This
implies that aligning procedures that reduce the prior for undesired behavior but also make the ill-
and well-behaved components more distinguishable, may render the resulting LLM to be prone to
shorter more realistic adversarial attacks via prompting.
Conjecture on relation to RLHF
Leading alignment tuning practices such as RLHF train the
LLM to maximize the likelihood of desired sentences and minimizes the likelihood of undesired ones.
The following conjecture implies that the leading practice of RLHF can make the two components
more β-distinguishable (deﬁnition 2):
Conjecture 1. An alignment loss that increases the likelihood of desired sentences and minimizes
the likelihood of undesired ones, increases the β-distinguishability of resulting aligned LLM.
The intuition behind this conjecture is that alignment tuning induces separability between desired
and undesired behaviors in the LLM representation space, and thus the LLM can serve as a basis
for a better classiﬁer between desired and undesired sentences (as motivated for example by results
in Nachum & Yang (2021); Saunshi et al. (2021); Ge et al. (2023)). Now observe that with such an
improved classiﬁer, for any sentence s that is misclassiﬁed as good by the pretrained LLM but correctly
classiﬁed as bad after alignment tuning, PRLHF
−
(s) > Ppretraining
−
(s), while PRLHF
+
(s) < Ppretraining
+
(s).
Therefore, the contribution of this classiﬁcation change to the KL divergence is positive since:
∆KL = PRLHF
−
(s) · log PRLHF
−
PRLHF
+
−Ppretraining
−
(s) · log Ppretraining
−
Ppretraining
+
> 0
(7)
Thus, the existence of misclassiﬁed examples by classiﬁers over the LLM distribution out of pre-
rtraining, which can then be classiﬁed correctly by a classiﬁer over the distribution over an LLM
after RLHF, can ensure increased KL-divergence between the ill- and well- behaved components,
increasing their β-distinguishability.
Though intuitive, we leave this as an open conjecture for follow up work. If correct, while lowering
the prior of the ill-behaved component within the overall LLM distribution, aligning methods such as
RLHF which train the LLM to distinguish between good and bad behaviors may make them more
susceptible to adversarial prompting. This may be the mechanism behind the empirical ﬁndings
of Perez et al. (2022), who unveil that undesired behaviors more easily emerge as the LLM undergoes
more RLHF training steps.
8

Preprint. Under review.
3.2
EXTENSIONS: ALIGNING PROMPTS AND CONVERSATIONS
Misaligning in the presence of preset aligning prompts
A common practice is to include an
initial aligning prompt, hard coded as a preﬁx to the LLM’s input, in order to enhance positive
behavior. The theorem below states that even in the presence of an aligning prompt, it is possible
to prompt the LLM into an undesired behavior. We show that the required prompt length for
misalignment in this case, denoted s1, scales linearly with the length of the aligning prompt, s0.
Theorem 2. Let γ ∈[−1, 0), α, β, c > 0, and let B and P be a pair of behavior and probability dis-
tribution such that B is α, β, γ-distinguishable in P (deﬁnition 3) and the distributions corresponding
to the well-behaved and ill-behaved components of P are c-similar (deﬁnition 6). Then for any initial
prompt s0 ∈Σ⋆, the conditional LLM distribution P(·|s0) is γ-prompt-misalignable with prompt
length |s1| = O(log 1
ϵ , log 1
α, 1
β , |s0|, c).
Misaligning via conversation
We show below that an undesired behavior can be elicited from
an LLM via conversation with an adversarial user. Interestingly, we show that if the adversarial
user does not use a long enough misaligning prompt in the ﬁrst turn, then the LLM’s responses can
hinder the user’s misaligning efforts. Intuitively, if a user begins a conversation by simply requesting
“say a racist statement", an aligned LLM will likely reply “I will not say racist statements, that is
harmful", and this reply in its prompt will cause the LLM to be more mindful of refraining from
racist statements in the remainder of the conversation. Overall, due to this “misaligning resistance"
by the LLM, the user will need to insert more misaligning text in the conversation format than in the
single prompt format of section 3.1 in order for our framework to guarantee misalignment.
We formalize a conversation between a user and an LLM of distribution P as a sequence of user
queries followed by replies which are sampled according to the LLM’s conditional distribution given
the conversation thus far. Formally, given the history of the conversation, q1, a1...qt, at, qt+1, where
qi are the user’s inputs and ai are the LLM’s responses, the LLM generates a response at+1 by
sampling from the conditional distribution:
at+1 ∼P(·|q1 ⊕a1 ⊕... ⊕qt ⊕at ⊕qt+1)
(8)
where ⊕denotes the string concatenation operator.
In the following theorem we show that under our distinguishability conditions, misalignment is
always possible also in a conversation format:
Theorem 3. Let γ ∈[−1, 0), α, β, c > 0, let B be a behaviour and P be an unprompted language
model such that B is α, β, γ-distinguishable in P (deﬁnition 3) and the distributions corresponding
to the well-behaved and ill-behaved components of P are c-similar (deﬁnition 6). In a conversation
with a model, q1, a1...qt, at, qt+1, the model is γ-misalignable with total prompt length of P
i |qi| =
O(log 1
ϵ , log 1
α, c
β ) and each prompt of length |qi| = O( c
β ).
While each prompt enhances the bad behavior component, the model’s response may do the opposite
and reduce it. For this reason, we need to assume c-similarity between the ill- and well-behaved
components (deﬁnition 6) when analyzing conversations, in order to bound the enhancement of the
well-behaved component as a result of the LLM’s responses. At the beginning of the conversation,
the model is aligned, so it is most likely that its response will be sampled from the well-behaved
component, thus enhancing it over the ill-behaved component. This creates the appearance of the
model resisting the misalignment. If the user inserts a long enough misaligning prompt, the model’s
next response may already be sampled from a misaligned distribution, thus the response is more
likely to be sampled from the bad behavior component, further enhancing it and contributing to the
misalignment. Overall, we show that the dynamics of misaligning during a conversation are more
elaborate than in the single prompting case, and may result in harder misaligning efforts.
Conversing with a misaligned LLM
We show that once an LLM has been misaligned via an
adversarial prompt, it will exhibit negative behavior long into the remainder of the conversation:
Proposition 1. Let γ ∈[−1, 0), α, β, c > 0, let B be a behaviour and P be an unprompted language
model such that B is α, β, γ-distinguishable in P (deﬁnition 3) and the distributions corresponding
to the well-behaved and ill-behaved components of P are c-similar (deﬁnition 6). Suppose the model
has been misaligned with a prompt s0, such that BP(s0) ≤γ. For the remainder of the conversation,
9

Preprint. Under review.
a1 ⊕q1 ⊕... ⊕an−1 ⊕qn, it will remain misaligned:
BP(s0 ⊕a1 ⊕q1 ⊕... ⊕an−1 ⊕qn) < γ
2
(9)
Unless Pn
i=1 |qi| + |ai| = Ω(|s0|).
Intuitively, after the LLM has been misaligned by s0, the model’s responses in the following conver-
sation maintain the negative behavior (more negative than γ/2 < 0), unless the conversation exceeds
a certain length that scales linearly with the length of the misaligning prompt.
3.3
IMITATING PERSONAS AS A “JAILBREAK" FOR LLM ALIGNMENT
Recent ﬁndings show that LLMs can be misaligned via a mechanism of prompting the LLM into
behaving as a persona it has clearly captured during the pretraining phase (Nardo, 2023). In this
subsection, we use our deﬁnition of “persona", presented in section 2.1.2, in order to show that this
adversarial misaligning strategy can be more efﬁcient than directly attempting to elicit the undesired
behavior.
We ﬁrst prove that if a distribution can be written as a mixture of personas, P = P
φ∈Φ wφPφ and
there exists a persona that is ill-behaved BPφ ≤γ and is β-distinguishable from all other personas,
then there exists a prompt which causes the LLM’s conditional behavior to resemble the ill-behaved
persona’s behavior:
Theorem 4. Let γ ∈[−1, 0), α, β, ϵ > 0, and let P be a mixture of personas, that is c-similar,
P = P
φ∈Φ wφPφ. Then for every behavior B : Σ⋆→[−1, 1] that is α, β, γ-distinguishable in
persona mixture P (deﬁnition 5), the distribution P is γ-prompt-misalignable (deﬁnition 1) with
prompt length |s| = O(log 1
ϵ , 1
β , log 1
α, c2, log |Φ|).
We ﬁnd that the length of a prompt required to get a LLM to imitate a persona of interest scales
inversely with the distinguishability, β and logarithmically with the persona’s prior in the distribution,
w: |s| = O( log 1
w
β
). The prior dependence is due to the O(log 1
α) dependence in the theorem, as
α ≤wφ for the ill-behaved persona φ (see deﬁnition 5).
Imitation of personas for "Jailbreaking"
The consequence of the above theorem is that personas
that have low priors w, may compensate for this with high distinguishability β, such that in some
cases, prompting the model for a low-weight high-distinguishability persona may be more efﬁcient at
triggering bad behavior than a high-weight low-distinguishability bad component. This is expected to
happen if a persona is very well captured by the LLM during pretraining.
Corollary 1. Let γ ∈[−1, 0), α, β, c > 0, let P be a mixture of personas, that is c-similar, P =
P
φ∈Φ wφPφ and B : Σ⋆→[−1, 1] a behavior that is α, β, γ-distinguishable in persona mixture P.
If during training the distinguishability of ill-behaved personas scales super-logarithmically relative
to their priors, β = Ω(log( 1
w)), invoking a persona for bad behavior requires prompts that are
asymptotically shorter than ones for invoking a general bad behavior.
Thus, in cases where an LLM captures a toxic persona very well during pretraining, it can be more
efﬁcient to prompt the LLM to imitate it rather than enhancing the ill-behaved component directly.
4
DISCUSSION
The need for robust methods for AI alignment is pressing. Prominent actors in our ﬁeld are ad-
vocating for halting LLM development until the means of controlling this technology are better
understood (O’Brien, 2023). This paper brings forward the Behavior Expectation Bounds (BEB)
theoretical framework, which is aimed at providing means for discussing core alignment issues in
leading contemporary interactions between humans and LLMs.
We used the BEB framework in order to make several fundamental assertions regarding alignment
in LLMs. First, we showed that any realistic alignment process can be reversed via an adversarial
prompt or conversation with an adversarial user. As a silver lining, we showed that the better aligned
10

Preprint. Under review.
the model is to begin with, the longer the prompt required to reverse the alignment, so limited prompt
lengths may serve as guardrails in theory. With that, we also show that this picture is more complex,
and the distinguishability of undesired behavior components also facilitates easier misalignment.
Thus, while attenuating undesired behaviors, the leading alignment practice of reinforcement learning
from human feedback (RLHF) may also render these same undesired behaviors more easily accessible
via adversarial prompts. This theoretical direction may explain the result in Perez et al. (2022), in
which RLHF increases undesired behaviors in language models.
Our BEB framework allowed us to make several further statements regarding different aspects of
LLM alignment, e.g., guaranteeing that a misaligned LLM will remain misaligned for a certain
duration of conversation, showing that the practice of misaligning via a multi-turn conversation an
LLM is more intricate and can be less efﬁcient than misaligning via a single prompt (due to the
aligned LLM “resisting" misalignment), and showing that invoking a well captured malicious persona
can be an efﬁcient “jailbreak" out of alignment.
Our framework has several limitations and we leave several issues open for future work. Andreas
(2022) describe modern LLMs as comprised of distinct agents that manifest when the right prompt is
inserted into the LLM. Our presented notions of decomposability into components and distinguisha-
bility between these components are one simple, analyzable choice of modeling multiple agents
or personas composing the LLM distribution. We showed that with this choice several theoretical
statements can be made that ﬁt empirical observations on misalignment via prompting. We leave
it to future work to (i) empirically reinforce or weaken the likelihood that our assumptions are in
fact plausible for actual LLM distributions of interest and (ii) make more elaborate or more realistic
assumptions on the manner in which agent or persona decomposition is manifested in actual LLM
distributions, and use them to gain further theoretical insight on LLM alignment.
Furthermore, our framework assumes ground truth behavior scores per sentence, where in reality
behavior scoring is more complex, e.g., over varying text granularities, hard to deﬁne behavior
verticals, and ambiguous scoring. A deeper linguistic deﬁnition of the behavior scoring setup may
lead to new insights that can be drawn from the BEB theoretical framework. Overall we hope that
our presented theoretical framework for analyzing LLM alignment can serve as a basis for further
advancement in understanding this important topic.
ACKNOWLEDGMENTS
We are thankful for insightful conversations and comments by Oshri Avnery. This research was
supported by the ERC (European Research Council) and the ISF (Israel Science Foundation).
REFERENCES
Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Mané.
Concrete problems in ai safety. arXiv preprint arXiv:1606.06565, 2016.
Jacob Andreas. Language models as agent models. In Findings of the Association for Computational
Linguistics: EMNLP 2022, pp. 5769–5779, Abu Dhabi, United Arab Emirates, December 2022.
Association for Computational Linguistics. URL https://aclanthology.org/2022.
findings-emnlp.423.
Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy Jones,
Nicholas Joseph, Ben Mann, Nova DasSarma, et al. A general language assistant as a laboratory
for alignment. arXiv preprint arXiv:2112.00861, 2021.
Imane El Atillah. Man ends his life after an ai chatbot ’encouraged’ him to sacriﬁce himself to stop
climate change. Euronews, 2023.
Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain,
Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training a helpful and harmless assistant with
reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022.
Emily M Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. On the
dangers of stochastic parrots: Can language models be too big? In Proceedings of the 2021 ACM
conference on fairness, accountability, and transparency, pp. 610–623, 2021.
11

Preprint. Under review.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are
few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020.
Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar,
Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of artiﬁcial general intelligence:
Early experiments with gpt-4. arXiv preprint arXiv:2303.12712, 2023.
Ameet Deshpande, Vishvak Murahari, Tanmay Rajpurohit, Ashwin Kalyan, and Karthik
Narasimhan. Toxicity in chatgpt: Analyzing persona-assigned language models. arXiv preprint
arXiv:2304.05335, 2023.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep
bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of
the North American Chapter of the Association for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers), pp. 4171–4186, Minneapolis, Minnesota, June
2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423. URL https:
//aclanthology.org/N19-1423.
Jiawei Ge, Shange Tang, Jianqing Fan, and Chi Jin. On the provable advantage of unsupervised
pretraining. arXiv preprint arXiv:2303.01566, 2023.
Samuel Gehman, Suchin Gururangan, Maarten Sap, Yejin Choi, and Noah A. Smith. RealTox-
icityPrompts: Evaluating neural toxic degeneration in language models. In Findings of the
Association for Computational Linguistics: EMNLP 2020, pp. 3356–3369, Online, November
2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.ﬁndings-emnlp.301.
URL https://aclanthology.org/2020.findings-emnlp.301.
Dan Hendrycks, Nicholas Carlini, John Schulman, and Jacob Steinhardt. Unsolved problems in ml
safety. arXiv preprint arXiv:2109.13916, 2021.
Ben Hutchinson, Vinodkumar Prabhakaran, Emily Denton, Kellie Webster, Yu Zhong, and Stephen
Denuyl. Social biases in NLP models as barriers for persons with disabilities. In Proceedings of
the 58th Annual Meeting of the Association for Computational Linguistics, pp. 5491–5501, Online,
July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.487. URL
https://aclanthology.org/2020.acl-main.487.
Stephanie Lin, Jacob Hilton, and Owain Evans. TruthfulQA: Measuring how models mimic hu-
man falsehoods. In Proceedings of the 60th Annual Meeting of the Association for Computa-
tional Linguistics (Volume 1: Long Papers), pp. 3214–3252, Dublin, Ireland, May 2022. As-
sociation for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.229. URL https:
//aclanthology.org/2022.acl-long.229.
Oﬁr Nachum and Mengjiao Yang. Provable representation learning for imitation with contrastive
fourier features. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan
(eds.), Advances in Neural Information Processing Systems, volume 34, pp. 30100–30112. Cur-
ran Associates, Inc., 2021. URL https://proceedings.neurips.cc/paper_files/
paper/2021/file/fd00d3474e495e7b6d5f9f575b2d7ec4-Paper.pdf.
Nikita Nangia, Clara Vania, Rasika Bhalerao, and Samuel R. Bowman. CrowS-pairs: A challenge
dataset for measuring social biases in masked language models. In Proceedings of the 2020
Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 1953–1967,
Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.
emnlp-main.154. URL https://aclanthology.org/2020.emnlp-main.154.
Cleo Nardo. The waluigi effect (mega-post). Less Wrong, 2023.
Richard Ngo.
The alignment problem from a deep learning perspective.
arXiv preprint
arXiv:2209.00626, 2022.
Harsha Nori, Nicholas King, Scott Mayer McKinney, Dean Carignan, and Eric Horvitz. Capabilities
of gpt-4 on medical challenge problems. arXiv preprint arXiv:2303.13375, 2023.
12

Preprint. Under review.
Matt O’Brien. Musk, scientists call for halt to ai race sparked by chatgpt. AP News, 2023.
OpenAI. Gpt-4 technical report, 2023.
Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong
Zhang, Sandhini Agarwal, Katarina Slama, Alex Gray, John Schulman, Jacob Hilton, Fraser Kelton,
Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and
Ryan Lowe. Training language models to follow instructions with human feedback. In Alice H. Oh,
Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Advances in Neural Information
Processing Systems, 2022. URL https://openreview.net/forum?id=TG8KACxEON.
Alexander Pan, Kush Bhatia, and Jacob Steinhardt. The effects of reward misspeciﬁcation: Mapping
and mitigating misaligned models. In International Conference on Learning Representations, 2022.
URL https://openreview.net/forum?id=JYtwGwIL7ye.
Joon Sung Park, Joseph C O’Brien, Carrie J Cai, Meredith Ringel Morris, Percy Liang, and
Michael S Bernstein. Generative agents: Interactive simulacra of human behavior. arXiv preprint
arXiv:2304.03442, 2023.
Ethan Perez, Sam Ringer, Kamil˙e Lukoši¯ut˙e, Karina Nguyen, Edwin Chen, Scott Heiner, Craig Pettit,
Catherine Olsson, Sandipan Kundu, Saurav Kadavath, et al. Discovering language model behaviors
with model-written evaluations. arXiv preprint arXiv:2212.09251, 2022.
Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language
models are unsupervised multitask learners. 2019.
Jack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John
Aslanides, Sarah Henderson, Roman Ring, Susannah Young, et al. Scaling language models:
Methods, analysis & insights from training gopher. arXiv preprint arXiv:2112.11446, 2021.
Kevin Roose. A conversation with bing’s chatbot left me deeply unsettled. New York Times, 2023.
Nikunj Saunshi, Sadhika Malladi, and Sanjeev Arora. A mathematical exploration of why language
models help solve downstream tasks. In International Conference on Learning Representations,
2021. URL https://openreview.net/forum?id=vVjIW3sEc1s.
John Schulman, Barret Zoph, Christina Kim, Jacob Hilton, Jacob Menick, Jiayi Weng, Juan Felipe,
Ceron Uribe, Liam Fedus, Luke Metz, Michael Pokorny, Rapha Gontijo Lopes, Shengjia Zhao,
Arun Vijayvergiya, Eric Sigler, Adam Perelman, Chelsea Voss, Mike Heaton, Joel Parish, Dave
Cummings, Rajeev Nayak, Valerie Balcom, David Schnurr, Tomer Kaftan, Chris Hallacy, Nicholas
Turley, Noah Deutsch, Vik Goel, Jonathan Ward, Aris Konstantinidis, Wojciech Zaremba, Long
Ouyang, Leonard Bogdonoff, Joshua Gross, David Medina, Sarah Yoo, Teddy Lee, Ryan Lowe,
Dan Mossing, Joost Huizinga, Roger Jiang, Carroll Wainwright, Diogo Almeida, Steph Lin,
Marvin Zhang, Kai Xiao, Katarina Slama, Steven Bills, Alex Gray, Jan Leike, Jakub Pachocki, Phil
Tillet, Shantanu Jain, Greg Brockman, Nick Ryder, Alex Paino, Qiming Yuan, Clemens Winter,
Ben Wang, Mo Bavarian, Igor Babuschkin, Szymon Sidor, Ingmar Kanitscheider, Mikhail Pavlov,
Matthias Plappert, Nik Tezak, Heewoo Jun, William Zhuk, Vitchyr Pong, Lukasz Kaiser, Jerry
Tworek, Andrew Carr, Lilian Weng, Sandhini Agarwal, Karl Cobbe, Vineet Kosaraju, Alethea
Power, Stanislas Polu, Jesse Han, Raul Puri, Shawn Jain, Benjamin Chess, Christian Gibson,
Oleg Boiko, Emy Parparita, Amin Tootoonchian, Kyle Kosic, and Christopher Hesse. Introducing
chatgpt. OpenAI blog, 2023.
Shai Shalev-Shwartz, Shaked Shammah, and Amnon Shashua. On the ethics of building ai in a
responsible manner. arXiv preprint arXiv:2004.04644, 2020.
Varshini Subhash. Can large language models change user preference adversarially? arXiv preprint
arXiv:2302.10291, 2023.
Jessica Taylor, Eliezer Yudkowsky, Patrick LaVictoire, and Andrew Critch. Alignment for advanced
machine learning systems. Ethics of Artiﬁcial Intelligence, pp. 342–382, 2016.
13

Preprint. Under review.
Pranav Narayanan Venkit, Mukund Srinath, and Shomir Wilson.
A study of implicit bias in
pretrained language models against people with disabilities.
In Proceedings of the 29th In-
ternational Conference on Computational Linguistics, pp. 1324–1332, Gyeongju, Republic of
Korea, October 2022. International Committee on Computational Linguistics. URL https:
//aclanthology.org/2022.coling-1.113.
Eric Wallace, Shi Feng, Nikhil Kandpal, Matt Gardner, and Sameer Singh. Universal adversarial
triggers for attacking and analyzing NLP. In Proceedings of the 2019 Conference on Empirical
Methods in Natural Language Processing and the 9th International Joint Conference on Natural
Language Processing (EMNLP-IJCNLP), pp. 2153–2162, Hong Kong, China, November 2019.
Association for Computational Linguistics. doi: 10.18653/v1/D19-1221.
URL https://
aclanthology.org/D19-1221.
Laura Weidinger, Jonathan Uesato, Maribeth Rauh, Conor Grifﬁn, Po-Sen Huang, John Mellor,
Amelia Glaese, Myra Cheng, Borja Balle, Atoosa Kasirzadeh, Courtney Biles, Sasha Brown,
Zac Kenton, Will Hawkins, Tom Stepleton, Abeba Birhane, Lisa Anne Hendricks, Laura Rimell,
William Isaac, Julia Haas, Sean Legassick, Geoffrey Irving, and Iason Gabriel. Taxonomy of
risks posed by language models. In 2022 ACM Conference on Fairness, Accountability, and
Transparency, FAccT ’22, pp. 214–229, New York, NY, USA, 2022. Association for Computing
Machinery. ISBN 9781450393522. doi: 10.1145/3531146.3533088. URL https://doi.org/
10.1145/3531146.3533088.
Colin G West. Advances in apparent conceptual physics reasoning in gpt-4. arXiv e-prints, pp.
arXiv–2303, 2023.
Jing Xu, Da Ju, Margaret Li, Y-Lan Boureau, Jason Weston, and Emily Dinan. Bot-adversarial dia-
logue for safe conversational agents. In Proceedings of the 2021 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language Technologies, pp.
2950–2968, Online, June 2021. Association for Computational Linguistics. doi: 10.18653/v1/
2021.naacl-main.235. URL https://aclanthology.org/2021.naacl-main.235.
Dian Yu and Kenji Sagae.
Automatically exposing problems with neural dialog models.
In
Proceedings of the 2021 Conference on Empirical Methods in Natural Language Process-
ing, pp. 456–470, Online and Punta Cana, Dominican Republic, November 2021. Associa-
tion for Computational Linguistics.
doi: 10.18653/v1/2021.emnlp-main.37.
URL https:
//aclanthology.org/2021.emnlp-main.37.
Eliezer Yudkowsky. Creating friendly ai 1.0: The analysis and design of benevolent goal architectures.
The Singularity Institute, San Francisco, USA, 2001.
14

Preprint. Under review.
A
PROOF OF THEOREMS 1 AND 2
Theorem 1 is technically a special case of Theorem 2 which includes the case of an initial prompt
s0 before the misaligning prompt. Below we detail the proof of theorem 2, which entails that of
theorem 1:
For convenience, we use B′ = −B, γ′ = −γ, in order to work with positive values. We start by
writing the behavior expectation value given the conversation history s0 and the new prompt s1:
B′
P(s0 ⊕s1) = Es∼P(·|s0⊕s1)[B′(s)] =
X
s
αP−(s0 ⊕s1 ⊕s) + (1 −α)P+(s0 ⊕s1 ⊕s)
αP−(s0 ⊕s1) + (1 −α)P+(s0 ⊕s1)
B′(s) >
The last transition is from the two component mixture assured by the α, β, γ-distinguishability.
Taking only the P−term in the numerator and rearranging the numerator and denominator:
>
X
s
αP−(s0 ⊕s1 ⊕s)
αP−(s0 ⊕s1) + (1 −α)P+(s0 ⊕s1)B′(s) =
X
s
αP−(s0 ⊕s1 ⊕s)
αP−(s0 ⊕s1)(1 + (1−α)P+(s0⊕s1)
αP−(s0⊕s1)
)
B′(s) =
=
X
s
P−(s|s0 ⊕s1)
(1 + (1−α)P+(s0⊕s1)
αP−(s0⊕s1)
)
B′(s)
If we were to bound (1−α)P+(s0⊕s1)
αP−(s0⊕s1)
< ϵ′, we would get:
>
X
s
P−(s|s0 ⊕s1)
(1 + ϵ′)
B′(s) = Es∼P−(·|s0⊕s1)[B′(s)]
1 + ϵ′
=
B′
P−(s0 ⊕s1)
1 + ϵ′
>
γ′
1 + ϵ′ > γ′(1 −ϵ′)
In the second to last transition we used B′
P−(s0 ⊕s1) > γ′ which is assured from the α, β, γ-
distinguishability and the last transition is Bernouli’s inequality. Taking ϵ′ < ϵ/γ′ gives:
B′
P(s0 ⊕s1) > γ′ −ϵ
If we multiply this inequality by −1 and map back B = −B′ and γ = −γ′, we get:
BP(s0 ⊕s1) < γ + ϵ
As required.
We are left with showing the bound (1−α)P+(s0⊕s1)
αP−(s0⊕s1)
< ϵ′. If we think of s1 as a sequence of sentences,
s1 = q1 ⊕q2 ⊕... ⊕qn, then:
log P−(s0 ⊕s1)
P+(s0 ⊕s1) = log P−(s0 ⊕q1 ⊕q2 ⊕... ⊕qn)
P+(s0 ⊕q1 ⊕q2 ⊕... ⊕qn) = Mn
For n = 1, given the β-distinguishability from the use’s prompts, we get:
Eq1∼P−(·|s0)[log P−(q|s0)
P+(q|s0)] > β
Where:
Eq1∼P−(·|s0)[log P−(q|s0)
P+(q|s0)] = Eq1∼P−(·|s0)[M1] −M0
Thus, in particular there exists q1 that satisﬁes the condition:
M1 > M0 + β
For some k ≤n, given the β-distinguishability from qk, we get:
Eqk∼P−(·|s0⊕q1⊕...⊕qk−1)[log P−(q|s0 ⊕q1 ⊕... ⊕qk−1)
P+(q|s0 ⊕q1 ⊕... ⊕qk−1)] > β
Where
Eqk∼P−(·|s0⊕q1⊕...⊕qk−1)[log P−(q|s0 ⊕q1 ⊕... ⊕qk−1)
P+(q|s0 ⊕q1 ⊕... ⊕qk−1)] = Eqk∼P−(·|s0⊕q1⊕...⊕qk−1)[Mk]−Mk−1
15

Preprint. Under review.
Thus, in particular there exists qk that satisﬁes the condition:
Mk > Mk−1 + β
Continuing this to k = n, given the choices for q1, ..., qn, we get in total:
Mn > n ∗β + M0
We are left with bounding M0. First, note that for the unprompted case (theorem 1), s0 is empty, so
there is nothing to bound. For the prompted case, we bound M0 with c-similarity. The proof is given
in the next appendix as a Lemma, M0 > −m∗c where m = |s0|. Choose n >
m∗c+log( 1−α
α
)+log( 1
ϵ′ )
β
and get (1−α)P+(s0⊕s1)
αP−(s0⊕s1)
< ϵ′ as desired. Indeed |s1| = n is O( 1
β , |s0|, log 1
α, log 1
ϵ , c).
B
LEMMA (BOUND FOR M0 WITH c-SIMILARITY)
Denote the conversation history s0 = a1 ⊕... ⊕am. Let Pφ and P ˜φ be c-similar distributions. Notice
that:
M0 = log Pφ(a1 ⊕... ⊕am)
P ˜φ(a1 ⊕... ⊕am) =
X
i
log Pφ(ai|a1 ⊕... ⊕ai−1)
P ˜φ(ai|a1 ⊕... ⊕ai−1) > −m ∗c
The last transition is from c-similarity. Proof of the third transition with induction:
• Base for induction m′ = 2, follows from conditional probability of two variables:
log Pφ(a1 ⊕a2)
P ˜φ(a1 ⊕a2) = log Pφ(a2|a1)
P ˜φ(a2|a1) + log Pφ(a1)
P ˜φ(a1)
• Induction step: assume m′ −1 holds true, now for m′:
log Pφ(a1 ⊕... ⊕am′)
P ˜φ(a1 ⊕... ⊕am′) = log Pφ(am′|a1 ⊕... ⊕am′−1)
P ˜φ(am′|a1 ⊕... ⊕am′−1) + log Pφ(a1 ⊕... ⊕am′−1)
P ˜φ(a1 ⊕... ⊕am′−1) =
From the induction base for m −1:
= log Pφ(am′|a1 ⊕... ⊕am′−1)
P ˜φ(am′|a1 ⊕... ⊕am′−1) +
m′−1
X
i=1
log Pφ(ai|a1 ⊕... ⊕ai−1)
P ˜φ(ai|a1 ⊕... ⊕ai−1) =
=
m′
X
i=1
log Pφ(ai|a1 ⊕... ⊕ai−1)
P ˜φ(ai|a1 ⊕... ⊕ai−1)
As desired.
C
PROOF OF THEOREM 3
The proof is similar to theorem 2, but here there are two types of steps, one in which the user inserts
a prompt qi that increases the distinguishability by β ∗|qi| and a second in which the model responds
ai, possibly resisting the bad behavior prompting, worst case decreasing the distinguishability by
−c. Following the exact same same steps as in theorem 2, we use B′ = −B, γ′ = −γ, in order to
work with positive values. We start by writing the behavior expectation value given the conversation
history s0:
B′
P(s0) = Es∼P(·|s0)[B′(s)] =
X
s
αP−(s0 ⊕s) + (1 −α)P+(s0 ⊕s)
αP−(s0) + (1 −α)P+(s0)
B′(s) >
The last transition is from the two component mixture assured by the α, β, γ-distinguishability.
Taking only the P−term in the numerator and rearranging the numerator and denominator:
>
X
s
αP−(s0 ⊕s)
αP−(s0) + (1 −α)P+(s0)B′(s) =
X
s
αP−(s0 ⊕s)
αP−(s0)(1 + (1−α)P+(s0)
αP−(s0)
)
B′(s) =
16

Preprint. Under review.
=
X
s
P−(s|s0)
(1 + (1−α)P+(s0)
αP−(s0)
)
B′(s)
If we were to bound (1−α)P+(s0)
αP−(s0)
< ϵ′, we would get:
>
X
s
P−(s|s0)
(1 + ϵ′) B′(s) = Es∼P−(·|s0)[B′(s)]
1 + ϵ′
=
B′
P−(s0)
1 + ϵ′
>
γ′
1 + ϵ′ > γ′(1 −ϵ′)
In the second to last transition we used B′
P−(s0) > γ′ which is assured from the α, β, γ-
distinguishability and the last transition is Bernouli’s inequality. Taking ϵ′ < ϵ/γ′ gives:
B′
P(s0) > γ′ −ϵ
If we multiply this inequality by −1 and map back B = −B′ and γ = −γ′, we get:
BP(s0) < γ + ϵ
As required.
Thus again we are left with showing the bound (1−α)P+(s0)
αP−(s0)
< ϵ′, but this time s0 is a series of user
prompts and model responses s0 = q1 ⊕a1 ⊕... ⊕qn ⊕an. We will prove (∗) as a lemma in the
next appendix:
log P−(s0)
P+(s0) = log P−(q1 ⊕a1 ⊕... ⊕qn ⊕an)
P+(q1 ⊕a1 ⊕... ⊕qn ⊕an) >(∗)
n
X
i=1
(β|qi| −c)
If this holds, then by choosing |qi| > c
β + 1, we obtain:
log P−(s0)
P+(s0) >
n
X
i=1
β = n ∗β
Taking n > log 1
ϵ +log 1−α
α
β
, we obtain:
(1 −α)P+(s0)
αP−(s0)
< ϵ
As desired. Indeed, the sum of prompt lengths is Pn
i=1 |qi| = O( c
β , log 1
ϵ , log 1
α) and each prompt is
of length |qi| = O( c
β ).
D
LEMMA (BOUND FOR COMPONENT PROBABILITY RATIO IN
CONVERSATION)
Here we will prove the following inequality, for P+, P−which are β-distinguishable and c-similar.
There exists a choice of prompts q1...qn such that:
log P−(q1 ⊕a1 ⊕... ⊕qn ⊕an)
P+(q1 ⊕a1 ⊕... ⊕qn ⊕an) >
n
X
i=1
(β|qi| −c)
We do this by induction.
• Base of induction:
log P−(q1 ⊕a1)
P+(q1 ⊕a1) = log P−(a1|q1)
P+(a1|q1) + log P−(q1)
P+(q1) > −c + log P−(q1)
P+(q1)
The ﬁrst transition is from conditional probability, the second transition is from the c-
similarity. Next, we need to construct an adversarial prompt q1 that satisﬁes log P−(q1)
P+(q1) >
β · |q1|.
It is constructed sentence by sentence, q1 = s1 ⊕... ⊕s|q1|.
From β-
distinguishability, Es1∼P−(·)[log P−(s1)
P+(s1)] > β, thus, in particular there exists a sen-
tence s1 that satisﬁes log P−(s1)
P+(s1) > β. For some k ≤|q1|, from β-distinguishability,
17

Preprint. Under review.
Esk∼P−(·|s1⊕...⊕sk−1)[log P−(sk|s1⊕...⊕sk−1)
P+(sk|s1⊕...⊕sk−1)] > β, thus, in particular there exists a sen-
tence sk that satisﬁes log P−(sk|s1⊕...⊕sk−1)
P+(sk|s1⊕...⊕sk−1)
> β.
Such that in total, log P−(q1)
P+(q1)
=
P|q1|
i=1 log P−(si|s1⊕...⊕si−1)
P+(si|s1⊕...⊕si−1) > |q1| · β as desired. Using this on the previous inequality, we
get:
log P−(q1 ⊕a1)
P+(q1 ⊕a1) > −c + log P−(q1)
P+(q1) > β · |qi| −c
• Assume that the inequality holds for k −1, let us prove for k ≤n:
log P−(q1 ⊕a1 ⊕... ⊕qk ⊕ak)
P+(q1 ⊕a1 ⊕... ⊕qk ⊕ak) = log P−(ak|q1 ⊕a1 ⊕... ⊕qk)
P+(ak|q1 ⊕a1 ⊕... ⊕qk)+log P−(q1 ⊕a1 ⊕... ⊕qk)
P+(q1 ⊕a1 ⊕... ⊕qk)
From c-similarity on the ﬁrst term:
> −c + log P−(q1 ⊕a1 ⊕... ⊕qk)
P+(q1 ⊕a1 ⊕... ⊕qk)
= −c + log P−(qk|q1 ⊕a1 ⊕... ⊕ak−1)
P+(qk|q1 ⊕a1 ⊕... ⊕ak−1) + log P−(q1 ⊕a1 ⊕... ⊕ak−1)
P+(q1 ⊕a1 ⊕... ⊕ak−1)
Next, we construct an adversarial prompt qk such that
log P−(qk|q1 ⊕a1 ⊕... ⊕ak−1)
P+(qk|q1 ⊕a1 ⊕... ⊕ak−1) > β · |qk|
The idea is the same as for the base case of the induction, but for a rigorous proof, view next
appendix. Using this on the previous inequality, we get:
log P−(q1 ⊕a1 ⊕... ⊕qk ⊕ak)
P+(q1 ⊕a1 ⊕... ⊕qk ⊕ak) > −c + β · |qk| + log P−(q1 ⊕a1 ⊕... ⊕ak−1)
P+(q1 ⊕a1 ⊕... ⊕ak−1)
From the base of the induction:
= −c + β · |qk| +
k−1
X
i=1
(β · |qi| −c)
Giving us:
log P−(q1 ⊕a1 ⊕... ⊕qk ⊕ak)
P+(q1 ⊕a1 ⊕... ⊕qk ⊕ak) >
k
X
i=1
(β · |qi| −c)
As desired.
D.1
LEMMA (ADVERSARIAL PROMPT CONSTRUCTION)
Given s0 and P+, P−being β-distinguishable, we construct a prompt sentence by sentence, q =
s1 ⊕... ⊕s|q|. Such that
log P−(qk|s0)
P−(qk|s0) > β · |q|
By induction:
• Base case: From β-distinguishability,
Es1∼P−(·|s0)[log P−(s1|s0)
P+(s1|s0)] > β
Thus, in particular there exists a sentence s1 that satisﬁes
log P−(s1|s0)
P+(s1|s0) > β
18

Preprint. Under review.
• Assume that the inequality holds for k−1, let us prove for k ≤|q|. From β-distinguishability,
Esk∼P−(s1⊕...⊕sk−1)[log P−(sk|s1 ⊕... ⊕sk−1)
P+(sk|s1 ⊕... ⊕sk−1)] > β
Thus, in particular there exists a sentence sk that satisﬁes
log P−(sk|s1 ⊕... ⊕sk−1)
P+(sk|s1 ⊕... ⊕sk−1) > β
Such that in total,
log P−(q|s0)
P+(q|s0) =
|q|
X
i=1
log P−(si|s0 ⊕s1 ⊕... ⊕si−1)
P+(si|s0 ⊕s1 ⊕... ⊕si−1) > |q| · β
As desired.
E
PROOF OF PROPOSITION 1
For convenience we will map B′ = −B and γ′ = −γ in order to work with positive values.
The idea is to look at the behavior expectation value given the continuation of the conversation,
s1 = q1 ⊕a1 ⊕... ⊕qn ⊕an:
B′
P(s0 ⊕s1) = Es∼P(·|s0⊕s1)[B′(s)] =
X
s
αP−(s0 ⊕s1 ⊕s) + (1 −α)P+(s0 ⊕s1 ⊕s)
αP−(s0 ⊕s1) + (1 −α)P+(s0 ⊕s1)
B′(s) >
The last transition if from the two-component mixture assured by the α, β, γ-distinguishability. We
then take only the negative component term in the numerator:
>
X
s
αP−(s0 ⊕s1 ⊕s)
αP−(s0 ⊕s1) + (1 −α)P+(s0 ⊕s1)B′(s) =
X
s
αP−(s0 ⊕s1 ⊕s)
αP−(s0 ⊕s1)(1 + (1−α)P+(s0⊕s1)
αP−(s0⊕s1)
)
B′(s) =
=
X
s
P−(s|s0 ⊕s1)
(1 + (1−α)P+(s0⊕s1)
αP−(s0⊕s1)
)
B′(s)
Notice that if (1−α)P+(s0⊕s1)
αP−(s0⊕s1)
< 1, then the denominator is smaller than 2, meaning that:
>
X
s
P−(s|s0 ⊕s1)
2
B′(s) = 1
2
X
s
P−(s|s0 ⊕s1)B′(s) = 1
2B′
P−(s0 ⊕s1) > γ′/2
The last transition is from the α, β, γ-distinguishability, which assures BP−(s) < γ for any s. Going
back to B = −B′ and γ = −γ′, we get:
BP(s0 ⊕s1) < γ/2
So let us see what is required for this condition to hold:
(1 −α)P+(s0 ⊕s1)
αP−(s0 ⊕s1)
< 1
iff:
log P+(s0 ⊕s1)
P−(s0 ⊕s1) < log
α
1 −α
Using c-similarity of P+ and P−gives, log P+(s0⊕s1)
P−(s0⊕s1) < c · |s1| + log P+(s0)
P−(s0) (similarly to lemma on
bounding M0). In the ﬁrst part of the theorem, we showed that the adversarial prompt s0 satisﬁes
log P+(s0)
P−(s0) < |s0| · β. Combining these two, we get:
log P+(s0 ⊕s1)
P−(s0 ⊕s1) < c · |s1| −β · |s0| <! log
α
1 −α
Such that the condition we require is assured to hold if:
|s1| < β
c |s0| + log
α
1 −α
This concludes our proof, that unless |s1| = Pn
i=1 |qi| + |ai| = Ω(|s0|), then BP(s0 ⊕s1) < γ/2.
19

Preprint. Under review.
F
PROOF OF THEOREM 4
Let ϵ > 0, and let ˜φ be the persona for which the conditions in the γ-distinguishable in persona
mixture (see deﬁnition 5) holds with ϵ
2. For convenience, we map B′ = −B, γ′ = −γ, in order to
work with positive values, and start by writing the behavior expectation value given a prompt s0:
B′
P(s0) = Es∼P (∗|s0)[B′(s)] =
X
s
P(s|s0)B′(s) =
Now, we can write the mixture decomposition explicitly and get that:
=
X
s
P
φ wφPφ(s0 ⊕s)
P
φ wφPφ(s0)
B′(s) >
X
s
w ˜φP ˜φ(s0 ⊕s)
P
φ wφPφ(s0)B′(s) =
In the transition, above we took only the ˜φ component in the numerator. Let us now rewrite the
denominator:
=
X
s
w ˜φP ˜φ(s0 ⊕s)
w ˜φP ˜φ(s0)(1 + P
φ̸= ˜φ
wφPφ(s0)
w ˜
φP ˜
φ(s0))
B′(s) =
Since ˜φ is c-similar to the other components in the mixture, we use the lemma on persona converging
(next appendix): there exists s0 (of length O(log 1
ϵ′ , 1
β , c2, log |Φ|)), such that, Pφ(s0)
P ˜
φ(s0) < ϵ′ for φ ̸= ˜φ.
Applying it:
≥
X
s
w ˜φP ˜φ(s0 ⊕s)
w ˜φP ˜φ(s0)(1 + ϵ′ P
φ̸= ˜φ
wφ
w ˜
φ )B′(s) =
From the α, β, γ-distinguishability, w ˜φ > α and P
φ̸= ˜φ wφ < 1, we get:
≥
X
s
w ˜φP ˜φ(s0 ⊕s)
w ˜φP ˜φ(s0)(1 + ϵ′
α )B′(s) =
1
1 + ϵ′
α
X
s
P ˜φ(s|s0)B′(s) =
1
1 + ϵ′
α
Es∼P ˜
φ(∗|s0)[B′(s)] =
=
1
1 + ϵ′
α
B′
P ˜
φ(s0) >
γ′
1 + ϵ′
α
The last transition, was since the α, β, γ-distinguishability assures B′
P ˜
φ(s0) > γ′. With Bernouli’s
inequality:
≥γ(1 −ϵ′
α)
Taking ϵ′ = 2αϵ
γ , we get:
B′
P(s1) ≥γ′ −ϵ
Mapping back B = −B′, γ = −γ′, we get:
BP(s1) ≤γ + ϵ
As desired. Notice that |s0| = O(log 1
ϵ′ ) = O(log 1
ϵ , log 1
α), also from the condition on the lemma,
|s0| = O( 1
β , c2, log |Φ|)
G
LEMMA (PERSONA CONVERGING)
Here, we show that if one persona is distinct enough from the rest, then there exists a prompt which
can enhance its probability distribution compared with all the rest:
Lemma 1. Let β, ϵ > 0 and mixture of personas P =
X
φ∈Φ
wφPφ then for each β-Martingale-
distinguishable persona ˜φ, there exists a prompt s0 ∈Σ∗such that:
∀φ ̸= ˜φ
Pφ(s0)
P ˜φ(s0) < ϵ
(10)
Additionally, |s0| = O

log 1
ϵ , 1
β , log |Φ| , c2
.
20

Preprint. Under review.
Intuitively, this means that no matter the initial prompt and initial priors of the mixture, a new prompt
can allow to enhance any speciﬁc distinguishable persona.
Proof of lemma:
Intuitively, we will use the probabilistic method and prove that the probability of s0 for which
Pφ(s0)
P ˜
φ(s0) < ϵ uphold simultaneously for any φ is greater than zero and hence such s0 exists. Speciﬁcally,
let φ from some other persona such that ˜φ is β-Martingale-distinguishable from φ. For a prompt Q
composed of n sentences, Q = q1 ⊕... ⊕qn, denote by:
M
˜φ,φ
n
= log
P ˜φ(q1 ⊕... ⊕qn)
Pφ(q1 ⊕... ⊕qn)
Then, since ˜φ is β-Martingale-distinguishable from φ we have that:
Esn+1∼P ˜
φ(·)[M
˜φ,φ
n+1|M
˜φ,φ
1
= m1...M
˜φ,φ
n
= mn] > mn + β
Intuitively, the expectation value of M ˜φ,φ
n
is n times β so we want to prove that indeed M ˜φ,φ
n
is close
to its expectation value simultaneously for any φ, and in addition choose n such that n · β is greater
than log 1
ϵ . Formally, since we want to apply sub-martingale concentration inequalities we will deﬁne
a new series of random variables Z0, . . . , zn which equals to Mn minus its expectation value:
Zn = M
˜φ,φ
n
−n ∗β
Then, by deﬁnition we have that Z0, . . . , zn is sub-martingale since:
Esn+1∼P ˜
φ(·)[Zn+1|Z1 = z1...Zn = zn]
= Esn+1∼P ˜
φ(·)[M
˜φ,φ
n+1|M
˜φ,φ
1
= m1...M
˜φ,φ
n
= mn] −(n + 1)β
> mn + β −(n + 1)β = mn + βn
= zn
In addition, Zn is bounded since from c-similarity we have that:
|Mn+1 −Mn| =
log
P ˜φ(qn+1 | q1 ⊕... ⊕qn)
Pφ(qn+1 | q1 ⊕... ⊕qn)
 < c
And therefore:
−c + β < Zn+1 −Zn < c + β
So, we conclude that Zn is bounded sub-martingales. Thus we can apply Azuma’s theorem (on
bounded sub-martingales) and get that:
Psn∼P ˜
φ(·) (Zn −Z0 ≤−˜ϵ) ≤exp

−˜ϵ2
8 · n · c2

for any ˜ϵ > 0.
Notice that M
˜φ,φ
0
= log
P ˜
φ(·)
Pφ(·) = 0 so we can choose ˜ϵ = n·β
2 and get that:
Psn∼P ˜
φ

Zn ≤−n · β
2

≤exp
 
−n
32
β
c
2!
We want to make a union bound for all φ ̸= ˜φ and show that even after the union bound the probability
is greater than zero. So we need that:
exp
 
−n
32
β
c
2!
< 1
|Φ|
while hold for any n > 32 log |Φ|

c
β
2
.
21

Preprint. Under review.
Finally, since we need that Mn will be grater than log 1
ϵ we will choose n that is also greater than
2
β log 1
ϵ and get that:
Mn = Zn + n · β > n · β
2
> log 1
ϵ
So we conclude that for any n > max

2
β log 1
ϵ , 32 log |Φ|

c
β
2
there exists a prompt satisfying
the following condition for all φ ̸= ˜φ:
Pφ(s ⊕q1 ⊕... ⊕qn)
P ˜φ(s ⊕q1 ⊕... ⊕qn) ≥1
ϵ
And the user may choose it.
22

