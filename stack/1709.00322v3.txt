Disintegration and Bayesian Inversion via
String Diagrams
K E N TA CHO† and BART JACOBS‡
†National Institute of Informatics
2-1-2 Hitotsubashi, Chiyoda-ku, Tokyo 101-8430, Japan
Email: cho@nii.ac.jp
‡Institute for Computing and Information Sciences, Radboud University
P.O.Box 9010, 6500 GL Nijmegen, the Netherlands
Email: bart@cs.ru.nl
Received 31 August 2017; revised 7 December 2018
The notions of disintegration and Bayesian inversion are fundamental in conditional
probability theory. They produce channels, as conditional probabilities, from a joint state,
or from an already given channel (in opposite direction). These notions exist in the
literature, in concrete situations, but are presented here in abstract graphical formulations.
The resulting abstract descriptions are used for proving basic results in conditional
probability theory. The existence of disintegration and Bayesian inversion is discussed for
discrete probability, and also for measure-theoretic probability — via standard Borel
spaces and via likelihoods. Finally, the usefulness of disintegration and Bayesian inversion
is illustrated in several examples.
1. Introduction
The essence of conditional probability can be summarised informally in the following
equation about probability distributions:
joint = conditional · marginal.
A bit more precisely, when we have joint probabilities P(x, y) for elements x, y ranging
over two sample spaces, the above equation splits into two equations,
P(y | x) · P(x) = P(x, y) = P(x | y) · P(y),
(1)
where P(x) and P(y) describe the marginals, which are obtained by discarding variables.
We see that conditional probabilities P(y | x) and P(x | y) can be constructed in two
directions, namely y given x, and x given y. We also see that we need to copy variables:
x on the left-hand-side of the equations (1), and y on the right-hand-side.
Conditional probabilities play a crucial role in Bayesian probability theory. They form
the nodes of Bayesian networks (Pearl, 1988; Bernardo and Smith, 2000; Barber, 2012),
arXiv:1709.00322v3  [cs.AI]  8 Feb 2019

K. Cho and B. Jacobs
2
which reﬂect the conditional independencies of the underlying joint distribution via their
graph structure. As part of our approach, we shall capture conditional independence in
an abstract manner.
The main notion of this paper is disintegration. It is the process of extracting a
conditional probability from a joint probability. Disintegration, as we shall formalise
it here, gives a structural description of the above equation (1) in terms of states and
channels. In general terms, a state is a probability distribution of some sort (discrete,
measure-theoretic, or even quantum) and a channel is a map or morphism in a probabilistic
setting, like P(y | x) and P(x | y) as used above. It can take the form of a stochastic
matrix, probabilistic transition system, Markov kernel, conditional probability table (in a
Bayesian network), or morphism in a Kleisli category of a ‘probability monad’ (Jacobs,
2018). A state is a special kind of channel, with trivial domain. Thus we can work in a
monoidal category of channels, where we need discarding and copying — more formally, a
comonoid structure on each object — in order to express the above conditional probability
equations (1).
In this article we abstract away from interpretation details and will describe disinte-
gration pictorially, in the language of string diagrams. This language can be seen as the
internal language of symmetric monoidal categories (Selinger, 2010) — with comonoids
in our case. The essence of disintegration becomes: extracting a conditional probability
channel from a joint state.
Categorical approaches to Bayesian conditioning have appeared for instance in (Cul-
bertson and Sturtz, 2014; Staton et al., 2016; Clerc et al., 2017) and in (Jacobs et al., 2015;
Jacobs and Zanasi, 2016; Jacobs, 2018). The latter references use eﬀectus theory (Jacobs,
2015; Cho et al., 2015), a new comprehensive approach aimed at covering the logic of
both quantum theory and probability theory, supported by a Python-based tool EfProb,
for ‘eﬀectus probability’. This tool is used for the (computationally extensive) examples
in this paper.
Disintegration, also known as regular conditional probability, is a notoriously diﬃcult
operation in measure-theoretic probability, see e.g. (Pollard, 2002; Panangaden, 2009;
Chang and Pollard, 1997): it may not exist (Stoyanov, 2014); even if it exists it may be de-
termined only up to negligible sets; and it may not be continuous or computable (Ackerman
et al., 2011). Disintegration has been studied using categorical language in (Culbertson
and Sturtz, 2014), which focuses on a speciﬁc category of probabilistic mappings. Our
approach here is more axiomatic.
We thus describe disintegration as going from a joint state to a channel. A closely
related concept is Bayesian inversion: it turns a channel (with a state) into a channel in
opposite direction. We show how Bayesian inversion can be understood and expressed
easily in terms of disintegration — and also how, in the other direction, disintegration
can be obtained from Bayesian inversion. Bayesian inversion is taken as primitive notion
in (Clerc et al., 2017). Here we start from disintegration. The diﬀerence is a matter of
choice.
Bayesian inversion is crucial for backward inference. We explain it informally: let σ be
a state of a domain/type X, and c: X →Y be a channel; Bayesian inversion yields a
channel d: Y →X. Informally, it produces for an element y ∈Y , seen as singleton/point

Disintegration and Bayesian Inversion
3
predicate {y}, the conditioning of the state σ with the pulled back evidence c−1({y}).
A concrete example involving such ‘point observations’ will be described at the end of
Section 8. More generally, disintegration and Bayesian inversion are used to structurally
organise state updates in the presence of new evidence in probabilistic programming,
see e.g. (Gordon et al., 2014; Borgström et al., 2013; Staton et al., 2016; Katoen et al.,
2015). See also (Shan and Ramsey, 2017), where disintegration is handled via symbolic
manipulation.
Disintegration and Bayesian inversion are relatively easy to deﬁne in discrete probability
theory. The situation is much more diﬃcult in measure-theoretic probability theory, ﬁrst
of all because point predicates {y} do not make much sense there, see also (Chang and
Pollard, 1997). A common solution to the problem of the existence of disintegration /
Bayesian inversion is to restrict ourselves to standard Borel spaces, as in (Clerc et al.,
2017). We take this approach too. There is still an issue that disintegration is determined
only up to negligible sets. We address this by deﬁning ‘almost equality’ in our abstract
pictorial formulation. This allows us to present a fundamental result from (Clerc et al.,
2017) abstractly in our setting, see Section 5.
Another common, more concrete solution is to assume a likelihood, that is, a probabilistic
relation X × Y →R≥0. Such a likelihood gives rise to probability density function (pdf),
providing a good handle on the situation, see (Pawitan, 2001). The technical core of
Section 8 is a generalisation of this likelihood-based approach.
The paper is organised as follows. It starts with a brief introduction to the graphical
language that we shall be using, and to the underlying monoidal categories with discarding
and copying. Then, Section 3 introduces both disintegration and Bayesian inversion in
this graphical language, and relates the two notions. Subsequently, Section 4 contains an
elaborated example, namely of naive Bayesian classiﬁcation. A standard example from
the literature (Witten et al., 2011) is redescribed in the current setting: ﬁrst, channels
are extracted via disintegration from a table with given data; next, Bayesian inversion
is applied to the combined extracted channels, giving the required classiﬁcation. This is
illustrated in both the discrete and the continuous version of the example.
Next, Section 5 is more technical and elaborates the standard equality notion of ‘equal
almost everywhere’ in the current setting. This is used for describing Bayesian inversion in
a more formal way, following (Clerc et al., 2017). Section 6 uses our graphical approach to
review conditional independence and to prove at an abstract level several known results,
namely the equivalence of various formulations of conditional independence, and the
‘graphoid’ axioms from (Verma and Pearl, 1988; Geiger et al., 1990). Section 7 relaxes the
requirement that maps are causal, so that ‘eﬀects’ can be used as the duals of states for
validity and conditioning. The main result relates conditioning of joint states to forward
and backward inference via the extracted channels, in the style of (Jacobs and Zanasi,
2016; Jacobs and Zanasi, 2018); it is illustrated in a concrete example, where a Bayesian
network is seen as a graph in a Kleisli category — following (Fong, 2012). Finally, Section 8
gives the likelihood formulation of disintegration and inversion, as brieﬂy described above.

K. Cho and B. Jacobs
4
2. Graphical language
The basic idea underlying this paper is to describe probability theory in terms of channels.
A channel f : X →Y is a (stochastic) process from a system of type X into that of Y .
Concretely, it may be a probability matrix or kernel. Our standing assumption is that
types (as objects) and channels (as arrows) form a symmetric monoidal category. For the
formal deﬁnition we refer to (Mac Lane, 1998). We informally summarise that we have
the following constructions.
1
Sequential composition g ◦f : X →Z for appropriately typed channels f : X →Y
and g: Y →Z.
2
Parallel composition f ⊗g: X ⊗Z →Y ⊗W for f : X →Y and g: Z →W. This
involves composition of types X ⊗Z.
3
Identity channels idX : X →X, which ‘do nothing’. Thus id ◦f = f = id ◦f.
4
A unit type I, which represents ‘no system’. Thus I ⊗X ∼= X ∼= X ⊗I.
5
Swap isomorphisms X ⊗Y ∼= Y ⊗X and associativity isomorphisms (X ⊗Y ) ⊗Z ∼=
X ⊗(Y ⊗Z), so the ordering in composed types does not matter.
The representation of such channels in the ordinary ‘formula’ notation easily becomes
complex and thus reasoning becomes hard to follow. A graphical language known as
string diagrams oﬀers a more convenient and intuitive way of reasoning in a symmetric
monoidal category.
In string diagrams, types/objects are represented as wires ‘ ’, with information ﬂowing
bottom to top. The composition of types is depicted by juxtaposition of wires, and the
unit type is ‘no diagram’ as below.
X ⊗Y
=
X Y
I =
Channels/arrows are represented by boxes with an input wire(s) and an output wire(s),
in upward direction. When a box does not have input or output, we write it as a triangle
or diamond. For example, f : X →Y ⊗Z, ω: I →X, p: X →I, and s: I →I are
respectively depicted as:
f
X
Y
Z
ω
X
p
X
s
The identity channels are represented by ‘no box’, i.e. just wires, and the swap isomor-
phisms are represented by crossing of wires:
id
X
X
=
X
X
X
Y
Y
Finally, the sequential composition of channels is depicted by connecting the input and

Disintegration and Bayesian Inversion
5
output wires, and the parallel composition is given by juxtaposition, respectively as below:
g ◦f
=
f
g
h ⊗k
=
h
k
The use of string diagrams is justiﬁed by the following ‘coherence’ theorem; see (Joyal
and Street, 1991; Selinger, 2010) for details.
Theorem 2.1. A well-formed equation between composites of arrows in a symmetric
monoidal category follows from the axioms of symmetric monoidal categories if and only
if the string diagrams of both sides are equal up to isomorphism of diagrams.
We further assume the following structure in our category. For each type X there are a
discarder
X : X →I and a copier
X : X →X ⊗X. They are required to satisfy the
following equations:
=
=
=
=
This says that (
X,
X) forms a commutative comonoid on X. By the associativity we
may write:
. . .
:=
. . .
Moreover we assume that the comonoid structures (
X,
X) are compatible with the
monoidal structure (⊗, I), in the sense that the following equations hold.
X ⊗Y
=
X
Y
I =
X ⊗Y
=
X
Y
I =
Note that we do not assume that these maps are natural. Explicitly, we do not necessarily
have
◦f = (f ⊗f) ◦
or
◦f =
.
We will use these symmetric monoidal categories throughout in the paper. For conve-
nience, we introduce a term for them.
Deﬁnition 2.2. A CD-category is a symmetric monoidal category (C, ⊗, I) with a
commutative comonoid (
X,
X) for each X ∈C, suitably compatible as described
above.
Here ‘CD’ stands for Copy/Discard.
Deﬁnition 2.3. An arrow f : X →Y in a CD-category is said to be causal if
f
=
.
A CD-category is aﬃne if all the arrows are causal, or equivalently, the tensor unit I is a
ﬁnal object.

K. Cho and B. Jacobs
6
The term ‘causal’ comes from (categorical) quantum foundation (Coecke and Kissinger,
2017; D’Ariano et al., 2017), and is related to relativistic causality, see e.g. (Coecke, 2016).
We reserve the term ‘channel’ for causal arrows. Explicitly, causal arrows c: X →Y
in a CD-category are called channels. A channel ω: I →X with input type I is called a
state (on X). For the time being we will only use channels (and states), and thus only
consider aﬃne CD-categories. Non-causal arrows will not appear until Section 7.
Our main examples of aﬃne CD-categories are two Kleisli categories Kℓ(D) and Kℓ(G),
respectively, for discrete probability, and more general, measure-theoretic probability. We
explain them in order below. There are more examples, like the Kleisli category of the
non-empty powerset monad, or the (opposite of) the category of commutative C∗-algebras
(with positive unital maps), but they are out of scope here.
Example 2.4. What we call a distribution or a state over a set X is a ﬁnite subset
{x1, x2, . . . , xn} ⊆X, called the support where each element xi occurs with a multiplicity
ri ∈[0, 1], such that P
i ri = 1. Such a convex combination is often written as r1|x1⟩+
· · · + rn|xn⟩with ri ∈[0, 1]. The ket notation |−⟩is meaningless syntactic sugar that
is used to distinguish elements x ∈X from occurrences in such formal sums. Notice
that a distribution can also be written as a function ω: X →[0, 1] with ﬁnite support
supp(ω) = {x ∈X | ω(x) ̸= 0}. We shall write D(X) for the set of distributions over X.
This D is a monad on the category Set of sets and functions.
A function f : X →D(Y ) is called a Kleisli map; it forms a channel X →Y . Such
maps can be composed as matrices, for which we use special notation ◦·.
(g ◦· f)(x)(z) = P
y∈Y f(x)(y) · g(y)(z)
for g: Y →D(Z) with x ∈X, z ∈Z.
We write 1 = {∗} for a singleton set, and 2 = 1 + 1 = {0, 1}. Notice that D(1) ∼= 1 and
D(2) ∼= [0, 1]. We can identify a state on X with a channel 1 →X.
The monad D is known to be commutative. This implies that ﬁnite products of sets X×Y
give rise to a symmetric monoidal structure on the Kleisli category Kℓ(D). Speciﬁcally, for
two maps f : X →D(Y ) and g: Z →D(W), the tensor product / parallel composition
f ⊗g: X × Z →D(Y × W) is given by:
(f ⊗g)(x, z)(y, w) = f(x, y) · g(z, w).
For each set X there are a copier
X : X →D(X × X) and a discarder
X : X →D(1)
given by
X(x) = 1|x, x⟩and
X(x) = 1|∗⟩, respectively. They come from the cartesian
(ﬁnite product) structure of the base category Set, through the obvious functor Set →
Kℓ(D). Therefore Kℓ(D) is a CD-category. It is moreover aﬃne, since the monad is aﬃne
in the sense that D(1) ∼= 1.
Example 2.5. Let X = (X, ΣX) be a measurable space, where ΣX is a σ-algebra on
X. A probability measure, also called a state, on X is a function ω: ΣX →[0, 1] which
is countably additive and satisﬁes ω(X) = 1. We write G(X) for the collection of all
such probability measures on X. This set G(X) is itself a measurable space with the
smallest σ-algebra such that for each A ∈ΣX the ‘evaluation’ map evA : G(X) →[0, 1],
evA(ω) = ω(A), is measurable. Notice that G(X) ∼= D(X) when X is a ﬁnite set (as

Disintegration and Bayesian Inversion
7
discrete space). In particular, G(2) ∼= D(2) ∼= [0, 1]. This G is a monad on the category
Meas of measurable spaces, with measurable functions between them; it is called the
Giry monad, after (Giry, 1982).
A Kleisli map, that is, a measurable function f : X →G(Y ) is a channel (or a probability
kernel, see Example 7.2). These channels can be composed, via Kleisli composition ◦·,
using integration†:
(g ◦· f)(x)(C) =
Z
Y
g(y)(C) f(x)(dy)
where g: Y →G(Z) and x ∈X, C ∈ΣZ.
It is well-known that the monad G is commutative and aﬃne, see also (Jacobs, 2018).
Thus, in a similar manner to the previous example, the Kleisli category Kℓ(G) is an aﬃne
CD-category. The parallel composition f ⊗g: X × Z →G(Y × W) for f : X →G(Y ) and
g: Z →G(W) is given as:
(f ⊗g)(x, z)(B × D) = f(x)(B) · g(z)(D),
for x ∈X, z ∈Z, B ∈ΣY , and D ∈ΣW . Since f(x) and g(z) are (σ-)ﬁnite measures,
this indeed determines a unique measure (f ⊗g)(x, z) ∈G(Y × W), namely the unique
product measure of f(x) and g(z). Explicitly, for E ∈ΣY ×W ,
(f ⊗g)(x, z)(E) =
Z
W
Z
Y
1E(y, w) f(x)(dy)

g(z)(dw)
where 1E is the indicator function.
3. Marginalisation, integration and disintegration
Let C be an aﬃne CD-category. We think of states ω: I →X in C as abstract (probability)
distributions on type X. States of the form ω: I →X ⊗Y , often called (bipartite) joint
states, are seen as joint distributions on X and Y . Later on we shall also consider n-partite
joint states, but for the time being we restrict ourselves to bipartite ones. For a joint
distribution P(x, y) in discrete probability, we can calculate the marginal distribution on
X by summing (or marginalising) Y out, as P(x) = P
y P(x, y). The marginal distribution
on Y is also calculated by P(y) = P
x P(x, y). In our abstract setting, given a joint state
ω: I →X ⊗Y , we can obtain marginal states simply by discarding wires, as in:
ω
X
marginal on X
←−−−−−−−−−[
ω
X
Y
marginal on Y
7−−−−−−−−−→
ω
Y
In other words, the marginal states are the state ω composed with the projection maps
π1 : X ⊗Y →X and π2 : X ⊗Y →Y , as below.
π1 :=
X
Y
π2 :=
X
Y
† We denote the integral of a function f with respect to a measure µ by
R
X f(x) µ(dx).

K. Cho and B. Jacobs
8
Example 3.1. For a joint state ω ∈D(X × Y ) in Kℓ(D), the ﬁrst marginal ω1 = π1 ◦· ω
is given by ω1(x) = P
y∈Y ω(x, y), as expected. Similarly the second marginal is given by
ω2(y) = P
x∈X ω(x, y).
Example 3.2. For a joint state ω ∈G(X × Y ) in Kℓ(G), the ﬁrst marginal is given by
ω1(A) = ω(A × Y ) for A ∈ΣX, and the second marginal by ω2(B) = ω(X × B) for
B ∈ΣY .
A channel c: X →Y is seen as an abstract conditional distribution P(y|x). In (discrete)
probability theory, we can calculate a joint distribution P(x, y) from a distribution P(x)
and a conditional distribution P(y|x) by the formula P(x, y) = P(y|x) · P(x), which is
often called the product rule. Similarly we have P(x, y) = P(x|y) · P(y). In our setting,
starting from a state σ: I →X and a channel c: X →Y , or a state τ : I →Y and
a channel d: Y →X, we can ‘integrate’ them into a joint state on X ⊗Y as follows,
respectively:
σ
c
X
Y
or
τ
d
X
Y
(2)
Example 3.3. Let σ ∈D(X) and c: X →D(Y ) be a state and a channel in Kℓ(D). An
easy calculation veriﬁes that ω = (id ⊗c) ◦·
◦· σ, the joint state on X × Y deﬁned as
in (2), satisﬁes ω(x, y) = c(x)(y) · σ(x), as we expect from the product rule.
Example 3.4. For a state σ ∈G(X) and a channel c: X →G(Y ) in Kℓ(G), the joint
state ω = (id ⊗c) ◦·
◦· σ is given by ω(A × B) =
R
A c(x)(B) σ(dx) for A ∈ΣX and
B ∈ΣY . This ‘integration’ construction of a joint probability measure is standard, see
e.g. (Pollard, 2002; Panangaden, 2009).
Disintegration is an inverse operation of the ‘integration’ of a state and a channel into
a joint state, as in (2). More speciﬁcally, it starts from a joint state ω: I →X ⊗Y and
extracts either a state ω1 : I →X and a channel c1 : X →Y , or a state ω2 : I →Y and a
channel c2 : Y →X as below,

ω1
X
,
c1
Y
X

disintegration
←−−−−−−−−[
ω
X
Y
disintegration
7−−−−−−−−→

ω2
Y
,
c2
X
Y

such that the equation on the left or right below holds, respectively.
ω1
c1
X
Y
=
ω
X
Y
=
ω2
c2
X
Y
(3)

Disintegration and Bayesian Inversion
9
We immediately see from the equation that ω1 and ω2 must be marginals of ω:
ω
=
ω1
c1
=
ω1
=
ω1
and similarly
ω
=
ω2
.
Therefore a disintegration of ω may be referred to by a channel ci only, rather than a
pair (ωi, ci). This leads to the following deﬁnition:
Deﬁnition 3.5. Let ω: I →X⊗Y be a joint state. A channel c1 : X →Y (or c2 : Y →X)
is called a disintegration of ω if it satisﬁes the equation (3) with ωi the marginals of ω.
Let us look at concrete instances of disintegrations, in the Kleisli categories Kℓ(D) and
Kℓ(G) from Examples 2.4 and 2.5.
Example 3.6. Let ω ∈D(X × Y ) be a joint state in Kℓ(D). We write ω1 ∈D(X) for
the ﬁrst marginal, given by ω1(x) = P
y ω(x, y). Then a channel c: X →D(Y ) is a
disintegration of ω if and only if ω(x, y) = c(x)(y) · ω1(x) for all x ∈X and y ∈Y . It
turns out that there is always such a channel c. We deﬁne a channel c by:
c(x)(y) := ω(x,y)
ω1(x)
if
ω1(x) ̸= 0 ,
(4)
and c(x) := τ if ω1(x) = 0, for an arbitrary state τ ∈D(Y ). (Here D(Y ) is nonempty, since
so is D(X ×Y ).) This indeed deﬁnes a channel c satisfying the required equation. Roughly
speaking, disintegration in discrete probability is nothing but the ‘deﬁnition’ of conditional
probability: P(y|x) = P(x, y)/ P(x). There is still some subtlety — disintegrations need
not be unique, when there are x ∈X with ω1(x) = 0. They are, nevertheless, ‘almost
surely’ unique; see Section 5.
Example 3.7. Disintegrations in measure-theoretic probability, in Kℓ(G), are far more
diﬃcult. Let ω ∈G(X × Y ) be a joint state, with ω1 ∈G(X) the ﬁrst marginal. A channel
c: X →G(Y ) is a disintegration of ω if and only if
ω(A × B) =
Z
A
c(x)(B) ω1(dx)
for all A ∈ΣX and B ∈ΣY . This is the ordinary notion of disintegration (of probability
measures), also known as regular conditional probability; see e.g. (Faden, 1985; Pollard,
2002; Panangaden, 2009). We see that there is no obvious way to obtain a channel c
here, unlike the discrete case. In fact, a disintegration may not exist (Stoyanov, 2014).
There are, however, a number of results that guarantee the existence of a disintegration
in certain situations. We will come back to this issue later in the section.
Bayesian inversion is a special form of disintegration, occurring frequently. We start

K. Cho and B. Jacobs
10
from a state σ: I →X and a channel c: X →Y . We then integrate them into a joint
state on X and Y , and disintegrate it in the other direction, as below.

σ
X
,
c
Y
X

integration
7−−−−−−−→
σ
c
X
Y
disintegration
7−−−−−−−−→

σ
c
Y
,
d
X
Y

We call the disintegration d: Y →X a Bayesian inversion for σ: I →X along c: X →Y .
By unfolding the deﬁnitions, a channel d: Y →X is a Bayesian inversion if and only if
σ
c
=
σ
d
c
.
(5)
The composite c ◦σ is also written as c∗(σ). The operation σ 7→c∗(σ), called state
transformation, is used to explain forward inference in (Jacobs and Zanasi, 2016).
Example 3.8. Let σ ∈D(X) and c: X →D(Y ) be a state and a channel in Kℓ(D).
Then a channel d: Y →D(X) is a Bayesian inversion for σ along c if and only if
c(x)(y) · σ(x) = d(y)(x) · c∗(σ)(y), where c∗(σ)(y) = P
x′ c(x′)(y) · σ(x′). In a similar
manner to Example 3.6, we can obtain such a d by:
d(y)(x) := c(x)(y) · σ(x)
c∗(σ)(y)
=
c(x)(y) · σ(x)
P
x′ c(x′)(y) · σ(x′)
for y ∈Y with c∗(σ)(y) ̸= 0. For y ∈Y with c∗(σ)(y) = 0, we may deﬁne d(y) to be an
arbitrary state in D(X). We can recognise the above formula as the Bayes formula:
P(x|y) = P(y|x) · P(x)
P(y)
=
P(y|x) · P(x)
P
x′ P(y|x′) · P(x′) .
Example 3.9. Let σ ∈G(X) and c: X →G(Y ) be a state and a channel in Kℓ(G). A
channel d: Y →G(X) is a Bayesian inversion if and only if
Z
A
c(x)(B) σ(dx) =
Z
B
d(y)(A) c∗(σ)(dy)
for all A ∈ΣX and B ∈ΣY . Here c∗(σ) ∈G(Y ) is the measure given by c∗(σ)(B) =
R
X c(x)(B) σ(dx). As we see below, Bayesian inversions are in some sense equivalent to
disintegrations, and thus, they are as diﬃcult as disintegrations. In particular, a Bayesian
inversion need not exist.
In practice, however, the state σ and channel c are often given via density functions.
This setting, so-called (absolutely) continuous probability, makes it easy to compute a
Bayesian inversion. Suppose that X and Y are subspaces of R, and that σ and c admit

Disintegration and Bayesian Inversion
11
density functions as
σ(A) =
Z
A
f(x) dx
c(x)(B) =
Z
B
ℓ(x, y) dy
for measurable functions f : X →R≥0 and ℓ: X × Y →R≥0. The conditional probability
density ℓ(x, y) of y given x is often called the likelihood of x given y. By the familiar
Bayes formula for densities — see e.g. (Bernardo and Smith, 2000) — the conditional
density of x given y is:
k(y, x) :=
ℓ(x, y) · f(x)
R
X ℓ(x′, y) · f(x′) dx′ .
This k then gives a channel d: Y →G(X) by
d(y)(A) =
Z
A
k(y, x) dx
for each y ∈Y such that
R
X ℓ(x′, y) · f(x′) dx′ ̸= 0. For the other y’s we deﬁne d(y) to be
some ﬁxed state in G(X). An elementary calculation veriﬁes that d is indeed a Bayesian
inversion for σ along c. Later, in Section 8, we generalise this calculation into our abstract
setting.
Although Bayesian inversions are a special case of disintegrations, we can conversely
obtain disintegrations from Bayesian inversions, as in the proposition below. Therefore,
in some sense the two notions are equivalent.
Proposition 3.10. Let ω be a state on X ⊗Y . Let d: X →X ⊗Y be a Bayesian
inversion for ω along the ﬁrst projection π1 : X ⊗Y →X on the left below.
π1 =
X
Y
π2 ◦d =
d
X
Y
Then the composite π2 ◦d: X →Y shown on the right above is a disintegration of ω.
Proof. We prove that the ﬁrst equation in (3) holds for c1 = π2 ◦d, as follows.
ω
d
=
ω
d
=
ω
d
∗=
ω
=
ω
=
ω
For the marked equality
∗= we used the equation (5) for the Bayesian inversion d.
We say that an aﬃne CD-category C admits disintegration if for every bipartite state
ω: I →X ⊗Y there exist a disintegration c1 : X →Y of ω. Note that in such categories
there also exists a disintegration c2 : Y →X of ω in the other direction, since it can be

K. Cho and B. Jacobs
12
obtained as a disintegration of the following state:
ω
Y
X
.
By Proposition 3.10, admitting disintegration is equivalent to admitting Bayesian inversion.
In Example 3.6, we have seen that Kℓ(D) admits disintegration, but that in measure-
theoretic probability, in Kℓ(G), disintegrations may not exist. There are however a
number of results that guarantee the existence of disintegrations in speciﬁc situations,
see e.g. (Pachl, 1978; Faden, 1985). We here invoke one of these results and show that
there is a subcategory of Kℓ(G) that admits disintegration. A measurable space is called
a standard Borel space if it is measurably isomorphic to a Polish space with its Borel
σ-algebra, or equivalently, if it is measurably isomorphic to a Borel subspace of R. Then
the following theorem is standard, see e.g. (Pollard, 2002, §5.2) or (Faden, 1985, §5).
Theorem 3.11. Let X be any measurable space and Y be a standard Borel space.
Then for any state (i.e. a probability measure) ω ∈G(X × Y ) in Kℓ(G), there exists a
disintegration c1 : X →G(Y ) of ω.
Let pKrnsb be the full subcategory of Kℓ(G) consisting of standard Borel spaces as
objects. Clearly pKrnsb contains the singleton measurable space 1, which is the tensor
unit of Kℓ(G). We claim that the product (in Meas, hence the tensor product in Kℓ(G)) of
two standard Borel spaces is again standard Borel. Let (X, ΣX) and (Y, ΣY ) be standard
Borel spaces. We ﬁx Polish (= separable completely metrisable) topologies on X and
Y that induce the σ-algebras ΣX and ΣY , respectively. Then the product X × Y , as
topological spaces, is Polish (Doberkat, 2007, Lemma 1.17). The Borel σ-algebra of
the Polish space X × Y coincides with the σ-algebra on the product X × Y in Meas
(Panangaden, 2009, Proposition 2.8). Hence X × Y is standard Borel. Therefore the
subcategory pKrnsb ,→Kℓ(G) is closed under the monoidal structure of Kℓ(G), so that
pKrnsb is an aﬃne CD-category. Then the previous theorem immediately shows:
Corollary 3.12. The category pKrnsb admits disintegration.
We note that pKrnsb can also be seen as the Kleisli category of the Giry monad
restricted on the category of standard Borel spaces, because G(X) is standard Borel
whenever X is standard Borel. To see this, let (X, ΣX) be a standard Borel space. If we
ﬁx a Polish topology on X that induces ΣX, then G(X) also forms a Polish space with the
topology of weak convergence, see (Giry, 1982) or (Doberkat, 2007, §1.5.2). The σ-algebra
on G(X), deﬁned in Example 2.5, coincides with the Borel σ-algebra with respect to the
Polish topology (Doberkat, 2007, Proposition 1.80). Therefore G(X) is standard Borel.
Since there are various ‘existence’ theorems like Theorem 3.11, there may be other
subcategories of Kℓ(G) that admit disintegration. A likely candidate is the category of
perfect probabilistic mappings in (Culbertson and Sturtz, 2014). We do not go into this
question here, since pKrnsb suﬃces for the present paper.

Disintegration and Bayesian Inversion
13
4. Example: naive Bayesian classiﬁers via inversion
Bayesian classiﬁcation is a well-known technique in machine learning that produces a
distribution over data classiﬁcations, given certain sample data. The distribution describes
the probability, for each data (classiﬁcation) category, that the sample data is in that
category. Here we consider an example of ‘naive’ Bayesian classiﬁcation, where the features
are assumed to be independent. We consider a standard classiﬁcation example from the
literature which forms an ideal setting to illustrate the use of both disintegration and
Bayesian inversion. Disintegration is used to extract channels from a given table, and
subsequently Bayesian inversion is applied to (the tuple of) these channels to obtain the
actual classiﬁcation. The use of channels and disintegration/inversion in this classiﬁcation
setting is new, as far as we know.
For the description of the relevant operations in this example we use notation for
marginalisation and disintegration that we borrowed from the EfProb library (Cho and
Jacobs, 2017). There are many ways to marginalise an n-partite state, namely one
for each subset of the wires {1, 2, . . . , n}. Such a subset can be described as a mask,
consisting of a list of n zero’s or one’s, where a zero at position i means that the i-th
wire/component is marginalised out, and a one at position i means that it remains. Such
a mask M = [b1, . . . , bn] with bi ∈{0, 1} is used as a post-ﬁx selection operation in ω.M
on an n-partite state ω. An example explains it all:
if
ω =
then
ω.[1, 0, 1, 0, 0] =
In a similar way one can disintegrate an n-partite state in 2n may ways, where a mask of
length n is now used to describe which wires are used as input to the extracted channel
and which ones as output. We write ω  M for such a disintegration, where M is a mask,
as above. A systematic description will be given in Section 6 below.
In practice it is often useful to be able to marginalise ﬁrst, and disintegrate next. The
general description in n-ary form is a bit complicated, so we use an example for n = 5.
We shall label the wires with xi, as on the left below. We seek the conditional probability
written conventionally as c = ω[x1, x4 | x2, x5] on the right below.
ω
x1 x2 x3 x4 x5
c
x1
x4
x2
x5
This channel c must satisfy:
ω
=
ω
c
This picture shows how to obtain the channel c from ω: we ﬁrst marginalise to restrict

K. Cho and B. Jacobs
14
Outlook
Temperature
Humidity
Windy
Play
Sunny
hot
high
false
no
Sunny
hot
high
true
no
Overcast
hot
high
false
yes
Rainy
mild
high
false
yes
Rainy
cool
normal
false
yes
Rainy
cool
normal
true
no
Overcast
cool
normal
true
yes
Sunny
mild
high
false
no
Sunny
cool
normal
false
yes
Rainy
mild
normal
false
yes
Sunny
mild
normal
true
yes
Overcast
mild
high
true
yes
Overcast
hot
normal
false
yes
Rainy
mild
high
true
no
Fig. 1. Weather and play data, copied from (Witten et al., 2011).
to the relevant wires x1, x2, x4, x5. This is written as ω.[1, 1, 0, 1, 1]. Subsequently we
disintegrate with x1, x4 as output and x2, x5 as input. Hence:
c := ω.[1, 1, 0, 1, 1]  [0, 1, 0, 1]
=: ω.

[1, 0, 0, 1, 0]
 [0, 1, 0, 0, 1]

as we shall write in the sequel.
We see that the latter post-ﬁx

[1, 0, 0, 1, 0]
 [0, 1, 0, 0, 1]

is a ‘variable free’ version of the
traditional notation [x1, x4 | x2, x5], selecting the relevant positions.
We have now prepared the ground and can turn to the classiﬁcation example that we
announced. It involves the classiﬁcation of ‘playing’ (yes or no) for certain weather data,
used in (Witten et al., 2011). We shall ﬁrst go through the discrete example in some detail.
The relevant data are in the table in Figure 1. The question is: given this table, what can
be said about the probability of playing if the outlook is Sunny, the temperature is Cold,
the humidity is High and it is Windy?
Our plan is to ﬁrst organise these table data into four channels dO, dT , dH, dW in a
network of the form:




Outlook




Temperature




Humidity




Windy




Play
dO
g
dH
A
dW
8
dT
_
(6)
We start by extracting the underlying sets for for the categories in the table in Figure 1.
We choose abbreviations for the entries in each of the categories.
O = {s, o, r}
W = {t, f}
T = {h, m, c}
P = {y, n}
H = {h, n}.

Disintegration and Bayesian Inversion
15
These sets are combined into a single product domain:
D = O × T × H × W × P.
It combines the ﬁve columns in Figure 1. The table itself in the ﬁgure is represented as
a uniform distribution τ ∈D(D). This distribution has 14 entries — like in the table —
and looks as follows.
τ =
1
14|s, h, h, f, n⟩+ 1
14|s, h, h, t, n⟩+ · · · + 1
14|r, m, h, t, n⟩.
We extract the four channels in Diagram (6) via appropriate disintegrations, from the
Play column to the Outlook / Temperature / Humidity / Windy columns.
dO = τ.

[1, 0, 0, 0, 0]
 [0, 0, 0, 0, 1]

dT = τ.

[0, 1, 0, 0, 0]
 [0, 0, 0, 0, 1]

dH = τ.

[0, 0, 1, 0, 0]
 [0, 0, 0, 0, 1]

dW = τ.

[0, 0, 0, 1, 0]
 [0, 0, 0, 0, 1]

.
Thus, as described in the beginning of this section, the ‘outlook’ channel dO : P →D(O)
is extracted by ﬁrst marginalising the table τ to the relevant (ﬁrst and last) wires, and
then disintegrating. Explicitly, dO is τ.[1, 0, 0, 0, 1]  [0, 1] and satisﬁes:
ω
O
P
=
π
dO
O
P
where
π
=
ω
P
In a next step we combine these four channels into a single channel d: P →O×T ×H ×W
via tupling:
d
:=
dO
dT
dH
dW
The answer that we are looking for will be obtained by Bayesian inversion of this channel
d wrt. the above ﬁfth marginal Play state π = τ.[0, 0, 0, 0, 1] =
9
14|y⟩+ 5
14|n⟩∈D(P). We
write this inversion as a channel e: O × T × H × W →P. It satisﬁes, by construction,
according to the pattern in (5):
π
dO
dT
dH
dW
=
π
dO
dT
dH
dW
e
We can ﬁnally answer the original classiﬁcation question. The assumptions — Sunny
outlook, Cold temperature, High humidity, true windiness — are used as input to the

K. Cho and B. Jacobs
16
inversion channel e. This yields the probability of play that we are looking for:
e(s, c, H, t) = 0.205|y⟩+ 0.795|n⟩.
The resulting classiﬁcation probability‡ of 0.205 coincides with the probability of 20.5%
that is computed in (Witten et al., 2011) — without the above channel-based approach.
One could complain that our approach is ‘too’ abstract, since it remains implicit what
these extracted channels do. We elaborate the outlook channel dO : P →O. For the two
elements in P = {y, n} we have:
dO(y) = 2
9|s⟩+ 4
9|o⟩+ 3
9|r⟩
dO(n) = 3
5|s⟩+ 0|o⟩+ 2
5|r⟩.
These outcomes arise from the general formula (4). But we can also understand them at a
more concrete level: for the ﬁrst distribution dO(y) we need to concentrate on the 9 lines
in Figure 1 for which Play is yes; in these lines, in the ﬁrst Outlook column, 2 out of 9
entries are Sunny, 4 out of 9 are Overcast, and 3 out of 9 are Rainy. This corresponds to
the above ﬁrst distribution dO(y). Similarly, the second distribution dO(n) captures the
Outlook for the 5 lines where Play is no: 3 out of 5 are Sunny and 2 out of 5 are Rainy.
5. Almost equality of channels
This section explains how the standard notion of ‘equal up to negligible sets’ or ‘equal
almost everywhere’ (with respect to a measure) can be expressed abstractly using string
diagrams. Via this equality relation Bayesian inversion can be characterised very neatly,
following (Clerc et al., 2017). We consider an aﬃne CD-category, continuing in the setting
of Section 3.
Deﬁnition 5.1. Let c, d: X →Y be two parallel channels, and σ: I →X be a state on
their domain. We say that c is σ-almost equal to d, written as c
σ∼d if
σ
c
=
σ
d
.
It is obvious that
σ∼is an equivalence relation on channels of type X →Y . When S is a
set of arrows of type X →Y , we write S/σ for the quotient S/
σ∼.
To put it more intuitively, we have c
σ∼d iﬀc and d can be identiﬁed whenever the input
wires are connected to σ, possibly through copiers. For instance, using the associativity
‡ This outcome has been calculated with the tool EfProb (Cho and Jacobs, 2017) that can do both
disintegration and inversion.

Disintegration and Bayesian Inversion
17
and commutativity of copiers, by c
σ∼d we may reason as:
σ
c
=
σ
c
=
σ
d
=
σ
d
.
In particular, c
σ∼d if and only if
σ
c
=
σ
d
.
Now the following is an obvious consequence from the deﬁnition.
Proposition 5.2. If both c, d: X →Y are disintegrations of a joint state ω: I →X ⊗Y ,
then c
ω1∼d, where ω1 : I →X is the ﬁrst marginal of ω.
For channels f, g: X →D(Y ) and a state σ ∈D(X) in Kℓ(D), it is easy to see that
f
σ∼g if and only if f(x)(y) · σ(x) = g(x)(y) · σ(x) for all x ∈X and y ∈Y if and only
if f(x) = g(x) for any x ∈X with σ(x) ̸= 0. Almost equality in Kℓ(G) is less trivial but
characterised in an expected way.
Proposition 5.3. Let f, g: X →G(Y ) be channels and µ ∈G(X) a state in Kℓ(G). Then
f
µ∼g if and only if for any B ∈ΣY , f(−)(B) = g(−)(B) µ-almost everywhere.
Proof. By expanding the deﬁnition, f
µ∼g if and only if
Z
A
f(x)(B) µ(dx) =
Z
A
g(x)(B) µ(dx)
for all A ∈ΣX and B ∈ΣY . This is equivalent to f(−)(B) = g(−)(B) µ-almost everywhere
for all B ∈ΣY , see (Fremlin, 2000, 131H).
Almost-everywhere equality of probability kernels f, g: X →G(Y ) is often formulated
by the stronger condition that f = g µ-almost everywhere. The next proposition shows
that the stronger variant is equivalent under a reasonable assumption (any standard Borel
space is countably generated, for example).
Proposition 5.4. In the setting of the previous proposition, additionally assume that
the measurable space Y is countably generated. Then f
µ∼g if and only if f = g µ-almost
everywhere.
Proof. Let the σ-algebra ΣY on Y be generated by a countable family (Bn)n. We may
assume that (Bn)n is a π-system, i.e. a family closed under binary intersections. Let
An = {x ∈X | f(x)(Bn) = g(x)(Bn)}, and A = T
n An. Each An is µ-conegligible, and
thus A is µ-conegligible. For each x ∈A, we have f(x)(Bn) = g(x)(Bn) for all n. By
application of the Dynkin π-λ theorem, it follows that f(x) = g(x). Therefore f = g
µ-almost everywhere.

K. Cho and B. Jacobs
18
We can now present a fundamental result from (Clerc et al., 2017, §3.3) in our abstract
setting. Let C be an aﬃne CD-category and (I ↓C) be the comma (coslice) category.
Objects in (I ↓C) are states in C, formally pairs (X, σ) of objects X ∈C and states
σ: I →X. Arrows from (X, σ) to (Y, τ) are state-preserving channels, namely c: X →Y
in C satisfying c ◦σ = τ. A joint state (X ⊗Y, ω) ∈(I ↓C) is called a coupling of two
states (X, σ), (Y, τ) ∈(I ↓C) if
ω
X
=
σ
X
and
ω
Y
=
τ
Y
.
We write Coupl((X, σ), (Y, τ)) for the set of couplings of (X, σ) and (Y, τ).
Theorem 5.5. Let C be an aﬃne CD-category that admits disintegration. For each pair
of states (X, σ), (Y, τ) ∈(I ↓C), there is the following bijection:
(I ↓C)
 (X, σ), (Y, τ)

/σ
∼=
Coupl((X, σ), (Y, τ))
Proof. For each c ∈(I ↓C)
 (X, σ), (Y, τ)

, we deﬁne a joint state I →X ⊗Y to be
the ‘integration’ of σ and c as below, for which we use the following ad hoc notation:
σ = c :=
σ
c
X
Y
It is easy to check that σ = c is a coupling of σ and τ. For two channels c, d: X →Y , we
have σ = c = σ = d if and only if c
σ∼d, by the deﬁnition of
σ∼. This means the mapping
c 7−→σ = c ,
(I ↓C)
 (X, σ), (Y, τ)

/σ −→Coupl((X, σ), (Y, τ))
is well-deﬁned and injective. To prove the surjectivity let (X⊗Y, ω) ∈Coupl((X, σ), (Y, τ)).
Let c: X →Y be a disintegration of ω. Then c is state-preserving since
τ
=
ω
=
ω
c
=
ω
c
=
σ
c
Moreover we have σ = c = ω, as desired.
Via the symmetry X⊗Y
∼
=
→Y ⊗X we have the obvious bijection Coupl((X, σ), (Y, τ)) ∼=
Coupl((Y, τ), (X, σ)). This immediately gives the following corollary.
Corollary 5.6. Let C be an aﬃne CD-category that admits disintegration. For any
states (X, σ), (Y, τ) ∈(I ↓C) we have
(I ↓C)
 (X, σ), (Y, τ)

/σ
∼=
(I ↓C)
 (Y, τ), (X, σ)

/τ
The bijection sends a channel c: X →Y to a Bayesian inversion d: Y →X for σ along c.

Disintegration and Bayesian Inversion
19
Theorem 2 of (Clerc et al., 2017) is obtained as an instance, for the category pKrnsb.
This bijective correspondence yields a ‘dagger’ (−)† functor on (a suitable quotient of)
the comma category (I ↓C) — as noted by the authors of (Clerc et al., 2017).
Strong almost equality
Unfortunately, the almost equality deﬁned above is not the most useful notion for
equational reasoning between string diagrams. Indeed, later in the proofs of Proposition 6.4
and Theorem 8.3 we encounter situations where we need a stronger notion of almost
equality. We deﬁne the stronger one as follows.
Deﬁnition 5.7. Let c, d: X →Y be channels and σ: I →X be a state. We say that c
is strongly σ-almost equal to d if
ω
c
=
ω
d
holds for any ω: I →X ⊗Z such that σ is the ﬁrst marginal of ω, that is:
σ
=
ω
.
Notice that strong almost quality implies almost quality, via the following ω:
ω
:=
σ
.
The good news is that the converse often holds too. We say that an aﬃne CD-category
admits equality strengthening if c is strongly σ-almost equal to d whenever c
σ∼d, i.e. c is
σ-almost equal to d. We present two propositions that guarantee this property.
Proposition 5.8. If an aﬃne CD-category admits disintegration, then it admits equality
strengthening.
Proof. Suppose that c
σ∼d holds for a state σ: I →X and for channels c, d: X →Y .
Let ω: I →X ⊗Z be a state whose ﬁrst marginal is σ. We can disintegrate ω as in:
ω
=
σ
e
.
Then we have
ω
c
=
σ
c
e
=
σ
d
e
=
ω
d
.

K. Cho and B. Jacobs
20
Therefore c is strongly σ-almost equal to d.
Recall that Kℓ(G) does not admit disintegration. Nevertheless, it admits equality
strengthening.
Proposition 5.9. The category Kℓ(G) admits equality strengthening.
Proof. Assume c
σ∼d for σ ∈G(X) and c, d: X →G(Y ). Let ω ∈G(X ⊗Z) be
a probability measure whose ﬁrst marginal is σ, i.e. (π1)∗(ω) = σ. We need to prove
(c ⊗ηZ) ◦· ω = (d ⊗ηZ) ◦· ω, which is equivalent to:
Z
X×Z
c(x)(B)1C(z) ω(d(x, z)) =
Z
X×Z
d(x)(B)1C(z) ω(d(x, z))
(7)
for all B ∈ΣY and C ∈ΣZ. Using
c(x)(B)1C(z) −d(x)(B)1C(z)
 =
c(x)(B) −d(x)(B)
1C(z) ≤
c(x)(B) −d(x)(B)
 ,
we have

Z
X×Z
 c(x)(B)1C(z) −d(x)(B)1C(z)

ω(d(x, z))

≤
Z
X×Z
c(x)(B)1C(z) −d(x)(B)1C(z)
 ω(d(x, z))
≤
Z
X×Z
c(x)(B) −d(x)(B)
 ω(d(x, z))
=
Z
X
c(x)(B) −d(x)(B)
 (π1)∗(ω)(dx)
=
Z
X
c(x)(B) −d(x)(B)
 σ(dx)
= 0 ,
where the last equality holds since c(−)(B) = d(−)(B) σ-almost everywhere by Proposi-
tion 5.3. Therefore the desired equality (7) holds.
6. Conditional independence
Throughout this section, we consider an aﬃne CD-category that admits disintegration.
6.1. Disintegration of multipartite states
So far we have concentrated on bipartite states — except in the classiﬁcation example in
Section 4. In order to deal with a general n-partite state ω: I →X1 ⊗· · · ⊗Xn, we will
introduce several notations and conventions in Deﬁnitions 6.1, 6.2 and 6.3 below; they
are in line with standard practice in probability theory.
In the conventions, an n-partite state, as below, is ﬁxed, and used implicitly.
ω
. . .
X1 X2
Xn

Disintegration and Bayesian Inversion
21
Deﬁnition 6.1. When we write
. . .
Xi1 Xi2
Xik
where i1, . . . , ik are distinct, it denotes the state I →Xi1 ⊗· · · ⊗Xik obtained from ω by
marginalisation and permutation of wires (if necessary). Let us give a couple of examples,
for n = 5.
X1 X4
:=
ω
X1
X2 X3
X4
X5
X4 X2 X5
:=
ω
X1
X2
X3
X4
X5
We permute wires via a combination of crossing. This is unambiguous by the coherence
theorem.
Below we will use symbols X, Y, Z, W, . . . to denote not only a single wire Xi but also
multiple wires Xi ⊗Xj ⊗· · · . Disintegrations more general than in the bipartite case are
now introduced as follows.
Deﬁnition 6.2. For X = Xi1⊗· · ·⊗Xik and Y = Xj1⊗· · ·⊗Xjl, with all i1, . . . , ik, j1, . . . , jl
distinct, a disintegration X →Y is deﬁned to be a disintegration of
X
Y
,
the marginal state given by the previous convention. We denote the disintegration simply
as on the left below,
Y
X
Y
X
=
X
Y
By deﬁnition, it must satisfy the equation on the right above. Let us give an example.
The disintegration X5 ⊗X2 →X1 ⊗X4 on the left below is deﬁned by the equation on
the right.
X1
X5 X2
X4
X1
X5 X2
X4
=
X1 X4 X5 X2
More speciﬁcally, assuming n = 5 and expanding the notation for marginals, the equation

K. Cho and B. Jacobs
22
is:
ω
X1
X2
X1
X3 X4
X4 X5
=
ω
X1
X2
X3
X4
X5
Note that disintegrations need not be unique. Thus when we write
Y
X
, we in fact choose
one of them. Nevertheless, such disintegrations are unique up to almost-equality with
respect to
X , which is good enough for our purpose.
Finally we make a convention about almost equality (Deﬁnition 5.1).
Deﬁnition 6.3. Let S and T be string diagrams of type X →Y that are made from
marginals and disintegrations of ω as deﬁned in Deﬁnitions 6.1 and 6.2. When we say S
is almost equal to T (or write S ∼T) without reference to a state, it means that S is
almost equal to T with respect to the state
X .
We shall make use of the following auxiliary equations involving discarding and compo-
sition of disintegrations.
Proposition 6.4.
In the conventions and notations above, the following hold.
1
X
Y
Z
∼
X
Z
2
X
Y
W
Z
∼
X
Y
W
Z
Proof. By the deﬁnition of almost equality, 1 is proved by:
X
Y
Z
=
X
Y
Z
=
X
Z
=
X
Z
.

Disintegration and Bayesian Inversion
23
Similarly, we prove 2 as follows.
X
Y W
Z
=
X
Y W
Z
=
X
Y W
Z
=
X
Y W
Z
⋆=
X Z Y W
=
X Z Y W
=
X
Y
Z
W
The marked equality
⋆= holds by strong almost equality, see Proposition 5.8.
The equations correspond respectively to P
y P(x, y|z) = P(x|z) and P(x|y, z)·P(z|y, w) =
P(x, z|y, w) in discrete probability.
Remark 6.5. We here keep our notation somewhat informal, e.g. using symbols X, Y, Z, . . .
as a sort of meta-variables. We refer to (Joyal and Street, 1991; Selinger, 2010) for more
formal aspects of string diagrams.
6.2. Conditional independence
We continue using the notations in the previous subsection. Recall that we ﬁx an n-partite
state
ω
. . .
X1 X2
Xn
and use symbols X, Y, Z, W, . . . to denote a wire Xi or multiple wires Xi ⊗Xj ⊗· · · .
We now introduce the notion of conditional independence. Although it is deﬁned
with respect to the underlying state ω, we leave the state ω implicit, like an underlying
probability space Ωin conventional probability theory.
Deﬁnition 6.6. Let X, Y, Z denote distinct wires. Then we say X and Y are conditionally
independent given Z, written as X ‚ Y | Z, if
X
Y
Z
∼
X
Y
Z
.
The deﬁnition is analogous to the condition P(x, y|z) = P(x|z) P(y|z) in discrete

K. Cho and B. Jacobs
24
probability. Indeed our deﬁnition coincides with the usual conditional independence, as
explained below.
Example 6.7. In Kℓ(D), let cX|Z : Z →D(X), cY |Z : Z →D(Y ), cXY |Z : Z →D(X ×Y )
be disintegrations of some joint state, say ω ∈D(X × Y × Z). Let ωZ ∈D(Z) be the
marginal on Z. Then X ‚ Y | Z if and only if
cXY |Z(z)(x, y) = cX|Z(z)(x) · cY |Z(z)(y)
whenever
ωZ(z) ̸= 0
for all x ∈X, y ∈Y and z ∈Z. If we write P(x, y|z) = cXY |Z(z)(x, y), P(x|z) =
cX|Z(z)(x), P(y|z) = cY |Z(z)(y), and P(z) = ωZ(z), then the condition will look more
familiar:
P(x, y|z) = P(x|z) · P(y|z)
whenever
P(z) ̸= 0 .
Example 6.8. Similarly, in Kℓ(G), let cX|Z : Z →G(X), cY |Z : Z →G(Y ), cXY |Z : Z →
G(X × Y ), and ωZ ∈G(Z) be appropriate disintegrations and a marginal of some joint
probability measure ω. Then X ‚ Y | Z if and only if
cXY |Z(z)(A × B) = cX|Z(z)(A) · cY |Z(z)(B)
for ωZ-almost all z ∈Z
for all A ∈ΣX and B ∈ΣY .
The equivalences in the next result are well-known in conditional probability. Our
contribution is that we formulate and prove them at an abstract, graphical level.
Proposition 6.9.
The following are equivalent.
1
X ‚ Y | Z
2
X Y
Z
=
X
Y
Z
3
X
Y
Z
∼
X
Y
Z
4
X Y
Z
=
X
Y
Z
As we will see below, conditional independence X ‚ Y | Z is symmetric in X and Y .
Therefore the obvious symmetric counterparts of 3 and 4 are also equivalent to them.

Disintegration and Bayesian Inversion
25
Proof. By deﬁnition of almost equality, 1 is equivalent to
X
Y
Z
=
X
Y
Z
=:
X
Y
Z
.
We then have 1 ⇔2, since the identity below holds by the deﬁnition of disintegration.
X
Y
Z
=
X Y
Z
Next, assuming 3, we obtain 4 as follows.
X Y
Z
=
X
Y Z
3=
X Y Z
=
X
Y
Z
We prove 4 ⇒3 similarly. Finally note that the following equation holds.
X
Y
Z
=
X
Y
Z
=
X
Y
Z
.
From this 2 ⇔4 is immediate.
Note that the condition 3 of the proposition is an analogue of P(x|y, z) = P(x|z).
The other conditions 2 and 4 say that the joint state can be factorised in certain ways,
corresponding to the following equations:
P(x, y, z) = P(x|z) P(y|z) P(z) = P(x|z) P(y, z).
The proposition below shows that our abstract formulation of conditional independence
does satisfy the basic ‘rules’ of conditional independence, which are known as (semi-)
graphoids axioms (Verma and Pearl, 1988; Geiger et al., 1990).
Proposition 6.10.
Conditional independence (−) ‚ (−) | (−) satisﬁes:
1
(Symmetry) X ‚ Y | Z if and only if Y ‚ X | Z.
2
(Decomposition) X ‚ Y ⊗Z | W implies X ‚ Y | W and X ‚ Z | W.
3
(Weak union) X ‚ Y ⊗Z | W implies X ‚ Y | Z ⊗W.

K. Cho and B. Jacobs
26
4
(Contraction) X ‚ Z | W and X ‚ Y | Z ⊗W imply X ‚ Y ⊗Z | W.
Proof. We will freely use Proposition 6.9.
(1) Suppose X ‚ Y | Z. Then
Y
X Z
=
X
Y
Z
(X‚Y |Z)
=
Y
X
Z
=
Y
X
Z
.
This means Y ‚ X | Z.
(2) Suppose X ‚ Y ⊗Z | W, namely:
X Y Z W
=
X
Y
Z W
Marginalising Z, we obtain
X Y W
=
X Y
Z
W
=
X
Y
Z
W
=
X
Y
W
,
by Proposition 6.4.1. Thus X ‚ Y | W. Similarly we prove X ‚ Z | W.
Finally, we prove 3 and 4 at the same time. Note that X ‚ Y ⊗Z | W implies
X ‚ Z | W, as shown above. Therefore what we need to prove is that X ‚ Y ⊗Z | W if
and only if X ‚ Y | Z ⊗W, under X ‚ Z | W. Assume X ‚ Z | W, so we have
X
Z
W
∼
X
Z
W
Then
X
Y
Z W
=
X
Y
Z W
=
X
Y
Z
W

Disintegration and Bayesian Inversion
27
=
X
Y
Z
W
=
X
Y
Z
W
=
X
Y
Z W
This proves X ‚ Y ⊗Z | W if and only if X ‚ Y | Z ⊗W.
The four properties from the graphoid axioms are essential in reasoning of conditional
independence with DAGs or Bayesian networks (Verma and Pearl, 1988; Geiger et al.,
1990).
Conditional independence has been studied categorically in (Simpson, 2018). There,
a categorical notion of (conditional) independence structure is introduced, generalising
algebraic axiomatisations of conditional independence, such as with graphoids and sep-
aroids (Dawid, 2001). We leave it to future work to precisely relate our approach to
conditional probability to Simpson’s categorical framework.
7. Beyond causal channels
All CD-categories C that we have considered so far are aﬃne in the sense that all arrows
f : X →Y are causal:
◦f =
. We now drop the aﬃneness, in order to enlarge our
category to include ‘non-causal’ arrows, which enables us to have new notions such as
scalars and eﬀects. Essentially, we lose nothing by this change: all the arguments so far
can still be applied to the subcategory Caus(C) ⊆C containing all the objects and causal
arrows. The category Caus(C) is an aﬃne CD-category, inheriting the monoidal structure
(⊗, I) and the comonoid structures (
,
) from C.
Recall that channels in C are causal arrows, i.e. arrows in Caus(C). States are channels
of the form σ: I →X. We call endomaps I →I on the tensor unit scalars. The set
C(I, I) of scalars forms a monoid via the composition s·t = s◦t and 1 = idI. The monoid
of scalars is always commutative — in fact, this is the case for any monoidal category,
see e.g. (Abramsky and Coecke, 2009, §3.2). In string diagram scalars are written as
s or simply as s. We can multiply scalars s to any arrows f : X →Y by the parallel
composition, or diagrammatically by juxtaposition:
f
s
We call an arrow σ: I →X is normalisable if the scalar
◦σ: I →I is (multiplicatively)
invertible. In that case we can normalise σ into a proper state as follows.
nrm(σ) :=
σ
σ

−1
Eﬀects in C are arrows of the form p: X →I; they correspond to observables, with

K. Cho and B. Jacobs
28
predicates as special case. Diagrammatically they are written as on the left below.
p
σ |= p :=
σ
p
On the right the validity σ |= p of a state σ: I →X and a eﬀect p: X →I is deﬁned.
It is the scalar given by composition. Note that eﬀects are not causal in general; by
deﬁnition, only discarders
are causal ones. States σ: I →X can be conditioned by
eﬀects p: X →I via normalisation, as follows.
σ|p := nrm
 
σ
p
!
=
σ
p
 
!−1
σ
p
The conditional state σ|p is deﬁned if the validity σ |= p is invertible. Conditioning σ|p
generalises conditional probability P(A|B) given event B, see Example 7.2 below. At the
end of the section we will explain how conditioning and disintegration are related.
Recall, from Examples 2.4 and 2.5, that our previous examples Kℓ(D) and Kℓ(G)
are both aﬃne. We give two non-aﬃne CD-categories that have Kℓ(D) and Kℓ(G) as
subcategories, respectively.
Example 7.1. For discrete probability, we use multisets (or unnormalised distributions)
over nonnegative real numbers R≥0 = [0, ∞), such as
1|x⟩+ 0.5|y⟩+ 3|z⟩
on a set X = {x, y, z, . . . }
We denote by M(X) the set of multisets over R≥0 on X. More formally:
M(X) = {φ: X →R≥0 | φ has ﬁnite support} .
It extends to a commutative monad M: Set →Set, see (Coumans and Jacobs, 2013). In
a similar way to the distribution monad D, we can check that the Kleisli category Kℓ(M)
is a CD-category. For a Kleisli map f : X →M(Y ), causality
◦f =
amounts to the
condition P
y f(x)(y) = 1 for all x ∈X. It is thus easy to see that Caus(Kℓ(M)) ∼= Kℓ(D).
In fact, the distribution monad D can be obtained from M as its aﬃne submonad,
see (Jacobs, 2018).
An eﬀect p: X →1 in Kℓ(M) is a function p: X →R≥0. Its validity σ |= p in a state
ω is given by the expected value P
x σ(x) · p(x). The state σ|p updated with ‘evidence’ p
is deﬁned as σ|p(x) = σ(x)·p(x)
σ|=p
.
Example 7.2. For general, measure-theoretic probability, we use s-ﬁnite kernels between
measurable spaces (Kallenberg, 2017; Staton, 2017). Let X and Y be measurable spaces.
A function f : X × ΣY →[0, ∞] is called a kernel from X to Y if
— f(x, −): ΣY →[0, ∞] is a measure for each X; and
— f(−, B): X →[0, ∞] is measurable§ for each B ∈ΣY .
§ The σ-algebra on [0, ∞] is the standard one generated by {∞} and measurable subsets of R≥0.

Disintegration and Bayesian Inversion
29
We write f : X ⇝Y when f is a kernel from X to Y . A probability kernel is a kernel
f : X ⇝Y with f(x, Y ) = 1 for all x ∈X. A kernel f : X ⇝Y is ﬁnite if there exists
r ∈[0, ∞) such that for all x ∈X, f(x, Y ) ≤r. (Note that it must be ‘uniformly’ ﬁnite.)
A kernel f : X ⇝Y is s-ﬁnite if f = P
n fn for some countable family (fn : X →Y )n∈N
of ﬁnite kernels.
For two s-ﬁnite kernels f : X ⇝Y and g: Y ⇝Z, we deﬁne the (sequential) composite
g ◦f : X ⇝Z by
(g ◦f)(x, C) =
Z
Y
g(y, C) f(x, dy)
for x ∈X and C ∈ΣZ. There are identity kernels ηX : X ⇝X given by ηX(x, A) = 1A(x).
With these data, measurable spaces and s-ﬁnite kernels form a category, which we denote
by sfKrn. There is a monoidal structure on sfKrn. For measurable spaces X, Y we deﬁne
the tensor product X ⊗Y = X ×Y to be the cartesian product of measurable spaces. The
tensor unit I = 1 is the singleton space. For s-ﬁnite kernels f : X ⇝Y and g: Z ⇝W,
we deﬁne f ⊗g: X × Z ⇝Y × W by
(f ⊗g)((x, z), E) =
Z
Y
Z
W
1E(y, w) g(z, dw)

f(x, dy)
=
Z
W
Z
Y
1E(y, w) f(x, dy)

g(z, dw)
for x ∈X, z ∈Z, E ∈ΣY ×W . The latter equality holds by the Fubini-Tonelli theorem
for s-ﬁnite measures. These make the category sfKrn symmetric monoidal. Finally, for
each measurable space X there is a ‘copier’
: X ⇝X × X and a ‘discarder’
: X ⇝1,
given by
(x, E) = 1E(x, x) and
(x, 1) = 1, so that sfKrn is a CD-category. For more
technical details we refer to (Kallenberg, 2017; Staton, 2017).
Note that an s-ﬁnite kernel f : X ⇝Y is causal if and only if it is a probability kernel,
which is nothing but a Kleisli map X →G(Y ) for the Giry monad. Therefore the causal
subcategory of sfKrn is the Kleisli category of the Giry monad: Caus(sfKrn) ∼= Kℓ(G).
In particular, states in sfKrn are probability measures σ ∈G(X).
An eﬀect p: X ⇝1 in sfKrn, i.e. an s-ﬁnite kernel p: X × Σ1 →[0, ∞], can be
identiﬁed with a measurable function p: X →[0, ∞]. The validity σ |= p is then the
integral
R
X p(x) σ(dx), deﬁned in [0, ∞]. The conditional state σ|p ∈G(X) is deﬁned by:
σ|p(A) =
R
A p(x) σ(dx)
σ |= p
for A ∈ΣX, when the validity σ |= p is neither 0 nor ∞. In particular, for any ‘event’
B ∈ΣX, the obvious indicator function 1B : X →[0, ∞] is an eﬀect. Then conditioning
yields a new state on X:
σ|1B(A) =
R
A 1B(x) σ(dx)
σ |= 1B
= σ(A ∩B)
σ(B)
for A ∈ΣX.
This amounts to conditional probability P(A|B) = P(A, B)/ P(B) given event B.
We need to generalise deﬁnitions from the previous sections in the non-aﬃne/causal
setting. Here we make only a minimal generalisation that is required in the next section.

K. Cho and B. Jacobs
30
For example, we still restrict ourselves to disintegration of (causal) states, although in
the literature, the notion of disintegration exists also for non-probability (i.e. non-causal)
measures and even for non-ﬁnite measures, see e.g. (Chang and Pollard, 1997, Deﬁnition 1).
We leave such generalisations to future work.
Let σ: I →X be a state. We deﬁne σ-almost equality f
σ∼g between arbitrary arrows
f, g: X →Y in the same way as Deﬁnition 5.1. Similarly, strong σ-almost equality
between arbitrary arrows is deﬁned as in Deﬁnition 5.7 (here ω still ranges over states).
Disintegrations of a joint state ω: I →X ⊗Y are deﬁned as in Deﬁnition 3.5, except
that an arrow c1 : X →Y (or c2 : Y →X) is not necessarily causal. Nevertheless,
disintegrations of a state are almost causal in the following sense.
Deﬁnition 7.3. Let σ: I →X be a state. We say that an arrow c: X →Y is σ-almost
causal if
◦c
σ∼
.
Then the following is immediate from the deﬁnition.
Proposition 7.4. Let c1 : X →Y be a disintegration of a state ω: I →X ⊗Y . Then c1
is ω1-almost causal, where ω1 : I →X is the ﬁrst marginal of ω.
Example 7.5. In sfKrn, a s-ﬁnite kernel f : X ⇝Y is σ-almost causal if and only
if f(x, Y ) = 1 for σ-almost all x ∈X. In that case, we can ﬁnd a probability kernel
f ′ : X ⇝Y such that f
σ∼f ′, by tweaking f in the obvious way. From this it follows that
a disintegration of a joint probability measure ω ∈G(X × Y ) exists in sfKrn if and only
if it exists in Kℓ(G).
By Proposition 5.9, we can strengthen almost equality between channels in sfKrn. It is
easy to see that equality strengthening is valid also for almost causal maps: if f, g: X ⇝Y
are σ-almost causal maps with f
σ∼g, then f, g are strongly σ-almost equal. (It is not
clear whether equality strengthening is valid for arbitrary maps in sfKrn, but we will
not need such a general result in this paper.)
In the remainder of the section, we present a basic relationship between disintegra-
tion and conditioning σ|p, introduced above. We assume a joint state ω with its two
disintegrations c1 and c2 in:
ω
c1
X
Y
=
ω
X
Y
=
ω
c2
X
Y
(8)
We write ω1 and ω2 for the ﬁrst and second marginals of ω. (Thus the equations (8) above
are the same as (3).)
Let q be an eﬀect on Y . It can be extended to an eﬀect 1 ⊗q on X ⊗Y , where:
1 ⊗q
:=
q
X
Y
Then we can form the conditioned state ω|1⊗q. In a next step we take its ﬁrst marginal,

Disintegration and Bayesian Inversion
31
written as
 ω|1⊗q

1. It turns out that, in general, this ﬁrst marginal is diﬀerent from the
original ﬁrst marginal ω1, even though the eﬀect q only applies to the second coordinate.
This is called ‘crossover inﬂuence’ in (Jacobs and Zanasi, 2017). It happens when the
state ω is ‘entwined’, that is, when its two coordinates are correlated.
A fundamental result in this context is that this crossover inﬂuence can also be captured
via the channels c1, c2 that are extracted from ω via disintegrations. This works via eﬀect
transformation c∗(p) := p ◦c and state transformation c∗(σ) := c ◦σ along a channel.
Theorem 7.6. In the above setting, assuming that the relevant conditioned states exist,
there are equalities of states:
ω1|c∗
1(q) =
 ω|1⊗q

1 =
 c2

∗(ω2|q).
(9)
Following (Jacobs and Zanasi, 2016) we can say that the expression on the left in (9)
uses backward inference, and the one on the right uses forward inference.
Proof. We ﬁrst note that the state in the middle of (9) is the ﬁrst marginal of:
ω
q
 
!−1
ω
q
Hence:
 ω|1⊗q

1
=
ω
q
 
!−1
ω
q
(10)
We note that the above scalar (that is inverted) can also be obtained as:
ω
c1
q
=
ω
c1
q
(8)
=
ω
q
(11)
Hence we can prove the equation on the left in (9):
ω1|c∗
1(q)
=
ω
c1
q
 
!−1
ω
c1
q
(8),(11)
=
ω
q
 
!−1
ω
q

K. Cho and B. Jacobs
32
In a similar way we prove the equation on the right in (9), since
 c2

∗(ω2|q) equals:
ω
q
 
!−1
ω
q
c2
=
ω
q
 
!−1
ω
q
c2
(8)
=
ω
q
 
!−1
ω
q
The two equations in Theorem 9 will be illustrated in the ‘disease and mood’ example
below, where a particular state (probability) will be calculated in three diﬀerent ways.
For a gentle introduction to Bayesian inference with channels, and many more examples,
we refer to (Jacobs and Zanasi, 2018).
Remark 7.7. Another way of relating conditioning to disintegration / Bayesian inversion
was pointed out by a reviewer of the paper: for a state σ: I →X and an eﬀect p: X →I,
a conditional state σ|p : I →X is a Bayesian inversion for σ along p. This, however,
involves disintegration of non-causal maps, which we leave to future work.
Disease and mood example
We describe an example of probabilistic (Bayesian) reasoning. The setting is the following.
We consider a joint state about the occurrence and non-occurrence of a disease, written
as a two-element set D = {d, d⊥}, jointly with the occurrence and non-occurrence of a
good mood, written as M = {m, m⊥}, where m⊥stands for a bad (not good) mood. The
joint distribution, called ω ∈D(M × D), that we start from is of the form:
ω = 0.05|m, d⟩+ 0.4|m, d⊥⟩+ 0.5|m⊥, d⟩+ 0.05|m⊥, d⊥⟩.
(12)
Suppose there is a test for the disease with the following sensitivity and speciﬁcity. It is
positive in 90% of all cases of people having the disease, and also 5% positive for people
without the disease. Suppose the disease comes out positive. What is then the mood? It
is expected that the mood will deteriorate, since the disease and the mood are ‘entwined’
(correlated) in the above joint state (12): a high likelihood of disease corresponds to a low
mood.
The test’s sensitivity and speciﬁcity is expressed as a channel s: D →2, where 2 = {t, f},
with:
c(d) =
9
10|t⟩+ 1
10|f⟩
c(d⊥) =
1
20|t⟩+ 19
20|f⟩.
We calculate the ﬁrst and second marginal of ω:
ω1 := ω.[1, 0] = 0.45|m⟩+ 0.55|m⊥⟩
ω2 := ω.[0, 1] = 0.55|d⟩+ 0.45|d⊥⟩.
The a priori probability of a positive test is obtained by state transformation, applied to
the second marginal:
s∗
 ω2

= 0.518|t⟩+ 0.482|f⟩.

Disintegration and Bayesian Inversion
33
As explained above, we are interested in the mood after a positive test. We write tt
for the predicate on 2 = {t, f} given by tt(t) = 1 and tt(f) = 0. We can transform it to
a predicate s∗(tt) on D = {d, d⊥}, namely s∗(tt)(d) =
9
10 and s∗(tt)(d⊥) =
1
20. We now
describe three equivalent ways to calculate the posterior mood, as in Theorem 7.6, with
predicate q = s∗(tt).
1
First we use the truth predicate 1 on M = {m, m⊥}, which is always 1, to extend
(weaken) the predicate s∗(tt) on D to 1 ⊗s∗(tt) on M × D. The latter predicate can
be used to update the joint state ω ∈D(M × D). If we then take the ﬁrst marginal
we obtain the updated mood probability:
 ω

1⊗s∗(tt)

.[1, 0] = 0.126|m⟩+ 0.874|m⊥⟩.
Clearly, a positive test leads to a lower mood: a reduction from 0.45 to 0.126.
2
Next we extract channels c1 : M →D and c2 : D →M from ω via disintegration as
in (8). For instance, c1 is deﬁned as:
c1(m) = ω(m,d)
ω1(m) |d⟩+ ω(m,d⊥)
ω1(m) |d⊥⟩=
1
9|d⟩+ 8
9|d⊥⟩
c1(m⊥) = ω(m⊥,d)
ω1(m⊥) |d⟩+ ω(m⊥,d⊥)
ω1(m⊥) |d⊥⟩=
10
11|d⟩+ 1
11|d⊥⟩.
We can now transform the predicate s∗(tt) on D along c1 to get the predicate
c∗
1(s∗(tt)) = (s1 ◦· c1)∗(tt)) on M given by: d 7→13/90 and d⊥7→181/220. We can now
use this predicate to update the a priori ‘mood’ marginal ω1 ∈D(M). This gives the
same a posteriori outcome as before:
ω1

c∗
1(s∗(tt)) = 0.126|m⟩+ 0.874|m⊥⟩.
3
We can also update the second ‘disease’ marginal ω2 ∈D(D) directly with the predicate
s∗(tt) and then do state transformation along the channel c2 : D →M. This gives the
same updated mood:
(c2)∗
 ω2|s∗(tt)

= 0.126|m⟩+ 0.874|m⊥⟩.
8. Disintegration via likelihoods
We continue in the setting of Section 7 in a CD-category that is not necessarily aﬃne.
The goal of this section is to present Theorem 8.3, which generalises a construction of
Bayesian inversions using densities/likelihoods shown in Example 3.9.
We ﬁrst introduce ‘likelihoods’ in our setting.
Deﬁnition 8.1. We say a channel c: X →Y is represented by a eﬀect ℓon X ⊗Y with
respect to an arrow ν : I →Y if
c
=
ℓ
ν
(13)
We call ℓa likelihood relation for the channel c with respect to ν.

K. Cho and B. Jacobs
34
Interpreted in the category sfKrn, the deﬁnition says: a kernel c: X ⇝Y satisﬁes
c(x, B) =
Z
B
ℓ(x, y) ν(dy)
for a kernel ℓ: X × Y ⇝1 (identiﬁed with a measurable function ℓ: X × Y →[0, ∞]) and
a measure ν : 1 ⇝Y . This is basically the same as what we have in Example 3.9, but
here ν is not necessarily the Lebesgue measure.
Deﬁnition 8.2. Let σ: I →X be a state. An eﬀect p: X →I is σ-almost invertible if
there is an eﬀect q: X →I such that:
p
q
σ∼
.
The deﬁnition allows us to normalise an arrow f : X →Y into an almost causal one,
as follows. If an eﬀect
◦f is σ-almost inverted by q, as on the left below,
f
q
σ∼
f
q
then clearly the arrow X →Y on the right is σ-almost causal.
We can now formulate and prove our main technical result.
Theorem 8.3. Let σ be a state on X, and c: X →Y be a channel represented by a
likelihood relation ℓwith respect to ν as in (13) above. Assume that the category admits
equality strengthening for almost causal maps, and that the eﬀect
ℓ
σ
=
ℓ
σ
is almost invertible w.r.t.
c∗(σ) =
ℓ
ν
σ
.
Then, writing q: Y →I for an almost inverse to the eﬀect, the channel
d: Y →X
:=
ℓ
σ
q
is a Bayesian inversion for σ along c: X →Y . Namely, together they satisfy the equa-
tion (5).

Disintegration and Bayesian Inversion
35
Proof. We reason as follows.
σ
c
d
=
σ
ℓ
ν
ℓ
σ
q
(i)
=
σ
q
ℓ
ν
ℓ
σ
(ii)
=
ν
ℓ
σ
=
σ
ℓ
ν
=
σ
c
For the equality
(i)
= we use associativity and commutativity of copiers
. The equality
(ii)
=
follows by:
σ
q
ℓ
∼
w.r.t.
ℓ
ν
σ
via equality strengthening.
Example 8.4. We instantiate the Theorem 8.3 in sfKrn. Let c: X ⇝Y be a probability
kernel represented by a likelihood relation ℓ: X × Y ⇝1 with respect to ν : 1 ⇝Y .
The relation ℓis identiﬁed with a measurable function ℓ: X × Y →[0, ∞] and ν with a
measure ν : ΣY →[0, ∞]. The equation (13) amounts to
c(x, B) =
Z
B
ℓ(x, y) ν(dy) .
In particular, each ℓ(x, −) is a probability density function, satisfying
R
Y ℓ(x, y) ν(dy) = 1.
Typically, we use the Lebesgue measure as ν, with Y a subspace of R. Let σ: 1 ⇝X be
a probability measure. Then c∗(σ): 1 ⇝Y is given as:
c∗(σ)(B) =
Z
X
c(x, B) σ(dx) =
Z
X
Z
B
ℓ(x, y) ν(dy) σ(dx)
The eﬀect
p: Y ⇝1
=
ℓ
σ
is given as:
p(y) =
Z
X
ℓ(x, y) σ(dx) .
To deﬁne an inverse of p, we claim that 0 < p < ∞, c∗(σ)-almost everywhere. We prove

K. Cho and B. Jacobs
36
that p−1({0, ∞}) = p−1(0) ∪p−1(∞) is c∗(σ)-negligible, as:
c∗(σ)
 p−1(0)

=
Z
X
Z
p−1(0)
ℓ(x, y) ν(dy) σ(dx)
=
Z
p−1(0)
p(x) ν(dy)
=
Z
p−1(0)
0 ν(dy) = 0
and, similarly we have
Z
p−1(∞)
∞ν(dy) =
Z
p−1(∞)
p(x) ν(dy) = c∗(σ)
 p−1(∞)

≤1
but this is possible only when ν(p−1(∞)) = 0, hence c∗(σ)
 p−1(∞)

=
R
p−1(∞) p(x) ν(dy) =
0. Now deﬁne an eﬀect q: Y ⇝1 by
q(y) =
(
p(y)−1
if 0 < p(y) < ∞
0
otherwise.
Then p is c∗(σ)-almost inverted by q. By Theorem 8.3, the Bayesian inversion for σ along
c is given by
d: Y →X
:=
ℓ
σ
q
,
namely,
d(y, A) = q(y)
Z
A
ℓ(x, y) σ(dx)
=
R
A ℓ(x, y) σ(dx)
R
X ℓ(x, y) σ(dx)
whenever
0 <
Z
X
ℓ(x, y) σ(dx) < ∞
(14)
This may be seen as a variant of the Bayes formula. The calculation in Example 3.9 is
reproduced when σ is also given via a density function.
In the end we reconsider the naive Bayesian classiﬁcation example from Section 4.
There we only considered the discrete version. The original source (Witten et al., 2011)
also contains a ‘hybrid’ version, combining discrete and continuous probability.
In that hybrid form the Temperature and Humidity columns in Figure 1 are diﬀerent,
and are given by numerical values. What these values are does not matter too much here,
since they are only used to calculate mean (µ) and standard deviation (σ) values. This is
done separately for the cases where Play is yes or no. It results in the following table.
Temperature
Humidity
Play = yes
µ = 73, σ = 6.2
µ = 79.1, σ = 10.2
Play = no
µ = 74.6, σ = 7.9
µ = 86.2, σ = 9.7
These µ, σ values are used to deﬁne two functions cT : P →G(R) and cH : P →G(R),

Disintegration and Bayesian Inversion
37
with normal distributions N as pdf. To be precise, these channels are deﬁned as follows,
where A ⊆R is a measurable subset.
cT (y)(A) =
Z
A
N(73, 6.2)(x) dx
cH(y)(A) =
Z
A
N(79.1, 10.2)(x) dx
cT (n)(A) =
Z
A
N(74.6, 7.9)(x) dx
cH(y)(A) =
Z
A
N(86.2, 9.7)(x) dx.
These functions cT and cH form channels for the Giry monad G.
The two columns Outlook and Windy in Figure 1 remain the same. The corresponding
maps dO : P →D(O) and dW : P →D(W) for the discrete probability monad D — in
Diagram (6) — are now written as maps cO : P →G(O) and cW : P →G(W) for the
monad G, using the obvious inclusion D ,→G.
We now have four channels cO : P →O, cT : P →R, cH : P →R and cW : P →W. We
combine them, as before, into a single channel c: P →T ×R×R×W. In combination with
the Play marginal distribution π from Section 4, we can compute c’s Bayesian inversion
f : T × R × R × W →G(P) via the formula (14). We apply it to the input data used
in (Witten et al., 2011), and get the following Play distribution.
f(s, 66, 90, t) = 0.207|y⟩+ 0.793|n⟩.
The latter inversion computation produces the probability of 0.207 for playing when
the outlook is Sunny, the temperature is 66 (Fahrenheit), the humidity is 90% and the
windiness is true. The value computed in (Witten et al., 2011) is 20.8%. The minor
diﬀerence of 0.001 with our outcome can be attributed to (intermediate) rounding errors.
Our computation is done via EfProb, using the formula (14).
Acknowledgements
The research leading to these results has received funding from the European Research
Council under the European Union’s Seventh Framework Programme (FP7/2007-2013) /
ERC grant agreement nr. 320571. The ﬁrst author (KC) is also supported by ERATO
HASUO Metamathematics for Systems Design Project (No. JPMJER1603), JST. The
main part of this work was done when the ﬁrst author was a PhD student at Radboud
University, Nijmegen.
References
Abramsky, S. and Coecke, B. (2009). Categorical quantum mechanics. In Handbook of Quantum
Logic and Quantum Structures: Quantum Logic, pages 261–323. Elsevier.
Ackerman, N. L., Freer, C. E., and Roy, D. M. (2011). Noncomputable conditional distributions.
In Logic in Computer Science (LICS 2011), pages 107–116. IEEE.
Barber, D. (2012). Bayesian Reasoning and Machine Learning. Cambridge Univ. Press.
Bernardo, J. and Smith, A. (2000). Bayesian Theory. John Wiley & Sons.
Borgström, J., Gordon, A., Greenberg, M., Margetson, J., and Gael, J. V. (2013). Measure
transformer semantics for Bayesian machine learning. Logical Methods in Comp. Sci., 9(3):1–39.

K. Cho and B. Jacobs
38
Chang, J. T. and Pollard, D. (1997). Conditioning as disintegration. Statistica Neerlandica,
51(3):287–317.
Cho, K. and Jacobs, B. (2017). The EfProb library for probabilistic calculations. In Conference
on Algebra and Coalgebra in Computer Science (CALCO 2017), volume 72 of LIPIcs. Schloss
Dagstuhl.
Cho, K., Jacobs, B., Westerbaan, A., and Westerbaan, B. (2015). An introduction to eﬀectus
theory. Preprint. arXiv:1512.05813v1 [cs.LO].
Clerc, F., Danos, V., Dahlqvist, F., and Garnier, I. (2017). Pointless learning. In Foundations of
Software Science and Computation Structures (FoSSaCS 2017), volume 10203 of Lect. Notes
Comp. Sci., pages 355–369. Springer.
Coecke, B. (2016). Terminality implies no-signalling ...and much more than that. New Generation
Computing, 34(1):69–85.
Coecke, B. and Kissinger, A. (2017). Picturing Quantum Processes: A First Course in Quantum
Theory and Diagrammatic Reasoning. Cambridge University Press.
Coumans, D. and Jacobs, B. (2013). Scalars, monads and categories. In Heunen, C., Sadrzadeh,
M., and Grefenstette, E., editors, Quantum Physics and Linguistics. A Compositional, Dia-
grammatic Discourse, pages 184–216. Oxford Univ. Press.
Culbertson, J. and Sturtz, K. (2014). A categorical foundation for Bayesian probability. Applied
Categorical Structures, 22(4):647–662.
D’Ariano, G. M., Chiribella, G., and Perinotti, P. (2017). Quantum Theory from First Principles:
An Informational Approach. Cambridge University Press.
Dawid, A. (2001). Separoids: A mathematical framework for conditional independence and
irrelevance. Annals of Mathematics and Artiﬁcial Intelligence, 32(1):335–372.
Doberkat, E.-E. (2007).
Stochastic Relations: Foundations for Markov Transition Systems.
Chapman & Hall/CRC.
Faden, A. M. (1985). The existence of regular conditional probabilities: Necessary and suﬃcient
conditions. The Annals of Probability, 13(1):288–298.
Fong, B. (2012). Causal theories: A categorical perspective on Bayesian networks. Master’s
thesis, Univ. of Oxford. arXiv:1301.6201 [math.PR].
Fremlin, D. H. (2000). Measure Theory (5 volumes). Torres Fremlin.
Geiger, D., Verma, T., and Pearl, J. (1990). Identifying independence in Bayesian networks.
Networks, 20:507–534.
Giry, M. (1982). A categorical approach to probability theory. In Categorical Aspects of Topology
and Analysis, volume 915 of Lecture Notes in Mathematics, pages 68–85. Springer.
Gordon, A., Henzinger, T., Nori, A., and Rajamani, S. (2014). Probabilistic programming. In
Future of Software Engineering, pages 167–181. ACM.
Jacobs, B. (2015). New directions in categorical logic, for classical, probabilistic and quantum
logic. Logical Methods in Comp. Sci., 11(3):1–76.
Jacobs, B. (2018). From probability monads to commutative eﬀectuses. Journ. of Logical and
Algebraic Methods in Programming, 94:200–237.
Jacobs, B., Westerbaan, B., and Westerbaan, A. (2015). States of convex sets. In Foundations
of Software Science and Computation Structures (FoSSaCS 2015), volume 9034 of Lect. Notes
Comp. Sci., pages 87–101. Springer.
Jacobs, B. and Zanasi, F. (2016). A predicate/state transformer semantics for Bayesian learning.
In Mathematical Foundations of Programming Semantics (MFPS XXXII), volume 325 of Elect.
Notes in Theor. Comp. Sci., pages 185–200. Elsevier.
Jacobs, B. and Zanasi, F. (2017). A formal semantics of inﬂuence in Bayesian reasoning. In
Mathematical Foundations of Computer Science (MFCS 2017), volume 83 of LIPIcs. Schloss
Dagstuhl.

Disintegration and Bayesian Inversion
39
Jacobs, B. and Zanasi, F. (2018).
The logical essentials of Bayesian reasoning.
Preprint.
arXiv:1804.01193v2 [cs.AI].
Joyal, A. and Street, R. (1991). The geometry of tensor calculus, I. Advances in Mathematics,
88(1):55–112.
Kallenberg, O. (2017). Random Measures, Theory and Applications. Springer.
Katoen, J.-P., Gretz, F., Jansen, N., Lucien Kaminski, B., and Olmedo, F. (2015). Understanding
probabilistic programs. In Correct System Design, volume 9360 of Lect. Notes Comp. Sci.,
pages 15–32. Springer.
Mac Lane, S. (1998). Categories for the Working Mathematician. Springer, second edition.
Pachl, J. K. (1978). Disintegration and compact measures. Mathematica Scandinavica, 43:157–168.
Panangaden, P. (2009). Labelled Markov Processes. Imperial College Press.
Pawitan, Y. (2001). In All Likelihood. Statistical Modelling and Inference Using Likelihood.
Clarendon Press.
Pearl, J. (1988). Probabilistic Reasoning in Intelligent Systems. Morgan Kaufmann.
Pollard, D. (2002). A User’s Guide to Measure Theoretic Probability. Cambridge University
Press.
Selinger, P. (2010). A survey of graphical languages for monoidal categories. In New Structures
for Physics, volume 813 of Lecture Notes in Physics, pages 289–355. Springer.
Shan, C.-c. and Ramsey, N. (2017). Exact Bayesian inference by symbolic disintegration. In
Principles of Programming Languages (POPL 2017), pages 130–144. ACM.
Simpson, A. (2018). Category-theoretic structure for independence and conditional independence.
In Mathematical Foundations of Programming Semantics (MFPS XXXIII), volume 336 of
Elect. Notes in Theor. Comp. Sci. Elsevier.
Staton, S. (2017). Commutative semantics for probabilistic programming. In European Symposium
on Programming (ESOP 2017), volume 10201 of Lect. Notes Comp. Sci., pages 855–879.
Springer.
Staton, S., Yang, H., Heunen, C., Kammar, O., and Wood, F. (2016). Semantics for probabilistic
programming: higher-order functions, continuous distributions, and soft constraints. In Logic
in Computer Science (LICS 2016), pages 525–534. ACM.
Stoyanov, J. M. (2014). Counterexamples in Probability. Dover, third edition.
Verma, T. and Pearl, J. (1988). Causal networks: Semantics and expressiveness. In Fourth
Conference on Uncertainty in Artiﬁcial Intelligence (UAI 1988), pages 352–359.
Witten, I., Frank, E., and Hall, M. (2011). Data Mining – Practical Machine Learning Tools and
Techniques. Elsevier.

