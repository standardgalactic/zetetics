Causal and Interventional Markov Boundaries
Soﬁa Triantaﬁllou1
Fattaneh Jabbari1
Gregory F. Cooper1
1Department of Biomedical Informatics, University of Pittsburgh, Pittsburgh, Pennsylvania, USA
Abstract
Feature selection is an important problem in ma-
chine learning, which aims to select variables that
lead to an optimal predictive model. In this paper,
we focus on feature selection for post-intervention
outcome prediction from pre-intervention vari-
ables. We are motivated by healthcare settings,
where the goal is often to select the treatment that
will maximize a speciﬁc patient’s outcome; how-
ever, we often do not have sufﬁcient randomized
controlled trial data to identify well the conditional
treatment effect. We show how we can use ob-
servational data to improve feature selection and
effect estimation in two cases: (a) using observa-
tional data when we know the causal graph, and (b)
when we do not know the causal graph but have
observational and limited experimental data. Our
paper extends the notion of Markov boundary to
treatment-outcome pairs. We provide theoretical
guarantees for the methods we introduce. In simu-
lated data, we show that combining observational
and experimental data improves feature selection
and effect estimation.
1
INTRODUCTION
Feature selection is a fundamental problem in machine learn-
ing that aims to select the minimal set of features that lead
to the optimal prediction of a target variable Y . For ob-
servational distributions, this set is the Markov boundary
of Y , MB(Y ). In causal graphical models, this set can be
identiﬁed from the causal graph G [Pearl, 2000]. This set
exhausts the predictive information for the state of a vari-
able Y , and can be used to obtain the best (and minimal)
predictive model P(Y |MB(Y )) for Y .
In decision making, we are often interested in ﬁnding the
optimal predictive model for the post-intervention distri-
bution of an outcome Y after we intervene on a treat-
ment X, when we only have observational data. Ideally,
we would like to include in our model the Markov bound-
ary Z of Y in the post-intervention causal graph that is
parameterized with the post-intervention distribution. How-
ever, under causal insufﬁciency in which latent confounding
may exist, the conditional post-interventional distribution
P(Y |do(X), Z) may not be identiﬁable. For example, in
Fig. 1, P(Y |do(X), A, B) is not identiﬁable from the obser-
vational distribution alone. In this case, we are interested in
identifying the optimal set Z for which the post-intervention
distribution P(Y |do(X), Z) is identiﬁable from observa-
tional data, which we call the causal Markov boundary.
Moreover, even when experimental data are available, they
typically have much smaller sample sizes and are not pow-
ered to identify conditional distributions. In that case, we
would like to combine large observational data with limited
experimental data to improve interventional feature selec-
tion and effect estimation.
Our methods are heavily motivated by embedded clinical
trials [Angus, 2015, Angus et al., 2020], which take place
within usual clinical care. In these trials, patients who agree
to participate are randomized to receive a treatment from
among those considered effective for that patient. The elec-
tronic health records (EHRs) of the health system in which
the trial is being conducted contains both experimental data
from the trial, and observational data obtained outside (e.g.,
before/after) the trial, all measuring the same variables.
Combining observational and experimental data has the
potential to better predict the most effective treatments for
individual patients, than either type of data alone.
Our contributions are the following:
• We deﬁne the interventional Markov boundary MBX(Y )
and the causal Markov boundaries CMBX(Y ) for an
outcome Y and a treatment X. These sets correspond
to the minimal set of covariates Z that are maximally
informative for Y |do(X), from experimental and obser-
vational data, respectively (Sec. 3). Table 1 summarizes
Proceedings of the Thirty-Seventh Conference on Uncertainty in Artiﬁcial Intelligence (UAI 2021), PMLR 161:1434–1443.

Table 1: Different Markov boundaries discussed in this paper.
Observational Markov Boundary (OMB) of
Y : MB(Y )
The Markov boundary of Y . Leads to optimal prediction of Y from
observational data.
Interventional Markov Boundary (IMB) of
Y relative to X: MBX(Y )
The Markov boundary of Y in the post-intervention distribution Px.
Leads to optimal prediction of Yx from experimental data.
Causal Markov Boundaries (CMB) of Y
relative to X: CMBX(Y )
Sets of measured variables that satisfy Deﬁnition 3.2. Possibly not
unique, and possibly empty. If not empty, one of the CMBs leads to
the optimal prediction of Yx from observational data.
the types of Markov boundaries discussed in this paper.
• We present a Bayesian method that combines obser-
vational and experimental data to learn interventional
Markov boundaries. The method provides estimates of
the post-interventional distribution that are based on both
observational and experimental data, when possible (Sec.
4), in which case the IMB is a CMB. In simulated data, we
show that our method improves causal effect estimation
(Sec. 6).
2
PRELIMINARIES
We use the framework of semi-Markovian causal models
[SMCMs, Tian and Shpitser, 2003], and assume the reader
is familiar with related terminology. Variables are denoted
in uppercase, their values in lowercase, and variable sets in
bold. We use G to denote a causal graph, and say G induces
a probability distribution P if P factorizes according to G.
We use Y |do(X) or YX to denote a variable Y after the
hard intervention on variable X. If we know the SMCM
G, a hard intervention in which a treatment X is set to x
can be represented with the do-operator, do(X=x). We use
Px to denote the interventional distribution over the same
variables for do(X=x). In the corresponding graph, this is
equivalent to removing all incoming edges into X, while
keeping all other mechanisms intact. We use GX to denote
the graph stemming from G after removing edges into X.
We use GX to denote the graph stemming from G after
removing edges out of X. We use the terms PaG(Z) and
ChG(Z) to denote the set of parents and children of Z in G,
X
A
B
Y
Figure 1:
An example SMCM. {X, A, B} is the
Markov boundary of Y in GX . P(Y |do(X), A, B) is
not identiﬁable from the observational distribution P, but
P(Y |do(X), A) and P(Y |do(X), B) are. {X, B} is the
causal Markov boundary for YX.
respectively. The set of variables that are connected with a
variable Y through a bidirected path (i.e., a path that only
has bidirected edges) is called the district of Y and denoted
DisG(Y ).
3
MARKOV BOUNDARIES
A Markov blanket of a variable Y in a set of variables V is
a subset Z of V conditioned on which other variables are
independent of Y : Y ⊥⊥V\Z|Z. The Markov boundary of Y
is the Markov blanket that is also minimal (i.e., no subset of
the Markov boundary is a Markov blanket) [Pearl, 2000]. In
distributions that satisfy the intersection property (including
faithful distributions), the Markov boundary of a variable Y
is unique [Pearl, 1988]. To distinguish from other types of
Markov boundaries deﬁned in this work, we often use the
terminology observational Markov boundary (OMB) to
denote the Markov boundary of a variable.
For a DAG G, the OMB of a variable Y in any distribution
faithful to G is the set parents, children, and spouses of
Y : MB(Y ) = PaG(Y ) ∪ChG(Y ) ∪PaG(ChG(Y )). For
SMCMs, it has been shown that the OMB of a variable Y is
the set of parents, children, children’s parents (spouses) of
Y , district of Y and districts of the children of Y , and the
parents of each node of these districts [Richardson, 2003,
Pellet and Elisseeff, 2008]1.
The OMB has been shown to be the minimal set of vari-
ables with optimal predictive performance for a given distri-
bution and response variable, given some assumptions on
the learner and the loss function [Tsamardinos and Aliferis,
2003]. In this work, we are interested in the model that gives
the optimal prediction of the post-intervention distribution,
with the goal of designing optimal policies. For this reason,
we do not include post-intervention covariates in this model,
because these variables are not known prior to treatment
assignment, and thus, cannot affect the assignment. In the
rest of this document, we make the following assumption:
Assumption 3.1. Covariates V are pre-treatment.
Making this assumption also simpliﬁes the expressions for
the OMBs, because we no longer need to consider children
1Pellet and Elisseeff [2008] prove this for maximal ancestral
graphs, but the proof can be readily adapted to SMCMs.
1435

of Y and their districts. Knowing the OMB allows a more
efﬁcient representation of the conditional distribution of Y
given V, since the following equation holds:
P(Y |V) = P(Y |MB(Y )).
(1)
3.1
INTERVENTIONAL MARKOV BOUNDARY
Our goal is to identify the set of variables that lead to the op-
timal model for the post-intervention distribution of a target
Y relative to a speciﬁc treatment X. We call this set the in-
terventional Markov boundary (IMB) of Y relative to X,
and denote it MBX(Y ). Obviously, MBX(Y ) ⊆MB(Y).
When we have data from the post-intervention distribution,
we can apply statistical methods for OMB identiﬁcation to
obtain the IMB of Y relative to X. However, experimental
data are often limited in sample sizes, while OMB identiﬁ-
cation methods may require large sample sizes.
If we know the causal graph G, the post-intervention dis-
tribution with respect to X is induced by the manipulated
graph GX . The IMB of Y is then the OMB of Y in GX ,
and can be identiﬁed using the deﬁnition of the Markov
boundary above. However, the post-intervention distribu-
tion P(Y |do(X), MBX(Y ) \ X), may not be identiﬁable
from the observational distribution. For example, in Fig.
1, MBX(Y ) = {X, A, B}, but P(Y |do(X), A, B) is not
identiﬁable from observational data. We then want to answer
the following question: What is the best model for predict-
ing YX from the observational distribution, when the causal
graph is known?
3.2
CAUSAL MARKOV BOUNDARIES
To answer this question, we deﬁne the causal Markov
boundaries of an outcome Y relative to a treatment X
as follows:
Deﬁnition 3.2. Let Z ⊆(V ∪X), and W = Z \ X. Then
Z is a causal Markov boundary (CMB) for Y relative to X
if it satisﬁes the following properties:
1. P(Y |do(X), W) is identiﬁable from P(X, Y, V).
2. For
every
subset
W′
of
V
\
W
either
P(Y |do(X), W, W′)=P(Y |do(X), W)
or
P(Y |do(X), W, W′)
is
not
identiﬁable
from
P(X, Y, V).
3. ∄W′ ⊂W s.t. P(Y |do(X), W′)=P(Y |do(X), W).
Condition (1) ensures that the post-intervention conditional
probability of YX given a CMB is identiﬁable. Condition
(2) states that the covariates that are not in that CMB are
either redundant for the prediction of YX given the CMB, or
they make the post-intervention distribution non-identiﬁable.
Condition (3) ensures that Z is additionally maximally in-
formative for YX in the sense that you cannot remove any
X
A
B
C
D
Y
Figure 2: Causal Markov boundaries are not necessarily
unique. Both {X, B, C} and {X, A, D} are causal Markov
boundaries for Y relative to X.
variable from Z without losing some information for YX.
This condition rules out sets like {X, A} in Fig. 1, where,
while P(Y |do(X), A) is identiﬁable from P, it is equal to
P(Y |do(X)). Thus, conditioning on A does not improve
the prediction of YX compared to its subset ∅.
Notice that this deﬁnition does not capture the spirit of
Markov boundaries precisely: Markov boundaries make all
remaining variables redundant for predicting Y ; however,
this does not necessarily hold with CMBs. For example,
in Fig. 1, {X, B} is a CMB according to the deﬁnition
above, but {A} remains relevant for predicting YX; however,
including it with B in the CMB leads to non-identiﬁability.
A CMB is not necessarily unique; it is possible that mul-
tiple sets satisfy Deﬁnition 3.2. For example, assume the
distribution P is induced by the SMCM shown in Fig. 2.
Both {X, B, C} and {X, A, D}, satisfy Deﬁnition 3.2. The
best predictive CMB for predicting YX will depend on the
parameters in P. We use the notation CMBX(Y ) to denote
the set of causal Markov boundaries of Y relative to X.
Thus, we will generally need to ﬁnd all CMBs and then
determine which of them leads to the best prediction of
YX. Also, notice that the CMBX(Y ) can be empty; thus,
no subset of V satisﬁes the Deﬁnition 3.2. This can happen
for example if X →Y and X ↔Y in G.
The CMB is useful in determining a minimal set of max-
imally predictive variables for which we can use observa-
tional data to predict post-interventional distributions. In
the next section we show that, for pre-treatment covariates,
CMBs satisfy the backdoor criterion and are subsets of the
observational Markov boundary. These results enable more
efﬁcient algorithms for ﬁnding CMBs, limiting the types
of estimators and the number of variable sets we need to
consider.
3.2.1
Characterization
Given a graph G, Shpitser and Pearl [2006a] provide a sound
and complete algorithm (IDC) for estimating conditional
post-intervention distributions from observational distribu-
tions induced by G. The output of this algorithm is an expres-
sion for P(Y |do(X), W) if the distribution is identiﬁable
1436

from distribution P and G, or N/A otherwise. Thus, we can
identify CMBs in a brute-force way by running IDC for
every possible subset of V, and then check for sets that
satisfy the conditions in Deﬁnition 3.2. This process is com-
putationally expensive and would not be possible for graphs
with more than a few variables.
In this section, we provide theoretical results that lead to a
much easier process when all candidate conditioning vari-
ables are pre-treatment (all proofs can be found in the supple-
mentary). For pre-treatment covariates, one obvious family
of sets for which the conditional post-intervention distribu-
tions are identiﬁable are sets that m-separate X and Y in GX.
These sets satisfy Rule 2 of do-calculus [Pearl, 2000], so
the conditional interventional distribution P(Y |do(X), W)
is equal to the observational distribution P(Y |X, W). Sets
of pre-treatment covariates that m-separate X and Y in GX
are also known to satisfy the backdoor criterion [Van der
Zander et al., 2014] and the adjustment criterion [Shpitser
et al., 2010]. However, these deﬁnitions are more general
to include possible post-treatment covariates, and are in-
tended for estimating marginal post-intervention distribu-
tions (or average effects). For brevity, we will call sets that
m-separate X and Y in GX backdoor sets, since they block
all backdoor paths between X and Y .
One question that arises is if there are sets that are not back-
door sets that may satisfy the conditions in Deﬁnition 3.2.
In that case, identiﬁability could stem from some sequen-
tial application of do-calculus rules. As we show next, this
is not possible for pre-treatment covariates. This ensures
that we only need to check CMB Conditions (2) and (3)
for sets for which P(Y |do(X), W) = P(Y |X, W). This
makes the identiﬁcation of P(Y |do(X), W) more straight-
forward than having to compute more complex probability
expressions.
Theorem 3.3. We assume that Px and GX are faithful to
each other. If Z is a CMB for Y relative to X, then Z \ X
is a backdoor set for X relative to Y .
The second theoretical result is that any CMB of Y relative
to X is a subset of the OMB of Y . While this sounds intu-
itive, it is not completely straightforward. It could be the
case that conditioning on every subset of the OMB opens
some m-connecting path between X and Y , that can only
be blocked by a variable that is not a member of the Markov
boundary. The following theorem proves that this is not
possible, allowing for more efﬁcient search algorithms:
Theorem 3.4. We assume that Px and GX are faithful to
each other. Every CMB Z of an outcome variable Y w.r.t a
treatment variable X is a subset of the OMB MB(Y ).
Based on Theorem 3.4, we only need to look for CMBs
within subsets of the OMB of Y . So far, we have shown that
both the IMB and any CMB are subsets of the OMB. We
can also show that when the IMB is a CMB, then it also
coincides with the OMB:
Theorem 3.5. If MBX(Y ) is a causal Markov boundary,
then MBX(Y ) = MB(Y ).
4
COMBINING OBSERVATIONAL AND
EXPERIMENTAL DATA
When the causal graph is known, we can obtain CMBs
by looking for subsets of MB(Y ) that satisfy Deﬁnition
3.2. Unfortunately, in most real-world applications, the true
graph is unknown, and selecting the causal/interventional
Markov boundary is not possible from observational data
alone. Experimental data may exist, but are typically much
fewer than observational data, due to expense or ethical con-
cerns. This scenario is common in embedded trials, where
non-randomized patients are much more common than trial
participants. In such cases, the experimental data may be
underpowered to accurately estimate conditional effects. As
a result, the conditional effects that can be derived from
the experimental data have high variance and may not be
reliable. In this case, combining all data (observational and
experimental) in a Bayesian manner may help improve the
prediction of Yx.
We assume that we have observational data Do and exper-
imental data De measuring treatment X, outcome Y , and
pre-treatment covariates V. We use No, Ne to denote the
number of samples in Do, De, respectively.
We present a Bayesian method, called FindIMB, that
uses both De and Do to estimate the probability of a
set being the MBX(Y ) , and estimate P(Y |do(X), V) =
P(Y |do(X), MBX(Y ) \ X). The method is presented in
Alg. 1. It ﬁrst estimates the OMB of Y in observational
data MB(Y ) (Line 1), and then looks among subsets of
MB(Y ) for sets that are IMBs (Line 2). It uses De and Do
to evaluate the probability that a set is an IMB (Line 3), and
then returns a weighted average for P(Y |do(X), V) based
on these probabilities (Line 5).
The enabling idea of the method is that, when the IMB is
a CMB, we can use both the Do and De to estimate the
conditional post-intervention distribution. Otherwise, we
use only De to derive the estimate. We use the following
notation to express these hypotheses:
• Hc
Z is a binary variable denoting the hypothesis that
Z is the IMB MBX(Y ) , and it is also a CMB: Z =
MBX(Y ) ∧Z ∈CMBX(Y ) .
• Hc
Z is a binary variable denoting the hypothesis that
Z is the IMB MBX(Y ) , but it is not a CMB: Z =
MBX(Y ) ∧Z ̸∈CMBX(Y ) .
For a set Z⋆, if either Hc
Z⋆or Hc
Z⋆is true, Z⋆is an IMB
and therefore P(Y |do(X), V) = P(Y |do(X), Z⋆\ X).
1437

Under Hc
Z⋆, however, Z⋆is also a CMB and therefore the
pre- and post- intervention distributions are the same, i.e.,
P(Y |do(X), Z⋆\ X, Hc
Z⋆) = P(Y |X, Z⋆\ X).
(2)
In contrast, under Hc
Z⋆(i.e., Z⋆is an IMB but not a CMB),
P(Y |do(X), Z⋆\ X) is not identiﬁable from observational
data.
Corollary 4.1. Under Hc
Z⋆, P(Y |do(X), Z⋆\ X) is not
identiﬁable from observational data.
Corollary 4.1 stems from the fact that Hc
Z⋆
is true only
when Z⋆is the IMB, and from the deﬁnition of the CMB.
Speciﬁcally, if P(Y |do(X), MBX(Y ) \ X) was identiﬁ-
able from observational data, then it would satisfy all con-
ditions in Deﬁnition 3.2, and it would therefore be a CMB.
Therefore, if Hc
Z⋆holds, P(Y |do(X), MBX(Y ) \ X) is
not identiﬁable from P, and we cannot use Do to estimate
P(Y |do(X), Z⋆\ X)2. Thus, if Hc
Z⋆
holds, we can use
both Do and De in our estimation of P(Y |do(X), Z⋆\ X),
while if Hc
Z⋆holds we can only use De.
Based
on
this
observation,
we
want
to
compute
P(Hc
Z |De, Do) and P(Hc
Z |De, Do) for possible IMBs
Z. These probabilities tell us both how likely it is that Z is
an IMB (their sum), and if we can include observational data
in the estimation of P(Y |do(X), V). Using Bayes’ rule, we
obtain:
P(Hc
Z′ |De, Do) =
P(De|Do, Hc
Z′ )P(Do|Hc
Z′ )P(Hc
Z′ )
X
Z
X
C=c,c
P(De|Do, HC
Z )P(Do|HC
Z )P(HC
Z )
.
(3)
We can similarly derive P(Hc
Z |De, Do) by replacing each
appearance of c with c in the numerator. The denominator is
the same for all sets. P(Hc
Z′ ) and P(Hc
Z′ ) are our priors
that Hc
Z′
and Hc
Z′
hold, respectively. We set this to be
uniform over both values of C and all Z.
As Eq. 3 shows, using Bayes’ rule, we can estimate the
posterior probabilities for the set of hypotheses Hc
Z and
Hc
Z
using marginal likelihoods of the experimental and
observational data. In the next sections, we show how we
can compute each term in Eq. 3.
We present our results for multinomial distributions, but
we believe the method can be readily extended to any type
of distribution with closed-form marginals. For Gaussian
distributions, the equations in this paper can be computed
using results provided in [Heckerman and Geiger, 1995].
Due to space constraints, the closed-form solution for each
equation appearing in the remainder of the paper is presented
in Supplementary Table S1.
2Notice however that Do may still place some constraints on
P(Y |do(X), Z⋆\ X), like for example provide bounds.
Estimating P(De|Do, Hc
Z′ ), P(De|Do, Hc
Z′ ):
Let W = Z \ X, and let θYx|W be a set of parameters ex-
pressing the conditional probabilities for P(Y |do(X), W).
Also, let θY |X,W denote the observational parameters for
P(Y |X, W). By integrating over all θYx|W, we obtain
P(De|Do, Hc
Z ) =
Z
Yx|W
P(De|θYx|W)f(θYx|W|Do, Hc
Z )dθYx|W,
(4)
where f(θYx|W|Do, Hc
Z ) is the posterior for θYx|W given
the observational data, when Z is the IMB and the CMB. In
this case, P(Y |do(X), W) = P(Y |X, W), and therefore
f(θYx|W|Do, Hc
Z ) = f(θY |X,W|Do). Eq. 4 can then be
rewritten in terms of the observational parameters as
P(De|Do, Hc
Z ) =
Z
θY |X,W
P(De|θY |X,W)f(θY |X,W|Do)dθY |X,W.
(5)
Eq. 5 is the marginal likelihood of Y in experimental data,
with parameter density f(θY |X,W|Do) being equal to the
parameter posterior given Do. In other words, under Hc
Z ,
the observational and experimental parameters coincide.
Therefore, Do gives us a strong "prior" for De. Eq. 5 can be
computed in closed-form for distributions with conjugate
priors.
Under Hc
Z , the equality of the observational and experi-
mental parameters does not hold, and we cannot use the
θY |X,W to inform θYX|W, at least not in a straightforward
way. Instead, we model that f(θYX|W|Do) = f(θYX|W).
Then P(De|Do, Hc
Z ) corresponds to the marginal likeli-
hood of Y in the experimental data, using a prior that we
model as being non-informative.
Estimating P(Do|Hc
Z ), P(Do|Hc
Z ):
These probabilities score how well the observational data ﬁt
with the hypotheses Hc
Z , Hc
Z . We can derive these terms
on the basis of the OMB and its connection to the IMB and
the CMBs. We ﬁrst need to express the hypothesis that a
set U is the OMB of Y : Let Ho
U denote this hypothesis;
thus, for any U ⊆V ∪X, Ho
U is true iff U is the OMB
for Y . Then we can write
P(Do|HC
Z ) =
X
U⊆V∪X
P(Do|Ho
U )P(Ho
U |HC
Z ),
(6)
for C = c, c. Under Hc
Z , Theorem 3.5 implies that
P(Ho
U |Hc
Z ) = 1 if U = Z, and zero otherwise. Un-
der Hc
Z , the IMB is not a CMB. Instead, the IMB has to be
a subset of U, therefore P(Ho
U |HC
Z ) = 0 for any Z ⊃U.
P(Do|Ho
U ) is the marginal likelihood of Y in Do, under
the hypothesis that U is the data-generating OMB for Y in
the observational data. We can obtain this likelihood using
a Bayesian scoring algorithm like FGES [Ramsey et al.,
2017], by scoring a DAG where Y is a child of variables U
1438

(and no other edges are in the graph). We call this algorithm
FGESMB. For discrete variables, in the large sample limit
this probability will be maximum only for the true OMB:
Theorem 4.2. Given dataset Do that contains samples from
a strictly positive distribution P, which is a perfect map
for a SMCM G, the BD score [Heckerman et al., 1995] will
assign the highest score to the OMB of Y in the large sample
limit.
Eq. 6 needs to be computed for all subsets of the covariate
sets. In practice, however, for large No, these probabilities
are dominated by the true OMB. Assuming our sample is
large enough, we can use an algorithm with asymptotic
guarantees for identifying the true OMB. In fact, once we
commit to the observational Markov boundary U⋆, Eq. 6
leads to the following equations
P(Do|Hc
Z )=P(Do|Ho
U⋆) for Z=U⋆
P(Do|Hc
Z ) = P(Do|Ho
U⋆) for all Z ⊆U⋆.
(7)
For the remaining cases (i.e., Hc
Z and Z ̸= U⋆, or Hc
Z and
Z ⊃U⋆), the corresponding probabilities are zero.
Bayesian estimation of P(Y |do(X), V, De, Do):
We now compute the P(Y |do(X), V) using Bayesian
model averaging over the hypotheses HC
Z . Let x, y, V = v,
denote given instances of X, Y and V, respectively. When
V = v, we use W = wv to denote the corresponding val-
ues of a set W ⊂V. Recall that under Hc
Z and under Hc
Z ,
P(Y |do(X), V) = P(Y |do(X), W), where W = Z \ X.
Then for a given instance of V = v, we have
P(y|do(x), v, De, Do) =
X
Z⊂V
X
C=c,c
P(y|do(x), wv, De, Do, HC
Z )P(HC
Z |Do, De).
(8)
This equation computes the expectation of the condi-
tional probability parameter. The individual probabilities
P(y|do(x), wv, De, Do, HC
Z ) can be estimated as poste-
rior expectations of P(Y |do(X), W) from the data. Specif-
ically, under given Hc
Z , P(Y |do(X), W) = P(Y |X, W),
and therefore we can use both De and Do for the posterior
expectation. In contrast, under Hc
Z , we only use De. An-
alytical equations for these probabilities for multinomial
distributions can be found in Supplementary Table S1.
5
RELATED WORK
We are not aware of other methods that try to identify causal
and interventional Markov boundaries. Our work has con-
nections and builds on work from many different areas. Due
to space constraints, we only focus on methods that do not
require causal sufﬁciency.
Algorithm 1: FindIMB
input :Do, De, treatment X, outcome Y ,
pre-treatment covariates V
output :Post-intervention distribution P(Y |do(X), V)
1 MB(Y ) ←MarkovBoundary(Y, Do);
2 foreach subset Z of MB(Y ) and C = c, c do
3
Compute P(HC
Z |De, Do) using Eq. 3;
4
Compute P(Y |do(X), V, De, Do, HC
Z ) using Eq.
8;
5 P(Y |do(X), V) ←
X
Z
X
C=c,c
P(Y |do(X), V, De, Do, HC
Z )P(HC
Z |De, Do);
Markov boundaries: Several algorithms learn OMBs from
data under causal insufﬁciency [Yu et al., 2018, 2020]. In
addition, FGESMB presented in Sec. 4 is also a sound and
complete method for learning OMBs from data. These meth-
ods can be used to identify the IMBs from the experimental
data, but they do not combine observational and experimen-
tal data to learn IMBs.
Identiﬁability: Shpitser and Pearl [2006a,b] and Tian and
Shpitser [2003] provide sound and complete identiﬁability
results for post-intervention distributions from observational
data when the causal graph is known. These methods can
answer queries for a speciﬁc marginal or conditional prob-
ability of interest. Hyttinen et al. [2015] and Jaber et al.
[2019] provide similar identiﬁability results when the graph
is unknown, using the Markov equivalence class of graphs
that are consistent with the observational data. Hyttinen et al.
[2015] can provide identiﬁability results for graphs that are
consistent with conditional independencies in both De and
Do. However, the method is not proven to be complete
for these settings. These methods are not directly compa-
rable with our method because they do not select features
for optimal prediction. Moreover, they provide expressions
for the post-intervention distributions that are based on ob-
servational data alone, not by combining Do and De like
FindIMB.
Combining observational and experimental data to learn
causal graphs: Several causal discovery methods combine
observational and experimental data to learn causal structure
[Triantaﬁllou and Tsamardinos, 2015, Hyttinen et al., 2014,
Mooij et al., 2020, Andrews et al., 2020]. These methods
return a summarized version of all the causal graphs that
are consistent with all the independence constraints in all
the data sets, observational and experimental. While these
methods can be used to improve the estimation of IMBs, it
is not clear that they can always provide a unique solution
in this setting. Two additional drawbacks they have for the
purpose of optimized target prediction are that (a) they rely
on conditional independence tests that are unreliable when
1439

***
**
***
***
***
***
***
***
***
Figure 3: Boxplots of absolute bias in the estimation of P(Y |do(X), V) using (a) FindIMB (b) IMB (c) OMB and (d)
FCIt-IMB. Data were simulated from random DAGs with 10 observed and 5 latent variables. Do included 10,000 samples,
and De included 100 (left), 200 (middle) or 1000 (right) samples. FindIMB improves the estimation of P(Y |do(X), V)
particularly for smaller experimental sample sizes. Black asterisks denote statistical signiﬁcance, assessed with the Wilcoxon
signed-rank test. Three stars correspond to p < 0.001.
Ne is low, and (b) they learn the entire graph and do not fo-
cus on ﬁnding the neighborhood of the target variable. This
can result in unreliable orientations due to error propagation.
The FCItiers method introduced by Andrews et al. [2020]
has the closest setting to ours ([Mooij et al., 2020] is also
related, but more general, and the two are equivalent for
our setting). FCItiers can learn a family of SMCMs from
De and Do when (a) the target of the intervention is known
and (b) we specify "tiered knowledge" on the variables (e.g.,
we know which variables are pre-treatment). The method is
complete in these settings. In the experimental section, we
develop a baseline comparison method based on FCItiers.
Selecting optimal adjustment sets: Some methods seek to
select optimal adjustment sets for efﬁcient average treatment
effect estimation. Given a graph (DAG/PDAG or SMCM),
these methods apply a graphical adjustment criterion to iden-
tify a set of valid adjustment sets for estimating the average
treatment effect of X on Y . Then, they try to identify the
set that leads to the estimator with the smallest asymptotic
variance among all the valid adjustment sets [Perkovic et al.,
2017, Rotnitzky and Smucler, 2019, 2020, Smucler et al.,
2020, Witte et al., 2020]. While these methods have a differ-
ent purpose than ours, they have some connections with our
work since, for pre-treatment variables, any CMB is also an
adjustment set. We point out that while optimal adjustment
sets and CMBs may often coincide (for example in DAGs),
they are not always the same (see example Fig. S1 in the
Supplementary). Moreover, these methods are not directly
comparable to ours since they focus on identifying average
treatment effects while our method focuses on conditional
effects and combines observational with experimental data
when the graph is unknown.
Potential outcomes approaches: Kallus et al. [2018]
present a method for estimating conditional average treat-
ment effects (CATEs) by combining Do and De. The
method assumes a binary treatment and uses the experi-
mental data to model the effect of possibly unmeasured
confounders as a function of the measured covariates. The
CATE is obtained from the Do by adding the modeled cor-
rection. The main assumption of the method is that the
hidden confounding has an identiﬁable parametric structure.
The method is implemented for continuous covariates and
outcome and a linear correction function, obtained by solv-
ing a least squares optimization problem. It is not directly
applicable to our settings of categorical covariates, and ex-
tending the optimization problem in these settings is not
straightforward.
Transportability: Finally, our work has some connections
with the ﬁeld of transportability [Bareinboim and Pearl,
2013], where knowledge of the causal graph is used to de-
termine if the results of an experimental trial apply to a
different population. However, the methods require know-
ing the causal graph, and focus on transferring estimators
across distributions rather than combining data to improve
estimators.
6
EXPERIMENTS
In this section, we show the performance of FindIMB using
simulated data. We simulated random DAGs with a varying
number of discrete variables, with mean in-degree 2. Each
DAG includes a binary treatment X and outcome Y , where
X →Y . The remaining covariates V are pre-treatment and
are binary or ternary, and 1/3 of the variables are set to be la-
tent at random. The observational data Do consist of 10,000
simulated samples from the ground truth DAGs, and do
not include values for the latent variables. The experimental
data De were simulated from the manipulated graph GX. For
sample sizes Ne = {100, 200, 1000}, we simulated Ne/2
samples for each value of binary treatment X. The code for
the data simulation and the following experiments are avail-
able at https://github.com/striantafillou/
UAI2021.
Comparison
to
other
approaches:
We
compared
1440

***
***
***
Figure 4: Boxplots of areas under the ROC curve for predicting Yx using (a) FindIMB (b) IMB and (c) OMB. Data were
simulated from random DAGs with 40 observed and 20 latent variables. Do included 10,000 samples, and De included
100 (left), 200 (middle) or 1000 (right) samples. Black asterisks denote statistical signiﬁcance, assessed with the Wilcoxon
signed-rank test. Three stars correspond to p < 0.001, no stars denote non-signiﬁcance.
X
A
B
M
Y
Figure 5: The m-bias graph used to simulate data
for Fig. 6. A and B are unobserved. All variables
are binary. Parameters are as follows: P(A=1)=0.8,
P(B=1)=0.8, P(M=1|A=1, B=1)=α, P(X=1|A=1)=α ,
P(Y =1|X=1, B=1)=α.
All
other
parameters
P(Y =1| . . . ),P(M=1| . . . ), P(X=1|A=0)
were
set
to zero.
Figure 6: Performance of FindIMB, IMB, and OMB for
estimating P(Y |do(X=1), M) with increasing m-bias.
FindIMB to the following approaches: (a) IMB: us-
ing only experimental data. We used De to identify the
MBX(Y ) using FGESMB. After identifying MBX(Y ) , we
used the posterior expectation P(Y |do(X), MBX(Y )
\
X, De) as the estimator for P(Y |do(X), V). (b) OMB:
using only observational data. We used FGESMB(Do) to
identify the OMB of Y , MB(Y ), and used the posterior ex-
pectation P(Y |do(X), MB(Y ) \ X, Do) estimated on Do
as the estimator of P(Y |do(X), V). This estimator is unbi-
ased when conditional ignorability holds for the OMB of Y .
(c) FCIt-IMB: using both observational and experimental
data based on FCIt-IMB. We applied FCIt-IMB using
as input a data set D constructed by concatenating De and
Do, and adding a binary variable Ie →X that corresponds
to the presence or absence of manipulation of X. So, Ie = 1
for samples in De and Ie = 0 otherwise. FCIt-IMB out-
puts a PAG P representing all possible underlying SMCMs.
Let PX denote the corresponding manipulated PAG. We
then considered the Markov boundary of Y in PX to be
the MBX(Y ) . After identifying MBX(Y ) , we tested if it
is a backdoor set in PX. If so, we used both De and Do
pooled together to estimate P(Y |do(X), MBX(Y ) \ X).
Otherwise, we only used De.
First, we tested if our FindIMB method improves esti-
mation of the probability P(Y |do(X), V). We simulated
DAGs with 10 observed and 5 latent variables, and ap-
plied the methods described above. Each method out-
puts a set of variables Z, that is used as an estimate for
P(Y |do(X), Z, V \ Z). Notice that even for 10 variables,
the number of possible conﬁgurations of Z can be very
large, and some of these conﬁgurations may be very rare.
To avoid computing these parameters for all possible conﬁg-
urations, we tested the methods in a test dataset Dtest
e
, that
includes 1000 treatment and 1000 control cases simulated
from the manipulated ground truth graph. For each sample
in Dtest
e
, we obtained an estimate ˆP(Y |do(X), V) with the
four methods. The ground truth probability was estimated
from the original manipulated Bayesian network with the
junction tree algorithm. We then computed the average abso-
lute bias | ˆP(Y |do(X), V) −P(Y |do(X), V)| over all test
samples. Fig. 3 shows that FindIMB produced the most
accurate probabilities, compared to using only observational
or only experimental data. Moreover, FindIMB outper-
forms FCIt-IMB (p < 10−3 in all cases for a Wilcoxon
signed-rank test). One reason is that FCIt-IMB selects
much larger IMBs than FindIMB, possibly due to error
propagation that results in many bidirected edges. Thus, the
resulting parameters are estimated based on much fewer
samples.
1441

We also tested the scalability of FindIMB, using DAGs
with 40 observed and 20 latent variables. For this exper-
iment, we could not test against FCIt-IMB because the
method results in very large IMBs due to the presence of
latent variables. Similarly, we could not estimate the true pa-
rameters P(Y |do(X), V) for computational reasons, since
the ground truth IMBs can also be very large and include
rare conﬁgurations, and ground truth values would need to
be computed using inference and marginalizing over latent
variables. For this reason, instead of measuring the average
absolute bias, we measured the performance of the meth-
ods to classify the samples in the test dataset Dtest
e
(2000
samples) correctly. Fig. 4 shows the area under the ROC
curve (AUC) of the models based on FindIMB, IMB, and
OMB. FindIMB performs on par or better than the two
alternatives. Average running time for FindIMB (learning
the model) was 1.71 ± 2.46 seconds for one run of the
algorithm.
One interesting ﬁnding is that in all experiments, using the
observational data only, performs better than using exper-
imental data and often is close to the performance when
combining De and Do using FindIMB. This happens be-
cause in random graphs, the effect of variables inducing bias
is often negligible [Greenland, 2003], and proxies of the
unmeasured confounders are often included in the observed
covariates. However, there are cases where the conditioning
on an observed variable in observational data can produce
heavily biased post-intervention probability estimates. A
very simple example is the graph in Fig. 5, which we call
the ”m-bias" graph. To illustrate how m-bias can affect
the prediction of Yx from observational data, we simulated
data from the m-bias graph with binary variables. We set
Y, X and M to be noisy-AND functions of their parents
with a parameter α. α has a monotonic relationship with
the bias in estimating P(Y |do(X = 1), M) using observa-
tional data: Larger α leads to a larger bias. We then varied
alpha from 0.5 to 1, and we simulated Do and De with
10,000 and 1000 samples, respectively. We used FindIMB,
IMB and OMB to estimate P(Y |do(X = 1), M) in a test
data set (2000 samples). Fig. 6 shows the bias in the esti-
mated parameter. We can see that while using Do to estimate
P(Y |do(X = 1), M) leads to increasing bias, combining
De and Do can identify the situations where the parameter
is not identiﬁable from observational data. We believe that
noisy-AND types of distributions are not rare in biomedical
data.
7
DISCUSSION
Our paper extends the concepts of Markov boundaries
for predicting post-intervention distributions, and presents
a method for learning such causal Markov boundaries
from mixtures of observational and experimental data. The
method introduced in this paper could be used in real-world
applications like embedded clinical trials, where we have
abundant observational and limited experimental data, to
predict the most effective treatment for patients. In real-
world settings, data often may include heterogeneous types
of variables (i.e., a mixture of continuous, nominal, ordinal,
etc.). Additionally, observational and experimental data may
not measure exactly the same set of covariates, rather, data
may contain overlapping covariates. Extending the method
to non-singleton treatments and outcomes is another possi-
ble avenue for future research.
Acknowledgements
This research was funded in part by grant R01LM012011
from the National Institutes of Health.
References
Bryan Andrews, Peter Spirtes, and Gregory F Cooper. On
the completeness of causal discovery in the presence of
latent confounding with tiered background knowledge.
In International Conference on Artiﬁcial Intelligence and
Statistics (AISTATS), pages 4002–4011. PMLR, 2020.
Derek C Angus. Fusing randomized trials with big data:
The key to self-learning health care systems? Journal of
American Medical Association (JAMA), 314(8):767–768,
2015.
Derek C Angus, Scott Berry, Roger J Lewis, Farah Al-Beidh,
Yaseen Arabi, Wilma van Bentum-Puijk, Zahra Bhimani,
Marc Bonten, Kristine Broglio, Frank Brunkhorst, et al.
The REMAP-CAP (randomized embedded multifactorial
adaptive platform for community-acquired pneumonia)
study rationale and design. Annals of the American Tho-
racic Society, 17(7):879–891, 2020.
Elias Bareinboim and Judea Pearl. A general algorithm for
deciding transportability of experimental results. Journal
of Causal Inference, 1(1):107–134, 2013.
Sander Greenland. Quantifying biases in causal models:
Classical confounding vs collider-stratiﬁcation bias. Epi-
demiology, 14(3):300–306, 2003.
David Heckerman and Dan Geiger. Learning Bayesian net-
works: A uniﬁcation for discrete and Gaussian domains.
In Proceedings of the 11th conference on Uncertainty in
Artiﬁcial Intelligence (UAI), pages 274–284, 1995.
David Heckerman, Dan Geiger, and David M Chickering.
Learning Bayesian networks: The combination of knowl-
edge and statistical data. Machine Learning, 20(3):197–
243, 1995.
Antti Hyttinen, Frederick Eberhardt, and Matti Järvisalo.
Constraint-based causal discovery: Conﬂict resolution
1442

with answer set programming. In Proceedings of the
30th Conference on Uncertainty in Artiﬁcial Intelligence
(UAI), pages 340–349, 2014.
Antti Hyttinen, Frederick Eberhardt, and Matti Järvisalo.
Do-calculus when the true graph is unknown. In Proceed-
ings of the 31st Conference on Uncertainty in Artiﬁcial
Intelligence (UAI), pages 395–404, 2015.
Amin Jaber, Jiji Zhang, and Elias Bareinboim. Causal identi-
ﬁcation under Markov equivalence: Completeness results.
In Proceedings of the 36th International Conference on
Machine Learning (ICML), pages 2981–2989, 2019.
Nathan Kallus, Aahlad Manas Puli, and Uri Shalit. Re-
moving hidden confounding by experimental grounding.
In Advances in Neural Information Processing Systems
(NeurIPS), pages 10888–10897, 2018.
JM Mooij, S Magliacane, and T Claassen. Joint causal
inference from multiple contexts. Journal of Machine
Learning Research, 21(99):1–108, 2020.
J Pearl. Causality: Models, Reasoning and Inference. Cam-
bridge University Press, 2000.
Judea Pearl. Probabilistic Reasoning in Intelligent Systems:
Networks of Plausible Inference.
Morgan Kaufmann
Publishers Inc., 1988.
Jean-Philippe Pellet and André Elisseeff. Finding latent
causes in causal networks: An efﬁcient approach based
on Markov blankets. In Advances in Neural Information
Processing Systems (NeurIPS), pages 1249–1256, 2008.
Emilija Perkovic, Johannes Textor, Markus Kalisch, and
Marloes H Maathuis. Complete graphical characteriza-
tion and construction of adjustment sets in Markov equiv-
alence classes of ancestral graphs. Journal of Machine
Learning Research, 18(1):8132–8193, 2017.
Joseph Ramsey, Madelyn Glymour, Ruben Sanchez-
Romero, and Clark Glymour. A million variables and
more: The fast greedy equivalence search algorithm
for learning high-dimensional graphical causal mod-
els, with an application to functional magnetic reso-
nance images. International Journal of Data Science
and Analytics, 3(2):121–129, Mar 2017. ISSN 2364-
4168. doi: 10.1007/s41060-016-0032-z. URL https:
//doi.org/10.1007/s41060-016-0032-z.
Thomas Richardson. Markov properties for acyclic directed
mixed graphs. Scandinavian Journal of Statistics, 30(1):
145–157, 2003. ISSN 03036898.
Andrea Rotnitzky and Ezequiel Smucler. Efﬁcient adjust-
ment sets for population average treatment effect estima-
tion in non-parametric causal graphical models. arXiv
preprint arXiv:1912.00306, 2019.
Andrea Rotnitzky and Ezequiel Smucler. Efﬁcient adjust-
ment sets for population average causal treatment effect
estimation in graphical models.
Journal of Machine
Learning Research, 21(188):1–86, 2020. URL http:
//jmlr.org/papers/v21/19-1026.html.
Ilya Shpitser and Judea Pearl. Identiﬁcation of joint interven-
tional distributions in recursive semi-Markovian causal
models. In Proceedings of the 21st National Conference
on Artiﬁcial Intelligence, pages 1219–1226, 2006a.
Ilya Shpitser and Judea Pearl. Identiﬁcation of conditional
interventional distributions. In Proceedings of the 22nd
Conference on Uncertainty in Artiﬁcial Intelligence (UAI),
pages 437–444, 2006b.
Ilya Shpitser, Tyler VanderWeele, and James M Robins.
On the validity of covariate adjustment for estimating
causal effects. In Proceedings of the 26th Conference on
Uncertainty in Artiﬁcial Intelligence (UAI), pages 527–
536, 2010.
Ezequiel Smucler, Facundo Sapienza, and Andrea Rotnitzky.
Efﬁcient adjustment sets in causal graphical models with
hidden variables. arXiv preprint arXiv:2004.10521, 2020.
Jin Tian and Ilya Shpitser. On the identiﬁcation of causal
effects. Technical report, Cognitive Systems Laboratory,
University of California at Los Angeles, 2003.
Soﬁa Triantaﬁllou and Ioannis Tsamardinos. Constraint-
based causal discovery from multiple interventions over
overlapping variable sets. Journal of Machine Learning
Research, 16(66):2147–2205, 2015.
Ioannis Tsamardinos and Constantin F Aliferis. Towards
principled feature selection: Relevancy, ﬁlters and wrap-
pers. In International Conference on Artiﬁcial Intelli-
gence and Statistics (AISTATS). Citeseer, 2003.
Benito Van der Zander, Maciej Liskiewicz, and Johannes
Textor. Constructing separators and adjustment sets in
ancestral graphs. In Proceedings of the 30th Conference
on Uncertainty in Artiﬁcial Intelligence (UAI), pages 11–
24, 2014.
Janine Witte, Leonard Henckel, Marloes H Maathuis, and
Vanessa Didelez. On efﬁcient adjustment in causal graphs.
Journal of Machine Learning Research, 21(246):1–45,
2020.
K. Yu, L. Liu, J. Li, and H. Chen. Mining Markov blankets
without causal sufﬁciency. IEEE Transactions on Neu-
ral Networks and Learning Systems, 29(12):6333–6347,
2018. doi: 10.1109/TNNLS.2018.2828982.
K. Yu, L. Liu, and J. Li. Learning Markov blankets from mul-
tiple interventional data sets. IEEE Transactions on Neu-
ral Networks and Learning Systems, 31(6):2005–2019,
2020. doi: 10.1109/TNNLS.2019.2927636.
1443

