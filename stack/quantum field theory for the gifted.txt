
Quantum Field Theory for the Gifted Amateur


Quantum Field Theory
for the Gifted Amateur
Tom Lancaster
Department of Physics, University of Durham
Stephen J. Blundell
Department of Physics, University of Oxford
3

3
Great Clarendon Street, Oxford, OX2 6DP,
United Kingdom
Oxford University Press is a department of the University of Oxford.
It furthers the University’s objective of excellence in research, scholarship,
and education by publishing worldwide. Oxford is a registered trade mark of
Oxford University Press in the UK and in certain other countries
c
⃝Tom Lancaster and Stephen J. Blundell 2014
The moral rights of the authors have been asserted
First Edition published in 2014
Impression: 1
All rights reserved. No part of this publication may be reproduced, stored in
a retrieval system, or transmitted, in any form or by any means, without the
prior permission in writing of Oxford University Press, or as expressly permitted
by law, by licence or under terms agreed with the appropriate reprographics
rights organization. Enquiries concerning reproduction outside the scope of the
above should be sent to the Rights Department, Oxford University Press, at the
address above
You must not circulate this work in any other form
and you must impose this same condition on any acquirer
Published in the United States of America by Oxford University Press
198 Madison Avenue, New York, NY 10016, United States of America
British Library Cataloguing in Publication Data
Data available
Library of Congress Control Number: 2013950755
ISBN 978–0–19–969932–2 (hbk.)
ISBN 978–0–19–969933–9 (pbk.)
Links to third party websites are provided by Oxford in good faith and
for information only. Oxford disclaims any responsibility for the materials
contained in any third party website referenced in this work.
 
Printed and bound by
CPI Group (UK) Ltd, Croydon, CR0 4YY

Preface
BRICK: Well, they say nature hates a vacuum, Big Daddy.
BIG DADDY: That’s what they say, but sometimes I think
that a vacuum is a hell of a lot better than some of the stuﬀ
that nature replaces it with.
Tennessee Williams (1911–1983) Cat on a Hot Tin Roof
Quantum ﬁeld theory is arguably the most far-reaching and beautiful
physical theory ever constructed. It describes not only the quantum vac-
uum, but also the stuﬀthat nature replaces it with. Aspects of quantum
ﬁeld theory are also more stringently tested, as well as veriﬁed to greater
precision, than any other theory in physics. The subject nevertheless has
a reputation for diﬃculty which is perhaps well-deserved; its practition-
ers not only manipulate formidable equations but also depict physical
processes using a strange diagrammatic language consisting of bubbles,
wiggly lines, vertices, and other geometrical structures, each of which
has a well deﬁned quantitative signiﬁcance. Learning this mathematical
and geometrical language is an important initiation rite for any aspir-
ing theoretical physicist, and a quantum ﬁeld theory graduate course is
found in most universities, aided by a large number of weighty quantum
ﬁeld theory textbooks. These books are written by professional quan-
tum ﬁeld theorists and are designed for those who aspire to join them
in that profession. Consequently they are frequently thorough, serious
minded and demand a high level of mathematical sophistication.
The motivation for our book is the idea that quantum ﬁeld theory is
too important, too beautiful and too engaging to be restricted to the
professionals. Experimental physicists, or theoretical physicists in other
ﬁelds, would beneﬁt greatly from knowing some quantum ﬁeld theory,
both to understand research papers that use these ideas and also to
comprehend and appreciate the important insights that quantum ﬁeld
theory has to oﬀer. Quantum ﬁeld theory has given us such a radically
diﬀerent and revolutionary view of the physical world that we think
that more physicists should have the opportunity to engage with it.
The problem is that the existing texts require far too much in the way
of advanced mathematical facility and provide too little in the way of
physical motivation to assist those who want to learn quantum ﬁeld
theory but not to be professional quantum ﬁeld theorists.
The gap
between an undergraduate course on quantum mechanics and a graduate
level quantum ﬁeld theory textbook is a wide and deep chasm, and one
of the aims of this book is to provide a bridge to cross it. That being
said, we are not assuming the readers of this are simple-minded folk who

vi
Preface
can be fobbed oﬀwith a trite analogy as a substitute for mathematical
argument. We aim to introduce all the maths but, by using numerous
worked examples and carefully worded motivations, to smooth the path
for understanding in a manner we have not found in the existing books.
We have chosen this book’s title with great care.1 Our imagined reader
1After all, with the number of chapters
we ended up including, we could have
called it ‘Fifty shades of quantum ﬁeld
theory’.
is an amateur, wanting to learn quantum ﬁeld theory without (at least
initially) joining the ranks of professional quantum ﬁeld theorists; but
(s)he is gifted, possessing a curious and adaptable mind and willing to
embark on a signiﬁcant intellectual challenge; (s)he has abundant cu-
riosity about the physical world, a basic grounding in undergraduate
physics, and a desire to be told an entertaining and intellectually stim-
ulating story, but will not feel patronized if a few mathematical niceties
are spelled out in detail. In fact, we suspect and hope that our book will
ﬁnd wide readership amongst the graduate trainee quantum ﬁeld theo-
rists who will want to use it in conjunction with one of the traditional
texts (for learning most hard subjects, one usually needs at least two
books in order to get a more rounded picture).
One feature of our book is the large number of worked examples, which
are set in slightly smaller type. They are integral to the story, and ﬂesh
out the details of calculations, but for the more casual reader the guts
of the argument of each chapter is played out in the main text.
To
really get to grips with the subject, the many examples should provide
transparent demonstrations of the key ideas and understanding can be
conﬁrmed by tackling the exercises at the end of each chapter.
The
chapters are reasonably short, so that the development of ideas is kept
at a steady pace and each chapter ends with a summary of the key ideas
introduced.
Though the vacuum plays a big part in the story of quantum ﬁeld the-
ory, we have not been writing in one. In many ways the present volume
represents a compilation of some of the best ideas from the literature
and, as a result, we are indebted to these other books for providing
the raw material for many of our arguments. There is an extensive list
of further reading in Appendix A where we acknowledge our sources,
but we note here, in particular, the books by Zee and by Peskin and
Schroeder and their legendary antecedent: the lectures in quantum ﬁeld
theory by Sidney Coleman. The latter are currently available online as
streamed videos and come highly recommended. Also deserving of spe-
cial mention is the text by Weinberg which is ‘a book to which we are
all indebted, and from which none of us can escape.’2
2T.S. Eliot on Ulysses.
It is a pleasure to acknowledge the help we have received from var-
ious sources in writing this book. Particular mention is due to S¨onke
Adlung at Oxford University Press who has helped steer this project to
completion. No authors could wish for a more supportive editor and
we thank him, Jessica White and the OUP team, particularly Mike Nu-
gent, our eagle-eyed copy editor. We are very grateful for the comments
and corrections we received from a number of friends and colleagues who
kindly gave up their time to read drafts of various chapters: Peter Byrne,
Claudio Castelnovo, John Chalker, Martin Galpin, Chris Maxwell, Tom

Preface
vii
McLeish, Johannes M¨oller, Paul Tulip and Rob Williams. They deserve
much credit for saving us from various embarrassing errors, but any that
remain are due to us; those that we ﬁnd post-publication will be posted
on the book’s website:
http://www.dur.ac.uk/physics/qftgabook
For various bits of helpful information, we thank Hideo Aoki, Nikitas Gi-
dopoulos, Paul Goddard and John Singleton. Our thanks are also due to
various graduate students at Durham and Oxford who have unwittingly
served as guinea pigs as we tried out various ways of presenting this
material in graduate lectures. Finally we thank Cally and Katherine for
their love and support.
TL & SJB
Durham & Oxford
January 2, 2014


Contents
0
Overture
1
0.1
What is quantum ﬁeld theory?
1
0.2
What is a ﬁeld?
2
0.3
Who is this book for?
2
0.4
Special relativity
3
0.5
Fourier transforms
6
0.6
Electromagnetism
7
I
The Universe as a set of harmonic oscillators
9
1
Lagrangians
10
1.1
Fermat’s principle
10
1.2
Newton’s laws
10
1.3
Functionals
11
1.4
Lagrangians and least action
14
1.5
Why does it work?
16
Exercises
17
2
Simple harmonic oscillators
19
2.1
Introduction
19
2.2
Mass on a spring
19
2.3
A trivial generalization
23
2.4
Phonons
25
Exercises
27
3
Occupation number representation
28
3.1
A particle in a box
28
3.2
Changing the notation
29
3.3
Replace state labels with operators
31
3.4
Indistinguishability and symmetry
31
3.5
The continuum limit
35
Exercises
36
4
Making second quantization work
37
4.1
Field operators
37
4.2
How to second quantize an operator
39
4.3
The kinetic energy and the tight-binding Hamiltonian
43
4.4
Two particles
44

x
Contents
4.5
The Hubbard model
46
Exercises
48
II
Writing down Lagrangians
49
5
Continuous systems
50
5.1
Lagrangians and Hamiltonians
50
5.2
A charged particle in an electromagnetic ﬁeld
52
5.3
Classical ﬁelds
54
5.4
Lagrangian and Hamiltonian density
55
Exercises
58
6
A ﬁrst stab at relativistic quantum mechanics
59
6.1
The Klein–Gordon equation
59
6.2
Probability currents and densities
61
6.3
Feynman’s interpretation of the negative energy states
61
6.4
No conclusions
63
Exercises
63
7
Examples of Lagrangians, or how to write down a theory 64
7.1
A massless scalar ﬁeld
64
7.2
A massive scalar ﬁeld
65
7.3
An external source
66
7.4
The φ4 theory
67
7.5
Two scalar ﬁelds
67
7.6
The complex scalar ﬁeld
68
Exercises
69
III
The need for quantum ﬁelds
71
8
The passage of time
72
8.1
Schr¨odinger’s picture and the time-evolution operator
72
8.2
The Heisenberg picture
74
8.3
The death of single-particle quantum mechanics
75
8.4
Old quantum theory is dead; long live ﬁelds!
76
Exercises
78
9
Quantum mechanical transformations
79
9.1
Translations in spacetime
79
9.2
Rotations
82
9.3
Representations of transformations
83
9.4
Transformations of quantum ﬁelds
85
9.5
Lorentz transformations
86
Exercises
88
10 Symmetry
90
10.1 Invariance and conservation
90

Contents
xi
10.2 Noether’s theorem
92
10.3 Spacetime translation
94
10.4 Other symmetries
96
Exercises
97
11 Canonical quantization of ﬁelds
98
11.1 The canonical quantization machine
98
11.2 Normalizing factors
101
11.3 What becomes of the Hamiltonian?
102
11.4 Normal ordering
104
11.5 The meaning of the mode expansion
106
Exercises
108
12 Examples of canonical quantization
109
12.1 Complex scalar ﬁeld theory
109
12.2 Noether’s current for complex scalar ﬁeld theory
111
12.3 Complex scalar ﬁeld theory in the non-relativistic limit
112
Exercises
116
13 Fields with many components and
massive electromagnetism
117
13.1 Internal symmetries
117
13.2 Massive electromagnetism
120
13.3 Polarizations and projections
123
Exercises
125
14 Gauge ﬁelds and gauge theory
126
14.1 What is a gauge ﬁeld?
126
14.2 Electromagnetism is the simplest gauge theory
129
14.3 Canonical quantization of the electromagnetic ﬁeld
131
Exercises
134
15 Discrete transformations
135
15.1 Charge conjugation
135
15.2 Parity
136
15.3 Time reversal
137
15.4 Combinations of discrete and continuous transformations
139
Exercises
142
IV
Propagators and perturbations
143
16 Propagators and Green’s functions
144
16.1 What is a Green’s function?
144
16.2 Propagators in quantum mechanics
146
16.3 Turning it around: quantum mechanics from the
propagator and a ﬁrst look at perturbation theory
149
16.4 The many faces of the propagator
151
Exercises
152

xii
Contents
17 Propagators and ﬁelds
154
17.1 The ﬁeld propagator in outline
155
17.2 The Feynman propagator
156
17.3 Finding the free propagator for scalar ﬁeld theory
158
17.4 Yukawa’s force-carrying particles
159
17.5 Anatomy of the propagator
162
Exercises
163
18 The S-matrix
165
18.1 The S-matrix: a hero for our times
166
18.2 Some new machinery: the interaction representation
167
18.3 The interaction picture applied to scattering
168
18.4 Perturbation expansion of the S-matrix
169
18.5 Wick’s theorem
171
Exercises
174
19 Expanding the S-matrix: Feynman diagrams
175
19.1 Meet some interactions
176
19.2 The example of φ4 theory
177
19.3 Anatomy of a diagram
181
19.4 Symmetry factors
182
19.5 Calculations in p-space
183
19.6 A ﬁrst look at scattering
186
Exercises
187
20 Scattering theory
188
20.1 Another theory: Yukawa’s ψ†ψφ interactions
188
20.2 Scattering in the ψ†ψφ theory
190
20.3 The transition matrix and the invariant amplitude
192
20.4 The scattering cross-section
193
Exercises
194
V
Interlude: wisdom from statistical physics
195
21 Statistical physics: a crash course
196
21.1 Statistical mechanics in a nutshell
196
21.2 Sources in statistical physics
197
21.3 A look ahead
198
Exercises
199
22 The generating functional for ﬁelds
201
22.1 How to ﬁnd Green’s functions
201
22.2 Linking things up with the Gell-Mann–Low theorem
203
22.3 How to calculate Green’s functions with diagrams
204
22.4 More facts about diagrams
206
Exercises
208

Contents
xiii
VI
Path integrals
209
23 Path integrals: I said to him, ‘You’re crazy’
210
23.1 How to do quantum mechanics using path integrals
210
23.2 The Gaussian integral
213
23.3 The propagator for the simple harmonic oscillator
217
Exercises
220
24 Field integrals
221
24.1 The functional integral for ﬁelds
221
24.2 Which ﬁeld integrals should you do?
222
24.3 The generating functional for scalar ﬁelds
223
Exercises
226
25 Statistical ﬁeld theory
228
25.1 Wick rotation and Euclidean space
229
25.2 The partition function
231
25.3 Perturbation theory and Feynman rules
233
Exercises
236
26 Broken symmetry
237
26.1 Landau theory
237
26.2 Breaking symmetry with a Lagrangian
239
26.3 Breaking a continuous symmetry: Goldstone modes
240
26.4 Breaking a symmetry in a gauge theory
242
26.5 Order in reduced dimensions
244
Exercises
245
27 Coherent states
247
27.1 Coherent states of the harmonic oscillator
247
27.2 What do coherent states look like?
249
27.3 Number, phase and the phase operator
250
27.4 Examples of coherent states
252
Exercises
253
28 Grassmann numbers: coherent states
and the path integral for fermions
255
28.1 Grassmann numbers
255
28.2 Coherent states for fermions
257
28.3 The path integral for fermions
257
Exercises
258
VII
Topological ideas
259
29 Topological objects
260
29.1 What is topology?
260
29.2 Kinks
262
29.3 Vortices
264
Exercises
266

xiv
Contents
30 Topological ﬁeld theory
267
30.1 Fractional statistics `a la Wilczek:
the strange case of anyons
267
30.2 Chern–Simons theory
269
30.3 Fractional statistics from Chern–Simons theory
271
Exercises
272
VIII
Renormalization: taming the inﬁnite
273
31 Renormalization, quasiparticles and the Fermi surface 274
31.1 Recap: interacting and non-interacting theories
274
31.2 Quasiparticles
276
31.3 The propagator for a dressed particle
277
31.4 Elementary quasiparticles in a metal
279
31.5 The Landau Fermi liquid
280
Exercises
284
32 Renormalization: the problem and its solution
285
32.1 The problem is divergences
285
32.2 The solution is counterterms
287
32.3 How to tame an integral
288
32.4 What counterterms mean
290
32.5 Making renormalization even simpler
292
32.6 Which theories are renormalizable?
293
Exercises
294
33 Renormalization in action:
propagators and Feynman diagrams
295
33.1 How interactions change the propagator in perturbation
theory
295
33.2 The role of counterterms: renormalization conditions
297
33.3 The vertex function
298
Exercises
300
34 The renormalization group
302
34.1 The problem
302
34.2 Flows in parameter space
304
34.3 The renormalization group method
305
34.4 Application 1: asymptotic freedom
307
34.5 Application 2: Anderson localization
308
34.6 Application 3: the Kosterlitz–Thouless transition
309
Exercises
312
35 Ferromagnetism: a renormalization group tutorial
313
35.1 Background: critical phenomena and scaling
313
35.2 The ferromagnetic transition and critical phenomena
315
Exercises
320

Contents
xv
IX
Putting a spin on QFT
321
36 The Dirac equation
322
36.1 The Dirac equation
322
36.2 Massless particles: left- and right-handed wave functions
323
36.3 Dirac and Weyl spinors
327
36.4 Basis states for superpositions
330
36.5 The non-relativistic limit of the Dirac equation
332
Exercises
334
37 How to transform a spinor
336
37.1 Spinors aren’t vectors
336
37.2 Rotating spinors
337
37.3 Boosting spinors
337
37.4 Why are there four components in the Dirac equation?
339
Exercises
340
38 The quantum Dirac ﬁeld
341
38.1 Canonical quantization and Noether current
341
38.2 The fermion propagator
343
38.3 Feynman rules and scattering
345
38.4 Local symmetry and a gauge theory for fermions
346
Exercises
347
39 A rough guide to quantum electrodynamics
348
39.1 Quantum light and the photon propagator
348
39.2 Feynman rules and a ﬁrst QED process
349
39.3 Gauge invariance in QED
351
Exercises
353
40 QED scattering: three famous cross-sections
355
40.1 Example 1: Rutherford scattering
355
40.2 Example 2: Spin sums and the Mott formula
356
40.3 Example 3: Compton scattering
357
40.4 Crossing symmetry
358
Exercises
359
41 The renormalization of QED and two great results
360
41.1 Renormalizing the photon propagator: dielectric vacuum
361
41.2 The renormalization group and the electric charge
364
41.3 Vertex corrections and the electron g-factor
365
Exercises
368
X
Some applications from the world
of condensed matter
369
42 Superﬂuids
370
42.1 Bogoliubov’s hunting license
370

xvi
Contents
42.2 Bogoliubov’s transformation
372
42.3 Superﬂuids and ﬁelds
374
42.4 The current in a superﬂuid
377
Exercises
379
43 The many-body problem and the metal
380
43.1 Mean-ﬁeld theory
380
43.2 The Hartree–Fock ground state energy of a metal
383
43.3 Excitations in the mean-ﬁeld approximation
386
43.4 Electrons and holes
388
43.5 Finding the excitations with propagators
389
43.6 Ground states and excitations
390
43.7 The random phase approximation
393
Exercises
398
44 Superconductors
400
44.1 A model of a superconductor
400
44.2 The ground state is made of Cooper pairs
402
44.3 Ground state energy
403
44.4 The quasiparticles are bogolons
405
44.5 Broken symmetry
406
44.6 Field theory of a charged superﬂuid
407
Exercises
409
45 The fractional quantum Hall ﬂuid
411
45.1 Magnetic translations
411
45.2 Landau Levels
413
45.3 The integer quantum Hall eﬀect
415
45.4 The fractional quantum Hall eﬀect
417
Exercises
421
XI
Some applications from the world
of particle physics
423
46 Non-abelian gauge theory
424
46.1 Abelian gauge theory revisited
424
46.2 Yang–Mills theory
425
46.3 Interactions and dynamics of Wµ
428
46.4 Breaking symmetry with a non-abelian gauge theory
430
Exercises
432
47 The Weinberg–Salam model
433
47.1 The symmetries of Nature before symmetry breaking
434
47.2 Introducing the Higgs ﬁeld
437
47.3 Symmetry breaking the Higgs ﬁeld
438
47.4 The origin of electron mass
439
47.5 The photon and the gauge bosons
440
Exercises
443

Contents
xvii
48 Majorana fermions
444
48.1 The Majorana solution
444
48.2 Field operators
446
48.3 Majorana mass and charge
447
Exercises
450
49 Magnetic monopoles
451
49.1 Dirac’s monopole and the Dirac string
451
49.2 The ’t Hooft–Polyakov monopole
453
Exercises
456
50 Instantons, tunnelling and the end of the world
457
50.1 Instantons in quantum particle mechanics
458
50.2 A particle in a potential well
459
50.3 A particle in a double well
460
50.4 The fate of the false vacuum
463
Exercises
466
A Further reading
467
B Useful complex analysis
473
B.1
What is an analytic function?
473
B.2
What is a pole?
474
B.3
How to ﬁnd a residue
474
B.4
Three rules of contour integrals
475
B.5
What is a branch cut?
477
B.6
The principal value of an integral
478
Index
480


0
Overture
0.1 What is quantum ﬁeld the-
ory?
1
0.2 What is a ﬁeld?
2
0.3 Who is this book for?
2
0.4 Special relativity
3
0.5 Fourier transforms
6
0.6 Electromagnetism
7
To begin at the beginning
Dylan Thomas (1914–1953)
Beginnings are always troublesome
George Eliot (1819–1880)
0.1
What is quantum ﬁeld theory?
Every particle and every wave in the Universe is simply an excitation
of a quantum ﬁeld that is deﬁned over all space and time.
That remarkable assertion is at the heart of quantum ﬁeld theory. It
means that any attempt to understand the fundamental physical laws
governing elementary particles has to ﬁrst grapple with the fundamentals
of quantum ﬁeld theory. It also means that any description of compli-
cated interacting systems, such as are encountered in the many-body
problem and in condensed matter physics, will involve quantum ﬁeld
theory to properly describe the interactions. It may even mean, though
at the time of writing no-one knows if this is true, that a full theory
of quantum gravity will be some kind of quantum upgrade of general
relativity (which is a classical ﬁeld theory). In any case, quantum ﬁeld
theory is the best theory currently available to describe the world around
us and, in a particular incarnation known as quantum electrodynamics
(QED), is the most accurately tested physical theory. For example, the
magnetic dipole moment of the electron has been tested to ten signiﬁcant
ﬁgures.
The ideas making up quantum ﬁeld theory have profound conse-
quences.
They explain why all electrons are identical (the same ar-
gument works for all photons, all quarks, etc.) because each electron is
an excitation of the same electron quantum ﬁeld and therefore it’s not
surprising that they all have the same properties. Quantum ﬁeld theory
also constrains the symmetry of the representations of the permutation
symmetry group of any class of identical particles so that some classes
obey Fermi–Dirac statistics and others Bose–Einstein statistics. Inter-
actions in quantum ﬁeld theory involve products of operators which are
found to create and annihilate particles and so interactions correspond
to processes in which particles are created or annihilated; hence there

2
Overture
is also the possibility of creating and destroying virtual particles which
mediate forces.
0.2
What is a ﬁeld?
This is all very well, but what is a ﬁeld? We will think of a ﬁeld as some
kind of machine that takes a position in spacetime and outputs an ob-
ject representing the amplitude of something at that point in spacetime
(Fig. 1). The amplitude could be a scalar, a vector, a complex number,
a spinor or a tensor. This concept of a ﬁeld, an unseen entity which
pervades space and time, can be traced back to the study of gravity due
to Kepler and ultimately Newton, though neither used the term and
the idea of action-at-a-distance between two gravitationally attracting
bodies seemed successful but nevertheless utterly mysterious. Euler’s
ﬂuid dynamics got closer to the matter by considering what we would
now think of as a velocity ﬁeld which modelled the movement of ﬂuid at
every point in space and hence its capacity to do work on a test parti-
cle imagined at some particular location. Faraday, despite (or perhaps
because of) an absence of mathematical schooling, grasped intuitively
the idea of an electric or magnetic ﬁeld that permeates all space and
time, and although he ﬁrst considered this a convenient mental picture
he began to become increasingly convinced that his lines of force had an
independent physical existence. Maxwell codiﬁed Faraday’s idea and the
electromagnetic ﬁeld, together with all the paraphernalia of ﬁeld theory,
was born.
xµ
φ(xµ)
Fig. 1 A ﬁeld is some kind of ma-
chine that takes a position in space-
time, given by the coordinates xµ, and
outputs an object representing the am-
plitude of something at that point in
spacetime.
Here the output is the
scalar φ(xµ) but it could be, for ex-
ample, a vector, a complex number, a
spinor or a tensor.
Thus in classical physics we understand that gravity is a ﬁeld, elec-
tromagnetism is a ﬁeld, and each can be described by a set of equations
which governs their behaviour. The ﬁeld can oscillate in space and time
and thus wave-like excitations of the ﬁeld can be found (electromagnetic
waves are well-known, but gravity waves are still to be observed). The
advent of quantum mechanics removed the distinction between what had
been thought of as wave-like objects and particle-like objects. Therefore
even matter itself is an excitation of a quantum ﬁeld and quantum ﬁelds
become the fundamental objects which describe reality.
0.3
Who is this book for?
Quantum ﬁeld theory is undoubtedly important, but it is also noto-
riously diﬃcult. Forbidding-looking integrals and a plethora of funny
squiggly Feynman diagrams are enough to strike fear in many a heart
and stomach. The situation is not helped by the fact that the many ex-
cellent existing books are written by exceedingly clever practioners who
structure their explanations with the aspiring professional in mind. This
book is designed to be diﬀerent. It is written by experimental physicists
and aimed at the interested amateur. Quantum ﬁeld theory is too in-
teresting and too important to be reserved for professional theorists.
However, though our imagined reader is not necessarily an aspiring pro-

0.4
Special relativity
3
fessional (though we hope quite a few will be) we will assume that (s)he
is enthusiastic and has some familiarity with non-relativistic quantum
mechanics, special relativity and Fourier transforms at an undergradu-
ate physics level. In the remainder of this chapter we will review a few
basic concepts that will serve to establish some conventions of notation.
0.4
Special relativity
Quantum ﬁelds are deﬁned over space and time and so we need a proper
description of spacetime, and so we will need to use Einstein’s special
theory of relativity which asserts that the speed c of light is the same
in every inertial frame. This theory implies that the coordinates of an
event in a frame S and a frame ¯S (moving relative to frame S at speed
v along the x-axis) are related by the Lorentz transformation
¯t
=
γ

t −vx
c2

,
¯x
=
γ(x −vt),
¯y
=
y,
¯z
=
z,
(1)
where γ = (1 −β2)−1/2 and β = v/c. Because the speed of light sets
the scale for all speeds, we will choose units such that c = 1. For similar
reasons1 we will also set ℏ= 1.
1In some of the early chapters we will
include the factors of ℏand c so that
the reader can make better contact with
what they already know, and will give
notice when we are going to remove
them.
A good physical theory is said to be covariant if it transforms sen-
sibly under coordinate transformations.2 In particular, we require that
2A good counterexample is the two-
component ‘shopping vector’ that con-
tains the price of ﬁsh and the price of
bread in each component.
If you ap-
proach the supermarket checkout with
the trolley at 45◦to the vertical, you
will soon discover that the prices of
your shopping will not transform ap-
propriately.
quantities should be Lorentz covariant if they are to transform ap-
propriately under the elements of the Lorentz group (which include the
Lorentz transformations of special relativity, such as eqn 1). This will
require us to write our theory in terms of certain well-deﬁned mathe-
matical objects, such as scalars, vectors and tensors.3
3We will postpone discussion of tensors
until the end of this section.
• Scalars: A scalar is a number, and will take the same value in
every inertial frame.
It is thus said to be Lorentz invariant.
Examples of scalars include the electric charge and rest mass of a
particle.
• Vectors: A vector can be thought of as an arrow. In a particular
basis it can be described by a set of components.
If the basis
is rotated, then the components will change, but the length of
the arrow will be unchanged (the length of a vector is a scalar).
In spacetime, vectors have four components and are called four-
vectors. A four-vector is an object which has a single time-like
component and three space-like components. Three-vectors will be
displayed in bold italics, such as x or p for position and momentum
respectively. The components of three-vectors are listed with a
Roman index taken from the middle of the alphabet: e.g. xi, with
i = 1, 2, 3.
Four-vectors are made from a time-like part and a
space-like part and are displayed in italic script, so position in

4
Overture
spacetime is written x where x = (t, x). Components for four-
vectors will be given a Greek index, so for example xµ where µ =
0, 1, 2, 3. We say that the zeroth component, x0, is time-like.
Example 0.1
Some other examples of four-vectors4 are:
4These are written with c = 1.
• the energy-momentum four-vector p = (E, p),
• the current density four-vector j = (ρ, j),
• the vector potential four-vector A = (V, A).
The four-dimensional derivative operator ∂µ is also a combination of a time-like part
and a space-like part, and is deﬁned5 by
5Though strictly ∂µ refers only to the
µth component of the four-vector oper-
ator ∂, rather than to the whole thing,
we will sometimes write a subscript (or
superscript) in expressions like this to
indicate whether coordinates are listed
with the indices lowered or with them
raised.
∂µ ≡
∂
∂xµ =
„ ∂
∂t, ∇
«
=
„ ∂
∂t, ∂
∂x, ∂
∂y , ∂
∂z
«
.
(2)
Note the lower index written in ∂µ, contrasting with the upper index on four-vectors
like xµ, which means that the four-dimensional derivative is ‘naturally lowered’. This
is signiﬁcant, as we will now describe.
A general coordinate transformation from one inertial frame to another
maps {xµ} →{¯xµ}, and the vector aµ transforms as
¯aµ =
∂¯xµ
∂xν

aν.
(3)
Here we have used the Einstein summation convention, by which
twice repeated indices are assumed to be summed.6 Certain other
6In full, this equation would be
¯aµ =
X
ν
„ ∂¯xµ
∂xν
«
aν.
vectors will transform diﬀerently.
For example, the gradient vector
∂µφ ≡∂φ/∂xµ transforms as
∂φ
∂¯xµ =
∂xν
∂¯xµ
 ∂φ
∂xν .
(4)
The jargon is that aµ transforms like a contravariant vector and
∂φ/∂xµ ≡∂µφ transforms like a covariant vector,7 though we will
7These
unfortunate
terms
are
due
to the English mathematician J. J.
Sylvester (1814–1897).
Both types of
vectors transform covariantly, in the
sense of ‘properly’, and we wish to re-
tain this sense of the word ‘covariant’
rather than using it to simply label one
type of object that transforms properly.
Thus we will usually specify whether
the indices on a particular object are
‘upstairs’ (like aµ) or ‘downstairs’ (like
∂µφ) and their transformation proper-
ties can then be deduced accordingly.
avoid these terms and just note that aµ has its indices ‘upstairs’ and
∂µφ has them ‘downstairs’ and they will then transform accordingly.
The Lorentz transformation (eqn 1) can be rewritten in matrix form
as




¯t
¯x
¯y
¯z



=




γ
−βγ
0
0
−βγ
γ
0
0
0
0
1
0
0
0
0
1








t
x
y
z



,
(5)
or for short as
¯xµ = Λµ
ν xν,
(6)
where Λµν ≡(∂¯xµ/∂xν) is the Lorentz transformation matrix. In the
same way, the energy-momentum four-vector transforms as
¯pµ = Λµ
ν pν.
(7)

0.4
Special relativity
5
A downstairs vector aµ transforms as
¯aµ = Λµ
ν aν,
(8)
where Λµν ≡(∂xν/∂¯xµ) is the inverse of the Lorentz transformation
matrix Λµν.8
8 Note that in the equation ¯xµ = Λµνxν
we have
Λµν =
0
B
B
@
γ
βγ
0
0
βγ
γ
0
0
0
0
1
0
0
0
0
1
1
C
C
A .
(9)
In fact
Λµν Λµρ =
„ ∂¯xµ
∂xν
« „ ∂xρ
∂¯xµ
«
= δρ
ν,
where the Kronecker delta δj
i is de-
ﬁned by
δj
i =
1
i = j
0
i ̸= j.
The δj
i symbol is named after Leopold
Kronecker (1823–1891).
The Lorentz transformation changes components but leaves the length
of the four-vector x unchanged. This length is given by the square root9
9Of course |x| can be negative in special
relativity, so it is better to deal with
|x|2, the square of the length.
of
|x|2 = x · x = (x0)2 −(x1)2 −(x2)2 −(x3)2.
(10)
In general, the four-vector inner product10 is
10Note that other conventions are pos-
sible and some books write a · b =
−a0b0 + a · b and deﬁne their metric
tensor diﬀerently. This is an entirely le-
gitimate alternative lifestyle choice, but
it’s best to stick to one convention in a
single book.
a · b = a0b0 −a · b,
(11)
which we can write
a · b = gµν aµbν,
(12)
where the metric tensor gµν is given by
gµν =




1
0
0
0
0
−1
0
0
0
0
−1
0
0
0
0
−1



.
(13)
Upstairs and downstairs vectors are related by the metric tensor via
aµ = gµν aν,
(14)
so that we can lower or raise an index by inserting the metric tensor.
The form of the metric tensor in eqn 13 allows us to write
a0 = a0
ai = −ai,
(15)
and hence
a · b = gµν aµbν = aµbµ = aµbµ.
(16)
Note also that a · b = gµν aµbν and gµν = gµν.
Exercise: You can check that
gµν gνρ = δρ
µ
and also that
Λµν = gµκ Λκρ gρν.
Example 0.2
(i) An example of an inner product is
p · p = pµpµ = (E, p) · (E, p) = E2 −p2 = m2,
(17)
where m is the rest mass of the particle.
(ii) The combination ∂µxν = ∂xν
∂xµ = δν
µ and hence the inner product ∂µxµ = 4
(remember the Einstein summation convention).
(iii) The d’Alembertian operator ∂2 is given by a product of two derivative op-
Named in honour of the French math-
ematician Jean le Rond d’Alembert
(1717–1783).
In
some
texts
the
d’Alembertian is written as ✷and in
some as ✷2. Because of this confusion,
we will avoid the ✷symbol altogether.
erators (and is the four-dimensional generalization of the Laplacian operator).
It is written as
∂2 = ∂µ∂µ
=
∂2
∂t2 −∂2
∂x2 −∂2
∂y2 −∂2
∂z2
(18)
=
∂2
∂t2 −∇2.
(19)

6
Overture
To complete this discussion, we can also deﬁne a general tensor T i···k
ℓ···n
with an arbitrary set of upstairs and downstairs indices. This transforms
as
¯T i′···k′
ℓ′···n′ = ∂¯xi′
∂xi · · · ∂¯xk′
∂xk
∂xℓ
∂¯xℓ′ · · · ∂xn
∂¯xn′ T i···k
ℓ···n.
(20)
Example 0.3
(i) The Kronecker delta δi
j is a ‘mixed tensor of second rank’,11 and one can check
11It has two indices (hence second
rank) and one is upstairs, one is down-
stairs (hence mixed).
that it transforms correctly as follows:
¯δj
i = ∂¯xi
∂xk
∂xℓ
∂¯xj δℓ
k = ∂¯xi
∂xk
∂xk
∂¯xj = δi
j.
(21)
Note that whenever δij or δij are written, they are not tensors and are simply a
shorthand for the scalar 1, in the case when i = j, or 0 when i ̸= j.
(ii) The antisymmetric symbol or Levi-Civita symbol12 εijkℓis deﬁned in four
12This is named after Italian physi-
cist
Tullio
Levi-Civita
(1873–1941).
Useful
relationships
with
the
Levi-
Civita symbol include results for three-
dimensional vectors:
(b × c)i = εijkbjck,
a · (b × c) = εijkaibjck,
and matrix algebra:
det A = εi1i2···inA1i1A2i2 · · · Anin,
where A is a n × n matrix with compo-
nents Aij.
dimensions by (i) all even permutations ijkℓof 0123 (such as ijkℓ= 2301) have
εijkℓ= 1; (ii) all odd permutations ijkℓof 0123 (such as ijkℓ= 0213) have εijkℓ=
−1; (iii) all other terms are zero (e.g. ε0012 = 0). The Levi-Civita symbol can be
deﬁned in other dimensions.13 We will not treat this symbol as a tensor, so the
13The version in two dimensions is
rather simple:
ε01 = −ε10 = 1,
ε00 = ε11 = 0.
In three dimensions, the nonzero com-
ponents are:
ε012 = ε201 = ε120 = 1,
ε021 = ε210 = ε102 = −1.
version with downstairs indices εijk is identical to εijk.
0.5
Fourier transforms
We will constantly be needing to swap between representations of an
object in spacetime and in the corresponding frequency variables, that
is spatial and temporal frequency. Spatial frequency k and temporal
frequency ω also form a four-vector (ω, k) and using E = ℏω and p =
ℏk we see that this is the energy-momentum four-vector (E, p).
(In
fact, with our convention ℏ= 1 the two objects are identical!)
To
swap between representations, we deﬁne the four-dimensional Fourier
transform ˜f(k) of a function f(x) of spacetime x as
Jean Baptiste Joseph Fourier (1768–
1830)
˜f(k) =
Z
d4x eik·xf(x),
(22)
where four-dimensional integration is deﬁned by
Z
d4x =
Z
dx0dx1dx2dx3.
(23)
The inverse transform is
f(x) =
Z
d4k
(2π)4 e−ik·x ˜f(k),
(24)
and contains four factors of 2π that are needed for each of the four
integrations. Another way of writing eqn 22 is
˜f(ω, k) =
Z
d3xdt ei(ωt−k·x)f(t, x).
(25)
In the spirit of this deﬁnition, we will try to formulate our equations so
that every factor of dk comes with a (2π), hopefully eliminating one of
the major causes of insanity in the subject, the annoying factors14 of 2π.
14Getting these right is actually impor-
tant: if you have (2π)4 on the top of an
equation and not the bottom, your an-
swer will be out by a factor of well over
two million.

0.6
Electromagnetism
7
Example 0.4
The Dirac delta function δ(x) is a function localized at the origin and which
has integral unity. It is the perfect model of a localized particle. The integral of a
d-dimensional Dirac delta function δ(d)(x) is given by
Z
ddx δ(d)(x) = 1.
(26)
It is deﬁned by
Z
ddx f(x)δ(d)(x) = f(0).
(27)
Consequently, its Fourier transform is given by
˜δ(d)(k) =
Z
ddx eik·xδ(d)(x) = 1.
(28)
Hence, the inverse Fourier transform in four-dimensions is
Z
d4k
(2π)4 e−ik·x = δ(4)(x).
(29)
0.6
Electromagnetism
In SI units Maxwell’s equations in free space can be written:
James Clerk Maxwell (1831–1879)
∇· E = ρ
ǫ0 ,
∇× E = −∂B
∂t ,
∇· B = 0,
∇× B = µ0J + 1
c2 ∂E
∂t .
(30)
In this book we will choose15 the Heaviside–Lorentz16 system of units
15Although SI units are preferable for
many applications in physics, the desire
to make our (admittedly often compli-
cated) equations as simple as possible
motivates a diﬀerent choice of units for
the discussion of electromagnetism in
quantum ﬁeld theory. Almost all books
on quantum ﬁeld theory use Heaviside–
Lorentz units, though the famous text-
books on electrodynamics by Landau
and Lifshitz and by Jackson do not.
16These units are named after the En-
glish electrical engineer O. Heaviside
(1850–1925)
and the Dutch physicist
H. A. Lorentz (1853–1928).
(also known as the ‘rationalized Gaussian CGS’ system) which can be
obtained from SI by setting ǫ0 = µ0 = 1. Thus the electrostatic potential
V (x) = q/4πǫ0|x| of SI becomes
V (x) =
q
4π|x|,
(31)
in Heaviside–Lorentz units, and Maxwell’s equations can be written
∇· E = ρ,
∇× E = −1
c
∂B
∂t ,
∇· B = 0,
∇× B = 1
c(J + ∂E
∂t ).
(32)
Using our other choice of c = ℏ= 1 obviously removes the factors of
c from these equations. In addition, the ﬁne structure constant α =
e2/4πℏc ≈
1
137 simpliﬁes to
α = e2
4π .
(33)
Note that we will give electromagnetic charge q in units of the electron
charge e by writing q = Q|e|. The charge on the electron corresponds
to Q = −1.


Part I
The Universe as a set of
harmonic oscillators
In this introductory part of the book, we trace the development of the
picture of the Universe which underpins quantum ﬁeld theory.
This
picture is one in which harmonic oscillators are ubiquitous, and such
oscillators form a central paradigm of quantum systems. We show that
non-interacting harmonic oscillators have eigenfunctions which look and
behave like particles and ﬁnd some elegant methods for describing sys-
tems of coupled oscillators.
• In Chapter 1 we provide a formulation of classical mechanics which
is suitable for a quantum upgrade. This allows us to talk about
functionals and Lagrangians.
• The simple harmonic oscillator, presented in Chapter 2, is well-
known from basic quantum physics as an elementary model of an
oscillating system. We solve this simple model using creation and
annihilation operators and show that the solutions have the charac-
teristics of particles. Linking masses by springs into a chain allows
us to generalize this problem and the solutions are phonons.
• The next step is to change our viewpoint and get rid of wave func-
tions entirely and develop the occupation number representation
which we do in Chapter 3. We show that bosons are described by
commuting operators and fermions are described by anticommut-
ing operators.
• Already we have many of the building blocks of a useful theory.
In Chapter 4 we consider how to build single-particle operators
out of creation and annihilation operators, and this already gives
us enough information to discuss the tight-binding model of solid
state physics and the Hubbard model.

1
Lagrangians
1.1 Fermat’s principle
10
1.2 Newton’s laws
10
1.3 Functionals
11
1.4 Lagrangians and least action
14
1.5 Why does it work?
16
Chapter summary
17
Exercises
17
In this chapter, we give an introduction to Lagrangians in classical me-
chanics and explain why the Lagrangian formulation is a sensible way
to describe quantum ﬁelds.
1.1
Fermat’s principle
Fig.
1.1 Refraction of a light ray
through a slab of glass. The ray ﬁnds
the path of least travel time from A to
B.
We begin with an example from the study of optics. Consider the passage
of a light ray through a slab of glass as shown in Fig. 1.1. The bending
of the light ray near the air/glass interface can be calculated using the
known refractive indices of air and glass using the famous Snell’s law of
refraction, named after Willebrord Snellius, who described it in 1621,
but discovered ﬁrst by the Arabian mathematician Ibn Sahl in 984. In
1662, Pierre de Fermat produced an ingenious method of deriving Snell’s
law on the basis of his principle of least time. This states that the
path taken between two points A and B by a ray of light is the path
that can be traversed by the light in the least time. Because light travels
more slowly in glass than in air, the ray of light crosses the glass at a
steeper angle so it doesn’t have so much path length in the glass. If the
light ray were to take a straight line path from A to B this would take
longer. This was all very elegant, but it didn’t really explain why light
would choose to do this; why should light take the path which took the
least time? Why is light in such a hurry?
Fermat’s principle of least time is cute, and seems like it is telling us
something, but at ﬁrst sight it looks unhelpful. It attempts to replace
a simple formula (Snell’s law), into which you can plug numbers and
calculate trajectories, with a principle characterizing the solution of the
problem but for which you need the whole apparatus of the calculus of
variations to solve any problems. Fermat’s principle is however the key
to understanding quantum ﬁelds, as we shall see.
1.2
Newton’s laws
t
x
0
τ
Fig. 1.2 A particle moves from A to B
in time τ and its path is described by
Newton’s laws of motion.
A somewhat similar problem is found in the study of dynamics. Consider
a particle of mass m subject to a force F which moves in one spatial
dimension x from point A to B, as shown in the spacetime diagram in
Fig. 1.2. Here time is on the horizontal axis and space is on the vertical
axis. The exact path x(t) that the particle takes is given by Newton’s

1.3
Functionals
11
laws of motion, i.e.
F = m¨x.
(1.1)
This equation can be integrated to ﬁnd x(t). However, when you stop
and think about it, this is a very quantum-unfriendly approach. The
solution gives you the position of the particle at every time t from t = 0
to t = τ.
Quantum mechanics tells us that you might measure the
particle’s position at t = 0 and ﬁnd it at A, and you might measure it
again at t = τ and ﬁnd it at B, but you’d not be able to know precisely
what it did in between. Having a method which lets you calculate x(t)
from a diﬀerential equation is not a good starting point.
Dynamics need to be formulated in a completely diﬀerent way if the
subject is to be generalized to quantum mechanics. This is exactly what
Joseph-Louis Lagrange and William Rowan Hamilton did, although they
J.-L.
Lagrange
(1736–1813)
was
an
Italian-born French mathematician and
physicist.
W. R. Hamilton (1805–1865) was an
Irish mathematician and physicist.
had no idea that what they were doing would make dynamics more
quantum-friendly. We will take a slightly diﬀerent approach to theirs
and arrive at the ﬁnal answer by asking ourselves how kinetic energy T
and potential energy V vary during the trajectory of the particle. We
know that they must sum to the total energy E = T +V which must be a
constant of the motion. But during the trajectory, the balance between
kinetic and potential energy might change.
It is simple enough to write down the average kinetic energy ¯T during
the trajectory, which is given by
¯T = 1
τ
Z τ
0
1
2m[ ˙x(t)]2 dt.
(1.2)
The average potential energy ¯V during the trajectory is given by
¯V = 1
τ
Z τ
0
V [x(t)] dt.
(1.3)
These two quantities must sum to give the total energy E = ¯E = ¯T + ¯V .
However, what we want to do is to consider how ¯T and ¯V vary as you
alter the trajectory. To do this, we need a little bit of mathematics,
which we cover in the next section.
1.3
Functionals
NUMBER
NUMBER
FUNCTION
NUMBER
3
27
e−xx2
3
19
function
functional
Fig. 1.3 A function turns a number
into another number. A functional op-
erates on an entire function and pro-
duces a number.
The expressions in eqns 1.2 and 1.3 are functionals of the trajectory x(t).
What does this mean?
Let us recall that a function is a machine (see Fig. 1.3) which turns a
number into another number. For example, the function f(x) = 3x2 will
turn the number 1 into the number 3, or the number 3 into the number
27. Give a function a number and it will return another number.
A functional is a machine which turns a function into a number. You
feed the machine with an entire function, like f(x) = x2 or f(x) = sin x
and it returns a number.

12
Lagrangians
Example 1.1
Here are some examples of functionals.
• The functional F[f] operates on the function f as follows:
F[f] =
Z 1
0
f(x) dx.
(1.4)
Hence, given the function f(x) = x2, the functional returns the number
F[f] =
Z 1
0
x2 dx = 1
3 .
(1.5)
• The functional G[f] operates on the function f as follows:
G[f] =
Z a
−a
5[f(x)]2 dx.
(1.6)
Hence, given the function f(x) = x2, the functional returns the number
G[f] =
Z a
−a
5x4 dx = 2a5.
(1.7)
• A function can be thought of as a trivial functional. For example, the func-
tional Fx[f] given by
Fx[f] =
Z ∞
−∞
f(y)δ(y −x)dy = f(x),
(1.8)
returns the value of the function evaluated at x.
We now want to see how a functional changes as you adjust the function
which is fed into it. The key concept here is functional diﬀerentiation.
Recall that a derivative of a function is deﬁned as follows:
df
dx = lim
ǫ→0
f(x + ǫ) −f(x)
ǫ
.
(1.9)
The derivative of the function tells you how the number returned by
the function f(x) changes as you slightly change the number x that you
feed into the ‘machine’. In the same way, we can deﬁne a functional
derivative of a functional F[f] as follows:
δF
δf(x) = lim
ǫ→0
F[f(x′) + ǫδ(x −x′)] −F[f(x′)]
ǫ
.
(1.10)
The functional derivative tells you how the number returned by the
functional F[f(x)] changes as you slightly change the function f(x) that
you feed into the machine.
Example 1.2
Here are some examples of calculations of functional derivatives.
You can work
through these if you want to acquire the skill of functional diﬀerentiation, or skip to
the next bit of text if you are happy to take the results on trust.

1.3
Functionals
13
• The functional I[f] =
R 1
−1 f(x) dx has a functional derivative given by
δI[f]
δf(x0)
=
lim
ǫ→0
1
ǫ
»Z 1
−1
[f(x) + ǫδ(x −x0)] dx −
Z 1
−1
f(x) dx
–
=
Z 1
−1
δ(x −x0) dx
=
(
1
−1 ≤x0 ≤1
0
otherwise.
(1.11)
• The functional J[f] =
R
[f(y)]pφ(y) dy has a functional derivative with respect
to f(x) given by
δJ[f]
δf(x)
=
lim
ǫ→0
1
ǫ
»Z
[f(y) + ǫδ(y −x)]pφ(y) dy −
Z
[f(y)]pφ(y) dy
–
=
p[f(x)]p−1φ(x).
(1.12)
• The functional H[f] =
R b
a g[f(x)] dx, where g is a function whose derivative is
g′ = dg/dx, has a functional derivative given by
δH[f]
δf(x0)
=
lim
ǫ→0
1
ǫ
»Z
g[f(x) + ǫδ(x −x0)] dx −
Z
g[f(x)] dx
–
=
lim
ǫ→0
1
ǫ
»Z
(g[f(x)] + ǫδ(x −x0)g′[f(x)]) dx −
Z
g[f(x)] dx
–
=
Z
δ(x −x0) g′[f(x)] dx
=
g′[f(x0)].
(1.13)
• Using the result of the previous example, the functional ¯V [x] = 1
τ
R τ
0 V [x(t)] dt
so that
δ ¯V [x]
δx(t) = 1
τ V ′[x(t)].
(1.14)
• The functional J[f] is deﬁned by J[f] =
R
g(f′) dy where f′ = df/dy. Hence
δJ[f]
δf(x) = lim
ǫ→0
1
ǫ
»Z
dy g
„ ∂
∂y [f(y) + ǫδ(y −x)]
«
−
Z
dy g
„ ∂f
∂y
«–
,
(1.15)
and using
g
„ ∂
∂y [f(y) + ǫδ(y −x)]
«
= g
`
f′ + ǫδ′(y −x)
´
≈g(f′) + ǫδ′(y −x) dg(f′)
df′
,
(1.16)
then the calculation can be reduced to an integral by parts
δJ[f]
δf(x) =
Z
dy δ′(y−x) dg(f′)
df′
=
»
δ(y −x) dg(f′)
df′
–
−
Z
δ(y−x) d
dy
„ dg(f′)
df′
«
.
(1.17)
The term in square brackets vanishes assuming x is inside the limits of the
integral, and we have simply
δJ[f]
δf(x) = −d
dx
„dg(f′)
df′
«
.
(1.18)
• An example of the previous result is that for F[φ] =
R “
∂φ
∂y
”2
dy,
δF[φ]
δφ(x) = −2∂2φ
∂x2 .
(1.19)
Equation 1.19 can be easily generalized
to three dimensions and leads to a very
useful result which is worth memoriz-
ing, namely that if
I =
Z
(∇φ)2 d3x,
then
δI
δφ = −2∇2φ.
• Another example is for ¯T = 1
τ
R τ
0
1
2m[ ˙x(t)]2 dt,
δ ¯T[x]
δx(t) = −m¨x
τ .
(1.20)

14
Lagrangians
1.4
Lagrangians and least action
With these mathematical results under our belt, we are now ready to
return to the main narrative. How does the average kinetic energy and
the average potential energy vary as we adjust the particle trajectory?
We now have the main results from eqns 1.14 and 1.20 which are:
δ ¯V [x]
δx(t) =
¯V ′[x(t)]
τ
,
δ ¯T[x]
δx(t) = −m¨x
τ .
(1.21)
In fact, Newton’s laws tell us that for the classical trajectory of the
particle we have that m¨x = −dV/dx. This means that for variations
about the classical trajectory
δ ¯V [x]
δx(t) = δ ¯T[x]
δx(t) .
(1.22)
This means that if you slightly deviate from the classical trajectory,
then both the average kinetic energy and the average potential energy
will increase1 and by the same amount. This equation can be rewritten
1They might both decrease by the same
amount if the classical trajectory max-
imizes (rather than minimizes) the ac-
tion, see below, but this case is not the
one usually encountered.
as
δ
δx(t)( ¯T[x] −¯V [x]) = 0,
(1.23)
i.e. that the diﬀerence between the average kinetic energy and the av-
erage potential energy is stationary about the classical trajectory. This
shows that there is something rather special about the diﬀerence be-
tween kinetic energy and potential energy, and motivates us to deﬁne a
quantity known as the Lagrangian L as
L = T −V.
(1.24)
The integral of the Lagrangian over time is known as the action S
S =
Z τ
0
L dt,
(1.25)
and so the action has dimensions of energy×time, and hence is measured
in Joule-seconds. This is the same units as Planck’s constant h, and we
will see later in this chapter why it is often appropriate to think of
measuring S in units of Planck’s constant.
Our variational principle
(eqn 1.23) connecting variations of average kinetic energy and average
potential energy can now be rewritten in a rather appealing way since
for this problem S =
R τ
0 (T −V ) dt = τ( ¯T[x] −¯V [x]), so that
t
x
0
τ
Fig. 1.4 Small adjustments to the path
of a particle from its classical trajectory
lead to an increase in the action S.
δS
δx(t) = 0,
(1.26)
and this is known as Hamilton’s principle of least action.2 It states
2To be pedantic, the principle only
shows the action is stationary. It could
be a maximum, or a saddle point, just
as easily as a minimum.
‘Stationary
action’ would be better than ‘least ac-
tion’. But we are stuck with the name.
that the classical trajectory taken by a particle is such that the action is
stationary; small adjustments to the path taken by the particle (Fig. 1.4)
only increase the action (in the same way that small adjustments to the
path taken by a ray of light from the one determined by Snell’s law
lengthen the time taken by the light ray).

1.4
Lagrangians and least action
15
Example 1.3
The Lagrangian L can be written as a function of both position and velocity. Quite
generally, one can think of it as depending on a generalized position coordinate x(t)
and its derivative ˙x(t), called the velocity.
Then the variation of S with x(t) is
δS/δx(t) and can be written as
δS
δx(t)
=
Z
du
»
δL
δx(u)
δx(u)
δx(t) +
δL
δ ˙x(u)
δ ˙x(u)
δx(t)
–
=
Z
du
»
δL
δx(u)δ(u −t) +
δL
δ ˙x(u)
d
dtδ(u −t)
–
=
δL
δx(t) +
»
δ(u −t)
δL
δ ˙x(u)
–tf
ti
−
Z
du δ(u −t) d
dt
δL
δ ˙x(u)
=
δL
δx(t) −d
dt
δL
δ ˙x(t),
(1.27)
and hence the principle of least action (eqn 1.26) yields
δL
δx(t) −d
dt
δL
δ ˙x(t) = 0,
(1.28)
which is known as the Euler–Lagrange equation.
Leonhard Euler (1707–1783).
In the
words of Pierre-Simon Laplace (1749–
1827):
Read Euler, read Euler, he is
the master of us all.
The Lagrangian L is related to the Lagrangian density L by
L =
Z
dx L,
(1.29)
so that the action S is given by
S =
Z
dt L =
Z
dt dx L.
(1.30)
The following example introduces the idea of a Lagrangian density, a
concept we will come back to frequently, but also provides a nice way to
derive the classical wave equation.
ψ(x, t)
x
dx
ρ dx
Fig. 1.5 Waves on a string. The dis-
placement from equilibrium is ψ(x, t)
and the equation of motion can be de-
rived by considering an element of the
string of length dx and mass ρ dx. The
ﬁgure shows a short section in the mid-
dle of the string which is assumed to be
tethered at either end so that ψ(0, t) =
ψ(ℓ, t).
Example 1.4
Consider waves on a string of mass m and length ℓ. Let us deﬁne the mass density
ρ = m/ℓ, tension T and displacement from the equilibrium ψ(x, t) (see Fig. 1.5). The
kinetic energy T can then be written as T = 1
2
R ℓ
0 dx ρ(∂ψ/∂t)2 and the potential
energy V = 1
2
R ℓ
0 dx T (∂ψ/∂x)2. The action is then
S[ψ(x, t)] =
Z
dt (T −V ) =
Z
dt dx L
„
ψ, ∂ψ
∂t , ∂ψ
∂x
«
,
(1.31)
where
L
„
ψ, ∂ψ
∂t , ∂ψ
∂x
«
= ρ
2
„ ∂ψ
∂t
«2
−T
2
„∂ψ
∂x
«2
(1.32)
is the Lagrangian density. We then have immediately
0 = δS
δψ
=
∂L
∂ψ −d
dx
∂L
∂(∂ψ/∂x) −d
dt
∂L
∂(∂ψ/∂t)
=
0 + T ∂2ψ
∂x2 −ρ ∂2ψ
∂t2 ,
(1.33)
and so the wave equation (∂2ψ/∂x2) = (1/v2)(∂2ψ/∂t2) with v =
p
T /ρ emerges
almost eﬀortlessly.

16
Lagrangians
As a ﬁnal trick, let us put the Euler–Lagrange equation on a fully rela-
tivistic footing, bracing ourselves to use some four-vector notation and
the Einstein summation convention (see Section 0.4). If the Lagrangian
density L depends on a function φ(x) (where x is a point in spacetime)
and its derivative3 ∂µφ, then the action S is given by
3Recall from the argument in Sec-
tion 0.4 that the index µ in ∂µφ is nat-
urally lowered; see eqn 0.2.
S =
Z
d4x L(φ, ∂µφ).
(1.34)
By analogy with eqn 1.27, the action principle gives4
4Remember that we are using the Ein-
stein summation convention, by which
twice
repeated
indices
are
assumed
summed.
Equation 1.35 is simply
the four-dimensional generalisation of
eqns 1.27 and 1.28.
δS
δφ = ∂L
∂φ −∂µ

∂L
∂(∂µφ)

= 0,
(1.35)
the four-vector version of the Euler–Lagrange equation.
Example 1.5
As a simple example of this, consider the case when L is given by
L = 1
2(∂µφ)2 −1
2m2φ2,
(1.36)
where (∂µφ)2 = (∂µφ)(∂µφ). Simple diﬀerentiation then gives
∂L
∂φ = −m2φ,
and
∂L
∂(∂µφ) = ∂µφ.
(1.37)
Use of the action principle (eqn 1.35) gives
δS
δφ = −m2φ −∂µ∂µφ = 0
and hence
(∂2 + m2)φ = 0.
(1.38)
1.5
Why does it work?
In this chapter, we have considered two variational principles: Fermat’s
principle of least time and Hamilton’s principle of least action.
One
describes the path taken by a ray of light, the other describes the path
taken by a classical particle. They are very elegant, but why not stick
to using Snell’s law and Newton’s law? And why do they both work?
The answer to both of these questions is quantum mechanics. We will
talk about this in more detail later in the book, but the motion of a
particle (photon or billiard ball) going from A to B involves all possible
paths, the sensible classical ones and completely insane ones. You have
to sum them all up, but they are each associated with a phase factor, and
for most sets of paths the diﬀerent phases mean that the contributions
cancel out. It is only when the phase is stationary that nearby paths all
give a non-cancelling contribution. The wave function for a particle has
a phase factor5 given by
5Fermat’s principle of least time tells us
something about optical path length of
a ray, that is the diﬀerence between the
phase at the beginning and end of a ray.
By analogy with Hamilton’s principle
of least action of a classical mechanical
system, one can posit that the action S
is given by a constant multiplied by the
phase of a wave function. This deﬁnes
the constant which is given the symbol
ℏ. Thus we take the phase to be S/ℏ.
eiS/ℏ,
(1.39)

Exercises
17
where S =
R
L dt is the action, so a stationary phase equates to a sta-
tionary action (eqn 1.26). (Running the argument in reverse, the phase
factor for a photon of energy E is e−iEt/ℏ, and so stationary phase
equates to stationary time, which is Fermat’s principle.) We will see
how this approach leads naturally to Feynman’s path-integral approach
later in the book (Chapter 23). But for now, notice simply that if the
action is stationary then the classical path which minimizes the action
is the one that is observed and everything else cancels.
Snell’s law and Newton’s law are enough to solve classical systems.
But neither allow the generalization to quantum systems to be performed
with ease. Thus, to formulate quantum ﬁeld theory (the grand task of
this book), we have to start with a Lagrangian picture. Our next step is
to gain some insight into what happens in non-classical systems, and so
in the next chapter we will turn our attention to an archetypal quantum
system: the simple harmonic oscillator.
Chapter summary
• Fermat’s principle of least time states that light takes a path which
takes the least time.
• The Lagrangian for a classical particle is given by L = T −V .
• Classical mechanics can be formulated using Hamilton’s principle
of least action.
δS
δx(t) = 0,
(1.40)
where the action S =
R
L dt.
• Both Fermat’s and Hamilton’s principles show how the classical
paths taken by a photon or massive particle are ones in which the
phase of the corresponding wave function is stationary.
Exercises
(1.1) Use Fermat’s principle of least time to derive Snell’s
law.
(1.2) Consider the functionals H[f] =
R
G(x, y)f(y) dy,
I[f] =
R 1
−1 f(x) dx and J[f] =
Z „∂f
∂y
«2
dy of the
function f. Find the functional derivatives δH[f]
δf(z) ,
δ2I[f 3]
δf(x0)δf(x1) and δJ[f]
δf(x).
(1.3) Consider the functional G[f] =
R
g(y, f) dy. Show
that
δG[f]
δf(x) = ∂g(x, f)
∂f
.
(1.41)
Now consider the functional H[f] =
R
g(y, f, f ′) dy
and show that
δH[f]
δf(x) = ∂g
∂f −d
dx
∂g
∂f ′ ,
(1.42)

18
Lagrangians
where f ′ = ∂f/∂y.
For the functional J[f] =
R
g(y, f, f ′, f ′′) dy show that
δJ[f]
δf(x) = ∂g
∂f −d
dx
∂g
∂f ′ + d2
dx2
∂g
∂f ′′ ,
(1.43)
where f ′′ = ∂2f/∂y2.
(1.4) Show that
δφ(x)
δφ(y) = δ(x −y),
(1.44)
and
δ ˙φ(t)
δφ(t0) = d
dtδ(t −t0).
(1.45)
(1.5) For a three-dimensional elastic medium, the poten-
tial energy is
V = T
2
Z
d3x (∇ψ)2,
(1.46)
and the kinetic energy is
T = ρ
2
Z
d3x
„∂ψ
∂t
«2
.
(1.47)
Use these results, and the functional derivative ap-
proach, to show that ψ obeys the wave equation
∇2ψ = 1
v2
∂2ψ
∂t2 ,
(1.48)
where v is the velocity of the wave.
(1.6) Show that if Z0[J] is given by
Z0[J] = exp
„
−1
2
Z
d4x d4y J(x)∆(x −y)J(y)
«
,
(1.49)
where ∆(x) = ∆(−x) then
δZ0[J]
δJ(z1) = −
»Z
d4y ∆(z1 −y)J(y)
–
Z0[J]. (1.50)

2
Simple harmonic oscillators
2.1 Introduction
19
2.2 Mass on a spring
19
2.3 A trivial generalization
23
2.4 Phonons
25
Chapter summary
27
Exercises
27
2.1
Introduction
The advent of quantum mechanics convinced people that things that had
previously been thought of as particles were in fact waves. For example,
it was found that electrons and neutrons were not simply little rigid
bits of matter but they obey a wave equation known as the Schr¨odinger
equation. This idea is known as ﬁrst quantization. To summarize:
First quantization: Particles behave like waves.
(2.1)
However, this is not the end of the story. It was also realized that
things that had been previously thought of as waves were in fact par-
ticles. For example, electromagnetic waves and lattice waves were not
simply periodic undulations of some medium but they could actually
behave like particles. These phenomena were given particle-like names:
photons and phonons. This idea is known as second quantization. To
summarize:
Second quantization: Waves behave like particles.
(2.2)
Quite how these ideas link up is one of main themes in quantum ﬁeld
theory which sees the underlying reality behind both waves and particles
as a quantum ﬁeld. However, before we get to this point, it is worth
spending some time reviewing second quantization in a bit more detail
as it is the less familiar idea. In this chapter, we focus on the most
famous example of a wave phenomenon in physics: the oscillations of a
mass on a spring.
2.2
Mass on a spring
m
K
Fig. 2.1 A mass m suspended on a
spring, of spring constant K.
Consider a mass on a spring (as shown in Fig. 2.1), one of the simplest
physical problems there is. Assume we have mass m, spring constant
K, the displacement of the mass from equilibrium is given by x and the
momentum of the mass is given by p = m ˙x. The total energy E is the
sum of the kinetic energy p2/2m and the potential energy 1
2Kx2.
In quantum mechanics, we replace p by the operator −iℏ∂/∂x, and
we then have the Schr¨odinger equation for a harmonic oscillator

−ℏ2
2m
∂2
∂x2 + 1
2Kx2

ψ = Eψ.
(2.3)

20
Simple harmonic oscillators
The solutions to this equation can be obtained by a somewhat involved
series-solution method and are given by
ψn(ξ) =
1
√
2nn!
mω
πℏ
1/4
Hn(ξ)e−ξ2/2,
(2.4)
where Hn(ξ) is a Hermite polynomial and ξ =
p
mω/ℏx. As shown
in Fig. 2.2, these eigenfunctions look very wave-like.
However, they
do have a ‘particle-like’ quality which is apparent from the eigenvalues,
which turn out to be
En =

n + 1
2

ℏω,
(2.5)
where ω =
p
K/m. This is a ladder of energy levels (see Fig. 2.3), though
note when n = 0 the energy is not zero but ℏω/2. This is called the
zero-point energy. Because the energy levels form a ladder, you must
add a quantum of energy ℏω to move up the ladder by one rung. It is as
if you are adding a lump, or particle, of energy. Can we make this vague
feeling any more concrete? Yes we can. Moreover, we can do it quite
elegantly and without dirtying our hands with Hermite polynomials!
Fig. 2.2 Eigenfunctions of the simple
harmonic oscillator.
E = 0
E
n = 0
n = 1
n = 2
n = 3
n = 4
n = 5
· · ·
¯hω
Fig. 2.3 The ladder of energy levels for
the simple harmonic oscillator.
To accomplish this, we start with the Hamiltonian for the simple har-
monic oscillator written out as follows:
ˆH = ˆp2
2m + 1
2mω2ˆx2,
(2.6)
where we have expressed the spring constant as K = mω2. One feels
the seductive temptation to factorize this Hamiltonian and write it as
1
2mω2

ˆx −
i
mω ˆp
 
ˆx +
i
mω ˆp

,
(2.7)
but we hit a problem: in quantum mechanics, the operators ˆx and ˆp do
not commute. Hence, multiplying out the brackets in eqn 2.7 doesn’t
quite give us back ˆH but instead produces
1
2mω2

ˆx −
i
mω ˆp
 
ˆx +
i
mω ˆp

= 1
2mω2ˆx2 + ˆp2
2m + iω
2 [ˆx, ˆp],
(2.8)
where [ˆx, ˆp] ≡ˆxˆp −ˆpˆx = iℏis the commutator of ˆx and ˆp. The right-
hand side of eqn 2.8 is then just ˆH −ℏω
2 , which is nearly ˆH but has
the correction −ℏω
2 due to the zero-point energy being subtracted. The
factorization can therefore be made to work and we realize that the
operators ˆx −
i
mω ˆp and ˆx +
i
mω ˆp are clearly going to be useful. Since ˆp
and ˆx are Hermitian, the operators ˆx −
i
mω ˆp and ˆx +
i
mω ˆp are adjoints
of each other (and therefore are not themselves Hermitian, so cannot
correspond to any observable). We will give them the special names ˆa†
and ˆa, although we will include a multiplicative constant
p
mω/2ℏin
our deﬁnition so that they have particularly nice properties. Hence we
will write
ˆa
=
rmω
2ℏ

ˆx +
i
mω ˆp

,
(2.9)
ˆa†
=
rmω
2ℏ

ˆx −
i
mω ˆp

.
(2.10)

2.2
Mass on a spring
21
Example 2.1
The commutator [ˆa, ˆa†] can be evaluated as follows (remembering that [ˆx, ˆp] = iℏ):
[ˆa, ˆa†]
=
mω
2ℏ
„
−
i
mω [ˆx, ˆp] +
i
mω [ˆp, ˆx]
«
=
mω
2ℏ
„ ℏ
mω +
ℏ
mω
«
=
1.
(2.11)
The deﬁnitions of ˆa and ˆa† can be inverted to give
ˆx
=
r
ℏ
2mω (ˆa + ˆa†),
(2.12)
ˆp
=
−i
r
ℏmω
2
(ˆa −ˆa†).
(2.13)
Putting our new deﬁnitions of ˆa and ˆa† into our equation for the Hamil-
tonian yields
ˆH = ℏω

ˆa†ˆa + 1
2

.
(2.14)
The active ingredient in this Hamiltonian is the combination ˆa†ˆa. If
ˆa†ˆa has an eigenstate |n⟩with eigenvalue n, then ˆH will also have an
eigenstate |n⟩with eigenvalue ℏω(n + 1
2), so that we have recovered the
eigenvalues of a simple harmonic oscillator in eqn 2.5. However, we need
to prove that n takes the values 0, 1, 2, . . .. The ﬁrst step is to show that
n ≥0. We can do that by noting that
n = ⟨n|ˆa†ˆa|n⟩= |ˆa|n⟩|2 ≥0.
(2.15)
Next, we have to show that n takes only integer values, and we will
do that below but beforehand let us introduce a bit of notation to save
some writing. We deﬁne the number operator ˆn by
ˆn = ˆa†ˆa,
(2.16)
and hence write
ˆn|n⟩= n|n⟩.
(2.17)
Number of what? The quantity n labels the energy level on the ladder
(see Fig. 2.3) that the system has reached, or equivalently the number
of quanta (each of energy ℏω) that must have been added to the system
when it was in its ground state. We can rewrite the Hamiltonian as
ˆH = ℏω

ˆn + 1
2

,
(2.18)
and therefore
ˆH|n⟩=

n + 1
2

ℏω|n⟩,
(2.19)
so that |n⟩is also an eigenstate of the Hamiltonian.
Thus |n⟩is a
convenient shorthand for the more complicated form of ψn(ξ) shown in
eqn 2.4. The next examples show that the eigenvalue n indeed takes
integer values.

22
Simple harmonic oscillators
Example 2.2
This example looks at the property of the state deﬁned by ˆa†|n⟩. One of the things
we can do is to operate on this with the number operator:
ˆnˆa†|n⟩= ˆa†ˆaˆa†|n⟩.
(2.20)
Using the commutator in eqn 2.11 gives ˆaˆa† = 1 + ˆa†ˆa and hence
ˆnˆa†|n⟩= (n + 1)ˆa†|n⟩.
(2.21)
The above example shows that the state ˆa†|n⟩is an eigenstate of ˆH but
with an eigenvalue one higher than the state |n⟩. In other words, the
operator ˆa† has the eﬀect of adding one quantum of energy. For this
reason ˆa† is called a raising operator.
Example 2.3
This example looks at the property of the state deﬁned by ˆa|n⟩. We can operate on
this with the number operator:
ˆnˆa|n⟩= ˆa†ˆaˆa|n⟩.
(2.22)
Using the commutator in eqn 2.11 gives ˆa†ˆa = ˆaˆa† −1 and hence
ˆnˆa|n⟩= (n −1)ˆa|n⟩.
(2.23)
The above example shows that the state ˆa|n⟩is an eigenstate of ˆH but
with an eigenvalue one lower than the state |n⟩. In other words, the
operator ˆa has the eﬀect of subtracting one quantum of energy. For this
reason ˆa is called a lowering operator.
Example 2.4
Question: Normalize the operators ˆa and ˆa†.
Solution: We have shown that ˆa|n⟩= k|n −1⟩, where k is a constant. Hence, taking
the norm of this state (i.e. premultiplying it by its adjoint) gives
|ˆa|n⟩|2 = ⟨n|ˆa†ˆa|n⟩= |k|2⟨n −1|n −1⟩= |k|2,
(2.24)
where the last equality is because the simple harmonic oscillator states are normalized
(so that ⟨n −1|n −1⟩= 1). However, we notice that ˆa†ˆa = ˆn is the number operator
and hence
⟨n|ˆa†ˆa|n⟩= ⟨n|ˆn|n⟩= n.
(2.25)
Equations 2.24 and 2.25 give k = √n. (This assumes k to be real, but any state
contains an arbitrary phase factor, so we are free to choose k to be real.)
In the same way, we have shown that ˆa†|n⟩= c|n + 1⟩, where c is a constant.
Hence,
|ˆa†|n⟩|2 = ⟨n|ˆaˆa†|n⟩= |c|2⟨n + 1|n + 1⟩= |c|2,
(2.26)
and using ˆaˆa† = 1 + ˆa†ˆa = 1 + ˆn, we have that
⟨n|ˆaˆa†|n⟩= ⟨n|1 + ˆn|n⟩= n + 1,
(2.27)
and hence c = √n + 1 (choosing c to be real). In summary, our results are:
ˆa|n⟩
=
√n|n −1⟩,
(2.28)
ˆa†|n⟩
=
√
n + 1|n + 1⟩.
(2.29)

2.3
A trivial generalization
23
If we keep hitting the state |n⟩with ˆa, we eventually get to |0⟩, the
ground state of the simple harmonic oscillator. At this point, we will
annihilate the state completely with a further application of ˆa because
ˆa|0⟩= 0. Notice that
ˆH|0⟩= ℏω

ˆn + 1
2

|0⟩= 1
2ℏω|0⟩,
(2.30)
so this really is the ground state and we see that the energy is 1
2ℏω, the
zero-point energy.1
1To recap, we can write
|ˆa|n⟩|2
=
⟨n|ˆa†ˆa|n⟩
=
⟨n|
ˆH
ℏω −1
2|n⟩
=
En
ℏω −1
2,
where En = (n + 1
2)ℏω. Thus the con-
dition ˆa|0⟩= 0 implies E0
ℏω −1
2 = 0, and
hence the energy of the ground state is
given by E0 = 1
2 ℏω.
It is also possible to go the other way, operating on |0⟩with the raising
operator ˆa†. This leads to
ˆa†|0⟩
=
|1⟩,
(2.31)
ˆa†|1⟩
=
√
2|2⟩
=⇒
|2⟩= (ˆa†)2
√
2 |0⟩,
(2.32)
ˆa†|2⟩
=
√
3|3⟩
=⇒
|3⟩=
(ˆa†)3
√3 × 2|0⟩,
(2.33)
and in general
|n⟩= (ˆa†)n
√
n!
|0⟩,
(2.34)
so the state |n⟩can be obtained by repeated application of the operator
ˆa†. This leads to a completely diﬀerent way of thinking about these
new operators: we call ˆa† a creation operator and ˆa an annihilation
operator. We imagine ˆa† acting to create a quantum of energy ℏω and
move the oscillator up one rung of the ladder. Its adjoint, ˆa, acts to
annihilate a quantum of energy ℏω and move the oscillator down one
rung of the ladder (see Fig. 2.4). These quanta of energy behave like
particles; we are adding and subtracting particles by the application of
these operators and we have realized the dream of second quantization.
A wave problem has spontaneously produced particles!
E = 0
E
|0⟩
|1⟩
|2⟩
|3⟩
|4⟩
|5⟩
· · ·
ˆa†
ˆa†
ˆa†
ˆa†
ˆa†
ˆa
ˆa
ˆa
ˆa
ˆa
ˆa
Fig. 2.4 The ladder of energy levels for
the simple harmonic oscillator. The op-
erator ˆa† moves the oscillator up one
rung of the ladder. Its adjoint, ˆa, acts
to move the oscillator down one rung
of the ladder. Note that ˆa|0⟩= 0, so
dropping oﬀthe bottom of the ladder
results in zero.
2.3
A trivial generalization
The next thing we can do is a completely trivial generalization of what
we have done before, but it is worth thinking about because certain
complicated problems reduce to it. Consider a set of N uncoupled simple
harmonic oscillators. They are uncoupled, so one could be on your desk,
another sitting in your bathroom, another one out in the park. They
don’t talk to each other, inﬂuence each other or aﬀect each other in
any way. Nevertheless, just for fun, we are going to consider their joint
Hamiltonian ˆH which is simply the sum of individual Hamiltonians ˆHk
where k runs from 1 to N. Hence
ˆH =
N
X
k=1
ˆHk,
(2.35)

24
Simple harmonic oscillators
and
ˆHk =
ˆp2
k
2mk
+ 1
2mkω2
kˆx2
k.
(2.36)
So far, nothing complicated. We now deﬁne the operator ˆa†
k which cre-
ates a quantum of energy in the kth oscillator (and leaves all the others
unaﬀected). We also deﬁne the operator ˆak which annihilates a quan-
tum of energy in the kth oscillator (and leaves all the others unaﬀected).
Acting with an operator ak only aﬀects the number of quanta for the
kth oscillator. We can therefore write
ˆa†
k|n1, n2, ..., nr, ...⟩∝|n1, n2, ..., nk + 1, ...⟩,
(2.37)
ˆak|n1, n2, ..., nk, ...⟩∝|n1, n2, ..., nk −1, ...⟩.
The operators will have the commutation rules:
[ˆak, ˆaq]
=
0,
(2.38)
h
ˆa†
k, ˆa†
q
i
=
0,
(2.39)

ˆak, ˆa†
q

=
δkq.
(2.40)
In eqn 2.40, the point is that operators acting on diﬀerent oscillators
commute (and therefore do not aﬀect each other). Hence we can follow
the results of the previous section and write the Hamiltonian in eqn 2.35
as
ˆH =
N
X
k=1
ℏωk

ˆa†
kˆak + 1
2

.
(2.41)
Again we have to deﬁne a vacuum state |0, 0, 0, 0, 0, ...⟩(usually just
called |0⟩), such that
ˆak|0⟩= 0,
(2.42)
for all k. This is the state in which every one of the oscillators is in its
ground state.
A general state of the system, written as |n1, n2, · · · , nN⟩is known as
the occupation number representation. Using the same techniques
as in Section 2.2, we can write this general state as
|n1, n2, · · · , nN⟩=
1
√n1!n2! · · · nN!(ˆa†
1)n1(ˆa†
2)n2 · · · (ˆa†
N)nN |0, 0, · · · 0⟩.
(2.43)
The idea here is that we are operating on the vacuum state with a
product of creation operators necessary to put n1 quanta of energy into
oscillator number 1, n2 quanta of energy into oscillator number 2, etc.
We can express this even more succinctly using the following notation
|{nk}⟩=
Y
k
1
√nk
(ˆa†
k)nk|0⟩,
(2.44)
where |0⟩is the vacuum state, as explained above.
In this model, all of the oscillators have been completely indepen-
dent. It is now time to tackle a more challenging problem, coupling the
oscillators together, and that is covered in the next section.

2.4
Phonons
25
2.4
Phonons
Consider a linear chain of N atoms (see Fig. 2.5), each of mass m, and
connected by springs of unstretched length a and which have spring
constant K. The masses are normally at position Rj = ja, but can be
displaced slightly by an amount xj. The momentum of the jth mass is
pj.
The Hamiltonian for this system is
m
m
m
j −1
j
j + 1
K
xj
Rj = ja
Fig. 2.5 A linear chain of atoms.
ˆH =
X
j
ˆp2
j
2m + 1
2K(ˆxj+1 −ˆxj)2.
(2.45)
In contrast to the model in the previous section, we are now dealing
with a coupled problem. Each mass is strongly coupled to its neighbour
by the springs and there is no way in which we can consider them as
independent. However, we will show that the excitations in this system
behave exactly as a set of totally independent oscillators. This works
because we can Fourier transform the problem, so that even though
the masses are coupled in real space, the excitations are uncoupled in
reciprocal space. How this is done is covered in the following example.
Example 2.5
We begin by Fourier transforming both xj and pj, by writing
For the time being, we will drop the
‘hats’ on ˆxj and ˆpj to save cluttering
the algebra. We note that there is an-
other way of solving this problem which
involves writing the operators xj and
pj in terms of creation and annihila-
tion operators of particular modes. It
is arguably a more elegant route to the
answer and will generalize more easily
to additional cases, but as it is a little
more abstract we will save it for later
in the book.
xj
=
1
√
N
X
k
˜xkeikja,
(2.46)
pj
=
1
√
N
X
k
˜pkeikja,
(2.47)
and equivalently of course
˜xk
=
1
√
N
X
j
xje−ikja,
(2.48)
˜pk
=
1
√
N
X
j
pje−ikja.
(2.49)
We impose periodic boundary conditions2 forcing eikja = eik(j+N)a The wave vector
2Periodic boundary conditions are de-
scribed in detail in the next chapter.
k therefore takes the values 2πm/Na, where m is an integer in the range −N/2 <
m ≤N/2. Note that
X
j
eikja = Nδk,0.
(2.50)
The commutation relations are
The second quantization trick
to evaluate sums of the form
1
N
X
j
X
kq
˜pk ˜pqei(k+q)ja :
(2.51)
Step
1:
Perform
the
spatial
sum,
making
use
of
the
identity
P
j ei(k−k′)ja = Nδkk′. This gives
X
pq
˜pk ˜pqδk,−q.
(2.52)
Step 2: Use the Kronecker delta to do
one of the momentum sums. This has
the eﬀect of setting q = −k, leaving us
with a sum over a single index:
X
k
˜pk ˜p−k.
(2.53)
ˆ
xj, pj′
˜
=
iℏδjj′,
(2.54)
[˜xk, ˜pk′]
=
1
N
X
j
X
j′
e−ikjae−ik′j′a ˆ
xj, pj′
˜
=
iℏ
N
X
j
e−i(k+k′)ja
=
iℏδk,−k′.
(2.55)
We can now work out terms in the Hamiltonian, so that
X
j
p2
j
=
X
j
 
1
√
N
X
k
˜pkeikja
!  
1
√
N
X
k′
˜pk′eik′ja
!
=
1
N
X
j
X
k
X
k′
˜pk ˜pk′ei(k+k′)ja.
(2.56)

26
Simple harmonic oscillators
This combination of sums over one spatial and two momentum indices will occur
frequently and is easily dealt with using the trick shown in the box. The result is
X
j
p2
j =
X
k
˜pk ˜p−k.
(2.57)
The other term in the Hamiltonian may be treated similarly and we have
X
j
(xj+1 −xj)2
=
1
N
X
j
X
k
X
k′
˜xk˜xk′ei(k+k′)ja(eika −1)(eik′a −1)
=
X
k
˜xk˜x−k
„
4 sin2 ka
2
«
,
(2.58)
where use has been made of the identity 1−cos ka = 2 sin2 ka
2 . We can thus express3
3Notational point: For clarity, we will
now omit the tilde from ˜xk and ˜pk, but
to remind ourselves that they are op-
erators we will reinstate the ‘hats’, so
they become ˆxk and ˆpk.
the Hamiltonian as
ˆH =
X
k
» 1
2m ˆpk ˆp−k + 1
2mω2
kˆxkˆx−k
–
,
(2.59)
where ω2
k = (4K/m) sin2(ka/2).
With our deﬁnitions we automatically have that ˆp†
k = ˆp−k and ˆx†
k = ˆx−k (which
follow from the requirement that ˆpj and ˆxj are Hermitian), so we can write down
the commutation relation as [ˆxk, ˆp†
k′] = iℏδkk′ and we can also write down creation
and annihilation operators as follows:
ˆak
=
rmωk
2ℏ
„
ˆxk +
i
mωk
ˆpk
«
,
(2.60)
ˆa†
k
=
rmωk
2ℏ
„
ˆxk −
i
mωk
ˆpk
«
,
(2.61)
which have commutation relations [ˆa†
k, ˆa†
k′] = [ˆak, ˆak′] = 0 and [ˆak, ˆa†
k′] = δk,k′. We
can invert eqns 2.60 and 2.61 to yield
ˆxk
=
s
ℏ
2mωk
“
ˆak + ˆa†
−k
”
,
(2.62)
ˆpk
=
−i
r
mℏωk
2
“
ˆak −ˆa†
−k
”
.
(2.63)
The Hamiltonian becomes
ˆH =
X
k
ℏωk
2
“
ˆakˆa†
k + ˆa†
−kˆa−k
”
,
(2.64)
and reindexing this gives
ˆH =
X
k
ℏωk
2
“
ˆakˆa†
k + ˆa†
kˆak
”
.
(2.65)
Using the commutator4 then produces
4i.e. ˆakˆa†
k −ˆa†
kˆak = 1.
ˆH =
N
X
k=1
ℏωk
„
ˆa†
kˆak + 1
2
«
.
(2.66)
This example has demonstrated that the Hamiltonian for a set of coupled
masses on a chain can be expressed in terms of a sum over modes,
labelled by wave vector k. Comparing this result (eqn 2.66) with that
obtained at the end of the previous section (eqn 2.44), one sees that
they are identical.
Thus these modes behave as if they are entirely
independent and uncoupled simple harmonic oscillators. We call these
modes phonons, and each phonon mode, labelled by k, can be given
integer multiples of the quantum of energy ℏωk.

Exercises
27
This is the key point. Each phonon mode behaves like a simple har-
monic oscillator and so can accept energy in an integer number of lumps,
each lump being of size ℏωk. This is because the energy eigenvalues of a
simple harmonic oscillator form a ladder of energy levels where the rung-
size is ﬁxed. These lumps of energy look like particles, and so we think
of phonons themselves as particles. Later on, we play exactly the same
trick with the electromagnetic ﬁeld and show that it behaves like a set
of uncoupled simple harmonic oscillators and call the quanta photons.
This is the heart of second quantization: wave problems have oscillator
solutions and hence produce particles! The key insight in this chapter
is essentially that the oscillator picture of physical systems leads to a
particle picture of those systems. This notion is summarized in Fig. 2.6.
Simple harmonic oscillator
in the nth level
En =

n + 1
2

¯hωk
n particles of energy ¯hωk
PARTICLE PICTURE
OSCILLATOR PICTURE
⇐⇒
Fig. 2.6 The equivalence of the oscil-
lator and particle pictures.
Chapter summary
• First quantization shows that particles behave like waves; second
quantization shows that waves behave like particles.
• The simple harmonic oscillator has energy eigenvalues given by
En = (n + 1
2)ℏω. Eigenstates can be written in the occupation
number representation.
• Using creation and annihilation operators the Hamiltonian for the
simple harmonic oscillator can be written ˆH = ℏω(ˆa†ˆa + 1
2).
• The phonon problem can be re-expressed as a sum over non-
interacting modes, each one of which behaves like a simple har-
monic oscillator.
Exercises
(2.1) For the one-dimensional harmonic oscillator, show
that with creation and annihilation operators de-
ﬁned as in eqns 2.9 and 2.10, [ˆa, ˆa] = 0, [ˆa†, ˆa†] = 0,
[ˆa, ˆa†] = 1 and ˆH = ℏω(ˆa†ˆa + 1
2).
(2.2) For the Hamiltonian ˆH =
ˆp2
2m + 1
2mω2ˆx2 + λˆx4,
where λ is small, show by writing the Hamiltonian
in terms of creation and annihilation operators and
using perturbation theory, that the energy eigenval-
ues of all the levels are given by
En =
„
n + 1
2
«
ℏω + 3λ
4
„ ℏ
mω
«2
(2n2 + 2n + 1).
(2.67)
(2.3) Use eqns 2.46 and 2.62 to show that
ˆxj =
1
√
N
„ ℏ
m
« X
k
1
(2ωk)1/2 [ˆakeikja + ˆa†
ke−ikja].
(2.68)
(2.4) Using ˆa|0⟩= 0 and eqns 2.9 and 2.10 together with
⟨x|p|ψ⟩= −iℏd
dx⟨x|ψ⟩, show that
0 =
„
x +
ℏ
mω
d
dx
«
⟨x|0⟩,
(2.69)
and hence
⟨x|0⟩=
“mω
πℏ
”1/4
e−mωx2/2ℏ.
(2.70)

3
Occupation number
representation
3.1 A particle in a box
28
3.2 Changing the notation
29
3.3 Replace state labels with op-
erators
31
3.4 Indistinguishability and sym-
metry
31
3.5 The continuum limit
35
Chapter summary
36
Exercises
36
I am not a number. I am a free man.
Patrick McGoohan in The Prisoner
In the previous chapter we considered simple harmonic oscillators,
showed that such oscillators have particle-like eigenfunctions, and in-
troduced en passant the occupation number representation to describe
sets of uncoupled oscillators. Now we are going to make further use of
this idea to describe systems of many particles more generally. The use
of the occupation number representation to describe a system of identi-
cal particles is partly a matter of notational administration and partly
a radical change in the way we look at the world. We (i) change the
way we label our states (this is administration) and (ii) get rid of wave
functions altogether (this is the radical part). We replace wave func-
tions with a version of the creation and annihilation operators we used
to solve the quantum oscillator problems. This turns out to simplify
things enormously. In setting up this new method we will need to en-
sure that we never violate the sacred laws of indistinguishable particles.
These are as follows:
• There are two types of particle in the Universe: Bose1 particles
1Satyendra
Nath
Bose
(1894–1974)
wrote a paper in 1924 which laid the
foundations for quantum statistics.
and Fermi2 particles.
2In addition to his many contribu-
tions to other branches of physics, En-
rico Fermi (1901–1954) made numerous
contributions to quantum ﬁeld theory.
His 1932 review of Quantum Electro-
dynamics (Rev. Mod. Phys. 4, 87) had
enormous inﬂuence on the ﬁeld.
• If you exchange two identical bosons you get the same state again.
• If you exchange two fermions, you get minus the state you started
with.3
3A consequence of this principle is that
fermions obey the Pauli exclusion prin-
ciple: It is impossible to put two in the
same quantum state. This aﬀects the
statistical distribution among a set of
discrete energy levels of these indistin-
guishable quantum particles, and so is
referred to as quantum statistics. In
Sections 3.1–3.3 we won’t worry about
quantum statistics, but return to the
issue in Section 3.4.
To make things simple we’ll conﬁne our particles to a box for most of
this chapter. Although this makes the notation ugly (and non-covariant)
it has several advantages (not least that we can normalize the states in
a simple way). We recap the physics of particle in a box in the next
section.
3.1
A particle in a box
We will now choose units equivalent to setting ℏ= 1. Therefore the
momentum operator for motion along the x-direction ˆp can be written
ˆp = −i ∂
∂x.
(3.1)

3.2
Changing the notation
29
Solutions to the Schr¨odinger equation for a particle in a box of size
L are running waves ψ(x) =
1
√
Leipx, and eigenstates of the momentum
operator [ˆpψ(x) = −i dψ
dx = pψ(x)]. We use periodic boundary conditions
which force us to have the same amplitude for our wave at equivalent
positions of each box, that is ψ(x) = ψ(x+L). Therefore eipx = eip(x+L),
and hence eipL = 1. In order to satisfy this condition, we need pL =
2πm, with m an integer. This imposes a quantization condition on the
possible momentum states that particles in the box can take. They must
satisfy
pm = 2πm
L .
(3.2)
We’ll call the possible momentum states things like p1 and p2, etc. The
energy of a single particle in a state pm is Epm, which depends on the
momentum state, but it doesn’t matter how yet.
p
np
p1
p2
1
2
3
Fig. 3.1 A two-particle state given by
|p1p2⟩.
To make a simple many-particle system we’ll put more and more non-
interacting Bose particles in the box.
How does this aﬀect the total
momentum and energy of the system? Applying the momentum opera-
tor to the two-particle state (see Fig. 3.1) yields
ˆp|p1p2⟩= (p1 + p2)|p1p2⟩,
(3.3)
and applying the Hamiltonian operator gives us
ˆH|p1p2⟩= (Ep1 + Ep2)|p1p2⟩.
(3.4)
Since the particles don’t interact, having two particles in a particular
energy state (p3 say) costs an energy 2Ep3, i.e. double the single-particle
energy. The total energy is given by P
m npmEpm, where npm is the
total number of particles in the state |pm⟩.
3.2
Changing the notation
As advertised, the ﬁrst thing we’ll do is change the notation. In conven-
tional quantum mechanics we label each identical particle (e.g. particle
A, particle B, particle C) and list its momentum. This gives rise to states
labelled like |ABC⟩= |p1p2p1⟩for a three-particle state. We read this
as ‘particle A in momentum state p1, particle B in state p2 and particle
C in state p1’. This is shown in Fig. 3.2 How could we streamline this
p
np
p1
p2
1
2
3
Fig. 3.2
A three-particle state given
by |p1p2p1⟩can be written as |2100 · · · ⟩
in the occupation number representa-
tion.
notation? We haven’t yet used the facts that the particles are indistin-
guishable (making the assignment A, B, C, etc. rather meaningless) and
that there are only certain allowable values of momentum (i.e. 2πm/L).
We can therefore decide that instead of listing which particle (A, B, C,
etc.) is in which state pm, we could just list the values of the momentum
pm, saying how many particles occupy each momentum state. For our
example, we’d say ‘two particles are in momentum state p1, one is in
momentum state p2’. We therefore deﬁne a state of N particles by list-
ing how many are in each of the momentum states. We will write these
as |n1n2n3 . . .⟩. Our example is written |2100 . . .⟩. Specifying a state by

30
Occupation number representation
listing the number of identical particles in each quantum state is another
instance of the occupation number representation, which we met
in the last chapter.
Example 3.1
Let’s compare the old and new notation for a system with only two possible states:
p1 = 2π/L and p2 = 4π/L. Into these states we can put any number of Bose particles
that we choose. The table in the margin compares the old and new notations.
Particles
old
new
0
|0⟩
|00⟩
1
|q1⟩
|10⟩
1
|q2⟩
|01⟩
2
|q1q1⟩
|20⟩
2
|q1q2⟩
|11⟩
2
|q2q2⟩
|02⟩
3
|q1q1q1⟩
|30⟩
...
...
...
and so on.
Table of old and new notations.
What happens when we act on a state in occupation number represen-
tation with the Hamiltonian? As we saw above, the answer needs to
be
ˆH|n1n2n3 . . .⟩=
"X
m
npmEpm
#
|n1n2n3 . . .⟩,
(3.5)
that is, we multiply the number of particles in each state by the energy
of that state and sum over all of the states. This is obviously the right
thing to do to ﬁnd the total energy.
Now for the reason why occupation numbers are useful. Equation 3.5
can be made to look quite similar to something we’ve seen before. Recall
that a quantum harmonic oscillator had a total energy given by E =
 n + 1
2

ℏω with n being some integer. Dropping the zero-point energy
(which just amounts to a new choice in our zero of energy) we can
write this as E = nℏω.
In words, the oscillator has n quanta in it.
Crucially, the energy levels are equally spaced, so having two quanta
in the system gives an energy which is twice that for a single quantum
in the system. Now imagine N independent oscillators labelled by the
index k. These might not be identical, so we say that the energy level
spacing of the kth oscillator is ℏωk. The total energy of this system
is now E = PN
k=1 nkℏωk. So the kth oscillator has nk quanta in it,
contributing an energy nkℏωk to the sum. Compare this to our system
of identical particles for which the total energy is E = P
pm npmEpm.
In words, this says that the momentum state pm has npm particles in
it. Each particle contributes an energy Epm giving a contribution of
npmEpm for the momentum mode pm.
These two systems therefore
have the same energy level structure.
We have built an analogy between two completely diﬀerent systems:
harmonic oscillators and identical particles.
SHO
Identical particles
Quanta in oscillators
→
Particles in momentum states
kth oscillator
→
mth momentum mode pm
E = PN
k=1 nkℏωk
→
E = PN
m=1 npmEpm.
(3.6)
As we’ll see in the next section, the real value to this analogy is that it
can be pushed further.4
4We should be a little careful here;
we haven’t physically replaced par-
ticles with pendulums.
We’ve sim-
ply exploited the fact that the sim-
ple harmonic oscillator energy levels are
equally spaced, so, for example, two
quanta in an oscillator have an energy
that is twice that of a single quantum
in that oscillator.

3.3
Replace state labels with operators
31
3.3
Replace state labels with operators
The notation change to occupation number representation is just win-
dow dressing. We’re now going to make a more dramatic change in an
attempt to almost remove state vectors altogether! We’ll do this by hav-
ing operators describing the physics rather than state vectors. We’ll have
to retain only one special state, the vacuum |0⟩. Following eqn 2.43,5
5For the oscillator problem in Chap-
ter 2 we had the very useful technol-
ogy of creation and annihilation oper-
ators to add and remove quanta from
the SHO system, and we use the same
idea here.
we will build up a general state of several harmonic oscillators by acting
on the vacuum state |0⟩and write
|n1n2 · · · ⟩=
Y
k
1
(nk!)
1
2 (ˆa†
k)nk|0⟩.
(3.7)
Example 3.2
We can build up a state with quanta arranged among various oscillators. A state of
the system in which there are two quanta in oscillator number 1 and one quantum
in oscillator number 2 (written in occupation number representation as |21000 . . .⟩)
can be written as
|21000 . . .⟩=
» 1
√
2!
(ˆa†
1)2
– » 1
√
1!
ˆa†
2
–
|0⟩.
(3.8)
However, what we are doing here is not just talking about creating
quanta in oscillators, but also particles in particular momentum states.
We want a creation operator ˆa†
pm to create one particle in a momentum
state |pm⟩. It’s hopefully now just a matter of changing k →pm to go
from operators describing oscillators to operators describing particles in
momentum states and we say that ˆa†
pm creates a particle in momentum
state |pm⟩rather than a quantum in oscillator k. Before we can do this,
we need to check that doing this doesn’t violate any of the symmetry
principles we stated at the start of this chapter. That is, we need to
know how this new formulation deals with Bose and Fermi particles.
3.4
Indistinguishability and symmetry
We now have to make sure that introducing creation and annihilation of
states will respect the exchange symmetry principles for identical par-
ticles.
It will turn out that the basic technology is supplied by the
commutation relations of the creation ˆa†
pm and annihilation ˆapm opera-
tors for a particular state. The key point is that it matters which order
you use these operators to put particles into states.
Example 3.3
Consider a system with two momentum states p1 and p2 described in the occupation
number representation as |n1n2⟩formed from creation operators acting on a vacuum
state |0⟩. We’ll deﬁne
ˆa†
p1|0⟩= |10⟩,
ˆa†
p2|0⟩= |01⟩.
(3.9)

32
Occupation number representation
Now let’s add another particle in the unoccupied state. This gives us
ˆa†
p2ˆa†
p1|0⟩∝|11⟩,
ˆa†
p1ˆa†
p2|0⟩∝|11⟩.
(3.10)
The proportionality sign is because the constant remains to be determined.
Whether you ﬁrst put one particle in state p1 and then another in state
p2, or do the same thing in the reverse order, you must end up with the
same ﬁnal state: |11⟩. This means that
ˆa†
p1ˆa†
p2 = λˆa†
p2ˆa†
p1,
(3.11)
where λ is a constant. We assume two possibilities,6 λ = ±1, and they
6In Chapter 29 we examine the possi-
bility of other choices.
correspond to wave functions which are either symmetric or antisym-
metric under particle exchange. The particles are then called bosons
or fermions and we will ﬁnd that the two cases correspond to two dif-
ferent types of commutation relation for the creation and annihilation
operators. We will examine each case in turn.
Case I: λ = 1, bosons: In this case
ˆa†
p2ˆa†
p1 = ˆa†
p1ˆa†
p2.
(3.12)
Rearranging (and generalizing the labels of the momentum states to i
and j) we ﬁnd
h
ˆa†
i, ˆa†
j
i
= ˆa†
iˆa†
j −ˆa†
jˆa†
i = 0,
(3.13)
which means that the creation operators for diﬀerent particle states com-
mute. We also have [ˆai, ˆaj] = 0 and, importantly, we deﬁne
h
ˆai, ˆa†
j
i
= δij,
(3.14)
so that we can use our harmonic oscillator analogy. We conclude that
these commutation relations are the same as those for simple harmonic
oscillators above. We’ve succeeding in developing a system to describe
many particles!
The formalism is identical to that we developed for
many independent oscillators in the previous section. We can therefore
build up a general state of many particles in momentum states using
|n1n2 · · · ⟩=
Y
m
1
(npm!)
1
2 (ˆa†
pm)npm |0⟩.
(3.15)
The particles this formalism describes are Bose particles: we can put any
number into quantum states and, as we’ll see later, they are symmetric
upon exchange of particles.
Finally we note that the commutation of diﬀerent operators implies
that
ˆa†
p1ˆa†
p2|0⟩= ˆa†
p2ˆa†
p1|0⟩= |1p11p2⟩,
(3.16)
that is, it doesn’t matter which order you put particles in the states,
you get exactly the same in either case.

3.4
Indistinguishability and symmetry
33
In general we can write
ˆa†
i|n1 · · · ni · · · ⟩
=
√
ni + 1|n1 · · · ni + 1 · · · ⟩,
(3.17)
ˆai|n1 · · · ni · · · ⟩
=
√ni|n1 · · · ni −1 · · · ⟩.
(3.18)
Case II: λ = −1, fermions: In this case we’ll write the fermion cre-
ation operators ˆc†
i to distinguish them from the boson operators (written
ˆa†
i). Plugging in λ = −1, we ﬁnd that
n
ˆc†
i, ˆc†
j
o
≡ˆc†
i ˆc†
j + ˆc†
jˆc†
i = 0,
(3.19)
where the curly brackets indicate an anticommutator. Fermion oper-
ators anticommute: when you change their order you pick up a minus
sign since ˆc†
i ˆc†
j = −ˆc†
jˆc†
i. In particular, setting i = j we ﬁnd
ˆc†
i ˆc†
i + ˆc†
i ˆc†
i = 0
and hence
ˆc†
i ˆc†
i = 0.
(3.20)
In other words, trying to shoehorn two fermions into the same state an-
nihilates it completely. This is the Pauli exclusion principle7 which
7The Pauli exclusion principle is named
after Wolfgang Pauli (1900–1958), the
Austrian theoretical physicist who was
one of the pioneers of quantum mechan-
ics, and also of quantum ﬁeld theory
and quantum electrodynamics, which
were his main concerns after 1927. He
referred to fellow-physicist Wolfgang
Paul (1913–1993) as his ‘real part’.
Pauli was merciless with his criticism of
other physicists and their work, famous
for his put-downs, one of the most well-
known of which was that a particular
scientiﬁc paper was ‘not even wrong’.
means that each state can either be unoccupied or contain a single
fermion. We also have
{ˆci, ˆcj} = 0.
(3.21)
Finally we deﬁne an anticommutator
n
ˆci, ˆc†
j
o
= δij,
(3.22)
which enables us to use the simple harmonic oscillator analogy here too.
Again we can describe putting particles in momentum states in the same
way as putting quanta in independent oscillators. The only diﬀerence is
that we use anticommutators rather than commutators.
Finally we note that since ˆc†
i ˆc†
j|0⟩= −ˆc†
jˆc†
i|0⟩, it really does matter
which order you put the particles into the states.
Example 3.4
Let’s check that the number operator still works for fermions. The number operator
is ˆni = ˆc†
i ˆci. We’ll see the action of ˆn1 on a state |11⟩≡ˆc†
1ˆc†
2|0⟩. We expect that this
is an eigenstate with eigenvalue 1 if all goes to plan because there is one particle in
the i = 1 state. We’ll now proceed to check this:
ˆn1|11⟩
=
ˆc†
1ˆc1|11⟩
=
ˆc†
1ˆc1(ˆc†
1ˆc†
2|0⟩)
=
ˆc†
1(1 −ˆc†
1ˆc1)ˆc†
2|0⟩
(using the anticommutation relation)
=
ˆc†
1ˆc†
2|0⟩≡|11⟩,
(3.23)
i.e. |11⟩is an eigenstate of ˆn1 with eigenvalue 1 as we expected.

34
Occupation number representation
For fermions, in order to get the signs right, the normalization of creation
and annihilation operators are deﬁned as follows
ˆc†
i|n1 · · · ni · · · ⟩
=
(−1)Σi√
1 −ni|n1 · · · ni + 1 · · · ⟩,
(3.24)
ˆci|n1 · · · ni · · · ⟩
=
(−1)Σi√ni|n1 · · · ni −1 · · · ⟩,
(3.25)
where
(−1)Σi = (−1)(n1+n2+···+ni−1).
(3.26)
This gives us a factor of −1 for every particle standing to the left of the
state labelled ni in the state vector.
The square roots are redundant in
eqns 3.24 and 3.25 since ni = 0 or 1,
so they can be omitted if desired.
Example 3.5
Let’s check to see we’ve got it right when we come to exchange the particles. Remem-
ber that for bosons we get the same state upon exchange and for fermions we pick
up a minus sign. To exchange particles we need to imagine a sequence of operations
which swap particles around. An example would be
|110⟩
a state with particles in states 1 and 2
(3.27)
→|101⟩
move particle from state 2 to 3
→|011⟩
move particle from state 1 to 2
→|110⟩
move particle from state 3 to 1.
To move a particle we annihilate it from its original state and then create it in its
new state. Moving a particle from state 2 to 3 is achieved using ˆa†
3ˆa2|110⟩The full
sequence of events can be described with the string of operators
ˆa†
1ˆa3ˆa†
2ˆa1ˆa†
3ˆa2|110⟩= ±|110⟩,
(3.28)
where we want the + sign for bosons and the −sign for fermions.
Let’s check that the commutation relations give the correct results for swapping
Bose particles. We can swap operators that refer to diﬀerent states (since [ˆai, ˆaj] = 0)
to get
ˆa†
1ˆa3ˆa†
2ˆa1ˆa†
3ˆa2|110⟩= ˆa3ˆa†
3ˆa†
1ˆa1ˆa†
2ˆa2|110⟩.
(3.29)
But ˆa†
i ˆai is just the number operator ˆni, counting the number of states in a mode i.
ˆa3ˆa†
3ˆn1ˆn2|110⟩= (1) × (1) × ˆa3ˆa†
3|110⟩,
(3.30)
we need to turn ˆa3ˆa†
3 into a number operator. Swap the order with the commutators
to get
(1 + ˆa†
3ˆa3)|110⟩= (1 + ˆn3)|110⟩,
(3.31)
and notice that ˆn3|110⟩= 0 to obtain (1 + ˆa†
3ˆa3)|110⟩= |110⟩, as expected.
Let’s now check that the commutation relations give the correct results for swap-
ping Fermi particles. Now when we swap any two fermion operators we pick up a
minus sign. Carrying out the same steps as before
ˆc†
1ˆc3ˆc†
2ˆc1ˆc†
3ˆc2|110⟩
=
−ˆc3ˆc†
3ˆc†
1ˆc1ˆc†
2ˆc2|110⟩
(3.32)
=
−(1 −ˆc†
3ˆc3)|110⟩
=
−|110⟩.
Exchanging the particles gives us the required minus sign.

3.5
The continuum limit
35
3.5
The continuum limit
So far our particles have been conﬁned in a box, but we will frequently
wish to work in the continuum limit of the formalism. As the box size
increases, the momentum states become more closely spaced until, in the
limit, the momentum becomes a continuous variable. The Kronecker δij
functions we’ve been using become Dirac δ(3)(p) functions and the sums
become integrals. We have, for the continuous case, that
[ˆap, ˆa†
q] = δ(3)(p −q),
(3.33)
and for energies
ˆH =
Z
d3p Epˆa†
pˆap.
(3.34)
Now that we have a working theory where the commutation properties
of the operators contain all of the information about the states, let’s
check that it works and gets the correct rules for inner products and for
position space wave functions.
Example 3.6
For single-particle states, we have
⟨p|p′⟩= ⟨0|ˆapˆa†
p′|0⟩.
(3.35)
We use the commutation relations to get
⟨p|p′⟩
=
⟨0|
h
δ(3)(p −p′) ± ˆa†
p′ˆap
i
|0⟩
(3.36)
=
δ(3)(p −p′),
which is the right answer. We can check this by working out a position space wave
function. We’ll start by making a change of basis8 |x⟩=
R
d3q φ∗
q(x)|q⟩, which in-
8The state |x⟩can be written in a new
basis by inserting the resolution of the
identity in the form
1 =
Z
d3q |q⟩⟨q|,
so that
|x⟩=
Z
d3q |q⟩⟨q|x⟩,
and writing ⟨x|q⟩= φq(x) [and hence
⟨q|x⟩= φ∗
q(x)] yields
|x⟩=
Z
d3q φ∗
q(x)|q⟩.
volves expanding a position state in terms of momentum states. Using this expansion,
we obtain
⟨x|p⟩=
Z
d3q φq(x)⟨q|p⟩= φp(x).
(3.37)
This is clearly okay, but rather trivial. More interesting is the case of a two-particle
state
⟨p′q′|qp⟩= ⟨0|ˆap′ˆaq′ˆa†
qˆa†
p|0⟩.
(3.38)
Commuting, we obtain
⟨p′q′|qp⟩= δ(3)(p′ −p)δ(3)(q′ −q) ± δ(3)(p′ −q)δ(3)(q′ −p),
(3.39)
where we take the plus sign for bosons and minus sign for fermions. Again we can
work out the position space wave function by making a change of basis |xy⟩=
1
√
2!
R
d3p′d3q′ φ∗
p′(x)φ∗
q′(y)|p′q′⟩(where the factor of
1
√
2! is needed to prevent the
double counting that results from the unrestricted sums). This gives us
1
√
2!
Z
d3p′d3q′φp′(x)φq′(y)⟨p′q′|pq⟩=
1
√
2
[φp(x)φq(y) ± φq(x)φp(y)] , (3.40)
which is the well-known expression for two-particle states.
It works. We have successfully reformulated quantum mechanics. The
eﬀort we’ve spent here writing everything in terms of creation and anni-
hilation operators will soon pay oﬀ. The machinery they represent is so
useful that we’ll try to build the quantum ﬁelds we’re searching for out
of these objects. As you might suspect, this search will be successful.

36
Occupation number representation
Chapter summary
• The occupation number representation describes states by listing
the number of identical particles in each quantum state.
• We focus on the vacuum state |0⟩and then construct many-particle
states by acting on |0⟩with creation operators.
• To obey the symmetries of many-particle mechanics, bosons are
described by commuting operators and fermions are described by
anticommuting operators.
Exercises
(3.1) For boson operators satisfying
h
ˆap, ˆa†
q
i
= δpq,
(3.41)
show that
1
V
X
pq
ei(p·x−q·y) h
ˆap, ˆa†
q
i
= δ(3)(x −y),
(3.42)
where V is the volume of space over which the sys-
tem is deﬁned. Repeat this for fermion commuta-
tion operators.
(3.2) Show that for the simple harmonic oscillator:
(a) [ˆa, (ˆa†)n] = n(ˆa†)n−1,
(b) ⟨0|ˆan(ˆa†)m|0⟩= n!δnm,
(c) ⟨m|ˆa†|n⟩= √n + 1δm,n+1,
(d) ⟨m|ˆa|n⟩= √nδm,n−1.
(3.3) The three-dimensional harmonic oscillator is de-
scribed by the Hamiltonian
ˆH =
1
2m(ˆp2
1+ˆp2
2+ˆp2
3)+ 1
2mω2(ˆx2
1+ˆx2
2+ˆx2
3). (3.43)
Deﬁne creation operators ˆa†
1, ˆa†
2 and ˆa†
3 so that
[ˆai, ˆa†
j] = δij and show that ˆH = ℏω P3
i=1( 1
2 +ˆa†
i ˆai).
Angular momentum can be deﬁned as
ˆLi = −iℏǫijkˆa†
jˆak,
(3.44)
so that for example ˆL3 = −iℏ(ˆa†
1ˆa2 −ˆa†
2ˆa1). Deﬁne
new creation operators
ˆb†
1
=
−1
√
2
(ˆa†
1 + iˆa†
2),
ˆb†
0
=
ˆa†
3,
ˆb†
−1
=
1
√
2
(ˆa†
1 −iˆa†
2),
(3.45)
and show that [ˆbi,ˆb†
j] = δij, the Hamiltonian is ˆH =
ℏω P1
m=−1( 1
2 +ˆb†
mˆbm) and ˆL3 = ℏP1
m=−1 mˆb†
mˆbm.
(3.4) Since for fermions we have that ˆc†
1ˆc†
2|0⟩= −ˆc†
2ˆc†
1|0⟩,
then the state |11⟩depends on which fermion is
added ﬁrst.
If we put the ﬁrst fermion into the
state ψi(r1) and the second fermion into the state
ψj(r2), a representation for |11⟩is
Ψ(r1, r2) =
1
√
2
[ψ1(r1)ψ2(r2) −ψ2(r1)ψ1(r2)] .
(3.46)
Show that a generalization of this result for N
fermions is Ψ(r1 · · · rN), which can be written out
as
1
√
N!
˛˛˛˛˛˛˛˛˛
ψ1(r1)
ψ2(r1)
· · ·
ψN(r1)
ψ1(r2)
ψ2(r2)
· · ·
ψN(r2)
...
...
...
ψ1(rN)
ψ2(rN)
· · ·
ψN(rN)
˛˛˛˛˛˛˛˛˛
.
(3.47)
This expression is known as a Slater determi-
nant.

4
Making second
quantization work
4.1 Field operators
37
4.2 How to second quantize an op-
erator
39
4.3 The kinetic energy and the
tight-binding Hamiltonian 43
4.4 Two particles
44
4.5 The Hubbard model
46
Chapter summary
48
Exercises
48
In this chapter we will bridge the gap between ﬁrst and second quan-
tization. This will result in new operators that act on states given in
the occupation number representation. For this discussion we’ll limit
ourselves to non-relativistic particles in a box.1 By working in the non-
1We
retain
the
‘particle-in-a-box’
normalization of Chapter 3. States are
deﬁned in a box of volume V (using
periodic boundary conditions).
For
later convenience, we list some useful
results using this normalization:
A state |α⟩can be described in posi-
tion coordinates ψα(x) = ⟨x|α⟩or mo-
mentum coordinates ˜ψα(p) = ⟨p|α⟩,
and since
⟨p|α⟩=
Z
d3x ⟨p|x⟩⟨x|α⟩,
(4.1)
then the Fourier transform
˜ψα(p) =
1
√
V
Z
d3x e−ip·xψα(x),
(4.2)
implies that
⟨p|x⟩=
1
√
V
e−ip·x.
(4.3)
The inverse transform is
ψα(x) =
1
√
V
X
p
eip·x ˜ψα(p),
(4.4)
since p takes discrete values. We also
have Z
d3x e−ip·x = Vδp,0
(4.5)
and
1
V
X
p
eip·x = δ(3)(x).
(4.6)
relativistic limit, we are taking a shortcut and thereby avoiding some
important complications.
A fuller, relativistic treatment will be pre-
sented in Chapter 11, but the approach taken in this chapter gives a
route to some immediately useful results and in particular to the be-
haviour of electrons in a solid.
4.1
Field operators
In the discussion so far we have introduced creation and annihilation
operators ˆa†
p and ˆap which create and annihilate particles into particu-
lar momentum states. So if we have the vacuum state |0⟩and we apply
ˆa†
p to it, then there is a big puﬀof smoke and a ﬂash of light and the
resulting state ˆa†
p|0⟩contains a single particle sitting in the momentum
state p. Being a momentum eigenstate it is of course extended in space
(but completely localized in momentum space). Now, by making appro-
priate linear combinations of operators, speciﬁcally using Fourier sums,
we can construct operators, called ﬁeld operators, that create and an-
nihilate particles, but this time they don’t create/annihilate particles in
particular momentum states but instead they create/annihilate parti-
cles localized at particular spatial locations. Thus the operator ˆψ†(x)
deﬁned by
ˆψ†(x) =
1
√
V
X
p
ˆa†
pe−ip·x,
(4.7)
creates a particle at position x, while ˆa†
p creates a particle in a state
with three-momentum p. Similarly, the operator ˆψ(x) deﬁned by
ˆψ(x) =
1
√
V
X
p
ˆapeip·x,
(4.8)
annihilates a particle at position x, while ˆap annihilates a particle in a
state with three-momentum p. In summary:

38
Making second quantization work
The ﬁeld operators ˆψ†(x) and ˆψ(x) respectively create or destroy a
single particle at a point x.
The following examples explore some properties of ﬁeld operators.
Example 4.1
We examine the state |Ψ⟩= ˆψ†(x)|0⟩and check that it does indeed correspond to a
single particle at a particular location. Note that
|Ψ⟩= ˆψ†(x)|0⟩=
1
√
V
X
p
e−ip·xˆa†
p|0⟩.
(4.9)
To calculate the total number of particles in this state we can use the number operator
ˆnp = ˆa†
pˆap (which measures the number of particles in state p) and then sum over
all momentum states. Consider
X
q
ˆnq|Ψ⟩=
1
√
V
X
qp
ˆa†
qˆaqˆa†
p|0⟩e−ip·x,
(4.10)
and again using ⟨0|ˆaqˆa†
p|0⟩= δpq we deduce that
X
q
ˆnq|Ψ⟩= |Ψ⟩,
(4.11)
and the state Ψ is then an eigenstate of the number operator with eigenvalue 1.
To ﬁnd out exactly where the particle is created, ﬁrst deﬁne |p⟩= ˆa†
p|0⟩and then
we can look at the projection of |Ψ⟩on another position eigenstate |y⟩:
We use the result
⟨y|p⟩=
1
√
V
eip·y,
which follows from eqn 4.3 that states
that the projection of a momentum
eigenstate in position space is a plane
wave. The last equality in eqn 4.13 fol-
lows directly from eqn 4.6.
⟨y|Ψ⟩= ⟨y|ψ†(x)|0⟩
=
1
√
V
X
p
e−ip·x⟨y|p⟩
=
1
V
X
p
e−ip·(x−y)
(4.12)
=
δ(3)(x −y),
showing that the single particle created by ˆψ†(x) may only be found at y = x.
Example 4.2
The ﬁeld operators satisfy commutation relations. For example
h
ˆψ(x), ˆψ†(y)
i
= 1
V
X
pq
ei(p·x−q·y)[ˆap, ˆa†
q],
(4.13)
and using [ˆap, ˆa†
q] = δpq we have that
h
ˆψ(x), ˆψ†(y)
i
= 1
V
X
pq
ei(p·x−q·y)δpq = 1
V
X
p
eip·(x−y) = δ(3)(x −y),
(4.14)
where the last equality follows from eqn 4.6. By the same method you can show
h
ˆψ†(x), ˆψ†(y)
i
=
h
ˆψ(x), ˆψ(y)
i
= 0.
(4.15)
These results are for boson operators, but we can derive analogous results for fermion
operators:
n
ˆψ(x), ˆψ†(y)
o
=
δ(3)(x −y),
n
ˆψ†(x), ˆψ†(y)
o
=
n
ˆψ(x), ˆψ(y)
o
= 0.
(4.16)

4.2
How to second quantize an operator
39
4.2
How to second quantize an operator
How can we upgrade the operators that we had in ﬁrst-quantized quan-
tum mechanics into second-quantized operators?
Let us ﬁrst remind
ourselves what an operator ˆ
A does. It maps a quantum state |ψ⟩(which
lives in a Hilbert space2 and describes the quantum mechanical state
2A Hilbert space is a complex vector
space endowed with an inner product,
whose elements look like P
n an|φn⟩
where an are complex numbers. It can
thus describe particular superpositions
of states such as, just for an example,
(|φ0⟩+ i|φ1⟩)/
√
2 in which the system
might equally be found in state |φ0⟩or
|φ1⟩, but the quantum amplitudes for
being in each state are π/2 out of phase.
This is a superposition of possibilities,
but it only describes a single particle.
of a single particle) into another state ˆ
A|ψ⟩. We will focus in this section
on ˆ
A being a single-particle operator, an example of which would be
ˆp1 = −iℏ∇= −iℏ
∂
∂x1 , which operates on a single coordinate x1 that
can only describe the position of one particle. Thus we will ignore for
now objects like the Coulomb potential operator ˆV (x1, x2) =
q
4π|x1−x2|
which depends on the coordinates of two particles. An operator that
acts on n particles is called an n-particle operator.
The operator ˆ
A can be described quite generally using a variety of
coordinate systems, as can be seen if you insert two resolutions of the
identity, 1 = P
α |α⟩⟨α| and 1 = P
β |β⟩⟨β| into the right-hand side of
the trivial identity ˆ
A = ˆ
A:
ˆ
A =
X
αβ
|α⟩⟨α| ˆ
A|β⟩⟨β| =
X
αβ
Aαβ|α⟩⟨β|,
(4.17)
where the matrix element Aαβ = ⟨α| ˆ
A|β⟩. This is all well and good, but
quantum ﬁeld theory is acted out on a larger stage than Hilbert space.
We need a space that includes not only single-particle states, but the
possibility of many-particle states. This is known as Fock space.3
3Fock
space
F
includes
N-particle
states for all possible values of N.
Mathematically, one can write this as
F =
∞
M
N=0
FN = F0 ⊕F1 ⊕· · · ,
where F0
= {|0⟩} is a set contain-
ing only the vacuum state and F1 is
the single-particle Hilbert space.
The
subspaces FN≥2, which describe N-
particle states with N ≥2, have to
be symmetrized for bosons and anti-
symmetrized for fermions (so, for ex-
ample, F2 is not simply F1 ⊗F1, but
only the symmetric or antisymmetric
part of F1 ⊗F1).
Creation operators
move an element of FN into an ele-
ment of FN+1, while annihilation op-
erators perform the reverse manoeuvre.
An element of F can describe a super-
position of states with diﬀerent particle
numbers.
Remarkably, the second-quantized many-body upgrade
ˆA of the
single-particle operator ˆ
A to Fock space is a very compact expression:
ˆA =
X
αβ
Aαβ ˆa†
αˆaβ.
(4.18)
This equation has the same single-particle matrix element Aαβ
=
⟨α| ˆ
A|β⟩that we had in eqn 4.17, even though it is now valid for the
many-particle case. The interpretation of eqn 4.18 is beautifully simple.
The operator ˆA is a sum over all processes in which you use ˆaβ to remove
a single particle in state |β⟩, multiply it by the matrix element Aαβ, and
then use ˆa†
α to place that particle into a ﬁnal state |α⟩. This operator ˆA
thus describes all possible single-particle processes that can occur and
operates on a many-particle state (see Fig. 4.1). The proof of eqn 4.18
is rather technical and is covered in the following example, which can
be skipped if you are happy to take the result on trust.
ˆaβ
ˆa†α
Aαβ
Fig. 4.1 A process represented by the
operator in eqn 4.18.
Example 4.3
To prove eqn 4.18, we need to introduce some formalism. Let us ﬁrst write down an
expression for an N-particle state
|ψ1, . . . , ψN⟩=
1
√
N!
X
P
ξP
N
Y
i=1
|ψP (i)⟩.
(4.19)

40
Making second quantization work
This is a sum over all N! permutations of single-particle states |ψi⟩. There are N!
permutations of the sequence of integers i = 1, . . . , N and P(i) labels a permutation
of this sequence. The factor ξ = 1 for bosons and ξ = −1 for fermions. For fermions,
the notation ξP gives a plus or minus sign for an even or odd permutation respectively
(and for bosons, of course, the factor can be ignored as it is always +1). The inner
product of two such N-particles states is
⟨χ1, . . . , χN|ψ1, . . . , ψN⟩=
1
N!
X
P
X
Q
ξP +Q
N
Y
i=1
⟨χQ(i)|ψP (i)⟩.
(4.20)
We can rewrite this using P ′ = P + Q which spans all the permutations N! times,
see Fig. 4.2, and hence
⟨χ1, . . . , χN|ψ1, . . . , ψN⟩=
X
P ′
ξP ′
N
Y
i=1
⟨χi|ψP ′(i)⟩.
(4.21)
If desired, this rather complicated expression can be written rather compactly as a
P
Q
Fig. 4.2 For N = 3, a sum over per-
mutations P and Q is equivalent to
N! copies of a sum over permutations
P ′ = P + Q.
determinant (for fermions, or a ‘permanent’ for bosons).4
4The right-hand side of eqn 4.21 can
be written det(⟨χi|ψj⟩) for fermions or
per(⟨χi|ψj⟩) for bosons. The determi-
nant of a matrix Aij is
det(A) =
X
P
(−1)P
N
Y
i=1
AiP (i).
The permanent of a matrix Aij is
per(A) =
X
P
N
Y
i=1
AiP (i).
Let’s now try using a creation operator ˆa†
φ to act on one of these N-particle states:
ˆa†
φ|ψ1, . . . , ψN⟩= |φ, ψ1, . . . , ψN⟩.
(4.22)
Thus the cuckoo lays its |φ⟩egg in the |ψ1, . . . , ψN⟩nest, see Fig. 4.3. Annihilation
is somewhat harder to formulate, mainly because you have to cater for the possibility
that the state you are annihilating is some admixture of states already present. To
proceed, it is helpful to realize that ˆaφ is the Hermitian conjugate of ˆa†
φ. Thus
ˆa†
φ
φ
ψ1
ψ2
ψN
Fig. 4.3 The action of a creation oper-
ator ˆa†
φ on a state |ψ1, . . . , ψN⟩.
⟨χ1, . . . , χN−1|ˆaφ|ψ1, . . . , ψN⟩
=
⟨ψ1, . . . , ψN|ˆa†
φ|χ1, . . . , χN−1⟩∗
(4.23)
=
⟨ψ1, . . . , ψN|φ, χ1, . . . , χN−1⟩∗
=
˛˛˛˛˛˛˛
⟨ψ1|φ⟩
⟨ψ1|χ1⟩
· · ·
⟨ψ1|χN−1⟩
...
...
...
⟨ψN|φ⟩
⟨ψN|χ1⟩
· · ·
⟨ψN|χN−1⟩
˛˛˛˛˛˛˛
∗
ξ
.
This last line has been obtained by recognizing that the expression to evaluate is a
determinant (fermions) or permanent (bosons). It can be expanded as a series of
minors:
⟨χ1, . . . , χN−1|ˆaφ|ψ1, . . . , ψN⟩
=
N
X
k=1
ξk−1⟨ψk|φ⟩∗⟨ψ1, . . . (no ψk) . . . ψN|χ1, . . . , χN−1⟩∗
=
N
X
k=1
ξk−1⟨φ|ψk⟩⟨χ1, . . . , χN−1|ψ1, . . . (no ψk) . . . ψN⟩.
(4.24)
This is true for any premultiplying ⟨χ1, . . . , χN−1|, and so
ˆaφ|ψ1, . . . , ψN⟩=
N
X
k=1
ξk−1⟨φ|ψk⟩|ψ1, . . . (no ψk) . . . ψN⟩.
(4.25)
This means that a combination of annihilation of |β⟩and creation of |α⟩gives
ˆa†
αˆaβ|ψ1, . . . , ψN⟩=
N
X
k=1
ξk−1⟨β|ψk⟩|α, ψ1, . . . (no ψk) . . . ψN⟩,
(4.26)
but now we can simply translate the inserted state |α⟩into the position formerly held
by the annihilated state |ψk⟩and thereby remove the annoying ξk−1 factor, since
ξk−1|α, ψ1, . . . (no ψk) . . . ψN⟩= |ψ1, . . . ψk−1, α, ψk+1, . . . ψN⟩
(4.27)
and hence
ˆa†
αˆaβ|ψ1, . . . , ψN⟩=
N
X
k=1
⟨β|ψk⟩|ψ1, . . . ψk−1, α, ψk+1, . . . ψN⟩.
(4.28)

4.2
How to second quantize an operator
41
After all this eﬀort, we are now ready to tackle the real problem.
Given a state
|ψ1, . . . , ψN⟩and a single-particle operator ˆ
A = P
αβ Aαβ|α⟩⟨β|, the obvious second-
quantized upgrade is to let
ˆ
A act separately on each individual component state
inside |ψ1, . . . , ψN⟩and then sum the result. For example, if ˆ
A is the single-particle
momentum operator, we would want ˆ
A to be the operator for the total momentum,
the sum of all the momenta of individual particles within the many-body state.
Let’s make it very simple to start with and set
ˆ
A = |α⟩⟨β|. In this case, for a
single-particle state |ψj⟩we would have ˆ
A|ψj⟩= ⟨β|ψj⟩|α⟩and hence
ˆ
A|ψ1 . . . ψN⟩=
N
X
j=1
⟨β|ψj⟩|ψ1, . . . ψk−1, α, ψk+1, . . . ψN⟩.
(4.29)
But this is simply ˆa†
αˆaβ|ψ1, . . . , ψN⟩as we found in eqn 4.28. Hence ˆ
A = ˆa†
αˆaβ. In
the general case of ˆ
A = P
αβ Aαβ|α⟩⟨β|, we easily then arrive at
ˆ
A =
X
αβ
Aαβ ˆa†
αˆaβ.
(4.30)
After the hard work of the previous example, which justiﬁed eqn 4.18,
we now have the power at our ﬁngertips to do some splendidly elegant
things.
Example 4.4
(i) The resolution of the identity ˆ1
=
P
α |α⟩⟨α| is a special case of
ˆ
A
=
P
αβ Aαβ|α⟩⟨β| with Aαβ = δαβ. Its second-quantized upgrade is immediately
ˆn =
X
α
ˆa†
αˆaα,
(4.31)
the number operator, that counts one for every particle in the state on which it
operates.
(ii) The usual momentum operator can be written
ˆ
A = ˆp = P
p p|p⟩⟨p| and this
upgrades immediately to
ˆp =
X
p
pˆa†
pˆap =
X
p
p ˆnp.
(4.32)
In much the same way, a function of the momentum operator ˆ
A = f(p) becomes
ˆ
A =
X
p
f(p) ˆa†
pˆap =
X
p
f(p) ˆnp.
(4.33)
One particular example of this is the free-particle Hamiltonian which one can write
down immediately in second-quantized form as
ˆap
ˆa†p
p2
2m
Fig. 4.4 A process represented by the
Hamiltonian
ˆH = P
p
p2
2m ˆnp.
Sum-
ming over all momentum states involves
repeating this counting process which
can be imagined as counting sheep
crossing through a gate (annihilate a
sheep on the left-hand side of the gate,
count its energy in the sum, create the
sheep again on the right-hand side of
the gate, now move on to the next sheep
. . . ).
ˆH =
X
p
p2
2m ˆnp.
(4.34)
Equation 4.34 can be thought about as the simple diagram shown in Fig. 4.4. Since
the occupation number states (with which we want to work) are eigenstates of ˆnp then
so is ˆH. We have found that the operator ˆH is diagonal in the basis in which we’re
working. In fact, to diagonalize any Hamiltonian (which means to ﬁnd the energies
of the eigenstates) we simply express it in terms of number operators. Having the
Hamiltonian in this form makes perfect sense. We’re saying that the total energy in
the system is given by the sum of the energies p2/2m of all of the particles.
(iii) What happens if our operator is a function of ˆx, rather than ˆp? Well, eqn 4.18 is
valid even if our states |α⟩and |β⟩are position states. Our creation and annihilation
operators are simply ﬁeld operators, and the sum over momentum values becomes
an integral over space. We then can write down our second-quantized operator ˆV as
ˆV
=
Z
d3x ˆψ†(x)V (x) ˆψ(x).
(4.35)

42
Making second quantization work
If we want, we can express this back in momentum space using eqns 4.7 and 4.8, and
hence
ˆV = 1
V
Z
d3x
X
p1p2
ˆa†
p1e−ip1·xV (ˆx) ˆap2eip2·x =
X
p1p2
˜Vp1−p2ˆa†
p1ˆap2,
(4.36)
where ˜Vp =
1
V
R
d3x V (x)e−ip·x is the Fourier transform of V (x).
The operator
ˆV isn’t diagonal, since the ˆa operators create and annihilate particles with diﬀerent
momenta. We can think of this term as representing a process of an incoming particle
with momentum p2 which interacts with a potential ﬁeld (represented by ˜Vp1−p2)
and then leaves again with momentum p1 (see Fig. 4.5).
ˆap2
ˆa†p1
˜Vp1−p2
Fig.
4.5 A process represented by
eqn 4.36. The sequence of events is: an-
nihilate a particle with momentum p2,
count the potential energy ˜Vp1−p2, cre-
ate a particle with momentum p1.
Example 4.5
Let’s have a look at the inﬂuence of the potential on a simple system described by a
Hamiltonian
ˆH = E0
X
p
ˆd†
p ˆdp −V
2
X
p1p2
ˆd†
p1 ˆdp2.
(4.37)
We’ll further constrain the system to have three energy levels, so states are expressed
using a basis |np1np2np3⟩. For simplicity, let’s ﬁrst set V = 0. In this case, if we
put a particle into the system then the three possible eigenstates of the system are
|100⟩, |010⟩, |001⟩. Now we put V ̸= 0, and so
V
2
X
p1p2
ˆd†
p1 ˆdp2|100⟩= V
2 (|100⟩+ |010⟩+ |001⟩) ,
(4.38)
and similarly for the other states (try them).
At this point it’s easiest to slip into matrix notation, for which the entire Hamil-
tonian takes the form
H =
2
4E0
0
@
1
0
0
0
1
0
0
0
1
1
A −V
2
0
@
1
1
1
1
1
1
1
1
1
1
A
3
5 .
(4.39)
The eigenvalues for this equation are ε = E0, E0, E0 −3V
2 . The ground state of the
system (called |Ω⟩) therefore has an energy E0 −3V
2 . The corresponding eigenstate
is
|Ω⟩=
1
√
3
(|100⟩+ |010⟩+ |001⟩) .
(4.40)
We conclude that the ground state is a special 1:1:1 superposition of the three states
we started with.
Example 4.6
What is the equivalent of the probability density of the wave function in second-
quantized language? The answer is that the number density of particles at a point
x is described by the number density operator ˆρ(x) given by
ˆρ(x)
=
ˆψ†(x) ˆψ(x)
=
1
V
X
p1p2
h
e−i(p1−p2)·xi
ˆa†
p1ˆap2.
(4.41)
This operator enables us to write the potential energy operator for a single particle
in an external potential as
ˆV =
Z
d3x V (x)ˆρ(x).
(4.42)

4.3
The kinetic energy and the tight-binding Hamiltonian
43
4.3
The kinetic energy and the
tight-binding Hamiltonian
One application of this formalism is to the tight-binding Hamiltonian
which is used in condensed matter physics. The basic idea behind this
model is very straightforward: electrons don’t like being conﬁned. We
know this from the well-known problem of a one-dimensional inﬁnite
square well with width L. There the kinetic part of the energy is given
by En =
1
2m
  nπ
L
2.
The smaller we make L, the larger the energy.
Conversely, kinetic energy can be saved by allowing the electron to move
in a larger volume. In the tight-binding model, one considers a lattice of
ﬁxed atoms with electrons moving between them (as shown in Fig. 4.6).
These electrons can lower their kinetic energies by hopping from lattice
site to lattice site. To deal with the discrete lattice in the model, we
need to work in a basis where the fermion creation operator ˆc†
i creates
a particle at a particular lattice site labelled by i. The kinetic energy
saving for a particle to hop between points j and i is called tij. Clearly
tij will have some fundamental dependence on the overlap of atomic
wave functions. The Hamiltonian ˆH contains a sum over all processes
in which an electron hops between sites, and so is a sum over pairs of
sites:
ˆH =
X
ij
(−tij)ˆc†
i ˆcj.
(4.43)
Each term in this sum consists of processes in which we annihilate a
particle at site j and create it again at site i (thus modelling a hopping
process), thus saving energy tij. For now, we’ll consider the simplest
possible case of tij = t for nearest neighbours and t = 0 otherwise. Thus
we rewrite ˆH as
ˆH = −t
X
iτ
ˆc†
i ˆci+τ,
(4.44)
where the sum over τ counts over nearest neighbours.
j
i
Fig. 4.6 The operator ˆc†
i ˆcj causing an
electron to hop from site j to i. Such a
process saves kinetic energy tij.
Example 4.7
The tight-binding Hamiltonian is not diagonal since we have a bilinear combination
of operators that look like ˆc†
i ˆcj. We need to diagonalize it and that can be achieved
by making a change of basis (again, essentially using a Fourier transform). Thus
ˆci =
1
√
V
X
k
eik·riˆck
and
ˆc†
i =
1
√
V
X
q
e−iq·riˆc†
q.
(4.45)
Substituting this into the Hamiltonian yields
ˆH = −t
V
X
iτ
X
kq
e−i(q−k)·ri−iq·rτ ˆc†
qˆck.
(4.46)
We play our usual trick and exploit the fact that
1
V
P
i e−i(q−k)·ri = δqk, which
removes the sum over i. Lastly we do the sum over q. After jumping through these
familiar mathematical hoops we end up with
ˆH = −t
X
k
X
τ
eik·rτ ˆc†
kˆck,
(4.47)
which is diagonal in momentum.

44
Making second quantization work
We can tidy up this result by writing the Hamiltonian as
ˆH =
X
k
Ekˆc†
kˆck,
(4.48)
where the dispersion Ek is given by
Ek = −
X
τ
t eik·rτ .
(4.49)
Example 4.8
For a two-dimensional square lattice, the sum over τ counts over vectors (a, 0),
(−a, 0), (0, a) and (0, −a). This leads to a dispersion relation (see also Fig. 4.7)
Ek = −2t(cos kxa + cos kya).
(4.50)
-π/a
0
π/a
ky
-π/a
0
π/a
kx
Fig. 4.7 The tight-binding model for
a two-dimensional square lattice. Con-
tours showing lines of constant energy
Ek based on eqn 4.50 are plotted as a
function of kx and ky. The lowest Ek
occurs at (kx, ky) = (0, 0), the centre of
this plot, for t > 0 and at the corners,
(kx, ky) = (± π
a , ± π
a ) for t < 0.
4.4
Two particles
We have been focussing on single-particle operators. What happens for
the more interesting case of two particles interacting with each other?
From the discussion above we can guess (quite correctly) that a second-
quantized two-particle operator ˆA is given by
ˆA =
X
αβγδ
Aαβγδ ˆa†
αˆa†
βˆaγˆaδ,
(4.51)
where Aαβγδ = ⟨α, β|γ, δ⟩is the matrix element for the Hilbert-space
version. This expression embodies the idea that two-particle operators
involve a sum over all processes involving the annihilation of two parti-
cles in particular states, multiplication by the matrix element, and cre-
ation of two particles in two new states. We will frequently encounter
two-particle operators ˆV written as a function of spatial coordinates,
and the corresponding expression involving ﬁeld operators is
ˆV = 1
2
Z
d3x d3y ˆψ†(x) ˆψ†(y)V (x, y) ˆψ(y) ˆψ(x).
(4.52)
The factor of
1
2 ensures that we don’t double count the interactions.
Notice the way we have arranged the operators: we put the creation
operators to the left and the annihilation operators to the right, a con-
vention which is known as normal ordering. Why is this important?
Normal ordering makes the evaluation of matrix elements much easier
and in particular it makes sure that our operator, ˆV , has zero vacuum
expectation value:
⟨0| ˆV |0⟩= 0.
(4.53)

4.4
Two particles
45
This avoids us to having to blush over embarrassing inﬁnite self-energy
terms that can otherwise creep into our answers. With creation oper-
ators to the left and annihilation operators to the right we make sure
that ⟨0| ˆV |0⟩is ﬁxed at zero.
An additional convention we have adopted in eqn 4.52 is that read-
ing the coordinates from right-to-left gives us x, y, y, x. (This can be
remembered as the dance step: left-in, right-in, right-out, left-out.) For
bosons the order would not matter, but fermions anticommute and so for
example interchanging the ﬁrst two creation operators would introduce
a minus sign.
It is useful to see what would happen if we hadn’t introduced normal
ordering. For example, an alternative guess for the form of the two-
particle interaction might have been
ˆVwrong = 1
2
Z
d3x d3y V (x, y)ˆρ(y)ˆρ(x).
(4.54)
This form is wrong and, because we’ve left out the all-important normal
ordering, will lead to an extra self-energy term, as shown in the following
example.
Example 4.9
We will consider the case of fermions and will need the anticommutation relations
n
ˆψ(x), ˆψ†(y)
o
= δ(3)(x −y) and
n
ˆψ(x), ˆψ(y)
o
= 0. Now consider the operator
ˆρ(x)ˆρ(y), which is given by
ˆρ(x)ˆρ(y) = ˆψ†(x) ˆψ(x) ˆψ†(y) ˆψ(y).
(4.55)
Notice that this is certainly not normal ordered, but we can make it so as follows.
Swap the second and third operators. This involves using an anticommutator. We
obtain
ˆρ(x)ˆρ(y)
=
−ˆψ†(x) ˆψ†(y) ˆψ(x) ˆψ(y) + δ(3)(x −y) ˆψ†(x) ˆψ(y)
=
ˆψ†(x) ˆψ†(y) ˆψ(y) ˆψ(x) + δ(3)(x −y) ˆψ†(x) ˆψ(y),
(4.56)
where, in the last line, we’ve made another swap (this time of the two left-most
operators, involving a second sign change). We are left with a normal ordered term
plus a term involving a delta function. Comparing this to the correct expression for
the two-particle interaction, we have
ˆVwrong = ˆV + 1
2
Z
d3x V (x, x) ˆψ†(x) ˆψ(x).
(4.57)
The ‘wrong’ form of the potential energy includes an unwanted self-energy term.
Normal ordering ensures we dispose of this abomination.
As before, we continue the procedure of second quantization by substi-
tuting the mode expansion into our equation
ˆV = 1
2
Z
d3x d3y ˆψ†(x) ˆψ†(y)V (x, y) ˆψ(y) ˆψ(x)
=
1
2V2
Z
d3x d3y ei(−p1·x−p2·y+p3·y+p4·x) X
p1p2p3p4
ˆa†
p1ˆa†
p2V (x −y)ˆap3ˆap4.
(4.58)

46
Making second quantization work
Example 4.10
To deal with this, we deﬁne z = x −y and eliminate x to obtain
ˆV =
1
2V2
X
p1p2p3p4
ˆa†
p1ˆa†
p2ˆap3ˆap4
Z
d3z V (z)ei(p4−p1)·z
Z
d3y ei(−p1−p2+p3+p4)·y.
(4.59)
The last integral gives us a Kronecker delta Vδp1+p2−p3,p4, which can be used to
eat up one of the momentum sums (the p4 one here) and sets p4 = p1 + p2 −p3.
Now we have
1
2V
X
p1p2p3
ˆa†
p1ˆa†
p2ˆap3ˆap1+p2−p3
Z
d3z V (z)e−i(p3−p2)·z.
(4.60)
The last integral is the Fourier transform of the potential
1
V
Z
d3z V (z)e−i(p3−p2)·z = ˜Vp3−p2.
(4.61)
Now for some interpretation: set q = p3 −p2 and eliminate p3, yielding
ˆV = 1
2
X
p1p2q
˜Vqˆa†
p1ˆa†
p2ˆap2+qˆap1−q.
(4.62)
Lastly we reindex the sum subtracting q from p2 and adding it to p1.
The result of these index acrobatics is a two-particle interaction potential
that looks like
ˆV = 1
2
X
p1p2q
˜Vqˆa†
p1+qˆa†
p2−qˆap2ˆap1.
(4.63)
This can be interpreted as a scattering problem in momentum space.
This scattering is illustrated in a cartoon known as a Feynman diagram.
The diagram in Fig. 4.8 is translated into a story as follows: A particle
comes in with momentum p2.
It sends out a force-carrying particle
with momentum q, reducing its ﬁnal momentum to p2 −q. The force-
carrying particle is absorbed by a second particle (which originally has
momentum p1) and ends up with a ﬁnal momentum of p1 + q. Notice
we conserve momentum at the vertices of the diagrams.
p1
p2
q
p1 + q
p2 −q
Fig. 4.8 A Feynman diagram for the
process described in the text.
Inter-
preting this diagram, one can think of
time increasing in the vertical direction.
Our aim is going to be to turn all of our combinations of operators into
number operators. This makes evaluating the energy a simple matter.
The ﬁrst thing to notice about eqn 4.63 is that it is not diagonal. In
general, problems involving the potential energy (as in eqn 4.63) cannot
be solved exactly.
Despite this, the two-particle potential leads to a
remarkably rich seam of physics.5
5As we will see, not only does this
tell us about seemingly pedestrian mat-
ters concerning the energies of elec-
trons in metals, but it also describes
interactions that lead to superﬂuidity,
superconductivity, magnetism, charge
density waves and pretty much every
other phenomenon you’ve ever encoun-
tered in condensed matter physics. Of
course, the fact that eqn 4.63 cannot
generally be diagonalized should give
us pause.
We need a programme to
turn the two-body potential diagonal.
As we will see in Part X, we do this
in two stages: ﬁrstly we massage our
Hamiltonian until it’s made up of bi-
linear combinations of operators.
We
may then make transformations until
we have number operators.
4.5
The Hubbard model
As this chapter’s ﬁnal example of a second-quantized Hamiltonian, we
turn to the Hubbard model.6 It is diﬃcult to overemphasize the im-
6The Hubbard model is named after
John Hubbard (1931–1980).
portance of this model in solid state physics, due to the fact that it
captures the essential physics of the competition between kinetic energy
(favouring electrons to become delocalized) and potential energy (favour-
ing electrons to become localized). Though relatively easy to state, the

4.5
The Hubbard model
47
Hubbard model is surprisingly complicated to solve and exact results are
only available in certain scenarios. The Hubbard model is based on a
tight-binding Hamiltonian (to model the kinetic energy) plus a two-body
potential energy term (to model electron–electron interactions):
ˆH =
X
ij
(−tij)ˆc†
i ˆcj + 1
2
X
ijkl
ˆc†
i ˆc†
jVijklˆckˆcl.
(4.64)
Recognizing that electrons carry a spin σ which can point up (| ↑⟩) or
down (| ↓⟩) this becomes
ˆH =
X
ijσ
(−tij)ˆc†
iσˆcjσ + 1
2
X
ijklσσ′
ˆc†
iσˆc†
jσ′Vijklˆckσ′ˆclσ.
(4.65)
We assume in this model that no spin-ﬂip processes can occur; thus
spins never ﬂip from up-to-down or vice versa on hopping. However, the
interaction between electrons is based upon the Coulomb interaction
between their charges and so electrons with opposite spin will interact
with each other just as much as electrons with the same spins. Now for
Hubbard’s model: the Coulomb interaction is assumed to be signiﬁcant
only between two electrons that are on the same site. These electrons
interact via a constant potential energy U = Viiii. However, the Pauli
exclusion principle ensures that should two spins hop into the same
site they must have diﬀerent spins, and this is where spin does make a
diﬀerence. The Hamiltonian for the Hubbard model is then
ˆH =
X
ijσ
(−tij)ˆc†
iσˆcjσ + U
X
i
ˆni↑ˆni↓,
(4.66)
which looks simple, but the eigenstates tend to be complex and highly
correlated. It is often only necessary to consider hopping elements for
nearest neighbours.
Example 4.11
Consider the Hubbard model with nearest-neighbour hopping only, and to make
things as simple as possible, consider a lattice of only two sites. If, in addition, we
only have one, spin-up electron, then that single electron can either be on the ﬁrst
site (a conﬁguration we will denote as | ↑, 0⟩, meaning ‘spin-up electron on site 1,
nothing on site 2’) or on the second site (a conﬁguration we will denote as |0, ↑⟩).
Hence a general state can be written as the superposition |ψ⟩= a| ↑, 0⟩+ b|0, ↑⟩,
where a and b are complex numbers. In this case, there’s no possibility of potential
terms (because there is only one electron) and in this basis the Hubbard Hamiltonian
can be written
ˆH =
„
0
−t
−t
0
«
.
(4.67)
Solving gives us a ground state |ψ⟩=
1
√
2 (| ↑, 0⟩+ |0, ↑⟩) with energy E = −t and an
excited state |ψ⟩=
1
√
2 (| ↑, 0⟩−|0, ↑⟩) with energy E = t.
Now we’ll have two electrons in the system. If they have the same spin the answer’s
simple and E = 0 (try it). If the electrons have diﬀerent spins we write a general
state as |ψ⟩= a| ↑↓, 0⟩+ b| ↑, ↓⟩+ c| ↓, ↑⟩+ d|0, ↑↓⟩and in this basis the Hubbard
Hamiltonian is
ˆH =
0
B
B
@
U
−t
−t
0
−t
0
0
−t
−t
0
0
−t
0
−t
−t
U
1
C
C
A .
(4.68)

48
Making second quantization work
This can be diagonalized to give a ground state energy of E = U
2 −1
2
`
U2 + 16t2´ 1
2
which corresponds to a wave function |ψ⟩= N (| ↑↓, 0⟩+ W| ↑, ↓⟩+ W| ↓, ↑⟩+ |0, ↑↓⟩)
with N some normalization constant and W = U
4t + 1
4t
`
U2 + 16t2´ 1
2 .
In this chapter we have considered models on discrete lattices which are
relevant to condensed matter physics. In the following chapter, we will
consider how to generalize this approach to the continuum. However,
readers who are eager to see some uses of the machinery developed in
this chapter should now be able to tackle the ﬁrst parts of the chapters
in Part X, which deal with some applications from condensed matter
physics.
Chapter summary
• Field operators ˆψ† and ˆψ are the analogues of creation and anni-
hilation operators in position space.
• Second quantization of models can be carried out by writing down
operators as products of creation and annihilation operators that
encode the physical particle processes. We write these products
in normal-ordered form (meaning the ﬁeld creation operators are
written to the left and the ﬁeld annihilation operators to the right).
• The vacuum expectation of normal-ordered products is always zero.
• Examples of this technique have been given for the tight-binding
model and the Hubbard model.
Exercises
(4.1) We can deﬁne a generalized commutator [ ˆA, ˆB]ζ as
[ ˆA, ˆB]ζ = ˆA ˆB −ζ ˆB ˆA,
(4.69)
so that ζ = 1 yields [ ˆA, ˆB]ζ = [ ˆA, ˆB] for bosons
and ζ = −1 yields [ ˆA, ˆB]ζ = { ˆA, ˆB} for fermions.
The generalized commutation relations can then be
written
[ ˆψ(x), ˆψ†(y)]ζ = δ(3)(x −y),
(4.70)
and
[ ˆψ(x), ˆψ(y)]ζ = 0.
(4.71)
Repeat the argument given in Example 4.9 using
the generalized commutator and show that ˆVwrong
yields the same result for both bosons and fermions.
(4.2) One can deﬁne a single-particle density matrix
as
ˆρ1(x −y) = ⟨ˆψ†(x) ˆψ(y)⟩.
(4.72)
By substituting the expansion of ˆψ†(x) and ˆψ(y) in
terms of ˆa†
q and ˆaq into this expression, show that
ˆρ1(x −y) = 1
V
X
pq
e−i(q·x−p·y)⟨ˆa†
qˆap⟩.
(4.73)
(4.3) Evaluate the eigenvalues and eigenvectors of the
Hubbard Hamiltonian given in eqn 4.68 in Exam-
ple 4.11.
(a) What are the energy eigenvalues in the limit
t = 0?
(b) How do these energy levels change as t ̸= 0 in
the limit t/U ≪1?

Part II
Writing down Lagrangians
An important step in writing down a physical model is to construct the
appropriate Lagrangian. This part of the book is concerned with how
to do that and is structured as follows:
• In Chapter 5 we describe how the arguments in the previous part
which worked on discrete systems can be generalized to the con-
tinuum limit. After reviewing Hamilton’s formulation of classical
mechanics and Poisson brackets, we shift our attention from the
Lagrangian L to the Lagrangian density L. We use the electromag-
netic ﬁeld as a ﬁrst example of this approach.
• We have our ﬁrst stab at constructing a relativistic quantum wave
equation, the Klein–Gordon equation in Chapter 6.
This turns
out to have some unsavoury characteristics that means that it is
not the right equation to describe electrons, but it is nevertheless
illuminating and illustrates some of the issues we are going to come
across later.
• In Chapter 7 we present a set of example Lagrangians and show
how they work. After reading this chapter the reader should have
a good working knowledge of the simplest examples of quantum
ﬁeld theories.

5
Continuous systems
5.1 Lagrangians
and
Hamiltoni-
ans
50
5.2 A charged particle in an elec-
tromagnetic ﬁeld
52
5.3 Classical ﬁelds
54
5.4 Lagrangian and Hamiltonian
density
55
Chapter summary
58
Exercises
58
In the last few chapters we have seen examples of how various systems
can be reduced to a set of simple harmonic oscillators, each one describ-
ing a diﬀerent normal mode of the system. Each normal mode may be
thought of as a momentum state for non-interacting, identical particles,
with the number of particles corresponding to the number of quantized
excitations of that mode. We have deﬁned operators that have created
or annihilated these particles. In the previous chapter, we particularly
focussed on discrete models (tight-binding, Hubbard, etc.) in which par-
ticles moved around on a lattice. In this chapter we want to extend this
notion to the continuum limit, that is when the discretization of the
lattice can be ignored. This will lead to the concept of a classical ﬁeld.
Before we scale those dizzy heights, there is some unﬁnished business
from classical mechanics to deal with, thereby introducing some useful
formalism.
5.1
Lagrangians and Hamiltonians
In this section, we return to the Lagrangian formulation of classical
mechanics and introduce the Hamiltonian and the Poisson bracket. The
Hamiltonian will give us an expression for the energy which appears as
a conserved quantity if the Lagrangian doesn’t change with time. The
Poisson bracket is also telling us about how quantities are conserved.
Let’s begin by considering the Lagrangian L(qi, ˙qi) for a system described
by coordinates qi. The rate of change of L is given by
dL
dt = ∂L
∂qi
˙qi + ∂L
∂˙qi
¨qi,
(5.1)
where the summation convention is assumed. Using the Euler–Lagrange
equation we can turn this into
dL
dt = d
dt
 ∂L
∂˙qi

˙qi + ∂L
∂˙qi
¨qi = d
dt
 ∂L
∂˙qi
˙qi

.
(5.2)
We deﬁne the canonical momentum by
pi = ∂L
∂˙qi
,
(5.3)
and then eqn 5.2 can be written
d
dt(pi ˙qi −L) = 0.
(5.4)

5.1
Lagrangians and Hamiltonians
51
This is clearly a conservation law, and we write it as
dH
dt = 0,
(5.5)
thereby deﬁning the Hamiltonian H by
H = pi ˙qi −L.
(5.6)
We will see later that the Hamiltonian corresponds to the energy of the
system. Since H is conserved it is instructive to look at variations in H.
Thus
δH = piδ ˙qi + δpi ˙qi −∂L
∂qi
δqi −∂L
∂˙qi
δ ˙qi.
(5.7)
The ﬁrst and fourth terms in this expression cancel because of eqn 5.3,
and so
δH = δpi ˙qi −∂L
∂qi
δqi.
(5.8)
Since H is a function of qi and pi, we expect that
δH = ∂H
∂qi
δqi + ∂H
∂pi
δpi,
(5.9)
and so matching terms in eqns 5.8 and 5.9 produces ˙qi = ∂H/∂pi and
∂H/∂qi = −∂L/∂qi = −˙pi,
and these two equalities are known as
Note that because pi = (∂L/∂˙qi) then
˙pi = d/dt(∂L/∂˙qi) = ∂L/∂qi, where
the last equality follows from the Euler–
Lagrange equation.
Hamilton’s equations:
∂H
∂pi
= ˙qi,
∂H
∂qi
= −˙pi.
(5.10)
Hamilton’s equations represent another way to ﬁnd the equations of
motion of a system.
We now turn to the other piece of nineteenth century classical mechan-
ics which we want to review. We deﬁne the Poisson bracket {A, B}PB
Sim´eon Denis Poisson (1781–1840)
by
{A, B}PB = ∂A
∂qi
∂B
∂pi
−∂A
∂pi
∂B
∂qi
.
(5.11)
Now consider any function F which depends on the coordinates qi and
pi. The rate of change of F is given by
dF
dt = ∂F
∂t + ∂F
∂qi
˙qi + ∂F
∂pi
˙pi = ∂F
∂t + {F, H}PB ,
(5.12)
where the last equality is achieved using Hamilton’s equations. Thus if
the ﬁeld is not itself a function of time, we have
dF
dt = {F, H}PB ,
(5.13)
so that if {F, H}PB = 0 then F is a constant of the motion. We have
therefore found a link between a conservation law (F is a conserved
quantity) with the Poisson bracket of the Hamiltonian and F being zero.
This is highly evocative of the commutator in quantum mechanics, but
note that this result is entirely classical!

52
Continuous systems
Example 5.1
There is an interesting graphical interpretation of this result of a conserved F (see
Fig. 5.1). If F = F(q, p), then the vector ( ˙q, ˙p) = (∂H/∂p, −∂H/∂q) is tangent to
the surface F(q, p) = constant, because
∇F · ( ˙q, ˙p) =
„ ∂F
∂q , ∂F
∂p
«
· ( ˙q, ˙p) = ∂F
∂qi
∂H
∂pi
−∂F
∂pi
∂H
∂qi
= {F, H}PB = 0,
(5.14)
and ( ˙q, ˙p) is also tangent to the surface H(q, p) = constant, because
∇H · ( ˙q, ˙p) =
„∂H
∂q , ∂H
∂p
«
· ( ˙q, ˙p) = −˙p · ˙q + ˙q · ˙p = 0,
(5.15)
and so the trajectory of the system moves on the intersection of these two surfaces.
∇H
∇F
F is constant
H is constant
Fig. 5.1
The trajectory of the sys-
tem moves on the intersection of sur-
faces of constant H
and F.
This
is because ( ˙q, ˙p) is perpendicular to
both ∇F ≡
“
∂F
∂q , ∂F
∂p
”
(eqn 5.14) and
∇H ≡
“
∂H
∂q , ∂H
∂p
”
(eqn 5.15).
In quantum mechanics, the rate of change of the expectation value of an
operator ˆF is given by
d⟨ˆF⟩
dt
= 1
iℏ⟨[ ˆF, ˆH]⟩,
(5.16)
and since this looks very much like the classical version
dF
dt = {F, H}PB ,
(5.17)
we are led to visualize the classical to quantum crossover as involving
the replacement
{F, H}PB →1
iℏ⟨[ ˆF, ˆH]⟩,
(5.18)
and in fact for any Poisson bracket
{A, B}PB →1
iℏ⟨[ ˆA, ˆB]⟩.
(5.19)
Example 5.2
The Poisson bracket between position and momentum coordinates is
{qj, pk}PB
=
∂qj
∂qi
∂pk
∂pi
−∂pj
∂qi
∂qk
∂pi
=
δijδik −0 × 0
=
δjk.
(5.20)
Using eqn 5.19 we have [ˆqj, ˆpk] = iℏδjk, familiar from quantum mechanics.
5.2
A charged particle in an
electromagnetic ﬁeld
To practise some of the ideas in the previous section, we will consider a
very simple example: a free particle of mass m. Shortly we will give it a

5.2
A charged particle in an electromagnetic ﬁeld
53
charge q and turn on an electromagnetic ﬁeld, but for now we will keep
all electromagnetic ﬁelds switched oﬀ. We want to include relativity,
and so therefore the action S must be a Lorentz-invariant scalar. We
can write S =
R τ2
τ1 Lγ dτ where τ is the proper time, and so γL must
be a Lorentz invariant. We can thus write L = constant/γ and we will
choose
L = −mc2
γ ,
(5.21)
where the constant mc2 is chosen so that L has the correct low-velocity
limit.1 The action S can now be written
1See Exercise 5.4.
S =
Z τ2
τ1
−mc2dτ = −mc
Z b
a
ds,
(5.22)
where the interval2 ds =
p
c2dt2 −dx2 −dy2 −dz2. By the principle of
2The interval ds is an invariant, and
so takes the same form in all inertial
frames. In the rest frame of the parti-
cle ds = c dτ and so this is true in all
inertial frames.
least action δS = 0 and so δ
R b
a ds = 0. The integral
R b
a ds takes its max-
imum value (see Fig. 5.2) along a straight world-line (see Exercise 5.5)
and so this implies reassuringly that free particles move along straight
lines.
x
t
A
B
Fig. 5.2
The straight-line path from
A to B has a larger
R b
a ds than the wig-
gly one (because c dt <
√
c2 dt2 −dx2),
even though it looks longer.
Example 5.3
Find expressions for the momentum and energy of a free particle.
Answer:
p = ∂L
∂v = ∂
∂v
“
−mc2(1 −v2/c2)1/2”
= γmv,
(5.23)
E = H = p · v −L = γmv2 + mc2
γ
= γmc2
»v2
c2 +
„
1 −v2
c2
«–
= γmc2.
(5.24)
In special relativity, we can assemble the energy and momentum into
a momentum four-vector pµ =
  E
c , p

[or equivalently pµ =
  E
c , −p

].
To make our free particle more interesting, let’s give it a charge q and
couple it to an electromagnetic ﬁeld. The electromagnetic ﬁeld can be
described by a four-vector ﬁeld Aµ(x) =

V (x)
c
, A(x)

where V (x) is the
scalar potential and A(x) is the magnetic vector potential. The action
becomes3
3The potential energy of the electro-
magnetic ﬁeld interaction is linear in
the ﬁeld Aµ and takes the Lorentz-
invariant
form
−qAµdxµ.
Note
that Aµ
=
“
V
c , −A
”
and dxµ
=
(c dt, dx) = (c, v)dt and so −qAµdxµ =
−q(V −A · v)dt.
S =
Z
−mc ds −qAµ dxµ =
Z t2
t1
−mc2
γ
+ qA · v −qV

dt.
(5.25)
The Lagrangian is the integrand of this equation, and so the canonical
momentum in this case becomes
p = ∂L
∂v = γmv + qA.
(5.26)
Now setting c = 1, the familiar E and B ﬁelds can be obtained from
the four-vector potential Aµ = (V, A) using the equations
B = ∇× A
and
E = −∂A
∂t −∇V,
(5.27)

54
Continuous systems
but to put this in a relativistically covariant form we deﬁne the second-
rank antisymmetric tensor Fµν by
Note that:
∂µ =
„ ∂
∂t, −∇
«
,
∂µ =
„ ∂
∂t, ∇
«
,
Aµ = (V, A),
Aµ = (V, −A),
Also, keep in mind that
E = (E1, E2, E3),
and
B = (B1, B2, B3),
are ordinary three-vectors.
Fµν = ∂µAν −∂νAµ.
(5.28)
This is known as the electromagnetic ﬁeld tensor and looks like a
four-dimensional version of a curl. In components, this yields
Fµν =




0
E1
E2
E3
−E1
0
−B3
B2
−E2
B3
0
−B1
−E3
−B2
B1
0



.
(5.29)
µ
ν
Similarly, the tensor
F µν =




0
−E1
−E2
−E3
E1
0
−B3
B2
E2
B3
0
−B1
E3
−B2
B1
0



.
(5.30)
Searching for a Lagrangian for the electromagnetic ﬁeld, we again look
for a Lorentz scalar. One very suitable choice is
The
equation
is
FµνF µν
=
2
`
B2 −(E/c)2´
if
you
include
the factors of c.
FµνF µν = 2(B2 −E2).
(5.31)
The Lagrangian may then be written
L = −1
4
Z
d3xFµνF µν,
(5.32)
where the choice of the factor −1
4 will be explained later in the chapter.4
4It’s useful to note that the components
of the electric ﬁeld may be extracted
from F µν using
Ei = −F 0i = F i0,
(5.33)
and the magnetic ﬁeld is given by
Bi = −εijkF jk,
(5.34)
where εijk is the antisymmetric symbol
and no sum over indices is implied in
the last equation.
Finally we note a particularly important feature of electromagnetism
is that electric charges are locally conserved. This fact is enshrined in
the continuity equation relating the charge density ρ(x) and current
density J(x), which is written
∂ρ
∂t + ∇· J = ∂µJµ = 0,
(5.35)
where the electromagnetic current four-vector Jµ = (ρ, J). The continu-
ity equation prevents the destruction of charge followed by its creation
at another arbitrarily distant point. Such a process would obey global
conservation of charge, but would allow information to travel at an ar-
bitrary velocity, which we know from special relativity is forbidden.
5.3
Classical ﬁelds
We will now take the continuum limit of a simple model from classi-
cal mechanics and see how this leads to the concept of a classical ﬁeld.
We consider the case of a classical system of balls connected by springs

5.4
Lagrangian and Hamiltonian density
55
(whose quantum mechanics was examined in Chapter 2). In the contin-
uum limit we are going to make the distance ℓbetween the balls shrink
to zero and simultaneously let the mass m of each ball shrink to zero,
while keeping the mass per unit length ρ = m/ℓalong the chain constant.
The displacement qj(t) of the jth mass at time t becomes a continuous
variable φ(x, t) which depends on both spatial position x and time t. At
each point x we have an independent degree of freedom φ(x, t), so we
say that the ﬁeld has a ‘continuous inﬁnity’ of degrees of freedom. The
function φ(x, t) is one example of a classical ﬁeld, but the concept can be
much more general than this. For example, rather than describing just
a simple displacement, our ﬁeld can describe more complicated objects
such as electromagnetic ﬁelds. In each case though, the ﬁeld depends
on position in spacetime. We can now make a working deﬁnition of the
idea of a classical ﬁeld:5
5We make the fussy assignment of a
classical ﬁeld here to distinguish it from
a quantum ﬁeld. As we will see shortly
the latter inputs a position in spacetime
and outputs an operator rather than an
amplitude.
A classical ﬁeld is a machine that takes a position in spacetime and
outputs an object representing the amplitude of the ﬁeld at that point.
The output might be a scalar (in which case we refer to a scalar ﬁeld),
a complex number (a complex scalar ﬁeld), a vector (in which case
we refer to a vector ﬁeld), a tensor or something more complicated.
Fields are locally deﬁned objects. The ﬁeld ‘machine’ that we have de-
scribed is like a computer subroutine which returns the value of φ at the
position x and time t that we’ve inputted. It is local in the sense that
it doesn’t tell you about the value of φ somewhere else.
Example 5.4
The temperature T(x, t) is an example of a scalar ﬁeld. The E(x, t) and B(x, t)
ﬁelds of electromagnetism are familiar examples of vector ﬁelds, which could be
time-dependent. The four-vector potential Aµ(x) is an example of a four-vector ﬁeld
(and here we have written it as a function of spacetime coordinates xν). The object
deﬁned in eqn 5.28, Fµν(x), is an example of a second-rank tensor ﬁeld.
5.4
Lagrangian and Hamiltonian density
The next task is to use this language of classical ﬁelds to formulate La-
grangians and Hamiltonians. We will do this ﬁrst for a simple example.
Example 5.5
In the discrete case considered in Section 2.4 we had a Hamiltonian given by
H =
X
j
p2
j
2m + 1
2K(qj+1 −qj)2
(5.36)

56
Continuous systems
and a Lagrangian given by
L =
X
j
p2
j
2m −1
2K(qj+1 −qj)2.
(5.37)
In the continuum limit, these need to be replaced as the number of masses goes to
inﬁnity while ℓ→0. The sums become integrals, so we make the substitution
X
j
→1
ℓ
Z
dx.
(5.38)
Using the substitution in eqn 5.38 the kinetic energy term in H and L becomes
X
j
1
2m
„ ∂qj
∂t
«2
→
1
ℓ
Z
dx 1
2m
„ ∂φ(x, t)
∂t
«2
=
Z
dx 1
2ρ
„∂φ
∂t
«2
, (5.39)
where we’ve used the string density ρ = m/ℓ. For the potential energy term we also
make a rule for the diﬀerences in coordinates: these become spatial derivatives and
hence
(qj+1 −qj)
ℓ
→∂φ(x, t)
∂x
.
(5.40)
This means that the potential energy term becomes
X
j
1
2 K(qj+1 −qj)2 →
X
j
1
2Kℓ2
„ ∂φ(x, t)
∂x
«2
=
Z
dx 1
2T
„∂φ
∂x
«2
(5.41)
where we’ve replaced the spring constant K and the lattice spacing ℓwith the ten-
sion in the string T = Kℓ. Putting this all together (and generalizing to a three-
dimensional system) gives a Hamiltonian
H =
Z
d3x
"
1
2ρ
„∂φ
∂t
«2
+ 1
2T (∇φ)2
#
,
(5.42)
and a Lagrangian
L =
Z
d3x
"
1
2ρ
„∂φ
∂t
«2
−1
2 T (∇φ)2
#
.
(5.43)
Our expressions for H and L are integrals over a position coordinate
and so we can deﬁne a Hamiltonian density H by
H =
Z
d3x H,
(5.44)
and similarly a Lagrangian density L as
L =
Z
d3x L.
(5.45)
In general, both the Hamiltonian density H and the Lagrangian density
L are functions of φ, ˙φ and φ′. We can deﬁne a conjugate momentum
π(x) in terms of the functional derivative6
6Just like pi = ∂L
∂˙qi
. For a careful jus-
tiﬁcation of the last equality see Wein-
berg, Chapter 7.
π(x) = δL
δ ˙φ
= ∂L
∂˙φ
,
(5.46)
and then this implies that H and L can be related by7
7In Example 5.5 the particular expres-
sions are:
L = 1
2ρ
„ ∂φ
∂t
«2
−1
2T (∇φ)2 ,
H = 1
2ρ
„∂φ
∂t
«2
+ 1
2T (∇φ)2 ,
and
π = ρ ∂φ
∂t .
You can check that eqn 5.47 holds.
H = π ˙φ −L.
(5.47)
Now recall from eqn 1.34 that the action S is related to L by the four-
dimensional integral S =
R
d4x L(φ, ∂µφ), and the action principle, given
by δS = 0, implies that (eqn 1.35)
∂L
∂φ −∂µ

∂L
∂(∂µφ)

= 0,
(5.48)
the four-vector version of the Euler–Lagrange equation.

5.4
Lagrangian and Hamiltonian density
57
Example 5.6
For the electromagnetic ﬁeld, the Lagrangian density is written
In SI units L = −
1
4µ0 FµνF µν and so
L = 1
2 ǫ0E2 −
1
2µ0 B2.
L = −1
4FµνF µν = 1
2(E2 −B2).
(5.49)
In the absence8 of an electric potential (V
= 0), we have L =
1
2 (E2 −B2) =
8Recall eqn 5.27 for the relationship be-
tween ﬁelds and potentials.
1
2 ( ˙A
2 −B2). The conjugate momentum is πi = ∂L/∂(∂0Ai), and so π = −˙A = E,
so that
H = πi ˙Ai −L = 1
2(E2 + B2).
(5.50)
The Euler–Lagrange equation (eqn 1.35) gives
In SI units H =
1
2µ0 B2 + 1
2 ǫ0E2.
∂L
∂Aµ
−∂λ
„
∂L
∂(∂λAµ)
«
= 0.
(5.51)
The ﬁrst term in this expression is zero since L = −1
4FµνF µν contains only deriva-
tives of Aµ. The second term can be rewritten as
∂λF λµ = 0,
(5.52)
which is a compact way of writing two of Maxwell’s equations in free space, namely
∇· E = 0 and ∇× B =
˙E.
The current density four-vector Jµ = (ρ, J) can
also be included, and couples linearly to the electromagnetic potential, and so the
Lagrangian becomes
L = −1
4FµνF µν −JµAµ.
(5.53)
In this case
∂L
∂Aµ
= −Jµ,
(5.54)
and so
∂λF λµ = Jµ,
(5.55)
which generates the other two Maxwell equations: ∇· E = ρ and ∇× B = J + ˙E.
In SI units ∂λF λµ = µ0Jµ.
Solving the Euler–Lagrange equation will produce all the allowable
modes of oscillation and hence the allowed wave vectors, the particu-
lar values kn, that may occur without dissipation. By the principle of
superposition the most general wave is one made up of a weighted sum
of all of the possible modes
φ(x, t) =
X
kn
akne−i(ωt−kn·x).
(5.56)
Thus we have seen how many classical continuous systems may be ex-
pressed in a Lagrangian formalism so that the Euler–Lagrange equation
allows us to derive the normal modes. Our experience with the simple
harmonic oscillator in Chapter 2 motivates the idea that these normal
modes are simply harmonic oscillators themselves and so can be quan-
tized, leading to particle-like solutions. For the electromagnetic ﬁeld,
those quanta are of course photons.9 Now with this classical ﬁeld the-
9We will put this on a ﬁrmer footing in
Chapter 14 and show in detail how pho-
tons emerge from quantizing the classi-
cal electromagnetic ﬁeld.
ory under our belt, we are ready to return to the quantum world, and
in the next chapter we will make our ﬁrst stab at relativistic quantum
mechanics.

58
Continuous systems
Chapter summary
• A ﬁeld takes a position in spacetime and outputs an object repre-
senting the amplitude (be it scalar, vector or complex) of the ﬁeld
at that point.
• The Lagrangian and Hamiltonian density are related by H = π ˙φ−L
where π = ∂L/∂˙φ is the generalized momentum.
• The Lagrangian for the electromagnetic ﬁeld is L = −1
4FµνF µν,
where Fµν = ∂µAν −∂νAµ is the electromagnetic ﬁeld tensor.
Exercises
(5.1) If the Lagrangian does depend explicitly on time,
then
dL
dt = ∂L
∂t + ∂L
∂xi ˙xi + ∂L
∂˙xi ¨xi.
(5.57)
In this case show that
∂L
∂t = −dH
dt .
(5.58)
(5.2) Show that Poisson brackets anticommute
{A, B}PB = −{B, A}PB ,
(5.59)
and also satisfy the Jacobi identity
˘
{A, B}PB , C}PB +
˘
{C, A}PB , B
¯
PB
+
˘
{B, C}PB , A
¯
PB = 0,
(5.60)
and show that quantum mechanical commutators
also have the same properties.
(5.3) Show that the commutator of two Hermitian oper-
ators ˆA and ˆB is anti-Hermitian, i.e. that
[ ˆA, ˆB]† = −[ ˆA, ˆB].
(5.61)
The factor of i in many commutator expressions
(e.g.
[ˆx, ˆp] = iℏ, [ˆLx, ˆLy] = iℏˆLz, and [ ˆA, ˆB] =
1
iℏ{A, B}PB) makes sure that this property is
obeyed.
(5.4) The Lagrangian for a free particle is L = −mc2/γ.
Find an expression for L, p and H when v ≪c.
(5.5) Show that
R b
a ds takes its maximum value when the
integration is done over a straight world-line be-
tween a and b [NB ds2 = c2dt2 −dx2 −dy2 −dz2].
(5.6) Use the Lagrangian L =
−mc2
γ
+ qA · v −qV for
a free particle of charge q and mass m in an elec-
tromagnetic ﬁeld to derive the Lorentz force, i.e.
show
d
dt(γmv) = q(E + v × B).
(5.62)
[Hint: ∇(a · b) = (a · ∇)b + (b · ∇)a + b × curl a +
a × curl b.]
(5.7) Show that for the Lagrangian
L = −mc2
γ
+ qA · v −qV,
(5.63)
when v ≪c the momentum becomes p = mv + qA
and the energy becomes
E = mc2 +
1
2m(p −qA)2 + qV.
(5.64)
(5.8) Another Lorentz scalar that can be obtained from
Fµν is
εαβγδFαβFγδ.
(5.65)
Show that this leads to E · B being an invariant.
(5.9) Show that ∂µF µν = Jν reduces to two of Maxwell’s
equations, and that the other two can be generated
from the equation
∂λFµν + ∂µFνλ + ∂νFλµ = 0.
(5.66)
(5.10) Show that ∂β∂αF αβ = 0 and hence that ∂µJµ = 0.
Interpret this as a continuity equation.

6
A ﬁrst stab at relativistic
quantum mechanics
6.1 The
Klein–––Gordon
equation
59
6.2 Probability currents and den-
sities
61
6.3 Feynman’s
interpretation
of
the negative energy states 61
6.4 No conclusions
63
Chapter summary
63
Exercises
63
Bohr: What are you working on?
Dirac: I’m trying to get a relativistic theory of the electron.
Bohr: But Klein has already solved that problem.
(Conversation at the 1927 Solvay Conference.)
We now take a break from the formalism of classical mechanics and turn
back to quantum mechanics. In this chapter, we will attempt to ﬁnd
a quantum mechanical equation of motion that is properly relativistic.1
1It’s a ﬁrst stab because the right an-
swer is due to Dirac, and we will deal
with that later. This chapter is about
the attempt introduced by Klein and
also by Gordon, which is why in the
opening quote Bohr didn’t think Dirac
was working on something worthwhile.
This will lead to an equation known as the Klein–Gordon equation,
though in fact it was derived ﬁrst by Schr¨odinger, who rejected it because
it had what seemed like a fatal ﬂaw (which we will explore later); he
then went on to a second attempt at the problem and produced what
is known as the Schr¨odinger equation, which works brilliantly but is
non-relativistic.
6.1
The Klein–––Gordon equation
We ﬁrst review the logical chain of ideas which leads to the Schr¨odinger
equation. In the non-relativistic quantum mechanics of a free particle,
we start with the dispersion relationship, which links the energy and
momentum of the particle using the simple expression for kinetic energy
E = p2
2m. We turn the variables E and p into operators (E →ˆE, p →ˆp),
and then we make the substitutions2 ˆE = iℏ∂
∂t and ˆp = −iℏ∇. This
2Just to make some of the equations a
bit more familiar, we will brieﬂy rein-
troduce all the factors of ℏand c.
then leads to the Schr¨odinger equation
iℏ∂φ(x, t)
∂t
= −ℏ2
2m∇2φ(x, t),
(6.1)
where the object that the operators act on is φ(x, t), the wave function.
The free-particle solutions to the Schr¨odinger equation are plane waves
of the form φ(x, t) = Ne−i(ωt−k·x), where N is a normalization constant.
We can write this solution in four-vector form φ(x) = Ne−ip·x. We’ll
call a wave with this sign combination in the exponential an incoming
wave. Operating on it with momentum and energy operators yields
ˆpφ(x, t)
=
−iℏ∇(Ne−i(ωt−k·x)) = ℏkφ(x, t),
(6.2)
ˆEφ(x, t)
=
iℏ∂
∂t(Ne−i(ωt−k·x)) = ℏωφ(x, t).

60
A ﬁrst stab at relativistic quantum mechanics
That is, an incoming wave has a positive momentum and energy.
Now, to get a relativistic wave equation we’ll try the same trick. For
a relativistic particle we have the dispersion relationship
E = (p2c2 + m2c4)
1
2 ,
(6.3)
and by making the same operator substitutions as we had before (E →
ˆE = iℏ∂
∂t, p →ˆp = −iℏ∇) we obtain
iℏ∂φ
∂t = (−ℏ2c2∇2 + m2c4)
1
2 φ.
(6.4)
This has two big problems. Number one: it doesn’t look covariant (the
spatial and temporal derivatives appear in diﬀerent forms).
Number
two: how do we cope with the square root sign which seems to imply
that we take the square root of a diﬀerential operator?
Instead of tackling these problems we take a side-step. To avoid the
square root we’ll simply square the dispersion relation and start again.
The squared form of the dispersion is E2 = p2c2 + m2c4. Making the
same operator substitutions as before, we obtain
−ℏ2 ∂2φ(x, t)
∂t2
= (−ℏ2c2∇2 + m2c4)φ(x, t).
(6.5)
This equation of motion for a wave function is called the Klein–Gordon
equation. To make things as easy to read as possible we’ll revert to
Oskar Klein (1894–1977)
Walter Gordon (1893–1939)
using natural units where ℏ= c = 1 from now on, and this gives us the
following form of the Klein–Gordon equation:
−∂2φ(x, t)
∂t2
= (−∇2 + m2)φ(x, t).
(6.6)
The Klein–Gordon equation ﬁts in nicely with our relativistically covari-
ant language, since we can write
(∂2 + m2)φ(x) = 0,
(6.7)
thereby amalgamating the space and time derivatives.3
3Note that ∂2 = ∂µ∂µ =
∂2
∂t2 −∇2.
Example 6.1
Let’s solve the Klein–Gordon equation for a free particle.
We’ll try a solution4
4In natural units k = p and ω = E
and since we’re mostly interested in the
energies and momenta of particles we’ll
use p and E.
φ(t, x) = Ne−iEt+ip·x = Ne−ip·x. Upon substituting we ﬁnd that φ(x) is indeed a
solution, yielding the expected dispersion
E2 = p2 + m2.
(6.8)
This looks ﬁne, until we realize that we have to take the square root to get the energy
which gives E = ±(p2 + m2). We might be tempted to just throw away the negative
solutions as unphysical, but in fact the appearance of negative energy solutions is
going to be telling us something.

6.2
Probability currents and densities
61
6.2
Probability currents and densities
One of the reasons that Schr¨odinger wasn’t happy with the Klein–
Gordon equation after he’d derived it was that something rather nasty
happens when you think about the ﬂow of probability density.
The
probability of a particle being located somewhere depends on φ∗(x)φ(x)
and so if this quantity is time-dependent then particles must be sloshing
around. The probability density ρ and probability current density5 j
5Here j is the current density of par-
ticles, whereas in Example 5.6 we used
J as the current density of charge. If
both appear together, as they will do
in Section 6.3 below, we will write the
former as j and the latter as Jem.
obey a continuity equation
dρ
dt + ∇· j = 0,
(6.9)
which is more easily written in four-vector notation as
∂µjµ = 0.
(6.10)
If, as is usual in non-relativistic quantum mechanics,6 we take the spatial
6In non-relativistic quantum mechan-
ics, j is deﬁned with some additional
constants:
j = −(iℏ/2m)(ψ∗∇ψ −ψ∇ψ∗).
We dispense with the ℏ/2m constant
here.
part to be
j(x) = −i [φ∗(x)∇φ(x) −φ(x)∇φ∗(x)] ,
(6.11)
then, for eqn 6.10 to work,7 we require the probability density to look
7It will work, and you can prove it as
follows. Take the Klein–Gordon equa-
tion (eqn 6.5) and premultiply it by
φ∗. Then take the complex conjugate
of eqn 6.5 and premultiply by φ. Sub-
tracting these two results will give an
equation of the form of eqn 6.9 with j
and ρ as given.
like8
8Note
that
this
is
very
diﬀerent
from non-relativistic quantum mechan-
ics where ρ = |ψ|2.
ρ(x) = i

φ∗(x)∂φ(x)
∂t
−∂φ∗(x)
∂t
φ(x)

.
(6.12)
The resulting covariant probability current for the Klein–Gordon equa-
tion is then given by
jµ(x) = i {φ∗(x)∂µφ(x) −[∂µφ∗(x)] φ(x)} ,
(6.13)
which, as the notation suggests, is a four-vector. Substituting in our
wave function φ(x) = Ne−ip·x gives the time-like component of the prob-
ability current as
j0 = ρ = 2|N|2E.
(6.14)
Since E can be positive or negative we can’t interpret ρ as a probability
density, since there’s no such thing as negative probability! This seems
to ring the death knell for the Klein–Gordon equation as a half-decent
single-particle quantum mechanical equation.
6.3
Feynman’s interpretation of the
negative energy states
Fortunately, all is not lost. Richard Feynman9 and, independently, Ernst
9Richard
Feynman
(1918–1988)
in-
vented much of the physics in this book.
The bongo-playing theoretical physicist
once deﬁned science as ‘the belief in the
ignorance of experts’. He was never sat-
isﬁed by an explanation until he had
worked it out for himself, and it is per-
haps ﬁtting that his quirky shorthand
for doing quantum ﬁeld theory calcu-
lations, the little cartoons we now call
Feynman diagrams, decorate the pages
of every book on quantum ﬁeld the-
ory, including this one. (He also noted
that bongo players didn’t feel the need
to point out that he was a theoretical
physicist.)
Stuckelberg10 came up with an interpretation of the negative energy
10Brilliant, eccentric and diﬃcult, the
Swiss
mathematician
and
physicist
Ernst Stueckelberg (1905–1984) may
also claim credit for the invention of
a signiﬁcant amount of the physics in
this book, although the opaqueness of
many of his arguments and his insis-
tence on inventing novel notation have
prevented this from being widely recog-
nized.
His achievements include: this
interpretation of antiparticles; the in-
vention of spacetime (‘Feynman’) dia-
grams; the explanation of (‘Yukawa’)
force-carrying particles; the discovery
of the renormalization group and the
conservation of baryon number.
See
Schweber, Chapter 10 for more details.
states as particles moving backward in time. We call these states an-
tiparticles.
Consider the classical equation of motion for a charged
particle in an electromagnetic ﬁeld which looks like
md2xµ
dτ 2 = qF µ
ν
dxν
dτ ,
(6.15)

62
A ﬁrst stab at relativistic quantum mechanics
where τ is proper time, q is charge and F µν is the electromagnetic ﬁeld
tensor. Changing the sign of the proper time τ has the same eﬀect as
changing the sign of the charge. Therefore, a particle travelling back-
ward in time looks the same as an oppositely charged antiparticle mov-
ing forward in time. One strategy to eliminate negative energy states
is therefore to reverse the charge and momentum of all negative energy
solutions, turning them into antiparticles. How does this eliminate the
negative sign of the energy? Since we write the phase of the wave func-
tion as Et −p · x, then making the substitution t →−t means that we
have to change −E →E for the ﬁrst term to make sense. Of course
reversing the time (like playing a ﬁlm backwards) reverses all momenta
so we also need to swap p →−p for consistency.
We can go further by invoking some quantum mechanics. Let’s ex-
amine the electromagnetic current density for a Klein–Gordon particle
which we will deﬁne as Jµ
em = qjµ where q is the charge of the particle.
The current four-vector Jµ
em for a positively charged, incoming particle11
11Incoming implies inclusion of a factor
e−ip·x.
with positive energy is given by
Jµ
em
=
(+q)2|N|2pµ
(6.16)
=
(+q)2|N|2 (E, p) .
(6.17)
In contrast, a negative energy particle with positive charge has Jµ
em =
(+q)2|N|2 (−E, p). Since we’ve decided that negative energies are dis-
gusting because they mess up the probability, we want to turn the energy
positive. We do this by taking the minus sign out to the front of the
equation to yield
Jµ
em = (−q)2|N|2 (E, −p) .
(6.18)
Notice that we have to swap the sign of charge and the three-momentum
p, changing an incoming particle into an outgoing antiparticle, in agree-
ment with our argument above.
We now have a rule to turn unphysical negative energies into physical
positive energies. Simply treat the particle as one with positive energy,
but change the particle into an antiparticle by reversing the charge and
swapping the sign of the three-momentum.
BEFORE
AFTER
(i)
E, e, p
EQq
E + E
Q + e
q + p
(ii)
E, e, p
EQq
E −E
Q −e
q −p
(iii)
−E, e, p
EQ
q
E −E
Q + e
q + p
(iv)
E, −e, −p
EQq
E −E
Q + e
q + p
Fig. 6.1 (i) An incoming particle (en-
ergy E, charge e (≡|e|), momentum
p) is absorbed by a system (energy E,
charge Q, momentum q). Also shown
are (ii) emission of particle, (iii) absorp-
tion of a particle in a negative energy
state and (iv) emission of an antiparti-
cle in a positive energy state.
Is this really permissible for thinking about interacting quantum par-
ticles, which we know from quantum mechanics can be created and de-
stroyed and emitted and absorbed?
It is, as we’ll see from Fig. 6.1.
In a typical process we might have an incoming particle absorbed by
a system, donating its energy, momentum and charge to the system
[Fig. 6.1(i)].
Alternatively a particle could be emitted, depleting the
system of energy, momentum and charge [Fig. 6.1(ii)]. The key point is
that absorption of a particle in a negative energy state [Fig. 6.1(iii)] is
equivalent to the emission of an antiparticle in a positive energy state
[Fig. 6.1(iv)], at least as far as the system is concerned.
To summarize then, although the dispersion relation E = ±
p
p2 + m2
admits positive and negative solutions, only positive energies are physi-
cal. We therefore make all energies positive, but we can’t just ignore the
formerly negative energy states. The formerly negative energy states are

Exercises
63
interpreted as positive energy antiparticles with momenta in the opposite
direction to the corresponding particle. A general solution to the Klein–
Gordon equation with a particular positive energy now corresponds to
a superposition of two states12
12The assignments ‘incoming’ and ‘out-
going’ are made to link up with the
construction of Feynman diagrams in
Chapter 19.
φ(x) =


Incoming positive
energy particle
∝e−i(Et−p·x)

+


Outgoing positive
energy antiparticle
∝e+i(Et−p·x)

.
(6.19)
This general solution is therefore not an inherently single-particle de-
scription, because we are forced to consider particles and antiparticles.
6.4
No conclusions
Although the Klein–Gordon equation has now been made semi-
respectable with Feynman’s interpretation it still has an uncomfortable
feeling about it. What is implied by the new interpretation? Does it
correspond to anything sensible in real life? We’ll leave these questions
for a moment. The Klein–Gordon equation will return later and we will
ﬁnd that it is, in fact, a perfectly respectable equation but in a slightly
diﬀerent context, namely that of describing the dynamics of ﬁelds whose
excitations are spinless particles. Our next step is to systematize our
procedure for writing down theories.
Chapter summary
• The Klein–Gordon equation is (∂2 + m2)φ = 0.
• It suﬀers from having negative energy states and negative proba-
bilities.
• We can think of these negative energy states as positive energy
antiparticles
Exercises
(6.1) This problem looks forward to ideas which we will
consider in more detail in the next chapter. Show
that the Lagrangian density L given by
L = 1
2 (∂µφ)2 −1
2m2φ2,
(6.20)
yields the Klein–Gordon equation when using the
Euler–Lagrange equation. Also derive expressions
for the momentum π = ∂L/∂˙φ and the Hamilto-
nian density H = π ˙φ −L for this Lagrangian and
interpret the result.

7
Examples of Lagrangians,
or how to write down a
theory
7.1 A massless scalar ﬁeld
64
7.2 A massive scalar ﬁeld
65
7.3 An external source
66
7.4 The φ4 theory
67
7.5 Two scalar ﬁelds
67
7.6 The complex scalar ﬁeld
68
Chapter summary
69
Exercises
69
I regard as quite useless the reading of large treatises of pure
analysis: too large a number of methods pass at once before
the eyes. It is in the works of applications that one must
study them; one judges their ability there and one apprises
the manner of making use of them.
Joseph-Louis Lagrange (1736–1813)
Theoretical physics is all about writing down models to describe the
behaviour of particular systems in the Universe. The insight that we
have gained from the previous chapters is that the main starting point
for a ﬁeld theory is writing down an appropriate Lagrangian.1 This is
1Lagrangian density is a bit of a mouth-
ful, so we are using the universal short-
hand of referring to Lagrangian densi-
ties simply as ‘Lagrangians.’
Pedants
beware.
such an important step in the process that we are devoting an entire
chapter to it, partly to reiterate some of the ideas introduced earlier and
partly to demystify the act of plucking a Lagrangian out of thin air as
practised by many professional theorists.
Why do we choose a particular Lagrangian L? Usually because it’s
the simplest model that contains all the key physical ideas and has the
correct symmetry for the problem.2 Its artful construction allows it to
2Symmetry will turn out to be essen-
tial to our understanding of ﬁeld the-
ory. However, we postpone its discus-
sion until Chapter 10.
spit out the right equation of motion once fed into the waiting jaws of
the Euler–Lagrange equation, given by
∂L
∂φ −∂µ

∂L
∂(∂µφ)

= 0.
(7.1)
Then, the wavelike solutions of this equation of motion can be inter-
preted as particles with the appropriate dispersion relation, i.e. the rela-
tion between energy and momentum. This process of upgrading classical
modes into quantum particles will be described in detail in the next part
of the book. Here we will just assume that it can be done somehow.
In this chapter, we will examine a succession of simple examples of
choices of Lagrangian, for massless and massive scalar ﬁelds, a scalar ﬁeld
coupled to an external source, a couple of scalar ﬁelds and a complex
ﬁeld (deferring vector and spinor ﬁelds to later in the book).
7.1
A massless scalar ﬁeld
We start with the simplest possible Lagrangian, one for a massless3
3The assignment of masslessness will be
explained shortly.

7.2
A massive scalar ﬁeld
65
scalar ﬁeld4 φ(x). The scalar ﬁeld assigns a scalar amplitude to each
4Throughout this book we assume the
scalar ﬁeld φ(x) outputs a real number
rather than a complex one. The com-
plex scalar ﬁeld ψ(x) will be discussed
separately.
position x in spacetime. It can change as a function of spacetime co-
ordinate and its gradient is ∂µφ ≡∂φ(x)/∂xµ.
The Lagrangian will
depend only on the rate of change of φ in time ∂0φ and in space ∇φ.
We have to make a choice of L which is relativistically covariant and so
we choose
L = 1
2∂µφ ∂µφ = 1
2 (∂µφ)2 .
(7.2)
This can be expanded as L = 1
2(∂0φ)2 −1
2∇φ · ∇φ, which has the look
of5 L = (kinetic energy)−(potential energy). The factor of 1
2 is included
5Note that 1
2 (∂µφ)2 is often called the
kinetic energy as a shorthand.
so that later results come out nicely, as we shall see.
Example 7.1
Using the Lagrangian in eqn 7.2, we have
∂L
∂φ = 0,
∂L
∂(∂µφ) = ∂µφ,
(7.3)
and hence plugging into the Euler–Lagrange equation (eqn 7.1) we have
∂µ∂µφ = 0.
(7.4)
This is the wave equation ∂2φ = 0, or
∂2φ
∂t2 −∇2φ = 0,
(7.5)
and has wave-like solutions
φ(x, t) =
X
p
ape−i(Ept−p·x),
(7.6)
with dispersion relation Ep = c|p| [though in our units c = 1], see Fig. 7.1.
p
Ep
Fig. 7.1 The dispersion relation Ep =
|p|.
Note that Ep = 0 at |p| = 0 and so the dispersion relation is gapless.
If we take this equation to describe the energies of the particles that
will (later) represent quantum excitations of this system then, since this
is the m = 0 version of the relativistic dispersion Ep =
p
p2 + m2,
we call the particles massless and the ﬁeld a massless scalar ﬁeld. The
wave solutions of this linear equation obey the principle of superposition.
We say that the particles which correspond to these waves are free or
non-interacting. If we send these particles towards each other they will
simply pass on through each other without scattering.
7.2
A massive scalar ﬁeld
The next step is to try and include mass in the problem. To do this, we
make L depend not only on ∂µφ (how the scalar ﬁeld changes through
spacetime) but also on φ (the magnitude of the scalar ﬁeld). This can
be done by introducing a potential energy term U(φ) (and since La-
grangians are kinetic energy minus potential energy, this will come with
a minus sign). The potential energy term expresses the cost of having a

66
Examples of Lagrangians, or how to write down a theory
ﬁeld there at all, rather than just vacuum. By choosing U(φ) ∝φ2 we
will have a quadratic potential energy with a minimum at φ = 0. Thus
we write down
L = 1
2 (∂µφ)2 −1
2m2φ2,
(7.7)
where the 1
2m2 factor is chosen to make the next bit come out right.
The parameter m has yet to be shown to be anything physical, but will
turn out (of course) to be the particle mass.
Example 7.2
Using the Lagrangian in eqn 7.7, we have
∂L
∂φ = −m2φ,
∂L
∂(∂µφ) = ∂µφ,
(7.8)
and hence plugging into the Euler–Lagrange equation (eqn 7.1) we have
(∂µ∂µ + m2)φ = 0.
(7.9)
The equation of motion for this ﬁeld theory is the Klein–Gordon equation!
The
solution of these equations is again φ(x, t) = ae−i(Ept−p·x), with dispersion E2
p =
p2 + m2 (see Fig. 7.2).
p
Ep
Ep = |p|
m
Ep =
p
p2 + m2
Fig. 7.2 The dispersion relation Ep =
p
p2 + m2 for a massive particle.
Obviously if m = 0 we revert to the case of Section 7.1. With m ̸= 0 we
have a gap in the dispersion relation (at p = 0, Ep = ±m) corresponding
to the particle’s mass. Again, the equations of motion are linear and
therefore the particles described by this theory don’t interact.
7.3
An external source
We now want to introduce interactions. The simplest way to do this is to
have the scalar ﬁeld interact with an external potential. This potential is
described by a function known as a source current J(x) which interacts
with the ﬁeld, giving a potential energy term −J(x)φ(x). The resulting
Lagrangian is written
L = 1
2[∂µφ(x)]2 −1
2m2[φ(x)]2 + J(x)φ(x).
(7.10)
The equations of motion become6
6A shorthand version of the same equa-
tion is
(∂2 + m2)φ(x) = J(x).
(∂µ∂µ + m2)φ(x) = J(x).
(7.11)
This is now an inhomogeneous partial diﬀerential equation. The use of
sources (or sorcery) will later turn up as a method of probing quantum
ﬁelds following a technique invented by Julian Schwinger.

7.4
The φ4 theory
67
7.4
The φ4 theory
How do we get particles to interact with other particles (or, equiva-
lently, ﬁelds to interact with other ﬁelds)? Here’s the simplest recipe.
The φ4 Lagrangian is like the scalar case, but with an extra potential
term U(φ) proportional to φ4. This term makes the theory interacting.
Unfortunately it also makes it unsolvable. As we’ll see, we have to use
a sort of perturbation theory to make predictions from this theory. The
Lagrangian is7
7The numerical coeﬃcient of the φ4
term is often chosen with convenience
in mind.
We will use several in the
course of the book, but the coeﬃcient
λ
4! will be most common.
L = 1
2∂µφ∂µφ −1
2m2φ2 −1
4!λφ4,
(7.12)
which leads to the (not-very-memorable) equation of motion
(∂2 + m2)φ = −λ
3!φ3.
(7.13)
The constant of proportionality λ is known as the interaction strength.
Clearly, when λ = 0 we return to the massive scalar ﬁeld Lagrangian
and the ﬁelds don’t interact.
7.5
Two scalar ﬁelds
Here’s another way to make particles interact. Why not have two diﬀer-
ent types of particle in the Universe, described by two ﬁelds φ1(x) and
φ2(x)? In our simple theory they have the same mass and will inter-
act with themselves and each other via a potential energy U(φ1, φ2) =
g
 φ2
1 + φ2
2
2 where g is the interaction strength here. Notice that mul-
tiplying out the bracket gives self-interacting φ4 terms and, crucially for
us, a cross-term 2φ2
1φ2
2 which forces the two types of ﬁeld to interact.
The resulting Lagrangian density is
L
=
1
2(∂µφ1)2 −1
2m2φ2
1 + 1
2(∂µφ2)2 −1
2m2φ2
2 −g
 φ2
1 + φ2
2
2 .
(7.14)
We notice that we get the same Lagrangian as in eqn 7.14 even if we
change our deﬁnition of the ﬁelds. (We say that the Lagrangian has a
symmetry.) If we transform the two ﬁelds using the mapping φ1 →φ′
1
and φ2 →φ′
2 where

φ′
1
φ′
2

=

cos θ
−sin θ
sin θ
cos θ
 
φ1
φ2

,
(7.15)
then the Lagrangian is unchanged. This looks a lot like we’ve rotated
the ﬁelds in φ1-φ2 space. We say that the particles have an internal
degree of freedom. It’s as if they carry round a dial labelled by θ and
by turning this dial (i.e. changing θ) a φ1 particle may be turned into
a superposition of φ1 and φ2 particles. The invariance of the physics
(via the Lagrangian) with respect to rotations by θ in the φ1-φ2 plane
expresses an SO(2) symmetry.8
8SO(2) is the two-dimensional special
orthogonal group, corresponding to the
group of 2 × 2 orthogonal matrices
with unit determinant. This is a con-
tinuous symmetry and we will later
show that continuous symmetries lead
to conserved quantities. More of this in
Chapters 13 and 15.

68
Examples of Lagrangians, or how to write down a theory
7.6
The complex scalar ﬁeld
We can also make a transformation that simpliﬁes the two-scalar-ﬁelds
Lagrangian considered in the previous section. We deﬁne two new ﬁelds
ψ and ψ† given by
ψ
=
1
√
2[φ1 + iφ2],
ψ†
=
1
√
2[φ1 −iφ2].
(7.16)
This transformation can be pictured by an Argand diagram with φ1
along the real axis and φ2 along the imaginary axis [Fig. 7.3(a)]. Using
the new ﬁelds we obtain a new Lagrangian
L = ∂µψ†∂µψ −m2ψ†ψ −g(ψ†ψ)2.
(7.17)
This is known as a complex scalar ﬁeld theory. Although we originally
made it up from two sorts of ﬁeld, we can imagine that it describes
one sort of complex-valued ﬁeld ψ. This new ﬁeld is constructed from
a bit of φ1 and a bit of φ2 and retains two degrees of freedom. The
new Lagrangian (eqn 7.16) is invariant with respect to rotations in the
complex plane [Fig. 7.3(b)]
ψ →ψeiα,
ψ† →e−iαψ†,
(7.18)
which expresses a U(1) symmetry.9
9U(1) is the one-dimensional group of
unitary transformations.
Mathemati-
cally we note that SO(2) is isomorphic
to U(1) (see Fig. 7.4), so nothing has
really changed at a fundamental level.
Real
φ1
√
2
Imaginary
φ2
√
2
ψ
ψ†
eiα ψ
e−iα ψ†
α
α
Real
φ1
√
2
Imaginary
φ2
√
2
ψ
ψ†
(a)
(b)
Fig. 7.3
The complex scalar ﬁeld on
an Argand diagram.
SO(2)
U(1)
rotate by θ
Re ψ
Im ψ
θ
Fig. 7.4
The group SO(2), describ-
ing rotations in two-dimensions, is iso-
morphic to U(1), the one-dimensional
group of unitary transformations de-
scribed by the phase of the complex
number ψ = |ψ|eiθ.
Example 7.3
The ψ†ψφ theory: Finally, as a rather more complicated example, we consider a
theory with three types of particle. We add together Lagrangians for the complex
scalar ﬁeld theory (with mass m) and for the scalar ﬁeld theory (with mass µ) and also
arrange for the φ and ψ particles to interact via a potential gψ†ψφ. The Lagrangian
is
L
=
∂µψ†∂µψ −m2ψ†ψ
+ 1
2(∂µφ)2 −1
2µ2φ2 −gψ†ψφ.
(7.19)
As we will see a little later, this theory looks a lot like quantum electrodynamics!
The ﬁelds in this chapter have all been classical and although we’ve
claimed that particles are the excitations of these ﬁelds we have yet to
see how they arise from quantum mechanical versions of the ﬁelds. This
is the subject of the next part of the book.

Exercises
69
Chapter summary
• Lagrangians are deduced for a massless scalar ﬁeld, massive scalar
ﬁeld, the inclusion of a source, two scalar ﬁelds and a complex ﬁeld.
• For example, the Lagrangian for a massive scalar ﬁeld coupled to
a source is
L = 1
2[∂µφ(x)]2 −1
2m2[φ(x)]2 + J(x)φ(x).
• The Euler–Lagrange equation can be used to deduce an equation
of motion.
• For example, the equation of motion for a massive scalar ﬁeld cou-
pled to a source comes out to be
(∂2 + m2)φ(x) = J(x).
• The complex scalar ﬁeld ψ =
1
√
2(φ1 + iφ2) possesses a U(1) sym-
metry.
Exercises
(7.1) For the Lagrangian L given by
L = 1
2∂µφ∂µφ −1
2m2φ2 −
∞
X
n=1
λnφ2n+2,
(7.20)
show that the equation of motion is given by
(∂2 + m2)φ +
∞
X
n=1
λn(2n + 2)φ2n+1 = 0.
(7.21)
(7.2) Consider a massive scalar ﬁeld φ(x) coupled to
a source J(x), described by the Lagrangian of
eqn 7.10. Show that the equations of motion are
those of eqn 7.11.
(7.3) Show that the equations of motion following from
the Lagrangian in eqn 7.14 are the coupled equa-
tions
∂µ∂µφ1 + m2φ1 + 4gφ1(φ2
1 + φ2
2)
=
0, (7.22)
∂µ∂µφ2 + m2φ2 + 4gφ2(φ2
1 + φ2
2)
=
0. (7.23)
(7.4) Show that eqn 7.7 is equivalent to
L = 1
2
˙φ2 −1
2(∇φ)2 −1
2m2φ2.
(7.24)
Show that
π = ∂L
∂˙φ
= ˙φ
(7.25)
and
H = 1
2
˙φ2 + 1
2(∇φ)2 + 1
2m2φ2.
(7.26)
Deﬁne the quantity
Πµ =
∂L
∂(∂µφ),
(7.27)
and show that Πµ = ∂µφ and Π0 = π = ˙φ.


Part III
The need for quantum
ﬁelds
In this part we introduce quantum ﬁelds, how to construct quantum ﬁeld
theories, and look at the transformations that can be applied to them.
Understanding the underlying symmetries of a quantum ﬁeld turns out
to be crucial for exploring the properties of quantum ﬁeld theory. This
part is structured as follows:
• To study quantum ﬁelds, we must ﬁrst describe how to include
time dependence, which is covered in Chapter 8. In doing this,
we inadvertently stumble on a reason why single-particle quantum
mechanics is insuﬃcient for fully describing the world in which we
live.
• In Chapter 9 we discuss how to describe the response of quantum
mechanical states to transformations: translations, rotations and
Lorentz boosts.
• This sets up the treatment of Noether’s theorem in Chapter 10,
demonstrating the fundamental relationship between symmetry
and conservation laws.
• We are then ready in Chapter 11 to plunge into the deep waters of
constructing quantum ﬁeld theories out of classical ﬁeld theories, a
manufacturing process termed canonical quantization. We give an
extended example of this process for complex scalar ﬁeld theory in
Chapter 12
• These ideas are extended to multicomponent ﬁelds in Chapter 13
and we work this through for the case of massive electromagnetism,
thereby studying a spin-1 vector ﬁeld.
• Transformations so far have been global. When examining local
transformations we ﬁnd ourselves introducing gauge theory and
this is covered in Chapter 14.
• In Chapter 15 we cover discrete symmetries: C, P and T. We look
at the connection between group theory and the symmetries we
have been studying in Part III.

8
The passage of time
8.1 Schr¨odinger’s picture and the
time-evolution operator
72
8.2 The Heisenberg picture
74
8.3 The
death
of
single-particle
quantum mechanics
75
8.4 Old quantum theory is dead;
long live ﬁelds!
76
Chapter summary
77
Exercises
78
The innocent and the beautiful
Have no enemy but time
W. B. Yeats (1865–1939)
He that will not apply new remedies must expect new evils;
for time is the great innovator
Francis Bacon (1561–1626)
There is more than one way of putting time into quantum mechanics.
In this chapter we’ll discuss Heisenberg’s version of quantum theory in
which all the time dependence is located in the operators, while the
wave functions stay ﬁxed and unchanging. This contrasts with the more
familiar version that Schr¨odinger formulated in which the wave func-
tions carry the time dependence, the formulation that is taught in most
introductory courses. Far from being a dry exercise in formalism, this
will allow us to kill single-particle quantum mechanics1 stone-dead and
1Single-particle quantum mechanics, as
the name suggests, describes the quan-
tum behaviour of a single particle con-
ﬁned in some potential. As we will de-
scribe, even in such a starting situation
it is possible to have to worry about
the appearance of particle–antiparticle
pairs, and thus the assumption that
‘you are just dealing with a single par-
ticle’ is not as straightforward as you
might think.
guide us in a direction towards quantum ﬁelds.
8.1
Schr¨odinger’s picture and the
time-evolution operator
The most familiar way of doing quantum mechanics is to think about
time-dependent wave functions. The manner in which the wave function
changes with time is given by the Schr¨odinger2 equation
2This is of course named after Austrian
physicist
Erwin
Schr¨odinger
(1887–
1961).
i∂ψ(x, t)
∂t
= ˆHψ(x, t),
(8.1)
which is an equation of motion for the wave function. Dynamic variables
like momentum and position are accessed with operators ˆp = −i∇and
ˆx = x respectively, which act on the wave function. This way of doing
quantum mechanics is known as the Schr¨odinger picture and is com-
pletely diﬀerent from classical mechanics, where the dynamic variables
depend on time [e.g. p(t), x(t), etc.] and are themselves described by
equations of motion. The root of this diﬀerence is that time itself is
a funny thing in quantum mechanics. There is no operator that tells
us when some event occurred; instead time is a parameter that in the
Schr¨odinger picture is located purely in our wave function ψ(x, t).
Although no operator tells us the time, there is an operator ˆU(t2, t1)
that can evolve a particle forward through time, from time t1 to t2. Thus

8.1
Schr¨odinger’s picture and the time-evolution operator
73
we can write
ψ(t2) = ˆU(t2, t1)ψ(t1),
(8.2)
and we will call ˆU(t2, t1) a time-evolution operator. Notice that the
time-evolution operator has two slots in which times can be inserted:
the start-time t1 and the stop-time t2.
t1
t2
t3
t1
t2
time
time
ˆU(t2, t1)
ˆU(t1, t2)
ˆU(t2, t1)
ˆU(t3, t2)
ˆU(t3, t1)
(b)
(a)
Fig. 8.1
(a) The composition law:
ˆU(t3, t2) ˆU(t2, t1) = ˆU(t3, t1). (b) The
inverse of a time-evolution operator is
equivalent to reversing the direction of
time: ˆU(t1, t2) = ˆU−1(t2, t1).
The time-evolution operator has ﬁve very useful properties:
(1) ˆU(t1, t1) = 1.
If the two times are the same, then nothing can happen and ˆU is
just the identity operator.
(2) ˆU(t3, t2) ˆU(t2, t1) = ˆU(t3, t1).
This is the composition law and shows that we can build up a time
translation by multiplying a set of smaller translations that do the
same evolution, but in steps [see Fig. 8.1(a)].
(3) i d
dt2
ˆU(t2, t1) = ˆH ˆU(t2, t1).
The proof of this equation is simple. Diﬀerentiating eqn 8.2 with
respect to t2 gives
dψ(t2)
dt2
= d ˆU(t2, t1)
dt2
ψ(t1),
(8.3)
but eqn 8.1 gives
idψ(t2)
dt2
= ˆHψ(t2) = ˆH ˆU(t2, t1)ψ(t1),
(8.4)
and the result follows. This shows that the time-evolution operator
itself obeys the Schr¨odinger equation.
(4) ˆU(t1, t2) = ˆU −1(t2, t1),
so by taking the inverse of a time-evolution operator, one can turn
back time [see Fig. 8.1(b)].
(5) ˆU †(t2, t1) ˆU(t2, t1) = 1,
i.e. the time-evolution operator is unitary.
The proof is short.
When t2 = t1, the result is trivial. If we vary t2 away from t1,
then ˆU †(t2, t1) ˆU(t2, t1) could change, but since3
3Note that in deriving eqn 8.5 we are
using property 3 of the time evolution
operator in the form
d ˆU
dt =
ˆH ˆU
i
,
d ˆU†
dt
= −
ˆU† ˆH
i
.
d
dt2
h
ˆU †(t2, t1) ˆU(t2, t1)
i
=
d ˆU †
dt2
U + U † d ˆU
dt2
=
−
ˆU † ˆH ˆU
i
+
ˆU † ˆH ˆU
i
= 0, (8.5)
it won’t. QED. One consequence of this result is that ˆU †(t2, t1) =
ˆU −1(t2, t1), that is, the adjoint is the inverse.
Property number 3 allows us to write an explicit expression for ˆU(t2, t1),
since integrating this equation gives4
4This exponential expression for an op-
erator is actually shorthand for an ex-
pansion:
e
ˆ
A = 1+ ˆ
A+ 1
2!
ˆ
A ˆ
A+ 1
3!
ˆ
A ˆ
A ˆ
A+. . . (8.6)
We will meet many such exponential
expressions.
ˆU(t2, t1) = e−i ˆ
H(t2−t1).
(8.7)

74
The passage of time
8.2
The Heisenberg picture
The Schr¨odinger picture puts all the time-dependence into the wave
functions, but is that the best way to look at things?
Consider the
expectation value of an operator
⟨ˆO(t)⟩= ⟨ψ(t)| ˆO|ψ(t)⟩.
(8.8)
This tells us the average value that we’ll get if we make a measurement
of the quantity corresponding to the operator ˆO at a time t. However we
construct our quantum mechanics, we must end up with this equation
for expectation values to guarantee that we get the same prediction for
the result of any measurement.5
5Another example of a quantity we
want to agree on is the amplitude for
a system being in state |φ(t2)⟩at a
time t2, having been prepared in a state
|ψ(t1)⟩at an earlier time t1, which is
given by ⟨φ(t2)|ψ(t1)⟩.
(Strictly we
should only insist on the probability be-
ing the same, so we could allow a dif-
ference in phase.)
We can also formulate things as follows by only worrying about the
wave function at t = 0.
After all, if we know ψ(0), we can always
calculate ψ(t) using
ψ(t) = ˆU(t, 0)ψ(0) = e−i ˆ
Htψ(0).
(8.9)
Applying this to the expectation value in eqn 8.8, we can write
⟨ψ(t)| ˆO|ψ(t)⟩= ⟨ψ(0)| ˆU †(t, 0) ˆO ˆU(t, 0)|ψ(0)⟩.
(8.10)
We can interpret this equation in two ways. In the Schr¨odinger picture
we think of time-independent operators ˆOS ≡ˆO and time-dependent
wave function states |ψS(t)⟩≡ˆU(t, 0)|ψ(0)⟩, so that
⟨ψ(0)| ˆU †(t, 0)
h
ˆO
i
ˆU(t, 0)|ψ(0)⟩= ⟨ψS(t)| ˆOS|ψS(t)⟩,
(8.11)
where the ‘S’ subscripts stand for ‘Schr¨odinger picture’. To work out how
the states evolve with time, we use the Schr¨odinger equation, eqn 8.1.
However, we can also move the square brackets in eqn 8.11, so that it
reads
⟨ψ(0)|
h
ˆU †(t, 0) ˆO ˆU(t, 0)
i
|ψ(0)⟩= ⟨ψH| ˆOH(t)|ψH⟩,
(8.12)
where the ‘H’ stands for the Heisenberg picture. We have made the
Werner Heisenberg (1901–1976)
wave functions ψH ≡ψ(0) time-independent (essentially by freezing the
wave functions at their t = 0 values) and all the time dependence is now
formally located in the operators ˆOH(t) which are given by
ˆOH(t) ≡ˆU †(t, 0) ˆOS ˆU(t, 0),
(8.13)
a situation which is analogous to the dynamical variables in classical me-
chanics. The time dependence of ˆOH(t) is simply given by diﬀerentiating
eqn 8.13, so that6
6Note that we are once again using
property 3 of the time evolution opera-
tor in the form
d ˆU
dt =
ˆH ˆU
i
,
d ˆU†
dt
= −
ˆU† ˆH
i
.
d ˆOH(t)
dt
= d ˆU †
dt
ˆOS ˆU + ˆU † ˆOS
d ˆU
dt = 1
i (−ˆU † ˆH ˆOS ˆU + ˆU † ˆOS ˆH ˆU), (8.14)
and hence we arrive7 at the result
7We do this using eqn 8.13 and the fact
that ˆU and ˆH commute.
d ˆOH(t)
dt
= 1
iℏ[ ˆOH(t), ˆH],
(8.15)
which is known as the Heisenberg equation of motion.

8.3
The death of single-particle quantum mechanics
75
8.3
The death of single-particle quantum
mechanics
The time-evolution operator gives us enough ammunition to kill oﬀ
single-particle quantum mechanics forever. We ask the question: ‘what
is the amplitude for a particle to travel outside its forward light-cone?’
(see Fig. 8.2), or in other words: ‘what is the amplitude for a particle
at the origin in spacetime (x = 0, t = 0) to travel to position x at time
t which would require it to travel faster than light?’ Faster than light
means |x| > t and the interval (x −0, t −0) is space-like.8 If the ampli-
8This means that |x| > ct in unnatural
units.
Our convention is that if x =
(t, x) then x2 = t2 −(x1)2 −(x2)2 −
(x3)2 and so the interval is space-like if
x2 < 0.
tude is nonzero then there will be a nonzero probability for a particle to
be found outside its forward light-cone. This is unacceptable and would
spell the death of quantum theory as we’ve known it so far.
t
x2
x1
timelike
spacelike
origin
(x1)2 + (x2)2 + (x3)2 = t2
Fig. 8.2
A particle shouldn’t be ob-
served outside its forward light-cone. If
it starts at the origin, it should only be
able to make time-like journeys.
To answer the question we are going to evaluate the expression
A = ⟨x|e−i ˆ
Ht|x = 0⟩,
(8.16)
for the space-like interval. We’ll work with a basis of momentum states
ˆH|p⟩= Ep|p⟩where the particle has a relativistic dispersion Ep =
p
p2 + m2. We’ll also need the transformation between position and
momentum bases:9 ⟨x|p⟩=
1
(2π)3/2 eip·x.
9This assumes a particular normaliza-
tion. For our current purpose, where all
we will do is to show a particular quan-
tity is nonzero, the value of the normal-
ization constant is irrelevant.
We start by inserting a resolution of the identity10
10i.e. since we have a complete set of
states then 1 =
R
d3p |p⟩⟨p|.
⟨x|e−i ˆ
Ht|x = 0⟩
=
Z
d3p ⟨x|e−i ˆ
Ht|p⟩⟨p|x = 0⟩
=
Z
d3p ⟨x|p⟩e−iEpt
1
(2π)3/2 e−ip·0
=
Z
d3p
1
(2π)3 eip·xe−iEpt.
(8.17)
This has been fairly straightforward so far.
Now we need to do the
integral.
Example 8.1
The technique for doing an integral like this is very useful in quantum ﬁeld theory,
so we’ll go through it in detail. We start by converting to spherical polars
A
=
Z 2π
0
dφ
Z ∞
0
d|p|
(2π)3 |p|2
Z 1
−1
d(cos θ) ei|p||x| cos θe−iEpt
=
1
(2π)2
1
i|x|
Z ∞
0
d|p| |p|
“
ei|p||x| −e−i|p||x|”
e−iEpt
=
−i
(2π)2|x|
Z ∞
−∞
d|p| |p|ei|p||x|e−it√
|p|2+m2.
(8.18)
To do this integral we need to resort to complex analysis.11 The integration path
11See Appendix B for a primer.
is shown in Fig. 8.3(a). There are cuts on the imaginary axis starting at ±im and
heading oﬀto ±∞. We can deform the contour into a large semicircle in the upper
half-plane that goes down the upper cut on the left side and up the cut on the right
[Fig. 8.3(b)].

76
The passage of time
The term ei|p||x| decreases exponentially as we head oﬀto large imaginary values
of |p| because we have |x| > 0. The term e−it√
|p|2+m2 also decreases exponentially
as you head up the left side of the cut but increases as you go up on the right side.
(This is because Im
“p
|p|2 + m2
”
< 0 on the left side and Im
“p
|p|2 + m2
”
> 0
on the right side.) The increase on the right side isn’t a problem since it’s multiplied
by ei|p||x| and for our condition that |x| > t the product e−i|p||x|e−it√
|p|2+m2
decreases. Jordan’s lemma therefore tells us that as we make the half-circle large
it doesn’t contribute to the integral, therefore, and we only have to worry about
the part going up and down the cut. This allows us to deform the integral further
[Fig. 8.3(c)]. We now substitute |p| = iz and the integral becomes
A
=
−i
(2π)2|x|
Z ∞
m
d(iz) ize−z|x|
„
et√
z2−m2 −e−t√
z2−m2«
=
i
2π2|x| e−m|x|
Z ∞
m
dz ze−(z−m)|x| sinh
“
t
p
z2 −m2
”
.
(8.19)
The integrand is positive, deﬁnite and the integral is nonzero. This means that |A|2
yields a nonzero number, heralding doom for the single-particle picture.
im
−im
Z ∞
−∞· · ·
im
−im
im
−im
Z im
∞left-side +
Z ∞
im right-side
Fig. 8.3 The contour integration.
We’ve discovered that the probability of ﬁnding a particle outside its
forward light-cone is proportional to e−m|x|. Although this is small for
large |x| it’s certainly unacceptable that it exists at all. We are unable
to reconcile single-particle quantum mechanics and special relativity.
Single-particle quantum mechanics is dead!
There is another way of understanding why single-particle quantum
mechanics fails when faced with special relativity. Imagine squeezing
a particle of mass m into a box with side smaller than the particle’s
Compton wavelength.12 The uncertainty in position ∆x ≪λ so ∆p ≫
12The Compton wavelength λ of a par-
ticle is given by λ = h/mc, i.e. h/m in
units where c = 1.
h/λ = m and so the energy of the particle is now much greater than m.
This is enough, according to special relativity, to lead to pair production
(i.e. the creation of particle–antiparticle pairs). Thus, on average, the
box has more than one particle in it. Single-particle quantum mechanics
can never describe this situation, by deﬁnition!
8.4
Old quantum theory is dead; long live
ﬁelds!
A desperate disease requires a dangerous remedy
Guy Fawkes (1570–1606)13
13The same sentiment was also ex-
pressed by Hippocrates and by Shake-
speare (in Hamlet).
What are we to do instead? We resort to ﬁelds. In classical mechanics
we assume that an observer has access to all possible measurements of
a system. Special relativity teaches us that no one person can measure
everything. If information would have to travel faster than c for a mea-
surement to be made it is impossible to make such a measurement. In
fact, you make a measurement at a single point in spacetime where you,
yourself, are localized. You can’t then make a measurement at a point
that has a space-like separation from you. More simply, an observer can’t
make a measurements here on Earth and then make a measurement of an
event in Andromeda14 that occurred one second later. The Andromeda
14The Andromeda galaxy is some 2.5
million light-years from Earth.

8.4
Old quantum theory is dead; long live ﬁelds!
77
information would have to travel to the Earthbound observer faster than
c, which is forbidden by special relativity.
Have we seen this limitation elsewhere in physics?
Absolutely.
In
electromagnetism we calculate and measure the value of an electric or
magnetic vector at a spacetime point x. We call these E(x) and B(x)
respectively and they are examples of ﬁelds. They are machines that take
spacetime coordinates xµ = (t, x) and output the value of the electric
or magnetic vector at that point. The ﬁeld machines are deﬁned locally,
and so they only tell us about the electromagnetic vectors at the point
x. We know that we can’t design an experiment that measures E(x) for
x = (now, here) and for x = (now, Andromeda).15
15Of course, if we solve some equa-
tions of motion for the ﬁelds we may
be able to work out what’s happening
elsewhere.
We need to ﬁx up this idea of a locally deﬁned ﬁeld to make it quantum
mechanical. In quantum mechanics we have operators that correspond
to observables. It seems, therefore, that we will need operator-valued
ﬁelds if we’re to reconcile quantum mechanics and special relativity.
We know two things about these ﬁeld operators already: (i) crucially,
they must not allow information to travel faster than c; (ii) they must
be Hermitian if they are to correspond to observables.16 We can state
16It is not essential that a general ﬁeld
should necessarily correspond to an ob-
servable. Indeed we will meet many ex-
amples where this is not the case. For
electric and magnetic ﬁelds we do seek
Hermitian ﬁelds.
the former property quantum mechanically. If we say that the operator
ˆO1 corresponds to an observable measured at spacetime point x and
ˆO2 corresponds to an observable measured at y, then if x and y have a
space-like separation [(x −y)2 < 0] then the operators must commute:
[ ˆO(x), ˆO(y)] = 0,
(8.20)
that is, the result of one measurement won’t aﬀect the result of the other.
How could they if the separation between the experiments is space-like?
To make quantum ﬁelds we need to ﬁnd operators that are deﬁned at
positions in spacetime. We’ll call these ˆφ(x). We hope that we can build
everything from these locally deﬁned ﬁelds.17 In the next few chapters
17This turns out to be the case!
we will continue our search for these objects.
Chapter summary
• The time-evolution operator is given by ˆU(t2, t1) = e−i ˆ
H(t2−t1),
and in the Schr¨odinger picture acts to time-evolve states.
The
operators ˆOS are time-independent.
• In the Heisenberg picture states don’t evolve in time, but the oper-
ators do: ˆOH(t) = ˆU †(t, 0) ˆOS ˆU(t, 0). Operators obey the Heisen-
berg equation of motion d ˆ
OH(t)
dt
= 1
iℏ[ ˆOH(t), ˆH].
• Squeezing a particle of mass m into a region smaller that its
Compton wavelength h/m can lead to pair production, invalidating
single-particle quantum mechanics.
• To ﬁx this, we need operator-valued ﬁelds.

78
The passage of time
Exercises
(8.1) Show that the form of the time evolution operator
ˆU(t2, t1) = exp[−i ˆH(t2 −t1)] (as given in eqn 8.7)
exhibits properties 1–5 in Section 8.1.
(8.2) For the Hamiltonian
ˆH =
X
k
Ekˆa†
kˆak,
(8.21)
use the Heisenberg equation of motion to show that
the time dependence of the operator ˆa†
k is given by
ˆa†
k(t) = ˆa†
k(0) eiEkt/ℏ,
(8.22)
and ﬁnd a similar expression for ˆak(t).
(8.3) For the Hamiltonian in the previous problem ﬁnd
an expression for time-dependence of the operator
ˆ
X = Xℓmˆa†
ℓˆam.
(8.4) A spin- 1
2 particle is in a magnetic ﬁeld aligned along
the y-direction. The Hamiltonian can be written as
ˆH = ω ˆSy where ω is a constant (proportional to
the magnetic ﬁeld). Use the Heisenberg equation of
motion to show that
d ˆSz
H
dt
=
−ω ˆSx
H,
(8.23)
d ˆSx
H
dt
=
ω ˆSz
H,
(8.24)
and give a physical interpretation of this result.

9
Quantum mechanical
transformations
9.1 Translations in spacetime
79
9.2 Rotations
82
9.3 Representations
of
transfor-
mations
83
9.4 Transformations of quantum
ﬁelds
85
9.5 Lorentz transformations
86
Chapter summary
88
Exercises
88
When Gregor Samsar awoke one morning from uneasy
dreams he found himself transformed in his bed into a gi-
ant insect.
Frank Kafka (1883–1924), The Metamorphosis
Quantum mechanical states transform (though usually less dramatically
than the transformation that aﬀected Gregor Samsar) when you trans-
late or rotate coordinates. In this chapter we are going to look at how
quantum states, operators and quantum ﬁelds will transform when sub-
jected to translations, rotations or Lorentz boosts. Where do we start?
One thing we certainly know how to do is transform coordinates in
spacetime. For example, we can translate a position three-vector x by a
three-vector a, which is achieved simply by adding a to the vector. To
put things formally, we act on x with a translation operator T(a) and
transform it into x′ given by
x′ = T(a)x = x + a.
(9.1)
We also know how to rotate a three-vector through an angle θ about the
z-axis, using
x′ = R(θ)x =


cos θ
−sin θ
0
sin θ
cos θ
0
0
0
1




x1
x2
x3

.
(9.2)
We need to ﬁnd operators that can transform quantum states, which
live in Hilbert space (rather than the Euclidean space of three-vectors
or Minkowski space of four-vectors).
Here we investigate what form
these operators take.
Remark about notation:
We will be careful to distinguish be-
tween, on one hand, operators T(a)
and R(θ) which act on single vectors
(like x) and, on the other hand, quan-
tum mechanical transformation oper-
ators like
ˆU(a) and
ˆU(θ) which act
on quantum states (like |x⟩) and also
(as we shall see) on operators (like
ˆx) and on quantum ﬁelds (like ˆφ(x)).
These quantum mechanical transfor-
mation operators will turn out to be
unitary, so we shall use the symbol ˆU
for all of them.
9.1
Translations in spacetime
Let’s start with looking at the properties of single-particle quantum
systems when we translate them in space. Suppose we’re examining a
particle localized at a coordinate and we want to translate it to another
point in space. We need a quantum mechanical operator ˆU that takes a
state localized at x and transforms it to a position x+a. The translation
operator is written as
ˆU(a)|x⟩= |x + a⟩.
(9.3)

80
Quantum mechanical transformations
We will treat this as an active transformation where we move a particle
to a new position (instead of changing the coordinates under the particle,
which is known as a passive transformation)1 (see Fig. 9.1).
1The transformation in eqn 9.2 is a
right-handed, active rotation or a left-
handed, passive rotation.
x
y
x
y
x′
y′
x′
y′
(b)
(a)
passive rotation
active rotation
Fig. 9.1 (a) A passive rotation acts on
the coordinates. (b) An active rotation
acts on the object.
Next we examine the properties we want in a translation operator.
Translating the particle shouldn’t change the probability density. Thus
⟨ψ(x)|ψ(x)⟩= ⟨ψ(x + a)|ψ(x + a)⟩= ⟨ψ(x)| ˆU †(a) ˆU(a)|ψ(x)⟩, (9.4)
which means that we may write
ˆU †(a) ˆU(a) = 1,
(9.5)
that is, the translation operator is unitary.2
2A unitary matrix is deﬁned as one for
which U† = U−1.
Note that some
transformations in quantum mechanics
must be performed by an antiunitary
operator as discussed in Chapter 15.
We know that a translation through a distance a followed by one
through a distance b can be represented by a single translation through
a distance a + b. We want our operators to have this property too, so
we add to our wish list
ˆU(a) ˆU(b) = ˆU(a + b).
(9.6)
We should also have the trivial property that the transformation can do
nothing to the particle, that is ˆU(0) = 1.
To recap then, we have identiﬁed three rules for our translation oper-
ator:
• ˆU(a) ˆU †(a) = 1 ( ˆU is unitary),
• ˆU(a) ˆU(b) = ˆU(a + b) (composition rule),
• ˆU(0) = 1 (a zero translation does nothing).
Notice that these rules are telling us that our transformations should
form a group.3 It is a rather special group because each element depends
3A group is a set G together with
an operation • which connects two el-
ements of the set (let’s say a, b ∈G)
and produces another element of the set
(written a • b). This has to satisfy four
conditions:
(1) ∀a, b ∈G, a • b ∈G (closure).
The symbol ∀means ‘for all’.
The ﬁrst property written in full
is ‘For all members of the group
a and b, the combination a • b is
also a member of the group’.
(2) ∀a, b, c ∈G, a•(b•c) = (a•b)•c
(associativity).
(3) ∃e ∈G such that ∀a ∈G, a•e =
e • a = a (identity).
(4) ∀a ∈G, ∃a−1 ∈G such that a •
a−1 = a−1 • a = e (inverse).
Note that one of the conditions was
not that a • b = b • a.
If that condi-
tion (commutativity) holds, then one
has a special type of group called an
abelian group, but many groups (in-
cluding ones which are important in
quantum ﬁeld theory) are non-abelian.
on a vector a, and thus the group is continuous, diﬀerentiable and has
an inﬁnite number of elements. Such a group is called a Lie group.4
4Named after the Norwegian mathe-
matician Sophus Lie (1842–1899).
Note that Lie is pronounced ‘lee’.
Example 9.1
Let’s check that ˆU(a) has the required conditions of a group.
A group has four
properties: the existence of an identity [ ˆU(0) = 1], closure (we can make any element
of the group by multiplying two other elements of the group), associativity [ ˆU(a +
b) ˆU(c) = ˆU(a) ˆU(b + c)] and the existence of an inverse for each element of the
group. We also know that the inverse of a translation ˆT −1(a), is just a translation
through −a. Putting this together with the unitarity of the operator, we can write
that ˆU−1(a) = ˆU†(a) = ˆU(−a).
Let’s now try out the new operator. It’s supposed to move a state though a and
we can use the position operator ˆx to check where it is:
ˆU(a)|x⟩
=
|x + a⟩
ˆxU(a)|x⟩
=
ˆx|x + a⟩= (x + a)|x + a⟩
U†(a) ˆxU(a)|x⟩
=
(x + a)U†(a)|x + a⟩= (x + a)|x⟩,
(9.7)
or comparing the eﬀects of operators on the state |x⟩, we see that
ˆU†(a) ˆx ˆU(a)
=
(ˆx + a).
(9.8)
That is, the space translation operator can be thought of as a transformation of an
operator like ˆx instead of a transformation of a wave function like |x⟩.

9.1
Translations in spacetime
81
Example 9.2
Often translating will have no eﬀect on a property that we determine using an oper-
ator ˆO. In that case the property probed by that operator is an invariant.5 In that
5See Chapter 10.
case we would have
⟨ψ(x)| ˆO|ψ(x)⟩= ⟨ψ(x)| ˆU−1(a) ˆO ˆU(a)|ψ(x)⟩.
(9.9)
Pulling out the operators we see that the condition for an invariant is
ˆU−1(a) ˆO ˆU(a) = ˆO.
(9.10)
Act on the left with ˆU and thus invariance implies that ˆO ˆU = ˆU ˆO, or [ ˆO, ˆU] = 0.
Recall from eqn 8.15 that operators that commute with the Hamiltonian operator ˆH
represent constants of the motion, i.e. quantities that are conserved.
Can we come up with an explicit expression for a translation operator
that acts on quantum states? We will do this for the special case of a
position wave function ψ(x) = ⟨x|p⟩(the result will turn out to also allow
us to transform general quantum mechanical states). We increment the
point at which we evaluate ψ(x) by an inﬁnitesimal amount δa in the
x-direction, for which we may write
ψ(x + δa) = ψ(x) + dψ(x)
dx
δa + . . . .
(9.11)
Remembering that the momentum operator ˆp = −i d
dx, we have
ψ(x + δa) = (1 + iˆpδa) ψ(x).
(9.12)
We say that the momentum operator ˆp is the generator for the space
translation. To make a translation through a distance a we can translate
through δa a large number N times, such that
ψ(x + a)
=
lim
N→∞(1 + iˆpδa)N ψ(x)
=
eiˆpaψ(x).
(9.13)
This gives us a space evolution operator. However, this is not quite the
ˆU(a) that we’re trying to ﬁnd! We want to translate the entire quantum
state through a distance a not study how it evolves over a distance a!
The translation operator is, therefore, ˆU(a) = e−iˆp·a.
Example 9.3
Acting on a momentum state with our operator we obtain
ˆU(a)|q⟩
=
e−iˆp·a|q⟩
=
e−iq·a|q⟩.
(9.14)
Projecting along the coordinate direction gives us a translated wave function
⟨x| ˆU(a)|q⟩= ⟨x|q⟩e−iq·a =
1
√
V
eiq·(x−a).
(9.15)

82
Quantum mechanical transformations
Previously we had a time-evolution operator ˆU(t2,t1) = e−i ˆ
H(t2−t1). We
can see how this relates to the argument in the previous section. Let’s
evolve a system through a time δta. We obtain
ψ(t + δta) = ψ(t) + dψ(t)
dt
δta + . . .
(9.16)
Now we need to remember that ˆH = i d
dt, which gives us
ψ(t + δta) =

1 −i ˆHδta

ψ(t).
(9.17)
Remembering the change of sign required to turn this from a time-
evolution operator6 into a time translation operator we easily obtain
6It might be helpful to view time evo-
lution as passive:
one studies how ψ
changes as coordinate t increases. Time
translation is then active: one moves a
quantum state through time.
ˆU(ta) = ei ˆ
Hta.
(9.18)
We can put the space and time translations together and deﬁne a space-
time translation operator ˆU(a) = eiˆp·a = ei ˆ
Hta−iˆp·a, where we choose
the deﬁnition of the four-momentum operator to be ˆp = ( ˆH, ˆp). We will
return to the consequences of this exponential form for the expression
for ˆU(a) later in the chapter.
Remark about notation:
Spacetime
translations,
rotations,
Lorentz boosts and combinations of
the
above
are
all
elements
of
the
Poincar´e group.
They can all be
represented by unitary operators.
To
keep ourselves from an overload of
brackets and indices, we will simplify
the
notation
by
denoting
all
these
unitary operators by
ˆU and let the
operator’s argument indicate whether
we are dealing with a translation ˆU(a),
rotation ˆU(θ) or Lorentz boost ˆU(φ).
This should not cause confusion as the
context will make it clear which type
of transformation we are dealing with.
9.2
Rotations
Next we ask how to rotate objects. A rotation matrix R(θ) acts on a
vector quantity, such as the momentum of a particle as follows: p′ =
R(θ)p.
We specify the rotation as R(θ), where the direction of the
vector θ is the axis of rotation and its magnitude is the angle.
For
rotations of quantum states, we propose an operator
|p′⟩= ˆU(θ)|p⟩= |R(θ)p⟩.
(9.19)
As before, we require that ˆU(θ) has: (i) unitarity, (ii) an identity element
and (iii) a composition rule. What we’re looking for then is that
• ˆU †(θ) ˆU(θ) = 1,
• ˆU(0) = 1,
• ˆU(θ1) ˆU(θ2) = ˆU(θ12) where R(θ12) = R(θ1)R(θ2),
and, just as before, our operators form a Lie group, called the rotation
group.
Example 9.4
We can prove that the operator deﬁned this way is unitary
ˆU(θ) ˆU†(θ)
=
ˆU(θ)
„Z
d3p |p⟩⟨p|
«
ˆU†(θ)
=
Z
d3p |R(θ)p⟩⟨R(θ)p|
(9.20)

9.3
Representations of transformations
83
and since p′ = R(θ)p and d3p′ = d3p we have
Z
d3p |R(θ)p⟩⟨R(θ)p| =
Z
d3p′ |p′⟩⟨p′| = 1,
(9.21)
as required. Notice that our proof rests on the fact that d3p′ = d3p, which is true
since det R(θ) = 1 for a proper rotation and so the Jacobian is unity.
For the translation case we had
ˆU †(a) ˆx ˆU(a) = ˆx + a,
(9.22)
and we ﬁnd7 an analogous expression for the rotations, i.e.
7The proof for this goes through much
like the version in Example 9.1 and is
left as an exercise.
ˆU †(θ) ˆp ˆU(θ) = R(θ)ˆp.
(9.23)
Thus the momentum operator is transformed in just the same way as
one would rotate a momentum vector.
Finally, we may ﬁnd an explicit expression for the rotation operator
that acts on wave functions. Let’s examine a rotation of a wave function
about the z-axis and expand
ψ(θz + δθz) = ψ(θz) + dψ(θz)
dθz
δθz + . . .
(9.24)
Here we recall that ˆJz = −i d
dθz , from which we see that rotations are
generated by the angular momentum operator. We obtain
ψ(θ + δθz) =

1 + i ˆJzδθz
ψ(θz),
(9.25)
and ﬁnally, repeating N →∞times and changing the sign gives ˆU(θz) =
e−i ˆ
Jzθz, and for a rotation about an arbitrary axis we have
ˆU(θ) = e−iˆJ·θ.
(9.26)
5
5
(a)
(b)
(c)
Fig. 9.2 (a) A spatial rotation sim-
ply rotates the coordinates of the scalar
ﬁelds but does not alter the value of the
scalar. (b) Simply rotating the coordi-
nates of a vector ﬁeld is not a correct
transformation since (c) you have to ro-
tate the vectors at each point in space
as well.
9.3
Representations of transformations
We have seen how to transform spacetime coordinates by translations
and rotations. But spacetime is an empty coordinate system without
ﬁlling it with quantum ﬁelds. So we now have to ask how the quantum
ﬁelds themselves transform. A quantum ﬁeld ˆφ(x) takes a position in
spacetime and returns an operator whose eigenvalues can be a scalar,
a vector (the W ± and Z0 particles are described by vector ﬁelds), a
spinor (the object that describes a spin- 1
2 particle such as an electron),
or a tensor. A spatial rotation simply rotates the coordinates for a scalar
ﬁeld [Fig. 9.2(a)] but for a vector ﬁeld simply rotating the coordinates is
not enough [Fig. 9.2(b)], you also have to rotate the vectors [Fig. 9.2(c)].
For this reason we need to think a bit more deeply about how trans-
formations are represented, and for concreteness let us keep focussing

84
Quantum mechanical transformations
on rotations. Any rotation R(θ) can be represented by a square matrix
D(θ). This takes a very similar form to ˆU(θ) = e−iˆJ·θ, namely
D(θ) = e−iJ·θ.
(9.27)
In this equation, J is a square matrix, a representation8 of the operator
8A representation of a group D is
obtained by mapping each element gi
of the group G into a continuous lin-
ear operator that acts on some vec-
tor space. This mapping preserves the
group product, so that if g1 • g2 = g3,
then D(g1)D(g2) = D(g3).
ˆJ. Equation 9.27 can be rewritten as
Ji = −1
i
∂D(θi)
∂θi

θi=0
.
(9.28)
Example 9.5
(a) Consider rotations about the z-axis. A trivial representation is D(θz) = 1. This
means that
Jz = −1
i
∂D(θz)
∂θz
˛˛˛˛
θz=0
= 0.
(9.29)
By extension Jx = Jy = 0. This representation is appropriate for a scalar ﬁeld.9
9This makes perfect sense because a
scalar can have no angular momentum.
(b) It turns out that the representation of a rotation about the z-axis for a spin- 1
2
particle is
D(θz) =
„ e−iθz/2
0
0
eiθz/2
«
,
(9.30)
and hence
Jz = −1
i
∂D(θz)
∂θz
˛˛˛˛
θz=0
= 1
2
„ 1
0
0
−1
«
,
(9.31)
which we recognise as the operator for the z-component of angular momentum for
a spin- 1
2 particle. This representation of the rotation group will be useful for the
spinor ﬁelds introduced in Chapter 37.
The representation for a rotation of
a ﬁeld with angular momentum quan-
tum number j is frequently given the
symbol D(j)(θ). Thus in this example,
part (a) describes D(0)(θ) = 1, part
(b) describes D(1/2)(θ), and part (c)
describes a representation related to
D(1)(θ) which uses the Cartesian axes
x, y and z as the basis of the spatial
components, rather than the three az-
imuthal quantum numbers. The gen-
eral formulae for the matrix elements
of D(j)(θ) are
[D(j)(θ)]m,m′
=
⟨jm′| ˆU(θ)|jm⟩
=
⟨jm′|e−iˆJ ·θ|jm⟩.
We can then use the standard relations
for angular momentum operators:
ˆJz|jm⟩= m|jm⟩,
ˆJ±|jm⟩=
p
(j∓m)(j+1±m)|j m±1⟩,
ˆJ± = ˆJx ± i ˆJy.
(c) For rotations about the x-axis we have
R(θx) =
0
B
B
@
0
0
0
0
0
1
0
0
0
0
cos θx
−sin θx
0
0
sin θx
cos θx
1
C
C
A ,
(9.32)
so we obtain
Jx = −1
i
∂R(θx)
∂θx
˛˛˛˛
θx=0
= i
0
B
B
@
0
0
0
0
0
0
0
0
0
0
0
−1
0
0
1
0
1
C
C
A .
(9.33)
Repeating for the y- and z-axes yields
Jy = i
0
B
B
@
0
0
0
0
0
0
0
1
0
0
0
0
0
−1
0
0
1
C
C
A
Jz = i
0
B
B
@
0
0
0
0
0
0
−1
0
0
1
0
0
0
0
0
0
1
C
C
A .
(9.34)
The important point about these representations is that they all share
the same underlying algebraic structure as the rotation operator. This
algebra is called a Lie algebra, and can turn up whenever you have a
continuous group.10 The Lie algebra is encoded within the commutation
10This means that there are elements of
the Lie group which are arbitrarily close
to the identity and so an inﬁnitesimal
group element can be written
g(α) = 1 + iαiT i + O(α2),
where T i are the generators of the
group. The Lie algebra is expressed by
the commutator
[T i, T j] = ifijkT k,
where fijk are called structure con-
stants.
For rotations T i = Ji and
fijk = εijk.
relations of the generators. In other words, whether a generator of rota-
tions rotates abstract quantum states in Hilbert space (like ˆJz = −i d
dθz ),
real space vectors (as in the previous example) or complex spinors (as in
Chapter 37) each set of generators should have the same commutation
relations: [Ji, Jj] = iεijkJk.

9.4
Transformations of quantum ﬁelds
85
9.4
Transformations of quantum ﬁelds
We have seen that a transformation associated with a unitary operator
ˆU will turn a locally-deﬁned operator ˆO (such as the position operator)
into ˆU † ˆO ˆU (see eqn 9.8). Now we need to determine how such transfor-
mations aﬀect quantum ﬁelds. We’ll start by examining a scalar ﬁeld:
an operator-valued ﬁeld whose matrix elements are scalars.
Example 9.6
If we translate both a state and an operator by the same vector distance a then
nothing should change. In equations
⟨y|ˆφ(x)|y⟩= ⟨y + a|ˆφ(x + a)|y + a⟩.
(9.35)
Since |y + a⟩= ˆU(a)|y⟩we must have
ˆU(a)ˆφ(x) ˆU†(a) = ˆφ(x + a).
(9.36)
Notice that this looks like ˆU and ˆU† have been bolted on the wrong way round when
compared to the expression
ˆU†(a)ˆxˆU(a) = (ˆx + a),
(9.37)
but these two expressions are talking about diﬀerent things. The position operator
ˆx tells us where a particle is localized, the ﬁeld operator ˆφ(x) acts on a localized
particle at x. There’s no reason that they should transform in the same way.
⟨
|ˆφ(x)|
⟩= 1
⟨
|ˆφ(x) ˆU(a)|
⟩= 0
⟨
| ˆU†(a)ˆφ(x) ˆU(a)|
⟩= 0
at x
at x
at x
x −a
x
x
x + a
x
(c)
(b)
(a)
Fig. 9.3 The ﬁeld operator ˆφ(x) act-
ing on a localized state is pictured as a
sniper’s bullet shooting a duck located
at position x. (a) Dead on target, the
state of a dead duck has perfect overlap
with the state produced by acting ˆφ(x)
on the live duck at x. (b) Translating
the state results in a miss, as does (c)
translating the operator.
The previous example demonstrates that there are two ways of thinking
about translations operators, and we shouldn’t confuse them. They can
be pictured:
• As acting on states: ˆU(a)|x⟩= |x + a⟩moves a locally deﬁned
state from being localized at x to being localized at x + a.
• As acting on locally deﬁned operators:
ˆU †(a)ˆφ(x) ˆU(a) = ˆφ(x −a).
(9.38)
These are not the same as we can see from Fig. 9.3. We can imagine
a ﬁeld operator ˆφ(x) as a sniper’s riﬂe shooting a state at a position x
[Fig. 9.3(a)]. If we translate the state from |x⟩to |x+a⟩, ˆφ(x) misses the
state because we’ve moved the state [Fig. 9.3(b)]. If, on the other hand,
we translate the operator from ˆφ(x) to ˆφ(x−a), we miss again, but this
time because we’ve changed the position we’re aiming at [Fig. 9.3(c)].
Since we’re interested in the ﬁelds, changing the operator will be the
most useful procedure and so the equation to focus on is eqn 9.38.
Example 9.7
The creation and annihilation operators are also actually quantum ﬁeld operators,
albeit in momentum space. That is, the operator ˆa†
m takes a momentum m and
outputs an operator that creates a particle at that position in momentum space.

86
Quantum mechanical transformations
We’ll examine what we get if we transform according to ˆU†(a)ˆa†
m ˆU(a), which is
a translation in spacetime by a vector a. We can try out this translated operator on
a momentum eigenstate |q⟩(assumed diﬀerent to m):
ˆU†(a)ˆa†
m ˆU(a)|q⟩
=
eiˆp·aˆa†
me−iˆp·a|q⟩
=
eiˆp·aˆa†
m|q⟩e−iq·a
=
eiˆp·a|m, q⟩e−iq·a
=
|m, q⟩ei(m+q)·ae−iq·a
=
|m, q⟩eim·a.
(9.39)
So the transformed operator ˆa†
m still creates a state with momentum m, but the
result of the transformation is an additional phase eim·a. We conclude that
ˆU†(a)ˆa†
m ˆU(a) = eim·aˆa†
m.
(9.40)
This discussion has been for scalar ﬁelds, but we must remember that
ﬁelds come in many forms, including vector ﬁelds as discussed in the
following example.
d
R(θ)d
d
R(θ)d
V max
R(θ)V max
(a)
(c)
(b)
(d)
Fig. 9.4 (a) A classical scalar ﬁeld has
a maximum at some point in space re-
moved from the origin by a vector d.
(b) Rotating the distribution with an
operator R(θ) moves the maximum to
a position R(θ)d. (c) A classical vec-
tor ﬁeld also has a maximum value at d.
(d) The transformed ﬁeld has a maxi-
mum at R(θ)d and this vector is ro-
tated to R(θ)V max.
Example 9.8
A classical scalar ﬁeld φ(x) (like the distribution of temperature across a metal block)
has a large maximum φ(d) = φmax at some point in space removed from the origin
by a vector d. Rotating the distribution with an operator R(θ) moves the maximum
to a position R(θ)d. This is embodied in the operator equation ˆU†(θ)ˆφ(x) ˆU(θ) =
ˆφ(R−1(θ)x), which demonstrates that we need to enter the position x = R(θ)d in
the transformed ﬁeld to ﬁnd the maximum [see Fig. 9.4(a) and (b)].
For a classical vector ﬁeld V , such as the velocity distribution in a liquid, let’s
imagine that we have a maximum velocity V (d) = V max. Being a vector, this has
a direction.
A rotation R(θ) now moves the position of the maximum and also
its direction.
The analogous mathematical description is now ˆU†(θ) ˆV (x) ˆU(θ) =
R(θ) ˆV (R−1(θ)x), demonstrating that the transformed ﬁeld has a maximum at
R(θ)d and that this vector is rotated to R(θ)V max [see Fig. 9.4(c) and (d)].
It is therefore important to note that if we transform a ﬁeld in some
way, we generally change the point at which the ﬁeld is evaluated and
also its polarization. We can write a more general rotation of a general
ﬁeld operator ˆΦ(x) as
ˆU †(θ) ˆΦ(x) ˆU(θ) = D(θ) ˆΦ(R−1(θ)x),
(9.41)
where D(θ) is the appropriate representation of the rotation operator,
as discussed in Section 9.3 (e.g. for scalar ﬁelds D(θ) = 1).
9.5
Lorentz transformations
A similar philosophy carries over to the case of Lorentz transformations
of states and quantum ﬁelds.11 Consider a boost of a four-vector in the
11As we are now explicitly considering
spacetime, we will now switch from la-
belling our spatial axes x, y and z and
go back to using 1, 2 and 3, with 0 for
the time axis.

9.5
Lorentz transformations
87
x-direction whose Lorentz transformation is given by x′µ = Λ(β1)µ
νxν,
where
Λ(β1) =




γ1
β1γ1
0
0
β1γ1
γ1
0
0
0
0
1
0
0
0
0
1



.
(9.42)
This transformation connects two inertial frames moving with relative
speed v = cβ1 along x. Using the substitutions γi = cosh φi, γiβi =
sinh φi and tanh φi = βi, where φi is called the rapidity, this matrix
becomes
Λ(φ1) =




cosh φ1
sinh φ1
0
0
sinh φ1
cosh φ1
0
0
0
0
1
0
0
0
0
1



.
(9.43)
We write an operator that acts in Hilbert space:12 ˆU(φ)|p⟩= |Λ(φ)p⟩.
12We will not pursue this operator fur-
ther here.
Unlike linear and angular
momentum the boost three-vector is
not conserved, so we do not use its
eigenvalues to label states.
We also want a generalized matrix form like we had with the rotation
e−iJ·θ. From our experience with rotations we suppose that we can write
a generalized Lorentz transformation matrix as13
13Note the plus sign in the exponent
compared to the rotation case.
D(φ) = eiK·φ,
(9.44)
and the generators of the Lorentz transformations are given by
Ki = 1
i
∂D(φi)
∂φi

φi=0
.
(9.45)
As shown in the exercises, we can ﬁnd the explicit forms for these gen-
erators for the case of vectors by taking D(φi) to be the Λ(φi) matrices
such as that in eqn 9.43 for a boost along x. We therefore require our
quantum ﬁelds to Lorentz transform according to
ˆU †(φ) ˆΦ(x) ˆU(φ) = D(φ) ˆΦ(Λ−1(φ)x).
(9.46)
Finally, we recall that the fundamentally important feature of the
generators of a transformation is their commutation relations.
Upon
working out a set of generators for the Lorentz transformations, as you’re
invited to in the exercises, it comes as a shock to ﬁnd
[K1, K2] = K1K2 −K2K1 = −iJ3.
(9.47)
In other words, the diﬀerence between (i) boosting along x and then
boosting along y and (ii) boosting along y and then boosting along x is
a rotation about z. Mathematically speaking, the fact that [K1, K2] =
−iJ3 implies that the Lie algebra of the Lorentz transformations isn’t
closed and their generators do not form a group. Let’s see what happens
if we examine some other commutators. Pressing on, we ﬁnd

J1, K1
=
0,
(9.48)

J1, K2
=
iK3,
(9.49)

88
Quantum mechanical transformations
along with cyclic permutations, that is to say

Ji, Kj
= iεijkKk. This
implies that the boosts and rotations taken together form a closed Lie
algebra and it is this larger group that is called the Lorentz group.
We may write a general Lorentz transformation combining both boosts
and rotations as
D(θ, φ) = e−i(J·θ−K·φ),
(9.50)
where, as always, the generators J and K are those appropriate for the
object being transformed. If you also include the spacetime translations
in the mix, you end up with an even larger group called the Poincar´e
group.
Chapter summary
• A transformation is associated with a unitary operator ˆU which
acts on states via |x′⟩= ˆU|x⟩.
• A general spacetime translation has ˆU(a) = eiˆp·a.
• A rotation has ˆU(θ) = e−iˆJ·θ.
• A Lorentz transformation has ˆU(φ) = ei ˆ
K·φ.
• Transformations can be represented by a matrix D(a, θ, φ) appro-
priate to the type of ﬁeld. For scalar ﬁelds D = 1.
• The quantum ﬁelds that we seek will have to have the following
transformation properties:
ˆU †(a) ˆΦ(x) ˆU(a) = D(a) ˆΦ(x −a),
ˆU †(θ) ˆΦ(x) ˆU(θ) = D(θ) ˆΦ(R−1(θ)x),
ˆU †(φ) ˆΦ(x) ˆU(φ) = D(φ) ˆΦ(Λ−1(φ)x),
i.e. they obey the translation, rotation and Lorentz transformation
properties of a local operator.
Exercises
(9.1) Deduce that the generators of the translation oper-
ator are given by
ˆp = −1
i
∂ˆU(a)
∂a
˛˛˛˛˛
a=0
.
(9.51)
(9.2) Show that explicit forms of the generators of the
Lorentz group for four-vectors are
K1 = 1
i
∂ˆU(φ1)
∂φ1
˛˛˛˛˛
φ1=0
= −i
0
B
B
@
0
1
0
0
1
0
0
0
0
0
0
0
0
0
0
0
1
C
C
A ,
(9.52)

Exercises
89
and similarly
K2 = −i
0
B
B
@
0
0
1
0
0
0
0
0
1
0
0
0
0
0
0
0
1
C
C
A ,
(9.53)
and
K3 = −i
0
B
B
@
0
0
0
1
0
0
0
0
0
0
0
0
1
0
0
0
1
C
C
A .
(9.54)
(9.3) Show that an inﬁnitesimal boost by vj along the
xj-axis is given by the Lorentz transformation
Λµ
ν =
0
B
B
@
1
v1
v2
v3
v1
1
0
0
v2
0
1
0
v3
0
0
1
1
C
C
A .
(9.55)
Show also that an inﬁnitesimal rotation by θj about
xj is given by
Λµ
ν =
0
B
B
@
1
0
0
0
0
1
θ3
−θ2
0
−θ3
1
θ1
0
θ2
−θ1
1
1
C
C
A .
(9.56)
Hence show that a general inﬁnitesimal Lorentz
transformation can be written x′µ = Λµ
νxν where
Λ = 1 + ω where
ωµ
ν =
0
B
B
@
0
v1
v2
v3
v1
0
θ3
−θ2
v2
−θ3
0
θ1
v3
θ2
−θ1
0
1
C
C
A .
(9.57)
Show that ωµν = ωµ
λgλν and ωµν = gµλωλ
ν are
antisymmetric. An antisymmetric 4×4 matrix has
six independent parameters. This makes sense as
we are encoding three rotations and three boosts.
Show that the relationships between θi, vi and ωij
are
θi = −1
2εijkωjk and vi = ω0i.
(9.58)
(9.4) A transformation of the Poincar´e group combines
translations (by vector aµ), rotations and Lorentz
boosts and can be written x′µ = aµ + Λµ
νxν. An
inﬁnitesimal transformation of the Poincar´e group
is thus x′µ = xµ + aµ + ωµ
νxν, where ωµ
ν is given
in the previous problem. Therefore a function f(x)
transforms to
f(x′)
=
f(x + a + ωx)
(9.59)
=
f(x) + aµ∂µf(x) + ωµ
νxν∂µf(x).
Use the antisymmetry of ωµν to write
f(x′) = [1 + aµ∂µ −1
2ωµν(xµ∂ν −xν∂ν)]f(x),
(9.60)
and hence
f(x′) = [1 −iaµpµ + i
2ωµνM µν]f(x),
(9.61)
where pµ = −i∂µ and M µν = −i(xµ∂ν −xν∂µ) are
generators of the Poincar´e group. Note that M µν
is antisymmetric and is related to the generators J
and K by
Ji = 1
2εijkM jk,
(9.62)
so that J = (M 23, M 31, M 12), and
Ki = M 0i,
(9.63)
so that K = (M 01, M 02, M 03). Also show that
Λ = exp(−i
2ωµνM µν),
(9.64)
with Λ = e−i(J·θ−K·φ).

10
Symmetry
10.1 Invariance and conservation
90
10.2 Noether’s theorem
92
10.3 Spacetime translation
94
10.4 Other symmetries
96
Chapter summary
97
Exercises
97
The Universe is built on a plan the profound symmetry of
which is somehow present in the inner structure of our intel-
lect
Paul Val´ery (1871–1945)
In the previous chapter we saw that translations were achieved with
an operator that contains the momentum operator, and rotations were
achieved using the angular momentum operator. A coincidence? Actu-
ally not, and in fact we can go further. If a system possesses some kind
of invariance (so that for example it is invariant to translations) then
a particular quantity will be conserved. This idea is bound up with a
result called Noether’s theorem, named after the great mathematician
Emmy Noether.1 This chapter will explore this theorem in detail.
1Emmy Noether (1832–1935).
Her
name should be pronounced to rhyme
with
Goethe,
Alberta
and
Norris
McWhirter.
A point to note from the outset. In this chapter we will be using the
word symmetry to mean global symmetry, i.e. a symmetry possessed
by the entire system under consideration. Thus rotational symmetry
refers to a symmetry associated with rotation of the coordinates applying
to every point in our system.2
2Consideration
of
local
symmetries,
where a transformation changes from
point to point, will be postponed to a
later chapter.
10.1
Invariance and conservation
Let’s get the terminology straight. A quantity is invariant when it takes
the same value when subjected to some particular transformation, and is
then said to exhibit a particular symmetry. A translationally invariant
system looks the same however you translate it. A rotationally invariant
system appears identical however it is rotated around a particular axis.
Electric charge is said to be Lorentz invariant because it takes the same
value when viewed in diﬀerent inertial frames of reference. A system is
said to possess a particular symmetry if a property is invariant under a
particular transformation, so for example a cylinder possesses rotational
symmetry about its axis.
A quantity is said to be conserved when
it takes the same value before and after a particular event.
Thus in
a collision between particles, the four-momentum is conserved within a
frame of reference but it is not invariant between frames of reference.
Having stressed that invariance and conservation are entirely diﬀerent
concepts, it is now time to point out that they are in fact connected!
For example:
• Conservation of linear momentum is related to invariance under
spatial translations.

10.1
Invariance and conservation
91
• Conservation of angular momentum is related to invariance under
rotations.
• Conservation of energy is related to invariance under time trans-
lations.
The general idea is clear: invariances lead to conservation laws.
Example 10.1
(i) The Euler–Lagrange equation (eqn 1.28) can be written as
∂L
∂xµ = d
dt
„ ∂L
∂˙xµ
«
,
(10.1)
and using the canonical momentum pµ = ∂L/∂˙xµ (eqn 5.3), we can write
eqn 10.1 as
∂L
∂xµ = ˙pµ.
(10.2)
This has the immediate consequence that if L does not depend on xµ, then
∂L/∂xµ = 0 and hence pµ is a constant. In other words if the Lagrangian
is invariant under a particular component of spacetime translation, the corre-
sponding component of four-momentum is a conserved quantity.
(ii) Another example of this follows from eqn 5.58 of Exercise 5.1 which showed
that
∂L
∂t = −dH
dt .
(10.3)
Thus if L does not depend explicitly on time, then the Hamiltonian H (i.e.
the energy) is a conserved quantity.
Under a symmetry transformation, various quantities will change.3 One
3In this chapter, we are going to as-
sume that the transformations are con-
tinuous. Thus we are ignoring discrete
symmetries which will be considered in
Chapter 15.
eﬃcient way to describe this is to say that a ﬁeld φ(xµ) will change under
a symmetry transformation by an amount parameterized by a quantity
λ. For example, for a translation along a spacetime vector aµ we could
write the transformation
φ(xµ) →φ(xµ + λaµ).
(10.4)
The larger λ, the larger the degree the transformation is applied. It will
be useful to consider inﬁnitesimal transformations, and for this purpose
we adopt the notation
Dφ = ∂φ
∂λ

λ=0
.
(10.5)
An inﬁnitesimal change in φ, induced by an inﬁnitesimal δλ, can be
written as
δφ = Dφ δλ.
(10.6)
Example 10.2
In the example of our spacetime translation φ(xµ) →φ(xµ + λaµ), we have
Dφ = aµ∂µφ.
(10.7)

92
Symmetry
This result arises as follows: by writing yµ = xµ + λaµ, we have
∂φ
∂λ = ∂φ
∂yµ
∂yµ
∂λ ,
(10.8)
and ∂yµ
∂λ = aµ. Taking the limit of λ →0 gives the required result.
10.2
Noether’s theorem
We now turn to Noether’s theorem, which shows that where we have a
continuous symmetry we also have a conservation law. To identify this
conservation law we will look for a divergenceless current (i.e. obeying
∂µJµ = 0). Such a current is locally conserved (see eqn. 5.35 if in doubt)
and its time-like component will give rise to a conserved charge.
Let the ﬁeld φ(x) change to φ(x) + δφ(x), where the variation
δφ(x) vanishes on the boundary of the region in spacetime that we’re
considering.4 The change in the Lagrangian density L is then given by
4This is a continuous transformation.
Noether’s theorem applies to these con-
tinuous transformations but not to the
discrete transformations examined in
Chapter 15.
δL = ∂L
∂φ δφ +
∂L
∂(∂µφ)δ(∂µφ).
(10.9)
We will slightly simplify this equation using the substitution
Πµ(x) =
∂L
∂(∂µφ).
(10.10)
This new object Πµ(x) is a generalization of what, in eqn 5.46, we called
the conjugate momentum π(x) = δL/δ ˙φ. Our new object Πµ(x), the
momentum density, is the four-vector generalization of π(x). In fact
π(x) is the time-like component of Πµ(x), i.e.
Π0(x) = π(x).
(10.11)
This substitution allows us to write eqn 10.9 as
δL = ∂L
∂φ δφ + Πµδ(∂µφ).
(10.12)
Using δ(∂µφ) = ∂µ(δφ), and the simple diﬀerentiation of a product
∂µ(Πµδφ) = Πµ∂µ(δφ) + (∂µΠµ)δφ,
(10.13)
we have
δL =
∂L
∂φ −∂µΠµ

δφ + ∂µ(Πµδφ).
(10.14)
The action S should not change under the transformation and so
δS =
Z
d4x
∂L
∂φ −∂µΠµ

δφ = 0,
(10.15)

10.2
Noether’s theorem
93
where the second term in eqn 10.14 vanishes when integrated over a
large enough volume (using the divergence theorem), and so
∂L
∂φ = ∂µΠµ,
(10.16)
which is the Euler–Lagrange equation. Now let’s play this argument the
other way round and state that φ obeys the equation of motion. In this
case eqn 10.14 becomes
δL = ∂µ(Πµδφ),
(10.17)
and eqn 10.6 then allows us to write
δL = ∂µ(ΠµDφ)δλ.
(10.18)
Because we are applying a symmetry transformation, the action S =
R
d4x L is unchanged as we have said, and so we don’t expect L to
change, i.e. δL = 0. However, this condition is too restrictive and so in
some circumstances we could be persuaded to allow L to change by the
four-divergence of some function W µ(x), so that
A
nµ
Fig. 10.1 A region of spacetime with
surface A. The outward normal is nµ.
δL = (∂µW µ)δλ.
(10.19)
This is because
δS =
Z
d4x δL = δλ
Z
d4x ∂µW µ = δλ
Z
dA nµW µ,
(10.20)
where A is the surface of the region of spacetime and nµ is the outward
normal (see Fig. 10.1). Since the ﬁelds are held constant on the surface in
order to have a well-deﬁned variational principle, the change in action is
at most a constant, and so S′ =
R
d4x (δL+∂µW µδλ) will be stationary
under the same conditions that S =
R
d4x δL will be stationary. Putting
eqns 10.18 and 10.19 together leads to
∂µ(ΠµDφ −W µ) = 0,
(10.21)
or more simply ∂µJµ
N = 0 where
Jµ
N(x) = Πµ(x)Dφ(x) −W µ(x),
(10.22)
is a locally conserved Noether current. We have deduced a form of
Noether’s theorem that states the following:
If a continuous symmetry transformation φ →φ + Dφ only changes L
by the addition of a four-divergence (i.e. DL = ∂µW µ) for arbitrary
φ, then this implies the existence of a current Jµ
N = ΠµDφ −W µ(x).
If φ obeys the equations of motion then the current is conserved, i.e.
∂µJµ
N = 0.

94
Symmetry
Conserved currents are important because they give rise to conserved
charges
QN =
R
Jµ
N dAµ (also called Noether charges). A helpful
way to apply this idea is to perform the integration over a surface at
constant time (see Fig. 10.2).
In four dimensions, this ‘surface’ is a
three-volume. Thus
QN =
Z
d3x J0
N,
(10.23)
where J0
N is the time-like component normal to the surface. Now since
time
J0N
J0N
Fig. 10.2 The integration is performed
over a surface in spacetime at constant
time.
The component of the Noether
current which is perpendicular to this
surface is J0
N, the time-like part.
∂µJµ
N = 0, then
0 =
Z
d3x (∂µJµ
N) =
Z
d3x (∂0J0
N + ∂kJk
N).
(10.24)
The second term in the integral is
R
d3x ∂kJk
N =
R
dAk Jk
N by the diver-
gence theorem, and this will vanish if the volume is big enough. Thus
R
d3x ∂0J0
N = 0 and since ∂0 = d/dt, this implies dQN/dt = 0, i.e. a
conserved charge.
Using Noether’s theorem to ﬁnd
conserved charges
I:
Find Dφ = ∂φ
∂λ
˛˛˛
λ→0.
II:
Find Πµ(x) =
∂L
∂(∂µφ) .
III:
Find W µ = DL.
IV:
Write Jµ
N = DφΠµ −W µ.
V:
Find QN =
R
d3xJ0
N.
Everything we have done so far is valid for classical ﬁeld theories.
We might wonder if this discussion can be carried over to the quantum
mechanical ﬁelds that are the main subject of this book. Fortunately
for us Noether’s theorem can be carried over to the quantum realm5
5This assumes that all symmetries of
the classical theory will turn out to be
symmetries of the quantum theory too.
This is true for many important sym-
metries. It turns out not to be the case
for a small number of cases, which are
known as anomalies and are discussed
in more advanced books such as Zee or
Peskin and Schroeder.
with one addition: the so-called ‘normal ordering interpretation’ which
we introduce in the next chapter. Noether’s theorem then provides us
with a conserved charge operator ˆQN, whose eigenvalues are (observable)
conserved charges. In fact, there is also a quantum mechanical corollary
to Noether’s theorem which says that if we know the conserved charge
operator ˆQN for a quantum mechanical ﬁeld ˆφ then we can use it to
generate the symmetry transformation D ˆφ via6
6This result is easily seen by recogniz-
ing that conserved charge may be writ-
ten QN =
R
d3x Π0Dφ and recognizing
that Π0 is the conjugate momentum to
φ.
h
ˆQN, ˆφ
i
= −iD ˆφ.
(10.25)
This is illustrated in Fig. 10.3. For now we return to classical ﬁeld theory
and continue to investigate the beneﬁts we gain from Noether’s theorem.
10.3
Spacetime translation
The ﬁeld theories that we will examine in this book will be sym-
metric with respect to spacetime translations.
That is to say that
the physical consequences of the theories will be the same whether
an experiment takes place at a point xµ = (1066AD, Hastings) or
yµ = (1863AD, Gettysburg).
Here we examine the consequences of
this using Noether’s theorem.
If we subject the coordinates to a variation, so that x′µ = xµ + δxµ,
then we have that Dφ = aµ∂µφ and also that
DL = aµ∂µL = ∂µ(aµL).
(10.26)

10.3
Spacetime translation
95
We recognize therefore that DL = ∂µW µ where W µ = aµL and so we
can write down the conserved current
Jµ
N
=
ΠµDφ −W µ
=
Πµaν∂νφ −aµL
=
aν[Πµ∂νφ −δµ
ν L]
=
aνT µν,
(10.27)
where T µν = Πµ∂νφ −gµνL is a quantity that is known as the energy-
momentum tensor. The conserved charges that arise from this current
can be written
P α =
Z
d3x T 0α.
(10.28)
Thus the time-like component of this conserved charge is
symmetry generator
conserved charge
commutator
Noether
current
Dφ
Q
Fig. 10.3
Noether’s theorem for the
ﬁeld φ.
P 0 =
Z
d3x T 00 =
Z
d3x [π(x) ˙φ(x) −L(x)] =
Z
d3x H,
(10.29)
which we recognize as the energy. The space-like components give us
P k =
Z
d3x T 0k =
Z
d3x π(x)∂kφ(x),
(10.30)
which we recognize as the momentum. (Note that g00 = 1 and g0k = 0.)
Example 10.3
The energy-momentum tensor T µν is not uniquely deﬁned.
If one adds a term
∂λXλµν to T µν, where Xλµν = −Xµλν, then show that the new tensor is still
divergenceless.
Solution: ∂µ(T µν +∂λXλµν) = ∂µT µν +∂µ∂λXλµν = 0+0 = 0. This works because
(i) ∂µT µν = 0 because T µν is a conserved current and (ii) ∂µ∂λXλµν = 0 follows
from the antisymmetry of Xλµν with respect to swapping the ﬁrst two indices and
the fact that you are summing over both indices µ and λ.
The energy-momentum tensor T µν, as we have constructed it, is not
symmetric, and that turns out to cause problems when it is used in
general relativity,7 but we can use the trick in the previous example
7Einstein’s ﬁeld equations are
Rµν −1
2gµνR = −8πG
c4 Tµν,
where Rµν is the Ricci tensor, express-
ing the curvature of spacetime, and R
is the Ricci scalar. For further details,
see any good book on general relativity,
e.g. Penrose (2004).
to redeﬁne T µν →T µν + ∂λXλµν and force it to be symmetric. This
symmetric nature of T µν is also important when we come to look at
Lorentz transformations.
Example 10.4
We will extract some information from T µν.
For an inﬁnitesimal Lorentz
transformation8 we can write
8See Exercise 9.3.
δxµ = ωµνxν δλ,
(10.31)
where ωµν is an antisymmetric tensor. Hence the change in ﬁeld φ is given by
δφ = δxµ∂µφ = ωµνxν∂µφδλ,
(10.32)

96
Symmetry
and so
Dφ = ωµνxν∂µφ = ωµνxν∂µφ
(10.33)
so that the eﬀect on the Lagrangian density L is9
9The second equality in eqn 10.34 fol-
lows from the following argument. Dif-
ferentiating the product xσL gives
∂ρ(xσL) = xσ∂ρL + L∂ρxσ,
and because ∂ρxσ = δρσ, the antisym-
metry of ωρσ implies that
ωρσ∂ρxσ = ωρσδρσ = 0.
Hence
ωρσxσ∂ρL
=
ωρσ∂ρ(xσL)
=
∂ρ(ωρσxσL).
DL = ωρσxσ∂ρL = ∂ρωρσxσL = ∂µ[gµρωρσxσL] = ∂µW µ,
(10.34)
where W µ = gµρωρσxσL. Thus we can go straight to our conserved current
Jµ
N
=
ΠµDφ −W µ
=
ωρσ[Πµxσ∂ρφ −gµρxσL]
=
ωρσxσT ρµ
=
ωρσ( ˜Jµ)ρσ,
(10.35)
where ( ˜Jµ)ρσ = xσT ρµ.
There are six parameters in ωρσ (remember that it is
antisymmetric) and so there are six conserved currents. We will write
(Jµ)ρσ = xρT µσ −xσT µρ,
(10.36)
where the currents have been antisymmetrized10 compared with eqn 10.35. These
10We have turned ( ˜Jµ)ρσ into (Jµ)ρσ
using (Jµ)ρσ = ( ˜Jµ)ρσ −( ˜Jµ)σρ.
are conserved currents and so ∂µ(Jµ)ρσ = 0, although this only works because T µν
is symmetric. The conserved charges11 are
11These are Noether conserved charges,
but we drop subscript N’s here to avoid
clutter.
Qλρ =
Z
d3x (Jλ)ρ0,
(10.37)
from which we can extract the angular momentum components
Qij =
Z
d3x (xiT j0 −xjT i0)
(10.38)
and the boosts
Q0i =
Z
d3x (x0T i0 −xiT 00).
(10.39)
Note that since dQ0i/dt = 0 and x0 = t, then
0 =
Z
d3x T i0+t
Z
d3x ˙T i0−d
dt
Z
d3x xiT 00 = P i+t ˙P i−d
dt
Z
d3x xiT 00, (10.40)
and hence
d
dt
R
d3x xiT 00 is a constant, showing that the centre of mass of the system
moves in uniform motion.
10.4
Other symmetries
We have seen how translations, rotations and boosts can transform
spacetime coordinates and also Lagrangians. However, the only eﬀect
on the ﬁelds has been due to the fact that the ﬁelds are functions of the
spacetime coordinates. A ﬁeld, you will remember, is a machine that
when fed a coordinate xµ returns a number φ(xµ). What we have been
considering is a scalar ﬁeld, but of course when the ﬁeld is more compli-
cated (for example, a vector or a spinor) then we have to worry about
how the symmetry transformation aﬀects the ﬁeld itself, in addition to
the eﬀect on the spacetime coordinates of which the ﬁeld is a function.
That will lead to additional layers of complexity which we will save for
later. One of the most exciting consequences of this is that if we have
a complex scalar ﬁeld we will be able to show that the U(1) symmetry
we noticed in Section 7.6 leads to the conservation of particle number.
We will save that gem for Chapter 12, but before that, in the following
chapter, we will outline a procedure for quantizing our ﬁeld theories.

Exercises
97
Chapter summary
• Noether’s theorem states that if a continuous symmetry transfor-
mation φ →φ + Dφ only changes L by the addition of a four-
divergence (i.e. DL = ∂µW µ) for arbitrary φ, then this implies the
existence of a current Jµ
N = ΠµDφ −W µ(x) and if φ obeys the
equations of motion then the current is conserved, i.e. ∂µJµ
N = 0.
• In short, Noether’s theorem states that continuous symmetries lead
to conserved currents.
Exercises
(10.1) Show that [φ(x), P α] = i∂αφ(x), where P α is
the conserved charges from spacetime translation
(eqn 10.28).
(10.2) Consider
a
system
characterized
by
N
ﬁelds
φ1, . . . , φN.
The
Lagrangian
density
is
then
L(φ1, . . . , φN; ∂µφ1, . . . , ∂µφN; xµ). Show that the
Noether current is
Jµ =
X
a
Πµ
aDφa −W µ(x),
(10.41)
where DL = ∂µW µ.
(10.3) For the Lagrangian
L = 1
2 (∂µφ)2 −1
2m2φ2,
(10.42)
evaluate T µν and show that T 00 agrees with what
you would expect from the Hamiltonian for this La-
grangian. Show that ∂µT µν = 0. Derive expres-
sions for P 0 =
R
d3x T 00 and P i =
R
d3x T 0i.
(10.4) For the Lagrangian
L = −1
4FµνF µν = 1
2(E2 −B2),
(10.43)
show that
Πσρ ≡
∂L
∂(∂σAρ) = −F σρ.
(10.44)
Hence show that the energy-momentum tensor
T µ
ν = Πµσ∂νAσ −δµ
ν L,
(10.45)
can be written as
T µν = −F µσ∂νAσ + 1
4gµνF αβFαβ.
(10.46)
This tensor is not symmetric but we can symmetrize
it by adding ∂λXλµν where Xλµν = F µλAν. Show
that Xλµν = −Xµλν. Show further that the sym-
metrized energy-momentum tensor ˜T µν = T µν +
∂λXλµν can be written
˜T µν = F µσFσ
ν + 1
4gµνF αβFαβ.
(10.47)
Hence show that ˜T 00 =
1
2(E2 + B2), the energy
density in the electromagnetic ﬁeld, and ˜T i0 =
(E × B)i, which is the Poynting vector and de-
scribes the energy ﬂow.

11
Canonical quantization of
ﬁelds
11.1 The
canonical
quantization
machine
98
11.2 Normalizing factors
101
11.3 What becomes of the Hamil-
tonian?
102
11.4 Normal ordering
104
11.5 The meaning of the mode ex-
pansion
106
Chapter summary
108
Exercises
108
You’ve got to have a system
Harry Hill (1964– )
Quantum ﬁeld theory allows us to consider a Universe in which there ex-
ist diﬀerent, yet indistinguishable, copies of elementary particles. These
particles can be created and destroyed by interactions amongst them-
selves or with external entities. Quantum ﬁeld theory allows us to de-
scribe such phenomena because particles themselves are simply excita-
tions of quantum ﬁelds.
To see how particles emerge from ﬁelds we need to develop a way
to quantize the classical ﬁelds we have looked at previously. The good
news is that there is a machine available that takes a classical ﬁeld the-
ory and, after we turn the handle, it spits out a quantum ﬁeld theory:
that is, a theory where quantities are described in terms of the number
of quantum particles in the system. The name of the machine is canon-
ical quantization. In developing canonical quantization we’ll see that
particles are added or removed from a system using ﬁeld operators1
1Field operators were introduced in
Chapter 4.
and these are formed from the creation and annihilation operators we
found so useful in previous chapters.
11.1
The canonical quantization machine
Canonical quantization is the turn-the-handle method of obtaining a
quantum ﬁeld theory from a classical ﬁeld theory. The method runs like
this:
• Step I: Write down a classical Lagrangian density in terms of
ﬁelds. This is the creative part because there are lots of possible
Lagrangians. After this step, everything else is automatic.
• Step II: Calculate the momentum density and work out the
Hamiltonian density in terms of ﬁelds.
• Step III: Now treat the ﬁelds and momentum density as opera-
tors. Impose commutation relations on them to make them quan-
tum mechanical.
• Step IV: Expand the ﬁelds in terms of creation/annihilation op-
erators. This will allow us to use occupation numbers and stay
sane.

11.1
The canonical quantization machine
99
• Step V: That’s it. Congratulations, you are now the proud owner
of a working quantum ﬁeld theory, provided you remember the
normal ordering interpretation.
We’ll illustrate the method with one of the simplest ﬁeld theories: the
theory of the massive scalar ﬁeld.
Step I: We write down a Lagrangian density for our theory.
For
massive scalar ﬁeld theory this was given in eqn 7.7, which we rewrite
as
L = 1
2 [∂µφ(x)]2 −1
2m2 [φ(x)]2 .
(11.1)
[Recall that the equation of motion for this theory is the Klein–Gordon
equation (∂2 + m2)φ = 0 leading to a dispersion E2
p = p2 + m2.]
Step II: Find the momentum density (eqn 10.10) Πµ(x) given by
Πµ(x) =
∂L
∂(∂µφ(x)).
(11.2)
For our Lagrangian in eqn 11.1 this gives Πµ(x) = ∂µφ(x). The time-like
component2 of this tensor is Π0(x) = π(x) = ∂0φ(x). This allows us to
2The metric (+ −−−), which allows
us to say A0 = A0, lets us swap up
and down indices for the zeroth com-
ponent of any object without incurring
the penalty of a minus sign.
deﬁne the Hamiltonian density in terms of the momentum density
H = Π0(x)∂0φ(x) −L,
(11.3)
and using our Lagrangian3 leads to a Hamiltonian density
3which may be helpfully rewritten as
L = 1
2
(„ ∂φ
∂t
«2
−(∇φ)2 −m2φ2
)
.
(11.4)
H
=
∂0φ(x)∂0φ(x) −L
=
1
2 [∂0φ(x)]2 + 1
2 [∇φ(x)]2 + 1
2m2 [φ(x)]2 .
(11.5)
The last line in eqn 11.5 tells us that the energy has contributions from
(i) a kinetic energy term reﬂecting changes in the conﬁguration in time,
(ii) a ‘shear term’ giving an energy cost for spatial changes in the ﬁeld
and (iii) a ‘mass’ term reﬂecting the potential energy cost of there being
a ﬁeld in space at all.
Taken together this has a reassuring look of
E = (kinetic energy)+(potential energy), which is what we expect from
a Hamiltonian.
Step III: We turn ﬁelds into ﬁeld operators. That is to say, we make
them operator-valued ﬁelds: one may insert a point in spacetime into
such an object and obtain an operator. We therefore take φ(x) →ˆφ(x)
and Π0(x) →ˆΠ0(x). To make these ﬁeld operators quantum mechanical
we need to impose commutation relations between them.
In single-
particle quantum mechanics we have [ˆx, ˆp] = iℏ. By analogy, we quantize
the ﬁeld theory by deﬁning the equal-time commutator for the ﬁeld
operators
[ˆφ(t, x), ˆΠ0(t, y)] = iδ(3)(x −y).
(11.6)
As the name suggests, this applies at equal times only and otherwise
the ﬁelds commute. We also have that [ˆφ(x), ˆφ(y)] = [ˆΠ0(x), ˆΠ0(y)] = 0
(and likewise for the daggered versions). Expressed in terms of these
ﬁelds, the Hamiltonian density H is now an operator ˆH which acts on

100
Canonical quantization of ﬁelds
state vectors. This is all well and good, except that we don’t know how
operators like ˆφ(x) act on occupation number states like |n1n2n3 . . .⟩.
What we do know is how creation and annihilation operators act on
these vectors. If only we could build ﬁelds out of these operators!
Step IV: How do we get any further? The machinery of creation and
annihilation operators is so attractive that we’d like to deﬁne everything
in terms of them. In particular, we found a neat analogy between par-
ticles in momentum eigenstates and quanta in oscillators. What we do,
therefore, is to expand the ﬁeld operators in terms of the creation and
annihilation operators. We’ve already seen this in the coupled oscilla-
tor problem (eqn 2.68) where we obtained a time-independent position
operator of the form
ˆxj =
 ℏ
m
 1
2 X
k
1
(2ωkN)
1
2 (ˆakeijka + ˆa†
ke−ijka).
(11.7)
By analogy we write down a time-independent ﬁeld operator for the
continuous case which will look like
We are rewriting our momenta k →p
and our energies ωk
→Ep, so the
factor(2ωk)1/2 →(2Ep)1/2. The nor-
malization will be discussed below.
ˆφ(x) =
Z
d3p
(2π)
3
2
1
(2Ep)
1
2
 ˆapeip·x + ˆa†
pe−ip·x
,
(11.8)
with Ep = +(p2+m2)
1
2 (that is, we consider positive roots only, a feature
discussed below) and where, as before, our creation and annihilation
operators have a commutation relation [ˆap, ˆa†
q] = δ(3)(p −q).
The ﬁeld operators that we’ve written down are intended to work in
the Heisenberg picture. To obtain their time dependence, we hit them
with time-evolution operators. We use the Heisenberg prescription for
making an operator time dependent:
ˆφ(x) = ˆφ(t, x) = ˆU †(t, 0)ˆφ(x) ˆU(t, 0) = ei ˆ
Ht ˆφ(x)e−i ˆ
Ht.
(11.9)
The only part that the ˆU(t, 0) = e−i ˆ
Ht operators aﬀect are the creation
and annihilation operators, and we have
ˆU †(t, 0)ˆap ˆU(t, 0) = e−iEptˆap,
(11.10)
which shows that the ˆap picks up a e−iEpt. Similarly, the ˆa†
p part will
pick up a eiEpt.
Example 11.1
To see this unfolding we’ll consider a simple example of one component of the ﬁeld
acting on an example state. The component we’ll consider is ˆaqeiq·x and the example
state is |npnqnr⟩. Taking it one step at a time:
ei ˆ
Htˆaqeiq·xe−i ˆ
Ht|npnqnr⟩
=
ei ˆ
Htˆaq|npnqnr⟩eiq·xe−i(npEp+nqEq+nrEr)t
=
√nqei ˆ
Ht|np(nq −1)nr⟩eiq·xe−i(npEp+nqEq+nrEr)t
=
√nq|np(nq −1)nr⟩ei(npEp+(nq−1)Eq+nrEr)teiq·xe−i(npEp+nqEq+nrEr)t
=
√nq|np(nq −1)nr⟩e−iEqteiq·x.
(11.11)

11.2
Normalizing factors
101
Part of what we’re left with, namely √nq|np(nq −1)nr⟩, is exactly the same as if
we’d just acted on the original state with ˆaq. The eﬀect of dynamicizing this operator
has just been to multiply by a factor e−iEqt, so we conclude that the operator we
seek is ˆaqe−i(Eqt−q·x) = ˆaqe−iq·x.
In summary, what we call the mode expansion of the scalar ﬁeld is
given by4
4This looks a little diﬀerent from what
we had in Chapter 4 when we intro-
duced ﬁeld operators. We will explain
the reason for the diﬀerence in Sec-
tion 11.5.
ˆφ(x) =
Z
d3p
(2π)
3
2
1
(2Ep)
1
2
 ˆape−ip·x + ˆa†
peip·x
,
(11.12)
with Ep = +(p2 + m2)
1
2 .
Note that by expanding out the position ﬁeld we’ll get the momentum
expansion for free, since for our scalar ﬁeld example Πµ(x) = ∂µφ(x).
Also note that because the ﬁeld in our classical Lagrangian is a real
quantity, the ﬁeld operator ˆφ(x) should be Hermitian. By inspection
ˆφ†(x) = ˆφ(x), so this is indeed the case.
11.2
Normalizing factors
This section can be skipped if you are
happy to take eqn 11.12 on trust. The
purpose here is simply to justify the fac-
tors (2π)3/2(2Ep)1/2 which otherwise
seem to appear by magic.
Before proceeding, we will justify the normalization factors in eqn 11.12.
In evaluating integrals over momentum states we have the problem that
d3p is not a Lorentz-invariant quantity. We can use d4p where p = (p0, p)
is the four-momentum, but for a particle of mass m then only values of
the four-momentum which satisfy5 p2 = m2 need to be considered. This
5The condition p2 = m2 means that
(p0)2 −p2 = E2
p −p2 = m2.
is known as the mass shell condition (see Fig. 11.1). Consequently
we can write our integration measure
d4p δ(p2 −m2) θ(p0).
(11.13)
We have included a Heaviside step function θ(p0) to select only positive
p2 = 0
p2 = m2
mass shell
light
m
E = p0
p1
Fig. 11.1 The mass shell p2 = m2 is a
hyperboloid in four-momentum space.
Also shown is the equivalent surface for
light, p2 = 0.
mass particles.
Example 11.2
Show that δ(p2 −m2) θ(p0) =
1
2Ep δ(p0 −Ep)θ(p0).
We use the identity
δ[f(x)] =
X
{x|f(x)=0}
1
|f′(x)|δ(x),
(11.14)
where the notation tells us that the sum is evaluated for those values of x that
make f(x) = 0. We take x = p0 and f(p0) = p2 −m2 = (p0)2 −p2 −m2. This
gives us that |f′(p0)| = 2|p0| and we use the fact that the zeros of f(p0) occur for
p0 = ±(p2 + m2)
1
2 = ±Ep to write
δ(p2 −m2) θ(p0) =
1
2Ep
[δ(p0 −Ep)θ(p0) + δ(p0 + Ep)θ(p0)],
(11.15)
and so the result follows (since the second term in eqn 11.15 is zero).

102
Canonical quantization of ﬁelds
Thus we will write our Lorentz-invariant measure as
d3p
(2π)32Ep
,
(11.16)
where the additional factor δ(p0 −Ep)θ(p0) is there in every calculation
and so we suppress it, and we have included the factor 1/(2π)3 because
the mode expansion is essentially an inverse Fourier transform (we have
one factor of 1/(2π) for every component of three-momentum). We are
now in a position to write down integrals, for example:
1 =
Z
d3p
(2π)32Ep
|p⟩⟨p|.
(11.17)
This requires us to have Lorentz-covariant four-momentum states |p⟩.
We previously normalized momentum states according to
⟨p|q⟩= δ(3)(p −q),
(11.18)
so our new four-momentum states |p⟩will need to be related to the
three-momentum states |p⟩by
|p⟩= (2π)3/2(2Ep)1/2|p⟩,
(11.19)
and then their normalization can be written
⟨p|q⟩= (2π)32Epδ(3)(p −q).
(11.20)
Similarly, to make creation operators ˆα† appropriately
normalized so
that they create Lorentz-covariant states, we must deﬁne them by
ˆα†
p = (2π)3/2(2Ep)1/2ˆa†
p,
(11.21)
so that ˆα†
p|0⟩= |p⟩. In this case our mode expansion would take the
form of a simple inverse Fourier transform using our Lorentz-invariant
measure
ˆφ(x) =
Z
d3p
(2π)3
1
(2Ep)
 ˆαpe−ip·x + ˆα†
peip·x
,
(11.22)
or writing in terms of ˆa†
p and ˆap rather than ˆα†
p and ˆαp:
ˆφ(x) =
Z
d3p
(2π)
3
2
1
(2Ep)
1
2
 ˆape−ip·x + ˆa†
peip·x
,
(11.23)
which is identical to eqn 11.12.
11.3
What becomes of the Hamiltonian?
We can now substitute our expansion of the ﬁeld operator ˆφ(x) into
the Hamiltonian to complete our programme of canonical quantization.
This will provide us with an expression for the energy operator in terms

11.3
What becomes of the Hamiltonian?
103
of the creation and annihilation operators. The Hamiltonian is given by
the volume integral of the Hamiltonian density,
ˆH =
Z
d3x 1
2
h
∂0 ˆφ(x)
i2
+ [∇ˆφ(x)]2 + m2[ˆφ(x)]2

.
(11.24)
All we need do is substitute in the mode expansion and use the com-
mutation relations to simplify. We’ll start by computing the momentum
density ˆΠµ(x) = ∂µ ˆφ(x) which is given by
ˆΠµ(x) = ∂µ ˆφ(x) =
Z
d3p
(2π)
3
2 (2Ep)
1
2 (−ipµ)
 ˆape−ip·x −ˆa†
peip·x
.
(11.25)
To obtain an expression for ∂0 ˆφ(x), simply take the time-like component
of Πµ(x):
∂0 ˆφ(x) =
Z
d3p
(2π)
3
2 (2Ep)
1
2 (−iEp)
 ˆape−ip·x −ˆa†
peip·x
,
(11.26)
while the space-like components provide us with ∇ˆφ(x):
∇ˆφ(x) =
Z
d3p
(2π)
3
2 (2Ep)
1
2 (ip)
 ˆape−ip·x −ˆa†
peip·x
.
(11.27)
We now have all of the ingredients to calculate the Hamiltonian.
Example 11.3
Substitution of the mode expansion leads to a multiple integral of the form
ˆH
=
1
2
Z
d3x d3p d3q
(2π)3(2Ep)
1
2 (2Eq)
1
2
h
(−EpEq −p · q)[ˆape−ip·x −ˆa†
peip·x]
× [ˆaqe−iq·x −ˆa†
qeiq·x] + m2[ˆape−ip·x + ˆa†
peip·x][ˆaqe−iq·x + ˆa†
qeiq·x]
i
. (11.28)
We use our favourite trick again: ﬁrst, we integrate over x and use
R
d3x eip·x =
(2π)3δ(3)(p) and obtain
ˆH
=
1
2
Z
d3p d3q
(2Ep)
1
2 (2Eq)
1
2
(11.29)
×
h
δ(3)(p −q)(EpEq + p · q + m2)[ˆa†
pˆaqe−i(Ep−Eq)t + ˆapˆa†
qe−i(Ep−Eq)t]
+ δ(3)(p + q)(−EpEq −p · q + m2)[ˆa†
pˆa†
qe−i(Ep+Eq)t + ˆapˆaqe−i(Ep+Eq)t]
i
.
Then we do the integral over q to mop up the delta functions:
ˆH
=
1
2
Z
d3p
1
2Ep
h
(E2
p + p2 + m2)(ˆa†
pˆap + ˆapˆa†
p)
+ (−E2
p + p2 + m2)(ˆa†
pˆa†
−pe2iEpt + ˆapˆa−pe−2iEpt)
i
,
(11.30)
and since E2
p = p2 + m2 this quickly simpliﬁes to
ˆH = 1
2
Z
d3p Ep(ˆapˆa†
p + ˆa†
pˆap).
(11.31)

104
Canonical quantization of ﬁelds
Using the commutation relations on the result in eqn 11.31 we obtain
E =
Z
d3p Ep

ˆa†
pˆap + 1
2δ(3)(0)

.
(11.32)
The last term should ﬁll us with dread. The term 1
2
R
d3p δ3(0) will give
us an inﬁnite contribution to the energy! However we should keep in
mind that by evaluating the total energy E we’re asking the theory to
tell us about something we can’t measure. This is a nonsensical question
and we have paid the price by getting a nonsensical answer. What we
can measure is diﬀerences in energy between two conﬁgurations. This
is ﬁne in our formalism, since upon taking the diﬀerences between two
energies the
1
2δ(3)(0) factors will cancel obligingly. However, it’s still
rather unsatisfactory to have inﬁnite terms hanging around in all of our
equations. We need to tame this inﬁnity.
11.4
Normal ordering
To get around the inﬁnity encountered at the end of the last section we
deﬁne the act of normal ordering. Given a set of free ﬁelds, we deﬁne
the normal ordered product as
N
h
ˆA ˆB ˆC† . . . ˆX† ˆY ˆZ
i
=

Operators rearranged with all
creation operators on the left

.
(11.33)
The N is not an operator in the sense that is doesn’t act on a state to
provide some new information. Instead, normal ordering is an interpre-
tation that we use to eliminate the meaningless inﬁnities that occur in
ﬁeld theories. So the rule goes that ‘if you want to tell someone about
a string of operators in quantum ﬁeld theory, you have to normal order
them ﬁrst’. If you don’t, you’re talking nonsense.6
6The origin of the inﬁnity in eqn 11.32
is due to an ordering ambiguity in the
classical Lagrangian.
Consider a gen-
eral classical theory formed from com-
plex values ﬁelds ψ(x). The Lagrangian
describing these ﬁelds will contain bilin-
ear terms like ψ†ψ, which could equally
well be written in a classical theory as
ψψ†. When we quantize and insert the
mode expansion the order that was cho-
sen suddenly becomes crucial. Normal
ordering removes the ambiguity to give
a meaningful quantum theory.
Rearranging operators is ﬁne for Bose ﬁelds, but swapping the order
of Fermi ﬁelds results in the expression picking up a minus sign. We
therefore need to multiply by a factor (−1)P , where P is the number of
permutations needed to normally order a product of operators.
Example 11.4
Some examples of normal ordering a string of Bose operators follow. We ﬁrst consider
a single mode operator
N
h
ˆaˆa†i
= ˆa†ˆa,
N
h
ˆa†ˆa
i
= ˆa†ˆa,
(11.34)
and
N
h
ˆa†ˆaˆaˆa†ˆa†i
= ˆa†ˆa†ˆa†ˆaˆa.
(11.35)
Next, consider the case of many modes
N
h
ˆapˆa†
qˆar
i
= ˆa†
qˆapˆar.
(11.36)
The order of ˆap and ˆar doesn’t matter since they commute.
For Fermi ﬁelds, N[ˆcpˆc†
qˆcr] = −ˆc†
qˆcpˆcr, the sign change occurring because we have
performed a single permutation (and hence an odd number of them).

11.4
Normal ordering
105
Step V: We can ﬁnally complete the programme of canonical quantiza-
tion. With the normal ordering interpretation the old expression
ˆH = 1
2
Z
d3p Ep(ˆapˆa†
p + ˆa†
pˆap),
(11.37)
becomes
N[ ˆH]
=
1
2
Z
d3p EpN[ˆapˆa†
p + ˆa†
pˆap]
=
1
2
Z
d3pEp2ˆa†
pˆap
=
Z
d3p Epˆnp,
(11.38)
where ˆnp = ˆa†
pˆap is the number operator. Acting on a state it tells you
how many excitations there are in that state with momentum p.
We now have a Hamiltonian operator that makes sense. It turns out
that the Hamiltonian for the scalar ﬁeld theory is exactly that which we
obtained for independent particles in Chapter 3. This isn’t so surprising
since we started with a Lagrangian describing waves that didn’t interact.
What we’ve seen though, is that the excited states of the wave equation
can be thought of as particles possessing quantized momenta. These
particles could be called scalar phions.7 They are Bose particles with
7This is because they are excitations of
the scalar ﬁeld φ (Greek letter ‘phi’).
spin8 S = 0.
8We shall see that spin information
is encoded by multiplying the creation
and annihilation operators by objects
that tell us about the particle’s spin
polarization. These objects are vectors
for S = 1 particles and spinors for a
S = 1/2 particles. For S = 0 they do
not feature.
Example 11.5
We have claimed that the vacuum energy is unobservable, so may be ignored. How-
ever changes in vacuum energy terms are physically signiﬁcant and lead to measurable
eﬀects. Such a change results if you adjust the boundary conditions for your ﬁeld
and this is the basis of the Casimir eﬀect.9 This is a small, attractive force be-
9The eﬀect was predicted by the Dutch
physicist Hendrik Casimir (1909–2000).
tween two closely spaced metal plates, which results from the vacuum energy of the
electromagnetic ﬁeld. The essential physics can be understood using a toy model10
10This approach follows Zee. A more
detailed treatment may be found in
Itzykson and Zuber, Chapter 3.
involving a massless scalar ﬁeld in one dimension.
x
L −x
I
III
II
Fig. 11.2 Two metal plates (I and II)
separated by a distance L, with a third
plate (III) inserted in between.
Consider two metal plates I and II separated by a distance L. We put a third
plate (III) in between them, a distance x from plate I (see Fig. 11.2). We will derive
the force on plate III resulting from the ﬁeld on either side of it. The presence of the
plates forces the ﬁeld to be quantized according to kn = nπ/x or nπ/(L −x). The
dispersion is En = kn and so the total zero-point energy is given by
E =
∞
X
n=1
» 1
2
“nπ
x
”
+ 1
2
„
nπ
L −x
«–
= f(x) + f(L −x),
(11.39)
that is, 1
2ℏωn per mode.11 These sums both diverge, just as we expect since we are
11Remember, for photons ωn = ckn,
and the zero-point energy is 1
2ℏωn, so in
units in which ℏ= c = 1 this becomes
1
2 kn, and so E = P
n
1
2kn.
Equa-
tion 11.39 contains two such sums.
evaluating the inﬁnite vacuum energy.
However, real plates can’t reﬂect radiation of arbitrarily high frequency: the high-
est energy modes leak out. To take account of this we cut oﬀthese high-energy modes
thus:
nπ
2x →nπ
2x e−nπa/x,
(11.40)
which removes those modes with wavelength signiﬁcantly smaller than the cut-oﬀ
a.
The value of this cut-oﬀis arbitrary, so we hope that it won’t feature in any
measurable quantity.

106
Canonical quantization of ﬁelds
The sums may now be evaluated:
f(x)
=
X
n
nπ
2x e−nπa/x
=
1
2
∂
∂a
X
n
e−nπa/x
=
1
2
∂
∂a
1
1 −eπa/x
=
π
2x
eπa/x
(1 −eπa/x)2 ≈
x
2πa2 −
π
24x + O(a2).
(11.41)
The total energy between plates I and II is
E = f(x) + f(L −x) =
L
2πa2 −π
24
„ 1
x −
1
L −x
«
+ O(a2).
(11.42)
If x is small compared to L, then we ﬁnd a force
F = −∂E
∂x = −
π
24x2 ,
(11.43)
which is independent of a, as we hoped. Thus, there is an attractive force between
the closely spaced plates I and III. This is the Casimir force. We can understand
this force intuitively by realizing that as the two plates are pulled together we lose
the high-energy modes.
This reduces the energy between the plates and leads to
an attractive force. A more quantum-ﬁeld-theory friendly interpretation is that the
eﬀect results from quantum ﬂuctuations in the vacuum, in which particles are spon-
taneously created and annihilated. These processes give rise to the vacuum Feynman
diagrams described in Chapter 19.
11.5
The meaning of the mode expansion
In the next chapter we’ll turn the crank on our canonical quantization
machine for the second simplest ﬁeld theory that we can imagine: that
of the complex scalar ﬁeld. Before doing that, let’s have a closer look at
the mode expansion for this theory. Our ﬁrst guess for the ﬁeld operator
might have been the simple Fourier transform
ˆφ−(x) =
Z
d3p
(2π)
3
2
1
(2Ep)
1
2 ˆape−ip·x,
(11.44)
which looks like the one we had in Chapter 4, where the e−ip·x tells
us that the particles are incoming. Unfortunately, this expansion won’t
work for the relativistic theory we’re considering. The problem is the
existence of negative energy solutions in the relativistic equations of
motion: each momentum state gave rise to two energies Ep = ±(p2+m2)
and we can’t just leave out half of the solutions.12 Looking back at our
12Why not? The result would be a ﬁeld
for which [ˆφ(x), ˆφ(y)] ̸= 0 for space-like
|x −y|.
See Weinberg, Chapter 5 for
the details.
discussion of the Klein–Gordon equation we saw the resolution of this
problem, and we’ll employ the same solution here.
What we need is a mode expansion that includes these negative energy
modes
ˆφ(x) =
X
p

positive Ep
mode

+
X
p

negative Ep
mode

.
(11.45)

11.5
The meaning of the mode expansion
107
The expansion is carried out in terms of incoming plane waves e−ip·x.
Now recall Feynman’s interpretation of the negative frequency modes in
which negative energies are assumed to be meaningless, all energies are
set to be positive and the signs of three-momenta for such modes are
ﬂipped. In this picture, the formerly negative energy states are inter-
preted as outgoing antiparticles. The positive energy modes continue to
represent incoming particles. Incoming here means, not only a factor
e−ip·x, but also that the particle is annihilated: it comes into the system
and is absorbed by it. Conversely, outgoing means a factor eip·x and
that the particle is created. We therefore interpret the mode expansion
as
φ(x) =
X
p
 incoming positive Ep
particle annihilated

+
X
p
 outgoing positive Ep
antiparticle created

.
(11.46)
The resulting expansion of a ﬁeld annihilation operator is then
ˆφ(x) =
Z
d3p
(2π)
3
2
1
(2Ep)
1
2

ˆape−ip·x + ˆb†
peip·x
,
(11.47)
where the particle and antiparticle energies are given by Ep = +(p2 +
m2)
1
2 . The rules are that ˆap annihilates particles, ˆa†
p creates particles
and the operators ˆbp and ˆb†
p respectively destroy and create antiparticles.
(For our scalar ﬁeld theory the particles and antiparticles are the same:
we say that ‘each particle is its own antiparticle’. This means that ˆap
can be thought of as either an operator that annihilates a particle or one
that annihilates an antiparticle.)13
13As we will see, in other theories we
will need separate operators to perform
these two distinct roles, but in scalar
ﬁeld theory there is no diﬀerence. Thus
in this case ˆap = ˆbp, because in a scalar
ﬁeld theory a particle is its own antipar-
ticle: particles and antiparticles are the
same. This will not necessarily be the
case for other ﬁeld theories, an example
of which will be described in the next
chapter.
Example 11.6
Let’s see what happens if we hit the vacuum state with our ﬁeld annihilation operator.
Since ˆa†
p|0⟩= |p⟩we have
ˆφ(x)|0⟩=
Z
d3p
(2π)
3
2 (2Ep)
1
2
eip·x|p⟩.
(11.48)
We have therefore created an outgoing superposition of momentum states. Pick out
one of these states and make an amplitude by folding in a relativistically normalized
state ⟨q| = (2π)
3
2 (2Ep)
1
2 ⟨q|:
(2π)
3
2 (2Ep)
1
2 ⟨q|ˆφ(x)|0⟩=
Z
d3p eip·x⟨q|p⟩=
Z
d3p eip·xδ(3)(q −p) = eiq·x.
(11.49)
In the language of second quantization, eiq·x tells us how much amplitude there is in
the qth momentum mode if we create a scalar particle at spacetime point x. Note
that eip·x ≡ei(Ept−p·x) and so
Z
d3p eip·xδ(3)(q −p) = ei(Eqt−q·x) = eiq·x,
(11.50)
demonstrating once again that our integral over three-momentum coordinates nev-
ertheless results in a single particle in a four-momentum state.

108
Canonical quantization of ﬁelds
It is important to note that canonical quantization will not succeed
in diagonalizing all ﬁeld theories. Roughly speaking it works only for
those Lagrangians which can be written as quadratic in a ﬁeld and its
derivatives.14 (We will revisit the mathematics of this point in Chap-
14or, for the complex valued ﬁelds, as
bilinear in the ﬁeld and its adjoint.
ter 23.) The result of canonical quantization is a system described by
single particles in momentum states which don’t interact with each other.
For this reason Lagrangians that may be canonically quantized are called
non-interacting theories. In contrast, those theories which cannot be
diagonalized with canonical quantization are called interacting theo-
ries; these will be described in terms of single particles in momentum
states which do interact with each other. Interacting theories are the
subject of much of the rest of this book.
For now we will continue
to look at non-interacting theories and in the next chapter we examine
some more uses of the canonical quantization routine that we have built.
Canonical
quantization
for
a
scalar ﬁeld:
I Write down a Lagrangian den-
sity L.
II Evaluate the momentum den-
sity Πµ(x) = ∂L/∂(∂µφ) and
the Hamiltonian density
H = Π0∂0φ −L.
III Turn ﬁelds into operators and
enforce the commutation rela-
tion [ˆφ(x), ˆΠ0(y)] = iδ(3)(x−y)
at equal times.
IV Express the ﬁeld in terms of a
mode expansion of the form
ˆφ(x) =
Z
d3p
(2π)
3
2
1
(2Ep)
1
2
×
“
ˆape−ip·x + ˆa†
peip·x”
,
where Ep = +(p2 + m2)
1
2 .
V Evaluate
ˆH and normal order
the result, leading to an expres-
sion like:
N[ ˆH] =
Z
d3p Epˆa†
pˆap.
Chapter summary
• Canonical quantization is an automated method for turning a clas-
sical ﬁeld theory into a quantum ﬁeld theory.
• Field operators are formed from mode expansions with amplitudes
made of creation and annihilation operators.
• Normal ordering is needed to make sense of quantum ﬁeld theories.
• The results of each step of the canonical quantization procedure
for the scalar ﬁeld theory are shown in the box in the margin.
Exercises
(11.1) One of the criteria we had for a successful theory
of a scalar ﬁeld was that the commutator for space-
like separations would be zero. Let’s see if our scalar
ﬁeld has this feature. Show that
[ˆφ(x), ˆφ(y)]=
Z
d3p
(2π)3
1
2Ep
“
e−ip·(x−y)−e−ip·(y−x)”
.
(11.51)
For space-like separation we are able to swap (y−x)
in the second term to (x −y). This gives us zero,
as required.
(11.2) Show that, at equal times x0 = y0,
[ˆφ(x), ˆΠ0(y)]= i
2
Z
d3p
(2π)3
“
eip·(x−y)+e−ip·(x−y)”
.
(11.52)
In this expression there’s nothing stopping us swap-
ping the sign of p in the second term, and show that
this leads to
[ˆφ(x), ˆΠ0(y)] = iδ(3)(x −y).
(11.53)

12
Examples of canonical
quantization
12.1 Complex scalar ﬁeld theory
109
12.2 Noether’s current for com-
plex scalar ﬁeld theory
111
12.3 Complex scalar ﬁeld theory
in the non-relativistic limit
112
Chapter summary
115
Exercises
116
Examples are like cans of over-strength lager. One is both
too many and never enough.
Anonymous
In this chapter we will apply the methods from the previous chapter to
the complex scalar ﬁeld. This theory illustrates several ideas that are
very important in quantum ﬁeld theory. We will also ﬁnd the Noether
current and ﬁnally, we will take the non-relativistic limit of the theory.
This last point is particularly important for thinking about applications
to condensed matter, where much of the physics is ﬁrmly in the non-
relativistic domain.
12.1
Complex scalar ﬁeld theory
The Lagrangian for the complex scalar ﬁeld1 is an example of a the-
1Recall where it came from. We con-
sidered the addition of two scalar ﬁelds
L
=
1
2 [∂µφ1(x)]2 −1
2m2 [φ1(x)]2
+ 1
2 [∂µφ2(x)]2 −1
2m2 [φ2(x)]2 .
We
noticed
that
we
can
make
a
transformation that simpliﬁed this La-
grangian. We deﬁned
ψ =
1
√
2 [φ1(x) + iφ2(x)],
ψ† =
1
√
2 [φ1(x) −iφ2(x)],
and we obtained the Lagrangian in
eqn 12.1. This Lagrangian looks rather
like that of the massive scalar ﬁeld, but
without the factors of 1/2.
ory with more than one component. Here we have two: ψ(x) and ψ†(x).
We’ll now canonically quantize this Lagrangian using the procedure pre-
sented in Chapter 11.
Step I: We start by writing down the Lagrangian we want to quantize.
The Lagrangian for complex scalar ﬁeld theory is
L = ∂µψ†(x)∂µψ(x) −m2ψ†(x)ψ(x).
(12.1)
Step II: Identify the momentum density and write down the Hamilto-
nian. Each component2 σ of the ﬁeld (i.e. σ = ψ and ψ†) has a diﬀerent
2Note carefully that σ isn’t a tensor
component, it simply labels the compo-
nent of the ﬁeld: ψ or ψ†. This implies
that Πµσ = Πµ
σ.
momentum density. The momentum of the ψ-ﬁeld and of the ψ†-ﬁeld are
given, as usual by the time-like (zeroth) component of the momentum
density Πµ
σ:
Π0
σ=ψ =
∂L
∂(∂0ψ) = ∂0ψ†,
Π0
σ=ψ† =
∂L
∂(∂0ψ†) = ∂0ψ.
(12.2)
Now that we have the momenta we can write the Hamiltonian. For the
case of more than one ﬁeld component we can write down
H
=
X
σ
Π0
σ(x)∂0ψσ(x) −L
=
∂0ψ†(x)∂0ψ(x) + ∇ψ†(x) · ∇ψ(x) + m2ψ†(x)ψ(x). (12.3)

110
Examples of canonical quantization
Step III: Make the ﬁelds into quantum mechanical operators. We
promote the ﬁelds to operators and impose equal-time commutation re-
lations thus
h
ˆψ(t, x), ˆΠ0
ψ(t, y)
i
=
h
ˆψ†(t, x), ˆΠ0
ψ†(t, y)
i
= iδ(3)(x −y),
(12.4)
and all other commutators vanish.
Step IV: Expand the ﬁelds in terms of modes. For the case of the
real scalar ﬁeld, we found that the ﬁeld operator3 ˆφ(x) =
R
p(ˆape−ip·x +
3Here we abbreviate
R
p ≡
R
d3p
(2π)
3
2
1
(2Ep)
1
2 .
ˆa†
peip·x), so that it is Hermitian [i.e. self-adjoint: ˆφ†(x) = ˆφ(x)]. For our
complex scalar ﬁeld there is no reason why the ﬁeld operator should be
Hermitian. The appropriate mode expansion of the complex scalar ﬁeld
operators take the more general form
ˆψ(x)
=
Z
d3p
(2π)
3
2
1
(2Ep)
1
2

ˆape−ip·x + ˆb†
peip·x
,
ˆψ†(x)
=
Z
d3p
(2π)
3
2
1
(2Ep)
1
2

ˆa†
peip·x + ˆbpe−ip·x
,
(12.5)
where Ep = +(p2 + m2)
1
2 . Here the operators ˆap and ˆbp annihilate
two diﬀerent types of particle. They satisfy the commutation relations

ˆap, ˆa†
q

=
h
ˆbp,ˆb†
q
i
= δ(3)(p −q) and all others combinations vanish.
Substituting these into the Hamiltonian we ﬁnd that, after some algebra
and then (Step V) normal ordering, that we obtain:
N
h
ˆH
i
=
Z
d3p Ep

ˆa†
pˆap + ˆb†
pˆbp

=
Z
d3p Ep

ˆn(a)
p
+ ˆn(b)
p

,
(12.6)
where ˆn(a)
p
counts the number of a-particles with momentum p and, sim-
ilarly, ˆn(b)
p
counts the number of b-particles. We notice that the a and b
particles have the same energy Ep, and so we interpret a and b as par-
ticles and antiparticles respectively. This is justiﬁed in the next section.
Thus ˆψ(x) involves a sum over all momenta of operators that annihilate
particles (ˆap) and create antiparticles (ˆb†
p). We therefore understand
these new operators as follows:
• ˆa†
p is the creation operator for a particle with momentum p,
• ˆb†
p is the creation operator for an antiparticle with momentum p.
We’ve quantized the theory and we’ve found that the excitations of the
ﬁeld are scalar particles and antiparticles.
It is worthwhile justifying how we have been able to treat ψ and ψ†
as if they are independent ﬁelds. We began with
ψ(x) =
1
√
2[φ1(x) + iφ2(x)]
and
ψ†(x) =
1
√
2[φ1(x) −iφ2(x)], (12.7)

12.2
Noether’s current for complex scalar ﬁeld theory
111
and this implies that
φ1 =
1
√
2[ψ + ψ†]
and
φ2 = −i
√
2[ψ −ψ†].
(12.8)
Thus all we have done in moving from a description based on φ1 and φ2
to one based on ψ and ψ† is to perform a change of basis. Allowing the
action S to vary with respect to ψ and ψ† gives as much freedom as if
we were varying φ1 and φ2. For example, we can adjust φ1 alone using
δψ = δψ† and we can adjust φ2 alone using δψ = −δψ†.
12.2
Noether’s current for complex scalar
ﬁeld theory
The complex scalar ﬁeld has an internal U(1) symmetry.4 This means
4Internal symmetries are discussed in
more detail in the next chapter.
that the global transformations of the ﬁelds
ψ →eiαψ,
ψ† →e−iαψ†
(12.9)
have no eﬀect on the Lagrangian. Remember that Noether’s theorem
says that every symmetry begets a conserved current. This is our ﬁrst
opportunity to see what is conserved for an internal symmetry. In fact,
we will see that the conserved Noether charge is particle number.
To get to Noether’s current for the internal U(1) symmetry, it’s easiest
to write out the transformation for an inﬁnitesimal change in the phase
α:
ψ →ψ + iψ δα,
Dψ = +iψ,
ψ† →ψ† −iψ†δα,
Dψ† = −iψ†.
(12.10)
Note that DL = 0 and since DL = ∂µW µ this implies that we can take5
5The property W µ = 0 turns out to be
true of all internal symmetries.
W µ = 0. The Noether current is therefore given by Jµ
N = P
σ Πµ
σDσ,
(because W µ = 0) where the index σ again labels the component of the
ﬁeld: in this case whether it’s σ = ψ or ψ†. As a result we have
Jµ
N
=
X
σ
Πµ
σDσ = Πµ
ψDψ + Πµ
ψ†Dψ†
=
i
 ∂µψ†
ψ −(∂µψ)ψ†
.
(12.11)
We may make this a Noether current operator ˆJµ
N by upgrading all of our
ﬁelds into ﬁeld operators. This creates an ordering ambiguity in writing
the ﬁelds and their derivatives. We will remove this ambiguity by normal
ordering. Substituting the mode expansion for the ﬁelds allows us to ﬁnd
the conserved charge operator ˆQN.
Example 12.1
The charge operator is
ˆQN =
Z
d3x ˆJ0
N = i
h“
∂0 ˆψ†”
ˆψ −(∂0 ˆψ) ˆψ†i
.
(12.12)

112
Examples of canonical quantization
Of course, there’s the ordering ambiguity here, but we’ll press on and insert the mode
expansion to yield
ˆQN = 1
2
Z
d3p
“
−ˆa†
pˆap + ˆbpˆb†
p −ˆapˆa†
p + ˆb†
pˆbp
”
.
(12.13)
Finally, we normal order to give a conserved charge of
N
h
ˆQN
i
=
Z
d3p
“
ˆb†
pˆbp −ˆa†
pˆap
”
.
(12.14)
Noether’s theorem: U(1) internal
symmetry
Dψ = iψ
Dψ† = −iψ†
Πµ
ψ = ∂µψ†
Πµ
ψ† = ∂µψ
DL = 0
W µ = 0
Jµ
N = i
ˆ
(∂µψ†)ψ −(∂µψ)ψ†˜
ˆQNc = −N
h
ˆQN
i
=
R
d3p (ˆn(a)
p
−ˆn(b)
p ).
The conserved charge is given by the diﬀerence in the number of an-
tiparticles n(b)
p
and the number of particles n(a)
p . This reason for this is
quite simple. Particles, which are excitations in the ﬁeld, carry Noether
charge of one sign. To have a conserved charge we need other sorts of
particle excitation carrying negative charge to cancel out the contribu-
tion from the particles. We are left with the conclusion that the reason
for the existence of antiparticles is that charge wouldn’t be conserved if
they didn’t exist.
Obviously, if a current Jµ
N is conserved, then so is −Jµ
N and so our
assignment of which excitations carry positive Noether charge and which
are negative is quite arbitrary. It is only when we attach some observable
property, such as electric charge q, to each that we decide which is which.
By convention, the number current ˆJµ
Nc is deﬁned to be positive for
particles (created by ˆa†
p operators) and negative for antiparticles (created
by ˆb†
p operators) and so we will deﬁne6 ˆJµ
Nc = −N
h
ˆJµ
N
i
, ˆQNc =
R
d3x ˆJ0
Nc
6With a subscript c for conventional,
just like conventional current direction
in circuit theory!
and therefore ˆQNc = −N[ ˆQN]. Hence using eqn 12.14 and recalling that
ˆn(a)
p
= ˆa†
pˆap and ˆn(b)
p
= ˆb†
pˆbp, we can conclude that
ˆQNc =
Z
d3p

ˆn(a)
p
−ˆn(b)
p

.
(12.15)
12.3
Complex scalar ﬁeld theory in the
non-relativistic limit
In the ﬁnal part of this chapter, we will road test our new complex scalar
ﬁeld theory and see how it works out in the non-relativistic limit and
whether or not it will regenerate the familiar results of non-relativistic
quantum mechanics.
In the non-relativistic domain the excitation energies of particles are
small compared to the particle mass contribution mc2, where here we’ll
temporarily reinstate factors of c and ℏ. This means that the mass term
Ep=0 = mc2 provides by far the largest fraction of the energy of an
excitation, that is E = mc2 + ε, where ε is small. To make a theory
non-relativistic, the strategy is to replace the relativistic ﬁeld φ with
φ(x, t) →Ψ(x, t)e−imc2t/ℏ,
(12.16)
allowing us to factor out the enormous rest energy.

12.3
Complex scalar ﬁeld theory in the non-relativistic limit
113
Example 12.2
To get an idea of how this works, we’ll take the non-relativistic limit of the Klein–
Gordon equation. We start (in natural units) with
(∂2 + m2)Ψ(x, t)e−imt = 0,
(12.17)
and moving to unnatural units we have
„
ℏ2 ∂2
∂t2 −ℏ2c2∇2 + m2c4
«
Ψ(x, t)e−imc2t/ℏ= 0.
(12.18)
The ﬁrst term on the left yields
ℏ2 ∂2
∂t2 Ψ(x, t)e−imc2t/ℏ= ℏ2
„∂2Ψ
∂t2 −2imc2
ℏ
∂Ψ
∂t −m2c4
ℏ2
Ψ
«
e−imc2t/ℏ,
(12.19)
and substitution gives us
ℏ2 ∂2Ψ
∂t2 −2imc2ℏ∂Ψ
∂t −ℏ2c2∇2Ψ = 0.
(12.20)
Lastly we notice that the ﬁrst term, with its lack of factors of c2, will be much smaller
than the other two. We can therefore drop the ﬁrst term and we end up with
iℏ∂
∂tΨ(x, t) = −ℏ2
2m ∇2Ψ(x, t).
(12.21)
The result is, of course, that we recover the Schr¨odinger equation for a free particle.
To take the non-relativistic limit of the complex scalar ﬁeld Lagrangian7
7We’ll use the full interacting form
L = ∂µψ†(x)∂µψ(x) −m2ψ†(x)ψ(x) −
g[ψ†(x)ψ(x)]2 and return to our con-
vention that ℏ= c = 1.
we substitute in ψ =
1
√
2me−imtΨ (with an extra normalization factor
1/
√
2m added with malice aforethought).
Example 12.3
As in the last example, the guts of taking the limit may be found in the time deriva-
tives. We ﬁnd
∂0ψ†∂0ψ =
1
2m
h
∂0Ψ†∂0Ψ −im
“
Ψ†∂0Ψ −(∂0Ψ†)Ψ
”
+ m2Ψ†Ψ
i
.
(12.22)
The ﬁrst term going as 1/m is negligible in comparison with the others. The third
term cancels against the mass term in the original Lagrangian. The dynamic part
of the theory is therefore contained in the second term.
We can see that a time
derivative of Ψ (which we may expand in plane wave modes as something like
P ape−ip·x) will bring down a factor −iEp. Since Ψ† is obtained from Ψ through
taking an adjoint, the time derivative of Ψ† will therefore involve bringing down a
factor iEp. The result is that (Ψ†∂0Ψ −Ψ∂0Ψ†) may be replaced8 by −2Ψ†∂0Ψ.
8We could have replaced this with
2Ψ(∂0Ψ†) if we had chosen modes vary-
ing as e+ip·x, but we choose to favour
matter over antimatter with our choice.
After these manipulations, we end up with
L = iΨ†(x)∂0Ψ(x) −1
2m∇Ψ†(x) · ∇Ψ(x) −g
2[Ψ†(x)Ψ(x)]2,
(12.23)
with g = λ/2m2.
This looks a lot less neat and covariant than the
relativistic form, because of the asymmetry between the matter and an-
timatter ﬁelds that resulted from the choice we made in simplifying the
time derivative term. This is acceptable because life is a lot less sym-
metrical in the non-relativistic world, but we will see the consequences
later. Anyway, we will now continue and feed this Lagrangian into the
canonical quantization machine.

114
Examples of canonical quantization
Example 12.4
Let’s attempt to quantize the non-relativistic Lagrangian, without the self-interaction
(g = 0), but in the presence of an external potential V (x) which enters as V Ψ†Ψ.
There’s nothing for it but to press on with the ﬁve point programme.
In this example we begin to see some
of the limitations of canonical quantiza-
tion. For a general potential V (x) the
procedure does not return a diagonal-
ized Hamiltonian (although it will work
for a constant potential V (x) = V0).
We will see in Chapter 16 how more
general potentials may be dealt with us-
ing perturbation theory.
Step I: The Lagrangian density with the external potential is given by
L = iΨ†(x)∂0Ψ(x) −
1
2m∇Ψ†(x) · ∇Ψ(x) −V (x)Ψ†(x)Ψ(x).
(12.24)
Feeding this through the Euler–Lagrange equations yields the Schr¨odinger equation
and, for V (x) = 0, a dispersion Ep =
p2
2m (Exercise 12.5). This only has positive
energy solutions so we might expect that the negative frequency part of the mode
expansion won’t be needed here. This will turn out to be the case.
Step II: We calculate the momentum densities thus:
Π0
Ψ =
∂L
∂(∂0Ψ) = iΨ†,
Π0
Ψ† =
∂L
∂(∂0Ψ†) = 0.
(12.25)
Notice that there’s no momentum density conjugate to the ﬁeld Ψ†, which is the
consequence of our choice in Example 12.3. The momentum allows us to calculate
the Hamiltonian density:
H
=
Π0
Ψ∂0Ψ −L
=
1
2m∇Ψ†(x) · ∇Ψ(x) + V (x)Ψ†(x)Ψ(x).
(12.26)
Lo and behold, we have obtained a Schr¨odinger-like equation for the Hamiltonian
density.
Step III: The equal-time commutation relation between the position and momen-
tum operators is
h
ˆΨ(t, x), ˆΠ0
Ψ(t, y)
i
=
iδ(3)(x −y),
(12.27)
h
ˆΨ(t, x), ˆΨ†(t, y)
i
=
δ(3)(x −y),
where the second equality follows from inserting our expression Π0
Ψ = iΨ†.
Step IV: A mode expansion with positive and negative frequency parts won’t
obey the commutation relation above. The mode expansion which does the trick is
simply
ˆΨ(x) =
Z
d3p
(2π)
3
2
ˆape−ip·x,
(12.28)
with Ep = p2
2m .
Step V: Substituting the mode expansion gives us a Hamiltonian operator
ˆH =
Z
d3p
„ p2
2m ˆa†
pˆap
«
+
Z
d3x d3p d3q
(2π)3
“
V (t, x)e−i(Ep−Eq)tei(p−q)·xˆa†
pˆaq
”
.
(12.29)
The time-dependent part will guarantee conservation of energy. It forces the potential
to have a time dependence of the form V (t, x) = ei(Ep−Eq)tV (x) if the momentum
of the particle is going to change. With this constraint we obtain a Hamiltonian
ˆH =
Z
d3p
„ p2
2m ˆa†
pˆap
«
+
Z
d3p d3q
“
˜V (p −q)ˆa†
pˆaq
”
,
(12.30)
where ˜V (p−q) =
R
d3x
1
(2π)3 V (x)ei(p−q)·x and the potential must impart an energy
Ep−Eq. Notice that this is equivalent to the Hamiltonian we argued for in Chapter 4.
We can look at the non-relativistic complex scalar ﬁeld in another way
by considering a diﬀerent form for the ﬁeld. We ﬁrst note that the non-
relativistic form of the Lagrangian is still invariant with respect to global
U(1) transformations. Motivated by this, we write
Ψ(x) =
p
ρ(x)eiθ(x),
(12.31)

12.3
Complex scalar ﬁeld theory in the non-relativistic limit
115
so that Ψ(x) is written in terms of amplitude and phase.
The U(1)
transformation is now enacted by θ →θ + α (see the box for the con-
served charges). Whereas before we had two ﬁelds, φ1(x) and φ2(x),
Noether’s theorem: U(1) internal
symmetry
Dθ = 1
Π0
θ = −ρ
Πi
θ =
ρ
m∂iθ
DL = 0
W µ = 0
J0
N = −ρ(x)
JN = −ρ
m ∇θ
QNc =
R
d3x ρ(x)
JNc =
ρ
m∇θ
now we have two ﬁelds, ρ(x) and θ(x). Substituting in the Lagrangian
in eqn 12.23, we obtain (Step I) the Lagrangian in terms of ρ and θ
ﬁelds
L = i
2∂0ρ −ρ∂0θ −1
2m
 1
4ρ(∇ρ)2 + ρ(∇θ)2)

−g
2ρ2.
(12.32)
Now we turn oﬀinteractions (by setting g = 0) and feed the resulting
Lagrangian into the canonical quantization machine and make selective
use of its features. First we ﬁnd the momenta (Step II):
Π0
ρ(x)
=
∂L
∂(∂0ρ(x)) = i
2,
Π0
θ(x)
=
∂L
∂(∂0θ(x)) = −ρ(x),
(12.33)
from which we see that the Π0
θ is most interesting since it’s not a con-
stant.
Let’s make this quantum mechanical (Step III) by imposing
commutation relations
h
ˆθ(x, t), ˆΠ0
θ(y, t)
i
= −
h
ˆθ(x, t), ˆρ(y, t)
i
=
iδ(3)(x −y).(12.34)
The box shows that the time-like component of the conserved current
is ρ(x). We therefore deﬁne the total number of particles as ˆN(t) =
R
d3x ˆρ(x, t) and by integrating9 the commutation relation equation we
9This calculation simply uses
Z
d3y [ˆρ(y, t), ˆθ(x, t)] = [ ˆ
N(t), ˆθ(x, t)]
and
Z
d3y δ(3)(x −y) = 1.
obtain
h
ˆN(t), ˆθ(x, t)
i
= i,
(12.35)
which is an important result.10 It tells us that in the sort of coherent
10Equation 12.35 is called the number-
phase uncertainty relation.
condensed matter system that can be described by the non-relativistic
complex scalar ﬁeld theory the operator for the number of excitations
is conjugate to the operator for the phase angle of the ﬁeld. We will
return to this important idea when we come to discuss superﬂuids and
superconductors.
Chapter summary
• Quantization of the complex scalar ﬁeld leads to two types of ex-
citation: particles and antiparticles. The conserved charge is the
number of particles minus the number of antiparticles.
• The non-relativistic limit of the theory may be taken. From this
we can derive a number-phase uncertainty relation.

116
Examples of canonical quantization
Exercises
(12.1) Fill in the missing steps of the algebra that led to
eqn 12.6.
(12.2) Evaluate (a) [ ˆψ(x), ˆψ†(y)] and (b) [ˆΨ(x), ˆΨ†(y)] us-
ing the appropriate mode expansions in the text.
(12.3) Consider the Lagrangian for two scalar ﬁelds from
Section 7.5.
(a) Evaluate [ ˆQN, ˆφ1] and (b) [ ˆQN, ˆφ2], where ˆQN is
the Noether charge.
Hint: You could do this by brute force and evalu-
ate ˆQN and then ﬁnd the commutator. A preferable
method is just to use: [ ˆQN, ˆφi] = −iD ˆφi from Chap-
ter 10.
(c) Use these results to show that [ ˆQN, ˆψ] = ˆψ.
(12.4) Consider the theory described by the Lagrangian
in eqn 12.32. Use Noether’s theorem in the form
[ ˆQN, ˆφ] = −iD ˆφ to provide an alternative deriva-
tion of eqn 12.35.
(12.5) Apply the Euler–Lagrange equations to eqn 12.24
and show that it yields Ep = p2/2m when V = 0.
(12.6) Find the Noether current for the Lagrangian in
eqn 12.24.
Check that it is what you expect for
non-relativistic quantum mechanics.
(12.7) Consider the complex scalar ﬁeld again. The inter-
nal transformation operator may be written ˆU(α) =
ei ˆ
QNcα, where ˆQNc is the conserved number charge
operator. Show that ˆU †(α) ˆψ(x) ˆU(α) = eiα ˆψ(x).

13
Fields with many
components and massive
electromagnetism
13.1 Internal symmetries
117
13.2 Massive
electromagnetism
120
13.3 Polarizations
and
projec-
tions
123
Chapter summary
125
Exercises
125
In the previous chapter we described a theory with two ﬁeld components.
Here we expand the discussion to theories with more components and
we will see that the interesting thing about these components is their
symmetries. We’ll examine two cases, the ﬁrst of which has an internal
symmetry and a second where the theory is described in terms of four-
vectors.
13.1
Internal symmetries
When we ﬁnd families of closely-related particles in Nature, there is
an inevitable lurking suspicion that these particles are really a single
physical entity but that there is some internal dial which can be rotated
to turn one member of the family into another. The invariance of the
Lagrangian with respect to turns on this internal dial is described as an
internal symmetry, quite unrelated to the symmetry of spacetime in
which the particles live, move and have their being.1
1We saw the consequence of a U(1) in-
ternal symmetry in the complex scalar
ﬁeld in the last chapter.
An early example of the idea of an internal symmetry was isospin.
The neutron and proton have almost the same mass (mp ≈mn) and this
led Heisenberg to wonder if they were the same particle in two diﬀerent
states with some kind of internal symmetry that he termed isospin. The
idea is that neutrons and protons both have isospin I = 1
2 but that, as for
conventional spin,2 there are two possible eigenvalues of the ˆIz operator:
2Note that isospin has nothing to do
with the actual spin angular momen-
tum of the particles.
Iz = 1/2 (the proton) and Iz = −1/2 (the neutron). By analogy with
spin angular momentum, we may arrange these into a two-component
object

p
n

, known as an isospin doublet. We can rotate these isospin
doublets with the same matrices3 that rotate spins-1/2. Before we get
3As explored in Chapter 37, these rota-
tions form a representation of the group
SU(2).
too carried away, we should remember that, strictly, mp ̸= mn so that
isospin is only an approximate description.4
4Of course protons and neutrons are ac-
tually each composed of quarks.
The idea of an internal symmetry nevertheless has wide applicability.
We will explore this idea using the example of three scalar particles. We
imagine arranging them into a vector (t, d, h) which transforms internally
according to the three-dimensional rotation group SO(3). Rotating the
internal dial by 90◦about the h-axis turns a t particle, denoted (1, 0, 0),
into a d particle (0, 1, 0) (see Fig. 13.1). This also implies that super-

118
Fields with many components and massive electromagnetism
positions of particles, obtained by some arbitrary rotation of the dial,
have just as much validity as the original particles do. Since particles
are excitations in ﬁelds, we should be able to examine isospin in ﬁeld
theory. In the ﬁeld case we arrange the ﬁelds that generate these par-
ticles into a vector such as (φ1, φ2, φ3). Subjecting this column vector
to internal rotations by turning the internal dial, one can carry out the
equivalent act of turning a φ1 ﬁeld into a φ2 ﬁeld by rotating about
the φ3-axis. If the Lagrangian describing the theory of these particles is
invariant with respect to these rotations, then Noether’s theorem gives
us a conservation law describing the charges of the ﬁelds.
To simplify our notation, let’s write the three scalar ﬁelds as a column
vector Φ(x) as follows
Φ(x) =


φ1(x)
φ2(x)
φ3(x)

.
(13.1)
The free Lagrangian (step I of the Canonical quantization process) for
I’m an h particle!
I’m just a d.
My internal pointer
says I’m a t.
Fig. 13.1 In our toy model, particles
can be imagined as having an internal
dial.
Whether the dial points along
(1, 0, 0), (0, 1, 0) or (0, 0, 1) determines
which kind of particle it is.
The po-
sition of the dial is not related to the
directions in spacetime through which
particles move, but expresses a direc-
tion in abstract space describing the in-
ternal symmetry.
this theory is given by
L = 1
2(∂µΦ) · (∂µΦ) −m2
2 Φ · Φ,
(13.2)
which is short for
L = 1
2

(∂µφ1)2 −m2φ2
1 + (∂µφ2)2 −m2φ2
2 + (∂µφ3)2 −m2φ2
3

, (13.3)
that is, the sum of the Lagrangians for the individual ﬁelds. Note care-
fully that the object Φ(x) is not a vector ﬁeld that lives in Minkowski
space like the vectors xµ and pµ do.
This point is examined in the
following example.
Example 13.1
Here, unlike elsewhere, the dot products aren’t short for a sum gµνAµAν.
The
vector Φ doesn’t live in Minkowski space at all, so has no dealings with the tensor
gµν. Instead, the dot products here are short for
Φ · Φ = φ1φ1 + φ2φ2 + φ3φ3,
(13.4)
and
∂µΦ · ∂µΦ = ∂µφ1∂µφ1 + ∂µφ2∂µφ2 + ∂µφ3∂µφ3.
(13.5)
This is to say that the α in Φα isn’t a tensor index and there is consequently no
diﬀerence between Φα and Φα.
Our example has an SO(3) internal symmetry: we can transform the
vector Φ using a three-dimensional rotation matrix and the Lagrangian
won’t change one iota.
That is to say that there is no eﬀect on the
Lagrangian on transforming to a ﬁeld Φ′ via a transformation such as a
rotation about the φ3-axis:


φ′
1
φ′
2
φ′
3

=


cos θ
−sin θ
0
sin θ
cos θ
0
0
0
1




φ1
φ2
φ3

.
(13.6)

13.1
Internal symmetries
119
The canonical quantization for this theory proceeds as before, but we
now implement our new notation. This leads to a Hamiltonian (step
II)
Note that Πµ
α =
∂L
∂(∂µΦα) .
ˆH =
X
α
1
2(∂0 ˆφα)2 + 1
2(∇ˆφα)2 + 1
2m2 ˆφ2
α

.
(13.7)
The equal-time commutation relations (step III) become
h
ˆΦα(t, x), ˆΠ0
β(t, y)
i
= iδ(3)(x −y)δαβ,
(13.8)
including a Kronecker delta because each component of Φ has a nonzero
commutator with the same component of its own momentum ﬁeld only.
The mode expansion (step IV) is then given by
Φ(x) =
Z
d3p
(2π)
3
2
1
(2Ep)
1
2



ˆap1e−ip·x + ˆa†
p1eip·x
ˆap2e−ip·x + ˆa†
p2eip·x
ˆap3e−ip·x + ˆa†
p3eip·x


,
(13.9)
where ˆapα are the annihilation operators for the α ﬁeld and we have the
commutators [ˆapα, ˆa†
qβ] = δ(3)(p −q)δαβ and all others vanish.
Example 13.2
We could also abbreviate the notation slightly by writing this as
Φ(x) =
Z
d3p
(2π)
3
2
1
(2Ep)
1
2
3
X
α=1
hα
“
ˆapαe−ip·x + ˆa†
pαeip·x”
,
(13.10)
where
h1 =
0
@
1
0
0
1
A , h2 =
0
@
0
1
0
1
A , h3 =
0
@
0
0
1
1
A ,
(13.11)
are internal vectors that tell us about the polarization of the ﬁeld in its internal space.
Substituting into the Hamiltonian for this theory then leads5 to the
5One needs to substitute the mode ex-
pansion and normal ordering steps IV
and V.
result that
ˆH =
Z
d3p Ep
3
X
α=1
ˆa†
pαˆapα,
(13.12)
and we observe that the energy of the particles may be found by summing
over all momenta and polarizations.
Example 13.3
Noether’s theorem reminds us that a symmetry gives us a conserved quantity, so we
Noether’s theorem: SO(3) inter-
nal symmetry
DbΦa = −εabcΦc
Πµa = ∂µΦa
DL = 0
W µ = 0
Jaµ
Nc = εabcΦb(∂µΦc)
QNc =
R
d3x (Φ × ∂0Φ)
N
h
ˆQa
Nc
i
= −i
R
d3p εabcˆa†
bpˆacp.
examine the internal SO(3) symmetry of our ﬁelds. The Lagrangian is symmetric
with respect to the transformation Φ →Φ −θ × Φ. For example (and assuming
sums over repeated indices), rotations about the φ3-axis give Φa →Φa −εa3cθ3Φc
and we have
D3φ1 = φ2,
D3φ2 = −φ1,
D3φ3 = 0,
(13.13)

120
Fields with many components and massive electromagnetism
where we call the symmetry from rotation around the b-axis DbΦa. Using the fact
that, as usual, W µ = 0 for this internal transformation (since DL = 0), this yields a
Noether current J3µ
N
for rotation around the internal 3-axis:
J3µ
N
= ΠaµD3Φa = (∂µφ1)φ2 −(∂µφ2)φ1.
(13.14)
Swapping signs over, the conserved charge is given in terms of (normally ordered6)
6We dispense with the normal ordering
symbol from this point.
It should be
assumed in all calculations of energies
and conserved charges.
creation and annihilation operators by
ˆQ3
Nc = −i
Z
d3p
“
ˆa†
1pˆa2p −ˆa†
2pˆa1p
”
.
(13.15)
Rotations about the 1- and 2-axes will lead to similar conserved charges in terms of
the other ﬁeld operators. In general the conserved charge will be given by
QNc =
R
d3x (Φ × ∂0Φ)
and
ˆQa
Nc = −i
R
d3p εabcˆa†
bpˆacp.
(13.16)
This conserved charge is isospin. As shown in Exercise 13.1, isospin has the structure
of angular momentum: in this case an isospin-1. Remember that, despite the similar-
ity between the angular momentum in Exercise 3.3 (which arises since the symmetry
is with respect to spatial rotations) and the conserved charge in this example, the
symmetry here is internal and so this angular momentum-like charge is internal to
the ﬁeld.
13.2
Massive electromagnetism
We now turn to the interesting case of the massive vector ﬁeld. This
theory describes an intriguing world7 in which photons have mass! The
7The theory described in this section
is rather like quantum electrodynam-
ics but, it turns out, without the inter-
esting complication of gauge invariance,
which we leave until the next chapter.
theory is described in terms of Aµ(x) which is called a ‘vector ﬁeld’
because, when we input a spacetime point x, the ﬁeld outputs a four-
vector Aµ corresponding to that point. The key observation about Aµ
is that it lives in Minkowski space and so transforms in the same way as
a four-vector does under the Lorentz transformation.8 We’ll see that the
8This is in contrast to our example of
the internal SO(3) ﬁeld Φ(x).
particles created by the quantum version of this ﬁeld have three possible
polarizations, which will lead to the excitations carrying a spin of S = 1.
The Lagrangian for this theory (Step I) is given by
L = −1
4FµνF µν + 1
2m2AµAµ,
(13.17)
where Fµν = ∂µAν −∂νAµ. This is just the Lagrangian for the electro-
magnetic ﬁeld but with a Klein–Gordon-esque mass term on the right-
hand side. Note that the sign of the second term that contains the factor
m2 is positive. In the Klein–Gordon equation the m2 term was nega-
tive, but the reason for our sign choice here will become apparent in the
following example.
Example 13.4
We can ﬁnd the equations of motion for the ﬁeld in this theory by plugging the
Lagrangian (eqn 13.17) into the Euler–Lagrange equation. The result is that9
9Equation 13.18 can of course be writ-
ten out purely in terms of Aµ as
∂µ (∂µAν −∂νAµ) + m2Aν = 0.
∂µF µν + m2Aν = 0.
(13.18)

13.2
Massive electromagnetism
121
This is known as the Proca equation.
Taking the divergence gives us zero from the
Alexandru Proca (1897–1955) devel-
oped the theory of massive spin-1 par-
ticles using ﬁeld theory.
ﬁrst term (since ∂µ∂νF µν = 0) and we obtain from the second term m2∂νAν = 0.
Since m2 ̸= 0, we have the additional condition that the ﬁeld obeys ∂µAµ = 0. This
allows us to write the Proca equations in another form
`
∂2 + m2´
Aν = 0,
(13.19)
which looks a lot like the Klein–Gordon equation10 for each of the four components
10This shows that we did indeed make
a sensible sign choice in eqn 13.17.
of the ﬁeld Aν(x).
The dispersion relation is therefore E2
p = p2 + m2 for each
component.
We now continue the procedure of quantizing this theory.11 Finding the
11The rest of this chapter works care-
fully through the nuts and bolts of get-
ting from the Lagrangian in eqn 13.17
to
the
ﬁnal
result
of
the
Hamil-
tonian
in
eqn
13.31,
which
is
our
familiar
second-quantized
harmonic-
oscillator type Hamiltonian.
momentum density Π0
σ(x) for each ﬁeld component σ is the ﬁrst part of
Step II, which is easily done. Extending the deﬁnition of Πµ
σ(x) from
the last chapter to include the vector index on Aν, we have:
Πµν(x)
=
∂L
∂(∂µAν) = ∂νAµ −∂µAν.
(13.20)
Note that Πµν is now a second-rank tensor whose two indices can be
raised or lowered using the metric tensor gµν. We then have momentum
densities
Π00 = 0,
Π0i = ∂iA0 −∂0Ai = Ei,
(13.21)
where E is the ‘electric ﬁeld’ of the theory (a useful name for notational
simplicity, but that’s all since this is not electromagnetism!). Notice that
Π00 = 0 means that A0 has no dynamics of its own.12 We will eliminate
12This lack of dynamics might cause us
to worry that the quantization routine
won’t work on this ﬁeld component. In
fact, we will eliminate A0 and so this
lack of a momentum won’t turn out to
be a problem. This is quite unlike the
case of QED where the problem of hav-
ing Π00 = 0 is a knotty one. See Aitchi-
son and Hey, Section 7.3 for a discus-
sion.
it in the next step (and justify this in step IV). We ﬁnd the Hamiltonian
density to be13
13 In four steps:
I. Write
H
=
−E · ˙A −1
2(E2 −B2)
−m2
2
ˆ
(A0)2 −A2˜
.
II. Use eqn 13.21 along with
E · ∇A0 = ∇· (EA0) −A0(∇· E).
III. Then use the equations of motion
(eqn 13.18) to eliminate A0 via
A0 = −1
m2 ∇· E.
IV. The Hamiltonian includes a term
∇· (EA0), but this is a total deriva-
tive. We constrain all of our ﬁelds to
vanish at inﬁnity, so we may drop the
total derivative from the Hamiltonian
density.
H
=
1
2

E2 + B2 + m2A2 + 1
m2 (∇· E)2

.
(13.22)
Notice the positive contribution from the mass term, which justiﬁes the
choice of sign in the Lagrangian in eqn 13.17.
Next we need some equal-time commutation relations (step III) to
make all of this quantum mechanical. Clearly we want a way to deal with
the diﬀerent components of the vector ﬁeld which, when we substitute in
a sensible mode expansion, will yield up a total energy which is the sum
of the positive energies of all of the particles. Since we have eliminated
A0 from the Hamiltonian in eqn 13.22 we only need relations between
the space-like components (i and j). We choose
h
ˆAi(t, x), ˆEj(t, y)
i
= −iδ(3)(x −y)gij,
(13.23)
which has the form we had for previous cases (namely [φ, ∂0φ] = iδ since
gij = −δij) for each spatial component.
We may now expand the ﬁeld Aµ (step IV) in terms of plane
waves. Since these will look something like A ∼ape−ip·x, the condi-
tion ∂µAµ = 0 implies that pµAµ = 0. This tells us that the four-vector
Aµ is orthogonal to pµ, just as we have in three dimensions if c · d = 0
for vectors c and d. Moreover, the condition pµAµ = 0 introduces a con-
straint on the components of Aµ: they are no longer independent and

122
Fields with many components and massive electromagnetism
we may express one of the components of Aµ in terms of the others. The
theory therefore describes a ﬁeld (and particles) with three independent
polarization degrees of freedom. The polarization degrees of freedom
are taken to be Ai with i = 1, 2, 3. The component A0 is completely
determined via p · A = p0A0, which shows that we were justiﬁed in
eliminating it from eqn 13.22.
Now for the mode expansion. As in the case of the SO(3) ﬁelds which
we wrote as Φ, we’ll need separate creation and annihilation operators
for each polarization. Unlike that case, however, we need to multiply
each by a polarization vector14 ǫµ
λ(p) which lives in Minkowski space
14The polarization of the vector parti-
cle tells us about its spin angular mo-
mentum. Vector particles have S = 1
and the three degrees of freedom cor-
respond to Sz = 1, 0 and −1. This is
examined further in the next chapter.
and whose components depend on the value of the momentum of the
particle that we’re considering.15
15Unlike µ, the label λ on the polar-
ization vector is not a tensor index, it’s
part of the name that tells us the polar-
ization vector to which we’re referring
We could equally have called the vec-
tors Tom, Dick and Harry.
However,
as we have done here, it’s often useful
to use λ as a shorthand to also tell us
along which direction the polarization
vector points in the rest frame of the
particle.
Since there are only three degrees of freedom there will be three po-
larizations λ = 1, 2, 3. We choose that there is no λ = 0 polarization
[that is, we will force the ﬁeld to be orthogonal to the 0-direction in
the rest frame of a particle, so we won’t require a time-like polarization
basis vector ǫµ
λ=0(m, 0) = (1, 0, 0, 0) (see below)]. The resulting mode
expansion is written
ˆAµ(x)
=
Z
d3p
(2π)
3
2
1
(2Ep)
1
2






ǫ0
1(p)
ǫ1
1(p)
ǫ2
1(p)
ǫ3
1(p)



ˆap1e−ip·x +




ǫ0∗
1 (p)
ǫ1∗
1 (p)
ǫ2∗
1 (p)
ǫ3∗
1 (p)



ˆa†
p1eip·x
+




ǫ0
2(p)
ǫ1
2(p)
ǫ2
2(p)
ǫ3
2(p)



ˆap2e−ip·x +




ǫ0∗
2 (p)
ǫ1∗
2 (p)
ǫ2∗
2 (p)
ǫ3∗
2 (p)



ˆa†
p2eip·x
+




ǫ0
3(p)
ǫ1
3(p)
ǫ2
3(p)
ǫ3
3(p)



ˆap3e−ip·x +




ǫ0∗
3 (p)
ǫ1∗
3 (p)
ǫ2∗
3 (p)
ǫ3∗
3 (p)



ˆa†
p3eip·x

,
(13.24)
or, more compactly16
16The commutation relations between
the creation and annihilation operators
are
h
ˆapλ, ˆa†
qλ′
i
= δ(3)(p−q)δλλ′. (13.25)
ˆAµ(x) =
Z
d3p
(2π)
3
2
1
(2Ep)
1
2
3
X
λ=1

ǫµ
λ(p)ˆapλe−ip·x + ǫµ∗
λ (p)ˆa†
pλeip·x
.
(13.26)
Since pµAµ = 0, we’ll require pµǫµ
λ(p) = 0, which shows how the polar-
ization vectors depend on the momentum.
Example 13.5
The next job is to work out what we want the polarization vectors to be. Being vectors
and living in Minkowski space, these will have to transform like vectors under the
Lorentz transformations. It will be a good idea to have these vectors orthonormal so
we want
ǫ∗
λ(p) · ǫλ′(p) = gµνǫµ∗
λ ǫν
λ′ = −δλλ′,
(13.27)

13.3
Polarizations and projections
123
where the minus sign comes from our metric.17 Consider a particle in its rest frame,
17Note that despite λ not being a ten-
sor index, the orthogonality condition
is often written gλλ′, where gµν is the
Minkowski tensor.
where it has momentum pµ = (m, 0, 0, 0). As stated above, we need pµǫλµ(p) = 0,
for all λ. That is, we want the polarization vectors in this frame to be normal to this
pµ. One possible choice is to use linear polarization vectors, given by
ǫ1(m, 0) =
0
B
B
@
0
1
0
0
1
C
C
A , ǫ2(m, 0) =
0
B
B
@
0
0
1
0
1
C
C
A , ǫ3(m, 0) =
0
B
B
@
0
0
0
1
1
C
C
A .
(13.28)
We can now work out the value of ǫλ(p) in an arbitrary frame of reference by boosting
with Lorentz transformation Λ(p). For example, a particle moving with momentum
pz = |p| along the z-direction has pµ = (Ep, 0, 0, |p|) which is achieved with a boost
matrix
Λµν(p) = 1
m
0
B
B
@
Ep
0
0
|p|
0
m
0
0
0
0
m
0
|p|
0
0
Ep
1
C
C
A ,
(13.29)
yielding the polarization vectors
ǫ1(Ep, 0, 0, |p|) =
0
B
B
@
0
1
0
0
1
C
C
A , ǫ2(Ep, 0, 0, |p|) =
0
B
B
@
0
0
1
0
1
C
C
A , ǫ3(Ep, 0, 0, |p|) =
0
B
B
@
|p|/m
0
0
Ep/m
1
C
C
A .
(13.30)
Finally, using the mode expansion,18 the Hamiltonian after normal or-
18The expansion is rather tedious. For
help see the careful explanation in
Greiner and Reinhardt, Chapter 6.
dering (step V) is diagonalized in the form
ˆH =
Z
d3p Ep
3
X
λ=1
ˆa†
pλˆapλ,
(13.31)
meaning that the total energy is the energy of all particles in all polariza-
tions as we might have expected. Although the complication of having
three ﬁeld components is irritating, the algebra involves little more than
the tedious job of keeping track of some new indices and the polarization
vectors.
13.3
Polarizations and projections
Clearly, the added complexity of the vector ﬁeld arises because of the
diﬀerent polarizations which are possible. In this section we’ll develop
a tensor toolkit to deal with this feature.19
19This section can be skipped on a ﬁrst
reading.
Let’s start with something very simple. To project a three-vector X
along the direction of a vector p (see Fig. 13.2), then all we have to do is
to take the scalar product between X and ˆp = p/|p| (to ﬁnd the length
of the component), and then multiply the result by ˆp to point it in the
correct direction. The projected vector is then XL where
XL = (p · X)p
|p|2
,
(13.32)
where the ‘L’ superscript stands for ‘longitudinal’. The transverse part
XT
XL
X
p
Fig. 13.2 Projecting a three-vector X
along the direction of a vector p.
of X is then XT = X−XL. We can write these equations in component
form as20
20Assume summation over repeated in-
dices.

124
Fields with many components and massive electromagnetism
Xi
L = P ij
L Xj
and
Xi
T = P ij
T Xj,
(13.33)
where the longitudinal and transverse projection matrices P ij
L and
P ij
T are given by
P ij
L = pipj
|p|2 ,
(13.34)
and
P ij
T = δij −pipj
|p|2 .
(13.35)
The four-dimensional upgrades of these are immediate: we deﬁne pro-
jection tensors (returning to tensor notation)
P µν
L
= pµpν
p2
and
P µν
T
= gµν −pµpν
p2 .
(13.36)
For our massive spin-1 modes pµ represents the energy-momentum four-
vector, so p2 = m2. If we work in a basis of linear polarizations, the
transverse projection tensor can be related to the polarization vectors
via the following equality21
21If we work in a basis of circular po-
larizations a slightly more complicated
identity is needed.
1
2
X
λ
ˆ
ǫµλ(p)ǫ∗
νλ(p) +ǫ∗
µλ(−p)ǫνλ(−p)
i
= −P T
µν.
(13.37)
See Maggiore, Chapter 4.
3
X
λ=1
ǫλµ(p)ǫλν(p) = −P T
µν.
(13.38)
This relationship is proved in the following example:
Example 13.6
Method 1: In the rest frame pµ = (m, 0, 0, 0) and the polarization vectors are given
by eqn 13.28 and hence we have
3
X
λ=1
ǫλµ(m, 0)ǫλν(m, 0) =
0
B
B
@
0
0
0
0
0
1
0
0
0
0
1
0
0
0
0
1
1
C
C
A .
(13.39)
One can then boost this second-rank tensor using the matrix of eqn 13.29 to give
3
X
λ=1
ǫλµ(p)ǫλν(p)
=
ΛρµΛσν
3
X
λ=1
ǫλρ(m, 0)ǫλσ(m, 0)
(13.40)
=
1
m2
0
B
B
@
|p|2
0
0
−Ep|p|
0
m2
0
0
0
0
m2
0
−Ep|p|
0
0
E2
p
1
C
C
A = −gµν + pµpν
m2 .
Method 2: The quantity P3
λ=1 ǫλµǫλν is a tensor and so it must be built up from
other tensors.
Since the only other tensors we can build from the variables in
the problem are gµν and pµpν we’ll assume it has to be a linear combination of
these: Agµν + Bpµpν. For a particle at rest we need A + Bm2 = 0 in order that
P3
λ=1 ǫλ0ǫλ0 = 0, which ﬁxes B = −A/m2. Next, for a particle at rest we need the
diagonal space-like components to equal 1, giving us A = −1.

Exercises
125
Chapter summary
• Putting more components into our theory introduces the notion of
the polarization of our ﬁeld. The Hamiltonian for a simple non-
interacting Lagrangian again yields harmonic oscillator solutions,
but this time the energy requires a sum over all momenta and over
polarizations.
• We have applied this to a Lagrangian for massive electromag-
netism,
L = −1
4FµνF µν + 1
2m2AµAµ,
and have shown that the polarization vectors ǫµ(p) are related to
the transverse projection tensor.
Exercises
(13.1) (a) Show that the conserved charge in eqn 13.16
may be written
ˆQNc =
Z
d3p ˆA
†
pJ ˆAp,
(13.41)
where ˆAp = (ˆa1p, ˆa2p, ˆa3p) and J = (Jx, Jy, Jz) are
the spin-1 angular momentum matrices from Chap-
ter 9.
(b) Use the transformations from Exercise 3.3 to
ﬁnd the form of the angular momentum matri-
ces appropriate to express the charge as ˆQNc =
R
d3p ˆB
†
pJ ˆBp where ˆBp = (ˆb1p,ˆb0p,ˆb−1p).
(13.2) (a) Conﬁrm that eqn 13.29 is the appropriate ma-
trix to boost a particle along the z-direction.
(b) Show that the boosted vectors in eqn 13.30 are
still correctly normalized.
(c)
Consider
the
circular
polarization
vectors
ǫµ∗
λ=R
=
−1
√
2(0, 1, i, 0), ǫµ∗
λ=L
=
1
√
2(0, 1, −i, 0),
ǫµ∗
λ=3 = (0, 0, 0, 1).
Show that these are correctly
normalized according to gµνǫµ∗
λ ǫν
λ′ = −δλλ′.
(13.3) Show that PL and PT are indeed projection opera-
tors by showing that P 2 = P.
(13.4) The Lagrangian for electromagnetism in vacuo is
L = −1
4F µνFµν. Show that this can be rewritten
as
L = −1
2(∂µAν∂µAν −∂µAν∂νAµ),
(13.42)
and hence show that using the transverse projection
operator, it may be expressed as
L = 1
2AµP T
µν∂2Aν.
(13.43)
This shows that L only includes the transverse
components of the ﬁeld, squaring with the idea of
electromagnetic waves only representing vibrations
transverse to the direction of propagation.

14
Gauge ﬁelds and gauge
theory
14.1 What is a gauge ﬁeld?
126
14.2 Electromagnetism is the sim-
plest gauge theory
129
14.3 Canonical
quantization
of
the electromagnetic ﬁeld 131
Chapter summary
134
Exercises
134
Nobody ever reads a paper in which someone has done an
experiment involving photons with the footnote that says
‘this experiment was done in Coulomb gauge’.
Sidney Coleman (1937–2007)
14.1
What is a gauge ﬁeld?
We continue our discussion of invariances possessed by physical systems
and arrive at gauge invariance. The idea here is that our ﬁeld theory may
admit diﬀerent conﬁgurations of the ﬁelds which yield identical observ-
ables. Our physical description thus contains some inherent vagueness
and so we can make a choice about which particular formulation, out
of the many possible ones, to adopt in a given situation (this is called
‘a choice of gauge’). A transformation from one description to another
is called a gauge transformation, and the underlying invariance is
called a gauge invariance.1 Note that gauge invariance is not a sym-
1‘Gauge’ is an awful bit of terminol-
ogy with which we are unfortunately
stuck.
Einstein’s general relativity
showed that spacetime geometry has
a dynamical role and Hermann Weyl
wondered if the scale of length could
itself be dynamical, varying through
spacetime.
In this picture, one could
make a choice of gauge which would
be a choice of scale-length: metal wire
comes in diﬀerent thickness or gauges,
so the term seemed entirely appropri-
ate.
He later adapted his scale argu-
ment to one involving phase, as out-
lined here, but the name ‘gauge’ stuck.
metry. Particles do not carry around a knob called ‘gauge’ that allow
us to change them into other particles. Gauge invariance is merely a
statement of our inability to ﬁnd a unique description of a system.
Example 14.1
(i) In electrical theory we have to choose a zero for our potential V (i.e. choose
which potential we call ‘ground’). This is because we only measure potential
diﬀerences (e.g. with a voltmeter) and so the choice of zero potential is an
arbitrary one.
(ii) In quantum mechanics we have to make a choice of what we mean by the zero
of phase of a wave function. The choice is arbitrary because we only measure
phase diﬀerences (e.g. in an interference experiment). There is nothing stop-
ping one transforming all wave functions according to ψ(x) →ψ(x)eiα and
the physics is unchanged.
(iii) In electromagnetism, the magnetic vector potential A can be transformed
according to A →A + ∇χ, where χ(x) is a function of position, and B =
∇× A will be unaﬀected (because ∇× ∇χ = 0). Thus one has to choose the
form of A.

14.1
What is a gauge ﬁeld?
127
The preceding example raises some important issues.
• The potential V and A, and the quantum mechanical phase, all
possess an intrinsic vagueness in description, and we have to make
speciﬁc choices in each case. We might wonder if the choices are
connected. It will turn out that they are.
• We thought about our choice of zero for V and phase as being
something we make once for our particular description of a particu-
lar problem, but which is valid for the whole Universe. If zero volts
is set diﬀerently in New York and in San Francisco, then problems
will occur if you run a conducting wire between their respective
electrical grounds. However, A(x) is chosen with a function χ(x)
which varies from place to place. Thus we will need to distinguish
between global (aﬀecting everywhere in the Universe) and local
(aﬀecting only a particular point) gauge transformations.
With these points in mind, it is time to start exploring this subtle in-
variance, and by doing so we will shed new light on the origin of elec-
tromagnetism.
We start with another look at the complex scalar ﬁeld theory
L = (∂µψ)†(∂µψ) −m2ψ†ψ.
(14.1)
Recall from Section 7.6 that this has a U(1) symmetry. We can make
the replacement
ψ(x) →ψ(x)eiα,
(14.2)
and the Lagrangian (and by extension the equations of motion) aren’t
changed. The most important point to note about eqn 14.2 is that it’s a
global transformation: it changes the ﬁeld ψ(x) by the same amount
at every spacetime point. We say that the theory, given in eqn 14.1, is
invariant with respect to global U(1) transformations.
It turns out that an incredibly rich seam of physics is revealed if
one asks what happens if the value of α given in eqn 14.2 depends on
spacetime. That is, we investigate the transformation
ψ(x) →ψ(x)eiα(x),
(14.3)
where the function α(x) allows us to transform the ﬁeld by a diﬀerent
amount at every point in spacetime. This is a very signiﬁcant change.
Your ﬁrst reaction to this might be to say that surely no theory could
be invariant with respect to a diﬀerent change in phase at every point
in spacetime! Let’s see what actually happens.
Example 14.2
The good news is that the mass term m2ψ†ψ doesn’t change when we transform
locally. The bad news is that the term involving the derivatives does. This is because
the derivative now acts on α(x). In fact, we have
∂µψ(x)
→
∂µψ(x)eiα(x)
=
eiα(x)∂µψ(x) + ψ(x)eiα(x)i∂µα(x)
=
eiα(x) [∂µ + i∂µα(x)] ψ(x).
(14.4)

128
Gauge ﬁelds and gauge theory
Similarly
∂µψ†(x)
→
e−iα(x) [∂µ −i∂µα(x)] ψ†(x).
(14.5)
The ﬁrst term in the Lagrangian becomes
(∂µψ†)(∂µψ) −i(∂µα)ψ†(∂µψ) + i(∂µψ†)(∂µα)ψ + (∂µα)(∂µα)ψ†ψ,
(14.6)
which is certainly not what we started with.
As expected, having α depend on x has robbed the theory of its U(1)
symmetry. We say that it is not invariant with respect to local U(1)
transformations. Undaunted, we’re going to see if we can restore this
local symmetry. We’ll do this by adding a new ﬁeld Aµ(x), designed
to roll around spacetime cancelling out terms that stop the theory from
being invariant. The way this happens will depend on the manner in
which the new ﬁeld transforms. The way to proceed is to introduce the
ﬁeld Aµ(x) via a new object Dµ given by
Dµ = ∂µ + iqAµ(x).
(14.7)
This new object Dµ is called the covariant derivative. This will ﬁx
up the local U(1) symmetry if we insist that the new ﬁeld Aµ transforms
according to Aµ →Aµ −1
q∂µα(x). The parameter q is known as the
coupling strength and will later tell us how strongly the Aµ ﬁeld interacts
with other ﬁelds.2
2For the case of QED examined in
Chapter 39 we will be able to identify q
as the electromagnetic charge. For now
we treat it as a parameter.
Example 14.3
If ψ(x) →ψ(x)eiα(x), then ∂µψ →(∂µψ)eiα + i(∂µα)ψ and so
Dµψ = (∂µ + iqAµ)ψ
→
(∂µψ)eiα + i(∂µα)ψ + iqAµψeiα −i(∂µα)ψ
=
Dµ(ψeiα).
(14.8)
This property makes the whole Lagrangian invariant if we replace ordinary derivatives
by covariant ones:
L = (Dµψ)†(Dµψ) −m2ψ†ψ,
(14.9)
since now with Dµψ →Dµψeiα, the ﬁrst term is invariant.
Thus we conclude that we need our theory to be invariant with respect
to two sets of transformations, which must be implemented together:
ψ(x)
→
ψ(x)eiα(x),
(14.10)
Aµ(x)
→
Aµ(x) −1
q ∂µα(x).
(14.11)
A theory which has a ﬁeld Aµ(x) introduced to produce an invariance
with respect to local transformations is known as a gauge theory.
The ﬁeld Aµ(x) is known as a gauge ﬁeld.
We might expect the ﬁeld Aµ(x) could make a contribution to the La-
grangian itself. It only exists in our description because we’ve invented it
to satisfy our demand for a locally invariant theory, but if such ambitions
have any groundings in reality then the ﬁeld Aµ should have dynamics of
its own! As we will see in the next section, it gives us electromagnetism.

14.2
Electromagnetism is the simplest gauge theory
129
14.2
Electromagnetism is the simplest
gauge theory
What are the possible forms of the part of the Lagrangian describing the
ﬁeld Aµ(x)? It must be a theory for which the Lagrangian is invariant
under transformations of the form Aµ(x) →Aµ(x) −1
q∂µα(x). Electro-
magnetism provides an example of such a theory. It is, after all, a theory
described in terms of a vector ﬁeld Aµ(x) = (V (x), A(x)) which is used
to form a Lagrangian
L = −1
4(∂µAν −∂νAµ)(∂µAν −∂νAµ) −Jµ
emAµ,
(14.12)
from which the equations of motion follow as
∂2Aν −∂ν(∂µAµ) = Jν
em,
(14.13)
which are known as the ﬁrst two Maxwell equations.3
3Recall that we wrote these in Chap-
ter 5 as ∂λF λν = Jν
em (where F λν =
∂λAν −∂νAλ), which is clearly the
same.
More interestingly, it is worth
considering the role of the Maxwell
equations in this context. In the next
section we will quantize the theory
in terms of electromagnetic particles
called photons, which are excitations of
the quantum ﬁeld ˆ
A(x). The Maxwell
equations are the equations of motion
for ˆ
A(x) and the closest we have to a
Schr¨odinger equation for the photon.
As it is not possible to construct a prob-
ability density from the ﬁeld A(x), the
photons that the quantum ﬁeld theory
of electromagnetism describes do not
have an interpretation in terms of a
probability amplitude of their spatial
localization.
See Berestetskii, Lifshitz
and Pitaevskii (Introduction and Chap-
ter 1) for further details.
One of the most important observations about this formulation of
electromagnetism is that neither the Lagrangian nor the equations of
motion are changed if one makes the swap
Aµ(x) →Aµ(x) −∂µχ(x),
(14.14)
which is short for the changes
V
→
V −∂0χ,
A
→
A + ∇χ.
(14.15)
This observation, which is known as gauge invariance, means that if Aµ
does the job of describing the electromagnetic ﬁelds in some situation
then so does Aµ−∂µχ. This also means that electromagnetism is a gauge
theory since we can call χ(x) by the name α(x)/q and the condition in
eqn 14.11 will clearly be satisﬁed.
Example 14.4
The theory can also be written in terms of the tensor Fµν = ∂µAν −∂νAµ with a
Lagrangian written as
L = −1
4FµνF µν.
(14.16)
Gauge invariance amounts to the statement that we can change the ﬁeld according
to Aµ →Aµ −∂µχ(x) and F µν is unchanged.
It may be helpful to think of the freedom of the choice of gauge as a
choice of language.
We can speak French, German or Venusian, but
we are able to communicate the same underlying message no matter
what language we speak. If we’re sensible, we can choose χ(x) (that is,
the language we’re speaking) in such a way as to make whatever we’re

130
Gauge ﬁelds and gauge theory
describing as simple as possible. There are several common choices, the
most famous of which are listed in the box in the margin.
Let’s try making a choice of χ(x). We’ll try choosing χ(x) in such a
way that
∂µAµ(x) = 0,
(14.17)
which is known as the Lorenz condition or as Lorenz gauge.4
4 Actually it’s more usually (and incor-
rectly) known as the Lorentz condition
due to its misattribution to Hendrik
Lorentz (1853–1928) rather than to the
less famous Ludvig Lorenz (1829–1891)
who used it ﬁrst. See J. D. Jackson and
L. B. Okun, Rev. Mod. Phys. 73, 663
(2001) for details of the history.
Example 14.5
To do this we write Aµ →A′
µ = Aµ −∂µχ. We want
∂µA′
µ = ∂µAµ −∂µ∂µχ = 0,
(14.18)
which is achieved by setting ∂2χ = ∂µAµ.
This results in the desired condition
∂µA′µ = 0.
Common gauges for electromag-
netism
Lorenz gauge
∂µAµ = 0
Coulomb gauge
∇· A = 0
Axial gauge
A3 = 0
Weyl gauge
A0 = 0
Lorenz gauge is useful because, in the absence of a current Jµ
em, it re-
sults in a massless Klein–Gordon equation for each component of the
electromagnetic ﬁeld. That is, with ∂µA′µ = 0, eqn 14.13 becomes
∂2A′µ = 0,
(14.19)
whose solutions are plane waves of the form Aµ = ǫµ(p)e−ip·x with Ep =
|p|. The Lorenz condition therefore makes electromagnetism resemble
the case of vector ﬁeld theory. Recall that there the Lorenz condition
∂µAµ wasn’t a choice, it was mandated by the mass term m2AµAµ in the
Lagrangian of that theory. In any case, the result is the same: it reduces
the number of independent components of A′µ from four to three.
However, this still doesn’t make A′µ unique! This is because we can
make a further shift A′
µ →A′′
µ = A′
µ −∂µξ as long as ∂2ξ = 0 (so that
both A′
µ and A′′
µ satisfy the Lorenz condition). To make A′′µ unique, we
will choose
∂0ξ = A′
0,
(14.20)
which implies A′′
0 = 0. With this further choice, the Lorentz condition
then reduces to ∇· A′′ = 0. This is known as Coulomb gauge and
further reduces the number of independent ﬁeld components by one.
This makes it clear that although the ﬁeld Aµ has four components,
the physics allows only two independent components. This can lead to
trouble when the ﬁeld is quantized.
Example 14.6
The equations of motion in Lorenz gauge read ∂2Aµ = 0, which, with A0 = 0,
have plane wave solutions5 A = ǫe−ip·x. The equation encoding the Coulomb gauge
5Remember that the polarization vec-
tors introduced here will carry the in-
formation about the spin state of the
photon.
condition, ∇· A = 0, leads to
p · A = p · ǫ = 0,
(14.21)

14.3
Canonical quantization of the electromagnetic ﬁeld
131
which tells us that the direction of propagation of the wave is perpendicular to the
polarization. For a wave propagating along z with momentum qµ = (|q|, 0, 0, |q|) we
could have
ǫ1(q) =
0
@
1
0
0
1
A ,
ǫ2(q) =
0
@
0
1
0
1
A ,
(14.22)
corresponding to linear polarization.
We could choose the waves to have circular
polarization instead, in which case we could have
ǫ∗
R(q) = −1
√
2
0
@
1
i
0
1
A ,
ǫ∗
L(q) =
1
√
2
0
@
1
−i
0
1
A .
(14.23)
In order to observe the eﬀects of electromagnetism, the electromagnetic
ﬁeld must couple to a matter ﬁeld. We might wonder how to write down
this coupling. It turns out that the most simple form may be achieved
with a simple recipe known as the minimal coupling prescription.
This simply involves swapping derivatives ∂µ of the matter ﬁeld for co-
variant derivatives Dµ and is illustrated in the example below.
Example 14.7
Consider complex scalar ﬁeld theory in the presence of an electromagnetic ﬁeld. The
Lagrangian is written as the sum of the Lagrangians for the two separate theories:
L = (∂µψ)†(∂µψ) −m2ψ†ψ −1
4FµνF µν.
(14.24)
To enact the minimal coupling prescription we upgrade derivatives of the matter ﬁeld
ψ to covariant derivatives:
L
=
(Dµψ)†(Dµψ) −m2ψ†ψ −1
4FµνF µν
=
(∂µψ† −iqAµψ†)(∂µψ + iqAµψ) −m2ψ†ψ −1
4 FµνF µνv
=
∂µψ†∂µψ −m2ψ†ψ −1
4FµνF µν
+
“
−iqAµψ†(∂µψ) + iq(∂µψ†)Aµψ + q2ψ†ψAµAµ
”
.
(14.25)
The ﬁnal line shows the coupling between the Aµ ﬁeld and the ψ and ψ† ﬁelds. The
strength of the coupling is set by the coupling strength q, which is also known as
electromagnetic charge.
The notion that a gauge ﬁeld, introduced to guarantee a local symmetry,
will dictate the form of the coupling, or interactions, in a theory is known
as the gauge principle and is a philosophy to which we will return
later.6
6See Chapter 46.
14.3
Canonical quantization of the
electromagnetic ﬁeld
The previous section reminded us that even though Aµ has four compo-
nents, only two are needed for a physical description (which makes good

132
Gauge ﬁelds and gauge theory
physical sense because light is a transverse wave and therefore has two
components).7
7The gauge ﬁeld Aµ should not, there-
fore, be confused with the vector ﬁeld
discussed in the last chapter. The vec-
tor ﬁeld was massive and this property
robs it of the possibility of gauge invari-
ance. It also led to its having three de-
grees of freedom. The massless nature
of the gauge ﬁeld leads to its gauge in-
variance and its only having two free
components.
Having two redundant components of Aµ ﬂapping around, reﬂecting
the freedom to choose which language that we speak, is something we
might worry about when we quantize the theory. There are now two
ways to go. We can ﬁx the gauge from the outset in order to reduce
the number of degrees of freedom from four to two, making a choice
of which language we’re going to speak throughout, or we can continue
and see how far we get before the redundancy comes back to bite us.8
8This second method may be found in
the book by Aitchison and Hey and the
one by Maggiore.
We choose the ﬁrst option in the following example, and working in the
Coulomb gauge we will canonically quantize electromagnetism simply
by following the usual ﬁve steps.
Example 14.8
Step I: The Lagrangian is still
L = −1
4FµνF µν = −1
4(∂µAν −∂νAµ)(∂µAν −∂νAµ),
(14.26)
although we’ll need to remember that our choice of gauge dictates A0 = 0 and
∇· A = 0. Actually, this will be implemented at step III.
Step II: We ﬁnd that the Πµν tensor has components
Πµν =
∂L
∂(∂µAν) = −(∂µAν −∂νAµ),
(14.27)
just as in the case of massive vector ﬁeld theory. Again, the momentum density of
the zeroth component Π00 = 0, but we don’t care since we have decided to eliminate
A0 = 0. The momentum component conjugate to the ith component of the ﬁeld is
Π0i = Ei, that is, the electric vector ﬁeld E(x). The Hamiltonian9 is then
9You should ﬁnd
H = 1
2(E2 + B2) + E · ∇A0.
The last term is removed using the
same method as employed in the pre-
vious chapter, noting that ∇· E = 0
and that all ﬁelds should vanish at in-
ﬁnity.
H = 1
2(E2 + B2).
(14.28)
Step III: Next we need commutation relations.
Our ﬁrst guess might be that
h
ˆ
Aµ(x), ˆΠ0ν(y)
i
= iδ(3)(x −y)gµν, similar to the massive vector ﬁeld. This won’t
work here though. Rewriting yields
h
ˆ
Ai(x), ˆEj(y)
i
= −iδ(3)(x −y)gij, which looks
ﬁne until you take the divergence (with respect to x) of this equation and get
h
∂i ˆ
Ai(x), ˆEj(y)
i
= iδ(3)(x −y)∂j.
(14.29)
But this should be zero! (This is because ∂i ˆ
Ai = ∇· ˆA = 0.) The problem is that
we haven’t yet ensured that only transverse components of ˆA are permitted to enter
the solution. We need a way of projecting out the unnecessary ones to implement
the gauge condition. The answer to this problem turns out to necessitate the use of
our projection tensor P T (see Chapter 13). The commutation relation we need is
given by
h
ˆ
Ai(x), ˆEj(y)
i
=
i
Z
d3p
(2π)3 eip·(x−y)
„
δij −pipj
p2
«
(14.30)
=
iδ(3)
tr (x −y),
which gives zero on the right when you take the divergence (exercise). The symbol
δ(3)
tr
is known as a transverse delta function due to the inclusion of the transverse
projection tensor P T. This guarantees that our creation and annihilation operators
satisfy the required relation [ˆapλ, ˆa†
qλ′] = δ(3)(p −q)δλλ′.

14.3
Canonical quantization of the electromagnetic ﬁeld
133
Step IV: Next the mode expansion, which only contains those two polarizations that
are transverse to the direction of propagation:10
10Note that the two polarization vec-
tors obey pµǫµ
λ(p) = 0 and are nor-
malized according to gµνǫµ∗
λ (p)ǫν
λ(p) =
−δλλ′ = gλλ′.
ˆ
Aµ(x) =
Z
d3p
(2π)
3
2
1
(2Ep)
1
2
2
X
λ=1
“
ǫµ
λ(p)ˆapλe−ip·x + ǫµ∗
λ (p)ˆa†
pλeip·x”
.
(14.31)
Inserting this and (Step V) normally ordering, we obtain the expected ﬁnal result:
ˆH =
Z
d3p
2
X
λ=1
Epˆa†
pλˆapλ,
(14.32)
with Ep = |p|. We conclude that the excitations of the electromagnetic ﬁeld are
photons, which may be observed with two transverse polarizations. Apart from the
slight complication of making the commutation relations compatible with the choice
of gauge and the hassle of the indices, there’s nothing hard about the quantization
of electromagnetism in Coulomb gauge.
The last example showed us that quantized particles of the electromag-
netic ﬁeld are photons. These particles have spin S = 1 and come in two
types: ˆa†
p1|0⟩and ˆa†
p2|0⟩, corresponding to the two transverse polariza-
tions of the electromagnetic ﬁeld. Consider a photon propagating along
the z-direction with momentum qµ = (|q|, 0, 0, |q|). If we work in a basis
of circularly polarized vectors, we may write ǫ∗
λ=R(q) = −1
√
2(0, 1, i, 0)
(corresponding to Sz = 1) and ǫλ=L(q) =
1
√
2(0, 1, −i, 0) (corresponding
to Sz = −1). There are no photons with Sz = 0 as this would corre-
spond to an unphysical longitudinal polarization ǫ∗
λ=3(p) = (0, 0, 0, 1).
We examine this in Exercise 14.2 and in the following example.
Example 14.9
We calculated the rotation matrices ˆJ for the vector ﬁeld in Chapter 9. These also
function as angular momentum operators for photon states. We will create a photon
travelling along the z-direction, so that qµ = (|q|, 0, 0, |q|). We write
|γλ⟩= ǫ∗
λ(q)ˆa†
qλ|0⟩.
(14.33)
Let’s calculate the helicity11 of photons with λ = R. The helicity operator is ˆh =
11Helicity, the projection of the spin
along the momentum direction, is ex-
plained in detail in Chapter 36. Since
the photon does not have a rest frame
the helicity is the most useful quantity
to use in talking about its spin.
ˆJ · q/|q|, leading to an operator ˆh = ˆJz. Operating with the angular momentum
matrix ˆJz suitable for S = 1 spins, we have
ˆJz|γλ=R⟩= −1
√
2
0
B
B
@
0
0
0
0
0
0
−i
0
0
i
0
0
0
0
0
0
1
C
C
A
0
B
B
@
0
1
i
0
1
C
C
A ˆa†
qR|0⟩= +|γλ=R⟩.
(14.34)
We see that the right-circularly polarized photon is in an Sz = 1 eigenstate and
also the h = +1 (positive) eigenstate of helicity.
It’s easy to show that the left-
circularly polarized photon is in the Sz = −1 state and has negative helicity. You
may conﬁrm that the disallowed polarization ǫ∗
λ=3(q) = (0, 0, 0, 1) would correspond
to a longitudinally polarized photon. Note that this polarization is allowed for the
vector particle from Chapter 13 as vector ﬁelds have three degrees of freedom

134
Gauge ﬁelds and gauge theory
Chapter summary
• A theory which has a ﬁeld Aµ(x) introduced to produce an in-
variance with respect to local transformations is known as a gauge
theory. The ﬁeld Aµ(x) is known as a gauge ﬁeld. Theories must
be invariant with respect to the transformations:
ψ(x)
→
ψ(x)eiα(x),
Aµ(x)
→
Aµ(x) −1
q ∂µα(x).
(14.35)
• Applied to the Lagrangian for electromagnetism, L = −1
4FµνF µν,
we have canonically quantized electromagnetism.
Exercises
(14.1) Fill in the algebra leading to eqn 14.32.
(14.2) A demonstration that the photon has spin-1, with
only two spin polarizations.
A photon γ propagates with momentum qµ =
(|q|, 0, 0, |q|).
Working with a basis where the
two transverse photon polarizations are ǫµ
λ=1(q) =
(0, 1, 0, 0) and ǫµ
λ=2(q) = (0, 0, 1, 0), it may be
shown, using Noether’s theorem, that the opera-
tor ˆSz, whose eigenvalue is the z-component spin
angular momentum of the photon, obeys the com-
mutation relation
h
ˆSz, ˆa†
qλ
i
= iǫµ=1∗
λ
(q)ˆa†
qλ=2 −iǫµ=2∗
λ
(q)ˆa†
qλ=1.
(14.36)
(i) Deﬁne creation operators for the circular polar-
izations via
ˆb†
qR
=
−1
√
2
“
ˆa†
q1 + iˆa†
q2
”
,
ˆb†
qL
=
1
√
2
“
ˆa†
q1 −iˆa†
q2
”
.
(14.37)
Show that
h
ˆSz,ˆb†
qR
i
=
ˆb†
qR,
h
ˆSz,ˆb†
qL
i
=
−ˆb†
qL.
(14.38)
(ii) Consider the operation of ˆSz on a state |γqλ⟩=
ˆb†
qλ|0⟩containing a single photon propagating along
z:
ˆSz|γλ⟩= ˆSzˆb†
qλ|0⟩,
λ = R, L.
(14.39)
Use the results of (i) to argue that the projection of
the photon spin along its direction of propagation
must be Sz = ±1.
See Bjorken and Drell Chapter 14 for the full ver-
sion of this argument.

15
Discrete transformations
15.1 Charge conjugation
135
15.2 Parity
136
15.3 Time reversal
137
15.4 Combinations of discrete and
continuous
transformations
139
Chapter summary
142
Exercises
142
In Chapter 9 we have explored how quantum ﬁelds behave under various
spacetime translations and rotations. Such transformations are contin-
uous and are thus represented by continuous groups (Lie groups). How-
ever, discrete transformations are also possible, represented this time by
ﬁnite groups, and this chapter examines some of the most commonly
encountered and important discrete transformations.
C
Fig. 15.1 The charge conjugation oper-
ator C turns a particle into its antipar-
ticle, reversing its charge and all other
‘charge-like’ numbers.
15.1
Charge conjugation
A particularly interesting discrete transformation is one which changes
all particles into their antiparticles (see Fig. 15.1). This is accomplished
by an operator C: this operator ﬂips not only the sign of a particle’s
charge, but also its lepton number, its hypercharge, and all the ‘charge-
like’ numbers which characterize a particular particle. We can write
C|p⟩= |¯p⟩,
(15.1)
where ¯p is the antiparticle of a particle p. Let’s say that p has charge1
1By ‘charge’ we could mean any prop-
erty like charge, or lepton number, or
baryon number, etc. These are generi-
cally called quantum charges.
q. If this charge can be measured by an operator ˆQ then ˆQ|p⟩= q|p⟩,
whereas ˆQ|¯p⟩= −q|¯p⟩. Thus C ˆQ|p⟩= qC|p⟩= q|¯p⟩, but ˆQC|p⟩= ˆQ|¯p⟩=
−q|¯p⟩. This implies that ˆQC = −C ˆQ, or equivalently
C−1 ˆQC = −ˆQ.
(15.2)
For the operator C to exchange particles and antiparticles we need
C−1ˆapC = ˆbp
and
C−1ˆb†
pC = ˆa†
p.
(15.3)
Since a complex scalar ﬁeld ˆψ(x) can be written ˆψ(x) =
R
p(ˆape−ip·x +
ˆb†
peip·x), we must have2 C−1 ˆψC = ˆψ†.
2Note that
ˆψ† =
Z
p
(ˆa†
peip·x + ˆbpe−ip·x).
Example 15.1
Since C2 = I, the eigenvalues of C can only be ±1. Most particles are not eigenstates
of C, since if they were then C|p⟩= |¯p⟩= ±|p⟩, but this would mean that |¯p⟩is the
same state as |p⟩and so the particle is its own antiparticle. This is true in some cases,
speciﬁcally for particles which have no quantum charges.3 In the case of the photon
3In other words for particles which have
zero electrical charge, zero lepton num-
ber, zero baryon number, etc.
(γ), it is an eigenstate of C with eigenvalue −1, since if you change all particles to
their antiparticles, the electromagnetic ﬁeld reverses (Aµ →−Aµ). It is also the
case with the neutral pion π0 but here the eigenvalue is +1. This explains why the
reaction π0 →γ + γ is allowed [the eigenvalues of C go from +1 →(−1) × (−1)], but
you cannot have π0 →γ + γ + γ.

136
Discrete transformations
15.2
Parity
scalar
vector
pseudovector
c = a × b
−a
a
b
a
c
π
π
π
b′
a′
a′×b′
−b −a
c
Fig. 15.2 A parity operation consists
of reﬂection in a mirror followed by a
π-rotation about an axis perpendicular
to the mirror.
The mirror reverses a
single spatial coordinate (the one per-
pendicular to the plane of the mirror),
the rotation reverses the other two. A
scalar is unaﬀected by this transforma-
tion, whereas an ordinary (polar) vec-
tor changes sign. A pseudovector (axial
vector), such as a vector product of two
ordinary vectors, is unchanged.
L
L
E
−E
*
*
*
B
B
*
*
*
π
mirror
Fig. 15.3 Electric ﬁeld E behaves like
an ordinary (polar) vector, while mag-
netic ﬁeld B and angular momentum L
(derived from vector products of polar
vectors) are pseudovectors (axial vec-
tors). The asterisk indicates a partic-
ular vector in the electric ﬁeld from a
point charge, and in the magnetic ﬁeld
from a current loop, so that you can
focus on how a speciﬁed vector trans-
forms.
When you look in a mirror, things look the same but there are subtle
diﬀerences. Five screws in your hand are reﬂected into ﬁve screws, so
numbers of things (scalars) don’t change. But look carefully and you’ll
see that the threads in the mirror screws turn in the opposite sense
to those in the real world. A mirror image has the spatial coordinate
perpendicular to the surface of the mirror reversed, with the other two
spatial coordinates left the same. This eﬀect cannot be achieved by a ro-
tation. If you follow a mirror operation by a 180◦rotation about an axis
perpendicular to the mirror, you will have reversed all three coordinates
and this transformation is known as inversion. This transformation
can be performed using the parity operator P which acts to invert spa-
tial coordinate, i.e. mapping x →−x. This means that the position
operator ˆx will anticommute with the parity operator:
ˆxP = −Pˆx,
(15.4)
or equivalently
P−1ˆxP = −ˆx.
(15.5)
The eﬀect on the momentum coordinates is also p →−p, and hence
P−1ˆpP = −ˆp.
(15.6)
However, the commutation relation [ˆx, ˆpx] = i will be preserved. Since
P is a Hermitian operator, and since also it is its own inverse (P2 = I,
where I is the identity) then P is also unitary. The parity operation has
no eﬀect on scalars but reverses vectors (see Fig. 15.2, and for physical
examples see Fig. 15.3). In fact, there are a class of scalars and vec-
tors for which this is not true: a pseudovector (sometimes called an
axial vector) is formed by a cross product between two ordinary vec-
tors (which are sometimes called polar vectors) and a pseudoscalar is
formed via a scalar triple product; these behave oppositely. In summary:
P(scalar) = scalar
P(vector) = −vector
P(pseudoscalar) = −pseudoscalar
P(pseudovector) = pseudovector.
Since P2 = I then the group formed by the elements {I, P} under mul-
tiplication is isomorphic to Z2, the cyclic group of order 2, pretty much
the simplest non-trivial group you can imagine. As for C, this means
The group multiplication table for the
parity operator P and the identity op-
erator I:
I
P
I
I
P
P
P
I
the eigenvalues are ±1. Scalars and pseudovectors have parity eigen-
value +1, while vectors and pseudoscalars have parity eigenvalue −1.
For example, the photon is an excitation in a massless vector ﬁeld and
therefore has intrinsic parity −1. The pion is described by a pseudoscalar
ﬁeld and also has parity −1. We will later ﬁnd (from using the Dirac
equation and considering what are called spinor ﬁelds) that the parity
of a fermion is opposite to that of its antiparticle.
In general, the problem we face is that our quantum ﬁelds can be
scalar-valued ﬁelds or vector-valued ﬁelds or even more complicated ob-

15.3
Time reversal
137
jects. Thus a parity transformation will aﬀect the nature of the ﬁeld
itself, as well as acting on the coordinates in which the ﬁeld is deﬁned.
Example 15.2
For now, let’s just keep things simple and focus on a scalar ﬁeld. If we operate on a
scalar ﬁeld then φ(t, x) →φ(t, −x). Let’s look at what it does to the creation and
annihilation operators. We have
P−1 ˆφ(t, x)P = ˆφ(t, −x) =
Z
p
ˆape−i(Et+p·x) + ˆa†
pei(Et+p·x),
(15.7)
and this will work if
P−1ˆapP = ˆa−p,
P−1ˆa†
pP = ˆa†
−p,
(15.8)
which means that in the case of scalar ﬁelds the parity operator simply reverses the
momentum on the creation and annihilation operators.
15.3
Time reversal
‘If I could turn back time. . .’ With the time-reversal operator T you
can! It maps a scalar ﬁeld φ(t, x) →φ(−t, x) and so while it leaves the
position operator well alone:
T−1ˆxT = ˆx,
(15.9)
it reverses momentum coordinates:
T−1ˆpT = −ˆp.
(15.10)
This means that the commutation relation [ˆx, ˆpx] = i will only be pre-
served if T−1iT = −i, and hence T must be antiunitary.4 The archetypal
4An antiunitary operator has T2 = −I
(compare this with a unitary operator
U which satisﬁes U2 = I).
Note also
that the condition
T−1iT = −i
can be written iT = −Ti, meaning that
i anticommutes with T. This looks non-
sensical at ﬁrst sight: how can a simple
number like i anticommute with any-
thing? However, T operates on every-
thing to the right of it, and if there is
a factor of i there, its sign will be re-
versed. In the simple case that T = K,
where K is the complex conjugation op-
erator, this condition is easily demon-
strated since
T(iψ) = −iTψ,
where ψ is any function.
antiunitary operator is K, the complex conjugation operator and in fact
you can make a general antiunitary operator by forming the product of
K and U, where U is a unitary operator. Accordingly we write
T = UK,
(15.11)
which means that5 U = TK.
5By
postmultiplying
both
sides
of
eqn 15.11 by K.
Example 15.3
(i) For spinless particles you can take U to be the identity and T = K. This is not so
surprising since the complex conjugate of the Schr¨odinger equation
ˆHψ = i ∂ψ
∂t
(15.12)
is
ˆHψ∗= −i∂ψ∗
∂t
= i ∂ψ∗
∂(−t) ,
(15.13)

138
Discrete transformations
and so the combined operation of time reversal and complex conjugation leaves the
form of the Schr¨odinger equation invariant. Thus indeed it looks like K and T are
the same thing.
(ii) Adding spin complicates things because angular momentum is reversed when time
reverses (it corresponds in some general sense to things going round in a particular
sense as a function of time). Thus
T−1 ˆST = −ˆS,
(15.14)
where ˆS is the spin operator. The operation of complex conjugation is a bit more
complicated since6
6Remember the form of the Pauli spin
matrices in which only σy has imagi-
nary components, while σx and σz are
real.
K−1 ˆSxK = ˆSx,
K−1 ˆSyK = −ˆSy,
K−1 ˆSzK = ˆSz.
(15.15)
An appropriate form for U is U = exp(−iπ ˆSy) so that
T = exp(−iπ ˆSy)K.
(15.16)
This operator has the property7 T2 = ±1. For a system containing many particles
7The proof of this goes as follows: T2 =
UKUK = UU∗but since U is unitary
U†
= (U∗)T
= U−1 and so T2
=
U(UT)−1 = X where X is a diagonal
matrix containing only phase factors.
Thus U = XUT and thus UT = UXT =
UX and thus U = XUX and hence the
diagonal elements of X are only ±1.
Thus T2 = ±1.
with spin, the time-reversal operator can be written
T =
Y
i
exp(−iπ ˆSiy)K,
(15.17)
where the product is evaluated over all the particles. Let’s put some ﬂesh on these
bones. For a single electron, the spin is one-half and so exp(−iπ ˆSy) will have eigen-
values ±i and so T2 = −1. This will also be the case for an odd number of electrons
but if the number of electrons is even then T2 = 1.
These expressions have some interesting consequences for the case in which T2 =
−1 (for an odd number of electrons). For a Hamiltonian H that is invariant under
time-reversal (so that H commutes with T), both a state |ψ⟩and its time-reversed
version T|ψ⟩will have the same energy. Are they the same state? If they were, then
T|ψ⟩= α|ψ⟩where α is a complex number. But in that case8 T2|ψ⟩= Tα|ψ⟩=
8We are using the fact that T is antiuni-
tary and this implies Tα|ψ⟩= α∗T|ψ⟩.
α∗T|ψ⟩= |α|2|ψ⟩but since we have assumed T2 = −1 then we would deduce |α|2 =
−1 and that’s not possible since |α|2 > 0. We have arrived at a contradiction of our
initial assumption, so we can deduce that |ψ⟩and T|ψ⟩are linearly-independent states
and are known as a Kramers doublet.
We have deduced Kramers’ theorem
which states that the energy levels of a time-reversal invariant system with an odd
number of electrons are n-fold degenerate where n is even. Essentially the energy
levels come in pairs of Kramers doublets, and you can only split these pairs by
introducing a perturbation that breaks time-reversal, such as a magnetic ﬁeld.
Hendrik Kramers (1894–1952).
Example 15.4
Reversing time t using T and reversing spatial coordinate x using P produces a
reversal of spacetime x. On a scalar ﬁeld this can be written
(PT)−1 ˆφ(x)(PT) = ˆφ(−x).
(15.18)
This operation leaves the creation and annihilation operators unchanged:
(PT)−1ˆap(PT) = ˆap,
(PT)−1ˆa†
p(PT) = ˆa†
p,
(15.19)
and this occurs because a parity transformation ﬂips all the momenta and the time
reversal ﬂips them all back. In the mode expansion of the scalar ﬁeld the only eﬀect
is to ﬂip the sign of i in the exponentials. It thus acts as an operator to perform a
complex conjugation of the scalar ﬁeld.

15.4
Combinations of discrete and continuous transformations
139
Each of C, P and T are conserved in many-particle physics processes,
but not all. Famously, P is ‘violated’ in the weak interaction. However,
a quantum ﬁeld theory that satisﬁes a fairly minimal set of assump-
tions (ones in which L is Lorentz invariant, local, Hermitian and normal
ordered) possesses the symmetry CPT. In other words, if you reverse
spacetime and replace particles by antiparticles, the theory should be
invariant. The proof of this CPT theorem is (in brief outline only)
based on showing that (CPT)−1L(x)(CPT) = L(−x), and hence deduc-
ing that CPT commutes with the Hamiltonian and is hence a symmetry.9
9The ﬁrst part can be deduced using
the fact that L is a Lorentz scalar and
hence any term in it contains terms in
which all tensor indices are contracted,
so the terms contain an even number
of tensor indices and under CPT these
produce an even number of minus signs.
More details can be found in the books
by Itzykson and Zuber, Srednicki and
Weinberg.
A consequence of the CPT theorem is that the mass and lifetime of any
particle is identical to that of its antiparticle. If experiment ever provides
conclusive evidence of a deviation from this then CPT symmetry will
have been shown not to hold in some particular case, casting doubt on
Lorentz invariance. So far, CPT symmetry has survived the test.
15.4
Combinations of discrete and
continuous transformations
We have previously examined the group of three-dimensional rotations.
These can be represented by orthogonal 3×3 matrices (let’s call them R)
but with the condition that the determinant of the matrix is det R =
+1. The group which describes this is known as SO(3), the special
orthogonal group, the ‘special’ being the determinant being +1. These
rotations are called proper rotations.
The parity transformation can be represented by a matrix


−1
0
0
0
−1
0
0
0
−1

,
(15.20)
[or, for short, diag(−1, −1, −1)] because it maps (x, y, z) →(−x, −y, −z)
and this matrix clearly has determinant −1. By combining this single
transformation with our group SO(3), we get a much larger group O(3),
the group of all orthogonal 3 × 3 matrices.10 This includes the proper
10Note
that
orthogonality
implies
RTR
=
I.
Hence taking the de-
terminant,
det R × det RT
=
1.
Now
det R
=
det RT
and
hence
(det R)2 = 1 or det R = ±1, showing
that O(3) contains matrices with both
det R = +1 and det R = −1.
rotations of SO(3) (with det R = +1) as well as improper rotations
in which det R = −1. Improper rotations are ordinary, proper rotations
followed by a parity operation (or equivalently by a reﬂection).
det= +1
det= −1
Fig. 15.4 The group O(3) consists of
two disjoint sets, which can be pictured
as two disconnected islands. These sets
are distinguished by their elements ei-
ther having determinant +1 [this island
is SO(3)] and −1.
Only the ﬁrst is-
land is independently a group because
the identify transformation has deter-
minant +1 and lives exclusively on the
left island.
Example 15.5
(i) Reﬂection by a mirror in the x-y plane is accomplished by diag(1, 1, −1). This
has determinant −1 (an improper rotation).
(ii) Rotation about the z-axis by π is accomplished by diag(−1, −1, 1). This has
determinant +1 (proper rotation).
(iii) The parity operation, diag(−1, −1, −1), is accomplished by the product of (i)
and (ii). It is therefore an improper rotation.

140
Discrete transformations
The group SO(3) is a connected group, meaning that you can imagine
continuously wandering through all the elements of the group; the group
is not the union of two or more disjoint sets. The same cannot be said of
O(3). It consists of two disjoint sets, one set with determinant +1 [which
is SO(3)] and the other with determinant −1 and is a disconnected
group (see Fig. 15.4).
We ﬁnd something similar when we consider the Lorentz group, the
group containing all rotations, reﬂections and Lorentz boosts. This is
often given the symbol O(1, 3), emphasizing that it has time-like (one-
dimensional) and space-like (three-dimensional) parts. This consists of
four topologically separated components because as well as P we have
to consider T. In a four-dimensional representation we have
P = diag(1, −1, −1, −1),
T = diag(−1, 1, 1, 1).
(15.21)
The subgroup of the Lorentz group that doesn’t reverse spatial or tem-
poral coordinates is the proper, orthochronous Lorentz subgroup
SO+(1,3). This connected group is one of the four components of the
Lorentz group. The other three components can be accessed by taking
elements of SO+(1,3) and (i) including P, or (ii) including T, or (iii)
including both P and T (see Fig. 15.5).
SO+(1, 3)
P
P
T
T
Fig. 15.5 The Lorentz group O(1, 3)
consists of four separated components.
The identity element resides in the
component SO+(1, 3), the proper, or-
thochronous Lorentz subgroup.
The
other ‘islands’ can be reached using the
operators P, T or the combination PT.
While wading in these deep topological waters, it is worth making
some important remarks about simple rotations which will have impor-
tant consequences later. Let’s consider proper rotations which we have
been representing using the group SO(3). Let’s think about the topol-
ogy of this. A rotation is characterized by a rotation axis and an angle.
Thus we could imagine a sphere with radius π and then any point in-
side the sphere could represent a rotation: the direction from the origin
would determine the rotation axis and the distance from the origin would
determine the rotation angle. However, a rotation of π about a partic-
ular axis is equivalent to a rotation of −π about the same axis. Thus
the topology of SO(3) is equivalent to that of a ball in which antipodal
points on the surface are identiﬁed.
(a)
(b)
Fig. 15.6 Closed paths in the space
SO(3), represented as a ball in which
antipodal points on the surface are
identiﬁed. (a) A path from the north
to south pole cannot be continuously
deformed to a point, while (b) a double
path can.
This topological space is clearly connected but is not simply con-
nected, which means that a closed path through the space cannot be
continuously shrunk to a point. To show this consider a path running
from the north pole to the south pole of the sphere [Fig. 15.6(a)]. You
can’t deform this continuously to a point. If you move the path near
the north pole to the right, the point near the south pole moves to the
left. You can’t get the two to join up. However, if you run the path
twice from the north pole to the south pole, you can do it [Fig. 15.6(b)].
This shows that a 4π rotation is continuously deformable to a point,
while a 2π rotation is not. This is the basis of the famous Dirac scis-
sors parlour-game trick, an amusing demonstration in which you thread
string around the eyes of a pair of scissors and a nearby chair. Two full
rotations of the scissors leave the string apparently highly tangled but
with a few deft movements which do not involve further rotations you
can untangle the string. The string serves to make the trick more com-
plicated than it need be, and a more transparent version can be carried

15.4
Combinations of discrete and continuous transformations
141
out using a scarf or a belt, with one end ﬁxed by, e.g. placing it under a
book, as shown in Fig. 15.7(a). The free end is rotated by two full turns
(i.e. by 720 degrees) in the same sense and looks very twisted, as shown
in Fig. 15.7(b). It can be untwisted without rotating the free end in the
opposite sense, simply by passing the free end around the middle of the
belt and pulling taut, see Fig. 15.7(c)-(f). Try it!
To show this point up more clearly in group theory, we could also
use another group to represent rotations: SU(2). This is the special
unitary group of 2 × 2 matrices, where again the ‘special’ means the
determinant is one. This can be used to rotate spinors.
(f)
(d)
(e)
(c)
(b)
(a)
Fig. 15.7 Dirac’s scissor trick is more
easily demonstrated with a belt with
one end free and the other end held
ﬁxed by placing it under a book. [Fig-
ure from S. J. Blundell, Magnetism: A
Very Short Introduction, OUP (2012).]
Example 15.6
To rotate by an angle θ about an axis deﬁned by ˆn we can use the rotation matrix
which we’ve previously written as R(θ). Here we will write θ = θˆn, and we will
employ a useful identity which is
σ · n =
„
nz
nx −iny
nx + iny
−nz
«
,
(15.22)
where σ = (σx, σy, σz) are the Pauli spin matrices. Our rotation matrix, which we
will now write as R(ˆn, θ), is given by
R(ˆn, θ) = exp
„
−i θ
2σ · n
«
= I cos θ
2 −i sin θ
2 σ · n,
(15.23)
where I is the identity matrix. These matrices have an interesting feature, which is
that while as you would expect
R(ˆn, 0) = I,
(15.24)
you also have
R(ˆn, 2π) = −I,
(15.25)
and you need a full 4π rotation to recover
R(ˆn, 4π) = I.
(15.26)
The previous example shows that SU(2) is actually a double cover of
SO(3), meaning that for a particular rotation there are two represen-
tations in SU(2) for every one representation in SO(3). For example,
the identity (no rotation at all) is just diag(1, 1, 1) in SO(3), but is
represented by both diag(1, 1) and diag(−1, −1) in SU(2).
A spinor can be written as a two-component entity

a
b

where a and
b are complex numbers and |a|2 + |b|2 = 1. Writing a = x0 + ix1 and
b = x2 + ix3 where xi are real we see that this condition is equivalent
to x2
0 + x2
1 + x2
2 + x2
3 = 1 and so SU(2) is isomorphic to S3, that is
the 3-sphere.11 Thus SU(2) is simply connected, in contrast to SO(3).
11It may be helpful to remember:
• S1 is a circle, a one-dimensional
space that we normally draw on
a two-dimensional piece of pa-
per. It can be described with the
equation x2 + y2 = 1.
• S2 is a 2-sphere, or more com-
monly just a sphere (meaning
the surface of a ball), a two-
dimensional space that can be
embedded in three-dimensions.
It can be described with the
equation x2 + y2 + z2 = 1.
• S3
is
a
3-sphere,
a
three-
dimensional space that can be
embedded in four-dimensions. It
can be described with the equa-
tion x2 + y2 + z2 + w2 = 1.
For more details on topological nota-
tion, see Chapter 29.
The formal relationship between the two groups is SO(3) ∼= SU(2)/Z2,
meaning that SO(3) is a quotient group.
These arguments can be generalized to the connected component of
the Lorentz group: SO(1, 3) ∼= SL(2, C)/Z2, where SL(2, C) is the group
of 2 × 2 complex matrices with unit determinant. This gives you some
ﬂavour of the way in which the mathematical structure of the spaces
describing the spacetime transformations can be described using group
theory.

142
Discrete transformations
circle
S1
(x0)2 + (x1)2 = 1
[S1 ∼= U(1)]
sphere
S2
(x0)2 + (x1)2 + (x2)2 = 1
n-sphere
Sn
(x0)2 + (x1)2 + · · · + (xn)2 = 1
n-torus
T n
S1 × S1 × · · · × S1 (n ≥2)
real n × n matrices
general linear group
GL(n, R)
n × n real matrices
special linear group
SL(n, R)
GL(n, R) with det M = 1
orthogonal group
O(n)
GL(n, R) with MM T = M TM = I
special orthogonal group
SO(n)
O(n) with det M = 1
Lorentz group
O(1, 3)
GL(n, R) with MgM T = g
special Lorentz group
SO(1, 3)
O(1, 3) with det M = 1
complex n × n matrices
general linear group
GL(n, C)
n × n complex matrices
special linear group
SL(n, C)
GL(n, C) with det M = 1
unitary group
U(n)
GL(n, C) with MM † = M †M = I
special unitary group
SU(n)
U(n) with det M = 1
Table 15.1 Some useful mathematical objects.
Chapter summary
• Discrete symmetries include charge conjugation C (which changes
all particles into their antiparticles and vice versa), parity P which
inverts spatial coordinates and time-reversal T which reverses time.
• The combined symmetry CPT holds for a Lorentz invariant, local,
Hermitian quantum ﬁeld theory.
• For reference, some useful mathematical objects are tabulated in
Table 15.1.
Exercises
(15.1) Why is the reaction π0 →γ + γ + γ not allowed?
(15.2) Classify the following as scalars, pseudoscalars, vec-
tors (polar vectors) or pseudovectors (axial vec-
tors): (a) magnetic ﬂux; (b) angular momentum;
(c) charge; (d) the scalar product of a vector and a
pseudovector; (e) the scalar product of two vectors;
(f) the scalar product of two pseudovectors.
(15.3) Find representations for the spinor rotation matri-
ces (a) R(ˆx, θ), (b) R(ˆy, θ) and (c) R(ˆz, θ).

Part IV
Propagators and
perturbations
Quantum ﬁeld theory is particularly challenging when one has to deal
with interacting systems. One method to calculate results uses perturba-
tion theory and this can be evaluated using various techniques involving
Green’s functions and Feynman diagrams. This part introduces some of
these techniques.
• In Chapter 16 we introduce Green’s functions and show how they
are related to a propagator, the amplitude that a particle in some
spacetime position will be later found at another spacetime posi-
tion.
• We connect Green’s functions to propagators in a quantum ﬁeld
picture in Chapter 17 and introduce the Feynman propagator which
involves time-ordering of operators. This is applied to Yukawa’s
model of virtual particle exchange.
• Interactions are added to the picture in Chapter 18 and in partic-
ular the idea of the S-matrix. We work this out in the interaction
representation, another way of writing down states and operators
which turns out to be very useful for these problems. These ideas
are applied to a perturbation expansion and use is made of Wick’s
theorem which provides a method for simplifying products of op-
erators.
• In Chapter 19 we introduce Feynman diagrams which provide a
wonderful pictorial way of visualizing terms in the perturbation
expansion. Rather than evaluating umpteen integrals we can sum-
marize many complex calculations in diagrammatic form and eval-
uate the results using Feynman rules for a particular situation.
• These ideas are applied to scattering theory in Chapter 20 for a
simple ψ†ψφ model which serves as a toy model of QED and the
scattering cross-section is deﬁned and then evaluated.

16
Propagators and Green’s
functions
16.1 What is a Green’s function?
144
16.2 Propagators in quantum me-
chanics
146
16.3 Turning it around: quantum
mechanics from the propaga-
tor and a ﬁrst look at pertur-
bation theory
149
16.4 The many faces of the prop-
agator
151
Chapter summary
152
Exercises
152
When will the world know that peace and propagation are
the two most delightful things in it?
Horace Walpole, 4th Earl of Oxford (1717–1797)
One way to do quantum mechanics is to calculate a wave function and
operate on it with quantum operators. Another way is to directly con-
sider amplitudes for a given process, such as ‘the amplitude that my
particle starts at point y at a time ty and ends up at point x at time
tx’, which we write down1 as ⟨x(tx)|y(ty)⟩. This amplitude is known
1As Feynman notes, quantum ampli-
tudes are like Hebrew in that you read
them right-to-left.
as a propagator. We’ve spent time in previous chapters removing the
wave function from its role as the most useful entity in quantum physics
and we’ll see that propagators represent an alternative to wave functions
and enable us to extract all of the same information. In fact, we’ll see
that wave functions are actually special cases of propagators and con-
tain less information! For our purposes, propagators represent the most
economical way to calculate all of the properties of quantum ﬁelds in an
interacting system of many particles. In addition propagators for sin-
gle particles have a neat mathematical property: they are the Green’s
functions2 of the equation of motion for a particle.
2These are named after George Green
of Sneinton, Nottinghamshire (1793–
1841) who was perhaps the greatest
self-taught mathematical physicist of
modern times. His achievements are all
the more remarkable since, until 1829,
his day job consisted of running a wind-
mill. While still a miller he published
his most celebrated work ‘An Essay on
the Application of Mathematical Anal-
ysis to the Theories of Electricity and
Magnetism’ (1828). The essay included
the ﬁrst use of what are now called
Green’s functions.
The plan for this chapter is as follows: (i) we’ll explain what’s meant
by a Green’s function; (ii) we’ll demonstrate that propagator amplitudes
are the Green’s’ functions of quantum mechanics: they tell us the am-
plitudes for a particle to start at a spacetime point (y, ty) and then be
detected at a point (x, tx); (iii) ﬁnally we’ll show that quantum mechani-
cal perturbation theory is conveniently expressed in terms of propagators
and that these propagators are most conveniently expressed in terms of
cartoons.
16.1
What is a Green’s function?
Here’s a very general looking diﬀerential equation formed from a linear
diﬀerential operator ˆL:
ˆL x(t) = f(t).
(16.1)
The Green’s function G(t, u) of the linear operator ˆL is deﬁned by the
equation
ˆL G(t, u) = δ(t −u).
(16.2)

16.1
What is a Green’s function?
145
Notice that the Green’s function G(t, u) is a function of two variables, t
and u.
Example 16.1
Let’s consider a familiar example from mechanics. We’ll take an oscillator of mass m
and spring constant K evolving under the inﬂuence of a time-dependent force f(t).
The equation of motion for this is
m d2
dt2 x(t) + Kx(t) = f(t),
(16.3)
so here the linear operator is ˆL = m d2
dt2 + K. We can build up a force function f(t)
by adding together lots of delta functions.
f(t) =
Z ∞
0
du f(u)δ(t −u),
(16.4)
where u us a dummy variable which allows us to break f(t) into deltas. This is, of
course, merely an example of superposition and is allowed because our diﬀerential
equation is linear.
Instead of hitting the problem head-on, we sidestep and solve eqn 16.3 for just one
of the delta functions. We’ll call this solution the Green’s function G(t, u):
»
m d2
dt2 + K
–
G(t, u) = δ(t −u).
(16.5)
The complete solution to eqn 16.3 is then given by the integral of the Green’s function,
weighted by the force function f(u):
x(t) =
Z ∞
0
du G(t, u)f(u).
(16.6)
The solution is therefore simply a sum of the responses of the system in x to the
stimuli f(u).
Why is the solution to eqn 16.3 given by eqn 16.6? Start by acting on eqn 16.6
with the operator ˆL (which passes through the integral sign on the right-hand side),
ˆL x(t)
=
Z
du ˆL G(t, u)f(u)
=
Z
du δ(t −u)f(u) by eqn 16.2
=
f(t) by eqn 16.4,
(16.7)
which is where we started! This implies we can solve an inhomogeneous diﬀerential
equation by ﬁnding the Green’s function G(t, u) and then integrating over f(u) to
get the solution x(t).
It’s now clear why the Green’s function G(t, u) needs two arguments:
one is the variable we’re interested in (here it’s the time t), the other u
is the variable that describes the position of the delta function δ(t −u)
that we use to deﬁne the Green’s function via ˆL G(t, u) = δ(t −u). The
inhomogeneous part of the equation [here f(t)] is built up from a set
of delta functions weighted by an amplitude function f(u), that is a
function of the dummy variable u.
Now we know what use they are, we can ﬁnd the Green’s functions
for some commonly encountered ˆL operators.3
3Remember that δ(x) = δ(−x) so we
can swap the order of the arguments in
the delta function for convenience.

146
Propagators and Green’s functions
Example 16.2
Examples of ﬁnding some Green’s functions.
• The Green’s function for ˆL = ∇2 can be read oﬀusing an argument
from electromagnetism. We know Poisson’s equation (reverting to SI units):
∇2V (x) = −ρ(x)
ǫ0 , which for a point charge of magnitude unity, located at u,
reads
ǫ0∇2V (x) = −δ(3)(x −u).
(16.8)
The potential that solves this equation is known from electromagnetism to be
V (x) =
1
4πǫ0|x−u|, and so
∇2
»
1
4π|x −u|
–
= −δ(x −u),
(16.9)
or
G(x, u) = −
1
4π|x −u|.
(16.10)
• The Green’s function for ˆL = (∇2 + k2)is deﬁned by
`
∇2 + k2´
Gk(x, u) = δ(3)(x −u).
(16.11)
The solution which will interest us is
Gk(x −u) = −ei|k||x−u|
4π|x −u|,
(16.12)
which describes the amplitude of an outgoing, spherical wave.
16.2
Propagators in quantum mechanics
We know that the equation that governs the change of a wave func-
tion φ(x, t) is the Schr¨odinger equation ˆHφ(x, t) = i ∂φ(x,t)
∂t
, where ˆH is
an operator function of x and we’ll only consider (1 + 1)-dimensional
spacetime for now.4 Why might the Green’s function of the Schr¨odinger
4(1+1)-dimensional spacetime means
one spatial dimension and one time
dimension.
We normally think about
(3+1)-dimensional
spacetime
with
three spatial dimensions and one time
dimension.
equation be useful, and what interpretation might such a function have?
The real beauty of a Green’s function is the property that we had in
eqn 16.6, namely that
φ(x, tx) =
Z
dy G+(x, tx, y, ty)φ(y, ty).
(16.13)
(The + sign notation will be explained shortly.) Here the Green’s func-
tion takes a wave function at some time and place and evolves it to
another time and place. To ﬁnd out how it does this we need to inte-
grate over space (but not time).5
5We
don’t
need
to
integrate
over
all
time
because
the
multiplication
G(x, tx, y, ty)φ(y) does that part au-
tomatically,
as
in
the
case
of
the
time-evolution operator which, as we’ll
shortly see, is closely related to the
Green’s function.
In fact, we say that the Green’s function propagates the particle from
the spacetime point (y, ty) to (x, tx), which explains why we call G+ a
propagator. Here the plus-sign superscript in G+ means that we con-
strain tx > ty and we deﬁne G+ = 0 for tx < ty, which prevents particles
going back in time.6
6With this in mind we call G+ the
time-retarded
Green’s
function,
deﬁned as
G+ =
(
G
tx > ty
0
tx < ty,
(16.14)
or more simply
G+ = θ(tx −ty)G.
(16.15)
We can also deﬁne the time-advanced
Green’s function by
G−=
(
0
tx > ty
G
tx < ty,
(16.16)
or more simply
G−= θ(ty −tx)G.
(16.17)
Looking at eqn 16.13 we can interpret φ(y, ty) as the amplitude to ﬁnd
a particle at (y, ty) and φ(x, tx) as the amplitude to ﬁnd a particle at

16.2
Propagators in quantum mechanics
147
(x, tx). It follows that the propagator G+(x, tx, y, ty) is the probability
amplitude that a particle in state |y⟩at time ty, ends up in a state |x⟩
at time tx. The interpretation means that the Green’s function may be
written
G+(x, tx, y, ty) = θ(tx −ty)⟨x(tx)|y(ty)⟩.
(16.18)
Note that, using this language, our old friend the wave function
φ(x, tx) = ⟨x|φ(t)⟩is simply the amplitude that a particle is found at
(x, tx) irrespective of where it started. The propagator therefore con-
tains more information, since it cares where the particle started.
Example 16.3
From the basic deﬁnition of the propagator G+(x, tx, y, ty) = θ(tx −ty)⟨x(tx)|y(ty)⟩
we can invoke the time-evolution operator so that the time dependence is taken away
from the states
G+(x, tx, y, ty)
=
θ(tx −ty)⟨x| ˆU(tx −ty)|y⟩
=
θ(tx −ty)⟨x|e−i ˆ
H(tx−ty)|y⟩.
(16.19)
We can also expand the amplitudes in terms of the eigenstates of the operator ˆH,
which we call |n⟩, which have eigenvalues En,
G+(x, tx, y, ty)
=
θ(tx −ty)⟨x|e−i ˆ
H(tx−ty)|y⟩
=
θ(tx −ty)
X
n
⟨x|e−i ˆ
H(tx−ty)|n⟩⟨n|y⟩.
(16.20)
Remembering that ⟨x|n⟩is just a fancy way of writing a wave function φn(x), we see
that, in general, the propagator may be written in terms of the eigenfunctions of ˆH
as
G+(x, tx, y, ty) = θ(tx −ty)
X
n
φn(x)φ∗
n(y)e−iEn(tx−ty),
(16.21)
allowing us to relate wave functions and propagators.
So far we’ve assumed the propagator G+(x, tx, y, ty) is some sort of
Green’s function of the Schr¨odinger equation: now we’ll conﬁrm this.
We deﬁne the Green’s function for the Schr¨odinger equation by consid-
ering G+(x, tx, y, ty) as a function of x and tx only and say

ˆHx −i ∂
∂tx

G+(x, tx, y, ty) = −iδ(x −y)δ(tx −ty) = −iδ(2)(x −y),
(16.22)
where ˆHx only touches x-coordinates and not y-coordinates. Here we
hold y and ty ﬁxed as dummy variables. Finally, we want to conﬁrm
that the amplitude ⟨x(tx)|y(ty)⟩is truly the Green’s function of the
Schr¨odinger equation.
Example 16.4
Here’s how is goes. The Green’s function is given by
G+(x, tx, y, ty) = θ(tx −ty)⟨x(tx)|y(ty)⟩= θ(tx −ty)
X
n
φn(x)φ∗
n(y)e−iEn(tx−ty),
(16.23)

148
Propagators and Green’s functions
where we’ve explicitly included the θ function to ensure that the particle can’t prop-
agate backward in time. Substituting the Green’s function into eqn 16.22, we ﬁnd
that we want to work out
»
ˆHx −i ∂
∂tx
–
θ(tx −ty)
X
n
φn(x)φ∗
n(y)e−iEn(tx−ty).
(16.24)
Take this in two stages. Stage I: We use the fact that
∂
∂tx θ(tx −ty) = δ(tx −ty),
to ﬁnd that the time derivative acting on the retarded Green’s function gives us
i ∂
∂tx
G+
=
iδ(tx −ty)
X
n
φn(x)φ∗
n(y)e−iEn(tx−ty)
+θ(tx −ty)
X
n
Enφn(x)φ∗
n(y)e−iEn(tx−ty).
(16.25)
Stage II: Now considering
ˆHx acting on the Green’s function, we note that
ˆHxφn(x) = Enφ(x) is the only thing that is aﬀected by the ˆHx operator. We have
ˆHxG+ = ˆHxθ(tx −ty)⟨x(tx)|y(ty)⟩= θ(tx −ty)
X
n
Enφn(x)φ∗
n(y)e−iEn(tx−ty).
(16.26)
Putting all of this together, we obtain
»
ˆHx −i ∂
∂tx
–
G+(x, tx, y, ty)
=
−iδ(tx −ty)
X
n
φn(x)φ∗
n(y)e−iEn(tx−ty)
=
−iδ(tx −ty)δ(x −y),
(16.27)
which is what we set out to prove.
Now let’s try to ﬁnd a propagator for free particles. Our ﬁrst stop will
be the free-particle propagator in position space and the time domain,
which we call G+(x, tx, y, ty).
Example 16.5
A non-relativistic, free particle has a Hamiltonian ˆH = ˆp2/2m, with eigenfunctions
φ(x) =
1
√
Leipx and eigenvalues Ep =
p2
2m . Treating p as a continuous variable, we
start with eqn 16.21, and transforming the sum into an integral7 we have
7We use the replacement
X
n
→L
Z
dp
2π .
Note for later that we will often make
the replacement in three dimensions
X
p
→V
Z
d3p
(2π)3 ,
where V is the volume.
G+(x, tx, y, ty)
=
θ(tx −ty)L
Z
dp
2π φp(x)φ∗
p(y)e−iEp(tx−ty)
=
θ(tx −ty)
Z
dp
2π eip(x−y)e−i p2
2m (tx−ty).
(16.28)
This integral can be done: it’s a Gaussian integral, of which, much more later (see
Chapter 23). For now, we use the result
R ∞
−∞dx e−ax2
2
+bx =
q
2π
a e
b2
2a with a =
i(tx−ty)
m
and b = i(x −y). This gives
G+(x, tx, y, ty) = θ(tx −ty)
r
m
2πi(tx −ty) e
im(x−y)2
2(tx−ty) .
(16.29)
There are more examples of ﬁnding propagators in the exercises at the
end of this chapter.

16.3
Turning it around
149
16.3
Turning it around: quantum
mechanics from the propagator and a
ﬁrst look at perturbation theory
So far we’ve used things we know about quantum mechanics to derive
some properties of a single-particle propagator. In fact, we want to turn
this around. If we start from the propagator, what can we learn about
the particle? This is most easily seen by considering yet another form
of the Green’s function, again as a function of space but this time in the
frequency/energy domain. This one looks like
G+(x, y, E) =
X
n
iφn(x)φ∗
n(y)
E −En
.
(16.30)
(You’re invited to prove this relation in Exercises 16.2.)
We can notice two things about this equation when we consider it as
a function of the complex8 variable E: (i) the singularities (or poles) on
8Readers
unfamiliar
with
complex
analysis can refer to Appendix B.
the real axis occur when E = En, that is, when the parameter E equals
the energies of the eigenstates φn(x); and (ii) the residues at the poles
are (i times) the wave functions. We see that by being able to write
down the Green’s function of a system we have access to the energies
of the system and its wave functions.9 Propagators, with their ‘from
9Actually, to ensure causality it’s nec-
essary to replace En with En −iǫ so we
have
G+(x, y, E) = lim
ǫ→0+
X
n
iφn(x)φ∗
n(y)
E −En + iǫ .
(16.31)
here to there’ deﬁnition, also have the appealing property that they
can be drawn in a cartoon form showing a particle travelling from y to
x. This isn’t quite as trivial as it sounds: Bohr was doubtful whether
the trajectory of a quantum particle could be thought about due to
position–momentum uncertainty, but it can. The quantum propagator,
which gives this trajectory some meaning, is illustrated in Fig. 16.1.
y, ty
x, tx
G+(x, y)
Fig. 16.1 A cartoon representation of
the propagator. It will turn out to be
the basic unit of the Feynman diagram.
This doesn’t explain what all the fuss is about.
Why are Green’s
functions so useful? One place to look is perturbation theory. In most
interesting cases we can’t solve a quantum mechanical problem exactly.
What we do is write the problem by splitting up the Hamiltonian into
two parts: H = H0 + V , where H0 is the solvable part and V is the
perturbing potential. Perturbation theory involves the tedious task of
evaluating changes to eigenfunctions and eigenvalues in a series of in-
creasingly complicated corrections.
Let’s look again at our Green’s functions and see if they lend any
insight to the perturbation problem.
For simplicity, we’ll write the
original Green’s function equation in a symbolic, matrix-like form as
(H −E)G = −1.10 (Remember throughout that G just describes a par-
10Objects like G should be treated with
caution: they’re not well deﬁned math-
ematical objects, just symbols com-
bined in the shape of the original equa-
tions that we’re using to suggest rela-
tionships. Needless to say these argu-
ments are repeated rigorously in the lit-
erature.
ticle propagating from y to x.) The symbolic Green’s function equation
is solved by writing G =
1
E−H, which bears a resemblance to the form in
eqn 16.31.
Green’s functions allow us to interpret a perturbation problem in
terms of a propagating particle. We think of the solvable part of the
problem in terms of a particle propagating from point to point.
We
think of the perturbation V as a scattering process that interrupts
the propagation. To visualize this we’ll write a symbolic expression for

150
Propagators and Green’s functions
the Green’s function in the presence of a perturbation as G =
1
E−H0−V,
where we’ve used H = H0+V. In perturbation problems G (with no sub-
script) is called the full propagator. Just considering the solvable part
of the Hamiltonian H0 we can write G0 =
1
E−H0 , that is, we can ﬁnd the
Green’s function for a solvable problem. The ‘0’ subscript means that
we’re talking about a particle freely propagating, with no scattering and
so, in general, G0 is known as the free propagator. If we now consider
the perturbed problem and try to expand the full propagator G as a
function of the free propagator G0 we can say
There’s a matrix identity that says
1
A + B = 1
A −1
A B 1
A + 1
A B 1
A B 1
A −. . .
G
=
1
E −H0 −V =
1
E −H0
+
1
E −H0
V
1
E −H0
+
1
E −H0
V
1
E −H0
V
1
E −H0
+ . . .
(16.32)
The point here is that, in general, we can’t calculate 1/(E −H0 −V).
However, we can calculate each of the terms on the right-hand side of
this expression and we might hope that G may be well approximated by
just the ﬁrst few of these.
Switching back from symbolic relations to real functions11 shows that
11In the rest of this section we will
write the functions as G, rather than
G+, since the expressions in eqns 16.33
and 16.34 apply generally to G, G+ and
G−.
we’ve succeeded in writing the full Green’s function as an expansion in
free propagators and potentials. The form of the expansion is
G = G0 + G0V G0 + G0V G0V G0 + . . .
(16.33)
This is a memorable equation and easily turned into pictures as shown
in Fig. 16.2. The perturbation problem, previously involving an inﬁnite
series of unmemorable terms, has been turned into a vivid picture of
particle scattering. The amplitude for a particle to go from y to x is
G. This is a superposition of the amplitude for moving freely from y
to x (which is G0); added to the amplitude for making the trip with a
single scattering event at some point along the way G0V G0; added to the
amplitude for making the trip with two scatters G0V G0V G0 and so on.
Thus the algebra of Green’s functions is highly amenable to visualization
of the physical processes.
G
G0
G0V G0
G0V G0V G0
G0
G0
G0
V
G0
G0
G0
V
V
Fig. 16.2
The scattering processes
that lead to the full propagator in per-
turbation theory.
Example 16.6
The perturbation expansion in eqn 16.33 is just a geometric series which we can
rewrite as
G
=
G0 (1 + V G0 + V G0V G0 + . . .)
=
G0
1 −V G0
=
1
G−1
0
−V
.
(16.34)
This is known as Dyson’s equation. This equation is also derived from the algebraic
manipulation of the cartoons, as shown in Fig. 16.3. This concept will turn out to be
very useful. Notice that it is non-perturbative: we don’t consider just the ﬁrst few
of the terms in a perturbation series, we sum all of them to inﬁnity.
V
V
V
V
V
V
V
V
−1
Fig. 16.3
The derivation of Dyson’s
equation in graphical form.

16.4
The many faces of the propagator
151
16.4
The many faces of the propagator
So far we have encountered the free-particle propagator in the space
and time domain G+
0 (x, tx, y, ty) and in the space and energy domain
G+
0 (x, y, E). There are other useful forms that we can derive.
Example 16.7
The physical interpretation of G+ allows us to work out another expression for the
Green’s function, this time in the momentum space and in the time domain, so we
write G+
0 (p, tx, q, ty). Remember the interpretation: we want the amplitude that a
particle that starts oﬀin a state |q⟩will, after a time tx −ty, end up in a state |p⟩.
Thus12
12Note that, for simplicity in these ex-
amples, we’re using the non-relativistic
normalization so that ⟨p|q⟩= δ(p −q).
G+
0 (p, tx, q, ty)
=
θ(tx −ty)⟨p| ˆU(tx −ty)|q⟩
=
θ(tx −ty)⟨p|q⟩e−iEq(tx−ty)
=
θ(tx −ty)δ(p −q)e−iEq(tx−ty).
(16.35)
We see that the free particle cannot change its momentum state, so having both p
and q is redundant, so we can write the same equation in the following shorthand:
G+
0 (p, tx, ty) = θ(tx −ty)e−iEp(tx−ty).
(16.36)
Notice that we often miss out the delta functions if we decrease the number of argu-
ments we list in G+. Strictly we should write all arguments and delta functions.
For dealing with perturbation theory problems, propagators are often
most useful to know in the momentum and energy domain.
Example 16.8
To get to G+
0 (p, E) we’ll take a Fourier transform of G+
0 (p, t, 0)
G+
0 (p, E)
=
Z
dt eiEtG+
0 (p, t, 0)
=
Z
dt eiEtθ(t)e−i(Ep−iǫ)t
=
−iei(E−Ep+iǫ)t
E −Ep + iǫ
˛˛˛˛˛
∞
0
=
i
E −Ep + iǫ .
(16.37)
Remember again, that if we are being strict we should write this as
We make the replacement Ep →Ep−iǫ
to ensure the convergence of the inte-
gral at t = ∞. It also eﬀectively ensures
causality is obeyed.
See Appendix B
and Exercises 16.2 and 16.4 for the de-
tails.
G+
0 (p, E, q, E′) =
i
E −Ep + iǫδ(p −q),
(16.38)
telling us the amplitude for a particle with energy E′ and momentum q to enter and
a particle with energy E and momentum p to leave. The delta function conserves
energy and momentum.

152
Propagators and Green’s functions
As a helpful summary, we assemble a table of all of the ﬂavours of Green’s
function that we’ve introduced so far.
G+
0 (x, tx, y, ty)
=
θ(tx −ty)
X
n
φn(x)φ∗
n(y)e−iEn(tx−ty)
G+
0 (x, y, E)
=
X
n
iφn(x)φ∗
n(y)
E −En + iǫ
G+
0 (p, tx, ty)
=
θ(tx −ty)e−iEp(tx−ty)
G+
0 (p, E)
=
i
E −Ep + iǫ
Chapter summary
• Green’s functions are used as propagators which express the am-
plitude that a particle goes from spacetime point y to spacetime
point x. Various forms of the Green’s function are shown in the
table above.
• Dyson’s equation for the full propagator G is the perturbation
expansion: G = G0 +G0V G0 +· · · = (G−1
0
−V )−1 where G0 is the
free propagator and V describes a scattering process.
Exercises
(16.1) (a) Solve the Schr¨odinger equation to ﬁnd the wave
functions φn(x) for a particle in a one-dimensional
square well deﬁned by V (x) = 0 for 0 ≤x ≤a and
V (x) = ∞for x < 0 and x > a.
(b) Show that the retarded Green’s function for this
particle is given by
G+(n, t2, t1) = θ(t2 −t1)e
−i
„
n2π2
2ma2
«
(t2−t1). (16.39)
(c) Find G+(n, ω) for the particle.
(16.2) (a)
Using
a
damping
factor
e−ǫt,
show
that
G+
0 (x, y, E) = P
n
iφn(x)φ∗
n(y)
E−En+iǫ .
(b) The θ-function may be written as an integral
θ(t) = i
Z ∞
−∞
dz
2π
e−izt
z + iǫ.
(16.40)
Use this to derive G+
0 (p, E) =
i
E−Ep+iǫ, without
recourse to a damping factor.
(16.3) Consider
the
one-dimensional
simple
harmonic
oscillator
with
a
forcing
function
f(t)
=
˜F(ω)e−iω(t−u) described by an equation of motion:
m ∂2
∂t2 A(t −u) + mω2
0A(t −u) = ˜F(ω)e−iω(t−u).
(16.41)
(a) Show that the solution to this equation is
A(t −u) = −
˜F(ω)
m
e−iω(t−u)
ω2 −ω2
0
+ B(t),
(16.42)
where B(t) is the solution to the homogeneous equa-
tion of motion.
(b) Use this result to show that the general form of

Exercises
153
the Green’s function G(t, u) for a simple harmonic
oscillator is given by
G(t, u) = −1
m
Z dω
2π
e−iω(t−u)
ω2 −ω2
0
+ B(t).
(16.43)
(c) If G(t, u) is subject to the boundary conditions
that at t = 0 we have G = 0 and ˙G = 0, show that
G+(t, u) =
1
mω0 sin ω0(t −u),
(16.44)
for 0 < u < t.
Hint: This can be done with a
Laplace transform of the diﬀerential equation for
the Green’s function, or by using complex analysis
to do the integral in the previous problem subject to
the boundary conditions.
(d) Show that the trajectory of a particle in a sim-
ple harmonic potential subject to a force f(t) =
F0 sin ω0t is given by
F0
2mω2
0 (sin ω0t −ω0t cos ω0t).
(16.4) (a) By taking the Fourier transform of the equation
(∇2 + k2)Gk(x) = δ(3)(x),
(16.45)
show that the momentum Green’s function implied
is
˜Gk(q) =
1
k2 −q2 .
(16.46)
Note that ˜Gk(q) is undeﬁned when q2 = k2. This
reﬂects the fact that there is an ambiguity in ˜Gk(q)
in that we don’t know if the wave is incoming, out-
going or a standing wave. This is sorted out with
an iǫ factor as shown in the next part.
(b) Take the Fourier transform of G+
k (x) = −ei|k||x|
4π|x|
to show that, for outgoing waves,
˜G+
k (q) =
1
k2 −q2 + iǫ.
(16.47)
Hint: To ensure outgoing waves, you could include
a damping factor e−ǫ|x| to damp out waves for
|x| →∞.
*(c) (For aﬁcionados of complex analysis.) Take an
inverse Fourier transform of ˜G+
k (q) =
1
k2−q2+iǫ to
show that it corresponds to the outgoing wave so-
lution G+
k (x) = −ei|k||x|
4π|x| .
Hint: On doing the angular part of the integral you
should obtain:
G+
k (x)= −1
4π2
Z ∞
0
|q| d|q|
q2−k2−iǫ
„ei|q||x|−e−i|q||x|
i|x|
«
.
(16.48)
Extend the lower limit of the integration to |q| =
−∞and identify the positions of the poles in the
complex |q| plane. For an outgoing wave, the con-
tour over which the ei|q||x| part is integrated must
be completed in the upper half-plane; the contour
over which the e−i|q||x| part is integrated must be
completed in the lower half-plane.
*(d) What do you expect for an incoming wave so-
lution?

17
Propagators and ﬁelds
17.1 The ﬁeld propagator in out-
line
155
17.2 The
Feynman
propagator
156
17.3 Finding the free propagator
for scalar ﬁeld theory
158
17.4 Yukawa’s force-carrying par-
ticles
159
17.5 Anatomy of the propagator
162
Chapter summary
163
Exercises
163
An error does not become truth by reason or multiplied prop-
agation, nor does truth become error because nobody sees it.
Mahatma Gandhi (1869–1948)
We describe the Universe by combining ﬁelds to form a Lagrangian den-
sity L[φ(x)]. Our canonical quantization process often allows us to quan-
tize these ﬁelds leading to a Universe pictured as a vacuum acted on by
ﬁeld operators like ˆa†
p.
The excitations of the vacuum that the ﬁeld
operators produce are particles and antiparticles.
Our next task is to come up with a scheme to keep track of these
particles.
Having done away with the wave functions we need some
objects that contain the information about excitations of the system.
These objects are propagator amplitudes denoted by the letter G. These
propagators are analogous to the Green’s function amplitudes discussed
in the previous chapter. The propagators describe the fate of particles
in a ﬁeld theory.
Interacting
and
non-interacting
theories
Non-interacting
Interacting
ˆH0
ˆH = ˆH0 + ˆH′
|0⟩
|Ω⟩
G0(x, y)
G(x, y)
No scattering
Scattering
An important distinction that we will make in this chapter is between
non-interacting and interacting theories. Non-interacting theories are
those which can be diagonalized using canonical quantization.1 Canoni-
1See Chapter 11.
cal quantization allows us to describe the system in terms of a vacuum |0⟩
and non-interacting particles in momentum states |p⟩created with oper-
ators like ˆa†
p. The Hamiltonian for a non-interacting theory is called ˆH0
which may be written as ˆH0 = P
p Epˆa†
pˆap, and we have ˆH0|p⟩= Ep|p⟩,
where Ep gives the energies of the non-interacting particles. In contrast,
interacting theories cannot be diagonalized with canonical quantization.
Put another way, these theories contain extra terms describing interac-
tions between particles that we cannot transform away with canonical
quantization. We call the ground states of interacting theories |Ω⟩and
the Hamiltonian ˆH. If we act on |Ω⟩with the operator ˆa†
p we won’t
necessarily get a state |p⟩. Instead we might produce a superposition of
many particles (whose momenta sum to p). Since most interacting ﬁeld
theories cannot be exactly solved to give, for example, the energies of the
excitations or particles of the theory, we need to develop a perturbation
process to make approximate calculations. This can be done by analogy
with the perturbation process we suggested in the last chapter, which
was based around a series written out in terms of propagators.

17.1
The ﬁeld propagator in outline
155
17.1
The ﬁeld propagator in outline
The deﬁnition of the ﬁeld propagator involves a simple thought experi-
ment. We start with our interacting system in its ground state, which we
denote by |Ω⟩. The thought experiment works as follows: we introduce
an extra particle of our choice. We will use this extra particle to probe
the system. The new particle is introduced (created) at a spacetime
point (y0, y). It interacts with the system, possibly causing excitations
in the ﬁelds and all manner of complicated things.
Then we remove
(annihilate) the particle at spacetime point (x0, x) and ask if the system
has remained in the interacting ground state |Ω⟩. We’re interested in the
amplitude G+(x, y) for the experiment, given by
G+(x, y) =

Ω


Particle annihilated
at (x0, x)
 
Particle created
at (y0, y)
 Ω

.
(17.1)
That is, the probability amplitude that the system is still in its ground
state after we create a particle at y and later annihilate it at x. The
amplitude for this process will depend on the complicated interaction of
the probe particle with the system and, it will turn out, will tell us a
great deal about the system itself. By analogy with the results of the
previous chapter, the amplitude G+(x, y) is called the Green’s function
or propagator.2 So much for deﬁnitions. The mathematical object that
2We call the propagator for an interact-
ing theory G(x, y) and its ground state
|Ω⟩. For a free theory (with no interac-
tions) we call the propagator G0(x, y)
[or sometimes we will use the equiva-
lent notation ∆(x, y)] and the ground
state |0⟩.
tells this story of the propagator outlined above is
G+(x, y) = θ(x0 −y0)⟨Ω|ˆφ(x)ˆφ†(y)|Ω⟩,
(17.2)
where the + sign on the left tells us that the particle is created at y
before being annihilated at x and the θ-function on the right guarantees
this.
Example 17.1
To see that this function does the job we can break down the action of the opera-
tors on the states into stages. Remembering that a Heisenberg operator has a time
dependence deﬁned by ˆφ(t, x) = e+i ˆ
Ht ˆφ(x)e−i ˆ
Ht, and substitution of this gives an
expression for the propagator
G(x, y) = ⟨Ω|ei ˆ
Hx0 ˆφ(x)e−i ˆ
H(x0−y0) ˆφ†(y)e−i ˆ
Hy0|Ω⟩.
(17.3)
Taking this one stage at a time (and temporarily ignoring the possibility of creating
and annihilating antiparticles), we see that:
• e−i ˆ
Hy0|Ω⟩is the state |Ω⟩evolved to a time y0.
• ˆφ†(y)e−i ˆ
Hy0|Ω⟩is that state with a particle added at time y0 at a position y.
• e−i ˆ
H(x0−y0) ˆφ†(y)e−i ˆ
Hy0|Ω⟩is that state time evolved to time x0.
• Now hitting this state with ei ˆ
Hx0 ˆφ(x) removes the particle at time x0.
• We terminate this string on the left with a ⟨Ω| to ﬁnd out how much of the
original state |Ω⟩is left in the ﬁnal state.
The propagator is, therefore, the amplitude that we put a particle in the system at
position y at time y0 and get it out at position x time x0.

156
Propagators and ﬁelds
17.2
The Feynman propagator
We don’t always want to deal with the particle propagator G+ (which is
constrained with the θ-function so that x0 > y0) when thinking about
ﬁelds. The reason is that although G+ accounts for the movement of
particles it misses out the essential information about antiparticles.
Example 17.2
Consider, for example, the ﬁeld creation operator for complex scalar ﬁeld theory
ˆψ†(x) =
R
d3p
(2π)
3
2
1
(2Ep)
1
2
“
ˆa†
peip·x + ˆbpe−ip·x”
, which creates a particle and annihi-
lates an antiparticle. If this acts on the free vacuum ˆψ†(y)|0⟩(no interactions, so
nothing unpredictable can occur) then we create a particle at y, but the antiparticle
part annihilates the vacuum ˆbp|0⟩= 0. Similarly, only the particle part contributes
to ⟨0| ˆψ(x), so the propagator ⟨0| ˆψ(x) ˆψ†(y)|0⟩describes a particle propagating from
y to x, but tells us nothing about antiparticles.
Notation for propagators
In general
G(x, y)
Free propagator
G0(x, y)
Free scalar ﬁelds
∆(x, y)
Photon ﬁelds
D(x, y)
Fermion ﬁelds
G(x, y) or iS(x, y) Richard Feynman struggled over how to include the antiparticle contri-
bution for some time before deciding that the most useful form of the
propagator was one that summed both particle and antiparticle parts.
This form of the propagator makes up the guts of most quantum ﬁeld
calculations and is called the Feynman propagator. How do we get
the propagator Feynman used which contains both particle and antipar-
ticle parts? We need to introduce a new piece of machinery: the Wick
time-ordering symbol T. This isn’t an operator, but is much more
Gian-Carlo Wick (1909–1992)
like the normal ordering symbol N in that it is just an instruction on
what to do to a string in order that it makes sense. The time-ordering
symbol is deﬁned for scalar ﬁelds3 as
3This form applies only to the bosonic
(i.e. commuting) scalar ﬁelds we’re con-
sidering in this chapter.
For an anti-
commuting Fermi ﬁeld ˆψ, we pick up
a minus sign for every permutation, so
we have
T ˆψ(x0) ˆψ(y0)
=
( ˆψ(x0) ˆψ(y0)
x0 > y0
−ˆψ(y0) ˆψ(x0)
x0 < y0.
T ˆφ(x0)ˆφ(y0) =
(ˆφ(x0)ˆφ(y0)
x0 > y0
ˆφ(y0)ˆφ(x0)
x0 < y0,
(17.4)
so that the scalar ﬁelds are always arranged earliest on the right, latest
on the left. The Feynman propagator is then deﬁned as
G(x, y)
=
⟨Ω|T ˆφ(x)ˆφ†(y)|Ω⟩
=
θ(x0 −y0)⟨Ω|ˆφ(x)ˆφ†(y)|Ω⟩+ θ(y0 −x0)⟨Ω|ˆφ(y)† ˆφ(x)|Ω⟩,
(17.5)
where |Ω⟩is the interacting ground state of the system. The propagator
is therefore made up of two parts. The ﬁrst part applies for x0 later
than y0: it creates a particle at y and propagates it to x where it is
destroyed. The second part applies when y0 is later than x0: it creates
an antiparticle at x and propagates the system to point y. Both processes
are included in the total propagator.
If, as in the example above, the system doesn’t contain any interac-
tions then particles just move around passing through each other. In

17.2
The Feynman propagator
157
this case we call the ground state |0⟩and we have the free propagator
sometimes called G0(x, y) or, for scalar ﬁelds, more usually just ∆(x, y)
where
∆(x, y)
=
⟨0|T ˆφ(x)ˆφ†(y)|0⟩.
(17.6)
The free-particle propagator is depicted in Fig. 17.1. The free propagator
∆
Fig. 17.1 The free-particle propagator
∆(x, y) is a line without an interaction
blob since a free particle doesn’t inter-
act with any other particles on its way
from y to x.
forms an essential part of the structure of all perturbation calculations.
This is because we think of interactions as events that take place at
particular spacetime points (the V -blobs in Fig. 17.2) and we imagine
that the particles propagate freely between interactions with the free
propagator.
∆
∆
∆
∆
V
V
V
Fig.
17.2
A third-order term in
the perturbation expansion of G in
terms
of
∆(x, y).
The
amplitude
for
the
process
is
proportional
to
∆(x −z3)V (z3)∆(z3 −z2)V (z2)∆(z2 −
z1)V (z1)∆(z1 −y).
We’ll now derive an expression for the free Feynman propagator
∆(x, y).
Example 17.3
Let’s get an expression for the free propagator. If we use the general expression for
the annihilation ﬁeld acting on the free vacuum φ†(y)|0⟩, we have
ˆφ†(y)|0⟩=
Z
d3p
(2π)
3
2 (2Ep)
1
2
“
ˆa†
p|0⟩eip·y + ˆbp|0⟩e−ip·y”
.
(17.7)
Only the particle creation part contributes (since ˆbp|0⟩= 0) and gives
ˆφ†(y)|0⟩=
Z
d3p
(2π)
3
2 (2Ep)
1
2
|p⟩eip·y.
(17.8)
How do we get the other half?
Take a complex conjugate of eqn 17.8 (swapping
y →x and p →q) to get
⟨0|ˆφ(x) =
Z
d3q
(2π)
3
2 (2Eq)
1
2
⟨q|e−iq·x.
(17.9)
Sandwiching together eqn 17.8 and eqn 17.9 we obtain
⟨0|ˆφ(x)ˆφ†(y)|0⟩
=
Z
d3p d3q
(2π)3(2Ep2Eq)
1
2
e−iq·x+ip·yδ(3)(q −p)
=
Z
d3p
(2π)3(2Ep) e−ip·(x−y),
(17.10)
which corresponds to a particle being created at (y0, y) and propagating to (x0, x)
where it is annihilated at the later time.
We also need to consider the reverse order for the advanced part of the propagator:
ˆφ(x)|0⟩=
Z
d3p
(2π)
3
2 (2Ep)
1
2
“
ˆap|0⟩e−ip·x + ˆb†
p|0⟩eip·x”
.
(17.11)
Here, only the antiparticle creation part contributes (since ˆap|0⟩= 0), to give
ˆφ(x)|0⟩=
Z
d3p
(2π)
3
2 (2Ep)
1
2
|p⟩eip·x.
(17.12)
Again we take the complex conjugate (and change x →y and p →q):
⟨0|ˆφ†(y) =
Z
d3q
(2π)
3
2 (2Eq)
1
2
⟨q|e−iq·y,
(17.13)
yielding, ﬁnally
⟨0|ˆφ†(y)ˆφ(x)|0⟩=
Z
d3p
(2π)3(2Ep)eip·(x−y).
(17.14)
This corresponds to an antiparticle being created at point (x0, x) and propagating
to (y0, y), where it is destroyed at the later time. Putting the two halves (eqns 17.10
and 17.14) together we have our ﬁnal answer for the free propagator ∆(x, y).

158
Propagators and ﬁelds
We have the ﬁnal result that the Feynman propagator is given by
∆(x, y)
=
⟨0|T ˆφ(x)ˆφ†(y)|0⟩
(17.15)
=
Z
d3p
(2π)3(2Ep)[θ(x0 −y0)e−ip·(x−y) + θ(y0 −x0)eip·(x−y)].
Notice the way that this came together. We added together a particle
part from the retarded half of the propagator and an antiparticle from
the advanced part. This is illustrated in Fig. 17.3.
y
x
y
x
ty
tx
ty
tx
Fig. 17.3 The Feynman propagator is
made up of a sum of particle and an-
tiparticle parts.
We’ve been calling these propagators Green’s functions, but are they
really Green’s functions in the sense of Chapter 16? The answer is no
for the interacting case. However, the free-propagator functions are the
Green’s functions of the equations of motion. So for the scalar ﬁeld we
have
(∂2 + m2)∆(x −y)
=
−iδ(4)(x −y).
(17.16)
This is examined in Exercise 17.2. In general, propagators are deﬁned
in such a way that they are true Green’s functions in the absence of in-
teractions. That is, free propagators ∆(x, y) are true Green’s functions.
17.3
Finding the free propagator for scalar
ﬁeld theory
Our expression for the free Feynman propagator for scalar ﬁeld the-
ory in eqn 17.15 is not very useful in its current form. We would like
an expression which doesn’t include the rather non-covariant looking
Heaviside step functions. A simpliﬁcation is achieved by using complex
analysis, as shown in the next example.
Example 17.4
How does the computation unfold?
One of the tools we’ll need is the expansion
of θ(x0 −y0) in the complex plane.4 Consider the ﬁrst term in eqn 17.15, call it
4This is given by
θ(x0 −y0) = i
Z ∞
−∞
dz
2π
e−iz(x0−y0)
z + iǫ
.
[∆(x, y)](1), and substitute this expression for the θ function:
[∆(x, y)](1)
≡
θ(x0 −y0)⟨0|ˆφ(x)ˆφ†(y)|0⟩
=
θ(x0 −y0)
Z
d3p
(2π)3(2Ep)e−iEp(x0−y0)+ip·(x−y)
=
i
Z ∞
−∞
dzd3p
(2π)4(2Ep)
e−i(Ep+z)(x0−y0)+ip·(x−y)
z + iǫ
.
(17.17)
Next we make the substitution z′ = z + Ep, leading to the integral
[∆(x, y)](1) = i
Z ∞
−∞
dz′d3p
(2π)4(2Ep)
e−iz′(x0−y0)+ip·(x−y)
z′ −Ep + iǫ
.
(17.18)
Then we change the deﬁnition of our four-momentum in the integral. We treat z′ as
the new p0, redeﬁning p = (z′, p) = (p0, p) leading to
[∆(x, y)](1) = i
Z ∞
−∞
d4p
(2π)4(2Ep)
e−ip·(x−y)
p0 −Ep + iǫ.
(17.19)

17.4
Yukawa’s force-carrying particles
159
With the redeﬁnition, p0 = E doesn’t equal (p2 + m2)
1
2 any more. It’s now just an
integration variable that we happen to have grouped with the three-momentum to
clean up the integral. Note that we still have that the energy Ep = (p2 + m2)
1
2 .
We can carry the same process out on the second term:
[∆(x, y)](2)
≡
θ(y0 −x0)⟨0|ˆφ†)(y)ˆφ(x|0⟩
=
θ(y0 −x0)
Z
d3p
(2π)3(2Ep)e−iEp(y0−x0)+ip·(y−x)
=
i
Z ∞
−∞
dzd3p
(2π)4(2Ep)
e−i(Ep+z)(y0−x0)+ip·(y−x)
z + iǫ
.
(17.20)
The substitution is now z′ = z + Ep, giving
[∆(x, y)](2) = i
Z ∞
−∞
dz′d3p
(2π)4(2Ep)
e−iz′(y0−x0)+ip·(y−x)
p0 −Ep + iǫ
.
(17.21)
Lastly, make the substitution p →−p, which is to say (p0, p) →(−p0, −p), we get
[∆(x, y)](2)
=
−i
Z ∞
−∞
dz′d3p
(2π)4(2Ep)
e−ip0(x0−y0)+ip·(x−y)
p0 + Ep −iǫ
=
−i
Z ∞
−∞
d4p
(2π)4(2Ep)
e−ip·(x−y)
p0 + Ep −iǫ.
(17.22)
The full propagator ∆(x, y) = [∆(x, y)](1) + [∆(x, y)](2) is, therefore, the sum of
eqns 17.19 and 17.22:
∆(x, y)
=
i
Z
d4p
(2π)4(2Ep) e−ip·(x−y)
„
1
p0 −Ep + iǫ −
1
p0 + Ep −iǫ
«
=
Z
d4p
(2π)4 e−ip·(x−y)
i
(p0)2 −E2p + iǫ.
(17.23)
Note further that using E2
p = p2 + m2 the term in the denominator of the integrand
is (p0)2 −E2
p + iǫ = p2 −m2 + iǫ.
The ﬁnal result of this example yields the key result that the free Feyn-
man propagator5 ∆(x, y) is given by
5Because ∆(x, y) is a function of x −y,
it is sometimes written ∆(x −y).
∆(x, y) = ⟨0|T ˆφ(x)ˆφ(y)|0⟩=
Z
d4p
(2π)4 e−ip·(x−y)
i
p2 −m2 + iǫ.
(17.24)
We can immediately extract ˜∆(p), the Fourier component6 of the Feyn-
6The connection between the quan-
tities ∆(x, y) and
˜∆(p) is a simple
Fourier transform:
∆(x, y) =
Z
d4p
(2π)4 e−ip·(x−y) ˜∆(p).
man propagator corresponding to a particle with momentum p:
˜∆(p) =
i
p2 −m2 + iǫ.
(17.25)
This turns out to be a very useful equation.
17.4
Yukawa’s force-carrying particles
One of the most interesting thing about particles is that they interact
with each other. Hideki Yukawa’s great insight was that this interac-
Hideki Yukawa (1907–1981)

160
Propagators and ﬁelds
tion process itself involved particles. These force-carrying particles are
quite diﬀerent to their more familiar cousins with whom we’re already
acquainted. They are, however, still described by propagators and pro-
vide an immediate application of our ﬁeld theory propagators. Yukawa’s
idea centres around one key notion:
Particles interact by exchanging virtual, force-carrying particles.
Virtual particles are deﬁned as particles existing ‘oﬀmass-shell’. The
mass-shell is the four-dimensional surface obeying the equation p2 =
E2
p −p2 = m2 [see Fig. 11.1]. This, of course, is the usual dispersion for
a relativistic particle. For an oﬀmass-shell particle we have p2 ̸= m2.
How can this be possible? The argument is that quantum mechanics
allows us to violate this classical dispersion, as long as we don’t do it for
too long! By invoking energy-time uncertainty ∆E∆t ∼ℏ, we can say
that particles of energy E are allowed to exist oﬀthe mass-shell as long
as they last for a short time ∆t ≲ℏ/E. Virtual particles, therefore, must
have a ﬁnite range since (i) they can’t live forever and (ii) they travel
at ﬁnite velocity. Yukawa guessed that the potential U(r) mediated by
the virtual particle would have the form
U(r) ∝−e−|r|/a
4π|r| ,
(17.26)
where a has the dimensions of length.
Example 17.5
Looking back, we see that eqn 17.26 bears a resemblance to G+
k (r) = −ei|k||r|
4π|r| , which
is the Green’s function for the operator ˆL = (∇2 + k2). From this, we see that if
we take i|k| = −m, then we have that the Yukawa potential U(r) ∝−e−m|r|
4π|r|
is the
Green’s function for the equation
`
∇2 −m2´
U(r) = δ(3)(r),
(17.27)
which is a time-independent version of the Klein–Gordon equation.
What we’ve
shown here is that Yukawa’s potential is actually the ﬁeld propagator describing the
propagation of virtual scalar ﬁeld particles. The price that’s paid for being oﬀthe
mass shell is that the virtual particles can travel a distance ﬁxed7 by their mass m.
7In our units, the length a = 1/m. (In
SI units, a = ℏ/mc.) For a pion (π0)
with mass 135 MeV/c2, a ≈1.5 fm,
roughly the size of a nucleus.
t
t
y
y
x
x
Fig. 17.4 (a) A virtual particle prop-
agates from y to x mediating a force.
(b) The same process occurs with a vir-
tual antiparticle propagating from x to
y.
(c) The processes are summed to
make the Feynman propagator.
The Yukawa particle exchange process is illustrated in Fig. 17.4. We can
imagine spacetime processes such as that shown in Fig. 17.4(a) where
at time y0 particle A emits a virtual particle Q, with mass mQ from
position y that at a later time x0 collides with particle B at position x.
The energetics of this process may be written
EA = E′
A + EQ,
(17.28)
that is, particle A loses energy EQ = (p2
Q + m2
Q)1/2. Of course in this
process particle B gains energy EQ.

17.4
Yukawa’s force-carrying particles
161
We may be in serious trouble if Yukawa exchange involves a net trans-
fer of energy from particle A. It makes the interaction rather one-sided
for one thing. To maintain the symmetry of the interaction, we must
also imagine the additional process shown in Fig. 17.4(b) where particle
B (at position x) emits an identical particle Q at time x0, which at later
time y0 collides with particle B at y. Both processes are necessary in
order to conserve energy so we must have an identical virtual particle
heading back to A, which gives back the same amount of energy.
What does the Feynman propagator have to say about Yukawa’s no-
tion of force-carrying virtual particles? Actually, the exchange of vir-
tual particles idea is exactly what the Feynman propagator is describing.
Recall that the Feynman propagator, describing a particle propagating
from spacetime point x to spacetime point y, looks like
∆(x, y) = θ(x0−y0)⟨0|ˆφ(x)ˆφ†(y)|0⟩+θ(y0−x0)⟨0|ˆφ(y)† ˆφ(x)|0⟩. (17.29)
The ﬁrst term describes a particle travelling from y to x for x0 > y0,
while the second describes an antiparticle travelling from x to y for
y0 > x0. These situations are what we showed in Fig. 17.4. and we see
that both processes are included in the Feynman propagator. If we now
turn to the Feynman propagator in momentum space we have
˜∆(p) =
i
p2 −m2 + iǫ =
i
(p0)2 −p2 −m2 + iǫ.
(17.30)
Notice that the denominator is always oﬀmass shell, that is to say that
the expression is only well behaved if p0 ̸=
p
p2 + m2. The message is
that the Green’s function describes the propagation of virtual particles.
i
p2−m2+iǫ
Fig. 17.5
A Feynman diagram for
two particles scattering.
The ampli-
tude for the process includes a factor
i/(p2 −m2 + iǫ), which describes the
propagation of a virtual, force-carrying
particle which can be thought of as me-
diating a Yukawa force.
When we come to calculating Feynman diagrams in a few chapters’
time, we’ll see that the amplitude A for two particles to scatter oﬀeach
other as shown in Fig. 17.5 contains a term ∝i/(p2 −m2 + iǫ). We can
check this immediately. If we want to calculate the scattering amplitude
for two particles in non-relativistic quantum mechanics we can use the
Born approximation, which tells us that the amplitude is proportional
to the Fourier transform of the scattering potential:
A
∝
˜U(p) =
Z
d3r U(r)e−ip·r,
(17.31)
where p is a scattering vector. If we use Yukawa’s suggestion for the
interaction potential U(r) ∝−e−m|r|
4π|r|
corresponding to an interaction
with a range of a, then doing the integral8 yields
8The integral can be evaluated using
Z
d3r e−ip·re−m|r|
|r|
=
Z ∞
0
dr |r|2
×
Z 2π
0
dφ
Z π
0
dθ sin θ e−i|p||r| cos θ−m|r|
|r|
=
4π
p2 + m2 .
A ∝
1
−|p|2 −m2 ,
(17.32)
which looks rather like the propagator ˜∆(p) with p0 = 0. Our result for
the propagator ˜∆(p) is the four-dimensional, relativistic generalization
of this result. Indeed, as we saw previously, the propagator
1
−|p|2−m2 is
the Green’s function for the time-independent Klein–Gordon equation
(−∇2 + m2)ˆφ(x) = 0, while ˜∆(p) is the Green’s function for the four-
dimensional Klein–Gordon equation (∂2 + m2)ˆφ(x) = 0.

162
Propagators and ﬁelds
17.5
Anatomy of the propagator
Let’s take a closer look at the anatomy of the free propagator9 ∆(x, y) =
9Remember that for a real scalar ﬁeld
ˆφ(x) = ˆφ†(x) and so we may write the
scalar ﬁeld propagator without the dag-
gers: ∆(x, y) = ⟨Ω|T ˆφ(x)ˆφ(y)|Ω⟩. For
simplicity’s sake, we will do this later
in the book.
⟨0|T ˆφ(x)ˆφ†(y)|0⟩for scalar ﬁeld theory. It arose from the amplitude that
a single ﬁeld excitation is created at spacetime point y and is found at
spacetime point x. It contains the sum of amplitudes corresponding to
the possibility that this excitation might be a particle or an antiparticle.
In position space we have that
∆(x, y) = ⟨0|T ˆφ(x)ˆφ†(y)|0⟩=
Z
d4p
(2π)4 e−ip·(x−y)
i
p2 −m2 + iǫ. (17.33)
It is important to notice that we have a singularity (or pole10) in the
10Another way of making sense of the
iǫ is that it is meant to ensure that
the integral never hits this pole. It is
an instruction to avoid it by adding an
inﬁnitesimal imaginary part to the de-
nominator.
This is discussed in Ap-
pendix B.
function when the particle is on mass shell, that is, when p2 = E2
p−p2 =
m2.
Now let’s return to the full propagator for a general interacting the-
ory G(x, y) = ⟨Ω|ˆφ(x)ˆφ†(y)|Ω⟩. (Remember that we call the interacting
ground state |Ω⟩.) The full name for G(x, y) is the ‘single-particle prop-
agator’ or the ‘two-point Green’s function’.
We can draw a diagram
illustrating G(x, y) as shown in Fig. 17.6(a).
The two stumps corre-
spond to the incoming or outgoing particles. The hatched blob in the
middle depicts interactions: the adventures (involving other particles)
that our test particle might have between its creation and annihilation.
The name ‘two-point’ comes from the fact that a single particle is cre-
ated and a single particle is annihilated, that is, the sum of particles
entering or leaving the system, i.e. (number in)+(number out) equals 2.
We often emphasise this by writing the propagator as G(2)(x, y) with
a superscript ‘(2)’. We can go further and deﬁne the n-point propaga-
tor, where n equals the sum of incoming and outgoing lines. These will
be denoted G(n). For example we might want to study the four-point
propagator [Fig. 17.6(b)]
G(4)(x1, x2, x3, x4) = ⟨Ω|T ˆφ(x1)ˆφ(x2)ˆφ†(x3)ˆφ†(x4)|Ω⟩,
(17.34)
which is useful when we’re considering two particles coming in and two
particles leaving as might occur in a two-particle scattering experiment.
Fig. 17.6 (a) The two-point propaga-
tor G(2).
The blob represents the in-
teractions that the particle undergoes
with other particles in the system. (b)
A four-point propagator G(4). (c) An
n-point propagator G(n).
The aim of much quantum ﬁeld theory is to ﬁnd the full interacting
Green’s functions G(n). One reason is that G(n) can be used to calculate
scattering amplitudes using an equation known as the LSZ reduction
formula.11 Another reason is that the form of the interacting Green’s
11See Peskin and Schroeder for a dis-
cussion of the LSZ reduction formula.
function tells us a lot about the system. For example, the two-point
propagator, G(2)(x, y) = ⟨Ω|ˆφ(x)ˆφ†(y)|Ω⟩, that tells us about the fate of
single particles, is especially useful.
To see this we can consider what happens if we start from a free
scalar theory and slowly turn on the interactions to form an interacting
theory. Before we turn on the interactions we have the free propagator
˜∆(p) =
i
p2−m2+iǫ. The mass of the particle is given by the position of
the pole. The residue at the pole (which is i) tells us that the operator

Exercises
163
ˆφ†(y) creates precisely one quantum from the vacuum |0⟩(and later we
destroy it). When we turn on the interactions it turns out12 that the full
12The details of the full propagator are
discussed in Chapter 33.
propagator resembles the free version. The general form of the two-point
propagator in momentum space is
˜G(2)(p) =
iZp
p2 −m2 −˜Σ(p) + iΓp
+
 Multiparticle
parts

,
(17.35)
where
• Zp tells us how likely it is for a particle with momentum p to exist
stably without being destroyed by the interactions,
• ˜Σ(p) tells us about how the particle of momentum p interacts with
the vacuum,
• (2Γp)−1 is the particle’s lifetime,
• The full propagator also contains a contribution from short-lived
multiparticle states which can be scattered out of the vacuum.
Chapter summary
• The Feynman propagator for single-particle excitations in the ﬁeld
is given by G(x, y) = ⟨Ω|T ˆφ(x)ˆφ†(y)|Ω⟩.
• For the scalar ﬁeld theory the momentum space propagator for
single particles is ˜∆(p) =
i
p2−m2+iǫ.
Exercises
(17.1) Show that the retarded ﬁeld propagator for a free
particle in momentum space and the time domain:
G+
0 (p, tx, q, ty) = θ(tx −ty)⟨0|ˆap(tx)ˆa†
q(ty)|0⟩is
given by θ(tx −ty)e−i(Eptx−Eqty)δ(3)(p −q).
(17.2) Demonstrate that the free scalar propagator is the
Green’s function of the Klein–Gordon equation.
That is, in (1 + 1) dimensions, show
„
∂2
∂(x0)2 −∂2
∂x2 + m2
«
⟨0|T ˆφ(x0, x)ˆφ†(y0, y)|0⟩
= −iδ(2)(x −y).
(17.36)
(17.3) Consider a scalar ﬁeld theory deﬁned by the La-
grangian
L = 1
2[∂µφ(x)]2 −m2
2 [φ(x)]2.
(17.37)
By considering the Fourier transform of the ﬁeld,
show that the action may be written
S = 1
2
Z
d4p
(2π)4 ˜φ(−p)
`
p2 −m2´ ˜φ(p).
(17.38)
This provides us with an alternative method for
identifying the free propagator ˜G0(p) as (i/2 times)
the inverse of the quadratic term in the momentum-
space action.
(17.4) Show that the Feynman propagator for the quan-
tum simple harmonic oscillator with spring constant
mω2
0 is given by
˜G(ω) =
„ 1
m
«
i
ω2 −ω2
0 + iǫ.
(17.39)

164
Propagators and ﬁelds
(17.5) (a) Consider a one-dimensional system with La-
grangian
L = 1
2
„∂φ(x)
∂x
«2
+ m2
2 [φ(x)]2.
(17.40)
The choice of sign makes this a Euclidean the-
ory (described in Chapter 25).
Descretize this
theory (that is, put it on a lattice) by deﬁning
φj =
1
√
Na
P
p ˜φpeipja, where j labels the lattice
site, a is the lattice spacing and N is the number of
lattice points. Using the method in Exercise 17.3
show that the action may be written
S = 1
2
X
p
˜φ−p
„ 2
a2 −2
a2 cos pa + m2
«
˜φp,
(17.41)
and read oﬀthe propagator for this theory.
(b) The Lagrangian for a one-dimensional elas-
tic string in (1+1)-dimensional Minkowski space is
written
L = 1
2
ˆ
(∂0φ)2 −(∂1φ)2˜
.
(17.42)
Descretize
the
theory
by
deﬁning
φj(t)
=
1
√
Na
P
p
R dω
2π ˜φp(ω)e−iωteipja. Show that the prop-
agator for the (phonon) excitations that this theory
describes is given by
˜G(ω, p) =
i
ω2 −ω2
0(1 −cos pa),
(17.43)
where ω2
0 = 2/a2.

18
The S-matrix
18.1 The S-matrix: a hero for our
times
166
18.2 Some new machinery: the in-
teraction representation 167
18.3 The interaction picture ap-
plied to scattering
168
18.4 Perturbation
expansion
of
the S-matrix
169
18.5 Wick’s theorem
171
Chapter summary
174
Exercises
174
The Matrix is a system, Neo. That system is our enemy.
But when you’re inside, you look around, what do you see?
Businessmen, teachers, lawyers, carpenters. The very minds
of the people we are trying to save. But until we do, these
people are still a part of that system, and that makes them
our enemy.
Morpheus (Laurence Fishburne) in The Matrix
Canonical quantization works well for a small number of theories, where
it results in a diagonalized Hamiltonian describing the excitations of the
quantum ﬁeld in terms of non-interacting, or ‘free’ particles. In fact, the
theories that can be solved exactly using the methods of quantum ﬁeld
theory generally describe free particles with no prospect for interactions.
A world of free particles would be a very boring place since particle
interactions are at the heart of the workings of Nature. Most interesting
theories involve particle interactions and cannot be solved exactly. We
therefore need to resort to some sort of approximate method, like the
perturbation theory of quantum particle mechanics.
We will explore a perturbation theory for quantum ﬁelds in the follow-
ing chapters. The Lagrangian describing the ﬁeld is often the addition
of two parts: a solvable part (frequently describing non-interacting par-
ticles only and therefore called the free part) and a part that makes
the problem unsolvable (frequently describing interactions and called
the interacting part). As before, the free part is able to be solved via
canonical quantization and results in non-interacting particles. The in-
teraction part is more interesting because of one key fact:
Interactions involve the creation or destruction of particles.
One example of an interaction process is particle scattering. Particles are
ﬁred at each other. At the start of the experiment they are far from each
other so don’t interact: they are free. When they’re smashed together
they interact but only for a short time. In this chapter we’ll describe
a method of dealing with the interactions in a scattering process. This
will be time well spent. It turns out that the machinery developed here
will be central to creating a more general perturbation theory that can
deal with other interactions.

166
The S-matrix
18.1
The S-matrix: a hero for our times
One of the basic building blocks of quantum ﬁeld theory is the scattering
or S-matrix.
The idea was one of John Wheeler’s and it’s a great
John Wheeler (1911–2008) made nu-
merous
contributions
to
theoretical
physics and is also famous for coining
the terms ‘black hole’ and ‘wormhole’
and ‘it from bit’.
one. The S-matrix is rather like the time-evolution operator, but for an
important special case: it describes a Scattering experiment.
In a prototypical scattering experiment we start oﬀwith well separated
particles [Fig. 18.1(a)].
We ﬁre the particles at each other and they
interact in some complicated way governed by the Hamiltonian of the
real world ˆH [Fig. 18.1(b)]. The particles then recede from each other
and end up well separated [Fig. 18.1(c)]. The ﬁrst thing to say about this
is that the complicated interactions of the real world make this process
impossible to analyse. Working in the Heisenberg picture, we can’t even
write down convincing expressions for the time-dependent operators. All
appears to be lost! What do we do?
(a)
(b)
(c)
Fig. 18.1 A prototypical scattering ex-
periment. (a) The particles begin well
separated. (b) They interact in a com-
plicated way. (c) They recede from each
other and end up well separated.
Wheeler says that we start by imagining a world without compli-
cated interactions. That is, we split ˆH into two parts ˆH = ˆH0 + ˆH′,
where ˆH0 describes a simple world of non-interacting particles described
by some set of state vectors.
Remember that we’re working in the
Heisenberg picture here: these state vectors don’t change at all. For
two particles one simple-world state might be a momentum state like
|ψ⟩= |p2p1⟩simpleworld. Now we look at the real world and ask ‘Is there
a real-world state that looks like the simple-world state |p2p1⟩simpleworld?’
There is such a state at the start of the scattering experiment (at a time
t →−∞) when the particles are very far apart. We’ll call this real-world
state |p2p1⟩in
realworld or an ‘in’ state, for short. Similarly, we pick out an-
other simple-world state, e.g. |φ⟩= |q2q1⟩simpleworld and ask if there’s a
real-world state that looks like this one. There is, but this time at the
end of the experiment (t →∞) when the particles are well separated
after their interaction. We’ll call this one |q2q1⟩out
realworld. Notice that the
simple-world states only describe the real world in the limits t →±∞.
For the real-world scattering process in which we’re interested the
amplitude A for starting with |p2p1⟩in
realworld and ending up with
|q2q1⟩out
realworld is given by
A =out
realworld ⟨q1q2|p2p1⟩in
realworld.
(18.1)
We must recreate this amplitude using the simple-world states which,
after all, are the only ones we can work with. How can this be achieved?
Wheeler’s answer is the S-matrix. We deﬁne
A =out
realworld ⟨q1q2|p2p1⟩in
realworld =simpleworld ⟨q1q2| ˆS|p2p1⟩simpleworld.
(18.2)
So the S-matrix (that is, the matrix elements of the ˆS-operator) contains
the amplitudes for starting with a particular ‘in’ state and ending up
with a particular ‘out’ state.
Let’s take this further and calculate a real amplitude. We now need
two things:

18.2
Some new machinery: the interaction representation
167
• A way of getting a suitable ˆH0 to describe some useful simple-world
states which resemble the ‘in’ and ‘out’ states.
• A way of calculating an expression for the ˆS-operator. We can
then use the eigenstates of the simple Hamiltonian to work out an
amplitude.
In the next section we’ll examine the machinery needed to get ˆH0, which
involves yet another new formulation of quantum mechanics. This one
is called the interaction representation.
18.2
Some new machinery: the interaction
representation
So far in our quantum mechanics we’ve either had a formulation with
time-dependent states and time-independent operators (the Schr¨odinger
picture) or one with time-independent states and time-dependent oper-
ators (the Heisenberg picture). A third way is available, called the in-
teraction representation, where both the states and the operators have
some time dependence. It turns out that this combination is the one we
need to obtain ˆH0, the simple-world Hamiltonian.
As stated above, to get ˆH0 we split up the Hamiltonian into two parts
ˆH = ˆH0 + ˆH′. We call these a free part ˆH0 and an interaction part ˆH′.
The free part will generally be one that is time-independent and can be
easily solved. We then say that operators in the interaction picture ˆOI
evolve in time via the free part ˆH0 of the Hamiltonian
ˆOI(t) = ei ˆ
H0t ˆOe−i ˆ
H0t.
(18.3)
This is just like the Heisenberg version of quantum mechanics, except
that we’re only using the free part of the Hamiltonian. We therefore
have a Heisenberg-like equation of motion for the operators
id ˆOI
dt = [ ˆOI(t), ˆH0],
(18.4)
but again we emphasize that it just involves ˆH0.
So far we’ve not included ˆH′. Its inclusion results in the wave function
having some time dependence, which it wouldn’t in the non-interacting
case.
To see what happens, we compare a matrix element from the
Schr¨odinger picture to one in the interaction picture
⟨φ(t)| ˆO|ψ(t)⟩= ⟨φI(t)|ei ˆ
H0t ˆOe−i ˆ
H0t|ψI(t)⟩,
(18.5)
where the interaction picture states are labelled with a subscript I. We
can see that, for the matrix elements to be the same as in the Schr¨odinger
picture, we’ll need to deﬁne
|ψI(t)⟩= ei ˆ
H0t|ψ(t)⟩.
(18.6)

168
The S-matrix
We can now get to the equation of motion for the interaction picture
wave function by diﬀerentiating eqn (18.6) with respect to time. We ﬁnd
i d
dt|ψI(t)⟩
=
ei ˆ
H0t

−ˆH0 + i d
dt

|ψ(t)⟩
=
ei ˆ
H0t 
−ˆH0 + ˆH

|ψ(t)⟩,
(18.7)
and recalling that ˆH = ˆH0 + ˆH′ and eqn 18.6 we have
i d
dt|ψI(t)⟩= ei ˆ
H0t ˆH′e−i ˆ
H0t|ψI(t)⟩,
(18.8)
which we can rewrite in a recognizable form as
i d
dt|ψI(t)⟩= ˆHI(t)|ψI(t)⟩,
(18.9)
where ˆHI(t) = ei ˆ
H0t ˆH′e−i ˆ
H0t. This then completely deﬁnes the interac-
tion picture.
To recap the description of the interaction picture:
• Both the operators and states can evolve in time.
• The operators evolve by the free part of the Hamiltonian: ˆφI(t) =
ei ˆ
H0t ˆφ e−i ˆ
H0t.
• The states evolve according to the interaction part of the Hamil-
tonian: i ∂
∂t|ψ(t)⟩I = ˆHI(t)|ψ(t)⟩I where ˆHI(t) = ei ˆ
H0t ˆH′e−i ˆ
H0t.
Note that all of our diﬀerent representations coincide at t = 0.
18.3
The interaction picture applied to
scattering
t
λ
Fig. 18.2
Turning on an interaction
Hamiltonian ˆH′. We multiply it by λ(t)
which has this proﬁle.
Why is the interaction picture useful for the scattering problem? The
point is that the interaction part of the Hamiltonian is zero at the start
and end of the problem. In fact, we imagine that the interaction part of
the Hamiltonian ˆH′ is turned on and oﬀslowly and smoothly as shown
in Fig. 18.2. When ˆHI = 0 we just have the Heisenberg picture for the
free part of the Hamiltonian ˆH0. This is good because we choose ˆH0 so
that we can solve it using our canonical quantization machine and we
certainly then know its eigenstates.
What becomes of the states? We can identify the simple-world states
as those of the interaction picture at the start and end:
|φ⟩simpleworld = |φI(±∞)⟩.
(18.10)
They are eigenstates of ˆH0, which is vital since it allows us to build
them up from the state |0⟩which is the vacuum of ˆH0 (called variously
the non-interacting, free or bare vacuum) using our beloved creation
and annihilation operators. During the interaction process we have that

18.4
Perturbation expansion of the S-matrix
169
ˆHI is nonzero so the states evolve in some mysterious and complicated
way. However, at the end of the experiment ˆHI is zero again and the
states freeze. It’s a bit like musical chairs in this respect. The states are
initially sitting down, they rearrange themselves during the music and
then freeze on the chairs when the music stops.
We’ve thought about the states, but what about the operators? Sim-
ple: the operators time-evolve according to ˆH0 at all times. We can
therefore use the freely evolving ﬁeld operators like ˆφ(x) that we’ve used
previously. These also enjoy an expansion in terms of creation and an-
nihilation operators.
Finally we need to ﬁgure out how to work out the ˆS-operator. We are
assisted by the useful fact that all of our quantum mechanical pictures
are deﬁned so that they coincide at t = 0, so that we’re free to write
simpleworld⟨φ| ˆS|ψ⟩simpleworld =out
realworld ⟨φ|ψ⟩in
realworld = ⟨φI(0)|ψI(0)⟩.
(18.11)
Then, if we knew the time-evolution operator in the interaction picture
ˆUI(t2, t1), we could say that
simpleworld⟨φ| ˆS|ψ⟩simpleworld
=
⟨φI(∞)| ˆUI(∞, 0) ˆUI(0, −∞)|ψI(−∞)⟩
=
⟨φI(∞)| ˆUI(∞, −∞)|ψI(−∞)⟩
(18.12)
=
simpleworld⟨φ| ˆUI(∞, −∞)|ψ⟩simpleworld.
We get the important result that the ˆS-operator is the time-evolution
operator for the interaction-picture ˆUI(t, −t) as t →∞.
18.4
Perturbation expansion of the
S-matrix
As in the Schr¨odinger picture, we have an equation of motion for the
interaction picture time-evolution operator
i d
dt2
ˆUI(t2, t1) = ˆHI(t2) ˆUI(t2, t1),
(18.13)
where ˆU(t, t) = 1. We might be tempted to treat ˆHI(t) as a number,
and if that were valid we could then integrate and write ˆU(t2, t1) =
e−i
R t2
t1 dt ˆ
HI(t), but this is wrong.
The interaction Hamiltonian ˆHI(t)
doesn’t necessarily commute with itself when evaluated at diﬀerent
times.
That is, in general, [ ˆHI(t2), ˆHI(t1)] ̸= 0.
To circumvent this
problem, we’ll deﬁne a new version of the exponential which will make
this equation right. The solution turns out to use the time-ordered
product
T[ ˆA1(t1) ˆA2(t2) . . . ˆAn(tn)],
(18.14)
deﬁned as the string arranged so that the later operators are on the left.1
1Just remember ‘later on the left’.
Note that the anticommuting nature of
Fermi ﬁelds causes us to pick up an ad-
ditional factor (−1)P on time ordering,
where P is the number of permutations
required to rearrange the operators.
From here until Chapter 35 we will fo-
cus our discussion on (bosonic) scalar
ﬁelds. Fermi ﬁelds, whose behaviour is
based on the same general principles we
will examine, are discussed speciﬁcally
in Part IX.
As discussed in the previous chapter, the Wick time ordering symbol
symbol T[ ] is a little like the normal ordering procedure N[ ] in that it is

170
The S-matrix
not an operator, it’s an instruction on what to do to a string of operators.
The reason that using T[ ] helps us here is that everything within a time-
ordered product commutes, so we may now take a derivative. We can
therefore write an expression for ˆUI(t2, t1) that incorporates correctly
time-ordered operators as follows:
ˆUI(t2, t1) = T
h
e−i
R t2
t1 dt ˆ
HI(t)i
.
(18.15)
This expression is known as Dyson’s expansion.
Freeman Dyson (1923– )
Example 18.1
We’ll pause to justify eqn 18.15. Using the fact that everything within a time-ordered
product commutes we can treat ˆUI(t2, t1) as a function of t2 and take a derivative
with respect to this time to obtain
i d
dt2
T
»
e−i
R t2
t1 dt ˆ
HI(t)
–
= T
»
ˆHI(t2)e−i
R t2
t1 dt ˆ
HI(t)
–
.
(18.16)
Next we notice that t2 is the latest time in the problem (since ˆU evolves the system
from t1 to t2) and the time ordering therefore puts the operator ˆHI(t2) left-most.
We can then pull ˆHI(t2) out of the time-ordered product to obtain
i d
dt2
T
»
e−i
R t2
t1 dt ˆ
HI(t)
–
= ˆHI(t2)T
»
e−i
R t2
t1 dt ˆ
HI(t)
–
.
(18.17)
Comparing this with eqn 18.13 we see that the time-evolution operator in the inter-
action picture is given by
ˆUI(t2, t1) = T
»
e−i
R t2
t1 dt ˆ
HI(t)
–
.
(18.18)
We also know that the ˆS-operator is the limit ˆS = ˆUI(t2 →∞, t1 →
−∞). Using this, we are left with Dyson’s expansion of the ˆS-operator
ˆS = T
h
e−i
R ∞
−∞d4x ˆ
HI(x)i
,
(18.19)
where we’ve replaced
R
dt ˆHI(t) with
R
d4x ˆHI(x), an integral of the
Hamiltonian density.
The exponential form of Dyson’s expansion in eqn 18.19 is beautifully
memorable, but not very useful. Usually the integral in the exponent
cannot be done exactly so we have to expand out the exponential in
Dyson’s expansion thus:
ˆS = T

1 −i
Z
d4z ˆHI(z) + (−i)2
2!
Z
d4y d4w ˆHI(y) ˆHI(w) + . . .

.
(18.20)
Provided that the interaction Hamiltonian ˆHI(x) is small compared to
the full Hamiltonian density, then this provides the basis for a perturba-
tion expansion of the S-matrix. We’ll deal with this expansion in detail
in the next chapter. Notice that in eqn 18.20 we integrate over diﬀerent
spacetime coordinates each time the interaction Hamiltonian appears.

18.5
Wick’s theorem
171
18.5
Wick’s theorem
To make progress in digesting long strings of operators that appear in
Dyson expansions, we are faced with solving the following conundrum:
• We will frequently have to evaluate a term like
⟨0|T[ ˆA ˆB ˆC . . . ˆZ]|0⟩,
(18.21)
a vacuum expectation value (or VEV for short) of a time-
ordered string of operators. This is hard.
• On the other hand, what is very easy to evaluate is
⟨0|N[ ˆA ˆB ˆC . . . ˆZ]|0⟩,
(18.22)
a VEV of a normal-ordered string of operators.
This is trivial
because normal ordering places annihilation operators on the right
and creation operators on the left, and so this VEV is identically
zero. So if we could ﬁnd a way of relating N[stuﬀ] to T[stuﬀ] then
the problem would be solved.
The next thing to notice is that the mode expansion of a ﬁeld opera-
tor ˆφ contains two parts, an annihilation part and a creation part (see
eqn 11.8) and so can be written ˆφ = ˆφ−+ ˆφ+. Thus we have2 ˆφ−|0⟩= 0
2Our deﬁnition assigns the superscript
+ to the creation part and −to the
annihilation part. Some books, e.g. Pe-
skin and Schroeder, choose an opposite
convention.
and ⟨0|ˆφ+ = 0. Let’s now take the simplest non-trivial case of a string
of operators and consider two ﬁeld operators ˆA and ˆB. Their product
is given by
ˆA ˆB = ( ˆA+ + ˆA−)( ˆB+ + ˆB−) = ˆA+ ˆB+ + ˆA−ˆB−+ ˆA+ ˆB−+ ˆA−ˆB+.
(18.23)
If we normal-order this product, we get
N[ ˆA ˆB] = ˆA+ ˆB+ + ˆA−ˆB−+ ˆA+ ˆB−+ ˆB+ ˆA−,
(18.24)
where we have only needed to swap operators in the ﬁnal term. Thus
N[ ˆA ˆB] only diﬀers from ˆA ˆB by
ˆA ˆB −N[ ˆA ˆB] = ˆA−ˆB+ −ˆB+ ˆA−= [ ˆA−, ˆB+],
(18.25)
i.e. by a simple commutator of operators (which we know from previous
experience3 will just be a complex number, or c-number for short). Now
3Commutators turn out to be quanti-
ties such as iℏ.
the ﬁelds ˆA and ˆB are functions of spacetime coordinates, so we could
also write down the time-ordered product
T[ ˆA(x) ˆB(y)] =
( ˆA(x) ˆB(y)
x0 > y0
ˆB(y) ˆA(x)
x0 < y0,
(18.26)
and therefore
T[ ˆA(x) ˆB(y)] −N[ ˆA(x) ˆB(y)] =
(
[ ˆA−(x), ˆB+(y)]
x0 > y0
[ ˆB−(y), ˆA+(x)]
x0 < y0.
(18.27)

172
The S-matrix
Since the VEV of a normal ordered product is zero, we can immediately
write down
⟨0|T[ ˆA(x) ˆB(y)]|0⟩=
(
⟨0|[ ˆA−(x), ˆB+(y)]|0⟩
x0 > y0
⟨0|[ ˆB−(y), ˆA+(x)]|0⟩
x0 < y0.
(18.28)
If we choose ˆA = ˆB = φ, then the quantity on the left is simply a
Feynman propagator.
Subtracting out a normal-ordered string of operators from the time-
ordered version is clearly a useful thing to do and so we formalize this
and give it a special name: a contraction, deﬁned as
ˆA ˆB = T[ ˆA ˆB] −N[ ˆA ˆB].
(18.29)
The contraction ˆA ˆB is simply a commutator (see eqn 18.27) because
the only diﬀerence between normal ordering and time ordering is that
we have shuﬄed various creation and annihilation operators, thereby
accumulating various c-numbers. Thus the contraction is a c-number,
and it takes the value
ˆA ˆB = ˆA ˆB⟨0|0⟩= ⟨0| ˆA ˆB|0⟩= ⟨0|T[ ˆA ˆB]|0⟩.
(18.30)
Moreover, since it is only a c-number, normal ordering has no eﬀect on
it and we can write
T[ ˆA ˆB] = N[ ˆA ˆB] + ˆA ˆB = N[ ˆA ˆB + ˆA ˆB].
(18.31)
This result can be generalized4 to longer strings of operators to yield
4The proof is by induction, and can
be found on page 90 of Peskin and
Schroeder.
Wick’s theorem, which can be stated as follows:
T[ ˆA ˆB ˆC . . . ˆZ] = N

ˆA ˆB ˆC . . . ˆZ +
all possible contractions of
ˆA ˆB ˆC . . . ˆZ

.
(18.32)
It’s worth noting that Wick’s theorem
applies for free ﬁelds only, such as those
in the interaction picture. This will be
important in Chapter 22.
Example 18.2
Wick’s theorem can be illustrated for the case of four operators
T[ ˆ
A ˆB ˆC ˆD]
=
N
h
ˆ
A ˆB ˆC ˆD
i
+ N
"
ˆ
A ˆB ˆC ˆD
#
+ N
"
ˆ
A ˆB ˆC ˆD
#
+ N
"
ˆ
A ˆB ˆC ˆD
#
+N
"
ˆ
A ˆB ˆC ˆD
#
+ N
"
ˆ
A ˆB ˆC ˆD
#
+ N
"
ˆ
A ˆB ˆC ˆD
#
+N
"
ˆ
A ˆB ˆC ˆD
#
+ N
2
4 ˆ
A ˆB ˆC ˆD
3
5 + N
2
4 ˆ
A ˆB ˆC ˆD
3
5 .
(18.33)
This is a sum of normal ordered terms including zero, one or two contractions.

18.5
Wick’s theorem
173
Let us look at a particular term involving a single contraction in a Wick
expansion of T[ ˆA ˆB ˆC . . . ˆZ], say for example N[ ˆA ˆB ˆC ˆD ˆE ˆF ˆG . . .]. Since
the contracted part is a c-number, we can factor it out of the normal
ordered product5 and write
5There
could
be
a
sign
change
in
this expression if the operators are
fermionic, so in that case you have to
pay attention to any sign changes ac-
cumulated with swapping the order of
operators.
N[ ˆA ˆB ˆC ˆD ˆE ˆF ˆG . . .] = ˆC ˆF × N[ ˆA ˆB ˆD ˆE ˆG . . .].
(18.34)
However, when we evaluate the VEV of this term it will vanish because
⟨0|N[anything]|0⟩= 0. Therefore, the only terms which survive when we
work out the VEV of T[ ˆA ˆB ˆC . . . ˆZ] are the ones involving contractions
of all operators.6 Thus
6By extension, the VEV of an odd-
numbered string of operators will be
zero, since you can only contract pairs
of operators and so there will always be
one operator left over.
⟨0|T[ ˆA ˆB ˆC . . . ˆY ˆZ]|0⟩
=
⟨0|T[ ˆA ˆB ˆC ˆD ˆE ˆF . . . ˆY ˆZ]|0⟩
(18.35)
+⟨0|T[ ˆA ˆB ˆC ˆD ˆE ˆF . . . ˆY ˆZ]|0⟩
+

all other combinations involving
contractions of every pair of operators

,
and using eqn 18.30 this reduces to
⟨0|T[ ˆA ˆB ˆC . . . ˆY ˆZ]|0⟩
=
⟨0|T[ ˆA ˆB]|0⟩⟨0|T[ ˆC ˆD]|0⟩⟨0|T[ ˆE ˆF]|0⟩. . . ⟨0|T[ ˆY ˆZ]|0⟩
+ ⟨0|T[ ˆA ˆC]|0⟩⟨0|T[ ˆB ˆD]|0⟩⟨0|T[ ˆE ˆF]|0⟩. . . ⟨0|T[ ˆY ˆZ]|0⟩
+ . . .
(18.36)
that is, the VEV of a time-ordered string of operators is given by the
sum of products of all possible combinations of VEVs of time ordered
pairs.
Example 18.3
For the case of four bosonic operators.
⟨0|T
h
ˆ
A ˆB ˆC ˆD
i
|0⟩
=
⟨0|T
"
ˆ
A ˆB ˆC ˆD
#
|0⟩+ ⟨0|T
2
4 ˆ
A ˆB ˆC ˆD
3
5 |0⟩+ ⟨0|T
2
4 ˆ
A ˆB ˆC ˆD
3
5 |0⟩
=
⟨0|T[ ˆ
A ˆB]|0⟩⟨0|T[ ˆC ˆD]|0⟩+ ⟨0|T[ ˆ
A ˆC]|0⟩⟨0|T[ ˆB ˆD]|0⟩
+⟨0|T[ ˆ
A ˆD]|0⟩⟨0|T[ ˆB ˆC]|0⟩.
(18.37)
In particular
⟨0|ˆφ(x1)ˆφ(x2)ˆφ(x3)ˆφ(x4)|0⟩
=
∆(x1 −x2)∆(x3 −x4) + ∆(x1 −x3)∆(x2 −x4)
+∆(x1 −x4)∆(x2 −x3),
(18.38)
where we have used the Feynman propagator ∆(x1 −x2) = ⟨0|T ˆφ(x1)ˆφ(x2)|0⟩.
Wick’s theorem is an essential tool for expanding ˆS in perturbation
theory which we discuss in the next chapter.

174
The S-matrix
Chapter summary
• Real-world scattering processes can be described by an S-matrix
element via out
realworld⟨φ|ψ⟩in
realworld =simpleworld ⟨φ| ˆS|ψ⟩simpleworld.
• The S-matrix works in the interaction picture where operators
time-evolve according to the free part of the Hamiltonian ˆH0 and
states time-evolve according to the interaction part ˆHI(t).
• The
ˆS-operator
is
given
by
the
Dyson
equation
ˆS
=
T[e−i
R
d4x ˆ
HI(x)].
• Wick’s theorem allows us to grind down long time-ordered strings
of operators. A vacuum expectation value (VEV) of a time-ordered
string is a sum of products of VEVs of time-ordered pairs.
Exercises
(18.1) Although the interaction picture is most useful to
us as a step towards a powerful version of pertur-
bation theory, it is also useful in itself for some
problems, such as this famous example from mag-
netic resonance. Consider a Hamiltonian describing
a spin-1/2 particle in a large, static magnetic ﬁeld
B0 which is subject to a perturbing, perpendicular,
oscillating ﬁeld B1:
ˆH = γB0 ˆSz + γB1( ˆSx cos γB0t + ˆSy sin γB0t).
(18.39)
Here γ is the particle’s gyromagnetic ratio. Notice
that the frequency γB0 of the oscillating ﬁeld with
amplitude B1 is chosen so that it matches the en-
ergy level separation caused by the static ﬁeld B0.
(a) Write the problem in the interaction represen-
tation by splitting the Hamiltonian into free and
interacting parts.
(b) Use the identities
ˆS±
=
ˆSx ± i ˆSy,
ˆS+eiωt
=
eiω ˆ
Szt ˆS+e−iω ˆ
Szt,
ˆS−e−iωt
=
eiω ˆ
Szt ˆS−e−iω ˆ
Szt,
(18.40)
to simplify the interaction Hamiltonian ˆHI(t). You
should ﬁnd that you’re able to remove the time de-
pendence completely.
(c) Find the interaction picture evolution operator
ˆUI(t2, t1).
(d) What is the probability that a particle initially
prepared in a state | ↑⟩at t = 0 will still be in that
state at time t. What is the probability that it will
be found in the | ↓⟩state?
(e) Find the expectation value of the ˆSz operator.
(18.2) Show that |ψ(t = ∞)⟩I = P
φ⟨φ| ˆS|ψ⟩|φ⟩, where the
states on the right-hand side are the simple-world
states deﬁned in the chapter.
(18.3) Use Wick’s theorem to express the string of Bose
operators ˆapˆa†
qˆak in terms of normal ordered ﬁelds
and contractions.
(18.4) (a) Normal order the string of Bose operators
ˆbˆgˆbˆb†ˆb† using the usual Bose commutation relations.
(b) Show that Wick’s theorem gives the same an-
swer.
(18.5) Use Wick’s theorem to simplify
⟨0|ˆc†
p1−qˆc†
p2+qˆcp2ˆcp1|0⟩,
where the operators ˆc†
p and ˆcp create and annihilate
fermions respectively.

19
Expanding the S-matrix:
Feynman diagrams
19.1 Meet some interactions
176
19.2 The example of φ4 theory 177
19.3 Anatomy of a diagram
181
19.4 Symmetry factors
182
19.5 Calculations in p-space
183
19.6 A ﬁrst look at scattering 186
Chapter summary
186
Exercises
187
They were funny-looking pictures.
And I did think con-
sciously: Wouldn’t it be funny if this turns out to be useful
and the Physical Review would be all full of these funny
looking pictures. It would be very amusing.
Richard Feynman (1919–1988)
Like the silicon chips of more recent years, the Feynman
diagram was bringing computation to the masses.
Julian Schwinger (1918–1994)
(a)
(b)
(c)
(d)
(e)
time
= particle
= antiparticle
t = t0
t = t1
t = t3
t = t2
Fig. 19.1 The Feynman diagram. (a)
A particle is represented by a line with
an arrow going in the direction of time
(conventionally up the page).
An an-
tiparticle is represented by a line with
an arrow going the opposite way. (b)
A particle and antiparticle created at
time t = t0. (c) A particle and antipar-
ticle are annihilated at time t = t1. (d)
Trajectory of a single particle can be
interpreted as follows: (e) A particle–
antiparticle pair are created at t2, the
antiparticle of which annihilates with a
third particle at t = t3.
One of Richard Feynman’s greatest achievements was his invention of
(what are now called) Feynman diagrams. These are cartoons that
represent terms in the perturbation expansion of the S-matrix. We will
see that Feynman diagrams exist in several related forms, but the most
straightforward of these are simply spacetime diagrams describing the
trajectory of particles. We imagine time running up the page and rep-
resent particles by lines with an arrow going in the direction of time
[Fig. 19.1(a)]; antiparticles are represented by lines with arrows going in
the opposite direction. (This is consistent with Feynman’s interpreta-
tion of negative energy states as positive energy antiparticles travelling
backwards in time.) We can draw particle and antiparticle pairs being
created at some time [Fig. 19.1(b)] or destroyed [Fig. 19.1(c)] at some
other time. We can also tell stories: the line in Fig. 19.1(d) may look
like it describes a single particle looping backward in time, but Feynman
has another interpretation. Just as it’s unclear to a bomber pilot ﬂying
over a curving road whether (s)he is above a single road or a number
of them, we’re asked to ﬂy over the diagram experiencing everything in
time order. From the point of view of the bombardier, we start with
a particle for t < t2, then at t = t2 a new particle–antiparticle pair is
created. The new particle and antiparticle coexist with the original par-
ticle in the interval t2 < t < t3. At t = t3 the newly created antiparticle
collides with our original particle and annihilates. The newly created
particle continues out of the diagram for t > t3 [see Fig. 19.1(e)]. Since
the particles are identical, no-one can tell the diﬀerence.
In this chapter we introduce the perturbation expansion of the S-
matrix and its representation in terms of Feynman diagrams.
Recall
from the previous chapter that an expression for the ˆS-operator is given

176
Expanding the S-matrix: Feynman diagrams
by Dyson’s expansion
ˆS = T
h
e−i
R
d4z ˆ
HI(z)i
.
(19.1)
To use this we must expand it as a power series in the operator ˆHI(z).
We’ll now consider some of the forms of ˆHI(z) that are encountered in
quantum ﬁeld theory and see how the expansion of Dyson’s equation
proceeds. The interaction Hamiltonians will generally involve the prod-
ucts of free-ﬁeld operators localized at some point z.
We’ll see later
that if we interpret the ﬁeld operators as describing particles then the
interaction1 can be thought of as a collision of the particles at spacetime
1Notice also that we integrate over all
z in eqn 19.1 thus taking into account
the possibility of the interaction taking
place at every point in spacetime.
point z.
19.1
Meet some interactions
There are a number of models of interactions to try. For each inter-
action we will draw a diagram in spacetime illustrating the interaction
processes. This will eventually lead to the famous diagrams invented by
Richard Feynman. At present we won’t make any interpretations of the
diagrams in terms of particles: this will only be possible a little later
when the ﬁeld operators are made to act on states.
Example 19.1
(i) The simplest interaction Hamiltonian HI(z) involves a scalar ﬁeld φ(z) interacting
with an external source ﬁeld J(z) at a spacetime point z:
HI(z) = J(z)φ(z).
(19.2)
We draw the source J(z) as a blob at a point z. The ﬁeld is drawn as a stumpy line
attached to the source blob [see Fig. 19.2(a)].
(ii) The simplest self-interaction of ﬁelds is φ4 (say ‘phi-fourth’ or ‘phi-four’) theory
described by a Hamiltonian
HI(z) = λ
4! φ(z)4.
(19.3)
This interaction causes four scalar φ-ﬁelds to meet at a spacetime point z [see
Fig. 19.2(b)]. The diagram shows four stumpy lines meeting at z.
(iii) Another simple interaction is
HI(z) = gψ†(z)ψ(z)φ(z),
(19.4)
where φ(z) is, again, a scalar ﬁeld and ψ(z) is a complex scalar ﬁeld. This interaction2
2This has a very similar structure to
the interaction used in QED. There the
electron corresponds to the excitations
in the psi-ﬁeld and the photon to exci-
tations in the phi-ﬁeld.
causes an adjoint psi-ﬁeld [represented by ψ†(z)], a psi-ﬁeld [ψ(z)] and a scalar phi-
ﬁeld [φ(z)] to meet at z.
To keep track of ﬁelds and their adjoints we use the
convention that a ﬁeld ψ is shown by a stumpy line with an arrow pointing towards
the interaction vertex [see Fig. 19.2(c)]. The adjoint ﬁeld ψ† is shown with an arrow
pointing away from the interaction.
(iv) Another very useful interaction is non-relativistic and describes the Coulomb
interaction
HI(x −y) = 1
2ψ†(x)ψ†(y)V (x −y)δ(x0 −y0)ψ(y)ψ(x).
(19.5)
This one is delocalized. It says that a psi-ﬁeld and an adjoint psi-ﬁeld meet at x.
These interact, via a potential V (x −y), with a psi-ﬁeld and adjoint psi-ﬁeld at y.
The δ-function ensures that the interaction is instantaneous (that is, occurs at the
same time). [This is shown in Fig. 19.2(d).]
(a)
(b)
(c)
(d)
J
φ(z)
φ(z)
φ(z)
φ(z)
φ(z)
φ(z)
ψ†(z)
ψ(z)
ψ(x)
ψ†(x)
ψ(y)
ψ†(y)
V (x −y)
Fig. 19.2
Spacetime diagrams rep-
resenting some commonly encountered
interactions.

19.2
The example of φ4 theory
177
19.2
The example of φ4 theory
We’ll now carry out the full procedure of calculating an S-matrix element
for the simple case of φ4 theory. The Lagrangian density describing the
interacting theory is given by
L = 1
2[∂µφ(x)]2 −m2
2 φ(x)2 −λ
4!φ(x)4.
(19.6)
The free part is given by the ﬁrst two terms L0 = 1
2(∂µφ)2−m2
2 φ2, which
give rise to a free Hamiltonian operator upon canonical quantization
ˆH0 = 1
2


 
∂ˆφ
∂t
!2
+

∇ˆφ
2
+ m2 ˆφ2

.
(19.7)
When we use the ˆS-operator we work in the interaction picture, so the
free Hamiltonian time evolves the ﬁeld operators ˆφ(x). The states evolve
via the interaction Hamiltonian which we read oﬀfrom the interacting
part of the Lagrangian LI = −λ
4!φ(x)4 as ˆHI = λ
4! ˆφ(x)4. Next we need a
programme to expand the S-matrix using this interaction Hamiltonian.
It’s a glorious ﬁve-step plan.
Step I
Decide what S-matrix element to calculate and write it as a vacuum
expectation value (VEV) of the free vacuum |0⟩. We do this so we can
use the simple form of Wick’s theorem later.
As an example we’ll take our ‘in’ state to be a single particle in a
momentum state p and the ‘out’ state will be a single particle in a
momentum state q. We’ll be calculating the amplitude
A
=
out⟨q|p⟩in = ⟨q| ˆS|p⟩
=
(2π)3(2Eq)
1
2 (2Ep)
1
2 ⟨0|ˆaq ˆSˆa†
p|0⟩,
(19.8)
where we recall that the relativistic normalization of our states means
that |p⟩= (2π)
3
2 (2Ep)
1
2 ˆa†
p|0⟩.
Step II
We expand the ˆS-operator using Dyson’s expansion
ˆS
=
T

exp

−i
Z
d4z ˆHI(z)

=
T

1 −i
Z
d4z ˆHI(z) + (−i)2
2
Z
d4yd4w ˆHI(y) ˆHI(w) + . . .

(19.9)
=
T
"
1 −iλ
4!
Z
d4z ˆφ(z)4 + (−i)2
2!
 λ
4!
2 Z
d4yd4w ˆφ(y)4 ˆφ(w)4 + . . .
#
.

178
Expanding the S-matrix: Feynman diagrams
Step III
Plug the resulting expression for the ˆS-operator into the expression for
the S-matrix element that we’re trying to calculate
A = ⟨q| ˆS|p⟩
= (2π)3(2Eq)
1
2 (2Ep)
1
2 ˆT

⟨0|ˆaqˆa†
p|0⟩+
−iλ
4
 Z
d4z ⟨0|ˆaq ˆφ(z)4ˆa†
p|0⟩
+ (−i)2
2!
 λ
4!
2 Z
d4yd4w ⟨0|ˆaq ˆφ(y)4 ˆφ(w)4ˆa†
p|0⟩+ . . .

.
(19.10)
Since the amplitude is a sum of terms ordered by powers of λ, it makes
sense to label the terms A = A(0) + A(1) + A(2) . . ., where A(n) is the
term proportional to λn.
Step IV
Use Wick’s theorem to grind down the terms. That is, for each term we
write the string of operators bookended by the vacuum states as a sum
of all of the contractions of pairs.
Let’s ﬁrst consider A(1),
the ﬁrst-order part of the expansion
(the part proportional to λ).
Using Wick’s theorem on the string
⟨0|ˆaq ˆφ(x)4ˆa†
p|0⟩≡⟨0|ˆaq ˆφ(z)ˆφ(z)ˆφ(z)ˆφ(z)ˆa†
p|0⟩, will yield up two sorts
of term. The ﬁrst contains contractions of the ˆa-operators and the ˆφ-
operators separately, like3
3Note that contractions involving ˆa-
operators don’t need T symbols since
here the ˆap-operators are understood
to create particles at t = −∞only,
while the ˆap-operators destroy particles
at t = ∞only, which ﬁxes the time or-
dering unambiguously.
⟨0|ˆaq ˆφ(z)ˆφ(z)ˆφ(z)ˆφ(z)ˆa†
p|0⟩
= ⟨0|ˆaqˆa†
p|0⟩⟨0|T ˆφ(z)ˆφ(z)|0⟩⟨0|T ˆφ(z)ˆφ(z)|0⟩.
(19.11)
There are two other combinations of contractions of the ˆφ-ﬁelds that
give this term, making three in total.
The second type of term contracts ˆa-operators with ˆφ operators.
There are twelve ways of doing this. One example is
⟨0|ˆaq ˆφ(z)ˆφ(z)ˆφ(z)ˆφ(z)ˆa†
p|0⟩
= ⟨0|ˆaq ˆφ(z)|0⟩⟨0|T ˆφ(z)ˆφ(z)|0⟩⟨0|ˆφ(z)ˆa†
p|0⟩.
(19.12)
The result of this set of manipulations is that the ﬁrst-order term in the
expansion of the S-matrix element for ⟨q| ˆS|p⟩is given by
A(1) = −iλ
4!
Z
d4z
h
3⟨0|ˆaqˆa†
p|0⟩⟨0|ˆφ(z)ˆφ(z)|0⟩⟨0|ˆφ(z)ˆφ(z)|0⟩
(19.13)
+ 12⟨0|ˆaq ˆφ(z)|0⟩⟨0|T ˆφ(z)ˆφ(z)|0⟩⟨0|ˆφ(z)ˆa†
p|0⟩
i
.
Here are some rules for making sense of the contractions:
• Contractions between two ﬁelds make a free propagator4 for that
4Remember
that
for
a
scalar
ﬁeld
φ(x) = φ†(x) and
∆(y −z) =
Z
d4q
(2π)4
ie−iq·(y−z)
q2 −m2 + iǫ,
where q is a dummy momentum, over
which we integrate.
ﬁeld, that is, ˆφ(y)ˆφ(z) = ⟨0|T ˆφ(y)ˆφ(z)|0⟩= ∆(y −z).

19.2
The example of φ4 theory
179
• The contraction between a ﬁeld and the creation operator
from the initial particle state ˆφ(z)ˆa†
p corresponds to a factor
1
(2π)
3
2
1
(2Ep)
1
2 e−ip·z.
Example 19.2
To see this, we use the expansion on the ﬁeld ˆφ(z):
⟨0|ˆφ(z)ˆa†
p|0⟩
=
Z
d3q
(2π)
3
2
1
(2Eq)
1
2
⟨0|
“
ˆaqe−iq·z + ˆa†
qeiq·z”
ˆa†
p|0⟩
=
Z
d3q
(2π)
3
2
1
(2Eq)
1
2
⟨0|
“
ˆaqe−iq·z + ˆa†
qeiq·z”
|p⟩
=
Z
d3q
(2π)
3
2
1
(2Eq)
1
2
e−iq·zδ(3)(q −p)
=
1
(2π)
3
2
1
(2Ep)
1
2
e−ip·z.
(19.14)
Notice that, following our conventions, the factor e−ip·z cor-
responds to an incoming particle.
Notice also that the fac-
tors
1
(2π)
3
2
1
(2Ep)
1
2 from the contraction exactly cancel against the
factors from the relativistic normalization of the states |p⟩=
(2π)
3
2 (2Ep)
1
2 , with the result that the contraction ˆφ(x)|p⟩= e−ip·x.
• Similarly, the contraction between a ﬁnal state annihilation oper-
ator and a ﬁeld ˆaq ˆφ(z) corresponds to a factor
1
(2π)
3
2
1
(2Eq)
1
2 eiq·z.
This is the outgoing particle.
• Contractions between initial and ﬁnal particles ˆaqˆa†
p = ⟨0|ˆaqˆa†
p|0⟩
yield a delta function δ(3)(q −p).
Example 19.3
As an example of the use of the rules, consider the term:
12⟨0|ˆaq ˆφ(z)ˆφ(z)ˆφ(z)ˆφ(z)ˆa†
p|0⟩.
Using the rules, this is represented by
12
"
1
(2π)
3
2
1
(2Eq)
1
2
eiq·z
# "Z
d4k
(2π)4
e−ik·(z−z)
k2 −m2 + iǫ
# "
1
(2π)
3
2
1
(2Ep)
1
2
e−ip·z
#
. (19.15)
Remember that to compute the contribution of this term to the S-matrix element
we will need to integrate the variable z over all space.
That is, on including the
normalization factors, we have that A, which is what we’ll call the contribution to
the amplitude ⟨q| ˆS(1)|p⟩from the above contraction, is given by
A
=
12 × (2π)3(2Eq)
1
2 (2Ep)
1
2 (−iλ)
4!
Z
d4z
"
1
(2π)
3
2
1
(2Eq)
1
2
eiq·z
×
d4k
(2π)4
e−ik·(z−z)
k2 −m2 + iǫ
1
(2π)
3
2
1
(2Ep)
1
2
e−ip·z
#
=
(−iλ)
2
Z
d4z d4k
(2π)4 eiq·z
1
k2 −m2 + iǫe−ip·z.
(19.16)

180
Expanding the S-matrix: Feynman diagrams
Notice that the normalization factors of (2π) and (2Ep) have cancelled. This was one
intention of the conventions for decorating the earlier equation with these factors.
We could, at this point, collect the terms together and prepare to in-
tegrate the position of the interactions over all spacetime.
However,
there’s still one more part of the plan to carry out.
Step V
Make sense of a term by drawing a Feynman diagram. A Feynman di-
agram represents an amplitude in the expansion of the S-matrix.
A
particular term in the expansion will comprise a certain number of in-
teractions. This number is the same as the order of expansion of the
ˆS-operator.
A second-order term has two interactions, a third-order
three, etc. To represent the interaction in a diagram we draw the inter-
action vertices exactly as shown in Fig. 19.2. Each line segment (we will
call these legs) emerging from an interaction vertex represents an uncon-
tracted ﬁeld operator ˆφ(z). The Wick contractions (that we carried out
in step IV) join the legs of the interaction to each other [for contractions
like ˆφ(x)ˆφ(y)] or to external particles (for contractions like ˆφ(x)ˆa†
p).
Here is a list of rules for drawing a diagram.
• Draw the interaction vertices and label them with their spacetime
coordinates [Fig. 19.3(a)].
• Contractions between the initial state and a ﬁeld, i.e. ˆφ(x)ˆa†
p, are
drawn as incoming lines connecting to one of the legs of the vertex
[Fig. 19.3(b)]. This corresponds to a real, on-mass-shell particle
coming into the story.
• Propagators resulting from the ﬁeld–ﬁeld contractions ˆφ(x)ˆφ(y)
are drawn as lines linking the points [Fig. 19.3(c)]. We can think
of these as virtual particles which are internal to the story the
diagram is telling.
• Contractions between the ﬁnal state and a ﬁeld ˆaq ˆφ(x) are drawn
as an outgoing lines [Fig. 19.3(d)]. These correspond to on-mass-
shell outgoing particles.
(a)
(b)
(c)
(d)
φ
φ
φ
φ
z (−iλ)
ˆφ(z)ˆa†
p
ˆa†
p
φ
ˆφˆφ
φ
φ
ˆaq ˆφ(z)
ˆaq
φ
Fig. 19.3 Steps in drawing a diagram.
Example 19.4
Let’s again consider the term
−12iλ
4!
Z
d4z⟨0|ˆap ˆφ(z)ˆφ(z)ˆφ(z)ˆφ(z)ˆa†
p|0⟩.
(19.17)
The process of drawing a diagram is shown in Fig. 19.3.
• It’s a ﬁrst-order diagram (it contains one copy of HI) so we draw one interac-
tion vertex at position z [Fig. 19.3(a)].

19.3
Anatomy of a diagram
181
• The contraction ˆφ(z)ˆa†
p is represented by an incoming line that grabs one of
the vertex legs [Fig. 19.3(b)].
• The contraction ˆφ(z)ˆφ(z) ties together two vertex legs [Fig. 19.3(c)].
• The contraction ˆaq ˆφ(z) grabs the remaining vertex stub and forms a line
leaving the diagram [Fig. 19.3(d)].
The diagrams that result from our Wick expansion of the S-matrix to
ﬁrst order are shown in Fig. 19.4. The expression for eqn 19.11 is shown
in Fig. 19.4(a), that for eqn 19.12 is shown in Fig. 19.4(b).
(a)
(b)
ˆφˆφ
ˆφˆφ
ˆφˆφ
ˆaq ˆφ
ˆφˆa†
p
ˆaqˆa†
p
Fig. 19.4 First-order contributions in
ˆφ4 theory. (a) A disconnected diagram
with two connected parts.
(b) Con-
nected diagram with one loop.
19.3
Anatomy of a diagram
The diagrams we’ve drawn so far could correspond to stories describing
the interaction of particles if you think of time running upwards.
A
contraction gives you a full diagram. This may be made of many pieces.
Figure 19.4(a) is a diagram made of two pieces, while Fig. 19.4(b) is a
diagram made of a single piece. We call a piece of a diagram a con-
nected diagram. Figure 19.4(a) is a disconnected diagram made up of
two connected diagrams. Disconnected processes cannot inﬂuence each
other (because they are not connected to each other!) and so physical
intuition tells us that we will probably only have to consider connected
diagrams (that expectation will turn out to be right on the money).5
5The cluster decomposition prin-
ciple guarantees that distant experi-
ments yield uncorrelated results.
An
S-matrix constructed out of creation
and annihilation operators turns out
automatically to satisfy this principle,
which therefore provides the deep rea-
son why these operators are required in
quantum ﬁeld theory, rather than just
being a convenience.
The cluster de-
composition principle may be justiﬁed
for connected and disconnected Feyn-
man diagrams using the linked cluster
theorem, discussed in Chapter 22. See
Weinberg, Chapter 4 for a detailed dis-
cussion.
There’s more vocabulary to be learnt in order to talk diagrams with the
professionals:
• External lines have one end which appears not to be connected
to anything. (Actually, they indicate a connection with the world
exterior to the physical process.) Figure 19.4(b) or the left-hand
piece of Fig. 19.4(a) are connected diagrams with external lines.
• A vacuum diagram has no external lines.
The right-hand piece of
Fig. 19.4(a) is a (connected) vacuum diagram. Vacuum diagrams
don’t aﬀect transition probabilities because they are not connected
to the incoming or outgoing particles; they only aﬀect ⟨0| ˆS|0⟩(and
therefore only contribute a phase eiφ to transition amplitudes).
Vacuum diagram
Incoming
external
line
Outgoing
external
line
internal
lines
vertex
Fig. 19.5
The anatomy of a discon-
nected third-order diagram.
A particular connected diagram might contain:
• Vertices where lines join together. These represent interactions.
• External incoming lines (we draw these below the vertices). They
represent on-mass-shell particles entering the process.
• External outgoing lines (we draw these above the vertices). They
represent on-mass-shell particles leaving the process.
• Internal lines (joining two vertices).
These represent virtual par-
ticles which are oﬀ-mass-shell and therefore exist internally within
the diagram.

182
Expanding the S-matrix: Feynman diagrams
These various properties are illustrated in Fig. 19.5.
Why do we draw Feynman diagrams? Simple: knowing how the di-
agrams relate to the contractions means that we can simply draw dia-
grams for a particular interaction and, instead of going through all of the
leg work of doing an expansion, we just write down the equation to which
a diagram corresponds. If someone were now just to give us a Feynman
diagram, how could we translate it into a term in the expansion? Here
are the rules:
Remember that these are the rules for
φ4 theory.
Other theories will have
slightly diﬀerent rules.
Feynman rules for φ4 theory in position space
To calculate an amplitude in the S-matrix expansion, translate a
Feynman diagram into equations as follows:
• Each vertex contributes a factor −iλ.
• Each line gives a propagation factor ∆(x−y), where x and y are
the start and end points of the line.
• External lines contribute incoming (−ip · x) or outgoing (+ip · x)
waves e±ip·x.
• Integrate the positions of the vertices over all spacetime.
• In order to get the right coeﬃcient in front of the term divide by
the symmetry factor D.
This last point is treated in the following section.
19.4
Symmetry factors
A potentially tricky thing is how to work out what number D we divide
by in working out our Feynman diagram amplitude. The number arises
from the number of ways there are to produce a certain diagram through
contractions, divided by 4! (In fact the reason for the 4! is to simplify
this number as much as possible.)
Rather than deriving them we’ll
quote the result which the interested (and/or masochistic) reader can
easily justify by working through the combinatorics.6 The general rule
6Often the symmetry factors aren’t re-
quired to understand the physics of
what a diagram is telling you, but when
combining several diagrams it’s useful
to know them.
is as follows: if there are m ways of arranging vertices and propagators
to give identical parts of a diagram (keeping the outer ends of external
lines ﬁxed and without cutting propagator lines) we get a factor Di = m.
The symmetry factor is given by the product of all symmetry factors
D = Q
i Di.
Two very useful special cases, which aren’t immediately apparent from
the general rule, are as follows:
• Every propagator with two ends joined to one vertex (a loop) gives
a factor Di = 2.
• Every pair of vertices directly joined by n propagators gives a
factor Di = n!

19.5
Calculations in p-space
183
Example 19.5
Figure 19.6 shows several diagrams to which we can apply the rules.
• (a) The bubble propagator has D = 2 since it contains a propagator with two
ends joined to a vertex. Remember you are not allowed to move the ends of
external lines, so there are no more factors to worry about.
• (b) The two-bubble propagator has D = 4.
Each bubble gives a factor of
2, so D = 2 × 2.
The bubble propagators in (a) and (b) are examples of
self-energy diagrams. They are basically free propagators with extra loops
in, but don’t interact with anything else. We will see later that these extra
loops just change the constants in the free propagator, or to use the lingo of
quantum ﬁeld theory, they ‘renormalize the free propagator’.
• (c) The double-bubble vacuum graph has two bubbles (or loops), each con-
tributing a factor 2, but the bubbles can be swapped over (rotating the diagram
180◦about a vertical axis passing through the middle of it), and we get the
same diagram, giving us an extra factor m = 2. Therefore, we have D = 8.
• The Saturn diagram (d) has a pair of vertices joined by three lines, so D =
3! = 6.
• Diagram (e) has a factor of 2 from the bubble. It also has two vertices joined
by two lines, contributing a factor 2!, so D = 2 × 2! = 4.
• Diagram (f) has three vertices. We think of these as being grouped in two
pairs (with the middle vertex counted twice).
Each pair is joined together
by two lines (and so each contributes a factor 2!). In total we therefore have
D = 2 × 2! = 4.
• Diagram (g) has a bubble, so D = 2.
• Diagram (h) has two bubbles but the top and bottom parts between the ver-
tices can be swapped giving an extra factor of 2, so D = 2 × 2 × 2 = 8.
• Diagram (i) has a pair of vertices joined by two lines so D = 2.
Other theories will have slightly diﬀerent rules for their symmetry factors.
(a)
(b)
(c)
(d)
(e)
(f)
(g)
(h)
(i)
Fig. 19.6 Examples of diagrams in φ4
theory.
The derivation of the Feynman rules represents the point where we can
start calculating scattering amplitudes for physical processes. A large
part of formulating a useful quantum ﬁeld theory may be reduced to
the process of deriving the Feynman rules following the path shown in
Fig. 19.7.
Lagrangian
free part
interacting part
Canonical
quantization
Dyson’s
expansion
for S
Wick expansion
Feynman’s rules
calculate
Amplitudes
Fig. 19.7 The process of deriving the
Feynman rules.
19.5
Calculations in p-space
Calculations turn out to be far easier in momentum space, as illustrated
by the following example: the second-order ‘Saturn diagram’.
Example 19.6
Let’s see how the Saturn diagram arises. Consider the second-order O(λ2) term in
the expansion of ⟨q| ˆS|p⟩(i.e. two copies of ˆ
HI(x) in the expansion),
ˆS(2)
=
(−i)2
2!
Z
d4yd4w ˆ
HI(y) ˆ
HI(z)
=
(−iλ)2
2!(4!)2
Z
d4yd4w ˆφ(y)ˆφ(y)ˆφ(y)ˆφ(y)ˆφ(w)ˆφ(w)ˆφ(w)ˆφ(w). (19.18)

184
Expanding the S-matrix: Feynman diagrams
We now apply Wick’s theorem and obtain the diagrams shown in Fig. 19.8. The one
in which we’re interested is Fig. 19.8(f), which comes from the term
⟨q| ˆS(2)|p⟩= (2π)3(2Eq)
1
2 (2Ep)
1
2
× (−iλ)2
D
⟨0|ˆaq ˆφ(y)ˆφ(y)ˆφ(y)ˆφ(y)ˆφ(w)ˆφ(w)ˆφ(w)ˆφ(w)ˆa†
p|0⟩,
(19.19)
where D = 6 is the symmetry factor here. The amplitude for the Saturn Feynman
(b)
(c)
(a)
(d)
(e)
(f)
Fig. 19.8
All of the second-order φ4
diagrams.
w
y
q
k1
k2
k3
p
Fig. 19.9 (a) Position and (b) momen-
tum space Feynman diagrams.
diagram in Fig. 19.9(a) is given by
−λ2
6
Z
d4yd4w eiq·y∆(y −w)∆(y −w)∆(y −w)e−ip·w.
(19.20)
Each spacetime propagator ∆gives us a factor of
∆(y −w) =
Z
d4k
(2π)4 e−ik·(y−w)
i
k2 −m2 + iǫ.
(19.21)
So, upon collecting all of the exponentials together, we have
Saturn = −λ2
6
Z
d4k1
(2π)4
d4k2
(2π)4
d4k3
(2π)4
i
(k2
1 −m2 + iǫ)
i
(k2
2 −m2 + iǫ)
i
(k2
3 −m2 + iǫ)
×
Z
d4yd4w
“
eiq·ye−i(k1+k2+k3)·y” “
ei(k1+k2+k3)·we−ip·w”
.
Next we notice that some of the integrals will give us delta functions, which simplify
things considerably.
Integrating y over all space gives us an additional factor of
R
d4y e−i(k1+k2+k3−q)·y = (2π)4δ(4)(k1 + k2 + k3 −q) and we have
Saturn = −λ2
6
Z
d4k1
(2π)4
d4k2
(2π)4
d4k3
(2π)4
i
(k2
1 −m2 + iǫ)
i
(k2
2 −m2 + iǫ)
i
(k2
3 −m2 + iǫ)
×
Z
d4w
“
ei(k1+k2+k3)·we−ip·w”
(2π)4δ(4)(k1 + k2 + k3 −q),
which means that we may set k1 = q −k2 −k3 upon doing the k1 momentum
integral. We see that the integral has given us a constraint on the allowed values of
the momenta. Translated into real life, this means that the sum of four-momenta
at each vertex is zero. Energy-momentum is therefore conserved at each interaction
vertex. So now we have
Saturn = −λ2
6
Z
d4k2
(2π)4
d4k3
(2π)4
i
[(q −k2 −k3)2 −m2 + iǫ]
i
(k2
2 −m2 + iǫ)
i
(k2
3 −m2 + iǫ)
×
Z
d4w
“
eiq·we−ip·w”
.
The w integration gives
R
d4w ei(q−p)·w = (2π)4δ(4)(q −p). The overall energy-
momentum is conserved and the entire diagram carries an overall energy-momentum
conserving delta function around with it. We’re left with two momentum integrals
to do. Finally we arrive at the momentum space result:
Saturn = −λ2
6
(2π)4δ(4)(q −p)
Z
d4k2
(2π)4
d4k3
(2π)4
i
[(q −k2 −k3)2 −m2 + iǫ]
×
i
(k2
2 −m2 + iǫ)
i
(k2
3 −m2 + iǫ) . (19.22)
Although we’ve ended up with the same number of integrals as with the position
space version, we’ve expunged all of the exponential factors. In addition, since the
energy-momentum conserving delta function is common to all diagrams, we often
choose not to carry it around, but to understand that energy-momentum is im-
plicitly conserved (and reinstate the factor when it’s needed in the mathematics!).

19.5
Calculations in p-space
185
We may now write down a set of Feynman rules for φ4 theory in mo-
mentum space:
Feynman rules for φ4 theory in momentum space
• Each vertex contributes a factor −iλ [Fig. 19.10(a)].
• Label each internal line with a momentum q ﬂowing along it and
describe it by a propagator
i
q2−m2+iǫ [Fig. 19.10(b)].
• Force the sum of each momentum coming into a vertex to be
equal to the momentum leaving it.
• Integrate over unconstrained internal momenta with a measure
d4q
(2π)4 .
• External lines contribute a factor 1 [Fig. 19.10(c)].
• Divide by the symmetry factor.
• Include an overall energy-momentum conserving delta function
for each diagram.
Remember that the external lines don’t contribute a propagator.
In
momentum space the external line parts are just ˆap ˆφ(x) = 1. Remember
also that you only need to integrate over unconstrained momenta. That
is, only integrate over momenta which are not already determined by the
conditions that momentum is conserved both at the vertices and overall
for a diagram. (If in doubt you can include a delta function for each
vertex and integrate over all momenta.)
(a)
(b)
(c)
= −iλ
=
i
q2 −m2 + iǫ
q
= 1
= 1
Fig. 19.10
Momentum space Feyn-
man rules for φ4 theory.
Example 19.7
Let’s now apply our rules to some example Feynman diagrams. Some terms in the
expansion of ⟨q| ˆS|p⟩up to second order are shown in Fig. 19.11. Applying the rules
to some of these yield the following for their contribution to ⟨q| ˆS|p⟩:
(b)
=
(2π)4δ(4)(q −p) (−iλ)
2
Z
d4k
(2π)4
i
k2 −m2 + iǫ,
(f)
=
(2π)4δ(4)(q −p) (−iλ)
2
Z
d4k3
(2π)4
i
k2
3 −m2 + iǫ ×
(−iλ)
8
Z
d4k1
(2π)4
d4k2
(2π)4
i
(k2
1 −m2 + iǫ)
i
(k2
2 −m2 + iǫ) ,
(g)
=
(2π)4δ(4)(q −p) ×
(−iλ)2
4
Z
d4k1
(2π)4
d4k2
(2π)4
i
(k2
1 −m2 + iǫ)
i
(p2 −m2 + iǫ)
i
(k2
2 −m2 + iǫ) ,
(h)
=
(2π)4δ(4)(q −p) ×
(−iλ)2
6
Z
d4k1
(2π)4
d4k2
(2π)4
i
(k2
1 −m2 + iǫ)
i
(k2
2 −m2 + iǫ)
i
[(p −k1 −k2)2 −m2 + iǫ] .
(a)
(b)
(c)
(d)
(e)
(f)
(g)
(h)
q
q
q
p
p
p
k
k1
k2
p
p
q
k1
k2
q
p
p −k1 −k2
k1
k2
Fig. 19.11 Examples of Feynman di-
agrams for φ4 theory, up to second or-
der in the interaction strength, for the
S-matrix element ⟨q| ˆS|p⟩.
In summary: the amplitude A =out ⟨q|p⟩in = ⟨q| ˆS|p⟩may be written
as a sum of diagrams. Each diagram stands for an integral. We call

186
Expanding the S-matrix: Feynman diagrams
diagrams which describe how interactions aﬀect the amplitudes of single
particles self-energy diagrams.
We haven’t yet discussed how to ﬁnally do the integral to ﬁnally get
the numbers out. This is because often the integrals give us divergent
(i.e. inﬁnite) results! Taming these fearsome inﬁnities reveals a huge
amount about the physics lying behind quantum ﬁeld theory and we
discuss that in Chapter 32. Some diagrams don’t diverge, of course, and
we examine some of these in Chapter 20. For now, we need some more
experience getting used to how the expansions go.
19.6
A ﬁrst look at scattering
We have looked at diagrams describing single particles. What about a
process of two particles entering and then leaving? The amplitude we’re
after is
⟨q1q2| ˆS|p2p1⟩= (2π)6(16Ep1Ep2Eq1Eq2)
1
2 ⟨0|ˆaq1ˆaq2 ˆSˆa†
p2ˆa†
p1|0⟩,
(19.23)
describing particles entering in momentum states p1 and p2 and leaving
in states q1 and q2. We could of course now start the process again and
work through all of the contractions. However, this is now unnecessary
since we have Feynman diagrams for the theory.
Example 19.8
Some diagrams contributing to the matrix element ⟨q1q2| ˆS|p2p1⟩are shown in
Fig. 19.12. The ﬁrst two yield up
(a)
(b)
(c)
(d)
(e)
(f)
(g)
(h)
(i)
(j)
(k)
(l)
p1
p2
q1
q2
p1
p2
q1
q2
p1
p2
q1
q2
q1
q2
p1
p2
k
p1 + p2 −k
Fig.
19.12
Examples of Feynman
diagrams
for
the
S-matrix
element
⟨q1q2| ˆS|p2p1⟩up to second order in the
interaction strength.
Diagram (a)
=
(2π)6 (16Ep1Ep2Eq1Eq2)
1
2 δ(3)(q1 −p1) δ(3)(q2 −p2),
Diagram (b)
=
(2π)6 (16Ep1Ep2Eq1Eq2)
1
2 δ(3)(q1 −p2) δ(3)(q2 −p1).
These clearly aren’t very interesting since no interactions occur.
They just arise
from the zeroth-order part of the ˆS-operator, ˆS(0) = 1. They don’t contribute to
scattering, and therefore aren’t measured in an experiment. We’ll ignore them.
Some more interesting examples of amplitudes are the following:
Diagram (c)
=
(2π)4δ(4)(q1 + q2 −p1 −p2)(−iλ),
Diagram (f)
=
(2π)4δ(4)(q1 + q2 −p1 −p2) (−iλ)2
2
×
Z
d4k
(2π)4
i
[k2 −m2 + iǫ]
i
[(p1 + p2 −k)2 −m2 + iǫ] .
Chapter summary
• The S-matrix may be expanded in a ﬁve-step process.
• The terms in the expansion can be encoded in Feynman diagrams.
• It’s easiest to work in momentum space.

Exercises
187
Exercises
(19.1) Write down the momentum space amplitudes for
the processes shown in the Feynman diagrams in
Fig. 19.6.
(19.2) (a) Draw the interaction vertex for φ3 theory,
which is a scalar ﬁeld theory with Lagrangian L =
1
2(∂µφ)2 −m2
2 φ2 −η
3!φ3.
(b) By expanding the S-matrix ﬁnd the contribu-
tions to the amplitude ⟨q| ˆS|p⟩up to second order
in the interaction strength. Draw the correspond-
ing Feynman diagrams.
What are the symmetry
factors?
(c) Draw the new, connected diagrams that con-
tribute to ⟨q| ˆS|p⟩when you consider the fourth-
order contribution.
(d) Which connected diagrams contribute to the
two-particle
S-matrix
element
⟨q1q2| ˆS|p2p1⟩
at
fourth order in the interaction.
For simplicity, just draw topologically distinct dia-
grams, ignoring new ones produced by permutations
of external lines.
(19.3) Consider the ABA theory, deﬁned by the La-
grangian
L
=
1
2(∂µφA)2 −m2
A
2 φ2
A
(19.24)
+1
2(∂µφB)2 −m2
B
2 φ2
B −g
2φAφBφA.
This theory describes the interaction of two scalar
ﬁelds.
(a) Draw the interaction vertex using diﬀerent
styles of line for the diﬀerent ﬁelds.
(b) Draw the Feynman diagrams that contribute to
the scattering amplitude ⟨qA| ˆS|pA⟩up to fourth or-
der (here pA refers to an A-particle in momentum
state p). Write expressions for the amplitudes for
these diagrams. You may ignore symmetry factors.
(c) Draw the Feynman diagrams that contribute to
the scattering ⟨qB| ˆS|pB⟩up to fourth order in the
interaction.
Write expressions for the amplitudes
for these diagrams. Again, you may ignore symme-
try factors.
(d) Draw Feynman diagrams that contribute to
the A-particle scattering ⟨qA1qA2| ˆS|pA2pA1⟩up to
fourth order.
(e) What are the rules for the symmetry factors of
this theory?
(19.4) (a) Show that the amplitude for the double bubble
diagram [Fig. 19.6(c)] is given by
A = −iλ
8
»Z
d4p
(2π)4
i
p2 −m2 + iǫ
–2
δ(4)(x = 0).
(19.25)
Here the δ(4)(x = 0) factor gives us a quantity pro-
portional to the volume of the system V multiplied
by the total time T. This factor is inﬁnite, but we
will show later that this will not worry us.
(b) Argue that this factor arises for all vacuum di-
agrams.

20
Scattering theory
20.1 Another
theory:
Yukawa’s
ψ†ψφ interactions
188
20.2 Scattering in the ψ†ψφ the-
ory
190
20.3 The
transition
matrix
and
the invariant amplitude
192
20.4 The scattering cross-section
193
Chapter summary
194
Exercises
194
And the LORD shall cause his glorious voice to be heard, and
shall shew the lighting down of his arm, . . . with scattering,
and tempest, and hailstones.
Isaiah 30:30
One important application of quantum ﬁeld theory is the calculation of
scattering cross-sections. It is important since scattering cross-sections
are measured in many experiments. In this chapter we examine scat-
tering using Hideki Yukawa’s ψ†ψφ theory, which is an illuminating toy
model describing the interactions of scalar ﬁelds. The joy of this model is
that it bears a strong resemblance to quantum electrodynamics (QED),
which describes electrodynamics to an astounding degree of accuracy.
Speciﬁcally, in ψ†ψφ theory the phion excitations in the φ-ﬁeld take the
role of (massive, scalar) photons and the psion excitations in the ψ-ﬁeld
describe (complex scalar, bosonic) electrons.
20.1
Another theory: Yukawa’s ψ†ψφ
interactions
ψ(x)
x
ψ†(x)
φ(x)
Fig. 20.1
The ψ†ψφ interaction ver-
tex.
The theory describes a complex scalar ﬁeld ψ and a real scalar ﬁeld φ
interacting. The full Lagrangian for this theory is
L = ∂µψ†∂µψ −m2ψ†ψ + 1
2(∂µφ)2 −1
2µ2φ2 −gψ†ψφ.
(20.1)
Here psions have mass m and phions have mass µ. This Lagrangian is
the sum of the free scalar ﬁeld and free complex scalar ﬁelds Lagrangian
with an interaction part, shown in Fig. 20.1, given by LI = −gψ†ψφ.
We start by writing down the mode expansions of each of the free ﬁelds:
ˆψ(x)
=
Z
d3p
(2π)
3
2
1
(2Ep)
1
2

ˆape−ip·x + ˆb†
peip·x
,
ˆψ†(x)
=
Z
d3p
(2π)
3
2
1
(2Ep)
1
2

ˆa†
peip·x + ˆbpe−ip·x
,
ˆφ(x)
=
Z
d3q
(2π)
3
2
1
(2εq)
1
2
 ˆcqe−iq·x + ˆc†
qeiq·x
,
(20.2)
where Ep = (p2 + m2)
1
2 and εq = (q2 + µ2)
1
2 . Here the ˆa-operators
describe the creation and annihilation of psions, the ˆb-operators de-
scribe the creation and annihilation of antipsions and the ˆc-operators

20.1
Another theory: Yukawa’s ψ†ψφ interactions
189
create and destroy scalar phions. The interaction Hamiltonian to use
in Dyson’s equation is1 ˆHI(z) = g ˆψ†(z) ˆψ(z)φ(z), whose interaction dia-
1Remember that
ˆ
HI = −ˆLI so ˆLI =
−g ˆψ† ˆψφ implies ˆ
HI = g ˆψ† ˆψφ.
gram is shown in Fig. 20.1. We decorate the interaction diagram with an
arrow pointing toward the interaction blob for the ˆψ-ﬁeld and an arrow
pointing away for the ˆψ†-ﬁeld. The meaning of these arrows will be to
show particle number ﬂow, as discussed a little later.
To calculate S-matrix elements we’ll work through our ﬁve-point plan.
The most important new feature of this theory is that now we have the
possibility of antiparticles in our interactions.
Step I: We decide what to calculate. The one psion in, one psion out
amplitude is
A = ⟨q| ˆS|p⟩= (2π)3(2Eq)
1
2 (2Ep)
1
2 ⟨0|ˆaq ˆSˆa†
p|0⟩.
(20.3)
Step II: We expand the ˆS-operator using Dyson’s expansion
ˆS
=
1 + (−ig)
Z
d4z ˆψ†(z) ˆψ(z)ˆφ(z)
(20.4)
+(−ig)2
2!
Z
d4yd4w
h
ˆψ†(y) ˆψ(y)ˆφ(y)
i h
ˆψ†(w) ˆψ(w)ˆφ(w)
i
+ . . .
Step III: The next stage is to plug in to get the amplitudes for the
various processes we’re going to calculate. All ﬁrst-order terms will turn
out to give zero (try it!) so we’ll consider the second-order term
A(2) = (−ig)2
2!
(2π)3(2Ep)
1
2 (2Eq)
1
2 ⟨0|ˆaq ˆψ†(y) ˆψ(y)ˆφ(y) ˆψ†(w) ˆψ(w)ˆφ(w)ˆap|0⟩.
(20.5)
Step IV: We use Wick to digest the matrix elements. Here are the rules
for the nonzero contractions for this theory:
ˆψ(x) ˆψ†(y) =
R
d4p
(2π)4 ie−ip·(x−y)
p2−m2+iǫ ,
ˆφ(x)ˆφ(y) =
R
d4q
(2π)4 ie−iq·(x−y)
q2−µ2+iǫ ,
ˆap ˆψ†(x) =
1
(2π)
3
2
1
(2Ep)
1
2 eip·x,
ˆψ(x)ˆa†
p =
1
(2π)
3
2
1
(2Ep)
1
2 e−ip·x,
ˆbp ˆψ(x) =
1
(2π)
3
2
1
(2Ep)
1
2 eip·x,
ˆψ†(x)ˆb†
p =
1
(2π)
3
2
1
(2Ep)
1
2 e−ip·x,
ˆcq ˆφ(x) =
1
(2π)
3
2
1
(2εq)
1
2 eiq·x,
ˆφ(x)ˆc†
q =
1
(2π)
3
2
1
(2εq)
1
2 e−iq·x.
(20.6)
All other contractions give zero. Finally, we’ll agree to work in momen-
tum space and note that the symmetry factor for all diagrams in ˆψ† ˆψ ˆφ
theory is D = 1.
Example 20.1
Using these contractions let’s see what sort of diagrams are thrown up. Consider the
second-order term with the following contraction
⟨0|ˆaq ˆψ†(y) ˆψ(y)ˆφ(y) ˆψ†(w) ˆψ(w)ˆφ(w)ˆa†
p|0⟩.
(20.7)

190
Scattering theory
This leads to the Feynman diagram shown in Fig. 20.2(a), which is known as a
tadpole.2 In a theory with no particles in the ground state this diagram gives zero.
2This name was coined by Sidney Cole-
man.
When the journal Physical Re-
view objected to the name, Coleman
suggested the alternative ‘Spermion’.
Physical Review relented.
Consider another set of contractions:
⟨0|ˆaq ˆψ†(y) ˆψ(y)ˆφ(y) ˆψ†(w) ˆψ(w)ˆφ(w)ˆa†
p|0⟩.
(20.8)
This leads to the Feynman diagram shown in Fig. 20.2(b), known as an oyster, which
when translated into equations gives an amplitude Aoyster:
(−ig)2
Z
d4k
(2π)4
i
(k2 −µ2 + iǫ)
i
[(p −k)2 −m2 + iǫ] (2π)4δ(4)(q −p).
(20.9)
There are also two disconnected contributions involving vacuum diagrams, shown in
Fig. 20.3.
(a)
(b)
aψ†
ψψ†
ψa†
φφ
aψ†
ψψ†
ψa†
φφ
Fig. 20.2 (a) A tadpole diagram. (b)
An oyster diagram.
(a)
(b)
ψψ†
ψψ†
ψψ†
ψψ†
φφ
φφ
aa†
aa†
Fig.
20.3
Disconnected
diagrams
making second-order contributions.
There are a few new things to note about the Feynman diagrams in this
theory. The ﬁrst is that we include arrows on the psion and antipsion
lines of our Feynman diagrams, but not the phi lines. Psions have arrows
going in the direction of ‘time’ (that is, up the page), antipsions have
arrows going against the ‘time’ direction. These arrows don’t represent
the direction of momentum. Actually arrows on the lines represent the
(conventional) direction of Noether current JNc (i.e. conserved particle
number ﬂow). Motivated by the expression ˆQNc =
R
d3p (ˆn(a)
p
−ˆn(b)
p )
we say that incoming particles increase particle number and correspond
to lines inwards on the diagram whilst incoming antiparticles reduce
particle number resulting in outward going lines in the diagram.
To
avoid confusion, it sometimes helps to draw extra lines showing the
directions of momenta (see, e.g. Fig. 20.6). Notice that this implies that
an antiparticle has momentum in the opposite direction to its number
ﬂow.
20.2
Scattering in the ψ†ψφ theory
So far we’ve only considered one particle coming in and one particle leav-
ing. Now let’s consider the process of two psions coming in, scattering,
then two psions leaving. The S-matrix element we’re after is
⟨q1q2| ˆS|p2p1⟩= ⟨0|ˆaq1ˆaq2 ˆSˆa†
p2ˆa†
p1|0⟩.
(20.10)
It’s fairly obvious that all ﬁrst-order terms are zero (since it’s impos-
sible to draw two-in two-out Feynman diagrams with only one of our
interaction vertices). We’ll consider the second-order terms.
Example 20.2
From the contraction
⟨0|ˆaq1ˆaq2 ˆψ†(y) ˆψ(y)ˆφ(y) ˆψ†(w) ˆψ(w)ˆφ(w)ˆa†
p2ˆa†
p1|0⟩,
(20.11)

20.2
Scattering in the ψ†ψφ theory
191
we have the diagram shown in Fig. 20.4(a). Two psions enter, one emits a force-
carrying, virtual phion which collides with the second psion. The two psions then
leave. This is known in particles physics as a t-channel process (following notation
introduced by Stanley Mandelstam in 1958 in which the three possible processes were
assigned the letters ‘s’, ‘t’ and ‘u’).
Stanley Mandelstam (1928– )
A closely related process arises from the following contraction
⟨0|ˆaq1ˆaq2 ˆψ†(y) ˆψ(y)ˆφ(y) ˆψ†(w) ˆψ(w)ˆφ(w)ˆa†
p2ˆa†
p1|0⟩,
(20.12)
represented by the diagram in Fig. 20.4(b) and is known as a u-channel process. It’s
the same as the t-channel process, except that the initial and ﬁnal particles have
changed places after the interaction. To obtain the amplitude ⟨q1q2| ˆS|p2p1⟩for the
scattering of indistinguishable particles at second order add the amplitudes from the
t- and u-channel processes.
What about the following process describing psions interacting with antipsions
(denoted |¯p⟩)? The expression
⟨¯q1q2| ˆS|p2 ¯p1⟩= ⟨0|ˆbq1ˆaq2 ˆSˆa†
p2ˆb†
p1|0⟩
(20.13)
describes a psion and antipsion entering, interacting, then leaving. We can still have
(a)
(b)
(c)
aq1ψ†(y)
aq2ψ†(w)
ψ(y)a†
p1
ψ(w)a†
p2
φ(y)φ(w)
φ(y)φ(w)
φ(y)φ(w)
aq1ψ†(w)
aq2ψ†(y)
ψ(y)a†
p1
ψ(w)a†
p2
ψ(w)a†
p2
ψ†(w)b†
p1
bq1ψ(y)
aq2ψ†(y)
Fig. 20.4
(a) A t-channel scattering
process. (b) A u-channel process. (c)
An s-channel process involving parti-
cles scattering from antiparticles.
the contraction which leads to a process analogous to Fig. 20.4(a), although this will
involve antipsion lines with arrows pointing against the direction of time (i.e. down
the page) attached to one vertex. In addition, another interesting new contraction
(a)
(b)
(c)
(d)
= −ig
=
i
q2 −µ2 + iǫ
q
= 1
= 1
p
p′
q = (p −p′)
Fig. 20.5 Feynman rules for ψ†ψφ the-
ory.
possibility occurs here. Look at
⟨0|ˆbq1ˆaq2 ˆψ†(y) ˆψ(y)ˆφ(y) ˆψ†(w) ˆψ(w)ˆφ(w)ˆa†
p2ˆb†
p1|0⟩,
(20.14)
which corresponds to the diagram in Fig. 20.4(c).
This depicts a psion and an
antipsion coming in and annihilating. They become a virtual phion which decays
into a psion and antipsion. This is known in particle physics as an s-channel process.
Note that the u-channel process now involves the exchange of the particle and an-
tiparticle, so becomes distinguishable from the t- and s-channel processes. It there-
fore contributes to ⟨q1¯q2| ˆS|p2 ¯p1⟩, rather than ⟨¯q1q2| ˆS|p2 ¯p1⟩. To obtain the amplitude
⟨¯q1q2| ˆS|p2 ¯p1⟩at second order we therefore need to add the amplitudes from the t-
and s-channel processes only.
The idea behind all of this exploration is to write down a set of Feynman
rules for this theory, which we now do.
Feynman rules for ψ†ψφ theory
• Each vertex contributes a factor −ig [Fig. 20.5(a)].
• For each phion internal line carrying momentum q include a prop-
agator
i
q2−µ2+iǫ [Fig. 20.5(b)]. For a psion internal line include
a propagator
i
q2−m2+iǫ.
• Integrate over all undetermined momenta.
• Incoming and outgoing lines contribute a factor 1 [Fig. 20.5(c-d)].
• All symmetry factors are 1.
• Include an overall energy-momentum conserving delta function
for each diagram.

192
Scattering theory
Example 20.3
The amplitude for the second-order scattering diagrams (including the overall energy-
momentum conserving delta function) is
A(2) = (−ig)2
i
q2 −µ2 + iǫ (2π)4 δ(4) “X
pf −
X
pi
”
.
(20.15)
Here we don’t integrate over q, as it is completely determined by momentum conser-
vation for these diagrams. Referring to Fig. 20.6 we have the following amplitudes:
• For the t-channel process q2 = (p′ −p)2 = t and we have
At = (−ig)2
i
t −µ2 + iǫ (2π)4 δ(4)(p′ + k′ −p −k).
(20.16)
• For the u-channel process we have q2 = (p′ −k)2 = u and
Au = (−ig)2
i
u −µ2 + iǫ(2π)4 δ(4)(p′ + k′ −p −k).
(20.17)
• For the s-channel process we have q2 = (p + p′)2 = s and
As = (−ig)2
i
s −µ2 + iǫ (2π)4 δ(4)(k + k′ −p −p′).
(20.18)
(a)
(b)
(c)
p
p′
k
k′
p
p′
k
k′
p
p′
k
k′
q = p′ −p
q = p′ −k
q = p + p′
Fig.
20.6
The
t-,
u-
and
s-
channel scattering processes in momen-
tum space.
20.3
The transition matrix and the
invariant amplitude
One thing that one immediately notices about the expansion of the ˆS-
operator is that the ﬁrst term is unity. This means that the only nonzero
amplitude from this term in the expansion has identical initial and ﬁnal
states. This is a rather dull result as far as scattering is involved (since
nothing happens) so it’s commonly removed by writing ˆS = 1+i ˆT, where
the matrix elements of ˆT are often called the transition or T-matrix.
Another irritation is that diagrams carry the overall energy-momentum
conserving delta function. This is often factored out to make the so-
called invariant amplitude M, deﬁned for a two-in, two-out process as
⟨p1fp2f|i ˆT|p2ip1i⟩= (2π)4δ(4)(p1f + p2f −p2i −p1i)iM,
(20.19)
which at least prevents us from having to continuously write down the
delta function.3
3The factor of i is included so that the
amplitudes match up with the results
from the conventions of non-relativistic
scattering theory.
We’re now ready to reveal a wonderful simpliﬁcation. Remember that
diagrams fall into two classes: connected diagrams and disconnected di-
agrams. It turns out that only fully connected diagrams contribute to the
T-matrix, a point which makes good physical sense since disconnected
diagrams correspond to physical processes which do not inﬂuence each
other. We also note that there are a class of diagrams which have loops
attached to the external legs. These are bad news and often lead to
unpleasant and non-trivial inﬁnities. The good news is that they don’t
contribute to the T-matrix either, so we can remove them.4 We are left
4To remove them, we deﬁne the act of
amputation, which involves chopping
oﬀall such carbuncles on the external
legs.

20.4
The scattering cross-section
193
with the appealing result that
iM(2π)4 δ(4) X
pf −
X
pi

=
X


All connected, amputated Feynman
diagrams with incoming momentum pi
and outgoing momentum pf

.
(20.20)
We now need to design an experiment which measures M and that
turns out to be a scattering experiment.
20.4
The scattering cross-section
(a)
(b)
Fig. 20.7
Scattering cross-sections.
(a)
σsheep
>
σﬁeld mouse.
(b)
σcow, side > σcow, front.
When ﬁring one particle at another, we want to think about the ampli-
tude of scattering of our incoming particle in terms of how big the other
particle appears to be in cross-sectional area terms. Imagine driving at
night, where your car headlights give you a beam of particles with which
you probe the inky blackness. A sheep shows up more in your headlights
than a ﬁeld mouse, simply because it’s bigger and scatters more photons
[Fig. 20.7(a)]. Similarly a cow will scatter more light if seen from the
side than from the front, because it oﬀers a larger cross-sectional area
in the former case [Fig. 20.7(b)]. However it’s not just about area –
white sheep are more visible than black sheep – but we fold this all in
to our deﬁnition of the scattering cross-section σ in which we measure
the rate R of a scattering process occurring as R = σL, where L is the
luminosity5 of our incoming beam of particles.
5Luminosity is a quantity with dimen-
sions Time−1× Area−1 to be evaluated
for the case under study.
Scattering can occur in all directions, and so in each bit of solid angle
dΩthere will be a bit of scattering dσ. A detector usually only covers a
range of solid angle, so it is usual to think in terms of the diﬀerential
cross-section6 deﬁned by dσ/dΩ.
Fermi’s golden rule allows us to
6The
relationship
between
σ
and
dσ/dΩis given by
Z 2π
0
dφ
Z 1
−1
d(cos θ) dσ
dΩ= σ.
relate the scattering cross-section to the modulus squared of a matrix
element. After taking proper account of all the normalization factors
one can show,7 for example, that for a scattering process involving two
7This result is discussed in Peskin and
Schroeder, Chapter 4.
equal-mass particles scattering oﬀeach other then
dσ
dΩ=
|M|2
64π2E2
CM
,
(20.21)
where E2
CM is the total energy in the centre of mass frame.
Example 20.4
We’re ﬁnally going to work out some scattering amplitudes and cross-sections for
ψ†ψφ theory. We start with the scattering of two distinguishable psion particles. In
this case only the t-channel diagram [Fig. 20.6(a)] contributes. Let’s further simplify
by considering only a non-relativistic case. This means that we can take the four-
momentum p = (Ep, p) to be p ≈(m, p). We have
p1i ≈(m, p),
p2i ≈(m, k),
p1f ≈(m, p′),
p2f ≈(m, k′),
(20.22)

194
Scattering theory
so t may be written t = q2 = (p1f −p1i)2 ≈−|p′ −p|2 = −|q|2, where q = p′ −p is
the three-momentum transfer. The scattering amplitude is given by evaluating the
t-channel Feynman diagram yielding
iM =
ig2
|q|2 + µ2 ,
(20.23)
where we ignore the iǫ, since it’s unnecessary here. Plugging into our expression for
the cross-section and noting that ECM = 2m, we obtain
dσ
dΩ=
1
256π2m2
„
g2
|q|2 + µ2
«2
.
(20.24)
We have a measurable prediction from the theory.
Interestingly this calculation
can also be done in non-relativistic quantum mechanics using Born’s approximation,
which says that the amplitude for scattering from a potential V (r) is ⟨p′|i ˆT|p⟩=
Max Born (1882–1970)
−i ˜V (q)(2π)δ(Ep′ −Ep), where ˜V (q) = ⟨p′| ˆV (r)|p⟩=
R
d3r ei(p−p′)·rV (r). Com-
paring with our quantum ﬁeld theory result, we see that Born says that our scattering
potential is
˜V (q) =
−g2
|q|2 + µ2 .
(20.25)
What function has a Fourier transform that looks like this? The answer is
V (r) = −
g2
4π|r|e−µ|r|;
(20.26)
the potential dies away over a distance 1/µ. But this is just Yukawa’s potential!8
8See Chapter 17.
This is the reason why the ψ†ψφ theory is often called Yukawa theory.
We now have enough formalism to start calculating amplitudes relevant
in real systems.9 However, in the next chapter we will take a break
9Readers interested in fermions, pho-
tons
and
quantum
electrodynamics
may now proceed to Chapter 36. How-
ever, the main route of the book will be
diﬀerent.
from quantum mechanics and discuss the seemingly unrelated subject
of statistical physics.
This subject has an unexpected connection to
quantum ﬁeld theory that will provide some powerful new insights.
Chapter summary
• Yukawa’s theory involves an interaction term ˆHI = g ˆψ† ˆψ ˆφ.
• We have introduced two new diagrams: tadpole and oyster.
• The scattering matrix and cross-section can be related to the in-
variant amplitude M. The only contribution to M comes from
connected, amputated Feynman diagrams.
Exercises
(20.1) Verify the rules for the contractions in eqn 20.6.
(20.2) (a) Verify that the Fourier transform of V (r) =
−
g2
4π|r|e−µ|r| is ˜V (q) =
−g2
|q|2+µ2 .
(b) By taking an appropriate limit, ﬁnd the form of
the Fourier transform of the Coulomb potential.

Part V
Interlude: wisdom from
statistical physics
Statistical physics is the study of large assemblies of atoms or particles
and allows one to extract thermodynamic quantities through a process
of averaging.
Typically you write down the partition function Z for
the system and manipulate it to generate the quantities you want. In
quantum ﬁeld theory there is an analogous process which involves a
generating function Z[J] which can be processed to bring forth Green’s
functions.
• A rapid crash course in basic statistical physics is given in Chap-
ter 21, showing how the partition function can be used to manufac-
ture any desired thermodynamic quantity or correlation function.
• Chapter 22 develops these ideas for quantum ﬁeld theory, intro-
ducing the generating functional Z[J] and linking this with the
S-matrix via the Gell-Mann–Low theorem and showing how these
concepts are connected with diagrams.

21
Statistical physics: a crash
course
21.1 Statistical
mechanics
in
a
nutshell
196
21.2 Sources in statistical physics
197
21.3 A look ahead
198
Chapter summary
199
Exercises
199
Relax, you know more than you think you do.
Benjamin Spock (1903–1998), Baby and Child Care
21.1
Statistical mechanics in a nutshell
Consider a system with a Hamiltonian ˆH0 that has energy eigenvalues
Eλ. The probability of the system being in some particular state |λ⟩
with energy Eλ at a temperature T is given by the Gibbs distribution
pλ = e−βEλ
Z
,
(21.1)
where β = 1/kBT and Z is known as the partition function which is
deﬁned as a sum over states Z = P
λ e−βEλ, or equivalently
Z =
X
λ
⟨λ|e−β ˆ
H0|λ⟩= Tr
h
e−β ˆ
H0i
.
(21.2)
The term Z is needed to guarantee that P
λ pλ = 1, which is a require-
ment of any probability distribution. However, the partition function
Z is more important than a mere normalization constant. It turns out
that all of the information about the system is contained in Z and for
this reason Z is also known as a generating function.
i = 1
2
3
4
5
6
7
8
9 10
...
Fig. 21.1
A simple model of a one-
dimensional magnet.
Consider a one-dimensional magnet which is made up of a lattice of
N sites, each one labelled by an index i, decorated with spins (shown in
Fig. 21.1). The spins have magnitude S = 1
2 and can point up (Sz = + 1
2)
or down (Sz = −1
2).1 We deﬁne the operator-valued ﬁeld ˆφi. This is the
1This, of course, is shorthand.
We
mean that the spin states are eigen-
states of the ˆSz operator, which has
eigenvalues Sz = ± 1
2.
same sort of ﬁeld as we had before: we input a lattice point position i
and get out an operator which we use to act on a state of the system.
In this case the operator is ˆSzi which we understand to act only on the
spin in the ith position.
Example 21.1
A state-vector |A⟩lists the values of each of the spins in our one-dimensional magnet,
e.g. |A⟩= | ↑↑↓↓↓↓. . .⟩. We input a position (e.g. i = 4) and obtain an operator
corresponding to that position ˆφ4. Acting on the state |A⟩with ˆφ4 yields
ˆφ4| ↑↑↓↓↓↓. . .⟩= −1
2| ↑↑↓↓↓↓. . .⟩,
(21.3)

21.2
Sources in statistical physics
197
since the spin at position i = 4 has eigenvalue Sz4 = −1
2 .
Acting on |λ⟩with ˆφi yields S(λ)
zi |λ⟩, where S(λ)
zi
is the eigenvalue of
the operator ˆSz applied to the ith spin for the state |λ⟩.
As usual,
the expectation value of the ˆφi operator for the state |λ⟩is ⟨λ|ˆφi|λ⟩.
At a temperature T we might want to know the thermal expectation
value of the ith spin, denoted ⟨ˆφi⟩t which is found by summing over
the expectation values weighted by the probabilities of ﬁnding a state of
energy Eλ thus:
⟨ˆφi⟩t =
X
λ
S(λ)
zi pλ
=
1
Z
X
λ
S(λ)
zi e−βEλ
=
1
Z
X
λ
⟨λ|ˆφi|λ⟩e−βEλ
=
1
Z
X
λ
⟨λ|ˆφie−β ˆ
H0|λ⟩.
=
Tr
h
ˆφie−β ˆ
H0
i
Z
.
(21.4)
The quantity ˆρ = e−β ˆ
H0/Z is known as the probability density operator.
Its matrix elements are known collectively as the density matrix. We
conclude that to ﬁnd the thermal expectation value of an operator in
statistical physics we need to multiply it by the density matrix and then
take a trace over all of the states. That’s pretty much all there is to
statistical physics.
In the absence of a magnetic ﬁeld, a
system of non-interacting spins at some
particular temperature is expected to
have ⟨ˆφi⟩t = 0 for all i. This reﬂects
the fact that spin i has equal proba-
bility to be found with Sz = 1/2 or
−1/2. If however a system shows spin
order (more usually called magnetic or-
der) then we would expect ⟨φi⟩t ̸= 0 for
all i.
21.2
Sources in statistical physics
There’s a neater way of getting the thermal average of the ﬁeld ⟨ˆφi⟩t.
It involves adding to the Hamiltonian
ˆH0 a ﬁctional term
ˆHs
=
−1
β
P
k Jk ˆφk. Such a term, which involves coupling a ﬁeld Jk to the
spin at position k, is known as a source term for reasons which will
come apparent later.2 With the inclusion of the source term, the parti-
2The source term for a magnet is actu-
ally proportional to the magnetic ﬁeld
Bi, i.e. the value of the B-ﬁeld evalu-
ated at position i.
tion function is now
Z(J) = Tr
h
e−β ˆ
Hi
= Tr
h
e−β ˆ
H0+P
k Jk ˆφk
i
.
(21.5)
The point of this procedure is that we can now use the partition function
Z(J) to work out the thermal average ⟨ˆφi⟩t by diﬀerentiating Z(J) with
respect to Ji, and evaluating this at Ji = 0 and dividing by Z(J = 0).
That is to say
⟨ˆφi⟩t =
1
Z(J = 0)
∂Z(J)
∂Ji

Ji=0
=
Tr
h
ˆφie−β ˆ
H0
i
Z(J = 0)
,
(21.6)
as we had before.

198
Statistical physics: a crash course
But that’s not all. We can also use this philosophy to calculate cor-
relation functions. A useful question in statistical physics is to ask
about paired averages. For example, if we know the value of the spin at
position i, we’d like to know the probability that the spin at position j is
in the same state. The quantity we’re after is Gij = ⟨ˆφi ˆφj⟩t. This tells
us about the degree of correlation between spins in diﬀerent parts of the
system. If the spins are completely random, then Gij = 0, whereas if
they are completely aligned then Gij = 1
4 (because the spin eigenvalues
are ± 1
2). More interesting is when the correlation is partial and then
we might expect to ﬁnd Gij = 1
4 when |i −j| is small but Gij →0 as
|i −j| →∞. Sometimes it is interesting to look for deviations from
order, which is the situation when ⟨ˆφi⟩t ̸= 0. In that case, one can
examine the connected correlation function
Gcij = ⟨ˆφi ˆφj⟩t −⟨ˆφi⟩t⟨ˆφj⟩t.
(21.7)
Our notation makes these correlation functions look rather like cousins
of the Green’s functions that we’ve been considering previously. Some
example correlations we can ﬁnd include
Gij
=
⟨ˆφi ˆφj⟩t =
1
Z(0)
∂2Z(J)
∂Ji∂Jj

J=0
,
Gijk
=
⟨ˆφi ˆφj ˆφk⟩t =
1
Z(0)
∂3Z(J)
∂Ji∂Jj∂Jk

J=0
,
(21.8)
and in general
⟨ˆφi1...ˆφin⟩t =
1
Z(0)
∂nZ(J)
∂Ji1...∂Jin

J=0
.
(21.9)
The moral of this story is that by linearly coupling an operator-valued
ﬁeld ˆφi to a source Ji our partition function becomes a generating func-
tion. Once we’ve found the generating function we gain access to the
average value of ˆφi and a vast set of related quantities.
21.3
A look ahead
There is a close analogy between statistical physics, as exempliﬁed by
the magnet, and quantum ﬁeld theory. In fact everything we’re looked
at in this chapter has an analogue in ﬁeld theory. In going to ﬁeld theory
we’ll take a continuum limit, turning the lattice label i into the position
x so that ˆφi →ˆφ(x). The correlation functions of statistical physics
become propagators in ﬁeld theory and our goal is to ﬁnd these.
This is all very nice and certainly deserves further examination,3 but
3See Chapter 25.
the point of this chapter is to tell you that to reach the goal of ﬁnding
propagators for a ﬁeld theory we can use a generating functional Z[J]
similar to that used in statistical physics. The generating functional for
ﬁelds will allow us, through (functional) diﬀerentiation, to ﬁnd all of the

Exercises
199
Statistical physics
Field theory
(on a lattice)
(continuum)
Source
Ji
J(x)
Generating function
Z(J)
Z[J(x)]
Green’s functions
G(n)
i1...in = ⟨ˆφi1...ˆφin⟩t
G(n)(x1, ..., xn) = ⟨Ω|T ˆφ(x1)...ˆφ(xn)|Ω⟩
Diﬀerentiation recipe
G(n)
i1...in =
1
Z(J=0)
∂nZ(J)
∂Ji1...∂Jin

J=0
G(n)(x1, ..., xn) = 1
in
1
Z[J=0]
δnZ[J]
δJ(x1)...δJ(xn)

J=0
Order
⟨ˆφi⟩t ̸= 0
⟨Ω|ˆφ(x)|Ω⟩̸= 0
Table 21.1 Analogies between quantities in statistical physics on a lattice and those
in quantum ﬁeld theory.
propagators. In equations, this can be written as
G(x1, ..., xn)
=
⟨Ω|T ˆφ(x1)...ˆφ(xn)|Ω⟩
=
1
in
1
Z[J = 0]
δnZ[J]
δJ(x1)...δJ(xn)

J=0
.
(21.10)
One of the marvellous things about ﬁeld theory is that even when
we can’t solve a theory exactly, the generating functional Z[J] and the
Green’s functions G(n)(x1, ..., xn) can be expressed in terms of Feynman
diagrams. We will examine this in the next chapter.
Chapter summary
• Correlation functions in statistical mechanics can be extracted by
diﬀerentiating the partition function Z with respect to the source
term J. This approach carries over into quantum ﬁeld theory be-
cause Green’s functions can be extracted from a generating func-
tional Z[J].
Some analogies between statistical mechanics and
quantum ﬁeld theory are tabulated in Table 21.1.
Exercises
(21.1) Deﬁne the density operator as ˆρ =
e−β ˆ
H
Z
.
Us-
ing the rule that thermal averages are given by
⟨ˆA⟩t = Tr
h
ˆρ ˆA
i
, show that for a quantum oscillator,
with Hamiltonian ˆH = ωˆa†ˆa, the thermal average
of the number of excitations is given by
⟨ˆn⟩t = ⟨ˆa†ˆa⟩=
1
eβω −1.
(21.11)

200
Statistical physics: a crash course
(21.2) Response functions or susceptibilities are ubiquitous
in physics. Consider the forced quantum oscillator
L = 1
2m ˙x(t)2 −1
2mω2x(t)2 + f(t)x(t),
(21.12)
and take the deﬁnition of the response function to
be
⟨ψ(t)|ˆx(t)|ψ(t)⟩=
Z ∞
−∞
dt′χ(t −t′)f(t′).
(21.13)
(a) Using the interaction representation and treat-
ing ˆH′ = −f(t)ˆx(t) as the interaction part, show
that, to ﬁrst order in the force function fI(t), we
have
|ψI(t)⟩= |0⟩+ i
Z t
−∞
dt′fI(t′)ˆxI(t′)|0⟩.
(21.14)
(b) Using the previous result, show that
χ(t −t′) = iθ(t −t′)⟨0|
ˆ
ˆxI(t), ˆxI(t′)
˜
|0⟩.
(21.15)
(c) Using the fact that ˆxI(t) =
`
1
2mω
´ 1
2 (ˆae−iωt +
ˆa†eiωt), calculate the response function for the
quantum oscillator at temperature T = 0.
(d) Using the fact that at nonzero T we have
⟨ˆn⟩t = (eβω −1)−1, calculate the response func-
tion for the quantum oscillator at nonzero T.
(e) Compare the behaviour of the response function
with that of the correlation function S deﬁned as
S = 1
2⟨{ˆxI(t′), ˆxI(t)}⟩= 1
2⟨ˆxI(t′)ˆxI(t)+ˆxI(t)ˆxI(t′)⟩.
(21.16)
(21.3) Diﬀusion problems are common in thermodynam-
ics. Here we meet the diﬀusion equation. The dif-
fusion equation for the number density of particles
n(x, t) is
∂n(x, t)
∂t
−D∇2n(x, t) = 0.
(21.17)
Consider a point source at time t = 0 and po-
sition x = y, giving us the boundary conditions
G(x −y, t = 0) = δ(3)(x −y). Show that, for times
t > 0 the Green’s function for this equation is given
by
˜G(ω, q) =
1
−iω + D|q|2 ,
(21.18)
and identify the position of the pole in the complex
ω plane.
Hint: one way to do this problem which makes the
boundary conditions clear is to take a Laplace trans-
formation in time and a Fourier transform in space.
See Chaikin and Lubensky, Chapter 7, for a discus-
sion.

22
The generating functional
for ﬁelds
22.1 How to ﬁnd Green’s func-
tions
201
22.2 Linking things up with the
Gell-Mann–––Low theorem 203
22.3 How
to
calculate
Green’s
functions with diagrams 204
22.4 More facts about diagrams
206
Chapter summary
207
Exercises
208
Since the ﬁnished product cannot be served to the table as it
is, this isn’t so much a recipe, a method or a list of ingredi-
ents: it is a way of life, for if there is fresh stock made every
week, a reputation as a good cook will follow.
Len Deighton (1929– ), Action Cook Book
In this chapter we will present a general method of ﬁnding Green’s func-
tions based on the philosophy of generating functionals. We will explain
the meaning of the generating functional of quantum ﬁeld theory, Z[J],
and show that all Green’s functions may be extracted from it though
diﬀerentiation. This will allow us to relate together the S-matrix and
Green’s functions, linking two of the most important topics in quantum
ﬁeld theory.
Although this chapter contains a lot of formal material the ideas are
quite simple (and have much in common with the world of statistical
physics). In fact the rules that emerge for perturbation theory allow us to
write generating functionals and Green’s functions as sums of Feynman
diagrams, relating formal objects such as G(x, y) and Z[J] to a series of
cartoons.
22.1
How to ﬁnd Green’s functions
All of the information about a ﬁeld theory is available in the form of
Green’s functions.
Happily there’s an easy way to work out Green’s
functions which is based on a trick which enables us to lump all of the
information about a quantum ﬁeld theory in a single entity called the
generating functional or Z[J]. The generating functional is replete
with all that juicy data in a zipped up and compressed form and there’s
a simple procedure for extracting it.
This is identical to the case of
statistical physics, where all correlation functions were obtainable from
the partition function Z(J).
Just as in statistical physics we make use of the philosophy of sources.
In order to make a generating functional we’ll add a source function J(x)
to our Lagrangian density, which couples linearly1 with the ﬁeld φ(x):
1Note that by adding Jφ to L, we will
end up subtracting Jφ from H.
L[φ(x)]
→
L[φ(x)] + J(x)φ(x).
(22.1)
The reason why J(x) is referred to as a source is that in quantum ﬁeld
theory this term will generate particle excitations in the ﬁeld φ(x). By

202
The generating functional for ﬁelds
turning on a source we are, essentially, grabbing the ﬁeld and shaking
excitations out.
We now deﬁne the generating functional for quantum ﬁelds as
Z[J]
=
⟨Ω| ˆU(∞, −∞)|Ω⟩J,
(22.2)
where ˆU is the time-evolution operator for the full Hamiltonian ˆH of the
theory and the J subscript on the right-hand side reads ‘in the presence
of a source J’ when said out loud. We also have that ˆH|Ω⟩= 0. That
is |Ω⟩is the physical ground state of the system and the ground state
energy is deﬁned to be zero. Turning eqn 22.2 into words we see that
the generating functional Z[J] tells us the amplitude
Z[J] =

no particles
at x0 = ∞

no particles
at y0 = −∞

J
,
(22.3)
that is, in the presence of a source, we start with no particles and end
with no particles. We could call Z[J] the ‘no particle propagator’.
As we have deﬁned it, Z[J] allows two classes of process apart from the
rather trivial option of nothing happening at all: (i) those where a par-
ticle is created by the source J and later absorbed by J; (ii) those where
a particle spontaneously appears and then disappears. The latter class,
corresponding to Z[J = 0], is described by vacuum diagrams and needs
to be removed by normalization in order for the generating functional
to work. We therefore deﬁne a normalized generating functional:
Z[J] =
Z[J]
Z[J = 0].
(22.4)
This guarantees that Z[J = 0] = 1 or, in other words, that the amplitude
to start and end with no particles in the absence of sources is unity.
We now need to ﬁnd a way of calculating Z[J]. Writing this in terms
of the time-evolution operator as Z[J] = ⟨Ω| ˆU(∞, −∞)|Ω⟩J turns this
into a problem of ﬁnding ˆU(∞, −∞). A clue is provided by the way
in which we worked out the S-matrix in the last few chapters. In the
interaction picture, the time-evolution operator ˆUI(∞, −∞) is known as
the ˆS-operator and the Dyson expansion gives us a way to calculate
ˆS via ˆS = Te−i
R
d4x ˆ
HI.
Things aren’t so simple here though, since
ˆU(∞, −∞) in eqn 22.2 is deﬁned in the Heisenberg picture. The prob-
lem is that we don’t have free ﬁelds in the Heisenberg picture so we
can’t use Wick’s theorem to grind down long strings of operators (since
Wick’s theorem only works on free ﬁelds). However we can still write
a Dyson-like equation for the time evolution in terms of the Heisenberg
ﬁelds ˆφH. If we treat the source term like we would an interaction by
saying ˆHI = −J ˆφH, then the time-evolution operator is given by Dyson’s
expression U(∞, −∞) = Te−i
R
d4x ˆ
HI. Splitting this up in this way will
only generate processes involving the sources and not the vacuum dia-
grams, and so will result in the normalized generating functional. We
can therefore write
Z[J] = ⟨Ω|Tei
R
d4x J(x) ˆφH(x)|Ω⟩,
(22.5)

22.2
Linking things up with the Gell-Mann–––Low theorem
203
where the ﬁelds ˆφH(x) evolve according to the full Hamiltonian (rather
than a free Hamiltonian H0 used in the interaction picture) and we use
the interacting ground state |Ω⟩rather than the free ground state |0⟩.
Again we stress this is all still permissible, as long as we remember that
we wouldn’t be able to use Wick’s theorem. We expand Z[J] to obtain
Z[J] = 1 +
∞
X
n=1
in
n!
Z
d4x1...d4xn J(x1)...J(xn)⟨Ω|T ˆφH(x1)...ˆφH(xn)|Ω⟩.
(22.6)
Recalling that Green’s functions are deﬁned by
G(n)(x1, ..., xn) = ⟨Ω|T ˆφH(x1)...ˆφH(xn)|Ω⟩,
(22.7)
we notice that we can get access to them through a manipulation of the
generating function Z[J] by diﬀerentiating eqn 22.6 as follows:
G(n)(x1, ..., xn) = 1
in
δnZ[J]
δJ(x1)...δJ(xn)

J=0
.
(22.8)
That is, we get the Green’s functions (which contain all of the informa-
tion about a system) by diﬀerentiating a single functional Z[J]. This
method of doing quantum ﬁeld theory is illustrated in Fig. 22.1. In terms
of the unnormalized generating functional Z[J] we have the equivalent
rule
G(n)(x1, ..., xn) = 1
in
1
Z[J = 0]
δnZ[J]
δJ(x1)...δJ(xn)

J=0
,
(22.9)
which closely resembles the version from statistical physics.
Lagrangian
Generating
functional
Z[J]
Green’s
functions
Physical
predictions
add a source current J
diﬀerentiate
Fig. 22.1 Doing quantum ﬁeld theory
using generating functionals
We’ve done what we set out to do: we have found a single functional
that contains all of the information about the system, which can be used
to ﬁnd Green’s functions. All we need to do now is ﬁnd an automated
way of calculating Z[J] for any theory. It turns out that there are two
ways: the ﬁrst is by relating Z[J] to the S-matrix; the second uses
Feynman’s path integral. In this chapter we pursue the ﬁrst approach,
leaving the second for later (Chapter 24).
22.2
Linking things up with the
Gell-Mann–––Low theorem
We’ll examine the ﬁrst method: calculating Z[J] from the S-matrix. We
stress that the S-matrix is deﬁned for the interaction picture, so we’ll
need an equation for Z[J] given explicitly in terms of interaction picture
ﬁelds. We’ll pluck the answer from the air and then prove we’ve plucked
correctly. We propose a form for the normalized generating function
Z[J] which looks like:
Z[J] = Z[J]
Z[0] = ⟨0|Te−i
R
d4x[ ˆ
HI−J(x) ˆφI(x)]|0⟩
⟨0|Te−i
R
d4x ˆ
HI|0⟩
,
(22.10)

204
The generating functional for ﬁelds
which looks like a good guess because it has the form of Z[J] in eqn 22.5,
but this time writing the interaction Hamiltonian in the form ˆHI −J ˆφI.
This isn’t quite in terms of an expression for the S-matrix yet as we have
the source term in the numerator. Using the rule that you diﬀerentiate
n times, divide by in and set J = 0 to yield a Green’s functions, we have
the Green’s function
G(n)(x1, ..., xn) = ⟨0|T ˆφI(x1)...ˆφI(xn)e−i
R
d4x ˆ
HI|0⟩
⟨0|Te−i
R
d4x ˆ
HI|0⟩
,
(22.11)
and this can be written in terms of the ˆS-operator as
G(n)(x1, ..., xn) = ⟨0|T ˆφI(x1)...ˆφI(xn) ˆS|0⟩
⟨0| ˆS|0⟩
.
(22.12)
All that’s left is now is to prove that we’ve made a correct guess, so that
⟨Ω|T ˆφH(x1)...ˆφH(xn)|Ω⟩= ⟨0|T ˆφI(x1)...ˆφI(xn) ˆS|0⟩
⟨0| ˆS|0⟩
.
(22.13)
This result turns out to be true and is known as the Gell-Mann–––Low
theorem.2 Let’s be completely clear about what eqn 22.13 is saying.
2The proof of the theorem can be found
in the book by P. Coleman.
It was
ﬁrst proved by Gell-Mann and Low in
1951. Murray Gell-Mann (1929– ) re-
ceived the 1969 Nobel Prize in Physics
and, borrowing a term from Finnegans
Wake, gave the name to ‘quarks’. Fran-
cis Low (1921–2007) worked with Gell-
Mann at Princeton, and spent most of
his later career at MIT.
First examine the left-hand side:
• |Ω⟩is the interacting ground state, deﬁned by the full Hamiltonian
as ˆH|Ω⟩= 0. ˆφH(x) are Heisenberg ﬁeld operators. They time-
evolve according to the full Hamiltonian ˆH.
Next consider the right-hand side:
• Since we’ve turned oﬀthe source term now, the Hamiltonian can
be split up into free (solvable) and interacting (usually unsolvable)
parts ˆH = ˆH0 + ˆH′.
The right-hand side is therefore written
in terms of the interaction picture. |0⟩is the free ground state,
deﬁned such that ˆH0|0⟩= 0.
Quantities ˆφI(x) are interaction
picture ﬁeld operators. These time-evolve according to the free
part of the Hamiltonian ˆH0.
22.3
How to calculate Green’s functions
with diagrams
We’ve seen how to get a Green’s functions from the generating func-
tional of a theory. It’s simply a matter of diﬀerentiating. It’s natural to
be curious about what happens if we try to directly relate the Green’s
functions to Feynman diagrams. The Gell-Mann–Low theorem is a quo-
tient:
G(n)(x1, ..., xn) = ⟨0|T ˆφI(x1)...ˆφI(xn) ˆS|0⟩
⟨0| ˆS|0⟩
.
(22.14)

22.3
How to calculate Green’s functions with diagrams
205
Presumably there might be some cancellation between the Feynman di-
agrams thrown up in the numerator against those coming from the de-
nominator. It turns out that this cancellation does indeed take place
and leaves us with a wonderfully simple result:
G(n) = ⟨Ω|T ˆφH(x1)...ˆφH(xn)|Ω⟩=
X  All connected diagrams
with n external lines

.
(22.15)
Recall from Section 19.3 (see particu-
larly Fig. 19.4) that a connected dia-
gram is made up of a single piece of
linked lines and loops. A disconnected
diagram is composed of a number (> 1)
of separate pieces.
Why is this? If we start with the Gell-Mann–Low formula (eqn 22.14),
then the numerator factorizes into
⟨0|T ˆφI(x1)...ˆφI(xn) ˆS|0⟩=
X  Connected diagrams
with n external lines

×exp
X 
Connected
vacuum diagrams

,
and the denominator is simply ⟨0| ˆS|0⟩= exp
P 
Connected
vacuum diagrams

,
so we are able to show that
G(n)(x1, ..., xn)
=
⟨Ω|T ˆφH(x1)...ˆφH(xn)|Ω⟩
=
P 
Connected diagrams
with n external lines

×
✭✭✭✭✭✭✭✭✭✭✭✭✭✭✭
exp
P 
Connected
vacuum diagrams

✭✭✭✭✭✭✭✭✭✭✭✭✭✭✭
exp
P 
Connected
vacuum diagrams

,
and that proves eqn 22.15, the most important result of this chapter.
Example 22.1
Let’s see how this unfolds for the single-particle propagator G(2)(x1, x2). A typical
term will result in a disconnected diagram made up of a connected diagram with
external lines and several connected vacuum diagrams. The value of the full diagram
is the product of all of the connected diagrams. We label each connected vacuum
diagram by some number i. There may be more than one copy of each connected
vacuum diagram: we’ll call the number of copies ni. Call the value of a connected
vacuum diagram Vi. Put all of this together and write the value of a full, disconnected
diagram, representing one particular contraction, as
„ Connected diagram
with external lines
«
×
Y
i
1
ni!(Vi)ni,
(22.16)
where we need a factor
1
ni! because we can interchange the ni copies of the connected
diagram i. The sum of all diagrams is then given by
X
Connected diagrams
with external lines
X
(all {ni})
„ Connected diagram
with external lines
«
×
Y
i
1
ni!(Vi)ni, (22.17)
where, to generate all possible disconnected diagrams, we have summed over all
possible connected diagrams with external lines and over all ordered sets of integers
{ni}. To break this down we may factorize out the sum over connected diagrams
with external lines:
»X „ Connected diagram
with external lines
«–
×
X
(all{ni})
Y
i
1
ni!(Vi)ni.
(22.18)

206
The generating functional for ﬁelds
Then the rest of the expression may be factorized thus:
»X „ Connected diagram
with external lines
«–
×
0
@X
n1
1
n1!(V1)n1
1
A
0
@X
n2
1
n2!(V2)n2
1
A ...
=
»X „ Connected diagram
with external lines
«–
×
Y
i
0
@X
ni
1
ni!(Vi)ni
1
A
=
»X „ Connected diagram
with external lines
«–
×
Y
i
eVi
=
»X „ Connected diagram
with external lines
«–
× e
P
i Vi.
(22.19)
Now, as long as we know the Feynman rules for a theory, we can for-
get about the S-matrix and the interaction picture. We’ve reduced the
whole perturbation theory to one expression. To get the n-point Green’s
function we just sum all Feynman diagrams with n external lines. Amaz-
ingly simple.
22.4
More facts about diagrams
We’ve seen that G(2)(x1, x2), which is the amplitude for a particle to
start at x2 and propagate to x1, may be written as a sum of diagrams
describing all the ways in which a particle can start at x2 and ﬁnish up at
x1. We ask next, is there a way of expressing Z[J] in terms of diagrams?
Since Z[J] describes a situation where we have sources (and sinks) of
particles, but that we start at t = −∞with no particles and end up at
t = ∞with no particles, we might expect to be able to express Z[J]
as a sum of diagrams describing ways of starting and ending without
particles. This is indeed the case and we have
Z[J]
=
⟨Ω(∞)|Ω(−∞)⟩J = ⟨0| ˆS|0⟩J
(22.20)
=
X 
Disconnected vacuum and
source-to-source diagrams

.
The sum here is made up of all of the ways of starting and ending with no
particles, which corresponds to the sum of all disconnected vacuum and
source-to-source diagrams, of which there are potentially a very great
number. Fortunately there’s an even simpler way of stating this result,
which is:
Z[J] = e
P
0
@
Connected vacuum and
source-to-source diagrams
1
A
.
(22.21)
This simpliﬁcation is based on the linked-cluster theorem,3 which
3The
linked-cluster
theorem,
which
equates
the
right-hand
sides
of
eqns 22.20 and 22.21, can be stated
more memorably as
X „
all
diagrams
«
= e
P
 connected
diagrams
!
. we now show follows from more combinatoric arguments, very similar
to those used earlier. We call a vacuum or source-to-source diagram

22.4
More facts about diagrams
207
Vi. Now, in general, the amplitude for a general, possibly disconnected
diagram is given by Q
i
(Vi)ni
ni!
and the generating functional is given by
the sum
Z =
Y
i
∞
X
ni=0
(Vi)ni
ni!
= e
P
i Vi,
(22.22)
proving the theorem. (Notice that all of the leg work was done in Ex-
ample 22.1!)
Example 22.2
We can use the linked-cluster theorem to simplify the expression for the normalized
generating functional Z[J] = Z[J]/Z[0]. If we set J = 0, we turn oﬀthe sources and
we have
Z[0] = e
P(Connected vacuum diagrams).
(22.23)
We write
Z[J] = e
P
 Connected source-
to-source diagrams
!
e
P(Connected vacuum diagrams).
(22.24)
The normalized generating functional is then
Z[J] = Z[J]
Z[0] = e
P
 Connected source-
to-source diagrams
!
,
(22.25)
allowing us to give more meaning to the normalized functional Z[J].
Notice that we derive most of the results on the way so that we never have
to use them. (One assumes this is a bit like learning knife ﬁghting.) The
end result for a particular theory is that the propagator is represented
by a sum of connected diagrams.
Of course, the details of what the
diagams look like come from a one-oﬀ, nitty-gritty calculation. We’ll see
a few more examples of these in the following chapters. Finally we’ll
need to do the integrals. This will lead into even stranger territory.
Chapter summary
• Green’s functions are derived from generating functionals. This
approach shows that G(n) is simply the sum of all diagrams with
n external lines.
• The generating functional Z[J] is the exponential of the sum of all
connected source-to-source and vacuum diagrams.

208
The generating functional for ﬁelds
Exercises
(22.1) Consider the forced quantum oscillator
L = 1
2m ˙x(t)2 −1
2mω2x(t)2 + f(t)x(t),
(22.26)
where f(t) is a constant force f0 which acts only for
a ﬁnite length of time: 0 ≤t ≤T. We can use the
S-matrix to work out the generating functional for
this theory, by treating f(t)x(t) as an interaction
term.
(a) Explain why we may write ⟨0| ˆS|0⟩= Z[f(t)] for
this theory.
(b) By expanding the
ˆS-matrix and show that
Z[f(t)] = e(Dumbbell), where (Dumbbell) is the
Feynman diagram shown in Fig. 22.2(a) whose am-
plitude is
(Dumbbell)= (−i)2
2
Z
dtdt′f(t)⟨0|T ˆx(t)ˆx(t′)|0⟩f(t′).
(22.27)
(c) Working in frequency space, show that the
Dumbbell may be written
(Dumbbell) = −1
2m
Z dν
2π
˜f(−ν)
i
ν2 −ω2 + iǫ
˜f(ν),
(22.28)
where ˜f(ν) = if0
ν
`
1 −eiνT ´
.
(d) What does the quantity |Z[f(t)]|2 describe
physically? Using the identity
1
E+iǫ = P 1
E −iπδ(E),
where P is the principal part (see Appendix B),
show that this quantity is given by
|Z[f(t)]|2 = exp
„
−2f 2
0
mω3 sin2 ωT
2
«
.
(22.29)
(a)
(b)
f(t)
f(t′)
J(x)
J(y)
Fig. 22.2 (a) Dumbbell diagram for Exercise 22.1. (b)
Dumbbell diagram for Exercise 22.2.
(22.2) Now consider scalar ﬁeld theory in the presence of
a source ﬁeld
L = 1
2[∂µφ(x)]2 −m2
2 φ(x)2 + gJ(x)φ(x). (22.30)
Notice the resemblance between this theory and the
forced oscillator.
(a) Show that the amplitude for the Dumbbell dia-
gram in this theory [Fig. 22.2(b)] is given by
(−ig)2
2
Z
d4xd4y
Z
d4p
(2π)4 J(x) ie−ip·(x−y)
p2 −m2 + iǫJ(y).
(22.31)
(b) Consider a source ﬁeld describing two static
sources localized at x1 and x2:
J(x) = δ(3)(x −x1) + δ(3)(x −x2).
(22.32)
Show that the interaction between the two sources
can be written
(Dumbbell)12 ∝−ig2
„Z
dx0dy0 dp0
2π e−ip0(x0−y0)
«
×
Z
d3p
(2π)3
eip·(x1−x2)
p2 −m2 + iǫ,
where the self-interaction of the sources is ignored.
(c) Show further that
(Dumbbell)12 ∝ig2
Z
dx0
Z
d3p
(2π)3
eip·(x1−x2)
p2 + m2 .
(22.33)
(d) Finally, using the fact that e−iET = ⟨0| ˆS|0⟩=
e(Dumbbell), where T is the time over which the
sources act (see Exercise 43.5 if in doubt), show
that the interaction energy E between two spatially
separate sources is given by
E ∝−g2
Z
d3p
(2π)3
eip·(x1−x2)
p2 + m2 .
(22.34)
The important feature here is that the energy is neg-
ative. The two sources therefore attract each other
by exchanging a virtual particle of mass m.
*(e) Compare the previous result to the correspond-
ing one for a (spin-1) vector ﬁeld dealt with in
Chapter 24. Is the interaction attractive or repul-
sive? A graviton is a spin-2 object. What would
you expect for the interaction of sources in a gravi-
tational ﬁeld. See Feynman’s Lectures on Gravita-
tion, Chapter 3, for a discussion.

Part VI
Path integrals
In this part we use a reformulation of quantum ﬁeld theory which is due
to Richard Feynman, the so-called path integral approach. Quantities
of interest in quantum ﬁeld theory, such as the propagator, can be cal-
culated using a functional integral, which is an integral over all possible
paths of the particle from one spacetime point to another.
• We describe Feynman’s path integral approach in Chapter 23 and
demonstrate how to evaluate certain Gaussian integrals, using the
simple harmonic oscillator as an example.
• These ideas are then applied in Chapter 24 to working out func-
tional integrals and thus calculating Green’s functions.
• There is a rather subtle connection between quantum ﬁeld theory
and statistical physics, and this is explored in Chapter 25, where
the concepts of imaginary time and the Wick rotation are intro-
duced.
• An important concept is symmetry breaking and this is treated
in Chapter 26.
This idea is introduced in the context of phase
transitions and we show how to describe symmetry breaking with
a Lagrangian. Breaking a continuous symmetry leads to Goldstone
modes and we also show the consequence of symmetry breaking in
a gauge theory, thereby introducing the Higgs mechanism.
• Chapter 27 describes coherent states which are eigenstates of the
annihilation operator. These states do not have a deﬁnite number
of particles but they can be made to have a well-deﬁned phase and
are used to describe laser ﬁelds and superﬂuids.
• We introduce a new type of mathematical object in Chapter 28:
Grassmann numbers. These anticommute and can be used to de-
scribe fermions. We show how to construct a fermion path integral
using Grassmann numbers.

23
Path integrals: I said to
him, ‘You’re crazy’
23.1 How to do quantum mechan-
ics using path integrals
210
23.2 The Gaussian integral
213
23.3 The propagator for the sim-
ple harmonic oscillator
217
Chapter summary
219
Exercises
220
Thirty-one years ago Dick Feynman told me about his ‘sum
over histories’ version of quantum mechanics. ‘The electron
does anything it likes’, he said. ‘It goes in any direction at
any speed, forward and backward in time, however it likes,
and then you add up the amplitudes and it gives you the
wave-function.’ I said to him, ‘You’re crazy’. But he wasn’t.
F. J. Dyson (1923– )
In this chapter, we introduce the path integral formulation of quantum
mechanics.
23.1
How to do quantum mechanics using
path integrals
Quantum mechanics is all about probability amplitudes. One way to do
quantum mechanics is to ask a question like ‘what’s the amplitude that
a particle starts at point qa at time ta and ends up at point qb at time
tb?’ The amplitude for the process is, as we’ve discussed, known as the
propagator. Previously, we found the propagator using a conventional
approach to quantum mechanics espoused by Schr¨odinger and Heisen-
berg. In this chapter we’ll ﬁnd the propagator using Richard Feynman’s
alternative approach. This will then be applied to quantum ﬁelds.
A
B
eiS1/¯h
eiS2/¯h
eiS3/¯h
eiS4/¯h
Fig. 23.1
Some possible paths with
their amplitudes. A particle will take
all of them.
The propagator depends on the trajectory that, for whatever reason,
the particle takes in getting from A to B. Some possible paths are shown
in Fig. 23.1. In classical mechanics, we’d know what to do: write down a
Lagrangian L; plug that Lagrangian into the Euler–Lagrange equation
and thus generate the equations of motion, which we’d then (at least
try) to solve. The trajectory that we’d calculate would be the one that
minimizes the action
S =
Z tB
tA
dt L[q(t)],
(23.1)
just as we discussed in Chapter 1.
How does this work in quantum mechanics? Richard Feynman’s sug-
gestion was that in getting from A to B a particle will take every pos-
sible trajectory. We’ll say that again. A particle will take every single
possible trajectory, forward and backward in time, zig-zagging, loop-
ing, whatever. To get the quantum amplitude Feynman says that each

23.1
How to do quantum mechanics using path integrals
211
trajectory contributes a complex factor eiS/ℏ, where S is the action de-
scribing that trajectory, and we add up the contributions to get the
amplitude. Crazy!
Let’s see if we can justify this approach of getting the amplitude
by summing over eiS/ℏfactors associated with each trajectory.
This
is also helpfully known as the sum over histories approach because
the present wave function is obtained by summing over all possible past
trajectories. The propagator amplitude G to get from spacetime point
A [≡(ta, qa)] to point B [≡(tb, qb)] may be written as ⟨qb| ˆU(tb, ta)|qa⟩,
where |q⟩is an eigenstate of position and ˆU(tb, ta) is the time-evolution
operator ˆU(tb, ta) = e−i ˆ
H(tb−ta) (where we’ve returned to our usual units
employing ℏ= 1). We have then
G = ⟨qb|e−i ˆ
H(tb−ta)|qa⟩.
(23.2)
To deal with this we split the trajectory into N inﬁnitesimal steps of
length ∆t as shown in Fig. 23.2. This is known as time-slicing. The
fact that ˆU(t) is a unitary operator allows us to time-slice. Unitarity
lets us say ˆU(tb −ta) = ˆU(tb −tx) ˆU(tx −ta) and so time-slicing ˆU(t) =
e−i ˆ
H(tb−ta) in this way yields
G
=
⟨qb|(e−i ˆ
H∆t)N|qa⟩
=
⟨qb|e−i ˆ
H∆t . . . e−i ˆ
H∆t . . . e−i ˆ
H∆t|qa⟩.
(23.3)
Notice that each of these mini time-evolution operators moves the par-
ticle for time ∆t between two closely spaced positions qn and qn+1 (say)
as shown in Fig. 23.2.
q
t
ta
tb
∆t
qa
q1
q2
q3
q4
q5
q6
q7
qb
Fig. 23.2
Time slicing a trajectory
from (ta, qa) to (tb, qb).
Example 23.1
Remember that in quantum mechanics we often make use of the completeness of a
set of states. For the position states qn, we have
R
dqn|qn⟩⟨qn| = 1. The trick to
get any further with G is to insert a ‘resolution of the identity’ (also known as a fat
unity) in between each mini time-evolution operator. We’ll need N −1 fat unities to
do this. Inserting these in (working from right to left) we obtain
G
=
⟨qb|e−i ˆ
H∆t
»Z
dqN−1|qN−1⟩⟨qN−1|
–
e−i ˆ
H∆t . . .
. . . e−i ˆ
H∆t
»Z
dqn+1|qn+1⟩⟨qn+1|
–
e−i ˆ
H∆t
»Z
dqn|qn⟩⟨qn|
–
e−i ˆ
H∆t . . .
. . . e−i ˆ
H∆t
»Z
dq1|q1⟩⟨q1|
–
e−i ˆ
H∆t|qa⟩.
(23.4)
Notice what’s happening here. In each resolution we’re varying the qn’s indepen-
dently and making new trajectories as we do so. In Fig. 23.2 this corresponds to
moving the blobs along each line independently to some new position to make a new
trajectory. We take a snapshot of the trajectory and then move the blobs again. The
path integral is done by adding up all of the diﬀerent trajectories that you make. In
this way we add all of the possible paths between qa and qb. A little rearranging
reveals that we have
G =
Z
dq1dq2 . . . dqN−1 ⟨qb|e−i ˆ
H∆t|qN−1⟩. . . ⟨qn+1|e−i ˆ
H∆t|qn⟩. . . ⟨q1|e−i ˆ
H∆t|qa⟩.
(23.5)

212
Path integrals: I said to him, ‘You’re crazy’
Between the two amplitudes at the end we have a string of mini-propagators Gn =
⟨qn+1|e−i ˆ
H∆t|qn⟩. Let’s look at one of the typical mini-propagators:
Gn
=
⟨qn+1|e−i ˆ
H∆t|qn⟩
=
⟨qn+1|e
−i
»
ˆ
p2
2m + ˆ
V (q)
–
∆t
|qn⟩,
(23.6)
where we’ve written ˆH = ˆp2/2m + ˆV (q). To evaluate the mini-propagator we need
to replace the operators by eigenvalues. The potential term acts1 on the position
1There’s
some
sleight
of
hand
here.
What
we’re
saying
is
that
e−i( ˆ
T + ˆV )∆t
=
e−i ˆ
T ∆te−i ˆV ∆t
+
O(∆t2),
and
since
∆t
is
assumed
small, all is well.
In general if two
operators have a commutation relation
[ ˆ
A, ˆB] = ˆC then e ˆ
A+ ˆ
B = e ˆ
Ae ˆ
Be−ˆ
C/2.
Here we take ˆ
A = −i ˆT∆t, ˆB = −i ˆV ∆t
and so
ˆC
= −(∆t)2[ ˆT, ˆV ] and thus
e−ˆ
C/2 ≈1 + O[(∆t)2].
state as follows: e−i ˆV (q)∆t|qn⟩= |qn⟩e−iV (qn)∆t [we’ve moved the state |qn⟩past
the potential operator ˆV (q), replacing the operator with its eigenvalue V (qn)]. We
have then
Gn = ⟨qn+1|e−i ˆ
p2
2m ∆t|qn⟩e−iV (qn)∆t.
(23.7)
Next we need to know how ˆp acts on a position state. Since |qn⟩isn’t an eigenstate
of ˆp (since [ˆq, ˆp] = i) we just expand |qn⟩in terms of momentum eigenstates in the
usual way |qn⟩=
R
dp
(2π)|p⟩⟨p|qn⟩=
R
dp|p⟩e−ipqn/(2π)
1
2 . Replacing |qn⟩in this way
gives us
Gn
=
Z
dp
(2π)
1
2
⟨qn+1|e−i ˆ
p2
2m ∆t|p⟩e−ipqne−iV (qn)∆t
=
Z
dp
(2π)
1
2
⟨qn+1|p⟩e−i p2
2m ∆te−ipqne−iV (qn)∆t
=
Z
dp
(2π) eipqn+1e−i p2
2m ∆te−ipqne−iV (qn)∆t
=
Z
dp
(2π) e−i p2
2m ∆t+ip(qn+1−qn)e−iV (qn)∆t.
(23.8)
We’ve now removed all of the operators in favour of numbers and are left with an
integral over p to do. This is a Gaussian integral which can be done exactly (see the
next section2). The result is
2The
result
we
need
is
that
R ∞
−∞dx e−ax2
2
+bx
=
q
2π
a e
b2
2a , with
a = i∆t/m and b = i(qn+1 −qn).
Gn =
„ −im
2π∆t
« 1
2
e
im
2
(qn+1−qn)2
∆t
e−iV (qn)∆t.
(23.9)
Inserting back into eqn 23.5, and writing the factor (−im/2π∆t)
1
2 = ξ−1, we ﬁnd that
the propagator amplitude G can be written as a product of these mini-propagators
G =
N−1
Y
n=1
Z
dqn
ξ e
im
2
(qn+1−qn)2
(∆t)2
∆te−iV (qn)∆t.
(23.10)
Lastly we take the limit N →∞, ∆t →0. This corresponds to upping the contrast
on our grid in Fig. 23.2, turning the jagged trajectories into smooth curves. Taking
these limits turns (qn+1−qn)2
(∆t)2
into ˙q2 and P
n ∆t →
R
dt.
We obtain our answer for the total amplitude
G =
Z
D [q(t)] e
i
R
dt
h
m ˙q2
2
−V (q)
i
,
(23.11)
where we deﬁne the integration measure
R
D[q(t)] as
Z
D [q(t)] = lim
N→∞
N−1
Y
n=1
Z dqn
ξ .
(23.12)
Equation 23.11 is an example of a functional integral, a sum over all
possible trajectories encoded as a vast multiple integral over all time-
sliced coordinates qn, and in fact as we let N →∞the functional integral

23.2
The Gaussian integral
213
becomes an inﬁnitely multiple integral. The curly D hides the guts of
the functional integral: the instruction to integrate over all possible
trajectories.
Noticing that the Lagrangian is L = m ˙q2/2 −V (q) and restoring ℏ,
we recognize
G =
Z
D[q(t)] e
i
ℏ
R
dt L[q(t)] =
Z
D[q(t)] eiS/ℏ,
(23.13)
which is what we set out to justify. To recap: we calculate an amplitude
by adding up contributions eiS/ℏfrom each possible trajectory, of which
there are an inﬁnite number.3
3Doing the sum relies on the shady
business of deﬁning an integration mea-
sure
R
D[q(t)] which doesn’t look very
well behaved in the limit N
→∞.
Although this is a legitimate concern,
we’ll see that we’ll always be able to
manoeuvre ourselves out of the trouble
caused by
R
D[q(t)].
A
B
Fig. 23.3 The classical path is shown
as a solid line, alternative paths allowed
by quantum mechanics are shown as
dashed lines.
What does the path integral tell us about quantum amplitudes? Al-
though we add up all of the paths, each path is weighted by a phase
factor, and we expect that there’s some degree of cancellation between
the contribution from diﬀerent paths. Also, as we approach the classical
world (mathematically achieved by sending ℏ→0) we should recover
Lagrange’s result that the trajectory the particle takes is the one for
which the action is stationary, i.e. δS/δq(t) = 0. This can be under-
stood by considering a particular trajectory with action S, typically
large compared to ℏ, so that its contribution is weighted by eiS/ℏ(the
phase S/ℏcan be anywhere on the Argand clock). A close neighbouring
trajectory will have a diﬀerent action S′, also large compared to ℏ, and
if δS = (S′ −S) ≫ℏit is likely to have a completely diﬀerent phase.
It doesn’t take much imagination to realize that a bunch of such tra-
jectories will all have essentially random phases and will cancel, giving
no net contribution.
In contrast, trajectories suﬃciently close to the
stationary value that Lagrange would have predicted (some are shown
in Fig. 23.3) all have similar actions and thus similar phases, giving a
strong in-phase contribution. Therefore, the sum is dominated by the
stationary result. The quantum mechanical eﬀects, known as quantum
corrections or quantum ﬂuctuations, are given, for the most part, by
the trajectories close to the classical path of least action.4
4An approach to path integral calcu-
lations where we ﬁnd the classical tra-
jectory and then ﬁnd the approximate
form of the quantum corrections, is
known as the stationary phase approx-
imation and is used in Chapter 50.
23.2
The Gaussian integral
We’ve outlined a new way to think about quantum mechanics, but it’s
not worth much if we can’t actually do a path integral. In this section
we’ll build up a mathematical armoury to deal with evaluating path
integrals. It turns out that it’s all related to a simple Gaussian integral
and we’ll build it up via a series of successive steps of generalization.
Step 0: The basic Gaussian integral around which this section is based
is given by the standard integral5
5Write I
=
R ∞
−∞dx e−x2 and I
=
R ∞
−∞dy e−y2, then multiply the two
equivalent integrals together to obtain
I2 =
Z ∞
x=−∞
Z ∞
y=−∞
dxdy e−(x2+y2)
=
Z 2π
θ=0
dθ
Z ∞
r=0
dr re−r2,
where
in
the
last
equality
we’ve
switched to polar coordinates.
These
are standard integrals, yielding I2 =
(2π) × 1
2 = π and hence I = √π.
Z ∞
−∞
dx e−x2 = √π.
(23.14)
Step 1: The ﬁrst generalization involves rescaling x2 →ax2/2 to get
Z ∞
−∞
dx e−ax2
2
=
r
2π
a .
(23.15)

214
Path integrals: I said to him, ‘You’re crazy’
Step 2: A very important variation, which turns out to be the useful
one for evaluating path integrals, is the integral:
Z ∞
−∞
dx e−ax2
2 +bx =
r
2π
a e
b2
2a ,
(23.16)
which is also not too diﬃcult to prove, as shown below.
Example 23.2
Equation 23.16 is proved by ‘completing the square’.
The maximum of P(x) =
−ax2
2
+ bx is Pmax = b2
2a at x = b
a. We rewrite
P(x) −Pmax
=
−ax2
2
+ bx −b2
2a
=
−a
2
„
x −b
a
«2
.
(23.17)
If we now change variables y = (x −b
a) so that dx = dy, then eqn 23.16 becomes
Z ∞
−∞
dx e−ax2
2
+bx
=
Z ∞
−∞
dx e−a
2 (x−b
a )2+ b2
2a
=
e
b2
2a
Z ∞
∞
dy e−ay2
2
=
r
2π
a e
b2
2a .
(23.18)
There are some things to note about this equation for later use. Most
of them are obvious, but important.
• Since x was our integration variable it doesn’t appear on the right-
hand side. We say that we have ‘integrated out’ x.
• We are left with an expression which has the structure
Z
dx e−x a
2 x+bx =
r
2π
a e
1
2(b 1
a b),
(23.19)
where on the left-hand side a touches two x’s and b touches one,
while on the right-hand side the two b’s are linked by the 1
a.
We’ll see this structure again a little later.
Step 3: In order to be able to integrate over functions like q(t), we
have to consider an intermediate step: integrating over vectors. This is
because a function can be thought of as an inﬁnite dimensional vector.
We start with an N-dimensional vector x, with components6 xj and
6For simplicity here we label ordinary
vectors in Euclidean space with indices
in the down position and assume a sum
over repeated indices.
We will return
to our usual vector notation in the next
chapter.
consider the integral
I =
Z
dx1dx2 . . . dxN e−1
2 xiAijxj =
Z
dNx e−1
2 xTAx,
(23.20)
where A is an N × N, real, symmetric matrix.7
7This is nothing to worry about as
we’re used to doing multiple dimen-
sional integrals over volumes or sur-
faces. Remember that the tricky thing
about these integrals is the dependence
of one component on another. Things
are a lot simpler if we can separate the
components out. That’s exactly what
we’re going to do here.

23.2
The Gaussian integral
215
Example 23.3
To evaluate the integral we make an orthogonal transformation A = OTDO, where
we choose our matrices O in such a way as to make D a diagonal matrix. Our integral
is then
I =
Z
dNx e−1
2 xTOTDOx.
(23.21)
Next make the transformation Ox = y, and since the matrix O is orthogonal, the
Jacobian of this transformation is 1 so
R
dNx =
R
dNy, and we get
I =
Z
dNy e−1
2 yTDy.
(23.22)
As D is diagonal, we have yTDy = P
i Dii(yi)2, which allows us to separate out our
multidimensional integral.
Z
dNy e−1
2 yTDy
=
Z
dy1 e−D11(y1)2
2
Z
dy2 e−D22(y2)2
2
. . .
Z
dyN e−DNN (yN )2
2
=
„ 2π
D11
« 1
2 „ 2π
D22
« 1
2
. . .
„
2π
DNN
« 1
2
=
N
Y
i
„ 2π
Dii
« 1
2
=
„(2π)N
detD
« 1
2
=
„(2π)N
detA
« 1
2
,
(23.23)
where we’ve used Q
i Dii = det D = det A.
Step 4: We’re ﬁnally in a position to generalize our important result,
eqn 23.19, to the multidimensional case. We want to evaluate the inte-
gral
K =
Z
dNx e−1
2 xTAx+bTx,
(23.24)
where b is an N-dimensional vector.
Example 23.4
By analogy with Example 23.2 we consider P(x) = −1
2 xTAx + bTx. We can easily
ﬁnd its minimum (remembering that Aij is symmetric and using ∂xi
∂xj = δij):
∂P
∂xk
=
−1
2δikAijxj −1
2xiAijδjk + bjδjk
=
−Akjxj + bk
=
−(Ax −b)k = 0.
(23.25)
This gives us a minimum of Pmin = bTA−1b
2
at x = A−1b. So again, just as in
eqn 23.17, we can write
P(x) −Pmin
=
−1
2xTAx + bTx −bTA−1b
2
=
−1
2(x −A−1b)TA(x −A−1b).
(23.26)
This all looks familiar, of course.
Again, we can perform a transformation and
integrate over our new variable y = (x −A−1b). We’ll obtain a factor of the form
e
1
2 bTA−1b emerging at the front.

216
Path integrals: I said to him, ‘You’re crazy’
The previous example leads us to
K =
Z
dNx e−1
2 xTAx+bTx =
(2π)N
detA
 1
2
e
1
2 bTA−1b.
(23.27)
Again we note that we’ve integrated out x, and while A touches two x’s
on the left-hand side, A−1 takes b to bT.
Step 5: Functions can be perfectly well described as inﬁnite dimensional
vectors, and so our next generalization is the functional integral
Q =
Z
D[f(x)] e−1
2
R
dxdy f(x)A(x,y)f(y)+
R
dx b(x)f(x).
(23.28)
To deal with this we’ll revert to the discrete world by turning the func-
tions into N-dimensional vectors; we’ll write down the solution and then
upgrade the result back into the world of functions.
Example 23.5
We convert A(x, y) →A, f(x) →x and so on to obtain
Z
D[f(x)] e−1
2
R
dxdy f(x)A(x,y)f(y)+
R
dx b(x)f(x) →
Z
dNx e−1
2 xTAx+bTx
=
„(2π)N
det A
« 1
2
e
1
2 bTA−1b
(and then returning to functions)
→
„
(2π)N
det A(x, y)
«1/2
e
1
2
R
dxdy b(x)A−1(x,y)b(y).
(23.29)
We have the ﬁnal result that
Q
=
Z
D[f(x)] e−1
2
R
dxdy f(x)A(x,y)f(y)+
R
dx b(x)f(x)
=
B [detA(x, y)]−1/2 e
1
2
R
dxdy b(x)A−1(x,y)b(y),
(23.30)
where we’ve introduced a constant B to soak up the factor of (2π)N
(which is a potential embarrassment in the limit N →∞).
The inverse function A−1(x, y) is that function that satisﬁes
Z
dz A(x, z) A−1(z, y) = δ(x −y).
(23.31)
That’s just another way of saying that A−1(x, y) is the Green’s function
for the operator A(x, y). It’s been a long slog, but we can now do one sort
of functional integral,8 as long as we can ﬁnd the determinant det A(x, y)
8The
key
path
integral
equation
(eqn 23.30), which we’ll use most fre-
quently in later chapters, may be sum-
marized as
Z
D[f] e−1
2
R
fAf+
R
bf = Ne
1
2
R
bA−1b,
where N is a constant.
and ﬁnd the Green’s function of A(x, y). In the next section, at long
last, we’ll ﬁnally do a functional integral!

23.3
The propagator for the simple harmonic oscillator
217
23.3
The propagator for the simple
harmonic oscillator
An illuminating example of the path integral approach is the simple
harmonic oscillator. We’re going to ﬁnd the probability amplitude that
if, at ta = 0, we put a particle in the minimum of a harmonic potential
located at q = 0, when we come back at time tb = T the particle will
still be at q = 0, where we originally put it. We need, therefore, to
compute the propagator
G(qb = 0, tb = T, qa = 0, ta = 0)
=
 Particle ends at
qb = 0, tb = T

Particle starts at
qa = 0, ta = 0

=
Z qb=0,tb=T
qa=0,ta=0
D[q(t)] ei
R T
0 dt L[q(t)].
(23.32)
Let’s do the integral. The Lagrangian for a simple harmonic oscillator
is L = m ˙q(t)2
2
−mω2
0q(t)2
2
, where ω0 = (K/m)1/2. The path integral is
therefore
G =
Z
D[q(t)] e
im
2
R
dt
h
(
dq(t)
dt )
2−ω2
0q(t)2i
.
(23.33)
Unfortunately this is not in the form of the one integral we know how
to do (that one looks like
R
Dq e−1
2 q ˆ
Aq+bq). Can we massage the simple-
harmonic oscillator into this form? We can and the trick, which is based
on integrating by parts, is one that we’ll use repeatedly for path integrals.
Example 23.6
The action in the argument of the exponential part of the path integral is
S =
Z
dt
» m ˙q(t)2
2
−mω2
0q(t)2
2
–
,
(23.34)
and the trick just involves integrating the ﬁrst term, which represents the kinetic
energy, by parts
I =
Z
dt
„∂q(t)
∂t
«2
=
Z
dt
„∂q(t)
∂t
« „ ∂q(t)
∂t
«
,
(23.35)
giving
I =✘✘✘✘✘✘
»
q(t) ∂q(t)
∂t
–T
t=0
−
Z
dt q(t) ∂2
∂t2 q(t).
(23.36)
The boundary conditions (q = 0 at the beginning and end of the trajectory) wipe
out the ﬁrst term and the second term has the form ‘q ˆCq’, where ˆC is an operator.
The use of our neat trick gives the functional integral
G(0, T, 0, 0) =
Z
D[q(t)] e
i
2
R
dt q(t) ˆ
Cq(t),
(23.37)

218
Path integrals: I said to him, ‘You’re crazy’
with ˆC = m

−∂2
∂t2 −ω2
0

.
We can do this integral using a slightly
modiﬁed form of the result in eqn 23.30. Setting A(x, y) = −i ˆC and
b(x) = 0 in eqn 23.30, we discretize the integral to obtain
G →
Z
dq1dq2 . . . dqN e
i
2
P
ij qi ˆ
Cijqj =
(2πi)N
det ˆC
 1
2
.
(23.38)
Again this ﬁlls us with a sense of dread since the result is quite frighten-
ing with its factor (2πi)N that won’t look pretty as N →∞. Undaunted,
we’ll assume that when we pop back into the world of functions the an-
swer will at least be that →G ∝[det ˆC(x, y)]−1/2.
Admittedly, the
determinant of the diﬀerential operator ˆC(x, y) might also send a chill
down the spine. To give such an object meaning, we can ﬁnd the eigen-
values (and eigenfunctions) of the operator and use the fact that the
determinant is given by the product of eigenvalues. Our answer is then
G(0, T, 0, 0) = B det

m

−∂2
∂t2 −ω2
0
−1
2
,
(23.39)
where B is some constant coming from the integration measure that
we’ll worry about shortly.
Example 23.7
The next job is to work out the determinant. This we do with the trick described
above that makes use of the fact that the determinant of a matrix is equal to the
product of its eigenvalues. Finding the eigenfunctions of the operator ˆC isn’t too
tough either. You can check that m
“
−∂2
∂t2 −ω2
0
”
q(t) = λnq(t) has eigenfunctions
q(t) = c sin(unt), with eigenvalues λn = m(u2
n −ω2
0), of which there are an inﬁnite
number. Remembering that our boundary conditions are that q = 0 at t = 0 and
q = 0 at t = T we need un = nπ/T for everything to work. Putting this all together,
we have an answer
G(0, T, 0, 0) = B
( ∞
Y
n=1
m
»“nπ
T
”2
−ω2
0
–)−1
2
.
(23.40)
At this stage we notice that the answer isn’t very useful in its present form. This is
because the inﬁnite product looks like it gives inﬁnity whenever ω0T = nπ and we
don’t know what B is yet.
A sanity check is useful here. When there’s no spring constant ω0 = 0, there’s no
harmonic potential and we have a free particle. In that case we have
Gω=0(0, T, 0, 0) = B
( ∞
Y
n=1
m
“nπ
T
”2
)−1
2
,
(23.41)
but we already know (from Example 16.5) that for a free particle Gfree(0, T, 0, 0) =
“
−im
2πT
” 1
2 .
This will be enough to provide the proportionality constant B for our
problem.

23.3
The propagator for the simple harmonic oscillator
219
We multiply eqn 23.40 by Gfree/Gω0=0 = 1:
G(0, T, 0, 0)
=
G(0, T, 0, 0) Gfree(0, T, 0, 0)
Gω0=0(0, T, 0, 0)
=
B
nQ∞
n=1 m
h` nπ
T
´2 −ω2
0
io−1
2
B
nQ∞
n=1 m
` nπ
T
´2o−1
2
„ −im
2πT
« 1
2
=
( ∞
Y
n=1
"
1 −
„ ω0T
nπ
«2#)−1
2 „−im
2πT
« 1
2
,
(23.42)
which removes all of the dangerous terms. Finally we use the identity Q∞
n=1[1 −
(x/nπ)2]−1 = x/ sin x and write
G(0, T, 0, 0) =
„
ω0T
sin ω0T
« 1
2 „−im
2πT
« 1
2
.
(23.43)
The ﬁnal answer is that
G(0, T, 0, 0) =

−imω0
2π sin ω0T
 1
2
,
(23.44)
and the probability density |G|2 derived from this is shown in Fig. 23.4.
Interestingly, the probability density peaks strongly for T = nπ/ω0,
where n is an integer, making it very likely that the particle will be
found at the origin at these times. Perhaps this shouldn’t come as too
much of a surprise: we know that classical particle oscillates back and
forth in an harmonic potential. From the path integral point of view,
the peaks in probability density result from the constructive interference
of all of the possible particle paths that start and end at the origin in a
time T; while the minima result from a large degree of cancellation of
such path sections. It’s as if the paths at the origin go in and out of
focus as a function of time!
0
2
4
6
8
10
2π|G|2/(mω0)
0
1
2
3
4
ω0T/π
Fig. 23.4
The (scaled) probability
density |G(0, T, 0, 0)|2 as a function of
time for a particle that starts at the ori-
gin of a harmonic oscillator potential to
be found there a time T later.
Chapter summary
• Feynman’s path integral approach involves ﬁnding amplitudes by
adding up contributions from all possible trajectories. Each tra-
jectory makes a contribution exp(iS/ℏ).
• One type of functional integral has been evaluated and yields the
result
R
D[f]e−1
2
R
fAf+
R
bf =

(2π)N
det A
 1
2 e
1
2
R
bA−1b.
• The simple harmonic oscillator provides an example showing the
inﬁnities that have to be tamed in a typical calculation.

220
Path integrals: I said to him, ‘You’re crazy’
Exercises
(23.1) A back of the envelope derivation of the most im-
portant result of the chapter.
Start with a Lagrangian L = 1
2x ˆ
Ax + bx, where ˆ
A
is an operator. We will be brash and treat ˆ
A as a
number. Use the Euler–Lagrange equation to ﬁnd
x and show that the Lagrangian may be expressed
equivalently as L = −b 1
2 ˆ
Ab.
(23.2) The path integral derivation of Wick’s theorem.
(a) Calculate
R ∞
−∞dx x2e−1
2 ax2.
(b) Deﬁne
⟨xn⟩=
R ∞
−∞dx xne−1
2 ax2
R ∞
−∞dx e−1
2 ax2
.
(23.45)
Calculate (i) ⟨x2⟩, (ii) ⟨x4⟩and (iii) ⟨xn⟩.
(c) Make sense of the result for ⟨xn⟩diagrammati-
cally, representing each factor of 1
a as a line linking
factors of x.
(d) Now consider the N-dimensional vector x and
the integral
K
=
Z
dx1 . . . dxN e−1
2 xTAx+bTx
=
„(2π)N
det A
« 1
2
e
1
2 bTA−1b.
(23.46)
By diﬀerentiating with respect to components of the
vector b, and then setting b = 0, show that
⟨xixj⟩=
R
dx1 . . . dxN xixje−1
2 xTAx
R
dx1 . . . dxN e−1
2 xTAx
= (A−1)ij.
(23.47)
(e) Using these results, argue that
⟨xixjxkxl⟩
=
(A−1)ij(A−1)kl + (A−1)ik(A−1)jl
+(A−1)il(A−1)jk.
(23.48)
(f) Write down an expression for the general case
⟨xi . . . xz⟩.
This is the basis of Wick’s theorem in the path in-
tegral approach.
(23.3) Consider the forced quantum oscillator from Exer-
cise 22.1
L = 1
2m ˙x(t)2 −1
2mω2x(t)2 + f(t)x(t).
(23.49)
Take f(t) to be a constant force f0 which acts only
for a ﬁnite length of time: 0 ≤t ≤T.
(a) Show that the amplitude for a particle that is in
the ground state at t = 0 to be in the ground state
at t = T is given by
A = e−1
2
R
dt′dtf(t)G(t,t′)f(t′),
(23.50)
where
G(t, t′) = θ(t −t′)e−iω(t−t′) + θ(t′ −t)eiω(t−t′)
2mω
.
(23.51)
(b) Carry out the integral (being careful of the lim-
its) and show that the amplitude is given by
A = exp
» if 2
0
2mω2
„
T −sin ωT
ω
+ i 2
ω sin2 ωT
2
«–
.
(23.52)
(c) What is the probability for the oscillator to still
be in its ground state at t = T? Compare this with
the result of Exercise 22.1.
(d) What is the physical meaning of the imaginary
part of the argument in the exponential?

24
Field integrals
24.1 The functional integral for
ﬁelds
221
24.2 Which ﬁeld integrals should
you do?
222
24.3 The generating functional for
scalar ﬁelds
223
Chapter summary
226
Exercises
226
Life is not a walk across a ﬁeld
Russian proverb, quoted by Boris Pasternak (1890–1960) in
his poem Hamlet
In this chapter we will apply the functional integrals of the last chapter to
quantum ﬁelds. Remarkably, the generating functional Z[J] for quantum
ﬁelds can be written as a functional integral. In this chapter we’ll ﬁnd
that integral. Using functional integrals can be seen as an alternative
to canonical quantization and the ˆS-operator (as shown in Fig. 24.1)
although it’s best to have the two approaches available since both have
their strengths and weaknesses.
Lagrangian
Path integral
gives
Z[J]
Green’s
functions
Physical
predictions
add a source current J
diﬀerentiate
Fig. 24.1 Using functional integrals to
do quantum ﬁeld theory.
24.1
The functional integral for ﬁelds
In the previous chapter we calculated a propagator amplitude for a par-
ticle by doing a functional integral. This involved summing all possible
trajectories for a single particle travelling between two spacetime points.
Our result was that
G(x, tx, y, ty) =
Z
D[q(t)] ei
R ty
tx dt L[q(t)].
(24.1)
However, we know that single-particle quantum mechanics isn’t enough
to describe reality: we need ﬁelds. We therefore want to ﬁnd a functional
integral that sums all possible ﬁeld conﬁgurations that can exist between
the two spacetime points. The bad news is that this integral won’t yield
a propagator. In the next example we’ll pursue a method of integrating
over ﬁeld conﬁgurations and in the following section we will show (the
good news) how this leads directly to the generating functional.
Example 24.1
The jump between integrating over particle trajectories to integrating over ﬁeld con-
ﬁgurations is quite straightforward.
We can still write down the eiS/ℏfactor for
ﬁelds, but now the action S is the integral of the Lagrangian density over space-
time: S =
R
d4x L[φ(x)]. Note that the ﬁelds appearing here are classical Heisenberg
ﬁelds.
Their time dependence is given by the full Hamiltonian and they haven’t
been through a canonical quantization machine so have no commutation relations.
A functional integral over ﬁelds may then be written as
Z
D [φ(x)] ei
R ∞
−∞d4x L[φ(x)].
(24.2)

222
Field integrals
The diﬀerence with the path integral that we had before is that the dynamic variable
is now the ﬁeld φ(x) rather than the trajectory q(t) and the integral is therefore over
functions of position and time rather than just the functions of time. Despite this
diﬀerence, we do the integral in the same way as before: we discretize. Previously we
just sliced up time, now we slice up spacetime. We obtain a multiple integral over N
spacetime variables.
We know how to do the integral for the special, but important, case that the
Lagrangian can be worked into the form L[q] =
i
2 qTAq + ibTq.
Based on the
observation that
Z
dx e
i
2 ax2+ibx =
„ 2πi
a
« 1
2
e−ib2
a ,
(24.3)
we can read oﬀthat the answer is
Here’s the recipe to revert back from
continuous functions to a discrete lat-
tice:
• The ﬁeld φ(x) evaluated at a
spacetime point x becomes an
N-component vector qj, where
j = 1, . . . , N labels a point in
spacetime.
• Diﬀerential
operators
become
real symmetric matrices. Things
like
∂φ
∂x become
1
l (qi −qj) =
Aijqj.
• Integrals become sums:
R
dx be-
comes l P
j.
P =
Z
dNq e
i
2 qTAq+ibTq =
„(2πi)N
detA
« 1
2
e−i
2 bTA−1b.
(24.4)
Our important result for a calculable functional integral over ﬁelds is that
Z
D[φ(x)] e
i
2
R
d4xd4y φ(x)A(x,y)φ(y)+i
R
d4x b(x)φ(x)
=
B [det A(x, y)]−1
2 e−i
2
R
d4xd4y b(x)A−1(x,y)b(y),
(24.5)
where we’re hidden the factor (2πi)N in the (divergent) constant B as we did before.
24.2
Which ﬁeld integrals should you do?
We’re now at the stage where we’d like to have a go at doing a functional
integral over some ﬁelds. Before getting too excited about all of this, we
need to decide which integrals are worth doing. What are we trying to
learn? Which ﬁelds do we start at, and which do we end with?
We want access to all of the Green’s functions for a theory, since these
contain all of the physics. If we only want to work out one thing, then
it should be the generating functional Z[J]. We saw before that this
functional contained all of the Green’s functions and therefore all of the
information for the theory. Remember that the generating functional
Z[J] tells us the amplitude
Z[J] =

no particles
no particles
at x0 = ∞
at y0 = −∞

J
,
(24.6)
that is, in the presence of a source, we start with no particles and end
with no particles.1 The remarkable answer turns out to be that the
1Also remember that our goal is ac-
tually
the
Green’s
functions.
To
get
to
these
we
can
take
func-
tional
derivatives
of
the
generat-
ing
functional
inG(n)(x1, ..., xn)
=
1
Z[J]
δnZ[J]
δJ(x1)...δJ(xn)
˛˛˛
J=0 (eqn 21.10).
generating functional can be written in terms of the functional integral
as2
2Note that this is the unnormalized
generating functional.
Recall that to
obtain the normalized generating func-
tional we can simply divide through by
Z[J = 0] since Z[J] = Z[J]/Z[0].
Z[J] =
Z
D[φ(x)] ei
R
d4x {L[φ(x)]+J(x)φ(x)}.
(24.7)
In order to go about calculating Z[J], we start by splitting the La-
grangian up into two parts L = L0 + LI. As usual, L0 is the free part:
it’s the part that can be cast in the quadratic form 1
2φ(x) ˆAφ(x) and is
therefore solvable by our one known ﬁeld integral. The other term LI is
the interaction part: that is, the part which isn’t solvable by canonical

24.3
The generating functional for scalar ﬁelds
223
quantization. Written in terms of functional integrals, the generating
functional is therefore
Z[J] =
Z
D[φ(x)] ei
R
d4x (L0+LI+Jφ).
(24.8)
Now let’s try to do the integral.
24.3
The generating functional for scalar
ﬁelds
We’ll calculate a generating functional for the free scalar ﬁeld. Following
the custom that quantities involving free ﬁelds are given a subscript ‘0’
this will be denoted Z0[J]. The free Lagrangian is given by
L0 = 1
2(∂µφ)2 −m2
2 φ2.
(24.9)
The generating functional Z0[J] for the free scalar ﬁeld is
Note that in eqn 24.10 and later we’ve
suppressed the spacetime variable x to
avoid unnecessary clutter.
Z0[J] =
Z
Dφ e
i
2
R
d4x{(∂µφ)2−m2φ2}+i
R
d4x Jφ.
(24.10)
As we’ve said above, we can do the integral if we massage Z0[J] into
the form of eqn 24.4. We again invoke the neat trick of doing by parts
the integral of the kinetic energy part of the Lagrangian density in the
argument of the exponential (e
i
2
R
d4x (∂µφ)2):
Z
d4x (∂µφ)2 = ✘✘✘✘✘✘
[φ(∂µφ)]∞
−∞−
Z
d4x φ ∂2φ,
(24.11)
where we assume that the ﬁrst term disappears at the boundary, that is
the ﬁeld dies oﬀas we head out to spacetime inﬁnity. This trick enables
us to make the replacement
R
d4x (∂µφ)2 →−
R
d4x φ ∂2φ and allows us
to rewrite the functional integral as
Z0[J] =
Z
Dφ e
i
2
R
d4x φ[−(∂2+m2)]φ+i
R
d4x Jφ,
(24.12)
which is in the form of the one ﬁeld integral that we know how to do.3
3We will see that this is telling us that
a Lagrangian that can be massaged
into this form may be quantized using
functional integration.
The family of
quadratic and bilinear Lagrangians for
which this is true are exactly those that
may be diagonalized by canonical quan-
tization and are conventionally called
non-interacting.
We have that ˆA = −(∂2 + m2) and our answer is that
Z0[J] = B [det A(x, y)]−1
2 e−1
2
R
d4xd4y J(x)[i ˆ
A−1(x,y)]J(y),
(24.13)
where B is a (potentially inﬁnite) constant determined by the integration
measure. The determinant also looks worryingly inﬁnite!
Normalization will save us here. The normalized generating functional
is Z0[J] = Z[J]/Z[J = 0], where Z0[J = 0] = B det
h
ˆA(x, y)
i−1
2 . This
removes two divergent quantities (B and the determinant) at a stroke.
Recall that the normalization ensures
that the amplitude to start and end
with no particles, in the absence of a
source, is unity.

224
Field integrals
Finally, we recognize ˆA(x, y) = −(∂2 + m2) as an operator giving
rise to the Klein–Gordon equation of motion and note that its inverse
determines the free scalar propagator via
i ˆA−1(x, y) = ∆(x, y) =
Z
d4p
(2π)4
ie−ip·(x−y)
p2 −m2 + iǫ.
(24.14)
Example 24.2
To check this, we recall that the inverse of an operator satisﬁes ˆ
A ˆ
A−1 = I, which in
the continuum limit becomes
−(∂2
x + m2) ˆ
A−1 = δ(4)(x −y),
(24.15)
where the ∂2
x operator acts on the x variable. Next we note that we deﬁned ∆(x, y)
via
−(∂2
x + m2)∆(x, y) = iδ(4)(x −y),
(24.16)
so we can identify i ˆ
A−1 = ∆(x, y), that is, the Green’s function of the equation of
motion, also known as the Feynman propagator.
Putting all of this together gives us a normalized generating functional
for the free scalar ﬁeld of
Notice that the ﬁeld φ doesn’t appear
on the right-hand side of eqn (24.17).
We say that we’ve ‘integrated out’ the
ﬁeld φ(x).
Z0[J]
=
R
Dφ e
i
2
R
d4x φ{−(∂2+m2)}φ+i
R
d4x Jφ
R
Dφ e
i
2
R
d4x φ{−(∂2+m2)}φ
=
e−1
2
R
d4xd4y J(x)∆(x−y)J(y).
(24.17)
Example 24.3
We now justify that eqn 24.17 is the same generating functional for the free scalar
ﬁeld that we deﬁned in Chapter 22.
The Chapter 22 deﬁnition of the generating
functional may be written
Z[J] =
∞
X
n=0
in
n!
Z
d4x1...d4xn J(x1)...J(xn)⟨Ω|T ˆφH(x1)...ˆφH(xn)|Ω⟩.
(24.18)
Note that for a non-interacting ﬁeld we have ˆφH = ei ˆ
H0t ˆφe−i ˆ
H0t = ˆφI and so we can
use Wick’s theorem (which applies to freely evolving interaction picture ﬁelds only)
to expand any time-ordered products. They fall apart into a sum over products of
free propagators. For example, the n = 4 term involves a product
⟨0|T ˆφI(x1)ˆφI(x2)ˆφI(x3)ˆφI(x4)|0⟩=∆(x1 −x2)∆(x3 −x4) + ∆(x1 −x3)∆(x2 −x4)
+ ∆(x1 −x4)∆(x2 −x3).
Renaming variables, we see that each of the three terms will therefore contribute a
factor
R
[J(x1)∆(x1 −x2)J(x2)]2. This enables us to write the fourth-order term as
i4
4!
Z
d4x1...d4x4 J(x1)...J(x4)⟨Ω|T ˆφH(x1)...ˆφH(x4)|Ω⟩
= 1
2!
„
−1
2
Z
d4x1d4x2 J(x1)∆(x1 −x2)J(x2)
«2
.
(24.19)
Repeating this process for other values of n generates the corresponding terms in the
exponential and the answer indeed reduces to eqn 24.17.

24.3
The generating functional for scalar ﬁelds
225
The point of ﬁnding the generating function is to give us access to prop-
agators. Speciﬁcally we have for free ﬁelds that the propagator is given,
in terms of the normalized generating functional, by
G(n)
0 (x1, ..., xn)
=
1
in
δnZ0[J]
δJ(x1)...δJ(xn)

J=0
=
1
in
1
Z0[J = 0]
δnZ0[J]
δJ(x1)...δJ(xn)

J=0
. (24.20)
We’ll evaluate this in two diﬀerent ways for the single-particle propagator
G0(x, y). Diﬀerentiating the expression for the functional integral Z0[J]
with respect to the J’s gives us
G0(x, y) =
R
Dφ φ(x)φ(y)ei
R
d4xL0[φ]
R
Dφ ei
R
d4xL0[φ]
,
(24.21)
while diﬀerentiating the expression for the normalized generating func-
tional Z0[J] = e−1
2
R
d4xd4y J(x)∆(x,y)J(y) gives us the expected answer
G0(x, y) = ∆(x, y). Thus the Green’s function, which we wrote down as
⟨Ω|T ˆφH(x)ˆφH(y)|Ω⟩, is obtained by integrating two ﬁelds weighted by
a factor ei
R
d4x L[φ] which tells us how the amplitudes of diﬀerent ﬁeld
trajectories are distributed.4
4This is just as we have in classical
statistical physics, where the analogous
expression for the average of two ﬁelds
is
⟨φ1φ2⟩t =
P
ij φiφje−βH(φi,φj)
P
ij e−βH(φi,φj)
(24.22)
where the sum is over all conﬁgurations
of φi and φj weighted by the Boltz-
mann factor and divided by the normal-
izing partition function.
Example 24.4
We’ll apply what we’ve learnt to the interesting case of the free massive vector ﬁeld
theory from Chapter 13 described by
L
=
−1
4(FµνF µν) + 1
2m2AµAµ
=
−1
2(∂µAν∂µAν −∂µAν∂νAµ) + 1
2 m2AµAµ
(24.23)
(see Exercise 13.4 if in doubt).
The only complication with this is the diﬀerent
components of the Aµ ﬁeld. Note that in this case the result of the one functional
integral that we can do is written
Z0[J] =
R
DA e
i
2
R
d4x Aµ ˆ
KµνAν+i
R
d4x JµAµ
R
DA e
i
2
R
d4x Aµ ˆ
KµνAν
= e−1
2
R
d4xd4y Jµ(x)[i ˆ
K−1
µν ]Jν(y).
(24.24)
We can, therefore, only do our functional integral if the Lagrangian is in the form
1
2 Aµ ˆ
KµνAν, where ˆ
Kµν is a diﬀerential operator (with an inverse). Just as before,
the way to achieve this is, as usual, to integrate L by parts. Of the three terms in
the Lagrangian, the ﬁnal one is already in the correct form. Integrating the ﬁrst one,
and disregarding the boundary term, we get 1
2 Aµ∂2Aµ = 1
2 Aµ∂2gµνAν. The second
term yields −1
2Aµ∂µ∂νAν. The result5 is that
5You are invited to conﬁrm this result
in Exercise 24.1.
1
2Aµ ˆ
KµνAν = 1
2Aµ ˆ
(∂2 + m2)gµν −∂µ∂ν
˜
Aν.
(24.25)
We therefore do the integral
Z0[J]
=
R
DA e
i
2
R
d4x Aµ[(∂2+m2)gµν−∂µ∂ν]Aν+i
R
d4x JµAµ
R
DA e
i
2
R
d4x Aµ[(∂2+m2)gµν−∂µ∂ν]Aν
=
e−1
2
R
d4xd4y Jµ(x)[i ˆ
K−1
µν ]Jν(y),
(24.26)

226
Field integrals
where i ˆ
K−1
µν is the free particle propagator for the theory G0µν(x, y). This already
tells us something very interesting. Namely, that we have access to the free prop-
agator for a theory if we can work out i ˆ
K−1
µν . This is often an easier method than
working out, for example, G0µν(x, y) = ⟨0|T ˆ
Aµ(x) ˆ
A†
ν(y)|0⟩as we would have had to
have done here using a mode expansion if we were still using canonical quantization.
Following this through, we can therefore obtain the free particle propagator from the
following equation (with the indices all included):
ˆ
(∂2 + m2)gµν −∂µ∂ν˜
G0νλ(x, y) = igµ
λδ(4)(x −y),
(24.27)
and the easiest way of doing this is to take a Fourier transform, yielding
[−(p2 −m2)gµν + pµpν] ˜G0νλ(p) = igµ
λ,
(24.28)
whose solution may be shown to be
˜G0νλ(p) = −i
`
gνλ −pνpλ/m2´
p2 −m2
,
(24.29)
which (with the addition of iǫ in the denominator) is the massive vector ﬁeld propaga-
tor. Interestingly, this has the form of the scalar propagator which we met previously,
but this time with the transverse projection tensor on top, containing all of the in-
dices.
Chapter summary
• The normalized generating functional for free scalar ﬁelds is
Z0[J] = e−1
2
R
d4xd4y J(x)∆(x−y)J(y).
Exercises
(24.1) (a) Verify eqn 24.25.
(b) Show that eqn 24.29 solves eqn 24.28.
(24.2) Consider the φ4 Lagrangian L = 1
2(∂µφ)2 −m2
2 φ2 −
g
8φ4. Shift the Lagrangian by adding a new ﬁeld σ
via
L′ = L + 1
2g
“
σ −g
2φ2”2
.
(24.30)
(a) By performing a functional integral over the
ﬁeld σ, show that σ doesn’t change the dynamics of
the theory.
(b) Conﬁrm that σ has no eﬀect on the dynamics of
the theory by ﬁnding the Euler–Lagrange equation
for σ and showing that is has no time derivatives.
This means that it can only provide a constraint and
may be eliminated, much like the component A0 in
the massive vector ﬁeld in Chapter 13.
(c) Contrast the Feynman diagrams of φ4 theory
with those of the shifted theory involving σ.
The introduction of a new ﬁeld to shift the La-
grangian in this way is known as a Hubbard–
Stratonovich transformation and is very useful in
removing the inconvenient φ4 term from a theory.
(24.3) In this problem we motivate the perturbation expan-
sion for the functional integral.
We want to do the integral
Z(J) =
Z
dx e−1
2 Ax2−λ
4! x4+Jx.
(24.31)
(a) Show that this integral may be recast as
Z(J) =
" ∞
X
n=0
1
n!
„
−λ
4!
∂4
∂J4
«n# Z
dx e−1
2 Ax2+Jx,
(24.32)

Exercises
227
which has the advantage that we’ve eﬀectively re-
moved the interaction from inside the integral.
(b) Re-exponentiate the series in the ﬁrst square
bracket to give a diﬀerential operator acting on an
integral and then do the integral to obtain the result
Z(J) =
»
e−λ
4!
∂4
∂J4
– "„2π
A
« 1
2
e
J2
2A
#
.
(24.33)
We now have a perturbation series that can be ex-
panded by operating on the free generating function
Z0(J) =
` 2π
A
´ 1
2 e
J2
2A .
(24.4) By analogy with the previous problem, the gener-
ating functional for φ4 theory is given by
Z[J] =
»
e
−iλ
4!
R
d4z
δ4
δJ(z)4
–
Z0[J],
(24.34)
where Z0[J] is the normalized generating functional
for the free scalar ﬁeld. In this problem we investi-
gate the ﬁrst-order term in the expansion of Z[J],
given by
Z1[J]=
»
−iλ
4!
R
d4z
δ4
δJ(z)4
–
e−1
2
R
d4xd4y J(x)∆(x−y)J(y).
(24.35)
We will act on Z0[J] = e−1
2
R
J∆J with
δ
δJ(z) four
times. This will rely on some functional diﬀerenti-
ation from Chapter 1.
(a) Verify that hitting Z0[J] once gives us
δZ0[J]
δJ(z) =
»
−
Z
d4y ∆(z −y)J(y)
–
Z0[J]; (24.36)
(b) the second time:
δ2Z0[J]
δJ(z)2
=
"
−∆(z −z)
(24.37)
+
Z
d4y ∆(z −y)J(y)
ﬀ2 #
Z0[J];
(c) the third:
δ3Z0[J]
δJ(z)3 =
»
3∆(z −z)
Z
d4y ∆(z −y)J(y)
ﬀ
−
Z
d4y ∆(z −y)J(y)
ﬀ3#
Z0[J];
(24.38)
(d) and ﬁnally the fourth:
δ4Z0[J]
δJ(z)4
=
ˆ
3∆(z −z)2
(24.39)
−6∆(z −z)
Z
d4y ∆(z −y)J(y)
ﬀ2
+
Z
d4y ∆(z −y)J(y)
ﬀ4#
Z0[J].
(e) Expand the brackets, multiply by −iλ/4! and
integrate
R
d4z to end up with the ﬁnal result that
Z1[J] is given by
Z1[J]
=
−iλ
"
1
8
Z
d4z ∆(z −z)2
(24.40)
−1
4
(Z
d4zd4y1d4y2∆(z −z)
×∆(z −y1)J(y1)∆(z −y2)J(y2)
)
+ 1
4!
( Z
d4zd4y1d4y2d4y3d4y4 ∆(z −y1)
×J(y1)∆(z −y2)J(y2)∆(z −y3)J(y3)
×∆(z −y4)J(y4)
)#
Z0[J].
We see that this has given us terms in J of order
O(J) = 0, 2 and 4.
(f) Interpret this result by drawing Feynman dia-
grams for each term. This procedure can be used to
work out the Feynman rules for a theory. For more
detail see Ryder, Chapter 6.

25
Statistical ﬁeld theory
25.1 Wick rotation and Euclidean
space
229
25.2 The partition function
231
25.3 Perturbation
theory
and
Feynman rules
233
Chapter summary
236
Exercises
236
It is the mark of a truly intelligent person to be moved by
statistics.
George Bernard Shaw (1856–1950)
We’ve seen how taking the derivatives of the generating functional
Z[J] =
R
Dφ ei
R
d4x(L[φ]+Jφ) allows us access to the all-important
Green’s functions of a quantum ﬁeld theory via
⟨Ω|T ˆφ(x1)...ˆφ(xn)|Ω⟩= 1
in
1
Z[J = 0]
δnZ[J]
δJ(x1)...δJ(xn)

J=0
.
(25.1)
This is very similar indeed to the state of aﬀairs in statistical mechanics
that we examined in Chapter 21. There we found that we had access to
all of the interesting correlation functions via derivatives of the partition
function
⟨ˆφi1...ˆφin⟩t =
1
Z(J = 0)
∂nZ(J)
∂Ji1...∂Jin

J=0
.
(25.2)
In fact, our motivation for searching for a quantum ﬁeld generating func-
tional was simply the ease with which calculations in statistical physics
can be made with a partition function. The similarity between the two
disciplines is not just a superﬁcial resemblance but reﬂects something
much more fundamental. The crux of the matter is that the Green’s
functions of quantum ﬁeld theory are related to time-evolution opera-
tors, which (roughly speaking) look like a complex exponential e−iEt/ℏ,
with E an energy and t the time. The probabilities (and density ma-
trices) of statistical physics are based on the Boltzmann factor, given
(roughly) by the real exponential e−E/kBT . The fundamental elements
of the two theories are therefore both exponentials, albeit one is real and
the other is complex. This similarity can be exploited by making the
substitution
i t
ℏ→
1
kBT ,
(25.3)
and the claim is that this will map a quantum ﬁeld theory on to a
statistical ﬁeld theory. The advantage of this mapping is that it allows us
to transfer results from one subject directly to the other, and vice versa.
Within the framework of the mapping, inverse temperature1 β = 1/kBT
1Usually we employ units where kB =
ℏ= 1 so that β = 1/T = it.
in statistical physics behaves like imaginary time in a quantum ﬁeld
theory. It turns out that the concept of imaginary time is a remarkably
powerful one with uses across quantum and statistical ﬁeld theory. We
examine the imaginary time formalism in the next section.

25.1
Wick rotation and Euclidean space
229
25.1
Wick rotation and Euclidean space
Imaginary time, assigned the symbol τ = it, is deﬁned, not simply by
multiplying time by i, but by making a rotation in the complex plane
by an angle2 −π/2 (Fig 25.1). This transformation is known as a Wick
2Positive angles correspond to anti-
clockwise
rotations
in
the
complex
plane.
rotation and leads to a very nice feature for spacetime four-vectors. By
rotating the time-like part of our vectors we essentially remove the an-
noyance of the Minkowski metric (+, −, −, −) and replace it (to within
an overall minus sign) by the more civilized (+, +, +, +). This means
that all four components of our vectors are treated equally, in the same
way that all spatial components are treated equally when doing Eu-
clidean geometry in three dimensions. We refer to this four-dimensional
space as Euclidean space.
Euclid of Alexandria was a Greek math-
ematician who lived around 300 BC.
He is best known for his Elements, the
textbook which laid the foundations
for geometry and has inﬂuenced gen-
erations mathematicians for more than
two thousand years.
p0
Re
Im
Fig. 25.1 The Wick rotation involves a
rotation of −π/2 in the complex plane.
We also show the poles of the Feynman
propagator, demonstrating that Wick
rotating of the contour of integration
from the real axis (integrating from
−∞to ∞as we do in calculating Feyn-
man amplitudes) to the imaginary axis
(integrating −i∞to i∞) is permissible
as we don’t cross any singularities.
Example 25.1
Let’s work through the consequences of this transformation.
To make the Wick
rotation to Euclidean space we will rotate the time coordinate by −π/2, deﬁning
x0 = −iτ. This means that
x2 = x2
0 −|x|2 = −(τ 2 + |x|2) = −x2
E,
(25.4)
where x2
E = τ 2 + x2. Note that it also implies that
d4x = −id4xE = −idτ d3x.
(25.5)
We still want things like Fourier transforms to work in Euclidean space.
The re-
quirement that the exponential factor eip·x will not blow up requires us to rotate the
energy part of the momentum vector p0 by π/2 in the complex plane. We therefore
deﬁne p0 = iω, giving
p2 = p2
0 −|p|2 = −(ω2 + |p|)2 = −p2
E.
(25.6)
We can also write the element of four-volume in momentum space as
d4p = id4pE = idω d3p,
(25.7)
and the inner product between p and x as
p · x = p0x0 −p · x = ωτ −p · x.
(25.8)
We also consider the derivatives. We have that ∂0 = i∂τ and
∂2 = ∂2
0 −∇2 = −(∂2
τ + ∇2) = −∂2
E,
(25.9)
where ∂2
E = ∂2
τ + ∇2. Finally we need to note that
(∂µφ)2 = −
ˆ
(∂τφ)2 + (∇φ)2˜
= −(∂Eµφ)2.
(25.10)
To summarize the previous example:
The rules of Euclidean space
p0 = iω
p2 = −(ω2 + p2) = −p2
E
d4p = idω d3p = id4pE
x0 = −iτ
x2 = −(τ 2 + x2) = −x2
E
d4x = −idτ d3x = −id4xE
∂0 = i∂τ
∂2 = −(∂2
τ + ∇2) = −∂2
E
(∂µφ)2 = −

(∂τφ)2 + (∇φ)2
(25.11)

230
Statistical ﬁeld theory
Example 25.2
Our goal is to see the eﬀect of the Wick rotation on the path integral. We start with
the φ4 theory with sources for which we have
iS = i
Z
d4x
» 1
2(∂µφ)2 −m2
2 φ2 −λ
4! φ4 + Jφ
–
,
(25.12)
Making the rotation results in the expression3
3Note
that
we
often
write
LE[φ]
=
1
2 (∂τφ)2 + U[φ],
where
U[φ]
h
= 1
2 (∇φ)2 + m2
2 φ2 + λ
4!φ4i
is
the potential energy density.
iS = −
Z
d4xE
»1
2(∂τφ)2 + 1
2(∇φ)2 + m2
2 φ2 + λ
4! φ4 −Jφ
–
.
(25.13)
Deﬁning
SE =
Z
d4xE (LE[φ] −Jφ) =
Z
d4xE
» 1
2(∂τφ)2 + 1
2(∇φ)2 + m2
2 φ2 + λ
4! φ4 −Jφ
–
,
(25.14)
we write a Euclidean path integral
Z[J]
=
Z
Dφ e−SE =
Z
Dφ e−
R
d4xE{LE[φ]−J(x)φ(x)}.
(25.15)
This is a useful mathematical object as it stands since it makes the path integral
somewhat more respectable. As we shall see in the next example, it removes the
ambiguity in deﬁning the propagator which forced us to introduce those factors of iǫ.
Example 25.3
This Wick rotation may be carried out on some of our most useful equations to
express them in Euclidean space. The Feynman propagator for free scalar ﬁelds in
Minkowski space is
∆(x) =
Z
d4p
(2π)4
ie−ip·x
p2 −m2 + iǫ ,
(25.16)
which is the solution to the equation
(∂2 + m2)∆(x −y) = −iδ(4)(x −y).
(25.17)
In Euclidean space4 this becomes
4We drop the subscript ‘E’ on position
and momentum variables in Euclidean
space from this point.
∆E(x) =
Z
d4p
(2π)4
e−ip·x
p2 + m2 .
(25.18)
In the last equation we’ve dropped the iǫ, since it is no longer required in this equation
which has no poles (see also Fig. 25.1). This is the solution to the equation
(−∂2
E + m2)∆E(x −y) = δ(4)(x −y).
(25.19)
That this is the case may be shown by transforming from Minkowski to Euclidean
space. We make the changes
∂2 →−∂2
E,
∆(x) →∆E(x).
(25.20)
To see how the delta function transforms note that its deﬁnition is
δ(x0)δ(3)(x)
=
Z
d4p
(2π)4 e−ip·x →i
Z
dωd3p
(2π)4 e−ip·x = iδ(τ)δ(3)(x),(25.21)
which may be substituted to give the desired result.

25.2
The partition function
231
25.2
The partition function
We will now attempt to express the partition function as a path integral
and will ﬁnd that it involves the Euclidean generating functional. The
partition function is given by Z = Tr
h
e−β ˆ
Hi
= P
λ⟨λ|e−β ˆ
H|λ⟩. We
know how to express a similar object as a path integral. In Chapter 23
we found
⟨qB|e−i ˆ
HtB|qA⟩=
Z
D[q(t)] ei
R tB
0
dtL[q(t)],
(25.22)
which tells us that to evolve the state at A into that at B we add up
all of the possible trajectories that start at A at t = 0 and end at B at
t = tB. Let’s cook up a similar description of P
λ⟨λ|e−β ˆ
H|λ⟩.
First set |qA⟩= |qB⟩= |λ⟩, sum over λ and make the replacement
tB →−iβ. This turns the left-hand side into the desired P
λ⟨λ|e−β ˆ
H|λ⟩,
where the consequences of the sum will be discussed below. Now for the
right-hand side. The replacement tB →−iβ provides the integral in the
exponent with the new limits
R t=−iβ
t=0
. To give this meaning we make the
Wick rotation t →−iτ and we obtain
X
λ
⟨λ|e−β ˆ
H|λ⟩=
X
λ
Z
D[q(τ)] e−
R β
0 dτLE[q(τ)],
(25.23)
where LE[q(τ)] = m
2

dq
dτ
2
+ V (q) is the Euclidean version of the La-
grangian for a single particle.
The question is, how does the sum on the left aﬀect the trajectories
over which we’ll need to integrate? If we think of e−β ˆ
H as an evolution
operator, then we’re taking a state |λ⟩, evolving τ from 0 to β and
requiring that we end up at the same state. This means that we must
have q(τ = 0) = q(τ = β).
But that’s not all.
If we imagine τ as
extending from −∞to ∞then the boundary conditions are periodic:
after a period β, the state returns to where it started. The sum on the
left means that we sum over all conﬁgurations subject to these periodic
boundary conditions. A quotable conclusion is that ‘statistical physics
takes place in periodic, imaginary time’.
Reψ
Imψ
t
Reψ
Imψ
τ
β
τ
3β
2β
β
0
φ
(a)
(b)
(c)
Fig. 25.2 (a) The trajectory of a com-
plex scalar ﬁeld ψ in time t, where
−∞≤t ≤∞. (b) In statistical physics
the ﬁeld is deﬁned only from imagi-
nary time τ = 0 to τ = β. For Bose
ﬁelds like ψ(τ) or the real scalar ﬁeld
φ(τ) we impose the boundary condition
φ(0) = φ(β) to give a picture of periodic
boundary conditions shown in (c).
This approach can be simply extended to ﬁelds. If ˆH is the Hamil-
tonian describing what’s happening to ﬁelds in three-dimensional space
then we have a partition function given by the ﬁeld integral
Z = Tr
h
e−β ˆ
Hi
=
Z
PBC
Dφ e−
R β
0 dτ
R
d3xLE[φ(x)],
(25.24)
where the periodic boundary conditions (illustrated in Fig. 25.2 and
denoted ‘PBC’) are now φ(0, x) = φ(β, x) and LE[φ] is the Euclidean
Lagrangian density.5 Identifying dτ d3x with d4xE, we see that this is
5Note, however, that these boundary
conditions are appropriate for Bose
ﬁelds only.
For Fermi ﬁelds we must
impose the antiperiodic boundary con-
dition that ψ(0, x) = −ψ(β, x).
the Wick rotated version of the functional integral.
We made the replacement x0 →−iτ and so in momentum space we
must make the replacement p0 →iω. In fact, to guarantee the periodic
boundary conditions are encoded in our functional integral we choose to
work with the Fourier transforms of the ﬁelds. Since statistical physics

232
Statistical ﬁeld theory
involves systems in boxes and explicit factors of volume we will put the
system in such a box of volume V which discretizes momentum space.
The Fourier representation is then given by6
6The rather awkward normalization is
chosen to make the ﬁeld φn(p) di-
mensionless.
This is done with mal-
ice aforethought in order to guaran-
tee that the expressions which will re-
sult make sense in describing a statisti-
cal system.
An important expression
required to manipulate these ﬁelds is
R
dτd3x eiωnτe−ip·x = βVδp,0δωn,0.
φ(τ, x) =
r
β
V
X
n,p
e−iωnτ+ip·x ˜φn(p),
(25.25)
where the frequencies (quantized by the boundary conditions) must sat-
isfy ωn = 2πn
β
with n an integer to ensure the periodicity.7 The ωn’s are
7This is another condition only appro-
priate for Bose ﬁelds.
To impose the
antiperiodic boundary conditions for
Fermi ﬁelds we require that the Mat-
subara frequencies obey ωn = 2π(n+1)
β
.
known as Matsubara frequencies and in our calculations the integral
Takeo Matsubara (1921– )
over imaginary time will become a sum over these frequencies.
We may now state the rules of statistical ﬁeld theory required to ﬁnd
the partition function.
The rules of statistical ﬁeld theory
To ﬁnd the partition function for scalar ﬁeld theory follow the following
recipe
• Write down the Euclidean Lagrangian LE = 1
2(∂τφ)2 + U[φ].
• The partition function is given by the functional integral of the
Euclidean action, with periodic boundary conditions (PBCs)
Z[J] =
Z
PBC
Dφ e−
R
dτd3x (LE[φ]−Jφ),
(25.26)
where φ is a function of τ and x.
• Impose the PBCs with the expansion
φ(τ, x)
=
r
β
V
X
n,p
e−iωnτ+ip·x ˜φn(p),
(25.27)
where ω = 2πn/β.
• Note that we now use
Z
dτ d3x eiωnτ−ip·x = βVδp,0δωn,0.
(25.28)
Example 25.4
Let’s examine the familiar example of free scalar ﬁeld theory in more detail. The
Euclidean Lagrangian is given by
LE = 1
2 (∂τφ)2 + 1
2 (∇φ)2 + m2
2 φ2.
(25.29)
This leads to a partition function
Z[J] =
Z
PBC
Dφ e
−
R
dτd3x
»
1
2 (∂τ φ)2+ 1
2 (∇φ)2+ m2
2 φ2−Jφ
–
.
(25.30)

25.3
Perturbation theory and Feynman rules
233
We will calculate some thermodynamic properties of this theory. We therefore set J =
0 and insert the Fourier expansion of the ﬁelds into the argument of the exponential.
We obtain an argument of
−β
2V
Z
dτd3x
X
n,m,p,q
˜φm(q)e−iωmτ+iq·x[(−iωn)(−iωm)+(−ip)·(−iq)+m2]˜φn(p)e−iωnτ+ip·x
=−β
2V
X
n,m,p,q
˜φm(q)(−ωnωm −p · q + m2)˜φn(p)βVδp,−qδωm,−ωn
= −β2
2
X
n,p
˜φn(−p)(ω2
n + p2 + m2)˜φn(p).
(25.31)
This results in a partition function
Z[J = 0] =
Z
PBC
D ˜φ e−β2
2
P
np ˜φn(−p)(ω2
n+p2+m2) ˜φn(p).
(25.32)
This is, of course, in the form of our favourite (and only) tractable functional integral.
The solution (for J = 0) is that
Z[J = 0] = N det
ˆ
β2(ω2
n + p2 + m2)
˜−1
2 .
(25.33)
This is diagonal in p, allowing us to write ln[Z(J
=
0)] as the expression
−1
2
P
n,p ln
ˆ
β2(ω2
n + p2 + m2)
˜
+const. The sum over n can be done (using a couple
of tricks described in the book by Kapusta and Gale) which allows the replacement
X
n
ln
ˆ
β2(ω2
n + E2
p)
˜
= βEp + 2 ln
“
1 −e−βEp
”
+ const.,
(25.34)
where Ep =
p
p2 + m2.
To obtain the thermodynamic limit we make our usual
replacement for summing over closely spaces levels in phase space, P
p →V
R
d3p
(2π)3 ,
leading to a result
ln Z = V
Z
d3p
(2π)3
»
−1
2βEp −ln
“
1 −e−βEp
”–
.
(25.35)
We then have a Helmholtz energy F = V
β
R
d3p
(2π)3
ˆ 1
2 βEp + ln
`
1 −e−βEp´˜
.
Notice that the zero-point energy leads to a ground-state energy given by E0 =
−∂
∂β ln Z =
V
2
R
d3p
(2π)3 Ep [which is just like the
1
2ℏω from (n + 1
2 ℏω)] and this is
associated with a pressure P0 = −∂F
∂V = E0
V . Our meaningful energies and pressures
will be expressed relative to these vacuum values.
Finally we obtain an energy,
expressed relative to the (arbitrary) ground state energy of
E −E0 = V
Z
d3p
(2π)3
Ep
eβEp −1,
(25.36)
and a pressure
P −P0 = 1
β
Z
d3p
(2π)3 ln
“
1 −e−βEp
”
.
(25.37)
Although we have not learnt anything we couldn’t have extracted with simpler
methods it is, as usual, heartening that the more sophisticated machinery pro-
duces the same results.
The power of statistical ﬁeld theory comes in extract-
ing predictions from unsolvable theories such as φ4 theory, to which we now turn.
25.3
Perturbation theory and Feynman
rules
The joy of statistical ﬁeld theory is that the rules we have already learnt
which allow us to encode perturbation theory in terms of Feynman dia-
grams carry over wholesale to statistical ﬁeld theory. This is not magic;

234
Statistical ﬁeld theory
we have made every eﬀort to construct things this way! We found before
that the generating functional could be expressed in terms of Feynman
diagrams as Z[J] = e
P
0
@ Connected source-to-source
and vacuum diagrams
1
A
. Since the re-
sults of statistical ﬁeld theory depend on ln Z rather than Z we use the
neat result that ln Z[J] = P  Connected source-to-source
and vacuum diagrams

. Remem-
ber that the sources are included in order that we can extract Green’s
functions. If we are just interested in the thermodynamic equilibrium
properties we can turn oﬀthe sources and we have the key result
ln Z[J = 0] =
X
(Connected vacuum diagrams) .
(25.38)
In statistical physics we often ﬁnd ourselves in a situation where the
average value of the operator ⟨ˆφ(x)⟩t ̸= 0. An example is the ordered
magnet encountered in the next chapter. In that case, more useful in
determining the ﬂuctuations in the ﬁelds is the connected correlation
function
Gc(x, y) = ⟨ˆφ(x)ˆφ(y)⟩t −⟨ˆφ(x)⟩t⟨ˆφ(y)⟩t.
(25.39)
This may also be derived8 directly from ln Z too:
8See Exercise 25.2.
Gc(x, y)
=
⟨ˆφ(x)ˆφ(y)⟩t −⟨ˆφ(x)⟩t⟨ˆφ(y)⟩t =
∂2
∂J(x)∂J(y) ln Z[J]

J=0
=
X  All connected diagrams with two
external legs

.
(25.40)
Since there are far fewer connected diagrams than disconnected dia-
grams, this is a wonderfully labour-saving simpliﬁcation. This also ex-
plains why Gc(x, y) is known as the connected correlation function. No-
tice that this is the same result we had in eqn 22.15, where we insisted
on normalizing the generating functional.
We already know how to draw diagrams, so the question we must
ask is how the calculations encoded by these diagrams diﬀer from those
we’ve seen before. Luckily, all of the Feynman rules and momentum
space tricks we’ve built up can be employed with little modiﬁcation.
In the case of scalar ﬁeld theory, the propagator in momentum space
i/(p2
0 −p2 −m2 + iǫ) becomes the Euclidean space version 1/(ω2
n + p2 +
m2). Integrals over all momenta will now involve a sum over Matsubara
frequencies in place of the integral over p0. The conversion rule is that
d4p
(2π)4 →1
β
X
n
Z
d3p
(2π)3 .
(25.41)
As long as we also have (2π)δ(p0
n −p0
m) →βδωn,ωm then all is well.
As an example, we now state the Feynman rules for translating dia-
grams into thermodynamic quantities for φ4 statistical ﬁeld theory:

25.3
Perturbation theory and Feynman rules
235
The Feynman rules for φ4 theory for T ̸= 0
To calculate the amplitude of a diagram in statistical ﬁeld theory
• Each internal propagator line contributes
1
ω2n + p2 + m2 .
• A factor −λ results from each vertex.
• Integrate over all unconstrained internal momenta with a mea-
sure 1
β
P
n
R
d3p
(2π)3 .
• Divide by the symmetry factor.
• An
overall
energy-momentum
conserving
delta
function
(2π)3δ(3)(pin −pout)βδωn,ωm is understood for each diagram.
For vacuum diagrams, this results in a factor β(2π)3δ3(0) →βV
in the thermodynamic limit.
(a)
(b)
Fig. 25.3
(a) The double-bubble di-
agram is the ﬁrst-order correction to
ln Z[J = 0] in φ4 theory. (b) The line-
bubble diagram is the ﬁrst-order correc-
tion to the propagator.
For practice, we now evaluate a few diagrams in statistical ﬁeld theory.
Example 25.5
The ﬁrst-order correction to ln Z[J = 0] is given by the double-bubble diagram shown
in Fig. 25.3(a). The symmetry factor for this one is D = 8, so it makes a contribution
to ln Z of
ln Z(1) = −λVβ
8
"
1
β
X
n
Z
d3p
(2π)3
1
ω2n + p2 + m2
#2
.
(25.42)
The ﬁrst-order correction to the propagator ˜G(1)(k, q) is the line-bubble shown in
Fig. 25.3(b). The symmetry factor is D = 2 and the amplitude for this diagram is
given by9
9Here the incoming line carries four-
momentum q = (ωl, q) and outgoing
line four-momentum k = (ωm, k).
˜G(1)(k, q)
=
1
ω2m + k2 + m2
(
−λ
2
"
1
β
X
n
Z
d3p
(2π)3
1
ω2n + p2 + m2
#)
×
1
ω2
l + q2 + m2 (2π)3δ(3)(k −q)βδωm,ωl,
(25.43)
Example 25.6
We ﬁnish this chapter by examining some of the consequences of these corrections on
the thermodynamics of the scalar ﬁeld with φ4 interactions. The loop integral and
Matsubara sum in brackets in eqns 25.42 and 25.43 can be done.10 In fact, it may
10See Kapusta and Gale, Chapter 3.
be shown that it falls apart into two pieces:
Z
d4p
(2π)4
1
(p0)2 + E2p
+
Z
d3p
(2π)3
1
Ep
„
1
eβEp −1
«
.
(25.44)
The ﬁrst term is independent of temperature and will not concern us here.
The
second term provides the ﬁrst-order thermodynamic correction to ln Z and to the
particle energies.
The resulting contribution to the temperature-dependent part of ln Z is
ln Z(1) = −λβV
8
»Z
d3p
(2π)3
1
Ep
„
1
eβEp −1
«–2
.
(25.45)

236
Statistical ﬁeld theory
This will give a contribution to the pressure that varies11 as T 4 in the limit that
11See Exercise 25.3.
m →0.
Turning to the line-bubble correction to the propagator: we note that in Chap-
ter 16 it was suggested that the stuﬀthat ﬁts between the two external lines of the
propagator, known as the self-energy, gives a correction to the particle dispersion
Ep. (In Chapter 33 we will see that this is true.) For our present purposes we claim
that the correction to Ep is provided by the part in curly brackets in eqn 25.43. In
particular, it is straightforward to show that in the limit that m →0 the correction
to Ep varies as λT 2.
Chapter summary
• Statistical ﬁeld theory is obtained from quantum ﬁeld theory by
a Wick rotation, so that (temperature)−1 behaves like imaginary
time.
• The diagrammatic techniques developed for quantum ﬁelds can
then be used to solve problems in statistical physics.
Exercises
(25.1) (a) What are the dimensions of the ﬁeld in φ4 the-
ory?
Show that the normalization in eqn 25.25
makes the ﬁeld φn(p) dimensionless.
(b) What normalization would be required for non-
relativistic particles? What about Dirac particles?
(c) Investigate the consequences of choosing a dif-
ferent normalization for φ4 theory.
(25.2) Verify that Gc(x, y) may be obtained by diﬀerenti-
ating ln Z[J] as suggested in the text.
(25.3) Verify the T dependence of the corrections to φ4
theory claimed in the chapter, in the limit that
m →0.
(25.4) (a) After Wick rotating the Schr¨odinger equation
and investigating the consequences, show that wave
functions evolve according to ψ(τ) = e−ˆ
Hτψ(0)
and the Heisenberg equation of motion becomes
∂ˆ
AH
∂τ
=
h
ˆH, ˆAH
i
.
(b) For a system described by a Hamiltonian ˆH =
ωˆc†ˆc show that the operators evolve according to
ˆc(τ) = e−ωτ ˆc and ˆc†(τ) = eωτ ˆc†.
(25.5) Using a Wick rotation of
R dE
2π ˜G(E)e−iE(tx−ty) =
R dE
2π ie−iE(tx−ty)/(E −Ep + iǫ), suggest a prop-
agator for describing non-relativistic electrons at
nonzero temperature. By convention the propaga-
tor for T ̸= 0 is deﬁned as minus this quantity,
preventing a proliferation of minus signs.
(25.6) Deﬁne the imaginary time free propagator for the
quantum oscillator as
G(τ) = −⟨T ˆx(τ)ˆx(0)⟩t,
(25.46)
where T is the imaginary time ordering symbol and
the average is a thermal one. Vitally important here
is that ⟨ˆa†ˆa⟩t = ⟨ˆn⟩t = (eβω −1)−1 rather than zero!
(a) Show that
G(τ) = −
1
2mω
ˆ
θ(τ)(⟨ˆn⟩t + 1)e−ωτ + θ(τ)⟨ˆn⟩teωτ
+ θ(−τ)(⟨ˆn⟩t + 1)eωτ + θ(−τ)⟨ˆn⟩te−ωτ˜
.
(25.47)
(b) Using the deﬁnition
˜G(iνn) =
Z β
0
dτ G(τ)eiνnτ,
(25.48)
where νn is a Matsubara frequency satisfying νn =
2πn
β , show that the propagator may be expressed as
˜G(iνn) =
1
2mω
»
1
iνn −ω −
1
iνn + ω
–
=
1/m
(iνn)2 −ω2 .
(25.49)

26
Broken symmetry
26.1 Landau theory
237
26.2 Breaking symmetry with a
Lagrangian
239
26.3 Breaking a continuous sym-
metry: Goldstone modes 240
26.4 Breaking a symmetry in a
gauge theory
242
26.5 Order in reduced dimensions
244
Chapter summary
245
Exercises
245
On the earth the broken arcs; in the heaven, a perfect round.
Robert Browning (1812–1889)
In this chapter we turn to the profoundly important topic of broken
symmetry. It’s hard to overestimate the inﬂuence this concept has had
on condensed matter and particle physics and on quantum ﬁeld theory
in general. We will start by discussing the arena where these ideas were
born: Lev Landau’s theory of the statistical physics of phase transitions.
We then turn to classical ﬁeld theory and will see how broken symmetry
manifests itself in a system described by a Lagrangian.
26.1
Landau theory
Our discussion of Landau’s theory of phase transitions begins with a
simple observation about magnets. We imagine a magnet whose spins
can point either up or down. It’s well known that at high temperatures
each spin is equally likely to be found up or down. The magnetization,
that is, the spatial average of the magnetic moment, is zero. This system
is shown in Fig. 26.1(a). This system has a symmetry: turn all of the
spins through 180◦[as in Fig. 26.1(b)] and the magnetization is still zero.
Of course, each individual moment is pointing in the opposite direction,
but the number pointing in the upwards direction is still half of the total
and the magnetization is still zero. The symmetry here is a global one:
we rotate all of the spins through the same angle, here 180◦.
(a)
(b)
Fig. 26.1 The magnet at T > Tc. The
magnetization, or average moment, is
zero in (a) and, after rotation of each
spin through 180◦, is still zero in (b).
(a)
(b)
Fig. 26.2
For temperatures T < Tc
the spins align along a single direction.
The magnetization in (a) is diﬀerent in
(b), where each spin has been rotated
by 180◦.
It is found experimentally that upon cooling the systems through a
critical temperature Tc, the system undergoes a phase transition and
the magnetization M becomes nonzero as all of the spins line up along a
single direction, as shown in Fig. 26.2(a). The direction along which the
spins align could either be all in the up direction or the down direction.
If we rotate each spin of the aligned system through 180◦[Fig. 26.2(b)]
M is obviously reversed. We say that the system has broken (or lowered)
its symmetry in the ordered phase.
One puzzling feature of this story is the reason why the system chose
to point all of the spins in the one direction rather than the other.
After all, there’s nothing in the Hamiltonian which describes the system
that distinguishes between up and down. The original symmetry of the
system appears to have spontaneously broken. The same thing happens
in the Euler strut shown in Fig. 26.3. A weight is balanced on top of
an elastic strut. If the weight is large enough the strut will buckle. The

238
Broken symmetry
buckling can be either to the left or to the right. There is nothing in
the underlying physics of the strut and weight that allows one to predict
which way it will go.1 The result is that the ground state does not have
1Of course, in real life, some very small
perturbation or ﬂuctuation will have
tipped the system towards its choice of
ground state.
the symmetry of the Hamiltonian describing the system. For the magnet
this Hamiltonian could be ˆH = −J P
i ˆSz
i ˆSz
i+1, where the up direction
is along z. This has no bias either favouring the spins pointing up or
favouring them pointing down.
Lev Landau had a scheme for thinking about phase transitions involv-
ing a breaking of symmetry.2 Since equilibrium in thermodynamic sys-
2Landau’s scheme is a form of mean-
ﬁeld theory, where the magnetization is
described by a uniform ﬁeld M.
We
will meet mean ﬁeld theories through-
out this book, most notably in Chap-
ter 43.
tems relies on both minimizing the internal energy U and maximizing the
entropy S of a system, Landau considered the free energy F = U −TS.
To ﬁnd the equilibrium state of the system we need to minimize F. The
free energy is a function of an order parameter, namely some ﬁeld
describing the system whose thermal average is zero in the T > Tc un-
broken symmetry state and nonzero in the T < Tc broken symmetry
state. For a magnet, the order parameter is simply the magnetization
ﬁeld M. With joyful and deliberate ignorance of any microscopic de-
scription, Landau wrote F(M) as a power series
F = F0 + aM 2 + bM 4 + . . . ,
(26.1)
where a and b are parameters which are independent of M, but may in
principle depend on the temperature. This free energy has the symmetry
M →−M, that is, reversing the magnetization doesn’t aﬀect the en-
ergy. If a is positive we have the free energy shown in Fig. 26.4(a), which
is minimized at M = 0, which is the correct prediction for the high-
temperature regime. If, however the parameter a is negative then we
have the free energy shown in Fig. 26.4(b), which is known, by some, as
Lifshitz’s buttocks. This has two minima at nonzero values of magnetiza-
tion. These minima correspond to the spins all aligning up (M = +M0)
or all aligning down (M = −M0). The previous minimum M = 0 is now
at a position of metastable equilibrium.3 If we take a = a0(T −Tc) (with
3This means that any slight perturba-
tion of the system which has been pre-
pared in a M = 0 state will propel it
into one, or other, of the new minima
that have emerged at ±M0.
a0 a constant) and b to be T-independent, then clearly a is positive for
T > Tc and negative for T < Tc, and so we predict a phase transition at
a temperatures T = Tc.
(a)
(b)
(c)
Fig. 26.3
The Euler strut. (a) This
has a vertical axis of symmetry and can
buckle either (b) one way or (c) the
other, in each case breaking symmetry.
(a)
M
F
(b)
M
F
Fig. 26.4 The Landau free energy for
(a) T > Tc with a minimum at M =
0 and (b) for T < Tc with minima at
±M0.
Example 26.1
We can ﬁnd the minima of F straightforwardly for T < Tc. We set
∂F
∂M = 2aM + 4bM3 = 0,
(26.2)
from which we conclude that the minima occur at
M2
0 = −a
2b .
(26.3)
For T < Tc we have two minima at M = ±M0. The system will choose
one of these as its new ground state and the symmetry will be broken.
When a symmetry is broken in a many-particle system there are four
features4 that one should look out for.
4This classiﬁcation is due to Philip
Anderson and described in detail in
his classic Basic Notions of Condensed
Matter Physics.

26.2
Breaking symmetry with a Lagrangian
239
• Phase transitions We saw that in Landau’s example, the pa-
rameter a in the free energy was temperature dependent. At a
temperature Tc, at which a changes sign, a phase transition takes
place. The transition separates two distinct states of diﬀerent sym-
metry. The low-temperature phase has lost some symmetry, more
precisely it is missing a symmetry element.5
5In fact, you don’t need to regard the
parameters in a model as being depen-
dent on anything:
in the Lagrangian
examples below we won’t, and then the
phase transition is not such a useful
concept.
However, the link between
phase transitions and symmetry break-
ing is a very powerful one that uni-
ﬁes our understanding of an enormous
range of phenomena in Nature.
• New excitations Our philosophy has been that every particle is
an excitation of the vacuum of a system. When a symmetry is bro-
ken we end up with a new vacuum (e.g. a vacuum with M = −M0).
The fact that the vacuum is diﬀerent means that the particle spec-
trum should be expected to be diﬀerent to that of the unbroken
symmetry state (such as M = 0 in our example).
We will see
that new particles known as Goldstone modes can emerge upon
symmetry breaking.6
6See Section 26.3.
• Rigidity Any attempt to deform the ﬁeld in the broken symmetry
state results in new forces emerging. Examples of rigidity include
phase stiﬀness in superconductors, spin stiﬀness in magnets and
the mechanical strength of crystalline solids.
• Defects These result from the fact that the symmetry may be
broken in diﬀerent ways in diﬀerent regions of the system, and are
topological in nature. An example is a domain wall in a ferromag-
net. These are described in Chapter 29.
26.2
Breaking symmetry with a
Lagrangian
Let us now apply the arguments developed for magnets to our La-
grangian treatment of (classical) ﬁeld theory.7 Here instead of follow-
7Classical ﬁeld theory will be enough
to show us the main features of broken
symmetry. See S. Coleman and Peskin
and Schroeder for more details on the
quantum mechanics of broken symme-
try.
ing the ground state magnetization, we’re interested in the ground state
value of φ(x) or (returning brieﬂy to quantum mechanics) the expecta-
tion value of the ﬁeld operator ⟨Ω|ˆφ(x)|Ω⟩, which is the order parameter
in ﬁeld theory. We start with a scalar ﬁeld theory
L = 1
2(∂µφ)2 −U(φ),
(26.4)
where we’ve split oﬀall of the potential energy terms and called them8
8More strictly U(φ) is the potential en-
ergy density. It should not be confused
with the internal energy, which is also
conventionally called U.
U(φ). In the familiar case of φ4 theory we have that
(a)
φ
U(φ)
(b)
φ
U(φ)
Fig. 26.5 Equation 26.5 for (a) µ2 > 0
with a minimum at φ = 0 and (b) for
µ2 < 0 with minima at ±
p
6µ2/λ.
U(φ) = µ2
2 φ2 + λ
4!φ4.
(26.5)
This function resembles the free energy in our magnet example above.
It admits the global symmetry φ(x) →−φ(x). Assuming µ2 is positive
we have a potential with a minimum at φ = 0 as shown in Fig. 26.5(a).
The minimum corresponds to a quantum mechanical ground state, also
known as the vacuum, of ⟨Ω|ˆφ(x)|Ω⟩= 0. As we’ve seen earlier, the
excitations of this ﬁeld are phions with mass m = µ.

240
Broken symmetry
Now we turn to a very interesting possibility: what if we swap the
sign in front of µ2? In that case we have U(φ) = −µ2
2 φ2 + λ
4!φ4 and
again have a potential that looks like Fig. 26.5(b). The minima9 of the
9From ∂U/∂φ = 0 we ﬁnd
0 = −µ2φ + λ
3! φ3,
from which we deduce stationary points
at (0, ±
p
6µ2/λ).
The second deriva-
tive,
∂2U
∂φ2 = −µ2 + λφ2
2 ,
is negative (and equals −µ2) for φ = 0
and is positive (and equals +2µ2) for
the minima at φ0 = ±
p
6µ2/λ.
potential now occur at
φ0 = ±
6µ2
λ
 1
2
,
(26.6)
so we have the choice of two new vacua. The system will spontaneously
choose one and the symmetry φ0 →−φ0 of the ground state is broken.
Next we ask if the new vacuum has the same excitations (a.k.a. par-
ticles) as those of the unbroken symmetry state.
Example 26.2
To do this we choose a vacuum (for deﬁniteness let’s choose +φ0) and we expand the
ﬁeld around this new minimum. The Taylor expansion gives us
U(φ −φ0)
=
U(φ0) +
„∂U
∂φ
«
φ0
(φ −φ0) + 1
2!
„∂2U
∂φ2
«
φ0
(φ −φ0)2 + . . .
=
U(φ0) + µ2(φ −φ0)2 + . . . ,
(26.7)
and since U(φ0) is an ignorable constant, we can write the Lagrangian in terms of
φ′ = φ −φ0 as
L
=
1
2 (∂φ′)2 −µ2φ′2 + O(φ′3).
(26.8)
By comparison with our original scalar ﬁeld theory,10 we ﬁnd from
10For the ordinary scalar ﬁeld La-
grangian, the factor multiplying the
term quadratic in ﬁeld is m2/2, with
m the mass of the particle excitations
in the theory.
eqn 26.8 that the mass of the phion excitations in the broken symmetry
state is now no longer m = µ but has become m =
√
2µ.
An important point to note is that the Lagrangian does not ‘break
symmetry’ itself. Indeed our Lagrangian, just like the Landau free en-
ergy for the magnet, is invariant with respect to the transformation
φ →−φ. The breaking of symmetry is a property of the ground state
(or vacuum) of the system.11
11In the quantum mechanical version
we say that the ground state does not
have the symmetry of the Hamiltonian.
To conclude, the eﬀect of spontaneous symmetry breaking is (i) for φ
or (again, brieﬂy returning to quantum mechanics) the vacuum expec-
tation value ⟨Ω|ˆφ(x)|Ω⟩to acquire a non-vanishing, constant amplitude
φ0, in this case φ0 =

6µ2
λ
1/2
; (ii) the particle excitations of the theory
are the same as before, but now have a mass
√
2µ.
26.3
Breaking a continuous symmetry:
Goldstone modes
The scalar ﬁeld theory had a discrete global symmetry (φ →−φ). More
dramatic and interesting still is the eﬀect of symmetry breaking for a

26.3
Breaking a continuous symmetry: Goldstone modes
241
theory with a continuous global symmetry. A two-component ﬁeld with
the sign of its mass term ﬂipped looks like
L = 1
2

(∂µφ1)2 + (∂µφ2)2
+ µ2
2
 φ2
1 + φ2
2

−λ
4!
 φ2
1 + φ2
2
2 .
(26.9)
This has a global SO(2) symmetry: it is symmetric with respect to
rotations in the (internal) φ1(x)-φ2(x) plane (where the same rotation
takes place at all points in spacetime). The potential U(φ1, φ2) for this
theory is sketched in Fig. 26.6(b) and looks a lot like the bottom of a
wine bottle [as shown in Fig. 26.6(a)]. There are an inﬁnite number of
potential minima, and these are found to lie on a circle [Fig. 15.4(c)]
whose equation can easily be found12 to be φ2
1 + φ2
2 = 6µ2
λ .
12To quickly show this, you can write
U(x) = −µ2
2 x+ λ
4! x2 where x = φ2
1+φ2
2,
and then ∂U/∂x = 0 yields the result.
(a)
(b)
(c)
φ1
φ2
Fig. 26.6
(a) The potential for the
SO(2) symmetry breaking looks like
the bottom of a punted wine bottle.
(b) There is a maximum at the point
φ1 = φ2 = 0, but surrounding this
there is a set of minima which lie on
a circle. (c) The circle of minima are
shown on a φ1-φ2 plot (this is therefore
viewing the surface sketched in (b) from
‘above’).
The symmetry can then be
broken by choosing a particular point
in the circle of minima and setting this
to be the ground state. We can then ex-
amine small deviations away from that
point.
Fig.
26.7
Breaking
symmetry
at
the position (φ1, φ2) =
„
+
q
6µ2
λ , 0
«
.
There are two possible excitations: the
particle can oscillate up and down in
the φ1 direction (a massive excitation)
or trundle round in the gutter (a mass-
less excitation).
Let’s break the symmetry by supposing the system chooses a par-
ticular vacuum.
Each is as good as any other, so for computational
simplicity, let’s choose (φ1, φ2) =

+
q
6µ2
λ , 0

and expand around this
broken symmetry ground state using variables φ′
1 = φ1 −
q
6µ2
λ
and
φ′
2 = φ2. This is illustrated in Fig. 26.7.
Example 26.3
With the potential U(φ1, φ2) given by
U(φ1, φ2) = −µ2
2 (φ2
1 + φ2
2) + λ
4! (φ2
1 + φ2
2)2,
(26.10)
Taylor expanding around the minimum (φ1, φ2) =
„q
6µ2
λ , 0
«
gives ∂2U/∂φ2
1 = 2µ2
and ∂2U/∂φ2
2 = 0 and so we obtain (ignoring constant terms and truncating the
expansion at order φ2)
L = 1
2
ˆ
(∂φ′
1)2 + (∂φ′
2)2˜
−µ2(φ′
1)2 + O(φ′3).
(26.11)
Equation 26.11 shows that the particles of the φ′
1 ﬁeld now have mass
m =
√
2µ, but that there is no quadratic term for the φ′
2 term: thus the
particles of the φ′
2 ﬁeld are completely massless! This seemingly magical
result is less surprising if we look again at the potential landscape shown
in Fig. 26.7. We can see immediately that excitations in the φ1 direction
will cost energy since such excitations have to climb up the walls of the
potential. However, a small movement in the φ2 direction corresponds to
rolling round the gutter. Such motion has no opposing force as it involves
no change in potential energy and therefore costs no energy, exactly what
we would expect for a massless excitation. In other words, we still have
particles in the φ2 ﬁeld, it’s just that their dispersion starts at the origin,
rather than from some oﬀset determined by the mass. In the language
of condensed matter physics one would say that the excitations in the
φ2 ﬁeld are gapless, rather than massless, but it amounts to the same
thing.

242
Broken symmetry
This vanishing of the mass is a manifestation of Goldstone’s
theorem, which says that breaking a continuous symmetry always
results in a massless excitation, known as a Goldstone mode. The
massless particle associated with this excitation is known as a
Goldstone boson.
Jeﬀrey
Goldstone
(1933–
).
The
bosons had previously been discovered
by Yoichiro Nambu (1921– ) in the con-
text of superconductivity.
Example 26.4
Perhaps more illuminating is the complex scalar ﬁeld theory. In this theory we’ll
break global U(1) symmetry. Of course U(1) is isomorphic to SO(2), so we won’t
have anything too diﬀerent from the previous example.
The U(1) theory, with a
positive mass term, reads
L = (∂µψ)†(∂µψ) + µ2ψ†ψ −λ(ψ†ψ)2.
(26.12)
This theory enjoys a global U(1) symmetry: a transformation ψ →ψeiα has no eﬀect
on the Lagrangian. We switch to ‘polar coordinates’13 via ψ(x) = ̺(x)eiθ(x) and this
13See chapter 12, although note that
there we were dealing with a non-
relativistic theory and wrote Ψ(x) =
p
ρ(x)eiθ(x).
Here, in dealing with
the relativistic theory we write ψ(x) =
̺(x)eiθ(x) and so
R
d3x |̺(x)|2 [rather
than
R
d3x ρ(x)] gives the number den-
sity of particles.
theory reads
L = (∂µ̺)2 + ̺2(∂µθ)2 + µ2̺2 −λ̺4,
(26.13)
where the two ﬁelds are now called ̺(x) and θ(x). The symmetry transformation for
this version is ̺ →̺ and θ →θ + α.
Let’s break this symmetry. The potential part is U = −µ2̺2 + λ̺4 whose minima
are on a circle of radius ̺ =
q
µ2
2λ . We set ̺0 =
p
µ2/2λ and (arbitrarily, but with
convenience in mind) choose a minimum by selecting θ0 = 0, then expand around
the minimum with ̺′ = ̺ −̺0 and θ′ = θ −θ0, to obtain
L =
“
µ2
2λ
”
(∂µθ′)2
(θ′-ﬁeld terms)
+(∂µ̺′)2 −2µ2̺′2 −4
“
µ2λ
2
” 1
2 ̺′3 −λ̺′4
(̺′-ﬁeld terms)
+
»
̺′2 +
“
2µ2
λ
” 1
2 ̺′
–
(∂µθ′)2 + . . .
(interaction terms) ,
(26.14)
where constant terms have been dropped and terms have been arranged according
to the contributions of the two individual ﬁelds and then the interactions between
them. As might be expected, there is no term in θ′2 and so the θ′-ﬁeld is massless.
As before, this is because it costs nothing to roll in the gutter. The mass of the
̺′-ﬁeld has been shifted to m =
√
2µ.
26.4
Breaking a symmetry in a gauge
theory
An extraordinary feature that emerges upon breaking a symmetry in a
gauge theory was spotted by Anderson, Higgs, Kibble, Guralnik, En-
glert and Brout and is famously known as the Higgs mechanism.14 As
14Philip Anderson (1923–), Francois
Englert (1932– ), Robert Brout (1928–
2011), Peter Higgs (1929– ), Tom Kib-
ble (1932– ), Gerald Guralnik (1936– )
and Carl Hagan (1937– ).
Higgs and
Englert shared the 2013 Nobel Prize
in physics for their discovery (Englert
and Brout worked together, but Brout
died before the prize was awarded).
Anderson (who won the Nobel prize
in 1977 for other work) had a non-
relativistic version in 1962 which was
inspired by superconductivity, and in
1964 all the others (Brout and En-
glert published ﬁrst, closely followed by
Higgs) produced relativistic treatments
of what we now call the Higgs mecha-
nism (though only Higgs mentioned the
massive boson).
we’ve said previously (see Chapter 14), a Lagrangian which has a local
symmetry contains gauge ﬁelds. The simplest example is the gauged
complex scalar ﬁeld theory, whose Lagrangian (with a ﬂipped signed for
the mass term) is given by
L = (∂µψ† −iqAµψ†)(∂µψ + iqAµψ) + µ2ψ†ψ −λ(ψ†ψ)2 −1
4FµνF µν,
(26.15)

26.4
Breaking a symmetry in a gauge theory
243
which is symmetric under the local transformation ψ →ψeiα(x) as long
as we also transform Aµ →Aµ −1
q∂µα(x). This Lagrangian describes a
world made up of two sorts of oppositely charged, massive scalar particles
with energies Ep = (p2 + µ2)
1
2 and two sorts of transversely polarized
photons with energies Ep = |p|, which are, of course, massless.
Now we’ll see what happens when we break symmetry in a system
with a local symmetry. Working in polars, the ground state has the
ﬁeld ψ(x) = ̺(x)eiθ(x) and takes on a unique phase angle θ(x) = θ0 for
all x. We are no longer permitted to change the phase of the ground
state at diﬀerent values of x (local symmetry) or indeed change the
phase of the ground state for the entire system (global symmetry). The
broken symmetry ground state therefore breaks global symmetry and,
as a consequence, local symmetry too.
Next we turn our attention to the excitations. Remember that the
Lagrangian is gauge invariant so we are able to make gauge transforma-
tions to simplify the physics as much as possible. In the next example
we show that the gauge transformation we make to elucidate the par-
ticle spectrum of the broken symmetry system reveals something quite
exciting.
Example 26.5
In polar coordinates, the second bracketed term in eqn 26.15 can be written
∂µψ + iqAµψ = (∂µ̺)eiθ + i(∂µθ + qAµ)̺eiθ.
(26.16)
Thus we notice that Aµ enters the theory as
Aµ + 1
q ∂µθ ≡Cµ,
(26.17)
and so we will replace Aµ by Cµ, thereby simplifying the term (∂µψ†−iqAµψ†)(∂µψ+
iqAµψ) = (∂µ̺)2 + ̺2q2CµCµ.
The replacement is a gauge invariant one since
Fµν = ∂µAν −∂νAµ = ∂µCν −∂νCµ. The Lagrangian in eqn 26.15 can now be
written in terms of ̺ and Cµ ﬁelds as
L = (∂µ̺)2 + ̺2q2C2 + µ2̺2 −λ̺4 −1
4F µνFµν,
(26.18)
where we use the shorthand C2 ≡CµCµ.
Now to break the symmetry! The minima of the potential are again on the circle
at ̺ =
q
µ2
2λ . We choose the symmetry to be broken with ̺0 =
q
µ2
2λ and θ0 = 0.
The expansion that reveals the excitations above the new ground state is most easily
done by expanding in terms of a ﬁeld χ, deﬁned as
χ
√
2 = ̺ −̺0. Ignoring constant
terms we obtain
L
=
1
2 (∂µχ)2 −µ2χ2 −
√
λµχ3 −λ
4 χ4
−1
4FµνF µν + M2
2 C2
+q2
„ µ2
λ
« 1
2
χC2 + 1
2 q2χ2C2 + . . . ,
(26.19)
where M = q
q
µ2
λ .

244
Broken symmetry
As usual, we have the contribution that derives from the radial ﬁeld (here
called χ), which has a mass
√
2µ. However, there is a big surprise here
in the second line of eqn 26.19: the theory also now contains a massive
vector ﬁeld Cµ, whose particles have mass M. But what of the θ ﬁeld?
It has completely disappeared! The excitations of the θ ﬁeld, which were
massless in the global symmetry breaking version, have disappeared and
have been replaced by those of a massive vector ﬁeld15 Cµ(x). As Sidney
15Recall from Chapter 13 that the La-
grangian for a massive vector ﬁeld is
L = −1
4F µνFµν + m2
2 AµAµ.
Coleman put it, it’s as if the massless photon ﬁeld Aµ(x) has eaten the
Goldstone bosons from the θ ﬁeld and grown massive, changing its name
to Cµ(x). The theory now describes one sort of scalar particle, which is
an excitation in the χ(x) ﬁeld with energy Ep = (p2 + 2µ2)
1
2 , and three
sorts of massive vector particle, which are excitations in the Cµ(x) ﬁeld
with energies Ep =
h
p2 +

q2µ2
λ
i 1
2 .
One might now worry that the loss of the Goldstone bosons means
that we have lost some of the precious degrees of freedom in the mathe-
matical description of the ﬁelds. This worry is unfounded because upon
symmetry breaking we have the change
 2 × massive scalar particles
2 × massless photon particles

→
 1 × massive scalar particles
3 × massive vector particles

,
(26.20)
that is, four types of particle are excitable before and after we break
symmetry.
So where did the Goldstone boson go? The answer becomes clear when
we reexamine what we did to remove it: we made the change of variables
Aµ + 1
q∂µθ = Cµ, which is a gauge transformation! If the Goldstone
particle can be removed with a simple gauge transformation then it
can’t have been there in the ﬁrst place. In other words, it must be pure
gauge.16 Another way of describing the Higgs mechanism, therefore, is
16The notion of a pure gauge will ap-
pear again in our treatment of topolog-
ical eﬀects in Chapter 29.
the removal by gauge transformation of all Goldstone modes.
This feature of symmetry breaking, where the Goldstone mode may
be removed and turned massive through combination with a gauge ﬁeld,
has become one of the most important in modern physics. Historically
it told particle physicists, puzzled by that apparent lack of Goldstone
particles in Nature, that symmetry breaking allows another way. We
will return to this undeniably important topic in Chapter 46.
26.5
Order in reduced dimensions
Field theory allows us to ask questions about systems which have less (or
more!) than the ordinary three spatial dimensions of our world. One fa-
mous result is the Coleman–––Mermin–––Wagner theorem, which tells
Sidney Coleman (1937–2007)
N. David Mermin (1935– )
Herbert Wagner (1935– )
us that symmetry breaking is impossible in two or fewer spatial dimen-
sions in systems with an order parameter with a continuous symmetry.
We’ll show that this is the case for complex scalar ﬁeld theory with
spontaneous symmetry breaking.17 The idea of the proof is to ask what
17We use the theory of a complex scalar
ﬁeld ψ(x) since there a continuous U(1)
internal symmetry is broken, leading to
the emergence of a Goldstone mode.
The presence of the Goldstone mode is
crucial here.
the ﬂuctuations look like at one point in space. We get this by evaluating
the Euclidean (two-point) correlation function at the origin. We want18
18Strictly we should write the integra-
tion measure as
R
DψDψ†, but we will
abbreviate it here to save on clutter.

Exercises
245
G(0, 0) = ⟨| ˆψ(0)|2⟩
=
1
Z
Z
Dψ |ψ(0)|2e−SE[φ]
=
lim
x→0
1
Z
Z
Dψ ψ(x)ψ†(0)e−SE[φ]. (26.21)
We can evaluate this since we know that the propagator is given in
d-dimensional Euclidean space by
G(x, y)=⟨ˆψ(x) ˆψ†(y)⟩= 1
Z
Z
Dψ ψ(x)ψ†(y)e−SE[φ] =
Z
ddp
(2π)d
e−ip·(x−y)
p2 + m2 .
(26.22)
If the system breaks symmetry then the propagating particles will be
Goldstone particles. These are massless, so give rise to a ﬂuctuation
with m = 0 and result in a correlation function
G(0, 0)
=
Z
ddp
(2π)d
1
p2 ,
(26.23)
whose integrand contains a singularity at p = 0. For d > 2 the singular-
ity is integrable and the ﬂuctuations in the ﬁeld are well behaved. For
d ≤2 the singularity is not integrable and the ﬂuctuations diverge. The
divergent inﬂuence of Goldstone bosons breaks up the order in the sys-
tem. We conclude that symmetry breaking is impossible in our complex
scalar ﬁeld theory for d ≤2.
A simple example of the Coleman–
Mermin–Wagner theorem can be found
in the behaviour of a ferromagnet.
Magnons (quantized spin waves) are
bosons and cost energy Ep = αp2 (in
the low p limit) where α > 0 is a con-
stant. Thus the number of magnons ex-
cited at temperature T is proportional
to
Z d|p| |p|d−1
eβEp −1 .
The integrand is proportional to |p|d−3
at low momentum (assuming T ̸= 0),
and therefore diverges when d ≤2.
Thus long-range order in a ferromag-
net is wiped out by spin waves at all
nonzero temperatures in two or fewer
dimensions.
This argument uses no
quantum ﬁeld theory, and holds only
in this speciﬁc case (the analogous ar-
gument is more elaborate for antifer-
romagnets, see the book by Auerbach)
but it is apparent that the mathemat-
ical origin is the same as that used for
G(0, 0) in eqn 26.23, namely that an in-
tegral of this form diverges when d ≤2.
Chapter summary
• In many phase transitions, symmetry is broken below Tc and the
ground state possesses only a subset of the symmetry of the Hamil-
tonian.
• Breaking a continuous symmetry results in a massless excitation,
known as a Goldstone mode or Goldstone boson. These are not
stable in two or fewer dimensions and symmetry breaking is then
not possible (Coleman–Mermin–Wagner theorem).
• In a gauge theory, the Goldstone mode can be removed and be-
comes massive through combination with a gauge ﬁeld (the Higgs
mechanism).
Exercises
(26.1) In this problem we prove that if the vacuum is in-
variant under an internal symmetry then a multi-
plet structure (that is, a diﬀerent ﬁeld with the same
energy) will emerge.
Consider creation operators for two types of parti-

246
Broken symmetry
cles ˆφ†
A and ˆφ†
B with the property
h
ˆQN, ˆφ†
A
i
= ˆφ†
B,
(26.24)
for some generator of a symmetry group, such that
[ ˆQN, ˆH] = 0. We impose the condition
ˆQN|0⟩= 0.
(26.25)
(a) Show that eiα ˆ
QN|0⟩= |0⟩.
(b) Show also that
EA = EB.
(26.26)
where ˆH ˆφ†
A|0⟩= EA ˆφ†
A|0⟩and ˆH ˆφ†
B|0⟩= EB ˆφ†
B|0⟩.
Note that symmetry breaking occurs in the con-
trasting case that ˆQN|0⟩̸= 0.
(26.2) The Fabri–Picasso theorem demonstrates that if a
Lagrangian is invariant under an internal symmetry
with charge operator ˆQN =
R
d3x ˆJ0, then there are
two possibilities: either
(i) (as above) we have ˆQN|0⟩= 0; or
(ii) ˆQN|0⟩has an inﬁnite norm.
Here we prove this fact.
(a) By using the translation operator eiˆp·a show that
⟨0| ˆJ0
N(x) ˆQN|0⟩= ⟨0| ˆJ0
N(0) ˆQN|0⟩.
(26.27)
(b) By considering the matrix element ⟨0| ˆQN ˆQN|0⟩
along with the result of (a), show that either
ˆQN|0⟩= 0 or ˆQN|0⟩has an inﬁnite norm.
Note that if ˆQN|0⟩has an inﬁnite norm, then it does
not exist in the same space as |0⟩. This is the case
in spontaneous symmetry breakdown. In this case,
the state ˆQN|0⟩is not zero, but another possible
vacuum state. See Aitchison and Hey, Chapter 17
for more details.
(26.3) We’ll work through a famous proof of Goldstone’s
theorem. This can be found in many books. Start
with a theory with a continuous symmetry with
charge ˆQN, where ˆQN|0⟩̸= 0. Consider a ﬁeld ˆφ(y)
which is not invariant under ˆQN such that
h
ˆQN, ˆφ(y)
i
= ˆψ(y),
(26.28)
where ˆψ is some other ﬁeld. We are going to exam-
ine ⟨0| ˆψ(0)|0⟩, which we assume takes on a nonzero
value when symmetry is broken.
(a) Show that
∂
∂x0 ⟨0| ˆψ(0)|0⟩= −
Z
dS · ⟨0|
h
ˆJ N(x), ˆφ(0)
i
|0⟩,
(26.29)
where J N is the space-like part of the Noether cur-
rent. Hint: You should assume ∂µ ˆJµ
N = 0.
Since the commutator in the previous equation
contains local operators potentially separated by
a
large
space-like
interval,
we
conclude
that
⟨0| ˆψ(0)|0⟩is independent of time.
(b) Insert a resolution of the identity to show
Z
d3x
X
n
h
⟨0| ˆJ0
N(0)|n⟩⟨n|ˆφ(0)|0⟩e−ipn·x
−
⟨0|ˆφ(0)|n⟩⟨n| ˆJ0
N(0)|0⟩eipn·xi
̸= 0.
(26.30)
(c) Evaluate the spatial integral to verify
X
n
(2π)3δ(3)(pn)
h
⟨0| ˆJ0
N(0)|n⟩⟨n| ˆψ(0)|0⟩eip0
n·x0
−⟨0| ˆψ(0)|n⟩⟨n| ˆJ0
N(0)|0⟩e−ip0
n·x0i
̸= 0.
(26.31)
From (a) we conclude that this expression is inde-
pendent of x0.
(d) Argue that for |n⟩= |0⟩, eqn. 26.31 vanishes.
(e) Argue that if |n⟩describes a massive particle
state then matrix element ⟨n| ˆJ0
N|0⟩must vanish.
(f) Argue that if we want ⟨n| ˆJ0
N|0⟩̸= 0 then we re-
quire p0
n →0 as pn →0.
(g) This proves Goldstone’s theorem:
the states
linked to the ground state via the Noether current
are massless Goldstone modes. Convince yourself
that this is so! Again, see Aitchison and Hey for
more details.
(26.4) Consider another gauge theory:
complex scalar
electrodynamics with a symmetry breaking mass
term, whose Lagrangian is
L = −1
4FµνF µν + |Dµψ|2 −V (ψ),
(26.32)
where V (ψ) = −m2(ψ†ψ) + λ
2 (ψ†ψ)2 and Dµ =
∂µ + iqAµ.
(a) Take the broken symmetry ground state to be
ψ0 =
“
m2
λ
” 1
2 .
Expand about this using ψ =
ψ0 +
1
√
2(φ1 + iφ2) and show that the potential of
the broken symmetry theory is
U(φ1, φ2) = −1
2λm4 + m2φ2
1 + O(φ3
i ),
(26.33)
exactly as in the case without electromagnetism.
(b) Consider the kinetic energy term. Show that,
on symmetry breaking, this becomes
|Dµψ|2
=
1
2(∂µφ1)2 + 1
2(∂µφ2)2
(26.34)
+
√
2qψ0Aµ∂µφ2 + q2ψ2
0AµAµ + . . .
Interpret this result in terms of massive photons.
This example shows that the massive gauge parti-
cle in a broken symmetry system may be found by
expanding the covariant derivative.

27
Coherent states
27.1 Coherent states of the har-
monic oscillator
247
27.2 What do coherent states look
like?
249
27.3 Number,
phase
and
the
phase operator
250
27.4 Examples of coherent states
252
Chapter summary
253
Exercises
253
’Tis all in pieces, all coherence gone
John Donne (1572–1631)
There exists a class of problems in which a macroscopic number of
quanta all pile into the same momentum state. Examples include the
laser, where we put a large number of photons in the same wave-vector
state into a cavity, and the superﬂuid, where a macroscopic number of
Bose particles sit in a zero-momentum state. The best way to describe
such systems uses a set of basis states discovered by Schr¨odinger and
christened coherent states by Roy Glauber. A coherent state is an
Roy Glauber (1925– )
eigenstate of an annihilation operator. In real space coherent states are
Gaussian wavepackets. As the occupation of the coherent state becomes
large, the wavepacket becomes more and more narrow, increasingly re-
sembling an object with classical properties. In this chapter we examine
the properties of these states. Having built up quantum ﬁeld theory from
an analogy between identical, non-interacting particles and harmonic os-
cillators we start with the coherent states of the harmonic oscillator.
27.1
Coherent states of the harmonic
oscillator
The harmonic oscillator has a Hamiltonian ˆH = ω
 ˆa†ˆa + 1
2

. Recall
from Chapter 2 that this was obtained from the Hamiltonian1 ˆH =
1In this chapter we use reduced vari-
ables ˆP 2 =
ˆp2
2mω and ˆQ2 =
1
2 mωˆx2,
and hence
ˆQ =
1
√
2 (ˆa + ˆa†),
ˆP = −
i
√
2 (ˆa −ˆa†).
ω( ˆP 2 + ˆQ2) by the introduction of creation and annihilation operators
ˆa and ˆa†. This produced energy eigenstates |n⟩containing n quanta
and with energy eigenvalue (n + 1
2)ω.
Thus ⟨n|( ˆP 2 + ˆQ2)|n⟩= n +
1
2, but P and Q are individually not good quantum numbers.2 For a
2In fact ⟨ˆQ⟩= ⟨ˆP⟩= 0 for energy
eigenstates.
classical harmonic oscillator, the position and momentum behave like
Q = Q0 cos(ωt −ϕ) and P = −P0 sin(ωt −ϕ), so that both quantities
oscillate but there is a deﬁnite 90◦phase relationship between them.3 A
3This feature is not captured in our
quantum mechanical treatment of the
harmonic oscillator in Chapter 2, since
energy eigenstates |n⟩are, by deﬁni-
tion, stationary states and individually
do not possess any signature of any-
thing oscillating at angular frequency
ω.
Coherent states will achieve this
by being combinations of energy eigen-
states. Coherent combination will give
rise to interference, and this will recover
wavelike behaviour.
quantum mechanical state that possesses this phase relationship is what
we are after, and so we are led to consider eigenstates of the operator
combination ˆQ+i ˆP in which this phase relationship is hard wired. Since
ˆa = ( ˆQ + i ˆP)/
√
2, we thus seek eigenstates of ˆa. We therefore deﬁne a
coherent state |α⟩by
ˆa|α⟩= α|α⟩,
(27.1)
where ˆa is the annihilation operator and α is the eigenvalue (which will

248
Coherent states
be a complex number).4 Of course, a coherent state will be a sum of many
4There is no restriction on α being real
since ˆa is manifestly not Hermitian.
energy eigenstates and so we may write |α⟩= P∞
n=0 cn|n⟩. Substituting
this into eqn 27.1 leads to the recursion relation cn+1 = αcn/√n + 1,
and hence5
5In eqn 27.2 we have used |n⟩
=
(ˆa†)n
√
n! |0⟩in the second line.
|α⟩
=
c0

|0⟩+
α
√
1!
|1⟩+ α2
√
2!
|2⟩+ α3
√
3!
|3⟩+ ...

=
c0

1 + α
1!ˆa† + α2
2! (ˆa†)2 + α3
3! (ˆa†)3 + ...

|0⟩,
(27.2)
and normalization ⟨α|α⟩= 1 ﬁxes c0 = e−|α|2/2. Thus we can write the
coherent state compactly as
|α⟩= e−|α|2
2 eαˆa†|0⟩.
(27.3)
The amplitude to ﬁnd |α⟩in the state |n⟩is cn = ⟨n|α⟩= e−|α|2
2 αn/
√
n!,
corresponding to a probability density Pn = |cn|2 = e−|α|2|α|2n/n! and
this represents a Poisson distribution (see Fig. 27.1). In the next exam-
ple, we establish some basic properties of this probability distribution of
coherent states.
n
Pn
Fig. 27.1
The Poisson distribution
Pn = e−|α|2|α|2n/n! = e−⟨n⟩⟨n⟩n/n!
plotted for |α|2 = ⟨n⟩= 4. Note that
a coherent state consists of a sum of
harmonic oscillator states: the quantity
Pn represents the probability of ﬁnding
this coherent state in the nth harmonic
oscillator state.
Example 27.1
We don’t know the number of quanta in a coherent state but we can work out the
average number6
6In fact this whole example can be done
very quickly if you use a property of
a Poisson distribution which is that its
mean and variance are the same. In this
case, the mean ⟨ˆn⟩is |α|2, and hence so
is the variance, meaning that ∆n, the
square root of the variance, is simply
|α|.
⟨ˆn⟩= ⟨α|ˆa†ˆa|α⟩= |α|2.
(27.4)
We can calculate the uncertainty ∆n in the number of quanta in a coherent state
from ∆n =
p
⟨ˆn2⟩−⟨n⟩2. Thus by working out
⟨ˆn2⟩= ⟨α|ˆa†ˆaˆa†ˆa|α⟩= |α|4 + |α|2,
(27.5)
we ﬁnd that the uncertainty ∆n is
∆n = |α|,
(27.6)
and therefore the fractional uncertainty is
∆n
⟨ˆn⟩= |α|
|α|2 =
1
|α| .
(27.7)
We conclude that although we don’t know exactly how many quanta are in a coherent
state, the fractional uncertainty in the number tends to zero as α (and hence the
average occupation) tends to inﬁnity.
Example 27.2
In this example we see how coherent states respond to an operator ˆU(θ) = e−iθˆn,
where θ is a real number and ˆn is our usual number operator. Let’s practise ﬁrst on
the state |n⟩(where ˆn|n⟩= n|n⟩). This is easily done when we realize that ˆU(θ) can
be written as a power series and hence
ˆU(θ)|n⟩=
„
1 + (−iθˆn) + (−iθˆn)2
2!
+ · · ·
«
|n⟩= e−iθn|n⟩,
(27.8)

27.2
What do coherent states look like?
249
since any power of the operator ˆn on the state |n⟩just gives that power of the number
n. This now allows us to work out
ˆU(θ)|α⟩= e−|α|2
2
∞
X
n=0
(αe−iθ)n
√
n!
|n⟩= |αe−iθ⟩.
(27.9)
Thus the action of ˆU(θ) on the coherent state |α⟩is to turn it into another coherent
state with an eigenvalue multiplied by e−iθ.
This last result tells us that the time-dependence of the coherent state
|α⟩under the Hamiltonian ˆH = (ˆn + 1
2)ω is given by7
7Equation 27.10 should be contrasted
with the analogous result for energy
eigenstates of the harmonic oscillator.
Equation 27.8 gives
|n(t)⟩= e−i(n+ 1
2 )ωt|n(0)⟩,
so
in
this
case
the
only
time-
dependence in the state is found in the
uninteresting phase factor.
|α(t)⟩= e−i ˆ
Ht|α(0)⟩= e−iωt/2|α(0)e−iωt⟩.
(27.10)
The exponential prefactor e−iωt/2 is a simple phase factor originating
from the zero-point energy, but the main message should be read from
the state |α(0)e−iωt⟩: coherent states remain coherent over time and the
eigenvalue α executes circular motion around the Argand diagram at an
angular speed ω. It looks as if we have recovered the coherent oscillatory
behaviour we were after. But is there a way to visualize these states?
27.2
What do coherent states look like?
To see what a state looks like as a function of some coordinate Q we
project it along Q to form a wave function, via the amplitude ⟨Q|α⟩.
Example 27.3
Let |Q⟩and |P⟩be the eigenstates of ˆQ and ˆP. We write ˆa|α⟩=
1
√
2
“
ˆQ + i ˆP
”
|α⟩=
α|α⟩and act on the right with a state ⟨Q| to obtain
1
√
2
⟨Q|
“
ˆQ + i ˆP
”
|α⟩= α⟨Q|α⟩.
(27.11)
Since ˆQ|Q⟩= Q|Q⟩and ˆP = −i ∂
∂Q , we can make the replacements ⟨Q| ˆQ = Q⟨Q|
(since position is real) and ⟨Q|i ˆP =
∂
∂Q ⟨Q|, from which we obtain the diﬀerential
equation for the wave function:
∂
∂Q⟨Q|α⟩= −
“
Q −
√
2α
”
⟨Q|α⟩.
(27.12)
This equation has the normalized solution
⟨Q|α⟩=
1
π
1
4
e−(Q−
√
2α)2/2,
(27.13)
so the wave function is a Gaussian. This one is displaced by
√
2α = ⟨ˆQ⟩+ i⟨ˆP⟩. You
can also show that the wave function in momentum space is given by
⟨P|α⟩=
1
π
1
4
e−(P +i
√
2α)2/2.
(27.14)
Using the position and momentum wave functions we can work out the uncertainty
in position and momentum. This is most easily done via the deﬁnitions
(∆Q)2 = ⟨ˆQ2⟩−⟨ˆQ⟩2,
(∆P)2 = ⟨ˆP 2⟩−⟨ˆP⟩2.
(27.15)

250
Coherent states
We ﬁnd (Exercise 27.3) that
(∆P)2(∆Q)2 = 1
4 .
(27.16)
Comparing this with the uncertainty principle ∆Q∆P ≥1/2 shows that the coherent
states have the minimum possible value of uncertainty. This is the sense in which
coherent states are the closest quantum mechanics allows us to come to classical
objects which are localized in position and momentum space.
Finally we ask what these states do as a function of time.
We have
shown that the state |α(0)⟩evolves to |α(t)⟩(apart from an uninteresting
phase factor) where α(t) = α(0)e−iωt. An immediate consequence of
this is that the expectation value of the position and momentum of
our state may be calculated.
Since we have ⟨ˆa⟩= ⟨α|ˆa|α⟩= α and
⟨ˆa†⟩= ⟨α|ˆa†|α⟩= α∗, we also have
⟨ˆQ⟩= α + α∗
√
2
and
⟨ˆP⟩= −iα −α∗
√
2
,
(27.17)
and we ﬁnd that (writing α(0) = α0eiϕ)
⟨ˆQ⟩=
√
2α0 cos(ωt −ϕ)
and
⟨ˆP⟩= −
√
2α0 sin(ωt −ϕ),
(27.18)
meaning that the expectation values execute simple harmonic motion
(see Fig. 27.2). These results are exactly as we expected for the classical
harmonic oscillator at the beginning of the chapter.
Q
P
“blob”
Fig. 27.2 A coherent state can be pic-
tured as a ‘blob’ orbiting the origin in a
plot of P against Q. Its mean position
is given by ⟨ˆQ⟩=
√
2Re α and ⟨ˆP⟩=
√
2Im α, where α(t) = α(0)e−iωt.
It
is a blob, rather than a point, because
there is uncertainty in both Q and P,
given by ∆Q = ∆P =
1
√
2 (see Exer-
cise 27.3).
27.3
Number, phase and the phase
operator
Because α is a complex number, we can write α = |α|eiθ and obtain
|α⟩= e−|α|2
2

1 + |α|eiθˆa†
1!
+ |α|2e2iθ(ˆa†)2
2!
+ |α|3e3iθ(ˆa†)3
3!
+ ...

|0⟩.
(27.19)
If we diﬀerentiate with respect to the phase θ we ﬁnd that
−i ∂
∂θ|α⟩= ˆn|α⟩.
(27.20)
This implies that just as we have conjugate variables momentum and
position linked via ˆp = −i ∂
∂x, we identify conjugate variables n and θ.
However, a closer examination of the phases of states shows that some
care is needed in their interpretation. The deﬁnition of an operator that
measures the phase of a system turns out to be rather problematical.
One possibility suggested by Dirac is to make a phase operator via
ˆa
=
ei ˆφ√
ˆn,
ˆa†
=
√
ˆne−i ˆφ.
(27.21)
These already look rather odd in that it’s unclear how to interpret the
square root of the number operator. We’ll examine the consequences of
these deﬁnitions.

27.3
Number, phase and the phase operator
251
Example 27.4
The exponential operators can be rearranged to read
ei ˆφ
=
ˆa(ˆn)−1
2 ,
e−i ˆφ
=
(ˆn)−1
2 ˆa†.
(27.22)
Since we know that ˆa†ˆa = ˆn, then we must also have
e−i ˆφei ˆφ = ˆn−1
2 ˆnˆn−1
2 = 1.
(27.23)
However, if we try to ﬁnd ei ˆφe−i ˆφ we encounter a problem since
ei ˆφe−i ˆφ = (ˆn)−1(ˆn + 1) ̸= 1,
(27.24)
which means that the exponential operator ei ˆφ is not unitary. Since these exponential
operators are not unitary, the phase operator ˆφ cannot be Hermitian8 and so cannot
8Remember that ˆU = ei ˆ
H, where ˆU is a
unitary operator and ˆH is a Hermitian
operator.
be used to describe an observable. There’s another problem. The phase operators
yield a commutation relation (exercise)
h
ˆn, ei ˆφi
= −ei ˆφ,
(27.25)
which implies9 that we have
9If [ ˆ
A, ˆB] = c then [ ˆ
A, eλ ˆ
B] = λceλ ˆ
B.
We take ˆ
A = ˆn, ˆB = ˆφ and λ = i. It
follows that c = i.
h
ˆn, ˆφ
i
= i.
(27.26)
This, in turn, gives us a number-phase uncertainty relation
∆n∆φ ≥1
2.
(27.27)
This is a problem if taken literally, since if we take ∆n to be small we must have ∆φ
greater than 2π. Dirac was so troubled by these inconsistencies that he left these
operators out of the third edition of his seminal textbook The Principles of Quantum
Mechanics.
One possible way forward was suggested by Peter Carruthers in the
Peter Carruthers (1935–1997) is per-
haps best known for his leadership of
the theoretical division at Los Alamos
National Laboratory from 1973–1980,
overseeing a notable period of expan-
sion.
1960s.10 Here we regard the exponentials d
e±iφ as a whole as the fun-
10Actually by three students in his
class.
His problem set contained the
following:
Apparently no one has in-
vestigated whether quantities ˆφ and
√
ˆn
‘deﬁned’ by equations (1) really exist.
A bonus will be given for an answer to
this question.
The bonus turned out
to be a bottle of beer.
See M.M. Ni-
eto, arXiv:hep-th/9304036v1 for more
details.
damental operators. For things to work out we need the exponentiated
phase operators to obey the relations
c
eiφ|n⟩
=
|n −1⟩,
d
e−iφ|n⟩
=
|n + 1⟩.
(27.28)
We may then form Hermitian operators from the exponentials via the
deﬁnitions
[
cos φ
=
1
2
c
eiφ + d
e−iφ

,
d
sin φ
=
1
2i
c
eiφ −d
e−iφ

.
(27.29)
These operators, which do not commute with each other, have the prop-
erties
h
ˆn, [
cos φ
i
=
i d
sin φ,
h
ˆn, d
sin φ
i
=
−i [
cos φ.
(27.30)

252
Coherent states
and uncertainty relations
∆n∆cos φ
≥
1
2|⟨d
sin φ⟩|,
∆n∆sin φ
≥
1
2|⟨[
cos φ⟩|.
(27.31)
Carruthers’ prescription then gives us a meaningful phase operator to
play with, as long as we agree only to ask questions about sines and
cosines of the phase. In the next section we examine some examples of
coherent states.
27.4
Examples of coherent states
Example 27.5
A box containing an electromagnetic ﬁeld (or in the jargon, a cavity containing
photons) allows various normal modes. If we put a ﬁxed number of photons into the
box and have them all in a single mode labelled by a wave vector k, then they will
be in an eigenstate of ˆnk, which we will write as |nk⟩. Since ˆnk|nk⟩= nk|nk⟩, we
know precisely the number of photons (i.e. we know nk) and we have absolutely no
uncertainty in this number (∆nk = 0). However, we have complete uncertainty in
the phase of this state.
If instead we put photons, still all with the same wave vector k, into a coherent
state |αk⟩, we are now uncertain about how many photons we have (∆nk = |αk|),
by the uncertainty in phase11 ∆cos φk →0 as |αk| →∞. As we put more photons
11It is possible to show that
∆cos φk = sin θk
2|αk| .
A proof is given in the book by Loudon.
into this state (⟨ˆnk⟩= |αk|2) then |αk| grows and the state becomes more phase
coherent. This situation is a good model for what occurs inside a laser.
Example 27.6
Another important use of the coherent states is in describing the matter ﬁelds of
superﬂuids and superconductors.12 These phenomena are examined in detail a little
12Superﬂuids are described in Chap-
ter 42 and superconductors in Chap-
ter 44.
later, but here we introduce a method of describing superﬂuids in terms of coherent
states. Like the laser, the superﬂuid also contains a large number of quanta in the
same quantum state. The particles are interacting bosons and are described by a
coherent state
|ψ⟩= |αp0αp1...⟩.
(27.32)
The coherent state |ψ⟩is an eigenstate of the ﬁeld annihilation operator ˆΨ(x) given
by
ˆΨ(x) =
1
√
V
X
p
ˆapeip·x,
(27.33)
so that
ˆΨ(x)|ψ⟩= ψ(x)|ψ⟩.
(27.34)
The eigenvalue ψ(x) can be written simply as
ψ(x) =
1
√
V
X
p
αpeip·x,
(27.35)

Exercises
253
and is often called a macroscopic wave function, for reasons which will become
apparent.13
13Note
that
the
expectation
value
⟨ψ|ˆΨ(x)|ψ⟩= ψ(x) =
1
√
V
P
p αpeip·x,
which
is
signiﬁcant
since
for
occupation-number
states
(with
ﬁxed numbers of particles) we would
expect
⟨np1np2...|ˆΨ(x)|np1np2...⟩
=
1
√
V
P
p⟨np1np2...|ˆap|np1np2...⟩eip·x
= 0,
since ⟨npi|ˆapi|npi⟩= 0 for all i.
So
clearly the interactions between bosons,
which lead to a coherent state |ψ⟩, are
doing something very special. In fact, it
will transpire that ψ(x) is the order pa-
rameter for the ordered, coherent state
of bosons, known as a superﬂuid and
discussed in Chapter 42.
In the superﬂuid a macroscopic number of particles reside in the p = 0 state. We
then have ψ(x) = α0/
√
V, a simple complex number which we can write as √n0 eiθ0.
Thus the expected number density of bosons is |α0|2/V = n0 and since n0 is very
large (we have macroscopically occupied the coherent state), the uncertainty in the
phase is vanishingly small.
We conclude that the macroscopic wave function ψ0(x) for a state where the p
state is coherently occupied is best written
ψ0(x) = √n0eiθ0,
(27.36)
where θ0 is a constant ﬁeld. We can therefore think of superﬂuid order as a sponta-
neous symmetry breaking of phase symmetry, resulting in the phase becoming ﬁxed
to the same value everywhere. This is examined in more detail in Chapter 42.
Chapter summary
• A coherent state is an eigenstate of the annihilation operator with
eigenvalue α, a complex number.
• An occupation-number state contains a ﬁxed number of particles
but the phase is not well deﬁned. A coherent state does not contain
a ﬁxed number of particles, but that number is Poisson distributed
with mean |α|2.
Its phase however can be well-deﬁned, and its
uncertainty decreases as |α| increases.
• Coherent states can be used to describe lasers and superﬂuids.
Exercises
(27.1) Verify that
⟨α|β⟩= eα∗β−|α|2
2
−|β|2
2 .
(27.37)
Compare this expression to the analogous one
for energy eigenstates of the harmonic oscillator,
⟨m|n⟩= δmn. Why is there a diﬀerence? Use these
results to show that ⟨n|ˆa|n⟩= 0, but ⟨α|ˆa|α⟩= α.
Again, why the diﬀerence?
(27.2) Show that
α = ⟨ˆQ⟩+ i⟨ˆP⟩
√
2
.
(27.38)
(27.3) (a) Show that ⟨ˆQ⟩=
√
2Re α, ⟨ˆP⟩=
√
2Im α,
⟨ˆQ2⟩= 1
2 + 2(Re α)2 and ⟨ˆP 2⟩= 1
2 + 2(Im α)2.
(b) Show that ∆Q = ∆P =
1
√
2.
(c) Using the previous results, show that the uncer-
tainty relation for coherent states is indeed
(∆P)2(∆Q)2 = 1
4.
(27.39)
(27.4) Verify that the momentum space coherent state is
given by
⟨P|α⟩=
1
π
1
4
e−(P +i
√
2α)2.
(27.40)
(27.5) Show that
“
ˆQ −⟨Q⟩
”
|α⟩= −i
“
ˆP −⟨P⟩
”
|α⟩.
(27.41)
One way of stating this result in words is that, for
coherent states, the uncertainty is balanced between
position and momentum.

254
Coherent states
(27.6) Show that
h
ˆn, ei ˆφi
= −ei ˆφ.
(27.42)
(27.7) Once again consider the forced quantum oscillator
L = 1
2m ˙x(t)2 −1
2mω2x(t)2 + f(t)x(t),
(27.43)
where f(t) is the force which you should assume
acts only for a ﬁnite length of time: 0 ≤t ≤T.
The evolution of ⟨ˆx(t)⟩will be given by the sum
of the time dependence in the absence of the force
added to the response:
ˆx(t) = ˆx0(t) +
Z
dt′χ(t −t′)f(t′).
(27.44)
(a) Show that ˆx(t) is given by
ˆx(t)
=
„
1
2mω
« 1
2 “
ˆae−iωt + ˆa†eiωt”
(27.45)
+θ(t −t′)
Z
dt′ if(t′)
2mω
“
e−iω(t−t′) −eiω(t−t′)”
.
Hint: Use the deﬁnition of the susceptibility in Ex-
ercise 21.2.
(b) For times t > T verify that this may be written
ˆx(t)
=
„
1
2mω
« 1
2
" 
ˆa +
i
(2mω)
1
2
˜f(ω)
!
e−iωt
+
 
ˆa† −
i
(2mω)
1
2
˜f(−ω)
!
eiωt
#
.(27.46)
(c) Argue that the Hamiltonian must take the form
ˆH = ω
 
ˆa† −
i
(2mω)
1
2
˜f(−ω)
!  
ˆa +
i
(2mω)
1
2
˜f(ω)
!
.
(27.47)
(d) Show that this Hamiltonian is diagonalized by
the coherent state |α⟩which is the eigenstate of the
ˆa operator and ﬁnd α.
(e) Show that the number of quanta emitted by the
source is
⟨ˆn⟩= | ˜f(ω)|2
2mω .
(27.48)
and ﬁnd the energy imparted to the system by the
source.
(f) Find an expression for |α⟩in terms of the un-
forced ground state |0⟩.
(27.8) Consider (yet again) the forced quantum oscillator
from the previous question with the forcing part ex-
pressed as an interaction.
(a) Show that
−i
Z
dt ˆHI(t)|0⟩=
i ˜f(ω)
(2mω)
1
2
|1⟩.
(27.49)
(b) By considering the S-matrix, show that the am-
plitude for the source to create a state containing a
single quantum is given by
A1 =
i ˜f(ω)
(2mω)
1
2
e(Dumbbell),
(27.50)
where (Dumbbell) denotes the Feynman diagram in
Fig. 22.2.
(c) Extend the previous result to show that the am-
plitude for the source emitting n quanta is given by
An =
1
√
n!
 
i ˜f(ω)
(2mω)
1
2
!n
e(Dumbbell).
(27.51)
(d) Show that the probability of the source emitting
n quanta is given by
Pn = |α|2n
n! e−|α|2,
(27.52)
where |α| =
| ˜
f(ω)|
(2mω)
1
2 . Comparing with the previous
question, explain why this result was inevitable.
(27.9) A coherent state is an eigenstate of the annihila-
tion operator ˆa. Why do we never bother with the
eigenstates of the creation operator ˆa†?

28
Grassmann numbers:
coherent states and the
path integral for fermions
28.1 Grassmann numbers
255
28.2 Coherent states for fermions
257
28.3 The
path
integral
for
fermions
257
Chapter summary
258
Exercises
258
Thus far, our discussion of ﬁelds and coherent states has concentrated
exclusively on bosons. We would also like to describe fermions with path
integrals and coherent states. This turns out to be a non-trivial matter.1
1For this reason, Chapter 28 can be
skipped on a ﬁrst reading.
Grass-
mann numbers will only be used in Sec-
tion 38.2.
Consider the path integral, for example. In calculating Z[J] for Bose
ﬁelds we have argued that we integrate over the classical action in the
presence of sources Z[J] =
R
Dφ e−i
R
d4x(L+Jφ). Unlike the canonical
formalism, the path integral formulation doesn’t deal with operators
and their commutation relations; just classical ﬁelds. (The quantum be-
haviour comes about through the constructive interference of the classi-
cal paths.) This formulation works for Bose ﬁelds because the ordinary
numbers outputted by the functions over which we integrate commute,
just like the quantum operators of the canonical formalism.
As we discussed in Chapter 3 the operator-valued ﬁelds that represent
fermions in the canonical approach anticommute. This is the property
that, for example
ˆψ(x) ˆψ(y) = −ˆψ(y) ˆψ(x),
(28.1)
where ˆψ(x) is an operator that annihilates a fermion at spacetime point
x. In order to be able to describe fermions with a path integral we need
a new sort of number: a number that anticommutes.
Even before the advent of quantum mechanics, the properties of such
numbers had already been examined by Hermann Grassmann. Although
Hermann Grassmann (1809–1877)
Grassmann’s work was largely neglected during most of his life, the
anticommuting numbers he invented are now known as Grassmann
numbers. In this chapter we will examine the properties of Grassmann
numbers. This will allow us to write down a coherent state for fermions
and will allow us to write a path integral for fermions. The latter will
be useful in our description of the quantum ﬁeld of the electron.
28.1
Grassmann numbers
Grassmann numbers anticommute.
This means that two Grassmann
numbers η and ζ have the property that
ηζ = −ζη.
(28.2)

256
Grassmann numbers: coherent states and the path integral for fermions
Immediately following from this is that η2 = ηη = −ηη = 0. In words:
the square of a Grassmann number is zero. This simpliﬁes the algebra
of Grassmann numbers. Because η2 = 0 we can’t have any terms in the
Taylor expansion of a function with more than a single power of η, and
so the most general function of η is then given by
f(η) = a + bη,
(28.3)
where a and b are real numbers.2
2Real numbers commute with Grass-
mann numbers.
We’d like to be able to do calculus with Grassmann numbers. We
deﬁne the act of diﬀerentiating via
∂
∂ηη = 1,
∂
∂ηa = 0.
(28.4)
Note also that this derivative operator itself anticommutes, so if ζ is
another Grassmann number we have
∂
∂ηζη = −ζ ∂
∂ηη = −ζ.
Example 28.1
Diﬀerentiating the most general function of a Grassmann variable, we have
∂f(η)
∂η
= b.
(28.5)
The most general function of two Grassmann variables is written
g(η, ζ) = a + bη + cζ + eηζ.
(28.6)
We can diﬀerentiate this to show that
∂
∂η g(η, ζ) = b + eζ,
∂
∂ζ g(η, ζ) = c −eη.
(28.7)
We would also like to integrate Grassmann numbers. We deﬁne integra-
tion over Grassmann variables via
R
dη η = 1,
R
dη = 0.
(28.8)
(Note again that the integration operator dη is to be regarded as a
Grassmann number.) As a result of the deﬁnitions, integration of our
most general function f(η), given in eqn 28.3, yields
Z
dη f(η) =
Z
dη a +
Z
dη bη = b.
(28.9)
Notice that this is the same result that we obtained by diﬀerentiating.
In fact it is a general result that integration and diﬀerentiation of Grass-
mann numbers yield exactly the same result!
Example 28.2
Let’s check this assertion by integrating the function g(η, ζ). Integrating with respect
to η, we have
Z
dη (a + bη + cζ + eηζ) = b + eζ,
(28.10)
while integrating with respect to ζ gives us
Z
dζ (a + bη + cζ + eηζ) = c −eη,
(28.11)
which is identical to what we had in eqn 28.7.

28.2
Coherent states for fermions
257
28.2
Coherent states for fermions
One use of Grassmann numbers is in deﬁning a coherent state that de-
scribed fermions. Taking the creation operator for a fermion to be ˆc† we
deﬁne a fermionic coherent state as |η⟩= e−ηˆc†|0⟩, where η, as usual, is
a Grassmann number. We can immediately simplify this using η2 = 0
and write
In order for things to work out, we re-
quire that Grassmann numbers like η
anticommute with fermion operators.
For example, we require {η, ˆc} = 0.
|η⟩= |0⟩−η|1⟩.
(28.12)
Example 28.3
We will check that a coherent state deﬁned this way is the eigenstate of the fermion
annihilation operator ˆc. We have
ˆc|η⟩= ˆc|0⟩−ˆcη|1⟩= 0 + ηˆc|1⟩= η|0⟩.
(28.13)
But
η|η⟩= η|0⟩−η2|1⟩= η|0⟩,
(28.14)
and so ˆc|η⟩= η|η⟩as required.
We can also deﬁne a state ⟨¯η|ˆc† = ⟨¯η|¯η, where
⟨¯η| = ⟨0| −⟨1|¯η = ⟨0| + ¯η⟨1|.
(28.15)
Note that ¯η is not the complex conjugate of η and ⟨¯η| is not the adjoint of
|η⟩. With these deﬁnitions it follows3 that the value of an inner product
3See Exercise 28.1.
is
⟨¯ζ|η⟩= e
¯ζη.
(28.16)
Finally, for these coherent states to be useful, we require that they form
a complete set via the completeness relation
Z
dηd¯η e¯ηη|¯η⟩⟨η| = 1.
(28.17)
28.3
The path integral for fermions
We start by evaluating the Gaussian integral for coherent states4
4Had these been ordinary c-numbers,
we would have
Z
dzdz∗e−z∗az = π
a ,
(Re a > 0),
where dzdz∗represents the indepen-
dent integration over real and imagi-
nary parts of z. While the factor of π
is neither here nor there, the important
point to note is that the a comes out in
the numerator for the Grassmann ver-
sion.
Z
dηd¯η e¯ηaη =
Z
dηd¯η (1 + ¯ηaη) =
Z
dη aη = a.
(28.18)
This carries over to the case of Grassmann-valued (N-component) vec-
tors η = (η1, η2, ..., ηN) and ¯η = (¯η1, ¯η2, ..., ¯ηN) and we ﬁnd5 that for a
5See Exercise 28.3.
matrix A:
Z
dNηdN ¯η e¯ηAη = det A,
(28.19)
where dNηdN ¯η = dη1d¯η1dη2d¯η2...dηNd¯ηN.
Finally, we can work out the most important integral of all. Starting
with Grassmann numbers ψ and η, we will want to integrate an equation
of the form
Z
dψd ¯ψ e
¯
ψKψ+¯ηψ+ ¯
ψη.
(28.20)

258
Grassmann numbers: coherent states and the path integral for fermions
We play the usual trick of completing the square6 and we ﬁnd
6Recall
that
we
originally
solved
the
path
integral
problem
I =
R
dx e−ax2
2
+bx by completing the
square, writing
−ax2
2
+ bx = −a
2
„
x −b
a
«2
+ b2
2a ,
allowing us to conclude that I
=
q
2π
a e
b2
2a .
¯ψKψ + ¯ηψ + ¯ψη =
  ¯ψ + ¯ηK−1
K
 ψ + K−1η

−¯ηK−1η,
(28.21)
which enables us to conclude that
Z
dψd ¯ψ e
¯
ψKψ+¯ηψ+ ¯
ψη = Ce−¯ηK−1η,
(28.22)
where the number C (which will be proportional to a determinant for
the vector case) will later be removed by normalization. (Remember
that most of our problems only require part of the solution: the part in
the exponential is important and the multiplying prefactor far less so.)
We can immediately extend this result to the functional integral we
need. Our goal is to ﬁnd the generating functional for fermions
Z[¯η, η] =
Z
DψD ¯ψ ei
R
d4x[L( ¯
ψ,ψ)+¯η(x)ψ(x)+η(x) ¯
ψ(x)].
(28.23)
If necessary we massage the Lagrangian L( ¯ψ, ψ) into the form
¯ψ(x) ˆKψ(x), with ˆK a diﬀerential operator, allowing us to read oﬀthe
answer that the normalized7 generating functional is given by:
7Recall that to normalize we deﬁne
Z[¯η, η] = Z[¯η, η]/Z[0, 0].
Z[¯η, η] = e−i¯η(x) ˆ
K−1(x,y)η(y).
(28.24)
As before, we can then read oﬀthe propagator as i ˆK−1 and the quanti-
zation process is complete!
Chapter summary
• Grassmann numbers anticommute and can be used to describe
fermions.
• The fermion coherent state can be written |η⟩= |0⟩−η|1⟩. Such
states can be used to construct the fermion path integral.
Exercises
(28.1) From the deﬁnitions of the coherent states |η⟩and
⟨¯ζ|, prove that ⟨¯ζ|η⟩= e
¯ζη.
(28.2) Find (a)
R
d¯ηdη η¯η and (b)
R
d¯ηdη ¯ηη.
(28.3) Show that
R
d2ηd2¯η e¯ηAη
= det A, where η =
„ η1
η2
«
, ¯η =
`
¯η1
¯η2
´
, d2ηd2¯η = dη1d¯η1dη2d¯η2,
and A is a 2 × 2 matrix.
(28.4) Prove that
Z
dηd¯η e¯ηη|¯η⟩⟨η| = |0⟩⟨0| + |1⟩⟨1|.
(28.25)
and may therefore be taken as a resolution of the
identity.
(28.5) Verify eqn 28.21.

Part VII
Topological ideas
In many cases in quantum ﬁeld theory we are concerned with working
out some integral over spacetime coordinates and therefore the precise
geometry of spacetime is important. However, there are some problems
where the problem is entirely insensitive to the shape of spacetime, that
is it has no way of sensing the spacetime metric. In this case, the theory
will give an answer which is some kind of invariant which nevertheless
does depend on some global property of the topology of the spacetime.
• In Chapter 29 we introduce some simple topological ideas and use
them to describe simple topological objects which can exist in ﬁeld
theories. These include kinks and vortices.
• Topological ﬁeld theories are discussed in Chapter 30 and we pro-
vide some concrete examples of applications of these ideas by dis-
cussing anyons, fractional statistics and Chern–Simons theories.

29
Topological objects
29.1 What is topology?
260
29.2 Kinks
262
29.3 Vortices
264
Chapter summary
266
Exercises
266
You better shape up
Olivia Newton-John (1948– ), Grease1
1Newton-John’s grandfather was Max
Born (1882–1970), one of the found-
ing fathers of quantum mechanics. The
quoted lyric is by John Farrar.
In this chapter we meet a new class of excitation in the ﬁeld known
as a topological object. These are conﬁgurations of the ﬁeld which
may exist in systems that show spontaneous symmetry breaking. As
discussed in Chapter 26, when a symmetry is broken we should watch
out for objects known as defects that reﬂect the possibility that a ﬁeld
can break symmetry in diﬀerent ways in diﬀerent regions of space. The
objects we will encounter in this chapter are examples of such defects.
We will discuss the two simplest topological objects: kinks and vortices.
Both of these may be thought of as time-independent lumps of energy
which are held together by interactions. An important additional feature
of vortices is that they’re not stable alone and we must add gauge ﬁelds
in order for them to be realized. As always, the presence of gauge ﬁelds
provides the theory with some particularly interesting properties. We
begin with a crash course on basic topology.
∼=
∼=
Fig. 29.1
A cup can be continuously
deformed into a doughnut (a torus).
29.1
What is topology?
The doughnut and the coﬀee mug are topologically equivalent because
one can be continuously deformed into the other (Fig. 29.1). We imagine
that everything is made of some kind of mouldable clay and so all objects
can be pressed and prodded so that their shape changes.
However,
you are not allowed to puncture or heal, to add or remove holes, so
that neighbouring points in the object remain neighbouring after the
deformation.
Topological spaces are given mathematical names: the
real line is denoted R, so the plane is R2 and the n-dimensional version
is Rn.
One can deﬁne a product space in an obvious way, and so
R × R = R2 [Fig. 29.2(a)]. Similarly R × R × · · · × R = Rn. A segment
of a R joined head to tail gives a circle, denoted S1.
The sphere is
S2.
(Note that this is a two-dimensional space, because by ‘sphere’
mathematicians mean the surface of a ball, not its interior.) The sphere
cannot be embedded in R2 (you can’t fully represent a sphere on a piece
of paper without cutting it2) but can be embedded in R3. (Similarly S1
2This is why projections of the Earth’s
surface on maps usually involve a cut
through one of the major oceans.
can’t be embedded in R.) The torus T 2 (back to the doughnut again)
can be constructed using the product T 2 = S1 × S1 [Fig. 29.2(b)].
∼=
∼=
×
×
S1
S1
T 2
R
R
R2
(a)
(b)
Fig. 29.2
A product space is con-
structed by attaching a copy of one
space to every single point in another
space. (a) A product space of the real
line (R) with itself produces the two-
dimensional plane R2. (b) The product
of two circles (S1) produces the torus
T 2.
A cup can be continuously de-
formed into a doughnut (a torus).
Once we have a topological space, we can deﬁne a path through the
space as a mapping f(x) of the real-line segment [a, b] (shorthand for

29.1
What is topology?
261
a ≤x ≤b) to the topological space. If f(a) = f(b), the path becomes a
loop. Paths can be continuously deformed into each other in an obvious
way and one ﬁnds that in a particular topological space the set of possible
loops can be divided up into a number of classes and these can be mapped
onto a group (called the fundamental group and given the symbol π1).
For example, in Rn all loops are contractible (i.e. can be continuously
deformed to a point) and so there is only one class of loop and π1 is a
trivial group just consisting of the identity element. It’s more interesting
if the space has a hole in it, so that the loops can be divided into classes,
each one characterized by the number of times the loop winds round the
hole. This integer is known as the winding number. Thus π1 = Z, the
set of integers. Further examples are π1(S1) = Z and π1(T 2) = Z × Z
(see Fig. 29.3).
Fig. 29.3
Loops on a surface of a
torus T 2 can be contractible (contin-
uously deformable to a point), such as
C, or non-contractible such as A and B
(each of these winds round once). Any
loop can be described using two wind-
ing numbers, the number of times the
loop winds round like A and the num-
ber of times like B. In topological par-
lance: π1(T 2) = Z × Z.
The crucial point about topological arguments is that they do not rely
on any notion of the geometrical structure of space. Topological spaces
are continuously deformable and therefore a topological description re-
ﬂects the underlying ‘knottiness’ of the problem.
Example 29.1
A thin solenoid (radius R, aligned along ˆez) inserted into the two-slit experiment
(see Fig. 29.4) gives rise to a magnetic vector potential outside the solenoid equal to
Aθ = BR2/2r and inside Aθ = Br/2. The magnetic ﬁeld B = ∇× A is then equal
to zero outside and (0, 0, B) inside the solenoid.
electrons
electrons
screen
B
Fig. 29.4 The Aharonov–Bohm eﬀect.
Electrons diﬀract through two slits but
their paths enclose a solenoid.
The
ﬁeld inside the solenoid is B, but the
electrons only pass through regions in
which there is no magnetic ﬁeld.
Even though B = 0 outside the solenoid, the electronic wave function can still
be aﬀected. This is because a plane wave ψ ∝eip·r/ℏand in the presence of A the
momentum p →p −qA and so over a trajectory ψ acquires an extra phase factor
ei∆α where ∆α = −q
ℏ
R
A·dr. This might not seem to matter because if you perform
a gauge transformation A →A + ∇χ then ∆α = −q
ℏ
R
∇χ · dr and so ∆α can be
anything you want, all you need to do is to pick a particular form for χ. However,
when we consider the two-slit experiment, we are interested in the phase diﬀerence
∆δ between the two paths which is given by
∆δ = ∆α1 −∆α2
=
−q
ℏ
Z
a→1→b
A · dr + q
ℏ
Z
a→2→b
A · dr
=
q
ℏ
I
A · dr,
(29.1)
where the ﬁnal integral is carried out anticlockwise around the trajectories. However,
we may use Stokes’ theorem to write
∆δ = q
ℏ
Z
∇× A · dS = q
ℏ
Z
B · dS = q
ℏΦ,
(29.2)
where Φ is the ﬂux through the solenoid. This leads to a shift in the interference
pattern on the screen and is observed experimentally. Gauge transformations make
no diﬀerence here because
H
∇χ · dr = 0. Remarkably we expect a nonzero gauge-
independent phase diﬀerence ∆δ even though electrons only pass through regions
without magnetic ﬁeld. The eﬀect is known as the Aharonov–––Bohm eﬀect and
Yakir Aharonov (1932– )
David Bohm (1917–1992)
is topological. The electronic wave function is deﬁned on the plane R2 minus the
origin (where we place the ﬂux), i.e. a sheet with a hole in it. Electromagnetism has
U(1) symmetry, which has the same topology as S1 (the phase can be deﬁned on a
circle in the Argand diagram). Thus to deﬁne a phase everywhere on a sheet with
a hole in it amounts to mapping S1 on to a path around the hole. These fall into
disjoint classes (labelled by an integer winding number, because π1(S1) = Z) and
these cannot be deformed into each other.

262
Topological objects
29.2
Kinks
Consider the Lagrangian L =
1
2(∂µφ)2 −U(φ) in (1 + 1)-dimensional
spacetime, with U(φ) = λ
4 (v2−φ2)2. This is just our symmetry breaking
Lagrangian for φ4 theory as can be quickly seen by expanding the bracket
and setting v2 = m2
λ . The potential, shown in Fig. 29.5, has two minima
with U(φ) = 0 at φ(x) = ±v. The vacuum of the theory is to be found
at φ(x) = v or φ(x) = −v. We know that one way to work out what sort
of particles live in this potential is to expand about one of the vacua.
The excitations are then predicted to be particles of mass m = (2λv2)
1
2 .
The point of this chapter is to tell you that objects other than particles
can live in the potential too. These are stationary conﬁgurations of the
ﬁeld φ(x) whose energy density 1
2(∂iφ)2 +U(φ) goes to zero at x = ±∞,
but which do something non-trivial in between.
φ
U
−a
a
Fig. 29.5 The double well potential.
x
φ(x)
φ(x) = v
Fig. 29.6 The uniform ground state of
the ﬁeld in a system with broken sym-
metry.
x
φ(x)
v
−v
l
Fig. 29.7 The kink solution.
Let’s examine a ﬁeld which has zero energy density at x = ±∞. Such
a ﬁeld must be stationary and occupy a zero of the potential at ±∞. A
boring way of achieving this is for φ(x) = v as shown in Fig. 29.6. The
conﬁguration φ(x) = −v is similar.
This is just symmetry breaking:
the system falls into a ground state of φ = v or φ = −v. However,
there is a more interesting conﬁguration of the ﬁeld. What if the ﬁeld
takes φ(−∞) = −v and φ(∞) = v?
Then we must have something
like the object shown in Fig. 29.7. This conﬁguration is known as a
kink and involves the curious state of aﬀairs of half of the ﬁeld living in
one vacuum and half in the other. Notice that the ﬁeld has broken the
symmetry in two diﬀerent ways. At x →∞the ﬁeld takes on the value
φ = v, whereas for x →−∞it takes on value φ = −v. We might say
that the ﬁelds at ±∞live in diﬀerence vacua. To ﬁnd out whether the
kink is stable, we may evaluate its total energy. If the energy is ﬁnite
then we’re in luck as such conﬁgurations are allowed to exist.
Example 29.2
We will evaluate the energy of a kink. We take the kink to be stationary in time, so
∂0φ = 0. We want to work out E =
R
dx
ˆ 1
2 (∂1φ)2 + U(φ)
˜
. From the equations of
motion we have that ∂2φ
∂x2 = ∂U
∂φ , which may be integrated to give 1
2
“
∂φ
∂x
”2
= U(φ).
We therefore have an expression for the energy given by
E
=
Z
dx
"
1
2
„ ∂φ
∂x
«2
+ U(φ)
#
=
Z ∞
−∞
dx 2U(φ) =
Z v
−v
dφ dx
dφ2U(φ)
=
Z v
−v
dφ [2U(φ)]
1
2 .
(29.3)
Plugging in [2U(φ)]
1
2 = (λ/2)1/2(v2 −φ2), we obtain the energy3
3Notice that the kink has energy in-
versely proportional to the coupling
constant.
This is signiﬁcant since it
implies that an expansion in powers
of the coupling constant, of the sort
we make in perturbation theory, would
never predict such a ﬁeld conﬁguration.
This is one of the hallmarks of topo-
logical objects: they are fundamentally
non-perturbative.
E =
1
√
2
4m3
3λ .
(29.4)
This is ﬁnite and the kink is therefore allowed to exist!

29.2
Kinks
263
The kink has a ﬁnite energy and, by construction, is a time-independent
ﬁeld conﬁguration. It is also of a ﬁnite size l (see Fig. 29.8), the extent
of which is determined by a balance of kinetic and potential energy. The
kinetic energy’s contribution
R
dx 1
2(∂1φ)2 ≈l×(v/l)2 tries to smear the
kink out, but the potential contribution
R
dx U ≈λv4l tries to limit the
region over which the ﬁeld changes. The minimum energy is then to be
found when the size of the kink is l ≈(λv2)−1
2 = 1/m, where m is the
mass of the conventional particles of the theory.
x
E(x)
l
Fig. 29.8 The energy density of a kink
is localized in an interval l.
Example 29.3
The domain wall in a ferromagnet has much in common with the kink described
here. For the ferromagnet the vacua correspond to all spins pointing up (φ = v) or
down (φ = −v). The kink corresponds a region where the ﬁelds change from up to
down over a ﬁnite distance. This separates two magnetic domains. A domain is a
region where the symmetry is broken in a particular way. It is, for example, part of
the Universe where the vacuum corresponds to all spins pointing up. Domain walls
therefore separate regions with diﬀerent vacua.
Domain walls are real and their existence may be detected in magnets through
a variety of means. In a magnet, domains allow a magnetic system of ﬁnite spatial
extent to save energy by reducing the ﬁeld density in free space. However, the domain
wall might be expected to exist on the more general grounds discussed here.
Also interesting is an idea of Nambu’s that in reaching its ﬁnal state the Universe
has broken various symmetries. This, he suggests, might imply that domains with
diﬀerent vacua may exists across the Universe.
We have a ﬁeld conﬁguration with ﬁnite energy above the ground state
which is of ﬁnite spatial size. The theory is translationally invariant:
the kink’s centre can be anywhere. The theory is Lorentz invariant: the
kink can be boosted to an arbitrary velocity. The kink is, therefore,
very much like a particle. In addition, the kink is stable: any attempt
to remove the kink involves lifting a (semi)inﬁnite length of ﬁeld from
one of the potential minima to another. This costs an inﬁnite amount
of energy and is therefore impossible. A more mathematical statement
of this is that the kink is not deformable.
We cannot hold the ends
of the kink tightly and then remove the part that crosses the axis in
between. The one way of annihilating a kink would be to have an anti-
kink: an object whose ﬁeld crosses the axis in the opposite direction so
that φ(−∞) = v and φ(∞) = −v, as shown in Fig. 29.9. The kink and
antikink pair is deformable. If we hold the ends of the ﬁeld tightly we
see that we can continuously change the part that crosses the axis to
remove the parts that cross the axis. We end with a ﬁeld lying in the
vacuum at φ = −v.
x
φ(x)
v
−v
kink
antikink
Fig. 29.9 A kink and antikink.
This stability of kinks is encoded in a description of its charge. A
charge, remember, is designed to be a conserved quantity. A single kink
should carry a topological kink-charge of QT = 1; an antikink a kink-
charge of QT = −1.
In order to come up with a sensible recipe for
ﬁnding the kink-charge, we deﬁne the kink-current4 as
4Note that this is diﬀerent to the
Noether currents that we have consid-
ered so far. Its existence owes nothing
to a symmetry of the Lagrangian.
Jµ
T = 1
2v εµν∂νφ,
(29.5)

264
Topological objects
where εµν is the antisymmetry symbol, whose signs are determined by
knowing that ε01 = 1. This leads to an expression for the charge of a
kink of
QT
=
Z ∞
−∞
dx J0 = 1
2ν
Z ∞
−∞
dx ∂φ
∂x
=
1
2v [φ(∞) −φ(−∞)] = 1,
(29.6)
whereas an antikink has QT = −1. Notice that the phions of the theory
do not have a kink-charge since QT = φ(∞) −φ(−∞) = 0. We call the
kink-charge a topological charge, which explains the subscript. Its
existence is independent of the geometry of spacetime. That geometry
is encoded in gµν, which is an instruction book telling us what watches
and measuring rods measure in spacetime. In contrast the topological
charge has its indices summed with the antisymmetric symbol5 εµν. The
5Remember that our convention is not
to treat this as a tensor and so εµν =
εµν
dependence on εµν rather than gµν turns out to be a general feature of
topological objects and is examined in more detail in the next chapter.6
6To summarize the argument so far, the
theory
L = 1
2 (∂µφ)2 −U(φ),
with
U(φ) = λ
4 (v2 −φ2)2,
allows the existence of kinks. These are
topological objects which behave rather
like particles. The ﬁeld at x = ∞and
x = −∞live in diﬀerent vacua.
29.3
Vortices
(a)
(b)
(c)
(d)
Fig. 29.10 Some vortices with winding
number n equal to (a) 1; (b) 1; (c) −1;
(d) 2.
Now we move to (2 + 1)-dimensional spacetime and examine the same
Lagrangian. Our potential now resembles a Mexican hat, whose minima
describe a circle in the φ1-φ2 plane. It will be convenient to work in polar
(internal) coordinates (r, θ) and write φ(x) = φ1(x)+iφ2(x) ≡r(x)eiθ(x).
In the case of the kink we found a topological ﬁeld whose two ends
lived in diﬀerent vacua at spatial |x| →∞. By analogy we ask what a
continuous ﬁeld would look like whose elements live in diﬀerent vacua
at spatial inﬁnity. Some examples of such conﬁgurations are shown in
Fig. 29.10. These objects are known as vortices. They are described by
a ﬁeld whose form at inﬁnity is written
φ(x) = Kei[nθ(x)+ϕ] (|x| →∞),
(29.7)
where θ(x) = tan−1(x2/x1) and ϕ is a constant, arbitrary phase.7 Notice
7The phase ϕ can be chosen to globally
alter the directions at inﬁnity. So, for
example, Fig. 29.10(a) corresponds to
ϕ = 0, while Fig. 29.10(b) corresponds
to ϕ = π/2. This can also be absorbed
into the complex amplitude K.
how this equation relates the direction in which the ﬁeld φ points in the
internal complex plane to the angle θ that tells us where we are in
coordinate space. Here n is known as the winding number; it tells us
how many times the ﬁeld winds around as we travel around the circle
at spatial inﬁnity.
Example 29.4
We may evaluate the energy of a vortex. For a static ﬁeld conﬁguration we have a
Hamiltonian
H = 1
2∇φ† · ∇φ + U(φ),
(29.8)
and we take U(φ) =
`
|K|2 −φ†φ
´2, so that U(φ) = 0 on the boundary at inﬁnity.
The gradient of the vortex in cylindrical (spatial) coordinates is
∇φ = 1
r (inKeinθ)ˆeθ.
(29.9)

29.3
Vortices
265
The core of the vortex looks frighteningly singular, but we will assume it is ultimately
well behaved and call the core energy Ecore(a), where a is the core size. We evaluate
the energy of the rest of the conﬁguration at distances larger than r = a and ﬁnd
E = Ecore +
Z ∞
a
drdθ rH = Ecore + πn2|K|2
Z ∞
a
dr 1
r .
(29.10)
This is a logarithmically divergent energy. We conclude that a single vortex is not a
stable object.
The single vortex is unstable. This is understandable if we look at the
form of the ﬁeld at large distances (Figs. 29.11 and 29.12). It is still
swirly as r →∞. Actually this is predicted by a general theorem due to
G.H. Derrick, which says that time-independent topological objects are
impossible for our theory in more than one spatial dimension.
x
y
Fig. 29.11
Fields at large distances
for an n = 1 vortex.
Fig. 29.12 The vortex ﬁeld conﬁgura-
tion for an n = 1 vortex.
To stabilize the vortex we must tamper with the Lagrangian. The way
to do this is to add a gauge ﬁeld. Recall that adding a gauge ﬁeld (known
as ‘gauging the theory’) is done by introducing a covariant derivative as
follows:
Dµφ = ∂µφ + iqAµφ.
(29.11)
In order to cancel the divergence of the vortex we’re going to choose our
gauge ﬁeld in such a way that it cancels the divergent part of the vortex
energy, which resulted from the derivative of the ﬁeld conﬁguration.
We therefore want the covariant derivative Dµφ to vanish at inﬁnity.
A gauge ﬁeld that does this is one whose limit as r →∞tends to
A(r, θ) = 1
q∇(nθ).
Example 29.5
The components of the proposed gauge ﬁeld at inﬁnity are8
8Recall that A = Ai = −Ai in follow-
ing the sign changes here.
Ar →0,
Aθ →−n
qr
as r →∞.
(29.12)
As r →∞the components of the covariant derivative become
Drφ = 0,
Dθφ = 1
r
∂φ
∂θ + iqAθφ = 0
as r →∞.
(29.13)
This shows that the covariant derivative (which represents the ‘kinetic energy’ of
the ﬁeld) vanishes at inﬁnity. Since it was this term that diverged for the ungauged
theory, we see that A has saved the day.
The gauge ﬁeld should also be expected to make a contribution to the
Lagrangian of the form −1
4FµνF µν. However, this is not the case here.
Usually we have a gauge ﬁeld which can be changed by a gauge trans-
formation which adds a contribution ∂µχ. In our case we notice that at
inﬁnity A = ∇χ, where χ = 1
qnθ. That is, our gauge ﬁeld is entirely
formed from a gauge transformation and is therefore another example
of the pure gauge which we ﬁrst saw in Chapter 26. This means that
Fµν = 0 and the gauge ﬁeld cannot result in a nonzero −1
4F 2 contribu-
tion to the Lagrangian. This is also a relief, since such a contribution
might also threaten to blow up, ruining the stability of the vortex.

266
Topological objects
What then is the contribution of the gauge ﬁeld to the physics? The
answer is found by carrying out the integral
H
A · dl around a circle at
inﬁnity. From Stokes’ theorem we know what this means physically: the
integral is
H
A · dl =
R
B · dS = Φ, that is the magnetic ﬂux through
the circle. Carrying out this integral we obtain
Φ =
I
A · dl =
I
Aθr dθ =
Z
dθ n
q = 2πn
q .
(29.14)
We see that vortices carry quantized magnetic ﬂux!
The next step in this line of argument to examine a topological object
in (3+1)-dimensional spacetime. Remarkably, this object turns out to
be a form of a magnetic monopole and is examined in Chapter 49. In the
next chapter we turn our attention to another aspect of topology and
describe a ﬁeld theory that can be legitimately described as topological.
Chapter summary
• Topology relates the structure of objects that are preserved un-
der continuous deformations involving stretching but no cutting or
gluing.
• Topological objects include kinks and vortices and can be charac-
terized by a topological charge.
Exercises
(29.1) Show that the antikink has kink-charge QT = −1
and that two kinks and three antikinks have a total
charge of QT = −1.
(29.2) This is the ﬁrst time we have used the antisymmet-
ric symbol εµν in anger. We regard the up and down
versions as identical since we are not using it as a
tensor. Let’s examine its properties. Take ε01 = 1
and ε012 = 1 and show the following:
(a) εijεij = 2.
(b) εijεin = δjn.
(c) εijεmn = δjnδim −δinδjm.
(d) εijkεimn = δjmδkn −δjnδkm.
(29.3) A simple model of a domain wall in a magnet
says that the interaction energy of two neighbour-
ing (classical) spins of magnitude S is given by
E = −JS2 cos θ where θ is the relative angle be-
tween the spins and J is a constant.
(a) Show that the energy of a domain wall sepa-
rating a domain of spins up and a domain of spins
down costs energy
U = 1
2JS2 π2
N ,
(29.15)
where N is the number of spins in the wall.
(b) Show that this picture will not lead to a stable
wall.
(c) In reality spin–orbit eﬀects lead to an energy
cost for not having spins pointing along certain spa-
tial directions. In this case these lead to a contri-
bution to the energy of NK
2 . Show that the wall is
now stable and ﬁnd its size.
(29.4) (a) Draw vortices with winding numbers n = −2
and n = 3.
(b) Draw the ﬁelds resulting from vortices with
n = 1 and n = −1 sharing the same region of space.
This situation is discussed in Chapter 34.

30
Topological ﬁeld theory
30.1 Fractional
statistics
`a
la
Wilczek: the strange case of
anyons
267
30.2 Chern–––Simons theory
269
30.3 Fractional
statistics
from
Chern–––Simons theory
271
Chapter summary
272
Exercises
272
Imagine the perplexity of a man outside time and space, who
has lost his watch, his measuring rod and his tuning fork.
Alfred Jarry (1873–1907), Exploits and Opinions of Doctor
Faustroll, Pataphysician
So far we have met topological objects that can exist in scalar ﬁeld the-
ories with a broken symmetry. In this chapter we will take the study
of topology further and cook up an inherently topological ﬁeld the-
ory. This theory will apply to the special case of (2+1)-dimensional
spacetime. We will see that physics in the ﬂatland of (2+1) dimensions
has some remarkable properties whose origin may be traced back to
topological considerations.
We start our discussion by examining the sorts of particle statistics
that apply in two spatial dimensions. We will see that ﬂatland supports
anyons: particles that are neither bosons nor fermions. We will then
go on to formulate a ﬁeld theory that not only captures anyon statistics,
but also describes some curious new electrodynamics.
30.1
Fractional statistics `a la Wilczek: the
strange case of anyons
We are quite used to the idea of bosons and fermions as entities distin-
guished by their behaviour upon exchange of identical particles. Specif-
ically, changing the labels on two identical particles results in a change
of the wave function according to the rule
ψ(x1, x2) = ±ψ(x2, x1),
(30.1)
where the + sign applies to bosons and the −sign to fermions.
For the case of two-dimensional space, the exchange of particles needs
more careful attention. In fact, we need to think more precisely about
what the exchange of particles actually involves.
We do not simply
make the two particles vanish and then reappear in diﬀerent positions.
Rather we imagine the process in which particles are moved around
each other in real space. Let’s identify two moving-around processes.
We start with two identical particles at positions x1 and x2 and identify
two fundamentally distinct ways of exchanging them.
The result of
processes of type A is to move x1 →x1 and x2 →x2. Some examples
are shown in Fig. 30.1(a) and (b). In this sort of process the particles

268
Topological ﬁeld theory
end up where they were originally, although they may move around each
other. The result of type B processes is to move x1 →x2 and x2 →x1
[Fig. 30.1(c) and (d)].
Again particles may move around each other
several times before settling at their ﬁnal positions [as in Fig. 30.1(d)].
This is particle exchange, albeit in a more tangible form than the usual
magic trick method.
We ask what the relative quantum mechanical phase diﬀerence is be-
tween processes of type A and type B. The key parameter to consider
is the angle that one particle is moved around the other. This is where
topology comes in: given a set of particle paths, we may smoothly dis-
tort the paths of the particles, but we may not change the number of
times particles wrap around each other without introducing singularities
in the particle paths. Processes of type A involves rotating particle 2
around particle 1 by angle φ = 2πp, where the winding number p takes
an integer value (including zero). Processes of type B involves rotations
of φ = π(2p + 1).
Each value of p describes a topologically distinct
process.
We suppose that these topologically distinct processes make a mul-
tiplicative contribution to the total wave function (or, in ﬁeld theory,
to the path integral) of Φ(φ) which we expect to be pure phase. If we
carry out a sequential string of these processes then we require that the
angles add, whilst the wave functions should multiply. That is, we need
Φ(φ1 + φ2) = Φ(φ1)Φ(φ2) which implies that Φ(φ) = eiηφ where η is a
parameter which, crucially, doesn’t need to be an integer.
Fig. 30.1
Examples of ways of ex-
changing particles. (a) and (b) are type
A processes, where the particles end up
at the same positions. In case (b) one
particle loops around the other once in
the exchanging process. (c) and (d) are
type B processes where the particles ex-
change positions. In case (d) one parti-
cle loops once around the other during
the exchange.
Now we compare our real life exchange to the old-fashioned deﬁnition
in eqn 30.1. If we carry out the exchange (x1, x2) →(x2, x1) [shown in
Fig. 30.1(c)] then the formal deﬁnition of exchange embodied in eqn 30.1
tells us that the wave function should be identical (for bosons) or pick
up a minus sign (fermions). However, the realistic version of exchange
merely tells us that φ = π, resulting in a phase factor of Φ(φ) = eiηπ.
The two versions of exchange are only identical for the special cases that
(a) we have η = even integer, when we recover the expected exchange
behaviour for bosons or (b) we have η = odd integer, when we recover
fermion exchange. However, this analysis shows that there are many
more possible values of η in two dimensions.
It doesn’t have to be
an integer! We are therefore not tied simply to bosons and fermions,
the freedom to choose η means we can have any exchange statistics.
Particles with such statistics were dubbed anyons by Frank Wilczek,
who has made important contributions to elucidating the study of these
Frank Wilczek (1951– ) was a co-
recipient of the Nobel Prize in Physics
in 2004 for their discovery of asymp-
totic freedom.
His popular book The
Lightness of Being (2008) is a very read-
able account of modern ideas in particle
physics.
particles and their ‘fractional statistics’.
Notice that having only two spatial dimensions to play with was vital
to the argument. In three spatial dimensions all of the type A processes
are topologically identical since they are all deformable into paths where
the particles don’t move. Similarly all of the type B processes are topo-
logically identical and may be reduced to a simple exchange of particles.
This reduction occurs because the extra dimension allows us to move
the paths past each other in the third dimension, shrinking all loops to
zero.

30.2
Chern–––Simons theory
269
30.2
Chern–––Simons theory
We now turn to the problem of writing down a Lagrangian that can be
described as topological. What do we mean by this? Our usual La-
grangians have relied on the use of the metric tensor gµν, which is an
object that tells us how to measure displacements in space and time.
It provides a set of instructions for taking the scalar product of vectors
through a contraction of indices. In contrast, in a topological theory
the contraction of indices is carried out through the use [in (2+1) di-
mensions] of the antisymmetric symbol εµνλ. Topological theories are
therefore blind to the details of the watches and measuring rods that
are contained in gµν. Their content is dependent purely on the topol-
ogy of the manifold1 in which we’re working. The ﬁrst consequence of
1A manifold is a topological space
that locally resembles Euclidean space.
this feature is a shock when we try to derive the Hamiltonian from a
topological Lagrangian: the Hamiltonian vanishes!
Example 30.1
To see this we note that a general method of ﬁnding the energy-momentum tensor
T µν is to vary the action with respect to the components of the metric. For an action
Stop derived by integrating a topological Lagrangian we have
T µν =
−2
√−det g
δStop
δgµν
.
(30.2)
Since topological theories don’t include gµν at all then we must have T µν = 0. Since
the Hamiltonian is the 00th component of this tensor then we conclude that H = 0.
A consequence of this is that all of the states of a topological Hamiltonian have
energy E = 0. So there is a signiﬁcant degeneracy in the system. Unfortunately it
is rather diﬃcult to determine how many states we have since this depends on the
topology of the manifold.
We will now describe a topological term that may be part of a La-
grangian. This is known as a Chern–––Simons Lagrangian and applies
Chern–Simons theories were ﬁrst intro-
duced into ﬁeld theory by Edward Wit-
ten (1951– ) based on work in diﬀeren-
tial geometry.
They are named after
Shiing-Shen Chern (1911–2004), math-
ematician, and James Harris Simons
(1938– ), mathematician, hedge fund
manager and philanthropist.
to the case of (2+1)-dimensional spacetime. It is written2
2Notice that the Chern–Simons theory
will work in (2+1)-dimensional space-
time but not in (3+1) dimensions. Try
writing it in (3+1) dimensions and
you’ll see that the indices won’t match
up.
L = −1
2κεµνλaµ∂νaλ,
(30.3)
where aµ is a U(1) gauge ﬁeld and κ is a constant. Chern–Simons terms
are easy to spot since they have the form L = εa∂a. We will see that
Chern–Simons theory contrasts in many respects with the other U(1)
gauge theory with which we’re familiar: electromagnetism.
Example 30.2
The Chern–Simons Lagrangian gives rise to a gauge invariant action. Actually the
gauge invariance is slightly more subtle than the usual version seen in electromag-
netism. To show this, we make the gauge transformation aµ →aµ + ∂µχ, and we
obtain
εµνλaµ∂νaλ →εµνλaµ∂νaλ + εµνλ∂µχ∂νaλ.
(30.4)

270
Topological ﬁeld theory
Here we have used the fact that εµνλ∂µ∂νχ = 0, which follows from the antisymmetry
of εµνλ. The resulting change in the action (that is, the second term in the above
equation) may be written as a total derivative
δS =
Z
d3x εµνλ∂µ(χ∂νaλ).
(30.5)
This vanishes under the assumption that we may drop boundary terms. We conclude
that the Chern–Simons Lagrangian is gauge invariant, as long as we discard the
boundary terms.
The next thing to note is that if we plug this term into the Euler–
Lagrange equations we obtain an equation of motion for the aµ ﬁelds
as
κεµνλ∂νaλ = 0,
(30.6)
or deﬁning a ﬁeld strength tensor fµν = ∂µaν−∂νaµ we have an equation
of motion 1
2κεµνλfνλ = 0 or
fµν = 0.
(30.7)
This looks very unexciting! Unlike electromagnetism, for which, in the
absence of sources, we have an equation of motion ∂µF µν = 0 which
supports plane waves, the Chern–Simons theory seems to have no dy-
namics of its own. Nevertheless, the Chern–Simons theory will turn out
to be interesting, but we just need to couple it to another ﬁeld.
Therefore, we now couple our Chern–Simons ﬁeld aµ to a source Jµ
which is the conserved current of some other ﬁeld and write
L = −1
2κεµνλaµ∂νaλ + aµJµ,
(30.8)
and this leads to an equation of motion
Jµ = κεµνλ∂νaλ = 1
2κεµνλfνλ.
(30.9)
Clearly this expression for current isn’t a consequence of Noether’s the-
orem; rather, it occurs as a result of our coupling a source to the Chern–
Simons gauge ﬁeld by hand. The Chern–Simons term therefore provides
a constraint on the current.
In order to understand the properties of the current we now point out
an odd, but undeniably relevant feature of (2+1)-dimensional spacetime.
This feature stems from the fact that, in two spatial dimensions, the
cross product results in a pseudoscalar, rather than a pseudovector. The
result is that the magnetic ﬁeld in (2+1)-dimensional spacetime is a
pseudoscalar: B = εij∂iAj.
Example 30.3
To see this we note that in three dimensions we have the deﬁnition of the cross
product in component form V i = εijkAjBk. In two dimensions we lose an index
from our antisymmetric symbol and we have S = εijAiBj. We run out of indices,
so the result of the cross product is a pseudoscalar as claimed.

30.3
Fractional statistics from Chern–––Simons theory
271
Now turning to the components of the current Jµ = (ρ, J) predicted
from eqn 30.9, we ﬁnd
ρ = κ(∂1a2 −∂2a1),
J1 = κ(−∂0a2 + ∂2a0),
J2 = κ(∂0a1 −∂1a0).
(30.10)
Although the ﬁeld aµ does not describe electromagnetism, we may use
the language of electromagnetism and call b = ∂2a1 −∂1a2 the aµ mag-
netic ﬁeld b and call ∂0ai −∂ia0 the ith component of the aµ electric
ﬁeld ei. We therefore have
ρ = −κb,
Ji = −κεijej,
(30.11)
where ε12 = 1. The second of these equations tells us that an aµ electric
ﬁeld in the y-direction gives a source current in the x-direction. The
meaning of the ﬁrst equation is made clear if we integrate over the two-
dimensional space to ﬁnd that the charge Q of the source ﬁeld is related
to the ﬂux of the aµ magnetic ﬁeld by
Q = −κ
Z
d2x b,
(30.12)
or

Charge of
source ﬁeld

∝
 Flux of
aµ ﬁeld

.
(30.13)
Thus we conclude the following:
Chern–Simons theory ties aµ-ﬂux to the charge of the source ﬁeld.
30.3
Fractional statistics from
Chern–––Simons theory
The fact that external source charge couples to aµ-ﬂux has an inﬂuence
on the statistics we ﬁnd on the exchange of particles. Recall that in
(2+1) dimensions moving one particle completely around another gives
rise to a phase contribution e2πiη, where η is an odd integer for fermions
and an even integer for bosons.
Now recall the Aharonov–Bohm eﬀect from Chapter 29. Moving one
charged particle completely around a source of ﬂux gives rise to a phase
contribution eiqΦ, where q is the particle charge and Φ is the ﬂux. Using
our Chern–Simons result that q = −κΦ, we obtain a prediction of the
Aharonov–Bohm phase of e−i q2
κ .
However, particle exchange (which
tells us about statistics) is slightly diﬀerent to this in that it involves
moving one particle around the other through an angle π. Interpreting
the above Aharonov–Bohm process as two lots of particle exchange (and
therefore contributing a phase 2πη) we obtain a result for the statistics
of our particles,3 namely that
3In fact, there’s another tricky factor of
two here. One is tempted to say that
since all particles carry both charge and
ﬂux then moving one charge around a
ﬂux also moves a ﬂux around a charge,
doubling the eﬀect compared to what
we have written. A more careful treat-
ment conﬁrms our naive picture of a
single charge moving around a single
ﬂux and gives eqn 30.14.
See Wen,
Chapter 7 for further discussion.
2πη = −q2
κ ,
(30.14)

272
Topological ﬁeld theory
and hence η = −q2
2πκ is not necessarily an integer. This implies that the
particles exhibit fractional statistics.
This chapter has explained how to construct a topological Lagrangian.
It is based on gauge ﬁelds; it supports excitations with fractional statis-
tics; its charged excitations carry ﬂux. In Chapter 45 we’ll examine a
phase of matter which is described by a Chern–Simons theory: this is
the fractional quantum Hall ﬂuid.
Chapter summary
• Topological ﬁeld theories such as Chern–Simons theory are built
from products of gauge theories summed together using the anti-
symmetric symbol εµνλ in place of a metric tensor. These theories
provide constraints on other ﬁelds to which they are coupled.
• Chern–Simons theory attaches ﬂux to the charge of the source ﬁeld.
• Chern–Simons theories may produce fractional excitations.
Exercises
(30.1) (i) Verify that εµνλ∂µ∂νχ = 0.
(ii) Verify eqn 30.4.
(30.2) Show that the Chern–Simons term won’t work in
(3+1)-dimensional spacetime. Suggest a form for
(4+1)-dimensional spacetime.
(30.3) Derive the equations of motion for the Chern–
Simons ﬁelds coupled to a source in eqn 30.9 using
the Euler–Lagrange equations.
(30.4) The propagator for Chern–Simons theory may be
found in a manner very similar to that used in
electromagnetism in Chapter 39.
Starting with
Chern–Simons theory we add a gauge ﬁxing term
−1
2ξ (∂µaµ)2 to obtain
L = −κ
2 εµνλaµ∂νaλ −1
2ξ (∂µaµ)2 + Jµaµ. (30.15)
(a) Show that the equations of motion for the ﬁeld
aµ are given by
Jµ =
»
κεµνλ∂ν −1
ξ ∂µ∂λ
–
aλ = M µλaλ.
(30.16)
(b) Show that the inverse of the equations of motion
is given, in momentum space, by the matrix
(M −1)λσ = −iελνσpν
κp2
+ ξ pλpσ
p4 ,
(30.17)
which provides the propagator for the theory via
i(M −1)λσ.
(c) Show that the Chern–Simons term therefore
leads to a term in the Lagrangian that may be writ-
ten
L = −1
2Jµ
„εµνλ∂ν
κ∂2
«
Jλ.
(30.18)
This is known as the Hopf term.
(30.5) Consider
Chern–Simons
electromagnetism
de-
scribed by a Lagrangian
L = −1
4fµνf µν −κ
2 εµνλaµ∂νaλ −1
2ξ (∂µaµ)2,
(30.19)
where fµν = ∂µaν −∂νaµ.
(a) Find the equations of motion for this theory.
(b) Show that the inverse of the equations of motion
is given by
(M −1)λσ = p2gλσ −pλpσ + iκελνσpν
p2(p2 −κ2)
+ ξ pλpσ
p4 .
(30.20)
What is the signiﬁcance of the pole in the propaga-
tor at p2 = κ2?

Part VIII
Renormalization: taming
the inﬁnite
This part is structured as follows:
• Interactions in quantum ﬁeld theories change the properties of the
particles, producing dressed particles, also known as quasiparticles.
These are examined in Chapter 31 which also gives an outline of
Landau’s Fermi liquid picture.
• Quantum ﬁeld theory has some uncomfortable divergences and
Chapter 32 shows how these can be tamed using renormalization
using additional counterterms.
• These ideas are applied to deriving propagators and making Feyn-
man diagrams in Chapter 33.
This allows us to write down a
Green’s function called the self-energy which describes the change
in the mass of a particle due to the interactions, and also a vertex
function which describes screening.
• The renormalization group is introduced in Chapter 34. Examples
treated include asymptotic freedom, Anderson localization, and
the Kosterlitz–Thouless transition.
• Chapter 35 treats the ferromagnetic transition as a further exam-
ple of the renormalization group procedure and shows how critical
exponents can be extracted.

31
Renormalization,
quasiparticles and the
Fermi surface
31.1 Recap: interacting and non-
interacting theories
274
31.2 Quasiparticles
276
31.3 The propagator for a dressed
particle
277
31.4 Elementary quasiparticles in
a metal
279
31.5 The Landau Fermi liquid 280
Chapter summary
284
Exercises
284
This chapter focuses on how the behaviour of particles is aﬀected by
interactions between them. We will see that the passage from a non-
interacting theory to an interacting theory involves not only a propensity
for particles to scatter from each other but, more dramatically, changes
in the properties of particles themselves, such as their mass and charge.
We will say that the values of these properties are renormalized. Fur-
thermore, we will ﬁnd that interactions even change what we mean by a
‘particle’. The process of renormalization may be imagined as a particle
dressing itself in interactions.
A rather familiar example of interactions changing the apparent prop-
erties of particles is the screening of a positive charge embedded in a
metal. The positive charge builds up a cloud of electron density around
it. Distant electrons do not feel its full charge because the nearby elec-
tron density screens the charge, reducing its apparent value. A second
example is the eﬀective mass of an electron in a crystal. The fact that a
crystal contains immobile ion cores which interact electrostatically with
the electron means that the apparent mass of the electron is not that of
the bare electron in a vacuum, but instead an eﬀective mass m∗.
We will discover that analogous processes occur in all interacting quan-
tum ﬁeld theories. Of course the diﬀerence between condensed matter
physics and a ‘fundamental’ theory such as quantum electrodynamics is
that, for the former, we may measure the mass and charge of electrons
both inside the crystal and in vacuo. In contrast, we cannot remove elec-
trons from the vacuum of quantum electrodynamics to see the diﬀerence
in their properties caused by interactions. However, the framework in
which we understand the fate of interacting particles is the same in both
cases and it is to this set of ideas which we now turn.
31.1
Recap: interacting and
non-interacting theories
Our simplest ﬁeld theories describe particles that don’t interact. We call
these free or non-interacting theories.1
1The simple scalar ﬁeld theory with La-
grangian L =
1
2 (∂µφ)2 −m2
2 φ2 is an
example of a free ﬁeld theory.

31.1
Recap: interacting and non-interacting theories
275
Example 31.1
Here’s a recap of the properties of a non-interacting theory. For a theory described
by a free Hamiltonian ˆH0, we deﬁne freely evolving ﬁeld operators and a vacuum
ground state. These may be deﬁned through
ˆH0|0⟩= 0,
ˆp|0⟩= 0,
⟨0|ˆφ(x)|0⟩= 0.
(31.1)
Particles are excitations of the vacuum.
Creation and annihilation operators cre-
ate single-particle modes, one at a time, labelled by their momenta. An example is
ˆa†
p|0⟩= |p⟩. The amplitude for a ﬁeld operator to create a (relativistically normal-
ized) single particle with momentum p is given by
⟨p|ˆφ†(x)|0⟩= eip·x.
(31.2)
Finally, to talk about particles we need a propagator. This describes a single particle,
inserted into the system at y and removed at x, and which, for a non-interacting
theory, will not aﬀect or be aﬀected by any other particles in the system.
The
Feynman propagator describing the evolution of a free, single particle is given by
˜G0(p) =
i
p2 −m2 + iǫ .
(31.3)
If we didn’t know the mass of the particle before (from its dispersion relation E2
p −
p2 = m2) then we could read it oﬀthe propagator as the pole of the Green’s function.
T
λ
0
1
ˆH = ˆH0
ˆH = ˆH0 + ˆH′
Fig. 31.1 Adiabatic turning on of ˆH′.
The full Hamiltonian is given by ˆH =
ˆH0 +λ(T) ˆH′. This thought experiment
allows us to examine what happens to
particles in an interacting theory.
So much for the world of non-interacting particles. What happens when
particles interact? Consider a Hamiltonian written as a sum of non-
interacting and interacting parts
ˆH = ˆH0 + λ ˆH′.
(31.4)
In order to examine the interacting system, we will imagine what hap-
pens to a free theory described by ˆH0 when we slowly turn on an inter-
action ˆH′. To do this we multiply ˆH′ by a function λ(T) which starts
vanishingly small and slowly approaches unity as we turn up some pa-
rameter T, as shown in Fig. 31.1. As λ grows, our system gradually
becomes a bubbling cauldron of interactions: the new eigenstates of the
full Hamiltonian ˆH (called |pλ⟩) are diﬀerent to those of ˆH0 (called |p⟩).
The interacting ground state |Ω⟩is diﬀerent from the non-interacting
ground state |0⟩and the dispersion of the particles is also altered. If
we put a test particle into this system it will interact with particles and
antiparticles, pulling them out of the vacuum. These particles and an-
tiparticles may also interact with still more particles and antiparticles,
doing all manner of complicated things. Our original particle may be
lost in all of this havoc and it might be doubted that we can even iden-
tify single particles in this system any more. We may have to abandon
our quantum ﬁeld theory which, after all, is based around the notion of
creating and annihilating single particles. We are led to ask: are there
such things as single-particle excitations in an interacting system? The
point of this chapter is to show that there are! They are called dressed
particles or quasiparticles.

276
Renormalization, quasiparticles and the Fermi surface
Quasiparticles are the excitations in interacting systems. They
resemble free particles, but have diﬀerent masses and interact
diﬀerently with their environment. The process of free particles
turning into dressed particles is called renormalization.
31.2
Quasiparticles
We imagine that as the interaction is turned on the particle begins its
transformation from a single non-interacting particle into a quasiparticle.
We say that the particle becomes dressed by interactions and becomes
a quasiparticle. The process of renormalization is therefore embodied in
the equation
(Quasiparticle) = (real particle) + (interactions),
(31.5)
where the word quasiparticle may be substituted by ‘dressed particle’ or
‘renormalized particle’, depending on the context.
Let’s examine what happens to a free particle as the interaction is
turned on. How do we know if it’s possible for an interacting quasi-
particle to exist with the same momentum once we’ve turned on the
interaction? To answer this, we act on the interacting vacuum with the
particle creation operator ˆφ†(x) ∝
R
p ˆa†
peip·x + ˆape−ip·x that we had for
the non-interacting system. As we turn on the interaction the ground
state is no longer |0⟩, it’s now |Ω⟩(where ˆH|Ω⟩= 0). The excited states
are called |pλ⟩, where λ tells us that we have interactions. The key to
understanding dressed particles is that the state ˆa†
p|Ω⟩created by our
ﬁeld operator won’t generally contain one single-particle excitation. In
fact, our operator ˆa†
p is a bull in a china shop: there’s nothing to stop
the free ﬁeld operator producing several excitations all at once!
To make a single-particle excitation with momentum p in the free
theory we acted on the free ground state with an operator ˆa†
p and said
|p⟩= ˆa†
p|0⟩. To make a single particle with momentum p in the inter-
acting theory we might act on the interacting ground state |Ω⟩with a
diﬀerent operator |pλ⟩= ˆq†
p|Ω⟩. This all makes sense, in that we need
to have the right tools for the job, that is, the right operators for the
ground states, and we make the expected number of particles (i.e. one).
Unfortunately we don’t know what ˆq†
p is and so can’t use it on the sys-
tem.
All we have is the single-particle creation operators ˆa†
p for the
non-interacting states.
But what happens if we use the wrong operator on the interacting
ground state? If we apply the operators for the non-interacting theory
ˆa†
p to the interacting ground state |Ω⟩we make a superposition of states.
One might be the state we intend to make |pλ⟩, but others will be
multiparticle states whose momenta each add up to p. That is
ˆa†
p|Ω⟩= |pλ⟩⟨pλ|ˆa†
p|Ω⟩+
X
(multiparticle parts).
(31.6)

31.3
The propagator for a dressed particle
277
For example, ˆa†
p|Ω⟩, might contain a multiparticle state2 with two par-
2Note that, in a metal, these multipar-
ticle parts may be thought of in terms
of the emission of electron–hole pairs.
See Chapter 43.
ticles and an antiparticle like this: ˆq†
p1 ˆq†
p2 ˆqp1+p2−p|Ω⟩.
In fact, it can be more complicated still, since we may only approxi-
mately create a state |pλ⟩(along with multiparticle parts) and in actual
fact create an ultimately unstable, narrow wave packet of width Γp re-
sembling this state. In particle physics this is known as a resonance.
In this case the energy of the particle may have an imaginary part Γp.
A complex energy Ep + iΓp will cause a particle to time evolve accord-
ing to eiEpte−Γpt or, in words, the particle is unstable to decay and has
a lifetime3 (2Γp)−1. After a time that’s long compared to (2Γp)−1 we
3Where the factor of two comes from
considering the square modulus of the
particle wave function.
can’t expect there to be a quasiparticle any more.
With all of these particles ﬂying about and possibly decaying how can
we say whether we have a single particle? We call the amplitude for
making the desired single-particle part the quasiparticle weight Z
1
2p ,
where Z
1
2p = ⟨pλ|ˆa†
p|Ω⟩, and where |pλ⟩is a relativistically normalized
momentum state. We say that we have a quasiparticle if Z
1
2p ̸= 0. In the
interacting theory, we can state that the amplitude in position space for
creating a single particle with a momentum p is
⟨pλ|ˆφ†(x)|Ω⟩= Z
1
2p eip·xe−Γpt.
(31.7)
In fact, to meaningfully describe a particle as a quasiparticle we will
require that Ep > Γp, that is, its energy must be greater than its decay
rate. Comparing with the analogous result obtained for non-interacting
scalar ﬁeld theory we conclude that, when Γp is small, we can describe
particles in an interacting theory by multiplying4 our single-particle wave
4In the jargon, one says ‘renormalizing’
rather than ‘multiplying’.
functions by the factor Z
1
2p .
It’s important to note that the energies of interacting single particles
are diﬀerent from those of free particles.
In the (relativistic) scalar
ﬁeld theory we’re describing5 the interactions change the mass of the
5In which the free particle dispersion is
given by Ep = (p2 + m2)
1
2 .
single-particle excitation from m (often called the bare mass) to mλ,
giving an energy for the state |pλ⟩of (p2 + m2
λ)
1
2 . Since mλ is the mass
of a single particle measured in our experiments, it’s often called the
physical mass and given the symbol mP. The following table shows
quantites for both non-interacting and interacting theories.
Non-interacting:
|p⟩= ˆa†
p|0⟩
⟨p|ˆφ(x)|0⟩= eip·x
m
Interacting:
|pλ⟩= ˆq†
p|Ω⟩
⟨pλ|ˆφ(x)|Ω⟩= Z
1
2p eip·xe−Γpt
mP
31.3
The propagator for a dressed particle
For serious calculations we need to identify a propagator for the inter-
acting system via our deﬁnition
G(x, y) = ⟨Ω|T ˆφH(x)ˆφ†
H(y)|Ω⟩.
(31.8)

278
Renormalization, quasiparticles and the Fermi surface
It can be shown, without the aid of perturbation theory, what the gen-
eral form for the interacting propagator G should be.6 The propagator,
6This can be achieved simply by sliding
a resolution of the identity between the
ﬁeld operators in eqn 31.8. This reso-
lution will be made up from the exact
eigenstates |pλ⟩of the full Hamiltonian
ˆH.
For details of the calculation, see
Peskin and Schroeder, Chapter 7.
which is built out of free operators ˆap and ˆa†
p, will not only describe the
propagation of single-particle states, but also describe the fate of the
multiparticle states, and takes the form
˜G(p) =
iZp
p2 −m2
P + iΓp
+

multiparticle
parts

,
(31.9)
where, as before, Z
1
2p = ⟨pλ|ˆφ(0)|Ω⟩is the amplitude for creating a single-
particle state |pλ⟩out of the interacting vacuum and mP is the mass of
the state |pλ⟩. Notice the role of Γp here: it is the inverse quasiparticle
lifetime in the place of the usual inﬁnitesimal factor ǫ.
In most cases involving long-lived ele-
mentary particles Zp is independent of
the three-momentum p and is simply
denoted Z. As we shall see, a depen-
dence on p is more relevant for metals
in solids.
Example 31.2
One way to write the full propagator that makes contact with experiment uses the
spectral density function ρ(M2). For example, for scalar ﬁeld theory we rewrite
the propagator in terms of the free propagator7 ∆(x, y, M2) which is now made a
7Recall that the free scalar ﬁeld propa-
gator ∆(x, y, M2) can be written in mo-
mentum space as
∆(p, M2) =
1
p2 −M2 + iǫ .
function of particle mass as well as the spacetime points x and y:
G(x, y) =
Z ∞
0
dM2
(2π) ρ(M2)∆(x, y, M2).
(31.10)
Suppose we have a spectral density function
ρ(M2) = (2π)δ(M2 −m2
P)Z +
„ multiparticle
parts
«
.
(31.11)
This is shown in Fig. 31.2(a). It is made up of a single-particle peak at mass M ≡mP:
the physical mass of a single particle in the interacting theory. It also contains a
multiparticle continuum which starts around 4m2
P, which is the threshold for making
two real particles in the centre of mass frame. This sort of spectral function is the
one usually seen in particle physics problems where the creation of more than one
particle is due to pair production.
Plugging in this spectral density we obtain
(a)
(b)
M2
ρ(M2)
m2
p
M2
ρ(M2)
m2
p
∼4m2
p
Fig. 31.2
(a) The spectral function
showing a single-particle peak. (b) The
spectral function showing a quasiparti-
cle peak with lifetime (2Γp)−1.
˜G(p) =
iZ
p2 −m2
P + iǫ +
Z ∞
≈4m2p
dM2
2π ρ(M2)
i
p2 −M2 + iǫ .
(31.12)
Notice that the lifetime of the particle is ǫ−1, which is eﬀectively inﬁnite since the
particle pole is inﬁnitesimally close to the real axis. We expect these particles to be
long-lived and stable.
Now suppose we have the spectral density in Fig. 31.2(b), which has a quasiparticle
peak with width 2Γp. This leads to a propagator
˜G(p) =
iZp
p2 −m2
P + iΓp
+
„ multiparticle
parts
«
,
(31.13)
where the quasiparticle now has a ﬁnite lifetime (2Γp)−1 (and we allow the possibility
that Z is a function of p). We see that our requirement that Ep > Γp for a meaningful
quasiparticle tells us that the width of the peak in the spectral weight should be
narrow compared to the distance of the peak from the origin.
A spectral weight
with a broad quasiparticle peak is often seen in condensed matter physics where
there is usually a large number of very closely spaced energy levels.
As a result
the quasiparticle is a wave packet spread over (potentially very many) levels which
eventually decays away.
In addition to the quasiparticle having a ﬁnite lifetime it can also be shown that
the multiparticle part usually undergoes some degree of destructive interference and
decays after a time that is fairly small compared to Γ−1
p , leaving the lone quasiparticle
amongst the ruins!

31.4
Elementary quasiparticles in a metal
279
Once we obtain the full propagator ˜G(p) we can immediately read oﬀ
the mass energy of the excited states mP from the position of the pole
in the function, the residue at that pole gives us iZp, the lifetime is
determined from Γp and, of course, we can use ˜G(p) to calculate other
quantities too, such as scattering amplitudes.
31.4
Elementary quasiparticles in a metal
The concept of dressed particles is especially important in condensed
matter physics.
Although, in reality, the condensed matter physicist
usually deals with a system made up from a large number of very strongly
interacting, real particles (such as electrons in a metal), renormalization
involves changing our point of view and describing the system in terms
of a small number of weakly interacting or non-interacting, ﬁctitious
particle-like excitations.
Despite the fact that the ground state of a
system, such as a crystal for example, isn’t an empty box8 we regard
8Far from it, it usually comprises N ≈
1023 particles of one sort or another.
it as a vacuum, in that it contains no excitations. A weakly excited
state of the whole system (that is, one at low temperature) can then be
described in terms of a small number of these excitations. These are
often also called the elementary excitations of the system.9
9The approach of describing a system
in terms of small numbers of excitations
works in condensed matter physics be-
cause we are most often interested in
the behaviour of a solid at low temper-
ature and this behaviour is dominated
by the occupation of only those energy
levels of the system very close to the
ground state.
A distinction can be made between two types of elementary excitation.
The ﬁrst are called collective excitations and correspond to particle-
like excitations that occur through motion of all of the constituent parts
of the underlying system. An example is the chain of coupled masses we
considered in Chapter 2. There we described the excited states of the
system, which involved the motion of all of the masses, as particle-like
entities called phonons.10
10Notice that if we turn oﬀthe inter-
actions in the system (in this case the
springs between the masses) then these
excitations cease to exist. Another ex-
ample of a collective excitation is the
plasmon, which is a collective excita-
tion of all of the electrons in a metal.
We will examine these in Chapter 43.
(a)
(b)
(c)
Fig. 31.3 (a) The non-interacting elec-
tron gas with a single electron above
the Fermi level. (b) The particle exci-
tations in the non-interacting electron
gas. (c) The quasiparticles in the inter-
acting electron gas [identical to (b) but
with renormalized masses].
The second sort of elementary excitation is the quasiparticle, a single-
particle excitation from the non-interacting system dressed in interac-
tions. An example is the single-particle excitation of an electron gas. In
a non-interacting electron gas [Fig. 31.3(a)] we have single-particle exci-
tations above the ground state called electrons and holes11 [Fig. 31.3(b)].
11Remember that in the picture dis-
cussed
here,
excitations
only
exist
above the ground state.
The ground
state here consists simply of electron
states ﬁlled up with electrons all the
way to the Fermi level.
In the non-
interacting system an electron above
the Fermi level is an excitation, which
becomes a quasiparticle upon turning
on the interactions.
An electron be-
low the Fermi sea is not an excita-
tion and does not become a quasipar-
ticle. A contrasting philosophy, where
all electrons become quasiparticles, is
Landau’s Fermi liquid theory, which is
examined later in the chapter.
As we turn on the interactions these excited electrons and holes become
quasielectrons and quasiholes [Fig. 31.3(c)] .
Example 31.3
Let’s look at the case of electron quasiparticles in more detail. We start with the
non-interacting version of a metal: the Fermi gas at T = 0. This comprises a box
of N electrons, stacked up in momentum eigenstates |p⟩up to the Fermi level |pF⟩.
(For simplicity we regard the electrons as spinless and, in that case, the momentum
states may contain either 1 or 0 electrons.) The momentum distribution n(0)
p
for the
Fermi gas is shown in Fig. 31.4(a). This is the ground state of this non-interacting
system, which we call |0⟩. The particle dispersion is given by E(0)
p
= p2/2me, where
me is the free electron mass. Note that near the Fermi level the dispersion may be
written E(0)
p
= vF(|p| −pF), where
vF = ∇pEp||p|=pF = pF
me
,
(31.14)

280
Renormalization, quasiparticles and the Fermi surface
is known as the Fermi velocity12 [see Fig. 31.5(b)].
12The Fermi surface is spherical here so
the Fermi velocity only depends on the
magnitude of pF and we write |pF| =
pF.
We now turn on interactions between the electrons. The interacting ground state
of the system is |Ω⟩, the eigenstates of the system are now called |pλ⟩, and the energies
of these states are now Ep. Near the Fermi level we again have Ep = vF(|p|−pF), but
now vF = pF/m∗, where we call m∗the eﬀective mass. Following from our assertion
that we may describe the weakly excited states of a system as quasiparticles, we
expect that those states that lie close to the Fermi level pF will have nonzero values
of Z
1
2
p and may therefore be described as containing quasiparticles. As will be shown
in Chapter 43 the free propagator for particles in a metal is given by
˜G0(p) =
iθ(|p| −pF)
E −E(0)
p
+ iǫ
+
iθ(pF −|p|)
E −E(0)
p
−iǫ
.
(31.15)
The interacting version becomes
˜G(p) = iZpθ(|p| −pF)
E −Ep + iΓp
+ iZpθ(pF −|p|)
E −Ep −iΓp
+
„ Multiparticle
parts
«
.
(31.16)
It will be useful in what follows to have the propagator in three-momentum space and
the time domain. This is achieved by doing a Fourier transform (see Appendix B)
with the result
G(p, t)
=
Zp
h
θ(t)θ(|p| −pF)e−iEpte−Γpt −θ(−t)θ(pF −|p|)e−iEpteΓpti
+
„ Multiparticle
parts
«
.
(31.17)
|p|
np
pF
|p|
np
pF
Fig. 31.4
(a) The momentum distri-
bution of the Fermi gas. (b) The mo-
mentum distribution of the interacting
Fermi system.
A discontinuity of size
ZpF exists at the Fermi surface.
|p|
Ep
pF
|p|
Ep
pF
Fig. 31.5
(a) The dispersion of the
free particles in a Fermi gas. (b) Near
the Fermi momentum the dispersion is
given by Ep = (|p|−pF)vF, where vF =
pF/m∗and m∗is the eﬀective mass of
the particles.
We may use the result of the last example to test whether we really have
quasiparticles in a real-life metal by making a direct measurement of the
quasiparticle weight Zp. To measure Zp we make a measurement of the
momentum distribution np of the weakly excited system (that is, when
it contains some quasiparticles), given by np = ⟨ˆa†
pˆap⟩which may be
related to the propagator via (see Exercise 31.1.)
np = −limt→0−G(p, t),
(31.18)
giving us a prediction
np = Zpθ(pF −|p|) −
 Multiparticle
parts

.
(31.19)
Assuming that the multiparticle contribution varies smoothly, we con-
clude that the interacting metal should have a discontinuity in its mo-
mentum distribution np at its Fermi surface of size Z|p|=pF, as shown
in Fig. 31.4(b). This prediction can be tested by performing Compton
scattering experiments on metals. The results are found to be in good
agreement with the theoretical predictions.
31.5
The Landau Fermi liquid
Philip Anderson is arguably the most
inﬂuential condensed matter physicist
since Landau. Not only is he the dis-
coverer/creator of a variety of theoret-
ical descriptions of electronic and mag-
netic phenomena (among many other
things); his elucidation of the structure
underlying the subject pervades current
condensed matter physics.
(Even the
name ‘condensed matter physics’ is one
of his contributions!)
In general, I believe that the attitude is at least justiﬁable
that the instructive use of quasi-particles by the founders of
solid state physics and by Landau’s school is hardly less rig-
orous than the sophisticated many-body theory, and perhaps
it is more foolproof, because it has less appearance of being
rigorous.
P. W. Anderson (1923– ), Concepts in Solids

31.5
The Landau Fermi liquid
281
A rather diﬀerent way of understanding metals in terms of ﬁctitious par-
ticles was formulated by Lev Landau13 and is known as Fermi liquid
13Lev Landau (1908–1968) was a So-
viet physicist and one of the great-
est scientists of the twentieth century
who made contributions in many ar-
eas, including phase transitions, mag-
netism, superconductivity, superﬂuid-
ity, plasma physics and neutrinos. He
was also the model for the principal
character, Viktor Shtrum, in Vassily
Grossman’s novel Life and Fate.
theory. It has been so useful in understanding metals that it is lit-
tle exaggeration to claim that it has become the standard model of the
metal.14 Landau’s theory is phenomenological, but appealingly intuitive
14The concept of a Fermi liquid is a
high successful phenomenological the-
ory of interacting electrons. It is a bit
of a departure from our main theme
of ﬁeld theories of interacting systems,
but it is too useful to omit and more ad-
vanced treatments than the one given
here demonstrate that the machinery of
diagrammatic perturbation theory can
indeed be employed to derive important
results in Fermi liquid theory.
since it involves a description of a strongly interacting metal as being
almost identical to the non-interacting Fermi gas! Moreover Fermi liq-
uid theory allows the complete description of an interacting system in
terms of a small number of parameters, while avoiding all of the com-
plexities of perturbation theory. Central to the model is the fact that
Landau describes an interacting metal in terms of quasiparticles; but
these are diﬀerent to the ﬁeld theory quasiparticles described thus far
in this chapter. We will call the former Landau quasiparticles to avoid
confusion.
In Landau’s picture we pay particular attention to the process of ‘turn-
ing on’ the interaction between the non-interacting electrons of the Fermi
gas. Landau assumed that if we very slowly turn on the interaction the
system evolves continuously from Fermi gas to Fermi liquid, with each
single-particle momentum eigenstate of the gas evolving into a single-
particle momentum eigenstate of the liquid. This adiabatic turning on
may be shown in ordinary quantum mechanics to lead to a very small
amplitude for transitions out of the level, providing the density of ﬁnal
states is small, as it is for electrons within the Fermi sea as a result of
the Pauli principle.
Vital to Landau’s Fermi liquid concept is the notion that all of the
the states |p⟩occupied by an electron in the gas with n(0)
p
= 1 be-
come single-particle eigenstates |p⟩in the liquid occupied by a Landau
quasiparticle. The Landau quasiparticles in the interacting ground state
therefore also have momentum distribution n(0)
p . We say that there is a
one-to-one correspondence between the free particles and Landau quasi-
particles. On turning on the interaction the Landau quasiparticles take
on an eﬀective mass m∗which parametrizes the change in energy of the
eigenstates due to the eﬀect of the ﬁeld from the other quasiparticles.
|
⟩
|
⟩
|
⟩
|
⟩
|
⟩
|
⟩
|
⟩
|
⟩
Fig.
31.6
Turning on interactions
changes the states but the energy lev-
els shouldn’t cross during the turning
on process. Thus we can have (a) but
not (b).
Despite the change in energy, there should be no ambiguity in the
identity of an occupied eigenstate after turning on the interaction and
therefore the energy levels of the states shouldn’t cross during the turn-
ing on process. That is to say, upon turning on the interaction we must
have the process shown in Fig. 31.6(a) rather than that in Fig. 31.6(b).
We might ask whether this non-crossing of levels is a realistic propo-
sition.
Recall that when two eigenstates are related by a symmetry
transformation which leaves the Hamiltonian invariant we obtain a de-
generacy in energy. If we turn on a perturbing potential V which leads
to a nonzero matrix element δ between the two states we have a Hamil-
tonian H =
 E
δ
δ
E

which leads to a splitting in energy of 2δ. It’s
as if the energy levels repel each other by virtue of a matrix element δ
existing between them, which prevents them from ever crossing. The
consequence of this for the metal is that, as we turn on the interaction,

282
Renormalization, quasiparticles and the Fermi surface
the levels repel each other when they get too close and never cross. This
is shown in Fig. 31.7. However, this repulsion does not occur when the
matrix element between two levels is zero. This is the case when the
levels have diﬀerent symmetries and then there is nothing stopping the
levels from crossing.15 Since in the absence of a phase transition the slow
15This is exactly the case at a phase
transition when, as we’ve seen, the sys-
tem ﬁnds a new ground state with
a lower symmetry than its previous
ground state. The requirement that the
levels don’t cross is therefore a require-
ment that the evolution of the system
doesn’t involve a change in thermody-
namic phase.
turning-on of interactions provides a one-to-one correspondence between
Landau quasiparticles and free particles, the theory is said to possess
adiabatic continuity.
non-interacting
interacting
λ
turning on
Energy
Fig. 31.7 Evolution of single-particle
energy levels as interactions are turned
on. Notice how they repel when they
draw close to each other and never
cross.
Notice how diﬀerent the Landau quasiparticles are to our ﬁeld the-
ory idea of quasiparticles.
In ﬁeld theory the ground state contains
no quasiparticles; in Fermi liquid theory it contains as many Landau
quasiparticles as we had electrons in the Fermi gas, guaranteeing charge
conservation, provided Landau quasiparticles carry the same charge as
electrons. Landau quasiparticles do not, therefore, rely on the renor-
malization constant Zp to guarantee their existence as our ﬁeld theory
quasiparticles do.
An insight comes when, instead of starting with a ground state Fermi
gas, we start with a ground state Fermi gas with an additional electron
added with |p′| > pF and, again, turn on the interaction. We would hope
that the excited electron evolves into a quasiparticle in a momentum
eigenstate |p′⟩. However, unlike the electrons within the Fermi sea, the
extra electron potentially has rather a lot of phase space to explore as we
turn on the interactions. Through interactions with the other electrons
it may therefore have a considerable amplitude to scatter out of its state
|p′⟩and so the excited quasiparticle will have a ﬁnite lifetime (2Γp′)−1
against scattering into other states and decaying.16 The phase space for
16Notice that this sets a limit on the
speed at which we may turn on the
interactions.
Although we would like
to turn them on inﬁnitely slowly, any
slower than Γ−1
p′ and our excited quasi-
particle loses its momentum and, con-
sequently, its identity.
For a general
quantum liquid to exist it must be pos-
sible to select a turning on time that
allows the ground state to evolve adia-
batically, but also retains the low-lying
quasiparticles.
this scattering, restricted by the Pauli principle, is such that those states
near the Fermi momentum have the smallest probabilities for scattering
and therefore survive for longest in quasiparticle momentum states. This
is examined in the exercises with the result that the decay rate varies as
Γp ∼(|p| −pF)2. We conclude that really it is meaningful to speak of
the properties of a Landau quasiparticle only near the Fermi surface in
a metal where Γp is small.
Finally, if we lift a Landau quasiparticle out of a level below pF in
the ground state and promote it to a level with momentum ∆p above
pF, we simultaneously create an unoccupied quasiparticle state or hole17
17Holes will be examined in Chap-
ter 43.
with momentum ∆p below pF. Since ∆p = |p| −pF, the quasielectrons
and quasiholes will both have decay rates Γp ∝(|p| −pF)2, so it is only
meaningful to discuss their properties if their momenta are close to pF.
To summarize the philosophy of the Fermi liquid:
The Fermi liquid ground state contains Landau quasiparticles stacked
up to the Fermi level. Landau quasiparticles have the same charge as
ordinary electrons but their mass takes on a renormalized value m∗.
A quasiparticle excitation outside the Fermi sea (or, equivalently, a
quasihole within the sea) will have a decay rate Γp ∝(|p| −pF)2.
A cartoon of the Landau Fermi liquid theory is shown in Fig. 31.8.

31.5
The Landau Fermi liquid
283
Where the Fermi liquid and quantum ﬁeld theories coincide is in the
description of the weakly excited states of the metal. The excited Landau
quasiparticles have exactly the same properties as our quantum ﬁeld
theory quasiparticles and the argument about their lifetime applies to
both. Near the Fermi surface the energy of an excited quasiparticle may
be written Ep ≈
pF
m∗(|p| −pF) and its decay rate Γp ∝(|p| −pF)2 We
see that for small (|p| −pF) the real part of the quasiparticle energy
will be larger than the imaginary part, or Ep > Γp, and a meaningful
quasiparticle exists in the metal.
(a)
(b)
Fig. 31.8
A cartoon of the Landau
Fermi liquid.
(a) The non-interacting
Fermi gas with a single electron pro-
moted into an excited state (indicated
as a dotted line).
(b) In the Landau
Fermi liquid the interactions are turned
on so that the electrons become quasi-
particles and the holes become quasi-
holes.
There is a one-to-one corre-
spondence between the quasiparticles
and quasiholes in the interacting liquid
and the electrons and holes in the non-
interacting picture.
We now complete our description of the Landau Fermi liquid with
a discussion of the total energy of a weakly excited state of a metal.
In the spirit of many of Landau’s arguments, in which any ignorance
of microscopic details is no barrier to formulating a meaningful power
series expansion, we write the energy of the Fermi liquid in the limit of
a low density of quasiparticles as
E = Eg +
X
p
(E(0)
p
−µ) δnp + 1
2
X
pp′
fpp′ δnp δnp′ + · · · ,
(31.20)
where δnp = np −n(0)
p
is the diﬀerence between a distribution with exci-
tations and the ground state distribution18 and Eg = P
p E(0)
p n(0)
p
is the
18This implies that it is neither np nor
n(0)
p
that is the crucial quantity, but
rather it is their diﬀerence δnp, which
tells us the number of excitations in the
excited state. This is fortunate since we
know neither np nor n(0)
p
with much ac-
curacy, but we can ﬁnd δnp.
energy of the ground state. The term linear in δnp describes the excita-
tion of isolated quasiparticles of energy E(0)
p −µ = ∂E/∂np. The second-
order coeﬃcient19 fpp′ = ∂2E/∂np∂np′ describes the contribution from
19The partial diﬀerentials in the ex-
pressions for E(0)
p
−µ and fpp′ are eval-
uated about the ground state conﬁgu-
rations, i.e. a frozen Fermi sea.
quasiparticle–quasiparticle scattering (i.e. from interactions), and can
be described via an interaction Hamiltonian HI = 1
2
P
pp′ fpp′ δnp δnp′.
Example 31.4
To understand the scattering process in more detail, one should include spin and
this amounts to replacing fpp′ by fpσ;p′σ′. One can show20 that for spin-conserving
20Further details and a fuller account
may be found in the book by P. Cole-
man.
interactions this quantity can be written as
fpσ;p′σ′ = fs(cos θ) + fa(cos θ)σ · σ′,
(31.21)
where cos θ =
p·p′
|p||p′|. The functions fs and fa can be expanded in terms of Legendre
polynomials so that
fs,a(cos θ) =
1
g(EF)
∞
X
ℓ=0
Pℓ(cos θ)F s,a
ℓ
,
(31.22)
where F s
ℓand F a
ℓare Landau parameters and g(EF) is the density of states at
the Fermi level.21 These expressions allow some of the key properties of the Landau
21See Chapter 43.
Fermi liquid to be deduced. For example, one can show that the eﬀective mass is
m∗= m(1+F s
1) and the spin susceptibility is χ = µ0µ2
Bg(EF)/(1+F a
0 ). Many of the
predictions of Landau’s Fermi liquid theory may be shown (after much hard graft!)
to coincide with those of quantum ﬁeld theory and, owing to its comparative ease of
use, it is still regarded as the best way of understanding the results of experiment on
numerous condensed matter systems.
We have seen in this chapter that renormalization is an essential step
in understanding what happens to particles in an interacting quantum

284
Renormalization, quasiparticles and the Fermi surface
ﬁeld theory. In fact, as shown in Fig. 31.9, we could summarize the steps
to make a working theory as: (i) write a Lagrangian; (ii) quantize the
free part; (iii) derive Feynman rules for interactions; (iv) renormalize.
However, renormalization is often presented as a method of eliminating
nonsensical inﬁnities from theories. The elimination of these inﬁnities is
the subject of the next chapter. It should be remembered throughout
that renormalization is necessary for all interacting theories, indepen-
dent of whether or not we have inﬁnities.
Lagrangian
Canonical
Quantization
Dyson’s
expansion
for S
Path integral
gives
Z[J]
Feynman
rules
Renormalize
Green’s
functions and
predictions
add a source
current J
free part
interacting part
infer
infer
quantization propagators
Fig. 31.9 The process of doing quan-
tum ﬁeld theory includes the necessary
step of renormalizing.
Chapter summary
• Turning on the interactions in a theory changes the properties of
the particles. We say that they dress themselves in interactions.
• The interacting propagator is
˜G(p) =
iZp
p2 −m2
P + iǫ + (multiparticle parts),
where Zp is the quasiparticle weight
• In Landau’s Fermi liquid theory there is a one-to-one correspon-
dence between electrons in the non-interacting theory and quasi-
particles in the interacting theory. The eﬀect of interactions is to
renormalize physical quantities, such as the eﬀective mass, via the
Landau parameters F s
ℓand F a
ℓ.
Exercises
(31.1) Verify that, for fermions,
np = −lim
t→0−G(p, t).
(31.23)
(31.2) Consider an electron with energy E1 ≥EF scatter-
ing with an electron with energy E2 ≤EF at T = 0.
In order for this to occur we must have ﬁnal elec-
tron state E3 ≥EF and E4 ≥EF.
(a) Show that this implies that the lifetime of an
electron with E1 = EF is inﬁnite.
(b) If E1 is a little diﬀerent to EF, why does the
scattering rate go as (E1 −EF)2?
(c) For T ̸= 0 argue that we expect a scattering
rate 1
τ = a(E1 −EF)2 + b(kBT)2, where a and b are
constants.
(31.3) (a) Using the relation
1
x0 + iǫ = P
x0 −iπδ(x0),
(31.24)
(discussed in Appendix B) identify the delta func-
tion part of the propagator
iZp
E−Ep+iǫ.
(b) For the propagator
iZp
E−Ep+iΓp , show that the
full width at half maximum of the peak is given by
2Γp.

32
Renormalization: the
problem and its solution
32.1 The problem is divergences
285
32.2 The solution is counterterms
287
32.3 How to tame an integral 288
32.4 What
counterterms
mean
290
32.5 Making
renormalization
even simpler
292
32.6 Which
theories
are
renor-
malizable?
293
Chapter summary
294
Exercises
294
It turns out that the obvious generalization of this idiotically
simple manipulation gets rid of all of the inﬁnities for any
ﬁeld theory with polynomial interactions, to any order in
perturbation theory.
Sidney Coleman (1937–2007)
In the last chapter, we saw how interactions dress a particle and change
its propagator. The dressed particles were quite diﬀerent to the bare par-
ticles that are revealed after canonical quantization of the free theory.
This might mean that our perturbation theory, which involved expan-
sions in the bare mass m and coupling constant λ, is at risk. It is: it
turns out that we’ve been doing our perturbation theory about the wrong
masses and coupling constants! The solution to this problem involves
making a shift so that we’re expanding in physical masses and coupling
constants. The procedure also has a bonus. It solves a big problem in
our perturbation theory: the problem of divergent amplitudes.
In this chapter we approach the problem by starting with the diver-
gences in our perturbation theory. We will look at why we obtain inﬁni-
ties in our calculations and suggest an idiotically simple mathematical
workaround. The physical content of this workaround will turn out to
shift the parameters of the theory from the wrong ones (m and λ) to
the right ones (mP and λP).
32.1
The problem is divergences
We’ve looked at calculating amplitudes and propagators for several in-
teracting theories. Perhaps you’re curious (or suspicious) as to why we
haven’t done the integrals at the end of the calculations. The reason is
that many of them diverge. Understanding the source of the divergences
relies purely on the following facts (with a ﬁnite and positive):
Z ∞
a
dx xn
=
 xn+1
n + 1
∞
a
diverges for n ≥0,
(32.1)
Z ∞
a
dx
x
=
[ln x]∞
a
diverges,
(32.2)
Z ∞
a
dx
xm
=
 x−m+1
−m + 1
∞
a
= a−m+1
m −1 for m > 1.
(32.3)

286
Renormalization: the problem and its solution
In the ﬁrst integral the divergence arises because there are more powers
of x in the numerator than the denominator. The second integral, where
the number of powers of x is the same on top and bottom, is called log-
arithmically divergent, for rather obvious reasons. The last example
is convergent.
Let us return to φ4 theory, which is described by a Lagrangian
L = 1
2(∂µφ)2 −m2
2 φ2 −λ
4!φ4.
(32.4)
We ended our analysis of this theory in Chapter 19 with a perturbation
expansion encoded in Feynman diagrams. Consider the amplitude for
two-particle scattering, which is equal to the sum of all connected, am-
putated Feynman diagrams with four external legs. Up to second order
in the expansion (i.e. drawing all diagrams with one or two interaction
vertices) we obtain the diagrams shown in Fig. 32.1. The main message
of this section is that the diagrams with loops in them [(b),(c) and (d)]
are divergent. This is to say, with the upper limit of the integral set
to inﬁnity we get inﬁnite and therefore nonsensical answers. This is a
disaster.
(a)
(b)
(c)
(d)
p3
p4
p1
p2
−iλ
p3
p4
p1
p2
q
p3
p4
p1
p2
q
p3
p4
p1
p2
q
p1 + p2 −q
p1 −p3 −q
p1 −p4 −q
s
t
u
Fig.
32.1
The expansion of four-
point diagrams for two-particle scatter-
ing up to second order in the interaction
strength. Diagrams (b)–(d) deﬁne the
processes labelled s, t and u, respec-
tively.
Example 32.1
Let’s see how this unfolds. We’ve encountered the ﬁrst diagram before. The result is
iMa = −iλ.
(32.5)
For the subsequent diagrams, the integral we’ll need is
Z Λ
0
d4q
(2π)4
i
q2 −m2 + iǫ
i
(p −q)2 −m2 + iǫ = −4ia ln
„ Λ
p
«
,
(32.6)
where a is some numerical constant whose exact value isn’t important to us. Notice
that the limits of this integral are zero and Λ. The parameter Λ is a large momentum
cut-oﬀ, and we want to send Λ →∞. By counting powers we see that there are four
powers of momentum on top and four below, telling us that the integral ∼
R d4q
q4
is
logarithmically divergent. In fact, for the diagrams in Figs. 32.1(b)–(d) we obtain
the amplitudes:
iMb = iaλ2 ˘
ln Λ2 −ln
ˆ
(p1 + p2)2˜¯
= iaλ2 ˘
ln Λ2 −ln s
¯
,
(32.7)
iMc = iaλ2 ˘
ln Λ2 −ln
ˆ
(p1 −p3)2˜¯
= iaλ2 ˘
ln Λ2 −ln t
¯
,
(32.8)
iMd = iaλ2 ˘
ln Λ2 −ln
ˆ
(p1 −p4)2˜¯
= iaλ2 ˘
ln Λ2 −ln u
¯
.
(32.9)
All of these tend to ∞as Λ →∞. The total amplitude for two-particle scattering is
given by the sum of these diagrams, giving
iM =
=
i (Ma + Mb + Mc + Md)
=
−iλ + iaλ2 ˘
3 ln Λ2 −ln s −ln t −ln u
¯
,
(32.10)
and so we need to ﬁnd a way to tame the iaλ2 ˘
3 ln Λ2¯
∝ln Λ term.

32.2
The solution is counterterms
287
We can avoid the divergence in these integrals if we make the pragmatic
choice to only integrate up to a large, but ﬁnite, value of the cut-oﬀΛ.
This amounts to deliberately ignoring any ﬁne-scale details in the ﬁelds
below a length scale ≈1/Λ. This is frequently what is done in condensed
matter physics, where we usually ignore all detail smaller than the size of
an atom. If we apply our theory to fundamental particles it’s not quite
so obvious why there should be a smallest scale but it might represent
some graininess in spacetime.1
1Actually, the idea of a length scale like
this emerges very naturally from a way
of looking at the Universe called the
renormalization group, which we dis-
cuss in Chapter 34.
For now, let’s see if we can live (uncomfortably) with Λ ̸= ∞. In-
troducing ﬁnite Λ has an immediate consequence: the amplitudes we
calculate using Feynman diagrams will depend on Λ. This is a serious
problem: if a prediction for something we want to measure depends on
Λ then we have an arbitrary constant which will need to be given some
value. One strategy to avoid this is to add terms to the Lagrangian that,
when we do the perturbation expansion up to some order (second order
say), will remove the dependence of amplitudes on Λ. Doing this will
clean up the theory up to second order. Although we should still expect
the higher order terms to diverge (i.e. blow up as Λ →∞), we’ll at least
have healed the theory to the extent that we can obtain a prediction of
physical behaviour that is valid up to second order.
32.2
The solution is counterterms
The terms that are added to the Lagrangian to remove the dependence
on Λ, and hence the divergences, are called counterterms. We saw
that for φ4 theory at second order the divergent part of the amplitude
looked like 6iaλ2 ln Λ; we therefore change the Lagrangian by adding a
counterterm −(6aλ2 ln Λ)φ4/4! thus:
L →L −
6aλ2 ln Λ
4!

φ4.
(32.11)
We now have a Lagrangian
L = 1
2(∂µφ)2 −m2
2 φ2 −λ
4!φ4 + C(2)
4! φ4,
(32.12)
where the counterterm C(2) = −6aλ2 ln Λ and we write the superscript
(2) to remind ourselves that we’re only getting rid of divergences at
second order.
Now we start again with our new Lagrangian. We canonically quan-
tize, use the Dyson expansion and ﬁgure out the Feynman rules and then
sum the new diagrams. With the counterterm, the Dyson expansion of
the ˆS-operator is now just
ˆS = Te−i
R
d4x 1
4!(λ ˆφ4−C(2) ˆφ4).
(32.13)
Every order of the expansion therefore also includes a contribution from
the counterterm, whose Feynman diagram is shown in Fig. 32.2. Since

288
Renormalization: the problem and its solution
the counterterm is proportional to φ4 it has the same behaviour as the
λ
4!φ4 term. It therefore represents a vertex which, instead of carrying a
factor −iλ carries a factor iC(2).
Fig. 32.2 The C-counterterm.
We calculate amplitudes to second order in λ and (we only need to)
include one diagram involving a counterterm and we obtain
iM(2) = −iλ + iaλ2 
3 ln Λ2 −ln s −ln t −ln u

+ iC(2),
(32.14)
and substituting C(2) = −6aλ2 ln Λ yields
iM(2) = −iλ −iaλ2 (ln s + ln t + ln u) .
(32.15)
This amplitude depends on the momenta of the incoming and outgoing
particles (which is no problem) but crucially it doesn’t depend on Λ.
We’ve done what we set out to do! Predictions for the theory will now
not depend on an arbitrary cut-oﬀΛ, and hence won’t yield inﬁnity
when we send Λ →∞.
32.3
How to tame an integral
We now have a plan, which is to start calculating using the Lagrangian
and then add a counterterm when we see a divergence. We then work
out the coeﬃcient C(n) in front of the counterterm and all is set. Quite
apart from the as yet unexamined physical consequences of this extra
term, we might worry that this process will turn out to be ultimately
futile. What if you calculate to second order as above, but at third order
not only do we need a counterterm C(3)φ4 but also a new counterterm
D(3)φ6 to swallow up some extra divergent term that emerges? What if
it gets even worse and, at 27th order in the expansion, we need to add
513 new types of counterterm to cancel all of the divergences and then,
at 28th order, we ﬁnd we need 752 new counterterms? A theory might
never stop absorbing new types of counterterm!
Amazingly, it often turns out that we only need to add a small number
of types of counterterm (three, for example, in QED) and that these
cancel divergences to all orders of perturbation theory! We still need to
know the right constants (like C(n)) for each order of perturbation theory
in order to cancel the divergences, but these will turn out to be numbers
that we can calculate. Theories that have the property that only a ﬁnite
number of counterterms are needed to cancel all divergences are said to
be renormalizable theories. This is the miracle of renormalization.
p
p
k
q
p −k −q
Fig. 32.3 The Saturn diagram.
Example 32.2
To get a feel for this we’ll look at the Saturn diagram, shown in Fig. 32.3. Written
out in full the amplitude is
iM = (−iλ)2
6
Z Λ
0
d4q
(2π)4
d4k
(2π)4
i
q2 −m2 + iǫ
i
k2 −m2 + iǫ
i
(p −q −k)2 −m2 + iǫ .
(32.16)

32.3
How to tame an integral
289
These integrals are getting worse! If we’re going to systematically renormalize any
theory we’ll need a system. The system involves writing the integral as a polynomial.
Usefully there’s a key fact that will help us: Feynman diagrams can be expanded as a
Taylor series in the external momentum. The polynomial representing the diagram
will have divergent coeﬃcients, reﬂecting the divergence of the integral. We’ll pick
our counterterms to cancel the divergences in the coeﬃcients. Happily, this results
in counterterms of the right type to cancel the divergences in a particular integral.
The integral in eqn 32.16 may be written in a series about p = 0 as
I = α + βp2 + γp4 + . . . ,
(32.17)
where we don’t have any odd terms because of the symmetry φ ≡−φ in the original
Lagrangian.
Here’s how to ﬁnd the counterterms:
• The Saturn diagram integral is quadratically divergent. It has eight powers of
momentum on top and six on the bottom. If we set the external momentum
p equal to zero in our expansion then I = α and we conclude that α di-
verges quadratically. This tells us that we need one counterterm that diverges
quadratically.
• If we diﬀerentiate the Saturn integral twice with respect to p then it becomes
logarithmically divergent, since we have reduced the number of momenta on
top by two. If we diﬀerentiate the series expansion I twice and set p = 0 we
get I′′ = 2β and we conclude that β diverges logarithmically. We therefore
need another counterterm which is logarithmically dependent on the cut-oﬀ
which will have a Feynman rule that it appears multiplied by p2.
• Diﬀerentiating the integral two more times makes it convergent so we don’t
need any more counterterms.
We need two counterterms to cancel the divergences in the Saturn integral I: one
with coeﬃcient A quadratically dependent on Λ and one with coeﬃcient B which is
logarithmically dependent. How do we make sure that the B counterterm will give
a Feynman amplitude that is multiplied by p2 and that A will not be multiplied by
anything? The answer is to introduce counterterms into the Lagrangian that look
like
Lct = A
2 φ2 + B
2 (∂µφ)2,
(32.18)
remembering
that
in
momentum
space
[∂µφ(x)]2
→
˜φ(−p)p2 ˜φ(p)
we
have
B(∂µφ)2 →p2B and A →A. Adding the counterterms gives the full Lagrangian for
renormalized φ4 theory:
L = 1
2(∂µφ)2 −m2
2 φ2 −λ
4! φ4 + A
2 φ2 + B
2 (∂µφ)2 + C
4! φ4.
(32.19)
Notice that all of the counterterms have the same form as the terms in the original
equation; they just have diﬀerent coeﬃcients.
These counterterms all have Feynman diagrams and rules attached to them. We
saw before that the C(n)-counterterm diagram corresponds to a rule iC(n). The B(n)
counterterm makes a contribution i(p2B(n)), where p is the momentum along the line
and the A counterterm contributes iA(n). The A and B terms are usually represented
by the single diagram shown in Fig. 32.4, which has a rule i(B(n)p2 + A(n)).
Fig. 32.4 The A and B counterterm
diagram
To summarize, we have the Feynman rules for renormalized φ4 theory
given in the box. These eliminate all inﬁnities.

290
Renormalization: the problem and its solution
Feynman rules for renormalized φ4 theory
• A factor
i
p2−m2+iǫ for each propagator.
• A factor −iλ for each interaction.
• Add suﬃcient counterterm diagrams to cancel all inﬁnities.
• A factor i
 B(n)p2 + A(n)
for each counterterm propagator,
where n is the order of the diagram.
• A factor iC(n) for each interaction counterterm, where n is the
order of the diagram.
• All other rules regarding integrating, symmetry factors and over-
all energy momentum conserving delta functions are identical as
for the case of unrenormalized perturbation theory.
32.4
What counterterms mean
We now turn to the question of the physics behind the counterterms.
We’ve added terms to the Lagrangian which surely alters the physics
we’re studying. What we are going to ﬁnd is that the counterterms shift
the parameters from ﬁctional ones to the real-life ones, simultaneously
removing inﬁnities and forcing our theory to describe real life. To recap:
we started with an interacting theory,
L
=
1
2(∂µφ)2 −m2
2 φ2 −λ
4!φ4,
(32.20)
with the mass and coupling constant m and λ respectively. We’ll call
eqn 32.20 the unrenormalized Lagrangian. The excitations made by the
ﬁeld operators when the coupling λ = 0 have mass m. However, when
we turn on the coupling this Lagrangian gives rise to revolting inﬁnities
unless we cut oﬀthe integrals at some momentum Λ.
To ﬁx the divergence encountered as Λ →∞we ﬁnd we need to renor-
malize the theory. As described in the previous chapter, renormalization
involves accepting that we’re dealing with dressed particles, which have
a mass mP and coupling λP.
These particles only involve a fraction
Z
1
2 of the ﬁelds. This realization doesn’t remove the inﬁnities, however.
To do that we need to include counterterms in our Lagrangian. The
renormalized Lagrangian is
L′
=
1
2(∂µφr)2 −m2
P
2 φ2
r −λP
4! φ4
r
+B
2 (∂µφr)2 + A
2 φ2
r + C
4!φ4
r,
(32.21)
where we have rescaled the ﬁelds using2 φ =
√
Zφr(x) and called the
2Note that φr(x) are known as renor-
malized ﬁelds. We assume Z is inde-
pendent of three-momentum here.
mass mP and coupling constant λP respectively. This theory won’t give
us nonsensical, inﬁnite results for Λ →∞.

32.4
What counterterms mean
291
Example 32.3
To see the consequence of renormalization, we now collect the coeﬃcients of the
renormalized Lagrangian
L
=
1 + B
2
(∂µφr)2 −(m2
P −A)
2
φ2
r −(λP −C)
4!
φ4
r ,
(32.22)
which suggests that the counterterms represent shifts in the parameters in the La-
grangian. Writing −A = δm2, −C = δλ and B = δZ we have
L = 1 + δZ
2Z
(∂µφ)2 −(m2
P + δm2)
2Z
φ2 −(λP + δλ)
4!Z2
φ4,
(32.23)
where we’ve restored the original, unrenormalized ﬁelds. From this, we read oﬀthat
we can relate our original and renormalized Lagrangians through
φ =
√
Zφr,
Z = 1 + δZ,
m2 = (m2
P+δm2)
Z
,
λ = (λP+δλ)
Z2
.
(32.24)
This tells us that renormalization is simply an exercise in shifting parameters.
To truly see what’s going on it may make more sense now to replay the argument
backwards. We start with the unrenormalized Lagrangian and shift from parameters
m and λ to parameters mP and λP. We start by renormalizing the ﬁelds to obtain
L
=
1
2(∂µφ)2 −m2
2 φ2 −λ
4! φ4
=
Z
2 (∂µφr)2 −Zm2
2
φ2
r −Z2λ
4! φ4
r .
(32.25)
We then use the shifts
Z = 1 + δZ,
Zm2 = m2
P + δm2,
Z2λ = λP + δλ,
(32.26)
giving us
L = 1
2 (∂µφr)2 −m2
P
2 φ2
r −λP
4! φ4
r + δZ
2 (∂µφr)2 −δ2
m
2 φ2
r −δλ
4! φ4
r ,
(32.27)
allowing us to identify the counterterms A, B and C as the shifts in the parameters
−δm2, δZ and −δλ.
We started the whole venture of quantum ﬁeld theory by writing down
a simple Lagrangian with parameters we thought would tell us about
the masses and couplings of real particles in Nature. It turns out we
were wrong. We were doing the wrong perturbation theory expanding a
series in terms of the wrong constant mass m and wrong coupling λ. We
therefore had asked a nonsensical question and got some inﬁnite (and
therefore nonsensical) answers. Actually we should have been expanding
our series in terms of a mass mP and coupling λP. The price we pay
for making this shift from wrong to right variables is counterterms. In a
sense, we’re not really ‘adding counterterms’ at all; we’re really taking a
bare Lagrangian and making a shift of variables from wrong ones to the
right ones and the shifts required are the counterterms. Renormalization
is not, therefore, an exercise in hiding inﬁnities, it’s an exercise in making
a theory describe real life. For φ4 theory, the counterterms are divergent,
meaning that these shifts may be inﬁnite! This implies that m and λ,
the bare mass and coupling are inﬁnite quantities which are shifted by
inﬁnite amounts upon dressing themselves by interactions.3
3This explains why it’s often claimed
that the bare charge of the electron is
inﬁnite.
The spontaneous appearance
of electron-positron pairs screens the
charge leading to the ﬁnite charge we
encounter in Nature.
There’s one last, but very important, point to make. What number
do we take for mP and λP? What are the right masses and coupling
we should expand around?
The answer is that they’re the ones Na-
ture’s given us! For this reason, mP and λP are known as the ‘physical’
parameters, in contrast to the ‘bare’ parameters m and λ.

292
Renormalization: the problem and its solution
32.5
Making renormalization even simpler
We’d like a method to avoid having to analyse each diagram separately
as we did for the Saturn diagram above. Is there a method which can,
once and for all, tell us how divergent a diagram or class of diagrams is?
There is and it’s based on simple dimensional analysis.
Deﬁne the superﬁcial degree of divergence4 of an integral arising from
4The clumsy name, ‘superﬁcial’ degree
of divergence, arises because the actual
degree of divergence can be complicated
in gauge theories and some pathological
cases, see Peskin and Schroeder, page
316.
a Feynman diagram in φ4 theory to be
D =

Powers of momentum
in numerator

−

Powers of momentum
in denominator

.
(32.28)
If D > 0 the integrals diverge, if D = 0 they logarithmically diverge and
if D < 0 they don’t diverge. For a diagram with BE external lines, the
degree of divergence in φ4 is given by D = 4 −BE.
Example 32.4
We can prove the above theorem for predicting whether diagrams diverge. In addition
to the previously deﬁned symbols
• V is the number of vertices,
• L is the number of loops,
• BI is the number of internal lines.
Each loop brings with it an integral with four powers of momentum. Each internal
line brings a propagator which brings −2 powers of momentum. Therefore
D = 4L −2BI.
(32.29)
To get the number of loops: L is the number of momenta we integrate over and
each internal line gives a momentum. The number of loops is less than BI though.
Momentum-conserving delta functions eat up integrals and there are V of them (one
for each vertex, since we must conserve momentum at every vertex). However, one
of the delta functions conserves momentum for the entire diagram, so doesn’t eat an
integral. The number of loops L is therefore equal to the number of internal lines
BI, minus (V −1) the number of vertices, with one removed for overall momentum
conservation:
L = BI −(V −1).
(32.30)
Each vertex has four lines coming out of it. Each external line comes out of one
vertex and each internal line connects two vertices. Therefore
4V = BE + 2BI.
(32.31)
Inserting eqn 32.30 and eqn 32.31 into eqn 32.29 we get D = 4−BE and the theorem
is proved.
The great signiﬁcance of this theorem is that it shows that the three
counterterms we have identiﬁed are the only ones that will ever appear.
The fact that D = 4−BE means that the only diagrams that diverge (i.e.
with D ≥0) have ≤4 external legs. There are no allowable diagrams
with one or three legs and we’ve considered the divergences for the two
and four leg diagrams. We’ll never need any more.

32.6
Which theories are renormalizable?
293
32.6
Which theories are renormalizable?
Renormalizable theories are those in which a ﬁnite number of countert-
erms cancel all divergences. Unfortunately, there are theories for which
this property does not hold true. An example is Fermi’s theory of weak
interactions between fermions, whose Lagrangian is given by
L = ¯ψ(ˆ✁p −m)ψ + G( ¯ψψ)2,
(32.32)
where ψ is the fermion ﬁeld.5 The superﬁcial degree of divergence of this
5The meaning of the cross through the
momentum operator p and the bar over
ψ will be explained in Chapter 36.
theory is given by
D = 4 −3
2FE + 2V,
(32.33)
where FE is the number of external Fermi lines in the diagram. This has
the unfortunate feature that D depends on V , the number of interaction
vertices.
If we consider fermion–fermion scattering, for which FE =
4 then for V > 1 we have divergent diagrams, which become more
divergent as V gets larger. We would need new counterterms at every
order of perturbation theory.
In fact, whether a theory is renormalizable or not may be read oﬀ
simply from the dimensions of the interaction coupling constant. The
rules are
• A super-renormalizable theory has only a ﬁnite number of superﬁ-
cially divergent diagrams. Its coupling constant has positive mass
dimension.
• A renormalizable theory has a ﬁnite number of superﬁcially diver-
gent diagrams, however, divergences occur at all orders of pertur-
bation theory. Its coupling constant is dimensionless.
• A non-renormalizable theory has diagrams that are all divergent
at a suﬃciently high order of perturbation theory. These theories
have a negative mass dimension.
In our units the action S =
R
d4x L must be dimensionless since eiS
appears in the path integral. We therefore need L to have dimensions
[Length]−4. Also note that, using these units, mass and energy have
dimensions [Length]−1. Our φ4 theory has a Lagrangian L = 1
2(∂µφ)2 −
m2
2 φ2−λ
4!φ4 from which we conclude that [φ] = [Mass] = [Length]−1 and
λ is dimensionless. The φ4 theory is therefore renormalizable. On the
other hand, looking at the mass term in Fermi’s theory we can conclude
that [ψ] = [Mass]
3
2 and that, therefore, [G] = [Mass]−2. The theory is
non-renormalizable.
The reason for the rule may be seen if we ask about the momentum
dependence of scattering in Fermi’s theory for small external momenta.
The lowest order contribution will vary as M1 ∼G. The next order
is G2 and so that the units agree with M1 we require M2 ∼G2p2
or ∼G2Λ2, when we integrate. This is divergent when we send Λ →
∞. Things only get worse as we go to higher orders. This increase in
powers of momentum will occur for any theory with a coupling which has

294
Renormalization: the problem and its solution
negative mass dimension. The disaster clearly doesn’t occur for theories
with dimensionless couplings, where no extra momentum dependence is
required. For super-renormalizable theories, we need momentum factors
raised to negative powers, improving convergence at large momenta.
Finally we note that this argument also allows us to see that non-
renormalizable interactions in Fermi’s theory only cause trouble when
Λ is larger than ≈G−1. That is, if we’re interested in physics at much
lower energies than G−1 then we never explore momenta between G−1
and inﬁnity and so the presence of the non-renormalizable term in the
Lagrangian wouldn’t cause us trouble. It may be that all of the theories
of Nature that, at one time, we believed to be renormalizable (including
quantum electrodynamics and the electroweak theory) actually contain
non-renormalizable terms in their true Lagrangians. It’s just that all of
our experience is limited to energies where we don’t notice them since
their coupling constants Gi are so small that our experiments cannot
reach the energies of G−1
i . In this way of looking at the world, our theo-
ries of Nature are low-energy, eﬀective theories, which will eventually
break down at high enough energies. The true theory of Nature may
not, therefore, be a quantum ﬁeld theory at all...
Chapter summary
• Quantum ﬁeld theory contains divergences and the solution to the
problem is renormalization.
Counterterms can be added to the
Lagrangian to remove the divergences and these counterterms have
the eﬀect of shifting parameters in the original Lagrangian.
Exercises
(32.1) Consider a theory described by the Lagrangian
L = 1
2(∂µφ)2 −(m2/2)φ2 −(g/3!)φ3.
(a) Write a renormalized Lagrangian and determine
the relationships between the bare and renormal-
ized parameters.
(b) Working in (5 + 1)-dimensional spacetime, de-
termine the superﬁcial degree of divergence of the
two diagrams shown in Fig. 32.5.
(c) Suggest Feynman rules for the counterterms in
the renormalized theory.
(a)
(b)
Fig. 32.5 Two divergent diagrams occurring in φ3 the-
ory.

33
Renormalization in action:
propagators and Feynman
diagrams
33.1 How interactions change the
propagator
in
perturbation
theory
295
33.2 The
role
of
counterterms:
renormalization
conditions
297
33.3 The vertex function
298
Chapter summary
300
Exercises
300
Now we are going to see yet another way in which the whole
world is built out of recursion ... We are going to see that
particles are - in a certain sense which can only be deﬁned
rigorously in relativistic quantum mechanics - nested inside
each other in a way which can be described recursively, per-
haps even by a sort of ‘grammar’.
Douglas Hofstadter (1945– ), G¨odel, Escher, Bach
Some of the most useful tools in quantum ﬁeld theory are propagators
and Feynman diagrams. We’ve seen in the previous two chapters how
renormalization aﬀects single particles and Lagrangians. Now we’ll ex-
amine how renormalization aﬀects Feynman diagrams and propagators.
This approach provides a particularly vivid picture of how a particle
dresses itself in interactions. The main features of the propagator ap-
proach to renormalization is that the dressing-up process is implemented
with two new Green’s functions.1 The ﬁrst results from the single particle
1The beauty of describing renormaliza-
tion in terms of Green’s functions is
that each of these new Green’s func-
tions may be written as a sum of Feyn-
man diagrams.
interacting with the vacuum as it propagates through spacetime. These
interactions gives rise to a Green’s function we call the self-energy.
The self-energy describes the changes to the particle’s mass caused by
interactions. The second new Green’s function comes from the virtual
ﬂuctuations screening the interactions between particles. The screening
is described by the vertex function
˜Γ. This screening changes the
The notation is unfortunate, but please
try not to confuse the vertex function
˜Γ and the quasiparticle decay rate Γp.
coupling constant of the theory.
33.1
How interactions change the
propagator in perturbation theory
We will discuss the usual example of a scalar ﬁeld theory with φ4 inter-
actions, described by a Lagrangian
L = 1
2(∂µφ)2 −m2
2 φ2 −λ
4!φ4,
(33.1)
where we’ve written the theory in terms of bare ﬁelds and parameters.
When there are no interactions present, the amplitude for a particle
with momentum p to propagate between two points is given by ˜G0(p) =

296
Renormalization in action: propagators and Feynman diagrams
i/(p2−m2+iǫ). We saw previously (without using perturbation theory at
all) that when you turn on interactions particles become dressed particles
described by a propagator
˜G(p) =
iZp
p2 −m2
P + iΓp
+
 Multiparticle
parts

,
(33.2)
where Z1/2
p
= ⟨pλ|ˆφ(0)|Ω⟩tells you how much particle we have in the
momentum state |pλ⟩, Γp is the particle decay rate and mP is the phys-
ical particle’s exact mass. Notice that the pole is found at p2 = m2
P
and the residue of the pole is iZp. We now take a propagator for non-
interacting particles with bare mass m and examine how it changes when
we add an interaction to our theory as a perturbation. Since we know
that the exact answer is given by eqn 33.2, we know how to extract the
exact mass mP of the particle from any propagator we calculate with
perturbation theory: we simply look for the pole. We also know how to
ﬁnd the quasiparticle weight Zp: we ﬁnd the residue at the pole. We
therefore have a very important rule:
The physical mass of a particle in an interacting theory is given by
the position of the pole in the propagator. The quasiparticle weight
is found from the residue at the pole.
In perturbation theory the propagator can be found by adding up Feyn-
man diagrams. That is
˜G(p) =
X 
All connected Feynman diagrams
with two external legs

.
(33.3)
One helpful way to think of the propagator is to picture it as being made
up of the two external legs and everything we can slot in between those
two legs. What we can slot in is called the self-energy, and we describe
that using what is called a 1-part irreducible diagram (or 1PI di-
agram for short), which is deﬁned as a connected diagram which can’t
be disconnected by cutting one internal propagator line. Some examples
of 1PI diagrams in φ4 theory are shown in Fig. 33.1(a) and some exam-
ples of diagrams that aren’t 1PI are shown in Fig. 33.1(b). These latter
diagrams contain cutlines, and so fall apart into disconnected pieces if
you attack one of the cutlines with a pair of scissors. In contrast, the
connectedness of an 1PI diagram is immune to a single scissor attack on
one of their internal lines. They essentially represent the smallest non-
trivial Feynman diagram and a basic building block of more complex
diagrams. They are, if you like, the guts of the self-energy.
(a)
(b)
Fig. 33.1 (a) Some 1PI diagrams from
φ4 theory. They can’t be turned into
two meaningful diagrams by cutting
one propagator line. (b) Diagrams that
aren’t 1PI. All can be turned into legit-
imate 1PI diagrams by cutting at some
point along the horizontal line.
The sum of 1PI diagrams with two external legs forms a Green’s func-
tion known as the 1PI self-energy ˜Σ(p), given by2
2Note that the 1PI self-energy does not
include an overall energy-momentum
conserving δ-function. We also ampu-
tate the external lines, that is, we don’t
include propagators for these lines.
−i˜Σ(p) =
X 
All amputated 1PI diagrams
with two external lines

.
(33.4)
Some contributions to −i˜Σ(p) in φ4 theory are shown in Fig. 33.2. In
=
+
+
+
+
+
+ · · ·
1PI
Fig. 33.2
Some contributions to the
1PI self-energy −iΣ(p) in φ4 theory.
the following example, we will explore how the propagator ˜G(p) can be
expressed in terms of the 1PI self-energy ˜Σ(p).

33.2
The role of counterterms: renormalization conditions
297
Example 33.1
The deﬁnition of ˜Σ leads to one of the neatest tricks in perturbation theory. We can
write the interacting propagator as a series of encounters with an interaction repre-
sented by ˜Σ. This is the story of how the propagator dresses itself: it’s the amplitude
for propagating with no interactions, added to the amplitude for propagation being
interrupted by one interaction with ˜Σ, added to the amplitude for two interactions
with ˜Σ, added to... The series, shown in Fig. 33.3, is written as follows:
˜G(p)
=
i
p2 −m2 +
i
p2 −m2
h
−i˜Σ(p)
i
i
p2 −m2
+
i
p2 −m2
h
−i˜Σ(p)
i
i
p2 −m2
h
−i˜Σ(p)
i
i
p2 −m2 + . . .
(33.5)
We now treat this as a geometric series and sum:
˜G(p)
=
i
p2 −m2

1 +
h
−i˜Σ(p)
i
i
p2 −m2
+
h
−i˜Σ(p)
i
i
p2 −m2
h
−i˜Σ(p)
i
i
p2 −m2 + . . .
ﬀ
=
i
p2 −m2
8
<
:
1
1 −
˜Σ(p)
p2−m2
9
=
;
=
i
p2 −m2 −˜Σ(p) + iǫ
,
(33.6)
where, in the last line, we’ve reinserted the iǫ from the free propagator. The sum can,
more amusingly, be done in terms of diagrams as shown in Fig. 33.4. Equation 33.6 is
=
+
+
+ · · ·
1PI
1PI
1PI
Fig. 33.3
Diagrammatic version of
eqn 33.5.
=
+
+
+ · · ·
=
(
)
1 −
=
(
)−1 −
1
1PI
1PI
1PI
1PI
1PI
=
i
p2 −m2 + iǫ −˜Σ(p)
Fig. 33.4
Diagrammatic version of
eqn 33.6
another instance of Dyson’s equation, which we met in Chapter 16. The form of the
equation makes it look a lot like the free propagator, albeit with an extra self-energy
term in the denominator.
To ﬁnd the mass-energy mP of the physical particles we just follow our renormal-
ization rule: we look for the position of the pole. It is found using the equation
p2 −m2 −Re
“
˜Σ(p)
”
= 0,
(33.7)
when p2 = m2
P. That is to say that the physical mass of the particles is given by
m2
P = m2 + Re
“
˜Σ(p2 = m2
P)
”
,
(33.8)
which tells us that the real part of the self-energy tells us the shift in energy caused
by interactions. The latter equation is known as a renormalization condition.3
3It allows us, at the level of Feynman
diagrams and perturbation theory, to
renormalize a theory. By the same to-
ken, the decay rate of the physical par-
ticle is found from Im
“
˜Σ(p2 = m2
P)
”
.
33.2
The role of counterterms:
renormalization conditions
But what of counterterms in this way of looking at things? Thus far
we’ve summed all of the 1PI self-energy diagrams to make an object
˜Σ(p). In the absence of counterterms, the role of the self-energy ˜Σ(p) is
to shift the mass from m to mP. However, we have ignored the fact that
many of the diagrams that contribute to ˜Σ(p) will be divergent, making
the shift in mass seem dangerously inﬁnite! One way to interpret this is
to say that m must be inﬁnite and requires an inﬁnite shift to bring it
down to the physical value mP. However all this talk of inﬁnity should

298
Renormalization in action: propagators and Feynman diagrams
make us nervous and we would be better to remove divergences as we
did in the previous chapter.
To remove the divergences in ˜Σ(p) we should really sum all of the self-
energy diagrams along with counterterms in order to cancel inﬁnities.
Recall that the eﬀect of including counterterms was to shift the mass
of the theory from the unphysical ﬁctional value of m to the measured
and reliable value mP. Using the 1PI self-energy ˜Σ(p), we won’t need to
shift the mass at all! That is, when we include counterterms we require
the renormalization condition
Re

˜Σ(p2 = m2
P)

= 0.
(33.9)
We see that there are two ways of doing business. We may use the
renormalization condition eqn 33.8, which neglects counterterms with
the result that the shifts from ﬁctional to real masses are potentially
inﬁnite. On the other hand we can use the renormalization condition in
eqn 33.9 which allows us to start with the mass set to the correct value
and we ensure that it doesn’t change as we do our perturbation theory.4
4In the theory of the electron gas,
where we do not encounter divergences
to the extent that we do in some other
theories, the ﬁrst option is the logical
one. In quantum electrodynamics the
second option is the one to choose.
33.3
The vertex function
−i˜Γ
Fig. 33.5
In φ4 theory the vertex
function is contained in the Greens
function
shown
here,
corresponding
to ⟨0|T ˆφ4 ˆS|0⟩/⟨0| ˆS|0⟩, or equivalently
˜G(p1) ˜G(p2)
h
−i˜Γ(p1, p2, p3)
i
˜G(p3) ˜G(p4).
Diagrammatically, the vertex part −i˜Γ,
the central blob in the diagram above,
may be extracted by amputating the
external propagator legs.
The next thing to do is to work out how to renormalize the coupling
constant. Physically, this is changed because vacuum ﬂuctuations screen
the interactions between two particles.
The interaction vertex in φ4
theory has the Feynman rule
(Vertex) = (2π)4δ(4)(p4 + p3 −p2 −p1)(−iλ),
(33.10)
that is, we get a factor of −iλ for every vertex and the δ-function ensures
that momentum is conserved. The vertex is plugged in between four
external lines to tell us about two particle scattering.
We deﬁne the vertex function ˜Γ for φ4 theory by5
5We could also deﬁne a 1PI vertex func-
tion, which is useful for many applica-
tions, but we won’t need it here.
−i˜Γ =
X  All connected four-point diagrams
with external legs amputated

.
(33.11)
This is shown diagrammatically in Fig. 33.5. (Note that −i˜Γ does not
include an overall energy-momentum conserving δ-function.) Like the
φ4 interaction vertex, the vertex function may be plugged in between
four propagators. It is designed to tell us how interactions (involving
virtual particles) aﬀect how real particles interact with each other. Some
examples of contributions to −i˜Γ are shown in Fig. 33.6.
=
+
+
+
+ · · ·
Fig. 33.6
Some contributions to the
vertex function −i˜Γ in φ4 theory. The
vertex function is represented as a
shaded blob with four external legs.
Example 33.2
For two-particle scattering ˜Γ can be obtained from the amplitude for the two-particle
scattering process. To ﬁrst order in the coupling, the four-point vertex function is
given by a single interaction vertex: −i˜Γ = −iλ. To second order in the coupling
we eliminated divergences using a counterterm (see Chapter 32) and saw that the
two-particle scattering amplitude was given by
−i˜Γ(p1, p2, p3) = −iλ −iaλ2 (ln s + ln t + ln u) ,
(33.12)

33.3
The vertex function
299
that is, it depends on the momenta of the incoming and outgoing particles
[parametrized by s = (p1 + p2)2, t = (p1 −p3)2 and u = (p2 −p3)2].
The
last
example
demonstrates
that
the
exact
vertex
function
˜Γ(p1, p2, p3) is a function of the incoming and outgoing momenta. (Of
course momentum conservation means we only need give three of the
four momenta in the problem.)
In order to deﬁne a renormalization
condition for ˜Γ we need to choose the values of these momenta at which
to ﬁx ˜Γ to some value. However, unlike mass which is unambiguously
deﬁned, there is no unique deﬁnition of the coupling constant. We are
therefore free to choose a renormalization condition at our convenience.
We ﬁnally write the deﬁnition of the physical coupling constant:
−iλP = −i˜Γ(p1, p2, p3) =
X


Amputated, connected diagrams with
four external legs
and momenta p1, p2 and p3

,
(33.13)
where the values of p1, p2 and p3 that we choose are called the renor-
malization point. The sign is chosen so that to ﬁrst order λP = λ.
In Chapter 41 we will use the analogous
Green’s functions ˜Σ and ˜Γ to assess how
electromagnetic interactions change the
properties of photons and electrons in
quantum electrodynamics.
In Chap-
ter 43 we will examine the renormaliza-
tion of the theory of metals in terms of
the self-energy of electrons and the pho-
tons that mediate the Coulomb force
that acts between them.
Example 33.3
We can use the deﬁnition of λP to calculate the amplitude for two-particle scattering
up to second order in unrenormalized perturbation theory (i.e. where we haven’t used
counterterms to remove the cut-oﬀ). If the physical coupling constant λP is deﬁned
to be that measured at a renormalization point s0, t0, u0 then, to second order, we
have that
−iλP = −i˜Γ(2)(s0, t0, u0) = −iλ + iaλ2 `
3 ln Λ2 −ln s0 −ln t0 −ln u0
´
.
(33.14)
The two-particle scattering amplitude is given, for particles with momenta s, t and
u, by
iM = −iλ + iaλ2 `
3 ln Λ2 −ln s −ln t −ln u
´
.
(33.15)
At the level of approximation to which we’re working we may use eqn 33.14 to write
−iλ = −iλP −iaλ2
P
`
3 ln Λ2 −ln s0 −ln t0 −ln u0
´
+ O(λ3),
(33.16)
which allows us to eliminate λ (and Λ) from eqn 33.15 to give the answer
iM = −iλP −iaλ2
P
»
ln
„ s
s0
«
+ ln
„ t
t0
«
+ ln
„ u
u0
«–
+ O(λ3).
(33.17)
We see that we obtain the amplitude in terms of the physical coupling constant λP,
where momenta are measured relative to the renormalization point. This argument
may be repeated using renormalized perturbation theory (i.e. with the use of coun-
terterms, which eliminate all ln Λ terms) to obtain an identical answer, but without
the troubling business of carrying around the (potentially inﬁnite) Λ-dependent term.

300
Renormalization in action: propagators and Feynman diagrams
Chapter summary
• The self-energy ˜Σ can be related to the sum of all 1-part irreducible
(1PI) diagrams with two external legs.
• In φ4 theory, the vertex function ˜Γ can be related to the sum of all
four-point diagrams with external legs amputated.
Exercises
(33.1) To ﬁnd the quasiparticle weight we can expand the
self-energy in a Taylor series. We’ll do this about
the interesting point p2 = m2
P.
˜Σ(p2) ≈˜Σ(m2
P) + (p2 −m2
P) d˜Σ(p2)
dp2
˛˛˛˛
p2=m2
P
+ . . .
(33.18)
(a) Using this expansion show that
˜G(p) ≈
i
(p2 −m2
P)
»
1 −d˜Σ(p2)
dp2
˛˛˛
p2=m2
P
–. (33.19)
(b) Use this to show, to the order to which we’re
working, that
Z ≈1 + d˜Σ(p2)
dp2
˛˛˛˛
p2=m2
P
.
(33.20)
This gives us a scheme for calculating Z, the quasi-
particle weight, in terms of the quasiparticle self-
energy.
(33.2) (a) Draw diagrams showing the contributions to the
self-energy −i˜Σ(p) of psions for ψ†ψφ theory up to
fourth order in the interaction.
(b) Draw the contributions to the vertex function
−i˜Γ in ψ†ψφ theory up to third order in the inter-
action.
Here the vertex function includes all amputated in-
sertions with one psion line, one antipsion line and
one phion line.
(33.3) Repeat the argument in Example 33.3 using renor-
malized perturbation theory (i.e. including the use
of counterterms to remove the momentum cut-oﬀ
Λ).
(33.4) Consider the action derived for a one-dimensional
lattice in (1 + 1)-dimensional spacetime
S =1
2
X
p
Z
dω
(2π)
˜φ−p(−ω)
`
ω2−ω2
0+ω2
0 cos pa
´ ˜φp(ω).
(33.21)
We will treat the ﬁnal term in the bracket as a per-
turbation.
(a) Identify the free propagator and the interaction
vertex.
(b) Write the full propagator as a sum to inﬁnity
involving the free propagator and interaction term.
(c) Show that, on carrying out the sum, the ex-
pected full propagator is recovered.
(33.5) Consider a model of an atom made up of two energy
levels (E2 > E1 > 0) occupied by a single fermion
and described by a Hamiltonian
ˆH = E1ˆc†
1ˆc1 + E2ˆc†
2ˆc2 + V (ˆc†
1ˆc2 + ˆc†
2ˆc1).
(33.22)
(a) Treat the non-diagonal term as a perturbation
and ﬁnd the free propagators ⟨0|T ˆc1(t)ˆc†
1(t′)|0⟩and
⟨0|T ˆc2(t)ˆc†
2(t′)|0⟩for electrons in level |1⟩and |2⟩
respectively.
(b) Now consider the interaction term.
Find the
Feynman rule for the processes that contribute to
the self-energy and invent appropriate Feynman di-
agrams to describe an electron in the system.
Hint: the potential here is time independent, so the
process to consider involves the electron changing
states and immediately changing back.
(c) Find the self-energy of the electron in each state
and show that the interaction causes a shift in the
energy of the particle in level |1⟩by an amount
∆E1 = −
V 2
E2 −E1 .
(33.23)

Exercises
301
(d) Compare this result with (i) the exact solution
and (ii) with second-order perturbation theory.
*(e) Harder In the time-dependent version of the
problem the interaction part is given by
H′ =
Z
d3q
(2π)3 V (ωq)(ˆaqe−iωqt+ˆa†
qeiωqt)(ˆc†
1ˆc2+ˆc†
2ˆc1).
(33.24)
By evaluating the self-energy of an electron in each
of the levels, show that the decay rates of the levels
are given by
2Γ1 = 2πV (ω0)2⟨n(ω0)⟩
Z
d3q
(2π)3 δ(q0 −ω0),
2Γ2 = 2πV (ω0)2[1 + ⟨n(ω0)⟩]
Z
d3q
(2π)3 δ(q0 −ω0),
where ω0 = E2 −E1.

34
The renormalization group
34.1 The problem
302
34.2 Flows
in
parameter
space
304
34.3 The
renormalization
group
method
305
34.4 Application 1:
asymptotic freedom
307
34.5 Application 2:
Anderson localization
308
34.6 Application 3:
the
Kosterlitz–––Thouless
transition
309
Chapter summary
312
Exercises
312
Everything is in motion.
Everything ﬂows.
Everything is vibrating.
William Hazlitt (1778–1830)
A question we keep asking in physics is ‘when is a theory valid’. In the
context of quantum ﬁeld theory an answer is provided by the philosophy
of the renormalization group.1 This is a way of looking at the world
1The renormalization group is a bit of
a misnomer as it is not really a group.
The name arises from the study of
how a system behaves under rescaling
transformations and such transforma-
tions do of course form a group. How-
ever, the ‘blurring’ that occurs when
we rescale and then integrate up to a
cut-oﬀ, thereby removing the ﬁne struc-
ture (and this is the very essence of
the renormalization group procedure)
is not invertible (the ﬁne details are
lost and you can’t put them back).
Thus the transformations consisting of
rescaling and integrating up to a cut-oﬀ
do not form a mathematical group be-
cause the inverse transformation does
not exist.
pioneered by several physicists, but most notably Kenneth Wilson. It
Kenneth
Wilson
(1936–2013)
was
awarded the Nobel Prize in Physics in
1982 in recognition of ‘his theory for
critical phenomena in connection with
phase transitions’, a theory that grew
out of his work on the renormalization
group.
allows us to make sense of why a renormalized quantum ﬁeld theory
describes Nature.
34.1
The problem
Theories are described by Lagrangians. These are the sum of several
terms, each of which is some combination of ﬁelds and their derivatives,
multiplied by a so-called coupling constant. For φ4 theory, we have the
Lagrangian
L = 1
2(∂µφ)2 −m2
2 φ2 −λ
4!φ4,
(34.1)
where m and λ are the coupling constants. Calculations that start with
a Lagrangian like this soon encounter disaster in the form of divergent
(i.e. inﬁnite) integrals.
That is, we take our Lagrangian, canonically
quantize the free part, treat the interacting part as a perturbation to
deﬁne an ˆS-operator using Dyson’s equation and then start working out
amplitudes in the form of S-matrix elements which lead to Feynman
rules. If a Feynman diagram involves a loop, we often get a divergence.
To cure a divergence, we can introduce a ﬁnite cut-oﬀin momentum Λ
in such a way that we integrate up to Λ rather than inﬁnity. In the
previous chapters our strategy was then to renormalize, that is to say
that we remove the mention of Λ from things that we calculate. This
involves expressing amplitudes in terms of physical coupling constants,
which are the ones we measure at some agreed point in momentum space.
Our deﬁnitions of coupling constants depend on this point in momentum
space.
Kenneth Wilson had another strategy: rather than hiding the cut-oﬀ,
we live with it. In Wilson’s view, to properly deﬁne the theory, we need
to admit that we do our integrals up to a maximum momentum Λ, which
we are entirely free to choose. The arbitrary choice of Λ immediately

34.1
The problem
303
raises the question: how do we know what value to choose? One answer
is that we want the length scale Λ−1 to be far smaller than the length2
2Remember that in units where ℏ=
c = 1, mass m, energy E and momen-
tum p have inverse units to length.
scale p−1 of any of the physics in which we’re interested. Consider, for
example, a gas of atoms in a box. If we’re interested in sound waves,
which are oscillations in the number density of the constituents of the
gas involving many millions of atoms, then the scale in which we’re
interested in p−1 is of the order of centimetres and we could take Λ−1
to be a few microns. If we’re interested in the electron cloud of an atom
in the gas, then we’re interested in a length scale p−1 of the order of
the size of an atom and we could take Λ−1 to be the size of an atomic
nucleus. The size of Λ−1 that we choose therefore changes depending on
the scale p−1 of the physics that we’re analysing.
The renormalization group is a machine that tells us how the predic-
tions of the theory change as we alter the scale of interest. It gives us an
equation telling us how a coupling ‘constant’ changes with the length or
momentum scale in which we’re interested. To summarize this chapter:
The goal of renormalization group analysis is to discover how the
coupling constants change with the scale of interest.
Example 34.1
In general, this goal may be achieved by examining how your Lagrangian behaves
when the cut-oﬀΛ is systematically varied. This approach will be outlined later. It is,
however, sometimes possible to leapfrog the systematic method to get an equation for
the coupling constant in terms of the scale of interest. We’ll follow such an approach
here ﬁrst. Recall how we deﬁned the renormalized coupling constant in φ4 theory.
We said that λP was the measured scattering amplitude when the particles had
squared momenta s0, t0 and u0, which is to say, the coupling constant is deﬁned at
a particular point in momentum space. How do we choose this point? A pragmatist
would say that you choose it to be a convenient energy scale for the problem you’re
addressing. If we’re dealing with electrons then we could set s0 = t0 = u0 = µ2 where
µ is close to the electron mass. If we’re dealing with pions, we’d use µ close to the
pion mass and so on. However, this ‘idiot-proof’ scheme could easily be challenged
if an ingenious idiot started thinking about electron physics but set µ equal to the
pion mass.
As we saw in Chapter 33 the physical coupling constant is given to second order
by
−iλP(s0, t0, u0) = −iλ + iaλ2 `
3 ln Λ2 −ln s0 −ln t0 −ln u0
´
.
(34.2)
This allowed us to write the two-particle scattering amplitude to second order:
iM = −iλP(µ) + ia[λP(µ)]2
»
ln
„ µ2
s
«
+ ln
„ µ2
t
«
+ ln
„ µ2
u
«–
,
(34.3)
which is to say that it’s given by the physical coupling constant added to a logarithmic
correction. This correction is small since we chose µ2 to be of order s0, t0 and u0.
If, on the other hand, we idiotically choose a renormalization point µ′2 to be much
larger than s0, t0 and u0, then the logarithmic term could be very large compared to
the ﬁrst term. Then we would have
iM = −iλP(µ′) + ia[λP(µ′)]2
»
ln
„µ′2
s
«
+ ln
„µ′2
t
«
+ ln
„ µ′2
u
«–
,
(34.4)
where the second term is large compared to the ﬁrst. Subtracting, we can ﬁnd an
expression for λP(µ′) in terms of λP(µ):
λP(µ′) = λP(µ) + 6a[λP(µ)]2 ln
„µ′
µ
«
+ O(λ3
P),
(34.5)

304
The renormalization group
which is expressed in diﬀerential form thus:
µ d
dµλP(µ) = 6aλP(µ)2 + O(λ3
P).
(34.6)
So in this case, we have a simple equation that tells us how the coupling λ changes
with the momentum scale µ.
If we are interested in high energies, we can examine how λ behaves as µ, rep-
resenting the scale of interest, gets larger and larger, that is, as µ →∞. We call
this the ultraviolet behaviour. Equally well, we might be interested in the physics
at very large length scale, which would involve asking how λ changed as µ →0.
This is known as the infrared behaviour, since it corresponds to the limit of small
Remember
that
[Mass]−1
≡
[Length]−1.
energies.
34.2
Flows in parameter space
A theory is characterized by {gi}, the set of coupling constants that
measure the strength of the various interactions.
(For φ4 theory we
have g2 = m and g4 = λ.) Each coupling constant is a function of the
cut-oﬀΛ, so we can write the ith coupling constant as gi(Λ). We are
going to examine what happens as we look at diﬀerent length scales,
which is achieved by changing the value of the cut-oﬀ. If we reduce the
cut-oﬀby a factor b, then we want to see how the coupling constants
change and so we will look at the transformation
gi(Λ) →gi(Λ/b).
(34.7)
If we are interested in the limit of large length scales (corresponding
g1
g2
Fig. 34.1 The renormalization group
ﬂow in a conﬁguration space deﬁned
by two coupling constants g1 and g2.
to what is measured in condensed matter physics) then we choose the
direction of positive ﬂow by ﬁxing b > 1.
This is known as coarse
graining.3 A theory can be described as a point in the multidimensional
3Alternatively there is nothing stop-
ping us examining the theory at short
length scales in which case we could set
b < 1.
space formed by all of the {gi}, i.e. by the point (g1, g2, · · · ) in that
space. The rescaling of the cut-oﬀcauses the point to travel through this
multidimensional space along a renormalization group trajectory,
and this is called a renormalization group ﬂow (see Fig. 34.1). We
will sometimes ﬁnd ﬁxed points in this space; these remain invariant
under coarse graining and correspond to scale-invariant Lagrangians.
We will see that such Lagrangians are very important in determining
the physics described by the theory.
Example 34.2
A very simple example of renormalization group ﬂow occurs for a theory when there
is only a single coupling constant g. A graph with only one axis is hard to illustrate
(much like the problem with the sound of one hand clapping) so to illustrate the ﬂow
it is conventional to plot the function
β(g) =
dg
d ln b ,
(34.8)
which is known as a Gell-Mann–Low equation4 or simply a ﬂow equation.5
4This is, of course, a diﬀerent ‘Gell-
Mann–Low equation’ to that described
in Chapter 22. See M. Gell-Mann and
F. Low, Phys. Rev. 84, 350 (1951)
5Sometimes one can write
β(g) = dg
db
instead. Since b divides Λ, the rescaling
is more usefully modelled by looking at
how g changes with ln b.
But, as we
shall see in this chapter, diﬀerent deﬁ-
nitions can, and are, made.
g
β
g
β
g
g∗
β
g
g∗
β
Fig. 34.2
Four examples of ﬂows in
a conﬁguration space deﬁned by a sin-
gle coupling constant g, visualized by
plotting β(g) = dg
db . Arrows show the
direction that the length scale ℓ→∞.
Note that:

34.3
The renormalization group method
305
• If β is always positive then g will be blasted out to inﬁnity, as shown in
Fig. 34.2(a). If β is always negative then g will be sucked backwards to g = 0
as shown in Fig. 34.2(b).
• If at position g = g∗we have β(g∗) = 0 then g is stuck and will remain at g∗
for evermore. We call g∗a ﬁxed point.
• Even if a ﬁxed point exists the system may never get stuck there.
If the
velocity function β looks like Fig. 34.2(c) then if g > g∗the system will end
up at ∞, while g < g∗will be drawn back to g = 0: here g∗is known as a
repulsive ﬁxed point.
• If the function β(g) looks like Fig. 34.2(d) then the system will be attracted
towards g∗regardless of its initial condition. There are no prizes for guessing
that under these circumstances g∗is known as an attractive ﬁxed point.
34.3
The renormalization group method
Now that we know what it’s useful for, we’d like to know how the renor-
malization group method works in more detail. We start with the func-
tional integral, written in Euclidean space
Z(Λ) =
Z
Λ
Dφ e−
R
ddxL[φ],
(34.9)
where Λ is an instruction to integrate over conﬁgurations of φ(x) con-
taining Fourier components up to Λ.
Fig. 34.3 Removing the largest Fourier
components results in a loss of detail.
We will use the renormalization group method to examine the physics
as we increase the length scale of interest. The method involves three
steps. In step I we’re going to remove the largest Fourier components of
momentum. This is achieved by doing that part of the functional integral
that involves the largest Fourier components in momentum space. The
removal of these components is equivalent to losing one’s glasses.
A
certain level of ﬁne detail is lost and we have a simpler looking ﬁeld
(Fig. 34.3). We then compare the theory to what we started with. Since
we now seem to be comparing apples (the original integral) and oranges
(the partly integrated integral) we need to scale up the partly integrated
integral so that it is deﬁned over the same momentum range as the
original.
To do this involves (step II) relabelling the momenta and
then we (step III) scale the size of the ﬁelds so that the Lagrangian
resembles what we had before. The beauty of the procedure is that, in
successful cases, it will not change the form of the Lagrangian, only the
coupling constants. We then imagine repeating the procedure over and
over again (losing successive pairs of spare glasses) and extract how the
couplings change as we partially integrate the functional integral.
Λ/b
Λ
Λ/b
Λ
(a)
(b)
(c)
Fig. 34.4
The renormalization group
process in momentum space. (a) The
ﬁeld φ(x) before the process. (b) The
result of integrating out the largest mo-
mentum states (step I) is to remove the
high spatial frequency components. (c)
The ﬁeld after rescaling (steps II and
III). Note that the region in the dashed
box is identical to the entire trace in
(b).
|p|
0
Λ
b
Λ
φs
φf
Fig. 34.5 Components in momentum
space are divided up into slowly varying
and fast varying, with any component
with |p| > Λ being ignored.
We now demonstrate the procedure, which is illustrated in Fig. 34.4.
First we need to set the scene.
We choose to work in momentum
space, where we divide the ﬁelds ˜φ(p) into those parts that vary slowly
with p [called ˜φs(p)] and those parts [called ˜φf(p)] that vary quickly
(see Fig. 34.5). We deﬁne the fast Fourier components as those in the
momentum shell Λ/b ≤|p| ≤Λ, where b > 1 is a scaling factor, which

306
The renormalization group
will be central to our analysis.
The slow Fourier components of the
ﬁelds are then those with momenta6 0 ≤p ≤Λ/b.
The functional
6We therefore have
˜φ(p) = ˜φf(p)
for Λ/b ≤p ≤Λ,
˜φ(p) = ˜φs(p)
for 0 ≤p ≤Λ/b.
integral becomes
Z(Λ) =
Z
Λ/b
Dφs e−
R
ddxL[φs]
Z Λ
Λ/b
Dφf e−
R
ddxLI[φs,φf].
(34.10)
We’re now ready to start the renormalization group procedure.
Step I: This is the diﬃcult step. We need to integrate over the quickly
varying ﬁelds. This is usually impossible, so we need an approximation
scheme involving perturbation theory. Assuming we can do this, we’ll
obtain an action in which the remaining slow ﬁelds and coupling con-
stants are diﬀerent to what they were before. If we can (somehow) do
the integral over the fast varying part φf we deﬁne the answer to be
Z Λ
Λ/b
Dφf e−
R
ddxLI[φs,φf] = e−
R
ddx δL[φs],
(34.11)
leading to the result for our integral:
Z(Λ) =
Z
Λ/b
Dφs e−
R
ddx (L[φs]+δL[φs]),
(34.12)
which only features the slow ﬁelds.
Step II: We relabel the momenta.
We do this by expressing our
theory in terms of scaled momenta p′ = pb. Since b > 1 this has the
eﬀect of stretching the momentum space out over the range we had
originally, as shown in Fig. 34.6. This is seen by examining the limit of
the integral, which is p′/b = Λ/b or p′ = Λ.
0
Λ
b
Λ
0
Λ
p
p′ = bp
Fig. 34.6 Rescaling the momentum.
Step III: Finally we scale the ﬁelds.
We select the term that we
imagine will be most important in determining the scaling behaviour
and require it to be invariant with the scaling of step II. We use this
to rescale the ﬁeld ˜φ(p) = ˜φ(p′/b) = bd−dφ ˜φ′(p′), where dφ is chosen to
leave the important term invariant.7
7The subject is notorious for its arcane
terminology, and dφ is known as the
anomalous dimension.
Now that the procedure has been carried out once, we repeat it an
inﬁnite number of times. In fact, we only need to carry out the procedure
once and then infer the consequence of repeating it. That is, we do the
approximate integral which tells us how all of the coupling constants
change with scale. We then imagine a continuous ﬂow of the coupling
constant caused by repeating the renormalization group method again
and again.
By removing the fastest varying ﬁelds we are looking at the physics
at lower momentum, or equivalently at longer length scale.
We can
therefore ask how the physics changes as we send the length scale of
interest ℓto inﬁnity. One way to do this is to set b = eℓand search for
ﬂow equations of the form
dgi
dℓ= βi(g1, g2, ..., gN).
(34.13)
This is another form of the Gell-Mann–Low equation.8
8Sending ℓto inﬁnity corresponds to
looking at the infrared behaviour of the
couplings (since large length is the same
as small energy). We could send ℓ→0,
if we want the ultraviolet behaviour.

34.4
Application 1: asymptotic freedom
307
The renormalization group is most famously applied to phase transi-
tions, an example we will defer until the next chapter. Here we focus
on three examples which illustrate other features of the renormalization
group.
34.4
Application 1: asymptotic freedom
Perturbation theory is carried out in terms of series expansions in cou-
pling constants.
The renormalization group teaches us that coupling
constants change depending on the region in energy-momentum space
in which we’re working.
If we work in a region of momentum space
where the coupling constants are small, then perturbation theory will
work well. If, however, we work in a region where the coupling con-
stants are large then we can expect perturbation theory to fail to yield
sensible results. The coupling ‘constant’ in quantum electrodynamics
is the electronic charge. As we’ll see in Chapter 41, to ﬁrst order, the
β function describing the evolution of e is written9 µ dβ
dµ = |e|3/12π2,
9Notice again that slightly diﬀerent
deﬁnitions of the ﬂow are used depend-
ing on context.
This is nothing to
worry about since the idea is always the
same: we want to know how the cou-
pling changes with scale.
where µ is the energy scale of interest. The important thing to note
is that the right-hand side is positive. This means that as we ﬂow to
large energy-momentum transfer (by increasing the mass-energy scale
µ), the coupling constant gets larger and larger. Eventually, when the
momentum involved in the processes we’re calculating implies a very
large mass-energy scale, perturbation theory ceases to be suﬃcient to
capture the physics. Of course, running this process backwards (cor-
responding to examining processes involving small momentum transfer
and large length scale) the coupling gets smaller and smaller and per-
turbation theory captures the physics better and better. This conﬁrms
the expected result that perturbation theory works best for processes
involving small momenta and energy and gets worse as the energy gets
larger. It also conﬁrms the physical picture that electric charge is largest
close to a charged source (that is at small length scales), and dies away
at large distances.10
10This
occurs
because
of
screening
by virtual charged particle–antiparticle
pairs. See Section 41.1 and Fig. 41.6.
A surprising result was discovered for a class of theories, which in-
cludes the famous non-abelian Yang–Mills theory that will be examined
in Chapter 46. In this case, β is negative! Therefore, if we ﬂow to large
momentum transfer, the coupling constants become small and perturba-
tion theory at low orders is a very good approximation. However, now
if we reverse the process, allowing the momentum transfer to become
small, then g gets very large and perturbation theory no longer gives
good results. Theories of this sort are said to have asymptotic free-
dom:11 they are most strongly interacting at low energy scales, but at
11Frank Wilczek now regrets coining
this term, which was ﬁrst suggested
by Sidney Coleman,
and wishes he
had used ‘charge without charge’ in-
stead. The idea is that closer and closer
to a quark the eﬀective colour charge
‘asymptotically’ approaches zero. Hav-
ing zero colour charge means complete
‘freedom’ from its interaction. Playing
the argument in reverse, at large dis-
tances away from the intrinsically weak
colour charge of the quark, the cloud
of virtual particles give rise to anti-
screening and this greatly enhances
the eﬀective colour charge and results
in the strong interaction being strong.
large scales they are weakly interacting (and therefore act like nearly
free theories).12
12One might think of masses connected
by springs as showing a sort of asymp-
totic freedom.
At small length scales
they interact weakly; as the length scale
is increased and the springs become
stretched they interact more strongly.
This is why you cannot isolate quarks
and gluons as free particles, but only
ﬁnd them in bound states inside mesons
(quark, antiquark) or baryons (three
quarks).
The importance of asymptotic freedom is that it allows us to explain
(among other things) the physics of the strong interaction. Under the
normal (i.e. low energy) circumstances of much of the Universe, the
quarks in a proton are very tightly bound. However, it was found exper-

308
The renormalization group
imentally in deep inelastic scattering experiments that quarks impacted
by very highly energetic electrons behave as if they’re free (or, at least
that the coupling between quarks is very small). This is entirely the
circumstance described by theories that show asymptotic freedom. The
strong interaction is described by a theory known as quantum chromo-
dynamics, which is a non-abelian gauge theory with an internal SU(3)
symmetry which gives rise to a conserved charge known as colour.13
13The virtual gluon excitations in the
vacuum carry colour charge and serve
to reinforce the ﬁeld, rather than screen
it. This eﬀect is sometimes called anti-
screening (see above) but is essentially
another way of describing asymptotic
freedom.
The theory shows asymptotic freedom. We introduce non-abelian gauge
theories in Chapter 46.
34.5
Application 2: Anderson localization
Crystalline metals conduct electricity, but what happens when you in-
troduce more and more disorder? Philip Anderson realized in 1958 that
beyond a critical amount of disorder, the diﬀusive motion that results
from impurity scattering would completely stop and the electrons would
become localized, residing in bound states rather than in the delocalized
band states of the pure metal. This process is called Anderson local-
ization. Thus we can imagine making a metal more and more impure
by implanting specks of dirt in it and at some point we expect to cross
from metallic behaviour to insulating behaviour, as Anderson localiza-
tion sets in. This tipping point in behaviour is known as the mobility
edge.
Let’s think about the scaling properties of conductivity, since that will
be important in a renormalization group treatment. A three-dimensional
metal of volume L3 conducts electrons. It has a small resistance R and
a large conductance G = 1/R. How does the conductance vary with
the length of a piece of metal? As examined in the exercises14 G(L) =
14See Exercise 34.1.
σL, where σ is the conductivity.
In fact, for a d-dimensional solid,
G(L) ∝Ld−2. In contrast, an insulator has a small conductance, given
by G(L) ∝e−L/ξ, where ξ is a length scale determined by microscopic
considerations. What we really want to know is that if we start with
a material with some ﬁxed amount of dirt in it, do we expect it to be
a conductor or an insulator? The renormalization group provides the
answer!
For this problem, David Thouless showed that the coupling
David Thouless (1934– )
constant of interest turns out to be the conductance G. We deﬁne our
β function as15
15This is another example of how the
β function is deﬁned in various ways to
suit the problem in hand.
β(g) = d ln g
d ln L = L
g
dg
dL,
(34.14)
where the dimensionless conductance is deﬁned as g = ℏG(L)/e2. Dif-
ferentiating our expressions, we obtain a prediction for the scaling in the
metallic and insulating regions for a d-dimensional solid:
β
g
gc
d = 3
d = 2
d = 1
Fig. 34.7
The renormalization group
ﬂows for Anderson localization. Arrows
show the ﬂow for long length scale.
β ≈
 (d −2)
metallic (large g)
ln g
insulating (small g).
(34.15)
Patching these limits together we get the ﬂow shown in Fig. 34.7. Notice
the ﬁxed point at gc, which will turn out to represent the mobility edge.

34.6
Application 3: the Kosterlitz–––Thouless transition
309
The ﬂow diagram predicts the behaviour of the system as we look at
larger and larger length scales. This is useful as the limit of large length
scales is what our measurements of electrical resistance probe.
A material containing a particular amount of dirt will be an insulator
or metal, depending on whether the ﬂows takes it to the left (small g)
or right (larger g) respectively.16 We see that, for d = 3, we have the
16Note that a ﬂow implying g →∞
doesn’t mean that we will measure this
behaviour in a real system. We would
expect some other piece of physics, not
included in our Lagrangian will cut oﬀ
such ﬂows eventually.
behaviour we predicted in our naive picture above: if the material has a
conductance above that at the mobility edge (the ﬁxed point in the ﬂow
known as gc) the material is destined to be a metal. If it has so much
dirt that g < gc it will be an insulator. The mobility edge is represented
by a repulsive ﬁxed point. The surprise comes when we look in d = 2
and d = 1 dimensions. In these cases there is no ﬁxed point and hence,
no mobility edge! If the system has any dirt in it at all it must ﬂow to
g = 0 and will be an insulator. This is a profound statement!
34.6
Application 3: the
Kosterlitz–––Thouless transition
J. Michael Kosterlitz.
The transi-
tion was simultaneously discovered by
Vadim L. Berezinskii in the Soviet
Union.
For any theory of a magnet in two spatial dimensions with a continu-
ous symmetry, the Coleman–Mermin–Wagner theorem (of Chapter 25)
predicts that there can be no symmetry breaking phase transition to
an ordered magnetic state. However, a phase transition of a diﬀerent
sort is predicted by a remarkable renormalization group analysis. This
is a topological phase transition, which separates two phases of matter
containing rather diﬀerent topological objects.
We work in two spatial dimensions, and examine vortices, which are
the topological objects that can exist in (2+1)-dimensional spacetime.
As discussed in Chapter 29, lone vortices are unstable17 at T = 0. An
17To recap:
this is because the spin
ﬁeld is swirly at spatial inﬁnity and this
costs the system an inﬁnite amount of
energy for a single vortex.
example of such a vortex is shown in Fig. 34.8. However, it’s a diﬀerent
story at high temperatures. This is because, rather than minimizing the
energy to ﬁnd the thermodynamic equilibrium state of the system, we
must minimize the free energy F = U −TS, and so at high temperature
there is a need to maximize entropy. The presence of a vortex gives the
system some entropy, since its centre may be placed at any one of a large
number of lattice sites and this provides a way of minimizing the free
energy. The entropic term in F is weighted by the temperature, making
the entropic advantage of vortices dominant at high T. We are led to
the conclusion that thermodynamics favours the proliferation of vortices
at high temperature.
+
Fig. 34.8 A vortex has inﬁnite energy,
due to the swirliness at spatial inﬁnity.
The question is, what happens at low temperatures? This is the ques-
tion that Kosterlitz, Thouless and Berezinskii addressed. Typically of
condensed matter applications of the renormalization group, we ask what
happens to the system of vortices as we look at successively longer length
scales. The renormalization group analysis of the two-dimensional vor-
tex problem therefore involves asking what happens to the important
coupling constants as we send the length scale of interest to inﬁnity.

310
The renormalization group
Example 34.3
We model this system with a complex scalar ﬁeld given in polar coordinates φ(x) =
eiθ(x). At a point in space x we see the ﬁeld as an arrow of unit length pointing along
a direction given by an angle θ(x). Such a picture is known as the two-dimensional
XY model18 since the arrows are constrained to the two-dimensional plane, where
18See also Chapters 42 and 44 for more
on the XY model.
their direction may vary continuously. The Euclidean action of a gas of vortices is
given by19
19The reason for writing the action this
way is to prevent the integral from di-
verging at the centre of the vortex,
where the ﬁeld looks to wind an inﬁ-
nite amount.
S = Score(a) + K
2
Z L
a
d2x |∇φ(x)|2,
(34.16)
where the ﬁrst term represents the energy of the core region of the vortex, which
has size a and the second term represents the energy cost of having a spin ﬁeld
that varies with position outside the core in a system with sides of length L. The
constant K = J/T where J is known as the stiﬀness of the spin system and T is
the temperature.
We may plug in our ansatz for the ﬁeld at a vortex, given by
φ(x) = eiθ(x), where θ(x) + ϕ = tan−1(x2/x1) for |x| →∞, just as we had in
Chapter 29. We integrate to ﬁnd an action20
20See Exercise 34.2.
S = Score(a) + πK ln(L/a).
(34.17)
What are the coupling constants that we change as the system ﬂows to
larger length scales?
• The spin stiﬀness J is one of the coupling constants and tells us
how much energy cost we pay for having the spins describe some
non-uniform conﬁguration. This is the parameter that causes a
single vortex to cost inﬁnite energy.21 At high temperature, where
21More generally J
arises from the
rigidity of the system.
Rigidity is a
general feature of ordered systems, re-
ﬂecting the energy cost associated with
deforming the order.
spins are presumably disordered, we do not expect any contribu-
tion of the spin stiﬀness to the action determining the physics of
the system over long distances. At low temperature J will be de-
cisive in determining the conﬁguration of spins. We thus need to
consider the ratio K = J/T which reﬂects the balance between
J and T and therefore we use K (actually K−1) as one of our
variables.
• The other coupling reﬂects the energy cost of a vortex core divided
by the temperature. It is known as the fugacity and is given by y =
e−Score(a) (note that the Euclidean action is eﬀectively a ratio of
energy and temperature in statistical mechanics, see Chapter 25).
The fugacity eﬀectively tells us how the system feels the presence of
the vortices. A small fugacity tells us that we have a conﬁguration
with small, energetic and compact vortex cores [with large Score(a)]
and that the spins don’t swirl much at inﬁnity; a large fugacity tells
us that we have less action in the cores and a signiﬁcant energy
cost from swirling ﬁelds at large distances owing to the existence
of free vortices. The energy cost here is, as usual for statistical
mechanics problems, measured with respect to the energy scale
set by kBT.
y
K−1
π
2
low T phase
high T phase
Fig.
34.9
Flow
diagram
for
the
Kosterlitz–Thouless transition.
The
horizontal axis is K−1 = T/J.
The behaviour of these two couplings predicted by the renormaliza-
tion group22 is shown in Fig. 34.9. Following the arrows (which show
22See Altland and Simons, Section 8.6
for details of the calculation.

34.6
Application 3: the Kosterlitz–––Thouless transition
311
the ﬂow to large length scales) we see that most of the trajectories point
to K−1, y →∞. This is the expected behaviour in the high-temperature
phase. The fact that y →∞tells us that we have free vortices present
making a non-uniform texture of spins at inﬁnity. In this phase entropy
wins out and J is ultimately an unimportant consideration in determin-
ing the thermodynamics, and so K = J/T ﬂows to zero.
The high-temperature phase, however, is not the only fate of the sys-
tem. We see that in the bottom-left of the plot there are ﬂows that lead
to ﬁxed points on the K−1 axis, with y = 0. This is the low-temperature
phase of the system, where stable conﬁgurations of vortices are possible,
even at T = 0. The fact that y ﬂows to zero means that the vortex con-
ﬁgurations must be quite compact, so that the cores look less and less
signiﬁcant at larger and larger length scales and that the spin texture
at inﬁnity must be uniform. We also have that K ̸= 0, meaning that
the system has rigidity. Since rigidity is one of the inevitable side-eﬀects
of magnetic order, this tells us that the low-temperature phase involves
some (possibly exotic) type of ordering.
+
−
(a)
(b)
Fig. 34.10
(a) A vortex. (b) An an-
tivortex.
−
+
−
+
−
+
−
+
−
+
−
+
−
+
Fig.
34.11
Schematic of the low-
temperature phase, viewed at long dis-
tance. The +s represent vortices, the
−s antivortices.
They form a gas of
dipole-like pairs.
How can this be?
The key to understanding the low-temperature
phase is the fact that it involves bound states of vortices and antivortices.
As shown in Fig. 34.10, an antivortex has spins that wind in the opposite
direction to a vortex. A picture of the low-temperature phase is shown
in Fig. 34.11 where we see that bound pairs of vortices and antivortices
are the stable ﬁeld conﬁgurations. The stability of these pairs at low
temperature is evident if we look at the ﬁelds at a large distance from
a vortex–antivortex pair as shown in Fig. 34.12. The clockwise swirl
of the vortex at inﬁnity is cancelled by the anticlockwise swirl of the
antivortex, making y →0. This means that in a system with nonzero
J, the pair costs a ﬁnite amount of energy.
−
+
Fig. 34.12 A vortex dipole made from
a vortex and antivortex in the same re-
gion of space.
Notice that the ﬁelds
may now be made uniform at inﬁnity.
We can even come up with an estimate of the transition temperature
between the low and high temperature phases. Consider the stability
of the high-temperature phase, containing a single vortex. The energy
of the vortex is given (from eqn 34.17) by πJ ln(L/a). The size of the
entire system is L and the size of a vortex core is a. We may therefore
ﬁt a vortex into the system in L2/a2 ways, giving an entropy kB ln Ω=
kB ln(L/a)2. The free energy is then F = (πJ −2kBT) ln(L/a). The
only way of preventing this free energy from becoming very large and
positive is for 2kBT > πJ. Below T ≈πJ/2kB the system is unstable
to the formation of bound vortex–antivortex pairs. Of course, this is
identical to the ﬁxed point in the renormalization-group ﬂow diagram
of Fig. 34.9 at K−1 = π
2 (apart, of course, from the factor kB that was
set to one in our hair-shirt quantum ﬁeld treatment). The transition is
often also called the vortex unbinding transition, describing the breakup
of the bound states on warming through the transition region.

312
The renormalization group
Chapter summary
• The renormalization group is a method for studying how coupling
constants change with rescaling.
• This method provides insight into many physical phenomena,
including asymptotic freedom, Anderson localization, and the
Kosterlitz–Thouless transition.
Exercises
(34.1) We will show using dimensional analysis that the
conductance of a d-dimensional solid varies as
G(L) ∝L(d−2).
(a) For d = 3 consider a cubic sample with sides of
length L and show G = σL, where σ is the conduc-
tivity.
(b) For d = 2 consider a ﬂat plate of thickness a
and other sides of length L and show G = σa.
(c) For d = 1 show G = σa/L.
(34.2) By considering the ansatz in the text verify that the
action of a vortex is given by
S = Score(a) + πK ln(L/a).
(34.18)
If instead the phase winds n times round a vortex,
show that
S = Score(a) + πKn2 ln(L/a).
(34.19)

35
Ferromagnetism: a
renormalization group
tutorial
35.1 Background:
critical
phe-
nomena and scaling
313
35.2 The ferromagnetic transition
and critical phenomena
315
Chapter summary
320
Exercises
320
In this chapter we are going to study how the coupling constants in a
Euclidean φ4 theory vary with increasing length scale. This will tell us
about the physics of phase transitions. To make things concrete, we will
discuss the magnetic transition separating a paramagnetic phase and a
ferromagnetic phase.1 Perhaps the most wonderful feature of a phase
1We have already discussed the mean-
ﬁeld treatment of this in Chapter 26.
In this chapter we will include the pos-
sibility of ﬂuctuations in the ﬁelds.
transition is universality.
This is the notion that some of the key
properties of a phase transition depend only on the dimensionality of a
system and dimensionality of its order parameter ﬁeld. The details of
the microscopic interactions between the constituent parts, whether the
transition is ferromagnetic or superconducting, or whether it involves
polymers or the early Universe, are all irrelevant. Thus focussing on the
ferromagnetic transition will nevertheless give results that are applicable
to any system described by the same Lagrangian.
35.1
Background: critical phenomena and
scaling
Magnetic phase transitions may be observed with several experimen-
tal techniques. Some thermodynamic properties of a magnet in which
we might be interested include the magnetization M (which is the or-
der parameter of the ferromagnet system); the magnetic susceptibility
χ = limB→0
µ0M
B , which tells us how much magnetization we can achieve
through applying a ﬁeld B; and the heat capacity C, which tells us how
the energy of the system varies with temperature. To these thermody-
namic properties we can add the correlation length ξ, which tells us
how well correlated the ﬂuctuations are in space (Fig. 35.1).
ξ
Fig. 35.1
The correlation length in
a paramagnet showing a ferromagnetic
ﬂuctuation of size ξ.
The correlation length is related to the correlation functions, which
are the Green’s functions of Euclidean space. As described in Chap-
ter 25, a slight modiﬁcation of the usual correlation function G(x, y) =
⟨ˆφ(x)ˆφ(y)⟩t is useful when studying phase transitions. This is the con-
nected correlation function Gc(x, y), given by
Gc(x, y) = ⟨ˆφ(x)ˆφ(y)⟩t −⟨ˆφ(x)⟩t⟨ˆφ(y)⟩t,
(35.1)

314
Ferromagnetism: a renormalization group tutorial
and diﬀers from G(x, y) by the subtraction of a product of thermally
averaged ﬁelds ⟨ˆφ(x)⟩t⟨ˆφ(y)⟩t.
At temperatures above the transition
temperature Tc we have ⟨ˆφ(x)⟩t = 0 and the deﬁnition is identical to
the usual form of the correlation function. At temperatures below Tc,
subtracting oﬀthe mean-ﬁelds allows us to just see the ﬂuctuations away
from average behaviour. For example, in a magnet above Tc, despite
the spins being disordered they may save energy by pointing in the
same direction, and will tend to do so within the correlation length ξ.
The correlation between spins at position x and position y decreases
with increasing |x −y| because thermal ﬂuctuations have a tendency to
randomize the spins. Thus we ﬁnd that2
2Closer to the disturbance the correla-
tions are found to decay more slowly, as
described below.
Gc(x, y) ∼e−|x−y|/ξ,
(35.2)
for |x −y| ≫ξ. The subtraction of the average ﬁelds in our deﬁnition
of Gc means that this is also the behaviour below Tc and represents the
ﬂuctuations away from perfect order.
At temperatures close to the ordering temperature Tc it is found exper-
imentally that the physical properties of a magnet described above fol-
low power law behaviour3 as a function of temperature, applied magnetic
3Unlike, for example, exponential func-
tions such as e−x/ξ, power laws are free
from any dependence on a length scale.
This scale-free property is a hallmark of
critical behaviour, where the character-
istic ﬂuctuations occur over all length
scales.
ﬁeld or distance. This motivates the deﬁnition of critical exponents
to describe the phase transition. Deﬁning the reduced temperature as
t = (T −Tc)/Tc, the critical exponents α, β, γ, δ, ν, ξ and η are related
to physical properties as follows:
• Heat capacity: C ∼|t|−α,
• Magnetization: M ∼(−t)β, for B →0, T < Tc,
• Magnetic susceptibility: χ ∼|t|−γ,
• Field dependence of χ at T = Tc: χ ∼|B|1/δ,
• Correlation length: ξ ∼|t|−ν,
• The correlation function G(r) behaves like
G(r) ∼
(
1
|r|d−2+η
|r| ≪ξ
e−|r|
ξ
|r| ≫ξ,
(35.3)
where r is distance and d is the dimensionality of the system.
A renormalization group analysis of the ferromagnet relies on varying
the length scale at which we examine the physics. Well before the as-
cent of the renormalization group, Benjamin Widom suggested that the
critical behaviour that is observed experimentally may be explained if
the free energy4 of the system has a particular form under such a scaling
4In these calculations, one studies the
reduced free energy f (dividing F
by both volume V = Ld and temper-
ature T) so that f = F/(LdT).
We
also deﬁne reduced temperature t =
|T −Tc|/Tc and reduced ﬁeld h = B/T.
of the length scale. The idea is that when rescaling lengths by a factor
b (so that a length L →bL), we should expect
f(t, h) = b−df(t′, h′),
(35.4)
where the factor b−d is because of the L−d factor in f. Widom assumed
that t′ = bytt and h′ = byhh, where yh and yt are exponents yet to be

35.2
The ferromagnetic transition and critical phenomena
315
determined. Thus the Widom hypothesis is
f(t, h) = b−df(bytt, byhh).
(35.5)
We may use the fact that b is an arbitrary parameter and choose a value
of b such that bytt = 1. This reduces the number of free parameters by
one, and we obtain
f(t, h) = td/ytf(1, h/tyh/yt),
(35.6)
which is Widom’s hypothesis, written in reduced variables. Using this
result, and also expressions relating the free energy to physical properties
such as magnetization, heat capacity, magnetic susceptibility, etc., one
can show that the critical exponents can be expressed in terms of the
y’s as follows:5
5See Exercise 35.1.
α = 2 −d
yt ,
β = d−yh
yt ,
γ = 2yh−d
yt
,
δ =
yh
d−yh ,
ν =
1
yt ,
η = 2 + d −2yh.
Our task is now to use the renormalization group to ﬁnd yt and yh.
We ﬁnd these quantities by noting how the quantities t and h ﬂow on
renormalization. Since we have the predictions t →bytt and h →byhh,
we should be able to simply read oﬀthe values of yt and yh. If we can
do this then we have access to all of the critical exponents, which we
can compare with experiment.
35.2
The ferromagnetic transition and
critical phenomena
Let’s feed a theory of the ferromagnet through the renormalization group
machine and extract some physics.
A continuum ﬁeld theory of the
ferromagnet is given by the Landau–Ginzburg model.
This model
Vitaly Ginzburg (1916–2009)
is equivalent to (time-independent) d-dimensional Euclidean φ4 theory,
which has an action6
6We can choose d to suit whatever di-
mensionality of system we need.
Or-
dinarily we imagine materials having
d = 3, but it is possible in condensed
matter physics to ﬁnd materials which
eﬀectively have d = 2 and d = 1.
SE =
Z
ddx LE =
Z
ddx
1
2(∇φ)2 + m2
2 φ2 + λ
4!φ4

.
(35.7)
Example 35.1
We may see what the Landau–Ginzburg model predicts in mean-ﬁeld theory where
we take φ to be uniform across the whole system.7 We then have a free energy
7Note that this approach recreates the
Landau
theory
of
phase
transitions
used in Chapter 26.
F = m2
2 Φ2 + λ
4! Φ4,
(35.8)
where Φ =
R
ddx φ(x). If λ = 0 then the minimum free energy occurs at M = 0 and
we have a paramagnet. If λ > 0 and m2 = a(T −Tc) < 0 then we have minima
at M = [6a(Tc −T)/λ]
1
2 ̸= 0, giving a ferromagnet.
However, mean-ﬁeld theory
is blind to ﬂuctuations. We will show with the more sophisticated approach of the
renormalization group that even if m2 < 0 and λ > 0 a phase transition is not
guaranteed due to the inﬂuence of these wobbles in the ﬁeld.

316
Ferromagnetism: a renormalization group tutorial
As a warm-up exercise we’ll set the coupling λ = 0 and carry out the
renormalization group procedure for the free theory.8
8In this context, the λ = 0 free theory
is called the Gaussian model.
Example 35.2
We choose to work in momentum space, so we write the action in terms of ﬁelds in
momentum space in the usual manner:
SE = 1
2
Z
ddp
(2π)d ˜φ(−p)
`
p2 + m2´ ˜φ(p).
(35.9)
To set the scene, we split the ﬁelds up into slow and fast via φ = φf + φs where
˜φ(p) = ˜φf(p)
for Λ/b ≤p ≤Λ,
˜φ(p) = ˜φs(p)
for 0 ≤p ≤Λ/b.
(35.10)
The action in eqn 35.9 is diagonal in p, which means that slow and fast ﬁelds will
not be mixed up in the calculation of S. Let’s go through the steps with this free
theory.
Step I involves integrating over fast momenta. This just results in a constant con-
tribution to the action, which we are free to ignore. We therefore have the integral
Z =
Z
Λ/b
D ˜φs e−SE[φs] =
Z
Λ/b
D ˜φs e
−1
2
R
d4p
(2π)d ˜φs(p)(p2+m2) ˜φs(p).
(35.11)
Step II: We scale the momenta with p′/b = p to obtain an action
SE = b−d
2
Z Λ
0
ddp′
(2π)d ˜φs(−p′/b)
`
b−2p′2 + m2´ ˜φs(p′/b).
(35.12)
Step III: We scale the ﬁelds via ˜φs(p′/b) = bd−dφ ˜φ′(p′) to obtain
SE = b2(d−dφ)b−d
2
Z Λ
0
ddp′
(2π)d ˜φ′(−p′)
`
b−2p′2 + m2´ ˜φ′(p′).
(35.13)
Fixing the constant dφ so that the gradient term ˜φ(−p)p2 ˜φ(p) is invariant, we have
d −2dφ −2 = 0 or dφ = (d −2)/2 and
SE = 1
2
Z Λ
0
ddp′
(2π)d ˜φ′(−p′)
`
p′2 + m2b2´ ˜φ′(p′).
(35.14)
The result of our renormalization procedure is that after scaling, we ﬁnd that we
have a mass term m′2 = m2b2. That is, since b > 1, the coupling constant m2 gets
larger with each rescaling.
Since b > 1, the fact that m′2 = m2b2 means that m2 is a relevant variable
and ˆφ2 is known as a relevant operator. This means that it gets larger with each
renormalization. (In contrast a variable that becomes smaller at each rescaling is
known as an irrelevant variable.) Identifying m2 from our action with the reduced
temperature t, we expect that each rescaling changes m2 by a factor byt. From this
analysis we can conclude that yt = 2 when λ = 0.
In order to ﬁnd yh we need to consider the application of an external magnetic
ﬁeld. Still holding λ = 0 we write an action that includes the inﬂuence of the ﬁeld as
SE =
Z
ddx
» 1
2(∇φ)2 + m2
2 φ2 −hφ
–
.
(35.15)
On rescaling the ﬁeld-dependent term will become −hbd−dφ ˜φ′(p′). Substituting for
dφ as before we have h′ = hb
d+2
2 , and so yh = (d + 2)/2. This allows us to write the
exponents predicted from this λ = 0 theory.

35.2
The ferromagnetic transition and critical phenomena
317
Now let’s turn on the interaction. Since we don’t know how to integrate
the functional integral for the φ4 theory it should come as little surprise
that neither can we do the integral over the fast modes for φ4 theory. We
therefore have to treat LI = λ
4!φ4 as a perturbation and do the integral
approximately. Our previous method for working out approximations to
the generating functional Z[J] using Feynman diagrams may be adapted
to get an approximate answer for the result of removing the fast modes.
The only diﬀerence in calculating the Feynman diagrams for the renor-
malization group analysis is that we integrate over internal momenta
only over the fast modes, up to the cut-oﬀ.
In order to work out the lowest order corrections to the couplings we
consider diagrammatic corrections to the theory in order of how many
loops they contain. The lowest order diagrammatic corrections to the
theory therefore involve diagrams with a single loop that correct the
self-energy (which determines m2) and the phion–phion vertex function
(which determines λ). These one-loop diagrams are shown in Fig. 35.2.
Fig.
35.2
One-loop Feynman dia-
grams in φ4 theory.
Example 35.3
We give the results of the important computations here.
The ﬁrst diagram in Fig. 35.2 corresponds to an amplitude −λ
2
R
ddp
(2π)d
1
p2+m2 ,
which yields a contribution δS(2) to the quadratic part of the action given9 by
9Although both of our diagrams have
symmetry factor 2, there is a subtlety
here in working out the prefactors re-
ﬂecting the fact that there is more than
one equivalent diagram for each loop.
As discussed in Binney, Dowrick, Fisher
and Newman, the correct prefactor for
a diagram with p loops is
2p!
2p2p .
δS(2) = λ
2
Z Λ
Λ/b
ddp
(2π)d
1
p2 + m2
Z
ddx φ2
s
2 .
(35.16)
Doing I: the integral over fast modes, II: rescaling the momentum and III: rescaling
the ﬁelds, we obtain a contribution to the quadratic term
δS(2) = b2
»
λΩd
2(d −2) (1 −b2−d) −m2λΩd
2(d −4) (1 −b4−d)
– Z
ddx φ2
s
2 ,
(35.17)
which we give in terms of Ωd, which is the volume of a unit sphere in d dimensions.
Measured in units of 2π this volume is given by Ωd =
1
(2π)d
2πd/2
Γ(d/2), where Γ(z) =
R ∞
0
dt e−ttz−1 is the Gamma function. The usual procedure in these calculations is to
work in four-dimensional spacetime where the integrals converge and Ωd=4 = 1/8π.
To see the eﬀects in other dimensions we set d = 4 −ǫ, and expand in the small
parameter ǫ. To examine the real-life case of three dimensions we then set ǫ = 1,
well outside the presumed range of applicability of this approach. Amazingly, this
procedure describes reality very well!
Turning to the second diagram in Fig. 35.2, it turns out that we only need to
evaluate the amplitude for zero external momentum,10 which means both internal
10See Altland and Simons, Section 8.4
for a discussion.
propagators carry the same momentum. We then obtain a contribution δS(4) to the
quartic term in the action
δS(4)
=
3λ2
2
Z Λ
Λ/b
ddq
(2π)d
1
(q2 + m2)2
Z
ddx φ4
s
4!
=
b4−d 3λ2Ωd
2(d −4) (1 −b4−d)
Z
ddx φ4
s
4! .
(35.18)
The result of this expansion is that we must shift the variables according to
m′2
=
b2
»
m2 +
λΩd
2(d −2) (1 −b2−d) −m2λΩd
2(d −4) (1 −b4−d)
–
,
λ′
=
b4−d
»
λ −3λ2Ωd
2(d −4) (1 −b4−d)
–
.
(35.19)

318
Ferromagnetism: a renormalization group tutorial
Now to extract the physics. We’re looking at a system of condensed mat-
ter, so we’re interested in a scaling that makes the length scale longer.
We therefore set b = el.
Since, as stated in the example, the inte-
grals only converge for d = 4, we set d = 4 −ǫ and expand in the
parameter ǫ.
Evaluating the expressions to leading order in ǫ, using
Ω4−ǫ ≈Ω4 = 1/8π2, we obtain the Gell-Mann–Low equations
dm′2
dl
=
2m2 +
λ
16π2 (1 −m2),
dλ′
dl
=
ǫλ −3λ2
16π2 .
(35.20)
As shown in Fig. 35.3, these equations have ﬁxed points at (m2, λ) =
(0, 0) and

−ǫ
6, 16π2
3 ǫ

. By expanding the ﬂow around the ﬁxed points
we can use this information to predict the behaviour of the ferromagnet.
Example 35.4
To see the behaviour near the ﬁxed point we employ a ﬁrst-order expansion to come
up with matrix equations predicting the ﬂow. A Taylor expansion of f(x) about a
ﬁxed point x∗yields f(x) = f(x∗)+f′(x∗)(x−x∗), but since f(x∗) = 0, by deﬁnition,
we simply need to evaluate the derivatives at the ﬁxed points. Our linearized ﬂow
equations take the form
„
dm′2/dl
dλ′/dl
«
=
0
@
∂
∂m2
“
dm′2
dl
”
∂
∂λ
“
dm′2
dl
”
∂
∂m2
“
dλ′
dl
”
∂
∂λ
“
dλ′
dl
”
1
A
„
m2 −m∗2
λ −λ∗
«
,
(35.21)
with the elements evaluated at each of the ﬁxed points.
β
λ
16πǫ
3
0
Fig. 35.3
The β function for the be-
haviour of the coupling constant λ.
For our ferromagnet, the matrices for the two sets of ﬁxed points are
W1 =
„ 2
1
16π2
0
ǫ
«
,
W2 =
„ 2 −ǫ
3
1
16π2
0
−ǫ
«
.
(35.22)
Let’s examine how these equations predict the behaviour of λ. For the ﬁxed point
at (0, 0) (corresponding to W1) we have λ′ = ǫλl. If ǫ > 0 (that is, we are examining
the physics in less than four dimensions since d = 4 −ǫ) then λ increases as we look
at larger scales. In that case λ is a relevant variable and will be important to the
physics. Moving on to the other ﬁxed point (−ǫ/6, 16π2ǫ/3), we see that close to
λ∗= 16πǫ/3 we have λ′ = −ǫ(λ −λ∗)l. In this case λ is relevant (i.e. gets larger on
rescaling) for λ < λ∗but irrelevant (i.e. gets smaller on rescaling) for λ > λ∗. We
will see the inﬂuence of this on the ﬂow of the system next.
λ
m2
W1
W2
A
B
C
D
FM
PM
Fig. 35.4 Renormalization group ﬂow
for a ferromagnet.
FM labels ‘ferro-
magnet’, PM labels ‘paramagnet’.
This information, along with an analogous analysis of how m2 behaves,
allows us to draw the phase diagram of the ferromagnet, as shown11
11The predictions of the previous ex-
ample are seen in the ﬂows close to W2,
where we see λ decrease or increase de-
pending, respectively, on whether we
start above (ﬂow B) or below (ﬂow C)
the value λ∗= 16πǫ/3. Flows A and
D are clearly more complex and rely on
the magnitude and behaviour of m2.
in Fig. 35.4 for ǫ > 0.
The ﬁxed point at W1 is repulsive and that
at W2 is mixed: some ﬂows are directed towards it, some away from
it. Let’s return to the physics of the system: at high temperatures we
have a disordered, paramagnetic phase; at low temperatures, we suspect
from our mean-ﬁeld analysis that there exists an ordered, ferromagnetic
phase. Relating this to our model, we write m2 = a(T −Tc), so ﬂows that
send m2 to large, positive values predict that the system will end in the
paramagnetic phase. Conversely, ﬂows that send m2 to negative values

35.2
The ferromagnetic transition and critical phenomena
319
predict that the system becomes ferromagnetic. By examining the ﬂows
we ask what inﬂuence the ﬂuctuations have on the phase transition.
Recall that our mean-ﬁeld analysis makes a prediction that any system
that starts with a negative m2 will be a ferromagnet and any system
that starts with a positive m2 will be paramagnetic.
The result of our renormalization group analysis is that ﬂows starting
with positive values of m2 are always driven towards the paramagnetic
phase, whatever the value of λ, in agreement with the mean-ﬁeld treat-
ment. However, starting with a negative value of m2 doesn’t guarantee
a ferromagnetic ground state! The fact that we have ﬂuctuations, whose
strength is given by λ, may force the system to be a paramagnet. This
is the case for the ﬂow B in Fig. 35.4, where starting with a signiﬁcant
enough λ is enough to drag the system towards paramagnetism. On the
other hand, ﬂows starting with suﬃciently small λ and suﬃciently neg-
ative m2 (such as paths C and D) eventually ﬂow oﬀto the left, pushing
the system towards ferromagnetism.
Example 35.5
So much for our qualitative analysis of the phase diagram.
Let’s extract yt and
yh and make some quantitative predictions regarding the thermodynamics of the
ferromagnet. One of the uses of the linearized ﬂow matrices is that they may be
used to read oﬀthe scaling constant yt.
Since we have set b = el, we have that
each rescaling goes as m′2 = elytm2. Close to the ﬁxed point, where our linearized
equations are valid this implies that ∆m′2 = yt(m2 −m∗2)∆l. We can therefore
read oﬀfrom the matrix describing the ﬂow around our nonzero ﬁxed point that
yt = 2 −ǫ
3 . The one-loop correction doesn’t change yh, which continues to be given
by yh = (d + 2)/2 = (6 −ǫ)/2 just as it was from the analysis of the λ = 0 case.
With these two pieces of information, we obtain all six of the critical parameters:
α = ǫ
6 ,
β = 1
2 −ǫ
6,
γ = 1 + ǫ
6,
δ = 3 + ǫ,
ν = 1
2 +
ǫ
12 ,
η = 0.
(35.23)
These results can be used to make an estimate of the critical exponents of the three-
dimensional Ising model, as shown in the following table.12
12For a discussion of the comparison
see the books by Kaku, Zinn-Justin
and Binney et al. See also Peskin and
Schroeder, Chapter 13.
α
β
γ
δ
ν
η
ǫ = 0 (d = 4, mean ﬁeld)
0
1
2
1
3
1
2
0
ǫ = 1
O(ǫ)
0.167
0.333
1.167
4
0.583
0
ǫ = 1
O(ǫ2)
0.077
0.340
1.244
4.462
0.626
0.019
3D Ising
0.110
0.327
1.237
4.789
0.630
0.036
2D Ising (exact)
0
1
8
7
4
15
1
1
4
The ﬁrst line of this table gives the mean-ﬁeld results, exact for d = 4 and so can
be obtained from eqn 35.23 by setting ǫ = 0. The second line gives a crude, but
surprisingly accurate, estimate for d = 3 by setting ǫ = 1. One can derive expressions
for the critical exponents to second order in ǫ [e.g. β = 1
2 −ǫ
6 +
ǫ2
162 + O(ǫ3)] and
these yield the estimates in the third line of the table. These approach the currently
accepted values13 (fourth line) which agree well with experimental values. In the last
13Taken from A. Pelissetto and E. Vi-
cari, Phys.
Rep.
368, 549 (2002),
which reviews a large number of theo-
retical studies using various techniques
and also summarises experimental re-
sults.
line we list the exact results for the two-dimensional Ising model for comparison.

320
Ferromagnetism: a renormalization group tutorial
Chapter summary
• This chapter has provided a renormalization group analysis of the
ferromagnetic transition following Widom’s hypothesis for the scal-
ing of the free energy.
Exercises
(35.1) (a) Using Widom’s hypothesis in the form f(t, h) =
t
d
yt ˜f(h/t
yh
yt ), show that critical parameters are
given in terms of the scaling parameters as follows
α = 2 −
d
yt ,
β = d−yh
yt ,
γ = 2yh−d
yt
.
(35.24)
(b) We now redeﬁne the scaling function slightly so
that it reads f(t, h) = h
d
yh ˜g(h/t
yh
yt ) where g(z) =
z
−d
yh f(z). Use this form of the function to show
that δ =
yh
d−yh .
(c) From these results prove the following relations:
α + 2β + γ = 2
Rushbrooke’s law,
α + β(δ + 1) = 2
Griﬃth’s law.
(35.25)
(d) Assuming the correlation function behaves as
Gc(x, t) = f
“
xt
2−α
d
”
/xd−2+η, argue that we re-
quire νd = 2 −α (known as Josephson’s law) and
use this to show ν =
1
yt .
(e) Finally use Fisher’s law (2 −η)ν = γ to ﬁnd η.
(35.2) (a) Starting from eqns 35.19, verify the Gell-Mann–
cLow equations in eqn 35.20.
Hint: let b = 1 + ρ with ρ small. Then ln b ≈ρ
and one may expand to order ρ to capture the ﬂow
equations.
(b) Verify the positions of the ﬁxed points.
(c) Verify the matrices in 35.22 and the behaviour
of the ﬂow near the ﬁxed points.
(35.3) What is the behaviour of λ for the case of dimen-
sions d > 4?

Part IX
Putting a spin on QFT
This part extends our treatment of quantum ﬁeld theory to spinors and
allows us to treat the electron spin. These ideas set us on a path that
inevitably arrives at quantum electrodynamics.
• The Dirac equation for an electron is introduced in Chapter 36.
We describe the γ-matrices necessary for formulating this and also
the concepts of chirality and helicity. The basis states are called
spinors and we describe two types: Dirac spinors and Weyl spinors.
• We show how spinors transform under rotations and boosts in
Chapter 37. We also demonstrate that the parity operator turns a
left-handed spinor into a right-handed spinor and vice versa.
• In Chapter 38 we apply the machinery of quantization to the Dirac
equation and derive the Noether current. This allows us to write
down the fermion propagator.
• Arguably the most important achievement of quantum ﬁeld theory
is quantum electrodynamics (QED), which describes the interac-
tion between electrons and photons, and this theory is presented
in outline in Chapter 39.
• Three examples of ideas presented in the previous chapter are
ﬂeshed out in Chapter 40 via three examples. These are Ruther-
ford scattering, the Mott formula and Compton scattering. We
brieﬂy discuss the useful feature of crossing symmetry.
• Some important consequences of QED arise when you consider pro-
cesses involving electron–positron loops. These include the renor-
malization of the electron charge due to vacuum polarization and
the deviation of the g-factor from g = 2. These ideas are explored
in Chapter 41.

36
The Dirac equation
36.1 The Dirac equation
322
36.2 Massless particles: left- and
right-handed wave functions
323
36.3 Dirac and Weyl spinors
327
36.4 Basis
states
for
superposi-
tions
330
36.5 The non-relativistic limit of
the Dirac equation
332
Chapter summary
334
Exercises
334
I have an equation, do you have one too?
Paul Dirac (1902–1984), on meeting Richard Feynman
Our story so far: as a single-particle equation of motion, the Klein–
Gordon equation has some serious problems. The ﬁrst is negative energy
states and the second is negative probabilities. Paul Dirac sees that the
root of these problems lies in the fact that the Klein–Gordon equation
contains a second-order time derivative as part of ∂2. While staring into
the ﬁre in the senior common room of St John’s College, Cambridge,
Dirac realized that there’s a way to construct an equation of motion
that’s ﬁrst order in the time derivative.
After seeing 2001: A Space Odyssey,
Paul Dirac was so impressed that, two
days later, he attended three consecu-
tive screenings of the ﬁlm in the cinema.
His favourite television programme was
the Cher show.
In this chapter we examine Dirac’s equation. We will treat it as an
equation of motion for single particles and examine the form of the
solutions and how features of these solutions describe the physical prop-
erties of real particles. Although our experience with the Klein–Gordon
equation has taught us that, ultimately, a single-particle treatment of
relativistic quantum mechanics will not provide an adequate description
of Nature, the experience gained with the solutions of Dirac’s equation
will carry over to the world of Dirac ﬁelds1 whose excitations are the
1This is the subject examined in the
two chapters following this one
fermions we observe in our Universe.
36.1
The Dirac equation
In order to avoid all of the problems with the Klein–Gordon equation,
you might wonder whether we could take the square root of the guts
of that equation (∂2 + m2) and then apply it to a wave function. A
naive guess might be that if we could treat things like ∂2 like ordinary
algebraic symbols then we could write
(∂2 + m2) = (
√
∂2 + im)(
√
∂2 −im).
(36.1)
Unfortunately,
√
∂2 isn’t deﬁned. How do we take the square root of the
∂2 operator? Dirac found the answer was to deﬁne a new four-vector
γµ, whose components have the properties
(γ0)2 = 1, (γ1)2 = −1, (γ2)2 = −1, (γ3)2 = −1.
(36.2)
Furthermore, these components aren’t ordinary numbers; they anticom-
mute. That is to say that for µ ̸= ν they have the intriguing property

36.2
Massless particles: left- and right-handed wave functions
323
γµγν + γνγµ = {γµ, γν} = 0. Taken with eqn 36.2, the properties of
these γ’s may be summarized in the compact form2
2Equation 36.3 deﬁnes what is known
as a Cliﬀord algebra, named after
William Kingdon Cliﬀord (1845–1879).
See Penrose, Chapter 11 for more de-
tails.
{γµ, γν} = 2gµν.
(36.3)
We use the gammas to cope with the square root in
√
∂2, via a strange
looking new symbol ✁a = γµaµ, which is pronounced ‘a-slash’. We can
then say that
✁∂2 = (γµ∂µ)2 =

γ0 ∂
∂t + γ1 ∂
∂x + γ2 ∂
∂y + γ3 ∂
∂z
2
.
(36.4)
Expanding this bracket and using the all-important anticommutation
relations we ﬁnd that ✁∂2 = ∂2. Substituting ✁∂2 for ∂2 in the Klein–
Gordon equation does indeed allow us to factorize
(∂2 + m2) = (✁∂2 + m2) = (✁∂−im)(✁∂+ im).
(36.5)
Now let’s just take the plus-sign bracket and treat it as an operator
acting on the wave function ψ(x),
(✁∂+ im)ψ(x) = 0,
(36.6)
and this may be tidied up to give us the famous Dirac equation:
(iγµ∂µ −m)ψ(x) = 0.
(36.7)
We note ﬁrst that the Dirac equation may also be written as (✁✁ˆp−m)ψ =
0, where ✁✁ˆp = γµ(i∂µ) and we recognize the energy-momentum operator3
3Recall that ˆpµ = i∂µ = i( ∂
∂t , ∇).
ˆpµ = i∂µ.
We’ll soon see that the dispersion relation of the particles described
by the Dirac equation of motion continues to be E2
p = (p2 + m2), which
still admits negative energy states.4
4It does successfully eliminate the neg-
ative probability problem.
Of course,
we’ve already found a way to deal with
the negative energy states of the Klein–
Gordon equation: Feynman’s prescrip-
tion of interpreting them as positive en-
ergy antiparticles, so the negative en-
ergy states are not a deal-breaker. In
fact, the Dirac equation will turn out
to provide the equation of motion for
a type of ﬁeld whose excitations are
fermions such as the electron. As such,
the results from this temporary foray
into wave mechanics should be treated
as provisional.
To get to Dirac’s equation, we have had to introduce the mysterious,
anticommuting γµ objects. These will have a profound inﬂuence on the
physics described by the Dirac equation because the simplest way to
satisfy the commutation relations is to represent the γ’s as four-by-four
matrices.
This means that the wave function ψ(x) described by the
Dirac equation is a special four-component wave function. Why four
components? What could they represent? To make progress, we’ll use
an explicit form for the matrices and examine some illuminating special
cases.
36.2
Massless particles: left- and
right-handed wave functions
There is no unique way of writing the γµ matrices and so we have a
choice about how to represent them. One set of matrices that satisﬁes

324
The Dirac equation
the commutation relations is
γ0 =




0
0
1
0
0
0
0
1
1
0
0
0
0
1
0
0



,
γ1 =




0
0
0
1
0
0
1
0
0
−1
0
0
−1
0
0
0



,
γ2 =




0
0
0
−i
0
0
i
0
0
i
0
0
−i
0
0
0



,
γ3 =




0
0
1
0
0
0
0
−1
−1
0
0
0
0
1
0
0



.
(36.8)
Writing the γ’s in this form is known as the chiral representation,5
5This is sometimes called the Weyl
representation.
for reasons that will become apparent.
Writing out lots of four-by-
four matrices is a tedious business, so to save on writing it’s useful to
write these four-by-four matrices out as two-by-two matrices, where each
element is itself a two-by-two matrix.
Example 36.1
There are several ways to simplify the notation. Deﬁning I =
„ 1
0
0
1
«
, we notice
that we can express eqn 36.8 in a 2 × 2 form:
γ0 =
„ 0
I
I
0
«
,
γ1 =
„
0
σx
−σx
0
«
,
γ2 =
„
0
σy
−σy
0
«
,
γ3 =
„
0
σz
−σz
0
«
,
(36.9)
where the Pauli matrices have their usual deﬁnitions.
It turns out that γµ transforms as a four-vector.
Motivated by this, we write
γµ = (γ0, γ1, γ2, γ3) = (γ0, γ) and then
γ0 =
„ 0
I
I
0
«
, γ =
„
0
σ
−σ
0
«
,
(36.10)
where σ = (σ1, σ2, σ3). Finally, in a desire to write as little as possible, we deﬁne
σµ = (I, σ) and ¯σµ = (I, −σ) to obtain6
6If, at any point in the future, any
of this notation seems overly compact,
then it may sometimes be helpful to
write things out in full.
γµ =
„
0
σµ
¯σµ
0
«
.
(36.11)
Substituting these γ’s into the Dirac equation gives
(γ0ˆp0 −γ · ˆp −m)ψ(x) = 0,
(36.12)
where ˆp0 = i∂0 and ˆp = −i∇. We’ll make use of a handy two-by-two
matrix form for the equations, where the four-component wave function
ψ is written

ψL
ψR

, where ψL and ψR are, themselves, two-component
column matrices.7 In this form the Dirac equation reads
7The meaning of these subscripts will
be explained shortly.
 0
ˆp0
ˆp0
0

−

0
σ · ˆp
−σ · ˆp
0

−
 m
0
0
m
  ψL
ψR

= 0,
(36.13)

36.2
Massless particles: left- and right-handed wave functions
325
which can be rewritten as
(ˆp0 −σ · ˆp)ψR
=
mψL,
(ˆp0 + σ · ˆp)ψL
=
mψR.
(36.14)
Now let’s further simplify things by considering the special case of a
massless particle (which obviously has m = 0). We then obtain
(ˆp0 −σ · ˆp)ψR
=
0,
(ˆp0 + σ · ˆp)ψL
=
0,
(36.15)
which implies that a four-component eigenstate ψ =
 ψL
ψR

splits into
two two-component pieces, ψL and ψR, that don’t get mixed up by the
equations of motion.
It’s as if the Dirac equation is telling us that
we have two sorts of massless Dirac particles in Nature: left-handed
particles described by wave functions ψL and right-handed particles
described by wave functions ψR. In the chiral representation, left-handed
functions live in the upper two slots of the Dirac wave function and right-
handed functions live in the lower slots.
Example 36.2
We need to be slightly more precise with our deﬁnition of left- and right-handed wave
functions. To do this we deﬁne the chirality operator as
γ5
=
iγ0γ1γ2γ3
=
„ −I
0
0
I
«
,
(36.16)
where the ﬁnal equality holds in the chiral representation only.
A wave function
„ ψL
0
«
is an eigenstate of γ5 with eigenvalue −1. An entirely right-handed wave
function
„
0
ψR
«
is then an eigenstate with eigenvalue +1.
The Dirac equation
therefore predicts the existence of two types of Dirac wave function in the Universe:
left-handed functions with chirality −1 and right-handed functions with chirality +1.
If we want to extract the left- and right-handed parts of a wave function we may
deﬁne projection operators via
ˆPL = 1−γ5
2
,
ˆPR = 1+γ5
2
,
(36.17)
which will project out the desired part.
We conclude that since ψL and ψR are eigenstates of the massless Dirac
equation, then a free and massless left-handed particle may never change
into a right-handed particle. Moreover, using the fact that, for massless
particles, the eigenvalue of ˆp0 is Ep = |p|, our eqns 36.15 give us
σ · ˆp
|p| ψR
=
ψR,
(36.18)
σ · ˆp
|p| ψL
=
−ψL.
(36.19)

326
The Dirac equation
This means that the massless states ψR and ψL, deﬁned as being chirality
eigenstates, are also eigenstates of the helicity operator
ˆh = σ · ˆp
|p| ,
(36.20)
with eigenvalues +1 and −1 respectively.
From this discussion it looks rather like chirality and helicity, while
being represented by diﬀerent operators, tell us the same thing. How-
ever, one needs to be careful here: helicity and chirality are identical
for massless particles, but are, in general, not the same. Clearly helic-
ity tells us whether the particle’s spin and momentum are parallel or
antiparallel. Helicity therefore depends on the frame in which it is mea-
sured. In the case of massive particles it is possible to describe a particle
with positive helicity and then to boost to a frame where the particle’s
momentum is reversed but its spin is unchanged, thereby reversing its
helicity.
If we now consider massive particles again then the equations of mo-
tion revert back to eqn 36.14. This shows us that massive Dirac particles
involve both left- and right-handed wave functions coupled by the par-
ticle’s mass. We can think of massive Dirac particles oscillating back
and forth in time between left- and right-handed at a rate determined
by their mass.
This is most easily seen by looking at massive Dirac
particles at rest, where we have i∂0ψR = mψL and i∂0ψL = mψR.
Example 36.3
We’ll ﬁnd the dispersion relation for massive Dirac particles. Assuming that ψL and
ψR are eigenstates of momentum and energy, we may eliminate ψL from eqns. 36.14,
to give us
(p0 + σ · p)(p0 −σ · p)ψR = m2ψR.
(36.21)
Multiplying this out gives
ˆ
(p0)2 −p2˜
= m2,
(36.22)
which demonstrates that Ep = ±(p2 +m2)
1
2 and we still have negative energy states
as claimed above. Another way of saying this is that the eigenvalue of p0 could be
taken to be positive or negative and we still get the same dispersion.
Finally we turn to antiparticle solutions of the Dirac equation. These are
the solutions which appear to have negative values of energy.8 Writing
8Of course we know that this is a bad
way of thinking about them since phys-
ical antiparticles have positive energy.
We apologize for the brief lapse into
bad habits.
energies as E = −|p0|, we ﬁnd, for massless antiparticles, that
(−|p0| −σ · ˆp)ψR = 0
(−|p0| + σ · ˆp)ψL = 0

antiparticles,
(36.23)
which implies
σ·ˆp
|p| ψR = −ψR
σ·ˆp
|p| ψL = ψL
)
antiparticles.
(36.24)
This is to say that right-handed antiparticles have negative helicity and
left-handed antiparticles have positive helicity.
Note that this is the
other way round compared to the particle case.

36.3
Dirac and Weyl spinors
327
To summarize our progress so far. We’ve discovered that the four-
component solutions to the Dirac wave function have left-handed parts
in the upper two slots and right-handed parts in the lower two slots and
can be written
ψ(x) =
 ψL(x)
ψR(x)

.
(36.25)
The four-component wave functions are known as Dirac spinors and
are quite diﬀerent to other objects that we have so far encountered
such as scalars and four-vectors. The two-component objects ψL(x) and
ψR(x) are themselves known as Weyl spinors. Spinors are best deﬁned
Hermann Weyl (1885–1955)
in terms of their transformation properties, which we turn to in the next
chapter. In the meantime we will examine them further as solutions to
Dirac’s equation.
36.3
Dirac and Weyl spinors
We now investigate what’s hidden away in ψL(x) and ψR(x). Let’s work
on the positive and negative energy solutions separately. Since physical
particles and antiparticles really have positive energies, we’ll conform to
convention and call these the positive and negative frequency solutions.
The positive frequency solutions will describe Fermi particles, while the
negative frequency solutions will describe Fermi antiparticles. It’s easy
to show that a set of solutions to the Dirac equation may be written
Particles: u(p)e−ip·x =

uL(p)
uR(p)

e−ip·x,
Antiparticles: v(p)eip·x =

vL(p)
vR(p)

eip·x.
(36.26)
Note that the negative frequency solutions are deﬁned such that p0 =
E > 0 so that antiparticles will have positive energies.
The four-
component objects9 u(p) and v(p) are momentum space Dirac spinors
9Just as a polarization vector ǫµ(p) car-
ries the spin information for the pho-
ton, the spinors u(p) and v(p) carry the
spin information of the fermion.
and themselves satisfy the momentum space Dirac equation:
(✁✁ˆp −m)u(p) = 0,
(−✁✁ˆp −m)v(p) = 0.
(36.27)
The left-handed [uL(p) and vL(p)] and right-handed [uR(p) and vR(p)]
parts are momentum space Weyl spinors.
We start by considering the particle solution at rest where pµ = (m, 0).
In this case the Dirac eqn 36.27 becomes
 −m
m
m
−m
  uL(p0)
uR(p0)

= 0,
(36.28)
with solutions
u(p0) ≡
 uL(p0)
uR(p0)

= √m
 ξ
ξ

,
(36.29)

328
The Dirac equation
where ξ =
 ξ1
ξ2

is a two-component column vector that we will choose
to normalize10 according to ξ†ξ = 1. Note that ξ is also often called a
10The
factor
√m
is
included
in
eqn 36.29 for later convenience. It re-
sults in ¯uu = 2m where ¯u ≡u†γ0 is
deﬁned later (eqn 36.41).
For mass-
less particles one includes a factor
p
Ep
instead.
See Peskin and Schroeder,
Chapter 3, for more details.
spinor. The argument may be repeated for antiparticle solutions with
the result
−

m
m
m
m
 
vL(p0)
vR(p0)

= 0,
(36.30)
leading to
v(p0) = √m

η
−η

,
(36.31)
where we write η =
 η1
η2

for the antiparticle spinor. Notice that the
solutions to the Dirac equation have two degrees of freedom. These are
the two components of ξ (for the positive frequency solutions) or η (for
the negative frequency solutions). It turns out that it is ξ and η that
tell us about the spin of the particle.
We know the form of the operator for the spin of a particle at rest
is given by ˆS = 1
2σ. The object on which this operator acts is η or ξ.
A particle with spin up along the z-axis therefore has ξ =
 1
0

and
one with spin down along the z-axis has ξ =
 0
1

. We can use these
states as a basis to represent arbitrary spin states.
What about antiparticles?
We deﬁne an antiparticle at rest with
physical spin up along the z-direction to have η =
 0
1

, so that ˆSzη =
−1
2η. An antiparticle with physical spin down along z has Sz = 1
2 and
η =

1
0

so that ˆSzη = 1
2η. Admittedly this is rather confusing! The
properties of particles and antiparticles are summarized in the table at
the end of the chapter.
Example 36.4
A positive helicity antiparticle with momentum directed along the positive z-axis
must have ση = +η. It must also, therefore, have η =
„ 1
0
«
, which corresponds to
physical spin down.
What about the spinor for a particle in the general case of particles that
aren’t at rest? We quote the result here (and will justify it in the next
chapter). For a particle or antiparticle with momentum pµ the spinors
are given by
u(p) =
 √p · σ ξ
√p · ¯σ ξ

,
v(p) =

√p · σ η
−√p · ¯σ η

.
(36.32)
In eqn 36.32 we are again using the no-
tation σ = (I, σ) and ¯σ = (I, −σ) and
we understand that in taking the square
root of a matrix we take the positive
root of each eigenvalue.

36.3
Dirac and Weyl spinors
329
Example 36.5
We will evaluate the spinor for a particle with pµ = (Ep, 0, 0, |p|) and ξ =
„ 0
1
«
.
Evaluating the left-handed part uL(p) = √p · σξ which lives in the upper two slots
of the spinor, we have
√p · σ =
„ Ep −|p|
0
0
Ep + |p|
« 1
2 „ 0
1
«
=
„ p
Ep −|p|
0
0
p
Ep + |p|
« „ 0
1
«
,
(36.33)
which gives uL(p) = √p · σξ =
p
Ep + |p|
„ 0
1
«
.
The analogous result for the right-handed, lower slots of u(p) follows by a similar
argument with the result that uR(p) = √p · ¯σξ =
p
Ep −|p|
„ 0
1
«
.
We obtain the ﬁnal result for the spinor
u(p) =
0
B
B
@
p
Ep + |p|
„ 0
1
«
p
Ep −|p|
„ 0
1
«
1
C
C
A .
(36.34)
The general expression for the spinors in eqn 36.32 allows us to explore
the helicity and chirality of particles and antiparticles in more detail in
the next example.
Example 36.6
We will look at particles and antiparticles in the ultra-relativistic limit, obtained by
giving the particles a big boost. Consider a particle with spin up along the z-axis and
momentum |p| along +z. This particle, shown in Fig. 36.1(a), has helicity h = 1,
corresponding to physical spin parallel to the momentum. This is described by a
spinor
u(p) =
„ uL(p)
uR(p)
«
=
0
B
B
@
p
Ep −p · σ
„ 1
0
«
p
Ep + p · σ
„ 1
0
«
1
C
C
A .
(36.35)
This leads us to
u(p) =
0
B
B
@
p
Ep −|p|
„ 1
0
«
p
Ep + |p|
„ 1
0
«
1
C
C
A
big
−−−−→
boost
p
2Ep
0
B
B
@
0
0
1
0
1
C
C
A ,
(36.36)
which has uL = 0 and uR =
p
2Ep
„ 1
0
«
.
e−
p
(a)
e−
p
(b)
e+
p
(c)
e+
p
(d)
Fig. 36.1
Helicity states of particles
and antiparticles.
(a) A highly rela-
tivistic electron with spin up along the
z-axis and momentum along z, which
has h = 1.
(b) The same with spin
down, giving helicity h = −1.
(c) A
highly relativistic positron with phys-
ical spin up along the z-axis and mo-
mentum along z, giving a helicity h =
−1 (d) The same with physical spin
down along z and h = 1.
We can carry out the same analysis for a negative helicity particle [Fig. 36.1(b)],
which has spin down along the z-axis and momentum along +z. This has a spinor
u(p) =
0
B
B
@
√E −p · σ
„ 0
1
«
√E + p · σ
„ 0
1
«
1
C
C
A =
0
B
B
@
p
E + |p|
„ 0
1
«
p
E −|p|
„ 0
1
«
1
C
C
A
big
−−−−→
boost
p
2Ep
0
B
B
@
0
1
0
0
1
C
C
A ,
(36.37)
which has uL =
p
2Ep
„ 0
1
«
and uR = 0.

330
The Dirac equation
Now for the antiparticle states. Again we take momentum to be along +z. An
antiparticle with physical spin up along the z-axis [Fig. 36.1(c)] has
„ 0
1
«
. It has
a helicity of +1 (opposite to what a particle would have) and has a spinor
v(p) =
0
B
B
@
√E −p · σ
„ 0
1
«
−√E + p · σ
„ 0
1
«
1
C
C
A =
0
B
B
@
0
p
E + |p|
0
−
p
E −|p|
1
C
C
A
big
−−−−→
boost
p
2Ep
0
B
B
@
0
1
0
0
1
C
C
A .
(36.38)
This has vL =
p
2Ep
„ 0
1
«
and vR = 0.
Finally, an antiparticle with spin down along the z-axis and momentum along +z
[Fig. 36.1(d)] has helicity −1 and a spinor
v(p) =
0
B
B
@
√E −p · σ
„ 1
0
«
−√E + p · σ
„ 1
0
«
1
C
C
A =
0
B
B
B
B
@
p
E −|p|
0
−
p
E + |p|
0
1
C
C
C
C
A
big
−−−−→
boost
−
p
2Ep
0
B
B
@
0
0
1
0
1
C
C
A ,
(36.39)
which has vL = 0 and vR = −
p
2Ep
„ 1
0
«
.
Notice that highly relativistic particles with positive helicity have right-handed
spinors, while relativistic antiparticles with positive helicity have left-handed spinors.
Imagine that there was an interaction that only coupled to left-handed
particles. In the ultra-relativistic limit, where chirality and helicity be-
come identical, we’ve demonstrated that only negative helicity particles
and positive helicity antiparticles would interact. Amazingly, this is in-
deed the case for the weak interaction, which describes a ﬁeld Wµ(x)
which only couples to left-handed Dirac particles!11 This means that
11Neutrinos only exist as left-handed
particles.
They are (at least approx-
imately) massless, which means that
only negative helicity neutrinos and
positive helicity antineutrinos have ever
been observed.
only a left-handed fermion may emit a W ± particle. For a massive elec-
tron, which oscillates between being left- and right-handed, it may only
emit a W −particle (turning into a neutrino as it does so) when it is
left-handed as shown in Fig. 36.2.
t
R
L
R
L
R
L
R
L
R
L
W −
R
L
R
L
R
L
R
L
R
L
(a)
(b)
Fig. 36.2 Parity violation in the weak
interaction. The electron oscillates be-
tween left- and right-handed chiralities.
It may only emit a W −particle when
it is left-handed.
To summarize this section: the information content of a spinor tells us
about the spin of a particle through the object ξ (or η for an antiparticle)
which has two degrees of freedom. If we know ξ and the momentum of
the particle then all four components of the spinor (two left-handed parts
and two right-handed parts) are completely determined.
36.4
Basis states for superpositions
We turn to the orthogonality of the Dirac spinors. This property is useful
if we are to use the spinors as a basis in order to express arbitrary wave
functions as linear superpositions which contain spinor parts. Spinors
carry information about the spin of the particle, so we want one spinor
to be orthogonal to another if they describe diﬀerent spin states. (The
orthogonality of momentum states will be handled, as usual, by the
e−ip·x factors.)
Now, it’s straightforward to show12 that
12See Exercise 36.3.
u†(p)u(p) = 2Epξ†ξ,
v†(p)v(p) = 2Epη†η.
(36.40)

36.4
Basis states for superpositions
331
However this isn’t going to be very useful.
The reason is that these
expressions are not Lorentz invariant: the combinations u†u and v†v
are proportional to Ep, which is frame-dependent. To make a Lorentz
invariant quantity we deﬁne the adjoint or bar of a spinor as
¯u(p) = u†(p)γ0.
(36.41)
This is useful as a Lorentz invariant quantity can now be formed via
¯u(p)u(p) = 2mξ†ξ.
(36.42)
Now for the orthogonality: let’s also choose a basis in which to describe
the spin of a particle. We usually choose the basis ξ1 =
 1
0

and
ξ2 =

0
1

such that ξs†ξr = δsr. The result of all of this is that we
can now write an expression for a Dirac spinor which is suitable for use
as a basis state as
us(p) =
 √p · σξs
√p · ¯σξs

,
(36.43)
where s = 1 or 2 and we have
¯us(p)ur(p) = 2mξs†ξr = 2mδsr.
(36.44)
We are now able to express an arbitrary particle solution of the Dirac
equation as an integral over momentum and a sum over spin states as
follows:
ψ−(x) =
Z
d3p
(2π)
3
2
1
(2Ep)
1
2
2
X
s=1
aspus(p)e−ip·x,
(36.45)
where asp is an amplitude here.
In the case of the antiparticle spinor we pick up a minus sign:
¯vs(p)vr(p) = −2mηs†ηr = −2mδsr,
(36.46)
and the antiparticle wave function is written
ψ+(x) =
Z
d3p
(2π)
3
2
1
(2Ep)
1
2
2
X
s=1
b∗
spvs(p)eip·x,
(36.47)
where the components have amplitude b∗
sp. Finally we note that the u’s
and v’s are orthogonal:
¯ur(p)vs(p) = ¯vr(p)us(p) = 0.
(36.48)
We summarize all of our deﬁnitions in the master table below.
Spinor
physical
physical
normalization
spin up
spin down
particle
 √p · σξ
√p · ¯σξ

 1
0

 0
1

¯us(p)ur(p) = 2mδsr
antiparticle

√p · ση
−√p · ¯ση

 0
1

 1
0

¯vs(p)vr(p) = −2mδsr

332
The Dirac equation
Example 36.7
One of the most useful expressions in subsequent calculations is the sum
2
X
s=1
us(p)¯us(p) =
X
s
„ √p · σξs
√p · ¯σξs
« `
ξs†√p · ¯σ
ξs†√p · σ
´
.
(36.49)
We use the fact that
2
X
s=1
ξsξs† =
„ 1
0
« ` 1
0 ´
+
„ 0
1
« ` 0
1 ´
=
„ 1
0
0
1
«
,
(36.50)
giving us a four-by-four matrix
2
X
s=1
us(p)¯us(p) =
„ √p · σ√p · ¯σ
√p · σ√p · σ
√p · ¯σ√p · ¯σ
√p · ¯σ√p · σ
«
=
„
m
p · σ
p · ¯σ
m
«
.
(36.51)
Finally we use our γ matrices to write this four-by-four object as
X
s
us(p)¯us(p) = γ · p + m.
(36.52)
It’s left as an exercise to prove the analogous expression for antiparticles
X
s
vs(p)¯vs(p) = γ · p −m.
(36.53)
So why care about these spin sums?
We’ll see a little later that when we ac-
tually descend from our ivory tower to
calculate something that people actu-
ally measure a certain amount of av-
eraging is going to be required.
This
is because the incoming and outgoing
polarizations of our particles are usu-
ally unknown.
In that case the usual
instruction is to average over initial
states and sum over ﬁnal states. This
will lead to sums over the spin states
where relaxations like P
s us(p)¯us(p) =
✁p + m come in very useful.
36.5
The non-relativistic limit of the Dirac
equation
It is often stated that the Dirac equation is required to describe quantum
mechanical spin. This isn’t quite true as there’s another equation that
does this job perfectly well at low energy: the Pauli equation given by
ˆHψ = (σ · ˆp)2
2m
ψ.
(36.54)
The Pauli equation emerges naturally as the non-relativistic limit of the
Dirac equation.
Example 36.8
As in Chapter 12, we extract the non-relativistic limit by factorizing out the large
mass contribution through a wave function ψL(t, x) = φL(t, x)e−imt. From Dirac’s
equation (✁ˆp −m)ψ = 0 we obtain
„
−m
ˆE −σ · ˆp
ˆE + σ · ˆp
−m
« „ φL(t, x)e−imt
φR(t, x)e−imt
«
= 0,
(36.55)
where ˆE = i∂0. Noting that ˆEφa(t, x)e−imt = e−imt(m + ˆE)φa(t, x) (where a = L
or R) we obtain
−mφL + (m + ˆE −σ · ˆp)φR
=
0,
(m + ˆE + σ · ˆp)φL −mφR
=
0.
(36.56)

36.5
The non-relativistic limit of the Dirac equation
333
Rewriting the second of these equations we produce
φR =
 
1 +
ˆE
m + σ · ˆp
m
!
φL,
(36.57)
allowing us to eliminate φR from the ﬁrst equation, so that
 
2 ˆE +
ˆE2
m −(σ · ˆp)2
m
!
φL = 0.
(36.58)
Recognizing that the ˆE2 term may be neglected in comparison with the other terms
at low energy, we end up with Pauli’s equation
ˆEφL = (σ · ˆp)2
2m
φL.
(36.59)
The same equation applies for φR too, as can easily be shown.13
13See Exercise 36.8.
Example 36.9
The previous example gives us enough ammunition to show that the Pauli equation
(and by extension, the Dirac equation) implies that the g-factor of the electron is
g = 2.
We may include the electromagnetic interaction in the Pauli equation by
making the replacements14 ˆp →ˆp −qA and ˆE →ˆE −qA0. The result is
14This is equivalent to the replacement
∂µ →Dµ that we examined in Chap-
ter 14.
( ˆE −qA0)φ = [σ · (ˆp −qA)]2
2m
φ.
(36.60)
Multiplying out the right-hand side and using the identity σiσj = δij + iεijkσk we
ﬁnd15
15This is made slightly tricky by the
fact that ˆp and A don’t commute. If
in doubt, write ˆp = −i∇and remem-
ber that the diﬀerential operator acts
on everything to its right.
( ˆE −qA0)φ = (ˆp −qA)2
2m
φ −
q
2m σ · Bφ,
(36.61)
where we have identiﬁed B = ∇× A. The expected interaction Hamiltonian of a
magnetic moment with an external B-ﬁeld may be written
ˆH = −ˆµ · B ≡−g q
2m
ˆS · B.
(36.62)
Comparing this with −q
2mB · σ from eqn 36.61 and identifying the spin operator as
ˆS = 1
2 σ we conclude that g = 2.
The prediction that g = 2 (rather than g = 1) was historically an important one
in conﬁrming the validity of the Dirac equation. However, it is found experimentally
that g = 2.002319304.... The reason for this discrepancy is that it is not appropriate
to treat Dirac’s equation as the equation of motion for a single particle as we have
done here. In fact, the Dirac equation is correctly interpreted as providing the equa-
tion of motion for fermionic quantum ﬁelds. As we shall see in Chapter 41 this will
lead to a prediction that agrees with experiment to a truly remarkable extent.
In this chapter we’ve encountered a new object, the spinor, to add to
the scalar and vector objects that we’ve considered before. We have,
however, treated the Dirac equation as a single-particle equation. We
know that in reality particles are excitations in quantum ﬁelds. In order
to be able to deﬁne spinor ﬁelds we need to know the all-important
transformation properties of these spinor ﬁelds. In the next chapter we
turn to the Lorentz transformation properties of these objects.

334
The Dirac equation
Chapter summary
• The Dirac equation (✁✁ˆp −m)ψ = 0 results from an attempt to take
the square root of the Klein–Gordon equation. The four compo-
nents are required to represent spin and reﬂect symmetry under
the parity operation.
• The operator ‘ˆp-slash’ is ✁✁ˆp = γµ(i∂µ) where γµ are the gamma
matrices. In the chiral representation γµ =

0
σµ
¯σµ
0

where
σµ = (I, σ), ¯σµ = (I, −σ) and γ5 = iγ0γ1γ2γ3.
• Massless solutions of the Dirac equation are also the eigenstates of
the chirality operator, and are known as left- and right-handed so-
lutions. Left- or right-handed states with mass are not eigenstates
of the Dirac equation because the mass term couples them. We
can think of Dirac particles as oscillating in time between being
left- and right-handed.
• There are positive and negative energy eigenstates of the Dirac
equation. These give rise to particles and antiparticles.
• The non-relativistic limit of the Dirac equation yields the Pauli
equation and predicts g = 2.
Exercises
(36.1) An illustration of the reason for anticommutation
and spin
(a) Show that the Dirac equation can be recast in
the form
i∂ψ
∂t = ˆHDψ,
(36.63)
where ˆHD = α · ˆp + βm and ﬁnd α and β in terms
of the γ matrices.
(b) Evaluate ˆH2
D and show that for a Klein–Gordon
dispersion to result we must have:
(i) that the αi and β objects all anticommute with
each other; and
(ii) (αi)2 = (β)2 = 1.
This provides some justiﬁcation for the anticommu-
tation relations we imposed on the γs.
(c) Prove the following commutation relations
(i)
h
ˆH, ˆLii
= i(ˆp × α)i where ˆL = ˆx × ˆp.
(ii)
h
ˆH, ˆSii
= −i(ˆp × α)i where ˆS =
1
2Σ and we
deﬁne Σ = i
2γ × γ.
This tells us that the Dirac Hamiltonian doesn’t
commute with the orbital angular momentum op-
erator ˆL, mandating the existence of another form
of angular momentum (i.e. the spin ˆS) so that ˆH
commutes with the sum ˆJ = ˆL + ˆS.
(36.2) (a) Using the facts that γi† = −γi and γ0† = γ0,
take the Hermitian conjugate of the Dirac equation
to show that we obtain the non-covariant form:
−i∂0ψ†γ0 + i∂iψ†γi −mψ† = 0.
(36.64)
(b) Show that to restore covariance we may multi-
ply through by γ0 which results in
i∂µ ¯ψγµ + m ¯ψ = 0.
(36.65)
(36.3) (a) Verify eqn 36.40.
(b) Verify eqn 36.42.
(c) Verify eqn 36.46.
(36.4) The γµ matrices and spinors don’t just have one
form. Any form that obeys the anticommutation

Exercises
335
relations will also work. Consider, for example, the
unitary transformation matrix
U =
1
√
2
„ 1
−1
1
1
«
.
(36.66)
Use this to transform the Dirac wave functions
ψ →ˆUψ and show that for the resulting wave func-
tion to satisfy the Dirac equation the γ matrices
have the explicit form
γ0 =
„ 1
0
0
−1
«
, γi =
„
0
σi
−σi
0
«
. (36.67)
This is known as the standard or Dirac–Pauli rep-
resentation.
(36.5) Show that in (1+1)-dimensional spacetime, the re-
quirement {γµ, γν} = 2gµν is satisﬁed by γ0 = σ2
and γ1 = iσ1.
(36.6) Prove the following identities:
(a)
˘
γµ, γ5¯
= 0.
(b) (γµ)† = γ0γµγ0.
(c) (γ5)† = γ5.
(d) (γ5)2 = 1.
(e) (γ5γµ)† = γ0γ5γµγ0.
(36.7) (a) Show that
i¯u(p′)σµν(p′ −p)νu(p)
(36.68)
= 1
2 ¯u(p′)
h
−γµ(✓✓
p′ −m) + (m −✁p)γµi
u(p),
where σµν = i
2 [γµ, γν] .
(b) Show γµ✁p = 2pµ −✁pγµ.
(c) Use these two results to prove the Gordon
identity
¯u(p′)γµu(p) = ¯u(p′)
»p′µ + pµ
2m
+ iσµνqν
2m
–
u(p),
(36.69)
where qµ = (p′ −p)µ. This identity will be used in
Chapter 41.
(36.8) Take the non-relativistic limit of the Dirac equation
and ﬁnd the equation of motion for φR.

37
How to transform a spinor
37.1 Spinors aren’t vectors
336
37.2 Rotating spinors
337
37.3 Boosting spinors
337
37.4 Why are there four compo-
nents in the Dirac equation?
339
Chapter summary
340
Exercises
340
‘Do you ever run across a fellow that even you can’t under-
stand?’
‘Yes,’ says he.
‘This will make a great reading for the boys down at the of-
ﬁce,’ says I. ‘Do you mind releasing to me who he is?’
‘Weyl,’ says he.
The interview came to a sudden end just then, for the doctor
pulled out his watch and I dodged and jumped for the door.
But he let loose a smile as we parted and I knew that all the
time he had been talking to me he was solving some problem
that no one else could touch. But if that fellow Professor
Weyl ever lectures in this town again I sure am going to take
a try at understanding him! A fellow ought to test his intel-
ligence once in a while.
Roundy interviews Professor Dirac (1929), Wisconsin State
Journal1
1TheWisconsin State Journal’s humor-
ous writer ‘Roundy’ began his article as
follows: I’ve been hearing about a fel-
low they have up at the U. this spring
– a mathematical physicist, or some-
thing, they call him – who is pushing
Sir Isaac Newton, Einstein and all the
others oﬀthe front page. So I thought
I better go up and interview him for
the beneﬁt of the State Journal readers,
same as I do all the other top notchers.
The article seems almost too good to be
true, and unfortunately it seems that
it is in fact a fake.
Joseph Coughlin
(nicknamed Roundy because of his ro-
tund appearance) certainly existed but,
as detailed in Graham Farmelo’s biog-
raphy of Dirac The Strangest Man, the
article is most probably a spoof. Dirac
however kept a copy of it.
The humble electron cannot be described by either the scalar or vector
ﬁeld theories that we have discussed thus far. The electron, and indeed
all spin- 1
2 fermions, are excitations in a spinor ﬁeld.
The diﬀerent
sorts of ﬁeld are classiﬁed by how they transform under rotations and
Lorentz transformations. We know how scalars transform: by deﬁnition
they don’t change. We also know how to Lorentz transform and rotate
vectors. In this chapter we discuss how to transform a spinor.
37.1
Spinors aren’t vectors
Recall that a general Lorentz transformation and rotation can be written
as a single matrix:
D(θ, φ) = e−iJ·θ+iK·φ,
(37.1)
where θ gives angle of rotation and φ gives the velocity boost.2 This
2Recall that we deﬁne tan φi = βi.
general transformation may be applied to scalars, vectors or tensors by
picking the appropriate form of the rotation generator J and boost gen-
erator K, subject to their all-important commutation relations.3 We’ve
3The commutation relations are
ˆ
Ji, Jj˜
=
iεijkJk,
ˆ
Ji, Kj˜
=
iεijkKk,
ˆ
Ki, Kj˜
=
−iεijkJk.
seen how rotations and boosts aﬀect scalars and vectors, but how do
they aﬀect spinors? We have a problem in that our previous explicit
expressions for K are only geared for vectors, which are objects whose
rotations are described by the group SO(3). We will see that spinors
are rotated by the elements of a diﬀerent group.

37.2
Rotating spinors
337
37.2
Rotating spinors
As Eugene Wigner proved, the rotations of spinors are generated by the
elements of the group SU(2). This is the group of two-by-two unitary
matrices with unit determinant. The generators are the Pauli spin matri-
ces whose commutation relations are [σi, σj] = 2iεijkσk. This comes as
no surprise since all of us learn at our mother’s knee that two-component
S = 1/2 spin states are acted on by an angular momentum operator
ˆ
J = σ
2 and the angular momentum operator is the generator of rota-
tions. The set of matrices that rotates two-component Weyl spinors is
therefore
D(θ) = e−i
2 σ·θ.
(37.2)
These operators rotate the left-handed Weyl spinors ψL(p) and the
right-handed Weyl spinors ψR(p) in exactly the same way, so the four-
component Dirac spinor is rotated by the matrix

ψL
ψR

→

e−i
2 σ·θ
0
0
e−i
2 σ·θ
 
ψL
ψR

.
(37.3)
Example 37.1
With this knowledge we can calculate the 2 × 2 matrix that rotates Weyl spinors
around a particular axis. We’ll try the rotation by an angle θ1 about the x-direction,
whose matrix is4
4Recall eqn 15.23.
D(θ1) = e−i
2 σ1θ1 = I cos θ1
2 −iσ1 sin θ1
2 .
(37.4)
To rotate four-component Dirac spinors we simply act on both the left- and right-
handed spinors with this 2 × 2 matrix.
37.3
Boosting spinors
Next we examine how to boost spinors by ﬁnding an explicit form for
the boost generators K. Since we know that spinors are rotated by the
Pauli matrices via J = 1
2σ and we know the commutation relations of
the generators K with themselves and J, we guess that K ∝σ. We
ﬁnd that two possible choices of proportionality constants satisfy the
commutation relations. It is easily checked that both
K = ±iσ
2
(37.5)
will work. It seems, therefore, that there must be two sorts of spinors:
those boosted by K = + iσ
2 and those boosted by K = −iσ
2 and the two
choices of sign correspond to two distinct representations of the group.
At rest, these two sorts of spinors should be indistinguishable. This is
indeed correct and the two sorts of spinors are exactly the left- and right-
handed Weyl spinors, ψL and ψR, that fell out of the Dirac equation in
the previous chapter.

338
How to transform a spinor
Example 37.2
To show that the Weyl spinors are transformed by diﬀerent representations of the
group we should be able to show that each representation obeys the Lie algebra of
the generators of SU(2), which is [Ci, Cj] = iεijkCk. This is possible if we deﬁne
J± = J ± iK
2
,
(37.6)
since then the commutation relations read
h
Ji
+, Jj
+
i
=
iεijkJk
+,
(37.7)
h
Ji
−, Jj
−
i
=
iεijkJk
−.
(37.8)
Our new generators J+ and J−now have independent Lie algebras and each sep-
arately generates the rotations and boosts for a type of Weyl spinor. The simplest
non-trivial case of this algebra corresponds to one or other of the new generators
being zero, which gives the rules
J = −iK
for
J+ = 0,
J = iK
for
J−= 0.
(37.9)
We then have generators J = σ/2, K = iσ/2 for the Weyl spinor ψL and generators
J = σ/2, K = −iσ/2 for the Weyl spinor ψR, leading to our ﬁnal answer for the
transformations appropriate for Dirac spinors:
„ ψL
ψR
«
→
„ DL(θ, φ)
0
0
DR(θ, φ)
« „ ψL
ψR
«
,
(37.10)
where
DL(θ, φ)
=
e
σ
2 ·(−iθ−φ),
DR(θ, φ)
=
e
σ
2 ·(−iθ+φ).
(37.11)
After all of this formalism, we’re left with the answer to our question of
how to Lorentz boost a Dirac spinor. The Dirac wave function, which is
made up of two spinors stacked on top of each other, is boosted according
to ψ →eiK·φψ, or

ψL
ψR

→

e−1
2 σ·φ
0
0
e
1
2 σ·φ
 
ψL
ψR

.
(37.12)
Example 37.3
We can get some practice by boosting the spinors along the z-direction.
„ ψL
ψR
«
→
 
e−1
2 σ3φ3
0
0
e
1
2 σ3φ3
! „ ψL
ψR
«
.
(37.13)
The elements of the boost matrix are given by
e± 1
2 σ3φ3
=
1 ± σ3φ3
2
+ 1
2!
„σ3φ3
2
«2
± . . .
=
I cosh(φ3/2) ± σ3 sinh(φ3/2).
(37.14)
So the boost is
„ ψL
ψR
«
→
 
I cosh φ3
2 −σ3 sinh φ3
2
0
0
I cosh φ3
2 + σ3 sinh φ3
2
! „ ψL
ψR
«
.
(37.15)

37.4
Why are there four components in the Dirac equation?
339
Reverting to 4 × 4 matrices we see that the components are boosted by a matrix
0
B
B
B
B
B
@
e−φ3
2
0
0
0
0
e
φ3
2
0
0
0
0
e
φ3
2
0
0
0
0
e−φ3
2
1
C
C
C
C
C
A
(37.16)
Because e±x = cosh x ± sinh x we can write e± φ3
2
=
p
cosh φ3 ± sinh φ3. Identifying
cosh φ3 = γ = E/m and sinh φ3 = βzγ = |p|/m we see that a boost along z is
enacted by a matrix
1
√m
0
B
B
@
p
E −|p|
0
0
0
0
p
E + |p|
0
0
0
0
p
E + |p|
0
0
0
0
p
E −|p|
1
C
C
A .
(37.17)
We found previously that the Dirac wave function for a particle at rest is given by
„ uL
uR
«
= √m
„ ξ
ξ
«
. Putting everything together we deduce that a boost in the
z-direction has the following eﬀect
0
B
B
@
u1
L
u2
L
u1
R
u2
R
1
C
C
A = √m
0
B
B
@
ξ1
ξ2
ξ1
ξ2
1
C
C
A →
0
B
B
@
p
E −|p| ξ1
p
E + |p| ξ2
p
E + |p| ξ1
p
E −|p| ξ2
1
C
C
A .
(37.18)
This provides some justiﬁcation for the expression
u(p) =
„ √p · σξ
√p · ¯σξ
«
,
(37.19)
as we claimed in the previous chapter.
37.4
Why are there four components in
the Dirac equation?
The Dirac equation is written in terms of a four-component Dirac spinor
ψ. These Dirac spinors have two degrees of freedom: the two components
of ξ (or η). So why do we have four components in the Dirac equation?
We found that for the case of massless particles the Dirac spinors
fell apart into two: left-handed Weyl spinors and right-handed Weyl
spinors. Each of these spinors carried the same spin information in ξ (or
η). The question is, then, why does the world need left- and right-handed
ﬁelds. We will answer this question by asking what fundamental opera-
tion turns left-handed spinors into right-handed spinors? The answer is
parity.
Many of the properties of fermions are invariant with respect to parity
and so we require that Dirac particles have the same properties in a
parity reversed world.
(The weak interaction violates parity, but we
ignore that for now.)
The parity operation P does the following: it
maps x →−x and v →−v and so it results in the changes to the
generators:
P−1KP = −K,
P−1JP = +J
(37.20)

340
How to transform a spinor
(recall that we say that K is a vector and J is a pseudovector). The
parity operation therefore turns a left-handed spinor into a right-handed
spinor
PψL →ψR,
PψR →ψL.
(37.21)
An explicit form for the parity operator, suitable for use on Dirac spinors,
is P ≡γ0 =
 0
I
I
0

.
As we have stressed, the Dirac wave functions are not actually single-
particle wave functions: they represent ﬁelds. In the next chapter we
will quantize these ﬁelds to make them operator-valued, so that when we
input a position in spacetime we output a ﬁeld operator. Dirac particles
are excitations in these quantum ﬁelds.
Chapter summary
• We have derived the transformations for rotating and boosting
spinors.
• Parity, enacted with the γ0 matrix, swaps left- and right-handed
parts of the wave function.
Exercises
(37.1) (a) Verify eqn 37.4.
(b) Show that the rotation matrix
D(θ3) = e−i
2 σ3θ3 =
 
e−iθ3
2
0
0
e
iθ3
2
!
.
(37.22)
(37.2) Show that both choices of sign K = ± iσ
2 obey the
necessary commutation relations.
(37.3) A one-line derivation of the Dirac equation.
(a) Given that the left- and right-handed parts of
the Dirac spinor for a fermion at rest are identical,
explain why we may write (γ0 −1)u(p0) = 0.
(b) Prove that eiK·φγ0e−iK·φ = ✁p/m.
(c) Use the result proven in (b) to boost
(γ0 −1)u(p0) = 0,
and show that you recover the Dirac equation.

38
The quantum Dirac ﬁeld
38.1 Canonical quantization and
Noether current
341
38.2 The fermion propagator 343
38.3 Feynman rules and scatter-
ing
345
38.4 Local symmetry and a gauge
theory for fermions
346
Chapter summary
347
Exercises
347
After examining the Dirac equation as an equation of motion for single
particles and investigating the transformation properties of the spinor
solutions we now turn to quantum ﬁelds that obey Dirac’s equation as
their equation of motion. We will approach the problem with a variety
of the weapons we have developed in previous chapters. We start with
canonical quantization. We will write down a Lagrangian density whose
equations of motion are the Dirac equation. We will then quantize using
the rules. The quantized ﬁeld that we will produce is of fundamental
importance since its excitations describe fermions such as the quarks
and leptons of particle physics and various excitations in solids such as
quasielectrons in metals. In order to carry out the sort of calculations
that can make contact with experiments, such as scattering, we will need
a propagator and Feynman rules. We will obtain the propagator with the
path integral. Finally the properties of Noether’s current will motivate
us to write down the Lagrangian for perhaps the most successful of all
quantum ﬁeld theories: quantum electrodynamics.
38.1
Canonical quantization and Noether
current
To quantize a ﬁeld we start by writing down a Lagrangian density de-
scribing classical ﬁelds. It seems rather wrong-headed to write down a
classical equation for a ﬁeld that describes fermions. Although we can
make a model of bosonic excitations like phonons out of masses and
springs there is no such obvious classical analogue for fermions. Un-
daunted, we will write down the Lagrangian by choosing a form that
when inserted into the Euler–Lagrange equation outputs Dirac’s equa-
tion. The Lagrangian that leads to the Dirac equation is given by
L = ¯ψ(i✁∂−m)ψ = ¯ψ(iγµ∂µ −m)ψ,
(38.1)
where ¯ψ = ψ†γ0. Just as free scalar ﬁeld theory describes ﬁelds whose
equation of motion is the Klein–Gordon equation, this Lagrangian de-
scribes fermion matter ﬁelds whose equation of motion is the Dirac equa-
tion.
We can use this Lagrangian to canonically quantize the Dirac ﬁeld.
This ﬁeld has two components, just like the complex scalar ﬁeld.
The momentum conjugate to ψ is easily calculated using the ordinary

342
The quantum Dirac ﬁeld
method. We ﬁnd that
Πµ
ψ =
δL
δ(∂µψ) = i ¯ψγµ,
(38.2)
and Πµ
¯
ψ = 0. We obtain a momentum ﬁeld
Π0
ψ = i ¯ψγ0 = iψ†.
(38.3)
The Hamiltonian follows in the usual fashion from H = Π0
ψ∂0ψ −L,
yielding
H = ψ†(−iγ0γ · ∇+ mγ0)ψ.
(38.4)
However, we also know another piece of information, namely the Dirac
equation itself which simpliﬁes things here. The Dirac equation tells us
that iγ0∂0ψ = (−iγ·∇+m)ψ. This allows us to rewrite the Hamiltonian
as
H = ψ†i∂0ψ.
(38.5)
This looks much simpler! Next we make the ﬁelds quantum mechanical
by imposing equal-time anticommutation relations:1
1We also put hats on ψ and ψ† to make
the point that they are now operators.
{ ˆψa(t, x), ˆψ†
b(t, y)} = δ(3)(x −y)δab,
(38.6)
{ ˆψa(t, x), ˆψb(t, y)} = { ˆψ†
a(t, x), ˆψ†
b(t, y)} = 0,
(38.7)
where a and b label the spinor components of the four-component ﬁeld
operators ˆψ(x). The mode expansions of these quantum ﬁelds are given
by
ˆψ(x)
=
Z
d3p
(2π)
3
2
1
(2Ep)
1
2
2
X
s=1

us(p)ˆaspe−ip·x + vs(p)ˆb†
speip·x
,
ˆ¯ψ(x)
=
Z
d3p
(2π)
3
2
1
(2Ep)
1
2
2
X
s=1

¯us(p)ˆa†
speip·x + ¯vs(p)ˆbspe−ip·x
. (38.8)
The creation and annihilation operators must themselves obey the anti-
commutation relations
{ˆasp, ˆa†
rq} = {ˆbsp,ˆb†
rq} = δ(3)(p −q)δsr.
(38.9)
Now it simply remains to insert the mode expansion into the Hamilto-
nian and normal order to obtain a quantized ﬁeld theory.
Example 38.1
We obtain
ˆH
=
Z
d3x
d3pd3q
(2π)3(2Ep2Eq)
1
2
X
s,r
“
¯us(p)ˆa†
speip·x + ¯vs(p)ˆbspe−ip·x”
×γ0Eq
“
ur(q)ˆarqe−iq·x −vr(q)ˆb†
rqeiq·x”
=
X
s,r
Z
d3p
2
“
us†(p)ˆa†
spus(p)ˆasp −vs†(p)ˆbspvs(p)ˆb†
sp
”
.
(38.10)
Lastly, we can use the properties of the normalized spinors to do the spin sum, since
we know that us†ur = 2Epδsr.

38.2
The fermion propagator
343
The (normal ordered) Hamiltonian becomes
ˆH =
Z
d3p
2
X
s=1
Ep

ˆa†
spˆasp + ˆb†
spˆbsp

.
(38.11)
We see that the energy is given by the sum of the energies of the particles
and antiparticles, exactly as we should expect.
The Lagrangian also has a global U(1) symmetry, meaning that the
Lagrangian is invariant with respect to the change ψ(x) →ψ(x)eiα, so
we can use Noether’s theorem to ﬁnd the conserved current and charge.
Example 38.2
With ψ →ψeiα we also have ¯ψ →¯ψe−iα. Since the transformation is global we also
can write ∂µψ →eiα∂µψ. Putting this all together we get
L
=
¯ψ(i✁∂−m)ψ
→
¯ψe−iα(i✁∂−m)ψeiα = ¯ψ(i✁∂−m)ψ.
(38.12)
The use of Noether’s theorem goes through similarly to the case for the
complex scalar ﬁeld, except that we have
Πµ
ψ = ¯ψiγµ,
Πµ
¯
ψ = 0,
(38.13)
and Dψ = iψ. So, upon normal ordering and swapping signs to give
the conventional number current direction, we obtain a Noether current
operator
ˆJµ
Nc = ˆ¯ψγµ ˆψ.
(38.14)
We can use this to work out the conserved U(1) charge which, upon
substitution of the mode expansion, yields up
ˆQNc =
Z
d3p
X
s

ˆa†
spˆasp −ˆb†
spˆbsp

.
(38.15)
The conserved charge is given by the number of particles minus the
number of antiparticles, as expected. Once again, this is just like the
complex scalar ﬁeld case.
Noether’s theorem: U(1) internal
symmetry
Dψ = iψ
Dψ† = −iψ†
Πµ
ψ = i ¯ψγµ
Πµ
¯
ψ = 0
DL = 0
W µ = 0
J0
Nc = ψ†ψ
Jµ
Nc = ¯ψγµψ
ˆQNc =
R
d3p P2
s=1(ˆn(a)
sp −ˆn(b)
sp )
38.2
The fermion propagator
The propagator for fermions can be worked out by calculating G0(x, y) =
⟨0|T ˆψ(x) ˆ¯ψ(y)|0⟩.
However, we can also use the path integral.
For
See Exercise 38.4.
fermions we treat the classical ﬁelds ψ and ¯ψ that appear in the path
integral as Grassmann numbers.2 In terms of the ﬁelds ψ and ¯ψ, the
2See Chapter 28 for an introduction to
Grassmann numbers.
generating functional is written
Z[η, ¯η] =
Z
DψD ¯ψ e
i
R
d4x
h
¯
ψ(x)
“
i✁∂−m
”
ψ(x)+¯η(x)ψ(x)+ ¯
ψ(x)η(x)
i
,
(38.16)

344
The quantum Dirac ﬁeld
where we notice that both ψ(x) and ¯ψ(x) must be coupled to separate
source ﬁelds ¯η(x) and η(x) respectively. The leg-work for this calculation
was carried out in Chapter 28 and so we note that the result is
Z[¯η, η] = Ce−i
R
d4xd4y ¯η(x)(i✁∂−m)−1η(y),
(38.17)
where C is proportional to a determinant. As usual, we normalize by di-
viding through by Z[0, 0] to obtain the normalized generating functional
Z[¯η, η] = e−
R
d4xd4y ¯η(x)iS(x−y)η(y),
(38.18)
where S(x −y) = (i✁∂−m)−1. The propagator is then the solution of
the equation
(i✁∂−m)iS(x) = iδ(4)(x).
(38.19)
The solution for the fermion propagator may be read oﬀfrom this3 as
3Note that there is a diﬀerence of a mi-
nus sign in the equations deﬁning the
scalar and fermion propagators in terms
of the Green’s function of the equations
of motion.
G0(x, y) = iS(x −y) =
Z
d4p
(2π)4
ie−ip·(x−y)
✁p −m + iǫ.
(38.20)
It’s important to note that the fermion propagator G0(x, y) is a four-
by-four matrix. This is revealed in momentum space if we multiply top
and bottom by (✁p + m) to give
˜G0(p)
=
i
✁p −m =
i(✁p + m)
p2 −m2 + iǫ
=
i
p2 −m2 + iǫ

m
p0 −p · σ
p0 + p · σ
m

,
(38.21)
where the latter equation is given in chiral representation.
An interpre-
Fig. 38.1 The fermion propagator in-
terpreted in terms of left- and right-
handed spinors.
tation of the four-by-four fermion propagator is revealed if we consider
the roles of the left- and right-handed parts of the Dirac spinor. The
fermion propagator G0 is formed from the combination ˆψ(x) ˆ¯ψ(y) which,
after substituting ψ =
 ψL
ψR

, has the form
 ψL(x)ψ†
R(y)
ψL(x)ψ†
L(y)
ψR(x)ψ†
R(y)
ψR(x)ψ†
L(y)

.
(38.22)
The top-left slot deals with a right-handed part entering and a left-
handed part leaving.
In contrast, the top-right slots tells us about
a left-handed part entering and a left-handed part leaving. Recalling
that a particle propagator is formed from a series G = G0 + G0V G0 +
G0V G0V G0 + G0V G0V G0V G0 + . . ., we may multiply the propagator
matrices and obtain the matrix represented in Fig. 38.1. We see that
the top-right slot still describes a left-handed part entering and a left-
handed part leaving, but now expresses this as a superposition of all of
the possible oscillations between left- and right-handed parts that result
in the left-handed part leaving. The other slots of the matrix represent
the other combinations of chiralities entering and leaving. In this sense

38.3
Feynman rules and scattering
345
we may think of a massive Dirac particle propagating while oscillating
between left- and right-handed.
With this in mind, our problem is solved since we have found the
propagator for free fermions. This enables us to start solving scattering
problems through the use of Feynman diagrams and it is the Feynman
rules for fermions that we turn to next.
38.3
Feynman rules and scattering
(a)
(b)
(c)
(d)
(e) p
p
p
p
= vs(p)
= ¯us(p)
= ¯vs(p)
= us(p)
=
i
✓✓p −m + iǫ
Fig. 38.2 Feynman rules for fermions.
The generating functional Z[¯η, η] can be used to ﬁnd the Feynman rules
for fermions. We won’t do this, but simply list the rules in momentum
space (see also Fig. 38.2).
Feynman rules for fermions
• A factor i/(✁p −m + iǫ) for each internal fermion line.
• A factor us(p) for an incoming fermion with momentum p and
spin s.
• A factor ¯vs(p) for an incoming antifermion with momentum p
and spin s.
• A ¯us(p) for an outgoing fermion with momentum p and spin s.
• A vs(p) for an outgoing fermion with momentum p and spin s.
• Trace over the matrix product arising from a fermion loop.
• Add minus sign factors reﬂecting fermion statistics, the most
common of which is a factor −1 for every fermion loop.
We will now get some practice with the use of these rules.
(a)
(b)
p
p′
k
k′
us(p)
ur(k)
¯us′(p′)
¯ur′(k′)
p
p′
k
k′
us(p)
ur(k)
¯us′(p′)
¯ur′(k′)
q = p′ −p
q = p′ −k
Fig.
38.3
(a) Electron–electron t-
channel
exchange.
(b)
Electron–
electron u-channel exchange.
Example 38.3
Let’s use our rules to work out some Feynman diagrams for a theory of scalar phions
interacting with electrons, via the interaction LI = −g ¯ψψφ. This is very much like
the example of the complex scalar ﬁeld interaction in Chapter 11, except that we
need to consider the complication of the spinor polarizations.
We can start with electron–electron scattering mediated by the t-channel exchange
of a virtual phion with mass mφ, as shown in Fig. 38.3(a). Applying the rules, we
obtain an amplitude
iM1 = (−ig)2¯us′(p′)us(p)
i
(p′ −p)2 −m2
φ
¯ur′(k′)ur(k),
(38.23)
where we have ignored an overall energy-momentum conserving delta function. Next
we consider electron–electron u-channel exchange scattering shown in Fig 38.3(b).
Swapping the lines over corresponds to swapping operators in the S-matrix element,
so this diagram comes with an extra minus sign. Applying the rules, we obtain
iM2 = −(−ig)2¯ur′(k′)us(p)
i
(p′ −k)2 −m2
φ
¯us′(p′)ur(k).
(38.24)
We’ll get a little further by considering the non-relativistic limit for distinguish-
able particles. This means that only the t-channel diagram contributes. For non-
relativistic particles, we have the approximate properties:
p ≈(me, p)
p′ ≈(me, p′)
k ≈(me, k)
k′ ≈(me, k′)
(38.25)

346
The quantum Dirac ﬁeld
and, for spinors, that us(p) = √me
„ ξs
ξs
«
. The spinor products become
¯us′(p)us(p) = 2meξs′†ξs = 2meδs′s,
(38.26)
showing that the scattering can’t change the spin. Putting these contributions to-
gether yields a scattering amplitude4
4This may be compared to the version
in Chapter 20, which only diﬀers in
normalization and the absence of spin
conserving Kronecker deltas. Here, as
before, the Yukawa potential is attrac-
tive.
In the next chapter we will re-
cover the expected result that the elec-
tromagnetic interaction scattering like
charges is repulsive.
iM =
4ig2m2
e
(p −p′)2 + m2
φ
δs′sδr′r.
(38.27)
We will see more examples of scattering in the next two chapters.
38.4
Local symmetry and a gauge theory
for fermions
Our ﬁnal job in this chapter will be to conjure up a gauge theory for
fermions.
Why do this?
We know that electrons interact with elec-
tromagnetic ﬁelds (since they carry electric charge), and so we there-
fore plan to extend the gauge ﬁeld Lagrangian of electromagnetism
L = −1
4FµνF µν to include fermions and an interaction term telling
us how photons and electrons interact.
The gauge principle that we
used in Chapter 14 allows us to do this since the act of promoting the
Dirac Lagrangian to a gauge theory by making it locally U(1) symmetric
will provide us with an interaction term through the minimal coupling
prescription.
As we’ve seen, the Dirac Lagrangian is invariant with respect to
global U(1) transformations, which is to say that the internal trans-
formation ψ →ψeiα leaves the Lagrangian invariant. The result of this
is a conserved number current JNc. To make a gauge theory we will
promote the global U(1) symmetry to a local symmetry exactly as we
have done before in Chapter 14. We simply replace our transformation
with ψ →eiα(x) and introduce a gauge ﬁeld via a covariant derivative
Dµ = ∂µ + iqAµ(x). In order for the gauge ﬁeld to ensure a local sym-
metry, it must itself transform as Aµ(x) →Aµ(x) −1
q∂µα(x).
The Dirac Lagrangian, ﬁxed up to have a local U(1) invariance, is
then given by5
5We write✚
D = γµDµ = γµ(∂µ+iqAµ).
L = ¯ψ(i✚
✚
D −m)ψ.
(38.28)
Inserting our covariant derivative, we have L = ¯ψ (iγµ[∂µ + iqAµ] −m) ψ
or
L = ¯ψ
 i✁∂−m

ψ −q ¯ψ Aψ.
(38.29)
Lo and behold, we are told how the gauge ﬁeld Aµ(x) interacts with
our fermion ﬁelds ψ(x) and ¯ψ(x): it’s via the interaction term LI =
−q ¯ψγµAµψ. This method of determining the form of the interaction
merely by considering the consequences of the substitution pµ →pµ −
qAµ is another example of the minimal coupling prescription we saw in
Chapter 14.6
6 It is also worth noting that the elec-
tromagnetic ﬁeld tensor F µν may be
generated by evaluating the commuta-
tor [Dµ, Dν] = iqF µν, as examined in
Exercise 46.3.

Exercises
347
In classical electromagnetism the conserved currents Jµ
em are the
sources of the electromagnetic ﬁelds. These ﬁelds interact with the elec-
tric charges q, telling them how to move. This is encoded in an inter-
action between the electromagnetic current density Jµ
em(x) and the ﬁeld
Aµ(x) giving a contribution to the Lagrangian LI = −Jµ
em(x)Aµ(x),
where Jµ
em is a conserved current obeying ∂µJµ
em = 0.
Our minimal
coupling prescription motivates us to make the step of identifying the
fermion Noether current Jµ
Nc(x) = ¯ψγµψ with the electromagnetic cur-
rent via Jµ
em = qJµ
Nc, where q is the electromagnetic charge. This con-
strains the source currents that may be coupled to the electromagnetic
ﬁeld and guarantees that the source current is a conserved quantity.
This philosophy is illustrated in Fig. 38.4.
Global
symmetry
Local
symmetry
Conserved
charge
upgrade
with
gauge
ﬁeld
covariant derivative
minimal coupling
Fig. 38.4
The minimal coupling pre-
scription in QED.
We end this chapter with a landmark equation. We will write down
perhaps the most successful theory in modern physics: Quantum elec-
trodynamics or QED. The QED Lagrangian includes the contributions
from the gauge ﬁeld of electromagnetism (eqn 5.49) and from the lo-
cally gauge invariant Dirac Lagrangian describing fermions and their
interactions (eqn 38.28) giving
L = −1
4FµνF µν + ¯ψ (iγµ∂µ −m) ψ −q ¯ψγµAµψ.
(38.30)
In the next chapter we will examine the predictions of this theory.
Chapter summary
• The Lagrangian that leads to the Dirac equation is L = ¯ψ(i✁∂−m)ψ
where ¯ψ = ψ†γ0 and can be quantized. The Noether current is
Jµ
Nc = ¯ψγµψ.
• The fermion propagator is i/(✁p −m).
• In order to have local gauge invariance, minimal coupling tells us
that we must add to L a term LI= −q ¯ψγµAµψ.
Exercises
(38.1) Try to canonically quantize the Dirac Lagrangian
using commutation relations rather than anticom-
mutation relations for the ﬁelds. You should get a
minus sign in front of the ˆb†
pˆbp term in the Hamil-
tonian. Explain why this is a catastrophe.
See Peskin and Schroeder, Chapter 3 for help.
(38.2) Verify eqn. (38.15).
(38.3) Show that the massless Dirac Lagrangian is invari-
ant with respect to the global chiral U(1) transfor-
mation ψ(x) →eiαγ5ψ(x). Show that the Noether
current is Jµ
Nc(x) = ¯ψ(x)γµγ5ψ(x).
(38.4) Calculate the free fermion propagator by inserting
the mode expansion into the equation G0(x, y) =
⟨0|T ˆψ(x) ˆ¯ψ(y)|0⟩. You will need to use the spin sum-
mation rules from Chapter 36.

39
A rough guide to quantum
electrodynamics
39.1 Quantum light and the pho-
ton propagator
348
39.2 Feynman
rules
and
a
ﬁrst
QED process
349
39.3 Gauge
invariance
in
QED
351
Chapter summary
353
Exercises
353
Quantum electrodynamics (QED) is the quantum ﬁeld theory that de-
scribes the interaction of light and matter. Arguably one of the greatest
intellectual achievements ever made, its discovery was due to several
people but notably Tomonaga, Schwinger and Feynman, who shared
Sin-Itiro Tomonaga (1906-1979).
the Nobel Prize in 1965.
Since QED describes an interaction it will come as little surprise that
this is not a solvable theory and we must rely on perturbation theory
and Feynman diagrams to understand the results of experiments. In this
chapter we take the view that the Feynman diagrams are the theory and
will therefore concentrate on ﬁnding the propagator for the photon and
the Feynman rules for the photon–fermion interaction. One complication
is that electrodynamics is a gauge invariant theory so we must expend
some eﬀort in ensuring that our dealings with the photon preserve this
invariance. We will ﬁnd that this can be done and, on the way, calculate
our ﬁrst amplitude in QED!
39.1
Quantum light and the photon
propagator
Our task is to ﬁnd the propagator for photons. We might guess that it’s
given by the m →0 limit of the propagator for the massive vector ﬁeld
we found1 in Chapter 24, which is to say
1See eqn 24.29.
Note that here we
follow the convention, introduced in
Chapter 17, that the free photon prop-
agator is called ˜D0µν(k).
˜D0µν(k) = lim
m→0
i(−gµν + kµkν/m2)
k2 −m2 + iǫ
.
(39.1)
However there’s a problem in taking this limit: the (longitudinally pro-
jecting) term kµkν/m2 threatens to blow up for photons on the mass
shell, which have k2 = m2 →0. We will show later that this disaster
does not occur and that gauge invariance allows the term kµkν/m2 to
be removed. Accepting this step for now, we are left with the photon
propagator, given by
˜D0µν(k) = −igµν
k2 + iǫ.
(39.2)
As usual the propagator describes the contribution from a virtual par-
ticle, in this case the photon.
By virtual we mean that the energy
Ek ̸= |k|, where k is the three-momentum of the photon.2
2All photons that we detect actually in-
teract with electrons in detectors such
as the eye. They must all then, in some
sense, be virtual!
How can this be?
We know that particles that are oﬀ-
shell have the range over which they
can propagate limited by the extent
to which they’re oﬀ-shell.
If we see
photons that have travelled from dis-
tant stars they have to be pretty close
to being on-shell.
We’ve seen before
that when a particle is on-shell we hit
the pole of the particle’s propagator.
Therefore photons from Andromeda,
visible on a moonless night, must be so
close to the pole that there can’t be any
observable eﬀects from being oﬀ-shell.

39.2
Feynman rules and a ﬁrst QED process
349
It’s worth checking that our propagator correctly predicts the mech-
anism through which photons mediate the interaction between charges.
Let’s consider the fate of two electric charges interacting via the ex-
change of a photon, as shown in Fig. 39.1. Since we’re interested in
the photons we’ll take the charges as forming electric currents Jµ. This
has the advantage that the fermion and vertex parts of the amplitude
are contained3 in the current Jµ, leaving us free to concentrate on the
3This assertion is justiﬁed later in the
chapter.
physics of the photon.
µ
ν
k
Jµa = q ¯ψaγµψa
Jνb = q ¯ψbγνψb
Fig. 39.1
The exchange of a virtual
photon.
We therefore consider the amplitude
A = Jµ
a
−igµν
k2

Jν
b ,
(39.3)
where we’ve left out the iǫ, since its presence won’t be important for
what follows.
Example 39.1
If we work in a frame where kµ = (k0, 0, 0, k3), then the amplitude looks like
A = −i
`
J0
a J0
b −Ja · Jb
´
(k0)2 −(k3)2
.
(39.4)
Current conservation implies that kµJµ = 0, so we have k0J0 −k3J3 = 0, which
allows us to eliminate the component J3 to yield
A = i J0
a J0
b
(k3)2 + i J1
a J1
b + J2
a J2
b
(k0)2 −(k3)2 .
(39.5)
Now for some interpretation.4 The ﬁrst term is in terms of J0, the charge density.
4Note that in the general case, where
kµ = (k0, k), the ﬁrst term in eqn 39.5
will be iJ0
a J0
b/k2.
If we (inverse) Fourier transform this quantity we obtain an instantaneously acting
Coulomb potential, which is repulsive between like charges
Z
d4k
(2π)4 e−ik·x J0
a J0
b
k2
∝
q2
4π|r| δ(ta −tb).
(39.6)
Don’t worry about the instantaneousness of the term. It only looks unphysically in-
stantaneous because we’ve split up the propagator in a non-covariant manner. More-
over, this is the term which dominates in the non-relativistic regime. This Coulomb
interaction is of course the basis of much of condensed matter physics.
We argued above that the photons we observe are those very close to the pole. For
our case of photons propagating along the z- (or 3-) direction, we look at the residue
of the second term and we see that there seem to be two sorts of photon: those that
couple J1 currents and those that couple J2 currents. These are the two physical
transverse photon polarizations.
39.2
Feynman rules and a ﬁrst QED
process
We now turn to the interaction of the photon with a fermion. We saw
in the previous chapter5 that the result is an interaction term which
5Remember that here, as elsewhere in
the book, q = Q|e|.
expressed in Hamiltonian form is
ˆHI = q ¯ˆψγµ ˆψ ˆAµ.
(39.7)

350
A rough guide to quantum electrodynamics
The diagrammatic version of this interaction is shown in Fig. 39.2(a).
This one simple vertex, whose translation in words is ‘fermions can emit
or absorb virtual photons’, governs the entire theory.
We now have the ingredients to state the Feynman rules for quantum
electrodynamics:
Feynman rules for QED
• Use all of the rules given previously for fermions.
• The interaction vertex contributes −iqγµ, where q(= Q|e|) is the
charge [Fig. 39.2(a)].
• Every internal photon line contributes ˜D0µν(k) = −igµν/(k2+iǫ)
[Fig. 39.2(b)].
• Incoming external photon lines contribute a polarization vector
ǫµλ(p) [Fig. 39.2(c)].
• Outgoing external photon lines contribute ǫ∗
µλ(p) [Fig. 39.2(d)].
(a)
(b)
(c)
(d)
p
p
k
p
p′
p′ −p
= ǫ∗
νλ(p)
= ǫµλ(p)
= −igµν
k2 + iǫ
= −iqγµ
ν
µ
ν
µ
µ
Fig. 39.2 The QED Feynman rules.
To get the feel of the rules, we will examine the case of e+e−→µ+µ−,
shown in the Feynman diagram in Fig. 39.3.
Using the Feynman rules
e−
e+
µ−
µ+
q
p
p′
k
k′
Fig.
39.3
A
ﬁrst
QED
process:
e+e−→µ+µ−.
for fermions and photons, we obtain an invariant amplitude of
iM = ¯vs′(p′)(−iQ|e|γµ)us(p)
−igµν
q2

¯ur(k)(−iQ|e|γν)vr′(k′), (39.8)
where we’ve written the charge as Q|e|. Note that the sign of the charge
doesn’t matter for many calculations and we will drop Q in what follows.
Example 39.2
We’ll get some more practice manipulating spinors in these calculations by consid-
ering this process in the ultra-relativistic limit. We imagine the kinematics for the
Our treatment here follows Peskin and
Schroeder.
process to be those depicted in Fig. 39.4. Remember that in the ultra-relativistic
limit particles in chirality eigenstates will simultaneously be in helicity eigenstates.
We’ll start with a right-handed electron with initial momentum along +z.
A
relativistic, right-handed electron always has helicity h = +1 and so must have a
spinor ξ =
„ 1
0
«
, corresponding to a physical spin-up along z.
We will collide
this electron with a left-handed positron with initial momentum along −z. A left-
handed, highly relativistic positron6 has h = +1, so its spinor is given by ξ =
„ 0
1
«
,
6Remember
that
relativistic,
left-
handed fermions have h = −1 while
left-handed antifermions have h = +1.
corresponding to physical spin-up along z. Taking the limit of very large momenta7
7See Chapter 36 if in doubt. We also
assume here that E = Ep ≈Ep′.
we have spinors for the incoming electrons given by
u(p) =
√
2E
0
B
B
@
0
0
1
0
1
C
C
A
v(p′) =
√
2E
0
B
B
@
0
0
0
−1
1
C
C
A .
(39.9)
To calculate our matrix element we need to evaluate products looking like ¯uγµu =
u†γ0γµu. It’s therefore useful to know that
γ0γµ =
„ 0
1
1
0
« „
0
σµ
¯σµ
0
«
=
„ ¯σµ
0
0
σµ
«
.
(39.10)

39.3
Gauge invariance in QED
351
Plugging in our spinors and using this result gives us the result for the incoming
interaction vertex that
¯v(p′)γµu(p) = 2E(0, 0, 0, −1)
„ ¯σµ
0
0
σµ
«
0
B
B
@
0
0
0
−1
1
C
C
A = −2E
0
B
B
@
0
1
i
0
1
C
C
A .
(39.11)
e−
µ+
θ
e+
µ−
x
z
p = (E, Eˆz)
p′ = (E, −Eˆz)
Fig. 39.4
Kinematics for our ﬁrst
QED process.
So far, so good. Next we need to evaluate the outgoing part describing the muons. A
useful insight here is that the quantity ¯v(p′)γµu(p) can be thought of as a four-vector
describing the spin and momenta of the incoming electron states. We will take the
inner product with a similar vector describing the outgoing muon states. We notice
that the muon vector ¯u(k)γνv(k′) is almost the same four-vector as ¯v(p′)γµu(p) which
we evaluated above for the incoming electrons. The diﬀerence is that it’s complex-
conjugated and rotated by an angle θ in the x-z plane. The complex conjugation is
made easy with the identity8 [¯u(p)γµu(k)]∗= ¯u(k)γµu(p) and we certainly know how
8See Exercise 39.3.
to rotate a four-vector. Conjugating and rotating the result in eqn 39.11 therefore
gives us
¯u(k)γµv(k′)
=
[¯v(k′)γµu(k)]∗
(Complex conjugation)
=
[−2E(0, cos θ, i, −sin θ]∗
(Rotation about y by angle θ)
=
−2E(0, cos θ, −i, −sin θ).
(39.12)
Putting the amplitude together via the dot product ¯vγµugµν ¯uγνv and including the
photon propagator yields up the invariant amplitude
iM = −i4e2E2
q2
(1 + cos θ).
(39.13)
Noting that q2 = (2E)2, we obtain the simple result that M(e−
Re+
L →µ−
Rµ+
L ) =
−e2(1 + cos θ).
This can be repeated for other combinations of chirality with the result that
M(e−
Re+
L →µ−
Rµ+
L ) = M(e−
L e+
R →µ−
L µ+
R) = −e2(1 + cos θ),
M(e−
Re+
L →µ−
L µ+
R) = M(e−
L e+
R →µ−
Rµ+
L ) = −e2(1 −cos θ),
(39.14)
and all other combinations yield an amplitude of zero.
39.3
Gauge invariance in QED
Gauge invariance
ψ(x) →eiα(x)ψ(x)
Aµ →Aµ −1
q∂µα
Local EM charge
conservation
∂µJµ
em = 0
Ward identity
kµMµ(k) = 0
Minimal
coupling
Noether’s
theorem
Feynman diagrams
Fig. 39.5 Gauge invariance, the Ward
identity and conservation of electro-
magnetic
current
are
all
intimately
linked.
kµ ·
= 0
µ
p
p′
k
S-matrix element
Fig. 39.6 The Ward identity. Dot the
vector kµ with the contribution to the
S-matrix shown and you get zero.
Gauge invariance is central to our notion of QED. Global U(1) symmetry
of the Dirac Lagrangian guarantees the conservation of fermion number.
Local U(1) symmetry [guaranteed by the addition of the electromagnetic
gauge ﬁeld Aµ(x)] allows us to identify the conserved fermion current
with the conserved electromagnetic current and ﬁxes the form of the
QED interaction. Gauge invariance therefore leads to the conservation
of electromagnetic current in QED. This is certainly central to the theory
and must be maintained in our Feynman diagrams. Gauge invariance is
indeed guaranteed in Feynman diagrams by the Ward identity. The
John Ward (1924–2000).
It has been
said that his advances were used by oth-
ers ‘often without knowing it, and gen-
erally without quoting him’ (M. Dun-
hill, The Merton Record, 1995).
symbiotic relationship between the concepts is shown in Fig. 39.5. The
Ward identity comes in many forms9 and the simpliﬁed one with which
9It is a special case of the Ward–
Takahashi identities.
See Peskin and
Schroeder for its derivation.
we’ll be concerned is shown diagrammatically in Fig. 39.6. It says that
if a sum of diagram parts contributing to an S-matrix element may be
written Mµ(k, p1, p2, ...), where µ labels the vertex to which a photon
line is attached, and that the photon line carries momentum k and the
external lines are on mass shell, then
kµMµ(k, p1, p2, ...) = 0.
(39.15)

352
A rough guide to quantum electrodynamics
This eﬀectively kills the term kµkν/k2 in the numerator of the photon
propagator, since it always combines kµ with the sort of vertex shown
in Fig. 39.6.
We began our discussion of the photon propagator by arguing that
the term kµkν/k2 disappears from the numerator of our propagator. We
conclude here that its disappearance is caused by gauge invariance and
is encoded in Feynman diagrams by the Ward identity. A diagrammatic
example of the Ward identity is examined in the exercises and a justiﬁ-
cation of the physics may be found in the next example.
Example 39.3
To see how gauge invariance leads to the disappearance of this term we naively
couple a general source Jµ (not the conserved current of electromagnetism) to a
massive vector ﬁeld Aµ. We will calculate the amplitude for starting with no vector
mesons and ending with one, represented by the half-dumbbell diagram shown in
Fig 39.7. We can write the amplitude for creating a massive vector meson with any
Fig. 39.7 The half-dumbbell
diagram.
polarization as
A ∝
X
λ
ǫ∗
λµ(k) ˜Jµ(k),
(39.16)
where the sum is over the polarizations. Assuming a basis of linear polarizations (i.e.
so ǫ∗
λµ = ǫλµ), this leads to a probability
P
=
|A|2 ∝
X
λ
˜Jµ(k) ˜Jν†(k)ǫλµ(k)ǫλν(k)
=
(−gµν + kµkν/m2) ˜Jµ(k) ˜Jν†(k),
(39.17)
where we’ve used the result for P
λ ǫλµ(k)ǫλν(k) = −P T
µν discussed in Chapter 13.
Next we ask what this tells us about the source emitting a photon, by attempting
to take a limit m →0.
We immediately see that the kµkν/m term blows up as
m →0, which is the same problem we had with trying to apply the massive vector
propagator to photons.
However, this won’t cause a problem if kµ ˜Jµ(k) = 0, since this kills the trouble-
some term. Notice that this is current conservation ∂µJµ = 0 written in momentum
space!
We conclude that the disappearance of the troublesome kµkν/m2 term is
due to electromagnetic current conservation, which is itself attributable to gauge
invariance.
k
p
p′
(p′ −k)
ℓ
p
ℓ′
p′
(ℓ′ −k)
(p′ −k)
k
q
p
k
p′
(p + k)
p
k
p′
(p + k)
ℓ
q
ℓ′
(ℓ−k)
µ
η
λ
ν
(a)
(b)
(c)
(d)
µ
ν
λ
η
Fig. 39.8
Illustration of gauge in-
variance in the diagrammatic language
used in Exercise 39.4.
Looked at in another way, the disappearance of the kµkν/k2 term allows
us to add any multiple of this quantity back into the propagator. That
is to say, vast helpings of a term that has no eﬀect on the physics may
be added for your convenience. A more general version of the photon
propagator can therefore be given by
˜D0µν(k) = −i
 gµν + (1 −ξ)kµkν/k2
k2 + iǫ
.
(39.18)
The simple choice ξ = 1 is known as Feynman gauge, while ξ = 0 is
known as Landau gauge. As examined in the exercises, exactly this prop-
agator follows from ﬁxing the gauge in which we work in the Lagrangian
itself. The freedom to choose ξ is therefore another manifestation of
gauge invariance.

Exercises
353
We have written down the Feynman rules for QED and examined a
ﬁrst process. In the next chapter we go further and look at the main (and
measurable) application of QED calculations: calculating the scattering
amplitudes for some fundamental processes.
Chapter summary
• The photon propagator has been derived and every internal photon
line contributes a factor −igµν/(k2+iǫ), though alternative choices
can be made in diﬀerent gauges.
• We have presented the Feynman rules for QED.
Exercises
(39.1) We will use the gauge ﬁxing technique to ﬁnd the
QED propagator.
(a) Show that the equations of motion for the
electromagnetic ﬁeld Aµ(x) written in momentum
space are given by
`
k2gµν −kµkν´ ˜Aµ(k) = 0.
(39.19)
(b) We will attempt to ﬁnd the inverse of these
equations of motion. Explain why this must be of
the form (M −1)νσ = C(k)gνσ + H(k)kνkσ.
(c)
Show
that
deﬁning
the
inverse
via
M µν(M −1)νσ = gµ
σ demands the impossible condi-
tion
−k2C(k)gµ
σ + C(k)kµkσ = gµ
σ.
(39.20)
(d) We may ﬁx this by adding a term to the La-
grangian that ensures we work in Lorentz gauge
∂µAµ = 0. We therefore add a gauge ﬁxing term
and obtain a Lagrangian
L = −1
4F µνFµν −1
2ξ (∂µAµ)2,
(39.21)
where ξ is an arbitrary parameter. The point here
is that the gauge ﬁxing term will force the theory
to be well behaved, but will contain the parameter
ξ which (we claim) will not enter into any measur-
able quantity. Show that the equations of motion,
written in momentum space, are now given by
„
−k2gµν + kµkν −1
ξ kµkν
«
˜Aν(k) = 0.
(39.22)
(e) Finally, verify that the photon propagator, de-
ﬁned as i(M −1)µν, is given by
˜D0µν(k) = −i
 
gµν −(1 −ξ) kµkν
k2
k2 + iǫ
!
.
(39.23)
(39.2) Consider the reaction e+e−→µ+µ−again. Most
experiments are done by ﬁring unpolarized beams of
electrons and positrons at each other. Since muon
detectors are usually not sensitive to muon polar-
ization we actually need to throw away the spin in-
formation. What we want is an average over initial
electron spin states and a sum over the ﬁnal muon
spin states.
(a) Explain why this is the right thing to do and
show that this prescription leads to a probability
1
2
X
s
1
2
X
s′
X
r
X
r′
|M(s, s′ →r, r′)|2.
(39.24)
(b) Show that we obtain a scattering proba-
bility for unpolarized electrons and muons of
1
4
P
spins |M|2 = e4(1 + cos2 θ).
We will revisit this next chapter.
(39.3) Verify [¯u(p)γµu(k)]∗= ¯u(k)γµu(p).
(39.4) We will examine how the term kµkν/k2 is killed
in a simple situation. Consider the Feynman dia-
gram shown in Fig. 39.8(a) which contributes, via
the S-matrix to the second-order correction to the
amplitude for fermion–fermion scattering.

354
A rough guide to quantum electrodynamics
(a) Using the Feynman rules write down the ampli-
tude for the whole process.
(b) Consider the bold part, shown in Fig. 39.8(b).
This features a virtual electron propagator, with a
photon line hitting it at the end labelled ν. Show
that the amplitude Apart for this part of the dia-
gram is
Apart
=
¯u(p′)(−ieγη)
„
i
✁p + ✁k −me
«
(−ieγν)
×
„−igµν + ikµkν/m2
γ
k2 + iǫ
«
u(p), (39.25)
where we’ve called the photon mass mγ and the
fermion mass me.
(c) Now just consider the dangerous part, which is
proportional to 1/m2
γ . Show that the guts of this
boil down to something proportional to
Aguts = ¯u(p′)γη kµkν/m2
γ
✁p + ✁k −me
γνu(p).
(39.26)
(d) Show that this may be further simpliﬁed to
Aguts =
1
m2γ
¯u(p′)γηkµu(p)
k2 + iǫ
.
(39.27)
Hint: rewrite the denominator as ✁k = (✁p + ✁k −
me)−(✁p−me) and use Dirac’s equation in the form
✁pu(p) = meu(p).
(e) Now turn to the process shown in Fig. 39.8(c)
which it is also necessary to consider when calcu-
lating the second-order correction to the fermion–
fermion scattering amplitude.
Just consider the
part shown in Fig 39.8(d) (and note the change in
labelling).
(f) Show that the dangerous guts of this part are
given by
Bguts = ¯u(p′)γν kµkν/m2
γ
✁p′ −✁k −me
γηu(p),
(39.28)
and that this may be reduced to
Bguts = −1
m2γ
¯u(p′)kµγηu(p)
k2 + iǫ
.
(39.29)
The point, then, is that we must consider both of
these processes and the sum of the dangerous guts
Aguts + Bguts = 0, which means that the dangerous
kµkν/m2
γ term disappears.
This argument is discussed in more depth in Zee,
Chapter II.7.

40
QED scattering: three
famous cross-sections
40.1 Example 1: Rutherford scat-
tering
355
40.2 Example 2:
Spin sums and
the Mott formula
356
40.3 Example 3:
Compton scat-
tering
357
40.4 Crossing symmetry
358
Chapter summary
359
Exercises
359
In this chapter we examine three famous scattering examples in QED: (1)
Rutherford scattering, which led to the discovery of the atom; (2) Mott
scattering, which is the relativistic version of Rutherford scattering and
(3) Compton scattering, which demonstrates the particle-like properties
of light. These examples will introduce a number of useful tricks which
are employed freely in more advanced applications.
40.1
Example 1: Rutherford scattering
i|e|γµ ˜Aµ
cl(q)
˜Aµ
cl(q)
us(p)
¯us′(p′)
p′
p
−iQ|e|γµ
(a)
(b)
Fig. 40.1
(a) Scattering from a clas-
sical potential. (b) The Feynman rule
for a Q = −1 electron to scatter from
the potential.
We’ll examine the scattering of an electron from a heavy, charged object
such as a point-like nucleus. This scattering of distinguishable particles
is described by a Feynman diagram shown in Fig. 40.1(a). The nucleus
is very heavy compared to the electron, so we treat the the process as
involving an electron passing through a classical potential Aµ
cl(x). The
Feynman rule for the vertex describing the electron interacting with
the potential is −iQ|e|γµ ˜Aµ
cl(q) [Fig. 40.1(b)], that is, the electron (with
Q = −1) interacts via the Fourier transform of the potential Aµ
cl(x).
This is easy to understand: the interaction between an electron and
a potential is HI = Q|e| ¯ψγµψAµ
cl.
The ψ brings an electron in and
¯ψ takes is out, so the interaction vertex is proportional to what’s left,
which is Q|e|γµAµ
cl. For a static, positive electromagnetic potential we
have A0
cl(r) =
Z|e|
4π|r|, and Acl(r) = 0, so the potential we are after is
the Fourier transform of the Coulomb interaction ˜A0
cl(q) = Z|e|
q2 . The
amplitude for the diagram in Fig. 40.1(a) for a (Q = −1) electron to be
scattered by a nuclear potential is given by
iM = iZe2
q2 ¯u(p′)γ0u(p).
(40.1)
p = mv
|p′| cos θ
|p′| sin θ
θ
p′
Fig. 40.2
Kinematics for Rutherford
scattering.
Example 40.1
The kinematics of the scattering is shown in Fig. 40.2. Calculating the transferred
momentum gives us1 q2 = −|q|2 = −4p2 sin2 θ
2 . In this example we will work in the
1See Exercise 40.1.
non-relativistic limit, for which we have that ¯u(p′)γ0u(p) = 2mξ†ξ = 2m. Putting
Recall that in the non-relativistic limit
we have u(p) ≈√m
„ ξ
ξ
«
.
everything together we obtain an amplitude
|M|2 =
4Z2e4m2
16p4 sin4(θ/2) .
(40.2)

356
QED scattering: three famous cross-sections
Finally, we can relate our amplitude to something measurable. For scattering from
a potential, the diﬀerential cross-section2 is given by
2See,
for
example,
Peskin
and
Schroeder, Chapters 4 and 5.
dσ
dΩ= |M|2
(4π)2 ,
(40.3)
and so plugging in our expression for |M|2, we obtain
dσ
dΩ=
Z2α2
4m2v4 sin4(θ/2) ,
(40.4)
which is Ernest Rutherford’s celebrated result.
An alleged scientiﬁc discovery has no
merit unless it can be explained to a
barmaid.
Ernest Rutherford (1871–1937)
40.2
Example 2: Spin sums and the Mott
formula
If we work through the previous example again, but this time without
invoking the non-relativistic approximation, we obtain the relativisti-
cally correct version of Rutherford’s result, which is known as the Mott
formula. The complication with working with relativistic particles will
be the manipulation of spinors.
We collect here some useful tricks for
evaluating expressions in QED.
Spinor trick 1:
[¯u(f)Γu(i)]∗=
h
¯u(i)γ0Γ†γ0u(f)
i
Spinor trick 2:
X
s
us(p)¯us(p) = ✁p + m
Trace trick 0:
Tr(I) = 4
Trace trick 1:
Tr(odd number of γ matrices) = 0
Trace trick 2:
Tr(γµγν) = 4gµν
Trace trick 3:
Tr(γµγνγργσ)
= 4(gµνgρσ −gµρgνσ + gµσgνρ)
Trace trick 4:
γµ✁aγµ = −2✁a
Photon trick:
X
polarizations
ǫµ(p)ǫ∗
ν(p) →−gµν
See Peskin and Schroeder, Chapter 5,
for the origin of the photon trick.
Example 40.2
The scattering amplitude is still given by iM = i Ze2
q2 ¯us′(p′)γ0us(p), but here we’ve
included the spin indices. We could work out the spinor part by brute force. How-
ever, this isn’t usually necessary in real life. Experiments involve scattering initially
unpolarized fermions and detecting the products of the scattering in detectors which
aren’t sensitive to polarization. We therefore average over the initial spin state and
sum over the ﬁnal spin states. We therefore want 1
2
P
s′,s |M|2, whose spinor part
is given by
1
2
X
s′,s
|¯us′(p′)γ0us(p)|2 = 1
2
X
s′,s
h
¯us′(p′)γ0us(p)
i h
¯us′(p′)γ0us(p)
i∗
.
(40.5)
The calculations of these spin sums are quite straightforward, but do rely on the
knowledge of a number of tricks (see sidenote).
We will draw freely from them
throughout this chapter.
Using spinor trick 1, we have |¯u(f)Γu(i)|2 =
ˆ
¯u(f)γ0u(i)
˜ ˆ
¯u(i)γ0u(f)
˜
.
Using
this, and writing out indices, our spinor term becomes
1
2
X
s,s′
¯us′(p′)αγ0
αβus(p)β ¯us(p)ηγ0
ηλus′(p′)λ,
(40.6)
where the explicit inclusion of subscripts allows us to rearrange the elements behind
the sum sign (and we assume a sum over repeated indices, as usual). Next we use
spinor trick 2 which says P
s [us(p)¯us(p)]αβ = [✁p + m]αβ. This is used to perform
the sum over the spin polarizations s and s′ as follows3
3Manipulations used in eqn 40.7 are:
Line 1: us(p)β ¯us(p)η = [✁p + m]βη.
Line 2: Rearranging and using spinor
trick 2 on primed variables.
Line 3: Identifying a trace.
1
2
P
s,s′ |¯us′(p′)γ0us(p)|2
=
1
2
P
s′ ¯us′(p′)αγ0
αβ [✁p + m]βη γ0
ηλus′(p′)λ
=
1
2 γ0
αβ [✁p + m]βη γ0
ηλ [✁p′ + m]λα
=
1
2 Tr
ˆ
γ0(✁p + m)γ0(✁p′ + m)
˜
.
(40.7)
To recap, we’ve boiled down the sum over spinors into the trace of a number of
matrices.

40.3
Example 3: Compton scattering
357
Next we note that the vector ✁a = γµaµ and we use some fun properties of the
traces of products of γ matrices. Trace trick 1 says that traces of odd numbers of γs
vanish which means we may write:
1
2 Tr
ˆ
γ0(✁p + m)γ0(✁p′ + m)
˜
= 1
2
˘
Tr
ˆ
γ0✁pγ0✁p′˜
+ m2Tr
ˆ
(γ0)2˜¯
.
(40.8)
It’s fairly easy to work out Tr(γ0✁pγ0✁p′) by plugging in explicit forms of the matrices,
or alternatively we can use the trace trick 3, to obtain 4(Ep′Ep +p′ ·p). Trace trick 0
gives us 4m2 for the last term. Remembering that the scattering is elastic, we obtain
1
2
X
s,s′
|¯us′(p′)γ0us(p)|2 = 2(E2
p + p · p′ + m2).
(40.9)
Referring back to the kinematics of the scattering, it’s straightforward to show that
2(E2
p +p2 cos θ+m2) = 4E2
p(1−β sin2 θ/2), which combined with |q|2 = 4p2 sin2 θ/2
allows us to complete the problem. We ﬁnally obtain
1
2
X
s,s′
|M|2
=
Ze4
4p2β2 sin4 θ/2(1 −β2 sin2 θ/2).
(40.10)
We can utilize our cross-section equation once more to obtain
dσ
dΩ=
Zα2
4p2β2 sin4 θ/2 (1 −β2 sin2 θ/2),
(40.11)
which is Mott’s formula.
I myself am neither an experimental-
ist nor a real mathematician; my the-
ory stops at the Schr¨odinger equation.
What I’ve done in this subject is to look
at the evidence, do calculations on the
back of an envelope and say to the the-
oretician ‘if you apply your techniques
to this problem, this is how it will come
out’ and to the experimentalists just
the same thing.
Nevill Mott (1905–1996)
40.3
Example 3: Compton scattering
Compton scattering is the process e−+γ →e−+γ. The two lowest order
Arthur Compton (1892–1962)
With his wife and two sons, Dr. Comp-
ton lives in Chicago in a big brick house
ﬁlled with souvenirs of their world
tour.
He does not know the taste of
hard liquor, almost never smokes, al-
ways oﬀers a cigaret to women visi-
tors.
He plays such a bang-up game
of tennis that he sometimes has a hard
time ﬁnding worthy opponents. Several
times a month he puts in an evening
of mandolin-playing with three friends.
When his graduate students have ﬁn-
ished an examination, he likes to dine
them and take them to the theatre.
Time Magazine (1936).
diagrams (which are second order in the interaction term) are shown in
Fig. 40.3.
p′
p
k
k′
p + k
p′
p
k
k′
p −k′
(a)
(b)
Fig. 40.3
Compton scattering.
(a)
The s-channel contribution. (b) The u-
channel contribution.
Example 40.3
As practice in translating Feynman diagrams, we may translate the Compton dia-
grams into amplitudes. The ﬁrst diagram yields (upon dropping Qs again)
iMs-channel = ¯u(p′)(i|e|γν)ǫ∗
νλ′(k′)
i
(✁p + ✁k) −m + iǫ
ǫµλ(k)(i|e|γµ)u(p),
(40.12)
and the second diagram gives
iMu-channel = ¯u(p′)(i|e|γν)ǫνλ(k)
i
(✁p −✁k′) −m + iǫ
ǫ∗
µλ′(k′)(i|e|γµ)u(p).
(40.13)
These amplitudes may be evaluated to give the Klein–Nishina formula. However, the
Yoshio Nishina (1890–1951).
full derivation of the Klein–Nishina formula is rather lengthy and can be found in
many books,4 so we’ll evaluate the s-channel contribution from the ﬁrst diagram for
4Peskin and Schroeder give, as usual,
an especially clear derivation.
the special case of a highly relativistic particle. (Recall that here ‘relativistic’ means
we can ignore the mass of the electron.)
We therefore have that p2 = 0 and (as
usual) k2 = 0. We’re interested in |M|2 averaged over initial spin states and photon
polarizations and summed over ﬁnal ones (we pick up a factor 1/4 in doing this).
We therefore want (employing spinor trick 15 and dropping the photon polarization
5We
have
the
identity
γ0 `
γνγλγµ´† γ0
=
γµγλγν,
and
so γ0 (γνǫ∗
ν✁pǫµγµ)† γ0 = γµǫ∗
µ✁pǫνγν.
labels):
1
4
X
s,s′
X
polarizations
e4
(p + k)2
h
¯us′(p′)γνǫ∗
ν(k′)(✁p + ✁k)ǫµ(k)γµus(p)
i
×
h
¯us(p)γσǫ∗
σ(k)(✁p + ✁k)ǫρ(k′)γρus′(p′)
i
.
(40.14)

358
QED scattering: three famous cross-sections
Freely employing our array of tricks we have
Successive lines of eqn 40.15 use the fol-
lowing ideas:
Line 1: Photon trick.
Line 2: Contracting indices and spinor
trick 2.
Line 3: Using ✁p2 = p2 = 0 and trace
trick 4.
Line 4: Trace trick 3.
Line 5: u = (p′ −k)2 = −2p′ · k,
s = (p + k)2 = 2p · k.
1
4
X
|Ms-channel|2
=
e2
4s2
X
s,s′
“
gµσgνρ¯us′(p′)γν(✁p + ✁k)γµus(p)
×¯us(p)γσ(✁p + ✁k)γρus′(p′)
”
=
e2
4s2 Tr
h
γν(✁p + ✁k)γµ✁pγµ(✁p + ✁k)γν✁p′i
=
e2
s2 Tr
h
✁k✁p✁k✁p′i
=
4e2
s2 2(p · k)(p′ · k) = −2e2u
s
.
(40.15)
It turns out that the second diagram contributes −2e4s
u
and there is
no interference term between the two. The electron spin and photon
polarization averaged, squared amplitude for Compton scattering is then
1
4
X
s,s′
X
polarizations
|M|2 = −2e4 u
s + s
u

.
(40.16)
40.4
Crossing symmetry
As a ﬁnal illustration of the utility of Feynman diagrams in describing
scattering, we brieﬂy note another very useful feature. Feynman’s inter-
pretation of antiparticles as particles travelling backward in time may
be used to manipulate Feynman diagrams, allowing us access to sev-
eral more physical amplitudes from the evaluation of a single Feynman
amplitude.
φ
p
¯φ
k = −p
Fig. 40.4 Crossing symmetry. A Feyn-
man diagram with an incoming par-
ticle may be manipulated to describe
a new process involving an outgoing
antiparticle with opposite charge and
four-momentum.
The principle is shown in Fig. 40.4, where we see that if we start with
a diagram with an incoming particle φ then we may ﬂip the leg to create
a valid process involving an outgoing antiparticle ¯φ with opposite charge
and momentum. This process, known as a crossing, has the wonderful
property that
M (φ(p) + ... →...) = M
 ... →... +
¯
φ(k)

,
p = −k.
(40.17)
Example 40.4
Consider the s-channel process e−e+ →µ−µ+ examined in Chapter 39 and shown
in Fig. 40.5. If we ‘cross’ the diagram by reversing the momentum direction of the
e−
e+
µ−
µ+
pA
pB
pD
pC
Fig.
40.5
The
s-channel
process
e−e+ →µ−µ+.
e−
µ−
e−
µ−
kA
kC
kB
kD
Fig.
40.6
The
t-channel
process
e−µ−→e−µ−found by crossing the
previous diagram.
incoming e+ and outgoing µ+ (changing the sign of their charge as we do so), then
upon ﬂipping the diagram over we obtain Fig. 40.6 describing the process e−µ−→
e−µ−. The amplitude for the resulting diagram is the same as the M we calculated
in Chapter 39 if we make the replacements: pA →kA, pB →−kB, pC →kC and
pD →−kD. Note that we have turned an s-channel process into a t-channel process
with our crossing, reﬂected by the fact that instead of reversing individual momentum
we may simply replace s →t in the amplitude.

Exercises
359
Crossing symmetries are clearly a very useful feature of the formalism
because calculating |M|2 for one process immediately gives us access
to |M|2 for all processes related by crossings. As we have only brieﬂy
touched on this topic here, we recommend you consult one of the stan-
dard texts before deploying crossings in anger. Peskin and Schroeder
and Halzen and Martin both contain several examples.
Chapter summary
• This chapter has illustrated three simple scattering examples from
QED: (1) Rutherford scattering; (2) the Mott formula; (3) Comp-
ton scattering.
• Crossing symmetry makes a Feynman amplitude for one process
describe another process.
Exercises
(40.1) Verify that the transferred momentum in the
Rutherford calculation is given by q2 = 4p2 sin2 θ
2.
(40.2) Verify the Trace Tricks used in this chapter.
(40.3) (a) Calculate Tr(γ0✁pγ0✁p′) using explicit forms of the
matrices.
(b) Repeat the calculation using Trace Trick 3.
(40.4) Consider the e−e+ →µ−µ+ problem from the pre-
vious chapter. Using the tricks considered in this
chapter show that
1
4
X
spins
|M|2
=
e4
4q4 Tr
h
(✓✓
p′ −me)γµ(✁p + me)γνi
×Tr
h
(✁k + mµ)γµ( k′ −mµ)γν
i
.(40.18)
(b) Taking all particles to be highly relativistic,
show that
1
4
X
spins
|M|2 = 8e4
q4
ˆ
(p · k)(p′ · k′) + (p · k′)(p′ · k)
˜
.
(40.19)
(c) Finally, use the kinematics discussed in Chap-
ter 39 to show 1
4
P
spins |M|2 = e4(1 + cos2 θ).
See Peskin and Schroeder, Chapter 5 for help.
(40.5) Show that eqn 40.19, which describes the process
e−e+ →µ−µ+, may be written in terms of Man-
delstam variables as
1
4
X
spins
|M|2 = 8e4
s2
"„ t
2
«2
+
“u
2
”2
#
.
(40.20)
Use a crossing to determine the analogous equation
for the process e−µ−→e−µ−.
(40.6) Møller scattering is the process e−e−→e−e−.
(a) Two diagrams contribute to the Feynman am-
plitude for this process.
Identify these and show
that
M
=
−e2
t ¯u(k)γµu(p)¯u(k′)γµu(p′)
+e2
u ¯u(k′)γµu(p)¯u(k)γµu(p′).(40.21)
(b) Show further that, in the ultra-relativistic limit,
1
4
X
spins
|M|2
=
e4
4
1
t2 Tr
ˆ
✁kγµ
✁pγν˜
Tr
h
 k′γµ✓✓
p′γν
i
+ 1
u2 Tr
h
 k′γµ
✁pγνi
Tr
h
✁kγµ✓✓
p′γν
i
−1
tuTr
h
✁kγµ
✁pγν k′γµ✓✓
p′γν
i
−1
tuTr
h
 k′γµ
✁pγν✁kγµ✓✓
p′γν
iﬀ
.
(c) Use a crossing symmetry to turn the previous
equation into one describing the process e−e+ →
e−e+, which is known as Bhabha scattering.

41
The renormalization of
QED and two great results
41.1 Renormalizing
the
photon
propagator:
dielectric vac-
uum
361
41.2 The
renormalization
group
and the electric charge
364
41.3 Vertex corrections and the
electron g-factor
365
Chapter summary
368
Exercises
368
His laboratory is his ball-point pen.
Caption to a photograph of Julian Schwinger (1918–1994)
(holding a pen)
α
2π
Engraved on Julian Schwinger’s tombstone
Some really stunning consequences of QED are seen when we examine
processes that contain electron–positron loops.
The integrals we en-
counter when considering these processes contain divergences that we
must tame with renormalization. In this chapter we will demonstrate
two of the great results that follow from renormalizing QED. These are
(1) the fact that the electronic charge is not a constant and (2) the fact
that the g-factor of the electron is not quite g = 2.
(a)
(b)
(c)
(d)
(e)
(f)
1PI
1PI
Fig.
41.1
Feynman diagrams
for
renormalized QED.
To renormalize QED we calculate three Green’s functions:
(a) Electron 1PI self-energy
−i˜Σ(✁p),
(b) Photon 1PI self-energy
i˜Πµν(q),
(c) Interaction vertex function
−iQ|e|˜Γµ(p, p′).
(41.1)
These functions, each made of an inﬁnite sum of Feynman diagrams,
are themselves shown diagrammatically in Fig. 41.1(a–c).
In calcu-
lating each of these Green’s functions we will encounter contributions
which lead to divergences. These divergences are ﬁxed through the in-
troduction of counterterms. QED is a renormalizable theory and only
three counterterm diagrams, each corresponding to one of the Green’s
functions above, are required.
The QED counterterms are shown in
Fig. 41.1(d–f). They have Feynman rules:
(d) Electron self-energy
(Bird on a wire)
i (✁pB + A) ,
(e) Photon self-energy
(Bird on a wave)
i(gµνq2 −qµqν)C,
(f) Interaction vertex
(Electrocuted bird)
iQ|e|γµD.
(41.2)
In order to renormalize the theory we impose conditions on each of
the Green’s functions which ensure that we are expanding in the correct
masses and coupling constants. For QED the renormalization conditions
are:
−i˜Σ(✁p = m) = 0
ﬁxes electron mass to m,
i˜Πµν(q = 0) = 0
ﬁxes photon mass to 0,
−iQ|e|˜Γµ(p′ −p = 0) = iQ|e|γµ
ﬁxes charge to Q|e|.
(41.3)

41.1
Renormalizing the photon propagator: dielectric vacuum
361
We will begin our tour of the consequences of renormalizing QED by ex-
amining the case of the photon propagator and show how renormalizing
it explains the dielectric properties of the vacuum.
41.1
Renormalizing the photon
propagator: dielectric vacuum
A dielectric is an insulator which may be polarized with an applied
electric ﬁeld. This is encoded in the dielectric constant of a material ǫ,
which tells us how much the dielectric alters the electrostatic potential
of a charge. Renormalized QED predicts that the interacting vacuum
is a dielectric! The dielectric properties of the vacuum arise because
interactions dress the bare photon in a cloud of electron–positron pairs.
To see how this works we will examine the photon’s self-energy. As in
Chapter 33 we deﬁne the 1PI photon self-energy which is given by
i˜Πµν(q) =
X 
All 1PI diagrams that can be
inserted between photon lines

.
(41.4)
(This diﬀers in sign from the 1PI self-energy Green’s functions encoun-
tered previously.) Some examples of contributions to i˜Πµν(q) are shown
in Fig. 41.2. Notice that these involve the photon removing electron–
positron pairs from the vacuum in various ways and then returning them.
=
+
+
+
+
+ · · ·
1PI
Fig. 41.2
Some contributions to the
Green’s function i˜Πµν(q).
Looked at in terms of pictures, it’s clear how the photon self-energy
will renormalize the photon propagator. As shown in Fig. 41.3, we sim-
ply sum the contribution of the self-energy to inﬁnity. The version in
equations is made a little trickier by the presence of the indices, which
reﬂect the vector-like nature of the photon ﬁeld.1
1Remember that the massless nature of
the photon ﬁeld means that photons are
not vector particles.
=
+
+
+ · · ·
=
1
(
)−1 −1PI q2
(a)
(b)
1PI
1PI
1PI
Fig. 41.3 Renormalization of the pho-
ton propagator.
Example 41.1
In equations the sum is
˜Dµν(q) = ˜D0µν(q) + ˜D0µλ(q)[i˜Πλη(q)] ˜D0ην(q) + . . .
= −igµν
q2
+
„ −igµλ
q2
«
i˜Πλη(q)
„−igην
q2
«
+ . . .
= −igµν
q2
+
„ −i
q2
«
i˜Πµν(q)
„−i
q2
«
+
„ −i
q2
«
i˜Πη
µ(q)
„ −i
q2
«
i˜Πην(q)
„ −i
q2
«
+ . . .
(41.5)
Summing the series we obtain
˜Dµν(q) = −igµν
q2
+ 1
q2 ˜Πµη(q) ˜Dη
ν(q),
(41.6)
or
“
q2gµη −˜Πµη(q)
”
˜Dη
ν(q) = −igµν.
(41.7)
Things are simpliﬁed rather by the use of the Ward identity, which guarantees that
qν ˜Πµν(q) = 0. As a result, we can write the photon self-energy as
˜Πµν(q) = (q2gµν −qµqν)˜Π(q).
(41.8)
Substitution of this new form yields
q2[1 −˜Π(q)]Dµν(q) + ˜Π(q)qµqηDη
ν(q) = −igµν,
(41.9)

362
The renormalization of QED and two great results
whose solution is
˜Dµν(q) =
−igµν
q2[1 −˜Π(q)]
+
iqµqν ˜Π(q)
q4[1 −˜Π(q)]
.
(41.10)
In practical calculations the qµqν part never contributes to any observable amplitude.
This is because ˜Dµν(q) is always coupled to conserved currents and we know that
qµJµ
em = 0.
The resulting renormalized photon propagator is given by
˜Dµν(q) =
−igµν
q2[1 −˜Π(q)]
.
(41.11)
Note that we would expect that ˜Π(q), which contains loops, will diverge
unless we use counterterms in our calculation of ˜Π(q2).
Example 41.2
Let’s see how to use the counterterm.
Instead of summing over all 1PI inser-
tions, we’ll limit our calculation at a single electron–positron loop, which we’ll call
i˜πµν(q) = i(gµνq2 −qµqν)˜π(q).
To prevent the loop causing any trouble we include
Fig. 41.4 The lowest order correction
to the photon propagator and the coun-
terterm that prevents its divergence.
a second-order counterterm which makes a contribution i(gµνq2 −qµqν)C(2). The
total insertion in the photon line is shown in Fig. 41.4. Summing this insertion to all
orders, we obtain
˜Dµν(q) =
−igµν
q2 ˘
1 −
ˆ
˜π(q) + C(2)˜¯.
(41.12)
The renormalization condition is that the photon self-energy Π(q) vanishes at q2 = 0,
implying that ˜π(q = 0) = −C(2) and the propagator becomes
˜Dµν(q) =
−igµν
q2 {1 −[˜π(q) −˜π(0)]}.
(41.13)
This will be enough to remove the divergences from the calculation of ˜Dµν(q) to
second order.
The bare photon propagates while tearing electron–positron pairs from
the vacuum. These processes have the eﬀect of modifying the photon’s
amplitude for propagation between two points.
More than that, we
know that virtual photons (and remember that all photons are, to some
extent, virtual photons) couple to a fermion line at both ends of their
trajectory. If we write the vertex Feynman rule as −iQ|e0|γµ, we can
bundle up the factors of |e0| with the photon propagator, so that we
have2 (considering Q = 1 for simplicity):
2Using the shorthand that e0 is the
unrenormalized charge and assuming
that the counterterm contribution is ac-
counted for by ˜Π(0).
˜Dµν(q) =
−ie2
0gµν
q2
n
1 −
h
˜Π(q) −˜Π(0)
io = −igµν
q2


e2
0
1 −
h
˜Π(q) −˜Π(0)
i

.
(41.14)
Written the second way, the term in the brackets may be considered the
momentum-dependent electric charge e(q). We could say that the real-
life electric charge, renormalized by the interactions with the vacuum, is
given by |e| = |e0|/
n
1 −
h
˜Π(q) −˜Π(0)
io
. The electric charge therefore

41.1
Renormalizing the photon propagator: dielectric vacuum
363
depends on the momentum transferred by the virtual photon that me-
diates the electromagnetic force! A physical explanation for this feature
of the dielectric properties of the vacuum is provided by the concept of
screening, as we discuss below.
Example 41.3
In what follows we will require an expression for the one-loop amplitude in Fig. 41.5,
q
q
p
p + q
µ
ν
Fig. 41.5 The photon propagator with
a single electron–hole loop insertion.
which may be translated from diagram to equation, giving
i˜π(q) = (−1)(−ie0)2
Z
d4p
(2π)4 Tr
„
γµ
i
✁p −mγν
i
✁p +✁q −m
«
.
(41.15)
This integral can be done. With the inclusion of the counterterm it is found that
˜π(q) −˜π(0), the one-loop contribution to ˜Π(q), is given by
˜π(q) −˜π(0) = −e2
0
2π2
Z 1
0
dx (x −x2) ln
„
m2
m2 −(x −x2)q2
«
.
(41.16)
We’ve seen that the renormalized propagator is given, to second order
in the interaction, by
˜Dµν(q)
=
−igµν
q2
e2
0
{1 −[˜π(q) −˜π(0)]}
≈
−igµνe2
0
q2
[1 + ˜π(q) −˜π(0)] .
(41.17)
To see the consequence of this on electrostatics, we need the static ver-
sion of the propagator, obtained by taking q2 = −q2 thus:
˜π(−q) −˜π(0) = −e2
0
2π2
Z 1
0
dx (x −x2) ln

m2
m2 + (x −x2)q2

. (41.18)
Expanding this in the limit q2 ≪m gives us
˜Dµν(q) ≈igµνe2
0
q2

1 +
α
15π
q2
m2 ,

,
(41.19)
where α = e2
0/4π.
Recall from Chapter 20 our (Born approximation) interpretation of
the momentum space propagator as proportional to a matrix element
of a potential V (r) via ˜D00(q) ∝−
R
d3r eiq·rV (r). Doing the inverse
Fourier transform of ˜D00(q) yields a potential due to the electron of
V (r) = −
 α
|r| + 4α2
15m2 δ(3)(r)

.
(41.20)
The ﬁrst term is Coulomb’s potential for a point charge. The second
term is the correction due to the screening of the electron charge that
is provided by the virtual electron–hole pairs. The screening eﬀect is
visualized in Fig. 41.6, where we represent the pairs as eﬀective dipoles.
The screening is known as vacuum polarization.
−
−
+
−
+
−
+
−
+
−
+
−
+
−
+
−
+
−
+
−
+
−
+
−
+
−
+
−
+
−
+
−
+
Fig. 41.6 Screening in QED.

364
The renormalization of QED and two great results
Importantly, this eﬀect can be measured! It causes a shift in the hy-
drogen energy levels of ∆E = −4α2
15m2 |ψ(0)|2, where ψ(x) is the hydrogen
wave function (which is only nonzero at the origin for l = 0 levels). This
eﬀect causes a shift in the 2S 1
2 to 2P 1
2 transition in hydrogen of −27 MHz
which, despite making up only a small part of the famous Lamb shift
(of +1057 MHz), has indeed been experimentally veriﬁed.3 The eﬀect of
3Willis E. Lamb, Jr (1913–2008) mea-
sured the energy level shift in hydro-
gen in 1947. [See W. E. Lamb and R.
C. Retherford, Phys.
Rev.
72, 241
(1947)]. Historically, this measurement
was of immense importance in convinc-
ing physicists to take corrections due to
virtual photons seriously. Shortly after
the announcement of the experimental
result, Hans Bethe (1906–2005) made a
non-relativistic calculation of the shift
caused by the self-energy of an elec-
tron in the electromagnetic ﬁeld of a
nucleus using an early form of renor-
malization.
The agreement with ex-
periment was good and the fully rela-
tivistic explanation soon followed from
a number of physicists. See Schweber
for the history and Weinberg, Chapter
14, for the full calculation of the Lamb
shift.
vacuum polarization is also very important in the quantum ﬁeld theory
of metals and will be discussed further in Chapter 43.
41.2
The renormalization group and the
electric charge
Using what we know about vacuum polarization along with the renor-
malization group (of Chapter 34) we can work out how the electric charge
changes, depending on the size of the momentum that the photon is car-
rying. That is, how the QED coupling depends on the energy scale. To
do this we need to calculate the β-function, given here by β = µ d|e|
dµ ,
which tells us how the renormalized charge e depends on the choice of
energy scale µ.
Example 41.4
Using our equation for the renormalized electric charge we can expand for small ˆπ(q)
to obtain
e2(µ) =
e2
0
1 −(˜π(µ) −˜π(0)) ≈e2
0[1 + (˜π(µ) −˜π(0))] + O(e4),
(41.21)
or
|e| ≈|e0|
»
1 + 1
2 (˜π(µ) −˜π(0))
–
.
(41.22)
The β-function is then given by
β = µ d|e|
dµ = 1
2µ|e0|d˜π(µ)
dµ
.
(41.23)
Now we use eqn 41.16 which states that
˜π(µ) −˜π(0) = −e2
0
2π2
Z 1
0
dx (x −x2) ln
»
m
m2 −(x −x2)µ2
–
.
(41.24)
In the large energy scale limit (µ ≫m) diﬀerentiation gets us
d˜π(µ)
dµ
=
e2
0
π2µ
Z 1
0
dx (x −x2) =
e2
0
6π2µ .
(41.25)
Note that, to the order to which we’re working, it’s permissible to swap e0 for e on
the right-hand side in what follows.
We obtain a β-function
β = µd|e|
dµ = + |e|3
12π2 ,
(41.26)
where we accentuate the + sign, since that’s the most important part.
This shows that the electromagnetic coupling increases with increasing

41.3
Vertex corrections and the electron g-factor
365
energy scale. Another way of saying this is that it increases with de-
creasing length scale. The electron appears to be more strongly charged
as you get closer to it. This makes sense in the context of the last sec-
tion, where we interpreted vacuum polarization as a screening process.
As you get closer to the electron you penetrate the cloud of screening
pairs and the charge seems to increase.
41.3
Vertex corrections and the electron
g-factor
Other people publish to show you how to do it, but Julian
Schwinger publishes to show you that only he can do it.
Anon, quoted from S. Schweber, QED and the Men who
Made It
p
p′
p′ −p
Fig. 41.7 The vertex Green’s function
−iQ|e|Γµ(p, p′).
The magnetic moment operator of the electron may be written
ˆµ = g
Q|e|
2m

ˆS,
(41.27)
where Q = −1 for the electron. The Dirac equation predicts4 that the
4See Example 36.9.
electron g-factor of the electron is exactly 2. In this section we will make
the link between renormalized QED and the g-factor. This will lead to
Julian Schwinger’s famous prediction that the ﬁrst-order correction to
the QED interaction vertex causes a shift from g = 2 to g = 2 + α/π.
That is to say that the fact that electrons can emit virtual photons
changes the form of the electron–photon interaction in a measurable
manner.
The Green’s function for the electron–photon vertex can be written
−iQ|e|˜Γµ(p, p′) and its diagram is shown in Fig. 41.7. It is deﬁned as5
5The overal sign is chosen because
the ﬁrst-order interaction is given by
−iQ|e|γµ so to ﬁrst-order ˜Γµ = γµ.
−iQ|e|˜Γµ(p, p′) =
X


All amputated insertions with one
incoming fermion line, one outgoing
fermion line and one photon line.

.
(41.28)
Some examples are shown in Fig. 41.8.
The vertex function can be
=
+
+
+
+ · · ·
Fig. 41.8 Contributions to the Green’s
function −iQ|e|˜Γµ. The contribution to
ﬁrst order is simply given by the inter-
action vertex −i|e|γµ.
read as describing an oﬀmass-shell photon decaying into an electron–
positron pair. Since the photon has6 JP = 1−, there are two possible
6In this notation we list the total angu-
lar momentum J and the intrinsic par-
ity P of the photon.
conﬁgurations of the pair. It can have L = 0 and S = 1, or L = 2 and
S = 1. This fact is reﬂected in the form in which we write the vertex
function as a sum of two terms.
˜Γµ(p, p′) = γµF1(q) + iσµνqν
2m
F2(q),
(41.29)
where qµ = p′µ −pµ and where σµν = i
2 [γµ, γν]. The function F1(q) is
known as the Dirac form factor7 and F2(q) is known as the Pauli form
7In fact, F1(q) may be thought of as
a Fourier transform of the charge dis-
tribution and has the property that
F1(0) = 1 at all orders of perturbation
theory.
factor. For vanishing q we need only consider the ﬁrst-order contribution
to ˜Γµ = γµ, which is the ﬁrst diagram in Fig. 41.8. We read oﬀthat for
q →0 we have F1(0) = 1 and F2(0) = 0.

366
The renormalization of QED and two great results
We will now sketch out the determination of the g-factor of the elec-
tron in a manner intended ‘to show you how to do it’. Since the calcula-
tion is quite lengthy and involved, we will break it down into a number
of steps.
Step I: Consider the diagram in Fig. 41.9 showing the interaction
vertex coupled to a classical source Acl
µ(x), giving a Feynman rule con-
tribution of −iQ|e|˜Γµ ˜Acl
µ(q), where qµ is the momentum of the photon.
Coupling this to the spinor current of the incoming and outgoing elec-
tron, we have an amplitude
iM = −iQ|e|¯u(p′)˜Γµ(p, p′)u(p) ˜Acl
µ(q),
(41.30)
where q = p′ −p and an energy momentum conserving delta function
has been suppressed. Inserting eqn 41.29 for ˜Γµ yields
iM = −iQ|e|¯u(p′)

γµF1(q) + iσµνqν
2m
F2(q)

u(p) ˜Acl
µ(q).
(41.31)
p
p′
q
˜Aclµ(q)
Fig. 41.9 The vertex function coupled
to a classical source.
Step II: The next step makes the ¯u′γµu in the ﬁrst term much more
complicated and uses an identity, known as the Gordon decomposition
(proved in Exercise 36.7), given by
¯u(p′)γµu(p) = ¯u(p′)
p′µ + pµ
2m
+ iσµνqν
2m

u(p).
(41.32)
The reason for this step is that is eliminates γµ, pushing the spin-
independent part into the ﬁrst term and the spin-dependent part into
the second. Substituting this into our Feynman amplitude yields
iM
=
−iQ|e|¯u(p′)
p′µ + pµ
2m

u(p) ˜Acl
µ(q)F1(q)
+Q|e|
2m [¯u(p′)σµνqνu(p)] ˜Acl
µ(q) {F1(q) + F2(q)} . (41.33)
We now have an expression for the amplitude which is separated into
the spin-independent ﬁrst term and a spin-dependent second term. The
information on the g-factor is to be found in the second term.
Step III: Now we take the non-relativistic limit. This involves writing
u(p) ≈√m

ξ
ξ

.
We also need to use the explicit forms for the
components of σµν
σ0i =

−iσi
0
0
iσi

and
σij =

εijkσk
0
0
εijkσk

.
(41.34)
Using these we ﬁnd that ¯u′σ0iu = 0 and, more importantly, that
¯u′σiju = 2m

ξ′†σkξ

εijk.
Putting this all together we have, for the spin-dependent part of the
amplitude of our diagram in the limit q →0, that
Spin-dependent
amplitude
=
(2m)Q|e|
2m

ξ′†σkξ

εijkqj ˜Acli(q) {F1(0) + F2(0)}
=
(2m)iQ|e|
2m

ξ′†σkξ
 ˜Bk(q) {1 + F2(0)} ,
(41.35)

41.3
Vertex corrections and the electron g-factor
367
where we’ve assumed Acl(x) = (0, Acl(x)) and used the fact that, in
momentum space, the components of the magnetic ﬁeld are given by
iεijkqi ˜Aclj(q) = ˜Bk(q). We conclude that the spin-dependent part of
the amplitude is proportional to Q|e|{1 + F2(0)}⟨ˆS⟩· B, where the ex-
pectation value of the spin operator is ⟨ˆS⟩= ξ′†σξ/2.
We now have an amplitude, so we may extract a scattering potential
using the Born approximation.
iM = −i⟨f| ˆV (x)|i⟩.
(41.36)
Since we are still using the relativistic normalization, we need to correct
by dividing through by twice the energy of the electron that’s scattered
(= 2m). We obtain a real-space potential
V (x) = −Q|e|
m {1 + F2(0)}⟨ˆS⟩· B(x).
(41.37)
Comparing this to the expected expression for the potential energy of a
magnetic moment µ in a magnetic ﬁeld, namely
V (x) = −⟨ˆµ⟩· B(x) = −

g
Q|e|
2m

⟨ˆS⟩

· B(x),
(41.38)
we may read oﬀthat g = 2 [1 + F2(0)].
So g is not exactly 2! There’s a correction determined by F2(0). The
correction can be calculated by evaluating the contributions to ˜Γµ.
p
p′
q
k
k′ = k + q
p −k
Fig. 41.10
The third-order diagram
which contributes to F2(0).
Step IV: Let’s look at the contributions to −iQ|e|˜Γµ, order by order.
To ﬁrst order we have the bare vertex whose contribution (for Q = −1)
is i|e|γµ, so the ﬁrst-order contribution to Γµ is γµ implying F1(0) = 1
and F2(0) = 0 as we said earlier. There are no second-order diagrams,
but to third order the only diagram that contributes to F2(q) is that
shown in Fig. 41.10.
Example 41.5
From the labelled diagram, shown in Fig. 41.10, we can write down the amplitude
for the vertex as
˜Γµ
diagram =
Z
d4k
(2π)4
−igνρ
(p −k)2 + iǫ ¯u(p′)(i|e|γν)
i
✁k′ −m + iǫ
γµ
i
✁k −m + iǫ
(i|e|γρ)u(p).
(41.39)
This is divergent, so we must also include a third-order counterterm.
With the
inclusion of the counterterm in Fig. 41.1(e) and two pages or so of Dirac algebra8
8See Peskin and Schroeder, which is
particularly informative on this sub-
ject.
one can derive a result for the Pauli form factor which is F2(0) = e2/8π2.
The result: We evaluate g = 2[1 + F2(0)] with F2(0) =
e2
8π2 and ﬁnd
the famous result for the shift in the g-factor:
Recall that in the units used here, the
ﬁne structure constant is given by α =
e2/4π. Because α = 1/137.036...
we
have g = 2.0023... In SI units, α =
e2/4πǫ0ℏc.
g = 2

1 + α
2π + . . .

.
(41.40)
This was also Julian Schwinger’s result and one for which he justiﬁably
gained a spontaneous round of applause when he revealed, in a talk at

368
The renormalization of QED and two great results
the New York APS meeting in 1948, that his theory agreed precisely with
the crucial experiments. Of course, higher order corrections to this and
other QED processes may be evaluated and have led to QED becoming
the most stringently tested theory in physics.9
9See Peskin and Schroeder, page 196,
for a discussion of the remarkable agree-
ment of QED with modern experimen-
tal tests.
Chapter summary
• Renormalizing the photon propagator shows that the electric
charge is given by |e| = |e0|/[1 −˜Π(q)].
The electric charge is
screened by virtual electron–positron pairs (known as vacuum po-
larization) and this gives rise to the Lamb shift.
• The Dirac theory predicts g = 2, but including the electron–photon
vertex −iQ|e|˜Γ(p, p′) (for an oﬀmass-shell photon decaying into an
electron–positron pair) yields g = 2
 1 + α
2π + · · ·

which agrees
extremely well with experiment.
Exercises
(41.1) Verify
˜Dµν(q) =
−igµν
q2[1 −˜Π(q)]
+
iqµqν ˜Π(q)
q4[1 −˜Π(q)]
,
(41.41)
is the solution to eqn 41.9.
(41.2) (a) Verify
˜Dµν(q) ≈igµνe2
0
q2
„
1 +
α
15π
q2
m2
«
.
(41.42)
(b) Take the inverse Fourier transform and show
you obtain the form in the text.
(41.3) The electron 1PI self-energy is deﬁned as
−i˜Σ =
X „ All 1PI insertions with incoming
and outgoing electron lines
«
.
(41.43)
Draw contributions to −i˜Σ in QED up to fourth
order in the interaction.
(41.4) The Schwinger model deals with quantum electro-
dynamics in (2+1)-dimensional spacetime and pre-
dicts a one-loop vacuum polarization
−i˜Πµν(p) = −ie2
πp2 (pµpν −p2gµν).
(41.44)
Show that the photon takes on a mass as a result
and determine its size.
(41.5) Verify eqn 41.25.
(41.6) (a) Show that
σ0i =
„ −iσi
0
0
iσi
«
(41.45)
and
σij =
„ εijkσk
0
0
εijkσk
«
.
(41.46)
(b) Show that ¯uσiju = 2mξ†σkξεijk.

Part X
Some applications from the
world of condensed matter
Quantum ﬁeld theory ﬁnds many applications in the ﬁeld of condensed
matter physics which is concerned with the properties of many-particle
systems whose fundamental excitations can be regarded as particles. We
have already seen that lattice vibrations can be treated as particles called
phonons. In the case of metallic systems which contain large numbers
of electrons, the interactions between electrons can be very important
in determining the properties.
• Our ﬁrst condensed matter physics example is the superﬂuid, and
in Chapter 42 we show how an approximation due to Bogoliubov
allows weakly interacting Bose systems to be treated. The quasi-
particles are called bogolons and the ground state of the system
can be treated as a coherent state. The Noether current is shown
to arise from a gradient in the phase of this coherent state.
• Interactions between electrons in a metal are treated in Chapter 43
and we introduce the Cooper, Hartree and Fock terms in the per-
turbation expansion for the ground state energy shift. We explain
how to model the excitations and study them using propagators
and we introduce the random phase approximation.
• The BCS theory of superconductivity is presented in Chapter 44
and we show how to write down the BCS coherent state, derive
the quasiparticles and explore the eﬀect of broken symmetry. This
allows us to explore the Higgs mechanism in a superconductor.
• The fractional quantum Hall eﬀect is introduced in Chapter 45 and
we explain how quasiparticles carrying fractional charge emerge in
this theory.
A note on notation: In this part we conform to the conventions of the
condensed matter literature by denoting the number of particles by N
and deﬁne the number density of particles n = N/V, where V is the
volume.
For consistency we write number operators and occupation
numbers for the state with momentum p as ˆNp and Np respectively.

42
Superﬂuids
42.1 Bogoliubov’s hunting license
370
42.2 Bogoliubov’s transformation
372
42.3 Superﬂuids and ﬁelds
374
42.4 The current in a superﬂuid
377
Chapter summary
379
Exercises
379
There is no doubt a special place in hell being reserved for
me at this very moment for this mean trick, for the task is
impossible.
Robert Laughlin (1950– ), on asking his students to deduce
superﬂuidity from ﬁrst principles
A gas of weakly interacting, non-relativistic bosons is a problem where
the methods of quantum ﬁeld theory allow us to understand the ground
state, the excitations and the breaking of symmetry. Not only is this
an illustration of many of the methods we’ve discussed; it also applies
to one of the most fascinating phenomena in condensed matter physics:
superﬂuidity. A superﬂuid is a state of matter where momentum ﬂows
without dissipation.
When 4He is cooled below 2.17 K it becomes a
superﬂuid and may ﬂow through capillaries without resistance. It also
exhibits the fountain eﬀect whereby a beaker of superﬂuid spontaneously
empties itself.
The dispersion of superﬂuid helium has been measured with inelastic
neutron scattering and is shown in Fig. 42.1. At low momentum the
dispersion is linear, at large momentum the energy goes as p2/2m as
expected for free particles. In the middle is a large minimum. We will
discover that our gas of weakly interacting bosons captures the low and
high momentum behaviour of the superﬂuid dispersion. We will also
ﬁnd that the form of the ﬁeld that we obtain on spontaneous symmetry
breaking necessarily gives us a superﬂow of momentum.
0
0.5
1
1.5
Ep (meV)
0
1
2
3
|p|/¯h (˚A−1)
Fig. 42.1
The measured dispersion
of superﬂuid helium. Data taken from
A.B.D. Woods and R.A. Cowley, Can.
J. Phys. 49, 177 (1971).
42.1
Bogoliubov’s hunting license
The problem revolves around the Hamiltonian for non-relativistic Bose
particles interacting with a momentum-independent potential g. The
Hamiltonian describing this state of aﬀairs is
ˆH
=
1
2m
Z
d3x ∇ˆφ†(x) · ∇ˆφ(x)
+
g
2
Z
d3xd3y ˆφ†(x)ˆφ†(y)ˆφ(y)ˆφ(x)δ(3)(x −y)
=
X
p
p2
2mˆa†
pˆap + g
2V
X
kpq
ˆa†
p−qˆa†
k+qˆakˆap,
(42.1)
where the second form follows from putting the system in a box and
inserting the mode expansion ˆφ(x) =
1
√
V
P
p ˆapeip·x. As usual, we call

42.1
Bogoliubov’s hunting license
371
the interacting ground state of this Hamiltonian |Ω⟩. This is intended
to describe a very large number of interacting particles. For a macro-
scopic sample of matter in a laboratory this number might be around
1023. Unfortunately, the interaction part of the Hamiltonian, given by
ˆHI =
g
2V
P
kpq ˆa†
p−qˆa†
k+qˆakˆap, is too complicated to solve, so we need
an approximation. Bogoliubov came up with one by thinking about the
Nikolay Bogoliubov (1909–1982)
low-energy behaviour of the system.1 Bogoliubov’s assumption (based
1For ‘low-energy’, you might read ‘low-
temperature’ if you’re imagining an ex-
periment in a laboratory.
upon Bose–Einstein condensation) is that, if the system has very little
energy, the number of particles N0 in the p = 0 ground state of the
Hamiltonian will be macroscopically large. The consequence of this is
that instead of the exact relation ˆap|Ω⟩=
√
N 0|N0 −1⟩(where |N0 −1⟩
is a state achieved by removing one zero momentum particle from the
ground state), we can take ˆap=0|Ω⟩≈
√
N 0|Ω⟩.
Bogoliubov used this as an approximation2 to allow the replacement
2In his words, this approximation pro-
vides a ‘hunting license’ to search for
new phenomena.
of operators ˆap=0 and ˆa†
p=0 with the number √N0. He then broke down
the sum over momentum states P
kpq ˆa†
p−qˆa†
k+qˆakˆap by considering, in
turn, the terms in the sum in which particular momentum subscripts are
zero, and replacing that operator with the number √N0. Since terms
with odd numbers of operators will always give zero expectation value,
we need only consider the cases of two or four of the indices being zero.
The method then relies on sending the momentum index of pairs, or all
four, of the operators to zero. Then we replace the operators ˆa0 and ˆa†
0
with √N0.
Example 42.1
We take turns setting pairs of momentum subscripts to zero.
Replacing pairs of
subscripts yields six terms:
p = 0, k = 0,
ˆa†
−qˆa†
qˆa0ˆa0 →N0ˆa†
−qˆa†
q,
p = 0, k + q = 0,
ˆa†
−qˆa†
0ˆakˆa0 →N0ˆa†
kˆak,
p = 0, p −q = 0,
ˆa†
0ˆa†
kˆakˆa0 →N0ˆa†
kˆak,
k = 0, k + q = 0,
ˆa†
pˆa†
0ˆa0ˆap →N0ˆa†
pˆap,
k = 0, p −q = 0,
ˆa†
0ˆa†
qˆa0ˆap →N0ˆa†
pˆap,
k + q = 0, p −q = 0,
ˆa†
0ˆa†
0ˆa−qˆaq →N0ˆa−qˆaq.
(42.2)
Lastly, we replaces all four of the subscripts to get
ˆa†
0ˆa†
0ˆa0ˆa0 = N2
0 .
(42.3)
With this approximation we obtain an approximate interaction Hamil-
tonian
ˆHI ≈g
2V

N 2
0 + 4N0
X
p̸=0
ˆa†
pˆap + N0
X
p̸=0
(ˆa†
pˆa†
−p + ˆapˆa−p)

.
(42.4)
Now we must prepare ourselves for a shock. This seemingly quite reason-
able approximation of saying that there are a large number of particles

372
Superﬂuids
in the lowest momentum state has a dramatic consequence: it breaks a
symmetry. Our original Hamiltonian is invariant under the global U(1)
transformation ˆφ(x) →ˆφ(x)eiα, or equivalently ˆap →ˆapeiα. Looking
at ˆHI, we see that the terms ˆa†
pˆa†
−p and ˆapˆa−p both change if we make
the latter transformation.3 We have therefore lost the U(1) symmetry
3Note that this global transformation
doesn’t depend on the momentum.
and its conserved quantity, which (remembering back to our discussion
of global U(1) symmetry) was particle number.4
4This result is perhaps unsurprising in
that our ‘hunting license’ approxima-
tion is to equate |N −1⟩with |Ω⟩, which
shows a slightly cavalier attitude to par-
ticle number.
Since we’ve now lost the conservation of particle number, we don’t
know what value to assign to N0.
To remedy this we set the total
number of particles N equal to the number in the p = 0 ground state,
plus all the rest, that is:
N = N0 +
X
p̸=0
ˆa†
pˆap.
(42.5)
The result of these manipulations is to yield an eﬀective Hamiltonian
ˆH
=
X
p̸=0
 p2
2m + ng

ˆa†
pˆap + 1
2
X
p̸=0
ng(ˆa†
pˆa†
−p + ˆapˆa−p), (42.6)
where n = N/V and we’ve dropped the constant 1
2gn2 term.
The next problem is that the potential term in the Hamiltonian isn’t
diagonal. That is, it isn’t expressed in terms of number operators of the
form ˆb†
qˆbq. This may spell trouble unless there’s a way to turn these non-
diagonal objects into diagonal ones. That is, we want to turn objects
like ˆa†
pˆa†
−p and ˆapˆa−p into number operators.
42.2
Bogoliubov’s transformation
The way to turn these non-diagonal bilinears into number operators is
to transform to a new set of operators. The procedure that does this
is a Bogoliubov transformation which deﬁnes a new set of operators
ˆαp and ˆα†
p via

ˆap
ˆa†
−p

=

up
−vp
−vp
up
 
ˆαp
ˆα†
−p

,
(42.7)
and we will use these operators to diagonalize the Hamiltonian. The
quantities up and vp obey the following rules:
u2
p −v2
p = 1,
u∗
p = up,
v∗
p = vp,
(42.8)
which are cunningly designed so that the new operators ˆαp and ˆα†
p obey
the same commutation relations as ˆap and ˆa†
p:

ˆαp, ˆα†
q

= δpq,
[ˆαp, ˆαq] =

ˆα†
p, ˆα†
q

= 0.
(42.9)
The physical signiﬁcance of the new operators is that they describe a new
sort of excitation: a new bosonic quasiparticle we will call a bogolon.

42.2
Bogoliubov’s transformation
373
Example 42.2
We can show how the bogolon operator diagonalizes the Hamiltonian. The Hamilto-
nian can be written in matrix form as
ˆH =
X
p
“
ˆa†
p
ˆa−p
” „
ǫp
1
2 ng
1
2 ng
0
« „
ˆap
ˆa†
−p
«
,
(42.10)
where ǫp =
p2
2m + ng. We’re going to make the transformation given in eqn 42.7
and ﬁx the constants up and vp to make the resulting Hamiltonian matrix diagonal,
which is to say
ˆH =
X
p
“
ˆα†
p
ˆα−p
” „ D11
0
0
D22
« „
ˆαp
ˆα†
−p
«
.
(42.11)
Assuming that we’re successful in diagonalizing, how do we get the excitation energy,
that is the constant in front of ˆα†
p ˆαp? The trick is to look at the diagonalized form
which is
ˆH
=
X
p
h
D11 ˆα†
p ˆαp + D22 ˆα−p ˆα†
−p
i
=
X
p
h
D11 ˆα†
p ˆαp + D22(1 + ˆα†
−p ˆα−p)
i
.
(42.12)
The sum over −p’s can be re-indexed to be a sum over positive p’s and we see that
the constant in front of ˆα†
p ˆαp is D11 + D22, otherwise known as the trace of the
diagonal matrix. The trace is given by
Ep = ǫp(u2
p + v2
p) −2ngupvp.
(42.13)
The condition for the oﬀ-diagonal elements to be zero is that
2upvp
u2p + v2p
= ng
ǫp
.
(42.14)
Substituting the condition allows us to eliminate up and vp and we obtain5 the
5You are invited to verify this in Exer-
cise 42.1.
answer given in eqn 42.15.
The diagonalized Hamiltonian is given by
ˆH =
X
p
Epˆα†
pˆαp,
where
Ep =
s
p2
2m
 p2
2m + 2ng

.
(42.15)
The point of diagonalizing the Hamiltonian is to tell us about the quasi-
particle excitations from the ground state. These are the bogolons, which
represent the motion of a large number of the original interacting bosons.
The operator ˆα†
p creates a single bogolon. These bogolons don’t interact
with each other so the problem is solved.
|p|
Ep
p2
2m
 ng
m
!1
2 |p|
Fig. 42.2 The dispersion predicted by
Bogoliubov’s model.
At low momen-
tum the energy is linear in |p|, at large
momentum it is quadratic.
Turning to the bogolon dispersion, shown in Fig. 42.2, we have that
for small momenta the dispersion is linear and looks like
Ep =
ng
m
 1
2 |p|.
(42.16)
This linear dispersion occurs in another important problem: that of
phonons. In a vibrating system phonon quasiparticles are collective ex-
citations of a large number of atoms. The small-momentum bogolons

374
Superﬂuids
are similar, and they may be thought of as representing the compli-
cated, collective motion of a large number of bosons. Loosely speaking,
the original bosons have a number density whose behaviour is wave-
like and these waves are quantized into bogolons. At large momentum
the dispersion becomes Ep = p2/2m telling us that large-momentum
bogolons behave6 like free particles with a mass m.
6We say that the particles are ballistic.
Finally we note that Bogoliubov’s treatment does not predict the mini-
mum seen in experimentally in Fig. 42.1. This owes its existence in liquid
helium to the fact that the interactions in that system are very strong,
not weak, as considered here. The minimum corresponds to excitations
known as rotons, which represent a back-ﬂow of Bose particles, rather
like the motion of particles in a smoke ring.7
7See Feynman, Statistical Physics.
42.3
Superﬂuids and ﬁelds
The argument presented above, in terms of creation and annihilation op-
erators, represents a quick and illuminating way of solving Bogoliubov’s
problem. An alternative approach is to attack the problem using the
Lagrangian and the machinery of spontaneous symmetry breaking that
we developed in Chapter 26.
We started our treatment with Bogoliubov’s approximation ˆap=0|Ω⟩≈
√N0|Ω⟩. Actually we know that something very similar to this relation
holds exactly, by deﬁnition, for a coherent state. If |Ω⟩is a coherent state,
we have ˆap=0|Ω⟩=
√
N 0 eiθ0|Ω⟩. Note also that if this coherent state is
macroscopically occupied then the uncertainty in the phase ∆cos θ tends
to zero, so that the phase θ0 is a well deﬁned quantity. Thus Bogoliubov’s
‘approximation’ holds exactly for a macroscopically occupied coherent
state.
We can also understand how the occurrence of such a state necessarily
implies broken symmetry. In real space (and ignoring time variation for
now) the annihilation ﬁeld ˆΦ(x) =
1
√
V
P
p ˆapeip·x has a coherent state
eigenstate |φ(x)⟩deﬁned such that
ˆΦ(x)|φ(x)⟩=
p
ρ(x)eiθ(x)|φ(x)⟩,
(42.17)
where ρ(x) is a number density of particles.
Substituting the mode
expansion shows that ˆΦ(x) has the property, for the coherent ground
state |Ω⟩, that
⟨Ω|ˆΦ(x)|Ω⟩
=
1
√
V
X
p
⟨Ω|ˆap|Ω⟩eip·x
=
1
√
V
X
p
p
Npeiθpeip·x,
(42.18)
but since, for |Ω⟩, only the p = 0 state is occupied with any sizeable
probability, this term dominates the sum and we have
⟨Ω|ˆΦ(x)|Ω⟩= √n eiθ0,
(42.19)

42.3
Superﬂuids and ﬁelds
375
where √n =
p
Np=0/V. An important point here is that the ﬁeld ˆΦ(x)
has developed a ground state with a nonzero vacuum expectation value,
which is a tell-tale sign of a broken symmetry state. In fact we take
⟨Ω|ˆΦ(x)|Ω⟩as the order parameter of the system. We also read oﬀfrom
eqn 42.17 that in the ground state |Ω⟩the number operator expectation
value is
p
ρ(x) = √n across the sample and that the state’s phase is
θ(x) = θ0. We stress that while there is still uncertainty in the number
n in a macroscopically occupied coherent state, the uncertainty in the
phase θ0 is zero. Thus, the breaking of symmetry involves ﬁxing the
phase to θ(x) = θ0 across the entire system. Clearly it is this picking of
a unique phase that breaks the U(1) symmetry of the system.8
8The breaking of symmetry is often
represented pictorially by drawing a
set of compass needles which depict
θ(x): a phase angle which may be dif-
ferent at every point in space.
The
breaking of symmetry, and formation
of a superﬂuid phase, corresponds to
all of the needles lining up, as shown
in Fig. 42.3(a). This picture of a super-
ﬂuid is known as the XY model, where
the name is motivated by the freedom
of the needles to point in the x-y plane
in the normal state.
How do we end up with a Lagrangian that describes non-relativistic
bosons which is unstable to symmetry breaking? A system containing
a ﬁxed number of non-interacting, non-relativistic bosons has a Hamil-
tonian ˆH0 = (Ep −µ)ˆa†
pˆap, with Ep = p2/2m and where µ is the
chemical potential. Translating back into position space ﬁelds we have a
Lagrangian for the non-relativistic system, including chemical potential,
given by
L = iΦ†∂0Φ −1
2m∇Φ† · ∇Φ + µΦ†Φ.
(42.20)
This is similar to the form we had in Chapter 12. The interaction is
included by subtracting a term g
2(Φ†Φ)2.
Fig. 42.3 (a) Order in the superﬂuid
corresponds to a uniform phase angle
across the entire system. This is repre-
sented on a lattice here. (b) A gradient
in the phase ∇θ(x) results in a current
in the superﬂuid.
Example 42.3
The full, interacting Lagrangian is
L = iΦ†∂0Φ −
1
2m ∇Φ† · ∇Φ + µΦ†Φ −g
2 (Φ†Φ)2,
(42.21)
which, with its positive mass-like term µΦ†Φ, is unstable to spontaneous symme-
try breaking. The Lagrangian may be helpfully rewritten (to within an ignorable
constant) as
L = iΦ†∂0Φ −
1
2m∇Φ† · ∇Φ −g
2
“
n −Φ†Φ
”2
,
(42.22)
and remembering that the number density of particles is given by Φ†Φ, we interpret
n = µ/g as the boson density of the ground state.
The Lagrangian for non-relativistic bosons with a short ranged repulsive
interaction is therefore given by
L = iΦ†∂0Φ −1
2m∇Φ† · ∇Φ −g
2(n −Φ†Φ)2,
(42.23)
which is unstable to symmetry-breaking.
To see the consequences of
broken symmetry we will use the polar coordinates of Chapter 12, so
that the ﬁeld looks like Φ(x) =
p
ρ(x)eiθ(x), and so is described by an
amplitude-ﬁeld part ρ(x) and a phase-ﬁeld part θ(x). Our Lagrangian
becomes9
9Note that Φ† = √ρ e−iθ and so
∂0Φ = i∂0θ√ρ eiθ +
1
2√ρ eiθ∂0ρ,
resulting in
Φ†∂0Φ = iρ∂0θ + 1
2∂0ρ.
L = −ρ∂0θ −1
2m
 1
4ρ(∇ρ)2 + ρ(∇θ)2

−g
2(n −ρ)2,
(42.24)

376
Superﬂuids
where we have dropped the term i∂0ρ/2 as it is a total derivative and
should therefore give a vanishing contribution to the action since our
ﬁelds are deﬁned to vanish at ±∞.
This Lagrangian has a Mexican hat potential (Fig. 42.4) which has a
minimum at ρ = n and is invariant with respect to global U(1) trans-
formations of phase. That is, we can swap θ(x) →θ(x) + α and the
Lagrangian is the same as long as α is the same at every point in space-
time. The possible ground states of the theory are therefore to be found
at Φ(x) =
p
ρ(x)eiθ(x) = √neiθ0, where θ0 is the same at every point
in space, but may itself take any value. We break the global U(1) sym-
metry of the theory by ﬁxing the phase of the ground state to be at
a particular value, say θ0 = 0 as shown in Fig. 42.4(c). Breaking this
continuous U(1) phase symmetry will result in the emergence of a Gold-
stone mode, i.e. an excitation which costs vanishingly little energy to
excite at low momentum.
(a)
(b)
(c)
Re Φ
Im Φ
U(|Φ|)
Re Φ
Im Φ
Re Φ
Im Φ
θ
Fig. 42.4 (a) The Mexican hat poten-
tial in the superﬂuid problem. There is
one of these at every point x in space.
(b) The potential viewed from above as
contours, with a minimum at ρ = n.
Each value of θ is equivalent in energy.
(c) Upon symmetry breaking the vac-
uum state picks a unique value of θ
(here θ = 0) at every point across the
system.
To ﬁnd the Goldstone mode we use a method much beloved of con-
densed matter physicists. We are going to remove the modes that cost
a lot of energy.10 This will leave behind a theory that only includes
10Particle
physicists
sometimes
call
this integrating out the high-energy
particles.
the low-energy excitations, which (we hope) will include the Goldstone
modes. Staring at the form of the potential, we see that excitations that
involve climbing the wall of the potential by changing ρ will cost more
energy than those involving rolling around in the gutter (changing θ).
We therefore examine the excitations that arise from departures from
the ground state by considering an excited state ﬁeld
p
ρ(x) = √n + h.
This species of excited state just involve climbing the wall slightly from
the ground state. We use these excited states to expand the Lagrangian,
which yield (on dropping11 the total time derivative −n∂0θ):
11This is a Berry phase term which re-
veals some interesting physics in the
presence of vortices.
See Wen, Sec-
tion 3.6.
L = −1
2m(∇h)2 −2gnh2 −
 2√n∂0θ

h −n
2m(∇θ)2 + . . .
(42.25)
To remove the energetic states we may plug a Lagrangian into a path
integral and integrate out the variable describing these states. We will
use this technique to remove the amplitude h from our Lagrangian.
Example 42.4
We’ll make use of two neat functional integral tricks here.
Remember that when
dealing with the Lagrangian in a path integral, we can make the integrate-by-parts
substitution to the quadratic parts:
(∂µφ)2 −m2φ2 →−φ
`
∂2 + m2´
φ,
(42.26)
which for our equation in terms of h requires
−1
2m(∇h)2 −2gnh2 →−h
„
−1
2m∇2 + 2gn
«
h.
(42.27)
Next we use the one functional integral we can do, i.e.
R
Dφ e
i
2
R
φ ˆ
Kφ+i
R
Jφ =
e−1
2
R
J(i ˆ
K−1)J, which allows us to make the replacement
1
2 φ
ˆ
−(∂2 + m2)
˜
φ + Jφ →1
2 J(x)
1
∂2 + m2 J(y).
(42.28)

42.4
The current in a superﬂuid
377
Setting φ = −
√
2h and J =
√
2n∂0θ, we have
h
„
−1
2m∇2 + 2gn
«
h −
`
2√n∂0θ
´
h →√n∂0θ
1
(−1/2m)∇2 + 2gn
√n∂0θ. (42.29)
Our Lagrangian becomes
L = n∂0θ
1
2gn −
1
2m∇2 ∂0θ −n
2m(∇θ)2 + . . .
(42.30)
To obtain the low-energy, small-momentum behaviour we treat the en-
ergy density term (1/2m)∇2 as small compared to the potential density
term 2gn and we ﬁnd a Lagrangian purely describing the low-energy
physics
L
=
1
2g
∂θ
∂t
2
−n
2m (∇θ)2 .
(42.31)
This is merely a wave equation Lagrangian with a linear dispersion and
a wave speed c =
p gn
m . We have found our Goldstone mode: the θ ﬁeld
has no mass term, so the energy of excitations tends to zero as p →0.
Our manipulations have resulted in a low-energy dispersion
Ep =
ng
m
 1
2 |p|,
(42.32)
just as we had before. We’ve shown through another method that break-
ing the U(1) symmetry gives the linearly dispersing mode at low mo-
mentum.
42.4
The current in a superﬂuid
Noether’s theorem: U(1) internal
symmetry
for
the
low-energy
theory
Dθ = 1
Π0
θ = 1
g ∂0θ
Πi
θ = n
m∂iθ
DL = 0
W µ = 0
J0
Nc = −1
g ∂0θ
JNc = n
m ∇θ
Let’s examine the fate of Noether’s symmetry current for the low-energy
theory. Looking at the Lagrangian in eqn 42.31, we see that the U(1)
translation θ(x) →θ(x) + α leaves the function invariant.12 This is
12See Chapter 12 for a reminder.
merely the U(1) symmetry that is spontaneously broken in the ground
state.
Applying Noether’s theorem to this symmetry we have that
Dθ(x) = 1 and
Π0
θ = 1
g∂0θ,
Πi
θ = n
m∂iθ.
(42.33)
As a result, we have the conserved currents
J0
Nc(x) = −1
g∂0θ(x),
JNc(x) = n
m∇θ(x).
(42.34)
This ﬁnal equation tells us that gradients in the phase in a superﬂuid
[as shown in Fig. 42.3(b)] result in currents.
The broken symmetry
ground state of the superﬂuid has a uniform phase and if we deform the
phase we set up a current in the system along the gradient. Notice that
the current JNc depends on the strength of the condensate (or vacuum
expectation value) given by n. In the absence of symmetry breaking,

378
Superﬂuids
this is zero, so it makes sense that this is indeed the current that results
from symmetry breaking.
What does this tell us about the dissipationless ﬂow of momentum
which, after all, is one of the most striking aspects of a superﬂuid? We
will use a beautiful argument based on topology that demonstrates that
a moving superﬂuid can’t dissipate momentum. That is, it’s a runaway
train that can’t slow down! The superﬂuid’s broken symmetry ground
state is a constant ﬁeld Φ = √neiθ0. We imagine putting our superﬂuid
in a box of size L with periodic boundaries Φ(x = 0) = Φ(L). Now we
boost the superﬂuid so it has a momentum P. This changes the ground
state Φ →eiP xΦ = √nei(θ0+P x). Notice that the boost eﬀectively twists
the phase of our ﬁeld. The boost also twists the boundary conditions;
instead of Φ(0) = Φ(L), they become Φ(0)eiP L = Φ(L). This is shown
in Fig. 42.5(a). To satisfy the periodic boundary conditions, we must
have PL = 2πw, where w is an integer (known as the winding number,
as discussed in Chapter 29).
(a)
(b)
(c)
w = 1
w = 1
w = −1
Fig. 42.5
(a) The superﬂuid with a
w = 1 phase twist.
Any attempt to
untwist the phase would tear the su-
perﬂuid. (b) The same superﬂuid and
a w = −1 vortex propagating to the
right. (c) The propagation of the vor-
tex unwinds the twist allowing the su-
perﬂuid to dissipate momentum.
The nonzero momentum state has more energy than the ground state.
If the superﬂuid can slow down it will lower its energy. Now we ask, can
we untwist the superﬂuid phase to remove this extra momentum? The
answer is no, we can’t. The nonzero momentum state involves an integer
number of 2π rotations of the phase angle between the ends of the box.
The angles at the ends of the box are locked to each other by the periodic
boundary conditions, so removing the twists will lead to singularities in
the phase ﬁeld, costing enormous amounts of energy.
The only way we could remove a twist is to create a vortex, a localized
phase twist. Such vortices carry integer winding numbers, so creating
a vortex which travels across the box will remove a twist as shown in
Fig. 42.5(b).13 The creation of a vortex is a tunnelling process with a ﬁ-
13We have considered annihilating our
broad phase twist in Fig. 42.5(a) with
a vortex in Fig. 42.5(b). The broadness
of the phase twist and the localized na-
ture of the vortex are irrelevant. What
matters is their topological character,
namely that one has w = 1 and the
other has w = −1 so that they annihi-
late each other.
nite energy barrier. Ignoring such subtleties would lead us to believe that
a moving superﬂuid is a perpetual motion machine. However, Nature
is not so kind and allows a quantum tunnelling process which unwinds
the condensate. As a result, the superﬂuid can, over a very long period
(and even in the absence of excitations) dissipate some momentum.
Our model of weakly interacting bosons captures the small and large
momentum behaviour of the dispersion, so explains at least some of the
physics of the excited superﬂuid. One might question whether this dis-
persion is essential to the existence of the superﬂuid state. It is essential
since it guarantees that the moving ﬂuid can’t dissipate too much mo-
mentum through its excitations. The reason is that a linear dispersion
doesn’t give rise to enough states for quasiparticles to scatter into and
dissipate momentum. To see this consider the density of states g(E) for
a three-dimensional ﬂuid of bosons. For the linear dispersion of a su-
perﬂuid, this varies as g(E) ∝E2, whereas for the quadratic dispersion
of a normal particle, this varies as g(E) ∝E
1
2 . As a result, there are
fewer excited states available for a superﬂuid than a normal ﬂuid at low
values of energy, reducing the phase space for scattering events that will
reduce momentum.

Exercises
379
Example 42.5
It is also worth noting another beautiful argument by Landau that shows an upper
bound on the critical velocity for superﬂuid ﬂow is given by (ng/m)
1
2 , a quantity
which only exists due to the presence of interactions. In the simplest version of this
argument, we imagine the superﬂuid as a body of mass M moving with velocity
v.
The body creates an excitation, with momentum p and energy Ep, with the
result that the body’s velocity changes to v′. By conservation of momentum, we
have Mv = Mv′ + p, from which we see that the body ends up with kinetic energy
Mv′2
2
= Mv2
2
−v·p+ p2
2M . The point is that the excitation cannot be created unless
we have Mv2
2
> Mv′2
2
+ Ep, from which we conclude that
Ep < v · p −p2
2M ≈v · p,
(42.35)
if M is assumed large. In other words, we have an upper bound on the superﬂow
against creating excitations, given by Ep/|p| = (ng/m)
1
2 . So while a non-interacting
Bose gas (which has g = 0) will undergo a transition into a Bose–Einstein condensate
at low temperatures, such a state will not exhibit a superﬂow as the critical velocity
is zero.
Chapter summary
• Bogoliubov’s approximation states that the number of particles in
the p = 0 ground state becomes macroscopically large, allowing the
replacement of operators ˆap=0 and ˆa†
p=0 with the number √N0.
• The eﬀective Hamiltonian in a non-relativistic Bose gas can be
diagonalized using a Bogoliubov transformation, and the quasi-
particles are called bogolons.
• The low-energy excitations in a superﬂuid are Goldstone modes,
the broken symmetry is the phase θ and the superﬂuid current is
JNc = n
m∇θ(x).
Exercises
(42.1) Verify eqn 42.15 using the method suggested in the
text. You may ﬁnd it useful to make the substitu-
tions
up
=
cosh θp,
vp
=
sinh θp.
(42.36)
(42.2) Verify the algebra leading to eqn 42.31.
(42.3) (a) Show that, in three dimensions, the density of
states g(E) = dN(E)/dE for a superﬂuid goes as
g(E) ∝E2.
(b) Show that for a Fermi gas g(E) ∝E
1
2 .

43
The many-body problem
and the metal
43.1 Mean-ﬁeld theory
380
43.2 The
Hartree–––Fock
ground
state energy of a metal
383
43.3 Excitations in the mean-ﬁeld
approximation
386
43.4 Electrons and holes
388
43.5 Finding the excitations with
propagators
389
43.6 Ground
states
and
excita-
tions
390
43.7 The random phase approxi-
mation
393
Chapter summary
398
Exercises
398
The sentries report Zulus to the south west. Thousands of
them.
Colour Sergeant Bourne (Nigel Green), Zulu (1964)
The physics of large numbers of non-relativistic particles and their in-
teractions is a branch of quantum ﬁeld theory known as the many-body
problem. In this chapter we’ll examine the properties of a large number
of non-relativistic fermions conﬁned in a box, which is the basis of the
description of electrons in solids. As we’ve seen, non-relativistic fermions
scatter via the diagram shown in Fig. 43.1. The full formalism of Dirac
spinors isn’t required to understand this problem owing to the low en-
ergies involved in metals and so we can treat the particles as interacting
via an instantaneous potential V (x−y) = P
q eiq·(x−y) ˜Vq. Rather than
starting with propagators and path integrals, we’ll gain some intuition
by treating the interaction with some very simple approximations.
p
k
q
p −q
k + q
Fig. 43.1 The Coulomb vertex. Note
that in this chapter the transferred mo-
mentum runs from left to right.
43.1
Mean-ﬁeld theory
We start with the Hamiltonian
ˆH = ˆH0 + ˆV ,
(43.1)
where ˆH0 = P
p
p2
2mˆa†
pˆap and
ˆV = 1
2
X
pkq
˜Vqˆa†
p−qˆa†
k+qˆakˆap,
(43.2)
where the anticommuting operators ˆa†
p and ˆap create and destroy
fermions respectively. We take ˆH0 as the dominant contribution and
treat ˆV as a perturbation. As usual we call |0⟩the ground state of H0
with energy E0 and |Ω⟩the ground state of the full Hamiltonian of H
with energy E.1
1It is worth noting that |0⟩doesn’t
mean that we have no particles in
the ground state.
Usually in con-
densed matter physics we study sys-
tems with a ﬁnite density of parti-
cles at zero temperature.
The non-
interacting ground state for a metal is
a gas of electrons stacked up in en-
ergy up to the Fermi level pF.
So
if we take spin into account then the
ground state of a metal may be written
|0⟩= Q
|p|<pF ˆa†
p↑ˆa†
p↓|Empty box⟩.
We need a way to deal with the perturbation provided by the com-
plicated two-body potential ˆV .
A good ﬁrst step is to ask what the
mean-ﬁeld behaviour of the system is. Mean-ﬁeld theory is a scheme
of approximations that crops up all over physics and involves taking
a system of many particles and asking how a single particle reacts to
the average behaviour of all of the others. In the context of many-body

43.1
Mean-ﬁeld theory
381
theory, we will interpret the mean-ﬁeld approximation as a method of re-
placing pairs of operators with averages, or more correctly, with vacuum
expectation values (VEVs), and we will later link this to the conven-
tional idea of mean-ﬁeld theory. The mean-ﬁeld correction ∆E to the
non-interacting ground state energy E0 is found by taking a VEV of the
perturbation, which is to say we approximate the expectation value of
the troublesome four-operator term taken with the unperturbed ground
state:2 ⟨0|ˆa†
p−qˆa†
k+qˆakˆap|0⟩.
2This is exactly how we usually work
out the energy shift caused by a pertur-
bation in ﬁrst-order perturbation the-
ory, so represents nothing new.
Nov-
elty comes with the next step, where
this VEV is boiled down to something
more useful using a version of Wick’s
theorem.
Example 43.1
Wick’s theorem reduces the four operators in eqn (43.2) into products of contractions
of pairs of (normally ordered) operators.3 We therefore apply
3Since we’re dealing with fermion oper-
ators in this chapter then we need to re-
member that swapping the order of two
operators when we bring them together
and make a VEV, gives us a minus sign.
ˆa†
p−qˆa†
k+qˆakˆap →
Y
N
»
all paired
contractions
–
.
(43.3)
Wick’s theorem on our string of four operators yields
ˆa†
p−qˆa†
k+qˆakˆap = N
h
ˆa†
p−qˆa†
k+qˆakˆap
i
+
ˆa†
p−qˆa†
k+q N [ˆakˆap] + N
h
ˆa†
p−qˆa†
k+q
i
ˆakˆap
+
ˆa†
p−qˆap N
h
ˆa†
k+qˆak
i
+ N
h
ˆa†
p−qˆap
i
ˆa†
k+qˆak
−
ˆa†
p−qˆakN
h
ˆa†
k+qˆap
i
−N
h
ˆa†
p−qˆak
i
ˆa†
k+qˆap
+
ˆa†
p−qˆa†
k+qˆakˆap + ˆa†
p−qˆapˆa†
k+qˆak −ˆa†
p−qˆakˆa†
k+qˆap.
(43.4)
Everything within the normal ordering signs is normally ordered as written, so these
can be dropped. Mean-ﬁeld theory involves replacing the contractions with averages
taken over the ground state: ⟨0| ˆO|0⟩.
The terms with uncontracted operators represent excitations, and so
we ignore them for now, since we’re concerned with the ground state
properties.
For the ground state energy shift ⟨ˆV ⟩, we therefore only
consider the completely contracted terms and this procedure therefore
yields up the three terms:
1
2
X
pkq
˜Vq

⟨ˆa†
p−qˆa†
k+q⟩⟨ˆakˆap⟩
|
{z
}
leads to C0
+ ⟨ˆa†
p−qˆap⟩⟨ˆa†
k+qˆak⟩
|
{z
}
leads to D0
−⟨ˆa†
p−qˆak⟩⟨ˆa†
k+qˆap⟩
|
{z
}
leads to F0


= C0 + D0 + F0,
(43.5)
where we’ve shortened ⟨0| ˆO|0⟩to ⟨ˆO⟩to save on clutter. We’ll call C0
the Cooper term,4 D0 is Hartree’s direct term and F0 Fock’s exchange
4The Cooper term will be important
when we come to deal with supercon-
ductors in the next chapter. We’ll post-
pone discussing it until then and, for
the moment, use the fact that under
normal circumstances, the combination
⟨0|ˆa†
nˆa†
m|0⟩= 0 for any n and m, and
so we expect no contribution from C0.
Douglas Hartree (1897–1956)
Vladimir Fock (1898–1974)
term.
Let’s examine the Hartree term in more detail.
It gives rise to a
contribution to the ground state energy of
D0
=
1
2
X
pkq
˜Vq⟨ˆa†
p−qˆap⟩⟨ˆa†
k+qˆak⟩.
(43.6)

382
The many-body problem and the metal
We know that ⟨0|ˆa†
rˆas|0⟩= 0 unless r = s, so we can immediately ﬁx
q = 0 in eqn 43.6. Recall that the ˜Vqs are Fourier components of the real
space potential V (r). A wave with wave vector q = 0 is a constant (that
is, it has an inﬁnitely long wavelength), so the Hartree term evaluates
the energy due to the constant part of the potential. Maybe this is what
we would expect from a ﬁrst-order guess! We obtain
D0
=
1
2
X
pk
˜Vq=0⟨ˆa†
pˆap⟩⟨ˆa†
kˆak⟩.
(43.7)
Notice that we’ve reduced the VEVs of operator pairs to simple number
operators and the Hartree term has become
D0 = 1
2
˜Vq=0
 X
p
⟨0| ˆNp|0⟩
!2
.
(43.8)
The mean-ﬁeld treatment has a diagrammatic interpretation: we can
take the interaction diagram and join the legs of the operators that we’re
averaging (Fig. 43.2). This turns out to be equivalent to the more usual
ways of getting diagrams with the S-matrix or path integral approach.
The Hartree term can therefore be represented in diagrammatic form as
the double-headed tadpole shown in Fig. 43.2(b).
(a)
(b)
(c)
p
k
p
k
q = 0
q
ap
a†
p−q
ak
a†
k+q
q = p −k
Fig. 43.2
(a) The basic interaction
diagram.
(b) The double tadpole di-
agram giving the Direct (or Hartree)
contribution to the ground state energy.
(c) The double oyster diagram giving
the Exchange (or Fock) contribution to
the ground state energy.
Another point to note is that for theories where there are no par-
ticles in the ground state |0⟩the tadpole [whose head corresponds to
⟨0|ˆa†
pˆap|0⟩, see Fig. 43.2(b)] gives zero. This isn’t the case for nuclear or
electronic matter however, where we have a ﬁnite number of fermions in
the ground state.
Example 43.2
Let’s see how the Hartree term looks in terms of position space ﬁelds. We examine
the product ⟨ˆa†
p−qˆap⟩We have that ˆap =
1
√
V
R
d3x ˆψ(x)e−ip·x, which enables us to
say
⟨ˆa†
p−qˆap⟩= 1
V
Z
d3xd3x′ ⟨ˆψ†(x) ˆψ(x′)⟩ei(p−q)·xe−ip·x′.
(43.9)
This in turn allows us to write the Hartree energy as
D0
=
1
2V2
X
p,k,q
Z
d3xd3x′d3yd3y′ ˜Vq⟨ˆψ†(x) ˆψ(x′)⟩⟨ˆψ†(y) ˆψ(y′)⟩(43.10)
×ei(p−q)·xe−ip·x′ei(k+q)·ye−ik·y′.
Doing the sums over p and k yields Vδ(3)(x −x′) and Vδ(3)(y −y′) respectively,
which eat up two of the space integrals. We’re left with
D0
=
1
2
X
q
Z
d3xd3y ˜Vq⟨ˆψ†(x) ˆψ(x)⟩⟨ˆψ†(y) ˆψ(y)⟩e−iq·(x−y).
(43.11)
Next we sum over q, which takes an inverse Fourier transform of the potential ˜Vq,
and we obtain
D0
=
1
2
Z
d3xd3y V (x −y)⟨ˆψ†(x) ˆψ(x)⟩⟨ˆψ†(y) ˆψ(y)⟩.
(43.12)
This is just what one might have guessed classically for the energy of two charge
distributions ρ(x) and ρ(y) interacting via a potential V (x −y).

43.2
The Hartree–––Fock ground state energy of a metal
383
Of course, decompressing the notation here
⟨ˆψ†(x) ˆψ(x)⟩= −
lim
t′ →0−
x′ →x
⟨0|T ˆψ(t′, x′) ˆψ†(0, x)|0⟩,
(43.13)
is related to a propagator.5 The internal lines that form the double
5See also eqn 31.18.
tadpole’s two heads are just propagators, as we expect for a respectable
Feynman diagram.
Next we examine the Fock term. This is represented by the double
oyster diagram in Fig. 43.2(c) which corresponds to the equation
F0 = −1
2
X
pkq
˜Vq⟨ˆa†
p−qˆak⟩⟨ˆa†
k+qˆap⟩.
(43.14)
Example 43.3
Again, in order to make the VEVs nonzero, this gives us a condition on q. This time
we have that p −q = k, leading to a result
F0
=
−1
2
X
pk
˜Vp−k⟨ˆa†
kˆak⟩⟨ˆa†
pˆap⟩
=
−1
2
X
pk
˜Vp−k⟨0| ˆ
Nk|0⟩⟨0| ˆ
Np|0⟩,
(43.15)
which is diagonal (that is, contains only number operators) but is clearly more com-
plicated than what we had for the Hartree term. This is shown diagrammatically
as the double oyster in Fig. 43.2(c). There is a lot of physics in the exchange term
including, for example, magnetic ordering in metals, as examined in the exercises.
The position space version unfolds much as before, although now the momentum
sums mix up x’s and y’s, giving us
F0 = −1
2
Z
d3xd3y V (x −y)⟨ˆψ†(x) ˆψ(y)⟩⟨ˆψ†(y) ˆψ(x)⟩.
(43.16)
43.2
The Hartree–––Fock ground state
energy of a metal
We will use our mean-ﬁeld results to work out an approximate value
of the total ground state energy of a metal. This will involve potential
energy contributions from the Hartree term and the Fock term and this
is therefore known as the Hartree–Fock ground state energy.6
6The other contribution is from ˆH0 and
reﬂects fact that each electron has ki-
netic energy E(0)
p
= p2
2m and obeys the
Pauli principle. Our approach will be
to add these kinetic energies up by not-
ing that the sums are carried out up
to pF and that states are so closely
spaced that we may make the replace-
ment P
|p|<pF →V
R
|p|<pF
d3p
(2π)3 .
Example 43.4
Recall that we put non-interacting electrons into a box, with one spin-up and one
spin-down electron in each momentum state. The electrons stack up in energy and the
upper-most occupied energy level is called the Fermi level. The number of occupied
momentum states is
X
|p|<pF
→V
Z
|p|<pF
d3p
(2π)3 = V
Z pF
|p|=0
(4π) d|p|
(2π)3 |p|2 = Vp3
F
6π2 ,
(43.17)

384
The many-body problem and the metal
and with two spin states per momentum level, the total number of electron states is
N = Vp3
F/3π2. The electron density of the system n = N/V is related to the Fermi
momentum pF via pF = (3π2n)
1
3 , and the Fermi energy is EF = p2
F/(2m). The total
energy of the box of non-interacting electrons is given by
W0
=
2
X
|p|<pF
E(0)
p
= 2V
Z
|p|<pF
d3p
(2π)3
p2
2m = 3
5NEF,
(43.18)
and so the kinetic energy per electron W0/N =
3
5 EF. A popular unit for energy
among theoreticians is the Rydberg (Ry), equal to ℏ2/(2mea2
0) where a0 is the Bohr
radius.
Writing the volume occupied per electron as 1/n =
4
3 πr3 where r is the
average distance per particle, and using the dimensionless length rs = r/a0, we can
express the kinetic energy per electron as
W0
N
= 3
5EF = 3
5
„9π
4
«2/3 1 Ry
r2s
≈2.21
r2s
Rydbergs
electron .
(43.19)
Now for the potential energy. This is not something we can calculate ex-
actly, but our mean-ﬁeld approach tells us that to ﬁrst order we should
add the Hartree term D0 and Fock term F0. However an immediate
simpliﬁcation is possible.
Metals may be modelled as charge-neutral
boxes full of electrons interacting via the Coulomb force.
Although,
in reality, metals contain positive ion cores and mobile electrons, this
level of detail is not always necessary. Instead, we use a model known7
7Named by John Bardeen.
as jellium in which the electrons are put in a box full of a homoge-
nous, positively charged jelly in order to guarantee charge neutrality.
This uniform positive charge exactly cancels the contribution from the
Hartree term which originates from the uniform electron distribution.
We therefore only need consider the Fock contribution to the energy
given by F0 = −1
2
P
pk ˜Vp−k⟨ˆNp⟩⟨ˆNk⟩. The ingredients of the sum are
the ground state occupation numbers
⟨ˆNp⟩=
(
1
|p| ≤pF
0
|p| > pF,
(43.20)
and the Fourier transform of the interaction potential ˜Vp.
Example 43.5
The electrostatic potential energy between two electrons is given by8 V (x −y) =
8Here we return to SI units, since these
are often used in the many-body liter-
ature.
e2
4πǫ0|x−y|.
The Fourier transform can be most easily evaluated by working with
the screened potential energy V (r) = (e2/4πǫ0)e−λ|r|/|r| and then sending λ →0.
Thus9
9The integral was evaluated in Chap-
ter 17.
˜Vq = lim
λ→0
1
V
e2
4πǫ0
Z
d3r e−iq·re−λ|r|
|r|
= lim
λ→0
e2
Vǫ0(q2 + λ2) =
e2
Vǫ0q2 .
(43.21)

43.2
The Hartree–––Fock ground state energy of a metal
385
Now to evaluate the Fock oyster term. It will be useful for what is to
follow if we arrange the terms as follows (including a factor of 2 for spin):
F0
=
2
X
|p|<pF
1
2

−
X
|k|<pF
˜Vp−k


=
2
X
|p|<pF
1
2

−
X
|k|<pF
e2
Vǫ0
1
|p −k|2

.
(43.22)
The part in the square bracket corresponds to the oyster diagram part
shown in Fig. 43.3 and is known as the Fock self-energy10 ˜Σ(F)
p .
10We will see this again in the next sec-
tion.
p
p
k
p −k
Fig. 43.3
The Fock contribution to
the self-energy Σ(F)
p
.
Example 43.6
The Fock self-energy may be evaluated as follows:11
11Use has been made of the integral
R
|k|<pF
d3k
|p−k|2
=
R pF
0
2πk2 dk
R π
0
sin θ dθ
k2+p2−2kp cos θ
= 2π
p
R pF
0
k dk ln
˛˛˛ k+p
k−p
˛˛˛
= 2πpF F
“ |p|
pF
”
.
˜Σ(F)
p
= −e2
ǫ0
Z
|k|<pF
d3k
(2π)3
1
|p −k|2 = −pF
π
„ e2
4πǫ0
«
F
„ |p|
pF
«
,
(43.23)
where
F(x) = 1 + 1 −x2
2x
ln
˛˛˛˛
1 + x
1 −x
˛˛˛˛ .
(43.24)
The function F(x) is shown in Fig. 43.4. It has an inﬁnite slope at x = 1 correspond-
ing, in our example, to the Fermi momentum.
Ep
|p|/pF
x
F(x)
0
1
2
1
2
(a)
(b)
Fig.
43.4
(a) The function F(x).
(b) The energy of an electron in the
Hartree–Fock
approximation
Ep
=
p2
2m + ΣF
p.
The Fock energy is given (ﬁnally) by
F0
=
2
X
|p|<pF
1
2
˜Σ(F)
p
= −V pF
π
 e2
4πǫ0
 Z
|p|<pF
d3p
(2π)3 F
|p|
pF

=
−3NpF
2π
 e2
4πǫ0
 Z 1
0
dx x2F(x).
(43.25)
The integral yields 1/2 and we obtain the potential energy per electron
F0
N = −3pF
4π
e2
4πǫ0
= −0.916
rs
Rydbergs
electron .
(43.26)
Putting this together with the kinetic energy yields the total Hartree–
Fock energy per electron of jellium metal
EHF
N
=
2.21
r2s
−0.916
rs
 Rydbergs
electron .
(43.27)
Although this is as far as our mean-ﬁeld theory will get us, this equation
has the appearance of the start of a series in powers of rs and we expect
additional terms to add to this series. These terms were given the name
correlation energy12 by Eugene Wigner and Frederick Seitz. The next
12Feynman suggested it could be called
the stupidity energy.
Eugene Wigner (1902–1995) made fun-
damental contributions to many areas
of physics and mathematics. His sister
was married to Dirac, who would often
refer to her as ‘Wigner’s sister’.
Frederick Seitz (1911–2008) has ar-
guably the best claim to be the found-
ing father of solid state physics.
two terms were calculated by Gell-Mann and Brueckner in 1957 with the
Keith Brueckner (1924– )
result
E
N =
2.21
r2s
−0.916
rs
−0.094 + 0.0622 ln rs
 Rydbergs
electron .
(43.28)

386
The many-body problem and the metal
43.3
Excitations in the mean-ﬁeld
approximation
Next we examine the changes the interactions can make to the dispersion
relations of single particles in the metal. For the moment we will return
to the simple minded picture of a box of fermions, which does not include
jellium’s positive background.
The excitations of a quantum ﬁeld theory are particles, which have a
characteristic dispersion Ep. To examine the excitations of a ﬁeld theory
we want to manoeuvre our Hamiltonian into the form ˆH = P
p Epˆa†
pˆap.
So when we examine the fate of particles in mean-ﬁeld theory we try
to massage the two-particle interaction, described by the four operators
in eqn 43.2, into one that describes a single particle interacting with an
external ﬁeld. The external ﬁeld here is caused by the eﬀect of all of the
other particles, which (ﬁnally) explains the logic of naming this proce-
dure a ‘mean ﬁeld’ approximation. Recall from Chapter 4 that a single
particle in an external potential is described by an operator ˜Vqˆa†
p+qˆap,
so we want to turn the tricky term ˆa†
p−qˆa†
k+qˆakˆap into a sum of terms
that look like this. Actually we want to go further than this and have
incoming and outgoing electrons residing in the same momentum state
giving P
p ˜V MF
p
ˆa†
pˆap, where ˜V MF
p
is the eﬀective mean-ﬁeld potential
through which the particle with momentum p passes. We do this with
a recipe: we use Wick’s theorem to generate terms containing the VEV
of two of the four operators and treat this VEV part as an eﬀective
potential.
Example 43.7
Returning to our Wick expansion of the operator we identify the terms with one
contraction and one pair of uncontracted operators. We obtain a sum of terms:
D
=
⟨ˆa†
k+qˆak⟩ˆa†
p−qˆap + ⟨ˆa†
p−qˆap⟩ˆa†
k+qˆak,
F
=
−⟨ˆa†
k+qˆap⟩ˆa†
p−qˆak −⟨ˆa†
p−qˆak⟩ˆa†
k+qˆap,
C
=
⟨ˆa†
k+qˆa†
p−q⟩ˆapˆak + ⟨ˆapˆak⟩ˆa†
k+qˆa†
p−q.
(43.29)
We will continue to ignore the Cooper terms for now. The two terms in the D sum
are identical after a re-indexing of the sums, and the same is true of the two terms
in the F sum. We end up with
D
=
2⟨ˆa†
k+qˆak⟩ˆa†
p−qˆap,
F
=
−2⟨ˆa†
p−qˆak⟩ˆa†
k+qˆap.
(43.30)
We now have a Hamiltonian that reads
ˆH =
X
p
p2
2mˆa†
pˆap +
X
qpk
˜Vq⟨ˆa†
k+qˆak⟩ˆa†
p−qˆap −
X
qpk
˜Vq⟨ˆa†
p−qˆak⟩ˆa†
k+qˆap.
(43.31)

43.3
Excitations in the mean-ﬁeld approximation
387
(a)
(b)
(c)
=
+
+
+
+ · · ·
+
+
+
+ · · ·
p
k
q
p −q
k + q
p
k
q
p −q
k + q
p
p
p −k
k
p
p
q = 0
k
Fig. 43.5 (a) The tadpole, representing the ﬁrst-order Hartree contribution to the
single-particle energy. (b) The oyster giving the ﬁrst-order Fock contribution to the
single-particle energy. (c) The sum to inﬁnity of oyster and tadpole diagrams.
Now we’re in a position to examine the simpliﬁed version of the two-
particle interaction.
First, let’s examine the direct term: exactly as
before the VEV part ﬁxes q = 0, giving a number operator and the
expression
ˆVdirect
=
X
pk
˜Vq=0⟨ˆa†
kˆak⟩ˆa†
pˆap
(43.32)
=
X
p
"
˜Vq=0
X
k
⟨ˆa†
kˆak⟩
#
ˆa†
pˆap.
Comparing with our equation for a single particle in an external poten-
tial, we see that it looks as if each of the particles is sailing through
an eﬀective potential ˜V MF
p
= ˜Vq=0
P
k⟨ˆNk⟩. This is represented by the
tadpole diagram in Fig. 43.5(a), which is generated by joining up two of
the four legs. Notice that there are two ways of forming this diagram,
re-creating the factor of two we had from Wick’s expansion.
Next we examine the exchange term
ˆVexchange = −
X
qpk
˜Vq⟨ˆa†
p−qˆak⟩ˆa†
k+qˆap.
(43.33)
In order to get a nonzero VEV, we just consider the terms with p = k+q.
Then we obtain
ˆVexchange
=
−
X
p
"X
k
˜Vp−k⟨ˆa†
kˆak⟩
#
ˆa†
pˆap.
(43.34)

388
The many-body problem and the metal
This corresponds to a single particle interacting with an eﬀective poten-
tial ˜V MF
p
= P
k ˜Vp−k⟨ˆNk⟩, which we represent as the oyster diagram in
Fig. 43.5(b), generated by joining two opposite legs from the interaction
vertex.
Notice that the eﬀective potentials we have derived are exactly the
ﬁrst-order contributions to the self-energy ˜Σp of a particle. They are
each represented by a part of a Feynman diagram we can ﬁt between
the two external legs. We can therefore write the energy of the particles
as a sum of kinetic and self-energy parts
Ep = p2
2m + ˜Σ(D)
p
+ ˜Σ(F)
p ,
(43.35)
where
˜Σ(D)
p
= ˜Vq=0
P
k⟨ˆNk⟩,
˜Σ(F)
p
= −P
k ˜Vp−k⟨ˆNk⟩.
(43.36)
For the case of jellium metal, we note that the Hartree self-energy ˜Σ(D)
p
is cancelled by the positive background charge (just as previously) and
the Fock term becomes ˜Σ(F)
p
from eqn 43.23, justifying its assignment
in the previous section. The energy of electrons in the Hartree–Fock
approximation is therefore given by
Ep = p2
2m −pF
π
 e2
4πǫ0

F
|p|
pF

,
(43.37)
which is shown in Fig. 43.4(b). It is worth stressing at this point that this
is not what is measured in a real metal! The Hartree–Fock approxima-
tion doesn’t capture the physics very well and higher order corrections
are necessary. This requires the full machinery of quantum ﬁeld theory
including propagators and Feynman rules and it is to this we now turn.
43.4
Electrons and holes
So far we have treated our metal as a box of electrons. For more serious
calculations it is useful to distinguish two sorts of excitations. This is
because a metal comprises electron states ﬁlled up to pF and so the
‘vacuum’ for the metal is not empty. With this in mind we deﬁne new
operators
ˆap
=
θ(|p| −pF)ˆcp + θ(pF −|p|)ˆb†
p,
ˆa†
p
=
θ(|p| −pF)ˆc†
p + θ(pF −|p|)ˆbp,
(43.38)
with anticommutation relations

ˆcp, ˆc†
q
	
= δ(3)(p −q),
n
ˆbp,ˆb†
q
o
= δ(3)(p −q),
(43.39)
with all other anticommutators vanishing. We say that the ˆcp operators
describe electrons, while the ˆbp operators describe holes.13
13Notice that, deﬁned this way, elec-
tron operators only act for states above
the Fermi surface |p| > pF while hole
operators only act for states below the
Fermi surface |p| < pF. This, in turn,
means that we describe the system as
having electron excitations above the
Fermi energy and hole excitations be-
low the Fermi energy.

43.5
Finding the excitations with propagators
389
Deﬁned in this way the kinetic energy of the system is given by
ˆH0 =
X
|p|<pF
Ep +
X
|p|>pF
Epˆc†
pˆcp −
X
|p|<pF
Epˆb†
pˆbp.
(43.40)
The ﬁrst term is the ground state energy of all of the states, ﬁlled up to
the Fermi level. The second term accounts for electron excitations. The
ﬁnal term accounts for hole excitations. Note that, with these deﬁnitions
the holes make a negative contribution to the energy. Although there
is a sense in which holes are antiparticles, they are not identical with
positrons whose contribution to the total energy is always positive.
To perform calculations we need an electron–hole ﬁeld operator, which
is given by
ˆψ(x) =
1
√
V
X
p
h
θ(|p| −pF)ˆcp + θ(pF −|p|)ˆb†
p
i
e−ip·x.
(43.41)
The use of this expansion to ﬁnd the
electron–hole propagator is examined
in Exercise 43.4.
43.5
Finding the excitations with
propagators
Our rather ad hoc mean-ﬁeld procedure will not get us much further.
Fortunately, we can use the technology of propagators to formalize the
procedure, turning it into a perturbation series. This will have the ad-
vantage of showing how the full expansion of the four-operator term
will go.
Recall that, since we are dealing with an interacting theory
we should think of our electrons and holes as quasiparticles. In the fol-
lowing example, we will recap the steps needed to make a perturbation
expansion and re-create all of our results so far in a ﬂash.14
14See Mahan, Chapter 2, P. Coleman,
Chapter 8, or Abrikosov, Gorkov and
Dzyaloshinski, Chapter 2, for the full
story. Note also that we return to rel-
ativistic notation here, so that xµ =
(t, x), pµ = (E, p), etc.
Example 43.8
The interaction Hamiltonian is HI(x−y) = 1
2 ψ†(x)ψ†(y)V (x−y)ψ(y)ψ(x)δ(x0−y0).
This interaction gives us an ˆS-operator
ˆS = e−i
2
R
d4xd4y ˆ
ψ†(x) ˆ
ψ†(y)V (x−y) ˆ
ψ(y) ˆ
ψ(x)δ(x0−y0).
(43.42)
The basic building block of the perturbation expansion is the Feynman propagator,
which we can obtain as the Green’s function of the equation of motion. The equation
of motion for non-relativistic electrons is the Schr¨odinger equation, so we want
„
Ep −i d
dt
«
˜G0(p) = −iδ(t),
(43.43)
with Ep = p2
2m . We obtain the propagator in momentum space as
˜G0(p) =
i
E −Ep + iǫ ,
(43.44)
where we have added iǫ in the denominator to avoid integrals hitting the electron
pole. This choice is discussed in Appendix B.

390
The many-body problem and the metal
With the inclusion of holes, the free propagator for electrons in a metal is given
by
˜G0(p) = iθ(|p| −pF)
E −Ep + iǫ + iθ(pF −|p|)
E −Ep −iǫ ,
(43.45)
which is a sum of electron and holes parts. This expression is similar to what we have
for scalar particles, although with a diﬀering sign. Since, for a particular choice of p,
we only need one of the terms in the expression for ˜G0(p), a much simpler approach
that is commonly employed is to write the electron–hole free propagator as a single
expression
˜G0(p) =
i
E −Ep + iδp
,
(43.46)
where δp = +ǫ for |p| > pF and δp = −ǫ for |p| < pF.
Now that we have a propagator and an interaction, we’re ready to write
down the Feynman rules for a metal.
The Feynman rules for a metal
• A factor ˜G0(p) =
i
E−Ep+iδp for each internal line.
• A factor −i ˜Vq for each interaction vertex.
• A factor −1 for each closed fermion loop.
• Conserve energy-momentum at the vertices.
• Integrate over unconstrained energies and momenta with a mea-
sure V
R
d4p
(2π)4 .
• A convergence factor eiE0+ for non-propagating lines.
(a)
(b)
p
p
Fig. 43.6 Non-propagating lines.
The ﬁnal point is necessary for the two diagram parts shown in Fig. 43.6
and stems from the instantaneous nature of the interaction, which ap-
pears to allow lines to propagate instantaneously.15 We’re now ready to
15As shown in the examples below, it
forces us to close the contour in our in-
tegrals in a particular manner, guaran-
teeing that we get a sensible answer.
go and calculate some things with the Feynman rules.
43.6
Ground states and excitations
Ground state energies may be evaluated from perturbation theory by
summing Feynman diagrams. As discussed in Exercise 43.5, this involves
ﬁnding the amplitude ⟨0| ˆS|0⟩= e
P(Connected vacuum diagrams), from
which it follows that
−iE
V =
P 
Connected
vacuum diagrams

VT
.
(43.47)
The lowest order vacuum diagrams are the double tadpole and oyster,
Fig. 43.7
Corrections to the ground
state energy of a metal.
just as we had with the mean-ﬁeld approximation. The mean-ﬁeld trick,
which gave us the Hartree–Fock approximation, represents ﬁrst-order
terms in the perturbation expansion of the ground state energy. Higher
order terms in the expansion give us diagrams such as those others shown
in Fig. 43.7.

43.6
Ground states and excitations
391
What are the particle energies? These can be accessed through the
interacting propagator of the theory, which includes the eﬀect of all self-
energy contributions. Recall the exact result that
˜G(p) =
iZp
E −Ep + iΓp
+

multiparticle
parts

,
(43.48)
so we should be able to read oﬀthe dispersion from the position of
the pole in the denominator. To ﬁnd the full propagator we use the
prescription
˜G(p) =
X 
Connected diagrams with
two external legs

.
(43.49)
We will use Dyson’s formula to formally sum the eﬀect of self-energy
processes to inﬁnity and renormalize the propagator.
As before, we
deﬁne the 1PI self-energy as
−i˜Σ(p) =
X 
Amputated 1PI diagrams
with two external legs

.
(43.50)
The sum to inﬁnity, shown for the example of Hartree–Fock theory in
Fig. 43.5(c), yields Dyson’s equation for the propagator
˜G(p)
=
1
˜G0(p)−1 + i˜Σ(p)
=
i
E −Ep −˜Σ(p) + iδp
,
(43.51)
so the energy of the excitations is E = Ep + Re
h
˜Σ(E, p)
i
. Some of the
contributions to ˜Σ(p) are shown in Fig. 43.8.
1PI =
+
+
+
+ · · ·
Fig. 43.8 Contributions to the electron 1PI self-energy of the metal.
Example 43.9
Let’s examine the renormalization of the electron lines by ﬁrst-order processes. The
only self-energy terms containing one interaction wiggle are the tadpole and oyster
diagrams shown in Fig. 43.5(a) and (b). This is, of course, identical to the mean-ﬁeld
result and demonstrates, again, that the mean-ﬁeld approximation predicts the same
contribution as ﬁrst-order perturbation theory, namely the Hartree–Fock contribu-
tions. The joy of the propagator theory is that it tells us how to make sense of the
self-energy contribution: we can use it to renormalize the propagator by summing to
inﬁnity.

392
The many-body problem and the metal
In terms of propagators the diagrams correspond to
−i˜Σ(Tadpole)p
=
(−1)
X
k
Z dE
2π (−i ˜Vq=0) ˜G0(k)eiE0+,
−i˜Σ(Oyster)p
=
X
k
Z dE
2π (−i ˜Vp−k) ˜G0(k)eiE0+.
(43.52)
It’s instructive to use the complex analysis of Appendix B to show the use of the
eiE0+ factors. Let’s examine the integral
Z dE
2π
˜G0(k)eiE0+,
(43.53)
whose pole structure is shown in Fig. 43.9(a).
If we complete the contour in the
upper half-plane [Fig. 43.9(b)] then we pick up all of the momentum state poles for
|k| < pF, completing in the lower half-plane picks up the momentum state poles for
|k| > pF. The convergence factor eiE0+ makes the choice for us! If we complete
in the lower half-plane (where imaginary E gets very large and negative) this factor
will diverge. On the other hand, it will vanish for a contour completed in the upper
half-plane in the limit that the contour becomes very large. We therefore pick up the
poles in the upper half-plane for all |k| < pF, each contributing residue i. We ﬁnd
Z
dE
2π
˜G0(k)eiE0+ = 1
2π × (2πi)iNk,
(43.54)
where Nk is the occupation number distribution which is unity for |k| < pF and 0
otherwise. We obtain expressions for our diagrams of
−i˜Σ(Tadpole)p = (−1)
X
k
(−i ˜Vq=0)(−Nk),
(43.55)
and
−i˜Σ(Oyster)p
=
X
k
(−i ˜Vp−k)(−Nk).
(43.56)
k0
k0
(a)
pF
(b)
Fig. 43.9 The integration in eqn 43.53
can be carried out in the complex plane.
For the case of jellium metal, with its positive background charge, the tadpole
contribution is cancelled and we only need consider the oyster contribution.
We
obtain for the oyster diagram
˜Σ(Oyster)p
=
−
X
|k|<pF
e2
Vǫ0
1
|p −k|2 = −e2
ǫ0
Z
|k|<pF
d3k
(2π)3
1
|p −k|2 ,
=
−pF
π
e2
4πǫ0
F
„|p|
pF
«
,
(43.57)
which is, of course, the same result as discussed earlier in this chapter.
Finally, the renormalization of the electron line from the oyster contribution,
achieved with a sum to inﬁnity, is shown in Fig. 43.10, showing that the quasiparticle
energy is shifted to p2/2m + ˜Σ(Oyster)p.
So far we have only re-created the same results as seen earlier. To see
the motivation for going further one can calculate the eﬀective mass for
Hartree–Fock theory.16
16Close to the Fermi level, we may ex-
pand the energy of an excitation as
Ep = EF+
„∂Ep
∂p
«˛˛˛˛
|p|=pF
(|p|−pF)+. . .
which, for a non-interacting system, is
given by
Ep = p2
F
2m + pF
m (|p| −pF) + . . .
from which we identify m∗=
pF
“ ∂Ep
∂|p|
”.
The result of computing the mass m∗corresponding to the disper-
sion Ep following from our expression for the Hartree–Fock energy
(eqn 43.57) is that m∗= 0 close to the Fermi energy, which is clearly
wrong: electrons should have eﬀective masses of order17 m. The prob-
17We ignore the heavy fermion materi-
als where m∗≈103m. See P. Coleman,
Chapter 16, for a discussion of these
systems.
lem here is that the Hartree–Fock theory is completely static. It treats
an electron as if it’s propagating in the static ﬁeld of all of the others.
In reality electrons will alter their conﬁguration dynamically resulting in
time-dependent correlations. These may be included in the calculation

43.7
The random phase approximation
393
=
+
+
+
+ · · ·
=
=
1
1−
( )−1−
Fig. 43.10 The summation of the Fock terms to inﬁnity.
through the inclusion of more processes in the electron self-energy as
shown in Fig. 43.8.
The next logical step would be to evaluate contributions to the self-
energy that include two interaction wiggles, including the pair-bubble
diagram in Fig. 43.11. However examining the amplitude for this dia-
gram we ﬁnd that it is divergent for small q. This looks like a problem!
However, this divergence is avoided if we take a side-step and, rather
than concentrating on corrections to the electron self-energy, we con-
sider a correction analogous to the photon self-energy of Chapter 41.
This will have the eﬀect of summing up a class of diagrams that are
similar to that shown in Fig. 43.11 which will remove the divergence
in the electron self-energy. This summation is the subject of the next
section.
Fig. 43.11 The pair-bubble contribu-
tion to the self-energy.
43.7
The random phase approximation
For an electron gas with a high density of particles the most important
correction to the Hartree–Fock approximation is provided by the lowest
order correction to the wiggle in the interaction vertex. For historical
reasons18 this is known as the random phase approximation or RPA
18The randomness of a phase is unim-
portant in our description of the RPA.
The reason for the name comes from
an alternative treatment where a term
P
l eiq·xl is shown to be neglectable,
where xl labels an electron’s position.
The physics of this approximation is
that if xl is distributed over a large
range then the random phases that re-
sult will tend to cancel.
and was ﬁrst formulated by David Bohm and David Pines.19 This correc-
19David Pines (1924– )
tion to the interaction vertex has much in common with the self-energy
correction to the photon propagator in QED and represents a change
in the potential felt by the electrons due to the eﬀect of shielding. The
idea is that the interaction wiggle creates an electron–hole pair, which
annihilates shortly afterwards.
Fig. 43.12
A ﬁrst-order polarization
process showing a bubble in the inter-
action line.
This state of aﬀairs is known here, as in QED, as a polarization
process and is shown in Fig. 43.12.
It is one example of a class of
1PI self-energy processes that can be inserted into an interaction wiggle
whose sum is a Green’s function denoted i˜Π(q), which we deﬁne similarly

394
The many-body problem and the metal
to its cousin in QED as
i˜Π(q) =
X 
All 1PI diagrams that can be inserted
into an interaction wiggle

.
(43.58)
Some of the terms contributing to i˜Π(q) are shown in Fig. 43.13.
=
+
+
+
+ · · ·
1PI
Fig. 43.13 Some of the diagrams contributing to i˜Π(q).
In order to fully take account of i˜Π(q) we need to sum the contribution
to inﬁnity. Since we don’t have a photon propagator in this theory, these
renormalize the potential, changing the interaction strength from −i ˜Vq
to the eﬀective potential −i ˜Veﬀ(q). The sum to inﬁnity is simply carried
out in the usual manner:
−i ˜Veﬀ(q)
=
−i ˜Vq +
h
−i ˜Vq
i h
i˜Π(q)
i h
−i ˜Vq
i
+
h
−i ˜Vq
i h
i˜Π(q)
i h
−i ˜Vq
i h
i˜Π(q)
i h
−i ˜Vq
i
+ . . .
=
−i ˜Vq
1 −˜Vq ˜Π(q)
.
(43.59)
The interaction potential ˜Vq therefore becomes an eﬀective potential
˜Veﬀ(q) = ˜Vq/[1 −˜Vq ˜Π(q)], which is often written ˜Veﬀ= ˜Vq/˜ǫ(q), where
the permittivity ˜ǫ(q) = 1 −˜Vq ˜Π(q) is a function of energy q0 and wave
vector q. So, just as in QED, the vacuum state of the metal takes on
dielectric properties caused by electron–hole pairs buzzing in and out of
existence. The physics of these dielectric properties lies in the screening
of the electrons from each other’s charge by the electron–hole pairs.
Finally we note that since ˜ǫ(q) is a function of energy, the inclusion
of polarization diagrams will provide the all-important time-dependent
correlations whose neglect resulted in a nonsensical eﬀective mass in the
previous section.
Our treatment of ˜Π(q) has so far been quite general. In the RPA we
evaluate the eﬀect of only the lowest order contribution to ˜Π(q), that is,
the electron–hole bubble shown in Fig. 43.12, whose amplitude we will
call ˜π(q). (The sum to inﬁnity of this interaction that renormalizes the
photon line in the RPA is shown in Fig 43.14.)

43.7
The random phase approximation
395
=
+
+
+ · · ·
=
=
1
1−
(
)−1
−
Fig. 43.14 The sum of bubbles in the interaction line.
Example 43.10
Including a minus sign for the fermion loop, the amplitude of the bubble in Fig. 43.12
is given by
i˜π(q)
=
−V
Z
d4q
(2π)4 ˜G0(p + q) ˜G0(p)
=
−V
Z
d3p
(2π)3
Z ∞
−∞
dp0
2π
i
p0 + q0 −Ep+q + iδp+q
i
p0 −Ep + iδp
=
V
Z
d3p
(2π)3
1
(Ep+q −iδp+q) −(Ep −iδp) −q0
×
Z ∞
−∞
dp0
2π
„
1
p0 + q0 −Ep+q + iδp+q
−
1
p0 −Ep + iδp
«
, (43.60)
where, in the ﬁnal line, we have rewritten the amplitude in a form which allows
us to do a contour integral (see Appendix B). The integrals over p0 have poles at
p0 = −q0 + Ep+q −iδp+q and p0 = Ep −iδp respectively. The residues for these
are Fermi functions and so we pick up factors 2πiNp+q and 2πiNp respectively and
obtain the result20
20Dropping the inﬁnitesimal in the de-
nominator, since we won’t use it fur-
ther.
˜π(q) = V
Z
d3p
(2π)3
Np+q −Np
Ep+q −Ep −q0 .
(43.61)
The integrand in eqn 43.61 is known as the Lindhard function. The evaluation of
the integral involving the Lindhard function is carried out by P. Coleman, Chapter 8.
The result is a rather complicated expression for ˜ǫ(q). Analytically continuing to
imaginary energies and doing the integral it is found that
˜π(iq0, q) = −1
2g(EF)F
„ iq0
4EF
, |q|
2pF
«
,
(43.62)
where g(EF) = dNp
dEp
˛˛˛
EF
is the density of states at the Fermi level and
F(y, x) = 1+ 1
4x
»
1 −
“
x −y
x
”2–
ln
˛˛˛˛˛
x −y
x + 1
x −y
x −1
˛˛˛˛˛+ 1
4x
»
1 −
“
x + y
x
”2–
ln
˛˛˛˛˛
x + y
x + 1
x + y
x −1
˛˛˛˛˛ ,
(43.63)
is a dynamicized version of the function F(x) that featured in the evaluation of
the Fock self-energy. This latter expression must be analytically continued back to
the result we want for real q0 (see Mahan, Chapter 5 for the details).
However,
for our purposes we will limit our attention to the important limit q0 →0, where
F(iq0/4EF, |q|/2pF) reduces to F(|q|/2pF).
We will extract three great results from the RPA by examining three
special cases of ˜ǫ(q0, q).

396
The many-body problem and the metal
Case
I: In the static (q0
→
0) limit we have
˜π(0, q)
=
−1
2g(EF)F(|q|/2pF). Particularly illuminating is the static and small
q limit, where F(|q|/2qF) →2 and limq→0 ˜π(0, q) = −g(EF) = −3N
2EF
(where we have included a factor of 2 to account for spin degeneracy)
and so
lim
q→0 ǫ(0, q)
=
1 +
 3ne2
2ǫ0EF

1
|q|2 = 1 + q2
TF
|q|2 ,
(43.64)
where qTF is known as the Thomas–Fermi wave vector. Notice that the
permittivity diverges as |q| →0. Perversely this is good news! It says
that a uniform electric ﬁeld can’t penetrate a metal, which is something
we certainly expect.21 Plugging ˜ǫ(0, q) into eqn 43.59 we see that the
21The order of the limits is crucial here.
Here we have started with the static
limit and examined what happens as
q →0.
The other order, correspond-
ing to considering a uniform ﬁeld (i.e.
q = 0) then taking the limit q0 →0
tells us about the response of a metal to
ac ﬁelds and hence its transport prop-
erties.
static interaction potential is changed to an eﬀective potential
|q|
˜Veﬀ(0, |q|)
Fig. 43.15 The static eﬀective poten-
tial lim|q|→0 ˜Veﬀ(0, q).
lim
q→0
˜Veﬀ(0, |q|) = e2
ǫ0V
1
|q|2 + q2
TF
,
(43.65)
which is shown in Fig. 43.15. Notice that this is well behaved as |q| →0.
We started this section by arguing that the RPA was going to tell us
about screening. The connection with screening arises because our result
for ˜Veﬀis the Fourier transform of the real-space interaction potential
lim
|x|→∞Veﬀ(x) ∝e2
|x|e−qTF|x|,
(43.66)
which is, of course, also known as the Yukawa potential, perhaps the
simplest screened Coulomb potential. We conclude that the creation and
annihilation of electron–hole pairs screens the electronic charge and in
the RPA approximation this screening results in the Coulomb potential
being replaced by an eﬀective Yukawa potential.
|x|
Veﬀ(0, |x|)
0
Fig. 43.16 The static eﬀective poten-
tial Veﬀ(0, |x|) given by the wave vector
behaviour close to pF.
Case II: In the static limit for nonzero q the inﬁnite slope of F(x) at
x = 1 means that ˜ǫ(0, q) has a weak singularity at |q|/2pF = 1. Near this
point ˜π(0, q) ∝(|q| −2pF) ln(|q| −2pF). When inserted into eqn 43.59
and Fourier transformed this leads to an eﬀective potential
Veﬀ(0, |x|) ∝cos(2pF|x|)
|x|3
,
(43.67)
shown in Fig. 43.16. This long-range, oscillatory potential reﬂects the
fact that the metal as a whole reacts to the presence of each charge. The
sharpness of the Fermi surface causes this reaction to ‘ring’ in space, seen
as the oscillations in the eﬀective potential of an electron.22
22These are known as Friedel oscilla-
tions after Jacques Friedel (1921– ).
Case III: Finally we examine a case with nonzero q0 in the limit
|q| →0.
Example 43.11
In this case we may expand23 the Lindhard function for small q:
23Strictly one should consider the real
part of ˜π(q) and so this derivation
should be regarded with caution!
Np+q −Np
Ep+q −Ep −q0 ≈
q · vp
q · vp −q0
„dNp
dEp
«
,
(43.68)

43.7
The random phase approximation
397
where vp = ∇pEp. Expanding in powers of momentum, and including a factor 2
for spin degeneracy, we have
lim
|q|→0 ˜π(q0, q) ≈−2V
Z
d3p
(2π)3
» q · vp
q0
+ (q · vp)2
(q0)2
– „dNp
dEp
«
,
(43.69)
=
2V
Z
d|p|d(cos θ)dφ
(2π)3
|p|2
"
|q||vp| cos θ
q0
+
„ |q||vp| cos θ
q0
«2#
m
|p|δ(|p| −pF),
where we’ve used the fact that
“ dNp
dEp
”
= −m
|p|δ(|p|−pF). The integration over angle
kills the ﬁrst term and we obtain from the second that
lim
|q|→0 ˜π(q0, q) = g(EF) |vF|2
3
|q|2
(q0)2 .
(43.70)
Inserting into our expression for the permittivity yields
lim
|q|→0 ˜ǫ(q0, q) = 1 −
ω2
p
(q0)2 ,
(43.71)
where ω2
p =
ne2
mǫ0 is known as the plasma frequency. The fact that
lim|q|→0 ˜ǫ(q0, q) vanishes at q0 = ωp is telling us about the presence of
a new, long-wavelength, mode of excitation in the system. This is the
plasma oscillation and corresponds to the entire system of electrons
oscillating with respect to the positive background.
=
+
+ · · ·
+
+
Fig. 43.17 The oyster diagram within the RPA.
Finally we mention the reason we took the diversion into the RPA in
the ﬁrst place: the correction to the excitation energy Ep. This is ob-
tained from the RPA by inserting the renormalized interaction shown by
the shaded blob in Fig. 43.14 into our Feynman diagrams. For example,
the corrected oyster diagram is shown in Fig. 43.17. The evaluation of
the numbers is, as might be imagined, quite involved. It may be shown
that the quasiparticle energy is predicted to be
Ep = p2
2m −0.166rs(ln rs + 0.203)|p|pF
2m + const.,
(43.72)
which leads to a nonzero eﬀective mass.
plasmons
electron-hole
excitations
E
|p|
|p|
|p| + 2pF
(a)
(b)
(c)
0
2pF
Fig. 43.18
(a) The excitation spec-
trum of a metal showing low-energy
particle–hole quasiparticle excitations
and the higher energy (collective) plas-
mon mode. A quasiparticle excitation
with energy Ep may be created with:
(b) a momentum transfer |p|, from one
side of the Fermi surface, and (c) ≈
|p| + 2pF from the other (and all mo-
menta in between) leading to the con-
tinuum of states shown in the shaded
region.
The insights provided by the RPA allow us to draw a diagram of the
dispersion relation of excitations of the metal, shown in Fig. 43.18. The

398
The many-body problem and the metal
low-energy excitations are quasielectron–quasihole pairs, while at higher
energies we see the plasmon mode. Immediately noticeable is the fact
that the low-energy excitations form a continuum of width 2pF. This
arises because excitations involve promoting an electron from within a
Fermi sphere which has a diameter 2pF. For a given excitation energy
we therefore have a range 2pF of possible momenta, with the limits
corresponding to forming the excitation from ﬁlled states on the nearest
and furthest points on the Fermi sphere, as shown in Figs. 43.18(b)
and (c).
Chapter summary
• A treatment of interactions in metals has been introduced at the
level of the Hartree–Fock approximation. The Hartree term gives
the energy due to the constant part of the potential and is repre-
sented by a tadpole diagram. The Fock term describes exchange
and is represented by an oyster diagram.
• The next correction to the Hartree–Fock approximation is the
RPA, introducing a bubble into the interaction line due to virtual
electron–hole pairs and this allows us to treat screening.
Exercises
(43.1) Verify that −3
4π
e2
4πǫ0 pF = −0.916
rs , where the right-
hand side is in units of Rydbergs per electron.
(43.2) (a) Using the Hartree–Fock expression for the quasi-
particle excitation energy in eqn 43.37, show that
the Hartree–Fock eﬀective mass is given by
m
m∗= e2m
2πpF
1
x2
„1 + x2
x
ln
˛˛˛˛
1 + x
1 −x
˛˛˛˛ −2
«
, (43.73)
with x = |p|/pF and show that this diverges at
x = 1.
(b) Using the RPA expression for the quasiparticle
excitation energy in eqn 43.72 show that the RPA
eﬀective mass is given by
m
m∗= 1 −0.083rs(ln rs + 0.203).
(43.74)
(43.3) Including spin leads to a many-body Hamiltonian
for electrons of
ˆH
=
X
pσ
p2
2mˆa†
pσˆapσ
(43.75)
+1
2
X
pkqσσ′
˜Vqˆa†
p−qσˆa†
k+qσ′ˆakσ′ˆapσ.
(a) Why does the kinetic energy term favour equal
numbers of spin-up and spin-down electrons?
(b) Show that the Hartree contribution to the total
energy is independent of the relative populations of
spin-up and spin-down electrons.
(c) Show that the Fock term gives a negative con-
tribution to the energy for electrons with like spins
and no contribution from the interaction of elec-
trons with unlike spins.
(d) Under what circumstances will this system be
a ferromagnet?

Exercises
399
(43.4) From the deﬁnition of the free electron–hole prop-
agator
G0(x, y) = ⟨0|T ˆψ(x) ˆψ†(y)|0⟩,
(43.76)
along with the electron–hole mode expansion, show
that the electron–hole propagator is given by
eqn 43.46.
(43.5) Use
the
fact
(which
was
proved
in
Chap-
ter
22.2)
that
⟨Ω(∞)|Ω(−∞)⟩
=
⟨0| ˆS|0⟩
=
exp
»P „
Connected
vacuum diagrams
«–
to show that
−iE
V =
P „
Connected
vacuum diagrams
«
VT
.
(43.77)
Notice that dividing by the volume and time VT re-
moves the factor of δ(4)(x = 0) that we showed in
Exercise 19.4 accompanies all vacuum diagrams.
(43.6) (a) Write down the amplitude for the pair-bubble
diagram in Fig. 43.11 and show that it varies as
R
dq
|q|4 , which is divergent at small q.
(b) By considering the other diagrams containing
two interaction wiggles, show that the pair-bubble
is the most divergent diagram at second order.
(c) What is the most divergent diagram at third or-
der?
(d) Show that the RPA corresponds to a summa-
tion of the most divergent diagrams in the electron
self-energy.
See Mattuck for help.
(43.7) Consider an electron gas in (1+1)-dimensional
spacetime.
(a) Show that the equation of motion for electrons
near the Fermi energy may be written
„ ∂
∂t ± vF ∂
∂x
«
ψ = 0,
(43.78)
where the + sign is applied for electrons moving to
the right and the −for electrons moving to the left.
(b) What is the Lagrangian for this system of left-
and right-moving electrons?
(c) Using the (1 + 1)-dimensional representation of
the γ matrices from Exercise 36.5, γ0 = σ2 and
γ1 = iσ1, show that the Lagrangian may be written
L = iψ†
„ ∂
∂t −vFσ3 ∂
∂x
«
ψ,
(43.79)
where ψ =
„ ψL
ψR
«
.
(d) Employing units where vF = 1, show that this
may be written the form of a massless Dirac La-
grangian.
This problem is discussed in more depth in Zee,
Chapter V.5.

44
Superconductors
44.1 A model of a superconductor
400
44.2 The ground state is made of
Cooper pairs
402
44.3 Ground state energy
403
44.4 The
quasiparticles
are
bo-
golons
405
44.5 Broken symmetry
406
44.6 Field theory of a charged su-
perﬂuid
407
Chapter summary
409
Exercises
409
I’ll use super-mathematics! One bean weighs 1/20 of an
ounce! The jar weighs 12 pounds! Allowing two pounds for
the jar, that makes 20 times 16 times 10 ... or 32,000 beans!
Next request, please!
Superman (1938–1992, 1993– )
Upon cooling, it is found that many metals undergo a transition to
a superconducting state where all magnetic ﬂux is expelled from their
interior and their electrical resistance is zero. Experiments have probed
the excitation spectra of these materials, which are found to have an
energy gap ∆per particle that separates the ground state from excited,
quasiparticle states. In this chapter we will use quantum ﬁeld theory to
describe this phase of matter.
44.1
A model of a superconductor
Like superﬂuidity, superconductivity involves a dissipationless ﬂow of
particles. In the late 1930s, Fritz London recognized that both eﬀects
Fritz London (1900–1954)
involve the condensation of particles into a macroscopically occupied
state. We saw that in the case of superﬂuidity this went hand in hand
with the spontaneous breaking of a global U(1) symmetry.
This will
also turn out to be the case for superconductors. However, since the
particles in a superconductor are charged (they must be since they carry
electrical current), we are motivated to look for a theory which includes
electromagnetism. Electrodynamics is a gauge theory which is invariant
with respect to local U(1) transformations; it is the global breaking of
this symmetry1 that leads to superconductivity and the expulsion of ﬂux
1By ‘breaking global symmetry’ we
mean that the ground state picks out a
unique direction for the phases to point
for the entire system.
and measured dispersion follow as consequences:
A superconductor is a state resulting from breaking global phase
symmetry in a system with local U(1) invariance.
Which particles should we be considering as the constituents of the su-
perconducting state?
Although originally it was assumed that these
were the electrons of a metal, this is incorrect. The superconductor is
built, not from electrons, but from pairs of electrons. In 1949 Herbert
Fr¨ohlich formulated the theory of the electron–phonon interaction2 and
Herbert Fr¨olich (1905–1991)
2The electron–phonon interaction is
quite similar to the electron–photon in-
teraction in QED, with the interac-
tion consisting of an electron emitting a
phonon, rather than a photon as shown
in Fig. 44.1.
this allowed the possibility of an attractive interaction between electrons
mediated by phonons. This interaction between two electrons looks like
e
e
phonon
Fig. 44.1 The electron–phonon inter-
action vertex.

44.1
A model of a superconductor
401
a t-channel electron–electron scattering process as shown in Fig. 44.2.
What makes an attractive interaction a realistic proposition is the vast
diﬀerence in time scales between the moving electrons and the move-
ment of ion cores. Electrons move at the Fermi velocity and so the time
scale over which they move through the crystal is E−1
F . The time scale
of the deformation of the lattice is set by the phonon Debye frequency3
Peter Debye (1884–1966)
3The Debye frequency ωD is the max-
imum energy that a phonon is allowed
to take.
Any larger and the phonon
has a wavelength so small that it begins
to explore length scales smaller than
the distance between atoms and the no-
tion of a phonon as a long-wavelength,
collective excitation of a lattice breaks
down.
Phonons move at around the
speed of sound while electrons veloci-
ties are typically a factor of 102 larger.
as ω−1
D . Since E−1
F
≪ω−1
D
then the original electron has moved well
clear by the time the phonon ﬁnds the second electron.
In the ﬁeld
theory picture, this diﬀerence in time scales corresponds to an eﬀective
electron–electron interaction that is strongly ‘retarded’, which is to say,
strongly dependent on the phonon frequency.
p ↑
k ↑
−p ↓
−k ↓
phonon
Fig. 44.2
The t-channel process in a
superconductor. The eﬀective interac-
tion is mediated by a virtual phonon.
Example 44.1
We can piece together the form of the eﬀective electron–electron interaction.
By
analogy with QED, the electron–phonon interaction vertex will be that shown in
Fig. 44.1 with a coupling constant κ. The eﬀective electron–electron interaction will
be given by the t-channel process, mediated by the phonon, which has a propagator
resembling the free propagator for the scalar ﬁeld ˜D0(q) ∝(ω2 −ω2
q)−1, where ωq
is the dispersion of the phonons.
This results in an eﬀective interaction between
electrons given by
Veﬀ(ω, q) ∝
κ2
ω2 −ω2q
.
(44.1)
Taking the characteristic frequency of the phonons as the Debye frequency ωq ≈ωD,
we deduce that the interaction is attractive for ω < ωD. Leon Cooper showed more
Leon Cooper (1930– )
rigorously that an arbitrarily small electron–electron attraction would lead to pairing
of electrons into bound states and that this pairing would occur between electrons
across the whole of the metallic Fermi surface, making the entire metal unstable
to the formation of pairs!
Speciﬁcally, Cooper showed that there is an attractive
interaction between two electrons with equal and opposite momenta and opposite
spins. This is mediated by phonons and occurs chieﬂy between electrons within an
energy ωD of the Fermi energy EF.
We will explore the consequences of the existence of these so-called
Cooper pairs of electrons further by using an eﬀective Hamiltonian
given by4
4Comparing our notation from Chap-
ter 42, one can identify κ2 and
g
2V .
ˆH =
X
pσ
εpˆc†
pσˆcpσ −κ2 X
pk
ˆc†
k↑ˆc†
−k↓ˆc−p↓ˆcp↑,
(44.2)
where σ(=↑, ↓) labels the electron spin.
This Hamiltonian describes
the attractive potential energy between pairs of electrons, with opposite
spins and momenta, whose energies will be taken in a shell of width ωD
at the Fermi surface.5
5Note that this form of the poten-
tial was motivated by the mean-ﬁeld
approach of Chapter 43.
There we
saw that the mean-ﬁeld Hamiltonian
for electrons in metals throws up terms
such as a contribution to the energy of
C0 = ⟨ˆc†
p−qˆc†
k+q⟩⟨ˆckˆcp⟩, which we dis-
carded for an ordinary metal since or-
dinary single-electron states give zero
for each expectation value. This is not
the case for Cooper pairs of spin- 1
2 elec-
trons.
This Hamiltonian in eqn 44.2 is the model that was used to solve the
problem of superconductivity by John Bardeen, Leon Cooper and Robert
John Bardeen (1908–1991) is the only
person to have been awarded the Nobel
Prize in physics twice.
Schrieﬀer, known colloquially as the BCS model. Perhaps the most
J. Robert Schrieﬀer (1931– )
signiﬁcant feature of the BCS model is not the Hamiltonian itself, but the
method used to solve it. Schrieﬀer invented a trial many-particle state
|ΨBCS⟩, known as the BCS wave function, which ingeniously captures
the crucial physics. The BCS wave function is a coherent state whose
p = 0 mode is macroscopically occupied when the pairs condense to form

402
Superconductors
the superconducting ground state. It is used as a variational state with
the model Hamiltonian to ﬁnd the ground state energy of the system.
We therefore start by building the BCS coherent state function.
44.2
The ground state is made of Cooper
pairs
The building blocks of the superconductor are Cooper pairs. These are
pairs of electrons with opposite spin and momenta. We can create a
Cooper pair from the vacuum |0⟩using the operator
ˆP †
p = ˆc†
p↑ˆc†
−p↓.
(44.3)
The BCS coherent state is built from the vacuum using this operator as
follows:
|ΨBCS⟩=
Y
p
Cp eαp ˆ
P †
p|0⟩.
(44.4)
The superconducting state is often described in terms of Cooper pairs of
In eqn 44.4, the Cp’s are normalization
constants and αp is a complex number
which depends on p.
electrons in some sense ‘becoming’ bosons which subsequently undergo
Bose condensation. This can’t be quite right, since the commutation
relations for the pair operator aren’t those of a boson (see box). The
Commutation relations for ˆPp
[ ˆPp, ˆPq] = 0
[ ˆP †
p, ˆP †
q ] = 0
h
ˆPp, ˆP †
q
i
= δpq(1 −ˆ
Np↑−ˆ
N−q↓).
pair operators do, however, commute with each other. Remember that
the ˆc†
p operators create fermions which obey the exclusion principle and,
in particular, we have
ˆP †
p ˆP †
p = ˆc†
p↑ˆc†
−p↓ˆc†
p↑ˆc†
−p↓= 0.
(44.5)
Example 44.2
The fact that ( ˆP †
p)2 = 0 gives us enough information to simplify the BCS coherent
state. We notice that only the ﬁrst two terms of the exponential can be nonzero
(since they contain no powers and one power of ˆP †
p respectively) and we have
|ΨBCS⟩=
Y
p
Cpeαp ˆ
P †
p|0⟩=
Y
p
Cp
“
1 + αp ˆP †
p
”
|0⟩.
(44.6)
We can also normalize the wave function:
1 = |Cp|2⟨0|
“
1 + α∗
p ˆPp
” “
1 + αp ˆP †
p
”
|0⟩= |Cp|2(1 + |αp|2),
(44.7)
giving Cp = (1 + |αp|2)−1
2 . Deﬁning
up =
1
(1+|αp|2)
1
2 ,
vp =
αp
(1+|αp|2)
1
2 ,
(44.8)
we have that |up|2 + |vp|2 = 1, we can write6
6It’s important to notice that |ΨBCS⟩
is very diﬀerent from the usual situa-
tion in a metal in which the wave func-
tion is a single speciﬁc conﬁguration of
empty and ﬁlled states. The product of
terms (up +vp ˆP †
p) ensures that |ΨBCS⟩
contains a coherent sum containing all
possible conﬁgurations in which every
pair state is ﬁlled or empty. The BCS
state combines all possibilities in a sin-
gle wave function.
|ΨBCS⟩=
Y
p
(up + vp ˆP †
p)|0⟩.
(44.9)
Finally we can give meaning to vp. If we evaluate the ground state expectation value
of the number operator for spin-up electrons, we obtain7
7See Exercise 44.3.
⟨ˆ
Np↑⟩= ⟨ΨBCS|ˆc†
p↑ˆcp↑|ΨBCS⟩
=
|vp|2,
(44.10)
which tells us that |vp|2 gives the average pair occupation of a state labelled with
momentum p with spin up.8
8Similarly ⟨ˆ
Np↓⟩= |vp|2.
Of course,
|up|2 = 1 −|vp|2 tells us the aver-
age non-occupation of the state (since a
fermion state can either be unoccupied
or singly occupied).

44.3
Ground state energy
403
Written in terms of our single-fermion operators we have a simpliﬁed
expression for the coherent state given by
|ΨBCS⟩=
Y
p
(up + vpˆc†
p↑ˆc†
−p↓)|0⟩.
(44.11)
The number operator ˆN is given by
ˆN =
X
pσ
ˆc†
pσˆcpσ =
X
p

ˆNp↑+ ˆNp↓

.
(44.12)
Thus the total number of electrons in the superconducting state is found
by summing over spin-up and spin-down occupancies and so
⟨ΨBCS| ˆN|ΨBCS⟩=
X
p

⟨ˆNp↑⟩+ ⟨ˆNp↓⟩

= 2
X
p
|vp|2.
(44.13)
We now have enough ammunition to attack the variational problem of
ﬁnding the ground state energy.
44.3
Ground state energy
We will now use the BCS ground state as a variational wave function for
the model Hamiltonian (eqn 44.2) describing the superconductor. By
minimizing the energy with respect to the parameters up and vp we will
extract the properties of the ground state of the superconductor. The
expectation value of the energy is
E = ⟨ΨBCS| ˆH|ΨBCS⟩,
(44.14)
which, upon substitution,9 gives us
9The ﬁrst term is easy since it is the
kinetic energy term in the BCS Hamil-
tonian (eqn 44.2). It can be written
X
pσ
εp ˆ
Npσ,
and so we can use eqn 44.13 to reduce
it to
X
p
2εp|vp|2.
The second term is evaluated in the ex-
ercises.
E =
X
p
2εp|vp|2 −κ2 X
pk
v∗
pvku∗
kup.
(44.15)
We will minimize this subject to two constraints. The ﬁrst ﬁxes the total
particle number via
N = 2
X
p
|vp|2,
(44.16)
which we impose as a constraint using a Lagrange multiplier10 µ. We
10As will be obvious to those with ex-
pertise in statistical mechanics, the La-
grange multiplier µ is the chemical po-
tential.
The Lagrange multiplier Ep
will turn out to give the quasiparticle
energy.
also introduce a Lagrange multiplier Ep to enforce the constraint that
|up|2 + |vp|2 = 1.
Example 44.3
Using the method of Lagrange multipliers, we write a function f using
f = E −µN +
X
p
Ep(|up|2 + |vp|2 −1),
(44.17)

404
Superconductors
and look for solutions to ∂f/∂up = 0 and ∂f/∂vp = 0. This yields
∂E
∂up
−µ ∂N
∂up
+ Epu∗
p
=
0,
∂E
∂vp
−µ ∂N
∂vp
+ Epv∗
p
=
0.
(44.18)
Doing the derivatives we ﬁnd, after a little algebra (examined in Exercise 44.5), the
matrix equation
„ (εp −µ)
∆
∆∗
−(εp −µ)
« „ u∗
p
v∗
p
«
= Ep
„ u∗
p
v∗
p
«
,
(44.19)
where we’ve written ∆= κ2 P
p u∗
pvp. This matrix has eigenvalues
Ep = ±
ˆ
(εp −µ)2 + |∆|2˜ 1
2 .
(44.20)
Finding the eigenvectors is made easier if we deﬁne
cos 2θp = εp−µ
Ep ,
sin 2θp =
∆
Ep .
(44.21)
Then we ﬁnd eigenvectors
u∗
p = cos θp,
v∗
p = sin θp,
(44.22)
or
|up|2
=
1
2
„
1 + εp −µ
Ep
«
,
|vp|2
=
1
2
„
1 −εp −µ
Ep
«
.
(44.23)
The behaviour of |up|2 and |vp|2 is shown in Fig. 44.3. We also ﬁnd that u∗
pvp =
cos θp sin θp =
1
2 sin 2θp = ∆/2Ep.
We conclude that the BCS ground state is
|ΩBCS⟩= Q
p(cos θp + sin θpˆc†
p↑ˆc†
−p↓)|0⟩.
Ep
εp −µ
εp −µ
|vp|2
|up|2
1
2
u∗pvp
εp −µ
Fig. 44.3 (a) The function Ep (which
we will later call the bogolon disper-
sion).
(b) The functions |up|2 and
|vp|2 which show that well below µ the
state is predominantly electron-like and
well above it is predominantly hole-like.
Near to µ it has a mixed electron-like
and hole-like character.
(c) u∗
pvp =
∆/2Ep, which has a maximum when
there are plenty of electron states and
available states to scatter into.
We will see in the next section that ∆is the energy gap between the
ground state and excited quasiparticle states. We may now eliminate
up and vp to reveal the physics of the BCS state. Using u∗
pvp = ∆/2Ep
with the deﬁnition of the gap ∆, we ﬁnd our ﬁrst result which is that
∆= κ2 X
p
∆
2Ep
.
(44.24)
Since the electrons that form the pairs only lie within ωD of the Fermi
energy, we can restrict our sum to |(εp −µ)| < ωD. We then make the
replacement P
p →g(εF)
R
dε, where g(εF) is the density of states at
the Fermi energy, and writing ε ≡εp −µ, eqn 44.24 becomes
∆= κ2 g(εF)
|
{z
}
Λ
Z ωD
−ωD
dε
∆
2(ε2 + ∆2)
1
2 ,
(44.25)
where we have deﬁned Λ (the eﬀective coupling constant) using Λ =
κ2g(εF). This integral gives
1
Λ = sinh−1 ωD
∆

,
(44.26)

44.4
The quasiparticles are bogolons
405
but since ∆≪ωD we obtain
|∆| ≈2ωD e−1/Λ.
(44.27)
This tells us how the energy gap that promotes superconductivity de-
pends on the eﬀective coupling and the Debye energy. Superconductivity
is most robust if there is a large energy gap and so, if we want to ﬁnd the
most robust superconductor with the highest possible11 critical temper-
11This
assumes
BCS-like
electron–
phonon pairing.
A higher Tc may be
found using other mechanisms.
ature Tc, we need a material with a large Debye frequency and a large
coupling constant Λ. The latter is achieved by having a large κ and a
large density of states at the Fermi energy.
44.4
The quasiparticles are bogolons
We now make good on our claim of the last section: we want to conﬁrm
that ∆is indeed the energy gap in the particle spectrum of the super-
conducting ground state. To ﬁnd the excited states we employ a similar
approach to that of the metal. We consider the model Hamiltonian with
averaged pairs of operators in the interaction part:
ˆH =
X
pσ
(εp−µ)ˆc†
pσˆcpσ−κ2 X
p

⟨ˆc†
p↑ˆc†
−p↓⟩ˆc−p↓ˆcp↑+ ⟨ˆc−p↓ˆcp↑⟩ˆc†
p↑ˆc†
−p↓

,
(44.28)
where the expectation values are taken with respect to the BCS ground
state |ΩBCS⟩. Substitution of |ΩBCS⟩allows us to identify the quantity
κ2 P
p⟨ΩBCS|ˆc−p↓ˆcp↑|ΩBCS⟩as the energy gap12 ∆= κ2 P
p u∗
pvp, and
12In manipulating these expressions in-
volving the energy gap, note that if ∆
is not real then, writing ∆= |∆|eiφ,
we may always make it real with the
global transformation ˆci
→
eiφ/2ˆci,
ˆc†
i
→e−iφ/2ˆc†
i .
In the next section
we will conﬁrm that choosing a particu-
lar value for ∆corresponds to breaking
symmetry.
we have
ˆH =
X
pσ
(εp −µ)ˆc†
pσˆcpσ −
X
p

∆∗ˆc−p↓ˆcp↑+ ∆ˆc†
p↑ˆc†
−p↓

.
(44.29)
We need to diagonalize this Hamiltonian, which contains ˆcˆc and ˆc†ˆc†
bilinears. As in the last chapter, the Bogoliubov transformation gives
us a method to do this.
Example 44.4
Written in matrix form we have the Hamiltonian13
13This looks a lot like eqn 44.19 but
with minus signs on the oﬀ-diagonal el-
ements.
ˆH =
X
p
“
ˆc†
p↑
ˆc−p↓
” „ εp −µ
−∆
−∆∗
−(εp −µ)
«  
ˆcp↑
ˆc†
−p↓
!
.
(44.30)
This may be diagonalized using the Bogoliubov procedure, whose details are exam-
ined in Exercise 44.7, to give
ˆH =
X
p
“
ˆb†
p↑
ˆb−p↓
” „ Ep
0
0
−Ep
«  
ˆbp↑
ˆb†
−p↓
!
,
(44.31)
where the operators ˆb†
pσ and ˆbpσ create and destroy bogolon quasiparticle excitations.
The bogolon operators obey anticommutation laws:
n
ˆbp1σ1,ˆb†
p2σ2
o
= δp1p2δσ1σ2,
(44.32)
n
ˆb†
p1σ1,ˆb†
p2σ2
o
= 0,
n
ˆbp1σ1,ˆbp2σ2
o
= 0.

406
Superconductors
The diagonalized Hamiltonian for the excitations in the superconductor
is given by
ˆH =
X
p
Ep(ˆb†
p↑ˆbp↑+ ˆb†
−p↓ˆb−p↓),
(44.33)
where Ep =
p
(εp −µ)2 + |∆|2. This is shown in Fig. 44.3. Notice that
the bogolons have a dispersion relation that p starts at an energy ∆
above the ground state energy. This is the energy gap:14 there are no
14This is key to superconductivity. Ex-
citations will dissipate momentum and
prevent the superﬂow of current. The
presence of a gap prevents their cre-
ation at low energies.
allowed quasiparticle excitations between Ep = −∆and Ep = ∆.
44.5
Broken symmetry
Fig. 44.4 Order in the superconductor
involves the lining up of phase angles,
just as in the case of an uncharged su-
perﬂuid.
As in the superﬂuid state examined earlier, superconductivity involves
the macroscopic occupation of a coherent ground state |Ω⟩. Recall that
the order parameter for the superﬂuid was given by
⟨Ω|ˆΦ(x)|Ω⟩
=
1
√
V
X
p
⟨Ω|ˆap|Ω⟩eip·x
≈
1
√
V
⟨Ω|ˆap=0|Ω⟩= √neiθ0.
(44.34)
For the superconductor we have (by the same token)
⟨ΩBCS|ˆΨ(x)|ΩBCS⟩
=
1
√
V
X
p
⟨ΩBCS|ˆc−p↓ˆcp↑|ΩBCS⟩ei(p−p)·x
=
1
√
V
X
p
⟨ΩBCS|ˆc−p↓ˆcp↑|ΩBCS⟩
=
√neiθ0.
(44.35)
From these expressions we see that we could also regard the supercon-
ducting energy gap as the order parameter for the system since we have
∆= κ2 X
p
⟨ΩBCS|ˆc−p↓ˆcp↑|ΩBCS⟩.
(44.36)
Returning to the real space expectation value ⟨ψ(x)|ˆΨ(x)|ψ(x)⟩=
p
ρ(x)eiθ(x) for a coherent state |ψ(x)⟩, we see that for the supercon-
ducting ground state |ΩBCS⟩has the phase ﬁeld θ(x) breaking symmetry
by becoming a constant θ0. (As in the case of the superﬂuid we can vi-
sualize this with an XY -model, as shown in Fig. 44.4.) As alluded to
before, the diﬀerence between superﬂuidity and superconductivity comes
from the fact that the phase symmetry that’s broken is a local one. Prior
to breaking symmetry the superconductor enjoys local U(1) symmetry:
we would alter the phase by θ(x) →θ(x) + α(x), where α(x) is diﬀer-
ent at all points in spacetime and the Lagrangian describing the system
would remain invariant. A gauge ﬁeld is needed in the Lagrangian to
guarantee this. We have seen in Chapter 26 that the gauge ﬁeld will
consume the Goldstone boson and become massive. This is the Higgs
mechanism and occurs in superconductors! We will examine the Higgs
phenomenon in superconductivity in the next section.

44.6
Field theory of a charged superﬂuid
407
44.6
Field theory of a charged superﬂuid
The superﬂuid Lagrangian is invariant with respect to the global trans-
formation Ψ →eiαΨ. We now promote this global symmetry to a local
one. As we have seen, this requires the introduction of a gauge ﬁeld Aµ
through the replacement of the derivative ∂µ by the covariant deriva-
tive Dµ = ∂µ + iqAµ. The coupling constant q is called the charge of
the Ψ ﬁeld, so by making a local phase transformation we are studying
the physics of a charged superﬂuid. Starting with the non-relativistic
superﬂuid Lagrangian (and denoting the coupling by g = 2Vκ2):
L = iΨ†∂0Ψ −1
2m∇Ψ† · ∇Ψ −g
2(n −Ψ†Ψ)2,
(44.37)
we gauge the theory by introducing the covariant derivative Dµ. This
will be easier in polar coordinates, so we introduce Ψ(x) =
p
ρ(x)eiθ(x)
and DµΨ =
h
1
2√ρ∂µρ + i√ρ (∂µθ + qAµ)
i
eiθ to obtain the gauged La-
grangian
L
=
−ρ(∂0θ + qA0) −1
2m
 1
4ρ(∇ρ)2 + ρ (∇θ −qA)2

−g
2 (n −ρ)2 −1
4FµνF µν,
(44.38)
where, as before, we drop the total time derivative term i∂0ρ/2. Note
that we have included the contribution of the gauge ﬁeld via Fµν =
∂µAν −∂νAµ.
The Lagrangian is now invariant with respect to the
change θ(x) →θ(x) + α(x) provided Aµ(x) →Aµ(x) −1
q∂µα(x).
As we’ve seen before, when we break the symmetry we’ll ﬁnd that
the massless photon ﬁeld Aµ(x) gobbles up the Goldstone mode θ(x)
and acquires a mass. This is the Higgs mechanism, at play in a charged
superﬂuid.
Example 44.5
Here’s how it unfolds.15 We notice that we can cast our Lagrangian in terms of a
15See Example 42.4 for the simpler ver-
sion of this argument appropriate for
the superﬂuid.
gauge invariant contribution Cµ(x) = 1
q ∂µθ(x) + Aµ(x), and the contribution of the
gauge ﬁeld is Fµν = ∂µAν −∂νAµ = ∂µCν −∂νCµ. We have
L = −ρqC0 −
1
2m
» 1
4ρ (∇ρ)2 + ρq2C2
–
−g
2 (n −ρ)2 −1
4FµνF µν.
(44.39)
Now break the symmetry of the ground state.16 We choose a ground state Ψ0(x) =
16Remember, you don’t break the sym-
metry of a Lagrangian; that always re-
tains the full symmetry. We simply ex-
pand the Lagrangian around the broken
symmetry ground state.
√neiθ0, where we’ll make our usual choice of θ0 = 0 as shown in Fig. 44.4.
We
expand about the ground state by examining small displacements √ρ = √n + h,
which yields
L = −nq2
2m C2 −
1
2m (∇h)2 −2gnh2 −2q√nC0h −1
4FµνF µν,
(44.40)
dropping the term −qnC0 as we did for superﬂuids. Already notice that the C-ﬁeld
appears to have acquired a mass (nq2/m)
1
2 . To complete our calculation we should
integrate out the energetic ﬁeld h to obtain a low-energy ﬁeld theory.
Just as in
Chapter 42, we recognize that the h dependent part can be written in the form
−h
„
−1
2m ∇2 + 2gn
«
h −(2q√nC0)h.
(44.41)

408
Superconductors
Again, just as before, we will integrate out the ﬁeld h with our path integral via
−1
2 φKφ + Jφ →1
2JK−1J. Setting φ = −
√
2h and J =
√
2nqC0 we have
L
=
−nq2
2m C2 + q√nC0
1
(−1/2m)∇2 + 2gnq√nC0 −1
4 FµνF µν
=
q2
2g C2
0 −nq2
2m C2 −1
4FµνF µν
=
q2
2g
`
C2
0 −v2C2´
−1
4FµνF µν,
(44.42)
where v2 = ng/m and we have neglected the term (1/2m)∇2 as small compared
to 2gn.
This is a non-relativistic equation telling us that the ﬁelds in a charged
superﬂuid give rise to excitations which are spin-1 vector particles.17
17See Chapter 13 if in doubt.
The consequence of the Higgs mechanism in a superconductor is the ex-
pulsion of magnetic ﬂux from the interior of the material. This is known
as the Meissner eﬀect. Starting with the low-energy Lagrangian from
eqn 44.42 we recognize that we can eliminate the component C0 (since a
massive vector ﬁeld has only three degrees of freedom, see Chapter 13).
We may therefore integrate out C0 (in the same manner as above) with
the result that
L = −nq2
2m C2 +

Terms involving
derivatives of C

.
(44.43)
Notice that the three massive ﬁelds described by this equation have mass
M 2 = nq2/m.
Our low-energy Lagrangian in eqn 44.43 may be used to ﬁnd the
current in the superconductor, which is given by
See Exercise 44.8.
J = nq
m (∇θ −qA) .
(44.44)
We conclude that the current in the superconductor responds to gradi-
ents in the phase ﬁeld (just like the superﬂuid) and also to the applied
ﬁeld A.
Example 44.6
Upon taking the curl of the current, we obtain an equation describing the magnetic
induction ﬁeld B in the superconductor:18
18This is one of the London equations;
the other is
∂J
∂t = nq2
m E,
which also follows from eqn 44.44 as-
suming ∇θ has no time-dependence.
∇× J = −nq2
m B,
(44.45)
which, combined with the static Maxwell equation ∇× B = J, yields
∇2B = nq2
m B.
(44.46)
In our units, the quantity nq2/m must have units (length)−2, so we deﬁne λ =
(m/nq2)−1/2.
In one dimension eqn 44.46 now reads
d2B
dx2 = 1
λ2 B,
(44.47)

Exercises
409
whose solutions have the form B(x) = B(0)e−x/λ.
In other words, the ﬁeld in
a superconductor falls away as we enter the interior of the superconductor over a
length scale determined by λ. Since λ tells us how far magnetic ﬂux can penetrate
the superconductor it is known as the penetration depth. Our treatment has all
been aimed at describing a charged superﬂuid. A superconductor is quite similar. In
a superconductor we have Cooper pairs, so we conventionally set q = −2|e| and write
ns = 2n and m = 2m∗for the superﬂuid density and eﬀective mass respectively.
This gives a penetration depth19 for a superconductor of
19In SI units,
1
λ2 = µ0nse2
m∗
.
1
λ2 = nse2
m∗.
(44.48)
Chapter summary
• The BCS wave function is given by |ΨBCS⟩
=
Q
p(up +
vpˆc†
p↑ˆc†
−p↓)|0⟩and is a coherent state of Cooper pairs.
• The quasiparticle excitations are bogolons, with dispersion given
by Ep =
p
(εp −µ)2 + |∆|2.
• The superconducting energy gap ∆can be regarded as the
order parameter of a superconductor, and is given by ∆=
κ2 P
p⟨ΩBCS|ˆc−p↓ˆcp↑|ΩBCS⟩.
• The Higgs mechanism in a superconductor leads to the London
equation, which can be written as J = nq
m (∇θ −qA).
Exercises
(44.1) Show that the amplitude
⟨ΨBCS|ˆc†
p↑ˆc†
−p↓|ΨBCS⟩⟨ΨBCS|ˆc−p↓ˆcp↑|ΨBCS⟩
(44.49)
is nonzero.
(44.2) Verify the commutator
h
ˆPp, ˆP †
q
i
= δpq(1 −ˆ
Np↑−
ˆ
N−q↓).
(44.3) Verify ⟨ΨBCS| ˆ
Np↑|ΨBCS⟩= |vp|2.
(44.4) Verify eqn 44.15:
⟨E⟩=
X
p
2εp|vp|2 −κ2 X
pk
v∗
pvku∗
kup.
(44.50)
(44.5) Starting with the total energy and total particle
number written in the form
E
=
X
p
εp
`
|vp|2 −|up|2 + 1
´
−κ2 X
pk
v∗
pvku∗
kup,
N
=
X
p
`
|vp|2 −|up|2 + 1
´
,
(44.51)
derive eqn 44.19:
„ (εp −µ)
∆
∆∗
−(εp −µ)
« „ u∗
p
v∗
p
«
= Ep
„ u∗
p
v∗
p
«
.
(44.52)
(b) Diagonalize the Hamiltonian and ﬁnd |up|2 and
|vp|2 using the method suggested in the text.
(44.6) (a) Deﬁne the Nambu spinor as ˆψp =
„
ˆcp↑
ˆc†
−p↓
«
and the energy gap as ∆= ∆1 −i∆2. Show that

410
Superconductors
the BCS Hamiltonian can be written
ˆH =
X
p
ˆψ†
php · τ ˆψp,
(44.53)
where hp = (∆1, ∆2, εp −µ) and τ is a vector made
up of the Pauli matrices.
(b) Show that eqn 44.19 is identical to the Dirac
equation upon making the replacements:
ψL
→
v∗
p,
ψR
→
u∗
p,
σ · p
→
εp −µ,
m
→
∆.
(44.54)
This is the basis of Nambu’s analogy.
(c) When local U(1) symmetry is globally broken
in the superconductor ∆takes on a nonzero value.
What is the equivalent of this symmetry breaking
in the Dirac problem?
Nambu’s analogy is discussed further in Aitchison
and Hey, Chapter 18.
(44.7) We will ﬁnd the excitations out of the BCS ground
state by diagonalizing eqn 44.30 using the Bogoli-
ubov procedure. The transformation to use in this
case is
„
ˆcp↑
ˆc†
−p↓
«
=
„
u∗
p
vp
−v∗
p
up
«  
ˆbp↑
ˆb†
−p↓
!
, (44.55)
and the algebra is simpliﬁed by using the substitu-
tions
up = cos θp,
(44.56)
vp = sin θp.
(44.57)
(a) Check that the transformation maintains the
anticommutation relations.
(b) Verify that this does the job of diagonalizing
the Hamiltonian.
(c) Conﬁrm that the bogolons are really the excita-
tions by checking ˆbp↑|ΩBCS⟩= 0 and ˆb−p↓|ΩBCS⟩=
0, that is, that there are no bogolons in the BCS
ground state.
(44.8) Show that the current in a superconductor is given
by eqn 44.44.

45
The fractional quantum
Hall ﬂuid
45.1 Magnetic translations
411
45.2 Landau Levels
413
45.3 The integer quantum Hall ef-
fect
415
45.4 The fractional quantum Hall
eﬀect
417
Chapter summary
421
Exercises
421
The more success the quantum theory has, the sillier it looks.
Albert Einstein (1879–1955)
The application of a magnetic ﬁeld to electrons in condensed matter has
surprising consequences, none more so than the integer and fractional
quantum Hall eﬀects. The latter eﬀect is a product of interactions, but
there are some interesting features even in the non-interacting case which
we review ﬁrst.
Throughout this chapter, it will be helpful to keep in mind that the
relevant length scale in the problem is the magnetic length ℓB, given
by1
1For ease of notation we follow conven-
tion in condensed matter physics and
write −e for the charge on the electron
in this chapter.
ℓB =
 ℏ
eB
 1
2
.
(45.1)
Note that2 the ﬂux Φ through a circle with radius
√
2ℓB is equal to
2The fact that we have to make the cir-
cle have radius
√
2ℓB and not ℓB is an-
noying, but we will have to live with it.
As we shall see, sometimes the relevant
length scale is ℓB and sometimes
√
2ℓB.
Φ = B(2πℓ2
B) = h/e.
This is double the value of the magnetic ﬂux
quantum Φ0 = h/2e which is important in superconductivity (where
the Cooper pairs have charge −2e) and plays the role of a ﬂux quantum
for problems involved with unpaired electrons.
It is often called the
Dirac ﬂux quantum.
45.1
Magnetic translations
Problems involving a lattice in solid state physics make use of transla-
tional symmetry. Recall from Chapter 9 that the translation operator
for a translation by a is e−iˆp·a, where ˆp is the momentum operator. For
a periodic system we would expect e−iˆp·a to commute with the Hamilto-
nian ˆH. Writing ˆU(a) = e−iˆp·a, we would then expect
h
ˆU(a), ˆH
i
= 0 or
ˆU −1(a) ˆH ˆU(a) = ˆH. This expectation is not realized in the presence of a
magnetic ﬁeld where we ﬁnd that ˆU(a) does not commute with ˆH. This
is because ˆH has become a function of the magnetic vector potential A
and this changes as you translate. We should therefore expect3
3Treating the magnetic vector poten-
tial as a function, rather than a second
quantized ﬁeld, we have ˆU−1(a)A(r) =
eiˆp·aA(r) = A(r −a).
ˆU −1(a) ˆH(A) ˆU(a) = ˆH

ˆU −1(a)A

.
(45.2)
However, if the energy of the system is independent of global translations
then this change in A can’t aﬀect the magnetic ﬁeld we would measure
with a magnetometer. We have, of course, met such shifts of the vector
potential before: they simply represent gauge transformations.

412
The fractional quantum Hall ﬂuid
Example 45.1
Consider4 a uniform ﬁeld B. In the gauge A(r) = 1
2 B × r we have
4Recall from Chapter 14 that a gauge
transformation involves changes ψ →
ψeiα and Aµ →Aµ −1
q ∂µα and we
sometimes write χ = α/q.
Note also
that the covariant derivative Dµ = ∂µ+
iqAµ implies ˆP = ˆp−qA. In this chap-
ter we will often deal with electrons,
where q = −e.
ˆU−1(a)A(r) = A(r −a) = A(r) −1
2B × a.
(45.3)
This is a gauge transformation Aµ →Aµ −1
q ∂µα(x) with 1
q ∇α(x) = −1
2 B × a or
α(r)
q
= χ(r) = −1
2(B × a) · r.
(45.4)
Along with a shift in Aµ, a gauge transformation also involves a shift of the matter
ﬁeld ψ →ψeiα(x) which here is ψ →ψ exp
ˆ ie
2 (B × a) · r
˜
.
To summarize: in the presence of a magnetic ﬁeld, we have a slightly
unusual form of translational symmetry in that it is invariant under a
combination of a translation with a gauge transformation.
We saw in Chapter 14 that the momentum operator ˆpµ = i∂µ doesn’t
transform properly under gauge transformations, but the covariant ver-
sion ˆPµ = iDµ = i(∂µ + iqAµ) does. It therefore comes as little surprise
to ﬁnd that an upgraded translation operator that does commute with
the gauged Hamiltonian is given by
ˆU(a) = e−i ˆP ·a,
(45.5)
where ˆP = −i∇−qA.
To see some consequences of this formalism, we now return to con-
densed matter physics and consider a two-dimensional square lattice
(with lattice constant a) of non-interacting electrons in the presence of
a magnetic ﬁeld. The Hamiltonian is5
5Since
we
are
discussing
two-
dimensional space here we will revert
to
the
ordinary
notation
of
vector
components where momentum along
the x-direction is written px and so on.
We will return to four-vector notation
in Section 45.4.
ˆH =
1
2m (ˆpx + eAx)2 + 1
2m (ˆpy + eAy)2 + V (x, y),
(45.6)
where V (x, y) = V (x + a, y) = V (x, y + a) is a periodic potential.
With a uniform magnetic ﬁeld parallel to z we can write the magnetic
vector potential A = B
2 (−y, x, 0). In this case the magnetic translation
operators that translate us through one lattice spacing in the x- and
y-directions are, respectively,
ˆUx
=
exp

−ia
ℏ

ˆpx −eBˆy
2

,
ˆUy
=
exp

−ia
ℏ

ˆpy + eBˆx
2

,
(45.7)
where we have restored factors of ℏ.
As advertised, these operators
commute with ˆH. However, they do not commute with each other and,
in fact,6
6This shows that the magnetic trans-
lation operators form a non-abelian
group. See Chapter 46.
ˆUx ˆUy = e2πiφ ˆUy ˆUx,
(45.8)
where φ = ea2B/h is the dimensionless magnetic ﬂux.7
7The proof of eqn 45.8 follows from
h
−ia
ℏ
“
ˆpx −eBˆy
2
”
, −ia
ℏ
“
ˆpy + eBˆx
2
”i
=
2πiφ
and the use of the identity
e
ˆ
Ae
ˆ
B = e
ˆ
Be
ˆ
Ae[ ˆ
A, ˆ
B],
which holds if
h
ˆ
A, ˆB
i
commutes with ˆ
A
and ˆB. See Exercise 45.1.

45.2
Landau Levels
413
The quantity φ represents the number of ﬂux quanta (h/e) passing
through a square on the lattice (area a2, usually called a plaquette).
Thus
φ =
Φ
h/e,
(45.9)
where Φ = Ba2 is the magnetic ﬂux through one plaquette. If φ = p/q
where p and q are integers then
h
ˆU q
x, ˆUy
i
= 0,
(45.10)
because (e2πiφ)p = 1. This means that a rectangle consisting of q ad-
jacent plaquettes contains an integer number of ﬂux quanta. Thus, the
system has an eﬀective periodicity equal to q times what would be the
case for B = 0.
Fig. 45.1 The Hofstadter butterﬂy.
This idea is beautifully illustrated by the Hofstadter butterﬂy8
8For more discussion of the Hofstadter
butterﬂy see Douglas Hofstadter’s pa-
per [Phys. Rev. B 14, 2239 (1976)]
and his well-known book G¨odel, Es-
cher, Bach, Chapter 5, which also fea-
tures his take on renormalization.
shown in Fig. 45.1, which shows the energy level spectrum for a two-
dimensional tight binding model (recall Example 4.8) with a magnetic
ﬁeld perpendicular to it. When B = 0 (and consequently φ = 0), the
allowed energies satisfy −4t ≤E ≤4t as you would expect from eqn 4.50.
For φ = p/q, the allowed energies form q sub-bands (note that sometimes
the sub-bands touch, so the two sub-bands at φ = 1/2 meet at E = 0).
The resulting band structure is fractal (and was published only a year
after Benoit Mandelbrot coined the term).
It has the extraordinary
Benoit
B.
Mandelbrot
(1924–2010)
chose his own middle initial, which
doesn’t stand for anything. There’s an
old joke that says that the ‘B’ in ‘Benoit
B. Mandelbrot’ stands for ‘Benoit B.
Mandelbrot’, thereby making the name
of the inventor of fractals a recursively-
deﬁned fractal.
feature that the spectrum depends on whether φ is rational or irrational
(and can only be plotted for rational φ).
45.2
Landau Levels
In order to examine the quantum mechanics of charges in applied mag-
netic ﬁelds we return to the problem of two-dimensional free electrons.
The Hamiltonian for this system may be written
ˆH =
ˆP 2
x + ˆP 2
y
2m
,
(45.11)
where
ˆPx = ˆpx + eAx,
ˆPy = ˆpy + eAy.
(45.12)
Classically, electrons with momentum p will orbit in circles of radius
p/eB in a magnetic ﬁeld. Consequently, we write the position operators
of an electron as
ˆx = ˆX +
1
eB ˆPy,
ˆy = ˆY −
1
eB ˆPx,
(45.13)
where (X, Y ) are the coordinates of the guiding centre of the orbit.
We ﬁnd that (X, Y ) and (Px, Py) are independent coordinates because
h
ˆX, ˆPx
i
=
h
ˆX, ˆPy
i
=
h
ˆY , ˆPx
i
=
h
ˆY , ˆPy
i
= 0.
(45.14)

414
The fractional quantum Hall ﬂuid
However, we also ﬁnd that
h
ˆX, ˆY
i
= iℓ2
B,
h
ˆPx, ˆPy
i
= −iℏ2
ℓ2
B .
(45.15)
Thus the guiding centre cannot be determined more accurately than an
area ≈ℓ2
B, i.e. the area occupied by approximately one quantum of ﬂux.
Example 45.2
Operators ˆ
X and ˆY commute with ˆH and hence d ˆ
X
dt = d ˆY
dt = 0. However,
d ˆPx
dt
=
1
iℏ
h
ˆPx, ˆH
i
= −ωc ˆPy,
d ˆPy
dt
=
1
iℏ
h
ˆPy, ˆH
i
= ωc ˆPx,
(45.16)
where ωc = eB/m is the cyclotron frequency. Thus (ignoring hats)
¨Px + ω2
cPx
=
0,
¨Py + ω2
cPy
=
0,
(45.17)
and one can easily show that electrons perform circular orbits around (X, Y ).
As with many subjects in quantum ﬁeld theory, the physics of this prob-
lem may be described using the machinery of the simple harmonic os-
cillator. To map this problem onto the simple harmonic oscillator we
introduce the operators
ˆa =
ℓB
√
2ℏ

ˆPx −i ˆPy

,
ˆa† =
ℓB
√
2ℏ

ˆPx + i ˆPy

,
ˆb =
1
√
2ℓB

ˆX + i ˆY

,
ˆb† =
1
√
2ℓB

ˆX −i ˆY

,
(45.18)
satisfying

ˆa, ˆa†
=
h
ˆb,ˆb†i
= 1 and
h
ˆa,ˆb
i
=
h
ˆa†,ˆb
i
= 0. The Hamiltonian
then assumes the expected form:
ˆH =
 ˆa†ˆa + 1
2

ℏωc and so there are
eigenstates |N⟩, where N is the number of a-quanta in the oscillator.
The energy levels of this system are called Landau levels.
However, note that we can write the states
|N, n⟩=
1
√
N!n!
(ˆa†)N(ˆb†)n|0⟩,
(45.19)
and so, even though the energy depends only on N, we shouldn’t forget
about n. This contributes to the degeneracy and, in fact, from this we
will now show that each electron occupies a real space area of πℓ2
B.
Example 45.3
The density of states in momentum space is given by
g(k)dk =
dk
(2π/L)2 × 2,
(45.20)

45.3
The integer quantum Hall eﬀect
415
where we write k ≡|k| = (k2
x + k2
y)
1
2 , the factor of 2 is for spin degeneracy and L is
the linear size of the system. The radius kN of the Nth Landau level in k-space is
found by writing
ℏ2k2
N
2m
=
„
N + 1
2
«
ℏωc,
(45.21)
and hence
k2
N = 1 + 2N
ℓ2
B
.
(45.22)
The area between two successive circles in k-space tells us how much momentum
space is taken up by each Landau level. It is given by
∆Ak = π(k2
N+1 −k2
N) = 2π
ℓ2
B
,
(45.23)
where the factor of 2 again accounts for spin. We conclude that, in each Landau
level, the number of states9 is given by
9Thus the degeneracy of a Landau level
is 2eB/h = 1/(πℓ2
B) per unit area of the
sample.
2π
ℓ2
B
1
(2π/L)2 × 2 = L2
πℓ2
B
= L2
„ 2eB
h
«
,
(45.24)
which has the form:
Number of states in one Landau level = (Area taken up by all of them)
(Area taken up by one of them) . (45.25)
So the eﬀective real-space area occupied by one Landau state within a Landau level
is πℓ2
B.
However, we also need to discuss the eﬀect of spin splitting. Each Landau level
contains electrons in both spin states and a magnetic ﬁeld will cause these to have
diﬀerent energies. The energy can be written
E = (N + 1
2)ℏωc ± 1
2g∗µBB,
(45.26)
where g∗is the eﬀective g-factor. This eﬀect will now double the number of levels
(these are now spin-split half Landau levels) and each of these levels now has half
the number of states in it.10 The eﬀective area occupied by one state in a spin-split
10Thus the degeneracy of a spin-split
half Landau level is eB/h = 1/(2πℓ2
B)
per unit area of the sample.
half Landau level is then11 2πℓ2
B.
11Note that 2πℓ2
B also equals the area
occupied by one Dirac ﬂux quantum.
45.3
The integer quantum Hall eﬀect
The Hall eﬀect depends on the number density of carriers and so our
treatment of the integer quantum Hall eﬀect will begin by considering
how many states are available for occupation by electrons in a two-
dimensional metal12 when subject to a large magnetic ﬁeld.
12Recall from Chapter 34 that we ex-
pect the conductance to vary as G(L) ∝
L(d−2), where d is the dimensional-
ity.
For d
=
2 we therefore ex-
pect behaviour that is independent of
the dimensions of the system, making
the quantum Hall phenomena we will
describe completely universal for two-
dimensional systems.
Example 45.4
A reminder of the physics of the conventional Hall eﬀect may be found in Exer-
cise 45.4. In brief, for an isotropic material Ohm’s law is written
„ Jx
Jy
«
=
„
σxx
σxy
−σxy
σxx
« „ Ex
Ey
«
,
(45.27)
and the Hall coeﬃcient is deﬁned as RH = σxy/B. In experiments we measure the
resistivities ρxx and ρxy, which are given by
ρxx ≡Ex
Jx
=
σxx
σ2xx + σ2xy
,
ρxy ≡Ey
Jx
=
σxy
σ2xx + σ2xy
,
(45.28)

416
The fractional quantum Hall ﬂuid
and in the high magnetic ﬁeld limit we ﬁnd |σxy| ≫|σxx| giving
ρxx ≈σxx
σ2xy
,
and
ρxy ≈
1
σxy
= RHB.
(45.29)
For a conventional system we expect ρxx to be a constant and ρxy to increase linearly
with ﬁeld. As shown in Fig. 45.2, systems approximating a two-dimensional electron
gas show something rather diﬀerent with increasing ﬁeld:
• Plateaux in ρxy occurring at values ρxy = 1
ν
h
e2 , with ν an integer [Fig 45.2(b)].
• Dramatic drops in ρxx which takes very small values when we have a plateau
in ρxy [Fig 45.2(a)].
• Maxima in ρxx in the transition region between the plateaux.
This is known as the integer quantum Hall eﬀect.13
13See
Singleton,
Band
Theory
and
Electronic
Properties
of
Solids,
for
more detail on Hall eﬀect physics.
Fig. 45.2 The quantum Hall eﬀect
in a GaAs-(Ga,Al)As heterojunction,
shown in resistivity components (a) ρxx
and (b) ρxy, measured at temperatures
between 0.03 and 1.5 K. (Figure from
J. Singleton, Band Theory and Elec-
tronic Properties of Solids, reprinted
with permission.)
(c) The fractional
quantum Hall eﬀect in a semiconduc-
tor heterojunction, shown in resistivity
components ρxy (dotted line) and ρxx
(solid line). Figure from P. Gee, D.Phil.
Thesis, Clarendon Laboratory, Univer-
sity of Oxford (1997).
0
25.8128
50
75
ρxy (kΩ)
0
2
4
ρxx (kΩ/□)
0
5
10
15
B (T)
(c)
The ﬁrst step in studying the quantum Hall eﬀect is to conﬁne an elec-
tron gas into a two-dimensional region.14 A magnetic ﬁeld is then applied
14Experimentally, this is accomplished
by ensuring that electrons occupy the
lowest sub-band in a quantum well in a
semiconductor heterostructure.
perpendicular to this two-dimensional electron gas. As shown in Exam-
ple 45.3 each spin-split Landau level has a number of states per unit
area of sample equal to 1/(2πℓ2
B). If the number of electrons per unit
area is n2d, then we deﬁne the ﬁlling factor ν by
ν
=
(Areal density of electrons)
(Areal density of states in a (spin-split half) Landau level)
=
n2d2πℓ2
B = n2dh
Be .
(45.30)
As a result, whenever the magnetic ﬁeld takes a value
B = n2dh
e
× 1
p,
(45.31)
where p is an integer, then the ﬁlling factor ν will be an integer and
hence ν (spin-split half) Landau levels15 will be completely ﬁlled.
15The ﬁlling factor ν counts the num-
ber of spin-split half Landau levels.
However, from now on we will simply
refer to the spin-split levels as Landau
levels. Whether the electrons interact
with the magnetic ﬁeld via their orbital
(cyclotron) motion or via their spin an-
gular momentum, the end result is the
same: orbits with discrete energy lev-
els; hence we will call these energy lev-
els Landau levels, and ν of them are
ﬁlled.

45.4
The fractional quantum Hall eﬀect
417
This then has the interesting consequence that the system realizes
a macroscopic quantum state in which there is an energy gap for the
creation of all charged excitations. The system is an incompressible
liquid, essentially because the energy of the system becomes indepen-
dent of area when the Landau levels are full (i.e. integer ν). In this case,
we can use the fact that the Hall coeﬃcient RH = −1/(n2de) to ﬁnd
that the Hall resistivity ρxy is16
16To prevent a minus sign here, we use
the convention that B < 0, so that
(−e)B > 0. This convention is also fol-
lowed in the discussion in Chapter 7 of
Wen.
ρxy = RHB =
B
n2de = 1
ν
h
e2 .
(45.32)
Thus we ﬁnd plateaux in ρxy whenever ν Landau levels are full. Re-
markably, the resistivity ρxy which is measured with a battery and some
lengths of wire depends only on two fundamental constants of Nature.
The quantity h/e2 = 25812.8 Ωhas been measured with incredible pre-
cision and is often called the resistance quantum.17 Equivalently, one
17It is also known as the von Klitz-
ing contant, after Klaus von Klitzing
(1943– ) the discoverer of the quantum
Hall eﬀect.
can write eqn 45.32 in terms of the Hall conductivity σxy as
σxy = ν e2
h .
(45.33)
Before feeling too pleased with ourselves we should remember that this
analysis only tells us that ρxy is quantized at those values of magnetic
ﬁeld at which an integer number of Landau levels are completely ﬁlled,
but does not explain the occurrence of the plateaux nor the vanishing
of ρxx. In fact, these two features rely crucially on the fact that some
of the states in a Landau level are localized via Anderson localization
(described18 in Chapter 34 ). The idea is that we imagine starting at a
18See Singleton for a simple explana-
tion of how this works in this context
or, for more detail, the article by J.T.
Chalker in Ecole des Houches: Topolog-
ical Aspects of Low Dimensional Sys-
tems, Eds. A. Comtet, T. Jolicoeur and
S. Ouvry. (1998).
ﬁeld where a single Landau level is exactly ﬁlled and then further increase
the magnetic ﬁeld. This reduces the degeneracy of the Landau level,
forcing some electrons into the next Landau level at a higher energy.
These electrons will end up in the localized states of that upper level
and hence cannot contribute to the conductivity. As a result ρxx = 0,
while ρxy remains at the quantized value corresponding to the complete
ﬁlling of the lower level, resulting in a plateau.
45.4
The fractional quantum Hall eﬀect
When ν = 1 only a single Landau level is occupied and this maximizes
the eﬀect of electron–electron interactions. Although our treatment of
the integer quantum Hall eﬀect ignored these and treated the electrons as
non-interacting, this will no longer be suﬃcient. The intriguing physics
that is revealed when ν < 1 is accessed by using magnetic ﬁelds with
B > n2dh/e. In addition to the plateaux observed at integer values of ν,
experiments in high-quality samples revealed further plateaux at frac-
tional values of ν [see Fig. 45.2(c)]. These extra plateaux are due to the
formation of a state of matter which can support fractional excitations
and which is descibed by a topological ﬁeld theory of the sort we met in
Chapter 30.19
19Although we haven’t stressed topol-
ogy in our treatment of the integer
quantum Hall eﬀect, it may also be un-
derstood in these terms.
See Altland
and Simons, Section 9.3 for the details
of Pruisken’s ﬁeld theory of the inte-
ger quantum Hall eﬀect and its link to
topology.

418
The fractional quantum Hall ﬂuid
We will assemble a quantum ﬁeld theory that describes this fractional
quantum Hall (FQH) eﬀect.20 This will be an eﬀective theory in that
20The original explanation of the FQH
eﬀect was made by Robert Laughlin
based on a trial wave function ap-
proach. We won’t pursue this method
here. The interested reader should con-
sult Wen, Chapter 7 for a more de-
tailed description of the other possible
approaches to describing the physics.
we won’t start with details of the electrons and holes found in a metal,
rather with a set of ﬁelds that will model the ground state and the low
energy or elementary excitations.
This is an important point. Nature doesn’t allow us the luxury of
being able to derive the properties of complex phases of matter from ﬁrst
principles. In the same way that the rigidity of solids is not derivable
from the physics of those atoms in a gas, so the FQH eﬀect is a diﬀerent
phase of matter from the two-dimensional electron gas from which it
is created.21 The properties of the phase of matter that give rise to a
21This is a form of emergence, where
the properties and excitations of a
phase of matter arise upon the conden-
sation of that phase.
fractional quantum Hall eﬀect cannot, therefore, be derived from the
electron gas in zero magnetic ﬁeld. So while most phases are separated
by a breaking of symmetry, as discussed in Chapter 26, this one is very
diﬀerent. The FQH phase is topologically distinct from the electron gas.
The phase transition between electron gas and FQH state is therefore
topological and cannot be described by a broken symmetry theory as we
had for the magnet, for example.
In the remainder of this chapter we will construct a quantum ﬁeld
theory of the FQH ﬂuid, which is a phase of matter showing a FQH
eﬀect in its ground state. Speciﬁcally, we will explain the existence of
a subset of the possible FQH states: those with 1/ν equal to an odd
integer.22 We will also examine the elementary excitations of the FHQ
22A slightly more general, but basically
similar, theory is needed to explain the
others. Again, see Wen, Chapter 7 for
the full story.
ﬂuid, which are the quasiparticles of this phase.23
23We stress again that these quasipar-
ticles can’t be expected to be elec-
trons with a shielded charge and in-
creased mass as we had for quasipar-
ticles in QED and in metals. In fact,
the FQH quasiparticles will turn out
to have quite diﬀerent properties from
electrons and holes, including fractional
statistics and even fractional electric
charge!
We begin by asking what we want from our theory. Obviously we want
the theory to predict the fundamental facts of the FQH eﬀect. These
are:
• In units where ℏ= 1, we want to predict a conductivity σxy =
Jx/Ey = e2
2πν with 1/ν taking odd integer values.
• The FQH ﬂuid is incompressible with a charge density given by
ρ = −eJ0
FQH
=
−e ×
 Number of states per
Landau level

× ν
L2
=
−e2B
2π ν,
(45.34)
where we’ve used the fact that the number of states per unit area
L2 in a Landau level is 1/(2πℓ2
B).
These two facts are embodied in an equation describing the electromag-
netic FQH current24
24Recall from Chapter 30 that in (2+1)
dimensions we may write the magnetic
ﬁeld as B = ∂1A2 −∂2A2 and, follow-
ing convention, we take B < 0 so that
(−e)B > 0.
−eJµ
FQH = −νe2
2π εµνλ∂νAλ,
(45.35)
where Aµ is the usual U(1) gauge ﬁeld of electromagnetism. We will cook
up a Lagrangian whose equation of motion, at the very least, predicts
this current.
The Lagrangian which will do this is of the Chern–Simons form and is
thus a topological theory. It will describe another U(1) gauge ﬁeld aµ,

45.4
The fractional quantum Hall eﬀect
419
which couples to the electromagnetic ﬁeld Aµ. We know from Chapter 30
that in a Chern–Simons theory the ﬁeld aµ gives rise to a current Jµ
CS ∝
εµνλ∂νaλ. We choose to adjust our normalization slightly25 here and
25We simply replace κ in Chapter 30
with 1/2π here.
will deﬁne the Chern–Simons current as
Jµ
CS = 1
2π εµνλ∂νaλ.
(45.36)
The key to constructing the theory is to say that the FQH current in
eqn 45.35 will contribute to the total Chern–Simons current. Our task
is now to ﬁnd a Lagrangian whose equation of motion tells us that the
Chern–Simons current JCS in eqn 45.36 is given by a contribution from
the FQH current in eqn 45.35 added to a contribution from any source
jµ of quasiparticles that we put in the system. This is achieved with the
Lagrangian
L = −1
2
s
2π εµνλaµ∂νaλ + e
2π εµνλAµ∂νaλ + jµaµ,
(45.37)
where s is a number. This Lagrangian is formed from three terms: I:
a Chern–Simons term; II: a term where the electromagnetic gauge ﬁeld
Aµ couples to the Chern–Simons current; III: a term where a source of
quasiparticles jµ couples to the topological gauge ﬁeld aµ.
Example 45.5
We can check that this works by feeding the Lagrangian through the Euler–Lagrange
equation to extract the equation of motion. We ﬁnd that
∂ν
∂L
∂(∂νaµ) = 1
2
s
2π εµνλ∂νaλ −
e
2π εµνλ∂νAλ,
∂L
∂aµ = −1
2
s
2π εµνλ∂νaλ + jµ.
(45.38)
We obtain
s
2π εµνλ∂νaλ = e
2π εµνλ∂νAλ + jµ,
(45.39)
or
Jµ
CS =
e
2πs εµνλ∂νAλ + 1
s jµ.
(45.40)
If we identify the dimensionless parameter s from the Lagrangian as being 1/ν, and
temporarily set the quasiparticle source jµ = 0 then this is the fractional quantum
Hall result we set out to predict.
Our low-energy Lagrangian would be of little use if it didn’t provide
some further insight into the workings of the fractional quantum Hall
ﬂuid. We will obtain these by asking about the quasiparticle excitations
in the FQH ground state which are created by the conserved current jµ.
Recall from Chapter 30 that coupling a current like jµ to the topological
gauge ﬁeld aµ constrains the properties of these particles, attaching the
ﬂux of the ﬁeld aµ to each particle created by jµ and imbuing them wth
fractional statistics. Speciﬁcally we will use the theory to show that:
• the FQH plateaux appear for half odd integer s;
• the FQH quasiparticles carry fractional electromagnetic charge;
• the FQH quasiparticles have fractional statistics.

420
The fractional quantum Hall ﬂuid
Our source of FQH quasiparticles jµ will creates excitations with aµ-
charge26 l, where l is constrained to be an integer. We will construct the
26Potential for confusion arises here as
there are two gauge ﬁelds in this prob-
lem: (i) the U(1) gauge ﬁeld of elec-
tromagnetism Aµ, which gives rise to
the electric ﬁeld E and magnetic ﬂux
density B, whose excitations couple to
the charge −e; and (ii) the topological
U(1) gauge ﬁeld aµ, giving rise to an
electric ﬁeld e and magnetic ﬂux den-
sity b, whose excitations couple to the
charge l. The FQH quasiparticles carry
integral aµ-charge l and also Aµ-charge
q, whose value we will determine. As
the current jµ of FQH quasiparticles is
coupled to the topological ﬁeld aµ we
expect, from Chapter 30, that they will
also carry ﬂux of the b-ﬁeld (which we
refer to as aµ-ﬂux).
source term jµaµ = la0δ(2)(x −x0), where we have chosen to localize
the quasiparticle source at position x0. The zeroth component of the
equations of motion predicts a total electric charge density of
−eJ0
CS = e2
2πsB −el
s δ(2)(x −x0),
(45.41)
where the ﬁrst term on the right-hand side is the electric charge density
expected from the FQH ground state and the second term is the extra
electric charge of the quasiparticle. This equation implies two things:
ﬁrstly that the FQH quasiparticle have electric charge −el/s; secondly,
returning to the properties of Chern–Simons theories from Chapter 30,
we see that this excitation carries aµ-ﬂux of 2πl/s. This is progress, but
we have yet to establish the values of s.
Next we ﬁnd the statistics of the FQH quasiparticles. We picture two
such excitations with charges l1 and l2. Move one in a circle around the
other and we obtain an Aharonov–Bohm phase:
∆Φ
=
(aµ ﬂux) × (quasiparticle charge)
=
2π l1
s × l2.
(45.42)
Now we set l1 = l2 = l to obtain identical quasiparticles. Using the
method of Chapter 30,27 we note that exchanging these particles means
27In
Chapter
30
we
saw
that
the
Chern–Simons charge density is related
to the ﬂux density via ρ(= J0) = −κb
where, for our quasiparticles ρ = l/s
per unit area. Note also that κ =
1
2π
and we take b < 0.
rotating one through an angle π around the other so that we expect a
phase change πη, where η will determine the statistics of the quasipar-
ticles. The corresponding phase change ∆Φ is half that in eqn 45.42,
implying that we have statistics given by
η = l2
s .
(45.43)
Our conclusions so far are that the excitations of the FQH ground state
are quasiparticles with aµ-charge of l, which carry an electric charge
−el/s, carry aµ-ﬂux of 2π/s, and cause a phase change on exchange
governed by η = l2/s. Since we know that ν = 1/s, which experiment
teaches us takes values such as 1/3, this suggests that the quasiparticles
carry a fraction of an electron charge and possess fractional statistics!
However we need to complete the puzzle by establishing the necessity
for s to be an odd integer.
In order to complete the description it turns out that we must appeal
to the presence of electrons and holes in the metal and link these to
our FQH quasiparticles.
Following convention, we will call our FQH
quasiparticle excitations quasielectrons and quasiholes since, despite the
fact that we cannot adiabatically tune between the FQH quasielectrons
and the quasielectrons of a metal, we hope that their properties can
at least be related.
To establish the relation, we note that a single
electron has electric charge −el/s = −e, it therefore has l/s = 1 and
carries l = s units of aµ-charge. Its exchange property follows as η =

Exercises
421
s. However, we know that electrons are fermions and must therefore
have an exchange angle η of an odd integer. We therefore deduce that
s = odd integer, which is exactly the property we wanted to explain.
We may now conclude that the FQH quasiparticle has fractional electric
charge −el/s and fractional statistics governed by η = l2/s.
Example 45.6
Let’s examine the case of s = 3. Electrons with l = s = 3 units of aµ-charge form
the FQH ﬂuid with ﬁlling factor ν = 1/3. The excitations of this liquid are FQH
quasielectrons with aµ-charge l = −1 and quasiholes with l = 1.
Quasielectrons
have fractional electric charge −e/3, they carry aµ-ﬂux of 2π/3 and have statistics
governed by η = 1/3.
Chapter summary
• In a magnetic ﬁeld the Hamiltonian is invariant under a combina-
tion of a translation and a gauge transformation. This leads to the
physics of the Hofstadter butterﬂy
• The integer quantum Hall eﬀect follows from considering the oc-
cupancy of Landau levels as a function of applied magnetic ﬁeld in
a two-dimensional electron gas.
• The fractional quantum Hall eﬀect is caused by interactions and
may be explained using an eﬀective, topological ﬁeld theory.
Exercises
(45.1) Verify eqn 45.8 using the method suggested in the
text.
(45.2) Verify eqns 45.14 and 45.15.
(45.3) (a) Use the creation and annihilation operators de-
ﬁned in eqns 45.18 to diagonalize the Hamiltonian
in eqn 45.11.
(b) Evaluate ˆ
X2 + ˆY 2 to show that each electron
occupies an area of 2πℓ2
B.
(45.4) A reminder of the Hall eﬀect.
Consider driving a current of density Jx in the x-
direction along a ﬁnite sized conductor, in the pres-
ence of a magnetic ﬁeld B directed along z. Con-
sider the equations of motion of a charge in the
relaxation time approximation
m¨v = −mv
τ
+ qE + qv × B,
(45.44)
where τ is the relaxation time.
(a) In the steady state, show that Ey/Ex = −ωcτ.
(b) Verify that the Hall coeﬃcient is given by
RH = Ey
JxB = −1
ne,
(45.45)
where n is the electron density.


Part XI
Some applications from the
world of particle physics
Quantum ﬁeld theory is foundational for particle physics, since particles
are simply excitations in quantum ﬁelds. In this part, we review several
important particle physics theories which use the ideas developed in this
book.
• Our treatment of gauge theory in Chapter 14 assumed that the
underlying symmetry group is abelian (i.e. its elements commute).
This is the case for U(1), the group that describes electromag-
netism. In Chapter 46 we introduce non-abelian gauge theory and
show that the non-commutativity of group elements leads to a non-
linear ﬁeld tensor.
• A non-abelian gauge theory is at the heart of the electroweak the-
ory due to Weinberg and Salam and this is presented in Chapter 47.
Symmetry breaking of the Higgs ﬁeld results in the emergence of
the photon, the charged W + and W −particles, and the neutral
Z0 particle. The photon is massless, but the other three acquire
mass and so the weak force becomes short-ranged.
• Majorana fermions, particles that are their own antiparticles, are
described in Chapter 48.
• Chapter 49 introduces two varieties of magnetic monopole: one
due to Dirac which provides a link between the quantization of
electric charge and the quantization of the magnetic charge of a
monopole; the other due to ’t Hooft and Polyakov which arises
from a non-abelian gauge theory.
• The ﬁnal chapter, Chapter 50, introduces the concept of instan-
tons, which allows us to treat problems involving tunnelling. We
ask what would happen if the Universe were to tunnel from a false
vacuum into the true vacuum.

46
Non-abelian gauge theory
46.1 Abelian gauge theory revis-
ited
424
46.2 Yang–––Mills theory
425
46.3 Interactions and dynamics of
Wµ
428
46.4 Breaking symmetry with a
non-abelian
gauge
theory
430
Chapter summary
432
Exercises
432
Physics is very muddled again at the moment; it is much too
hard for me anyway, and I wish I were a movie comedian
or something like that and had never heard anything about
physics.
Wolfgang Pauli (1900–1958)
QED is a very successful ﬁeld theory built on the gauge principle which
accurately describes electromagnetism. Applying this principle involved
taking a theory with a global (internal) U(1) symmetry and promoting
this to a local symmetry, necessitating the introduction of a gauge ﬁeld
Aµ(x). Turning our attention now to particle physics, we would also like
to explain the weak and strong forces using ﬁelds. The weak and strong
interactions can also be described by gauge theories, but unfortunately
neither are based on the simple case of local U(1) symmetry. In fact,
both are based on local symmetry under more complicated groups of
transformations: for the weak interaction it is the group U(1) ⊗SU(2)
and for the strong interaction it is SU(3). Unlike U(1), these groups are
non-abelian.
A non-abelian group is formed from elements that don’t commute: a
Niels Hendrik Abel (1802–1829), af-
ter whom abelian groups are named,
died aged 26 from tuberculosis.
Non-
abelian group theory was introduced by
Evariste Galois (1811–1832) who was
killed in a duel aged 20, having spent
the entire night before writing down a
revolutionary theory involving the use
of groups to investigate the solubility of
algebraic equations.
transformation by element a followed by one by element b has a diﬀerent
eﬀect to the transformation by b followed by a.
Non-abelian groups
are quite familiar from everyday life, the rotation group SO(3) being
a simple example. Figure 46.1 shows that a rotation R1 of a book by
π/2 about x followed by rotation R2 by π/2 about z leads to a diﬀerent
ﬁnal orientation of the book from the rotations R2 then R1. In this
x
y
z
R2
x
y
z
R1
x
y
z
x
y
z
R1
x
y
z
R2
x
y
z
(a)
(b)
Fig. 46.1 A demonstration that the el-
ements of the non-abelain group SO(3)
don’t commute.
chapter we examine the features of a gauge theory with a local non-
abelian symmetry. We will concentrate on the simplest possible non-
abelian cases SU(2) and SO(3). We start by reviewing the important
properties of an abelian gauge theory.
46.1
Abelian gauge theory revisited
In Chapter 14 we looked at the simplest possible gauge theory. Fun-
damentally it is a theory with a local symmetry. It is symmetric with
respect to U(1) transformations which are diﬀerent at every point in
space time. U(1) is abelian since two transformations, applied one after
another, always lead to the same result, no matter the order in which
they’re applied.
As a reminder, here is the complex scalar ﬁeld La-

46.2
Yang–––Mills theory
425
grangian:
L = (Dµψ)†(Dµψ) −m2ψ†ψ −1
4FµνF µν,
(46.1)
where Dµψ = (∂µψ + iqAµψ). This is a locally gauge invariant quantity
under the simultaneous transformations
ψ →ψeiα(x),
Aµ →Aµ −1
q∂µα(x).
(46.2)
In order to make the jump to more complicated symmetries we will deal
with these in the form of inﬁnitesimal transformations.
Example 46.1
We’ll replay the proofs that the Lagrangian is invariant with respect to local gauge
transformations using inﬁnitesimal transformations.
ψ
→
(1 + iα)ψ,
∂µψ
→
∂µψ + i(∂µα)ψ + iα(∂µψ),
Aµ
→
Aµ −1
q ∂µα(x).
(46.3)
This works up to order O(α) because we have
ψ†ψ
→
(ψ† −iαψ†)(ψ + iαψ) = ψ†ψ + O(α2),
(46.4)
and
Dµψ
→
»
∂µψ + i(∂µα)ψ + iα(∂µψ) + iq
„
Aµ −1
q ∂µα
«
(ψ + iαψ)
–
=
[∂µψ + iqAµψ] + iα [∂µψ + iqAµψ] + O(α2)
=
(1 + iα)Dµψ + O(α2).
(46.5)
Equation 46.5 shows that Dµψ transforms exactly the same way as ψ. This, in turn,
means that (Dµψ)†(Dµψ) →(Dµψ)†(Dµψ) + O(α2). So all of the terms involving
ψ are invariant with respect to the transformations.
46.2
Yang–––Mills theory
In 1954 Chen-Ning Yang and Robert Mills asked if the theory of local
Chen-Ning Yang (1922– )
Robert Mills (1927–1999).
A gauge theory based on the SU(N)
group is called a Yang–––Mills the-
ory in honour of Yang and Mills who
extended gauge theory to non-abelian
groups in 1954.
symmetry could be extended to more complicated groups of transfor-
mations than U(1), such as SU(2) and SO(3). These groups are non-
abelian. Although an SU(2) gauge theory doesn’t describe the weak or
strong interactions, it does display the properties of a more complicated
non-abelian theory and so we will consider it in this chapter.
We ask what needs to be done to make a theory with a global internal
SU(2) symmetry also symmetric under local internal SU(2) transfor-
mations. We’ll use exactly the same strategy we used to upgrade the
global internal U(1) symmetry to a local one. We will ﬁrst work out the
consequences of imposing a local symmetry and then we will add a ﬁeld
which cancels out any extra terms that arise.

426
Non-abelian gauge theory
Here’s the Dirac Lagrangian for two sorts of fermions, each with mass
m:
L = ¯f(iγµ∂µ −m)f + ¯g(iγµ∂µ −m)g,
(46.6)
which we write more compactly as
L = ¯Ψ(iγµ∂µ −m)Ψ,
(46.7)
where
Ψ =
 f
g

,
¯Ψ =
  ¯f
¯g

.
(46.8)
This Lagrangian is invariant under a global SU(2) transformation
Ψ →e
i
2 τ·αΨ,
¯Ψ →¯Ψe−i
2 τ·α,
(46.9)
where τ = (τx, τy, τz) are the Pauli matrices.1
The Noether currents
1We use
τ
for the Pauli
matrices
here, rather than σ, to emphasize that
they act on our states of two sorts of
fermions, ¯Ψ =
`
¯f
¯g ´
, rather than
on the spin of a single fermion, i.e. they
are Pauli ‘isospin’ matrices, rather than
Pauli ‘spin’ matrices.
Noether’s theorem: SU(2) inter-
nal symmetry
DaΨ = i
2 τ aΨ
(a: internal)
Πµ
Ψ = ¯Ψiγµ
(µ: vector)
DL = 0
W µ = 0
Jµa
Nc = 1
2 ¯Ψγµτ aΨ
Qa
Nc =
R
d3x 1
2Ψ†τ aΨ
arising from this symmetry are given in the box. The conserved charge-
like quantity for this theory is the isospin ˆI =
R
d3x ˆΨ† τ
2 ˆΨ.
As examined in the exercises,
ˆIz
=
1
2
R
d3x( ˆf† ˆf −ˆg†ˆg). We might say that
the Ψ-ﬁeld carries I = 1/2 units of
isospin, with f-on particles correspond-
ing to I3 = 1/2 and g-on particles to
I3 = −1/2. Isospin will be important
in the next chapter.
To investigate further we’ll be working with inﬁnitesimal transforma-
tions, so in inﬁnitesimal form we can write
Ψ →

1 + i
2τ · α

Ψ.
(46.10)
Example 46.2
We can show that this is a global symmetry of our Lagrangian. As before we write
¯ΨΨ
→
¯Ψ
„
1 −i
2τ · α
« „
1 + i
2τ · α
«
Ψ
=
¯ΨΨ
ˆ
1 + O(α2)
˜
.
(46.11)
Since we’re working with inﬁnitesimals, we agree to jettison any term above ﬁrst
order. Since, for a global symmetry the derivative ∂µΨ transforms exactly as Ψ does,
(since α has no dependence on spacetime) the entire Lagrangian is invariant.
Now for the pivotal point of this chapter. We upgrade this argument to
a local transformation by letting α be a function of spacetime x. This
is no problem for the mass term in the Lagrangian ¯ΨmΨ, which is still
invariant. The trouble comes from the derivative, since
∂µΨ →∂µΨ + i
2 [τ · α(x)] ∂µΨ + i
2 [τ · ∂µα(x)] Ψ,
(46.12)
and the ﬁnal term in eqn 46.12 prevents ∂µΨ transforming like Ψ and
therefore prevents the derivative term from being invariant.2
2Remember that the U(1) version of
this is ∂µψ →∂µψ+iα(∂µψ)+i(∂µα)ψ.
Just as we introduced the gauge ﬁeld Aµ(x) for the U(1) case, we
need to introduce a new gauge ﬁeld which will mop up the term in-
volving ∂µα(x). This gauge ﬁeld is called Wµ(x) and has three internal
components [that is (W 1
µ(x), W 2
µ(x), W 3
µ(x))], each of which have four
components in Minkowski spacetime (labelled with the index µ). Just

46.2
Yang–––Mills theory
427
as before, the new ﬁeld is combined with the derivative operator to make
a covariant derivative. We therefore deﬁne a covariant derivative3
3Recall that for U(1) we have Dµ =
∂µ + iqAµ.
The equation here is for-
mulated analogously with, by conven-
tion, a sign change reﬂecting the fact
that our chosen SU(2) rotation e
i
2 τ ·α
rotates in the opposite (internal) direc-
tion to the U(1) rotation eiα.
Dµ = ∂µ −i
2gτ · Wµ(x),
(46.13)
where g is the charge of the theory (it’s the quantity which will even-
tually tell us how strongly the gauge ﬁeld Wµ interacts with Ψ).
If
this procedure is going to work, then DµΨ must transform in exactly
the same way as Ψ does. That is, the manner in which the gauge ﬁeld
transforms must cancel out the extra term in the transformation of the
ordinary derivative.4 This will indeed be the case, but the fact that the
4Note
that,
written
out
ex-
plicitly,
the
covariant
deriva-
tive
reads
DµΨ
=
„ ∂µf
∂µg
«
−
i
2 g
„
W 3
µ
W 1
µ −iW 2
µ
W 1
µ + iW 2
µ
−W 3
µ
«„ f
g
«
.
transformation is non-abelian leads to a new complication.
Example 46.3
We want the derivative to transform in the same way as Ψ, which is to say that it
should transform as
DµΨ →
„
1 + i
2τ · α(x)
«
DµΨ.
(46.14)
This implies
DµΨ
→
„
1 + i
2τ · α(x)
« „
∂µΨ −i
2 gτ · Wµ(x)Ψ
«
=
∂µΨ −i
2 gτ · Wµ(x)Ψ + i
2τ · α(x)∂µΨ
−
„ i
2
«2
g [τ · α(x)] [τ · Wµ(x)Ψ] .
(46.15)
So how do we get it to work? We suppose that the covariant derivative transforms
as
Dµ →∂µ −i
2gτ · Wµ(x) −i
2gτ · δWµ(x),
(46.16)
where δWµ(x) is the change in the gauge ﬁeld Wµ(x) with the transformation. We
also know that the ﬁeld transforms as Ψ →
ˆ
1 + i
2 τ · α(x)
˜
Ψ.
Combining these
transformations shows that
DµΨ
→
„
∂µ −i
2 gτ · Wµ(x) −i
2 gτ · δWµ(x)
« „
1 + i
2 τ · α(x)
«
Ψ
=
∂µΨ + i
2τ · [∂µα(x)] Ψ + i
2τ · α(x)∂µΨ
−i
2gτ · Wµ(x)Ψ −
„ i
2
«2
g [τ · Wµ(x)] [τ · α(x)] Ψ
−i
2gτ · δWµ(x)Ψ,
(46.17)
ignoring the αδWµ term as it’s second order.
By comparing terms in eqns 46.15 and 46.17 we ﬁnd that for our gauge ﬁeld to
work we require
τ · δWµ(x) = 1
g τ · [∂µα(x)] + i
2 {[τ · α(x)] [τ · Wµ(x)] −[τ · Wµ(x)] [τ · α(x)]} .
(46.18)
Notice that this last term survives only because the transformation is non-abelian
and the two transformations involve τ entering in diﬀerent orders. Finally, we can
use the identity
(τ · a)(τ · b) = (a · b) + iτ · a × b,
(46.19)
and conclude that
τ · δWµ(x) = 1
g τ · [∂µα(x)] −τ · [α(x) × Wµ(x)] .
(46.20)

428
Non-abelian gauge theory
The result of the preceding example is that the gauge ﬁeld must
transform5 according to
5For comparison, in the U(1) case this
transformation is Aµ →Aµ −1
q ∂µα.
τ · Wµ →τ · Wµ + 1
g τ · (∂µα) −τ · (α × Wµ).
(46.21)
46.3
Interactions and dynamics of Wµ
Next we ask how the gauge ﬁeld couples to the fermion particle ﬁeld Ψ.
Minimal coupling, which we ﬁrst met in Chapter 14, tells us that this
is simply a matter of multiplying out the term in the Dirac Lagrangian
involving the covariant derivative Dµ. We have
¯ΨiγµDµΨ = ¯Ψiγµ∂µΨ + g
2
¯Ψγµτ · WµΨ.
(46.22)
Compare this to the U(1) version
¯ψiγµDµψ = ¯ψiγµ∂µψ −q ¯ψγµAµψ,
(46.23)
which told us that the excitations in the U(1) gauge ﬁeld Aµ(x) mediated
interactions between electrons by coupling to the charge q. Furthermore,
the Feynman rule for the interaction was that a ¯ΨΨAµ interaction vertex
contributed a factor −iqγµ. By analogy, the SU(2) version suggests that
the Feynman rule for a ¯ΨΨWµ vertex is the contribution of a factor
proportional to igτγµ. After quantization, there will be excitations in
the Wµ gauge ﬁeld which mediate the interactions between the fermions
of the theory. Explicitly, we expect there to be massless W 1, W 2 and W 3
photon-like particles, each with two possible transverse polarizations.
The particles couple to the isospin of the Ψ ﬁeld with a strength set by
the charge g.
When we looked at the U(1) theory, an additional, but very impor-
tant contribution to the Lagrangian was provided by the gauge ﬁeld
Aµ(x) itself, whose dynamics gave us electromagnetism through the term
−1
4FµνF µν. We now ask what contribution the gauge ﬁeld Wµ(x) makes
to the Lagrangian. An obvious guess would be that we should add a term
of the form −1
4Gµν · Gµν where Gµν is a tensor analogous to Fµν in the
electromagnetic case. The obvious guess for the form of this tensor is
Gµν = ∂µWν −∂νWµ, but since we will want Gµν to transform like Φ
and ∂µΦ, it turns out that the tensor we need is6
6This is a surprising result: the ﬁeld
strength Gµν is a nonlinear function
of the gauge ﬁeld Wµ.
We thus have
found ourselves a nonlinear interacting
theory, even in the absence of matter!
Gµν = ∂µWν −∂νWµ + g(Wµ × Wν).
(46.24)
Our ﬁnal expression for the Lagrangian of a locally invariant SU(2)
theory is then given by
L = ¯Ψ(iγµDµ −m)Ψ −1
4Gµν · Gµν.
(46.25)
The incredible thing about this Lagrangian is that the nonlinearity of
Gµν allows a new sort of process to take place, namely, the direct in-
teraction of particle excitations in one of the Wµ ﬁelds with other Wµ

46.3
Interactions and dynamics of Wµ
429
particles, as shown in the interaction vertex in Fig. 46.2, where we have
three Wµ particles interacting. (The Lagrangian also allows the inter-
action of four Wµ particles.7) Note that photon–photon interactions are
7The terms leading to the interactions
of three and four Wµ particles can be
seen by expanding Gµν · Gµν.
not possible in abelian electromagnetism as the photon carried no elec-
tromagnetic charge of its own. In contrast, the isospin ﬁeld Wµ carries
an isospin charge of one unit (I = 1) so remarkably it may act as a
source of itself.8
8Since the algebra of isospin mirrors
that of ordinary spin, we say that the
excitations of the I = 1 ﬁeld Wµ are
particles with Iz
=
0 created (and
destroyed) by the ﬁeld
ˆ
W 3
µ; particles
with Iz
= 1 created by the combi-
nation
1
√
2 ( ˆ
W 1
µ + i ˆ
W 2
µ) [and destroyed
by
1
√
2 ( ˆ
W 1
µ −i ˆ
W 2
µ)]; and particles with
Iz = −1 created by
1
√
2 ( ˆ
W 1
µ−i ˆ
W 2
µ) [and
destroyed by
1
√
2 ( ˆ
W 1
µ + i ˆ
W 2
µ)].
Fig. 46.2 The interaction of three W
particles is possible in a gauge theory
with local SU(2) symmetry.
This self-interaction of the gauge ﬁeld provides an explanation for a
feature that makes Yang–Mills theories so interesting: the asymptotic
freedom described in Chapter 34. As we probe further away from an
electromagnetic charge (described by abelian U(1) electromagnetism)
the charge is shielded by dipoles formed from virtual electron–positron
pairs and the electric charge appears to get smaller. However, the non-
abelian gauge ﬁeld produces a marked diﬀerence as we probe further
away from a quark (where the strong interaction of colour charge is
described by a non-abelian SU(3) gauge theory). The screening eﬀect
would be the same for the quark as for the electron, except that we have
the gauge ﬁeld self-interaction which allows the gauge ﬁeld quantum (a
gluon) to decay into a pair of gluons. As the gluons themselves carry
colour charge they lead to a proliferation of like colour charges around
the quark leading to an increased colour charge at large distances. This
is asymptotic freedom: the interaction gets stronger as we probe further
from the quark.
Example 46.4
A free, globally SO(3) invariant scalar ﬁeld theory was introduced in Chapter 13. Its
Lagrangian is
L = 1
2∂µΦ · ∂µΦ −m2
2 Φ · Φ,
(46.26)
where Φ = (φ1, φ2, φ3). This theory is invariant with respect to the global (internal)
rotation given by Φ(x) →Φ(x) −θ × Φ(x). Again, we upgrade this to a local
transformation by letting θ, the internal angle of rotation, be a function of spacetime
x. That is, the transformation is now Φ(x) →Φ(x)−θ(x)×Φ(x). To make a locally
invariant theory we need a gauge ﬁeld, leading to the covariant derivative:
DµΦ = ∂µΦ −gΦ × Wµ.
(46.27)
The correct Wµ to choose is one that transforms as
Wµ →Wµ + 1
g ∂µθ −(θ × Wµ),
(46.28)
where g is the charge of the theory, and as for SU(2) we add a term −1
4 Gµν · Gµν to
the Lagrangian in eqn 46.26, where for this theory Gµν = ∂µWν −∂νWµ + g(Wµ ×
Wν). This theory describes three massive scalar ﬁelds (the three components of Φ)
and three massless gauge ﬁelds (the three internal components of Wµ).
This completes our discussion of the properties of a non-abelian gauge
theory.9 As a summary, the gauge ﬁelds that we have met so far are
9The next logical step would be to
quantize the Yang–Mills Lagrangian.
This can be done using the path inte-
gral but requires some tricks that we
won’t pursue here.
See Peskin and
Schroeder, Chapter 16 for the details.
listed in the margin on page 430.

430
Non-abelian gauge theory
46.4
Breaking symmetry with a
non-abelian gauge theory
We saw in Chapter 26 that spontaneous symmetry breaking in a gauge
theory led to the famous Higgs mechanism. Here we treat exactly the
same problem with the more complicated example of a non-abelian
ﬁeld.
This is useful, not only because it provides yet another exam-
ple of the wonderful physics of symmetry breaking, but also because the
electroweak theory examined in the next chapter relies on exactly this
physics.
(1) Abelian U(1) gauge theory for
the complex scalar ﬁeld ψ:
L=(Dµψ)†(Dµψ)−m2ψ†ψ−1
4FµνF µν
where
Dµψ
=
∂µψ + iqAµ(x)ψ
Fµν
=
∂µAν −∂νAµ.
The symmetry is
ψ →ψeiα(x)
and the gauge ﬁeld transforms as
Aµ →Aµ −1
q ∂µα.
(2) Non-abelian SU(2) gauge the-
ory for fermions described by a
ﬁeld Ψ:
L = ¯Ψ(iγµDµ −m)Ψ −1
4Gµν · Gµν
where
DµΨ
=
∂µΨ −i
2gτ · Wµ(x)Ψ
Gµν
=
∂µWν −∂νWµ
+g(Wµ × Wν).
The symmetry is
Ψ →Ψe
i
2 τ ·α(x)
and the gauge ﬁeld transforms as
Wµ →Wµ + 1
g ∂µα −(α × Wµ).
(3) Non-abelian SO(3) gauge the-
ory for real scalar ﬁelds Φ:
L= 1
2 DµΦ·DµΦ−m2
2 Φ·Φ−1
4Gµν·Gµν
where
DµΦ
=
∂µΦ −gΦ × Wµ
Gµν
=
∂µWν −∂νWµ
+g(Wµ × Wν).
The symmetry is
Φ →Φ −θ(x) × Φ
and the gauge ﬁeld transforms as
Wµ →Wµ + 1
g ∂µθ −(θ × Wµ).
We will examine the case of SO(3) gauge theory with an φ4-type in-
teraction and, crucially, a positive, symmetry-breaking mass term, with
a Lagrangian given by
L = 1
2DµΦ · DµΦ + m2
2 Φ · Φ −λ(Φ · Φ)2 −1
4Gµν · Gµν.
(46.29)
The potential possesses a spherical shell of minima at a radius (in in-
ternal Φ space) of |Φ0| =

m2
4λ
 1
2 . Symmetry breaking involves picking
one of the inﬁnite number of equivalent vacua, which breaks global (and
local) symmetry. For simplicity, we chose the one that points along the
3-direction in isospin space, that is Φ0 =

m2
4λ
 1
2 ˆe3. For the excitations
above this ground state, we write
Φ =


Φ1(x)
Φ2(x)
|Φ0| + χ(x)

,
(46.30)
where χ(x) describes deviations in the 3-direction. Then, after a bout
of tedious algebra, we ﬁnd
L
=
1
2

(∂µΦ1)2 + (∂µΦ2)2 + (∂µχ)2
−4|Φ0|2λχ2
+g|Φ0|

(∂µΦ1)W 2
µ −(∂µΦ2)W 1
µ

+g2|Φ0|2
2

(W 1
µ)2 + (W 2
µ)2
−1
4Gµν · Gµν + . . . , (46.31)
where we ignore higher order terms. The ﬁrst line describes the dynamics
of the χ(x) scalar ﬁeld, the ﬁnal line the dynamics of the Wµ(x) vector
ﬁeld, but the terms that are mixed in components of Φ and Wµ aren’t
easy to interpret. However, there is a useful trick we can use: we select
a gauge which ensures that the excited state ﬁeld Φ points along the
3-direction in isospace at every point in spacetime. (This is known as
unitary gauge.) This removes all mention of the components Φ1 and
Φ2 and means that the excited state ﬁeld becomes simply
Φ(x) = [|Φ0| + χ(x)] ˆe3.
(46.32)

46.4
Breaking symmetry with a non-abelian gauge theory
431
Example 46.5
Since the term DµΦ = ∂µΦ −gΦ × Wµ mixes up the components of Φ, our choice
of unitary gauge yields
DµΦ1
=
gW 2
µ(|Φ0| + χ),
DµΦ2
=
−gW 1
µ(|Φ0| + χ),
DµΦ3
=
∂µχ,
(46.33)
giving,
(DµΦ)2 = (∂µχ)2 + g2|Φ0|2 ˆ
(W 1
µ)2 + (W 2
µ)2˜
+ . . .
(46.34)
This solves the problem of the mixing of components of Φ and Wµ ﬁelds.
The broken symmetry Lagrangian is now given by
L
=
1
2(∂µχ)2 −4|Φ0|λχ2
+g2|Φ0|2
2

(W 1
µ)2 + (W 2
µ)2
−1
4Gµν · Gµν + . . . , (46.35)
which is the sum of Lagrangians for a massive, single-component scalar
ﬁeld χ and a massive vector ﬁeld with two components W 1
µ and W 2
µ.
Symmetry breaking has caused two massive scalar ﬁelds (Φ1 and Φ2) to
disappear from our original Lagrangian and we are left with one, called
χ. The vector ﬁelds W 1
µ and W 2
µ have grown massive, taking on a mass
g|Φ0| (see Fig. 46.3). The ﬁeld W 3
µ remains massless.
Fig. 46.3
The gauge ﬁelds W 1
µ and
W 2
µ eat the Φ Goldstone modes and
acquire mass when global symmetry is
broken in a non-abelian gauge theory
with local SU(2) or SO(3) symmetry.
In summary, symmetry breaking of the SO(3) gauge theory causes:

3 massive scalar ﬁelds Φ
3 massless photon-like ﬁelds Wµ

→


1 massive scalar ﬁeld χ
2 massive vector ﬁelds W 1
µ, W 2
µ
1 massless photon-like ﬁeld W 3
µ

.
We may check that we haven’t lost any degrees of freedom. Massive
scalar ﬁelds only have a single degree of freedom, while massless photon-
like ﬁelds have two.10 This makes nine on the left. Noting that massive
10Remember the case of electromag-
netism!
vector ﬁelds have three degrees of freedom we see that we also have
nine on the right and all is well. Notice, ﬁnally, that we could describe
electromagnetism by taking W 3
µ(x) = Aµ(x). This immediately leads to
the question of whether electromagnetism in our Universe results from
breaking symmetry in an SO(3) or SU(2) non-abelian theory.
Is all this non-abelian gauge theory simply a mathematical exercise
to amuse the theoretically inclined? On the contrary, it seems that the
Universe has some non-abelian symmetries etched into it. In the next
chapter we will apply what we have learnt about symmetry breakdown
in a non-abelian gauge theory to formulating the electroweak theory
of Steven Weinberg and Abdus Salam, and we will discover that the
symmetry group we break is a little more complicated than SO(3).

432
Non-abelian gauge theory
Chapter summary
• A non-abelian group is formed from elements which do not com-
mute. The introduction of a non-abelian local symmetry leads to a
non-abelian gauge ﬁeld. The ﬁeld tensor has an extra term which
makes it nonlinear.
• The consequences of symmetry breaking were demonstrated for
SO(3) theory.
Exercises
(46.1) Show that ˆIz = 1
2
R
d3x( ˆf † ˆf −ˆg†ˆg).
(46.2) (a) Write an SO(3) rotation about the z-axis in in-
ﬁnitesimal form.
(b) Show that the Lagrangian in eqn 46.26 is invari-
ant with respect to global SO(3) transformations.
(c) We want to arrange matters so that the theory
is invariant with respect to local SO(3) transforma-
tion. As in the abelian case, the trouble comes from
the derivative term. Show that
∂µΦ(x)→∂µΦ(x) −∂µθ(x)×Φ(x) −θ(x)×∂µΦ(x).
(46.36)
(d) Verify that DµΦ transforms in exactly the same
way as Φ.
(46.3) (a) Show that, for U(1) gauge theory, the commu-
tator of the covariant derivatives gives
[Dµ, Dν] = iq(∂µAν −∂νAµ) = iqF µν.
(46.37)
This suggests that we can extract the analogue of
F µν for non-abelian gauge theory by evaluating the
commutator. This turns out to be the case!
(b) Show that the analogous commutator for SU(2)
gauge theory is
[Dµ, Dν] = i
2gτ · (∂µW ν −∂νW µ + gW µ × W ν)
= i
2gτ · Gµν.
(46.4) Verify eqn 46.31 and the simpliﬁed form eqn 46.35.

47
The Weinberg–––Salam
model
47.1 The
symmetries
of
Nature
before
symmetry
breaking
434
47.2 Introducing the Higgs ﬁeld
437
47.3 Symmetry
breaking
the
Higgs ﬁeld
438
47.4 The origin of electron mass
439
47.5 The photon and the gauge
bosons
440
Chapter summary
443
Exercises
443
As theorists sometimes do, I fell in love with this idea. But as
often happens with love aﬀairs, at ﬁrst I was rather confused
about its implications.
Steven Weinberg (1933– ) on symmetry breaking
I remember travelling back to London on an American
Air Force (MATS) transport ﬂight.
Although I had been
granted, for the night, the status of a Brigadier or a Field
Marshal – I forget which – the plane was very uncomfortable;
full of crying servicemen’s children – that is, the children
were crying, not the servicemen. I could not sleep. I kept
reﬂecting on why Nature should violate left-right symmetry
in the weak interactions.
Abdus Salam (1926–1996)
The Universe as we know it is the result of a symmetry breaking phase
transition. In this chapter we will explain the features of a Universe in
which:
• Electrons have mass, but neutrinos do not.1
1Of course neutrinos are not really
massless, as shown by neutrino ﬂavour
oscillation experiments. Their mass is,
however, undoubtedly small compared
to the electron mass and in this chapter
the Universe we describe is an approx-
imation to ours.
• Electrons may occur with left- or right-handed chirality, neutrinos
are only observed with left-handed chirality.
• The photon is massless.
The explanation of these fundamental properties emerges from a model
formulated by Abdus Salam and Steven Weinberg which unites the elec-
Abdus Salam (1926–1996) is notable,
not only for his many achievements in
quantum ﬁeld theory, but for a tireless
advocacy for the development of science
in the third world.
Steven Weinberg’s (1933– ) inﬂuence on
quantum ﬁeld theory is diﬃcult to over-
state. See his three volume masterwork
on the subject for a profound and dif-
ferent take on the material.
tromagnetic and weak interactions. At the heart of the Weinberg–Salam
electroweak theory lies a proposition: we live in a broken symmetry
Universe and the particles that we observe in Nature are a result of
the symmetry breaking.2 In order to make progress with this idea we
2Although we won’t consider muons
and tauons, these leptons may also be
included in addition to the electron.
will write down a Lagrangian for the electroweak Universe before this
symmetry breaking phase transition took place. We will propose a set
of local symmetries for the Lagrangian. As usual, this will require that
we introduce gauge ﬁelds to force the systems to be locally symmetric.
We will then examine the consequences of the spontaneous breaking of
symmetry on the particle spectrum of the system. Let’s make a start!

434
The Weinberg–––Salam model
47.1
The symmetries of Nature before
symmetry breaking
Imagine the Universe a moment after the Big Bang. It is more symmet-
rical than the Universe in which we live. Speciﬁcally, the lepton ﬁelds
enjoy an internal SU(2)⊗U(1) symmetry.3 In around 10−12 seconds the
3Strictly speaking this should be called
a U(2) symmetry. See Penrose for de-
tails.
Universe will cool below a temperature 1016 K and undergo a symmetry
breaking phase transition. Before this occurs we will assess the state of
the lepton ﬁelds of the Universe as described by the Weinberg–Salam
model.
Before symmetry breaking neutrinos and electrons are both massless
particles. Electrons are possible in both left- and right-handed forms,
but neutrinos only exist in left-handed form.4 Neutrinos and electrons
4That this model doesn’t provide an
explanation for this feature of Nature
has led to much work over the last few
decades regarding more general ‘super-
symmetries’ that may be required to
describe the Universe more completely.
are fermions, and are therefore excitations in Fermi ﬁelds. We know
what to expect from their Lagrangian: it’s a Dirac Lagrangian for each
ﬁeld, without any mass terms:
L = ¯νe(x)i✁∂νe(x) + ¯eL(x)i✁∂eL(x) + ¯eR(x)i✁∂eR(x),
(47.1)
where we’ve written the right-handed electron ﬁeld as eR(x) and so forth.
The interesting thing about having several ﬁelds in a theory is the
possibility of internal symmetries. As we have seen, symmetries lead to
conserved charges and one of our tasks in formulating the theory is to
decide the values of the conserved charges that excitations in each of
our ﬁelds will carry. Below we will assign two mysterious charges to our
ﬁelds, Y and I, which we apparently pluck from the air on the grounds
that they lead to a successful theory that agrees with all experimental
observations. It should be noted that these charges are related to the
more familiar electromagnetic charge Q via the Gell-Mann–Nishijima
relation:
Kazuhiko Nishijima (1926–2009). The
relation was found by Nishijima and
Tadao Nakano, and independently by
Murray Gell-Mann.
Thus it is often
also known as the NNG rule.
Q = I3 + Y
2 .
(47.2)
The known values of Q for the particles in our Universe will constrain
the values of I and Y we assign.
We begin by writing the ﬁelds as components of a large column vector:
Ψ(x) =


νe(x)
eL(x)
eR(x)

.
(47.3)
Next we ask what symmetries this theory has. The ﬁrst is a local U(1)
symmetry. The U(1) transformation causes the ﬁeld to pick up a phase
factor eiβ(x).
To ensure invariance we introduce a gauge ﬁeld Bµ(x)
which transforms as Bµ(x) →Bµ(x)+
1
Y g′ ∂µβ(x), where Y is the charge
of the theory and the parameter g′ tells us how strongly particles will
couple to the hypercharge (just as we write q = Q|e| and |e| tells us
how strongly a photon couples to a single electronic charge).
Rear-
ranging the position of charge Y slightly in the equations and rescaling
for future convenience, we’ll rewrite the transformation as e
i
2 Y β(x) and

47.1
The symmetries of Nature before symmetry breaking
435
Bµ →Bµ + 1
g′ ∂µβ. The former expression tells us that the strength of
the transformation at a point in spacetime depends on how much charge
a ﬁeld carries.5 Note carefully that the U(1) charge Y isn’t the electric
5The reason for doing this here is that
the diﬀerent components of the ﬁeld
are going to carry diﬀerent amounts of
charge. We will call Y a ‘U(1) charge’
because it controls the coupling be-
tween the fermions and the gauge ﬁelds
we need to introduce to guarantee local
U(1) symmetry.
charge; instead, Y is known as the weak hypercharge. We assign the
ﬁelds hypercharges as shown in the table in the margin. Thus under
Field
νe
eL
eR
Y
−1
−1
−2
Table of weak hypercharge Y
U(1) transformations we have


νe
eL
eR

→



e−iβ(x)
2
0
0
0
e−iβ(x)
2
0
0
0
e−iβ(x)





νe
eL
eR

.
(47.4)
To summarize so far: we may turn the internal dial labelled ‘U(1) weak
hypercharge’ without changing the properties of the state of the early
Universe. The dial serves to multiply the ﬁelds by position-dependent
phase factors, whose eﬀects are cancelled by the gauge ﬁeld Bµ(x).
The internal U(1) symmetry is not the only local symmetry possessed
by the early Universe. There is also a local SU(2) symmetry. A SU(2)
transformation results in the ﬁeld components acquiring phases whose
values depend on the details of the conserved charge6 I, known as weak
6See the previous chapter for further
details of how this arises.
isospin, carried by each ﬁeld component. The weak isospin I obeys
the usual rules of spin angular momentum so, for example, the third
component of isospin for a ﬁeld with I = 1/2 has eigenvalues I3 = ±1/2.
We assign isospin quantum numbers to particles as shown in the table in
the margin, with the result that the ﬁelds undergo the transformation:

νe
eL

→e
i
2 τ·α(x)

νe
eL

eR →eR.
(47.5)
For SU(2) transformations the gauge ﬁeld needed to guarantee local
invariance is Wµ(x), which transforms according to τ · Wµ →τ · Wµ +
1
gτ · (∂µα) −τ · (α × Wµ), where g is the coupling to the weak isospin.7
7As in the previous chapter, the Wµ
ﬁeld itself has I = 1.
Note also that
this ﬁeld has Y = 0 units of weak hy-
percharge.
Summarizing again: the dial labelled ‘SU(2) weak isospin’ may be
turned at will in the early Universe without any physical consequence.
This dial mixes up the massless ﬁelds νe and eL and multiplies them by
position dependent phases. Its eﬀects are cancelled by the gauge ﬁeld
Wµ(x). The transformation has no eﬀect on eR.
Field
νe
eL
eR
I
1
2
1
2
0
I3
1
2
−1
2
0
Table of isospin quantum
numbers I3 and I
We have now described a set of local U(1) and SU(2) transformations
for our set of ﬁeld components. Together they make the theory locally
invariant with respect to the enlarged group of transformations SU(2)⊗
U(1). In order to make these symmetries of the Lagrangian we now need
to replace derivatives ∂µ in the Lagrangian with covariant derivatives.
We write
U(1)
:
DµΨ
=
∂µΨ −i
2g′Y Bµ(x)Ψ,
SU(2)
:
DµΨ
=
∂µΨ −igIτ · Wµ(x)Ψ.
(47.6)
Since the original column vector (νe, eL, eR) is distinguished by the ﬁrst
two (left-handed) components having the same Y charge and I charge
while the right-handed components have diﬀerent ones, we will rewrite

436
The Weinberg–––Salam model
the ﬁelds as
Ψ =

L
R

, where
L =

νe
eL

,
R = eR.
(47.7)
This prevents our having to keep writing Is and Y s in the Lagrangian.8
8The logic of this notation is that it
puts the ﬁeld with I3 = 1 in the upper
slot of the L doublet and the one with
I3 = −1 in the lower slot.
In terms of these new ﬁelds, the covariant derivatives are written:
DµL
=
∂µL −i
2gτ · WµL + i
2g′BµL,
(47.8)
DµR
=
∂µR + ig′BµR.
(47.9)
Our resulting, locally symmetric, Lagrangian is given by
L
=
¯Riγµ (∂µ + ig′Bµ) R + ¯Liγµ

∂µ −i
2gτ · Wµ + i
2g′Bµ

L
−1
4G(W )
µν
· G(W )µν −1
4F (B)
µν F (B)µν,
(47.10)
where G(W )
µν
= ∂µWν −∂νWµ + gWµ × Wν and F (B)
µν
= ∂µBν −∂νBµ,
which are the correct contributions from the respective gauge ﬁelds for
SU(2) and U(1) theories.9
9Note that the addition of mass terms
to the Lagrangian in eqn 47.10 would
violate the local symmetry we have set
up. As shown in eqn 47.26, a mass term
links the left- and right-handed parts of
the electron ﬁeld via a term −m(¯eLeR+
¯eReL) but, since the SU(2) transforma-
tion aﬀects the left- and right-handed
parts of the ﬁeld diﬀerently, this term
cannot be admitted.
Example 47.1
Although apparently plucked from the air, the assignment of charges to the ﬁeld com-
ponents may be physically motivated as follows. We want our theory of electroweak
interactions to have both the weak interaction and electromagnetism correctly em-
bedded within it. In terms of left- and right-handed components, we may write the
weak isospin currents as
Jµ = 1
2
¯LγµτL,
(47.11)
and the electromagnetic current as
Jµ
em = Q (¯eLγµeL + ¯eRγµeR) .
(47.12)
Note that, in terms of net charge transfer, Jµ
em and Jµ
3 are neutral currents, whereas
Jµ
1,2 are charged currents. Since Jµ
3 does not involve eR, but Jµ
em does, we need to
add an extra current to the problem. This should ensure that we have a consistent
gauge theory that has electromagnetism correctly embedded within it. The simplest
solution is to write
Jµ
em = Jµ
3 + 1
2Jµ
Y,
(47.13)
where Jµ
Y is the weak hypercurrent and the factor 1/2 is included by convention.
Equation 47.13 then immediately implies the Gell-Mann-Nishijima relation Q = I3 +
Y
2 . If we now expand out the expressions for Jµ
3 and Jµ
Y we obtain
Jµ
3
=
1
2 (¯νeγµνe −¯eLγµeL)
Jµ
Y
=
−¯νeγµνe −¯eLγµeL −2¯eRγµeR,
(47.14)
where we have taken Q = −1 for the electron ﬁeld in the ﬁnal line. The values of
Q, I3 and Y may then be read oﬀfrom the coeﬃcients of the ¯νeγµνe, ¯eLγµeL and
¯eRγµeR terms in the expressions for Jµ
em, Jµ
3 and Jµ
Y, in agreement with those listed
in the tables above.
So far this seems merely an elaborate example of writing down a La-
grangian with some potentially interesting local symmetries. The amaz-
ing things start happening when we add a further ﬁeld into the mix. It
is this ﬁeld which, from behind the scenes, will determine the properties
of our electroweak Universe.

47.2
Introducing the Higgs ﬁeld
437
47.2
Introducing the Higgs ﬁeld
We now introduce a massive, complex scalar ﬁeld called the Higgs ﬁeld
into the Universe. It is the interaction of this ﬁeld with eL and eR which
will make electrons massive particles. The Higgs ﬁeld has four compo-
nents which are conveniently arranged into a two-component vector10 as
10Again the upper slot contains the
ﬁelds with I3 = 1, while the I3 = −1
ﬁelds sit in the lower one.
follows:
φ =
 φ+
φ0

=
1
√
2
 φ3 + iφ4
φ1 + iφ2

.
(47.15)
This arrangement works since we have
φ†φ = (φ+)∗φ+ + (φ0)∗φ0 = 1
2(φ2
1 + φ2
2 + φ2
3 + φ2
4),
(47.16)
giving something that looks like a magnitude.
We need to know how the Higgs ﬁeld transforms under U(1) and SU(2)
transformations. The Higgs ﬁeld is deﬁned to have weak hypercharge
Y = +1 and weak isospin I = 1/2. It therefore transforms according to

φ+
φ0

→
 
ei β
2
0
0
ei β
2
! 
φ+
φ0

,

φ+
φ0

→e
i
2 τ·α

φ+
φ0

.
(47.17)
This choice of charges requires a covariant derivative for the Higgs ﬁeld
of the form
Dµφ = ∂µφ −i
2gτ · Wµφ −i
2g′Bµφ.
(47.18)
The Higgs ﬁeld will give a contribution Lφ to the Lagrangian of
Lφ = (Dµφ)†(Dµφ) + m2
h
2 φ†φ −λ
4 (φ†φ)2.
(47.19)
The positive mass term gives rise to the potential shown in Fig. 47.1,
from which we see that we have set the Higgs ﬁeld up for a fall: it’s
unstable to symmetry breaking. Recall that an ordinary scalar ﬁeld,
with negative mass term, has a minimum in potential at φ(x) = 0 and so,
classically at least, it takes the value zero in the vacuum. However, with
its Mexican hat potential (Fig. 47.1), the ground state of the Higgs ﬁeld
is at (φ)0 ̸= 0, implying that the ground state of a Universe containing
the broken symmetry Higgs ﬁeld, will be permeated by the uniform ﬁeld
(φ)0. As we shall see, it is the fact that electrons are moving through
(and interacting with) this ether that gives them mass.
Re φ
Im φ
U(φ)
Fig. 47.1
The potential of the Higgs
ﬁeld.
Of course, all things related to the Higgs ﬁeld would be irrelevant to
our discussion of electrons and neutrinos if we didn’t have an interaction
between the Higgs ﬁeld and the electron/neutrino ﬁeld. This is given by
LI = −Ge(¯LφR + ¯Rφ†L),
(47.20)

438
The Weinberg–––Salam model
where Ge is the coupling strength.
Putting everything together, the
Lagrangian for the Weinberg–Salam model is given by
L
=
¯LiγµDµL + ¯RiγµDµR + (Dµφ)†(Dµφ)
+m2
h
2 φ†φ −λ
4 (φ†φ)2 −Ge(¯LφR + ¯Rφ†L)
−1
4G(W )
µν
· G(W )µν −1
4F (B)
µν F (B)µν,
(47.21)
where the covariant derivatives are those appropriate for each ﬁeld, as
discussed above. To be clear, the massless ﬁelds described by this La-
grangian are the left- and right-handed electron spinor ﬁelds, the left-
handed neutrino spinor ﬁeld, the massless Wµ gauge ﬁeld and the mass-
less Bµ gauge ﬁeld.
The Higgs ﬁeld is unstable to symmetry breaking. What will hap-
pen? After 10−12 s have elapsed the ground state of the Universe will
break symmetry, with the Higgs ﬁeld choosing a unique state from the
inﬁnity of possible ones in the gutter of the potential in Fig. 47.1. If
the Weinberg–Salam theory describes reality then, as this happens, the
electron should take on a mass, the neutrino should remain massless and
the massless photon of electromagnetism should emerge. This will turn
out to be how things fall and, as a bonus, the existence of two more
massive particles, the W ± and Z0 bosons, will be predicted.
47.3
Symmetry breaking the Higgs ﬁeld
It is now a time t > 10−12 s after the Big Bang, the Universe has cooled
below 1016 K and its ground state breaks the SU(2) ⊗U(1) symmetry
we have been discussing. Just as in Chapters 26 and 46 we’re going to
examine the consequences of this broken symmetry.11 The minimum in
11Since we have the non-abelian sym-
metry SU(2) then this will bear a re-
semblance to the case examined in the
last chapter.
potential of the Higgs ﬁeld described in eqn 47.19 is not at (φ)0 = 0,
but at (φ†φ)0 = v =

m2
h
λ

. We’ll choose to break the symmetry with a
new ground state at
(φ1)2
0 = 2m2
h
λ ,
(φ2)0 = 0,
(φ3)0 = 0,
(φ4)0 = 0.
(47.22)
Writing (φ1)0 =

2m2
h
λ
 1
2 =
√
2v we then have a ground state ﬁeld
(φ)0 =

φ+
φ0

0
=

0
v

.
(47.23)
An important point here is that when we break a non-abelian symme-
try, our choice of vacuum may be invariant under a subset of the original
symmetry transformations, meaning that those symmetries aren’t bro-
ken at all.
The vacuum chosen here has the property that it is still
invariant with respect to the local transformation ˆU = ei( Y
2 +I3τ 3)α(x)
which, using the Gell-Mann–Nishijima relation of eqn 47.2, we see is
equivalent to the U(1) transformation eiQα(x). Invariance with respect

47.4
The origin of electron mass
439
to this transformation gives our broken symmetry Universe the gift of
Maxwell’s electromagnetism.
We could search for excitations from this ground state ﬁeld in a very
general way by allowing every component to vary.12 However, just as in
12So, for example, we could write our
excited state ﬁeld as
φ(x) =
 
1
√
2 [φ3(x) + iφ4(x)]
v +
1
√
2 [φ1(x) + iφ2(x)]
!
.
the last chapter, we are at liberty to perform a diﬀerent gauge transfor-
mation at each point in spacetime, and thus reduce the excited ﬁeld to
the much simpler unitary gauge form
φ =
 
0
v + h(x)
√
2
!
.
(47.24)
We are now ready to examine the consequences of spontaneous sym-
metry breaking for this theory. We will demonstrate that it predicts13
13Note that the masses of both the pro-
ton and the neutron (which contribute
most to the masses of everyday objects)
are dominated by the conﬁnement en-
ergy of quarks inside them. Thus the
popular notion of the Higgs ﬁeld as
some kind of unique giver of mass to ev-
erything in the Universe is rather wide
of the mark.
the existence of (i) massive electrons and massless neutrinos; (ii) mass-
less photons; and (iii) massive W ± and Z0 particles.
47.4
The origin of electron mass
The physics of the Weinberg–Salam model is revealed by picking out the
choice parts of the Lagrangian. Firstly we examine where we expect to
ﬁnd a fermion mass in a Lagrangian theory written in terms of left- and
right-handed ﬁelds. If we start with the Dirac Lagrangian L = ¯ψ(✁p−m)ψ
and write ψ = ψL + ψR, we have
L
=
¯ψL✁pψL + ¯ψL✁pψR + ¯ψR✁pψL + ¯ψR✁pψR
−¯ψLmψL −¯ψLmψR −¯ψRmψL −¯ψRmψR.
(47.25)
In fact a number of these terms cancel.14 We are left with
14To show this we use the fact that the
left- and right-handed parts are projec-
tions of the full ﬁeld ψ, obtained via
ψL =
„ 1 −γ5
2
«
ψ
and
ψR =
„1 + γ5
2
«
ψ.
We commute the operators so that the
projection parts sit together. We ﬁnd
that terms containing the momentum
and mixed right- and left-handed ﬁelds
always contain the combination
(1 −γ5)(1 + γ5) = 1 −(γ5)2
and since (γ5)2 = 1 we conclude that
all mixed terms involving momentum
cancel.
Totally left-handed or right-
handed terms involving the mass terms
also contain this combination, so we
lose those too.
L = ¯ψL✁pψL + ¯ψR✁pψR −m( ¯ψLψR + ¯ψRψL).
(47.26)
This provides us with a clue for where to search for the masses of the
electrons in the Weinberg–Salam Lagrangian: we are looking for a term
containing the combination ( ¯ψLψR + ¯ψRψL), which we expect to be mul-
tiplied by a scalar quantity, which will be the electron mass for which
we’re searching. Just such a term is to be found in the interaction be-
tween the Higgs ﬁeld and the lepton ﬁeld, given by
LI = −Ge
 ¯L φR + ¯R φ†L

.
(47.27)
We will insert our broken symmetry excited state (eqn 47.24) into this
equation and see what happens. The ﬁrst term yields
¯L φR = ¯eL

v + h(x)
√
2

eR.
(47.28)
Notice that the neutrino ﬁeld has dropped out. This is good news since
it implies that it will remain massless, just as we hope for a realistic
theory. The second term gives
¯R φ†L = ¯eR

v + h(x)
√
2

eL.
(47.29)

440
The Weinberg–––Salam model
The neutrino ﬁeld doesn’t feature here either and is destined to be mass-
less. Putting the two halves together we obtain
Lint = −Ge(¯eLveR + ¯eRveL) −Ge

¯eL
h(x)
√
2 eR + ¯eR
h(x)
√
2 eL

.
(47.30)
The ﬁrst term is in the predicted form of a scalar mass multiplied by
¯ψLψR + ¯ψRψL. It predicts the electron mass to be me = Gev.
We conclude that, as claimed, the broken symmetry Higgs ﬁeld, which
takes the value
√
2v throughout the vacuum, has provided a source of
quantum mechanical treacle that causes the electrons to acquire mass.
The nature of the interaction between the Higgs ﬁeld and the left- and
right-handed electrons is represented in Fig. 47.2, which depicts the in-
teraction in eqn 47.27.
v
v
v
v
eL
eR
eL
eR
eL
Fig. 47.2 The interaction of the left-
and right-handed electron ﬁelds and the
Higgs ﬁeld from eqn 47.27.
47.5
The photon and the gauge bosons
Next we need to show that the theory predicts a massless photon. We’ll
do this by looking at the place in the Lagrangian where the gauge ﬁelds
interact with the Higgs ﬁeld. Minimal coupling tells us that this is the
term involving the covariant derivative of the Higgs ﬁeld: (Dµφ)†(Dµφ).
Example 47.2
Inserting the broken symmetry version of the Higgs ﬁeld φ, the covariant derivative
Dµφ becomes
 
0
1
√
2 ∂µh(x)
!
−
»ig
2
„
W 3
µ
W 1
µ −iW 2
µ
W 1
µ + iW 2
µ
−W 3
µ
«
+ ig′
2 Bµ
–  
0
v + h(x)
√
2
!
.
(47.31)
Multiplying out, we obtain
Dµφ = −i
2
0
@
gv(W 1
µ −iW 2
µ) + gh(x)
√
2 (W 1
µ −iW 2
µ)
i
√
2∂µh(x) + v(−gW 3
µ + g′Bµ) + h(x)
√
2 (−gW 3
µ + g′Bµ)
1
A .
(47.32)
Finally we multiply this by its adjoint to form the term in the Lagrangian
(Dµφ)†(Dµφ)
=
1
2[∂µh(x)]2 + g2v2
4
(W 1
µ)2 + g2v2
4
(W 2
µ)2
+ v2
4
ˆ
(gW 3
µ −g′Bµ)2˜
+ (higher order terms). (47.33)
Noting, as usual, that a boson mass always enters the Lagrangian in
the form 1
2(mass)2 × (ﬁeld)2, we see that in breaking the symmetry of
the Higgs ﬁeld we have developed massive W 1
µ and W 2
µ vector particles,
with masses M 2
W = g2v2/2. The linear combination15 of gauge ﬁelds
15Recall from Chapter 13 that such a
linear combination of ﬁelds gives rise to
a perfectly valid description of particles
for a theory.
(gW 3
µ −g′Bµ) has also grown massive, but this component has a mass
that depends on the relative sizes of the coupling constants g and g′.
Notice that three Goldstone modes of the Higgs ﬁeld have been con-
sumed with the result that three gauge ﬁelds have grown massive. The

47.5
The photon and the gauge bosons
441
excitations of these three ﬁelds will become the W +, the W −and the Z0
vector bosons in the next step. We therefore have three massive vector
particles. Where’s the massless photon? To ﬁnd it, we note the absence
of the orthogonal linear combination (g′W 3
µ + gBµ) and propose that
this combination of ﬁelds is (proportional to) Aµ(x), the photon ﬁeld of
electromagnetism.
g
g′
θW
Fig. 47.3
Deﬁnition of the Weinberg
angle θW in terms of g′ and g.
Since our particles depend, in a non-trivial manner, on the ratio of
g and g′, we may simplify things by drawing the right-angled triangle
shown in Fig. 47.3, with the coupling constants g and g′ as the lengths
of the orthogonal sides and deﬁne the Weinberg angle tan θW = g′/g.
We then deﬁne two new ﬁelds Zµ and Aµ by16
16The ﬁelds may be written equiva-
lently as
Zµ
=
(gW 3
µ −g′Bµ)
(g2 + g′2)
1
2
,
Aµ
=
(g′W 3
µ + gBµ)
(g2 + g′2)
1
2
.
(47.34)

Zµ
Aµ

=

cos θW
−sin θW
sin θW
cos θW,
  W 3
µ
Bµ

,
(47.35)
where, as suggested by the notation, Aµ is to be interpreted as our old
friend, the electromagnetic ﬁeld. In terms of the new ﬁelds, our original
equation becomes
(Dµφ)†(Dµφ)
=
1
2[∂µh(x)]2 + g2v2
4
(W 1
µ)2 + g2v2
4
(W 2
µ)2
+
g2v2
4 cos2 θW
Z2
µ + (higher order terms),(47.36)
and from this equation we can read oﬀ17 that the mass of the Z0 particle
17The mass of the W boson is given by
M2
W = g2v2/2. The term premultiply-
ing Z2
µ in eqn 47.36 is
g2v2
4 cos2 θW = 1
2 M2
Z
and hence MZ = MW/ cos θW.
is MW/ cos θW.
The fact that the Wµ and Zµ ﬁelds have taken on a mass means that
their propagators will have the form
Gµν
p2−M 2+iǫ.
Recall from our dis-
cussion of the Yukawa force in Chapter 17 that this implies a potential
varying as U(r) ∝e−M|r|. We conclude that, compared to the elec-
tromagnetic interaction which is mediated by the massless photon, the
weak interaction is short-ranged, falling oﬀover a distance 1/M.
We now have the ingredients for the non-interacting Lagrangian of
the broken symmetry Universe (see margin on page 443). However, the
only way to observe the predictions of the theory is via the interactions
that the particles enter into with each other. Our ﬁnal task is therefore
to substitute our results back to see what the theory predicts for these
interactions.
Wµ
W †µ
Zµ
Zµ
Zµ
Aµ
νe
¯eL
eL
¯νe
νe
¯νe
eL
¯eL
eR
¯eR
e
¯e
Fig. 47.4 The electroweak interaction
vertices predicted by the Weinberg–
Salam theory.
Example 47.3
As in the case of QED, minimal coupling allows us to see the interactions by consid-
ering the covariant derivatives. Starting with
i ¯Rγµ(∂µ + ig′Bµ)R + i¯Lγµ
„
∂µ −i
2gτ · Wµ + i
2 g′Bµ
«
L,
(47.37)
and expanding out and using our expressions for Aµ, Zµ and Weinberg’s trigonom-
etry, we may eliminate Bµ, W 3
µ and g′ and we obtain
i¯eγµ∂µe + i¯νeγµ∂µνe −g sin θW¯eγµeAµ
+
g
cos θW
(sin2 θW¯eRγµeR −1
2 cos 2θW¯eLγµeL + 1
2 ¯νeγµνe)Zµ
+ g
√
2
h
¯νeγµeLW †
µ + ¯eLγµνeWµ
i
,
(47.38)

442
The Weinberg–––Salam model
where e = eL + eR, likewise for ν and we deﬁne the ﬁeld Wµ = (W 1
µ + iW 2
µ)/
√
2. We
expect the quanta of the Wµ ﬁeld to be two species of oppositely charged, massive
vector particles: the W + (the quanta created by the ˆ
Wµ(x)-ﬁeld, with I3 = 1) and
the W −(annihilated by ˆ
Wµ(x), with I3 = −1). The Gell-Mann–Nishijima formula
(eqn 47.2) tells us that W + has Q = 1 units of electric charge, while the W −has
Q = −1 units. In contrast the Zµ(x) ﬁeld is uncharged and we expect a single species
of massive quantum Z0.
In eqn 47.38 we have an expression for some of the interactions18 in the
18For a more complete description of
the 18 interactions described by the
theory see Mandl and Shaw, Chap-
ter 19.
electroweak theory in terms of the coupling g and the Weinberg angle θW.
Some important points to note from our result is that Aµ only couples to
electrons and not to neutrinos, just as experiment shows the photon does.
The vector ﬁeld Wµ only couples left-handed electrons to neutrinos and
doesn’t touch right-handed ones. The vector ﬁeld Zµ couples to left-
and right-handed electrons with diﬀerent strengths.
Notice also that
that parity operation P−1HIP, which exchanges left- and right-handed
ﬁelds, is not a symmetry of those parts of the interaction involving the
Wµ(x) and Zµ(x) ﬁelds. This is the sense in which, famously, the weak
interaction is said to ‘violate parity’.
The vertices for these interaction are shown in Fig. 47.4. The most
startling of these involve the Wµ ﬁeld and show that the emission or ab-
sorption of a W ± particle can result in the transformation of an electron
into a neutrino (and vice versa). Some examples of particle processes
allowed by the weak interaction are shown in Fig. 47.5.
Note what the theory predicts for the electric charge. In QED the
charge is the coupling constant in front of the fermion–photon interaction
term: −q ¯ψγµψAµ. Taking q = |e| for a single electronic charge we read
oﬀfrom eqn 47.38 that the electric charge is given by
|e| = g sin θW.
(47.39)
W −
W +
νe
µ−
νµ
e−
u
¯d
µ+
νµ
Fig. 47.5 Some example electroweak
processes. (a) µ−+νe →νµ + e−; (b)
positive pion decay involving quarks u
and ¯d decaying into µ+ + νµ.
The Weinberg–Salam electroweak theory and its veriﬁcation are two
of the greatest achievements of twentieth century science. The Z0 and
W ± particles were detected with masses MZ = 91.19 GeV and MW =
80.40 GeV and correspond to a Weinberg angle (via cos θW = MW/MZ)
of θW ∼28◦.19 The ﬁnal prediction of the theory is the existence of a
19Historically, a value for the Weinberg
angle had been estimated from ear-
lier measurements of lepton scattering
cross-sections and provided a predic-
tion for the masses of the W ± and Z0
particles which were subsequently dis-
covered at the predicted energies. See
Gary Taubes’ highly recommended No-
bel Dreams for the frightening history.
scalar ﬁeld excitation in the Higgs ﬁeld, known as the Higgs boson.20
20Often
incorrectly
pronounced
‘Boatswain’ by the UK media, possibly
as some kind of nautical throwback.
At the time of writing, experiments at the Large Hadron Collider have
found a particle, thought to be the Higgs, with a mass ≈125 GeV.
Thus in summary our current picture is of a symmetry-broken Higgs
ﬁeld that permeates the Universe. Other ﬁelds are set to zero in the
vacuum and ﬂuctuate around that, giving rise to particle–antiparticle
excitations that wink in and out of existence; the Higgs ﬁeld is nonzero
by default. With no Higgs ﬁeld, the electron and electron neutrino would
be identical particles, and the W and Z particles, and in fact all standard
model fermions, would be massless. That indeed was the situation in
the very early Universe before the Higgs ﬁeld became symmetry broken.
But immediately after the electroweak phase transition (that occurred
perhaps 10−12 s after the Big Bang) the present broken-symmetry state

Exercises
443
emerged and the masses of these particles became simply a function
of how strongly they couple to the Higgs ﬁeld. And couple they will,
because the Higgs ﬁeld is both nonzero and everywhere.
Chapter summary
• In the period t ≲10−12s after the Big Bang the leptons of the early
Universe enjoy an internal SU(2) ⊗U(1) local symmetry.
• The Weinberg–Salam electroweak theory describes the breaking of
the symmetry of the Higgs ﬁeld with the result that a nonzero
Higgs ﬁeld permeates the vacuum, providing electrons with mass.
• This also results in the emergence of the photon and three-vector
particles W +, W −and Z0. The excitations of the electromagnetic
ﬁeld Aµ do not acquire a mass, whereas the W ± and Z0 particles
become massive.
Non-interacting electroweak La-
grangian
L0
=
¯e(i✁∂−me)e + ¯νe(i✁∂)νe
−1
4FµνF µν
−1
2W †
µνW µν + M2
WW †
µW µ
−1
4ZµνZµν + 1
2M2
ZZµZµ
+ 1
2(∂µh)2 −1
2 m2
hh2
where Wµν = ∂µWµ −∂νWµ and sim-
ilarly for Zµν.
Exercises
(47.1) Show that the broken symmetry ground state
vacuum expectation value (φ)0 =
„ 0
v
«
is in-
variant with respect to the transformation ˆU =
ei( Y
2 +I3τ3)α(x).
(47.2) Verify eqn 47.26.
(47.3) By considering the covariant derivative of the Higgs
ﬁeld, verify eqn 47.33.
(47.4) Verify eqn 47.38.
(47.5) Fermi’s theory of the weak interaction.
(a) Read oﬀthe interaction vertices for weak in-
teractions between electrons and neutrinos from
eqn 47.38.
(b) Draw a Feynman diagram for negative muon
decay µ−→e−+ ¯νe + νµ and show that, at low
momentum, the process is described by an interac-
tion Hamiltonian
ˆHI = g2
2 ¯νµγαµL gαβ
M 2
W
¯eLγβνe.
(47.40)
(c) Deﬁning the current for the lepton species l as
Jα
l
= ¯lγα(1 −γ5)νl, show that the interaction for
the muon decay can be recast as
ˆHI = G
√
2
Jα†
µ Jeα,
(47.41)
where G =
g2
4
√
2M2
W . This is known as the Fermi
current–current interaction.
(d) For muon decay G ≈10−5m−2
p , where mp is
the proton mass.
Comment on the ‘weakness’ of
the weak interaction.

48
Majorana fermions
48.1 The Majorana solution
444
48.2 Field operators
446
48.3 Majorana mass and charge
447
Chapter summary
450
Exercises
450
Tragically I was an only twin
Peter Cook (1937–1995), title of an unrealized project.
Can a fermion particle be its own antiparticle? That is to say, if we
take the charge conjugate of the creation operator of a Fermi particle
c†
p, can it return the same operator C−1ˆc†
pC = ˆc†
p without creating an
inconsistency in the theory? We know that particle excitations of the
real scalar ﬁeld are identical to antiparticle excitations of the scalar ﬁeld.
We also know that the photon excitations in the electromagnetic ﬁeld
are identical to antiphotons. Scalar particles (spin-0), photons (spin-1)
and gravitons (spin-2) can be described by real ﬁelds: ˆφ = ˆφ†. Since
ˆφ† creates a particle and ˆφ an antiparticle, we expect these particles
to be identical to their antiparticles. The Dirac theory of the electron
(Chapter 36) seems to require complex ﬁelds (though see below), and so
it might seem at ﬁrst glance as if fermion ﬁelds necessarily involve dis-
tinct particle and antiparticle parts. This would explain why electrons
and positrons (anti-electrons) are distinct particles, and also why even
the electrically-neutral neutron is not identical to its antiparticle, the
antineutron. However, this is not the whole story, and in this chapter
we examine the steps required to formulate a theory containing fermions
whose particles and antiparticles are identical or, if you prefer, symmet-
ric.
MAJORANA
FERMION
(particle)
(antiparticle)
=
MAJORANA
FERMION
Fig. 48.1 A majorana fermion is its
own antiparticle.
48.1
The Majorana solution
Etorre Majorana came up with a theory in which a fermion particle was
Etorre Majorana (1906–[1938+x]) had
a short, but brilliant, career in theoret-
ical physics which was cut short when
he disappeared whilst travelling by pas-
senger ship from Palermo to Naples. It
has been variously suggested that he
committed suicide, was murdered, or
escaped to Argentina.
its own antiparticle (see Fig. 48.1). Such particles are called Majorana
fermions and are unchanged by the act of charge conjugation.
Example 48.1
For charged particles we know that the charge conjugation operator swaps particles
for antiparticles, so for a complex scalar ﬁeld we have, for example: C−1ˆa†
pC = ˆb†
p.
Let’s start by taking a naive view and examining a mode expansion of a classical
scalar ﬁeld (that is, the expansion of a function; not an operator):
φ(x) =
Z
d3p
(2π)
3
2
1
(2Ep)
1
2
“
ape−ip·x + a∗
peip·x”
.
(48.1)

48.1
The Majorana solution
445
Notice that the second term is the complex conjugate of the ﬁrst and that φ(x) is
real. Compare this with the case of a (classical) complex scalar ﬁeld:
ψ(x) =
Z
d3p
(2π)
3
2
1
(2Ep)
1
2
“
ape−ip·x + b∗
peip·x”
.
(48.2)
Here the complex conjugate of the ﬁrst term yields something which is not identical
to the second term, i.e. the particle is diﬀerent from the antiparticle.
Speaking very roughly, charge conjugation involves swapping each ﬁeld for its
complex conjugate and so a real ﬁeld like φ(x) will describe particles that are their
own antiparticles. This will not be the case for the complex ﬁeld ψ(x).
Motivated by the previous example, we will search for a real (rather
than complex) solution to the Dirac equation. Such a solution follows
if the Dirac equation itself is real. Whether the Dirac equation (i✁∂−
m)Ψ = 0 is real or complex depends on the choice of the γ matrices.
Recall that there are many possible γs out there, constrained only by
the requirement that {γµ, γν} = 2gµν. If we can ﬁnd a set of γs which
are purely imaginary, then the Dirac equation itself will be real and
consequently so will its solutions.
Majorana found such a set of purely imaginary γ matrices, given in
eqn 48.3 in the margin. Using the Dirac equation given by these matrices
Majorana’s γ matrices:
˜γ0
=
„
0
σ2
σ2
0
«
˜γ1
=
„ iσ1
0
0
iσ1
«
˜γ2
=
„
0
σ2
−σ2
0
«
˜γ3
=
„ iσ3
0
0
iσ3
«
. (48.3)
we will ﬁnd solutions ˜ν(x) which will have the property that ˜ν(x) =
˜ν∗(x), which is known as the Majorana condition and reﬂects the
fact that the solutions are identical to their complex, or more precisely,
to their charge conjugates.
Having demonstrated the possibility of Majorana fermions existing as
viable quantum mechanical entities, we would now like to make contact
with our previous approach and describe Majorana’s solution in the
chiral representation, which involves stacking up pairs of two-component
Weyl spinors to make four-component Dirac, or in this case Majorana,
spinors. We therefore need to consider how charge conjugation works for
Weyl spinors. (Note that we’re treating our spinors as c-number wave
functions for the moment; ﬁelds will follow shortly.)1
1Note also that, throughout the rest
of this chapter, we reserve the sym-
bol ψ for two-component Weyl spinors
only, which may be left-handed (ψL) or
right-handed (ψR), while the symbols
Ψ and ν denote four-component Dirac
and Majorana spinors respectively.
Example 48.2
There is a recipe for obtaining the charge conjugate ΨC(x) of a Dirac spinor Ψ(x) in
chiral representation. It is
ΨC(x) = C−1Ψ(x)C = C0Ψ∗(x),
(48.4)
with C0 = −iγ2. As an example, we could start with a Dirac spinor Ψ =
„ ψL
0
«
containing only a left-handed Weyl spinor ψL and take the charge conjugate as fol-
lows:
ΨC
=
−iγ2
„ ψL
0
«∗
=
„
0
−iσ2
iσ2
0
« „ ψ∗
L
0
«
=
„
0
iσ2ψ∗
L
«
. (48.5)
This gives a component in the lower slot involving the original left-handed Weyl
spinor.2 You may verify that conjugating the charge twice returns the original spinor.
2Note that charge conjugation of the
right-handed spinor gives
„
0
ψR
«
→
„
−iσ2ψ∗
R
0
«
.

446
Majorana fermions
Here’s the solution: in the chiral basis a Majorana spinor may be built
out of a left-handed Weyl spinor and its charge conjugate as follows:
ν(x) =

ψL(x)
0

+

0
iσ2ψ∗
L(x)

=

ψL(x)
iσ2ψ∗
L(x)

, (48.6)
that is ν(x) =

ψL(x)
ψL,C(x)

, where we write the charge conjugate of a
Weyl spinor as ψL,C = iσ2ψ∗
L. This solution ν(x) obeys the all-important
property that νC(x) = ν(x), which is a more general expression of the
Majorana condition above.
Notice that a Majorana particle may be
built starting with only a left-handed Weyl spinor, putting its conjugate
part into the slot where the right-handed part usually lives.3 This is
3We could also have started with a
right-handed spinor and then built a
Majorana fermion ν′ =
„
−iσ2ψ∗
R
ψR
«
.
The point is that we only need to
start with either a left-handed spinor
or right-handed spinor.
diﬀerent to the ordinary Dirac fermion we met in Chapter 36, which has
independent left- and right-handed parts.
48.2
Field operators
So far we’ve been working with wave functions.
To make the the-
ory respectable we should write things in terms of ﬁeld operators.
Like the wave functions, these ﬁelds must obey the Majorana con-
dition, which expressed in terms of charge conjugation operators is
ˆν(x) = ˆνC(x) = C0ˆν∗(x).
A slightly jarring abuse of notation here
is the complex conjugate of the ﬁeld operator, since the complex con-
jugate of a creation or annihilation operator is not, strictly speaking,
deﬁned. In taking the complex conjugate we are seeking to avoid the
matrix transpose that goes along with Hermitian conjugation. There-
fore a fussier, but more correct, way of writing the conjugation operation
would be ˆν†T. However, we’ll stick with the complex conjugate notation
where one should note that us∗(p) =
 us∗
L (p)
us∗
R (p)

and we freely abuse
the notation with the understanding that here ˆa∗
p ≡ˆa†
p.
Example 48.3
For a Dirac particle, recall that the ﬁeld annihilation operator is given by
ˆΨ(x) =
Z
d3p
(2π)
3
2
1
(2Ep)
1
2
X
s
“
us(p)ˆaspe−ip·x + vs(p)ˆb†
speip·x”
,
(48.7)
whose charge conjugate is C−1 ˆΨ(x)C = C0 ˆΨ∗(x)
=
Z
d3p
(2π)
3
2
1
(2Ep)
1
2
X
s
“
−iγ2us∗(p)ˆa†
speip·x −iγ2vs∗(p)ˆbspe−ip·x”
.
(48.8)
Moreover, using the explicit forms of us(p) and vs(p) it may be shown4 that
4Consider,
for
example,
the
charge
conjugation
of
the
posi-
tive
helicity
antiparticle
spinor
from
Chapter
36:
−iγ2vs∗(p)
=
„
0
−iσ2
iσ2
0
«
0
B
B
@
0
p
E + |p|
0
−
p
E −|p|
1
C
C
A
=
0
B
B
@
p
E −|p|
0
p
E + |p|
0
1
C
C
A = us(p),
i.e. a positive helicity particle spinor,
as claimed.
−iγ2us∗(p) = vs(p) and −iγ2vs∗(p) = us(p) leading to a charge conjugate ﬁeld
C−1 ˆΨ(x)C =
Z
d3p
(2π)
3
2
1
(2Ep)
1
2
X
s
“
vs(p)ˆa†
speip·x + us(p)ˆbspe−ip·x”
.
(48.9)
For what follows, note that this can be made the same as the original Dirac ﬁeld if we
were to make the replacements ˆa†
sp →ˆb†
sp and ˆbsp →ˆasp, from which we conclude
that the prescription indeed enacts charge conjugation on the Dirac ﬁeld in that it
returns a Dirac ﬁeld with particle operators exchanged for antiparticle operators.

48.3
Majorana mass and charge
447
Finally we will write down a Majorana ﬁeld. We start by noting that
the general form of a quantum ﬁeld is
(Quantum Field)
=
(Particle part) + (Antiparticle part)
=
(Particle part) + C−1(Particle part)C.
If particles and antiparticles are identical (which is what we’re after) then
C−1ˆa†
spC = ˆa†
sp and so C−1us(p)ˆa†
spC = −iγ2us∗(p)ˆa†
sp. To construct the
Majorana ﬁeld we therefore replace the antiparticle part of the Dirac ﬁeld
vs(p)ˆb†
sp with −iγ2us∗(p)ˆa†
sp. The result is the Majorana ﬁeld
ˆν(x) =
Z
d3p
(2π)
3
2
1
(2Ep)
1
2
X
s
 us(p)ˆaspe−ip·x −iγ2us∗(p)ˆa†
speip·x
.
(48.10)
As may be checked, the Majorana ﬁeld does indeed enjoy the Majorana
condition:
C−1ˆν(x)C =
Z
d3p
(2π)
3
2
1
(2Ep)
1
2
X
s
 −iγ2us∗(p)ˆa†
speip·x + us(p)ˆaspe−ip·x
= ˆν(x).
(48.11)
Example 48.4
If we start with a left-handed (momentum space) Weyl spinor us
L(p) the Majorana
ﬁeld takes the form
ˆν(x) =
Z
d3p
(2π)
3
2
1
(2Ep)
1
2
X
s
»„ us
L(p)
0
«
ˆaspe−ip·x +
„
0
iσ2us∗
L (p)
«
ˆa†
speip·x
–
.
(48.12)
As in the case of wave functions, we see that Majorana ﬁelds may be built out of
Weyl spinors of only one handedness.
Although the argument of this chapter has been very formal, the point is
a simple one: it is indeed quite possible to build a Majorana ﬁeld whose
excitations are Majorana fermions. We will now discuss the physics of
these particles.
48.3
Majorana mass and charge
First we consider the mass of the particle excitations of the Majorana
ﬁeld. Recall that Weyl spinors are necessarily massless. Massive Dirac
spinors must contain independent left- and right-handed parts and the
particles may be thought of as oscillating between the two. The mass
term in the Dirac Lagrangian may be written in terms of Weyl spinors
as5
5See the previous chapter for an expla-
nation of this.
mD(¯ΨLΨR + ¯ΨRΨL),
(48.13)

448
Majorana fermions
where we write ΨL =
 ψL
0

and ΨR =

0
ψR

. Vitally, eqn 48.13 is
a Lorentz invariant quantity.6
6Notice
that
we
don’t
have
terms
¯ΨLΨL
and
¯ΨRΨR
which
are
not
Lorentz invariant and therefore are for-
bidden.
As Majorana solutions may be written in terms of Weyl spinors of
a single chirality we might question whether it is possible to identify a
massive Majorana ﬁeld. This worry looks to be valid since, as shown
in the exercises, a mass term in the Lagrangian of the form ¯ν(x)ν(x)
vanishes if we treat the ﬁelds in the Lagrangian as c-numbers, as we do
for the Dirac case. However, it turns out that we may deﬁne massive
Majorana ﬁelds, as long as they anticommute.
In that case we may
have massive Majorana ﬁelds built out of left-handed Weyl spinors only,
with mass mL, or Majorana ﬁelds built from right-handed Weyl ﬁelds
only, with mass mR. Turning to the Lagrangian, we may write the most
general, Lorentz invariant mass term for Majorana ﬁelds as7
7Note that the N objects in this equa-
tion are built from four-component Ma-
jorana spinors ν, so themselves have
eight components.
−1
2
  ¯N L,CMN L + ¯N LMN L,C

,
(48.14)
where
N L =

νL
νR,C

,
N L,C =
 νL,C
νR

,
M =
 mL
mD
mD
mR

,
(48.15)
and where νL =

ψL
0

, νL,C =

0
iσ2ψ∗
L

, νR =

0
ψR

and νR,C =

−iσ2ψ∗
R
0

. Here we have three types of mass: the Dirac mass mD
along with the right- and left-handed Majorana masses mR and mL.
Example 48.5
Working with Majorana ﬁelds ν(x) built from left-handed Weyl spinors, we set mD =
mR = 0 and mL ̸= 0. We may then write a Majorana Lagrangian as
L = 1
2 ¯νi✁∂ν −1
2(¯νL,CmLνL + ¯νLmLνL,C),
(48.16)
where ν = νL + νL,C.
Varying this Lagrangian with respect to ψ†
L, leads to the
Majorana equation for the ﬁeld ψL, given by
i¯σµ∂µψL −mLiσ2ψ∗
L = 0,
(48.17)
which may be recast in the form of a Dirac equation
(i✁∂−mL)ν(x) = 0.
(48.18)
To complete this cavalcade of formalism, we brieﬂy turn to the electro-
magnetic charge of the Majorana ﬁeld: it has none. This is, of course,
necessary for the particle excitations of this ﬁeld to be identical to the
antiparticles. It can also be seen from the ﬁeld equations by noting that
if ν = νL+νL,C, then if we make the transformation νL →νLeiα, we must
have νL,C →νL,Ce−iα (because νL and νL,C are related by complex con-
jugation). It is therefore impossible to deﬁne a U(1) transformation for

48.3
Majorana mass and charge
449
Majorana spinors which simultaneously provides both upper and lower
slots with the same phase factor eiα. The Majorana equation cannot,
therefore, be made invariant under local U(1) transformations. Put the
other way: a particle carrying the conserved U(1) charge of electromag-
netism cannot be a Majorana particle.
Having formulated this theory we now ask whether it’s of any use.
Indeed, for many years it seemed to be an interesting solution in need of
a problem. However, in recent years, several such problems have arisen.
• The vast number of emergent quasiparticles in condensed matter
physics present us with an ideal playground for searching for ex-
otic excitations such as Majorana fermions. In a semiconductor or
a metal, electrons and holes look diﬀerent because they are oppo-
sitely charged, and so it does not seem possible that the particles
(electrons) and antiparticles (holes) can be symmetrically related.
A superconductor, on the other hand, blurs the distinction between
electrons and holes and so seems like a possible environment for
realizing Majorana fermions. One of the great things about su-
perconductors is that they screen electric and magnetic ﬁelds, and
so charge is not a good observable. The bogolon excitations of a
superconductor superpose electrons and holes and, under rather
special circumstances, it is possible that the quasiparticles of a
superconducting system are Majorana fermions. An example of
such a circumstance involves a superconductor in the presence of
vortices, which changes the equations of motion of the electrons
and can lead to the trapping of electron–hole pairs which can be
described as Majorana fermions.
• At ﬁrst sight neutrinos seem to be well described as solutions to
Weyl’s equation. All neutrinos are left-handed massless particles
with negative helicity whereas all antineutrinos are left-handed
massless particles with positive helicity. However, the discovery
that neutrinos emitted from the Sun with one ﬂavour may be de-
tected with a diﬀerent ﬂavour suggests that these particles actually
possess a nonzero, but small, mass. Whether this is a Dirac mass
is an open question and there exists the possibility that neutrinos
are actually Majorana particles with Majorana mass.
• Consider the possibility that for every species of boson in the Uni-
verse there exists a corresponding species of fermion (and vice
versa) with the same mass. This would require the Lagrangian
describing our Universe to exhibit a symmetry known as super-
symmetry. In a supersymmetric Universe we should expect the
existence of the ‘selectron’: a spin-0 particle with the mass of an
electron; and the ‘photino’: a spin-1/2 massless particle. If the
photino mirrors the properties of the photon then it must be its
own antiparticle.
This implies that the photino is a Majorana
fermion, as will be the ‘Higgsino’ and various types of ‘gaugino’.
(Supersymmetry is a subject replete with wonderful terms, and
happily the supersymmetric partner of the W boson is called a

450
Majorana fermions
‘wino’.) Despite much eﬀort, any direct evidence for supersym-
metric particles has so far proved to be elusive.
Chapter summary
• A Majorana fermion is its own antiparticle and is a solution to a
version of the Dirac equation in which the γ-matrices are purely
imaginary.
• We have shown how to build Majorana ﬁelds from Weyl spinors of
a single handedness and we have written down a Lagrangian that
leads to their equation of motion.
Exercises
(48.1) Show that two operations of the charge conjuga-
tion operator used in eqn 48.5 return the original
particle.
(48.2) Show that −iγ2us∗(p) = vs(p) for a negative helic-
ity particle spinor.
(48.3) (a) Show that, for a Majorana ﬁeld ν built from
left-handed Weyl spinors:
¯νν = −iψT
L σ2ψL + h.c.,
(48.19)
where h.c. denotes the Hermitian conjugate.
Ex-
plain why this must vanish if the ﬁelds are repre-
sented by c-numbers.
(b) Show that ¯νν won’t vanish if the ﬁelds are rep-
resented by anticommuting Grassmann numbers.
(48.4) Verify eqns 48.16 and 48.17.
(48.5) Verify that eqn 48.17 may be recast in the form of
a Dirac equation as claimed.

49
Magnetic monopoles
49.1 Dirac’s
monopole
and
the
Dirac string
451
49.2 The
’t
Hooft–––Polyakov
monopole
453
Chapter summary
456
Exercises
456
One would be surprised if nature had made no use of it.
Paul Dirac (1902–1984), on magnetic monopoles.
Do magnetic monopoles exist? We are taught at our Mother’s knee
that Maxwell’s equation ∇· B = 0 insists that they do not. However
it is more the case that they have never been observed rather than
that their existence is a physical impossibility. In fact there seems to
be nothing preventing the existence of the magnetic monopole and in
this chapter we will investigate the properties of these objects. We will
present descriptions of two rather diﬀerent sorts of monopole: those
of Paul Dirac and those of Gerard ’t Hooft and Alexander Polyakov.
The existence of Dirac’s monopoles are not mandated by any theory.
However, we’ll see that if we really do live in a Universe that results
from the sort of non-abelian gauge theories with symmetry breaking
discussed in previous chapters, then the existence of ’t Hooft–Polyakov
magnetic monopoles is almost inevitable.
49.1
Dirac’s monopole and the Dirac
string
The discovery of a certain kind of magnetic monopole with a magnetic
charge g wouldn’t be a disaster for Maxwell’s electromagnetism. We
could include them in his equations, which would become rather more
symmetrical:
Remember
we
are
using
Heaviside
units.
∇· E = ρe,
∇· B = ρm,
∇× E = −Jm −∂B
∂t ,
∇× B = Je + ∂E
∂t .
The extra terms involve ρm, the magnetic charge density, and Jm, the
magnetic current.1 We would also need to adjust the Lorentz force law
1In
conventional
electromagnetism
ρm = 0 and Jm = 0.
which would become
F = q(E + v × B) + g (B −v × E) .
(49.1)
This variety of magnetic monopole was ﬁrst considered by Paul Dirac
and is therefore known as a Dirac monopole.
As a ﬁrst step in our investigation of the properties of these objects,
let’s recap the physics of electric monopoles, which certainly do exist!

452
Magnetic monopoles
They are the charges of classical electromagnetism (see Fig. 49.1). For
an electrical charge q at the origin, Maxwell’s equation yields
∇· E = q δ(3)(r),
(49.2)
whose solution, due to Monsieur Coulomb, takes the form
E =
qr
4π|r|3 = −∇

q
4π|r|

.
(49.3)
The electric ﬂux ΦE =
R
E · dS through a spherical surface surrounding
the electric monopole is then just q.
Fig. 49.1 Monopoles (electric or mag-
netic) are sources or sinks of ﬁeld, be-
cause ∇· E = qδ(3)(r) and ∇· B =
gδ(3)(r).
Fig. 49.2 You can think of a monopole
as originating from an extremely long
bar magnet.
AN
AS
Dirac string
monopole
θ r
Fig.
49.3 The vector ﬁeld from a
monopole can be described everywhere
except along a line of singularities (the
Dirac string). AN is deﬁned for θ < π.
AS is deﬁned for θ > 0.
If analogous magnetic monopoles exist then their behaviour should be
governed by similar equations. Instead of the Maxwell equation ∇·B =
0, we should have for a magnetic monopole of charge g at the origin
∇· B = g δ(3)(r),
(49.4)
and a radial ﬁeld equation (that is, the analogue of Coulomb’s law):
B =
gr
4π|r|3 .
(49.5)
The magnetic ﬂux through a sphere surrounding the magnetic monopole
is given by ΦM = g.
Example 49.1
The problem with the Dirac monopole comes when you try and write down the
magnetic vector potential A. There is no single function, without singularities, that
does the job everywhere; actually, this shouldn’t be a surprise. If you could ﬁnd such
a function, then the ﬂux ΦM out of a closed region S (of volume V ) would be
ΦM =
Z
S
∇× A
| {z }
B
· dS =
Z
∇· (∇× A) dV ≡0,
(49.6)
whereas this should come out as ΦM = g. One way of understanding this physically
is to think of the monopole originating from an inﬁnitely long and very thin solenoid,
or permanent magnet, which has one end at the origin and the other end very far
away (somewhere out at inﬁnity).
This looks like a monopole at the origin (see
Fig. 49.2), and the physical Universe can be modelled everywhere, except at a line of
singularities where the solenoid or permanent magnet lies. To illustrate this problem
(see Fig. 49.3), consider the function AN given by
AN =
g
4πr
(1 −cos θ)
sin θ
ˆeφ =
g
4πr tan θ
2 ˆeφ
(θ < π).
(49.7)
This is well deﬁned everywhere except at θ = π and taking the curl of it quickly2
2The expression for the curl of a vector
function only containing an azimuthal
(ˆeφ) component in spherical polars is
∇× A =
1
r sin θ
∂
∂θ
ˆ
Aφ sin θ
˜ ˆer
−1
r
∂
∂r
ˆ
rAφ
˜ ˆeθ.
brings back eqn 49.5. The function blows up at θ = π for all radii r, and so this
magnetic vector potential is singular along the whole of the negative z-axis. It’s as
if the monopole carries around a tail of singularities. This tail is known as a Dirac
string. Similarly, we can deﬁne another function
AS = −g
4πr
(1 + cos θ)
sin θ
ˆeφ =
g
4πr cot θ
2 ˆeφ
(θ > 0),
(49.8)
which does the same job but has a singularity at θ = 0, so although we’ve removed
the tail of singularities along the negative z-axis, we now have them along the positive
z-axis: we’ve only managed to move the problem, not remove it. This illustrates the
point that no single function will work everywhere.

49.2
The ’t Hooft–––Polyakov monopole
453
One interesting consequence of these results is that the two magnetic vector po-
tential functions AN and AS only diﬀer by3
3Use has been made of the expression
for grad in spherical polars:
∇f = ∂f
∂r ˆer + 1
r
∂f
∂θ ˆeθ +
1
r sin θ
∂f
∂φ ˆeφ.
AN −AS =
g
2πr
1
sin θ ˆeφ = ∇
„ gφ
2π
«
,
(49.9)
which looks right since AN and AS correspond to the same ﬁeld B and so they
should diﬀer only by a gradient of a scalar function.
The two functions AN and AS represent diﬀerent choices of electromag-
netic gauge. The wave function ψ of an electric charge q surrounding
the monopole can be written in either gauge, but the two expressions
will be related by a gauge transformation:
ψS(r) = ψN(r) exp

−iq
ℏ
gφ
2π

.
(49.10)
Thus as we take φ from 0 to 2π, the wave function will only be single-
valued if
g = nh
q ,
(49.11)
where n is an integer.4 This equation tells us that if the electric charge q
4This is called Dirac’s quantization
condition.
is quantized, then so is the magnetic charge g. The argument also works
in the other direction: if we were to discover quantized magnetic charges
then this would explain the existence of quantized electric charge. This
is Dirac’s formulation of monopoles. There is no necessity for them to
exist, but if they are found, they provide a rationale for the quantization
of electric charge.
49.2
The ’t Hooft–––Polyakov monopole
So far we have been considering single particles: the electrically charged
particle and, by analogy, the magnetically charged particle.5 Now we ask
5In fact, taking the Dirac monopole to
be a point particle is something of a ne-
cessity since, like the electric monopole
it represents a singular solution to the
Maxwell equations.
whether there is a quantum ﬁeld theory for which a magnetic monopole
represents a stable solution? There is: magnetic monopoles are topolog-
ically stable solutions of a non-abelian gauge theory with spontaneous
symmetry breaking.6 As we will show, these ﬁeld theory monopoles, pro-
6Since, in Chapter 47, we saw that such
a theory seems to describe the elec-
troweak part of Nature, it seems likely
that magnetic monopoles might be re-
alized in our Universe!
There should
also, we hope, be a good reason why
we haven’t observed them yet.
posed in 1974 by ’t Hooft and Polyakov, are rather diﬀerent entities to
Gerard ’t Hooft (1946– )
Alexander Polyakov, (1945– )
the Dirac monopoles of the last section.
Example 49.2
The monopole solution is most easily seen if we review the topological objects of
Chapter 29.
There we started with a Lagrangian which was unstable to sponta-
neous symmetry breaking. Then the topological solutions arose by considering time-
independent solutions which sat in a diﬀerent vacuum at diﬀerent points in space:
• The simplest was the kink in (1+1) dimensions, which was a ﬁeld existing in
diﬀerent vacuum states at ±∞.

454
Magnetic monopoles
• The second simplest was the vortex in (2+1) dimensions, where a ﬁeld with
broken U(1) symmetry enjoyed diﬀerent vacua at all points on a circle at
spatial inﬁnity.
Obviously the next step is to examine (3+1) dimensions and a conﬁguration with
broken SO(3) symmetry which enjoys a diﬀerent vacuum at all points at a spherical
shell at inﬁnity.
The solution with this topological property in (3+1)-dimensional space-
time is the monopole, which has the form as |r| →∞that
Φ(r) = A r
|r|
(|r| →∞),
(49.12)
with A a constant.
The thing to notice here (as with the vortex) is
the distinction between internal and real space components. The vector
Φ lives in internal isospace but the radial vector r lives in real space.
However, eqn 49.12 links these two together. In components we have,
for example φ1(|r| →∞) = A x
|r|, that is, along the spatial 1-direction
(also known as the x-direction) the ﬁeld points along the internal 1-
direction.
Similarly along the y-direction the ﬁeld points along the
internal 2-direction and so on. In this sense, the ﬁeld points radially
outwards at inﬁnity, which is why Polyakov called this the hedgehog
solution. This is represented in Fig. 49.4.
Fig. 49.4 The hedgehog solution.
Our task is now to ﬁnd a theory for which the hedgehog is a solu-
tion and then to show that it has the electromagnetic properties of a
monopole. Let’s consider a gauge theory with local SO(3) symmetry,
deﬁned by a Lagrangian
L = 1
2(DµΦ) · (DµΦ) + m2
2 Φ · Φ −λ(Φ · Φ)2 −1
4Gµν · Gµν, (49.13)
where Gµν is the gauge ﬁeld tensor and DµΦ is the covariant derivative
given, respectively, by
Gµν
=
∂µWν −∂νWµ + qWµ × Wν,
DµΦ
=
∂µΦ −qΦ × Wµ,
(49.14)
with q the SO(3) charge of the theory (which will be shown to be equiv-
alent to electric charge a little later). Notice that the Lagrangian we
have written is unstable to spontaneous symmetry breaking so Φ is a
Higgs ﬁeld.7 As stated above, we seek a topological ﬁeld conﬁguration
7Recall what happens in such a the-
ory. Owing to the positive mass term,
the symmetry is spontaneously broken
and the Higgs ﬁeld φ takes on a con-
stant value in the ground state |Φ0| =
“
m2
4λ
” 1
2 . The broken symmetry ground
states form a spherical shell S2 in Φ-
space of radius |Φ0|.
Previously we
have chosen to direct the Φ-ﬁeld along
the Φ3-direction.
In that case W 3
µ
is equivalent to the usual electromag-
netic ﬁeld Aµ and we recover Maxwell’s
electromagnetism.
We therefore take
the charge of the theory to be q: the
electromagnetic charge.
(Although of
course we know that our electromag-
netism resulted from symmetry break-
ing in a SU(2)×U(1) gauge theory, the
SO(3) theory will be suﬃcient for ex-
amining monopoles solutions.)
for this theory with a diﬀerent ground state at each point on the spatial
boundary of the theory, which is the spherical shell S2 at |r| →∞. The
resulting hedgehog solution of the Lagrangian in eqn 49.14 is given by
Φ =

m2
4λ
 1
2
r
|r|
(|r| →∞).
(49.15)
As with all time-independent soliton-like solutions in more than one
spatial dimension,8 Derrick’s theorem9 tells us that this object, taken
8Recall that the vortex is one such ob-
ject.
9See Section 29.3.
alone, gives rise to an inﬁnite energy. We therefore need to appeal to
the gauge ﬁeld to stabilize the monopole and guarantee that its energy

49.2
The ’t Hooft–––Polyakov monopole
455
remains ﬁnite. For this to be true we require the gauge ﬁeld Wµ for
|r| →∞to be (see Exercise 49.1)
W a
i
=
−εiab rb
qr2 ,
W a
0
=
0,
(49.16)
where here a is the internal index and i is the real-space vector index.
Example 49.3
We have identiﬁed a topological object that can exist in (3+1)-dimensional spacetime,
but we’ve yet to ﬁnd its electromagnetic properties. This turns out to be a rather
non-trivial matter since we have three internal components of the ﬁeld strength tensor
Gµν and, therefore, three diﬀerent choices of magnetic ﬁeld10 Bai = −εijkGa
jk (with
10Here,
as
elsewhere
in
this
chap-
ter, a = 1, 2, 3 is the internal index
and i, j, k are vector indices.
Com-
pare these three choices with the sin-
gle choice from conventional electro-
magnetism: Bi = −εijkFjk (no sum
implied over repeated indices).
no sum implied over repeated indices). For the more usual uniform broken symmetry
ground state, where Φ points along the 3-direction, the choice is simple, the a = 3
component of Ga
µν reduces to the usual form Fµν. More generally for an arbitrarily
oriented ground state (i.e. one not necessarily pointing along the 3-direction) we have
Fµν = Φ
|Φ| · Gµν.
(49.17)
For any situation other than the ground state of Φ we need a more complicated
expression containing terms in DµΦ which vanish in the ground state.11 Fortunately,
11See Exercise 49.3.
these complexities may be ignored since we only require the magnetic ﬂux emerging
from the hedgehog solution, for which we ﬁnd
Fij = F ij = −εijk rk
q|r|3 .
(49.18)
Finally we’re ready to see what the hedgehog ﬁeld predicts for the magnetic ﬁeld of
a monopole: it is
B = 1
q
r
|r|3 ,
(49.19)
which is exactly the radial ﬁeld expected for a monopole. It results in a magnetic
ﬂux
ΦM = 4π
q .
(49.20)
The hedgehog solution is therefore conﬁrmed as a magnetic monopole. Comparing
the ﬂux from the hedgehog with the ﬂux expected from a magnetic monopole Φ = g,
we must have that 4π
q = g or, on restoring factors of ℏ,
g = 2h
q .
(49.21)
Comparing this with the result for a single Dirac monopole, for which g = h/q, we see
that the magnetic charge of the ’t Hooft–Polyakov monopole is twice the magnitude
of the Dirac monopole.
Notice how the ’t Hooft–Polyakov monopole and Dirac monopole are
very diﬀerent beasts. Dirac monopoles are singular mathematical so-
lutions of electrodynamics which necessitate the introduction of point
particles as the sources of the magnetic ﬂux. These particles have arbi-
trary spin and mass. In contrast ’t Hooft–Polyakov monopoles are non-
singular solutions arising from the interaction of a non-abelian gauge
theory and a scalar ﬁeld. All of their properties, such as their mass, are
determined by the original theory.12
12The fact that these monopoles are so-
lutions to the sort of broken symme-
try theory that describes our Universe
leads us to question why we have not
yet observed any of them.
’t Hooft
estimated the mass of these beasts as
MW/α ≈137MW ≈11 TeV, making
them very heavy indeed and outside the
range of our experiments. Their exis-
tence therefore remains an open ques-
tion.

456
Magnetic monopoles
Chapter summary
• Dirac’s magnetic monopole leads to a singularity in the magnetic
vector potential which cannot be removed (the so-called Dirac
string).
The quantization of magnetic charge of this monopole
is inextricably linked to the quantization of electric charge.
• The ’t Hooft–Polyakov monopole is a topological object in a non-
abelian gauge theory.
Exercises
(49.1) Show that in order to kill the divergent derivative
of the monopole ﬁeld Φ =
“
m2
4λ
” 1
2
r
|r| at r →∞we
must introduce a covariant derivative with a gauge
ﬁeld with components Wµ
W a
i
=
−εiab rb
qr2 ,
W a
0
=
0.
(49.22)
(49.2) Verify eqn 49.19.
(49.3) We can generalize the deﬁnition of the Maxwell
electromagnetic ﬁeld Fµν so that it describes elec-
tromagnetism in the presence of the topologi-
cal monopole and reduces to ordinary electromag-
netism under the normal circumstance of a broken
symmetry Universe with components Φ3 ̸= 0 and
Φ(1,2) = 0. We deﬁne the modiﬁed electromagnetic
ﬁeld tensor as
Fµν = Φ
|Φ| · Gµν −
Φ
q|Φ|3 · (DµΦ × DνΦ). (49.23)
(a) Check that this reduces to the ordinary form of
F µν as claimed.
We may also deﬁne Aµ =
1
|Φ|Φ · Wµ, which, in
a broken symmetry Universe without monopoles,
picks out the part of the potential that gives ordi-
nary electromagnetism.
(b) Show that this leads to a cleaned up ﬁeld equa-
tion
Fµν = ∂µAν −∂νAµ−
Φ
q|Φ|3 ·(∂µΦ×∂νΦ). (49.24)
(c) Starting with eqn 49.24 verify the algebra that
leads to the value of the B-ﬁeld in eqn 49.19.

50
Instantons, tunnelling and
the end of the world
50.1 Instantons in quantum parti-
cle mechanics
458
50.2 A particle in a potential well
459
50.3 A particle in a double well
460
50.4 The fate of the false vacuum
463
Chapter summary
466
Exercises
466
There is a theory which states that if ever anybody discov-
ers exactly what the Universe is for and why it is here, it
will instantly disappear and be replaced by something even
more bizarre and inexplicable. There is another theory which
states that this has already happened.
Douglas Adams (1952–2001)
In the true vacuum, the constants of nature, the masses and
couplings of the elementary particles, are all diﬀerent from
what they were in the false vacuum, and thus the observer is
no longer capable of functioning biologically, or even chemi-
cally.
Sidney Coleman (1937–2007)
One aspect of conventional quantum mechanics that we have not yet
addressed with quantum ﬁeld theory is tunnelling. In this chapter we
will discuss a class of objects known as instantons which are found
in the path integral version of quantum mechanics. Their ﬁeld theory
analogues allow us to address the problem of tunnelling in quantum ﬁeld
theory. In particular we will address the apocalyptic question of the end
of the world!
We know that when a system breaks a symmetry the
result is that the vacuum of a system is one of (potentially) a number
of equivalent vacua. This is presumably the case in our own Universe.
But what would happen if the vacuum that the system adopts is not
actually the lowest energy state? What if there is one with a slightly
lower energy? If this is the case with our own Universe then we might
worry that the Universe will undergo a transition, via a tunnelling event,
into the true vacuum with catastrophic consequences for those of us who
have grown dependent on the physics of the current (false) vacuum.1
1This problem was called the ‘The fate
of the false vacuum’ by Sidney Cole-
man and our approach to this problem
follows his treatment closely.
See the
superb lecture in his collection Aspects
of Symmetry for the full story.
We start by examining instantons. These are very similar to the kinks
we examined in Chapter 29 which exist as stable entities localized in
space. Instantons are constant energy solutions to quantum mechanical
equations of motion with a characteristic, stable structure localized in
time. The name ‘instanton’, coined by ’t Hooft, reﬂects this particle-
like existence in time, rather than space. Instantons are associated with
critical points (local maxima, minima or saddle points) of the action.

458
Instantons, tunnelling and the end of the world
50.1
Instantons in quantum particle
mechanics
To reveal the physics of the instanton we will temporarily leave behind
ﬁelds and deal with single-particle quantum mechanics, albeit described
by the path integral and Green’s functions. We will examine a single
particle in one spatial dimension described by a Lagrangian L = p2/2m−
V (x). This gives rise to an action S =
R T/2
−T/2 dt L and a path integral
G =
R
D[x(t)] eiS/ℏwhich gives us the Green’s function, or amplitude, for
any given scenario. (For simplicity we will henceforth assume the particle
described here has unit mass.)
We now make a switch to Euclidean
space,2 where we have a Euclidean action
2In Chapter 25 we used the Wick ro-
tation, which turns Minkowski space-
time into Euclidean spacetime via the
rotation taking x0 →−iτ, purely as a
means of relating quantum ﬁeld theory
to statistical physics. In this chapter we
will see that working in Euclidean space
has the advantage of leading to new
and useful solutions to the equations
of motion of quantum particles. Note
that here we do not impose the peri-
odic boundary conditions of statistical
physics and so imaginary time stretches
from −∞≤τ ≤∞. We will also call
the time interval over which the calcu-
lations are carried out T in Minkowski
space and TE in Euclidean space.
SE =
Z TE/2
−TE/2
dτ
"
1
2
dx
dτ
2
+ V (x)
#
,
(50.1)
and a path integral
G(xf, TE/2, xi, −TE/2) =
Z
D[x(τ)] e−SE/ℏ,
(50.2)
where G is the Green’s function describing the amplitude for starting at
(−TE/2, xi) and ending up at (TE/2, xf), and the integral is carried out
over all trajectories that have this property.
Let’s suppose a stationary trajectory exists called ¯x(τ). Applying the
Euler–Lagrange equations to the Euclidean action leads to
d2¯x
dτ 2 = dV (¯x)
dx
,
(50.3)
which diﬀers from the usual equation of motion for a particle in a poten-
tial by a minus sign. Doing our analysis in Euclidean space has resulted
in equations of motion that seem to describe the motion of a particle in a
potential −V (x). As a result, the constant of the motion corresponding
to energy is given by
E = 1
2
d¯x
dτ
2
−V (¯x).
(50.4)
The solutions to the equations of motion that minimize the action have
constant energy and so this equation for E enables us to identify them
very conveniently.
Example 50.1
In order to ﬁnd the Green’s functions which describe amplitudes in this potential we
will use what is known as the stationary phase approximation whose guts are
described here. Since we know that the path integral will be dominated by those
paths closest to the stationary trajectory3, we split the integral into
3See Chapter 23.
G = e−(Stationary action)/ℏ× (quantum corrections).
(50.5)

50.2
A particle in a potential well
459
The key to solving problems is then to ﬁnd the stationary action for the situation
we are trying to describe and expanding around this point. The implementation of
this approximation becomes clear if we look at a simple integral I =
R
dx e−f(x).
Expanding f(x) about a stationary point at ¯x we have
f(x) = f(¯x) + 1
2f′′(¯x)(x −¯x)2 + ...
(50.6)
The second term will present us with a Gaussian integral and so we obtain
I ≈e−f(¯x)
„
2π
f′′(¯x)
« 1
2
.
(50.7)
This result carries over to the case of the functional integral and we obtain an ex-
pression for the Green’s function, for our Euclidean theory of a unit mass particle,
given by the stationary phase approximation as
G(xf, TE/2, xi, −TE/2) = Ne−S[¯x]/ℏ
»
det
„
−∂2
∂τ 2 + V ′′(0)
«–−1
2
,
(50.8)
which is accurate to order O(ℏ), which will be suﬃcient for our purposes, and where
N supplies the normalization.
The conclusion from all of this scene-setting is simply that using the
Euclidean action has the eﬀect of ﬂipping the sign of the potential. The
point of this is that there are stationary solutions which exist in the
Euclidean world of upside-down potentials which may be analytically
continued back to Minkowski space. We therefore potentially gain access
to lots of new possibilities for the motion of particles to which we were
previously ignorant. One of these is tunnelling, which would never be
predicted by calculating the quantum corrections to a stationary path
found in Minkowski space.
In the next sections we will build some conﬁdence in this approach by
examining the stationary solutions that follow from some simple upside-
down potentials.
50.2
A particle in a potential well
Example 50.2
As a warm up exercise in getting used to the formalism, we will examine the case of a
particle in a potential well such as that shown in Fig. 50.1, which has V (x = 0) = 0.
In the Euclidean version of the story the well is turned upside down into the
x
V
Fig. 50.1 A potential well.
x
−V
Fig. 50.2
An upside-down potential
well, inverted by the Wick rotation to
Euclidean space.
potential hill shown in Fig. 50.2. If we impose boundary conditions that we want
xi = xf = 0 then clearly the only solution is for a particle to balance at the top of the
hill. This involves the particle stopped at x = 0 for the entire interval TE and so has
action S(¯x) = 0. Using our equation for the Green’s function we ﬁnd the quantum
amplitude that the particle starts at x = 0 at time τ = −TE/2 and is found there
again at τ = TE/2 is given by
G(0, TE/2, 0, −TE/2) = N
»
det
„
−∂2
∂τ 2 + ω2
«–−1
2
,
(50.9)
where ω2 = V ′′(0). Recall that we’ve previously solved a related (exactly solvable)
path integral problem: that of the simple harmonic oscillator potential. In that case
we concluded that (in Minkowski spacetime) we have
G(0, T/2, 0, −T/2) = ⟨0|e−i ˆ
HT/ℏ|0⟩= N
»
det
„
−∂2
∂t2 −ω2
0
«–−1
2
,
(50.10)

460
Instantons, tunnelling and the end of the world
where ω0 = V ′′(0) and V (x) = 1
2 ω2
0x2. The stationary phase version above is simply
an approximate generalization of this result with a sign change due to our working
in Euclidean space.
Returning to the more general potential and to Euclidean space we see that work-
ing out G is made diﬃcult by the determinant. However, we can argue the general
form of the answer. We know that in Euclidean space the Green’s function will look
like G = ⟨0|e−ˆ
HTE/ℏ|0⟩= e−E0TE/ℏ, and we expect that the ground state energy
should be something close to E0 ≈
1
2ℏω for small oscillations in a potential well,
suggesting that G ∝e−ωTE/2. In fact, for large TE, it may be shown that
G(0, TE/2, 0, −TE/2) = N
»
det
„
−∂2
∂τ 2 + ω2
«–−1
2
=
“ ω
πℏ
” 1
2 e−ωTE
2
,
(50.11)
and we read oﬀthe ground state energy as E0 = 1
2 ℏω [1 + O(ℏ)], as we expect.
We’ll follow the same procedure again for more interesting potentials.
The procedure is to write down the potential and shift it to Euclidean
space, ﬁnd the stationary conﬁguration, work out its action and then
ﬁnd the approximate Green’s function using eqn 50.8.
50.3
A particle in a double well
x
V
−a
a
Fig. 50.3 The double potential well.
x
−V
−a
a
Fig. 50.4
The double potential well,
turned upside-down by the Euclidean
rotation.
The instanton solution is found when we repeat the procedure outlined
above for the double potential well. We deﬁne the well with minima
at ±a and take V (−x) = V (x).
We also deﬁne V ′′(±a) = ω2. The
potential for the double well is that shown in Fig. 50.3 and shifting
to Euclidean space turns it upside down, as shown in Fig. 50.4. We
look for solutions to the equations of motion with constant energy with
boundary conditions that the particle itself is stationary at τ = ±TE/2.
These are useful since we will eventually make TE very large and so
they will represent well deﬁned solutions of ﬁnite energy.
There are
two obvious sets of possible solutions: one involves a particle ﬁxed at a
or at −a, just as we had in the previous section. However, there is a
more interesting possibility: the particle starts at −a at −TE/2 and ends
up at a at TE/2 as shown in Fig. 50.5. This is the instanton solution.
Viewed with the potential ﬂipped the right way up, the end points of
this solution correspond to a particle tunnelling though the barrier.
x
−V
Fig. 50.5 The instanton solution.
Example 50.3
We may calculate the properties of the instanton. With the potential as deﬁned in
Fig. 50.5 this solution has E = 0, so we have
dx
dτ = (2V )
1
2 .
(50.12)
The action of an instanton is given by
S0 =
Z
dt
"
1
2
„dx
dτ
«2
+ V
#
=
Z
dτ
„dx
dτ
«2
=
Z a
−a
dx (2V )
1
2 .
(50.13)
At large TE we have that ˙x = ω(a −x) leading to
(a −x) ∝e−ωτ,
(50.14)
which tells us that the (temporal) size of an instanton is 1/ω.

50.3
A particle in a double well
461
The previous example demonstrates that the instanton has a well deﬁned
structure in time of size τ ∼1/ω. It is shown in Fig. 50.6, where its
temporal extent is given by the width of the region where it crosses the
axis. Notice the similarity between this and the kink in Chapter 29.
(The kink represented a well deﬁned structure in space, with a spatial
extent l ∼1/m.)
τ
φ(τ)
a
−a
Fig. 50.6
The instanton as a tightly
bound structure in (imaginary) time.
In order to use the instanton to predict the properties of the double
well we must allow the possibility of the existence of more than one tun-
nelling event. This corresponds to examining more than one instanton.
This is the subject of the next example.
Example 50.4
We will now examine the case of a dilute gas of instanton objects. Using the same
double well, the ‘gas’ corresponds to a particle on top of one hill that occasionally
rolls down the hill and up to its neighbour, and then some time later rolls back down
the hill and up to where it started. Some time later it undergoes another rolling
event, and so on. The gas then comprises n rolls, constrained so that an instanton
a →−a must be followed by an anti-instanton −a →a. (Recall that we had the
same constraints on kinks and antikinks.) The gas is dilute in that each instanton
has a size (in time) far smaller than the gaps between instantons and anti-instantons.
The gas is shown in Fig. 50.7. This may the strangest picture of a gas it’s possible
to draw!
τ
x
−TE/2
TE/2
τ1
τ2
τ3
Fig. 50.7 A gas of instantons for the
double well problem.
The action of n dilute instantons is given by nS0, where S0 is the action of one
instanton that we calculated above. For a single well we had G(0, TE/2, 0, −TE/2) =
(ω/πℏ)
1
2 e−ωTE/2. For the gas in the double well we need to consider this amplitude
multiplied by the contribution K from each of the n instantons:
“ ω
πℏ
” 1
2 e−ωTE/2Kn,
(50.15)
where K is the amplitude for the occurrence of a single instanton. Since we don’t
know where each instanton is centred in time, we must integrate this quantity over
the centres of all of the instantons. This is conveniently carried out by noting that
Z TE/2
−TE/2
dτ1
Z τ1
−TE/2
dτ2...
Z τn−1
−TE/2
dτn = T n
E
n! .
(50.16)
Putting all of this together we have, for example, to order O(ℏ), an amplitude for
the particle to start and ﬁnish at x = −a of
Note that ℏhas been reintroduced for
clarity.
G(−a, TE/2, −a, −TE/2) =
“ ω
πℏ
” 1
2 e−ωTE/2 X
ne
(Ke−S0/ℏTE)ne
ne!
,
(50.17)
where ne means that n only includes even numbers, ensuring that, however large we
make n, the particle ends up where it started.4 These sorts of sums may be done and
4The same argument says that amnesia
may be avoided if one is hit over the
head an even number of times.
the general result is that
G
„
±a, TE
2 , −a, −TE
2
«
= 1
2
“ ω
πℏ
” 1
2 e−ωTE
2
»
exp(Ke−S0
ℏTE) ∓exp(−Ke−S0
ℏTE)
–
.
(50.18)
We may read oﬀthe physics by comparing with e−ETE/ℏas we did for the single
well. In this case we ﬁnd that we have two low-lying energy states with energies
corresponding to whether we ﬁnished at −a (summing over even n) or +a (summing
over odd n). The energies of the two states are given by
E± = 1
2ℏω ± ℏKe−S0/ℏ.
(50.19)
It’s worth pausing at this stage to take stock of what we’ve done by returning to
Minkowski space and ﬂipping the potential back the right way up.

462
Instantons, tunnelling and the end of the world
The conventional quantum mechanics of the double well is instructive here. We
have solutions that are localized in the left-hand well |−a⟩and in the right-hand well
|a⟩. We’ll call the energy for sitting in a potential minimum U0, but we allow the
Hamiltonian to permit barrier penetrating transitions between minima with matrix
element ⟨−a| ˆH|a⟩= ⟨a| ˆH| −a⟩= −V . The solutions are
1
√
2
“
| −a⟩+ |a⟩
”
E = U0 −V,
1
√
2
“
| −a⟩−|a⟩
”
E = U0 + V.
(50.20)
The system saves some energy through the wave function spreading out over the two
wells. The symmetric combination lies lowest, separated by an energy 2V from the
antisymmetric combination.
In the present case, we can identify U0 =
1
2 ℏω as the energy for sitting in
the bottom of a well and V = ℏKe−S0/ℏ.
This makes sense as the diﬀerence in
energies is proportional to the factor accounting for barrier penetration e−S0/ℏ.
x
V
a
Fig. 50.8 A potential with a barrier.
x
−V
a
Fig. 50.9 The barrier potential turned
upside-down.
Example 50.5
We now examine tunnelling to freedom through a barrier as another example of the
use of instantons. We start with the potential shown in Fig. 50.8 and up-end it by
working in Euclidean space to obtain Fig. 50.9. The stationary solution of interest is a
diﬀerent instanton to those considered thus far. This instanton, shown in Fig. 50.10,
involves the particle starting at xi = 0, travelling to x = a, where it bounces, reverses
its direction and returns to its starting point at xf = 0.
x
−V
Fig. 50.10
The bounce solution: an-
other example of an instanton.
This is rather similar to the previous problem of the instanton gas, except that
the Green’s function is given by summing over all n, since any number of instantons
result in the particle back where it started at x = 0. We may carry out the sum to
give
G(0, TE/2, 0, −TE/2) =
“ ω
πℏ
” 1
2 e−ωTE/2 exp
h
Ke−S0/ℏTE
i
,
(50.21)
which we might think tells us of an energy eigenvalue of E0 = 1
2ℏω + ℏKe−S0/ℏ.
However, the interpretation of this solution is slightly more subtle.
Let’s return to the right-way-up potential of Fig. 50.8. We’ve seen that the factor
ℏKe−S0/ℏtells us about barrier penetration, but penetrating through the barrier in
this problem results in the particle rolling away to inﬁnity. This suggests that the
bound state in the potential is unstable. Additional evidence arises when we plot the
trajectory of the bouncing particle, which is shown in Fig. 50.11. The fact that the
trajectory has a maximum tells us that the wave function must have a node. This
means it’s not the lowest energy wave function and there must be a lower one with a
negative energy. Since K is a function of the square-root of energy then this implies
that K is imaginary. This ﬁts with the idea of barrier penetration: we know that
unstable states have energies with imaginary parts which, when plugged into e−iEt
give e−iE0t−Γt where 2Γ is the decay rate of the state. What our instanton tells us is
not a correction to the energy of a bound state, but rather the imaginary part of the
energy, which gives is the decay rate. We conclude therefore that ImE0 ≈ℏ|K|e−S0/ℏ
and so the decay rate of the state is roughly given by
Γ ≈ℏ|K|e−S0/ℏ.
(50.22)
τ
x
Fig. 50.11 The trajectory of the parti-
cle during the bounce has a maximum,
signalling a node in the wave function.

50.4
The fate of the false vacuum
463
50.4
The fate of the false vacuum
Finally we turn to ﬁelds. As stated in the introduction, our goal is to
describe what happens to a broken symmetry system in the case that
the vacuum that the system has chosen (known as the false vacuum) lies
slightly higher in energy than another minimum in the potential, which
we call the true vacuum. A potential describing this state of aﬀairs is
shown in Fig. 50.12. To be more colourful, we imagine that the vacuum
of the Universe in which we live is a false vacuum (at φ = φ+) and ask
whether the Universe is about to tunnel through a potential maximum
into the real vacuum (at φ = φ−).
φ
U
φ−
φ+
ε
Fig. 50.12 False vacuum potential.
By analogy with the case of the magnet in Chapter 26, the catas-
trophic quantum collapse of the false vacuum would seem rather un-
likely.
In the magnet example the process of tunnelling to the true
vacuum would rely on each electronic spin in the system simultaneously
tunnelling. We might have expected this to have a vanishingly small
probability since there are so many particles. However, the possibility
of instanton solutions in the ﬁeld allow a scenario where a cluster of spins
ﬁnds the true vacuum and then an instanton causes the spins forming
the boundary of this ‘bubble’ to ﬂip, causing the Universe to pass a
tipping-point beyond which there is suﬃcient volume of true vacuum
to make this the preferred phase. Whether a change in phase of the
Universe actually occurs depends on the relative balance of the energy
saving in realizing the true vacuum versus the cost of the domain wall
between true and false vacua.
Example 50.6
Supercooled liquid presents us with a similar problem. In that case we cool a liquid
which reaches a metastable state of liquidity, separated by a small energy from the
true vacuum of solidity. A similar situation occurs for the superheated liquid, which
remains in a metastable state of liquidity rather than evaporating.5
5Although in this case, you should note
that there’s no diﬀerence in symmetry
between a liquid and a gas, so the anal-
ogy with symmetry breaking no longer
carries.
r
E
Fig. 50.13 The energy of a bubble of
gas of radius r in a superheated liquid.
For the superheated liquid, thermal ﬂuctuations often result in a small bubble of
gas appearing. The inside of the bubble is the true ground state and so results in
an energy saving ǫ, per unit volume. On the other hand, the surface energy of the
bubble costs energy σ per unit surface area. We might write the energy
E = −4
3πr3ǫ + 4πr2σ,
(50.23)
shown in Fig. 50.13. Small bubbles, with a small volume energy saving compared
to surface area cost will disappear. At some point a bubble will be large enough
that it is energetically favourable for it to increase in size. By ﬁnding the stationary
point in E we can work out how large a bubble must be to become ever-expanding.
Extremizing, we ﬁnd a maximum in E at
r = 2σ
ǫ .
(50.24)
Bubbles larger than this ﬁnd it energetically favourable to expand. As this occurs,
more and more space is taken up by the true vacuum, which eventually spreads
throughout all space.

464
Instantons, tunnelling and the end of the world
We examine the decay of the false vacuum by analogy with the particle
problems looked at so far in this chapter. The decay process is described
by a bouncing instanton, which gives a decay rate. Since in quantum ﬁeld
theory we deal with a path integral6 R
Dφ e−
R
d4x LE, then the quantity
6again setting ℏ= 1.
we will ﬁnd is Γ/V, a decay rate per unit volume, which we expect to
be given by
Γ/V = |K|e−S0,
(50.25)
where S0 is the action of the instanton bounce. Calculating the approxi-
mate action of the bounce will be possible: since the minima of the false
vacuum potential are only separated by a small energy diﬀerence, they
closely resemble the exact double well problem for which we know the
action.
Let’s now be speciﬁc. We consider a quantum ﬁeld theory in four-
dimensional Euclidean space. This has an action
S =
Z
d4x
1
2(∂µφ)2 + U(φ)

.
(50.26)
We cook up a symmetric, double-well potential Usym, which we then
modify slightly to obtain the false vacuum potential U:
U = Usym + ε(φ −a)/2a.
(50.27)
The potential has minima at φ± = ±a, although that at φ+ = a lies at
a slightly higher energy, by an amount ε (Fig. 50.12).
We will follow the procedure we developed for identifying instantons
in particle mechanics. We may employ the results from the particle, as
long as we remember that the imaginary time τ in the Euclidean particle
problem has become a spacetime point r in four-dimensional Euclidean
space. The Euclidean potential is upside-down as shown in Fig. 50.14.
The bouncing instanton clearly starts on the hill of the false vacuum
and bounces from the point φc, where U(φ) cuts the φ-axis.
φ
−U
φ−
φ+
φc
Fig. 50.14 False vacuum potential in
Euclidean space.
We can also imagine this running backwards, with the particle starting
at a point φc near the true vacuum at φ−. We choose to look at it this
way around because the bubble that will result in 4-space then resembles
the bubble in the superheated liquid with true vacuum within and false
vacuum without (shown in Fig. 50.15). The particle will roll down the
hill and come to a rest exactly on top of the hill at φ+ (the false vacuum).
We’ll imagine this takes place very rapidly around some point in time r
that we’ll call r = R. Back in ﬁeld language we ask what the hill-roll
looks like. In Euclidean space we have a large spherical bubble of radius
r = R.
This has a thin wall representing the rolling process.
(The
wall is thin because the roll takes place very quickly.) The bubble wall
separates the false vacuum on the outside of the bubble from the true
vacuum on the inside. The wall of the bubble contains the instanton. If
R, the spacetime point where the instanton is centred, is greater than
the critical radius Rc above which the bubble is self-sustaining, then the
instanton creates a bubble that will drag all of spacetime into the true
vacuum.
Rc
True
vacuum
False
vacuum
Fig. 50.15
The bubble in Euclidean
space.

50.4
The fate of the false vacuum
465
We will now construct an action S for the rolling process. As r in-
creases we start inside the true vacuum bubble, move through the wall
and end up outside. The action is given for this spherically symmetric
problem by
S0 = 2π2
Z ∞
0
dr r3
"
1
2
d¯φ
dr
2
+ U
#
.
(50.28)
Unlike the previous examples, where only the instanton part contributed
to the action, we need to be more careful here. Inside the bubble we are
in the true vacuum where we have φ−and U = −ǫ, so only the second
term in the integral contributes, yielding an action
Strue vacuum = −1
2π2R4ε.
(50.29)
Over the thin surface of the bubble we have a contribution from the
instanton part:
Sinstanton = 2π2R3
Z
dr
"
1
2
d¯φ
dr
2
+ Usym
#
,
(50.30)
where, as advertised, we choose to neglect the diﬀerence ε in energy
between the minima and so only need consider the symmetric double
well potential Usym. We know that the instanton solution takes us from
−a to a as r increases through R. By analogy with the particle problem,
such an instanton has an action S1 =
R a
−a dφ (2Usym)
1
2 . This gives us a
contribution to the action from the instanton of
Sinstanton = 2π2R3S1.
(50.31)
Outside the bubble we are in the false vacuum where φ = φ+ and U = 0,
which makes no contribution to the action. The total action is therefore
given by
Stot = −1
2π2R4ε + 2π2R3S1.
(50.32)
We may now extremize the action to ﬁnd the critical radius Rc at which
the tunnelling destroys the false vacuum. We vary the total action with
respect to R and ﬁnd a radius Rc = 3S1/ε. Substituting this back into
the expression for the total action of the bubble we predict a rate of
tunnelling to annihilation of
Γ/V ≈|K|e−Stot,
(50.33)
where Stot = 27π2S4
1/2ε3.
x
t
O
Bubble light cone
Bubble wall
|x| = t
Rc
Fig. 50.16
Spacetime diagram show-
ing the growth of the bubble. An ob-
server at O only knows of the bubble
when its forward light-cone intersects
the observer’s world-line. A time Rc/c
seconds later the bubble wall intersects
the observer’s world-line.
Now we swap back into the Minkowski space of our Universe, and ask
how the Universe will end. An analytical continuation of these results
back to Minkowski space shows that the shape of the bubble in Euclidean
4-space is the same as that of the bubble in (3+1) dimensions. As R
increases, the bubble expands, sweeping out a region in spacetime given
by the hyperboloid
|x|2 −(ct)2 = R2
c.
(50.34)

466
Instantons, tunnelling and the end of the world
We might expect that since the initial bubble is created by quantum
mechanical ﬂuctuations then Rc should be a very small number, perhaps
of the order of 1 fm. This would mean, from eqn 50.34, that |x|2 ≈
(ct)2 and the bubble expands at almost the speed of light. This, rather
inconveniently, gives an observer eﬀectively no warning of the coming of
the true vacuum. Looked at on the spacetime diagram in Fig. 50.16 we
see that the observer O doesn’t know about the existence of the bubble
until the light from the forward light-cone of the bubble’s outer wall
meets her world-line (see Fig. 50.17). We see that the true vacuum itself
collides with her world-line a time R/c seconds later, and she ceases
to exist.
This time (by assumption, ≈1 fm/c seconds) is orders of
magnitude shorter than the time it takes a neuron to ﬁre. In Sidney
Coleman’s words she ‘has literally nothing to worry about’.
Fig. 50.17
We have essentially no
warning of the coming of the true vac-
uum. However, we can give some ad-
vance notice of the imminent end of this
book.
Chapter summary
• Instantons are kink-like solutions of the equation of motion asso-
ciated with critical points in the action and allow tunnelling prob-
lems to be treated.
• Instantons can be used to study the fate of the false vacuum in
a symmetry broken system, such as the Universe, where the true
vacuum state is only a tunnelling event away.
Exercises
(50.1) Verify the expression for the stationary phase ap-
proximation in eqn 50.8.
(50.2) Verify the solution of the double well problem in
eqns 50.20.

A
Further reading
Many of the ideas in the preceding essay have been taken
from another...
John Berger (1926– )
There are many other excellent books which introduce and describe
quantum ﬁeld theory.1 Whether you are after the friendly approachabil-
1See the detailed list below, beginning
on page 470.
ity of Mattuck or Zee, or perhaps the elegant but unforgiving grandeur
of Weinberg, or something in between, there is something for everyone.
We particularly like Peskin and Schroeder, and also Ryder, and either
(or both) could serve as a useful complement to this book. Many of the
arguments we have used are adapted from these books (Zee and Peskin
and Schroeder deserve special mention here) and also from the legendary
lecture course given by Sidney Coleman.2 Finally, for those eager to prac-
2These mid-1970s lectures are, at least
at the time of writing, viewable on the
Harvard Physics website.
tise, a useful set of solved problems may be found in Radanovic.
Further reading by chapter:
Chapter 1: An accessible introduction to Lagrangian mechanics is given
in Feynman, Leighton and Sands. See Landau and Lifshitz (vol. I) for the
full story. Chapters 2 and 3: The simple harmonic oscillator and occupa-
tion numbers are explained in most books on quantum ﬁeld theory. The
books by Aitchison and Hey,3 P. Coleman and Feynman are especially
3Assume volume I of multivolume texts
unless stated otherwise.
clear. Chapter 4: Several examples of non-relativistic applications of sec-
ond quantization may be found in P. Coleman. Chapter 5: Classical ﬁeld
theory is covered in Maggiore, Ryder and Itzykson and Zuber amongst
many others. Chapter 6: The Klein–Gordon equation and the problems
with its interpretation are explored in Aitchison and Hey. Chapter 7:
The Lagrangians we describe are all examined in most books on quantum
ﬁeld theory. Ryder, Huang, and Zee are good sources of information.
Chapter 8: Time evolution operators and related issues are discussed in
most books on advanced quantum mechanics (Ziman, for example) and
in the lectures by S. Coleman. Chapter 9: Transformations are described
very clearly in Ryder. For a more sophisticated treatment see Weinberg.
Chapter 10: An in-depth discussion of Noether’s theorem may be found
in Neuenschwander. Both Ryder and Huang provide accessible introduc-
tions. Chapter 11: Our treatment of canonical quantization follows S.
Coleman’s lectures. Aitchison and Hey, Bjorken and Drell4 (RQF) and
4We abbreviate Bjorken and Drell’s
Relativistic Quantum Fields as here
RQF to avoid confusion with their Rel-
ativistic Quantum Mechanics (RQM).
Schiﬀare other sources of clear information. Chapter 12: The properties
of the complex scalar ﬁeld are examined in nearly all books on quan-

468
Further reading
tum ﬁeld theory [see Aitchison and Hey or Bjorken and Drell (RQF)].
Our discussion of its non-relativistic properties follows Zee. Chapter 13:
Internal symmetries and vector ﬁelds are discussed in Ryder, Zee and
in Aitchison and Hey. Chapter 14: Gauge theory and its consequences
are very clearly described in Aitchison and Hey who also give a good
exposition of the diﬃculties inherent in the canonical quantization of the
electromagnetic ﬁeld. The solution to these diﬃculties is discussed in
Peskin and Schroeder. Chapter 15: Charge conjugation, parity and time
inversion are described in Aitchison and Hey and in Bjorken and Drell
(RQF). A more sophisticated account is Weinberg. Relevant background
on the mathematics may be found in the books by Georgi and Nakahara.
Chapter 16: Green’s functions are introduced in Mattuck in a similar
way to that described here. An approach to perturbation theory based
on propagators may be found in Schiﬀ. Classical Green’s functions are
described in Barton. Chapter 17: The Feynman propagator is discussed
in Aitchison and Hey. The non-relativistic version is discussed in Mat-
tuck. Chapter 18: Useful information on the S-matrix may be found in
Aitchison and Hey, and also Peskin and Schroeder. Weinberg provides
the philosophy. Chapter 19: The expansion of the S-matrix is carried
out for a simple toy theory in Aitchison and Hey, while the φ4 theory is
discussed in Peskin and Schroeder and in Ryder. Chapter 20: Scatter-
ing is covered by most books on quantum ﬁeld theory and on particle
physics. An introduction to scattering for particle physics applications
is given in Griﬃths. Clear treatments from the ﬁeld theory perspective
appear in Peskin and Schroeder (who discuss ψ†ψφ theory) and also in
Weinberg. Chapter 21: Statistical physics is described in Blundell and
Blundell and at a more advanced level in Chaikin and Lubensky. Linear
response theory is described in Binney, Newman, Dowrick and Fisher.
The analogy between ﬁeld theory and magnetism is described in more
detail in Peskin and Schroeder. See also Zinn-Justin. Chapter 22: Gen-
erating functionals are described in Ryder, Peskin and Schroeder, and
in Binney, Newman Dowrick and Fisher. Chapter 23: The path integral
is described in Zee and Ryder.
A diﬀerent approach is described by
its inventor in Feynman and Hibbs. Chapter 24: Functional integrals
are clearly described in Zee, Peskin and Schroeder and in S. Coleman.
Chapter 25: The Wick rotation is discussed in Peskin and Schroeder
and Altland and Simons.
Kapusta and Gale is a good book on the
applications of statistical ﬁeld theory and Mahan describes how these
techniques are used in solid state physics. Chapter 26: Broken symme-
try is discussed in Blundell and in more detail in Anderson. A lively,
advanced, ﬁeld-centred treatment may be found in S. Coleman. Chap-
ter 27: Coherent states are introduced in Annett and in Loudon. The
history of the phase operator problem is described in the paper by Nieto,
arXiv:hep-th/9304036v1. Altland and Simons and P. Coleman describe
coherent states for ﬁeld applications. Chapter 28: Grassmann variables
are treated in Chapter 44 of Srednicki, and in great detail by Negele and
Orland. Chapter 29: Zee is a very clear source on topological eﬀects and
we have followed his treatment in our presentation. Topological objects

469
are described in Zee and Ryder, with more advanced treatments in S.
Coleman, Weinberg, Nakahara, and in Altland and Simons. Chapter 30:
Our description of topological ﬁeld theory follows Zee and the notes by
Dunne (arXiv:hep-th/9902115v1). It is examined in more advanced form
in Wen. Chapter 31: Quasiparticles are described in Mattuck and P.
Coleman. The Landau Fermi liquid is introduced in P. Coleman. Chap-
ter 32: The philosophy of renormalization is made clear in Peskin and
Schroeder and in Weinberg. Chapter 33: The use of renormalization in
perturbation theory is stressed in Peskin and Schroeder. Chapter 34:
Our presentation of the philosophy of the renormalization group is sim-
ilar to that of Zee and of Peskin and Schroeder. Good sources on the
use of the method may be found in Altland and Simons, Peskin and
Schroeder, Binney, Newman, Dowrick and Fisher and McComb. Chap-
ter 35: More detail on ferromagnetism and the renormalization group
may be found in Altland and Simons. Chapter 36: There are many dif-
ferent approaches to introducing the Dirac equation. Aitchison and Hey
and Ryder are good places to start. A more modern approach is found
in Maggiore. We have followed the approach in Penrose. Chapter 37:
The transformation of spinors is discussed in Ryder and Maggiore and
in more detail in Weinberg. Chapter 38: The quantum mechanics of
fermion ﬁelds is introduced very clearly in Peskin and Schroeder and
in Bjorken and Drell (RQM and RQF). Chapter 39: Simple examples
from QED feature in Aitchison and Hey, Mandl and Shaw and Peskin
and Schroeder.
The history of the subject is explored in Schweber.
Chapter 40: The examples of scattering in QED covered in this chapter
are more fully explained in Peskin and Schroeder. See also Berestet-
skii, Lifshitz and Pitaevskii for lots more examples. Chapter 41: The
renormalization of QED is covered clearly and in detail in Peskin and
Schroeder. Chapter 42: Bolgoliubov’s treatment is described in Ziman.
The Lagrangian treatment of a superﬂuid may be found in Zee and in
more detail in Wen. See also Fulde. Chapter 43: An introduction to
the ﬁeld theory of metals may be found in P. Coleman. See the detailed
tome by Mahan for the full story and also Giuliani and Vignale for more
background. Chapter 44: Our treatment of superconductivity in terms
of second quantized operators follows Annett. The Lagrangian formu-
lation is described by Wen. See also Weinberg (vol. II) and Aitchison
and Hey (vol. II). Chapter 45: The ﬁeld theory of the fractional quan-
tum Hall eﬀect is described straightforwardly in Zee and in more detail
in Ezawa and Wen.
Chapters 46 and 47: Non-abelian gauge theory
and the Weinberg–Salam model are described in Ryder, in Peskin and
Schroeder and, of course, in Weinberg (vol. II). An approach motivated
by particle physics phenomenology is presented in Aitchison and Hey
(vol. II). Chapter 48: Majorana fermions are described in Aitchison and
Hey (vol. II) and in a review by Pal (arXiv:1006.1718v2). Chapter 49:
Magnetic monopoles are described in Ryder, Zee and by S. Coleman.
Chapter 50: Our treatment of instantons is based very closely on the
lecture in the collection by S. Coleman. Appendix B: Useful background
may be found in Penrose. Our treatment follows that of Boas.
Further reading

470
Further reading
Bibliography
• A. A. Abrikosov, L. P. Gorkov and I. E. Dzyaloshinski, Methods of quantum
ﬁeld theory in statistical physics, Dover, New York (1963).
• I. J. R. Aitchison and A. J. G. Hey, Gauge theory in particle physics, vol. I,
3rd edition, IOP, Bristol (2003).
• I. J. R. Aitchison and A. J. G. Hey, Gauge theory in particle physics, vol. II,
3rd edition, Taylor and Francis, New York (2004).
• A. Altland and B. D. Simons, Condensed matter ﬁeld theory, CUP, Cambridge
(2006).
• P. W. Anderson, Basic notions of condensed matter physics, Benjamin-Cumm-
ings, Menlo Park (1984).
• J. F. Annett, Superconductivity, superﬂuids and condensates, OUP, Oxford
(2004).
• A. Auerbach, Interacting electrons and quantum magnetism, Springer-Verlag,
New York (1994).
• T. Banks, Modern quantum ﬁeld theory, CUP, Cambridge (2008).
• G. Barton, Elements of Green’s function and propagation, OUP, Oxford
(1989).
• V. B. Berestetskii, E. M. Lifshitz and L. P. Pitaevskii, Quantum electrodynam-
ics, 2nd edition, (volume IV of Landau and Lifshitz), Butterworth-Heinemann,
Oxford (1982).
• J. J. Binney, N. J. Dowrick, A. J. Fisher and M. E. J. Newman, The theory
of critical phenomena, OUP, Oxford (1992).
• J. D. Bjorken and S. Drell, Relativistic quantum mechanics, Dover, New York
(2012).
• J. D. Bjorken and S. Drell, Relativistic quantum ﬁelds, Dover, New York
(2012).
• S. J. Blundell, Magnetism in condensed matter, OUP, Oxford (2001).
• S. J. Blundell and K. M. Blundell, Concepts in thermal physics, 2nd edition,
OUP, Oxford (2010).
• M. L. Boas, Mathematical methods in the physical sciences, 2nd edition, Wiley,
New York (1983).
• P. M. Chaikin and T. C. Lubensky, Principles of condensed matter physics,
CUP, Cambridge (1995).
• P. Coleman, Introduction to many body physics, CUP, Cambridge, to be pub-
lished. (See also http://www.physics.rutgers.edu/∼coleman/.)
• S. Coleman, Aspects of symmetry, CUP, Cambridge (1985).
• S. Coleman, Physics 253: Quantum ﬁeld theory: lectures by Sidney Coleman.
Lectures from 1975-1976 available to stream from
http://www.physics.harvard.edu/events/videos/Phys253.
• S. Doniach and E. H. Sondheimer, Green’s functions for solid state physicists,
Imperial College Press, London (1998).
• Z. F. Ezawa, Quantum Hall eﬀects, World Scientiﬁc, Singapore (2008).
• R. P. Feynman, Statistical mechanics, Westview, Boulder (1998).
• R. P. Feynman and A. R. Hibbs, Quantum mechanics and path integrals,
Emended edition, Dover, New York (2005).
• R. P. Feynman, R. B. Leighton and M. L. Sands, Lectures in physics, vol. II,
Addison-Wesley, Reading (1963).
• P. Fulde, Correlated electrons in quantum matter, World Scientiﬁc, Singapore
(2012).

471
• H. Georgi, Lie algebras in particle physics, 2nd edition, Westview, Boulder
(1999).
• G. F. Giuliani and G. Vignale, Quantum theory of the electron liquid, CUP,
Cambridge (2005).
• D. J. Griﬃths, Introduction to elementary particles, 2nd edition, Wiley VHC,
Weinheim (2008).
• F. Halzen and A. D. Martin, Quarks and leptons, John Wiley and Sons, Hobo-
ken (1984).
• K. Huang, Quantum ﬁeld theory, Wiley VCH, Weinheim (2010).
• C. Itzykson and J.-B. Zuber, Quantum ﬁeld theory, Dover, New York (1980).
• J. I. Kapusta and C. Gale, Finite-temperature ﬁeld theory, CUP, Cambridge
(2006).
• M. Kaku, Quantum ﬁeld theory, OUP, Oxford (1993).
• L. P. Landau and E. M. Lifshitz, Mechanics (volume I of Landau and Lifshitz),
Pergamon, Oxford (1976).
• E. M. Lifshitz and L. P. Pitaevskii, Statistical Physics, part 2 (volume IX of
Landau and Lifshitz), Pergamon, Oxford (1980).
• R. Loudon, The quantum theory of light, OUP, Oxford (2000).
• M. Maggiore, A modern introduction to quantum ﬁeld theory, OUP, Oxford
(2005).
• G. D. Mahan, Many-particle physics, Plenum, New York (1990).
• F. Mandl and G. Shaw, Quantum ﬁeld theory, 2nd edition, Wiley, Chichester
(2010).
• R. D. Mattuck, A guide to Feynman diagrams in the many-body problem,
Dover, New York (1967).
• W. D. McComb, Renormalization methods, OUP, Oxford (2004).
• V. F. Mukhanov and S. Winitzki, Introduction to quantum eﬀects in gravity,
CUP, Cambridge (2007).
• M. Nakahara, Geometry, topology and physics, Adam Hilger, Bristol (1990).
• D. E. Neuenschwander, Emmy Noether’s wonderful theorem, Johns Hopkins,
Baltimore (2011).
• J. W. Negele and H. Orland, Quantum many-particle systems, Addison Wes-
ley, Reading (1988).
• R. Penrose, The road to reality, Vintage, London (2004).
• M. E. Peskin and D. V. Schroeder, An introduction to quantum ﬁeld theory,
Westview Press, Boulder (1995).
• V. Radovanovic, Problem book in quantum ﬁeld theory, 2nd edition, Springer,
Berlin (2008).
• L. H. Ryder, Quantum ﬁeld theory, CUP, Cambridge (1985).
• J. J. Sakurai, Modern quantum mechanics, revised edition, Addison-Wesley,
Reading (1994).
• L. I. Schiﬀ, Quantum mechanics, 3rd edition, McGraw-Hill, New York (1968).
• S. S. Schweber, QED and the men who made it, Princeton University Press,
New Jersey (1994).
• M. Srednicki, Quantum ﬁeld theory, CUP, Cambridge (2007).
• R. Ticciati, Quantum ﬁeld theory for mathematicians, CUP, Cambridge
(1999).
• A. M. Tvselik, Quantum ﬁeld theory in condensed matter physics, CUP, Cam-
bridge (1995).
Bibliography

472
Further reading
• S. Weinberg, The quantum theory of ﬁelds, vol. I, CUP, Cambridge (1995).
• S. Weinberg, The quantum theory of ﬁelds, vol. II, CUP, Cambridge (1996).
• X.-G. Wen, Quantum ﬁeld theory of many-body systems, OUP, Oxford (2004).
• A. Zee, Quantum ﬁeld theory in a nutshell, CUP, Cambridge (2003).
• J. M. Ziman, Elements of advanced quantum mechanics, CUP, Cambridge
(1969).
• J. Zinn-Justin, Quantum ﬁeld theory and critical phenomena, OUP, Oxford
(1989).

B
Useful complex analysis
B.1 What is an analytic function?
473
B.2 What is a pole?
474
B.3 How to ﬁnd a residue
474
B.4 Three rules of contour inte-
grals
475
B.5 What is a branch cut?
477
B.6 The principal value of an in-
tegral
478
For every complex problem there is an answer that is clear,
simple, and wrong.
H. L. Mencken (1880–1956)
Throughout the book we have tried to keep the amount of complex
analysis to a minimum. This appendix provides a simple guide to some
of the complex analysis commonly employed in quantum ﬁeld theory.
The guide is illustrated by examples drawn from the subject, including
the most important function of a complex variable in quantum ﬁeld
theory: the propagator.
B.1
What is an analytic function?
If a function is analytic in a region close to a point z, then it has a
derivative at every point in that region. We deﬁne the derivative of a
complex number as
f ′(z) = df
dz = lim
∆z→0
∆f(z + ∆z) −f(z)
∆z
.
(B.1)
Importantly, for the function to be analytic, the derivative shouldn’t
depend on the way the interval in the complex plane ∆z is selected.
Example B.1
The function f(z) = z2 is analytic; g(z) = |z|2 is not. To see the ﬁrst write
f′(z)
=
lim
∆z→0
(z + ∆z)2 −z2
∆z
=
lim
∆z→0
z2 + 2z + (∆z)2 −z2
∆z
=
2z,
(B.2)
just as for a normal derivative. Note that we didn’t have a choice of ∆z, this procedure
works whatever we choose.
On the other hand for g(z) = |z|2 we have
g′(z) =
lim
∆z→0
|z + ∆z|2 −|z2|
∆z
.
(B.3)
If ∆z = i∆y (with y real) then you will get a diﬀerent derivative to the case of
∆z = ∆x, with x real. See Boas for more details.

474
Useful complex analysis
B.2
What is a pole?
A pole is a type of singularity1 that behaves like the singularity of 1/zn
1By singularity, we mean a point where
a mathematical object is not deﬁned.
at z = 0. Let f(z) be analytic between two circles C1 and C2. In the
region between them we can write f(z) as a so-called Laurent series
expanded about a point z0:
f(z) = a0+a1(z−z0)+a2(z−z0)2+. . .+
b1
(z −z0) +
b2
(z −z0)2 +. . . (B.4)
The part with b coeﬃcients is known as the principal part of the se-
ries. Don’t confuse this with the principal value of an integral, which is
diﬀerent, and discussed below.
A result and some deﬁnitions:
• If all b’s are zero then f(z0) is analytic at z = z0.
• If all b’s after bn are zero then we say that we have a pole of order
n at z = z0. If n = 1 we have a simple pole.
• The coeﬃcient b1 is called the residue of f(z) at z = z0.
An important example is the function
f(z) =
α
z −β ,
(B.5)
which has residue b1 = α and all other ai and bi zero. This function has
a simple pole at z = β.
Example B.2
Let’s examine the pole structure of two of our propagators.
The non-relativistic,
(a)
E
Ep
−ε
(b)
p0
Ep
−Ep
−ε
ε
Fig. B.1
(a) Position of the pole in
the complex E plane for ˜G+
0 (E).
(b)
Positions of the poles in the complex
p0 plane for the Feynman propagator
for free scalar ﬁelds.
retarded, free electron propagator is given by
˜G+
0 (E) =
i
E −Ep + iǫ .
(B.6)
This has a ﬁrst-order pole at Ep −iǫ. This is shown in Fig. B.1(a).
The Feynman propagator for the free scalar ﬁeld is usually written
˜∆(p) =
i
p2 −m2 + iǫ;
(B.7)
this is helpfully rewritten (see Chapter 17) as a function of the complex variable p0:
˜∆(p) =
1
2Ep
»
i
(p0) −Ep + iǫ −
i
(p0) + Ep −iǫ
–
.
(B.8)
The ﬁrst (particle) part has a simple pole at p0 = Ep −iǫ. The second (antiparticle)
part has a simple pole at −Ep + iǫ. This is shown in Fig. B.1(b).
B.3
How to ﬁnd a residue
You can ﬁnd a residue R(z0) at the pole z0 by writing a Laurent series.
There are more direct methods too. For example, when we have a simple
pole, we can write
R(z0) = lim
z→z0(z −z0)f(z).
(B.9)

B.4
Three rules of contour integrals
475
Example B.3
The residue of ˜G(E) =
iZ
E−Ep+iǫ at the simple pole E = Ep −iǫ is given by
R(Ep −iǫ) =
lim
E→Ep−iǫ(E −Ep + iǫ)
iZ
E −Ep + iǫ = iZ.
(B.10)
B.4
Three rules of contour integrals
A contour C is a closed path in the complex plane with a ﬁnite number
of corners which doesn’t cross itself. Integrals around such contours have
a number of useful properties.
Example B.4
We can get some practice with a contour integral by calculating
I
C
dz z2,
(B.11)
where the contour is shown in Fig. B.2. We split the contour into two parts and start
with the straight line along the real axis. Take z = reiθ and this part becomes
(straight line) =
Z 1
r=−1
dr r2 =
»r3
3
–1
−1
= 2
3.
(B.12)
Now for the semicircle, described by z = r0eiθ, where r0 = 1. We have dz = ir0eiθdθ
giving
(semicircle) =
Z π
θ=0
dθ ir3
0e3iθ =
» e3iθ
3
–π
0
= −2
3.
(B.13)
Adding the contributions we have
(line) + (semicircle) = 0.
(B.14)
z
1
−1
Fig. B.2 An example of a contour in
the complex z plane.
There are three useful theorems for evaluating integrals taken around
contours. The ﬁrst is Cauchy’s theorem:
Augustin-Louis Cauchy (1789–1857)
If f(z) is analytic on and inside C then
I
C
dz f(z) = 0.
(B.15)
This is good news, since it says that if the region in a contour contains
no poles then the integral gives zero. It also explains why the previous
example gives zero: z2 is analytic on and inside the contour.
The second theorem is known as Cauchy’s integral formula:

476
Useful complex analysis
If f(z) is analytic on and inside a simple closed curve C, and the point
a is inside C, then the value of f(a) is given by
f(a) =
1
2πi
I
C
dz f(z)
z −a.
(B.16)
The third is the residue theorem:
If f(z) has singularities at points zi, then, for a closed curve enclosing
these points we have
I
C
dz f(z) = 2πi
X
i

Residue at f(zi)
inside C

,
(B.17)
where the integral around C is performed in the anticlockwise direction.
(You merely change the sign of the answer if you perform the integral
in the clockwise direction.)
Often we want to do diﬃcult integrals over real variables. These may
be turned into easier integrals if we form a contour in the complex plane
which includes the original domain of integration and use the rules given
above. The art is in choosing the best contour to do the integral.
Example B.5
We can use Cauchy’s theorem along with the residue theorem to justify some of the
more seemingly cavalier tricks employed in the discussion of propagators in Chap-
ters 16 and 17. Let’s ﬁnd the inverse Fourier transform of the retarded propagator
˜G+
0 (E), given by
G+
0 (t −t′) =
Z ∞
−∞
dE
2π
ie−iE(t−t′)
E −Ep + iǫ ,
(B.18)
for which the integration path is along the real axis. To use our contour integral rules
we must complete the contour by joining up this path with a further section of path
which will either be in the upper half of the complex E plane or in the lower half.
(a)
E
(b)
E
Fig. B.3 (a) The contour completed in
the lower half-plane enclosing the pole.
Note that the direction here is clock-
wise, earning us an extra minus sign.
(b) The contour completed in the up-
per half-plane.
No poles are enclosed
so the answer is given by Cauchy’s the-
orem.
Suppose we take it in the lower half-plane. Then, as we take the limits along the
real axis to ±∞the semicircular path gets larger and larger. This will make a large,
negative imaginary contribution to E. Let’s call it −i|η|. The exponential will then
involve a contribution e−|η|(t−t′). If (t −t′) is positive then this contribution gets
smaller, eventually vanishing as the contour becomes inﬁnitely large.2 We conclude
2This
disappearance
of
semicircular
contours in the limit of inﬁnite radius
is a result of Jordan’s lemma. This
says that the integral
R ∞
−∞dz f(z)eiaz
along the inﬁnite upper semicircle is
zero, provided (i) a > 0, and (ii) f(z)
is a well-behaved function satisfying
limR→∞
˛˛f(Reiθ)
˛˛ = 0.
that, for the case t −t′ > 0, the integral above is equivalent to the contour shown in
Fig. B.3(a).
Let’s do that integral. The contour contains the pole at E = Ep −iǫ so we use
the residue theorem to say
I
C
dE
2π
ie−iE(t−t′)
E −Ep + iǫ = −2πi
„
Residue at
E = Ep −iǫ
«
,
(B.19)
where the minus sign follows from our attempt to take the integral in the clockwise
direction. The residue at the pole is ie−iEp(t−t′)e−ǫ(t−t′) and the answer is
G+(t −t′) = e−iEp(t−t′)e−ǫ(t−t′),
(B.20)
which we stress applied for (t −t′) > 0.

B.5
What is a branch cut?
477
What if we tried to complete the contour in the upper half-plane? Then we would
have obtained a large, positive, imaginary contribution to the exponential resulting
in a contribution e|η|(t−t′) which blows up for t −t′ > 0. Such a badly behaved
integral is certainly not suitable for evaluating the Fourier transform. However, for
t −t′ < 0 the semicircular contour has a vanishing contribution at inﬁnity and we
again have the equivalence of the Fourier transform and the contour C′ shown in
Fig. B.3(b). Notice that C′ contains no poles, so Cauchy’s theorem says that the
integral is zero.
We conclude that G+(t−t′) = 0 for t−t′ < 0 and noting that ǫ is an inﬁnitesimal
quantity, we may replace both (i) zero for t −t′ < 0 and (ii) eǫ(t−t′) for t −t′ > 0
with θ(t −t′) and conclude
G+(t −t′) = θ(t −t′)e−iEp(t−t′),
(B.21)
just as we had in Chapter 16 without the need for adding damping factors by hand!
The meaning of the iǫ factors now becomes clear. These inﬁnitesimals
position the poles of the propagators in such a way as to ensure the
correct causality relationships. Returning to the scalar ﬁeld propagator
of Fig. B.1 we see that closing the contour in the lower half-plane picks
up the positive energy pole, leading to a factor θ(x0 −y0), while closing
the contour in the upper half-plane picks up the negative energy pole
and leads to the factor θ(y0 −x0). This motivates the deﬁnition of the
Feynman propagator in Chapter 17.
B.5
What is a branch cut?
The function eiθ is multivalued. We are always at liberty to add whole
numbers of 2π to the argument in the exponential, i.e. eiθ = ei(θ+2πn)
where n is an integer. The fact that this function is multivalued means
that care must be taken when taking roots and logarithms.
Example B.6
For the case of the logarithm we have, taking z = reiθ:
ln z = ln r + iθ.
(B.22)
Clearly, for ﬁxed r, ln z takes diﬀerent values for θ and θ + 2πn, even though both
choices correspond to the same z.
We therefore agree that we should only consider angles in some interval
in θ of size 2π, known as a branch of the function. In order to make this
clear in the complex plane, we can deﬁne lines that we will agree not
to cross with any of our operations. These are known as branch cuts.
The points from which these emerge are known as branch points.
For ln z the branch point is the origin and the branch cut may be taken
along the positive real axis as shown in Fig. B.4(a). It may also be taken
along the negative real axis, or indeed any convenient line. Crossing the
branch cut makes the function jump by 2πi.
(a)
x
(b)
|p|
Fig. B.4
(a) The branch cut along
the positive, real axis.
(b) The com-
plex plane for eqn. B.23.

478
Useful complex analysis
Recall from Chapter 8 the integral
−i
(2π)2|x|
Z ∞
−∞
d|p| |p|ei|p||x|e−it√
|p|2+m2,
(B.23)
which we consider in the complex |p| plane. The square root in this
equation
p
p2 + m2 must be restricted to a single branch. The square
root vanishes for |p| = ±im.
which are therefore the branch points.
For convenience, we take the branch cuts to extend along the imaginary
axis as shown in Fig. B.4(b). Notice that when we do the integral in
Chapter 8 we can’t cross the cuts with our contour, so we must direct
the contour around the cuts.
Example B.7
Another occasion where we must consider a function with a branch cut is the full
propagator discussed in Chapter 31, given by
˜G(p) =
iZ
p2 −m2 + iǫ +
Z ∞
≈4m2
dM2
2π ρ(M2)
i
p2 −M2 + iǫ,
(B.24)
which has a pole structure shown in Fig. B.5. The second term in the expression tells
us to expect multiparticle contributions. These lead to a line of poles with inﬁnites-
imal separation between them. Such a line of poles is another way of describing a
branch cut and so we draw a cut extending along the real axis from the branch point
given by the two-particle production threshold (p0)2 ≈4m2.
(p0)2
m2 4m2
Fig. B.5 The pole structure of the full
propagator in eqn. B.24.
B.6
The principal value of an integral
The Cauchy principle value is a method of giving improper integrals
a value.
This is not complex analysis, but is often used in integrals
involving the propagator.
Suppose we want to evaluate the integral
R c
a dxf(x) but we have the problem that the integrand f(x) diverges at
x = b (where a < b < c) and so both
R b
a dx f(x) and
R c
b dx f(x) will also
blow up.3 In that case we may take the Cauchy principal value of the
3Speciﬁcally we require
Z b
a
dx f(x) = ±∞,
for a < b and
Z c
b
dx f(x) = ∓∞,
for c > b. (That is, one sign in front of
the ∞is plus and one minus.)
integral, denoted by P and deﬁned by
P
Z c
a
dx f(x) = lim
ǫ→0+
"Z b−ǫ
a
dx f(x) +
Z c
b+ǫ
dx f(x)
#
.
(B.25)
This gives the integral an unambiguous value, as demonstrated in the
example below.
Example B.8
The integral
I =
Z 10
0
dx
x −2 ,
(B.26)

B.6
The principal value of an integral
479
is not well deﬁned as the integrand diverges at x = 2. You can see that the two
integrals
R 2
0
dx
x−2 and
R 10
2
dx
x−2 both diverge, giving −∞and +∞respectively.
To get around this we integrate from 0 up to 2 −ǫ and then from 2 + ǫ up to 10,
thereby cutting out the troublesome part of the problem. We obtain
I1 =
R 2−ǫ
0
dx
x−3 = ln ǫ −ln 2,
I2 =
R 10
2+ǫ
dx
x−3 = ln 10 −ln ǫ.
(B.27)
We ﬁnd that I = I1 + I2 = ln 5, independent of ǫ. We may therefore take the limit
ǫ →0 and obtain an unambiguous result. We conclude that
P
Z 10
0
dx
x −2 = ln 5.
(B.28)
The principal value arises in quantum ﬁeld theory when we want to do
integrals of the form
R b
a dx f(x)
x+iǫ, where f(x) is a complex-valued function
and a and b are real, obeying a < 0 < b. In this case we use the following
theorem which says that
lim
ǫ→0+
Z b
a
dx f(x)
x ± iǫ = P
Z b
a
dx f(x)
x
∓iπf(0).
(B.29)
Often we take f(x −x0) = δ(x −x0) and obtain the identity,4
4Sometimes called the Dirac relation in
the physics literature.
1
x0 ± iǫ = P
x0
∓iπδ(x0),
(B.30)
as used in Exercises 22.1 and 31.3.

Index
1-part irreducible (1PI) diagram, 296
1PI self-energy, 296
abelian gauge theory, 424
abelian group, 80, 424
action, 14
action, principle of least, 14
active transformation, 80
adiabatic continuity, 282
Aharonov, Yakir, 261
Aharonov–Bohm eﬀect, 261, 271–272
amputation, 192
analytic function, 473
Anderson localization, 308, 417
Anderson, Philip, 242, 280
annihilation operator, 23, 20–23
anomalous dimension, 306
anticommutator, 33
Dirac ﬁelds, 342
fermion, 33
antiparticle, 61
antiscreening, 307
anyons, 267
asymptotic freedom, 307
axial gauge, 130
axial vector, 136
Bardeen, John, 401
BCS theory, 400, 400–406
BCS wave function, 402–403
Berezinskii, Vadim L., 309
Bethe, Hans, 364
Bhabha scattering, 359
Bogoliubov transformation
in a superconductor, 405
in a superﬂuid, 372
Bogoliubov, Nikolay, 371
bogolon, 372
in a superconductor, 405
in a superﬂuid, 374
Bohm, David, 261
Born approximation, 363
Born, Max, 194
Bose, S. N., 28
Bose–Einstein condensate, 379
boson, 32
branch cut, 477
broken symmetry, 237, 237–245
false vacuum, 463
for continuous symmetries, 240
Higgs ﬁeld, 438
in a gauge theory, 242
Lagrangian formulation, 239
non-abelian gauge theory, 430
SO(2) symmetry, 240
superconductor, 406
superﬂuid, 374
U(1) gauge theory, 242
U(1) symmetry, 242
Brout, Robert, 242
Brueckner, Keith, 385
canonical momentum, 50
canonical quantization, 98, 98–108
fermion, 341
for a complex scalar ﬁeld, 109–111
for a scalar ﬁeld, 108
Carruthers, Peter, 251
Casimir eﬀect, 105
Casimir, Hendrik, 105
Cauchy principle value, 478
Cauchy’s integral formula, 475
Cauchy’s theorem, 475
Cauchy, Augustin-Louis, 475
charge
conserved, 94
electric, 90
in electromagnetism, 52
Noether, 94
renormalization, 364
charge conjugation, 135
Chern, Shiing-Shen, 269
Chern–Simons theory, 269, 269–272, 418
chiral representation, 324
chirality, 325
Cliﬀord algebra, 323
Cliﬀord, William, 323
cluster decomposition principle, 181
coarse graining, 304
coherent states, 247, 247–253
fermion, 257
harmonic oscillator, 247
superﬂuid, 374
Coleman, Sidney, 126, 190, 244, 457
Coleman–Mermin–Wagner theorem, 244
collective excitations, 279
complex scalar ﬁeld, 55, 68
canonical quantization, 109–111
Noether current, 111
non-relativistic limit, 112
Compton wavelength, 76
Compton, Arthur, 357
condensed matter physics, 369, 369–422
conjugate momentum, 56
connected correlation function, 198, 313
conservation law
connection with invariance, 91
conserved charge, 94
continuity equation, 54, 61
continuum limit, 50
contour integration, 475
contraction, 172
contravariant vector, 4
Cooper pair, 401
Cooper, Leon, 401
correlation energy, 385
correlation function, 198
connected, 234
correlation length, 313
Coulomb gauge, 130
counterterms, 287, 287–294, 297–298
in QED, 360
covariant, 3
covariant derivative, 128
covariant vector, 4
CPT theorem, 139
creation operator, 23, 20–23, 102
critical exponents, 314, 319
critical phenomena, 313
cross-section, scattering, 193
crossing symmetry, 358
current
charged, 436
in a superconductor, 407
in a superﬂuid, 377
in electromagnetism, 54
in QED, 346
neutral, 436
Noether, see also Noether current
probability, 61
cut-oﬀ, momentum space, 286–288,
302–307
cutlines, 296
d’Alembert, Jean le Rond, 5
d’Alembertian operator, 5
Debye, Peter, 401
Derrick’s theorem, 265
determinant, 40
diﬀerential cross-section, 193

Index
481
Dirac equation, 323, 322–334
four components of, 339–340
non-relativistic limit, 332
Dirac ﬂux quantum, 411
Dirac monopole, 451
Dirac spinor, 327
Dirac string, 452
Dirac’s quantization condition, 453
Dirac, Paul, 322
dispersion
for weakly interacting bosons, 373
in superﬂuid helium, 370
massive scalar ﬁeld, 66
massless scalar ﬁeld, 65
of a metal, 397
divergence, 285
logarithmic, 286
superﬁcial degree of, 292
domains, 263
double cover, 141
dressed particle, 275
Dyson’s equation, 150
Dyson’s expansion, 170
Dyson, Freeman, 170
eﬀective coupling constant, 404
eﬀective ﬁeld theory, 294, 418
Einstein summation convention, 4
Einstein’s ﬁeld equations, 95
electromagnetic ﬁeld tensor, 54
electromagnetism, 7, 52, 52–57, 97, 125,
129
Chern–Simons, 272
gauge theory, 129
massive, 120–123
electroweak theory, 433, 433–443
elementary excitations, 279, 418
emergence, 418
energy-momentum tensor, 95, 97
Englert, Francois, 242
equal-time commutator, 99
Euclid, 229
Euclidean space, 229, 458
Euler, Leonhard, 15
Euler–Lagrange equation, 15, 91, 93
four-vector version, 16
exchange symmetry, 28
fate of the false vacuum, 457
Fermat’s principle, 10, 17
Fermat, Pierre de, 10
Fermi liquid theory, 281
Fermi’s weak interaction theory, 293
Fermi, Enrico, 28
fermion, 32, 321–348
canonical quantization, 341
equation of motion, 322
Feynman rules, 345
generating functional, 343
Majorana, 444–450
Noether current, 341
non-relativistic, 332, 380
origin of mass, 439
path integral, 343
propagator, 343
scattering, 345
ferromagnet, 237–238
renormalization group analysis,
313–320
Feynman diagram, 175
amputated, 192
connected, 181, 192
disconnected, 181
double oyster, 382
double tadpole, 382
dumbbell, 208
external line, 181
general rules for drawing, 180
internal line, 181
link with generating functional, 206
link with Green’s functions, 204–206
oyster, 190, 387
pair-bubble, 393
reason for drawing, 182
Saturn, 183, 288
symmetry factors, 182–183
tadpole, 190, 387
two-particle scattering, 161
vacuum diagram, 181
Feynman propagator, 156, 156–158, 173
Feynman rules
fermion, 345
metal, 390
φ4 theory, 182, 185
ψ†ψφ theory, 191
QED, 350
renormalized φ4 theory, 289
statistical ﬁeld theory, 235
Feynman, Richard, 61, 175
ﬁeld, 2, 76–77
complex scalar, 68, 109
massive scalar, 65–67
massive vector, 120–123
massless scalar, 64–65
quantum, 19
ﬁeld operator, 37, 37–38, 98
ﬁlling factor, 416
ﬁrst quantization, 19
ﬁxed point, 305
ﬂow equation, 304
Fock space, 39
Fock, Vladimir, 381
four-point function, 162
four-vector, 3
four-vector inner product, 5
Fourier transform, 6–7, 25
Fourier, Joseph, 6
Fr¨olich, Herbert, 400
fractional quantum Hall eﬀect, 417,
417–421
fractional statistics, 267, 271, 420
free propagator, 157
Friedel oscillations, 396
function, 11
functional, 11, 11–13
functional diﬀerentiation, 12
functional integral, 212, see also path
integral, 221–226
ﬁeld version, 221
scalar ﬁeld, 223
fundamental group, 261
g-factor
g = 2 from Dirac equation, 333
QED correction, 365–368
γ matrices, 322–325
Majorana basis, 445
gauge, 126
axial, 130
common gauges, 130
Coulomb, 130
Lorenz, 130
unitary, 430
Weyl, 130
gauge ﬁeld, 128
SO(3), 429
SU(2), 428
SU(2) ⊗U(1), 434
U(1), 128
gauge invariance, 126, 129
QED, 351
Ward identity, 351
gauge principle, 131
gauge theory, 128, 126–134
U(1), 346
abelian, 424
fermion, 346
in a superconductor, 407
non-abelian, 424–431
SO(3), 430, 453
SU(2), 425–428
SU(2) ⊗U(1), 434
U(1), 351, 407, 424
gauge transformation, 126
Gaussian integral, 213
Gaussian model, 316
Gell-Mann, Murray, 204
Gell-Mann–Low equation, 304
Gell-Mann–Low theorem, 204
Gell-Mann–Nishijima relation, 434
general linear group, 142
generating function, 196
generating functional, 198, 201, 201–207
fermion, 343
from functional integral, 222–226
in statistical ﬁeld theory, 234
link with Feynman diagrams, 206
scalar ﬁeld theory, 223
generator, 81
Gibbs distribution, 196

482
Index
Ginzburg, Vitaly, 315
Glauber, Roy, 247
global symmetry, 90
global transformation, 127
Goldstone boson, 242
Goldstone’s theorem, 242, 246
Goldstone, Jeﬀrey, 242
Gordon identity, 335, 366
Gordon, Walter, 60
Grassmann numbers, 255
Grassmann, Hermann, 255
Green’s function
two-point, 162
Green’s function, 144, 144–153
from Feynman diagrams, 204–206
from generating functional, 201–203
link with S-matrix, 203–204
Green, George, 144
group, 80
abelian, 80
cyclic, 136
Lie, 80
representation of, 84
table of groups, 142
guiding centre, 413
Guralnik, Gerald, 242
Hagan, Carl, 242
Hamilton’s equations, 51
Hamilton’s principle of least action, 14
Hamilton, W. R., 11
Hamiltonian, 51, 50–59
BCS, 401
electromagnetism, 57
for a metal, 380
for a superﬂuid, 370
Hubbard, 47
masses coupled by springs, 25
particles a magnetic ﬁeld, 412
simple harmonic oscillator, 20
tight-binding, 43–44
Hartree, Douglas, 381
Hartree–Fock approximation, 383–388
Heaviside, Oliver, 7
Heaviside–Lorentz units, 7
hedgehog solution, 454
Heisenberg equation of motion, 74
Heisenberg picture, 74
Heisenberg, Werner, 74
helicity, 326
Hermite polynomial, 20
Higgs ﬁeld, 437
Higgs mechanism, 242, 430–431
in a superconductor, 407
in electroweak theory, 438
Higgs, Peter, 242
Hilbert space, 39
Hofstadter butterﬂy, 413
hole, 388, 449
Hubbard model, 46
Hubbard, John, 46
imaginary time, 228
improper rotation, 139
incoming line, 181
incoming wave, 59
incompressible liquid, 417
infrared behaviour, 304
insect, giant, 79
instanton, 460, 457–466
in quantum ﬁeld theory, 463
in quantum particle mechanics, 458
integer quantum Hall eﬀect, 415–417
integration measure, 212
interacting theory, 108, 154, 162
interaction, 165
Coulomb, 176, 380
electron–phonon, 400
electron-electron, 380
electron-phonon, 400
electroweak, 441
φ4 theory, 176
ψ†ψφ theory, 176
interaction representation, 167, 167–168
internal degree of freedom, 67
internal line, 181
internal symmetry, 111, 117, 117–120
invariance
connection with conservation laws, 91
invariant, 81, 90
invariant amplitude, 192
inversion, 136
irrelevant variable, 316
isospin, 117, 426
weak, 435
jellium, 384
Jordan’s lemma, 476
Kibble, Tom, 242
kink, 262
Klein, Oskar, 60
Klein–Gordon equation, 60, 59–63
Kosterlitz, J. Michael, 309
Kosterlitz–Thouless transition, 309
Kramers doublet, 138
Kramers’ theorem, 138
Kramers, Hendrik, 138
Kronecker delta, 5
Kronecker, Leopold, 5
ladder operator, 20–23
Lagrange, J. L., 11, 64
Lagrangian, 14, 10–18, 50–59, 64–69,
92–94, 99
charged superﬂuid, 407
complex scalar ﬁeld, 68
electromagnetism, 125
massive scalar ﬁeld, 65
massless scalar ﬁeld, 64
φ4 theory, 67
ψ†ψφ theory, 188
QED, 347
renormalized, 291
SO(3) gauge theory, 430, 454
SU(2) gauge theory, 428
superﬂuid, 375
Weinberg–Salam model, 438
Lagrangian density, 15, 64–69
Lamb shift, 364
Lamb, Willis, 364
Landau Fermi liquid, 280
Landau free energy, 237
Landau levels, 414
Landau parameters, 283
Landau, Lev, 281
Landau–Ginzburg model, 315
laser, 252
Laughlin, Robert, 370, 418
least action, principle of, 14
left-handed particle, 325
Levi-Civita symbol, 6
Levi-Civita, Tullio, 6
Lie algebra, 84
Lie group, 80
Lie, Sophus, 80
light cone, 75, 101
Lindhard function, 395
linked-cluster theorem, 206
logarithmically divergent, 286
London, Fritz, 400
Lorentz covariant, 3
Lorentz group, 88, 140, 142
Lorentz invariance, 90
Lorentz invariant, 3
Lorentz transformation, 3, 86–88
Lorentz, Hendrik, 7, 130
Lorentz-invariant measure, 102
Lorenz gauge, 130
Lorenz Ludvig, 130
Low, Francis, 204
lowering operator, 22, 20–23
macroscopic wave function, 253
magnetic ﬂux quantization, 265
magnetic length, 411
magnetic monopole, 451, 451–456
Dirac, 451
’t Hooft–Polyakov, 453
Majorana condition, 445
Majorana equation, 448
Majorana fermion, 444–450
Majorana, Ettore, 444
Mandelbrot, Benoit B., 413
Mandelstam variable, 191
Mandelstam, Stanley, 191
manifold, 269
mass
in scalar ﬁeld theory, 65
Majorana, 447

Index
483
origin of electron, 439
physical, 290, 297
mass shell condition, 101
massive electromagnetism, 120, see also
massive vector ﬁeld
massive scalar ﬁeld, 65–67
massive vector ﬁeld, 120–123
propagator, 225–226
massless scalar ﬁeld, 64–65
Matsubara frequencies, 232
Matsubara, Takeo, 232
Maxwell’s equations, 7, 57
including magnetic monopoles, 451
Maxwell, James Clerk, 7
McWhirter, Norris, 90
mean-ﬁeld theory, 237, see also Landau
theory, 315, 380
excitations, 386
metal, 380
Meissner eﬀect, 408
Mermin, N. David, 244
metal, 279, 380–398
eﬀective potential, 394
excitations, 386, 389
Feynman rules, 390
permittivity, 394
random phase approximation, 393
screening, 393–396
metric tensor, 5
Mills, Robert, 425
minimal coupling prescription, 131
in QED, 346
mode expansion, 101, 106–108
momentum density, 92, 99
monopole, magnetic, 451
Mott’s formula, 357
Mott, Nevill, 357
Møller scattering, 359
n-particle operator, 39
Nambu spinor, 409
Nambu’s analogy, 410
Nambu, Yoichiro, 242
negative energy states, 61–63
neutrino, 433–443, 449
Nishijima, Kazuhiko, 434
Noether charge, 94, 111, 125
Noether current, 93, 111
fermion, 341
SO(3) internal symmetry, 119
SU(2) symmetry, 426
superﬂuid, 377
U(1) symmetry, 112, 115
Noether’s theorem, 93, 92–97, 125
Noether, Emmy, 90
non-abelian gauge theory, 424, 424–431
broken symmetry, 430
non-abelian group, 424
non-interacting theory, 108, 154
non-relativistic limit
complex scalar ﬁeld, 112
Dirac equation, 332
normal ordering, 44, 99, 104, 104–105,
169, 171
number current, 112
number density operator, 42
number operator, 21, 105
number, I am not a, 28
number-phase uncertainty, 115, 251
O(1, 3), 140, 142
O(3), 139
O(n), 142
occupation number representation, 24,
30, 28–37
operator
annihilation, 20–23
anticommuting, 255
charge conjugation, 135
chirality, 325
creation, 20–23, 102
ﬁeld, 37–38, 99
helicity, 326
ladder, 20–23
lowering, 20–23
magnetic translation, 411
number density, 42
parity, 136, 339
projection, 123
raising, 20–23
representation of, 84
rotation, 79
single-particle, 39
time-evolution, 72–74
time-reversal, 137
translation, 79
two-particle, 44
unitary, 73, 80
operator valued ﬁeld, 99
order parameter, 238
orthogonal group, 142
oyster diagram, 190
parity, 136, 339
particle exchange, 267–268
particle physics, 423, 423–466
partition function, 196, 231
passive transformation, 80
path integral, see also functional
integral, 210–219
fermion, 257, 343
Pauli equation, 332
Pauli exclusion principle, 28, 33
Pauli, Wolfgang, 33
penetration depth, 409
periodic boundary conditions, 25
in statistical ﬁeld theory, 231
particle in a box, 28
permanent, 40
phase operator, 250
phase transition, 237
renormalization group treatment, 313
φ4 theory
ferromagnetic transition, 315–319
Feynman diagrams, 177
Feynman rules, 182, 185
Lagrangian, 67
S-operator, 177
thermodynamics, 232–236
phonon, 26, 25–27
photon, 27, 133
in electroweak theory, 440
propagator, 348
renormalized propagator, 361
physical coupling constant, 291
physical mass, 277, 291
Pines, David, 393
plaquette, 413
plasma oscillation, 397
Poincar´e group, 88, 89
Poisson bracket, 51
Poisson distribution, 248
Poisson, Sim´eon, 51
polar vector, 136
polarization process, 361, 393
polarization vector, 122
pole, 474
Polyakov, Alexander, 453
principal value, 478
principle of least time, 10
probability current, 61
Proca equation, 121
Proca, Alexandru, 121
product space, 260
projection matrices, 124
propagator, 144
Euclidean, 230
fermion, 343
Feynman, 156–158, 173
ﬁeld, 154–155
four-point, 162
free, 150, 157
full, 150
in the complex plane, 476
massive vector ﬁeld, 225–226
metal, 390
n-point, 162
non-relativistic fermions, 389
notation for, 156
photon, 348
renormalized, 277, 295–298
renormalized photon, 361
scalar ﬁeld theory, 158–159
simple harmonic oscillator, 217–219
single-particle, 162
two-point, 162
proper rotation, 139
proper, orthochronous Lorentz subgroup,
140
pseudoscalar, 136

484
Index
pseudovector, 136
ψ†ψφ theory, 188
Feynman diagrams, 188
Feynman rules, 191
Lagrangian, 188
scattering, 190
pure gauge, 244, 265
QED, 346–353
Feynman rules, 350
gauge invariance, 351
renormalization, 360–368
scattering, 349, 355–359
quantization
canonical, 98–108
ﬁrst, 19
second, 19–49
quantum charge, 135
quantum ﬁeld, 19
mode expansion, 106–108
transformation of, 83, 85–86
quantum ﬁeld theory, 1
quantum ﬂuctuations, 213
quantum Hall eﬀect
fractional, 417–421
integer, 415–417
quantum statistics, 28
quantum tunnelling, 457–466
quasiparticle, 276–280
in a fractional quantum Hall ﬂuid, 420
in a metal, 279–280
Landau, 281–284
quasiparticle weight, 277
raising operator, 22, 20–23
random phase approximation, 393
rapidity, 87
reduced free energy, 314
relativity
general, 95
special, 3–6
relevant variable, 316
renormalizable theories, 288
renormalization, 276, 273–320
electric charge, 364
physical coupling constant, 291
physical mass, 291, 295
propagator, 295–298
QED, 360–368
vertex, 298–299
renormalization group, 302, 302–312
Anderson Localization, 308
asymptotic freedom, 307
for phase transitions, 313–320
Kosterlitz–Thouless transition, 309
QED, 364
renormalization group ﬂow, 304
renormalized ﬁelds, 290
representation
Heisenberg, 74
interaction, 167–168
occupation number, 28–37
Schr¨odinger, 72–73
representation of a group, 84
residue, 474
residue theorem, 476
resonance, 277
response function, 199
Ricci scalar, 95
Ricci tensor, 95
right-handed particle, 325
rigidity, 310
rotation, 82–83, 91
improper, 139
proper, 139
Rutherford, Ernest, 356
s-channel process, 191
S-matrix, 166, 165–187, 192
link with Green’s functions, 203–204
S1 (circle), 141
S2 (2-sphere), 141
S3 (3-sphere), 141
Sahl, Ibn, 10
Salam, Abdus, 433
Saturn diagram, 183
scalar, 3
scalar ﬁeld, 55
canonical quantization, 108
complex, 68
free propagator, 158–159
generating functional, 223
massive, 65–67
massless, 64–65
scattering, 186
Bhabha, 359
Compton, 357
fermion, 345
Mott formula, 356
Møller, 359
φ4 theory, 186
prototypical experiment, 166
ψ†ψφ theory, 190
QED, 349, 355–359
Rutherford, 355
theory, 188–195
scattering cross-section, 193
Schrieﬀer, J. Robert, 401
Schr¨odinger picture, 72
Schr¨odinger, Erwin, 72
Schwinger, Julian, 175, 360
screening, 363, 429
strong interaction, 307
second quantization, 19, 19–49
of operators, 39–42
second quantization trick, 25
Seitz, Frederick, 385
self-energy, 295, 296
metal, 391
self-energy diagrams, 183
Simons, James Harris, 269
simple harmonic oscillator, 19–28
path integral treatment, 217–219
simply connected topological space, 140
single-particle density matrix, 48
single-particle operator, 39
single-particle quantum mechanics
death of, 76
singularity, 474
SL(2, C), 141
Slater determinant, 36
Snell’s law, 10, 17
Snellius, Willebrord, 10
SO(1, 3), 142
SO(3), 117, 139–141
SO(n), 142
SO+(1, 3), 140
source current, 66, 201, 222
source term, 197
special linear group, 142
special Lorentz group, 142
special orthogonal group, 139, 142
special relativity, 3–6
special unitary group, 141, 142
spectral density function, 278
spermion, 190
spin, 328
spin splitting, 415
spinor, 327, 327–330
boosts, 337
Dirac, 327
rotations, 337
Weyl, 327
spinor ﬁeld, 336
stationary phase, 17
stationary phase approximation, 458
statistical ﬁeld theory, 228–236
statistical mechanics, 196–199
string, waves on a, 15
strong interaction, 307, 429
structure constant, 84
Stueckelberg, Ernst, 61
stupidity energy, 385
SU(2), 141
SU(n), 142
sum over histories, 211
superconductor, 252, 400–409, 449
BCS theory, 400–406
broken symmetry, 406
current, 408
energy gap, 403–405
excitations, 405
ground state, 402–405
Higgs mechanism, 407
superﬂuid, 252, 370, 370–379
broken symmetry, 374
charged, 407
excitations, 372, 377
Landau’s argument, 379
Noether current, 377

Index
485
supersymmetry, 449
susceptibility, 199
symmetry, 90–97
exchange, 28
internal, 111
symmetry factors, 182–183
t-channel process, 191
T-matrix, 192
tadpole diagram, 190
tensor, 6
Thomas-Fermi wave vector, 396
’t Hooft, Gerard, 453
’t Hooft–Polyakov monopole, 453
Thouless, David, 308
tight-binding model, 43–44
time reversal, 137
time-advanced Green’s function, 146
time-evolution operator, 73, 72–74
time-ordered product, 169
time-retarded Green’s function, 146
time-slicing, 211
Tomonaga, Sin-Itiro, 348
topological charge, 264
topological ﬁeld theory, 267
topological object, 260, 453, 457
topological space, 140
topology, 260
transformation
continuous, 91
discrete, 135
transition matrix, 192
translation
spacetime, 79–82
spatial, 90
time, 72–74, 91
two-point function, 162
two-point Green’s function, 162
U(1) symmetry, 111
U(n), 142
u-channel process, 191
ultraviolet behaviour, 304
unitary gauge, 430, 439
unitary group, 142
unitary matrix, 80
unitary operator, 80
universality, 313
vacuum diagram, 181
vacuum expectation value (VEV), 171,
381
vacuum polarization, 363
vector, 3
vector ﬁeld, 55
vertex
in a Feynman diagram, 181
renormalization, 298–299
renormalization in QED, 365
vertex function, 295, 298
virtual particle, 160
vortex, 264
unbinding transition, 309
W ± particle, 330, 440
Wagner, Herbert, 244
Ward identity, 351
Ward, John, 351
weak hypercharge, 435
weak interaction, 330, 433–443
weak isospin, 435
Weinberg angle, 441
Weinberg, Steven, 433
Weinberg–Salam model, 433–443
Weyl gauge, 130
Weyl representation, 324
Weyl spinor, 327
Weyl, Hermann, 327
Wheeler, John, 166
Wick rotation, 229, 458
Wick time-ordering symbol, 156
Wick’s theorem, 172, 171–173, 184
Wick, Gian-Carlo, 156
Widom’s hypothesis, 314
Wigner, Eugene, 385
Wilczek, Frank, 268
Wilson, Kenneth, 302
winding number, 261, 264
Witten, Edward, 269
world, end of the, 466
Yang, Chen-Ning, 425
Yang–Mills theory, 425, 425–428
Yukawa interaction, 188
Yukawa particle exchange, 160
Yukawa potential, 194
in a metal, 396
Yukawa theory, 194, see also ψ†ψφ
theory
Yukawa, Hideki, 159
Z0 particle, 440
Z2, 136
zero-point energy, 20


