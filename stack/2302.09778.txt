Composer: Creative and Controllable Image Synthesis with
Composable Conditions
Lianghua Huang 1 Di Chen 1 Yu Liu 1 Yujun Shen 2 Deli Zhao 1 Jingren Zhou 1
Abstract
Recent large-scale generative models learned on
big data are capable of synthesizing incredible
images yet suffer from limited controllability.
This work offers a new generation paradigm
that allows ﬂexible control of the output im-
age, such as spatial layout and palette, while
maintaining the synthesis quality and model
creativity. With compositionality as the core idea,
we ﬁrst decompose an image into representative
factors, and then train a diffusion model with
all these factors as the conditions to recompose
the input.
At the inference stage, the rich
intermediate representations work as composable
elements, leading to a huge design space (i.e.,
exponentially proportional to the number of
decomposed factors) for customizable content
creation.
It is noteworthy that our approach,
which we call Composer, supports various levels
of conditions, such as text description as the
global information, depth map and sketch as
the local guidance, color histogram for low-level
details, etc. Besides improving controllability,
we conﬁrm that Composer serves as a general
framework and facilitates a wide range of classical
generative tasks without retraining. Code and
models will be made available.
1. Introduction
“The inﬁnite use of ﬁnite means.”
– Noam Chomsky (Chomsky, 1965)
Generative image models conditioned on text can now
produce photorealistic and diverse images (Ramesh et al.,
1Alibaba Group 2Ant Group. Emails: Lianghua Huang, Di
Chen, Yu Liu <xuangen.hlh, guangpan.cd, ly103369@alibaba-
inc.com>, Yujun Shen <shenyujun0302@gmail.com>, Deli Zhao
<zhaodeli@gmail.com>, Jingren Zhou <jingren.zhou@alibaba-
inc.com>.
2022; Saharia et al., 2022; Rombach et al., 2021; Yu
et al., 2022; Chang et al., 2023).
To further achieve
customized generation, many recent works extend the text-
to-image models by introducing conditions such as segmen-
tation maps (Rombach et al., 2021; Wang et al., 2022b;
Couairon et al., 2022), scene graphs (Yang et al., 2022),
sketches (Voynov et al., 2022), depthmaps (stability.ai,
2022), and inpainting masks (Xie et al., 2022; Wang et al.,
2022a), or by ﬁnetuning the pretrained models on a few
subject-speciﬁc data (Gal et al., 2022; Mokady et al., 2022;
Ruiz et al., 2022). Nevertheless, these models still provide
only a limited degree of controllability for designers when
it comes to using them for practical applications.
For
example, generative models often struggle to accurately
produce images with speciﬁcations for semantics, shape,
style, and color all at once, which is common in real-world
design projects.
We argue that the key to controllable image generation
relies not only on conditioning, but even more signiﬁcantly
on compositionality (Lake et al., 2017). The latter can
exponentially expand the control space by introducing an
enormous number of potential combinations (e.g., a hundred
images with eight representations each yield about 1008
combinations). Similar concepts are explored in the ﬁelds
of language and scene understanding (Keysers et al., 2019;
Johnson et al., 2016), where the compositionality is termed
compositional generalization, the skill of recognizing or
generating a potentially inﬁnite number of novel combina-
tions from a limited number of known components.
In this work, we build upon the above idea and present
Composer, a realization of compositional generative models.
By compositional generative models, we refer to generative
models that are capable of seamlessly recombining visual
components to produce new images (Figure 1). Speciﬁcally,
we implement Composer as a multi-conditional diffusion
model with a UNet backbone (Nichol et al., 2021). At every
training iteration of Composer, there are two phases: in the
decomposition phase, we break down images in a batch into
individual representations using computer vision algorithms
or pretrained models; whereas in the composition phase, we
optimize Composer so that it can reconstruct these images
from their representation subsets. Despite being trained
arXiv:2302.09778v2  [cs.CV]  22 Feb 2023

Composer: Creative and Controllable Image Synthesis with Composable Conditions
3d model of
a cat
painting of
a dog
3d model of
a dog
a confused
grizzly bear
a 9 years old
kid
ancient makeup
woman
decomposition
composition
a girl with 
a pearl 
earring
style
content
intensity
palette
shape
semantics
sketch
masking
Figure 1. Concept of compositional image synthesis, which ﬁrst decomposes an image to a set of basic components and then recomposes
a new one with high creativity and controllability. To this end, the components in various formats serve as conditions in the generation
process and allow ﬂexible customization at the inference stage. Best viewed in large size.
with only a reconstruction objective, Composer is capable
of decoding novel images from unseen combinations of
representations that may come from different sources and
potentially incompatible with one another.
While conceptually simple and easy to implement, Com-
poser is surprisingly powerful, enabling encouraging perfor-
mance on both traditional and previously unexplored image
generation and manipulation tasks, including but not limited
to: text-to-image generation, multi-modal conditional image
generation, style transfer, pose transfer, image translation,
virtual try-on, interpolation and image variation from
various directions, image reconﬁguration by modifying
sketches, depth or segmentation maps, colorization based
on optional palettes, and more. Moreover, by introducing
an orthogonal representation of masking, Composer is able
to restrict the editable region to a user-speciﬁed area for
all the above operations, more ﬂexible than the traditional
inpainting operation, while also preventing modiﬁcation of
pixels outside this region. Despite being trained in a multi-
task manner, Composer achieves a zero-shot FID of 9.2 in
text-to-image synthesis on the COCO dataset (Lin et al.,
2014) when using only caption as the condition, indicating
its ability to produce high-quality results.
2. Method
Our framework comprises the decomposition phase, where
an image is divided into a set of independent components;
and the composition phase, where the components are
reassembled utilizing a conditional diffusion model. We
ﬁrst give a brief introduction to diffusion models and the
guidance directions enabled by Composer. Subsequently,
we explain the implementation of image decomposition and
composition in details.
2.1. Diffusion Models
Diffusion models (Ho et al., 2020; Dhariwal & Nichol,
2021; Song & Ermon, 2020; Song et al., 2020b; Nichol
et al., 2021) are a type of generative models that produce
data from Gaussian noise via an iterative denoising process.
Typically, a simple mean-squared error is used as the
denoising objective:
Lsimple = Ex0,c,ϵ,t(∥ϵ −ϵθ(atx0 + σtϵ, c)∥2
2),
(1)
where x0 are training data with optional conditions c,
t ∼U(0, 1), ϵ ∼N(0, I) is the additive Gaussian noise,
at, σt are scalar functions of t, and ϵθ is a diffusion model
with learnable parameters θ. Classiﬁer-free guidance is
most widely employed in recent works (Nichol et al., 2021;
Ramesh et al., 2022; Rombach et al., 2021; Saharia et al.,
2022) for conditional data sampling from a diffusion model,
where the predicted noise is adjusted via:
ˆϵθ(xt, c) = ωϵθ(xt, c) + (1 −ω)ϵθ(xt),
(2)
where xt = atx0 + σtϵ, and ω is a guidance weight.
Sampling algorithms such as DDIM (Song et al., 2020a)
and DPM-Solver (Lu et al., 2022a;b; Bao et al., 2022) are
often adopted to speed up the sampling process of diffusion
models. DDIM can also be utilized to deterministically
reverse a sample x0 back to its pure noise latent xT ,
enabling various image editing operations.
Guidance directions: Composer is a diffusion model accept-
ing multiple conditions, which enables various directions
with classiﬁer-free guidance:
ˆϵθ(xt, c) = ωϵθ(xt, c2) + (1 −ω)ϵθ(xt, c1),
(3)
where c1 and c2 are two sets of conditions. Different choices
of c1 and c2 represent different emphasis on conditions.

Composer: Creative and Controllable Image Synthesis with Composable Conditions
(b) Image interpolations.
(a) Image variations.
Figure 2. (a) Image variation. For each example, the ﬁrst column shows the source image, while the subsequent four columns are
variations of the source image produced by conditioning Composer on different subsets of its representations. (b) Image interpolation.
On the ﬁrst row are the results of interpolating all the components between the source image (ﬁrst column) and the target image (last
column). The remaining rows stand for the results where some components (i.e., listed on the left) of the source image are kept unchanged.
Conditions within (c2 \ c1) are emphasized with a guidance
weight of ω, those within (c1 \ c2) are suppressed with a
guidance weight of (1 −ω), and conditions within c1 ∩c2
are given a guidance weight of 1.0.
Bidirectional guidance: By reversing an image x0 to its
latent xT using condition c1, and then sampling from xT
using another condition c2, we are able to manipulate the
image in a disentangled manner using Composer, where the
manipulation direction is deﬁned by the difference between
c2 and c1. Similar scheme is also used in (Wallace et al.,
2022). We use this approach in Section 3.2 and Section 3.3.
2.2. Decomposition
We decompose an image into decoupled representations
which capture various aspects of it. We describe eight
representations we use in this work, where all of them are
extracted on-the-ﬂy during training.
Caption: We directly use title or description information
in image-text training data (e.g., LAION-5B (Schuhmann
et al., 2022)) as image captions. One can also leverage
pretrained image captioning models when annotations are
not available.
We represent these captions using their
sentence and word embeddings extracted by the pretrained
CLIP ViT-L/14@336px (Radford et al., 2021) model.
Semantics and style: We use the image embedding extracted
by the pretrained CLIP ViT-L/14@336px (Radford et al.,
2021) model to represent the semantics and style of an
image, similar to unCLIP (Ramesh et al., 2022).
Color: We represent the color statistics of an image using
the smoothed CIELab histogram (Sergeyk, 2016).
We
quantize the CIELab color space to 11 hue values, 5
saturation values, and 5 light values, and we use a smoothing
sigma of 10. We empirically ﬁnd these settings to work well.
Sketch: We apply an edge detection model (Su et al., 2021)
followed by a sketch simpliﬁcation algorithm (Simo-Serra
et al., 2017) to extract the sketch of an image. Sketches
capture local details of images and have less semantics.
Instances: We apply instance segmentation on an image
using the pretrained YOLOv5 (Jocher, 2020) model to
extract its instance masks. Instance segmentation masks
reﬂect the category and shape information of visual objects.

Composer: Creative and Controllable Image Synthesis with Composable Conditions
Figure 3. Image reconﬁguration. Composer supports reconﬁguring an image simply by altering its representations, such as sketch and
segmentation map.
Depthmap: We use a pretrained monocular depth estimation
model (Ranftl et al., 2022) to extract the depthmap of an
image, which roughly captures the image’s layout.
Intensity: We introduce raw grayscale images as a represen-
tation to force the model to learn a disentangled degree of
freedom for manipulating colors. To introduce randomness,
we uniformly sample from a set of predeﬁned RGB channel
weights to create grayscale images.
Masking: We introduce image masks to enable Composer
to restrict image generation or manipulation to an editable
region. We use a 4-channel representation, where the ﬁrst 3
channels correspond to the masked RGB image, while the
last channel corresponds to the binary mask.
It should be noted that, while this work experiments with
eight conditions described above, users are allowed to freely
customize their conditions using Composer.
2.3. Composition
We use diffusion models to recompose images from a
set of representations.
Speciﬁcally, we leverage the
GLIDE (Nichol et al., 2021) architecture and modify its
conditioning modules. We explore two different mecha-
nisms to condition the model on our representations:
Global conditioning: For global representations including
CLIP sentence embeddings, image embeddings and color
palettes, we project and add them to the timestep embedding.
In addition, we project image embeddings and color palettes
into eight extra tokens and concatenate them with CLIP
word embeddings, which are then used as the context
for cross-attention in GLIDE, similar to unCLIP (Ramesh
et al., 2022). Since conditions are either additive or can be
selectively masked in cross-attention, it is straightforward
to either drop conditions during training and inference, or
to introduce new global conditions.
Localized conditioning: For localized representations in-
cluding sketches, segmentation masks, depthmaps, intensity
images, and masked images, we project them into uniform-
dimensional embeddings with the same spatial size as the
noisy latent xt using stacked convolutional layers. We then
compute the sum of these embeddings and concatenate
the result to xt before feeding it into the UNet. Since
the embeddings are additive, it is easy to accommodate
for missing conditions or to incorporate new localized
conditions.
Joint training strategy: It is essential to devise a joint
training strategy that enables the model to learn to decode
images from a variety of combinations of conditions. We
experiment with several conﬁgurations and identify a simple
yet effective conﬁguration, where we use an independent
dropout probability of 0.5 for each condition, a probability
of 0.1 for dropping all conditions, and a probability of
0.1 for retaining all conditions. We use a special dropout
probability of 0.7 for intensity images because they contain
the vast majority of information about the images and may
underweight other conditions during training.
The base diffusion model produces images of 64 × 64
resolution. To generate high-resolution images, we train
two unconditional diffusion models for upsampling to
respectively upscale images from 64 × 64 to 256 × 256
and from 256 × 256 to 1024 × 1024 resolutions.
The
architectures of the upsampling models are modiﬁed from
unCLIP (Ramesh et al., 2022), where we use more channels
in low-resolution layers and introduce self-attention blocks
to scale up the capacity. We also introduce an optional
prior model (Ramesh et al., 2022) that produces image
embeddings from captions.
We empirically ﬁnd that
the prior model is capable of improving the diversity of
generated images for certain combinations of conditions.

Composer: Creative and Controllable Image Synthesis with Composable Conditions
(b) Interpolations within the editable region.
(a) Variations within the editable region.
(c) Text-conditional generation within the editable region.
(e) Reconfigurations within the editable region.
(d) Colorizations within the editable region.
painting of a cat
3d model of a dog
photo of a confused
grizzly bear
photo of a frog
oil painting of a
burger
Nikola Tesla
terminator
panda
penguin
a fluffy baby sloth
Figure 4. Region-speciﬁc image editing. Through introducing a masked image as an additional condition, Composer manages to direct
the manipulation to the region of interest.
3. Experiments
3.1. Training Details
We train a 2B parameter base model for conditional image
generation at 64 × 64 resolution, a 1.1B parameter model
for upscaling images to 256 × 256 resolution, and a
300M parameter model for further upscaling images to
1024 × 1024 resolution. Additionally, we trained a 1B
parameter prior model for optionally projecting captions
to image embeddings. We use batch sizes of 4096, 1024,
512, and 512 for the prior, base, and two upsampling
models, respectively.
We train on a combination of
public datasets, including ImageNet21K (Russakovsky et al.,
2014), WebVision (Li et al., 2017), and a ﬁltered version of
the LAION dataset (Schuhmann et al., 2022) with around
1B images. We eliminate duplicates, low resolution images,
and images potentially contain harmful content from the
LAION dataset. For the base model, we pretrain it with 1M
steps on the full dataset using only image embeddings as the
condition, and then ﬁnetune the model on a subset of 60M
examples (excluding LAION images with aesthetic scores
below 7.0) from the original dataset for 200K steps with all
conditions enabled. The prior and upsampling models are
trained for 1M steps on the full dataset.
3.2. Image Manipulation
Variations: Using Composer, we can create new images that
are similar to a given image but vary in certain aspects by
conditioning on a speciﬁc subset of its representations. By
carefully selecting combinations of different representations,
we have a great degree of ﬂexibility to control the scope
of image variations (Figure 2a). When more conditions
are incorporated, our approach easily yields more accurate
reconstructions than unCLIP (Ramesh et al., 2022), which
is conditioned solely on image embeddings.

Composer: Creative and Controllable Image Synthesis with Composable Conditions
(b) Style transfer.
(e) Virtual try-on.
(d) Pose transfer.
(a) Palette-based colorization.
(c) Image translation.
“photograph of a zebra”
“photo of a tiger”
“photo of a bear”
“a landscape photo, sunshine, summer”
“photograph of zebras”
“a cartoon picture”
“3d rendering”
“3d rendering”
“an artwork of oil painting”
“an artwork of oil painting”
color interpolation
style interpolation
Figure 5. Reformulation of traditional image generation tasks using our Composer. Note that the model is directly applied to all tasks
without any retraining, highlighting the potential and ﬂexibility of the proposed compositional generation framework.

Composer: Creative and Controllable Image Synthesis with Composable Conditions
“A ﬂuﬀy baby sloth with
a knitted hat”
“A pencil drawing of
a cat”
“A realistic photo of
a cactus”
“A photo of a dog
wearing glasses”
“A painting of a cat”
“A 3d model of a dog”
“A blue jay holding a
basket of ﬂowers”
“A brightly colored 3d icon
of a fox”
Figure 6. Compositional image generation results produced by Composer. The conditions used to generate each image are presented
below the image. Best viewed in large size.
Interpolations: By traversing in the embedding space of
global representations between two images, we can blend
the two images for variations. Composer further gives us
precise control over which elements to interpolate between
two images and which to keep unchanged, resulting in a
multitude of interpolation directions (Figure 2b).
Reconﬁgurations: Image reconﬁguration (Sun & Wu, 2019)
refers to manipulating an image through direct modiﬁcation
of one or more of its representations. Composer offers a
variety of options for image reconﬁguration (Section 2.1).
Speciﬁcally, given an image x, we can obtain its latent xT
by applying DDIM inversion conditioned on a set of its
representations ci; we then apply DDIM sampling starting
from xT conditioned on a modiﬁed set of representations cj
to obtain a variant of the image ˆx. The variant ˆx is expected
to differ from x along the variation direction deﬁned by the
difference between cj and ci, but they are otherwise similar.
By following this process, we are able to manipulate an
image from diverse directions (Figure 3).
Editable region: By conditioning Composer on a set of
representations c along with a masked image m, it is
possible to restrict the variations within the area deﬁned by
m. Remarkably, editable region is orthogonal to all image
generation and manipulation operations, offering Composer
substantially greater ﬂexibility of image editing than mere
inpainting (Figure 4).
3.3. Reformulation of Traditional Generation Tasks
Many traditional image generation and manipulation tasks
can be reformulated using the Composer architecture.
Below we describe several examples.
Palette-based colorization:
There are two methods to
colorize an image x according to palette p using Composer:
one entails conditioning the sampling process on both the
grayscale version of x and p, while the other involves
applying a reconﬁguration (Section 2.1) on x in terms
of color palette. We ﬁnd the latter approach yields more
reasonable and diverse results and we use it in Figure 5a.
Style transfer: Composer roughly disentangles the content
and style representations, which allows us to transfer
the style of image x1 to another image x2 by simply
conditioning on the style representations of x1 and the
content representations of x2. It is also possible to control
the transfer strength by interpolating style representations
between the two images. We show examples in Figure 5b.
Image translation: Image translation refers to the task
of transforming an image to a variant with content kept

Composer: Creative and Controllable Image Synthesis with Composable Conditions
unchanged but style converted to match a target domain. We
use all available representations of an image to depict its
content, with a text description to capture the target domain.
We leverage the reconﬁguration approach described in
Section 2.1 to manipulate images (Figure 5c).
Pose transfer: The CLIP embedding of an image captures
its style and semantics, enabling Composer to modify the
pose of an object without compromising its identity. We use
the object’s segmentation map to represent its pose and the
image embedding to capture its semantics, then leverage the
reconﬁguration approach described in Section 2.1 to modify
the pose of the object (Figure 5d).
Virtual try-on: Given a garment image x1 and a body image
x2, we can ﬁrst mask the clothes in x2, and then condition
the sampling process on the masked image m2 along with
the CLIP image embedding of x1 to produce a virtual try-on
result (Figure 5e). Despite moderate quality, the results
demonstrate the possibilities of Composer to cope with
difﬁcult problems with one uniﬁed framework.
3.4. Compositional Image Generation
By conditioning Composer on a combination of visual
components from different sources, it is possible to produce
an enormous number of generation results from a limited
set of materials. Figure 6 shows some selected examples.
3.5. Text-to-Image Generation
To further assess Composer’s image generation quality, we
compare its performance with the state-of-the-art text-to-
image generation models on the COCO dataset (Lin et al.,
2014). We use sampling steps of 100, 50, and 20 for the
prior, base, and 64 × 64 to 256 × 256 upsampling models
respectively and a guidance scale of 3.0 for the prior and
base models. Despite its multi-task training, Composer
achieves an competitive FID score of 9.2 and a CLIP score of
0.28 on COCO, comparable to the best-performing models.
4. Related Work
Diffusion models (Ho et al., 2020; Nichol & Dhariwal, 2021;
Dhariwal & Nichol, 2021; Rombach et al., 2021; Nichol
et al., 2021; Ramesh et al., 2022; stability.ai, 2022; Saharia
et al., 2022) are emerging as a successful paradigm for
image generation, outperforming GANs (Xu et al., 2017;
Zhu et al., 2019; Zhang et al., 2021) and comparable to
autoregressive models (Ramesh et al., 2021; Yu et al., 2021;
Esser et al., 2020; Yu et al., 2022; Ding et al., 2021) in
terms of ﬁdelity and diversity. Our method builds on recent
hierarchical diffusion models (Ramesh et al., 2022; Saharia
et al., 2022), where one large diffusion model is used
to produce small-resolution images, while two relatively
smaller diffusion models upscale the images to higher
resolutions. However, unlike these text-to-image models,
our method supports composable conditions and exhibits
better ﬂexibility and controllability.
Many recent works extend pretrained text-to-image dif-
fusion models to achieve multi-modal or customized
generation, typically by introducing conditions such as
inpainting masks (Xie et al., 2022; Wang et al., 2022a),
sketches (Voynov et al., 2022), scene graphs (Yang et al.,
2022), keypoints (Li et al., 2023), segmentation maps (Rom-
bach et al., 2021; Wang et al., 2022b; Couairon et al.,
2022), a composition of multiple text descriptions (Liu et al.,
2022), and depthmaps (stability.ai, 2022), or by ﬁnetuning
parameters on a few subject-speciﬁc data (Gal et al., 2022;
Mokady et al., 2022; Ruiz et al., 2022). Besides, GAN-
based methods can also accept a combination of multiple
conditions to enable controllable generation (Huang et al.,
2021). Compared to these approaches, Composer merits
the compositionality across conditions, enabling a larger
control space and greater ﬂexibility in image generation and
manipulations.
5. Conclusion and Discussion
Our decomposition-composition paradigm demonstrates
that when conditions are composable rather than used
independently, the control space of generative models can
be vastly expanded. As a result, a broad range of traditional
generative tasks can be reformulated using our Composer
architecture, and previously unexplored generative abilities
are unveiled, motivating further research into a variety
of decomposition algorithms that can achieve increased
controllability. In addition, we present multiple ways to
utilize Composer for a range of image generation and
manipulation tasks based on classiﬁer-free and bidirectional
guidance, giving useful references for future research.
Although we empirically ﬁnd a simple, workable conﬁgura-
tion for joint training of multiple conditions in Section 2.3,
the strategy is not perfect, e.g., it may downweight the
single-conditional generation performance. For example,
without access to global embeddings, sketch- or depth-
based generation usually produces relatively dark images.
Another issue is that conﬂicts may exist when incompatible
conditions occur. For instance, text embeddings are often
downweighted in generated results when image and text
embeddings with different semantics are jointly used.
Previous studies (Nichol et al., 2021; Ramesh et al., 2022;
Saharia et al., 2022) highlight the potential risks associated
with image generation models, such as deceptive and harm-
ful content. Composer’s improvements in controllability
further raise this risk. We intend to thoroughly investigate
how Composer can mitigate the risk of misuse and possibly
creating a ﬁltered version before making the work public.

Composer: Creative and Controllable Image Synthesis with Composable Conditions
References
Bao, F., Li, C., Zhu, J., and Zhang, B.
Analytic-dpm:
an analytic estimate of the optimal reverse variance in
diffusion probabilistic models. ArXiv, abs/2201.06503,
2022.
Chang, H., Zhang, H., Barber, J., Maschinot, A., Lezama,
J., Jiang, L., Yang, M., Murphy, K. P., Freeman, W. T.,
Rubinstein, M., Li, Y., and Krishnan, D. Muse: Text-
to-image generation via masked generative transformers.
ArXiv, abs/2301.00704, 2023.
Chomsky, N. Aspects of the Theory of Syntax. The MIT
Press, Cambridge, 1965.
Couairon, G., Verbeek, J., Schwenk, H., and Cord, M.
Diffedit: Diffusion-based semantic image editing with
mask guidance. ArXiv, abs/2210.11427, 2022.
Dhariwal, P. and Nichol, A. Diffusion models beat GANs
on image synthesis. ArXiv, abs/2105.05233, 2021.
Ding, M., Yang, Z., Hong, W., Zheng, W., Zhou, C., Yin,
D., Lin, J., Zou, X., Shao, Z., Yang, H., and Tang,
J.
CogView: Mastering text-to-image generation via
Transformers. In Neural Information Processing Systems,
2021.
Esser, P., Rombach, R., and Ommer, B. Taming Transform-
ers for high-resolution image synthesis. 2021 IEEE/CVF
Conference on Computer Vision and Pattern Recognition
(CVPR), pp. 12868–12878, 2020.
Gal, R., Alaluf, Y., Atzmon, Y., Patashnik, O., Bermano,
A. H., Chechik, G., and Cohen-Or, D. An image is worth
one word: Personalizing text-to-image generation using
textual inversion. ArXiv, abs/2208.01618, 2022.
Ho, J., Jain, A., and Abbeel, P.
Denoising diffusion
probabilistic models. ArXiv, abs/2006.11239, 2020.
Huang, X., Mallya, A., Wang, T.-C., and Liu, M.-Y.
Multimodal conditional image synthesis with product-
of-experts GANs. In European Conference on Computer
Vision, 2021.
Jocher, G. YOLOv5 by Ultralytics, 2020. URL https:
//github.com/ultralytics/yolov5.
Johnson, J., Hariharan, B., van der Maaten, L., Fei-Fei, L.,
Zitnick, C. L., and Girshick, R. B. Clevr: A diagnostic
dataset for compositional language and elementary visual
reasoning. 2017 IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), pp. 1988–1997, 2016.
Keysers, D., Sch¨arli, N., Scales, N., Buisman, H., Furrer, D.,
Kashubin, S., Momchev, N., Sinopalnikov, D., Staﬁniak,
L., Tihon, T., Tsarkov, D., Wang, X., van Zee, M., and
Bousquet, O. Measuring compositional generalization:
A comprehensive method on realistic data.
ArXiv,
abs/1912.09713, 2019.
Lake, B. M., Ullman, T. D., Tenenbaum, J. B., and
Gershman, S. J. Building machines that learn and think
like people. Behavioral and Brain Sciences, 40, 2017.
Li, W., Wang, L., Li, W., Agustsson, E., and Gool, L. V.
Webvision database: Visual learning and understanding
from web data. ArXiv, abs/1708.02862, 2017.
Li, Y., Liu, H., Wu, Q., Mu, F., Yang, J., Gao, J., Li, C., and
Lee, Y. J. GLIGEN: Open-set grounded text-to-image
generation. ArXiv, abs/2301.07093, 2023.
Lin, T.-Y., Maire, M., Belongie, S. J., Hays, J., Perona, P.,
Ramanan, D., Doll´ar, P., and Zitnick, C. L. Microsoft
COCO: Common objects in context.
In European
Conference on Computer Vision, 2014.
Liu, N., Li, S., Du, Y., Torralba, A., and Tenenbaum,
J. B. Compositional visual generation with composable
diffusion models. ArXiv, abs/2206.01714, 2022.
Lu, C., Zhou, Y., Bao, F., Chen, J., Li, C., and Zhu,
J.
DPM-Solver:
A fast ODE solver for diffusion
probabilistic model sampling in around 10 steps. ArXiv,
abs/2206.00927, 2022a.
Lu, C., Zhou, Y., Bao, F., Chen, J., Li, C., and Zhu, J. DPM-
Solver++: Fast solver for guided sampling of diffusion
probabilistic models. ArXiv, abs/2211.01095, 2022b.
Mokady, R., Hertz, A., Aberman, K., Pritch, Y., and Cohen-
Or, D. Null-text inversion for editing real images using
guided diffusion models. ArXiv, abs/2211.09794, 2022.
Nichol, A. and Dhariwal, P. Improved denoising diffusion
probabilistic models. ArXiv, abs/2102.09672, 2021.
Nichol, A., Dhariwal, P., Ramesh, A., Shyam, P., Mishkin,
P., McGrew, B., Sutskever, I., and Chen, M. GLIDE:
Towards photorealistic image generation and editing
with text-guided diffusion models.
In International
Conference on Machine Learning, 2021.
Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G.,
Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J.,
et al. Learning transferable visual models from natural
language supervision. In International Conference on
Machine Learning, pp. 8748–8763. PMLR, 2021.
Ramesh, A., Pavlov, M., Goh, G., Gray, S., Voss, C.,
Radford, A., Chen, M., and Sutskever, I. Zero-shot text-
to-image generation. ArXiv, abs/2102.12092, 2021.

Composer: Creative and Controllable Image Synthesis with Composable Conditions
Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., and Chen,
M. Hierarchical text-conditional image generation with
clip latents. ArXiv, abs/2204.06125, 2022.
Ranftl, R., Lasinger, K., Hafner, D., Schindler, K., and
Koltun, V. Towards robust monocular depth estimation:
Mixing datasets for zero-shot cross-dataset transfer.
IEEE Transactions on Pattern Analysis and Machine
Intelligence, 44(3), 2022.
Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and
Ommer, B. High-resolution image synthesis with latent
diffusion models.
2022 IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR), pp.
10674–10685, 2021.
Ruiz, N., Li, Y., Jampani, V., Pritch, Y., Rubinstein, M., and
Aberman, K. DreamBooth: Fine tuning text-to-image
diffusion models for subject-driven generation. ArXiv,
abs/2208.12242, 2022.
Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S.,
Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein,
M. S., Berg, A. C., and Fei-Fei, L. Imagenet large scale
visual recognition challenge. International Journal of
Computer Vision, 115:211–252, 2014.
Saharia, C., Chan, W., Saxena, S., Li, L., Whang, J.,
Denton, E. L., Ghasemipour, S. K. S., Ayan, B. K.,
Mahdavi, S. S., Lopes, R. G., Salimans, T., Ho, J., Fleet,
D. J., and Norouzi, M.
Photorealistic text-to-image
diffusion models with deep language understanding.
ArXiv, abs/2205.11487, 2022.
Schuhmann, C., Beaumont, R., Vencu, R., Gordon, C.,
Wightman, R., Cherti, M., Coombes, T., Katta, A., Mullis,
C., Wortsman, M., Schramowski, P., Kundurthy, S.,
Crowson, K., Schmidt, L., Kaczmarczyk, R., and Jitsev,
J. Laion-5b: An open large-scale dataset for training next
generation image-text models. ArXiv, abs/2210.08402,
2022.
Sergeyk. Rayleigh: Search image collections by multiple
color palettes or by image color similarity., 2016. URL
https://github.com/sergeyk/rayleigh.
Simo-Serra, E., Iizuka, S., and Ishikawa, H. Mastering
sketching: Adversarial augmentation for structured pre-
diction. arXiv: Computer Vision and Pattern Recognition,
2017.
Song, J., Meng, C., and Ermon, S. Denoising diffusion
implicit models. ArXiv, abs/2010.02502, 2020a.
Song, Y. and Ermon, S. Improved techniques for training
score-based generative models. ArXiv, abs/2006.09011,
2020.
Song, Y., Sohl-Dickstein, J. N., Kingma, D. P., Kumar,
A., Ermon, S., and Poole, B. Score-based generative
modeling through stochastic differential equations. ArXiv,
abs/2011.13456, 2020b.
stability.ai.
Stable
diffusion
2.0
release.,
2022.
URL
https://stability.ai/blog/
stable-diffusion-v2-release.
Su, Z., Liu, W., Yu, Z., Hu, D., Liao, Q., Tian, Q.,
Pietik¨ainen, M., and Liu, L. Pixel difference networks for
efﬁcient edge detection. 2021 IEEE/CVF International
Conference on Computer Vision (ICCV), pp. 5097–5107,
2021.
Sun, W. and Wu, T. Image synthesis from reconﬁgurable
layout and style. 2019 IEEE/CVF International Con-
ference on Computer Vision (ICCV), pp. 10530–10539,
2019.
Voynov, A., Aberman, K., and Cohen-Or, D. Sketch-guided
text-to-image diffusion models. ArXiv, abs/2211.13752,
2022.
Wallace, B., Gokul, A., and Naik, N.
EDICT: Exact
diffusion inversion via coupled transformations. ArXiv,
abs/2211.12446, 2022.
Wang, S., Saharia, C., Montgomery, C., Pont-Tuset, J., Noy,
S., Pellegrini, S., Onoe, Y., Laszlo, S., Fleet, D. J., Soricut,
R., Baldridge, J., Norouzi, M., Anderson, P., and Chan, W.
Imagen editor and EditBench: Advancing and evaluating
text-guided image inpainting. ArXiv, abs/2212.06909,
2022a.
Wang, T., Zhang, T., Zhang, B., Ouyang, H., Chen, D., Chen,
Q., and Wen, F. Pretraining is all you need for image-to-
image translation. ArXiv, abs/2205.12952, 2022b.
Xie, S., Zhang, Z., Lin, Z., Hinz, T., and Zhang, K.
SmartBrush: Text and shape guided object inpainting
with diffusion model. ArXiv, abs/2212.05034, 2022.
Xu, T., Zhang, P., Huang, Q., Zhang, H., Gan, Z.,
Huang, X., and He, X. AttnGAN: Fine-grained text to
image generation with attentional generative adversarial
networks. 2018 IEEE/CVF Conference on Computer
Vision and Pattern Recognition, pp. 1316–1324, 2017.
Yang, L., Huang, Z., Song, Y., Hong, S., Li, G., Zhang, W.,
Cui, B., Ghanem, B., and Yang, M.-H. Diffusion-based
scene graph to image generation with masked contrastive
pre-training. ArXiv, abs/2211.11138, 2022.
Yu, J., Li, X., Koh, J. Y., Zhang, H., Pang, R., Qin, J.,
Ku, A., Xu, Y., Baldridge, J., and Wu, Y.
Vector-
quantized image modeling with improved VQGAN.
ArXiv, abs/2110.04627, 2021.

Composer: Creative and Controllable Image Synthesis with Composable Conditions
Yu, J., Xu, Y., Koh, J. Y., Luong, T., Baid, G., Wang, Z.,
Vasudevan, V., Ku, A., Yang, Y., Ayan, B. K., Hutchinson,
B. C., Han, W., Parekh, Z., Li, X., Zhang, H., Baldridge,
J., and Wu, Y. Scaling autoregressive models for content-
rich text-to-image generation. ArXiv, abs/2206.10789,
2022.
Zhang, H., Koh, J. Y., Baldridge, J., Lee, H., and Yang,
Y. Cross-modal contrastive learning for text-to-image
generation. 2021 IEEE/CVF Conference on Computer
Vision and Pattern Recognition (CVPR), pp. 833–842,
2021.
Zhu, M., Pan, P., Chen, W., and Yang, Y.
DM-GAN:
Dynamic memory generative adversarial networks for
text-to-image synthesis. 2019 IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR), pp.
5795–5803, 2019.

Composer: Creative and Controllable Image Synthesis with Composable Conditions
A. Architecture Details
Prior
64
64 →256
256 →1024
Diffusion steps
1000
1000
1000
1000
Noise schedule
cosine
cosine
cosine
linear
Sampling steps
100
50
20
10
Sampling variance method
dpm-solver
dpm-solver
dpm-solver
dpm-solver
Model size
1B
2B
1.1B
300M
Channels
-
512
320
192
Depth
-
3
3
2
Channels multiple
-
1,2,3,4
1,2,3,5
1,1,2,2,4,4
Heads channels
-
64
64
-
Attention resolution
-
32,16,8
32,16
-
Dropout
-
0.1
0.1
-
Weight decay
6.0e-2
-
-
-
Batch size
4096
1024
512
512
Iterations
1M
1M
1M
1M
Learning rate
1.1e-4
1.2e-4
1.1e-4
1.0e-4
Adam β2
0.96
0.999
0.999
0.999
Adam ϵ
1.0e-6
1.0e-8
1.0e-8
1.0e-8
EMA decay
0.9999
0.9999
0.9999
0.9999
Table 1. Hyperparameters for Composer. We use DPM-Solver++ (Lu et al., 2022b) as the sampling algorithm for all diffusion models.

Composer: Creative and Controllable Image Synthesis with Composable Conditions
B. Conditioning Modules
Figure 7. Global conditioning module of Composer. For global conditions such as CLIP sentence embeddings, image embeddings, and
color histograms, we project and add them to the timestep embedding. Moreover, we project image embeddings and color palettes into
eight extra tokens and concatenate them with CLIP word embeddings, which are then used as the context input for cross-attention layers.
Figure 8. Local conditioning module of Composer. For local conditions such as segmentation maps, depthmaps, sketches, grayscale
images, and masked images, we project them into uniform-dimensional embeddings with the same spatial size as the noisy image using
stacked convolutional layers. Subsequently, we compute the sum of these embeddings and concatenate the result to the noisy image.

Composer: Creative and Controllable Image Synthesis with Composable Conditions
C. Additional Samples
Image embedding
Color palette
Depthmap
Sketch
Segmentation map
Output
Figure 9. This ﬁgure indicates how Composer resolves conﬂicting conditions by illustrating extreme cases in which the conditions come
from disparate sources. One of our observations is that Composer typically gives less weight to conditions with fewer details when
conﬂicts exist, such as segmentation maps in comparison to sketches and depthmaps, and text embeddings versus image embeddings.

Composer: Creative and Controllable Image Synthesis with Composable Conditions
Gray-
scale
Palette
Figure 10. Additional colorization results, visualized at a resolution of 256 × 256 to reduce ﬁle size.

Composer: Creative and Controllable Image Synthesis with Composable Conditions
Content
tyle
Figure 11. Additional style transfer results, visualized at a resolution of 256 × 256 to reduce ﬁle size.

