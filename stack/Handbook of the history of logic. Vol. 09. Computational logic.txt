
Handbook of the History 
of Logic 
Series Editors 
Dov M. Gabbay 
John Woods 


North Holland is an imprint of Elsevier 
Radarweg 29, PO Box 211, 1000 AE Amsterdam, The Netherlands 
The Boulevard, Langford lane, Kidlington, Oxford OX5 1GB, UK 
225 Wyman Street, Waltham, MA 02451, USA 
First edition 2014 
Copyright © 2014 Elsevier B.V. All rights reserved 
No part of this publication may be reproduced or transmitted in any form or by any means, 
electronic or mechanical, including photocopying, recording, or any information storage and 
retrieval system, without permission in writing from the publisher. Details on how to seek 
permission, further information about the Publisher’s permissions policies and our arrangements 
with organizations such as the Copyright Clearance Center and the Copyright Licensing Agency, 
can be found at our website: www.elsevier.com/permissions. 
This book and the individual contributions contained in it are protected under copyright by the 
Publisher (other than as may be noted herein). 
Knowledge and best practice in this field are constantly changing. As new research and 
experience broaden our understanding, changes in research methods, professional practices, or 
medical treatment may become necessary. 
Practitioners and researchers must always rely on their own experience and knowledge in 
evaluating and using any information, methods, compounds, or experiments described herein. In 
using such information or methods they should be mindful of their own safety and the safety of 
others, including parties for whom they have a professional responsibility. 
To the fullest extent of the law, neither the Publisher nor the authors, contributors, or editors, 
assume any liability for any injury and/or damage to persons or property as a matter of products 
liability, negligence or otherwise, or from any use or operation of any methods, products, 
instructions, or ideas contained in the material herein.  
ISBN: 978-0-444-51624-4 
ISSN: 1874-5857 
For information on all North Holland publications  
visit our web site at http://store.elsevier.com/ 
Notices

CONTENTS 
Editorial Note 
vii 
Jörg Siekmann and Dov Gabbay 
List of Authors and Readers 
viii 
Part I. Introduction 
Computational Logic 
15 
Jörg Siekmann 
Logic and the Development of the Computer 
31 
Martin Davis 
Part II. General 
What is a Logical System? An Evolutionary View: 1964-2014 
41 
Dov G a b b a y 
Part III. Automated Reasoning 
Interactive Theorem Proving 
135 
John Harrison, Josef Urban and Freek Wiedijk 
Automation of Higher Order Logic 
215 
Christoph Benzmüller and Dale Miller 
Equational Logics and Rewriting 
255 
Claude Kirchner and Hélène Kirchner 
Possibilistic Logic — An Overview 
283 
Didier Dubois and Henri Prade 
Computerizing Mathematical Text 
343 
Fairouz Kamareddine, Joe Wells, Christoph Zengler and 
Henk Barendregt 

vi 
Contents 
Part IV. Computer Science 
Concurrency Theory: a Historical Perspective on Coinduction 
399 
and Process Calculi 
Jos Baeten and Davide Sangiori 
Degrees of Unsolvability 
443 
Klaus Ambos-Spies and Peter A. Fejer 
Computational Complexity 
495 
Lance Fortnow and Steven Homer 
Logic Programming 
523 
Robert Kowalski 
Logic and Databases: A History of Deductive databases 
571 
Jack Minker, Dietmar Seipel and Carlo Zaniolo 
Logics for Intelligent Agents and Multi Agent Systems 
629 
John-Jules Ch. Meyer 
Description Logics 
659 
Matthias Knorr and Pascal Hitzler 
Logics for the Semantic Web 
679 
Pascal Hitzler, Jens Lehmann and Axel Polleres 
Index 
711 

EDITORIAL NOTE
J¨org Siekmann and Dov Gabbay
Because of space and time considerations, not all topics could be covered by chap-
ters in this Handbook. They will appear in a separate publication soon.

CONTRIBUTORS
AUTHORS
Klaus Ambos-Spies
Heidelberg University, Germany.
ambos@math.uni-heidelberg.de
Jos Baeten
CWI Amsterdam, The Netherlands.
Jos.Baeten@cwi.nl
Henk Barendregt
Radboud University Nijmegen, The Netherlands.
henk@cs.ru.nl
Christoph Benzm¨uller
Freie Universit¨at Berlin, Gemany.
c.benzmueller@googlemail.com
Martin Davis
New York University, USA.
martin@eipye.com
Didier Dubois
IRIT, France.
dubois@irit.fr
Peter A. Fejer
University of Massachusetts Boston, USA.
fejer@cs.umb.edu
Lance Fortnow
Georgia Institute of Technology, USA.
fortnow@cc.gatech.edu
Dov Gabbay
Bar Ilan University, Israel; King’s College London, UK; University of Luxembourg,
Luxembourg; University of Manchester, UK.
dov.gabbay@kcl.ac.uk
John R. Harrison
Intel Corporation, USA.
johnh@ichips.intel.com

Pascal Hitzler
Wright State University, USA.
pascal.hitzler@wright.edu
Steven Homer
Boston University, USA.
homer@cs.bu.edu
Fairouz Kamareddine
Herriot Watt University, UK.
f.d.kamareddine@hw.ac.uk
Matthias Knorr
Universidade Nova de Lisboa, Portugal.
mkn@fct.unl.pt
Robert Kowalski
Imperial College London, UK
rak@doc.ic.ac.uk
Claude Kirchner
Inria, France.
claude.kirchner@inria.fr
H´el`ene Kirchner
Inria, France.
helene.kirchner@inria.fr
Jens Lehmann
University of Leipzig, Germany
lehmann@informatik.uni-leipzig.de
John-Jules Meyer
Universiteit Utrecht, The Netherlands.
j.j.c.meyer@uu.nl
Dale Miller
Inria, France.
dale.miller@inria.fr
Jack Minker
University of Maryland, USA.
minker@cs.umd.edu
Henri Prade
IRIT, France.
prade@irit.fr
Davide Sangiorgi
University of Bologna, Italy.
davide.sangiorgi@cs.unibo.it
Contributors
ix

Contributors
x
Axel Polleres
Vienna University of Economics and Business, Austria.
axel@polleres.net
Dietmar Seipel
University of W¨urzburg, Germany.
dietmar.seipel@uni-wuerzburg.de
J¨org Siekmann
Saarland University, DFKI, Germany.
joerg.siekmann@dfki.de
Josef Urban
Radboud University Nijmegen, The Netherlands.
josef.urban@gmail.com
Joe Wells
Herriot Watt University, UK.
joe.wells@hw.ac.uk
Freek Wiedijk
Radboud University Nijmegen, The Netherlands.
freek@cs.ru.nl
Carlo Zaniolo
University of California, Los Angeles, USA.
zaniolo@cs.ucla.edu
Christoph Zengler
University of Tuebingen, Germany.
christoph@zengler.eu

READERS
Luca Aceto
Reykjavik University, Iceland.
luca.aceto@gmail.com
Peter B. Andrews
Carnegie Mellon University, USA.
pa01@gtps.math.cmu.edu
Serge Autexier
DFKI, Germany.
autexier@dfki.de
Franz Baader
Dresden University, Germany.
baader@tcs.inf.tu-dresden.de
Johan van Benthem
Universiteit Amsterdam, The Netherlands
j.vanBenthem@uva.nl
Jasmin Blanchette
Technical Univesity of Munich, Germany.
blanchette@in.tum.de
Maarten H. van Emden
University of Victoria, Canada.
vanemden@gmail.com
William Farmer
McMaster University, Canada.
wmfarmer@mcmaster.ca
Peter Fejer
University of Massachusetts Boston, USA.
fejer@cs.umb.edu
Herman Geuvers
Radboud University Nijmegen, The Netherlands.
h.geuvers@cs.ru.nl
Robert van Glabbeek
Nicta, Australia.
robert.vanglabbeek@nicta.com.au
Lluis Godo Lacasa
Universitat Autonoma de Varcelona, Spain.
godo@iiia.csic.es
Readers
xi

Contributors
xii
Georg Gottlob
Univesity of Oxford, UK.
gottlob@dbai.tuwien.ac.at
Patrick Hayes
Florida Institute for Human and Machine Cognition, USA.
phayes@ihmc.us
Ian Horrocks
Oxford University, UK.
ian.horrocks@comlab.ox.ac.uk
Deepak Kapur
University of New Mexico, USA.
kapur@cs.unm.edu
Kurt Mehlhorn
Max-Planck-Institut f¨ur Informatik, Germany.
mehlhorn@mpi-inf.mpg.de
Lawrence C. Paulson
Cambridge University, UK.
lp15@cam.ac.uk
Lu´ıs Moniz Pereira
University of Lisbon, Portugal.
lmp@fct.unl.pt
Richard Shore
Cornell University, USA.
shore@math.cornell.edu
J¨org Siekmann
DFKI, Germany.
Joerg.Siekmann@dfki.de
Bruno Woltzenlogel Paleo
Vienna University of Technology, Austria.
bruno@logic.at
Michael Wooldridge
Liverpool University, UK.
mjw@liverpool.ac.uk

Part I 
Introduction 


COMPUTATIONAL LOGIC
J¨org Siekmann
Computational logic was born in the twentieth century and evolved in close sym-
biosis with the ﬁrst electronic computers and the growing importance of computer
science, informatics and artiﬁcial intelligence (AI). The ﬁeld has now outgrown its
humble beginnings and early expectations by far: with more than ten thousand
people working in research and development of logic and logic-related methods,
with several dozen international conferences and several times as many workshops
addressing the growing richness and diversity of the ﬁeld, and with the founda-
tional role and importance these methods now assume in mathematics, computer
science, artiﬁcial intelligence, cognitive science, linguistics, law, mechatronics and
many other engineering ﬁelds where logic-related techniques are used inter alia to
state and settle correctness issues, the ﬁeld has diversiﬁed in ways that the pure
logicians working in the early decades of the twenties century could have hardly
anticipated - let alone those researchers of the previous centuries presented in this
eleven volume account of the history of logic.
Dating back to its roots in Greek, Indian, Chinese and Arabic philosophy the
ﬁeld has grown in richness and diversity over the centuries to ﬁnally reach the
modern methodological approach ﬁrst expressed in the work of Gottlob Frege.1
Logical calculi, which not only capture formal reasoning, but also an important
aspect of human thought, are now amenable to investigation with mathemati-
cal rigour and computational support and fertilized the early Leibniz’ dream of
mechanized reasoning: “Calculemus”. The beginning of the last century saw the
inﬂuence of these developments in the foundations of mathematics in the works of
David Hilbert and Paul Bernays, Bertrand Russell and Alfred North Whitehead2
and others, in the foundations of syntax and semantics of language, and in ana-
lytic philosophy most vividly expressed in the previous century by the logicians
and philosophers in the Vienna Circle.
The Dartmouth Conference in 1956 generally considered the birthplace of ar-
tiﬁcial intelligence — raised explicitly the hopes for the new possibilities that the
advent of electronic computing machinery oﬀered: logical statements could now be
executed on a machine with all the far-reaching consequences that ultimately led
to logic programming,3 question answering systems, deduction systems for math-
1See volume 3, “The Rise of modern Logic : from Leibnitz to Frege” in this eleven volume
handbook on the history of logic.
2See volume 4 “British Logic in the 19th Century” and volume 5 “Logic from Russell to
Church” of this handbook on the history of logic.
3See the chapter by Robert Kowalski.
Handbook of the History of Logic. Volume 9: Computational Logic. 
Volume editor: Jörg Siekmann 
Series editors: Dov M. Gabbay and John Woods 
Copyright © 2014 Elsevier BV. All rights reserved.

16
J¨org Siekmann
ematics and engineering,4 logical design and veriﬁcation of computer software and
hardware, deductive databases5 and software synthesis as well as logical techniques
for the analysis and veriﬁcation in the ﬁelds of mechanical engineering. In this way
the growing richness of foundational and purely logical investigations that had led
to such developments as:
•
ﬁrst order calculi
• type theory and higher order logic
• nonclassical logics
• semantics
• constructivism
and others, was extended by new questions and problems, in particular from com-
puter science and artiﬁcial intelligence, leading to:
• denotational semantics for programming languages
• ﬁrst- and higher-order logical calculi for automated reasoning
• non-monotonic reasoning
• logical foundations for computing machinery such as CSP, π-Calculus and
others for program veriﬁcation
• knowledge representation formalisms such as description logics
• logics for the semantic web
• logical foundations for cognitive robotics
• syntax and semantics for natural language processing
• logical foundations of databases
• linear logics, probabilistic reasoning and uncertainty management
• logical foundations and its relationship to the philosophy of mind, and many
others.
4See the chapter by John Harrison, Freek Wiedijk and Josef Urban; the chapter by Christoph
Benzm¨uller and Dale Miller; the chapter by Gilles Dowek and Herman Geuvers; the chapter by
Claude and H´el`en Kirchner and the chapter by Fairouz Kamareddine, J.B. Wells, C. Zengler and
Henk Barendregt.
5See the chapter by Jack Minker, Dietmar Seipel and Carlo Zaniolo.

Computational Logic
17
In many respects, logic provides computer science with both a unifying founda-
tional framework and a tool for modeling.6 In fact, logic has been called the cal-
culus of computer science, most prominently represented at the LICS conference
(Logic in Computer Science), playing a crucial role in diverse areas such as com-
putational complexity,7 unsolvability,8 distributed computing, concurrency,9 multi
agent systems,10 database systems, hardware design, programming languages,
knowledge representation,11 the semantic web,12 and software engineering.
As
John McCarthy succinctly coined it: “Newtonian physics is to mechanical engi-
neering as logic is to computer science”. But the demands from artiﬁcial intel-
ligence, computational linguistics and philosophy have spawned even more new
ideas and developments as we shall argue below.13
This growing diversity is reﬂected in the numerous conferences and workshops
that address particular aspects of the ﬁelds mentioned above. For example, only
forty years ago, there was just one international conference on Automated De-
duction (later to be called CADE). Today there is not only the annual CADE
but also the biannual IJCAR, the International Joint Conference on Automated
Deduction, which unites every two years CADE, FroCos (the International Sym-
posium on Frontiers of Combining Systems), FTP ( International Worshop on
First-order Theorem Proving), the TABLEAUX conference (Automated Reason-
ing with Analytic Tableaux and Related Methods) and sometimes other smaller
conferences as well.
There is also the RTA conference (Rewriting Techniques
and Applications), the LPAR (Logic Programming and Automated Reasoning),
TPHOL (Theorem Proving in Higher Order Logic), and UNIF (the Uniﬁcation
Workshop). Several conferences on mathematical theorem proving have recently
united into CICM (Conference on Intelligent Computer Mathematics), among oth-
ers CALCULEMUS, MKM (Mathematical Knowledge Management), DML (Dig-
ital Mathematical Libraries) and the OpenMath workshops. CICM has usually
half a dozen workshops colocated such as MathUI (Mathematical User Interfaces)
and ThEdu (Theorem Prover Components for Educational Software) and others.
Each of these conferences is held regularly on its own or back to back with a re-
lated conference, but with its own set of proceedings and supported by a mature
scientiﬁc community. Frequently, these conferences spawn dozens of national and
international workshops, that drive the development and represent the creative
innovation, but may not be ready yet for archival presentation. Some of these
that united the young rebels of the ﬁeld are CALCULEMUS, MKM (Mathemat-
ical Knowledge Management), CIAO the Workshop that started with induction
but soon became an event of its own, the Proof Presentation workshop, UITP
6See the chapter by Martin Davis.
7See the chapter by Lance Fortnow and Steven Homer.
8See the chapter by Klaus Ambos-Spies and Peter Fejer.
9See the chapter by Jos Baeten and Davide Sangiorgi.
10See the chapter by John-Jules Meyer
11See the chapter by Matthias Knorr and Pascal Hitzler.
12See the chapter by Pascal Hitzler, Jens Lehmann andAxel Polleres
13See the chapter by Dov Gabbay and the chapter by Didier Dubois and Henri Prade.

18
J¨org Siekmann
(User Interfaces for ATP) and JELIA (European Conference on Logics in Arti-
ﬁcial Intelligence). Furthermore there are all the conferences focussing on logic
for veriﬁcation and speciﬁcation of hardware and software like CAV (International
Conference on Computer Aided Veriﬁcation), FM (Formal Methods), WING (the
International Workshop on Invariant Generation), RR (the Web Reasoning and
Rules Systems Conference series) and dozens more.
Finally there are the related conferences and workshops in AI, like the Non-
monotonic Reasoning Conference and workshops, the Knowledge Representation
Conferences, the Frame Problem meetings, the workshops on nonclassical reason-
ing and many more including all the numerous national workshops in Europe, the
US and nowadays increasingly in China and the Paciﬁc Rim as well. Furthermore
there are dozens of highly specialized workshops at the large AI conferences such
as IJCAI, ECAI, PRICAI, and the AAAI conference.
In the summer of 2014, Vienna hosted VSL, the Vienna Summer of Logic, with
FLoC (the Federated Logic Conference) plus some more related conferences, which
up to then was the largest event so far in the history of logic. It consisted of twelve
large federated conferences and over seventy workshops attracting more than 2000
researchers from all over the world. It wass organized into three main thematic
sections: logic in computer science, mathematical logic and logic in artiﬁcial in-
telligence. While not every conference on mathematical and computational logic
was present at this event (in fact there may be twice as many), it gives, neverthe-
less, a fair account of the current breadth and scope of our subject in 2014. As a
historical snapshot its worth listing it in detail.
The ﬁrst section “Logic in Computer Science” united the following independent
conferences: the 26th International Conference on Computer Aided Veriﬁcation
(CAV), the 27th IEEE Computer Security Foundations Symposium (CSF), the
30th International Conference on Logic Programming (ICLP), the 7th Interna-
tional Joint Conference on Automated Reasoning (IJCAR) - which in itselftself
conglomerates the Conference on Automated Deduction (CADE), Theorem Prov-
ing in Higher-Order Logics (TPHOLs), and (TABLEAUX) - , furthermore the
5th Conference on Interactive Theorem Proving (ITP), the Annual Conference on
Computer Science Logic (CSL) and the 29th ACM/IEEE Symposium on Logic in
Computer Science (LICS), the 25th International Conference on Rewriting Tech-
niques and Applications (RTA) joint with the 12th International Conference on
Typed Lambda Calculi and Applications (TLCA), the 17th International Con-
ference on Theory and Applications of Satisﬁability Testing (SAT) and various
system competitions.
The second section, Mathematical Logic comprised the Logic Colloquium (LC),
the Logic, Algebra and Truth Degrees conference (LATD), the Compositional
Meaning in Logic (GeTFun 2.0), The Inﬁnity Workshop (INFINITY), the Work-
shop on Logic and Games (LG) and the Kurt G¨odel Fellowship Competition.
The third section, Logic in Artiﬁcial Intelligence consisted of the 14th Inter-
national Conference on Principles of Knowledge Representation and Reasoning
(KR), the 27th International Workshop on Description Logics (DL), the15th In-

Computational Logic
19
ternational Workshop on Non-Monotonic Reasoning (NMR) and the International
Workshop on Knowledge Representation for Health Care 2014 (KR4HC).
A similar growth of logic related meetings has been seen in other academic
ﬁelds as well, for example in logics for computational linguistics, in logic and law,
in sociology as well as in subareas of artiﬁcial intelligence like multi-agent systems,
deductive data bases, logic and the semantic web, logics for knowledge representa-
tion, argumentation, artiﬁcial intelligence in education (AIEd) and many more, a
subset of these is presented in this volume. Other interesting areas for the appli-
cation of computational logic are bioinformatics and biochemistry, but expansion
and diversity can also be found in philosophical logic and in the philosophy of sci-
ence with its world conference Congress of Logic, Methodology and the Philosophy
of Science accompanied by national and international conferences.
There are also many new conferences, workshops and other events that reﬂect
the growing industrial importance of these techniques.
The richness and diversity of this knowledge, with well over several million
publications in these subareas of computational logic and related ﬁelds,14 which we
have learned to access via specialized repositories such as DBLP (computer science
bibliography) and with smart parameter settings of search engines such as Google
Scholar and other more specialized computer support systems, is generally not well
structured and at ﬁrst overwhelming. For example if we ask Google Scholar for
Computational Logic, we get 1.630.000 hits in 0.13 sec — so we better narrow the
query down to Overview of Computational Logic which gives us 699 000 entries in
0.13 sec. May be a better wording may help, so we try Survey on Computational
Logic and obtain 524.000 entries in 0.21 sec. Since we still do not like to scan
half a million entries searching for gold in garbage, we may decide to better search
the subareas of computational logic: for example Survey on Logic Programming,
which gives us 524 000 hits in 0.18 sec. Another area? Survey on Automated
Reasoning yields 142 000 hits in 0.08 sec — and hence any student (and scholar)
will probably give up at this point and and look for other entry points, either by
a more informed search, i.e. better key words and smarter search parameters, or
for example by looking for handbooks covering any important related subﬁeld.
However here again we are confronted with a plenitude that is as diﬃcult to
comprehend as it is to assess: there has always been a long scholarly tradition for
handbooks of logic. Even the seminal Handbook of Logic by J. D. Morell from
1857 is now available again in digitized form [Morell, 1857, digitized 2006].
Since there is unfortunately no standard website or repository to collect and
uniformly represent all these handbooks and survey articles, it may be worthwhile
14In the small subarea of uniﬁcation theory within automated reasoning, there is a problem
called string uniﬁcation or word equations. Google scholar ﬁnds 1.330.000 entries in 0.11 sec for
word equations this year (not all of which is relevant for this topic of course) and needs to be
narrowed down by quoting “word equations” as well as searching for “word equations in maths”,
“word equations in Comp. science” etc. still shows a few thousand hits. In the year 2008 at the
uniﬁcation workshop, where we published a preliminary result, we asked Dr. Google and the
system found 70.300 entries for “word equations” in 0.13 sec - so what are we to make of these
facts?

20
J¨org Siekmann
here to name at least the better known standard references: There is the standard
ﬁve volume Handbook of Logic in Artiﬁcial Intelligence and Logic Programming by
D. Gabbay, C. J. Hogger and J. A. Robinson [Gabbay et al., 1993-1998] and there is
an excellent reference in the open access Stanford Encyclopedia of Philosophy under
the key Logic and Artiﬁcial Intelligence by Richmond Thomason [Thomason, 2013]
(revised 2013), which gives in 12 chapters a comprehensive overview of logic and
AI. This is probably the best entry point today into the literature for the interested
scholar.
Jack Minker’s Logic-Based Artiﬁcial Intelligence [Minker, 2000] and the Hand-
book of the Logic of Argument and Inference: The turn towards the practical by
Dov Gabbay [Gabbay, 2002] as well as the “red” books in the series Practical
Logic and Reasoning by Dov Gabbay, J¨org Siekmann, Johan van Benthem and
John Woods are good entry points for AI and reasoning.
A standard reference for description logics and logics for the semantic web,
as presented in chapter 19 and 20, is Franz Baader’s Description Logic Hand-
book: Theory, Implementation and Applications [Baader, 2003]. The Handbook of
Defeasible Reasoning and Uncertainty Management Systems [Gabbay and Smets,
1998] by Dov Gabbay and P. Smets and the Handbook of Paraconsistency [Beziau
et al., 2007] by J. Y. Beziau, W. A. Carnielli and D. Gabbay are also standard ref-
erences for AI-inspired logics. The importance of stable model semantics for logic
based AI is covered inter alia in the ﬁrst chapter of [Minker, 2000]. Knowledge
representation in AI is another source for the development of specialized logics,
a standard reference is Knowledge Representation and Reasoning [Brachman and
Levesque, 2004] by R. J. Brachman and H. Levesque and the Handbook of Knowl-
edge Representation [van Harmelen et al., 2008] by Frank van Harmelen, Vladimir
Lifschitz and B. Porter.
The subject of automated reasoning, one of the oldest subareas in AI which is
covered in part one of this this volume, has attracted many handbooks and surveys,
among them is the two volume Handbook of Practical Logic and Automated Rea-
soning [Harrison, 2009] by John Harrison, the Handbook of Automated Reasoning
by Alan Robinson and Andreij Voronkov [Robinson and Voronkov, 2001] and the
Handbook of Tableau Methods [D’Agostino and Gabbay, 1999] by M. D’Agostino
and Dov Gabbay. Temporal reasoning is another active area within AI and logic,
see Temporal logic: mathematical foundations and computational aspects [Gabbay
et al., 1994] by Dov Gabbay, I. Hodkinson, M. Reynolds and M. Finger. Logic, lan-
guage and AI is another proliﬁc ﬁeld, see the Handbook of Logic and Language [van
Benthem and Meulen, 1996] by Johan van Benthem and A. Ter Meulen. A stan-
dard reference for logic in computer science is the four volumes Handbook of Logic
in Computer Science by S. Abramsky, D. Gabbay and T. Maibaum [Abramsky et
al., 1992-1995] (with Henk Barendregt for the fourth volume) and the Handbook
of mathematical logic by John Barwise [Barwise, 1982].
Other interesting sources to ﬁnd our way into the ﬁeld are the collections of the
most inﬂuential papers which shaped a subﬁeld like the wellknown “readings” of
Morgan Kaufman. For example Ronald Brachman and Hector Levesque‘s Read-

Computational Logic
21
ings in Knowledge Representation [Brachman and Levesque., 1985] or Goldman’s
Readings in Philosophy and Cognitive Science [Goldman, 1993]. Other readers are
M. Ginsberg Readings in nonmontonic Reasoning [Ginsberg, 1980], Logic and Phi-
losophy for Linguists: A Book of Readings by and J. M. E. Moravcsik and Austin
Tate, A. Hendler and James Allen: Readings in planning [Tate et al., 1994] A
collection of the early papers in automated theorem proving is: Automation of
Reasoning: Classical papers on computational logic 1957-1966 by J¨org Siekmann
and Graham Wrightson [Siekmann and Wrightson, 1983].
Of course logic in general and computational logic in particular are not the only
research areas that witness such an unprecedented growth — many subareas of
mathematics or physics would show similar quantative growth.
But why do we witness such an exponential growth of our written corpus of
knowledge in this ﬁeld that is still viewed even by many academics as rather
moot and part of philosophy? There is of course the general growth and economic
importance of science and technology in our modern society and the transformation
from agriculture based subsistence to a knowledge based economy, which leads to
the paradox that there are currently more scientists and technicians living on this
planet then the sum of all its predecessors and the corpus of written knowledge
grows exponentially, in fact it almost doubles every 3 years15. But apart from this
general transition, we could still ask: why is it that a subject like logic, widely
considered part of philosophy in the previous centuries, is now also part of this
transition? It appears that there are two main reasons:
(i) an inherent academic and scientiﬁc reason that drives the development
(ii) but also the fact that the number of industrial applications and their com-
mercial use increases exponentially as well.
So let us look at the ﬁrst issue, the academic diversity and quote from our
roadmap for computational logic from the International Federation for Computa-
tional Logic (IFCoLog), where we addressed this issue.
To understand the nature and the connections between the diﬀerent areas of
computational logic we need to adopt a focus and a point of view. This logical
point of view has been dominated during the previous two and a half centuries by
the role logic plays in the foundation of mathematics in particular and logicism in
general. In contrast, today’s logic is also trying to understand and model human
beings and their computational avatars in their daily activities and hence there is in
addition strong pressure and urgency from artiﬁcial intelligence, robotics, natural
language analysis, psychology, philosophy, law, social decision theory, computer
science, formal methods and engineering to understand and model human and
machine behaviour in these areas with a view to developing and marketing devices
which help or replace humans in their activities in these areas. This is a diﬃcult
but lucrative and rewarding task and these needs have accelerated the evolution
of logic in the last century far more than the traditional roles it used to play.
Let us adopt this human activity point of view and try to see how the various
areas of computational logic relate to one another, when viewed from this angle.
15see http://www.sims.berkeley.edu/research/projects/how-much-info-2003/

22
J¨org Siekmann
Imagine early man living in prehistoric times, trying to survive and provide for
his family in a hostile and dangerous world. Our human has scarce resources and
very little time to make decisions crucial to his survival. He comes up naturally
with two immediate logical principles:
Categorical classiﬁcation
He classiﬁes his environment into categories: animate, edible, good, bad, danger-
ous and others. In logical terms this means that he naturally deﬁnes predicates
A(x), B(x), . . . and has class relationships between them. So
A1(x) ∧A2(x) →B(x)
means that by deﬁnition, every A1∩A2 is a B. This kind of deductive logic was
formalised by Aristotle, later reﬁned into ﬁrst- and higher-order logical formalisms.
Hasty generalisation
He also has to learn very quickly about the dangers of his environment.
For
example if something from the edible category, say, greenish ﬂat leaf, turns out
to be several times the cause of stomach upset then our prehistoric man has to
generalise and decide that such leaves are bad for his health. So he introduces the
approximate quick rule by induction:
Green(x)∧Flat(x)∧Leaf(x) ⇒Bad(x).
This rule is not absolute like the classiﬁcation rules as it is defeasible. He may later
ﬁnd out that when cooked, the leaves are OK. Thus another rule is introduced and
learned, namely:
Green(x)∧Flat(x)∧Leaf(x)∧Cooked(x) ⇒¬ Bad(x).
Here we use ⇒to indicate that the rules are defeasible. This simple minded
resource bounded approach manifests itself in modern diagnostic terms into what
is known as Defeasible Logics and resource bounded argumentation. These logics
now have absolute rules of the form
A1(x)∧A2(x) →B(x)
and defeasible rules, which are not absolute, of the form
A1(x)∧A2(x) ⇒C(x)
If we have a query whether B(a) or ¬B(a) holds for some element a, we may
have more than one rule which may give us an answer.
For example, we may have A1(x)∧A2(x) ⇒B(x) and E(x)∧A1(x)∧A2(x) ⇒
¬B(x).
Now the second rule has priority, because it is more speciﬁc, relying on more
experience and more information.
The above example points to a connection
between three diﬀerent areas of computational logic:

Computational Logic
23
1. General logical theory, required to provide discipline and methodology for
proper reasoning and how to handle such logics as they arise in applications:
this was the subject of logic in the previous millennia.
2. Artiﬁcial intelligence methodology investigates such observations as hasty
generalizations and a variety of diagnostic principles and other human com-
mon sense reasoning rules leading to persistence, abduction, negation as
failure, non monotonic reasoning and other logical formalisms.
3. Automated deduction and constraint bound programming is designed to give
quick answers from the database to our human agent in real time.
In practice the automated deduction area and the constraints programming ar-
eas developed into large central subareas of computational logic with many diverse
applications.
Speciﬁcation and Veriﬁcation
If we move from prehistoric time forward to the 21st century, we ﬁnd that the
situation confronting man has not changed in its basic nature - it has only become
more complex. Instead of trying to survive in the forest, we are now wandering
in our modern technological and highly bureaucratic society trying to survive the
intricacies of our system.
Our resources are still limited and our time is still
scarce. To model these environments we need more complex formal languages,
more sophisticated logics and faster real time automated reasoning systems. What
had been rather easy for the simple prehistoric model has now evolved into a new
discipline for modern times. As an example take the area of formal methods and
the speciﬁc case of controlling and running the entire train system of the European
Community. This is a complex structure, with many aspects and many constraints
and demands so that we can no longer just write rules of the form
A1(x) →B1(x) and A2(x)∧B2(x) ⇒C(x).
One reason is that such a language is not expressive enough for these needs. A
second reason is that we cannot allow for errors in this application domain. So
the very defeasibility of ‘⇒’-rules can no longer be tolerated and instead we need
to develop a new systematic area of formal methods for:
4. Speciﬁcation; i.e. writing exactly what we want.
5. Synthesis. i.e. developing executable code from speciﬁcations.
6. Veriﬁcation: methods for proving that programs do what they are supposed
to do without error and ﬁnally
7. Logic based Safety and Security engineering technologies.

24
J¨org Siekmann
The large number of companies interested in these areas have turned it into a
big and central area. Computational logic is at the core of it.
There is another corrollary from the above points 4, 5, 6 and 7 as well: learning
and teaching logic. A good case in point is the car manufacturing industry: more
than one third of the future overall value of a motor car will be in its electronic
equipment. A car is — like an aeroplane, a military tank or a ship — only at
ﬁrst sight a moving physical object: its essence is the dozens of processors and
their complex communication devices that share information among themselves as
well as with the outside world - which may be a human or another vehicle or just
a street sign. From the internal control of the combustion engine, the brake or
the steering wheel (drive-by-wire) to the more elaborate speech controlled devices
that activate your iPhone, the park-in-algorithm or your satellite bound navigation
system up to the driverless car of the future: computer systems and their reliability
are central and paramount. For many years I worked as a consultant and head of
joint projects with Daimler: they knew what was coming well before the turn of
the last century. But how do you reorganise and re-educate a large company with
its justiﬁed pride in its traditional engineering skills, which still builds the best
cars in the world? How do you convert your selfconﬁdent employees into the more
humble role of learning new skills — including formal methods?16 A task that
even now is far from being accomplished. Not only Daimler had serious problems
with the reliability of their car electronics and costly recalls. Jointly with other
car manufacturers they developed complex testing and veriﬁcation processes. So a
very important task today is to teach formal methods and a basic understanding
of logical formalism. Hence there is another point:
8. Teaching logic, i.e. to pass the knowledge on from one generation to the
next with pedagogical insight and appropriate computer support such as
intelligent tutor systems (ITS) and other (world wide) computer supported
media for logic.
Master and Ph.D. in Computational Logic
The International Center for Computational Logic (ICCL) at the University of
Dresden (Germany) is an interdisciplinary center of competence in research and
teaching in the ﬁeld of Computational Logic, with special emphasis on Algebra,
Logic, and Formal Methods in Computer Science.
It oﬀers a distributed two-
years European Master’s Programme in Computational Logic (EMCL)17 by four
European universities. They have also joined other European universities for a
distributed European PhD Programme in Computational Logic (EPCL)18 in co-
operation with the Free University of Bozen-Bolzano (Italy), the Technische Uni-
16My favourite quote from the heart of a frustrated industrial advisor is from Upton Sin-
clair: “It is diﬃcult to get a man to understand something, when his salary depends on his not
understanding it” (Wikiquote).
17see http://www.emcl-study.eu/home.html
18see http://www.epcl-study.eu/

Computational Logic
25
versitt Wien (Austria) and the Universidade Nova de Lisboa (Portugal). These
training and education opportunities, which include ﬁnancial support and stu-
dent scholarships, are supported by the IPID program of the DAAD (Deutscher
Akademischer Austauschdienst), the German Federal Ministry of Education and
Research (BMBF) and the DFG (the German National Science Foundation).
Time and Interaction
But coming back to our ancient man: he is not alone and his survival and his
capabilities derive from the fact that he lives in small communities with a well
deﬁned role for each of its member.
The overall behaviour of the tribe is an
emergent functionalty of the individual behaviours and the “egoistic gene” must
learn altruistic values as well. The analysis and modelling of multi-agent systems
in artiﬁcial intelligence (and some artiﬁcial life systems) address these interaction
problems and there are specially tailored logics to capture these phenomena.
There is also another aspect in modern times that requires modelling, which is
the supremacy of time, action and change. As societies gained in complexity, in
resources and especially in long term planning, the time and change aspect has
become more and more dominant. Whereas prehistoric man did not think beyond
his daily problems, modern man developed plans, actions, laws and commitments.
For example the huge task of planning and moving the complete equipment for
one hundred thousand soldiers and their support troops - from the tent pole to the
maintenance tools for a tank - into a foreign land by air is carried out these days
by computer planning and other techniques from artiﬁcial intelligence that have
to be reliable (ﬁrst time for example in the Iraq war). So a large area of logic is
now devoted to temporal processes and interactions. This manifests itself on two
fronts:
9. Models of interactive agents in artiﬁcial intelligence,
10. Analysis of interactive, parallel and evolving processes in software engineer-
ing and theoretical computer science.
These two subjects have evolved independently of each other, studied by separate
communities, in spite of the fact that they are highly related. Also there is the
“logic of action” research community, see [Kracht et al., 2009] for a survey.
Another aspect related to time is revision, uncertainty, change and the resolution
of conﬂicts: with worldwide access to knowledge and people, we receive conﬂicting
information from diﬀerent sources and with diﬀerent degrees of reliability so we
have to reconcile and deal with this kind of information. Special logics have been
constructed and technical machinery has been developed for this purpose.
Language and Law
Finally let us look at two additional major areas where computational logic is
needed and frequently used: logic and language and logic and law. Human activ-

26
J¨org Siekmann
ity and reasoning is reﬂected in its language and in its laws. The language is the
medium and instrument of interaction of the active human agents and the laws
govern that interaction, largely in a compatible way to common sense. It is no
surprise therefore that computational logic is a core discipline for analysing and
modelling the structure of language and in modelling reasoning and fundamental
concepts in law. The language community, in particular in computational linguis-
tics, is well aware of the central role which logic plays in language both as a tool
for analysis and computation as well as a resident component in the structure of
language, shaping language use and substructure. The subcommunity of linguists
interested in semantics and using logic is large and well organised and connected.
This is unfortunately only to a lesser degree the case in law: There is a com-
munity of philosophical logicians and artiﬁcial intelligence researchers interested
in law and well structured argumentation, but the law community in general does
not always realise how closely their core concepts are related to logic and its com-
putational support systems. In fact, the theoretical logicians as a community do
not realise in turn how much they can learn by observing the discussions and legal
manoeuvring of the law community. Many new logical principles can be extracted
from legal behaviour much in the same way that new mathematics was extracted
from attempts to model physical behaviour in the material universe. Logic and
law is one of the most promising and at the same time unexplored area for logi-
cal analysis. The same holds for theological reasoning and the respective logical
analysis within the main religious writings: Christian, Buddhist as well as Islamic
scholarship and the reasoning and argumentation of the Torah.
Industrial Applications
When I ask my colleague from the maths department, whose research is in abstract
algebra, about applications of his work, he would answer enthusiastically: “oh
there are so many, you can hardly count them: for example in vector analysis, in
Galois theory and in Boolean Algebra, in ....”
In Amir Pnueli’s hallel (laudatio) for Moshe Vardi’s doctorship honoraris causa
here at Saarbr¨ucken in the year 2002 he justiﬁedly praised Moshe as one of the most
oustanding logicians in our community particulariliy focussing on Moshes many
contributions to the applications of logic, like the logical theory of data bases, the
computational logic of knowledge, the logic and automata theoretic approach to
veriﬁcation as well as his ﬁnite model theory and applications in model checking for
hard and software veriﬁcation. The subsequent academic presentations empasized
these application aspects even more and we all departed with a deep impression
of the sizeble market opportunities.
In my time as a director of the German Institute for Artiﬁcial Intelligence
(DFKI) where the logic related projects were mainly in my research lab, I worked
as an advisor for many European government institutions as well as a consultant to
German and other Multi National Companies, so I also showed and discussed the
presented material to one of our representatives on the industrial advisory board

Computational Logic
27
of the DFKI (whose boss and predecessor in the nullary years might have earned
about a million Euros a year, whereas his successor today may claim at least as
much per month)19, a mild not necessarily unfriendly expression entered his face,
as he might choose to display to his aspiring son, who as everbody expects would
one day grow up to enter the footsteps of his father, and he started a fascinating
but slightly patronising conversation about multi nationals and international mar-
ket sizes, time-to-market and market indicators, venture capital, multinationals,
investment funds and more.
When I ask “Dr. Google Scholar” for industrial applications of fuzzy logic, for
example, the system comes up with 350 000 hits in 0.37 sec including excellent
scholarly survey articles. The same query just sent to Google delivers 741 000 hits
in 0,22 sec showing some of the scholarly articles as well, but also a myriad of
concrete devices, companies and market shares. The amount even of this single
application area is overwhelming!
The original plan for the establishment of the DFKI was based on about 200
Million Euros of seed money over a time span of ten years from the German Federal
Ministry BMBF. It included a milestone after ﬁve years, whereupon the institute
was to be evaluated and in case of a positive outcome it was to be turned into an
industrial application centre, i.e. a link between industry and academia. So we,
the academic directors and founders, were blessed with a new managing director
from one of the largest German companies and we were all sent to management
courses and training: a great journey into market shares, market entry barrieres,
time-to-market indices as well as the order of magnitude of a market, i.e. the
fact that there is more to the diﬀerence between a market of a million or a billion
dollars than just ten to the power of three.
We, the academics, tend to ignore the enormeous complexity of this economic
world and vice versa, we are more often then not seen as the small constant
gardeners cultivating our “akademisches Vorg¨artchen” - to quote from the opening
speech of our SIEMENS representative.
Looking for an application of our veriﬁcation tool VSE at the DFKI a long
time ago, we managed in our lab to verify “in-principle” a cardiac pacemaker and
presented our results proudly to one of our share holders and manufacturers of
these devices. After a warm wellcome and a well received presentation we were
asked for a conﬁdential meeting with the top management board, where we learned
that the company had moved the oﬃcial residence of its daughter to Sweden with
its well known less restrictive liability laws, and within that legal framework it
turned out that the costs of full scale veriﬁcation by far exceeds the potential
liability sum multiplied by the current risk factor of their device — and so we
were politely sent back to our institute. Hardly the kind of reasoning a young
postdoc trying to establish his own application driven research and development
group is accustomed to.
19Justiﬁed or not, there is currently an interesting debate about Thomas Piketty’s book “Cap-
ital in the Twenty-First Century” [Piketty, 2014], for example: Paul Krugman “Why we are in
a new guilded age” in the New York Review of books, May 2014, vol LXI, No 8.

28
J¨org Siekmann
In precis: a single person will need more than one lifetime to fully appreciate
both worlds, academic as well as market economy, and a competent account of
industrial applications of logic would require several volumes in a set of handbooks
of its own.
Logical techniques in stand-alone systems such as model checking,
theorem proving by induction or fuzzy controllers have found their welldeﬁned
markets that can be quantiﬁed and accounted for. However logic as an enabling
or at least supporting technology in otherwise independent markets such as say
manufacturing, car, train or aeroplane industries, mechatronics or chemistry —
to just name some major application areas — are no longer within the order of
magnitude of millions but billions of Euros.
The International Federation for Computational Logic: IFCoLog
The enormeous diversity outlined above is not necessarily disadvantageous, as
every of these evolved communities addresses its own important set of problems
and issues, and it is clear that one group cannot address them all.
However,
fragmentation can carry a heavy price intellectually as well as politically in the
wider arena of scientiﬁc activity where, unfortunately, logical investigations are
often still perceived as limited in scope and value.
So how can these hundreds of societies, sociologically evolved communities of
workshop and conference aﬃliates be reunited without losing their historical iden-
tities? Our solution is inspired by the manner in which the European AI societies
are organised: there is one registered society namely ECCAI (European Coordi-
nating Committee for Artiﬁcial Intelligence), whose members are the European
national AI societies. With the growing uniﬁcation of Europe there are currently
more than two dozen members, who represent all European AI researchers and
whose representatives meet every two years at the time of ECAI, the European
Conference on Artiﬁcial Intelligence.
So the idea for IFCoLog is the same as in AI and most other scientiﬁc ﬁelds
these days: An International Federation for Computational Logic (IFCoLog)20 has
been created more than twenty years ago with the help of Dana Scott and legally
registered in the Netherlands as well as a charity in London with the ﬁnancial aid
of the European Network of Excellence for Computational Logic (CologNet and
COMPULOG), whose members are the current (and future) communities related
to computational logic. Some of these are actually organised into legal societies,
others are simply associated with a conference, but nevertheless form a scientiﬁc
community of considerable size and importance.
In as much as the Federation aims to counterbalance the growing division in
the ﬁeld and to represent it once again in its entirety, it is working on the four
major goals: Information, Representation, Promotion and Cooperation.
More
speciﬁcally, its activities are: to inﬂuence funding policy and to increase our inter-
national visibility, to set up concrete educational curricula and to encourage high
quality teaching materials, to maintain an active information policy by creating an
20see http://www.ifcolog.net/

Computational Logic
29
infrastructure for web sites and links and to found and maintain formal scientiﬁc
journals in the name of IFCoLog such as the Oxford IGPL journal, the Journal of
Applied Logic (JAL), the journal Computational Logic and the recently founded
IFCoLog Journal for Applied Logic- Also there is an informal journal PhiNews. Fi-
nally it supports and identiﬁes with FLOC, the major federated conference which
is held every three years. We want to establish a permanent oﬃce for the fed-
eration that coordinates and maintains all of these activities similar to, say, the
Royal Society for Science in Great Britain or the scientiﬁc academies in various
countries. At the time of writing we have applied with positive response for mem-
bership in the International Council for Science (ICSU) in order to realize our ﬁnal
goal, namely to establish computational logic as an academic ﬁeld of its own.
Special Interest Group of the Association for Computing Machinery:
SIGLOG
At the time of writing the American Association for Computing Machinery (ACM)
approved a new special interest group for computational logic, called SIGLOG21,
after several years of negotiation with the ACM authorities by Moshe Vardi, Dana
Scott and Prakash Panangaden, who is now its chairman. Representing almost
every major area of computing, ACM’s Special Interest Groups oﬀer the infrastruc-
ture to announce conferences, publications and scientiﬁc activities and to provide
opportunities for sharing technical expertise and ﬁrst-hand knowledge - so this
is another important step towards the recognition of Computational Logic as an
academic ﬁeld.
ACKNOWLEDGEMENTS
I would like to thank the authors of this volume for their critical reading of this
chapter, particular thanks to Alan Bundy, Moshe Vardi, Jack Minker, Pascal Hit-
zler and John-Jules Meyer for their corrections and helpful information.
BIBLIOGRAPHY
[Abramsky et al., 1992-1995] S. Abramsky, D. Gabbay, and T. Maibaum, editors. Handbook of
logic in computer science, volume four volumes. Oxford University Press, 1992-1995.
[Baader, 2003] F. Baader, editor. The description logic handbook: theory, implementation, and
applications. Cambridge University Press, 2003.
[Barwise, 1982] J. Barwise, editor. Handbook of mathematical logic. North Holland, Elsevier,
1982.
[Beziau et al., 2007] J. Y. Beziau, W. A. Carnielli, and D. Gabbay, editors. Handbook of para-
consistency. Kings College Publications, 2007.
[Brachman and Levesque., 1985] Ronald J. Brachman and Hector J. Levesque., editors. Read-
ings in knowledge representation. Morgan Kaufmann Publishers Inc, 1985.
[Brachman and Levesque, 2004] R. J. Brachman and H. Levesque, editors. Knowledge Repre-
sentation and Reasoning,. Amsterdam, Elsevier, 2004.
21see SIGLOG.org, SIGLOG.net and SIGLOG.inf

30
J¨org Siekmann
[D’Agostino and Gabbay, 1999] M. D’Agostino and D. Gabbay, editors. Handbook of tableau
methods. Springer, 1999.
[Gabbay and Smets, 1998] D. Gabbay and P. Smets, editors. Handbook of Defeasible Reasoning
and Uncertainty Management Systems, Volume 1: Quantiﬁed Representation of Uncertainty
and Imprecision, volume volume 1. Springer, 1998.
[Gabbay et al., 1993-1998] D. Gabbay, C. J. Hogger, and J. A. Robinson, editors. Handbook of
Logic in Artiﬁcial Intelligence and Logic Programming, volume ﬁve volumes. Oxford Univer-
sity Press, 1993-1998.
[Gabbay et al., 1994] D. Gabbay, I. Hodkinson, M. Reynolds, and M. Finger, editors. Temporal
logic: mathematical foundations and computational aspects. Vol. 1. Oxford: Clarendon Press,
1994.
[Gabbay, 2002] D. Gabbay, editor. Handbook of the Logic of Argument and Inference: The turn
towards the practical. North Holland, Elsevier, 2002.
[Ginsberg, 1980] Matthew Ginsberg, editor.
Readings in nonmonotonic reasoning.
Morgan
Kaufmann Publishers Inc, 1980.
[Goldman, 1993] Alvin I. Goldman, editor. Readings in philosophy and cognitive science. MIT
Press, 1993.
[Harrison, 2009] J. Harrison, editor. Handbook of practical logic and automated reasoning. Cam-
bridge University Press, 2009.
[Kracht et al., 2009] M. Kracht, J.-J. Ch. Meyer and K. Segerberg. The Logic of Action. In The
Stanford Encyclopedia of Philosophy, Edward N. Zalta, ed., 2009 Edition.
[Minker, 2000] Jack Minker, editor. Logic-based Artiﬁcial Intelligence, Springer, 2000.
[Morell, 1857, digitized 2006] John Daniell Morell, editor. Handbook of logic. Oxford University,
1857, digitized 2006.
[Piketty, 2014] Thomas Piketty. Capital in the Twenty-First Century. Belknab Press Harvard
University (translated from French), 2014.
[Robinson and Voronkov, 2001] Alan Robinson and Andreij Voronkov, editors.
Handbook of
automated reasoning. Elsevier, 2001.
[Siekmann and Wrightson, 1983] J. Siekmann and G. Wrightson, editors. Automation of Rea-
soning: Classical papers on computational logic. Vol 1 and 2. Springer Publishing Company,
1983.
[Tate et al., 1994] A. Tate, J. Hendler, and J. Allen, editors. Readings in planning. Morgan
Kaufmann Publishers Inc., 1994.
[Thomason, 2013] R.
Thomason.
Logic
and
artiﬁcial
intelligence.
http://plato.stanford.edu/entries/logic-ai/, 2013.
[van Benthem and Meulen, 1996] J. F. van Benthem and A. Ter Meulen, editors. Handbook of
logic and language. Elsevier, 1996.
[van Harmelen et al., 2008] F. van Harmelen, V. Lifschitz, and B. Porter, editors. Handbook of
Knowledge Representation. Elsevier, 2008.

LOGIC AND THE DEVELOPMENT
OF THE COMPUTER
Martin Davis
Reader: J¨org Siekmann
In the fall of 1945, as the ENIAC, a gigantic calculating engine containing
thousands of vacuum tubes, neared completion at the Moore School of Electrical
Engineering in Philadelphia, a committee of experts were meeting regularly to
discuss the design of its successor, the proposed EDVAC. As the weeks progressed,
their meetings became acrimonious; the experts found themselves dividing into
two groups: the “engineers” and the “logicians”. John Eckart was the leader of
the engineers. He was justly proud of his accomplishment with the ENIAC. It
had been thought impossible that 15,000 hot vacuum tubes could all be made to
function without failing long enough for anything useful to be accomplished, and
Eckart, by using careful conservative design principles had succeeded brilliantly in
doing exactly this. The leading “logician” was the eminent mathematician John
von Neumann. Eckart was particularly furious over von Neumann’s circulating
his draft EDVAC report under his own name. This report paid little attention to
engineering details, but set forth the fundamental logical computer design known
to this day as the von Neumann architecture [Von Neumann, 1945].
Although the ENIAC was an engineering tour de force, it was a logical mess.
It was von Neumann’s expertise as a logician that enabled him to understand the
fundamental fact that a computing machine is a logic machine. In its circuits it em-
bodies the distilled insights of a remarkable collection of logicians, developed over
centuries. Nowadays when computer technology is advancing with such breathtak-
ing rapidity, as we admire the truly remarkable accomplishments of the engineers,
it is all too easy to overlook the logicians whose ideas made it all possible.1 We be-
gin with the amazing G.W. Leibniz. No project was too ambitious for him. He was
ready to try to talk Louis XIV into invading Egypt and building a canal through
the Isthmus of Suez, to work on convincing the leaders of Christianity that the
doctrinal diﬀerences between the various sects could be overcome, and to oﬀer his
boss, the Duke of Hanover, to the English to be their king. Incidentally, he devel-
oped a philosophical system that purported to show how God’s perfection could be
reconciled with the apparent imperfections of the world, and (with Newton) was
1This and the preceding paragraph are copied verbatim from my book [Davis, 2012] in which
this history is discussed in more detail.
Handbook of the History of Logic. Volume 9: Computational Logic. 
Volume editor: Jörg Siekmann 
Series editors: Dov M. Gabbay and John Woods 
Copyright © 2014 Elsevier BV. All rights reserved.

32
Martin Davis
one of the inventors of the diﬀerential and integral calculus. But what was perhaps
his most audacious conception of all was of a universal language of human thought.
In this language the symbols would directly represent ideas and concepts thereby
making their relationships transparent. Just as algebraic notation facilitates cal-
culations revealing numerical relationships, so Leibniz’s language would enable
logical relationships to be discovered by straightforward calculations. In Leibniz’s
vision, his language would be used by a group of scholars sitting around a table
and addressing some profound question in philosophy or human aﬀairs. “Let us
calculate,” they would say, out would come their pencils, and the required answer
would soon be forthcoming. Leibniz was well aware of the essentially mechanical
nature of computation, and despite the rudimentary technology of his time, Leib-
niz looked forward to the mechanization of computation and grasped the potential
of mechanical calculation for saving humanity from mindless drudgery.2
Engineers designing the circuits used in computers, make use of a special kind
of mathematics called Boolean algebra. This is an algebra of logic, fragments of
which Leibniz had already developed. But it was the Englishman George Boole,
quite unaware of what Leibniz had begun, who completed the endeavor a century
and a half later. Upright to a fault, Boole was “the sort of man to trust your
daughter with”. Denied a university education by the poverty of his family, his
outstanding contributions were eventually rewarded by a professorship in Ireland.
Boole was only 49 when he died after walking three miles in a cold rainstorm to
lecture in his wet clothes. The pneumonia he developed was certainly not helped
by his wife, who placed him between cold soaking bed sheets.3
“There is nothing worse that can happen to a scientist than to have the foun-
dation collapse just as the work is ﬁnished. I have been placed in this position
by a letter from Mr. Bertrand Russell . . . ” These despondent words were written
by the German mathematician Gottlob Frege in 1902 in an appendix to a treatise
that was to have crowned his life’s work. Frege never recovered from the blow
administered by the young Englishman. He died in obscurity a little over two
decades later leaving behind a diary ﬁlled with extreme right-wing ideas close to
those that were soon to prove so devastating to Germany and the rest of Europe.
Actually, counter to Frege’s perception, his work had by no means been destroyed
by Bertrand Russell’s letter. Frege would have been astonished to learn that his
seminal ideas are viewed as fundamental by computer scientists engaged in at-
tempting to program computers to perform logical reasoning. Researchers in this
ﬁeld marked the 100th anniversary in 1979 of Frege’s publication of a pamphlet
with the almost untranslatable title Begriﬀsschrift, with a special historical lecture
at their annual conference. (Begriﬀmeans “concept”, and schrift means “script”
or “mode of writing”.) Frege was thoroughly familiar with Leibniz’s thought, and
indeed, he believed that in his pamphlet, he had brought into being the universal
language of reasoning that Leibniz had sought. While the logical reasoning used by
2For Leibniz’s life see [Aiton, 1985], for his mathematical work see [Edwards, 1979; Hofmann,
1974], for his philosophy see [Mates, 1986], and for his work on logic [Couturat, 1961].
3For Boole’s life see [MacHale, 1985], for his work in logic [Boole, 1847; Boole, 1958].

Logic and the Development of the Computer
33
mathematicians in developing proofs goes far beyond what Boole’s algebra allows,
Frege’s rules of inference proved to be completely adequate.4
Although Frege saw himself as carrying out part of Leibniz’s program, he showed
no interest in following up on Leibniz’s “Let us calculate”. He was content to
show how his rules could be applied to the most fundamental parts of mathe-
matics, never trying to develop a computational procedure (algorithm, we would
say) for determining whether some proposed inference is or is not correct. More
than half a century was to elapse before the English mathematician Alan Turing
and, independently, the American logician Alonzo Church, proved that no such
computational procedure could exist. While Turing’s paper showed that one of
Leibniz’s goals was unattainable, it also provided a revolutionary insight into the
very nature of computation. In order to prove that there is no possible algorithm
for accomplishing some task, it was necessary for Turing to furnish a clear and
precise analysis of the very concept of computation. In doing so he developed
a mathematical theory of computation in terms of which a machine for carrying
out a particular computation and a list of instructions for carrying out the same
computation were seen to be two sides of the same coin: either could be readily
transformed into the other. Turing saw that all computational tasks that could in
principle be carried out by a machine could also be accomplished by “software”
running on one single all-purpose machine. This insight ultimately changed the
way builders understood what it means to construct a calculating machine. It
made it possible to think of a single machine that, suitably “programmed”, is
capable of accomplishing the myriad tasks for which computers are used today.
Farsighted people began to envision digital computing machines that would be
capable of things that Leibniz could hardly have imagined.5
The path from Frege to Turing was no straight line. Logicians during the ﬁrst
third of the twentieth century found themselves faced with what was thought to
be a “crisis” in the very foundations of mathematics itself. The “crisis” originated
in the eﬀorts of Georg Cantor to expand the grasp of mathematics into the realm
of the “transﬁnite” where he found not merely one inﬁnity, but cascades of larger
and larger inﬁnities. Cantor himself was not disturbed by the seeming contradic-
tion between the theorem that there is always another inﬁnity larger than any
of his transﬁnites and the evident fact that one could not exceed the “absolute
inﬁnity” consisting of all of his transﬁnites. For the deeply religious Cantor, this
absolute had a clear theological interpretation, but others were not so sanguine.
Matters were brought to a head when Bertrand Russell showed that the reasoning
embodied in this paradox could be distilled into what appeared to be an outright
contradiction in our simplest logical intuitions. This was communicated in Rus-
sell’s letter that so upset Frege, and Frege saw that it rendered his own system
self-contradictory.6
4[Kreiser, 2001] is an excellent biography but is only available in the original German; for an
English translation of Frege’s Begriﬀsschrift see [van Heijenoort, 1967].
5For Turing’s revolutionary paper see [Turing, 1936].
[Petzold, 2008] provides a carefully
guided tour of the article.
6For Cantor’s life see [Grattan-Guinness, 1971; Meschkowski, 1983; Purkert-Ilgauds, 1987],for

34
Martin Davis
The perceived crisis arose over the issue of incorporating Cantor’s transﬁnites
into the body of respectable mathematics while avoiding the contradictions to
which they seemed inevitably to lead. The technical developments on which Alan
Turing based his crucial insights emerged from eﬀorts to deal with the “crisis”.
Some leading mathematicians proposed a radical revisionism that would have ban-
ished not only Cantor’s transﬁnites, but a number of other modes of reasoning
now felt to be suspect. The great German mathematician David Hilbert whose
mathematical trademark was the use of general abstract ideas instead of brute
calculation, would have none of this. He contemptuously rejected this attempt “to
dismember our science”. Cantor’s theory of transﬁnites was “the ﬁnest product of
mathematical genius, and one of the supreme achievements of purely intellectual
human activity”. Hilbert extracted from Frege’s language a set of rules of inference
for dealing with individual elements of a perfectly arbitrary nature. Any particu-
lar part of mathematics, including Cantor’s transﬁnites, could be expressed as a
system consisting of these rules with an appropriate collection of axioms adjoined
to serve as premises. Transcending Frege and Russell’s conception, Hilbert saw
that such a system had an inside and an outside. Viewed from the inside, it was
simply a piece of mathematics turned into an artiﬁcial language in which all of
the reasoning had been reduced to symbol manipulation. But from the outside,
it could be viewed as a purely formal game with rules for the manipulation of
symbols. Hilbert’s program for dealing with the “crisis” was what he called meta-
mathematics: viewed from the outside the supposed fact that no contradictions
were derivable using the rules of the game could be regarded as a mathematical
theorem to be proved. To silence the opposition, Hilbert insisted that the proof of
such a consistency theorem be carried out using “ﬁnitary” methods that everyone
would admit were impeccable.7
In Vienna during the 1920s, a group of scholars with radical ideas about the
content and nature of philosophy met regularly, calling themselves the “Vienna
Circle”. They embraced Hilbert’s restriction to ﬁnitary methods in logical inves-
tigations, and indeed tended to go much further. Where Hilbert had conceived of
his “games” as a device to insure the validity of abstract and inﬁnitary methods in
mathematics, in the Vienna Circle the tendency was to reject any notions of mean-
ing and truth in mathematics except as symbol manipulation permitted by these
games. Hilbert had added an “outside” to the conceptual basis of mathematics for
the purpose of justifying the “inside”. For the participants in the Vienna Circle,
there was only the “inside”. Among those attending the meetings of the Vienna
Circle was the young mathematician Kurt G¨odel who was quite skeptical about
what he was hearing. He saw that Hilbert’s insistence on ﬁnitary methods was in
some cases an obstacle to obtaining very important results. Moreover, embracing
Hilbert’s invitation to study the interplay between the “inside” and “outside” of
his systems, G¨odel showed that for such a system, certain simple true statements
about the whole numbers 0, 1, 2, . . . could not be proved using the rules of that
his transﬁnite numbers [Cantor, 1941].
7For Hilbert’s life see [Reid, 1970], for his polemics [Mancosu, 1998].

Logic and the Development of the Computer
35
system. One could not, as the members of the Vienna Circle had wished, ignore
the “outside”, the meaningful content, of Hilbert’s formal systems! Hilbert was
almost 70 in 1930 when G¨odel announced this result.
Soon afterwards, G¨odel
realized that a corollary of his work showed that with the restriction to Hilbert’s
ﬁnitary methods, it would be impossible to prove that his systems were free from
contradictions.8
G¨odel’s work killed Hilbert’s program, but it also opened new vistas in a num-
ber of directions. In line with Leibniz’s ideas, he made it clear that the study of
the structure of artiﬁcial languages is worthwhile. G¨odel’s technique showed how
to embed the “outside” of one of Hilbert’s systems in its “inside”. To do so, the
strings of symbols by which a system presents its “outside” were represented by
numbers, which are available “inside”. This anticipated the technique of repre-
senting a computer program, which the programmer sees as consisting of strings
of symbols, by strings of zeros and ones. In fact, in order to show that various
metamathematical relations could be expressed “inside”, G¨odel developed what
amounted to a programming language.
In his writings on logic, Hilbert had emphasized the crucial importance of what
he called the Entscheidungsproblem. This was nothing but the problem of provid-
ing the calculational methods for Frege’s rules of inference that would have been
needed to carry out Leibniz’s dream of mechanizing reason. Alan Turing’s decisive
proof that no such methods could exist came a few years after G¨odel’s work and
was heavily inﬂuenced by it. The all-purpose digital computers in Turing’s paper
were mathematical abstractions, but he was intrigued by the possibility of building
a real one. Such thoughts had to be put on hold because of the Second World War
during which Turing played a key role in the successful deciphering of the German
secret military codes. In his spare time, he thought about how computers could
be made to play a good chess game, and indeed to behave “intelligently”. When
the war was over he wrote a report showing how to build a computer using the
available technology. Turing took a job at the University of Manchester working
with the computer that was being built there. After the police in Manchester
learned of Turing’s sexual involvement with a young man, he was arrested, con-
victed of “gross indecency” and compelled to undergo “treatments” with female
sex hormones. He died of cyanide poisoning, apparently a suicide, two years later.9
As a young mathematician, John von Neumann had been attracted by Hilbert’s
program and had written a number of articles on consistency proofs and on sys-
tems of axioms for Cantor’s set theory. He was in the audience at the conference
at K¨onigsberg in 1930 where G¨odel ﬁrst announced his fateful results, and imme-
diately grasped their signiﬁcance. Von Neumann was so impressed by what G¨odel
had accomplished that a number of his lectures during that period were devoted
to G¨odel’s results. He knew Turing personally, and was certainly familiar with
8For G¨odel’s life see [Dawson, 1997; Kreisel, 1981].
9The little textbook in which Hilbert emphasized the Entscheidungsproblem was [Hilb-Acker,
1928]. The deﬁnitive biography of Alan Turing is [Hodges, 1983]. The briefer [Leavitt, 2006] is
also excellent.

36
Martin Davis
his work. Von Neumann’s draft EDVAC report, that so inﬂuenced subsequent
computer development and inﬂamed John Eckart, shows unmistakable evidence of
Turing’s inﬂuence.
With the construction of general purpose digital computers, the role of logi-
cians in the developing new technology became a day-to-day matter. From the
beginning, in the 1950s, logic has played a key role in the design of programming
languages.
The PROLOG language (PROgramming in LOGic) made the con-
nection quite explicit, and can be thought of as a partial realization of Leibniz’s
dream. Going beyond what Leibniz could have dreamt of, is the eﬀort to demon-
strate that the full power of human thought can be emulated by computers. This
very possibility has been denied by various thinkers, some by appealing to G¨odel’s
work, others by more general philosophical analysis. Meanwhile, “artiﬁcial intel-
ligence”, has become a signiﬁcant branch of computer science, and while it has
hardly accomplished what its advocates had expected, it has certainly achieved
some impressive results, including the defeat of the world’s leading chess champion
by a computer. From a dispassionate point of view all that can really be said is
that neither side of the ongoing debate over whether computers can exhibit truly
intelligent behavior has really settled the matter. What is clear is that the revo-
lution wrought by computers is just beginning, and that their ability to perform
any symbolic task whose logical structure can be explicitly deﬁned will lead to
developments far beyond what we can imagine today.
BIBLIOGRAPHY
[Aiton, 1985] Aiton, E.J., Leibniz: a Biography, Adam Hilger Ltd. Bristol and Boston 1985.
[Boole, 1847] Boole, George, The Mathematical Analysis of Logic, Being an Essay towards a
Calculus of Deductive Reasoning, Macmillan, Barclay and Macmillan, Cambridge, 1847.
[Boole, 1958] Boole, George, An Investigation of the Laws of Thought on which Are Founded
the Mathematical Theories of Logic and Probabilities, Walton and Maberly, London 1854;
reprinted Dover, New York, 1958.
[Cantor, 1941] Cantor, Georg, Contributions to the Founding of the Theory of Transﬁnite Num-
bers, translated from the German with an introduction and notes by Philip E.B. Jourdain,
Open Court, La Salle, Illinois, 1941.
[Carp-Doran, 1977] Carpenter, B.E. and R.W. Doran, 1977, “The Other Turing Machine,”
Computer Journal, vol. 20(1977), pp. 269-279.
[Ceruzzi, 1983] Ceruzzi, Paul E., Reckoners, the Prehistory of the Digital Computer, from Re-
lays to the Stored Program Concept, 1933-1945, Greenwood Press, Westport, Connecticut
1983.
[Copeland, 2004] Copeland, B. Jack, editor The Essential Turing, Oxford 2004.
[Couturat, 1961] Couturat, Louis, La Logique de Leibniz d’Apr`es des Documents In´edits, Paris,
F. Alcan, 1901. Reprinted Georg Olms, Hildesheim 1961.
[Dauben 1, 1979] Dauben, Joseph Warren, Georg Cantor: His Mathematics and Philosophy of
the Inﬁnite, Princeton University Press, 1979.
[Davis, 1988] Davis, Martin “Mathematical Logic and the Origin of Modern Computers,” Stud-
ies in the History of Mathematics, pp. 137-165. Mathematical Association of America, 1987.
Reprinted in The Universal Turing Machine - A Half-Century Survey, Rolf Herken, ed.,
pp. 149-174. Verlag Kemmerer & Unverzagt, Hamburg, Berlin 1988; Oxford University Press,
1988.
[Davis, 2004] Davis, Martin, ed. The Undecidable, Raven Press 1965. Reprinted: Dover 2004.

Logic and the Development of the Computer
37
[Davis, 2012] Davis, Martin The Universal Computer: The Road from Leibniz to Turing, W.W.
Norton, 2000. Paperback edition titled Engines of Logic: Mathematicians and the Origin of
the Computer W.W. Norton, 2001. Turing Centenary Edition, CRC Press, Taylor & Francis
2012.
[Dawson, 1997] Dawson, John W.,Jr. Logical Dilemmas: The Life and Work of Kurt G¨odel, A
K Peters, Wellesley, Massachusetts, 1997.
[Edwards, 1979] Edwards Jr., Charles Henry, The Historical Development of the Calculus,
Springer-Verlag, New York, 1979.
[Goldstine, 1972] Goldstine, Herman H., The Computer from Pascal to von Neumann, Prince-
ton University Press 1972.
[Grattan-Guinness, 1971] Grattan-Guinness, I., “Towards a Biography of Georg Cantor,” An-
nals of Science, vol. 27(1971), pp. 345-391.
[Hilb-Acker, 1928] Hilbert, D., and W. Ackermann, Grundz¨uge der Theoretischen Logik, Julius
Springer 1928.
[Hodges, 1983] Hodges, Andrew, Alan Turing: The Enigma, Simon and Schuster, New York
1983.
[Hofmann, 1974] Hofmann, J.E., Leibniz in Paris 1672-1676, Cambridge University Press, Lon-
don 1974.
[Kreisel, 1981] Kreisel, Georg, “Kurt G¨odel: 1906-1978,” Biographical Memoirs of Fellows of
the Royal Society, vol.26 (1980), pp. 149-224; corrigenda, vol.27(1981), p.697.
[Kreiser, 2001] Kreiser, Lothar, Gottlob Frege: Leben – Werk – Zeit, Felix Meiner Verlag, Ham-
burg 2001.
[Leavitt, 2006] Leavitt, David, The Man Who Knew too Much: Alan Turing and the Invention
of the Computer, Norton, New York 2006.
[MacHale, 1985] MacHale, Desmond, George Boole: his Life and Work, Boole Press, Dublin
1985.
[Mancosu, 1998] Mancosu, Paolo, From Brouwer to Hilbert, Oxford 1998.
[Mates, 1986] Mates, Benson, The Philosophy of Leibniz: Metaphysics & Language, Oxford
University Press 1986.
[McCull-Pitts, 1943] McCulloch, W.S. and W. Pitts, “A Logical Calculus of the Ideas Immanent
in Nervous Activity,” Bulletin of Mathematical Biophysics, 5(1943), 115-133. Reprinted in
McCulloch, W.S., Embodiments of Mind, M.I.T. Press 1965, 19-39.
[Meschkowski, 1983] Meschkowski, Herbert, Georg Cantor: Leben, Werk und Wirkung, Bibli-
ographisches Institut, Mannheim, Vienna, Z¨urich 1983.
[Petzold, 2008] Petzold,Charles, The Annotated Turing: A Guided Tour through Alan Turing’s
Historic Paper on Computability and the Turing Machine, Wiley, Indianapolis 2008.
[Purkert-Ilgauds, 1987] Purkert, Walter, and Hans Joachim Ilgauds, Georg Cantor: 1845-1918,
Vita mathematica, v. 1. Birkhauser, Stuttgart 1987.
[Randell, 1982] Randell, Brian, ed. 1982, The Origins of Digital Computers, Selected Papers
(third edition), Springer-Verlag 1982.
[Reid, 1970] Reid, Constance, Hilbert–Courant, Springer-Verlag, New York 1986. [originally
published by Springer-Verlag as two separate works:
“Hilbert” 1970 and “Courant in
G¨ottingen and New York: The Story of an Improbable Mathematician” 1976]
[Stern, 1981] Stern, Nancy, From Eniac to Univac: An Appraisal of the Eckert-Mauchly Ma-
chines, Digital Press 1981.
[Turing, 1936] Turing, Alan, “On Computable Numbers with an Application to the Entschei-
dungsproblem,” Proceedings of the London Mathematical Society, ser. 2, 42(1936), pp. 230-
267. Correction: ibid, 43(1937), pp. 544-546. Reprinted in [Davis, 2004] pp. 116-154. Reprinted
in [Turing, 2001] pp. 18-56. Reprinted in [Copeland, 2004] pp. 58-90;94-96. Reprinted in [Pet-
zold, 2008] (the original text interspersed with commentary).
[Turing, 1950] Turing, Alan, “Computing Machinery and Intelligence,” Mind, vol. LIX(1950),
pp. 433-460. Reprinted in [Turing, 1992] pp. 133-160. Reprinted in [Copeland, 2004] pp. 433-
464.
[Turing, 1992] Turing, Alan Collected Works: Mechanical Intelligence, D.C. Ince, editor. North-
Holland, Amsterdam 1992.
[Turing, 2001] Turing, Alan Collected Works: Mathematical Logic, R.O Gandy & C.E.M. Yates,
editors. North-Holland, Amsterdam 2001.
[van Heijenoort, 1967] van Heijenoort, Jean, From Frege to G¨odel, Harvard 1967.

38
Martin Davis
[Von Neumann, 1945] von Neumann, John, First Draft of a Report on the EDVAC, Moore
School of Electrical Engineering, University of Pennsylvania, 1945. First printed in [Stern,
1981], pp. 177-246.
[Von Neumann, 1963] von Neumann, John, Collected Works, vol. 5, A.H. Taub (editor). Perg-
amon Press 1963.
[Welchman, 1982] Welchman, Gordon, The Hut Six Story, McGraw-Hill 1982.
[Weyl, 1944] Weyl, Hermann, “David Hilbert and His Mathematical Work,” Bulletin of the
American Mathematical Society, vol. 50(1944). pp. 612-654.
[White-Russ, 1925] Whitehead, Alfred North and Bertrand Russell, Principia Mathematica,
vol. I, second edition, Cambridge 1925.

Part II 
General 


WHAT IS A LOGICAL SYSTEM?
AN EVOLUTIONARY VIEW: 1964–2014
Dov M. Gabbay
Reader: Johan van Benthem
1
INTRODUCTION
In the past half century, there has been an increasing demand from many disci-
plines such as law, artiﬁcial intelligence, logic programming, argumentation, agent
theory, planning, game theory, social decision theory, mathematics, automated
deduction, economics, psychology, theoretical computer science, linguistics and
philosophy for a variety of logical systems. This was prompted by the extensive
applications of logic in these areas and especially in agent theory, linguistics, the-
oretical computer science, artiﬁcial intelligence and logic programming. In these
ﬁelds there is a growing need for a diversity of semantically meaningful and algo-
rithmically presented logical systems which can serve various applications. There-
fore renewed research activity is being devoted to analysing and tinkering with old
and new logics.
This activity has produced a shift in the notion of a logical system. Tradition-
ally a logic was perceived as a ‘consequence relation’ or a proof system between
sets of formulas. Problems arising in application areas have emphasised the need
for consequence relations between structures of formulas (such as multi sets, se-
quences or even richer structures). The general notion of a structured consequence
relation was put forward in [Gabbay, 1993a]. This ﬁner-tuned approach to the
notion of a logical system introduces new problems which called for an improved
general framework in which many of the new logics arising from computer science
applications can be presented and investigated.
This chapter is a systematic study of the notion of what is a logical system.1 It
will incrementally motivate a notion of logical system through the needs of various
1It is a description of a personal evolution of the author’s view of what is a logical system,
resulting from a systematic eﬀort to record and develop such systems.(The author was heavily
involved with the development of the new logics; in the past 50 years the author edited over
60 handbook volumes of applied logic, authored or coauthored over 35 research monographs,
published over 500 papers and founded and was editor in chief of 5 top journals in the ﬁeld). I
state my own views and those of the people in a similar school of thought. I make no attempt
to arrive at a sort of community opinion drawing on other strands in the literature (say, the
work of Barwise and Seligman and Johan van Benthem on what logic and logical systems are,
or the vast tradition of abstract model theory or category-theoretic approaches, etc. See http:
Handbook of the History of Logic. Volume 9: Computational Logic. 
Volume editor: Jörg Siekmann 
Series editors: Dov M. Gabbay and John Woods 
Copyright © 2014 Elsevier BV. All rights reserved.

42
Dov M. Gabbay
applications and applied logical activity. The chapter proposes an increasingly
more detailed and evolving image of a logical system. The initial position is that
of a logical system as a consequence relation on sets of formulas. Thus any set the-
oretical binary relation of the form ∆|∼Γ satisfying certain conditions (reﬂexivity,
monotonicity and cut) is a logical system. Such a relation has to be mathemat-
ically presented. This can be done either semantically, or set theoretically, or it
can be algorithmically generated. There are several options for the latter. Gen-
erate ﬁrst the {A|∅|∼A} as a Hilbert system and then generate {(∆, Γ)|∆|∼Γ}
or generate the pairs (∆, Γ) directly (via Gentzen rules) or use any other means
(semantical interpretations, dynamics, dialogues, games or other proof theories).
The concepts of a logical system, semantics and proof theory are not sharp
enough even in the traditional literature. There are no clear deﬁnitions of what is
a proof theoretic formulation of a logic (as opposed to, e.g. a decision procedure
algorithm) and what is e.g. a Gentzen formulation. Let us try here to propose a
working deﬁnition, only for the purpose of making the reader a bit more comfort-
able and not necessarily for the purpose of giving a deﬁnitive formulation of these
concepts.
• We start with the notion of a well formed formula of the language L of the
logic.
• A consequence relation is a binary relation on ﬁnite sets of formulas, ∆, Γ,
written as ∆|∼Γ, satisfying certain conditions, namely reﬂexivity, mono-
tonicity and cut.
• Such a relation can be deﬁned in many ways. For example, one can list all
pairs (∆, Γ such that ∆|∼Γ should hold. Another way is to give ∆, Γ to
some computer program and wait for an answer (which should always come).
• A semantics is an interpretation of the language L into some family of set
theoretical structures, together with a deﬁnition of the consequence relation
|∼in terms of the interpretation. What I have just said is not clear in itself
because I have not explained what ‘structures’ are and what an interpretation
is. Indeed, there is no clear deﬁnition of what a semantics is. In my book
[Gabbay, 1976], following Scott, I deﬁned a model as a function s giving each
wﬀof the language a value in {0, 1}. A semantics S is a set of models, and
∆|∼S Γ is deﬁned as the condition:
(∀s ∈S)[∀X ∈∆(s(X) = 1) →∃Y ∈Γ(s(Y ) = 1)]
//en.wikipedia.org/wiki/Abstract_model_theory and see references under Barwise and under
van Benthem. I am, however, optimistic, and I don’t think that reconciliation is impossible. I
emphasize the practice in AI/CS, and I draw general conclusions about what a logical system is.
Not all logicians will agree that one’s concept of logic has to be dominated by what happens in
the practice of CS/AI. See reference [Gabbay, 1994a]. Also note that the emphasis on deduction
as crucial in CS/AI might be contrasted with the Halpern Vardi manifesto which claims that
most signiﬁcant uses of logic in computer science revolve around model checking.

What is a Logical System?
43
• There can be algorithmic systems for generating |∼. Such systems are not
to be considered ‘proof theoretical systems’ for |∼. They could be decision
procedures or just optimal theorem proving machines.
• The notion of a proof system is not well deﬁned in the literature. There are
some recognised methodologies such as ‘Gentzen formulations’, ‘tableauxs’,
‘Hilbert style axiomatic systems’, but these are not sharply deﬁned.
For
our purpose, let us agree that a proof system is any algorithmic system for
generating |∼using rules of the form:
∆1 |∼Γ1, . . . , ∆n |∼Γn
∆|∼Γ
and ‘axioms’ of the form:
∅
∆|∼Γ
The axioms are the initial list of (∆, Γ) ∈|∼and the other rules generate
more. So a proof system is a particular way of generating |∼. Note that
there need not be structural requirement on the rule (that each involves a
main connective and some sub formulas, etc.).
A Hilbert formulation is a proof system where all the ∆s involved are ∅. A
Gentzen formulation would be a proof system where the rules are very nicely
structured (try to deﬁne something reasonable yourself; again, there is no
clear deﬁnition !). A Gentzen system can be viewed as a higher level Hilbert
system of the ‘connective’ ‘|∼’.
A tableaux formulation is a syntactical counter model construction relative to
some semantics. We have ∆|∼Γ if the counter model construction is ‘closed’,
i.e. must always fail. It is also possible to present tableaux formulations for
logics which have no semantics if the consequence |∼and the connectives
satisfy some conditions.
The central role which proof theoretical methodologies play in generating logics
compels us to put forward the view that a logical system is a pair (|∼, S|∼), where
S|∼is a proof theory for |∼. In other words, we are saying that it is not enough to
know |∼to ‘understand’ the logic, but we must also know how it is presented (i.e.
S|∼).
The next shift in our concept of a logic is when we observed from application
areas whose knowledge representation involves data and assumptions the need to
add structure to the assumptions and the fact that the reasoning involved relies on
and uses the structure. This view also includes non-monotonic systems. This led
us to develop the notion of Labelled Deductive Systems and adopt the view that
this is the framework for presenting logics. Whether we accept these new systems
as logics or not, classical logic must be able to represent them.

44
Dov M. Gabbay
The real departure from traditional logics (as opposed to just giving them more
structure) comes with the notion of aggregating arguments. Real human reason-
ing does aggregate argument (circumstantial evidence in favour of A as opposed
to evidence for ¬A) and what is known as quantitative (fuzzy =) reasoning system
make heavy use of that. Fortunately LDS can handle that easily. The section con-
cludes with the view that a proper practical reasoning system has ‘mechanisms’
for updates, inputs, abduction, actions, etc., as well as databases (theories, as-
sumptions) and that a proper logic is an integrated LDS system together with a
speciﬁc choice of such mechanisms.2
2
LOGICAL SYSTEMS AS CONSEQUENCE RELATIONS
Traditionally, to present a logic L, we need to preset ﬁrst the set of well formed
formulas of that logic. This is the language of the logic. We deﬁne the sets of atomic
formulas, connectives, quantiﬁers and the set of arbitrary formulas. Secondly we
deﬁne mathematically the notion of consequence namely, for a given set of formulas
∆and a given formula Q, we deﬁne the consequence relation ∆|∼L Q, reading ‘Q
follows from ∆in the logic L’.
The consequence relation is required to satisfy the following intuitive properties:
(∆, ∆′ abbreviates ∆∪∆′).
Reﬂexivity
∆|∼Q if Q ∈∆
Monotonicity
∆|∼Q implies ∆, ∆′ |∼Q
2My personal view in 1994 was that this is a logic, i.e. Logic = LDS system + several mecha-
nisms. In AI circles this might be called an agent. Unfortunately, the traditional logic community
were (in 1994) still very conservative in the sense that they have not even accepted non-monotonic
reasoning systems as logics yet. They believe that all this excitement is transient, temporarily
generated by computer science and that it will ﬁzzle out sooner or later. They believe that we
will soon be back to the old research problems, such as how many non-isomorphic models does
a theory have in some inaccessible cardinal or what is the ordinal of yet another subsystem of
analysis. I think this is ﬁne for mathematical logic but not for the logic of human reasoning.
There is no conﬂict here between the new and the old, just further evolution of the subject. We
shall see later that a more reﬁned view is called for in 2014.

What is a Logical System?
45
Transitivity (cut)3:
∆|∼A; ∆, A |∼Q imply ∆|∼Q
The consequence relation may be deﬁned in various ways. Either through an
algorithmic system S|∼, or implicitly by postulates on the properties of |∼.
Thus a logic is obtained by specifying L and |∼. Two algorithmic systems S1
and S2 which give rise to the same |∼are considered the same logic.
If you think of ∆as a database and Q as a query, then reﬂexivity means that
the answer is yes to any Q which is oﬃcially listed in the database. Monotonicity
reﬂects the accumulation of data, and transitivity is nothing but lemma generation,
namely if ∆|∼A, then A can be used as a lemma to obtain B from ∆.
The above properties seemed minimal and most natural for a logical system to
have, given that the main applications of logic were in mathematics and philosophy.
The above notion was essentially put forward by [Tarski, 1936] and is referenced
to as Tarski consequence.
[Scott, 1974], following ideas from [Gabbay, 1969],
generalised the notion to allow Q to be a set of formulas Γ. The basic relation is
then of the form ∆|∼Γ, satisfying:
Reﬂexivity
∆|∼Γ if ∆∩Γ ̸= ∅
Monotonicity
∆|∼Γ
∆, ∆′ |∼Γ
Transitivity (cut)
∆, A |∼Γ; ∆′ |∼A, Γ′
∆′, ∆|∼Γ, Γ′
Scott has shown that for any Tarski consequence relation there exist two Scott
consequence relations (a maximal one and a minimal one) that agree with it (see
my book [Gabbay, 1986]).
3There are several versions of the cut rule in the literature, they are all equivalent for the
cases of classical and intuitionistic logic but are not equivalent in the context of this section. The
version in the main text we call transitivity (lemma generation). Another version is
Γ |∼A; ∆, A |∼B
∆, Γ |∼B
This version implies monotonicity, when added to reﬂexivity.
Another version we call internal cut:
∆, A |∼Γ; ∆|∼A, Γ
∆|∼Γ
A more restricted version of cut is unitary cut:
∆|∼A; A |∼Q
∆|∼Q

46
Dov M. Gabbay
The above notions are monotonic. However, the increasing use of logic in arti-
ﬁcial intelligence has given rise to logical systems which are not monotonic. The
axiom of monotonicity is not satisﬁed in these systems.
There are many such
systems, satisfying a variety of conditions, presented in a variety of ways. Fur-
thermore, some are proof theoretical and some are model theoretical. All these
diﬀerent presentations give rise to some notion of consequence ∆|∼Q, but they
only seem to all agree on some form of restricted reﬂexivity (A |∼A). The essen-
tial diﬀerence between these logics (commonly called non-monotonic logics) and
the more traditional logics (now referred to as monotonic logics) is the fact that
∆|∼A holds in the monotonic case because of some ∆A ⊆∆, while in the non-
monotonic case the entire set ∆is used to derive A. Thus if ∆is increased to
∆′, there is no change in the monotonic case ,while there may be a change in the
non-monotonic case.4
The above describes the situation current in the early 1980s. We have had a
multitude of systems generally accepted as ‘logics’ without a unifying underlying
theory and many had semantics without proof theory. Many had proof theory
without semantics, though almost all of them were based on some sound intu-
itions of one form or another. Clearly there was the need for a general unifying
framework. An early attempt at classifying non-monotonic systems was [Gabbay,
1985]. It was put forward that basic axioms for a consequence relation should be
reﬂexivity, transitivity (cut) and restricted monotonicity, namely:
Restricted monotonicity
∆|∼A; ∆|∼B
∆, A |∼B
A variety of systems seem to satisfy this axiom. Further results were obtained
[Kraus et al., 1990; Lehman and Magidor, 1992; Makinson, 1989; W´ojcicki, 1988;
W´ojcicki, 1988] and the area was called ‘axiomatic theory of the consequence
relate’ by W´ojcicki.5
Although some classiﬁcation was obtained and semantical results were proved,
the approach does not seem to be strong enough. Many systems do not satisfy
restricted monotonicity. Other systems such as relevance logic, do not satisfy even
reﬂexivity. Others have richness of their own which is lost in a simple presentation
as an axiomatic consequence relation. Obviously a diﬀerent approach is needed,
one which would be more sensitive to the variety of features of the systems in
the ﬁeld. Fortunately, developments in a neighbouring area, that of automated
deduction, seem to give us a clue.
This is the 1994 view. This view was modiﬁed after 2009, see [D’Agostino and
Floridi, 2009; D’Agostino et al., 2013; D’Agostino and Gabbay, 2014].
4These systems arise due to various practical mechanisms compensating for lack of reasoning
resources. See for example [Gabbay and Woods, 2008].
5In general, the exact formulations of transitivity and reﬂexivity can force some form of
monotonicty.

What is a Logical System?
47
3
LOGICAL SYSTEMS AS ALGORITHMIC PROOF SYSTEMS
The relative importance of automated deduction is on the increase, in view of
its wide applicability. New automated deduction methods have been developed
for non-classical logics, and resolution has been generalised and modiﬁed to be
applicable to these logics.
In general, because of the value of those logics in
theoretical computer science and artiﬁcial intelligence, a greater awareness of the
computational aspects of logical systems is developing and more attention being
devoted to proof theoretical presentations. It became apparent to us that a key
feature in the proof theoretic study of these logics is that a slight natural variation
in an automated or proof theoretic system of one logic (say L1) , can yield another
logic (say L2).
Although L1 and L2 may be conceptually far apart (in their philosophical mo-
tivation, and mathematical deﬁnitions) when we given them a proof theoretical
presentation, they turn out to be brother and sister. This kind of relationship is
not isolated and seems to be widespread. Furthermore, non-monotonic systems
seem to be obtainable from monotonic ones through variations on some of their
monotonic proof theoretical formulation.
This seems to give us some handle on classifying non-monotonic systems.
This phenomenon has prompted us to put forward the view that a logical system
L is not just the traditional consequence relation |∼(monotonic or non-monotonic)
but a pair (|∼, S|∼), where |∼is a mathematically deﬁned consequence relation (i.e.
the set of pairs (∆, Γ) such that (∆|∼Γ) satisfying whatever minimal conditions
on a consequence one happens to agree to, and S|∼is an algorithmic system for
generating all those pairs [Gabbay, 1992]. Thus according to this deﬁnition classi-
cal propositional logic |∼perceived as a set of tautologies together with a Gentzen
system S|∼is not the same as classical logic together with the two valued truth
table decision procedure T|∼for it. In our conceptual framework, (|∼, S|∼) is not
the same logic as (|∼, T|∼).
To illustrate and motivate our way of thinking, observe that it is very easy
to move from T|∼for classical logic to a truth table system Tn
|∼for  Lukasiewicz
n-valued logic. It is not so easy to move an algorithmic system for intuitionistic
logic. In comparison, for a Gentzen system presentation, exactly the opposite is
true. Intuitionistic and classical logics are neighbours, while  Lukasiewicz logics
seem completely diﬀerent.
In fact for a Hilbert style or Gentzen style formu-
lations, one can show proof theoretic similarities between  Lukaseiewicz’s inﬁnite
valued logic and Girard’s linear logic, which in turn is proof theoretically similar
to intuitionistic logic.
This issue has a bearing on the notion of ‘what is a classical logic’. Given an
algorithmic proof system S|∼c for classical logic |∼c, then (|∼c, S|∼c) is certainly
classical logic. Now suppose we change S|∼c a bit by adding heuristics to obtain S′.
The heuristics and modiﬁcations are needed to support an application area. Can
we still say that we are essentially in ‘classical logic’? I suppose we can because S′
is just a slight modiﬁcation of S|∼c. However, slight modiﬁcations of an algorithmic

48
Dov M. Gabbay
system may yield another well known logic. So is linear logic essentially classical
logic, slightly modiﬁed, or vice versa?
We give an example from goal directed implicational logic. Consider a language
with implication only. It is easy to see that all wﬀs A have the form A1 →(A2 →
. . . →(An →q) . . .), q atomic, where Ai has the same form as A. We now describe
a computation with database a multi set ∆of wﬀs of the above form and goal a
wﬀof the above form. We use the metapredicate ∆⊢A to mean the computation
succeeds, i.e. A follows from ∆. Here are the rules:
1. ∆, q ⊢q, q atomic and ∆empty. (Note that we are not writing A ⊢A for
arbitrary A. We are not writing a Gentzen system.)
2. ∆⊢A1 →(A1 →. . . →(An →q) . . .) if ∆∪{A1, . . . , An} ⊢q. Remember
we are dealing with multisets.
3. ∆′ = ∆∪{A1 →(A2 →. . . (An →q) . . .)} ⊢q if ∆= ∆1 ∪∆2 ∪. . . ∪
∆n, ∆i, i = 1, . . . , n are pairwise disjoint and ∆i ⊢Ai.
The above computation characterises linear implication. If we relinquish the side
condition in (3) and let ∆i = ∆′ and the side condition (1) that ∆is empty, we
get intuitionistic implication.
The diﬀerence in logics is serious. In terms of proof methodologies, the diﬀerence
is minor. More examples in [Gabbay, 1992].
Given a consequence |∼we can ask for a theory ∆and a wﬀA, how do we check
when ∆|∼A? We need an algorithmic proof system S|∼. Here are some examples
of major proof methodologies:
• Gentzen
• Tableaux
• Semantics (eﬀective truth tables)
• Goal directed methodology
• Resolution
• Dialogue systems
• Game theoretic interpretation
• Labelled Deductive systems, etc.
To summarise, we have argued that a logical system is a pair (|∼, S|∼), where |∼is
a consequence relation and S|∼is an algorithmic metapredicate S|∼(∆, A) which
when succeeding means that ∆|∼A.
The reasons for this claim are:

What is a Logical System?
49
•
•
•
•
??
Classical
logic
Intuitionistic
logic
 Lukasiewicz
inﬁnite valued
logic
Truth
tables
Gentzen
??
Figure 1. Logics landscape
1. We have an intuitive recognition of the diﬀerent proof methodologies and
show individual preferences to some of them depending on the taste and the
need for applications.
2. Slight variations in the parameters of the proof systems can change the logics
signiﬁcantly. (For a systematic example of this see [Gabbay and Olivetti,
2000].)
Figure 1 is an example of such a relationship.
In the truth table methodology classical logic and  Lukasiewicz logic are a slight
variation of each other. They are not so close to intuitionistic logic. In the Gentzen
approach, classical and intuitionistic logics are very close while  Lukasiewicz logic
is a problem to characterise.
This evidence suggests strongly that the landscape of logics is better viewed as
a two-dimensional grid.6
4
LOGICAL SYSTEMS AS ALGORITHMIC STRUCTURED
CONSEQUENCE RELATIONS
Further observation of ﬁeld examples shows that in many cases the database is not
just a set of formulas but a structured set of formulas. The most common is a list
or multiset.7 Such structures appear already in linear and concatenation logics
6See however, our book [Metcalfe et al., 2008].
7Classical logic cannot make these distinctions using conjunction only. It needs further anno-
tation or use of predicates.

50
Dov M. Gabbay
and in many non-monotonic systems such as priority and inheritance system. In
many algorithmically presented systems much use is made (either explicitly or
implicitly) of this additional structure.
A very common example is a Horn clause program. The list of clauses
(a1)
q
(a1)
q →q
does not behave in the same way as the list
(b1)
q →q
(b2)
q
The query ?q succeeds from one and loops from the other.
It is necessary to formulate axioms and notions of consequence relations for
structures. This is studied in detail in [Gabbay, 1993a]. Here are the main features:
• Databases (assumptions) are structured. They are not just sets of formulas
but have a more general structure such as multisets, lists, partially ordered
sets, etc. To present a database formally, we need to describe the structures.
Let M be a class of structures (e.g. all ﬁnite trees). Then a database ∆has
the form ∆= (M, f), where M ∈M and f : M 7→wﬀ’s, such that for each
t ∈M, f(t) is a formula. We assume the one point structure {t} is always in
M. We also assume that we know how to take any single point t ∈M out
of M and obtain (M ′, f ′), f ′ = f ↾M. This we need for some versions of the
cut rule and the deduction theorem.
• A structured-consequence relation |∼is a relation ∆|∼A between struc-
tured databases ∆and formulas A. (We will not deal with structured conse-
quence relations between two structured databases ∆|∼Γ here. See [Gabbay,
1993a].)
• |∼must satisfy the minimal conditions, namely
Identity
{A} |∼A
Surgical cut
∆|∼A; Γ[A] |∼B
Γ[∆] |∼B
where Γ[A] means that A resides somewhere in the structure Γ and Γ[∆]
means that ∆replaces A in the structure. These concepts have to be deﬁned
precisely. If ∆= (M, f1) and ∆= (M2, f2) then Γ[A] displays the fact that
for some t ∈M2, f2(t) = A. We also allow for the case that M2 = f2 = ∅
(i.e. taking A out). We need a notion of substitution, which is a three place
function Sub(Γ, ∆, t), meaning that for t ∈M2 we substitute M1 in place

What is a Logical System?
51
of t. This gives us a structure (M3, f3) according to the deﬁnition of Sub.
(M3, f3) is displayed as Γ[∆], and Γ[∅] displays the case of taking A out.
Many non-monotonic systems satisfy a more restricted version of surgical
cut:
Γ[∅/A] |∼A; Γ[A] |∼B
Γ[Γ[∅/A]] |∼B
Another variant would be
Deletional cut
Γ[∅/A] |∼A; Γ[A] |∼B
Γ[∅/A] |∼B
• A logical system is a pair (|∼, S|∼), where |∼is a structured-consequence and
S|∼is an algorithmic system for it.
5
ALGORITHMIC APPROXIMATIONS OF LOGICAL SYSTEMS
Logical systems are idealizations and, as such, not intended to faithfully describe
the actual deductive behaviour of rational agents. As Gabbay and Woods put it:
A logic is an idealization of certain sorts of real-life phenomena. By their very nature, idealizations
misdescribe the behaviour of actual agents. This is to be tolerated when two conditions are met.
One is that the actual behaviour of actual agents can defensibly be made out to approximate
to the behaviour of the ideal agents of the logician’s idealization. The other is the idealization’s
facilitation of the logician’s discovery and demonstration of deep laws.
[Gabbay and Woods,
2001, p. 158]
This should not necessarily be intended as a plea for a more descriptive approach to
the actual inferential behaviour of agents that takes into account their “cognitive
biases”. Even from a prescriptive viewpoint, the requirements that Logic imposes
on agents are too strong, since it is known that most interesting logics are either
undecidable or (likely to be) computationally intractable. Therefore we cannot
assume any realist agent to be always able to recognize the logical consequences
of her assumptions or to realize that such assumptions are logically inconsistent.
This raises what can be called the approximation problem that, in the context of
logical systems, can be concisely stated as follows:
PROBLEM 1 (Approximation Problem). Can we deﬁne, in a natural way, a hi-
erarchy of logical systems that indeﬁnitely approximate a given idealized Logic in
such a way that, in all practical contexts, suitable approximations can be taken as
prescriptive models of the inferential power of realistic, resource-bounded agents?
Stable solutions to this problem are likely to have a signiﬁcant practical impact
in all research areas — from knowledge engineering to economics — where there is
an urgent need for more realistic models of deduction. From this point of view, we
now claim that a logical system is not only given by a logic L and an algorithmic
presentation A of it, but must include also a deﬁnition of how the ideal logic L
can be approximated in practice by realistic agents (whether human or artiﬁcial).

52
Dov M. Gabbay
This idea has occasionally received some attention in Computer Science and Ar-
tiﬁcial Intelligence [Cadoli and Schaerf, 1992; Dalal, 1996; Dalal, 1998; Crawford
and Etherington, 1998; Sheeran and Stalmarck, 2000; Finger, 2004; Finger, 2004b;
Finger and Wasserman, 2004; Finger and Wassermann, 2006; Finger and Gabbay,
2006] but comparatively little attention has been devoted to embedding such ef-
forts in a systematic proof-theoretical and semantic framework. In [D’Agostino
et al., 2013], starting from ideas put forward in [D’Agostino and Floridi, 2009],
the authors aimed to ﬁll this gap and propose a unifying approach. They also
argue that traditional Gentzen-style presentations of classical logic are not apt to
address the approximation problem and that its solution must therefore involve an
imaginative re-examination of the proof-theory and semantics of a logical system
as they are usually presented in the literature.
6
LOGICAL SYSTEMS AS LABELLED DEDUCTIVE SYSTEMS
Of course we continue to maintain our view that diﬀerent algorithmic systems for
the same structured consequence relation deﬁne diﬀerent logics. Still although we
now have a fairly general concept of a logic, we do not have a general framework.
Monotonic and non-monotonic systems still seem conceptually diﬀerent. There
are many diverse examples among temporal logics, modal logics, defeasible logics
and more. Obviously, there is a need for a more unifying framework. The question
is, can we adopt a concept of a logic where the passage from one logic to another is
natural, and along predeﬁned acceptable modes of variation? Can we put forward a
framework where the computational aspects of a logic also play a role? Is it possible
to ﬁnd a common home from a variety of seemingly diﬀerent techniques introduced
for diﬀerent purposes in seemingly diﬀerent intellectual logical traditions?
To ﬁnd an answer, let us ask ourselves what makes one logic diﬀerent from
another? How is a new logic presented and described and compared to another?
The answer is obvious. These considerations are performed in the metalevel. Most
logics are based on modus ponens anyway. The quantiﬁer rules are formally the
same anyway and the diﬀerences between them are metalevel considerations on the
proof theory or semantics. If we can ﬁnd a mode of presentation of logical systems
where metalevel features and semantic parts can reside side by side with object
level features then we can hope for a general framework. We must be careful here.
In the logical community the notions of object level vs. metalevel are not so clear.
Most people think of naming and proof predicates in this connection. This is not
what we mean by metalevel here. We need a more reﬁned understanding of the
concept. There is a similar need in computer science.
We found that the best framework to put forward is that of a Labelled Deductive
System, LDS, see [Gabbay, 1996]. Our notion of what is a logic is that of a pair
(|∼, S|∼), where |∼is a structured (possibly non-monotonic) consequence relation
on a language L and S|∼is an LDS, and where |∼is essentially required to satisfy
no more than Identity (i.e. {A} |∼A) and a version of Cut. This is a reﬁnement
of our concept of a logical system presented in [Gabbay, 1992]. We now not only

What is a Logical System?
53
say that a logical system is a pair (|∼, S|∼), but we are adding that S|∼itself has
a special presentation, that of an LDS.
As a ﬁrst approximation, we can say that an LDS system is a triple (L, A, M),
where L is a logical language (connectives and wﬀs) and A is an algebra (with
some operations) of labels and M is a discipline of labelling formulas of the logic
(from the algebra of labels A), together with deduction rules and with agreed ways
of propagating the labels via the application of the deduction rules. The way the
rules are used is more or less uniform to all systems.
To present an LDS system we ﬁrst need to deﬁne its set of formulas and its set
of labels.
For example, we can take the language of classical logic as the formula (with
variables, constants and quantiﬁers) and take some set of function symbols on the
same variables and constants as generating the labels. More precisely, we allow
ordinary formulas of predicate logic with quantiﬁers to be our LDS formulas. Thus
∃xA(x, y) is a formula with free variable y and bound variable x. To generate the
labels, we start with a new set of function symbols t1(y), t2(x, y), . . . of various
arities which can be applied to the same variables which appear in the formulas.
Thus the labels and formulas can share variables, or even some constants and
function symbols. In other words, in some applications, it might be useful to allow
some labels to appear inside formulas A. We can form declarative units of the
form t1(y) : ∃xA(x, y). When y is assigned a value y = a, so is the label and we
get t1(a) : ∃A(x, a). The labels should be viewed as more information about the
formulas, which is not coded inside the formula (hence dependence of the labels
on variables x makes sense as the extra information may be diﬀerent for diﬀerent
x.) A formal deﬁnition of an algebraic LDS system will be given later, meanwhile,
let us give an informal deﬁnition of an LDS system and some examples which help
us understand what and why we would want labels.8
8The idea of annotating formulas for various purposes is not new. A. R. Anderson and N.
Belnap in their book on Entailment, label formulas and propagate labels during proofs to keep
track of relevance of assumptions. Term annotations (Curry-Howard formula as type approach)
are also known where the propagation rules are functional application. The Lambek Calculus
and the categorial approach is also related to labelling. the extra arguments sometimes present
in the Demo predicate of metalogic programming are also a form of labelling. What is new is
that we are proposing that we use an arbitrary algebra for the labels and consider the labelling
as part of the logic. We are creating a discipline of LDS and claiming that we have a unifying
framework for logics and that almost any logic can be given an LDS formulation. We can give
|∼an LDS formulation provided |∼is reﬂexive and transitive and each connective is either |∼
monotonic or anti monotonic in each of its arguments. See [Gabbay, 1993]. We are claiming that
the notion of a logic is an LDS. This is not the same as the occasional use of labelling with some
speciﬁc purpose in mind. We are translating and investigating systematically all the traditional
logical concepts into the context of LDS an generalising them.
I am reminded of the story of the Yuppy who hired an interior decorator to redesign his sitting
room. After much study, the decorator recommended that the Yuppy needed a feeling of space
and so the best thing to do is to arrange the furniture against the wall, so that there will be a
lot of space in the middle. The cleaning person, when she/he ﬁrst saw the new design was very
pleased. She/he thought it was an excellent idea. ‘Yes; said the Yuppy, ‘and I paid ‘£1000 for
it’. ‘That was stupid’, exclaimed the cleaning person, ‘I could have told you for free! I arrange
the furniture this way every time I clean the ﬂoor!’.

54
Dov M. Gabbay
DEFINITION 2 (Prototype LDS system). Let A be a ﬁrst-order language of the
form A = (A, R1, . . . , Rk, f1, . . . , fm) where A is the set of terms of the algebra
(individual variables and constants) and Ri are predict symbols (on A, possibly
binary but not necessarily so) and f1, . . . , fm are function symbols (on A) of various
arities. We think of the elements of A as atomic labels and of the functions as
generating more labels and of the predicates as giving additional strutter to the
labels. A typical example would be (A, R, f1, f2) where R is binary and f1, f2 are
unary.
A diagram of labels is a set M containing elements generated from A by the
function symbols together with formulas of the form ±R(t1, . . . , tk), where ti ∈M
and R is a predicate symbol of the algebra.
Let L be a predicate language with connectives ♯1, . . . , ♯n, of various arities,
with quantiﬁers and with the same set of atomic terms A as the algebra.
We deﬁne the notions of a declarative unit, a database and a label as follows:
1. An atomic label is any t ∈A. A label is any term generated from the atomic
labels by the symbols f1, . . . , fm
2. A formula is any formula of L.
3. A declarative unit is a pair t : A, where t is a label and A is a formula.
4. A database is either a declarative unit, or has the form (a, M, f), where M
is a ﬁnite diagram of labels, a ∈M is the distinguished label, and f is a
function associating with each label t in M either a database or a ﬁnite set
of formulas. (Note that this is a recursive clause. We get simple databases if
we allow f to associate with each label t only single or ﬁnite sets of formulas.
Simple databases are adequate for a large number of applications.)
Deﬁnition 2 is simpliﬁed.
To understand it intuitively, think of the atomic
labels as atomic places and times (top of the hill, 1 January, 1992, etc.)
and
the function symbols as generating more labels, namely more times and more
places (behind(x), day after(t), etc.). We form declarative units by taking labels
and attaching formulas to them. Complex structures (a, M, f) of these units are
databases. This deﬁnition can be made more complex. Here the labels are terms
generated by function symbols form atomic labels. We can complicate matters by
using databases themselves as labels. This will give us recursively more complex,
richer labels. We will not go into that now. The ﬁrst simpliﬁcation is therefore that
we are not using databases as labels. The second simpliﬁcation is that we assume
constant domains. All times and places have the same elements (population) on
them. If this were not the case we would need a function Ut giving the elements
residing in t, and a database would have the form (A, M, f, Ut).
EXAMPLE 3. Consider a language with the predicate VS900(x, t). This is a two-
sorted predicate, denoting Virgin airline ﬂight London–Tokyo, where t is the ﬂight
Of course she/he is right, but she/he used the idea of the new arrangement purely as a side
eﬀect!

What is a Logical System?
55
date and x is the name of an individual. For example VS900 (Dov, 15.11.91) may
be put in the database denoting that Dov is booked on this ﬂight scheduled to
embark on 15.11.91.
If the airline practices overbooking and cancellation procedures (whatever that
means), it might wish to annotate the entries by further useful information such
as
• Time of booking;
• Individual/group travel booking;
• Type of ticket;
• ± VIP.
This information may be of a diﬀerent nature to that coded in the main predicate
and it is therefore more convenient to keep it as an annotation, or label. It may
also be the case that the manipulation of the extra information is of a diﬀerent
nature to that of the predicate.
In general, there may be many uses for the label t in the declarative unit t : A.
Here is a partial list:
• Fuzzy reliability value:
(a number x, 0 ≤x ≤1.) Used mainly in expert systems.
• Origin of A:
t indicates where the input A came from. Very useful in complex databases.
• Priority of A:
t can be a date of entry of updates and a later date (label) means a higher
priority.
• Time when A holds:
(temporal logic)
• Possible world where A holds:
(modal logic)
• t indicates the proof of A:
(which assumptions were used in deriving A and the history of the proof).
This is a useful labelling for Truth Maintenance Systems.
• t can be the situation and A the infon (of situation semantics)
EXAMPLE 4. Let us look at one particular example, connected with modal logic.
Assume the algebra A has the form (A, <), with a set of atomic labels A, no
function symbols and a binary relation <. According to the previous deﬁnition, a
diagram of labels would contain a (ﬁnite) set M ⊆A, together with a set of pairs

56
Dov M. Gabbay
of the form {t < s}, t, s, ∈M. A database has the form (a, M, f) where M is a
ﬁnite diagram and f is a function, say giving a formula At = f(t), for each t ∈M.9
The perceptive reader may feel resistance to the idea of the label at this stage.
First be assured that you are not asked to give up your favourite logic or proof
theory, nor is there any hint of a claim that your activity is now obsolete. In
mathematics a good concept can rarely be seen or studied from one point of view
only, and it is a sign of strength to have several views connecting diﬀerent concepts.
So the traditional logical views are as valid as ever and add strength to the new
point of view. In fact, manifestations of our LDS approach already exist in the
literature in various forms, they were locally regarded as convenient tools and
there was not the realisation that there is a general framework to be studied and
developed. None of us is working in a vacuum and we build on each other’s work
Further, the existence of a general framework in which any particular case can be
represented does not necessarily mean that the best way to treat that particular
case is within the general framework. Thus if some modal logics can be formulated
in LDS, this does not mean that in practice we should replace existing ways of
treating the logics by their LDS formulation. The latter may not be the most
eﬃcient for those particular logics. It is suﬃcient to show how the LDS principles
specialise and manifest themselves in the given known practical formulation of the
logic.
The reader may further have doubts about the use of labels from the computa-
tional point of view. What do we mean by a unifying framework? Surely a Turing
machine can simulate any logic. Is that a unifying framework? The use of labels
is powerful, as we know from computer science. Are we using labels to play the
role of a Turing machine? The answer to the question is twofold. First that we are
not operating at the metalevel but at the object level (see point 4 below). Second,
there are severe restrictions on the way we use LDS. Here is a preview:
1. The only rules of inference allowed are the traditional ones, modus ponens
and some form of deduction theorem for implication, for example.
2. Allowable modes of label propagation are ﬁxed for all logics. They can be
adjusted in agreed ways to obtain variations but in general the format is the
same. For example, it has the following form for implications:
Let LDS։ be a particular LDS system with labels A, and with ։ a special
implication characteristic to this particular LDS system. Then there exists
a ﬁxed set of labels Γ, which can characterise ։ as follows. For any theory
∆(of labelled wﬀs) we have: ∆proves (A ։ B) with label t iﬀ∀x ∈∆[B
can be proved from ∆and x : A with label t + x], where Γ is a set of labels
characterising the implication in that particular logic. For example Γ may
be restricted to atomic labels only, or to labels related to t, or some other
restrictions. The freedom that diﬀerent logics have is in the choice of Γ and
9Note that this modal logic LDS brings part of the semantics into the syntax; the labels are
possible worlds!

What is a Logical System?
57
the (possibly not only equational) properties of ‘+’. For example we can
restrict the use of modus ponens by a wise propagation of labels.
3. The quantiﬁer rules are the same for all logics.
4. Metalevel feature are implemented via the labelling mechanism, which is
object language.
The reader who prefers to remain within the traditional point of view of
assumptions (data) proving a conclusion
can view the labelled formulas as another form of data.
There are many occasions when it is most intuitive to present an item of data
in the form t : A, where t is a label and A is a formula. The common underlying
reason for the use of the label t is that t represents information which is needed to
modify A or to supplement (the information in) A with information which is not of
the same type or nature as (the information represented by) A itself. A is a logical
formula representing information declaratively, and the additional information of
t can certainly be added declaratively to A to form A′, however, we may ﬁnd it
convenient to put forward the additional information through the label t as part
of a pair t : A.
Take, for example, a source of information which is not reliable. A natural way
of representing an item of information from that source is t : A, where A is a
declarative presentation of the information itself and t is a number representing
its reliability. Such expert systems exist (e.g. Mycin) with rules which manipulate
both t and A as one unit, propagating the reliability values t through applications
of modus ponens. We may also use a label among the source of information and
this would give us a qualitative idea of its reliability.
Another area where it is natural to use labels is in reasoning from data and
rules. If we want to keep track, for reasons of maintaining consistency and/or
integrity constraints, of where and how a formula was deduced, we use a label t.
In this case, the label in t : A can be the part of the data which was used to get
A. Formally in this case t is a formula, the conjunction of the data use.d We thus
get pairs of the form ∆i : Ai, where Ai are formulas and ∆i are the parts of the
database from which Ai was derived.
A third example where it is natural to use labels is time stamping of data.
Where data is constantly revised and updated, it is important to time stamp the
data items. Thus the data items would look like ti : Ai, where ti are time stamps.
Ai itself may be a temporal formula. Thus there are two times involved, the logical
time si in Ai(si) and the time stamping ti of Ai. For reasons of clarity, we may
wish to regard ti as a label rather than incorporate it into the logic (by writing,
for example A∗(ti, si)).
To summarise then, we replace the traditional notion of consequence between
formulas of the form A1, . . . , An ⊢B by the notion of consequence between labelled
formulas
t1 : A1, t2 : A2, . . . , tn : An ⊢s : B

58
Dov M. Gabbay
Depending on the logical system involved, the intuitive meaning of the labels varies.
In querying databases, we may be interested in labelling the assumptions so that
when we get an answer to a query, we can record, via the label of the answer, from
which part of the database the answer was obtained. Another area where labelling
is used is temporal logic. We can time stamp assumptions as to when they are true
and query, given those assumption, whether a certain conclusion will be true at
a certain time. Thus the consequence notion for labelled deduction is essentially
the same as that of any logic: given assumptions does a conclusion follow?
Whereas in the traditional logical system the consequence is deﬁned using proof
rules on the formulas, in the LDS methodology the consequence is deﬁned by
using rules on both formulas and their labels. Formally we have formal rules for
manipulating labels and this allows for more scope in decomposing the various
features of the consequence relation. The metafeatures can be reﬂected in the
algebra or logic of the labels and the object features can be reﬂected in the rules
of the formulas. Recall, however, that there are severe restrictions on how we use
LDS rules, as we discussed earlier.
The notion of a database or of a ‘set of assumptions’ also has to be changed.
A database is a hierarchical conﬁguration of labelled formulas. The conﬁguration
depends on the labelling discipline.
For example, it can be a linearly ordered
set {a1 : aA1, . . . ,n : Am}, a1 < a2 < . . . < an.
The proof discipline for the
logic will specify how the assumptions are to be used. See, for example, the logic
programming case study.
We summarise our (1994) position on what is a logical system. A logical system
is a pair (|∼, LDS|∼), where |∼is a consequence relation between labelled databases
∆and declarative units t : A and LDS|∼is an algorithmic system for |∼.
We need one more component to the notion of a logical system. In previous
subsections, a logical system was presented as (|∼, S|∼), where |∼is a structured
consequence relation satisfying Identity and Surgical Cut and S|∼is an algorithmic
proof system for computing |∼. We are now saying that we need to reﬁne this
notion and deal with Labelled Deductive Systems, where |∼is a consequence relation
between labelled databases ∆and declarative units t : A and that S|∼is replaced by
some speciﬁc LDS discipline (algorithm) for computing the above. We need to be
able to retrieve the old notion i.e. (|∼1, S|∼1) from the new notion (|∼2, LDS|∼2).
In other words, we must add into LDS the capability of proving and reasoning
without labels. To achieve this we can ﬁrst reason with labels and then strip the
labels and give a conclusion without labels. The additional algorithm which we
can use to strip the labels is called ﬂattening. Thus a labelled theory ∆may prove
ti : A and si : ¬A with many diﬀerent labels ti and si, depending on various
labelling considerations and proof paths. The ﬂattening algorithm will allow us to
decide whether we ﬂatten the pair of sets ({ti}, {si}) to + or −, i.e. whether we
say ∆|∼A or ∆|∼¬A.
For example if the labels represent moments of time or priorities, we may say
the value is + if max{ti} ≥max{sj}. Or we may interlace the ﬂattening with the
deduction itself.

What is a Logical System?
59
Thus, given a structured theory ∆(without labels) and a candidate A, we can
have the following procedure, using LDS, for deciding whether ∆|∼?A.
• Label the elements of ∆with completely diﬀerent atomic labels, representing
the existing structure in ∆.
• Use the LDS machinery to deduce all possibilities ∆|∼ti : A and ∆|∼si :
¬A.
• Flatten and get A (or interlace with ﬂattening and get A).
EXAMPLE 5. Here is an example of interlacing. The database has
t1 : A
t2 : ¬A
t3 : ¬A →B
t4 : A →¬B.
Assume priority is t1 < t2 < t3 < t4, and assume a ﬂattening process which
gives higher priority rules superiority over low priority rules and similarly for facts
but gives lexicographic superiority for rules over facts. Thus t4t1 is stronger than
t3t2. If we deduce and then ﬂatten, we get
t4t1 : ¬B
t3t2 : B
The ﬂattening process would take ¬B.
If we pursue an interlace argument, we ﬁrst ﬂatten the premisses and take ¬A
and then perform the modus ponens and get B.
7
AGGREGATED SYSTEMS
So far all our logical systems have either proved or not proved a conclusion A.
There was no possibility of aggregating arguments in favour of A as against argu-
ments in favour of negation of A. The lack of aggregation is a basic characteristic
which currently separates the symbolic, qualitative school of reasoning from the
numerical, quantitative one.
There are many systems around (many are recognised as probabilistic systems,
expert systems, fuzzy systems) which attach numerical values to assumptions and
rules, use various functions and numerical procedures to manipulate these values
and formulas and reach conclusions.
In many cases we get systems which give answers which seem to make sense,
which can be very successfully and proﬁtably applied but which cannot be recog-
nised or understood by traditional logic. The main feature common to all of these
numerical systems (which is independent of how they calculate and propagate their
values) is that their ‘proofs’ aggregate. They can add the numbers involved and

60
Dov M. Gabbay
thus aggregate arguments. The spirit is: ﬁve good rumours are better than one
proof.
To further illustrate, consider the following example:
EXAMPLE 6. The assumptions are
t1 : A →C
t2 : B →C
t3 : A
t4 : B
t5 : D →¬C
t6 : D
Here we can conclude C in two diﬀerent ways and conclude ¬C in one way.
Non-monotonic systems like defeasible logic will not allow us to draw any con-
clusion unless one rule defeats all others. If we had a numerical evaluation of the
at a, say ti are numbers in [0 1], then we could aggregate our conﬁdence in the
conclusion. Thus we get
(t1 · t3 + t2 · t4) : C
t5 · t6 : ¬C
the two numbers can be compared and a conclusion reached.
If we operate in the context of LDS, we can use the labels to aggregate argu-
ments. Any conclusion is proved with a label indicating its proof path. These can
be formally (algebraically) added (aggregated) and an additional process (called
ﬂattening) can compare them.
In consequence relation terms, the property of aggregation destroys the cut rule.
The reason is as follows:
Assume ∆, A |∼B. This now means that the aggregated proofs in favour of B
are stronger than the aggregated proofs in favour of ¬B. Similarly ∆|∼A would
mean the balance from Γ is in favour of A.
If we perform the cut rule we get
∆, Γ |∼?B
∆and Γ may interact, forming new additional proofs of ¬B, which outweigh the
proofs for B.
Cut is a very basic rule in traditional logical systems and can be found in
one form or another in each one of them. Thus it is clear that aggregation of
arguments is a serious departure from traditional formal logic. Yet, it cannot be
denied. In practical reasoning we do aggregate arguments and so logic, if it is to
be successfully applied and be able to mirror human reasoning, must be able to
cope with aggregation. Classical logic, if it is to be a universal language, must also
be able to deal naturally with aggregation.
One form of cut is still valid. The unitary cut:
∆|∼A; A |∼B
∆|∼B

What is a Logical System?
61
This holds because there is nothing for ∆to interact with.
We thus require from our reasoning system that it satisfy only Identity (A |∼A)
and Unitary Cut.
To show how real and possibly destructive aggregation can be, consider the
example of Prince Karlos and Princess Laura.
EXAMPLE 7 (Prince Karlos and Princess Laura). The prince and princess are
separated. Both made it clear to the press that no third parties were involved
and the separation was purely due to a personality clash. However, the editor
Mr Angel of the Daily Tabloid, thought otherwise. First he observed that after
her separation the princess moved to a house very near the Imperial Institute of
Logic, Language and Computation. This in itself did not mean much, because
both the Institute and the residence were in the centre of town. However, Mr
Angel further found out that in the past two years, whenever the princess went
on a European holiday, there was a research project meeting in the same hotel,
and surprisingly all projects involved a certain professor from the Institute, who
is known for introducing the logical theory of Labelled Deductive Systems. Again,
this could be a coincidence, because it is a well known fact that research project
consortia ﬁnd it most inspiring to be in the most expensive holiday resorts in
Europe and it is equally well known that certain dynamic professors participate
in many such projects.
However, the plot thickens when the princess, as part of her general social
activity, seems to actively support the new logics for computation. This could also
be a coincidence because after all, this subject is going to transform the nature of
our society. The various little arguments do seem to be aggregating, though not
conclusively enough to risk an article in such a responsible newspaper as the Daily
Tabloid. The situation changed when it became known that the princess actively
supports Labelled Deductive Systems and the Universality of Classical Logic. Under
this aggregation of arguments an obvious conclusion could be drawn!
8
PRACTICAL REASONING SYSTEMS
Our discussion so far has generalised the notion of a deductive system; namely,
given a database ∆and a formula Q, we ask the basic question, does ∆prove
Q? The various concepts we have studied had to do with what form do ∆and Q
take and what kinds of consequence relations |∼and algorithmic systems S|∼are
involved.
In practical reasoning systems, the deductive question is but one of many which
interest us. Other operations such as computationally feasible deduction, updating,
abduction, action, explanation are also involved. If we rethink of Q as an input, we
can partially list the kind of operations which may be involved. These operations
are performed using algorithms which accompany the deductive component. We
refer to them as mechanisms.
• The input Q is a query from ∆. We are interested in whether ∆|∼Q and

62
Dov M. Gabbay
possibly ask what proofs are available.
• The input Q is an update. We want to insert Q into ∆to obtain ∆′. We
may possibly have to deal with inconsistency and restructuring of ∆.10
• The input Q is an abductive stimulus (goal). We are interested in ∆′ such
that ∆+ ∆′ |∼Q. Where + is a symbol (to be precisely deﬁned) which
‘adds’ or ‘joins’ ∆and ∆′ to ‘combine’ their declarative information.
The + operation may or may not be the same as update. The abductive
question is to ﬁnd (possibly the minimal) ∆′ which helps prove the input.11
• The input Q may be a stimulus for action on the database outputting a new
database or outputting an explanation or any other output of interest.
The new possibilities of a formula Q interacting with a database ∆(via action
or abduction or other mechanisms) allow for a new way of answering queries from
∆. To see this, consider the query ∆?Q. In the declarative aspect, we want an
answer, namely we are asking whether ∆|∼Q. This can be checked via S|∼, or
semantically. Q does not act or change ∆in any way. In the interactive case, we
trigger an action. Q acts on ∆to produce a new ∆′. Q is read imperatively. We
can write ∆!Q to stress this fact.
The result of the interaction is ∆′. Thus ∆!Q = ∆′. Thus given a ∆and a
Q, we have two options. We can ask whether ∆|∼Q holds (written ∆?Q, with
|∼implicit) or we can let Q act on ∆, written ∆!Q, where ! denotes the action.
When an action ! is given, it is possible to derive a new consequence relation |∼
dependent on the action ! (really we should write ∆!). The new |∼is deﬁned by
∆|∼Q iﬀ∆!Q = ∆. This view was put forward particularly by F. Veldman and
pursued by J. van Benthem.12
Let us introduce clearly our current view on the question of what is a logical
system.
DEFINITION 8 (1994 tentative view of a logical system). A logical system has
the form (|∼, S|∼, Sabduce, Supdate, . . .) where e|∼is a labelled consequence relation,
10Such a view has been presented in Chapter 13 of Bob Kowalski’s book [Kowalski, 1979]. The
systematic study of updates and theory change was initiated in [Alchourr´on et al., 1985]. Note
that the theory of updates could be an entire discipline, of whether to accept an update or not.
We have this in law as the Theory of Evidence. New evidence may be or may not be accepted
according to a body of rules of the Theory of Evidence. It is not just a matter of consistency.
The appropriate logical treatment of that is in the discipline of Labelled Deductive Systems. See
for example [Gabbay and Woods, 2003; Cross, 1999; Dennis, 1992].
11For a given system (|∼, S|∼), the abductive mechanism is usually dependent on S|∼, the
particular algorithmic proof system involved. Diﬀerent applications might require diﬀerent ab-
ductive procedures.
12Note that if our logic is presented semantically, the input may restrict the semantics. Public
announcement logic is such an example. We are dealing with Kripke models with a set of possible
worlds and we get a public announcement (an input) saying Q holds. This means we consider
from now on only worlds in which Q holds.

What is a Logical System?
63
S|∼is a Labelled Deductive System with Flattening procedures and Sabduce, Supdate,
etc. are mechanisms which are dependent on (make use of) S|∼.
It would be instruct to construct a logical system in the sense of the above
deﬁnition. We now present one incrementally.
EXAMPLE 9. Our starting point is minimal propositional implicational logic with
a constant for falsity. The language contains atomic propositions {p, q, r, . . .} and
the implication connective {→} together with the falsity constant {⊥}.
As a
Hilbert system, minimal logic satisﬁes the following schemas:
• A →(B →A)
• (A →(B →C) →((A →B) →(A →C))
and the rule of modus ponens
• A; A →B
B
The following theorems can be proved
• ⊢A →A
• ⊢(A →(B →C)) →(B →(A →C)).
A consequence relation |∼m can be deﬁned by:
• A1, . . . , An |∼m B iﬀ(def) ⊢A1 →(A2 →. . . →(An →B) . . .).
Note that in minimal logic ⊥does not imply anything in particular, just itself.
If we add the axiom schema ⊥→A, we get intiuitionistic logic based on the
connectives {→, ⊥}.
The following additional Hilbert axioms yield full minimal logic with ∧and ∨.
• A →(B →A ∧B)
• A ∧B →A
• A ∧B →B
• (A →C) →((B →C) →(A ∨B →C))
• A →A ∨B
• B →A ∨B
We have now deﬁned |∼m, the consequence relation of our logic. We proceed to
deﬁne Sm (actually S|∼m), an algorithmic proof system for |∼m. There are many
options to choose from, such as Gentzen systems, Tableaux, Term Translation into
Classical logic, etc. We choose a goal directed formulation.
DEFINITION 10.

64
Dov M. Gabbay
1. First note that any formula B of the language (without ∨and ∧) has the
form B = (B1 →(B2 →. . . →(Bn →q) . . .), where q is attic and Bi has
the same form as B. q is called the head of B and {Bi} is the body. If we
allow ∧in the language, then every formula is equivalent to a set of wf’s of
the above form. This holds because of the following equivalences in minimal
logic:
A ∧B →C and A →(B →C)
A →(B ∧C) and (A →B) ∧(A →C)
2. A theory ∆is a list of wﬀs of the logic.
3. We deﬁne the following metapredicates:
• ∆?A = 1, reading ‘the goal A succeeds from the theory ∆’.
• ∆?A = 0, reading ‘the goal A ﬁnitely fails from ∆’.
The deﬁnition is as follows:
(a) ∆?q = 1, for q atomic if q is listed in ∆.
(b) ∆?q = 0, for q atomic if q is not the head of any element in ∆.
(c) ∆?A1 →(. . . (An →q) . . .) = 1 (resp. 0) if ∆∗(A1, . . . , An)?q = 1
(resp. 0) where ∗is concatenation.
(d) ∆?q = 1 if for some B = (B1 →. . . →(Bn →q) . . .) in ∆we have that
∆?B1, for i = 1, . . . , n.
(e) ∆?q = 0 if for each B in ∆of the form B1 →(B2 . . . (Bn →q) . . .)
there exists an 1 ≤i ≤n such that ∆?Bi = 0.
THEOREM 11. A1, . . . , An |∼m B iﬀ(A1, . . . , An)?B = 1
Proof. See [Gabbay, 1992].
■
To obtain a proper algorithm for S|∼m, we need to specify exactly how we
compute ∆?Q. It is convenient for the purpose of ease of control to let the goal
be a list of formulas Γ, as in some Prolog interpreters. Thus clause 10(d) will
now read:
∆?q ∗Γ = 1 if for some B = (B1 →(. . . →(Bn →q) . . .) in ∆we have
∆?(B1, . . . , Bn ∗Γ) = 1.
We can agree to search the list ∆top down and agree where to continue the
search when starting a new goal in the list of goals. The policy of some Prolog
interpreters is always to start the search at the top of the database list ∆. This
would yield a precise algorithm but may cause loops. For example (q →q)?q will
loop. With a loop checker, however, we get decidability in P-space.
We now proceed with the incremental deﬁnition of our logic. We need next the
notion of a database. This will contain integrity constraints and some clauses as

What is a Logical System?
65
data. The data are divided into two parts, permanent data and added hypothetical
data. (E.g. to show A →B, we hypothetically assume A and try to show B). The
next deﬁnition does the job.
DEFINITION 12.
1. A formula of the form B1 →(. . . →(Bn →⊥) . . .) where ⊥does not appear
in Bi, i = 1, . . . , n, is called an integrity constraint.
2. A formula in the pure →fragment (i.e. without ⊥) is called a clause.
3. A simple database is a concatenation of three (possibly empty) lists of formu-
las of the form ∆= ∆I ∗∆P ∗∆A, where ∆I is a list of integrity constraints,
∆P is a list of classes called the protected clauses (the signiﬁcance of ∆P will
emerge later when we update), and ∆A is another list of clauses called the
additional clauses.
4. A database ∆is inconsistent if ∆?⊥= 1. Note that ∆is also a theory, so
∆?⊥can be computed.
DEFINITION 13.
1. Let ∆= ∆I ∗∆P ∗∆A be a consistent database with ∆A = (C1, . . . , Cm)
and let Q be a clause. We deﬁne the update of ∆by Q, denoted by ∆!Q, to
be the following database ∆′:
• ∆′ = ∆if ∆I ∗∆P ∗(Q) is not consistent.
• Otherwise, let ∆′ be ∆I ∗∆P ∗(Ci, Ci+1, . . . , Cm, Q), where i is the
least number ≥1 such that the above theory is consistent.
An update ∆!Q insists if possible, on putting Q into ∆and maintains consis-
tency by taking out from ∆those assumptions that are unprotected and older (i.e.
earlier in the list ∆A). If Q is inconsistent with ∆I ∗∆P , then and only then do
we reject the input.
In Deﬁnition 10 the algorithm S|∼m for computing ∆?A →B is based on the
deduction there and the query ∆?A →B is reduced to ∆∗(A)?B. In minimal
logic, where ∆is a theory, ∆∗(A) is always consistent because we do not mind
deriving ⊥, and there is no notion of inconsistency. When we move to the notion
of databases, integrity constraints and clauses, databases can be inconsistent and
the old reduction of ∆?A →B to ∆∗(A)?B may need to face the fact that ∆∗(A)
is inconsistent. However we do have in this case the notion of the update ∆!A,
and we could reduce the query ∆?A →B to that of ∆!A?B. This new reduction
actually deﬁnes a new conditional implication A ⇒B, meaning B would be true
if A were true, i.e. update ∆by A and then (query) check B. The next deﬁnition
gives the details. We are going to keep using the ‘→’ symbol for both kinds of
implication and the context will decide what meaning we give to ‘→’. See [Gabbay
et al., 1993].

66
Dov M. Gabbay
DEFINITION 14 (Computation for conditional →). We deﬁne a new computation
∆?!Q = 1 and ∆?!Q = 0 for clauses Q as follows:
1. ∆?!q = 1 if q is in ∆, for atomic q.
2. ∆?!q = 0 for q atomic, if q is not the head of any clause in ∆.
3. ∆?!B1 →(B2 →. . . →(Bn →q) . . .) = 1 (resp. 0) iﬀ(((∆!B1)!B2) . . .!Bn)?!q =
1 (resp. 0).
4. ∆?!q = 1 if for some B1 →(. . . (Bn →q) . . .) in ∆we have ∆!Bi = 1, for
i = 1, . . . , n.
5. ∆?!q = 0 if for each B1 →(. . . →(Bn →q) . . .) in ∆there exists an i such
that ∆?!Bi = 0.
6. ∆?!⊥is not deﬁned. We only compute ∆?⊥.
We now have an update mechanism and a new consequence relation. Let us
deﬁne some more mechanisms. First we deal with normal defaults of the form
A:B
B , reading: if A is in ∆and it is consistent to add B, then we do add B. Note
that the default notion we are proposing here is straightforward and tailored for
our case and is not a general theory of default. We are building a ‘new logic’ and
we want to put some default aspects to it.
DEFINITION 15.
1. A normal default δ is a pair δ = (A, B) where A and B are clauses.
2. A default database is a concatenation of several lists of clauses and con-
straints containing at least the following:
∆= ∆I ∗∆P ∗∆A ∗∆D
where ∆D is a list added because of default.
3. We now deﬁne the default update ∆!δ as follows
∆!δ = ∆I ∗∆P ∗∆A ∗∆D(B)
provided this theory is consistent and ∆?A = 1.
So far we have built a logical system with a consequence relation, an algorithmic
procedure for it and some mechanisms such as updates and default. We will add
one more mechanism and then rest our case. This time we add abduction.
From the purely logical point of view, abduction is a syntactical action on a
theory ∆and a goal Q, consistent with ∆, in a logic (|∼, S|∼), yielding some
additional data ∆B, consistent with ∆(denoted by ∆B = Abduce(∆, Q)) such

What is a Logical System?
67
that ∆, ∆B |∼Q.
That is, we ‘answer’ the question of ‘what do we need to
consistently add to ∆to make it prove Q’?
Let us deﬁne Abduce(∆, Q) for the logic (|∼m, S|∼m). The deﬁnition will be by
induction on the computation steps of Q from ∆as in Deﬁnition 10.
DEFINITION 16. Let ∆be a theory and Q be a goal in the minimal logic |∼m of
Example 9 (for the language with →, ⊥and possibly ∧), using the algorithm S|∼m
of Deﬁnition 10(3) which is complete by Theorem 11.
We deﬁne a formula Abduce(∆, Q) in the full language of minimal logic, with ∧
and ∨such that
• ∆∗Abduce(∆, Q) |∼m Q
1. Abduce(∆, Q) = ⊤if ∆?Q = 1.
2. Abduce(∆, q) = q, for q atomic such that q is not the head of any clause in
∆.
3. Abduce(∆, A1 →(A2 →. . . (An →q) . . .)) = A1 →. . . (An →Abduce(∆∗
(A1, . . . , An), q) . . .).
4. Let q be atomic and let Bj = (Bj
1 →. . . →(Bj
nj →q) . . .), j = 1, . . . , m be all
clauses in ∆with head q. Then Abduce(∆, q) = Wm
j=1
Vnj
i=1 Abduce(∆, Bj
i ).
5. In case we have conjunctions : Abduce(∆, A∧B) = Abduce(∆, A)∧Abduce(∆, B).
Note that clause 4 of the above deﬁnition of Abduce may be simpliﬁed to be that
any one of the disjuncts (say of the ﬁrst Bj in the list ∆) is always chosen. However,
when we take the disjunction we get a logically weaker abduced formula. Also note
that clause 5 may give rise to inconsistency: Abduce(∆, A) and Abduce(∆, B) may
be consistent but not necessarily their conjunction.
If we adopt the policy of taking disjunctions in clause 4, we increase the chances
of ﬁnding a consistent abduced formula. In the →fragment, of course, we do not
want disjunctions.
EXAMPLES 17.
1. Let ∆be {a} and let the goal be q. The abduced formula is Abduce((a), q) =
q. Note that if we were to take γ = (a →q) then certainly
∆, γ |∼m q
so the abduced formula is not the logically weakest which can be added to
∆to prove the goal (since γ ̸|∼m q but q |∼m γ). However, in the presence
of ∆it is the weakest.
2. Let ∆be
(a) a →(b →q)

68
Dov M. Gabbay
(b) a
(c) (c →d) →q
and let the goal be q.
The abduced theory is b ∨(c →d).
LEMMA 18. ∆, Abduce(∆, Q) |∼m Q.
Proof. The proof is by induction on the deﬁnition of the abduced formula.
1. If the abduced formula is ⊤then this means ∆|∼m Q.
2. Assume that Q = q is atomic and that it is not the head of any clause. Then
the abduced formula is q and clearly ∆, q |∼q.
3. Assume Q has the form
Q = (A1 →. . . →(An →q) . . .).
Then the abduced formula is
A1 →. . . →(An →Abduce(∆∗(A1, . . . , An), q), . . .).
We need to show
∆, A1 →(. . . →(An →Abduce(∆∗A1, . . . , An), q) . . .) |∼m Q.
By the induction hypothesis
∆, A1, . . . , An, Abduce(∆∗(A1, . . . , An), q) |∼m q,
hence
∆, A1, . . . , An, A1 →. . . (An →Abduce(∆∗(A1, . . . , An), q) . . .) |∼m, q
hence
∆, A1 →(. . . An →Abduce(∆∗(A1, . . . ,n ), q) . . .) |∼m Q.
4. Assume Q = q is atomic and let Bj = (Bj
1 →. . . →(Bj
nj →q) . . .), j =
1, . . . , m be all the clauses in ∆with head q.
Then we need to show
∆, Abduce(∆, q) |∼m q
where
Abduce(∆, q) =
m
_
j=1
nj
^
i=1
Abduce(∆, Bj
i ).

What is a Logical System?
69
By the induction hypothesis of j ﬁxed, we have
∆, Abduce(∆, Bj
i ) |∼m Bj
i
for i = 1, . . . , nj.
Hence for each j, since Bj is in ∆, we have
∆,
nj
^
i=1
Abduce(∆, Bj
i ) |∼m q.
Hence
∆,
m
_
j=1
nj
^
i=1
Abduce(∆, Bj
i ) |∼m q.
This completes the induction step and the lemma is proved.
■
REMARK 19. The above deﬁned abduction mechanism gives rise to abduced
formula which may be disjunctions, but not necessarily. If we are dealing with
the →fragment only, we will not be able to add the abduce formula into ∆. We
notice however, that conjunctions are no problem because in the →fragment, every
formula with conjunctions is equivalent to a conjunction of our →formulas. This
conjunction can be added to ∆as an additional list. We have already observed
that disjunctions arise from the fact that an atom q may have several clauses in
the database with head q (item 4 of the inductive deﬁnition). Since all clauses
are ordered, we can choose as part of our abduction policy to use only an agreed
one of them (say the top of the list). This will give us no disjunctions. Call such
modiﬁed abduction algorithm by Abdtop(∆, Q).
EXAMPLE 20. Consider the database ∆with one integrity constant in it and one
data item in it.
1. a ∧s ∧e →⊥
2. a
We are using conjunction but it can be eliminated. We can write item 1 as
a →(s →(e →⊥)).
Consider the following two goals:
• Q1 = a ∧e
• Q2 = a ∧e →⊥.
Note that for conjunctions the obvious rule to use (at the risk of integrity con-
straints being violated) is:
• Abduce(∆, A ∧B) = Abduce(∆, A) ∧Abduce(∆, B)

70
Dov M. Gabbay
Therefore our example:
Abduce(∆, Q1) = Abduce(∆, a) ∧Abduce(∆, e) = Abduce(∆, e)
Clearly Abduce(∆, e) = e, as the only way to prove e from ∆is to abduce e
itself (note we do not have ⊥|∼m e).
We now try to abduce the second goal, Q2:
Abduce(∆, Q2) = (a ∧e →Abduce(∆∗(a) ∗(e), ⊥) = a ∧e →s.
Since
Abduce(∆∗(a) ∗(e), ⊥) = Abduce(∆∗(a) ∗(e), a ∧s ∧e) = s.
REMARK 21. There is a sense in which Abduce(∆, q) is the logically minimal
addition to ∆which can prove Q, namely:
If ∆, X |∼m Q , then ∆′, X |∼m Abduce(∆, Q), where ∆′ is some completion of
∆. It is not clear to me at this stage exactly what ∆′ should be. Consider the
following:
∆= {a →b}, Q = b.Abduce(∆, Q) = a.
Clearly, ∆, b ⊢Q but ∆, b ̸⊢a. However, Clark’s completion ∆′ = {a ↔b} does
the job; ∆′, b ⊢a.
There is work for Horn clauses in this direction in [Console et al., 1991], but
our language here contains embedded implications.
EXAMPLE 22 (An example of a logical system). We can now deﬁne an example
of a ‘logical’ system in our sense as follows:
• The language has →only.
• The notion of a theory and of a consequence relation is that of minimal logic
|∼m.
• The algorithmic system is S|∼m of Deﬁnition 10.
Theorem 11 shows the
algorithm is complete and sound.
• The abduction mechanism is Abdtop of Remark 19. The result is appended
at the end of the ∆B list.
• A database is comprised of several lists of clauses and integrity constraints
of the form
∆= ∆I ∗∆P ∗∆B ∗∆A ∗∆D
where ∆I is the integrity constraint, ∆P is the permanent data, ∆B is the
abduced data, ∆A is the additional data, and ∆D is the default data.
• The update mechanism is as in Deﬁnitions 13 and 14, where for the purpose
of performing an update the list ∆B is considered ‘protected’ data while
∆A ∗∆D is considered ‘additional data’. This gives defaults higher priority
than hypotheticals.

What is a Logical System?
71
• Input of hypotheticals is appended to the end of the ∆A list.
• The default mechanism is as in Deﬁnition 15 and the result of default is
appended at the end of the ∆D list.
Note that some of our decisions in deﬁning the logic of Example 12 are not the
most reasonable. They need to be reﬁned. We can be more careful how we update
and more careful where to input the results of abduction by looking at what part
of the database was used in the abduction. For example if we abduce q because of
default rules, it makes more sense to put the result in the default database than
in the abduced database.
However, for the purpose of illustrating what we ran by a logical system with
mechanisms, the above is suﬃcient.
In the most general case, databases are LDS databases and not just lists. In
this case the mechanisms and algorithms will be more complex.
9
NEW NOTIONS OF LOGICAL CONSEQUENCE
Let us continue with our new notions for logics. First let us summarise what we
have got so far:
Logical Systems
• Structured data;
• algorithmic proof theory on the structure;
• mechanisms make use of data and algorithms to extend data;
• inconsistency is no longer a central notion. It is respectable and is most
welcome;
• acceptability is the right notion;
• the deduction theorem is connected with cut and the insertion and deletion
notions.
Notice the following two points about the notion of a logical system up to the year
1999:
• time and actions are not involved;
• proofs and answers are conceptually instantaneous.
Our new notion of logic and consequence of year 2000 shall make the following
points:
• proofs take time (real time!);

72
Dov M. Gabbay
• proofs involve actions and revisions;
• logics need to be presented as part of a mutually dependent family of logics
of various modes.
We explain the above points.
In classical geometry, we have axioms and rules. To prove a geometrical theorem
may take 10 days and 20 pages, but the time involved is not part of the geometry.
We can conceptually say that all theorems follow instantaneously from the axioms.
Let us refer to this situation as a timeless situation.
Our notion of a logic developed so far is timeless in this sense.
We have a
structured database ∆, we have a consequence relation |∼, we have an algorithm
S|∼, we have various mechanisms involved and they all end up giving us answers
to the timeless question: does ∆|∼?Γ hold?
In practice, in many applications where logic is used, time and actions are
heavily involved. The deduction ∆|∼?Γ is not the central notion in the application,
it is only auxiliary and marginal. The time, action, payoﬀs, planning and strategic
considerations are the main notions and the timeless consequence relation is only a
servant, a tool that plays a minor part in the overall picture. In such applications
we have several databases and queries ∆i |∼?Γi arising in diﬀerent contexts. The
databases involved are ambiguous, multiversion and constantly changing.
The
users are in constant disagreement about their contents. The logical deductive
rules are non-monotonic, commonsense and have nuances that anticipate change
and the reasoning itself heavily involves possible, alternative and hypothetical
courses of action in a very real way. The question of whether ∆i |∼?Γi hold plays
only a minor part, probably just as a question enabling or justifying a sequence of
actions.
It is therefore clear that to make our logics more practical and realistic for
such applications, we have no alternative but to bring in these features as serious
components in our notion of what is a logic.
Further, to adequately reason and act we need to use a family of diﬀerent
logics at diﬀerent times, ∆i |∼i?Γi, and their presentation and proof theory are
interdependent. We anticipate a heavy use of modes in the deduction.
Consider the following example, which shows that even a simple modus ponens
is not as simple as it looks.
EXAMPLE 23.
Consider data as labelled by labels t, s, etc, which have some meaning in the
application, and assume that in addition we have two reliability labels h (high)
and l (low).
We have two reasoners. Reasoner 1 who is keen on proving B with high relia-
bility and any label and Reasoner 2 who wants Reasoner 1 to fail.
Data
Goal
(1) (t, h) : A →B
(label, h) : B
(2) (s, h) : χ

What is a Logical System?
73
Reasoner 1 realises that to get B with any sort of label he needs A.
In a
context where actions can be taken and actions result in postconditions, Action
1 is available to Reasoner 1 and can result in A, provided the preconditions are
available. Namely,
• Action 1: if s : χ is available, this action gives you s : A.
The reasoner proposes the following proof (the proof takes time).
• Reasoner 1: Proof of ts : B:
Step 1: take Action 1
Result: (3) s : A is added to the data.
Step 1 can take 2 days
Step 2: (ts, h) : B by labelled modus ponens from (1), (3).
Reasoner 2 is keen on obstructing this proof. He has two options:
Option 1:
In this option, Reasoner 2 can attack the reliability of the data. He has available
what is known as Fallacy rule:13
Ad Hominem: attack label (s, h).
This may involve a whole deduction based on a new database, designed to
discredit the reliability of A.
Time: 2 weeks.14
Result: label of B becomes (s, low).
Result: B is derived by Reasoner 1 with low value.
Reasoner 2 has another option. He can take a new action and undermine and
change A to ¬A.
Option 2:
Take a new Action 2 whose post condition is (r, h) : ¬A.
Takes 1 day
Result: (r, h) : ¬A must be added to the database.
This results in an internal contradiction.
The database needs to be revised or more actions need to be taken.
One thing is clear: B is now in doubt.
If A is factual (e.g. Dov has a valid passport) then Action 2 could be to destroy
the passport.
If A is obtained non-monotonically, a revision is needed in view of ¬A. Whatever
we do depends on the logic.
In view of the above example, we return to our question of what is a logical
system? We shall see that it is a discipline of data, actions and revision rules
13The fallacy ad hominem is committed when during a course of an argument one reasoner
launches a personal attack on the other. For example, ‘I don’t accept your political argument
because you are adulterous’.
14(s : χ) may come from a witness and, it may take some time to undermine his credibility.

74
Dov M. Gabbay
that allow a system to evolve through time. It is a dynamical system of proofs,
conditionals, updates and revision processes.
The best way to present such a system is in a dialogue context.
10
EXAMPLE: A TAR-LOGIC
To illustrate our new notion of a logical system we present a sample Time-Action-
Revision Logic (TAR-logic).
Imagine two reasoners, Black and White. White is trying to prove A from the
database and Black is trying to obstruct him (make him fail). The database is a
sequence of three databases, ∆= (∆c, ∆W , ∆B), where each database ∆X is a list
of wﬀs.
The databases ∆c, ∆W , ∆B describe a state of the world. ∆c is a database of
indisputable facts. Both Black and White have to accept it. ∆W and ∆B are
the additional data according to the versions of White and Black (respectively) of
the state of the world. Thus ∆c ∪∆W and ∆c ∪∆B have to be consistent but
∆c ∪∆W ∪∆B may not be consistent.
To present our model we need to start with an underlying logic and language
for our system and so let it be the {∧, →, ⊥} fragment of intuitionistic logic.15
Thus the wﬀs in the databases are from this fragment. We can assume they are
written in a convenient ready to compute clause form, as follows:
1. an atom q or ⊥is a clause and is also the head of the clause.
2. If Ai are clauses and x is an atom or ⊥then V
i Ai →x is a clause. V
i Ai is
its body and x is its head.
We use ∗for concatenation of lists. Thus we write ∆c ∗∆W for concatenating the
two databases and for a wﬀA, ∆W ∗A to mean the obvious. We also write A ∈∆
to mean that A appears in the list. When we write ∆1 ∪∆2 for lists, this means
we regard them as sets.
So far there is nothing new in what we have described and we get no more than
intuitionistic logic in this set-up. This is the case where neither White nor Black
are allowed any actions. This is the (0, 0) action case. Let us describe the proof
theory so far for the sake of the record, and so that we can modify it later. We
can have goal wﬀs to be proved. We write
∆= (∆c, ∆W , ∆B) ⊢W ?A
to mean that White wants to prove A from ∆. This would mean that in intuition-
istic logic ∆c ∪∆W ⊢A. Similarly for ∆⊢B?A meaning ∆c ∪∆B ⊢A. The goal
directed computation for ⊢W ? for example, follows the following rules:
DEFINITION 24 (Goal directed rules for (0, 0) action case).
15This logic has a very simple proof theory. Basically it is the smallest consequence relation
which gives ∧and ⊥their traditional meaning and gives →the deduction theorem. It also has
a very simple goal directed formulation.

What is a Logical System?
75
1. ∆⊢W ?x succeeds immediately for x atomic or ⊥, if x ∈∆c ∪∆W or ⊥∈
∆c ∪∆W .
2. ∆⊢W ?A →B succeeds (resp. fails) if ∆+W A ⊢W ?B succeeds, (resp. fails)
where ∆+W A = (∆c, ∆W ∗A, ∆B). Note that ∆c ∪∆W ∗A may now be
inconsistent.
3. ∆⊢W ?y succeeds for y atomic or ⊥if for some clause V
i Ai →x in ∆c ∪∆W
we have x = ⊥or x = y and for each i, ∆⊢W ?Ai succeeds.
4. ∆⊢W ?A ∧B succeeds iﬀ∆⊨W A and ∆⊢W ?B succeed.
It fails if either ∆⊢W ?A or ∆⊢W ?B fail.
5. ∆⊢W ?x immediately fails for x atomic or ⊥if there is no clause i n ∆c∪∆W
with head ⊥or x.
6. We write ∆⊢A to mean ∆⊢?A succeeds and ∆̸⊢A to mean ∆⊢?A fails.
The algorithm characterises intuitionistic consequence of A from ∆c ∪∆W (see
[Gabbay, 1998]).
Similarly we can deﬁne the algorithm for ∆⊢B?A, replacing ∆W by ∆B every-
where.
We now introduce actions. Assume Black wants ⊢W ? to fail. Is there anything
he can do about it? The answer is no. However we can allow Black and White to
have some actions, which we will present as Ai ⇒B Bi (and similarly for White,
Ci ⇒W Di) which he can activate at certain points to obstruct the proof for White.
A ⇒B B is an action for Black. Its precondition is A and postcondition is B. If
the database is ∆= (∆c, ∆W , ∆B) then if ∆⊢B A then Black can update/revise
the database with B. We denote the revised database by ∆+c B.
To explain how Black can obstruct White, let us assume that Black has one
action only, A1 ⇒B B1. He can use it in two ways: (1) the modular way and (2)
the runtime way.
In (1) Black waits for White to try and prove ∆⊢W ?A without intervention.
If White fails then all is well. If White succeeds according to the algorithm of
Deﬁnition 24, then Black can say, ‘very nice but I have an action A1 ⇒B B1 that
I can apply. I’ll show you that ∆⊢B A1 and hence I can take the action and the
database is now ∆+c B1. I know you are anxious to get A but you do not have A
yet; you have to check it from the new database, i.e. show that ∆+c B1 ⊢W ?A’
(2) The second way Black can intervene is by viewing his action as a runtime
move; as part of the algorithm. At any time during the execution of the algorithm
of Deﬁnition 24 Black can apply his action and change the database. In Deﬁnition
26 below, we allow Black to use his action whenever White is about to succeed
locally with an atom x or ⊥, through the rule of immediate success, see item 1 of
Deﬁnition 24.
The reader should bear in mind that diﬀerent policies for Black’s intervention
may give rise to diﬀerent logics.

76
Dov M. Gabbay
Similar considerations apply to White’s actions taken to make himself succeed.
He may use his actions (say C ⇒W D) in a modular way, that is try to prove ﬁrst
∆⊢W ?A. If it fails, then say to Black, ‘let me change the database by applying my
action and maybe now I’ll succeed’. Alternatively, White may use his actions in
runtime during the computation (i.e. the algorithm of Deﬁnition 24) and whenever
he is stuck at an immediate failure (item 5 of Deﬁnition 24) he can change the
database.
Our policy choice in Deﬁnition 26 below is to allow actions as a move at runtime
but only when facing respective immediate success or immediate failure. We need
some technical deﬁnitions.
We said before that ∆c, ∆W and ∆B are all lists. The priority goes from left
to right.
So if ∆c is, say (X1, . . . , Xm), then ∆+c B is the result of revising
∆′ = ((B, X1, . . . , Xm), ∆W , ∆B) and making it consistent.
This means that
(B, X1, . . . , Xm)∗∆W and (B, X1, . . . , xm)∗∆B have each to be revised and made
consistent. The revision algorithm must have the same eﬀect on the common part,
(B, X1, . . . , Xm).
We therefore propose the following revision algorithm.
DEFINITION 25. Let Γ = (X1, . . . , Xk) be a list.
1. Let Γ1 = (X1) if X1 is consistent and let Γ1 = ∅otherwise.
2. Assume Γi, i < k has been deﬁned. Let Γi+1 = Γi ∗Xi+1 if consistent and
let Γi+1 = Γi otherwise.
3. Let Γ+ be the result of revising Γ.
4. Note that if Γ1 = Γ ∗∆W and Γ2 = Γ ∗∆B then the revision process will
revise Γ in the same way, as it works left to right.
Thus Γ+
1 and Γ+
2 can be split into Γ+ ∗∆W
+ and Γ+ ∗∆B
+.
5. Let X be a wﬀand let Γ = X ∗∆c, then we need to revise Γ1 = Γ ∗∆W and
Γ2 = Γ ∗∆B and the new database ∆+c B is (Γ+, ∆W
+ , ∆B
+).
We allow Black and White to use their actions at most once. (Actions cost
money). Our computation is now as follows:
DEFINITION 26 (Deﬁnition ⊢W and ⊢B with actions at runtime). A metabase
has the form
∆= (∆c, ∆W , ∆B, AW , AB)
where ∆c, ∆W , ∆B are as before and AW is a set of actions of the form Ci ⇒W Di
and AB is a set of actions of the form Ai ⇒B Bi.
We deﬁne the notion of ∆⊢W ?A and ∆⊢B?A succeeds or fails as follows. We
give the rules for ⊢W , the case of ⊢B is the mirror image.
1. ∆⊢W ?x succeeds immediately fore x atomic or ⊥if AB = ∅and x ∈
∆c ∪∆W or ⊥∈∆c ∪∆W .

What is a Logical System?
77
2. ∆⊢W ?A →B succeeds (resp. fails) if ∆+W A ⊢W ?B succeeds (resp. fails),
where
∆+W A = (∆c, ∆W ∗A, ∆B, AW , AB).
3. ∆⊢W ?y succeeds (resp. fails) for y atomic or ⊥if for some (respectively
each) clause V
i Ai →x in ∆c ∪∆W we have x = ⊥or x = y and for each i
(resp. some i), ∆⊢W ?Ai succeeds (resp. fails).
4. ∆⊢W ?A ∧B succeeds (resp. fails) iﬀboth (resp. one of) ∆⊢W ?A and
∆⊢W ?B succeeds (resp. fails).
5. ∆⊢W ?x immediately fails for x atomic or ⊥if AW = ∅and there is no
clause in ∆c ∪∆W with head ⊥or x.
6. ∆⊢W ?x succeeds (resp. fails) for the case where x atomic or ⊥and where
there is no clause in ∆c ∪∆W with head x or ⊥if for some action (resp.
all actions) C ⇒W D ∈AW such that ∆⊢W ?C succeeds we have that
(∆+c D, AW −{C ⇒W D}, AB) ⊢W ?x succeeds (resp. fails).16
7. ∆⊢W ?X succeeds (resp. fails) if x is atomic or ⊥and x or ⊥∈∆c ∪∆W
and for every (resp. some) action A ⇒B B ∈AB for which (∆c, ∆W , ∆B,
AW , AB −{A ⇒B B}) ⊢B?A succeeds, we have that (∆+c A, AW , AB −
{A ⇒B B}) ⊢W ?x succeeds (resp. fails).
Note that the computation is such that Black can apply his actions only when
White is about to succeed immediately (atomic goal which is in the database
as data) and white can apply actions only when he is about to fail immediately
(atomic goal which is not a head of any clause in the database). If e.g. Black wants
to apply action A ⇒B B, then he has to prove the precondition A. In doing so
the action A ⇒B B is not available to him because he is using it now (to update
with B) and so he cannot use it a second time to prove A. The following example
illustrates our point.
EXAMPLE 27.
Consider the following case:
∆c = a
∆W = a →b
∆B = ∅
AW = ∅
AB = a ⇒B (a →⊥).
Let us check whether ∆⊢W ?b.
1. ∆⊢W ?b
if
16By ∆+c D we mean (∆c, ∆W , ∆B) +c D as in item 5 of Deﬁnition 25.

78
Dov M. Gabbay
2. ∆⊢W ?a
3. Black can take the action which puts into ∆c the item a →⊥since ∆⊢B a
succeeds and after doing so the revision process yields the new ∆c = {a →⊥}
and so ∆̸⊢W b.
Suppose White also has this action, i.e. a ⇒W (a →⊥) is available.
Then White can play as follows. Black wants to apply his action a ⇒B (a →⊥).
For that ∆⊢B a has to succeed. But it cannot succeed because White can block
it with the action a ⇒W (a →⊥). To activate this action White has to show
∆⊢W ?a. Black does not have his action a ⇒B (a →⊥) to block it because its use
is already designated. Thus White succeeds in blocking Black’s use of his action.
Further note that although actions can be used at most once, the ‘counting’
is doen at runtime locally and not globally.
Thus to prove ∆⊢A ∧B, the
computation splits into two parallel branches and the action can be used once on
each branch. We are not going into ﬁne tuning of the options in this chapter. We
need only give a sample logic of this kind.
DEFINITION 28 (Deﬁnition of ⊢W , ⊢B with modular actions).
Let
∆= (∆c, ∆W , ∆B, AW , AB). The deﬁnition of ∆⊢W A and ∆⊢B A will be
given by induction on the number of elements (m, n), m of AW and n of AB.
1. For AW = AB = ∅let ∆⊢(0,0)
W
A and ∆⊢(0,0)
B
A be deﬁned according to
the algorithm of Deﬁnition 24. This means that in this case ∆⊢(0,0)
W
A iﬀ
∆c ∪∆W ⊢A in intuitionistic logic and similarly ∆⊢(0,0)
B
A iﬀ∆c ∪∆B ⊢A
in intuitionistic logic.
2. Assume AB = ∅, and AW has m > 0 elements.
Then ∆⊢(m,0)
W
A iﬀ
∆⊢(0,0)
W
A or for some action a = C ⇒W D in AW such that
∆′
a = (∆c, ∆W , ∆B, AW −{C ⇒W D}, ∅⊢(m−1,0)
W
C,
we have that
∆′′
a = (∆+c D, AW −{C ⇒W D}, ∅) ⊢(m−1,0)
W
A.
Similarly we deﬁne
∆⊢(0,n)
B
A
3. Assume AB = ∅and AW has m elements. Deﬁne ∆⊢(m,0)
B
A as follows
(recall Black wants to prove and White wants to obstruct)
∆⊢(m,0)
B
A iﬀ∆⊢(0,0)
B
A and for all actions a = C ⇒W D ∈AW such that
∆′
a ⊢(m−1,0)
W
C we have ∆′′
a ⊢(m−1,0)
B
A, where ∆′
a and ∆′′
a are as deﬁned in
the previous item.
Similarly, we deﬁne ∆⊢(0,n)
W
A.

What is a Logical System?
79
4. Assume ∆⊢(a,b)
W
A and ∆⊢(a,b)
B
A is deﬁned for all (a, b) such that (a ≤m
and b < n) or (a < m and b ≤n). We deﬁne ⊢(m,n) as follows:
∆⊢(m,n)
W
A iﬀeither for some a = C ⇒W D ∈AW such that ∆′
a ⊢(m−1,n)
W
C
we have ∆′′
a ⊢(m−1,n)
W
A or ∆⊢(0,0)
W
A and for all b = A1 ⇒B B1 ∈AB such
that
∆′
b = (∆c, ∆W , ∆B, AW , AB −{A1 ⇒B B1}) ⊢(m,n−1)
B
A1
we have
∆′′
b = (∆+c B1, AW , AB −{A1 ⇒B B1}) ⊢(m,n−1)
W
A.
EXAMPLE 29. The following example shows the diﬀerence between the runtime
and modular proof theory. Take the case of ∆c = ∅, ∆W = r and White has the
action r ∧p ⇒W q. Black has no actions. White wants to prove p →q. In the
modular approach White asks r ⊢? p →q and fails. Then he tries to apply the
action but the precondition r ⊢?r ∧p fails.
In the runtime proof approach, White can succeed by applying his action at
runtime, as follows:
r ⊢?p →q
if
r ∧p ⊢?q
apply action since r ∧p ⊢?r ∧p succeeds.
And ask
r ∧p ∧q ⊢?q
and success.
Assume now that the common database is {p →q} and White wants to prove
p →q and assume that Black has the action ⊤⇒B ¬q.
In the modular ap-
proach White can succeed because although Black can activate his action and
insert ¬q, (p →q) ∧¬q ⊢p →q. However in the runtime proof theory Black can
block White as follows:
p →q ⊢W ?p →q
if
p →q, p ⊢W ?q
Black can apply his action and add ¬q and thus force a revision.
To revise ¬q, p →q, p we delete p and we no longer can prove q.
Thus we see the two algorithms are diﬀerent.

80
Dov M. Gabbay
11
SEMANTICAL INTERPRETATION
We do not really need semantics to explain the nature of our TAR-logic.
Its
syntactical/operational deﬁnition is clear enough and its motivation and nature
are quite intuitive. However, it is traditional to give semantics to a logic and so
we will do this here.
We saw that when actions are involved there are two approaches to deﬁning
the consequence relation.
The modular approach (Deﬁnition 28) and the run-
time approach (Deﬁnition 26). We start by indicating possible semantics for our
approaches. We propose a syntactical model.
In order to describe our semantics, we need some convenient notation. In Def-
inition 28 items 2 and 4 we used the notation ∆= (∆c, ∆W , ∆B, AW , AB) for
theories. If we let t = (∆c, ∆W , ∆B) and consider it as our ‘world’ (as opposed to
the actions), we can write ∆= (t, AW , AB) and rewrite ∆⊢W A into t ⊢AW ,AB
W
A.
Similarly for Black, we write t ⊢AW ,AB
B
A.
Let a denote an action for White
(C ⇒W D) and b an action for black (A ⇒B B). For a ∈AW or b ∈AB we
can rewrite ∆′
a ⊢W A and ∆′
b ⊢B A of Deﬁnition 28 as t ⊢AW −{a},AB
W
A and
t ⊢AW ,AB−{b}
B
A, respectively.
We now look at ∆′′
a for a = (C ⇒W D) ∈AW . We have ∆′′
a = ((∆c, ∆W ,
∆B)+cD, AW −{a}, AB). If we use the notation ta = (∆c, ∆W , ∆B)+cD then we
can write ∆′′
a ⊢W A as ta ⊢AW −{a},AB
W
A. Similarly for tb and tb ⊢AW ,AB−{b}
B
A.
In our presentation of semantics for our logic we need two concepts: the notion
of mode of evaluation and the notion of two dimensional logics.
The latter kind of logics are well known. The evaluation depends on two indices
and not just on one. So, for example, in modal logic, we use a pair of worlds, to
evaluate
(t, s) ⊨A.
A two dimensional logic is ﬂat if the assignment for atoms q is calculated from
two assignments, h1(q) and h2(q), one for each dimension. For example, we might
have
• (t, s) ⊨q iﬀt ∈h1(q) ∨s ∈h2(q).
We are now ready to describe semantics for the modular and runtime approaches.
We begin with the modular approach. We take as our set S of possible worlds,
all consistent theories of the form t = (∆c, ∆W , ∆B). For each t and each pair
of sets of actions AW and AB we recursively deﬁne the notions t ⊨AW ,AB
X
A for
X = W or X = B in the same way as ⊢was deﬁned in Deﬁnition 28. This makes
⊨equal ⊢and gives us syntactical semantics, like a Henkin canonical model. This
model can easily be turned into a more traditional Kripke model. The mode of
evaluation is determined by X = B or X = W and the action sets AW and AB,
and the initial starting theories tW = ∆c ∪∆W and tB = ∆c ∪∆B.
Let us now see how to turn this syntactical model into a proper possible world
semantics model. We have to address two problems

What is a Logical System?
81
1. The syntactical worlds t = (∆c, ∆W , ∆B) are intuitionistic theories and not
real worlds.
2. The ‘accessibility’ or ‘change of world’ is done via actions a ∈AW or b ∈AB.
The ﬁrst problem would have been easily overcome had t been in classical logic. We
chose intuitionistic logic as our starting underlying logic because its proof theory
is simple. We could have started with classical logic. However intuitionistic logic
presents no problem either. We can replace t with two Kripke models mW
t
and
mB
t one for tW = ∆c ∪∆W and one for tB = ∆c ∪∆B, respectively.
REMARK 30. In fact, for the {→, ∧, ⊥} of intutionistic logic, there is a universal
Kripke model of all wﬀs as follows:
• T = set of all wﬀs.
• for α, β ∈T let α ≤β be deﬁned as β ⊢α.
• Let for atomic q, and α ∈T, α ⊨q hold (by deﬁnition) iﬀα ⊢q.
We can prove by induction that for all wﬀs A and all α ∈T, we have
• α ⊨A iﬀα ⊢A.
For t = (∆c, ∆W , ∆B), let tW = V(∆c ∪∆W ) and tB = V(∆c ∪∆B). Then
the models mX
t , X = W or B can be taken as (T, ≤, tX, h). We have t ⊨X A iﬀ
mX
t ⊨A.
We can present both models together using two dimensions
(T, ≤, tW , tB, hW , hB)
where hX(q) = {y ∈T | tB ≤y ∧y ⊢q}.
Let us now address our second problem, of how to represent the relations s = ta
and s = tb, for a ∈AW , b ∈AB. Let a = C ⇒W D. To be able to apply the
action we need to ask whether t ⊨AW −{a},AB
W
C. This is context dependent on the
mode of evaluation (⊨AW ,AB
W
).
If the action is permitted, then we move from t to ta = t +c D, and this move
is not context dependent. It depends only on t, on D and on the revision process
of Deﬁnition 25.
Let f be a revision function, giving for each t and D the revised theory t +c D.
When we write it as
f(t, D) = t +c D
it looks like a selection function. Let ˆD = {x | x ⊨D} then we can write f as
f(t, ˆD) = {s}.

82
Dov M. Gabbay
ˆD is characterised by a ≤-closed set in the model (T, ≤) ( ˆD = {β | β ⊢D}.17
Thus f is a function giving for each (t, Y ), Y a closed subset of T, another point
s ∈Y , (s is in Y since t +c D ⊢D.)
We are now ready to deﬁne the semantics.
DEFINITION 31 (Semantics for a modular approach).
1. Our models have the form (T, ≤, aW , aB, f, h) where (T, ≤, aW , aB, h) is an
intuitionistic Kripke model with two actual worlds. h is an assignment giving
each atomic q a ≤-closed subset h(q) ⊆T. f is a selection function, giving
for each pair of worlds (αW , αB) and a closed subset Y ⊆T a new pair
f(αW , αB, Y ) = (βW , βB), such that βW , βB are both in Y . Some other
properties of f can be stipulated in accordance with the properties of the
revision process.
2. Satisfaction is two dimensional and is deﬁned at pairs (x, y) as follows, ac-
cording to the mode µ = (αW , αB, X, AW , AB), where X is W or B and
αW ≤x and αB ≤y. We ﬁrst consider the case where X = W and either
x = αW or AW = AB = ∅.
• (x, y) ⊨q iﬀx ∈h(q), for atomic q.
• (x, y) ⊨A →B iﬀfor all s ≥x if (s, y) ⊨A then (s, y) ⊨B.
• (x, y) ⊨A ∧B iﬀ(x, y) ⊨A and (x, y) ⊨B.
3. Note that the value of (x, y) ⊨A in this case is independent of y. (In the
syntactical model it just means x ⊢A), and so we can write x ⊨(αW ,αB,W,∅,∅)
A.
4. The deﬁnition of ⊨for case of X = B and either y = αB or AW = AB = ∅
is symmetrical.
5. In the case where X = W and x = αW and AW ∪AB ̸= ∅, we deﬁne
(αW , y) ⊨µ A in accordance with the clauses of Deﬁnition 28, namely, let
µ = (αW , αB, W, AW , AB), then
• (αW , y) ⊨µ A iﬀeither (i) or (ii) hold:
(i) For some a = C ⇒W D ∈AW such that (αW , y) ⊨µ′
a C we have
(βW , βB) ⊨µ′′
a A, where
µ′
a = (αW , αB, W, AW −{a}, AB)
(βW , βB) = f(αW , αB, ˆD)
ˆD = {x | (x, y) ⊨(αW ,αB,W,∅,∅) D}
µ′′
a = (βW , βB, W, AW −{a}, AB).
17t = (∆c, ∆W , ∆B) is also viewed as a pair of points tW = V(∆c∪∆W ) and tB = V(∆c∪∆B).
Thus ˆD can also be considered as a pair of sets
ˆDX = {y | y ⊢tX}.
The function f is a two dimensional selection function. For each (tW , tB) and sets ( ˆDW , ˆDB),
it gives a new pair (sW , sB).

What is a Logical System?
83
(ii) (αW , y) ⊨(αW ,αB,W,∅,∅) A and for all b = (A1 ⇒B B1) ∈AB such that
(αW , y) ⊨µ′
b A1 holds we have that (γW , γB) ⊨µ′′
b A, where
µ′′
b = (γW , γB, W, AW , AB −{b})
µ′
b = (αW , αB, B, AW , AB −{b})
(γW , γB) = f(αW , αB, ˆB1)
ˆB1 = {x | (x, y) ⊨(αW ,αB,B,∅,∅) A1}.
6. A symmetrical deﬁnition is given for the case of µ = (αW , αB, B, AW , AB)
for (x, αB) ⊨µ A and AW ∪AB ̸= ∅.
DEFINITION 32 (Semantics for runtime approach).
1. The models are the same as in the modular case. The modes have the form
µ = (X, AW , AB), X = W or B.
2. Evaluation is as follows for µ = (W, AW , AB).
(x, y) ⊨µ A iﬀeither (i) or (ii) holds:
(i) For some a = C ⇒W D ∈AW such that (x, y) ⊨µ′
a C we have that
(u, v) ⊨µ′′
a A where µ′
a = (W, AW −{a}, AB), (u, v) = f(x, y, ˆD) where
ˆD = {x | (x, y) ⊨(W,∅,∅) D}, and µ′′
a = (W, AW −{a}, AB).
(ii) (x, y) ⊨(W,∅,∅) A and for all b = A1 ⇒B B1 ∈AB such that (x, y) ⊨µ′
b
A1 holds, we have (w, z) ⊨µ′′
b A, where
µ′
b = (B, AW , AB −{b})
µ′′
b = (W, AW , AB −{b})
and (w, z) = f(x, y, ˆB1) where ˆB1 = {x | (x, y) ⊨(B,∅,∅) A1}.
3. A symmetrical deﬁnition is given for the case µ = (B, AW , AB).
12
DISCUSSION 1999
The reader may feel some disappointment by now. We boldly announced a new
way of doing logic, where time and actions are involved in an essential way and
we boldly declared that the static questions of whether ∆⊢?Γ holds are really
marginal. We gave as an example of the new kind of logic a TAR-system and yet,
when we looked at its semantics, we got something like conditional semantics with
selection functions looking like some modal action logic. So, we may ask, what
was all the fuss about? What is really new here?
The answer to that is the realisation that what really counts in a ‘new’ logic is
the internal structure, the relative emphasis of its components and its perception

84
Dov M. Gabbay
by the user. We all know that most logics, if not all, are translatable into classical
logic (ﬁrst-order or higher-order).
There are still people around claiming that
classical logic is ‘the only logic’. So their answer to the question ‘what is a logical
system’ is ‘it is classical logic’. Other colleagues believe in intuitionistic logic and
more recently many swear by linear logic. Each one of these systems is strong
enough to have the others embedded in it. The logics diﬀer in their structure,
proof dynamics and natural representation. It is this diﬀerence that compels the
user, the consumer of logic, to choose one logic over another. An application has
natural structure and natural dynamics to it and we want a logic which reﬂects
these features naturally and not forcibly. We see this in computer languages as
well. All strong enough languages are equivalent to Turing machines and can do
everything recursive but their natural structure is diﬀerent and we choose to use
the one best suited for our application.
So my message is that many important real life practical reasoning applications
involve time-action-revision in an integral and dominant way and we need logics,
(for example TAR-logics) whose structure and natural internal movements reﬂect
these features.
It is, we concede, possible to use simpler logics (maybe multimodal dynamic
logic) to express (or shall we say ‘implement’) these features but the natural struc-
ture will be lost.
The situation is similar to the debate of whether we need modal logics as such
or whether we are better oﬀdoing ‘modality’ from within classical logic. There
are arguments supporting modal logic as a separate new logic. Similar arguments
apply in support of our new TAR-logics.
13
MODES OF EVALUATION
The above discussion of what is a logical system presented the state of aﬀairs of
the mid nineties. See [Gabbay, 1994a]. We now move to ideas emerging around
the year 2000 and later.
When we present logics semantically, through say Kripke models, there are
several features involved in the semantics.
We can talk about Kripke frames,
say of the form (S, R, a), where S is a set of possible worlds, R ⊆Sn+1 is the
accessibility relation (for an n-place connective ♯(q1, . . . , qn)) and a ∈S is the
actual world. We allow arbitrary assignments h(q) ⊆S to atomic variables q and
the semantical evaluation for the non-classical connective ♯is done through some
formula Ψ♯(t, R, Q1, . . . , Qn), Qi ⊆S in some language. We have:
• t ⊨♯(q1, . . . , qn) under assignment h iﬀΨ♯(t, R, h(q1), . . . , h(qn)) holds in
(S, R, a).
For example, for a modality □we have
• t ⊨□q iﬀ∀s(tRs →s ⊨q).

What is a Logical System?
85
Here Ψ□(t, h(q)) = ∀s(tRs →s ∈h(q)).
Diﬀerent logics are characterised by
diﬀerent properties of R.18
We can think of Ψ♯as the mode of evaluation of ♯. The mode is ﬁxed throughout
the evaluation process.
In the new concept of logic, mode shifting during evaluation is common and
allows for the deﬁnition of many new logics. We can view the mode as the recipe
for where to look for the accessible points s needed to evaluate □A.
Consider the following:
• t ⊨□A iﬀ∀n∀s(tRns →s ⊨A)
where xRny is deﬁned by the clauses:
– xR0y iﬀx = y
– xRn+1y iﬀ∃z(xRz ∧zRny).
Clearly {(t, s) | ∃ntRns} is the transitive and reﬂexive closure of R.
Thus in this evaluation mode, we look for points in the reﬂexive and transitive
closure of R.
We can have several evaluation modes available over the same frame (S, R, a).
Let ρi(x, y, R), i = 1, . . . , k, be a family of binary formulas over (S, R, a), deﬁned
in some possibly higher-order mode language M, using R, a and h as parameters.
We can have a mode shifting function ε : {1, . . . , k} 7→{1, . . . , k} and let
• t ⊨i □A iﬀfor all s such that ρi(t, s, R) holds we have s ⊨ε(i) A.
EXAMPLE 33. Consider now the following deﬁnition for ⊨for two modes ρ0 and
ρ1 and x = 0 or 1:
• t ⊨x q for q atomic iﬀt ∈h(q)
• t ⊨x ¬A iﬀt ̸⊨x A
• t ⊨x A ∧B iﬀt ⊨x A and t ⊨x B
• t ⊨x □A iﬀfor all s such that ρx(t, s) holds we have s ⊨1−x A.
We see that we have a change of modes as we evaluate.
We are thus deﬁning ⊨0 not independently on its own but together with ⊨1 in
an interactive way.
Let us repeat here Example 1.5 of [Gabbay, 1999].
EXAMPLE 34. We consider two modes for a modality □.
ρ1(x, y)
=
xRy ∨x = y
ρ1(x, y)
=
xRy.
18Some logics presented axiomatically, cannot be characterised by properties of R alone but
the family of allowed assignments h needs to be restricted. This is a minor detail as far as the
question of ‘what is a logic’ is concerned.

86
Dov M. Gabbay
Deﬁne a logic L as the family of all wﬀs A such that for all models (S, R, a) and
all assignments h we have a ⊨0 A.
In such a logic we have the following tautologies.
⊨□A →A
̸⊨□(□A →A)
⊨□2n(□A →A)
It is easy to see that this logic cannot be characterised by any class of frames.
For let (S, R, a) be a frame in which all tautologies hold and ♦(¬q ∧□q) holds.
Then aRa must hold since □A →A is a tautology for all A. Also there must be a
point t, such that aRt and t ⊨¬q ∧□q. But now we have aRaRt and this falsiﬁes
□2(□A →A) which is also a tautology.
The idea of a mode of evaluation is not just semantical. If we have proof rules
for □, then there would be a group of rules for logic L1 (say modal K) and a group
for L2 (say modal T). We can shift modes by alternating which group is available
after each use of a □rule.
This way we can jointly deﬁne a family of consequence relations |∼µ dependent
on a family of modes {µ}.
14
THE REACTIVE PARADIGM IN GENERAL
Consider a system S containing components and some internal connections and
procedures governing the interactive behaviour of these components in response
to external inputs. We can think for example of S as a graph (to be traversed or
manipulated) or as an automaton which responds to inputs and changes states,
or as a Kripke model in which a formula is evaluated, or as a bureaucratic system
responding to an application form or even as more familiar daily objects such as
a washing machine, a television set or a car.
We can represent such a system as a network containing possibly labelled ver-
tices representing the components and arrows between the vertices representing
possible “control ﬂows” within the system of whatever the system internal pro-
cesses do.
Figure 2 is a typical situation we want to consider:
The arrows from a to b and from c to d indicate that internally whatever the
system does in response to input or command, there is a possible path where
component a passes the “ﬂow” to component b and similarly maybe in another
part of the system, there is a connection between c and d.
So far we have nothing more than a possible network graphic representation of
some system.
Now comes the reactive idea. Consider the possibility that the system develops
faults due to overuse or stress. The double arrow from the arc (a, b) to the arc
(c, d) (which we denote by ((a, b), (c, d))) indicates how the fault may develop.
When we pass from a to b we put internal pressure on the connection from c to

What is a Logical System?
87
•
•
a
c
d
b
•
•
Figure 2.
d. If signiﬁcant pressure builds up on the connection from c to d it may fail. For
example everyone who drove a car long distance during a very hot day knows that
the engine may overheat. This is an example of a fault. In general these faults are
predictable and are predetermined by the construction of the system. They are
represented in Figure 2 by the double arrows.
In fact, Figure 2 also indicates a remedy to this pressure, this is a double
arrow from the arc (a, b) to the double arrow ((a, b), (c, d)), namely ((a, b), ((a, b),
(c, d))).
The double arrows represent known weaknesses or remedies in the system. We
can look at the double arrows as indicating the ways in which the system reacts
and adjusts itself under pressure. The simplest model is a once only bridge in
Figure 3
•
a
b
Figure 3.
When we cross the bridge from a to b, a signal (representing stress) is sent to the
bridge to collapse and so after we cross the bridge the arc from a to b is cancelled.
The above is the basic intuition behind the idea of reactivity. Think of it as
“fault” reactivity, or better it is “fault-remedy” reactivity.
We now elaborate more about this concept to distinguish it from other somewhat
similar concepts.
The “fault” reactivity should be distinguished from the idea of including dy-
namic metalevel operators in the input. To give example of such metalevel opera-
tors, consider the possibility that the system may have several modes of operations
say mode 1 ,. . . , mode n.
The input may contain instructions of the form Ji (jump to mode i) which tells

88
Dov M. Gabbay
the system how to operate on the input. This is a metalevel instruction inducing
a change in the system. It is diﬀerent from “fault” reactivity, though the latter
can be case-by-case represented in it. If we replace double arrows to (c, d) by the
metalevel instruction “disconnect(c, d)”, we might be able to simulate the fault
induced by an input by adding (interleaving) some “disconnect” instructions to
the input to form a new input.
This option is not reactivity but “ﬁbring” of metalevel instructions giving the
illusion of reactivity.
The notion of fault reactivity should also be distinguished from the notion of
“sabotage”, independently introduced by J. van Benthem [van Benthem, 2005] and
further developed by P. Rohde [Rohde, 2004]. The notion is similar; we envisage a
saboteur disconnecting components within the system. There are both conceptual
and technnical diﬀerences between the two notions (“fault” reactivity and “sabo-
tage” reactivity). We note that faults and remedies are built into the system while
sabotage is not and also sabotage is presented as a metalevel connective and fault
is not.
It turns out that as we apply this idea in various application areas where diﬀer-
ent non-reactive systems are used we can get new kinds of systems, the reactive
systems, which can have better applicability in each respective area.
We give seven immediate examples:
1. Ordinary Kripke models become reactive Kripke models which can change
during the process of evaluation, aﬀording a wider class of models for modal
logic. In [Gabbay, 2012b] it is shown that there exists a ﬁnitely axiomatisable
modal logic with a single modality which can be characterised by (is complete
for) a class of reactive modal (Kripke) frames but cannot be charaterised by
any class of ordinary modal (Kripke) frames.
2. Ordinary non-deterministic automata become reactive automata whose tran-
sition table changes every time they make a transition, see [Crochemore and
Gabbay, 2011]. Although it is shown in [Crochemore and Gabbay, 2011] that
every reactive automaton is equivalent to another ordinary non-reactive au-
tomaton, the reactive automata can perform tasks (recognise inputs) with
much less states and represent substantial savings in computational costs!
3. An ordinary graph becomes a reactive graph where there are also arrows
from edges to edges, see [Gabbay and Marcelino, 2009]. This area is very
promising and is now under investigation.
4. Reactive grammars and rewrite systems [Barringer et al., 2009]. For reactive
rewrite systems we get new hierarchies of languages.
Again this area is
actively investigated.
5. A proof system becomes a reactive proof system which changes as we advance
in the proof process, see [Gabbay, 1999; Barringer et al., 2009] and Section
15 below.

What is a Logical System?
89
6. Using Reactive arrows we can solve Contrary to duty paradoxes in Deontic
logic, as well as model delegation in Talmudic logic , see [Gabbay, 2012b;
Gabbay, 2012c; Abraham et al., 2011].
7. In argumentation networks reactive arrows become higher level attacks and
this allows us to generalise the notions of various networks, see [Gabbay,
2012b; Gabbay, 2009; Barringer et al., 2012].
In each case a new class of models/graphs/automata/proof theory is introduced
and some natural questions can be asked.
The following are some sample questions:
1. What is the expressive power/complexity of the new class and what can it
do or not do?
2. What do traditional investigations yield when applied to this new class?
(E.g. completeness theorems, cut elimination, correspondence theory, ax-
ioms, hierarchies of automata, theorems about cycles or classes of graphs,
etc, etc.)
3. How does this concept relate to other dynamic/change concepts already ex-
isting in the literature?
4. Can the new models be reduced to known models via traditional interpre-
tations? What is the edge/advantage we have by using the new reactive
models?
5. Comparison and evaluation of the options for representing reactivity in the
object level vs. metalevel.
Let us give some concrete examples, which will help us get a feel for the notion
of reactivity.
EXAMPLE 35 (Modal logic). Here we take the reactive Kripke models of the
previous sections. The system is the evaluation process (for a given formula). The
reactive parameter is a Kripke model with a point of evaluation t. The environment
(or the faults in the system) is realised by the double arrows ։, which change the
accessibility relation every time we make a move (i.e. every time we continue with
the evaluation).
This example generalises slightly, giving it a familiar everyday meaning. Think
of a network of nodes representing various components of a system. The arrows
represent connections between components and a path through this graph following
the arrows represents a way in which the system can be used. Imagine that the
system is prone to faults and failures. So the use of one component may send a
(double arrow) signal to other components and weaken or inﬂuence them.
We know this to be true for many real life systems.
So the reactivity of a
network is just a measure of the ways it can fail and disappoint you!

90
Dov M. Gabbay
EXAMPLE 36 (Quantiﬁer games). Recall the basic game semantics for the quan-
tiﬁers. Given a classical model m and a formula, say Ψ = ∀x∀y∃z(ϕ(x, y, z) with
ϕ(x, y, z) quantiﬁer free, we play a game over the model between two players, A
(claiming that Ψ holds) and player B claiming that Ψ does not hold. At step 0,
A puts forward Ψ. At step 1 B challenges by choosing a in the domain of m,
and continues to choose an element b in the domain of m. It is the task of player
A to supply a c such that ϕ(a, b, c) holds. Player A has a winning strategy iﬀ
he has a function f such that for any a, b the element c = f(a, b) is such that
m ⊨ϕ(a, b, f(a, b)).
In this case the moves are the choices of elements. The reactive environment
player E has a tinkering function τ : (a, m) 7→mτ
a which changes any model m
and an element a in the domain of m into a new model ma with the same domain.
Thus in order to win, A has to have a winning function λτλxyfτ(x, y) such that
∀τ∀a∀b mτ
a,b ⊨ϕ(a, b, fτ(a, b)).
So as we can see, this deﬁnition covers the modal logic case, if viewed through
its translation to classical logic.
For consider a reactive Kripke model m with actual world a.
Consider the
evaluation a ⊨□2♦q. This is translated into classical logic as
Ψ = ∀x∀y∃z(aRx ∧xRy →yRz ∧Q(z)).
The function τ for changing the model is governed by the double arrows ։ and
we clearly have a special case of the quantiﬁer games tinkering.
We get in this case the formula Ψ′:
Ψ′ = ∀x∀y∃z[aRx ∧xRay →yRa,xz ∧Q(z)]
where Rt denotes the new accessibility relation obtained from R after implementing
the double arrows emanating from it.
EXAMPLE 37 (Automata). A non-deterministic automaton A = (S, M, a, F, Σ)
is characterised by a set of states S, an initial state a ∈S, a set F ⊆S of ﬁnal
states and an alphabet Σ. M is a function giving for each state t ∈S and a letter
σ ∈Σ a new set of states M(t, σ) ⊆S, which are the states that the automaton
A can non-deterministically move to.
Another way to view the automaton is as a multi-modal Kripke model of the
form (S, Rσ, a, F), where Rσ for σ ∈Σ is deﬁned by xRσy iﬀy ∈M(x, σ).
A word of the form (σ1, . . . , σn) is said to be recognisable by the automaton A iﬀ
there exists a sequence of states x1, . . . , xn such that aRσx1∧. . .∧xn−1Rσnxn holds
and xn ∈F. If the atom q is assigned the set F, then (σ1, . . . , σn) is recognisable
by A iﬀA ⊨♦σ1, . . . , ♦σnq, where ♦σ corresponds to Rσ, σ ∈Σ.
Thus naturally we can deﬁne a reactive automaton by adding double arrows
։σ for every σ ∈Σ in the places we want. Figure 4 is an example of such an
automaton. Let Σ = {σ1, σ2}, S = {a, b, c} and F = {c}.

What is a Logical System?
91
b
c
σ2
σ1
σ1
a
σ1
σ1
σ2
σ2
σ2
Figure 4.
We have
Rσ1 = {(a, c), (b, c)}
Rσ2 = {(a, b), (b, c)}
Figure 4 indicates the available double arrows. For example, the ﬁgure shows that
as we move through node a in response to the letter σ1 we disconnect (a, c) and
(b, c) from the Rσ1 relation. If we move through node a in response to the letter
σ2 then we disconnect (a, b) from Rσ2.
It is interesting to note that according to this ﬁgure, if we move out of state
b in response to the letter σ2 then we disconnect the connection (b, c) in the Rσ1
relation but not in the Rσ2 relation.
So there can be an interplay between the modalities here.
The reader should note that we can get a new hierarchy of automata here by
taking the usual hierarchy and making it reactive. See Chapters 8 and 9 below.
The next remark, however, lists some possible options for creating reactivity.
REMARK 38 (Options for the reactive property). We saw that the basic idea of
a reactive action is to change the model every time a move is made. To explain
our options on how to change the model consider Figure 5 below.
From the point t arrows and double arrows emanate. Some may be active and
some may not be. Also observe that we have a double arrow emanating from an
arrow. Let us list what we have:
1. t →b
2. t ։ (t →b)
3. a →b

92
Dov M. Gabbay
a
b
t
Figure 5.
4. (a →b) ։ (t →b).
Our ﬁrst list of options relates to what kind of arrows we allow. Do we want
only arrows emanating from a point or do we also allow arrows emanating from
arcs?
Items 1–3 above emanate from points and item 4 emanates from arcs.
The second list of options has to do with how we use the arrows to change the
model as we pass through a point or an arc.
The ﬁrst possibility is the switch-like use of the arrows. At any given moment
some of the arrows are active (on) and some are dormant (oﬀ). When we pass
through a point or an arc a signal is sent along the arrows emanating from the
said point or arc and reverses the status of the target arrows at the destination.
This is a simple switch action. In general, the reactivity can be more intelligent.
The second possibility is to allow each node and arc from which arrows emanate
to decide, depending on the state (on oﬀ) of the target arrows, which of the target
arrows to switch on and which to switch oﬀ. Care must be taken to ensure that
this decision process is of the same complexity as the original non-reactive model.
So for example in the case of reactive ﬁnite state automaton of Example 37, the
decision of each node which double arrows to activate should be done by another
automaton.
Consider Figure 6 below
Suppose we start with all arcs being switched on at the starting point a. As

What is a Logical System?
93
b
b
. . .
a
b
4
3
2
1
Figure 6.
we move out of a to b, a switch behaviour will switch oﬀall arcs 1, 2, 3, . . .. If we
move through node a again, these arcs will be switched on again.
A more intelligent option might switch them on selectively one at a time, which
each passage through the node a. This would allow us to use them as markers
emulating aspects of a stack. Note that the idea is intuitively sound, and is not
just a technicality. It makes sense to give nodes some intelligence to decide how
to react, based on the situation it ‘sees’.
The above options were deterministic. The most general option was to attach an
automaton at each exit point of arrows (of any kind) to decide what to switch on
and what to switch oﬀ. It is also possible to make these decisions non-deterministic
or probabilistic. Consider Figure 5. We can make all double arrows in this ﬁgure
non-deterministic. So as we pass, for example, from node a to node b the double
arrow to the arc t →b (namely (a →b) ։ (t →b)), if active, may or may not
(non-deterministically) send a signal.
Similarly we may attach probabilities to such connection, say 0.7, and so with
0.7 probability a signal will be sent. We shall elaborate more about this option in
a subsequent paper.
EXAMPLE 39 (Intelligent switches). Previous examples used on and oﬀswitches.
The present example uses an intelligent switching system. In fact, we build on
Example 37 (automata) and show in the present example how to simulate a stack
automaton. We make sure the intelligent switching process is also done by a ﬁnite
automaton. Consider the model described in Figure 7
This model has two relations Rσ1 and Rσ2. It corresponds to an automaton
with alphabet {q+
1 , q−
1 , q+
2 , q−
2 , t, a} and stack letter α. The initial state is a and
the terminal state is t. It is designed to recognise the words of the form σm
i σm
j , i ̸=
j, m ≥1.
The starting state is a. Upon seeing σi the automaton moves to state q+
i and

94
Dov M. Gabbay
b
. . .
3
2
1
α
b
b
b
b
b
t
a
σ2
σ1
σ2
σ1
σ1
σ2
σ2
σ1
σ2
σ1
σ2
σ2
q−
1
q+
1
q+
2
q−
2
σ1
σ2
σ1
σ2
Figure 7.

What is a Logical System?
95
writes α in the stack. It continues to write α as long as it sees σi. When it sees
σj, j ̸= i it moves to q−
i and starts deleting from the stack.
The double arrows are intelligent. The ones emanating from node a or node
q+ activate the ﬁrst highest non-active arrow or double arrow at α. The ones
emanating from a q−deactivate the highest active double arrow from α. If q−
sees that all arrows at α are not active it starts to activate arrow 2 and higher.So
if arrow 1 is active then q−deactivates the top arrow and if arrow 1 is not active
then q−activates the ﬁrst non active arrow above arrow 1. We need to assume
that only a ﬁnite number of arrows and double arrows from α are active at any
given time.
The following table describes the moves of the automaton of Figure 7. Note
that only arrows at α may switch. The initial position is state a with all arrows
at α not active. The terminal state is t.
So let us simulate an input computation.
Starting state
All double arrows and arrows are active except those at α which are not active.
Input step 1
We can assume without loss of generality that we get σ1. So we move along Rσ1
from node a to node q+
1 . This move activates arrow 1 at α. The more σ1 we see
the more double arrows at α are activated in sequence. So if we see a total of m
σ1s, i.e. σm
1 we get that arrows 1, . . . , m are active. The ﬁrst time we see σ2 we
move from q+
1 to q−
1 along Rσ2. There is no Rσ2 double arrow from q+
1 to α.
If we continue to get σ2 at q−
1 we cancel an arrow at α. The minute we get σ1
again or the input ﬁnishes we move to a terminal state t. If the number of σ2 is
equal to σ1, we end up stopping at t with no α arrow active. If the number of σ2
is less than σ1, we end up stopping with some α arrows active. If the number of
σ2 is larger than σ1 we are faced with a situation where the q−
1 automaton sees
no arrow connections at α at all and needs to decide what to do. We can tell it
to activate arrow 2, leaving arrow 1 not active and to continue to activate arrows
3, 4, . . . as long as the input is σ2. If the input is empty or q−
1 sees σ1 again then
it stops.
EXAMPLE 40 (Options for interpreting necessity when double arrows emanate
from arcs). Consider Figure 8 below:
In this ﬁgure we have two double arrows emanating from arcs. We ask: What
semantic meaning can we give to □?
We want to evaluate
a ⊨□□⊥
How do we go about it? Do we go to every accessible point one-by-one and check
whether □⊥holds? Or do we go to all points simultaneously?
When double arrows emanate from points, there is no diﬀerence in the mode of
operation, but when they emanate also from arcs, then there is a diﬀerence

96
Dov M. Gabbay
Table 1.
state
stack at α
Input letter
Reaction
a
all arrows not ac-
tive
σi
move to state q+
i
and
activate arrow 1 at
α
a
some arrows active
any
do not care.
Case
will not arise
q+
i
arrows 1, . . . , m are
active m ≥1
σi
stay at q+
i and acti-
vate arrow m + 1
q+
i
arrows 1, . . . , m are
active
σj, j ̸= i
move to q−
i
q+
i
not the above
any
don’t care.
Case
will not arise
q−
i
arrows 1, . . . , m are
active m ≥1
σj, j ̸= i
stay at q−
i
and de-
activate arrow m.
q−
i
arrows 1, . . . , m are
active, m ≥1
σi or no input
move to t and deac-
tivate arrow m
q−
i
all arrows at α are
not active or arrows
2, . . . , m are active
m ≥2 and arrow 1
not active
no input or input σi
move to t, activate
arrow m + 1. If no
arrow at α is active
then activate arrow
2.
q−
i
same as previous
σj, j ̸= i
remain at q−
i , acti-
vate arrow m + 1 or
if no arrows at α are
active then activate
arrow 2.
q−
i
Diﬀerent from pre-
vious
any
don’t care.
Case
will not arise.
t
any
no input. terminal position
no reaction. termi-
nal position.

What is a Logical System?
97
d
a
b
c
Figure 8.
Case 1
Separate evaluation of □
When we move from a to c, the arc (c, d) is still connected and so we have that
c ⊨¬□⊥. Similarly when we move from a to b, the arc (b, d) is still connected and
so we have that b ⊨¬□⊥.
Therefore, according to the separate evaluation of □, we get that a ⊨□¬□⊥.
Case 2
Simultaneous evaluation of □
If we move to b and to c simultaneously and evaluate at b and at c both arcs
(b, d) and (c, d) are disconnected and so we have b ⊨□⊥and c ⊨□⊥and hence
a ⊨□□⊥.
It makes more sense to adopt the separate evaluation of □because of the tra-
ditional connection of □with ♦, namely □= ¬♦¬ we have:
a ⊨♦A iﬀfor some accessible point s, when we move to s we ahve s ⊨A.
We also have a ⊨□A iﬀfor all accessible points s when we move to s we have
s ⊨A.
To preserve the duality ♦= ¬□¬ we must adopt the separate evaluation of □.
15
REACTIVE PROOF THEORY
We give an example of how to do reactive proof theory. See also [Gabbay, 2013].
The proper environment for developing reactive proof theory is the methodology of
Labelled Deductive Systems, see [Gabbay, 1996]. See also [Barringer et al., 2010]
where reactive rules were independently introduced.
The usual natural deduction propositional system has elimination rules and
introduction rules. In principle an elimination rule has the form
ER
A1, . . . , An
B
where A1, . . . , An, B, are well formed formulas.
and an introduction rule has the form of a subproof:

98
Dov M. Gabbay
IR: To show ϕ(A1, . . . , An, B), start a subproof with A1, . . . , An as addi-
tional assumptions and conclude the subproof successfully with obtaining
B, where ϕ, A1, . . . , An, B are well formed formulas. ϕ is built up from
A1, . . . , An, B.
Well known examples of such rules are the implicational rules:
⇒E
A, A ⇒B
B
⇒I
To show A ⇒B, assume A and prove B
DEFINITION 41 (Proofs of level ≤n). Let S be a set of proof rules.
1. A line (in a proof) is a sequence of the form
ℓ: A, J, ρ, α
where ℓis a line reference (line number), A is the formula of the line, J is
the justiﬁcation of the line, and ρ is the set of proof rules active at this line
and α is a set of line references accessible at the line. ρ changes only in a
reactive proof system. In an ordinary proof system ρ is always the full set
of rules of the logic, and α contains all previous line references.
The justiﬁcation in ℓ: B, J, ρ, α can be either the word “assumption” or the
phrase “A is obtained using the elimination rule ER : A1, . . . , An
A
, where Ai
are formulas obtained in lines ℓi, i = 1, . . . , n and ℓi are accessible”, or the
phrase “A = ϕ(A1, . . . , An, B) where A is a formula of an introduction rule
IR and the justiﬁcation is a subproof π whose assumptions are A1, . . . , An
together with whatever is accessible at ℓ”.
2. A (non-reactive) proof π of level 0 is a sequence of numbered lines containing
formulas and justiﬁcations of the form
line number: wﬀ, Justiﬁcation
such that the beginning of the sequence contains formulas justiﬁed as “as-
sumptions” and any subsequent line has the form ℓ: B, R where R is an
elimination rule of the form A1, . . . , An
B
here ℓi : Ai Justiﬁcation are previ-
ous lines in the sequence. We say that the sequence is a proof of the formula
of the last line from the initial sequence of assumptions.
3. A proof of level ≤n+1 is deﬁned as a sequence π of lines containing formulas
and justiﬁcations where the initial elements of the sequence are all justiﬁed
as “assumptions” and each subsequent formula in line ℓis either justiﬁed
from previous lines using an elimination rule, as described in (2) above, or
is a formula of the form ϕ(A1, . . . , An, B) appearing in an introduction rule,

What is a Logical System?
99
justiﬁed by a proof πϕ of level ≤n, of B from the assumptions A1, . . . , An
and whatever is accessible to ℓ.
If the last line of π contains the formula E and the initial sequence of the
proof are the assumptions D1, . . . , Dk, then we say that π is a level ≤n + 1
proof of E from D1, . . . , Dk.
DEFINITION 42 (Reactive proof system). A reactive system has reactive rules of
the form
Rn : (R, r+
i , r−
j )
i = 1, . . . , k+, j = 1, . . . , k−, n = 1, . . . , m, r±
i ≤m.
where R is a rule (elimination or introduction).
The reading of Rn is that if
we use rule R then the rules Rr+
i , i = 1, . . . , k+ should be activated and rules
Rr−
j , j = 1, . . . , k−should be deactivated.
The notion of a proof is modiﬁed as follows.
At the start of the proof (line 1) we add a third component ρ indicating which
rules are active and a fourth component α indicating which previous lines are
accessible for the purpose of justiﬁcation. The general nature of the logic will
have procedures for telling us given a line ℓin the proof and its α, what will be
the α of the next line? For example if in line ℓ, the justiﬁcation is “assumption”,
then line ℓis available to be used in justiﬁcation of the next line or for example
if line ℓis justiﬁed by an elimination rule of the form A1, . . . , An
B
, where Ai are
from accessible lines ℓi, resp. then lines ℓi are no longer accessible at the next line
(line ℓ+ 1). Lines which are assumptions do not change which rules are active
and which are not. A line which uses a rule Rn as justiﬁcation (either elimination
or introduction) activates or deactivates the other rules as indicated in the rule
itself and the result of the change are the rules ρ which are available for the next
line of the proof. The changes in accessibility α can depend on the logic at hand.
For example as we have mentioned, a resource logic may make assumptions not
accessible once they are used in a justiﬁcation.
EXAMPLE 43. Take the ordinary natural deduction rules for classical logic in-
cluding
A, A ⇒B
B
and
⊥
B
and

100
Dov M. Gabbay
To show A ⇒B assume A and prove B
Assume that using modus ponens deactivates the negation rule.
Thus A, A ⇒⊥̸⊢C by modus ponens alone using level 0 proofs because once
we use modus ponens to get ⊥we do not have the negation rule anymore. We
could work our way around this in this case, using level 1 proofs as follows:
1. A, assumption
2. A ⇒⊥, assumption
3. ⊥⇒C
using ⇒introduction rule,
3.1. ⊥, assumption
3.2. C from 3.1 and the negation rule
3.3. Exit with ⊥⇒C proved
4. ⊥, from 1 and 2 using modus ponens. Note that the negation rule which was
used in the subproof at 3.2 does not cancel the modus ponens in the main
proof.
5. C from 4 and 5 using modus ponens again.
16
EQUATIONAL APPROACH TO LOGIC
The Equational approach to classical logic has its conceptual roots in the 19th
century following the algebraic equational approach to logic by George Boole,
Louis Couturat and Ernst Schr¨oder [Boole, 1847; Couturat, 1914; Schr¨oder, 1890–
1904]. See also [Brown, 2012]. The equational algebraic approach was historically
followed, in the ﬁrst half ofthe 20th century, by the Logical Truth (Tautologies)
approach supported by giants such as G. Frege, D. Hilbert, B. Russell and L.
Wittgenstein. In the second half of the twentieth Century the new current ap-
proach has emerged, which was to study logic through it consequence relations, as
developed by A. Tarski, G. Gentzen, D. Scott and (for non-monotonic logic) D.
Gabbay.
Let us give a brief introduction.
DEFINITION 44. Classical propositional logic has the language of a set of atomic
propositions Q (which we assume to be ﬁnite for our purposes) and the connectives
¬ and ∧. A classical model is an assignment h : Q 7→{0, 1}. h can be extended
to all wﬀs by the following clauses:
• h(A ∧B) = 1 iﬀh(A) = h(B) = 1
• h(¬A) = 1 −h(A)

What is a Logical System?
101
The set of tautologies are all wﬀs A such that for all assignments h, h(A) = 1.
The other connectives can be deﬁned as usual
a →b = def. ¬(a ∧¬b)
a ∨b = ¬a →b = ¬(¬a ∧¬b)
DEFINITION 45.
1. A numerical conjunction is a binary function µ(x, y) from [0, 1]2 7→[0, 1]
satisfying the following conditions
(a) µ is associative and commutative
µ(x, µ(y, z)) = µ(µ(x, y), z)
µ(x, y) = µ(y, x)
(b) µ(x, 1) = x
(c) x < 1 ⇒µ(x, y) < 1
(d) µ(x, y) = 1 ⇒x = y = 1
(e) µ(x, 0) = 0
(f) µ(x, y) = 0 ⇒x = 0 or y = 0
2. We give two examples of a numerical conjunction
n(x, y) = min(x, y)
m(x, y) = xy
For more such functions see the Wikipedia entry on t-norms [Wikipedia, ].
However, not all t-norms satisfy condition (f) above.
DEFINITION 46.
1. Given a numerical conjunction µ, we can deﬁne the following numerical
(fuzzy) version of classical logic.
(a) An assignment is any function h from wﬀinto [0, 1].
(b) h can be extended to hµ deﬁned for any formula by using µ by the
following clauses:
• hµ(A ∧B) = µ(hµ(A), hµ(B))
• hµ(¬A) = 1 −hµ(A)
2. We call µ-tautologies all wﬀs A such that for all h, hµ(A) = 1.
REMARK 47. Note that on {0, 1}, hµ is the same as h. In other words, if we
assign to the atoms value in {0, 1}, then hµ(A) ∈{0, 1} for any A. This is why we
also refer to µ as “semantics”.

102
Dov M. Gabbay
The diﬀerence in such cases is in solving equations, and the values they give to
the variables 0 < x < 1.
Consider the equation arising from (x →x) ↔¬(x →x). We want
hm(x →x) = hm(¬(x →x))
We get
(1 −m(x))m(x) = [1 −m(x) · (1 −m(x))]
or equivalently
m(x)2 −m(x) + 1
2 = 0.
Which is the same as
(m(x) −1
2)2 + 1
4 = 0.
There is no real numbers solution to this equation.
However, if we use the n semantics we get
hn(x →x) = hn(¬(x →x))
or
min(n(x), (1 −n(x)) = 1 −min(n(x), 1 −n(x))
n(x) = 1
2 is a solution.
Note that if we allow n to give values to the atoms in {0, 1
2, 1}, then all formulas
A will continue to get values in {0, 1
2, 1}. I.e. {0, 1
2, 1} is closed under the function
n, and the function ν(x) = 1 −x.
Also all equations with n can be solved in {0, 1
2, 1}.
This is not the case for m. Consider for the example the the equation corre-
sponding to x ≡x ∧. . . ∧x, (n + 1 times).
The equation is x = xn+1. We have the solutions x = 0, x = 1 and all roots of
unity of xn = 1.
DEFINITION 48. Let I be a set of real numbers {0, 1} ⊆I ⊆[0, 1]. Let µ be a
semantics. We say that I supports µ iﬀthe following holds:
1. For any x, y ∈I, µ(x, y) and ν(x) = 1 −x are also in I.
2. By a µ expression we mean the following
(a) x is a µ expression, for x atomic
(b) If X and Y are µ expressions then so are ν(X) = (1 −X) and µ(X, Y )
3. We require that any equation of the form E1 = E2, where E1 and E2 are µ
expressions has a solution in I, if it is at all solvable in the real numbers.

What is a Logical System?
103
A
B
¬A
A ∧B
A ∨B
A →B
0
0
1
0
0
1
0
1
2
1
0
1
2
1
0
1
1
0
1
1
1
2
0
1
2
0
1
2
1
2
1
2
1
2
1
2
1
2
1
2
1
2
1
2
1
1
2
1
2
1
1
1
0
0
0
1
0
1
1
2
0
1
2
1
1
2
1
1
0
1
1
1
Figure 9.
REMARK 49. Note that it may look like we are doing fuzzy logic, with numerical
conjunctions instead of t-norms.
It looks like we are taking the set of values
{0, 1} ⊆I ⊆[0, 1] and allowing for assignments h from the atoms into I and
assuming that I is closed under the application of µ and ν(x). For µ = n, we do
indeed get a three valued fuzzy logic with the following truth table, Figure 9.
Note that we get the same system only because our requirement for solving
equations is also supported by {0, 1
2, 1} for n.
The case for m is diﬀerent. The values we need are all solutions of all possible
equations. It is not the case that we choose a set I of truth values and close under
m, and ν.
It is the case of identifying the set of zeros of certain polynomials (the polyno-
mials arising from equations). This is an algebraic geometry exercise.
REMARK 50. The equational approach allows us to model what is considered
traditionally inconsistent theories, if we are prepared to go beyond {0, 1} values.
Consider the liar paradox a ↔¬a. The equation for this is (both for m for n)
a = 1 −a (we are writing ‘a’ for ‘m(a)’ or ‘n(a)’ f). This solves to a = 1
2.
16.1
Theories and equations
The next series of deﬁnitions will introduce the methodology involved in the equa-
tional point of view.
DEFINITION 51.
1.
(a) A classical equational theory has the form
∆= {Ai ↔Bi | i = 1, 2, . . .}
where Ai, Bi are wﬀs.

104
Dov M. Gabbay
(b) A theory is called a B-theory19 if it has the form
xi ↔Ai
where xi are atomic, and for each atom y there exists at most one i
such that y = xi.
2.
(a) A function f: wﬀ→[0, 1] is an µ model of the theory if we have that f
is a solution of the system of equations Eq(∆).
hµ(Ai) = hµ(Bi), i = 1, 2, . . .
(b) ∆is µ consistent if it has an µ model
3. We say that a theory ∆µ semantically (equationally) implies a theory Γ if
every solution of Eq(∆) is also a solution of Eq(Γ).
We write
∆⊨µ Γ.
Let K be a family of functions from the set of wﬀto [0, 1]. We say that
∆⊨(µ,K) Γ if every µ solution f of Eq(∆) such that f ∈K is also an µ
solution of Eq(Γ).
4. We write
A ⊨µ B
iﬀthe theory ⊤↔A semantically (equationally) implies ⊤↔B.
Similarly we write A ⊨(µ,K) B. In other words, if for all suitable solutions
f, f(A) = 1 implies f(B) = 1.
EXAMPLE 52.
1. Consider A ∧(A →B) does it m imply B? The answer is yes.
Assume m(A ∧(A →B)) = 1 then m(A)(1 −m(A)(1 −m(B))) = 1. Hence
m(A) = 1 and m(A)(1 −m(B)) = 0. So m(B) = 1.
We now check whether we always have that m(A ∧(A →B) →B) = 1.
We calculate m(A ∧(A →B) →B) = [1 −m(A ∧(A →B))(1 −m(B))].
= [1 −m(A)(1 −m(A)(1 −m(B))x(1 −m(B))]
Let m(A) = m(B) = 1
2. we get
= [1 −1
2(1 −1
2 × 1
2) · 1
2 = 1 −3
16 = 13
16.
19B for Brouwer, because we are going to use Brouwer’s ﬁxed point theorem to show that
theories always have models.

What is a Logical System?
105
Thus the deduction theorem does not hold. We have
A ∧(A →B) ⊨B
but
̸⊨A ∧(A →B) →B.
2.
(a) Note that the theory ¬a ↔a is not ({0, 1}, m) consistent while it is
({0, 1
2, 1}, m) consistent.
(b) The theory (x →x) ↔¬(x →x) is not ([0, 1], m) consistent but it is
({0, 1
2, 1}, n) consistent, but not ({0, 1}, n) consistent.
REMARK 53. We saw that the equation theory x ∧¬x ↔¬(x ∧¬x) has no
solutions (no m-models) in [0, 1]. Is there a way to restrict m theories so that we
are assured of solutions? The answer is yes. We look at B-theories of the form
xi ↔Ei where xi is atomic and for each x there exists at most one clause in the
theory of the form x ↔E. These we called B theories. Note that if x = ⊤, we
can have several clauses for it. The reason is that we can combine
⊤↔E1
⊤↔E2
into
⊤↔E1 ∧E2.
The reason is that the ﬁrst two equations require
m(Ei) = m(⊤) = 1
which is the same as
m(E1 ∧E2) = m(E1) · m(E2) = 1.
If x is atomic diﬀerent from ⊤, this will not work because
x ↔Ei
requires m(x) = m(Ei) while x ↔E1 ∧E2 requires m(x) = m(E1)m(E2).
The above observation is important because logical axioms have the form ⊤↔A
and so we can take the conjunction of the axioms and that will be a theory in our
new sense.
In fact, as long as our µ satisﬁes
µ(A ∧B) = 1 ⇒µ(A) = µ(B) = 1
we are OK.
THEOREM 54. Let ∆be a B-theory of the form
xi ↔Ei.
Then for any continuous µ, ∆has a ([0, 1], µ) model.

106
Dov M. Gabbay
Proof. Follows from Brouwer’s ﬁxed point theorem, because our equations have
the form
f(⃗x) = f( ⃗E(⃗x))
in [0, 1]n where ⃗x = (x1, . . . , xn) and ⃗E = (E1, . . . , En).
■
REMARK 55. If we look at B-theories, then no matter what µ we choose, such
theories have µ-models in [0, 1]. We get that all theories are µ-consistent. A logic
where everything is consistent is not that interesting.
It is interesting, therefore, to deﬁne classes of µ models according to some
meaningful properties.
For example the class of all {0, 1} models.
There are
other classes of interest. The terminology we use is intended to parallel semantical
concepts used and from argumentation theory.
DEFINITION 56. Let ∆be a B-theory. Let f be a µ-model of ∆. Let A be a wﬀ.
1. We say f(A) is crisp (or decided) if f(A) is either 0 or 1. Otherwise we say
f(A) is fuzzy or undecided.
2.
(a) f is said to be crisp if f(A) is crisp for all A.
(b) We say that f ≤g, if for all A, if f(A) = 1 then g(A) = 1, and if
f(A) = 0 then g(A) = 0.
We say f < g if f ≤g and for some A, f(A) ̸∈{0, 1} but g(A) ∈{0, 1}.
Note that the order relates to crisp values only.
3. Deﬁne the µ-crisp (or µ-stable) semantics for ∆to be the set of all crisp
µ-model of ∆.
4. Deﬁne the µ-grounded semantics for ∆to be the set of all µ-models f of ∆
such that there is no µ-model g of ∆such that g < f.
5. Deﬁne the µ-preferred semantics of ∆to be the set of all µ-models f of ∆
such that there is no µ-model g of ∆with f < g.
6. If K is a set of µ models, we therefore have the notion of ∆⊨K Γ for two
theories ∆and Γ.
16.2
Generating B-theories
DEFINITION 57. Let S be a ﬁnite set of atoms and let Ra and Rs be two binary
relations on S. We use A = (S, Ra, Rs) to generate a B-theory which we call the
argumentation network theory generated on S from the attack relation Ra and
the support relation Rs.
For any x ∈S, let y1, . . . , ym be all the elements y of S such that yRax and let
z1, . . . , zn be all the elements z of S such that xRsz (of course m, n depend on x).
Write the theory ∆A.
{x ↔
^
zj ∧
^
¬yi | x ∈S}

What is a Logical System?
107
b
a
Figure 10.
We understand the empty conjunction as ⊤.
These generate equations
x = min(zj, 1 −yi)
using the n function or
x = (Πjzj)(Πi(1 −yi))
using the m function.
REMARK 58.
1. If we look at a system with attacks only of the form A = (S, Ra) and consider
the n(min) equational approach for [0, 1] then n models of the corresponding
B-theory ∆A correspond exactly to the complete extensions of (S, Ra). This
was extensively investigated in [Gabbay, 2012; Gabbay, 2013a]. The seman-
tics deﬁned in Deﬁnition 56, the stable, grounded an preferred n-semantics
correspond to the same named semantics in argumentation, when restricted
to B-theories arising from argumentation.
If we look at µ other than n, example we look at µ = m, we get diﬀer-
ent semantics and extensions for argumentation networks. For example the
network of Figure 10 has the n extensions {a = 1, b = 0} and {a = b = 1
2}
while it has the unique m extension {a = 1, b = 0}.
2. This correspondence suggests new concepts in the theory of abstract argu-
mentation itself. Let ∆A, ∆B be two B-theories arising from two abstract
argumentation system A = (S, RA) and B = (S, RB) based on the same set
S. Then the notion of ∆A ⊨K ∆B as deﬁned in Deﬁnition 51 suggest the
following consequence relation for abstract argumentation theory.
• A ⊨K B iﬀany K-extension (K=complete, grounded, stable, preferred)
of A is also a K-extension of B.
So, for example, the network of Figure 11(a) semantically entails the network
of Figure 11(b).
REMARK 59. We can use the connection of equational B-theories with argumen-
tation networks to export belief revision and belief merging from classical logic

108
Dov M. Gabbay
(b)
x
y
x
y
(a)
Figure 11.
into argumentation. There has been considerable research into merging of argu-
mentation networks. Classical belief merging oﬀers a simple solution. We only
hint here, the full study is elsewhere [Gabbay and Rodrigues, 2012].
Let Ai = (S, Ri), i = 1, . . . , n, be the argumentation networks to be merged
based on the same S. Let ∆i be the corresponding equational theories with the
corresponding semantics, based on n. Let fi be respective models of ∆i and let µ
be a merging function, say µ = m.
Let f = µ(f1, . . . , fn). Then the set of all such fs is the semantics for the merge
result. Each such an f yields an extension.
REMARK 60. The equational approach also allows us to generate more general
abstract argumentation networks. The set S in (S, Ra) need not be a set of atoms.
It can be a set of wﬀs.
Thus following Deﬁnition 57 and remark 58, we get the equations (for each
A, Bj and where Bj are all the attackers of A:
f(A) = µ(f(¬B1), . . . , ).
There may not be a solution.
17
NON-DETERMINISTIC SEMANTICS FOR LOGICAL SYSTEMS AND
THE EQUATIONAL APPROACH
Recently Arnon Avron and his collaborators, (see [Avron and Zamansky, 2011] for
a survey) came with the idea of non-deterministic semantics for logical systems
. This enabled him to give semantical map of many paraconsistent systems. We
quote Avron own words
The principle of truth-functionality (or compositionality) is a basic
principle in many-valued logic in general, and in classical logic in par-
ticular. According to this principle, the truth-value of a complex for-
mula is uniquely determined by the truth-values of its subformulas.
However, real-world information is inescapably incomplete, uncertain,

What is a Logical System?
109
vague, imprecise or inconsistent, and these phenomena are in an ob-
vious conﬂict with the principle of truth-functionality. One possible
solution to this problem is to relax this principle by borrowing from
automata and computability theory the idea of non-deterministic com-
putations, and apply it in evaluations of truth-values of formulas. This
leads to the introduction of non-deterministic matrices (Nmatrices) —
a natural generalization of ordinary multi-valued matrices, in which the
truth-value of a complex formula can be chosen non-deterministically
out of some non-empty set of options. There are many natural motiva-
tions for introducing non-determinism into the truth-tables of logical
connectives.
A non-deterministic ﬁnite truth table can be deﬁned by a system of equations
on the truth values. Thus we get a connection with the equational approach . The
way this works is best illustrated by an example
EXAMPLE 61. Consider the following Hilbert style system for what is frequently
called Beziau Logic K/2 [Beziau, 1999], and is a dual of the logic CLuN of Batens
[Batens et al., 1999]:
Hilbert Axioms with Modus Ponens
1. a →(b →a)
2. (a →(b →c)) →((a →b) →(a →c))
3. ((a →b) →a) →a
4. a →(¬a →b)
The ﬁrst 3 axioms deﬁne classical implication, and the truth table and equation
for it are known:
a →b = max(b, 1 −a)
However the fourth axiom is only one of the axioms for classical negation and it
is not suﬃcient by itself to force the classical semantics for this negation. Can we
supply semantics adequate for the negation of this system?
Let us use the equations for implication for axiom 4, regarding both a and ¬a
as unknown variables . We get Equation (*)
max(max(1 −¬a), b), 1 −a) = 1
(∗)
We observe that if a = 0 then ¬a can be any value and still satisfy (*), while if
b = 0, and a = 1, then ¬a must be 0.
We thus get the following non-deterministic truth table for negation
¬⊥= {⊤, ⊥}
¬⊤= ⊥

110
Dov M. Gabbay
This is Avron and Lev’s non-deterministic truth table semantics (Nmatrix) for
this logic that was given in [Avron and Lev, 2005] (and is essentially identical to
the bivaluation semantics of it given in [Beziau, 1999]).
The equation for it is
¬a × a = 0
(∗∗)
Of course we still need to prove completeness for the three equations, the one
for implication and (*) and (**) for negation, namely the set of formulas which
identically satisfy these equations is exactly what can be proved in the Hilbert
system formulation. This is done by Avron for the non-deterministic truth table.
EXAMPLE 62. Let us add to the logic of Example 61 the additional negation
axiom (6).
6. ¬¬a →a
We need to provide non-deterministic matrix semantics for this set of axioms.
Avron has done that in [Avron, 2014]. He uses 3-valued non-determinstic matrix
logic. The values are {⊥, 1
2, ⊤} where 1
2 is an additioanl intermediate value. ⊤is
the designated value. The table for →is as follows:
a →b =



⊤, if a = 1
2 or a = ⊥or b = ⊤
{⊥, 1
2}, if otherwise.
(i.e. if a = ⊤and b ∈{⊥, 1
2} then the value of a →b is in {⊥, 1
2}).
The table for ¬ is as follows:
¬a =



{⊥, 1
2} if a = ⊤
1
2 if a = 1
2
⊤if a = ⊥
Under these tables all axioms get value ⊤under any substitution.
The question we can ask is why suddenly we move to guessing a 3-valued table?
The heuristic answer is that we have axioms involving ¬a and also ¬¬a. We may
need separate values for ¬a and ¬¬a. Had we had axioms involving ¬a, ¬¬a and
¬¬¬a, we would have tried to guess a suitable 4-valued table. Each ¬ma requires
possibly an intermediate value for it.
Note, by the way, that being a ﬁnite Boolean table it can be presented equa-
tionally. Write 1 = ⊤and 0 = ⊥, let us look at equations for our truth tables.
Let δx(y) be the well known delta fumction.
δx(y) =
 1, if x = y
0, if x ̸= y
Let x1, . . . , xn be truth values and let q be an atom. Then the assignment q = x
can be written as δq(x) = 1.
So the truth table for →can be written as:
({[δa( 1
2)∨δa(0)∨δb(1)]∧δa→b(1)}∨{δa(1)∧(δb(0)∨δb( 1
2))∧((δa→b(0)∨δa→b( 1
2))}) = 1

What is a Logical System?
111
Similarly
[δa(1) ∧((δ¬a(0) ∨δ¬a( 1
2)) ∨δa( 1
2) ∧δ¬a( 1
2) ∨δa(0) ∧δ¬a(1)] = 1.
The fact that the non-deterministic matrix approach can be presented by the
equational approach is not the main idea from the point of view of what is a
logical system. The important idea is that we have here a new more powerful
semantics.
Avron and Lev have shown [Avron and Lev, 2005, Thorem 3.4] that there
exist logics which can be characterised by a ﬁnite non-deterministic matrix (and
therefore equally be equational semantics) which cannot be characterised by any
ﬁnite truth table deterministic matrix. In fact, the Beziau Baten logic (axioms
1–5) of Example 61, is one such logic.
18
INPUT OUTPUT LOGICS
So far the notions of consequence we had all agreed that if a declarative unit (well
formed formula) of a logic is a member of a theory/database ∆of such logic, then
it is also a logical consequence of it In particular we have the identity
X ⊢X.
D. Makinson and L. van der Torre rejected this rule for their own reasons and
preferred to look at a database as a dynamic input output engine. We give ∆an
input x and it outputs some y.
Thus the database is a set of pairs of the form (x,y), together with rules of how
to keep on iterating the use of the the input output engine. Diﬀerent logics are
obtained by diﬀerent iteration procedures. To quote Makinson and Torre:
Input/output logic takes its origin in the study of conditional norms.
These may express desired features of a situation, obligations under
some legal, moral or practical code, goals, contingency plans, advice,
etc. Typically they may be expressed in terms like: In such-and-such a
situation, so and- so should be the case, or . . . should be brought about,
or . . . should be worked towards, or . . . should be followed — these lo-
cutions corresponding roughly to the kinds of norm mentioned. To be
more accurate, input/output logic has its source in a tension between
the philosophy of norms and formal work of deontic logicians . . . Like
every other approach to deontic logic, input/output logic must face the
problem of accounting adequately for the behaviour of what are called
‘contrary-to-duty’ norms. The problem may be stated thus: given a
set of norms to be applied, how should we determine which obligations
are operative in a situation that already violates some among them.
It appears that input/output logic provides a convenient platform for
dealing with this problem by imposing consistency constraints on the

112
Dov M. Gabbay
generation of output. We do not treat conditional norms as bearing
truth-values.
They are not embedded in compound formulae using
truth-functional connectives. To avoid all confusion, they are not even
treated as formulae, but simply as ordered pairs (a, x) of purely boolean
(or eventually ﬁrst-order) formulae. Technically, a normative code is
seen as a set G of conditional norms, i.e. a set of such ordered pairs
(a, x). For each such pair, the body a is thought of as an input, repre-
senting some condition or situation, and the head x is thought of as an
output, representing what the norm tells us to be desirable, obligatory
or whatever in that situation. The task of logic is seen as a modest
one.
It is not to create or determine a distinguished set of norms,
but rather to prepare information before it goes in as input to such a
set G, to unpack output as it emerges and, if needed, coordinate the
two in certain ways. A set G of conditional norms is thus seen as a
transformation device, and the task of logic is to act as its ‘secretarial
assistant’.
See [Makinson and van der Torre, 2000; Makinson and van der Torre, 2001; Parent
et al., 2014; Makinson and van der Torre, 2001b].
In the above we emphasized the lack of identity, and the non-monotonic nature
of constrained output for contrary to duty reasoning. The lack of identity is surely
an important feature, from the “What is a logic” point of view, as well as the
issues it raises (like contrary to duty reasoning). What does (p, p) mean (e.g. self
explanatory in causal reasoning), what does (p, ¬p) mean (e.g. violation in deontic
logic), and how do the logics work? However, from the point of view of the input
output logic itself, the constraints/non-monotonic reasoning part is less central,
and alternative ways could be imagined. In addition, we would like to emphasize
the role of norms in input/output logic, as it derives from normative systems in
the tradition of Alchourron. It is a classic issue in preference: some preference
is intrinsic, so concerns the agent itself (desires), and some other preferences are
extrinsic, coming from the outside (obligation). We refer to the extrinsic source
as the “norms”.
19
ARGUMENTATION NETWORKS AS LOGICS
Consider an argumentation network (S, R), where S is a non empty set and R
is a binary relation on S. How can we turn it into a logic? One obvious way
of doing it is to go through a semantical interpretation. This is an ordering, a
set of worlds with a binary relation and so we can regard it as a Kripke model
for some logic. It can thus deﬁne a modal logic, or an intuitionistic intermediate
logic, or a provability logic, etc., etc. This is not satisfactory. We must do this in
a meaningful way compatible with its meaning as an argumentation network and
compatible with our discussion so far of what is a logical system. The common
ground is that of a coherent position. Our logical systems so far can prove many

What is a Logical System?
113
conﬂicting outputs.
We want to see what coherent positions we can adopt as
subsets of these outputs. Here comes the role of argumentation.
Now that we have the idea, all that remains is to ﬁgure out the coherent technical
details. This we now do.
DEFINITION 63 (Function spaces). By a function space we mean the following
1. A set V with a family of functions F of the form
h : V n 7→V
For n = 1, 2, . . ..
The set V may have some operations on it by which means the functions h
in F are deﬁned. We give two examples:
(a) V = [0, 1], the set of all real numbers between 0 and 1 and F is the set
of all continuous functions with any number of variables.
(b) V is a complete partial order (i.e. it has a partial ordering on it and any
subset V0 ⊆V has a least upper bound and a greatest lower bound) and
F is the set of all monotonic functions on V in any number of variables.
2. We assume (V, F) satisfy the following.
Let h1, . . . , hn be n functions in
x1, . . . , xn (same n). Consider the vector function on V n deﬁned by
⃗y = ⃗h(⃗x)
Where ⃗x = (x1, . . . , xn), ⃗y = (y1, . . . , yn) and ⃗h = (h1, . . . , hn).
Then we assume that any such vector function for any n has at least one
ﬁxed point ⃗x0, i.e. we have
⃗x0 = ⃗h(⃗x0).
Note that the ﬁxed point condition holds for [0, 1] because of Brouwer’s ﬁxed
point theorem and for the complete partial orders because of Tarksi’s ﬁxed
point theorem.
DEFINITION 64 (Logics, models, theories and inconsistency). Let (V, F) be a
function space.
1. By a (V, F) logic we mean a system (S, R, ht), t ∈S where S is a ﬁnite set,
R ⊆S2, and for each t ∈S, the following holds:
Let x1, . . . , xn be all points in S such that tRxi holds. Then ht is an n-place
function from F.
It is important to note that when we write ht, then the variables x1, . . . , xn
are ordered inside ht. We are not assuming that ht is a symmetric function in
its variables. Where does this order come from? It comes from the geometry
of (S, R).

114
Dov M. Gabbay
x
y
x′
z
w
u = end point
Figure 12.
Consider for example Figure 12, the points x′, y, w are distinguishable indi-
vidually by the geometry of the graph and so we can order them as we see
ﬁt when we plug them into hz.
It is expected that should the geometry not distinguish between any two
points, say x1 and x2, then ht must be symmetrical in these variables.
2. By a model for the logic (S, R, ht) we mean any function f from S into V
such that for all t ∈S we have
(*1) f(t) = ht(f(x1), . . . , f(xn)).
Such fs exist because of the ﬁxed point theorem for (V, F). f is not unique
for solving (*1)
3. Let (S, R, ht) be a logic and let s, t ∈S. We say s ⊨t iﬀfor any model f we
have
(*2) f(s) = 1 ⇒f(t) = 1.
4. Note that (S, R) corresponds to the set of wﬀs of the language and f to truth
value assignments to the wﬀs. The partial ordering of (1b) of Deﬁnition 63
above corresponds to the Lindenbaum algebra of the logic. The functions
ht are the truth tables of the connectives. R corresponds to the inductive
construction tree of the wﬀs, except that it needs not the tree but could be
a general binary dependence relation!
This is a sort of “free style” notion of a wﬀ.
A wﬀis just an abstract point with a relation R saying xRy, which means y
is an immediate subformula of x.
5. A theory in traditional logic is a set of wﬀs required to be true.
In our
context, to require a set of nodes to be true means to impose constraints
on the models, i.e. on the function f, i.e. on the solution of the equations of
(*1).

What is a Logical System?
115
Since f is a general function from S into V , there are many types of con-
straints we can impose, e.g. that f has a minimal number of 0 values or that it
has a minimal number of 1
2 values (this would yield circumscription minimal
models in the right context and right formulation for the case of 0 values, or
would yield semi-stable semantics for the case of argumentation and value
1
2).
So we deﬁne
(*3) A logical theory is a set of constraints on the solutions (models) fs. It is
inconsistent if the constraints have no solution.
See [Gabbay, 2013] for examples.
EXAMPLE 65 (Geometrical proof rules, geometric inconsistency).
1. Let (S, R) be an argumentation frame. A unary geometrical proof rule r is
a ﬁnite ordering (which can be represented visually by a ﬁgure) of the form
r = (Pr, Rr, x, y) where Pr is a set of nodes and Rr is a binary relation on
Pr, and x, y are two distinct points in Pr. x is called the input point (the
premise of the rule) and y is called the output point (the conclusion of the
rule). We require that x and y be geometrically deﬁnable in (S, R) in terms
of information in (Pr, Rr). Figure 13 shows two such rules corresponding to
the rules
r1 :
A
¬¬A
And
r2 : ¬¬A
A
2. A binary geometrical proof rule for (S, R) has the form r = (Pr, Rr, x, z′, y)
where (Pr, Rr) is ﬁnite ordering and x, z′, y ∈Pr.
x is the input point,
z′ is the base point and y is the output point.
We require that x, y, z′
be geometrically identiﬁable in (S, R) in terms of information available in
(Pr, Rr).
3. What do we mean by a point being geometrically identiﬁable in (S, R) in
terms of information available in (Pr, Rr)? Consider Figure 14.
This pattern requires in (S, R) two points; the ﬁrst being an endpoint at-
tacking the second. The information a = endpoint is for (S, R).
4. What do we mean by geometric inconsistency? We need to have a family
of subsets of the network S which are marked unacceptable.
Intuitively
each such subset corresponds to a constraint of, say, wanting an equational
solution which gives all members of the subset value 1. So the family is of
all subsets for which there is no solution.
A formal deﬁnition is given in Deﬁnition 66.

116
Dov M. Gabbay
u
y
Rule r1
u
x
y
x
Rule r2
Figure 13.
b
a
end point
Figure 14.

What is a Logical System?
117
x
x′
y
z
z′
Figure 15.
Another example of a pattern for geometrical modus ponens is Figure 15.
The points x, y, z′ in this ﬁgure are deﬁnable using Rr.
DEFINITION 66 (Formal deﬁnition of geometric proof rules and inconsistency).
1. Let (S, R) be an argumentation network with R ⊆S × S. Consider ﬁrst
order predicate logic with a binary relation R. Let Ψ(x) be a formula in this
language with a free variable x. Let a ∈S. We can ask
(S, R) ⊨?Ψ(a)
For example
Ψ1(x) = ¬∃y(xRy)
Says that x is an endpoint in (S, R) or
Ψ2(x) = ∃y(xRy ∧yRy)
We call such formulas Ψ(x) as additional information about node x formu-
lated in the ﬁrst order predicate logic of the binary relation R.
2. A geometric rule r has the form
r = (Pr, Rr, Ψt(x), a1, . . . , ak, c, b), for t in Pr
Where a, ai, c, b ∈Pr.
a1, . . . , ak are inputs. c is the base and b is the output.
Intuitively think of it as a1 ∧. . . ∧ak ⇒c b.
(Pr, Rr) is a ﬁnite ordering and for each t ∈Pr, Ψt(t) is additional informa-
tion about t in the sense of item (1) above.

118
Dov M. Gabbay
x
y
x′
z
w
u = end point
Figure 16.
3. Let r be a geometric rule, and let (S, R) be an argumentation network. Let
µ : Pr 7→S be a one to one embedding of Pr into S. We say µ identiﬁes the
pattern of the rule r in µ(Pr) iﬀthe following holds.
(a) xRry iﬀµ(x)Rµ(y) for all x, y ∈Pr.
(b) For all x ∈Pr, (S, R) ⊨Ψx(µ(x)).
4. We require also that the points µ(a1), . . . , µ(ak) µ(c) and µ(b) are uniquely
identiﬁable in (S, R). More precisely there are properties ϕa1(x), . . . , ϕak(x),
ϕc(x), ϕb(x) such that for each y′, y ∈{a1, . . . , ak, c, b) we have
(S, R) ⊨ϕy(µ(y)) ∧
^
y′̸=y
¬ϕy′(µ(y)).
5. An inconsistency notion is just a family of subsets of S. For monotonic logic
the family is closed under enlargement
EXAMPLE 67. Consider the pattern of Figure 14.
Here
Ψa(x) = ¬∃yxRy
Ψb(x) = x = x.
Consider the following embeddings of this pattern into Figure 16. (I chose this
ﬁgure at random, just for illustration).
If we match (a, b) with (y, z), we get a good embedding because y satisﬁes Ψa,
but if we embed (a, b) as (x′, z), then the conditions are not satisﬁed.
Note that from now on to simplify notation we regard µ as the identity and talk
about Pr ⊆S and Rr ⊆R.
DEFINITION 68 (Geometrical proofs).
Let (S, R) be part of a logic as deﬁned in
Deﬁnition 64. Let T ⊆S be any subset. Let r1, . . . , rk be proof rules. We deﬁne
the notion of
• the sequence (x1, . . . , xn), n ≥1, xi ∈S is a proof of level m ≥0 of xn from
T, using r1, . . . , rk.

What is a Logical System?
119
The deﬁnition is by induction on m and n.
Case n = 1, m = 0
x1 is a proof from T if x1 ∈T.
Case n + 1, m = 0
(x1, . . . , xn+1) is a proof from T iﬀone of the following holds:
1. xn+1 ∈T
2. xn+1 is obtained from some xi, i ≤n using a unary geometrical rule r =
(Pr, Rr, x, y) such that Pr ⊆S, Rr ⊆R and xi is the input (x = xi) and xn
is the output (y = xn).
3. For some xj1, . . . , xjk, xj, i, j ≤n and some geometrical rule r = (Pr, Rr, x1, . . . ,
xk, z′, y) we have Pr ⊆S, Rr ⊆R, xji = xi, z′ = xj and y = xn+1.
Case level m + 1
Assume that for each T and any m′ ≤m and any n we have deﬁned the
notion of x1, . . . , xn is a proof of xn of level ≤m′. We now deﬁne this notion
for level m + 1 and n ≥1. Let cases (1)–(3) be as above for level ≤m. We
add more cases
4. Case m + 1, n = 1
For some rule r = (Pr, Rr, x′
j, z′, y) we have that there exists (y1, . . . , yn′), yn′ =
y which is a proof of level ≤m of y from T ∪{x′
j}. We also have z′ = x1.
5. Case m + 1, n > 1
For some rule as in (4), we have that there exists (y1, . . . , yn′) yn′ = y, which
is a proof of level ≤m of y from T ∪{x′
j} ∪{x1, . . . , xn−1}. We also have
z′ = xn.
REMARK 69. Note that in logic based argumentation networks (see [Caminada
and Amgoud, 2007] or [Hunter, 2010]) only level 0 proofs are used. The rules have
the form A1 ∧. . . ∧An ⇒c B and only ⇒c eliminations are used.
DEFINITION 70 (Soundness of rules). Let (S, R, ht) be a logic in the sense of
Deﬁnition 64. Let r1, . . . , rk be rules in the sense of Deﬁnition 66. We say the
rules are sound iﬀwhenever b is proved from a in (S, R) as in Deﬁnition 68 for
a, b ∈S then a ⊢b holds as deﬁned in Deﬁnition 64.
We say the rules are complete iﬀwe have
• a ⊢b iﬀb is provable from a using the rules.
If a subset of S is marked inconsistent then the constraint arising from that set
cannot be solved.
EXAMPLE 71 (Defeasible rules). Ordinary implication (strict implication) we can
write as A1 ∧. . . ∧An →B or as A1 →(A2 →. . . →(An →B) . . .).

120
Dov M. Gabbay
input node x
output node y
z
x′
z = x →y, base node x′ is an auxiliary node so that we can tell the diﬀerence
between input and output. When identifying this pattern in an argumentation
network it is required that nodes z and x′ bear exactly the attacks shown in the
pattern.
Figure 17.
e
input node x
output node y
z
x′
z = x ⇒y, base node x′ is an auxiliary node. When identifying this pattern in
an argumentation network it is required that nodes z, e and x′ bear exactly the
attacks shown in the pattern.
Figure 18.

What is a Logical System?
121
a′
n
a1
a2
a3
...
an−1
an
b1
b2
b3
...
bn
bn−1
e1
e2
e3
en−1
en
b0
a′
1
a′
2
a′
3
...
a′
n−1
Figure 19.
Let us do geometrically A →B and A ⇒B. All we need are some markers in
the ﬁgures representing these two implications, to distinguish one from another.
See Figures 17, 18
Consider now (S, R) of Figure 19 and consider the rule r of Figure 18. Take the
theory T = {a1, . . . , an} ∪{bn}. What can it prove using r?
The answer is that it can prove b0 and all bn−1, . . . , b1 along the way.
The
deduction is essentially aj and bj = [aj ⇒(aj−1 ⇒. . . (a1 ⇒b0) . . .)] yields for
bj−1 = [aj−1 ⇒. . . ⇒(a1 ⇒b0)].
We can turn this ordering into a logic if we give the functions ht, for any
t of Figure 19.
Try hei =
1
2.
hai = arbitrary ha′
i = hai.
hb0 = arbitrary.
hbj = min(1, 1 −a′
j + bj−1) for j ≥1.
We need to show the rule is sound in this semantics, but in this case it is clear
because the rules are versions of modus ponens and the function hx are from
 Lukasiewicz many valued logic.
20
SEMANTICS
We cannot address the problem of what is a logical system without saying some-
thing about our view of semantics. The traditional view, for classical, intuitionis-
tic, or modal logic is to have some notion of a class of models and of an evaluation
procedure of a formula in a model. Thus we may have a set K of models and

122
Dov M. Gabbay
a notion of validity in m ∈K of a formula A of the logic. We use the notation
m ⊨A. Given no details on the internal structure of m and on how m ⊨A is
evaluated, all we can say about the model is that m is a {0, 1} function on wﬀs.
Completeness of K for |∼means that the following holds:
A |∼B iﬀfor all m ∈K (if m ⊨A then m ⊨B).
We would like to present a diﬀerent view of semantics.
We would like to remain totally within the world of logical systems (in our
sense, i.e. LDS with mechanisms) and to the extent that semantics is needed, we
bring it into the syntax. This can obviously and transparently be done in modal
logic where the labels denote possible worlds and the proof rules closely reﬂect
semantical evaluation rules. This in fact can also be done in general. So what
then is the basic notion involved in a purely syntactical set up? What replaces
the notions of a ‘model’, ‘evaluation’, and completeness? We give the following
deﬁnition.
DEFINITION 72 (Syntactical semantics). Let |∼be a consequence relation and
let K be a class of consequence relations, not necessarily of the same language. For
each |∼∗∈K, let k|∼∗be an interpretation of |∼into |∼∗. This involves mapping
of the language of |∼into the language of |∼∗and the following homomorphic
commitment:
A |∼B implies A∗|∼∗B∗(where A∗is K|∼∗(A) and resp. B∗).
We say |∼is complete for (K, k) iﬀwe have
A |∼B iﬀfor all |∼∗∈K, A∗|∼∗B∗.
EXAMPLE 73. The following can be considered as semantical interpretations in
our sense:
1. The Solovay–Boolos interpretation of modal logic G (with L¨ob’s axiom) in
Arithmetic, with □mean sing ‘provable’.
2. The interpretation of intuitionistic propositional logic into various sequences
of intermediate logic whose intersection is intuitionistic logic (e.g. the Ja´skowski
sequence).
3. The interpretation of modal logic into classical logic.
REMARK 74. We gave a deﬁnition of interpretation for consequence relations
|∼. Of course, there are always trivial interpretations which ‘technically’ qualify
as semantics.
This is not intended.
Further note that in the general case we
have a general LDS proof system with algorithmic proof systems S|∼and various
mechanisms.
These should also be interpreted.
Each algorithmic move in S|∼
should be interpreted as a move package in S|∼∗, and similarly for mechanisms.

What is a Logical System?
123
It is possible to justify and motivate our syntactical notion of semantics from
the more traditional one. Let us take as our starting point the notion of Scott-
semantics described in [Gabbay, 1976].
DEFINITION 75. Let L be a propositional language, for example the modal lan-
guage with □or intuitionistic language with →.
1. A model for the language is a function s assigning a value in {0, 1} to each
wﬀof the language.
2. A semantics S is a class of models.
3. Let ∆be a set of wﬀs and A a wﬀ. We say ∆⊨S A iﬀfor all s ∈S if
s(B) = 1 for all B ∈∆then s(A) = 1.
The above deﬁnition relies on the intuition that no matter what our basic con-
cepts of a ‘model’ or interpretation is, sooner or later we have to say whether a
formula A ‘holds’ in it or does not ‘hold’ in it. Thus the technical ‘essence’ of a
model is a {0, 1} function s (we ignore the possibility of no value).
It can be shown that this notion of semantics can characterise any monotonic
(syntactical) consequence relation i.e. any relation |∼between sets ∆(including
∆= ∅) of wﬀs and wﬀs A satisfying reﬂexivity, monotonicity and cut. Thus for
any |∼there exists an S such that |∼equals ⊨S.
The semantics S can be given further structure, depending on the connectives
of L. The simplest is through the binary relation ≤, deﬁned as follows:
• t ≤s iﬀ(deﬁnition) for all wﬀs A, t(A) ≤s(A).
Other relations can be deﬁned on S. For example, if the original language is modal
logic we can deﬁne:
• tRs iﬀfor all □A of L if t(□A) = 1 then s(A) = 1.
One can then postulate connections between values such as:
• t(□A) = 1 iﬀ∀s[tRs ⇒s(A) = 1]
or for a language with →:
• t(A →B) = 1 iﬀ∀s(t ≤s and s(A) = 1 imply s(B) = 1).
In some logics and their semantics the above may hold.
For example, the
respective conditions above hold of rthe modal logic K and for intuitionistic logic.
For other logics, further reﬁnements are needed.
The nature of what is happening here can best be explained through a translate
into classical logic. The language L can be considered as a Herbrand universe of
terms (i.e. the free algebra based on the atomic propositions and the connectives
acting as function symbols), and the models considered as another sort of terms,
(i.e. the names of the models can be terms). The ‘predicate’ t(A) = 1 can be
considered as a two sorted predicate H¯old(t, A). Thus the reductions above become

124
Dov M. Gabbay
• Hold(t, □A) iﬀ∀s(tRs ⇒Hold(s, A)),
where tRs is ∀B(Hold(t, □B) ⇒Hold(sB)).
This condition reduces to
• ∀s[∀X(Hold(t, □X) ⇒Hold(s, X)) ⇒Hold(s, A)] ⇒Hold(t, □A)
This is an internal reduction on Hold.
In general we want to deﬁne Hold(t, ♯(A1, . . . , An)) in terms of some rela-
tions Ri(x1, . . . , xni) on sort t (ﬁrst coordinate of Hold), and the predicates
Hold(x, Aj) for subformulas of ♯(A1, . . . , An).
Ri(t1, . . . , tni) in turn, are ex-
pected to be deﬁned using Hold(ti, Xj) for some formulas Xj.
Thus in predicate logic we have formulas ϕi and Ψ♯such that:
• Hold(t, ♯(Ai, . . . , An) iﬀΨ♯(t, Ri, Hold(xi, Aj))
• Ri(t1, . . . , tni) iﬀ(deﬁnition ϕi(t1, . . . , tniHold(tj, Xk)).
Together they imply a possible closure condition on the semantics.
• Hold(t, ♯(A1, . . . , Sn) iﬀΨ♯(t, ϕi(. . . , Hold(tj, Xk), Hold(xi, Ak))
which may or may not hold.
REMARK 76 (Representation of algebras). The above considerations can be viewed
as a special case of a general set-represenation problem for algebras. Let A be an
algebra with some function symbols fi satisfying some aims. Take for example
the language of lattices A = (A, ⊓, ⊔). We ask the following question: can A be
represented as an algebra of sets? In other words, is there a set S and a mapping
h(a) ⊆S, for a ∈A and a monadic ﬁrst-order language L1 on S involving possibly
some relation symbols R1, . . . ,k on S such that for all s ∈S and function symbol
f of the algebra we have the following inductive reduction, for all x1, . . . , xn ∈A
s(hf(x1, . . . , xn) iﬀ⊨Ψf(s, h(x1), . . . , h(xn))
where Ψf is a non-monadic wﬀof L1 involving 1, . . . , rk and the subsets h(xj).
If the relations R(t1, . . . , tm) on S can be deﬁned using h by some formula ϕR
of the algebra (involving the classical connectives and equality and the monadic
predicates on the algebra Ti(x) meaning ti ∈h(x) then
⊨r(t1, . . . , tm) iﬀA ⊨ϕR(T1, . . . , Tm, R).
REMARK 77 (Dependent semantics). The above considerations are not the most
general and so not reﬂect all that might happen. The considerations explain nicely
semantics like that of modal K but we need reﬁnements.
Consider the logic K1 obtained by collecting all theorems of modal logic K
together with the schema □A →A and the rule of modus ponens. Necessitation
is dropped, so although K1 ⊢□A →A, we can still have K1 ̸⊢□(□A →A).
This logic is complete for the class for all Kripke structures of the form m =
(Sm, Rm, am, hm), where amRam holds. Completeness means

What is a Logical System?
125
1. K1 ⊢A iﬀfor every m as above am ⊨A
Let am be the function satisfying
2. am(A) = 1 iﬀam ⊨A
and let
3. S0 = {am|m as above}.
Then we have a semantics S0 ⊆S (of the language L of modal logic) where
Am(□A) cannot be reduced to values of s(A) for s ∈S0, but can be reduced to
values s(A), for s ∈S, This is so because when we evaluate am ⊨A, we evaluate
at points b ∈Sm such that amRmb and the Kripke structure (Sm, Rmb, hm) is a
K structure, but not necessarily a K1 structure, as bRb need not hold. Let bm be
the function deﬁned by
4. bm(A) = 1 iﬀb ⊨A in m. We get
5. am(□A) = 1 iﬀfor all s ∈{bm | amRbm}, we have s(A) = 1.
Let ϕ(a, b) mean as follows:
6. ϕ(a, b) iﬀ(deﬁnition) for some m, a = am and b = bm and amRbm. Then
we have that K1 is characterised by a designated subset S0 of S and the
truth deﬁnition:
7. s(□A = 1) iﬀfor all s′ϕ(′s, s′) and sRs′ imply s′(A) = 1.
8. A ⊨B iﬀor all s ∈S0, s(A) = 1 implies S(B) = 1.
We are now ready to say what it means to give technical semantics to a conse-
quence relation |∼.
DEFINITION 78 (What is semantics for |∼). Let |∼be a consequence relation
(reﬂexive and transitive) in a language with connectives. Then a semantics for |∼
is any set theoretic representation (in the sense of Remark 77) of the free term
algebra based on |∼.
The previous deﬁnition does not take account of Remark 77.
If we want a
better concept of what is semantics, we need to talk about ﬁbred semantics and
label dependent connectives. These topics are addressed in [Gabbay, 1996].
21
CONCLUDING DISCUSSION 2014
We have, incrementally, gone through several notions of ‘what is a logical system’
and ended up with a concept of a logic that is very far from the traditional concept.
In artiﬁcial intelligence circles, what we call a ‘logic’ is perceived as an ‘agent’
or ‘intelligence agent’. This is no accident. Whereas traditional logical systems

126
Dov M. Gabbay
(classical logic, intuitionistic logic, linear logic) model mathematical reasoning and
mathematical proof, our new concept of logic attempts to model, and stay tuned
to, human practical reasoning. What we tried to do is to observe what features and
mechanisms are at play in human practical reasoning, and proceed to formalise
them. The systems emerging from this formalisation we accept as the new ‘logics’.
It is therefore no surprise that in AI circles such systems are perceived as intelligent
agents. However, compared with AI, our motives are diﬀerent. We are looking for
general logical principles of human reasoning and not necessarily seeking to build
practical applied systems.
There is one more point to make before we can close this chapter. The above
‘logics’ manipulate formulas, algebraic terms and in general syntactical symbols.
We have maintained already in 1988 [Gabbay and Reyle, 1994] that deduction is a
form of stylised movement, which can be carried out erectly on natural objects from
an application area. Thus ‘logic’ can be done not only on syntactical formulas,
but on any set of structured objects, naturally residing in some application area.
To reason about gardening, for example, we can either represent the area in some
language and manipulate the syntax in some logic, or we can directly nameplate
and move the plants themselves and ‘show’ the conclusion. The style of movement
is the ‘logic’. This concept of logic as movement is clearly apparent in automated
reasoning. Diﬀerent kinds of ‘shuﬄing’ licensed by a theorem prover can lead to
diﬀerent ‘logics’, because then diﬀerent sets of theorems become provable. Our
insight twas that similar movements can be applied directly on the objects of
the application areas, and therefore reasoning can be achieved directly in the
application area without formalisation. This philosophy has been carried out on
Discourse Representation Structures in [Gabbay and Reyle, 1994].
Our approach is compatible with the more mixed approach in the contribution
by Barwise and Hammer in [Gabbay, 1994a].
We have summarised, with many examples, the various means and mechanisms,
syntactical semantical and algorithmic for deﬁning a logic. In any application case
study in need for logical formalisation, we can construct a required suitable logic
by drawing on and interleaving these available means and mechanisms and deﬁne
a successful suitable logic for the application.
The following further points (I am indebted to Johan van Benthem for his
valuable comments) need to be further addressed:
• Since our notion of a logical system has changed and evolved, the traditional
notions associated with logical systems need also to change. Notions like
Interpolation, the notion of Cut, the notions of Expansion, Revision etc etc,
all have to be redeﬁned and studied for the new logical systems. This has
been studied in [Gabbay, 1996; Gabbay, 1999]. In particular we need to study
the question of when are two logical systems equivalent, what are appropriate
notions of translation, etc? Indeed, one could wonder generally what sort of
Abstract Model Theory (or Abstract Proof Theory) might make sense here.
For instance, could there be generalized Lindstr¨om theorems capturing our
richer notions of ﬁrst-order logic as a system?

What is a Logical System?
127
• Architecture: How do diﬀerent logical systems work together, combine? We
have studied this notion of ﬁbring in [Gabbay, 1999] but we feel further
systematic study is needed.
• Representation: Can one logical system have diﬀerent natural grain levels?
Can/should there be an account of ‘zooming in’ and ‘zooming out’ between
such levels? This also seems connected to the following: we emphasized the
role of ‘presentation’ in a richer view of a logical system. Would not just
the proof engine but also the choice of the system language itself (ﬁner or
coarser) be an essential parameter of this? Our presentation in this paper is
implicitly saying yes.
• Our notion of a logical system seems largely (not exclusively) inference-
driven. This might be seen as a bias by semantics-oriented people who want a
semantics to provide a deeper explanation of why particular packages of proof
rules (among the multitude of combinatorial possibilities) make particular
sense. It is true that using labels,(in Labelled Deductive Systems), we can
bring semantics into the syntax as we have seen, [Gabbay, 1996]. But should
we blur the distinctions or maybe continue to maintain it?
• As we enrich our notion of a system (e.g., with ‘approximations’ for bounded
agents) we hit an issue, (also confronted by Johan van Benthem), which is
whether we are really describing “agents using systems” instead of the sys-
tems themselves? I said in print on several past occasions that the ultimate
logical system is what is in the head of a living human agent. So is then,
according to my view, a logical system the same as what other people call
(a formalisation of) an agent? I now think that it is not, in which case what
is an agent?
I am addressing this problem now together with Michael Luck. It will take
some time to ﬁgure out.
Note the daring thought that according to the
equational approach to logic, logics can be characterised by equations and
so since an agent can be characterised by logic, then an agent can be char-
acterised by a system or equations, just like a particle in mechanics. We are
going to check this thought. By the way, Johan van Benthem has also come
up against this question, of the connection between logic and agents, in the
context of complexity of logical systems. He is exploring the idea that we
should not be thinking so much of ﬁnding new decidable fragments of logi-
cal systems, but rather of simple agents using only tractable parts of larger
systems. In other words: would not it make sense to separate system and
agent? This is connected with our Section 5 and see reference [D’Agostino
and Gabbay, 2014]. See also [Gabbay and Woods, 2008], where we show how
an agent with very limited resources would naturally come up with Donald
Nute’s system of defeasible reasoning.
• Computational issues. Our motivation for enriching the notion of a logical
system comes from computation, but we say little about the computational

128
Dov M. Gabbay
aspects of richer logical systems in our sense, such as the computational
complexity of (old, and new) key tasks associated with them. This needs to
be studied. There is, however, an important point about our approach which
the reader must keep in mind. We develop our notion bottom up. For any
application area , the logic consumer/ practitioner can add incrementally to
his logic the additional features he needs. So complexity issues become more
understandable and manageable.
• One main piece of evidence for our view is the current variety of logical
systems in practice. There are two reasons for this. The ﬁrst is that diﬀer-
ent systems arise from diﬀerent applications and the fact that practitioners
/consumers of logic just build what they need locally. The second reason
comes from philosophy, where diﬀerent philosophical motivations and views
give rise to diﬀerent systems. My view is that we should not seek the one
true all encompassing logical system but that part of the notion of logic is
how to combine diﬀerent systems and have them work together. So any new
logical system should have as part of its nature procedures for working with
other systems. I note that Johan van Benthem for example, while in great
sympathy with my view, does raise the issue of whether some more funda-
mental level of motivation exists as well, perhaps for mathematical system
reasons?
• Our emphasis is ‘what is a logical system’. Actually, one might just as well
say that our topic in this paper is really a more ambitious one: “what is
logic”? I just do not want to go this way at this stage , before we ﬁgure out
“what is an agent”, and get a better understanding of the philosophical and
psychological literature on what is logic.
ACKNOWLEDGEMENTS
I am grateful to Arnon Avron, Johan van Benthem, Marcello D’Agostino, J¨org
Siekmann and Leon van der Torre for valuable comments on this paper.
BIBLIOGRAPHY
[Abraham et al., 2011] M. Abraham, I. Belfer, D. Gabbay, and U. Schild. Delegation, count as
and security in Talmudic logic, a preliminary study. In Logic without Frontiers: Festschrift
for Walter Alexandre Carnielli on the occasion of his 60th Birthday. Jean-Yves B´eziau and
Marcelo Esteban Coniglio, eds., pp. 73–96. Volume 17 of Tribute Series, College Publications.
London, 2011.
[Alchourr´on et al., 1985] C. E. Alchourr´on, P. G¨ardenfors, and D. Makinson. On the logic of theory
change: partial meet contraction and revision functions. Journal of Symbolic Logic, 50, 510–
530, 1985.
[Allwein and Barwise, 1996] . Allwein and J. Barwise. Logical Reasoning with Diagrams, Studies
in Logic and Computation, 1996.
[Arieli et al., 2011] O. Arieli, A. Avron and A. Zamansky. Ideal paraconsistent logics. Studia Logca,
99(1–3), 31–60, 2011.

What is a Logical System?
129
[Avron, 2014] A. Avron. Paraconsistemcy, paracompleteness, Gentzen systems, and trivalent se-
mantics. Jouranl of Applied Non-classical Logic, DOI: 10.1080/11663081.2014.911515.
[Avron and Lev, 2005] A. Avon and I. Lev. Non-deterministic multi-valued structures. Journal of
Logic and Computation, 15:241–261, 2005.
[Avron and Zamansky, 2011] A. Avron and A. Zamansky. Non deterministic semantics for logical
systems. A survey. In Handbook of Philosophical Logic, D. Gabbay and F. Guenthner, eds.
Vol 16, pp. 227–304, Kluwer, 2011.
[Barringer et al., 2009] H. Barringer, D. M. Gabbay, and D. Rydeheard. Reactive grammars. In
N. Dershowitz and E. Nissan, editors, Language, Culture, Computation, LNCS. Springer,
2009. In honour of Yakov Choueka, to appear.
[Barringer et al., 2010] H. Barringer, K. Havelund and D. Rydeheard. Rule systems for run-time
monitoring: from Eagle to RuleR (extended version). Journal of Logic and Computation,
Oxford University Press, 20(3): 675 – 706, 2010.
[Barringer et al., 2012] H. Barringer, D. Gabbay and J. Woods. Temporal, Numerical and Met-
alevel Dynamics in Argumentation Networks. Argumentation and Computation, 3(2-3), 143–
202, 2012.
[Barwise and Seligman, 2008] J. Barwise and J. Seligman. Information Flow: The Logic of Dis-
tributed Systems. Cambridge Tracts in Theoretical Computer Science, 2008.
[Barwise and Perry, 1983] J. Barwise and J. Perry. Situations and Attitudes. A Bradford
Book/MIT Press. 1983.
[Batens et al., 1999] D. Batens, K. De Clercq and N. Kurtonina. Embedding and interpolation for
some paralogics. The propositional cse. reports on Mathematical Logic, 33:29–44, 1999.
[van Benthem, 2005] J. van Benthem. An essay on sabotage and obstruction. In D. Hutter and
W. Stephan, editors, Mechanizing Mathematical Reasoning, volume 2605 of Lecture Notes in
Computer Science, pages 268–276. Springer, 2005.
[van Benthem et al., 2009] J. van Benthem, G. Heinzmann, M. Rebuschi, eds. The Age of Alter-
native Logics: Assessing Philosophy of Logic and Mathematics Today (Logic, Epistemology,
and the Unity of Science), Springer, 2009.
[van Benthem, 2014] J. van Benthem. Logic in Games, MIT Press, 2014.
[van Benthem, et al., 2011] J. van Benthem, A. Gupta and R. Parikh, eds. Proof computation and
Agency: Logic at the Crossroads, SPringer, 2011.
[van Benthem, 2011a] J. van Benthem. Logical Dynamics of Information and Interaction. Cam-
bridge University Press, 2011.
[Beziau, 1999] J. Y. Beziau. Classical negation can be expressed by one of its halves. Logic Journal
of the IGPL, 7:145–151, 1999.
[Boole, 1847] G. Boole. The Mathematical Analysis of Logic, Cambridge and London, 1847.
[Brown, 2012] F. Brown. Boolean Reasoning: The Logic of Boolean Equations. Dover, 2012.
[Cadoli and Schaerf, 1992] M. Cadoli and M. Schaerf. Approximate reasoning and non-omniscient
agents. In TARK ’92: Proceedings of the 4th conference on Theoretical aspects of reasoning
about knowledge, pages 169–183, San Francisco, CA, USA, 1992. Morgan Kaufmann Publish-
ers Inc.
[Caminada and Amgoud, 2007] Martin Caminada and Leila Amgoud. On the evaluation of argu-
mentation formalisms. Artiﬁcial Intelligence, 171(5-6):286–310, 2007.
[Console et al., 1991] L. Console, D. T. Dupre, and P. Torasso. On the relationship between de-
duction and abduction. Journal of Logic and Computation, 1, 661–690, 1991.
[Couturat, 1914] L. Couturat. The Algebra of Logic. Open Court, 1914.
[Crawford and Etherington, 1998] J.M. Crawford and D.W. Etherington. A non-deterministic se-
mantics fortractable inference. In AAAI/IAAI, pages 286–291, 1998.
[Crochemore and Gabbay, 2011] M. Crochemore and D. M. Gabbay. Reactive Automata. Infor-
mation and Computation, 209(4), 692–704. Published online: DOI: 10.1016/j.ic.2011.01.002
[Cross, 1999] Sir Rupert Cross. On Evidence. Butterworth, 1999.
[D’Agostino and Floridi, 2009] M. D’Agostino and L. Floridi. The enduring scandal of deduction:
Is propositional logic really uninformative? Synthese 167:271-315 (2009).
[D’Agostino et al., 2013] M. D’Agostino, M. Finger and D. M. Gabbay. Semantics and proof-
theory of depth-bounded Boolean logics. Theoretical Computer Science 480:43-68 (2013).
[D’Agostino and Gabbay, 2014] M. D’Agostino and D. M. Gabbay. Feasible Deduction for Realis-
tic Agents, College Publications, 2014.

130
Dov M. Gabbay
[Dalal, 1996] M. Dalal. Anytime families of tractable propositional reasoners. In Proceedings of
the Fourth International Symposium on AI and Mathematics(AI/MATH-96), pages 42–45,
1996.
[Dalal, 1998] M. Dalal. Anytime families of tractable propositional reasoners. Annals of Mathe-
matics and Artiﬁcial Intelligence, 22: 297–318, 1998.
[Dennis, 1992] H. Dennis. Law of Evidence, Sweet and Maxwell, 1992.
[Finger, 2004] M. Finger. Polynomial approximations of full propositional logic via limitedbiva-
lence. In 9th European Conference on Logics in Articial Intelligence(JELIA 2004), volume
3229 of Lecture Notes in Articial Intelligence, pages 526–538. Springer, 2004.
[Finger, 2004b] M. Finger. Towards polynomial approximations of full propositional logic.In
A.L.C. Bazzan and S. Labidi, editors, XVII Brazilian Symposium on Ar-ticial Intel ligence
(SBIA 2004), volume 3171 of Lecture Notes in Artiﬁcial Intellingence, pages 11–20. Springer,
2004.
[Finger and Gabbay, 2006] M. Finger and D.M. Gabbay. Cut and pay. Journal of Logic, Language
and Information, 15(3):195–218, 2006.
[Finger and Wasserman, 2004] M. Finger and R. Wassermann. Approximate and limited reason-
ing: Semantics, proof theory, expressivity and control. Journal of Logic and Computation,
14(2):179–204, 2004.
[Finger and Wassermann, 2006] M. Finger and R. Wassermann. The universe of propositional ap-
proximations. Theoretical Computer Science, 355(2):153–66, 2006.
[Gabbay, 1969] D. Gabbay. Semantic proof of the Craig interpolation theorem for intuitionistic
logic. In Logic Colloquium ’69, pp . 391–410, North-Holland, 1969.
[Gabbay, 1976] D. Gabbay. Semantical Investigations in Modal and Temporal Logics. D. Reidel,
1976.
[Gabbay, 1985] D. Gabbay. Theoretical foundations for non monotonic reasoning in expert system.
In K. Apt, editor, Logics and Models of Concurrent Systems, pp. 439–459, Springer-Verlag,
Berlin, 1985.
[Gabbay, 1986] D. Gabbay. Investigations in Heyting Intuitionistic Logic. D. Reidel, 1986.
[Gabbay, 1992] D. Gabbay. Theory of algorithmic proof. In Handbook of Logic in Theoretical
Computer Science, Volume 1, S. Abramsky, D. Gabbay and T. Maibaum, eds., pp 307–408
Oxford University Press, 1992.
[Gabbay, 1993] D. Gabbay. Classical vs. nonclassical logic. In D. M. Gabbay, C. J. Hogger and J.
A. Robinson, eds., Handbook of Logic in AI and Logic Programming, Volume 1, pp. 349–489.
Oxford University Press, 1993.
[Gabbay, 1993a] D. Gabbay. General theory of structured consequence relations. In Substructural
Logics, K. Doˇsen and P. Schr¨eder-Heister, eds., pp. 109–151. Studies in Logic and Computa-
tion, Oxford University Press, 1993.
[Gabbay, 1994a] D. M. Gabbay. What is a Logical System?, Oxford University Press, 1994.
[Gabbay, 1996] D. M. Gabbay. Labelled Deductive Systems, Vol. 1. Oxford University Press, 1996.
[Gabbay, 1998] D. M. Gabbay. Elementary Logic. Prentice Hall, 1998.
[Gabbay, 1999] D. M. Gabbay. Fibring Logics, OUP, 1999.
[Gabbay, 2009] D. Gabbay. Semantics for higher level attacks in extended argumentation frames
Part 1 : Overview. Studia Logica, 93: 355–379, 2009.
[Gabbay, 2012] D. Gabbay. An Equational Approach to Argumentation Networks, Argument and
Computation, 2012, vol 3 issues (2-3), pp 87-142
[Gabbay, 2012b] D. Gabbay. Reactive Beth tableaux for modal logic. AMAI vol 66, issue 1–4,
55–79, 2012.
[Gabbay, 2012a] D. M. Gabbay. Equational approach to default logic. In preparation, 90pp, 2012.
[Gabbay, 2012b] D. Gabbay. Bipolar argumentation frames and contrary to duty obligations, a
position paper. In Proceedings of CLIMA 2012, M. Fisher et al., eds. pp. 1–24. LNAI 7486,
Springer, 2012.
[Gabbay, 2012c] D. Gabbay. Temporal deontic logic for the generalised Chisholm set of contrary
to duty obligations. In T. Agotnes, J. Broersen, and D. Elgesem, eds., DEON 2012, LNAI
7393, pp. 91–107. Springer, Heidelberg, 2012.
[Gabbay, 2013] D. Gabbay. Reactive Kripke Semantics, Theory and Applications. Springer 2013,
450pp.
[Gabbay, 2013a] D. Gabbay. Meta-Logical Investigations in Argumentation Networks. Research
Monograph College publications 2013, 770 pp

What is a Logical System?
131
[Gabbay et al., 1993] D. Gabbay, A. Martelli, L. Giordano and N. Olivetti. Conditional logic pro-
gramming. technical report, University of Turin, 1993. Proceedings of ICLP ’94, MIT Press.
[Gabbay and Hunter, 1991] D. M. Gabbay and A. Hunter. Making Inconsistency Respectable, part
I. In Proceeding of Fundamental of Artiﬁcial Intelligence Resarch (Fair ’91), Ph Jorrand and
J. Kelement, eds, pp. 19–32. Vol 535 of LNAI, Springer Verlag, 1991.
[Gabbay and Hunter, 1993] D. M. Gabbay and A. Hunter. Making Inconsistency Respectable, part
II. In Proceeding of Euro Conference on Symbolic and Quantitive Approaches to Reasoning
and Uncertainty, M. Clarke, R. Kruse andS. Moral eds., pp. 129–136. Vol 747 of LNC, Springer
Verlag, 1993.
[Gabbay and Marcelino, 2009] D. Gabbay and S. Marcelino. Modal logics of reactive frames. Stu-
dia Logica, 93, 403–444, 2009.
[Gabbay and Olivetti, 2000] D. Gabbay and N. Olivetti. Goal Directed Algorithmic Proof Theory,
Kluwer Academic Publishers, 2000. 266pp.
[Gabbay and Reyle, 1994] D. Gabbay and U. Reyle. Direct deductive computation on discourse
representation structures. Technical report, University of Stuttgart, 1988. Linguisitics and
Philosophy, 17, 345–390, 1994.
[Gabbay and Rodrigues, 2012] D. Gabbay, O. Rodrigues. Voting and Fuzzy Argumentation Net-
works, submitted to Journal of Approximate Reasoning. Short version to appear in Proceed-
ings of CLIMA 2012, Springer. New revised version now entitled: Equilibrium States on
Numerical Argumentation Networks.
[Gabbay and Woods, 2001] D.M. Gabbay and J. Woods. The new logic. Logic Journal of the
IGPL,9(2):141–174, 2001.
[Gabbay and Woods, 2003] D. M. Gabbay and J. Woods. The law of evidence and labelled de-
ductive systems. Phi-News, 4, 5–46, October 2003. http//phinews.ruc.dk/phinews4.pdf. Also
Chapter 15 in D. M. Gabbay et al., eds Approaches to legal Rationality, Logic, Episemology
and the Unity of Science 20, pp. 295–331, Springer, 2010.
[Gabbay and Woods, 2008] D. Gabbay and J. Woods. Resource Origins of Non-Monotonicity, in
Studia Logica, Vol 88, 2008, pp 85-112.
[Hunter, 2010] A. Hunter. Base Logics in Argumentation. In Proceedings of COMMA 2010, pp
275–286.
[Kowalski, 1979] R. A. Kowalski. Logic for Problem Solving. North-Holland, 1979.
[Kraus et al., 1990] S. Kraus, D. Lehmann, and M. Magidor. Nonmonotonic reasoning, preferential
models and cumulative logics. Artiﬁcial Intelligence, 44, 167–207, 1990.
[Lehman and Magidor, 1992] D. Lehman and M. Magidor. What does a conditional knowledge
base entail? Artiﬁcial Intelligence, 55, 1–60, 1992.
[Makinson, 1989] D. Makinson. General theory of cumulative inference. In M. Reinfrank et al.,
eds., Non-monotonic Reasoning. Volume 346 of Lecture Notes in Artiﬁcial Intelligence, pp.
1–18, Springer-Verlag, Berlin, 1989.
[Makinson and van der Torre, 2000] D. Makinson and L. van der Torre. Input/output logics. Jour-
nal of Philosophial Logic, 29(4): 383–408, 2000.
[Makinson and van der Torre, 2001] D. Makinson and L. van der Torre. Constraints for in-
put/output logics. Jouranl of Philosophical Logic, 3-(2): 155–185, 2001.
[Makinson and van der Torre, 2001b] D. Makinson and L. van der Torre. What is input/output
logic? ESSLLI, 2001.
[Metcalfe et al., 2008] G. Metcalfe, N. Olivetti and D. Gabbay. Proof theory for Fuzzy Logics.
(Monograph)Springer 2008.
[Nute and Cross, 2002] D. Nute and C. Cross. Conditional Logic. In Handbook of Philosophical
Logic, D. Gabbay and F. Guenthner, eds., pp. 1–98, Volume 4, 2nd edition. Kluwer, 2002.
[Parent et al., 2014] X. Parent, D. Gabbay and L. van der Torre. Intuitionistic basis for in-
put/output logic. In David Makinson on Classical Methods to Non-Classical Problems, S.
O. Hansson, ed., pp. 263–286. Springer 2014.
[Rohde, 2004] P. Rohde. Moving in a crumbling network: the balanced case. In J. Marcinkowski
and A. tarlecki, eds. CSL 2004, LNCS, pp. 1–25, Springer.
[Schr¨oder, 1890–1904] E. Schr¨oder. Vorlesungen ¨uber die Algebra die Logik, 3 vols. B. G. Tuebner,
Leipzig, 1890–1914. Reprints, Chelsea, 1996, Thoemmes Press, 2000.
[Scott, 1974] D. Scott. Completeness and axiomatizability in many-valued logics. In Proceedings
of the Tarski Symposium, pp. 411–436, American Mathematical Society, Providence, Rhode
Island, 1974.
[Shapiro, 2014] S. Shapiro. Varieties of Logic, OUP, 2014.

132
Dov M. Gabbay
[Sheeran and Stalmarck, 2000] M. Sheeran and G. Stalmarck. A tutorial on Stalmarck’s proof
procedure for propositional logic. Formal Methods in System Design, 16:23–58, 2000.
[Tarski, 1936] A. Tarski. On the concept of logical consequence. In Logic, Semantics, Metamath-
ematics. Oxford University Press, 1936.
[Varzi, 1999] A. C. Varzi, ed. The Nature of Logic. European Review of Philosophy, Volume 4,
CSLI Publications, 1999.
[Wikipedia, ] http://en.wikipedia.org/wiki/T-norm
[W´ojcicki, 1988] R. W´ojcicki. Theory of Logical Calculi. Reidel, Doredrecht, 1988.
[W´ojcicki, 1988] R. W´ojcicki. An axiomatic treatment of non-monotonic arguments. Bulletin of
the Section of Logic, 17(2): 56–61, 1988.

Part III 
Automated Reasoning 


HISTORY OF INTERACTIVE THEOREM
PROVING
John Harrison, Josef Urban and Freek Wiedijk
Reader: Lawrence C. Paulson
1
INTRODUCTION
By interactive theorem proving, we mean some arrangement where the machine
and a human user work together interactively to produce a formal proof. There is
a wide spectrum of possibilities. At one extreme, the computer may act merely
as a checker on a detailed formal proof produced by a human; at the other the
prover may be highly automated and powerful, while nevertheless being subject
to some degree of human guidance. In view of the practical limitations of pure
automation, it seems today that, whether one likes it or not, interactive proof is
likely to be the only way to formalize most non-trivial theorems in mathematics
or computer system correctness.
Almost all the earliest work on computer-assisted proof in the 1950s [Davis,
1957; Gilmore, 1960; Davis and Putnam, 1960; Wang, 1960; Prawitz et al., 1960]
and 1960s [Robinson, 1965; Maslov, 1964; Loveland, 1968] was devoted to truly
automated theorem proving, in the sense that the machine was supposed to prove
assertions fully automatically. It is true that there was still a considerable diver-
sity of methods, with some researchers pursuing AI-style approaches [Newell and
Simon, 1956; Gelerntner, 1959; Bledsoe, 1984] rather than the dominant theme of
automated proof search, and that the proof search programs were often highly tun-
able by setting a complicated array of parameters. As described by Dick [2011],
the designers of automated systems would often study the details of runs and
tune the systems accordingly, leading to a continuous process of improvement and
understanding that could in a very general sense be considered interactive. Nev-
ertheless, this is not quite what we understand by interactive theorem proving
today.
Serious interest in a more interactive arrangement where the human actively
guides the proof started somewhat later. On the face of it, this is surprising, as full
automation seems a much more diﬃcult problem than supporting human-guided
proof. But in an age when excitement about the potential of artiﬁcial intelligence
was widespread, mere proof-checking might have seemed dull. In any case it’s not
so clear that it is really so much easier as a research agenda, especially in the
Handbook of the History of Logic. Volume 9: Computational Logic. 
Volume editor: Jörg Siekmann 
Series editors: Dov M. Gabbay and John Woods 
Copyright © 2014 Elsevier BV. All rights reserved.

136
John Harrison, Josef Urban and Freek Wiedijk
context of the technology of the time. In order to guide a machine proof, there
needs to be a language for the user to communicate that proof to the machine, and
designing an eﬀective and convenient language is non-trivial, still a topic of active
research to this day.
Moreover, early computers were typically batch-oriented,
often with very limited facilities for interaction.
In the worst case one might
submit a job to be executed overnight on a mainframe, only to ﬁnd the next day
that it failed because of a trivial syntactic error.
The increasing availability of interactive time-sharing computer operating sys-
tems in the 1960s, and later the rise of minicomputers and personal workstations
was surely a valuable enabler for the development of interactive theorem prov-
ing.
However, we use the phrase interactive theorem proving to distinguish it
from purely automated theorem proving, without supposing any particular style of
human-computer interaction. Indeed the inﬂuential proof-checking system Mizar,
described later, maintains to this day a batch-oriented style where proof scripts
are checked in their entirety per run. In any case, perhaps the most powerful
driver of interactive theorem proving was not so much technology, but simply the
recognition that after a ﬂurry of activity in automated proving, with waves of
new ideas like uniﬁcation that greatly increased their power, the capabilities of
purely automated systems were beginning to plateau. Indeed, at least one pio-
neer clearly had automated proving in mind only as a way of ﬁlling in the details
of a human-provided proof outline, not as a way of proving substantial theorems
unaided [Wang, 1960]:
The original aim of the writer was to take mathematical textbooks
such as Landau on the number system, Hardy-Wright on number the-
ory, Hardy on the calculus, Veblen-Young on projective geometry, the
volumes by Bourbaki, as outlines and make the machine formalize all
the proofs (ﬁll in the gaps).
and the idea of proof checking was also emphasized by McCarthy [1961]:
Checking mathematical proofs is potentially one of the most interesting
and useful applications of automatic computers. Computers can check
not only the proofs of new mathematical theorems but also proofs that
complex engineering systems and computer programs meet their spec-
iﬁcations. Proofs to be checked by computer may be briefer and easier
to write than the informal proofs acceptable to mathematicians. This
is because the computer can be asked to do much more work to check
each step than a human is willing to do, and this permits longer and
fewer steps. [. . . ] The combination of proof-checking techniques with
proof-ﬁnding heuristics will permit mathematicians to try out ideas
for proofs that are still quite vague and may speed up mathematical
research.
McCarthy’s emphasis on the potential importance of applications to program
veriﬁcation may well have helped to shift the emphasis away from purely auto-

History of Interactive Theorem Proving
137
Figure 1: Proof-checking project for Morse’s ‘Set Theory’
matic theorem proving programs to interactive arrangements that could be of
more immediate help in such work. A pioneering implementation of an interactive
theorem prover in the modern sense was the Proofchecker program developed
by Paul Abrahams [1963]. While Abrahams hardly succeeded in the ambitious
goal of ‘veriﬁcation of textbook proofs, i.e. proofs resembling those that normally
appear in mathematical textbooks and journals’, he was able to prove a number
of theorems from Principia Mathematica [Whitehead and Russell, 1910]. He also
introduced in embryonic form many ideas that became signiﬁcant later: a kind
of macro facility for derived inference rules, and the integration of calculational
derivations as well as natural deduction rules.
Another interesting early proof
checking eﬀort [Bledsoe and Gilbert, 1967] was inspired by Bledsoe’s interest in
formalizing the already unusually formal proofs in his PhD adviser A.P. Morse’s
‘Set Theory’ [Morse, 1965]; a ﬂyer for a conference devoted to this research agenda
is shown in Figure 1. We shall have more to say about Bledsoe’s inﬂuence on our
ﬁeld later.
Perhaps the earliest sustained research program in interactive theorem prov-
ing was the development of the SAM (Semi-Automated Mathematics) family of

138
John Harrison, Josef Urban and Freek Wiedijk
provers. This evolved over several years starting with SAM I, a relatively simple
prover for natural deduction proofs in propositional logic. Subsequent members
of the family supported more general logical formulas, had increasingly powerful
reasoning systems and made the input-output process ever more convenient and
accessible, with SAM V ﬁrst making use of the then-modern CRT (cathode ray
tube) displays. The provers were applied in a number of ﬁelds, and SAM V was
used in 1966 to construct a proof of a hitherto unproven conjecture in lattice theory
[Bumcrot, 1965], now called ‘SAM’s Lemma’. The description of SAM explicitly
describes interactive theorem proving in the modern sense [Guard et al., 1969]:
Semi-automated mathematics is an approach to theorem-proving which
seeks to combine automatic logic routines with ordinary proof proce-
dures in such a manner that the resulting procedure is both eﬃcient
and subject to human intervention in the form of control and guid-
ance. Because it makes the mathematician an essential factor in the
quest to establish theorems, this approach is a departure from the
usual theorem-proving attempts in which the computer unaided seeks
to establish proofs.
Since the pioneering SAM work, there has been an explosion of activity in the
area of interactive theorem proving, with the development of innumerable diﬀerent
systems; a few of the more signiﬁcant contemporary ones are surveyed by Wiedijk
[2006]. Despite this, it is diﬃcult to ﬁnd a general overview of the ﬁeld, and one of
the goals of this chapter is to present clearly some of the most inﬂuential threads
of work that have led to the systems of today. It should be said at the outset
that we focus on the systems we consider to have been seminal in the introduction
or ﬁrst systematic exploitation of certain key ideas, regardless of those systems’
present-day status. The relative space allocated to particular provers should not
be taken as indicative of any opinions about their present value as systems. After
our survey of these diﬀerent provers, we then present a more thematic discussion
of some of the key ideas that were developed, and the topics that animate research
in the ﬁeld today.
Needless to say, the development of automated theorem provers has continued
apace in parallel. The traditional ideas of ﬁrst-order proof search and equational
reasoning [Knuth and Bendix, 1970] have been developed and reﬁned into pow-
erful tools that have achieved notable successes in some areas [McCune, 1997;
McCune and Padmanabhan, 1996]. The formerly neglected area of propositional
tautology and satisﬁability checking (SAT) underwent a dramatic revival, with
systems in the established Davis-Putnam tradition making great strides in eﬃ-
ciency [Moskewicz et al., 2001; Goldberg and Novikov, 2002; E´en and S¨orensson,
2003], other algorithms being developed [Bryant, 1986; St˚almarck and S¨aﬂund,
1990], and applications to new and sometimes surprising areas appearing. For
veriﬁcation applications in particular, a quantiﬁer-free combination of ﬁrst-order
theories [Nelson and Oppen, 1979; Shostak, 1984] has proven to be especially valu-
able and has led to the current SMT (satisﬁability modulo theories) solvers. Some

History of Interactive Theorem Proving
139
more domain-speciﬁc automated algorithms have proven to be highly eﬀective in
areas like geometry and ideal theory [Wu, 1978; Chou, 1988; Buchberger, 1965],
hypergeometric summation [Petkovˇsek et al., 1996] and the analysis of ﬁnite-state
systems [Clarke and Emerson, 1981; Queille and Sifakis, 1982; Burch et al., 1992;
Seger and Bryant, 1995], the last-mentioned (model checking) being of great value
in many system veriﬁcation applications. Indeed, some researchers reacted to the
limitations of automation not by redirecting their energy away from the area,
but by attempting to combine diﬀerent techniques into more powerful AI-inspired
frameworks like MKRP [Eisinger and Ohlbach, 1986] and Ωmega [Huang et al.,
1994].
Opinions on the relative values of automation and interaction diﬀer greatly. To
those familiar with highly eﬃcient automated approaches, the painstaking use of
interactive provers can seem lamentably clumsy and impractical by comparison.
On the other hand, attacking problems that are barely within reach of automated
methods (typically for reasons of time and space complexity) often requires prodi-
gious runtime and/or heroic eﬀorts of tuning and optimization, time and eﬀort
that might more productively be spent by simple problem reduction using an in-
teractive prover.
Despite important exceptions, the clear intellectual center of
gravity of automated theorem proving has been the USA while for interactive the-
orem proving it has been Europe. It is therefore tempting to ﬁt such preferences
into stereotypical national characteristics, in particular the relative importance at-
tached to eﬃciently automatable industrial processes versus the painstaking labor
of the artisan. Such speculations aside, in recent years, we have seen something of
a rapprochement: automated tools have been equipped with more sophisticated
control languages [de Moura and Passmore, 2013], while interactive provers are in-
corporating many of the ideas behind automated systems or even using the tools
themselves as components — we will later describe some of the methodological
issues that arise from such combinations. Even today, we are still striving towards
the optimal combination of human and machine that the pioneers anticipated 50
years ago.
2
AUTOMATH AND SUCCESSORS
Automath might be the earliest interactive theorem prover that started a tradition
of systems which continues until today. It was the ﬁrst program that used the
Curry-Howard isomorphism for the encoding of proofs. There are actually two
variants of the Curry-Howard approach [Geuvers and Barendsen, 1999], one in
which a formula is represented by a type, and one in which the formulas are
not types, but where with each formula a type of proof objects of that formula
is associated. (The popular slogan ‘formulas as types’ only applies to the ﬁrst
variant, while the better slogan ‘proofs as objects’ applies to both.)
The ﬁrst
approach is used by modern systems like Coq, Agda and NuPRL. The second
approach is used in the LF framework [Harper et al., 1987], and was also the one
used in the Automath systems. The idea of the Curry-Howard isomorphism in

140
John Harrison, Josef Urban and Freek Wiedijk
either style is that the type of ‘proof objects’ associated with a formula is non
empty exactly in the case that that formula is true.
As an example, here is an Automath text that introduces implication and the
two natural deduction rules for this connective (this text appears almost verbatim
on pp. 23–24 of [de Bruijn, 1968b]). The other connectives of ﬁrst order logic are
handled analogously.
* bool
:= PN
: TYPE
* b
:= --- : bool
b
* TRUE
:= PN
: TYPE
b
* c
:= --- : bool
c
* impl
:= PN
: bool
c
* asp1
:= --- : TRUE(b)
asp1 * asp2
:= --- : TRUE(impl)
asp2 * modpon := PN
: TRUE(c)
c
* asp4
:= --- : [x,TRUE(b)]TRUE(c)
asp4 * axiom
:= PN
: TRUE(impl)
This code ﬁrst introduces (axiomatically: PN abbreviates ‘Primitive Notion’) a
type for the formulas of the logic called bool1, and for every such formula b a
type of the ‘proof objects’ of that formula TRUE(b). The --- notation extends a
context with a variable, where contexts are named by the last variable, and are
indicated before the * in each line. Next, it introduces a function impl(b,c) that
represents the implication b ⇒c. Furthermore, it encodes the Modus Ponens rule
b
b ⇒c
c
using the function modpon. If asp1 is a ‘proof object’ of type TRUE(b) and asp2 is a
‘proof object’ of type TRUE(impl(b,c)), then the ‘proof term’ modpon(b,c,asp1,
asp2) denotes a ‘proof object’ of type TRUE(c). This term represents the syntax
of the proof in ﬁrst order logic using higher order abstract syntax. Finally, the rule
b
...
c
b ⇒c
is encoded by the function axiom. If asp4 is a ‘function’ that maps ‘proof objects’
of type TRUE(b) to those of type TRUE(c), then axiom(b,c,asp4) is a ‘proof
object’ of type TRUE(impl(b,c)).
1For a modern type theorist bool will be a strange choice for this name. However, in HOL
the same name is used for the type of formulas (which shows that HOL is a classical system).

History of Interactive Theorem Proving
141
This Automath code corresponds directly to the modern typing judgments:
bool : ∗
TRUE : bool →∗
impl : bool →bool →bool
modpon : Πb : bool. Πc : bool. TRUE b →TRUE (impl b c) →TRUE c
axiom : Πb : bool. Πc : bool. (TRUE b →TRUE c) →TRUE (impl b c)
The way one codes logic in LF style today is still exactly the same as it was in the
sixties when Automath was ﬁrst designed.
Note that in this example, the proof of p ⇒p is encoded by the term
axiom p p (λH : TRUE(p). H)
which has type TRUE (impl p p). In the ‘direct’ Curry-Howard style of Coq, Agda
and NuPRL, p is itself a type, and the term encoding the proof of p ⇒p becomes
simply
λH : p. p
which has type p →p. Another diﬀerence between Automath and the modern
type theoretical systems is that in Automath the logic and basic axioms have to
be introduced axiomatically (as PN lines), while in Coq, Agda and NuPRL these
are given by an ‘inductive types’ deﬁnitional package, and as such are deﬁned using
the type theory of the system.
The earliest publication about Automath is technical report number 68-WSK-
05 from the Technical University in Eindhoven, dated November 1968 [de Bruijn,
1968a]. At that time de Bruijn already was a very successful mathematician, had
been full professor of mathematics for sixteen years (ﬁrst in Amsterdam and then
in Eindhoven), and was ﬁfty years old. The report states that Automath had been
developed in the years 1967–1968. Two other people already were involved at that
time: Jutting as a ﬁrst ‘user’, and both Jutting and van Bree as programmers that
helped with the ﬁrst implementations of the language. These implementations
were written in a variant of the Algol programming language (probably Algol 60,
although Algol W was used at some point for Automath implementations too, and
already existed by that time).
Automath was presented in December 1968 at the Symposium on Automatic
Demonstration, held at INRIA Rocquencourt in Paris. The paper presented there
was later published in 1970 in the proceedings of that conference [de Bruijn, 1968c].
The Automath system that is described in those two publications is very similar
to the Twelf system that implements the LF logical framework. A formalization
in this system essentially consists of a long list of deﬁnitions, in which a sequence
of constants are deﬁned as abbreviations of typed lambda terms. Through the
Curry-Howard isomorphism this allows one to encode arbitrary logical reasoning.
It appears that de Bruijn was not aware of the work by Curry and Howard at
the time. Both publications mentioned contain no references to the literature,

142
John Harrison, Josef Urban and Freek Wiedijk
and the notations used are very unlike what one would expect from someone who
knew about lambda calculus. For example, function application is written with
the argument in front of the function that is applied to it (which is indeed a more
natural order), i.e., instead of MN one writes ⟨N⟩M, and lambda abstraction is
not written as λx:A.M but as [x:A]M (this notation was later inherited by the Coq
system, although in modern Coq it has been changed.) Also, the type theory of
Automath is quite diﬀerent from the current type theories. In modern type theory,
if we have the typings M : B and B : s, then we have (λx:A.M) : (Πx:A.B) and
(Πx:A.B) : s. However, in Automath one would have ([x:A]M) : ([x:A]B) and
([x:A]B) : ([x:A]s). In other words, in Automath there was no diﬀerence between
λ and Π, and while in modern type theory binders ‘evaporate’ after two steps when
calculating types of types, in Automath they never will. This means that the typed
terms in Automath do not have a natural set theoretic interpretation (probably
the reason that this variant of type theory has been largely forgotten). However,
this does not mean that this is not perfectly usable as a logical framework.
Apparently de Bruijn rediscovered the Curry-Howard isomorphism mostly in-
dependently (although he had heard from Heyting about the intuitionistic inter-
pretation of the logical connectives). One of the inspirations for the Automath
language was a manual check by de Bruijn of a very involved proof, where he
wrote all the reasoning steps on a large sheet of paper [de Bruijn, 1990]. The
scoping of the variables and assumptions were indicated by drawing lines in the
proof with the variables and assumptions. written in a ‘ﬂag’ at the top of this line.
This is very similar to Ja´skowski-Fitch style natural deduction, but in Automath
(just like in LF) this is not tied to a speciﬁc logic.
There essentially have been four groups of Automath languages, of which only
the ﬁrst two have ever been properly implemented:
AUT-68 This was the ﬁrst variant of Automath, a simple and clean system,
which was explained in the early papers through various weaker and less
practical systems, with names like PAL, LONGPAL, LONGAL, SEMIPAL
and SEMIPAL 2, where ‘PAL’ abbreviates ‘Primitive Automath Language’
[de Bruijn, 1969; de Bruijn, 1970].
Recently there has been a revival of
interest in these systems from people investigating weak logical frameworks
[Luo, 2003].
AUT-QE This was the second version of the Automath language. ‘QE’ stands for
‘Quasi-Expressions’. With this Automath evolved towards the current type
theories (although it still was quite diﬀerent), one now both had ([x:A]B) :
([x:A]s) as well as ([x:A]B) : s. This was called type inclusion. AUT-QE is
the dialect of Automath in which the biggest formalization, Jutting’s trans-
lation of Landau’s Grundlagen, was written [van Benthem Jutting, 1979].
It has much later been re-implemented in C by one of the authors of this
chapter [Wiedijk, 2002].
Later AUT languages These are later variants of Automath, like AUT-QE-NTI
(a subset of AUT-QE in which subtyping was removed, the ‘NTI’ standing

History of Interactive Theorem Proving
143
for ‘no type inclusion’), and the AUT-Π and AUT-SYNTH extensions of
AUT-QE. These languages were modiﬁcations of the AUT-QE framework,
but although implementations were worked on, it seems none of them was
really ﬁnished.
AUT-SL This was a very elegant version of Automath developed by de Bruijn
(with variants of the same idea also developed by others). In this language
the distinction between deﬁnitions and redexes is removed, and the formal-
ization, including the deﬁnitions, becomes a single very large lambda term.
The ‘SL’ stands for ‘single line’ (Section B.2 of [Nederpelt et al., 1994]). The
system also was called ∆Λ, and sometimes Λ∆(Section B.7 of [Nederpelt et
al., 1994]). A more recent variant of this system was De Groote’s λλ type
theory [de Groote, 1993]. The system AUT-QE-NTI can be seen as a step
towards the AUT-SL language.
There were later languages, by de Bruijn and by others, that were more loosely
related to the Automath languages. One of these was WOT, the abbreviation of
‘wiskundige omgangstaal’, Dutch for mathematical vernacular [de Bruijn, 1979].
Unlike Trybulec with Mizar, de Bruijn only felt the need to have this ‘vernacular’
be structurally similar to actual mathematical prose, and never tried to make it
natural language-like.
In 1975, the Dutch science foundation ZWO (nowadays called NWO) gave a
large ﬁve year grant for the project Wiskundige Taal AUTOMATH to further
develop the Automath ideas. From this grant ﬁve researchers, two programmers
and a secretary were ﬁnanced [de Bruijn, 1978]. During the duration of this project
many students of the Technical University Eindhoven did formalization projects.
Examples of the subjects that were formalized were:
• two treatments of set theory
• a basic development of group theory
• an axiomatic development of linear algebra
• the K¨onig-Hall theorem in graph theory
• automatic generation of Automath texts that prove arithmetic identities
• the sine and cosine functions
• irrationality of π
• real numbers as inﬁnite sequences of the symbols 0 and 1
We do not know how complete these formalizations were, nor whether they were
written using an Automath dialect that actually was implemented.
In 1984 De Bruijn retired (although he stayed scientiﬁcally active), and the
Automath project was eﬀectively discontinued. In 1994 a volume containing the

144
John Harrison, Josef Urban and Freek Wiedijk
most important Automath papers was published [Nederpelt et al., 1994], and in
2003 most other relevant documents were scanned, resulting in the Automath
Archive, which is freely available on the web [Scheﬀer, 2003].
Automath has been one of the precursors of a development of type systems
called type theory:
• On the predicative side of things there were the type theories by Martin-
L¨of, developed from 1971 on (after discovery of the Girard paradox, in 1972
replaced by an apparently consistent system), which among other things
introduced the notion of inductive types [Nordstr¨om et al., 1990].
• On the impredicative side there were the polymorphic lambda calculi by
Girard (1972) and Reynolds (1974). This was combined with the depen-
dent types from Martin-L¨of’s type theory in Coquand’s CC (the Calculus
of Constructions), described in his PhD thesis [Coquand and Huet, 1988].
CC was structured by Barendregt into the eight systems of the lambda cube
[Barendregt, 1992], which was then generalised into the framework of pure
type systems (PTSs) by Berardi (1988) and Terlouw (1989).
Both the Martin-L¨of theories and the Calculus of Constructions were further de-
veloped and merged in various systems, like in ECC (Extended Calculus of Con-
structions) [Luo, 1989] and UTT (Universal Type Theory) [Luo, 1992], and by
Paulin in CIC (the Calculus of Inductive Constructions) [Coquand and Paulin,
1990], later further developed into pCIC (the predicative version of CIC, which
also was extended with coinductive types), the current type theory behind the
Coq system [Coq development team, 2012].
All these type theories are similar and diﬀerent (and have grown away from
the Automath type theory). Two of the axes on which one might compare them
are predicativity (‘objects are not allowed to be deﬁned using quantiﬁcation over
the domain to which the object belongs’) and intensionality (‘equality between
functions is not just determined by whether the values of the functions coincide’).
For example, the type theory of Coq is not yet predicative but it is intensional,
the type theory of NuPRL is predicative but it is not intensional, and the type
theory of Agda is both predicative and intensional.
Recently, there has been another development in type theory with the introduc-
tion of univalent type theory or homotopy type theory (HoTT) by Voevodsky in
2009 [Univalent Foundations Program, 2013]. Here type theory is extended with
an interpretation of equality as homotopy, which gives rise to the axiom of uni-
valence. This means that representation independence now is hardwired into the
type theory. For this reason, some people consider HoTT to be a new foundation
for mathematics. Also, in this system the inductive types machinery is extended to
higher inductive types (inductive types where equalities can be added inductively
as well). Together, this compensates for several problems when using Coq’s type
theory for mathematics: one gets functional extensionality and can have proper
deﬁnitions of subtypes and quotient types. This ﬁeld is still young and in very
active development.

History of Interactive Theorem Proving
145
We now list some important current systems that are based on type theory
and the Curry-Howard isomorphism, and as such can be considered successors to
Automath. We leave out the history of important historical systems like LEGO
[Pollack, 1994], and focus on systems that still have an active user community.
For each we give a brief overview of their development.
NuPRL
In 1979 Martin-L¨of introduced an extensional version of his type theory. In the
same year Bates and Constable, after earlier work on the PL/CV veriﬁcation
framework [Constable and O’Donnell, 1978] had founded the PRL research group
at Cornell University to develop a program development system where programs
are created in a mathematical fashion by interactive reﬁnement (PRL at that time
stood for Program Reﬁnement Logic). In this group various systems were devel-
oped: the AVID system (Aid Veriﬁcation through the techniques of Interactive
program Development) [Kraﬀt, 1981], the Micro-PRL system, also in 1981, and
the λPRL system [Constable and Bates, 1983].
In 1984 the PRL group implemented a variant of Martin-L¨of’s extensional type
theory in a system called NuPRL (also written as νPRL, to be read as ‘new
PRL’; PRL now was taken to stand for Proof Reﬁnement Logic). This system
[Constable, 1986] has had ﬁve versions, where NuPRL 5, the latest version, is
also called NuPRL LPE (Logical Programming Environment). In 2003, a new
architecture for NuPRL was implemented called MetaPRL (after ﬁrst having been
called NuPRL-Light) [Hickey et al., 2003]. The NuPRL type theory always had
a very large number of typing rules, and in the MetaPRL system this is handled
through a logical framework. In that sense this system resembles Isabelle.
Part of the NuPRL/MetaPRL project is the development of a library of formal
results called the FDL (Formal Digital Library).
Coq
In 1984 Huet and Coquand at INRIA started implementing the Calculus of Con-
structions in the CAML dialect of ML. Originally this was just a type checker for
this type theory, but with version 4.10 in 1989 the system was extended in the style
of the LCF system, with tactics that operate on goals. Subsequently many people
have worked on the system. Many parts have been redesigned and re-implemented
several times, including the syntax of the proof language and the kernel of the sys-
tem. The Coq manual [Coq development team, 2012] gives an extensive history of
the development of the system, which involved dozens of researchers. Of these a
majority made contributions to the system that have all turned out to be essential
for its eﬃcient use. Examples of features that were added are: coercions, canonical
structures, type classes, coinductive types, universe polymorphism, various deci-
sion procedures (e.g., for equalities in rings and ﬁelds, and for linear arithmetic),
various tactics (e.g., for induction and inversion, and for rewriting with a congru-

146
John Harrison, Josef Urban and Freek Wiedijk
ence, in type theory called ‘setoid equality’), mechanisms for customizing term
syntax, the coqdoc documentation system, the Ltac scripting language and the
Program command (which deﬁnes yet another functional programming language
within the Coq system). The system nowadays is a very feature rich environment,
which makes it the currently most popular interactive theorem prover based on
type theory.
The latest released version of Coq is 8.4. This system can be seen as a theorem
prover, but also as an implementation of a functional programming language with
an execution speed comparable to functional languages like OCaml. A byte code
machine similar to the one of OCaml was implemented by Gr´egoire and Leroy, but
there now also is native code compilation, implemented by Denes and Gr´egoire.
Also, computations on small integers can be done on the machine level, due to
Spiwack.
Another feature of Coq is that Coq programs can be exported in the syntax of
other functional programming languages, like OCaml and Haskell.
Coq has more than one user interface, of which Proof General [Aspinall, 2000]
and CoqIDE [Bertot and Th´ery, 1998] are currently the most popular.
There are two important extensions of Coq. First there is the SSReﬂect proof
language and associated mathematical components library by Gonthier and others
[Gonthier et al., 2008; Gonthier and Mahboubi, 2010], which was developed for the
formalization of the proofs of the 4-color theorem (2005) and the Feit-Thompson
theorem (2012). This is a compact and powerful proof language, which has not
been merged in the mainstream version of Coq. Second there are implementations
of Coq adapted for homotopy type theory.
Finally there is the Matita system from Bologna by Asperti and others [Asperti
et al., 2006]. This started out in 2004 as an independent implementation of a
type checker of the Coq type theory.
It was developed in the HELM project
(Hypertextual Electronic Library of Mathematics), which was about presenting
Coq libraries on the web [Asperti et al., 2003], and therefore at the time was
quite similar to Coq, but has in the meantime diverged signiﬁcantly, with many
improvements of its own.
Twelf
In 1987, Harper, Honsell and Plotkin introduced the Edinburgh Logical Frame-
work, generally abbreviated as Edinburgh LF, or just LF [Harper et al., 1987].
This is a quite simple predicative type theory, inspired by and similar to Au-
tomath, in which one can deﬁne logical systems in order to reason about them.
An important property of an encoded logic, which has to be proved on the meta
level, is adequacy, the property that the beta-eta long normal forms of the terms
that encode proofs in the system are in one-to-one correspondence with the proofs
of the logic themselves.
A ﬁrst implementation of LF was EFS (Environment for Formal Systems) [Grif-
ﬁn, 1988]. Soon after, in 1989, Pfenning implemented the Elf system, which added

History of Interactive Theorem Proving
147
a meta-programming layer [Pfenning, 1994]. In 1999 a new version of this sys-
tem was implemented by Pfenning and Sch¨urmann, called Twelf [Pfenning and
Sch¨urmann, 1999]. In the meta layer of Twelf, one can execute Twelf speciﬁca-
tions as logic programs, and it also contains a theorem prover that can establish
properties of the Twelf encoding automatically, given the right deﬁnitions and
annotations.
Agda
In 1990 a ﬁrst implementation of a type checker for Martin-L¨of’s type theory was
created by Coquand and Nordstr¨om. In 1992 this turned into the ALF (Another
Logical Framework) system, implemented by Magnusson in SML [Magnusson and
Nordstr¨om, 1993]. Subsequently a Haskell implementation of the same system was
worked on by Coquand and Synek, called Half (Haskell Alf). A C version of this
system called CHalf, also by Synek, was used for a signiﬁcant formalization by
Cederquist of the Hahn-Banach theorem in 1997 [Cederquist et al., 1998]. Synek
developed for this system an innovative Emacs interface that allows one to work in
a procedural style on a proof that essentially is declarative [Coquand et al., 2005].
A new version of this system called Agda was written by Catarina Coquand in
1996, for which a graphical user interface was developed around 2000 by Hallgren.
Finally Norell implemented a new version of this system in 2007 under the name
of Agda2 [Bove et al., 2009]. For a while there were two diﬀerent versions of this
system, a stable version and a more experimental version, but by now there is just
one version left.
3
LCF AND SUCCESSORS
The LCF approach to interactive theorem proving has its origins in the work of
Robin Milner, who from early in his career in David Cooper’s research group in
Swansea was interested speciﬁcally in interactive proof:
I wrote an automatic theorem prover in Swansea for myself and be-
came shattered with the diﬃculty of doing anything interesting in that
direction and I still am. I greatly admired Robinson’s resolution prin-
ciple, a wonderful breakthrough; but in fact the amount of stuﬀyou
can prove with fully automatic theorem proving is still very small. So
I was always more interested in amplifying human intelligence than I
am in artiﬁcial intelligence.2
Milner subsequently moved to Stanford where he worked in 1971–2 in John Mc-
Carthy’s AI lab. There he, together with Whitﬁeld Diﬃe, Richard Weyhrauch and
Malcolm Newey, designed an interactive proof assistant for what Milner called the
Logic of Computable Functions (LCF). This formalism, devised by Dana Scott in
2http://www.sussex.ac.uk/Users/mfb21/interviews/milner/

148
John Harrison, Josef Urban and Freek Wiedijk
1969, though only published much later [Scott, 1993], was intended for reasoning
about recursively deﬁned functions on complete partial orders (CPOs), such as
typically occur in the Scott-Strachey approach to denotational semantics. The
proof assistant, known as Stanford LCF [Milner, 1972], was intended more for
applications in computer science rather than mainstream pure mathematics. Al-
though it was a proof checker rather than an automated theorem prover, it did
provide a powerful automatic simpliﬁcation mechanism and convenient support
for backward, goal-directed proof.
There were at least two major problems with Stanford LCF. First, the stor-
age of proofs tended to ﬁll up memory very quickly. Second, the repertoire of
proof commands was ﬁxed and could not be customized.
When he moved to
Edinburgh, Milner set about ﬁxing these defects. With the aid of his research
assistants, Lockwood Morris, Malcolm Newey, Chris Wadsworth and Mike Gor-
don, he designed a new system called Edinburgh LCF [Gordon et al., 1979]. To
allow full customizability, Edinburgh LCF was embedded in a general program-
ming language, ML.3 ML was a higher-order functional programming language,
featuring a novel polymorphic type system [Milner, 1978] and a simple but use-
ful exception mechanism as well as imperative features. Although the ML lan-
guage was invented as part of the LCF project speciﬁcally for the purpose of
writing proof procedures, it has in itself been seminal: many contemporary func-
tional languages such as CAML Light and OCaml [Cousineau and Mauny, 1998;
Weis and Leroy, 1993] are directly descended from it or at least heavily inﬂuenced
by it, and their applications go far beyond just theorem proving.
In LCF, recursive (tree-structured) types are deﬁned in the ML metalanguage to
represent the (object) logical entities such as types, terms, formulas and theorems.
For illustration, we will use thm for the ML type of theorems, though the exact
name is not important.
Logical inference rules are then realized concretely as
functions that return an object of type thm. For example, a classic logical inference
rule is Modus Ponens or ⇒-elimination, which might conventionally be represented
in a logic textbook or paper as a rule asserting that if p ⇒q and p are both provable
(from respective assumptions Γ and ∆) then q is also provable (from the combined
assumptions):4
Γ ⊢p ⇒q
∆⊢p
Γ ∪∆⊢q
The LCF approach puts a concrete and computational twist on this by turning
each such rule into a function in the metalanguage. In this case the function, say
MP, takes two theorems as input, Γ ⊢p ⇒q and ∆⊢p, and returns the theorem
Γ ∪∆⊢q; it therefore has a function type thm->thm->thm in ML (assuming
3ML for metalanguage; following Tarski [1936] and Carnap [1937], it has become usual in
logic and linguistics to to distinguish carefully the object logic and the metalogic (which is used
to reason about the object logic).
4We show it in a sequent context where we also take the union of assumption lists.
In a
Hilbert-style proof system these assumptions would be absent; in other presentations we might
assume that the set of hypotheses are the same in both cases and have a separate weakening
rule. Such ﬁne details of the logical system are orthogonal to the ideas we are explaining here.

History of Interactive Theorem Proving
149
curried functions).
When logical systems are presented, it’s common to make
some terminological distinctions among the components of the foundation, and all
these get reﬂected in the types in the metalanguage when implemented in LCF
style:
• An axiom is simply an element of type thm
• An axiom schema (for example a ﬁrst-order induction principle with an in-
stance for each formula) becomes a function that takes some argument(s)
like a term indicating which instance is required, and returns something of
type thm.
• A true inference rule becomes an ML object like the MP example above that
takes objects, at least one of which is of type thm, as arguments and returns
something of type thm.
The traditional idea of logical systems is to use them as a foundation, by choos-
ing once and for all some relatively small and simple set of rules, axioms and axiom
schemas, which we will call the primitive inference rules, and thereafter perform
all proof using just those primitives. In an LCF prover one can, if one wishes, cre-
ate arbitrary proofs using these logical inference rules, simply by composing the
ML functions appropriately. Although a proof is always performed, the proof itself
exists only ephemerally as part of ML’s (strict) evaluation process, and therefore
no longer ﬁlls up memory. Gordon [2000] makes a nice analogy with writing out
a proof on a blackboard, and rubbing out early steps to make room for more. In
order to retain a guarantee that objects of type thm really were created by ap-
plication of the primitive rules, Milner had the ingenious idea of making thm an
abstract type, with the primitive rules as its only constructors. After this, one
simply needs to have conﬁdence in the fairly small amount of code underlying
the primitive inference rules to be quite sure that all theorems must have been
properly deduced simply because of their type.
But even for the somewhat general meta-arguments in logic textbooks, and cer-
tainly for concretely performing proofs by computer, the idea of proving something
non-trivial by decomposing it to primitive inference rules is usually daunting in
the extreme. In practice one needs some other derived rules embodying convenient
inference patterns that are not part of the axiomatic basis but can be derived from
them. A derived inference rule too has a concrete realization in LCF systems as a
function whose deﬁnition composes other inference rules, and using parametriza-
tion by the function arguments can work in a general and schematic way just like
the metatheorems of logic textbooks. For example, if we also have a primitive
axiom schema called ASSUME returning a theorem of the form p ⊢p:
{p} ⊢p

150
John Harrison, Josef Urban and Freek Wiedijk
then we can implement the following derived inference rule, which we will call
UNDISCH:
Γ ⊢p ⇒q
Γ ∪{p} ⊢q
as a simple function in the metalanguage. For example, the code might look some-
thing like the following. It starts by breaking apart the implication of the input
theorem to determine the appropriate p and q. (Although objects of type thm
can only be constructed by the primitive rules, they can be examined and decon-
structed freely.) Based on this, the appropriate instance of the ASSUME schema is
used and the two inference rules plugged together.
let UNDISCH th =
let Imp(p,q) = concl th in
MP th (ASSUME p);;
This is just a very simple example, but because a full programming language
is available, one can implement much more complex derived rules that perform
sophisticated reasoning and automated proof search but still ultimately reduce
to the primitive rules. Indeed, although LCF and most of its successors use a
traditional forward presentation of logic, it is easy to use a layer of programming
to support goal-directed proof in the style of Stanford LCF, via so-called tactics.
This ﬂexibility gives LCF an appealing combination of reliability and exten-
sibility. In most theorem proving systems, in order to install new facilities it is
necessary to modify the basic code of the prover. But in LCF an ordinary user
can write an arbitrary ML program to automate a useful inference pattern, while
all the time being assured that even if the program has bugs, no false theorems
will arise (though the program may fail in this case, or produce a valid theorem
other than the one that was hoped for). As Slind [1991] puts it ‘the user controls
the means of (theorem) production’.
LCF was employed in several applications at Edinburgh, and this motivated
certain developments in the system. By now, the system had attracted attention
elsewhere. Edinburgh LCF was ported to LeLisp and MacLisp by G´erard Huet,
and this formed the basis for a rationalization and redevelopment of the system by
Paulson [1987] at Cambridge, resulting in Cambridge LCF. First, Huet and Paul-
son modiﬁed the ML system to generate Lisp code that could be compiled rather
than interpreted, which greatly improved performance. Among many other im-
provements Paulson [1983] replaced Edinburgh LCF’s complicated and monolithic
simpliﬁer with an elegant scheme based on on conversions.
A conversion is a particular kind of derived rule (of ML type :term->thm) that
given a term t returns a theorem of the form Γ ⊢t = t′ for some other term t′.
(For example, a conversion for addition of numbers might map the term 2 + 3
to the theorem ⊢2 + 3 = 5.)
This gives a uniform framework for converting
ad hoc simpliﬁcation routines into those that are justiﬁed by inference: instead
of simply taking t and asserting its equality to t′, we actually carry theorems
asserting such equivalences through the procedure. Via convenient higher-order

History of Interactive Theorem Proving
151
functions, conversions can be combined in various ways, applied recursively in a
depth-ﬁrst fashion etc., with all the appropriate inference to plug the steps together
(transitivity and congruences and so on) happening automatically.
3.1
HOL
As emphasized by Gordon [1982], despite the name ‘LCF’, nothing in the Edin-
burgh LCF methodology is tied to the Logic of Computable Functions. In the early
1980s Gordon, now in Cambridge, as well as supervising Paulson’s development
of LCF, was interested in the formal veriﬁcation of hardware. For this purpose,
classical higher order logic seemed a natural vehicle, since it allows a rather direct
rendering of notions like signals as functions from time to values. The case was
ﬁrst put by Hanna and Daeche [1986] and, after a brief experiment with an ad hoc
formalism ‘LSM’ based on Milner’s Calculus of Communicating Systems, Gordon
[1985] also became a strong advocate.
Gordon modiﬁed Cambridge LCF to support classical higher order logic, and so
HOL (for Higher Order Logic) was born. Following Church [1940], the system is
based on simply typed λ-calculus, so all terms are either variables, constants, ap-
plications or abstractions; there is no distinguished class of formulas, merely terms
of boolean type. The main diﬀerence from Church’s system is that polymorphism
is an object-level, rather than a meta-level, notion; essentially the same Hindley-
Milner automated typechecking algorithm used in ML [Milner, 1978] is used in
the interface so that most general types for terms can be deduced automatically.
Using deﬁned constants and a layer of parser and pretty-printer support, many
standard syntactic conventions are broken down to λ-calculus. For example, the
universal quantiﬁer, following Church, is simply a higher order function, but the
conventional notation ∀x.P[x] is supported, mapping down to ∀(λx.P[x]). Sim-
ilarly there is a constant LET, which is semantically the identity and is used
only as a tag for the pretty-printer, and following Landin [1966], the construct
‘let x = t in s’ is broken down to ‘LET (λx.s) t’.5 The advantage of keeping the
internal representation simple is that the underlying proof procedures, e.g. those
that do term traversal during simpliﬁcation, need only consider a few cases.
The exact axiomatization of the logic was partly inﬂuenced by Church, partly
by the way things were done in LCF, and partly through consultation with the
logicians Mike Fourman, Martin Hyland and Andy Pitts in Cambridge. HOL orig-
inally included a simple constant deﬁnitional mechanism, allowing new equational
axioms of the form ⊢c = t to be added, where t is a closed term and c a new con-
stant symbol. A mechanism for deﬁning new types, due to Mike Fourman, was also
included. Roughly speaking one may introduce a new type in bijection with any
nonempty subset of an existing type (identiﬁed by its characteristic predicate). An
important feature of these deﬁnitional mechanisms bears emphasizing: they are
not metalogical translations, but means of extending the signature of the object
5Landin, by the way, is credited with inventing the term ‘syntactic sugar’, as well as this
notable example of it.

152
John Harrison, Josef Urban and Freek Wiedijk
logic, while guaranteeing that such extension preserves consistency. In fact, the
deﬁnitional principles are conservative, meaning roughly that no new statements
not involving the deﬁned concept become provable as a result of the extension.
HOL emphasized the systematic development of theories by these principles of
conservative extension to the point where it became the norm, purely axiomatic
extensions becoming frowned on. Such an approach is obviously a very natural ﬁt
with the LCF philosophy, since it entails pushing back the burden of consistency
proofs or whatever to the beginning, once and for all, such that all extensions,
whether of the theory hierarchy or proof mechanisms, are correct by construction.
(Or at least consistent by construction. Of course, it is perfectly possible to intro-
duce deﬁnitions that do not correspond to the intuitive notion being formalized,
but no computable process can resolve such diﬃculties.) This contrasts with LCF,
where there was no distinction between deﬁnitions and axioms, and new types
were often simply characterized by axioms without any formal consistency proof.
Though there was usually a feeling that such a proof would be routine, it is easy
to make mistakes in such a situation. It can be much harder to produce useful
structures by deﬁnitional extension than simply to assert suitable axioms — the
advantages were likened by Russell [1919] to those of theft over honest toil.
For example, Melham’s derived deﬁnitional principle [Melham, 1989] for recur-
sive data types was perhaps at the time the most sophisticated LCF-style derived
rule ever written, and introduced important techniques for maintaining eﬃciency
in complex rules that are still used today — we discuss the issues around eﬃcient
implementations of decision procedures later. This was the ﬁrst of a wave of de-
rived deﬁnitional principles in LCF-like systems for deﬁning inductive or coinduc-
tive sets or relations [Andersen and Petersen, 1991; Camilleri and Melham, 1992;
Roxas, 1993; Paulson, 1994a], general recursive functions [Ploegaerts et al., 1991;
van der Voort, 1992; Slind, 1996; Krauss, 2010], quotient types with automated
lifting of deﬁnitions and theorems [Harrison, 1998; Homeier, 2005], more general
forms of recursive datatypes with inﬁnite branching, nested and mutual recursion
or dual codatatypes [Gunter, 1993; Harrison, 1995a; Berghofer and Wenzel, 1999;
Blanchette et al., 2014] as well as special nominal datatypes to formalize variable
binding in a natural way [Urban, 2008]. Supporting such complex deﬁnitions as a
primitive aspect of the logic, done to some extent in systems as diﬀerent as ACL2
and Coq, is a complex, intricate and error-prone activity, and there is a lot said for
how the derived approach maintains foundational simplicity and security. In fact
general wellfounded recursive functions in Coq are also supported using derived
deﬁnitional principles [Balaa and Bertot, 2000], while quotients in the current
foundations of Coq are problematic for deeper foundational reasons too.
The HOL system was consolidated and rationalized in a major release in late
1988, which was called, accordingly, HOL88. It became fairly popular, acquired
good documentation, and attracted many users around the world. Nevertheless,
despite its growing polish and popularity, HOL88 was open to criticism. In par-
ticular, though the higher-level parts were coded directly in ML, most of the term
operations below were ancient and obscure Lisp code (much of it probably written

History of Interactive Theorem Proving
153
by Milner in the 1970s). Moreover, ML had since been standardized, and the new
Standard ML seemed a more promising vehicle for the future than Lisp, especially
with several new compilers appearing at the time. These considerations motivated
two new versions of HOL in Standard ML. One was developed by Roger Jones,
Rob Arthan and others at ICL Secure Systems and called ProofPower. This was
intended as a commercial product and has been mainly used for high-assurance
applications, though the current version is freely available and has also been used
in other areas like the formalization of mathematics.6 The other, called hol90, was
written by Konrad Slind [1991], under the supervision of Graham Birtwistle at the
University of Calgary. The entire system was coded in Standard ML, which made
all the pre-logic operations such as substitution accessible. Subsequently several
other versions of HOL were written, including HOL Light, a version with a sim-
pliﬁed axiomatization and rationalized structure written in CAML Light by one
of the present authors and subsequently ported to OCaml [Harrison, 2006a], and
Isabelle/HOL, described in more detail in the next section. The ‘family DAG’ in
Figure 2 gives an approximate idea of some of the inﬂuences. While HOL88, hol90
and hol98 are little used today (though HOL88 is available as a Debian package),
all the other provers in this picture are under active development and/or have
signiﬁcant user communities.
3.2
Isabelle
We will discuss one more LCF-style system in a little more depth because it has
some distinguishing features compared to others in the family, and is also perhaps
currently the most widely used member of the LCF family. This is the Isabelle
system, originally developed by Paulson [1990]. The initial vision of Isabelle was as
an LCF-style logical framework in which to embed other logics. Indeed, the subtitle
‘The Next 700 Theorem Provers’ in Paulson’s paper (with its nod to Landin’s ‘The
Next 700 Programming Languages’) calls attention to the proliferation of diﬀerent
theorem proving systems already existing at the time. Many researchers, especially
in computer science, were (and still are) interested in proof support for particular
logics (classical, constructive, many-valued, temporal etc.) While these all have
their distinctive features, they also have many common characteristics, making
the appeal of a re-usable generic framework obvious.
Isabelle eﬀectively introduces yet another layer into the meta-object distinc-
tion, with the logic directly implemented in the LCF style itself being considered
a meta-logic for the embedding of other logics. The Isabelle metalogic is a simple
form of higher-order logic. It is intentionally weak (for example, having no induc-
tion principles) so that it does not in itself introduce foundational assumptions
that some might ﬁnd questionable or cause incompatibilities with the way object
logics are embedded. But it serves its purpose well as a framework for embedding
object logics and providing a common infrastructure across them. The inference
rules in the object logic are then given in a declarative fashion as meta-implications
6See http://www.lemma-one.com/ProofPower/index/.

154
John Harrison, Josef Urban and Freek Wiedijk
HOL88
     ✠
hol90
❅
❅
❅
❅
❅
❘
ProofPower
❍❍❍❍❍❍❍❍❍❍❍
❥
Isabelle/HOL
❄
HOL Light
❄
hol98
❅
❅
❅❅
❘
     ✠
❄
HOL4
❅
❅
❅
❅
❅
❘
HOL Zero
❄
Figure 2: The HOL family tree

History of Interactive Theorem Proving
155
(implications in the meta-logic). For example, our earlier example of Modus Po-
nens can be represented as the following (meta) theorem. The variables starting
with ‘?’ are metavariables, i.e. variables in the metalogic; →denotes object-level
implication while ⇒denotes meta-level implication.
[?P →?Q; ?P] ⇒?Q
By representing object-level inference rules in this fashion, the actual implemen-
tations often just need to perform forward or backward chaining with matching
and/or uniﬁcation. Isabelle supports the eﬀective automation of this process with
a powerful higher-order uniﬁcation algorithm [Huet, 1975] giving a kind of higher-
order resolution principle.
Many design decisions in Isabelle were based on Paulson’s experience with
Cambridge LCF and introduced a number of improvements. In particular, back-
ward proof (‘tactics’) in LCF actually worked by iteratively creating function clo-
sures to eventually reverse the reﬁnement process into a sequence of the prim-
itive forward rules. This non-declarative formulation meant, for example, that
it was a non-trivial change to add support in LCF for logic variables allow-
ing the instantiation of existential goals to be deferred [Soko lowski, 1983].
Is-
abelle simply represented goals as theorems, with tactics eﬀectively working for-
wards on their assumptions, making the whole framework much cleaner and giv-
ing metavariables with no extra eﬀort. This variant of tactics was also adopted
in the ProofPower and LAMBDA systems (see next section).
Isabelle’s tac-
tic mechanism also allowed backtracking search over lazy lists of possible out-
comes in its tactic mechanism. Together with uniﬁcation, this framework could
be used to give very simple direct implementations of some classic ﬁrst-order
proof search algorithms like tableaux `a la leanTAP [Beckert and Posegga, 1995]
(fast_tac in Isabelle) and the Loveland-Stickel presentation [Loveland, 1978;
Stickel, 1988] of model elimination (meson_tac). While nowadays largely super-
seded by much more powerful automation of the kind that we consider later, these
simple tactics were at the time very convenient in making typical proofs less low-
level.
It’s customary to use appellations like ‘Isabelle/X’ to describe the particular in-
stantiation of Isabelle with object logic X. Among the many object logics embedded
in Isabelle are constructive type theory, classical higher order logic (Isabelle/HOL)
and ﬁrst-order Zermelo-Fraenkel set theory (Isabelle/ZF) [Paulson, 1994b]. De-
spite this diversity, only a few have been extensively used. Some axiom of choice
equivalences have been formalized in Isabelle/ZF [Paulson and Gr¸abczewski, 1996],
as has G¨odel’s hierarchy L of constructible sets leading to a proof of the relative
consistency of the Axiom of Choice [Paulson, 2003]. But by far the largest user
community has developed around the Isabelle/HOL instantiation [Nipkow et al.,
2002]. This was originally developed by Tobias Nipkow as an instantiation of Is-
abelle with something very close to the logic of the various HOL systems described
above, but with the addition (at the level of the metalogic) of a system of axiomatic
type classes similar to those of Haskell. In this instantiation, the ties between the

156
John Harrison, Josef Urban and Freek Wiedijk
Isabelle object and metalogic are particularly intimate.
Since its inception, Isabelle/HOL has become another full-ﬂedged HOL imple-
mentation. In fact, from the point of view of the typical user the existence of a
separate metalogic can largely be ignored, so the eﬀective common ground between
Isabelle/HOL and other HOL implementations is closer than might be expected.
However, one more recent departure (not limited to the HOL instantiation of Is-
abelle) takes it further from its LCF roots and other HOL implementations. This is
the adoption of a structured proof language called Isar, inspired by Mizar [Wenzel,
1999]. For most users, this is the primary interaction language, so they no longer
use the ML read-eval-print loop as the interface. The underlying LCF mechanisms
still exist and can be accessed, but many facilities are mediated by Isar and use
a sophisticated system of contexts. We describe some of the design decisions in
proof languages later in this chapter.
3.3
Other LCF systems
There have been quite a few other theorem provers either directly implemented in
the LCF style or at least heavily inﬂuenced by it. Some of them, such as NuPRL
and Coq, we have discussed above because of their links to constructive type the-
ory, and so we will not discuss them again here, but their LCF implementation
pedigree is also worth noting. For example, Bates and Constable [1985] describe
the LCF approach in detail and discuss how NuPRL developed from an earlier
system PL/CV. Another notable example is the LAMBDA (Logic And Mathe-
matics Behind Design Automation) system, which was developed in a team led by
Mike Fourman for use in hardware veriﬁcation. Among other distinctive features,
it uses a logic of partial functions, as did the original LCF system and as does the
non-LCF system IMPS [Farmer et al., 1990].
4
MIZAR
The history of Mizar [Matuszewski and Rudnicki, 2005] is a history of a team of
Polish mathematicians, linguists and computer scientists analyzing mathematical
texts and looking for a satisfactory human-oriented formal counterpart. One of
the mottos of this eﬀort has been Kreisel’s ‘ENOD: Experience, Not Only Doc-
trine’ [Rudnicki and Trybulec, 1999], which in the Mizar context was loosely un-
derstood as today’s ideas on rapid/agile software development. There were Mizar
prototypes with semantics that was only partially clear, and with only partial
veriﬁcation procedures. A lot of focus was for a long time on designing a suit-
able language and on testing it by translating real mathematical papers into the
language. The emphasis has not been just on capturing common patterns of rea-
soning and theory development, but also on capturing common syntactic patterns
of the language of mathematics. A Mizar text is not only supposed to be written
by humans and then read by machines, but it is also supposed to be directly easily
readable by humans, avoid too many parentheses, quotation marks, etc.

History of Interactive Theorem Proving
157
The idea of such a language and system was proposed in 1973 by Andrzej Try-
bulec, who was at that time ﬁnishing his PhD thesis in topology under Karol
Borsuk, and also teaching at the P lock Branch of the Warsaw University of Tech-
nology. Trybulec had then already many interests: since 1967 he had been pub-
lishing on topics in topology and linguistics, and in P lock he was also running the
Informatics Club. The name Mizar (after a star in Big Dipper)7 was proposed by
Trybulec’s wife Zinaida, originally for a diﬀerent project. The writing of his PhD
thesis and incorporating of Borsuk’s feedback prompted Trybulec to think about
languages and computer systems that would help mathematicians with such tasks.
He presented these ideas for the ﬁrst time at a seminar at the Warsaw University
on November 14, 1973, and was soon joined by a growing team of collaborators
that were attracted by his vision8 and personality, in many cases for their whole
lives: Piotr Rudnicki, Czes law Byli´nski, Grzegorz Bancerek, Roman Matuszewski,
Artur Kornilowicz, Adam Grabowski and Adam Naumowicz, to name just a few.
The total count of Mizar authors in May 2014 grew to 246.
In his ﬁrst note
(June 1975 [Trybulec, 1977]) about Mizar, Trybulec called such languages Logic-
Information Languages (LIL) and deﬁned them as facto-graphic languages that
enable recording of both facts from a given domain as well as logical relationships
among them. He proposed several applications for such languages, such as:
• Input to information retrieval systems which use logical dependencies.
• Intermediate languages for machine translation (especially of science).
• Automation of the editorial process of scientiﬁc papers, where the input
language is based on LIL and the output language is natural (translated into
many languages).
• Developing veriﬁcation procedures for such languages, not only in mathe-
matics, but also in law and medicine, where such procedures would typically
interact with a database of relevant facts depending on the domain.
• Education, especially remote learning.
• Artiﬁcial intelligence research.
For the concrete instantiation to mathematics, the 1975 note already speciﬁed the
main features of what is today’s Mizar, noting that although such a vision borders
on science ﬁction, it is a proper research direction:
7Some backronyms related to Mathematics and Informatics have been proposed later.
8It was easy to get excited, for several reasons.
Andrzej Trybulec and most of the Mizar
team have been a showcase of the popularity of science ﬁction in Poland and its academia. A
great selection of world sci-ﬁbooks has been shared by the Mizar team, by no means limited to
famous Polish sci-ﬁauthors such as Stanis law Lem. Another surprising and inspiring analogy
appeared after the project moved in 1976 to Bia lystok: the city where Ludwik Zamenhof grew
up and started to create Esperanto in 1873 [Zalewska, 2010] – 100 years before the development
of Mizar started.

158
John Harrison, Josef Urban and Freek Wiedijk
• The paper should be directly written in a LIL.
• The paper is checked automatically for syntactic and some semantic errors.
• There are procedures for checking the correctness of the reasoning, giving
reports about the reasoning steps that could not be automatically justiﬁed.
• A large database of mathematics is built on top of the system and used to
check if the established results are new, providing references, etc.
• When the paper is veriﬁed, its results are included in such database.
• The paper is automatically translated into natural languages, given to other
information retrieval systems such as citation indexes, and printed.
The proposal further suggested the use of classical ﬁrst-order logic with a rich set
of reasoning rules, and to include support for arithmetics and set theory. The
language should be rich and closely resemble natural languages, but on the other
hand it should not be too complicated to learn, and at the lexical and syntactic
level it should resemble programming languages. In particular, the Algol 60 and
later the Pascal language and compiler, which appeared in 1970, became sources of
inspiration for Mizar and its implementation languages. Various experiments with
Pascal program veriﬁcation were done later with Mizar [Rudnicki and Drabent,
1985].
It is likely that in the beginning Trybulec did not know about Automath,
LCF, SAM, and other Western eﬀorts, and despite his good contacts with Rus-
sian mathematicians and linguists, probably neither about the work of Glushkov
and his team in Kiev [Letichevsky et al., 2013] on the Evidence Algorithm and
the SAD system. However, the team learned about these projects quite quickly,
and apart from citing them, the 1978 paper on Mizar-QC/6000 [Trybulec, 1978]
also makes interesting references to Kaluzhnin’s 1962 paper on ‘information lan-
guage for mathematics’ [Kaluzhnin, 1962], and even earlier (1959, 1961) related
papers by Paducheva and others on such languages for geometry.
As in some
other scientiﬁc ﬁelds, the wealth of early research done by these Soviet groups has
been largely unknown in the western world, see [Lyaletski and Verchinine, 2010;
Verchinine et al., 2008] for more details about them.
4.1
Development of Mizar
The construction of the Mizar language was started by looking at the paper by H.
Patkowska9 A homotopy extension theorem for fundamental sequences [Patkowska,
1969], and trying to express it in the designed language. During the course of the
following forty years, a number of versions of Mizar have been developed (see
the timeline in Figure 3), starting with bare propositional calculus and rule-based
proof checking in Mizar-PC, and ending with today’s version of Mizar (Mizar 8 as
9Another PhD student of Karol Borsuk.

History of Interactive Theorem Proving
159
1975
1977
1979
1981
1983
1985
1987
1989
Mizar-PC
Mizar-MS
Mizar-FC
Mizar-QC
Mizar 2
Mizar-MSE
Mizar HPF
Mizar 3
Mizar 4
(PC-)Mizar
MML started
Figure 3: The Mizar timeline
of 2014) in which a library of 1200 interconnected formal articles is written and
veriﬁed.
Mizar-PC 1975-1976
While a suﬃciently expressive mathematical language was a clear target of the
Mizar language from the very beginning, the ﬁrst implementation [Trybulec, 1977]
of Mizar (written in Algol 1204) was limited to propositional calculus (Mizar-PC).
A number of features particular to Mizar were however introduced already in this
ﬁrst version, especially the Mizar vocabulary and grammar motivated by Algol
60, and the Mizar suppositional proof style which was later found10 to correspond
to Ja´skowski-Fitch natural deduction [Ja´skowski, 1934]. An example proof taken
from the June 1975 description of Mizar-PC is as follows:
begin
((p ⊇q) ∧(q ⊇r)) ⊇(p ⊇r)
proof
let
A: (p ⊇q) ∧(q ⊇r) ;
then
B: p ⊇q ;
C: q ⊇r by A ;
let
p ;
then
q by B ;
hence
r by C
end
end
The thesis (contents, meaning) of the proof in Mizar-PC is constructed from
assumptions introduced by the keyword let (later changed to assume) by placing
10Indeed, the Mizar team found only later that they re-invented Ja´skowski-Fitch proof style.
Andrzej Trybulec was never completely sure if he had not heard about it earlier, for example at
Roman Suszko’s seminars.

160
John Harrison, Josef Urban and Freek Wiedijk
an implication after them, and from conclusions introduced by keywords thus and
hence by placing a conjunction after them (with the exception of the last one).
The by keyword denotes inference steps where the formula on the left is justiﬁed
by the conjunction of formulas whose labels are on the right. The immediately
preceding formula can be used for justiﬁcation without referring to its label by
using the linkage mechanism invoked by keywords then for normal inference steps
and hence for conclusions. The proof checker veriﬁes that the formula to be proved
is the same as the thesis constructed from the proof, and that all inference steps
are instances of about ﬁve hundred inference rules available in the database of im-
plemented schematic proof-checking rules. This rule-based approach was changed
in later Mizar versions to an approach based on model checking (in a general sense,
not in connection with temporal logic model checking). Mizar-PC already allowed
the construction of a so-called compound statement (later renamed to diﬀuse state-
ment), i.e., a statement that is constructed implicitly from its suppositional proof
given inside the begin ... end brackets (later changed to now ... end) and can be
given a label and referred to in the same way as normal statements. An actual
use of Mizar-PC was for teaching propositional logic in P lock and Warsaw.
Mizar-QC 1977-1978
Mizar-QC added quantiﬁers to the language and proof rules for them. An example
proof (taken from [Matuszewski and Rudnicki, 2005]) is:
BEGIN
((EX X ST (FOR Y HOLDS P2[X,Y])) > (FOR X HOLDS (EX Y ST P2[Y,X])))
PROOF
ASSUME THAT A: (EX X ST (FOR Y HOLDS P2[X,Y]));
LET X BE ANYTHING;
CONSIDER Z SUCH THAT C: (FOR Y HOLDS P2[Z,Y]) BY A;
SET Y = Z;
THUS D: P2[Y,X] BY C;
END
END
The FOR and EX keywords started to be used as quantiﬁers in formulas, and the LET
statement started to be used for introducing a local constant in the proof corre-
sponding to the universal quantiﬁer of the thesis. The keyword ASSUME replaced the
use of LET for introducing assumptions. The CONSIDER statement also introduces
a local constant with a proposition justiﬁed by an existential statement. The SET
statement (replaced by TAKE later) chooses an object that will correspond to an
existential quantiﬁer in the current thesis. While the LET X BE ANYTHING statement
suggests that a sort/type system was already in place, in the QC version the only
allowed sort was just ANYTHING.
The BY justiﬁcation proceeds by transforming the set of formulas into a standard
form that uses only conjunction, negation and universal quantiﬁcation and then

History of Interactive Theorem Proving
161
applying a set of rewrite rules restricted by a bound on a sum of complexity
coeﬃcients assigned to the rules. The veriﬁer was implemented in Pascal/6000
for the CDC CYBER-70, and run in a similar way to the Pascal compiler itself,
i.e., producing a list of error messages for the lines of the Mizar text. The error
messages are inspected by the author, who modiﬁes the text and runs the veriﬁer
again. This batch/compilation style of processing of the whole text is also similar
to TEX, which was born at the same time. It has become one of the distinctive
features of Mizar when compared to interpreter-like ITPs implemented in Lisp and
ML.
Mizar Development in 1978-1988
The development of Mizar-QC was followed by a decade of additions leading to the
ﬁrst version of PC-Mizar11 in 1989. In 1989 the building of the Mizar Mathematical
Library (MML) started, using PC-Mizar.
Mizar MS (Multi Sorted) (1978) added predicate deﬁnitions and syntax
for schemes (such as the Induction and Replacement schemes), i.e., patterns of
theorems parameterized by arbitrary predicates and functors. Type declarations
were added, the logic became many-sorted, and equality was built in.
Mizar FC (1978-1979) added function (usually called functor in Mizar) def-
initions. The syntax allowed both equational deﬁnitions and deﬁnitions by condi-
tions that guarantee existence and uniqueness of the function. The BY justiﬁcation
procedure based on rewrite rules was replaced by a procedure based on ‘model
checking’: the formula to be proved is negated, conjoined with all its premises, and
the procedure tries to refute every possible model of this conjunction. This proce-
dure [Wiedijk, 2000] has been subject to a number of additions and experiments
throughout the history of Mizar development, focusing on the balance between
speed, strength, and obviousness to the human reader [Davis, 1981; Rudnicki,
1987a]. They were mainly concerned with various restricted versions of matching
and uniﬁcation, algorithms such as congruence closure for handling equality eﬃ-
ciently, handling of the type system and various built-in constructs [Naumowicz
and Bylinski, 2002; Naumowicz and Bylinski, 2004].
Mizar-2 (1981) introduced the environment part of an article, at that time
containing statements that are checked only syntactically, i.e., without requiring
their justiﬁcation. Later this part evolved into its current form used for importing
theorems, deﬁnitions and other items from other articles. Type deﬁnitions were
introduced: types were no longer disjoint sorts, but non empty sets or classes
deﬁned by a predicate. This marked the beginning of another large Mizar research
topic: its soft type system added on top of the underlying ﬁrst-order logic. Types
in Mizar are neither foundational nor disjoint as in the HOL and Automath families
[Wiedijk, 2007]. The best way in which to think of the Mizar types is as a hierarchy
of predicates (not just monadic: n-ary predicates result in dependent types), where
traversing of this hierarchy – the subtyping, intersection, disjointness relations, etc.
11PC stands here for Personal Computer.

162
John Harrison, Josef Urban and Freek Wiedijk
– is to a large extent automated and user-programmable, allowing the automation
of large parts of the mundane mathematical reasoning.12 Where such automation
fails, the RECONSIDER statement can be used, allowing one to change a type of an
object explicitly after a justiﬁcation.
Mizar-3 and Mizar-4 (1982-1988) divided the processing into multiple
cheaper passes with ﬁle-based communication, such as scanning, parsing, type
and natural-deduction analysis, and justiﬁcation checking. The use of special vo-
cabulary ﬁles for symbols together with allowing inﬁx, preﬁx, postﬁx notation and
their combinations resulted in greater closeness to mathematical texts. Reserva-
tions were introduced, for predeclaring variables and their default types. Other
changes and extensions included uniﬁed syntax for deﬁnitions of functors, predi-
cates, attributes and types, keywords for various correctness conditions related to
deﬁnitions such as uniqueness and existence, etc. In 1986, Mizar-4 was ported to
the IBM PC platform running MS-DOS and renamed to PC-Mizar in 1988.
PC-Mizar and the Mizar Mathematical Library (1988-)
In 1987-1991 a relatively large quantity of national funding was given to the Mizar
team to develop the system and to use it for substantial formalization. The main
modiﬁcation to Mizar to allow that were mechanisms for importing parts of other
articles. The oﬃcial start of building of the Mizar Mathematical Library (MML)
dates to January 1, 1989, when three basic articles deﬁning the language and
axioms of set theory and arithmetic were submitted. Since then the development
of Mizar has been largely driven by the growth of the MML. Apart from the further
development of the language and proof-checking mechanisms, a number of tools
for proof and library refactoring have been developed. The Library Committee
has been established, and gradually more and more work of the core Mizar team
shifted to refactoring the library so that duplication is avoided and theories are
developed in general and useful form [Rudnicki and Trybulec, 2003]. Perhaps the
largest project done in Mizar so far has been the formalization of about 60% of
the Compendium of Continuous Lattices [Bancerek and Rudnicki, 2002], which
followed the last QED workshop organized by the Mizar team in Warsaw.13 This
eﬀort resulted in about 60 MML articles. One of the main lessons learned (see
also the Kreisel’s motto above) by the Mizar team from such large projects has
been expressed in [Rudnicki and Trybulec, 1999] as follows:
The MIZAR experience indicates that computerized support for math-
ematics aiming at the QED goals cannot be designed once and then
simply implemented. A system of mechanized support for mathematics
12This soft typing system bears some similarities to the sort system implemented in ACL2,
and also to the type system used by the early Ontic system. Also the more recent soft (non-
foundational) typing mechanisms such as type classes in Isabelle and Coq, and canonical struc-
tures in Coq, can be to a large extent seen as driven by the same purpose as types have in Mizar:
non-foundational mechanisms for automating the work with hierarchies of deﬁned concepts that
can overlap in various ways.
13http://mizar.org/qed/

History of Interactive Theorem Proving
163
is likely to succeed if it has an evolutionary nature. The main compo-
nents of such a system – the authoring language, the checking software,
and the organization of the data base - must evolve as more experi-
ence is collected. At this moment it seems diﬃcult to extrapolate the
experience with MIZAR to the fully ﬂedged goals of the QED Project.
However, the people involved in MIZAR are optimistic.
5
SYSTEMS BASED ON POWERFUL AUTOMATION
The LCF approach and the systems based on type theory all tend to emphasize
a highly foundational approach to proof, with a (relatively) small proof kernel
and a simple axiomatic basis for the mathematics used. While Mizar’s software
architecture doesn’t ostensibly have the same foundational style, in practice its
automation is rather simple, arguably an important characteristic since it also
enables batch proof script checking to be eﬃcient. Thus, all these systems em-
phasize simple and secure foundations and try to build up from there. Nowadays
LCF-style systems in particular oﬀer quite powerful automated support, but this
represents the culmination of decades of sometimes arduous research and devel-
opment work in foundational implementations of automation. In the ﬁrst decade
of their life, many systems like Coq and Isabelle/HOL that nowadays seem quite
powerful only oﬀered rather simple and limited automation, making some of the
applications of the time seem even more impressive.
A contrasting approach is to begin with state-of-the-art automation, regardless
of its foundational characteristics. Systems with this philosophy were usually in-
tended to be applied immediately to interesting examples, particularly in software
veriﬁcation, and in many cases were intimately connected with custom program
veriﬁcation frameworks. (For example, the GYPSY veriﬁcation framework [Good
et al., 1979] tried to achieve just the kind of eﬀective blend of interaction and
automation we are considering, and had a signiﬁcant impact on the development
of proof assistants.) Indeed, in many cases these proof assistants became vehi-
cles for exploring approaches to automation, and thus pioneered many techniques
that were later re-implemented in a foundational context by the other systems.
Although there are numerous systems worthy of mention — we note in passing
EVES/Never [Craigen et al., 1991], KIV [Reif, 1995] and SDVS [Marcus et al.,
1985] — we will focus on two major lines of research that we consider to have
been the most inﬂuential. Interestingly, their overall architectures have relatively
little common ground — one emphasizes automation of inductive proofs with it-
erated waves of simpliﬁcation by conditional rewriting, the other integration of
quantiﬁer-free decision procedures via congruence closure. In their diﬀerent ways,
both have profoundly inﬂuenced the ﬁeld. One might of course characterize them
as automated provers rather than interactive ones, and some of this work has cer-
tainly been inﬂuential in the ﬁeld of pure automation. Nevertheless, we consider
that they belong primarily to the interactive world, because they are systems that
are normally used to attack challenging problems via a human process of inter-

164
John Harrison, Josef Urban and Freek Wiedijk
action and lemma generation, even though the automation in the background is
unusually powerful. For example, the authors of NQTHM say the following [Boyer
and Moore, 1988]:
In a shallow sense, the theorem prover is fully automatic: the system
requires no advice or directives from the user once a proof attempt has
started. The only way the user can alter the behavior of the system
during a proof attempt is to abort the proof attempt. However, in a
deeper sense, the theorem prover is interactive: the data base – and
hence the user’s past actions – inﬂuences the behavior of the theorem
prover.
5.1
NQTHM and ACL2
The story of NQTHM and ACL2 really starts with the fertile collaboration between
Robert Boyer and J Strother Moore, both Texans who nevertheless began their
work together in 1971 when they were both at the University of Edinburgh. How-
ever, we can detect the germ of some of the ideas further back in the work of Boyer’s
PhD advisor, Woody Bledsoe. Bledsoe was at the time interested in more human-
oriented approaches to proof, swimming against the tide of the then-dominant
interest in resolution-like proof search. For example, Bledsoe and Bruell [1974]
implemented a theorem prover that was used to explore semi-automated proof,
particularly in general topology. In a sense this belongs in our list of pioneering
interactive systems because it did provide a rudimentary interactive language for
the human to guide the proof, e.g. PUT to explicitly instantiate a quantiﬁed vari-
able. The program placed particular emphasis on the systematic use of rewriting,
using equations to simplify other formulas. Although this also appeared in other
contexts under the name of demodulation [Wos et al., 1967] or as a special case
of superposition in completion [Knuth and Bendix, 1970], and has subsequently
developed into a major research area in itself [Baader and Nipkow, 1998], Bled-
soe’s emphasis was instrumental in establishing rewriting and simpliﬁcation as a
key component of many interactive systems.
Although Boyer and Moore brieﬂy worked together on the popular theme of
resolution proving, they soon established their own research agenda: formalizing
proofs by induction. In a fairly short time they developed their ‘Pure LISP theo-
rem prover’, which as the name suggests was designed to reason about recursive
functions in a subset of pure (functional) Lisp.14 The prover used some relatively
simple but remarkably eﬀective techniques. Most of the interesting functions were
deﬁned by primitive recursion of one sort or another, for example over N by deﬁn-
ing f(n + 1) in terms of f(n) or over lists by deﬁning f(CONS h t), where CONS is
the Lisp list constructor, h the head element and t the tail of remaining elements,
in terms of f(t). (In fact, only the list form was primitive in the prover, with
14Note that the prover was not then implemented in Lisp, but rather in POP-2, a language
developed at Edinburgh by Robin Popplestone and Rod Burstall.

History of Interactive Theorem Proving
165
Destructor elimination
Formula
Induction
Elimination
of irrelevance
Generalization
Cross−fertilization
POOL
Simplification
Figure 4: The Boyer-Moore ‘Waterfall’ model
natural numbers being represented via lists in zero-successor form.) The pattern
of recursive deﬁnitions was used to guide the application of induction principles
and so produce explicit induction hypotheses. Moreover, the prover was also able
to generalize the statement to be proved in order better to apply induction — it
is a well-known phenomenon that this can make inductive proofs easier because
one strengthens the inductive hypothesis that is available. These two distinctive
features were at the heart of the prover, but it also beneﬁted from a number of
additional techniques like the systematic use of rewriting. Indeed, it was empha-
sized that proofs should ﬁrst be attempted using more simple and controllable
techniques like rewriting, with induction and generalization only applied if that
was not suﬃcient.
The overall organization of the prover was a prototypical form of what has
become known as the ‘Boyer-Moore waterfall model’. One imagines conjectures
as analogous to water ﬂowing down a waterfall down to a ‘pool’ below. On their
path to the pool below conjectures may be modiﬁed (for example by rewriting),
they may be proven (in which case they evaporate), they may be refuted (in
which case the overall process fails) or they may get split into others. When all
the ‘easy’ methods have been applied, generalization and induction take place,
and the new induction hypotheses generated give rise to another waterfall. This
process is graphically shown in the traditional picture in Figure 4, although not
all the initial steps were present from the beginning.
The next stage in development was a theorem prover concisely known as THM,
which then evolved via QTHM (‘quantiﬁed THM’) into NQTHM (‘new quantiﬁed THM’).

166
John Harrison, Josef Urban and Freek Wiedijk
This system was presented in book form [Boyer and Moore, 1979] and brought
Boyer and Moore’s ideas to a much wider audience as well as encouraging actual
use of the system. Note that Boyer and Moore did not at that time use the term
NQTHM in their own publications, and although it was widely known simply as ‘the
Boyer-Moore theorem prover’, they were too modest to use that term themselves.
NQTHM had a number of developments over its predecessors. As the name implies,
it supported formulas with (bounded) quantiﬁcation. It made more extensive and
systematic use of simpliﬁcation, using previously proved lemmas as conditional,
contextual rewrite rules. A so-called shell principle allowed users to deﬁne new
data types instead of reducing everything explicitly to lists. The system was able to
handle not only primitive recursive deﬁnitions and structural inductions [Burstall,
1969] over these types, but also deﬁnitions by wellfounded recursion and proofs
by wellfounded induction, using an explicit representation of countable ordinals
internally. A decision procedure was also added for rational linear arithmetic. All
these enhancements made the system much more practical and it was subsequently
used for many non-trivial applications, including Hunt’s pioneering veriﬁcation of
a microprocessor [Hunt, 1985], Shankar’s checking of G¨odel’s First Incompleteness
Theorem [Shankar, 1994], as well as others we will discuss brieﬂy later on. In
a signiﬁcant departure from the entirely automatic Pure Lisp Theorem Prover,
NQTHM also supported the provision of hints for guidance and a proof checker
allowing each step of the proof to be speciﬁed interactively.
The next step in the evolution of this family, mainly the work of Moore and
Matt Kaufmann, was a new system called ACL2, ‘A Computational Logic for
Applicative Common Lisp’ [Kaufmann et al., 2000b]. (Boyer himself helped to es-
tablish the project and continued as an important inspiration and source of ideas,
but at some point stopped being involved with the actual coding.)
Although
many aspects of NQTHM including its general style were retained, this system all
but eliminated the distinction between its logic and its implementation language
— both are a speciﬁc pure subset of Common Lisp. One advantage of such an
identiﬁcation is eﬃciency. In many of the industrial-scale applications of NQTHM
mentioned above, a key requirement is eﬃcient execution of functions inside the
logic. Instead of the custom symbolic execution framework in NQTHM, ACL2 sim-
ply uses direct Lisp execution, generally much faster. This identiﬁcation of the
implementation language and logic also makes possible a more principled way of
extending the system with new veriﬁed decision procedures, a topic we discuss
later.
ACL2 has further accelerated and consolidated the use of the family of
provers in many applications of practical interest [Kaufmann et al., 2000a].
Besides such concrete applications of their tools, Boyer and Moore’s ideas on
induction in particular have spawned a large amount of research in automated
theorem proving. A more detailed overview of the development we have described
from the perspective of the automation of inductive proof is given by Moore and
Wirth [2013]. Among the research topics directly inspired by Boyer and Moore’s
work on induction are Bundy’s development of proof planning [Bundy et al., 1991]
and the associated techniques like rippling. Since this is somewhat outside our

History of Interactive Theorem Proving
167
purview we will not say more about this topic.
5.2
EHDM and PVS
There was intense interest in the 1970s and 1980s in the development of frameworks
that could perform computer system veriﬁcation. This was most pronounced, ac-
companied by substantial funding in the US, for veriﬁcation of security properties
such as isolation in time-sharing operating systems (these were then quite new and
this property was a source of some concern), which was quite a stimulus to the
development of formal veriﬁcation and theorem proving in general [MacKenzie,
2001]. Among the other systems developed were AFFIRM, GYPSY [Good et al.,
1979], Ina Jo and the Stanford Pascal Veriﬁer. Closely associated with this was
the development of combined decision procedures by Nelson and Oppen [1980] and
by Shostak [1984]. One other inﬂuential framework was HDM, the ‘hierarchical
development methodology’. The ‘hierarchical’ aspect meant that it could be used
to describe systems at diﬀerent levels of abstraction where a ‘black box’ at one
level could be broken down into other components at a lower level.
HDM top-level speciﬁcations were written in SPECIAL, a ‘Speciﬁcation and
Assertion Language’. A security ﬂow analyzer generated veriﬁcation conditions
that were primarily handled using the Boyer-Moore prover discussed previously.
Unfortunately, the SPECIAL language and the Boyer-Moore prover were not de-
signed together, and turned out not to be very smoothly compatible. This meant
that a layer of translation needed to be applied, which often rendered the back-end
formulas diﬃcult to understand in terms of the original speciﬁcation. Together
with the limited interaction model of the prover, this eﬀectively made it clumsy
for users to provide any useful interactive guidance.
Based on the experiences with HDM, a new version EHDM (‘Enhanced HDM’)
was developed starting in 1983, with most of the system designed by Michael
Melliar-Smith, John Rushby and Richard Schwarz, while Shostak’s decision proce-
dure suite STP was further developed and used as a key component [Melliar-Smith
and Rushby, 1985]. Technically this was somewhat successful, introducing many
inﬂuential ideas such as a system of modules giving parametrization at the theory
level (though not ﬁne-grained polymorphism in the HOL sense). It was also used
in a number of interesting case studies such as the formalization [Rushby and von
Henke, 1991] of an article by Lamport and Melliar-Smith [1985] containing a proof
of correctness for a fault-tolerant clock synchronization algorithm, which identiﬁed
several issues with the informal proof.
Working with Sam Owre and Natarajan Shankar, John Rushby led the project
to develop PVS (originally at least standing for ‘Prototype Veriﬁcation System’)
[Owre et al., 1992] as a new prover for EHDM. Over time it took on a life of
its own while EHDM for a variety of technical, pragmatic and social reasons fell
into disuse. Among other things, Shostak and Schwarz left to start the database
company Paradox, and US Government restrictions made it inordinately diﬃcult
for many prospective users to get access to EHDM. Indeed, it was common to

168
John Harrison, Josef Urban and Freek Wiedijk
hear PVS expanded as ‘People’s Veriﬁcation System’ to emphasize the more liberal
terms on which it could be used.
The goal of PVS was to retain the advantages of EHDM, such as the richly typed
logic and the parametrized theories, while addressing some of its weaknesses, mak-
ing automated proof more powerful (combining Shostak-style decision procedures
and eﬀective use of rewriting) and supporting top-down interactive proof via a
programmable proof language. At the time there was a widespread belief that one
had to make an exclusive choice between a rich logic with weak automation (Au-
tomath) or a weak logic with strong automation (NQTHM). One of the notable
successes of PVS was in demonstrating convincingly that it was quite feasible to
have both.
The PVS logic (or ‘speciﬁcation language’) is a strongly typed higher-order
logic. It does not have the sophisticated dependent type constructors found in some
constructive type theories, but unlike HOL it allows some limited use of dependent
types, where types are parametrized by terms. In particular, given any type α and
a subset of (or predicate over) the type α, there is always a type corresponding to
that subset. In other words, PVS supports predicate subtypes. In HOL, the simple
type system has the appealing property that one can infer the most general types
of terms fully automatically. The price paid for the predicate subtypes in PVS
is that in general typechecking (that is, deciding whether a term has a speciﬁc
type) may involve arbitrarily diﬃcult theorem proving, and the processes of type
checking and theorem proving are therefore intimately intertwined. On the other
hand, because of the powerful automation, many of the type correctness conditions
(TCCs) can still be decided without user interaction.
The PVS proof checker presents the user with a goal-directed view of the prov-
ing process, representing goals using multiple-conclusion sequents.
Many basic
commands for decomposing and simplifying goals are as in many other interactive
systems like Coq or HOL. But PVS also features powerful and tightly integrated
decision procedures that are able to handle many routine goals automatically in
response to a simple invocation of the simplify command. Although PVS does
not make the full implementation language available for programming proof proce-
dures, there is a special Lisp-like language that can be used to link proof commands
together into custom strategies.
6
RESEARCH TOPICS IN INTERACTIVE THEOREM PROVING
Having seen some of the main systems and the ideas they introduced in founda-
tions, software architecture, proof language etc., let us step back and reﬂect on
some of the interesting sources of diversity and examine some of the research topics
that naturally preoccupy researchers in the ﬁeld.

History of Interactive Theorem Proving
169
6.1
Foundations
For those with only a vague interest in foundations who somehow had the idea
that ZF set theory was the standard foundation for mathematics, the diversity, not
to say Balkanization, of theorem provers according to foundational system may
come as a surprise. We have seen at least the following as foundations even in the
relatively few systems we’ve surveyed here:
• Quantiﬁer-free logic with induction (NQTHM, ACL2)
• Classical higher-order logic (HOLs, PVS)
• Constructive type theory (Coq, NuPRL)
• First-order set theory (Mizar, EVES, Isabelle/ZF)
• Logics of partial terms (LCF, IMPS, Isabelle/HOLCF)
Some of this diversity arises because of speciﬁc philosophical positions among
the systems’ developers regarding the foundations of mathematics. For example,
modern mathematicians (for the most part) use nonconstructive existence proofs
without a second thought, and this style ﬁts very naturally into the framework
of classical ﬁrst-order set theory. Yet ever since Brouwer’s passionate advocacy
[van Dalen, 1981] there has been a distinct school of intuitionistic or constructive
mathematics [Beeson, 1984; Bishop and Bridges, 1985]. While Brouwer had an al-
most visceral distaste for formal logic, Heyting introduced an intuitionistic version
of logic, and although there are workable intuitionistic versions of formal set the-
ory, the type-theoretic frameworks exploiting the Curry-Howard correspondence
between propositions and types, such as Martin-L¨of’s type theory [Martin-L¨of,
1984], are arguably the most elegant intuitionistic formal systems, and it is these
that have inspired Coq, NuPRL and many other provers.
Other motivations for particular foundational schemes are pragmatic. For ex-
ample, HOL’s simple type theory pushes a lot of basic domain reasoning into
automatic typechecking, simplifying the task of producing a reasonable level of
mechanical support, while the very close similarity with the type system of the
ML programming language makes it feel natural to a lot of computer scientists.
The quantiﬁer-free logic of NQTHM may seem impoverished, but the very restric-
tiveness makes it easier to provide powerful automation, especially of inductive
proof, and forces deﬁnitions to be suitable for actual execution.
Indeed, a little reﬂection shows that the distinction between philosophical and
pragmatic motivations is not clear-cut. While one will not ﬁnd any philosophical
claims about constructivism associated with NQTHM and ACL2, it is a fact that
the logic is even more clearly constructive than intuitionistic type theories.15 De-
spite the Lisp-like syntax, it is conceptually close to primitive recursive arithmetic
15At least in its typical use — we neglect here the built-in interpreter axiomatized in NQTHM,
which could be used to prove nonconstructive results [Kunen, 1998].

170
John Harrison, Josef Urban and Freek Wiedijk
(PRA) [Goodstein, 1957]. And many people ﬁnd intuitionistic logic appealing not
so much because of philosophical positions on the foundations of mathematics but
because at least in principle, the Curry-Howard correspondence has a more prag-
matic side: one can consider a proof in a constructive system actually to be a
program [Bates and Constable, 1985].
The language we use can often signiﬁcantly inﬂuence our thoughts, whether it
be natural language, mathematical notation or a programming language [Iverson,
1980]. Similarly, the chosen foundations can inﬂuence mathematical formalization
either for good or ill, unifying and simplifying it or twisting it out of shape. Indeed,
it can even inﬂuence the kinds of proofs we may even try to formalize. For example,
ACL2’s lack of traditional quantiﬁers makes it unappealing to formalize traditional
epsilon-delta proofs in real analysis, yet it seems ideally suited to the reasoning
in nonstandard analysis, an idea that has been extensively developed by Gamboa
[1999]; for another development of this topic in Isabelle/HOL see [Fleuriot, 2001].
In particular, the value of types is somewhat controversial. Both types [White-
head and Russell, 1910; Ramsey, 1926; Church, 1940] and the axiomatic approach
to set theory culminating in modern systems like ZF, NBG etc., originated in
attempts to resolve the paradoxes of naive set theory, and may be seen as two
competing approaches. Set theory has long been regarded as the standard foun-
dation, but it seems that at least when working in concrete domains, most math-
ematicians do respect natural type distinctions (points versus lines, real numbers
versus sets of real numbers). Even simpler type systems like that of HOL make
a lot of formalizations very convenient, keeping track of domain conditions and
compatibility automatically and catching blunders at an early stage.
However, for some formalizations the type system ceases to help and becomes an
obstacle. This seems to occur particularly in traditional abstract algebra where
constructions are sometimes presented in a very type-free way. For example, a
typical “construction” of the algebraic closure of a ﬁeld proceeds by showing that
one can extend a given ﬁeld F with a root a of a polynomial p ∈F[x], and then
roughly speaking, iterating that construction transﬁnitely (this is more typically
done via Zorn’s Lemma or some such maximal principle, but one can consider it
as a transﬁnite recursion). Yet the usual way of adding a single root takes one
from the ﬁeld F to a equivalence class of polynomials over F (its quotient by
the ideal generated by p). When implemented straightforwardly this might lie
two levels above F itself: if we think of elements of F as belonging to a type α
then polynomials over F might be functions N →F (albeit with ﬁnite support)
and then equivalence classes represented as Boolean functions over that type, so
we have moved to (N →F) →2. And that whole process needs to be iterated
transﬁnitely. Of course one can use cardinality arguments to choose some suﬃ-
ciently large type once and for all and map everything back into that type at each
stage. One may even argue that this gives a more reﬁned theorem with informa-
tion about the cardinality of the algebraic closure, but the value of being forced
to do so by the foundation is at best questionable.
Another limitation of the
simple HOL type system is that there is no explicit quantiﬁer over polymorphic

History of Interactive Theorem Proving
171
type variables, which can make many standard results like completeness theo-
rems and universal properties awkward to express, though there are extensions
with varying degrees of generality that ﬁx this issue [Melham, 1992; Voelker, 2007;
Homeier, 2009]. Inﬂexibilities of these kinds certainly arise in simple type theories,
and it is not even clear that more ﬂexible dependent type theories (where types
can be parametrized by terms) are immune. For example, in one of the most im-
pressive formalization eﬀorts to date [Gonthier et al., 2013] the entire group theory
framework is developed in terms of subsets of a single universe group, apparently
to avoid the complications from groups with general and possibly heterogeneous
types.
Even if one considers types a profoundly useful concept, it does not follow of
course that they need to be hardwired into the logic. Starting from a type-free
foundation, it is perfectly possible to build soft types as a derived concept on top,
and this is eﬀectively what Mizar does, arguably giving a good combination of
ﬂexibility, convenience and simplicity [Wiedijk, 2007]. In this sense, types can be
considered just as sets or something very similar (in general they can be proper
classes in Mizar). On the other hand, some recent developments in foundations
known as homotopy type theory or univalent foundations give a distinctive role to
types, treating equality on types according to a homotopic interpretation that may
help to formalize some everyday intuitions about identifying isomorphic objects.
Another interesting diﬀerence between the various systems (or at least the way
mathematics is usually formalized in them) is the treatment of undeﬁned terms
like 0−1 that arise from the application of functions outside their domain.
In
informal mathematics we often ﬁlter out such questions subconsciously, but the
exact interpretation of such undeﬁnedness can be critical to the assertion being
made. We can identify three main approaches taken in interactive provers:
• Totalization (usual in HOL) — functions are treated as total, either giv-
ing them an arbitrary value outside their domain or choosing one that is
particularly convenient for making handy theorems work in the degenerate
cases too. For example, setting 0−1 = 0 [Harrison, 1998] looks bizarre at
ﬁrst sight, but it lets us employ natural rewrite principles like (x−1)−1 = x,
−x−1 = (−x)−1, (xy)−1 = x−1y−1 and x−1 ≥0 ⇔x ≥0 without any
special treatment of the zero case. (There is actually an algebraic theory
of meadows, eﬀectively ﬁelds with this totalization [Bergstra et al., 2007].)
While simple, it has the disadvantage that equations like f(x) = y do not
carry with them the information that f is actually deﬁned at point x, ar-
guably a contrast with informal usage, so one must add additional conditions
or use relational reformulations.
• Type restrictions (usual in PVS) — the domain restrictions in the partial
functions are implemented via the type system, for example giving the inverse
operation a type : R′ →R where R′ corresponds to R−{0}. This seems quite
natural in some ways, but it can mean that types become very intricate for
complicated theorems. It can also mean that the precise meaning of formulas

172
John Harrison, Josef Urban and Freek Wiedijk
like ∀x ∈R. tan(x) = 0 ⇒∃n ∈Z.x = nπ, or even whether such a formula
is acceptable or meaningful, can depend on quite small details of how the
typechecking and basic logic interact.
• Logics of partial terms (as supported by IMPS [Farmer et al., 1990]) — here
there is a ﬁrst-class notion of ‘deﬁned’ and ‘undeﬁned’ in the foundational
system itself. Note that while it is possible to make the logic itself 3-valued
so there is also an ‘undeﬁned’ proposition [Barringer et al., 1984], this is
not necessary and many systems allow partial terms while maintaining biva-
lence. One can have diﬀerent variants of the equality relation such as ‘either
both sides are undeﬁned or both are deﬁned and equal’. While apparently
complicated and apt to throw up additional proof obligations, this sort of
logical system and interpretation of the equality relation arguably gives the
most faithful analysis of informal mathematics.
6.2
Proof language
As we noted at the beginning, one signiﬁcant design decision in interactive theorem
proving is choosing a language in which a human can communicate a proof outline
to the machine. From the point of view of the user, the most natural desideratum
might be that the machine should understand a proof written in much the same
way as a traditional one from a paper or textbook. Even accepting that this is
indeed desirable, there are two problems in realizing it: getting the computer to
understand the linguistic structure of the text, and having the computer ﬁll in the
gaps that human mathematicians consider as obvious. Recently there has been
some progress in elucidating the structure of traditional mathematical texts such
that a computer could unravel much of it algorithmically [Ganesalingam, 2013],
but we are still some way from having computers routinely understand arbitrary
mathematical documents. And even quite intelligent human readers sometimes
have diﬃculty in ﬁlling in the gaps in mathematical proofs. Subjectively, one can
sometimes argue that such gaps amount to errors of omission where the author
did not properly appreciate some of the diﬃculties, even if the ﬁnal conclusion is
indeed accurate. All in all, we are some way from the ideal of accepting existing
documents, if ideal it is. The more hawkish might argue that formalization presents
an excellent opportunity to present proofs in a more precise, unambiguous and
systematic — one might almost say machine-like — way [Dijkstra and Scholten,
1990].
In current practice, the proof languages supported by diﬀerent theorem proving
systems diﬀer in a variety of ways. One interesting dichotomy is between pro-
cedural and declarative proof styles [Harrison, 1996c]. This terminology, close to
its established meaning in the world of programming languages, was suggested
by Mike Gordon. Roughly, a declarative proof outlines what is to be proved, for
example a series of intermediate assertions that act as waystations between the
assumptions and conclusions. By contrast, a procedural proof explicitly states how
to perform the proofs (‘rewrite the second term with lemma 7 . . . ’), and some

History of Interactive Theorem Proving
173
procedural theorem provers such as those in the LCF tradition use a full program-
ming language to choreograph the proof process. To exemplify procedural proof,
here is a HOL Light proof of the core lemma in the theorem that
√
2 is irrational,
as given in [Wiedijk, 2006]. It contains a sequence of procedural steps and even for
the author, it is not easy to understand what they all do without stepping through
them in the system.
let NSQRT_2 = prove
(‘!p q. p * p = 2 * q * q ==> q = 0‘,
MATCH_MP_TAC num_WF THEN REWRITE_TAC[RIGHT_IMP_FORALL_THM] THEN
REPEAT STRIP_TAC THEN FIRST_ASSUM(MP_TAC o AP_TERM ‘EVEN‘) THEN
REWRITE_TAC[EVEN_MULT; ARITH] THEN REWRITE_TAC[EVEN_EXISTS] THEN
DISCH_THEN(X_CHOOSE_THEN ‘m:num‘ SUBST_ALL_TAC) THEN
FIRST_X_ASSUM(MP_TAC o SPECL [‘q:num‘; ‘m:num‘]) THEN
POP_ASSUM MP_TAC THEN CONV_TAC SOS_RULE);;
By contrast, consider the following declarative proof using the Mizar mode for
HOL Light [Harrison, 1996b], which is a substantial fragment of the proof of the
Knaster-Tarski ﬁxed point theorem [Knaster, 1927; Tarski, 1955].16 There is not
a single procedural step, merely structuring commands like variable introduction
(‘let a’) together with a sequence of intermediate assertions and the premises
from which they are supposed (somehow) to follow:
consider a such that
lub: (!x. x IN Y ==> a <= x) /\
(!a’. (!x. x IN Y ==> a’ <= x) ==> a’ <= a)
by least_upper_bound;
take a;
!b. b IN Y ==> f a <= b
proof
let b be A;
assume b_in_Y: b IN Y;
then L0: f b <= b by Y_thm;
a <= b by b_in_Y, lub;
so f a <= f b by monotonicity;
hence f a <= b by L0, transitivity;
end;
so Part1: f(a) <= a by lub;
so f(f(a)) <= f(a) by monotonicity;
so f(a) IN Y by Y_thm;
so a <= f(a) by lub;
hence thesis by Part1, antisymmetry;
16See http://code.google.com/p/hol-light/source/browse/trunk/Examples/mizar.ml.

174
John Harrison, Josef Urban and Freek Wiedijk
A more declarative style can oﬀer signiﬁcant advantages.
Declarative proof
scripts are generally easier to write and understand independent of the prover,
whereas one usually needs to develop, or even understand, a procedural script by
running it step-by-step through the system to see the intermediate results. (By
analogy, consider replaying a chess game from a newspaper chess column given
just the sequence of moves with no diagrams of the board state.) Because they
lack explicit inference instructions, declarative proofs are also usually less tied to
the details of the prover’s implementation and so are likely to be more readable
for non-experts and portable in a general sense. On the other hand, declarative
proofs can be clumsy and verbose when they involve large and complex terms or
are naturally expressed as a simple process of transformation, something that is
particularly common in veriﬁcation applications. As for making modiﬁcations to
an existing proof, which is often an important activity [Curzon, 1995], there are
pluses and minuses on both sides. In a declarative proof, key variable introductions
and intermediate statements are made explicit instead of arising as a side-eﬀect
of some intermediate step, and are more amenable to systematic changes. Some
existing experience [Chen, 1992; Gonthier, 1996] supports the declarative style as
yielding proofs that are easier to maintain. On the other hand, some procedural
proofs can be surprisingly insensitive to small changes and may even work with
no changes at all. These questions are surveyed in a bit more detail in [Harrison,
1996c].
At one extreme, the most declarative proof language is the one supported by
a completely automated theorem prover: just state the theorem to be proved
without any hint as to how to prove it! Of course, the whole point of interactive
theorem proving is to allow user guidance, but some theorem provers that we
have classiﬁed as interactive do indeed use this approach, the only diﬀerence being
that the user must identify a series of intermediate lemmas that can be stepping-
stones to the main result such that the gap between successive steps is within
the scope of the automation. In particular, proof scripts in NQTHM and ACL2
are usually just sequences of lemmas without much procedural information. The
primary additional data is that for each lemma, the user may add hints about
how the automation should use that lemma in the future (rewrite, elimination,
generalization or induction lemma).
Mainstream mathematics normally uses much richer quantiﬁer structures than
are possible (or at least conveniently usable) in NQTHM/ACL2. This can make it
less appealing to use a simple series of toplevel lemmas as a proof outline, because
one often wants to structure the proof according to the introduction and elimi-
nation of variables and localize reasoning to some assumed environment. Several
theorem proving systems and program veriﬁers adopted a structured style of proof
close to natural deduction, organized around variable elimination and introduction
rules. Consider for example the following sample proof from NuPRL’s precursor,
the program veriﬁer PL/CV [Constable and O’Donnell, 1978]:
LEMMA_T:
/# TRANSITIVITY OF DIVIDES #/

History of Interactive Theorem Proving
175
ALL (A, B, C) FIXED . (DIV(A,B) & DIV(B, C) => DIV(A, C))
BY INTRO,
PROOF;
CHOOSE M1 FIXED WHERE M1*A = B;
CHOOSE M2 FIXED WHERE M2*B = C;
M2*(M1*A) = C;
/# BY SUBSTITUTION #/
(M2*M1)*A = C;
/# BY ASSOCIATIVITY OF * #/
DIV(A,C) BY SOMIN, M2*M1;
QED;
Exactly such a structured approach was used by Mizar, inspired not only by
the Ja´skowski-Fitch approach to natural deduction [Ja´skowski, 1934; Fitch, 1952]
but also by the block structure of the Pascal programming language [Jensen and
Wirth, 1974]. As the examples given above show, the proof is structured around
a ‘skeleton’ indicating the introduction and elimination of variables and assump-
tions, and then the intermediate steps are just stated as assertions, without any
procedural information. In this sense Mizar’s proof style successfully combined
structure and a declarative style. (By contrast, the PL/CV example does have
some explicit inference rules and instantiation, although they are only used in one
line of the above example.)
The relative readability of Mizar proofs contrasts quite starkly with the ob-
scurity of many procedural languages, particularly the traditional tactic scripts
in LCF-style systems with their invocations of mysterious and arcane transfor-
mations, once parodied by Conor McBride as EAR_OF_BAT_TAC. Because of this,
there has been considerable interest in supporting more declarative proof styles in
other systems. This started with the ‘Mizar mode for HOL’ [Harrison, 1996b] and
was followed by several other declarative languages for other systems [Syme, 1997;
Wenzel, 1999; Zammit, 1999]. In particular the Isar language for Isabelle, already
discussed above, has now largely superseded the use of the traditional ML level, al-
though the Isar proofs are usually a mix of a structured skeleton with intermediate
proof steps, making them highly structured but only partly declarative. Indeed,
it is natural to desire a smooth combination of both procedural and declarative
approaches. A number of experiments in HOL Light have resulted in quite usable
systems [Wiedijk, 2001; Wiedijk, 2012b] that have been subsequently applied and
reﬁned by Bill Richter.17
The ability to program other proof styles like ‘Mizar mode’ in LCF systems
shows that although the traditional style of such systems is highly procedural, one
can layer virtually any proof style on top. This applies even if one wants a purely
procedural language that diﬀers from the full implementation language. Among
procedural systems, the traditional LCF approach where the implementation lan-
guage read-eval-print loop is the primary interface stands at one end of a spectrum,
with very simple macro languages at the other. A high level of programmability
17See
http://code.google.com/p/hol-light/source/browse/trunk/
RichterHilbertAxiomGeometry/HilbertAxiom_read.ml

176
John Harrison, Josef Urban and Freek Wiedijk
can be extremely valuable, for example in allowing simple custom inference rules
to be implemented, or a slew of related subgoals to be disposed of by a single
script.
In order to maintain programmability and ﬂexibility while keeping the
language somewhat more constrained — for example to close oﬀobscure language
loopholes, enforce a more uniform style, or allow more straightforward parsing and
processing by other tools — there is a good case for adopting some intermediate
position on this spectrum. The main Ltac language of Coq is a good example
[Delahaye, 2000], as is the more recent SSReﬂect language.
Even if one has a declarative style as the intention, it can be diﬃcult to avoid
explicit invocations of proof commands unless there is a kind of default automa-
tion that is rather powerful. Mizar’s built-in checker has a deliberately constrained
ﬁrst-order prover, though it also includes other features like congruence closure,
representing a certain point of view on what constitutes an obvious inference [Rud-
nicki, 1987b]. This prover is simple and eﬃcient, but quite limited compared to
state-of-the-art automated systems [Wiedijk, 2000]. Thus, to support a suitably
high-level declarative proof style, automation is an important component, and we
turn to this question next.
6.3
Automation and certiﬁcation
The convenience and practicality of interactive proof greatly increases if the sys-
tem is able to automate as much routine work as possible. Given the extensive
development of automated methods for propositional logic, ﬁrst-order logic, arith-
metic etc. and special tools like model checkers and computer algebra systems, it
is natural to want to use some of the same ideas in interactive tools, and perhaps
even use automated systems themselves as subcomponents.
It is usually not too diﬃcult to identify appropriate subsets of the logic sup-
ported by an interactive prover with those automated by special tools like SAT
solvers. Once this is done, it is usually feasible to to implement similar algorithms
oneself, and often much easier and faster to use highly engineered oﬀ-the-shelf
tools themselves as subroutines. For example [Seger and Joyce, 1991] and [Rajan
et al., 1995] describe eﬀective combinations of theorem provers and model checkers.
However, even if one makes the eﬀort to maintain full control over the implemen-
tation, it can be diﬃcult to get such complex algorithms right, so there is a danger
of compromising the high standards of rigour. Milner [MacKenzie, 2001] compared
using an unveriﬁed decision procedure to ‘selling your soul to the Devil [. . . ] you
get this enormous power [. . . but] you’ve lost proof in some sense’. Since many
share these qualms, though they might not express them so forthrightly, there has
been considerable interest in the implementation of automation in a highly reliable
way. We can identify three main approaches:
• Fully-expansive algorithm design: rewrite the algorithm to perform inference
at each step and carry through formally proven theorems.

History of Interactive Theorem Proving
177
• Reﬂection: use the theorem proving tool to prove the correctness of the new
proof procedure’s implementation and only then include it in the trusted
code base.
• Certiﬁcation: organize the algorithm so that it produces not only a result
but some kind of ‘proof’ or ‘certiﬁcate’ that can be independently checked.
The ﬁrst approach, fully expansive recoding of the algorithm, is the traditional
LCF answer to most such problems. A long-established methodology — see [Mel-
ham, 1989] for early non-trivial examples — means that it is often a fairly routine
matter to translate many symbolic algorithms into inference-producing versions,
for example replacing each ad-hoc term transformation with a conversion as out-
lined above. A good example of such methods at work is the implementation by
Slind [1991] of Knuth-Bendix completion in HOL. Other methods are even easier
to integrate into an LCF-style system because they mainly involve heuristics and
other techniques for putting together existing rules or tactics — this applies for ex-
ample to Boyer-Moore automation of induction and related techniques like proof
planning, which have indeed been implemented in LCF-style provers [Boulton,
1992; Dixon, 2005; Papapanagiotou, 2007].
One reason why this fully-expansive inference-producing style is more practical
than might appear at ﬁrst sight is that to implement many derived rules, all the
complicated reasoning can be embedded in a single ‘proforma theorem’. To take
a trivial example, the fact that from p ∧q we can deduce p can be embedded in
the theorem ⊢p ∧q ⇒p. Now in any particular instance ⊢a ∧b, we need only
instantiate this theorem, to get ⊢a ∧b ⇒a, and perform Modus Ponens and get
⊢a. Of course, this is a trivial example but one can embed much more interesting
reasoning in a single theorem that later merely needs to be instantiated.
In more elaborate cases, it may even be worth deﬁning some special syntactic
forms inside the logic so that the workings of the algorithm can be expressed more
directly via general proforma theorems, at the cost of some folding and unfolding of
equivalent forms to apply it to concrete cases. (This is setting up a deep embedding
of a sublogic, in the terminology we discuss near the end of this paper.)
For
example, in the implementation of Cooper’s algorithm [Cooper, 1972] for integer
quantiﬁer elimination in HOL Light, a kind of ‘shadow syntax’ is deﬁned for a class
of quantiﬁer-free ﬁrst-order formulas inside the logic, with their semantics deﬁned
using an interpretation function interp. The key syntactic transforms involved
can then be expressed as a simple equivalence in interpretations of the shadow
syntax. Now, in order to eliminate a quantiﬁer in HOL from an expression ∃x.P[x],
one ﬁrst rewrites backwards with the deﬁnition of interp to map it into a formula
in the canonical form ∃x.interp x p, appeals to the general theorem to transform
it into a quantiﬁer-free equivalent, then rewrites forward with the deﬁnition of
interp to eliminate the internal syntax. In general, we can justify a transformation
of some formula ψ to another one F(ψ) via a formalized transformation on the
syntactic form ⌜ψ⌝(see Figure 5).
Taking the ‘proforma theorem’ approach to its logical extreme, one can formally

178
John Harrison, Josef Urban and Freek Wiedijk
ψ
F(ψ)
⌜ψ⌝
⌜F(ψ)⌝
✲
✛
✻
✻
Semantics to syntax
Syntax to semantics
F
Syntactic
transform
Figure 5: Using ‘reﬂection’ inside the logic
deﬁne the entire workings of an algorithm as operations on the embedded ‘shadow
syntax’ inside the logic.
Most algorithms that can be written in a functional
language can also be developed entirely inside the logic in this way. Of course
the process of ‘executing’ inside the logic involves inference steps, meaning that is
likely to be substantially slower, perhaps only by a constant factor but most likely
a substantial one [McLaughlin and Harrison, 2005].
However, Coq features a
highly eﬃcient reduction mechanism that comes close in performance to a native
functional language, so provided the shadow syntax lives in the right subset, it
can be executed quite fast. On the other side of the coin, conventional inference
in Coq is relatively slow and memory-hungry because it actually creates explicit
proof objects. All this means that the beneﬁts of the ‘shadow syntax’ approach,
commonly called ‘reﬂection’ in the Coq world, are much more compelling even for
relatively simple algorithms.
More generally, we use ‘reﬂection’ to refer to any scheme where one is basi-
cally ‘verifying code and executing it’.
In most established examples like the
pioneering use of metafunctions in NQTHM [Boyer and Moore, 1981], the code
is actually extracted to a conventional programming language rather than, as
in Coq, executed inside the logical kernel.
(On the other hand for ACL2 this
distinction essentially does not exist.)
A nexus of distinct but related ideas
often going under the name of ‘reﬂection’ have been tried in theorem proving
systems like FOL/GETFOL [Weyhrauch, 1980; Weyhrauch and Talcott, 1994;
Weyhrauch, 1982] and NuPRL [Knoblock and Constable, 1986; Allen et al., 1990;
Howe, 1992]. These ideas are surveyed in more detail in [Giunchiglia and Smaill,
1989; Harrison, 1995b]. Signiﬁcant recent examples of some classical logical deci-
sion procedures implemented using reﬂection include [Chaieb and Nipkow, 2008]
and [Cohen and Mahboubi, 2010].
In many important cases, there is a simpler approach to implementing correct
proof procedures: have the proof procedure produce some kind of certiﬁcate that
can be checked relatively simply and eﬃciently by proof. The general merit of this

History of Interactive Theorem Proving
179
kind of approach — not limited to foundational theorem proving — was empha-
sized by Blum [1993]. He suggests that in many situations, checking results may
be more practical and eﬀective than verifying code. A very simple case is verifying
that a number is not prime, which can be done easily, even by logical inference,
given a factorization as the certiﬁcate, even if ﬁnding that certiﬁcate is diﬃcult.
An early example of the certiﬁcation approach in our context is the linkup between
the HOL theorem prover and Maple computer algebra system reported by Har-
rison and Th´ery [1998]. Here Maple is used to perform polynomial factorization
and transcendental function integration. In each case the checking process (re-
spectively multiplying polynomials and taking derivatives) is substantially easier
than the process of ﬁnding the certiﬁcate. And note that by taking this path we
are often able to inherit the extensive implementation and optimization work done
by others on the external tools without any additional work on our own part.
In the cases considered so far (integer and polynomial factorization and an-
tiderivatives) the certiﬁcate is just what one might intuitively call ‘the answer’.
Sometimes, though, it is useful to have a more information in the certiﬁcate in
order to verify it without an expensive and complicated search process.
This
applies in particular to various decision procedures for quantiﬁer-free ﬁrst-order
arithmetic theories. These can provide relatively compact certiﬁcates that can be
checked reasonably easily. For example, an invalid conjunction of linear arithmetic
constraints can be checked by providing a linear combination that sums to give a
trivial inequality like 1 < 0; the existence of such linear combinations is essentially
the content of Farkas’s lemma [Webster, 1995]. This method was ﬁrst used in a
formal context by Boulton [1993] in a linear arithmetic procedure for HOL, using
his own implementation of the certiﬁcate-ﬁnding, and by Necula (in connection
with proof-carrying code), using an oﬀ-the-shelf linear programming package to
ﬁnd the certiﬁcate [Necula and Lee, 2000]. It has reached its apotheosis in the work
of Alexey Solovyev [Solovyev and Hales, 2011], who has checked the very large lin-
ear programs in the Flyspeck project (discussed in more detail later) inside HOL
Light using such certiﬁcation, remarkably eﬃciently. Analogous techniques based
on the Hilbert Nullstellensatz can handle the universal theory of integral domains
or ﬁelds (see [Harrison, 2009b] for a detailed discussion), and this ﬁrst done in our
context in [Harrison, 2001]. A similar but more complicated technique works for
real-closed ﬁelds like the real numbers using certiﬁcates involving sums of squares
that can be found using semideﬁnite programming tools [Parrilo, 2003], and this
has also been exploited for formal proof [Harrison, 2007]. This is perhaps best
illustrated by a speciﬁc example. Suppose we want to show that if a quadratic
equation has a (real) solution, its discriminant is nonnegative:
∀a b c x. ax2 + bx + c = 0 ⇒b2 −4ac ≥0
A suitable certiﬁcate is b2 −4ac = (2ax + b)2 −4a(ax2 + bx + c). Since the
ﬁrst term on the right is a square, and the second is zero by hypothesis, it is clear
that the LHS is nonnegative. Almost all the conceptual/implementation diﬃculty
and computational cost is in coming up with the right algebraic rearrangement;

180
John Harrison, Josef Urban and Freek Wiedijk
checking this and the consequent reasoning is then easy.
In other cases like ﬁrst-order theorem proving, successful runs of the prover
may perform a lot of search, but usually ﬁnd a relatively short proof. Provided
the automated system is indeed able to produce the ﬁnal proof, it is in principle
a fairly straightforward matter to check it by inference in the ITP system. This
approach has been used for a long time to incorporate ﬁrst-order proof methods
into LCF-style provers [Kumar et al., 1991], ﬁrst of all with custom code to ﬁnd the
proof and later with oﬀ-the-shelf external provers [Hurd, 1999]. This brings with it
a host of apparently minor but sometimes quite knotty problems. First of all, one
is usually interested in using provers for pure ﬁrst-order logic to tackle problems
in ITPs with richer logics like polymorphically typed higher-order logic, which
raises interesting choices about how to relate the two worlds [Harrison, 1996b;
Dahn and Wernhard, 1997; Hurd, 2003; Meng and Paulson, 2006; Blanchette et al.,
2013]. Many oﬀ-the-shelf ﬁrst-order provers are not particularly good at returning
proofs, sometimes necessitating quite elaborate programming to reconstruct them.
And typically, oﬀ-the-shelf systems have a bias towards relatively small synthetic
problems and can be swamped if they are presented with a large database of
lemmas to use, making premise selection an issue.
The ﬁrst convincing arrangement that successfully addresses all these problems
is the ‘Sledgehammer’ framework for Isabelle developed by Paulson [Paulson and
Blanchette, 2010]. For many Isabelle users this has led to a distinct change in
their approach to theorem proving, tending to let the automation plug the gaps
eagerly, even working in the background while the user thinks.
Traditionally,
LCF systems have tended to encourage tight and careful control over the proof
process, but as a result of the success of Sledgehammer, there is now a trend
among Isabelle users at least back to the earlier automated approaches — for
example, Ontic [McAllester, 1989] already anticipated the eﬀective automated use
of a large background database. Related ‘hammer frameworks’ for HOL Light and
Mizar oﬀering a variety of premise selection schemes [K¨uhlwein et al., 2012] based
on machine learning and allowing use of a central server over the network, are
described by Kaliszyk and Urban [2014a] and Urban et al. [2013].
Quite generally, the current trend in many Artiﬁcial Intelligence domains (for
example, translation between languages) is to use general machine learning on huge
datasets [Shawe-Taylor and Cristianini, 2004] in preference to intricately hand-
crafted algorithms. In the light of this the relative lack of such techniques in the
world of theorem proving seems surprising. Apart from the premise selection task,
Urban, Schulz, Bridge, Kaliszyk, K¨uhlwein and others have recently used machine
learning for selecting suitable theorem-proving strategies [K¨uhlwein et al., 2013;
Schulz, 2002; Bridge et al., 2014] and automated construction of such strategies
[Urban, 2014] over large sets of related problems, mining the large inference graphs
of ITP systems for suitable lemmas and conjectures [Kaliszyk and Urban, 2014b;
Denzinger and Schulz, 1996], and ﬁne-grained guidance of the automated theorem
provers [Urban et al., 2011; Schulz, 2000]. Combinations of such machine learning
systems with automated theorem provers on the large ITP libraries may result in

History of Interactive Theorem Proving
181
AI-style feedback loops, where the learning and the proving components gradu-
ally improve from the data supplied by the other component [Urban et al., 2008].
Note that since these methods usually contribute high-level guidance such as a
set of lemmas, it’s generally straightforward to integrate them into fully-expansive
systems without any issues. And they greatly beneﬁt from training on the large li-
braries of formal mathematics that are associated with interactive systems. Thus,
somewhat paradoxically, machine learning impinges at least as eﬀectively on in-
teractive theorem proving as traditional automated proving.
As with ﬁrst-order provers, many SAT solvers are capable of emitting proofs
(usually recast as resolution proofs even if that doesn’t reﬂect how they were
found), which have applications not only in proof-checking but for invariant gen-
eration via interpolation [McMillan, 2003]. Unlike FOL proofs, SAT proofs tend
to be relatively large, but nevertheless it has turned out to be possible to check
them in a fully expansive way in a time comparable to the time used to generate
them [Weber and Amjad, 2009]. Combined decision procedure suites can also be
checked in a fully-expansive way; this was ﬁrst done by Boulton for his own imple-
mentation [Boulton, 1993] and then for an oﬀ-the-shelf SMT system by Mclaughlin
et al. [2005]. However, the case of quantiﬁed Boolean formulas (QBF) appears
to be a less favorable one where the proof reconstruction in a formal prover can
be considerably more time-consuming than the process of ﬁnding it [Weber, 2010;
Kun˘car, 2011; Kumar and Weber, 2011].
Finally, we noted above the relative triviality of proving non-primality. The
dual problem of proving primality doesn’t admit quite such a straightforward cer-
tiﬁcation, but there are known certiﬁcates of primality that are usable for this
purpose, the ﬁrst to be presented being due to Pratt [Pratt, 1975]. At the time,
the key interest of Pratt’s observation was to establish that primality testing is in
the complexity classes NP and co-NP. Though primality testing was much later
established to be in P [Agrawal et al., 2004], Pratt’s result retains its interest as
an eﬀective way of certifying primality. Caprotti and Oostdijk [2001] ﬁrst imple-
mented primality proving in Coq using an optimized Pocklington variant of Pratt
certiﬁcates. Proving primality of p in this way requires at least a partial prime
factorization of p −1 and hence recursive proofs of primality of those factors;
these certiﬁcates can be generated without proof using sophisticated oﬀ-the-shelf
factorization software and checked by applying Pocklington’s theorem. Much sub-
sequent work has optimized the implementation in Coq and extended this to more
sophisticated primality-proving methods based on elliptic curves [Th´ery and Han-
rot, 2007]. While this has mainly been pursued for its pure intellectual interest and
as a motivation for optimizing basic operations inside Coq [Gr´egoire et al., 2006],
proving the primality of speciﬁc numbers can actually have genuine applications
in veriﬁcation [Harrison, 2003].

182
John Harrison, Josef Urban and Freek Wiedijk
6.4
Sharing
This chapter has repeatedly hinted at the variety of diﬀerent proof assistants, often
with radically diﬀerent foundations. On the positive side this means that there
is diversity of experience in using various systems in applications, which helps us
to better understand the strengths and weaknesses of approaches. However, given
the overhead in learning to use even one of these systems eﬀectively, there is a
tendency for researchers to get trammelled into using just one system, as a result
of which essentially similar work gets duplicated many times. It would obviously
be appealing to have some practical way of sharing work among diﬀerent systems.
One approach, arguably the simplest, is to import theorems from one system
to another without in any sense attempting to check their correctness, though
perhaps tagging them in some way to indicate their provenance. If we assume that
we are importing from a similarly reliable tool, then such results should have been
checked at a similar level of rigour. The primary diﬃculty is ensuring a meaningful
semantic match between the two systems, i.e. making sure that precise deﬁnitions,
types, treatment of partial functions etc. in the source system are compatible with
the target. This tends to work better when the target system’s logic is at least as
rich in some sense, so that there are indeed faithful models of the source results
on the target side, whatever constructs get used there. The ﬁrst such example
was the pioneering work of Felty and Howe [Felty and Howe, 1997] on importing
mathematics from HOL to NuPRL. Other more recent examples have included
the import from ACL2 into hol90 [Staples, 1999] and HOL4 [Gordon et al., 2006].
Still more challenging is importing not only statements but actually reading in
and checking proofs in the target system. Given the strong tendencies to foun-
dational precision and rigour in this research community, this has attracted more
attention recently. Examples include
• hol90 →Coq [Denney, 2000]
• hol90 →NuPRL [Naumov et al., 2001]
• HOL4 →Isabelle/HOL [Obua and Skalberg, 2006]
• HOL Light →Isabelle/HOL [Obua and Skalberg, 2006; Kaliszyk and Krauss,
2013]
• Isabelle/HOL →HOL Light [McLaughlin, 2006]
• HOL Light →Coq [Keller and Werner, 2010]
The work of McLaughlin [2006] on importing Isabelle/HOL to HOL Light shows
that the general requirement that the target system should be at least as rich as
the source is not an absolute prohibition. Isabelle/HOL extends the simple type
theory of HOL with a system of axiomatic type classes, so in some sense is richer.
McLaughlin handles this by mapping an Isabelle theorem to an ML functor that
can be instantiated to produce speciﬁc instances.

History of Interactive Theorem Proving
183
Translation between systems like HOL4, HOL Light and ProofPower is particu-
larly appealing since despite their diﬀerences as systems they implement eﬀectively
the same logic. (They do not have exactly the same primitive rules, but they have
the same provable theorems and it is easy to implement the primitive rules of
one in terms of those of another.) OpenTheory [Hurd, 2010] is a general frame-
work designed to support the transfer of theorems and proofs between HOL family
provers, while HOL Zero [Adams, 2010] is a specially simple and transparent ver-
sion of HOL designed as a vehicle for proof import and checking.
Yet another alternative is still to transfer results without proofs but to have
a machine-formalized argument about why the two systems correspond. In this
case it would be natural to have a simple theorem proving system to act as a
metatheory to analyze other systems, perhaps proving relative consistency results
etc. and so justifying the importing of results from one to another. Just such a
general scheme was proposed as a solution to the current Tower of Babel in the
ambitious ‘QED manifesto’ [Anonymous, 1994]. Perhaps the closest concrete work
is in the Logosphere project,18 using Twelf as the metalogic.
6.5
User interfaces, search and presentation tools
Interacting with theorem provers and their large formal libraries combines aspects
of programming, writing mathematical papers and system speciﬁcations, and also
aspects of managing and searching large encyclopedias. A comprehensive overview
of the early ITP interfaces and of the human-computer interaction (HCI) research
[Hewett, 1992] in the ITP world is given by Aitken et al. [1998], focusing on HOL
and LCF-style systems, and diﬀerentiating three views of formal proof: (i) proof
as programming as for example the tactical programming in the LCF world, (ii)
proof by pointing as proposed by Bertot et al. [1994] for selecting subexpressions of
the current goal (typically, using a mouse), and proof as structure editing used in
the ALF system for editing the proof objects [Magnusson and Nordstr¨om, 1993].
A number of these ideas have been incorporated in the Proof General Emacs
interface by Aspinall [2000], which has dominated proof development in Coq and
Isabelle for over a decade. More recent widely used non-Emacs interfaces to Coq
and Isabelle include the GTK-based CoqIDE [Bertot and Th´ery, 1998], the web-
based ProofWeb19 by Kaliszyk [2007] and the Isabelle/jEdit IDE by Wenzel [2012].
The traditional mode of interaction in such interfaces for LCF-style systems has
been coupled with the read-eval-print loop of the underlying interpreter-like ITPs,
using region-locking corresponding to the processed part of the formal article,
together with forward and backward (undo/reload) commands. For the compiler-
style Mizar system, the Emacs user interface [Urban, 2006a] instead processes
the whole article at one go (similarly to some IDEs for TEX and for compiled
languages like C and Pascal), directly putting the error messages afterwards into
the edited buﬀer. This relies on the general speed of the Mizar veriﬁer, and also on
18http://www.logosphere.org/
19http://proofweb.cs.ru.nl/

184
John Harrison, Josef Urban and Freek Wiedijk
mechanisms that speed up the veriﬁcation by omitting the parts that had already
been veriﬁed and by parallelizing the veriﬁcation process [Urban, 2012]. An early
example of a rapprochement between these two user-interface approaches is tmEgg
[Mamane and Geuvers, 2007] – a document oriented Coq plugin for TEXMacs
that allows more liberal editing mode (inspired by LATEX) than Proof General and
CoqIDE. Similar document-centric eﬀort is done in the Isabelle/JEdit plugin.
As in programming and scientiﬁc writing, the authoring can be done with diﬀer-
ent degrees of collaboration: there are essentially one-person projects such as HOL
Light and its core libraries, but also widely distributed loosely managed projects
such as the construction of the Mizar Mathematical Library. A number of projects
are in between these two extremes. In the Flyspeck project, the formal proofs of
a large part of the formalization outlined by Tom Hales have been carried out
by a group of mathematicians in Hanoi. Another example of such collaborative
project with a strong leader is the formal proof of the Feit-Thompson theorem,
with Georges Gonthier proposing and controlling the overall formalization plan,
the proof style, the integrity of concept and theorem naming, the automation
methods used, etc. While in the one-person projects, most of the library devel-
oped is typically remembered by its author, this is no longer the case with large
collaborative projects where library re-use turns into a major issue. This has led to
the development of a number of search facilities. The most obvious and still widely
used search method is just grep (regular expression search), which can be further
modiﬁed in various ways: for example searching the whole multiline statements
and searching the formalizations in their library-processing order has turned quite
useful when working with the Mizar library [Urban, 2006a]. Regular expressions
however have only limited knowledge about the term and type structure, symbol
overloading, and other ITP speciﬁcs.
The next level of ITP search tools is typically aware of such issues, providing
more semantic ways for specifying the search patterns. In HOL, Isabelle and Coq
this includes tools such as find_theorems and SearchAbout, provided directly
by the interactive shell of these systems and well-integrated with the underlying
datastructures used for representing terms, formulas and theories. A potential dis-
advantage of such deeply integrated tools is that they usually work only with the
theories loaded by the user into the current interactive session, possibly omitting a
number of other developments and libraries because the user does not know about
them, or because they require additional eﬀort to load due to various incompat-
ibilities. This has led to the development of tools that are both globally usable
across all developments (in the same way as grep is) and ‘semantic’, i.e., allow-
ing the ITP-speciﬁc search patterns. Such tools include Bancerek’s MML Query
[Bancerek and Rudnicki, 2003; Bancerek, 2006], which processes and searches the
whole Mizar library and all its versions, and the Whelp system [Asperti et al.,
2004] developed at University of Bologna for searching Coq libraries.
Such semantic search tools are already close to the present automated theorem
proving linkups for Isabelle, Mizar and HOL Light mentioned above. Where such
full-scale proof ﬁnding turns out to be too hard, the use of ATP-indexing methods

History of Interactive Theorem Proving
185
such as perfect discrimination trees is still possible for performing more restricted
search such as type-aware subsumption over the millions of lemmas in the whole
ITP libraries [Urban, 2006b]. Also, the machine learning premise-selection meth-
ods, used today mostly in the context of large-theory ATP, can be used as separate
search tools for the libraries and integrated into the authoring interfaces. An early
example is the Mizar Proof Advisor [Urban, 2006a]. All such ‘global’ tools are
typically external to the ITP systems and often work in a server mode over an
internet connection, usually providing further presentation capabilities.
This introduces another large topic, which is presentation of the ITP devel-
opments, targeting not just their authors (writers), but a much wider audience
interested in their possible re-use, or just in the study of fully formal proofs.
Particularly with the arrival of HTML and the World Wide Web in the 1990s,
formal mathematical libraries, where the meaning of each symbol and proof step
is completely disambiguated, seemed to be a strong candidate for general pre-
sentation of mathematics in a cross-linked HTML-ized form where learning the
exact deﬁnition of a particular concept is just one click away.
The ﬁrst large
project that took advantage of HTML-based cross-linking was the Journal of
Formalized Mathematics20 started in 1995, presenting the Mizar abstracts (i.e.,
with proofs omitted) in a completely cross-linked HTML format. Similar cross-
linking is today available for Coq21 (using the coqdoc tool) and for MetaMath
[Megill, 1996] developments.22
Practically all large ITP libraries, such as the
Isabelle Archive of Formal Proofs23, the Coq Users’ Contributions24, and the
HOL Light, Flyspeck and HOL4 libraries, are easily accessible on the web to-
day. The HTML presentations have gradually acquired more features than just
symbol-linking: particularly useful is the optional display of the proof state (the-
sis) after each proof step in tools like Proviola [Tankink et al., 2010] and in
the more recent Mizar HTML25 [Urban, 2005], linking with online interfaces
to the ITPs and ATPs such as ProofWeb and MizAR26 [Urban and Sutcliﬀe,
2010], experimental wiki platforms for formal mathematics [Urban et al., 2010;
Alama et al., 2011], and even linking with the world of the Semantic Web [Tankink
et al., 2012] which has taken oﬀin the meantime, and particularly beneﬁted from
informal resources such as Wikipedia.
Describing mathematics in Wikipedia has in some sense inherited some of the
library and encyclopedia-building spirit of the formal libraries projects, but allowed
much more massive collaboration in the informal setting, resulting in much faster
coverage and cross-linking of the mathematical landscape. In 2010 — only a decade
since the inception of Wikipedia — the number of Wikipedia mathematical articles
20http://mizar.org/JFM
21http://coq.inria.fr/library/
22http://us.metamath.org/mpegif/mmset.html
23http://afp.sourceforge.net/
24http://coq.inria.fr/contribs
25http://mizar.org/version/current/html/
26http://mizar.cs.ualberta.ca/~mptp/MizAR.html

186
John Harrison, Josef Urban and Freek Wiedijk
grew over 25,000,27 with the number of active participants in the Mathematics
WikiProject counting over 400. These numbers dwarf the long-developed formal
ITP libraries by an order of magnitude. Allowing a similarly explosive level of
collaboration and providing further alignment of such shallow semantic corpora
with the fully formal ITP libraries are again very interesting research topics. An
early eﬀort in this direction is Hales’s cross-linking of his informal LATEX book on
Flyspeck [Hales, 2012] with the formal Flyspeck development in HOL Light and a
wiki platform for such joint informal/formal alignment [Tankink et al., 2013].
6.6
Ultimate reliability
For purely automated theorem proving, the main emphasis is usually on power
and convenience. While of course reliability is always an important goal, it is not
usually at the forefront of the research agenda. In interactive theorem proving,
by contrast, we are not usually so interested in having the computer impress us
with its creativity (though we are always willing to be pleasantly surprised). The
primary goal is to rather to assist the human to construct a proof while checking
all the low-level details precisely. Because of this, reliability comes higher up the
list of desiderata, since it’s hard to justify the extra labor usually involved in
formalization if the end result is likely to be just as fallible as a human proof.
However, the degree of importance attached to reliability varies among the ITP
system communities and among individual members. In the PVS world there has
traditionally been a rather relaxed and pragmatic view of correctness, while the
HOL world has usually attached a very high level of importance to — one might
also say fetishized — secure foundations and reliability. A simpliﬁed caricature of
the case against caring might look something like this:
Even if a theorem prover does have obscure bugs that in principle af-
fect soundness, for proofs in veriﬁcation it is still orders of magnitude
more reliable than a human proof. In any case such issues pale into
insigniﬁcance compared with the very real problems of getting speci-
ﬁcations right and correctly modeling the system, where a small error
can make any ‘correctness proof’ meaningless. Veriﬁcation is almost
always more valuable for discovering bugs rather than producing some
nebulous ‘guarantee’, and a prover that lets you ﬁnd interesting bugs
quickly is the most useful, even if it has issues of its own.
As the reader may guess, we do not entirely accept this critique. In any case,
there is surely considerable intellectual interest in seeing how far a rigorously foun-
dational approach can be pushed. So, without necessarily taking any particular
position on this issue, we will just consider how systems may be made highly re-
liable if one does indeed care enough for it to be an issue. First of all, we can
identify two major sources of unreliability:
27https://web.archive.org/web/20101222014223/http://en.wikipedia.org/wiki/Portal:
Mathematics

History of Interactive Theorem Proving
187
• The logic implemented by the theorem prover or the mathematical axioms
assumed may be unsound or even inconsistent.28
• The actual implementation of the logical system as computer code may be
incorrect, or the implementation may include additional infrastructure like
arithmetic decision procedures not part of the abstract description of the
logic.
We should not ignore the ﬁrst possibility.
Many notable logicians including
Frege, Curry and Martin-L¨of have proposed logical systems that turned out to be
inconsistent for fundamental conceptual reasons. And even starting from a valid
semi-formal idea, specifying the details of a logical system can be a painstaking and
error-prone activity where even famously careful and precise workers like Church
have erred — problems with variable capture are a perennial source of errors. Such
errors seem fairly unlikely using relatively simple and time-tested foundations like
ﬁrst order set theory or HOL-style simple type theory, and more likely when
the foundations are newer and more complex. Yet even in a system as simple as
HOL, early versions had a conceptual error in the deﬁnitional mechanism (allowing
polymorphic type variables occurring in the deﬁniens but not the constant) that
led to their being inconsistent, as was later discovered independently by Roger
Jones and Mark Saaltink.
Nevertheless, we will concentrate mainly on errors of the second kind. Since se-
rious proof checkers are large and complex systems of software, skepticism about
their correctness is certainly reasonable. In general, it is much easier to feel conﬁ-
dent about a theorem proving program that has a relatively small trusted kernel
so that correctness of this kernel is all that needs to be established. Type theory
provers are often organized according to what has been called the de Bruijn cri-
terion [Barendregt, 1997]: they can output a proof that is checkable by a much
simpler checker program that can be considered to be the logical kernel. LCF
systems of course already do perform all inference using a small trusted kernel,
and they satisfy the de Bruijn criterion into the bargain because it is very easy
to instrument the kernel so that it actually records proofs that can be separately
checked [Wong, 1993]. Most of the experiments on sharing proofs among diﬀer-
ent systems outlined above depend on exactly this kind of proof export, and such
checking using another system provides an additional level of conﬁrmation even
beyond the rigor of the native LCF kernel. Thus, systems that are architected
around logical kernels, and those in the LCF style in particular, can reasonably
be considered more reliable than those with freer design principles, other things
being equal.
But of course, the relative size and complexity of the kernels is a signiﬁcant
factor in reliability. For example, HOL Light [Harrison, 1996a] has a logical kernel
28Since G¨odel we have known that a system can be consistent yet unsound — for example add
to a consistent system an axiom asserting the inconsistency of that system. We will just refer
vaguely to this nexus of concepts when we use words like ‘sound’, ‘correct’ or ‘reliable’.

188
John Harrison, Josef Urban and Freek Wiedijk
consisting of about 600 lines of mostly functional OCaml, and the most complex
inference rule (type instantiation, INST_TYPE) can be described as
Γ[α1, . . . , αn] ⊢p[α1, . . . , αn]
Γ[γ1, . . . , γn] ⊢p[γ1, . . . , γn]
On the other hand, Coq’s logical kernel consists of about 20,000 lines of code,
sometimes quite stateful and with some 2,500 of them being in C, one of the more
complex rules (K-match) being the following29
Even if the system demonstrably only produces valid theorems, there is the
danger that humans can misunderstand the appearance of those theorems. First
of all, one can be confused over the precise deﬁnitions involved, particularly if
they have some less intuitive features like aggressive totalization, though of course
all systems make it easy to inspect those deﬁnitions. But for many purposes the
designers of theorem provers tend to think at the level of abstract syntax and
neglect the concrete representation that the user sees. From a practical point of
view, one might consider all this as part of the logical kernel, since not many
users in practice would be willing to read the abstract syntax trees.
As such,
even nominally reliable systems can produce quite confusing and counterintuitive
results, as Adams [2010] and Wiedijk [2012a] observe.
One of the primary intended applications of interactive theorem provers is in
computer system correctness, that is, ‘proving programs correct’ (we discuss this
in more detail in the next section). As such, just as compilers compile themselves,
it seems natural to use proof assistants to verify themselves. There are two distinct
but related ideas that we might try to pursue here: prove metaproperties such as
consistency of the logical system itself, and verify that the prover’s code correctly
implements that logical system. Combining these, we could actually conclude with
pretty high conﬁdence that the output of the actual implementation is correct.
29Strictly speaking this is drawn from the documentation for Matita [Asperti et al., 2006],
which is supposed to be an implementation of essentially the same foundations, though it is no
longer exactly the same. Indeed, there does not seem to be any precise written speciﬁcation of
Coq’s current foundations, other than the actual code.

History of Interactive Theorem Proving
189
However, the analogy with compilers breaks down because of limitative results
of logic. Tarski’s theorem on the undeﬁnability of truth tells us that no formal
system of the type we consider here can formalize its own semantics, and G¨odel’s
Second Incompleteness Theorem tells us that it cannot prove its own consistency
in any way at all — unless of course it isn’t consistent, in which case it can
prove anything [Smullyan, 1992]. Thus, even ignoring implementation aspects,
successfully proving ⊢P Con(P), the consistency of the logic implemented by P
within P itself, would actually imply either that P’s logic is not consistent or that
the implementation is wrong!
One can still use a proof checker to formalize its own inference system (far from
being ruled out by G¨odel-type theorems, this is a key idea in their usual proof)
and so discuss the correctness of a proof checking program relative to the assumed
correctness of the logic [Wright, 1994]. If one wants a truly semantic correctness
theorem, perhaps the most satisfying approach would be to prove the correctness
of prover P inside a diﬀerent prover Q implementing a suﬃciently strong logic
for the limitative results not to present an obstacle — for example proving HOL
correct using Mizar.
However, probably because not many people are suﬃciently conversant with two
diﬀerent systems, most existing experiments have been closer to self-veriﬁcations
where the same or similar systems are used, but one either proves the correctness
of a weakened version or uses additional axioms in the proof. Barras and Werner
[1996] describe the formalization inside Coq itself of a proof-checker for the core
Calculus of Constructions. Harrison [2006b] presents two HOL-in-HOL consistency
proofs that include not only the abstract logic but a reasonably faithful model of
the actual system code with the exception of its deﬁnitional mechanisms.
• ⊢HOL Con(HOL −{∞}) proves in plain HOL the consistency of HOL with
the axiom of inﬁnity removed. This removal allows the whole type hierarchy
to be modeled inside one inﬁnite set; although this is a mathematically
trivial universe there is considerable interest in the basic correctness of the
implementations of the various syntax operations and inference rules (no
variable capture etc.)
• I ⊢HOL Con(HOL) proves the consistency of plain HOL inside HOL strength-
ened with a new axiom I about sets, that there is an uncountable cardinal
κ so that whenever λ < κ, we also have 2λ < κ. From the point of view of
ZF set theory this trivially holds (e.g. κ = |Vω+ω|), but with respect to the
simple type construction principles of HOL this plays a role analogous to the
existence of inaccessible cardinals.
Even Harrison’s work does not model all aspects of the actual implementation,
and the correspondence between code and its formalization is naive. These short-
comings, however, are being systematically addressed. Kumar et al. [2014] have
ported the proof to use HOL4 as the metaframework (while still being a veriﬁ-
cation of HOL Light), extended the consistency proof to cover the deﬁnitional

190
John Harrison, Josef Urban and Freek Wiedijk
principles and proven the correctness of an implementation in a special ML di-
alect CakeML with a machine-checked formal semantics. In fact, we could not
even expect to prove a similarly strong result for the main OCaml implementa-
tion of HOL Light, since OCaml has additional ‘real world programming language’
mechanisms beyond traditional functional constructs that can be used to break the
LCF abstraction boundaries, such as a general type-casting operation Obj.magic.
CakeML even has a formal semantic link via decompilation to machine code, mean-
ing that one can anticipate a correctness proof for an implementation of HOL Light
from a toplevel semantic soundness right down to the level of the machine code
that runs it. An analogous result has already been achieved [Myreen and Davis,
2014] for Milawa, a simpliﬁed bootstrapping version of ACL2 developed by Jared
Davis [2009]. These are remarkable achievements very much in the spirit of the
pioneering CLInc stack [Young, 1993].
Even if the implementation is proved correct at this level, we can never ulti-
mately banish skepticism completely. We are talking about things running on a
computer, a real physical object, and we cannot make any ﬁnal statement con-
necting any mathematical model to reality, nor can we exclude soft errors in the
computer hardware (these are intermittent faults usually resulting from external
particle bombardment, which are becoming increasingly signiﬁcant as miniatur-
ization advances). However, using a small logical kernel, performing rigorous veri-
ﬁcation from its semantics down to the machine code that runs it, and being able
to independently check proofs in other systems — perhaps repeatedly so to make
the chance of soft errors even more astronomically small — seems to give about
the best guarantee one could possibly hope for.
6.7
Applications
Interactive theorem provers have found applications in two main areas, the for-
malization of mathematics and the formal veriﬁcation of computer systems. In a
general sense, both of these involve the formalization of mathematical proof, but
the applications tend to place diﬀerent demands on a system, and usually those
we have described here were developed for some more or less speciﬁc application.
For example, Mizar was clearly intended for formalizing mathematics, while HOL
was intended for verifying hardware. Yet most of the systems have found unex-
pected applications in other areas too. And sometimes the two areas are mutually
reinforcing — for example in order to verify some very concrete and practical
ﬂoating-point algorithms, one may need ﬁrst to verify a signiﬁcant amount of real
analysis and number theory [Harrison, 2000].
The formalizability in principle of mathematical proof is widely accepted among
professional mathematicians as the ﬁnal arbiter of correctness. Bourbaki [1968]
clearly says that ‘the correctness of a mathematical text is veriﬁed by comparing
it, more or less explicitly, with the rules of a formalized language’, while Mac Lane
[1986] is also quite explicit (p. 377):

History of Interactive Theorem Proving
191
As to precision, we have now stated an absolute standard of rigor: A
Mathematical proof is rigorous when it is (or could be) written out in
the ﬁrst-order predicate language L(∈) as a sequence of inferences from
the axioms ZFC, each inference made according to one of the stated
rules. [. . . ] When a proof is in doubt, its repair is usually just a partial
approximation to the fully formal version.
However, before the advent of computerization, the idea of actually formalizing
proofs had seemed quite out of the question. The painstaking volumes of proofs in
Principia Mathematica [Whitehead and Russell, 1910] are for extremely elemen-
tary results compared with even classical real analysis, let alone mathematics at
the research level. It is only because of the availability of modern interactive proof
assistants that we can contemplate the actual formalization of non-trivial amounts
of contemporary mathematics.
Such formalization may answer a real need. Well-established branches of math-
ematics such as elementary real analysis are by now precisely formulated (one
might almost say ossiﬁed) and presented in rigorous way.
But at the research
level, mathematics is often quite vaguely formulated, because mathematicians can
usually rely on the deep understanding and intuition of themselves and their fellows
to keep them out of trouble. Indeed, as Lakatos [1976] describes, mathematicians
often begin to prove theorems before the fundamental concepts they involve are
clearly articulated, and the concepts often change in response to criticism of such
theorems. Even if mathematical assertions are formulated precisely, there is still
plenty of scope for errors in proofs. Mathematical proofs are subjected to peer
review before publication, but there are plenty of well-documented cases where
published results turned out to be faulty. A notable example is the ﬁrst purported
proof of the 4-color theorem [Kempe, 1879]; the error in this proof was eventu-
ally pointed out in print a decade later [Heawood, 1890]. A book by Lecat [1935]
gave 130 pages of errors made by major mathematicians up to 1900. With the
abundance of theorems being published today, often emanating from writers who
are not trained mathematicians, one fears that a project like Lecat’s would be
practically impossible, or at least would demand a journal to itself! Consider for
example the ﬁve footnotes from a single page of [Jech, 1973] shown in Figure 6.
Such examples can be adduced to argue that the usual social process works, at
least for results that are considered suﬃciently important. Yet we live in an age of
increasingly large proofs relying on highly specialized knowledge, where the num-
ber of people who reasonably could check them is small indeed. In cases where the
proof relies on extensive computer calculation, it is diﬃcult to really say convinc-
ingly that any human being has checked it [Lam, 1990]. Even Ruﬃni’s, arguably
quite correct, 1799 proof of what is now a very classical result, the insolvability
of general quintics by radicals, was in its day considered too unwieldy to reward
study and was largely ignored. Nowadays we have proofs of key results that are
much larger, such as the classiﬁcation of ﬁnite simple groups, which is spread over
numerous journal articles with a total page count of the order of 10,000. Is the
social process really a reliable method of checking such huge proofs?

192
John Harrison, Josef Urban and Freek Wiedijk
Figure 6: Footnotes from Jech’s ‘Set Theory’, p. 118
Over the last few decades there has been a signiﬁcant amount of mathematics
formalized in interactive proof assistants.
Wiedijk [2009] surveys the libraries
of formal mathematics provided by several interactive theorem provers, with the
Mizar MML discussed above being the largest.30 Despite this, the frontier of what
has been formalized is on average perhaps 100 years behind the actual development
of that mathematics — for example it was only quite recently that some of the
jewels of 19th century mathematics such as the Prime Number Theorem were
formalized [Avigad et al., 2007; Harrison, 2009a].
The ﬁrst notable exception
to this state of aﬀairs was arguably Gonthier’s machine-checked proof [Gonthier,
2005; Gonthier, 2008] of the 4-color theorem, originally proved in 1976 [Appel
and Haken, 1976]. This also showed convincingly that proofs involving a large
amount of computation could be brought within the purview of formalization.
More recently Gonthier has also led a team that formalized a proof of of the
Feit-Thompson theorem, a milestone in group theory from 1963 that forms an
important part of the classiﬁcation of ﬁnite simple groups [Gonthier et al., 2013].
In one case formalization has reached the frontier of mathematics research itself
and led to the advocacy of formalization by a notable contemporary mathemati-
cian, Thomas Hales. The venerable Kepler conjecture states that no arrangement
of identical balls in ordinary 3-dimensional space has a higher packing density than
the obvious cannonball arrangement. Hales, working with Ferguson, ﬁnally proved
this conjecture in 1998, but the size of the proof was daunting: about 300 pages
of traditional mathematics: geometry, measure, graph theory and related combi-
natorics, as well as about 40,000 lines of supporting computer code performing
graph enumeration, nonlinear optimization and linear programming. Hales sub-
mitted the proof to Annals of Mathematics, and after four years of deliberation,
the referees were still unable to provide a satisfactory review:
30The list http://www.cs.ru.nl/~freek/100/ gives a more selective ideas about formalizations
of speciﬁc theorems.

History of Interactive Theorem Proving
193
The news from the referees is bad, from my perspective. They have
not been able to certify the correctness of the proof, and will not be
able to certify it in the future, because they have run out of energy to
devote to the problem. This is not what I had hoped for.
Hales’s proof was indeed eventually published [Hales, 2005], and no signiﬁcant
error has been found in it [Hales et al., 2010]. Nevertheless, the verdict is dis-
appointingly lacking in clarity and ﬁnality. As a result, Hales initiated a project
called Flyspeck to completely formalize the proof [Hales, 2006]. This project in-
volved a sustained eﬀort by a large and geographically distributed team, many of
whom were originally complete novices in the ﬁeld of formalization. In a major
milestone for formalization of mathematics, the project has just been completed at
time of writing.31 That is, all the ordinary mathematics has been formalized, the
linear and nonlinear optimizations have been reimplemented in a proof-producing
fashion and the graph enumeration code has been rewritten in ML and formally
proved correct. That a contemporary proof so large, intricate and heterogeneous
can be completely formalized down to the most basic logical principles indicates
just how much can already be achieved with suﬃcient labour.
The process of
formalization, and some other parallel developments, have resulted in a signiﬁcant
simpliﬁcation of the proof and its reorganization into a formalization-friendly form
[Hales, 2012].
Much of the early development of interactive theorem provers, as we have noted
above, was motivated not by pure mathematics but by problems in computer
system veriﬁcation, with the veriﬁcation of security properties a particular focus
[MacKenzie, 2001]. Using formalization to verify the correct behavior of computer
systems (e.g. hardware, software, protocols and their combinations) is an easy
application to justify on utilitarian grounds. We might wish to prove that a sorting
algorithm really does always sort its input list, that a numerical algorithm does
return a result accurate to within a speciﬁed error bound, that a server will under
certain assumptions always respond to a request, or will ensure certain security
properties, etc. etc.
Although recently many of the most notable successes in veriﬁcation have been
in hardware veriﬁcation, and there was great interest in higher-level system veri-
ﬁcation, the traditional focus has been software veriﬁcation or ‘proving programs
correct’.
(Perhaps in the 1970s, hardware was considered too simple to need
verifying?) Traditionally, program veriﬁcation has usually been based on special
axiomatic systems for simple imperative programming languages, which were de-
veloped by Hoare [1969] and Dijkstra [1976] among others from earlier work by
Naur [1966] and Floyd [1967]. The axiomatic approach to program semantics as-
serts certain relationships between the precondition and postcondition, which are
predicates over the state before and after execution — in Dijkstra’s formulation a
program is identiﬁed with a predicate transformer mapping any postcondition to
the weakest precondition that ensures that postcondition.
31https://code.google.com/p/flyspeck/wiki/AnnouncingCompletion

194
John Harrison, Josef Urban and Freek Wiedijk
Actual correctness proofs can be done in several ways. One can use a proof
system embodying the Hoare logic rules, or use reﬁnement of the speciﬁcation into
a program [Back, 1980; Morgan, 1990]. Alternatively, one can annotate a program
with special assertions indicating that certain properties of the state should hold
whenever that point is reached. Such an annotated program can be distilled into a
set of purely mathematical assertions called veriﬁcation conditions: if these can be
proved then the correctness of the whole program in terms of Hoare logic follows
automatically. The ﬁrst mechanical veriﬁer was built by King [1969] in a PhD
supervised by Floyd, with another more interactive one soon developed by Good
[1970] under the supervision of Ralph London, who had himself pioneered manual
proofs of programs [London, 1970]. Subsequently Good et al. [1979] developed the
GYPSY interactive program veriﬁer which was applied quite successfully to some
non-trivial problems [Good, 1983], and had a signiﬁcant inﬂuence on the ﬁeld of
automated reasoning.
The use of special veriﬁcation frameworks continues to this day with systems
such as KIV [Reif, 1995] and various tools supporting the B method [Abrial, 1996].
But the use of general interactive theorem provers for veriﬁcation proofs has be-
come a popular alternative to the use of such systems. Of course, the boundary
between the two is not always sharp, but interactive provers generally make avail-
able a richer mathematical infrastructure, make it easier to call on a wide variety
of automated methods, and oﬀer good standards of reliability. In particular, it
is not necessary to hardwire into the framework a connection with a speciﬁc pro-
gramming language, and one can reason explicitly about the semantics of diﬀerent
languages and their relationship, for example to talk about compiler correctness.
A signiﬁcant theme here is the embedding of other formalisms, whether they be
programming languages, hardware description languages, speciﬁcation languages,
and even other logics, inside general theorem provers like HOL and PVS. One
approach, perhaps the most obvious, is to create formal models of the syntax and
semantics of the formalism inside the prover’s logic and use the system to reason
about them.
Following Boulton et al.
[1993], this has become known as deep
embedding, contrasting with the alternative of shallow embedding. In the latter,
language constructs are associated directly with logical entities, and the notation
is merely a convenience. This ﬁts naturally with the view, expressed for exam-
ple by Dijkstra [1976], that a programming language should be thought of ﬁrst
and foremost as an algorithm-oriented system of mathematical notation, and only
secondarily as something to be run on a machine. A seminal paper by Gordon
[1989] showed how a simple imperative programming language could be semanti-
cally embedded in higher order logic in such a way that the classic Floyd-Hoare
rules simply become derivable theorems.32
Such shallow embedding seems especially natural in the case of embedding less
expressive logics — for instance the basic operators of temporal logic [Gabbay et
al., 1994] can easily be considered just as shorthands for quantiﬁed statements
32Gordon (private communication) recalls that the idea of embedding Hoare logic directly in
HOL in this way may have come from Roger Jones.

History of Interactive Theorem Proving
195
about sequences in higher-order logic.
When one writes ‘□P’ in LTL, Linear
Temporal Logic [Pnueli, 1977], for example, it means that ‘P is true now and at all
future times’. Considering temporal propositions systematically as mappings from
natural numbers (‘times’) to Booleans, we can actually deﬁne □by the equation
(□P)(t) =def ∀t′.t′ ≥t ⇒P(t′). One then doesn’t need any layer of translation
between LTL and the richer mathematical framework. Actually, the fact that this
is a ﬂexible and satisfying way to reason about other logics may explain why any
explosion of interest in provers for diﬀerent logics (as might have been anticipated
by the designers of logical frameworks) has so far been relatively muted.
A kind of reversal of the idea of semantic embedding is to consider a subset of the
logic as a programming language and then translate (often ‘extract’, though some
use that in the sense of extracting programs from constructive proofs) into a real
programming language, which is most naturally a functional one like Lisp or ML,
languages that can be said to ‘wear their semantics on their sleeves’ [Henson, 1987].
For a system like ACL2, there is by design a near-equivalence between the logic and
the implementation language, so this works in a particularly straightforward and
elegant way. For richer logics like HOL or Coq, one needs to carefully demarcate
a subset that has a natural correspondence with a real functional language. Even
without the actual extraction to code, Coq supports a highly eﬃcient reduction
mechanism inside the logic so that one can eﬀectively ‘run programs’ without
stepping out of the formal inference system, at least in principle. This can be
done in other systems like HOL too, but because of the much more parsimonious
logical kernel, the performance penalty is considerable.
Many in the 1970s dreamed of the pervasive use of correctness proofs from top
to bottom: applications, operating systems, compilers, programming languages
and hardware. A pioneering example of such a veriﬁed tower of basic systems
was the ‘CLInc stack’ [Young, 1993]. There have been more recent projects in
the same style such as Verisoft [Paul, 2008], but many recent eﬀorts have been
more piecemeal and focused on particular areas, with applications to ﬂoating-
point arithmetic being particularly successful [Moore et al., 1998; Russinoﬀ, 1998;
O’Leary et al., 1999; Harrison, 2000; Kaivola and Aagaard, 2000; Kaivola and
Kohatsu, 2001; Sawada and Gamboa, 2002; Slobodov´a, 2007; Hunt and Swords,
2009; O’Leary et al., 2013].
A key feature of such examples is that they are
being carried out by those in the hardware industry, not only by academics in
universities, indicating at least some successful penetration of interactive theorem
proving into the ‘real world’.
Two substantial formal veriﬁcations carried out with interactive theorem prov-
ing in recent years are the correctness proofs of a ‘CompCert’ optimizing compiler
from a signiﬁcant subset of C using Coq [Leroy, 2009], and of a designed-for-
veriﬁcation version of the commercial L4 microkernel using Isabelle/HOL [Klein
et al., 2010].
While these are currently isolated and independent eﬀorts, they
demonstrate the applicability of formal methods to key items of system software,
and have to some extent inspired further interconnecting veriﬁcations — for ex-
ample the CompCert compiler has been extended with a formalized treatment of

196
John Harrison, Josef Urban and Freek Wiedijk
ﬂoating-point arithmetic [Boldo et al., 2013] while the actual ARM binary for the
L4 microkernel has also been veriﬁed, using HOL4, by Sewell et al. [2013]. Much
of the impetus for the development of theorem provers and veriﬁcation frameworks
arose from the interest in proving isolation properties of time-sharing operating
systems, as we have seen, so it is especially pleasing to see that this was not just
a dream, but can be realized — albeit with considerable human eﬀort — using
today’s technology.
ACKNOWLEDGEMENTS
The authors are grateful to J¨org Siekmann for inviting them to prepare this chapter
and for his patience in the face of our lengthy delays. The helpful review of the
oﬃcial reader Larry Paulson as well as useful comments from Mike Gordon, Tom
Hales and J Moore, have signiﬁcantly improved the eventual form of the chapter.
BIBLIOGRAPHY
[Aagaard and Harrison, 2000] M. Aagaard and J. Harrison, editors. Theorem Proving in Higher
Order Logics: 13th International Conference, TPHOLs 2000, volume 1869 of Lecture Notes
in Computer Science. Springer-Verlag, 2000.
[Abrahams, 1963] Paul Abrahams. Machine Veriﬁcation of Mathematical Proof. PhD thesis,
Massachusetts Institute of Technology, 1963.
[Abrial, 1996] Jean-Raymond Abrial. The B-Book: Assigning Programs to Meanings. Cam-
bridge University Press, 1996.
[Adams, 2010] Mark Adams. Introducing HOL Zero. In Komei Fukuda, Joris van der Hoeven,
Michael Joswig, and Nobuki Takayama, editors, Proceedings of ICMS 2010, the Third In-
ternational Congress on Mathematical Software, volume 6327 of Lecture Notes in Computer
Science, pages 142–143, Kobe, Japan, 2010. Springer-Verlag.
[Agrawal et al., 2004] Manindra Agrawal, Neeraj Kayal, and Nitin Saxena. PRIMES is in P.
Annals of Mathematics, 160:781–793, 2004.
[Aitken et al., 1998] J.S. Aitken, P. Gray, T. Melham, and M. Thomas. Interactive theorem
proving: An empirical study of user activity. Journal of Symbolic Computation, 25(2):263 –
284, 1998.
[Alama et al., 2011] Jesse Alama, Kasper Brink, Lionel Mamane, and Josef Urban. Large formal
wikis: Issues and solutions. In James H. Davenport, William M. Farmer, Josef Urban, and
Florian Rabe, editors, Calculemus/MKM, volume 6824 of LNCS, pages 133–148. Springer,
2011.
[Allen et al., 1990] S. Allen, R. Constable, D. Howe, and W. Aitken. The semantics of reﬂected
proof. In Proceedings of the Fifth Annual Symposium on Logic in Computer Science, pages
95–107, Los Alamitos, CA, USA, 1990. IEEE Computer Society Press.
[Andersen and Petersen, 1991] Flemming Andersen and Kim Dam Petersen. Recursive boolean
functions in HOL. In Archer et al. [1991], pages 367–377.
[Anonymous, 1994] Anonymous. The QED Manifesto. In Bundy [1994], pages 238–251.
[Appel and Haken, 1976] K. Appel and W. Haken. Every planar map is four colorable. Bulletin
of the American Mathematical Society, 82:711–712, 1976.
[Archer et al., 1991] Myla Archer, Jeﬀrey J. Joyce, Karl N. Levitt, and Phillip J. Windley,
editors. Proceedings of the 1991 International Workshop on the HOL theorem proving system
and its Applications, University of California at Davis, Davis CA, USA, 1991. IEEE Computer
Society Press.
[Asperti et al., 2003] Andrea Asperti, Luca Padovani, Claudio Sacerdoti Coen, Ferruccio Guidi,
and Irene Schena. Mathematical Knowledge Management in HELM. Ann. Math. Artif. Intell.,
38(1-3):27–46, 2003.

History of Interactive Theorem Proving
197
[Asperti et al., 2004] Andrea Asperti, Ferruccio Guidi, Claudio Sacerdoti Coen, Enrico Tassi,
and Stefano Zacchiroli. A content based mathematical search engine: Whelp. In TYPES,
volume 3839 of Lecture Notes in Computer Science, pages 17–32. Springer, 2004.
[Asperti et al., 2006] Andrea Asperti, Claudio Sacerdoti Coen, Enrico Tassi, and Stefano Zac-
chiroli. Crafting a proof assistant. In Thorsten Altenkirch and Conor McBride, editors, Types
for Proofs and Programs, International Workshop, TYPES 2006, Nottingham, UK, April
18-21, 2006, Revised Selected Papers, volume 4502 of LNCS, pages 18–32. Springer, 2006.
[Aspinall, 2000] David Aspinall.
Proof General:
A generic tool for proof development.
In
Susanne Graf and Michael I. Schwartzbach, editors, TACAS, volume 1785 of Lecture Notes
in Computer Science, pages 38–42. Springer, 2000.
[Autexier et al., 2010] Serge Autexier, Jacques Calmet, David Delahaye, Patrick D. F. Ion, Lau-
rence Rideau, Renaud Rioboo, and Alan P. Sexton, editors. Intelligent Computer Mathemat-
ics, 10th International Conference, AISC 2010, 17th Symposium, Calculemus 2010, and 9th
International Conference, MKM 2010, Paris, France, July 5-10, 2010. Proceedings, volume
6167 of LNCS. Springer, 2010.
[Avigad et al., 2007] Jeremy Avigad, Kevin Donnelly, David Gray, and Paul Raﬀ. A formally
veriﬁed proof of the prime number theorem. ACM Transactions on Computational Logic,
9(1:2):1–23, 2007.
[Baader and Nipkow, 1998] Franz Baader and Tobias Nipkow. Term Rewriting and All That.
Cambridge University Press, 1998.
[Back, 1980] Ralph Back. Correctness Preserving Program Transformations: Proof Theory and
Applications, volume 131 of Mathematical Centre Tracts. Mathematical Centre, Amsterdam,
1980.
[Balaa and Bertot, 2000] Antonia Balaa and Yves Bertot. Fix-point equations for well-founded
recursion in type theory. In Aagaard and Harrison [2000], pages 1–16.
[Bancerek and Rudnicki, 2002] Grzegorz Bancerek and Piotr Rudnicki. A compendium of con-
tinuous lattices in Mizar. J. Autom. Reasoning, 29(3-4):189–224, 2002.
[Bancerek and Rudnicki, 2003] Grzegorz Bancerek and Piotr Rudnicki. Information retrieval in
MML. In MKM, volume 2594 of Lecture Notes in Computer Science, pages 119–132. Springer,
2003.
[Bancerek, 2006] Grzegorz Bancerek. Information retrieval and rendering with MML Query. In
Jonathan M. Borwein and William M. Farmer, editors, MKM, volume 4108 of Lecture Notes
in Computer Science, pages 266–279. Springer, 2006.
[Barendregt, 1992] H. P. Barendregt.
Lambda calculi with types.
In S. Abramsky, Dov M.
Gabbay, and S. E. Maibaum, editors, Handbook of Logic in Computer Science (Vol. 2), pages
117–309. Oxford University Press, 1992.
[Barendregt, 1997] Henk Barendregt. The impact of the lambda calculus on logic and computer
science. Bulletin of Symbolic Logic, 3:181–215, 1997.
[Barras and Werner, 1996] Bruno Barras and Benjamin Werner.
Coq in Coq.
Unpublished
report, available at www.lix.polytechnique.fr/~barras/publi/coqincoq.pdf, 1996.
[Barringer et al., 1984] H. Barringer, J. Cheng, and C. B. Jones. A logic covering undeﬁnedness
in program proofs. Acta Informatica, 21:251–269, 1984.
[Bates and Constable, 1985] Joseph L. Bates and Robert L. Constable.
Proofs as programs.
ACM Transactions on Programming Languages and Systems, 7:113–136, 1985.
[Beckert and Posegga, 1995] Bernhard Beckert and Joachim Posegga. leanTAP: Lean, tableau-
based deduction. Journal of Automated Reasoning, 15:339–358, 1995.
[Beeson, 1984] M. J. Beeson.
Foundations of constructive mathematics:
metamathematical
studies, volume 3 of Ergebnisse der Mathematik und ihrer Grenzgebiete.
Springer-Verlag,
1984.
[Berghofer and Wenzel, 1999] Stefan Berghofer and Markus Wenzel.
Inductive datatypes in
HOL — lessons learned in formal-logic engineering. In Bertot et al. [1999], pages 19–36.
[Bergstra et al., 2007] J. A. Bergstra, Y. Hirshﬁeld, and J. V. Tucker. Meadows and the equa-
tional speciﬁcation of division. Theoretical Computer Science, 410:1261–1271, 2007.
[Bertot and Th´ery, 1998] Yves Bertot and Laurent Th´ery. A generic approach to building user
interfaces for theorem provers. J. Symb. Comput., 25(2):161–194, 1998.
[Bertot et al., 1994] Yves Bertot, Gilles Kahn, and Laurent Th´ery. Proof by pointing. In Masami
Hagiya and John C. Mitchell, editors, TACS, volume 789 of Lecture Notes in Computer
Science, pages 141–160. Springer, 1994.

198
John Harrison, Josef Urban and Freek Wiedijk
[Bertot et al., 1999] Yves Bertot, Gilles Dowek, Andr´e Hirschowitz, Christine Paulin, and Lau-
rent Th´ery, editors. Theorem Proving in Higher Order Logics: 12th International Confer-
ence, TPHOLs’99, volume 1690 of Lecture Notes in Computer Science, Nice, France, 1999.
Springer-Verlag.
[Biggs et al., 1976] Norman L. Biggs, E. Keith Lloyd, and Robin J. Wilson.
Graph Theory
1736–1936. Clarendon Press, 1976.
[Birtwistle and Subrahmanyam, 1989] Graham Birtwistle and P. A. Subrahmanyam, editors.
Current Trends in Hardware Veriﬁcation and Automated Theorem Proving. Springer-Verlag,
1989.
[Bishop and Bridges, 1985] Errett Bishop and Douglas Bridges. Constructive analysis, volume
279 of Grundlehren der mathematischen Wissenschaften. Springer-Verlag, 1985.
[Blanchette et al., 2013] Jasmin Christian Blanchette, Sascha B¨ohme, Andrei Popescu, and
Nicholas Smallbone. Encoding monomorphic and polymorphic types. In Nir Piterman and
Scott A. Smolka, editors, TACAS, volume 7795 of Lecture Notes in Computer Science, pages
493–507. Springer, 2013.
[Blanchette et al., 2014] J. C. Blanchette, J. H¨olzl, A. Lochbihler, L. Panny, A. Popescu, and
D. Traytel. Truly modular (co)datatypes for Isabelle/HOL. In Klein and Gamboa [2014],
pages 93–110.
[Bledsoe and Bruell, 1974] W. W. Bledsoe and Peter Bruell. A man-machine theorem-proving
system. Artiﬁcial Intelligence, 5:51–72, 1974.
[Bledsoe and Gilbert, 1967] W. W. Bledsoe and E. J. Gilbert.
Automatic theorem proof-
checking in set theory: A preliminary report. Technical Report SC-RR-67-525, Sandia Na-
tional Lab, 1967.
[Bledsoe, 1984] W. W. Bledsoe. Some automatic proofs in analysis. In W. W. Bledsoe and D. W.
Loveland, editors, Automated Theorem Proving: After 25 Years, volume 29 of Contemporary
Mathematics, pages 89–118. American Mathematical Society, 1984.
[Blum, 1993] Manuel Blum. Program result checking: A new approach to making programs
more reliable. In Andrzej Lingas, Rolf Karlsson, and Svante Carlsson, editors, Automata,
Languages and Programming, 20th International Colloquium, ICALP93, Proceedings, volume
700 of Lecture Notes in Computer Science, pages 1–14, Lund, Sweden, 1993. Springer-Verlag.
[Boldo et al., 2013] Sylvie
Boldo,
Jacques-Henri
Jourdan,
Xavier
Leroy,
and
Guillaume
Melquiond. A formally-veriﬁed C compiler supporting ﬂoating-point arithmetic. In Alberto
Nannarelli, Peter-Michael Siedel, and Ping Tak Peter Tang, editors, 21st IEEE symposium
on computer arithmetic, ARITH 21, proceedings, pages 107–115. IEEE, 2013.
[Boulton et al., 1993] Richard Boulton, Andrew Gordon, Mike Gordon, John Harrison, John
Herbert, and John Van Tassel. Experience with embedding hardware description languages
in HOL. In Victoria Stavridou, Thomas F. Melham, and R. T. Boute, editors, Proceedings of
the IFIP TC10/WG 10.2 International Conference on Theorem Provers in Circuit Design:
Theory, Practice and Experience, volume A-10 of IFIP Transactions A: Computer Science
and Technology, pages 129–156, Nijmegen, The Netherlands, 1993. North-Holland.
[Boulton, 1992] Richard Boulton. Boyer-Moore automation for the HOL system. In Claesen
and Gordon [1992], pages 133–142.
[Boulton, 1993] Richard John Boulton. Eﬃciency in a fully-expansive theorem prover. Technical
Report 337, University of Cambridge Computer Laboratory, New Museums Site, Pembroke
Street, Cambridge, CB2 3QG, UK, 1993. Author’s PhD thesis.
[Bourbaki, 1968] Nicolas Bourbaki.
Theory of sets.
Elements of mathematics. Addison-
Wesley, 1968.
Translated from French ‘Th´eorie des ensembles’ in the series ‘El´ements de
math´ematique’, originally published by Hermann in 1968.
[Bove et al., 2009] Ana Bove, Peter Dybjer, and Ulf Norell.
A Brief Overview of Agda - A
Functional Language with Dependent Types. In Stefan Berghofer, Tobias Nipkow, Christian
Urban, and Makarius Wenzel, editors, Theorem Proving in Higher Order Logics, 22nd Inter-
national Conference, TPHOLs 2009, Munich, Germany, August 17-20, 2009. Proceedings,
volume 5674 of LNCS, pages 73–78. Springer, 2009.
[Boyer and Moore, 1979] Robert S. Boyer and J Strother Moore. A Computational Logic. ACM
Monograph Series. Academic Press, 1979.
[Boyer and Moore, 1981] Robert S. Boyer and J Strother Moore. Metafunctions: proving them
correct and using them eﬃciently as new proof procedures. In Robert S. Boyer and J Strother
Moore, editors, The Correctness Problem in Computer Science, pages 103–184. Academic
Press, 1981.

History of Interactive Theorem Proving
199
[Boyer and Moore, 1988] Robert S. Boyer and J Strother Moore. A Computational Logic Hand-
book, volume 23 of Perspectives in Computing. Academic Press, 1988.
[Bridge et al., 2014] James P. Bridge, Sean B. Holden, and Lawrence C. Paulson.
Machine
learning for ﬁrst-order theorem proving. Journal of Automated Reasoning, pages 1–32, 2014.
[Bryant, 1986] Randall E. Bryant. Graph-based algorithms for Boolean function manipulation.
IEEE Transactions on Computers, C-35:677–691, 1986.
[Buchberger, 1965] Bruno Buchberger. Ein Algorithmus zum Auﬃnden der Basiselemente des
Restklassenringes nach einem nulldimensionalen Polynomideal. PhD thesis, Mathematisches
Institut der Universit¨at Innsbruck, 1965. English translation in Journal of Symbolic Compu-
tation vol. 41 (2006), pp. 475–511.
[Bumcrot, 1965] R. Bumcrot. On lattice complements. Proceedings of the Glasgow Mathematical
Association, 7:22–23, 1965.
[Bundy et al., 1991] A. Bundy, F. van Harmelen, J. Hesketh, and A. Smaill. Experiments with
proof plans for induction. Journal of Automated Reasoning, 7:303–323, 1991.
[Bundy, 1994] Alan Bundy, editor.
12th International Conference on Automated Deduction,
volume 814 of Lecture Notes in Computer Science, Nancy, France, 1994. Springer-Verlag.
[Burch et al., 1992] J. R. Burch, E. M. Clarke, K. L. McMillan, D. L. Dill, and L. J. Hwang.
Symbolic model checking: 1020 states and beyond. Information and Computation, 98:142–
170, 1992.
[Burstall, 1969] R. M. Burstall. Proving properties of programs by structural induction. The
Computer Journal, 12:41–48, 1969.
[Camilleri and Melham, 1992] Juanito Camilleri and Tom Melham. Reasoning with inductively
deﬁned relations in the HOL theorem prover. Technical Report 265, University of Cambridge
Computer Laboratory, New Museums Site, Pembroke Street, Cambridge, CB2 3QG, UK,
1992.
[Caprotti and Oostdijk, 2001] Olga Caprotti and Martin Oostdijk. Formal and eﬃcient primal-
ity proofs by the use of computer algebra oracles. Journal of Symbolic Computation, 32:55–70,
2001.
[Carnap, 1937] Rudolf Carnap. The Logical Syntax of Language. International library of psy-
chology, philosophy and scientiﬁc method. Routledge & Kegan Paul, 1937. Translated from
‘Logische Syntax der Sprache’ by Amethe Smeaton (Countess von Zeppelin), with some new
sections not in the German original.
[Cederquist et al., 1998] Jan Cederquist, Thierry Coquand, and Sara Negri. The Hahn-Banach
Theorem in Type Theory. In G. Sambin and J. Smith, editors, Twenty-ﬁve years of Con-
structive Type Theory, pages 57–72, 1998.
[Chaieb and Nipkow, 2008] Amine Chaieb and Tobias Nipkow. Proof synthesis and reﬂection
for linear arithmetic. Journal of Automated Reasoning, 41, 2008.
[Chen, 1992] Wilfred Chen. Tactic-based theorem proving and knowledge-based forward chain-
ing. In Kapur [1992], pages 552–566.
[Chou, 1988] S.-C. Chou. Mechanical Geometry Theorem Proving. Reidel, 1988.
[Church, 1940] Alonzo Church.
A formulation of the Simple Theory of Types.
Journal of
Symbolic Logic, 5:56–68, 1940.
[Claesen and Gordon, 1992] Luc J. M. Claesen and Michael J. C. Gordon, editors. Proceedings
of the IFIP TC10/WG10.2 International Workshop on Higher Order Logic Theorem Proving
and its Applications, volume A-20 of IFIP Transactions A: Computer Science and Technology,
IMEC, Leuven, Belgium, 1992. North-Holland.
[Clarke and Emerson, 1981] Edmund M. Clarke and E. Allen Emerson. Design and synthesis
of synchronization skeletons using branching-time temporal logic. In Dextrer Kozen, editor,
Logics of Programs, volume 131 of Lecture Notes in Computer Science, pages 52–71, Yorktown
Heights, 1981. Springer-Verlag.
[Cohen and Mahboubi, 2010] Cyril Cohen and Assia Mahboubi. A formal quantiﬁer elimination
for algebraically closed ﬁelds. In Symposium on the Integration of Symbolic Computation and
Mechanised Reasoning, Calculemus, volume 6167 of Lecture Notes in Computer Science,
pages 189–203. Springer-Verlag, 2010.
[Constable and Bates, 1983] Robert L. Constable and Joseph L. Bates. The Nearly Ultimate
PEARL. Technical Report TR 83-551, Department of Computer Science, Cornell University,
January 1983.
[Constable and O’Donnell, 1978] Robert L. Constable and Michael J. O’Donnell. A program-
ming logic, with an introduction to the PL/CV veriﬁer. Winthrop, 1978.

200
John Harrison, Josef Urban and Freek Wiedijk
[Constable, 1986] Robert Constable. Implementing Mathematics with The Nuprl Proof Devel-
opment System. Prentice-Hall, 1986.
[Cooper, 1972] D. C. Cooper.
Theorem proving in arithmetic without multiplication.
In
B. Melzer and D. Michie, editors, Machine Intelligence 7, pages 91–99. Elsevier, 1972.
[Coq development team, 2012] Coq development team. The Coq proof assistant reference man-
ual, 2012. Version 8.4pl4.
[Coquand and Huet, 1988] Thierry Coquand and G´erard Huet. The calculus of constructions.
Inf. Comput., 76(2/3):95–120, 1988.
[Coquand and Paulin, 1990] Thierry Coquand and Christine Paulin. Inductively deﬁned types.
In Per Martin-L¨of and Grigori Mints, editors, COLOG-88, International Conference on Com-
puter Logic, Tallinn, USSR, December 1988, Proceedings, volume 417 of LNCS, pages 50–66.
Springer, 1990.
[Coquand et al., 2005] Catarina Coquand, Dan Synek, and Makoto Takeyama.
An Emacs-
Interface for Type-Directed Support for Constructing Proofs and Programs.
In European
Joint Conferences on Theory and Practice of Software, 2005.
[Cousineau and Mauny, 1998] Guy Cousineau and Michel Mauny. The Functional Approach to
Programming. Cambridge University Press, 1998.
[Craigen et al., 1991] D. Craigen, S. Kromodimeoljo, I. Meisels, B. Pase, and M. Saaltink.
EVES: An overview. In S. Prehn and W. J. Toetenel, editors, VDM ’91: Formal Software
Development Methods, volume 551 of Lecture Notes in Computer Science, pages 389–405.
Springer-Verlag, 1991.
[Curzon, 1995] Paul Curzon. Tracking design changes with formal machine-checked proof. The
Computer Journal, 38:91–100, 1995.
[Dahn and Wernhard, 1997] Ingo Dahn and Christoph Wernhard. First order proof problems
extracted from an article in the MIZAR mathematical library. In Maria Paola Bonacina and
Ulrich Furbach, editors, International Workshop on First-Order Theorem Proving, volume
97-50 of RISC-Linz Report Series, pages 58–62. RISC-Linz, 1997.
[Davis and Putnam, 1960] Martin Davis and Hilary Putnam. A computing procedure for quan-
tiﬁcation theory. Journal of the ACM, 7:201–215, 1960.
[Davis, 1957] M. Davis. A computer program for Presburger’s algorithm. In Summaries of talks
presented at the Summer Institute for Symbolic Logic, Cornell University, pages 215–233.
Institute for Defense Analyses, Princeton, NJ, 1957. Reprinted in Siekmann and Wrightson
[1983], pp. 41–48.
[Davis, 1981] Martin Davis.
Obvious logical inferences.
In Patrick J. Hayes, editor, IJCAI,
pages 530–531. William Kaufmann, 1981.
[Davis, 2009] Jared Davis. A Self-Verifying Theorem Prover. PhD thesis, University of Texas
at Austin, 2009.
[de Bruijn, 1968a] N.G. de Bruijn. AUTOMATH, a language for mathematics. Technical Report
68-WSK-05, Department of Mathematics, Eindhoven University of Technology, November
1968.
[de Bruijn, 1968b] N.G. de Bruijn. The mathematical language AUTOMATH, its usage, and
some of its extensions, 1968.
[de Bruijn, 1968c] N.G. de Bruijn. The mathematical language AUTOMATH, its usage, and
some of its extensions. In Symposium on Automatic Demonstration, IRIA, Versailles, France,
number 125 in LNCS, pages 29–61. Springer, December 1968.
Also published in Selected
Papers on Automath, Studies in Logic and the Foundations of Mathematics, Vol. 133, Ed.
R.P. Nederpelt, J.H. Geuvers and R.C. de Vrijer, chapter A2, pp. 73-100.
[de Bruijn, 1969] N.G. de Bruijn. SEMIPAL 2, an extension of the mathematical notational lan-
guage SEMIPAL. Technical Report Notitie 1969/43, Department of Mathematics, Eindhoven
University of Technology, March 1969.
[de Bruijn, 1970] N.G. de Bruijn.
A Processor for PAL.
Technical Report Notitie 1970/30,
Department of Mathematics, Eindhoven University of Technology, March 1970.
[de Bruijn, 1978] N.G. de Bruijn.
Verslag over het project Wiskundige Taal AUTOMATH,
September 1978.
[de Bruijn, 1979] N.G. de Bruijn. Wees contextbewust in WOT. Euclides, (55):7–12, 1979.
[de Bruijn, 1990] N.G. de Bruijn. Gedachten rondom AUTOMATH, March 1990.

History of Interactive Theorem Proving
201
[de Groote, 1993] Philippe de Groote. Deﬁning Lambda-Typed Lambda-Calculi by Axiomatiz-
ing the Typing Relation. In Patrice Enjalbert, Alain Finkel, and Klaus W. Wagner, editors,
STACS 93, 10th Annual Symposium on Theoretical Aspects of Computer Science, W¨urzburg,
Germany, February 25-27, 1993, Proceedings, volume 665 of LNCS, pages 712–723. Springer,
1993.
[de Moura and Passmore, 2013] Leonardo de Moura and Grant Passmore. The strategy chal-
lenge in SMT solving.
In Maria Paola Bonacina and Mark E Stickel, editors, Automated
Reasoning and Mathematics: Essays in Memory of William McCune, volume 7788 of Lec-
ture Notes in Computer Science, pages 15–44. Springer-Verlag, 2013.
[Delahaye, 2000] David Delahaye.
A tactic language for the system Coq.
In Michel Parigot
and Andrei Voronkov, editors, Logic Programming and Automated Reasoning, LPAR 2000,
volume 1955 of Lecture Notes in Computer Science, pages 85–95, La reunion, France, 2000.
Springer-Verlag.
[Denney, 2000] Ewen Denney. A prototype proof translator from HOL to Coq. In Aagaard and
Harrison [2000], pages 108–125.
[Denzinger and Schulz, 1996] J. Denzinger and S. Schulz. Recording and Analysing Knowledge-
Based Distributed Deduction Processes. Journal of Symbolic Computation, 21(4/5):523–541,
1996.
[Dick, 2011] Stephanie Dick. The work of proof in the age of human-machine collaboration. Isis,
102:494–505, 2011.
[Dijkstra and Scholten, 1990] E. W. Dijkstra and C. S. Scholten. Predicate Calculus and Pro-
gram Semantics. Springer-Verlag, 1990.
[Dijkstra, 1976] E. W. Dijkstra. A Discipline of Programming. Prentice-Hall, 1976.
[Dixon, 2005] Lucas Dixon. A Proof Planning Framework for Isabelle. PhD thesis, University
of Edinburgh, 2005.
[Eekelen et al., 2011] Marko Van Eekelen,
Herman Geuvers,
Julien Schmaltz,
and Freek
Wiedijk, editors. Interactive Theorem Proving, Second International Conference ITP, volume
6898 of Lecture Notes in Computer Science. Springer-Verlag, 2011.
[E´en and S¨orensson, 2003] Niklas E´en and Niklas S¨orensson.
An extensible SAT-solver.
In
E. Giunchiglia and A. Tacchella, editors, Theory and Applications of Satisﬁability Testing:
6th International Conference SAT 2003, volume 2919 of Lecture Notes in Computer Science,
pages 502–518. Springer-Verlag, 2003.
[Eisinger and Ohlbach, 1986] Norbert Eisinger and Hans J¨urgen Ohlbach. The Markgraf Karl
refutation procedure (MKRP). In Jorg H. Siekmann, editor, 8th International Conference
on Automated Deduction, volume 230 of Lecture Notes in Computer Science, pages 681–682,
Oxford, England, 1986. Springer-Verlag.
[Farmer et al., 1990] William Farmer, Joshua Guttman, and Javier Thayer. IMPS: an inter-
active mathematical proof system. In M. E. Stickel, editor, 10th International Conference
on Automated Deduction, volume 449 of Lecture Notes in Computer Science, pages 653–654,
Kaiserslautern, Federal Republic of Germany, 1990. Springer-Verlag.
[Feigenbaum and Feldman, 1995] Edward A. Feigenbaum and Julian Feldman, editors. Com-
puters & Thought. AAAI Press / MIT Press, 1995.
[Felty and Howe, 1997] A.P. Felty and D. Howe. Hybrid interactive theorem proving using Nuprl
and HOL. In William McCune, editor, Automated Deduction — CADE-14, volume 1249 of
Lecture Notes in Computer Science, pages 351–365, Townsville, Australia, 1997. Springer-
Verlag.
[Fitch, 1952] Frederic Brenton Fitch. Symbolic Logic: an introduction. The Ronald Press Com-
pany, New York, 1952.
[Fleuriot, 2001] Jacques Fleuriot. A Combination of Geometry Theorem Proving and Nonstan-
dard Analysis with Application to Newton’s Principia. Distinguished dissertations. Springer-
Verlag, 2001. Revised version of author’s PhD thesis.
[Floyd, 1967] R. W. Floyd.
Assigning meanings to programs.
In Proceedings of AMS Sym-
posia in Applied Mathematics, 19: Mathematical Aspects of Computer Science, pages 19–32.
American Mathematical Society, 1967.
[Furbach and Shankar, 2006] Ulrich Furbach and Natarajan Shankar, editors.
Proceedings of
the third International Joint Conference, IJCAR 2006, volume 4130 of Lecture Notes in
Computer Science, Seattle, WA, 2006. Springer-Verlag.
[Gabbay et al., 1994] D. Gabbay, I. Hodkinson, and M. Reynolds.
Temporal Logic.
Oxford
University Press, 1994.

202
John Harrison, Josef Urban and Freek Wiedijk
[Gamboa, 1999] Ruben Gamboa.
Mechanically Verifying Real-Valued Algorithms in ACL2.
PhD thesis, University of Texas at Austin, 1999.
[Ganesalingam, 2013] Mohan Ganesalingam. The Language of Mathematics: A Linguistic and
Philosophical Investigation, volume 7805 of Lecture Notes in Computer Science. Springer-
Verlag, 2013.
[Gelerntner, 1959] H. Gelerntner. Realization of a geometry-theorem proving machine. In Pro-
ceedings of the International Conference on Information Processing, UNESCO House, pages
273–282, 1959. Also appears in Siekmann and Wrightson [1983], pp. 99–117 and in Feigenbaum
and Feldman [1995], pp. 134–152.
[Geuvers and Barendsen, 1999] Herman Geuvers and Erik Barendsen. Some logical and syntac-
tical observations concerning the ﬁrst-order dependent type system lambda-P. Mathematical
Structures in Computer Science, 9(4):335–359, 1999.
[Gilmore, 1960] P. C. Gilmore. A proof method for quantiﬁcation theory: Its justiﬁcation and
realization. IBM Journal of Research and Development, 4:28–35, 1960.
[Giunchiglia and Smaill, 1989] Fausto Giunchiglia and Alan Smaill. Reﬂection in constructive
and non-constructive automated reasoning. In Harvey Abramson and M. H. Rogers, editors,
Meta-Programming in Logic Programming, pages 123–140. MIT Press, 1989.
[Goldberg and Novikov, 2002] Evgueni Goldberg and Yakov Novikov.
BerkMin:
a fast and
robust Sat-solver. In Carlos Delgado Kloos and Jose Da Franca, editors, Design, Automation
and Test in Europe Conference and Exhibition (DATE 2002), pages 142–149, Paris, France,
2002. IEEE Computer Society Press.
[Gonthier and Mahboubi, 2010] Georges Gonthier and Assia Mahboubi.
An introduction to
small scale reﬂection in Coq. Journal of Formalized Reasoning, 3(2):95–152, 2010.
[Gonthier et al., 2008] Georges Gonthier, Assia Mahboubi, and Enrico Tassi.
A Small Scale
Reﬂection Extension for the Coq system. Technical Report RR-6455, INRIA, 2008.
[Gonthier et al., 2013] Georges Gonthier, Andrea Asperti, Jeremy Avigad, Yves Bertot, Cyril
Cohen, Fran¸cois Garillot, St´ephane Le Roux, Assia Mahboubi, Russell O’Connor, Sidi Ould
Biha, Ioana Pasca, Laurence Rideau, Alexey Solovyev, Enrico Tassi, and Laurent Th´ery.
A machine-checked proof of the odd order theorem.
In S. Blazy, C. Paulin-Mohring, and
D. Pichardie, editors, Fourth International Conference on Interactive Theorem Proving, ITP
2013, volume 7998 of Lecture Notes in Computer Science, pages 163–179, Rennes, France,
2013. Springer-Verlag.
[Gonthier, 1996] Georges Gonthier. Verifying the safety of a practical concurrent garbage collec-
tor. In Rajeev Alur and Thomas A. Henzinger, editors, Proceedings of the 8th international
conference on computer aided veriﬁcation (CAV’96), volume 1102 of Lecture Notes in Com-
puter Science, pages 462–465, New Brunswick, NJ, 1996. Springer-Verlag.
[Gonthier, 2005] Georges Gonthier. A computer-checked proof of the four colour theorem. Avail-
able at http://research.microsoft.com/~gonthier/4colproof.pdf, 2005.
[Gonthier, 2008] Georges Gonthier.
Formal proof — the four-color theorem.
Notices of the
AMS, 35:1382–1393, 2008.
[Good et al., 1979] D. I. Good, R. L. London, and W. W. Bledsoe.
An interactive program
veriﬁcation system. IEEE Transactions on Software Engineering, 1:59–67, 1979.
[Good, 1970] D. I. Good. Toward a Man-Machine System for Proving Program Correctness.
PhD thesis, University of Wisconsin, 1970.
[Good, 1983] D. I. Good. Proof of a distributed system in Gypsy. In M. J. Elphick, editor, For-
mal Speciﬁcation: Proceedings of the Joint IBM/University of Newcastle upon Tyne Seminar,
pages 44–89. University of Newcastle upon Tyne, Computing Laboratory, 1983.
[Goodstein, 1957] R. L. Goodstein. Recursive Number Theory. Studies in Logic and the Foun-
dations of Mathematics. North-Holland, 1957.
[Gordon et al., 1979] Michael J. C. Gordon, Robin Milner, and Christopher P. Wadsworth. Ed-
inburgh LCF: A Mechanised Logic of Computation, volume 78 of Lecture Notes in Computer
Science. Springer-Verlag, 1979.
[Gordon et al., 2006] Michael J. C. Gordon, James Reynolds, Warren Hunt, and Matt Kauf-
mann. An integration of HOL and ACL2. In Proceedings of the 6th international conference
on Formal Methods in Computer-Aided Design (FMCAD), pages 153–160. IEEE Computer
Society Press, 2006.
[Gordon, 1982] Michael J. C. Gordon.
Representing a logic in the LCF metalanguage.
In
D. N´eel, editor, Tools and notions for program construction: an advanced course, pages 163–
185. Cambridge University Press, 1982.

History of Interactive Theorem Proving
203
[Gordon, 1985] Mike Gordon. Why higher-order logic is a good formalism for specifying and
verifying hardware.
Technical Report 77, University of Cambridge Computer Laboratory,
New Museums Site, Pembroke Street, Cambridge, CB2 3QG, UK, 1985.
[Gordon, 1989] M. J. C. Gordon. Mechanizing programming logics in higher order logic. In
Birtwistle and Subrahmanyam [1989], pages 387–439.
[Gordon, 2000] M. J. C. Gordon.
From LCF to HOL: A short history.
In Gordon Plotkin,
Colin Stirling, and Mads Tofte, editors, Proof, language, and interaction: essays in honour
of Robin Milner. MIT Press, 2000.
[Gr´egoire et al., 2006] Benjamin Gr´egoire, Laurent Th´ery, and Benjamin Wener. A computa-
tional approach to Pocklington certiﬁcates in type theory. In Proceedings of the 8th Interna-
tional Symposium on Functional and Logic Programming, volume 3945 of Lecture Notes in
Computer Science, pages 97–113. Springer-Verlag, 2006.
[Griﬃn, 1988] Timothy Griﬃn. Efs - an interactive environment for formal systems. In Ewing L.
Lusk and Ross A. Overbeek, editors, 9th International Conference on Automated Deduction,
Argonne, Illinois, USA, May 23-26, 1988, Proceedings, volume 310 of LNCS, pages 740–741.
Springer, 1988.
[Guard et al., 1969] J. R. Guard, F. C. Oglesby, J. H. Bennett, and L. G. Settle. Semi-automated
mathematics. Journal of the ACM, 16:49–62, 1969.
[Gunter, 1993] Elsa L. Gunter. A broader class of trees for recursive type deﬁnitions for HOL.
In Joyce and Seger [1993], pages 141–154.
[Hales et al., 2010] Thomas C. Hales, John Harrison, Sean McLaughlin, Tobias Nipkow, Steven
Obua, and Roland Zumkeller. A revision of the proof of the Kepler conjecture. Discrete and
Computational Geometry, 44:1–34, 2010.
[Hales, 2005] Thomas C. Hales.
A proof of the Kepler conjecture.
Annals of Mathematics,
162:1065–1185, 2005.
[Hales, 2006] Thomas C. Hales. Introduction to the Flyspeck project. In Thierry Coquand,
Henri Lombardi, and Marie-Fran¸coise Roy, editors, Mathematics, Algorithms, Proofs, volume
05021 of Dagstuhl Seminar Proceedings. Internationales Begegnungs- und Forschungszentrum
fuer Informatik (IBFI), Schloss Dagstuhl, Germany, 2006.
[Hales, 2012] Thomas Hales. Dense Sphere Packings: A Blueprint for Formal Proofs, volume
400 of London Mathematical Society Lecture Note Series. Cambridge University Press, 2012.
[Hanna and Daeche, 1986] F. K. Hanna and N. Daeche.
Speciﬁcation and veriﬁcation using
higher-order logic: A case study.
In G. Milne and P. A. Subrahmanyam, editors, Formal
Aspects of VLSI Design: Proceedings of the 1985 Edinburgh Workshop on VLSI, pages 179–
213, 1986.
[Harper et al., 1987] Robert Harper, Furio Honsell, and Gordon D. Plotkin. A Framework for
Deﬁning Logics. In Proceedings, Symposium on Logic in Computer Science, 22-25 June 1987,
Ithaca, New York, USA, pages 194–204. IEEE Computer Society, 1987.
[Harrison and Th´ery, 1998] John Harrison and Laurent Th´ery. A sceptic’s approach to combin-
ing HOL and Maple. Journal of Automated Reasoning, 21:279–294, 1998.
[Harrison, 1995a] John Harrison. Inductive deﬁnitions: automation and application. In Phillip J.
Windley, Thomas Schubert, and Jim Alves-Foss, editors, Higher Order Logic Theorem Proving
and Its Applications: Proceedings of the 8th International Workshop, volume 971 of Lecture
Notes in Computer Science, pages 200–213, Aspen Grove, Utah, 1995. Springer-Verlag.
[Harrison, 1995b] John Harrison. Metatheory and reﬂection in theorem proving: A survey and
critique. Technical Report CRC-053, SRI Cambridge, Millers Yard, Cambridge, UK, 1995.
Available on the Web as http://www.cl.cam.ac.uk/users/jrh/papers/reflect.ps.gz.
[Harrison, 1996a] John Harrison. HOL Light: A tutorial introduction. In Mandayam Srivas
and Albert Camilleri, editors, Proceedings of the First International Conference on Formal
Methods in Computer-Aided Design (FMCAD’96), volume 1166 of Lecture Notes in Computer
Science, pages 265–269. Springer-Verlag, 1996.
[Harrison, 1996b] John Harrison. A Mizar mode for HOL. In Wright et al. [1996], pages 203–220.
[Harrison, 1996c] John Harrison.
Proof style.
In Eduardo Gim´enez and Christine Paulin-
Mohring, editors, Types for Proofs and Programs: International Workshop TYPES’96, vol-
ume 1512 of Lecture Notes in Computer Science, pages 154–172, Aussois, France, 1996.
Springer-Verlag.
[Harrison, 1998] John Harrison.
Theorem Proving with the Real Numbers.
Springer-Verlag,
1998. Revised version of author’s PhD thesis.

204
John Harrison, Josef Urban and Freek Wiedijk
[Harrison, 2000] John Harrison. Formal veriﬁcation of ﬂoating point trigonometric functions. In
Warren A. Hunt and Steven D. Johnson, editors, Formal Methods in Computer-Aided Design:
Third International Conference FMCAD 2000, volume 1954 of Lecture Notes in Computer
Science, pages 217–233. Springer-Verlag, 2000.
[Harrison, 2001] John Harrison. Complex quantiﬁer elimination in HOL. In Richard J. Boul-
ton and Paul B. Jackson, editors, TPHOLs 2001: Supplemental Proceedings, pages 159–
174. Division of Informatics, University of Edinburgh, 2001. Published as Informatics Re-
port Series EDI-INF-RR-0046. Available on the Web at http://www.informatics.ed.ac.uk/
publications/report/0046.html.
[Harrison, 2003] John Harrison. Isolating critical cases for reciprocals using integer factorization.
In Jean-Claude Bajard and Michael Schulte, editors, Proceedings, 16th IEEE Symposium on
Computer Arithmetic, pages 148–157, Santiago de Compostela, Spain, 2003. IEEE Computer
Society. Currently available from symposium Web site at http://www.dec.usc.es/arith16/
papers/paper-150.pdf.
[Harrison, 2006a] John Harrison. The HOL Light tutorial. Unpublished manual available at
http://www.cl.cam.ac.uk/~jrh13/hol-light/tutorial.pdf, 2006.
[Harrison, 2006b] John Harrison.
Towards self-veriﬁcation of HOL Light.
In Furbach and
Shankar [2006], pages 177–191.
[Harrison, 2007] John Harrison. Verifying nonlinear real formulas via sums of squares. In Schnei-
der and Brandt [2007], pages 102–118.
[Harrison, 2009a] John Harrison. Formalizing an analytic proof of the Prime Number Theorem
(dedicated to Mike Gordon on the occasion of his 60th birthday).
Journal of Automated
Reasoning, 43:243–261, 2009.
[Harrison, 2009b] John Harrison. Handbook of Practical Logic and Automated Reasoning. Cam-
bridge University Press, 2009.
[Heawood, 1890] Percy John Heawood. Map-colour theorem. Quarterly Journal of Pure and
Applied Mathematics, 24:332–338, 1890. Reprinted in Biggs et al. [1976].
[Henson, 1987] Martin C. Henson. Elements of functional languages. Blackwell Scientiﬁc, 1987.
[Hewett, 1992] Thomas T. Hewett. ACM SIGCHI curricula for human-computer interaction.
Technical report, New York, NY, USA, 1992.
[Hickey et al., 2003] Jason Hickey, Aleksey Nogin, Robert L. Constable, Brian E. Aydemir, Eli
Barzilay, Yegor Bryukhov, Richard Eaton, Adam Granicz, Alexei Kopylov, Christoph Kreitz,
Vladimir Krupski, Lori Lorigo, Stephan Schmitt, Carl Witty, and Xin Yu. MetaPRL - A
Modular Logical Environment.
In David A. Basin and Burkhart Wolﬀ, editors, Theorem
Proving in Higher Order Logics, 16th International Conference, TPHOLs 2003, Rom, Italy,
September 8-12, 2003, Proceedings, volume 2758 of LNCS, pages 287–303. Springer, 2003.
[Hoare, 1969] C. A. R. Hoare. An axiomatic basis for computer programming. Communications
of the ACM, 12:576–580, 583, 1969.
[Homeier, 2005] Peter V. Homeier. A design structure for higher order quotients. In Joe Hurd
and Tom Melham, editors, Theorem Proving in Higher Order Logics, 18th International
Conference, TPHOLs 2005, volume 3603 of Lecture Notes in Computer Science, pages 130–
146, Oxford, UK, 2005. Springer-Verlag.
[Homeier, 2009] Peter V. Homeier. The HOL-Omega logic. In Stefan Berghofer, Tobias Nipkow,
Christian Urban, and Makarius Wenzel, editors, Proceedings of the 22nd International Con-
ference on Theorem Proving in Higher Order Logics, TPHOLs 2009, volume 5674 of Lecture
Notes in Computer Science, pages 244–259, Munich, Germany, 2009. Springer-Verlag.
[Howe, 1992] Douglas J. Howe. Reﬂecting the semantics of reﬂected proof. In Peter Aczel, Harold
Simmons, and Stanley Wainer, editors, Proof Theory, pages 229–250. Cambridge University
Press, 1992.
[Huang et al., 1994] Xiaorong Huang, Manfred Kerber, Michael Kohlhase, Erica Melis, Dan
Nesmith, J¨orn Richts, and J¨org Siekmann. Omega-MKRP: A proof development environment.
In Bundy [1994], pages 788–792.
[Huet, 1975] G´erard Huet. A uniﬁcation algorithm for typed λ-calculus. Theoretical Computer
Science, 1:27–57, 1975.
[Hunt and Swords, 2009] Warren A. Hunt and Sol Swords.
Centaur Technology media unit
veriﬁcation. In Ahmed Bouajjani and Oded Maler, editors, CAV ’09: Proceedings of the 21st
International Conference on Computer Aided Veriﬁcation, volume 5643 of Lecture Notes in
Computer Science, pages 353–367, Grenoble, France, 2009. Springer-Verlag.

History of Interactive Theorem Proving
205
[Hunt, 1985] W. A. Hunt. FM8501: A Veriﬁed Microprocessor. PhD thesis, University of Texas,
1985. Published by Springer-Verlag as volume 795 of the Lecture Notes in Computer Science
series, 1994.
[Hurd, 1999] Joe Hurd. Integrating Gandalf and HOL. In Bertot et al. [1999], pages 311–321.
[Hurd, 2003] Joe Hurd.
First-order proof tactics in higher-order logic theorem provers.
In
Workshop on Design and Application of Strategies/Tactics in Higher Order Logics. NASA,
2003. NASA technical report NASA/CP-2003-212448.
[Hurd, 2010] Joe Hurd. The OpenTheory standard theory library. In C´esar Mu˜noz, editor, Pro-
ceedings of the Second NASA Formal Methods Symposium (NFM 2010), number NASA/CP-
2010-216215 in Technical Report, pages 177–191, Langley Research Center, Hampton VA
23681-2199, USA, 2010.
[Iverson, 1980] Kenneth E. Iverson.
Notation as a tool of thought.
Communications of the
ACM, 23:444–465, 1980.
[Ja´skowski, 1934] S. Ja´skowski.
On the rules of supposition in formal logic.
Studia Logica,
1:5–32, 1934.
[Jech, 1973] T. J. Jech. The Axiom of Choice, volume 75 of Studies in Logic and the Foundations
of Mathematics. North-Holland, 1973.
[Jensen and Wirth, 1974] Kathleen Jensen and Niklaus Wirth. Pascal user manual and report.
Springer-Verlag, 1974.
[Jeuring et al., 2012] J. Jeuring, J. Campbell, J. Carette, G. Dos Reis, P. Sojka, M. Wenzel,
and V. Sorge, editors. Intelligent Computer Mathematics - 11th International Conference,
AISC 2012, 19th Symposium, Calculemus 2012, 5th International Workshop, DML 2012,
11th International Conference, MKM 2012, Systems and Projects, Held as Part of CICM
2012, Bremen, Germany, July 8-13, 2012. Proceedings, volume 7362 of LNCS. Springer,
2012.
[Joyce and Seger, 1993] Jeﬀrey J. Joyce and Carl Seger, editors. Proceedings of the 1993 Inter-
national Workshop on the HOL theorem proving system and its applications, volume 780 of
Lecture Notes in Computer Science, UBC, Vancouver, Canada, 1993. Springer-Verlag.
[Kaivola and Aagaard, 2000] Roope Kaivola and Mark D. Aagaard. Divider circuit veriﬁcation
with model checking and theorem proving. In Aagaard and Harrison [2000], pages 338–355.
[Kaivola and Kohatsu, 2001] Roope Kaivola and Katherine Kohatsu. Proof engineering in the
large: Formal veriﬁcation of the Pentium (R) 4 ﬂoating-point divider. In T. Margaria and Tom
Melham, editors, 11th IFIP WG 10.5 Advanced Research Working Conference, CHARME
2001, volume 2144 of Lecture Notes in Computer Science, pages 196–211, Edinburgh, Scot-
land, 2001. Springer-Verlag.
[Kaliszyk and Krauss, 2013] Cezary Kaliszyk and Alexander Krauss. Scalable LCF-style proof
translation. In Sandrine Blazy, Christine Paulin-Mohring, and David Pichardie, editors, Proc.
of the 4th International Conference on Interactive Theorem Proving (ITP’13), volume 7998
of LNCS, pages 51–66. Springer Verlag, 2013.
[Kaliszyk and Urban, 2014a] Cezary Kaliszyk and Josef Urban. HOL(y)Hammer: Online ATP
service for HOL Light. Mathematics in Computer Science, 2014. http://dx.doi.org/10.
1007/s11786-014-0182-0.
[Kaliszyk and Urban, 2014b] Cezary Kaliszyk and Josef Urban.
Learning-assisted theorem
proving with millions of lemmas.
Journal of Symbolic Computation, 2014.
In press,
http://arxiv.org/abs/1402.3578 .
[Kaliszyk, 2007] Cezary Kaliszyk.
Web interfaces for proof assistants.
Electr. Notes Theor.
Comput. Sci., 174(2):49–61, 2007.
[Kaluzhnin, 1962] L. A. Kaluzhnin.
On an information language for mathematics.
Applied
Linguistic and Machine Translation, pages 21–29, 1962.
[Kapur, 1992] Deepak Kapur, editor. 11th International Conference on Automated Deduction,
volume 607 of Lecture Notes in Computer Science, Saratoga, NY, 1992. Springer-Verlag.
[Kaufmann and Paulson, 2010] Matt Kaufmann and Lawrence C. Paulson, editors. First In-
ternational Conference on Interactive Theorem Proving, ITP 2010, volume 6172 of Lecture
Notes in Computer Science, Edinburgh, UK, 2010. Springer-Verlag.
[Kaufmann et al., 2000a] Matt
Kaufmann,
Panagiotis
Manolios,
and
J
Strother
Moore.
Computer-Aided Reasoning: ACL2 Case Studies. Kluwer, 2000.
[Kaufmann et al., 2000b] Matt
Kaufmann,
Panagiotis
Manolios,
and
J
Strother
Moore.
Computer-Aided Reasoning: An Approach. Kluwer, 2000.

206
John Harrison, Josef Urban and Freek Wiedijk
[Keller and Werner, 2010] Chantal Keller and Benjamin Werner.
Importing HOL Light into
Coq. In Kaufmann and Paulson [2010], pages 307–322.
[Kempe, 1879] Alfred Braye Kempe. On the geographical problem of the four colours. American
Journal of Mathematics, 2:193–200, 1879. Reprinted in Biggs et al. [1976].
[King, 1969] J. C. King. A Program Veriﬁer. PhD thesis, Carnegie-Mellon University, 1969.
[Klein and Gamboa, 2014] Gerwin Klein and Ruben Gamboa, editors. Fifth International Con-
ference on Interactive Theorem Proving, ITP 2014, volume 8558 of Lecture Notes in Com-
puter Science, Vienna, Austria, 2014. Springer-Verlag.
[Klein et al., 2010] Gerwin Klein, Kevin Elphinstone, Gernot Heiser, June Andronick, David
Cock, Philip Derrin, Dhammika Elkaduwe, Kai Engelhardt, Rafal Kolanski, Michael Norrish,
Thomas Sewell, Harvey Tuch, and Simon Winwood. seL4: Formal veriﬁcation of an os kernel.
Communications of the ACM, 53:107–115, 2010.
[Knaster, 1927] B. Knaster. Un th´eor`eme sur les fonctions d’ensembles. Annales de la Soci´et´e
Polonaise de Math´ematique, 6:133–134, 1927. Volume published in 1928.
[Knoblock and Constable, 1986] T. Knoblock and R. Constable. Formalized metareasoning in
type theory. In Proceedings of the First Annual Symposium on Logic in Computer Science,
pages 237–248, Cambridge, MA, USA, 1986. IEEE Computer Society Press.
[Knuth and Bendix, 1970] Donald Knuth and Peter Bendix. Simple word problems in universal
algebras. In J. Leech, editor, Computational Problems in Abstract Algebra. Pergamon Press,
1970.
[Kraﬀt, 1981] Dean Blackmar Kraﬀt. AVID: a system for the interactive development of ver-
iﬁably correct programs. PhD thesis, Department of Computer Science, Cornell University,
1981.
[Krauss, 2010] Alexander Krauss. Partial and nested recursive function deﬁnitions in higher-
order logic. Journal of Automated Reasoning, 44:303–336, 2010.
[K¨uhlwein et al., 2012] Daniel K¨uhlwein, Twan van Laarhoven, Evgeni Tsivtsivadze, Josef Ur-
ban, and Tom Heskes.
Overview and evaluation of premise selection techniques for large
theory mathematics. In Bernhard Gramlich, Dale Miller, and Uli Sattler, editors, IJCAR,
volume 7364 of LNCS, pages 378–392. Springer, 2012.
[K¨uhlwein et al., 2013] Daniel K¨uhlwein, Stephan Schulz, and Josef Urban. E-MaLeS 1.1. In
Maria Paola Bonacina, editor, CADE, volume 7898 of Lecture Notes in Computer Science,
pages 407–413. Springer, 2013.
[Kumar and Weber, 2011] Ramana Kumar and Tjark Weber. Validating QBF validity in HOL4.
In Eekelen et al. [2011], pages 168–183.
[Kumar et al., 1991] Ramaya Kumar, Thomas Kropf, and Klaus Schneider. Integrating a ﬁrst-
order automatic prover in the HOL environment. In Archer et al. [1991], pages 170–176.
[Kumar et al., 2014] Ramana Kumar, Rob Arthan, Magnus O. Myreen, and Scott Owens. HOL
with deﬁnitions: Semantics, soundness and a veriﬁed implementation. In Klein and Gamboa
[2014], pages 308–324.
[Kun˘car, 2011] Ond˘rej Kun˘car. Proving valid quantiﬁed boolean formulas in HOL Light. In
Eekelen et al. [2011], pages 184–199.
[Kunen, 1998] Kenneth Kunen. Nonconstructive computational mathematics. Journal of Au-
tomated Reasoning, 21:69–97, 1998.
[Lakatos, 1976] Imre Lakatos. Proofs and Refutations: the Logic of Mathematical Discovery.
Cambridge University Press, 1976. Edited by John Worrall and Elie Zahar. Derived from
Lakatos’s Cambridge PhD thesis; an earlier version was published in the British Journal for
the Philosophy of Science vol. 14.
[Lam, 1990] C. W. H. Lam.
How reliable is a computer-based proof?
The Mathematical
Intelligencer, 12:8–12, 1990.
[Lamport and Melliar-Smith, 1985] Leslie Lamport and P. Michael Melliar-Smith. Synchroniz-
ing clocks in the presence of faults. Journal of the ACM, 32:52–78, 1985.
[Landin, 1966] Peter J. Landin. The next 700 programming languages. Communications of the
ACM, 9:157–166, 1966.
[Lecat, 1935] Maurice Lecat.
Erreurs de Math´ematiciens des origines `a nos jours.
Ancne
Libraire Castaigne et Libraire ´Em Desbarax, Brussels, 1935.
[Leroy, 2009] Xavier Leroy. Formal veriﬁcation of a realistic compiler. Communications of the
ACM, 52(7):107–115, 2009.

History of Interactive Theorem Proving
207
[Letichevsky et al., 2013] A.A. Letichevsky, A.V. Lyaletski, and M.K. Morokhovets. Glushkovs
evidence algorithm. Cybernetics and Systems Analysis, 49(4):489–500, 2013.
[London, 1970] R. L. London. Computer programs can be proved correct. In Proceedings of the
IVth Systems Symposium at Cape Western Reserve University. Springer-Verlag, 1970.
[Loveland, 1968] Donald W. Loveland. Mechanical theorem-proving by model elimination. Jour-
nal of the ACM, 15:236–251, 1968.
[Loveland, 1978] Donald W. Loveland.
Automated theorem proving: a logical basis.
North-
Holland, 1978.
[Luo, 1989] Zhaohui Luo.
ECC, an Extended Calculus of Constructions.
In Proceedings of
the Fourth Annual Symposium on Logic in Computer Science (LICS ’89), Paciﬁc Grove,
California, USA, June 5-8, 1989, pages 386–395. IEEE Computer Society, 1989.
[Luo, 1992] Zhaohui Luo. A unifying theory of dependent types: The schematic approach. In
Anil Nerode and Michael A. Taitslin, editors, Logical Foundations of Computer Science - Tver
’92, Second International Symposium, Tver, Russia, July 20-24, 1992, Proceedings, volume
620 of LNCS, pages 293–304. Springer, 1992.
[Luo, 2003] Zhaohui Luo.
PAL+:
a lambda-free logical framework.
J. Funct. Program.,
13(2):317–338, 2003.
[Lyaletski and Verchinine, 2010] Alexander V. Lyaletski and Konstantin Verchinine. Evidence
algorithm and system for automated deduction: A retrospective view.
In Autexier et al.
[2010], pages 411–426.
[Mac Lane, 1986] Saunders Mac Lane.
Mathematics: Form and Function.
Springer-Verlag,
1986.
[MacKenzie, 2001] Donald MacKenzie. Mechanizing Proof: Computing, Risk and Trust. MIT
Press, 2001.
[Magnusson and Nordstr¨om, 1993] Lena Magnusson and Bengt Nordstr¨om.
The ALF Proof
Editor and Its Proof Engine.
In Henk Barendregt and Tobias Nipkow, editors, Types for
Proofs and Programs, International Workshop TYPES’93, Nijmegen, The Netherlands, May
24-28, 1993, Selected Papers, volume 806 of LNCS, pages 213–237. Springer, 1993.
[Mamane and Geuvers, 2007] Lionel Mamane and Herman Geuvers. A document-oriented Coq
plugin for TeXmacs. In Manuel Kauers, Manfred Kerber, Robert Miner, and Wolfgang Wind-
steiger, editors, MKM 2007 - Work in Progress, volume 07-06 of RISC Report, pages 47–60.
University of Linz, Austria, 2007.
[Marcus et al., 1985] Leo Marcus, Stephen D. Crocker, and Jalsook R. Landauer.
SDVS: A
system for verifying microcode correctness. ACM Sigsoft Software Engineering Notes, 10(4):7–
14, 1985.
[Martin-L¨of, 1984] Per Martin-L¨of. Intuitionistic type theory. Bibliopolis, 1984.
[Maslov, 1964] S. Ju. Maslov. An inverse method of establishing deducibility in classical predi-
cate calculus. Doklady Akademii Nauk, 159:17–20, 1964.
[Matuszewski and Rudnicki, 2005] Roman Matuszewski and Piotr Rudnicki. Mizar: the ﬁrst 30
years. Mechanized Mathematics and Its Applications, 4:3–24, 2005.
[McAllester, 1989] David A. McAllester.
ONTIC: A Knowledge Representation System for
Mathematics. MIT Press, 1989.
[McCarthy, 1961] John McCarthy. Computer programs for checking mathematical proofs. In
Proceedings of the Fifth Symposium in Pure Mathematics of the American Mathematical
Society, pages 219–227. American Mathematical Society, 1961.
[McCune and Padmanabhan, 1996] W. McCune and R. Padmanabhan. Automated Deduction
in Equational Logic and Cubic Curves, volume 1095 of Lecture Notes in Computer Science.
Springer-Verlag, 1996.
[McCune, 1997] W. McCune. Solution of the Robbins problem. Journal of Automated Reason-
ing, 19:263–276, 1997.
[McLaughlin and Harrison, 2005] Sean McLaughlin and John Harrison. A proof-producing de-
cision procedure for real arithmetic. In Robert Nieuwenhuis, editor, CADE-20: 20th Inter-
national Conference on Automated Deduction, proceedings, volume 3632 of Lecture Notes in
Computer Science, pages 295–314, Tallinn, Estonia, 2005. Springer-Verlag.
[Mclaughlin et al., 2005] Sean Mclaughlin, Clark Barrett, and Yeting Ge. Cooperating theo-
rem provers: A case study combining HOL-Light and CVC Lite. In Proceedings of the 3rd
Workshop on Pragmatics of Decision Procedures in Automated Reasoning, volume 144 of
Electronic Notes in Theoretical Computer Science, 2005.

208
John Harrison, Josef Urban and Freek Wiedijk
[McLaughlin, 2006] Sean McLaughlin.
An interpretation of Isabelle/HOL in HOL Light.
In
Furbach and Shankar [2006], pages 192–204.
[McMillan, 2003] K. L. McMillan. Interpolation and SAT-based model checking. In Warren A.
Hunt and Fabio Somenzi, editors, Computer Aided Veriﬁcation, 15th International Confer-
ence, CAV 2003, volume 2725 of Lecture Notes in Computer Science, pages 1–13, Boulder,
CO, 2003. Springer-Verlag.
[Megill, 1996] Norman D. Megill. Metamath: A computer language for pure mathematics. Un-
published; available on the Web from ftp://ftp.shore.net/members/ndm/metamath.ps.gz.,
1996.
[Melham, 1989] Thomas F. Melham. Automating recursive type deﬁnitions in higher order logic.
In Birtwistle and Subrahmanyam [1989], pages 341–386.
[Melham, 1992] Thomas F. Melham. The HOL logic extended with quantiﬁcation over type
variables. In Claesen and Gordon [1992], pages 3–18.
[Melliar-Smith and Rushby, 1985] P. Michael Melliar-Smith and John Rushby. The Enhanced
HDM system for speciﬁcation and veriﬁcation.
ACM Software Engineering Notes, 10(4),
1985.
[Meng and Paulson, 2006] Jia Meng and L. C. Paulson. Translating higher-order problems to
ﬁrst-order clauses. In GeoﬀSutcliﬀe, Renate Schmidt, and Stephan Schulz, editors, ESCoR:
Empirically Succesful Computerized Reasoning, 2006.
[Milner, 1972] Robin Milner. Implementation and applications of Scott’s logic for computable
functions. ACM SIGPLAN Notices, 7(1):1–6, January 1972.
[Milner, 1978] Robin Milner.
A theory of type polymorphism in programming.
Journal of
Computer and Systems Sciences, 17:348–375, 1978.
[Moore and Wirth, 2013] J Strother Moore and Claus-Peter Wirth. Automation of mathemati-
cal induction as part of the history of logic. Available from http://arxiv.org/abs/1309.6226,
2013.
[Moore et al., 1998] J Strother Moore, Tom Lynch, and Matt Kaufmann.
A mechanically
checked proof of the correctness of the kernel of the AMD5K86 ﬂoating-point division pro-
gram. IEEE Transactions on Computers, 47:913–926, 1998.
[Morgan, 1990] Carroll Morgan. Programing from Speciﬁcations. Prentice-Hall, 1990.
[Morse, 1965] A. P. Morse. A theory of sets. Academic Press, 1965.
[Moskewicz et al., 2001] Matthew W. Moskewicz, Conor F. Madigan, Ying Zhao, Lintao Zhang,
and Sharad Malik. Chaﬀ: Engineering an eﬃcient SAT solver. In Proceedings of the 38th
Design Automation Conference (DAC 2001), pages 530–535. ACM Press, 2001.
[Myreen and Davis, 2014] Magnus O. Myreen and Jared Davis. The reﬂective Milawa theorem
prover is sound, down to the machine code that runs it. To appear in ITP 2014, 2014.
[Naumov et al., 2001] Pavel Naumov, Mark-Oliver Stehr, and Jos´e Meseguer. The HOL/NuPRL
proof translator (a practical approach to formal interoperability). In Richard J. Boulton and
Paul B. Jackson, editors, TPHOLs, volume 2152 of Lecture Notes in Computer Science, pages
329–345. Springer, 2001.
[Naumowicz and Bylinski, 2002] Adam Naumowicz and Czeslaw Bylinski.
Basic elements of
computer algebra in MIZAR. Mechanized Mathematics and Its Applications, 2, 2002.
[Naumowicz and Bylinski, 2004] Adam Naumowicz and Czeslaw Bylinski.
Improving Mizar
texts with properties and requirements.
In Andrea Asperti, Grzegorz Bancerek, and An-
drzej Trybulec, editors, MKM, volume 3119 of Lecture Notes in Computer Science, pages
290–301. Springer, 2004.
[Naur, 1966] Peter Naur. Proof of algorithms by general snapshots. BIT, 6:310–216, 1966.
[Necula and Lee, 2000] George C. Necula and Peter Lee. Proof generation in the Touchstone
theorem prover. In David McAllester, editor, Automated Deduction — CADE-17, volume 1831
of Lecture Notes in Computer Science, pages 25–44, Pittsburgh, PA, USA, 2000. Springer-
Verlag.
[Nederpelt et al., 1994] R. P. Nederpelt, J. H. Geuvers, and R. C. De Vrijer. Selected papers on
Automath. Studies in logic and the foundations of mathematics. Elsevier, Amsterdam, 1994.
[Nelson and Oppen, 1979] Greg Nelson and Derek Oppen. Simpliﬁcation by cooperating deci-
sion procedures. ACM Transactions on Programming Languages and Systems, 1:245–257,
1979.
[Nelson and Oppen, 1980] Greg Nelson and Derek Oppen. Fast decision procedures based on
congruence closure. Journal of the ACM, 27:356–364, 1980.

History of Interactive Theorem Proving
209
[Newell and Simon, 1956] A. Newell and H. A. Simon. The logic theory machine. IRE Trans-
actions on Information Theory, 2:61–79, 1956.
[Nipkow et al., 2002] Tobias Nipkow, Lawrence C. Paulson, and Markus Wenzel. Isabelle/HOL
— A Proof Assistant for Higher-Order Logic, volume 2283 of Lecture Notes in Computer
Science. Springer-Verlag, 2002.
[Nordstr¨om et al., 1990] Bengt Nordstr¨om, Kent Petersson, and Jan M. Smith. Programming
in Martin-L¨of’s Type Theory. Oxford Sciences Publication, Oxford, 1990.
[Obua and Skalberg, 2006] Steven Obua and Sebastian Skalberg.
Importing HOL into Is-
abelle/HOL. In Furbach and Shankar [2006], pages 298–302.
[O’Leary et al., 1999] John O’Leary, Xudong Zhao, Rob Gerth, and Carl-Johan H. Seger. For-
mally verifying IEEE compliance of ﬂoating-point hardware.
Intel Technology Journal,
1999-Q1:1–14, 1999. Available on the Web as http://download.intel.com/technology/itj/
q11999/pdf/floating_point.pdf.
[O’Leary et al., 2013] John O’Leary, Roope Kaivola, and Tom Melham. Relational STE and
theorem proving for formal veriﬁcation of industrial circuit designs. In Barbara Jobstmann
and Sandip Ray, editors, FMCAD 2013: Formal Methods in Computer-Aided Design, pages
97–104. IEEE, 2013.
[Owre et al., 1992] S. Owre, J. M. Rushby, and N. Shankar.
PVS: A prototype veriﬁcation
system. In Kapur [1992], pages 748–752.
[Papapanagiotou, 2007] Petros Papapanagiotou. On the automation of inductive proofs in HOL
Light. Master’s thesis, University of Edinburgh, 2007.
[Parrilo, 2003] Pablo A. Parrilo. Semideﬁnite programming relaxations for semialgebraic prob-
lems. Mathematical Programming, 96:293–320, 2003.
[Patkowska, 1969] Hanna Patkowska.
A homotopy extension theorem for fundamental se-
quences. Fundamenta Mathematicae, 64(1):87–89, 1969.
[Paul, 2008] Wolfgang Paul.
Towards a worldwide veriﬁcation techology.
In B. Meyer and
J. Woodcock, editors, Veriﬁed Software, volume 4171 of Lecture Notes in Computer Science,
pages 19–25. Springer-Verlag, 2008.
[Paulson and Blanchette, 2010] Lawrence C. Paulson and Jasmin Christian Blanchette. Three
years of experience with Sledgehammer, a practical link between automatic and interactive
theorem provers. In GeoﬀSutcliﬀe, Eugenia Ternovska, and Stephan Schulz, editors, Pro-
ceedings of the 8th International Workshop on the Implementation of Logics, pages 1–11,
2010.
[Paulson and Gr¸abczewski, 1996] Lawrence C. Paulson and Krzysztof Gr¸abczewski. Mechaniz-
ing set theory: Cardinal arithmetic and the axiom of choice. Journal of Automated Reasoning,
17:291–323, 1996.
[Paulson, 1983] Lawrence C. Paulson. A higher-order implementation of rewriting. Science of
Computer Programming, 3:119–149, 1983.
[Paulson, 1987] Lawrence C. Paulson. Logic and computation: interactive proof with Cambridge
LCF, volume 2 of Cambridge Tracts in Theoretical Computer Science. Cambridge University
Press, 1987.
[Paulson, 1990] Lawrence C. Paulson.
Isabelle:
The next 700 theorem provers.
In P. G.
Odifreddi, editor, Logic and Computer Science, volume 31 of APIC Studies in Data Pro-
cessing, pages 361–386. Academic Press, 1990.
[Paulson, 1994a] Lawrence C. Paulson. A ﬁxedpoint approach to implementing (co)inductive
deﬁnitions. In Bundy [1994], pages 148–161.
[Paulson, 1994b] Lawrence C. Paulson. Isabelle: a generic theorem prover, volume 828 of Lec-
ture Notes in Computer Science. Springer-Verlag, 1994. With contributions by Tobias Nipkow.
[Paulson, 2003] Lawrence C. Paulson. The relative consistency of the axiom of choice mecha-
nized using Isabelle/ZF. LMS Journal of Logic and Computation, 6:198–248, 2003.
[Petkovˇsek et al., 1996] Marko Petkovˇsek, Herbert S. Wilf, and Doron Zeilberger. A = B. A K
Peters, 1996.
[Pfenning and Sch¨urmann, 1999] Frank Pfenning and Carsten Sch¨urmann. System Description:
Twelf – A Meta-Logical Framework for Deductive Systems.
In Harald Ganzinger, editor,
Automated Deduction - CADE-16, 16th International Conference on Automated Deduction,
Trento, Italy, July 7-10, 1999, Proceedings, volume 1632 of LNCS, pages 202–206. Springer,
1999.

210
John Harrison, Josef Urban and Freek Wiedijk
[Pfenning, 1994] Frank Pfenning. Elf: A Meta-Language for Deductive Systems (System Des-
crition). In Alan Bundy, editor, Automated Deduction - CADE-12, 12th International Con-
ference on Automated Deduction, Nancy, France, June 26 - July 1, 1994, Proceedings, volume
814 of LNCS, pages 811–815. Springer, 1994.
[Ploegaerts et al., 1991] Wim Ploegaerts, Luc Claesen, and Hugo De Man. Deﬁning recursive
functions in HOL. In Archer et al. [1991], pages 358–366.
[Pnueli, 1977] A. Pnueli. The temporal logic of programs. In Proceedings of the 18th IEEE
Symposium on Foundations of Computer Science, pages 46–67, 1977.
[Pollack, 1994] Robert Pollack.
The Theory of LEGO – A Proof Checker for the Extended
Calculus of Constructions. PhD thesis, University of Edinburgh, 1994.
[Pratt, 1975] Vaughan Pratt. Every prime has a succinct certiﬁcate. SIAM Journal of Com-
puting, 4:214–220, 1975.
[Prawitz et al., 1960] Dag Prawitz, H˚aken Prawitz, and Neri Voghera.
A mechanical proof
procedure and its realization in an electronic computer.
Journal of the ACM, 7:102–128,
1960.
[Queille and Sifakis, 1982] J. P. Queille and J. Sifakis. Speciﬁcation and veriﬁcation of concur-
rent programs in CESAR. In Proceedings of the 5th International Symposium on Program-
ming, volume 137 of Lecture Notes in Computer Science, pages 195–220. Springer-Verlag,
1982.
[Rajan et al., 1995] S. Rajan, N. Shankar, and M. K. Srivas. An integration of model-checking
with automated proof-checking. In Pierre Wolper, editor, Computer-Aided Veriﬁcation: CAV
’95, volume 939 of Lecture Notes in Computer Science, pages 84–97, Liege, Belgium, 1995.
Springer-Verlag.
[Ramsey, 1926] F. P. Ramsey.
The foundations of mathematics.
Proceedings of the London
Mathematical Society (2), 25:338–384, 1926.
[Reif, 1995] W. Reif. The KIV-approach to software veriﬁcation. In M. Broy and S. J¨ahnichen,
editors, KORSO: Methods, Languages and Tools for the Construction of Correct Software —
Final Report, volume 1009 of Lecture Notes in Computer Science. Springer-Verlag, 1995.
[Robinson, 1965] J. A. Robinson. A machine-oriented logic based on the resolution principle.
Journal of the ACM, 12:23–41, 1965.
[Roxas, 1993] Rachel E. O. Roxas. A HOL package for reasoning about relations deﬁned by
mutual induction. In Joyce and Seger [1993], pages 129–140.
[Rudnicki and Drabent, 1985] Piotr Rudnicki and Wlodzimierz Drabent. Proving properties of
Pascal programs in MIZAR 2. Acta Inf., 22(3):311–331, 1985.
[Rudnicki and Trybulec, 1999] Piotr Rudnicki and Andrzej Trybulec. On equivalents of well-
foundedness. J. Autom. Reasoning, 23(3-4):197–234, 1999.
[Rudnicki and Trybulec, 2003] Piotr Rudnicki and Andrzej Trybulec.
On the integrity of a
repository of formalized mathematics. In MKM, volume 2594 of Lecture Notes in Computer
Science, pages 162–174. Springer, 2003.
[Rudnicki, 1987a] Piotr Rudnicki.
Obvious Inferences.
Journal of Automated Reasoning,
3(4):383–393, 1987.
[Rudnicki, 1987b] Piotr Rudnicki. Obvious inferences. Journal of Automated Reasoning, 3:383–
393, 1987.
[Rushby and von Henke, 1991] John Rushby and Friedrich von Henke. Formal veriﬁcation of
algorithms for critical systems. In Proceedings of the Conference on Software for Critical
Systems, pages 1–15. Association for Computing Machinery, 1991.
[Russell, 1919] Bertrand Russell.
Introduction to mathematical philosophy.
Allen & Unwin,
1919.
[Russinoﬀ, 1998] David Russinoﬀ.
A mechanically checked proof of IEEE compliance of a
register-transfer-level speciﬁcation of the AMD-K7 ﬂoating-point multiplication, division, and
square root instructions. LMS Journal of Computation and Mathematics, 1:148–200, 1998.
Available on the Web at http://www.russinoff.com/papers/k7-div-sqrt.html.
[Sawada and Gamboa, 2002] Jun Sawada and Ruben Gamboa.
Mechanical veriﬁcation of a
square root algorithms using Taylor’s theorem. In M. Aagaard and John O’Leary, editors,
Formal Methods in Computer-Aided Design: Fourth International Conference FMCAD 2002,
volume 2517 of Lecture Notes in Computer Science, pages 274–291. Springer-Verlag, 2002.
[Scheﬀer, 2003] Mark Scheﬀer. The Automath Archive, 2003.

History of Interactive Theorem Proving
211
[Schneider and Brandt, 2007] Klaus Schneider and Jens Brandt, editors.
Proceedings of the
20th International Conference on Theorem Proving in Higher Order Logics, TPHOLs 2007,
volume 4732 of Lecture Notes in Computer Science, Kaiserslautern, Germany, 2007. Springer-
Verlag.
[Schulz, 2000] Stephan Schulz.
Learning search control knowledge for equational deduction,
volume 230 of DISKI. Inﬁx Akademische Verlagsgesellschaft, 2000.
[Schulz, 2002] Stephan Schulz. E - A Brainiac Theorem Prover. AI Commun., 15(2-3):111–126,
2002.
[Scott, 1993] Dana Scott. A type-theoretical alternative to ISWIM, CUCH, OWHY. Theoretical
Computer Science, 121:411–440, 1993. Annotated version of a 1969 manuscript.
[Seger and Bryant, 1995] Carl-Johan H. Seger and Randal E. Bryant. Formal veriﬁcation by
symbolic evaluation of partially-ordered trajectories.
Formal Methods in System Design,
6:147–189, 1995.
[Seger and Joyce, 1991] Carl Seger and Jeﬀrey J. Joyce. A two-level formal veriﬁcation method-
ology using HOL and COSMOS. Technical Report 91-10, Department of Computer Science,
University of British Columbia, 2366 Main Mall, University of British Columbia, Vancouver,
B.C, Canada V6T 1Z4, 1991.
[Sewell et al., 2013] Thomas Sewell, Magnus O. Myreen, and Gerwin Klein. Translation vali-
dation for a veriﬁed OS kernel. In Proceedings of the 34th ACM SIGPLAN conference on
Programming language design and implementation, PLDI ’13, pages 471–482. Association for
Computing Machinery, 2013.
[Shankar, 1994] N. Shankar.
Metamathematics, Machines and G¨odel’s Proof, volume 38 of
Cambridge Tracts in Theoretical Computer Science. Cambridge University Press, 1994.
[Shawe-Taylor and Cristianini, 2004] John Shawe-Taylor and Nello Cristianini. Kernel Methods
for Pattern Analysis. Cambridge University Press, New York, NY, USA, 2004.
[Shostak, 1984] Robert Shostak. Deciding combinations of theories. Journal of the ACM, 31:1–
12, 1984.
[Siekmann and Wrightson, 1983] J. Siekmann and G. Wrightson, editors. Automation of Rea-
soning — Classical Papers on Computational Logic, Vol. I (1957-1966). Springer-Verlag,
1983.
[Slind, 1991] Konrad Slind.
An implementation of higher order logic.
Technical Report 91-
419-03, University of Calgary Computer Science Department, 2500 University Drive N. W.,
Calgary, Alberta, Canada, TN2 1N4, 1991. Author’s Masters thesis.
[Slind, 1996] Konrad Slind. Function deﬁnition in higher order logic. In Wright et al. [1996],
pages 381–398.
[Slobodov´a, 2007] Anna Slobodov´a. Challenges for formal veriﬁcation in industrial setting. In
Lubos Brim, Boudewijn R. Haverkort, Martin Leucker, and Jaco van de Pol, editors, Pro-
ceedings of 11th FMICS and 5th PDMC, volume 4346 of Lecture Notes in Computer Science,
pages 1–22. Springer-Verlag, 2007.
[Smullyan, 1992] Raymond M. Smullyan. G¨odel’s Incompleteness Theorems, volume 19 of Ox-
ford Logic Guides. Oxford University Press, 1992.
[Soko lowski, 1983] Stefan Soko lowski. A note on tactics in LCF. Technical Report CSR-140-83,
University of Edinburgh, Department of Computer Science, 1983.
[Solovyev and Hales, 2011] A. Solovyev and T. Hales. Veriﬁcation of bounds of linear programs.
In James H. Davenport, William M. Farmer, Florian Rabe, and Josef Urban, editors, Pro-
ceedings of CICM conference on intelligent computer mathematics, volume 6824 of Lecture
Notes in Computer Science, pages 123–132. Springer-Verlag, 2011.
[St˚almarck and S¨aﬂund, 1990] Gunnar St˚almarck and M. S¨aﬂund. Modeling and verifying sys-
tems and software in propositional logic. In B. K. Daniels, editor, Safety of Computer Control
Systems, 1990 (SAFECOMP ’90), pages 31–36, Gatwick, UK, 1990. Pergamon Press.
[Staples, 1999] Mark Staples. Linking ACL2 and HOL. Technical Report 476, University of
Cambridge Computer Laboratory, New Museums Site, Pembroke Street, Cambridge, CB2
3QG, UK, 1999.
[Stickel, 1988] Mark E. Stickel. A Prolog Technology Theorem Prover: Implementation by an
extended Prolog compiler. Journal of Automated Reasoning, 4:353–380, 1988.
[Syme, 1997] Donald Syme. DECLARE: A prototype declarative proof system for higher order
logic. Technical Report 416, University of Cambridge Computer Laboratory, New Museums
Site, Pembroke Street, Cambridge, CB2 3QG, UK, 1997.

212
John Harrison, Josef Urban and Freek Wiedijk
[Tankink et al., 2010] Carst Tankink, Herman Geuvers, James McKinna, and Freek Wiedijk.
Proviola: A tool for proof re-animation. In Autexier et al. [2010], pages 440–454.
[Tankink et al., 2012] Carst Tankink, Christoph Lange, and Josef Urban. Point-and-write. In
Jeuring et al. [2012], pages 169–185.
[Tankink et al., 2013] Carst Tankink, Cezary Kaliszyk, Josef Urban, and Herman Geuvers. For-
mal mathematics on display:
A wiki for Flyspeck.
In Jacques Carette, David Aspinall,
Christoph Lange, Petr Sojka, and Wolfgang Windsteiger, editors, MKM/Calculemus/DML,
volume 7961 of Lecture Notes in Computer Science, pages 152–167. Springer, 2013.
[Tarski, 1936] Alfred Tarski. Der Wahrheitsbegriﬀin den formalisierten Sprachen. Studia Philo-
sophica, 1:261–405, 1936.
English translation, ‘The Concept of Truth in Formalized Lan-
guages’, in Tarski [1956], pp. 152–278.
[Tarski, 1955] Alfred Tarski. A lattice-theoretical ﬁxpoint theorem and its applications. Paciﬁc
Journal of Mathematics, 5:285–309, 1955.
[Tarski, 1956] Alfred Tarski, editor. Logic, Semantics and Metamathematics. Clarendon Press,
1956.
[Th´ery and Hanrot, 2007] Laurent Th´ery and Guillaume Hanrot. Primality proving with elliptic
curves. In Schneider and Brandt [2007], pages 319–333.
[Trybulec, 1977] Andrzej Trybulec.
Informationslogische sprache Mizar.
Dokumentation-
Information: IX. Kolloquium ¨uber Information und Dokumentation vom 12. bis 14. Novem-
ber 1975, (33):46–53, 1977.
[Trybulec, 1978] Andrzej Trybulec.
The Mizar-QC/6000 logic information language.
ALLC
Bulletin, 6(2), 1978.
[Univalent Foundations Program, 2013] The Univalent Foundations Program. Homotopy Type
Theory: Univalent Foundations of Mathematics. Institute for Advanced Study, 2013.
[Urban and Sutcliﬀe, 2010] Josef Urban and GeoﬀSutcliﬀe. Automated reasoning and presen-
tation support for formalizing mathematics in Mizar. In Autexier et al. [2010], pages 132–146.
[Urban et al., 2008] Josef Urban, GeoﬀSutcliﬀe, Petr Pudl´ak, and Jiˇr´ı Vyskoˇcil.
MaLARea
SG1 - Machine Learner for Automated Reasoning with Semantic Guidance. In Alessandro
Armando, Peter Baumgartner, and Gilles Dowek, editors, IJCAR, volume 5195 of LNCS,
pages 441–456. Springer, 2008.
[Urban et al., 2010] Josef Urban, Jesse Alama, Piotr Rudnicki, and Herman Geuvers. A wiki
for Mizar: Motivation, considerations, and initial prototype. In Autexier et al. [2010], pages
455–469.
[Urban et al., 2011] Josef Urban, Jiˇr´ı Vyskoˇcil, and Petr ˇStˇep´anek. MaLeCoP: Machine learning
connection prover. In Kai Br¨unnler and George Metcalfe, editors, TABLEAUX, volume 6793
of LNCS, pages 263–277. Springer, 2011.
[Urban et al., 2013] Josef Urban, Piotr Rudnicki, and GeoﬀSutcliﬀe. ATP and presentation
service for Mizar formalizations. J. Autom. Reasoning, 50:229–241, 2013.
[Urban, 2005] Josef Urban. XML-izing Mizar: Making semantic processing and presentation of
MML easy. In Michael Kohlhase, editor, MKM, volume 3863 of Lecture Notes in Computer
Science, pages 346–360. Springer, 2005.
[Urban, 2006a] Josef Urban. MizarMode - an integrated proof assistance tool for the Mizar way
of formalizing mathematics. Journal of Applied Logic, 4(4):414 – 427, 2006.
[Urban, 2006b] Josef Urban.
MoMM - fast interreduction and retrieval in large libraries of
formalized mathematics. Int. J. on Artiﬁcial Intelligence Tools, 15(1):109–130, 2006.
[Urban, 2008] Christian Urban. Nominal techniques in Isabelle/HOL. JAR, 40:327–356, 2008.
[Urban, 2012] Josef Urban. Parallelizing Mizar. CoRR, abs/1206.0141, 2012.
[Urban, 2014] Josef Urban.
BliStr: The Blind Strategymaker.
CoRR, abs/1301.2683, 2014.
Accepted to PAAR’14.
[van Benthem Jutting, 1979] L.S. van Benthem Jutting. Checking Landau’s “Grundlagen” in
the Automath system. Number 83 in Mathematical Centre Tracts. Mathematisch Centrum,
Amsterdam, 1979.
[van Dalen, 1981] Dirk van Dalen, editor. Brouwer’s Cambridge lectures on intuitionism. Cam-
bridge University Press, 1981.
[van der Voort, 1992] Mark van der Voort.
Introducing well-founded function deﬁnitions in
HOL. In Claesen and Gordon [1992], pages 117–132.

History of Interactive Theorem Proving
213
[Verchinine et al., 2008] Konstantin Verchinine, Alexander Lyaletski, Andrei Paskevich, and
Anatoly Anisimov. On correctness of mathematical texts from a logical and practical point
of view. In Serge Autexier, John Campbell, Julio Rubio, Volker Sorge, Masakazu Suzuki, and
Freek Wiedijk, editors, Intelligent Computer Mathematics, AISC/Calculemus/MKM 2008,
volume 5144 of Lecture Notes in Computer Science, pages 583–598, Birmingham, United
Kingdom, July 2008. Springer.
[Voelker, 2007] Norbert Voelker. HOL2P — a system of classical higher order logic with second
order polymorphism. In Schneider and Brandt [2007], pages 333–350.
[Wang, 1960] Hao Wang.
Toward mechanical mathematics.
IBM Journal of Research and
Development, 4:2–22, 1960.
[Weber and Amjad, 2009] Tjark Weber and Hasan Amjad. Eﬃciently checking propositional
refutations in HOL theorem provers. Journal of Applied Logic, 7:26–40, 2009.
[Weber, 2010] Tjark Weber. Validating QBF invalidity in HOL4. In Kaufmann and Paulson
[2010], pages 466–480.
[Webster, 1995] Roger Webster. Convexity. Oxford University Press, 1995.
[Weis and Leroy, 1993] Pierre Weis and Xavier Leroy. Le langage Caml. InterEditions, 1993.
See also the CAML Web page: http://pauillac.inria.fr/caml/.
[Wenzel, 1999] Markus Wenzel. Isar - a generic interpretive approach to readable formal proof
documents. In Bertot et al. [1999], pages 167–183.
[Wenzel, 2012] Makarius Wenzel. Isabelle/jEdit - a prover IDE within the PIDE framework. In
Jeuring et al. [2012], pages 468–471.
[Weyhrauch and Talcott, 1994] Richard W. Weyhrauch and Carolyn Talcott. The logic of FOL
systems: Formulated in set theory. In Neil D. Jones, Masami Hagiya, and Masahiko Sato,
editors, Logic, Language and Computation: Festschrift in Honor of Satoru Takasu, volume
792 of Lecture Notes in Computer Science, pages 119–132. Springer Verlag, 1994.
[Weyhrauch, 1980] Richard W. Weyhrauch.
Prolegomena to a theory of mechanized formal
reasoning. Artiﬁcial Intelligence, 13:133–170, 1980.
[Weyhrauch, 1982] Richard W. Weyhrauch. An example of FOL using metatheory. In Donald W.
Loveland, editor, Proceedings of the 6th Conference on Automated Deduction, number 138 in
Lecture Notes in Computer Science, pages 151–158, New York, 1982. Springer Verlag.
[Whitehead and Russell, 1910] Alfred North Whitehead and Bertrand Russell. Principia Math-
ematica (3 vols). Cambridge University Press, 1910.
[Wiedijk, 2000] Freek Wiedijk. CHECKER - notes on the basic inference step in Mizar. available
at http://www.cs.kun.nl/∼freek/mizar/by.dvi, 2000.
[Wiedijk, 2001] Freek Wiedijk. Mizar light for HOL Light. In Richard J. Boulton and Paul B.
Jackson, editors, 14th International Conference on Theorem Proving in Higher Order Logics:
TPHOLs 2001, volume 2152 of Lecture Notes in Computer Science, pages 378–394. Springer-
Verlag, 2001.
[Wiedijk, 2002] Freek Wiedijk. A New Implementation of Automath. J. Autom. Reasoning,
29(3-4):365–387, 2002.
[Wiedijk, 2006] Freek Wiedijk. The Seventeen Provers of the World, volume 3600 of Lecture
Notes in Computer Science. Springer-Verlag, 2006.
[Wiedijk, 2007] Freek Wiedijk. Mizar’s soft type system. In Schneider and Brandt [2007], pages
383–399.
[Wiedijk, 2009] Freek Wiedijk.
Statistics on digital libraries of mathematics.
In Adam
Grabowski and Adam Naumowicz, editors, Computer Reconstruction of the Body of Mathe-
matics, volume 18(31) of Studies in Logic, Grammar and Rhetoric. University of Bia lystok,
2009.
[Wiedijk, 2012a] Freek Wiedijk. Pollack inconsistency. Electronic Notes in Theoretical Com-
puter Science, 285:85–100, 2012.
[Wiedijk, 2012b] Freek Wiedijk. A synthesis of the procedural and declarative styles of interac-
tive theorem proving. Logical Methods in Computer Science, 8(1), 2012.
[Wong, 1993] Wai Wong. Recording HOL proofs. Technical Report 306, University of Cambridge
Computer Laboratory, New Museums Site, Pembroke Street, Cambridge, CB2 3QG, UK,
1993.
[Wos et al., 1967] L. Wos, G. Robinson, D. Carson, and L. Shalla. The concept of demodulation
in theorem proving. Journal of the ACM, 14:698–709, 1967.

214
John Harrison, Josef Urban and Freek Wiedijk
[Wright et al., 1996] Joakim von Wright, Jim Grundy, and John Harrison, editors. Theorem
Proving in Higher Order Logics: 9th International Conference, TPHOLs’96, volume 1125 of
Lecture Notes in Computer Science, Turku, Finland, 1996. Springer-Verlag.
[Wright, 1994] Joakim von Wright. Representing higher-order logic proofs in HOL. In Thomas F.
Melham and Juanito Camilleri, editors, Higher Order Logic Theorem Proving and Its Appli-
cations: Proceedings of the 7th International Workshop, volume 859 of Lecture Notes in
Computer Science, pages 456–470, Valletta, Malta, 1994. Springer-Verlag.
[Wu, 1978] Wen-ts¨un Wu. On the decision problem and the mechanization of theorem proving
in elementary geometry. Scientia Sinica, 21:157–179, 1978.
[Young, 1993] W. D. Young. System veriﬁcation and the CLI stack. In Jonathan Bowen, editor,
Towards Veriﬁed Systems. Elsevier Science Publications, 1993.
[Zalewska, 2010] Gabriela Zalewska. Zamenhof, Ludwik (1859 – 1917). In The YIVO Encyclo-
pedia of Jews in Eastern Europe. YIVO Institute for Jewish Research, 2010.
[Zammit, 1999] Vincent Zammit.
On the implementation of an extensible declarative proof
language. In Bertot et al. [1999], pages 185–202.

AUTOMATION OF HIGHER-ORDER LOGIC
Christoph Benzm¨uller and Dale Miller
Readers: Peter Andrews, Jasmin Blanchette, William Farmer, Herman Geuvers,
and Bruno Woltzenlogel Paleo
1
INTRODUCTION
Early eﬀorts to formalize mathematics in order to make parts of it more rigorous
and to show its consistency started with the codiﬁcation of parts of logic. There
was work on the logical connectives by, for example, Boole and Pierce, and later
work to formalize additionally quantiﬁers (Frege, Church, Gentzen, etc.). Once
the basic concepts of logic—logical connectives, (ﬁrst-order) quantiﬁcation, and in-
ference rules—were formalized, the consistency of various ﬁrst-order logics was es-
tablished by G¨odel’s completeness theorem [1930a] and Gentzen’s cut-elimination
theorem [1969a; 1969b]. Equipped with such logical systems, logicians turned to
the formalizations of mathematics that had been started by Peano [1889] and
Hilbert [1899] and attempted to encode the objects of mathematics, such as real
numbers, sets, groups, etc., by building them on top of logic.
There are several ways to undertake such formalizations. One early and suc-
cessful approach involved building various theories for, say, Zermelo-Fraenkel set
theory, as ﬁrst-order logic extended with axioms postulating the existence of cer-
tain sets from the existence of other sets. Instead of sets, one could also explore the
use of algebra and universal properties to develop a categorical theory of mathe-
matics. This chapter addresses yet another approach to formalizing mathematical
concepts on top of quantiﬁcation logic: one can use higher-order quantiﬁcation
instead of only ﬁrst-order quantiﬁcation. In the syntax of ﬁrst-order logic, there
are terms and predicates: the terms denote individuals of some intended domain
of discourse, and predicates denote some subset of that domain. Inspired by set
theory, it is also natural to ask if certain predicates hold of other predicates, e.g., is
a given binary relation transitive or not. Other natural notions, such as Leibniz’s
equality—which states that x is equal to y if every predicate true of x is also true
of y—would naturally be written as the formula ∀P Px ⊃Py.1
Such higher-order quantiﬁcation was developed ﬁrst by Frege and then by Rus-
sell in his ramiﬁed theory of types, which was later simpliﬁed by others, including
Chwistek and Ramsey, Carnap, and ﬁnally Church in his simple theory of types
(STT), also referred to as classical higher-order logic.
1Occasionally we use the
notation in this paper to separate the body of quantiﬁed formulas
or λ-abstractions from the binder, and parentheses may be avoided if the formulas structure is
obvious. An alternative notation for ∀P Px ⊃Py would thus be ∀P(Px ⊃Py).
Handbook of the History of Logic. Volume 9: Computational Logic. 
Volume editor: Jörg Siekmann 
Series editors: Dov M. Gabbay and John Woods 
Copyright © 2014 Elsevier BV. All rights reserved.

216
Christoph Benzm¨uller and Dale Miller
1.1
Formalizing set comprehension as λ-abstraction
Church’s STT [1940], which is the focus of this chapter, based both terms and
formulas on simply typed λ-terms and the equality of terms and formulas is given
by equality of such λ-terms. The use of the λ-calculus had at least two major
advantages. First, λ-abstractions over formulas allow the explicit naming of sets
and predicates, something that is achieved in set theory via the comprehension
axioms. For example, if ϕ(x) is a formula with one free variable x, then the set
comprehension axiom provides the existence of the set {x ∈A | ϕ(x)}, for some
set A. Typed λ-abstraction achieves this in a simple step by providing the term
λx.ϕ(x): here, the variable x is given a type that, in principle, can be identiﬁed
with the same set A. Second, the complex rules for quantiﬁer instantiation at
higher-types is completely explained via the rules of λ-conversion (the so-called
rules of α-, β-, and η-conversion) which were proposed earlier by Church [1932;
1936].
Higher-order substitution can be seen (from the inference step point-of-view)
as one step, but it can pack a signiﬁcant computational inﬂuence on a formula:
normalization of λ-terms can be explosive (and expressive), in particular, since
λ-terms may contain logical connectives and quantiﬁers. Bindings are also treated
uniformly for all structures and terms that have bindings. For example, if p is
a variable of predicate type and A is the formula ∀p B(p), then the universal
instantiation of A with the term, say, t, namely the formula [t/p]B, can be a
formula with many more occurrences of logical connectives and quantiﬁers than
there are in the formula B(p).
1.2
Packing more into inference rules
Given that fewer axioms are needed in STT than in axiomatic set theory, and that
the term and formula structure is enriched, some earlier researchers in automated
theorem proving were attracted to STT since traditionally such early provers were
not well suited to deal with large numbers of axioms. If small theories could be
achieved by making the notion of terms and formula more complex, this seemed
like a natural choice. Thus, if one extended, say, ﬁrst-order resolution with a more
complex form of uniﬁcation (this time, on λ-terms), then one might be addressing
theorems that would otherwise need explicit notions of sets and their axioms.
G¨odel [1036] pointed out that higher-order logic (actually, higher-order arith-
metic) can yield much shorter proofs than are possible in ﬁrst-order logic. Parikh
[1973] proved this result years later: in particular, he proved that there exist arith-
metical formulas that are provable in ﬁrst-order arithmetic, but whose shortest
proofs in second-order arithmetic are arbitrarily smaller than any proof in ﬁrst-
order. Similarly, Boolos [1987] presented a theorem of ﬁrst-order logic comprising
about 60 characters but whose shortest proof (allowing the introduction and use
of lemmas) is so large that the known size of the universe could not be used to
record it: on the other hand, a proof of a few pages is possible using higher-order
logic.

Automation of Higher-Order Logic
217
The embedding of λ-conversion within inference rules that is available within
STT is related to modern approaches to making inference rules more expressive
by placing computation into them. A modern updating of this approach is found
in the work on deduction modulo [Dowek et al., 2003; Cousineau and Dowek, 2007]
where functional programming style computations on formulas and terms are per-
mitted within inference steps. Indeed, the connection between Church’s approach
to using higher-order quantiﬁcation and λ-terms can be closely simulated using
deduction modulo Burel [2011a]. Recent developments in focused proof systems
[Andreoli, 1992; Liang and Miller, 2009] provides a means of deﬁning large scale
inference rules that include possibly non-deterministic computation [Miller, 2011].
1.3
Plan of this chapter
We refer the reader looking for more details about higher-order logic and STT to
the textbook of Andrews [2002] and to the handbook and encyclopedia articles
by Andrews [2001; 2009], Enderton [2012], and Leivant [1994]. Another recom-
mended article has been contributed by Farmer [2008]. Here we shall focus on
the issues surrounding theorem proving, particularly, automated theorem proving
in subsets and variants of Church’s STT. In particular, in Section 2, we describe
some of the history and the background of formal treatments of quantiﬁcation and
the closely associated notions of binding and substitution. In Section 3, we present
the technical description of STT. The meta-theory of STT, including general mod-
els and cut-elimination, are addressed in Section 4. Skolemization, uniﬁcation,
pre-uniﬁcation, and pattern uniﬁcation, which are central concepts for proof au-
tomation, are discussed in Section 5.
Section 6 then addresses core challenges
for higher-order automated theorem provers, such as substitutions for predicate
variables and the automation of extensionality. An overview of interactive and
automatic theorem provers of (fragments of) STT is presented in Section 7. We
conclude brieﬂy in Section 8.
2
FORMALIZATION OF QUANTIFICATIONAL LOGIC
Quantiﬁcation is a key feature of natural language and its treatment has been
widely studied by linguists and logicians. A core interest has been to appropriately
match informal use of quantiﬁcational expressions in natural languages with their
formal treatment in logic formalisms. In that context, quantiﬁcation also plays
a pivotal role. However, the focus is on the widely adopted traditional notion of
universal and existential quantiﬁcation only.2 A crucial question is what kind of
objects an existential or universal quantiﬁer may range over, or, in other words,
what kind of objects the universe may contain. In classical ﬁrst-order logic these
objects are strictly of elementary nature, like the person ‘Bob’ or the number ‘5’;
2Traditional quantiﬁcation and generalized quantiﬁcation are contrasted in the text by We-
serst˚ahl [2011]; see also the references therein.

218
Christoph Benzm¨uller and Dale Miller
we call them ﬁrst-order objects. Not allowed are quantiﬁcations over properties of
objects (these are identiﬁed with sets of objects) or functions. In higher-order logic,
quantiﬁcation is not restricted to only elementary objects: quantiﬁcation over sets
or functions of objects is generally allowed. Peano’s induction principle for the
natural numbers is a good example. Using quantiﬁcation over ﬁrst-order objects
(∀x) and over properties (i.e., sets) of ﬁrst-order objects (∀P), this principle can
be elegantly expressed in higher-order logic by the axiom
∀P P0 ⊃(∀x (Px ⊃P(s x)) ⊃∀y Py),
where s denotes the successor function. This formula belongs to second-order logic,
since the variable P ranges only over sets of ﬁrst-order objects. In higher-order
logic one may move arbitrarily up in this hierarchy; that is, quantiﬁcations over
sets of sets of objects, etc, are allowed. First-order and second-order logic are, in
this sense, fragments of higher-order logic.
There are signiﬁcant theoretical and practical diﬀerences between ﬁrst-order
logic and higher-order logic regarding, for example, expressive power and proof
theory.
These fundamental diﬀerences—some of which will be outlined in the
next sections—have alienated many logicians, mathematicians, and linguists. A
(rather unfortunate) consequence has been that the community largely focused
on the theory and mechanization of ﬁrst-order logic. In particular automation
of higher-order logic, the topic of this article, has often been considered as too
challenging, at least until recently.
2.1
Earliest work on higher-order logic
Publication of Frege’s [1879] Begriﬀsschrift is commonly considered the birth of
modern symbolic logic. The Begriﬀsschrift presents a 2-dimensional formal no-
tation for predicate calculus and develops an adequate and still relevant notion
of formal proof. Frege’s notation for universal quantiﬁcation appropriately marks
both the bound variable and the scope of quantiﬁcation. Most importantly, quan-
tiﬁcation in Frege’s notation is not restricted to ﬁrst-order objects, and in his
notation the above induction axiom can be formalized as:3
P
y
P(y)
x
P(s(x))
P(x)
P(0)
Thus, the quantiﬁcation over predicate, relation and function symbols is explicitly
allowed in the language of the Begriﬀsschrift. A representative example of such
3Here
y
P(y) corresponds to ∀y Py and
P(s(x))
P(x)
to Px ⊃Psx; the rest is obvious.
The vertical bar on the left marks that the entire statement is asserted.

Automation of Higher-Order Logic
219
quantiﬁcation is Frege’s statement (76) in the Begriﬀsschrift, which described the
transitive closure of a relation.
The fact that Frege’s logic of the Begriﬀsschrift is indeed higher-order can
also be retraced by following some of his substitutions for relation symbols. For
example, in his derivation of statement (70), Frege substitutes a relation symbol
f with a function of one argument. In modern notation his concrete instantiation
can be expressed by the lambda term λz Fz ⊃∀a fza ⊃Fa. The support for
such kind of higher-order substitutions is another distinctive feature of higher-order
logic. Frege carries out this substitution and he implicitly applies normalization.
He does not give, however, a suﬃciently precise deﬁnition of how such substitutions
are applied.
It was Bertrand Russell [1902; 1903] who ﬁrst pointed out that unrestricted
quantiﬁcation, as considered by Frege, in connection with the comprehension
principles,4 enables the encoding of paradoxes and leads to inconsistency. The
most prominent such paradox, widely known as Russell’s paradox, involves the
set of all non-self-containing sets.
Russell [1908] suggested a few years later a
ramiﬁed theory of types as a paradox-free basis for the formalization of math-
ematics that diﬀerentiates between objects and sets (or functions) consisting of
these kinds of objects. On one hand, Russell was trying to avoid the paradoxes
that had plagued earlier work and he attributed some of the paradoxes to a vi-
cious circle principle in which some mathematical objects are deﬁned in terms
of collections that include the object being deﬁned.
In modern terms, Russell
wanted to disallow impredicative deﬁnitions and ramiﬁed types were one way to
avoid such impredicativity.
On the other hand, Russell was trying to reduce
mathematics to logic and since ramiﬁcation made it diﬃcult to encode mathe-
matics, Russell introduced the axiom of reducibility which essentially collapses
ramiﬁcations and allows one to still make impredicative deﬁnitions [Ramsey, 1926;
Chwistek, 1948].
The ramiﬁed theory of types was subsequently selected by Russell & Whitehead
as the logical foundation for the Principia Mathematica [Whitehead and Russell,
1910–1913]. They shared the philosophical standpoint of logicism, initiated by the
work of Frege, and they wanted to show that all mathematics can be reduced to
logic. The Principia succeeded to a signiﬁcant extent and an impressive number
of theorems of set-theory and ﬁnite and inﬁnite arithmetic, for example, were
systematically derived from the Principia’s logical foundations.
However, the
Principia was also criticized for its use of the axiom of reducibility and its use of
the axiom of inﬁnity, which asserts that a set with inﬁnitely many objects exists. It
thus remained debatable what Principia actually demonstrated: was it a reduction
of mathematics to logic or a reduction of mathematics to some (controversial) set
theory?
In the 1920s, a number of people suggested a simple theory of types as an
4The
comprehension
principles
assure
the
existence
of
abstractions
over
formula
expressions;
an
example
for
a
type
restricted
comprehension
axiom
(schema)
is
∃uα1→...→αn→o ∀x1 . . . ∀xn (ux1 . . . xn) = bo.

220
Christoph Benzm¨uller and Dale Miller
alternative to Russell’s ramiﬁed type theory.5 These suggestions led to the seminal
paper by Church [1940], which will be addressed in some detail in the next section.
The terms simple theory of types and classical higher-order logic typically refer to
the logic presented in [Church, 1940].
It should be remarked that the idea of employing a type hierarchy can, to some
extent, be attributed to Frege: in his writings he usually mentions explicitly the
kind of objects—predicates, predicates of predicates, etc.—a quantiﬁed variable is
representing (cf. [Quine, 1940]).
In summary, higher-order logic is an expressive formalism that supports quan-
tiﬁcation over predicate, relation, and function variables and that supports com-
plex substitutions of such variables. Such a rich language has several pitfalls with
which to be concerned. One such pitfall involves providing a technically precise
and sound notion of substitution involving bindings. Another (more important)
pitfall involves the careful treatment of self-referential, impredicative deﬁnitions
since these may lead to inconsistencies. A possible solution to the latter pitfall is
to consider syntactical restrictions based on type hierarchies and to use these to
rule out problematic impredicative deﬁnitions.
2.2
Diﬀerent notions of higher-order logic
The notion of higher-order when applied to logic formalisms is generally not as
unambiguous as the above text might suggest. We mention below three diﬀerent
groups of people who appear to use this term in three diﬀerent ways.
Philosophers of mathematics often distinguish between ﬁrst-order logic and
second-order logic only. The latter logic, which is used as a formal basis for all of
mathematics, involves quantiﬁcation over the domain of all possible functions. In
particular Kurt G¨odel’s work draws an important theoretical line between ﬁrst-
and second-order logic.
Shortly after proving completeness of ﬁrst-order logic
[1929; 1930b], G¨odel presented his celebrated ﬁrst incompleteness theorem [1931].
From this theorem it follows that second-order logic is necessarily incomplete: that
is, truth in higher-order logic can not be recursively axiomatized. Thus, higher-
order logic interpreted in this sense consists largely of a model-theoretic study,
typically of the standard model of arithmetic (cf. [Shapiro, 1985]).
Proof-theoreticians take logic to be synonymous with a formal system that pro-
vides a recursive enumeration of the notion of theoremhood. A higher-order logic
is understood no diﬀerently. The distinctive characteristic of such a logic, instead,
is the presence of predicate quantiﬁcation and of comprehension (i.e., the abil-
ity to form abstractions over formula expressions). These features, especially the
ability to quantify over predicates, profoundly inﬂuence the proof-theoretic struc-
ture of the logic. One important consequence is that the simpler induction argu-
ments of cut-elimination that are used for ﬁrst-order logic do not carry over to the
5In [1940], Church attributes the simple theory of types to Chwistek, Ramsey, and, ulti-
mately, Carnap. Simple type theory corresponds to the ramiﬁed theory of type plus the axiom
of reducibility.

Automation of Higher-Order Logic
221
higher-order setting and more sophisticated techniques, such as the “candidats de
r´eductibilit´e” due to Jean-Yves Girard [1971], must be used. Semantical methods
can also be employed, but the collection of models must now include non-standard
models that use restricted function spaces in addition to the standard models used
for second-order logic.
Implementers of deduction systems usually interpret higher-order logic as any
computational logic that employs λ-terms and quantiﬁcation at higher-order types,
although not necessarily at predicate types. Notice that if quantiﬁcation is ex-
tended only to non-predicate function variables,6 then the logic is similar to a
ﬁrst-order one in that the cut-elimination process can be deﬁned using an induc-
tion involving the sizes of (cut) formulas. However, such a logic may incorporate
a notion of equality based on the rules of λ-conversion and the implementation of
theorem proving in it must use (some form of) higher-order uniﬁcation.
3
CHURCH’S SIMPLE THEORY OF TYPES (CLASSICAL
HIGHER-ORDER LOGIC)
3.1
The λ-calculus as computation (middle and late 1930s)
The λ-calculus is usually thought of as a framework for computing functions. In the
setting of STT, however, where the discipline of simple types is applied, those func-
tions are greatly limited in what they can compute. A typical use of λ-conversion
in STT is to provide the function over syntax that instantiate a quantiﬁed formula
with a term. If one wants to describe the function that, for example, returns the
smallest prime divisor of an integer, one would specify relational speciﬁcations
of primality, division, etc., and then show that such relations are, in fact, total
functions. Thus, the foundations that STT provides to mathematics is based on
relations: this is in contrast to, say, the function-centric foundation of Martin-L¨of
type theory [1982]. It is worth pointing out that although typed λ-calculi are con-
sidered the quintessential proof structure for intuitionistic logic, Church, as the
father of the λ-calculus, has shown little interest in intuitionistic logic himself: in
particular, his development of STT was based on classical logic.
3.2
Mixing λ-calculus and logic
Church applied his λ-calculus to the description of not only quantiﬁcational struc-
tures and higher-order substitution but also many familiar mathematical construc-
tions. For example, the usual notion for membership x ∈P, i.e., “x is a member
of the set P”, can be written instead using the notion Px which is familiar from
ﬁrst-order logic, i.e., “the predicate P is true of x”. Thus, the concept of set is rep-
resented not as a primitive concept itself but is constructed using logic. Of course,
to allow interesting constructions involving sets, we need a higher-order logic that
6This is meant to also exclude, for example, ∀F(ι→o)→ι, where predicates can be used as
arguments.

222
Christoph Benzm¨uller and Dale Miller
allows operations on predicates that can mimic similar operations on sets. For
example, if predicates A and B denote sets then the expressions λx Ax ∧Bx
and λx Ax ∨Bx denote the intersection and union of the sets described by these
predicates.
Furthermore, λC ∀x Cx ⊃Ax describes the power-set of A (i.e.,
the set of sets C that are subsets of A).
We can even use λ-abstractions to
make more abstractions possible: the notion of set union, for example, can be
deﬁned as the λ-abstraction λA λB λx Ax ∧Bx and the notion of power-set can
be λA λC ∀x Cx ⊃Ax.
As Kleene and Rosser [1935] discovered, a direct mixing of the λ-calculus with
logic can lead to an inconsistent logic. One of the simplest presentations of an
inconsistency arising from mixing the untyped λ-calculus with (classical) logic is
called Curry’s paradox [1942]. Let y be a formula and let r be the λ-abstraction
λx.xx ⊃y. Via λ-conversion rr is equal and, hence, equivalent to rr ⊃y. Hence,
we have the two implications rr ⊃(rr ⊃y) and (rr ⊃y) ⊃rr. From the former
we get (by contracting assumptions) rr ⊃y, and hence, by modus ponens with
the latter we know rr. By a further modus ponens step we thus get y. Since y
was an arbitrary formula, we have proved a contradiction.
One way to avoid inconsistencies in a logic extended with the λ-calculus is to
adopt a variation of Russell’s use of types (thereby eliminating the self application
rr in the above counterexample). When Church modiﬁed Russell’s ramiﬁed theory
of types to a “simple theory” of types, the core logic of this chapter was born
[Church, 1940]. Mixing λ-terms and logic as is done in STT permits capturing
many aspects of set theory without direct reference to axioms of set theory.
There are costs to using the strict hierarchy of sets enforced by typing: no set
can contain both a member of A as well as a subset of A. The deﬁnition of subset
is based on a given type: asking if a set of integers is a subset of another set of
integers is necessarily diﬀerent than asking if a binary relation on integers is a
subset of another set of binary relations on integers.
3.3
Simple types and typed λ-terms
The primitive types are of two kinds: o is the type of propositions and the rest
are the types of basic domains of individuals: thus we are adopting a many-
sorted approach to logic. However, analogously to Church [1940], we shall just
admit one additional primitive type, namely, ι, for the domain of individuals which
correspond to the domain of ﬁrst-order variables. (Admitting more than this one
additional primitive type is no challenge.) The set of type expressions is the least
set containing the primitive types and such that (γ →τ) is a type expression
whenever γ and τ are type expressions. Here, types of the form (γ →τ) denote
the type of functions with domain type γ and codomain type τ. If parentheses
are omitted, we shall assume that the arrow constructor associates to the right:
i.e., δ →γ →τ denotes (δ →(γ →τ)). The order ord(τ) of a type τ is deﬁned

Automation of Higher-Order Logic
223
by structural recursion:7 if τ is a primitive type, then ord(τ) = 0; otherwise
ord(γ →τ) = max(ord(γ) + 1, ord(τ)). Thus, the order of ι →ι →ι is 1 and the
order of (ι →ι) →ι is 2.
Let Σ be a set of typed constants, i.e., tokens with a subscript that is a simple
type and denote by Στ the subset of Σ of constants with subscript τ. Lowercase
letters with subscripts, e.g., cτ, are syntactic variables ranging over constants with
subscript τ. For each type τ, let Vτ be an inﬁnite set of variables x1
τ, x2
τ, . . ., all with
subscript τ. The uppercase letters X, Y , and Z with type expression subscripts are
syntactic variables ranging over particular variables: e.g., Xτ is a syntactic variable
ranging over the particular variables in Vτ. Subscripts of syntactic variables may
be omitted when they are obvious or irrelevant in a particular context. Given the
constants in Σ and variables in Vτ (τ ∈T ), we can now deﬁne the binary relation
of a term with a type as the least relation satisfying the following clauses.
1. If cτ ∈Σ then cτ is a term of type τ.
2. If Xτ ∈Vτ then Xτ is a term of type τ.
3. If Xτ is a variable with subscript τ and Mγ is a term of type γ, then (λXτ Mγ)
is a term of type (τ →γ).
4. If Fτ→γ is a term of type (τ →γ), and Aτ is a term of type τ, then (Fτ→γ Aτ)
is a term of type γ.
The uppercase letters A, B, C, and M with type expression subscripts, e.g., Mτ,
are syntactic variables ranging over terms of their displayed type. The parentheses
in (λXτ Mγ) and (Fτ→γ Aτ) can be omitted if it is clear from context how to
uniquely insert them.
Each occurrence of a variable in a term is either bound by a λ or free. We
consider two terms A and B to be equal (and write A ≡B), if they are the same
up to a systematic change of bound variable names (i.e., we consider α-conversion
implicitly). A term A is closed if it has no free variables.
A substitution of a term Aα for a variable Xα in a term Bβ is denoted by
[A/X]B. Since we consider α-conversion implicitly, we assume that the bound
variables of B are appropriately renamed to avoid variable capture. For example,
[x1
ι /x2
ι ]λx1 x2 is equal to, say, λx3 x1, which is not equal to λx1 x1.
Two important relations on terms are given by β-reduction and η-reduction. A
β-redex (λXA)B (i.e., the application of an abstraction to an argument) β-reduces
to [B/X]A (i.e., the substitution of an actual argument for a formal argument).
If X is not free in C, then λX(CX) is an η-redex and it η-reduces to C. If A
β-reduces to B then we say that B β-expands to A. Similarly, if A η-reduces to
B then we say that B η-expands to A. For terms A and B of the same type, we
write A≡βB to mean A can be converted to B by a series of β-reductions and
expansions. Similarly, A≡βηB means A can be converted to B using both β- and
7Diﬀerent notions of ‘order’ have actually been discussed in the literature. We may, e.g., start
with ord(ι) = 0 and ord(o) = 1.

224
Christoph Benzm¨uller and Dale Miller
η-conversion. For each simply typed λ-term A there is a unique β-normal form
(denoted A↓β) and a unique βη-normal form (denoted A↓βη). From this fact we
know A ≡β B (A ≡βη B) if and only if A↓β ≡B↓β (A↓βη ≡B↓βη).
The simply typed λ-terms of Church [1940] are essentially the ones in common
use today (cf. [Barendregt, 1997; Barendregt et al., 2013]). One subtlety is that
all variables and constants carry with them their type as part of their name: that
is, constants and variable are not associated with a type which could vary with
diﬀerent type contexts. Instead, constants and variables have ﬁxed types just as
they have ﬁxed names: thus, the variable fι→ι has the name fι→ι and the type
ι →ι. This handling of type information is also called Church-style (as opposed
to Curry-style). In this paper we often omit the type subscript if it can easily be
inferred in the given context.
3.4
Formulas as terms of type o
In most presentations of ﬁrst-order logic, terms can be components of formulas
but formulas are never components of terms. Church’s STT allows for this later
possibility as well: formulas and logical connectives are allowed within terms. Such
a possibility will greatly increase both the expressive strength of the logic and the
complexities of automated reasoning in the logic. STT achieves this intertwining
of terms and formulas by using the special primitive type o to denote those simply
typed terms that are the formulas of STT. Thus, we introduce logical connectives as
speciﬁc constant constructors of type o. Since Church’s version of STT was based
on classical logic, he chose for the primitive logical connectives ¬o→o for negation,
∨o→o→o for disjunction, and for each type γ, a symbol ∀(γ→o)→o. Other logical
connectives, such as ∧o→o→o, ⊃o→o→o, and ∃(γ→o)→o (for every type γ), can be
introduced by either also treating them as primitive or via deﬁnitions. A formula
or proposition of STT is a simply typed λ-term of type o and a sentence of STT is a
closed proposition (i.e., containing no free variables). In order to make the syntax
of λ-terms converge more to the conventional syntax of logical formulas, we shall
adopt the usual conventions for quantiﬁed expressions by using the abbreviations
∀xγ.B and ∃xγ.B for ∀(γ→o)→oλxγ.B and ∃(γ→o)→oλxγ.B, respectively. Similarly,
the familiar inﬁx notion B ∨C, B ∧C, and B ⊃C abbreviate the expressions
((∨o→o→oB)C), ((∧o→o→oB)C), and ((⊃o→o→o B)C), respectively.
Beyond the logical connectives and quantiﬁers of classical logic, STT also con-
tains the constant ι(γ→o)→γ for each simple type γ. This constant is axiomatized
in STT so that ι(γ→o)→γB denotes a member of the set that is described by the
expression B of type γ →o. Thus, ι(γ→o)→γ is used variously as a description
operator or a choice function depending on the precise set of axioms assumed for
it. Choice selects a member from B if B is non-empty and description selects a
member from B only if B is a singleton set.

Automation of Higher-Order Logic
225
3.5
Elementary type theory
Church [1940] gives a Frege-Hilbert style logical calculus for deriving formulas.
The inference rules can be classiﬁed as follows.
I–III. One step of α-conversion, β-reduction, or β-expansion.
IV. Substitution: From Fτ→oXτ, infer Fτ→oAτ if X is not free in F.
V. Modus Ponens: From A ⊃B and A, infer B.
VI. Generalization: From Fτ→oXτ, infer ∀(τ→o)→oFτ→o if X is not free in F.
In addition to the inference rules, Church gives various axiom schemas. Consider
ﬁrst the following axiom schemas.
1–4. Classical propositional axioms
5τ. For every simple type τ, ∀(τ→o)→oFτ→o ⊃FX.
6τ. For every simple type τ, ∀Xτ(po ∨Fτ→oX) ⊃p ∨∀(τ→o)→oF.
These axioms (together with the inference rules above) describe the theorems of
what is often called elementary type theory (ETT) [Andrews, 1974]: these axioms
simply describe an extension of ﬁrst-order logic with quantiﬁcation at all simple
types and with the term structure upgraded to be all simply typed λ-terms. In
the last century, much of the work on the automation of higher-order logic focused
on the automation of the elementary type theory.
3.6
Simple type theory
In order to encode mathematical concepts, additional axioms are needed which,
in turn, requires that we introduce expressions for denoting equality and natural
numbers.
Equality
Equality for terms of type τ, Aτ
.=τ Bτ, is deﬁned using Leibniz’s
formula ∀Pτ→o PA ⊃PB. By Aτ ̸ .=τ Bτ we mean ¬(Aτ .=τ Bτ).
Natural numbers
An individual natural number n is denoted by the Church
numeral encoding n-fold iteration [Church, 1936]. Thus, the following denote the
λ-calculus encoding of zero, one, two and three (here, τ is any simple type).
λfτ→τλxτ x
λfτ→τλxτ fx
λfτ→τλxτ f(fx)
λfτ→τλxτ f(f(fx))
Notice that if we denote by ˆτ the type (τ →τ) →τ →τ, then all these terms
are of type ˆτ. The λ-abstraction λnˆτλfτ→τλxτ .f(nfx) is denoted Sˆτ→ˆτ and has
the property that it computes the successor of a number encoded in this fashion.

226
Christoph Benzm¨uller and Dale Miller
The set of all natural numbers (based on iteration of functions over type τ) can
be deﬁned as the λ-abstraction
λnˆτ∀pˆτ→o (p0ˆτ ⊃((∀xˆτ px ⊃p(Sˆτ→ˆτx)) ⊃pn))
of type ˆτ →o. This expression uses higher-order quantiﬁcation to deﬁne the set of
all natural numbers as being the set of terms n that are members of all sets that
contain zero and are closed under successor. It is unfortunate that the encoding
of numbers is dependent on a speciﬁc type: in other words, there is a diﬀerent
set of natural numbers for every type τ. The polymorphic type system of Girard
[1971; 1986] and Reynolds [1974] ﬁxed this problem by admitting within λ-terms
the ability to abstract and apply types.
Adding the following axioms to those of the elementary type theory yields
Church’s simple theory of types (STT).
7. There exists at least two individuals: ∃XιYι X ̸ .=ι Y .
8. Inﬁnity: The successor function on Church numerals at type (ι →ι) →ι →ι
is injective.
9τ. Description: Fτ→oXτ ⊃(∀Yτ FY ⊃X .= Y ) ⊃F(ι(τ→o)→τF).
10τ→γ. Functional extensionality: (∀Xτ FX .= GX) ⊃F .=τ→γ G.
11τ. Choice: Fτ→oXτ ⊃F(ι(τ→o)→τF).
Church also mentions the possibility of including an additional axiom of exten-
sionality of the form P ⇔Q ⊃P .=o Q. In fact, Henkin [1950] includes this axiom
as part of his Axiom 10 (and he excludes axioms 7–9τ). We follow Henkin and
include the following axiom as part of Axiom 10.
10o. Boolean extensionality: P ⇔Q ⊃P .=o Q.
The description axioms (Axioms 9τ) allow us to use ι(τ→o)→τ to extract the
unique element of any singleton set. If we assume the description axioms, then
we can prove that every functional relation corresponds to a function. That is, we
can prove
(∀xτ∃yγ rτ→γ→oxy ∧∀zγ (rxz ⊃y .= z)) ⊃∃fτ→γ∀xτ rx(fx).
This fact may be used to justify restricting the relational perspective for the foun-
dation of mathematics since functions are derivable from relations in STT.
The choice axioms (Axioms 11τ) are strictly stronger than the description ax-
ioms (see [Andrews, 1972a]), i.e., choice implies description.
Many interactive
theorem provers include a choice operator, but the systematic inclusion of choice
(or description) into automated procedures has only happened recently (see also
Section 6).
Finally, Axioms 7 and 8 guarantee that there will be inﬁnitely many individuals.
There are many ways to add an axiom of inﬁnity and Church’s choice is convenient
for developing some basic number theory using Church numerals.

Automation of Higher-Order Logic
227
3.7
Variants of elementary and simple type theory
Besides the various subsets of STT that involve choosing diﬀerent subsets of the
axioms to consider, other important variants of ETT and STT have been devel-
oped.
Adding to ETT the axioms of Boolean and functional extensionality (Axioms
10o and 10τ→β), and possibly choice, gives a theory sometimes called extensional
type theory (ExTT): equivalently STT without description, inﬁnity and axiom 7.
This is the logic studied by [Henkin, 1950], and it is the logic that is automated by
the theorem provers described in Section 7.3. One cannot prove from ETT alone
that η-conversion preserves the equality of terms: a fact that is provable, however,
using ExTT. Also, Boolean extensionality can be considered without including
functional extensionality and vice versa. Most modern implementations of ETT
generally treat the equality of typed λ-terms up to βη-conversion. By doing this
some weak form of extensionality is thus automatically guaranteed. However, this
should not be confused with supporting full functional and Boolean extensionality
(cf. the discussion of non-functional models and extensionality in Section 4.1).
While Church axiomatized the logical connectives of STT in a rather conven-
tional fashion (using, for example, negation, conjunction, and universal quantiﬁca-
tion as the primitive connectives), [Henkin, 1963] and [Andrews, 1972a; Andrews,
2002] provided a formulation of STT in which the sole logical connective was
equality (at all types). Not only was a formulation of logic using just this one log-
ical connective perspicuous, it also improved on the semantics of Henkin’s general
models [1950].
Probably the most signiﬁcant other variant of STT is one that replaces the
classical logic underlying it with intuitionistic logic: several higher-order logic
systems (e.g., Coq) are based on such a variant of STT. Intuitionistic variants of
STT are easily achieved by changing the logical axioms of STT from those for
classical logic to those for intuitionistic logic.
A logic of unity
ETT (and analogously ExTT or STT) provides a framework
for considering propositional logic and ﬁrst-order logic as fragments of higher-order
logic. In the era of computer implementations of logic, this unifying aspect of ETT
is of great value: an implementation of aspects of ETT immediately can be seen
as an implementation that is also eﬀective for these two kinds of simpler logics.
3.8
An example
Consider formalizing the most basic notions of point-set topology in STT. First, we
formalize some simple set-theoretic concepts using the following typed constants
and λ-expressions.

228
Christoph Benzm¨uller and Dale Miller
empty set
∅ι→o
λx.x ̸ .= x (or λx.⊥)
membership
∈τ→(τ→o)→o
λxλC Cx
subset
⊆(ι→o)→(ι→o)→o
λAλB∀x Ax ⊃Bx
intersection
∩(ι→o)→(ι→o)→ι→o
λAλBλx Ax ∧Bx
family union
S
((ι→o)→o)→ι→o
λDλx∃C DC ∧Cx
We now deﬁne the symbol open((ι→o)→o)→(ι→o)→o so that (open C S) holds when
C is a topology (a collection of open sets) on S. Informally, this should hold when
C is a set of subsets of S such that C contains the empty set as well as the set S
and it is closed under (binary) intersections and arbitrary unions. Formally, the
symbol open can be deﬁned as the λ-abstraction
λCλS.(∅∈C) ∧(S ∈C) ∧[∀A∀B.(A ∈C ∧B ∈C ⊃(A ∩B) ∈C]
∧[∀B.(B ⊆C) ⊃(S B) ∈C]
A simple fact about open sets is the following. Assume that C is a topology for S.
If G is a subset of S and all elements of G are also members of an open set (i.e.,
of a member of C) that is a subset of G, then G itself is open. We can formalize
this theorem as the following formula in STT.
∀C∀S∀G. (open C S) ⊃[∀x. x ∈G ⊃∃S. S ∈C ∧x ∈S ∧S ⊆G] ⊃(G ∈C)
This formula is provable in STT if we employ the functional extensionality axiom
10ι→o in order to show that the two predicates G and
[
(λH. (open C H) ∧(H ⊆G))
(both of type ι →o) are equal. Since it is an easy matter to prove that this second
expression is in C, Leibniz’s deﬁnition of equality immediately concludes that G
must also be in C.
One weakness of using STT for formalizing an abstract notion of topology is that
we provided above a deﬁnition in which open sets were sets of individuals: that is,
they were of type ι →o. Of course, it might be interesting to consider topologies
on other types, for example, on sets of sets. We could adopt the technique used
in [Church, 1940] of indexing most notions with types, such as, for example, ⊆τ.
More expressive logics with richer treatment of types and their quantiﬁcation are
desirable: examples of such logics include Girard’s System F [1986], Reynold’s
polymorphic λ-calculus [1974], and Andrews’s transﬁnite type system [1965].
3.9
Church used diﬀerent syntax not adopted here
Church’s introduction of λ as a preﬁx operator to denote function abstraction
and the use of juxtaposition to denote function application is now well established
syntax. On the other hand, Church used a number of syntactic conventions and
choices that appear rather odd to the modern reader. While Church used a simpli-
ﬁcation of the dot notation used in [Whitehead and Russell, 1910–1913], most uses

Automation of Higher-Order Logic
229
of dots in syntax have been dropped in modern systems, although a dot is some-
times retained to separate a bound variable from the body of a λ-term. Church
similarly used concatenation to denote function types, but most modern systems
use an arrow. The use of omicron as the type for propositions survives in some
systems while many other systems use Prop (the latter is used more frequently in
systems for ETT, while the former seems more prominent in systems for ExTT;
see also the distinction between types prop and bool in the Isabelle system; more
provers for ETT and ExTT are presented in Section 7). Similarly, the connectives
∀(γ→o)→o and ∃(γ→o)→o are often replaced by the binders ∀and ∃, respectively, al-
though they are often used to denote quantiﬁcation at the level of types in certain
strong type systems.
4
META-THEORY
Church [1940] proved that the deduction theorem holds for the proof system con-
sisting of the axioms and inference rules described in Section 3. The availability
of the deduction theorem means that the familiar style of reasoning from assump-
tions is valid in STT. Church also proved a number of theorems regarding natural
numbers and the possibility of deﬁning functions using primitive recursive deﬁni-
tions. The consistency of STT and a formal model theory of STT were left open
by Church.
4.1
Semantics and cut-elimination
We outline below several major meta-theoretic results concerning STT and closely
related logics.
Standard models
G¨odel’s incompleteness theorem [1931] can be extended di-
rectly to ETT (and ExTT or STT) since second-order quantiﬁcation can be used
to deﬁne Peano arithmetic: that is, there is a “true” formula of ETT (or any
extension of it) that is not provable. The notion of truth here, however, is that
arising from what is called the standard model of ETT (resp. any extension of it)
in which a functional type, say, γ →τ, contains all functions from the type γ to
the type τ. Moreover, the type o is assumed to contain exactly two truth values,
namely true and false.
Henkin models
Henkin [1950] introduced a broader notion of general model
in which a type contains “enough” functions but not necessarily all functions.
Henkin then showed soundness and completeness. More precisely, he showed that
provability in ExTT coincides with truth in all general models (the standard one
as well as the non-standard ones). Andrews [1972b] provided an improvement on
Henkin’s deﬁnition of general models by replacing the notion that there be enough
functions to provide denotations for all formulas of ETT with a more direct means

230
Christoph Benzm¨uller and Dale Miller
to deﬁne general models based on combinatory logic.
Andrews [1972a] points
out that Henkin’s deﬁnition of general model technically was in error since his
deﬁnition of general models admitted models in which the axiom of functional
extensionality (10τ→β) does not hold. Andrews then showed that there is a rather
direct way to ﬁx that problem by shifting the underlying logical connectives away
from the usual Boolean connectives and quantiﬁers for a type-indexed family of
connectives {Qτ→τ→o}τ in which Qτ→τ→o denotes equality at type τ.
Non-functional models and extensionality
Henkin models are fully exten-
sional, i.e., they validate the functional and Boolean extensionality axioms 10τ→γ
and 10o. The construction of non-functional models for ETT has been pioneered
by Andrews [1971].
In Andrews’s so-called v-complexes, which are based on
Sch¨utte’s semi-valuation method [1960], both the functional and the Boolean ex-
tensionality principles fail. Assuming β-equality, functional extensionality 10τ→γ
splits into two weaker and independent principles η (F
.= λX FX, if X is not
free in term F) and ξ (from ∀X.F
.= G infer λX F
.= λX G, where X may
occur free in F and G).
Conversely, βη-conversion, which is built-in in many
modern implementations of ETT, together with ξ implies functional extension-
ality. Boolean extensionality, however, is independent of any of these principles.
A whole landscape of respective notions of models structures for ETT between
Andrews’s v-complexes and Henkin semantics that further illustrate and clar-
ify the above connections is developed in [Benzm¨uller et al., 2004; Brown, 2004;
Benzm¨uller, 1999a], and an alternative development and discussion has been con-
tributed by Muskens [2007].
Takeuti’s conjecture
Takeuti [1953] deﬁned GLC (“generalize logical calcu-
lus”) by extending Gentzen’s LK with (second-order) quantiﬁcation over predi-
cates. He conjectured cut-elimination for the GLC proof system and he showed
that this conjecture proved the consistency of analysis (second-order arithmetic).
Sch¨utte [1960] presented a simpliﬁed version of Takeuti’s GLC and gave a semantic
characterization of the Takeuti conjecture. This important conjecture was proved
by Tait [1966] for the second-order fragment using Sch¨utte’s semantic results. The
higher-order version of the conjecture was later proved by Takahashi [1967] and
by Prawitz [1968]. The proof of strong normalization for System F given by Gi-
rard [1971] also proves Takeuti’s conjecture as a consequence.
Andrews [1971]
used the completeness of cut-free proofs (but phrased in the contrapositive form
as the abstract consistency principle [Smullyan, 1963]) in order to give a proof of
the completeness of resolution in ETT. Takeuti [1975] presented a cut-free sequent
calculus with extensionality that is complete for Henkin’s general models. The
abstract consistency proof technique, as used by Andrews, has been further ex-
tended and applied to obtain cut-elimination results for diﬀerent systems between
ETT and ExTT by Brown [2004], Benzm¨uller et al. [2004; 2008a], and Brown and
Smolka [2010]. For a diﬀerent semantic approach to proving cut-elimination for
intuitionistic variants of STT see [Hermant and Lipton, 2010].

Automation of Higher-Order Logic
231
Candidates of reducibility
In the setting of the intuitionistic variants of STT,
the proofs themselves are of interest since they can be seen as programs that carry
the computational content of constructive proofs. Girard [1971; 1986] proved the
strong normalization of such proofs (expressed as richly typed λ-terms). To achieve
this strong normalization result, Girard introduces the candidats de reductibilit´e
technique which is, today, a common technique used to prove results such as cut-
elimination for higher-order logics.
Herbrand’s theorem for ETT
In [Andrews et al., 1984], Andrews introduced
a notion of proof called a development that resembles Craig-style linear reason-
ing in which a formula can be repeatedly rewritten until a tautologous formula
is encountered. Three kinds of formula rewritings are possible: instantiate a top-
level universal quantiﬁer (with an eigenvariable), instantiate a top-level existential
quantiﬁer (with a term), or duplicate a top-level existential quantiﬁer. Complete-
ness of developments for ETT can be taken as a kind of Herbrand theorem for
ETT. Miller [1983; 1987] presented the rewrites of developments as a tree instead
of a line. The resulting proof structure, called expansion trees, provides a com-
pact presentation of proofs for higher-order classical logic. Expansion trees are a
natural generalization of Herbrand disjunctions to formulas which might not be in
prenex normal form and where higher-order quantiﬁcation might be involved.
4.2
Cut-simulation properties
Cut-elimination in ﬁrst-order logic gives rise to the subformula property: that
is, cut-free proofs are arrangements of formulas which are just subformulas of
the formulas in the sequent at the root of the proof.
In ETT (and ExTT or
STT), however, cut-free proofs do not necessarily satisfy this subformula property.
To better understand this situation remember that predicate variables may be
instantiated with terms that introduce new formula structure. For this reason,
the subformula property may break (cf. Section 6.1). However, at the same time
this oﬀers the opportunity to mimic cut-introductions by appropriately selecting
such instantiations for predicate variables. For example, a cut formula ϕ may be
introduced by instantiating the law of excluded middle ∀P.P ∨¬P with ϕ and
by applying disjunction elimination (i.e., the rule of cases). In other words, one
may trivially eliminate cut-rule applications by instead working with the axiom of
excluded middle.8 As shown by Benzm¨uller et al. [2009], eﬀective cut-simulation
is also supported by other prominent axioms, including comprehension, induction,
extensionality, description, and choice. Also arbitrary (positive) Leibniz equations
can be employed for the task.
Cut-simulations have in fact been extensively used in literature. For example,
Takeuti showed that a conjecture of G¨odel could be proved without cut by using the
induction principle instead [Takeuti, 1960]; McDowell and Miller [2002] illustrated
8For automating higher-order logic it is thus very questionable to start with intuitionistic
logic ﬁrst and to simply add the law of excluded middle to arrive at classical logic.

232
Christoph Benzm¨uller and Dale Miller
how the induction rule can be used to hide the cut rule; and Sch¨utte [1960] used
excluded middle to similarly mask the cut rule.
In higher-order logic, cut-elimination and cut-simulation should always be con-
sidered in combination: a pure cut-elimination result may indeed mean little if at
the same time axioms are assumed that support eﬀective cut-simulation.
Church’s use of the λ-calculus to build comprehension principles into the lan-
guage can therefore be seen as a ﬁrst step in the program to eliminate the need for
cut-simulating axioms. Further steps have recently been achieved, and tableaux
and resolution calculi have been presented that employ primitive equality and
which provide calculus rules (as opposed to an axiomatic treatment) for exten-
sionality and choice (cf. Section 6.3). These calculus rules do not support cut-
simulation.
4.3
Higher-order substitutions and normal forms
One of the challenges posed by higher-order substitution is that the many normal
forms on which theorem provers often rely are not stable under such substitution.
Clearly, a formula in βη-normal form may no longer be in βη-normal form after a λ-
term instantiates a higher-order free variable in it. Similarly, many other normal
forms—e.g., negation normal, conjunctive normal, and Skolem normal—are not
preserved under such substitutions.
In general, this instability is not a major
problem since often one can re-normalize after performing such substitutions. For
example, one often immediately places terms into βη-normal form after making
a substitution. Since there can be an explosion in the size of terms when such
normalization is made, there are compelling reasons to delay such normalization
[2005].
Andrews [1971], for example, integrates the production of conjunctive
normal and Skolem normal forms within the process of doing resolution.
4.4
Encodings of higher-order logic into ﬁrst-order logic
Given the expressiveness of ﬁrst-order logic and that theoremhood in both ﬁrst-
order logic and ETT (and ExTT or STT) is recursively enumerable, it is not a
surprise that provability in the latter can be formalized in ﬁrst-order logic. Some
of the encodings have high-enough ﬁdelity to make it possible to learn something
structural about ETT from its encoding. For example, Dowek [2008] and Dowek
et al. [2001] use an encoding of ETT in ﬁrst-order logic along with Skolemization
in ﬁrst-order logic in order to explain the nature of Skolemization in ETT.
Mappings of second-order logic into many-sorted ﬁrst-order logic have been
studied by Enderton [1972]. Henschen [1972] presents a mapping from higher-order
logic and addresses the handling of comprehension axioms. For (type restricted)
ExTT with Henkin-style semantics, complete translations into many-sorted, ﬁrst-
order logic have been studied by Kerber [1991; 1994].
Modern interactive theorem provers such as Isabelle nowadays employ trans-
lations from polymorphic higher-order logic into (unsorted or many-sorted) ﬁrst-

Automation of Higher-Order Logic
233
order logic in order to employ ﬁrst-order theorem provers to help prove subgoals.
Achieving Henkin completeness is thereby typically not a main issue. The focus
is rather on practical eﬀectiveness. Even soundness may be abandoned if other
techniques, such as subsequent proof reconstruction, can be employed to identify
unsound proofs. Relevant related work has been presented by Hurd [2003], Meng
and Paulson [2008], and Blanchette et al. [2013b].
5
SKOLEMIZATION AND UNIFICATION
In the latter sections of this chapter, we describe a number of theorem provers for
various subsets of STT. They all achieve elements of their automation in ways that
resemble provers in ﬁrst-order logic. In particular, when quantiﬁers are encoun-
tered, they are either instantiated with eigenvariables (in the sense of Gentzen
[1969a]) or, dually, instantiated by new free variables called logic variables: such
variables denote a term that is determined later via uniﬁcation. To simplify the
relationship between eigenvariables and logic variables and, hence, simplify the
implementation of uniﬁcation, it is customary to simplify quantiﬁers prior to per-
forming proof search. In classical ﬁrst-order logic theorem provers, Skolemization
provides such simpliﬁcation and uniﬁcation does not need to deal with eigenvari-
ables at all.
While such a simpliﬁcation of quantiﬁcational structure is possible in classical
higher-order theorem provers, some important issues arise concerning quantiﬁer
alternation, Skolemization, and term uniﬁcation that are not genuine issues in a
ﬁrst-order setting. We discuss these diﬀerences below.
5.1
Skolemization
A typical approach to simplifying the alternation of quantiﬁers in ﬁrst-order logic
is to use Skolemization. Such a technique replaces an assumption of the form, say,
∀xτ∃yδ Pxy with the assumption ∀xτ Px(fx), where f is a new constant of type
τ →δ. The original assumption is satisﬁable if and only if the Skolemized formula
is satisﬁable: in a model of the Skolemized formula, the meaning of the Skolem
function f is a suitable choice function.
Lifting Skolemization into higher-order logic is problematic for a number of rea-
sons. First, for a logic such as ETT which does not accept the axiom of choice,
Skolem functions should not be allowed, at least not without restrictions. For ex-
ample, the resolution system for ETT introduced by Andrews [1971] used Skolem
functions to simplify quantiﬁer alternations. While Andrews was able to prove that
resolution was complete for ETT, he did not provide the converse result of sound-
ness since some versions of the axiom of choice could be proved [Andrews, 1973].
As was shown by Miller [1983; 1992], the soundness of Skolem functions can be
guaranteed by placing suitable restrictions on the occurrences of Skolem functions
within λ-terms. In particular, consider an assumption of the form ∀xτ∃yδ→θ Pxy
and its Skolemized version ∀xτ Px(fx), where f is a new Skolem function of type

234
Christoph Benzm¨uller and Dale Miller
τ →δ →θ. In order for a proof not to “internalize” the choice function named by
f, every substitution term t used in that proof must be restricted so that every
occurrence of f in t must have at least one argument and any free variable occur-
rences in that argument must also be free in t. Thus it is not possible to form an
abstraction involving the Skolemization-induced argument and, in that way, the
Skolem function is not used as a general choice function.
A second problem with using Skolemization is that there are situations where
a type may have zero or one inhabitant prior to Skolemization but can have an
inﬁnite number of inhabitants after Skolemization [Miller, 1992]. Such a change in
the nature and number of terms that appear in types before and after Skolemiza-
tion introduces new constants is a serious problem when a prover wishes to present
its proofs in forms that do not use Skolemization (such as natural deduction or
sequent calculus).
A third problem using Skolemization is that in the uniﬁcation of typed λ-
terms, the treatment of λ-abstractions and the treatment of eigenvariables are
intimately related. For example, the uniﬁcation problems ∃wι.(λxι.x) = (λxι.w)
and ∃wι∀xι.x = w are essentially the same: since the second (purely ﬁrst-order)
problem is not provable in ﬁrst-order logic (since it is true only in a singleton
domain), the original uniﬁcation problem also has no solutions. Explaining the
non-uniﬁcation of terms λxι.x and λxι.w in terms of Skolemization and choice
functions seems rather indirect.
5.2
Uniﬁcation of simply typed λ-terms
Traditionally, the uniﬁcation of simply typed λ-terms can be described as proving
the formula
∃x1
τ1 . . . ∃xn
τn. t1 = s1 ∧· · · ∧tm = sm
(n, m ≥0).
If we make the additional assumption that no variable in the quantiﬁer preﬁx is free
in any of the terms s1, . . . , sm then this formula is also called a matching problem.
The order of the uniﬁcation problem displayed above is 1 + max{ord(τ1), . . . ,
ord(τn)}; thus, if n = 0 that order is 1. Andrews showed [1974, Theorem 2] that
such a formula is provable in ETT if and only if there is a substitution θ for the
variables in the quantiﬁer preﬁx such that for each i = 1, . . . , n, the terms tιθ and
sιθ have the same normal form. Such a substitution as θ is called a uniﬁer for
that uniﬁcation problem. Such uniﬁcation problems have been studied in which the
common normal form is computed using just β-conversion or with βη-conversion:
thus one speaks of uniﬁcation or matching modulo β or modulo βη. This theorem
immediately generalizes a similar theorem for ﬁrst-order logic.
Although Guard and his student Gould investigated higher-order versions of
uniﬁcation as early as 1964 [Guard, 1964; Gould, 1966], it was not until 1972 that
the undecidability of such uniﬁcation was demonstrated independently by Huet
[1973a] and Lucchesi [1972]. Those two papers showed that third-order uniﬁca-
tion was undecidable; later Goldfarb [1981] showed that second-order uniﬁcation

Automation of Higher-Order Logic
235
was also undecidable. The decidability of higher-order matching was shown after
several decades of eﬀort: it was ﬁrst shown for second-order matching in [Huet
and Lang, 1978]; for third-order matching in [Dowek, 1992]; and for fourth-order
matching in [Padovani, 2000]. Finally, Stirling [2009] has shown that matching at
all orders is decidable.
Following such undecidability results for uniﬁcation, the search for uniﬁcation
procedures for simply typed λ-terms focused on the recursive enumeration of uni-
ﬁers. The ﬁrst such enumeration was presented in [Pietrzkowski and Jensen, 1972;
Pietrzkowski, 1973; Jensen and Pietzkowski, 1976]. Their enumeration was in-
tractable in implemented systems since when it enumerated a uniﬁer, subsequent
uniﬁers in the enumeration would often subsume it, thus leading to a highly re-
dundant search for uniﬁers.
Huet [1975] presented a diﬀerent approach to the enumeration of uniﬁers. In-
stead of solving all uniﬁcation problems, some uniﬁcation pairs (the so-called
ﬂex-ﬂex pairs) were deemed too unconstrained to schedule for solving. In such
problems, the head of all terms in all equalities are existentially quantiﬁed. For
example, the uniﬁcation problem ∃fι→ι∃gι→ι.fa = ga is composed of only ﬂex-ﬂex
pairs and it has a surprising number of uniﬁers. In particular, let t be any βη-
normal closed term of type ι and assume that the constant a has n occurrences in
t. There are 2n diﬀerent ways to abstract a from t and by assigning one of these to
f and possibly another to g we have a uniﬁer for this uniﬁcation problem. Clearly,
picking blindly from this exponential set of choices on an arbitrary term is not
a good idea. An important design choice in the semi-decision procedure of Huet
[1975] is the delay of such uniﬁcation problems. In particular, Huet’s procedure
computed “pre-uniﬁers”; that is, substitutions that can reduce the original uni-
ﬁcation problem to one involving only ﬂex-ﬂex equations. Huet showed that the
search for pre-uniﬁers could be done, in fact, without redundancy. He also showed
how to build a resolution procedure for ETT on pre-uniﬁcation instead of uniﬁ-
cation by making ﬂex-ﬂex equations into “constraints” on resolution [Huet, 1972;
Huet, 1973b]. The earliest theorem provers for various supersets of ETT—TPS
[Andrews et al., 1996], Isabelle [Paulson, 1989], and λProlog [Nadathur and Miller,
1988; Miller and Nadathur, 2012]—all implemented rather directly Huet’s search
procedure for pre-uniﬁers.
The uniﬁcation of simply typed λ-terms does not have the most-general-uniﬁer
property: that is, there can be two uniﬁers and neither is an instance of the other.
Let g be a constant of type ι →ι →ι and a a constant of type ι. Then the
second-order uniﬁcation problem ∃fι→ι.fa = gaa has four uniﬁers in which f is
instantiated with λw.gww, λw.gwa, λw.gaw, and λw.gaa. A theorem prover that
encounters such a uniﬁcation problem may need to explore all four of these uniﬁers
during the search for a proof. It is also possible for a uniﬁcation problem to have
an inﬁnite number of uniﬁers that are not instances of one another. Such is the
case for the uniﬁcation problem ∃fι→ι.λx.f(hx) = λx.h(fx), where h is a constant
of type ι →ι. All the following instantiations for f yield a uniﬁer: λw.w, λw.hw,
λw.h(hw), λw.h(h(hw)), . . ..

236
Christoph Benzm¨uller and Dale Miller
For more details about Huet’s search procedure for uniﬁers, we recommend
Huet’s original paper [1975] as well as the subsequent papers by Snyder and Gallier
[1989] and Miller [1992], and the handbook chapter by Dowek [2001]. Here we
illustrate some of the complexities involved with this style of uniﬁcation.
5.3
Mixed preﬁx uniﬁcation problems
As we motivated above, it is natural to generalize uniﬁcation problems away from
a purely existential quantiﬁer preﬁx to one that has a mixed quantiﬁer preﬁx, i.e.,
a uniﬁcation problem will be a formula of the form
Q1x1
τ1 . . . Qnxn
τn. t1 = s1 ∧· · · ∧tm = sm
(n, m ≥0).
Here, Qι is either ∀or ∃for i = 1, . . . , n. There is, in fact, a simple technique avail-
able in higher-order logic that is not available in ﬁrst-order logic which can simplify
quantiﬁer alternation in such uniﬁcation problems. In particular, if ∀xτ∃yσ occurs
within the preﬁx of a uniﬁcation problem, it is a simple matter to “rotate” the ∀x
to the right: this requires “raising” the type of the ∃y quantiﬁer. That is, ∀xτ∃yσ
can be replaced by ∃hτ→σ∀xτ if all occurrences of y in the scope of ∃y are substi-
tuted by (hx). The resulting two uniﬁcation problems are equivalent in the sense
that uniﬁers for these two problems can be put into a one-to-one correspondence
by a simple mapping. For example, the uniﬁcation problem ∀xι∀yι∃zι.fzx = fyz
(for some constant f of type ι →ι →ι) can be rewritten to the uniﬁcation problem
∃hι→ι→ι∀xι∀yι.f(hxy)x = fy(hxy).
This latter problem can be replaced by the equivalent uniﬁcation problem
∃hι→ι→ι.λxλy.f(hxy)x = λxλy.fy(hxy). Using the technique of raising, any uni-
ﬁcation problem with a mixed quantiﬁer preﬁx can be rewritten to one with a
preﬁx of the form ∃∀. Furthermore, the block of ∀quantiﬁers can be removed
from the preﬁx if they are converted to a block of λ-bindings in front of all
terms in all the equations. In this way, a mixed preﬁx can be rewritten to an
equivalent one involving only existential quantiﬁers. Details of performing uni-
ﬁcation under a mixed preﬁx can be found in [Miller, 1992]. The notion of ∀-
lifting employed by the Isabelle prover can be explained using raising [Miller, 1991;
Paulson, 1989].
5.4
Pattern uniﬁcation
There is a small subset of uniﬁcation problems, ﬁrst studied by Miller [1991],
whose identiﬁcation has been important for the construction of practical systems.
Call a uniﬁcation problem a pattern uniﬁcation problem if every occurrence of
an existentially quantiﬁed variable, say, M, in the preﬁx is applied to a list of
arguments that are all distinct variables bound by either a λ-binder or a universal
quantiﬁer in the scope of the existential quantiﬁer. Thus, existentially quantiﬁed

Automation of Higher-Order Logic
237
variables cannot be applied to general terms but a very restricted set of bound
variables. For example,
∃M∃N.λxλy.f(Mxy) = λxλy.Ny
∃M∀x∀y.f(Mxy) = fy
∃M∀x.λy.Mxy = λy.Myx
∃M∃N.∀x∀y.Mxy = Ny
are all pattern uniﬁcation problems.
All these uniﬁcation problems have most
general uniﬁers, respectively, [M 7→λxλy.Py, N 7→λy.f(Py)], [M 7→λxλy.y],
[M 7→λxλy.P], and [M 7→λxλy.Ny], where P is a new (existentially quantiﬁed)
variable. Notice that although the last two of these are examples of ﬂex-ﬂex uniﬁ-
cation problems, they both have a most general uniﬁer. The following uniﬁcation
problems do not fall into this fragment:
∃M∃N.λx.f(Mxx) = Nx
∃M.∀x.f(Mx) = M(fx).
Notice that all ﬁrst-order uniﬁcation problems are, in fact, pattern uniﬁcation
problems, and that pattern uniﬁcation problems are stable under the raising tech-
nique mentioned earlier. The main result about pattern uniﬁcation is that—like
ﬁrst-order uniﬁcation—deciding uniﬁability is decidable and most general uniﬁers
exist for solvable problems. Also like ﬁrst-order uniﬁcation, types attributed to
constructors are not needed for doing the uniﬁcation.
5.5
Practical considerations
Earlier we mentioned that uniﬁcation problems can be addressed using either just
β-conversion or βη-conversion. Although Huet [1975] considered both uniﬁcation
modulo β and βη conversion separately, almost no implemented system considers
only the pure β conversion rules alone: term equality for STT is uniformly treated
as βη-convertibility.
Skolemization is a common technique for simplifying quantiﬁer alternation in
many implemented higher-order theorem provers (cf. Section 7). On the other
hand, several other systems, particularly those based on the intuitionistic fragment
of ETT, do not use Skolemization: instead they either use raising, as is done in
Isabelle [Paulson, 1989; Paulson, 1994] or they work directly with a representation
of an unaltered quantiﬁer preﬁx, as is done in the Teyjus implementation [Nadathur
and Linnell, 2005] of λProlog.
It is frequently the case that in computational logic systems that unify simply
typed λ-terms, only pattern uniﬁcation problems need to be solved. As a result,
some systems—such as the Teyjus implementation of λProlog and the interactive
theorem provers Minlog [Benl et al., 1998] and Abella [Gacek et al., 2012]—only
implement the pattern fragment since this makes their design and implementation
easier.

238
Christoph Benzm¨uller and Dale Miller
6
CHALLENGES FOR AUTOMATION
While theorem provers for ETT, ExTT, and STT can borrow many techniques
from theorem provers for ﬁrst-order logic, there are several challenges to the direct
implementation of such provers. We discuss some of these challenges below.
6.1
Instantiation of predicate variables
During the search for proofs in quantiﬁcational logics, quantiﬁers need to be in-
stantiated (possibly more than once) with various terms. Choosing such terms is
a challenge partly because when a quantiﬁer needs to be instantiated, the role of
that instantiation term in later proof steps is not usually known. To address this
gap between when a quantiﬁer needs an instantiation term and when that term’s
structure is ﬁnally relevant to the completion of a proof, the techniques of uniﬁ-
cation described in the previous section are used. When uniﬁcation is involved,
quantiﬁers are instantiated not with terms but with variables which represent a
promise: before a proof is complete, those variables will be replaced by terms.
The variables that are introduced in this way are sometimes called logic variables:
these variables correspond to those marked using existential quantiﬁcation in the
uniﬁcation problems of Section 5.3.
In this way, one can delay the choice of which term to use to instantiate the
quantiﬁer until the point where that term is actually used in the proof. As an
illustration of using uniﬁcation in the search for a proof, consider attempting a
proof of the formula (q (f a)) from the conjunctive assumption
pa ∧(∀x px ⊃p(fx)) ∧(∀y py ⊃qy).
One way to prove this goal would be to assume, for example, that each universally
quantiﬁed premise is used once for some, currently, unspeciﬁed term. In this case,
instantiate ∀x and ∀y with logic variables X and Y , respectively, and we have an
assumption of the form
pa ∧(pX ⊃p(fX)) ∧(pY ⊃qY ).
We can then observe that the proof is complete if we chain together two applica-
tions of modus ponens: for that to work, we need to ﬁnd substitution terms for X
and Y to solve the equations
pa = pX ∧p(fX) = pY ∧qY = q(fa).
Clearly, this uniﬁcation problem is solvable when X and Y are replaced by a and
fa, respectively. Thus, if we were to repeat the steps of the proof but this time
instantiate the quantiﬁers ∀x and ∀y with a and (f a), respectively, the chaining
of the modus ponens steps would now lead to a proper proof.
A key property of ﬁrst-order quantiﬁcational logic is that the terms needed
for instantiating quantiﬁers can all be found using uniﬁcation of atomic formulas.

Automation of Higher-Order Logic
239
When predicate variables are present, however, the uniﬁcation of atomic formulas
is no longer suﬃcient to generate all quantiﬁer instantiations needed for proofs.
For example, the ETT theorem
∃p (px ⊃(ax ∧bx)) ∧((ax ∧bx) ⊃px).
is proved by instantiating p with λw.aw ∧bw. If that quantiﬁer were, instead,
instantiated by the logic variable P to yield the formula
(Px ⊃(ax ∧bx)) ∧((ax ∧bx) ⊃Px)
no equalities between occurrences of atomic formulas will provide a uniﬁcation
problem that has this uniﬁer. Similarly, the theorem
∀q (qa ⊃qb) ⊃pb ⊃pa
is proved (in intuitionistic and classical logic) by instantiating ∀q with the term
λw.pw ⊃pa. Once again, however, if the quantiﬁer ∀q was instantiated with a
logic variable Q, then no uniﬁcation of that atomic formulas Qa, Qb, pa, and pb
would have yielded this substitution terms for Q.
Of course, it is not surprising that simple syntactic checks involving subformulas
are not sophisticated enough to compute substitutions for predicates. Often the
key insight into proving a mathematical theorem is the production of the right set
or relation to instantiate a predicate variable: in ETT (ExTT or STT), these would
be encoded as λ-abstractions containing logical connectives. Similarly, induction
can be encoded and the invariants for inductive proofs would be encoded as similar
terms and used to instantiate predicate quantiﬁers.
Nonetheless, a number of
researchers have described various schemes for inventing substitution terms for
predicate variables. We mention a few below.
Enumeration of substitutions
An early approach at the generation of pred-
icate substitutions was provided by Huet [1972; 1973b] by essentially providing
a mechanism for guessing the top-level, logical structure of a substitution for a
predicate variable. Such guessing (called splittings in that paper) was interleaved
with resolution steps by a system of constraints. Thus, his system suggested a
candidate top-level connective for the body of a predicate substitution and then
proceeded with the proof under that assumption.
A simple, prominent example to illustrate the need for splittings is ∃Po P. When
using resolution the formula is ﬁrst negated and then normalized to clause ¬Xo,
where X is a predicate variable. There is no resolution partner for this clause
available, hence the empty clause can not be derived. However, when guessing
some top-level, logical structure for X, here the substitution [¬Y/X] is suitable,
then ¬¬Y is derived, which normalizes into a new clause Y .
Now, resolution
between the clauses ¬X and Y with substitution [Y/X] directly leads to the empty
clause.
Andrews’s primitive substitutions [1989] incorporates Huet’s notion of splitting,
and an alternative description of splitting can be found in [Dowek, 1993].

240
Christoph Benzm¨uller and Dale Miller
Maximal set variables and set constraints
Bledsoe [1979] suggested a dif-
ferent strategy for coming up with predicate substitutions: in some cases, one can
tell the maximal set that can solve a subgoal. Consider, for example, the formula
∃A (∀x.Ax ⊃px) ∧C(A)
Clearly there are many instantiations possible for A that will satisfy the ﬁrst
conjunct. For example, the empty set λw ⊥, is one of them but it seems not to be
the best one. Rather, a more appropriate substitution for A might be λw pw∧Bw,
where B is a new variable that has the same type as A. This extension of the latter
expression can then range from the empty set (where B is substituted by λw ⊥)
to λw pw (where B is substituted by λw ⊤). Felty [2000] generalized and applied
Bledsoe’s technique, which was restricted to a subset of second-order logic, to the
higher-order logic found in the calculus of constructions. Moreover, Brown [2002]
generalized Bledsoe’s technique to ExTT. His solution, which employs reasoning
with set constraints, has been applied within the TPS theorem prover.
6.2
Induction invariants
Induction and, to a lesser extent, co-induction are important inference rules in com-
puter science and mathematics. Most forms of the induction rule require showing
that some set is, in fact, an invariant of the induction. Even if one is only inter-
ested in ﬁrst-order logic, the induction rule in this form requires discovering the
instantiation of the predicate variable that codes the induction invariant. While
Bledsoe [1979] provides some weak approaches to generating such invariants, a
range of techniques are routinely used to provide either invariants explicitly or
some evidence that an invariant exists. For an example of the latter, the work on
cyclic proofs [Spenger and Dams, 2003] attempts to identify cycles in the unfold-
ing of a proof attempt as a guarantee that an invariant exists. Descente inﬁnie
(sometimes also called inductionless induction) and proof by consistency [Comon,
2001] are also methods for proving inductive theorems without explicitly needing
to invent an invariant (cf. also [Wirth, 2004]).
6.3
Equality, extensionality, and choice
There has been work on automating various axioms beyond those included in ETT.
As mentioned above, various works have focused on automation of induction, which
is based roughly on axioms 7 and 8. For many applications, including mathematics,
one certainly wants and needs to have extensionality and maybe also choice (or
description).
However, the idea to treat such principles axiomatically, as e.g.,
proposed in [Huet, 1973b] for extensionality, leads to a signiﬁcant increase of the
search space, since these axioms (just like the induction axiom) introduce predicate
variables and support cut-simulation (cf. Section 4.2). Another challenge is that
uniﬁcation modulo Boolean extensionality subsumes theorem proving: proving a
proposition ϕ is the same as unifying ϕ and ⊤modulo Boolean extensionality.

Automation of Higher-Order Logic
241
More information on these challenges is provided in [Benzm¨uller et al., 2009] and
[Benzm¨uller, 2002].
Signiﬁcant progress in the automation of ExTT in existing prover implemen-
tations has therefore been achieved after providing calculus level support for ex-
tensionality and also choice. Respective extensionality rules have been provided
for resolution [Benzm¨uller, 1999b], expansion and sequent calculi [Brown, 2004;
Brown, 2005], and tableaux [Brown and Smolka, 2010]. Similarly, choice rules have
been proposed for the various settings: sequent calculus [Mints, 1999], tableaux
[Backes and Brown, 2011] and resolution [Benzm¨uller and Sultana, 2013].
Analogously, (positive) Leibniz equations are toxic for proof automation, since
they also support cut-simulation. For this reason, the automation oriented tableaux
and resolution approaches above support primitive equality and provide respec-
tive rules. The use of Leibniz equations can hence be omitted in the modeling of
theories and conjectures in these approaches.
7
AUTOMATED THEOREM PROVERS
7.1
Early systems
Probably the earliest project to mention is Peter Andrews NSF grant Proof pro-
cedures for Type Theory (1965-67). The goal was to lift ideas from propositional
and ﬁrst-order logic to the higher-order case. Also J. A. Robinson [1969; 1970]
argued for the construction of automated tools for higher-order logic. Together
with E. Cohen, Andrews started a ﬁrst computer implementation based on the
inference rules of Andrews [1971] and the uniﬁcation algorithm of Huet [1975] in
a subsequent project (1971–76).
In 1977 this system did prove Cantor’s theo-
rem automatically in 259 seconds [Andrews and Cohen, 1977]. After 1980, when
D. Miller, F. Pfenning, and other students got involved, this theorem prover
got substantially revised.
The revised system was then called TPS. The TPS
proof assistant [Miller et al., 1982; Andrews et al., 1996; Andrews et al., 2000;
Andrews and Brown, 2006], was, in fact, not based on resolution but on matrix-
style theorem proving. Both λProlog [Nadathur and Miller, 1988] and the Isabelle
theorem prover [Paulson, 1989] were early systems that implemented sizable frag-
ments of the intuitionistic variants of ETT: they were tractable systems because
they either removed or greatly restricted predicate quantiﬁcation. Below we survey
other higher-order systems that attempted to deal with interactive and automatic
theorem proving in the presence of predicate quantiﬁcation.
HOL
The ML based provers of the HOL family include HOL88, HOL98, and
HOL4 [Gorden and Melham, 1993].
These systems are all based on the LCF
approach [Gordon et al., 1979], in which powerful proof tactics are iteratively
built up from a small kernel of basic proof rules. Other LCF-based provers for
higher-order logic are the minimalist system HOL Light [Harrison, 2009], which
provides powerful automation tactics and which has recently played a key role in

242
Christoph Benzm¨uller and Dale Miller
the veriﬁcation of Kepler’s conjecture [Hales, 2013], and the ProofPower system
[Arthan, 2011], which provides special support for a set-theoretic speciﬁcation
language.
Isabelle/HOL
Isabelle [Paulson, 1989] is a theorem prover with a core tactic
language built on a fragment of the intuitionistic variant of ETT. Built on this core
is the Isabelle/HOL [Nipkow et al., 2002] interactive theorem prover for classical
higher-order logic. Isabelle/HOL includes several powerful features such as bridges
to external theorem provers, sophisticated user interaction, and the possibility to
export executable speciﬁcations written in Isabelle/HOL as executable code in
various programming languages.
PVS
The prototype veriﬁcation system PVS [Owre et al., 1992] combines a
higher-order speciﬁcation languages with an interactive theorem proving environ-
ment that integrates decision procedures, a model checker, and various other util-
ities to improve user productivity in large formalization and veriﬁcation projects.
Like Isabelle and the HOL provers, PVS also includes a rich library of formalized
theories.
IMPS
The higher-order interactive proof assistant IMPS [Farmer, 1993] pro-
vides good support for partial functions and undeﬁned terms in STT [Farmer,
1990]. Moreover, it supports human oriented formal proofs which are nevertheless
machine checked. Most importantly, IMPS organizes mathematics using the “lit-
tle theories” method in which reasoning is distributed over a network of theories
linked by theory morphisms [Farmer et al., 1992]. It is the ﬁrst theorem proving
system to employ this approach.
ΩMEGA
The higher-order proof assistant ΩMEGA [Benzm¨uller et al., 1997]
combines tactic based interactive theorem proving with automated proof planning.
With support from an agent-based model, it integrates various external reasoners:
including ﬁrst-order automated theorem provers, the higher-order automated the-
orem provers LEO [Benzm¨uller and Kohlhase, 1998; Benzm¨uller, 1999a] and TPS,
and computer algebra systems [Autexier et al., 2010]. Proof certiﬁcates from these
external systems can be transformed and veriﬁed in ΩMEGA.
λClam and IsaPlanner
λ-Clam [Richardson et al., 1998] is a higher-order vari-
ant of the CLAM proof planner [Bundy et al., 1990] built in λProlog. This prover
focuses on induction proofs based on the rippling technique. IsaPlanner [Dixon
and Fleuriot, 2003] is a related generic proof planner built on top of the Isabelle
system.
Deduction Modulo
In the deduction-modulo approach to theorem proving
[Dowek et al., 2003], a ﬁrst-order presentation of (intensional) higher-order logic

Automation of Higher-Order Logic
243
can be exploited to automate higher-order reasoning [Dowek et al., 2001]. A recent
implementation of the deduction modulo approach (still restricted to ﬁrst-order)
has been presented by [Burel, 2011b]; see also the Dedukti proof checker [Boespﬂug
et al., 2012].
Other early interactive proof assistants, for variants of constructive higher-order
logic, include Automath [Nederpelt et al., 1994], Nuprl [Constable et al., 1986],
LEGO [Pollack, 1994], Coq [Bertot and Casteran, 2004], and Agda [Coquand and
Coquand, 1999]. The logical frameworks Elf [Pfenning, 1994], Twelf [Pfenning
and Sch¨urmann, 1999], and Beluga [Pientka and Dunﬁeld, 2010] are based on
dependently typed higher-order logic. Related provers include the general-purpose,
interactive, type-free, equational higher-order theorem prover Watson [Holmes and
Alves-Foss, 2001] and the fully automated theorem prover Otter-λ [Beeson, 2006]
for λ-logic (a combination of λ-calculus and ﬁrst-order logic). Abella [Gacek et al.,
2012] is a recently implemented interactive theorem prover for an intuitionistic,
predicative higher-order logic with inference rules for induction and co-induction.
ACL2 [Kaufmann and Moore, 1997] and KeY [Beckert et al., 2007] are prominent
ﬁrst-order interactive proof assistants that integrate induction.
7.2
The TPTP THF initiative
To foster the systematic development and improvement of higher-order automated
theorem proving systems, Sutcliﬀe and Benzm¨uller [2010], supported by several
other members of the community, initiated the TPTP THF infrastructure (THF
stands for typed higher-order form). This project has introduced the THF syntax
for higher-order logic, it has developed a library of benchmark and example prob-
lems, and it provides various support tools for the new THF0 language fragment.
The THF0 language supports ExTT (with choice) as also studied by Henkin [1950],
that is, it addresses the most commonly used and accepted aspects of Church’s
type theory.
Version 6.0.0 of the TPTP library contains more than 3000 problems in the
THF0 language. The library also includes the entire problem library of Andrews’s
TPS project, which, among others, contains formalizations of many theorems of his
textbook [Andrews, 2002]. The ﬁrst-order TPTP infrastructure [Sutcliﬀe, 2009]
provides a range of resources to support usage of the TPTP problem library. Many
of these resources are now immediately applicable to the higher-order setting al-
though some have required changes to support the new features of THF. The de-
velopment of the THF0 language, has been paralleled and signiﬁcantly inﬂuenced
by the development of the LEO-II prover [Benzm¨uller et al., 2008b]. Several other
provers have quickly adopted this language, leading to fruitful mutual compar-
isons and evaluations. Several implementation bugs in diﬀerent systems have been
detected this way.

244
Christoph Benzm¨uller and Dale Miller
7.3
TPTP THF0 compliant higher-order theorem provers
We brieﬂy describe the currently available, fully automated theorem provers for
ExTT (with choice).
These systems all support the new THF0 language and
they can be employed online (avoiding local installations) via Sutcliﬀe’s Syste-
mOnTPTP facility.9
TPS
The TPS prover can be used to prove theorems of ETT or ExTT automat-
ically, interactively, or semi-automatically. When searching for a proof automat-
ically, TPS ﬁrst searches for an expansion proof [Miller, 1987] or an extensional
expansion proof [Brown, 2004] of the theorem. Part of this process involves search-
ing for acceptable matings [Andrews, 1981]. Using higher-order uniﬁcation, a pair
of occurrences of subformulas (which are usually literals) is mated appropriately
on each vertical path through an expanded form of the theorem to be proved.
Skolemization and pre-uniﬁcation is employed, and calculus rules for extensional-
ity reasoning are provided. The behavior of TPS is controlled by sets of ﬂags, also
called modes. About ﬁfty modes have been found that collectively suﬃce for au-
tomatically proving virtually all the theorems that TPS has proved automatically
thus far. A simple scheduling mechanism is employed in TPS to sequentially run
these modes for a limited amount of time. The resulting fully automated system
is called TPS (TPTP).
LEO-II
Benzm¨uller et al. [2008b], the successor of LEO, is an automated theo-
rem prover for ExTT (with choice) which is based on extensional higher-order res-
olution. More precisely, LEO-II employs a reﬁnement of extensional higher-order
RUE resolution [Benzm¨uller, 1999b]. LEO-II employs Skolemization, (extensional)
pre-uniﬁcation, and calculus rules for extensionality and choice are provided. LEO-
II is designed to cooperate with specialist systems for fragments of higher-order
logic. By default, LEO-II cooperates with the ﬁrst-order prover systems E [Schulz,
2002]. LEO-II is often too weak to ﬁnd a refutation among the steadily growing
set of clauses on its own. However, some of the clauses in LEO-II’s search space
attain a special status: they are ﬁrst-order clauses modulo the application of an
appropriate transformation function. Therefore, LEO-II regularly launches time
limited calls with these clauses to a ﬁrst-order theorem prover, and when the
ﬁrst-order prover reports a refutation, LEO-II also terminates. Communication
between LEO-II and the cooperating ﬁrst-order theorem prover uses the TPTP
language and standards. LEO-II outputs proofs in TPTP TSTP syntax.
Isabelle/HOL
The Isabelle/HOL system has originally been designed as an in-
teractive prover. However, in order to ease user interaction several automatic proof
tactics have been added over the years. By appropriately scheduling a subset of
these proof tactics, some of which are quite powerful, Isabelle/HOL has in recent
years been turned also into an automatic theorem prover, that can be run from a
9See http://www.cs.miami.edu/~tptp/cgi-bin/SystemOnTPTP

Automation of Higher-Order Logic
245
command shell like other provers. The latest releases of this automated version of
Isabelle/HOL provide native support for diﬀerent TPTP syntax formats, includ-
ing THF0. The most powerful proof tactics that are scheduled by Isabelle/HOL
include the sledgehammer tool [Blanchette et al., 2013a], which invokes a sequence
of external ﬁrst-order and higher-order theorem provers, the model ﬁnder Nitpick
[Blanchette and Nipkow, 2010], the equational reasoner simp [Nipkow, 1989], the
untyped tableau prover blast [Paulson, 1999], the simpliﬁer and classical reasoners
auto, force, and fast [Paulson, 1994], and the best-ﬁrst search procedure best. The
TPTP incarnation of Isabelle/HOL does not yet output proof terms.
Satallax
The higher-order, automated theorem prover Satallax [Brown, 2012;
Brown, 2013] comes with model ﬁnding capabilities. The system is based on a
complete ground tableau calculus for ExTT (with choice) [Backes and Brown,
2011]. An initial tableau branch is formed from the assumptions of a conjecture
and negation of its conclusion. From that point on, Satallax tries to determine
unsatisﬁability or satisﬁability of this branch.
Satallax progressively generates
higher-order formulas and corresponding propositional clauses. Satallax uses the
SAT solver MiniSat as an engine to test the current set of propositional clauses
for unsatisﬁability. If the clauses are unsatisﬁable, the original branch is unsatisﬁ-
able. Satallax employs restricted instantiation and pre-uniﬁcation, and it provides
calculus rules for extensionality and choice. If there are no quantiﬁers at func-
tion types, the generation of higher-order formulas and corresponding clauses may
terminate. In that case, if MiniSat reports the ﬁnal set of clauses as satisﬁable,
then the original set of higher-order formulas is satisﬁable (by a standard model in
which all types are interpreted as ﬁnite sets). Satallax outputs proofs in diﬀerent
formats, including Coq proof scripts and Coq proof terms.
Nitpick and Refute
These systems are (counter-)model ﬁnders for ExTT.
The ability of Isabelle to ﬁnd (counter-)models using the Refute and Nit-
pick [Blanchette and Nipkow, 2010] commands has also been integrated into au-
tomatic systems. They provide the capability to ﬁnd models for THF0 formu-
las, which conﬁrm the satisﬁability of axiom sets, or the unsatisﬁability of non-
theorems. The generation of models is particularly useful for exposing errors in
some THF0 problem encodings, and revealing bugs in the THF0 theorem provers.
Nitpick employs Skolemization.
agsyHOL
The agsyHOL prover [Lindblad, 2013] is based on a generic lazy nar-
rowing proof search algorithm. Backtracking is employed and a comparably small
search state is maintained. The prover outputs proof terms in sequent style which
can be veriﬁed in the Agda system.
coqATP
The coqATP prover [Bertot and Casteran, 2004] implements (the non-
inductive) part of the calculus of constructions. The system outputs proof terms

246
Christoph Benzm¨uller and Dale Miller
which are accepted as proofs by Coq (after the addition of a few deﬁnitions).
The prover has axioms for functional extensionality, choice, and excluded middle.
Propositional extensionality is not supported yet. In addition to axioms, a small
library of basic lemmas is employed.
7.4
Recent applications of automated THF0 provers
Over the years, the proof assistants from Section 7.1 have been applied in a wide
range of applications, including mathematics and formal veriﬁcation. Typically
these applications combine user interaction and partial proof automation.
For
further information we refer to the websites of these systems.
With respect to full proof automation the TPS system has long been the lead-
ing system, and the system has been employed to build up the TPS library of
formalized and automated mathematical proofs. More recently, however, TPS is
outperformed by several other THF0 theorem provers. Below we brieﬂy point to
some selected recent applications of the leading systems.
Both Isabelle/HOL and Nitpick have been successfully employed to check a for-
malization of a C++ memory model against various concurrent programs written
in C++ (such as a simple locking algorithm) [Blanchette et al., 2011]. Moreover,
Nitpick has been employed in the development of algebraic formal methods within
Isabelle/HOL [Guttmann et al., 2011].
Isabelle/HOL, Satallax, and LEO-II performed well in recent experiments re-
lated to the Flyspeck project [Hales, 2013], in which a formalized proof of the
Kepler conjecture is being developed (mainly) in HOL Light; cf. the experiments
reported by Kaliszyk and Urban [2012, Table 7].
Most recently, LEO-II, Satallax, and Nitpick were employed to achieve a for-
malization, mechanization, and automation of G¨odel’s ontological proof of the
existence of God [Benzm¨uller and Woltzenlogel Paleo, 2013]. This work employs a
semantic embedding of quantiﬁed modal logic in THF0 [Benzm¨uller and Paulson,
2013]. Some previously unknown results were contributed by the provers.
Using the semantic embeddings approach, a wide range of propositional and
quantiﬁed non-classical logics, including parts of their meta-theory and their
combinations, can be automated with THF0 reasoners (cf. [Benzm¨uller, 2013;
Benzm¨uller et al., 2012] and [Benzm¨uller, 2011]). Automation is thereby com-
petitive, as recent experiments for ﬁrst-order modal logic show [Benzm¨uller and
Raths, 2013].
THF0 reasoners can also be fruitfully employed for reasoning in expressive on-
tologies [Benzm¨uller and Pease, 2012]. Furthermore, the heterogeneous toolset
HETS [Mossakowski et al., 2007] employs THF0 to integrate the automated higher-
order provers Satallax, LEO-II, Nitpick, Refute, and Isabelle/HOL.

Automation of Higher-Order Logic
247
8
CONCLUSION
We have summarized the development of theorem provers for Church’s simple
theory of types (and elementary type theory) in the 20th century. Given that
the model theory and proof theory for ETT, ExTT, and STT is mature, a sig-
niﬁcant number of interactive and, most recently, automated theorem proving
systems have been built for them. Many applications of these systems support
Church’s original motivation for STT, namely that it could be an elegant, pow-
erful, and mechanized foundations for mathematics. In addition to mathematics,
various other application areas (including non-classical logics) are currently being
explored.
ACKNOWLEDGMENTS
We thank Chad Brown for sharing notes that he has written related to the material
in this chapter. Besides the readers of this chapter, we thank Zakaria Chihani,
Julian R¨oder, Leon Weber, and Max Wisnieswki for proofreading the document.
The ﬁrst author has been supported by the German Research Foundation under
Heisenberg grant BE2501/9-1 and the second author has been supported by the
ERC Advanced Grant ProofCert.
BIBLIOGRAPHY
[Andreoli, 1992] Andreoli, J.M., 1992. Logic programming with focusing proofs in linear logic.
J. of Logic and Computation 2, 297–347.
[Andrews and Brown, 2006] Andrews, P., Brown, C., 2006. Tps: A hybrid automatic-interactive
system for developing proofs. J. Applied Logic 4, 367–395.
[Andrews and Cohen, 1977] Andrews, P., Cohen, E., 1977. Theorem proving in type theory, in:
Proc. of IJCAI-77, 5th International Joint Conference on Artiﬁcial Intelligence.
[Andrews, 1965] Andrews, P.B., 1965. A Transﬁnite Type Theory with Type Variables. Studies
in Logic and the Foundations of Mathematics, North-Holland Publishing Company.
[Andrews, 1971] Andrews, P.B., 1971. Resolution in type theory. Journal of Symbolic Logic 36,
414–432.
[Andrews, 1972a] Andrews, P.B., 1972a. General models and extensionality. Journal of Symbolic
Logic 37, 395–397.
[Andrews, 1972b] Andrews, P.B., 1972b. General models, descriptions, and choice in type the-
ory. Journal of Symbolic Logic 37, 385–394.
[Andrews, 1973] Andrews, P.B., 1973. Letter to Roger Hindley dated January 22, 1973.
[Andrews, 1974] Andrews, P.B., 1974. Provability in elementary type theory. Zeitschrift fur
Mathematische Logic und Grundlagen der Mathematik 20, 411–418.
[Andrews, 1981] Andrews, P.B., 1981. Theorem proving via general matings. J. ACM 28, 193–
214.
[Andrews, 1989] Andrews, P.B., 1989. On connections and higher order logic. J. of Autom.
Reasoning 5, 257–291.
[Andrews, 2001] Andrews, P.B., 2001. Classical type theory, in: Robinson, A., Voronkov, A.
(Eds.), Handbook of Automated Reasoning. Elsevier Science, Amsterdam. volume 2. chap-
ter 15, pp. 965–1007.
[Andrews, 2002] Andrews, P.B., 2002. An Introduction to Mathematical Logic and Type The-
ory: To Truth Through Proof. Second ed., Kluwer Academic Publishers.

248
Christoph Benzm¨uller and Dale Miller
[Andrews, 2009] Andrews, P.B., 2009. Church’s type theory, in: Zalta, E.N. (Ed.), The Stanford
Encyclopedia of Philosophy. spring 2009 ed.. Stanford University.
[Andrews et al., 2000] Andrews, P.B., Bishop, M., Brown, C.E., 2000. TPS: A theorem prov-
ing system for type theory, in: McAllester, D. (Ed.), Proceedings of the 17th International
Conference on Automated Deduction, Springer, Pittsburgh, USA. pp. 164–169.
[Andrews et al., 1996] Andrews, P.B., Bishop, M., Issar, S., Nesmith, D., Pfenning, F., Xi, H.,
1996. TPS: A theorem proving system for classical type theory. J. Autom. Reasoning 16,
321–353.
[Andrews et al., 1984] Andrews, P.B., Longini-Cohen, E., Miller, D., Pfenning, F., 1984. Au-
tomating higher order logics. Contemp. Math 29, 169–192.
[Arthan, 2011] Arthan, R., 2011. Proofpower website. http://www.lemma-one.com/ProofPower/
index/.
[Autexier et al., 2010] Autexier, S., Benzm¨uller, C., Dietrich, D., Siekmann, J., 2010. OMEGA:
Resource-adaptive processes in an automated reasoning system, in: Crocker, M.W., Siekmann,
J. (Eds.), Resource-Adaptive Cognitive Processes, Springer, Cognitive Technologies. pp. 389–
423.
[Backes and Brown, 2011] Backes, J., Brown, C.E., 2011. Analytic tableaux for higher-order
logic with choice. J. Autom. Reasoning 47, 451–479.
[Barendregt, 1997] Barendregt, H., 1997. The impact of the lambda calculus in logic and com-
puter science. Bulletin of Symbolic Logic 3, 181–215.
[Barendregt et al., 2013] Barendregt, H., Dekkers, W., Statman, R., 2013. Lambda Calculus
with Types. Perspectives in Logic, Cambridge University Press.
[Beckert et al., 2007] Beckert, B., H¨ahnle, R., Schmitt, P.H. (Eds.), 2007. Veriﬁcation of Object-
Oriented Software: The KeY Approach. LNCS 4334, Springer-Verlag.
[Beeson, 2006] Beeson, M., 2006. Mathematical induction in Otter-lambda. J. Autom. Reason-
ing 36, 311–344.
[Benl et al., 1998] Benl, H., Berger, U., Schwichtenberg, H., Seisenberger, M., Zuber, W., 1998.
Proof theory at work: Program development in the minlog system, in: Bibel, W., Schmitt, P.
(Eds.), Automated Deduction. Kluwer. volume II.
[Benzm¨uller, 1999a] Benzm¨uller, C., 1999a. Equality and Extensionality in Automated Higher-
Order Theorem Proving. Ph.D. thesis. Saarland University.
[Benzm¨uller, 1999b] Benzm¨uller, C., 1999b. Extensional higher-order paramodulation and RUE-
resolution, in: Ganzinger, H. (Ed.), Proc. of CADE-16, Springer. pp. 399–413.
[Benzm¨uller, 2002] Benzm¨uller, C., 2002.
Comparing approaches to resolution based higher-
order theorem proving. Synthese 133, 203–235.
[Benzm¨uller, 2011] Benzm¨uller, C., 2011. Combining and automating classical and non-classical
logics in classical higher-order logic. Annals of Mathematics and Artiﬁcial Intelligence 62,
103–128.
[Benzm¨uller, 2013] Benzm¨uller, C., 2013. Automating quantiﬁed conditional logics in HOL, in:
Rossi, F. (Ed.), Proc. of IJCAI-23, Beijing, China.
[Benzm¨uller et al., 2004] Benzm¨uller, C., Brown, C., Kohlhase, M., 2004. Higher-order seman-
tics and extensionality. Journal of Symbolic Logic 69, 1027–1088.
[Benzm¨uller et al., 2008a] Benzm¨uller, C., Brown, C., Kohlhase, M., 2008a. Cut elimination
with xi-functionality, in: Benzm¨uller, C., Brown, C., Siekmann, J., Statman, R. (Eds.), Rea-
soning in Simple Type Theory: Festschrift in Honor of Peter B. Andrews on His 70th Birthday.
College Publications. Studies in Logic, Mathematical Logic and Foundations, pp. 84–100.
[Benzm¨uller et al., 2009] Benzm¨uller, C., Brown, C., Kohlhase, M., 2009. Cut-simulation and
impredicativity. Logical Methods in Computer Science 5, 1–21.
[Benzm¨uller et al., 1997] Benzm¨uller, C., Cheikhrouhou, L., Fehrer, D., Fiedler, A., Huang, X.,
Kerber, M., Kohlhase, M., Konrad, K., Melis, E., Meier, A., Schaarschmidt, W., Siekmann,
J., Sorge, V., 1997.
OMEGA: Towards a mathematical assistant, in: McCune, W. (Ed.),
Proceedings of CADE-14, Springer. pp. 252–255.
[Benzm¨uller et al., 2012] Benzm¨uller, C., Gabbay, D., Genovese, V., Rispoli, D., 2012.
Em-
bedding and automating conditional logics in classical higher-order logic. Ann. Math. Artif.
Intell. 66, 257–271.
[Benzm¨uller and Kohlhase, 1998] Benzm¨uller, C., Kohlhase, M., 1998. LEO – a higher-order
theorem prover, in: Kirchner, C., Kirchner, H. (Eds.), Proc. of CADE-15, Springer. pp. 139–
143.

Automation of Higher-Order Logic
249
[Benzm¨uller and Paulson, 2013] Benzm¨uller, C., Paulson, L., 2013. Quantiﬁed multimodal log-
ics in simple type theory. Logica Universalis (Special Issue on Multimodal Logics) 7, 7–20.
[Benzm¨uller and Pease, 2012] Benzm¨uller, C., Pease, A., 2012. Higher-order aspects and context
in SUMO. Journal of Web Semantics 12-13, 104–117.
[Benzm¨uller and Raths, 2013] Benzm¨uller, C., Raths, T., 2013. HOL based ﬁrst-order modal
logic provers, in: McMillan, K., Middeldorp, A., Voronkov, A. (Eds.), Proceedings of LPAR-
19, Stellenbosch, South Africa.
[Benzm¨uller and Sultana, 2013] Benzm¨uller, C., Sultana, N., 2013.
LEO-II version 1.5, in:
Blanchette, J.C., Urban, J. (Eds.), PxTP 2013, EasyChair EPiC Series 14, 2-12. pp. 2–10.
[Benzm¨uller et al., 2008b] Benzm¨uller, C., Theiss, F., Paulson, L., Fietzke, A., 2008b. LEO-
II - a cooperative automatic theorem prover for higher-order logic (system description), in:
Proc. of IJCAR 2008, Springer. pp. 162–170.
[Benzm¨uller and Woltzenlogel Paleo, 2013] Benzm¨uller, C., Woltzenlogel Paleo, B., 2013. For-
malization, Mechanization and Automation of G¨odel’s Proof of God’s Existence. ArXiv e-
prints arXiv:1308.4526.
[Bertot and Casteran, 2004] Bertot, Y., Casteran, P., 2004. Interactive Theorem Proving and
Program Development - Coq’Art: The Calculus of Inductive Constructions. Texts in Theo-
retical Computer Science, Springer.
[Bishop, 1999] Bishop, M., 1999.
Mating Search Without Path Enumeration.
Ph.D. thesis.
Carnegie Mellon University.
[Blanchette et al., 2013a] Blanchette, J.C., B¨ohme, S., Paulson, L.C., 2013a. Extending Sledge-
hammer with SMT solvers. J. Autom. Reasoning 51, 109–128.
[Blanchette et al., 2013b] Blanchette, J.C., B¨ohme, S., Popescu, A., Smallbone, N., 2013b. En-
coding monomorphic and polymorphic types, in: Piterman, N., Smolka, S.A. (Eds.), Proc. of
TACAS-19, Springer. pp. 493–507.
[Blanchette and Nipkow, 2010] Blanchette, J.C., Nipkow, T., 2010. Nitpick: A counterexample
generator for higher-order logic based on a relational model ﬁnder, in: Kaufmann, M., Paulson,
L.C. (Eds.), Proc. of ITP 2010, Springer. pp. 131–146.
[Blanchette et al., 2011] Blanchette, J.C., Weber, T., Batty, M., Owens, S., Sarkar, S., 2011.
Nitpicking C++ concurrency, in:
Schneider-Kamp, P., Hanus, M. (Eds.), Proceedings of
the 13th International ACM SIGPLAN Conference on Principles and Practice of Declarative
Programming, July 20-22, 2011, Odense, Denmark, ACM. pp. 113–124.
[Bledsoe, 1979] Bledsoe, W.W., 1979. A maximal method for set variables in automatic theorem-
proving, in: Machine Intelligence 9. John Wiley & Sons, pp. 53–100.
[Boespﬂug et al., 2012] Boespﬂug, M., Carbonneaux, Q., Hermant, O., 2012. The lambda-pi-
calculus modulo as a universal proof language, in: Pichardie, D., Weber, T. (Eds.), PxTP
2012, CEUR Workshop Proceedings. pp. 28–43.
[Boolos, 1987] Boolos, G., 1987. A curious inference. Journal of Philosophical Logic 16, 1–12.
[Brown, 2012] Brown, C., 2012. Satallax: an automatic higher-order prover. J. Autom. Rea-
soning , 111–117.
[Brown, 2002] Brown, C.E., 2002. Solving for set variables in higher-order theorem proving, in:
Voronkov, A. (Ed.), Proc. of CADE-18, Springer. pp. 408–422.
[Brown, 2004] Brown, C.E., 2004. Set Comprehension in Church’s Type Theory. Ph.D. thesis.
Department of Mathematical Sciences, Carnegie Mellon University. See also Chad E. Brown,
Automated Reasoning in Higher-Order Logic, College Publications, 2007.
[Brown, 2005] Brown, C.E., 2005.
Reasoning in extensional type theory with equality, in:
Nieuwenhuis, R. (Ed.), Proc. of CADE-20, Springer. pp. 23–37.
[Brown, 2013] Brown, C.E., 2013. Reducing higher-order theorem proving to a sequence of sat
problems. J. Autom. Reasoning 51, 57–77.
[Brown and Smolka, 2010] Brown, C.E., Smolka, G., 2010. Analytic tableaux for simple type
theory and its ﬁrst-order fragment. Logical Methods in Computer Science 6.
[Bundy et al., 1990] Bundy, A., van Harmelen, F., Horn, C., Smaill, A., 1990. The Oyster-Clam
system, in: Stickel, M.E. (Ed.), 10th International Conference on Automated Deduction,
Kaiserslautern, FRG, July 24-27, 1990, Proceedings, Springer. pp. 647–648.
[Burel, 2011a] Burel, G., 2011a. Eﬃciently simulating higher-order arithmetic by a ﬁrst-order
theory modulo. Logical Methods in Computer Science 7, 1–31.
[Burel, 2011b] Burel, G., 2011b.
Experimenting with deduction modulo, in:
Bjørner, N.,
Sofronie-Stokkermans, V. (Eds.), Proc. of CADE-23, Springer. pp. 162–176.

250
Christoph Benzm¨uller and Dale Miller
[Church, 1932] Church, A., 1932. A set of postulates for the foundation of logic. Annals of
Mathematics 33, 346–366.
[Church, 1936] Church, A., 1936. An unsolvable problem of elementary number theory. Ameri-
can Journal of Mathematics 58, 354–363.
[Church, 1940] Church, A., 1940.
A formulation of the simple theory of types.
Journal of
Symbolic Logic 5, 56–68.
[Chwistek, 1948] Chwistek, L., 1948. The Limits of Science: Outline of Logic and of the Method-
ology of the Exact Sciences. London: Routledge and Kegan Paul.
[Comon, 2001] Comon, H., 2001. Inductionless induction, in: Robinson, A., Voronkov, A. (Eds.),
Handbook of Automated Reasoning. Elsevier Science. volume I. chapter 14, pp. 913–962.
[Constable et al., 1986] Constable, R., Allen, S., Bromly, H., Cleaveland, W., Cremer, J.,
Harper, R., Howe, D., Knoblock, T., Mendler, N., Panangaden, P., Sasaki, J., Smith, S.,
1986. Implementing Mathematics with the Nuprl Proof Development System. Prentice-Hall.
[Coquand and Coquand, 1999] Coquand, C., Coquand, T., 1999. Structured type theory, in:
Felty, A. (Ed.), Proc. of LMF99: Workshop on Logical Frameworks and Meta-languages.
[Cousineau and Dowek, 2007] Cousineau, D., Dowek, G., 2007. Embedding pure type systems in
the lambda-pi-calculus modulo, in: Rocca, S.R.D. (Ed.), Typed Lambda Calculi and Applica-
tions, 8th International Conference, TLCA 2007, Paris, France, June 26-28, 2007, Proceedings,
Springer. pp. 102–117.
[Curry, 1942] Curry, H., 1942. The inconsistency of certain formal logics. Journal of Symbolic
Logic 7, 115–117.
[Dixon and Fleuriot, 2003] Dixon, L., Fleuriot, J.D., 2003. IsaPlanner: A prototype proof plan-
ner in isabelle, in: Baader, F. (Ed.), Automated Deduction - CADE-19, 19th International
Conference on Automated Deduction Miami Beach, FL, USA, July 28 - August 2, 2003,
Proceedings, Springer. pp. 279–283.
[Dowek, 1992] Dowek, G., 1992. Third order matching is decidable, in: 7th Symp. on Logic in
Computer Science, IEEE Computer Society Press, Santa Cruz, California. pp. 2–10.
[Dowek, 1993] Dowek, G., 1993. A complete proof synthesis method for the cube of type systems.
Journal of Logic and Computation 3, 287–315.
[Dowek, 2001] Dowek, G., 2001.
Higher-order uniﬁcation and matching, in:
Robinson, A.,
Voronkov, A. (Eds.), Handbook of Automated Reasoning. Elsevier Science, New York. vol-
ume II. chapter 16, pp. 1009–1062.
[Dowek, 2008] Dowek, G., 2008. Skolemization in simple type theory: the logical and the the-
oretical points of view, in: Reasoning in Simple Type Theory: Festschrift in Honor of Peter
B. Andrews on His 70th Birthday. College Publications. number 17 in Studies in Logic, pp.
244–255.
[Dowek et al., 2001] Dowek, G., Hardin, T., Kirchner, C., 2001. HOL-λσ an intentional ﬁrst-
order expression of higher-order logic.
Mathematical Structures in Computer Science 11,
1–25.
[Dowek et al., 2003] Dowek, G., Hardin, T., Kirchner, C., 2003. Theorem proving modulo. J.
Autom. Reasoning 31, 33–72.
[Enderton, 1972] Enderton, H.B., 1972. A Mathematical Introduction to Logic. Academic Press.
[Enderton, 2012] Enderton, H.B., 2012. Second-order and higher-order logic, in: Zalta, E.N.
(Ed.), The Stanford Encyclopedia of Philosophy. fall 2012 ed.. Stanford University.
[Farmer, 1990] Farmer, W.M., 1990. A partial functions version of church’s simple theory of
types. J. Symb. Log. 55, 1269–1291.
[Farmer, 1993] Farmer, W.M., 1993.
IMPS: An interactive mathematical proof system.
J.
Autom. Reasoning 11, 213–248.
[Farmer, 2008] Farmer, W.M., 2008. The seven virtues of simple type theory. J. Applied Logic
6, 267–286.
[Farmer et al., 1992] Farmer, W.M., Guttman, J.D., Thayer, F.J., 1992.
Little theories, in:
Kapur, D. (Ed.), Automated Deduction - CADE-11, 11th International Conference on Auto-
mated Deduction, Saratoga Springs, NY, USA, June 15-18, 1992, Proceedings, Springer. pp.
567–581.
[Felty, 2000] Felty, A., 2000. The calculus of constructions as a framework for proof search with
set variable instantiation. Theoretical Computer Science 232, 187–229.
[Frege, 1879] Frege, G., 1879.
Begriﬀsschrift, eine der arithmetischen nachgebildete Formel-
sprache des reinen Denkens. Halle. Translated in [van Heijenoort, 1967].

Automation of Higher-Order Logic
251
[Gacek et al., 2012] Gacek, A., Miller, D., Nadathur, G., 2012. A two-level logic approach to
reasoning about computations. J. Autom. Reasoning 49, 241–273.
[Gentzen, 1969a] Gentzen, G., 1969a.
Investigations into logical deduction, in: Szabo, M.E.
(Ed.), The Collected Papers of Gerhard Gentzen. North-Holland, Amsterdam, pp. 68–131.
Translation of articles that appeared in 1934-35.
[Gentzen, 1969b] Gentzen, G., 1969b.
New version of the consistency proof for elementary
number theory, in: Szabo, M.E. (Ed.), Collected Papers of Gerhard Gentzen. North-Holland,
Amsterdam, pp. 252–286. Originally published 1938.
[Girard, 1971] Girard, J.Y., 1971. Une extension de l’interpretation de G¨odel `a l’analyse, et son
application `a l’´elimination des coupures dans l’analyse et la th´eorie des types, in: Fenstad,
J.E. (Ed.), 2nd Scandinavian Logic Symposium. North-Holland, Amsterdam, pp. 63–92.
[Girard, 1986] Girard, J.Y., 1986. The system F of variable types: Fifteen years later. Theo-
retical Computer Science 45, 159–192.
[G¨odel, 1929] G¨odel, K., 1929. ¨Uber die Vollst¨andigkeit des Logikkalk¨uls. Ph.D. thesis. Univer-
sit¨at Wien.
[G¨odel, 1930a] G¨odel, K., 1930a. Die vollst¨andigkeit der axiome des logischen funktionenkalk¨uls.
Monatshefte f¨ur Mathematik 37, 349360.
[G¨odel, 1930b] G¨odel, K., 1930b.
Die Vollst¨andigkeit der Axiome des logischen Funktio-
nenkalk¨uls. Monatshefte f¨ur Mathematik und Physik 37, 349–360.
[G¨odel, 1931] G¨odel, K., 1931. ¨Uber formal unentscheidbare S¨atze der Principia Mathematica
und verwandter Systeme I. Monatshefte der Mathematischen Physik 38, 173–198. English
Version in [van Heijenoort, 1967].
[G¨odel, 1036] G¨odel, K., 1936. ¨Uber die L¨ange von Beweisen, in: Ergebnisse eines Mathematis-
chen Kolloquiums, pp. 23–24. English translation “On the length of proofs” in Kurt G¨odel:
Collected Works, Volume 1, pages 396-399, Oxford University Press, 1986.
[Goldfarb, 1981] Goldfarb, W., 1981. The undecidability of the second-order uniﬁcation prob-
lem. Theoretical Computer Science 13, 225–230.
[Gorden and Melham, 1993] Gordon, M., Melham, T., 1993. Introduction to HOL: A Theorem-
Proving Environment for Higher-Order Logic. Cambridge University Press.
[Gordon et al., 1979] Gordon, M.J.C., Milner, R., Wadsworth, C.P., 1979.
Edinburgh LCF.
volume 78 of LNCS. Springer.
[Gould, 1966] Gould, W.E., 1966. A Matching Procedure for ω-Order Logic. Technical Report
Scientiﬁc Report No. 4. A F C R L.
[Guard, 1964] Guard, J.R., 1964. Automated logic for semi-automated mathematics, in: Scien-
tiﬁc Report No 1. A F C R L, pp. 64–411.
[Guttmann et al., 2011] Guttmann, W., Struth, G., Weber, T., 2011.
Automating algebraic
methods in isabelle, in: Qin, S., Qiu, Z. (Eds.), Proc. of ICFEM 2011, Springer. pp. 617–632.
[Hales, 2013] Hales, T., 2013. Mathematics in the Age of the Turing Machine. ArXiv e-prints
arXiv:1302.2898.
[Harrison, 2009] Harrison, J., 2009. HOL Light: An overview, in: Berghofer, S., Nipkow, T.,
Urban, C., Wenzel, M. (Eds.), Theorem Proving in Higher Order Logics, 22nd International
Conference, TPHOLs 2009, Munich, Germany, August 17-20, 2009. Proceedings, Springer.
pp. 60–66.
[van Heijenoort, 1967] van Heijenoort, J., 1967. From Frege to G¨odel: A Source Book in Math-
ematics, 1879-1931. Source books in the history of the sciences series. 3rd printing, 1997 ed.,
Harvard Univ. Press, Cambridge, MA.
[Henkin, 1950] Henkin, L., 1950.
Completeness in the theory of types. Journal of Symbolic
Logic 15, 81–91.
[Henkin, 1963] Henkin, L., 1963. A theory of propositional types. Fundamatae Mathematicae ,
323–344.
[Henschen, 1972] Henschen, L.J., 1972. N-sorted logic for automatic theorem-proving in higher-
order logic, in: Proceedings of the ACM Annual Conference - Volume 1, ACM, New York,
NY, USA. pp. 71–81.
[Hermant and Lipton, 2010] Hermant, O., Lipton, J., 2010. Completeness and cut-elimination
in the intuitionistic theory of types - part 2. J. Logic and Computation 20, 597–602.
[Hilbert, 1899] Hilbert, D., 1899. Grundlagen der geometrie, in: Festschrift zur Feier der Enth-
llung des Gauss-Weber-Denkmals in G¨ottingen. B.G. Teubner, Leipzig, pp. 1–92.
[Holmes and Alves-Foss, 2001] Holmes, M.R., Alves-Foss, J., 2001. The Watson theorem prover.
J. Autom. Reasoning 26, 357–408.

252
Christoph Benzm¨uller and Dale Miller
[Huet, 1973a] Huet, G., 1973a. The undecidability of uniﬁcation in third order logic. Information
and Control 22, 257–267.
[Huet, 1975] Huet, G., 1975. A uniﬁcation algorithm for typed λ-calculus. Theoretical Computer
Science 1, 27–57.
[Huet and Lang, 1978] Huet, G., Lang, B., 1978. Proving and applying program transformations
expressed with second-order patterns. Acta Informatica 11, 31–55.
[Huet, 1972] Huet, G.P., 1972. Constrained Resolution: A Complete Method for Higher Order
Logic. Ph.D. thesis. Case Western Reserve University.
[Huet, 1973b] Huet, G.P., 1973b. A mechanization of type theory, in: Proceedings of the 3rd
International Joint Conference on Artiﬁcial Intelligence, pp. 139–146.
[Hurd, 2003] Hurd, J., 2003.
First-order proof tactics in higher-order logic theorem provers,
in: Design and Application of Strategies/Tactics in Higher Order Logics, number NASA/CP-
2003-212448 in NASA Technical Reports, pp. 56–68.
[Jensen and Pietzkowski, 1976] Jensen, D.C., Pietrzykowski, T., 1976.
Mechanizing omega-
order type theory through uniﬁcation. Theor. Comput. Sci. 3, 123–171.
[Kaliszyk and Urban, 2012] Kaliszyk, C., Urban, J., 2012. Learning-assisted automated reason-
ing with ﬂyspeck. CoRR abs/1211.7012.
[Kaufmann and Moore, 1997] Kaufmann, M., Moore, J.S., 1997. An industrial strength theorem
prover for a logic based on Common Lisp. IEEE Trans. Software Eng. 23, 203–213.
[Kerber, 1991] Kerber, M., 1991. How to prove higher order theorems in ﬁrst order logic, in:
Mylopoulos, J., Reiter, R. (Eds.), Proc. of IJCAI-12, Morgan Kaufmann. pp. 137–142.
[Kerber, 1994] Kerber, M., 1994. On the translation of higher-order problems into ﬁrst-order
logic, in: Proc. of ECAI, pp. 145–149.
[Kleene and Rosser, 1935] Kleene, S., Rosser, J., 1935.
The inconsistency of certain formal
logics. Annals of Mathematics 36, 630–636.
[Kohlhase, 1994] Kohlhase, M., 1994. A Mechanization of Sorted Higher-Order Logic Based on
the Resolution Principle. Ph.D. thesis. Saarland University.
[Leivant, 1994] Leivant, D., 1994. Higher-order logic, in: Gabbay, D.M., Hogger, C.J., Robinson,
J.A. (Eds.), Handbook of Logic in Artiﬁcial Intelligence and Logic Programming. Oxford
University Press. volume 2, pp. 229–321.
[Liang and Miller, 2009] Liang, C., Miller, D., 2009. Focusing and polarization in linear, intu-
itionistic, and classical logics. Theoretical Computer Science 410, 4747–4768.
[Liang et al., 2005] Liang, C., Nadathur, G., Qi, X., 2005. Choices in representing and reduction
strategies for lambda terms in intensional contexts. Journal of Automated Reasoning 33, 89–
132.
[Lindblad, 2013] Lindblad, F., 2013.
agsyHOL website.
https://github.com/frelindb/
agsyHOL.
[Lucchesi, 1972] Lucchesi, C.L., 1972. The Undecidability of Uniﬁcation for Third Order Lan-
guages. Technical Report Report CSRR 2059. Dept of Applied Analysis and Computer Sci-
ence, University of Waterloo.
[Martin-L¨of, 1982] Martin-L¨of, P., 1982. Constructive mathematics and computer programming,
in: Sixth International Congress for Logic, Methodology, and Philosophy of Science, North-
Holland, Amsterdam. pp. 153–175.
[McDowell and Miller, 2002] McDowell, R., Miller, D., 2002. Reasoning with higher-order ab-
stract syntax in a logical framework. ACM Trans. Comput. Log. 3, 80–136.
[Meng and Paulson, 2008] Meng, J., Paulson, L.C., 2008. Translating higher-order clauses to
ﬁrst-order clauses. J. Autom. Reasoning 40, 35–60.
[Miller, 1983] Miller, D., 1983. Proofs in Higher-Order Logic. Ph.D. thesis. Carnegie-Mellon
University.
[Miller, 1987] Miller, D., 1987. A compact representation of proofs. Studia Logica 46, 347–370.
[Miller, 1991] Miller, D., 1991. A logic programming language with lambda-abstraction, function
variables, and simple uniﬁcation. Journal of Logic and Computation 4, 497–536.
[Miller, 1992] Miller, D., 1992. Uniﬁcation under a mixed preﬁx. Journal of Symbolic Compu-
tation 14, 321–358.
[Miller, 2011] Miller, D., 2011. A proposal for broad spectrum proof certiﬁcates, in: Jouannaud,
J.P., Shao, Z. (Eds.), CPP: First International Conference on Certiﬁed Programs and Proofs,
pp. 54–69.
[Miller and Nadathur, 2012] Miller, D., Nadathur, G., 2012. Programming with Higher-Order
Logic. Cambridge University Press.

Automation of Higher-Order Logic
253
[Miller et al., 1982] Miller, D.A., Cohen, E.L., Andrews, P.B., 1982. A look at TPS, in: Love-
land, D.W. (Ed.), Sixth Conference on Automated Deduction, Springer, New York. pp. 50–69.
[Mints, 1999] Mints, G., 1999. Cut-elimination for simple type theory with an axiom of choice.
J. Symb. Log. 64, 479–485.
[Mossakowski et al., 2007] Mossakowski, T., Maeder, C., L¨uttich, K., 2007. The heterogeneous
tool set, Hets, in: Proceedings of TACAS 2007, Springer. pp. 519–522.
[Muskens, 2007] Muskens, R., 2007. Intensional models for the theory of types. J. Symb. Log.
72, 98–118.
[Nadathur and Linnell, 2005] Nadathur, G., Linnell, N., 2005. Practical higher-order pattern
uniﬁcation with on-the-ﬂy raising, in: ICLP 2005: 21st International Logic Programming
Conference, Springer, Sitges, Spain. pp. 371–386.
[Nadathur and Miller, 1988] Nadathur, G., Miller, D., 1988. An Overview of λProlog, in: Fifth
International Logic Programming Conference, MIT Press, Seattle. pp. 810–827.
[Nederpelt et al., 1994] Nederpelt, R.P., Geuvers, J.H., Vrijer, R.C.D. (Eds.), 1994. Selected
Papers on Automath. volume 133 of Studies in Logic and The Foundations of Mathematics.
North Holland.
[Nipkow, 1989] Nipkow, T., 1989. Equational reasoning in Isabelle. Sci. Comput. Program. 12,
123–149.
[Nipkow et al., 2002] Nipkow, T., Paulson, L., Wenzel, M., 2002. Isabelle/HOL: A Proof Assis-
tant for Higher-Order Logic. Number 2283 in LNCS, Springer.
[Owre et al., 1992] Owre, S., Rushby, J., Shankar, N., 1992.
PVS: A Prototype Veriﬁcation
System, in: D., K. (Ed.), Proceedings of the 11th International Conference on Automated
Deduction, Springer. pp. 748–752.
[Padovani, 2000] Padovani, V., 2000.
Decidability of fourth-order matching.
Mathematical
Structures in Computer Science 10, 361–372.
[Parikh, 1973] Parikh, R.J., 1973. Some results on the length of proofs. Transactions of the
ACM 177, 29–36.
[Paulson, 1989] Paulson, L.C., 1989. The foundation of a generic theorem prover. J. Autom.
Reasoning 5, 363–397.
[Paulson, 1994] Paulson, L.C., 1994. Isabelle - A Generic Theorem Prover (with a contribution
by T. Nipkow). volume 828 of LNCS. Springer.
[Paulson, 1999] Paulson, L.C., 1999. A generic tableau prover and its integration with isabelle.
Journal of Universal Computer Science 5, 51–60.
[Peano, 1889] Peano, G., 1889. Arithmetices principia, nova methodo exposita. Fratres Bocca,
Turin.
[Pfenning, 1987] Pfenning, F., 1987. Proof Transformations in Higher-Order Logic. Ph.D. thesis.
Carnegie Mellon University. 156 pp.
[Pfenning, 1994] Pfenning, F., 1994. Elf: A meta-language for deductive systems (system des-
crition), in: Bundy, A. (Ed.), Automated Deduction - CADE-12, 12th International Confer-
ence on Automated Deduction, Nancy, France, June 26 - July 1, 1994, Proceedings, Springer.
pp. 811–815.
[Pfenning and Sch¨urmann, 1999] Pfenning, F., Sch¨urmann, C., 1999. System description: Twelf
- a meta-logical framework for deductive systems, in: Ganzinger, H. (Ed.), Automated De-
duction - CADE-16, 16th International Conference on Automated Deduction, Trento, Italy,
July 7-10, 1999, Proceedings, Springer. pp. 202–206.
[Pientka and Dunﬁeld, 2010] Pientka, B., Dunﬁeld, J., 2010. Beluga: A framework for program-
ming and reasoning with deductive systems (system description), in: Giesl, J., H¨ahnle, R.
(Eds.), Automated Reasoning, 5th International Joint Conference, IJCAR 2010, Edinburgh,
UK, July 16-19, 2010. Proceedings, Springer. pp. 15–21.
[Pietrzkowski, 1973] Pietrzykowski, T., 1973. A complete mechanization of second-order type
theory. J. ACM 20, 333–364.
[Pietrzkowski and Jensen, 1972] Pietrzykowski, T., Jensen, D.C., 1972. A complete mechaniza-
tion of ω-order type theory, in: ACM ’72: Proceedings of the ACM annual conference, ACM
Press, New York, NY, USA. pp. 82–92.
[Pollack, 1994] Pollack, R., 1994. The Theory of LEGO. Ph.D. thesis. University of Ediburgh.
[Prawitz, 1968] Prawitz, D., 1968. Hauptsatz for higher order logic. Journal of Symbolic Logic
33, 452–457.
[Quine, 1940] Quine, W.V.O., 1940. Mathematical Logic. Harvard University Press, Boston,
MA.

254
Christoph Benzm¨uller and Dale Miller
[Ramsey, 1926] Ramsey, F.P., 1926. The foundations of mathematics, in: Proceedings of the
London Mathematical Society, pp. 338–384.
[Reynolds, 1974] Reynolds, J.C., 1974. Towards a theory of type structure, in: Colloque sur la
Programmation, Paris, France, Springer, New York. pp. 408–425.
[Richardson et al., 1998] Richardson, J., Smaill, A., Green, I., 1998. System description: Proof
planning in higher-order logic with lambda-Clam, in:
Kirchner, C., Kirchner, H. (Eds.),
Automated Deduction - CADE-15, 15th International Conference on Automated Deduction,
Lindau, Germany, July 5-10, 1998, Proceedings, Springer. pp. 129–133.
[Robinson, 1969] Robinson, J.A., 1969. Mechanizing higher-order logic, in: Machine Intelligence
4. Edinburgh University Press, pp. 151–170.
[Robinson, 1970] Robinson, J.A., 1970. A note on mechanizing higher order logic, in: Machine
Intelligence 5. Edinburgh University Press, pp. 121–135.
[Russell, 1902] Russell, B., 1902. Letter to Frege. Translated in [van Heijenoort, 1967].
[Russell, 1903] Russell, B., 1903. The principles of mathematics. Cambridge University Press,
Cambridge, England.
[Russell, 1908] Russell, B., 1908. Mathematical logic as based on the theory of types. American
Journal of Mathematics 30, 222–262.
[Schulz, 2002] Schulz, S., 2002. E – a brainiac theorem prover. AI Communications 15, 111–126.
[Sch¨utte, 1960] Sch¨utte, K., 1960. Semantical and syntactical properties of simple type theory.
Journal of Symbolic Logic 25, 305–326.
[Shapiro, 1985] Shapiro, S., 1985. Second-order languages and mathematical practice. Journal
of Symbolic Logic 50, 714–742.
[Smullyan, 1963] Smullyan, R.M., 1963. A unifying principle for quantiﬁcation theory. Proc.
Nat. Acad Sciences 49, 828–832.
[Snyder and Gallier, 1989] Snyder, W., Gallier, J.H., 1989. Higher order uniﬁcation revisited:
Complete sets of transformations. Journal of Symbolic Computation 8, 101–140.
[Spenger and Dams, 2003] Spenger, C., Dams, M., 2003. On the structure of inductive reason-
ing: Circular and tree-shaped proofs in the µ-calculus, in: Gordon, A. (Ed.), FOSSACS’03,
Springer. pp. 425–440.
[Stirling, 2009] Stirling, C., 2009. Decidability of higher-order matching. Logical Methods in
Computer Science 5, 1–52.
[Sutcliﬀe, 2009] Sutcliﬀe, G., 2009. The TPTP problem library and associated infrastructure.
Journal of Automated Reasoning 43, 337–362.
[Sutcliﬀe and Benzm¨uller, 2010] Sutcliﬀe, G., Benzm¨uller, C., 2010. Automated reasoning in
higher-order logic using the TPTP THF infrastructure. Journal of Formalized Reasoning 3,
1–27.
[Tait, 1966] Tait, W.W., 1966. A nonconstructive proof of Gentzen’s Hauptsatz for second order
predicate logic. Bulletin of the American Mathematical Society 72, 980983.
[Takahashi, 1967] Takahashi, M., 1967. A proof of cut-elimination theorem in simple type theory.
Journal of the Mathematical Society of Japan 19, 399–410.
[Takeuti, 1953] Takeuti, G., 1953. On a generalized logic calculus. Japanese Journal of Mathe-
matics 23, 39–96. Errata: ibid, vol. 24 (1954), 149–156.
[Takeuti, 1960] Takeuti, G., 1960. An example on the fundamental conjecture of GLC. Journal
of the Mathematical Society of Japan 12, 238–242.
[Takeuti, 1975] Takeuti, G., 1975. Proof Theory. volume 81 of Studies in Logic and the Foun-
dations of Mathematics. Elsevier.
[Westerst˚ahl, 2011] Westerst˚ahl, D., 2011. Generalized quantiﬁers, in: Zalta, E.N. (Ed.), The
Stanford Encyclopedia of Philosophy. summer 2011 ed.
[Whitehead and Russell, 1910–1913] Whitehead, A.N., Russell, B., 1910, 1912, 1913. Principia
Mathematica, 3 vols. Cambridge: Cambridge University Press. Second edition, 1925 (Vol.
1), 1927 (Vols 2, 3). Abridged as Principia Mathematica to *56, Cambridge: Cambridge
University Press, 1962.
[Wirth, 2004] Wirth, C.P., 2004. Descente inﬁnie + deduction. Logic Journal of the IGPL 12,
1–96.

EQUATIONAL LOGIC AND REWRITING
Claude Kirchner and H´el`ene Kirchner
Readers: Franz Baader and Deepak Kapur
1
INTRODUCTION
Equational logic is the logic of equality, a concept that looks familiar and is often
encountered, but that has actually diﬀerent facets. In high-school mathematics,
equality is a binary relation between objects meaning that two objects are identical,
in the sense that replacement of one by the other in an expression does not change
its value. An equality is written with the predicate symbol “=” and two expressions
of the same nature, in mathematics usually numbers, vectors, functions, sets,
matrices, . . . Equalities are used in many diﬀerent situations, either to abbreviate
expressions (e.g. ∆= b2 −4ac), to express the result of a calculus (e.g. 2+3 = 5),
to write identities (e.g. E = mc2), to model a problem under the form of an
equation (e.g. x2 + y2 = z2) and to describe the axiomatic properties of a given
structure (e.g. x+y = y+x) . It may happen that this variety of usage introduces
some confusion.
If equality looks so familiar, it is maybe also because it has a very long history. It
is generally admitted that the ﬁrst printed occurrence of the = sign is found in the
book entitled The Whetstone of Witte by the mathematician and physician Robert
Recorde (1510-1558) in 1557.
The sign was found also in italian manuscripts
earlier in the 16th century and was already used by egyptians to denote friendship.
Equality could also be associated to the result of a calculation, i.e. mathematical
determination of the amount or number of something. Not surprisingly, it is also
a primary concept in computation, i.e. the action of mathematical calculation,
and associated to mechanical calculators, invented by Blaise Pascal since 1642,
followed by Leibniz in 1685 who gave a deﬁnition of equality intuitively stating
that two entities are identical if and only if they have the same properties so that
they are undistinguishable.
Logicians have addressed early the need for clariﬁcation and rigourous deﬁnition.
Axiomatization of equality on natural numbers provided by Peano in 1889 can be
considered as the ﬁrst step to deﬁne equational logic. Later on, in the 20th century,
with Russell, Tarski and Church, set and type theories were developed and also
addressed equality. In set theory for instance, a set is completely characterized
by its elements, although it may be deﬁned by diﬀerent properties.
Formally,
∀x, y, (x = y) iﬀ(∀P, P(x) iﬀP(y)) where P denotes a predicate. However this
Handbook of the History of Logic. Volume 9: Computational Logic. 
Volume editor: Jörg Siekmann 
Series editors: Dov M. Gabbay and John Woods 
Copyright © 2014 Elsevier BV. All rights reserved.

256
Claude Kirchner and H´el`ene Kirchner
deﬁnition needs a quantiﬁcation on predicate and so does not belong to ﬁrst-order
logic. In set theory, its instance is the property of extensionality: is A and B are
two sets, A = B iﬀ(∀x, x ∈A iﬀx ∈B). The idea that types comes equipped
with an equality relation was made explicit by Bishop in 1967 and in Martin-L¨of’s
type theory in 1972. In Church’s type theory, only one notion of equality exists:
it expresses that two expressions denote the same object, either as a consequence
of a deﬁnition or as the result of a more complex reasoning such as replacement
of equals by equals. In that case the two expressions contain variables that are
universally quantiﬁed.
For instance ∀x, y ∈Reals, (x + y)2 = x2 + y2 + 2xy.
Intuitionistic type theory introduces a second notion called equality by deﬁnition,
where two propositions are equal if every proof of one of them is also a proof of
the other one.
The transition between logic and computer science is largely due to Alan Turing
(1912-1954), by giving a formalisation of the concepts of ”algorithm” and ”compu-
tation” with the Turing machine. After Turing’s famous work on the halting prob-
lem, showing the existence of a problem unsolvable by mechanical means, Church
and Turing then proved that the lambda calculus and the Turing machine used
in the halting problem were equivalent in capabilities, and subsequently provided
a mechanical process for computation. With this mechanization and the need to
give an operational (implementable) version of equality, while avoiding the halting
problem intrinsically related to the symetrical nature of equality, the notion of
rewriting as an oriented equality came up. Then rewrite relation and equality,
studied as two diﬀerent concepts, nevertheless have had interleaved developments,
at the intersection of logic and computation, i.e. of proof and algorithm.
In this survey, we do not address higher-order logics nor type theory, but rather
restrict to ﬁrst-order concepts. We focus on equational logic and its relation to
rewriting logic and we consider their impact on automated deduction and pro-
gramming languages.
We organize our view of equational logic and rewriting history along three lines
of approach, corresponding more or less to time periods where the research has
been especially active on these respective approaches:
• From 1935 to 1970, the theory and model settings main period, study-
ing equational logics and their models, with the development of data types
and algebraic speciﬁcation languages;
• From 1970 to 2000, the operational semantics period, based on the study
of rewriting on words or terms, with its use in main algebraic language im-
plementations as well as automated deduction for a large class of equational
theories;
• From 2000, the logical framework period with the formalisation of rewrit-
ing calculi and logics, covering diﬀerent types of structures (words, terms,
graphs, formulas, proofs) with types, sorts, conditions, constraints,. . . and
abstracted in the concept of abstract reduction systems.

Equational Logic and Rewriting
257
In this survey, our intention is to describe the evolution of ideas rather than
to cover all places where equality and equational logic are useful and have been
embedded. Therefore, we do not attempt to be exhaustive in the description of
all the results but rather give our personal perception and vision of this ﬁeld main
historical steps. However, references to technical surveys are expected to provide
detailed information to interested readers.
2
EQUATIONAL LOGICS AND THEIR MODELS
At the beginning of the 20th century, the notions of abstract algebra, algebraic
logic and universal algebra emerge in particular in the works of Alfred Tarski
and Garrett Birkhoﬀ.
Function symbols and equalities at the logical level are
interpreted by operations on algebras.
In equational logic, formulas are built from the equality predicate and ﬁrst-order
terms, i.e. well-formed expressions built on a set F of function symbols with arity
(the number of arguments) and a set X of variables. The set of terms is denoted
T (F, X). Variables of a term may be instantiated by substitutions, deﬁned as
as mappings σ from a subset of variables to terms and extended on terms by
preserving their structure.
An equational axiom or equality is denoted (∀X, l = r) where X is the set of
variables occurring in the terms l and r. From a set of axioms, new equalities can
be deduced via inference rules. A deduction system for equational deduction is
given in Figure 1.
Reﬂexivity
t = t
Symmetry
t = t′
t′ = t
Transitivity
t1 = t2
t2 = t3
t1 = t3
Congruence For any f ∈Fn
ti = t′
i, i = 1 . . . n
f(t1, . . . , tn) = f(t′
1, . . . , t′n)
Substitutivity. For each substitution σ
t = t′
σ(t) = σ(t′)
Figure 1. Deduction rules for Equational Logic

258
Claude Kirchner and H´el`ene Kirchner
Given a set of equational axioms E and a set of terms T (F, X), the equational
theory of E, denoted T h(E), is the set of equalities that can be obtained, starting
from E, by applying the inference rules given in Figure 1. We write
E ⊢s = t if (s = t) ∈T h(E).
A more compact inference system is the so-called replacement of equals by
equals: given a set E of axioms, we write s ←→E t if there is an equality l = r (or
r = l) in E, such that the subterm of s at position ω, denoted s|ω, is an instance
σ(l) of l for some substitution σ and if t just diﬀers from s by containing σ(r) at
position ω, which is denoted as t = s[σ(r)]ω. The reﬂexive transitive closure of
the above symmetric relation is denoted
∗
←→E. It is a congruence relation, the
quotient of the set of terms T (F, X) by
∗
←→E is denoted by T (F, X)/E and the
equivalence class of a term t is denoted ⟨t⟩E.
The following theorem due to Birkhoﬀ[Birkhoﬀ, 1935] states the equivalence
between the two inference systems for equality deduction:
E ⊢s = t iﬀs
∗
←→E t.
Models of equational logic are F-algebras, that are non-empty sets (called the
carriers), with operations interpreting the symbols in F. Mappings preserving the
symbol interpretations are called homomorphisms.
In a non-empty class C of F-algebras, there may exist two particularily inter-
esting algebras: one is an initial algebra I such that for any algebra A in C, there
exists a unique homomorphism φ : I →A. The other one is the free algebra T
over a set of variables X such that: for any algebra A in C and any assignment
ν : X →A, ν can be extended by a unique homomorphism φ : T →A.
Note that by deﬁnition, the initial algebra can also be seen as the free algebra
over the empty set of variables. When it exists, it is unique up to an isomorphism.
Given a set E of equality axioms, an algebra A is a model of E if for every axiom
s = t in E, for any assignment ν of the variables from s and t into the carrier of A,
ν(s) = ν(t). We also say that the equality s = t is valid in A, and this is denoted
by A |= s = t. Mod(E) denotes the set of models of E. The fondamental theorem
due to Birkhoﬀ[Birkhoﬀ, 1935] relates models and equational deduction: for any
set of axioms E, for any terms s, t ∈T (F, X),
Mod(E) |= s = t iﬀT (F, X)/E |= s = t iﬀE ⊢s = t iﬀs
∗
←→E t
i.e. validity in the models is equivalent to replacement of equals by equals, therefore
a semantic check can be achieved completely syntactically. We write s =E t iﬀ
Mod(E) |= s = t. Thanks to these results, the notations s =E t and s
∗
←→E t
may be used interchangeably.
In the class of algebras that are models of E, the free algebra over X is (iso-
morphic to) the algebra T (F, X)/E. In the class of algebras that are models of
E, the initial algebra is (isomorphic to) the algebra T (F)/E.

Equational Logic and Rewriting
259
When only arity of functions is speciﬁed, it is easy to build terms such as
divide(2, 0) that do not have interpretations. By introducing a type Nat for nat-
ural numbers and Nat+ for non-zero natural numbers, and by typing the function
divide with the rank Nat, Nat+ 7→Nat, such written terms do not satisfy the
typing constraint. Sorts have been introduced for classifying terms and typing
arguments and results of functions. In a many-sorted signature, a function f has
a rank (or type) f : s1 . . . sn 7→s, with si, s in a set of sorts S and variables
are typed too, for instance x : s means that variable x has sort s. Many-sorted
terms are built on many-sorted signatures and classiﬁed according to their sorts.
Many-sorted algebras have carriers corresponding to each sort and operations with
sorted arguments.
Deduction rules for equational deduction given in Figure 1 generalize to the
many-sorted framework provided there is no empty sort. Goguen and Meseguer
made a precise analysis of the possible problems that may arise when this hypoth-
esis is not satisﬁed [Meseguer and Goguen, 1985]. For keeping valid the deduction
rules of Figure 1 for many-sorted logic, only models with non-empty sorts should
be considered.
Such type structure supports conceptual clarity and detection of many errors.
However, implementing strong typing in many-sorted logic is quite rigid and lacks
the expressive power needed for handling errors and partiality, see the survey by
Mosses and all of diﬀerent approaches to address these problems [Mosses, 1997].
One of them, initiated in 1977 by Goguen leads to [Goguen and Meseguer, 1987],
where an order-sorted type structure is proposed, making many seemingly partial
or problematic functions total and well-deﬁned on the right subsort. This order-
sorted type structure is provided by a partial ordering on the set of sorts that is in-
terpreted as set inclusion in order-sorted algebras. For instance, a subsort relation
Nat < Int is interpreted as the inclusion N ⊆Z of the naturals into the integers
in the standard model. In addition, operator symbols such as + may have over-
loaded declarations, for instance + :
Nat Nat -> Nat, + :
Int Int -> Int,
that are required to yield the same result when restricted to arguments of the same
subsorts. Overloading is a syntactical facility for handling operators deﬁned on
diﬀerent subsets that may intersect, for achieving a kind of polymorphism. More-
over such operators are in general partial functions, which is expressed thanks to
the subsort relationship.
Although all basic results of equational logic generalize to the many-sorted case,
order-sorted deduction is more subtle [Goguen, 1992]. For example, replacement
of equals by equals and term rewriting require a careful analysis that was initiated
in [Goguen et al., 1985] and is further developed in [Isakowitz and Gallier, 1988;
Kirchner et al., 1988].
All these concepts have been used to deﬁne the semantics of several speciﬁcation
and programming languages. Let us mention two of them.
The OBJ programming language originates from Goguen’s pioneering work at
UCLA [Goguen and Tardo, 1979; Goguen and Malcolm, 2000] in the late seventies,
later fully developed at SRI International by Goguen, Meseguer and several invited

260
Claude Kirchner and H´el`ene Kirchner
collaborators [Futatsugi et al., 1985; Goguen et al., 1987; Jouannaud et al., 1992;
Kirchner et al., 2000]. OBJ is algebraic programming language whose latest ver-
sions (OBJ-2 [Futatsugi et al., 1985] and OBJ-3 [Goguen et al., 1987]) are based on
order-sorted equational logic. Programs are order-sorted equational speciﬁcations
and computation is order-sorted equational deduction [Kirchner et al., 1988].
In the late 1990’s, the CoFI initiative has been created and sponsored by the
IFIP working group WG1.3.
It was an open collaborative eﬀort to produce a
Common Framework for Algebraic Speciﬁcation and Development, together with
the so-called Common Algebraic Speciﬁcation Language Casl, a general-purpose
speciﬁcation language based on ﬁrst-order logic [Astesiano et al., 2002] that also
supports partial functions and subsorting. Casl has been designed with the aim
to subsume many existing speciﬁcation languages and to implement the most
important features of algebraic speciﬁcations.
3
REWRITING TECHNIQUES
From about 1970, the need for an operational version of equational deduction came
up in two diﬀerent communities, in formal speciﬁcations and functional program-
ming on one hand, in automated reasoning on the other. With the emergence
of equationally speciﬁed abstract data type, the question was to get executable
speciﬁcations and to deﬁne operational semantics of function evaluation. In au-
tomated reasoning, the problem was to mechanize equational deduction and ﬁnd
decision procedures for equational theories. Giving a common solution to these
two questions also provides a bridge between programming language theory and
program veriﬁcation.
In both situations, automating equational deduction needs to compute which
is the right axiom to be applied at each step, in which direction, and possibly to
backtrack. The idea of rewriting is to suppress the need for backtracking, ﬁrst by
using oriented axioms from left to right only, second by giving enough oriented
axioms to have the same deduction power than originally. For instance, group
theory is deﬁned by three axioms (neutral element, inverse and associativity) but
deciding equality in group theory needs ten oriented axioms (equivalent to the three
previous ones), ﬁrst discovered in the pioneer work of Knuth and Bendix [Knuth
and Bendix, 1970]. We now recall the basic notions of rewrite systems, together
with properties that rewriting must satisfy in order to provide a decision procedure
for equational theories.
3.1
The initial concept
With the purpose to operationalize equation deduction, term rewriting was ﬁrst
proposed by Evans [Evans, 1951], then by Knuth and Bendix [Knuth and Bendix,
1970].
The original purpose was to generate canonical term rewriting systems
which can be used as decision procedures for proving the validity of equalities in
ﬁrst-order equational theories.

Equational Logic and Rewriting
261
The central idea of rewriting is to impose directionality in the use of equalities.
So a rewrite rule is an ordered pair of terms l, r ∈T (F, X), denoted l ⇒r, where
it is often required that l is not a variable and all variables of r are also variables of
r. The terms l and r are respectively called the left-hand side and the right-hand
side of the rule. A rewrite system is a (ﬁnite or inﬁnite) set of rewrite rules.
A rule is applied by replacing an instance of the left-hand side by the same
instance of its right-hand side, but never the converse, contrary to equalities.
Given a rewrite system R, a term t in T (F, X) rewrites to a term t′ if there
exists a rewrite rule l ⇒r of R, a position ω in t and a substitution σ such that
t|ω = σ(l) and t′ = t[σ(r)]ω.
This is denoted t −→ω,l⇒r,σ t′ or simply t →t′
when there is no need to make precise which rewrite rule is used nor where. A
subterm t|ω where the rewriting step is applied is called redex. A term that has
no redex is said to be irreducible for R or to be in R-normal form. The induced
rewrite relation on terms is the reﬂexive transitive closure of −→. From a logical
point of view, the deduction rules are similar to equational logic (cf. deduction
rules in Figure 1) without the Symmetry rule and with the rewrite relation
instead of equality. Surveys on term rewriting include [Huet and Oppen, 1980;
Dershowitz and Jouannaud, 1990; Avenhaus and Madlener, 1990; Klop, 1990;
Plaisted, 1993; Baader and Nipkow, 1998; Terese, 2003; Ohlebusch, 2002; Bezem
et al., 2003].
3.2
Termination
The normalizing property of a rewrite system (i.e. every term has a normal form)
is not enough if inﬁnite computations of normal forms must be avoided. Indeed,
a term may have a normal form and nevertheless an inﬁnite rewriting derivation.
So a stronger property is often needed, namely the termination property.
In
general, it is undecidable whether a rewrite system is terminating. The idea of the
undecidability proof is the following: given any Turing machine M, there exists
a rewrite system RM such that RM terminates for all terms iﬀM halts for all
input tapes. Since it is undecidable if a Turing machine halts uniformly, it is also
undecidable if rewrite systems terminate [Dershowitz, 1985b]. This shows that
ﬁrst-order term rewriting has the expressive power of any Turing machine that
can even be encoded using a single rewrite rule left linear rewrite rule as shown
by Dauchet [Dauchet, 1989].
A ﬁrst approach, developped from the late 70th and landmarked by the works of
Dershowitz, is to relate termination with orderings [Dershowitz, 1982] and rewrite
derivations to decreasing chains in a well-founded ordered structure. A reduction
ordering is a well-founded ordering on terms closed under congruence and substi-
tution. Then termination of rewriting can be proved by just comparing left and
right-hand sides of rules: a rewrite system R over the set of terms is terminating
iﬀthere exists a reduction ordering > such that each rule l ⇒r ∈R satisﬁes
l > r. To take advantage of known ordered structures, terms may be interpreted,
for instance by polynomials [Steinbach, 1994].

262
Claude Kirchner and H´el`ene Kirchner
Another approach, emerging in the early 80’s, is the notion of simpliﬁcation
ordering. In addition to be reduction orderings, simpliﬁcation orderings enjoy the
so-called subterm property: any term is greater than any of its subterms. If the
set F of operator symbols is ﬁnite, a rewrite system R is terminating if R is sim-
ply terminating [Dershowitz, 1982]. Simpliﬁcation orderings can be built from a
well-founded ordering on the function symbols F called a precedence. Examples
are the multiset path ordering [Jouannaud and Lescanne, 1982], also called recur-
sive path ordering, the lexicographic path ordering or the recursive decomposition
ordering [Lescanne, 1990].
Simple termination is reviewed in [Middeldorp and
Zantema, 1997].
During the 80’s and 90’s, many orderings and termination proof techniques
have been developed:
Knuth-Bendix orderings, forward closures, semantic in-
terpretations, transformation ordering, dummy elimination, semantic labelling.
Comprehensive surveys on termination are [Dershowitz, 1987; Zantema, 1994;
Zantema, 1995; Steinbach, 1995].
Then, since 2000, the interest in termination was renewed with the dependency
pairs approach introduced by Aart and Giesl in [Arts and Giesl, 2000], an auto-
mated method where left-hand sides are compared with special subterms of the
right-hand sides to capture forward closure. A lot of eﬀorts have been devoted to
make the method faster, more powerful and able to handle termination of func-
tional programs.
Several systems for automated termination proofs have been
developed, such as CiME [Contejean et al., 2005], TTT [Hirokawa and Middel-
dorp, 2003], AProVE [Giesl et al., 2006], TORPA [Zantema, 2005], and integrated
in software development tools that analyze the termination of programs in pro-
gramming languages like Java, C, Haskell, and Prolog.
Considering termination of programming languages also led to study termina-
tion of rewriting under evaluation strategies. The dependency pairs approach has
been adapted to such cases and termination of rewriting under strategies is also
addressed in [Gnaedig and Kirchner, 2009].
To complete this brief historical panorama, modular termination techniques
have to be mentioned. In complex proofs of termination, diﬀerent orderings are
combined. Usual combinations are multiset extension or lexicographic combina-
tion. Another simple idea would be to simply combine two terminating rewrite
systems by taking their union. But in general, as shown by Toyama [Toyama,
1986], the union of two terminating rewrite systems is not terminating, even if the
function symbols are disjoint. Many results about the modularity of the termina-
tion property have been studied in particular by Gramlich and can be found for
instance in [Gramlich, 1994].
3.3
Conﬂuence and Completion
By deﬁnition a function is expected to always return the same value when the
function is applied to the same arguments. When this function is computed with
rewrite rules, the property required for the rewrite system is therefore the unique-

Equational Logic and Rewriting
263
ness of the normal form for any term. This is crucial when computing normal forms
must be independent of the strategy of rule application. Uniqueness of the normal
form is implied by adding to termination another property called conﬂuence. A
rewrite system is conﬂuent when two rewrite sequences beginning from the same
term can always be extended to end with the same term. Although undecidable
in general, conﬂuence is decidable for terminating ﬁnite rewrite systems.
Conﬂuence (which may be deﬁned as an abstract property of relations, not
speciﬁcally of rewriting relations) is equivalent to the Church-Rosser property that
gives the relation between equational proofs and rewrite proofs: given a Church-
Rosser rewrite system R, every equational theorem t =R t′ has a rewrite proof
t ↓R t′, meaning that t and t′ both rewrite to a same term t′′, thus have the same
normal form if any. Assuming termination, conﬂuence is equivalent to local conﬂu-
ence [Newman, 1942], itself equivalent to the convergence of critical pairs [Knuth
and Bendix, 1970; Huet, 1980]. Critical pairs characterize minimal conﬂicts that
can happen when two rewrite rules apply to a same term and overlap each other.
Then the term can be rewritten in two diﬀerent ways that may or may not have
a common normal form.
A completion procedure is aimed at building a Church-Rosser and terminating
rewrite system from a set of equalities. Completion orients equalities into rewrite
rules, computes critical pairs, and keeps terms in normal form for the current set of
rewrite rules. In [Bachmair et al., 1986; Bachmair, 1991], completion is described
through inference rules with a fair strategy, which transform a set of equalities E
and a set of rewrite rules R.
Completion is initialized with a given set of equalities in E and an empty set
of rules R. It also requires a well-founded ordering on terms, used to determine
in which direction an equality must be oriented. A completion process has three
possible outcomes: either it terminates, or fails on an unorientable equality, or
diverges, that is, generates inﬁnitely many new rules.
When the completion ends up with an empty set of equalities in E and a rewrite
system in R that by construction is then locally conﬂuent and terminating, the
validity of an equational theorem (t = t′) w.r.t. the initial set of equalities is decid-
able by reducing both terms to their normal forms and by checking the syntactic
equality of the results, i.e. t ↓R t′.
The completion procedure fails when a non-orientable critical pair is generated.
In this case, nothing can be said on the set of rewrite rules generated so far,
except that they are equational consequences of the initial set of equalities in
E. Practical implementations of a completion procedure often postpone the non-
orientable equality, with the hope that a further generated rule will simplify it.
If the completion does not terminate and generates an inﬁnite set of rewrite
rules, it can be used as a semi-decision procedure for proving the validity of a
theorem: the theorem (t = t′) is valid iﬀthere exists a step i and thus a set of
rewrite rules Ri such that t ↓Ri t′.
In 1986, following Bachmair, Dershowitz and Hsiang works [Bachmair et al.,
1986], completion has been explained as a proof simpliﬁcation process, where each

264
Claude Kirchner and H´el`ene Kirchner
inference rule decreases the complexity of some equational proofs. The minimal
(i.e. less complex) proofs in this setting are the rewrite proofs, i.e. those of the form
t ↓R t′. Interestingly, the completion concept is indeed independent of rewriting as
shown much later by Dershowitz and Kirchner [Dershowitz and Kirchner, 2006].
3.4
Various extensions of Rewriting
Various extensions of the rewriting relation on terms exist. The two ﬁrst notions,
ordered rewriting and class rewriting, emerged from the problem of permutative
axioms like commutativity that can be applied indeﬁnitely at the same position.
The ﬁrst proposed solutions initiated by Lankford and Ballantyne [Lankford
and Ballantyne, 1977] followed by the seminal papers of Huet [Huet, 1980] and
of Peterson and Stickel [Peterson and Stickel, 1981] amounted to deﬁne a rewrite
relation on equivalence classes of terms and to simulate it by another rewrite
relation on terms that transforms a representative element of the equivalence
class. In some cases, the problematic non-orientable axioms can be built in the
matching process which becomes equational matching. This requires the elabo-
ration of a new abstract concept, namely the notion of rewriting modulo a set
of equalities in which the matching takes into account non-oriented equalities.
Adequate notions of conﬂuence and coherence modulo a set of equalities [Huet,
1980; Jouannaud, 1983] have been deﬁned for this kind of rewriting relation. A
class rewrite system is deﬁned by a set R of rewrite rules and a set A of equal-
ity axioms.
It is Church-Rosser modulo A if any equational theorem deduced
from R ∪A has a rewrite proof using rewriting in equivalence classes modulo
A, deﬁned as the compound relation =A ◦→R ◦=A.
The class rewrite sys-
tem is terminating modulo A if =A ◦→R ◦=A terminates. This general set-
ting introduced in full generality by Jouannaud and Kirchner [Jouannaud and
Kirchner, 1986] allows to deﬁne equational completion procedures generalizing
the standard case [Peterson and Stickel, 1981; Jouannaud and Kirchner, 1986;
Bachmair and Dershowitz, 1989], but they must be carefully set-up to minimize
sources of ineﬃciency.
Typical examples handled by such a method are theo-
ries with associativity and commutativity axioms [Peterson and Stickel, 1981;
Kirchner and Kirchner, 1986; Hullot, 1980].
More recently in deduction mod-
ulo has pioneered by Dowek, Hardin and Kirchner [Dowek et al., 2003], not only
terms but also propositions are identiﬁed modulo a congruence and this idea gave
rise to a new generation of higher-order proof assistants.
Another approach of the same problem is taken in the notion of ordered rewriting
which appeared in [Bachmair et al., 1989]. There, only orientable instances of
axioms can be used in the rewrite relation.
Conditional rewriting started from a quite diﬀerent motivation issued from ab-
stract data types and algebras with partial functions and exceptions, and this
algebraic point of view was the base of earlier approaches [Kaplan, 1984; Zhang
and R´emy, 1985; Bergstra and Klop, 1986]. In conditional rewriting, the rule ap-
plication requires the satisﬁability of its condition. However to be able to associate

Equational Logic and Rewriting
265
with a conditional rewrite system a decidable and terminating reduction relation,
it is necessary to provide a reduction ordering to compare terms involved in the
left and right-hand sides and in the condition of a conditional rewrite rule [Jouan-
naud and Waldmann, 1986; Kaplan, 1987; Dershowitz et al., 1987]. Then condi-
tional rewriting has been shown to provide a computational paradigm combining
logic and functional programming [Fribourg, 1985a; Dershowitz and Plaisted, 1988;
Goguen and Meseguer, 1986]. Only later the connection between conditional equal-
ities and equational Horn clauses has been exploited [Zhang and Kapur, 1988;
Bachmair and Ganzinger, 1994; Bachmair and Ganzinger, 2001].
Rewriting with constraints emerged in the 90’s [Kirchner et al., 1990; Kirchner
and Kirchner, 1989; Kirchner, 1995] as a uniﬁed way to cover the previous concepts
by looking at ordering and equations as symbolic constraints on terms.
Even
further, it provides a framework to incorporate disequations, built-in data types
and sort constraints [Comon, 1992].
As will be shown later on, all these rewrite relations can be studied as abstract
reduction systems. Note also that Church-Rosser properties have been adapted
and generalized for these diﬀerent rewrite relations.
4
REWRITING AND/FOR EQUALITY IN THEOREM PROVING
Reasoning about equality has been one of the most challenging problems in auto-
mated deduction. In the past ﬁfty years, a number of methods have been proposed.
In this section, we focus on the contribution of the term rewriting techniques to
ﬁrst-order theorem proving.
The term rewriting approach to theorem proving is unique in several aspects.
First, for equational theories enjoying a Church-Rosser property, rewriting pro-
vides a (semi-)decision procedure.
As such, a rewrite proof procedure can be
embedded in more general provers and combined with other proof methods. But
even without this property, a simpliﬁcation process with valid rewrite rules can
help to reduce the search space of the provers. Second, the analogy of the comple-
tion process with saturation methods in theorem proving and with Prolog theorem
provers has given rise to eﬃcient proof by consistency or by refutation procedures.
Third, it is one of the few methods which can be applied to a variety of prob-
lem domains. In addition to the validity problem of equational logic, it has also
been applied to inductive theorem proving, ﬁrst-order theorem proving, uniﬁcation
theory, geometry theorem proving.
4.1
Rewriting in theorem proving
From the operational point of view, term rewriting provides a forward chaining
method in automated deduction. In other words, its main inference mechanism de-
duces new lemmas from known theorems, rather than reduces the intended problem
into sub-problems and tries to solve them separately. Forward chaining methods
are usually not eﬃcient due to the large number of theorems produced which makes

266
Claude Kirchner and H´el`ene Kirchner
the search space unmanageable. As a tradeoﬀ, forward chaining methods usually
do not require backtracking, which is necessary in most backchaining methods.
In fact, term rewriting is one of the very few successful forward chaining meth-
ods. The problem of space explosion is handled in term rewriting through two
techniques: a notion of critical pairs, which tries to ﬁnd only “useful” lemmas,
and more importantly, a notion of simpliﬁcation. Basically, simpliﬁcation replaces
current terms or formulas by others which are logically equivalent but “smaller”
according to some well-founded ordering. Since the ordering is well-founded, sim-
pliﬁcation cannot go on indeﬁnitely.
It has been demonstrated through various implementations and experiments
that simpliﬁcation is indeed an eﬀective way of controlling the search space. In
addition to ﬁnding complete sets of rewrite rules, other notable problems have
been solved using term rewriting including the one-axiom group theory [Lescanne,
1983], the commutativity problem of rings [Stickel, 1984], Boolean algebra prob-
lems [Hsiang, 1985; Fribourg, 1985b], SAM’s lemma in lattice theory [Zhang and
Kapur, 1988], Moufang identities of alternative rings [Anantharaman and Hsiang,
1990]. In [Hsiang et al., 1992], a survey of the use of term rewriting in automated
theorem proving for various systems available in the 1990’s is given. Rewriting
techniques also contribute to one of the main successes of automated theorem
proving, namely the fully automated solution, obtained by McCune and his sys-
tem Otter in 1996, of the Robbins problem that had challenged mathematicians
for over sixty years [McCune, 1997].
In a diﬀerent community, interactive proof systems such as PVS [Owre et al.,
1992], Isabelle [Nipkow et al., 2002], or Coq [Delahaye, 2000] have also exten-
sively used rewriting techniques through tactics to handle the automated part of
reductions. indexordered (or unfailing) completion
4.2
Completion as a saturation process
The completion procedure must be supplied with a well-founded ordering used to
determine in which direction an equality must be oriented. Even when such an
ordering is provided, the procedure may fail to ﬁnd a conﬂuent and terminating
set of rules though one exists [Dershowitz et al., 1988]. An ordered (or unfailing)
completion procedure does not stop with a non-orientable equality and may ter-
minate with a non-empty set of equalities. This amounts to work with the notion
of ordered rewriting previously deﬁned and to adapt the Church-Rosser prop-
erty [Bachmair et al., 1989]. Ordered completion provides also a semi-decision
procedure for equational problems and so can be used as a refutationally complete
equational theorem prover. As such, it is possible to prove existentially quanti-
ﬁed theorems by negating them and deriving a contradiction by ordered comple-
tion [Hsiang and Rusinowitch, 1987]. This idea has been further explored later
on, in the context of automated theorem provers for ﬁrst-order clause logic with
equality, where the paramodulation inference rule was used to deal with equality.
Bachmair and Ganzinger present in [Bachmair and Ganzinger, 2001] the superpo-

Equational Logic and Rewriting
267
sition calculus that generalizes the previous idea to ﬁrst-order logic and provides a
reﬁnement of paramodulation. In the same vein, Nieuwenhuis and Rubio address
in [Nieuwenhuis and Rubio, 2001] equational theorem proving with constrained
formulas and explain the connection with paramodulation based theorem proving.
4.3
Rewriting and equality for inductive reasoning
Some proofs of properties on classical data structures, such as integers, require
induction. An inductive theorem is an equality that is true in the initial model of
the axioms. Three main approaches have been developed for mechanizing inductive
proofs, two of them using rewriting techniques.
Explicit induction.
The ﬁrst one, explicit induction, is used in proof assistants, for instance Nqthm-
ACL2 [Kaufmann and Moore, 1996], Coq [Bertot and Casteran, 2004], Isabelle [Nip-
kow et al., 2002] or Inka [Autexier et al., 1999]. Explicit induction uses structural
induction on data types and induction rules as for example Peano induction. These
forms of induction are in fact subsumed by the single general schema of Noethe-
rian induction, called noetherian induction principle. It is based on a well-founded
relation < on a set τ, so that there is no inﬁnite decreasing sequence of elements
in τ. The Noetherian induction principle states that if, for any element x ∈τ, a
proposition P holds for x whenever it holds for all elements x such that x < x,
then P holds for all x ∈τ. Mechanizing proof by induction [Bundy, 1999] is hard
due to the intrinsic diﬃculty of ﬁnding the most convenient inductive rule to show
a given conjecture. Indeed there may be a huge variety of possible noetherian
relations and choosing an appropriate induction rule introduces a ﬁrst branching
point in the search space. Furthermore, such proofs involve in general two tasks:
generalizing the induction formula and introducing an intermediate lemma. This
is why user interaction and expertise is needed to develop an inductive reasoning
in proof assistants.
Induction by consistency.
The second approach is based on the notion of consistency: a set of ﬁrst-order
axioms A, assumed here to be a set of equational clauses, is consistent if it has a
non-trivial model. Induction by consistency (or inductionless induction), roughly
works as follows: given a consistent set of axioms A and a set of conjectures E, we
add E to A and run a deduction engine until one gets a saturated and consistent
set of clauses (i.e. on which no more deduction rule applies). Historically, this de-
duction engine was given by the Knuth-Bendix completion procedure [Knuth and
Bendix, 1970] in the pioneer works of Musser [Musser, 1980], Goguen [Goguen,
1980], Lankford [Lankford, 1981], Huet and Hullot [Huet and Hullot, 1982]. So
it worked when A and E were both sets of equalities. The principle of a proof by
consistency is to assume the validity of the intended inductive theorem and show

268
Claude Kirchner and H´el`ene Kirchner
that there no contradiction or inconsistency is generated with the axioms. The
deﬁnition of a contradiction has also evolved in various ways: in 1980, in work
of Musser [Musser, 1980] and independently Goguen [Goguen, 1980], an incon-
sistency is the equality between boolean terms (true = false). This approach
assumes an axiomatization of booleans and an equality predicate eqs for each sort
of data s. This also requires that any expression eqs(t, t′) reduces either to true
or to false. In 1982, Huet and Hullot showed how the notion of constructors
(function symbols with which data are built) allows dropping the requirements
about the equality predicate [Huet and Hullot, 1982], provided there is no re-
lation between constructors. An inconsistency is then just an equality between
two diﬀerent terms built only with constructors. When there exists a conﬂuent
and terminating rewrite system for A, the method can take advantage of the ex-
istence of free constructors, characterized as a subset C of F such that T (C) is
exactly the set of normal forms of T (F). In this case, every non-constructor oper-
ator must be completely deﬁned [Kounalis, 1985; Dershowitz, 1985a; Thiel, 1984;
Lazrek et al., 1990], which means that any ground term containing this operator is
reducible. The latter property is decidable for a conﬂuent and terminating rewrite
system. In 1986, Jouannaud and Kounalis introduced the concept of ground re-
ducibility [Jouannaud and Kounalis, 1986] of a term, meaning that any instantia-
tion of its variables by ground terms is reducible. Their approach allows handling
relations between constructors. Ground reducibility is decidable for ﬁnite rewrite
systems [Kapur et al., 1987; Plaisted, 1985] and is EXPTIME-complete in the
general case [Comon and Jacquemard, 2003].
The completion procedure attempts to complete the initial system A ∪E by it-
eratively adding new equalities computed with the critical pairs mechanism. One
main problem is that such a completion often generates inﬁnitely many critical
pairs. Fribourg [Fribourg, 1986] ﬁrst observed that only overlaps of axioms on
conjectures are necessary: it is the so-called “linear strategy”. Moreover, when-
ever A can be turned into a ground convergent and suﬃciently complete rewrite
system, overlaping can be performed at speciﬁc positions. A further improvement
is to use ordered completion to avoid failure due to a non-orientable inductive
theorem like commutativity. The method, called in this case unfailing proof by
consistency [Bachmair, 1988; Bachmair, 1991] is refutationally complete: it refutes
any equality which is not an inductive theorem. For that, it detects any provably
inconsistent equality, i.e.
any equality (s = t) which satisﬁes either s > t (in
the reduction ordering >) and s is not inductively reducible, or (s = t) is not
inductively reducible (i.e. neither s nor t is inductively reducible). Inference rules
for proof by consistency are given in [Bachmair, 1988] and work with a conﬂuent
and terminating rewrite system R that describes the equational theory, a set C of
conjectures to be proved and a set L of inductive lemmas. (Inductive lemmas are
equational theorems (s = t) such that σ(s) =R σ(t) for any ground substitution
σ). The procedure adds in C new conjectures to be proved and obtained by com-
putation of critical pairs obtained by superposition of rules in R into conjectures in
C. Once a conjecture has been proved valid, it can be deleted from C, then added

Equational Logic and Rewriting
269
to L. It is also possible to introduce in L inductive lemmas given by the user or
produced by another system. Conjectures are simpliﬁed using either rules of R,
lemmas of L or other smaller conjectures of C. Finally, a refutation is produced
when a provably inconsistent conjecture is detected.
Comon and Nieuwenhuis [Comon and Nieuwenhuis, 2000] gave a more general
view of the deduction system, without restricting the conjectures to be equalities.
The interested reader may refer to [Comon, 2001, section 1.3] for all relevant
references on this approach.
Implicit induction by rewriting.
The third approach, implicit induction (or induction by rewriting), is used in au-
tomated theorem provers like Spike [Bouhoula et al., 1992] or RRL [Kapur and
Zhang, 1995]. The main idea of implicit induction is as follows: given a terminating
rewrite system, the corresponding rewrite relation is noetherian and can be used
for induction. Reddy [Reddy, 1990] provided a method to prove equalities in this
way. Proving an inductive conjecture amounts to proving some speciﬁc instances.
Each one is simpliﬁed by either a rule or a smaller instance of the conjecture until
an identity is obtained. The pragmatic advantage of his method is that it does not
need to explicitly check that the inductive hypothesis is applied to smaller terms.
In [Reddy, 1990], the relationship between this new method and the induction-
less induction procedures is clariﬁed. Bouhoula and Rusinowitch [Bouhoula and
Rusinowitch, 1995] designed a proof technique which works in conditional rewrite
systems and applies to non-Horn clauses as well: it is the so-called test set induc-
tion. It works by computing an appropriate implicit induction scheme called a test
set and then applies a refutation principle using proof by consistency techniques.
Their method is refutationaly complete and can refute false conjectures even in the
cases where the functions are not completely deﬁned. It has been implemented in
the prover Spike. The implicit induction techniques have been also used for induc-
tion modulo associativity and commutativity in [Berregeb et al., 1996] and [Aoto,
2006].
In [Deplagne et al., 2003; Kirchner et al., 2013], a bridge between explicit and
implicit induction is provided by a proof search mechanism for inductive proofs,
relying on the deduction modulo approach [Dowek et al., 2003]. A semi-decision
procedure, which is refutationaly correct and complete, comes along with a con-
structive proof of soundness which associates a proof in deduction modulo to every
successful proof of the procedure. This feature opens the door to the procedure’s
integration as a tactic in proof assistants that require some proof witness like Coq.
5
UNIVERSAL POWER OF REWRITING
Since the 80s, many aspects of rewriting have been studied in automated deduc-
tion, programming languages, equational theory decidability, program or proof

270
Claude Kirchner and H´el`ene Kirchner
transformation, but also in various domains such as chemical or biological com-
puting, plant growth modelling, security policies, etc.
Faced to this variety of
applications, the question arises to understand rewriting in a more abstract way,
especially as a logical framework able to encode diﬀerent logics and semantics. Dis-
covering the universal power of rewriting, in particular through its matching and
transformation power, led ﬁrst to the emergence of Rewriting Logic and Rewriting
Calculus.
On the other hand, with the development of rewrite frameworks and languages,
more and more reasoning systems have been modeled, for proof search, program
transformation, constraint solving, SAT solving.
It then appeared that deter-
ministic rule-based computations or deductions are often not suﬃcient to capture
complex computations or proof developments.
A formal mechanism is needed,
for instance, to sequentialize the search for diﬀerent solutions, to check context
conditions, to request user input to instantiate variables, to process subgoals in a
particular order, etc. This is the place where the notion of strategy comes in and
this led to the design and study of strategy languages and semantics.
More recent approaches consider that rules describe local transformations and
strategies describe the control of rule application. Most often, it is useful to dis-
tinguish between rules for computations, where a unique normal form is required
and where the strategy is ﬁxed, and rules for deductions, in which case no con-
ﬂuence nor termination is required but an application strategy is necessary. Due
to the strong correlation of rules and strategy in many applications, claiming the
universal character of rewriting also requires the formalisation of its control.
5.1
Matching and transformation power
A ﬁrst step is to note that in all its applications, rewriting deﬁnitions have the same
basic ingredients. Rewriting transforms syntactic structures that may be words,
terms, propositions, dags, graphs, geometric objects like segments, and in general
any kind of structured objects. Transformations are expressed with patterns or
rules. Rules are buit on the same syntax but with an additional set of variables,
say X, and with a binder ⇒, relating the left-hand side and the right-hand side
of the rule, and optionally with a condition or constraint that restricts the set
of values allowed for the variables. Performing the transformation of a syntactic
structure t is applying the rule labelled ℓon t, which is basically done in three
steps: (1) match to select a redex of t at position ω denoted t|ω (possibly modulo
some axioms, constraints,...); (2) instantiate the rule variables by the result(s)
of the matching substitution σ; (3) replace the redex by the instantiated right-
hand side. Formally: t rewrites to t′ using the rule ℓ: l ⇒r if t|ω = σ(l) and
t′ = t[σ(r)]ω. This is denoted, as in Section 3.1, t −→ω,ℓ,σ t′ or simply t →t′.
In this process, there are many possible choices: the rule itself, the position(s) in
the structure, the matching substitution(s). For instance, one may choose to apply
a rule concurrently at all disjoint positions where it matches, or using matching
modulo an equational theory like associativity-commutativity, or also according to

Equational Logic and Rewriting
271
some probability. These choices are most often implicit in systems or languages
based on rewriting, such as OBJ, ML, Haskell, Maude,... where they are built
in the interpreters or compilers. They can be instead considered as part of the
control of the rewriting process and explicitely addressed in a strategy deﬁnition
or language.
5.2
Rewriting logic
The Rewriting Logic is due to Meseguer [Meseguer, 1992; Mart´ı-Oliet and Meseguer,
2000]: Rewriting logic (RL) is a natural model of computation and an expressive
semantic framework for concurrency, parallelism, communication, and interac-
tion. It can be used for specifying a wide range of systems and languages in var-
ious application ﬁelds. It also has good properties as a metalogical framework for
representing logics. In recent years, several languages based on RL (ASF+SDF,
CafeOBJ, ELAN, Maude) have been designed and implemented.
1
In Rewriting Logic, the syntax is based on a set of terms T (F) built with an
alphabet F of function symbols with arities, a theory is given by a set R of labeled
rewrite rules denoted ℓ(x1, . . . , xn) : l ⇒r, where the label ℓ(x1, . . . , xn) records
the set of variables occurring in the rewrite rule. Formulas are sequents of the
form π : ⟨t⟩E →⟨t′⟩E, where π is a proof term recording the proof of the sequent:
R ⊢π : ⟨t⟩E →⟨t′⟩E if π : ⟨t⟩E →⟨t′⟩E can be obtained by ﬁnite application
of equational deduction rules [Meseguer, 1992] given in Fig. 2. In this context, a
proof term π encodes a sequence of rewriting steps called a derivation.
Reﬂexivity For any t ∈T (F):
t : ⟨t⟩E →⟨t⟩E
Transitivity
π1 : ⟨t1⟩E →⟨t2⟩E
π2 : ⟨t2⟩E →⟨t3⟩E
π1; π2 : ⟨t1⟩E →⟨t3⟩E
Congruence For any f ∈F with arity(f) = n:
π1 : ⟨t1⟩E →⟨t′
1⟩E
. . .
πn : ⟨tn⟩E →⟨t′
n⟩E
f(π1, . . . , πn) : ⟨f(t1, . . . , tn)⟩E →⟨f(t′
1, . . . , t′n)⟩E
Replacement For any ℓ(x1, . . . , xn) : l ⇒r ∈R,
π1 : ⟨t1⟩E →⟨t′
1⟩E
. . .
πn : ⟨tn⟩E →⟨t′
n⟩E
ℓ(π1, . . . , πn) : ⟨l(t1, . . . , tn)⟩E →⟨r(t′
1, . . . , t′n)⟩E
Figure 2. Deduction rules for Rewriting Logic
1http://wrla2012.lcc.uma.es/

272
Claude Kirchner and H´el`ene Kirchner
The ELAN language, designed in the early 90’s, introduced the concept of
strategy by giving explicit constructs for expressing control on the rule appli-
cation [Borovansk´y et al., 2002]. Beyond labeled rules and concatenation, other
constructs for deterministic or non-deterministic choice, failure, iteration, were
also deﬁned in ELAN. A strategy is there deﬁned as a set of proof terms in
rewriting logic and can be seen as a higher-order function : if the strategy ζ
is a set of proof terms π, applying ζ to the term t means ﬁnding all terms t′
such that π : ⟨t⟩E →⟨t′⟩E with π ∈ζ.
Since rewriting logic is reﬂective,
strategy semantics can be deﬁned inside the rewriting logic by rewrite rules at
the meta-level. This is the approach followed by Maude in [Clavel et al., 2007;
Mart´ı-Oliet et al., 2008].
5.3
Rewriting Calculus
The rewriting calculus, also called rho-calculus, has been introduced in 1998 by
Cirstea and Kirchner [Cirstea and Kirchner, 2001]: The rho-calculus has been
introduced as a general means to uniformly integrate rewriting and λ-calculus. This
calculus makes explicit and ﬁrst-class all of its components: matching (possibly
modulo given theories), abstraction, application and substitutions.
The rho-calculus is designed and used for logical and semantical purposes. It
could be used with powerful type systems and for expressing the semantics of rule
based as well as object oriented paradigms. It allows one to naturally express excep-
tions and imperative features as well as expressing elaborated rewriting strategies.
2
Some features of the rewriting calculus are worth emphasizing here: ﬁrst-order
terms and λ-terms are ρ-terms (λx.t is (x ⇒t)); a rule is a ρ-term as well as a
strategy, so rules and strategies are abstractions of the same nature and “ﬁrst-
class concepts”; application generalizes β−reduction; composition of strategies is
like function composition; recursion is expressed as in λ calculus with a recursion
operator µ.
5.4
Abstract reduction systems
Another view of rewriting is to consider it as an abstract relation on structural
objects. An Abstract Reduction System (ARS) [van Oostrom and de Vrijer, 2003;
Kirchner et al., 2008; Bourdier et al., 2009] is a labelled oriented graph (O, S)
with a set of labels L. The nodes in O are called objects. The oriented labelled
edges in S are called steps: a
φ−→b or (a, φ, b), with source a, target b and label φ.
Derivations are composition of steps.
The concept of Abstract Reduction System (ARS for short) has been introduced
to take into account abstract properties of relations induced by rewrite systems.
But thanks to the ﬂexibility due to the deﬁnition of objects and steps, ARS are
quite convenient to give a rewrite semantics to state transformation systems.
2http://rho.loria.fr/index.html

Equational Logic and Rewriting
273
Another interest is to give a semantics to strategies in this context. Reduc-
tion strategies in term rewriting study which expressions should be selected for
evaluation and which rules should be applied. These choices usually increase eﬃ-
ciency of evaluation but may aﬀect fundamental properties of computations such
as conﬂuence or (non-)termination.
Regarding rewriting as a relation and considering abstract rewrite systems leads
to consider derivation tree exploration: derivations are computations and strate-
gies describe selected computations.
5.5
Strategies
In the classical setting of ﬁrst-order term rewriting, strategies have been used
to determine at each step of a derivation which is the next redex.
Thus they
have often been deﬁned as functions on the set of terms like in [Terese, 2003].
Let us illustrate this point of view by a few examples of strategies that have
been primarily provided to describe the operational semantics of functional pro-
gramming languages and the related notions of call by value, call by name, call
by need.
Leftmost-innermost (resp.
outermost) reduction chooses the rewrit-
ing position according to suﬃx (resp.
preﬁx) ordering on the set of positions
in the term. Lazy reduction, as performed in Haskell for instance, or described
in [Fokkink et al., 2000], combines innermost and outermost strategies, in order
to avoid redundant rewrite steps. For that, operators of the signature have labels
specifying which arguments are lazy or eager. Positions in terms are then anno-
tated as lazy or eager, and the strategy consists in reducing the eager subterms
only when their reduction allows a reduction step higher in the term [Nguyen,
2001]. Lazy reduction can also be performed with other mechanisms, such as local
strategies or context-sensitive rewriting. Local strategies on operators, used for
instance in the OBJ-like languages [Goguen et al., 1987; Alpuente et al., 2004],
introduce a function to specify which and in which order the arguments of each
function symbol have to be reduced. In context-sensitive rewriting [Lucas, 2001a;
Lucas, 2001b], rewriting is allowed only at some speciﬁed positions in the terms.
Already in 1979, later published in [Huet and L´evy, 1991a; Huet and L´evy, 1991b],
Huet and L´evy deﬁned the notions of needed and strongly needed redexes for or-
thogonal rewrite systems. The main idea here is to ﬁnd the optimal way, when it
exists, to reach the normal form a term. A redex is needed when there is no way
to avoid reducing it to reach the normal form. Reducing only needed redexes is
clearly the optimal reduction strategy, as soon as needed redexes can be decided,
which is not the case in general.
Coming back to abstract reduction systems, abstract strategies are deﬁned
in [Kirchner et al., 2008] and in [Bourdier et al., 2009] as follows: for a given
ARS A, an abstract strategy ζ is a subset of the set of all derivations (ﬁnite or
not) of A. Playing with these deﬁnitions, [Bourdier et al., 2009] explored adequate
deﬁnitions of termination, normal form and conﬂuence under strategy.
This very general deﬁnition of abstract strategies is called extensional in [Bour-

274
Claude Kirchner and H´el`ene Kirchner
dier et al., 2009] in the sense that a strategy is deﬁned explicitly as a set of
derivations of an abstract reduction system. The concept is useful to understand
and unify reduction systems and deduction systems as explored in [Kirchner et al.,
2008].
But abstract strategies do not capture another point of view, also frequently
adopted in rewriting: a strategy is a partial function that associates to a reduction-
in-progress, the possible next steps in the reduction sequence. In this case, the
strategy as a function depends only on the object and the derivation so far. This
notion of strategy coincides with the deﬁnition of strategy in sequential path-
building games, with applications to planning, veriﬁcation and synthesis of con-
current systems. This remark leads to the following intensional deﬁnition given
in [Bourdier et al., 2009]. The essence of the idea is that strategies are considered
as a way of constraining and guiding the steps of a reduction. So at any step in
a derivation, it should be possible to say whether a contemplated next step obeys
the strategy ζ. In order to take into account the past derivation steps to decide the
next possible ones, the history of a derivation can be memorized to be available
at each step.
5.6
Strategy languages
In the 1990s, generalizing OBJ’s concept of local strategies, the idea has emerged
to better formalize the control on rewriting which was performed implicitly in
interpreters or compilers of rule-based programming languages. Then instead of
describing a strategy by a property of its derivations, the idea is to provide a strat-
egy language to specify which derivations we are interested in. Various approaches
have followed, yielding diﬀerent strategy languages such as Elan [Kirchner et al.,
1995; Borovansk´y et al., 2001], APS [Letichevsky, 1993], Stratego [Visser, 2001],
Tom [Balland et al., 2007; Balland et al., 2006] or more recently Maude [Mart´ı-
Oliet et al., 2005] and Porgy [Andrei et al., 2011]. All these languages share the
concern to provide abstract ways to express control of rule applications. In these
ﬂexible and expressive strategy languages, high-level strategies are deﬁned by com-
bining low-level primitives. The semantics of the Tom strategy language as well as
others are naturally described in the rewriting calculus [Cirstea and Kirchner, 2001;
Cirstea et al., 2003].
6
CONCLUSION
We have retraced in this overview the main evolutions of rewriting techniques in
relation with equational deduction mainly during a 40 years period going roughly
from 1970 to 2010. Useful complements can be found in RTA proceedings and
on the rewriting web page (rewriting.loria.fr/systems.html).
Strongly involved
in automated deduction before 2000, rewriting is now understood as an abstract
process of computation, well-adapted to model dynamic processes. Departing from
equational logic, this evolution is opening new ﬁelds of application.

Equational Logic and Rewriting
275
BIBLIOGRAPHY
[Alpuente et al., 2004] M. Alpuente, S. Escobar, and S. Lucas. Correct and complete (positive)
strategy annotations for OBJ. In Proceedings of the 5th International Workshop on Rewriting
Logic and its Applications (RTA), volume 71 of Elecronic Notes In Theoretical Computer
Science, pages 70–89, 2004.
[Anantharaman and Hsiang, 1990] S. Anantharaman and J. Hsiang. An automated proof of the
Moufang identities in alternative rings. Journal of Automated Reasoning, 6:79–109, 1990.
[Andrei et al., 2011] O. Andrei, M. Fernandez, H. Kirchner, G. Melan¸con, O. Namet, and
B. Pinaud.
PORGY: Strategy-Driven Interactive Transformation of Graphs.
In Rachid
Echahed, editor, TERMGRAPH, 6th Int. Workshop on Computing with Terms and Graphs,
volume 48, pages 54–68, 2011.
[Aoto, 2006] T. Aoto.
Dealing with Non-orientable Equations in Rewriting Induction.
In
F. Pfenning, editor, Proceedings of the 17th International Conference on Rewriting Tech-
niques and Applications, volume 4098, pages 242–256, Nara (Japan), apr 2006. Lecture Notes
in Computer Science.
[Arts and Giesl, 2000] T. Arts and J. Giesl. Termination of term rewriting using dependency
pairs. Theoretical Computer Science, 236:133–178, 2000.
[Astesiano et al., 2002] E. Astesiano, M. Bidoit, H. Kirchner, B. Krieg-Br¨uckner, P.D. Mosses,
D. Sannella, and A. Tarlecki. Casl: The Common Algebraic Speciﬁcation Language. Theo-
retical Computer Science, 286(2):153–196, 2002.
[Autexier et al., 1999] S. Autexier, D. Hutter, H. Mantel, and A. Schairer. System description:
Inka 5.0 – a logic voyager. In Harald Ganzinger, editor, Proceedings of the 16th International
Conference on Automated Deduction (CADE-16), volume 1632 of Lecture Notes in Artiﬁcial
Intelligence, pages 207–211, Trento, Italy, july 1999. Springer.
[Avenhaus and Madlener, 1990] J. Avenhaus and K. Madlener. Term rewriting and equational
reasoning. In R. B. Banerji, editor, Formal Techniques in Artiﬁal Intelligence, pages 1–43.
Elsevier Science Publishers B. V. (North-Holland), Amsterdam, 1990.
[Baader and Nipkow, 1998] F. Baader and T. Nipkow. Term rewriting and all that. Cambridge
University Press, 1998.
[Bachmair and Dershowitz, 1989] L. Bachmair and N. Dershowitz.
Completion for rewriting
modulo a congruence. Theoretical Computer Science, 67(2-3):173–202, October 1989.
[Bachmair and Ganzinger, 1994] L. Bachmair and H. Ganzinger. Rewrite-based equational the-
orem proving with selection and simpliﬁcation. Journal of Logic and Computation, 4(3):217–
247, 1994.
[Bachmair and Ganzinger, 2001] L. Bachmair and H. Ganzinger. Resolution theorem proving.
Handbook of automated reasoning, 1:19–99, 2001.
[Bachmair et al., 1986] L. Bachmair, N. Dershowitz, and J. Hsiang. Orderings for equational
proofs.
In Proceedings 1st IEEE Symposium on Logic in Computer Science, Cambridge
(Mass., USA), pages 346–357. IEEE, 1986.
[Bachmair et al., 1989] L. Bachmair, N. Dershowitz, and D. Plaisted. Completion without fail-
ure. In H. A¨ıt-Kaci and M. Nivat, editors, Resolution of Equations in Algebraic Structures,
Volume 2: Rewriting Techniques, pages 1–30. Academic Press inc., 1989.
[Bachmair, 1988] L. Bachmair. Proof by consistency in equational theories. In Proceedings 3rd
IEEE Symposium on Logic in Computer Science, Edinburgh (UK), pages 228–233, 1988.
[Bachmair, 1991] L. Bachmair. Canonical equational proofs. Computer Science Logic, Progress
in Theoretical Computer Science. Birkh¨auser Verlag AG, 1991.
[Balland et al., 2006] E. Balland, P. Brauner, R. Kopetz, P.-E. Moreau, and A. Reilles. Tom
Manual. LORIA, Nancy (France), version 2.4 edition, October 2006.
[Balland et al., 2007] E. Balland, P. Brauner, R. Kopetz, P.-E. Moreau, and A. Reilles. Tom:
Piggybacking rewriting on java. In Proceedings of the 18th Conference on Rewriting Tech-
niques and Applications, volume 4533 of Lecture Notes in Computer Science, pages 36–47.
Springer-Verlag, 2007.
[Bergstra and Klop, 1986] J.A. Bergstra and J.W. Klop. Conditional rewrite rules: Conﬂuency
and termination. Journal of Computer and System Sciences, 32(3):323–362, 1986.

276
Claude Kirchner and H´el`ene Kirchner
[Berregeb et al., 1996] N. Berregeb, A. Bouhoula, and M. Rusinowitch. Automated veriﬁcation
by induction with associative-commutative operators. In Rajeev Alur and Thomas A. Hen-
zinger, editors, CAV, volume 1102 of Lecture Notes in Computer Science, pages 220–231.
Springer, 1996.
[Bertot and Casteran, 2004] Y. Bertot and P. Casteran. Interactive Theorem Proving and Pro-
gram Development. Springer-Verlag, 2004.
[Bezem et al., 2003] M. Bezem, J.W. Klop, and R. de Vrijer. Term rewriting systems. Cam-
bridge University Press, 2003.
[Birkhoﬀ, 1935] G. Birkhoﬀ. On the structure of abstract algebras. Proceedings Cambridge Phil.
Soc., 31:433–454, 1935.
[Borovansk´y et al., 2001] P. Borovansk´y, C. Kirchner, H. Kirchner, and C. Ringeissen. Rewriting
with strategies in ELAN: a functional semantics. International Journal of Foundations of
Computer Science, 12(1):69–98, February 2001.
[Borovansk´y et al., 2002] P. Borovansk´y, C. Kirchner, H. Kirchner, and P.-E. Moreau. ELAN
from a rewriting logic point of view.
Theoretical Computer Science, 2(285):155–185, July
2002.
[Bouhoula and Rusinowitch, 1995] A. Bouhoula and M. Rusinowitch. Implicit induction in con-
ditional theories. Journal of Automated Reasoning, 14(2):189–235, 1995.
[Bouhoula et al., 1992] A. Bouhoula, E. Kounalis, and M. Rusinowitch. SPIKE: An automatic
theorem prover. In Proceedings of the 1st International Conference on Logic Programming
and Automated Reasoning, St. Petersburg (Russia), volume 624 of Lecture Notes in Artiﬁcial
Intelligence, pages 460–462. Springer-Verlag, July 1992.
[Bourdier et al., 2009] T. Bourdier, H. Cirstea, D.J. Dougherty, and H. Kirchner. Extensional
and intensional strategies. In Proceedings Ninth International Workshop on Reduction Strate-
gies in Rewriting and Programming, volume 15 of Electronic Proceedings In Theoretical Com-
puter Science, pages 1–19, 2009.
[Bundy, 1999] A. Bundy. The automation of proof by mathematical induction. In Alan Robinson
and Andrei Voronkov, editors, Handbook of automated reasonning. Elsevier Science Publishers
B. V. (North-Holland), 1999.
[Cirstea and Kirchner, 2001] H. Cirstea and C. Kirchner. The rewriting calculus — Part I and
II. Logic Journal of the Interest Group in Pure and Applied Logics, 9:427–498, May 2001.
[Cirstea et al., 2003] H. Cirstea, C. Kirchner, L. Liquori, and B. Wack. Rewrite strategies in
the rewriting calculus. In Bernhard Gramlich and Salvador Lucas, editors, Electronic Notes
in Theoretical Computer Science, volume 86. Elsevier, 2003.
[Clavel et al., 2007] M. Clavel, F. Dur´an, S. Eker, P. Lincoln, N. Mart´ı-Oliet, Meseguer J., and
C. Talcott.
All About Maude - A High-Performance Logical Framework, How to Specify,
Program and Verify Systems in Reriting Logic, volume 4350. Springer, 2007.
[Comon and Jacquemard, 2003] H. Comon and F. Jacquemard. Ground reducibility is exptime-
complete. Information and Computation, 187(1):123 – 153, 2003.
[Comon and Nieuwenhuis, 2000] H. Comon and R. Nieuwenhuis. Induction = I-Axiomatization
+ First-Order Consistency. Inf. Comput., 159(1-2):151–186, 2000.
[Comon, 1992] H. Comon.
Completion of rewrite systems with membership constraints.
In
W. Kuich, editor, Proceedings of ICALP 92, volume 623 of Lecture Notes in Computer Sci-
ence. Springer-Verlag, 1992.
[Comon, 2001] H. Comon. Inductionless induction. In A. Robinson and A. Voronkov, editors,
Handbook of Automated Reasoning, volume I, chapter 14, pages 914–959. Elsevier Science,
2001.
[Contejean et al., 2005] E. Contejean, C. March´e, A.P. Tom´as, and X. Urbain. Mechanically
proving termination using polynomial interpretations.
Journal of Automated Reasoning,
34(4):325–363, 2005.
[Dauchet, 1989] M. Dauchet. Simulation of Turing machines by a left-linear rewrite rule. In
N. Dershowitz, editor, Rewriting Techniques and Applications (RTA’03), volume 355 of Lec-
ture Notes in Computer Science, pages 109–120. Springer-Verlag, April 1989.
[Delahaye, 2000] D. Delahaye. A tactic language for the system Coq. In Michel Parigot and An-
drei Voronkov, editors, Proceedings of Logic Programming and Automated Reasoning (LPAR),
volume 1955 of Lecture Notes in Computer Science, pages 85–95. Springer, 2000.
[Deplagne et al., 2003] E. Deplagne, C. Kirchner, H. Kirchner, and Q.-H. Nguyen. Proof search
and proof check for equational and inductive theorems. In Franz Baader, editor, Proceedings
of CADE-19, Miami, Florida, July 2003. Springer-Verlag.

Equational Logic and Rewriting
277
[Dershowitz and Jouannaud, 1990] N. Dershowitz and J.-P. Jouannaud. Handbook of Theoreti-
cal Computer Science, volume B, chapter 6: Rewrite Systems, pages 244–320. Elsevier Science
Publishers B. V. (North-Holland), 1990.
[Dershowitz and Kirchner, 2006] N. Dershowitz and C. Kirchner. Abstract canonical presenta-
tions. Theoretical Computer Science, 357(1-3):53–69, July 2006.
[Dershowitz and Plaisted, 1988] N. Dershowitz and D. A. Plaisted. Equational programming.
In J. E. Hayes, D. Michie, and J. Richards, editors, Machine Intelligence 11: The logic and
acquisition of knowledge, chapter 2, pages 21–56. Oxford Press, Oxford, 1988.
[Dershowitz et al., 1987] N. Dershowitz, M. Okada, and G. Sivakumar. Conﬂuence of condi-
tional rewrite systems. In J.-P. Jouannaud and S. Kaplan, editors, Proceedings 1st Interna-
tional Workshop on Conditional Term Rewriting Systems, Orsay (France), volume 308 of
Lecture Notes in Computer Science, pages 31–44. Springer-Verlag, July 1987.
[Dershowitz et al., 1988] N. Dershowitz, L. Marcus, and A. Tarlecki. Existence, uniqueness and
construction of rewrite systems. SIAM Journal of Computing, 17(4):629–639, August 1988.
[Dershowitz, 1982] N. Dershowitz. Orderings for term rewriting systems. Theoretical Computer
Science, 17:279–301, 1982.
[Dershowitz, 1985a] N. Dershowitz. Computing with rewrite systems. Information and Control,
65(2/3):122–157, 1985.
[Dershowitz, 1985b] N. Dershowitz. Termination. In Rewriting Techniques and Applications,
volume 202 of Lecture Notes in Computer Science, pages 180–224, Dijon (France), May 1985.
Springer.
[Dershowitz, 1987] N. Dershowitz. Termination of rewriting. Journal of Symbolic Computation,
3(1 & 2):69–116, 1987.
[Dowek et al., 2003] G. Dowek, T. Hardin, and C. Kirchner. Theorem proving modulo. Journal
of Automated Reasoning, 31(1):33–72, Nov 2003.
[Evans, 1951] T. Evans.
On multiplicative systems deﬁned by generators and relations.
In
Proceedings of the Cambridge Philosophical Society, pages 637–649, 1951.
[Fokkink et al., 2000] W. Fokkink, J. Kamperman, and P. Walters.
Lazy rewriting on eager
machinery. ACM Transactions on Programming Languages and Systems, 22(1):45–86, 2000.
[Fribourg, 1985a] L. Fribourg. SLOG: A logic programming language intepreter based on clausal
superposition and rewriting. In Proceedings of the IEEE Symposium on Logic Programming,
pages 172–184, Boston, MA, July 1985.
[Fribourg, 1985b] L. Fribourg. A superposition oriented theorem prover. Theoretical Computer
Science, 35:129–164, 1985.
[Fribourg, 1986] L. Fribourg. A strong restriction of the inductive completion procedure. In Pro-
ceedings 13th International Colloquium on Automata, Languages and Programming, volume
226 of Lecture Notes in Computer Science, pages 105–115. Springer-Verlag, 1986.
[Futatsugi et al., 1985] K. Futatsugi, J. A. Goguen, J.-P. Jouannaud, and J. Meseguer. Princi-
ples of OBJ-2. In B. Reid, editor, Proceedings 12th ACM Symp. on Principles of Programming
Languages, pages 52–66. ACM, 1985.
[Giesl et al., 2006] J. Giesl, P. Schneider-Kamp, and R. Thiemann. AProVE 1.2: Automatic
termination proofs in the dependency pair framework. In Proceedings IJCAR ’06, volume
4130 of Lecture Notes in Artiﬁcial Intelligence, pages 281–286. Springer, 2006.
[Gnaedig and Kirchner, 2009] I. Gnaedig and H. Kirchner.
Termination of rewriting under
strategies. ACM Trans. of Comput. Logic, 10(2):1–52, 2009.
[Goguen and Malcolm, 2000] J.A. Goguen and G. Malcolm, editors. Software Engineering with
OBJ: Algebraic Speciﬁcation in Action, volume 2 of Advances in Formal Methods. Kluwer
Academic Publishers, Boston, 2000. ISBN 0-7923-7757-5.
[Goguen and Meseguer, 1986] J.A. Goguen and J. Meseguer.
EQLOG: Equality, types, and
generic modules for logic programming. In Douglas De Groot and Gary Lindstrom, editors,
Functional and Logic Programming, pages 295–363. Prentice Hall, Inc., 1986.
An earlier
version appears in Journal of Logic Programming, Volume 1, Number 2, pages 179–210,
September 1984.
[Goguen and Meseguer, 1987] J. A. Goguen and J. Meseguer. Order-sorted algebra solves the
constructor-selector, multiple representation and coercion problem. In Proceedings 2nd IEEE
Symposium on Logic in Computer Science, Ithaca (N.Y., USA), pages 18–29. IEEE Computer
Society Press, 1987.

278
Claude Kirchner and H´el`ene Kirchner
[Goguen and Tardo, 1979] J. A. Goguen and J. Tardo. An introduction to OBJ: A language
for writing and testing software speciﬁcations. In M. K. Zelkowitz, editor, Speciﬁcation of
Reliable Software, pages 170–189. IEEE Press, Cambridge (MA, USA), 1979. Reprinted in
Software Speciﬁcation Techniques, N. Gehani and A. McGettrick, editors, Addison-Wesley,
1985, pages 391-420.
[Goguen et al., 1985] J. A. Goguen, J.-P. Jouannaud, and J. Meseguer. Operational semantics
for order-sorted algebra. In W. Brauer, editor, Proceeding of the 12th International Collo-
quium on Automata, Languages and Programming, Nafplion (Greece), volume 194 of Lecture
Notes in Computer Science, pages 221–231. Springer-Verlag, 1985.
[Goguen et al., 1987] J. A. Goguen, C. Kirchner, H. Kirchner, A. M´egrelis, J. Meseguer, and
T. Winkler. An introduction to OBJ-3. In J.-P. Jouannaud and S. Kaplan, editors, Proceedings
1st International Workshop on Conditional Term Rewriting Systems, Orsay (France), volume
308 of Lecture Notes in Computer Science, pages 258–263. Springer-Verlag, July 1987.
[Goguen, 1980] J. A. Goguen. How to prove algebraic inductive hypotheses without induction,
with applications to the correctness of data type implementation. In W. Bibel and R. Kowal-
ski, editors, Proceedings 5th International Conference on Automated Deduction, Les Arcs
(France), volume 87 of Lecture Notes in Computer Science, pages 356–373. Springer-Verlag,
1980.
[Goguen, 1992] J.A. Goguen.
Order-Sorted Algebra I: Equational deduction for multiple in-
heritance, overloading, exceptions and partial operations.
Theoretical Computer Science,
105:217–273, 1992.
[Gramlich, 1994] B. Gramlich.
Generalized suﬃcient conditions for modular termination of
rewriting. Applicable Algebra in Engineering, Communication and Computing, 5(3-4):131–
158, May 1994.
[Hirokawa and Middeldorp, 2003] N. Hirokawa and A. Middeldorp. Tsukuba termination tool.
In Proc. 14th Rewriting Techniques and Applications, volume 2706 of Lecture Notes in Com-
puter Science, pages 311–320. Springer-Verlag, 2003.
[Hsiang and Rusinowitch, 1987] J. Hsiang and M. Rusinowitch. On word problem in equational
theories. In Th. Ottmann, editor, Proceedings of 14th International Colloquium on Automata,
Languages and Programming, Karlsruhe (Germany), volume 267 of Lecture Notes in Com-
puter Science, pages 54–71. Springer-Verlag, 1987.
[Hsiang et al., 1992] J. Hsiang, H. Kirchner, P. Lescanne, and M. Rusinowitch. The term rewrit-
ing approach to automated theorem proving. Journal of Logic Programming, 14(1&2):71–99,
October 1992.
[Hsiang, 1985] J. Hsiang. Refutational theorem proving using term-rewriting systems. Artiﬁcial
Intelligence, 25(3):255 – 300, 1985.
[Huet and Hullot, 1982] G. Huet and J.-M. Hullot. Proofs by induction in equational theories
with constructors. Journal of Computer and System Sciences, 25(2):239–266, October 1982.
Preliminary version in Proceedings 21st Symposium on Foundations of Computer Science,
IEEE, 1980.
[Huet and L´evy, 1991a] G. Huet and J.-J. L´evy. Computations in orthogonal rewriting systems,
I. In J.-L. Lassez and G. Plotkin, editors, Computational Logic, chapter 11, pages 395–414.
MIT press, 1991.
[Huet and L´evy, 1991b] G. Huet and J.-J. L´evy. Computations in orthogonal rewriting systems,
II. In J.-L. Lassez and G. Plotkin, editors, Computational Logic, chapter 12, pages 415–443.
MIT press, 1991.
[Huet and Oppen, 1980] G. Huet and D. Oppen. Equations and rewrite rules: A survey. In
R. V. Book, editor, Formal Language Theory: Perspectives and Open Problems, pages 349–
405. Academic Press inc., 1980.
[Huet, 1980] G. Huet.
Conﬂuent reductions:
Abstract properties and applications to term
rewriting systems. Journal of the ACM, 27(4):797–821, October 1980. Preliminary version in
18th Symposium on Foundations of Computer Science, IEEE, 1977.
[Hullot, 1980] J.-M. Hullot. Canonical forms and uniﬁcation. In W. Bibel and R. Kowalski, ed-
itors, Proceedings 5th International Conference on Automated Deduction, Les Arcs (France),
volume 87 of Lecture Notes in Computer Science, pages 318–334. Springer-Verlag, July 1980.
[Isakowitz and Gallier, 1988] T. Isakowitz and J. Gallier. Rewriting in order-sorted equational
logic. In R. Kowalski and K. Bowen, editors, Proceedings of Logic Programming Conference,
Seattle (USA), 1988. The MIT press.

Equational Logic and Rewriting
279
[Jouannaud and Kirchner, 1986] J.-P. Jouannaud and H. Kirchner. Completion of a set of rules
modulo a set of equations. SIAM Journal of Computing, 15(4):1155–1194, 1986.
[Jouannaud and Kounalis, 1986] J.-P. Jouannaud and E. Kounalis. Proof by induction in equa-
tional theories without constructors. In Proceedings 1st IEEE Symposium on Logic in Com-
puter Science, Cambridge (Mass., USA), pages 358–366, 1986.
[Jouannaud and Lescanne, 1982] J.-P. Jouannaud and P. Lescanne. On multiset orderings. In-
formation Processing Letters, 10:57–63, 1982.
[Jouannaud and Waldmann, 1986] J.-P. Jouannaud and B. Waldmann. Reductive conditional
term rewriting systems. In M. Wirsing, editor, Proceedings of the Third IFIP Working Confer-
ence on Formal Description of Programming Concepts, Ebberup, (Denmark), 1986. Elsevier
Science Publishers B. V. (North-Holland).
[Jouannaud et al., 1992] J.-P. Jouannaud, C. Kirchner, H. Kirchner, and A. M´egrelis. Program-
ming with equalities, subsorts, overloading and parameterization in OBJ. Journal of Logic
Programming, 12(3):257–280, February 1992.
[Jouannaud, 1983] J.-P. Jouannaud. Conﬂuent and coherent equational term rewriting systems.
Applications to proofs in abstract data types. In G. Ausiello and M. Protasi, editors, Proceed-
ings of the 8th Colloquium on Trees in Algebra and Programming, L’Aquila (Italy), volume
159 of Lecture Notes in Computer Science, pages 269–283. Springer-Verlag, 1983.
[Kaplan, 1984] S. Kaplan. Conditional rewrite rules. Theoretical Computer Science, 33:175–193,
1984.
[Kaplan, 1987] S. Kaplan. Simplifying conditional term rewriting systems: Uniﬁcation, termi-
nation and conﬂuence. Journal of Symbolic Computation, 4(3):295–334, December 1987.
[Kapur and Zhang, 1995] D. Kapur and H. Zhang.
An overview of rewrite rule laboratory
(RRL). J. Computer and Mathematics with Applications, 29(2):91–114, 1995.
[Kapur et al., 1987] D. Kapur, P. Narendran, and H. Zhang. On suﬃcient completeness and
related properties of term rewriting systems. Acta Informatica, 24:395–415, 1987.
[Kaufmann and Moore, 1996] M. Kaufmann and J.S. Moore.
ACL2: An industrial strength
version of nqthm.
In Compass’96: Eleventh Annual Conference on Computer Assurance,
page 23, Gaithersburg, Maryland, 1996. National Institute of Standards and Technology.
[Kirchner and Kirchner, 1986] C. Kirchner and H. Kirchner.
Reveur-3: Implementation of a
general completion procedure parametrized by built-in theories and strategies.
Science of
Computer Programming, 20(8):69–86, 1986.
[Kirchner and Kirchner, 1989] C. Kirchner and H. Kirchner.
Constrained equational reason-
ing. In Proceedings of the ACM-SIGSAM 1989 International Symposium on Symbolic and
Algebraic Computation, Portland (Oregon), pages 382–389. ACM Press, July 1989.
[Kirchner et al., 1988] C. Kirchner, H. Kirchner, and J. Meseguer.
Operational semantics of
OBJ-3. In Proceedings of 15th International Colloquium on Automata, Languages and Pro-
gramming, volume 317 of Lecture Notes in Computer Science, pages 287–301. Springer-Verlag,
1988.
[Kirchner et al., 1990] C. Kirchner, H. Kirchner, and M. Rusinowitch. Deduction with symbolic
constraints. Revue d’Intelligence Artiﬁcielle, 4(3):9–52, 1990. Special issue on Automatic
Deduction.
[Kirchner et al., 1995] C. Kirchner, H. Kirchner, and M. Vittek.
Designing constraint logic
programming languages using computational systems. In P. Van Hentenryck and V. Saraswat,
editors, Principles and Practice of Constraint Programming. The Newport Papers, chapter 8,
pages 131–158. The MIT press, 1995.
[Kirchner et al., 2000] C. Kirchner, H. Kirchner, and A. M´egrelis. OBJ for OBJ. In J.A. Goguen
and G. Malcolm, editors, Software Engineering with OBJ: Algebraic Speciﬁcation in Action,
chapter 6, pages 307–330. Kluwer, Boston, 2000.
[Kirchner et al., 2008] C. Kirchner, F. Kirchner, and H. Kirchner. Strategic computations and
deductions. In Festchrift in honor of Peter Andrews, Studies in Logic and the Foundations
of Mathematics. Elsevier, 2008.
[Kirchner et al., 2013] C. Kirchner, H. Kirchner, and F. Nahon.
Narrowing based inductive
proof search. In A. Voronkov and C. Weidenbach, editors, Programming Logics, volume 7797
of Lecture Notes in Computer Science, pages 216–238. Springer, 2013.
[Kirchner, 1995] H. Kirchner. On the use of constraints in automated deduction. In A. Podel-
ski, editor, Constraint Programming: Basics and Trends, volume 910 of Lecture Notes in
Computer Science, pages 128–146. Springer-Verlag, 1995.

280
Claude Kirchner and H´el`ene Kirchner
[Klop, 1990] J. W. Klop.
Term Rewriting Systems.
In S. Abramsky, D. Gabbay, and
T. Maibaum, editors, Handbook of Logic in Computer Science, volume 1, chapter 6. Oxford
University Press, 1990.
[Knuth and Bendix, 1970] D.E. Knuth and P. B. Bendix. Simple word problems in universal
algebras. In J. Leech, editor, Computational Problems in Abstract Algebra, pages 263–297.
Pergamon Press, Oxford, 1970.
[Kounalis, 1985] E. Kounalis.
Completeness in data type speciﬁcations.
In B. Buchberger,
editor, Proceedings EUROCAL Conference, Linz (Austria), volume 204 of Lecture Notes in
Computer Science, pages 348–362. Springer-Verlag, 1985.
[Lankford and Ballantyne, 1977] D. S. Lankford and A. Ballantyne.
Decision procedures for
simple equational theories with associative commutative axioms: complete sets of associative
commutative reductions. Technical report, Univ. of Texas at Austin, Dept. of Mathematics
and Computer Science, 1977.
[Lankford, 1981] D. S. Lankford. A simple explanation of inductionless induction. Louisiana
Tech. University, Dep. of Math., Memo MTP-14, Ruston, 1981.
[Lazrek et al., 1990] A. Lazrek, P. Lescanne, and J.-J. Thiel. Tools for proving inductive equal-
ities, relative completeness and ω-completeness. Information and Computation, 84(1):47–70,
January 1990.
[Lescanne, 1983] P. Lescanne. Computer experiments with the REVE term rewriting systems
generator. In Proceedings of 10th ACM Symposium on Principles of Programming Languages,
pages 99–108. ACM, 1983.
[Lescanne, 1990] P. Lescanne.
On the recursive decomposition ordering with lexicographical
status and other related orderings. Journal of Automated Reasoning, 6:39–49, 1990.
[Letichevsky, 1993] A.A. Letichevsky.
Development of rewriting strategies.
In Maurice
Bruynooghe and Jaan Penjam, editors, PLILP, volume 714 of Lecture Notes in Computer
Science, pages 378–390. Springer, 1993.
[Lucas, 2001a] S. Lucas. Termination of on-demand rewriting and termination of OBJ programs.
In H. Sondergaard, editor, Proceedings of the 3rd International ACM SIGPLAN Conference
on Principles and Practice of Declarative Programming, PPDP’01, pages 82–93, Firenze,
Italy, September 2001. ACM Press, New York.
[Lucas, 2001b] S. Lucas. Termination of rewriting with strategy annotations. In A. Voronkov
and R. Nieuwenhuis, editors, Proceedings of the 8th International Conference on Logic for
Programming, Artiﬁcial Intelligence and Reasoning, LPAR’01, volume 2250 of Lecture Notes
in Artiﬁcial Intelligence, pages 669–684, La Habana, Cuba, December 2001. Springer-Verlag,
Berlin.
[Mart´ı-Oliet and Meseguer, 2000] N. Mart´ı-Oliet and J. Meseguer. Rewriting logic as a logical
and semantic framework. In J. Meseguer, editor, Electronic Notes in Theoretical Computer
Science, volume 4. Elsevier Science Publishers, 2000.
[Mart´ı-Oliet et al., 2005] N. Mart´ı-Oliet, J. Meseguer, and A. Verdejo.
Towards a strategy
language for Maude.
In N. Mart´ı-Oliet, editor, Proceedings Fifth International Workshop
on Rewriting Logic and its Applications, WRLA 2004, Barcelona, Spain, March 27 – April
4, 2004, volume 117 of Electronic Notes in Theoretical Computer Science, pages 417–441.
Elsevier, 2005.
[Mart´ı-Oliet et al., 2008] N. Mart´ı-Oliet, J. Meseguer, and A. Verdejo. A rewriting semantics
for Maude strategies. EPTCS, 238(3):227–247, 2008.
[McCune, 1997] W. McCune. Solution of the robbins problem. Journal of Automated Reasoning,
19:263–276, 1997.
[Meseguer and Goguen, 1985] J. Meseguer and J. A. Goguen.
Initiality, induction and com-
putability. In M. Nivat and J. C. Reynolds, editors, Algebraic Methods in Semantics. Cam-
bridge University Press, 1985.
[Meseguer, 1992] J. Meseguer. Conditional rewriting logic as a uniﬁed model of concurrency.
Theoretical Computer Science, 96(1):73–155, 1992.
[Middeldorp and Zantema, 1997] A. Middeldorp and H. Zantema. Simple termination of rewrite
systems. Theoretical Computer Science, 175(1):127–158, 1997.
[Mosses, 1997] P.D. Mosses. CoFI: The Common Framework Initiative for Algebraic Speciﬁca-
tion and Development. In TAPSOFT ’97: Theory and Practice of Software Development,
volume 1214 of Lecture Notes in Computer Science, pages 115–137. Springer-Verlag, 1997.

Equational Logic and Rewriting
281
[Musser, 1980] D. R. Musser. On proving inductive properties of abstract data types. In Pro-
ceedings 7th ACM Symp. on Principles of Programming Languages, pages 154–162. ACM,
1980.
[Newman, 1942] M. H. A. Newman. On theories with a combinatorial deﬁnition of equivalence.
In Annals of Math, volume 43, pages 223–243, 1942.
[Nguyen, 2001] Q.-H. Nguyen. Compact normalisation trace via lazy rewriting. In S. Lucas and
B. Gramlich, editors, Proceedings of the 1st International Workshop on Reduction Strategies
in Rewriting and Programming (WRS 2001), volume 57 of Elecronic Notes In Theoretical
Computer Science. Elsevier, 2001.
[Nieuwenhuis and Rubio, 2001] R. Nieuwenhuis and A. Rubio. Paramodulation-based theorem
proving. Handbook of automated reasoning, 1:371–443, 2001.
[Nipkow et al., 2002] T. Nipkow, L.C. Paulson, and M. Wenzel. Isabelle/HOL — A Proof As-
sistant for Higher-Order Logic, volume 2283 of Lecture Notes in Computer Science. Springer-
Verlag, 2002.
[Ohlebusch, 2002] E. Ohlebusch. Advanced Topics in Term Rewriting. Springer, 2002. ISBN
978-1-4757-3661-8.
[Owre et al., 1992] S. Owre, J.M. Rushby, and N. Shankar. PVS: A prototype veriﬁcation sys-
tem.
In Deepak Kapur, editor, 11th International Conference on Automated Deduction
(CADE), volume 607 of Lecture Notes in Artiﬁcial Intelligence, pages 748–752, Saratoga,
NY, jun 1992. Springer-Verlag.
[Peterson and Stickel, 1981] G. Peterson and M.E. Stickel. Complete sets of reductions for some
equational theories. Journal of the ACM, 28:233–264, 1981.
[Plaisted, 1985] D. Plaisted.
Semantic conﬂuence and completion method.
Information and
Control, 65:182–215, 1985.
[Plaisted, 1993] D. Plaisted.
Equational reasoning and term rewriting systems.
In D. Gab-
bay, C. Hogger, J. A. Robinson, and J. Siekmann, editors, Handbook of Logic in Artiﬁcial
Intelligence and Logic Programming, volume 1, pages 273–364. Oxford University Press, 1993.
[Reddy, 1990] U. Reddy. Term rewriting induction. In M. E. Stickel, editor, Proceedings 10th
International Conference on Automated Deduction, Kaiserslautern (Germany), volume 449
of Lecture Notes in Computer Science, pages 162–177. Springer-Verlag, 1990.
[Steinbach, 1994] J. Steinbach. Generating polynomial orderings. Information Processing Let-
ters, 49:85–93, 1994.
[Steinbach, 1995] J. Steinbach. Simpliﬁcation orderings: History of results. Fundam. Inform.,
24(1/2):47–87, 1995.
[Stickel, 1984] M. E. Stickel. A case study of theorem proving by the Knuth-Bendix method:
Discovering that x3 = x implies ring commutativity. In R. Shostak, editor, Proceedings 7th
International Conference on Automated Deduction, Napa Valley (Calif., USA), volume 170
of Lecture Notes in Computer Science, pages 248–258. Springer-Verlag, 1984.
[Terese, 2003] Terese. Term Rewriting Systems. Cambridge University Press, 2003. M. Bezem,
J. W. Klop and R. de Vrijer, eds.
[Thiel, 1984] J.-J. Thiel. Stop losing sleep over incomplete data type speciﬁcations. In Proceeding
11th ACM Symp. on Principles of Programming Languages, pages 76–82. ACM, 1984.
[Toyama, 1986] Y. Toyama. Counterexamples to termination for the direct sum of term rewriting
systems. Information Processing Letters, 25(3):141–143, May 1986.
[van Oostrom and de Vrijer, 2003] V. van Oostrom and R. de Vrijer. Strategies, volume 2, chap-
ter 9. Cambridge University Press, 2003.
[Visser, 2001] E. Visser. Stratego: A language for program transformation based on rewriting
strategies. System description of Stratego 0.5. In A. Middeldorp, editor, Rewriting Techniques
and Applications (RTA’01), volume 2051 of Lecture Notes in Computer Science, pages 357–
361. Springer-Verlag, May 2001.
[Zantema, 1994] H. Zantema. Termination of term rewriting: interpretation and type elimina-
tion. Journal of Symbolic Computation, 17(1):23–50, 1994.
[Zantema, 1995] H. Zantema. Termination of term rewriting by semantic labelling. Fundamenta
Informaticae, 24(1):89–105, 1995.
[Zantema, 2005] H. Zantema. Termination of string rewriting proved automatically. Journal of
Automated Reasoning, 34(2):105–139, 2005.

282
Claude Kirchner and H´el`ene Kirchner
[Zhang and Kapur, 1988] H. Zhang and D. Kapur. First-order theorem proving using condi-
tional rewrite rules.
In E. Lusk and R. Overbeek, editors, 9th International Conference
on Automated Deduction, volume 310 of Lecture Notes in Computer Science, pages 1–20.
Springer Berlin Heidelberg, 1988.
[Zhang and R´emy, 1985] H. T. Zhang and J.-L. R´emy. Contextual rewriting. In J.-P. Jouan-
naud, editor, Proceedings 1st Conference on Rewriting Techniques and Applications, Dijon
(France), volume 202 of Lecture Notes in Computer Science, pages 46–62, Dijon (France),
1985. Springer-Verlag.

DIDIER DUBOIS AND HENRI PRADE
POSSIBILISTIC LOGIC — AN OVERVIEW
Reader: Lluis Godo
1
INTRODUCTION
Uncertainty often pervades information and knowledge. For this reason,
the handling of uncertainty in inference systems has been an issue for a
long time in artiﬁcial intelligence (AI). Indeed rather early in the history of
AI, in the early 1970’s, the second expert system to be designed, MYCIN
[Buchanan and Shortliﬀe, 1984], was the occasion of proposing an original
setting for the representation of uncertainty in terms of degree of belief,
degree of disbelief, and certainty factor, with empirical rules for combining
them [Shortliﬀe, 1976].
Since that time, diﬀerent new proposals have been developed for repre-
senting uncertainty including imprecise probabilities [Walley, 1991], belief
function-based evidence theory [Shafer, 1976; Yager and Liu, 2008], possi-
bility theory [Zadeh, 1978; Dubois and Prade, 1988], while Bayesian proba-
bilities [Pearl, 1988; Jensen, 2001] have become prominent at the forefront
of AI methods, challenging the original supremacy of logical representation
settings [Minker, 2000].
The need for classical methods, well-mastered at the algorithmic level
thanks to many implementation-oriented developments, has contributed to
the present success of probabilistic representations. However, meanwhile,
there has been a constant interest for methods providing tools oriented
towards the representation of imprecise epistemic states of knowledge per-
vaded with uncertainty (these states may range between complete infor-
mation and complete ignorance). These methods are characterized by the
existence of a pair of dual measures for assessing uncertainty, leaving the
freedom of neither believing p nor ¬p in case of ignorance about p, as per-
mitted in modal logic as well as in the MYCIN approach, while this is
forbidden by the auto-duality of probabilities (Prob(p) = 1 −Prob(¬p)).
Possibility theory has a remarkable situation among the settings devoted
to the representation of imprecise and uncertain information. First, pos-
sibility theory may be numerical or qualitative [Dubois and Prade, 1998].
In the ﬁrst case, possibility measures and the dual necessity measures can
be regarded respectively as the upper bounds and the lower bounds of ill-
known probabilities; they are also particular cases of plausibility and belief
Handbook of the History of Logic. Volume 9: Computational Logic. 
Volume editor: Jörg Siekmann 
Series editors: Dov M. Gabbay and John Woods 
Copyright © 2014 Elsevier BV. All rights reserved.

284
DIDIER DUBOIS AND HENRI PRADE
functions respectively [Shafer, 1976; Dubois and Prade, 1988]. In fact, possi-
bility measures and necessity measures constitute the simplest, non trivial,
imprecise probabilities system [Walley, 1996].
Second, when qualitative,
possibility theory provides a natural approach to the grading of possibility
and necessity modalities on ﬁnite ordinal scales. Besides, possibility theory
has a logical counterpart, namely possibilistic logic [Dubois et al., 1994c],
which remains close to classical logic. In this overview chapter, we focus
our attention on possibilistic logic.
Possibilistic logic is a weighted logic that handles uncertainty (but it also
models preferences), in a qualitative way by associating certainty, or pri-
ority levels, to classical logic formulas. Moreover, possibilistic logic copes
with inconsistency by taking advantage of the stratiﬁcation of the set of
formulas induced by the associated levels.
These are the basic features
of standard possibilistic logic. However, there exist further extensions of
possibilistic logic that this paper also reports about. Since its introduc-
tion in the mid-eighties, multiple facets of possibilistic logic have been laid
bare and various applications addressed, such as the handling of exceptions
in default reasoning, the modeling of belief revision, the development of a
graphical Bayesian-like network counterpart to a possibilistic logic base, or
the representation of positive and negative information in a bipolar setting.
The paper aims primarily at oﬀering an introductory survey of possibilistic
logic developments, but also outlines new research trends that are relevant
in preference representation, or in reasoning about epistemic states. The
intended purpose of this paper is to lay bare the basic ideas and the main in-
ference methods. For more technical details and more examples, the reader
is referred to the rich bibliography that is provided.
The chapter is structured as follows. We ﬁrst provide a background on
possibility theory. Then, the syntax and the semantics of basic possibilistic
logic, where classical logic formulas are associated with lower bounds of ne-
cessity measures, are presented. Moreover, three other related formalisms,
possibilistic networks, extensions of possibilistic logic for handling inconsis-
tency, and possibilistic logic with symbolic weights are also surveyed. The
next section proposes an overview of diﬀerent applications of basic possi-
bilistic logic to default reasoning, to causality ascription, to belief revision,
to information fusion, to decision under uncertainty, to the handling of un-
certainty in databases, and more brieﬂy to description logic and to logic
programming. The last section deals respectively i) with a multiple agent
extension of possibilistic logic, ii) with bipolar representations that involve
measures other than possibility and necessity and have applications in pref-
erence modeling, and ﬁnally iii) with generalized possibilistic logic, a two-

POSSIBILISTIC LOGIC — AN OVERVIEW
285
tiered logic having a powerful representation power for modeling uncertain
epistemic states, which can capture answer set programming.
2
QUALITATIVE POSSIBILITY THEORY - A REFRESHER
Possibility theory is an uncertainty theory devoted to the handling of in-
complete information. To a large extent, it is comparable to probability
theory because it is based on set-functions. It diﬀers from the latter by
the use of a pair of dual set functions (possibility and necessity measures)
instead of only one. Besides, it is not additive and makes sense on ordinal
structures. Before providing the basics of possibility theory, we start with
a brief historical account of the idea of possibility.
2.1
Historical background
Zadeh [1978] was not the ﬁrst scientist to speak about formalizing notions
of possibility.
The modalities possible and necessary have been used in
philosophy at least since the Middle-Ages in Europe, based on Aristotle’s
and Theophrastus’ works [Boche´nski, 1947]. More recently these notions
became the building blocks of modal logics that emerged at the beginning of
the XXth century from the works of C. I. Lewis (see [Chellas, 1980]). In this
approach, possibility and necessity are all-or-nothing notions, and handled
at the syntactic level. More recently, and independently from Zadeh’s view,
the notion of possibility, as opposed to probability, was central in the works
of one economist, and in those of two philosophers.
Indeed a graded notion of possibility was introduced as a full-ﬂedged
approach to uncertainty and decision in the 1940-1970’s by the English
economist G. L. S. Shackle [1949; 1961], who called degree of potential sur-
prise of an event its degree of impossibility, that is, retrospectively, the
degree of necessity of the opposite event. Shackle’s notion of possibility is
basically epistemic, it is a “character of the chooser’s particular state of
knowledge in his present.” Impossibility is understood as disbelief [Shackle,
1979]. Potential surprise is valued on a disbelief scale, namely a positive
interval of the form [0, y∗], where y∗denotes the absolute rejection of the
event to which it is assigned. In case everything is possible, all mutually
exclusive hypotheses have zero surprise. At least one elementary hypothesis
must carry zero potential surprise. The degree of surprise of an event, a set
of elementary hypotheses, is the degree of surprise of its least surprising real-
ization. Shackle also introduces a notion of conditional possibility, whereby
the degree of surprise of a conjunction of two events A and B is equal to

286
DIDIER DUBOIS AND HENRI PRADE
the maximum of the degree of surprise of A, and of the degree of surprise
of B, should A prove true. The disbelief notion introduced later by Spohn
[1988; 2012] employs the same type of convention as potential surprise, but
uses the set of natural integers as a disbelief scale; his conditioning rule uses
the subtraction of natural integers.
In his 1973 book [Lewis, 1973] the philosopher David Lewis considers
a graded notion of possibility in the form of a relation between possible
worlds he calls comparative possibility. A comparative possibility relation
is modeled by a well-ordered partition (in terms of plausibility) of possible
worlds, so-called “system of spheres”. He connects this concept of possibility
to a notion of similarity between possible worlds. This asymmetric notion
of similarity is also comparative, and is meant to express statements of the
form: a world j is at least as similar to world i as world k is. Comparative
similarity of j and k with respect to i is interpreted as the comparative
possibility of j with respect to k viewed from world i. Such relations are
assumed to be complete pre-orderings and are instrumental in deﬁning the
truth conditions of counterfactual statements (of the form “If I were rich, I
would buy a big boat”). Comparative possibility relations ≥Π obey the key
axiom: for all events A, B, C,
A ≥Π B implies C ∪A ≥Π C ∪B.
This axiom was later independently proposed by the ﬁrst author [Dubois,
1986] in an attempt to derive a possibilistic counterpart to comparative
probabilities. See also Grove [1988] who uses a “system of spheres”, anal-
ogous to an ordinal possibility distribution as already said, to describe a
belief revision process in the sense of G¨ardenfors [G¨ardenfors, 1988].
A framework very similar to the one of Shackle was also proposed by
the philosopher L. J. Cohen [1977] who considered the problem of legal
reasoning.
He introduced so-called Baconian probabilities understood as
degrees of provability (or degrees of “inductive support”). The idea is that
it is hard to prove someone guilty at the court of law by means of pure
statistical arguments. The basic feature of degrees of provability is that a
hypothesis and its negation cannot both be provable together to any extent
(the contrary being a case for inconsistency). Such degrees of provability
coincide with what is known as necessity measures. I. Levi [1966; 1967],
starting from Shackle’s measures of surprise viewed as “measures contribut-
ing to the explication of what Keynes called ’weight of argument’ ” [Levi,
1979], also wrote a property identical to the minimum-based composition
of necessity measures for conjunction, for so-called “degrees of conﬁdence
of acceptance”.

POSSIBILISTIC LOGIC — AN OVERVIEW
287
Independently from the above works, Zadeh [1978] proposed an interpre-
tation of membership functions of fuzzy sets as possibility distributions en-
coding ﬂexible constraints induced by natural language statements. Zadeh
tentatively articulated the relationship between possibility and probability,
noticing that what is probable must preliminarily be possible. However, the
view of possibility degrees developed in his paper refers to the idea of graded
feasibility (degrees of ease, as in the example of “how many eggs can Hans
eat for his breakfast”) rather than to the epistemic notion of plausibility
laid bare by Shackle. Nevertheless, the key axiom of “maxitivity” for pos-
sibility measures is highlighted. In two subsequent articles [Zadeh, 1979a;
Zadeh, 1982], the author acknowledged the connection between possibility
theory, belief functions and upper/lower probabilities, and proposed their
extensions to fuzzy events and fuzzy information granules.
2.2
Basic notions of possibility theory.
The basic building blocks of possibility theory originate in Zadeh’s paper
[1978] and have been more extensively described and investigated in books
by Dubois and Prade [1980; 1988]. See also [Dubois and Prade, 1998] for
an introduction. Zadeh starts from the idea of a possibility distribution, to
which he associates a possibility measure.
Possibility distributions
Let U be a set of states of aﬀairs (or descrip-
tions thereof), or states for short. This set can be the domain of an attribute
(numerical or categorical), the Cartesian product of attribute domains, the
set of interpretations of a propositional language, etc. A possibility distri-
bution is a mapping π from U to a totally ordered scale S, with top denoted
by 1 and bottom by 0. In the ﬁnite case S = {1 = λ1 > . . . λn > λn+1 = 0}.
The possibility scale can be the unit interval as suggested by Zadeh, or
generally any ﬁnite chain, or even the set of non-negative integers1. For a
detailed discussion of diﬀerent types of scales in a possibility theory per-
spective, the reader is referred to [Benferhat et al., 2010].
The function π represents the state of knowledge of an agent (about
the actual state of aﬀairs), also called an epistemic state distinguishing
what is plausible from what is less plausible, what is the normal course
of things from what is not, what is surprising from what is expected. It
represents a ﬂexible restriction on what is the actual state of facts with
the following conventions (similar to probability, but opposite to Shackle’s
potential surprise scale which refers to impossibility):
1If S = N, the conventions are opposite: 0 means possible and ∞means impossible.

288
DIDIER DUBOIS AND HENRI PRADE
• π(u) = 0 means that state u is rejected as impossible;
• π(u) = 1 means that state u is totally possible (= plausible).
The larger π(u), the more possible, i.e., plausible the state u is. Formally,
the mapping π is the membership function of a fuzzy set [Zadeh, 1978],
where membership grades are interpreted in terms of plausibility. If the
universe U is exhaustive, at least one of the elements in S should be the
actual world, so that ∃u, π(u) = 1 (normalization). This condition expresses
the consistency of the epistemic state described by π.
Distinct values may simultaneously have a degree of possibility equal to
1. Moreover, as Shackle wrote, as early as 1949: “An outcome that we
looked on as perfectly possible before is not rendered less possible by the
fact that we have extended the list of perfectly possible outcomes” (see p.
114 in [Shackle, 1949]).
In the {0, 1}-valued case, π is just the characteristic function of a subset
E ⊆U of mutually exclusive states, ruling out all those states outside
E considered as impossible. Possibility theory is thus a (fuzzy) set-based
representation of incomplete information.
Speciﬁcity
A possibility distribution π is said to be at least as speciﬁc
as another π′ if and only if for each state of aﬀairs u: π(u) ≤π′(u) [Yager,
1983].
Then, π is at least as restrictive and informative as π′, since it
rules out at least as many states with at least as much strength. In the
possibilistic framework, extreme forms of partial knowledge can be captured,
namely:
• Complete knowledge: for some u0, π(u0) = 1 and π(u) = 0, ∀u ̸= u0
(only u0 is possible);
• Complete ignorance: π(u) = 1, ∀u ∈U (all states are possible).
Possibility theory is driven by the principle of minimal speciﬁcity. It states
that any hypothesis not known to be impossible cannot be ruled out. It is a
minimal commitment, cautious, information principle. Basically, we must
always try to maximize possibility degrees, taking constraints into account.
Given a piece of information in the form x is F where F is a fuzzy set
restricting the values of the ill-known quantity x, it leads to represent the
knowledge by the inequality π ≤µF , the membership function of F. The
minimal speciﬁcity principle enforces possibility distribution π = µF , if no
other piece of knowledge is available. Generally there may be impossible
values of x due to other piece(s) of information. Thus given several pieces

POSSIBILISTIC LOGIC — AN OVERVIEW
289
of knowledge of the form x is Fi, for i = 1, . . . , n, each of them translates
into the constraint π ≤µFi; hence, several constraints lead to the inequality
π ≤minn
i=1 µFi and on behalf of the minimal speciﬁcity principle, to the
possibility distribution
π =
n
min
i=1 πi
where πi is induced by the information item x is Fi (πi = µFi). It justiﬁes
the use of the minimum operation for combining information items. It is
noticeable that this way of combining pieces of information fully agrees with
classical logic, since a classical logic knowledge base (i.e. a set of formulas)
is equivalent to the logical conjunction of the logical formulas that belong
to the base, and its models is obtained by intersecting the sets of models of
its formulas. Indeed, in propositional logic, asserting a logical proposition
a amounts to declaring that any interpretation (state) that makes a false is
impossible, as being incompatible with the state of knowledge.
Possibility and necessity functions
Given a simple query of the form
“does event A occur?” (is the corresponding proposition a true?), where A
is a subset of states, the set of models of a, the response to the query can
be obtained by computing the degrees of possibility of A [Zadeh, 1978] and
of its complement Ac:
Π(A) = sup
u∈A
π(u) ;
Π(Ac) = sup
s/∈A
π(u).
Π(A) evaluates to what extent A is consistent with π, while Π(Ac) can be
easily related to the idea of certainty of A. Indeed, the less Π(Ac), the more
Ac is impossible and the more certain is A. If the possibility scale S is
equipped with an order-reversing map denoted by λ ∈S 7→ν(λ), it enables
a degree of necessity (certainty) of A to be deﬁned in the form N(A) =
ν(Π(Ac)), which expresses the well-known duality between possibility and
necessity. N(A) evaluates to what extent A is certainly implied by π. If
S is the unit interval then, it is usual to choose ν(λ) = 1 −λ, so that
N(A) = 1 −Π(Ac) [Dubois and Prade, 1980]. Generally, Π(U) = N(U) = 1
and Π(∅) = N(∅) = 0 (since π is normalized to 1). In the {0, 1}-valued
case, the possibility distribution comes down to the disjunctive (epistemic)
set E ⊆U, and possibility and necessity are then as follows:
• Π(A) = 1 if A ∩E ̸= ∅, and 0 otherwise: function Π checks whether
A is logically consistent with the available information or not.
• N(A) = 1 if E ⊆A, and 0 otherwise: function N checks whether A is
logically entailed by the available information or not.

290
DIDIER DUBOIS AND HENRI PRADE
Possibility measures satisfy the characteristic “maxitivity” property
Π(A ∪B) = max(Π(A), Π(B)).
Necessity measures satisfy an axiom dual to that of possibility measures:
N(A ∩B) = min(N(A), N(B)).
On inﬁnite spaces, these axioms must hold for inﬁnite families of sets.
As a consequence, of the normalization of π, min(N(A), N(Ac)) = 0 and
max(Π(A), Π(Ac)) = 1, where Ac is the complement of A, or equivalently
Π(A) = 1 whenever N(A) > 0, which totally ﬁts the intuition behind this
formalism, namely that something somewhat certain should be ﬁrst fully
possible, i.e. consistent with the available information. Moreover, one can-
not be somewhat certain of both A and Ac, without being inconsistent.
Note also that we only have N(A ∪B) ≥max(N(A), N(B)). This goes well
with the idea that one may be certain about the event A∪B, without being
really certain about more speciﬁc events such as A and B.
Certainty qualiﬁcation
Human knowledge is often expressed in a declar-
ative way using statements to which belief degrees are attached. Certainty-
qualiﬁed pieces of information of the form “A is certain to degree α” can
be modeled by the constraint N(A) ≥α. It represents a family of possible
epistemic states π that obey this constraint. The least speciﬁc possibility
distribution among them exists and is given by [Dubois and Prade, 1988]:
π(A,α)(u) =

1
if u ∈A
1 −α
otherwise.
If α = 1 we get the characteristic function of A. If α = 0, we get total
ignorance. This possibility distribution is a key building-block to construct
possibility distributions from several pieces of uncertain knowledge. It is
instrumental in possibilistic logic semantics. Indeed, e.g. in the ﬁnite case,
any possibility distribution can be viewed as a collection of nested certainty-
qualiﬁed statements. Let Ei = {u|π(u) ≥λi ∈L} be the λi-cut of π. Then
it can be check that π(u) = mini:s̸∈Ei 1−N(Ei) (with convention min∅= 1).
We can also consider possibility-qualiﬁed statements of the form Π(A) ≥
β; however, the least speciﬁc epistemic state compatible with this constraint
is trivial and expresses total ignorance.
Two other measures
A measure of guaranteed possibility or strong pos-
sibility can be deﬁned, that diﬀers from the functions Π (weak possibility)

POSSIBILISTIC LOGIC — AN OVERVIEW
291
and N (strong necessity) [Dubois and Prade, 1992; Dubois et al., 2000]:
∆(A) = inf
u∈A π(u).
It estimates to what extent all states in A are actually possible according
to evidence. ∆(A) can be used as a degree of evidential support for A. Of
course, this function possesses a dual conjugate ∇such that ∇(A) = 1 −
∆(Ac) = supu̸∈A 1 −π(u). Function ∇(A) evaluates the degree of potential
or weak necessity of A, as it is 1 only if some state s out of A is impossible.
It follows that the functions ∆and ∇are decreasing with respect to set
inclusion, and that they satisfy the characteristic properties ∆(A ∪B) =
min(∆(A), ∆(B)) and ∇(A ∩B) = max(∇(A), ∇(B)) respectively.
Uncertain statements of the form “A is possible to degree β” often mean
that any realization of A are possible to degree β (e.g. “it is possible that
the museum is open this afternoon”).
They can then be modeled by a
constraint of the form ∆(A) ≥β. It corresponds to the idea of observed
evidence.
This type of information is better exploited by assuming an informational
principle opposite to the one of minimal speciﬁcity, namely, any situation
not yet observed is tentatively considered as potentially impossible. This is
similar to the closed-world assumption. The most speciﬁc distribution δ(A,β)
in agreement with ∆(A) ≥β is:
π[A,β](u) =
 β
if u ∈A
0
otherwise.
Note that while possibility distributions induced from certainty-qualiﬁed
pieces of knowledge combine conjunctively, by discarding possible states,
evidential support distributions induced by possibility-qualiﬁed pieces of
evidence combine disjunctively, by accumulating possible states. Given sev-
eral pieces of knowledge of the form x is Fi is possible (in the sense of
guaranteed or strong possibility), for i = 1, . . . , n, each of them translates
into the constraint π ≥µFi; hence, several constraints lead to the inequality
π ≥maxn
i=1 µFi and on behalf of a closed-world assumption-like principle
based on maximal speciﬁcity, expressed by the possibility distribution
π =
n
max
i=1 πi
where πi represents the information item x is Fi is possible. This principle
justiﬁes the use of the maximum for combining evidential support functions.
Acquiring pieces of possibility-qualiﬁed evidence leads to updating π[A,β]
into some wider distribution π > π[A,β]. Any possibility distribution can be

292
DIDIER DUBOIS AND HENRI PRADE
represented as a collection of nested possibility-qualiﬁed statements of the
form (Ei, ∆(Ei)), with Ei = {u|π(u) ≥λi}, since π(u) = maxi:u∈Ei ∆(Ei),
dually to the case of certainty-qualiﬁed statements.
The possibilistic cube of opposition
Interestingly enough, it has been
shown [Dubois and Prade, 2012] that the four set function evaluations of
an event A and the four evaluations of its opposite Ac can be organized in
a cube of opposition (see Figure 1), whose front and back facets are graded
extension of the traditional square of opposition [Parsons, 1997].
Counterparts of the characteristic properties of the square of opposition
do hold. First, the diagonals (in dotted lines) of these facets link dual mea-
sures through the involutive order-reversing function 1 −(·). The vertical
edges of the cube, as well as the diagonals of the side facets, which are
bottom-oriented arrows, correspond to entailments here expressed by in-
equalities. Indeed, provided that π and 1 −π are both normalized, we have
for all A,
max(N(A), ∆(A)) ≤min(Π(A), ∇(A)).
The thick black lines of the top facets express mutual exclusiveness under
the form min(N(A), N(Ac)) = min(∆(A), ∆(Ac)) = min(N(A), ∆(Ac)) =
min(∆(A), N(Ac)) = 0. Dually, the double lines of the bottom facet corre-
spond to max(Π(A), Π(Ac)) = max(∇(A), ∇(Ac)) = max(Π(A), ∇(Ac)) =
max(∇(A), Π(Ac)) = 1. Thus, the cube in Figure 1 summarizes the inter-
play between the diﬀerent measures in possibility theory.
i: ∇(A)
I: Π(A)
O: Π(Ac)
o: ∇(Ac)
a: ∆(A)
A: N(A)
E: N(Ac)
e: ∆(Ac)
Figure 1. The cube of opposition of possibility theory

POSSIBILISTIC LOGIC — AN OVERVIEW
293
3
NECESSITY-BASED POSSIBILISTIC LOGIC
AND RELATED ISSUES
Possibilistic logic has been developed for about thirty years; see [Dubois
and Prade, 2004] for historical details. Basic possibilistic logic (also called
standard possibilistic logic) has been ﬁrst introduced in artiﬁcial intelligence
as a tool for handling uncertainty in a qualitative way in a logical setting.
Later on, it has appeared that basic possibilistic logic can also be used for
representing preferences [Lang, 1991]. Then, each logic formula represents
a goal to be reached with its priority level (rather than a statement that
is believed to be true with some certainty level). Possibilistic logic heavily
relies on the notion of necessity measure, but may be also related [Dubois
et al., 1991a] to Zadeh’s theory of approximate reasoning [Zadeh, 1979b].
A basic possibilistic logic formula is a pair (a, α) made of a classical
logic formula a associated with a certainty level α ∈(0, 1], viewed as a
lower bound of a necessity measure, i.e., (a, α) is understood as N(a) ≥α.
Formulas of the form (a, 0), which do not contain any information (N(a) ≥0
always holds), are not part of the possibilistic language. As already said, the
interval [0,1] can be replaced by any linearly ordered scale. Since necessity
measures N are monotonic functions w.r.t. entailment, i.e. if a |= b then
N(a) ≤N(b). The min decomposability property of necessity measures for
conjunction, i.e., N(a ∧b) = min(N(a), N(b)), expresses that to be certain
about a∧b, one has to be certain about a and to be certain about b. Thanks
to this decomposability property, a possibilistic logic base can be always put
in a clausal equivalent form.
An interesting feature of possibilistic logic is its ability to deal with in-
consistency. Indeed a possibilistic logic base Γ, i.e. a set of possibilistic
logic formulas, viewed as a conjunction thereof, is associated with an in-
consistency level inc-l(Γ), which is such that the formulas associated with a
level strictly greater than inc-l(Γ) form a consistent subset of formulas. A
possibilistic logic base is semantically equivalent to a possibility distribution
that restricts the set of interpretations (w. r. t. the considered language)
that are more or less compatible with the base.
Instead of an ordinary
subset of models as in classical logic, we have a fuzzy set of models, since
the violation by an interpretation of a formula that is not fully certain (or
imperative) does not completely rule out the interpretation.
The certainty-qualiﬁed statements of possibilistic logic have a clear modal
ﬂavor. Possibilistic logic can also be viewed as a special case of a labelled
deductive system [Gabbay, 1996].
Inference in possibilistic logic propa-
gates certainty in a qualitative manner, using the law of the weakest link,

294
DIDIER DUBOIS AND HENRI PRADE
and is inconsistency-tolerant, as it enables non-trivial reasoning to be per-
formed from the largest consistent subset of most certain formulas. A char-
acteristic feature of this uncertainty theory is that a set of propositions
{a ∈L : N(a) ≥α}, in a propositional language L, that are believed at least
to a certain extent is deductively closed (thanks to the min-decomposability
of necessity measures with respect to conjunction). As a consequence, pos-
sibilistic logic remains very close to classical logic.
From now on, we shall use letters such as a, b, c for denoting propositional
formulas, and letters such as p, q, r will denote atomic formulas.
This section is organized in four main subparts. First, the syntactic and
semantic aspects of basic possibilistic logic are presented. Then we brieﬂy
survey extended inference machineries that take into account formulas that
are “drowned” under the inconsistency level, as well as an extension of possi-
bilistic logic that handles certainty levels in a symbolic manner, which allows
for a partially known ordering of these levels. Lastly, we review another no-
ticeable, Bayesian-like, representation framework, namely possibilistic net-
works. They are associated to a possibility distribution decomposed thanks
to conditional independence information, and provide a graphical counter-
part to possibilistic logic bases to which they are semantically equivalent.
3.1
Basic possibilistic logic
Basic possibilistic logic [Dubois et al., 1994c] has been mainly developed as
a formalism for handling qualitative uncertainty (or preferences) with an
inference mechanism that is a simple extension of classical logic. A possi-
bilistic logic formula is a pair made of i) any well-formed classical logic for-
mula, propositional or ﬁrst-ordered, and ii) a weight expressing its certainty
or priority. Its classical logic component can be only true or false: fuzzy
statements with intermediary degrees of truth are not allowed in standard
possibilistic logic (although extensions exist for handling fuzzy predicates
[Dubois et al., 1998b; Alsinet and Godo., 2000; Alsinet et al., 2002]).
Syntactic aspects
In the following, we only consider the case of (basic)
possibilistic propositional logic, ΠL for short, i.e., possibilistic logic formulas
(a, α) are such that a is a formula in a propositional language; for (basic)
possibilistic ﬁrst order logic, the reader is referred to [Dubois et al., 1994c].
Axioms and inference rules. The axioms of ΠL are those of propo-
sitional logic, PL for short, where each axiom schema is now supposed to
hold with the maximal certainty, i.e. is associated with level 1 [Dubois et
al., 1994c]. It has two inference rules:

POSSIBILISTIC LOGIC — AN OVERVIEW
295
• if β ≤α then (a, α) ⊢(a, β) (certainty weakening)
• (¬a ∨b, α), (a, α) ⊢(b, α), ∀α ∈(0, 1] (modus ponens)
We may equivalently use the certainty weakening rule with the ΠL counter-
part of the resolution rule:
(¬a ∨b, α), (a ∨c, α) ⊢(b ∨c, α), ∀α ∈(0, 1] (resolution)
Using certainty weakening, it is then easy to see that the following inference
rule is valid
(¬a ∨b, α), (a ∨c, β) ⊢(b ∨c, min(α, β)) (weakest link resolution)
The idea that in a reasoning chain, the certainty level of the conclusion is
the smallest of the certainty levels of the formulas involved in the premises
is at the basis of the syntactic approach proposed by [Rescher, 1976] for
plausible reasoning, and would date back to Theophrastus, an Aristotle’s
follower.
The following inference rule we call formula weakening holds also as a
consequence of α-β-resolution.
if a ⊢b then (a, α) ⊢(b, α), ∀α ∈(0, 1] (formula weakening)
Indeed a ⊢b expresses that ¬a ∨b is valid in PL and thus (¬a ∨b, 1) holds,
which by applying the α-β-resolution rule with (a, α) yields the result.
It turns out that any valid deduction in propositional logic is valid in
possibilistic logic as well where the corresponding propositions are associ-
ated with any level α ∈(0, 1]. Thus since a, b ⊢a∧b, we have (a, α), (b, α) ⊢
(a∧b, α). Note that we also have (a∧b, α) ⊢(a, α) and (a∧b, α) ⊢(b, α) by
the formula weakening rule. Thus, stating (a ∧b, α) is equivalent to stating
(a, α) and (b, α). Thanks to this property, it is always possible to rewrite a
ΠL base under the form of a collection of weighted clauses.
Note also that if we assume that for any propositional tautology t, i.e.,
such that t ≡⊤, (t, α) holds with any certainty level, which amounts to
saying that each PL axiom holds with any certainty level, then the α-β-
resolution rule entails the level weakening rule, since (¬a ∨a, β) together
with (a ∨c, α) entails (a ∨c, β) with β ≤α.
Inference and consistency. Let Γ = {(ai, αi), i = 1, ..., m} be a set
of possibilistic formulas.
Inference in ΠL from a base Γ is quite similar
to the one in PL. We may either use the ΠL axioms, certainty weakening
and modus ponens rules, or equivalently proceed by refutation (proving

296
DIDIER DUBOIS AND HENRI PRADE
Γ ⊢(a, α) amounts to proving Γ, (¬a, 1) ⊢(⊥, α) by repeated application
of the weakest link-resolution rule, where Γ stands for a collection of ΠL
formulas (a1, α1), ..., (am, αm). Moreover, note that
Γ ⊢(a, α) if and only if Γα ⊢(a, α) if and only if (Γα)∗⊢a
where Γα = {(ai, αi) ∈Γ, αi ≥α} and Γ∗= {ai | (ai, αi) ∈Γ}.
The
certainty levels stratify the knowledge base Γ into nested level cuts Γα, i.e.
Γα ⊆Γβ if β ≤α. A consequence (a, α) from Γ can only be obtained from
formulas having a certainty level at least equal to α, so from formulas in
Γα; then a is a classical consequence from the PL knowledge base (Γα)∗,
and α = max{β|(Γβ)∗⊢a}.
The inconsistency level of Γ is deﬁned by
inc-l(Γ) = max{α | Γ ⊢(⊥, α)}.
The possibilistic formulas in Γ whose level is strictly above inc-l(Γ) are safe
from inconsistency, namely inc-l({(ai, αi) | (ai, αi) ∈Γ and αi > inc-l(Γ)}) =
0. Indeed, if α > inc-l(Γ), (Γα)∗is consistent. In particular, we have the
remarkable property that the classical consistency of Γ∗is equivalent to
saying that Γ has a level of inconsistency equal to 0. Namely,
inc-l(Γ) = 0 if and only if Γ∗is consistent.
Semantic aspects
The semantics of ΠL [Dubois et al., 1994c] is ex-
pressed in terms of possibility distributions, (weak) possibility measures
and (strong) necessity measures. Let us ﬁrst consider a ΠL formula (a, α)
that encodes the statement N(a) ≥α. Its semantics is given by the follow-
ing possibility distribution π(a,α) deﬁned, in agreement with the formula of
the certainty qualiﬁcation in Section 2.2, by:
π(a,α)(ω) = 1 if ω ⊨a and π(a,α)(ω) = 1 −α if ω ⊨¬a
Intuitively, the underlying idea is that any model of a should be fully pos-
sible, and that any interpretation that is a counter-model of a, is all the
less possible as a is more certain, i.e. as α is higher. When α = 0, the
(trivial) information N(a) ≥0 is represented by π(a,0) = 1, and the for-
mula (a, 0) can be ignored. It can be easily checked that the associated
necessity measure is such that N(a,α)(a) = α, and π(a,α) is the least in-
formative possibility distribution (i.e. maximizing possibility degrees) such
that this constraint holds. In fact, any possibility distribution π such that
∀ω, π(ω) ≤π(a,α)(ω) is such that its associated necessity measure N satisﬁes
N(a) ≥N(a,α)(a) = α (hence is more committed).

POSSIBILISTIC LOGIC — AN OVERVIEW
297
Due to the min-decomposability of necessity measures, N(V
i ai) ≥α ⇔
∀i, N(ai) ≥α, and then any possibilistic propositional formula can be put
in clausal form. Let us now consider a ΠL knowledge base Γ = {(ai, αi), i =
1, ..., m}, thus corresponding to the conjunction of ΠL formulas (ai, αi), each
representing a constraint N(ai) ≥αi. The base Γ is semantically associated
with the possibility distribution:
πΓ(ω) =
min
i=1,...,m π(ai,αi)(ω) =
min
i=1,...,m max([ai](ω), 1 −αi)
where [ai] is the characteristic function of the models of ai, namely [ai](ω) =
1 if ω ⊨ai and [ai](ω) = 0 otherwise. Thus, the least informative induced
possibility distribution πΓ is obtained as the min-based conjunction of the
fuzzy sets of interpretations (with membership functions π(ai,αi)), repre-
senting each formula. It can be checked that
NΓ(ai) ≥αi for i=1,. . . ,m,
where NΓ is the necessity measure deﬁned from πΓ.
Note that we may
only have an inequality here since Γ may, for instance, include two formulas
associated to equivalent propositions, but with distinct certainty levels.
Remark 1. Let us mention that a similar construction can be made in an
additive setting where each formula is associated with a cost (in N∪{+∞}),
the weight (cost) attached to an interpretation being the sum of the costs
of the formulas in the base violated by the interpretation, as in penalty
logic [Dupin de Saint Cyr et al., 1994; Pinkas, 1991]. The so-called “cost of
consistency” of a formula is then deﬁned as the minimum of the weights of
its models (which is nothing but a ranking function in the sense of Spohn
[1988], or the counterpart of a possibility measure deﬁned on N ∪{+∞}
where now 0 expresses full possibility, and +∞complete impossibility since
it is a cost that cannot be paid).
So a ΠL knowledge base is understood as a set of constraints N(ai) ≥αi
for i = 1, . . . , m, and the set of possibility distributions π associated with N
that are compatible with this set of constraints has a largest element which
is nothing but πΓ, i.e. we have ∀ω, π(ω) ≤mini=1,...,m π(ai,αi) = πΓ(ω).
Thus, the possibility distribution πΓ representing semantically a ΠL base
Γ, is the one assigning the largest possibility degree to each interpretation,
in agreement with the semantic constraints N(ai) ≥αi for i = 1, . . . , m
that are associated with the formulas (ai, αi) in Γ. Thus, any possibility
distribution π ≤πΓ semantically agrees with Γ, which can be written π ⊨Γ.
The semantic entailment is deﬁned by
Γ ⊨(a, α) if and only if ∀ω, πΓ(ω) ≤π{(a,α)}(ω).

298
DIDIER DUBOIS AND HENRI PRADE
We also have Γ ⊨(a, α) if and only if Nπ(a) ≥α, ∀π ≤πΓ, where Nπ is the
necessity measure associated with π. It can be shown [Dubois et al., 1994c]
that possibilistic logic is sound and complete w.r.t. this semantics, namely
Γ ⊢(a, α) if and only if Γ ⊨(a, α).
Moreover, we have inc-l(Γ) = 1 −maxω∈ΩπΓ(ω).
This acknowledges the fact that the normalization of πΓ is equivalent to
the classical consistency of Γ∗. Thus, an important feature of possibilis-
tic logic is its ability to deal with inconsistency. The consistency of Γ is
estimated by the extent to which there is at least one completely possi-
ble interpretation for Γ, i.e. by the quantity cons-l(Γ) = 1 −inc-l(Γ) =
maxω∈ΩπΓ(ω) = maxπ|=Γ maxω∈Ωπ(ω) (where π |= Γ iﬀπ ≤πΓ).
EXAMPLE 1. Let us illustrate the previously introduced notions on the
following ΠL base Γ, which is in clausal form (p, q, r are atoms): {(¬p ∨
q, 0.8), (¬p ∨r, 0.9), (¬p ∨¬r, 0.1), (p, 0.3), (q, 0.7), (¬q, 0.2), (r, 0.8)}.
First, it can be checked that inc-l(Γ) = 0.2.
Thus, the sub-base Γ0.3 ={(¬p∨q, 0.8),(¬p∨r, 0.9),(p, 0.3),(q, 0.7),(r, 0.8)}
is safe from inconsistency, and its deductive closure is consistent, i.e. ∄a,
∄α > 0, ∄β > 0 such that Γ0.3 ⊢(a, α) and Γ0.3 ⊢(¬a, β). By contrast,
Γ0.1 ⊢(¬r, 0.1) and Γ0.1 ⊢(r, 0.8). Note also that, while (¬p∨r, 0.9), (p, 0.3) ⊢
(r, 0.3), we clearly have Γ ⊢(r, 0.8) also. This illustrates the fact that in
possibilistic logic, we are interested in practice in the proofs having the
strongest weakest link, and thus leading to the highest certainty levels. Be-
sides, in case Γ contains (r, 0.2) rather than (r, 0.8), then (r, 0.2) is of no use,
since subsumed by (r, 0.3). Indeed, it can be checked that Γ \ {(r, 0.8)} and
Γ \ {(r, 0.8)} ∪{(r, 0.2)} are associated to the same possibility distribution.
The possibility distribution associated with Γ, whose computation is de-
tailed in Table1, is given by πΓ(pqr)=0.8; πΓ(¬pqr)=0.7; πΓ(¬p¬qr)=0.3;
πΓ(p¬qr)=πΓ(¬pq¬r)=πΓ(¬p¬q¬r)=0.2; πΓ(pq¬r)=πΓ(p¬q¬r) = 0.1.
ω
π(¬p∨q,.8) π(¬p∨r,.9) π(¬p∨¬r,.1) π(p,.3) π(q,.7) π(¬q,.2) π(r,.8)
πΓ
pqr
1
1
0.9
1
1
0.8
1
0.8
pq¬r
1
0.1
1
1
1
0.8
0.2
0.1
p¬qr
0.2
1
0.9
1
0.3
1
1
0.2
p¬q¬r
0.2
0.1
1
1
0.3
1
0.2
0.1
¬pqr
1
1
1
0.7
1
0.8
1
0.7
¬pq¬r
1
1
1
0.7
1
0.8
0.2
0.2
¬p¬qr
1
1
1
0.7
0.3
1
1
0.3
¬p¬q¬r
1
1
1
0.7
0.3
1
0.2
0.2
Table 1. Detailed computation of the possibility distribution in the example

POSSIBILISTIC LOGIC — AN OVERVIEW
299
Thus cons-l(Γ) = maxω∈ΩπΓ(ω) = 0.8 and inc-l(Γ) = 1 −0.8 = 0.2. More-
over inc-l(Γ\{(¬q, 0.2)})=0.1, and inc-l(Γ\{(¬q, 0.2), (¬p∨¬r, 0.1)})=0.
□
Remark 2. Using the weakest link-resolution rule repeatedly, leads to a
refutation-based proof procedure that is sound and complete w. r. t. the
semantics exists for propositional possibilistic logic [Dubois et al., 1994c].
It exploits an adaptation of an A∗search algorithm in to order to reach the
empty clause with the greatest possible certainty level [Dubois et al., 1987].
Algorithms and complexity evaluation (similar to the one of classical logic)
can be found in [Lang, 2001].
Remark 3. It is also worth pointing out that a similar approach with
lower bounds on probabilities would not ensure completeness [Dubois et al.,
1994a]. Indeed the repeated use of the probabilistic counterpart of the above
resolution rule, namely (¬a ∨b, α), (a ∨c, β) |= (b ∨c, max(0, α + β −1))
(where (d, α) here means Probability(d) ≥α), is not always enough for
computing the best probability lower bounds on a formula, given a set of
probabilistic constraints of the above form. This is due to the fact that a
set of formulas all having a probability at least equal to α is not deductively
closed in general (except if α = 1).
Remark 4. Moreover, a formula such as (¬a∨b, α) can be rewritten under
the semantically equivalent form (b, min(t(a), α)), where t(a) = 1 if a is true
and t(a) = 0 if p is false. This latter formula now reads “b is α-certain,
provided that a is true” and can be used in hypothetical reasoning in case
(a, γ) is not deducible from the available information (for some γ > 0)
[Benferhat et al., 1994a; Dubois and Prade, 1996].
Formulas associated with lower bounds of possibility
A piece of
information of the form (a, α) (meaning N(a) ≥α) is also semantically
equivalent to Π(¬a) ≤1 −α, i.e. in basic possibilistic logic we are dealing
with upper bounds of possibility measures. Formulas associated with lower
bounds of possibility measures (rather than necessity measures) have been
also introduced [Dubois and Prade, 1990b; Lang et al., 1991; Dubois et al.,
1994c]. A possibility-necessity resolution rule then governs the inference:
N(a ∨b) ≥α, Π(¬a ∨c) ≥β ⊨Π(b ∨c) ≥α ∗β
with α ∗β = α if β > 1 −α and α ∗β = 0 if 1 −α ≥β.2
2Noticing that Π(¬a∧¬b) ≤1−α, and observing that ¬a∨c ≡(¬a∧¬b)∨[(¬a∧b)∨c],
we have β ≤Π(¬a ∨c) ≤max(1 −α, Π(b ∨c)). Besides, β ≤max(1 −α, x) ⇔x ≥α ∗β,
where∗is a (non-commutative) conjunction operator associated by residuation to the mul
tiple-valued implication α→x=max(1−α,x)=inf{β ∈[0, 1]|α∗β ≤x}. Hence the result.

300
DIDIER DUBOIS AND HENRI PRADE
This rule and the weakest link-resolution rule of basic possibilistic logic
are graded counterparts of two inference rules well-known in modal logic
[Fari˜nas del Cerro, 1985].
The possibility-necessity resolution rule can be used for handling partial
ignorance, where fully ignoring a amounts to writing that Π(a) = 1 =
Π(¬a). This expresses “alleged ignorance” and corresponds more generally
to the situation where Π(a) ≥α > 0 and Π(¬a) ≥β > 0. This states
that both a and ¬a are somewhat possible, and contrasts with the type of
uncertainty encoded by (a, α), which expresses that ¬a is rather impossible.
Alleged ignorance can be transmitted through equivalences. Namely from
Π(a) ≥α > 0 and Π(¬a) ≥β > 0, one can deduce Π(b) ≥α > 0 and
Π(¬b) ≥β > 0 provided that we have (¬a ∨b, 1) and (a ∨¬b, 1) [Dubois
and Prade, 1990b; Prade, 2006].
3.2
Reasoning under inconsistency
As already emphasized, an important feature of possibilistic logic is its abil-
ity to deal with inconsistency [Dubois and Prade, 2011b]. Indeed all formu-
las whose level is strictly greater than inc-l(Γ) are safe from inconsistency
in a possibilistic logic base Γ. But, any formula in Γ whose level is less or
equal to inc-l(Γ) is ignored in the standard possibilistic inference process;
these formulas are said to be “drowned”. However, other inferences that sal-
vage formulas that are below the level of inconsistency, but are not involved
in some inconsistent subsets of formulas, have been deﬁned and studied;
see [Benferhat et al., 1999a] for a survey. One may indeed take advantage
of the weights for handling inconsistency in inferences, while avoiding the
drowning eﬀect (at least partially). The main approaches are now reviewed.
Degree of paraconsistency and safely supported-consequences
An
extension of the possibilistic inference has been proposed for handling para-
consistent information [Dubois et al., 1994b; Benferhat et al., 1999a]. It is
deﬁned as follows. First, for each formula a such that (a, α) is in Γ, we
extend the language and compute triples (a, β, γ) where β (resp. γ) is the
highest degree with which a (resp. ¬a) is supported in Γ. More precisely, a
is said to be supported in Γ at least at degree β if there is a consistent sub-
base of (Γβ)∗that entails a, where (Γβ)∗= {ai|(ai, αi) ∈Γ and αi ≥β}.
Let Γo denote the set of bi-weighted formulas which is thus obtained.
EXAMPLE 2. Take
Γ = {(p, 0.8), (¬p ∨q, 0.6), (¬p, 0.5), (¬r, 0.3), (r, 0.2), (¬r ∨q, 0.1)}.

POSSIBILISTIC LOGIC — AN OVERVIEW
301
Then, Γo = {(a,0.8,0.5), (¬p,0.5,0.8), (¬r,0.3,0.2), (r,0.2,0.3), (¬p∨q,0.6,0),
(¬r ∨q,0.6,0)}.
Indeed consider, e.g., (¬r ∨q, 0.6, 0).
Then we have that (p, 0.8) and
(¬p ∨q, 0.6) entail (q, 0.6) (modus ponens), which implies (¬r ∨q, 0.6, 0)
(logical weakening); it only uses formulas above the level of inconsistency
0.5; but there is no way to derive ¬q from any consistent subset of Γ∗; so
γ = 0 for ¬r ∨q.
□
A formula (a, β, γ) is said to have a paraconsistency degree equal to
min(β, γ).
Then the following generalized resolution rule (¬a ∨b, β, γ),
(a ∨c, β′, γ′) ⊢(b ∨c, min(β, β′), max(γ, γ′)) is valid [Dubois et al., 1994b].
In particular, the formulas of interest are such that β ≥γ, i.e. the formula
is at least as certain as it is paraconsistent. Clearly the set of formulas
of the form (a, β, 0) in Γo has an inconsistency level equal to 0, and thus
leads to safe conclusions. However, one may obtain a larger set of consistent
conclusions from Γo as explained now.
Deﬁning an inference relation from Γo requires two evaluations:
- the undefeasibility degree of a consistent set A of formulas:
UD(A) = min{β | (a, β, γ) ∈Γo and a ∈A}
- the unsafeness degree of a consistent set A of formulas:
US(A) = max{γ | (a, β, γ) ∈Γo and a ∈A}.
We say that A is a reason (or an argument) for b if A is a minimal (for set
inclusion) consistent subset of Γ that implies b, i.e.,
• A ⊆Γ
• A∗̸⊢PL ⊥
• A∗⊢PL b
• ∀B ⊂A, B∗̸⊢PL b
Let label(b) = {(A, UD(A), US(A)) | A is a reason for b}, and label(b)∗=
{A | (A, UD(A), US(A)) ∈label(b)}. Then (b, UD(A′), US(A′)) is said to
be a DS-consequence of Γo (or Γ), denoted by Γo ⊢DS (b, UD(A′), US(A′)),
if and only if UD(A′) > US(A′), where A′ is maximizing UD(A) in label(b)∗
and in case of several such A′, the one which minimizes US(A′). It can be
shown that ⊢DS extends the entailment in possibilistic logic [Benferhat et
al., 1999a].

302
DIDIER DUBOIS AND HENRI PRADE
EXAMPLE 3. (Example 2 continued) In the above example, label(b) =
{(A, 0.6, 0.5), (B, 0.2, 0.3)} with A = {(p, 0.8, 0.5), (¬p ∨q, 0.6, 0)} and B =
{(r, 0.2, 0.3), (¬r ∨q, 0.6, 0)}. Then, Γo ⊢DS (q, 0.6, 0.5).
□
But, if we rather ﬁrst minimize US(A′) and then maximize UD(A′), the
entailment would not extend the possibilistic entailment.
Indeed in the
above example, we would select (B, 0.2, 0.3) but 0.2 > 0.3 does not hold,
while Γ ⊢(q, 0.6) since 0.6 > inc-l(Γ) = 0.5.
Note also that ⊢DS is more productive than the possibilistic entailment,
as seen on the example, e.g., Γo ⊢DS (¬r, 0.3, 0.2), while Γ ⊢(¬r, 0.3) does
not hold since 0.3 < inc-l(Γ) = 0.5.
Another entailment denoted by ⊢SS, named safely supported consequence
relation, less demanding than ⊢DS, is deﬁned by Γo ⊢SS b if and only
∃A ∈label(b) such that UD(A) > US(A). It can be shown that the set
{b | Γo ⊢SS b} is classically consistent [Benferhat et al., 1999a].
Another proposal that sounds natural, investigated in [Benferhat et al.,
1993a] is the idea of argued consequence ⊢A, where Γ ⊢A b if there exists
a reason A for b stronger than any reason B for ¬b in the sense that the
possibilistic inference from A yields b with a level strictly greater than the
level obtained for ¬b from any reason B. ⊢A is more productive than ⊢SS.
Unfortunately, ⊢A may lead to classically inconsistent of conclusions. There
are several other inference relations that have been deﬁned, in particular
using a selection of maximal consistent sub-bases based on the certainty or
priority levels. However these approaches are more adventurous than ⊢SS
and may lead to debatable conclusions. See for details [Benferhat et al.,
1999a].
From quasi-classical logic to quasi-possibilistic logic
Besnard and
Hunter [1995] [Hunter, 2000] have deﬁned a new paraconsistent logic, called
quasi-classical logic. This logic has several nice features, in particular the
connectives behave classically, and when the knowledge base is classically
consistent, then quasi-classical logic gives almost the same conclusions as
classical logic (with the exception of tautologies or formulas containing tau-
tologies). Moreover, the inference in quasi-classical logic has a low com-
putational complexity. The basic ideas behind this logic is to use all rules
of classical logic proof theory, but to forbid the use of resolution after the
introduction of a disjunction (it allows to get rid of the ex falso quodlibet
sequitur).
So the rules of quasi-classical logic are split into two classes:
composition and decomposition rules, and the proofs cannot use decompo-
sition rules once a composition rule has been used. Intuitively speaking,

POSSIBILISTIC LOGIC — AN OVERVIEW
303
this means that we may have resolution-based proofs both for a and ¬a.
We also derive the disjunctions built from such previous consequences (e.g.
¬a ∨b) as additional valid consequences. But it is forbidden to reuse such
additional consequences for building further proofs [Hunter, 2000].
Although possibilistic logic takes advantage of its levels for handling in-
consistency, there are situations where it oﬀers no useful answers, while
quasi-classical logic does. This is when formulas involved in inconsistency
have the same level, especially the highest one, 1. Thus, consider the exam-
ple Γ = {(p, 1), (¬p ∨q, 1), (¬p, 1)}, where quasi-classical logic infers a, ¬p,
q from Γ∗, while everything is drowned in possibilistic logic, and nothing
can be derived by the safely supported consequence relation. This has led
to a preliminary proposal of a quasi-possibilistic logic [Dubois et al., 2003].
It would also deserve a careful comparison with the use of the generalized
resolution rule, mentioned above, applied to the set Γo of bi-weighted for-
mulas. Note that in the above example, this rule yields (p, 1, 1), (¬p, 1, 1)
and (q, 1, 1), as expected. Such concerns should also be related to Belnap’s
four-valued logic which leaves open the possibility that a formula and its
negation be supported by distinct sources [Belnap, 1977; Dubois, 2012].
3.3
Possibilistic logic with symbolic weights
In basic possibilistic logic, the certainty levels associated to formulas are
assumed to belong to a totally ordered scale. In some cases, their value
and their relative ordering may be unknown, and it may be then of interest
to process them in a purely symbolic manner, i.e.
computing the level
from a derived formula as a symbolic expression. For instance, we have
{(p, α), (¬p ∨q, β), (q, γ)} ⊢(q, max(min(α, β), γ)). This induces a partial
order between formulas based on the partial order between symbolic weights
(e.g., max(min(α, β), α, γ) ≥min(α, δ) for any values of α, β, γ, δ).
Possibilistic logic formulas with symbolic weights have been used in pref-
erence modeling [Dubois et al., 2006; Dubois et al., 2013b; Dubois et al.,
2013c]. Then, interpretations (corresponding to the diﬀerent alternatives)
are compared in terms of vectors acknowledging the satisfaction or the vio-
lation of the formulas associated with the diﬀerent (conditional) preferences,
using suitable order relations. Thus, partial orderings of interpretations can
be obtained, and may be reﬁned in case some additional information on the
relative priority of the preferences is given.
Possibilistic formulas with symbolic weights can be reinterpreted as two-
sorted classical logic formulas. Thus, the formula (a, α) can be re-encoded
by the formula a∨A. Such a formula can be intuitively thought as expressing

304
DIDIER DUBOIS AND HENRI PRADE
that a should be true if the situation is not abnormal (a ∨A ≡¬A →a).
Then it can be seen that {a∨A, ¬a∨b∨B} ⊢b∨A∨B is the counterpart of
{(a, α), (¬a∨b, β)} ⊢(b, min(α, β)) in possibilistic logic, as {a∨A, a∨A′} ⊢
a ∨(A ∧A′) is the counterpart of {(a, α), (a, α′)} ⊢(a, max(α, α′)).
Partial information about the ordering between levels associated to possi-
bilistic formulas can be also represented by classical logic formulas pertain-
ing to symbolic levels. Thus, the constraint α ≥β translates into formula
¬A ∨B3. This agrees with the ideas that “the more abnormal ‘a false’ is,
the more certain a”, and that “if it is very abnormal, it is abnormal”. It
can be checked that {a ∨A, ¬a ∨b ∨B, ¬A ∨B} ⊢a ∨B and b ∨B, i.e., we
do obtain the counterpart of (b, β), while β = min(α, β). The possibilistic
logic inference machinery can be recast in this symbolic setting, and eﬃcient
computation procedures can be developed taking advantage of the compi-
lation of the base in a dNNF format [Benferhat and Prade, 2005], including
the special case where the levels are totally ordered [Benferhat and Prade,
2006]. In this latter case, it provides a way for compiling a possibilistic logic
base and then process inference from it in polynomial time.
One motivation for dealing with a partial order on formulas relies on the
fact that possibilistic logic formulas coming from diﬀerent sources, may not
always be stratiﬁed according to a complete preorder. Apart from the above
one, several other extensions of possibilistic logic have been proposed when
the total order on formulas is replaced by a partial preorder [Benferhat et
al., 2004b; Cayrol et al., 2014]. The primary focus is usually on semantic
aspects, namely the construction of a partial order on interpretations from a
partial order on formulas and conversely. The diﬃculty lies in the fact that
equivalent deﬁnitions in the totally ordered case are no longer equivalent
in the partially ordered one, and that a partial ordering on subsets of a
set cannot be expressed by means of a single partial order on the sets of
elements in general.
3.4
Possibilistic networks
(Basic) possibilistic logic bases provide a compact representation of pos-
sibility distributions involving a ﬁnite number of possibility levels.
An-
other compact representation of such qualitative possibility distributions is
in terms of possibilistic directed graphs, which use the same conventions as
Bayesian nets, but rely on conditional possibility [Benferhat et al., 2002a].
An interesting feature of possibilistic logic is then that a possibilistic logic
3Note that one cannot express strict inequalities (α > β) in this way (except on a
ﬁnite scale).

POSSIBILISTIC LOGIC — AN OVERVIEW
305
base has thus graphical representation counterparts to which the base is
semantically equivalent. We start with a brief reminder on the notions of
conditioning and independence in possibility theory.
Conditioning
Notions of conditioning exist in possibility theory. Con-
ditional possibility can be deﬁned similarly to probability theory using a
Bayesian-like equation of the form [Dubois and Prade, 1990a]
Π(B ∩A) = Π(B | A) ⋆Π(A)
where Π(A) > 0 and ⋆may be the minimum or the product; moreover
N(B | A) = 1 −Π(Bc | A). The above equation makes little sense for
necessity measures, as it becomes trivial when N(A) = 0, that is under lack
of certainty, while in the above deﬁnition, the equation becomes problematic
only if Π(A) = 0, which is natural as then A is considered impossible (see
[Coletti and Vantaggi, 2009] for the handling of this situation). If operation
⋆is the minimum, the equation Π(B ∩A) = min(Π(B | A), Π(A)) fails
to characterize Π(B | A), and we must resort to the minimal speciﬁcity
principle to come up with a qualitative conditioning of possibility [Dubois
and Prade, 1988]:
Π(B | A) =
(
1 if Π(B ∩A) = Π(A) > 0,
Π(B ∩A) otherwise.
It is clear that N(B | A) > 0 if and only if Π(B ∩A) > Π(Bc ∩A).
Note also that N(B | A) = N(Ac ∪B) if N(B | A) > 0.
Moreover,
if Π(B | A) > Π(B) then Π(B | A) = 1, which points out the limited
expressiveness of this qualitative notion (no gradual positive reinforcement
of possibility). However, it is possible to have that N(B) > 0, N(Bc | A1) >
0, N(B | A1 ∩A2) > 0 (i.e., oscillating beliefs).
In the numerical setting, we must choose ⋆= product that preserves
continuity, so that Π(B | A) = Π(B∩A)
Π(A)
which makes possibilistic and prob-
abilistic conditionings very similar [De Baets et al., 1999] (now, gradual
positive reinforcement of possibility is allowed).
Independence
There are also several variants of possibilistic indepen-
dence between events. Let us mention here the two basic approaches:
• Unrelatedness: Π(A∩B) = min(Π(A), Π(B)). When it does not hold,
it indicates an epistemic form of mutual exclusion between A and B.
It is symmetric but sensitive to negation. When it holds for all pairs

306
DIDIER DUBOIS AND HENRI PRADE
made of A, B and their complements, it is an epistemic version of
logical independence.
• Causal independence: Π(B | A) = Π(B).
This notion is diﬀerent
from the former one and stronger. It is a form of directed epistemic
independence whereby learning A does not aﬀect the plausibility of
B. It is neither symmetric not insensitive to negation: in particular,
it is not equivalent to N(B | A) = N(B).
Generally, independence in possibility theory is neither symmetric, nor in-
sensitive to negation. For Boolean variables, independence between events
is not equivalent to independence between variables. But since the possi-
bility scale can be qualitative or quantitative, and there are several forms
of conditioning, there are also various possible forms of independence. For
studies of these diﬀerent notions and their properties see [De Cooman, 1997;
De Campos and Huete, 1999; Dubois et al., 1997; Dubois et al., 1999a;
Ben Amor et al., 2002].
Graphical structures
Like joint probability distributions, joint possi-
bility distributions can be decomposed into a conjunction of conditional
possibility distributions (using ⋆= minimum, or product), once an ordering
of the variables is chosen, in a way similar to Bayes nets [Benferhat et al.,
2002a]. A joint possibility distribution associated with ordered variables
X1, . . . , Xn, can be decomposed by the chain rule
π(X1, . . . , Xn) = π(Xn | X1, . . . , Xn−1) ⋆· · · ⋆π(X2 | X1) ⋆π(X1).
Such a decomposition can be simpliﬁed by assuming conditional indepen-
dence relations between variables, as reﬂected by the structure of the graph.
The form of independence between variables at work here is conditional non-
interactivity: Two variables X and Y are independent in the context Z, if for
each instance (x, y, z) of (X, Y, Z) we have: π(x, y | z) = π(x | z) ⋆π(y | z).
Possibilistic networks are thus deﬁned as counterparts of Bayesian net-
works [Pearl, 1988] in the context of possibility theory. They share the same
basic components, namely:
(i) a graphical component which is a DAG (Directed Acyclic Graph) G=
(V, E) where V is a set of nodes representing variables and E a set of edges
encoding conditional (in)dependencies between them.
(ii) a valued component associating a local normalized conditional possibility
distribution to each variable Vi ∈V in the context of its parents. The two
deﬁnitions of possibilistic conditioning lead to two variants of possibilistic

POSSIBILISTIC LOGIC — AN OVERVIEW
307
networks: in the numerical context, we get product-based networks, while in
the ordinal context, we get min-based networks (also known as qualitative
possibilistic networks). Given a possibilistic network, we can compute its
joint possibility distribution using the above chain rule. Counterparts of
product-based numerical possibilistic nets using ranking functions exist as
well [Spohn, 2012].
Ben Amor and Benferhat [2005] have investigated the properties of qual-
itative independence that enable local inferences to be performed in possi-
bilistic nets. Uncertainty propagation algorithms suitable for possibilistic
graphical structures have been studied [Ben Amor et al., 2003; Benferhat
et al., 2005], taking advantage of the idempotency of min operator in the
qualitative case [Ben Amor et al., 2003]. Such graphical structures may
be also of particular interest for representing preferences [Ben Amor et al.,
2014].
Possibilistic nets and possibilistic logic
Since possibilistic nets and
possibilistic logic bases are compact representations of possibility distribu-
tions, it should not come as a surprise that possibilistic nets can be di-
rectly translated into possibilistic logic bases and vice-versa, both when
conditioning is based on minimum or on product [Benferhat et al., 2002a;
Benferhat et al., 2001b]. Hybrid representations formats have been intro-
duced where local possibilistic logic bases are associated to the nodes of
a graphical structure rather than conditional possibility tables [Benferhat
and Smaoui, 2007a]. An important feature of the possibilistic logic setting
is the existence of such equivalent representation formats: set of prioritized
logical formulas, preorders on interpretations (possibility distributions) at
the semantical level, possibilistic nets, but also set of conditionals of the
form Π(p ∧q) > Π(p ∧¬q), and there are algorithms for translating one
format in another [Benferhat et al., 2001a].
4
APPLICATIONS OF BASIC POSSIBILISTIC LOGIC
Basic possibilistic logic has found many applications in diﬀerent reasoning
tasks, beyond the simple deductive propagation of certainty levels or pri-
ority levels. In the following, we survey its use in default reasoning (and
its utilization in causality ascription), in belief revision and information fu-
sion, in decision under uncertainty, and in the handling of uncertainty in
information systems.
In a computational perspective, possibilistic logic has also impacted logic

308
DIDIER DUBOIS AND HENRI PRADE
programming [Dubois et al., 1991c; Benferhat et al., 1993b; Alsinet and
Godo., 2000; Alsinet et al., 2002; Nicolas et al., 2006; Nieves et al., 2007;
Confalonieri et al., 2012; Bauters et al., 2010; Bauters et al., 2011; Bauters
et al., 2012]. It has also somewhat inﬂuenced the handling of soft constraints
in constraint satisfaction problems [Schiex, 1992; Schiex et al., 1995]. Let
us also mention applications to diagnosis and recognition problems [Dubois
et al., 1990; Benferhat et al., 1997a; Dubois and Prade, 2000; Grabisch and
Prade, 2001; Grabisch, 2003], and to the encoding of control access policies
[Benferhat et al., 2003].
4.1
Default reasoning and causality
Possibilistic logic can be used for describing the normal course of things and
a possibilistic logic base reﬂects how entrenched are the beliefs of an agent.
This is why possibilistic logic is of interest in default reasoning, but also in
causality ascription, as surveyed in this subsection.
Default reasoning
Nonmonotonic reasoning has been extensively stud-
ied in AI in relation with the problem of reasoning under incomplete infor-
mation with rules having potential exceptions [L´ea Somb´e Group, 1990], or
for dealing with the frame problem in dynamic worlds [Brewka et al., 2011].
In the following, we recall the possibilistic approach [Benferhat et al., 1998b;
Dubois and Prade, 2011c], which turned out [Benferhat et al., 1997b] to
provide a faithful representation of the postulate-based approach proposed
by Kraus, Lehmann and Magidor [1990], and completed in [Lehmann and
Magidor, 1992].
A default rule “if a then b, generally”, denoted a ⇝b, is then understood
formally as the constraint
Π(a ∧b) > Π(a ∧¬b)
on a possibility measure Π describing the semantics of the available knowl-
edge. It expresses that in the context where a is true, there exists situations
where having b true is strictly more satisfactory than any situations where
b is false in the same context. As already said, this constraint is equivalent
to N(b | a) = 1 −Π(¬b | a) > 0, when Π(b | a) is deﬁned as the greatest
solution of the min-based equation Π(a ∧b) = min(Π(b | a), Π(a))4.
The above constraint can be equivalently expressed in terms of a compar-
ative possibility relation, namely a ∧b >Π a ∧¬b. Any ﬁnite consistent set
4It would be true as well with the product-based conditioning.

POSSIBILISTIC LOGIC — AN OVERVIEW
309
of constraints of the form ai ∧bi ≥Π ai ∧¬bi, representing a set of defaults
∆= {ai ⇝bi, i = 1, · · · , n}, is compatible with a non-empty family of
relations >Π, and induces a partially deﬁned ranking >π on Ω, that can be
completed according to the principle of minimal speciﬁcity, e.g. [Benferhat
et al., 1999b]. This principle assigns to each world ω the highest possibility
level (in forming a well-ordered partition of Ω) without violating the con-
straints. This deﬁnes a unique complete preorder. Let E1, . . . , Em be the
obtained partition. Then ω >π ω′ if ω ∈Ei and ω′ ∈Ej with i < j, while
ω ∼π ω′ if ω ∈Ei and ω′ ∈Ei (where ∼π means ≥π and ≤π).
A numerical counterpart to >π can be deﬁned by π(ω) = m+1−i
m
if ω ∈
Ei, fori = 1, . . . , m. Note that this is purely a matter of convenience to use
a numerical scale, and any other numerical counterpart such that π(ω) >
π(ω′) iﬀω >π ω′ will work as well. Namely the range of π is used as an
ordinal scale.
EXAMPLE 4. Let us consider the following classical example with default
rules d1: “birds ﬂy”, d2: “penguins do not ﬂy”, d3: “penguins are birds”,
symbolically written
d1 : b ⇝f; d2 : p ⇝¬f; d3 : p ⇝b.
The set of three defaults is thus represented by the following set C of con-
straints:
b ∧f ≥Π b ∧¬f ; p ∧¬f ≥Π p ∧f ; p ∧b ≥Π p ∧¬b.
Let Ωbe the ﬁnite set of interpretations of the considered propositional
language, generated by b, f, p in the example, that is Ω= {ω0 : ¬b∧¬f ∧¬p,
ω1 : ¬b∧¬f ∧p, ω2 : ¬b∧f ∧¬p, ω3 : ¬b∧f ∧p, ω4 : b∧¬f ∧¬p, ω5 : b∧¬f ∧p,
ω6 : b ∧f ∧¬p, ω7 : b ∧f ∧p}. Any interpretation ω thus corresponds
to a particular proposition. One can then compute the possibility of any
proposition. For instance, Π(b ∧f) = max(π(ω6), π(ω7)).
Then the set of constraints C on interpretations can be written as:
C1 : max(π(ω6), π(ω7)) > max(π(ω4), π(ω5)),
C2 : max(π(ω5), π(ω1)) > max(π(ω3), π(ω7)),
C3 : max(π(ω5), π(ω7)) > max(π(ω1), π(ω3)).
The well ordered partition of Ωwhich is obtained in this example is
{ω0, ω2, ω6} >π {ω4, ω5} >π {ω1, ω3, ω7}.
In the example, we have m = 3 and π(ω0) = π(ω2) = π(ω6) = 1;
π(ω4) = π(ω5) = 2/3; π(ω1) = π(ω3) = π(ω7) = 1/3.
□
From the possibility distribution π associated with the well ordered parti-
tion, we can compute the necessity level N(a) of any proposition a. The

310
DIDIER DUBOIS AND HENRI PRADE
method then consists in turning each default pi ⇝qi into a possibilistic
clause (¬pi ∨qi, N(¬pi ∨qi)), where N is computed from the greatest pos-
sibility distribution π induced by the set of constraints corresponding to
the default knowledge base, as already explained. We thus obtain a possi-
bilistic logic base K. This encodes the generic knowledge embedded in the
default rules. Then we apply the possibilistic inference for reasoning with
the formulas in K encoding the defaults together with the available factual
knowledge encoded as fully certain possibilistic formulas in a base F.
However, the conclusions that can be obtained from K∪F with a certainty
level strictly greater than the level of inconsistency of this base are safe.
Roughly speaking, it turns out that in this approach, the most speciﬁc rules
w.r.t. a given context remain above the level of inconsistency.
EXAMPLE 5. Example 4 continued. Using the possibility distribution ob-
tained at the previous step, we compute:
N(¬p ∨¬f)
=
min{1 −π(ω)|ω |= p ∧f}
=
min(1 −π(ω3), 1 −π(ω7)) = 2/3,
N(¬b∨f) = min{1−π(ω) | ω |= b∧¬f} = min(1−π(ω4), 1−π(ω5)) = 1/3,
and N(¬p ∨b) = min(1 −π(ω1), 1 −π(ω3)) = 2/3.
Thus, we have the possibilistic logic base
K = {(¬p ∨¬f, 2/3), (¬p ∨b, 2/3), (¬b ∨f, 1/3)}.
Suppose that all we know about the factual situation under consideration
is that “Tweety” is a bird, which is encoded by F = {(b, 1)}. Then we
apply the weakest link resolution rule, and we can check that K ∪{(b, 1)} ⊢
(f, 1/3), i.e., we conclude that if all we know about “Tweety” is that it
is a bird, then it ﬂies. If we are told that “Tweety” is in fact a penguin,
i.e., F = {(b, 1), (p, 1)}, then K ∪F ⊢(⊥, 1/3), which means that K ∪
{(b, 1)} augmented with the new piece of factual information {(p, 1)} is now
inconsistent (at level 1/3). But the following inference K ∪F ⊢(¬f, 2/3) is
valid (since 2/3 > inc-l(K ∪F) = 1/3). Thus, knowing that “Tweety” is a
penguin, we now conclude that it does not ﬂy.
□
This encoding takes advantage of the fact that when a new piece of in-
formation is received, the level of inconsistency of the base cannot decrease,
and if it strictly increases, some inferences that were safe before are now
drowned in the new inconsistency level of the base and are thus no longer
allowed, hence a nonmonotonic consequence mechanism takes place.
Such an approach has been proved to be in full agreement with the Kraus-
Lehmann-Magidor postulates-based approach to nonmonotonic reasoning

POSSIBILISTIC LOGIC — AN OVERVIEW
311
[Kraus et al., 1990]. More precisely, two nonmonotonic entailments can be
deﬁned in the possibilistic setting, the one presented above, based on the less
speciﬁc possibility distribution compatible with the constraints encoding the
set of defaults, and another one more cautious, where one considers that b
can be deduced in the situation where all we know is F = {a} if and only if
the inequality Π(a∧b) > Π(a∧¬b) holds true for all the Π compatible with
the constraints encoding the set of defaults. The ﬁrst entailment coincides
with the rational closure inference [Lehmann and Magidor, 1992], while the
later corresponds to the (cautious) preferential entailment [Kraus et al.,
1990]; see [Dubois and Prade, 1995a; Benferhat et al., 1997b].
Besides,
the ranking of the defaults obtained above from the well-ordered partition
[Benferhat et al., 1992] is the same as the Z-ranking introduced by Pearl
[1990].
While the consequences obtained with the preferential entailment are
hardly debatable, the ones derived with the rational closure are more ad-
venturous.
However these latter consequences can be always modiﬁed if
necessary by the addition of further defaults. These added defaults may
express independence information [Dubois et al., 1999a] of the type “in
context c, the truth or the falsity of a has no inﬂuence on the truth of b”
[Benferhat et al., 1994b; Benferhat et al., 1998b]. Lastly, a default rule may
be itself associated with a certainty level; in such a case each formula will
be associated with two levels, namely a priority level reﬂecting its relative
speciﬁcity in the base, and its certainty level respectively [Dupin de Saint
Cyr and Prade, 2008].
Let us also mention an application to possibilistic inductive logic pro-
gramming. Indeed learning a stratiﬁed set of ﬁrst-order logic rules as an
hypothesis in inductive logic programming has been shown of interest for
learning both rules covering normal cases and more speciﬁc rules that handle
more exceptional cases [Serrurier and Prade, 2007].
Causality
Our understanding of sequences of reported facts depends on
our own beliefs on the normal course of things. We have seen that N(b|a) >
0 can be used for expressing that in context a, b is normally true. Then
qualitative necessity measures may be used for describing how (potential)
causality is perceived in relation with the advent of an abnormal event that
precedes a change.
Namely, if on the one hand an agent holds the two following beliefs rep-
resented by N(b|a) > 0 and N(¬b|a ∧c) > 0 about the normal course of
things, and if on the other hand it has been reported that we are in context
a, and that b, which was true, has become false after c takes place, then the

312
DIDIER DUBOIS AND HENRI PRADE
agent will be led to think that “a caused ¬b”. See [Bonnefon et al., 2008]
for a detailed presentation and discussion, and [Bonnefon et al., 2012] for
a study of the very restricted conditions under which causality is transitive
in this approach.
The theoretical consequences of this model have been
validated from a cognitive psychology point of view.
Perceived causality may be badly aﬀected by spurious correlations. For
a proper assessing of causality relations, Pearl [2000] has introduced the no-
tion of intervention in Bayesian networks, which comes down to enforcing
the values of some variables so as to lay bare their inﬂuence on other ones.
Following the same line, possibilistic networks have been studied from the
standpoint of causal reasoning, using the concept of intervention; see [Ben-
ferhat and Smaoui, 2007b; Benferhat, 2010; Benferhat and Smaoui, 2011],
where tools for handling interventions in the possibilistic setting have been
developed. Finally, a counterpart of the idea of intervention has been inves-
tigated in possibilistic logic knowledge bases, which are non-directed struc-
tures (thus contrasting with Bayesian and possibilistic networks) [Benferhat
et al., 2009].
4.2
Belief revision and information fusion
In belief revision, the new input information that ﬁres the revision process
has priority on the information contained in the current belief set. This
contrasts with information fusion where sources play symmetric roles (even
if they have diﬀerent reliability levels). We brieﬂy survey the contributions
of possibilistic logic to these two problems.
Belief revision and updating
Keeping in mind that nonmonotonic rea-
soning and belief revision can be closely related [G¨ardenfors, 1990], it should
not be a surprize that possibilistic logic ﬁnds application also in belief revi-
sion. In fact, comparative necessity relations (which can be encoded by ne-
cessity measures) [Dubois, 1986] are nothing but the epistemic entrenchment
relations [Dubois and Prade, 1991] that underly well-behaved belief revision
processes [G¨ardenfors, 1988]. This enables the possibilistic logic setting to
provide syntactic revision operators that apply to possibilistic knowledge
bases, including the case of uncertain inputs [Dubois and Prade, 1997a;
Benferhat et al., 2002c; Benferhat et al., 2010; Qi, 2008; Qi and Wang,
2012]. Note that in possibilistic logic, the epistemic entrenchment of the
formulas is made explicit through the certainty levels. Formulas (a, α) are
viewed as pieces of belief that are more or less certain. Moreover, in a re-
vision process it is expected that all formulas independent of the validity

POSSIBILISTIC LOGIC — AN OVERVIEW
313
of the input information should be retained in the revised state of belief;
this intuitive idea may receive a precise meaning using a suitable deﬁni-
tion of possibilistic independence between events [Dubois et al., 1999a]. See
[Dubois et al., 1998a] for a comparative overview of belief change operations
in the diﬀerent representation settings (including possibilistic logic).
Updating in a dynamic environment obeys other principles than the revi-
sion of a belief state by an input information in a static world, see, e.g., [L´ea
Somb´e Group (ed.), 1994]. It can be related to the idea of Lewis’imaging
[1976], whose possibilistic counterpart has been proposed in [Dubois and
Prade, 1993]. A possibilistic logic transposition of Kalman ﬁltering that
combines the ideas of updating and revision can be found in [Benferhat et
al., 2000b].
Information fusion
Information fusion can take place in the diﬀerent
representation formats of the possibilistic setting. In particular, the combi-
nation of possibility distributions can be equivalently performed in terms of
possibilistic logic bases. Namely, the syntactic counterpart of the pointwise
combination of two possibility distributions π1 and π2 into a distribution
π1 ⊕π2 by any monotonic combination operator ⊕5 such that 1 ⊕1 =
1, can be computed, following an idea ﬁrst proposed in [Boldrin, 1995;
Boldrin and Sossai, 1997]. Namely, if the possibilistic logic base Γ1 is asso-
ciated with π1 and the base Γ2 with π2, a possibilistic base that is semanti-
cally equivalent to π1 ⊕π2 can be obtained in the following way [Benferhat
et al., 1998a]:
Γ1⊕2 = {(ai, 1 −(1 −αi) ⊕1) s.t. (ai, αi) ∈Γ1}
∪{(bj, 1 −1 ⊕(1 −βj)) s.t. (bj, βj) ∈Γ2}
∪{(ai ∨bj, 1 −(1 −αi) ⊕(1 −βj)) s.t. (ai, αi) ∈Γ1, (bj, βj) ∈Γ2}.
For
⊕= min, we get
Γ1⊕2 = Γ1 ∪Γ2
with
πΓ1∪Γ2 = min(π1, π2)
as expected (conjunctive combination). For ⊕= max (disjunctive combi-
nation), we get
Γ1⊕2 = {(ai ∨bj, min(αi, βj)) s.t. (ai, αi) ∈Γ1, and (bj, βj) ∈Γ2}.
5⊕is supposed to be monotonic in the wide sense for each of its arguments: α ⊕β ≥
γ ⊕δ as soon as α ≥γ and β ≥δ.
Examples of such combination operators ⊕are
triangular norms (a non-decreasing semi-group of the unit interval having identity 1
and absorbing element 0) and the dual triangular co-norms that respectively extend
conjunction and disjunction to multiple-valued settings [Klement et al., 2000].

314
DIDIER DUBOIS AND HENRI PRADE
With non idempotent ⊕operators, some reinforcement eﬀects may be ob-
tained. Moreover, fusion can be applied directly to qualitative or quanti-
tative possibilistic networks [Benferhat and Titouna, 2005; Benferhat and
Titouna, 2009]. See [Benferhat et al., 1999c; Benferhat et al., 2001c; Kaci et
al., 2000; Qi et al., 2010b; Qi et al., 2010a] for further studies on possibilistic
logic merging operators.
Besides, this approach has been also applied to the syntactic encoding of
the merging of classical logic bases based on Hamming distance (where dis-
tances are computed between each interpretation and the diﬀerent classical
logic bases, thus giving birth to counterparts of possibility distributions)
[Benferhat et al., 2002b].
4.3
Qualitative handling of uncertainty in decision and infor-
mation systems
Uncertainty often pervades the available information.
Possibility theory
oﬀers an appropriate setting for the representation of incomplete and un-
certain epistemic information in a qualitative manner. In this subsection,
we provide a brief presentation of the possibilistic logic approach to decision
under uncertainty and to the management of uncertain databases.
Qualitative decision under uncertainty
Possibility theory provides
a valuable setting for qualitative decision under uncertainty where a pes-
simistic and an optimistic decision criteria have been axiomatized [Dubois
and Prade, 1995b; Dubois et al., 2001a; Benferhat et al., 2000a]. The exact
counterpart of these pessimistic and optimistic criteria, when the knowl-
edge and the preferences are respectively expressed under the form of two
distinct possibilistic logic bases, have been shown in [Dubois et al., 1999b]
to correspond to the following deﬁnitions:
• The pessimistic utility u∗(d) of a decision d is the maximal value of
α ∈S such that
Kα ∧d ⊢P L Pν(α)
• The u∗(d) of a decision d is the maximal value of n(α) ∈S such that
Kα ∧d ∧Pα ̸⊢⊥
where S denotes a ﬁnite bounded totally ordered scale, ν is the ordered
reversing map of this scale, Kα is a set of classical logic formulas gathering
the pieces of knowledge that are certain at a level at least equal to α, and

POSSIBILISTIC LOGIC — AN OVERVIEW
315
where Pβ is a set of classical logic formulas made of a set of goals (modeling
preferences) whose priority level is strictly greater than β.
As can be seen, an optimal pessimistic decision leads for sure to the
satisfaction of all the goals in Pν(α) whose priority is greater than a level as
low as possible, according to a part Kα of our knowledge which is as certain
as possible. An optimal optimistic decision maximizes only the consistency
of all the more or less important goals with all the more or less certain
pieces of knowledge. Optimal pessimistic or optimistic decisions can then
be computed in an answer set programming setting [Confalonieri and Prade,
2014]. Besides, this possibilistic treatment of qualitative decision can be also
related to an argumentative view of decision [Amgoud and Prade, 2009].
See also [Liau, 1999; Liau and Liu, 2001] for other possibility theory-based
logical approaches to decision.
Handling uncertainty in information systems
In the possibilistic ap-
proach to the handling of uncertainty in databases, the available informa-
tion on the value of an attribute A for an item x is usually represented by
a possibility distribution deﬁned on the domain of attribute A. Then, con-
sidering a classical query, we can compute two sets of answers, namely the
set of items that more or less certainly satisfy the query (this corresponds
to the above pessimistic viewpoint), and the larger set of items that more
or less possibly satisfy the query (this corresponds to the above optimistic
viewpoint) [Dubois and Prade, 1988].
Computation may become tricky for some basic relational operations such
as the join of two relations, for which it becomes necessary to keep track
that some uncertain values should remain equal in any extension. As in the
probabilistic case, methods based on lineage have been proposed to handle
such problems [Bosc and Pivert, 2005]. Their computational cost remain
heavy in practice.
However, uncertain data can be processed at a much more aﬀordable
cost provided that we restrict ourselves to pieces of information of the
form (a(x), α) expressing that it is certain at level α that a(x) is the
value of attribute A for the item x. More generally, a(x) can be replaced
by a disjunction of values.
Then, a possibilistic logic-like treatment of
uncertainty in databases can take place in a relational database frame-
work. It can be shown that such an uncertainty modeling is a represen-
tation system for the whole relational algebra.
An important result is
that the data complexity associated with the extended operators in this
context is the same as in the classical database case [Bosc et al., 2009;
Pivert and Prade, 2014]. An additional beneﬁt of the possibilistic setting is

316
DIDIER DUBOIS AND HENRI PRADE
R
Name
Married
City
1
John
(yes, α)
(Toulouse, µ)
2
Mary
(yes, 1)
(Albi, ρ)
3
Peter
(no, β)
(Toulouse, φ)
S
City
Flea Market
1
Albi
(yes, γ)
2
Toulouse
(yes, δ)
Table 2. A database with possibilistic uncertainty
an easier elicitation of the certainty levels. We illustrate the idea with the
following simple example.
EXAMPLE 6. Let us consider a database example with two relations R
and S containing uncertain pieces of data. See Table 2. If we look here
for the persons who are married and leave in a city with a ﬂea market,
we shall retrieve John with certainty min(α, µ, δ) and Mary with certainty
min(ρ, γ).
It is also possible to accommodate disjunctive information in this setting.
Assume for instance that the third tuple of relation R is now (Peter, (no, β),
(Albi ∨Toulouse, φ)). Then, if we look for persons who are not married
and leave in a city with a ﬂea market, one retrieve Peter with certainty
min(β, φ, γ, δ). Indeed we have in possibilistic logic that (¬Married, β) and
(Albi ∨Toulouse, φ), (¬Albi ∨Flea Market, γ), (¬Toulouse ∨Flea Market, δ)
entail (¬Married, β) and (Flea Market, min(φ, γ, δ)).
□
This suggests the potentials of a necessity measure-based approach to the
handling of uncertain pieces of information. Clearly, the limited setting of
certainty-qualiﬁed information is less expressive than the use of general pos-
sibility distributions (we cannot here retrieve items that are just somewhat
possible without being somewhat certain), but this framework seems to be
expressive enough for being useful in practice. Let us also mention a possi-
bilistic modeling of the validity and of the completeness of the information
(pertaining to a given topic) in a database [Dubois and Prade, 1997b].
Besides, the possibilistic handling of uncertainty in description logic [Qi et
al., 2011; Zhu et al., 2013] has also computational advantages, in particular
in the case of the possibilistic DL-Lite family [Benferhat and Bouraoui, 2013;
Benferhat et al., 2013]. Lastly, possibilistic logic has been recently shown
to be of interest in database design [Koehler et al., 2014a; Koehler et al.,
2014b].

POSSIBILISTIC LOGIC — AN OVERVIEW
317
5
EXTENSIONS OF POSSIBILISTIC LOGIC
Possibilistic logic has been extended in diﬀerent manners. In this section,
we consider three main types of extension: i) replacing the totally ordered
scale of the certainty levels by a partially ordered structure; ii) dealing with
logical formulas that are weighted in terms of lower bounds of a strong
(guaranteed) possibility measure ∆(see subsection 2.2); iii) allowing for
negation of basic possibilistic logic formulas, or for their disjunction (and
no longer only for their conjunction), which leads to generalized possibilistic
logic.
5.1
Lattice-based possibilistic logics
Basically, a possibilistic formula is a pair made of a classical logic formula
and a label that qualiﬁes in what conditions or in what manner the classical
logic formula is regarded as true. One may think of associating “labels”
other than certainty levels.
It may be lower bounds of other measures
in possibility theory, such as in particular strong possibility measures, as
reviewed in the next subsection.
It may be also labels taking values in
partially ordered structures, such as lattices.
This can be motivated by
diﬀerent needs, as brieﬂy reviewed now.
Diﬀerent intended purposes
Timed possibilistic logic [Dubois et al.,
1991b] has been the ﬁrst proposed extension of this kind. In timed possi-
bilistic logic, logical formulas are associated with sets of time instants where
the formula is known as being certainly true. More generally certainty may
be graded as in basic possibilistic logic, and then formulas are associated
with fuzzy sets of time instants where the grade attached to a time instant
is the certainty level with which the formula is true at that time. At the se-
mantic level, it leads to an extension of necessity (and possibility) measures
now valued in a distributive lattice structure.
Taking inspiration of possibilistic logic, Lafage, Lang and Sabbadin [1999]
have proposed a logic of supporters, where each formula a is associated with
a set of logical arguments in favor of a. More recently, an interval-based
possibilistic logic has been presented [Benferhat et al., 2011] where classical
logic formulas are associated with intervals, thought as imprecise certainty
levels.
Another early proposed idea, in an information fusion perspective, is to
associate each formula with a set of distinct explicit sources that support
its truth [Dubois et al., 1992]. Again, a certainty level may be attached to

318
DIDIER DUBOIS AND HENRI PRADE
each source, and then formulas are associated with fuzzy sets of sources.
This has led to the proposal of a “multiple agent” logic where formulas are
of the form (a, A), where A denotes a subset of agents that are known to
believe that a is true. In contrast with timed possibilistic logic where it is
important to make sure that the knowledge base remains consistent over
time, what matters in multiple agent logic is the collective consistency of
subsets of agents (while the collection of the beliefs held by the whole set
of agents may be inconsistent. We now indicate the main features of this
latter logic.
Multiple agent logic
Multiple agent possibilistic logic was outlined in
[Dubois and Prade, 2007], but its underlying semantics has been laid bare
more recently [Belhadi et al., 2013]. A multiple agent propositional formula
is a pair (a, A), where a is a classical propositional formula of L and A is
a non-empty subset of All, i.e., A ⊆All (All denote the ﬁnite set of all
considered agents). The intuitive meaning of formula (a, A) is that at least
all the agents in A believe that a is true. In spite of the obvious parallel with
possibilistic logic (where propositions are associated with levels expressing
the strength with which the propositions are believed to be true), (a, A)
should not be just used as another way of expressing the strength of the
support in favor of a (the larger A, the stronger the support), but rather as
a piece of information linking a proposition with a group of agents.
Multiple agent logic has two inference rules:
• if B ⊆A then (a, A) ⊢(a, B) (subset weakening)
• (¬a ∨b, A), (a, A) ⊢(b, A), ∀A ∈2ALL \ ∅(modus ponens)
As a consequence, we also have the resolution rule
if A ∩B ̸= ∅, then (¬a ∨b, A), (a ∨c, B) ⊢(b ∨c, A ∩B).
If A ∩B = ∅, the information resulting from applying the rule does not
belong to the language, and would make little sense: it is of no use to
put formulas of the form (a, ∅) in a base as it corresponds to information
possessed by no agent.
Since 2ALL is not totally ordered as in the case of certainty levels, we
cannot “slice” a multiple agent knowledge base Γ = {(ai, Ai), i = 1, . . . , m}
into layers as in basic possibilistic logic. Still, one can deﬁne the restriction
of Γ to a subset A ⊆All as
ΓA = {(ai, Ai ∩A) | Ai ∩A ̸= ∅and (ai, Ai) ∈Γ}.

POSSIBILISTIC LOGIC — AN OVERVIEW
319
Moreover, an inconsistency subset of agents can be deﬁned for Γ as
inc-s(Γ) =
[
{A ⊆All | Γ ⊢(⊥, A)} and inc-s(Γ) = ∅if ∄A s.t. Γ ⊢(⊥, A).
Note that in this deﬁnition A = ∅is not forbidden. For instance, let Γ =
{(p, A), (q, B), (¬p ∨q, C), (¬q, D)}, then inc-s(Γ) = (A ∩C ∩D) ∪(B ∩D),
and obviously inc-s(ΓA∩B∩C∩D) = ∅.
Clearly, it is not the case that the consistency of Γ (inc-s(Γ) = ∅) implies
that Γ◦is consistent. This feature contrasts with possibilistic logic. Just
consider the example Γ = {(a, A), (¬a, A)}, then inc-s(Γ) = A∩A = ∅while
Γ◦= {ai | (ai, Ai) ∈Γ, i = 1, . . . , m} is inconsistent. This is compatible
with situations where agents contradict each other. Yet, the consistency of
Γ◦does entail inc-s(Γ) = ∅.
The semantics of ma-L is expressed in terms of set-valued possibility
distributions, set-valued possibility measures and set-valued necessity mea-
sures. Namely, the semantics of formula (a, A) is given by set-valued distri-
bution π{(a,A)}:
∀ω ∈Ω, π{(a,A)}(ω) =
 All
if ω |= a
Ac
if ω |= ¬a
where Ac = All \ A, and the formula (a, A) is understood as expressing the
constraints N(a) ⊇A where N is a set-valued necessity measure. Soundness
and completeness results can be established with respect to this semantics
[Belhadi et al., 2013].
Basic possibilistic logic and multiple agent logic may then be combined
in a possibilistic multiple agent logic. Formulas are pairs (a, F) where F is
now a fuzzy subset of All. One may in particular consider the fuzzy sets
F = (α/A) such that (α/A)(k) = α if k ∈A, and (α/A)(k) = 0 if k ∈A,
i.e., we restrict ourselves to formulas of the form (a, α/A) that encode the
piece of information “at least all agents in A believe a at least at level α”.
Then the resolution rule becomes
(¬p ∨q, α/A); (p ∨r, β/B) ⊢(q ∨r, min(α, β)/(A ∩B)).
5.2
Uses of the strong possibility set function in possibilistic
logic
As recalled in subsection 2.2, a possibility distribution can be associated not
only with the increasing set functions Π and N, but also with the decreasing
set functions ∆and ∇.
As we are going to see, this enables a double
reading, from above and from below, of a possibility distribution.
This

320
DIDIER DUBOIS AND HENRI PRADE
double reading may be of interest in preference representation by allowing
the use of diﬀerent but equivalent representation formats. Moreover a ∆-
based possibilistic logic, handling formulas associated with lower bounds of
a ∆-set function, can be developed. This is reviewed ﬁrst.
The last part of this subsection is devoted to a diﬀerent use of ∆-set func-
tions. Namely, the modeling of bipolar information. Then one distinguishes
between positive information (expressed by means of ∆-based possibilistic
logic formulas) and negative information (expressed by means of N-based
possibilistic logic formulas), these two types of information being associated
with two distinct possibility distributions.
Double reading of a possibility distribution
In basic possibilistic
logic, a base ΓN = {(ai, αi), i = 1, ..., m} is semantically associated with
the possibility distribution πN
Γ (ω) = mini=1,...,m max([ai](ω), 1 −αi), where
[ai] is the characteristic function of the models of ai. As being the result
of a min-combination, this corresponds to a reading “from above” of the
possibility distribution.
Let us consider another type of logical formula (now denoted between
brackets rather than parentheses) as a pair [b, β], expressing the constraint
∆(b) ≥β, where ∆is a guaranteed or strong possibility measure. Then, a
∆-base Γ∆= {[bj, βj] | j = 1, . . . , n} is associated to the distribution
π∆
Γ (ω) =
max
j=1,...,n π[bj,βi](ω) with π[bj,βi](ω) =
 βj if ω ∈[bj]
0 otherwise.
As being the result of a max-combination, this corresponds to a reading
“from below” of the possibility distribution. It can be proved [Dubois et
al., 2014b] that the N-base ΓN = {(ai, αi) | i = 1, . . . , m} is semantically
equivalent to the ∆-base
Γ∆= {[∧i∈Jai, min
k̸∈J (1 −αk)] : J ⊆{1, . . . , m}}.
Although it looks like the translated knowledge base is exponentially
larger than the original one, it can be simpliﬁed. Indeed, suppose, without
loss of generality that 1 = α1 > α2 > · · · > αm and αm+1 = 0 by convention
(we combine conjunctively all formulas with the same level). Then it is easy
to check that,
max
J⊆{1,...,m} min(min
k̸∈J (1−αk), [∧j∈Jaj](ω)) =
max
k=1,...,m+1 min(1−αk, [∧k−1
j=1aj](ω))
which corresponds to the ∆-base
Γ∆= {[∧k−1
j=1aj, 1 −αk] : k = 1, . . . , m + 1},

POSSIBILISTIC LOGIC — AN OVERVIEW
321
with ∧0
j=1aj = ⊤(tautology). Of course, likewise the ∆-base Γ∆= {[bj, βj] |
j = 1, . . . , n} is semantically equivalent to the N-base
ΓN = {(∨j∈Jbj, max
k̸∈J (1 −βk)) : J ⊆{1, . . . , n}},
which can be simpliﬁed as
ΓN = {(∨k−1
j=1bj, 1 −βk) : k = 1, . . . , n + 1},
with β1 > β2 > · · · > βn > βn+1 = 0, ∨0
j=1ψj = ⊥(contradiction), by
convention.
Thus, a possibilistic logic base Γ∆expressed in terms of a strong possi-
bility measure can always be rewritten equivalently in terms of a standard
possibilistic logic base ΓN using necessity measures and conversely, enforc-
ing the equality πN
Γ = π∆
Γ . The transformation from πN
Γ to π∆
Γ corresponds
to writing the min-max expression of πN
Γ as a max-min expression (applying
the distributivity of min over max) and conversely. This is now illustrated
on a preference example.
Preference representation
As already emphasized, possibilistic logic
applies to the representation of both knowledge and preferences . In case of
preferences, the level α associated a formula a in (a, α) is understood as a
priority.
EXAMPLE 7. Thus, a piece of preference such as “I prefer p to q and q
to r” (where p, q, r may not be mutually exclusive) can be represented
by the possibilistic base ΓN = {(p ∨q ∨r, 1), (p ∨q, 1 −γ), (p, 1 −β)}
with γ < β < 1, by translating the preference into a set of more or less
imperative goals. Namely, Γ states that p is somewhat imperative, that
p ∨q is more imperative, and that p ∨q ∨r is compulsory.
Note that
the preferences are here expressed negatively: “nothing is possible out-
side p, q, or r”, “nothing is really possible outside p, or q”, and “noth-
ing is strongly possible outside p”.
The possibilistic base ΓN is associ-
ated with the possibility distribution πN
Γ which rank-orders the alterna-
tives:
πN
Γ (pqr) = 1, πN
Γ (p¬qr) = 1, πN
Γ (pq¬r) = 1, πN
Γ (p¬q¬r) = 1,
πN
Γ (¬pqr) = β, πN
Γ (¬pq¬r) = β, πB(¬p¬qr) = γ, πN
Γ (¬p¬p¬r) = 0.
From this possibility distribution, one can compute the associated mea-
sure of strong possibility for some events of interest:
∆(p) = min(πN
Γ (pqr), πN
Γ (p¬qr), πN
Γ (pq¬r), πN
Γ (p¬b¬r)) = 1
∆(q) = min(πN
Γ (pqr), πN
Γ (¬pqr), πN
Γ (pq¬r), πN
Γ (¬pq¬r)) = β

322
DIDIER DUBOIS AND HENRI PRADE
∆(r) = min(πN
Γ (pqr), πN
Γ (¬pqr), πN
Γ (p¬qr), πN
Γ (¬p¬qr)) = γ.
It gives birth to the positive base
Γ∆= {[p, 1], [q, β], [r, γ]},
itself associated with a possibility distribution
π∆
Γ (pqr) = 1, π∆
Γ (p¬qr) = 1, π∆
Γ (pq¬r) = 1, π∆
Γ (p¬q¬r) = 1,
π∆
Γ (¬pqr) = β, π∆
Γ (¬pq¬r) = β, π∆
Γ (¬p¬qr) = γ, π∆
Γ (¬p¬q¬r) = 0.
It can be checked that πN
Γ = π∆
Γ . Thus, the preferences are here equiv-
ently expressed in a positive manner as a “weighted” disjunction of the three
choices p, q and r, stating that p is fully satisfactory, q is less satisfactory,
and that r is still less satisfactory.
□
This shows that the preferences here can be equivalently encoded under
the form of the positive base Γ∆, or of the negative base ΓN [Benferhat et
al., 2001d]. Let us mention the representational equivalence [Benferhat et
al., 2004a] between qualitative choice logic [Brewka et al., 2004; Benferhat
and Sedki, 2008] and ∆-based possibilistic logic, which can be viewed itself
as a kind of DNF-like counterpart of standard (CNF-like) possibilistic logic
at the representation level.
The above ideas have been applied to preference queries to databases
[Bosc et al., 2010; Dubois and Prade, 2013] for modeling the connectives
“and if possible” and “or at least” in queries. Besides, it has been shown
that the behavior of Sugeno integrals, a well-known family of qualitative
multiple criteria aggregation operators, can be described under the form
of possibilistic logic bases (of the N-type, or of the ∆-type) [Dubois et al.,
2014b]. It is also possible to represent preferences with an additive structure
in the possibilistic setting thanks to appropriate fusion operators as noticed
in [Prade, 2009].
Inference in ∆-based possibilistic logic
While in basic possibilistic
logic formulas, the certainty level assesses the certainty that the interpre-
tations violating the formulas are excluded as possible worlds, ∆-based for-
mulas rather express to what extent the models of the formulas are actually
possible in the real world. This is a consequence of the decreasingness of set
functions ∆, which leads to a non standard behavior with respect to infer-
ence. Indeed, the following cut rule can be established [Dubois et al., 2000;
Dubois and Prade, 2004] (using the notation of ∆-based formulas):
[a ∧b, α], [¬a ∧c, β] ⊢[b ∧c, min(α, β)]

POSSIBILISTIC LOGIC — AN OVERVIEW
323
This is due to the fact that in terms of models, we have [b∧c] ⊆[a∧b]∪[¬a∧
c]. Thus, if both any model of [a ∧b] any model of [¬a ∧c] are satisfactory,
it should be also the case of any model of [b ∧c]. Moreover, there is also
an inference rule mixing strong possibility and weak necessity, established
in [Dubois et al., 2013a]:
∆([a ∧b]) ≥α and ∇([¬a ∧c]) ≥β entails ∇([b ∧c]) ≥α ∗β
where α ∗β = α if α > 1 −β and α ∗β = 0 if 1 −β ≥α.
Besides, it has been advocated in [Casali et al., 2011; Dubois et al., 2013a]
that desires obey the characteristic postulate of set functions ∆, namely
∆(a ∨b) = min(∆(a), ∆(b)). Indeed, all the models of a ∨b are satisfactory
(or desirable), if both all the models of a and all the models of b are actually
satisfactory.
Then desiring a amounts to ﬁnd satisfactory any situation where a is
true. However, this may be a bit too strong since there may exist some
exceptional situations that are not satisfactory although a is true.
This
calls for a nonmonotonic treatment of desires in terms of ∆function. This
is outlined in [Dubois et al., 2014a].
Bipolar representation
The representation capabilities of possibilistic
logic are suitable for expressing bipolar information [Dubois and Prade,
2006; Benferhat et al., 2008]. Indeed this setting allows the representation
of both negative information and positive information. The bipolar setting
is of interest for representing observations and knowledge, or for representing
positive and negative preferences. Negative information reﬂects what is not
(fully) impossible and thus remains potentially possible (non impossible).
It induces constraints restricting where the real world is (when expressing
knowledge), or delimiting the potentially satisfactory choices (when dealing
with preferences).
Negative information can be encoded by basic (i.e., necessity-based) pos-
sibilistic logic formulas. Indeed, (a, α) encodes N(a) ≥α, which is equiva-
lent to Π(¬a) ≤1 −α, and thus reﬂects the impossibility of ¬a, which is all
the stronger as α is high. Positive information expressing what is actually
possible, or what is really desirable, is encoded by ∆-based formulas [b, β],
which expresses the constraint ∆(b) ≥β. Positive information and nega-
tive information are not necessarily provided by the same sources: in other
words, they may rely on two diﬀerent possibility distributions.
The modeling of beliefs and desires provides another example where two
possibility distributions are needed, one for restricting the more or less plau-
sible states of the world according to the available knowledge, another for

324
DIDIER DUBOIS AND HENRI PRADE
describing the more or less satisfactory states according to the expressed
desires [Dubois et al., 2013a]
Fusion operations can be deﬁned at the semantic and at the syntactic
level in the bipolar setting [Benferhat and Kaci, 2003; Benferhat et al.,
2006].
The fusion of the negative part of the information is performed
by using the formulas of subsection 4.2 for basic possibilistic logic. Their
counterpart for positive information is
Γ∆
1⊕2 =

{[ai, αi ⊕0]
s.t. [ai, αi] ∈Γ∆
1 }
∪
{[bj, 0 ⊕βj]
s.t. [bj, βj] ∈Γ∆
2 }
∪
{[ai ∧bj, αi ⊕βj]
s.t. [ai, αi] ∈Γ∆
1 , [bj, βj] ∈Γ∆
2 },
while πΓ∆
1⊕2 = πΓ∆
1 ⊕π∆∆
2 .
This may be used for aggregating positive (together with negative) pref-
erences given by diﬀerent agents who state what would be really satisfactory
for them (and what they reject more or less strongly). This may also be used
for combining positive (together with negative) knowledge. Then positive
knowledge is usually made of reported cases that testify what is actually
possible, while negative knowledge excludes what is (more or less certainly)
impossible.
A consistency condition is natural between positive and negative infor-
mation, namely what is actually possible (positive information) should be
included in what is not impossible (complement of the negative informa-
tion). Since positive information is combined disjunctively (the more pos-
itive information we have, the more the interpretations that are actually
possible), and negative information conjunctively in a fusion process (the
more negative information we have, the less the worlds that are non impos-
sible), this consistency condition should be enforced in the result. This can
be done by a revision step that gives priority either to the negative side (in
general when handling preferences, where rejections are more important), or
to the positive side (it may apply for knowledge when reliable observations
are conﬂicting with general beliefs) [Dubois et al., 2001b].
5.3
Generalized possibilistic logic
In basic possibilistic logic, only conjunctions of possibilistic logic formulas
are allowed (since a conjunction is equivalent to the conjunction of its con-
juncts, due to the min-decomposability of necessity measures). However,
the negation and the disjunction of possibilistic logic formulas make sense
as well. Indeed, the pair (a, α) is both a possibilistic logic formula at the

POSSIBILISTIC LOGIC — AN OVERVIEW
325
object level, and a classical formula at the meta level. Since (a, α) is seman-
tically interpreted as N(a) ≥α, a possibilistic formula can be manipulated
as a formula that is true (if N(a) ≥α) or false (if N(a) < α). Then pos-
sibilistic formulas can be combined with all propositional connectives. We
are then in the realm of generalized possibilistic logic (GPL) [Dubois and
Prade, 2011a], ﬁrst suggested in [Dubois and Prade, 2007]. Note that for
disjunction, the set of possibility distributions representing the disjunctive
constraint ‘N(a) ≥α or N(b) ≥β’ has no longer a unique extremal element
in general, as it is the case for conjunction. Thus the semantics of GPL
is in terms of set of possibility distributions rather than given by a unique
possibility distribution as in basic possibilistic logic.
More precisely GPL is a two-tier propositional logic, in which proposi-
tional formulas are encapsulated by modal operators that are interpreted in
terms of uncertainty measures from possibility theory. Let Sk = {0, 1
k, 2
k,...,1}
with k ∈N \ {0} be the ﬁnite set of certainty degrees under consideration,
and let S+
k = Sk \ {0}. Let L be the language of all propositional formulas.
The language of GPL Lk
N with k + 1 certainty levels is as follows:
• If a ∈L and α ∈S+
k , then Nα(a) ∈Lk
N.
• If ϕ ∈Lk
N and ψ ∈Lk
N, then ¬ϕ and ϕ ∧ψ are also in Lk
N.
Here we use the notation Nα(a), instead of (a, α), emphasizing the close-
ness with modal logic calculus and allowing the introduction of other associ-
ated modalities. So, an agent asserting Nα(a) has an epistemic state π such
that N(a) ≥α > 0. Hence ¬Nα(a) stands for N(a) < α, which, given the
ﬁniteness of the set of considered certainty degrees, means N(a) ≤α−1
k and
thus Π(¬a) ≥1−α+ 1
k. Let ν(α) = 1−α+ 1
k. Then, ν(α) ∈S+
k iﬀα ∈S+
k ,
and ν(ν(α)) = α, ∀α ∈S+
k .
Thus, we can write Πα(a) ≡¬Nν(α)(¬a).
Thus, in GPL, one can distinguish between the absence of certainty that
a is true (¬Nα(a)) and the (stronger) certainty statement that a is false
(Nα(¬a)).
The semantics of GPL is deﬁned in terms of normalized possibility dis-
tributions over propositional interpretations, where possibility degrees are
limited to Sk. A model of a GPL formula is any Sk-valued possibility dis-
tribution which satisﬁes:
• π is a model of Nα(a) iﬀN(a) ≥α;
• π is a model of ϕ1 ∧ϕ2 iﬀπ is a model of ϕ1 and a model of ϕ2;
• π is a model of ¬ϕ iﬀπ is not a model of ϕ;

326
DIDIER DUBOIS AND HENRI PRADE
where N is the necessity measure induced by π. As usual, π is called a
model of a set of GPL formulas K, written π |= K, if π is a model of each
formula in K. We write K |= Φ, for K a set of GPL formulas and Φ a GPL
formula, iﬀevery model of K is also a model of Φ.
The soundness and completeness of the following axiomatization of GPL
has been established with respect to the above semantics [Dubois et al.,
2012; Dubois et al., 2014c]:
(PL) The Hilbert axioms of classical logic
(K) Nα(a →b) →(Nα(a) →Nα(b))
(N) N1(⊤)
(D) Nα(a) →Π1(a)
(W) Nα1(a) →Nα2(a), if α1 ≥α2
with modus ponens as the only inference rule.
Note in particular that when α is ﬁxed we get a fragment of the modal
logic KD. See [Herzig and Fari˜nas del Cerro, 1991; Dubois et al., 1988;
Liau and Lin, 1992; Dubois et al., 2000] for previous studies of the links
between modal logics and possibility theory. The case where k = 1 coincides
with the Meta-Epistemic Logic (MEL) that was introduced by Banerjee
and Dubois [2009; 2014]. This simpler logic, a fragment of KD with no
nested modalities nor objective formulas, can express full certainty and
full ignorance only and its semantics is in terms of non-empty subsets of
interpretations. Moreover, an extension of MEL [Banerjee et al., 2014] to
a language containing modal formulas of depth 0 or 1 only has been shown
to be in some sense equivalent to S5 with a restricted language, but with
the same expressive power, the semantics being based on pairs made of an
interpretation (representing the real world) and a non-empty set of possible
interpretations (representing an epistemic state). Note that in MEL, we
have Π1(a) ≡¬N1(¬a) whereas in general we only have Π1(a) ≡¬N 1
k (¬a).
GPL is suitable for reasoning about the revealed beliefs of another agent.
It captures the idea that while the consistent epistemic state of an agent
about the world is represented by a normalized possibility distribution over
possible worlds, the meta-epistemic state of another agent about the for-
mer’s epistemic state is a family of possibility distributions.
Modalities associated with set functions ∆and ∇can also be introduced
in the GPL language [Dubois et al., 2014c]. For a propositional interpreta-
tion ω let us write conjω for the conjunction of all literals made true by ω,
i.e. conjω = V
ω|=a a ∧V
ω|=¬a ¬a. Since ∆(a) = minω∈[a] Π({ω}), we deﬁne:

POSSIBILISTIC LOGIC — AN OVERVIEW
327
∆α(a) =
^
ω∈[a]
Πα(conjω) ;
∇α(a) = ¬∆ν(α)(¬α)
Using the modality ∆, for any possibility distribution π over the set of
interpretations Ω, we can easily deﬁne a GPL theory which has π as its only
model [Dubois et al., 2014c]. In particular, let a1, ..., ak be propositional
formulas such that [ai] = {ω | π(ω) ≥i
k}. Then we deﬁne the theory Φπ as:
Φπ =
k^
i=1
Nν( i
k )(ai) ∧∆i
k (ai).
In this equation, the degree of possibility of each ω ∈[ai] is deﬁned by
inequalities from above and from below. Indeed, ∆i
k (ai) means that π(ω) ≥
i
k for all ω ∈[ai], whereas, Nν( i
k )(ai) means π(ω) ≤i−1
k
for all ω /∈[ai]. It
follows that π(ω) = 0 if ω /∈[a1], π(ω) = i
k if ω ∈[ai] \ [ai+1] (for i < k)
and π(ω) = 1 if ω ∈[ak]. In other words, π is indeed the only model of
Φπ. If we view the epistemic state of an agent as a possibility distribution,
this means that every epistemic state can be modeled using a GPL theory.
Conceptually, the construction of Φπ relates to the notion of “only knowing”
from Levesque [1990]. See [Dubois et al., 2014c] for a detailed study.
Another remarkable application of generalized possibilistic logic is its
capability to encode any Answer Sets Programs , choosing S+
2 = {1/2, 1}.
In this case, we can discriminate between propositions in which we are fully
certain and propositions which we consider to be more plausible than not.
This is suﬃcient to enable us to capture the semantics of rules (with negation
as failure) within GPL. See [Dubois et al., 2011] for the introduction of
basic ideas in a possibility theory and approximate reasoning perspective,
and [Dubois et al., 2012] for theoretical results (including the encoding of
equilibrium logic [Pearce, 2006]).
In GPL modalities cannot be nested. Still, it seems possible to give a
meaning in the possibility theory setting to a formula of the form ((a, α), β).
Its semantics, viewing (a, α) as a true or false statement, is given by a pos-
sibility distribution over the possibility distributions π such that π ≤π(a,α)
(that makes N(a) ≥α true) and all the other possibility distributions,
with respective weights 1 and 1 −β.
This may reduce to one possibil-
ity distribution corresponding to the semantics of (a, min(α, β)), via the
disjunctive weighted aggregation max(min(π(a,α), 1), min(1, 1 −β), which
expresses that either it is the case that N(a) ≥α with a possibility level
equal to 1, or one knows nothing with possibility 1 −β. Nested modalities
are in particular of interest for expressing mutual beliefs of multiple agents.

328
DIDIER DUBOIS AND HENRI PRADE
This suggests to hybridize GPL with possibilistic multiple agent logic, and
to study if the Booleanization of possibilistic formulas may give us the capa-
bility of expressing mutual beliefs between agents in a proper way, as well as
validating inferences with nested modalities such that (¬(a, 1), α), ((a, 1) ∨
b, β) ⊢(b, min(α, β)), following ideas suggested in [Dubois and Prade, 2007;
Dubois and Prade, 2011a].
Other possibility theory-based logical formalisms have been developed,
which are at least as expressible as GPL, but based on fuzzy logic [H´ajek
et al., 1995; H´ajek, 1998], instead of keeping a Boolean view of possibilistic
formulas as in GPL. Moreover, these formalisms have been extended to
cope with fuzzy propositions as well [Flaminio et al., 2011; Flaminio et al.,
2011b; Flaminio et al., 2012], and may allow for nested modalities [H´ajek
et al., 1994; Bou et al., 2014], or propose a fuzzy modal logic of possibilistic
conditionals [Marchioni, 2006]. See also [Liau, 1998; Liau and Lin, 1996]
for other possibilistic logic formalisms (over classical logic propositions). A
careful comparison of GPL with these diﬀerent formalisms is still to be done.
However, the distinctive feature of basic possibilistic logic, as well as of GPL,
is to remain as close as possible to classical logic, which makes possibilistic
logic simple to handle, and should also have computational advantages.
6
CONCLUSION
Possibilistic logic is thirty years old. Although related to the idea of fuzzy
sets through possibility measures, possibilistic logic departs from other fuzzy
logics [Dubois et al., 2007], since it primarily focuses on classical logic for-
mulas pervaded with qualitative uncertainty. Indeed basic possibilistic logic,
as well as generalized possibilistic logic remain close to classical logic, but
still allow for a sophisticated and powerful treatment of modalities.
The chapter is an attempt at oﬀering a broad overview of the basic ideas
underlying the possibilistic logic setting, through the richness of its rep-
resentation formats, and its various applications to many AI problems, in
relation with the representation of epistemic states and their handling when
reasoning from and about them. In that respect possibilistic logic can be
compared to other approaches including nonmonotonic logics, modal logics,
or Bayesian nets.
Directions for further research in possibilistic logic includes theoretical
issues and application concerns. On the theoretical side, extensions to non-
classical logics [Besnard and Lang, 1994], to the handling of fuzzy pred-
icates [Dellunde et al., 2011; El-Zekey and Godo, 2012], to partially or-

POSSIBILISTIC LOGIC — AN OVERVIEW
329
dered sets of logical formulas [Cayrol et al., 2014] are worth continuing,
relations with conditional logics [Halpern, 2005; Lewis, 1973; H´ajek, 1998]
worth investigating. On the applied side, it seems that the development
of eﬃcient implementations, of applications to information systems, and of
extensions of possibilistic logic to multiple agent settings and to argumen-
tation [Ches˜nevar et al., 2005; Alsinet et al., 2008; Nieves and Cort´es, 2006;
Godo et al., 2012; Amgoud and Prade, 2012] would be of particular interest.
ACKNOWLEGMENTS
Some people have been instrumental in the development of possibilistic logic
over three decades. In that respect, particular thanks are especially due to
Salem Benferhat, Souhi Kaci, J´erˆome Lang, and Steven Schockaert. The au-
thors wish also to thank Leila Amgoud, Mohua Banerjee, Philippe Besnard,
Claudette Cayrol, Florence Dupin de Saint-Cyr, Luis Fari˜nas del Cerro,
Henri Farreny, H´el`ene Fargier, Dov Gabbay, Lluis Godo, Andy Herzig, Tony
Hunter, Sebastian Link, Weiru Liu, Olivier Pivert, Agn`es Rico, R´egis Sab-
badin, Mary-Anne Williams, and LotﬁZadeh for discussion, encouragement
and support over the years.
BIBLIOGRAPHY
[Alsinet and Godo., 2000] T. Alsinet and L. Godo. A complete calculus for possibilistic
logic programming with fuzzy propositional variables. In Proc. 16th Conf. on Uncer-
tainty in Artiﬁcial Intelligence (UAI’00), Stanford, Ca., pages 1–10, San Francisco,
2000. Morgan Kaufmann.
[Alsinet et al., 2002] T. Alsinet, L. Godo, and S. Sandri. Two formalisms of extended
possibilistic logic programming with context-dependent fuzzy uniﬁcation: a compar-
ative description. Elec. Notes in Theor. Computer Sci., 66 (5), 2002.
[Alsinet et al., 2008] T. Alsinet, C. I. Ches nevar, and L. Godo. A level-based approach
to computing warranted arguments in possibilistic defeasible logic programming. In
Ph. Besnard, S. Doutre, and A. Hunter, editors, Proc. 2nd. Inter. Conf. on Compu-
tational Models of Argument (COMMA’08), Toulouse, May 28-30, pages 1–12. IOS
Press, 2008.
[Amgoud and Prade, 2009] L. Amgoud and H. Prade. Using arguments for making and
explaining decisions. Artiﬁcial Intelligence, 173:413–436, 2009.
[Amgoud and Prade, 2012] L. Amgoud and H. Prade. Towards a logic of argumentation.
In E. H¨ullermeier, S. Link, Th. Fober, and B. Seeger, editors, Proc. 6th Int. Conf. on
Scalable Uncertainty Management (SUM’12), Marburg, Sept. 17-19, volume 7520 of
LNCS, pages 558–565. Springer, 2012.
[Banerjee and Dubois, 2009] M. Banerjee and D. Dubois. A simple modal logic for rea-
soning about revealed beliefs. In C. Sossai and G. Chemello, editors, Proc. 10th Europ.
Conf. on Symbolic and Quantitative Approaches to Reasoning with Uncertainty (EC-
SQARU), Verona, July 1-3, number 5590 in LNCS, pages 805–816. Springer, 2009.

330
DIDIER DUBOIS AND HENRI PRADE
[Banerjee and Dubois, 2014] M. Banerjee and D. Dubois. A simple logic for reasoning
about incomplete knowledge. Int. J. of Approximate Reasoning, 55:639–653, 2014.
[Banerjee et al., 2014] M. Banerjee, D. Dubois, and L. Godo. Possibilistic vs. relational
semantics for logics of incomplete information. In A. Laurent, O. Strauss, B. Bouchon-
Meunier, and R. R. Yager, editors, Proc. 15th Int. Conf. on Information Processing
and Management of Uncertainty in Knowledge-Based Systems (IPMU’14), Part I,
Montpellier, July 15-19, volume 442 of Comm. in Comp. and Inf. Sci., pages 335–
344. Springer, 2014.
[Bauters et al., 2010] K. Bauters, S. Schockaert, M. De Cock, and D. Vermeir. Possi-
bilistic answer set programming revisited.
In P. Gr¨unwald and P. Spirtes, editors,
Proc. 26th Conf. on Uncertainty in Artiﬁcial Intelligence (UAI’10), Catalina Island,
July 8-11, pages 48–55. AUAI Press, 2010.
[Bauters et al., 2011] K. Bauters, S. Schockaert, M. De Cock, and D. Vermeir. Weak and
strong disjunction in possibilistic ASP. In S. Benferhat and J. Grant, editors, Proc. 5th
Int. Conf. on Scalable Uncertainty Management (SUM’11), Dayton, October 10-13,
volume 6929 of LNCS, pages 475–488. Springer, 2011.
[Bauters et al., 2012] K. Bauters, S. Schockaert, M. De Cock, and D. Vermeir. Possible
and necessary answer sets of possibilistic answer set programs. In Proc. I 24th EEE
Int. Conf. on Tools with Artiﬁcial Intelligence (ICTAI’12), Athens, Nov. 7-9, pages
836–843, 2012.
[Belhadi et al., 2013] A. Belhadi, D. Dubois, F. Khellaf-Haned, and H. Prade. Multiple
agent possibilistic logic. J. of Applied Non-Classical Logics, 23:299–320, 2013.
[Belnap, 1977] N. D. Belnap. A useful four-valued logic. In J. M. Dunn and G. Epstein,
editors, Modern Uses of Multiple-Valued Logic, pages 7–37. D. Reidel, Dordrecht,
1977.
[Ben Amor and Benferhat, 2005] N. Ben Amor and S. Benferhat. Graphoid properties
of qualitative possibilistic independence relations. Int. J. Uncertainty, Fuzziness &
Knowledge-based Syst., 13:59–97, 2005.
[Ben Amor et al., 2002] N. Ben Amor, S. Benferhat, D. Dubois, K. Mellouli, and
H. Prade.
A theoretical framework for possibilistic independence in a weakly or-
dered setting. Int. J. Uncertainty, Fuzziness & Knowledge-based Syst., 10:117–155,
2002.
[Ben Amor et al., 2003] N. Ben Amor, S. Benferhat, and K. Mellouli. Anytime prop-
agation algorithm for min-based possibilistic graphs.
Soft Comput., 8(2):150–161,
2003.
[Ben Amor et al., 2014] N. Ben Amor, D. Dubois, H. Gouider, and H. Prade. Possi-
bilistic networks: A new setting for modeling preferences. In U. Straccia and A. Cali,
editors, Proc. 8th Int. Conf. on Scalable Uncertainty Management (SUM 2014), Ox-
ford, Sept. 15-17, volume 8720 of LNCS, pages 1–7. Springer, 2014.
[Benferhat and Bouraoui, 2013] S. Benferhat and Z. Bouraoui. Possibilistic DL-Lite. In
W.r. Liu, V. S. Subrahmanian, and J. Wijsen, editors, Proc. 7th Int. Conf. on Scalable
Uncertainty Management (SUM’13), Washington, DC, Sept. 16-18, volume 8078 of
LNCS, pages 346–359. Springer, 2013.
[Benferhat and Kaci, 2003] S. Benferhat and S. Kaci. Logical representation and fusion
of prioritized information based on guaranteed possibility measures: Application to
the distance-based merging of classical bases. Artiﬁcial Intelligence, 148(1-2):291–333,
2003.
[Benferhat and Prade, 2005] S. Benferhat and H. Prade. Encoding formulas with par-
tially constrained weights in a possibilistic-like many-sorted propositional logic. In
L. Pack Kaelbling and A. Saﬃotti, editors, Proc. of the 9th Inter. Joint Conf. on
Artiﬁciel Intelligence (IJCAI’05), Edinburgh, July 30-Aug. 5, pages 1281–1286, 2005.
[Benferhat and Prade, 2006] S. Benferhat and H. Prade. Compiling possibilistic knowl-
edge bases. In G. Brewka, S. Coradeschi, A. Perini, and P. Traverso, editors, Proc.

POSSIBILISTIC LOGIC — AN OVERVIEW
331
17th Europ. Conf. on Artiﬁcial Intelligence (ECAI’06), Riva del Garda, Aug. 29 -
Sept. 1, pages 337–341. IOS Press, 2006.
[Benferhat and Sedki, 2008] S. Benferhat and K. Sedki. Two alternatives for handling
preferences in qualitative choice logic. Fuzzy Sets and Systems, 159(15):1889–1912,
2008.
[Benferhat and Smaoui, 2007a] S. Benferhat and S. Smaoui. Hybrid possibilistic net-
works. Int. J. Approx. Reasoning, 44(3):224–243, 2007.
[Benferhat and Smaoui, 2007b] S. Benferhat and S. Smaoui.
Possibilistic causal net-
works for handling interventions: A new propagation algorithm. In Proc. 22nd AAAI
Conf. on Artiﬁcial Intelligence (AAAI’07), Vancouver, July 22-26,, pages 373–378,
2007.
[Benferhat and Smaoui, 2011] S. Benferhat and S. Smaoui.
Inferring interventions in
product-based possibilistic causal networks. Fuzzy Sets and Systems, 169:26–50, 2011.
[Benferhat and Titouna, 2005] S. Benferhat and F. Titouna. Min-based fusion of pos-
sibilistic networks. In E. Montseny and P. Sobrevilla, editors, Proc. 4th Conf. of the
Europ. Soc. for Fuzzy Logic and Technology (EUSFLAT’05), Barcelona, Sept. 7-9,
pages 553–558. Universidad Polytecnica de Catalunya, 2005.
[Benferhat and Titouna, 2009] S. Benferhat and F. Titouna. Fusion and normalization
of quantitative possibilistic networks. Applied Intelligence, 31(2):135–160, 2009.
[Benferhat et al., 1992] S. Benferhat, D. Dubois, and H. Prade. Representing default
rules in possibilistic logic.
In Proc. 3rd Inter. Conf. on Principles of Knowledge
Representation and Reasoning (KR’92), Cambridge, Ma, Oct. 26-29, pages 673–684,
1992.
[Benferhat et al., 1993a] S. Benferhat, D. Dubois, and H. Prade. Argumentative infer-
ence in uncertain and inconsistent knowledge base. In Proc. 9th Conf. on Uncertainty
in Artiﬁcial Intelligence, Washington, DC, July 9-11, pages 411–419. Morgan Kauf-
mann, 1993.
[Benferhat et al., 1993b] S. Benferhat, D. Dubois, and H. Prade.
Possibilistic logic:
From nonmonotonicity to logic programming. In M. Clarke, R. Kruse, and S. Moral,
editors, Proc. Europ. Conf. on Symbolic and Quantitative Approaches to Reasoning
and Uncertainty (ECSQARU’93), Granada, Nov. 8-10, volume 747 of LNCS, pages
17–24. Springer, 1993.
[Benferhat et al., 1994a] S. Benferhat, D. Dubois, J. Lang, and H. Prade. Hypothetical
reasoning in possibilistic logic: basic notions and implementation issues. In P. Z. Wang
and K. F. Loe, editors, Between Mind and Computer, Fuzzy Science and Engineering,
pages 1–29. World Scientiﬁc Publ., Singapore, 1994.
[Benferhat et al., 1994b] S. Benferhat, D. Dubois, and H. Prade. Expressing indepen-
dence in a possibilistic framework and its application to default reasoning. In Proc.
11th Europ. Conf. on Artiﬁcial Intelligence (ECAI’94), Amsterdam, Aug. 8-12, pages
150–154, 1994.
[Benferhat et al., 1997a] S. Benferhat, T. Chehire, and F. Monai. Possibilistic ATMS
in a data fusion problem. In D. Dubois, H. Prade, and R.R. Yager, editors, Fuzzy
Information Engineering: A Guided Tour of Applications, pages 417–435. John Wiley
& Sons, New York, 1997.
[Benferhat et al., 1997b] S. Benferhat, D. Dubois, and H. Prade. Nonmonotonic reason-
ing, conditional objects and possibility theory. Artiﬁcial Intelligence, 92(1-2):259–276,
1997.
[Benferhat et al., 1998a] S. Benferhat, D. Dubois, and H. Prade.
From semantic to
syntactic approaches to information combination in possibilistic logic. In B. Bouchon-
Meunier, editor, Aggregation and Fusion of Imperfect Information, pages 141–161.
Physica-Verlag, Heidelberg, 1998.

332
DIDIER DUBOIS AND HENRI PRADE
[Benferhat et al., 1998b] S. Benferhat, D. Dubois, and H. Prade. Practical handling of
exception-tainted rules and independence information in possibilistic logic. Applied
Intelligence, 9(2):101–127, 1998.
[Benferhat et al., 1999a] S. Benferhat, D. Dubois, and H. Prade.
An overview of
inconsistency-tolerant inferences in prioritized knowledge bases. In D. Dubois, E. P.
Klement, and H. Prade, editors, Fuzzy Sets, Logic and Reasoning about Knowledge,
volume 15 of Applied Logic Series, pages 395–417. Kluwer, Dordrecht, 1999.
[Benferhat et al., 1999b] S. Benferhat, D. Dubois, and H. Prade. Possibilistic and stan-
dard probabilistic semantics of conditional knowledge bases. J. of Logic and Compu-
tation, 9(6):873–895, 1999.
[Benferhat et al., 1999c] S. Benferhat, D. Dubois, H. Prade, and M.-A. Williams.
A
practical approach to fusing prioritized knowledge bases.
In Proc. 9th Portuguese
Conf. on Artiﬁcial Intelligence (EPIA’99), Evora, Sept. 21-24, volume 1695 of LNCS,
pages 222–236. Springer, 1999.
[Benferhat et al., 2000a] S. Benferhat, D. Dubois, H. Fargier, H. Prade, and R. Sab-
badin. Decision, nonmonotonic reasoning and possibilistic logic. In J. Minker, editor,
Logic-Based Artiﬁcial Intelligence, pages 333–358. Kluwer Acad. Publ., 2000.
[Benferhat et al., 2000b] S. Benferhat, D. Dubois, and H. Prade. Kalman-like ﬁltering
in a possibilistic setting. In W. Horn, editor, Proc.14th Europ. Conf. on Artiﬁcial
Intelligence (ECAI’00), Berlin, Aug. 20-25, pages 8–12. IOS Press, 2000.
[Benferhat et al., 2001a] S. Benferhat, D. Dubois, S. Kaci, and H.Prade. Bridging logi-
cal, comparative and graphical possibilistic representation frameworks. In S. Benfer-
hat and P. Besnard, editors, Proc. 6th Europ. Conf. on Symbolic and Quantitative
Approaches to reasoning with Uncertainty (ECSQARU’01), Toulouse, Sept. 19-21,
volume 2143 of LNAI, pages 422–431. Springer, 2001.
[Benferhat et al., 2001b] S. Benferhat, D. Dubois, S. Kaci, and H. Prade.
Graphical
readings of a possibilistic logic base. In J. Breese and D. Koller, editors, Proc. 17th
Conf. on Uncertainty in Artiﬁcial Intelligence (UAI’01), Seattle, Aug. 2-5, pages
24–31. Morgan Kaufmann, 2001.
[Benferhat et al., 2001c] S. Benferhat, D. Dubois, and H. Prade. A computational model
for belief change and fusing ordered belief bases.
In M.-A. Williams and H. Rott,
editors, Frontiers in Belief Revision, pages 109–134. Kluwer Acad. Publ., 2001.
[Benferhat et al., 2001d] S. Benferhat, D. Dubois, and H. Prade. Towards a possibilistic
logic handling of preferences. Applied Intelligence, 14(3):303–317, 2001.
[Benferhat et al., 2002a] S. Benferhat, D. Dubois, L. Garcia, and H. Prade.
On the
transformation between possibilistic logic bases and possibilistic causal networks. Int.
J. Approx. Reasoning, 29(2):135–173, 2002.
[Benferhat et al., 2002b] S. Benferhat, D. Dubois, S. Kaci, and H. Prade. Possibilistic
merging and distance-based fusion of propositional information. Annals of Mathemat-
ics and Artiﬁcial Intelligence, 34(1-3):217–252, 2002.
[Benferhat et al., 2002c] S. Benferhat, D. Dubois, H. Prade, and M.-A. Williams.
A
practical approach to revising prioritized knowledge bases. Studia Logica, 70(1):105–
130, 2002.
[Benferhat et al., 2003] S. Benferhat, R. El Baida, and F. Cuppens. A possibilistic logic
encoding of access control. In I. Russell and S. M. Haller, editors, Proc. 16th Int.
Florida Artiﬁcial Intelligence Research Society Conf., St. Augustine, Fl., May 12-14,
pages 481–485. AAAI Press, 2003.
[Benferhat et al., 2004a] S. Benferhat, G. Brewka, and D. Le Berre.
On the relation
between qualitative choice logic and possibilistic logic. In Proc. 10th Inter. Conf. In-
formation Processing and Management of Uncertainty in Knowledge-Based Systems
(IPMU 04), July 4-9, Perugia, pages 951–957, 2004.

POSSIBILISTIC LOGIC — AN OVERVIEW
333
[Benferhat et al., 2004b] S. Benferhat, S. Lagrue, and O. Papini. Reasoning with par-
tially ordered information in a possibilistic logic framework. Fuzzy Sets and Systems,
144(1):25–41, 2004.
[Benferhat et al., 2005] S. Benferhat, F. Khellaf, and A. Mokhtari. Product-based causal
networks and quantitative possibilistic bases. Int. J. of Uncertainty, Fuzziness and
Knowledge-Based Systems, 13:469–493, 2005.
[Benferhat et al., 2006] S. Benferhat, D. Dubois, S. Kaci, and H. Prade. Bipolar pos-
sibility theory in preference modeling: Representation, fusion and optimal solutions.
Information Fusion, 7(1):135–150, 2006.
[Benferhat et al., 2008] S. Benferhat, D. Dubois, S. Kaci, and H. Prade. Modeling pos-
itive and negative information in possibility theory.
Int. J. of Intelligent Systems,
23(10):1094–1118, 2008.
[Benferhat et al., 2009] S. Benferhat, D. Dubois, and H. Prade. Interventions in possi-
bilistic logic. In L. Godo and A. Pugliese, editors, Proc. 3rd Int. Conf. on Scalable
Uncertainty Management (SUM’09), Washington, DC, Sept. 28-30, volume 5785 of
LNCS, pages 40–54. Springer, 2009.
[Benferhat et al., 2010] S. Benferhat, D. Dubois, H. Prade, and M.-A. Williams.
A
framework for iterated belief revision using possibilistic counterparts to Jeﬀrey’s rule.
Fundam. Inform., 99(2):147–168, 2010.
[Benferhat et al., 2011] S. Benferhat, J. Hu´e, S. Lagrue, and J. Rossit. Interval-based
possibilistic logic. In T. Walsh, editor, Proc. 22nd Inter. Joint Conf. on Artiﬁcial
Intelligence (IJCAI’11), Barcelona, July 16-22,, pages 750–755, 2011.
[Benferhat et al., 2013] S. Benferhat, Z. Bouraoui, and Z. Loukil. Min-based fusion of
possibilistic DL-Lite knowledge bases. In Proc. IEEE/WIC/ACM Int. Conf. on Web
Intelligence (WI’13), Atlanta, GA Nov. 17-20, pages 23–28. IEEE Computer Society,
2013.
[Benferhat, 2010] S. Benferhat. Interventions and belief change in possibilistic graphical
models. Artiﬁcial Intelligence, 174:177–189, 2010.
[Besnard and Hunter, 1995] Ph. Besnard and A. Hunter.
Quasi-classical logic: Non-
trivializable classical reasoning from inconsistent information. In Ch. Froidevaux and
J. Kohlas, editors, Proc. 3rd Europ. Conf. Symbolic and Quantitative Approaches to
Reasoning and Uncertainty (ECSQARU’95), Fribourg, July 3-5, volume 946 of LNCS,
pages 44–51. Springer, 1995.
[Besnard and Lang, 1994] Ph. Besnard and J. Lang. Possibility and necessity functions
over non-classical logics. In R. L´opez de M´antaras and D. Poole, editors, Proc. 10th
Conf. on Uncertainty in Artiﬁcial Intelligence (UAI’94), Seattle, July 29-31, pages
69–76. Morgan Kaufmann, 1994.
[Boche´nski, 1947] I. M. Boche´nski. La Logique de Th´eophraste. Librairie de l’Universit´e
de Fribourg en Suisse, 1947.
[Boldrin and Sossai, 1997] L. Boldrin and C. Sossai.
Local possibilistic logic.
J. of
Applied Non-Classical Logics, 7(3):309–333, 1997.
[Boldrin, 1995] L. Boldrin.
A substructural connective for possibilistic logic.
In Ch.
Froidevaux and J. Kohlas, editors, Proc. 3rd Europ. Conf. on Symbolic and Quantita-
tive Approaches to Reasoning and Uncertainty (ECSQARU-95), Fribourg, July 3-5,
volume 946 of LNCS, pages 60–68. Springer, 1995.
[Bonnefon et al., 2008] J.-F. Bonnefon, R. Da Silva Neves, D. Dubois, and H. Prade.
Predicting causality ascriptions from background knowledge: model and experimental
validation. Int. J. Approximate Reasoning, 48(3):752–765, 2008.
[Bonnefon et al., 2012] J.-F. Bonnefon, R. Da Silva Neves, D. Dubois, and H. Prade.
Qualitative and quantitative conditions for the transitivity of perceived causation -
Theoretical and experimental results. Ann. Math. Artif. Intell., 64(2-3):311–333, 2012.

334
DIDIER DUBOIS AND HENRI PRADE
[Bosc and Pivert, 2005] P. Bosc and O. Pivert. About projection-selection-join queries
addressed to possibilistic relational databases. IEEE Trans. on Fuzzy Systems, 13:124–
139, 2005.
[Bosc et al., 2009] P. Bosc, O. Pivert, and H. Prade.
A model based on possibilistic
certainty levels for incomplete databases. In L. Godo and A. Pugliese, editors, Proc.
3rd Int. Conf. on Scalable Uncertainty Management (SUM’09), Washington, DC,
Sept. 28-30, volume 5785 of LNCS, pages 80–94. Springer, 2009.
[Bosc et al., 2010] P. Bosc, O. Pivert, and H. Prade. A possibilistic logic view of pref-
erence queries to an uncertain database.
In Proc. 19th IEEE Int. Conf. on Fuzzy
Systems (FUZZ-IEEE’10), Barcelona, July 18-23, pages 379–384, 2010.
[Bou et al., 2014] F. Bou, F. Esteva, and L. Godo. On possibilistic modal logics deﬁned
over MTL-chains.
In Petr H´ajek on Mathematical Fuzzy Logic, Trends in Logic,
Springer, in press, 2014.
[Brewka et al., 2004] G. Brewka, S. Benferhat, and D. Le Berre. Qualitative choice logic.
Artiﬁcial Intelligence, 157(1-2):203–237, 2004.
[Brewka et al., 2011] G. Brewka, V. Marek, and M. Truszczynski, eds. Nonmonotonic
Reasoning. Essays Celebrating its 30th Anniversary., volume 31 of Studies in Logic.
College Publications, 2011.
[Buchanan and Shortliﬀe, 1984] B. G. Buchanan and E. H. Shortliﬀe, editors.
Rule-
Based Expert Systems. The MYCIN Experiments of the Stanford Heuristic Program-
ming Project. Addison-Wesley, Reading, Ma., 1984.
[Casali et al., 2011] A. Casali, L. Godo, and C. Sierra. A graded BDI agent model to
represent and reason about preferences. Artiﬁcial Intelligence, 175(7-8):1468–1478,
2011.
[Cayrol et al., 2014] C. Cayrol, D. Dubois, and F. Touazi.
On the semantics of par-
tially ordered bases. In Ch. Beierle and C. Meghini, editors, Proc. 8th Int. Symp.
on Foundations of Information and Knowledge Systems (FoIKS’14), Bordeaux, Mar.
3-7, volume 8367 of LNCS, pages 136–153. Springer, 2014.
[Chellas, 1980] B. F. Chellas.
Modal Logic, an Introduction.
Cambridge University
Press, Cambridge, 1980.
[Ches˜nevar et al., 2005] C. I. Ches˜nevar, G. R. Simari, L. Godo, and T. Alsinet.
Argument-based expansion operators in possibilistic defeasible logic programming:
Characterization and logical properties.
In L. Godo, editor, Proc. 8th Europ.
Conf. on Symbolic and Quantitative Approaches to Reasoning with Uncertainty (EC-
SQARU’05), Barcelona, July 6-8, volume 3571 of LNCS, pages 353–365. Springer,
2005.
[Cohen, 1977] L. J. Cohen. The Probable and the Provable. Clarendon Press, Oxford,
1977.
[Coletti and Vantaggi, 2009] G. Coletti and B. Vantaggi.
T-conditional possibilities:
Coherence and inference. Fuzzy Sets and Systems, 160(3):306–324, 2009.
[Confalonieri and Prade, 2014] R. Confalonieri and H. Prade. Using possibilistic logic
for modeling qualitative decision: Answer set programming algorithms. Int. J. Ap-
proximate Reasoning, 55(2):711–738, 2014.
[Confalonieri et al., 2012] R. Confalonieri, J. C. Nieves, M. Osorio, and J. V´azquez-
Salceda. Dealing with explicit preferences and uncertainty in answer set programming.
Ann. Math. Artif. Intell., 65(2-3):159–198, 2012.
[De Baets et al., 1999] B. De Baets, E. Tsiporkova, and R. Mesiar.
Conditioning in
possibility with strict order norms. Fuzzy Sets and Systems, 106:221–229, 1999.
[De Campos and Huete, 1999] L. M. De Campos and J. F. Huete. Independence con-
cepts in possibility theory. Fuzzy Sets and Systems, 103:127–152 & 487–506, 1999.
[De Cooman, 1997] G. De Cooman. Possibility theory. Part i: Measure- and integral-
theoretic groundwork; Part ii: Conditional possibility; Part iii: Possibilistic indepen-
dence. Int. J. of General Syst., 25:291–371, 1997.

POSSIBILISTIC LOGIC — AN OVERVIEW
335
[Dellunde et al., 2011] P. Dellunde, L. Godo, and E. Marchioni. Extending possibilistic
logic over g¨odel logic. Int. J. Approx. Reasoning, 52(1):63–75, 2011.
[Dubois and Prade, 1980] D. Dubois and H. Prade. Fuzzy Sets and Systems - Theory
and Applications. Academic Press, New York, 1980.
[Dubois and Prade, 1988] D. Dubois and H. Prade. Possibility Theory. An Approach to
Computerized Processing of Uncertainty. Plenum Press, New York and London, 1988.
With the collaboration of H. Farreny, R. Martin-Clouaire and C. Testemale.
[Dubois and Prade, 1990a] D. Dubois and H. Prade. The logical view of conditioning
and its application to possibility and evidence theories. Int. J. Approx. Reasoning,
4(1):23–46, 1990.
[Dubois and Prade, 1990b] D. Dubois and H. Prade. Resolution principles in possibilis-
tic logic. Int. J. Approximate Reasoning, 4(1):1–21, 1990.
[Dubois and Prade, 1991] D. Dubois and H. Prade. Epistemic entrenchment and possi-
bilistic logic. Artiﬁcial Intelligence, 50:223–239, 1991.
[Dubois and Prade, 1992] D. Dubois and H. Prade.
Possibility theory as a basis for
preference propagation in automated reasoning. In Proc. 1st IEEE Inter. Conf. on
Fuzzy Systems (FUZZ-IEEE’92), San Diego, Ca., March 8-12, pages 821–832, 1992.
[Dubois and Prade, 1993] D. Dubois and H. Prade. Belief revision and updates in nu-
merical formalisms: An overview, with new results for the possibilistic framework. In
R. Bajcsy, editor, Proc. 13th Int. Joint Conf. on Artiﬁcial Intelligence. Chamb´ery,
Aug. 28 - Sept. 3, pages 620–625. Morgan Kaufmann, 1993.
[Dubois and Prade, 1995a] D. Dubois and H. Prade.
Conditional objects, possibility
theory and default rules. In L. Fari˜nas Del Cerro G. Crocco and A. Herzig, editors,
Conditionals: From Philosophy to Computer Science, Studies in Logic and Compu-
tation, pages 301–336. Oxford Science Publ., 1995.
[Dubois and Prade, 1995b] D. Dubois and H. Prade. Possibility theory as a basis for
qualitative decision theory. In Proc. 14th Int. Joint Conf. on Artiﬁcial Intelligence
(IJCAI’95), Montr´eal , Aug. 20-25, pages 1924–1932. Morgan Kaufmann, 1995.
[Dubois and Prade, 1996] D. Dubois and H. Prade. Combining hypothetical reasoning
and plausible inference in possibilistic logic. J. of Multiple Valued Logic, 1:219–239,
1996.
[Dubois and Prade, 1997a] D. Dubois and H. Prade.
A synthetic view of belief revi-
sion with uncertain inputs in the framework of possibility theory.
Int. J. Approx.
Reasoning, 17:295–324, 1997.
[Dubois and Prade, 1997b] D. Dubois and H. Prade. Valid or complete information in
databases - a possibility theory-based analysis.
In A. Hameurlain and A.M. Tjoa,
editors, Database and Expert Systems Applications, Proc. of the 8th Inter. Conf.
DEXA’97, Toulouse, Sept. 1-5, volume 1308 of LNCS, pages 603–612. Springer, 1997.
[Dubois and Prade, 1998] D. Dubois and H. Prade. Possibility theory: Qualitative and
quantitative aspects. In D. M. Gabbay and Ph. Smets, editors, Quantiﬁed Represen-
tation of Uncertainty and Imprecision, volume 1 of Handbook of Defeasible Reasoning
and Uncertainty Management Systems, pages 169–226. Kluwer Acad. Publ., 1998.
[Dubois and Prade, 2000] D. Dubois and H. Prade. An overview of ordinal and numer-
ical approaches to causal diagnostic problem solving. In D.M. Gabbay and R. Kruse,
editors, Abductive Reasoning and Learning, Vol. 4 in Handbooks of Defeasible Rea-
soning and Uncertainty Management Systems, pages 231–280. Kluwer Acad. Publ.,
Boston, 2000.
[Dubois and Prade, 2004] D. Dubois and H. Prade. Possibilistic logic: A retrospective
and prospective view. Fuzzy Sets and Systems, 144:3–23, 2004.
[Dubois and Prade, 2006] D. Dubois and H. Prade. A bipolar possibilistic representa-
tion of knowledge and preferences and its applications.
In I. Bloch, A. Petrosino,
A. Tettamanzi, and G. B. Andrea, editors, Revised Selected Papers from the Inter.

336
DIDIER DUBOIS AND HENRI PRADE
Workshop on Fuzzy Logic and Applications (WILF’05), Crema, Italy, Sept. 2005,
volume 3849 of LNCS, pages 1–10. Springer, 2006.
[Dubois and Prade, 2007] D. Dubois and H. Prade. Toward multiple-agent extensions
of possibilistic logic. In Proc. IEEE Inter. Conf. on Fuzzy Systems (FUZZ-IEEE’07),
London, July 23-26, pages 187–192, 2007.
[Dubois and Prade, 2011a] D. Dubois and H. Prade. Generalized possibilistic logic. In
S. Benferhat and J. Grant, editors, Proc. 5th Int. Conf. on Scalable Uncertainty
Management (SUM’11), Dayton, Oh, Oct. 10-13, volume 6929 of LNCS, pages 428–
432. Springer, 2011.
[Dubois and Prade, 2011b] D. Dubois and H. Prade. Handling various forms of inconsis-
tency in possibilistic logic. In F. Morvan, A. Min Tjoa, and R. Wagner, editors, Proc.
2011 Database and Expert Systems Applications, DEXA, Int. Workshops, Toulouse,
Aug. 29 - Sept. 2, pages 327–331. IEEE Computer Society, 2011.
[Dubois and Prade, 2011c] D. Dubois and H. Prade. Non-monotonic reasoning and un-
certainty theories. In G. Brewka, V. Marek, and M. Truszczynski, editors, Nonmono-
tonic Reasoning. Essays Celebrating its 30th Anniversary, volume 31 of Studies in
Logic, pages 141–176. College Publications, 2011.
[Dubois and Prade, 2012] D. Dubois and H. Prade. From Blanch´e’s hexagonal organiza-
tion of concepts to formal concept analysis and possibility theory. Logica Universalis,
6 (1-2):149–169, 2012.
[Dubois and Prade, 2013] D. Dubois and H. Prade.
Modeling “and if possible” and
“or at least”: Diﬀerent forms of bipolarity in ﬂexible querying.
In O. Pivert and
S. Zadrozny, editors, Flexible Approaches in Data, Information and Knowledge Man-
agement, volume 497 of Studies in Computational Intelligence, pages 3–19. Springer,
2013.
[Dubois et al., 1987] D. Dubois, J. Lang, and H. Prade. Theorem proving under un-
certainty - A possibility theory-based approach. In J. P. McDermott, editor, Proc.
10th Int. Joint Conf. on Artiﬁcial Intelligence. Milan, Aug., pages 984–986. Morgan
Kaufmann, 1987.
[Dubois et al., 1988] D. Dubois, H. Prade, and C. Testemale. In search of a modal system
for possibility theory. In Y. Kodratoﬀ, editor, Proc. 8th Europ. Conf. on Artiﬁcial
Intelligence (ECAI’88), Munich, Aug. 1-5, pages 501–506, London: Pitmann Publ.,
1988.
[Dubois et al., 1990] D. Dubois, J. Lang, and H. Prade. Handling uncertain knowledge
in an ATMS using possibilistic logic. In Proc. 5th Inter. Symp. on Methodologies for
Intelligent Systems, Knoxville, Oct. 25-27, pages 252–259. North-Holland, 1990.
[Dubois et al., 1991a] D. Dubois, J. Lang, and H. Prade.
Fuzzy sets in approximate
reasoning. Part 2: Logical approaches. Fuzzy Sets and Systems, 40:203–244, 1991.
[Dubois et al., 1991b] D. Dubois, J. Lang, and H. Prade. Timed possibilistic logic. Fun-
damenta Informaticae, 15:211–234, 1991.
[Dubois et al., 1991c] D. Dubois, J. Lang, and H. Prade.
Towards possibilistic logic
programming. In K. Furukawa, editor, Proc. 8th Int. Conf. on Logic Programming
(ICLP’91), Paris, June 24-28, 1991, pages 581–595. MIT Press, 1991.
[Dubois et al., 1992] D. Dubois, J. Lang, and H. Prade. Dealing with multi-source in-
formation in possibilistic logic. In B. Neumann, editor, Proc. 10th Europ. Conf. on
Artiﬁcial Intelligence (ECAI’92), Vienna, Aug. 3-7, pages 38–42. IEEE Computer
Society, 1992.
[Dubois et al., 1994a] D. Dubois, J. Lang, and H. Prade. Automated reasoning using
possibilistic logic: semantics, belief revision and variable certainty weights.
IEEE
Trans. on Data and Knowledge Engineering, 6(1):64–71, 1994.
[Dubois et al., 1994b] D. Dubois, J. Lang, and H. Prade. Handling uncertainty, context,
vague predicates, and partial inconsistency in possibilistic logic. In D. Driankov, P. W.

POSSIBILISTIC LOGIC — AN OVERVIEW
337
Eklund, and A. L. Ralescu, editors, Fuzzy Logic and Fuzzy Control, Proc. IJCAI ’91
Workshop, Sydney, Aug. 24, 1991, volume 833 of LNCS, pages 45–55. Springer, 1994.
[Dubois et al., 1994c] D. Dubois, J. Lang, and H. Prade. Possibilistic logic. In D. M.
Gabbay, C. J. Hogger, J. A. Robinson, and D. Nute, editors, Handbook of Logic in
Artiﬁcial Intelligence and Logic Programming, Vol. 3, pages 439–513. Oxford Univ.
Press, 1994.
[Dubois et al., 1997] D. Dubois, L. Fari˜nas del Cerro, A. Herzig, and H. Prade. Qual-
itative relevance and independence: A roadmap. In Proc. 15h Int. Joint Conf. on
Artiﬁcial Intelligence, Nagoya, pages 62–67, 1997.
[Dubois et al., 1998a] D. Dubois, S. Moral, and H. Prade. Belief change rules in ordinal
and numerical uncertainty theories. In D. Dubois and H. Prade, editors, Belief Change,
pages 311–392. Kluwer, Dordrecht, 1998.
[Dubois et al., 1998b] D. Dubois, H. Prade, and S. Sandri.
A possibilistic logic with
fuzzy constants and fuzzily restricted quantiﬁers.
In T. P. Martin and F. Arcelli-
Fontana, editors, Logic Programming and Soft Computing, pages 69–90. Research
Studies Press, Baldock, UK, 1998.
[Dubois et al., 1999a] D. Dubois, L. Fari˜nas del Cerro, A. Herzig, and H. Prade.
A
roadmap of qualitative independence. In D. Dubois, H. Prade, and E. P. Klement,
editors, Fuzzy Sets, Logics and Reasoning about Knowledge , volume 15 of Applied
Logic series, pages 325–350. Kluwer Acad. Publ., Dordrecht, 1999.
[Dubois et al., 1999b] D. Dubois, D. Le Berre, H. Prade, and R. Sabbadin. Using possi-
bilistic logic for modeling qualitative decision: ATMS-based algorithms. Fundamenta
Informaticae, 37(1-2):1–30, 1999.
[Dubois et al., 2000] D. Dubois, P. Hajek, and H. Prade. Knowledge-driven versus data-
driven logics. J. Logic, Language, and Information, 9:65–89, 2000.
[Dubois et al., 2001a] D. Dubois, H. Prade, and R. Sabbadin. Decision-theoretic founda-
tions of qualitative possibility theory. Europ. J. of Operational Research, 128(3):459–
478, 2001.
[Dubois et al., 2001b] D. Dubois, H. Prade, and Ph. Smets. “Not impossible” vs. “guar-
anteed possible” in fusion and revision. In S. Benferhat and Ph. Besnard, editors, Proc.
6th Europ. Conf. Symbolic and Quantitative Approaches to Reasoning with Uncer-
tainty (ECSQARU’01), Toulouse, Sept. 19-21, volume 2143 of LNCS, pages 522–531.
Springer, 2001.
[Dubois et al., 2003] D. Dubois, S. Konieczny, and H. Prade. Quasi-possibilistic logic
and its measures of information and conﬂict. Fundamenta Informaticae, 57(2-4):101–
125, 2003.
[Dubois et al., 2006] D. Dubois, S. Kaci, and H. Prade. Approximation of conditional
preferences networks ?CP-nets? in possibilistic logic. In Proc. IEEE Int. Conf. on
Fuzzy Systems (FUZZ-IEEE’06), Vancouver, July 16-21, pages 2337– 2342, 2006.
[Dubois et al., 2007] D. Dubois, F. Esteva, L. Godo, and H. Prade.
Fuzzy-set based
logics - An history-oriented presentation of their main developments. In D. M. Gabbay
and J. Woods, editors, Handbook of the History of Logic, Vol. 8, The Many-Valued
and Nonmonotonic Turn in Logic, pages 325–449. Elsevier, 2007.
[Dubois et al., 2011] D. Dubois, H. Prade, and S. Schockaert. Rules and metarules in the
framework of possibility theory and possibilistic logic. Scientia Iranica, Transactions
D, 18:566–573, 2011.
[Dubois et al., 2012] D. Dubois, H. Prade, and S. Schockaert. Stable models in general-
ized possibilistic logic. In G. Brewka, Th. Eiter, and S. A. McIlraith, editors, Proc. 13th
Int. Conf. Principles of Knowledge Representation and Reasoning (KR’12), Rome,
June 10-14, pages 519–529. AAAI Press, 2012.
[Dubois et al., 2013a] D. Dubois, E. Lorini, and H. Prade. Bipolar possibility theory
as a basis for a logic of desire and beliefs.
In W.r. Liu, V.S. Subramanian, and

338
DIDIER DUBOIS AND HENRI PRADE
J. Wijsen, editors, Proc. Int. Conf. on Scalable Uncertainty Management (SUM’13),
Washington, DC, Sept. 16-18, number 8078 in LNCS, pages 204–218. Springer, 2013.
[Dubois et al., 2013b] D. Dubois, H. Prade, and F. Touazi. Conditional preference nets
and possibilistic logic. In L. C. van der Gaag, editor, Proc. 12th Europ. Conf. on Sym-
bolic and Quantitative Approaches to Reasoning with Uncertainty (ECSQARU’13),
Utrecht, July 8-10, volume 7958 of LNCS, pages 181–193. Springer, 2013.
[Dubois et al., 2013c] D. Dubois, H. Prade, and F. Touazi. Conditional preference-nets,
possibilistic logic, and the transitivity of priorities. In M. Bramer and M. Petridis,
editors, Proc. of AI-2013, the 33rd SGAI Int. Conf. on Innovative Techniques and
Applications of Artiﬁcial Intelligence, Cambridge, UK, Dec. 10-12, pages 175–184.
Springer, 2013.
[Dubois et al., 2014a] D. Dubois, E. Lorini, and H. Prade. Nonmonotonic desires - A
possibility theory viewpoint. In Proc. ECAI Int. Workshop on Defeasible and Am-
pliative Reasoning (DARe’14), Prague, Aug. 19. CEUR, 2014.
[Dubois et al., 2014b] D. Dubois, H. Prade, and A. Rico. The logical encoding of Sugeno
integrals. Fuzzy Sets and Systems, 241:61–75, 2014.
[Dubois et al., 2014c] D. Dubois, H. Prade, and S. Schockaert. Reasoning about uncer-
tainty and explicit ignorance in generalized possibilistic logic. In Proc. 21st Europ.
Conf. on Artiﬁcial Intelligence (ECAI’14), Prague, Aug. 20-22, 2014.
[Dubois, 1986] D. Dubois. Belief structures, possibility theory and decomposable mea-
sures on ﬁnite sets. Computers and AI, 5:403–416, 1986.
[Dubois, 2012] D. Dubois. Reasoning about ignorance and contradiction: many-valued
logics versus epistemic logic. Soft Computing, 16(11):1817–1831, 2012.
[Dupin de Saint Cyr and Prade, 2008] F. Dupin de Saint Cyr and H. Prade. Handling
uncertainty and defeasibility in a possibilistic logic setting. Int. J. Approximate Rea-
soning, 49(1):67–82, 2008.
[Dupin de Saint Cyr et al., 1994] F. Dupin de Saint Cyr, J. Lang, and Th. Schiex.
Penalty logic and its link with Dempster-Shafer theory.
In R. Lopez de Mantaras
and D. Poole, editors, Proc. Annual Conf. on Uncertainty in Artiﬁcial Intelligence
(UAI’94), Seattle, July 29-31, pages 204–211. Morgan Kaufmann, 1994.
[El-Zekey and Godo, 2012] M. El-Zekey and L. Godo. An extension of G¨odel logic for
reasoning under both vagueness and possibilistic uncertainty. In S. Greco, B. Bouchon-
Meunier, G. Coletti, M. Fedrizzi, B. Matarazzo, and R. R. Yager, editors, Proc. 14th
Int. Conf. on Information Processing and Management of Uncertainty in Knowledge-
Based Systems (IPMU’12), Part II, Catania, July 9-13, volume 298 of Comm. in
Comp. and Inf. Sci., pages 216–225. Springer, 2012.
[Fari˜nas del Cerro, 1985] L. Fari˜nas del Cerro. Resolution modal logic. Logique et Anal-
yse, 110-111:153–172, 1985.
[Flaminio et al., 2011] T. Flaminio, L. Godo, and E. Marchioni. On the logical formal-
ization of possibilistic counterparts of states over n-valued  Lukasiewicz events. J. Log.
Comput., 21(3): 429–446, 2011.
[Flaminio et al., 2011b] T. Flaminio, L. Godo and E. Marchioni. Reasoning about un-
certainty of fuzzy events: an overview. In P. Cintula, C. G. Ferm¨uller, L. Godo and
P. H´ajek, editors, Understanding Vagueness - Logical, Philosophical, and Linguistic
Perspectives, Studies in Logic no. 36, London: College Publications, pages 367–401,
2011.
[Flaminio et al., 2012] T. Flaminio, L. Godo, and E. Marchioni. Geometrical aspects of
possibility measures on ﬁnite domain MV-clans. Soft Comput., 16(11): 1863–1873,
2012.
[Gabbay, 1996] D. Gabbay. Labelled Deductive Systems. Volume 1. Oxford University
Press, Oxford, 1996.
[G¨ardenfors, 1988] P. G¨ardenfors. Knowledge in Flux: Modeling the Dynamics of Epis-
temic States. The MIT Press, 1988. 2nd ed., College Publications, 2008.

POSSIBILISTIC LOGIC — AN OVERVIEW
339
[G¨ardenfors, 1990] P. G¨ardenfors. Belief revision and nonmonotonic logic: Two sides of
the same coin? In L. Aiello, editor, Proc. 9th Europ. Conf. in Artiﬁcial Intelligence
(ECAI’90), Stockholm, Aug. 8-10, pages 768–773, London, 1990. Pitman.
[Godo et al., 2012] L. Godo, E. Marchioni, and P. Pardo. Extending a temporal defea-
sible argumentation framework with possibilistic weights.
In L. Fari˜nas del Cerro,
A. Herzig, and J. Mengin, editors, Proc. 13th Europ. Conf. on Logics in Artiﬁcial In-
telligence (JELIA’12), Toulouse, Sept. 26-28, volume 7519 of LNCS, pages 242–254.
Springer, 2012.
[Grabisch and Prade, 2001] M. Grabisch and H. Prade. The correlation problem in sen-
sor fusion in a possibilistic framework. Int. J. of Intelligent Systems, 16(11):1273–1283,
2001.
[Grabisch, 2003] M. Grabisch. Temporal scenario modelling and recognition based on
possibilistic logic. Artiﬁcial Intelligence, 148(1-2):261–289, 2003.
[Grove, 1988] A. Grove. Two modellings for theory change. J. Philos. Logic, 17:157–170,
1988.
[H´ajek, 1998] P. H´ajek. Metamathematics of Fuzzy Logic, volume 4 of Trends in Logic
– Studia Logica Library. Kluwer Acad. Publ., Dordrecht, 1998.
[H´ajek et al., 1995] P. H´ajek, L. Godo and F. Esteva. Fuzzy logic and probability. In
Ph. Besnard and S. Hanks, editors, Proc. 11th Conf. on Uncertainty in Artiﬁcial
Intelligence (UAI’95), Montreal, Aug. 18-20, pages 237–244, 1995.
[H´ajek et al., 1994] P. H´ajek, D. Harmancov´a, F. Esteva, P. Garcia and L. Godo: On
modal logics for qualitative possibility in a fuzzy setting. In R. Lopez de Mantaras and
D. Poole, editors, Proc. 10th Conf. on Uncertainty in Artiﬁcial Intelligence (UAI’94)
, Seattle, Jul. 29-31, San Francisco: Morgan Kaufmann, pages 278–285, 1994.
[Halpern, 2005] J. Y. Halpern. Reasoning about Uncertainty. MIT Press, Cambridge,
Ma, 2005.
[Herzig and Fari˜nas del Cerro, 1991] A. Herzig and L. Fari˜nas del Cerro. A modal anal-
ysis of possibility theory.
In Ph. Jorrand and J. Kelemen, editors, Fundamentals
of Artiﬁcial Intelligence Research (FAIR’91), Smolenice, Sept. 8-13, volume 535 of
LNCS, pages 11–18. Springer, 1991.
[Hunter, 2000] A. Hunter.
Reasoning with contradictory information using quasi-
classical logic. J. Log. Comput., 10(5):677–703, 2000.
[Jensen, 2001] F. V. Jensen. Bayesian Networks and Graphs. Springer Verlag, 2001.
[Kaci et al., 2000] S. Kaci, S. Benferhat, D. Dubois, and H. Prade. A principled analysis
of merging operations in possibilistic logic. In Proc. 16th Conf. on Uncertainty in
Artiﬁcial (UAI’00), Stanford, June 30 - July 3, pages 24–31, 2000.
[Klement et al., 2000] E. P. Klement, R. Mesiar, and E. Pap.
Triangular Norms.
Springer, 2000.
[Koehler et al., 2014a] H. Koehler, U. Leck, S. Link, and H. Prade. Logical foundations
of possibilistic keys.
In E. Ferm´e and J. Leite, editors, Proc. 14th Europ. Conf.
on Logics in Artiﬁcial Intelligence (JELIA’14), Madeira, Sept. 24-26, LNCS 8761,
Springer, pages 181–195, 2014.
[Koehler et al., 2014b] H. Koehler, S. Link, H. Prade, and X.f. Zhou. Cardinality con-
straints for uncertain data. In Proc. 33rd Int. Conf. on Conceptual Modeling (ER’14),
Atlanta, Oct. 27-29, 2014.
[Kraus et al., 1990] S. Kraus, D. Lehmann, and M. Magidor. Nonmonotonic reasoning,
preferential models and cumulative logics. Artiﬁcial Intelligence, 44:167–207, 1990.
[Lafage et al., 1999] C. Lafage, J. Lang, and R. Sabbadin. A logic of supporters. In
B. Bouchon-Meunier, R. R. Yager, and L. A. Zadeh, editors, Information, Uncertainty
and Fusion, pages 381–392. Kluwer Acad. Publ., 1999.
[Lang et al., 1991] J. Lang, D. Dubois, and H. Prade.
A logic of graded possibility
and certainty coping with partial inconsistency. In B. D’Ambrosio and Ph. Smets,

340
DIDIER DUBOIS AND HENRI PRADE
editors, Proc 7th Annual Conf. on Uncertainty in Artiﬁcial Intelligence (UAI ’91),
Los Angeles, July 13-15, pages 188–196. Morgan Kaufmann, 1991.
[Lang, 1991] J. Lang.
Possibilistic logic as a logical framework for min-max discrete
optimisation problems and prioritized constraints.
In P. Jorrand and J. Kelemen,
editors, Proc. Inter. Workshop on Fundamentals of Artiﬁcial Intelligence Research
(FAIR’91), Smolenice, Sept. 8-13, volume 535 of LNCS, pages 112–126. Springer,
1991.
[Lang, 2001] J. Lang. Possibilistic logic: complexity and algorithms. In D. Gabbay, Ph.
Smets, J. Kohlas, and S. Moral, editors, Algorithms for Uncertainty and Defeasible
Reasoning, Vol. 5 of Handbook of Defeasible Reasoning and Uncertainty Management
Systems, pages 179–220. Kluwer Acad. Publ., Dordrecht, 2001.
[L´ea Somb´e Group (ed.), 1994] L´ea Somb´e Group (ed.). Ph. Besnard, L. Cholvy, M. O.
Cordier, D. Dubois, L. Fari˜nas del Cerro, C. Froidevaux, F. L´evy, Y. Moinard, H.
Prade, C. Schwind, and P. Siegel. Revision and Updating in Knowledge bases. Int.
J. Intelligent Systems, 9(1):1–182, 1994. Also simultaneously published as a book by
John Wiley & Sons, New York.
[L´ea Somb´e Group, 1990] L´ea Somb´e Group.
Ph. Besnard, M. O. Cordier, D. Dubois,
L. Fari˜nas del Cerro, C. Froidevaux, Y. Moinard, H. Prade, C. Schwind, and P. Siegel.
Reasoning under incomplete information in Artiﬁcial Intelligence: A comparison of
formalisms using a single example. Int. J. Intelligent Systems, 5(4):323–472, 1990.
Also simultaneously published as a book by John Wiley & Sons, New York.
[Lehmann and Magidor, 1992] D. Lehmann and M. Magidor. What does a conditional
knowledge base entail? Artiﬁcial Intelligence, 55:1–60, 1992.
[Levesque, 1990] H. J. Levesque. All I know: A study in autoepistemic logic. Artiﬁcial
Intelligence, 42:263–309, 1990.
[Levi, 1966] I. Levi. On potential surprise. Ratio, 8:107–129, 1966.
[Levi, 1967] I. Levi. Gambling with Truth, chapters VIII and IX. Knopf, New York,
1967.
[Levi, 1979] I. Levi. Support and surprise: L. J. Cohen’s view of inductive probability.
Brit. J. Phil. Sci., 30:279–292, 1979.
[Lewis, 1973] D. K. Lewis. Counterfactuals. Basil Blackwell, Oxford, 1973.
[Lewis, 1976] D. K. Lewis. Probabilities of conditionals and conditional probabilities.
Philosophical Review, 85:297–315, 1976.
[Liau, 1998] C.-J. Liau. Possibilistic residuated implication logics with applications. Int.
J. of Uncertainty, Fuzziness and Knowledge-Based Systems 6(4): 365–386, 1998.
[Liau, 1999] C.-J. Liau. On the possibility theory-based semantics for logics of prefer-
ence. Int. J. Approx. Reasoning, 20(2): 173–190,1999.
[Liau and Lin, 1992] C.-J. Liau and B. I-P. Lin.
Quantitative modal logic and pos-
sibilistic reasoning.
In B. Neumann, editor, Proc. 10th Europ. Conf. on Artiﬁcial
Intelligence (ECAI 92), Vienna, Aug. 3-7, pages 43–47, John Wiley and Sons, 1992.
[Liau and Lin, 1996] C.-J. Liau and B. I-P. Lin. Possibilistic reasoning - A mini-survey
and uniform semantics. Artiﬁcial Intelligence 88(1-2): 163–193,1996.
[Liau and Liu, 2001] C.-J. Liau and D.-R. Liu. A possibilistic decision logic with appli-
cations. Fundam. Inform., 46(3): 199–217, 2001.
[Marchioni, 2006] E. Marchioni. Possibilistic conditioning framed in fuzzy logics. Int.
J. Approx. Reasoning, 43(2): 133–165, 2006.
[Minker, 2000] J. Minker, editor. Logic-Based Artiﬁcial Intelligence. Kluwer, Dordrecht,
2000.
[Nicolas et al., 2006] P. Nicolas, L. Garcia, I. St´ephan, and C. Lef`evre.
Possibilistic
uncertainty handling for answer set programming. Ann. Math. Artif. Intell., 47(1-
2):139–181, 2006.

POSSIBILISTIC LOGIC — AN OVERVIEW
341
[Nieves and Cort´es, 2006] J. C. Nieves and U. Cort´es. Modality argumentation program-
ming. In V. Torra, Y. Narukawa, A. Valls, and J. Domingo-Ferrer, editors, Proc. 3rd
Inter. Conf. on Modeling Decisions for Artiﬁcial Intelligence (MDAI’06), Tarragona,
Spain, April 3-5, volume 3885 of LNCS, pages 295–306. Springer, 2006.
[Nieves et al., 2007] J. C. Nieves, M. Osorio, and U. Cort´es. Semantics for possibilistic
disjunctive programs. In C. Baral, G. Brewka, and J. S. Schlipf, editors, Proc. 9th Int.
Conf. on Logic Programming and Nonmonotonic Reasoning (LPNMR’07), Tempe,
AZ, May 15-17, volume 4483 of LNCS, pages 315–320. Springer, 2007.
[Parsons, 1997] T. Parsons. The traditional square of opposition. In E. N. Zalta, editor,
The Stanford Encyclopedia of Philosophy. Stanford University, spring 2014 edition,
1997.
[Pearce, 2006] D. Pearce. Equilibrium logic. Annals of Mathematics and Artiﬁcial In-
telligence, 47:3–41, 2006.
[Pearl, 1988] J. Pearl. Probabilistic Reasoning in Intelligent Systems: Networks of Plau-
sible Inference. Morgan Kaufmann Publ., 1988.
[Pearl, 1990] J. Pearl. System Z: A natural ordering of defaults with tractable applica-
tions to nonmonotonic reasoning. In R. Parikh, editor, Proc. 3rd Conf. on Theoretical
Aspects of Reasoning about Knowledge, Paciﬁc Grove, pages 121–135. Morgan Kauf-
mann, 1990.
[Pearl, 2000] J. Pearl. Causality: Models, Reasoning and Inference. Cambridge Univer-
sity Press, 2000. 2nd edition, 2009.
[Pinkas, 1991] G. Pinkas. Propositional non-monotonic reasoning and inconsistency in
symmetric neural networks. In Proc. 12th Int. Joint Conf. on Artiﬁcial Intelligence
(IJCAI’91) - Vol. 1, pages 525–530, San Francisco, 1991. Morgan Kaufmann Publ.
[Pivert and Prade, 2014] O. Pivert and H. Prade. A certainty-based model for uncertain
databases. IEEE Trans. on Fuzzy Systems, 2014. To appear.
[Prade, 2006] H. Prade. Handling (un)awareness and related issues in possibilistic logic:
A preliminary discussion. In J. Dix and A. Hunter, editors, Proc. 11th Int. Workshop
on Non-Monotonic Reasoning (NMR 2006), Lake District, May 30-June 1, pages
219–225. Clausthal Univ. of Techn., 2006.
[Prade, 2009] H. Prade. Current research trends in possibilistic logic: Multiple agent rea-
soning, preference representation, and uncertain database. In Z. W. Ras and A. Dard-
zinska, editors, Advances in Data Management, pages 311–330. Springer, 2009.
[Qi and Wang, 2012] G.l. Qi and K.w. Wang. Conﬂict-based belief revision operators in
possibilistic logic. In J. Hoﬀmann and B. Selman, editors, Proc. 26th AAAI Conf. on
Artiﬁcial Intelligence, Toronto, July 22-26. AAAI Press, 2012.
[Qi et al., 2010a] G.l. Qi, J.f. Du, W.r. Liu, and D. A. Bell. Merging knowledge bases in
possibilistic logic by lexicographic aggregation. In P. Gr¨unwald and P. Spirtes, editors,
UAI 2010, Proc. 26th Conf. on Uncertainty in Artiﬁcial Intelligence, Catalina Island,
July 8-11, pages 458–465. AUAI Press, 2010.
[Qi et al., 2010b] G.l. Qi, W.r. Liu, and D. A. Bell. A comparison of merging operators
in possibilistic logic. In Y.x. Bi and M.-A. Williams, editors, Proc. 4th Int. Conf. on
Knowledge Science, Engineering and Management (KSEM’10), Belfast, Sept. 1-3,
volume 6291 of LNCS, pages 39–50. Springer, 2010.
[Qi et al., 2011] G.l. Qi, Q. Ji, J. Z. Pan, and J.f. Du. Extending description logics with
uncertainty reasoning in possibilistic logic. Int. J. Intell. Syst., 26(4), 2011.
[Qi, 2008] G.l. Qi. A semantic approach for iterated revision in possibilistic logic. In
D. Fox and C. P. Gomes, editors, Proc. 23rd AAAI Conf. on Artiﬁcial Intelligence
(AAAI’08), Chicago, July 13-17, pages 523–528. AAAI Press, 2008.
[Rescher, 1976] N. Rescher. Plausible Reasoning. Van Gorcum, Amsterdam, 1976.
[Schiex et al., 1995] T. Schiex, H. Fargier, and G. Verfaillie. Valued constraint satisfac-
tion problems: Hard and easy problems. In Proc. 14th Int. Joint Conf. on Artiﬁcial

342
DIDIER DUBOIS AND HENRI PRADE
Intelligence (IJCAI’95), Montr´eal, Aug. 20-25, Vol.1, pages 631–639. Morgan Kauf-
mann, 1995.
[Schiex, 1992] Th. Schiex. Possibilistic constraint satisfaction problems or “how to han-
dle soft constraints”. In D. Dubois and M. P. Wellman, editors, Proc. 8th Annual
Conf. on Uncertainty in Artiﬁcial Intelligence (UAI’92), Stanford, July 17-19, pages
268–275, 1992.
[Serrurier and Prade, 2007] M. Serrurier and H. Prade. Introducing possibilistic logic in
ILP for dealing with exceptions. Artiﬁcial Intelligence, 171:939–950, 2007.
[Shackle, 1949] G. L. S. Shackle.
Expectation in Economics.
Cambridge University
Press, UK, 1949. 2nd edition, 1952.
[Shackle, 1961] G. L. S. Shackle. Decision, Order and Time in Human Aﬀairs. (2nd
edition), Cambridge University Press, UK, 1961.
[Shackle, 1979] G. L. S. Shackle. Imagination and the Nature of Choice. Edinburgh
University Press, 1979.
[Shafer, 1976] G. Shafer. A Mathematical Theory of Evidence. Princeton Univ. Press,
1976.
[Shortliﬀe, 1976] E. H. Shortliﬀe. Computer-based Medical Consultations MYCIN. El-
sevier, 1976.
[Spohn, 1988] W. Spohn. Ordinal conditional functions: a dynamic theory of epistemic
states. In W. L. Harper and B. Skyrms, editors, Causation in Decision, Belief Change,
and Statistics, volume 2, pages 105–134. Kluwer, 1988.
[Spohn, 2012] W. Spohn. The Laws of Belief: Ranking Theory and Its Philosophical
Applications. Oxford Univ. Press, 2012.
[Walley, 1991] P. Walley. Statistical Reasoning with Imprecise Probabilities. Chapman
and Hall, London, 1991.
[Walley, 1996] P. Walley. Measures of uncertainty in expert systems. Artiﬁcial Intelli-
gence, 83:1–58, 1996.
[Yager and Liu, 2008] R. R. Yager and L. P. Liu, editors.
Classic Works of the
Dempster-Shafer Theory of Belief Functions. Springer Verlag, Heidelberg, 2008.
[Yager, 1983] R. R. Yager. An introduction to applications of possibility theory. Human
Systems Management, 3:246–269, 1983.
[Zadeh, 1978] L. A. Zadeh. Fuzzy sets as a basis for a theory of possibility. Fuzzy Sets
and Systems, 1:3–28, 1978.
[Zadeh, 1979a] L. A. Zadeh. Fuzzy sets and information granularity. In M. M. Gupta,
R. Ragade, and R. R. Yager, editors, Advances in Fuzzy Set Theory and Applications,
pages 3–18. North-Holland, Amsterdam, 1979.
[Zadeh, 1979b] L. A. Zadeh.
A theory of approximate reasoning.
In J. E. Hayes,
D. Mitchie, and L. I. Mikulich, editors, Machine intelligence, Vol. 9, pages 149–194.
Ellis Horwood, 1979.
[Zadeh, 1982] L. A. Zadeh. Possibility theory and soft data analysis. In L. Cobb and
R. Thrall, editors, Mathematical Frontiers of Social and Policy Sciences, pages 69–
129. Westview Press, Boulder, Co., 1982.
[Zhu et al., 2013] J.f. Zhu, G.l. Qi, and B. Suntisrivaraporn. Tableaux algorithms for
expressive possibilistic description logics. In Proc. IEEE/WIC/ACM Int. Conf. on
Web Intelligence (WI’13), Atlanta, Nov.17-20, pages 227–232. IEEE Comp. Soc.,
2013.

COMPUTERISING MATHEMATICAL TEXT
Fairouz Kamareddine, Joe Wells, Christoph Zengler and
Henk Barendregt
Reader: Serge Autexier
1
BACKGROUND AND MOTIVATION
Mathematical texts can be computerised in many ways that capture diﬀering
amounts of the mathematical meaning.
At one end, there is document imag-
ing, which captures the arrangement of black marks on paper, while at the other
end there are proof assistants (e.g., Mizar, Isabelle, Coq, etc.), which capture the
full mathematical meaning and have proofs expressed in a formal foundation of
mathematics. In between, there are computer typesetting systems (e.g., LATEX and
Presentation MathML) and semantically oriented systems (e.g., Content MathML,
OpenMath, OMDoc, etc.). In this paper we advocate a style of computerisation
of mathematical texts which is ﬂexible enough to connect the diﬀerent approaches
to computerisation, which allows various degrees of formalisation, and which is
compatible with diﬀerent logical frameworks (e.g., set theory, category theory,
type theory, etc.) and proof systems. The basic idea is to allow a man-machine
collaboration which weaves human input with machine computation at every step
in the way. We propose that the huge step from informal mathematics to fully
formalised mathematics be divided into smaller steps, each of which is a fully
developed method in which human input is minimal.
Let us consider the following two questions:
1. What is the relationship between the logical foundations of mathematical
reasoning and the actual practice of mathematicians?
2. In what ways can computers support the development and communication
of mathematical knowledge?
1a
Logical Foundations
Our ﬁrst question, of the relationship between the practice of mathematics and
its logical foundations, has been an issue for at least two millennia. Logic was
already inﬂuential in the study and development of mathematics since the time
of the ancient Greeks. One of the main issues was already known by Aristotle,
namely that for a logical/mathematical proposition Φ,
Handbook of the History of Logic. Volume 9: Computational Logic. 
Volume editor: Jörg Siekmann 
Series editors: Dov M. Gabbay and John Woods 
Copyright © 2014 Elsevier BV. All rights reserved.

344
Fairouz Kamareddine, Joe Wells, Christoph Zengler and Henk Barendregt
• given a purported proof of Φ, it is not hard to check whether the argument
really proves Φ, but
• in contrast, if one is asked to ﬁnd a proof of Φ, the search may take a very
long time (or even go forever without success) even if Φ is true.
Aristotle used logic to reason about everything (mathematics, farming, medicine,
law, etc.). A formal logical style of deductive reasoning about mathematics was
introduced in Euclid’s geometry [Heath, 1956].
The 1600s saw a increase in the importance of logic. Researchers like Leib-
niz wanted to use logic to address not just mathematical questions but also
more esoteric questions like the existence of God. In the 1800s, the need for a
more precise style in mathematics arose, because controversial results had ap-
peared in analysis [Kamareddine et al., 2004a]. Some controversies were solved by
Cauchy’s precise deﬁnition of convergence in his Cours d’Analyse [Cauchy, 1821],
others beneﬁted from the more exact deﬁnition of real numbers given by Dedekind
[Dedekind, 1872], while at the same time Cantor was making a tremendous con-
tribution to the formalisation of set theory and number theory [Cantor, 1895;
Cantor, 1897] and Peano was making inﬂuential steps in formalised arithmetic
[Peano, 1889] (albeit without an extensive treatment of logic or quantiﬁcation).
In the last decades of the 1800s, the contributions of Frege made the move
toward formalisation much more serious. Frege found
“. . . the inadequacy of language to be an obstacle; no matter how un-
wieldy the expressions I was ready to accept, I was less and less able,
as the relations became more and more complex, to attain precision”
Based on this understanding of a need for greater preciseness, Frege presented
Begriﬀsschrift [Frege, 1879], the ﬁrst formalisation of logic giving logical concepts
via symbols rather than natural language. “Begriﬀsschrift” is the name both of
the book and of the formal system the book presents. Frege wrote:
“[Begriﬀsschrift’s] ﬁrst purpose, therefore, is to provide us with the
most reliable test of the validity of a chain of inferences and to point
out every presupposition that tries to sneak in unnoticed, so that its
origin can be investigated.”
Later, Frege wrote the Die Grundlagen der Arithmetik and Grundgesetze der
Arithmetik [Frege, 1893; Frege, 1903; van Heijenoort, 1967] where he argued that
mathematics is a branch of logic and described arithmetic in the Begriﬀsschrift.
Grundgesetze was the culmination of Frege’s work on building a formal foundation
for mathematics.
One of the major issues in the logical foundations of mathematics is that the
naive approach of Frege’s Grundgesetze (and Cantor’s earlier set theory) is incon-
sistent. Russell discovered a paradox in Frege’s system (and also in Russell’s own
system) that allows proving a contradiction, from which everything can be proven,
including all the false statements [Kamareddine et al., 2004a]. The need to build

Computerising Mathematical Text
345
logical foundations for mathematics that do not suﬀer from such paradoxes has
led to many diverging approaches. Russell invented a form of type theory which
he used in the famous Principia Mathematica [Whitehead and Russel, 1910–1913].
Others have subsequently introduced many kinds of type theories and modern type
theories are quite diﬀerent from Russell’s [Barendregt et al., 2013]. Brouwer in-
troduced a diﬀerent direction, that of intuitionism. Later, ideas from intuitionism
and type theory were combined, and even extended to cover the power of classical
logic (which Brouwer’s intuitionism rejects). Zermelo followed a diﬀerent direction
in introducing an axiomatisation of set theory [Zermelo, 1908], later extended by
Fraenkel and Skolem to form the well known Zermelo/Fraenkel (ZF) system. In
yet another direction, it is possible to use category theory as a foundation. And
there are other proposed foundations, too many to discuss here.
Despite the variety of possible foundations for mathematics, in practice real
mathematicians do not express their work in terms of a foundation. It seems that
most modern mathematicians tend to think in terms that are compatible with
ZFC (which is ZF extended with the Axiom of Choice), but in practice they al-
most never write the full formal details. And it is quite rare for mathematicians
to do their thinking while regarding a type theory as the foundation, even though
type theories are among the most thoroughly developed logical foundations (in
particular with well developed computer proof software systems). Instead, math-
ematicians write in a kind of common mathematical language (CML) (sometimes
called a mathematical vernacular), for a number of reasons:
• Mathematicians have developed conventional ways of using nouns, adjectives,
verbs, sentences, and larger chunks of text to express mathematical meaning.
However, the existing logical foundations do not address the convenient use
of natural language text to express mathematical meanings.
• Using a foundation requires picking one speciﬁc foundation, and any foun-
dation commits to some number of ﬁxed choices. Such choices include what
kinds of mathematical objects to take as the primitives (e.g., sets, functions,
types, categories, etc.), what kinds of logical rules to use (e.g., “natural de-
duction” vs. “logical deduction”, whether to allow the full power of classical
logic, etc.), what kinds of syntax and semantics to allow for logical propo-
sitions (ﬁrst-order vs. higher-order), etc. Having made some initial choices,
further choices follow, e.g., for a set theory one must then choose the axioms
(Zermelo/Fraenkel, Tarski/Grothendieck, etc.), or for a type theory the kinds
of types and the typing rules (Calculus of Constructions, Martin-L¨of, etc.).
Fixed choices make logical foundations undesirable to use for three reasons:
– Much of mathematics can be built on top of all of the diﬀerent foun-
dations. Hence, committing to a particular foundation would seem to
unnecessarily limit the applicability of mathematical results.
– The details of how to build some mathematical concepts can vary quite
a bit from foundation to foundation. Issues that cause diﬃculty include

346
Fairouz Kamareddine, Joe Wells, Christoph Zengler and Henk Barendregt
how to handle “partial functions”, induction, reasoning modulo equa-
tions, etc. Since these issues can be handled in all foundations, mathe-
maticians tend to see the low-level details of these issues as inessential
and uninteresting, and are not willing to write the low-level details.
– Some mathematics only works for some foundations. Hence, for a math-
ematician to develop the specialised expertise needed to express mathe-
matics in terms of one particular foundation would seem to unnecessar-
ily limit the scope of mathematics he/she could address. A mathemati-
cian is happy to be reassured by a mathematical logician that what they
are doing can be expressed in some foundation, but the mathematician
usually does not care to work out precisely how. Moreover there is no
universal agreement as to which is the best logical foundation.
• In practice, formalising a mathematical text in any of the existing founda-
tions is an extremely time-consuming, costly, and mentally painful activity.
Formalisation also requires special expertise in the particular foundation
used that goes far beyond the ordinary expertise of even extremely good
mathematicians. Furthermore, mathematical texts formalised in any of the
existing foundations are generally structured in a way which is radically
diﬀerent from what is optimal for the human reader’s understanding, and
which is diﬃcult for ordinary mathematicians to use. (Some proof software
systems like Mizar, which is based on Tarski/Grothendieck set theory, at-
tempt to reduce this problem, and partially succeed.) What is a single step
in a usual human-readable mathematical text may turn into a multitude of
smaller steps in a formalised version. New details completely missing from
the human-readable version may need to be woven throughout the entire
text. The original text may need to be reorganised and reordered so radi-
cally that it seems like it is almost turned inside out in the formal version.
So, although mathematics was a driving force for the research in logic in the
19th or 20th century, mathematics and logic have kept a distance from each other.
Practising mathematicians do not use mathematical logic and have for centuries
done most mathematical work outside of the strict boundaries of formal logic.
1b
Computerisation of Mathematical Knowledge
Our second question, of how to use mechanical computers to support mathemat-
ical knowledge, is more recent but is unavoidable since automation and compu-
tation can provide tremendous services to mathematics. There are also extensive
opportunities for combining progress in logic and computerisation not only in
mathematics but also in other areas: bio-informatics, chemistry, music, etc.
Mechanical computers have been used from their beginning for mathematical
purposes. Starting in the 1960s, computers began to play a role in handling not
just computations, but abstract mathematical knowledge. Nowadays, computers
can represent mathematical knowledge in various ways:

Computerising Mathematical Text
347
• Pixel map images of pages of mathematical articles may be stored on the
computer. While useful, it is very diﬃcult for computer programs to access
the semantics of mathematical knowledge presented this way [Autexier et al.,
2010]. Even keyword searching is hard, since OCR (Optical Character Recog-
nition) must be performed and high quality OCR for mathematical texts is
an area with signiﬁcant research challenges rather than a proven technology
(e.g., there is great diﬃculty with matrices [Kanahori et al., 2006]).
• Typesetting systems like LATEX or TEXMACS [van der Hoeven, 2004], can
be used with mathematical texts for editing them and formatting them for
viewing or printing. The document formats of these systems can also be used
for storage and archiving.
Such systems provide good defaults for visual
appearance and allow ﬁne control when needed. They support commonly
needed document structures and allow custom structures to be created, at
least to the extent of being able to produce the correct visual appearance.
Unfortunately, unless the mathematician is amazingly disciplined, the logical
structure of symbolic formulas is not directly represented. Furthermore, the
logical structure of mathematics as embedded in natural language text is not
represented at all. This makes it diﬃcult for computer programs to access
document semantics because fully automated discovery of the semantics of
natural language text still performs too poorly to use in practical systems.
Even human-assisted semi-automated semantic analysis of natural language
is primitive, and we are aware of no such systems with special support for
mathematical text. As a consequence, there is generally no computer support
for checking the correctness of mathematics represented this way or for doing
searching based on semantics (as opposed to keywords).
• Mathematical texts can be written in more semantically oriented document
representations like OpenMath [Abbott et al., 1996] and OMDoc [Kohlhase,
2006], Content MathML [W3C, 2003], etc. There is generally support for
converting from these representations to typesetting systems like LATEX or
Presentation MathML in order to produce readable/printable versions of
the mathematical text. These systems are 1) better than the typesetting
systems at representing the knowledge in a computer-accessible way, and 2)
can represent Some aspects of the semantics of symbolic formulas.
• There are software systems like proof assistants (also called proof check-
ers, these include Coq [Team, 1999–2003], Isabelle [Nipkow et al., 2002],
NuPrL [Constable and others, 1986], Mizar [Rudnicki, 1992], HOL [Gordon
and Melham, 1993], etc.) and automated theorem provers (Boyer-Moore,
Otter, etc.), which we collectively call proof systems.
Each proof system
provides a formal language (based on some foundation of logic and mathe-
matics) for writing/mechanically checking logic, mathematics, and computer
software. Work on computer support for formal foundations began in the
late 1960s with work by de Bruijn on Automath (AUTOmating MATHe-

348
Fairouz Kamareddine, Joe Wells, Christoph Zengler and Henk Barendregt
matics) [Nederpelt et al., 1994]. Automath supported automated checking
of the full correctness of a mathematical text written in Automath’s formal
language. Generally, most proof systems support checking full correctness,
and it is possible in theory (although not easy) for computer programs to
access and manipulate the semantics of the mathematical statements.
Closely related to proof systems, we ﬁnd proof development/planning sys-
tems (e.g., Ωmega [Siekmann et al., 2002; Siekmann et al., 2003] and λClam
[Bundy et al., 1990]) which are mathematical assistant tools that support
proof development in mathematical domains at a user-friendly level of ab-
straction. An additional advantage of these systems is that they focus on
proof planning and hence can provide diﬀerent styles of proof development.
Unfortunately, there are great disadvantages in using proof systems. First,
all of the problems mentioned for logical foundations in section 1a are in-
curred, e.g., the enormous expense of formalisation. Furthermore, one must
choose a speciﬁc proof system (Isabelle, Coq, Mizar, PVS, etc.) and each
software system has its own advantages and pitfalls and takes quite some
time to learn. In practice, some of these systems are only ever learnt from a
“master” in an “apprenticeship” setting. Most proof systems have no mean-
ingful support for the mathematical use of natural language text. A notable
exception is Mizar, which however requires the use of natural language in a
rigid and somewhat inﬂexible way. Most proof systems suﬀer from the use
of proof tactics, which make it easier to construct proofs and make proofs
smaller, but obscure the reasoning for readers because the meaning of each
tactic is often ad hoc and implementation-dependent. As a result of these
and other disadvantages, ordinary mathematicians do not generally read
mathematics written in the language of a proof system, and are usually not
willing to spend the eﬀort to formalise their own work in a proof system.
• Computer algebra systems (CAS: e.g., Maxima, Maple, Mathematica, etc.)
are widely used software environments designed for carrying out computa-
tions, primarily symbolic but sometimes also numeric.
Each CAS has a
language for writing mathematical expressions and statements and for de-
scribing computations.
The languages can also be used for representing
mathematical knowledge. The main advantage for such a language is inte-
gration with a CAS. Typically, a CAS language is not tied to any speciﬁc
foundation and has little or no support for guaranteeing correctness of math-
ematical statements. A CAS language also typically has little or no support
for embedded natural language text, or for precise control over typesetting.
So a CAS is often used for calculating results, but these results are usually
converted into some other language or format for dissemination or veriﬁca-
tion. Nonetheless, there are useful possibilities for using a CAS for archiving
and communicating mathematical knowledge.
It is important to build a bridge between more than one of the above categories
of ways of representing mathematical knowledge, and to make easier (without re-

Computerising Mathematical Text
349
quiring) the partial or full formalisation of mathematical texts in some foundation.
In this paper, we discuss two approaches aimed at achieving this: Barendregt’s
approach [Barendregt, 2003] towards an interactive mathematical proof mode and
Kamareddine and Wells’ MathLang approach [Kamareddine and Wells, 2008] to-
wards the gradual computerisation of mathematics.
2
INTRODUCTION
Mathematical assistants are workstations running a program that veriﬁes the cor-
rectness of mathematical theorems, when provided with enough evidence. Systems
for automated deduction require less evidence or even none at all; proof-checkers
on the other hand require a fully formalised proof.
In the pioneering systems
Automath1 (of N.G. de Bruijn, based on dependent type theory), and Mizar (of
Andrzej Trybulec based on set-theory), proofs had to be given ready and well. On
the other hand for systems like NuPrl, Isabelle, and Coq, the proofs are obtained
in an interactive fashion between the user and the proof-checker. Therefore one
speaks about an interactive mathematical assistant. The list of statements that
have to be given to such a checker, the proof-script, is usually not mathematical in
nature, see e.g. table 8. The problem is that the script consists of ﬁne-grained steps
of what should be done, devoid of any mathematical meaning. Mizar is the only
system having a substantial library of certiﬁed results in which the proof-script
is mathematical in nature. Freek Wiedijk [Wiedijk, 2006] speaks of the declara-
tive style of Mizar. In [de Bruijn, 1987] a plea was given to use a mathematical
vernacular for formalising proofs.
This paper discusses two approaches inﬂuenced by de Bruijn’s mathematical
vernacular: MathLang [Kamareddine and Wells, 2008] and MPL (Mathematical
Proof Language) [Barendregt, 2003]. These approaches aim to develop a frame-
work for computerising mathematical texts which is ﬂexible enough to connect the
diﬀerent approaches to computerisation, which allows various degrees of formali-
sation, and which is compatible with diﬀerent logical frameworks (e.g., set theory,
category theory, type theory, etc.) and proof systems. Both approaches aim to
bridge informal mathematics and formalized mathematics via automatic transla-
tions into the formalised language of interactive proof-assistants. In particular:
• MPL aims to provide an interactive script language for an interactive proof
assistant like Coq, that is declarative and hence mathematical in ﬂavor.2
• MathLang is embodied in a computer representation and associated software
tools, and its progress and design are driven by the need for computerising
representative mathematical texts from various branches of mathematics.
At this stage, MPL remains a script language and has no associated software tools.
MathLang on the other hand, supports entry of original mathematical texts either
1<www.cs.kun.nl/~freek/aut>
2A similar approach to MPL is found in Isar3, with an implemented system.

350
Fairouz Kamareddine, Joe Wells, Christoph Zengler and Henk Barendregt
in an XML format or using the TEXMACS editor and these texts are manipulated by
a number of MathLang software tools. These tools provide methods for adding,
checking, and displaying various information aspects.
One aspect is a kind of
weak type system that assigns categories (term, statement, noun (class), adjective
(class modiﬁer), etc.) to parts of the text, deals with binding names to meanings,
and checks that a kind of grammatical sense is maintained. Another aspect allows
weaving together mathematical meaning and visual presentation and can associate
natural language text with its mathematical meaning.
Another aspect allows
identifying chunks of text, marking their roles (theorem, deﬁnition, explanation,
example, section, etc.), and indicating relationships between the chunks (A uses
B, A contradicts B, A follows from B, etc.). Software tool support can use this
aspect to check and explain the overall logical structure of a text.
Further aspects are being designed to allow adding additional formality to a text
such as proof structure and details of how a human-readable proof is encoded into
a fully formalised version (previously [Kamareddine et al., 2007b; Lamar, 2011]
we used Mizar and Isabelle but here, for the ﬁrst time we develop the MathLang
formalisation into Coq). [Kamareddine and Wells, 2008] surveyed the status of the
MathLang project up to November 2007. This paper picks on from that survey, ﬁlls
in a number of formalisation and implementation gaps and creates a formalisation
path via MathLang into Coq. We show for the ﬁrst time how the DRa information
can be used to automatically generate proof skeletons for diﬀerent theorem provers,
we formalise and implement the textual order of a text and explain how it can be
derived from the original text. Our proposed generic algorithm (for generating the
proof skeleton which depends on the original mathematical text and the desired
theorem prover), is highly conﬁgurable and caters for arbitrary theorem provers.
This generic algorithm as well as all the new algorithms and concepts we present
here, are implemented in our software tool. We give hints for the development of
an algorithm which is able to convert parts of a CGa annotated text automatically
into the syntax of a special theorem prover.
To test our approaches we specify using MPL a feasible interactive mathemat-
ical proof development for Newman’s Lemma and we create the complete path of
encoding in and formalising through MathLang, for the ﬁrst chapter of Landau’s
book ”Grundlagen der Analysis”. For Newman’s Lemma in MPL, we show that
the declarative interactive mathematical mode is more pleasant than the opera-
tional mode of Coq. For Landau’s chapter in MathLang, we show that the entire
path from the informal text into the fully formalised Coq text is much easier to
construct and comprehend in MathLang than in Coq. For this, we show how the
plain text document of Landau’s chapter, can be easily annotated with categories
and mathematical roles and how a Coq and a Mizar proof skeletons can be au-
tomatically generated for the chapter. We then use hints to convert parts of the
annotated text of Landau’s ﬁrst chapter into Coq. Both the Coq proof skeleton
and the converted parts into Coq, simpliﬁed the process of the full formalisation
of the ﬁrst chapter of Landau’s book in Coq.

Computerising Mathematical Text
351
Although in this paper we only illustrate MPL and MathLang for Coq, the
proposed approaches should work equally well for other proof systems (indeed, we
have previously illustrated MathLang for Mizar and Isabelle [Kamareddine et al.,
2007a; Kamareddine et al., 2007b]).
3
THE GOALS OF MATHLANG AND MPL
Sections 1a and 1b described issues with the practice of mathematics: the diﬃ-
culty for the normal mathematician in directly using a formal foundation, and the
disadvantages of the various computer representations of mathematics. To address
these issues, we set out to develop two new mathematical languages, so that texts
written in CML (the common mathematical language, expressed either with pen
and paper, or LATEX) is written instead in a way that satisﬁes these goals:
1. A MathLang/MPL text should support the usual features of CML: natural
language text, symbolic formulas, images, document structures, control over
visual presentation, etc. And the usual computer support for editing such
texts should be available.
2. It should be possible to write a MathLang/MPL text in a way that is signiﬁ-
cantly less ambiguous than the corresponding CML text. A MathLang/MPL
text should somehow support representing the text’s mathematical semantics
and structure. The support for semantics should cover not just individual
pieces of text and symbolic formulas but also the entire document and the
document’s relationship to other documents (to allow building connected li-
braries). The degree of formality in representing the mathematical semantics
should be ﬂexible, and at least one choice of degree of formality should be
both inexpensive and useful. There should be some automated checking of
the well-formedness of the mathematical semantics.
3. The structure of a MathLang/MPL text should follow the structure of the
corresponding CML, so that the experience of reading and writing Math-
Lang/MPL should be close to that of reading and writing CML. This should
make it easier for an author to see and have conﬁdence that a MathLang/MPL
text correctly represents their intentions. Thus, if any foundational formal
systems are used in MathLang/MPL, then the latter should somehow adapt
the formal systems to the needs of the authors and readers, rather than
requiring the authors and readers to adapt their thinking to ﬁt the rigid
conﬁnes of any existing foundations.
4. The structure of a MathLang/MPL text should make it easier to support fur-
ther post-authorship computer manipulations that respect its mathematical
structure and meaning. Examples include semantics-based searches, com-
putations via computer algebra systems, extraction of proof sketches (to be
completed into a full formalisation in a proof system), etc.

352
Fairouz Kamareddine, Joe Wells, Christoph Zengler and Henk Barendregt
5. A particular important case of the previous point is that MathLang/MPL
should support (but not require) interfacing with proof systems so that a
MathLang/MPL text can contain full formal details in some foundation and
the formalisation can be automatically veriﬁed.
6. Authoring of a MathLang/MPL text should not be signiﬁcantly harder for
the ordinary mathematician than authoring LATEX. Features that the author
does not want (such as formalisation in a proof system) should not require
any extra eﬀort from an author.
7. The design of MathLang/MPL should be compatible with (as yet undeter-
mined) future extensions to support additional uses of mathematical knowl-
edge.
Also, the design of MathLang/MPL should make it easy to com-
bine with existing languages (e.g., OMDoc, TEXMACS). This way, Math-
Lang/MPL might end up being a method for extending an existing language
in addition to (or instead of) a language on its own.
None of the previously existing representations for mathematical texts satisﬁes our
goals, so we have been developing new techniques. In this paper we discuss where
we are with both MathLang and MPL.
MathLang/MPL are intended to support diﬀerent degrees of formalisation. Fur-
thermore, for those documents where full formalisation is a goal, we intend to allow
this to be accomplished in gradual steps. Some of the motivations for varying de-
grees of formalisation have already been discussed in sections 1a and 1b.
Full
formalisation is sometimes desirable, but also is often undesirable due to its ex-
pense and the requirement to commit to many inessential foundational details.
Partial formalisation can be desirable for various reasons; as examples, it has the
potential to be helpful with automated checking, semantics-based searching and
querying, and interfacing with computer algebra systems (and other mathematical
computation environments). In both our languages, MathLang and MPL, partial
formalisation can be carried out to diﬀerent degrees. For example:
• The abstract syntax trees of symbolic formulas can be represented accu-
rately. This is usually missing when using systems like LATEX or Presenta-
tion MathML, while more semantically oriented systems provide this to some
degree. This can provide editing support for algebraic rearrangements and
simpliﬁcations, and can help interfacing with computer algebra systems.
• The mathematical structure of natural language text can be represented in a
way similar to how symbolic formulas are handled. Furthermore, mixed text
and symbols can be handled. This can help in the same way as capturing
the structure of symbolic formulas can help. Nearly all previous systems do
not support handling natural language text in this way.
• A weak type system can be used to check simple grammatical conditions
without checking full semantic sensibility.

Computerising Mathematical Text
353
• Justiﬁcations (inside proofs and between formal statements) can be linked
(without necessarily always indicating precisely how they are used). Some
examples of potential uses of this feature include the following:
– Extracting only those parts of a document that are relevant to speciﬁc
results. (This could be useful in educational systems.)
– Checking that each instance of apparently circular reasoning is actually
handled via induction.
– Calculating proof gaps as a ﬁrst step toward fuller formalisation.
• If one commits to a foundation (or in some cases, to a family of founda-
tions), one can start to use more sophisticated type systems in formulas and
statements for checking more aspects of well-formedness.
• And there are further possibilities.
4
AN OVERVIEW OF MATHLANG
The design of MathLang is gradually being reﬁned based on experience testing the
use of MathLang for representative mathematical texts. Throughout the devel-
opment, the design is tested by evaluating encodings of real mathematical texts,
during which issues and diﬃculties are encountered, which lead to new needs being
discovered and corresponding design adjustments. The design includes formal rules
for the representation of mathematical texts, as well as patterns and methodology
for entering texts in this representation, and supporting software.
The choice of mathematical texts for testing is primarily oriented toward texts
that represent the variety of mathematical writing by ordinary mathematicians
rather than texts that represent the interests of formalists and mathematical logi-
cians. Much of the testing has been with pre-existing texts. In some cases, texts
that have previously been formalised by others were chosen in order to compare
representations, e.g., A Compendium of Continuous Lattices [Gierz et al., 1980] of
which at least 60% has been formalised in Mizar [Rudnicki, 1992], and Landau’s
Foundations of Analysis [Landau, 1951] which was fully formalised in Automath
[van Benthem Jutting, 1977a]. In other cases, texts of historical value which are
known to have errors were chosen to ensure that MathLang’s design will not ex-
clude them, e.g., Euclid’s Elements [Heath, 1956]. Other texts were chosen to
exercise other aspects of MathLang. Authoring new texts has also been tested.
In addition to the design of MathLang itself, there has been work on relating
a MathLang text to a fully formalised version of the text. Using the information
in the CGa and DRa aspects of a MathLang text, [Kamareddine et al., 2007b;
Retel, 2009] developed a procedure for producing a corresponding Mizar document,
ﬁrst as a proof sketch with holes and then as a fully completed proof. [Lamar,
2011] attempted to follow suit with Isabelle.
In this paper, we make further
progress in completing the path in MathLang in order to reach full formalisation
and we introduce a third theorem prover (Coq) as a test bed for MathLang (in
addition to Mizar and Isabelle). We develop the proof skeleton idea presented

354
Fairouz Kamareddine, Joe Wells, Christoph Zengler and Henk Barendregt
Figure 1. Overall situation of work in MathLang
earlier in [Kamareddine et al., 2007b] speciﬁcally for Mizar, into an automatically
generated proof skeleton in a choice of theorem provers (including Mizar, Isar and
Coq). To achieve this, we give a generic algorithm for proof skeleton generation
which takes the required prover as one of its arguments. We also give hints for the
development of a generic algorithm which automatically converts parts of a CGa
annotated text into the syntax of the theorem prover it is given as an argument.
Figure 1 (adapted from [Kamareddine et al., 2007b]) diagrams the overall cur-
rent situation of work on MathLang. In the rest of this paper, we discuss the
aspects CGa, TSa, and DRa in more detail, we introduce the generic automatic
proof skeleton generator and how parts of the CGa annotated text can be for-
malised into a theorem prover. We also discuss interfacing MathLang with Coq.
4a
The Core Grammatical aspect (CGa)
The Core Grammatical aspect (CGa) [Kamareddine et al., 2004c; Kamareddine
et al., 2006; Maarek, 2007] is based on the Weak Type Theory (WTT) of Ned-
erpelt [Nederpelt, 2002] whose metatheory was established by Kamareddine [Ka-
mareddine and Nederpelt, 2004]. WTT in turn was heavily inspired by the Math-
ematical Vernacular (MV) [de Bruijn, 1987].
In WTT, a document is a book which is a sequence of lines, each of which is
a pair of a sentence (a statement or a deﬁnition) and a context of facts (declara-
tions or statements) assumed in the sentence. WTT has four ways of introducing
names. A deﬁnition introduces a name whose scope is the rest of the book and
associates the name with its meaning. A name introduced by a deﬁnition can have
parameters whose scope is the body of the deﬁnition. A declaration in a context
introduces a name (with no parameters) whose scope is only the current line. Fi-

Computerising Mathematical Text
355
nally, a preface gives names whose scope is the document; names introduced by
prefaces have parameters but unlike deﬁnitions their meanings are not provided
(and thus presumed to be given externally to the document). Declarations, deﬁni-
tions, and statements can contain phrases which are built from terms, sets, nouns,
and adjectives. Using the terminology of object-oriented programming languages,
nouns act like classes and adjectives act like mixins (a special kind of function
from classes to classes).
WTT uses a weak type system with types like noun,
set, term, adjective, statement definition, context, and book to check basic
well-formedness. Sets are used when something is deﬁnitely known to be a set and
the richer structure of a noun is not needed, and terms are used for things that
are not sets (and sometimes for sets in cases where the type system is too weak).
Although WTT provides many useful ideas, the deﬁnition of WTT has many
limitations. The many diﬀerent ways of introducing names are too complicated and
awkward. WTT provides no way to indicate which statements are used to justify
other statements and in general does not deal with proofs and logical correctness.
WTT provides no ways to present the structure of a text to human readers; there
is no way of grouping statements and identifying their mathematical/discourse
roles such as theorem, lemma, conjecture, proof, section, chapter. WTT provides
no way to give human names to statements (e.g., “Newman’s Lemma”). WTT
provides no way to use in one document concepts deﬁned in another document.
The Core Grammatical aspect (CGa) was shaped by repeated experiences of
annotating mathematical texts.
CGa simpliﬁes diﬃcult aspects of WTT, and
enhances the nouns and adjectives of WTT with ideas from object-oriented pro-
gramming so that nouns are more like classes and adjectives are more like mixins.
In CGa, the diﬀerent kinds of name-introducing forms of WTT are uniﬁed; all def-
initions by default have indeﬁnite forward scope and a local scope operator allows
local deﬁnitions. The basic constructs of CGa are the step and the expression.
The tasks handled in WTT by books, prefaces, lines, declarations, deﬁnitions, and
statements are all represented as steps in CGa. A step can be a block {s1, . . . , sn},
which is merely a sequence of steps. A step can be a local scoping s1 ⊲s2, which is
a pair of steps s1 and s2 where the deﬁnitions and declarations of s1 are restricted
in scope to s2 and the assertions of s1 are assumptions of s2. A step can also be a
deﬁnition, a declaration, or an expression (which asserts a truth). Expressions are
also used for the bodies of deﬁnitions and inside the types in declarations. The
possibilities for expressions include uses of deﬁned identiﬁers, identiﬁer declara-
tions, and noun descriptions. A noun description allows specifying characteristics
of a class of entities. For example,
{M : set; y : natural number; x : natural number; ∈(x, M)}⊲=(+(x, y), +(y, x))
is an encoding of this (silly) CML text:
“Given that M is a set, y and x are natural numbers, and x belongs to
M, it holds that x + y = y + x.”
This example assumes that earlier in the document there are declarations like:
. . . ; ∈(term, set) : stat; =(term, term) : stat; natural number : noun;
+ (natural number, natural number) : natural number; . . .

356
Fairouz Kamareddine, Joe Wells, Christoph Zengler and Henk Barendregt
Here, M, y, x, ∈, =, and + are identiﬁers4 while term, set, stat, and noun
are keywords of CGa.
The semicolon, colon, comma, parentheses, braces, and
right triangle (⊲) symbols are part of the syntax of CGa. The statements like
∈(term, set) : stat are declarations; this example declares ∈to be an operator
that takes two arguments, one of type term and one of type set, and yields a
result of type stat (statement).
The statement M : set is an abbreviation for
M() : set which declares the identiﬁer M to have zero parameters.
CGa uses grammatical/linguistic/syntactic categories (also called types) to make
explicit the grammatical role played by the elements of a mathematical text. In
the above example, we see the category expressions term, set, stat, noun, and
natural number. In fact, the category expression natural number acts as an ab-
breviation for term(natural number), and term, set, and noun are abbreviations
for term(Noun {}), set(Noun {}), and noun(Noun {}), which all use the uncharac-
terised noun description Noun {}. A noun description is of the form Noun s and
describes a class of entities with characteristics (declared operations and true facts)
deﬁned by the step s. The arguments of the category constructors term, set, and
noun are expressions which evaluate to noun descriptions. The category term(e)
describes individual entities belonging to the class described by the noun expres-
sion e, and the category set(e) describes any set of such entities. The category
noun(e) describes any noun which deﬁnes all the operations described by e with
the same types. So in the above example, the abbreviation term is the type of all
mathematical entities, the abbreviation set is the type of any set, noun is the type
of any noun (and speciﬁes no characteristics for it), and natural number is the
type of any mathematical entity having the characteristics described by the noun
natural number.5 The behaviour of nouns in CGa is similar to that of classes in
object-oriented programming languages. CGa also has adjectives which are like
object-oriented mixins and act as functions from nouns to nouns. These linguistic
levels and syntactic elements are summarised in the following deﬁnitions.
DEFINITION 1 (Linguistic levels). The syntax of CGa is based on a hierarchy of
the ﬁve diﬀerent linguistic levels given below. Elements from I and C are part of
E, expressions are part of the phrases of P and steps S are built from phrases.
1. Identiﬁer level I
2. Category level C
3. Expression level E
4. Phrase level P
5. Step level S
DEFINITION 2 (Syntactic elements). The syntactic elements at each level are:
1. At identiﬁer level: term identiﬁers IT , set identiﬁers IS, noun identiﬁers IN ,
adjective identiﬁers IA and statement identiﬁers IP.
2. At category level: term categories T , set categories S, noun categories N,
adjective categories A, statement categories P and declaration categories D.
4Our current implementation only allows ASCII characters in identiﬁers, but we plan to
support any graphic Unicode characters.
5CGa has other mechanisms that allow specifying additional characteristics of the noun
natural number separate from its declaration, and we assume in this example that this is done.

Computerising Mathematical Text
357
3. At expression level: declaration expressions DEC, instantiation expressions
INST, description expressions DSC, reﬁnement expressions REF and the
self expression SEL.
4. At phrase level: sub reﬁnement phrases SUB, deﬁnition phrases DEF, dec-
laration expressions DEC and statement expressions P.
5. At step level: local scoping steps LOC, block steps BLO and each phrase
of P is a basic step.
For details of the rules of CGa see [Kamareddine et al., 2006; Maarek, 2007].
Here, it is crucial to mention the following colour codings for these CGa categories:
term
set
noun
adjective
statement
deﬁnition
declaration
step
context .
The types of CGa are more sophisticated than the weak types of WTT and
allow tracking which operations are meaningful in some additional cases. Although
CGa’s types are more powerful than WTT’s, there are still signiﬁcant limitations.
One limitation is that higher-order types are not allowed. For example, although
CGa allows the type (term, term) →term, which is the type of an operator that
takes two arguments of type term and returns a result of type term, CGa does not
allow using the type ((term) →term, term) →term, which would be the type of an
operator that takes another operator as its ﬁrst argument. Higher-order types can
be awkwardly and crudely emulated in CGa by encapsulation with noun types, but
this emulation does not work well due to the fact that CGa’s type polymorphism
is shallow, which is another signiﬁcant limitation. To work around the weakness
of CGa’s type polymorphism, in practice we ﬁnd ourselves often giving entities
the type term instead of a more precise type. We continue to work on making
the type system more ﬂexible without making it too complex. It is important to
understand that the goal of CGa’s type system is not to ensure full correctness, but
merely to check whether the reasoning parts of a document are coherently built in
a sensible way. CGa provides a kind of grammar for well-formed mathematics with
grammatical categories and allows checking for basic well-formedness conditions
(e.g., the origin of all names/symbols can be tracked).
The design of CGa is due to Kamareddine, Maarek and Wells [Kamareddine et
al., 2006]. The implementation of CGa is due to Maarek [Maarek, 2007].
4b
The Text and Symbol aspect (TSa)
The Text and Symbol aspect (TSa) [Kamareddine et al., 2004b; Kamareddine
et al., 2007a; Maarek, 2007; Lamar, 2011] allows integrating normal typesetting
and authoring software with the mathematical structure represented with CGa.
TSa allows weaving together usual mathematical authoring representations such
as LATEX, XML, or TEXMACS with CGa data. Thanks to a notion of souring rules
(called “souring” because it does the opposite of syntactic sugar), TSa allows the
structure of the mathematical text to follow that of the CML text as conceived
by the mathematician. TSa allows interleaving pieces of CGa with pieces of CML
in the form of mixtures of natural language, symbolic formulas, and formatting
instructions for visual presentation. The interleaving can be at any level of gran-
ularity: meanings can be associated at a coarse grain with entire paragraphs or

358
Fairouz Kamareddine, Joe Wells, Christoph Zengler and Henk Barendregt
<> <∃>There is <> <0>an element 0 in <R>R such that <=> <+> <a>a + <0>0 = <a>a
∃(
0 :
R,
= (
+ (
a, 0
), a
)
)
Figure 2. Example of CGa encoding of CML text
sections, or at a ﬁne grain with individual words, phrases, and symbols. Arbitrary
amounts of mathematically uninterpreted text can be included. The TSa represen-
tation is inspired by the XQuery/XPath Data Model (XDM) [WC3, 2007] used for
representing the information content of XML documents. In TSa, a document d
is built from the empty document ([ ]) by sequencing (d1, d2) and labelling (ℓ⟨d⟩).
For example, the CML text and its CGa representation given in ﬁgure 2 could be
represented in TSa by the following ﬁne-grained interleaving of CGa6 and LATEX:
“There is #1 such that #2.”
⟨∃⟨“#1 in #2”⟨:⟨“an element $0$”⟨0⟩, “$R$”⟨R⟩⟩⟩,
“$#1 = #2$”⟨=⟨“#1 + #2”⟨+⟨“a”⟨a⟩, “0”⟨0⟩⟩⟩, “a”⟨a⟩⟩⟩⟩
This example (see [Kamareddine et al., 2007a]) uses the abbreviation that ℓstands
for ℓ⟨[ ]⟩. For example, “a”⟨a⟩actually stands for “a”⟨a⟨[ ]⟩⟩.
Associated with TSa are methods for extracting separately the CGa and the
typesetting instructions or other visual representation. E.g., from the TSa above
can be extracted the following TSa representation of just the CGa portion:
∃⟨:⟨0, R⟩, =⟨+⟨a, 0⟩, a⟩⟩
The CGa portion of this text can be type checked and used for processing that
needs to know the mathematical meaning of the text.
Similarly, the following
pieces of LATEX can also be extracted:
“There is #1 such that #2.”
⟨“#1 in #2”⟨“an element $0$”, “$R$”⟩,
“$#1 = #2$”⟨“#1 + #2”⟨“a”, “0”⟩, “a”⟩⟩
This tree of LATEX typesetting instructions can be further ﬂattened for actual
processing by LATEX into a string such as:
“There is an element $0$ in $R$ such that $a + 0 = a$.”
The idea of the TSa representation is independent of the visual formatting language
used. Although we use LATEX in our example here, in our implementations so far
we have used the TEXMACS internal representation and also XML.
As part of using TSa to interleave CGa and more traditional natural language
and typesetting information, we needed to develop techniques for handling certain
challenging CML formations where the mathematical structure and the CML rep-
resentation do not nicely match. For example, in the text 0+a0 = a0 = a(0+0) =
6The representation shown here omits type/category annotations that we usually include with
the CGa identiﬁers used in the TSa representation.

Computerising Mathematical Text
359
<=> <>0 + a0 =
<shared> <>a0
<=> =
<shared> <>a(0 + 0)
<=> = <>a0 + a0
<eq> <>0 + a0
<>a0
<eq> <>a0
<>a(0 + 0)
<eq> <>a(0 + 0)
<>a0 + a0
Figure 3. Example of using souring in TSa to support sharing
a0 + a0, the terms a0 and a(0 + 0) are each shared between two equations. Most
formal representations would require either duplicating these shared terms, like
for example 0 + a0 = a0 ∧a0 = a(0 + 0) ∧a(0 + 0) = a0 + a0, or explicitly
abstracting the shared terms. To allow the TSa representation to be as close to
CML as possible, we instead solve this by using “souring” annotations in the TSa
representation [Kamareddine et al., 2007a]. These annotations are a third kind of
node label used in TSa, in addition to the CGa and formatting labels. Souring
annotations are used to extract the correct mathematical meaning and the nice
visual presentation in the CML style. For the above example, see ﬁgure 3.
We have developed more sophisticated annotations that can handle more com-
plicated cases of sharing of terms between equations. Souring annotations have also
been developed to support several other common CML formulations. Support for
folding and mapping over lists allows using forms like ∀a, b, c ∈S.P as shorthand
for ∀a ∈S.∀b ∈S.∀c ∈S.P and {a, b, c} as shorthand for {a}∪({b}∪({c}∪∅)). We
have not yet developed folding that is sophisticated enough to handle ellipsis (. . .)
as in CML formulations like the next example (from [Sexton and Sorge, 2006]):
f[x, . . . , x
| {z }
n + 1 arguments
] = f (n)(x)
n!
We have implemented a user interface as an extension of the TEXMACS editor
for entering the TSa MathLang representation. The author can use mouse and
keyboard commands to annotate CML text entered in TEXMACS with boxes rep-
resenting the CGa grammatical categories in order to assign CGa identiﬁers and
explicitly indicate mathematical meanings. The user interface allows displaying
either a pure CML view which hides the TSa and CGa information, a pure CGa
view, or various combined views including a view like that of ﬁgure 2. This inter-
face allows adding souring annotations like those of ﬁgure 3. We plan to develop
techniques for not just pairing a single CML presentation with its CGa meaning,
but also allowing multiple parallel visual presentations such as multiple natural
languages (not just English), both natural language and symbolic formulas, and
presentations in diﬀerent symbolic notations. We plan also to develop better soft-
ware support to aid in semi-automatically converting existing CML texts into
MathLang via TSa and CGa.
The design of TSa is due to Kamareddine, Maarek, and Wells with contribu-
tions by Lamar to the souring rules [Kamareddine et al., 2007a; Maarek, 2007;
Lamar, 2011]. The implementation is primarily by Maarek [Maarek, 2007] with
contributions from Lamar [Lamar, 2011].

360
Fairouz Kamareddine, Joe Wells, Christoph Zengler and Henk Barendregt
4c
The Document Rhetorical aspect (DRa)
The Document Rhetorical aspect (DRa) [Kamareddine et al., 2007c; Retel, 2009;
Zengler, 2008] supports identifying portions of a text and expressing the relation-
ships between them. Any portion of text (e.g., phrase, step, block, etc.) can be
given an identity. Many kinds of relationships can be expressed between identiﬁed
pieces of text. E.g., a chunk of text can be identiﬁed as a “theorem”, and another
can be identiﬁed as the “proof” of that theorem. Similarly, one chunk of text can
be a “subsection” or “chapter” of another. Given these relationships, it becomes
possible to do computations to check whether all dependencies are identiﬁed, to
check whether the relationships are sensible or problematic (and whether there-
fore the author should be warned), and to extract and explain the logical structure
of a text. Dependencies identiﬁed this way have been used in generating formal
proof sketches and identifying the proof holes that remain to be ﬁlled. This paper
presents further formalisation and implementation of notions related to DRa.
DRa is a system for attaching annotations to mathematical documents that
indicate the roles played by diﬀerent parts of a document.
DRa assumes the
underlying mathematical representation (which can be the MathLang aspects CGa
or TSa) has some mechanism for identifying document parts.
Some DRa annotations can be unary predicates on parts; these include annota-
tions indicating ordinary document sectioning roles such as part, chapter, section,
etc. (like the sectioning supported by LATEX, OMDoc, DocBook, etc.) and others
indicating special mathematical roles such as theorem, lemma, proof, etc.
Other DRa annotations can be binary predicates on parts; these include such
relationships between parts as “justiﬁes”, “uses”, “inconsistent with”, and “ex-
ample of ”. Regarding the annotation of justiﬁcations, remember that a CML text
is usually incomplete: a mathematical thought process makes jumps from one in-
teresting point to the next, skipping over details. This does not mean that many
mistakes can occur; these details are usually so obvious for the mathematician
that a couple of words are enough (e.g., “apply theorem 35”). The mathematician
knows that too many details hinder concentration. To allow MathLang text to be
close to the CML text, DRa allows informal justiﬁcations, which can be seen as
hints about which statements would be used in the proof of another statement.
Figure 4 gives an example (taken from [Kamareddine et al., 2007b] and imple-
mented by Retel [Retel, 2009]) where the mathematician has identiﬁed parts of the
text (indicated by letters A through I in the ﬁgure). Figure 5, shows the underlying
mathematical representation of some example DRa annotations for the example
in ﬁgure 4. Here, the mathematician has given each identiﬁed part a structural
(e.g., chapter, section, etc.) and/or mathematical (e.g., lemma, corollary, proof,
etc.) rhetorical role, and has indicated the relation between wrapped chunks of
texts (e.g., justiﬁes, uses, etc.). Note that all the DRa annotations are represented
as triples; this allows using the machinery of RDF [WC3, 2004] (a W3C standard
that is aimed at the “semantic web”) to represent and manipulate them.
The DRa structure of a text can be represented as a tree (which is exactly the

Computerising Mathematical Text
361
Lemma 1.
For m, n ∈N one has:
m2 = 2n2 =⇒m = n = 0
A
Proof.
Deﬁne on N the predicate:
P (m) ⇐⇒∃n.m2 = 2n2 & m > 0.
E
Claim.
P (m) =⇒∃m′ < m.P (m′).
F
Indeed suppose m2 = 2n2 and m > 0. It follows that m2 is even, but
then m must be even, as odds square to odds. So m = 2k and we have
2n2 = m2 = 4k2 =⇒n2 = 2k2 Since m > 0, if follows that m2 > 0,
n2 > 0 and n > 0. Therefore P (n). Moreover, m2 = n2 + n2 > n2,
so m2 > n2 and hence m > n. So we can take m′ = n.
G
By the claim ∀m ∈N.¬P (m), since there are no inﬁnite descending sequences of
natural numbers.
Now suppose m2 = 2n2
with m ̸= 0. Then m > 0 and hence P (m). Contradiction.
H
Therefore m = 0. But then also n = 0.
I
■
B
Corollary 2.
√2 /∈Q
C
Proof. Suppose √2 ∈Q, i.e. √2 = p/q with p ∈Z, q ∈Z −{0}. Then √2 = m/n
with m = |p|, n = |q| ̸= 0. It follows that m2 = 2n2. But then n = 0 by the lemma.
Contradiction shows that √2 /∈Q.
■
D
justifies
justifies
uses
uses
justifies
uses
uses
subpartOf
subpartOf
Figure 4. Wrapping/naming chunks of text and marking relationships in DRa
(A, hasMathematicalRhetoricalRole, lemma)
(B, justiﬁes, A)
(E, hasMathematicalRhetoricalRole, deﬁnition)
(D, justiﬁes, C)
(F , hasMathematicalRhetoricalRole, claim)
(D, uses, A)
(G, hasMathematicalRhetoricalRole, proof)
(G, uses, E)
(B, hasMathematicalRhetoricalRole, proof)
(F , uses, E)
(H, hasMathematicalRhetoricalRole, case)
(H, uses, E)
(I, hasMathematicalRhetoricalRole, case)
(H, caseOf, B)
(C, hasMathematicalRhetoricalRole, corollary)
(H, caseOf, I)
(D, hasMathematicalRhetoricalRole, proof)
Figure 5. Example of DRa relationships between chunks of text in ﬁgure 4
tree of the XML representation of the DRa annotated MathLang document). Due
to the tree structure of a DRa annotated document, we refer to an annotated part
of a text as a DRa node. We see an example of such a DRa node in ﬁgure 8.
The role of this node is declaration and its name is decA. Note that the content
of a DRa node is the user’s CGa and TSa annotation. In the DRa annotation
of a document, there is a dedicated root node (the Document node) where each
top-level DRa node is a child of this root node. In ﬁgure 6, we see a tree consisting
of 10 nodes. The root node (labelled Document) has four children nodes and ﬁve
grandchildren nodes (which are all children of B).
We distinguish between proved nodes (theorem, lemma, etc.) with a solid line
in the picture and unproved nodes (axiom, deﬁnition, etc.) with a broken line. We
introduce this distinction because with the current implementation of DRa the
user has the possibility to create its own mathematical and structural roles. Since
we want to check a DRa annotated document for validity, the information whether
a node is to be proved or not is important. For example such information would
result in an error if someone tries to prove an unproved node e.g. by proving

362
Fairouz Kamareddine, Joe Wells, Christoph Zengler and Henk Barendregt
Figure 6. Example of a tree of the DRa nodes of a document
Figure 7. Dependency graph for an example DRa tree
a deﬁnition or an axiom.
When document D2 references document D1 it can
reference the root node of document D1 to include all of its mathematical text.
In ﬁgure 4 one can see, that there are four top-level nodes: A, B, C and D,
representing respectively lemma 1, a proof of lemma 1, corollary 2 and a proof of
corollary 2. The proof of lemma 1 has ﬁve children: E, F, G, H, I representing
respectively the deﬁnition of the predicate, a claim, the proof of the claim, case 1
and case 2. The visual representation of this tree can be seen in ﬁgure 6.
By traversing the tree in pre-order we derive the original linear order of the
DRa nodes of the text. Pre-order means that the traversal starts with the root
node and for each node we ﬁrst visit the parent node before we visit its children.
It is important to mention that we have also an order of the nodes at the same
level from left to right. This means that we enumerate the children of a node form
1 to n and process them in this way. In the example of ﬁgure 6, the pre-order
would yield the order A, B, E, F, G, H, I, C, D.
The DRa implementation can automatically extract a dependency graph (as
seen in ﬁgure 7) that shows how the parts of a document are related.
Textual Order
To be able to examine the proper structure of a DRa tree we introduce the concept
of textual order between two nodes in the tree. The concept of textual order is
a modiﬁcation of the logical precedence presented in [Kamareddine et al., 2007c].
In what follows, we formalise this concept of order and show how it can be used
to automatically generate a proof skeleton. The textual order expresses the de-
pendencies between parts of the text. For example if a node A uses a part of a
node B, then in a sequence of reasoning steps, B has to be before A. In order to

Computerising Mathematical Text
363
Figure 8. An example for a single DRa node
formally deﬁne textual order, we introduce some notions for DRa nodes.
Recall that the content of a DRa node is its CGa and TSa part and that we have
nine kinds of CGa annotations: term set noun adjective statement declaration
deﬁnition step context . A DRa node can also have further DRa nodes as children
(e.g. B in ﬁgure 6 has the children E, F, G, H and I). We give diﬀerent sets for a
DRa node n. All these sets can be automatically generated from the user’s CGa
and TSa annotations of the text. Table 1 deﬁnes these sets and gives examples for
the CGa annotated text in ﬁgure 9 which is the deﬁnition of the subset relation.
Figure 9. CGa annotations for the deﬁnition of the subset relation
Set
Description
Example of Fig. 9
T (n)
{x | x is part of n and x is annotated as term }
{x}
S(n)
{x | x is part of n and x is annotated as set }
{A, B}
N(n)
{x | x is part of n and x is annotated as noun }
{}
A(n)
{x | x is part of n and x is annotated as adjective }
{}
ST (n)
{x | x is part of n and x is annotated as statement }
{A ⊂B, x ∈A, x ∈
B, x ∈A =⇒x ∈
B, ∀x(x ∈A =⇒x ∈
B)}
DC(n)
{x | ∃q part of n, q is annotated as declaration and
x is the declared symbol of q }
{x}
DF(n)
{x | ∃q part of n, q is annotated as deﬁnition and
x is the deﬁned symbol of q }
{⊂}
SP(n)
{x | x is part of n and x is annotated as step }
{A ⊂B ⇐⇒∀x(x ∈
A =⇒x ∈B)}
C(n)
the set of all parts of n annotated as context
{}
ENV(n)
{x|∃m ̸= n, m is a node in the pre-order path from
the root node to the node n, x is a part of m, and
x is annotated as statement }
Table 1. Sets for a DRa node n and examples
Let us give further examples of DC(n) and DF(n). In section 4a we had the
following example of a list of declarations (call it ex):
. . . ; ∈(term, set) : stat; =(term, term) : stat; natural number : noun;
+ (natural number, natural number) : natural number; . . .
For ex, we have that DC(ex) = {∈, =, natural number, +}.
Now take the example of ﬁgure 10 (and call it ex′). This example introduces
the deﬁnition of ¬ (Deﬁnition 1). We have that DF(ex′) = {¬}.

364
Fairouz Kamareddine, Joe Wells, Christoph Zengler and Henk Barendregt
The syntax of a deﬁnition in the internal representation of CGa (which is not
necessarily the same as that given by the reader), is an identiﬁer with a (possibly
empty list of arguments) on the left-hand side followed by “:=” and an expression.
The introduced symbol is the identiﬁer of the left-hand side. For the example of
ﬁgure 9 (call it ex′′), the introduced symbol is ⊂, hence DF(ex′′) = {⊂} and the
internal CGa representation is: ⊂(A, B) := forall(a, impl(in(a, A), in(a, B))).
Note that ENV(n) is the environment of all mathematical statements that occur
before the statements of n (from the root node). Note furthermore that in the
CGa syntax of MathLang, a deﬁnition or a declaration can only introduce a term ,
set ,
noun,
adjective ,
or
statement . Furthermore, recall that mathematical
symbols or notions can only be introduced by deﬁnition or
declaration and that
mathematical facts can only be introduced by a statement . We deﬁne the set
IN(n) of introduced symbols and facts of a DRa node n as follows:
IN(n) := DF(n) ∪DC(n) ∪{s|s ∈ST (n) ∧s ̸∈ENV(n)} ∪
[
c childOf n
IN(c)
At the heart of a context , step , deﬁnition, or declaration, is a set of statement ,
set ,
noun ,
adjective , and term . A DRa node n uses the set USE(n) where:
USE(n) := T (n) ∪S(n) ∪N(n) ∪A(n) ∪ST (n) ∪
[
c childOf n
USE(c)
Lemma 1. For every DRa node n we have:
1. DF(n) ∪DC(n) ⊆T (n) ∪S(n) ∪N(n) ∪A(n) ∪ST (n).
2. IN(n) ⊆USE(n).
Proof. We prove 2 by induction on the depth of parenthood of n. If n has no
children then use lemma 1. Assume the property holds for all children c of n. By
lemma 1 and the induction hypothesis, we have IN(n) ⊆USE(n).
■
We demonstrate these notions with an example. Consider a part of a mathe-
matical text and its corresponding DRa tree with relations as in ﬁgure 10.
We assume the document starts with an environment which contains two state-
ments, <True>True and <False>False. Hence ENV(def1) = {True, False}. When
traversing the tree we start with the given environment for the node def1:
ENV(def1) = {True, False}
The environment for case1 consists of the environment of def1 and all new state-
ments of def1. In def1 there is only the new statement ¬ which is added to the
environment: ENV(case1) = {¬} ∪ENV(def1). After case1 all the statements of
this node are added to the environment. These are ¬ True and ¬ True = False:
ENV(case2) = {¬ True, ¬ True = False} ∪ENV(case1)
We can proceed with the building of the environment in the same way and get the
last two environments of lem1 and pr1:
ENV(lem1) = {¬ False, ¬ False = True} ∪ENV(case2)
ENV(pr1) = {¬¬ True, ¬¬ True = True} ∪ENV(lem1)
With this information we derive the sets as shown in table 2 for the single nodes.
We can now formalise three diﬀerent kinds of textual order ≺, ⪯and ↔:

Computerising Mathematical Text
365
Document
Case
case1
Case
case2
Deﬁnition
def1
Lemma
lem1
Proof
pr1
uses
uses
justiﬁes
caseOf
caseOf
Figure 10. Example of an annotated text and its corresponding DRa tree
Node n
IN(n)
USE(n)
def1
{¬} ∪IN(Case 1) ∪IN(Case 2)
{¬} ∪USE(Case 1) ∪USE(Case 2)
case1
{¬ True, ¬True = False}
{True, False, ¬, ¬ True, ¬True
=
False}
case2
{¬False, ¬False = True}
{True, False, ¬, ¬False, ¬False
=
True}
lem1
{¬¬ True, ¬¬True = True}
{True, ¬, ¬ True, ¬¬ True, ¬¬ True =
True}
pr1
{¬¬ True = ¬ False}
{True, False, ¬, ¬ True, ¬¬ True,
¬ False, ¬¬ True
=
¬ False,
¬ False = True}
Table 2. The sets IN and USE for the example
• Strong textual order ≺: If a node A uses a declared/deﬁned symbol x or
a statement x introduced by a node B, we say that A succeeds B and write
B ≺A. More formally: B ≺A := ∃x(x ∈IN(B) ∧x ∈USE(A)).
• Weak textual order ⪯: This order describes a subpart relation between
two nodes (A is a subpart of B, written as A ⪯B). More formally:
A ⪯B := IN(A) ⊆IN(B) ∧USE(A) ⊆USE(B)
• Common textual order ↔: This order describes the relation that two
nodes use at least one common symbol or statement. More formally:
A ↔B := ∃x(x ∈USE(A) ∧x ∈USE(B))
When B ≺A (resp. A ⪯B) we also write A ≻B (resp. B ⪰A). A DRa relation
induces a textual order. Table 3 gives some relations and their textual order.
We can now verify the relations of the example of ﬁgure 10 and their textual
orders (Table 4). It is obvious that all ﬁve conditions hold and hence the relations
are valid. For example the relation (case2, uses, lem1) would not be valid, because
¬∃x(x ∈USE(case1) ∧x ∈IN(lem1)).
Note that these conditions are only of a syntactical form. There is no semantical
checking if e.g. a “justiﬁes” relation really connects a proved node and its proof.

366
Fairouz Kamareddine, Joe Wells, Christoph Zengler and Henk Barendregt
Relation
Meaning
Order
A uses B
A uses a statement or a symbol of B
B ≺A
A
inconsistentWith
B
some statement in A contradicts a statement in B
B ≺A
A justiﬁes B
A is the proof for B
A
↔
B
A relatesTo B
There is a connection between A and B but no de-
pendence
A
↔
B
A caseOf B
A is a case of B
A ⪯B
Table 3. Example of DRa relations and their textual order
Relation
Condition
Order
(case1, caseOf, def1)
IN(case1) ⊆IN(def1) ∧USE(case1) ⊆
USE(def1)
case1 ⪯def1
(case2, caseOf, def1)
IN(case2) ⊆IN(def1) ∧USE(case2) ⊆
USE(def1)
case2 ⪯def1
(pr1, justiﬁes, lem1)
∃x(x ∈USE(pr1) ∧x ∈USE(lem1))
pr1 ↔lem1
(lem1, uses, def1)
∃x(x ∈USE(lem1) ∧x ∈IN(def1))
def1 ≺lem1
(pr1, uses, def1)
∃x(x ∈USE(pr1) ∧x ∈IN(def1))
def1 ≺pr1
Table 4. Conditions for the relations of the example
The GoTO
The GoTO is the Graph of textual order. For each kind of relation in the depen-
dency graph (DG) of a DRa tree we can provide a corresponding textual order
≺, ⪯or ↔. These diﬀerent kinds of order can be interpreted as edges in a directed
graph. So we can transform the dependency graph into a GoTO by transforming
each edge of the DG. So far there are two reasons why the GoTO is produced:
1. Automatic Checking of the GoTO can reveal errors in the document (e.g.
loops in the structure of the document).
2. The GoTO is used to automatically produce a proof skeleton for a prover.
To transform an edge of the DG we need to know which textual order it induces.
Each relation has a speciﬁc order ≺, ≻, ⪯, ⪰, ↔.
Table 5 shows the graphical
representation of such edges and an example relation we have seen in our examples.
There is also a relation between a DRa node and its children: For each child c of
(A, uses, B)
A ≻B
(A, caseOf, B)
A ⪯B
(A, justiﬁes, B)
A ↔B
Table 5. Graphical representation of edges in the GoTO
a node n we have the edge c ⪯n in the GoTO. This “childOf” relation is added
automatically when producing the GoTO. But it can be added manually by the
user. This can be useful e.g. in papers with a page restriction, where some parts

Computerising Mathematical Text
367
of the text are relocated in the appendix but would be originally within the main
text. The algorithm for producing the GoTO from the DG works in two steps:
1. transform each relation of the DG into its corresponding edge in the GoTO
2. for each child c of a node n add the edge c ⪯n to the GoTO
When performing this algorithm on the example of ﬁgure 7 we get the GoTO as
demonstrated in ﬁgure 11. Each relation of the DG which induces a ↔textual
order is replaced by the corresponding edge in the GoTO. We can see these edges
between a proved node and its proof where the “justiﬁes” relation induces a ↔
order (e.g. between A and B, C and D, and F and G). The children of the node
B are connected to B via ⪯edges in the GoTO. For the “caseOf” relation, the
user has manually speciﬁed the relation, the other edges were added automatically
by the algorithm generating the GoTO. The relations which induce the order ≺
are transformed into the corresponding directed edges in the GoTO. We see that
the direction of the nodes has changed with respect to the DG. This is because
we only have “uses” relations, and for a relation (A, uses, B) we have the textual
order B ≺A which means, that the direction of the edge changes.
Figure 11. Graph of Textual Order for an example DRa tree
Automatic checking of DG and GoTO
We implemented two kinds of failures: warnings and errors. At the current devel-
opment of DRa we check for four diﬀerent kinds of failures:
1. Loops in the GoTO (error)
2. Proof of an unproved node (error)
3. More than one proof for a proved node (warning)
4. Missing proof for a proved node (warning)
The checks for 2) – 4) are performed in the DG. For 2) we check for every node of
type “unproved” if there is an incoming edge of type “justiﬁes”. If so, an error is
returned (e.g. when someone tries to prove an axiom or a deﬁnition). For 3) and
4) we check for each node of type “proved” if there is an incoming edge of type
“justiﬁes”. If not, we return a warning (this can be a deliberate omission of the
proof or just a mistake). If there is more than one proof for one node we return
also a warning (most formal systems cannot handle multiple proofs).
For 1) we search for cycles in the GoTO. Therefore we have to deﬁne how we
treat the three diﬀerent kinds of edges. Edges of type ≺and ⪯are treated as
directed edges. Edges of type ↔are in principal undirected edges, which means

368
Fairouz Kamareddine, Joe Wells, Christoph Zengler and Henk Barendregt
for an edge A ↔B, one can get from A to B and from B to A in the GoTO. It is
vital, that within one cycle such an edge is only used in one direction. Otherwise
we would have a trivial cycle between two nodes connected by a ↔edge.
As we will see in the next section, a single node in the DRa tree can ﬁrst be
translated when all its children nodes are ready to be translated. To reﬂect this
circumstance we have to add certain nodes in the GoTO for the cycle check. Let
us demonstrate this with an example. Consider a DG and GoTO as in ﬁgure 12.
Figure 12. Example of a not recognised loop in a DRa (left DG, right GoTO)
Apparently there is a cycle in this tree, because to be able to translate C we
need to translate its children D and E. Since C uses A, A must be translated before
translating C. But the child D of C is used by A leading to a deadlock. Neither
A nor C can be processed. To recognise such cycles we add certain edges to the
GoTO when checking for cycles. Therefore we have to look at the children of a
node n: hidden cycles can only evolve, when there are edges ei from a child node
ci to a target node ti which is not a sibling of ci. Hence we add an edge ci ≻n for
each such node ei to the GoTO. This can be done via algorithm 1.
We could also
foreach node n of the tree do
foreach child c of n do
foreach outgoing edge e of c do
if target node t of e is no sibling of c then
add a Strong textual precedence edge from n to t;
end
end
end
end
Algorithm 1: Adding additional edges to the GoTO
add new edges for all incoming edges of the children ci but this is not necessary
since the textual order of the “childOf” relation is a directed edge from each child
ci to its parent node n and the transitivity of the edges helps ﬁnd a cycle anyway.
In the example from ﬁgure 12, algorithm 1 would add one edge to the GoTO:
The child node D of C has an outgoing node to the non-sibling node A. So a new
directed edge from C to A is added which yields the result of ﬁgure 13 where a
cycle between the nodes A, C and A with the edges A-C and C-A appears.
Figure 13. GoTO graph of the example of ﬁgure 12 with added edges

Computerising Mathematical Text
369
Document
Case 2
F
Case 1
E
uses
justiﬁes
caseOf
Lemma 1 
A
Proof 1
B
Lemma 2 
C
Proof 2
D
justiﬁes
caseOf
uses
Figure 14. Example of a loop in the GoTO (DG left, GoTO right)
Figure 14 demonstrates another situation of a cycle in a DRa annotated text.
The problem is mainly, that lemma 1 uses lemma 2 but the proof of lemma 2
uses a part of the proof of lemma 1. This situation would end up in a deadlock
when processing the GoTO e.g. when producing the proof skeleton. We see a cycle
between the nodes A, C, D, F, B and A with the edges A-C, C-D, D-F, F-B, and
B-A. Here we also see why we do not need to add incoming edges to the parent
nodes.
For node F we have an incoming edge but due to the direction of the
“childOf” edge from F to B, we can use the transivity. In both examples, an error
would be returned with the corresponding nodes and edges.
The design and implementation of DRa were the subject of Retel’s thesis [Retel,
2009]. Further additions have since been carried out by Zengler as reported here.
5
CONNECTING MATHLANG TO FORMAL FOUNDATIONS
Current approaches to formalising CML texts generally involve rewriting the text
from scratch; there is no clear methodology in which the text can gradually change
in small steps into its formal version.
One of MathLang’s goals is to support
formalising a text in small steps that do not require radically reorganising the text.
Also, a text with fully formal content should continue to be able to be presented
in the same way as a less formal version originally developed by a mathematician.
We envision formalisation as working by adding additional layers of information to
a MathLang document to support embedding formal proofs. Ideally, there should
be ﬂexible control over how much of the additional information is presented to the
reader; the additional information could form part of the visual presentation, or
could exist “behind the scenes” to provide assurance of correctness.
As part of the goal of supporting formalisation in MathLang, we desire to keep
MathLang independent of any particular formal foundation. However, as proofs
embedded in a MathLang document become more formal, it will be necessary to
tie them more closely to a particular proof system.
It might be possible that
fully formal documents could be kept independent of any particular foundation
by allowing the most formal parts of a document to be expressed redundantly in
multiple proof systems. (This is similar in spirit to the way the natural language
portion of a document might be expressed simultaneously in multiple natural lan-
guages.) In this section we report on a methodology and software for connecting
a MathLang document with formal versions of its content. We mainly concen-
trate on a formal foundation in Coq but for Mizar see [Kamareddine et al., 2007b;
Retel, 2009] and for Isabelle see [Lamar, 2011]. Our formalisation into Mizar, in-
volved constructing a skeleton of a Mizar document (e.g. ﬁgure 15) from a Math-

370
Fairouz Kamareddine, Joe Wells, Christoph Zengler and Henk Barendregt
Lemma 1.
For m, n ∈N one has:
m2 = 2n2 =⇒m = n = 0
A
Proof.
Deﬁne on N the predicate:
P (m) ⇐⇒∃n.m2 = 2n2 & m > 0.
E
Claim.
P (m) =⇒∃m′ < m.P (m′).
F
Indeed suppose m2 = 2n2 and m > 0. It follows that m2 is
even, but then m must be even, as odds square to odds. So
m = 2k and we have 2n2 = m2 = 4k2 =⇒n2 = 2k2 Since
m > 0, if follows that m2 > 0, n2 > 0 and n > 0. Therefore
P (n). Moreover, m2 = n2 + n2 > n2, so m2 > n2 and hence
m > n. So we can take m′ = n.
G
By the claim ∀m ∈N.¬P (m), since there are no inﬁnite descending sequences
of natural numbers.
Now suppose m2 = 2n2
with m ̸= 0. Then m > 0 and hence P (m). Contradiction.
H
Therefore m = 0. But then also n = 0.
I
■
B
Corollary 2.
√2 /∈Q
C
Proof.
Suppose √2 ∈Q, i.e.
√2 = p/q with p ∈Z, q ∈Z −{0}. Then
√2 = m/n with m = |p|, n = |q| ̸= 0. It follows that m2 = 2n2. But then
n = 0 by the lemma. Contradiction shows that √2 /∈Q.
■
D
justifies
justifies
uses
uses
justifies
uses
uses
subpartOf
subpartOf
18Lemma:
19 proof
21
defpred
22
Claim:
23
proof
54
end;
63
per cases;
64
suppose
71
end;
72
suppose
77
end;
78 end;
80Corollary:
81
proof
95
end;
Figure 15. Generating a Mizar Text-Proper skeleton from DRa and CGa
Lang document, and then completing the Mizar skeleton separately.
A Mizar
document consists of an Environment-Declaration and a Text-Proper. In Mizar,
the Environment-Declaration is used to generate the Environment which has the
needed knowledge from MML (Mizar’s Mathematical Library). The Text-Proper
is checked for correctness using the knowledge in the Environment.
In this paper, we present the automation of the skeleton generation for arbitrary
theorem provers and we give a generic algorithm for transforming the DRa tree
into a proof skeleton. Since at this stage of formalisation we do not want to tie
to any particular foundation, the algorithm is highly conﬁgurable which means it
takes the desired theorem prover as an argument and generates the proof skeleton
within this theorem prover. The aim of this skeleton generation is once again to
stay as close as possible to the mathematician’s original CML text. But due to
certain restrictions for diﬀerent theorem provers the original order cannot always
be respected. We give some classical examples when this can happen:
• Nested lemmas/theorems: Sometimes mathematicians deﬁne new lemmas
or theorems inside proofs. Not every theorem prover can handle such an
approach (e.g. Coq). In the case of such theorem provers, it is necessary to
“de-nest” the theorems/lemmas.
• Forward references: Sometimes a paper ﬁrst gives an example for a theorem
before it states the theorem.
Some theorem provers (e.g. Mizar) do not
support such forward references. The text has to be rewritten so that it only
has backward references (i.e. to already stated mathematical constructs).
• Outsourced proofs: The practise in mathematical writing is to outsource in
the appendix complex proofs that are not mandatory for the main results.

Computerising Mathematical Text
371
When formalising such texts, these proofs need to be put in the right place.
The algorithm for re-arranging the parts of the text and generating the proof
skeleton performs reordering only when necessary for the theorem prover at hand.
5a
The generic automated Skeleton Generation Algorithm (gSGA)
The proof skeleton generation algorithm takes as arguments (cf. table 6):
1. the input MathLang XML ﬁle with DRa annotations; and
2. a conﬁguration ﬁle (in XML format) for the theorem prover.
Table 6. The skeleton generation algorithm
This algorithm works on the DRa tree as seen in the last section. A DRa node
can have one of three states: processed (black), in-process (grey) and unprocessed
(white). A processed node has already been translated into a part of the proof
skeleton, a node in-process is one that is being checked, while an unprocessed node
is still awaiting translation. This information allows to identify which nodes have
already been translated and which are still to be translated. The method for gen-
erating the output of a single node is shown in algorithm 2.
The algorithm starts
while foundwhite do
foreach child c of the node do
if c is unprocessed && isReady(c) then
processNode(c);
generateOutput(c);
foundwhite := true;
break;
end
end
end
Algorithm 2: generateOuput(Node node)
at the Document root node, recursively searches for nodes in need of processing,
and processes them so that the node at hand is translated and added to the proof
skeleton. The decision whether a node is ready to be processed or not is only
dependent on the GoTO of the DRa tree. A node is ready to be processed if:
1. It has no incoming ≺edges (in the GoTO) of unprocessed (white) nodes.
2. All its children are ready to be processed.
3. If it is a proved node: its proof is ready to be processed.
Algorithm 3 tests these three properties of a node and returns the result. It is
important when checking if each child of the n children is ready, to perform the
test n times because a rearrangement can also be required for the children. If there

372
Fairouz Kamareddine, Joe Wells, Christoph Zengler and Henk Barendregt
foreach incoming edge e of the node do
if type of e is ≺&& source of e is unprocessed (white) then
return false
end
end
mark node n as grey;
n = number of children of the node;
for 1..n do
foreach child c of the node do
if c is not processed && isReady(c) then
mark c as grey;
break;
end
end
end
if still a white node is among the children of the node then
reset all grey nodes back to white;
return false
end
if node is a proved node then
proof = proof of the node;
if not isReady(proof) then
reset all grey nodes back to white;
return false
end
end
reset all grey nodes back to white;
return true
Algorithm 3: isReady(Node node)
are still white children after n steps, then the children cannot be yet processed
and so the node cannot be processed.
To illustrate algorithm 3 we look at a (typical and not well structured) mathe-
matical text whose DG and GoTO edges are in ﬁgure 16.
Figure 16. To illustrate Skeleton generation (DG at top, GoTO at bottom)
The root node of the document can be marked as processed and the algorithm starts at this node. The
ﬁrst child is Lemma 1. Criterion 1) is fulﬁlled, since the node has no incoming ≺edges in the GoTO.
Criterion 2) is fulﬁlled because the node has no children. For criterion 3) Proof 1 has to be ready to
be processed before we can mark Lemma 1 as ready to be processed. Proof 1 has no incoming ≺edges.
So criterion 1) is fulﬁlled. For criterion 2) the children of the proof have to be ready to be processed.
Deﬁnition 1 is ready, but the proof of Claim 1, Proof C1 has an incoming node of an unprocessed node
(Lemma 2). So Claim 1 is not ready and hence, neither are Proof 1 and Lemma 1.

Computerising Mathematical Text
373
The next Node to check is Lemma2. Criteria 1) and 2) are fulﬁlled, for criterion 3) the proof Proof 2 has
to be ready to be processed. Criteria 1) and 3) of the proof are fulﬁlled, so its children are ready to
be processed. The ﬁrst child that can be processed is Deﬁnition 2. So it is marked as in-process (grey).
In a second run of the for loop for checking the children of Proof 2, Claim 2 and its proof are now ready,
because Deﬁnition 2 is not white anymore but grey. This situation is the reason, why we must perform
the check whether the n children of a node are ready n times exactly.
Since now a node has been processed, the algorithm starts again with the ﬁrst white node. So Lemma
1 is checked again. Now the children of its proof can be processed because Lemma 2 is now processed
and does not prevent the processing of Proof C1.
Output:
Lemma 2
Proof 2
Definition 2
Claim 2
Proof C2
Since now all the children of Proof 2 are ready, the complete proof is ready and so is Lemma 2. The
grey ﬂags are unassigned and the output for Lemma 2 is generated. In this step all nodes Lemma 2,
Proof 2, Claim 2, Proof C2 and Deﬁnition 2 are permanently marked as processed (black).

374
Fairouz Kamareddine, Joe Wells, Christoph Zengler and Henk Barendregt
Output:
Lemma 1
Proof 1
Definition 1
Claim 1
Proof C1
At the end Lemma 1 and its proof can be processed. The ﬁnal order of the nodes is:
Lemma 2
Proof 2
Definition 2
Claim 2
Proof C2
Lemma 1
Proof 1
Definition 1
Claim 1
Proof C1
We see that with this order no node references other nodes which are not already translated.
Lemma 2 is translated ﬁrst. Its proof follows immediately. Deﬁnition 2 is reordered, because
Claim 1 and its proof refer to it. So it has to be written in front of them. Lemma 1 can then
be translated because now Lemma 2 which it refers to, is already translated.
5b
The conﬁguration of gSGA
The transformation of the DRa annotated text into a proof skeleton has two steps:
• Reorder the text to satisfy the constraints of the particular theorem prover.
• Translate each DRa annotation into the language of the theorem prover.
The conﬁguration ﬁle for a particular theorem prover for the gSGA reﬂects these
two steps: there is a dictionary part and a constraints part. The dictionary con-
tains a rule for each mathematical or structural role of DRa. A single DRa node
has two important properties: a name and a content. This information is used in
the translation. Within the conﬁguration ﬁle we can refer to the name of a node
with %name and to the body with %body. A new line (for better readability) can
be inserted with %nl. Consider the example of the DRa node from ﬁgure 8. The
role of this node is declaration and its name is decA. The body of this node is the
sentence Let A be a set or its CGa annotation. A translation into Mizar could be:
reserve <body of decA> ;
The rule for this translation would be:
reserve %body ;
Such a kind of declaration in Coq would be:
Variable <body of decA> .
And the for this translation would be:
Variable %body .
Here, we let a single rule be embedded in an XML tag whose attribute ”name” is
the corresponding keyword:
<skeleton:keyword name="declaration">
reserve %body ;
</skeleton:keyword>

Computerising Mathematical Text
375
The constraints section of the conﬁguration ﬁle for a theorem prover conﬁgures two
main properties: the allowance of forward properties and of nested mathematical
constructs. Forward references can be allowed via the tag:
<skeleton:forwardrefs>true</skeleton:forwardrefs>
Changing the content of the tag to ”false” forbids forward references. If there is
no such tag, the default value is ”false”.
For a conﬁguration of nested constructs there are two possibilities:
• Either allow in general the nesting of constructs deﬁning those exceptions
for which nesting is not allowed;
• Or forbid in general the nesting of constructs deﬁning those exceptions for
which nesting is allowed.
The next conﬁguration allows nesting in general but not for deﬁnitions and axioms:
<skeleton:nesting>true</skeleton:nesting>
<skeleton:nest role="definition">false</skeleton:nest>
<skeleton:nest role="axiom">false</skeleton:nest>
5c
The ﬂattening of the DRa graph
The next question we have to deal with, is how to perform changes to the tree
when certain nestings are not allowed.
We call this a ﬂattening of the graph,
because certain nodes are removed from their original position and inserted as
direct children of the DRa top-level node. Algorithm 4 achieves this eﬀect.
foreach (child c of the node) do
ﬂattenNode(c);
if c cannot be nested then
nodelist := transitive closure of incoming nodes of c;
foreach node n of nodelist do
remove n of list of children of node;
add n in front of node as a sibling;
end
end
end
Algorithm 4: ﬂattenNode(Node node)
We refer to every child of the DRa top-level node as a node at level 1. Every
child of such a node is at level 2 and so on. If a mathematical role must not be
nested, it can only appear at level 1. So we check for each node at a level greater
than level 1, if its corresponding mathematical role can be nested. If not, then the
node and all its required siblings are removed from this level and put in front of
their parent node. Since there is no “childOf” relation between this no-longer-child
and its parent node, the relation between child and parent changes form ⪯to ≺.
The required sibling nodes are determined in the GoTO. When a node is moved
in front of its parent node, there is a ≺edge between this node and its former
parent. Each sibling of the removed node from whom there is a incoming node
must be moved with the node. This includes its children or - for a proved node -
its proof. Since for these children we have to move the related nodes too, we can

376
Fairouz Kamareddine, Joe Wells, Christoph Zengler and Henk Barendregt
Figure 17. A ﬂattened graph of the GoTO of ﬁgure 16 without nested deﬁnitions
Figure 18. A ﬂattened graph of the GoTO of ﬁgure 16 without nested claims
build the transitive closure over the incoming nodes of the node which has to be
moved. All nodes in this closure have to be relocated in front of the parent node.
We demonstrate this algorithm again on the example from ﬁgure 16. For a
ﬁrst demonstration we assume that the nesting of deﬁnitions is not allowed. So
Deﬁnition 1 and Deﬁnition 2 have to be removed from level 2 and be relocated
in front of their parent nodes. The transitive closure over incoming edges in the
GoTO yields no new nodes for removing (because the deﬁnitions have no incoming
edges in the GoTO). The resulting new ﬂattened graph can be seen in ﬁgure 17.
We see that the two deﬁnition are now at level 1 and their edges to their former
parent nodes have changed from ⪯to ≺. The output for this graph according to
the algorithm from the last section is given on the left-hand side of table 7.
Definition 1
Definition 2
Lemma 2
Proof 2
Claim 2
Proof C2
Lemma 1
Proof 1
Claim 1
Proof C1
Definition 2
Claim 2
Proof C2
Lemma 2
Proof 2
Claim 1
Proof C1
Lemma 1
Proof 1
Definition 1
Table 7. Outputs of the graphs of ﬁgures 17 (left-hand side) and 18 (right-hand
side)
On the other hand, if we allow deﬁnitions to be nested but forbid nested claims,
we get the graph of ﬁgure 18. The ﬁrst claim which is found in the graph is Claim 1.
The transitive closure yields that Proof C1 needs also to be removed since there is
a ↔edge to the claim. The second claim which is found is Claim 2. The transitive
closure yields again that its proof as well as Deﬁnition 2 have to be removed.
The output for this graph is given on the right-hand side of table 7.

Computerising Mathematical Text
377
6
CONNECTING MPL TO FORMAL FOUNDATION
6a
Newman’s Lemma
As a case study we specify for Newman’s Lemma a feasible interactive mathemat-
ical proof development. It should be accepted by an interactive proof assistant,
if these are to be accepted by a working mathematician. Table 8 gives an actual
proof development in Coq for the main lemma is given. We start with the informal
statement and proof.
Let A be a set and let R be a binary relation on A. R+ is the transitive closure
of R and R∗is the transitive reﬂexive closure of R.
Conﬂuence of R, notation CR(R) (Church-Rosser property), is deﬁned as follows
(the notion cr(R, a) denotes conﬂuence from a ∈A). WCR(R) stands for weak
conﬂuence.
1. crR(a) ⇐⇒∀b1, b2 ∈A.[aR∗b1 /\ aR∗b2 ⇒∃c.b1R∗c /\ b2R∗c].
2. CR(R) ⇐⇒∀a ∈A.crR(a).
3. WCR(R) ⇐⇒∀a, b1, b2 ∈A.[aRb1 /\ aRb2 ⇒∃c.b1R∗c /\ b2R∗c].
Newman’s lemma states that for well-founded relations weak conﬂuence implies
strong conﬂuence. The notion of well-foundedness is formulated as the possibility
to prove statements by transﬁnite induction. Let P ∈P(A).
4. INDR(P) ⇐⇒∀a ∈A.(∀y ∈A.aRy ⇒P(y)) ⇒P(a).
5. WF(R) ⇐⇒∀P ∈P(A).[INDR(P) ⇒∀a ∈A.P(a)].
Lemma 3 (Main Lemma.). WCR(R) ⇒INDR(crR).
Proof.Assume WCR(R). Remember INDR(crR) ⇔
(∀a : A.(∀y : A.a R y→cr(R, y))→(crR(a)).
Let a : A and assume
∀y : A.a R y→crR(y),
(IH)
in order to show crR(a), i.e.
∀b1, b2 : A.a R∗b1 /\ a R∗b2→(∃c A.b1 R∗c /\ b2 R∗c).
So let b1, b2 : A with a R∗bi, in order to show ∃c.bi R∗c.
If a = b1 or a = b2, then the result is trivial (take c = b2 or c = b1 respectively).
So by lemma p7 (below) we may assume a R+ bi,
which by lemma p6 (below) means a R xi R∗bi, for some x1, x2.

378
Fairouz Kamareddine, Joe Wells, Christoph Zengler and Henk Barendregt
a
// //

b2

b1
// // c
a
//

x2

/ // b2

x1
// //

x

b1
// // b
// // c
By WCR(R) there is an x such that xi R∗x.
By (IH) one has crR(x1). So x R∗b /\ b1 R∗b, for some b.
Again crR(x2). As x2 R∗x R∗b one has b R∗c /\ b2 R∗c, for some c.
Then b1 R∗b R∗c and we are done.
■
PROPOSITION 3 (Newman’s Lemma). WCR(R) /\ WF(R) ⇒CR(R).
Proof.By WCR(R) and the main lemma we have INDR(crR). Hence by WF(R)
it follows that for P(a) = crR(a), one has ∀a ∈A.crR(a). This is CR(R).
■
Now we will start a proof development for Newman’s lemma.
Variable A:Set.
Definition Bin:=[B:Set](B->B->Prop).
Inductive TC [R:(Bin A)]: (Bin A) :=
TCb:
(x,y:A)(R x y)->(TC R x y)|
TCf:
(x,y,z:A)((R x z)->(TC R z y)->(TC R x y)).
Inductive TRC [R:(Bin A)]: (Bin A) :=
TRCb:
(x:A)(TRC R x x)|
TRCf:
(x,y,z:A)((R x z) -> (TRC R z y)->(TRC R x y)).
Definition Trans [R:(Bin A)]: Prop:=
(x,y,z:A)((R x y)->(R y z)->(R x z)).
Definition IND [R: (Bin A);P:(A->Prop)]:
Prop :=
((a:A)((y:A)(a R y)-> (P y))->(P a)).
Definition cr [R:(Bin A);a:A]:=
(b1,b2:A)(TRC R a b1)/\(TRC R a b2)->(EX c:A|(TRC R b1 c)/\(TRC R b2 c)).
Definition CR [R:(Bin A)]:=(a:A)(cr R a).
Definition WCR [R:(Bin A)]:=
(a,b1,b2:A)(a R b1)->(a R b2)->(EX c:A|(TRC R b1 c)/\(TRC R b2 c)).
Definition WF [R:(Bin A)]:Prop:= (P:A->Prop)(IND R P)->(a:A)(P a).

Computerising Mathematical Text
379
Variable R:(Bin A).
Lemma p0:
(x,y:A)((R x y) -> (TC R x y)).
Lemma p1:
(x,y:A)((R x y) -> (TRC R x y)).
Lemma p2:
(x,y:A)((TC R x y) -> (TRC R x y)).
Lemma p3:
(Trans (TC R)).
Lemma p4:
(Trans (TRC R)).
Lemma p5:
(x,y,z:A)(R x y)->(TRC R y z)->(TRC R x z).
Lemma p6:
(x,y:A)((TC R x y)->(EX z:A | (R x z)/\(TRC R z y))).
Lemma p7:
(x,y:A)((TRC R x y)-> (eq A x y)\/(TC R x y)).
The proof-script for these lemmas are not shown. The main lemma is as follows.
Lemma main :
(WCR R)->(IND R (cr R)).
The proof-script in Coq is given in table 8.
Now we will give an interactive
mathematical proof script, for which we claim that it should essentially be accept-
able by a mathematician-friendly proof-assistant. On the left we ﬁnd the math-
ematical script, on the right the proof-state. These may contain some Coq-like
statements, like “Intros”, but these disappear and are replaced by mathematical
statements. For an interactive version see www.cs.kun.nl/~henk/mathmode.{ps,
dvi}. The dvi version has to be viewed in advi obtainable from pauillac.inria.
fr/~miquel.
First we introduce some user-friendly notation.
Notation 4. For a,b:A we write
(i) a R b := (R a b).
(ii) a R+ b := (TC R a b).
(iii) a R* b := (TRC R a b).
Proof.
Assume WCR(R). Remember IND. Let a:A. Assume
(y:A)((aRy)->(cr R y)).
(IH)
Remember cr.
Let a,b1,b2:A. Assume a R* bi, i=1,2.
We have
[a=b1 \/ a R+ b1],
[a=b2 \/ a R+ b2],
by lemma p6.
Case a=b1, take c=b2.
Trivial.
Hence wlog (a R+ b1).
Case a=b2, take c=b1.
Trivial.
Hence wlog (a R+ b2).
Therefore (EX xi:A|a R xi R* bi), i=1,2, by lemma p7.
Pick x1.
Pick x2.
We have (EX x.xi R* x), i=1,2, by (WCR R). Pick x.

380
Fairouz Kamareddine, Joe Wells, Christoph Zengler and Henk Barendregt
We have (cr R x1), by IH. Hence
(EX b.b1 R* b /\ x R* b).
Pick b.
Moreover (cr R x2), by IH. Hence
(EX c.b R* c /\ b2 R* c),
by x2 R* b.
Pick c.
Since b1 R* c, by (Trans R*), we have
(bi R* c), i=1,2.
Thus c works.
QED
Newman’s Lemma.
WCR(R)/\WF(R)->CR(R).
Proof.
Assume WCR(R) and WF(R). Then (IND R( cr R)),
by WCR(R) and main.
Remember CR and WF. We have
(P:(A->Prop))((a:A)((y:A)(a R y)-> (P y))->(P a)).
(+)
Apply (+) to (cr R). Then CR(R). QED
6b
Towards a Mathematical Proof Language MPL
We will now sketch rather loosely a language that may be called MPL: Mathe-
matical Proof Language. The language will need many extensions, but this kernel
may be already useful.
DEFINITION 5. The phrases used in MPL for the proposed proof-assistant with
interactive mathematical mode belong to the following set.
Assume B
Then B [, by C]
Towards A
Suffices
Remember t
Wlog B, [since B \/ C]
Let x:D
Pick [in L] x
and
Case B
QED
Take x=t [in B]
Apply B to t
As to
Here A,B, C are propositions in context Gamma, D is a type, x is a variable and
t is a term of the right type. “Wlog” stands for “Without loss of generality”.
DEFINITION 6 (Synonyms).
Suffices = In order to show = We must show = Towards;
Let = Given;
Then = We have = It follows that = Hence = Moreover = Again;
and = with;
by = since.
Before giving a grammar for tactic statements we will give their semantics.
They have a precise eﬀect on the proof-state. In the following deﬁnition we show
what the eﬀect is of a statement on the proof-state. In some cases the tactic has
a side-eﬀect on the proof-script, as we saw in the case of Newman’s lemma.

Computerising Mathematical Text
381
DEFINITION 7.
(i) A proof-state (within a context Gamma) is a set of statements Delta and a
statement A, such that all members of Delta are well-formed in Gamma and
A is well-formed in Gamma, Delta. If the proof-state is (Delta;A), then the
goal is to show Delta ⊢A.
(ii) The initial proof-state of a statement A to be proved is of course (∅;A).
(iii) A tactic is map from proof-states to a list of proof-states, usually having a
formula or an element as extra argument.
DEFINITION 8.
Assume C (Delta,C->B)
=
(Delta,C;B), and ‘‘Towards B’’
may be left in the script.
Let a:D (Delta,(x:D.P))
=
(Delta,a:A;P[x =a]).
Remember name (Delta;A)
=
(Delta;A’), where A’ results
from A by unfolding the defined
concept ‘name’.
This can be
applied to an occurrence of
‘name’, by clicking on it.
Other
occurrences remain closed but
become transparent (as if opened).
Pick [in L] x (Delta,L;A)
=
(Delta,x:D,B(x);A), where L is
a formula reference of (EX x:D.B).
Take x=name (Delta;EX x:D.A)
=
(Delta;A[x =name]),
if Delta|- name:D.
Apply B to name (Delta;A)
=
(Delta,P[y =name];A), where B of
the form ((y:D).P) is in Delta.
Case B (Delta;A)
=
(Delta,B;A),(Delta,C;A),
if B \/ C in Delta; the second
proof-state represents
the next subgoal.
As to Bi (Delta;B0 /\ B1)
=
(Delta;Bi),(Delta;B(1-i)), the
second proof-state represents
the next subgoal;
As to B (Delta; B)
=
(Delta; B);
QED (Delta,B)
=
<proof terminated> if B in Delta.
In all cases nothing happens if the side conditions are not satisﬁed. One should
be able to refer to a statement C in two ways: either by naming C directly of by
referring to a label for C, like “IH” in the proof of the main lemma above. We
say that L is a formula reference of formula B if L is B or if L is a label for B.
Labels are sometimes handy, but they should also be suppressed in order to keep
the proof-state clean.
If the argument of a tactic occurs at several places the
system should complain. Then reference should be made to a unique label. It is
assumed that proof-states (Delta,A) are in normal form, that is, if B /\ C is in
Delta, then it is replaced by the pair B,C. If the ﬁnal QED is accepted, then all

382
Fairouz Kamareddine, Joe Wells, Christoph Zengler and Henk Barendregt
the statements in the proof that did not have an eﬀect on the proof-state will be
suppressed in the ﬁnal lay-out of the proof (or may be kept in color orange as an
option in order to learn where one did superﬂuous steps).
The following tactics require some automated deduction. If the proof-assistant
cannot prove the claimed result, an extra proof-state will be generated so that this
result will be treated as the next subgoal.
DEFINITION 9.
[Since B\/C] wlog C (Delta;A)
=
(Delta,C;A), if B\/C in Delta
and the assistant can establish
Delta|-B->A.
Then B[, by C] (Delta;A)
=
(Delta,B;A), if C is a known
lemma and the assistant
can establish Delta,C|-B.
Suffices B (Delta;A)
=
(Delta;B) and the assistant
can establish Delta|-B->A.
May assume B (Delta,A)
=
(Delta,B;A) if the assistant
can establish Delta|-~B->A and
Delta|- B \/~B.
The tactic language MPL is deﬁned by the following grammar.
formref
:=
label | form
form+
:=
formref | form+ and formref
tactic
:=
Assume form+ | Towards form | Remember name |
Let var:set | Pick [in formref] var | Case form |
Take var = term [in formref ] |
Apply formref to term | Then form[, by form+] |
Suffices formref | Wlog form[, since form\/form]
tactic+
:=
tactic.
| tactic, tactic+ | tactic.
tactic+
Here label is the proof-variable, used as a name for a statement (like IH in the
proof of the main lemma), form is a Gamma, Delta inhabitant of Prop, name is
any deﬁned notion during the proof development, and var is an variable.
An extension of MPL capable of dealing with computations will be useful.
We have A(t).
Then A(s), since t=s.
Another one:
Then t=s, by computation.
It would be nice to have this in an ambiguous way: computation is meant to
be pure conversion or an application of reﬂection. This corresponds to the actual
mathematical usage:

Computerising Mathematical Text
383
5!=120, by computation.
In a commutative ring, (x + y)2 = x2 + 2xy + y2, by computation.
In type theory the ﬁrst equality would be an application of the conversion rule,
but for the second one reﬂection, see e.g. [Constable, 1995], is needed.
6c
Procedural statements in the implementation of MPL
As we have seen in section 6a it is handy to have statements that modify the proof
state, but are not recorded as such. For example if the proof state is
(Delta; (x : D)(A(x)−> B(x)),
then Intros is a fast way to generate
Let x:D. Assume A(x), in order to prove B(x).
in the proof. Another example is Clear L which removes formula L in the assump-
tions of the current subgoal. Also renaming variables is useful, as some statements
may come from libraries and have a “wrong” choice of bound variables.
7
A FULL FORMALISATION IN COQ VIA MATHLANG: CHAPTER 1 OF
LANDAU’S “GRUNDLAGEN DER ANALYSIS”
Landau’s “Grundlagen der Analysis” [Landau, 1951] remains the only book which
has been fully formalised in a theorem prover [van Benthem Jutting, 1977b]. This
section summarises the encoding of the ﬁrst chapter (natural numbers) of Landau’s
book into all aspects of MathLang up to a full formalisation in Coq. We give a
complete CGa, TSa and DRa annotation for the chapter, we generate a proof
skeleton automatically with the gSGA forboth Mizar and Coq and then we give a
complete formalised version of the chapter in Coq. To accomplish this, we have
used the MathLang TEXMACS plugin to annotate the existing plaintext of the book.
To clarify the path we took, we look once again at the overall diagram of the
diﬀerent paths in MathLang (ﬁgure 1). We ﬁrst used path a⃝and annotated the
complete text with CGa, TSa and DRa annotations with the help of the MathLang
TEXMACS plugin. The second step was to automatically generate a proof skeleton
of the annotated text. With the help of the proof skeleton and the CGa annotations
we fully formalised the proofs in Coq completing the paths d⃝and e⃝. The ﬁnal
result is a fully formalised version of the ﬁrst chapter of Landau’s book in Coq.
7a
CGa and TSa annotations
The Preface
In the preface of a MathLang document we introduce symbols that are not deﬁned
in the text but are used throughout it. These are often quantiﬁers or Boolean
connectives like ∧or ∨. These symbols are often pre-encoded in theorem provers
(e.g. Coq has special symbols for the logical and, or, implication, etc.). The preface

384
Fairouz Kamareddine, Joe Wells, Christoph Zengler and Henk Barendregt
of the ﬁrst chapter of Landau’s book consists of 17 diﬀerent symbols as given in
table 7a. Two functions deserve further explanation:
1. The “is a” function is used to express that a particular term is an instance
of a noun. E.g. the ﬁrst axiom of the book is that 1 is a natural number, so
the encoding of this axiom is
<isa> <1>1 is a <<natural number>>natural number
2. The “index” function is used to express a notion in the style of ab = c which
can be deﬁned as a function index(a, b) = c. So the index function has two
terms as argument and yields a term as a result.
The ﬁrst section
The ﬁrst section of the ﬁrst chapter introduces the natural numbers, equality on
natural numbers and ﬁve axioms (an extension of the Peano axioms). We intro-
duce a noun <natural numbers>natural numbers and the set <N>N of natural numbers.
Equality <eq> <#> = <#> and inequality <neq> <#> = <#> between natural numbers
are declared rather than deﬁned. Three properties of equality are encoded. We
will show one encoding of these to recapitulate TSa annotations with sharing and
to see how to use the symbols given in the preface. The original statement is
x = x for every x
We see that this is a universal quantiﬁcation of x and a well formed equivalent
statement would be: ∀x(x = x). Since the positions are swapped in Landau’s text
we use the position souring. The souring annotation of this statement is:
<forall> <2>x = x
for every
<1> x
This yields the ﬁnal statement
<> <forall> <2> <eq> <x>x = <x>x
for every
<1> <> <x>x
Next we show how to encode axiom 2 showing that “wordy” parts of a text can
also be annotated, not only mathematical statements. The original statement is:
For each x there exists exactly one natural number, called the successor of x,
which will be denoted by x′
The “for each” can be translated with a universal quantiﬁer, the “exactly one”
with the ∃! quantiﬁer. So we get the general structure:
The complete statement can be e.g. encoded corresponding to the following formal
statement ∀x(∃!x′(succ(x) = x′)):

Computerising Mathematical Text
385
Sections 2 - 4
Within the next sections, addition (section 2), ordering (section 3) and multi-
plication (section 4) are introduced. There are 36 theorems with proofs and 6
deﬁnitions: addition, greater than, less then, greater or equal than, less or equal
than and multiplication. There are many simple structured theorems like that of
ﬁgure 19. We want to examine our way of annotating these theorems. The main
Figure 19. Simple Theorem of the second section
theorem x + y = y + x is annotated in a straightforward manner. x and y are an-
notated as terms, plus as a function, taking two terms as arguments and yielding
a term as a result. The equality between these terms is a statement. Since we did
not declare x and y in the preface or in a global context we do this with a local
scoping. This information is added in the ﬁrst annotated line where we declare x
and y as terms and put these two annotations into a context which means that
this binding holds within the whole step.
Figure 20. Souring in chains of equations
Landau often used chains of equations for proofs as in this proof for the equality
of x(y + z′) and xy + xz′ in the proof of Theorem 30 of the ﬁrst chapter:
x(y + z′) = x((y + z)′) = x(y + z) + x = (xy + xz) = x = xy = (xz + x) = xy + xz′
Here we beneﬁt from our souring methods - especially the sharing of variables (See
ﬁgure 20). There are also often hidden quantiﬁcation like the one in the example
x = x for every x, where we need the souring for swapping positions. These TSa
functionalities save a lot of time in annotating mathematical documents.
For some theorems we use the Boolean connectives although they are not men-
tioned explicitly in the text. E.g. Theorem 16 states:
If x ≤y, y < z or x < y, y ≤z
then x < z
We annotate the premise of the theorem as a disjunction of two conjunctions as
seen in ﬁgure 21. Another use of Boolean connectives is when we have formulations
like “exactly one of the following must be the case...”. There we use the exclusive
or ⊕to annotate the fact that exactly one of the cases must hold. We deﬁned
the exclusive or in the preface and therefore have to take care that we ﬁnd a
corresponding construct in the used theorem prover (see table 7a).

386
Fairouz Kamareddine, Joe Wells, Christoph Zengler and Henk Barendregt
Figure 21. The annotated Theorem 16 of the Landau’s ﬁrst chapter
Figure 22. The DRa tree of sections 1 and 2 of chapter 1 of Landau’s book
7b
DRa annotation
The structure of Landau’s “Grundlagen der Analysis” is very clear: in the ﬁrst sec-
tion he introduces ﬁve axioms. We annotate these axioms with the mathematical
role “axiom”, give them the names “ax11” - “ax15” and classify them as unproved
nodes. In the following sections we have 6 deﬁnitions which we annotate with the
mathematical role “deﬁnition”, give them names “def11” - “def16” and classify as
unproved nodes. We have 36 proved nodes with the role “theorem”, named “th11”
- “th136” and with proofs “pr11” - “pr136”.
Some proofs are partitioned into an existential part and a uniqueness part. This
partitioning can be useful e.g. for Mizar where we have keywords for these parts of
a proof. In the Coq formalisation, we used this partitioning to generate two single
proofs in the proof skeleton which makes it easier to formalise. Other proofs consist
of diﬀerent cases which we annotate as unproved nodes with the mathematical
role “case”. This can be translated in the Mizar “per cases” statement or in single
proofs in Coq. The DRa tree for sections 1 and 2 can be seen in ﬁgure 22.
The relations are annotated in a straightforward manner. Each proof justiﬁes
its corresponding theorem. Some of the axioms depend on each other. Axiom
5 (“ax15”) is the axiom of induction. So every proof which uses induction, uses
also this axiom. Deﬁnition 1 (“def11”) is the deﬁnition of addition. Hence every
node which uses addition also uses this deﬁnition.
Some theorems use other
theorems via texts like: “By Theorem ...”. In total we have 36 justiﬁes relations,
154 uses relations, 6 caseOf, 3 existencePartOf and 3 uniquenessPartOf relations.
Figures 23 and 24 give the DG and GOTO of sections 1 and 2 resp. of the whole
book. The DGs and GOTOs are automatically produced from the DRa annotated
text. There are no errors or warning in the document which means we have no
loops in the GoTO, no proofs for unproved nodes, no double proofs for a node and
no missing proofs for proved nodes.
7c
Generation of the proof skeleton
Since there are no errors in the GoTO, the proof skeleton can be produced without
warnings. We have 8 mathematical roles in the document: axioms, deﬁnitions,
theorems, proofs, cases, case, existenceParts and uniquenessParts. We make a

Computerising Mathematical Text
387
Figure 23. DG (top) and GOTO (bottom) of sections 1 and 2 of chapter 1 of
Landau’s book
distinction between cases and case because e.g. in Mizar we have a special keyword
introducing cases (per cases;) and then keywords for each case (suppose ...).
So we annotated the cases as child nodes of the case node. Table 10 gives an
overview of the rules that were used to generate the Mizar and the Coq proof
skeleton. Since in Coq there are no special keywords for uniqueness, existence or
cases, these rules translate only the body of these nodes and add no keywords.
In table 11 we give a part of the Mizar and Coq skeletons for section 4.7
7d
Completing the proofs in Coq
As we already explained, MathLang aims to remain as independent as possible of
a particular foundation, while in addition facilitating the process of formalising
mathematics in diﬀerent theorem provers. Two PhD students of the MathLang
project (Retel, respectively Lamar) are concerned with the MathLang paths into
Mizar, respectively Isar. In this paper we study for the ﬁrst time the MathLang
path into Coq. We show how the CGa, TSa and DRa encoding of chapter one of
Landau’s book is taken into a fully formalised Coq code.
Currently we use the proof skeleton produced in the last section and ﬁll all the
%body parts by hand. We intend to investigate in the future how parts of the CGa
and DRa annotations can be transformed automatically to Coq. In this section
we explain why the process of formalising a mathematical text into Coq through
MathLang is simpler than the formalisation of the text directly into Coq.
To begin with, we code the preface of the document (see table 7a). The most
complicated section to code in Coq was the ﬁrst one, because we had to translate
the axioms in a way we can use them productively in Coq. We deﬁned the natural
numbers as an inductive set - just as Landau does in his book.
Inductive nats : Set :=
| I : nats
7The complete output of the skeleton for Mizar and Coq for the whole chapter can be found
in the extended article on the web pages of the authors.

388
Fairouz Kamareddine, Joe Wells, Christoph Zengler and Henk Barendregt
Figure 24. DG (top) and GOTO (bottom) of all of chapter 1 of Landau’s book
| succ : nats -> nats
Then we translate axioms 2 - 4 almost literally from our CGa annotations. For
example the annotation of Axiom 3 (“ax13” ) in our document is:
<forall>We always have <> <x>
<neq> <succ> <x>x ′ ̸= <1>1
By just viewing the interpretations of the annotations we get:
forall x (neq (succ(x), 1))
(a)
The automatically generated Coq proof skeleton for this axiom is:
Axiom ax13 : <ax13> .
(b)
Now, we simply replace the <ax13> placeholder of (b) with the literal translation
of the interpretations in (a) to get the valid Coq axiom (this literal translation
could also be done by an algorithm that we plan to implement soon):
Axiom ax13 : forall x:nats, neq (succ x) I .
The other axioms could be completed in a similar way and as seen, this is a very
simple process that can be carried out using automated tools that reduce the bur-
den on the user (the proof skeleton is automated, the interpretations are obtained
automatically from the CGa annotations which are simple to do, and for many
parts of the text, the combination of the proof skeleton with the interpretations
can also be automated).

Computerising Mathematical Text
389
Similarly for the theorems of chapter 1 of Landau’s book, full formalisation is
straightforward: E.g. Theorem 1 is written by Landau as:
If x ̸= y then x′ ̸= y′
Its annotation in MathLang CGa is:
<> <> <> <x>x
<> <y>y If <neq> <x>x ̸= <y>y
then <neq> <succ> <x>x ′ ̸= <succ> <y>y ′
The CGa annotation of the context (called local scoping) can also be seen as the
premise of an implication. So the upper statement can be translated via a simple
rewriting of the interpretations of the annotations to:
decl(x), decl(y) : neq x y -> neq (succ x) (succ y)
And when we compare this line with its Coq translation we see again, it is just
a literal transcription of the interpretation parts of CGa and therefore could be
easily performed by an algorithm.
Theorem th11 (x y:nats) : neq x y -> neq (succ x) (succ y) .
From the 36 theorems of the chapter 28 could be translated literally into their
corresponding Coq theorems.
Now, we want also to look at a simple proof and how it can be translated into
Coq. The encoding of Theorem 2 of the ﬁrst chapter in Coq is
theorem th12 (x:nats) : neq (succ x) x .
Landau proves this theorem with induction. He ﬁrst shows, that 1′ ̸= 1 and then
that with the assumption of x′ ̸= x it also holds that (x′)′ ̸= x′.
Since we deﬁned the natural numbers as an inductive set, we can also do our
proof in the Landau style. We introduce the variable x and eliminate it, which
yields two subgoals that we need to prove. These subgoals are exactly the induction
basis and the induction step.
Proof.
intro x. elim x.
2 subgoals
x : nats
___________________________________(1/2)
neq (succ I) I
_______________________________________________________________(2/2)
forall n : nats, neq (succ n) n -> neq (succ (succ n)) (succ n)
To prove the ﬁrst case, Landau used Axiom 3 which states, that for all x it
holds that x′ ̸= 1. We can just apply this axiom in Coq to prove the ﬁrst case:
apply ax13.
1 subgoal
x : nats
_______________________________________________________________(1/1)
forall n : nats, neq (succ n) n -> neq (succ (succ n)) (succ n)
The next step is to introduce n as natural number and the induction hypothesis:

390
Fairouz Kamareddine, Joe Wells, Christoph Zengler and Henk Barendregt
intros N H.
1 subgoal
x : nats
n : nats
H : neq (succ n) n
______________________________________(1/1)
neq (succ (succ n)) (succ n)
We see that this is exactly the second case of Landau’s proof. He proved this
case with Theorem 1 - we do the same:
apply th11.
1 subgoal
x : nats
n : nats
H : neq (succ n) n
______________________________________(1/1)
neq (succ n) n
And of course this is exactly the induction hypotheses which we already have
as an assumption and we can ﬁnish the proof:
assumption.
Proof completed.
The complete theorem and its proof in Coq ﬁnally look like this:
Theorem th12 (x:nats) : neq (succ x) x .
Proof.
intro x. elim x.
apply ax13.
intros n H.
apply th11.
assumption.
Qed.
We also used another hint for translating from the CGa part to the Coq for-
malisation. When we have a Theorem of the following kind:
Theorem th11 (x y:nats) : neq x y -> neq (succ x) (succ y) .
This is equivalent to:
Theorem th11 : forall x y:nats, neq x y -> neq (succ x) (succ y) .
A proof of such a theorem always starts with the introduction of the universal
quantiﬁed variables, so in this case x and y. In terms of Coq this means:
intros x y.
We can do this for every proof. If it is a proof by induction we can also choose the
induction variable in the next step. For example if we have an induction variable
x we would write:
elim x.
We took the proof skeleton for Coq and extended it with these hints and the
straightforward encoding of the 28 theorems.
The result can be found in the
extended article on the authors’ web pages.
With the help of these hints we
were able to produce 234 lines of correct Coq lines. The completed proof has 957

Computerising Mathematical Text
391
lines. In other words, we could automatically generate one fourth of the complete
formalised text. This is a large simpliﬁcation of the formalisation process, even
for an expert in Coq who can then better devote his attention to the important
issues of formalisation: the proofs.
Of course there are some proofs within this chapter whose translation is not as
straightforward as the proof of Theorem 2 given above. But with the help of the
CGa annotations and the automatically generated proof skeleton, we have com-
pleted the Coq proofs of the whole of chapter one in a couple of hours. Moreover,
the combination of interpretations and proof skeletons can be implemented so that
it leads for parts of the text, into automatically generated Coq proofs. This will
speed further the formalisation and again will remove more burdens from the user.
The complete Coq proof of chapter 1 of landau’s book can again be found in the
extended article on the authors’ web pages.
8
CONCLUSION
MathLang and MPL are long-term projects and we expect there will be years of
design, implementation, and evaluation, followed by repeated redesign, reimple-
mentation, and re-evaluation. There are many areas which we have identiﬁed as
needing more work and investigation. One area is improvements to the MathLang
and MPL software (currently MathLang is based on the TEXMACS editor) to make
it easier to enter information for the core MathLang aspects (currently CGa, TSa
and DRa). This is likely to include work on semi-automatically recognising the
mathematical meaning of natural language text. A second area is further design-
ing and developing the portions of MathLang and MPL needed for better support
of formalisation. An issue here is how much expertise in any particular target
proof system will be needed for authoring. It may be possible to arrange things
in MathLang and MPL to make it easy for an expert in a proof system to col-
laborate with an ordinary mathematician in completing a formalisation. A third
area where work is needed is in the overall evaluation process needed to ensure
MathLang and MPL meet actual needs. This will require testing MathLang and
MPL with ordinary mathematicians, mathematics students, and other users. And
there are additional areas where work will be needed, including areas we have not
yet anticipated.
The MathLang and MPL projects aim for a number of outcomes. MathLang
aims to support mathematics as practised by the ordinary mathematician, which
is generally not formalised, as well as work toward full formalisation. MPL aims
to improve the interactive mathematical mode for proof assistants so that they
can be user-friendly. We expect that after further improvements on the MathLang
and MPL designs and software, writing MathLang documents (without formalis-
ing them) will be easy for ordinary mathematicians. MathLang and MPL aim to
support various kinds of consistency checking even for non-formalised mathemat-
ics. MathLang and MPL will be independent of any particular logical foundation
of mathematics; individual documents will be able to be formal in one or more

392
Fairouz Kamareddine, Joe Wells, Christoph Zengler and Henk Barendregt
particular foundations, or not formalised.
MathLang and MPL hope to open a new useful era of collaboration between
ordinary mathematicians, logicians (who ordinarily stay apart from other math-
ematicians), and computer science researchers working in such areas as theorem
proving and mathematical knowledge management who can develop tools to link
them together. MathLang and MPL’s document representation are intended to
help with various kinds of automated computerised processing of mathematical
knowledge. It should be possible to link MathLang and MPL documents together
to form a public library of reusable mathematics. MathLang and MPL aim to
better support translation between natural languages of mathematical texts and
multi-lingual texts. They also aim to better support the diﬀering uses of mathe-
matical knowledge by diﬀerent kinds of people, including ordinary practising math-
ematicians, students, computer scientists, logicians, linguists, etc.
BIBLIOGRAPHY
[Abbott et al., 1996] J. Abbott, A. van Leeuwen, and A. Strotmann. Objectives of openmath.
Technical Report 12, RIACA (Research Institute for Applications of Computer Algebra),
1996. The TR archives of RIACA are incomplete. Earlier versions of this paper can be found
at the “old OpenMath Home Pages” archived at the Uni. K¨oln.
[Autexier et al., 2010] Serge Autexier, Petr Sojka, and Masakazu Suzuki. Foreword to the spe-
cial issue on authoring, digitalization and management of mathematical knowledge. Mathe-
matics in Computer Science, 3(3):225–226, 2010.
[Barendregt et al., 2013] H Barendregt, Will Dekker, and Richard Statman. Lambda Calculus
with Types. Cambridge University Press, 2013.
[Barendregt, 2003] Henk Barendregt.
Towards an interactive mathematical proof mode.
In
Kamareddine [2003], pages 25–36.
[Bundy et al., 1990] Alan Bundy, Frank van Harmelen, Christian Horn, and Alan Smaill. The
oyster-clam system.
In Mark E. Stickel, editor, CADE, volume 449 of Lecture Notes in
Computer Science, pages 647–648. Springer, 1990.
[Cantor, 1895] Georg Cantor. Beitr¨age zur Begr¨undung der transﬁniten Mengenlehre (part 1).
Mathematische Annalen, 46:481–512, 1895.
[Cantor, 1897] Georg Cantor. Beitr¨age zur Begr¨undung der transﬁniten Mengenlehre (part 2).
Mathematische Annalen, 49:207–246, 1897.
[Cauchy, 1821] Augustin-Louis Cauchy. Cours d’Analyse de l’ ´Ecole Royale Polytechnique. De-
bure, Paris, 1821. Also in Œuvres Compl`etes (2), volume III, Gauthier-Villars, Paris, 1897.
[Constable and others, 1986] R. Constable et al.
Implementing Mathematics with the Nuprl
Proof Development System. Prentice-Hall, 1986.
[Constable, 1995] Robert L. Constable. Using reﬂection to explain and enhance type theory.
In H. Schwichtenberg, editor, Proof and Computation, Computer and System Sciences 139,
pages 109–144. Springer, 1995.
[de Bruijn, 1987] N.G. de Bruijn. The mathematical vernacular, a language for mathematics
with typed sets. In Workshop on Programming Logic, 1987. Reprinted in [Nederpelt et al.,
1994, F.3].
[Dedekind, 1872] Richard Dedekind. Stetigkeit und irrationale Zahlen. Vieweg & Sohn, Braun-
schweig, 1872. Fourth edition published in 1912.
[Frege, 1879] Gottlob Frege.
Begriﬀsschrift: eine der arithmetischen nachgebildete Formel-
sprache des reinen Denkens. Nebert, Halle, 1879. Can be found on pp. 1–82 in [van Heijenoort,
1967].
[Frege, 1893] Gottlob Frege. Grundgesetze der Arithmetik, volume 1. Hermann Pohle, Jena,
1893. Republished 1962 (Olms, Hildesheim).
[Frege, 1903] Gottlob Frege. Grundgesetze der Arithmetik, volume 2. Hermann Pohle, Jena,
1903. Republished 1962 (Olms, Hildesheim).

Computerising Mathematical Text
393
[Gierz et al., 1980] G. Gierz, K. H. Hofmann, K. Keimel, J. D. Lawson, M. W. Mislove, and
D. S. Scott. A Compendium of Continuous Lattices. Springer-Verlag, 1980.
[Gordon and Melham, 1993] M. Gordon and T. Melham.
Introduction to HOL – A theorem
proving environment for higher order logic. Cambridge University Press, 1993.
[Heath, 1956] Thomas L. Heath. The 13 Books of Euclid’s Elements. Dover, 1956. In 3 volumes.
Sir Thomas Heath originally published this in 1908.
[Kamareddine and Nederpelt, 2004] Fairouz Kamareddine and Rob Nederpelt. A reﬁnement of
de Bruijn’s formal language of mathematics. J. Logic Lang. Inform., 13(3):287–340, 2004.
[Kamareddine and Wells, 2008] Fairouz Kamareddine and J. B. Wells. Computerizing mathe-
matical text with mathlang. Electron. Notes Theor. Comput. Sci., 205:5–30, 2008.
[Kamareddine et al., 2004a] Fairouz Kamareddine, Twan Laan, and Rob Nederpelt. A Modern
Perspective on Type Theory from Its Origins Until Today, volume 29 of Kluwer Applied Logic
Series. Kluwer Academic Publishers, May 2004.
[Kamareddine et al., 2004b] Fairouz Kamareddine, Manuel Maarek, and J. B. Wells. Flexible
encoding of mathematics on the computer. In Mathematical Knowledge Management, 3rd
Int’l Conf., Proceedings, volume 3119 of Lecture Notes in Computer Science, pages 160–174.
Springer, 2004.
[Kamareddine et al., 2004c] Fairouz Kamareddine, Manuel Maarek, and J. B. Wells.
Math-
lang: Experience-driven development of a new mathematical language. In Proc. [MKMNET]
Mathematical Knowledge Management Symposium, volume 93 of ENTCS, pages 138–160,
Edinburgh, UK (2003-11-25/---29), February 2004. Elsevier Science.
[Kamareddine et al., 2006] Fairouz Kamareddine, Manuel Maarek, and J. B. Wells. Toward an
object-oriented structure for mathematical text. In Mathematical Knowledge Management,
4th Int’l Conf., Proceedings, volume 3863 of Lecture Notes in Artiﬁcial Intelligence, pages
217–233. Springer, 2006.
[Kamareddine et al., 2007a] Fairouz Kamareddine, Robert Lamar, Manuel Maarek, and J. B.
Wells. Restoring natural language as a computerised mathematics input method. In MKM
’07 [2007], pages 280–295.
[Kamareddine et al., 2007b] Fairouz Kamareddine, Manuel Maarek, Krzysztof Retel, and J. B.
Wells. Gradual computerisation/formalisation of mathematical texts into Mizar. In Roman
Matuszewski and Anna Zalewska, editors, From Insight to Proof: Festschrift in Honour of
Andrzej Trybulec, volume 10(23) of Studies in Logic, Grammar and Rhetoric, pages 95–120.
University of Bia lystok, 2007. Under the auspices of the Polish Association for Logic and
Philosophy of Science.
[Kamareddine et al., 2007c] Fairouz Kamareddine, Manuel Maarek, Krzysztof Retel, and J. B.
Wells. Narrative structure of mathematical texts. In MKM ’07 [2007], pages 296–311.
[Kamareddine, 2003] Fairouz Kamareddine, editor. Thirty Five Years of Automating Mathe-
matics, volume 28 of Kluwer Applied Logic Series. Kluwer Academic Publishers, November
2003.
[Kanahori et al., 2006] Toshihiro Kanahori, Alan Sexton, Volker Sorge, and Masakazu Suzuki.
Capturing abstract matrices from paper.
In Mathematical Knowledge Management, 5th
Int’l Conf., Proceedings, volume 4108 of Lecture Notes in Computer Science, pages 124–138.
Springer, 2006.
[Kohlhase, 2006] Michael Kohlhase. An Open Markup Format for Mathematical Documents,
OMDoc (Version 1.2), volume 4180 of Lecture Notes in Artiﬁcial Intelligence.
Springer-
Verlag, 2006.
[Lamar, 2011] Robert Lamar.
A Partial Translation Path from MathLang to Isabelle.
PhD
thesis, Heriot-Watt University, Edinburgh, Scotland, May 2011.
[Landau, 1930] Edmund Landau. Grundlagen der Analysis. Chelsea, 1930.
[Landau, 1951] Edmund Landau. Foundations of Analysis. Chelsea, 1951. Translation of [Lan-
dau, 1930] by F. Steinhardt.
[Maarek, 2007] Manuel Maarek. Mathematical Documents Faithfully Computerised: the Gram-
matical and Text & Symbol Aspects of the MathLang Framework. PhD thesis, Heriot-Watt
University, Edinburgh, Scotland, june 2007.
[MKM ’07, 2007] Towards Mechanized Mathematical Assistants (Calculemus 2007 and MKM
2007 Joint Proceedings), volume 4573 of Lecture Notes in Artiﬁcial Intelligence. Springer,
2007.

394
Fairouz Kamareddine, Joe Wells, Christoph Zengler and Henk Barendregt
[Nederpelt et al., 1994] Rob Nederpelt, J. H. Geuvers, and Roel C. de Vrijer. Selected Papers
on Automath, volume 133 of Studies in Logic and the Foundations of Mathematics. North-
Holland, Amsterdam, 1994.
[Nederpelt, 2002] Rob Nederpelt.
Weak Type Theory: a formal language for mathematics.
Technical Report 02-05, Eindhoven University of Technology, 2002.
[Nipkow et al., 2002] Tobias Nipkow, Lawrence C. Paulson, and Markus Wenzel. Isabelle/HOL
— A Proof Assistant for Higher-Order Logic, volume 2283 of LNCS. Springer-Verlag, 2002.
[Peano, 1889] Giuseppe Peano. ´Arithmetices Principia, Nova Methodo Exposita. Bocca, Turin,
1889. An English translation can be found on pp. 83–97 in [van Heijenoort, 1967].
[Retel, 2009] Krzysztof Retel.
Gradual Computerisation and veriﬁcation of Mathematics:
MathLang’s Path into Mizar. PhD thesis, Heriot-Watt University, Edinburgh, Scotland, April
2009.
[Rudnicki, 1992] P. Rudnicki. An overview of the Mizar project. In Proceedings of the 1992
Workshop on Types for Proofs and Programs, 1992.
[Sexton and Sorge, 2006] Alan Sexton and Volker Sorge.
The ellipsis in mathematical docu-
ments. Talk overhead images presented at the IMA (Institute for Mathematics and its Ap-
plications, University of Minnesota) “Hot Topic” Workshop The Evolution of Mathematical
Communication in the Age of Digital Libraries held on 2006-12-08/---09, 2006.
[Siekmann et al., 2002] J¨org H. Siekmann, Christoph Benzm¨uller, Vladimir Brezhnev, Lassaad
Cheikhrouhou, Armin Fiedler, Andreas Franke, Helmut Horacek, Michael Kohlhase, Andreas
Meier, Erica Melis, Markus Moschner, Immanuel Normann, Martin Pollet, Volker Sorge,
Carsten Ullrich, Claus-Peter Wirth, and J¨urgen Zimmer. Proof development with omega. In
Andrei Voronkov, editor, CADE, volume 2392 of Lecture Notes in Computer Science, pages
144–149. Springer, 2002.
[Siekmann et al., 2003] Siekmann, Benzm¨ullerand Fiedler, Meier, Normann, and Pollet. Proof
development with Ωmega: The irrationality of
√
2. In Kamareddine [2003], pages 271–314.
[Team, 1999–2003] Coq Development Team. The coq proof assistant reference manual. INRIA,
1999–2003.
[van Benthem Jutting, 1977a] Lambert S. van Benthem Jutting. Checking Landau’s “Grundla-
gen” in the AUTOMATH System. PhD thesis, Eindhoven, 1977. Partially reprinted in [Ned-
erpelt et al., 1994, B.5,D.2,D.3,D.5,E.2].
[van Benthem Jutting, 1977b] Lambert S. van Benthem Jutting. Checking Landau’s “Grund-
lagen” in the AUTOMATH system. PhD thesis, Eindhoven, 1977.
[van der Hoeven, 2004] Joris van der Hoeven. GNU TeXmacs. SIGSAM Bulletin, 38(1):24–25,
2004.
[van Heijenoort, 1967] J. van Heijenoort. From Frege to G¨odel: A Source Book in Mathematical
Logic, 1879–1931. Harvard University Press, 1967.
[W3C, 2003] W3C. Mathematical markup language (MathML) version 2.0. W3C Recommen-
dation, October 2003. W3C (World Wide Web Consortium).
[WC3, 2004] WC3. RDF Primer. W3C Recommendation, February 2004. W3C (World Wide
Web Consortium).
[WC3, 2007] WC3. XQuery 1.0 and XPath 2.0 data model (XDM). W3C Recommendation,
2007. W3C (World Wide Web Consortium).
[Whitehead and Russel, 1910–1913] Alfred North Whitehead and Bertrand Russel. Principia
Mathematica. Cambridge University Press, 1910–1913. In three volumes published from 1910
through 1913. Second edition published from 1925 through 1927. Abridged edition published
in 1962.
[Wiedijk, 2006] F. Wiedijk, editor. The Seventeen Provers of the World, foreword by Dana S.
Scott, volume 3600 of LNCS. Springer Berlin, Heidelberg, 2006.
[Zengler, 2008] Christoph Zengler. Research report. Technical report, Heriot-Watt University,
November 2008.
[Zermelo, 1908] Ernst Zermelo. Untersuchungen ¨uber die Grundlagen der Mengenlehre (part
1). Mathematische Annalen, 65:261–281, 1908. An English translation can be found on pp.
199–215 in [van Heijenoort, 1967].

Computerising Mathematical Text
395
Lemma main :
(WCR R)->(IND R (cr R)).
Proof.
Intros.
Unfold IND.
Intro a.
Intro IH.
Unfold cr.
Intuition.
Assert
(a=b1\verb+\/+(TC R a b1))\verb+/\+
(a=b2\verb+\/+(TC R a b2)).
Split.
Apply p7.
Assumption.
Apply p7.
Assumption.
Tactic Definition
Get x :=
Elim x; Intros; Clear x.
Get H0; Get H3.
Exists b2.
Split.
Rewrite <- H0.
Assumption.
Apply TRC\_b.
Get H4.
Exists b1.
Split.
Apply TRC_b.
Rewrite <- H3.
Assumption.
Assert
(EX x1|(a R x1)/\(TRC R x1 b1)).
Apply p6.
Assumption.
Assert
(EX x2|(a R x2)/\(TRC R x2 b2)).
Apply p6.
Assumption.
Tactic Definition
Pick x y :=
Elim x; Intro y; Intros; Clear x.
Pick H4 x1; Pick H5 x2.
Intuition.
Assert
(EX x|(TRC R x1 x)/\(TRC R x2 x)).
Unfold WCR in H.
Apply (H a x1 x2).
Assumption.
Assumption.
Pick H4 x.
Intuition.
Assert (cr R x1).
Apply IH.
Assumption.
Assert
(EX b|(TRC R b1 b)/\(TRC R x b)).
Unfold cr in H.
Apply (IH x1).
Assumption. Split.
Assumption.
Assumption.
Pick H4 x.
Intuition.
Assert (cr R x1).
Apply IH.
Assumption.
Assert
(EX b|(TRC R b1 b)/\(TRC R x b)).
Unfold cr in H.
Apply (IH x1).
Assumption. Split.
Assumption.
Assumption.
Pick H10 b.
Intuition.
Assert (cr R x2).
Apply IH.
Assumption.
Assert
(EX c:A|(TRC R b2 c)/\(TRC R b c)).
Apply (H11 b2 b).
Split.
Assumption.
Apply (p4
x2 x b).
Assumption.
Assumption.
Pick H13 c.
Intuition.
Exists c.
Intuition.
Apply (p4 b1 b c).
Assumption.
Assumption.
Qed.
Theorem newman :
((WF R)/\(WCR R))->(CR R).
Proof.
Intros.
Intuition.
Assert (Ind R (cr R)).
Apply main.
Assumption.
Unfold CR.
Unfold WF in H0.
Apply (H0 (cr R)).
Assumption.
Qed.
Table 8. The Coq Script of Newman’s Lemma

396
Fairouz Kamareddine, Joe Wells, Christoph Zengler and Henk Barendregt
Group
Meaning
Encoding
Quantifiers
∀
for all
<forall>∀<#> <#>
.
<#>
∃
exists
<exists>∃<#> <#>
.
<#>
∃!
exists exactly one
<exists one>∃! <#> <#>
.
<#>
Boolean connectives
∧
and
<and> <#>
∧<#>
∨
or
<or> <#>
∨<#>
=⇒
implication
<impl> <#>
=⇒<#>
⊕
exclusive or
<xor> <#>
⊕<#>
Set theory
∈
element of
<in> <#>
∈<#>
⊂
subset of
<subset> <#>
⊂<#>
{|}
constructor for a set
<<Set>>{ <#> <#>
| <#>
}
∅
empty set
<emptyset>∅
=
equality of sets
<seteq> <#>
= <#>
̸=
inequality of sets
<setneq> <#>
̸= <#>
Special functions
:=
is a
<isa> <#>
isa <#>
1
one
<1>1
S(x)
the successor function
<succ> <#>
function for indexing
<index> <#>
<#>
Table 9. The preface for the ﬁrst chapter of Landau’s book
Role
Mizar rule
Coq rule
axiom
%name : %body ;
Axiom %name : %body .
deﬁnition
definition %name : %nl %body %nl end;
Definition : %body .
theorem
theorem %name: %nl %body
Theorem %name : %body .
proof
proof %nl %body
%nl end;
Proof %name : %body .
cases
per cases; %nl
%body
case
suppose %nl %body %nl end;
%body
existencePart
existence %nl %body
%body
uniquenessPart
uniqueness %nl %body
%body
Table 10. The Mizar and Coq rules for the dictionary
theorem th131:
<th131>
proof
<pr131>
end;
theorem th132:
<th132>
proof
per cases;
suppose
<pr132case1>
end;
suppose
<pr132case2>
end;
suppose
<pr132case3>
end;
end;
Theorem th131: <th131> .
Proof.
<pr131>
Qed.
Theorem th132: <th132> .
Proof.
<pr132case1>
<pr132case2>
<pr132case3>
Qed.
Table 11. Part of the Mizar (left) and Coq (right) output from gSGA

Part IV 
Computer Science 


CONCURRENCY THEORY:
A HISTORICAL PERSPECTIVE ON
COINDUCTION AND
PROCESS CALCULI
Jos C. M. Baeten and Davide Sangiorgi
Readers: Luca Aceto and Robert J. van Glabbeek
1
INTRODUCTION
Computer science is a very young science, that has its origins in the middle of the
twentieth century. For this reason, describing its history, or the history of an area
of research in computer science, is an activity that receives scant attention.
The discipline of computer science does not have a clear demarcation, and even
its name is a source of debate. Computer science is sometimes called informatics
or information science or information and communication science. A quote widely
attributed to Edsger Dijkstra is
Computer science is no more about computers than astronomy is about
telescopes.
If computer science is not about computers, what is it about?
We claim it
is about behavior.
This behavior can be exhibited by a computer executing a
program, but equally well by a human being performing a series of actions. In
general, behavior can be shown by any agent. Here, an agent is simply an entity
that can act.
So, computer science is about behavior, about processes, about
things happening over time. We are concerned with agents that are executing
actions, taking input, emitting output, communicating with other agents.
Thus, an agent has behavior, e.g., the execution of a software system, the actions
of a machine or even the actions of a human being. Behavior is the total of events
or actions that an agent can perform, the order in which they can be executed and
maybe other aspects of this execution such as timing, probabilities or evolution.
Always, we describe certain aspects of behavior, disregarding other aspects, so we
are considering an abstraction or idealization of the ‘real’ behavior. Rather, we
can say that we have an observation of behavior, and an action is the chosen unit
of observation.
Handbook of the History of Logic. Volume 9: Computational Logic. 
Volume editor: Jörg Siekmann 
Series editors: Dov M. Gabbay and John Woods 
Copyright © 2014 Elsevier BV. All rights reserved.

400
Jos C. M. Baeten and Davide Sangiorgi
But behavior is also the subject of dynamics or mechanics in physics. How
does computer science distinguish itself from these ﬁelds? Well, the most impor-
tant diﬀerence is that dynamics or mechanics considers behavior using continuous
mathematics (diﬀerential equations, analysis), and that computer science considers
behavior using discrete mathematics (algebra, logic).
So, behavior considered in computer science is discrete, takes place at separate
moments in time, and we can observe separate things happening consecutively,
diﬀerent actions are separated in time. This is why a process is sometimes also
called a discrete event system. Discrete behavior is important in systems biology,
e.g. considering metabolic pathways or behavior of a living cell or components of
a cell. We see more and more scientists active in systems biology with a computer
science background. Another example of discrete behavior is the working of an
organization or part of an organization. This is called workﬂow.
There are two more ingredients involved before we can get to a deﬁnition and
a demarcation of computer science. These are interaction and information. Com-
puter science started oﬀconsidering a single agent executing actions by itself, but
with the birth of concurrency theory, it was realized that interaction is also an
essential part of computer science. Agents interact amongst themselves, and inter-
act with the outside world. Usually, a computer is not stand-alone, with limited
interaction with the user, executing a batch process, but is always connected to
other devices and the Internet. Information is the stuﬀof informatics, the things
that are communicated, processed, transformed, sent around.
So now we have all ingredients in place and can formulate the following deﬁnition
of computer science. It may be obvious that we ﬁnd informatics a better name
than computer science.
Informatics or computer science is the study of discrete behavior of
interacting information processing agents.
Given this deﬁnition of informatics, we can explain familiar notions in terms of
it. A computer program is a prescription of behavior, an interpreter or compiler
can turn this into speciﬁc behavior. An algorithm is a description of behavior.
Computation refers to sequential behavior, not considering interaction. Commu-
nication is interaction with information exchange.
Data is a manifestation of
information. Intelligence has to do with a comparison between diﬀerent agents, in
particular between a human being and a computer.
Finally, concurrency theory is that part of the foundations of computer science
where interaction is considered explicitly.
2
CONCURRENCY
The simplest model of behavior is to see behavior as an input/output function. A
value or input is given at the beginning of the process, and at some moment there
is a(nother) value as outcome or output. This view is in close agreement with

Concurrency Theory
401
the classic notion of “algorithmic problem”, which is described by giving its legal
inputs and, for each legal input, the expected output. The input/output model was
used to advantage as the simplest model of the behavior of a computer program
in computer science, from the start of the subject in the middle of the twentieth
century. It was instrumental in the development of (ﬁnite state) automata theory.
In automata theory, a process is modeled as an automaton. An automaton has a
number of states and a number of transitions, going from one state to a(nother)
state. A transition denotes the execution of an (elementary) action, the basic unit
of behavior. Besides, there is an initial state (sometimes, more than one) and a
number of ﬁnal states. A behavior is a run, i.e. a path from the initial state to a
ﬁnal state. Important from the beginning is when to consider two automata equal,
expressed by a notion of equivalence. On automata, the basic notion of equivalence
is language equivalence: a behavior is characterized by the set of executions from
the initial state to a ﬁnal state.
Later on, this model was found to be lacking in several situations. Basically,
what is missing is the notion of interaction: during the execution from initial state
to ﬁnal state, an agent may interact with another agent. This is needed in order to
describe parallel or distributed systems, or so-called reactive systems (see [Harel
and Pnueli, 1985]). When dealing with interacting systems, we say we are doing
concurrency theory, so concurrency theory is the theory of interacting, parallel
and/or distributed systems.
The history of concurrency theory can be traced back to the early sixties of
the twentieth century with the theory of Petri nets, conceived by Petri starting
from his thesis in 1962 [Petri, 1962]. This remained, for some time, a separate
strand in the development of the foundations of computer science, with not so
many connections to other foundational research. On the other hand, the theory
of Petri nets is a strong and continuing contribution to the foundations of computer
science, and it has yielded many and beautiful results. The history of Petri nets
has been described elsewhere (see e.g. [Brauer and Reisig, 2006]), so we will not
consider it any further at this point.
In 1970, we can distinguish three main styles of formal reasoning about computer
programs, focusing on giving semantics (meaning) to programming languages.
1. Operational semantics. A computer program is modeled as an execution of
an abstract machine, an automaton. A state of such a machine is a valuation
of variables, a transition between states describes the eﬀect of an elementary
program instruction. Pioneer of this ﬁeld is McCarthy [McCarthy, 1963].
2. Denotational semantics. This is more abstract than operational semantics;
computer programs are usually modeled by a function transforming input
into output. Pioneers of this line of research are Scott and Strachey [Scott
and Strachey, 1971].
3. Axiomatic semantics. Here, emphasis is put on proof methods to prove pro-
grams correct. Central notions are program assertions, proof triples consist-

402
Jos C. M. Baeten and Davide Sangiorgi
ing of precondition, program statement and postcondition, and invariants.
Pioneers are Floyd [Floyd, 1967] and Hoare [Hoare, 1969].
Then, the question was raised how to give semantics to programs containing
a notion of parallelism. It was found that this is diﬃcult using the methods of
denotational, operational or axiomatic semantics as they existed at that time,
although several attempts were made (later on, it became clear how to extend the
diﬀerent types of semantics to parallel programming, see e.g. [Owicki and Gries,
1976] or [Plotkin, 1976]).
There are two paradigm shifts that needed to be made, before a theory of
parallel programs could be developed. First of all, the idea of a behavior as an
input/output function needed to be abandoned. A program could still be modeled
as an automaton, but the notion of language equivalence is no longer appropriate.
This is because the interaction a process has between input and output inﬂu-
ences the outcome, disrupting functional behavior. Secondly, the notion of global
variables needed to be overcome. Using global variables, a state of a modeling
automaton is given as a valuation of the program variables, that is, a state is
determined by the values of the variables. The independent execution of parallel
processes makes it diﬃcult or impossible to determine the values of global vari-
ables at a given moment. It turned out to be simpler to let each process have its
own local variables, and to denote exchange of information explicitly.
For each of these paradigm shifts, we will consider an important notion exem-
plifying this development. For the shift away from functional behavior we consider
the history of bisimulation, for the shift away from global variables we consider
the history of process calculi. Examining the developments through which certain
concepts have come to light and have been discovered is not a matter of purely
intellectual curiosity, but it also useful to understand the concepts themselves
(e.g., problems and motivations behind them, relationship with other concepts,
alternatives, etc.).
3
BISIMULATION AND COINDUCTION
Bisimulation and coinduction are generally considered as one of the most important
contributions of concurrency theory to computer science.
In concurrency, the
bisimulation equality, called bisimilarity, is the most studied form of behavioural
equality for processes, and is widely used, for a number of reasons, notably the
following ones.
• Bisimilarity is accepted as the ﬁnest behavioural equivalence one would like
to impose on processes.
• The bisimulation proof method is exploited to prove equalities among pro-
cesses. This occurs even when bisimilarity is not the behavioural equivalence
chosen for the processes. For instance, one may be interested in trace equiv-

Concurrency Theory
403
alence and yet use the bisimulation proof method, since bisimilarity implies
trace equivalence.
• The eﬃciency of the algorithms for bisimilarity checking and the composi-
tionality properties of bisimilarity are exploited to minimise the state-space
of processes.
• Bisimilarity, and variants of it such as similarity, are used to abstract from
certain details of the systems of interest.
For instance, we may want to
prove behavioural properties of a server that do not depend on the data that
the server manipulates.
Further, abstracting from the data may turn an
inﬁnite-state server into a ﬁnite one.
Bisimulation has also spurred the study of coinduction; indeed bisimilarity is an
example of a coinductive deﬁnition, and the bisimulation proof method is an in-
stance of the coinduction proof method.
Bisimulation and, more generally, coinduction are employed today in a num-
ber of areas of computer science: functional languages, object-oriented languages,
types, data types, domains, databases, compiler optimisations, program analy-
sis, veriﬁcation tools, etc.. Today, they are also used in other ﬁelds, e.g., artiﬁcial
intelligence, cognitive science, mathematics, modal logics, philosophy, and physics.
In this section, we look at the origins of bisimulation (and bisimilarity). Bisim-
ulation has been discovered independently, and more or less at the same time, not
only in computer science, but also in philosophical logic (more precisely, modal
logics), and in set theory. Thus while we focus on computer science, we also give
an account of the discovery of the concepts in these other areas. It is fair to say,
however, that the motivations for the study of bisimulation and coinduction that
came from computer science were decisive in the development of the concept and
its wide success. For more details we refer to [Sangiorgi, 2009]
In computer science, philosophical logic, and set theory, bisimulation has been
derived through reﬁnements of notions of morphism between algebraic structures.
Roughly, morphisms are maps (i.e., functions) that are “structure-preserving”.
The notion is fundamental in all mathematical theories in which the objects of
study have some kind of structure, or algebra. The most basic forms of morphism
are the homomorphisms. These essentially give us a way of embedding a structure
(the source) into another one (the target), so that all the relations in the source are
present in the target. The converse however, need not be true; for this, stronger
notions of morphism are needed. One such notion is isomorphism, which is how-
ever extremely strong—isomorphic structures must be essentially the same, i.e.,
“algebraically identical”. It is the quest for notions in between homomorphism
and isomorphism that led to the discovery of bisimulation.
The kind of structures studied in computer science, in philosophical logic, and
in set theory were forms of rooted directed graphs. On such graphs bisimulation
is coarser than graph isomorphism because, intuitively, bisimulation allows us to
observe a graph only through the movements that are possible along its edges.

404
Jos C. M. Baeten and Davide Sangiorgi
By contrast, with isomorphisms the identity of the nodes is observable too. For
instance, isomorphic graphs have the same number of nodes, which need not be
the case for bisimilar graphs (bisimilarity between two graphs indicates that their
roots are related in a bisimulation).
Before digging into history, however, we recall a few basic deﬁnitions and results
for bisimulation and bisimilarity. For this, we use some very simple set theory; a
reader even with little knowledge of the topic should be able to grasp the essential
ideas.
3.1
Bisimulation
We present bisimulation on Labelled Transition Systems (LTSs) because these
are the most common structures on which bisimulation has been studied. LTSs
are essentially “edge-labelled” directed graphs. Bisimulation can be deﬁned on
variant structures, such as relational structures (i.e., unlabeled directed graphs),
non-deterministic ﬁnite automata or Kripke structures, in a similar way.
We let R range over relations on sets, i.e., if ℘denotes the powerset construct,
then a relation R on a set W is an element of ℘(W × W).
The composition
of relations R1 and R2 is R1R2 (i.e., (s, s′) ∈R1R2 holds if for some s′′, both
(s, s′′) ∈R1 and (s′′, s′) ∈R2 hold). We often use the inﬁx notation for relations;
hence s R t means (s, t) ∈R.
DEFINITION 1 (Labelled Transition Systems). A Labelled Transition System is
a triple (W, Act, {
a→: a ∈Act}) with domain W as above, set of labels Act, and
for each label a, a (binary) relation
a→on W called the transition relation.
In the two deﬁnitions above, the elements of W will be called states or points,
sometimes even processes as this is the usual terminology in concurrency. We use
s, t to range over such elements, and µ to range over the labels in Act. Following
the inﬁx notation for relations, we write s
µ→t when (s, t) ∈
µ→; in this case we
call t a µ-derivative of s, or sometimes simply a derivative of s.
DEFINITION 2 (Bisimulation). A binary relation R on the states of an LTS is a
bisimulation if whenever s1 R s2:
• for all s′
1 with s1
µ→s′
1, there is s′
2 such that s2
µ→s′
2 and s′
1 R s′
2;
• the converse, on the transitions emanating from s2 (i.e., for all s′
2 with s2
µ→
s′
2, there is s′
1 such that s1
µ→s′
1 and s′
1 R s′
2).
Bisimilarity, written ∼, is the union of all bisimulations; thus s ∼t holds if there
is a bisimulation R with s R t.
The deﬁnition of bisimilarity has a strong impredicative ﬂavor, for bisimilarity
itself is a bisimulation and is therefore part of the union from which it is deﬁned.
Also, the deﬁnition immediately suggests a proof technique: to demonstrate that
s1 and s2 are bisimilar, ﬁnd a bisimulation relation containing the pair (s1, s2).
This is the bisimulation proof method.

Concurrency Theory
405
We will not discuss here the eﬀectiveness of this proof method; the interested
reader may consult concurrency textbooks in which bisimilarity is taken as the
main behavioural equivalence for processes, such as [Milner, 1989]. We wish how-
ever to point out two features of the deﬁnition of bisimulation that make its proof
method practically interesting:
• the locality of the checks on the states;
• the lack of a hierarchy on the pairs of the bisimulation.
The checks are local because we only look at the immediate transitions that em-
anate from the states. An example of a behavioural equality that is non-local is
trace equivalence (two states are trace equivalent if they can perform the same
sequences of transitions). It is non-local because computing a sequence of tran-
sitions starting from a state s may require examining other states, diﬀerent from
s.
There is no hierarchy on the pairs of a bisimulation in that no temporal order
on the checks is required: all pairs are on a par. As a consequence, bisimilarity
can be eﬀectively used to reason about inﬁnite or circular objects. This is in sharp
contrast with inductive techniques, that require a hierarchy, and that therefore are
best suited for reasoning about ﬁnite objects. For instance, here is a deﬁnition of
equality that is local but inherently inductive:
s1 = s2 if:
for all s′
1 with s1
µ→s′
1, there is s′
2 such that s2
µ→s′
2 and s′
1 = s′
2;
the converse, on the transitions from s2.
This deﬁnition requires a hierarchy, as the checks on the pair (s1, s2) must follow
those on derivative pairs such as (s′
1, s′
2). Hence the deﬁnition is ill-founded if the
state space of the derivatives reachable from (s1, s2) is inﬁnite or includes loops.
(The deﬁnition would actually yield bisimilarity if we add that we refer to the
largest relation “=” that satisﬁes the above clauses; what we wish to stress here
is that, as it stands, the deﬁnition makes sense only if a hierarchy on the states
exists in which the derivative pair is before the initial pair.)
We will also sometimes mention simulations, which are “half bisimulations”.
DEFINITION 3 (Simulation). A binary relation R on the states of an LTS is a
simulation if s1 R s2 implies that for all s′
1 with s1
µ→s′
1 there is s′
2 such that
s2
µ→s′
2 and s′
1 R s′
2. Similarity is the union of all simulations.
We have presented the standard deﬁnitions of bisimulation and bisimilarity. A
number of variants have been proposed and studied. For instance, on LTSs in
which labels have a structure, which may be useful when processes may exchange
values in communications; or on LTSs equipped with a special action to represent
movements internal to processes, in which case one may wish to abstract from
such action in the bisimulation game yielding the so-called weak bisimulations

406
Jos C. M. Baeten and Davide Sangiorgi
and weak bisimilarity. Examples of these kinds may be found, e.g., in [Milner,
1989; Sangiorgi and Walker, 2001; Aceto et al., 2007; Baeten and Weijland, 1990;
Sangiorgi, 2012]. Also, we do not discuss in this paper enhancements of the bisim-
ulation proof method, intended to relieve the amount of work needed to prove
bisimilarity results, such as bisimulation up-to techniques; see, e.g., [Pous and
Sangiorgi, 2012].
3.2
Approximants of bisimilarity
We can approximate bisimilarity using the following inductively-deﬁned relations
and their intersection. Similar constructions can be given for similarity.
DEFINITION 4 (Stratiﬁcation of bisimilarity). Let W be the set of states of an
LTS. We set:
• ∼0
def
= W × W
• s ∼n+1 t, for n ≥0, if
1. for all s′ with s
µ→s′, there is t′ such that t
µ→t′ and s′ ∼n t′;
2. the converse, i.e., for all t′ with t
µ→t′, there is s′ such that s
µ→s′
and s′ ∼n t′.
• ∼ω
def
= T
n≥0 ∼n
In general, ∼ω does not coincide with ∼, as the following example shows.
EXAMPLE 5. Suppose a ∈Act, and let a0 be a state with no transitions, aω a
state whose only transition is
aω
a→aω ,
and an, for n ≥1, states with only transitions
an
a→an−1 .
Also, let s, t be states with transitions
s
a→an
for all n ≥0
and
t
a→an
for all n ≥0
t
a→aω
It is easy to prove, by induction on n, that, for all n, s ∼n t, hence also s ∼ω t.
However, it holds that s ̸∼t: the transition t
a→aω can only be matched by s
with one of the transitions s
a→an. But, for all n, we have aω ̸∼an, as only from
the former state a sequence of n + 1 transitions is possible.
In order to reach ∼, in general we need to replace the ω-iteration that deﬁnes
∼ω with a transﬁnite iteration, using the ordinal numbers. However, the situation

Concurrency Theory
407
changes if the LTS is ﬁnitely branching, meaning that for all s the set {s′ : s
µ→
s′, for some µ} is ﬁnite. (In Example 5, the LTS is not ﬁnitely branching.) Indeed,
on ﬁnitely branching LTSs, relations ∼and ∼ω coincide. (The result also holds
with the weaker condition of image ﬁniteness, requiring that for all s and µ the
set {s′ : s
µ→s′} is ﬁnite.)
3.3
Coinduction
Intuitively, a set A is deﬁned coinductively if it is the greatest solution of an
inequation of a certain form; then the coinduction proof principle just says that
any set that is a solution of the same inequation is contained in A. Dually, a
set A is deﬁned inductively if it is the least solution of an inequation of a certain
form, and the induction principle then says that any other set that is a solution
to the same equation contains A. Familiar inductive deﬁnitions and proofs can
be formalised in this way. To see how bisimulation and its proof method ﬁt the
coinductive schema, let (W, Act, {
a→:
a ∈Act}) be an LTS, and consider the
function F∼: ℘(W × W) →℘(W × W) so deﬁned:
F∼(R) is the set of all pairs (s, t) such that:
1. for all s′ with s
µ→s′, there is t′ such that t
µ→t′ and s′ R t′.
2. for all t′ with t
µ→t′, there is s′ such that s
µ→s′ and s′ R t′.
We call F∼the functional associated to bisimulation, for we have:
PROPOSITION 6.
1. ∼is the greatest ﬁxed point of F∼;
2. ∼is the largest relation R such that R ⊆F∼(R); thus R ⊆∼for all R with
R ⊆F∼(R).
Proposition 6 is a simple application of ﬁxed-point theory, in particular the
Fixed-Point Theorem, that we discuss below. We recall that a complete lattice is a
partially ordered set with all joins (i.e., all its subsets have a supremum, also called
least upper bound); this implies that there are also all meets (i.e., all subsets have
an inﬁmum, also called greatest lower bound). Using ≤to indicate the partial
order, a point x in the lattice is a post-ﬁxed point of an endofunction F on the
lattice if x ≤F(x); it is a pre-ﬁxed point if F(x) ≤x.
THEOREM 7 (Fixed-Point Theorem). On a complete lattice, a monotone endo-
function (i.e., a function from the lattice onto itself) has a complete lattice of ﬁxed
points. In particular the greatest ﬁxed point of the function is the join of all its
post-ﬁxed points, and the least ﬁxed point is the meet of all its pre-ﬁxed points.
We deduce from the theorem that:
• a monotone endofunction on a complete lattice has a greatest ﬁxed point;

408
Jos C. M. Baeten and Davide Sangiorgi
• for an endofunction F on a complete lattice the following rule is sound:
(1)
F monotone
x ≤F(x)
x ≤gfp (F)
where gfp (F) indicates the greatest ﬁxed point of F.
The existence of the greatest ﬁxed point justiﬁes coinductive deﬁnitions, while rule
(1) expresses the coinduction proof principle, following the Fixed-Point Theorem.
Proposition 6 is a consequence of the Fixed-Point Theorem because the func-
tional associated to bisimulation gives us precisely the clauses of a bisimulation,
and is monotone on the complete lattice of the relations on W, in which the join
is given by relational union, the meet by relational intersection, and the partial
order by relation inclusion:
LEMMA 8.
• R is a bisimulation iﬀR ⊆F∼(R);
• F∼is monotone (that is, if R ⊆S then also F∼(R) ⊆F∼(S)).
For such functional F∼, (1) asserts that any bisimulation only relates pairs of
bisimilar states. Example 5 shows that ∼ω is not a ﬁxed point for it.
Also the approximation of bisimilarity using the natural numbers, mentioned at
the end of Section 3.2, can be seen as an application of ﬁxed-point theory, in which
one uses an extra hypothesis of the functional (cocontinuity, which is stonger than
monotonicity) [Sangiorgi, 2012].
Complete lattices are “dualisable” structures: we can reverse the partial order
and get another complete lattice. Thus the deﬁnitions and results above about
joins, post-ﬁxed points, greatest ﬁxed points, cocontinuity have a dual in terms
of meets, pre-ﬁxed points, least ﬁxed points, and continuity. As the results we
gave justify coinductive deﬁnitions and the coinductive proof method, so the dual
theorems can be used to justify familiar inductive deﬁnitions and inductive proofs
for sets. More details can be found in [Sangiorgi, 2012; Sangiorgi and Rutten,
2012].
Another well-known example of application of coinduction is in deﬁnition and
proofs involving divergence. Divergence represents an inﬁnite computation and
can be elegantly deﬁned coinductively; then the coinduction proof method can be
used to prove that speciﬁc computations diverge.
3.4
The origins of bisimulation in computer science
In computer science, the search for the origins of bisimulation takes us back to
the algebraic theory of automata, well-established in the 1960s. A good reference
is Ginzburg’s book [1968]. Homomorphisms can be presented on diﬀerent forms
of automata. From the bisimulation perspective, the most interesting notions are
formulated on Mealy automata. In these automata, there are no initial and ﬁnal

Concurrency Theory
409
states; however, an output is produced whenever an input letter is consumed.
Thus Mealy automata can be compared on the set of output strings produced.
Formally, a Mealy automaton is a 5-tuple (W, Σ, Θ, T , O) where
• W is the ﬁnite set of states;
• Σ is the ﬁnite set of inputs;
• Θ is a ﬁnite set of outputs;
• T is the transition function, that is a set of partial functions {Ta : a ∈Σ}
from W to W;
• O is the output function, that is, a set of partial functions {Oa : a ∈Σ}
from W to Θ.
The output string produced by a Mealy automaton is the translation of the input
string with which the automaton was fed; of course the translation depends on the
state in which the automaton is started. Since transition and output functions of
a Mealy automaton are partial, not all input strings are consumed entirely.
Homomorphism is deﬁned on Mealy automata following the standard notion
in algebra, e.g., in group theory: a mapping that commutes with the operations
deﬁned on the objects of study. Below, if A is an automaton, then W A is the set
of states of A, and similarly for other symbols. As we deal with partial functions,
it is convenient to view these as relations, and thereby use for them relational
notations. Thus fg is the composition of the two function f and g where f is used
ﬁrst (that is, (fg)(a) = g(f(a))); for this, one requires that the codomain of f be
included in the domain of g. Similarly, f ⊆g means that whenever f is deﬁned
then so is g, and they give the same result.
A homomorphism from the automaton A to the automaton B is a surjective
function F from W A to W B such that for all a ∈Σ:
1. T A
a F ⊆FT B
a
(condition on the states); and
2. OA
a ⊆FOB
a (condition on the outputs).
(We assume here for simplicity that the input and output alphabets are the same,
otherwise appropriate coercion functions would be needed.)
At the time (the 1960s), homomorphism and similar notions are all expressed in
purely algebraic terms. Today we can make an operational reading of them, which
for us is more enlightening. Writing s
a
→
b t if the automaton, on state s and input
a, produces the output b and evolves into the state t, and assuming for simplicity
that OA
a and T A
a
are undeﬁned exactly on the same points, the two conditions
above become:
• for all s, s′ ∈W A, if s
a
→
b s′ then also F(s)
a
→
b F(s′).

410
Jos C. M. Baeten and Davide Sangiorgi
Homomorphisms are used in that period to study a number of properties of au-
tomata. For instance, minimality of an automaton becomes the condition that
the automaton has no proper homomorphic image. Homomorphisms are also used
to compare automata. Mealy automata are compared using the notion of cov-
ering (written ≤): A ≤B (read “automaton B covers automaton A”) if B can
do, statewise, at least all the translations that A does. That is, there is a total
function ψ from the states of A to the states of B such that, for all states s of
A, all translations performed by A when started in s can also be performed by
B when started in ψ(s). Note that B can however have states with a behaviour
completely unrelated to that of any state of A; such states of B will not be the
image of states of A. If both A ≤B and B ≤A hold, then the two automata are
deemed equivalent.
Homomorphism implies covering, i.e., if there is a homomorphism from A to
B then A ≤B. The converse result is (very much) false. The implication be-
comes stronger if one uses weak homomorphisms. These are obtained by relaxing
the functional requirement of homomorphism into a relational one. Thus a weak
homomorphism is a total relation R on W A × W B such that for all a ∈Σ:
1. R−1T A
a ⊆T B
a R−1 (condition on the states); and
2. R−1OA
a ⊆OB
a (condition on the outputs).
where relational composition, inverse, and inclusion are deﬁned in the usual way for
relations (and functions are taken as special forms of relations). In an operational
interpretation as above, the conditions give:
• whenever s R t and s
a
→
b s′ hold in A, then there is t′ such that t
a
→
b t′ holds
in B and s′ R t′.
(On the correspondence between the algebraic and operational deﬁnitions, see also
Remark 9 below.) Weak homomorphism reminds us of the notion of simulation for
Labelled Transition Systems (LTSs). The former is however stronger, because the
relation R is required to be total. (Also, in automata theory, the set of states and
the sets of input and output symbols are required to be ﬁnite, but this diﬀerence
is less relevant.)
REMARK 9. To understand the relationship between weak homomorphisms and
simulations, we can give an algebraic deﬁnition of simulation on LTSs, taking these
to be triples (W, Σ, {Ta : a ∈Σ}) whose components have the same interpretation
as for automata. A simulation between two LTSs A and B becomes a relation R
on W A ×W B such that, for all a ∈Σ, condition (1) of weak homomorphism holds,
i.e.
• R−1T A
a ⊆T B
a R−1
This is precisely the notion of simulation deﬁned operationally in Deﬁnition 3.
Indeed, given a state t ∈W B and a state s′ ∈W A, we have t R−1 T A
a s′ whenever

Concurrency Theory
411
s1
a
  B
B
B
B
B
B
B
B
s2
b
~~||||||||
s3
t1
a

t2
b

t3
t4
Figure 1. On homomorphisms and weak homomorphisms
there is s ∈W A such that s
a→s′. Then, requiring that the pair (t, s′) is also in
T B
a R−1 is the demand that there is t′ such that t
a→t′ and s′ R t′.
As homomorphisms, so weak homomorphisms imply covering. The result for
weak homomorphism is stronger as the homomorphisms are strictly included in
the weak homomorphisms.
An example of the strictness is given in Figure 1,
where the states si belong to an automaton and the states ti to another one, there
are two input letters a and b, and for simplicity we ignore the automata outputs.
We cannot establish a homomorphism from the automaton on the left to the
automaton on the right, since a homomorphism must be surjective. Even leaving
the surjective condition aside, a homomorphism cannot be established because
the functional requirement prevents us from relating s3 with both t3 and t4. By
contrast, a weak homomorphism exists, relating s1 with t1, s2 with t2, and s3 with
both t3 and t4.
As weak homomorphisms are total relations, however, covering does not imply
weak homomorphism. Indeed we are not aware, in the literature of that time,
of characterisations of covering, or equivalence, in terms of notions akin to ho-
momorphism. Such characterisations would have taken us closer to the idea of
bisimulation.
In conclusion: in the algebraic presentation of automata in the 1960s we ﬁnd
concepts that remind us of bisimulation, or better, simulation.
However there
are noticeable diﬀerences, as we have outlined above. But the most important
diﬀerence is due to the fact that the objects studied are deterministic. To see
how signiﬁcant this is, consider the operational reading of weak homomorphism,
namely “whenever s R t ...
then there is t′ such that....”.
As automata are
deterministic, the existential quantiﬁer in front of t′ does not play a role. Thus
the alternation of universal and existential quantiﬁers—a central aspect of the
deﬁnitions of bisimulation and simulation—does not really show up in the setting
of deterministic automata.
Robin Milner
Decisive progress towards bisimulation is made by Robin Milner
in the 1970s. Milner transplants the idea of weak homomorphism into the study
of the behaviour of programs in a series of papers in the early 1970s ([Milner,
1970; Milner, 1971b; Milner, 1971a], with [Milner, 1971a] being a synthesis of the
previous two). He studies programs that are sequential, imperative, and that may

412
Jos C. M. Baeten and Davide Sangiorgi
not terminate. He works on the comparisons among such programs. The aim is
to develop techniques for proving the correctness of programs, and for abstracting
from irrelevant details so that it is clear when two programs are realisations of
the same algorithm. In short, the objective is to understand when and why two
programs can be considered “intensionally” equivalent.
To this end, Milner proposes — appropriately adapting it to his setting — the
algebraic notion of weak homomorphism that we have described in Section 3.4.
He renames weak homomorphism as simulation, a term that better conveys the
idea of the application in mind.
Although the deﬁnition of simulation is still
algebraic, Milner now clearly spells out its operational meaning. But perhaps the
most important contribution in his papers is the proof technique associated to
simulation that he strongly advocates. This techniques amounts to exhibiting the
set of pairs of related states, and then checking the simulation clauses on each pair.
The strength of the technique is precisely the locality of the checks that have to be
made, in the sense that we only look at the immediate transitions that emanate
from the states, as commented in Section 3.1. The technique is proposed to prove
not only results of simulation, but also results of input/output correctness for
programs, as a simulation between programs implies appropriate relationships on
their inputs and outputs. Besides the algebraic theory of automata, other earlier
works that have been inﬂuential for Milner are those on program correctness,
notably Floyd [Floyd, 1967], Manna [Manna, 1969], and Landin [Landin, 1969],
who pioneers the algebraic approach to programs.
Formally, however, Milner’s simulation remains the same as weak homomor-
phism and as such it is not today’s simulation.
Programs for Milner are de-
terministic, with a total transition function, and these hypotheses are essential.
Non-deterministic and concurrent programs or, more generally, programs whose
computations are described by trees rather than sequences, are mentioned in the
conclusions for future work. It is quite possible that if this challenge had been
quickly taken up, then today’s notion of simulation (or even bisimulation) would
have been discovered much earlier.
Milner himself, later in the 1970s, does study concurrency very intensively, but
under a very diﬀerent perspective: he abandons the view of parallel programs as
objects with an input/output behaviour akin to functions, in favor of the view
of parallel programs as interactive objects. This leads Milner to develop a new
theory of processes and a calculus — the Calculus of Communicating Systems,
CCS — in which the notion of behavioural equivalence between processes is fun-
damental. Milner however keeps, from his earlier works, the idea of ”locality” —
an equivalence should be based on outcomes that are local to states.
The behavioural equivalence that Milner puts forward, and that is prominent in
the ﬁrst book on CCS [Milner, 1980], is inductively deﬁned. It is the stratiﬁcation
of bisimilarity, ∼ω, that we discuss in Section 3.2. Technically, in contrast with
weak homomorphisms, ∼ω has also the reverse implication (on the transitions of
the second components of the pairs in the relation), and can be used on non-
deterministic structures. The addition of a reverse implication was not obvious.

Concurrency Theory
413
For instance, a natural alternative would have been to maintain an asymmetric
basic deﬁnition, possibly reﬁne it, and then take the induced equivalence closure
to obtain a symmetric relation (if needed). Indeed, among the main behavioural
equivalences in concurrency — there are several of them, see [Glabbeek, 1993;
Glabbeek, 1990b] — bisimilarity is the only one that is not naturally obtained as
the equivalence-closure of a preorder.
With Milner’s advances, the notion of bisimulation is almost there: what was
left was to turn an inductive deﬁnition into a coinductive one. This will be David
Park’s contribution.
It is worth pointing out that, towards the end of the 1970s, homomorphism-like
notions appear in other attempts at establishing “simulations”, or even “equiva-
lences”, between concurrent models — usually variants of Petri Nets. Good exam-
ples are John S. Gourlay, William C. Rounds, and Richard Statman [Gourlay et
al., 1979] and Kurt Jensen [Jensen, 1980], which develop previous work by Daniel
Brand [Brand, 1978] and Y. S. Kwong [Kwong, 1977].
Gourlay, Rounds, and
Statman’s homomorphisms (called contraction) relate an abstract system with a
more concrete realisation of it — in other words, a speciﬁcation with an imple-
mentation. Jensen’s proposal (called simulation), which is essentially the same
as Kwong’s strict reduction [Kwong, 1977], is used to compare the expressive-
ness of diﬀerent classes of Petri Nets. The homomorphisms in both papers are
stronger than today’s simulation or bisimulation; for instance they are functions
rather than relations. Interestingly, in both cases there are forms of “reverse im-
plications” on the correspondences between the transitions of related states. Thus
these homomorphisms, but especially those in [Gourlay et al., 1979], remind us
of bisimulation, at least in the intuition. In [Gourlay et al., 1979] and [Jensen,
1980], as well as other similar works of that period, the homomorphisms are put
forward because they represent conditions that are suﬃcient to preserve certain
important properties (such as Church-Rosser and deadlock freedom). In contrast
with Milner, little emphasis is given to the proof technique based on local checks
that they support.
For instance the deﬁnitions of the homomorphisms impose
correspondence on sequences of actions from related states.
David Park
In 1980, Milner returns to Edinburgh after a six-month appoint-
ment at Aarhus University, and completes his ﬁrst book on CCS. Towards the end
of that year, David Park begins a sabbatical in Edinburgh, and stays at the top
ﬂoor of Milner’s house.
Park is one of the leading experts in ﬁxed-point theory at the time. He makes the
ﬁnal step in the discovery of bisimulation precisely guided by ﬁxed-point theory.
Park notices that the inductive notion of equivalence that Milner is using for his
equivalence on CCS processes is based on a monotone functional over a complete
lattice. And by adapting an example by Milner, he sees that Milner’s equivalence
(∼ω) is not a ﬁxed point for the functional, and that therefore the functional is
not cocontinuous. He then deﬁnes bisimilarity as the greatest ﬁxed point of the

414
Jos C. M. Baeten and Davide Sangiorgi
functional, and derives the bisimulation proof method from the theory of greatest
ﬁxed points.
Further, Park knows that, to obtain the greatest ﬁxed point of
the functional in an inductive way, the ordinals and transﬁnite induction, rather
then the naturals and standard induction, are needed ([Sangiorgi, 2012]). Milner
immediately and enthusiastically adopts Park’s proposal.
Milner knew that ∼ω is not invariant under transitions. Indeed he is not so much
struck by the diﬀerence between ∼ω and bisimilarity as behavioural equivalences,
as the processes exhibiting such diﬀerences can be considered rather artiﬁcial.
What excites him is the coinductive proof technique for bisimilarity. Both bisim-
ilarity and ∼ω are rooted in the idea of locality, but the coinductive method of
bisimilarity further facilitates proofs. In the years to come Milner makes bisimu-
lation popular and the cornerstone of the theory of CCS [Milner, 1989].
In computer science, the standard reference for bisimulation and the bisimu-
lation proof method is Park’s paper “Concurrency on Automata and Inﬁnite Se-
quences” [Park, 1981a] (one of the most quoted papers in concurrency). However,
Park’s discovery is only partially reported in [Park, 1981a], whose main topic is
a diﬀerent one, namely omega-regular languages (extensions of regular languages
to inﬁnite sequences) and operators for fair concurrency. Bisimulation appears at
the end, as a secondary contribution, as a proof technique for trace equivalence
on automata. Bisimulation is ﬁrst given on ﬁnite automata, but only as a way
of introducing the concept on the B¨uchi-like automata investigated in the paper.
Here, bisimulation has additional clauses that make it non-transitive and diﬀerent
from the deﬁnition of bisimulation we know today. Further, bisimilarity and the
coinduction proof method are not mentioned in the paper.
Indeed, Park never writes a paper to report on his ﬁndings about bisimulation.
It is possible that this does not appear to him a contribution important enough
to warrant a paper: he considers bisimulation a variant of the earlier notion of
simulation by Milner [Milner, 1970; Milner, 1971a]; and it is not in Park’s style
to write many papers. A good account of Park’s discovery of bisimulation and
bisimilarity are the summary and the slides of his talk at the 1981 Workshop on
the Semantics of Programming Languages [Park, 1981b].
3.5
Discussion
It remains puzzling why bisimulation has been discovered so late in computer sci-
ence. For instance, in the 1960s weak homomorphism is well-known in automata
theory and, as discussed in Section 3.4, this notion is not that far from simula-
tion. Another emblematic example, again from automata theory, is given by the
algorithm for minimisation of deterministic automata, already known in the 1950s
[Huﬀman, 1954; Moore, 1956] (also related to this is the Myhill-Nerode theorem
[Nerode, 1958]). The aim of the algorithm is to ﬁnd an automaton equivalent
to a given one but minimal in the number of states. The algorithm proceeds by
progressively constructing a relation S with all pairs of non-equivalent states. It
roughly goes as follows. First step (a) below is applied, to initialise S; then step

Concurrency Theory
415
(b), where new pairs are added to S, is iterated until a ﬁxed point is reached, i.e.,
no further pairs can be added.
a. For all states s, t, if s ﬁnal and t is not, or vice versa, then s S t.
b. For all states s, t such that ¬(s S t): if there is a such that Ta(s) S Ta(t)
then s S t.
The ﬁnal relation gives all pairs of non-equivalent states. Then its complement,
say S, gives the equivalent states. In the minimal automaton, the states in the
same equivalence class for S are collapsed into a single state.
The algorithm strongly reminds us of the partition reﬁnement algorithms for
computing bisimilarity and for minimisation modulo bisimilarity, discussed in
[Aceto et al., 2012]. Indeed, the complement relation S that one wants to ﬁnd
has a natural coinductive deﬁnition, as a form of bisimilarity, namely the largest
relation R such that
1. if s R t then either both s and t are ﬁnal or neither is;
2. for each a, if s R t then Ta(s) R Ta(t).
Further, any relation R that satisﬁes the conditions (1) and (2) — that is, any
bisimulation — only relates pairs of equivalent states and can therefore be used
to determine equivalence of speciﬁc states.
The above deﬁnitions and algorithm are for deterministic automata. Bisimula-
tion would have been interesting also on non-deterministic automata. Although
on such automata bisimilarity does not coincide with trace equivalence — the
standard equality on automata — at least bisimilarity implies trace equivalence
and the bisimilarity-checking problem has a better complexity (P-complete [Al-
varez et al., 1991; Balc´azar et al., 1992], rather than PSPACE-complete [Meyer
and Stockmeyer, 1972; Kanellakis and Smolka, 1990]).
Lumpability in probability theory
An old concept in probability theory that
today may be viewed as somehow reminiscent of bisimulation is Kemeny and Snell’s
lumpability [Kemeny and Snell, 1960]. A lumping equivalence is a partition of the
states of a continuous-time Markov chain. The partition must satisfy certain con-
ditions on probabilities guaranteeing that related states of the partition can be
collapsed (i.e., “lumped”) into a single state.
These conditions, having to do
with sums of probabilities, are rather diﬀerent from the standard one of bisim-
ulation. (Kemeny and Snell’s lumpability roughly corresponds to what today is
called bisimulation for continuous-time Markov chains in the special case where
there is only one label for transitions.)
The ﬁrst coinductive deﬁnition of behavioural equivalence, as a form of bisim-
ilarity, that takes probabilities into account appears much later, put forward by
Larsen and Skou [Larsen and Skou, 1991]. This paper is the initiator of a vast
body of work on coinductive methods for probabilistic systems in computer science.

416
Jos C. M. Baeten and Davide Sangiorgi
Larsen and Skou were not inﬂuenced by lumpability. The link with lumpability
was in fact noticed much later [Buchholz, 1994].
In conclusion: in retrospective we can see that Kemeny and Snell’s lumpability
corresponds to a very special form of bisimulation (continuous-time Markov chains,
only one label). However, Kemeny and Snell’s lumpability has not contributed to
the discovery of coinductive concepts such as bisimulation and bisimilarity.
Coinduction
In computer science, the discovery of bisimulation and bisimilarity
led to the formulation of the more general principles of coinduction. For this, an
important paper has been Milner and Tofte’s [Milner and Tofte, 1991], who use
coinduction to prove the soundness of a type system, and describe coinduction
to explain the analogy between the technique for types in their paper and the
bisimulation techniques. The main objective of the paper is indeed to advocate
the proof technique and to suggest the name coinduction for it.
Coinduction has to do with greatest-ﬁxed points, and certainly these had al-
ready appeared in computer science. For instance, David Park, throughout the
1970s, works intensively on fairness issues for programs that may contain con-
structs for parallelism and that may not terminate. The ﬁxed-point techniques
he uses are rather sophisticated, involving alternation of least and greatest ﬁxed
points. Park discusses his ﬁndings in several public presentations. A late overview
paper is [Park, 1979]; we already pointed out, talking of Park, that he did not
publish much. Willem-Paul de Roever [de Roever, 1977] strongly advocates the
coinduction principle as a proof technique (he calls it “greatest ﬁxed point in-
duction”).
De Roever uses the technique to reason about divergence, bring-
ing up the duality between this technique and inductive techniques that had
been proposed previously to reason on programs. Coinduction and greatest ﬁxed
points are implicit in a number of earlier works in the 1960s and 1970s.
Im-
portant examples, with a huge literature, are the works on uniﬁcation, for in-
stance on structural equivalence of graphs, and the works on invariance prop-
erties of programs.
Fixed points are also central in stream processing systems
(including data ﬂow systems). The introduction of streams in computer science
is usually attributed to Peter Landin, in the early 1960s (see [Landin, 1965a;
Landin, 1965b] where Landin discusses the semantics of Algol 60 as a mapping
into a language based on the λ-calculus and Landin’s SECD machine [Landin,
1964], and historical remarks in [Burge, 1975]). However, ﬁxed points are explic-
itly used to describe stream computations only after Scott’s theory of domain,
with the work of Gilles Kahn [1974].
3.6
The origins of bisimulation in Modal Logics
Philosophical Logic studies and applies logical techniques to problems of interest
to philosophers, somewhat similarly to what Mathematical Logic does for prob-
lems that interest mathematicians. Of course, the problems do not only concern
philosophers or mathematicians; for instance nowadays both philosophical and

Concurrency Theory
417
mathematical logics have deep and important connections with computer science.
Strictly speaking, in philosophical logic a modal logic is any logic that uses
modalities. A modality is an operator used to qualify the truth of a statement,
that is, it creates a new statement that makes an assertion about the truth of the
original statement.
Labelled Transition Systems (LTSs) with a valuation, also called Kripke models,
are the standard models for modal logics; these are like the LTSs of Deﬁnition 1 ex-
cept that each state is associated to a set of proposition letters. For the discussion
below we use the following logic:
φ
def
= p | ¬φ | φ1 ∧φ2 | ⟨µ⟩φ | ⊥
where p is a proposition letter. Formula ⟨µ⟩φ holds at a state t if φ holds in at
least one of the µ-derivatives of t; and p holds at t if p is among the letters assigned
to t; the interpretation of the other operators is the standard one of propositional
logic.
Today, some of the most interesting results on the expressiveness of modal logics
rely on the notion of bisimulation. Bisimulation is indeed discovered in modal
logic when researchers begin to investigate seriously issues of expressiveness for
the logics, in the 1970s. For this, important questions tackled are: When is the
truth of a formula preserved when the model changes? Or, even better, under
which model constructions are modal formulas invariant?
Which properties of
models can modal logics express? (When moving from a model M to another
model N, preserving a property means that if the property holds in M then it
holds also when one moves to N; the property being invariant means that also the
converse is true, that is, the property holds in M iﬀit holds when one moves to
N.)
To investigate such questions, it is natural to start from the most basic structure-
preserving construction, that of homomorphism. A homomorphism from a model
M to a model N is a function F from the points of M to the points of N such
that
• whenever a proposition letter holds at a point s of M then the same letter
also holds at F(s) in N;
• whenever there is a µ-transition between two points s, s′ in M then there is
also a µ-transition between F(s) and F(s′) in N.
Thus, contrasting homomorphism with bisimulation, we note that
(i) homomorphism is a functional, rather than relational, concept;
(ii) in the deﬁnition of homomorphism there is no back condition; i.e., the reverse
implication, from transitions in N to those in M, is missing.
Homomorphisms are too weak to respect the truth of modal formulas. That is,
a homomorphism H from a model M to a model N does not guarantee that if a

418
Jos C. M. Baeten and Davide Sangiorgi
M =
P

Q
N =
R
@
@
@
@
@
@
@

S
T
Figure 2. On p-morphisms and p-relations
formula holds at a point t of M then the same formula also holds at H(t) in N.
For instance, consider a model M with just one point and no transitions, and a
model N with two points and µ-transitions between them. A homomorphism can
send the point of M onto any of the points of N. The formula ¬⟨µ⟩¬⊥, however,
which holds at points that have no transitions, will be true in M, and false in N.
The culprit for the failure of homomorphisms is the lack of a back condition.
Krister Segerberg added a back condition in his famous dissertation [Segerberg,
1971], as the requirement of p-morphisms:
• if a propositional letter holds at F(s) in N then it also holds at s in M; and
if in N there is a transition F(s)
µ→t, for some point t, then in M there
exists a point s′ such that s
µ→s′ and t = F(s′).
Segerberg starts the study of morphisms between models of modal logics that
preserve the truth of formulas in [Segerberg, 1968].
The p-morphisms can be
regarded as the natural notion of homomorphism in LTSs or Kripke models.
Still, p-morphisms do not capture all situations of invariance. That is, there
can be states s of a model M and t of a model N that satisfy exactly the same
modal formulas and yet there is no p-morphisms that takes s into t or vice versa.
The next step is made by Johan van Benthem in his PhD thesis [Benthem, 1976]
(the book [Benthem, 1983] is based on the thesis), who generalises the directional
relationship between models in a p-morphism (the fact that a p-morphism is a
function) to a symmetric one. This leads to the notion of bisimulation, which van
Benthem calls p-relation. (Later [Benthem, 1984] he renames p-relations as zigzag
relations.) On Kripke models, a p-relation between models M and N is a total
relation S on the states of the models (the domain of S are the states of M and
the codomain the states of N) such that whenever s S t then: a propositional
letter holds at s iﬀit holds at t; for all s′ with s
µ→s′ in M there is t′ such that
t
µ→t′ in N and s′ S t′; the converse of the previous condition, on the transitions
from t.
To appreciate the diﬀerence between p-morphisms and p-relations, consider the
models in Figure 2 (where the letters are used to name the states, they do not
represent proposition letters — there are no proposition letters, in fact). There
is no p-morphisms from M to N: the image of Q must be either S or T; in any
case, there is always a transition from R that P cannot match. We can however

Concurrency Theory
419
establish a p-relation on the models, relating P with R, and Q with both S and T.
(There is a p-morphism in the opposite direction, from N to M; but the example
could be developed a bit so that there is no p-morphisms in either direction.)
Van Benthem deﬁnes p-relations while working on Correspondence Theory, pre-
cisely the relationship between modal and classical logics. Van Benthem’s objec-
tive is to characterise the fragment of ﬁrst-order logic that “corresponds” to modal
logic — an important way of measuring expressiveness. He gives a sharp answer
to the problem, via a theorem that is today called “van Benthem Characterisation
Theorem”. In today’s terminology, van Benthem’s theorem says that a ﬁrst-order
formula A containing one free variable is equivalent to a modal formula iﬀA is
invariant for bisimulations. That is, modal logic is the fragment of ﬁrst-order logic
whose formulas have one free variable and are invariant for bisimulation. We refer
to [Stirling, 2012] for discussions on this theorem.
After van Benthem’s theorem, bisimulation has been used extensively in modal
logic, for instance, to analyze the expressive power of various dialects of modal
logics, to understand which properties of models can be expressed in modal logics,
and to deﬁne operations on models that preserve the validity of modal formulas.
3.7
The origins of bisimulation in set theory
In Mathematics, bisimulation and coinduction have been introduced in the study
of the foundations of theories of non-well-founded sets. Non-well-founded sets are,
intuitively, sets that are allowed to contain themselves; they are ’inﬁnite in depth’.
More precisely, the membership relation on sets may give rise to inﬁnite descending
sequences
. . . An ∈An−1 ∈. . . ∈A1 ∈A0 .
For instance, a set Ωwhich satisﬁes the equation Ω= {Ω} is circular and as such
non-well-founded. A set can also be non-well-founded without being circular; this
can happen if there is an inﬁnite membership chain through a sequence of sets that
are all diﬀerent from each other. Bisimulation was introduced as a tool for deﬁning
the meaning of equality on non-well-founded sets; in other words, for deﬁning
what it means for two inﬁnite sets to have ‘the same’ internal structure. In model
theory, this issue is technically called extensionality: it guarantees that equal
objects cannot be distinguished within the given model. When the structure of the
objects, or the way in which the objects are supposed to be used, are non-trivial,
the ‘correct’ deﬁnition of extensionality may be non-obvious.
This is certainly
the case for non-well-founded sets, as they are objects with an inﬁnite depth.
Bisimulation was derived from the notion of isomorphism (and homomorphism),
intuitively with the objective of obtaining an equality relation that is coarser than
isomorphism but still with the guarantee that related sets have ‘the same’ internal
structure.
In ordinary (i.e., well-founded) sets, the notion of equality is expressed by Zer-
melo’s extensionality axiom: two sets are equal if they have exactly the same
elements. In other words, a set is precisely determined by its elements. This is

420
Jos C. M. Baeten and Davide Sangiorgi
uncontroversial because it is very intuitive and because it naturally allows us to
reason on equality proceeding by (transﬁnite) induction on the membership rela-
tion. For instance, we can thus establish that the relation of equality is unique.
Non-well-founded sets, by contrast, may be inﬁnite in depth, and therefore induc-
tive arguments may not be applicable. For instance, consider the sets A and B
deﬁned via the equations A = {B} and B = {A}. If we try to establish that
they are equal via the extensionality axiom we end up with a tautology (“A and
B are equal iﬀA and B are equal”) that takes us nowhere. Indeed, to reason on
non-well-founded sets we need coinductive techniques, bisimulation in primis.
A major motivation for the study of non-well-founded sets in Mathematics has
been the need of giving semantics to processes, following Robin Milner’s work in
concurrency theory. Similarly, the development of Final Semantics [Aczel, 1988;
Rutten and Turi, 1994; Rutten and Jacobs, 2012], an area of mathematics based
on coalgebras and category theory and used in the semantics of programming
languages, has been largely motivated by the interest in bisimulation. As a subject,
Final Semantics is today well developed, and gives us a rich and deep perspective
on the meaning of coinduction and its duality with induction.
Bisimulation is ﬁrst introduced in set theory by Forti and Honsell [Forti and
Honsell, 1983] (a similar notion, independently, was used by Hinnion, [Hinnion,
1980; Hinnion, 1981]), around the beginning of the 1980s). It is recognised and
becomes important with the work of Aczel and Barwise, see [Aczel, 1988; Barwise
and Moss, 1996]. We brieﬂy describe below the most important works, those by
Forti and Honsell, and by Aczel.
In [Forti and Honsell, 1983], Forti and Honsell study a number of anti-foundation
axioms, including axioms that had already appeared in the literature (such as
Scott’s [Scott, 1960]), and a new one, called X1, that gives the strongest exten-
sionality properties, in the sense that it equates more sets (we discuss X1 below
in the section, together with Aczel’s version of it).
The main objective of the
paper is to compare the axioms, and deﬁne models that prove their consistency.
Bisimulations and similar relations are used in the constructions to guarantee the
extensionality of the models.
Forti and Honsell use, in their formulation of bisimulation, functions f : A 7→
℘(A) from a set A to its powerset ℘(A). Bisimulations are called f-conservative
relations and are deﬁned along the lines of the ﬁxed-point interpretation of bisim-
ulation in Section 3.3.
We can make a state-transition interpretation of their
deﬁnitions, for a comparison with today’s deﬁnition (Deﬁnition 2). If f is the
function from A to ℘(A) in question, then we can think of A as the set of the
possible states, and of f itself as the (unlabeled) transition function; so that f(x)
indicates the set of possible “next states” for x. Forti and Honsell deﬁne the ﬁxed
point behaviour of f on the relations on A, via the functional F deﬁned as follows1.
If R is a relation on A, and s, t ∈A, then (s, t) ∈F(R) if:
• for all s′ ∈f(s) there is t′ ∈f(t) such that s′ R t′;
1We use a notation diﬀerent from Forti and Honsell here.

Concurrency Theory
421
2
>
>
>
>
>
>
>
         0
1
oo
c
        
a
b
PP
Figure 3. Sets as graphs
• the converse, i.e. for all t′ ∈f(t) there is s′ ∈f(s) such that s′ R t′.
A reﬂexive and symmetric relation R is f-conservative if R ⊆F(R); it is f-
admissible if it is a ﬁxed point of F, i.e., R = F(R).
The authors note that
F is monotone over a complete lattice, hence it has a greatest ﬁxed point (the
largest f-admissible relation). They also prove that such greatest ﬁxed point can
be obtained as the union over all f-conservative relations (the coinduction proof
principle), and also, inductively, as the limit of a sequence of decreasing relations
over the ordinals that starts with the universal relation A×A. The main diﬀerence
between f-conservative relations and today’s bisimulations is that the former are
required to be reﬂexive and symmetric.
However, while the bisimulation proof method is introduced, as derived from
the theory of ﬁxed points, it remains rather hidden in Forti and Honsell’s works,
whose main goal is to prove the consistency of anti-foundation axioms. For this
the main technique uses the f-admissible relations.
Aczel reformulates Forti and Honsell’s anti-foundation axiom X1. In Forti and
Honsell [1983], the axiom says that from every relational structure there is a unique
homomorphism onto a transitive set (a relational structure is a set equipped with
a relation on its elements; a set A is transitive if each set B that is an element of A
has the property that all the elements of B also belong to A; that is, all composite
elements of A are also subsets of A). Aczel calls the axiom AFA and expresses it
with the help of graph theory, in terms of graphs whose nodes are decorated with
sets. For this, sets are thought of as (pointed) graphs, where the nodes represent
sets, the edges represent the converse membership relation (e.g., an edge from a
node x to a node y indicates that the set represented by y is a member of the
set represented by x), and the root of the graph indicates the starting point, that
is, the node that represents the set under consideration. For instance, the sets
{∅, {∅}} and D = {∅, {D}} naturally corresponds to the graphs of Figure 3 (where
for convenience nodes are named) with nodes 2 and c being the roots. The graphs
for the well-founded sets are those without inﬁnite paths or cycles, such as the
graph on the left in Figure 3. AFA essentially states that each graph represents
a unique set. This is formalised via the notion of decoration. A decoration for a
graph is an assignment of sets to nodes that respects the structure of the edges;
that is, the set assigned to a node is equal to the set of the sets assigned to the
children of the node. For instance, the decoration for the graph on the left of
Figure 3 assigns ∅to node 0, {∅} to node 1, and {∅, {∅}} to node 2, whereas that

422
Jos C. M. Baeten and Davide Sangiorgi
for the graph on the right assigns ∅to a, {D} to b, and {∅, {D}} to c. Axiom AFA
stipulates that every graph has a unique decoration. (In Aczel, the graph plays
the role of the relational structure in Forti and Honsell, and the decoration the
role of the homomorphism into a transitive set.) In this, there are two important
facts: the existence of the decoration, and its uniqueness. The former tells us that
the non-well-founded sets we need do exist. The latter tell us what is equality for
them. Thus two sets are equal if they can be assigned to the same node of a graph.
For instance the sets Ω, A and B mentioned at the beginning of this section are
equal because the graph
•
 •
__
has a decoration in which both nodes receive Ω, and another decoration in which
the node on the left receives A and that on the right B. Bisimulation comes out
when one tries to extract the meaning of equality. A bisimulation relates sets A
and B such that
• for all A1 ∈A there is B1 ∈B with A1 and B1 related; and the converse,
for the elements of B1.
Two sets are equal precisely if there is a bisimulation relating them. The bisimula-
tion proof method can then be used to prove equalities between sets, for instance
the equality between the sets A and B above.
Aczel formulates AFA towards end 1983; he does not publish it immediately
having then discovered the earlier work of Forti and Honsell and the equivalence
between AFA and X1.
Instead, he goes on developing the theory of non-well-
founded sets, mostly through a series of lectures in Stanford between January and
March ’85, which leads to the book [Aczel, 1988]. Aczel shows how to use the
bisimulation proof method to prove equalities between non-well-founded sets, and
develops a theory of coinduction that sets the basis for the coalgebraic approach
to semantics (Final Semantics).
Up to Aczel’s book [Aczel, 1988], all the works on non-well-founded sets had
remained outside the mainstream. This changes with Aczel, for two main reasons:
the elegant theory that he develops, and the concrete motivations for studying non-
well-founded sets that he brings up, namely mathematical foundations of processes,
in this prompted by the work of Milner on CCS and his way of equating processes
with an inﬁnite behaviour via a bisimulation quotient.
4
PROCESS CALCULI
At this point we switch our attention from bisimulation and coinduction to process
calculi. Process calculi start from a syntax, a language describing the objects of
interest, elements of concurrent behavior and how they are put together. In this
section the history of process calculi is traced back to the early seventies of the
twentieth century, and developments since that time are sketched.

Concurrency Theory
423
The word ‘process’ refers to discrete behavior of agents, as discussed in Section 2.
The word ‘calculus’ refers to doing calculations with processes, in order to calculate
a property of a process, or to prove that processes are equal. We sketch the state
of research in the early seventies, and state which breakthroughs were needed in
order for the theories to appear. We consider the development of CCS, CSP and
ACP. In Section 4.7, we sketch the main developments since then.
The calculations are based on a basic set of laws that are established or postu-
lated for processes. These laws are usually stated in the form of an algebra, using
techniques and results from mathematical universal algebra (see e.g. [MacLane and
Birkhoﬀ, 1967]). To emphasize the algebraical basis, the term ‘process algebra’ is
often used instead of ‘process calculus’. Strictly speaking, a process algebra only
uses laws stated in the form of an algebra, while a process calculus can also use
laws that use multiple sorts and binding variables, thus going outside the realm
of universal algebra. A process calculus can start from a given syntax (set of op-
erators) and try to ﬁnd the laws concerning these operators that hold in a given
semantical domain, while a process algebra can start from a given syntax and a set
of laws or axioms concerning these operators, and next consider all the diﬀerent
semantical domains where these laws hold. Comparing process calculi that have
diﬀerent semantical domains works best by considering the set of laws that they
have [Glabbeek, 1990a].
On the basis of the set of laws or axioms, we can calculate, perform equational
reasoning. To compare, calculations with automata can be done by means of the
algebra of regular expressions (see e.g. [Linz, 2001]).
Since a process calculus addresses interaction, agents acting in parallel, a process
calculus will usually (but not necessarily) have a form of parallel composition as
a basic operator.
To repeat, the study of process calculi is the study of the behavior of parallel or
distributed systems based on a set of (algebraic) laws. It oﬀers means to describe
or specify such systems, and thus it has means to talk about parallel composition.
Besides this, it can usually also talk about alternative composition (choice) and a
form of sequential composition (sequencing). By means of calculation, we can do
veriﬁcation, i.e. we can establish that a system satisﬁes a certain property.
What are these basic laws of process algebra?
We can list some, that can
be called structural laws. We start out from a given set of atomic actions, and
use the basic operators to compose these into more complicated processes. We
use notations from [Baeten et al., 2010], that uniﬁes the literature on process
calculi. As basic operators, we use + denoting alternative composition, · denoting
sequential composition and ∥denoting parallel composition. Usually, there are also
neutral elements for some or all of these operators, but we do not consider these
yet. Some basic laws are the following (+ binding weakest, · binding strongest).
• x + y = y + x (commutativity of alternative composition)
• x + (y + z) = (x + y) + z (associativity of alternative composition)
• x + x = x (idempotence of alternative composition)

424
Jos C. M. Baeten and Davide Sangiorgi
• (x + y) · z = x · z + y · z (right distributivity of + over ·)
• (x · y) · z = x · (y · z) (associativity of sequential composition)
• x ∥y = y ∥x (commutativity of parallel composition)
• (x ∥y) ∥z = x ∥(y ∥z) (associativity of parallel composition)
These laws list some general properties of the operators involved. Note there is
a law stating the right distributivity of + over ·, but no law of left distributivity.
Adding the left distributivity law leads to a so-called linear time theory. Usually,
left distributivity is absent, and we speak of a branching time theory, where the
moment of choice is relevant.
We can see that there is a law connecting alternative and sequential composition.
In some cases, other connections are considered. On the other hand, we list no
law connecting parallel composition to the other operators.
It turns out such
a connection is at the heart of process algebra, and it is the tool that makes
calculation possible. In most process calculi, this law allows one to express parallel
composition in terms of the other operators, and is called an expansion theorem.
Process calculi with an expansion theorem are called interleaving process calculi,
those without (such as a calculus of Petri nets) are called partial order or true
concurrency calculi.
For a discussion concerning this dichotomy, see [Baeten,
1993].
So, we can say that any mathematical structure with three binary operations
satisfying these 7 laws is a process algebra. Most often, these structures are for-
mulated in terms of automata-like models, namely the labeled transition systems
of Deﬁnition 1. The notion of equivalence studied is usually not language equiva-
lence. Prominent among the equivalences studied is bisimulation, as discussed in
the previous section. Strictly speaking, the study of labeled transition systems,
ways to deﬁne them and equivalences on them are not part of a process calcu-
lus. We can use the term process theory as a wider notion, that encompasses also
semantical issues.
Below, we describe the history of process algebra from the early seventies to the
early eighties, by focusing on the central people involved. By the early eighties, we
can say process algebra is established as a separate area of research. Subsection 4.7
will consider the main extensions since the early eighties until the present time.
4.1
Bekiˇc
One of the people studying the semantics of parallel programs in the early seventies
was Hans Bekiˇc. He was born in 1936, and died due to a mountain accident in
1982. In the period we are speaking of, he worked at the IBM lab in Vienna,
Austria. The lab was well-known in the sixties and seventies for the work on the
deﬁnition and semantics of programming languages, and Bekiˇc played a part in
this, working on the denotational semantics of ALGOL and PL/I. Growing out
of his work on PL/I, the problem arose how to give a denotational semantics for

Concurrency Theory
425
parallel composition. Bekiˇc tackled this problem in [Bekiˇc, 1971]. This internal
report, and indeed all the work of Bekiˇc is made accessible to us through the work
of CliﬀJones [Bekiˇc, 1984]. On this book, we base the following remarks.
In [Bekiˇc, 1971], Bekiˇc addresses the semantics of what he calls “quasi-parallel
execution of processes”. From the introduction, we quote:
Our plan to develop an algebra of processes may be viewed as a high-
level approach: we are interested in how to compose complex processes
from simpler (still arbitrarily complex) ones.
Bekiˇc uses global variables, so a state ξ is a valuation of variables, and a program
determines an action A, which gives, in a state (non-deterministically) either null
iﬀit is an end-state, or an elementary step f, giving a new state fξ and rest-action
A′. Further, there are ⊔and cases denoting alternative composition, ; denoting
sequential composition, and // denoting (quasi-)parallel composition.
On page 183 in [Bekiˇc, 1984], we see the following law for quasi-parallel com-
position:
(A//B)ξ
=
(cases
Aξ : null →Bξ
(f, A′) →f, (A′//B))
⊔
(cases
Bξ : null →Aξ
(g, B′) →g, (A//B′))
and this is called the “unspeciﬁed merging” of the elementary steps of A and B.
This is deﬁnitely a pre-cursor of what later would be called the expansion law of
process calculi. It also makes explicit that Bekiˇc has made the ﬁrst paradigm shift:
the next step in a merge is not determined, so we have abandoned the idea of a
program as a function.
The book [Bekiˇc, 1984] goes on with clariﬁcations of [Bekiˇc, 1971] from a lecture
in Amsterdam in 1972. Here, Bekiˇc states that an action is tree-like, behaves like
a scheduler, so that for instance f; (g ⊔h) is not the same as (f; g) ⊔(f; h) for
elementary steps f, g, h, another example of non-functional behavior. In a letter
to Peter Lucas from 1975, Bekiˇc is still struggling with his notion of an action,
and writes:
These actions still contain enough information so that the normal op-
erations can be deﬁned between them, but on the other hand little
enough information to fulﬁl certain desirable equivalences, such as:
a; 0 = a
a; (b; c) = (a; b); c
a//b = b//a
etc.

426
Jos C. M. Baeten and Davide Sangiorgi
In a lecture on the material in 1974 in Newcastle, Bekiˇc has changed the notation
of // to ∥and calls the operator parallel composition. In giving the equations, we
even encounter a “left-parallel” operator, with laws, with the same meaning that
Bergstra and Klop will later give to their left-merge operator [Bergstra and Klop,
1982].
Concluding, we can say that Bekiˇc contributed a number of basic ingredients
to the emergence of process algebra, but we see no coherent comprehensive theory
yet.
4.2
CCS
The central person in the history of process calculi without a doubt is Robin
Milner —we already mentioned his relevance for concurrency theory in Section 3.4,
discussing bisimulation. A.J.R.G. Milner, who was born in 1934 and died in 2010,
developed his process theory CCS (Calculus of Communicating Systems) over the
years 1973 to 1980, culminating in the publication of the book [Milner, 1980] in
1980.
The oldest publications concerning the semantics of parallel composition are
[Milner, 1973; Milner, 1975], formulated within the framework of denotational
semantics, using so-called transducers. He considers the problems caused by non-
terminating programs, with side eﬀects, and non-determinism. He uses the opera-
tions ∗for sequential composition, ? for alternative composition and ∥for parallel
composition. He refers to [Bekiˇc, 1971] as related work.
Next, chronologically, are the articles [Milner, 1979; Milne and Milner, 1979].
Here, Milner introduces ﬂow graphs, with ports where a named port synchronizes
with the port with its co-name. Operators are | for parallel composition, restriction
and relabeling. The symbol ∥is now reserved for restricted parallel composition.
Structural laws are stated for these operators.
The following two papers are [Milner, 1978a; Milner, 1978b], putting in place
most of CCS as we know it. The operators preﬁxing and alternative composition
are added and provided with laws. Synchronization trees are used as a model.
The preﬁx τ occurs as a communication trace (what remains of a synchronization
of a name and a co-name). The paradigm of message passing is taken over from
[Hoare, 1978]. Interleaving is introduced as the observation of a single observer of
a communicating system, and the expansion law is stated. Sequential composition
is not a basic operator, but a derived one, using communication, abstraction and
restriction.
The paper [Hennessy and Milner, 1980], with Matthew Hennessy, formulates
basic CCS, with observational equivalence and strong equivalence deﬁned induc-
tively. Also, so-called Hennessy-Milner logic is introduced, which provides a log-
ical characterization of process equivalence. Next, the book [Milner, 1980] is the
standard process calculus reference. Here we have for the ﬁrst time in history a
complete process calculus, with a set of equations and a semantical model. He
presents the equational laws as truths about his chosen semantical domain, rather

Concurrency Theory
427
than considering the laws as primary, and investigating the range of models that
they have.
We pointed out in Section 3.4 that an important contribution, realized just after
the appearance of [Milner, 1980], is the formulation of bisimulation. This became
a central notion in process theory subsequently. The book [Milner, 1980] was later
updated in [Milner, 1989].
A related development is the birth of structural operational semantics in [Plotkin,
1981]. More can be read about this in the historical paper [Plotkin, 2004b].
To recap, CCS has the following syntax:
• A constant 0 that is the neutral element of alternative composition, the
process that shows no behavior.
It is the seed process from which other
processes can be constructed using the operators.
• For each action a from a given set of actions A, the action preﬁx operator
a. , that preﬁxes a given process with the action a: after execution of a, the
process continues. This is a restricted form of sequential composition.
• Alternative composition +. It is important to note that the choice between
the given alternatives is made by the execution of an action from one of them,
thereby discarding the alternative, not before. As a consequence, there is
the law x + 0 = x, as 0 is an alternative that cannot be chosen.
• Parallel composition |. In a parallel composition x | y, an action from x
or from y can be executed, or they can jointly execute a communication
action.
The set of actions A is divided into a set of names and a set of
co-names (for each name a there is a co-name ¯a). The joint execution of a
name and its corresponding co-name results in the execution of the special
communication action τ. The action preﬁx operator τ. has a special set of
laws called the τ-laws, that allow one to eliminate the τ action in a number
of cases. Thus, the parallel composition operator does two things at a time:
it allows a communication, and hides the result of the communication in a
number of cases (a form of abstraction).
• Recursion or ﬁxed point construction. If P is a process expression possibly
containing the variable x, then µx.P is the smallest ﬁxed point of P, the pro-
cess showing the least behavior satisfying the equation µx.P = P[µx.P/x],
where the last construct is the process expression P with all occurrences of
the variable x replaced by µx.P. The notions “smallest” and “least” refer to
the fact that only behavior is included that can be inferred from the behavior
of P and this equation. This construct is used to deﬁne processes that can
execute an unrestricted number of actions, a so-called reactive process. In
later work, Milner does not use binding of variables, but instead sees the
ﬁxed point as a new constant X, whose behavior is given by the recursive
equation X = P.

428
Jos C. M. Baeten and Davide Sangiorgi
• Restriction or encapsulation ∂H, where H is a set of names and their cor-
responding co-names, will block execution of the actions in H. By blocking
execution of the names and co-names, but always allowing τ, communication
in a parallel composition can be enforced. CCS uses a diﬀerent notation for
this operator.
• Relabeling or renaming ρf, where f is a function on actions that preserves
the co-name relation and does not rename τ.
This operator is useful to
obtain diﬀerent instances of some generically deﬁned process. CCS uses a
diﬀerent notation for this operator.
4.3
CSP
A very important contributor to the development of process calculi is Tony Hoare.
C.A.R. Hoare, born in 1934, published the inﬂuential paper [Hoare, 1978] as a
technical report in 1976. The important step is that he does away completely with
global variables, and adopts the message passing paradigm of communication,
thus realizing the second paradigm shift.
The language CSP (Communicating
Sequential Processes) described in [Hoare, 1978] has synchronous communication
and is a guarded command language (based on [Dijkstra, 1975]). No model or
semantics is provided.
This paper inspired Milner to treat message passing in
CCS in the same way.
A model for CSP was elaborated in [Hoare, 1980]. This is a model based on
trace theory, i.e. on the sequences of actions a process can perform. Later on, it
was found that this model was lacking, for instance because deadlock behavior is
not preserved. For this reason, a new model based on failure pairs was presented
in [Brookes et al., 1984] for the language that was then called TCSP (Theoretical
CSP). Later, TCSP was called CSP again. Some time later it was established
that the failure model is the least discriminating model that preserves deadlock
behavior (see e.g. [Glabbeek, 2001]). In the language, due to the presence of two
alternative composition operators, it is possible to do without a silent step like τ
altogether. The book [Hoare, 1985] gives a good overview of CSP.
Between CCS and CSP, there is some debate concerning the nature of alterna-
tive composition. Some say the + of CCS is diﬃcult to understand (“the weather
of Milner”), and CSP proposes to distinguish between internal and external non-
determinism, using two separate operators. See also [Hennessy, 1988].
The syntax of CSP from [Hoare, 1985] constitutes:
• A constant called STOP that acts like the 0 CCS, but also a constant called
SKIP (that we call 1) that is the neutral element of sequential composition.
Thus, 0 stands for unsuccessful termination and 1 for successful termination.
Both processes do not execute any action.
• Action preﬁx operators a. as in CCS. There is no τ preﬁxing.

Concurrency Theory
429
• CSP has two alternative composition operators, ⊓denoting non-deterministic
or internal choice, and □denoting external choice. The internal-choice op-
erator denotes a non-deterministic choice that cannot be inﬂuenced by the
environment (other processes in parallel) and can simply be deﬁned in CCS
terms as follows:
x ⊓y = τ.x + τ.y.
The external choice operator is not so easily deﬁned in terms of CCS (see
[Glabbeek, 1986]). It denotes a choice that can be inﬂuenced by the environ-
ment. If the arguments of the operator have initial silent non-determinism,
then these τ-steps can be executed without making a choice, and the choice
will be made as soon as a visible (non-τ) action occurs.
Because of the presence of two choice operators and a semantics that equates
more processes than bisimilarity, all silent actions that might occur in a
process expression can be removed [Bergstra et al., 1987].
• There is action preﬁxing like in CCS, but also full sequential composition.
• The parallel composition operator of CSP allows interleaving but also syn-
chronization on the same name, so that execution of an action a by both
components results in a communication action again named a. This enables
multi-way synchronization.
• Recursion is handled as in CCS.
• There is a concealment or abstraction operator that renames a set of actions
into τ. These introduced τ can subsequently be removed from an expression.
4.4
Some Other Process Calculi
Around 1980, concurrency theory and in particular process theory is a vibrant
ﬁeld with a lot of activity world wide. We already mentioned research on Petri
nets, that is an active area [Petri, 1980]. Another partial order process theory is
given in [Mazurkiewicz, 1977]. Research on temporal logic has started, see e.g.
[Pnueli, 1977].
Some other process calculi can be mentioned. We already remarked that Hoare
investigated trace theory. More work was done in this direction, e.g. by Rem [Rem,
1983]. There is also the invariants calculus [Apt et al., 1980].
Another process theory that should be mentioned is the metric approach by
De Bakker and Zucker [Bakker and Zucker, 1982a; Bakker and Zucker, 1982b].
There is a notion of distance between processes: processes that do not diﬀer in
behavior before the nth step have a distance of at most 2−n.
This turns the
domain of processes into a metric space, that can be completed, and solutions to
guarded recursive equations (a type of well-behaved recursive equations) exist by
application of Banach’s ﬁxed point theorem [Banach, 1922].

430
Jos C. M. Baeten and Davide Sangiorgi
4.5
ACP
Jan Bergstra and Jan Willem Klop in 1982 started work on a question of De
Bakker as to what can be said about solutions of unguarded recursive equations.
As a result, they wrote the paper [Bergstra and Klop, 1982]. In this paper, the
phrase “process algebra” is used for the ﬁrst time. We quote:
A process algebra over a set of atomic actions A is a structure A =
⟨A, +, ·, T, ai(i ∈I)⟩where A is a set containing A, the ai are constant
symbols corresponding to the ai ∈A, and + (union), · (concatenation
or composition, left out in the axioms), T (left merge) satisfy for all
x, y, z ∈A and a ∈A the following axioms:
PA1
x + y = y + x
PA2
x + (y + z) = (x + y) + z
PA3
x + x = x
PA4
(xy)z = x(yz)
PA5
(x + y)z = xz + yz
PA6
(x + y)Tz = xTz + yTz
PA7
axTy = a(xTy + yTx)
PA8
aTy = ay
This clearly establishes a process calculus in the framework of universal algebra.
In the paper, process algebra was deﬁned with alternative, sequential and parallel
composition, but without communication.
A model was established based on
projective sequences (a process is given by a sequence of approximations by ﬁnite
terms), and in this model, it is established that all recursive equations have a
solution. In adapted form, this paper was later published as [Bergstra and Klop,
1992]. In [Bergstra and Klop, 1984b], this process algebra PA was extended with
communication to yield the theory ACP (Algebra of Communicating Processes).
The book [Baeten and Weijland, 1990] gives an overview of ACP.
The syntax of ACP comprises:
• A constant 0 denoting inaction, as in CCS and CSP (written δ).
• A set of actions A, each element denoting a constant in the syntax. Expressed
in terms of preﬁxing and successful termination, each such constant can
be denoted as a.1, execution of a followed by successful termination. This
lumping together of two notions causes problems when the theory is extended
with explicit timing, see [Baeten, 2003].
• Alternative composition + as in CCS.
• Sequential composition · as in CSP.

Concurrency Theory
431
• The set of actions A has given on it a partial binary commutative and asso-
ciative communication function that tells when two actions can synchronize
in a parallel composition, and what is the resulting communication action.
ACP does not have a special silent action τ, each communication action is
just a regular action. Parallel composition then has interleaving and com-
munication. The ﬁnite axiomatization of parallel composition then uses an
auxiliary operator left merge as shown above, and in addition another aux-
iliary operator called communication merge.
• Encapsulation ∂H blocking a subset of actions H of A as in CCS.
• Recursion. A process constant X can be deﬁned by means of a recursive
equation X = P, where the constant may appear in the expression P. Also,
a set of constants can be deﬁned by a set of recursive equations, one for each
constant.
4.6
CCS, CSP and ACP
Comparing the three most well-known process calculi CCS, CSP and ACP, we
can say there is a considerable amount of work and applications realized in all
three of them. In that sense, there seem to be no fundamental diﬀerences between
the theories with respect to the range of applications.
Historically, CCS was
the ﬁrst with a complete theory. Diﬀerent from the other two, CSP has a least
distinguishing equational theory. More than the other two, ACP emphasizes the
algebraic aspect: there is an equational theory with a range of semantical models.
Also, ACP has a more general communication scheme: in CCS, communication
is combined with abstraction, in CSP, there is also a restricted communication
scheme.
In ensuing years, other process calculi were developed. We can mention SCCS [Mil-
ner, 1983], CIRCAL [Milne, 1983], MEIJE [Austry and Boudol, 1984], the process
calculus of Hennessy [Hennessy, 1988].
We see that over the years many process calculi have been developed, each
making its own set of choices in the diﬀerent possibilities. The reader may wonder
whether this is something to be lamented. In the paper [Baeten et al., 1991], it
is argued that this is actually a good thing, as long as there is a good exchange
of information between the diﬀerent groups, as each diﬀerent process calculus will
have its own set of advantages and disadvantages. When a certain notion is used
in two diﬀerent process calculi with the same underlying intuition, but with a
diﬀerent set of equational laws, there are some who argue for the same notation,
in order to show that we are really talking about the same thing, and others who
argue for diﬀerent notations, in order to emphasize the diﬀerent semantical setting.
With the book [Baeten et al., 2010], an integrated overview is presented of all
features of CCS, CSP and ACP, based on an algebraic presentation, together with
highlights of the main extensions since the 1980s. A good overview of developments
is also provided by the impressive handbook [Bergstra et al., 2001].

432
Jos C. M. Baeten and Davide Sangiorgi
4.7
Developments
Theory
A nice overview of the most important theoretical results since the start of process
calculi is the paper [Aceto, 2003]. Also, remaining open problems are stated there.
For a process calculus based on partial order semantics, see [Best et al., 2001].
There is a wealth of results concerning process calculi extended with some form
of recursion, see e.g. the complete axiomatization of regular processes by Milner
[Milner, 1984] or the overview on decidability in [Burkart et al., 2001]. Also, there
is a whole range of expressiveness results, some examples can be found in [Bergstra
and Klop, 1984a].
Tooling
Over the years, several software systems have been developed in order to facilitate
the application of process calculi in the analysis of systems. Here, we only men-
tion general process calculus tools. Tools that deal with speciﬁc extensions are
mentioned below.
The most well-known general tool is the Concurrency Workbench, see [Moller
and Stevens, 1999], dealing with CCS-type process calculus. There is also the
variant CWB-NC, see [Zhang et al., 2003] for the current state of aﬀairs. There
is the French set of tools CADP, see e.g. [Fernandez et al., 1996]. Further, in the
CSP tradition, there is the FDR tool (see http://www.fsel.com/).
The challenge in tool development is to combine an attractive user interface
with a powerful and fast veriﬁcation engine.
Veriﬁcation
A measure of success of process calculus is the systems that have been successfully
veriﬁed by means of techniques that come from process calculus. A good overview
can be found in [Groote and Reniers, 2001]. Process calculus focuses on equational
reasoning. Other successful techniques are model checking and theorem proving.
Combination of these diﬀerent approaches proves to be very promising.
Data
Process calculi are very successful in describing the dynamic behavior of systems.
In describing the static aspects, treatment of data is very important.
Actions
and processes are parametrized with data elements. The combination of processes
and data has received much attention over the years.
A standardized formal
description technique is LOTOS, see [Brinksma, 1989]. Another combination of
processes and data is PSF, see [Mauw, 1991] with associated tooling. The process
calculus with data µCRL (succeeded by mCRL2 [Groote and Mousavi, 2013]) has
tooling focusing on equational veriﬁcation, see e.g. [Groote and Lisser, 2001].

Concurrency Theory
433
Time
Research on process calculus extended with a quantitative notion of time started
with the work of Reed and Roscoe in the CSP context, see [Reed and Roscoe,
1988]. A textbook in this tradition is [Schneider, 2000].
There are many variants of CCS with timing, see e.g. [Yi, 1990], [Moller and
Tofts, 1990]. In the ACP tradition, work starts with [Baeten and Bergstra, 1991].
An integrated theory, involving both discrete and dense time, both relative and
absolute time, is presented in the book [Baeten and Middelburg, 2002]. Also the
theory ATP can be mentioned, see [Nicollin and Sifakis, 1994]. An overview and
comparison of diﬀerent process algebras with timing can be found in [Baeten,
2003].
Tooling has been developed for processes with timing mostly in terms of timed
automata, see e.g. UPPAAL [Larsen et al., 1997] or KRONOS [Yovine, 1997].
Equational reasoning is investigated for µCRL with timing [Usenko, 2002].
Mobility
Research on networks of processes where processes are mobile and conﬁguration
of communication links is dynamic has been dominated by the π-calculus. An
early reference is [Engberg and Nielsen, 1986], the standard reference is [Milner et
al., 1992] and the textbooks are [Milner, 1999; Sangiorgi and Walker, 2001]. The
associated tool is the Mobility Workbench, see [Victor, 1994]. Also in this domain,
it is important to gain more experience with protocol veriﬁcation. On the theory
side, there are a number of diﬀerent equivalences that have been deﬁned, and it is
not clear which is the ‘right’ one to use.
Following, other calculi concerning mobility have been developed, notably the
ambient calculus, see [Cardelli and Gordon, 2000].
As to unifying frameworks
for diﬀerent mobile calculi, Milner investigated action calculus [Milner, 1996] and
bigraphs [Milner, 2001].
Over the years, the π-calculus is considered more and more as the standard
process calculus to use. Important extensions that simplify some things are the
psi-calculi, see [Bengtson et al., 2011].
Probabilities and Stochastics
Process calculi extended with probabilistic or stochastic information have gener-
ated a lot of research. An early reference is [Hansson, 1991]. In the CSP tradition,
there is [Lowe, 1993], in the CCS tradition [Hillston, 1996], in the ACP tradition
[Baeten et al., 1995]. There is the process algebra TIPP with associated tool, see
e.g. [G¨otz et al., 1993], and EMPA, see e.g. [Bernardo and Gorrieri, 1998].
The insight that both (unquantiﬁed) alternative composition and probabilistic
choice are needed for a useful theory has gained attention, see e.g. the work in
[D’Argenio, 1999] or [Andova, 2002].

434
Jos C. M. Baeten and Davide Sangiorgi
Notions of abstraction are still a matter of continued research. The goal is to
combine functional veriﬁcation with performance analysis. A notion of approx-
imation is very important here, see e.g. [Desharnais et al., 2004]. Some recent
references are [Jonsson et al., 2001; Markovski, 2008; Georgievska, 2011].
Hybrid Systems
Systems that in their behavior depend on continuously changing variables other
than time are the latest challenge to be addressed by process calculi.
System
descriptions involve diﬀerential algebraic equations, so here we get to the border
of computer science with dynamics, in particular dynamic control theory. When
discrete events are leading, but aspects of evolution are also taken into account, this
is part of computer science, but when dynamic evolution is paramount, and some
switching points occur, it becomes part of dynamic control theory. Process calculus
research that can be mentioned is [Bergstra and Middelburg, 2005; Cuijpers and
Reniers, 2003].
In process theory, work centres around hybrid automata [Alur et al., 1995] and
hybrid I/O automata [Lynch et al., 1995]. A tool is HyTech, see [Henzinger et
al., 1995]. A connection with process calculus can be found in [Willemse, 2003;
Baeten et al., 2008].
Other Application Areas
Application of process calculus in other areas can be mentioned. A process calculus
dealing with shared resources is ACSR [Lee et al., 1994]. Process calculus has been
used to give semantics of speciﬁcation languages, such as POOL [Vaandrager, 1990]
or MSC [Mauw and Reniers, 1994]. There is work on applications in security, see
e.g. [Focardi and Gorrieri, 1995], [Abadi and Gordon, 1999] or [Schneider, 2001].
Work can be mentioned on the application of process calculi to biological processes,
see e.g. [Priami et al., 2001]. Other application areas are web services [Bravetti
and Zavattaro, 2008; Laneve and Padovani, 2013], ubiquitous computing [Honda,
2006] and workﬂow [Puhlmann and Weske, 2005].
5
CONCLUSION
In this chapter, a brief history is sketched of concurrency theory, following two
central breakthroughs. Early work centred around giving semantics to program-
ming languages involving a parallel construct. Two breakthroughs were needed:
ﬁrst of all, abandoning the idea that a program is a transformation from input to
output, replacing this by an approach where all intermediate states are important.
We consider this development by considering the history of bisimulation.
The
second breakthrough consisted of replacing the notion of global variables by the
paradigm of message passing and local variables. We consider this development
by considering the history of process calculi.

Concurrency Theory
435
In the seventies of the twentieth century, both these steps were taken, and
full concurrency theories evolved. In doing so, concurrency theory became the
underlying theory of parallel and distributed systems, extending formal language
and automata theory with the central ingredient of interaction.
In the following years, much work has been done, and many concurrency the-
ories have been formulated, extended with data, time, mobility, probabilities and
stochastics. The work is not ﬁnished, however. We formulated some challenges
for the future. More can be found in [Aceto et al., 2005].
An interesting recent development is a reconsideration of the foundations of
computation including interaction from concurrency theory. This yields a theory
of executability, which is computability integrated with interaction, see [Baeten et
al., 2012].
ACKNOWLEDGEMENTS
The authors are grateful to Luca Aceto and Rob van Glabbeek for comments on
an earlier draft of the paper. Sangiorgi’s work has been partially supported by the
ANR project 12IS02001 “PACE”.
BIBLIOGRAPHY
[Abadi and Gordon, 1999] M. Abadi and A.D. Gordon. A calculus for cryptographic protocols:
The spi calculus. Inf. Comput., 148(1):1–70, 1999.
[Aceto et al., 2005] L. Aceto, W.J. Fokkink, A. Ing´olfsd´ottir, and Z. ´Esik. Guest editors’ fore-
word: Process algebra. Theor. Comput. Sci., 335(2-3):127–129, 2005.
[Aceto et al., 2007] L. Aceto, A. Ing´olfsd´ottir, K. G. Larsen, and J. Srba. Reactive Systems:
Modelling, Speciﬁcation and Veriﬁcation. Cambridge University Press, 2007.
[Aceto et al., 2012] L. Aceto, A. Ingolfsdottir, and J. Srba. The algorithmics of bisimilarity. In
Sangiorgi and Rutten [2012].
[Aceto, 2003] L. Aceto. Some of my favourite results in classic process algebra. Bulletin of the
EATCS, 81:90–108, 2003.
[Aczel, 1988] P. Aczel. Non-well-founded Sets. CSLI lecture notes, no. 14, 1988.
[Alur et al., 1995] R. Alur,
C. Courcoubetis,
N. Halbwachs,
T.A. Henzinger,
P.-H. Ho,
X. Nicollin, A. Olivero, J. Sifakis, and S. Yovine. The algorithmic analysis of hybrid sys-
tems. Theoretical Computer Science, 138:3–34, 1995.
[Alvarez et al., 1991] Carme Alvarez, Jos´e L. Balc´azar, Joaquim Gabarr´o, and Miklos Santha.
Parallel complexity in the design and analysis on conurrent systems. In Proc. PARLE ’91:
Parallel Architectures and Languages Europe, Volume I: Parallel Architectures and Algo-
rithms, volume 505 of Lecture Notes in Computer Science, pages 288–303. Springer, 1991.
[Andova, 2002] S. Andova. Probabilistic Process Algebra. PhD thesis, Technische Universiteit
Eindhoven, 2002.
[Apt et al., 1980] K.R. Apt, N. Francez, and W.P. de Roever. A proof system for communicating
sequential processes. TOPLAS, 2:359–385, 1980.
[Austry and Boudol, 1984] D. Austry and G. Boudol. Alg`ebre de processus et synchronisation.
Theoretical Computer Science, 30:91–131, 1984.
[Baeten and Bergstra, 1991] J.C.M. Baeten and J.A. Bergstra. Real time process algebra. For-
mal Aspects of Computing, 3(2):142–188, 1991.
[Baeten and Middelburg, 2002] J.C.M. Baeten and C.A. Middelburg. Process Algebra with Tim-
ing. EATCS Monographs. Springer Verlag, 2002.

436
Jos C. M. Baeten and Davide Sangiorgi
[Baeten and Weijland, 1990] J. Baeten and W. Weijland. Process Algebra, volume 18 of Cam-
bridge Tracts in Theoretical Computer Science. Cambridge Uuniversity Press, 1990.
[Baeten et al., 1991] J.C.M. Baeten, J.A. Bergstra, C.A.R. Hoare, R. Milner, J. Parrow, and
R. de Simone. The variety of process algebra. Deliverable ESPRIT Basic Research Action
3006, CONCUR, 1991.
[Baeten et al., 1995] J.C.M. Baeten, J.A. Bergstra, and S.A. Smolka. Axiomatizing probabilistic
processes: ACP with generative probabilities. Information and Computation, 121(2):234–255,
1995.
[Baeten et al., 2008] J.C.M. Baeten, D.A. van Beek, P.J.L. Cuijpers, M.A. Reniers, J.E. Rooda,
R.R.H. Schiﬀelers, and R.J.M. Theunissen. Model-based engineering of embedded systems
using the hybrid process algebra Chi. In C. Palamidessi and F.D. Valencia, editors, Electronic
Notes in Theoretical Computer Science, volume 209. Elsevier Science Publishers, 2008.
[Baeten et al., 2010] J.C.M. Baeten, T. Basten, and M.A. Reniers.
Process Algebra: Equa-
tional Theories of Communicating Processes. Number 50 in Cambridge Tracts in Theoretical
Computer Science. Cambridge University Press, 2010.
[Baeten et al., 2012] J.C.M. Baeten, B. Luttik, and P. van Tilburg. Turing meets Milner. In
M. Koutny and I. Ulidowski, editors, Proceedings CONCUR 2012, number 7454 in Lecture
Notes in Computer Science, pages 1–20, 2012.
[Baeten, 1993] J.C.M. Baeten.
The total order assumption.
In S. Purushothaman and
A. Zwarico, editors, Proceedings First North American Process Algebra Workshop, Work-
shops in Computing, pages 231–240. Springer Verlag, 1993.
[Baeten, 2003] J.C.M. Baeten. Embedding untimed into timed process algebra: The case for
explicit termination. Mathematical Structures in Computer Science, 13:589–618, 2003.
[Bakker and Zucker, 1982a] J.W. de Bakker and J.I. Zucker. Denotational semantics of con-
currency. In Proceedings 14th Symposium on Theory of Computing, pages 153–158. ACM,
1982.
[Bakker and Zucker, 1982b] J.W. de Bakker and J.I. Zucker. Processes and the denotational
semantics of concurrency. Information and Control, 54:70–120, 1982.
[Balc´azar et al., 1992] Jos´e L. Balc´azar, Joaquim Gabarr´o, and Miklos Santha. Deciding Bisim-
ilarity is P-Complete. Formal Asp. Comput., 4(6A):638–648, 1992.
[Banach, 1922] S. Banach. Sur les op´erations dans les ensembles abstraits et leur application
aux ´equations int´egrales. Fundamenta Mathematicae, 3:133–181, 1922.
[Barwise and Moss, 1996] J. Barwise and L. Moss.
Vicious Circles: On the Mathematics of
Non-Wellfounded Phenomena. CSLI (Center for the Study of Language and Information),
1996.
[Bekiˇc, 1971] H. Bekiˇc.
Towards a mathematical theory of processes.
Technical Report TR
25.125, IBM Laboratory Vienna, 1971.
[Bekiˇc, 1984] H. Bekiˇc. Programming Languages and Their Deﬁnition (Selected Papers edited
by C.B. Jones). Number 177 in LNCS. Springer Verlag, 1984.
[Bengtson et al., 2011] J. Bengtson, M. Johansson, J. Parrow, and B. Victor.
Psi-calculi: a
framework for mobile processes with nominal data and logic. Logical Methods in Computer
Science, 7:1–44, 2011.
[Benthem, 1976] J. van Benthem. Modal Correspondence Theory. PhD thesis, Mathematisch
Instituut & Instituut voor Grondslagenonderzoek, University of Amsterdam, 1976.
[Benthem, 1983] J. van Benthem. Modal Logic and Classical Logic. Bibliopolis, 1983.
[Benthem, 1984] J. van Benthem. Correspondence theory. In D.M. Gabbay and F. Guenthner,
editors, Handbook of Philosophical Logic, volume 2, pages 167–247. Reidel, 1984.
[Bergstra and Klop, 1982] J.A. Bergstra and J.W. Klop. Fixed point semantics in process alge-
bra. Technical Report IW 208, Mathematical Centre, Amsterdam, 1982.
[Bergstra and Klop, 1984a] J.A. Bergstra and J.W. Klop. The algebra of recursively deﬁned
processes and the algebra of regular processes.
In J. Paredaens, editor, Proceedings 11th
ICALP, number 172 in LNCS, pages 82–95. Springer Verlag, 1984.
[Bergstra and Klop, 1984b] J.A. Bergstra and J.W. Klop. Process algebra for synchronous com-
munication. Information and Control, 60(1/3):109–137, 1984.
[Bergstra and Klop, 1992] J.A. Bergstra and J.W. Klop. A convergence theorem in process alge-
bra. In J.W. de Bakker and J.J.M.M. Rutten, editors, Ten Years of Concurrency Semantics,
pages 164–195. World Scientiﬁc, 1992.
[Bergstra and Middelburg, 2005] J.A. Bergstra and C.A. Middelburg. Process algebra for hybrid
systems. Theoretical Computer Science, 335, 2005.

Concurrency Theory
437
[Bergstra et al., 1987] J.A. Bergstra, J.W. Klop, and E.-R. Olderog. Failures without chaos: A
new process semantics for fair abstraction. In M. Wirsing, editor, Proceedings IFIP Conference
on Formal Description of Programming Concepts III, pages 77–103. North-Holland, 1987.
[Bergstra et al., 2001] J.A. Bergstra, A. Ponse, and S.A. Smolka, editors. Handbook of Process
Algebra. North-Holland, Amsterdam, 2001.
[Bernardo and Gorrieri, 1998] M. Bernardo and R. Gorrieri. A tutorial on EMPA: A theory
of concurrent processes with non-determinism, priorities, probabilities and time. Theoretical
Computer Science, 202:1–54, 1998.
[Best et al., 2001] E. Best, R. Devillers, and M. Koutny. A uniﬁed model for nets and process
algebras. In [Bergstra et al., 2001], pp. 945–1045, 2001.
[Brand, 1978] D. Brand. Algebraic simulation between parallel programs. Research Report RC
7206, Yorktown Heights, N.Y., 39 pp., 1978.
[Brauer and Reisig, 2006] Wilfried Brauer and Wolfgang Reisig.
Carl Adam Petri und die
”Petrinetze”. Informatik Spektrum, 29:369–374, 2006.
[Bravetti and Zavattaro, 2008] M. Bravetti and G. Zavattaro. A foundational theory of contracts
for multi-party service composition. Fundamenta Informaticae, 89:451–478, 2008.
[Brinksma, 1989] E. Brinksma, editor. Information Processing Systems, Open Systems Inter-
connection, LOTOS – A Formal Description Technique Based on the Temporal Ordering of
Observational Behaviour, volume IS-8807 of International Standard. ISO, Geneva, 1989.
[Brookes et al., 1984] S.D. Brookes, C.A.R. Hoare, and A.W. Roscoe. A theory of communicat-
ing sequential processes. Journal of the ACM, 31(3):560–599, 1984.
[Buchholz, 1994] P. Buchholz.
Markovian process algebra: composition and equivalence.
In
U. Herzog and M. Rettelbach, editors, Proc. 2nd Workshop on Process Algebras and Perfor-
mance Modelling, pages 11–30. Arbeitsberichte des IMMD, Band 27, Nr. 4, 1994.
[Burge, 1975] William H. Burge. Stream processing functions. IBM Journal of Research and
Development, 19(1):12–25, 1975.
[Burkart et al., 2001] O. Burkart, D. Caucal, F. Moller, and B. Steﬀen. Veriﬁcation on inﬁnite
structures. In [Bergstra et al., 2001], pp. 545–623, 2001.
[Cardelli and Gordon, 2000] L. Cardelli and A.D. Gordon. Mobile ambients. Theoretical Com-
puter Science, 240:177–213, 2000.
[Cuijpers and Reniers, 2003] P.J.L. Cuijpers and M.A. Reniers. Hybrid process algebra. Tech-
nical Report CS-R 03/07, Technische Universiteit Eindhoven, Dept. of Comp. Sci., 2003.
[D’Argenio, 1999] P.R. D’Argenio. Algebras and Automata for Timed and Stochastic Systems.
PhD thesis, University of Twente, 1999.
[de Roever, 1977] Willem P. de Roever. On backtracking and greatest ﬁxpoints. In Arto Salomaa
and Magnus Steinby, editors, Fourth Colloquium on Automata, Languages and Programming
(ICALP), volume 52 of Lecture Notes in Computer Science, pages 412–429. Springer, 1977.
[Desharnais et al., 2004] J. Desharnais, V. Gupta, R. Jagadeesan, and P. Panangaden. Metrics
for labeled Markov systems. Theoretical Computer Science, 318:323–354, 2004.
[Dijkstra, 1975] E.W. Dijkstra. Guarded commands, nondeterminacy, and formal derivation of
programs. Communications of the ACM, 18(8):453–457, 1975.
[Engberg and Nielsen, 1986] U. Engberg and M. Nielsen. A calculus of communicating systems
with label passing. Technical Report DAIMI PB-208, Aarhus University, 1986.
[Fernandez et al., 1996] J.-C. Fernandez, H. Garavel, A. Kerbrat, R. Mateescu, L. Mounier,
and M. Sighireanu.
CADP (CAESAR/ALDEBARAN development package): A protocol
validation and veriﬁcation toolbox. In R. Alur and T.A. Henzinger, editors, Proceedings CAV
’96, number 1102 in Lecture Notes in Computer Science, pages 437–440. Springer Verlag,
1996.
[Floyd, 1967] R. W. Floyd.
Assigning meaning to programs.
In Proc. Symposia in Applied
Mathematics, volume 19, pages 19–32. American Mathematical Society, 1967.
[Focardi and Gorrieri, 1995] R. Focardi and R. Gorrieri. A classiﬁcation of security properties
for process algebras. Journal of Computer Security, 3:5–33, 1995.
[Forti and Honsell, 1983] M. Forti and F. Honsell. Set theory with free construction principles.
Annali Scuola Normale Superiore, Pisa, Serie IV, X(3):493–522, 1983.
[Georgievska, 2011] S. Georgievska. Probability and Hiding in Concurrent Processes. PhD the-
sis, Eindhoven University of Technology, Department of Mathematics and Computer Science,
Eindhoven, the Netherlands, 2011.
[Ginzburg, 1968] A. Ginzburg. Algebraic Theory of Automata. Academic Press, 1968.

438
Jos C. M. Baeten and Davide Sangiorgi
[Glabbeek, 1986] R.J. van Glabbeek. Notes on the methodology of CCS and CSP. Technical
Report CS-R8624, Centrum Wiskunde & Informatica, Amsterdam, 1986.
[Glabbeek, 1990a] R.J. van Glabbeek. Comparative Concurrency Semantics and Reﬁnement of
Actions. PhD thesis, Vrije Universiteit, Amsterdam, 1990.
[Glabbeek, 1990b] R.J. van Glabbeek. The linear time-branching time spectrum (extended ab-
stract). In Jos C. M. Baeten and Jan Willem Klop, editors, First Conference on Concurrency
Theory (CONCUR’90), volume 458 of Lecture Notes in Computer Science, pages 278–297.
Springer, 1990.
[Glabbeek, 1993] R.J. van Glabbeek. The linear time — branching time spectrum II (the se-
mantics of sequential systems with silent moves). In E. Best, editor, Fourth Conference on
Concurrency Theory (CONCUR’93), volume 715, pages 66–81. Springer, 1993.
[Glabbeek, 2001] R.J. van Glabbeek. The linear time – branching time spectrum I. The seman-
tics of concrete, sequential processes. In [Bergstra et al., 2001], pp. 3–100, 2001.
[G¨otz et al., 1993] N. G¨otz, U. Herzog, and M. Rettelbach.
Multiprocessor and distributed
system design: The integration of functional speciﬁcation and performance analysis using
stochastic process algebras. In L. Donatiello and R. Nelson, editors, Performance Evaluation
of Computer and Communication Systems, number 729 in LNCS, pages 121–146. Springer,
1993.
[Gourlay et al., 1979] John S. Gourlay, William C. Rounds, and Richard Statman. On prop-
erties preserved by contraction of concurrent systems. In Gilles Kahn, editor, International
Symposium on Semantics of Concurrent Computation, volume 70 of Lecture Notes in Com-
puter Science, pages 51–65. Springer, 1979.
[Groote and Lisser, 2001] J.F. Groote and B. Lisser. Computer assisted manipulation of alge-
braic process speciﬁcations. Technical Report SEN-R0117, CWI, Amsterdam, 2001.
[Groote and Mousavi, 2013] J.F. Groote and M.R. Mousavi. Modelling and Analysis of Com-
municating Systems. MIT Press, 2013.
[Groote and Reniers, 2001] J.F. Groote and M.A. Reniers. Algebraic process veriﬁcation. In
[Bergstra et al., 2001], pp. 1151–1208, 2001.
[Hansson, 1991] H. Hansson. Time and Probability in Formal Design of Distributed Systems.
PhD thesis, University of Uppsala, 1991.
[Harel and Pnueli, 1985] D. Harel and A. Pnueli. On the development of reactive systems. In
Logic and Models of Concurrent Systems, NATO Advanced Study Institute on Logics and
Models for Veriﬁcation and Speciﬁcation of Concurrent Systems. Springer, 1985.
[Hennessy and Milner, 1980] M. Hennessy and R. Milner. On observing nondeterminism and
concurrency. In J.W. de Bakker and J. van Leeuwen, editors, Proceedings 7th ICALP, num-
ber 85 in Lecture Notes in Computer Science, pages 299–309. Springer Verlag, 1980.
[Hennessy, 1988] M. Hennessy. Algebraic Theory of Processes. MIT Press, 1988.
[Henzinger et al., 1995] T.A. Henzinger, P. Ho, and H. Wong-Toi. Hy-Tech: The next genera-
tion. In Proceedings RTSS, pages 56–65. IEEE, 1995.
[Hillston, 1996] J. Hillston. A Compositional Approach to Performance Modelling. PhD thesis,
Cambridge University Press, 1996.
[Hinnion, 1980] R. Hinnion. Contraction de structures et application `a NFU. Comptes Rendus
Acad. des Sciences de Paris, 290, S´er. A:677–680, 1980.
[Hinnion, 1981] R. Hinnion. Extensional quotients of structures and applications to the study
of the axiom of extensionality. Bulletin de la Soci´et´e Mathmatique de Belgique, XXXIII (Fas.
II, S´er. B):173–206, 1981.
[Hoare, 1969] C.A.R. Hoare. An axiomatic basis for computer programming. Communications
of the ACM, 12:576–580, 1969.
[Hoare, 1978] C.A.R. Hoare.
Communicating sequential processes.
Communications of the
ACM, 21(8):666–677, 1978.
[Hoare, 1980] C.A.R. Hoare. A model for communicating sequential processes. In R.M. McKeag
and A.M. Macnaghten, editors, On the Construction of Programs, pages 229–254. Cambridge
University Press, 1980.
[Hoare, 1985] C.A.R. Hoare. Communicating Sequential Processes. Prentice Hall, 1985.
[Honda, 2006] K. Honda. Process algebras in the age of ubiquitous computing. Electr. Notes
Theor. Comput. Sci., 162:217–220, 2006.
[Huﬀman, 1954] D.A. Huﬀman. The synthesis of sequential switching circuits. Journal of the
Franklin Institute (Mar. 1954) and (Apr. 1954), 257(3–4):161–190 and 275–303, 1954.

Concurrency Theory
439
[Jensen, 1980] Kurt Jensen. A method to compare the descriptive power of diﬀerent types of
petri nets.
In Piotr Dembinski, editor, Proc. 9th Mathematical Foundations of Computer
Science 1980 (MFCS’80), Rydzyna, Poland, September 1980, volume 88 of Lecture Notes in
Computer Science, pages 348–361. Springer, 1980.
[Jonsson et al., 2001] B. Jonsson, Yi Wang, and K.G. Larsen. Probabilistic extensions of process
algebras. In J.A. Bergstra, A. Ponse, and S.A. Smolka, editors, Handbook of Process Algebra,
pages 685–710. North-Holland, 2001.
[Kahn, 1974] Gilles Kahn. The semantics of simple language for parallel programming. In IFIP
Congress, pages 471–475. North-Holland, 1974.
[Kanellakis and Smolka, 1990] Paris C. Kanellakis and Scott A. Smolka. CCS expressions, ﬁnite
state processes, and three problems of equivalence. Inf. Comput., 86(1):43–68, 1990.
[Kemeny and Snell, 1960] J. Kemeny and J. L. Snell. Finite Markov Chains. Van Nostrand Co.
Ltd., London, 1960.
[Kwong, 1977] Y. S. Kwong.
On reduction of asynchronous systems.
Theoretical Computer
Science, 5(1):25–50, 1977.
[Landin, 1964] Peter J. Landin.
The mechanical evaluation of expressions.
The Computer
Journal, 6(4):308–320, 1964.
[Landin, 1965a] Peter J. Landin. Correspondence between ALGOL 60 and Church’s Lambda-
notation: Part I. Commun. ACM, 8(2):89–101, 1965.
[Landin, 1965b] Peter J. Landin. A correspondence between ALGOL 60 and Church’s Lambda-
notations: Part II. Commun. ACM, 8(3):158–167, 1965.
[Landin, 1969] P. Landin. A program-machine symmetric automata theory. Machine Intelli-
gence, 5:99–120, 1969.
[Laneve and Padovani, 2013] C. Laneve and L. Padovani. An algebraic theory for web service
contracts. In E. Broch Johnsen and L. Petre, editors, IFM, volume 7940 of Lecture Notes in
Computer Science, pages 301–315. Springer, 2013.
[Larsen and Skou, 1991] Kim Guldstrand Larsen and Arne Skou. Bisimulation through proba-
bilistic testing. Inf. Comput., 94(1):1–28, 1991. Preliminary version in POPL’89, 344–352,
1989.
[Larsen et al., 1997] K.G. Larsen, P. Pettersson, and Wang Yi. Uppaal in a nutshell. Journal
of Software Tools for Technology Transfer, 1, 1997.
[Lee et al., 1994] I. Lee, P. Bremond-Gregoire, and R.Gerber. A process algebraic approach to
the speciﬁcation and analysis of resource-bound real-time systems. Proceedings of the IEEE,
1994. Special Issue on Real-Time.
[Linz, 2001] P. Linz. An Introduction to Formal Languages and Automata. Jones and Bartlett,
2001.
[Lowe, 1993] G. Lowe. Probabilities and Priorities in Timed CSP. PhD thesis, University of
Oxford, 1993.
[Lynch et al., 1995] N. Lynch, R. Segala, F. Vaandrager, and H.B. Weinberg. Hybrid I/O au-
tomata. In T. Henzinger, R. Alur, and E. Sontag, editors, Hybrid Systems III, number 1066
in Lecture Notes in Computer Science. Springer Verlag, 1995.
[MacLane and Birkhoﬀ, 1967] S. MacLane and G. Birkhoﬀ. Algebra. MacMillan, 1967.
[Manna, 1969] Z. Manna.
The correctness of programs.
J. Computer and System Sciences,
3(2):119–127, 1969.
[Markovski, 2008] J. Markovski. Real and Stochastic Time in Process Algebras for Performance
Evaluation. PhD thesis, Eindhoven University of Technology, Department of Mathematics and
Computer Science, Eindhoven, the Netherlands, 2008.
[Mauw and Reniers, 1994] S. Mauw and M.A. Reniers. An algebraic semantics for basic message
sequence charts. The Computer Journal, 37:269–277, 1994.
[Mauw, 1991] S. Mauw. PSF: a Process Speciﬁcation Formalism. PhD thesis, University of
Amsterdam, 1991. See http://carol.science.uva.nl/~psf/.
[Mazurkiewicz, 1977] A. Mazurkiewicz. Concurrent program schemes and their interpretations.
Technical Report DAIMI PB-78, Aarhus University, 1977.
[McCarthy, 1963] J. McCarthy. A basis for a mathematical theory of computation. In P. Braﬀort
and D. Hirshberg, editors, Computer Programming and Formal Systems, pages 33–70. North-
Holland, Amsterdam, 1963.
[Meyer and Stockmeyer, 1972] Albert R. Meyer and Larry J. Stockmeyer. The equivalence prob-
lem for regular expressions with squaring requires exponential space. In 13th Annual Sympo-
sium on Switching and Automata Theory (FOCS), pages 125–129. IEEE, 1972.

440
Jos C. M. Baeten and Davide Sangiorgi
[Milne and Milner, 1979] G.J. Milne and R. Milner.
Concurrent processes and their syntax.
Journal of the ACM, 26(2):302–321, 1979.
[Milne, 1983] G.J. Milne. CIRCAL: A calculus for circuit description. Integration, 1:121–160,
1983.
[Milner and Tofte, 1991] R. Milner and M. Tofte. Co-induction in relational semantics. Theo-
retical Computer Science, 87:209–220, 1991. Also Tech. Rep. ECS-LFCS-88-65, University of
Edinburgh, 1988.
[Milner et al., 1992] R. Milner, J. Parrow, and D. Walker.
A calculus of mobile processes.
Information and Computation, 100:1–77, 1992.
[Milner, 1970] R. Milner. A formal notion of simulation between programs. Memo 14, Computers
and Logic Resarch Group, University College of Swansea, U.K., 1970.
[Milner, 1971a] R. Milner. An algebraic deﬁnition of simulation between programs. In Proc.
2nd Int. Joint Conferences on Artiﬁcial Intelligence. British Comp. Soc. 1971.
[Milner, 1971b] R. Milner. Program simulation: an extended formal notion. Memo 17, Com-
puters and Logic Resarch Group, University College of Swansea, U.K., 1971.
[Milner, 1973] R. Milner. An approach to the semantics of parallel programs. In Proceedings
Convegno di informatica Teoretica, pages 285–301, Pisa, 1973. Instituto di Elaborazione della
Informazione.
[Milner, 1975] R. Milner. Processes: A mathematical model of computing agents. In H.E. Rose
and J.C. Shepherdson, editors, Proceedings Logic Colloquium, number 80 in Studies in Logic
and the Foundations of Mathematics, pages 157–174. North-Holland, 1975.
[Milner, 1978a] R. Milner. Algebras for communicating systems. In Proc. AFCET/SMF joint
colloquium in Applied Mathematics, Paris, 1978.
[Milner, 1978b] R. Milner.
Synthesis of communicating behaviour.
In J. Winkowski, editor,
Proc. 7th MFCS, number 64 in LNCS, pages 71–83, Zakopane, 1978. Springer Verlag.
[Milner, 1979] R. Milner. Flowgraphs and ﬂow algebras. Journal of the ACM, 26(4):794–818,
1979.
[Milner, 1980] R. Milner. A Calculus of Communicating Systems, volume 92 of Lecture Notes
in Computer Science. Springer, 1980.
[Milner, 1983] R. Milner. Calculi for synchrony and asynchrony. Theoretical Computer Science,
25:267–310, 1983.
[Milner, 1984] R. Milner. A complete inference system for a class of regular behaviours. Journal
of Computer System Science, 28:439–466, 1984.
[Milner, 1989] R. Milner. Communication and Concurrency. Prentice Hall, 1989.
[Milner, 1996] R. Milner. Calculi for interaction. Acta Informatica, 33:707–737, 1996.
[Milner, 1999] R. Milner.
Communicating and Mobile Systems: the π-Calculus.
Cambridge
University Press, 1999.
[Milner, 2001] R. Milner. Bigraphical reactive systems. In K.G. Larsen and M. Nielsen, editors,
Proceedings CONCUR ’01, number 2154 in LNCS, pages 16–35. Springer Verlag, 2001.
[Moller and Stevens, 1999] F. Moller and P. Stevens. Edinburgh Concurrency Workbench user
manual (version 7.1). Available from http://www.dcs.ed.ac.uk/home/cwb/, 1999.
[Moller and Tofts, 1990] F. Moller and C. Tofts. A temporal calculus of communicating systems.
In J.C.M. Baeten and J.W. Klop, editors, Proceedings CONCUR’90, number 458 in LNCS,
pages 401–415. Springer Verlag, 1990.
[Moore, 1956] E.F. Moore. Gedanken experiments on sequential machines. Automata Studies,
Annals of Mathematics Series, 34:129–153, 1956.
[Nerode, 1958] A. Nerode. Linear automaton transformations. In Proc. American Mathematical
Society, volume 9, pages 541–544, 1958.
[Nicollin and Sifakis, 1994] X. Nicollin and J. Sifakis.
The algebra of timed processes ATP:
Theory and application. Information and Computation, 114:131–178, 1994.
[Owicki and Gries, 1976] S. Owicki and D. Gries. Verifying properties of parallel programs: An
axiomatic approach. Communications of the ACM, 19:279–285, 1976.
[Park, 1979] D. Park. On the semantics of fair parallelism. In Proc. Abstract Software Spec-
iﬁcations, Copenhagen Winter School, Lecture Notes in Computer Science, pages 504–526.
Springer, 1979.
[Park, 1981a] D.M.R. Park. Concurrency on automata and inﬁnite sequences. In P. Deussen,
editor, Conf. on Theoretical Computer Science, volume 104 of Lecture Notes in Computer
Science, pages 167–183. Springer, 1981.

Concurrency Theory
441
[Park, 1981b] D.M.R. Park. A new equivalence notion for communicating systems. In G. Maurer,
editor, Bulletin EATCS, volume 14, pages 78–80, 1981. Abstract of the talk presented at the
Second Workshop on the Semantics of Programming Languages, Bad Honnef, March 16–20
1981. Abstracts collected in the Bulletin by B. Mayoh.
[Petri, 1962] C.A. Petri.
Kommunikation mit Automaten.
PhD thesis, Institut fuer Instru-
mentelle Mathematik, Bonn, 1962.
[Petri, 1980] C.A. Petri. Introduction to general net theory. In W. Brauer, editor, Proc. Ad-
vanced Course on General Net Theory, Processes and Systems, number 84 in LNCS, pages
1–20. Springer Verlag, 1980.
[Plotkin, 1976] G.D. Plotkin. A powerdomain construction. SIAM Journal of Computing, 5:452–
487, 1976.
[Plotkin, 1981] G.D. Plotkin. A structural approach to operational semantics. Technical Report
DAIMI FN-19, Aarhus University, 1981. Reprinted as [Plotkin, 2004a].
[Plotkin, 2004a] G.D. Plotkin. A structural approach to operational semantics. Journal of Logic
and Algebraic Programming, 60:17–139, 2004.
[Plotkin, 2004b] G.D. Plotkin. The origins of structural operational semantics. Journal of Logic
and Algebraic Programming, 60(1):3–16, 2004.
[Pnueli, 1977] A. Pnueli. The temporal logic of programs. In Proceedings 19th Symposium on
Foundations of Computer Science, pages 46–57. IEEE, 1977.
[Pous and Sangiorgi, 2012] Damien Pous and Davide Sangiorgi. Enhancements of the bisimula-
tion proof method. In Sangiorgi and Rutten [2012].
[Priami et al., 2001] C. Priami, A. Regev, W. Silverman, and E. Shapiro. Application of stochas-
tic process algebras to bioinformatics of molecular processes. Information Processing Letters,
80:25–31, 2001.
[Puhlmann and Weske, 2005] F. Puhlmann and M. Weske. Using the pi-calculus for formalizing
workﬂow patterns.
In W.M.P. van der Aalst, B. Benatallah, F. Casati, and F. Curbera,
editors, Business Process Management, volume 3649, pages 153–168, 2005.
[Reed and Roscoe, 1988] G.M. Reed and A.W. Roscoe. A timed model for communicating se-
quential processes. Theoretical Computer Science, 58:249–261, 1988.
[Rem, 1983] M. Rem. Partially ordered computations, with applications to VLSI design. In
J.W. de Bakker and J. van Leeuwen, editors, Foundations of Computer Science IV, volume
159 of Mathematical Centre Tracts, pages 1–44. Mathematical Centre, Amsterdam, 1983.
[Rutten and Jacobs, 2012] Jan Rutten and Bart Jacobs.
(co)algebras and (co)induction.
In
Sangiorgi and Rutten [2012].
[Rutten and Turi, 1994] J. Rutten and D. Turi. Initial algebra and ﬁnal coalgebra semantics for
concurrency. In Proc. Rex School/Symposium 1993 “A Decade of Concurrency — Reﬂexions
and Perspectives”, volume 803 of Lecture Notes in Computer Science. Springer, 1994.
[Sangiorgi and Rutten, 2012] Davide Sangiorgi and Jan Rutten, editors. Advanced Topics in
Bisimulation and Coinduction. Cambridge University Press, 2012.
[Sangiorgi and Walker, 2001] D. Sangiorgi and D. Walker. The π-calculus: a Theory of Mobile
Processes. Cambridge University Press, 2001.
[Sangiorgi, 2009] Davide Sangiorgi. On the origins of bisimulation and coinduction. ACM Trans.
Program. Lang. Syst., 31(4), 2009.
[Sangiorgi, 2012] Davide Sangiorgi. Introduction to Bisimulation and Coinduction. Cambridge
University Press, 2012.
[Schneider, 2000] S.A. Schneider.
Concurrent and Real-Time Systems (the CSP Approach).
Worldwide Series in Computer Science. Wiley, 2000.
[Schneider, 2001] S.A. Schneider. Process algebra and security. In K.G. Larsen and M. Nielsen,
editors, Proceedings CONCUR ’01, number 2154 in LNCS, pages 37–38. Springer Verlag,
2001.
[Scott and Strachey, 1971] D.S. Scott and C. Strachey. Towards a mathematical semantics for
computer languages. In J. Fox, editor, Proceedings Symposium Computers and Automata,
pages 19–46. Polytechnic Institute of Brooklyn Press, 1971.
[Scott, 1960] D. Scott. A diﬀerent kind of model for set theory. Unpublished paper, given at
the 1960 Stanford Congress of Logic, Methodology and Philosophy of Science, 1960.
[Segerberg, 1968] K. Segerberg. Decidability of S4.1. Theoria, 34:7–20, 1968.
[Segerberg, 1971] Krister Segerberg.
An essay in classical modal logic.
Filosoﬁska Studier,
Uppsala, 1971.
[Stirling, 2012] Colin Stirling. Bisimulation and logic. In Sangiorgi and Rutten [2012].

442
Jos C. M. Baeten and Davide Sangiorgi
[Usenko, 2002] Y.S. Usenko. Linearization in µCRL. PhD thesis, Technische Universiteit Eind-
hoven, 2002.
[Vaandrager, 1990] F.W. Vaandrager. Process algebra semantics of POOL. In J.C.M. Baeten,
editor, Applications of Process Algebra, number 17 in Cambridge Tracts in Theoretical Com-
puter Science, pages 173–236. Cambridge University Press, 1990.
[Victor, 1994] B. Victor. A Veriﬁcation Tool for the Polyadic π-Calculus. Licentiate thesis,
Department of Computer Systems, Uppsala University, Sweden, May 1994.
Available as
report DoCS 94/50.
[Willemse, 2003] T.A.C. Willemse. Semantics and Veriﬁcation in Process Algebras with Data
and Timing. PhD thesis, Technische Universiteit Eindhoven, 2003.
[Yi, 1990] Wang Yi. Real-time behaviour of asynchronous agents. In J.C.M. Baeten and J.W.
Klop, editors, Proceedings CONCUR’90, number 458 in LNCS, pages 502–520. Springer Ver-
lag, 1990.
[Yovine, 1997] S. Yovine. Kronos: A veriﬁcation tool for real-time systems. Journal of Software
Tools for Technology Transfer, 1:123–133, 1997.
[Zhang et al., 2003] D. Zhang, R. Cleaveland, and E. Stark. The integrated CWB-NC/PIOAtool
for functional veriﬁcation and performance analysis of concurrent systems. In H. Garavel and
J. Hatcliﬀ, editors, Proceedings TACAS ’03, number 2619 in Lecture Notes in Computer
Science, pages 431–436. Springer-Verlag, 2003.

DEGREES OF UNSOLVABILITY
Klaus Ambos-Spies and Peter A. Fejer
Reader: Richard Shore
1
INTRODUCTION
Modern computability theory took oﬀwith Turing [1936], where he introduced the
notion of a function computable by a Turing machine. Soon after, it was shown
that this deﬁnition was equivalent to several others that had been proposed previ-
ously and the Church-Turing thesis that Turing computability captured precisely
the informal notion of computability was commonly accepted. This isolation of
the concept of computable function was one of the greatest advances of twentieth
century mathematics and gave rise to the ﬁeld of computability theory.
Among the ﬁrst results in computability theory was Church’s and Turing’s work
on the unsolvability of the decision problem for ﬁrst-order logic. Computability
theory to a great extent deals with noncomputable problems. Relativized compu-
tation, which also originated with Turing, in [Turing, 1939], allows the comparison
of the complexity of unsolvable problems. Turing formalized relative computation
with oracle Turing machines. If a set A is computable relative to a set B, we say
that A is Turing reducible to B (A ≤T B). By identifying sets that are reducible
to each other, we are led to the notion of degree of unsolvability ﬁrst introduced
by Post [1944]. The degrees form a partially ordered set whose study is called
degree theory.
From the start, the study of the computably enumerable degrees has played a
prominent role in degree theory. This may be partially due to the fact that until
recently most of the unsolvable problems that have arisen outside of computability
theory are computably enumerable (c.e.). The c.e. sets can intuitively be viewed
as unbounded search problems, a typical example being those formulas provable
in some eﬀectively given formal system. Reducibility allows us to isolate the most
diﬃcult c.e. problems, the complete problems. The standard method for showing
that a c.e. problem is undecidable is to show that it is complete. Post [1944] asked
if this technique always works, i.e., whether there is a noncomputable, incomplete
c.e. set. This problem came to be known as Post’s Problem and it was the origin
of degree theory.
Degree theory became one of the core areas of computability theory and at-
tracted some of the most brilliant logicians of the second half of the twentieth
Handbook of the History of Logic. Volume 9: Computational Logic. 
Volume editor: Jörg Siekmann 
Series editors: Dov M. Gabbay and John Woods 
Copyright © 2014 Elsevier BV. All rights reserved.

444
Klaus Ambos-Spies and Peter A. Fejer
century. The fascination with the ﬁeld stems from the quite sophisticated tech-
niques needed to solve the problems that arose, many of which are quite easy
to state. The hallmark of the study of the c.e. degrees is the priority method
introduced by Friedberg and Muˇcnik to solve Post’s Problem. Advances in c.e.
degree theory were closely tied to developments of this method. For the degrees
as a whole, forcing arguments are central. Forcing techniques originated in set
theory, but their use in degree theory can be traced back to the paper of Kleene
and Post [1954] which predated the introduction of forcing in set theory.
Degree theory has been central to computability theory in the sense that the
priority method was developed to solve problems in degrees but has been applied
throughout computability theory.
In this chapter, we will limit ourselves to Turing reducibility though many other
reducibilities have been studied in computability theory. By formalizing relative
computability, Turing reducibility is the most general eﬀective reducibility but by
limiting the access to the oracle in various ways interesting special cases arise such
as many-one or truth-table reducibilities. (See e.g. Odifreddi [1999b] for more on
these so-called strong reducibilities.) Other reducibilities are obtained by either
giving up eﬀectivity of the reduction as is done for instance in the enumeration
reducibilities or the arithmetical reducibilities where computability is replaced by
computable enumerability and ﬁrst-order deﬁnability in arithmetic or by consider-
ing resource-bounded computability as is done in computational complexity. The
most prominent examples of the latter are the polynomial time reducibilities lead-
ing to the notion of NP-completeness (see the chapter of Fortnow and Homer in
this volume).
The concentration on Turing reducibility is also justiﬁed by the fact that the
core technical work in classical computability theory was done to prove results
about the Turing degrees and the main techniques in the ﬁeld were developed to
prove these results. The two structures of Turing degrees that will concern us are
the upper semi-lattices of all the Turing degrees and of the computably enumerable
Turing degrees, denoted by D and R.
Our focus will be on the structure R of the c.e. degrees. This emphasis may be
justiﬁed by the particularly important role played by the c.e. sets in mathematical
logic and by the speciﬁc challenges of this area at the dividing line of computability
and noncomputability which led to the development of fascinating new techniques
- but it also reﬂects some of our prejudice based on our research interests.
The period covered in this chapter is from the beginning of degree theory in
the 1940s till the end of the 20th century. More recent results which are directly
related to the work falling in this period are included in the presentation, and in
the ﬁnal section there are a few pointers to some of the more recent developments.
We feel, however, that it is too early for a general review and evaluation of this
more recent work from a historical point of view. The emphasis of the chapter will
be on the early developments of degree theory.
We use the term “computable” rather than “recursive” following the suggestion
of Soare [1999]. This change in terminology has been widely adopted and reﬂects

Degrees of Unsolvability
445
more accurately the nature of the subject. In the same vein, we use “computably
enumerable” for “recursively enumerable” and so on. The old terminology survives
in our use of the symbol R for the structure of the c.e. degrees. We have used
little notation in this chapter and what we do use is standard and can be found in
the relevant chapters of the Handbook of Computability Theory [Griﬀor, 1999].
2
FROM PROBLEMS TO DEGREES
In this section we trace the origins of the central concepts underlying degree theory.
The history of the concept of computable function has been dealt with in detail in
the literature (see for instance Kleene [1981] and Soare [1999]). For this reason,
we do not discuss the topic here. The study of degrees of unsolvability begins with
the concept of Turing reducibility which originated with Turing in Section 4 of
[Turing, 1939]. As Post puts it in [Post, 1944], Turing presents the deﬁnition as
a “side issue.” In the paper, Turing deﬁnes an “o-machine” (oracle machine) as
an oracle Turing machine as we understand the concept today, but with a ﬁxed
oracle, namely, the set of well-formed formulas A (i.e., λ-terms) that are dual
(i.e., have the property that A(n) is convertible to 2 for every well-formed formula
n representing a positive integer). Turing is interested in such oracle machines
because he considers a problem to be number-theoretic if it can be solved by an
o-machine. Turing proves the problem of determining if an o-machine is “circle-
free” (i.e., prints out inﬁnitely many 0s and 1s) is not a number-theoretic problem.
He then does not come back to the idea of oracle machine in the rest of the paper.
Although Turing does not consider arbitrary oracles and hence does not give a
general deﬁnition of relative computability, as Post puts it in [Post, 1944], Turing’s
formulation “can immediately be restated as the general formulation of ‘recursive
reducibility’ of one problem to another.” Post himself does not give any formal
deﬁnition of Turing reducibility in his 1944 paper, but instead relies on an intuitive
description. Although it is clear then that it was known since at least 1944 how to
give a general deﬁnition of relative computability based on oracle Turing machines,
the ﬁrst occurrence of such a deﬁnition given completely is in Kleene’s Introduction
to Metamathematics [Kleene, 1952]. By diﬀerent means, the ﬁrst formal deﬁnition
of relative computability to appear in print was in Kleene [1943] which used general
recursive functions. A deﬁnition of relative computability using canonical sets is
in Post’s 1948 abstract [Post, 1948].
The next concept fundamental to degree theory is that of degree itself. Post
[1944] deﬁnes two unsolvable problems to have the same degree of unsolvability if
each is reducible to the other, one to have lower degree of unsolvability than the
other if the ﬁrst is reducible to the second but the second is not reducible to the
ﬁrst, and to have incomparable degree of unsolvability if neither is reducible to the
other. The abstraction of this idea to achieve the current concept of degree as an
equivalence class of sets of natural numbers each reducible to the other appears
ﬁrst in the Kleene-Post paper [1954]. (Actually, in this paper a degree is deﬁned
as an equivalence class of number-theoretic functions, predicates and sets, but the

446
Klaus Ambos-Spies and Peter A. Fejer
authors realize that there would be no loss of generality in considering sets only.)
This same paper is the ﬁrst place where the upper semi-lattice structure of the
Turing degrees is described in print.
The origin of the concept of computable enumerability is more straightforward.
The concept ﬁrst appeared in print in Kleene’s article [1936] and his deﬁnition
is equivalent to the modern one except that he does not allow the empty set as
computably enumerable. (Of course he used the term “recursively enumerable”
instead of “computably enumerable”.) Post in 1921 invented an equivalent concept
which he called generated set. This work was not submitted for publication until
1941 and did not appear until 1965 in a collection of early papers in computability
theory edited by Martin Davis [Post, 1965].
The ﬁnal concept whose origins we wish to comment on is the jump operator. In
1936, Kleene showed in [Kleene, 1936] that K = {x : {x}(x) ↓} (or more precisely,
the predicate ∃yT(x, x, y)) is computably enumerable but not computable. Not
having a deﬁnition of reducibility at this point, Kleene could not show that K was
complete (i.e., that every computably enumerable set is reducible to K). In his
1943 paper, Kleene again shows that K is c.e. but not computable and here he has
a deﬁnition of reducibility, but the completeness of K is not shown. Thus it was
Post in his 1944 paper who ﬁrst showed the completeness of K, in fact he showed
that every c.e. set is 1-reducible to K. (Actually, Post’s set K is equivalent to
{⟨x, y⟩: {x}(y) ↓}.) Post used the term “complete” to describe K, but wrote in
a footnote “Just how to abstract from K the property of completeness is not, at
the moment, clear.” By 1948, the abstract concept of completeness had become
clear to Post, because he wrote in his abstract [Post, 1948], that to each set S
of positive integers, he associated a “complete” S-canonical set S′ (S-canonical
is equivalent to computably enumerable in S) and each S-canonical set is Turing
reducible to S′, while S′ is not reducible to S. Post did not give the deﬁnition
of S′ in his abstract, nor did he publish his work later. Thus, the ﬁrst published
proof that for each set A there is a set A′ complete for A in the sense of Post is
due to Kleene [1952]. The ﬁnal step in the introduction of the jump operator is in
Kleene and Post [1954], where it is shown that if A and B are in the same Turing
degree, then so are their jumps, so the jump is well-deﬁned on degrees.
The arithmetic hierarchy was invented by Kleene [1943] and independently by
Mostowski [1947]. The connection between the arithmetic hierarchy and the jump
appears to be due to Post, but he never published it. In his 1948 abstract, Post
announces the result that for all n, both of the classes Σn+1, Πn+1 contain a set
of higher degree of unsolvability than any set in ∆n+1. The obvious way to see
this is the recognition that a set is in Σn+1 if and only if it is one-one reducible
to ∅(n+1) and that a set is ∆n+1 if and only if it is Turing reducible to ∅(n). Post
gives no indication of how his theorem is proven except that it is connected with
the scale of sets ∅, ∅′, ∅′′, . . .. The theorem that a set is ∆n+1 if and only if it is
Turing reducible to a ﬁnite collection of Σn and Πn sets is attributed by Kleene
[1952] to Post and this abstract, and while this result does not explicitly involve
the jump, it suggests again that Post was using the sets ∅(n) for his result.

Degrees of Unsolvability
447
3
ORIGINS OF DEGREE THEORY
Having looked at the origin of the basic concepts of degree theory, we now turn to
the papers that founded the subject.
The ﬁrst paper in degree theory, and perhaps the most important, is Emil
Post’s 1944 paper “Recursively enumerable sets of positive integers and their de-
cision problems.”
Beyond the completeness of K, this paper does not contain
any results on the Turing degrees. Its importance lies rather in what has become
known as Post’s Problem and Post’s Program, as well as in the attention it drew
to the ﬁeld of degree theory, particularly the computably enumerable degrees, and
the clarity of its exposition. The results that do occur in the paper were of great
importance in two other related ﬁelds of computability theory, strong reducibili-
ties and the lattice of c.e. sets under inclusion. Post’s Problem is the question of
whether there exists a computably enumerable set that is neither computable nor
complete. In degree-theoretic terms, the problem is whether there are more than
two c.e. Turing degrees. Post’s Problem received a lot of attention, and the solution
ﬁnally obtained for the problem introduced the priority method, the most impor-
tant proof technique in computably enumerable degree theory. Post’s Program
was to try to construct a c.e. set that is neither computable nor complete by deﬁn-
ing a structural property of a set, proving that sets with the structural property
exist, and then showing that any set with the structural property must be noncom-
putable and incomplete. Post in particular tried to use thinness properties of the
complement of a set to achieve this goal. Though Post failed to achieve this goal
for Turing reducibility, he succeeded for some stronger reducibilities he introduced
in his paper, namely one-one (1), many-one (m), bounded truth-table (btt) and
truth-table (tt) reducibilities. These reducibilities, although not as fundamental
as Turing reducibility, are very natural and have been widely studied. For show-
ing the existence of noncomputable btt-incomplete (hence m- and 1-incomplete)
sets, Post introduced simple sets, i.e., c.e. sets whose complements are inﬁnite
but contain no inﬁnite c.e. sets. He proved that simple sets exist and cannot be
bounded truth-table complete, but can be truth-table complete. Post also intro-
duced hypersimple sets, a reﬁnement of simple sets, and proved that hypersimple
sets exist and are truth-table incomplete. He suggested a further strengthening
of simplicity, namely hyperhypersimplicity, but he left open the question whether
hyperhypersimple sets exist and whether they have to be Turing incomplete.
Thus, Post initiated the study of the c.e. sets under reducibilities stronger than
Turing reducibility and showed that the structural approach is a powerful tool
in this area. Strong reducibilities have been widely studied, particularly in the
Russian school of computability theory, where the structural approach has been
used very fruitfully, although this approach has not been very successful in study-
ing the Turing degrees. Another area inﬂuenced by the results in Post’s paper
is the study of the lattice of c.e. sets. In this ﬁeld, the simple, hypersimple and
hyperhypersimple sets have played an important role.
Even though the initial solution to Post’s Problem made no use of Post’s Pro-

448
Klaus Ambos-Spies and Peter A. Fejer
gram, the program has had an inﬂuence for many decades and eventually was
justiﬁed. We describe the relevant results here. Myhill [1956] introduced the no-
tion of maximal set. A maximal set is a c.e. set whose complement is as thin
as possible, from the computability theoretic point of view, without being ﬁnite.
Yates [1965] constructed a complete maximal set thereby showing that Post’s Pro-
gram, narrowly deﬁned, cannot succeed.
However, taken in a broader sense, namely if one allows any structural property
of a c.e. set not just a thinness property of the complement, then Post’s Program
does succeed. The ﬁrst solution, due to Marchenkov [1976] and based on some
earlier result of D¨egtev [1973], in part follows Post’s approach quite closely. The
thinness notions of Post are generalized by replacing numbers with equivalence
classes of any c.e. equivalence relation η. Then it is shown that, for Tennenbaum’s
Q-reducibility, η-hyperhypersimple sets are Q-incomplete. Finally this result is
transferred to Turing reducibility by observing that any Turing complete semire-
cursive set is already Q-complete and by showing that there are semirecursive
η-hyperhypersimple sets for appropriately chosen η. So this solution combines a
thinness property, η-hyperhypersimplicity, with some other structural property,
semirecursiveness.
In an attempt to deﬁne what a natural incompleteness property is, it has been
suggested to consider lattice-theoretic properties. After Myhill [1956] observed
that the partial ordering of c.e. sets under inclusion is a lattice, this lattice E
became a common setting for studying structural properties of the c.e. sets. A
property is called lattice-theoretic if it is deﬁnable in E. Simplicity, hyperhypersim-
plicity and maximality are lattice-theoretic but hypersimplicity and Marchenkov’s
incompleteness property are not. The question whether there is a lattice-theoretic
solution of Post’s Program was answered positively by Harrington and Soare [1991].
To ﬁnish our discussion of Post’s paper, we make some comments on the style
of exposition. In general, exposition in degree theory has gone from formal to
informal. However, Post’s paper is written in a very informal and easy to read
style and has often been cited as a good example of exposition.
Post’s paper
is the text of an invited talk at the February 1944 New York meeting of the
American Mathematical Society. Post states as one of his goals to give an intuitive
presentation that can be followed by a mathematician not familiar with the formal
basis. This does not mean that Post felt that the formal proofs were not needed.
In fact, he assures his listeners that with a few exceptions, all of the results he is
reporting have been proven formally, and he indicates that he intends to publish
the results with formal proofs. (This publication was never completed.) Post adds
“Yet the real mathematics must lie in the informal development. For in every
instance the informal ‘proof’ was ﬁrst obtained; and once gotten, transforming it
into the formal proof turned out to be a routine chore.”
The next milestone in the history of degree theory was the 1954 paper of Kleene
and Post. As mentioned above, this paper introduced the degrees as an upper
semi-lattice and deﬁned the jump as an operator on degrees. The paper begins
the study of the algebraic properties of this upper semi-lattice and points out

Degrees of Unsolvability
449
additional questions about the structure which inspired much of the earliest work
on it. The idea of writing down conditions which a set to be constructed must
meet and then breaking down each condition into inﬁnitely many subconditions,
called requirements, appears here for the ﬁrst time. The paper also introduces the
coinﬁnite extension technique for constructing sets. In this technique, an increasing
sequence of coinﬁnite sets S0 ⊆S1 ⊆· · · of natural numbers is constructed along
with a sequence of binary-valued functions f0, f1, . . ., where each fi has domain
Si and each fi+1 extends fi. fn is deﬁned so that any set whose characteristic
function extends fn meets the nth requirement. Any set of natural numbers whose
characteristic function extends all the fn’s (if as usual S
i Si is the set of all natural
numbers, then there is only one such set) meets all the requirements. When each
set Si is ﬁnite, this method is called the ﬁnite extension method. The authors also
noted that the degree of the sets obtained by their constructions is bounded by
the jump of the given sets used in the construction.
Using this technique, the authors showed a large number of results including
the following:
• between every degree and its jump, there are countable anti-chains and dense
countable chains (so in particular there are incomparable degrees below 0′);
• for every nonzero degree, there is a degree incomparable with the given
degree;
• there are countable subsets of the degrees that do not have a least upper
bound;
• the degrees do not form a lattice.
All but the last of these results used the ﬁnite extension method. The last result
introduced another technique that proved to be useful - exact pairs. An ideal of
the degrees (i.e., a nonempty subset closed downward and closed under joins) has
an exact pair if there is a pair of degrees a0, a1 such that the ideal consists of
exactly those degrees below both a0 and a1. An exact pair for an ideal with no
greatest element is necessarily a pair without a meet and the paper shows that for
every degree a, the ideal consisting of the downward closure of {a, a′, a′′, . . .} has
an exact pair.
The Kleene-Post paper is signiﬁcant for many reasons. Perhaps most important
is the fact that it introduced the study of the algebraic properties of the upper
semi-lattice of the degrees as a legitimate activity. This study is still being pursued
vigorously 60 years later.
Also very important are the techniques introduced.
This includes not just the coinﬁnite extension method and the use of exact pairs,
but also a general viewpoint towards constructing sets with desired properties -
rather than the structural approach attempted earlier by Post, the Kleene-Post
approach is to list the requirements to be met and then construct a set to meet
those requirements directly. No attempt is made to ﬁnd “natural” examples. This
approach has characterized the ﬁeld till today. More speciﬁcally, the ﬁnite (or

450
Klaus Ambos-Spies and Peter A. Fejer
coinﬁnite) extension method may be viewed as both a precursor to general forcing
arguments and an important step towards the priority method, so that it led the
way to the predominant techniques (except for coding) in the ﬁeld.
Also signiﬁcant were the many questions raised in the paper. These included
questions concerning what relationships are possible between the jumps of two
degrees given the relationship between the degrees themselves, the question of
which degrees are in the range of the jump operator, and whether the degrees are
dense. Another question raised by the following sentence in the paper was the
deﬁnability of the jump:
While the operation a ∪b is characterizable intrinsically from the ab-
stract partially ordered system of the degrees as the l.u.b. of a and b,
the operation a′ may so far as we know merely be superimposed upon
this ordering.
This question has itself been studied intensely, but the question is also signiﬁcant
for having introduced a program of determining which natural operations and
subsets of the degrees are deﬁnable from the ordering. This program is still being
actively pursued and there have been notable successes (see Section 11 below).
Many of the questions raised by Kleene and Post were answered by Spector
[1956]. Most of these results were proven using the coinﬁnite extension technique,
but the fact that there are minimal degrees (i.e., minimal nonzero elements of
the degree ordering) and hence the degrees are not dense, needed a new technique.
Spector’s technique is best explained using trees (as was done by Shoenﬁeld [1966]),
although Spector did not present his method this way. A sequence of total binary
trees T0, T1, . . . is constructed with each tree a subtree of the previous one. The
trees are selected so that any set whose characteristic function lies on Tn meets the
nth requirement. A set whose characteristic function lies on all the trees meets all
the requirements.
Spector also proved that every countable ideal of the degrees has an exact pair
(an intermediate result about exact pairs was announced by Lacombe [1954]) and
that the degrees below 0′ are not a lattice.
Shoenﬁeld [1959] was also clearly
inspired by the Kleene-Post paper and proves among other things that there are
degrees below 0′ which are not computably enumerable.
The Kleene-Post paper was signiﬁcant as well for the style of presentation it
introduced. Although motivation and intuition are provided in a readable man-
ner, the actual proofs themselves are very formal, using the T predicate, and by
contemporary standards are very hard to read even though the results would not
be considered today to be that diﬃcult. Most papers in the ﬁeld, including the
papers of Spector and Shoenﬁeld cited above, were written in this style for many
years after the appearance of the Kleene-Post paper.
Two aspects of the legacy of the Kleene-Post paper have come in for criticism -
the use of purely computability-theoretic methods to prove results when techniques
from other areas could be used, and the explication of proofs in a formal way which

Degrees of Unsolvability
451
makes them hard to read. Myhill [1961] was probably making both criticisms when
he wrote:
The heavy symbolism used in the theory of recursive functions has per-
haps succeeded in alienating some mathematicians from this ﬁeld, and
also in making mathematicians who are in this ﬁeld too embroiled in
the details of thier[sic] notation to form as clear an overall picture of
their work as is desirable. In particular the study of degrees of recur-
sive unsolvability by Kleene, Post, and their successors [in a footnote,
Shoenﬁeld and Spector are mentioned here] has suﬀered greatly from
this defect, so that there is considerable uncertainty even in the minds
of those whose specialty is recursion theory as to what is superﬁcial
and what is deep in this area.
In the paper, Myhill advocates the use of Baire category methods to prove results
in degree theory. Those results which do not have such proofs can be considered
“truly ‘recursive’ ” while those results with such proofs are “merely set-theoretic”.
In his paper, Myhill proves Shoenﬁeld’s theorem [Shoenﬁeld, 1960] that there is an
uncountable collection of pairwise incomparable degrees using category methods.
He also states that a Baire category proof of the Kleene-Post theorem that there
are incomparable degrees below 0′ will be given in another publication, but this
never appeared (and it is not at all clear how such a proof would look unless Myhill
had in mind some bounded Baire category theory as described below).
Baire category methods in degree theory are also investigated in Sacks [1963b],
Martin [1967], Stillwell [1972] and Yates [1976]. If the collection of all sets with
a certain property is a comeager subset of 2ω (under the usual topology) then by
the Baire category theorem the collection is nonempty and a set with the property
exists. Martin showed the existence of a noncomputable set whose degree has no
minimal predecessors using this method.
Measure theory has also been proposed as a means to prove theorems about
degrees. This was ﬁrst done in Spector [1958]. This paper mainly concerns hy-
perdegrees and hyperjumps, but it reproves the Kleene-Post result that there is
a countably inﬁnite collection of pairwise incomparable degrees. The measure-
theoretic approach was also considered in most of the papers listed above that
considered Baire category. One way to use measure theory is to show that the col-
lection of all sets with a desired property has measure 1 (in the Lebesgue measure).
Martin’s result on minimal predecessors can be obtained this way as well.
Still, extremely few results in degree theory can be obtained by just quoting
results about Baire category or measure. There are some close relations between
Baire category and the ﬁnite extension method of Kleene and Post, however. In
this method, one shows that the collection of sets meeting each requirement con-
tains a dense open set. Thus the collection of sets meeting all the requirements is
comeager and so nonempty. (It follows that if the collection of all sets meeting all
requirements is not comeager, then the ﬁnite extension method cannot be used to
produce a set meeting all the requirements.) In fact, the standard proof of Baire’s

452
Klaus Ambos-Spies and Peter A. Fejer
Theorem may be viewed as a ﬁnite extension argument. What distinguishes a ﬁ-
nite extension argument from a pure Baire category argument is that by analyzing
the complexity of the extension strategies one obtains bounds on the complexity
of the constructed sets which a category argument does not provide since many of
the complexity classes which are of interest in computability theory are countable
hence meager.
Baire category can be made more suitable for degree theory, however, by eﬀec-
tivizing this concept. Such eﬀectivizations have been considered in terms of forcing
notions and, in particular, the typical sets obtained this way, called generic sets,
played a signiﬁcant role in the analysis of the global degrees. Feferman [1965]
introduced arithmetically generic sets and Hinman [1969] reﬁned this concept by
considering n-generic sets related to the nth level Σn of the arithmetical hier-
archy. Roughly speaking, an n-generic set has all properties that can be forced
by a Σn-extension strategy. Since the class of n-generic sets is comeager, advan-
tages of the Baire category approach are preserved but since there are n-generic
sets computable in the nth jump ∅(n), at the same time we can obtain results on
initial segments of D. For instance, we can show the existence of incomparable
degrees below 0′ by observing that the even and odd parts of any 1-generic set
are Turing-incomparable. It was Jockusch [1980] who emphasized the applicabil-
ity of these bounded genericity concepts to degree theory. For a comprehensive
survey of genericity in degree theory see Kumabe [1996]. In a similar way the ap-
plication of algorithmic randomness concepts, in particular 1-randomness due to
Martin-L¨of [1966], has made the measure approach more suitable for degree the-
ory. Good overviews of this approach can be found in the monographs by Downey
and Hirschfeldt [2010] and Nies [2009].
The arithmetical forcing notions can be viewed as special cases of Cohen forcing
[Cohen, 1963] which plays a fundamental role not only in set theory but also in
global degree theory. In general, forcing techniques became the most fundamental
tool for the study of the structure D. The techniques which were developed here are
not only Cohen style forcing notions based on the Baire category idea, however,
and there is a variety of other forcing techniques, for instance forcing notions
based on trees (Spector - Sacks forcing) and on measure, to name just a few. In
his monograph [1983], Lerman introduces a framework for forcing which is tailored
to applications in the degrees and which captures the fundamental forcing notions
used there.
The second criticism of the Kleene-Post legacy, concerning style of presentation,
was eventually accepted. Starting around 1965, a more informal style of exposition
as in Post’s 1944 paper became the norm. We will discuss this in Section 6.
The genesis of [Kleene and Post, 1954] was described this way by Kleene in
[Crossley, 1975]:
This [anyone who does not publish his work should be penalized] is just
what I wrote to Emil Post, on construction of incomparable degrees
and things like that, and he made some remarks and hinted at having
some results and I said (in substance): “Well, when you leave it this

Degrees of Unsolvability
453
way, you say you have these results, you don’t publish them. The fact
that you have them prevents anyone else who has heard of them from
doing anything on it.” So he said (in substance): “You have sort of
pricked my conscience and I shall write something out”, and he wrote
some things out, in a very disorganized form, and he suggested that I
give them to a graduate student to turn into a paper. As I recall, I think
I did try them on a graduate student, and the graduate student did
not succeed in turning them into a paper, and then I got interested in
them myself, and the result was eventually the Post-Kleene paper.[...]
There were things that Post did not know, like that there was no least
upper bound. You see, Post did not know whether it was an upper
semi-lattice or a lattice. I was the one who settled that thing.
The paper itself does not state which author is responsible for which contribu-
tion. Davis, who was Post’s student as an undergraduate, states in [Post, 1994]
that Post announced in his 1948 abstract the result that there are incomparable
degrees below 0′ and discussed this result with Davis in a reading course. Al-
though it is not true that Post announces his result in the abstract, it is clear from
Davis’ recollection that the result is due to Post. A complete understanding of
who proved what in this paper will probably never be obtained.
Post struggled with manic-depressive disease his whole life and according to
Davis (see [Post, 1994]) died of a heart attack in a mental institution shortly after
an electro-shock therapy session. The Kleene-Post paper was his last. For more
details on Post’s life see [Post, 1994]. Kleene’s real interests were in generalized
recursion theory and [Kleene and Post, 1954] is his only paper in the Turing
degrees.
4
SOLUTION TO POST’S PROBLEM: THE PRIORITY METHOD
Post’s Problem was solved independently by Friedberg [1957c] and Muˇcnik [1956]
(see [Muˇcnik, 1958] for an expanded version). Both show that there are incom-
parable c.e. degrees and therefore that incomplete, noncomputable c.e. sets exist.
In his abstract [Friedberg, 1956], Friedberg refers to his solution as making the
Kleene-Post construction of incomparable degrees below 0′ “recursive”. The new
technique introduced by both papers to solve the problem has come to be known
as the priority method. The version used in these papers is speciﬁcally known as
the ﬁnite injury priority method.
In the priority method, one has again requirements or conditions which the
sets being constructed must meet, as in the ﬁnite extension method.
Usually
when the priority method is used, the set to be constructed must be c.e., so it is
constructed as the union of a uniformly computable increasing sequence of ﬁnite
sets, the ith ﬁnite set consisting of those elements enumerated into the set by
the end of stage i of the construction. The requirements are listed in some order
with requirements earlier in the order having higher priority than ones later in

454
Klaus Ambos-Spies and Peter A. Fejer
the order. In a coinﬁnite extension argument, at stage n action is taken to meet
requirement n. This action consists of specifying that certain numbers are in the
set being constructed and others are not in the set. The status of inﬁnitely many
numbers is left unspeciﬁed. Action at all future stages obeys these restrictions.
Because the determination of what action to take at a given stage cannot be made
eﬀectively, the set constructed by this method is not c.e. In the priority method, at
stage n action is taken for whichever is the highest priority requirement Rin that
appears to need attention at the stage. Action consists of adding numbers into
the set (which cannot be undone later) and wanting to keep other numbers out
of the set. If at a later stage a higher priority requirement acts and wants to put
a number into the set which Rin wanted to keep out, then this number is added
and Rin is injured and must begin again. On the other hand, no lower priority
requirement can injure Rin. In a ﬁnite injury priority argument, each requirement
only needs to act ﬁnitely often to be met, once it is no longer injured. (For the
solution to Post’s problem, each requirement needs to act at most twice after it is
no longer injured.) By induction, it follows that each requirement is injured only
ﬁnitely often, is met, and acts only ﬁnitely often.
In the Friedberg-Muˇcnik solution to Post’s Problem, requirements are of the
form A ̸= {e}B and B ̸= {e}A, where A and B are the two c.e. sets being built
whose degrees are to be incomparable and {e} is the eth Turing reduction. Action
for A ̸= {e}B consists of choosing a witness x not restrained by any higher priority
requirement on which it is desired to obtain A(x) ̸= {e}B(x) and then waiting for
a stage s with the current approximation {e}Bs
s (x) equal to 0. Then x is put into
A and numbers less than the use of the computation {e}Bs
s (x) that are not in Bs
are restrained from B. If this restraint is never violated, the requirement is met.
A higher priority requirement of the form B ̸= {i}A may act later and injure the
original requirement, but each requirement acts only ﬁnitely often after it stops
being injured, so all requirements are met.
The priority method is fundamental for the study of the computably enumer-
able degrees and has applications in other areas of computability theory as well.
Friedberg’s paper [1958] contains three further applications of the ﬁnite injury
method. He shows that every noncomputable c.e. set is the union of two disjoint
noncomputable c.e. sets (the Friedberg Splitting Theorem), that maximal sets ex-
ist, and that there is an eﬀective numbering of the c.e. sets such that each c.e.
set occurs exactly once in the numbering. The Friedberg Splitting Theorem is a
particularly simple priority argument as there are no injuries. Priority is just used
to decide which requirement to satisfy at a given stage when there is more than
one requirement that can be satisﬁed. In the maximal set construction, there is a
set of movable markers {Γe}e∈ω. Each marker Γe has associated with it a binary
string of length e called its e-state. The e-state is determined by the position of
Γe. The eth requirement is that the e-state of Γe be lexicographically at least as
great as the e′-state of all markers Γe′ with e′ > e. Once markers Γe′′ with e′′ < e
stop moving, Γe moves at most 2e −1 times. Here is a case where the maximum
number of times a requirement Rn can act after higher priority requirements stop

Degrees of Unsolvability
455
acting depends on n but is still computable.
Finite injury constructions can often be combined with a method called the
permitting method to push constructions below a nonzero c.e. degree.
In the
simplest version of the permitting method, two eﬀective enumerations {As}s∈ω and
{Bs}s∈ω of c.e. sets A and B have the property that for all x, s, x ∈As+1 −As
implies (∃y ≤f(x))(y ∈Bs+1 −Bs), where f(x) is a computable function. It
follows that A ≤T B because if s is a stage such that every number less than
or equal to f(x) that belongs to B is already in Bs, then x ∈A if and only if
x ∈As. In many cases, the function f is the identity function. The ﬁrst argument
that uses permitting is in Dekker [1954], but the principle is not stated in a more
abstract manner until Yates [1965].
The ﬁrst theorem in degree theory that can be proven using permitting is the
result claimed by Muˇcnik [1956] and proven by Friedberg [1957a] (after seeing
Muˇcnik’s claim) that below any nonzero c.e. degree there are two incomparable
c.e. degrees. When this result is proven using permitting to construct the two
incomparable c.e. sets A and B both reducible to a noncomputable c.e. set C,
the requirements are as given above, but before a number x can be put into say
A to meet a requirement, a number y less than or equal to x has to enter C. A
single requirement A ̸= {e}B can now have more than one follower, i.e., number
x on which the requirement tries to make A and {e}B diﬀerent. A follower x
is appointed and if later {e}Bs
s (x) = 0, then the follower is realized. Once the
follower is realized, restraint is put on the lower priority requirements to preserve
the computation and the follower will be put into A if C permits at some later
stage. Meanwhile another follower is appointed and it goes through the same cycle.
This action continues until either a follower is never realized or a realized follower
is permitted to be put into A.
Each requirement acts only ﬁnitely often after
it stops being injured because if not, then there are inﬁnitely many followers, all
realized. Once a follower x is realized at stage s, no number less than x enters C at
a stage greater than s. This makes C computable, contradicting the assumption.
Thus, each requirement acts only ﬁnitely often after it stops being injured. The
requirement is met because either a follower is never realized or a diagonalization
is successfully carried out. Note that here we have no computable bound on how
often a requirement acts; however, it is only negative action that we cannot bound.
Once a requirement stops being injured, it only acts once positively.
While the ﬁnite injury technique had many successes, it has obvious limitations
as well. In general, any construction that involves coding a given noncomputable
c.e. set into a set being built will involve inﬁnite injury. As we will discuss in
the following sections, more powerful techniques were invented to deal with this
type of construction. Nonetheless, important results (for example [Downey and
Lempp, 1997]) were proven using the ﬁnite injury technique, albeit in sophisticated
ways, long after inﬁnite injury techniques were invented. Just as category can be
used to help investigate the limits of what can be proven with the ﬁnite extension
method, Maass [1982], Jockusch [1985] and Nerode and Remmel [1986] introduced
some eﬀective genericity concepts for c.e. sets designed to determine what can be

456
Klaus Ambos-Spies and Peter A. Fejer
proven about a c.e. set with ﬁnite injury constructions.
Given the ubiquity of the priority method in proving results about the c.e.
degrees and the importance of Post’s Problem, it is natural to ask if this prob-
lem can be solved without the priority method. The two solutions mentioned in
the previous section that are in the spirit of Post’s Program also use the priority
method. However, Kuˇcera [1986] has given a priority-free solution. Kuˇcera ob-
tained his solution from the existence of low ﬁxed point free functions (Jockusch
and Soare [1972]) by observing that any such function bounds a simple set. The
sets constructed by the priority method to solve Post’s Problem have as their only
purpose to be a solution. One might then ask if there are any natural solutions to
Post’s Problem. Since naturalness is not a precisely deﬁned notion, this question
is rather vague, but it is fair to say that every particular c.e. set of natural num-
bers that has arisen from nonlogical considerations so far is either computable or
complete. (For some of the strong reducibilities there are “natural” examples of
incomplete c.e. sets: Kolmogorov [1965] has observed that the set of algorithmi-
cally compressible strings is simple, hence not btt-complete. As Kummer [1996]
has shown, however, this set is tt-complete, hence T-complete.) Thus one could
say that the great complexity in the structure of the c.e. degrees arises solely from
studying unnatural problems. However, it is true that every c.e. degree can be ob-
tained by a process studied outside of computability theory, even if the particular
instances of the process that produce noncomputable, incomplete degrees do not
arise in practice. For example, Boone [1965] shows that every c.e. degree contains
the word problem for a ﬁnitely presented group, while Feferman [1957] shows that
every c.e. degree is the degree of a recursively axiomatizable theory.
While, in general, we do not provide biographical details in this chapter, we
feel that some background information on the invention of the priority method by
Friedberg and Muˇcnik would be of interest.
In a phone conversation of July 1999 we asked Richard Friedberg about the gen-
esis of his work in computability theory. Friedberg was a mathematics and physics
major at Harvard. In the summer of 1955 he was looking for a topic for a senior
thesis. He was advised by David Mumford to look into metamathematics and so
read Kleene’s book [1952]. In the book, Kleene asks if there are incomparable
degrees and Friedberg solved this problem on his own. When he found out that
the solution was in the Kleene-Post paper, he was encouraged because the solution
had only been published recently. He next found two degrees neither of which is
computable in the jump of the other. Friedberg wrote to Kleene about this and
Kleene suggested that Friedberg work on Post’s Problem. Friedberg worked on
the problem the whole fall of 1955 without making any progress. He was taking a
seminar with Hao Wang at Harvard and for his term paper decided to write about
his attempts to solve the problem and why they didn’t work. In the course of
writing this paper, he solved the problem. Friedberg’s oﬃcial advisor at Harvard
was Willard Quine, but his real advisor was Hartley Rogers. Friedberg explained
his result to Rogers and then sent it to Kleene. There was a mistake in his write-up
and he received a skeptical reply from Kleene. He ﬁxed the mistake and resent

Degrees of Unsolvability
457
his proof. This time Kleene said it was correct. Friedberg then sent in a notice
to the Bulletin of the AMS (received January 10, 1956). After this, Friedberg was
invited to speak at Princeton University and the Institute for Advanced Studies,
where he met Kurt G¨odel, Georg Kreisel and Freeman Dyson. He gave talks at
other universities and met Hilary Putnam and Alfred Tarski among others.
After graduating from Harvard, Friedberg went to medical school for two and
a half years.
During the summers of 1957 and 1958, he worked at IBM with
Bradford Dunham. In 1957, Dunham took his group to Cornell University for
the AMS meeting in Recursion Theory.
There, Putnam told Friedberg about
the maximal set problem, which he solved once he got back to IBM. Friedberg’s
favorite among his theorems is his theorem on numberings, which he believes is
his hardest.
After leaving medical school, Friedberg went to graduate school in Physics at
Columbia. He received his PhD in 1962 and is a professor emeritus of physics at
Barnard College and Columbia University.
According to Al.
A. Muˇcnik’s son Andrei A. Muˇcnik, who was among the
leading experts in algorithmic randomness, his father was a Ph.D. student at the
Pedagogical Institute in Moscow when he learned about Post’s Problem. In 1954
his thesis advisor, Petr Sergeevich Novikov, presented the problem in a seminar
talk. Novikov expressed his expectation that this question would be resolved within
the next two years.
When Muˇcnik worked on Post’s Problem he was familiar
with the papers by Post and by Kleene and Post.
He solved the problem in
1955 and the solution became the core of his Ph.D. thesis. Muˇcnik’s results were
highly appreciated and he presented his work at some of the major mathematics
conferences in the USSR. After his Ph.D. Muˇcnik became a researcher at the
Institute of Applied Mathematics at the Academy of Sciences in Moscow.
He
continued to work in computability theory and mathematical logic but he did not
obtain any further results on the degrees of unsolvability.
Thus, like Friedberg, Muˇcnik left degree theory shortly after he obtained his
fundamental result though, unlike Friedberg, he stayed in the ﬁeld of logic. While
Friedberg’s work had a deep impact on the further development of computability
theory in the United States and Britain, Muˇcnik’s lasting inﬂuence on the Russian
computability community was much more limited.
5
FROM FINITE TO INFINITE INJURY
A typical requirement that cannot be handled by a ﬁnite injury strategy is a Fried-
berg-Muˇcnik type requirement A ̸= {e}B, where B is subject to inﬁnitary positive
requirements. It was exactly this type of requirement for which the inﬁnite injury
method was ﬁrst used, by Shoenﬁeld [1961]. For any set X and number e, let
A[e] = {⟨x, e⟩: ⟨x, e⟩∈A} and call a subset A of a set B a thick subset of B if
for all e, B[e] −A[e] is ﬁnite. Shoenﬁeld’s theorem was that if B is a c.e. set with
B[e] ﬁnite or equal to ω[e] for every e, then there is a thick c.e. subset A of B that
is not complete. In the proof, the incompleteness of A is shown by constructing

458
Klaus Ambos-Spies and Peter A. Fejer
a c.e. set D with D ̸≤T A. The requirements D ̸= {e}A have to be met in spite
of the inﬁnitary positive requirements on A to be a thick subset of B. Shoenﬁeld
applied his theorem to constructing theories, not to degree theory, but one can
show the existence of an incomplete high c.e. degree using the theorem. (A c.e.
degree a is high if it has the highest possible jump, i.e., a′ = 0′′.)
The next step towards using the inﬁnite injury technique in degree theory was
the Sacks Splitting Theorem shown in [Sacks, 1963b] the proof of which requires a
variant of the ﬁnite injury technique more widely applicable than the original one
used by Friedberg and Muˇcnik. The theorem states that if B is a c.e. set and C
is a noncomputable set Turing reducible to ∅′, then there are disjoint c.e. sets A0
and A1 such that B = A0 ∪A1 and C ̸≤T Ai for i = 0, 1. The key requirements
are of the form C ̸= {e}Ai. These are harder to meet than the requirements in
the Friedberg-Muˇcnik Theorem because C is a given set. Sacks’ insight was to
use a preservation strategy. By putting restraint on Ai to preserve computations
Cs(x) = {e}Ai,s
s
(x), one forces a diﬀerence between {e}Ai and C since otherwise C
would be computable. This construction is ﬁnite injury, but there is no computable
bound on both the negative and positive injuries.
The ﬁrst theorem in degree theory proven using the inﬁnite injury method was
the Sacks Jump Theorem [Sacks, 1963c]. This theorem states that a degree c is
the jump of a c.e. degree if and only if 0′ ≤c and c is c.e. in 0′. Furthermore,
given such a degree c and a degree b with 0 < b ≤0′, one can ﬁnd a c.e. degree
a with a′ = c and b ̸≤a. Previously, Shoenﬁeld [1959] had shown that a degree c
is the jump of a degree ≤0′ if and only if 0′ ≤c and c is c.e. in 0′ and Friedberg
[1957b] had shown a degree c is the jump of another degree if and only if c ≥0′.
Sacks’ proof made use of the preservation strategy.
Inﬁnite injury proofs vary greatly, but they have some common features. The
most basic feature is the existence of inﬁnitary positive and/or negative require-
ments. Inﬁnitary negative requirements put on a restraint that has an inﬁnite
lim sup; however, in order for it to be possible for the positive requirements to be
met, a method is found to ensure that the restraint for each negative requirement
has ﬁnite lim inf.
Even after this is done, there is a synchronization problem.
Two negative requirements, each with ﬁnite lim inf of restraint, can still have a
combined restraint with inﬁnite lim inf. One way of dealing with this problem is
to have followers of positive requirements get past the restraints of negative re-
quirements one at a time. This was Sacks’ approach and it was later formalized
in the so-called pinball machine model introduced in [Lerman, 1973]. Another
approach is the nested strategies method ([Lachlan, 1966b]) where the restraint of
one negative requirement is based on the current restraint of the higher priority
negative requirements. In this way, it is sometimes possible to get all the negative
restraints to fall back simultaneously. Yet another model is the priority tree model
([Lachlan, 1975b]). Here, each requirement has several strategies. Each strategy
makes a guess about the outcomes of the higher priority requirements. Each strat-
egy is assigned a node in a tree. In the simplest case, the strategies for the nth
requirement are put on level n of the tree. There is then a true path through the

Degrees of Unsolvability
459
tree consisting of those strategies whose guess is correct and along this path the
action is ﬁnitary even though the overall action for a requirement is still inﬁni-
tary. At any stage of the construction, there is a guess about the true path, called
the accessible path, and action is limited to accessible strategies. The accessible
paths approximate the true path in the sense that for any given length n, the
initial segment of length n of the true path is the lim inf of the initial segments
of length n of the accessible paths. This means that a 0′′ oracle can determine
both the true path and how each requirement is met. This tree representation
can be used to explain the diﬀerence between ﬁnite and inﬁnite injury. When we
model a ﬁnite injury argument using a priority tree, due to the fact that every
strategy acts only ﬁnitely often, the accessible paths approximate the true path
more eﬀectively, namely, the true path becomes the limit of the accessible paths,
not just the lim inf, so here the true path and the way a requirement is satisﬁed
can be recognized by using 0′ as an oracle. So in modern terminology, the ﬁnite
injury and inﬁnite injury methods are also called the 0′-priority method and the
0′′-priority method.
Shoenﬁeld’s original inﬁnite injury construction does not use a tree, but he
has strategies that make guesses about the outcomes of higher priority positive
requirements, so his proof could be viewed as a forerunner of the tree model for
inﬁnite injury constructions.
The next signiﬁcant result in degree theory after the Jump Theorem that used
inﬁnite injury was the Density Theorem of Sacks [1964]. This theorem states that
the c.e. degrees are dense. Given two c.e. sets C, D with C <T D, it is necessary to
construct a c.e. set A with C <T A <T D. C ≤T A is obtained by directly coding
C into A. D ̸≤T A is obtained by the preservation method which is used to ensure
that if D = {e}A then D would be computable in C. A ≤T D is not obtained by
permitting, but rather because D can compute all of the numbers that have to be
put into A to meet the other requirements. The key new idea in the density proof
is the method to obtain A ̸≤T C. As long as {e}C looks like A, more and more of
D is put into A. Because D ̸≤T C, eventually a diﬀerence between A and {e}C
must appear. Thus diagonalization is obtained by coding. The density theorem
and the techniques used in its proof were very inﬂuential in the study of the c.e.
degrees.
An elegant and somewhat technically simpler proof of the Density Theorem
was given by Yates [1966b] using index set methods. An index set is a set A such
that e ∈A and {e} = {i} imply i ∈A. Index sets arise naturally in the study of
problems involving the behavior of computable partial functions. Classifying the
degrees of naturally arising index sets helps to understand the diﬃculty of problems
involving computable partial functions. For example, the fact that EMP = {e :
We = ∅} (where We is the domain of {e}) has lower Turing degree than TOT =
{e : {e} is total} makes precise the idea that it is harder to determine if a given
Turing machine halts for at least one input than it is to decide if the Turing
machine halts for all inputs. This line of investigation goes back to Post’s 1944
paper, where he wrote:

460
Klaus Ambos-Spies and Peter A. Fejer
Thus, only partly leaving the ﬁeld of decisions problems of recursively
enumerable sets, work of Turing suggests the question is the problem
of determining of an arbitrary basis B whether it generates a ﬁnite,
or inﬁnite, set of positive integers of absolutely higher degree of un-
solvability than K. And if so, what is its relationship to that decision
problem of absolutely higher degree of unsolvability than K yielded by
Turing’s theorem [i.e., to K′].
Post’s question was answered by Dekker and Myhill [1958] who showed that
FIN = {e : We is ﬁnite} is Σ2 complete and hence has degree 0′′. Rogers [1959]
showed many results of this type. In particular, in this paper, Rogers is able to
distinguish the complexity of the index sets REC = {x : Wx is computable} and
COMP = {x : Wx is complete} suﬃciently to show that REC ̸= COMP. This
gives another method to obtain the solution to Post’s Problem. Carrying out this
program further, Yates was able to combine index set techniques with inﬁnite in-
jury constructions to show theorems in degree theory. Yates’ proof of the Density
Theorem begins by showing that for any c.e. set A, {e : We ≡T A} is ΣA
3 -complete.
Then a representation theorem for ΣA
3 sets and an index set theorem are proven
using inﬁnite injury. An application of the ﬁxed point theorem yields the Density
Theorem in a surprising way.
Yates was able to use his index set methods to derive various already known
results in pure degree theory and to strengthen some of them. Since Yates’ work,
index set methods have not been much used for this purpose. However, studies on
the classiﬁcation of various index sets continued (see Chapter X.9 of [Odifreddi,
1999a]).
The Density Theorem and the Jump Theorem were combined by Robinson
[1971b], who showed the following Jump Interpolation Theorem: If C, D are c.e.
sets with C <T D and B is c.e. in D and C′ ≤T B, then there is a c.e. set A
such that C <T A <T D and A′ ≡T B. This paper of Robinson as well as his
[Robinson, 1971a] contains many more general results as well.
As we mentioned in Section 3, the existence of complete maximal sets put an end
to Post’s Program narrowly deﬁned, but if more broadly deﬁned, the Program has
succeeded. If we broaden our view even further, we could say that Post’s intuition
was that structural properties of a c.e. set can be related to the degree of the set. In
this sense, Post’s Program was successful. Since the proofs of some of the relevant
theorems involve inﬁnitary requirements, we summarize these results here. For
n ≥0, a c.e. degree a is called lown if a(n) = 0(n) and highn if a(n) = 0(n+1). The
classes Hn and Ln consist of the highn and lown c.e. degrees, respectively. Sacks
[1963c] derives from his Jump Theorem that this high-low hierarchy is proper, i.e.,
Hn+1 −Hn and Ln+1 −Ln are nonempty for all n ≥0.
Surprising connections have been found between these jump classes and struc-
tural properties of c.e. sets. The ﬁrst of these is due to Martin [1966] who showed
that a degree is in H1 if and only if it contains a maximal set. Another such result
is due to Lachlan [1968a] and Shoenﬁeld [1976] who showed that a degree belongs
to L2 if and only if it contains a coinﬁnite c.e. set with no maximal superset. These

Degrees of Unsolvability
461
results show that the jump classes H1 and L2 are invariant under automorphisms
of the lattice of c.e. sets. Here a class C of c.e. sets is invariant if, for every auto-
morphism f of the lattice of c.e. sets and every A ∈C, f(A) is again in C, and a
class C of c.e. degrees is invariant if C is equal to the set of degrees of an invariant
class C. (Note that L0 and L0 are trivially invariant since the computable sets are
deﬁnable in the lattice of the c.e. sets as the complemented elements.)
These invariance results together with some results on projective determinacy
led in the late 1960s to a conjecture (referred to as “Martin’s Conjecture” by Har-
rington and Soare [1996c]) that among the jump classes Hn and Ln for n > 0
and their complements Hn and Ln, the invariant classes are exactly H2n−1 and
L2n. (In fact, the original version of the conjecture stated that these were the
only invariant degree classes among all nontrivial classes. This stronger conjec-
ture, however, was refuted by Lerman and Soare [1980] who showed that the
deﬁnable class of d-simple sets splits the class of low degrees.) Martin’s invari-
ance conjecture was supported by a number of results of Cholak, Harrington and
Soare. Cholak [1995] and, independently, Harrington and Soare [1996b] showed
that every noncomputable c.e. set is automorphic to some high c.e. set and that
there is an incomplete c.e. degree a such that all c.e. sets in a are automorphic
to some complete set (actually they showed that this is true for any promptly
simple degree a) thereby conﬁrming the conjecture for the downward closed jump
classes. Harrington and Soare [1996a] also announced that L1 is not invariant, but
the envisioned proof using techniques from [Harrington and Soare, 1996b], was
not successful, so the noninvariance of this class remained open until shown by
Epstein [2013]. Cholak and Harrington [2002] have refuted Martin’s conjecture,
however, by showing that Hn and Ln are invariant for all n ≥2. Since Harrington
(see e.g. [Soare, 1987], Chapter XV.1) proved that the class H0 consisting of the
complete degree is invariant because the creative sets are deﬁnable in the lattice of
c.e. sets, it follows that the invariant classes of the low-high hierarchy are exactly
L0, and
L0 ⊃L2 ⊃L3 ⊃. . . ⊃H2 ⊃H1 ⊃H0.
The work of Martin [1966] and others also reveals some interesting relationships
between the complexity of the degree of a (c.e.) set A and the growth rate of the
functions the set A can compute. We say that a function f dominates a partial
function ψ if ψ(x) ≤f(x) for almost all x such that ψ(x) is deﬁned. It is easy
to observe that a c.e. set A computes a function f which dominates all partial
computable functions if and only if A is complete. As ﬁrst observed by Tennen-
baum in 1962, the principal function pM of the complement of a maximal set M
(where pM(n) is the nth element of M in order of magnitude) dominates all total
computable functions, and Martin [1966] shows that a c.e. set A is high if and
only if there is a function f ≤T A which dominates all total computable functions.
Moreover, he shows that a c.e. set A is non-low2 if and only if there is no function
f ≤T K which dominates all total A-computable functions. In more recent work,
not only on the c.e. degrees but also on the degrees in general, domination prop-
erties play a signifcant role (see Downey and Hirschfeldt [2010]). In this context,

462
Klaus Ambos-Spies and Peter A. Fejer
the array noncomputable (a.n.c.) degrees introduced in Downey, Jockusch and
Stob [1990] are of particular interest. Originally deﬁned only for the c.e. case and
designed in order to obtain the c.e. degrees relative to which multiple permitting
constructions can be performed (i.e., constructions in which an attempt to meet a
single requirement requires a computably bounded number of permissions by the
oracle set), Downey, Jockusch and Stob [1996] have extended this deﬁnition to the
degrees in general and have shown that Martin’s characterization of the non-low2
degrees yields the class of a.n.c. degrees if the condition f ≤T A is replaced by the
stronger condition f ≤wtt A. In a similar fashion and preceding this work, Cooper
[1974] observed that Martin’s characterization of the c.e. high degrees yields a still
stronger form of permitting based on the observation that, for an inﬁnite com-
putable ascending sequence, not only inﬁnitely many members but almost all of
the members are permitted by a high c.e. degree. Along the same lines, recently,
an interesting variant of array computability, total ω-computable enumerability
was introduced by Downey and Greenberg (where a set A is totally ω-c.e. if any
function g which is Turing reducible to A is ω-c.e.) and it has been shown that
the c.e. sets without this property may characterize the sets allowing multiple per-
mitting arguments even more adequately than the a.n.c. sets do. (See Downey
and Greenberg [2006] and Downey, Greenberg and Weber [2007] for more details).
Another recent result to be mentioned here is due to Cai and Shore [2012] and
Cai [2012]. They show that the a.n.c. degrees can be deﬁned in terms of relative
computable enumerability: a degree is a.n.c. if and only if any degree above it is
computably enumerable in a lesser degree.
6
PROGRAMMATIC PAPERS AND BOOKS
After the period of rapid advance in technique beginning with Friedberg and Muˇc-
nik’s introduction of the ﬁnite injury priority method and leading to Sacks’ use
of the inﬁnite injury priority method to prove the Density Theorem, came a time
of consolidation of known results through monographs and texts, and attempts
to set the agenda for further study in the ﬁeld through papers asking questions
and making conjectures. There was also a growing recognition at this point of the
importance of considering global questions about the degrees and the c.e. degrees,
even if the answers looked far oﬀ.
In 1963 Sacks published the ﬁrst edition of his monograph Degrees of Unsolv-
ability [Sacks, 1963a]. This was the ﬁrst monograph concerned with degree theory.
Essentially it only contained original results by the author. Some of these results,
however, extended previously obtained results and used the previously introduced
proof techniques, so that the monograph presented a comprehensive development
of results and techniques of degree theory as it existed at the time and it became
the prime source for researchers interested in the ﬁeld. The book describes the
Kleene-Post ﬁnite extension method, Spector’s minimal degree technique, and the
Friedberg-Muˇcnik ﬁnite injury technique, as well as Sacks’ extensions of the pri-
ority method, i.e., the unbounded ﬁnite injury and inﬁnite injury techniques. The

Degrees of Unsolvability
463
presentation combined intuitive discussion with formal, Kleene-style proofs.
The Kleene-Post and Friedberg-Muˇcnik techniques are pushed to their limits.
By the Kleene-Post technique some very general extensions of embeddings results
are proven which imply that there are uncountable independent anti-chains of
degrees (a set of degrees is independent if no element of the set is computable in a
ﬁnite join of other elements of the set), that no countable anti-chain is maximal,
and that every partial order the size of the continuum which has the countable
predecessor property (i.e., each element has at most countably many predecessors)
and such that every element has at most ℵ1 many successors is embeddable into
the degrees (In particular, assuming the continuum hypothesis, the last result
shows that the degrees are a universal partial order with size the continuum and
the countable predecessor property, that is, every such partial order is embeddable
into D and D itself is such a partial order.)
New results in the monograph based on reﬁnements of Spector’s minimal degree
construction include the following. 1) There is a minimal degree below 0′. (First
proven in [Sacks, 1961].) 2) Every countable set of degrees has a minimal upper
bound. 3) The diamond lattice (i.e., the four element Boolean algebra) is isomor-
phic to an initial segment of the degrees. The ﬁrst result is of particular technical
interest since it exploits the ﬁnite injury priority method in the construction of
a non-c.e. degree. The third result is, following Titgemeyer’s result that there is
a nonzero degree with a unique nonzero predecessor [Titgemeyer, 1962], another
step towards exploring the possible (ﬁnite) initial segments of D. Furthermore the
Baire category and Lebesgue measure approaches to degree theory are taken up.
In particular, it is shown that the class of minimal degrees has measure 0.
By the Friedberg-Muˇcnik technique, it is shown that every countable partial
order is embeddable into R. The core of the monograph is the results obtained by
Sacks’ extension of the priority method. The Sacks Splitting and Jump Theorems,
mentioned above, are presented, as well as further results on the jump operator.
The book ends with a list of conjectures and open questions on the structure
of the c.e. degrees and the degree structure in general. These conjectures and
questions, most of them related to the new results obtained in the monograph,
were very inﬂuential in the following development of degree theory. (An extended
discussion of their impact can be found in [Shore, 1997].)
There are three conjectures on the c.e. degrees: 1) The c.e. degrees are dense. 2)
There is a minimal pair (that is, a pair of incomparable degrees whose meet is 0). 3)
The c.e. degrees are not a lattice. In the monograph, Sacks says that he believes the
conjectures because “behind each of them stand several false but plausible proofs.”
In fact each of these conjectures was shown to be true shortly after the monograph
was written.
As mentioned above, Sacks [1964] showed the ﬁrst conjecture to
be true. (In fact there already is a footnote in the monograph added in proof
announcing this result.)
This is considered by many to be the most beautiful
application of the inﬁnite injury technique.
The second and third conjectures
were both proven by Lachlan [1966b] and Yates [1966a], independently, by using
a new type of inﬁnite-injury argument, the so-called minimal pair technique. We

464
Klaus Ambos-Spies and Peter A. Fejer
will discuss these results in Section 8 in more detail.
The other three conjectures deal with the general degree structure: 4) A par-
tially ordered set is embeddable into the degrees if and only if it has cardinality at
most that of the continuum and each member has only countably many predeces-
sors. 5) If S is an independent set of degrees of cardinality less than that of the
continuum, then there exists a degree d ̸∈S such that S ∪{d} is independent. 6)
S is a ﬁnite initial segment of the degrees if and only if S is order-isomorphic to a
ﬁnite initial segment of some upper semi-lattice with a least element. These con-
jectures are interesting extensions of some results in the monograph. For instance,
as mentioned above, Sacks showed that conjectures 4 and 5 are true assuming the
continuum hypothesis. Without set-theoretic assumptions, however, conjecture
4 has remained open until today, while conjecture 5 has turned out to be inde-
pendent of ZFC, Zermelo-Fraenkel set theory with the axiom of choice: Groszek
and Slaman [1983] show the relative consistency of the assumption that 2ℵ0 > ℵ1
and there are maximal independent sets of degrees of size ℵ1. Conjecture 6 was
shown by Lerman [1971]. We will discuss this and further initial segment results
in Section 7.
Finally, there are ﬁve questions raised, all of which were answered within the
next decade. The ﬁrst three were related to minimal degrees, while the last two
dealt with the c.e. degrees. Some of the solutions led to interesting proof tech-
niques: in his proof that every nonzero c.e. degree bounds a minimal degree, Yates
[1970] introduced the full approximation method which became a powerful stan-
dard tool for the analysis of the non c.e. degrees below 0′ and Shoenﬁeld [1966]
introduced the tree method for constructing minimal degrees in order to show that
for every nonzero degree below 0′ there is a minimal degree incomparable with the
given degree and also below 0′.
Sacks’ conjectures and questions were proof-technique driven and had as one
of their goals to ﬁnd interesting extensions of the techniques available then. As
Sacks put it himself: “We regard an unsolved problem as interesting only if it
seems likely that its solution requires a new idea.” Furthermore, the conjectures
and questions have in common that they address local properties of the degree
orderings D and R.
A revised edition [Sacks, 1966] of Sacks’ monograph appeared in 1966. It re-
ﬂected an interest in global questions about degrees missing from the ﬁrst edition.
This change can probably be traced to the eﬀect of inﬂuential talks by Shoenﬁeld
and Rogers.
Shoenﬁeld [1965], based on a talk given at a Model Theory Symposium in Berke-
ley in 1963, proposes the application of model theory to the degree structures and
raises the question of the complexity of the elementary theory of D and R. Though
Shoenﬁeld guesses that D has an undecidable theory and is diﬃcult to character-
ize, he thinks that this does not apply to R. He proposes a very speciﬁc conjecture
on this structure which became famous as Shoenﬁeld’s conjecture though it was
refuted soon after it was published. Roughly speaking this conjecture says that
R is a countably inﬁnite homogeneous upper semi-lattice with least and greatest

Degrees of Unsolvability
465
elements, just as the rational numbers are a countably inﬁnite homogeneous linear
ordering without endpoints. If this conjecture were true, it would characterize R
up to isomorphism and the elementary theory of R would be decidable. Shoenﬁeld
listed three consequences of the conjecture: 1) Density. 2) Every c.e. degree a has
the cupping property, i.e., for every nonzero c.e. degree b less than a, there is a
c.e. degree c < a such that a is the join of b and c. 3) If a and b are incomparable
c.e. degrees, then they have no greatest lower bound in R. Shoenﬁeld was led to
his conjecture by Sacks’ density conjecture. The Shoenﬁeld conjecture also implies
Sacks’ third conjecture on the c.e. degrees, but not Sacks’ second conjecture, which
is incompatible with the third consequence of homogeneity.
Consequence 2 of Shoenﬁeld’s conjecture was refuted by Lachlan [1966a] and
consequence 3 independently by Lachlan and Yates by showing the existence of
minimal pairs.
Rogers [1967a] in his talk at the Tenth Logic Colloquium in Leicester, England,
raised some more fundamental global questions on the structure of the degrees of
unsolvability (as well as on other recursion-theoretic structures, namely, the lattice
of c.e. sets, the upper semi-lattice of partial degrees, and the Medvedev lattice).
He drew particular attention to deﬁnability and invariance under automorphisms.
Speciﬁcally, he asked if there is a nontrivial automorphism of D and whether the
jump operation and the relation “computably enumerable in” (as a relation on
degrees) are invariant under all automorphisms of this partial ordering. Led by
the observation that the standard proofs in computability theory relativize, he
asked whether for any degree a, D(≥a), the upper cone of degrees greater than or
equal to a, is isomorphic to D. He reﬁned this question by asking whether these
structures are isomorphic if the jump operation and the relation “computably
enumerable in” are added. These problems became known as the homogeneity
problem and the strong homogeneity problem and they were the ones that were
solved ﬁrst.
Feiner [1970] gave a negative solution to the strong homogeneity
problem and Shore [1979] gave a negative solution to the homogeneity problem.
We will come back to these homogeneity questions in Section 7.
The existence of nontrivial automorphisms and the deﬁnability of the jump and
the relation “computably enumerable in” turned out to be more complex than the
homogeneity questions. We will discuss more recent developments in Section 11.
As Rogers points out in the introduction of his article, he came across these
questions during his work on a book [Rogers, 1967b] on recursive function theory.
The impact of this book can hardly be overestimated. For 20 years, it was the
standard source for computability theory and it introduced many of the later
researchers to the ﬁeld and did much to popularize it.
Rogers’ book gives an easy informal introduction, avoiding the heavy notation
present in much of the earlier work, starting with the basic concepts and leading up
to some of the most current research results, including a very readable approach
to priority arguments. Though the book covers computability theory widely, it
concentrates on the degree structures including the strong reducibilities.
The shift from local problems to global questions originating from Shoenﬁeld

466
Klaus Ambos-Spies and Peter A. Fejer
and Rogers’ papers is reﬂected in the second edition of Sacks’ monograph [Sacks,
1966]. There the conjectures and questions stated in the ﬁrst edition and solved by
that time, all of them local and seemingly approachable by current technology, were
replaced by global, much more speculative conjectures and questions. Reﬂecting
the belief of the time in the simplicity of R, Sacks conjectured that the ﬁrst-order
theory of R is decidable. On the other hand, following his conjecture on the ﬁnite
initial segments of D taken over from his ﬁrst edition, he writes: “It appears to
follow from (C6) and some work of J. R. Shoenﬁeld that the elementary theory of
the ordering of degrees is unsolvable.” Sacks also conjectures a positive solution
to Rogers’ homogeneity problem and in the same vein, that for any degree a,
the degrees c.e. in and above a are isomorphic to R. All three conjectures have
been refuted: Harrington and Shelah [1982] announced the undecidability of R; as
mentioned above, the homogeneity conjecture fails; and Shore [1982a] also showed
that the third conjecture was false.
One of the questions related to the homogeneity problem asks whether the de-
grees and the arithmetic degrees are elementarily equivalent.
Using results on
minimal covers, Jockusch and Soare [1970] and Jockusch [1973] gave a negative
answer to this question. A further question is related to the theory of the c.e. de-
grees and asks whether this theory is the same as the theory of the metarecursively
enumerable degrees, a question which reﬂects Sacks’ growing interest in general-
ized recursion theory. A negative answer to this question was given only fairly
recently by Shore and Slaman (see [Greenberg et al., 2006]). Two more questions
go back to the original Post’s problem. One revives Post’s program in a slightly
broader sense by asking “Is there some simple property of complements of com-
putably enumerable sets (in the style of Post) which implies non-completeness?”
We have discussed this issue in Section 3. The ﬁnal new question asks for a degree
invariant solution to Post’s problem:“Does there exist a G¨odel number e such that
for all sets A, W A
e (the e-th set computably enumerable in A) is of higher degree
than A and lower degree than A′, and such that if A and B have the same degree,
then W A
e and W B
e have the same degree?” There has been some deep work related
to it, but the question is still open. Lachlan [1975a] has shown that there is no
uniform invariant solution in the following sense: There is no G¨odel number e as
above such that in addition, indices for the reductions between W A
e and W B
e can
be found uniformly from ones for reductions between A and B. Sacks’ question
about a degree invariant solution to Post’s problem inspired Martin (in [Kechris
and Moschovakis, 1978], page 281) to make a conjecture that states roughly that
under set theory with dependent choice and the axiom of determinacy, the only
nontrivial deﬁnable Turing invariant functions are the jump and its iterates (in-
cluding transﬁnte iterates). Much deep work has been done on this conjecture.
(See [Marks et al., 2011] for a recent summary of these results.)
The expository papers and books just described allow us to draw the follow-
ing picture of the common beliefs of the leading workers on the subject in the
mid-sixties: while the feeling emerged that the global degree structure might be
quite diﬃcult and there might be no feasible way to describe it, there was still a

Degrees of Unsolvability
467
belief that the structure of the c.e. degrees is much more well-behaved and may
allow an easy characterization. This latter hope was dashed by the work of the
next decade which exposed many pathologies in this structure, though the ﬁnal
breakthrough in answering some of the global questions, in particular showing the
undecidability, had to wait until the beginning of the eighties. The proposed proof
of the undecidability of the theory of R by Harrington and Shelah was based on
structural results requiring a much more involved variant of the priority method
which was ﬁrst introduced by Lachlan and due to its complexity became known
as the “monstrous injury method.”
Since the developments in the global degrees and the c.e. degrees were not so
closely linked in the next 25 years, we will describe these developments separately.
In the next section we will describe the main results obtained in the global degree
structure. Then the following three sections will be devoted to the c.e. degrees,
accounting for the local structural results, giving a short explanation of the mon-
strous injury technique, then ﬁnishing with the global results.
7
GLOBAL QUESTIONS ABOUT THE DEGREE STRUCTURE D
Most of the work on the global degrees in the second half of the sixties and the
ﬁrst half of the seventies was directed to the problem of initial segments. The
countable case was completely solved giving another universal property and initial
segments results became the basis for the solution for some of the global questions
raised by Shoenﬁeld, Rogers, and Sacks.
Lerman’s result, mentioned previously, that every ﬁnite lattice is isomorphic
to an initial segment of D (Lerman [1971]) extended the Spector minimal degree
technique by using appropriate lattice representations, as Sacks had expected.
There were some important intermediate steps leading to Lerman’s result.
First, Rosenstein [1968] and Shoenﬁeld (unpublished) extended Sacks’ result by
showing that every ﬁnite Boolean algebra can be embedded as an initial segment.
Next, Hugill [1969], extending Titgemeyer [1962] (see also [Titgemeyer, 1965]),
showed that every countable linear order with least element is embeddable as an
initial segment of D. Using Hugill’s representation of lattices, Lachlan [1968b]
subsumed these two results by showing that every countable distributive lattice
with least element occurs as an initial segment of D. The nondistributive case
required some more involved lattice representations, ﬁrst applied in Thomason
[1970].
Lachlan and Lebeuf [1976] completely characterize the countable case by show-
ing that every initial segment of a countable upper semi-lattice with least element
is isomorphic to an initial segment of D. The uncountable case turns out to be
independent of ZFC. The strongest absolute result is due to Abraham and Shore
[1986]: every initial segment of an upper semi-lattice of size ℵ1 with the countable
predecessor property occurs as an initial segment of D. However, Groszek and
Slaman [1983] show that it is relatively consistent with ZFC that the continuum

468
Klaus Ambos-Spies and Peter A. Fejer
hypothesis fails and there is an upper semi-lattice with the countable predecessor
property of size ℵ2 that cannot be embedded as an initial segment of D.
Shoenﬁeld’s question whether the theory of the degrees is undecidable was the
ﬁrst to be solved using initial segments results. Since the theory of the countable
distributive lattices is undecidable ([Grzegorczyk, 1951]), Lachlan’s initial segment
theorem implies undecidability of the theory of D. This result has been improved in
two directions by considering fragments of the theory and computing the degree of
the theory. By showing the strong undecidability of the ∃∀theory of ﬁnite lattices
in the language of partial orderings, and applying Lerman’s theorem, Schmerl (see
Lerman [1983]) proved the undecidability of the three quantiﬁer theory of D (in the
language of partial orderings). This result is optimal since by Lerman’s theorem
together with an extension of embeddings argument, the two quantiﬁer theory is
decidable. This was shown independently and roughly simultaneously by Shore
[1978] and Lerman (see [Lerman, 1983]).
The borderline between decidable and undecidable fragments was drawn even
more precisely by adding symbols for join and meet to the language.
While
Jockusch and Slaman [1993] have shown that the two quantiﬁer theory remains
decidable if the join is added, Miller, Nies, and Shore [Miller et al., 2004] have
proved that the two quantiﬁer fragment becomes undecidable if not only the join,
but also the meet (or to be more precise, any total extension of the meet oper-
ation) is added. The proof of the latter result is interesting because, in contrast
to the previous proofs of undecidability, where some undecidable theory is inter-
preted in the degree ordering, here a direct coding of register machines is used.
Another interesting result along the same lines is due to Shore and Slaman [2006]
who have shown that the two quantiﬁer theory with join and jump is undecidable.
In the opposite direction, recently, Lerman [2010] has shown that the existential
theory with 0 and jump is decidable (see also Section 9 below). The reader can
ﬁnd a more complete account of decidability questions for fragments in [Shore and
Slaman, 2006].
The complexity of the theory of D was determined by Simpson [1977] who has
shown that the theory is as complex as possible, namely, computably isomorphic
to second-order arithmetic. The main ingredients for obtaining an interpretation
of second-order arithmetic in the degree structure are Spector’s exact pair theo-
rem and some new initial segment results where, in addition, joins are controlled.
The proof is ﬁrst done in the language with jump.
Then it is argued that by
Spector’s theorem, mentioning of the jump can be omitted. The natural numbers
are represented by Ω= {0(n) : n ∈ω}. Deﬁnability of Ωis obtained as follows.
By Simpson’s new initial segment results, there is an ideal I = {an : n ∈ω} of
order type ω such that an ∪0(2) = 0(n+2) for all n, hence Ωcan be described in
terms of any exact pair of any such ideal I using the jump. The graphs of addition
and multiplication are deﬁned along the same lines. Finally, in order to deﬁne
arbitrary subsets of Ω, Simpson shows that, for any set A ⊆ω, there is a degree a
such that n ∈A iﬀ0(n+1) ≤a ∪0(n).
The interpretability of second-order arithmetic into the degrees has some other

Degrees of Unsolvability
469
important applications to be mentioned below. Nerode and Shore [1980b] and
[1980a] introduce a new coding method giving an alternative proof of Simpson’s
result on the degree of the theory of D using only very few structural results like
Lachlan’s initial segment theorem and Spector’s exact pair theorem, so that the
proof could be carried over to many other degree structures based on reducibilities
such as many-one reducibility, truth table reducibility, weak truth table reducibil-
ity, and arithmetic reducibility. One of the key steps in the coding theorems of
Simpson and Nerode and Shore is to deﬁne with parameters a certain countable
relation (representing a standard model of arithmetic) on the degrees. Slaman
and Woodin [1986] prove a general deﬁnability lemma subsuming these results:
Every countable relation on D is deﬁnable with parameters in D.
In fact, for
a given arity n of the relations, this is done uniformly, i.e., there is a formula
ϕ(x1, . . . , xn, y1, . . . , ym) such that for any n-ary countable relation R on D, there
are parameters a1, . . . , am such that R is deﬁned by ϕ(x1, . . . , xn, a1, . . . , am) in
D.
After the decidability problem, the next question to be settled was the strong
homogeneity conjecture. Feiner [1970] and Jockusch (unpublished) independently
refuted the conjecture. It is interesting to note that although the homogeneity con-
jecture was motivated by the observation that all results in computability theory
known at the time relativize, relativizing proofs played a crucial role in the refu-
tation of the conjecture. The outline of Feiner’s proof, which is based on Hugill’s
initial segment result, is as follows: It easily follows from Hugill’s proof that any
computable linear ordering can be embedded as an initial segment of the degrees
below 0′′. By relativization, every a-computable linear ordering is isomorphic to
an initial segment of the interval [a, a′′]. Furthermore, by a straightforward anal-
ysis, the interval [0, 0′′] is a 0(5)-computable partial ordering, hence any initial
segment of the interval has at most this complexity. So to complete the proof it
suﬃces to show that there is a 0(6)-computable linear ordering L that is not iso-
morphic to any 0(5)-computable linear ordering. Then no initial segment of [0, 0′′]
is isomorphic to L, but by the above observation on the relativized Hugill result,
L is embeddable as an initial segment in [0(6), 0(8)]. Hence, these intervals are
not isomorphic. This shows that there is no jump preserving isomorphism from
D to D(≥0(6)). In fact, if (D(≥a), ≤,′ ) and (D(≥b), ≤,′ ) are isomorphic, then
a ≤b(6) and b ≤a(6), so in particular a and b have the same arithmetic degree.
So we may say that the key to Feiner’s refutation of the strong homogeneity con-
jecture is based on the observation that the complexity of a degree a (modulo the
sixth jump) is reﬂected by the structure of the interval from a to a′′.
Reﬁnements of Feiner’s result and the refutation of the homogeneity conjecture
without jump were based on stronger observations on the relation between a de-
gree and the structure of the degrees above it. Interpretations of second order
arithmetic in the degrees were used to distinguish cones by ﬁrst-order properties.
Simpson [1977] did this in the presence of the jump operator by showing that there
are degrees a and b such that (D(≥a), ≤,′ ) and (D(≥b), ≤,′ ) are not elementar-
ily equivalent. By applying their coding method, Nerode and Shore improved this

470
Klaus Ambos-Spies and Peter A. Fejer
by showing that (D(≥a), ≤,′ ) ≡(D, ≤,′ ) implies that a(4) ≤0(5).
The general homogeneity conjecture without jump was eventually refuted by
Shore [1979] by showing that for no Π1
1 hard degree a is the cone above a isomorphic
to D.
Ingredients of his proof are results on cones of minimal covers and the
analysis of cones ﬁxed under any automorphism. In [Shore, 1982b], Shore extended
this to elementary equivalence by using the coding machinery of Nerode and Shore.
In fact, he has shown that for elementarily equivalent upper cones, the bases must
have the same triple jump.
Cones of minimal covers had been used before by Jockusch and Soare [1970]
and Jockusch [1973] to give a negative answer to Sacks’ question, related to the
homogeneity problem, of whether the degrees D and the arithmetic degrees A are
elementarily equivalent. By observing that none of the jumps 0(n) is a minimal
cover, Jockusch and Soare showed that A does not contain a cone of minimal covers
so that it suﬃces to show that there is such a cone in D. They also point to a possi-
ble way for obtaining such a cone. Martin [1968] related determinacy to cones: For
any determinate class of degrees C (i.e., C consists of the degrees of a determinate
degree invariant class), C or the complement of C contains a cone. So since, as
observed in [Jockusch and Soare, 1970], minimal covers can be easily described by
a Σ5-game, this level of determinacy would give the desired result. Then Jockusch
observed that Σ4-determinacy proven by Paris [1972] suﬃces thereby completing
the proof. Of course Martin’s later proof of Borel determinacy in [Martin, 1975]
made the Jockusch result superﬂuous. Harrington and Kechris [1975] showed that
Kleene’s O, i.e., the Π1
1 complete degree, is the base of a cone of minimal covers.
This was further improved by Jockusch and Shore [1984] who showed that in fact
0(ω) is the base of a cone of minimal covers. We will come back to this result in
Section 11 where we will discuss deﬁnability questions and questions related to
automorphisms, questions which were central in the more recent work on D.
The above investigations of the global structure were matched by corresponding
work on the degrees below 0′. We conclude this section with a short summary of
these results. (For a more detailed account see [Cooper, 1999a].) Since arguments
had to be eﬀectivized, most of the proofs became more subtle. We already men-
tioned that Sacks constructed a minimal degree below 0′ and that Yates [1970]
improved this result by showing that every nonzero c.e. degree bounds a minimal
degree. The proofs of these results are prototypes of the two approaches to con-
structing degrees below 0′, now referred to as oracle constructions and full approx-
imation constructions, respectively. Sacks reﬁned Spector’s tree-forcing argument
for constructing a minimal degree in order to make the construction eﬀective rel-
ative to 0′. Yates replaced the use of the oracle by eﬀectively approximating the
objects to be built (here the set of minimal degree and the trees it lies on). The
full approximation method can be viewed as an adaption of the priority method
(designed for constructing c.e. sets) to the construction of ∆2 sets. While in a
priority argument a computable enumeration {As}s≥0 of a (c.e.) set A is given
(i.e., once a number x is put into A at some stage s it cannot be extracted from
A at a later stage), here just a computable approximation {As}s≥0 of a (∆2) set

Degrees of Unsolvability
471
A is given. So, now, a number x put into A at some stage s may be extracted
from A at a later stage and so on, as long as this happens only ﬁnitely often. By
Shoenﬁeld’s Limit Lemma this guarantees that A is ∆2 hence of degree ≤0′.
The full approximation method - extended by Cooper in [Cooper, 1972] and
[Cooper, 1973] where he studied minimal degrees below 0′ - became instrumental
for obtaining initial segment results below 0′. Lerman in his monograph [Lerman,
1983] shows that every ﬁnite lattice is embeddable as an initial segment in the
degrees below 0′. In fact, as he points out, the proof can be extended to show that
every 0′′ presentable upper semi-lattice can be embedded in this way. This shows
that the theory of D(≤0′) is undecidable. The undecidability was independently
obtained by Epstein [1979] who constructed initial segments of D(≤0′) of order
type ω + 1 meeting suﬃcient additional conditions to adapt some of the coding of
[Simpson, 1977] to this setting. The full analog of Simpson’s result, namely that
the theory of D(≤0′) is as complex as possible, was obtained by Shore [1981] who,
using Lerman’s initial segment results, showed that the degree of Th(D(≤0′)) is
0ω. A complete characterization of the topped initial segments of D(≤0′) that
are lattices was given by Kjos-Hanssen in his thesis [Kjos-Hanssen, 2002] (see also
[Kjos-Hanssen, 2003]): A lattice with least and greatest elements is isomorphic to
an initial segment of D(≤0′) if and only if it is Σ3 presentable.
Results on decidable and undecidable fragments of Th(D(≤0′)) largely parallel
those for Th(D). By [Kleene and Post, 1954], the ∃theory of the structure is
decidable, and Lerman and Shore [1988] extended this to the ∃∀theory by using
initial segment and extension of embedding results. On the other hand Lerman
[1983], using his initial segment results, localized Schmerl’s argument to get unde-
cidability of the ∃∀∃theory. Miller, Nies and Shore [Miller et al., 2004] show that
the ∃∀theory in the language with join and meet added is undecidable but the
decidability of the two quantiﬁer theory with just the join added is unknown.
Besides the investigation of initial segments, some of the earlier work on the
degrees below 0′ addressed the question of complements. One of the highlights
of these developments was Posner and Robinson’s result that this structure is
complemented ([Posner and Robinson, 1981] and [Posner, 1981]). Another line of
research was directed to the Ershov hierarchy, the Boolean closure of the c.e. sets.
In particular the degrees of the d.c.e. sets, i.e., the sets that can be expressed as
the diﬀerence of two c.e. sets, have been extensively studied. In contrast to Sacks’
Density Theorem, Cooper, Harrington, Lachlan, Lempp and Soare [Cooper et al.,
1991] have shown that the partial ordering of the d.c.e. degrees is not dense.
8
BASIC ALGEBRAIC PROPERTIES OF R
Following Sacks’ Splitting and Density Theorems, which emphasized the homo-
geneity of the c.e. degrees, a more detailed analysis of the algebraic structure was
initiated, leading to the view that the structure is more complex than expected.
One of the questions addressed ﬁrst was that of meets. It is not clear whether from
two c.e. problems we can extract a greatest common information content and in

472
Klaus Ambos-Spies and Peter A. Fejer
fact Shoenﬁeld conjectured that this is not the case for Turing incomparable prob-
lems. As Lachlan and Yates have shown, however, there are both pairs with and
without inﬁma. In fact there are minimal pairs of c.e. degrees i.e., noncomputable
c.e. sets A, B such that any set which can be reduced to both is computable. A
new variant of the inﬁnite injury method is used to construct such sets. A typical
requirement is: Given a set W and Turing reductions {e0}, {e1} from W to the
sets A and B to be built, one has to ensure that the set W is computable. Now,
if at a stage s of the construction, A seems to determine the value of W at some
number x, i.e., {e0}As
s (x) = i for some i ≤1, then we can guarantee that W(x) = i
by the further construction of A not allowing this computation to be changed, that
is by not allowing A to change below the use of this computation. By doing this
all the time obviously W will be computable, but so will A. By having a reduction
to both A and B, we can reﬁne this idea for making W computable and at the
same time allow small numbers to be put into A and B at late stages in order to
make A and B noncomputable. The idea is to wait for a stage s at which both
computations give the same value for W(x), i.e., {e0}As
s (x) = {e1}Bs
s (x) = i. Now
we can hold the value of W by restraining A or B and allowing the other set to
change on small numbers. Moreover, by holding one side, the other side is forced
to come back to computing the right value. Now we may interchange the roles of
A and B and release the set which previously held the computation. In this way,
both A and B can change on arbitrarily small numbers at arbitrarily large stages
and still the ﬁrst apparent value for W(x) is preserved. So there is no conﬂict
between computing W(x) and making A and B be noncomputable. Though this
basic idea is quite simple, coordination of the restraints for the diﬀerent require-
ments in the actual construction is quite delicate. As mentioned in the section on
the inﬁnite injury method already, several solutions have been proposed to solve
this problem: nested strategies ([Lachlan, 1966b]) forcing the restraints to drop
back simultaneously; pinball machines ([Lerman, 1973]) where positive action for a
noncomputability requirement has to be allowed stage-by-stage by the higher pri-
ority negative requirements; and priority trees ([Lachlan, 1975b]). Each of these
methods proved to be of fundamental importance in the further development of
inﬁnite injury arguments.
Using the minimal pair technique, Yates [1966a] constructed a strictly ascending
sequence of c.e. degrees with an exact pair of c.e. degrees for the downward clo-
sure of the sequence, thereby showing that there are pairs of c.e. degrees without
inﬁmum and thus the upper semi-lattice R is not a lattice. This was indepen-
dently shown by Lachlan [1966b] by relativizing his nondiamond theorem which
says that there is no incomparable pair of c.e. degrees with join 0′ and meet 0.
The proof of the nondiamond theorem is of independent technical interest. Given
two incomparable c.e. sets A and B such that A ⊕B is complete, a c.e. set C
computable in A and B is constructed and an attempt is made to ensure that it
is noncomputable. This attempt might fail, but there will be a back-up strategy
taking advantage of the failure and producing another c.e. set witnessing that A
and B are not a minimal pair. Such nonuniform strategies have been later ex-

Degrees of Unsolvability
473
ploited in many other constructions. Yates’ and Lachlan’s original proofs of the
existence of pairs of c.e. degrees without inﬁma are fairly indirect. A surprisingly
simple direct construction was given by Jockusch [1981].
Lachlan and Yates produced more results in their minimal pair papers, showing
that the situation with meets is very complex compared to the situation with
joins.
Lachlan showed that in contrast to Sacks’ Splitting Theorem, which in
algebraic terms says that every nonzero c.e. degree is join-reducible, the dual
fails: by the minimal pair theorem, 0 is meet-reducible (branching), there are
non-zero meet-reducible degrees, but there are also incomplete non-meet-reducible
(nonbranching) degrees.
In addition, there is a minimal pair of high degrees,
which shows that the intuition that the degrees forming a minimal pair have low
information content is incorrect. Yates showed that not every incomplete degree
is half of a minimal pair. In modern terminology, not every degree is cappable.
Subsequent work on these matters exhibited some algebraically nice features,
but also further stressed the complexity of the c.e. degrees. Fejer [1983] showed
the density of the nonbranching degrees and Slaman [1991b] showed the density of
the branching degrees, so that both these classes are homogeneously distributed
throughout the c.e. degrees. Ambos-Spies, Jockusch, Shore and Soare [Ambos-
Spies et al., 1984] show that the classes of the cappable and noncappable degrees
give an algebraic decomposition of the c.e. degrees into an ideal and a (strong) ﬁlter
and show that the noncappable degrees coincide with the low-cuppable degrees,
the degrees of the promptly simple sets and the degrees of the non-hyperhyper-
simple sets with a certain splitting property deﬁned by Maass, Shore and Stob
[Maass et al., 1981]. This related a natural deﬁnable subclass with the jump, an
important dynamical property of c.e. sets, namely prompt simplicity introduced
by Maass [1982], and the lattice of the c.e. sets under inclusion.
Some more
information is given on the meet operator by the fact that every degree is half
of a pair without inﬁmum, but there are degrees that are not half of a pair with
inﬁmum (Ambos-Spies [1984] and Harrington (unpublished)).
The minimal pair technique has been further developed in lattice embedding
results. Thomason [1971] and Lerman (unpublished) show that every ﬁnite dis-
tributive lattice is embeddable in R. Lachlan [1972] and Lerman (unpublished)
extended this to the countable case. While these results only required a straight-
forward extension of the minimal pair technique, the embeddability problem for
nondistributive lattices turned out to be much more diﬃcult and has not been
completely resolved yet. Lachlan [1972] embeds the two ﬁve element nondistribu-
tive lattices, the nonmodular lattice N5 and the modular lattice M5. (These are
of particular interest since every nondistributive lattice contains a copy of one of
these lattices.) The embeddings required reductions where the use is not bounded
computably. Systems of traces were introduced to describe these reductions. While
for the embedding of the lattice N5, a very simple trace model suﬃces, the trace
system required for the embedding of M5 is much more elaborate. Both Robinson
[1971a] and Shoenﬁeld [1975] conjectured that all ﬁnite lattices were embeddable
in R, but attempts to extend Lachlan’s techniques to a general embedding tech-

474
Klaus Ambos-Spies and Peter A. Fejer
nique failed. Lerman pointed to the lattice S8, consisting of the lattice M5 with
a diamond on top, as an example of a lattice which could not be embedded by
current techniques and he conjectured that it is not embeddable. This conjecture
was veriﬁed by Lachlan and Soare [1980]. Ambos-Spies and Lerman [1986] and
[1989] gave quite general nonembeddability (NEC) and embeddability (EC) condi-
tions and left open the question whether these conditions were complementary. In
[Lempp and Lerman, 1997a] a new obstacle to embeddability was found and a 20
element nonembeddable lattice was given that does not satisfy NEC. Subsequently
Lerman [1998] and [2000] isolated a necessary and suﬃcient Π2 -criterion for the
embeddability of a large class of ﬁnite lattices, the so-called join-semidistributive
lattices. For a survey of the status of the embedding problem see Lempp, Lerman
and Solomon [Lempp et al., 2006].
All the known lattice embeddings can be done preserving the least element.
There is a large body of work directed to lattice embeddings with some other ad-
ditional constraints like preserving the greatest element or the least and greatest
elements as well as local embeddings into initial segments or intervals of the c.e.
degrees.
We want to mention here only two of these results.
For embeddings
preserving 0 and 1, by Lachlan’s nondiamond theorem, the embeddability prob-
lem is nontrivial even for distributive lattices. Ambos-Spies, Lempp and Lerman
[Ambos-Spies et al., 1994] have shown that a ﬁnite distributive lattice can be em-
bedded this way if and only if the lattice contains a join irreducible, noncappable
element. Note that in particular the double diamond lattice has this property.
Downey [1990] showed that there are ﬁnite embeddable lattices which cannot be
embedded into every initial segment; in particular this is true for the lattice M5.
(Some recent improvements of this result can be found in Downey and Greenberg
[2006] and Downey, Greenberg and Weber [2007].)
As we have seen above, often a question on meets in the c.e. degrees is much
more complex than the dual question about joins, but there are also quite complex
questions about the join.
An example is the dual to the partition of R into
the cappable and noncappable degrees. A c.e. degree a is cuppable if there is a
c.e. degree b < 0′ with a ∪b = 0′. Yates (unpublished) claimed in the mid-
sixties that there is a nonzero noncupping degree, showing that this partition is
nontrivial. Sacks writes in [Sacks, 1966] that the proof “is almost too diﬃcult for
even the greatest lover of degrees to endure.” In the mid-seventies, Harrington
proved a series of results on cuppability. For example, he gave a generalization
of Yates’ theorem where 0′ is replaced by any high degree and he showed that
every c.e. degree is cuppable or cappable (Cup or Cap Theorem), there is a c.e.
degree which is cuppable and cappable (Cup and Cap Theorem), and there is a
nonzero incomplete c.e. degree a such that every nonzero degree below it cups to
every c.e. degree above it (Plus Cupping Theorem). Only handwritten notes were
circulated. Some of the results were later published by Miller [1981] and Fejer
and Soare [1981]. By Harrington’s results, the noncappable degrees are strictly
contained in the cuppable degrees and by a straightforward variant of the Cup and
Cap Theorem one can show [Ambos-Spies, 1980] that the cuppable c.e. degrees do

Degrees of Unsolvability
475
not form a ﬁlter, while trivially the noncuppable degrees form an ideal.
Much research has been devoted to other cupping and capping questions. In
particular, cupping to degrees other than 0′ and capping to degrees other than
0 has been investigated. These results further emphasized the complexity of the
structure of the c.e. degrees. Some of them played a crucial role in the further
development since they required a new much more powerful proof technique and
became the basis for solving global questions.
9
THE 0′′′ PRIORITY METHOD
It had been commonly believed that Sacks’ splitting and density theorems could be
combined. In [Lachlan, 1975b], however, Lachlan shows that this is not the case,
i.e., there are c.e. degrees a < b such that b cannot be split over a. The proof of
this surprising result introduced a new, combinatorially extremely complex method
which turned out to be a very powerful tool and became the basis for proving a
number of important results. For several years, the method was barely understood
and the community usually referred to the paper as the “monster paper” and the
technique used as the “monstrous injury” method. It is interesting to note that
Lachlan himself described the genesis of the proof as follows: “From very crude
beginnings the ﬁnal format of the construction was achieved only after a series
of modiﬁcations each designed to eliminate a ﬂaw found in the previous attempt.
This process of evolution yielded only a cloudy intuition as to why the construction
should work.”
Lachlan’s method can be described as a ﬁnite injury argument on top of an
inﬁnite injury argument.
In this proof, priority trees were ﬁrst introduced to
handle the conﬂicts between the diﬀerent requirements. As pointed out in our
discussion of the inﬁnite injury method, there is a true path describing the correct
outcomes of the strategies which is eﬀectively approximated at the stages of the
construction. In an inﬁnite injury argument, the true path is the lim inf of these
approximations and a 0′′ oracle suﬃces to describe how the strategies for meeting
the requirements succeed. In Lachlan’s proof, on any path, there may be inﬁnitely
many strategies working on the same requirement, so that knowing the true path
does not yet reveal how a requirement is met. An additional quantiﬁer is needed,
so that only 0′′′ can see how a requirement is met. Using modern terminology,
Lachlan’s proof is an instance of the 0′′′ priority method.
Lachlan [1979] gives another example of the 0′′′-method to prove that there
is a c.e. degree that does not bound any minimal pair.
Here the interactions
between the strategies are much less complex and this paper helped increase the
understanding of some of the basic features of 0′′′-arguments. The full power of
the method is only present in the original paper.
Work by Harrington presented in handwritten notes led gradually to a better
understanding of the technique. In [Harrington, 1978], he proves the Plus Cup-
ping Theorem by isolating the so-called “gap-cogap” method. Harrington’s 1980
notes entitled “Understanding Lachlan’s Monster Paper” [Harrington, 1980] were

476
Klaus Ambos-Spies and Peter A. Fejer
eagerly studied by the community. In these notes, Harrington improved Lachlan’s
Nonsplitting Theorem by showing that 0′ does not split over some c.e. degree.
Notes entitled “A Gentle Approach to Priority Arguments” [Harrington, 1982]
were prepared for a talk at the AMS Summer Research Institute in Recursion
Theory held at Cornell University in 1982.
Later, higher level priority arguments extending through all ﬁnite (and even
transﬁnite) levels of the arithmetical hierarchy were used, and some frameworks
for describing and classifying priority arguments were developed.
In the early
1980s Harrington (in unpublished notes and some talks) proposed a ﬁrst general
framework for priority arguments based on his “worker at level n” model. Here
a worker on level n works with oracle 0n (so the lowest-level worker, who in
the end has to do the job, works eﬀectively without oracle), and instructions
passed down by a higher level worker have to be approximated by the worker
on the next lower level via the limit lemma. This approach is ﬁrst applied and
further extended in computable model theory by Ash, Knight and others. Ash
[1986b] and [1986a] describes higher level priority arguments in terms of iterated
trees, and he introduces a general framework by providing some metatheorems.
An extension of this framework and further applications can be found in Ash
and Knight [2000].
The most advanced framework(s) for higher level priority
arguments were developed by Lempp and Lerman in a series of papers ([1990],
[1995], [1997b]) culminating in Lerman’s monograph [2010] where, by a priority
argument using all ﬁnite levels, it is shown that the existential theory of the
partial order (D, ≤, 0,′ ) of the Turing degrees with least element and jump is
decidable.
An interesting related result of comparable complexity had already
been given in Lempp and Lerman [Lempp and Lerman, 1996], where it is shown
that the existential theory of the partial ordering of c.e. degrees remains decidable
if predicates for nth-jump comparability are added for all numbers n.
At the end of the seventies, there was still a widespread belief, however, that the
use of Lachlan’s proof technique might be limited to showing some speciﬁc results
demonstrating pathological properties of R. A general interest in the method only
arose when the undecidability of the theory of R was shown based on Lachlan’s
technique.
10
GLOBAL QUESTIONS ABOUT THE STRUCTURE R
Given the pathologies mentioned above, it seems inevitable that the early belief
that the theory of the c.e. degrees is decidable was eventually shattered. In [Har-
rington and Shelah, 1982], Harrington and Shelah announced the undecidability
of R. They achieved this by reducing the theory of partial orderings to the c.e.
degrees. For this sake, a formula ϕ with four parameters is given such that any
∆2-partial ordering can be deﬁned in R by this formula ϕ with appropriate choice
of parameters. The formula ϕ talks about some maximal noncupping elements
in a local setting reminiscent of nonsplittability. The proof of the main technical
lemma is based on Lachlan’s nonsplitting technique.

Degrees of Unsolvability
477
Harrington and Slaman (unpublished) proved along the same lines that the
degree of the theory of R is as high as possible, namely, the degree of ﬁrst-order
arithmetic. Neither of the two previous proofs ever became accessible probably
due to the fact that Slaman and Woodin came up with a simpler coding also based
on cupping properties but requiring only a quite tame 0′′′-argument. Though the
original proof of Slaman and Woodin also did not appear, their coding was used
later in Nies, Shore and Slaman [Nies et al., 1998] to obtain even stronger results
(which will be discussed below).
The ﬁrst published proof of the undecidability of R was given in Ambos-Spies
and Shore [1993]. Similar to the previous proofs, the theory of ﬁnite partial or-
derings is reduced but the coding scheme is quite diﬀerent as it is based on meets
rather than joins. Surprisingly, the proof involves only a fairly standard inﬁnite
injury argument combining the basic branching and nonbranching degree tech-
niques. The proof can be combined with the permitting technique to show that
the theory of every nontrivial initial segment of R is also undecidable. Building on
the density of the nonbranching and branching degrees proofs of Fejer and Slaman,
Ambos-Spies, Hirschfeldt and Shore [Ambos-Spies et al., 2000] extended the latter
proof and result to show the undecidability of the theory of any nontrivial interval
of R. Like the proof of density of branching degrees, this result requires the 0′′′
method. Because the other previously mentioned techniques for proving undecid-
ability and determining the degree of the theory do not seem to be applicable to
intervals, the question whether the degree of the theory of every interval of R is
as high as possible is still open.
The analysis of undecidable fragments of the theory of R started with Ambos-
Spies and Shore [1993] where it is observed that the undecidability proof given
there shows the undecidability of the ﬁve quantiﬁer theory and that the Harrington-
Slaman undecidability proof actually gives a stronger result, namely, the undecid-
ability of the four quantiﬁer theory. An alternative proof of the undecidability
of the four quantiﬁer theory is given in [Lempp and Nies, 1995] where the corre-
sponding result for the c.e. weak truth-table degrees is shown. In fact, the proof
for the weak truth-table degrees is transferred to the Turing degrees using con-
tiguous degrees, i.e., c.e. Turing degrees containing only one c.e. weak truth-table
degree. Such transfers of results from the weak truth-table to the Turing degrees
go back to Ladner and Sasso [1975] where this method was used to prove some
noncuppability results. Since the c.e. weak truth-table degrees are a distributive
upper semi-lattice, contiguity yields some local distributivity in the nondistribu-
tive structure of the c.e. Turing degrees. This frequently used observation is ex-
ploited by Lempp and Nies too. It is noteworthy that in this undecidability proof
a distributive coding scheme is used whereas the previous undecidability proofs
depended heavily on the nondistributivity of the c.e. Turing degrees. Lempp, Nies
and Slaman [Lempp et al., 1998] brought the undecidability down to the three
quantiﬁer level. As in Lempp-Nies, an interpretation of the theory of ﬁnite bipar-
tite graphs without equality is given, but now by a more delicate method a Σ1
deﬁnition is obtained. The strongest result on undecidable fragments of the theory

478
Klaus Ambos-Spies and Peter A. Fejer
of R is due to Miller, Nies, and Shore [Miller et al., 2004] who have shown that
the two quantiﬁer theory becomes undecidable if we add symbols for the join and
any total extension of the meet to the language (thus giving another proof of the
undecidability of the three quantiﬁer theory). Just like the corresponding result
for D this result is shown by a direct coding of register machines.
The status of the two quantiﬁer theory (in the language of partial orders) is still
open, though it is widely conjectured that this fragment of the theory is decidable
(see e.g. [Shore, 1999]). The one-quantiﬁer theory is decidable by Sacks’ result
that any ﬁnite partial ordering can be embedded into R (and, as already men-
tioned in Section 9 above, by a very sophisticated argument, Lempp and Lerman
[Lempp and Lerman, 1996] show that decidability is preserved if n-jump relations
are added for all n). A decision procedure for the two-quantiﬁer theory will re-
quire the solution to the still open embedding problem, that is, the question of
which ﬁnite lattices can be embedded.
In fact, this question has to be solved
for embeddings preserving both least and greatest elements. Another typical in-
gredient of a decision procedure of a two-quantiﬁer theory is the solution of the
extension of embeddings problem. This problem was solved by Slaman and Soare
[2001] for R by using techniques related to splitting, density, minimal pairs, and
Lachlan’s nonsplitting. For a more thorough discussion of the possible design of a
decision procedure for ∀∃−Th(R) and possible obstacles, see Lerman [1996] and
Shore [1999]. The work on undecidability was paralleled by the investigation of
the number of n-types realized in R. In fact, Lerman, Shore and Soare’s [Lerman
et al., 1984] result that there are inﬁnitely many three types was the ﬁrst global
result pointing to the complexity of the theory since, by the Ryll-Nardzewski The-
orem, it implies that the theory is not countably categorical. This result has been
obtained by extending the embeddability of the lattice N5 to a class of ﬁnite (par-
tial) lattices with the so-called trace-probe property and showing that there are
inﬁnitely many distinct lattices of this type generated by three elements. (Shore
[1982a] later also used these techniques in his refutation of Sacks’ isomorphism
conjecture on c.e.a. degrees.) The question of the number of one-types was settled
by Ambos-Spies and Soare [1989] who showed that for any number n, there is a c.e.
degree a such that the n-atom Boolean algebra, but not the n + 1-atom Boolean
algebra, can be embedded into the degrees below a with least and greatest element
preserved, thereby showing that there are inﬁnitely many pairwise disjoint deﬁn-
able classes of degrees. The proof is based on Lachlan’s nonbounding technique
and uses contiguous degrees for creating local distributivity. Later, the above-
mentioned undecidability proof of Ambos-Spies and Shore gave an alternative and
simpler proof of the existence of inﬁnitely many one-types, in fact, it showed that
there are continuum many one-types consistent with the theory of R. By localizing
the embedding result of Lerman, Shore, and Soare, in 1990 Ambos-Spies, Lempp,
and Soare (unpublished) have shown that the theory of every nontrivial interval
realizes inﬁnitely many three types, hence is not countably categorical. This result
was superseded by Ambos-Spies, Hirschfeldt and Shore [Ambos-Spies et al., 2000]
giving continuum many one-types consistent with the theory of each interval of R.

Degrees of Unsolvability
479
11
DEFINABILITY AND AUTOMORPHISMS
The major themes of the work of the last thirty years, especially for the global
degrees, were deﬁnability questions and the question of the existence of nontrivial
automorphisms.
These questions are related by the observation that deﬁnable
properties are invariant under automorphisms. Hence, deﬁnability results might
impose restrictions on the possible behavior of automorphisms, while existence
results for automorphisms might lead to some undeﬁnability results. For instance,
the existence of minimal degrees rules out the existence of automorphisms that
move every nonzero degree up to a higher degree.
Though some interesting classes of degrees have been deﬁned by giving some
simple algebraic characterizations, many of the most interesting deﬁnability results
in D are based on some codings of second-order arithmetic which allow the transfer
of a deﬁnition in arithmetic to a deﬁnition in the degree ordering. Examples of
“natural” deﬁnitions are the deﬁnitions of the arithmetical and hyperarithmetical
degrees given in Jockusch and Shore [1984]. (Previously, Jockusch and Simpson
[1976] gave natural deﬁnitions for these classes using the jump.) Since the pattern
of the proof of the deﬁnability of the arithmetical degrees will reoccur as an ingre-
dient in the deﬁnition of the jump we sketch it here before we turn to deﬁnability
results obtained by coding.
Recall from Section 7 that Jockusch and Soare had given an elementary diﬀer-
ence between the theories of D and A based on minimal covers. Improving their
results Jockusch and Shore show that the class of the arithmetical degrees is the
downward closure C≤of the deﬁnable class
C = {a : ∀x(x ∨a is not a minimal cover of x)}.
The inclusion of A in C≤follows from observations in [Jockusch and Soare, 1970].
The reverse inclusion is proven by introducing pseudo jump operators. A 1-CEA
operator J : 2ω →2ω is an operator of the form J(A) = A ⊕W A where W is a
c.e. set, and the 1-CEA operator induced by the eth c.e. set We is denoted by Je.
(Note that CEA stands for “computably enumerable in and above”.) An n-CEA
operator is the composition of n 1-CEA operators, J⟨e1,...,en⟩= Jen ◦. . . ◦Je1,
and an ω-CEA operator is an operator of the form Jf(A) = L{Jf↾n(A) : n ∈ω}
for some computable function f. Obviously, the jump, the n-iterated jump, and
the ω-jump are canonical examples of 1-CEA, n-CEA, and ω-CEA operators,
respectively. Further n-CEA and ω-CEA operators are induced by the sets in the
Ershov diﬀerence hierarchy or, to be more precise, by the corresponding operators
as follows. Call an operator J an n-c.e. operator if J(A)(x) = lim{e}A(x, s) where
{e}A is total for all A and, for all A and x, {e}A(x, 0) = 0 and {e}A(x, s) ̸=
{e}A(x, s + 1) for at most n many s. If the last condition is relaxed to the extent
that there is a computable function f such that, for all A and x, the number of
s such that {e}A(x, s) ̸= {e}A(x, s + 1) is bounded by f(x) then call J an ω-c.e.
operator. Then, for any n ≤ω and any n-c.e. operator J, ˆJ(A) = A ⊕J(A) is an
n-CEA operator.

480
Klaus Ambos-Spies and Peter A. Fejer
Jockusch and Shore show that the Friedberg jump completeness theorem can
be extended to pseudo jump operators, namely for any α-CEA operator J and any
set C ≥T ∅α there is a set A such that J(A) =T C. For the above deﬁnition of the
arithmetical degrees, a join theorem, i.e., an extended version of this completeness
theorem including joins is proven for the case of ω-c.e. operators: For such an
operator J and any nonarithmetic set C there is a set A such that J(A) ⊕A =T
C ⊕∅ω =T C ⊕A.
Then the proof is completed by pointing out that Sacks’
construction of a minimal degree below 0′ actually yields an ω-c.e. set and hence
induces an ω-c.e. operator.
Simpson [1977] was the ﬁrst to translate deﬁnitions from arithmetic to degrees.
Based on his coding theorem, Simpson showed that every relation on the degrees
above 0ω which is deﬁnable in second-order arithmetic is ﬁrst-order deﬁnable in
the partial ordering of degrees with jump. (This in particular yields the above
deﬁnability results of Jockusch-Simpson.) Note that this result is an optimal de-
ﬁnability result for the degrees above 0ω in the structure of the degrees with jump
since obviously any deﬁnable properties in the degrees (with jump) are deﬁnable
in second-order arithmetic. Later improvements to Simpson’s deﬁnability theorem
were obtained in two ways: the degree 0ω was lowered, and the jump was removed.
Jockusch and Shore [1984] removed the jump from Simpson’s result and Slaman
and Woodin [ta] in addition replaced 0ω with 0′′. (Here and in the following we
give the reference “Slaman and Woodin [ta]”, since it can be found this way in the
literature quite frequently, though it is not clear whether it will ever appear, as
some of the work to be presented there has been included or superseded in some
published papers (see below). In 2005, Slaman and Woodin [Slaman and Woodin,
July 21 2005] circulated a draft version which contains most of the material. Pub-
lished papers which come closest to Slaman and Woodin [ta] are Slaman’s research
announcement [1991a] and lecture notes [2008].)
The optimum result along these lines would be that every relation on the degrees
which is deﬁnable in second-order arithmetic is also deﬁnable in D. Slaman and
Woodin [1986] show that every countable n-ary relation on D is uniformly deﬁnable
(i.e. by a single formula depending only on n) from a sequence of degrees whose
length also depends only on n. Thus there is a uniformly deﬁnable collection of
subsets Np of D and relations +p and ×p on Np (with p ranging over the sequences
of degrees of ﬁxed length satisfying some ﬁxed formula) such that, for each such p,
(Np, +p, ×p) is isomorphic to (N, +, ×). Moreover, by the same general fact, there
are isomorphisms between any two such Np and Nq which are uniformly deﬁnable
from p and q and a formula ψ(c, m, p) which codes subsets Ac of Np by c in the
sense that m ∈Ac if and only if m ∈Np and ψ(c, m, p) (again c is a sequence
of degrees of ﬁxed length). Their Biinterpretability Conjecture then states that
there is a deﬁnable relation R(x, c, p) on D which holds if and only if c is a code
for a subset of Np whose image under the isomorphism with N is of degree x. This
conjecture easily implies that every relation on D is deﬁnable in D if and only if it
is deﬁnable in second order arithmetic. The point is that it basically says we can
translate a given degree invariant property deﬁnable in second order arithmetic to

Degrees of Unsolvability
481
a deﬁnable relation on D using the relation R and the models Np as R picks out
exactly the (codes c of) sets of degree x in the model Np. The above mentioned
partial results were achieved by weakening the requirements on R. (Of course,
Simpson and Jockusch-Shore also used diﬀerent coding methods than Slaman and
Woodin to get their versions of Np and ψ(c, m, p).) In Simpson’s result R was
deﬁned only using the jump and works correctly only for degrees x ≥T 0ω.
In
Jockusch-Shore and Slaman-Woodin, R is deﬁnable in D and works correctly for
x ≥T 0ω and 0′′, respectively.
The Slaman-Woodin results in particular yield the deﬁnability of the double
jump operation. Extending the pseudo jump machinery (already exploited in the
deﬁnition of the arithmetical degrees; see above) signiﬁcantly, Shore and Slaman
[1999] deﬁne the jump in terms of the double jump by showing that 0′ is the
greatest degree x such that there is no degree y such that x ∨y = y′′ thereby
solving this long standing open problem of Kleene and Post [1954] (see Section 3
above). Since the technical work of Slaman and Woodin which is not only based on
the common computability theoretic methods (like coding and forcing) but also
on set theoretical techniques (like collapsing cardinals and absoluteness) is still
unpublished to a large extent, a complete proof of this fundamental result was
not available for a long time. Fairly recently, however, Shore [2007] gave a new
simpliﬁed proof of the deﬁnability of the double jump (together with extensions
of some of the other consequences of Slaman and Woodin’s work) which not only
avoids set theoretical methods but also replaces coding of arithmetic by some
simpler coding schemes.
Moreover Shore shows that the formula deﬁning the
single jump relation y = x′ derived from this new deﬁnition of the double jump by
the Shore-Slaman formula above is Π8 in the language (≤, ∨, ∧). Shore’s deﬁnition
of the double jump still being based on some codings, it remains open, however,
whether there is a “natural” deﬁnition of the jump.
Almost a decade before Shore and Slaman’s deﬁnition of the jump, Cooper
[1990] announced that the jump operator is deﬁnable, and that this deﬁnition can
be extended to a deﬁnition of the relation “computably enumerable in”. The def-
inition of the jump proposed by Cooper is a natural one and the proposed proof
follows the pattern of the proof of the deﬁnability of the arithmetical degrees out-
lined above. In order to deﬁne 0′, which by relativization would give the deﬁnition
of the jump, Cooper considers some splitting properties and he introduces the no-
tion of a degree d being relatively splittable over a predecessor a. He claims that 0′
is the greatest degree x with the property that, for all a, x∨a is relatively splittable
over a. By relativizing the Sacks Splitting Theorem, 0′ has this property. The
proposed proof of maximality has two parts. First, the main theorem asserts that
there is a 2-c.e. degree d which is relatively unsplittable over all a < d. Second,
an appropriate version of a join theorem for 2-c.e. operators is proven and applied
to the 2-c.e. operator induced by d. A proof of the main theorem is not given in
[Cooper, 1990], but Cooper claims it uses a 0′′′ priority argument in the style of
the proof of Lachlan’s nonsplitting theorem. Shore and Slaman [1999], however,
have refuted Cooper’s main theorem and have shown the proposed deﬁnition of

482
Klaus Ambos-Spies and Peter A. Fejer
0′ is not correct. In [Cooper, 2001], Cooper attempts an amended proof of the
deﬁnability of the jump using a variant of his proposed deﬁnition of 1990. It is
not known if Cooper’s proposed revised deﬁnition does in fact deﬁne the jump
operator, but Cooper’s proof that it does has been discredited. (See for instance
Jockusch’s review [Jockusch, 2002] of [Cooper, 2001] and the discussion in [Shore,
2006].) So it is an interesting open problem whether a natural deﬁnition of the
jump can be obtained along the lines proposed by Cooper. Also, the deﬁnability
of the relation “computably enumerable in” remains open.
Another major consequence of the Biinterpretability Conjecture is the rigidity
of D, i.e., there are no nontrivial automorphisms. This is an easy consequence
of the rigidity of (N, +, ·). Slaman and Woodin have in fact shown that biinter-
pretability is equivalent to rigidity. The partial solutions of the Biinterpretability
Conjecture obtained so far give stronger and stronger restrictions on the ﬂexibil-
ity of automorphisms: Every automorphism respecting the jump operator ﬁxes
the cone above 0ω (Simpson [1977]); every automorphism ﬁxes the cone above
0ω (Shore [1982b]); every automorphism ﬁxes the cone above 0′′ (Slaman-Woodin
[ta]). In fact, for automorphisms respecting the jump, the partial rigidity result
of Simpson had been previously obtained by Solovay (unpublished) and was later
reﬁned by Jockusch and Solovay [1977] and Richter [1979] by replacing 0ω with
0(4) and 0(3), respectively.
These proofs were not based on interpretations of
second-order arithmetic, but used some simpler codings.
Further severe limitations on automorphisms were obtained by Slaman and
Woodin [ta] by proving the Biinterpretability Conjecture with parameters. This
implies that any relation on the degrees deﬁnable in second-order arithmetic with
parameters can be deﬁned in D with parameters.
Moreover, there is a ﬁnite
set of degrees such that every automorphism of D is determined by its behavior
on this set, so there is a ﬁnite automorphism base. (Automorphism bases were
introduced by Lerman [1977] (see [Lerman, 1983], Chapter IV.5) in the setting of
D. Further examples of automorphism bases were given by Jockusch and Posner
[1981].) In fact, the Slaman-Woodin proof of the Biinterpretability Conjecture
with parameters is based on the construction of a generic degree which is an
automorphism base. The existence of a ﬁnite automorphism base together with
the fact that there is an upper cone ﬁxed by every automorphism implies that
there are only countably many automorphisms.
Though the above results lead many researchers to believe that the Biinterpret-
ability Conjecture is true and hence D is rigid, in [Cooper, 1997a], Cooper an-
nounced the existence of a nontrivial automorphism of D, which would disprove
the Biinterpretability Conjecture.
The proposed proof, which can be found in
[Cooper, 1997b] is constructive and though it only uses the ﬁnite injury method,
it is extremely complicated. In [Cooper, 1999b], Cooper claims the stronger re-
sult that there is an automorphism moving a degree above 0′, but the proofs in
these papers have been discredited (see for example Lempp’s review of the latter
[Lempp, 2002]) and the questions are still unresolved.
We turn now to deﬁnability in the c.e. degrees. There are very few natural

Degrees of Unsolvability
483
deﬁnability results. Besides the characterization of the promptly simple degrees
and the low cuppable degrees by the (deﬁnable) noncappable degrees mentioned
above, the deﬁnition of the contiguous degrees is the most interesting example.
Downey and Lempp [1997] show that the local distributivity property of the con-
tiguous degrees referred to before actually deﬁnes these degrees. Ambos-Spies and
Fejer [2001] extend this result by showing that a c.e. degree is contiguous if and
only if it is not the top of a copy of the nonmodular lattice N5. Thus, in R, local
nondistributivity is equivalent to local nonmodularity.
Strong deﬁnability results based on coding ﬁrst-order arithmetic in R were ob-
tained by Nies, Shore and Slaman [Nies et al., 1998], who actually proved a weak
version of the Biinterpretability Conjecture for R.
They prove that there is a
uniformly deﬁnable collection of subsets Np of R and relations +p and ×p on Np
(with p ranging over the sequences of degrees of some ﬁxed length satisfying a
ﬁxed formula) such that, for each such p, (Np, +p, ×p) is isomorphic to (N, +, ×).
Moreover, there are also isomorphisms between any two such Np and Nq which
are uniformly deﬁnable from p and q and a formula ψ(c, m, p) which codes sub-
sets Ac of Np by c in the sense that m ∈Ac ⇔m ∈Np & ψ(c, m, p).
The
Biinterpretability Conjecture for R now states that there is a deﬁnable relation
R(x, c, p) on R which holds if and only if c is a code for a subset of Np whose
image under the isomorphism with N is of degree x. In this case, the conjecture
is equivalent to the existence of a deﬁnable relation Q(x, c, p) on R which holds
if and only if c ∈Np and, if e is the image of c under the isomorphism between
Np and N, then We ∈x. Now Nies, Shore and Slaman prove a weaker version of
this conjecture in which R holds if and only if c is a code of a set with the same
double jump as x (Q holds if and only if deg(W ′′
e ) = x′′). They deduce from this
result that every relation on the c.e. sets which is invariant under the double jump
and deﬁnable in ﬁrst-order arithmetic is deﬁnable in R. In particular, this implies
that the jump classes Ln, n ≥2 and Hn, n ≥2 are deﬁnable. Moreover, by some
natural algebraic properties relating the high degrees to double jump equivalence,
the class of high degrees H1 is also deﬁnable.
The question of the deﬁnability of the low degrees is not settled. In fact, Cooper
has announced that there is an automorphism of R which moves a low degree to a
nonlow degree. This would show that the low degrees are not invariant, hence not
deﬁnable. Since, as pointed out above, Cooper’s automorphism proofs have been
discredited, this question too is still open.
Turning ﬁnally to the degrees below 0′, Nies, Shore and Slaman [Nies et al.,
1998] have argued that their proof of the Biinterpretability Conjecture for R mod-
ulo the double jump can be adapted to D(≤0′). They conclude that the jump
classes Ln, n ≥2 and Hn, n ≥1 are deﬁnable in D(≤0′) too. This claim extends
Shore’s work in [Shore, 1988] where he argued that Ln and Hn are deﬁnable in this
structure for n ≥3. Very recently, however, Shore [2014] observed that both of
the above arguments are missing a crucial ingredient and he ﬁlled the gap thereby
completing the proof of these deﬁnability results. Just as in the cases of the de-
grees in general and of the c.e. degrees, the question of the deﬁnability of the class

484
Klaus Ambos-Spies and Peter A. Fejer
L1 of the low degrees in D(≤0′) remains one of the major open problems about
this degree structure.
12
CONCLUSION
The continuous interest and developments in the ﬁeld of degree theory have been
documented by a series of monographs and conferences.
Following the ground
breaking monographs of Sacks and Rogers discussed above in detail, the intro-
duction to degree theory by Shoenﬁeld [1971] and the monographs by Lerman
[1983] and Soare [1987] on the global degree structure and the c.e. degrees, respec-
tively, became the inseparable companions of any degree theorist. The two volume
monograph of Odifreddi [1989] and [1999a] embedded the work on the degrees of
unsolvability in the general development of computability theory and became the
major source for strong reducibilities. The state of the ﬁeld as of 15 years ago is
presented in the Handbook of Computability Theory [Griﬀor, 1999].
The AMS Summer Institute of Symbolic Logic held in 1957 at Cornell University
([Cornell Summer Institute, 1957]), a one month gathering of the most prominent
American logicians, was the ﬁrst platform for the emerging theory of degrees.
Here Friedberg presented his solution to Post’s Problem for the ﬁrst time at a
conference.
Twenty ﬁve years later, another AMS Summer Research Institute
was held at the same place, now solely devoted to recursion theory. It gathered
more than one hundred of the leading experts from all over the world. Here the
emerging understanding of the 0′′′-priority technique was expounded by Soare
and Harrington and short courses on degree theory were presented by Shore and
Soare ([Nerode and Shore, 1985]). The AMS-IMS-SIAM Joint Summer Research
Conference on Computability Theory and Its Applications held in the summer of
1999 in Boulder focused on open problems. The proceedings of this conference
[Cholak et al., 2000] gave directions for future research in the main areas of the
ﬁeld.
The era covered in this chapter spans from the beginnings of degree theory in the
1940s to the end of the 20th century, though in some places we have added pointers
to the more recent work directly related to the topics dealt with there. We feel
that it is too early to put the work of the past 15 years into historical perspective,
so we give here only a few quite general impressions of some of the more recent
developments. Research in degree theory in the past years is widely characterized
by interactions with other areas of computability theory and logic, like Computable
Model Theory, the Theory of Π0
1-Classes (see the forthcoming monograph of Cenzer
and Remmel [ta]), and Reverse Mathematics (see the monograph by Simpson
[2009]). Probably the most intensive interactions have been with the theory of
Algorithmic Randomness.
The monographs by Downey and Hirschfeldt [2010]
and Nies [2009] provide excellent introductions in this area, and the list of open
problems by Miller and Nies [2006] clearly illustrates the strong interactions with
degree theory. Another characteristic of the more recent and current research in
degree theory is the focus on the investigation of some special degree classes - some

Degrees of Unsolvability
485
of them classical (like the jump classes, the n-c.e. degrees, the generic degrees (with
respect to various genericity notions), the array noncomputable degrees, the ﬁxed-
point-free degrees etc.), but many of them newly evolved from either the above
mentioned interactions (like the random degrees and low-for-random degrees (with
respect to various randomness notions), the K-trivial degrees, etc.) or from work
in degree theory itself (like the totally ω-c.e. degrees mentioned before). There is
a large body of work exploring the relations among these classes, which helped to
gain a better understanding of the relations among some of the basic concepts of
computability. “The Computability Menagerie” by Joseph Miller (to be found on
his home page) gives a good impression of the variety of classes considered and
the work done in this context in the past years.
Seventy years after Post founded the subject, the degrees of unsolvability is
still one of the central areas of research in computability theory. The structure
turned out to be much more complex than originally expected and probably no one
anticipated the interesting techniques that grew out of attempts to understand the
degree structure. Many of the problems raised in the ﬁfties and sixties, such as the
deﬁnability of the jump and decidability questions, which seemed to be completely
out of reach at that time, have been settled. Work on deﬁnability has greatly
increased our understanding of the relation between the information content of
problems and the degree structure and the solution to the automorphism problem
might be emerging. These questions will keep the ﬁeld vital for the foreseeable
future.
We ﬁnish our history by quoting Gerald Sacks, who through his own work and
the work of his students (including, Harrington, Robinson, Shore, Simpson, Sla-
man, and Thomason) shaped degree theory throughout a good part of its existence:
“Farewell to higher recursion theory, but not to recursion theory; there is no way
to say good-bye to recursion theory” ([Sacks, 1999]).
ACKNOWLEDGEMENTS
We thank the following people for their responses to our questions and their com-
ments on preliminary forms of this article: Barry Cooper, John Crossley, Mar-
tin Davis, Yuri Ershov, Richard Friedberg, Paulo Garrido, Aki Kanamori, Bjørn
Kjos-Hanssen, Steﬀen Lempp, Manuel Lerman, Andrei A. Muˇcnik, Anil Nerode,
Gerald Sacks, James Schmerl, Victor Selivanov, Stephen Simpson, Theodore Sla-
man, Robert Soare, Frank Stephan, and Vladimir Uspensky. We especially thank
Richard Shore for numerous extended discussions, Steven Homer for his hospital-
ity at Boston University where we wrote major parts of the article, and Dirk van
Dalen, J¨org Siekmann, and John Woods for inviting us to write this history of
degrees of unsolvability.

486
Klaus Ambos-Spies and Peter A. Fejer
BIBLIOGRAPHY
[Abraham and Shore, 1986] Uri Abraham and Richard A. Shore. Initial segments of the degrees
of size ℵ1. Israel J. Math., 53(1):1–51, 1986.
[Ambos-Spies and Fejer, 2001] Klaus Ambos-Spies and Peter A. Fejer. Embeddings of N5 and
the contiguous degrees. Ann. Pure Appl. Logic, 112(2-3):151–188, 2001.
[Ambos-Spies and Lerman, 1986] K. Ambos-Spies and M. Lerman. Lattice embeddings into the
recursively enumerable degrees. J. Symbolic Logic, 51(2):257–272, 1986.
[Ambos-Spies and Lerman, 1989] K. Ambos-Spies and M. Lerman. Lattice embeddings into the
recursively enumerable degrees. II. J. Symbolic Logic, 54(3):735–760, 1989.
[Ambos-Spies and Shore, 1993] Klaus Ambos-Spies and Richard A. Shore. Undecidability and
1-types in the recursively enumerable degrees. Ann. Pure Appl. Logic, 63(1):3–37, 1993.
[Ambos-Spies and Soare, 1989] Klaus Ambos-Spies and Robert I. Soare. The recursively enu-
merable degrees have inﬁnitely many one-types. Ann. Pure Appl. Logic, 44(1-2):1–23, 1989.
[Ambos-Spies et al., 1984] Klaus Ambos-Spies, Carl G. Jockusch, Jr., Richard A. Shore, and
Robert I. Soare. An algebraic decomposition of the recursively enumerable degrees and the
coincidence of several degree classes with the promptly simple degrees. Trans. Amer. Math.
Soc., 281(1):109–128, 1984.
[Ambos-Spies et al., 1994] Klaus Ambos-Spies, Steﬀen Lempp, and Manuel Lerman.
Lattice
embeddings into the r.e. degrees preserving 1.
In Logic, Methodology and Philosophy of
Science, IX (Uppsala, 1991), volume 134 of Stud. Logic Found. Math., pages 179–198. North-
Holland, Amsterdam, 1994.
[Ambos-Spies et al., 2000] Klaus Ambos-Spies, Denis R. Hirschfeldt, and Richard A. Shore.
Undecidability and 1-types in intervals of the computably enumerable degrees. Ann. Pure
Appl. Logic, 106(1-3):1–47, 2000.
[Ambos-Spies, 1980] Klaus Ambos-Spies. On the Structure of the Recursively Enumerable De-
grees. PhD thesis, University of Munich, 1980.
[Ambos-Spies, 1984] Klaus Ambos-Spies. On pairs of recursively enumerable degrees. Trans.
Amer. Math. Soc., 283(2):507–531, 1984.
[Ash and Knight, 2000] C. J. Ash and J. Knight. Computable structures and the hyperarithmeti-
cal hierarchy, volume 144 of Studies in Logic and the Foundations of Mathematics. North-
Holland Publishing Co., Amsterdam, 2000.
[Ash, 1986a] C. J. Ash.
Recursive labelling systems and stability of recursive structures in
hyperarithmetical degrees. Trans. Amer. Math. Soc., 298(2):497–514, 1986.
[Ash, 1986b] C. J. Ash. Stability of recursive structures in arithmetical degrees. Ann. Pure
Appl. Logic, 32(2):113–135, 1986.
[Boone, 1965] William W. Boone. Finitely presented group whose word problem has the same
degree as that of an arbitrarily given Thue system (an application of methods of Britton).
Proc. Nat. Acad. Sci. U.S.A., 53:265–269, 1965.
[Cai and Shore, 2012] Mingzhong Cai and Richard A. Shore. Domination, forcing, array nonre-
cursiveness and relative recursive enumerability. J. Symbolic Logic, 77(1):33–48, 2012.
[Cai, 2012] Mingzhong Cai.
Array nonrecursiveness and relative recursive enumerability.
J.
Symbolic Logic, 77(1):21–32, 2012.
[Cenzer and Remmel, ta] D. Cenzer and J. B. Remmel. Eﬀectively Closed Sets. ta.
[Cholak and Harrington, 2002] Peter A. Cholak and Leo A. Harrington. On the deﬁnability of
the double jump in the computably enumerable sets. J. Math. Log., 2(2):261–296, 2002.
[Cholak et al., 2000] Peter A. Cholak, Steﬀen Lempp, Manuel Lerman, and Richard A. Shore,
editors.
Computability Theory and its Applications. Current Trends and Open Problems.
Proceedings of the AMS-IMS-SIAM Joint Summer Research Conference Held at the Univer-
sity of Colorado, Boulder, CO, June 13–17, 1999, volume 257 of Contemporary Mathematics,
Providence, RI, 2000. American Mathematical Society.
[Cholak, 1995] Peter Cholak. Automorphisms of the lattice of recursively enumerable sets. Mem.
Amer. Math. Soc., 113(541):viii+151pp., 1995.
[Cohen, 1963] Paul Cohen. The independence of the continuum hypothesis. Proc. Nat. Acad.
Sci. U.S.A., 50:1143–1148, 1963.
[Cooper et al., 1991] S. Barry Cooper, Leo Harrington, Alistair H. Lachlan, Steﬀen Lempp, and
Robert I. Soare. The d.r.e. degrees are not dense. Ann. Pure Appl. Logic, 55(2):125–151,
1991.

Degrees of Unsolvability
487
[Cooper, 1972] S. B. Cooper. Degrees of unsolvability complementary between recursively enu-
merable degrees. I. Ann. Math. Logic, 4:31–73, 1972.
[Cooper, 1973] S. B. Cooper.
Minimal degrees and the jump operator.
J. Symbolic Logic,
38:249–271, 1973.
[Cooper, 1974] S. B. Cooper. Minimal pairs and high recursively enumerable degrees. J. Sym-
bolic Logic, 39:655–660, 1974.
[Cooper, 1990] S. Barry Cooper. The jump is deﬁnable in the structure of the degrees of un-
solvability. Bull. Amer. Math. Soc. (N.S.), 23(1):151–158, 1990.
[Cooper, 1997a] S. Barry Cooper. Beyond G¨odel’s theorem: the failure to capture information
content. In Complexity, Logic, and Recursion Theory, volume 187 of Lecture Notes in Pure
and Appl. Math., pages 93–122. Dekker, New York, 1997.
[Cooper, 1997b] S. Barry Cooper. The Turing universe is not rigid. Preprint Series 16, University
of Leeds, Department of Pure Mathematics, 1997.
[Cooper, 1999a] S. B. Cooper.
Local degree theory.
In Handbook of Computability Theory,
volume 140 of Stud. Logic Found. Math., pages 121–153. North-Holland, Amsterdam, 1999.
[Cooper, 1999b] S. B. Cooper.
Upper cones as automorphism bases.
Siberian Adv. Math.,
9(3):17–77, 1999.
[Cooper, 2001] S. Barry Cooper. On a conjecture of Kleene and Post. Math. Log. Q., 47(1):3–33,
2001.
[Cornell Summer Institute, 1957] Summaries of talks presented at the Summer Institute of Sym-
bolic Logic in 1957 at Cornell University, sponsored by the American Mathematical Society
under a grant from the National Science Foundation, volume I–III, 1957.
[Crossley, 1975] J. N. Crossley. Reminiscences of logicians. With Contributions by C. C. Chang,
John Crossley, Jerry Keisler, Steve Kleene, Mike Morley, Vivienne Morley, Andrzej Mostowski,
Anil Nerode, Gerald Sacks, Peter Hilton and David Lucy. In Algebra and logic (Fourteenth
Summer Res. Inst., Austral. Math. Soc., Monash Univ., Clayton, 1974), volume 450 of Lec-
ture Notes in Math., pages 1–62. Springer, Berlin, 1975.
[D¨egtev, 1973] A. N. D¨egtev. tt- and m-degrees. Algebra i Logika, 12:143–161, 243, 1973.
[Dekker and Myhill, 1958] J. C. E. Dekker and J. Myhill. Some theorems on classes of recursively
enumerable sets. Trans. Amer. Math. Soc., 89:25–59, 1958.
[Dekker, 1954] J. C. E. Dekker.
A theorem on hypersimple sets.
Proc. Amer. Math. Soc.,
5:791–796, 1954.
[Downey and Greenberg, 2006] Rod Downey and Noam Greenberg. Totally < ωω computably
enumerable and m-topped degrees. In Theory and applications of models of computation,
volume 3959 of Lecture Notes in Comput. Sci., pages 46–60. Springer, Berlin, 2006.
[Downey and Hirschfeldt, 2010] Rodney G. Downey and Denis R. Hirschfeldt. Algorithmic ran-
domness and complexity. Theory and Applications of Computability. Springer, New York,
2010.
[Downey and Lempp, 1997] Rodney G. Downey and Steﬀen Lempp. Contiguity and distribu-
tivity in the enumerable Turing degrees. J. Symbolic Logic, 62(4):1215–1240, 1997.
[Downey et al., 1990] Rod Downey, Carl Jockusch, and Michael Stob. Array nonrecursive sets
and multiple permitting arguments. In Recursion theory week (Oberwolfach, 1989), volume
1432 of Lecture Notes in Math., pages 141–173. Springer, Berlin, 1990.
[Downey et al., 1996] Rod Downey, Carl G. Jockusch, and Michael Stob. Array nonrecursive
degrees and genericity. In Computability, enumerability, unsolvability, volume 224 of London
Math. Soc. Lecture Note Ser., pages 93–104. Cambridge Univ. Press, Cambridge, 1996.
[Downey et al., 2007] Rod Downey,
Noam Greenberg,
and Rebecca Weber.
Totally ω-
computably enumerable degrees and bounding critical triples. J. Math. Log., 7(2):145–171,
2007.
[Downey, 1990] Rod Downey.
Lattice nonembeddings and initial segments of the recursively
enumerable degrees. Ann. Pure Appl. Logic, 49(2):97–119, 1990.
[Epstein, 1979] Richard L. Epstein. Degrees of unsolvability: structure and theory, volume 759
of Lecture Notes in Mathematics. Springer, Berlin, 1979.
[Epstein, 2013] Rachel Epstein. The nonlow computably enumerable degrees are not invariant
in E. Trans. Amer. Math. Soc., 365(3):1305–1345, 2013.
[Feferman, 1957] Solomon Feferman. Degrees of unsolvability associated with classes of formal-
ized theories. J. Symbolic Logic, 22:161–175, 1957.

488
Klaus Ambos-Spies and Peter A. Fejer
[Feferman, 1965] Solomon Feferman. Some applications of the notions of forcing and generic
sets: Summary. In Theory of Models (Proc. 1963 Internat. Sympos. Berkeley), pages 89–95.
North-Holland, Amsterdam, 1965.
[Feiner, 1970] L. Feiner. The strong homogeneity conjecture. J. Symbolic Logic, 35:375–377,
1970.
[Fejer and Soare, 1981] Peter A. Fejer and Robert I. Soare. The plus-cupping theorem for the
recursively enumerable degrees. In Logic Year 1979–80 (Proc. Seminars and Conf. Math.
Logic, Univ. Connecticut, Storrs, Conn., 1979/80), volume 859 of Lecture Notes in Math.,
pages 49–62. Springer, Berlin, 1981.
[Fejer, 1983] Peter A. Fejer. The density of the nonbranching degrees. Ann. Pure Appl. Logic,
24(2):113–130, 1983.
[Friedberg, 1956] Richard M. Friedberg. The solution of Post’s problem by the construction
of two recursively enumerable sets of incomparable degrees of unsolvability (abstract). Bull.
Amer. Math. Soc., 62:260, 1956.
[Friedberg, 1957a] R. M. Friedberg. The ﬁne structure of degrees of Unsolvability of recursively
enumerable sets. In Summaries of talks presented at the Summer Institute of Symbolic Logic
in 1957 at Cornell University, sponsored by the American Mathematical Society under a
grant from the National Science Foundation, volume III, pages 404–406. 1957.
[Friedberg, 1957b] Richard M. Friedberg. A criterion for completeness of degrees of unsolvability.
J. Symbolic Logic, 22:159–160, 1957.
[Friedberg, 1957c] Richard M. Friedberg.
Two recursively enumerable sets of incomparable
degrees of unsolvability (solution of Post’s problem, 1944). Proc. Nat. Acad. Sci. U.S.A.,
43:236–238, 1957.
[Friedberg, 1958] Richard M. Friedberg. Three theorems on recursive enumeration. I. Decompo-
sition. II. Maximal set. III. Enumeration without duplication. J. Symbolic Logic, 23:309–316,
1958.
[Greenberg et al., 2006] Noam Greenberg, Richard A. Shore, and Theodore A. Slaman. The
theory of the metarecursively enumerable degrees. J. Math. Log., 6(1):49–68, 2006.
[Griﬀor, 1999] Edward R. Griﬀor, editor. Handbook of Computability Theory, volume 140 of
Studies in Logic and the Foundations of Mathematics. North-Holland Publishing Co., Ams-
terdam, 1999.
[Groszek and Slaman, 1983] Marcia J. Groszek and Theodore A. Slaman. Independence results
on the global structure of the Turing degrees.
Trans. Amer. Math. Soc., 277(2):579–588,
1983.
[Grzegorczyk, 1951] Andrzej Grzegorczyk. Undecidability of some topological theories. Fund.
Math., 38:137–152, 1951.
[Harrington and Kechris, 1975] Leo Harrington and Alexander S. Kechris. A basis result for Σ0
3
sets of reals with an application to minimal covers. Proc. Amer. Math. Soc., 53(2):445–448,
1975.
[Harrington and Shelah, 1982] Leo Harrington and Saharon Shelah. The undecidability of the
recursively enumerable degrees. Bull. Amer. Math. Soc. (N.S.), 6(1):79–80, 1982.
[Harrington and Soare, 1991] Leo Harrington and Robert I. Soare. Post’s program and incom-
plete recursively enumerable sets. Proc. Nat. Acad. Sci. U.S.A., 88(22):10242–10246, 1991.
[Harrington and Soare, 1996a] Leo Harrington and Robert I. Soare.
Deﬁnability, automor-
phisms, and dynamic properties of computably enumerable sets.
Bull. Symbolic Logic,
2(2):199–213, 1996.
[Harrington and Soare, 1996b] Leo Harrington and Robert I. Soare.
The ∆0
3-automorphism
method and noninvariant classes of degrees. J. Amer. Math. Soc., 9(3):617–666, 1996.
[Harrington and Soare, 1996c] Leo Harrington and Robert I. Soare. Dynamic properties of com-
putably enumerable sets. In Computability, Enumerability, Unsolvability, volume 224 of Lon-
don Math. Soc. Lecture Note Ser., pages 105–121. Cambridge Univ. Press, Cambridge, 1996.
[Harrington, 1978] Leo Harrington. Plus cupping in the recursively enumerable degrees., 1978.
Handwritten notes.
[Harrington, 1980] Leo Harrington. Understanding Lachlan’s monster paper., 1980. Handwrit-
ten notes.
[Harrington, 1982] Leo Harrington. A gentle approach to priority arguments., 1982. Handwrit-
ten notes for a talk at AMS-ASL Summer Institute on Recursion Theory, Cornell University.
[Hinman, 1969] Peter G. Hinman. Some applications of forcing to hierarchy problems in arith-
metic. Z. Math. Logik Grundlagen Math., 15:341–352, 1969.

Degrees of Unsolvability
489
[Hugill, 1969] D. F. Hugill. Initial segments of Turing degrees. Proc. London Math. Soc. (3),
19:1–16, 1969.
[Jockusch and Soare, 1970] Carl G. Jockusch, Jr. and Robert I. Soare.
Minimal covers and
arithmetical sets. Proc. Amer. Math. Soc., 25:856–859, 1970.
[Jockusch and Posner, 1981] Carl G. Jockusch, Jr. and David B. Posner. Automorphism bases
for degrees of unsolvability. Israel J. Math., 40(2):150–164, 1981.
[Jockusch and Shore, 1984] Carl G. Jockusch, Jr. and Richard A. Shore. Pseudojump operators.
II. Transﬁnite iterations, hierarchies and minimal covers. J. Symbolic Logic, 49(4):1205–1236,
1984.
[Jockusch and Simpson, 1976] Carl G. Jockusch, Jr. and Stephen G. Simpson.
A degree-
theoretic deﬁnition of the ramiﬁed analytical hierarchy. Ann. Math. Logic, 10(1):1–32, 1976.
[Jockusch and Slaman, 1993] Carl G. Jockusch, Jr. and Theodore A. Slaman. On the Σ2-theory
of the upper semilattice of Turing degrees. J. Symbolic Logic, 58(1):193–204, 1993.
[Jockusch and Soare, 1972] Carl G. Jockusch, Jr. and Robert I. Soare. Π0
1 classes and degrees
of theories. Trans. Amer. Math. Soc., 173:33–56, 1972.
[Jockusch and Solovay, 1977] Carl G. Jockusch, Jr. and Robert M. Solovay.
Fixed points of
jump preserving automorphisms of degrees. Israel J. Math., 26(1):91–94, 1977.
[Jockusch, 1973] Carl G. Jockusch, Jr.
An application of Σ0
4 determinacy to the degrees of
unsolvability. J. Symbolic Logic, 38:293–294, 1973.
[Jockusch, 1980] Carl G. Jockusch, Jr. Degrees of generic sets. In Recursion Theory: Its Gen-
eralisation and Applications (Proc. Logic Colloq., Univ. Leeds, Leeds, 1979), volume 45 of
London Math. Soc. Lecture Note Ser., pages 110–139. Cambridge Univ. Press, Cambridge,
1980.
[Jockusch, 1981] Carl G. Jockusch, Jr. Three easy constructions of recursively enumerable sets.
In Logic Year 1979–80 (Proc. Seminars and Conf. Math. Logic, Univ. Connecticut, Storrs,
Conn., 1979/80), volume 859 of Lecture Notes in Math., pages 83–91. Springer, Berlin, 1981.
[Jockusch, 1985] Carl G. Jockusch, Jr. Genericity for recursively enumerable sets. In Recursion
Theory Week (Oberwolfach, 1984), volume 1141 of Lecture Notes in Math., pages 203–232.
Springer, Berlin, 1985.
[Jockusch, 2002] Carl G. Jockusch, Jr. Review of On a conjecture of Kleene and Post by S. B.
Cooper, 2002. Mathematical Reviews, MR1808943 (2002m:03061).
[Kechris and Moschovakis, 1978] Alexander S. Kechris and Yiannis N. Moschovakis, editors.
Cabal Seminar 76–77, volume 689 of Lecture Notes in Mathematics, Berlin, 1978. Springer.
[Kjos-Hanssen, 2002] Bjørn Kjos-Hanssen. Lattice Initial Segments of the Turing Degrees. PhD
thesis, University of California at Berkeley, 2002.
[Kjos-Hanssen, 2003] Bjørn Kjos-Hanssen. Local initial segments of the Turing degrees. Bull.
Symbolic Logic, 9(1):26–36, 2003.
[Kleene and Post, 1954] S. C. Kleene and Emil L. Post. The upper semi-lattice of degrees of
recursive unsolvability. Ann. of Math. (2), 59:379–407, 1954.
[Kleene, 1936] S. C. Kleene.
General recursive functions of natural numbers.
Math. Ann.,
112:727–742, 1936.
[Kleene, 1943] S. C. Kleene. Recursive predicates and quantiﬁers. Trans. Amer. Math. Soc.,
53:41–73, 1943.
[Kleene, 1952] Stephen Cole Kleene. Introduction to Metamathematics. D. Van Nostrand Co.,
Inc., New York, N. Y., 1952.
[Kleene, 1981] Stephen C. Kleene. Origins of recursive function theory. Ann. Hist. Comput.,
3(1):52–67, 1981.
[Kolmogorov, 1965] A. N. Kolmogorov. Three approaches to the deﬁnition of the concept “quan-
tity of information”. Problemy Peredaˇci Informacii, 1(vyp. 1):3–11, 1965.
[Kuˇcera, 1986] Anton´ın Kuˇcera. An alternative, priority-free, solution to Post’s problem. In
Mathematical Foundations of Computer Science, 1986 (Bratislava, 1986), volume 233 of
Lecture Notes in Comput. Sci., pages 493–500. Springer, Berlin, 1986.
[Kumabe, 1996] Masahiro Kumabe. Degrees of generic sets. In Computability, Enumerability,
Unsolvability, volume 224 of London Math. Soc. Lecture Note Ser., pages 167–183. Cambridge
Univ. Press, Cambridge, 1996.
[Kummer, 1996] Martin Kummer. On the complexity of random strings (extended abstract).
In STACS 96 (Grenoble, 1996), volume 1046 of Lecture Notes in Comput. Sci., pages 25–36.
Springer, Berlin, 1996.

490
Klaus Ambos-Spies and Peter A. Fejer
[Lachlan and Lebeuf, 1976] A. H. Lachlan and R. Lebeuf.
Countable initial segments of the
degrees of unsolvability. J. Symbolic Logic, 41(2):289–300, 1976.
[Lachlan and Soare, 1980] A. H. Lachlan and R. I. Soare. Not every ﬁnite lattice is embeddable
in the recursively enumerable degrees. Adv. in Math., 37(1):74–82, 1980.
[Lachlan, 1966a] A. H. Lachlan. The impossibility of ﬁnding relative complements for recursively
enumerable degrees. J. Symbolic Logic, 31:434–454, 1966.
[Lachlan, 1966b] A. H. Lachlan. Lower bounds for pairs of recursively enumerable degrees. Proc.
London Math. Soc. (3), 16:537–569, 1966.
[Lachlan, 1968a] A. H. Lachlan. Degrees of recursively enumerable sets which have no maximal
supersets. J. Symbolic Logic, 33:431–443, 1968.
[Lachlan, 1968b] A. H. Lachlan. Distributive initial segments of the degrees of unsolvability. Z.
Math. Logik Grundlagen Math., 14:457–472, 1968.
[Lachlan, 1972] A. H. Lachlan. Embedding nondistributive lattices in the recursively enumerable
degrees.
In Conference in Mathematical Logic—London ’70 (Proc. Conf., Bedford Coll.,
London, 1970), pages 149–177. Lecture Notes in Math., Vol. 255. Springer, Berlin, 1972.
[Lachlan, 1975a] A. H. Lachlan. Uniform enumeration operations. J. Symbolic Logic, 40(3):401–
409, 1975.
[Lachlan, 1975b] Alistair H. Lachlan. A recursively enumerable degree which will not split over
all lesser ones. Ann. Math. Logic, 9:307–365, 1975.
[Lachlan, 1979] A. H. Lachlan.
Bounding minimal pairs.
J. Symbolic Logic, 44(4):626–642,
1979.
[Lacombe, 1954] Daniel Lacombe. Sur le semi-r´eseau constitu´e par les degr´es d’ind´ecidabilit´e
r´ecursive. C. R. Acad. Sci. Paris, 239:1108–1109, 1954.
[Ladner and Sasso, 1975] Richard E. Ladner and Leonard P. Sasso, Jr. The weak truth table
degrees of recursively enumerable sets. Ann. Math. Logic, 8(4):429–448, 1975.
[Lempp and Lerman, 1990] Steﬀen Lempp and Manuel Lerman. Priority arguments using it-
erated trees of strategies.
In Recursion theory week (Oberwolfach, 1989), volume 1432 of
Lecture Notes in Math., pages 277–296. Springer, Berlin, 1990.
[Lempp and Lerman, 1995] Steﬀen Lempp and Manuel Lerman. A general framework for pri-
ority arguments. Bull. Symbolic Logic, 1(2):189–201, 1995.
[Lempp and Lerman, 1996] Steﬀen Lempp and Manuel Lerman. The decidability of the exis-
tential theory of the poset of recursively enumerable degrees with jump relations. Adv. Math.,
120(1):1–142, 1996.
[Lempp and Lerman, 1997a] Steﬀen Lempp and Manuel Lerman. A ﬁnite lattice without critical
triple that cannot be embedded into the enumerable Turing degrees. Ann. Pure Appl. Logic,
87(2):167–185, 1997.
[Lempp and Lerman, 1997b] Steﬀen Lempp and Manuel Lerman. Iterated trees of strategies and
priority arguments. Arch. Math. Logic, 36(4-5):297–312, 1997. Sacks Symposium (Cambridge,
MA, 1993).
[Lempp and Nies, 1995] Steﬀen Lempp and Andr´e Nies. The undecidability of the Π4-theory
for the r.e. wtt and Turing degrees. J. Symbolic Logic, 60(4):1118–1136, 1995.
[Lempp et al., 1998] Steﬀen Lempp, Andr´e Nies, and Theodore A. Slaman.
The Π3-theory
of the computably enumerable Turing degrees is undecidable.
Trans. Amer. Math. Soc.,
350(7):2719–2736, 1998.
[Lempp et al., 2006] Steﬀen Lempp, Manuel Lerman, and Reed Solomon.
Embedding ﬁnite
lattices into the computably enumerable degrees—a status survey. In Logic Colloquium ’02,
volume 27 of Lect. Notes Log., pages 206–229. Assoc. Symbol. Logic, La Jolla, CA, 2006.
[Lempp, 2002] Steﬀen Lempp. Review of Upper cones as automorphism bases by S. B. Cooper,
2002. Mathematical Reviews, MR1796986 (2002g:03092).
[Lerman and Shore, 1988] Manuel Lerman and Richard A. Shore. Decidability and invariant
classes for degree structures. Trans. Amer. Math. Soc., 310(2):669–692, 1988.
[Lerman and Soare, 1980] Manuel Lerman and Robert I. Soare. d-simple sets, small sets, and
degree classes. Paciﬁc J. Math., 87(1):135–155, 1980.
[Lerman et al., 1984] M. Lerman, R. A. Shore, and R. I. Soare. The elementary theory of the
recursively enumerable degrees is not ℵ0-categorical. Adv. in Math., 53(3):301–320, 1984.
[Lerman, 1971] Manuel Lerman. Initial segments of the degrees of unsolvability. Ann. of Math.
(2), 93:365–389, 1971.

Degrees of Unsolvability
491
[Lerman, 1973] Manuel Lerman.
Admissible ordinals and priority arguments.
In Cambridge
Summer School in Mathematical Logic (Cambridge, 1971), pages 311–344. Lecture Notes in
Math., Vol. 337. Springer, Berlin, 1973.
[Lerman, 1977] M. Lerman. Automorphism bases for the semilattice of recursively enumerable
degrees. J. Symbolic Logic, 24:A–251, 1977.
[Lerman, 1983] Manuel Lerman. Degrees of Unsolvability. Perspectives in Mathematical Logic.
Springer-Verlag, Berlin, 1983.
[Lerman, 1996] Manuel Lerman. Embeddings into the recursively enumerable degrees. In Com-
putability, Enumerability, Unsolvability, volume 224 of London Math. Soc. Lecture Note Ser.,
pages 185–204. Cambridge Univ. Press, Cambridge, 1996.
[Lerman, 1998] M. Lerman. A necessary and suﬃcient condition for embedding ranked ﬁnite
partial lattices into the computably enumerable degrees. Ann. Pure Appl. Logic, 94(1-3):143–
180, 1998.
[Lerman, 2000] M. Lerman.
A necessary and suﬃcient condition for embedding principally
decomposable ﬁnite lattices into the computably enumerable degrees. Ann. Pure Appl. Logic,
101(2-3):275–297, 2000.
[Lerman, 2010] Manuel Lerman.
A framework for priority arguments, volume 34 of Lecture
Notes in Logic. Association for Symbolic Logic, La Jolla, CA, 2010.
[Maass et al., 1981] Wolfgang Maass, Richard A. Shore, and Michael Stob. Splitting properties
and jump classes. Israel J. Math., 39(3):210–224, 1981.
[Maass, 1982] Wolfgang Maass.
Recursively enumerable generic sets.
J. Symbolic Logic,
47(4):809–823, 1982.
[Marchenkov, 1976] S. S. Marchenkov. A class of incomplete sets. Math. Notes, 20:823–825,
1976.
[Marks et al., 2011] Andrew Marks, Theodore Slaman, and John Steel.
Martin’s conjec-
ture, arithmetic equivalence, and countable borel equivalence relations.
arXiv preprint
arXiv:1109.1875, 2011.
[Martin-L¨of, 1966] Per Martin-L¨of. The deﬁnition of random sequences. Information and Con-
trol, 9:602–619, 1966.
[Martin, 1966] Donald A. Martin. Classes of recursively enumerable sets and degrees of unsolv-
ability. Z. Math. Logik Grundlagen Math., 12:295–310, 1966.
[Martin, 1967] D. A. Martin.
Measure, category, and degrees of unsolvability.
Unpublished
manuscript, 16pp., 1967.
[Martin, 1968] Donald A. Martin. The axiom of determinateness and reduction principles in
the analytical hierarchy. Bull. Amer. Math. Soc., 74:687–689, 1968.
[Martin, 1975] Donald A. Martin. Borel determinacy. Ann. of Math. (2), 102(2):363–371, 1975.
[Miller and Nies, 2006] Joseph S. Miller and Andr´e Nies. Randomness and computability: open
questions. Bull. Symbolic Logic, 12(3):390–410, 2006.
[Miller et al., 2004] Russell G. Miller, Andre O. Nies, and Richard A. Shore. The ∀∃-theory of
R(≤, ∨, ∧) is undecidable. Trans. Amer. Math. Soc., 356(8):3025–3067, 2004.
[Miller, 1981] David P. Miller. High recursively enumerable degrees and the anticupping prop-
erty. In Logic Year 1979–80 (Proc. Seminars and Conf. Math. Logic, Univ. Connecticut,
Storrs, Conn., 1979/80), volume 859 of Lecture Notes in Math., pages 230–245. Springer,
Berlin, 1981.
[Mostowski, 1947] Andrzej Mostowski.
On deﬁnable sets of positive integers.
Fund. Math.,
34:81–112, 1947.
[Muˇcnik, 1956] A. A. Muˇcnik. On the unsolvability of the problem of reducibility in the theory
of algorithms. Dokl. Akad. Nauk SSSR (N.S.), 108:194–197, 1956.
[Muˇcnik, 1958] A. A. Muˇcnik. Solution of Post’s reduction problem and of certain other prob-
lems in the theory of algorithms. Trudy Moskov. Mat. Obˇsˇc., 7:391–405, 1958.
[Myhill, 1956] J. Myhill.
The lattice of recursively enumerable sets (abstract).
J. Symbolic
Logic, 21:220, 1956.
[Myhill, 1961] J. Myhill. Category methods in recursion theory. Paciﬁc J. Math., 11:1479–1486,
1961.
[Nerode and Remmel, 1986] A. Nerode and J. B. Remmel. Generic objects in recursion theory.
II. Operations on recursive approximation spaces. Ann. Pure Appl. Logic, 31(2-3):257–288,
1986.
[Nerode and Shore, 1980a] Anil Nerode and Richard A. Shore. Reducibility orderings: theories,
deﬁnability and automorphisms. Ann. Math. Logic, 18(1):61–89, 1980.

492
Klaus Ambos-Spies and Peter A. Fejer
[Nerode and Shore, 1980b] Anil Nerode and Richard A. Shore.
Second order logic and ﬁrst
order theories of reducibility orderings.
In The Kleene Symposium (Proc. Sympos., Univ.
Wisconsin, Madison, Wis., 1978), volume 101 of Stud. Logic Foundations Math., pages 181–
200. North-Holland, Amsterdam, 1980.
[Nerode and Shore, 1985] A. Nerode and R. A. Shore, editors. Recursion Theory. Proceedings of
the AMS-ASL Summer Institute Held in Ithaca, N.Y., June 28–July 16, 1982, volume 42 of
Proceedings of Symposia in Pure Mathematics, Providence, RI, 1985. American Mathematical
Society.
[Nies et al., 1998] Andr´e Nies, Richard A. Shore, and Theodore A. Slaman. Interpretability and
deﬁnability in the recursively enumerable degrees. Proc. London Math. Soc. (3), 77(2):241–
291, 1998.
[Nies, 2009] Andr´e Nies. Computability and randomness, volume 51 of Oxford Logic Guides.
Oxford University Press, Oxford, 2009.
[Odifreddi, 1989] Piergiorgio Odifreddi. Classical Recursion Theory, volume 125 of Studies in
Logic and the Foundations of Mathematics. North-Holland Publishing Co., Amsterdam, 1989.
[Odifreddi, 1999a] P. Odifreddi. Classical Recursion Theory. Vol. II, volume 143 of Studies in
Logic and the Foundations of Mathematics. North-Holland Publishing Co., Amsterdam, 1999.
[Odifreddi, 1999b] Piergiorgio Odifreddi. Reducibilities. In Handbook of Computability Theory,
volume 140 of Stud. Logic Found. Math., pages 89–119. North-Holland, Amsterdam, 1999.
[Paris, 1972] J. B. Paris. ZF ⊢Σ0
4 determinateness. J. Symbolic Logic, 37:661–667, 1972.
[Posner and Robinson, 1981] David B. Posner and Robert W. Robinson. Degrees joining to 0′.
J. Symbolic Logic, 46(4):714–722, 1981.
[Posner, 1981] David B. Posner. The upper semilattice of degrees ≤0′ is complemented. J.
Symbolic Logic, 46(4):705–713, 1981.
[Post, 1944] Emil L. Post. Recursively enumerable sets of positive integers and their decision
problems. Bull. Amer. Math. Soc., 50:284–316, 1944.
[Post, 1948] E. L. Post. Degrees of recursive unsolvability: preliminary report (abstract). Bull.
Amer. Math. Soc., 54:641–642, 1948.
[Post, 1965] E. L. Post. Absolutely unsolvable problems and relatively undecidable propositions.
In M. Davis, editor, The Undecidable. Basic Papers on Undecidable Propositions, Unsolvable
Problems and Computable Functions, pages 338–433. Raven Press, Hewlett, N.Y., 1965.
[Post, 1994] Emil L. Post. Solvability, Provability, Deﬁnability: The Collected Works of Emil
L Post. (Davis, M., editor). Contemporary Mathematicians. Birkh¨auser Boston Inc., Boston,
MA, 1994.
[Richter, 1979] Linda Jean Richter.
On automorphisms of the degrees that preserve jumps.
Israel J. Math., 32(1):27–31, 1979.
[Robinson, 1971a] Robert W. Robinson. Interpolation and embedding in the recursively enu-
merable degrees. Ann. of Math. (2), 93:285–314, 1971.
[Robinson, 1971b] Robert W. Robinson. Jump restricted interpolation in the recursively enu-
merable degrees. Ann. of Math. (2), 93:586–596, 1971.
[Rogers, 1959] Hartley Rogers, Jr. Computing degrees of unsolvability. Math. Ann., 138:125–
140, 1959.
[Rogers, 1967a] H. Rogers, Jr. Some problems of deﬁnability in recursive function theory. In
Sets, Models and Recursion Theory (Proc. Summer School Math. Logic and Tenth Logic
Colloq., Leicester, 1965), pages 183–201. North-Holland, Amsterdam, 1967.
[Rogers, 1967b] Hartley Rogers, Jr. Theory of Recursive Functions and Eﬀective Computability.
McGraw-Hill Book Co., New York, 1967.
[Rosenstein, 1968] Joseph G. Rosenstein. Initial segments of degrees. Paciﬁc J. Math., 24:163–
172, 1968.
[Sacks, 1961] Gerald E. Sacks. A minimal degree less than 0′. Bull. Amer. Math. Soc., 67:416–
419, 1961.
[Sacks, 1963a] Gerald E. Sacks. Degrees of Unsolvability, volume 55 of Ann. of Math. Studies.
Princeton University Press, Princeton, N.J., 1963.
[Sacks, 1963b] Gerald E. Sacks. On the degrees less than 0′. Ann. of Math. (2), 77:211–231,
1963.
[Sacks, 1963c] Gerald E. Sacks. Recursive enumerability and the jump operator. Trans. Amer.
Math. Soc., 108:223–239, 1963.

Degrees of Unsolvability
493
[Sacks, 1964] Gerald E. Sacks. The recursively enumerable degrees are dense. Ann. of Math.
(2), 80:300–312, 1964.
[Sacks, 1966] Gerald E. Sacks. Degrees of Unsolvability, second edition, volume 55 of Ann. of
Math. Studies. Princeton University Press, Princeton, N.J., 1966.
[Sacks, 1999] Gerald E. Sacks. Selected Logic Papers, volume 6 of World Scientiﬁc Series in
20th Century Mathematics. World Scientiﬁc Publishing Co. Inc., River Edge, NJ, 1999.
[Shoenﬁeld, 1959] J. R. Shoenﬁeld. On degrees of unsolvability. Ann. of Math. (2), 69:644–653,
1959.
[Shoenﬁeld, 1960] J. R. Shoenﬁeld. Degrees of models. J. Symbolic Logic, 25:233–237, 1960.
[Shoenﬁeld, 1961] J. R. Shoenﬁeld. Undecidable and creative theories. Fund. Math., 49:171–179,
1961.
[Shoenﬁeld, 1965] J. R. Shoenﬁeld. Applications of model theory to degrees of unsolvability. In
Theory of Models (Proc. 1963 Internat. Sympos. Berkeley), pages 359–363. North-Holland,
Amsterdam, 1965.
[Shoenﬁeld, 1966] J. R. Shoenﬁeld. A theorem on minimal degrees. J. Symbolic Logic, 31:539–
544, 1966.
[Shoenﬁeld, 1971] Joseph R. Shoenﬁeld. Degrees of Unsolvability. North-Holland Publishing
Co., Amsterdam, 1971.
[Shoenﬁeld, 1975] J. R. Shoenﬁeld. The decision problem for recursively enumerable degrees.
Bull. Amer. Math. Soc., 81(6):973–977, 1975.
[Shoenﬁeld, 1976] J. R. Shoenﬁeld. Degrees of classes of RE sets. J. Symbolic Logic, 41(3):695–
696, 1976.
[Shore and Slaman, 1999] Richard A. Shore and Theodore A. Slaman.
Deﬁning the Turing
jump. Math. Res. Lett., 6(5-6):711–722, 1999.
[Shore and Slaman, 2006] Richard A. Shore and Theodore A. Slaman. The ∀∃theory of D(≤
, ∨,′ ) is undecidable. In Logic Colloquium ’03, volume 24 of Lect. Notes Log., pages 326–344.
Assoc. Symbol. Logic, La Jolla, CA, 2006.
[Shore, 1978] Richard A. Shore.
On the ∀∃-sentences of α-recursion theory.
In Generalized
Recursion Theory, II (Proc. Second Sympos., Univ. Oslo, Oslo, 1977), volume 94 of Stud.
Logic Foundations Math., pages 331–353. North-Holland, Amsterdam, 1978.
[Shore, 1979] Richard A. Shore. The homogeneity conjecture. Proc. Nat. Acad. Sci. U.S.A.,
76(9):4218–4219, 1979.
[Shore, 1981] Richard A. Shore. The theory of the degrees below 0′. J. London Math. Soc. (2),
24(1):1–14, 1981.
[Shore, 1982a] Richard A. Shore. Finitely generated codings and the degrees r.e. in a degree d.
Proc. Amer. Math. Soc., 84(2):256–263, 1982.
[Shore, 1982b] Richard A. Shore. On homogeneity and deﬁnability in the ﬁrst-order theory of
the Turing degrees. J. Symbolic Logic, 47(1):8–16, 1982.
[Shore, 1988] Richard A. Shore. Deﬁning jump classes in the degrees below 0′. Proc. Amer.
Math. Soc., 104(1):287–292, 1988.
[Shore, 1997] Richard A. Shore.
Conjectures and questions from Gerald Sacks’s
Degrees of
Unsolvability. Arch. Math. Logic, 36(4-5):233–253, 1997.
[Shore, 1999] Richard A. Shore.
The recursively enumerable degrees.
In Handbook of Com-
putability Theory, volume 140 of Stud. Logic Found. Math., pages 169–197. North-Holland,
Amsterdam, 1999.
[Shore, 2006] Richard A. Shore. Degree structures: local and global investigations. Bull. Sym-
bolic Logic, 12(3):369–389, 2006.
[Shore, 2007] Richard A. Shore. Direct and local deﬁnitions of the Turing jump. J. Math. Log.,
7(2):229–262, 2007.
[Shore, 2014] Richard A Shore. Biinterpretability up to double jump in the degrees below 0′.
Proc. Amer. Math. Soc., 142:351–360, 2014.
[Simpson, 1977] Stephen G. Simpson. First-order theory of the degrees of recursive unsolvability.
Ann. of Math. (2), 105(1):121–139, 1977.
[Simpson, 2009] Stephen G. Simpson. Subsystems of second order arithmetic. Perspectives in
Logic. Cambridge University Press, Cambridge, second edition, 2009.
[Slaman and Soare, 2001] Theodore A. Slaman and Robert I. Soare. Extension of embeddings
in the computably enumerable degrees. Ann. of Math. (2), 154(1):1–43, 2001.
[Slaman and Woodin, 1986] Theodore A. Slaman and W. Hugh Woodin.
Deﬁnability in the
Turing degrees. Illinois J. Math., 30(2):320–334, 1986.

494
Klaus Ambos-Spies and Peter A. Fejer
[Slaman and Woodin, July 21 2005] T. A. Slaman and W. H. Woodin. Deﬁnability in degree
structures, July 21, 2005. Typescript.
[Slaman and Woodin, ta] T. A. Slaman and W. H. Woodin. Deﬁnability in Degree Structures.
ta.
[Slaman, 1991a] Theodore A. Slaman. Degree structures. In Proceedings of the International
Congress of Mathematicians, Vol. I, II (Kyoto, 1990), pages 303–316, Tokyo, 1991. Math.
Soc. Japan.
[Slaman, 1991b] Theodore A. Slaman.
The density of inﬁma in the recursively enumerable
degrees. Ann. Pure Appl. Logic, 52(1-2):155–179, 1991.
[Slaman, 2008] Theodore A. Slaman. Global properties of the Turing degrees and the Turing
jump. In Computational prospects of inﬁnity. Part I. Tutorials, volume 14 of Lect. Notes
Ser. Inst. Math. Sci. Natl. Univ. Singap., pages 83–101. World Sci. Publ., Hackensack, NJ,
2008.
[Soare, 1987] Robert I. Soare. Recursively Enumerable Sets and Degrees. Perspectives in Math-
ematical Logic. Springer-Verlag, Berlin, 1987.
[Soare, 1999] Robert I. Soare. The history and concept of computability. In Handbook of Com-
putability Theory, volume 140 of Stud. Logic Found. Math., pages 3–36. North-Holland, Am-
sterdam, 1999.
[Spector, 1956] Cliﬀord Spector.
On degrees of recursive unsolvability.
Ann. of Math. (2),
64:581–592, 1956.
[Spector, 1958] Cliﬀord Spector. Measure-theoretic construction of incomparable hyperdegrees.
J. Symbolic Logic, 23:280–288, 1958.
[Stillwell, 1972] John Stillwell. Decidability of the “almost all” theory of degrees. J. Symbolic
Logic, 37:501–506, 1972.
[Thomason, 1970] S. K. Thomason. A theorem on initial segments of degrees. J. Symbolic Logic,
35:41–45, 1970.
[Thomason, 1971] S. K. Thomason. Sublattices of the recursively enumerable degrees. Z. Math.
Logik Grundlagen Math., 17:273–280, 1971.
[Titgemeyer, 1962] D. Titgemeyer.
Untersuchungen ¨uber die Struktur des Kleene-Postschen
Halbverbandes der Grade der rekursiven Unl¨osbarkeit. PhD thesis, University of M¨unster,
1962.
[Titgemeyer, 1965] Dieter Titgemeyer. Untersuchungen ¨uber die Struktur des Kleene-Postschen
Halbverbandes der Grade der rekursiven Unl¨osbarkeit. Arch. Math. Logik Grundlagenforsch.,
8:45–62 (1965), 1965.
[Turing, 1936] A. M. Turing. On computable numbers, with an application to the Entschei-
dungsproblem. Proc. Lond. Math. Soc., II. Ser., 42:230–265, 1936.
[Turing, 1939] A. M. Turing. Systems of logic based on ordinals. Proc. Lond. Math. Soc., II.
Ser., 45:161–228, 1939.
[Yates, 1965] C. E. M. Yates. Three theorems on the degrees of recursively enumerable sets.
Duke Math. J., 32:461–468, 1965.
[Yates, 1966a] C. E. M. Yates. A minimal pair of recursively enumerable degrees. J. Symbolic
Logic, 31:159–168, 1966.
[Yates, 1966b] C. E. M. Yates. On the degrees of index sets. Trans. Amer. Math. Soc., 121:309–
328, 1966.
[Yates, 1970] C. E. M. Yates.
Initial segments of the degrees of unsolvability. I. A survey.
In Mathematical Logic and Foundations of Set Theory (Proc. Internat. Colloq., Jerusalem,
1968), pages 63–83. North-Holland, Amsterdam, 1970.
[Yates, 1976] C. E. M. Yates. Banach-Mazur games, comeager sets and degrees of unsolvability.
Math. Proc. Cambridge Philos. Soc., 79(2):195–220, 1976.

COMPUTATIONAL COMPLEXITY
Lance Fortnow and Steven Homer
Readers: Peter A. Fejer and Kurt Mehlhorn
1
INTRODUCTION
It all started with a machine. In 1936, Turing developed his theoretical computa-
tional model based his model on how he perceived that mathematicians think. As
digital computers were developed in the 40’s and 50’s, the Turing machine proved
itself as the right theoretical model for computation.
Quickly though we discovered that the basic Turing machine model fails to
account for the amount of time or memory needed by a computer, a critical issue
today but even more so in those early days of computing. The key idea to measure
time and space as a function of the length of the input came in the early 1960’s
by Hartmanis and Stearns. And thus computational complexity was born.
In the early days of complexity, researchers just tried understanding these new
measures and how they related to each other. We saw the ﬁrst notion of eﬃcient
computation by using time polynomial in the input size. This led to complexity’s
most important concept, NP-completeness, and its most fundamental question,
whether P = NP.
The work of Cook and Karp in the early 70’s showed a large number of com-
binatorial and logical problems were NP-complete, i.e., as hard as any problem
computable in nondeterministic polynomial time. The P = NP question is equiv-
alent to an eﬃcient solution of any of these problems. In the thirty years hence this
problem has become one of the outstanding open questions in computer science
and indeed all of mathematics.
In the 70’s we saw the growth of complexity classes as researchers tried to
encompass diﬀerent models of computations. One of those models, probabilistic
computation, started with a probabilistic test for primality, led to probabilistic
complexity classes and a new kind of interactive proof system that itself led to
hardness results for approximating certain NP-complete problems. We have also
seen strong evidence that we can remove the randomness from computations and
now have a deterministic algorithm for the original primality problem.
In the 80’s we saw the rise of ﬁnite models like circuits that capture computa-
tion in an inherently diﬀerent way. A new approach to problems like P = NP
arose from these circuits and though they have had limited success in separating
Handbook of the History of Logic. Volume 9: Computational Logic. 
Volume editor: Jörg Siekmann 
Series editors: Dov M. Gabbay and John Woods 
Copyright © 2014 Elsevier BV. All rights reserved.

496
Lance Fortnow and Steven Homer
complexity classes, this approach brought combinatorial techniques into the area
and led to a much better understanding of the limits of these devices.
In the 90’s we have seen the study of new models of computation like quantum
computers and propositional proof systems.
Tools from the past have greatly
helped our understanding of these new areas.
One cannot in the short space of this article mention all of the amazing research
in computational complexity theory of the twentieth century. We survey various
areas in complexity choosing papers more for their historical value than necessarily
the importance of the results. We hope that this gives an insight into the richness
and depth of this still quite young ﬁeld.
2
EARLY HISTORY
While we can trace the idea of “eﬃcient algorithms” to the ancient Greeks, our
story starts with the seminal 1965 paper of Hartmanis and Stearns, “On the Com-
putational Complexity of Algorithms” [Hartmanis and Stearns, 1965]. This paper
laid out the deﬁnitions of quantiﬁed time and space complexity on multitape Tur-
ing machines and showed the ﬁrst results of the form given more time (or space)
one can compute more things.
A multitape Turing machine consists of some ﬁxed number of “tapes” each of
which contains an inﬁnite number of tape cells. The contents of a tape cell comes
from a ﬁnite set of symbols called the tape alphabet of the Turing machine. All the
tapes initially contain only a special “blank” character except for the ﬁnite input
written at the beginning of the ﬁrst tape. Each tape has a tape head sitting on the
ﬁrst character on each tape. The Turing machine also has a ﬁnite state memory
to control its operations. In each step, it can move each tape independently one
character left or right, read and possibly change the characters under each head,
change its current state and decide whether to halt and accept or reject the input.
Time is measured by the number of steps before halting as a function of the length
of the input. Space is measured as the number of diﬀerent character locations
touched by the various heads.
The Hartmanis-Stearns paper did not develop in a vacuum. Turing [1936], of
course, developed his notion of a computational device back in 1936. This machine
model did and still does form the basis for most of computational complexity. In
the early 1960’s Yamada [1962] studied “real-time computable functions”, My-
hill [1960] looked at linear bounded automata and Smullyan [1961] considered
rudimentary sets. These models looked at speciﬁc time and space-bounded ma-
chines but did not give a general approach to measuring complexity.
After Hartmanis and Stearns developed the general method for measuring com-
putational resources, one can ask how the diﬀerent variations of Turing machines
aﬀect the complexity of problems. Rabin [1963] shows problems solvable faster
by two-tape machine than by one-tape machines. Hennie and Stearns [1966] show
that a 2-tape Turing machine can simulate any constant tape machine taking only
a logarithmic factor more time.

Computational Complexity
497
Hartmanis and Stearns show that given space functions s1 and s2 with a “con-
structibility” condition and s1(n) = o(s2(n)), i.e., s1(n)/s2(n) goes to zero, then
there are problems computable in space s2(n) but not space s1(n). The Hennie-
Stearns result gives the best known time-hierarchy, getting a separation if
t1(n) log t1(n) = o(t2(n)). These proofs use straightforward diagonalization ar-
guments that go back to Cantor [1874].
Nondeterministic computation allows a Turing machine to make a choice of sev-
eral possible transitions. We say the machine accepts if any collection of choices
leads to an accepting state. Nondeterministic time and space hierarchies are much
trickier to prove because one cannot do straightforward diagonalization on nonde-
terministic computations.
Savitch [1970] showed that problems computable in nondeterministic space s(n)
are computable in deterministic space s2(n).
In 1972, Ibarra [1972] combined
translational techniques of Ruby and Fischer [1965] and Savitch’s theorem to show
that there exist problems computable in nondeterministic space na but not space
nb for a > b ≥1. Sixteen years later, Immerman [1988] and Szelepcs´enyi [1988]
independently showed that nondeterministic space is closed under complement.
The Immerman-Szelepcs´enyi result immediately gives a nondeterministic space
hierarchy as tight as the the deterministic hierarchy.
For nondeterministic time, Cook [1973] uses a more careful translation argu-
ment to show problems computable in nondeterministic time na but not time nb
for a > b ≥1. Seiferas, Fischer and Meyer [1978] give the current best known
nondeterministic time hierarchy, getting a separation if t1(n + 1) = o(t2(n)).
In 1967, Blum [1967] had his speed-up theorem: For any computable unbounded
function r(n) there exists a computable language L such that for any Turing
machine accepting L in time t(n) there is another Turing machine accepting L in
time r(t(n)). This seems to violate the time hierarchy mentioned earlier but one
must realize t(n) will not necessarily be time-constructible.
Blum’s speed-up theorem holds not only for time but also for space and any
other measure fulﬁlling a small list of axioms, which we now call Blum complexity
measures.
Soon after we saw two other major results that we will state for time but
also hold for all Blum complexity measures. Independently Borodin [1972] and
Trakhtenbrot [1964] proved the gap theorem: For any computable unbounded r(n)
there exist a computable time bound t(n) such that any language computable in
time t(n) is also computable in time r(t(n)). McCreight and Meyer [1969] showed
the union theorem: Given any computably presentable list of computable time
bounds t1, t2, . . . such that ti+1 > ti for all i then there exist a time bound t such
that a problem is computable in time t if and only if it is computable in time ti
for some i.
In 1964, Cobham [1964] noted that the set of problems computable in poly-
nomial time remains independent of the particular deterministic machine model.
He also showed that many common mathematical functions can be computed in
polynomial time.

498
Lance Fortnow and Steven Homer
In 1965, Edmonds [1965a] in his paper showing that the matching problem has
a polynomial-time algorithm, argues that polynomial-time gives a good formaliza-
tion of eﬃcient computation. He noted the wide range of problems computable
in polynomial time and as well the fact that this class of problems remains the
same under many diﬀerent reasonable models of computation. In another paper,
Edmonds [1965] gave an informal description of nondeterministic polynomial-time.
This set the stage for the P = NP question, the most famous problem in theoret-
ical computer science that we discuss in Section 3.
Several Russians, notably Barzdin and Trakhtenbrot, independently developed
several of these notions of complexity during the sixties though their work was not
known to the West until the seventies.
3
NP-COMPLETENESS
It was in the early 1970’s that complexity theory ﬁrst ﬂowered, and came to play
a central role in computer science.
It did so by focusing on one fundamental
concept and on the results and ideas stemming from it. This concept was NP-
completeness and it has proved to be one of the most insightful and fundamental
theories in the mathematics of the last half century. NP-completeness captures
the combinatorial diﬃculty of a number of central problems which resisted eﬃcient
solution and provides a method for proving that a combinatorial problem is as
intractable as any NP problem.
By the late 1960’s, a sizable class of very applicable and signiﬁcant problems
which resisted polynomial time solution was widely recognized. These problems
are largely optimization problems such as the traveling salesman problem, certain
scheduling problems, or linear programming problems. They all have a very large
number of possible solution where there is no obvious way to ﬁnd an optimal solu-
tion other than a brute force search. As time passed and much eﬀort was expended
on attempts at eﬃciently solving these problems, it began to be suspected that
there was no such solution. However, there was no hard evidence that this was the
case nor was there any reason to suspect that these problems were in any sense
diﬃcult for the same reasons or in the same ways. The theory of NP-completeness
provided precisely this evidence.
Proving a problem in NP to be NP-complete tells us that it is as hard to
solve as any other NP problem. Said another way, if there is any NP-complete
problem that admits an eﬃcient solution then every NP problem does so. The
question of whether every NP problem has an eﬃcient solution has resisted the
eﬀorts of computer scientists since 1970. It is known as the P versus NP problem
and is among the most central open problems of mathematics. The fact that a
very large number of fundamental problems have been shown to be NP-complete
and that the problem of proving that P is not NP has proved to be so diﬃcult
has made this problem and the connected theory one of the most celebrated in
contemporary mathematics. The P = NP problem is one of the seven Millennium
Prize Problems and solving it brings a $1,000,000 prize from the Clay Mathematics

Computational Complexity
499
Institute [Clay Math. Inst., 2000].
Quite surprisingly, one of the earliest discussions of a particular NP-complete
problem and the implications of ﬁnding an eﬃcient solution came from Kurt G¨odel.
In a 1956 letter to von Neumann [Hartmanis, 1986; Sipser, 1983] G¨odel asks von
Neumann about the complexity of what is now known to be an NP-complete
problem concerning proofs in ﬁrst-order logic and asks if the problem can be
solved in linear or quadratic time. In fact, G¨odel seemed quite optimistic about
ﬁnding an eﬃcient solution. He fully realized that doing so would have signiﬁcant
consequences.
It is worth noting that in about the same period there was considerable ef-
fort by Russian mathematicians working on similar combinatorial problems trying
to prove that brute force was needed to solve them. Several of these problems
eventually turned out to be NP-complete as well [Trakhtenbrot, 1964].
The existence of NP-complete problems was proved independently by Stephen
Cook in the United States and Leonid Levin in the Soviet Union. Cook, then
a graduate student at Harvard, proved that the satisﬁability problem is NP-
complete [Cook, 1971]. Levin, a student of Kolmogorov at Moscow State Uni-
versity, proved that a variant of the tiling problem is NP-complete [Levin, 1973].
Researchers strove to show other interesting, natural problems NP-complete.
Richard Karp, in a tremendously inﬂuential paper [Karp, 1972], proved that eight
central combinatorial problems are all NP-complete. These problems included the
the clique problem, the independent set problem, the set cover problem, and the
traveling salesman problem, among others.
Karp’s paper presented several key methods to prove NP-completeness using
reductions from problems previously shown to be NP-complete. It set up a gen-
eral framework for proving NP-completeness results and established several useful
techniques for such proofs. In the following years, and continuing until today, lit-
erally thousands of problems have been shown to be NP-complete. A proof of
NP-completeness has come to signify the (worst case) intractability of a problem.
Once proved NP-complete, researchers turn to other ways of trying to solve the
problem, usually using approximation algorithms to give an approximate solution
or probabilistic methods to solve the problem in “most” cases.
Another fundamental step was taken around 1970 by Meyer and Stockmeyer
[1972; 1976]. They deﬁned the polynomial hierarchy in analogy with the arithmetic
hierarchy of Kleene. This hierarchy is deﬁned by iterating the notion of polynomial
jump, in analogy with the Turing jump operator. This hierarchy has proven useful
in classifying many hard combinatorial problems which do not lie in NP. It is
explored in more detail in Section 4.2.
Of course, all problems in the polynomial hierarchy are computable and in
fact very simple problems within the vast expanse of all computable sets.
So
are there natural problems which are computable and are not captured by the
hierarchy? The answers is yes and results in the exploration of several important
larger complexity classes which contain the polynomial hierarchy. One such class
is PSPACE, those problems which can be solved using work space which is of

500
Lance Fortnow and Steven Homer
polynomial length relative to the length of the problem’s input. Just as with P and
NP, the full extent of PSPACE is not known. PSPACE contains P and NP.
It is not known if either of these conclusions are proper. Settling these questions
would again be signiﬁcant steps forward in this theory.
The notion of PSPACE-completeness is deﬁned very similarly to NP-complete-
ness, and has been studies alongside the the NP-completeness notion. Namely, a
problem C is PSPACE-complete if it is in PSPACE and if any other PSPACE
problem can be reduced to it in polynomial time. As is the case with NP-complete
problems, PSPACE-complete problems are quite common and often arise quite
naturally.
Typical PSPACE-complete problems are or arise from generalized
games such as hex or checkers played on boards of unbounded ﬁnite size (see [Garey
and Johnson, 1979]). Beyond PSPACE lie the exponential time (EXPTIME)
and exponential space complexity classes. A small number of natural problems
have been shown complete for these classes (see [Garey and Johnson, 1979]), and
as well EXPTIME is the smallest deterministic class which has been proved to
contain NP.
4
STRUCTURAL COMPLEXITY
By the early 1970’s, the deﬁnitions of time and space-bounded complexity classes
were precisely established and the import of the class NP and of NP-complete
problems realized. At this point eﬀort turned to understanding the relationships
between complexity classes and the properties of problems within the principal
classes. In particular, attention was focused on NP-complete problems and their
properties and on the structure of complexity classes between LOGSPACE and
PSPACE. We brieﬂy survey some of these studies here.
4.1
The Isomorphism Conjecture
In the mid-70’s, building on earlier work on G¨odel numberings [Hartmanis and
Baker, 1975; Hartmanis, 1982] and in analogy with the well-known result of Myhill
from computability theory [Myhill, 1955], Berman and Hartmanis [Berman and
Hartmanis, 1977; Hartmanis and Berman, 1978] formulated their isomorphism
conjecture. The conjecture stated that all NP-complete sets are P-isomorphic
(that is, isomorphic via polynomial time computable and invertible isomorphisms).
This conjecture served as a springboard for the further study of the structure of
NP-complete sets. As evidence for their conjecture, Berman and Hartmanis and
others [Mahaney and Young, 1985; Kurtz et al., 1987] were able to give simple,
easily checkable properties of NP-complete sets which implied they were isomor-
phic. Using these, they proved that all of the known NP-complete sets were in
fact P-isomorphic. This conjecture remains an open question today. A positive
resolution of the conjecture would imply that P is not equal to NP. Much ef-
fort was focused on proving the converse, that assuming P is not NP then the
isomorphism conjecture holds. This remains an open question today.

Computational Complexity
501
As the number of known NP-complete problems grew during the 1970’s, the
structure and properties of these problems began to be examined. While very
disparate, the NP-complete sets have certain common properties. For example,
they are all rather dense sets. Density of a set is measured here simply in the
sense of how many string of a given length are in the set. So (assuming a binary
encoding of a set) there are 2n diﬀerent strings of length n. We say that set S
is sparse if there is a polynomial p(n) which bounds the number of strings in S
of length n, for every n. It is dense otherwise. All known NP-complete sets are
dense.
One consequence of the isomorphism conjecture is that no NP-complete set
can be sparse. As with the isomorphism conjecture, this consequence implies that
P is not NP and so it is unlikely that a proof of this consequence will soon be
forthcoming. Berman and Hartmanis also conjectured that if P in not equal to NP
there are no sparse NP-complete sets. This conjecture was settled aﬃrmatively by
the famous result of Mahaney [1982]. Mahaney’s elegant proof used several new
counting techniques and had a lasting impact on work in structural complexity
theory.
4.2
The Polynomial Hierarchy
While numerous hard decision problems have been proved NP-complete, a small
number are outside NP and have escaped this classiﬁcation. An extended classi-
ﬁcation, the polynomial time hierarchy (PH), was provided by Meyer and Stock-
meyer [1976]. They deﬁned the hierarchy, a collection of classes between P and
PSPACE, in analogy with Kleene’s arithmetic hierarchy.
The polynomial time hierarchy (PH) consists of an inﬁnite sequence of classes
within PSPACE. The bottom (0th) level of the hierarchy is just the class P. The
ﬁrst level is the class NP. The second level are all problems in NP relative to an
NP oracle, etc. Iterating this idea to all ﬁnite levels yields the full hierarchy.
If P=PSPACE then the whole PH collapses to the class P. However, quite
the opposite is believed to be the case, namely that the PH is strict in the sense
that each level of the hierarchy is a proper subset of the next level. While the
PH is contained in PSPACE, the converse is not true if the hierarchy is strict.
In this case, PSPACE contains many problems not in the PH and in fact has a
very complex structure (see, for example, [Ambox-Spies, 1989]).
4.3
Alternation
Another unifying and important thread of results which also originated during the
1970’s was the work on alternation initiated out by Kozen, Chandra and Stock-
meyer [1981]. The idea behind alternation is to classify combinatorial problems
using an alternating Turing machine, a generalization of a nondeterministic Turing
machine. Intuitively a nondeterministic Turing machine can be thought of as hav-
ing an existential acceptance criterion. That is, an input to the TM is accepted

502
Lance Fortnow and Steven Homer
if there exists a computation path of the machine which results in acceptance.
Similarly, we could consider a universal acceptance criterion whereby a nondeter-
ministic machine accepts of all computation paths lead to acceptance. Restricting
ourselves to polynomial length alternation, we see that NP can be characterized
as those problems accepted by nondeterministic TM running in polynomial time
using the existential acceptance criterion. Similarly, the universal acceptance crite-
rion with the same type of machines deﬁnes the class co-NP consisting of problems
whose complements are in NP. Furthermore, we can iterate these two acceptance
methods, for example asking that there exist a path of a TM such that for all
paths extending that path there exists an extension of that path which accepts.
This idea gives a machine implementation of the notion of alternations of universal
and existential quantiﬁers. Finitely many alternations is equivalent to ﬁnite levels
of the polynomial time hierarchy and unbounded alternating polynomial time is
the same thing as PSPACE. Other relationship between time and space classes
deﬁned using alternation can be found in Chandra, Kozen and Stockmeyer [1981],
for example, alternating log space = P and alternating PSPACE = EXPTIME.
4.4
Logspace
To this point all the complexity classes we have considered contain the class P of
polynomial time computable problems. For some interesting problems it is useful
to consider classes within P and particularly the seemingly smaller space classes
of deterministic log space, denoted L, and nondeterministic log space, denoted
NL. These classes provide a measure with which to distinguish between some
interesting problems within P, and present important issues in their own right.
At ﬁrst glance logarithmic space is a problematic notion at best. An input of
length n takes n squares by itself, so how can a computation on such an input take
only log n space? The answer lies in changing our computation model slightly to
only count the space taken by the computation and not the space of the input.
Formally, this is done by considering an “oﬀ-line Turing machine.”
This is a
(deterministic or nondeterministic) Turing machine whose input is written on a
special read-only input tape. Computation is carried out on read-write work tapes
which are initially blank. The space complexity of the computation is then taken
to be the amount of space used on the work tapes. So in particular this space can
be less than n, the length of the input to the computation. We deﬁne logspace, L,
to be the class of languages decided by deterministic Turing machines which use
at most O(log n) tape squares. Similarly, NL is deﬁned using nondeterministic
Turing machines with the same space bound.
It is straightforward to check that L ⊆NL ⊆P, and these three classes are
thought to be distinct. There are a number of nontrivial problems solvable in
L (for example see [Lipton and Zalcstein, 1977]) as well as problems known to
be in NL which are not believed to be in L (for example see [Savitch, 1973;
Jones, 1975]). Numerous problems in P are thought to lie outside of L or NL. For
example, one such problem is the circuit value problem, the problem of determining

Computational Complexity
503
the value of a Boolean circuit, given inputs to the circuit.
The circuit value
problem is one of many problems in P which is known to be P complete. These
are problems in P which are proved to be complete with respect to log-space
bounded reductions, reductions deﬁned analogously to polynomial time bounded
reduction in the previous section. Proving a P-complete problem is in L would
imply that L = P.
4.5
Oracles
Oracle results play a unique role in complexity theory. They are meta-mathematical
results delineating the limitations of proof techniques and indicating what results
might be possible to achieve and which are likely beyond our current reach. Oracle
results concern relativized computations. We say that a computation is carried
out “relative to an oracle set O” if the computation has access to the answers to
membership queries of O. That is, the computation can query the oracle O about
whether or not a string x is in O. The computation obtains the answer (in one
step) and proceeds with the computation, which may depend on the answer to the
oracle query.
The ﬁrst, and still most fundamental oracle results in complexity were carried
out by Baker, Gill and Solovay [1975]. They proved that there is an oracle reactive
to which P=NP and another oracle relative to which P and NP diﬀer.
What do these results say about the P vs NP question? They say little about
the actual answer to this question. The existence of an oracle making a statement
S true is simply a kind of consistency result about S. It says that the statement
is true in one particular model or “world” (that is, the oracle set itself).
As
such, we can conclude that a proof of the negation of S will not itself relativize to
any oracle. Thus, as many proof methods do relativize to every oracle, an oracle
result provides a limitation to the possible methods used to prove S and hence
are evidence that the result is, in this sense, hard. Oracle results have been most
useful in delineating theorems which are diﬃcult to prove (i.e., those which do
no relativize), from those which might more likely be settled by well-understood,
relativizing proof techniques. In particular, the Baker, Gill and Solovay results
concerning P and NP question indicate that a proof will be diﬃcult to come by,
as has indeed been the case.
Since 1978 numerous other oracle results have been proved. Techniques used
to achieve these results have become quite sophisticated and strong. For instance,
Fenner, Fortnow and Kurtz [1994] gave a relativized world where the isomorphism
conjecture holds where Kurtz, Mahaney and Royer [Kurtz et al., 1989] had showed
that it fails relative to most oracles. They were the culmination of a long series of
partial results addressing this question.
There are a few results in complexity that do not relativize, mostly relating to
interactive proof systems (see Section 6.1) but these tend to be the exception and
not the rule.

504
Lance Fortnow and Steven Homer
5
COUNTING CLASSES
Another way to study NP computations is to ask how many computations paths
in the computation lead to acceptance. For example, consider the satisﬁability
problem.
Given an instance of this problem, that is a propositional formula,
instead of asking if the formula has a solution (a truth assignment making the
formula true), ask how many such assignments there are. In 1979, Valiant [1979]
deﬁned the complexity class #P as the class of functions computing the number
of accepting paths of a nondeterministic Turing machine. Every #P function is
computable in polynomial space. Valiant used this class to capture the complexity
of the counting version of satisﬁability as well as other interesting problems such
as computing the permanent function.
Counting complexity has since played an important role in computational com-
plexity theory and theoretical computer science. The techniques used in counting
complexity have signiﬁcant applications in circuit complexity and in the series
of recent results on interactive proof systems. (See the next section.) The two
most important counting function classes are #P, described above, and GapP.
GapP consists of the class of functions which compute the diﬀerence between the
number of accepting paths and the number of rejecting paths of a nondeterminis-
tic Turing machine. For example, the function which tells, for any propositional
formula, computes the diﬀerence of the number of accepting and rejecting truth
assignments is in the class GapP.
Perhaps the two most important recent results in counting complexity are
Toda’s theorem [Toda, 1991] and the closure theorem of Beigel, Reingold and
Spielman’s [1995]. Toda’s theorem asserts that one can reduce any language in
the polynomial-time hierarchy to a polynomial time computation which uses a #P
function as an oracle. Hence, that in terms of complexity, hard functions in #P
lie above any problem in the polynomial time hierarchy. In 1994, Beigel, Reingold
and Spielman [1995] proved that PP is closed under union. This result solved
a longstanding open problem in this area, ﬁrst posed by Gill in 1977 [1977] in
the initial paper on probabilistic classes. It implies that PP is also closed under
intersection Those interested in further exploring counting classes and the power
of counting in complexity theory should consult the papers of Sch¨oning [1990] and
Fortnow [1997].
6
PROBABILISTIC COMPLEXITY
In 1977, Solovay and Strassen [1977] gave a new kind of algorithm for testing
whether a number is prime. Their algorithm ﬂipped coins to help search for a
counterexample to primality. They argued that if the number was not prime then
with very high conﬁdence a counterexample could be found.
This algorithm suggested that we should revise our notion of “eﬃcient compu-
tation”. Perhaps we should now equate the eﬃciently computable problems with
the class of problems solve in probabilistic polynomial time. A whole new area

Computational Complexity
505
of complexity theory was developed to help understand the power of probabilistic
computation.
Gill [1977] deﬁned the class BPP to capture this new notion. Adleman and
Manders [1977] deﬁned the class R that represented the set of problems with one-
sided randomness–the machine only accepts if the instance is guaranteed to be in
the language. The Solovay-Strassen algorithm puts compositeness in R.
Babai introduced the concept of a “Las Vegas” probabilistic algorithm that
always gives the correct answer and runs in expected polynomial time. This class
ZPP is equivalent to those problems with both positive and negative instances in
R. Adleman and Huang [1987] building on work of Goldwasser and Kilian [1999]
show that primality is in R and thus ZPP.
Early in the 21st century Agrawal, Kayal and Saxena [2002] would give a de-
terministic polynomial-time algorithm for primality. If this result was known in
the 70’s, perhaps the study of probabilistic algorithms would not have progressed
as quickly.
In 1983, Sipser [1983] showed that BPP is contained in the polynomial-time
hierarchy. G´acs (see [Sipser, 1983]) improves this result to show BPP is in the
second level of the hierarchy and Lautemann [1983] gives a simple proof of this
fact.
One can also consider probabilistic space classes.
Aleliunas, Karp, Lipton,
Lov´asz and Rackoﬀ[1979] show that undirected graph connectivity can be com-
puted in one-sided randomized logarithmic space, a class called RL. Similarly
one can deﬁne the classes BPL and ZPL. Borodin, Cook, Dymond, Ruzzo and
Tompa [1989] showed that undirected graph nonconnectivity also sits in RL and
thus ZPL.
In the early 21st century, Omer Reingold [2008] would show that
undirected graph connectivity sits in deterministic logarithmic space.
6.1
Interactive Proof Systems
One can think of the class NP as a proof system: An arbitrarily powerful prover
gives a proof that say a formula is satisﬁable. One can generalize this notion of
proof system by allowing probabilistic veriﬁcation of the proof. This yields the
complexity class MA. One can also consider interaction where the veriﬁer sends
messages based on her random coins. The bounded round version of this class
is AM and the unbounded round version is IP. The incredible power of these
interactive proof systems has led to several of the most surprising and important
recent results in computational complexity theory.
Babai [1985] deﬁned interactive proof systems to help classify some group ques-
tions. An alternative interactive proof system was deﬁned by Goldwasser, Micali
and Rackoﬀ[1989] as a basis for the cryptographic class zero-knowledge. Zero-
knowledge proof systems have themselves played a major role in cryptography.
The two models diﬀered on whether the prover could see the veriﬁer’s random
coins, but Goldwasser and Sipser [1989] showed the two models equivalent. Babai
and Moran [1988] showed that any bounded-round protocol needs only one ques-

506
Lance Fortnow and Steven Homer
tion from the veriﬁer followed by a response from the prover. F¨urer, Goldreich,
Mansour, Sipser and Zachos [1989] showed that one can assume that for positive
instances the prover can succeed with no error.
Goldreich, Micali and Wigderson [1991] show that the set of pairs of noniso-
morphic graphs has a bounded-round interactive proof system. Boppana, H˚astad
and Zachos [1987] show that if the complement of any NP-complete language has
bounded-round interactive proofs than the polynomial-time hierarchy collapses.
This remains the best evidence that the graph isomorphism problem is probably
not NP-complete.
In 1990, Lund, Fortnow, Karloﬀand Nisan [1992] showed that the comple-
ments of NP-complete languages have unbounded round interactive proof systems.
Shamir [1992] quickly extended their techniques to show that every language in
PSPACE has interactive proof system. Feldman [1986] had earlier shown that
every language with interactive proofs lies in PSPACE.
Interactive proofs are notable in that in general proofs concerning them do not
relativize, that is they are not true relative to every oracle. The classiﬁcation of
interactive proofs turned out not to be the end of the story but only the beginning
of a revolution connecting complexity theory with approximation algorithms. For
the continuation of this story we turn to probabilistically checkable proofs.
6.2
Probabilistically Checkable Proofs
In 1988, Ben-Or, Goldwasser, Kilian and Wigderson [1988] developed the mul-
tiprover interactive proof system. This model has multiple provers who cannot
communicate with each other or see the conversations each has with the veriﬁer.
This model allows the veriﬁer to play one prover oﬀanother.
Fortnow, Rompel and Sipser [1994] show this model is equivalent to probabilis-
tically checkable proofs, where the prover writes down a possibly exponentially
long proof that the veriﬁer spot checks in probabilistic polynomial time. They
also show that every language accepted by these proof systems lie in NEXP,
nondeterministic exponential time.
In 1990, Babai, Fortnow and Lund [1991] show the surprising converse–that
every language in NEXP has probabilistically checkable proofs. Babai, Fortnow,
Levin and Szegedy [1991a] scale this proof down to develop “holographic” proofs
for NP where, with a properly encoded input, the veriﬁer can check the correctness
of the proof in very short amount of time.
Feige, Goldwasser, Lov´asz, Safra and Szegedy [1996] made an amazing connec-
tion between probabilistically checkable proofs and the clique problem. By viewing
possible proofs as nodes of a graph, they showed that one cannot approximate the
size of a clique well without unexpected collapses in complexity classes.
In 1992, Arora, Lund, Motwani, Sudan and Szegedy [1998] building on work of
Arora and Safra [1998] showed that every language in NP has a probabilistically
checkable proof where the veriﬁer uses only a logarithmic number of random coins
and a constant number of queries to the proof.

Computational Complexity
507
The Arora et al. result has tremendous implications for the class MAXSNP
of approximation problems.
This class, developed by Papadimitriou and Yan-
nakakis [1991], has many interesting complete problems such as max-cut, vertex
cover, independent set, traveling salesman on an arbitrary metric space and max-
imizing the number of satisﬁable clauses of a formula.
Arora et. al. show that, unless P = NP, every MAXSNP-complete set does
not have a polynomial-time approximation scheme. For each of these problems
there is some constant δ > 1 such that they cannot be approximated within a
factor of δ unless P = NP.
Since these initial works on probabilistically checkable proofs, we have seen
a large number of outstanding papers improving the proof systems and getting
stronger hardness of approximation results. H˚astad [1997] gets tight results for
some approximation problems. Arora [1998], after failing to achieve lower bounds
for traveling salesman in the plane, has developed a polynomial-time approxima-
tion algorithm for this and related problems.
A series of results due to Cai, Condon, Lipton, Lapidot, Shamir, Feige and
Lov´asz [Cai et al., 1992; Cai et al., 1990; Cai et al., 1991; Feige, 1991; Lapidot
and Shamir, 1991; Feige and Lov´asz, 1992] have modiﬁed the protocol of Babai,
Fortnow and Lund [1991] to show that every language in NEXP has a two-prover,
one-round proof systems with an exponentially small error. This problem remained
so elusive because running these proof systems in parallel does not have the ex-
pected error reduction [Fortnow et al., 1994]. In 1995, Raz [1998] showed that the
error does go down exponentially when these proofs systems are run in parallel.
6.3
Derandomization
If you generate a random number on a computer, you do not get a truly random
value, but a pseudorandom number computed by some complicated function on
some small, hopefully random seed. In practice this usually works well so perhaps
in theory the same might be true. Many of the exciting results in complexity theory
in the 1980’s and 90’s consider this question of derandomization–how to reduce or
eliminate the number of truly random bits to simulate probabilistic algorithms.
The ﬁrst approach to this problem came from cryptography. Blum and Mi-
cali [1984] ﬁrst to show how to create randomness from cryptographically hard
functions. Yao [1990] showed how to reduce the number of random bits of any al-
gorithm based on any cryptographically secure one-way permutation.
H˚astad,
Impagliazzo, Levin and Luby [1999] building on techniques of Goldreich and
Levin [1989] and Goldreich, Krawczyk and Luby [1993] show that one can get
pseudorandomness from any one-way function.
Nisan and Wigderson [1994] take a diﬀerent approach. They show how to get
pseudorandomness based on a language hard against nonuniform computation.
Impagliazzo and Wigderson [1997] building on this result and Babai, Fortnow,
Nisan and Wigderson [1993] show that BPP equals P if there exists a language
in exponential time that cannot be computed by any subexponential circuit.

508
Lance Fortnow and Steven Homer
For derandomization of space we have several unconditional results. Nisan [1992]
gives general tools for derandomizing space-bounded computation. Among the ap-
plications, he gets a O(log2 n) space construction for universal traversal sequences
for undirected graphs.
Saks and Zhou [1999] show that every probabilistic logarithmic space algorithm
can be simulated in O(log3/2 n) deterministic space.
7
DESCRIPTIVE COMPLEXITY
Many of the fundamental concepts and methods of complexity theory have their
genesis in mathematical logic, and in computability theory in particular. This
includes the ideas of reductions, complete problems, hierarchies and logical de-
ﬁnability. It is a well-understood principle of mathematical logic that the more
complex a problem’s logical deﬁnition (for example, in terms of quantiﬁer alter-
nation) the more diﬃcult its solvability. Descriptive complexity aims to measure
the computational complexity of a problem in terms of the complexity of the log-
ical language needed to deﬁne it. As is often the case in complexity theory, the
issues here become more subtle and the measure of the logical complexity of a
problem more intricate than in computability theory. Descriptive complexity has
its beginnings in the research of Jones, Selman, Fagin [Jones and Selman, 1974;
Fagin, 1973; Fagin, 1974] and others in the early 1970’s. More recently descriptive
complexity has had signiﬁcant applications to database theory and to computer-
aided veriﬁcation.
The ground breaking theorem of this area is due to Fagin [1973]. It provided the
ﬁrst major impetus for the study of descriptive complexity. Fagin’s Theorem gives
a logical characterization of the class NP. It states that NP is exactly the class of
problems deﬁnable by existential second order Boolean formulas. This result, and
others that follow, show that natural complexity classes have an intrinsic logical
complexity.
To get a feel for this important idea, consider the NP-complete problem of 3
colorability of a graph. Fagin’s theorem says there is a second order existential
formula which holds for exactly those graphs which are 3-colorable. This formula
can be written as (∃A, B, C)(∀v)[(A(v)∨B(v)∨C(v))∧(∀w)(E(v, w) →¬(A(v)∧
A(w)) ∧¬(B(v) ∧B(w)) ∧¬(C(v) ∧C(w)))]. Intuitively this formula states that
every vertex is colored by one of three colors A, B, or C and no two adjacent
vertices have the same color. A graph, considered as a ﬁnite model, satisﬁes this
formula if and only if it is 3-colorable.
Fagin’s theorem was the ﬁrst in a long line of results which prove that complexity
classes can be given logical characterizations, often very simply and elegantly.
Notable among these is the theorem of Immerman and Vardi [Immerman, 1982;
Vardi, 1982] which captures the complexity of polynomial time. Their theorem
states that the class of problems deﬁnable in ﬁrst order logic with the addition of
the least ﬁxed point operator is exactly the complexity class P. Logspace can be
characterized along these same lines, but using the transitive closure (TC) operator

Computational Complexity
509
rather than least ﬁxed point. That is, nondeterministic logspace is the class of
problems deﬁnable in ﬁrst order logic with the addition of TC (see Immerman
[1988]).
And if one replaces ﬁrst order logic with TC with second order logic
with TC the result is PSPACE (see Immerman [1983]). Other, analogous results
in this ﬁeld go on to characterize various circuit and parallel complexity classes,
the polynomial time hierarchy, and other space classes, and even yield results
concerning counting classes.
The intuition provided by looking at complexity theory in this way has proved
insightful and powerful. In fact, one proof of the famous Immerman-Szelepcsenyi
Theorem [Immerman, 1988; Szelepcs´enyi, 1988] (that by Immerman) came from
these logical considerations. This theorem say that any nondeterministic space
class which contains logspace is closed under complement. An immediate con-
sequence is that the context sensitive languages are closed under complement,
answering a question which had been open for about 25 years.
To this point we have considered several of the most fully developed and fun-
damental areas of complexity theory. We now survey a few of the more central
topics in the ﬁeld dealing with other models of computation and their complexity
theory. These include circuit complexity, communication complexity and proof
complexity.
8
FINITE MODELS
8.1
Circuit Complexity
The properties and construction of eﬃcient Boolean circuits are of practical im-
portance as they are the building block of computers. Circuit complexity studies
bounds on the size and depth of circuits which compute a given Boolean func-
tions. Aside from their practical value, such bounds are closely tied to important
questions about Turing machine computations.
Boolean circuits are directed acyclic graphs whose internal nodes (or “gates”)
are Boolean functions, most often the “standard” Boolean functions, and, or and
not. In a circuit, the nodes of in-degree 0 are called input nodes and labeled with
input variables. The nodes with out-degree 0 are called output nodes. The value of
the circuit is computed in the natural way by giving values to the input variables,
applying the gates to these values, and computing the output values.
The size, s(C), of a circuit C is the number of gates it contains. The depth, d(C),
of a circuit C is the length of the longest path from an input to an output node.
A circuit with n inputs can be thought of as a recognizer of a set of strings
of length n, namely those which result in the circuit evaluating to 1. In order
to consider circuits as recognizing an inﬁnite set of strings, we consider circuit
families which are inﬁnite collections of circuits, Cn, one for each input length. In
this way a circuit family can recognize a language just as a Turing machine can.
A circuit family is a nonuniform model, the function taking n to Cn may not be
computable. A nonuniform circuit family can recognize noncomputable sets. We

510
Lance Fortnow and Steven Homer
can measure the size and depth of circuit families using asymptotic notation. So,
for example, we say that a circuit family has polynomial size if s(Cn) is O(p(n)),
for some polynomial p(n). Any language in P has polynomial size circuits. That
is, it is recognized by a circuit family which has polynomial size. And so proving
that some NP problem does not have polynomial size circuits would imply that
P ̸= NP.
Largely because of many such implications for complexity classes,
considerable eﬀort has been devoted to proving circuit lower bounds. However, to
this point this eﬀort has met with limited success.
In an early paper, Shannon [1949] showed that most Boolean functions require
exponential size circuits. This proof was nonconstructive and proving bounds on
speciﬁc functions is more diﬃcult. In fact, no non-linear lower bound is known for
the circuit size of a concrete function.
To get more positive results one needs to restrict the circuit families being
considered. This can be done by requiring some uniformity in the function mapping
n to Cn, or it can be done by restricting the size or depth of the circuits themselves.
For example, the class AC0 consists of those languages recognized by uniform,
constant depth, polynomial size circuits with and, or and not gates which allow
unbounded fan-in. One early and fundamental results, due to Furst, Saxe and
Sipser [1988] and Ajtai [1983] is that the parity function is not in AC0, and in
fact requires exponential size AC0-type circuits [Yao, 1990]. This immediately
implies that AC0 diﬀers from the class ACC of languages which have circuit
families made from AC0 circuits with the addition of Modm gates, with m ﬁxed
for the circuit family. It can also be shown to imply the existence of an oracle
separating the polynomial hierarchy from PSPACE.
It is also known that the classes ACC(p) are all distinct, where only Modp gates
are allowed, for p a prime. This was shown by Smolensky [1987] and Razborov
[1998]. ACC itself has resisted all lower bound techniques and in fact it is not
even know to be properly contained in NP.
Razborov [1985a] showed that clique does not have small monotone circuits, i.e.,
just AND and OR gates without negations. However, this result says more about
the limitations of monotone circuits as Razborov [1985] showed that the matching
problem, known to be in P, also does not have small monotone circuits.
8.2
Communication Complexity
Much of modern computer science deals with the speed and eﬃciency at which dig-
ital communication can take place. Communication complexity is an attempt to
model the eﬃciency and intrinsic complexity of communication between comput-
ers. It studies problems which model typical communication needs of computations
and attempts to determine the bounds on the amount of communication between
processors that these problems require.
The basic question of communication complexity is, how much information do
two parties need to exchange in order to carry out a computation? We assume
both parties have unlimited computational power.

Computational Complexity
511
For example, consider the case where both parties have n input bits and they
want to determine if there is a position i ≤n where the two bits in position i
match. It is not hard to see that the communication complexity of this problem
is n, as the n bits are independent and in the worst case, all n bits of one party
have to be transmitted to the other.
Now consider the problem of computing the parity of a string of bits where 1/2
of the bits are given to party 1 and the other half to party 2. In this case, party 1
need only compute the parity of her bits and send this parity to party 2 who can
then compute the parity of the whole bit string. So in this case the communication
complexity is a single bit.
Communication complexity has provided upper and lower bounds for the com-
plexity of many fundamental communication problems. It has clariﬁed the role
which communication plays in distributed and parallel computation as well as in
the performance of VLSI circuits. It also applies and has had an impact on the
study of interactive protocols. For a good survey of the major results in this ﬁeld,
consult Nisan and Kushelevitz [1996].
8.3
Proof Complexity
The class NP can be characterized as those problems which have short, easily ver-
iﬁed membership proofs. Dual to NP-complete problems, like SAT, are co−NP-
complete problems, such as TAUT (the collection of propositional tautologies).
TAUT is not known to have short, easily veriﬁed membership proofs, and in fact
if it did then NP = co−NP (see Cook and Reckhow [1973]). Proof complexity
studies the lengths of proofs in propositional logic and the connections between
propositional proofs and computational complexity theory, circuit complexity and
automated theorem proving. In the last decade there have been signiﬁcant ad-
vances in lower bounds for propositional proof complexity as well as in the study
of new and interesting proof systems.
Cook and Reckhow [1973] were the ﬁrst to make the notion of a proposi-
tional proof system precise. They realized that to do this they needed to spec-
ify exactly what a proof is and to give a general format for presenting and eﬃ-
ciently verifying a proof p. They deﬁned a propositional proof system S to be a
polynomial-time computable predicate, R, such that for all propositional formu-
las, F, F ∈TAUT
⇐⇒∃p R(F, p). The complexity of S is then deﬁned to be
the smallest function f : N −→N which bounds the lengths of the proofs of S
as a function of the lengths of the tautologies being proved. Eﬃcient proof sys-
tems, those with complexity bounded by some polynomial, are called polynomial-
bounded proof systems.
Several natural proof systems have been deﬁned and their complexity and rela-
tionship explored. Among the most studied are Frege and extended-Frege Proof
systems [Urquhart, 1987] and [Krajicek and Pudlak, 1989], refutation systems,
most notably resolution [Robinson, 1965] and circuit based proof systems [Ajtai,
1983] and [Buss, 1987]. We brieﬂy discuss the complexity of resolution systems

512
Lance Fortnow and Steven Homer
here, but see Beame and Pitassi [1998] for a nice overview of results concerning
these other proof systems.
Resolution proof systems are the most well-studied model. Resolution is a very
restricted proof system and so has provided the setting for the ﬁrst lower bound
proofs. Resolution proof systems are refutation systems where a statement D is
proved by assuming its negation and deriving a contradiction from this negation.
In a resolution proof system there is a single rule of inference, resolution, which is a
form of cut. In its propositional form it says that if F ∨x and G∨¬x are true then
F ∨G follows. A restricted form of resolution, called regular resolution, was proved
to have a superpolynomial lower bound by Tseitin [1968] on certain tautologies
representing graph properties. The ﬁrst superpolynomial lower bound for general
resolution was achieved by Haken [1989] who in 1985 proved an exponential lower
bound for the pigeonhole principle. Since then several other classes of tautologies
have been shown to require superpolynomial long resolution proofs.
9
QUANTUM COMPUTING
The mark of a good scientiﬁc ﬁeld is its ability to adapt to new ideas and new
technologies. Computational complexity reaches this ideal. As we have developed
new ideas of probabilistic and parallel computation, the complexity community
has not thrown out the previous research, rather they have modiﬁed the exist-
ing models to ﬁt these new ideas and have shown how to connect the power of
probabilistic and parallel computation to our already rich theory. Most recently
complexity theorists have begun to analyze the computational power of machines
based on quantum mechanics.
In 1982, Richard Feynman [1982], the physicist, noted that current computer
technology could not eﬃciently simulate quantum systems. He suggested the pos-
sibility that computers built on quantum mechanics might be able to perform this
task. David Deutch [1985] in 1985 developed a theoretical computation model
based on quantum mechanics and suggested that such a model could eﬃciently
compute problems not computable by a traditional computer.
Two unexpected quantum algorithms have provided the central motivation for
studying quantum computation: Shor’s [1997] procedure for factoring integers
in polynomial time on a quantum computer and Grover’s [1996] technique for
searching a database of n elements in O(√n) time.
We know surprisingly little about the computational complexity of quantum
computing.
Bernstein and Vazirani [1997] give a formal deﬁnition of the class
BQP of language eﬃciently computable by quantum computers. They show the
surprising robustness of BQP which remains unscathed under variations of the
model such as restricting to a small set of rational amplitudes, allowing quantum
subroutines and a single measurement at the end of the computation.
Bernstein and Vazirani show that BQP is contained in PSPACE. Adleman,
DeMarrais and Huang [1997] show that BQP is contained in the counting class
PP. Bennett, Bernstein, Brassard and Vazirani [1997] give a relativized world

Computational Complexity
513
where NP is not contained in BQP. We do not know any nonrelativized conse-
quences of NP in BQP or if BQP lies in the polynomial-time hierarchy.
What about quantum variations of NP and interactive proof systems? Fenner,
Green, Homer and Pruim [1999] consider the class consisting of the languages L
such that for some polynomial-time quantum Turing machine, x is in L when
M(x) accepts with positive probability. They show the equivalence of this class
to the counting class co−C=P.
Watrous [1999] shows that every language in PSPACE has a bounded-round
quantum interactive proof system. Later Watrous with Jain, Ji and Upadhyay [2011]
would show that quantum interactive proofs accept the same languages as PSPACE.
We have seen quite a bit of progress on quantum decision tree complexity. In
this model we count the number of queries made to a black-box database of size
n. Quantum queries can be made in superposition.
Deutsch and Josza [1992] gave an early example of a simple function that can
be solved with one query quantumly but requires Ω(n) queries deterministically or
probabilistically with no error. Bernstein and Vazirani [1997] give the ﬁrst example
of a problem that can be solved with polynomial number of queries quantumly
but requires a superpolynomial number of queries probabilistically with bounded
error. Simon [1997] gives another example with an exponential gap. Brassard and
Høyer [1997] gave a zero-error quantum algorithms for Simon’s problem. Shor’s
factoring algorithm [Shor, 1997] can be viewed as an extension of Simon’s problem
that ﬁnds the period in a periodic black-box function.
All of these examples require a promise, i.e., restricting the allowable inputs to
be tested. Fortnow and Rogers [1999] and Beals, Buhrman, Cleve, Mosca and de
Wolf [1998] show that a promise is necessary to get a superpolynomial separation.
10
FUTURE DIRECTIONS
Despite the plethora of exciting results in computational complexity over the last
four decades of the 20th century, true complexity class separations have remained
beyond our grasp. Tackling these problems, especially showing a separation of P
and NP, is our greatest challenge for the future.
How will someone prove that P and NP diﬀer? As of this writing, we have
no serious techniques that could help separate these classes. What kind of future
ideas could lead us to answer this diﬃcult question? Some possibilities:
• A unexpected connection to other areas of mathematics such as algebraic
geometry or higher cohomology. Perhaps even an area of mathematics not
yet developed.
Perhaps someone will develop a whole new direction for
mathematics in order to handle the P versus NP question.
• New techniques to prover lower bounds for circuits, branching programs
and/or proof systems in models strong enough to give complexity class sep-
arations.

514
Lance Fortnow and Steven Homer
• A new characterization of P or NP that makes separation more tractable.
• A clever twist on old-fashioned diagonalization, still the only techniques that
has given any lower bounds on complexity classes.
Complexity theory will progress in areas beyond class separation. Still, quite a
few interesting questions remain in many areas, even basic questions in quantum
computational complexity remain. Complexity theorists will continue to forge new
ground and ﬁnd new and exciting results in these directions.
As with probabilistic, parallel and quantum complexity, new models of com-
putation will be developed. Computational complexity theorists will be right on
top of these developments leading the way to understand the inherent eﬃcient
computational power of these models.
We have seen many books and popular news stories about the other “complex-
ity”, complex systems that occur in many aspects of society and nature such as
ﬁnancial markets, the internet, biological systems, the weather and debatably even
physical systems. This theory suggests that such systems have a very simple set of
rules that when combined produce quite a complex behavior. Computer programs
exhibit a very similar behavior. We will see computational complexity techniques
used to help understand the eﬃciency of the complex behavior of these systems.
Finally, computational complexity will continue to have the Big Surprise. No
one can predict the next big surprise but it will happen as it always does.
Let us end this survey with a quote from Juris Hartmanis’ notebook (see [Hart-
manis, 1981]) in his entry dated December 31, 1962
This was a good year.
This was a good forty years and complexity theory is only getting started.
11
FURTHER READING
There have been several articles on various aspects of the history of complexity
theory, many of which we have used as source material for this article. We give a
small sampling of pointers here:
• [Fortnow, 2013] The ﬁrst author has a recent popular science book on the P
versus NP problem with a chapter on the early history of P and NP.
• [Hartmanis, 1981] Juris Hartmanis reminisces on the beginnings of complex-
ity theory.
• [Trakhtenbrot, 1984] Boris Trakhtenbrot describes the development of NP-
completeness from the Russian perspective.
• [Sipser, 1992] Michael Sipser gives a historical account of the P versus NP
question including a copy and translation of G¨odel’s historic letter to von
Neumann.

Computational Complexity
515
• [Garey and Johnson, 1979] Michael Garey and David Johnson give a “ter-
minological history” of NP-completeness and a very readable account of the
basic theory of NP-completeness.
• The collection of papers edited by Hochbaum [1995] is a good overview of
progress made in approximating solutions to NP-hard problems.
• Consult the book by Greenlaw, Hoover and Ruzzo [1995] to learn more of
complexity theory within P and for many more P-complete problems.
• The Turing award lectures of Cook [1983], Karp [1986], Hartmanis [1994] and
Stearns [1994] give interesting insights into the early days of computational
complexity.
• The textbook of Homer and Selman [2000] contains a careful development of
the deﬁnitions and basic concepts of complexity theory, and proofs of many
central facts in this ﬁeld.
• The complexity columns of SIGACT news and the Bulletin of the EATCS
have had a number of excellent surveys on many of the areas described in
this article.
• The two collections Complexity Theory Retrospective [Selman, 1988] and
Complexity Theory Retrospective II [Hemaspaandra and Selman, 1997] con-
tain some excellent recent surveys of several of the topics mentioned here.
ACKNOWLEDGMENTS
Earlier versions of this survey have appeared in the Bulletin of the European
Association for Theoretical Computer Science (volume 80, June 2003) and as an
invited presentation at the 17th Annual Conference on Computational Complexity
in 2002.
The authors would like to thank their colleagues, far too numerous to mention,
whom we have had many wonderful discussions about complexity over the past few
decades. Many of these discussions have aﬀected how we have produced various
aspects of this article.
BIBLIOGRAPHY
[Adleman et al., 1997] L. Adleman, J. DeMarrais, and M. Huang.
Quantum computability.
SIAM Journal on Computing, 26(5):1524–1540, 1997.
[Adleman and Huang, 1987] L. Adleman and M. Huang. Recognizing primes in random poly-
nomial time. In Proceedings of the 19th ACM Symposium on the Theory of Computing, pages
462–469. ACM, New York, 1987.
[Ajtai, 1983] M. Ajtai. σ1
1 formulea on ﬁnite structures. Journal of Pure and Applied Logic,
24:1–48, 1983.

516
Lance Fortnow and Steven Homer
[Aleliunas et al., 1979] R. Aleliunas, R. Karp, R. Lipton, L. Lov´asz, and C. Rackoﬀ. Random
walks, universal traversal sequences, and the complexity of maze problems. In Proceedings of
the 20th IEEE Symposium on Foundations of Computer Science, pages 218–223. IEEE, New
York, 1979.
[Agrawal et al., 2002] M. Agrawal, N. Kayal, and N. Saxena. PRIMES is in P. Unpublished
manuscript, Indian Institute of Technology Kanpur, 2002.
[Arora et al., 1998] S. Arora, C. Lund, R. Motwani, M. Sudan, and M. Szegedy. Proof veriﬁca-
tion and the hardness of approximation problems. Journal of the ACM, 45(3):501–555, May
1998.
[Adleman and Manders, 1977] L. Adleman and K. Manders.
Reducibility, randomness, and
intractibility. In Proceedings of the 9th ACM Symposium on the Theory of Computing, pages
151–163. ACM, New York, 1977.
[Arora, 1998] S. Arora. Polynomial time approximation schemes for Euclidean traveling sales-
man and other geometric problems. Journal of the ACM, 45(5):753–782, September 1998.
[Ambox-Spies, 1989] K. Ambos-Spies. On the relative complexity of hard problems for com-
plexity classes without complete problems. TCS, 64:43–61, 1989.
[Arora and Safra, 1998] S. Arora and S. Safra. Probabilistic checking of proofs: A new charac-
terization of NP. Journal of the ACM, 45(1):70–122, January 1998.
[Babai, 1985] L. Babai. Trading group theory for randomness. In Proceedings of the 17th ACM
Symposium on the Theory of Computing, pages 421–429. ACM, New York, 1985.
[Bennett et al., 1997] C. Bennett, E. Bernstein, G. Brassard, and U. Vazirani. Strengths and
weaknesses of quantum computing. SIAM Journal on Computing, 26(5):1510–1523, 1997.
[Beals et al., 1998] R. Beals, H. Buhrman, R. Cleve, M. Mosca, and R. de Wolf.
Quantum
lower bounds by polynomials. In Proceedings of the 39th IEEE Symposium on Foundations
of Computer Science, pages 352–361. IEEE, New York, 1998.
[Borodin et al., 1989] A. Borodin, S. A. Cook, P. W. Dymond, W. L. Ruzzo, and M. Tompa.
Two applications of inductive counting for complementaion problems. SIAM J. Computing,
13:559–578, 1989.
[Babai et al., 1991] L. Babai, L. Fortnow, and C. Lund. Non-deterministic exponential time has
two-prover interactive protocols. Computational Complexity, 1(1):3–40, 1991.
[Babai et al., 1991a] L. Babai, L. Fortnow, L. Levin, and M. Szegedy. Checking computations
in polylogarithmic time.
In Proceedings of the 23rd ACM Symposium on the Theory of
Computing, pages 21–31. ACM, New York, 1991.
[Babai et al., 1993] L. Babai, L. Fortnow, N. Nisan, and A. Wigderson. BPP has subexponential
simulations unless EXPTIME has publishable proofs. Computational Complexity, 3:307–318,
1993.
[Ben-Or et al., 1988] M. Ben-Or, S. Goldwasser, J. Kilian, and A. Wigderson.
Multi-prover
interactive proofs: How to remove intractability assumptions.
In Proceedings of the 20th
ACM Symposium on the Theory of Computing, pages 113–131. ACM, New York, 1988.
[Baker et al., 1975] T. Baker, J. Gill, and R. Solovay. Relativizations of the P = NP question.
SIAM Journal on Computing, 4(4):431–442, 1975.
[Berman and Hartmanis, 1977] L. Berman and H. Hartmanis. On isomorphisms and density of
NP and other complete sets. SIAM Journal on Comput., 6:305–322, 1977.
[Brassard and Høyer, 1997] G. Brassard and P. Høyer.
An exact quantum polynomial-time
algorithm for Simon’s problem. In Proceedings of the 5th Israeli Symposium on Theory of
Computing and Systems (ISTCS’97), pages 12–23. IEEE, New York, 1997.
[Boppana et al., 1987] R. Boppana, J. H˚astad, and S. Zachos. Does co-NP have short interactive
proofs? Information Processing Letters, 25(2):127–132, 1987.
[Blum, 1967] M. Blum. A machine-independent theory of the complexity of recursive functions.
Journal of the ACM, 14(2):322–336, April 1967.
[Blum and Micali, 1984] M. Blum and S. Micali.
How to generate cryptographically strong
sequences of pseudo-random bits. SIAM Journal on Computing, 13:850–864, 1984.
[Babai and Moran, 1988] L. Babai and S. Moran. Arthur-Merlin games: a randomized proof
system, and a hierarchy of complexity classes. Journal of Computer and System Sciences,
36(2):254–276, 1988.
[Borodin, 1972] A. Borodin. Computational complexity and the existence of complexity gaps.
Journal of the ACM, 19(1):158–174, January 1972.
[Beame and Pitassi, 1998] P. Beame and T. Pitassi.
Propositional proof complexity:
Past,
present and future. Bull. of the EATCS, 65:66–89, 1998.

Computational Complexity
517
[Beigel et al., 1995] R. Beigel, N. Reingold, and D. Spielman. PP is closed under intersection.
Journal of Computer and System Sciences, 50(2):191–202, 1995.
[Buss, 1987] S. Buss. Polynomial size proofs of the pigeon hole principle. Journal of Symbolic
Logic, 57:916–927, 1987.
[Bernstein and Vazirani, 1997] E. Bernstein and U. Vazirani.
Quantum complexity theory.
SIAM Journal on Computing, 26(5):1411–1473, 1997.
[Cantor, 1874] G. Cantor.
Ueber eine Eigenschaft des Inbegriﬀes aller reellen algebraischen
Zahlen. Crelle’s Journal, 77:258–262, 1874.
[Cai et al., 1990] J. Cai, A. Condon, and R. Lipton. On bounded round multi-prover interactive
proof systems. In Proceedings of the 5th IEEE Structure in Complexity Theory Conference,
pages 45–54. IEEE, New York, 1990.
[Cai et al., 1991] J. Cai, A. Condon, and R. Lipton. PSPACE is provable by two provers in one
round. In Proceedings of the 6th IEEE Structure in Complexity Theory Conference, pages
110–115. IEEE, New York, 1991.
[Cai et al., 1992] J. Cai, A. Condon, and R. Lipton.
On games of incomplete information.
Theoretical Computer Science, 103(1):25–38, 1992.
[Chandra et al., 1981] A. Chandra, D. Kozen, and L. Stockmeyer. Alternation. Journal of the
ACM, 28:114–133, 1981.
[Clay Math. Inst., 2000] Clay
Mathematics
Institute.
Millennium
prize
problems.
http://www.claymath.org/prizeproblems/, 2000.
[Cobham, 1964] A. Cobham. The intrinsic computational diﬃculty of functions. In Proceedings
of the 1964 International Congress for Logic, Methodology, and Philosophy of Science, pages
24–30. North-Holland, Amsterdam, 1964.
[Cook, 1971] S. Cook. The complexity of theorem-proving procedures. In Proc. 3rd ACM Symp.
Theory of Computing, pages 151–158, 1971.
[Cook, 1973] S. Cook. A hierarchy for nondeterministic time complexity. Journal of Computer
and System Sciences, 7(4):343–353, August 1973.
[Cook, 1983] S. Cook. An overview of computational complexity. Communications of the ACM,
26(6):400–408, June 1983.
[Cook and Reckhow, 1973] S. Cook and R. Reckhow. Time bounded random access machines.
JCSS, 7(4):354–375, 1973.
[Deutsch, 1985] D. Deutsch. Quantum theory, the Church-Turing principle and the universal
quantum computer. Proceedings of the Royal Society of London A, 400:97, 1985.
[Deutsch and Jousza, 1992] D. Deutsch and R. Jousza. Rapid solution of problems by quantum
computation. Proceedings of the Royal Society of London A, 439:553, 1992.
[Edmonds, 1965] J. Edmonds. Maximum matchings and a polyhedron with 0,1-vertices. Journal
of Research at the National Bureau of Standards (Section B), 69B:125–130, 1965.
[Edmonds, 1965a] J. Edmonds. Paths, trees and ﬂowers. Canadian Journal of Mathematics,
17:449–467, 1965.
[Fagin, 1973] R. Fagin. Contributions to the model theory of ﬁnite structures. Ph.D. Thesis,
U.C. Berkeley, 1973.
[Fagin, 1974] R. Fagin. Generalized ﬁrst-order spectra and polynomial-time recognizable sets.
In Complexity of Computation (ed. R. Karp), pages 27–41. SIAM-AMS Proc. 7, 1974.
[Feige, 1991] U. Feige. On the success probability of the two provers in one round proof systems.
In Proceedings of the 6th IEEE Structure in Complexity Theory Conference, pages 116–123.
IEEE, New York, 1991.
[Feldman, 1986] P. Feldman. The optimum prover lives in PSPACE. Manuscript, 1986.
[Feynman, 1982] R. Feynman. Simulating physics with computers. International Journal of
Theoretical Physics, 21:467, 1982.
[Fenner et al., 1994] S. Fenner, L. Fortnow, and S. Kurtz. Gap-deﬁnable counting classes. Jour-
nal of Computer and System Sciences, 48(1):116–148, 1994.
[Feige et al., 1988] U. Feige, A. Fiat, and A. Shamir. Zero knowledge proofs of identity. Journal
of Cryptology, 1(2):77–94, 1988.
[Fenner et al., 1999] S. Fenner, F. Green, S. Homer, and R. Pruim. Determining acceptance
possibility for a quantum computation is hard for PH. Proceedings of the Royal Society of
London, 455:3953–3966, 1999.
[Feige et al., 1996] U. Feige, S. Goldwasser, L. Lov´asz, S. Safra, and M. Szegedy. Interactive
proofs and the hardness of approximating cliques. Journal of the ACM, 43(2):268–292, March
1996.

518
Lance Fortnow and Steven Homer
[F¨urer et al., 1989] M. F¨urer, O. Goldreich, Y. Mansour, M. Sipser, and S. Zachos. On com-
pleteness and soundness in interactive proof systems. In S. Micali, editor, Randomness and
Computation, volume 5 of Advances in Computing Research, pages 429–442. JAI Press, Green-
wich, 1989.
[Feige and Lov´asz, 1992] U. Feige and L. Lov´asz. Two-prover one-round proof systems: Their
power and their problems. In Proceedings of the 24th ACM Symposium on the Theory of
Computing, pages 733–744. ACM, New York, 1992.
[Fortnow, 1997] L. Fortnow. Counting complexity. In In Lane Hemaspaandra and Alan Selman,
editor, Complexity Theory Retrospective II, pages 81–107. Springer, New York, 1997.
[Fortnow, 2013] L. Fortnow. The Golden Ticket: P, NP, and the Search for the Impossible.
Princeton University Press, 2013.
[Fortnow and Rogers, 1999] L. Fortnow and J. Rogers.
Complexity limitations on quantum
computation. Journal of Computer and System Sciences, 59(2):240–252, 1999.
[Fortnow et al., 1994] L. Fortnow, J. Rompel, and M. Sipser.
On the power of multi-prover
interactive protocols. Theoretical Computer Science A, 134:545–557, 1994.
[Gill, 1977] J. Gill. Computational complexity of probabilistic complexity classes. SIAM Journal
on Computing, 6:675–695, 1977.
[Garey and Johnson, 1979] M. Garey and D. Johnson. Computers And Intractability: A Guide
To The Theory of NP-Completeness. W. H. Freeman, San Francisco, 1979.
[Goldwasser and Kilian, 1999] S. Goldwasser and J. Kilian.
Primality testing using elliptic
curves. Journal of the ACM, 46(4):450–472, July 1999.
[Goldreich et al., 1993] O. Goldreich, H. Krawczyk, and M. Luby. On the existence of pseudo-
random generators. SIAM Journal on Computing, 22(6):1163–1175, December 1993.
[Goldreich and Levin, 1989] O. Goldreich and L. Levin. A hard-core predicate for all one-way
functions. In Proceedings of the 21st ACM Symposium on the Theory of Computing, pages
25–32. ACM, New York, 1989.
[Goldwasser et al., 1989] S. Goldwasser, S. Micali, and C. Rackoﬀ. The knowledge complexity
of interactive proof-systems. SIAM Journal on Computing, 18(1):186–208, 1989.
[Goldreich et al., 1991] O. Goldreich, S. Micali, and A. Wigderson. Proofs that yield nothing
but their validity or all languages in NP have zero-knowledge proof systems. Journal of the
ACM, 38(3):691–729, 1991.
[Grover, 1996] L. Grover. A fast quantum mechanical algorithm for database search. In Pro-
ceedings of the 28th ACM Symposium on the Theory of Computing, pages 212–219. ACM,
New York, 1996.
[Goldwasser and Sipser, 1989] S. Goldwasser and M. Sipser. Private coins versus public coins
in interactive proof systems. In S. Micali, editor, Randomness and Computation, volume 5 of
Advances in Computing Research, pages 73–90. JAI Press, Greenwich, 1989.
[Hartmanis, 1981] J. Hartmanis. Observations about the development of theoretical computer
science. Annals of the History of Computing, 3(1):42–51, 1981.
[Hartmanis, 1982] J. Hartmanis. A note on natural complete sets and godel numberings. TCS,
17:75–89, 1982.
[Hartmanis, 1986] J. Hartmanis. G¨odel, Von neumann and the P=?NP problem. In Current
Trends in Theoretical Computer Science, pages 445–450. World Scientiﬁc Press, New York,
1986.
[Hartmanis, 1994] J. Hartmanis. Turing Award Lecture: On computational complexity and the
nature of computer science. Communications of the ACM, 37(10):37–43, October 1994.
[H˚astad, 1989] J. H˚astad. Almost optimal lower bounds for small depth circuits. In S. Micali,
editor, Randomness and Computation, volume 5 of Advances in Computing Research, pages
143–170. JAI Press, Greenwich, 1989.
[H˚astad, 1997] J. H˚astad. Some optimal inapproximabiity results. In Proceedings of the 29th
ACM Symposium on the Theory of Computing, pages 1–10. ACM, New York, 1997.
[Hartmanis and Baker, 1975] J. Hartmanis and T. Baker.
On simple godel numberings and
translations. SIAM Journal on Computing, 4:1–11, 1975.
[Hartmanis and Berman, 1978] J. Hartmanis and L. Berman. On polynomial time isomorphisms
and some new complete sets. JCSS, 16:418–422, 1978.
[H˚astad et al., 1999] J. H˚astad, R. Impagliazzo, L. Levin, and M. Luby. A pseudorandom gen-
erator from any one-way function. SIAM Journal on Computing, 28(4):1364–1396, August
1999.

Computational Complexity
519
[Hochbaum, 1995] D. Hochbaum.
Approximation Algorithms for NP-Hard Problems.
PSW
Publishing Company, Boston, 1995.
[Hartmanis and Stearns, 1965] J. Hartmanis and R. Stearns. On the computational complexity
of algorithms. Transactions of the American Mathematical Society, 117:285–306, 1965.
[Hennie and Stearns, 1966] F. Hennie and R. Stearns. Two-tape simulation of multitape Turing
machines. Journal of the ACM, 13(4):533–546, October 1966.
[Hemaspaandra and Selman, 1997] L. Hemaspaandra and A. Selman. Complexity Theory Ret-
rospective II. Springer, New York, 1997.
[Homer and Selman, 2000] S. Homer and A. Selman. Computability and Complexity Theory.
Springer, 2000.
[Ibarra, 1972] O. Ibarra. A note concerning nondeterministic tape complexities. Journal of the
ACM, 19(4):608–612, 1972.
[Immerman, 1982] N. Immerman. Relational queries computable in polynomial time. In Proc.
14th Symposium on Theory of Computation, pages 147–152. ACM Press, 1982.
[Immerman, 1983] N. Immerman. Languages which capture complexity classes. In Proc. 15th
Symposium on Theory of Computation, pages 760–778. ACM Press, 1983.
[Immerman, 1988] N. Immerman.
Nondeterministic space is closed under complementation.
SIAM Journal on Computing, 17(5):935–938, 1988.
[Impagliazzo and Wigderson, 1997] R. Impagliazzo and A. Wigderson. P = BPP if E requires
exponential circuits: Derandomizing the XOR lemma. In Proceedings of the 29th ACM Sym-
posium on the Theory of Computing, pages 220–229. ACM, New York, 1997.
[Jain et al., 2011] Rahul Jain, Zhengfeng Ji, Sarvagya Upadhyay, and John Watrous. Qip =
pspace. J. ACM, 58(6):30:1–30:27, December 2011.
[Jones, 1975] N. Jones. Space-bounded reducibility among combinatorial problems. Journal of
Computer and System Sciences, 11:68–85, 1975.
[Jones and Selman, 1974] N. Jones and A. Selman. Turing machines and the spectra of ﬁrst-
order formulae. Journal Symbolic Logic, 39:139–150, 1974.
[Karp, 1972] R. Karp. Reducibility among combinatorial problems. In Complexity of Computer
Computations, pages 85–104. Plenum Press, New York, 1972.
[Karp, 1986] R. Karp.
Combinatorics, complexity and randomness.
Communications of the
ACM, 29(2):98–109, February 1986.
[Kurtz et al., 1987] S. Kurtz, S. Mahaney, and J. Royer. Progress on collapsing degrees. In
Proc. Structure in Complexity Theory Second Annual Conference, pages 126–131, 1730 Mas-
sachusetts Avenue, N.W., Washington, D.C. 20036-1903, 1987. Computer Society Press of the
IEEE.
[Kurtz et al., 1989] S. Kurtz, S. Mahaney, and J. Royer.
The isomorphism conjecture fails
relative to a random oracle (extended abstract). In ACM Symposium on Theory of Computing,
pages 157–166, 1989.
[Kushilevitz and Nisan, 1996] E. Kushilevitz and N. Nisan. Communication Complexity. Cam-
bridge University Press, Cambridge, 1996.
[Krajicek and Pudlak, 1989] J. Krajicek and P. Pudlak. Propositional proof systems, the con-
sistency of ﬁrst order theories and the complexity of computation. Journal of Symbolic Logic,
53(3):1063–1079, 1989.
[Lautemann, 1983] C. Lautemann. BPP and the polynomial hierarchy. Information Processing
Letters, 17(4):215–217, 1983.
[Levin, 1973] L. Levin.
Universal sorting problems.
Problems of Information Transmission,
9:265–266, 1973. English translation of original in Problemy Peredaci Informacii.
[Lund et al., 1992] C. Lund, L. Fortnow, H. Karloﬀ, and N. Nisan.
Algebraic methods for
interactive proof systems. Journal of the ACM, 39(4):859–868, 1992.
[Lapidot and Shamir, 1991] D. Lapidot and A. Shamir. Fully parallelized multi prover protocols
for NEXP-time. In Proceedings of the 32nd IEEE Symposium on Foundations of Computer
Science, pages 13–18. IEEE, New York, 1991.
[Lipton and Zalcstein, 1977] R. Lipton and E. Zalcstein. Word problems solvable in logspace.
Journal of the ACM, 3:522–526, 1977.
[Mahaney, 1982] S. Mahaney. Sparse complete sets for NP: solution of a conjecture of Berman
and Hartmanis. Journal of Comput. System Sci., 25:130–143, 1982.
[McCreight and Meyer, 1969] E. McCreight and A. Meyer.
Classes of computable functions
deﬁned by bounds on computation.
In Proceedings of the First ACM Symposium on the
Theory of Computing, pages 79–88. ACM, New York, 1969.

520
Lance Fortnow and Steven Homer
[Meyer and Stockmeyer, 1972] A. Meyer and L. Stockmeyer. The equivalence problem for reg-
ular expressions with squaring requires exponential space. In Proc. of the 13th IEEE Sympo-
sium on Switching and Automata Theory, pages 125–129, 1730 Massachusetts Avenue, N.W.,
Washington, D.C. 20036-1903, 1972. Computer Society Press of the IEEE.
[Mahaney and Young, 1985] S. Mahaney and P. Young. Orderings of polynomial isomorphism
types. Theor. Comput. Sci., 39(2):207–224, August 1985.
[Myhill, 1955] J. Myhill. Creative sets. Zeitschrift f¨ur Mathematische Logik und Grundlagen
der Mathematik, 1:97–108, 1955.
[Myhill, 1960] J. Myhill. Linear bounded automata. Technical Note 60–165, Wright-Patterson
Air Force Base, Wright Air Development Division, Ohio, 1960.
[Nisan, 1992] N. Nisan. Pseudorandom generators for space-bounded computation. Combina-
torica, 12(4):449–461, 1992.
[Nisan and Wigderson, 1994] N. Nisan and A. Wigderson. Hardness vs. randomness. Journal
of Computer and System Sciences, 49:149–167, 1994.
[Papadimitriou and Yannakakis, 1991] C. Papadimitriou and M. Yannakakis. Optimization, ap-
proximation, and complexity classes. Journal of Computer and System Sciences, 43:425–440,
1991.
[Rabin, 1963] M. Rabin. Real time computation. Israel Journal of Mathematics, 1:203–211,
1963.
[Razborov, 1985] A. Razborov. Lower bounds of monotone complexity of the logical permanent
function. Mathematical Notes of the Academy of Sciences of the USSR, 37:485–493, 1985.
[Razborov, 1985a] A. Razborov. Lower bounds on the monotone complexity of some boolean
functions. Doklady Akademii Nauk SSSR, 281(4):798–801, 1985. In Russian. English Trans-
lation in [Razborov, 1985b].
[Razborov, 1985b] A. Razborov. Lower bounds on the monotone complexity of some boolean
functions. Soviet Mathematics–Doklady, 31:485–493, 1985.
[Raz, 1998] R. Raz. A parallel repetition theorem. SIAM Journal on Computing, 27(3):763–803,
June 1998.
[Reingold, 2008] Omer Reingold. Undirected connectivity in log-space. J. ACM, 55(4):17:1–
17:24, September 2008.
[Ruby and Fischer, 1965] S. Ruby and P. Fischer. Translational methods and computational
complexity. In Proceedings of the Sixth Annual Symposium on Switching Circuit Theory and
Logical Design, pages 173–178, New York, 1965. IEEE.
[Hoover et al., 1995] H. Hoover R. Greenlaw and W. Ruzzo. Limits to Parallel Computation:
P-Completeness Theory. Oxford University Press, Oxford, 1995.
[Robinson, 1965] J.A. Robinson. A machine oriented logic based on resolution. Journal of the
ACM, 12(1):23–41, 1965.
[Savitch, 1970] W. Savitch.
Relationship between nondeterministic and deterministic tape
classes. Journal of Computer and System Sciences, 4:177–192, 1970.
[Savitch, 1973] W. Savitch. Maze recognizing automata and nondeterministic tape complexity.
JCSS, 7:389–403, 1973.
[Schoning, 1990] U. Schoning. The power of counting. In In Alan Selman, editor, Complexity
Theory Retrospective, pages 204–223. Springer, New York, 1990.
[Selman, 1988] A. Selman. Complexity Theory Retrospective. Springer, New York, 1988.
[Seiferas et al., 1978] J. Seiferas, M. Fischer, and A. Meyer. Separating nondeterministic time
complexity classes. Journal of the ACM, 25(1):146–167, 1978.
[Shannon, 1949] C.E. Shannon. Communication in the presence of noise. IRE, 37:10–21, 1949.
[Shamir, 1992] A. Shamir. IP = PSPACE. Journal of the ACM, 39(4):869–877, 1992.
[Shor, 1997] P. Shor. Polynomial-time algorithms for prime factorization and discrete logarithms
on a quantum computer. SIAM Journal on Computing, 26(5):1484–1509, 1997.
[Simon, 1997] D. Simon. On the power of quantum computation. SIAM Journal on Computing,
26(5):1474–1483, 1997.
[Sipser, 1983] M. Sipser. A complexity theoretic approach to randomness. In Proceedings of the
15th ACM Symposium on the Theory of Computing, pages 330–335. ACM, New York, 1983.
[Sipser, 1992] M. Sipser. The history and status of the P versus NP question. In Proceedings
of the 24th ACM Symposium on the Theory of Computing, pages 603–618. ACM, New York,
1992.

Computational Complexity
521
[Smolensky, 1987] R. Smolensky. Algebraic methods in the theory of lower bounds for boolean
circuit complexity. In Proc. 19th Symposium on Theory of Computation, pages 77–82. ACM
Press, 1987.
[Smullyan, 1961] R. Smullyan. Theory of Formal Systems, volume 47 of Annals of Mathematical
Studies. Princeton University Press, 1961.
[Solovay and Strassen, 1977] R. Solovay and V. Strassen. A fast Monte-Carlo test for primality.
SIAM Journal on Computing, 6:84–85, 1977. See also erratum 7:118, 1978.
[Stearns, 1994] R. Stearns. Turing award lecture: It’s time to reconsider time. Communications
of the ACM, 37(11):95–99, November 1994.
[Stockmeyer, 1976] L. Stockmeyer. The polynomial-time hierarchy. Theor. Computer Science,
3:1–22, 1976.
[Saks and Zhou, 1999] M. Saks and S. Zhou. BPHSPACE(S) ⊆DPSPACE(S3/2). Journal of
Computer and System Sciences, 58(2):376–403, April 1999.
[Szelepcs´enyi, 1988] R. Szelepcs´enyi. The method of forced enumeration for nondeterministic
automata. Acta Informatica, 26:279–284, 1988.
[Toda, 1991] S. Toda.
PP is as hard as the polynomial-time hierarchy.
SIAM Journal on
Computing, 20(5):865–877, 1991.
[Trakhtenbrot, 1964] B. Trakhtenbrot. Turing computations with logarithmic delay. Algebra i
Logika, 3(4):33–48, 1964.
[Trakhtenbrot, 1984] R. Trakhtenbrot. A survey of Russian approaches to Perebor (brute-force
search) algorithms. Annals of the History of Computing, 6(4):384–400, 1984.
[Tseitin, 1968] G. S. Tseitin. On the complexity of derivations in the propositional calculus. In
In Studies in Constructive Mathematics and Mathematical Logic, volume Part II. Consultants
Bureau, New-York-London, 1968.
[Turing, 1936] A. Turing. On computable numbers, with an application to the Etscheidungs
problem. Proceedings of the London Mathematical Society, 42:230–265, 1936.
[Urquhart, 1987] A. Urquhart. Hard examples for resolution. Journal of the ACM, 34:209–219,
1987.
[Valiant, 1979] L. Valiant. The complexity of computing the permanent. Theoretical Computer
Science, 8:189–201, 1979.
[Vardi, 1982] M. Vardi. Complexity of relational query languages. In Proc. 14th Symposium on
Theory of Computation, pages 137–146. ACM Press, 1982.
[Watrous, 1999] J. Watrous. PSPACE has constant-round quantum interactive proof systems.
In Proceedings of the 40th IEEE Symposium on Foundations of Computer Science, pages
112–119. IEEE, New York, 1999.
[Yamada, 1962] H. Yamada. Real-time computation and recursive functions not real-time com-
putable. IEEE Transactions on Computers, 11:753–760, 1962.
[Yao, 1990] A. Yao. Coherent functions and program checkers. In Proceedings of the 22nd ACM
Symposium on the Theory of Computing, pages 84–94. ACM, New York, 1990.


LOGIC PROGRAMMING
Robert Kowalski
Readers: Maarten van Emden and Luis Moniz Pereira
1
INTRODUCTION
The driving force behind logic programming is the idea that a single formalism
suﬃces for both logic and computation, and that logic subsumes computation.
But logic, as this series of volumes proves, is a broad church, with many denomi-
nations and communities, coexisting in varying degrees of harmony. Computing is,
similarly, made up of many competing approaches and divided into largely disjoint
areas, such as programming, databases, and artiﬁcial intelligence.
On the surface, it might seem that both logic and computing suﬀer from a similar
lack of cohesion. But logic is in better shape, with well-understood relationships
between diﬀerent formalisms. For example, ﬁrst-order logic extends propositional
logic, higher-order logic extends ﬁrst-order logic, and modal logic extends classical
logic. In contrast, in Computing, there is hardly any relationship between, for
example, Turing machines as a model of computation and relational algebra as a
model of database queries. Logic programming aims to remedy this deﬁciency and
to unify diﬀerent areas of computing by exploiting the greater generality of logic.
It does so by building upon and extending one of the simplest, yet most powerful
logics imaginable, namely the logic of Horn clauses.
In this paper, which extends a shorter history of logic programming (LP) in the
1970s [Kowalski, 2013], I present a personal view of the history of LP, focusing
on logical, rather than on technological issues. I assume that the reader has some
background in logic, but not necessarily in LP. As a consequence, this paper might
also serve a secondary function, as a survey of some of the main developments in
the logic of LP.
Inevitably, a history of this restricted length has to omit a number of important
topics. In this case, the topics omitted include meta LP, high-order LP, concurrent
LP, disjunctive LP and complexity. Other histories and surveys that cover some
of these topics and give other perspectives include [Apt and Bol, 1994; Brewka et
al., 2011; Bry et al. 2007; Ceri et al., 1990; Cohen, 1988; Colmerauer and Roussel,
1996; Costantini, 2002; Dantsin et al., 1997; Eiter et al., 2009; Elcock, 1990; van
Emden, 2006; Hewitt, 2009; Minker, 1996; Ramakrishnan and Ullman, 1993].
Perhaps more signiﬁcantly and more regrettably, in omitting coverage of tech-
nological issues, I may be giving a misleading impression of their signiﬁcance.
Handbook of the History of Logic. Volume 9: Computational Logic. 
Volume editor: Jörg Siekmann 
Series editors: Dov M. Gabbay and John Woods 
Copyright © 2014 Elsevier BV. All rights reserved.

524
Robert Kowalski
Without Colmerauer’s practical insights [Colmerauer et al., 1973],
Boyer and
Moore’s [1972] structure sharing implementation of resolution [Robinson, 1965a],
and Warren’s abstract machine and Prolog compiler [Warren, 1978, 1983; Warren
et al., 1977], logic programming would have had far less impact in the ﬁeld of
Computing, and this history would not be worth writing.
1.1
The Horn clause basis of logic programming
Horn clauses are named after the logician Alfred Horn, who studied some of their
mathematical properties. A Horn clause logic program is a set of sentences (or
clauses) each of which can be written in the form:
A0 ←A1 ∧. . . ∧An where n ≥0.
Each Ai is an atomic formula of the form p(t1, ..., tm), where p is a predicate
symbol and the ti are terms. Each term is either a constant symbol, variable, or
composite term of the form f(t1, ..., tm), where f is a function symbol and the ti
are terms. Every variable occurring in a clause is universally quantiﬁed, and its
scope is the clause in which the variable occurs. The backward arrow ←is read as
“if”, and ∧as “and”. The atom A0 is called the conclusion (or head) of the clause,
and the conjunction A1 ∧... ∧An is the body of the clause. The atoms A1, ..., An
in the body are called conditions. If n = 0, then the body is equivalent to true,
and the clause A0 ←true is abbreviated to A0 and is called a fact. Otherwise if
n ̸= 0, the clause is called a rule.
It is also useful to allow the head A0 of a clause to be false, in which case, the
clause is abbreviated to ←A1 ∧... ∧An and is called a goal clause. Intuitively, a
goal clause can be understood as denying that the goal A1 ∧...∧An has a solution,
thereby issuing a challenge to refute the denial by ﬁnding a solution.
Predicate symbols represent the relations that are deﬁned (or computed) by a
program, and functions are treated as a special case of relations, as in relational
databases. Thus the mother function, exempliﬁed by mother(john) = mary, is
represented by a fact such as mother(john, mary).
The deﬁnition of maternal
grandmother, which in functional notion is written as an equation:
maternal-grandmother(X) = mother(mother(X))
is written as a rule in relational notation:
maternal-grandmother(X) ←mother(X, Z) ∧mother(Z, Y )1
Although all variables in a rule are universally quantiﬁed, it is often more natural
to read variables in the conditions that are not in the conclusion as existentially
quantiﬁed with the body of the rule as their scope. For example, the following two
sentences are equivalent:
1In this paper, I use the Prolog notation for clauses: Predicate symbols, function symbols and
constants start with a lower case letter, and variables start with an upper case letter. Numbers
can be treated as constants.

Logic Programming
525
∀XY Z [maternal-grandmother(X) ←
mother(X, Z) ∧mother(Z, Y )]
∀XY
[maternal-grandmother(X) ←∃Z [mother(X, Z) ∧mother(Z, Y )]]
Function symbols are not used for function deﬁnitions, but are used to construct
composite data structures.
For example, the composite term cons(s, t) can be
used to construct a list with ﬁrst element s followed by the list t. Thus the term
cons(john, cons(mary, nil)) represents the list [john, mary], where nil represents
the empty list.
Terms can contain variables, and logic programs can compute input-output
relations containing variables.
However, for the semantics, it is convenient to
regard terms that do not contain variables, called ground terms, as the basic data
structures of logic programs. Similarly, a clause or other expression is said to be
ground, if it does not contain any variables.
Logic programs that do not contain function symbols are also called Datalog
programs. Datalog is more expressive than relational databases, but is also de-
cidable. Horn clause programs with function symbols have the expressive power
of Turing machines, and consequently are undecidable. Horn clauses are suﬃcient
Figure 1. An and-or tree and corresponding propositional Horn clause program.
for many applications in artiﬁcial intelligence. For example, and-or trees can be
represented by ground Horn clauses.2 See ﬁgure 1.
2And-or trees were employed in many early artiﬁcial intelligence programs, including the
geometry theorem proving machine of Gelernter [1963]. Search strategies for and-or trees were
investigated by Nils Nilsson [1968], and in a theorem-proving context by Kowalski [1970].

526
Robert Kowalski
1.2
Logic programs with negation
Although Horn clauses are the underlying basis of LP and are theoretically suf-
ﬁcient for all programming and database applications, they are not adequate for
artiﬁcial intelligence, most importantly because they fail to capture non-monotonic
reasoning. For non-monotonic reasoning, it is necessary to extend Horn clauses to
clauses of the form:
A0 ←A1 ∧... ∧An ∧not B1 ∧... ∧not Bm where n ≥0 and m ≥0.
Each Ai and Bi is an atomic formula, and “not” is read as not. Atomic formulas
and their negations are also called literals. Here the Ai are positive literals, and
the not Bi are negative literals. Sets of clauses in this form are called normal logic
programs, or just logic programs for short.
It can be argued that normal logic programs, with appropriate semantics for
negation, are suﬃcient to solve the frame problem in artiﬁcial intelligence. Here is a
candidate solution using an LP representation of the situation calculus [McCarthy
and Hayes, 1969]:
holds(F, do(A, S)) ←poss(A, S) ∧initiates(A, F, S)
holds(F, do(A, S)) ←poss(A, S) ∧holds(F, S) ∧not terminates(A, F, S)
Here holds(F, S) expresses that a fact F (also called a ﬂuent) holds in a state (or
situation) S; poss(A, S) that the action A is possible in state S; initiates(A, F, S)
that the action A performed in state S initiates F in the resulting state do(A, S);
and terminates(A, F, S) that A terminates F. Together, the two clauses assert
that a fact holds in a state either if it is initiated by an action or if it held in the
previous state and was not terminated by an action.
This representation of the situation calculus also illustrates meta-logic program-
ming, because the predicates holds, poss, initiates and terminates can be under-
stood as meta-predicates, where the variable F ranges over names of sentences.
Alternatively, they can be interpreted as second-order predicates, where F ranges
over ﬁrst-order predicates.
1.3
Logic programming issues
In this article, I will discuss the development of LP and its extensions, their se-
mantics, and their proof theories. We will see that lurking beneath the deceptively
simple syntax of logic programs are profound issues concerning semantics, proof
theory and knowledge representation.
For example, what does it mean for a logic program P to solve a goal G? Does
it mean that P logically implies G, in the sense that G is true in all models of
P? Does it mean that some larger theory than P, which includes assumptions
implicit in P, logically implies G? Or does it mean that G is true in some natural,
intended model of P?

Logic Programming
527
And how should G be solved? Top-down by using the clauses in P as goal-
reduction procedures, to reduce goals that match the conclusions of clauses to
sub-goals that correspond to their conditions?
Or bottom-up to generate new
conclusions from conditions, until the generated conclusions include all the infor-
mation needed to solve the goal G in one step?
We will see that these two issues — what it means to solve a goal G, and
whether to solve G top-down or bottom-up — are related. In particular, bottom-
up reasoning can be interpreted as generating a model in which G is true.
These issues are hard enough for Horn clause programs. But they are much
harder for logic programs with negative conditions. In some semantics, a negative
condition not B has the same meaning as classical negation ¬B, and solving a
negative goal not B is interpreted as reasoning with ¬B.
But in most proof
theories, not B is interpreted as some form of negation as failure:
not B holds if all attempts to show B fail.
In addition to these purely logical problems concerning semantics and proof theory,
LP has been plagued by controversies concerning declarative versus procedural
representations. Declarative representations are naturally supported by bottom-
up model generation. But both declarative and procedural representations can
be supported by top-down execution. For many advocates of purely declarative
representations, such exploitation of procedural representations undermines the
logic programming ideal.
These issues of semantics, proof theory and knowledge representation have been
a recurring theme in the history of LP, and they continue to be relevant today.
They are reﬂected, in particular, by the growing divergence between Prolog-style
systems that employ top-down execution on one side, and answer set programming
and Datalog systems that employ bottom-up model generation on the other side.
2
THE HISTORICAL BACKGROUND
The discovery of the top-down method for executing logic programs occurred in
the summer of 1972, as the result of my collaboration with Alain Colmerauer in
Marseille. Colmerauer was developing natural language question-answering sys-
tems, and I was developing resolution theorem-provers, and trying to reconcile
them with procedural representations of knowledge in artiﬁcial intelligence.
2.1
Resolution
Resolution was developed by John Alan Robinson [1965a] as a technique for in-
dexautomated theorem-proving automated theorem-proving, with a view to mech-
anising mathematical proofs. It consists of a single inference rule for proving that
a set of assumptions P logically implies a theorem G. The resolution method is a
refutation procedure, which does so by reductio ad absurdum, converting P and the

528
Robert Kowalski
negation ¬G of the theorem into a set of clauses and deriving the empty clause,
which represents falsity.
Clauses are disjunctions of literals. In Robinson’s original deﬁnition, clauses
were represented as sets. In the propositional case:
given two clauses {A} ∪F and {¬A} ∪G
the resolvent is the clause F ∪G.
The two clauses {A}∪F and {¬A}∪G are said to be the parents of the resolvent,
and the literals A and ¬A are said to be the literals resolved upon. If F and G are
both empty, then the resolvent of {A} and {¬A} is the empty clause, representing
a contradiction or falsity.
In the ﬁrst-order case, in which all variables are universally quantiﬁed with
scope the clause in which they occur, it is necessary to unify sets of literals to
make them complementary:
given two clauses K ∪F and L ∪G
the resolvent is the clause Fθ ∪Gθ.
where θ is a most general substitution of terms for variables that uniﬁes the atoms
in K and L, in the sense that Kθ = {A} and Lθ = {¬A}. It is an important
property of resolution, which greatly contributes to its eﬃciency, that if there is
any substitution that uniﬁes K and L, then there is a most general such unifying
substitution, which is unique up to renaming of variables.
The set representation of clauses (and sets of clauses) builds in the inference
rules of commutativity, associativity and idempotency of disjunction (and con-
junction). The resolution rule itself generalises modus ponens, modus tollens, dis-
junctive syllogism, and many other separate inference rules of classical logic. The
use of the most general uniﬁer, moreover, subsumes in one operation, the inﬁnitely
many inferences of the form “derive P(t) from ∀XP(X)” that are possible with
the inference rule of universal instantiation. Other inference rules are eliminated
(or used) in the conversion of sentences of standard ﬁrst-order logic into clausal
form.
Set notation for clauses is not user-friendly. It is more common to write clauses
{A1, . . . , An, ¬B1, . . . , ¬Bm} as disjunctions A1 ∨. . .∨An ∨¬B1 ∨. . .∨¬Bm. How-
ever, sets of clauses, representing conjunctions of clauses, are commonly written
simply as sets. Clauses can also be represented as conditionals in the form:
A1 ∨. . . ∨An ←B1 ∧. . . ∧Bm.
where ←is material implication →(or ⊃) written backwards.
The discovery of resolution revolutionised research on automated theorem prov-
ing, as many researchers turned their hands towards developing reﬁnements of the
resolution rule. It also inspired other applications of logic in artiﬁcial intelligence,
most notably to the development of question-answering systems, which represent

Logic Programming
529
data or knowledge in logical form, and query that knowledge using logical in-
ference. One of the most successful and most inﬂuential such system was QA3,
developed by Cordell Green [1969].
In QA3, given a knowledge base and goal to be solved, both expressed in
clausal form, an extra literal answer(X) is added to the clause or clauses rep-
resenting the negation of the goal, where the variables X represent some value
of interest in the goal.
For example, to ﬁnd the capital of the usa, the goal
∃Xcapital(X, usa) is negated and the answer literal is added, turning it into the
clause ¬capital(X, usa)∨answer(X). The goal is solved by deriving a clause con-
sisting only of answer literals. The substitutions of terms for variables used in the
derivation determine values for the variables X. In this example, if the knowledge
base contains the clause capital(washington, usa), the answer answer(washington)
is obtained in one resolution step.
Green also showed that resolution has many other problem-solving applications,
including robot plan formation.
Moreover, he showed how resolution could be
used to automatically generate a program written in a conventional programming
language, such as LISP, from a speciﬁcation of its input-output relation written in
the clausal form of logic. As he put it:
“In general, our approach to using a theorem prover to solve program-
ming problems in LISP requires that we give the theorem prover two
sets of initial axioms:
1. Axioms deﬁning the functions and constructs of the subset of LISP
to be used
2. Axioms deﬁning an input-output relation such as the relation
R(x, y), which is to be true if and only if x is any input of the ap-
propriate form for some LISP program and y is the corresponding
output to be produced by such a program.”
Green also seems to have anticipated the possibility of dispensing with (1) and
using only the representation (2) of the relation R(x, y), writing:
“The theorem prover may be considered an ‘interpreter’ for a high-level
assertional or declarative language — logic. As is the case with most
high-level programming languages the user may be somewhat distant
from the eﬃciency of ‘logic’ programs unless he knows something about
the strategies of the system.”
“I believe that in some problem solving applications the ‘high-level
language’ of logic along with a theorem-proving program can be a
quick programming method for testing ideas.”
However, he does not seem to have pursued these ideas much further. Moreover,
there was an additional problem, namely that the resolution strategies of that time
behaved unintuitively and were very redundant and ineﬃcient. For example, given
a clause of the form L1 ∨. . . ∨Ln, and n clauses of the form ¬Li ∨Ci, resolution
would derive the same clause C1 ∨. . . ∨Cn redundantly in n! diﬀerent ways.

530
Robert Kowalski
2.2
Procedural representations of knowledge
Green’s ideas ﬁred the enthusiasm of researchers working in contact with him at
Stanford and Edinburgh, but they also attracted ﬁre from MIT, where researchers
were advocating procedural representations of knowledge. Terry Winograd’s PhD
thesis gave the most compelling and most inﬂuential voice to this opposition.
Winograd [1971] argued (page 232):
“Our heads don’t contain neat sets of logical axioms from which we
can deduce everything through a ‘proof procedure’. Instead we have a
large set of heuristics and procedures for solving problems at diﬀerent
levels of generality.”
He quoted (pages 232-3) Green’s own admission of some of the diﬃculties:
“It might be possible to add strategy information to a predicate calcu-
lus theorem prover, but with current systems such as QA3, ‘To change
strategies in the current version, the user must know about set-of-
support and other program parameters such as level bound and term
depth. To radically change the strategy, the user presently has to know
the LISP language and must be able to modify certain strategy sections
of the program.’ (<Green 1969>p. 236).”3
Winograd’s procedural alternative to purely “uniform” logical representations was
based on Carl Hewitt’s language Planner. Winograd [1971] describes Planner in
the following terms (page 238):
“The language is designed so that if we want, we can write theorems
in a form which is almost identical to the predicate calculus, so we
have the beneﬁts of a uniform system. On the other hand, we have the
capability to add as much subject-dependent knowledge as we want,
telling theorems about other theorems and proof procedures. The sys-
tem has an automatic goal-tree backup system, so that even when we
are specifying a particular order in which to do things, we may not
know how the system will go about doing them. It will be able to
follow our suggestions and try many diﬀerent theorems to establish a
goal, backing up and trying another automatically if one of them leads
to a failure (see section 3.3).”
In contrast (page 215):
“Most ‘theorem-proving’ systems do not have any way to include this
additional intelligence. Instead, they are limited to a kind of ‘working
in the dark’. A uniform proof procedure gropes its way through the
collection of theorems and assertions, according to some general proce-
dure which does not depend on the subject matter. It tries to combine
facts which might be relevant, working from the bottom-up.”
3We will see later that the set-of-support strategy was critical, because it allowed QA3 to
incorporate a form of backward reasoning from the theorem to be proved.

Logic Programming
531
Winograd’s PhD thesis presented a natural language understanding system that
was a great advance at the time, and its advocacy of Planner was enormously
inﬂuential. Even Stanford and Edinburgh were aﬀected by these ideas.
Pat Hayes and I had been working in Edinburgh on a book [Hayes and Kowal-
ski, 1971] about resolution theorem-proving, when he returned from a second visit
to Stanford (after the ﬁrst visit, during which he and John McCarthy wrote the
famous situation calculus paper [McCarthy and Hayes, 1968]). He was greatly im-
pressed by Planner, and wanted to rewrite the book to take Planner into account.
I was not enthusiastic, and we spent many hours discussing and arguing about
the relationship between Planner and resolution theorem proving. Eventually, we
abandoned the book, unable to agree.
2.3
Resolution, part two
At the time that QA3 and Planner were being developed, resolution was not
well understood. In particular, it was not understood that a proof procedure, in
general, is composed of an inference system that deﬁnes the space of all proofs and
a search strategy that explores the proof space looking for a solution of a goal. We
can represent this combination as an equation:
proof procedure = proof space + search strategy
A typical proof space has the structure of an and-or tree turned upside down.
Typical search strategies include breadth-ﬁrst search, depth-ﬁrst search and some
form of best-ﬁrst or heuristic search.
In the case of the resolution systems at the time, the proof spaces were horren-
dously redundant, and most search strategies used breadth-ﬁrst search. Attempts
to improve eﬃciency focussed on restricting (or reﬁning) the resolution rule with-
out losing completeness, to reduce the size of the proof space. The best known
reﬁnements were hyper-resolution and set of support.
Hyper-resolution [Robinson, 1965b] is a generalised form of bottom-up (or for-
ward) . In the propositional case, given an input clause:
D0 ∨¬B1 ∨. . . ∨¬Bm
and m input or derived positive clauses:
B1 ∨D1,
. . . ,
Bm ∨Dm
where each Bi is an atom and each Di is a disjunction of atoms, hyper-resolution
derives the positive clause:
D0 ∨D1 ∨. . . ∨Dm.
Bottom-up reasoning with Horn clauses is the special case in which D0 is a single
atom and each other Di is an empty disjunction, equivalent to false. In this special

532
Robert Kowalski
case, rewriting disjunctions as conditionals, hyper-resolution derives B0 from the
input clause:
B0 ←B1 ∧. . . ∧Bm
and the input or derived facts, B1, . . . , Bm.
The problem with hyper-resolution, as Winograd observed, is that it derives
new clauses from the input clauses, without paying attention to the problem to
be solved. It is “uniform” in the sense that, given a theorem to be proved, it
uniformly performs the same inferences bottom-up from the axioms, ignoring the
theorem until it generates it, as if by accident.
In contrast with hyper-resolution, the set of support strategy [Wos et al., 1965]
focuses on a subset of clauses that are relevant to the problem at hand:
A subset S′ of an input set S of clauses is a set of support for S iﬀ
S −S′ is satisﬁable. The set of support strategy restricts resolution
so that at least one parent clause belongs to the set of support or is
derived by the set of support restriction.
If the satisﬁable set of clauses S −S′ represents a set of axioms, and the set of
support S′ represents the negation of a theorem, then the set of support strategy
implements an approximation of top-down reasoning by reductio ad absurdum. It
also ensures that any input clauses (or axioms) used in a derivation are relevant
to the theorem, in the spirit of relevance logics [Anderson and Belnap, 1962].4
The set of support strategy only approximates top-down reasoning. A better ap-
proximation is obtained by linear resolution, which was discovered independently
by Loveland [1970], Luckham [1970] and Zamov and Sharonov [1969]. Linear res-
olution addresses the problem of relevance by focusing on a top clause C0, which
could represent an initial goal:
Let S be a set of clauses. A linear derivation of a clause Cn from a top
clause C0 ∈S is a sequence of clauses C0, ..., Cn such that every clause
Ci+1 is a resolvent of Ci with some input clause in S or with some
ancestor clause Cj where j < i. (It was later realised that ancestor
resolution is unnecessary if S is a set of Horn clauses.)
The top clause C0 in a linear derivation can be restricted to one belonging to a set
of support. The resulting space of all linear derivations from a given top clause
C0 has the structure of a proof tree whose nodes are clauses and whose branches
are linear derivations. Using linear resolution to extend the derivation of a clause
Ci to the derivation of a clause Ci+1 generates the derived node Ci+1 as a child of
4Another important case is the one in which S −S′ represents a database (or knowledge base)
together with a set of integrity constraints that are satisﬁed by the database, and S′ represents
a set of updates to be added to the database. The set of support restriction then implements
a form of bottom-up reasoning from the updates, to check that the updated database continues
to satisfy the integrity constraints.
Moreover, it builds in the assumption that the database
satisﬁed the integrity constraints prior to the updates, and therefore if there is an inconsistency,
the update must be “relevant” to the inconsistency.

Logic Programming
533
the node Ci. The same node Ci can have diﬀerent children Ci+1, corresponding
to diﬀerent linear resolutions.
In retrospect, the relationship with Planner is obvious. If the top clause C0
represents an initial goal, then the tree of all linear derivations is a goal tree, and
generating the tree top-down is a form of goal reduction. The tree can be explored
using diﬀerent search strategies. Depth-ﬁrst search, in particular, can be informed
by Planner-like strategies that both specify “a particular order in which to do
things”, but also “back up” automatically in the case of failure.
The relationship with Planner was not obvious at the time. Even as recently
as 2005, Paul Thagard in Mind: Introduction to Cognitive Science, compares logic
unfavourably with production systems, stating on page 45:
“In logic-based systems, the fundamental operation of thinking is log-
ical deduction, but from the perspective of rule-based systems, the
fundamental operation of thinking is search.”5
But it wasn’t just this lack of foresight that stood in the way of understanding
the relationship with Planner: there was still the n! redundant ways of resolving
upon n literals in the clauses Ci. This redundancy was recognized and eliminated
without the loss of completeness by Loveland [1972], Reiter [1971], and Kowalski
and Kuehner [1971], independently at about the same time. The obvious solution
was simply to resolve upon the literals in the clauses Ci in a single order. This
order can be determined statically, by ordering the literals in the input clauses, and
imposing the same order on the resolvents. Or it could be determined dynamically,
as in selected linear (SL) resolution [Kowalski and Kuehner, 1971], by selecting a
single literal to resolve upon in a clause Ci when the clause is chosen for resolution.
Both methods eliminate redundancy, but dynamic selection can lead to smaller
search spaces.6
Ironically, both Loveland [1972] and Kowalski and Kuehner [1971] also noted
that linear resolution with an ordering restriction is equivalent to Loveland’s [1968]
earlier model elimination proof procedure. The original model elimination proce-
dure was presented so diﬀerently that it took years even for its author to recognise
the equivalence. The SL resolution paper also pointed out that the set of all SL
derivations forms a search space, and described a heuristic search strategy for
ﬁnding simplest proofs. In the conclusions, with implicit reference to Planner, it
claimed:
5This claim makes more sense if Thagard, like Winograd before him, associates logic exclu-
sively with forward reasoning. As Sherlock Holmes explained to Dr. Watson, in A Study in
Scarlet: “In solving a problem of this sort, the grand thing is to be able to reason backward.
That is a very useful accomplishment, and a very easy one, but people do not practise it much.
In the everyday aﬀairs of life it is more useful to reason forward, and so the other comes to be
neglected. There are ﬁfty who can reason synthetically for one who can reason analytically.”
6Dynamic selection is useful, for example, to solve goals with diﬀerent input-output arguments.
For example, given the clause p(X, Y ) ←q(X, Z)∧r(Z, Y ) and the goal p(a, Y ), then the subgoal
q(a, Z) should be selected before r(Z, Y ). But given the goal p(X, b), the subgoal r(Z, b) should
be selected before q(X, Z).

534
Robert Kowalski
“Moreover, the amenability of SL-resolution to the application of heuris-
tic methods suggests that, on these grounds alone, it is at least compet-
itive with theorem-proving procedures designed solely from heuristic
considerations.”
3
THE PROCEDURAL INTERPRETATION OF HORN CLAUSES
The development of various forms of linear resolution with set of support and
ordering restrictions brought resolution systems closer to Planner-like theorem-
provers.
But these resolution systems did not yet have an explicit procedural
interpretation.
3.1
The representation of grammars in logical form
Among the various confusions that prevented a better understanding of the rela-
tionship between logical and procedural representations was the fact that Wino-
grad’s thesis, which so advanced the Planner cause, employed a diﬀerent pro-
cedural language, Programmar, for natural language grammars. To add to the
confusion, Winograd’s natural language understanding system was implemented
in a combination of Programmar, LISP and micro-Planner (a simpliﬁed subset of
Planner, developed by Charniak, Sussman and Winograd [Charniak et al., 1971]),
So it wasn’t obvious whether Planner (or micro-Planner) was supposed to be a
general-purpose programming language, or a special purpose language for proving
theorems, for writing plans or for some other purpose.
In the theorem-proving group in Edinburgh, where I was working at the time,
much of the debate surrounding Planner focused on whether “uniform”, resolution
proof procedures are adequate for proving theorems, or whether they need to be
augmented with Planner-like, domain-speciﬁc control information. In particular,
I was puzzled by the relationship between Planner and Programmar, and began
to investigate whether grammars could be written in a logical form. This was
auspicious, because in the summer of 1971 Alain Colmerauer invited me for a
short visit to Marseille.
Colmerauer knew everything there was to know about formal grammars and
their application to programming language compilers. During 1967–1970 at the
University of Montreal, he developed Q-systems [1969] as a rule-based formalism
for processing natural language. Q-systems were later used on a daily basis from
1982 to 2001 to translate English weather forecasts into French for Environment
Canada. Since 1970, he had been in Marseille, building up a team working on
natural language question-answering, investigating SL-resolution for the question-
answering component.
I arrived in Marseille, anxious to get Colmerauer’s feedback on my preliminary
ideas about representing grammars in logical form.
My representation used a
function symbol to concatenate words into strings of words, and axioms to express

Logic Programming
535
that concatenation is associative. It was obvious that reasoning with such asso-
ciativity axioms was ineﬃcient. Colmerauer immediately saw how to avoid the
axioms of associativity, in a representation that later came to be known as meta-
morphosis grammars [Colmerauer, 1975] (or deﬁnite clause grammars [Pereira and
Warren, 1980]). We saw that diﬀerent kinds of resolution applied to the resulting
grammars give rise to diﬀerent kinds of parsers. For example, forward reasoning
with hyper-resolution performs bottom-up parsing, while backward reasoning with
SL-resolution performs top-down parsing.7
3.2
Horn clauses and SLD-resolution
It was during my second visit to Marseille in April and May of 1972 that the idea
of using SL-resolution to execute Horn clause programs emerged. By the end of
the summer, Colmerauer’s group had developed the ﬁrst version of Prolog, and
used it to implement a natural language question-answering system [Colmerauer
et al., 1973]. I reported an abstract of my own ﬁndings at the MFCS conference
in Poland in August 1972 [Kowalski, 1972].8
The ﬁrst Prolog system was an implementation of SL-resolution for the full
clausal form of ﬁrst-order logic, including ancestor resolution. But the idea that
Horn clauses were an interesting case was already in the air. Donald Kuehner
[1969], in particular, had already been working on bi-directional strategies for
Horn clauses. However, the ﬁrst explicit reference to the procedural interpretation
of Horn clauses appeared in [Kowalski, 1974]. The abstract begins:
“The interpretation of predicate logic as a programming language is
based upon the interpretation of implications: B if A1 and. . . and An
as procedure declarations, where B is the procedure name and A1
and . . . and An is the set of procedure calls constituting the procedure
body.”
The theorem-prover described in the paper is a variant of SL-resolution, to which
Maarten van Emden later attached the name SLD-resolution, standing for “se-
lected linear resolution with deﬁnite clauses”:
A deﬁnite clause is a Horn clause of the form B ←B1 ∧. . . ∧Bn.
A goal clause is a Horn clause of the form ←A1 ∧. . . ∧An.
Given a goal clause ←A1 ∧. . . ∧Ai−1 ∧Ai ∧Ai+1 ∧. . . ∧An with
selected atom Ai and a deﬁnite clause B ←B1 ∧. . .∧Bm, where θ is a
most general substitution that uniﬁes Ai and B, the SLD-resolvent is
the goal clause ←(A1 ∧. . . ∧Ai−1 ∧B1 ∧. . . ∧Bm ∧Ai+1 ∧. . . ∧An)θ.
7However, Colmerauer [1991] remembers coming up with the alternative representation of
grammars, not during my visit in 1971, but after my visit in 1972.
8In the abstract, I used a predicate val(f(X), Y ) instead of a predicate f(X, Y ), using Phillip
Roussel’s idea of val as “formal equality”. Roussel was Colmerauer’s PhD student and the main
implementer of the ﬁrst Prolog system.

536
Robert Kowalski
Given a set of deﬁnite clauses S and an initial goal clause C0, an SLD-
derivation of a goal clause Cn is a sequence of goal clauses C0, . . . , Cn
such that every Ci+1 is the SLD-resolvent of Ci with some input clause
in S.
An SLD-refutation is an SLD-derivation of the empty clause.
SLD-resolution is more ﬂexible than SL-resolution restricted to Horn clauses.9
In SL-resolution the atoms Ai must be selected last-in-ﬁrst-out, but in SLD-
resolution, there is no restriction on their selection. Both reﬁnements of linear
resolution avoid the redundancy of unrestricted linear resolution, and both are
complete, in the sense that if a set of Horn clauses is unsatisﬁable, then there
exists both an SL-resolution refutation and an SLD-resolution refutation in their
respective search spaces. In both cases, diﬀerent selection strategies give rise to
diﬀerent, complete search spaces. But the more ﬂexible selection strategy of SLD-
resolution means that search spaces can be smaller, and therefore more eﬃcient
to search.
In SLD resolution, goal clauses have a dual interpretation. In the strictly logic
interpretation, the symbol ←in a goal clause ←A1 ∧. . .∧An is equivalent to clas-
sical negation; the empty clause is equivalent to falsity; and a refutation indicates
that the top clause is inconsistent with the initial set of clauses S.
However, in a problem-solving context, it is natural to think of the symbol ←
in a goal clause ←A1 ∧. . . ∧An as a question mark ? or command !, and the
conjunction A1 ∧. . . ∧An as a set of subgoals, whose variables are all existentially
quantiﬁed. The empty clause represents an empty set of subgoals, and a “refuta-
tion” indicates that the top clause has been solved. The solution is represented by
the substitutions of terms for variables in the top clause, generated by the most
general uniﬁers used in the refutation — similar to, but without the answer literals
of QA3.
As in the case of linear resolution more generally, the space of all SLD-derivations
with a given top clause has the structure of a goal tree, which can be explored
using diﬀerent search strategies. From a logical point of view, it is desirable that
the search strategy be complete, so that the proof procedure is guaranteed to ﬁnd
a solution if there is one in the search space. Complete search strategies include
breadth-ﬁrst search and various kinds of best-ﬁrst and heuristic search. Depth-
ﬁrst search is incomplete in the general case, but it takes up much less space than
the alternatives. Moreover, it is complete if the search space is ﬁnite, or if there
is only one inﬁnite branch that is explored after all of the others.
Notice that there are two diﬀerent, but related notions of completeness: one
for search spaces, and the other for search strategies. A search space is complete
if it contains a solution whenever the semantics dictates that there is a solution;
and a search strategy is complete if it ﬁnds a solution whenever there is one in the
9If SL-resolution is applied to Horn clauses, with a goal clause as top clause, then ancestor
resolution is not possible, because all clauses in the same SL-derivation are then goal clauses,
which cannot be resolved with one another.

Logic Programming
537
search space. For a proof procedure to be complete, both its search space and its
search strategy need to be complete.
The diﬀerent options for selecting atoms to resolve upon in SLD-resolution and
for searching the space of SLD-derivations were left open in [Kowalski, 1974],
but were pinned down in the Marseille Prolog interpreter. In Prolog, subgoals
are selected last-in-ﬁrst-out in the order in which the subgoals are written, and
branches of the search space are explored depth-ﬁrst in the order in which the
clauses are written.
By choosing the order in which subgoals and clauses are
written, a Prolog programmer can exercise considerable control over the eﬃciency
of a program.
3.3
Logic + Control
In those days, it was widely believed that logic alone is inadequate for problem-
solving, and that some way of controlling the theorem-prover is needed for eﬃ-
ciency. Planner combined logic and control in a procedural representation that
made it diﬃcult to identify the logical component. Logic programs with SLD-
resolution also combine logic and control, but make it possible to read the same
program both logically and procedurally. I later expressed this as Algorithm =
Logic + Control (A = L + C) [Kowalski, 1979a], inﬂuenced by Pat Hayes′ [1973]
Computation = Controlled Deduction.
The most direct implication of the equation is that, given a ﬁxed logical rep-
resentation L, diﬀerent algorithms can be obtained by applying diﬀerent control
strategies, i.e. A1 = L + C1 and A2 = L + C2. Pat Hayes [1973], in particu-
lar, argued that logic and control should be expressed in separate languages, with
the logic component L providing a pure, declarative speciﬁcation of the problem,
and the control component C supplying the problem solving strategies needed
for an eﬃcient algorithm A. Moreover, he argued against the idea, expressed by
A1 = L1 + C and A2 = L2 + C, of using a ﬁxed control strategy C, as in Prolog,
and formulating the logic Li of the problem to obtain a desired algorithm Ai.
This idea of combining logic and control in separate object and meta-level lan-
guages has been a recurring theme in the theorem-proving and AI literature. It
was a major inﬂuence, for example, on the development of PRESS, which solved
equations by expressing the rules of algebra in an object language, and the rules for
controlling the search for solutions in a meta-language. According to its authors,
Alan Bundy and Bob Welham [1981]:
“PRESS consists of a collection of predicate calculus clauses which to-
gether constitute a Prolog program. As well as the procedural meaning
attached to these clauses, which deﬁnes the behaviour of the PRESS
program, they also have a declarative meaning - that is, they can be
regarded as axioms in a logical theory.”
In retrospect, PRESS was an early example of a now common use of Prolog to
write meta-interpreters.

538
Robert Kowalski
But most applications do not need such an elaborate combination of logic and
control. For example, the meta-level control program in PRESS does not need a
meta-meta-level control program. In fact, for some applications, even the modest
control available to the Prolog programmer is unnecessary For these applications,
it suﬃces for the programmer to specify only the logic of the problem, and to leave
it to Prolog to solve the problem without any help
But often, leaving it to Prolog alone can result, not only in unacceptable inef-
ﬁciency, but even in non-terminating failure to ﬁnd a solution. Here is a simple
example, written in Prolog notation, where :- stands for ←and every clause ends
in a full stop:
likes(bob, X) : −likes(X, bob)
likes(bob, logic)
: −likes(bob, X).
Prolog fails to ﬁnd the solution X = logic, because it explores the inﬁnite branch
generated by repeatedly using the ﬁrst clause, without getting a chance to explore
the branch generated by the second clause. If the order of the two clauses is re-
versed, Prolog ﬁnds the solution. If only one solution is desired then it terminates.
But if all solutions are desired, then it encounters the inﬁnite branch, and goes
into the same inﬁnite loop. Perhaps the easiest way to avoid such inﬁnite loops in
ordinary Prolog is to write a meta-interpreter, as in PRESS.
10
Problems and ineﬃciencies with the Prolog control strategy led to numerous
proposals for LP languages incorporating enhanced control features. Some of them,
such as Colmerauer’s [1982] Prolog II, which allowed insuﬃciently instantiated
subgoals to be suspended, were developed as extensions of Prolog. Other proposals
that departed more dramatically from ordinary Prolog included the use of co-
routining in IC-Prolog [Clark et al., 1972] selective backtracking [Bruynooghe and
Pereira, 1984] and meta-level control for logic programs [Gallaire and Lasserre,
1982; Pereira, 1984]
IC-Prolog, in particular, led to the development by Clark and Gregory [1983,
1986] of the concurrent logic programming language Parlog, which led in turn to
numerous variants of concurrent LP languages, one of which KL1, developed by
Kazunori Ueda [1986], was adopted as the basis for the systems software of the
Fifth Generation Computer Systems (FGCS) Project in Japan.
The FGCS Project was a ten year project beginning in 1982, sponsored by
Japan’s Ministry of International Trade and Industry and involving all the major
Japanese computer manufacturers. Its main objective was to develop a new gen-
eration of computers employing massive parallelism and oriented towards artiﬁcial
10In other cases, much simpler solutions are often possible. For example, to avoid inﬁnite loops
with the program path(X, X) and path(X, Y ) ←link(X, Z) ∧path(Z, Y ), it suﬃces to add an
extra argument to the path predicate to record the list of nodes visited so far, and to add an extra
condition to the second clause to check that the node Z in link(X, Z) is not in this path. For
some advocates of declarative programming this is considered cheating. For others, it illustrates
a practical application of A = L1 + C1 = L2 + C2.

Logic Programming
539
intelligence applications. From the start of the project, logic programming was
identiﬁed as the preferred software technology.
The FGCS project did not achieve its objectives, and all three of its main areas
of research — parallel hardware, logic programming software, and AI applications
— suﬀered a world-wide decline.
These days, however, there is growing evidence that the FGCS project was
ahead of its time. In the case of logic programming, in particular, SLD-resolution
extended with tabling [Tamaki and Sato, 1986; Sagonas et al., 1994; Chen and
Warren, 1996; Telke and Liu, 2011] avoids many inﬁnite loops, like the one in the
example above. Moreover, there also exist alternative techniques for executing
logic programs that do not rely upon the procedural interpretation, including the
model generation methods of Answer Set programming (ASP) and the bottom-up
execution strategies of Datalog.
ASP and Datalog have greatly advanced the ideal of purely declarative rep-
resentations, relegating procedural representations to the domain of imperative
languages and other formalisms of dubious character. However, not everyone is
convinced that purely declarative knowledge representation is adequate either for
practical computing or for modelling human reasoning.
Thagard [2005], for example, claims that the following, useful procedure cannot
easily be expressed in logical terms (page 45):
If you want to go home and you have the bus fare, then you can catch a bus.
On the contrary, the sentence can be expressed literally in the logical form:
can(you, catch-bus) ←want(you, go-home) ∧have(you, bus-fare)
But this rendering requires the use of modal operators or modal predicates for
want and can. More importantly, it misses the real logic of the procedure:
go(you, home) ←have(you, bus-fare) ∧catch(you, bus).
Top-down reasoning applied to this logic generates the procedure, without sacri-
ﬁcing either the procedure or the declarative belief that justiﬁes it
4
THE SEMANTICS OF HORN CLAUSE PROGRAMS
The earliest inﬂuences on the development of logic programming had come primar-
ily from automated theorem-proving and artiﬁcial intelligence. But researchers in
the School of AI in Edinburgh also had strong interests in the theory of compu-
tation, and there was a lot of excitement about Dana Scott’s [1970] recent ﬁxed
point semantics for programming languages. Maarten van Emden suggested that
we investigate the application of Scott’s ideas to Horn clause programs and that
we compare the ﬁxed point semantics with the logical semantics.

540
Robert Kowalski
4.1
What is the meaning of a program?
But ﬁrst we needed to establish a common ground for the comparison.
If we
identify the data structures of a logic program P with the set of all ground terms
constructible from the vocabulary of P, also called the Herbrand universe of P,
then we can view the “meaning” (or denotation) of P as the set of all ground
atoms A that can be derived from P 11, which is expressed by:
P ⊢A.
Here ⊢can represent any derivability relation. Viewed in programming terms, this
is analogous to the operational semantics of a programming language. But viewed
in logical terms, this is a proof-theoretic deﬁnition, which is not a semantics at all.
In logical terms, it is more natural to understand the semantics of P as given by
the set of all ground atoms A that are logically implied by P, written:
P ⊨A
The operational and model-theoretic semantics are equivalent for any sound and
complete notion of derivation – the most important kinds being top-down and
bottom-up.
Top-down derivations include model-elimination, SL-resolution and SLD-resolution.
Model-elimination and SL-resolution are sound and complete for arbitrary clauses.
So they are sound and complete for Horn clauses in particular. Moreover, ancestor
resolution is impossible for Horn clauses. So model-elimination and SL-resolution
without ancestor resolution are sound and complete for Horn clause programs.
The selection rule in both SL-resolution and SLD-resolution constructs a linear
representation of an and-tree proof.
In SL-resolution the linear representation
is obtained by traversing the and-tree depth-ﬁrst. In SLD-resolution the linear
representation can be obtained by traversing the and-tree in any order.12
The
completeness of SLD-resolution was ﬁrst proved by Robert Hill [1974].
Bottom-up derivations are a special case of hyper-resolution, which is also sound
and complete for arbitrary clauses, and therefore for Horn clauses as well. More-
over, as we soon discovered, hyper-resolution is equivalent to the ﬁxed point se-
mantics.
4.2
Fixed point semantics
In Dana Scott’s [1970] ﬁxed point semantics, the denotation of a recursive func-
tion is given by its input-output relation. The denotation is constructed by ap-
proximation, starting with the empty relation, repeatedly plugging the current
11Notice that this excludes programs which represent perpetual processes. Moreover, it ignores
the fact that, in practice, logic programs can compute input-output relations containing variables.
This is sometimes referred to as the “power of the logical variable”.
12Note that and-or trees suggest other strategies for executing logic programs, for example by
decomposing goals into subgoals top-down, searching for solutions of subgoals in parallel, then
collecting and combining the solutions bottom-up. This is like the
MapReduce programming
model used in Google [Dean and Ghemawat, 2008].

Logic Programming
541
approximation of the denotation into the deﬁnition of the function, transforming
the approximation into a better one, until the complete denotation is obtained in
the limit, as the least ﬁxed point.
Applying the same approach to a Horn clause program P, the ﬁxed point seman-
tics uses a similar transformation TP , called the immediate consequence operator,
to map a set I of ground atoms representing an approximation of the input-output
relations of P into a more complete approximation TP (I):
TP (I) = {A0 | A0 ←A1 ∧. . . ∧An ∈ground(P) and {A1, . . . , An} ⊆I}.
Here ground(P) is the set of all ground instances of the clauses in P over the
Herbrand universe of P. The application of TP to I is equivalent to applying one
step of hyper-resolution to the clauses in ground(P) ∪I.
Not only does every Horn clause program P have a ﬁxed point I such that
TP (I) = I, but it has a least ﬁxed point, lfp(TP ), which is the denotation of P
according to the ﬁxed point semantics. The least ﬁxed point is also the smallest
set of ground atoms I closed under TP , i.e. the smallest set I such that TP (I) ⊆I.
This alternative characterisation provides a link with the minimal model semantics,
as we will see below.
The least ﬁxed point can be constructed, as in Scott’s semantics, by starting
with the empty set {} and repeatedly applying TP :
If T 0
P = {} and T i+1
P
= TP (T i
P ), then lfp(TP ) = ∪0≤iT i
P .
The result of the construction is equivalent to the set of all ground atoms that
can be derived by applying ﬁnitely many steps of hyper-resolution to the clauses
in ground(P).
The equality lfp(TP ) = ∪0≤iT i
P is usually proved in ﬁxed point theory by ap-
pealing to the Tarski-Knaster theorem. However, in [van Emden and Kowalski,
1976], we showed that the equivalence follows from the completeness of hyper-
resolution and the relationship between least ﬁxed points and minimal models.
Here is a sketch of the argument:
A ∈lfp(TP ) iﬀA ∈min(P)
i.e. least ﬁxed points and minimal models coincide.
A ∈min(P) iﬀP ⊨A
i.e. truth in the minimal model and all models coincide.
P ⊨A iﬀA ∈∪0≤iT i
P
i.e. hyper-resolution is complete.
4.3
Minimal model semantics
The minimal model semantics was inspired by the ﬁxed point semantics, but it
was based on the notion of Herbrand interpretation. The key idea of Herbrand

542
Robert Kowalski
interpretations is to identify an interpretation of a set of sentences with the set of
all ground atomic sentences that are true in the interpretation.
In a Herbrand interpretation, the domain of individuals is the set of ground
terms in the Herbrand universe of the language. A Herbrand interpretation is any
subset of the Herbrand base, which is the set of all ground atoms of the language.
The most important property of Herbrand interpretations is that, in ﬁrst-order
logic, a set of sentences has a model if and only if it has a Herbrand model. This
property is a form of the Skolem-L¨owenheim-Herbrand theorem.13
Thus the model-theoretic denotation of a Horn clause program:
M(P) = {A | A is a ground atom and P ⊨A}
is actually a Herbrand interpretation of P in its own right. Moreover, it is easy
to show that M(P) is also a Herbrand model of P. In fact, it is the smallest
Herbrand model min(P) of P. Therefore:
A ∈min(P) iﬀP ⊨A.
It is also easy to show that the Herbrand models of P coincide with the Herbrand
interpretations that are closed under the operator TP , i.e.:
I is a Herbrand model of P iﬀTP (I) ⊆I.
This is because the immediate consequence operator mimics, not only hyper-
resolution, but also the deﬁnition of truth for Horn clauses: A set of Horn clauses
P is true in a Herbrand interpretation I if and only if, for every ground instance
A0 ←A1 ∧. . . ∧An of a clause in P, A0 is true in I if A1, . . . , An are true in I.
It follows that the least ﬁxed point and the minimal model are identical:
lfp(TP ) = min(P).
4.4
Computability
The logicians Andr´eka and N´emeti visited Edinburgh in 1975, and wrote a report,
published in [Andr´eka and N´emeti, 1978], proving the Turing completeness of Horn
clause logic. Sten-˚Ake T¨arnlund [1977] obtained a similar result independently. It
was a great shock, therefore, to learn that Raymond Smullyan [1956] had already
published an equivalent result. Here is the complete abstract:
13The property can be proved in two steps: First, convert S into clausal form by using “Skolem”
functions to eliminate existential quantiﬁers. Although the resulting set S′ of clauses and S are
not equivalent, S has a model iﬀS′ has a model. A set of clauses S′ has a model iﬀS′ has a
Herbrand model M, constructed using the Herbrand universe of S′. Therefore S has a model if
and only if it has a Herbrand model M. (Contrary claims in the literature that S may have a
model, but no Herbrand model, are based on the assumption that the Herbrand interpretations
of S are constructed using the Herbrand universe of S.)

Logic Programming
543
A new approach to recursive enumerability is considered based on the
notion of “minimal models”. A formula of the lower functional calculus
of the form F1 · F2 · · · Fn−1· ⊃·Fn (or F1 alone, if n = 1) in which
each Fi is atomic, and Fn contains no predicate constants, is termed
regular. Let A be a ﬁnite set of regular formulae; Σ a collection of
sets and relations, on some universe U;
I an interpretation of the
predicate constants (occurring in A) as elements of Σ. The ordered
triple L viz.
(A, U, I) is a recursive logic over Σ. A model of L is an
interpretation of the predicate variables Pi in which each formula of A
is valid. Let P ∗
i be the intersection of all attributes assignable to Pi in
some model; these P ∗
i are called deﬁnable in L. If each Pi is interpreted
as P ∗
i , it can be proved that there is a model — this is the minimal
model. Sets deﬁnable in some L over Σ are termed recursively deﬁnable
from Σ. It is proved: (1) the recursively enumerable sets are precisely
those which are recursively deﬁnable from the successor relation and
the unit set {0}; (2) Post’s canonical sets in an alphabet a1 · · · an, are
those recursively deﬁnable from the concatenation relation and the unit
sets {a1} · · · {an}.
Smullyan seems not to have published the details of his proofs. But he investigated
the relationship between derivability and computability in his book on the Theory
of Formal Systems [Smullyan, 1961]. These formal systems are variants of the
canonical systems of Post, with strong similarities to Horn clause programs.
4.5
Logic and databases
The question-answering systems of the 1960s and 1970s represented information in
logical form, and used theorem-provers to answer questions represented in logical
form. It was the application of SL-resolution to such deductive question-answering
that led to Colmerauer’s work on Prolog. In the meanwhile, Ted Codd [1970] pub-
lished his relational model , which represented data as relations in logical form,
but used the “non-deductive” algebraic operations of selection, projection, Carte-
sian product, set union and set diﬀerence, to specify database queries. However,
he also showed [Codd, 1972] that the relational algebra is equivalent to a more
declarative relational calculus, in which relations are deﬁned in ﬁrst-order logic.
I ﬁrst learned about relational databases in 1974 at a course on the foundations
of computer science at the Mathematics Centre in Amsterdam. I was giving a
short course of lectures on logic for problem solving, using a set of notes, which I
later expanded into my 1979 book [Kowalski, 1979b]. Erich Neuhold was giving a
course about formal properties of databases, with a focus on the relational model.
It was immediately obvious that the relational model and logic programming had
much in common.
I organised a ﬁve day workshop at Imperial College London in May 1976, using
the term “logic programming” to describe the topic of the workshop. A full day

544
Robert Kowalski
was devoted to presentations about logic and databases. Herv´e Gallaire and Jean-
Marie Nicholas presented the work they were doing in Toulouse, and Keith Clark
talked about his work on negation as failure.
Jack Minker visited Gallaire and Nicholas in 1976, and together they organised
the ﬁrst workshop on logic and databases in Toulouse in 1977. The proceedings of
the workshop, published in 1978, included Clark’s results on negation as failure,
and Reiter’s paper on closed world databases.
5
NEGATION AS FAILURE — PART 1
The practical value of extending Horn clause programs to normal logic programs
with negative conditions was recognized from the earliest days of logic program-
ming, as was the obvious way to reason with them — by negation as failure
(abbreviated as NAF): to prove not p, show that all attempts to prove p fail. Intu-
itively, NAF is justiﬁed by the assumption that the program contains a complete
deﬁnition of its predicates. The assumption is very useful in practice, but was
neglected in formal logic. The problem was to give this proof-theoretic notion a
logical semantics.
Ray Reiter [1978] investigated NAF in the context of a ﬁrst-order database D,
interpreting it as the closed world assumption (CWA) that the negation not p of
a ground atom p holds in D if there is no proof of p from D. He showed that
the CWA can lead to inconsistencies in the general case — for example, given the
database D = {p∨q}, it implies not p, and not q; but for Horn data bases no such
inconsistencies can arise.
However, Keith Clark was the ﬁrst to investigate NAF in the context of logic
programs with negative conditions.
5.1
The Clark completion
Clark’s solution was to interpret logic programs as short hand for deﬁnitions in
if-and-only-if form, as illustrated for the propositional program in ﬁgure 2.
In the non-ground case, the logic program needs to be augmented with an
equality theory, which mimics the uniﬁcation algorithm, and which essentially
speciﬁes that ground terms are equal if and only if they are syntactically identical.
An example with a fragment of the necessary equality theory, is given in ﬁgure
3. Together with the equality theory, the if-and-only-if form of a logic program
P is called the completion of P, written comp(P). It is also sometimes called the
predicate completion or the Clark completion.
As ﬁgure 3 illustrates, negation as failure correctly simulates reasoning with the
completion in classical logic.
Although NAF is sound with respect to the completion semantics, it is not

Logic Programming
545
Figure 2. The logic program of ﬁgure 1, and its completion.
Figure 3. A proof of not likes(logic, logic) using negation as failure and backward
reasoning compared with a proof of ¬likes(logic, logic) classical logic, presented
upside down. Notice that the use of classical negation turns the disjunction of
alternatives into a logical conjunction.

546
Robert Kowalski
complete. For example, if P is the program:
p ←q
p ←¬q
q ←q
then comp(P) implies p. But given the goal ←p, NAF goes into an inﬁnite loop
trying, but failing to show q. The completion semantics does not recognise such
inﬁnite failure, because proofs in classical logic are ﬁnite. For this reason, the
completion semantics is also called the semantics of negation as ﬁnite failure.
In contrast with the completion semantics, the CWA formalises negation as
potentially inﬁnite failure, inferring ¬q from q ←q. Similarly, the minimal model
semantics of Horn clauses concludes that ¬q is true in the minimal model of q ←q.
Clark did not investigate the relationship between the completion semantics
and the various alternative semantics of Horn clauses. Probably the ﬁrst such
investigation was by Apt and van Emden [1982], who showed, among other things,
that if P is a Horn clause program then:
I is a Herbrand model of comp(P) iﬀTP (I) = I.
Compare this with the property that I is a Herbrand model of P iﬀTP (I) ⊆I.
5.2
The analogy with arithmetic
Clark’s 1978 paper was not the ﬁrst to propose the completion semantics. [Clark
and T¨arnlund, 1977] proposed using the completion together with induction schemas
on the structure of terms to prove program properties, by analogy with the use of
induction in ﬁrst-order Peano arithmetic.
Consider the Horn clause deﬁnition of append(X, Y, Z), which holds when the
list Z is the concatenation of the list X followed by the list Y :
append(nil, X, X)
append(cons(U, X), Y, cons(U, Z)) ←append(X, Y, Z)
This is analogous to the deﬁnition of plus(X, Y, Z), which holds when X +Y = Z:
plus(0, X, X)
plus(s(X), Y, s(Z)) ←plus(X, Y, Z)
Here the successor function s(X) represents X + 1, as in Peano arithmetic.
These deﬁnitions alone are adequate for computing their denotations. More
generally, they are adequate for solving any goal clause (which is an existentially
quantiﬁed conjunction of atoms). However, to prove program properties expressed
in the full syntax of ﬁrst-order logic, the deﬁnitions need to be augmented with
their completions and induction axioms. For example, the completion and induc-
tion over the natural numbers are both needed to show that the plus relation
deﬁned above is functional:
∀XY UV [plus(X, Y, U) ∧plus(X, Y, V ) →U = V ]

Logic Programming
547
Similarly, to show, for example, that append is associative, the deﬁnition of append
needs to be augmented both with the completion and induction over lists.
Because many program properties can be expressed in the logic programming
sublanguage of ﬁrst-order logic, it can be hard to distinguish between clauses
that are needed for computation, and clauses that are emergent properties. A
similar problem arises with deductive databases. As Nicolas and Gallaire [1978]
observed, it can be hard to distinguish between clauses that deﬁne data, and
integrity constraints that restrict data.
For real applications, these distinctions are essential.
For example, without
making these distinctions, a programmer can easily write a program that includes
both the deﬁnition of append and the property that append is associative. The
resulting, ”‘purely declarative” logic program would be impossibly ineﬃcient.
The analogy with arithmetic helps to clarify the relationships between the dif-
ferent semantics of logic programs: It suggests that the completion augmented
with induction schemas is like the ﬁrst-order axioms for Peano arithmetic, and
the minimal model is like the standard model of arithmetic. The fact that both
notions of arithmetic have a place in mathematics suggests that both kinds of
“semantics” also have a place in logic programming.
Interestingly, the analogy also works in the other direction. The fact that min-
imal models are the denotations of logic programs shows that the standard model
of arithmetic has a syntactic core, which consists of the Horn clauses that de-
ﬁne addition and multiplication. Martin Davis [1980] makes a similar point, but
his core is essentially the Horn clause deﬁnitions of addition and multiplication
augmented with the Clark Equality Theory:
∃x.Z(x)
∀xy.[Z(x) ∧Z(y) ⊃x = y]
∀x.∃y.S(x, y)
∀xy.[S(x, y) ⊃¬Z(y)]
∀xy.[Z(y) ⊃A(x, y, x)]
∀xyzuv.[A(x, y, z) ∧S(y, u) ∧S(z, v) ⊃A(x, u, v)]
∀xy.[Z(y) ⊃P(x, y, y)]
∀xyzuv.[P(x, y, z) ∧S(y, u) ∧A(z, x, v) ⊃P(x, u, v)]
Here Z(x) stands for “x is zero”, S(x, y) for “y is the successor of x”, A(x, y, z)
for “x + y = z” and P(x, y, z) for “xy = z”.
Arguably, the syntactic core of the standard model of arithmetic explains how
we can understand what it means for a sentence of arithmetic to be true, even if
it may be impossible to prove that the sentence is true.
5.3
Database semantics
In the same workshop in which Clark presented his work, Nicolas and Gallaire
[1978] considered related issues from a database perspective. They characterised

548
Robert Kowalski
the relational database approach as viewing databases as model-theoretic struc-
tures (or interpretations), and the deductive database approach as viewing databases
as theories. They argued that, in relational databases, both query evaluation and
integrity constraint satisfaction are understood as evaluating the truth value of a
sentence in an interpretation. But in deductive databases, they are understood as
determining whether the sentence is a theorem, logically implied by the database
viewed as a theory. Hence the term “deductive”. In retrospect, it is now clear that
both kinds of databases, whether relational or “deductive”, can be viewed either
as an interpretation or as a theory.
A more fundamental issue at the time of the 1978 workshop was the inability
of the relational calculus and relational algebra to deﬁne recursive relations, such
as the transitive closure of a binary relation. Aho and Ullman [1979] proposed to
remedy this by extending the relational algebra with ﬁxed point operators. This
proposal was pursued by Chandra and Harel [1982], who classiﬁed and analysed the
complexity of the resulting hierarchy of query languages. Previously, Harel [1980]
had published a harsh review of the logic and databases workshop proceedings
[Gallaire and Minker, 1979], criticising it for claiming that deductive databases
deﬁne relations in ﬁrst-order logic despite the fact that transitive closure cannot
be deﬁned in ﬁrst-order logic.
During the 1980s, the deductive database community, with roots mainly in ar-
tiﬁcial intelligence, became assimilated into a new Datalog community, inﬂuenced
by logic programming, but with its roots ﬁrmly in the database ﬁeld. In keeping
with its database perspective, Datalog excludes function symbols. So all Herbrand
models are ﬁnite, and are computable bottom-up. But pure bottom-up compu-
tation, whether viewed as model generation or as theorem-proving, ignores the
query until it derives it as though by accident. To make model generation relevant
to the query, Datalog uses transformations such as Magic Sets [Bancilhon, et al
1985] to incorporate the query into the transformed database rules.
As a consequence of its model generation approach, Datalog ignores the com-
pletion semantics in favour of the minimal model and ﬁxed point semantics. For
example, the surveys by Ceri, Gottlob and Tanca [1989], and Ramakrishnan and
Ullman [1993], and even the more general survey of the complexity and expres-
sive power of logic programming by Dantsin, Eiter, Gottlob and Voronkov [2001]
mention the completion only in passing.
Minker’s [1996] retrospective on Logic and Databases acknowledges the distinc-
tive character of Datalog, but also includes the completion semantics. In partic-
ular, the completion semantics contributed to investigations of the semantics of
integrity constraints, which was an important topic in deductive databases, before
the ﬁeld of Datalog fully emerged.
6
NEGATION AS FAILURE — PART 2
Theoretical investigations of the completion semantics continued, and were high-
lighted in John Lloyd’s [1985, 1987] inﬂuential Foundations of Logic Programming

Logic Programming
549
book, which included results from Keith Clark’s [1980] unpublished PhD thesis.
Especially important among the later results were the three-valued completion
semantics of Fitting [1985] and Kunen [1987], which give, for example, the truth
value undeﬁned to p in the program p ←not p, whose completion is inconsistent
in two-valued logic. This and other work on the completion semantics are pre-
sented in Shepherdson’s [1988] survey. Much of this work concerns the correctness
and completeness of SLDNF resolution (SLD resolution extended with negation
as ﬁnite failure), relative to the completion semantics.
6.1
Stratiﬁcation
The most signiﬁcant next step in the investigation of negation was the study of
stratiﬁed negation in database queries by Chandra and Harel [1985] and Naqvi
[1986].
The simplest example of a stratiﬁed logic program is that of a deductive database
E ∪I whose predicates are partitioned into extensional predicates, deﬁned by
facts E, and intensional predicates, deﬁned in terms of the extensional predicates
by facts and rules I. Consider, for example, a network of nodes, some of whose links
at any given time may be broken14. This can be represented by an extensional
database, say:
E:
link(a, b)
link(a, c)
link(b, c)
broken(a, c)
Two nodes in the network are connected if there is a path of unbroken links. This
can be represented intensionally by the clauses:
I:
connected(X, Y ) ←link(X, Y ) ∧not broken(X, Y )
connected(X, Y ) ←connected(X, Z) ∧connected(Z, Y )
The conditions of the ﬁrst clause in I are completely deﬁned by E. So they can be
evaluated independently of I. The use of E to evaluate these conditions results in
a set of Horn clauses I′ ∪E, which intuitively has the same meaning as I ∪E:
I′:
connected(a, b)
connected(b, c)
connected(X, Y ) ←connected(X, Z) ∧connected(Z, Y )
The natural, intended model of the original deductive database E
∪I is the
minimal model M of the resulting set of Horn clauses I′ ∪E:
M:
link(a, b)
link(a, c)
link(b, c)
broken(a, c)
connected(a, b)
connected(b, c)
connected(a, c)
14This example is inspired by the following quote from Hellerstein [2010]: “Classic discus-
sions of Datalog start with examples of transitive closure on family trees: the dreaded anc and
desc relations that aﬄicted a generation of graduate students. My group’s work with Datalog
began with the observation that more interesting examples were becoming hot topics: Web in-
frastructure such as webcrawlers and PageRank computation were essentially transitive closure
computations, and recursive queries should simplify their implementation.”

550
Robert Kowalski
This construction can be iterated if the intensional part of the database is also
partitioned into layers (or strata). The further generalisation from databases to
logic programs with function symbols was investigated independentl van Gelder
[1989] and by Apt, Blair and Walker [1988].
Let P be a logic program, and let Pred = Pred0 ∪. . . ∪Predn be a partition-
ing and ordering of the predicate symbols of P. If A is an atomic formula, let
stratum(A) = i if and only if the predicate symbol of A is in Predi. Then P is
stratiﬁed (with respect to this stratiﬁcation of the predicate symbols), if and only
if for every clause head ←body in P and for every condition C in body:
if C is an atomic condition, then stratum(C) ≤stratum(head)
if C is a negative condition notA, then stratum(A)<stratum(head).
The stratiﬁcation Pred = Pred0 ∪. . . ∪Predn of the predicate symbols of P
induces a corresponding stratiﬁcation of the program P = P0 ∪. . . ∪Pn where
head ←body in Pi if and only if stratum(head) = i.
The perfect model of a stratiﬁed program P is constructed by starting from the
minimal model M0 of the Horn clause program P0 and iteratively extending the
perfect model Mi−1 of P0 ∪... ∪Pi−1 to the perfect model Mi of P0 ∪... ∪Pi.
Assuming that the perfect model Mi−1 has already been constructed, then Mi
is constructed by using Mi−1 to evaluate the conditions in Pi that are already
deﬁned in P0 ∪... ∪Pi−1 obtaining a set of Horn clauses P ′
i, and generating Mi as
the minimal model of the Horn clauses P ′
i ∪Mi−1. Constructed in this way, Mn
is the perfect model of P.
For example, suppose we want to extend the logic program above by a clause
that says a pair of nodes is unconnected if it is not connected:
unconnected(X, Y ) ←not connected(X, Y )
The resulting program is stratiﬁed, with its predicates partitioned into the strata
Pred0 = {link, broken}, Pred1 = {connected}, Pred2 = {unconnected}. The per-
fect model M2 of the program is constructed as follows:
M0
=
{link(a, b), link(a, c), link(b, c), broken(a, c)}
M1
=
M0 ∪{connected(a, b), connected(b, c), connected(a, c)}
M2
=
M1 ∪{unconnected(a, a), unconnected(b, b), unconnected(c, c),
unconnected(b, a), unconnected(c, b), unconnected(c, a)}
It is useful to express the construction of perfect models using the notion of reduct,
which relates it to the later construction of stable models by Gelfond and Lifschitz:
Given a set of ground atoms E and a logic program P, the reduct of P by E, written
reduct(P, E) is the set of Horn clauses obtained from P by using E to evaluate the
negative literals in P and using classical logic to simplify the resulting program.
This is equivalent to deleting all clauses containing a condition not B that is false
in E and deleting all conditions not B that are true in E. Intuitively, if E deﬁnes
all of the non-atomic conditions in P, then reduct(P, E) ∪E is a set of Horn
clauses that has the same meaning as P
∪E. With this notion of reduct, the
deﬁnition of perfect model can be expressed more concisely:

Logic Programming
551
Given a stratiﬁed logic program P = P0 ∪. . . ∪Pn, the perfect model
of P is Mn where:
M0 = min(P0)
Mi = min(reduct(Pi, Mi−1) ∪Mi−1)
Interestingly, the deﬁnition exploits the ambiguity of sets of atomic sentences,
which can be viewed both as theories and as Herbrand interpretations. In its ﬁrst
occurrence in min(reduct(Pi, Mi−1) ∪Mi−1), Mi−1 is a Herbrand interpretation.
In its second occurrence, Mi−1 is part of a Horn clause program.
6.2
Local stratiﬁcation
Przymusinski [1988] extended the notion of stratiﬁcation from predicate symbols
to ground atoms. In eﬀect, this replaces a program P by the program ground(P).
In ground(P), diﬀerent atoms with the same predicate symbol are treated as dis-
tinct 0-ary predicates, which can be assigned to diﬀerent strata. Because of func-
tion symbols, ground(P) can be countably inﬁnite. Here is possibly the simplest
sensible example that illustrates this:
Even :
even(0)
even(s(X)) ←not even(X))
The program ground(Even) can be partitioned into a countably inﬁnite number
of subprograms ground(Even) = ∪i<ω Eveni where:
Even0 :
even(0)
Eveni :
even(si(0)) ←not even(si−1(0)) for i > 0
The perfect model is the limit ∪i<ωMi = {even(0), even(s(s(0))), . . .} where:
M0 = min(Even0) = {even(0)}
M1 = min(reduct(Even1, M0) ∪M0) = min({} ∪M0) = M0
M2 = min(reduct(Even2, M1) ∪M1) = min({even(s(s(0)))} ∪M0)
= {even(0), even(s(s(0)))}
. . . etc.
In general, let P be a logic program, and let H = ∪i<αHi be a partitioning and
ordering of the Herbrand base H of P, where α is a countable, possibly transﬁnite
ordinal. If A ∈H, let stratum(A) = i if and only if A ∈Hi. Then P is locally
stratiﬁed (with respect to this stratiﬁcation of H) if and only if for every clause
head ←body in ground(P) and for every condition C in body:
if C is an atomic condition, then stratum(C) ≤stratum(head)
if C is a negative condition notA, then stratum(A)<stratum(head).
The stratiﬁcation ∪i<αHi of H induces a corresponding stratiﬁcation of ground(P) =
∪i<αPi where head ←body is in Pi if and only if stratum(head) = i. The perfect
model of P is Mα where:

552
Robert Kowalski
M0 = min(P0)
Mi = min(reduct(Pi, Mi−1) ∪Mi−1)
Mβ = ∪i<βMi if β is a limit ordinal.
Unfortunately, although this construction gives the intended model for many nat-
ural programs, like Even above, it can fail even for minor syntactic variants of
those programs. For example:
successor(X, s(X))
even(0)
even(Y ) ←successor(X, Y ) ∧not even(X)
This program cannot be locally stratiﬁed, because its ground instances contain
such unstratiﬁable clauses as even(0) ←successor(0, 0) ∧not even(0).
Having recognised the problem, a number of authors proposed further reﬁne-
ments of stratiﬁcation. However, it now seems to be generally agreed that these
reﬁnements are superseded by the well-founded semantics of [Van Gelder, Ross and
Schlipf 1991]. In particular, [Denecker et al., 2001] argues that the well-founded
semantics “provides a more general and more robust formalization of the principle
of iterated inductive deﬁnition that applies beyond the stratiﬁed case.”
6.3
Well-founded semantics
[Denecker et al., 2001] presents a simpliﬁed deﬁnition of the well-founded semantics
in terms of candidate proofs. Here is a further simpliﬁcation, in which candidate
proofs are viewed as arguments supported by sets ∆of assumptions that are
negative literals:
Given a ground normal program P, an argument for an atom p supported by
assumptions ∆is a ﬁnite tree T labelled with literals such that:
• p is the root of T.
• Each non-leaf node q of T is the head of some clause q ←B in P, and its
children are the literals in the body B of the clause. If q is a fact, with an
empty body B, then it has a single child labelled by true.
• Each leaf is either the label true or a negative literal contained in ∆.
Arguments can be used to construct a three-valued Herbrand model M of P,
represented by the set of all ground literals that are true in M. In general a three-
valued Herbrand interpretation I = Ipos ∪Ineg is a set of ground literals, such
that every atom A in Ipos is true in I, and every atom A whose negation not A
is in Ineg is false in I. No atom A is both true and false, and an atom A that is
neither true nor false is undeﬁned.
The well-founded model can be generated bottom-up, starting with the empty
set {} and repeatedly applying a three-valued consequence operator ConP , which
extends a partial three-valued interpretation I of P to a more complete three-
valued interpretation:

Logic Programming
553
ConP (I) = {p | there exists an argument for p supported by Ineg} ∪
{not p | every argument for p has a leaf not q with q ∈Ipos}
The well-founded model of P is the smallest three-valued interpretation
I such that ConP (I) ⊆I.
The well-founded model can also be queried top-down using SLG resolution [Chen
and Warren, 1996], which is a variant of SLDNF with tabling. We will see later that
the well-founded semantics also has an intuitive argumentation-theoretic interpre-
tation, in which the negative literals not p in ConP (I) are all the assumptions
defended by I.
[Denecker, 1998] argued that the well-founded semantics formalizes the infor-
mal notion of inductive deﬁnition. In particular, the survey by [Denecker et al.,
2001] of theories of inductive deﬁnitions in mathematical logic identiﬁes two main
approaches: inﬂationary inductive deﬁnitions and deﬁnitions. The abstract ver-
sion of inﬂationary induction was investigated by Moschovakis [1974], and iterated
induction was introduced by Kreisel [1963] and studied by Feferman [1970] and
others.
Van Gelder [1993] observed that even simple concepts can be diﬃcult to ex-
press using inﬂationary induction. For example, the complement of the transitive
closure of a graph can be deﬁned simply by a stratiﬁed logic program (as in the def-
inition of unconnected in terms of connected). But it was considered a signiﬁcant
achievement when a solution was found using inﬂationary induction. [Denecker et
al., 2001] argued that, in contrast, the well-founded semantics builds on the same
principle as iterated inductive deﬁnitions, but is “more general and more robust”.
6.4
Stable model semantics
Whereas the development of the well-founded semantics was inﬂuenced by strat-
iﬁcation in deductive databases, the development of the stable model semantics
[Gelfond and Lifschitz, 1988] was inﬂuenced by the problem of formalising default
reasoning.
The earliest attempts to formalise default reasoning in artiﬁcial intelligence
employed non-logical, object-oriented representations, such as semantic networks
and frames [Minsky, 1975]. The ﬁrst workshop dealing with logic-based approaches
was held at Stanford in November 1978. A special issue of the Artiﬁcial Intelligence
journal on non-monotonic reasoning, based on the workshop, was published in
1980. It contained papers by John McCarthy [1980] on circumscription, Ray Reiter
[1980] on default logic, and Drew McDermott and Jon Doyle [1980]
on non-
monotonic modal logic. It was later shown that circumscription and default logic
have interesting relationships with the semantics of negation in logic programming.
Robert Moore [1985] developed a simple and elegant reconstruction of non-
monotonic modal logic, which was used later by Michael Gelfond [1987] to give an
autoepistemic interpretation to normal logic programs. In Gelfond’s translation,

554
Robert Kowalski
a logic program of the form p ←q ∧not r, for example, is translated into the
sentence p ←q ∧¬ Lr of autoepistemic logic, where Lr means r is believed.
Gelfond and Lifschitz [1988] further simpliﬁed this translation in the stable model
semantics, interpreting a negative literal not p as meaning that p is not believed,
in eﬀect because the assumption that p is not believed correctly leads to the
conclusion that p is not believed.
In general, given a normal logic program P:
a Herbrand interpretation M of P is a stable model of P iﬀ
M = min(reduct(ground(P), M)).
For example, given P = {q,
p ←q ∧not r} , and the Herbrand interpretation
M = {p, q}, the condition not r is true in M, and therefore the set of Horn clauses
reduct(P, M) = {q, p ←q} has the same meaning as P in the context of M. This
meaning is the minimal model of reduct(P, M), which is identical to M. Therefore
M is a stable model of P. Moreover, it is the only stable model of P.
A program can have one, many or no stable models. For example the program
{p ←not p} has no stable models, but {p ←not q,
q ←not p} has two stable
models, {p} and {q}.
The stable model semantics is non-monotonic, because adding clauses to a pro-
gram can non-monotonically decrease the consequences of the program. For ex-
ample, adding r to the program {q,
p ←q ∧not r} changes the minimal model
from {p, q} to {q}.
The stable model semantics has been extended to programs of the form:
D1 ∨. . . ∨Dl ←A1 ∧. . . ∧An ∧not B1 ∧. . . ∧not Bm
where l ≥0, n ≥0 and m ≥0. Here each Di, Ai and Bi is an atomic formula or
the “explicit” negation ¬A of an atomic formula A. If l = 0, then the conclusion
is equivalent to false.
The use of explicit negation
(also called strong, or even classical negation)
enables more natural knowledge representation, including the representation of
rules and exceptions. For example:
canfly(X) ←bird(X) ∧not ¬canfly(X)
¬canfly(X) ←penguin(X)
In the extended stable model semantics, explicit negations ¬A are treated as
syntactic sugar for “contrary” positive atoms, say A∗. For example, the literal
¬canﬂy(X) can be renamed as a positive atom, say abnormal(X). With this re-
naming and the addition of clauses ←A ∧A∗for every pair of contrary atoms,
the stable models of a program with explicit negation are isomorphic to the stable
models of the same program without explicit negation.
Arguably, the extension to disjunctive conclusions is more problematic, because
it creates the need to decide between alternative representations, for example
between the two representations:

Logic Programming
555
p ←not q
q ←not p
versus
p ∨q
In the general case, diﬀerent representations have diﬀerent stable models.
In any case, the extension of the stable model semantics to include disjunctive
conclusions clearly distances it from the stratiﬁed and well-founded semantics,
which are naturally understood as the semantics of inductive deﬁnitions.
The
primer by Eiter et al., [2009], which presents a comprehensive survey of the stable
model semantics and its associated Answer Set Programming paradigm, refers to
the diﬀerence between these two kinds of semantics as the Great Logic Program-
ming Schism.
It might be more appropriate to call this the Second Great Schism, with the
First Great Schism being between two diﬀerent views of what it means for a logic
program P to solve a goal G.
In the theorem-proving view, solving G means
showing that P (or comp(P)) logically implies G. But both in the stratiﬁed/well-
founded semantics and in the stable model semantics, solving G means showing
that G is true in some appropriate minimal model of P.
6.5
Answer set programming
In the stratiﬁed/well-founded semantics, the intended model M of a program is
unique, the goal G typically contains (existentially quantiﬁed) variables, and a
solution of G is a substitution σ of terms for variables such that Gσ is true in M.
In the paper that introduced the stable model semantics, Gelfond and Lifschitz
[1988] similarly viewed the purpose of the stable model semantics as identifying a
unique (or canonical) model of a program, writing:
“The stable model semantics is deﬁned for a logic program Π, if Π
has exactly one stable model, and it declares that model to be the
canonical model of Π.”
According to this view, programs that have multiple stable models do not have a
semantics at all.
The answer set programming (ASP) paradigm, proposed independently by Ilkka
Niemel¨a [1999] and by Victor Marek and Miroslaw Truszczy´nski [1999] turns this
point of view upside down. In ASP, the program itself is the problem to be solved,
a stable model of the program is a solution, and diﬀerent stable models are diﬀerent
solutions.
One of the simplest and most typical ASP examples is the map colouring prob-
lem. Given a map with countries X represented by the predicate country(X) and
adjacent countries X and Y represented by adjacent(X, Y ), the problem is to ﬁnd
a colouring red, yellow or green for each country, such that no two adjacent coun-
tries have the same colour. Here, ignoring the constraint that no country should
have two diﬀerent colours, is a simple representation:
colour(X, red) ∨colour(X, yellow) ∨colour(X, green) ←country(X)
←colour(X, C) ∧colour(Y, C) ∧adjacent(X, Y )

556
Robert Kowalski
Clearly, the ﬁrst clause can be rewritten as three normal clauses with negative
conditions, in this case without aﬀecting the stable models of the program. The
second clause, which implicitly has the conclusion false, is a constraint, which
excludes models that satisfy the conditions of the clause.
ASP is almost certainly the most active area of research in logic programming
today. Because, for practical applications, solutions (and therefore stable models)
need to be ﬁnite, it is common to restrict programs P to ones whose grounding
ground(P) is ﬁnite. To ensure ﬁnite groundings, ASP programs are often restricted
to ones without function symbols, as in Datalog. For this reason, and because ASP
and Datalog both employ bottom-up problem solving methods, the two areas have
much in common.
7
ABDUCTIVE LOGIC PROGRAMMING
ASP also overlaps with constraint logic programming (CLP) and abductive logic
programming (ALP). In a recent ASP programming competition [Denecker et al.,
2009], competitors included one CLP system and three ALP systems. The top
two systems were both ASP solvers, but an ALP-like solver achieved a respectable
third place.
7.1
Abduction
Abduction was identiﬁed by the logician and philosopher Charles Sanders Peirce
(1839–1914) as a form of logical reasoning, comparable to, but
distinct from,
deduction and induction. Whereas deduction derives conclusions from assumptions
(e.g. p(a) from p(X) ←q(X) and q(a)), and induction derives general rules from
facts (e.g. p(X) ←q(X) from p(a), p(b) and q(a)), abduction derives assumptions
from rules and conclusions (e.g.
q(a) from p(X) ←q(X) and p(a)).
As the
last example shows, abduction is closely related to top-down reasoning in logic
programming.
Peirce’s notion of abduction has had a big inﬂuence on epistemology and the
philosophy of science, and inspired numerous applications in artiﬁcial intelligence.
In my case, it was one of the inspirations of the concluding chapter of my 1979
book. It also inspired the development of Theorist [Poole et al., 1987] and the
application of abduction to default reasoning.
In Poole [1988], abduction is used to extend a ﬁrst-order clausal theory T with
assumptions ∆from a set of candidate hypotheses A, restricted by a set of ﬁrst-
order constraints I:
Given T, A, I and observations G, an abductive explanation of G is a
subset ∆of A, such that
T ∪∆⊨G
T ∪∆∪I is consistent.

Logic Programming
557
The implementation of Theorist used a combination of linear resolution to generate
candidate ∆, by reasoning backwards from G, and a refutation proof procedure to
show that T ∪∆∪I is consistent, by failing to refute T ∪∆∪I. Although such a
procedure is not even semi-decidable in theory, it is often suﬃcient in practice.
7.2
Horn clause ALP and the relationship with stable models
[Eshghi, 1988] and [Eshghi and Kowalski, 1989] reformulated Theorist in a logic
programming setting, deﬁning an abductive framework as a triple ⟨P, I, A⟩, where
P is a Horn clause program, I is a set of integrity constraints, and A is a set of
ground atoms (whose predicates are called abducible):
Given an abductive framework ⟨P, I, A⟩and a set G of goal clauses, an
abductive solution (or explanation) of G is a subset ∆of A, such that
P ∪∆solves G
P ∪∆satisﬁes I.
The requirement that P
∪∆solves G was deﬁned as in Theorist, namely as
P ∪∆⊨G. But because P ∪∆is a set of Horn clauses, it can also be deﬁned
as G being true in the minimal model of P ∪∆. For integrity constraints I in the
form of denials ←A1 ∧. . . ∧An, the requirement that P ∪∆satisﬁes I was also
deﬁned as in Theorist, namely as P ∪∆∪I is consistent.
Whereas Poole [1988] investigated a translation of default logic into Theorist,
Eshghi and Kowalski [1989] investigated a translation of the stable model semantics
into ALP. The translation treats the set of all ground negative literals as a set Neg
of abducible atoms. In doing so, it treats a normal logic program P with negative
conditions as a Horn clause program P ∗. It uses integrity constraints I to ensure
that ordinary atoms a and their abducible negations not a are complementary.
This translates P into an ALP program ⟨P ∗, I, Neg⟩. The correspondence is given
by the relationship:
Let H be the Herbrand base of a normal logic program P.
Let M ⊆H and ∆⊆Neg.
Then M is a stable model of P if and only if P ∗∪∆satisﬁes I.
The integrity constraints I needed for this correspondence include both all the
denials ←a ∧not a and all the disjunctions a ∨not a. But at the time, we did not
realise that the disjunctive constraints could be represented so simply. Instead, we
represented them in a meta-logical form, in the spirit of Reiter’s [1988] epistemic,
modal representation of integrity constraints. I will come back to this problem of
representing disjunctive constraints in the next subsection, 7.3.
Although [Eshghi and Kowalski, 1989] showed how to translate stable mod-
els into ALP, many ASP programs can be represented in ALP directly without
the translation. For example, the map colouring program of section 6.5 can be
represented by the ALP framework ⟨P, I, A⟩where:

558
Robert Kowalski
P contains the deﬁnitions of the predicates country and adjacent.
A is the predicate colour.
I is the denial and disjunctive clause of the ASP program.
[Satoh and Iwayama, 1991] showed that the correspondence between stable models
and ALP also works in the opposite direction: Let ⟨P, I, A⟩be an ALP framework
with denial integrity constraints I. For every a ∈A, let not-a be a distinct atom
not occurring in P. Let A∗be the set of all clauses:
a(X) ←not not-a(X)
not-a(X) ←not a(X)
Then M is a stable model of P ∪I ∪A∗if and only if P ∪∆⊨I
7.3
The ALP tower of Babel
The deﬁnition of an ALP framework ⟨P, I, A⟩given in section 7.2 can be general-
ized so that P is a normal logic program with negation. But then the requirements
that P ∪∆solves G and P ∪∆satisﬁes I have even more interpretations than
before. In particular, the requirement that P ∪∆solves G can be interpreted
either as comp(P ∪∆) ⊨G, or as G is true in some appropriate minimal model of
P ∪∆.
The requirement that P ∪∆satisﬁes I is even more problematic. The problem
was already the subject of extensive debate in the 1980s in the context of deductive
databases D.
The alternatives included the interpretation that the constraints
I are consistent with D [Kowalski, 1978], that they are consistent with comp(D)
[Sadri and Kowalski, 1988], that they are logically implied by comp(D) [Reiter,
1984; 1988; Lloyd and Topor, 1985], and that they are epistemic sentences that
are true in D [Reiter, 1988].
In the context of ALP, the diﬀerent interpretations of solving a goal are multi-
plied by the diﬀerent interpretations of satisfying integrity constraints. Compared
with the stable model semantics and ASP, where everyone speaks with one voice,
in ALP everyone argues in a diﬀerent language. So is there any prospect of clearing
up the confusion?
We can start by eliminating the semantic distinction between goals and in-
tegrity constraints. Arguably, the distinction is mainly a pragmatic one between
goals G that are one oﬀ(or ad hoc) and goals I that are persistent and need
to be maintained. As a consequence, the deﬁnition of abductive solution can be
simpliﬁed:
Given an abductive framework ⟨P, I, A⟩and a set G of goal clauses, an
abductive solution is a subset ∆of A, such that P ∪∆satisﬁes G ∪I.
The hard part of the problem remains: How to understand integrity satisfaction?
Having been involved in the early debates about the semantics of integrity con-
straints, and contributed to proof procedures for both integrity checking [Sadri and

Logic Programming
559
Kowalski, 1988] and ALP [Fung and Kowalski, 1997], I am now convinced that the
requirement that P ∪∆satisﬁes G ∪I is best understood as G ∪I is true in some
appropriate model M of P ∪∆. This interpretation has the added attraction that,
no matter how M is deﬁned, G ∪I can include arbitrary ﬁrst-order sentences. In
particular, I can include the disjunctive constraints a ∨not a, needed to simulate
the stable model semantics, as was shown by Kakas and Mancarella [1990].
It remains to identify the nature of the model M. In the case of Horn clause
programs P, it is obvious that M should be the minimal model of P ∪∆. Similarly,
in the case of locally stratiﬁed programs, M should be the perfect model of P ∪∆.
But if P is an arbitrary normal logic program, then it is not immediately obvious
whether M should be a stable model or some canonical model, such as the well-
founded model.
However, the correspondence between the stable model semantics and abduction
shows that stable models and abduction are diﬀerent ways of achieving the same
functionality.
Allowing M to be any stable model would be double counting.
This leaves only one sensible alternative, namely restricting M to some canonical
model, with the well-founded model being the strongest candidate. The resulting
combination of logic programs P ∪∆deﬁning well-founded models M in which
ﬁrst-order sentences G ∪I are true is closely related to the combination of ﬁrst-
order logic with inductive deﬁnitions developed by Marc Denecker [1998] and his
colleagues [Denecker et al., 2001].
The argument for understanding abduction
in terms of the well-founded semantics was also made, from the viewpoint of
representing default and hypothetical reasoning, by [Pereira et al., 1991].
8
CONSTRAINT LOGIC PROGRAMMING
Proof procedures for ALP and Constraint Logic Programming (CLP) have much
in common: Both generate a set (or conjunction) C of conditions (abducible or
constraint formulas) such that C solves the initial goal G and C is satisﬁable. In
both cases, they do so by reasoning top-down, backwards from G, incrementally
generating C and testing C for satisﬁability.
Constraints were ﬁrst introduced into LP by Colmerauer [1982] in Prolog II.
Their introduction was motivated mainly by the ineﬃciency of the “occur check”
need to ensure that a term, such as f(X), does not unify with a subterm, such as
X. Clark [1978] had shown that uniﬁcation with the occur check implements the
identity relation for the domain D of Herbrand interpretations, in which ground
terms can be viewed as ﬁnite trees. Colmerauer [1982] showed that uniﬁcation
without the occur check implements the identity relation for the domain D in
which terms can be viewed as possibly inﬁnite, rational trees.
Jaﬀar and Lassez [1987] introduced the CLP Scheme, which generalized the
domain D to an arbitrary model-theoretic structure deﬁning the semantics of con-
straint predicates. The resulting programs are sets P of Horn clauses whose bodies
contain both user-deﬁned predicates and constraint predicates, but whose heads
contain only user-deﬁned predicates.
The most important new instance of the

560
Robert Kowalski
scheme was CLP(R) [Jaﬀar et al., 1992], in which the constraint domain is the set
of real numbers with addition, multiplication, identity and inequality.
The semantics of a CLP program [Jaﬀar et al., 1998] P is given both by a struc-
ture D and a theory T, which is a ﬁrst-order axiomatization of D. The relationship
between D and T is analogous to the relationship between the standard model of
arithmetic and ﬁrst-order Peano arithmetic.
The “algebraic” semantics is deﬁned in terms of truth in the minimal model
of P ∪D, where D is the set of all ground constraint atoms that are true in D.
According to the algebraic semantics, given a constraint logic program P, goal
clause G and constraint formula C:
G is satisﬁable iﬀG is true in the minimal model of P ∪D.
C solves G iﬀC and C →G are true in the minimal model of P ∪D.
According to the “logical” semantics:
G is satisﬁable iﬀP ∪T ⊨G.
C solves G iﬀP ∪T ⊨C →G and C is consistent with P ∪T.
According to the operational semantics:
C solves G iﬀC can be generated by reasoning backwards from G, and
C is satisﬁable, where satisﬁability is determined by a constraint solver
solve(C), which may also simplify C.
There is an obvious parallel here, not only with proof procedures for ALP, but
also with the two main alternative ways of deﬁning the semantics of ALP.
The “algebraic” and “logical” semantics of the CLP Scheme coincide for Horn
clause programs and theories T that satisfy certain natural completeness condi-
tions. But, in the case of CLP programs with negation, the relationship between
the two kinds of semantics is much more problematic, but seems to have received
relatively little attention.
If all these problems and confusions about the semantics of negation are not
enough, there is one more twist to the story.
9
ARGUMENTATION
The proof procedure in the [Eshghi and Kowalski, 1989] paper was intended to
compute the stable model semantics, but failed to implement the disjunctive in-
tegrity constraint in its totality. Phan Minh Dung [1991] showed that it imple-
mented instead a localized form of the disjunctive constraint, to which he and
[Kakas et al., 1992] gave an argumentation interpretation.
Dung [1993, 1995]
generalized this interpretation and developed an abstract argumentation theory,
which has wide-ranging applications beyond logic programming.
In the case of logic programming, given a ground normal program P, an argu-
ment for a claim p supported by assumptions ∆⊆Neg is a ﬁnite tree T labelled
with literals such that:

Logic Programming
561
• p is the root of T.
• Each non-leaf node q of T is the head of some clause q ←B in P, and its
children are the literals in the body B of the clause. If q is a fact, with an
empty body B, then it has a single child labelled by true.
• Each leaf is either the label true or an assumption in ∆.
Viewed abstractly, it is the set ∆of assumptions supporting an argument that
determines whether the argument and its claim are acceptable, and this depends,
in turn, on whether or not ∆is able to defend itself against attack. Given a ground
normal program P:
∆1 ⊆Neg attacks ∆2 ⊆Neg iﬀthere exist an argument for a claim p
supported by ∆1 and an assumption not p ∈∆2.
∆1 ⊆Neg defends ∆2 ⊆Neg iﬀ
∆1 attacks every ∆3 ⊆Neg that attacks ∆2.
The notions of argument, assumption, attack and defence are suﬃcient to recon-
struct not only most logic programming semantics, but also most semantics for
non-monotonic reasoning [Bondarenko et al., 1997]. In the case of logic program-
ming, sets ∆⊆Neg of assumptions that do not attack themselves correspond
to three-valued Herbrand interpretations M = M pos ∪M neg whose true atoms
M pos are supported by their false atoms M neg = ∆. If ∆also contains all the as-
sumptions that it defends, then the corresponding interpretation is a three-valued
stable model, as deﬁned by Przymusinski [1990] and shown by [Wu et al., 2009],
and therefore a partial stable model, as deﬁned by Sacca and Zaniolo [1990, 1991].
Sets ∆that not only contain all the assumptions that they defend, but also
attack all the assumptions that they do not contain, correspond to two-valued,
stable models:
Given a ground logic program P:
If ∆⊆Neg does not attack ∆, and ∆attacks Neg−∆, then
M = {p | ∆supports an argument for p} is a stable model of P.
If M is a stable model of P, then
∆= {not p ∈Neg | p /∈M} does not attack ∆, and ∆attacks Neg−∆.
From an argumentation point of view, the stable model semantics is all-out warfare:
For a set of assumptions ∆to correspond to a stable model, every assumption not
p has to take a side: Either not p is in ∆, or ∆attacks {not p}.
Dung [1993; 1995] argued that the stable model semantics is too extreme: It is
suﬃcient for a set of assumptions to defend itself against all attacks:
∆⊆Neg is admissible iﬀ∆defends ∆, and ∆does not attack ∆.

562
Robert Kowalski
Dung [1991] showed that the proof procedure of [Eshghi and Kowalski, 1989] is
sound with respect to the admissibility semantics.
Dung also gave an abductive interpretation of the well-founded semantics in
[Dung, 1991], and an argumentation interpretation in [Dung, 1995]. Whereas in
the admissibility and stable semantics an assumption can be used in its own self-
defense, in the well-founded semantics an assumption has to be defended by other
assumptions:
A set ∆of assumptions is well-founded iﬀ∆is the smallest set of
assumptions that contains all the assumptions that it defends.
This is similar to Przymusinski’s [1990] characterization of the well-founded se-
mantics in terms of three-valued stable models.
The well-founded set can be constructed bottom-up by starting with the empty
set {} of assumptions and repeatedly adding new assumptions defended by the
previously added set of assumptions, until no further assumptions can be added.
This bottom-up construction is similar to van Gelder’s [1993] alternating ﬁxed
point characterisation of the well-founded semantics.
10
CONCLUSIONS
This history covers some of the highlights of the development of logic programming
from the late 1960s into the 21st century. It focuses on a number of issues that
are still relevant today, in particular on:
• the diﬀerence between solving a goal by theorem-proving and solving it by
model generation,
• the diﬀerence between solving a goal top-down and solving it bottom-up.
• the relationship between declarative and procedural representations.
Perhaps the biggest change over the years has been the move away from viewing
computation as deduction to viewing it as model generation. The seeds of this
change were planted with the minimal model semantics of Horn clauses in 1976,
but really got going only in the 1980s, when it was applied to the semantics of
negation as failure. As a consequence, except perhaps in the context of CLP, the
completion semantics of negation has been overshadowed by the model-theoretic
approach.
The recent revival of Datalog [Green et al., 2013] suggests that the old promise
that logic programming can unify programming and databases may have new
prospects. However, the query evaluation strategies of Datalog are mainly bottom-
up with magic set transformations used to simulate top-down execution. Is this
simulation of top-down execution really necessary? Or might some more direct
combination of top-down and bottom-up execution be more useful.

Logic Programming
563
Recent years have also seen a shift away from reconciling declarative and pro-
cedural representations to a more purely declarative approach. In the meanwhile,
imperative languages dominate the world of practical computing. Does this mean
that logic programming is destined to become a niche technology concerned exclu-
sively with declarative representations, based on ASP and Datalog? Or will it split
into separate declarative and procedural camps, with procedural representations
being relegated to the domain of Prolog? Or might it still be possible to recon-
cile declarative and procedural representations, perhaps by combining Prolog-like
top-down execution with tabling?
ACKNOWLEDGEMENTS
Many thanks to Maurice Bruynooghe, Keith Clark, Marc Denecker, Phan Minh
Dung, Maarten van Emden, Michael Gelfond, Pat Hayes, Tony Kakas, Vladimir
Lifschitz, Luis Pereira, Alan Robinson, John Schlipf, J¨org Siekmann, Allen Van
Gelder and David Scott Warren for their helpful comments on the paper. Special
thanks to Luis and Maarten for acting as the oﬃcial readers of the paper.
It is important to stress that, in this case more than in most others, the author
alone is fully responsible for any errors of fact or judgement.
BIBLIOGRAPHY
[Anderson and Belnap, 1962] A. R. Anderson and N. D. Belnap. The pure calculus of entail-
ment.Journal of Symbolic Logic, 19-52, 1962.
[Andr´eka and N´emeti, 1978] H. Andr´eka and I. N´emeti. The Generalized Completeness of Horn
Predicate Logic as a Programming Language. Acta Cybernetica, 4:3–10, 1978. (This is the
publish version of a 1975 report entitled “General Completeness of Prolog” Department of
Artiﬁcial Intelligence, University of Edinburgh).
[Apt and Bol, 1994] K. R. Apt and R. Bol. Logic programming and negation: a survey. Journal
of Logic Programming 19-20, 9–71, 1994.
[Apt et al., 1988] K. R. Apt, H. Blair, and A. Walker. Towards a Theory of Declarative Knowl-
edge. In: J. Minker (ed.), Foundations of Deductive Databases and Logic Programming, pp.
89–148. Morgan Kaufman, Los Altos, CA, 1988.
[Apt and van Emden, 1982] K. R. Apt and M. van Emden. Contributions to the Theory of Logic
Programming. Journal of the ACM 29, 3, 841–862, 1982.
[Bancilhon et al., 1985] F. Bancilhon, D. Maier, Y. Sagiv, and J. D. Ullman. Magic sets and
other strange ways to implement logic programs. In Proceedings of the ﬁfth ACM SIGACT-
SIGMOD symposium on Principles of database systems (pp. 1-15). ACM, 1985.
[Bondarenko et al., 1997] A. Bondarenko, P. M. Dung, R. Kowalski, and F. Toni. An Abstract
Argumentation-theoretic Approach to Default Reasoning. Journal of Artiﬁcial Intelligence 93
(1-2), 63-101, 1997.
[Boyer and Moore, 1972] R. S. Boyer and J. S. Moore. The sharing of structure in theorem-
proving programs. Machine Intelligence, 7, 101–116, 1972.
[Brewka et al., 2011] G. Brewka, T. Eiter, and M. Truszczy´nski. Answer set programming at a
glance. Communications of the ACM, 54(12), 92-103, 2011.
[Bruynooghe and Pereira, 1984] M. Bruynooghe and L. M. Pereira. Deduction revision through
intelligent backtracking. In J. Campbell, ed., “Issues in Prolog Implementation”. Ellis Hor-
wood, 1984.

564
Robert Kowalski
[Bry et al., 2007] F. Bry, N. Eisinger, T. Eiter, T. Furche, G. Gottlob, C. Ley, and F. Wei
Foundations of rule-based query answering. In Proceedings of the Third international summer
school conference on Reasoning Web (pp. 1-153). Springer-Verlag, 2007.
[Bundy and Welham, 1981] A. Bundy and B. Welham. Using meta-level inference for selec-
tive application of multiple rewrite rule sets in algebraic manipulation. Artiﬁcial Intelli-
gence, 16(2), 189-211, 1981.
[Ceri et al., 1990] S. Ceri, G. Gottlob, and L. Tanca. Logic Programming and Databases. Surveys
in Computer Science. Springer-Verlag, 1990.
[Chandra and Harel, 1982] A. K. Chandra and D. Harel. Structure and complexity of relational
queries. Journal of Computer and System Sciences 25, 1 (Aug.), 99–128, 1982.
[Chandra and Harel, 1985] A. K. Chandra and D. Harel. Horn clause queries and generaliza-
tions. Journal of Logic Programming 2, 1 (April), 1–15, 1985. A preliminary version of this
paper, “Horn Clauses and the Fixpoint Query Hierarchy,” appeared in the ACM Symp. on
Principles of Database Systems, 1982.
[Charniak et al., 1971] E. Charniak, G. Sussman, G. and T. Winograd. MICRO-PLANNER
reference manual (Vol. 203). MIT AI Memo, 1971.
[Chen and Warren, 1996] W. Chen and D. Warren. Tabled Evaluation with Delaying for General
Logic Programs. JACM 43, 20–74, 1996.
[Clark, 1978] K. L. Clark/ Negation by failure. In Gallaire, H. and Minker, J. [eds], Logic and
Databases, Plenum Press, 293-322, 1978.
[Clark, 1980] K. L. Clark. Predicate logic as a computational formalism (Doctoral dissertation,
Queen Mary, University of London), 1980.
[Clark and Gregory, 1983] K. L. Clark and S. Gregory). Parlog: A parallel logic programming
language. Imperial College of Science and Technology Department of Computing, 1983.
[Clark and Gregory, 1986] K. L. Clark and S. Gregory. Parlog:
Parallel programming in
logic. ACM Transactions on Programming Languages and Systems (TOPLAS), 8(1), 1-49,
1986.
[Clark et al., 1982] K. L. Clark, F. G. McCabe, and S. Gregory. IC-Prolog language features,
Logic programming, ed. Clark/Tarnlund, 254-266, 1982.
[Clark and T¨arnlund, 1978] K. L. Clark and S.-A. T¨arnlund. A ﬁrst-order theory of data and
programs. In Proceedings of the IFIP Congress 77, 939-944, 1978.
[Codd, 1970] E. F. Codd. Relational Completeness of Data Base Sublanguages. Database Sys-
tems: 65–98, 1970.
[Codd, 1972] E. F. Codd. Relational completeness of data base sublanguages (pp. 65-98). IBM
Corporation, 1972.
[Cohen, 1988] J. Cohen. A View of the Origins and Development of Prolog, Communications
ACM, vol 31, pp 26-36, 1988.
[Colmerauer, 1969] A. Colmerauer. Les syst`emes Q ou un formalisme pour analyser et
synth´etiser des phrases sur ordinateur. Mimeo, Montr´eal, 1969.
[Colmerauer, 1970] A. Colmerauer. Total Precedence Relations, Journal ACM 1970, vol 17, pp
14-30, 1970.
[Colmerauer, 1975] A. Colmerauer. Les grammaires de metamorphose, Groupe d’Intelligence
Artiﬁcielle, University de Marseille-Luminy (November 1975). Appears as ‘Metamorphosis
Grammars” in: L. Bolc (Ed.), Natural Language Communication with Computers, (Springer,
1978).
[Colmerauer, 1982] A. Colmerauer. Prolog II: Reference manual and theoretical model. Groupe
D’intelligence Artiﬁcielle, Facult´e Des Sciences De Luminy, Marseille, 1982.
[Colmerauer et al., 1973] A. Colmerauer, H. Kanoui, R. Pasero, and P. Roussel. Un syst`eme
de communication homme-machine en fran¸cais. Research report, Groupe d’Intelligence Arti-
ﬁcielle, Universit´e d’Aix-Marseille II, Luminy, 1973.
[Colmerauer and Roussel, 1996] A. Colmerauer and P. Roussel. The birth of Prolog. In History
of programming languages—II (pp. 331-367). ACM, 1996.
[Console et al., 1991] L. Console, D. Theseider Dupre, and P. Torasso. On the relationship be-
tween abduction and deduction. Journal of Logic and Computation 2(5) 661-690, 1991.
[Costantini, 2002] S. Costantini. Meta-reasoning: A Survey. In Kakas, A.C., Sadri, F. (Eds.):
Computational Logic: Logic Programming and Beyond. Springer Verlag. Vol. 2. 253-288, 2002.
[Chen and Warren, 1996] W. Chen and D. Warren. Tabled evaluation with delaying for general
logic programs. JACM 43, 1 (January), 20–74, 1996.

Logic Programming
565
[Dantsin et al., 1997] E. Dantsin, T. Eiter, G. Gottlob, and A. Voronkov. Complexity and ex-
pressive power of logic programming. In Proceedings of the Twelfth Annual IEEE Conference
on Computational Complexity, June 24–27, 1997, Ulm, Germany, pp. 82–101. IEEE Computer
Society Press, 1997.
[Davis, 1980] M. Davis. The mathematics of non-monotonic reasoning. Artiﬁcial Intelligence,
13(1), 73-80, 1980.
[Dean and Ghemawat, 2008] J. Dean and S. Ghemawat. MapReduce: simpliﬁed data processing
on large clusters. Communications of the ACM, 51(1), 107-113, 2008.
[Denecker, 1998] M. Denecker. The well-founded semantics is the principle of inductive deﬁni-
tion. In Logics in Artiﬁcial Intelligence, J. Dix, L. Farinas del Cerro, and U. Furbach, Eds.
Lecture Notes in Artiﬁcial Intelligence, vol. 1489. Springer-Verlag, 1–16, 1998.
[Denecker et al., 2001] M. Denecker, M. Bruynooghe, and V. Marek. Logic programming re-
visited:
logic programs as inductive deﬁnitions. ACM Transactions on Computational
Logic, 2(4), 623-654, 2001.
[Denecker and Kakas, 2002] M. Denecker and A. Kakas. Abduction in logic programming. In
Computational Logic: Logic Programming and Beyond (pp. 402-436). Springer Berlin Heidel-
berg, 2002.
[Denecker et al., 2009] M. Denecker, J. Vennekens, S. Bond, M. Gebser, and M. Truszczynski.
The second Answer Set Programming competition. In LPNMR, E. Erdem, F. Lin, and T.
Schaub, Eds. LNCS, vol. 5753. Springer, 637-654, 2009.
[Dung, 1991] P. M. Dung. Negation as hypothesis: an abductive foundation for logic program-
ming. Proc. 8th International Conference on Logic Programming. MIT Press, 1991.
[Dung, 1993] P. M. Dung. On the acceptability of arguments and its fundamental roles in non-
monotonic reasoning and logic programming, Proceedings of IJCAI 1993, pp. 852-857, Morgan
Kaufmann, 1993.
[Dung, 1995] P. M. Dung. On the acceptability of arguments and its fundamental role in non-
monotonic reasoning, logic programming and n-person games. Artiﬁcial intelligence, 77(2),
321-357, 1995.
[Eiter et al., 2009] T. Eiter, G. Ianni, and T. Krennwallner. Answer set programming: A primer.
In Reasoning Web. Semantic Technologies for Information Systems (pp. 40-110). Springer
Berlin Heidelberg, 2009.
[Elcock, 1990] E. W. Elcock. Absys: The First Logic Programming Language—a Retrospective
and a Commentary. Journal of Logic Programming, 9(1), 1-17, 1990.
[van Emden, 2006] M. van Emden. The Early Days of Logic Programming: A Personal Per-
spective. The Association of Logic Programming Newsletter, Vol. 19 n. 3, August 2006.
http://www.cs.kuleuven.ac.be/\textasciitildedtai/projects/ALP/newsletter/aug06/
[van Emden and Kowalski, 1976] M. van Emden and R. Kowalski. The Semantics of Predicate
Logic as a Programming Language JACM , Vol. 23, No. 4, 733-742. 1976. Earlier version
DCL Memo. School of Artiﬁcial Intelligence, University of Edinburgh (1974)
[Eshghi and Kowalski, 1989] K. Eshghi and R. Kowalski. Abduction Compared with Negation
by Failure. In Sixth International Conference on Logic Programming, (eds. G. Levi and M.
Martelli) MIT Press, 234-254, 1989.
[Feferman, 1970] S. Feferman. Formal theories for transﬁnite iterations of generalised inductive
deﬁnitions and some subsystems of analysis. In Intuitionism and Proof theory, A. Kino, J.
Myhill, and R. Vesley, Eds. North Holland, 303–326, 1970.
[Fitting, 1985] M. Fitting. A Kripke-Kleene semantics for logic programs*. The Journal of Logic
Programming, 2(4), 295-312, 1985.
[Fung and Kowalski, 1997] T. H. Fung and R. Kowalski. The IFF Proof Procedure for Abductive
Logic Programming. Journal of Logic Programming, 1997.
[Gallaire and Minker, 1978] H. Gallaire and J. Minker. Logic and Data Bases. Plenum Press,
New York, 1978.
[Gallaire and Lasserre, 1982] H. Gallaire and C. Lasserre. Metalevel control for logic pro-
grams.Logic Programming, 173-185, 1982.
[Gelernter, 1963] H. Gelernter. Machine generated problem solving graphs. In Proc. Symp,
Math. Theory of Automata (pp. 179-203), 1963.
[Gelfond, 1987] M. Gelfond. On Stratiﬁed Autoepistemic Theories. In Proc. of AAAI87. Morgan
Kaufman, 207–211, 1987.

566
Robert Kowalski
[Gelfond and Lifschitz, 1988] M. Gelfond and V. Lifschitz. The stable model semantics for logic
programming. In: Proceedings of the Fifth International Conference on Logic Programming
(ICLP), 1070-1080, 1988.
[Gelfond and Lifschitz, 1991] M. Gelfond and V. Lifschitz. Classical negation in logic programs
and disjunctive databases. New Generation Computing 9, 3–4, 365–386, 1991.
[Green, 1969] C. Green. Application of theorem proving to problem solving. Proceedings of
the 1st International Joint Conference on Artiﬁcial Intelligence. Morgan Kaufmann. 219-239,
1996.
[Green et al., 2013] T. J. Green, S. S. Huang, B. T. Loo and W. Zhou. Datalog and Recursive
Query Processing. Foundations and Trends in Databases, 5(2), 105-195, 2013.
[Harel, 1980] D. Harel. Review on Logic and Data Bases, Computing Reviews #36,671 (Aug
1980), 367-369.
[Hayes, 1973] P. J. Hayes. Computation as Deduction, Proceedings 2nd MFCS Symposium,
Czechoslovakia Academy of Sciences, Prague, Czechoslovakia, 1973.
[Hayes and Kowalski, 1971] P. J. Hayes and R. A. Kowalski. Lecture Notes on Automatic
Theorem-Proving. DCL Memo 40. School of Artiﬁcial Intelligence, University of Edinburgh,
1971.
[Hellerstein, 2010] J. M. Hellerstein. The Declarative Imperative: Experiences and Conjectures
in Distributed Logic, SIGMOD Record 39(1), 2010.
[Hewitt, 1971] C. Hewitt. Procedural Embedding of Knowledge In Planner. Proceedings of the
2nd International Joint Conference on Artiﬁcial Intelligence. Morgan Kaufmann, 1971.
[Hewitt, 2009] C. Hewitt. Middle History of Logic Programming: Resolution, Planner, Edin-
burgh LCF, Prolog, Simula, and the Japanese Fifth Generation Project, 2009. arXiv preprint
arXiv:0904.3036
[Hill, 1974] R. Hill. LUSH Resolution and its Completeness. DCL Memo 78. School of Artiﬁcial
Intelligence, University of Edinburgh, 1974.
[Jaﬀar and Lassez, 1987] J. Jaﬀar and J. L. Lassez. Constraint logic programming. In Proceed-
ings of the 14th ACM SIGACT-SIGPLAN symposium on Principles of programming lan-
guages (pp. 111-119). ACM, 1987.
[Jaﬀar et al., 1998] J. Jaﬀar, M. Maher, K. Marriott, and P. Stuckey. The semantics of constraint
logic programs1. The Journal of Logic Programming, 37(1-3), 1-46, 1998.
[Jaﬀar et al., 1992] J. Jaﬀar, S. Michaylov, P. Stuckey, and R. H. Yap The CLP(R) language
and system. ACM Transactions on Programming Languages and Systems (TOPLAS), 14(3),
339-395, 1992.
[Kakas and Mancarella, 1990] A. C. Kakas and P. Mancarella. Generalized Stable Models: A
Semantics for Abduction. In ECAI (Vol. 90, pp. 385-391), 1990.
[Kakas et al., 1992] A. C. Kakas,
R. Kowalski,
and F. Toni. Abductive logic program-
ming.Journal of Logic and Computation, 2(6), 719-770, 1992.
[Kakas et al., 1998] A. C. Kakas, R. Kowalski, and F. Toni. The Role of Logic Programming in
Abduction, Handbook of Logic in Artiﬁcial Intelligence and Programming 5, Oxford Univer-
sity Press, 235-324, 1998.
[Kowalski, 1970] R. Kowalski. Search strategies for theorem proving. Machine Intelligence, 5,
181-201, 1970.
[Kowalski, 1972] R. Kowalski. The Predicate Calculus as a Programming Language (abstract).
Procedings of the First MFCS Symposium, Jablonna, Poland, 1972.
[Kowalski, 1974] R. Kowalski. Predicate logic as a programming language. In Proceedings of
IFIP 1974 [Stockholm, Sweden). North-Holland, Amsterdam, 1974, 569-574. This is the pub-
lished version of DCL Memo 70, School of Artiﬁcial Intelligence, Univ. of Edinburgh, U.K.,
Nov. 1973.
[Kowalski, 1978] R. Kowalski. Logic for data description, in: H. Gallaire and J. Minker (eds.),
Logic and Data Bases, Plenum Press, New York, pp. 77-103, 1978.
[Kowalski, 1979a] R. Kowalski. Algorithm = Logic+ Control. CACM, 22(7), 424-436, 1979.
[Kowalski, 1979b] R. Kowalski. Logic for Problem Solving. North Holland Elsevier, 1979. This is
an expanded version of DCL Memo 75, Department of Artiﬁcial Intelligence, U. of Edinburgh
(1974). Also http://www.doc.ic.ac.uk/\textasciitilderak/.
[Kowalski, 2011] R. Kowalski. Computational Logic and Human Thinking: How to be Artiﬁ-
cially Intelligent, Cambridge University Press, 2011.
[Kowalski, 2013] R. Kowalski. Logic Programming in the 1970s. In: P. Cabalar and T.C. Son
(eds.) LPNMR 2013. Springer Verlag, 2013.

Logic Programming
567
[Kowalski and Kuehner, 1971] R. Kowalski and D. Kuehner. Linear Resolution with selection
function, Artiﬁcial Intelligence, vol 2, 1971, 227-260, 1971.
[Kreisel, 1963] G. Kreisel. Generalized inductive deﬁnitions. Tech. rep., Stanford University,
1963.
[Kuehner, 1969] D. Kuehner.
Bi-directional search with Horn clauses. Edinburgh University,
1969.
[Kunen, 1987] K. Kunen. Negation in logic programming. The Journal of Logic Program-
ming, 4(4), 289-308, 1987.
[Lifschitz, 1988] V. Lifschitz. On the Declarative Semantics of Logic Programs with Negation,
in: J. Minker (ed.), Foundations of Deductive Databases and Logic Programming, Morgan
Kaufmann, Los Altos, CA, 1988, pp. 177-192.
[Lloyd, 1985/1987] J. W. Lloyd. Foundations of Logic Programming. New York: Springer Verlag
(1985 and 1987)
[Lloyd and Torpor, 1985] J. W. Lloyd and R. W. Topor. A Basis for Deductive Database Sys-
tems. J. Logic Programming 2: 93–109, 1985.
[Loveland, 1968] D. W. Loveland. Mechanical theorem-proving by model elimination. Journal
of the ACM, 15, 236-251, 1968.
[Loveland, 1970] D. W. Loveland. A Linear Format for Resolution. In Symposium on Automatic
Demonstra-tion, pp. 147-162. Springer, Berlin Heidelberg, 1970.
[Loveland, 1972] D. W. Loveland. A Unifying View of Some Linear Herbrand Procedures.
JACM, 19(2), 366-384, 1972.
[Luckham, 1970] D. Luckham. Reﬁnement Theorems in Resolution Theory. In Symposium on
Automatic Demonstration (pp. 163-190). Springer, Berlin Heidelberg, 1970.
[Mari¨en et al., 2004] M. Mari¨en, D. Gilis, & M. Denecker. On the relation between ID-logic and
answer set programming. In Logics in Artiﬁcial Intelligence (pp. 108-120). Springer Berlin
Heidelberg, 2004.
[Marek and Truszczy´nski, 1999] V. W. Marek and M. Truszczy´nski Stable models and an al-
ternative logic programming paradigm. In The Logic Programming Paradigm (pp. 375-398).
Springer Berlin Heidelberg, 1999.
[McCarthy and Hayes, 1969] J. McCarthy and P. Hayes.
Some philosophical problems from
the standpoint of artiﬁcial intelligence. In Meltzer, B. and Michie, D. and Swann, M. (eds.)
Machine intelligence 4, Edinburgh University Press (pp. 463-502), 1969.
[McCarthy, 1980] J. McCarthy. Circumscription — a form of non-monotonic reasoning. Artiﬁcial
intelligence, 13(1), 27-39, 1980.
[McDermott and Doyle, 1980] D. McDermott and J. Doyle. Non-monotonic logic I. Artiﬁcial
intelligence,13(1), 41-72, 1980.
[Minker, 1996] J. Minker. Logic and databases: A 20 year retrospective (pp. 1-57). Springer
Berlin Heidelberg, 1996.
[Minsky, 1975] M. Minsky. A framework for representing knowledge. In Winston, P. H. (ed.)
The psychology of Computer Vistino. McGraw-Hill, New York, 211-277, 1975.
[Moore, 1985] R. C. Moore. Semantical considerations on nonmonotonic logic. Artiﬁcial intelli-
gence, 25(1), 75-94, 1985.
[Naqvi, 1988] S. A. Naqvi. A Logic for Negation in Database Systems, in: J. Minker (ed.),
Foundations of Deductive Databases and Logic Programming, Morgan Kaufmann, Los Altos,
CA, 1988, pp. 378-387.
[Nicolas and Gallaire, 1978] J. M. Nicolas and H. Gallaire. Database: Theory vs. Interpretation.
In: Gallaire, H., Minker, J. (eds.), Logic and Databases, Plenum, New York, 1978.
[Niemel¨a, 1999] I. Niemel¨a. Logic programs with stable model semantics as a constraint pro-
gramming paradigm. Annals of Mathematics and Artiﬁcial Intelligence, 25(3-4), 241-273,
1999.
[Nilsson, 1968] N. J. Nilsson. Searching problem-solving and game-playing trees for minimal
cost solutions. In IFIP Congress (2) (pp. 1556-1562, 1968).
[Peirce, 1931] C. S. Peirce. Collected Papers. C. Hartshorn & P. Weiss (eds.) Cambridge, MA:
Harvard University Press, 1931.
[Pereira, 1984] L. M. Pereira. Logic Control with Logic, in: Implementations of Prolog, pp.
177-193, J. Campbell (ed.), Ellis Horwood, 1984.
[Pereira et al., 1991] L. M. Pereira, J. N. Apar´ıcio, and J. J. Alferes. Hypothetical Reasoning
with Well Founded Semantics. In SCAI (pp. 289-300), 1991.

568
Robert Kowalski
[Pereira and Warren, 1980] F. C. Pereira and D. H. Warren. Deﬁnite clause grammars for lan-
guage analysis—a survey of the formalism and a comparison with augmented transition net-
works. Artiﬁcial intelligence, 13(3), 231-278, 1980.
[Poole, 1988] D. Poole. A logical framework for default reasoning. Artiﬁcial intelligence, 36(1),
27-47, 1988.
[Poole et al., 1987] D. Poole, R. Goebel, and R. Aleliunas. Theorist: a logical reasoning system
for defaults and diagnosis. In N. Cercone and G. McCalla (Eds.) The Knowledge Frontier:
Essays in the Representation of Knowledge, Springer Verlag, New York, 331-352, 1987.
[Przymusinski, 1988] T. C. Przymusinski. On the Declarative Semantics of Deductive Databases
and Logic Programs, In: J. Minker (ed.), Foundations of Deductive Databases and Logic
Programming, Morgan Kaufmann, Los Altos, CA, 1988, pp. 193-216.
[Przymusinski, 1990] T. C. Przymusinski. The well-founded semantics coincides with the three-
valued stable semantics. Fundamenta Informaticae, 13(4):445–463, 1990.
[Ramakrishnan and Ullman, 1995] R. Ramakrishnan and J. D. Ullman. A survey of deductive
database systems. The journal of logic programming, 23(2), 125-149, 1995.
[Reiter, 1971] R. Reiter. Two Results on Ordering for Resolution with Merging and Linear
Format. JACM 18(4), 630-646, 1971.
[Reiter, 1978] R. Reiter. On Closed World Data Bases. In: Gallaire H. and Minker J. (eds.),
Logic and Data Bases, Plenum Press, New York, pp. 55-76, 1978.
[Reiter, 1980] R. Reiter. A logic for default reasoning. Artiﬁcial intelligence, 13(1), 81-132, 1980.
[Reiter, 1988] R. Reiter. On Integrity Constraints. In: 2nd Conference on Theoretical Aspects
of Reasoning about Knowledge, 97—111, 1988.
[Robinson, 1965a] J. A. Robinson. A Machine-Oriented Logic Based on the Resolution Principle.
JACM, 12(1) 23–41, 1965.
[Robinson, 1965b] J. A. Robinson. Automatic deduction with hyper-resolution, International J.
Computer Math. 1, 3. 227-234, 1965.
[Sacca and Zaniolo, 1990] D. Sacca and C. Zaniolo. Stable models and non-determinism in logic
programs with negation. In: Proceedings of the Ninth ACM SIGACT-SIGMOD-SIGART
Symposium on Principles of Database Systems, ACM, 205-217, 1990.
[Sacca and Zaniolo, 1991] D. Sacca and C. Zaniolo. Partial Models and Three-Valued Models
in Logic Programs with Negation. In LPNMR, 87-101, 1991.
[Sadri and Kowalski, 1988] F. Sadri and R. Kowalski. A Theorem-Proving Approach to
Database Integrity. In: Minker, J. [ed.], Foundations of Deductive Databases and Logic Pro-
gramming, Morgan Kaufmann, 313-362, 1988.
[Sagonas et al., 1994] K. Sagonas, T. Swift, and D. S. Warren. XSB as an Eﬃcient Deductive
Database Engine. Pro-ceedings of the ACM SIGMOD International Conference on the Man-
agement of Data pp. 442-453, 1994.
[Satoh and Iwayama, 1992] K. Satoh and N. Iwayama. Computing abduction using the TMS.
Proceedings of ICLP. MIT Press 505-518, 1992.
[Scott, 1970] D. Scott. Outline of a mathematical theory of computation. Proc. of the Fourth
Annual Princeton Conference on Information 5ciences and Systems, pp. 169-176, 1970.
[Shepherdson, 1988] J. Shepherdson. Negation in Logic Programming. In: Minker, J. [ed.], Foun-
dations of Deductive Databases and Logic Programming, Morgan Kaufmann, 19-88, 1988.
[Smullyan, 1956] R. M. Smullyan. On deﬁnability by recursion (Abstract 782t). Bulletin AMS
62, 601, 1956.
[Smullyan, 1961] R. M. Smullyan. Theory of Formal Systems. Annals of Mathematical Studies
Vol 47. Princeton University Press, Princeton, New Jersey, 1961.
[Tamaki and Sato, 1986] H. Tamaki and T. Sato. OLD Resolution with Tabulation. Third In-
ternational Conference on Logic Programming pp. 84-98. Springer, Berlin, Heidelberg, 1986.
[Tekle and Liu, 2011] K. T. Tekle and Y. A. Liu. More Eﬃcient Datalog Queries: Subsumptive
Tabling beats Magic Sets. In Proceedings of SIGMOD International Conference on Manage-
ment of Data 661-672, 2011.
[Thagard, 2005] P. Thagard. Mind: Introduction to Cognitive Science. Second edition. MIT
Press, 2005.
[Ueda, 1986] K. Ueda. Guarded Horn Clauses: A Parallel Logic Programming Language with the
Concept of a Guard. ICOT Technical Report TR-208, Institute for New Generation Computer
Technology (ICOT), Tokyo, 1986. Revised version in Programming of Future Generation
Computers, Nivat, M. and Fuchi, K. (eds.), North-Holland, Amsterdam, pp.441-456, 1988.

Logic Programming
569
[Van Gelder, 1989] A. Van Gelder. Negation as failure using tight derivations for general logic
programs, The Journal of Logic Programming, Volume 6, Issues 1–2, Pages 109-133, 1989.
[Van Gelder, 1993] A. Van Gelder. The alternating ﬁxpoint of logic programs with negation,
Journal of Computer and System Sciences, Volume 47, Issue 1, Pages 185-221, 1993.
[Van Gelder et al., 1991] A. Van Gelder, K. A. Ross, and J. Schlipf. The Well-Founded Seman-
tics for General Logic Programs. JACM 38, 3, 620–650, 1991.
[Warren, 1978] D. H. Warren. Applied logic: its use and implementation as a programming tool.
Ph.D. thesis. University of Edinburgh. Also Technical Note 290, AI Center, SRI International,
1978.
[Warren, 1983] D. H. Warren). An abstract Prolog instruction set (Vol. 309). Menlo Park, Cal-
ifornia: SRI International, 1983.
[Warren et al., 1977] D. H. Warren, L. M. Pereira, and F. Pereira. Prolog −the language and
its implementation compared with Lisp. In ACM SIGPLAN Notices (Vol. 12, No. 8, pp.
109-115). ACM. page 48. Warren et al.,. 1977.
[Winograd, 1971] T. Winograd. Procedures as a Representation for Data in a Computer Pro-
gram for Under-standing Natural Language. MIT AI TR-235 (1971) Also: Understanding
Natural Language. Academic Press, New York, 1972.
[Wos et al., 1965] L. Wos, G. A . Robinson, and D. F. Carson. Eﬃciency and completeness of
the set of support strategy in theorem proving. Journal of the ACM (JACM), 12(4), 536-541,
1965.
[Wu et al., 2009] Y. Wu, M. Caminada, and D. M. Gabbay. Complete extensions in argumenta-
tion coincide with 3-valued stable models in logic programming.Studia logica, 93(2-3), 383-403,
2009.
[Zamov and Sharonov, 1969] N. K. Zamov and V. I. Sharonov. On a Class of Strategies for the
Resolution Method. Zapiski Nauchnykh Seminarov POMI, 16, 54-64, 1969.


LOGIC AND DATABASES:
A HISTORY OF DEDUCTIVE DATABASES
Jack Minker, Dietmar Seipel and Carlo Zaniolo
Reader: Georg Gottlob
1
INTRODUCTION
The ﬁeld of logic and databases is an outgrowth of work in logic programming
and databases. It is assumed that the reader is familiar with work in relational
databases as developed by Codd [Codd, 1970] and described in many textbooks
on databases (see, e.g., Ullman [Ullman, 1990]). We also assume familiarity with
logic programming as described by Kowalski in his book [Kowalski, 1974] and in
his Chapter of this handbook, and by Lloyd [Lloyd, 1987]. Whereas Codd based
his work on the relational calculus and the relational algebra, the work in logic and
databases is based on logic programming and there predicate calculus. As we will
show, Codd’s relational databases is a subset of logic programming. Amalgamating
the two ﬁelds, more powerful systems can be developed than by either approach
alone, e.g., knowledge–based systems, and nonmonotonic reasoning systems.
Early work in relational databases emphasized implementation techniques and
optimization methods using a bottom–up approach to query answering. This work
led to eﬃcient database systems of importance to commercial applications. Logic–
based eﬀorts emphasized a top–down procedure and theoretical issues related to
artiﬁcial intelligence problems. The major contribution of Codd was to under-
stand that the application of the relational calculus and the relational algebra to
databases could lead to the development of a declarative query language and op-
timization techniques. This resulted in the implementation of sophisticated com-
mercially available systems that handled large relational tables. It was achieved
by the separation of the data model from the physical model through a data deﬁni-
tion language specifying the structural aspects of the data and a data modiﬁcation
language for accessing or updating the data. This led to queries written in a declar-
ative language to be translated into a computer language. The separation of the
logical and physical levels, called the data independence principle, permitted users
to retrieve data without the need to write complex programs and thus is more
convenient for database users as they were isolated from the structural aspects
of the data. The development of optimization techniques enabled the high–level
Handbook of the History of Logic. Volume 9: Computational Logic. 
Volume editor: Jörg Siekmann 
Series editors: Dov M. Gabbay and John Woods 
Copyright © 2014 Elsevier BV. All rights reserved.

logic–based queries of relational databases to be competitive with the low–level
procedural queries of network and hierarchical databases.
Those who approached databases through logic were inﬂuenced by work in the-
orem proving by J. Alan Robinson [Robinson, 1965] who developed a uniform
method to perform deduction in logic. The development of the programming lan-
guage Prolog by Colmerauer and his group [Colmerauer et al., 1973], was important
for artiﬁcial intelligence applications. Whereas relations in relational databases
were generally function–free, Prolog permitted function symbols in atoms. In ad-
dition, Prolog was a top–down problem solver, rather than a bottom–up approach.
Recursion was also permitted in logic programming, while early relational systems
did not permit recursion in queries or views. Early emphasis by the logic program-
ming community was on answering questions such as: What is a query? What is
an answer to a query? How does one handle negation? How does one deﬁne the
semantics of a logic program?
The database community in the early days believed that logic programming
approaches were not relevant to their eﬀorts.
It took several years before this
community recognized that the work in logic–based systems was indeed relevant
and could handle problems that relational databases could not. On the other hand,
it took the logic–based community several years to understand that there was a
need for commercially available systems to handle large scale industrial problems
that could not be handled by relational database technology.
In 1977, Gallaire and Nicolas, with assistance by Minker, organized a workshop
on deductive databases in Toulouse, France. Gallaire and Minker [Gallaire and
Minker, 1978] edited papers from the workshop. In the foreword to the book they
stated that logic and databases was a ﬁeld in its own right1. The ﬁeld was in-
ﬂuenced by research in relational databases, theorem proving, logic programming,
and nonmonotonic reasoning.
As will be discussed subsequently, all results in logic programming apply to re-
lational, deductive, and disjunctive deductive databases. Whereas logic programs
permit function symbols in logical atoms, deductive and disjunctive deductive
databases are generally function–free and deal almost exclusively with a ﬁnite set
of constants in relations, but inﬁnitely many numbers. We shall use the terminol-
ogy Datalog with subscripts and/or superscripts when referring to function–free
databases. We generally provide essential results for logic programs as they en-
compass function–free databases.
One area of databases not yet considered in detail by the database community
is that of disjunctive databases. Such databases will be discussed in Section 2.
The ﬁeld of disjunctive logic programming (DLP) had its beginnings in 1982.
Work prior to 1982 focused primarily on Horn theories of logic programming as
described by Kowalski [Kowalski, 1974]. van Emden and Kowalski [van Emden
and Kowalski, 1976] developed three semantics for normal logic programs: ﬁxpoint,
1Although Nicolas chose not to be one of the co–authors, he was the major organizer of the
workshop that led to the book. He agreed with the assessment that logic and databases was a
ﬁeld, and is an important contributor to the resurgence of the ﬁeld of DDBs.
Jack Minker, Dietmar Seipel and Carlo Zaniolo
572

model theoretic, and proof procedure and proved that the three semantics were
identical.
The ﬁrst result in the ﬁeld of disjunctive logic programming (DLP) was by
Minker [Minker, 1982] who developed a consistent theory for default negation
in disjunctive theories (see Section 2.1), the generalized closed world assumption
(GCWA). The paper also set forth the concept of minimal models for computing
answers both for positive and negative atoms. The GCWA reduces to the CWA
for Horn theories developed by Reiter [Reiter, 1978].
There was no further work in DLP until 1986 when renewed interest arose as a
consequence of a Workshop on Foundations of Deductive Databases and Logic Pro-
gramming, organized by Minker [Minker, 1986] (Selected papers from the Work-
shop appear in the book by Minker [Minker, 1988]).
Following the workshop,
Minker and Rajasekar [Minker and Rajasekar, 1990] developed ﬁxpoint, model
theoretic and proof theoretic semantics for DLP. This work extended the results
of van Emden and Kowalski [van Emden and Kowalski, 1976] for disjunctive logic
programs. Minker [Minker, 1989] presents the ﬁrst historical paper on how the
foundation of disjunctive logic programming and disjunctive databases arose. The
early books by Loveland [Loveland, 1978] that provided a logical basis for auto-
mated theorem proving and by Lloyd [Lloyd, 1987] who provided a theoretical
basis for logic programming were important for theoretical developments.
It led to many theoretical developments in disjunctive logic programs, including
theories of negation and disjunctive deductive databases. Much of this work in the
late 1980s and early 1990s was performed at the University of Maryland and is the
subject of a research monograph, Foundations of Disjunctive Logic Programming
[Lobo et al., 1992]. The theoretical results contained in that monograph extend
some of the results in the theory of logic programs as developed in the monograph
by Lloyd [Lloyd, 1987] and the paper by Apt [Apt, 1990]. The work in DLP has
also been shown to be important for representing and implementing nonmonotonic
and abductive reasoning, and knowledge–based systems.
In this chapter we provide a history of the ﬁeld of logic and databases, starting
in the late 1950s to the present time. We cover both deductive databases and
disjunctive deductive databases, now referred to, respectively, as Datalog and Dis-
junctive Datalog. Important sytems that have been implemented are discussed,
and we provide an assessment of the ﬁeld. Articles cited in this chapter provide
additional details. The Chapter by Kowalski in this handbook provides the corre-
sponding information for normal logic programs and Datalog. It will be useful to
read Kowalski’s Chapter, before reading this chapter.
There have been several surveys of work on deductive databases. The ﬁrst sur-
vey was by Gallaire, Minker and Nicolas [Gallaire et al., 1984]. Several surveys
have appeared since then. Minker [Minker, 1999] provides historical material, re-
sults in complexity and early systems. Ramakrishnan and Ullman [Ramakrishnan
and Ullman, 1995] focus on implementation techniques in Datalog, and they brieﬂy
describe a number of projects that have implemented systems. Minker [Minker,
1994] wrote an overview of disjunctive logic programming.
Minker and Seipel
Logic and Databases: History of Deductive Databases
573

[Minker and Seipel, 2002] survey and assess disjunctive logic programming.
The chapter is organized as follows. The next section covers basic deﬁnitions,
and Section 3 describes the early years of deductive databases. Section 4 covers
the representation of relational databases in logic and various developments in
deductive databases, including their renaming to Datalog . Section 5 covers the
early work in Disjunctive Datalog , and Section 6 discusses systems, companies
and applications.
Needed future work is outlined in Section 7, and Section 8
summarizes our assessment of logic and databases.
2
BACKGROUND
In this section some background is provided that will be necessary for the chapter.
2.1
Syntax of Logic Programs
The syntactical building blocks of logic programs are from predicate calculus:
terms and atoms. Terms are built inductively: (i) constants and variable symbols
are terms; (ii) a complex term f(t1, . . . , tn) is built from an n–ary function symbol
f and terms ti. Similarly, an atom p(t1, . . . , tn) is built from an n–ary predicate
symbol p and terms ti.
Atoms are used for forming facts, rules, and goals (queries). Syntactically, a
disjunctive logic program P is a set of rules of the form
r = A1 ∨. . . ∨Ak ←B1 ∧. . . ∧Bm ∧not C1 ∧. . . ∧not Cn
over atoms A1, . . . , Ak, B1, . . . , Bm, C1, . . . , Cn, where not denotes default nega-
tion. The conjunction of the positive body B1 ∧. . . ∧Bm and the negative body
not C1 ∧. . . ∧not Cn form the body of the rule. A few special cases of rules are of
special interest. r is called a fact, if its body is empty (i.e., m = n = 0), and r is
called a goal, if its head is empty (i.e., k = 0). If the rules are free of disjunction
(i.e., k = 1), then they are called normal, and P is a normal logic program. If the
rules are free of disjunction and default negation (i.e., k = 1 and n = 0), then they
are called deﬁnite (or Horn rules), and P is a deﬁnite logic program.
A rule is called range–restricted, if all variable symbols of r occur in its positive
body. A normal logic program P is called a Datalog program, if its rules are free of
function symbols and range–restricted; if the rules are also free of default negation,
then P is called a positive Datalog program.
gnd (P) denotes the set of all ground instances of the rules in P, where the
variable symbols are instantiated with terms from the Herbrand universe of P.
Example (Family Tree).
The family tree stored in the following table Parent
in a relational database can be represented by one fact parent(t1, t2) for each tuple
(t1, t2) in the table Parent.
Jack Minker, Dietmar Seipel and Carlo Zaniolo
574

Parent
Name
Parent
a
b
b
c
b
d
From the corresponding facts parent(a, b), parent(b, c) and parent(b, d), the an-
cestors can be computed using the following recursive Datalog rules:
r1 = ancestor(X , Z) ←parent(X , Z),
r2 = ancestor(X , Z) ←parent(X , Y ) ∧ancestor(Y , Z).
A person Z is an ancestor of a person X, if Z is a parent of X (rule r1 ), or if Z is
an ancestor of a parent Y of X (rule r2 ). The rule r2 can be visualized as follows:
X
Y
Z
parent
ancestor
ancestor
✻
✻■
Starting with the parents, ancestor facts for more and more distant ancestors are
produced.
The ability to handle some forms of recursive views has been incorporated into
SQL:2003 [Wikipedia, 2003]. Now it is also possible to compute the ancestors in
standard SQL.
2.2
Semantics of Logic Programs
The semantics of logic programs can be deﬁned using three fundamental ap-
proaches: model theory, proof theory, and ﬁxpoint theory.
As shown by van
Emden and Kowalski [van Emden and Kowalski, 1976], for deﬁnite logic programs,
all approaches yield equivalent semantics.
Herbrand Interpretations
To deﬁne the semantics, subsets of the Herbrand base HBP of a logic program P
are used. HBP consists of all ground atoms (i.e., variable–free atoms) formed over
the language of P – i.e., the constants, function and predicate symbols.
An Herbrand interpretation I is represented as a subset of HBP, which contains
all ground atoms A ∈HBP, such that I(A) = t; all atoms A ̸∈HBP are false under
I.
An Herbrand interpretation I is an Herbrand model of a logic program P
without default negation, if I(α) = t for all rules r = α ←β ∈gnd (P), where
I(β) = t.
A partial Herbrand interpretation I can also assign the truth–value unknown
to a ground atom A ∈HBP.
Logic and Databases: History of Deductive Databases
575

Semantics of Deﬁnite Logic Programs
A deﬁnite logic program P has a unique minimal Herbrand model MP. Proof
theoretically, MP can be computed by hyperresolution with the consequence op-
erator TP of van Emden and Kowalski, which maps an Herbrand interpretation I
to another Herbrand interpretation
TP(I) = { A ∈HBP | A ←β ∈gnd (P), I |= β }.
E.g., applying the deﬁnite logic program P = {r1, r2} containing the two rules for
the predicate ancestor/2 shown in the previous subsection to the interpretation
I
=
{ parent(a, b), ancestor(b, c) }
produces
TP(I)
=
{ ancestor(a, b),
ancestor(a, c) }. The fact ancestor(a, b) is derived using the ground instance
ancestor(a, b) ←parent(a, b) of r1, and ancestor(a, c) is derived using the ground
instance ancestor(a, c) ←parent(a, b) ∧ancestor(b, c) of r2.
The operator TP is monotonic (and continuous) in the lattice of set containment:
I ⊆J implies TP(I) ⊆TP(J). Thus, the ordinal powers TP ↑α deﬁned by TP ↑0 =
∅, TP ↑α + 1 = TP(TP ↑α), for a successor ordinal α, and TP ↑α = ∪β<αTP ↑β,
for a limit ordinal α, cf. [Lloyd, 1987], are growing and reach the least ﬁxpoint of
TP, which is identical to the minimal Herbrand model MP. For positive Datalog
programs, MP is ﬁnite, and it is reached after ﬁnitely many iterations of TP.
For general deﬁnite logic programs, that may have function symbols, the ordinal
powers also converge towards the minimal Herbrand model MP; since TP is also
continuous, it holds MP = TP ↑ω = ∪n<ωTP ↑n, where ω is the smallest limit
ordinal. E.g., for the deﬁnite logic program P = {r1, r2} ∪I with the facts in
I = { parent(a, b), parent(b, c), parent(b, d) }, we obtain
TP ↑ω = { ancestor(a, b), ancestor(b, c), ancestor(b, d),
ancestor(a, c), ancestor(a, d) } ∪I.
Semantics of Normal Logic Programs
Normal logic programs allow for default negation in rule bodies. Some normal
logic programs P can be stratiﬁed. This means that P can be split into strata Pi,
1 ≤i ≤n, such that a default negated body atom not C of a rule A ←B1 ∧
. . . ∧Bm ∧not C1 ∧. . . ∧not Cn in one stratum Pi may only be deﬁned in the
head of rules from lower strata Pj, such that j < i, and a positive body atom B
may be deﬁned in the head of rules from strata Pj, such that j ≤i. Thus, each
stratum is closed under (positive) recursion. In that case, the operator TP can be
applied successively opof P. When a normal logic program cannot be stratiﬁed,
i.e., rules are recursive through negation, it is termed non–stratiﬁed. Alternative
semantics have been developed, including the stable model!semantics (SMS) and
the well–founded semantics (WFS).
The stable model semantics of Gelfond and Lifschitz [Gelfond and Lifschitz,
1988] is a ﬁxpoint approach for assigning a semantics to a general normal logic
program P. They call an Herbrand interpretation I a stable model of P, if I is the
Jack Minker, Dietmar Seipel and Carlo Zaniolo
576

minimal Herbrand model of the GL–reduct PI. PI is a deﬁnite logic program, i.e.,
free of default negation, obtained from the ground instances r = A ←B1 ∧. . . ∧
Bm ∧not C1 ∧. . . ∧not Cn ∈gnd (P) of the rules in P by interpreting the default
negated atoms. PI consists of all reduced rules r′ = A ←B1 ∧. . . ∧Bm, such that
I(not Ci) = t, for all 1 ≤i ≤n. Using Reiter’s Gamma operator [Reiter, 1980],
which is deﬁned as ΓP(I) = MPI, we can say that a stable model is a ﬁxpoint of
ΓP.
Alternatively, three–valued approaches have been proposed for assigning aics to
logic programs. Van Gelder, Ross, and Schlipf [Van Gelder et al., 1991] developed
a three–valued semantics based on the truth values true, false, and unknown,
termed the well–founded semantics for normal logic programs P; they use an
alternating ﬁxpoint approach (developed by Van Gelder [Van Gelder, 1993]) based
on ΓP. Since ΓP is anti–monotonic, its square Γ2
P is monotonic, where Γ2
P(I) =
ΓP(ΓP(I)). A partial Herbrand interpretation I is given by two sets TI ⊆HBP
and FI ⊆HBP containing its true and its false atoms, respectively; ground atoms
neither in TI nor in FI are evaluated as unknown. The well–founded model I = WP
is deﬁned as follows: TI is the least ﬁxpoint of Γ2
P, and FI = HBP \ΓP(TI). TI can
be obtained as an ordinal power Γ2
P ↑α of the monotonic operator Γ2
P; for Datalog
programs, α = ω steps are suﬃcient, but for general normal logic programs, α can
be greater than ω, since Γ2
P is not continuous.
The diﬀerent versions of stable models (P–stable, M–stable, and L–stable mod-
els) that can be deﬁned under three–valued logic are analyzed in [Sacc`a, 1997]
along with their expressive power, for bound (i.e., ground) and unbound queries.
Semantics of Disjunctive Logic Programs
Disjunctive logic programs allow for disjunction and default negation for repre-
senting incomplete knowledge. The semantics of a disjunctive logic program that
is free of default negation is given by its minimal Herbrand models – or, equiv-
alently, by its minimal model state, which contains all disjunctions α of ground
atoms, such that α is true in all minimal Herbrand models. Minker and Rajasekar
[Minker and Rajasekar, 1990] generalized the operator TP from deﬁnite logic pro-
grams to an operator T s
P for disjunctive logic programs without default negation
by mapping disjunctions of atoms to disjunctions of atoms. Fern´andez and Minker
[Fern´andez and Minker, 1992] applied their result to sets of models, which they
represent as model trees where each branch represents an interpretation. They ob-
tain an operator T INT
P
deriving the set of all minimal models of a disjunctive logic
program without default negation. Seipel, Minker, and Ruiz [Seipel et al., 1997]
have proven a Galois connection — a generalization of an order isomorphism –
between the ﬁxpoint iterations of T s
P and T INT
P
[Seipel et al., 1997].
Shepherdson [Shepherdson, 1988] showed that Minker’s theory of default nega-
tion for disjunctive deductive databases [Minker, 1982] can also be applied to
disjunctive logic programs. Alternative semantics for disjunctive logic programs
without default negation have been proposed based on the paradigm of minimal
Logic and Databases: History of Deductive Databases
577

Herbrand models. As shown in [Minker, 1982], it is suﬃcient to answer positive
queries over Disjunctive Datalog by showing that a query is satisﬁed in every
minimal Herbrand model. Thus, for the database P = { a ∨b }, there are three
Herbrand models: {a, b}, {a}, and {b}; only the latter two are minimal. The query
←a is not satisﬁed in the model {b}, and hence, a cannot be true and similarly for
the query, ←b. However, the answer to the query ←a ∨b is yes, since it is satis-
ﬁed in both minimal models. To answer negated queries, it is not suﬃcient to use
Reiter’s CWA [Reiter, 1978] since, as Reiter noted, from the theory P = { a ∨b },
it is not possible to prove a, and it is not possible to prove b. Hence, by the CWA,
¬a and ¬b follow. But, { a ∨b, ¬a, ¬b } is not consistent. The Generalized Closed
World Assumption GCWA [Minker, 1982] resolves this problem by specifying that
a negated atom ¬A be considered true if ¬A is true in all minimal Herbrand models
of the database. By the GCWA, no negated atom follows in this example. But the
Extended Generalized Closed World Assumption EGCWA [Gelfond et al., 1986;
Yahya and Henschen, 1985] considers disjunctions of negated atoms as true if they
are true in all minimal Herbrand models of the database. Thus, for the theory
P = { a ∨b }, the EGCWA derives the disjunction ¬a ∨¬b.
If there is – additionally – default negation, then again the stable models are
used, or – as an extension – the partial–stable models, which generalize the well–
founded model of normal logic programs. The deﬁnition of stable models can be
generalized to disjunctive logic programs P with default negation: the GL–reduct
PI is again deﬁned by interpreting the default negated body atoms; instead of
saying ”I is the minimal model of PI” we simply have to say ”I is a minimal
model of PI”. Later, Przymusinski [Przymusinski, 1992] generalized the deﬁnition
of stable models to three–valued stable models, called partial stable models, and
Seipel, Minker, and Ruiz have proven that the partial stable models correspond
to the stable models of a transformed program P tu [Seipel et al., 1997a]: each rule
(or fact) A1 ∨. . . ∨Ak ←B1 ∧. . . ∧Bm ∧not C1 ∧. . . ∧not Cn of P is suitably
annotated to two rules Ax
1 ∨. . . ∨Ax
k ←Bx
1 ∧. . . ∧Bx
m ∧not Cy
1 ∧. . . ∧not Cy
n, with
the annotations x = t and y = u or the annotations x = u and y = t, and there is
also an additional rule Au ←At for all atoms A.
Sometimes, a disjunctive logic program can be transformed to a normal logic
program with the same set of stable models by shifting atoms from the head to
the negative body. E.g., P1 = { a∨b } and P2 = { a ←not b, b ←not a } both have
the two stable models {a} and {b}. The extensions P ′
1 = { a ∨b, a ←b, b ←a }
and P ′
2 = { a ←not b, b ←not a, a ←b, b ←a }, however, have diﬀerent stable
models: P ′
1 has a single stable model {a, b}, while P ′
2 does not have any stable
models. As discussed in Section 5.2, Disjunctive Datalog is more expressive than
Datalog with default negation.
P–stable, M–stable, and L–stable models have been generalized to Disjunctive
Datalog by Eiter, Leone, and Sacc`a [Eiter et al., 1997].
Gelfond and Lifschitz [Gelfond and Lifschitz, 1990; Gelfond and Lifschitz, 1991]
have extended the concept of stable models to extended disjunctive logic programs
P, i.e. logic programs that may contain classical negation. They have deﬁned the
Jack Minker, Dietmar Seipel and Carlo Zaniolo
578

so–called answer set semantics as follows: P is transformed to a disjunctive logic
program P′ devoid of classical negation, which is obtained by replacing literals
¬A = ¬p(t1, . . . , tn) with classical negation by positive literals A′ = p′(t1, . . . , tn)
over new predicate symbols p′. Every stable model M ′ of P′ deﬁnes an answer set
M of P, which is a set of ground literals. If two correlated literals A and A′ are
both true under M ′, then
L = { A ∈HBP | M ′(A) = t } ∪{ ¬A ∈¬HBP | M ′(A′) = t }
contains a complementary pair A and ¬A of literals, and the answer set is the
set M = HBP ∪¬HBP of all ground literals, which is inconsistent. Otherwise, the
answer set is M = L, which is consistent, i.e., it does not contain complementary
literals.
3
HISTORICAL BACKGROUND OF DEDUCTIVE DATABASES
3.1
The Pre–History of Deductive Databases
The pre–history of DDBs is considered to be from 1957–1967. The eﬀorts in this
period used primarily ad–hoc or simple approaches to perform deduction.
One of the ﬁrst eﬀorts to use deduction in databases was by Gurk and Minker
[Gurk and Minker, 1961]. In 1957 they worked on a system, to automate work in
Army intelligence. An objective was to derive new data based upon given infor-
mation and general rules. Chains of related data were sought and data contained
reliability estimates. A prototype system was implemented to derive new data
whose reliability values depended upon the reliability of the original data. The
deduction used was modus ponens (i.e., from p and p →q, one concludes q, where
p and q are propositions).
In 1963, Levien and Maron [Levien and Maron, 1967] were the ﬁrst to use
the concept of a relation in a database to perform deductions. The RDS system,
developed at the Rand Corporation, answered simple queries and had an inferential
capability, implemented through a language termed INFEREX. An INFEREX
program could be stored in the system (such as in current systems that store views)
and re–executed, if necessary. There were no formal theorem proving techniques
(except for modus ponens). The programmer had to specify the reasoning rules
through an INFEREX program. RDS also handled credibility rules of sentences
in forming deduction. The system could also be programmed to count, correlate,
and provide statistics about the database. Additional work on the RDS system
may be found in [Levien, 1967; Levien, 1969].
Several DDBs were developed in the 1960s that used the concept of relations.
Although in 1970 Codd, [Codd, 1970], founded the ﬁeld of Relational Databases,
relational systems were in use before then.
Kuhns [Kuhns, 1967], who worked on the RDS project, recognized that there
were classes of questions that were, in a sense, not “reasonable.” For example, let
the database consist of the statement, ‘Reichenbach wrote Elements of Symbolic
Logic and Databases: History of Deductive Databases
579

Logic.’ Whereas the question, ‘What books has Reichenbach written?’, is reason-
able, the questions, ‘What books has Reichenbach not written?’, or, ‘Who did not
write Elements of Symbolic Logic?’, are not reasonable. It is one of the ﬁrst times
that the issue of negation in queries was explored. Kuhns related the imprecise
notion of “a reasonable question” with a precisely deﬁned notion of a “deﬁnite
formula”.
The notion of deﬁniteness is approximately as follows: Given a set of sentences
S, a dictionary containing known terms DS, a particular query Q, and an arbitrary
name n, Q is said to be semi–deﬁnite iﬀfor any name n the answer to query Q
is independent of whether or not DS contains n. Q is said to be deﬁnite iﬀQ is
semi–deﬁnite on every sentence set S. DiPaola [DiPaola, 1969] proved there is no
algorithm to determine whether or not a query is deﬁnite. This may be the ﬁrst
application of theory of computing to databases.
Kuhns also considered the general problem of quantiﬁcation in query systems.
Related to work by Kuhns were papers in the late 1950s and early 1960s devoted
to a general theory or formalization of questions, by Aqvist in 1965, Belnap in
1963, Carnap in 1956, Harrah in 1963, Jespersen in 1965 and Kasher in 1967.
In 1964, Raphael [Raphael, 1964], for his Ph.D. thesis at MIT developed a
system, Semantic Information Retrieval (SIR), which had a limited capability
with respect to deduction, using special rules. Following his thesis he joined SRI,
where he worked with Green on QA–1, an extension of SIR. QA–1 is based on
relational information organized in a list–structure memory.
The advantage of
QA–1 over SIR was that it could hold logical statements about how various kinds
of facts could interact. For more information about SIR, see [Raphael, 1968].
In 1966, Marill developed a system, Relational Structure System (RSS) that
consisted of 12 rules that permitted such capabilities as chaining. He used a de-
duction procedure termed, ‘a breadth–ﬁrst–followed–by–depth’ manner (now called
a search strategy).
For additional information about early work in deduction in databases and
references, see the report by Minker and Sable [Minker and Sable, 1970]
3.2
The Formative Years of Deductive Databases
Whereas the pre–history developments in inference systems used ad–hoc approaches
for deduction, this changed starting around 1968 when Green, a student of Mc-
Carthy, at Stanford University, joined SRI. McCarthy introduced Green to the
paper by Robinson [Robinson, 1965] the developer of the Robinson Resolution
Principle, a uniform method to perform inference in ﬁrst–order logic systems.
Green was the ﬁrst to recognize the importance and applicability of Robinson’s
development for automated theorem proving to databases and planning [Green,
1969]. In 1968, with Raphael [Green and Raphael, 1968a], they extended QA–
1 to QA–2, the ﬁrst system to incorporate the Robinson Resolution Principle
to perform deduction in databases. Green then developed QA–3 for question–
answering systems. QA–3 was an outgrowth of QA–2, but was more sophisticated
Jack Minker, Dietmar Seipel and Carlo Zaniolo
580

and practical, and was used for applications. The Robinson Resolution Principle is
a generalization of modus ponens to ﬁrst–order predicate logic. It is the standard
method used to deduce new data in DDBs and logic programming. Additional
work on intelligent answering systems may be found in [Green and Raphael, 1968].
Work in theorem proving using resolution followed Robinson’s paper. In 1971,
Kowalski and Kuehner [Kowalski and Kuehner, 1971] developed a variant of Robin-
son’s method, termed SL resolution (discussed in the Chapter by Kowalski).
Two important papers related to deductive databases were presented at the
IFIP Congress in 1974. The SYNTEX system by Nicolas and Syre [Nicolas and
Syre, 1974], used resolution for deduction in databases. Kowalski [Kowalski, 1974]
discussed the use of predicate logic as a programming language. He explained that
if the set of all possible clauses in ﬁrst–order predicate calculus were restricted to
Horn clauses, it could be used as a predicate logic language. A theorem prover
would then result in an eﬃcient implementation (see the Chapter by Kowalski).
van Emden and Kowalski [van Emden and Kowalski, 1976] in 1976 developed a
semantics for predicate logic as a programming language. The semantics was based
on deﬁnite logic programs (sets of Horn clauses). They developed three diﬀerent
semantics: ﬁxed point, model theoretic, and proof procedure. They proved that
these semantics were identical.
Minker, who, with his students, was developing a system, MRPPS to be used
for theorem proving and databases, attended both lectures and spoke with both
Nicolas and Kowalski. Based on these conversations he decided to visit Nicolas
in Toulouse, France in 1975, followed by a visit to Kowalski in London, England.
Nicolas worked in a group headed by Herv´e Gallaire. In Toulouse, Minker pre-
sented lectures on his research and learned more about the research in deductive
databases in Gallaire’s group. Gallaire and Nicolas asked Minker what he thought
of holding a workshop on Logic and Databases. Minker thought their idea was
excellent.
In his visit to London he met with Kowalski at the Imperial College of London
and discussed work that he was doing. He also met Keith Clark who told him
about his work on negation that he would later present in Toulouse.
The workshop, “Logic and Data Bases,” organized by Gallaire and Nicolas in
collaboration with Minker, was held in Toulouse, France in November 1977. The
workshop included researchers who had performed work in deduction from 1969
to 1977 and used the Robinson Resolution Principle. The book, “Logic and Data
Bases”, edited by Gallaire and Minker [Gallaire and Minker, 1978] consisted of
selected papers presented at the workshop.
Many signiﬁcant contributions were described in the book. Nicolas and Gallaire
discussed the diﬀerence between model theory and proof theory. They demon-
strated that the approach taken by the database community was model theoretic,
i.e., the database represents the truths of the theory and queries are answered by
a bottom–up search. However, in logic programming, answers to a query used a
proof theoretic approach, starting from the query, in a top–down search. Reiter
contributed two papers. One on compiling axioms for a database that consisted
Logic and Databases: History of Deductive Databases
581

of relational facts, the Extensional Database EDB and the Intensional Database
IDB which contained Horn clause rules. Reiter was the ﬁrst to note that if the
IDB contained no recursive rules, then a theorem prover could be used to generate
a new set of rules where the head of each rule was deﬁned in terms of relations in
a database [Reiter, 1978a]. Once this was done, a theorem prover was no longer
needed during query operations. A conventional relational database query lan-
guage could then be used to answer the query. His second paper discussed the
Reiter closed world assumption (CWA) [Reiter, 1978] whereby in a Horn clause
theory, if one cannot prove that an atomic formula is true, then the negation of the
atomic formula is assumed to be true. Reiter’s paper elucidated three major issues:
the deﬁnition of a query, an answer to a query, and how one deals with default
negation. Clark [Clark, 1978] presented an alternative theory of negation. He in-
troduced the Clark completion theory, if–and–only–if conditions that underly the
meaning of default negation, called negation–as–ﬁnite–failure. Both approaches
generally yield the same solution. The Clark and Reiter papers are the ﬁrst to
formally deﬁne default negation in logic programs and deductive databases.
Several implementations of deductive databases were reported at the Toulouse
workshop. Chang developed DEDUCE; Kellogg, Klahr and Travis developed De-
ductively Augmented Data Management System (DADM); and Minker and his stu-
dents developed Maryland Refutation Proof Procedure 3.0 (MRPPS 3.0). Kowalski
discussed the use of logic for data description. Darvas, Futo and Szeredi presented
applications of Prolog to drug data and drug interactions and it was one of the
ﬁrst published papers on Prolog applications. Nicolas and Yazdanian described the
importance of integrity constraints in deductive databases. The book provided,
for the ﬁrst time, a comprehensive description of the interaction between logic
and databases. That is, deductive databases were function–free logic programs,
where the EDB contained the facts and the IDB deﬁned what is called views in
relational database terminology, and integrity constraints, like queries, are Horn
clauses with an empty head.
Gallaire, Minker and Nicolas organized a total of three workshops on logic and
databases [Gallaire and Minker, 1978], [Gallaire et al., 1981], [Gallaire et al., 1984].
Following these workshops, Minker and Nicolas [Minker and Nicolas, 1982] realized
the importance of recursive rules in databases and logic programs and were the
ﬁrst to show that there are forms of recursive rules that lead to bounded recursion,
also referred to as 0–sided linear recursion (see page 27). That is, the deduction
process using these rules must terminate in a ﬁnite number of steps. This work
has been extended by Naughton and Sagiv [Naughton and Sagiv, 1987] and by
several others.
The use of logic in databases was initially received by the database community
with skepticism.
But in the 80s many mainstream database people joined the
Datalog camp with enthusiasm, and produced theoretical and system research
that led to the addition of recursive queries in SQL:2003. More recent work has
produced signiﬁcant results on Datalog, Disjunctive Datalog, and new application
areas, such as parallel deduction on big–data that are discussed in Section 4.4.
Jack Minker, Dietmar Seipel and Carlo Zaniolo
582

Therefore, history has proven that deductive databases are indeed a well–deﬁned
area of research, complete with important past achievements and stimulating areas
for future work – thus meeting the challenge laid down by Harel [Harel, 1980].
For deductive databases, renamed Datalog by Maier [Abiteboul et al., 1995], the
study of their expressive power has produced many interesting results which are
discussed in an excellent survey by Dantsin, Gottlob, Eiter, and Voronkov [Dantsin
et al., 2001]. The discussion of this topic is outside the scope of this paper, but
we mention a few signiﬁcant results from [Dantsin et al., 2001] and [Abiteboul et
al., 1995]. We have that safe Datalog programs with stratiﬁed default negation
can express every relational algebra query, and compute in PTIME in the size of
the database. Because of recursion, Datalog is signiﬁcantly more powerful than
relational algebra but, unless the database is ordered, it is not PTIME–complete.
(On ordered structures, default negation, even if restricted to the input predicates,
yields a version of Datalog that captures exactly PTIME, and the same is true for
well–founded default negation.) On the other hand, stable default negation yields
a language that precisely expresses NP under credulous (any stable model will do)
semantics; however, for credulous semantics, the choice construct deﬁnes a narrow
class of stable models [Giannotti et al., 1991; Giannotti et al., 2001] which entail
PTIME–completeness without exponential costs.
4
THEORETICAL FOUNDATIONS OF DEDUCTIVE DATABASES
We assume that the reader is familiar with the relational database model proposed
by Codd [Codd, 1970] in 1970. The relational model was deﬁned in terms of the
relational calculus and the relational algebra, and it has been the dominant model
in use. The model has several operators, and a language SQL is used to write
queries.
4.1
Relational and Deductive Databases
As will be made precise, the relational model and its extensions are a subset of
logic. Reiter [Reiter, 1984] was the ﬁrst to formalize relational databases in terms
of logic. Reiter noted that underlying relational databases there were a number of
assumptions that were not made explicit. With respect to negation, an assumption
was being made that facts not known to be true are assumed false. This assumption
is Reiter’s well–known Closed World Assumption (CWA), expounded earlier in
1978 [Reiter, 1978]. A second assumption, the unique name assumption, states
that any item in a database has a unique name, and that individuals with diﬀerent
names are not the same. The last assumption, the domain closure assumption,
states that there are no other individuals than those in the database.
Reiter then formalized relational databases as follows. He stated that a rela-
tional database is a set of ground assertions over a language L together with a set
of axioms. The language L does not contain function symbols. These assertions
and axioms are of the following form:
Logic and Databases: History of Deductive Databases
583

• Assertions: p(a1, . . . , an), where p is an n–ary relational symbol in L, and
a1, . . . , an are constant symbols in L.
• Unique Names Axiom: If a1, . . . , an are all the constant symbols of L, then
ai ̸= aj for all 1 ≤i < j ≤n.
• Domain Closure Axiom: If a1, . . . , an are all the constant symbols of L, then
∀X (X = a1 ∨. . . ∨X = an).
• Completion Axioms: If p(a1
1, . . . , a1
n), . . . , p(am
1 , . . . , am
n ) are all facts for a
relational symbol p, then the completion axiom for p is
∀X1 . . . ∀Xn p(X1, . . . , Xn) →
(X1 = a1
1 ∧. . . ∧Xn = a1
n) ∨. . . ∨(X1 = am
1 ∧. . . ∧Xn = am
n )).
• Equality Axioms:
∀X (X = X),
∀X∀Y (X = Y →Y = X),
∀X∀Y ∀Z (X = Y ∧Y = Z →X = Z),
∀X1 . . . ∀Xn (p(X1, . . . , Xn) ∧X1 = Y1 ∧. . . ∧Xn = Yn
→p(Y1, . . . , Yn)).
The completion axiom was proposed by Clark [Clark, 1978] as the basis for his
Negation–as–Failure rule. It states that the only tuples that a relation can have
are those that are speciﬁed in the relational table. This statement is implicit in
every relational database. The completion axiom makes this explicit.
In addition to Reiter’s precise deﬁnition of relational databases in terms of logic,
Gallaire and Nicolas in [Nicolas and Gallaire, 1978] provide several approaches to
databases in terms of logic. One approach is to regard a relational database as
an Herbrand interpretation, and to regard an answer to a query as the set of all
ground instances of the query that are true in the model.
4.2
Deductive Databases, Integrity Constraints, and User Constraints
Integrity Constraints (ICs) play an important role in database systems. In addition
to deﬁning a database in terms of an EDB and an IDB, it is necessary to formalize
what is meant by an IC. Kowalski [Sadri and Kowalski, 1987] suggests that an IC is
a formula that is consistent with the DDB, while for Reiter, and Lloyd and Topor
[Reiter, 1988; Lloyd and Topor, 1985], an IC is a theorem of the DDB. Reiter
proposed other deﬁnitions of ICs in [Reiter, 1988].
He states that ICs should
be statements about the content of a database. ICs can be written in a modal
logic to use a belief operator to express beliefs that the database must satisfy.
He explores Levesque’s KFOPCE [Levesque, 1984], an epistemic modal logic, as
a suitable framework. Demolombe and Jones [Demolombe and Jones, 1996], view
ICs as statements true about the world, whereas a DB is a collection of beliefs
about the world. ICs then can be used to qualify certain information in the DB
Jack Minker, Dietmar Seipel and Carlo Zaniolo
584

as valid or complete. They deﬁne these properties formally in the framework of
doxatic logic.
In DDBs, the semantics of the DB design are captured by ICs. Information
about functional dependencies (FDs) that a relation’s key functionally determines
the rest of the relation’s attributes can be written via ICs. Likewise, inclusion
dependencies, as well as other dependencies, are easily represented.
The major use of ICs in the DB community has been in updating to assure
database consistency. Nicolas [Nicolas, 1979] has shown how, using techniques
from DDBs, improvements can be made to the speed of update.
Reiter [Reiter, 1978] showed that one can query a Horn DB with or without ICs
and the answer to the query is the same. However, this does not preclude the use
of ICs in the query process. While ICs do not aﬀect the result of a query, they may
aﬀect computational eﬃciency. If a query requests a join for which there will never
be an answer because of system constraints, this can be used to advantage by not
performing the query and returning an empty answer. This avoids an unnecessary
join on two potentially large relational tables in a relational database system, or
a long deduction in a DDB system.
The process of using ICs to constrain a
search is called semantic query optimization (SQO) [Chakravarthy et al., 1990].
McSkimin and Minker [McSkimin and Minker, 1977] were the ﬁrst to use ICs
for SQO in DDBs. Hammer and Zdonik [Hammer and Zdonik, 1980], and King
[King, 1981], were the ﬁrst to apply SQO to relational databases. Chakravarthy,
Grant and Minker [Chakravarthy et al., 1990] formalized SQO and developed the
partial subsumption algorithm and method of residues. These provide a general
technique applicable to any relational or DDB that is able to perform SQO. In
[Godfrey et al., 1996] it is shown how to apply SQO bottom–up. Gaasterland
and Lobo [Gaasterland and Lobo, 1993] extended the work to include DBs with
negation in the body of a clause, and Levy and Sagiv [Levy and Sagiv, 1995]
showed how to handle recursive IDB rules in SQO. They show that SQO can be
done in some recursive rules provided that negated EDB subgoals appear only in
the recursive rules, but not in the ICs. If negated EDB subgoals are introduced in
ICs, then the problem of SQO becomes undecidable. For SQO to be an eﬀective
tool, it will need to be integrated with conventional syntactic query optimization
techniques [Chaudhuri, 1998] used for SQL queries in relational databases.
Cooperative answering systems is a topic related to SQO. A cooperative an-
swering system provides information to a user as to why a query succeeds or fails.
When a query fails, the user, in general, cannot tell why the failure occurred.
There may be several reasons: the database currently does not contain informa-
tion to respond to the user, or there will never be an answer to the query. The
distinction could be important to the user. One of the ﬁrst papers on coopera-
tive answers is by Gal and Minker [Gal and Minker, 1985]. A survey of work in
cooperative answering systems may be found in [Gaasterland et al., 1992a].
Another aspect related to ICs is that of user constraints (UCs). A user con-
straint is a formula, that models a user’s preferences. It may constrain providing
answers to queries in which the user may have no interest (e.g., stating that in
Logic and Databases: History of Deductive Databases
585

developing a route of travel, the user does not want to pass through a particu-
lar city), or provide other constraints that may restrict the search. As shown by
[Gaasterland, 1992; Gaasterland et al., 1992], UCs which are identical in form to
integrity constraints, can be used for this purpose. While ICs provide the seman-
tics of the entire database, UCs provide the semantics of the user. UCs may be
inconsistent with the database and hence, a separation of these two semantics is
essential. To maintain the consistency of the database, only ICs are relevant. A
query may be thought of as the conjunction of the query and the UCs. Hence, a
query can be semantically optimized based both on ICs and UCs.
As noted above, ICs are more versatile than just to represent dependencies.
General semantic information can be captured as well. Assume at Reagan National
Airport in Washington, D.C., (DCA) no ﬂights are allowed (departures or arrivals)
after ten at night or before eight in the morning so as not to disturb city residents.
This statement can be written as an integrity constraint.
Such knowledge, captured and recorded via ICs, can be used to answer queries
more intelligently and informatively. If someone asks for ﬂights out of DCA to,
say, LAX leaving between 10:30 at night and midnight, a DB usually returns the
empty answer.
(There will be no such ﬂights if the DB is consistent with its
constraints.) It would be better, however, for the DB system to inform the user
that there can be no such ﬂights, because of Washington, D.C., ﬂight regulations.
With UCs and ICs it is possible to develop a system that provides responses to
users that inform them as to the reasons why queries succeed or fail [Godfrey et
al., 1994]. Other features may be built into a system, such as the ability to relax
a query given that it fails, so that an answer to a related request may be found.
This has been termed query relaxation [Gaasterland et al., 1992].
User constraints and cooperative answering systems are important both for
relational and DDB systems.
4.3
Stratiﬁed and Non–Stratiﬁed Deductive Databases
Although Prolog had a default negation operator, there was a need to handle
default negation properly in normal logic programs. This led to two signiﬁcant
developments: stratiﬁed and non–stratiﬁed theories, as discussed in Section 2.2.
Stratiﬁed Databases
Stratiﬁed theories for default negation in the body of clauses of logic programs
ﬁrst appeared in a workshop held in 1986 [Minker, 1986]. Selected papers from
the workshop appeared in 1988 [Minker, 1988]. The concept of stratiﬁcation was
discussed ﬁrst by Chandra and Harel [Chandra and Harel, 1985] in the context
of databases. The concept of stratiﬁcation was introduced to logic programs by
Apt, Blair and Walker [Apt et al., 1988], and by Van Gelder [Van Gelder, 1986].
For stratiﬁed theories, one can compute a unique preferred minimal model, from
stratum to stratum. Przymusinski [Przymusinski, 1988] terms this minimal model
the perfect model and extends the concept to locally stratiﬁed theories. Thus, one
Jack Minker, Dietmar Seipel and Carlo Zaniolo
586

can evaluate the positive predicates in a lower stratum and a negated body atom
not C is true if the corresponding positive atom C has not been computed in the
lower stratum.
The theory of stratiﬁed DDBs was followed by permitting recursion through
default negation. Such DDBs are termed non–stratiﬁed. There have been a large
number of papers devoted to deﬁning the semantics of these databases. The most
prominent of these are the well–founded semantics (WFS) of Van Gelder, Ross and
Schlipf [Van Gelder et al., 1991], and the stable model semantics (SMS) of Gelfond
and Lifschitz [Gelfond and Lifschitz, 1988], deﬁned in Section 2. The WFS leads to
a unique three–valued model. Stable semantics may lead to a collection of minimal
models. For some DDBs this collection may be empty. Additional semantics for
non–stratiﬁed theories may be found in [Minker and Ruiz, 1994] and [Minker,
1999].
There has been a larger community of users for SMS than for any other seman-
tics. The Gelfond and Lifschitz paper on SMS [Gelfond and Lifschitz, 1988] was
deemed the Most Inﬂuential Paper in the 20 Years Award from the Association
for Logic Programming (ALP) (2004). It is interesting that the paper was ﬁrst
sent to a conference in 1987, but was rejected. Lifschitz, in an article in the ALP
Newsletter [Lifschitz, 2006] wrote how the SMS was conceived, and the rejection
of the paper. With respect to the rejection Lifschitz said, “The reason, I sus-
pect, was simply that the referees couldn’t believe in what we had demonstrated
– that the deﬁnition of an iterated least ﬁxed point can be replaced by something
quite simple, and, at the same time, more general.” Gelfond in [Gelfond, 2006]
also wrote about his work related to stable model semantics. The outstanding
survey article by David Pearce, “Sixty Years of Stable Models” [Pearce, 2008]2,
provides an interesting discussion of how Gelfond and Lifschitz devised the SMS
and a history of stable models.
A brief description of how they came to SMS is through work by Gelfond in
autoepistemic logic and by Lifschitz in minimal models. As Lifschitz wrote in the
ALP Newsletter, Gelfond had the idea of characterizing the meaning of a logic
program by translating it into autoepistemic logic or default logic. Gelfond, in the
paper “On stratiﬁed autoepistemic theories” [Gelfond, 1987], proposed to insert
a modal operator after every occurrence of default negation in a logic program,
and to interpret the resulting set of formulas in accordance with the semantics of
autoepistemic logic, i.e., in terms of stable expansions. The intuition behind this
translation is that default negation in a logic program is understood as “is not
believed.” If the given program is stratiﬁed then Gelfond’s translation produces
the same result as the iterated least ﬁxpoint semantics [Apt et al., 1988], with
the advantage that there is no need to stratify a program – the “right” minimal
model is automatically produced by the deﬁnition of a “stable expansion” [Gelfond,
1987]. For additional information about autoepistemic logic and stable models,
see [Marek and Truszczy´nski, 1991].
2The
full
paper
can
be
obtained
from
David
Pearce’s
web
site
http://www.ia.urjc.es/∼dpearce/publications.html.
Logic and Databases: History of Deductive Databases
587

References to a number of normal DB semantics and their relationships may be
found in [Minker, 1999].
Non–Stratiﬁed Databases
There have been several implementations of the WFS semantics. Chen and Warren
[Chen and Warren, 1993] developed a top–down approach to answer queries in this
semantics, while Leone and Rullo [Leone and Rullo, 1992] developed a bottom–up
method for Datalog databases. Sagonas, Swift and Warren [Sagonas et al., 1996]
show that a ﬁxed order of computation does not suﬃce for answering queries in
the WFS. They introduce a variant of SLG resolution SLGstrat [Swift, 1994],
which uses a ﬁxed computation rule to evaluate ground right-to-left dynamically
stratiﬁed programs. Warren, Swift and their associates [Rao et al., 1996] developed
an eﬃcient deductive logic programming system, XSB, that computes the well–
founded semantics. The system extends the full–functionality of Prolog to the
WFS. XSB forms the core technology of a start–up company, XSB, Inc. In [Swift,
1999] it is shown how nonmonotonic reasoning may be done within XSB and
describes mature applications in medical diagnosis, model checking and parsing.
Many individuals have contributed to eﬃcient implementations of stable models
and answer set programming (see, for example, [Gebser et al., 2011; Eiter et al.,
2000a; Niemel¨a and Simons, 2000]).
Niemel¨a and Simons [Niemel¨a and Simons, 2000], developed a system, Smodels,
to compute stable models of programs in Datalog with default negation. Smodels
is based on three important ideas: intelligent grounding of the program, limiting
the size of the grounding, and using the WFS computation as a pruning technique.
A further extension of normal DDBs, proposed ﬁrst by Gelfond and Lifschitz
[Gelfond and Lifschitz, 1990] is to include both default and classical negation in
formulas. That is, all atoms in formuals may be literals including both classical
and default negation. As described, in “Classical negation in logic programs and
disjunctive databases” [Gelfond and Lifschitz, 1991], they extend this work to dis-
junctive systems. Answer Set Programs (ASP), which uses stable model semantics
now applies to both Datalog and Disjunctive Datalog. The extension to classical
negation is achieved by considering that the classical negation of a predicate p is
treated as a new, disjoint positive predicate p′. Although seemingly trivial, this
extension has had a huge impact. Combined with SMS it has become the favorite
method to use in deductive databases. The great success of SMS–based ASP is
documented in “Answer Set Programming at a Glance” by Brewka, Eiter, and
Truszczynski [Brewka et al., 2011]. The article provides an overview of ASP and
various systems and applications.
The ASP paradigm was proposed independently by Niemel¨a [Niemel¨a, 1999] and
by Marek and Truszczynski [Marek and Truszczy´nski, 1999]. Their approach to
ASP diﬀers from that of Gelfond and Lifschitz. See also the Chapter by Kowalski
for a discussion of relationships between the diﬀerent approaches. For historical
information about other methods, see [Minker, 1999].
Jack Minker, Dietmar Seipel and Carlo Zaniolo
588

Eiter et al. [Eiter et al., 2000] describe the application of DLV (Datalog with Or),
that computes answer sets (in the sense of Gelfond and Lifschitz) for disjunctive
logic programs in the syntax generalizing Datalog with default negation. Both
Smodels and DLV use a very powerful grounding engine and some variants of WFS
computation as a pruning mechanism. The method used to compute disjunctive
stable models is described in [Leone et al., 1997].
Work on the implementation of semantics relating to extended DDBs has been
very impressive. These systems can also be used for
nonmonotonic reasoning.
Brewka and Niemel¨a [Brewka and Niemel¨a, 1998] state at the 1998 Nonmonotonic
Workshop,
The participants in the plenary panel identiﬁed the following major
trends in the ﬁeld: First, serious systems for nonmonotonic reasoning
are now available (XSB, Smodels, DLV). Second. people outside the
community are starting to use these systems with encouraging success
(for example, in planning). Third, nonmonotonic techniques for reason-
ing about action are used in highly ambitious long–term projects (for
example, the WITAS Project, www.ida.liu.se/ext/witas/eng.html).
Fourth, causality is still an important issue; some formal models of
causality have surprisingly close connections to standard nonmonotonic
techniques. Fifth, the nonmonotonic logics being used most widely are
the classical ones: default logic, circumscription, and autoepistemic
logic.
In addition to XSB, Smodels and DLV, there have been several other signiﬁcant
solvers, such as clasp, Cmodels, IDP and SUP. clasp [Gebser et al., 2007] is an
answer set solver for (extended) normal logic programs using techniques from
boolean constraint solving and conﬂict–driven nogood learning. Nogood learning
records sets of domain restrictions that led to failure as new clausal propagation
and provides orders of magnitude speedups for many problems. claspD [Drescher
et al., 2008] is an extension of clasp that is able to solve unrestricted disjunctive
logic programs.
Cmodels [Lierler and Maratea, 2004] handles disjunctive logic
programs and exploits a SAT solver3 as a search engine for enumerating models
and verifying model minimality whenever needed.
IDP4 [Wittocx et al., 2008]
is a ﬁnite model generator for extended ﬁrst–order logic theories. SUP [Lierler,
2008] is a combination of computational ideas underlying Cmodels and Smodels.
All of the solvers in this paragraph and others compete in The Open Answer Set
Programming Competition biannually on a set of problems on a common platform.
The competition is a result of the Dagstuhl Initiative discussed in Section 4.5.
Results of the competition are published and appear on the web. See [Calimeri et
3SAT is an abbreviation for satisﬁability. A SAT solver determines if there exists an inter-
pretation that satisﬁes a given Boolean formula. That is, it establishes if the variables of a given
Boolean formula can be assigned in such a way as to make the formula evaluate to true.
4IDP stands for Imperative–Declarative Programming
Logic and Databases: History of Deductive Databases
589

al., 2012] for descriptions and results of The Third Open Answer Set Programming
competition and the web page for the pdf–ﬁle.
Schlipf [Schlipf, 1995] has written a comprehensive survey that summarizes
complexity results for deductive databases. A user may wish to determine which
semantics to be used based upon its complexity.
Knowledge bases are important for AI and expert system developments. Baral
and Gelfond [Baral and Gelfond, 1994] and Baral in his book [Baral, 2004] describe
how extended DDBs may be used to represent knowledge bases. Extended DDBs,
together with ICs permit a wide range of knowledge bases to be implemented.
Many papers devoted to knowledge bases consider them to consist of facts and
rules. Certainly, this is one aspect of a knowledge base, as is the ability to extract
proofs. However, ICs supply another aspect of knowledge and diﬀerentiate knowl-
edge bases which may have the same rules, but diﬀerent ICs. Thus, knowledge
bases are deductive databases.
Since alternative extended deductive semantics have been implemented, the
knowledge base expert should focus on specifying rules and ICs that character-
ize the database, selecting the particular semantics that meets the needs of the
problem, and employing a DDB system that uses the required semantics.
4.4
Datalog
In the early 1980s, relational databases, a concept that had been introduced by
Codd only a decade earlier, was turning into a major success in both the com-
mercial world and the research world. Codd’s proposal was based on describing
the database as a set of facts without function symbols, and then using ﬁrst–order
logic on such facts as the DB query language (which actually came in two ver-
sions: the domain calculus or the tuple calculus). Although both calculi suﬀer
from several limitations, particularly in terms of expressive power, they repre-
sented a signiﬁcant improvement over previous query languages, and paved the
way to commercial query languages, such as SQL, and their very eﬃcient imple-
mentations. Indeed, relational database management systems (RDBMS) proved
that declarative queries can be given eﬃcient and scalable implementations via
various indexing and query optimization techniques, whereby the user remains re-
sponsible for the logical correctness of queries, but the system assumes primary
responsibility for performance. Given this background, it is hardly surprising that
in the early 1980s leading researchers from the database ﬁeld, motivated by re-
search in deductive databases, became quite interested in deductive databases and
Prolog, initiating a research thrust that later became known as Datalog (a name
coined by Maier) [Abiteboul et al., 1995]. These researchers recognized the supe-
rior expressive power of Horn rules, their elegant declarative semantics, and the
eﬀectiveness of rules in supporting expert applications; therefore, they sought to
design data and knowledge management systems that combine these qualities with
the superior performance, scalability and parallelizability of RDBMS. Indeed the
two most distinctive traits of Datalog research are as follows:
Jack Minker, Dietmar Seipel and Carlo Zaniolo
590

1. Bottom–Up Execution: In RDBMS, the answers to queries are constructed
via relational algebra (RA) operators that are applied to the database and
feed their results to the next level of RA operators; thus the implementation
approaches chosen by most Datalog systems [Ceri et al., 1990; Chimenti et
al., 1990; Morris et al., 1986; Ramakrishnan et al., 1993; Arni et al., 2003]
use a bottom–up computation where execution proceeds from the database
toward the query goal. For recursive queries, this bottom–up approach dove-
tails with the declarative least–ﬁxpoint semantics of Horn clauses and its
equivalent operational semantics based on iterating over the immediate con-
sequence operator TP. Thus, much of Datalog research has focused on turn-
ing this approach into a very eﬃcient evaluation for recursive queries. Thus,
much of Datalog research has focused on turning this approach into a very
eﬃcient evaluation for recursive queries.
2. Declarative Semantics: Nonmonotonic constructs, such as default negation
and aggregates, are required to achieve the level of expressive power and per-
formance required by real–life applications. While the possibility of falling
back on operational semantics was also considered [Kifer et al., 1995], most
Datalog researchers sought solutions based on nonmonotonic reasoning se-
mantics that remain declarative, while being conducive to eﬃcient imple-
mentation. Thus the notion of programs that are stratiﬁed w.r.t. default
negation and aggregates was soon adopted in most Datalog projects, and
research work then continued toward ﬁnding and supporting eﬃciently more
powerful semantics, e.g., restricted forms of locally stratiﬁed programs and
answer set programs that can be evaluated eﬃciently.
Early work produced signiﬁcant contributions along these two lines of research,
resulting into major technology transfers to mainstream RDBMS, where recur-
sive queries are now part of the SQL standard [Wikipedia, 2003], and they are
supported by most vendors using Datalog techniques such as diﬀerential ﬁxpoint,
stratiﬁcation w.r.t. default negation and aggregates, and magic sets [Seshadri et
al., 1996].
On the other hand, early research prototypes did not grow into commercial
systems, and the focus of many researchers in the ﬁeld shifted toward other topics,
such as combining Datalog with programming paradigms such as Xml, active DB
rules, semantic web, and in particular object–oriented programming [Kifer et al.,
1995].
But development work continued on other systems, including the DLV
prototype that extends Datalog with disjunction [Faber et al., 2003],
and the
XSB system that supports tabled resolution and limited higher–order logic using
Prolog technology [Rao et al., 1996].
A strong research interest in the core Datalog technology and its applications
has reemerged in the recent years, leading to what has been called a resurgence
[Hellerstein, 2010] or even a renaissance [Abiteboul, 2012] for Datalog. This revival
is propelled by (i) important new applications areas, such as distributed compu-
tations and big–data applications, (ii) the success of a ﬁrst commercial Datalog
Logic and Databases: History of Deductive Databases
591

System, and (iii) progress in semantics extensions to support nonmonotonic con-
structs such as default negation and aggregates (a theoretical thread that had
actually continued through the years).
In the following, we will ﬁrst overview the bottom–up implementation technol-
ogy developed for Datalog, then discuss extensions to support default negation
and aggregates, and ﬁnally overview the recent renaissance.
Compilation and Optimization
Most Datalog systems support arithmetic expressions and many also support lists
and complex objects represented using functors. But the most distinctive feature
that sets Datalog apart from early RDBMS and Prolog are, The interest in re-
cursion is driven by real–life applications, such as those involving networks and
graphs. A most basic graph problem is the computation of transitive closures.
The transitive closure of a directed graph with edges arc(X, Z) can be expressed
by the following program P:
EXAMPLE 1. All connected node–pairs in a directed graph
trcl(X, Z) ←
arc(X, Z).
trcl(X, Z) ←
trcl(X, Y ) ∧arc(Y, Z).
If TP is the immediate consequence operator for our program P, the rules above
establish a least–ﬁxpoint equation I = TP(I). The solution of this equation can
be computed by the repeated ﬁring of the rules above as follows. Initially we can
set I0 = ∅and then repeat the computation of In+1 = TP(In) until TP(In) = In.
Now, In+1 = TP(In) can be rewritten as In+1 = TP(In \ In−1) ∪In, which is
the basis of the semi–naive ﬁxpoint optimization. This important optimization
technique is also known as diﬀerential ﬁxpoint since it is based on the symbolic
diﬀerentiation of rules, and can be applied directly on the rules [Sacc`a and Zaniolo,
1988]. For that, we can add an argument that models the iteration step to our
recursive predicate in Example 1 (renamed ntrlc):
EXAMPLE 2. Forward chaining rules expressing In+1 = TP(In \ In−1) ∪In
ntrcl(X, Z, 1) ←
arc(X, Z).
ntrcl(X, Z, K) ←
ntrcl(X, Y, J) ∧arc(Y, Z)∧
not ntrcl(X, Z, J) ∧K = J+1.
ntrcl(X, Z, K) ←
ntrcl(X, Z, J) ∧K = J+1.
These rules can be viewed as operational forward–chaining rules.5 The second rule
in Example 2 generates tuples at level J+1 that were not present at level J (let
us call them delta tuples at level J+1 ). Now delta tuples at level J+1 can only
be generated by delta tuples at level J. Thus the above production rules can be
replaced by the following rules:
EXAMPLE 3. Delta rules used in the iterative computation
5As we shall see later, this program also has a perfect model semantics.
Jack Minker, Dietmar Seipel and Carlo Zaniolo
592

δtrcl(X, Z, 1) ←
arc(X, Z).
δtrcl(X, Z, K) ←
δtrcl(X, Y, J) ∧arc(Y, Z)∧
not ntrcl(X, Z, J) ∧K = J+1.
ntrcl(X, Z, K) ←
δtrcl(X, Z, I) ∧K ≤J.
Indeed, if our arcs are from a person to his/her parents, then when computing
ancestors of level n + 1 we only need to consider the parents of the ancestors of
level n and ignore the ancestors from levels lower than n.
The diﬀerential ﬁxpoint transformation rules are simple and can be derived
using templates akin to those used for diﬀerentiating functions, with recursive
predicates treated as variables and the others as constants. using templates akin
to those used for diﬀerentiating functions, with recursive predicates treated as
variables and the others as constants. Thus for linear recursive rules, such as the
one above, containing only one recursive predicate, each rule is diﬀerentiated into
a single delta rule. The diﬀerentiation of a quadratic rule (two recursive predicates
in the body) instead yields two rules (as per δ(X × Y ) = (δX) × Y + X × (δY ).
For instance a transitive closure can be expressed by replacing the linear rule by
the following quadratic one:
EXAMPLE 4. The quadratic rule replacing the linear rule of Example 1.
trcl(X, Z) ←
trcl(X, Y ) ∧trcl(Y, Z).
Then, the symbolic diﬀerentiation of this rule yields the following two rules6:
δtrcl(X, Z, K) ←
δtrcl(X, Y, J) ∧ntrcl(Y, Z, J)∧
not ntrcl(X, Z, J) ∧K = J+1.
δtrcl(X, Z, K) ←
ntrcl(X, Y, J) ∧δtrcl(Y, Z, J)∧
not ntrcl(X, Z, J) ∧K = J+1.
From these examples it should be clear that the actual execution of Datalog pro-
grams must be preceded by an analysis step that determines if the rules are linear
or non–linear and applies the appropriate rewriting to each kind of rule. This
analysis is carried out by a compiler that also determines the safety of the pro-
gram at hand and performs optimization steps to take advantage of constraints in
the query goal. This optimization is described next.
Recursive Query Processing for Datalog
The standard bottom–up evaluation of a Datalog program P computes its complete
minimal model. A user, however, will usually pose a query Q, and is only interested
in the answers to this query. Say for instance that we have a query that seeks all
nodes that are reachable from an initial node, say aa; for this situation, generating
all pairs of nodes and then discard those that do not have aa as their ﬁrst argument
6A
further
improvement
consists
of
replacing
the
goal
ntrcl(Y, Z, J)
by
the
goal
ntrcl(Y, Z, J−1); this avoids the generation of the delta triplets at level J in both rules.
Logic and Databases: History of Deductive Databases
593

would be wasteful. A much better solution consists in specializing the program
of Example 1 by replacing variable X with the value aa and then implement the
program so re–written by the usual diﬀerential ﬁxpoint approach. However, if the
query instead asks for all pairs that have bb as their second argument, replacing
the variable Z by bb produces a program that only uses arcs that lead directly
to bb, and these arcs are not suﬃcient to answer a query that seeks paths of
arbitrary length. However, much in the way in which every left–regular grammar
derivation can be replaced by a right–linear grammar derivation, the recursive rule
of Example 1 can be rewritten into an equivalent one by switching trcl(X, Y ) ∧
arc(Y, Z) into arc(X, Y ) ∧trcl(Y, Z). With that and replacing every Z by bb,
our compiler produces an eﬃcient implementation for our query. Therefore, for
simple programs the Datalog technology leads to simple executions which, unlike
SLD–resolution, memorize previous answers to avoid duplicate computations and
achieve termination on cyclic graphs.
However as recursive programs become more complex, diﬀerent and more so-
phisticated methods are needed. Many of the techniques proposed focus on linear
rules since it was soon realized that these are the workhorse of deductive databases.
Thus for general linear rules, the magic sets technique transforms a Datalog pro-
gram into one that derives only a subset MP,Q ⊆MP [Bancilhon et al. , 1986].
Related techniques, such as supplementary magic sets [Sacc`a, 1987] and magic
counting [Sacc`a, 1987a] followed shortly afterwards.
Han and Henschen have investigated special linear recursions – later called chain
recursions, that include Example 1 as a very simple case – which can be evaluated
eﬃciently by wavefront techniques [Han and Henschen, 1987]. Han [Han, 1989]
and Seipel [Seipel, 1990] have independently proposed a normalization of general
linear recursions to chain recursions based on a graph–theoretic analysis. Earlier,
Naughton [Naughton, 1987] had investigated chains in the expansion of a linear
recursive rule and called a recursion k–sided, if there are k chains. He could only
characterize 1–sided linear recursions [Naughton, 1991]. 0–sided linear recursions
are called bounded, since they are eﬀectively equivalent to a non–recursive deﬁnite
logic program [Naughton and Sagiv, 1991]. Han and Seipel have given a complete
characterization for all k–sided recursions including 0– and 1–sided linear recur-
sions. Zhang et al. [Zhang et al., 1990] have characterized bilinear recursive rules
that can be normalized to linear recursive rules. Kang et al. [Kang et al., 2004]
have given suﬃcient conditions for the linearizability of multiple bilinear and linear
rules. So far, a general normal form for non–linear recursions could not be found.
Moreover, non–linear rules, such as those shown for Example 4.
require more
complex magic rules with sideways information passing [Beeri and Ramakrishnan,
1987].
Default Negation and Aggregates
Datalog’s emphasis on declarative semantics underscores the importance of ad-
dressing the nonmonotonic reasoning issues brought in by default negation and
Jack Minker, Dietmar Seipel and Carlo Zaniolo
594

also by aggregate functions, which are widely used in database applications. Three
broad research directions emerged as follows:
A Identify canonical semantics that incorporate nonmonotonic constructs. In
particular, the notions of perfect models [Przymusinski, 1989], well–founded
models [Van Gelder et al., 1991], and stable models [Gelfond and Lifschitz,
1988] have found many uses.
B Extend Datalog with special aggregates that preserve monotonicity w.r.t.
set–containment [Mazuran et al., 2013], and/or replace set–containment with
more general lattices in which aggregates behave in a monotonic fashion [Ross
and Sagiv, 1997; Van Gelder, 1993; Mumick et al., 1990].
C Resort to operational semantics based on iterated applications of the TP
operators for all programs, including those where this computation does
produce a solution for the least–ﬁxpoint equation when nonmonotonic con-
structs are used [Kolaitis and Papadimitriou, 1991].
As an example of approach A we see that the program in Example 2 and the
diﬀerential rules for Example 4 can be turned into a locally stratiﬁed program
by introducing a third argument into ntrcl and use it to express the successor
computations that are now indicated by the subscripts. Then the resulting pro-
gram becomes locally stratiﬁed according to the values of this third argument in
ntrcl7. Thus an iterated ﬁxpoint computation will produce the perfect model for
this program [Przymusinski, 1988a]. Although the iterated ﬁxpoint is a simple
procedure, the identiﬁcation of the strata to be used in this computation is not
– in fact it is undecidable in general [Palopoli, 1992]. Because of this problem,
many Datalog systems only support programs where the existence of stratiﬁcation
can be determined on the basis of the predicate names, as in the following exam-
ple which seeks to ﬁnd the least distance between nodes in a digraph, using the
negated goal shorter to exclude paths for which shorter paths exist between the
same two nodes.
EXAMPLE 5. Least distance between nodes in a directed graph.
mdist(X, Z, D) ←
path(X, Z, D) ∧not shorter(X, Z, D).
shorter(X, Z, D1) ←
path(X, Z, D2) ∧D2 < D1.
path(X, Z, D) ←
arc(X, Z, D).
path(X, Z, D) ←
pathX, Y, D1) ∧arc(Y, Z, D2) ∧D = D1 + D2.
This program can be stratiﬁed by letting the rules deﬁning path and shorter oc-
cupy the lower stratum, and the rule deﬁning mdist occupies the higher stratum
(as needed since the rule contains the negated shorter goal). Therefore, the iter-
ated fxpoint will ﬁrst produce all the possible paths between the nodes of the graph
and then select the min–cost ones. However, this computation does not terminate
7The ﬁrst rule derives atoms at stratum 1 using database facts. Then from the atoms at
stratum 1, the second and third rule allows us to infer atoms at stratum 2, and then from the
atoms so derived at stratum 2, we will infer those at stratum 3, and so on.
Logic and Databases: History of Deductive Databases
595

on cyclic graphs and it is very ineﬃcient in any case. For better performance and
guaranteed termination, the minimization should be “pushed” into the recursive
rules, whereby non–min paths between two nodes are discarded after each step of
the recursive computation. One way to express this improved computation con-
sists in using locally stratiﬁed rules similar to those in Example 2 (where negated
goals are now used to eliminate longer paths, rather than just duplicates as the
current rules of Example 2 do). The use of these programs, where the existence
of a local stratiﬁcation can be easily determined by a compiler on the basis of the
integer values used in a particular argument of the recursive predicate (the third
one in our example) was advocated in [Zaniolo et al., 1993] and [Lausen et al.,
1998] because of their power and amenability to eﬃcient implementation. How-
ever, recent advances obtained using approach B have revealed more elegant and
eﬀective solutions to this problem, and will be discussed in the next section. Before
that, we would also like to mention that well–founded semantics [Van Gelder et
al., 1991] is, or was, supported in several systems. The choice construct produces
multiple canonical models and deﬁnes a specialized subclass of stable models [Gi-
annotti et al., 1991], which can be computed eﬃciently and proved useful in many
applications [Giannotti et al., 2001]. In passing, we will also mention that using
an operational semantics instead of a declarative one, adopting the shortcut pro-
posed in [Kolaitis and Papadimitriou, 1991], has not found much following in the
Datalog community — a commitment to declarative semantics that is vindicated
by recent developments.
Recent Developments
The recent revival of interest in Datalog [Barcel´o and Pichler, 2012] is driven by
(i) the emergence of natural application areas [Loo et al., 2009; Hellerstein, 2010;
Abiteboul et al., 2011], (ii) the successful introduction of a commercial Datalog
system LogicBlox by industry [Green et al., 2012], and (iii) a number of theoretical
advances of practical signiﬁcance.
Declarative networking, which represents an important research area for net-
work technology, was shown in [Loo et al., 2009] to provide a natural application
area for Datalog. In fact, the introduction of location speciﬁcation into predicates
entails the derivation and correctness–proof for many network protocols, using ex-
tensions of Datalog bottom–up execution techniques such as diﬀerential ﬁxpoint.
Building on this initial success, the boom and Dedalus projects were launched that
seek to provide the foundation for the next generation of parallel and distributed
programming languages, by enhancing Datalog with declarative extensions that
capture location and time [Hellerstein, 2010].
WebdamLog, that supports the
distribution of both data and knowledge (i.e., programs) over a network of au-
tonomous peers, is another example of the many distributed Datalog systems that
were proposed recently [Abiteboul et al., 2011]. Approaches to data mining and
machine learning using systems such as LDL++ [Arni et al., 2003] were explored
in the past [Giannotti and Manco, 1999], and the need for scalable and portable
Jack Minker, Dietmar Seipel and Carlo Zaniolo
596

analytics for big–data has now rekindled interest in this key application area [Bu
et al., 2012].
These new applications, and the fact that a Datalog system developed in the
commercial world is now competing with systems, such as XSB Prolog that come
from the research world, go a long way toward dispelling the doubts on the prac-
ticality of Datalog–based languages and systems.
Recent theoretical advances are also paving the way to important new applica-
tions.8 For instance, there has been recent progress on the old problem of mono-
tonic aggregates in recursion mentioned in point B of the previous section. While
previous approaches using diﬀerent lattices for diﬀerent aggregates [Ross and Sa-
giv, 1997] faced diﬃcult compilation challenges [Van Gelder, 1993], monotonicity
under set–containment is preserved in a recently proposed approach [Mazuran
et al., 2013]. As a result, a large set of new algorithms can now be expressed
declaratively in Datalog and then computed eﬃciently using techniques such as
diﬀerential ﬁxpoint and magic set [Mazuran et al., 2013]. Also, there have been
many theoretical advances on the diﬃcult problem of extending Datalog to support
ontological reasoning on the Semantic Web [Gottlob et al., 2011]; this represents
another application area of great
4.5
Implementations of Deductive Databases – 1980–to–Present
Deductive Databases and the Database Community
In the mid–1980s, the database community, motivated by developments in deduc-
tive databases, initiated projects to develop prototype systems and implementation
algorithms. The earliest two groups were ECRC in Europe in 1984 directed by
Nicolas; and MCC in the U.S. in 1984, by Tsur and Zaniolo. These were followed
by J. Ullman and his group at Stanford in 1985. ECRC and MCC were among
the ﬁrst non–university organizations to develop prototypes.
Nicolas’s group developed the DDB system EKS–V1 [Vieille et al., 1990] that
included a top–down evaluation method, deductive query evaluation methods
(QSQ/SLD) [Vieille, 1986], integrity checking (Soundcheck), consistency check-
ing (SATCHMO) [Manthey and Bry, 1988], and aggregation through recursion.
The system was completed in 1990. In the early 1990s, Nicolas and his group
moved to the Groupe Bull. In 1992, Nicolas and Vieille headed an eﬀort to de-
velop the VALIDITY DDB [Friesen et al., 1996] system, an outgrowth of their
work at ECRC. VALIDITY integrated object–oriented features. VALIDITY was
being further developed and marketed by Next Century Media, Inc., a California
corporation.
Tsur and Zaniolo at MCC, developed the DDB system LDL. They empha-
sized bottom–up evaluation methods, query evaluation using semi–naive evalua-
8E.g., continuous count aggregates return all integer values from one until their ﬁnal counts;
thus, they are monotonic w.r.t. set containment, whereby their use in recursion preserves formal
semantics. Moreover, when simple equivalence conditions hold, they can be replaced by the more
eﬃcient traditional counts in the actual implementation of the recursive predicates.
Logic and Databases: History of Deductive Databases
597

tion, magic sets and counting, semantics for stratiﬁed default negation and set–
grouping, answer sets, and join order optimization. LDL was implemented in 1988,
and released in the period 1989–1991 [Tsur and Zaniolo, 1986]. It was among the
ﬁrst widely available DDBs and was distributed to universities and shareholder
companies of MCC. A new system named LDL++ was started in 1990 that ex-
tended LDL with powerful nonmonotonic constructs, such as nondeterministic
choice constructs and XY–stratiﬁcation [Zaniolo et al., 1993] that were later used
in Dedalus [Hellerstein, 2010]. The project was then continued at UCLA where
Zaniolo moved to in 1991.
Ullman’s project, NAIL! (Not Another Implementation of Logic!) developed
an early system NAIL, which was abandoned. A revised system used a core lan-
guage called Glue. Working with the NAIL! project, the ﬁrst paper on Magic Sets
[Bancilhon et al. , 1986] was published. Some other contributions by this group
are one of the ﬁrst papers on stratiﬁed default negation [Van Gelder, 1986]; the
ﬁrst paper on the well–founded semantics [Van Gelder et al., 1991]; and modular
stratiﬁcation [Ross, 1990].
Other deductive database systems of interest are discussed brieﬂy in the 1995
survey on deductive databases in [Ramakrishnan and Ullman, 1995]. The survey
focuses on implementation techniques and contains a table that lists 15 projects
that have led to implemented systems.
The systems listed are: Aditi, COL,
Concept–Base,
CORAL,
EKS–VI,
Logic–Base,
DECLARE,
Hy+,
X4,
LDL/LDL++, Logres, Glue–Nail, Starburst, and XSB, together with the follow-
ing features of each system (References, Recursion, Default Negation, Aggrega-
tion, Updates, Constraints, Optimizations, Storage and Interfaces). Some of the
systems may still be operational. See [Ramakrishnan and Ullman, 1995] for ad-
ditional information on these systems. Only one, XSB, is commercially in use at
the present time.
Dagstuhl Workshops on LPNMR
Logic programming and nonmonotonic reasoning (LPNMR) yearly workshops have
taken place at Dagstuhl, Germany starting in the 1990s. Developments in the late
1980s on default negation for deductive and disjunctive deductive systems led to
semantics for stratiﬁed and non–stratiﬁed systems, including answer sets, stable
models, and well–founded semantics. It was realized that to supplement the the-
oretical developments, it was necessary to develop systems. Following the 1996
workshop, U. Furbach at Koblenz University, alleges that motivated by a sugges-
tion by Minker at the workshop, he developed the ﬁrst web site of information
about logic program systems and applications. The web site listed 32 systems
and was last updated in 2000. Truszczynski at the 1997 workshop suggested the
need for testing current systems and for real applications. These early suggestions
were followed in 2002 by the recognition for systematic benchmarking of ASP sys-
tems. This inaugurated the Dagstuhl Initiative [Borchert et al., 2004] to create
a web–based benchmark archive, asparagus, at the University of Potsdam. The
Jack Minker, Dietmar Seipel and Carlo Zaniolo
598

web page consists of a depository of test problems and a uniform platform on
which to test ASP solvers. Test and application problems are continually added
to the depository. The Initiative is signiﬁcant since an international board has
been appointed to supervise the eﬀort and to assure continuity. Yearly competi-
tions take place between existing ASP and other solvers. Principal implementors
engage in discussions as to how to improve systems. Major improvements resulted
with ASP solvers. Results of contests are published on the web site. For example,
see https://www.mat.unical.it/aspcomp2013/FrontPage for information about the
Third ASP Competition and an announcement of the Fourth ASP Competition.
The 1997 workshop may have been the ﬁrst to feature demonstrations of de-
ductive systems for NMR using logic programs with default negation, and hence
for deductive databases. In a report on the workshop, Milnikel [Milnikel, 1997]
stated:
LPNMR reﬂected a new stage in the development of this cross-disciplinary
ﬁeld, with demonstrations of ten implemented systems joining the sub-
mitted papers, invited talks and panel discussions.
Two of the ten demonstrated systems eventually became available commercially:
Smodels [Niemel¨a and Simons, 1997] and XSB [Rao et al., 1996]. They were among
the ﬁrst to implement the well–founded semantics (WFS). The XSB system may
have preceded Smodels.
Smodels also computes stable model semantics for range–restricted function-
free normal programs. Extensive pruning is done during backtracking searches,
using approximations to stable models and thus runs in linear space. Smodels has
been improved over these past years (see [Niemel¨a and Simons, 2000]). Neotide, a
commercial ﬁrm, licensed and used Smodels, but no longer does so.
XSB uses SLG resolution to compute the WFS for normal programs. In addition
it is a full Prolog system and handles HiLog terms. Much of the speed of XSB is
due to its extensive tabling and indexing algorithms developed by Warren and his
group. Warren formed XSB, Inc. to utilize XSB on problems for the government
and industry. Many improvements have been made to XSB, including the ability to
use disjunctive rules that contain default negation. XSB, Inc. is still in existence.
Two of the systems discussed, DLV, [Leone et al., 2006] and DeReS [Cholewinski
et al., 1996] were disjunctive logic and deductive database systems implemented a
few years after the workshop. While subsequently DLV became available commer-
cially, DeReS is no longer available. See Section 6 for further information about
DLV.
Other systems discussed and/or demonstrated were: DisLop [Aravindan et al.,
1997] a disjunctive logic program; REVISE [Damasio et al., 1997] for revising con-
tradictory extended logic programs; DeRes [Cholewinski et al., 1996] a disjunctive
logic program; LOLA [Zukowski, 1997] a deductive database system for modu-
larly stratiﬁed programs; ACLP [Kakas and Mourlas, 1997] integrates abduction
into a constraint logic programming environment; FLORID, F–Logic reasoning in
databases [Kandzia, 1997]; and XRay, an implementation platform for local query
Logic and Databases: History of Deductive Databases
599

answering in default logics; and DDBase [Seipel, 1997], which mixes Datalog and
Prolog evaluation, allows for data structures and aggregation over arbitrary user–
deﬁned aggregation functions.
Finally, ASP researchers have often used diﬀerent techniques to address prob-
lems ﬁrst studied in Datalog: among the many examples, we ﬁnd work on aggre-
gates [Faber et al., 2008; Faber et al., 2011] and dynamic magic sets optimization
[Alviano and Faber, 2011].
Logic–Based AI (LBAI) Workshop 1999
The 1999 LBAI workshop [Minker, 2000] featured theory and demonstrations of
deductive systems. Twelve systems were demonstrated. Only one system, XSB,
based on WFS by Warren [Rao et al., 1996] and his group was commercially avail-
able. A second system, DLV, based on ASP by Eiter et al. [Eiter et al., 2000a], im-
plemented in 1997 for disjunctive deductive databases, and logic programs, became
commercially available in 2005 and remains commercially available. Problems can
be solved in a natural way using a Guess&Check paradigm where solutions are
guessed and veriﬁed by parts of the program. DLV is the ﬁrst comprehensive sys-
tem developed that handles disjunction. The commercially available system (see
Section 6) contains many sophisticated features. A third system, Smodels, was
based on ASP and WFS and incorporated new constructs such as cardinality and
weight constraints [Niemel¨a and Simons, 2000].
Subsequently, the architecture
incorporated two interacting Smodels solvers for non–disjunctive programs. One
is responsible for generating as good as possible model candidates while the other
checks for minimality, as required from disjunctive stable models [Janhunen et al.,
2006].
Of the twelve systems, three, in addition to DLV and Smodels represented im-
plementations of logic programming useful for databases. XSB, discussed earlier, is
a full–ﬂedged Prolog system (with debugger, tracer, etc.) handles programs under
ASP and WFS (using tabling).
LDL++ a deductive database system, has in-
corporated user–deﬁned aggregates, XY–stratiﬁed programs [Zaniolo et al., 1993],
and nondeterministic choice constructs under stable model semantics [Greco et al.,
1990].
All twelve systems in this paragraph and the preceding paragraphs, demon-
strated the ﬁeld of NMR had matured beyond theoretical papers to systems used
to solve signiﬁcant problems. The diversity of topics covered agents, causal the-
ories, deductive databases, dynamic domains, inductive logic programming, plan-
ning (the ﬁrst solution of McCarthy’s planning problem), and robotics.
In addition to the above, very eﬃcient solvers have been developed by Schaub
and his colleagues, who show the importance of careful implementations to use
with large systems by their work on ASP systems such as clasp [Gebser et al.,
2009].
clasp and related systems are part of Potassco, the Potsdam Answer
Set Solving Collection, which bundles tools for ASP, developed at the Univer-
sity of Potsdam.
(For details on the Potassco systems, access the web pages
Jack Minker, Dietmar Seipel and Carlo Zaniolo
600

http://potassco.sourceforge.net/ and http://www.cs.uni-potsdam.de/wv/oclingo/.
Additional information about eﬃcient solvers may be found by an Internet search
for ‘answer set programming web site’. The results of the search will give websites
for competitions. Accessing each competition provides details of the systems in
the competition and the results of the competition.)
5
THEORY OF DISJUNCTIVE DEDUCTIVE DATABASES
The ﬁeld of disjunctive logic programming (DLP) started approximately in 1982
with the paper by Minker [Minker, 1982], who devised a consistent theory of nega-
tion for disjunctive deductive databases (DDDBs). Work in disjunctive theories
was pursued seriously after a workshop organized by Minker in 1986 [Minker,
1986]. Motivated by the developments at the workshop, on a ﬁxpoint operator
for stratiﬁed programs, Minker realized that there must be a ﬁxpoint operator for
disjunctive theories. Together with Rajasekar, Minker developed ﬁxpoint, model
theoretic, and proof theoretic semantics for positive disjunctive logic programs.
The work extended the theory developed by van Emden and Kowalski for nor-
mal logic programs. Minker and his students and colleagues, during the period
1986–1997 developed results for disjunctive databases and disjunctive logic pro-
gramming, many of these results appear in the monograph [Lobo et al., 1992].
This work laid the foundation for work in disjunctive theories. For a historical
perspective of DLPs and DDDBs, see [Minker, 1999] and [Minker and Seipel,
2002]. There is a major diﬀerence between Datalog databases and Disjunctive
Datalog databases. Whereas Datalog databases have a unique minimal model,
Disjunctive Datalog databases generally have multiple minimal models.
5.1
Overview
The ﬁeld of disjunctive logic programming has made large strides in studies since
the mid–1990’s so that there are now signiﬁcant semantics available, implemen-
tations of large systems, and a wide range of practical applications, such as data
integration and diagnosis, that cannot be solved by normal logic programs.
Semantics
For disjunctive theories, the work of Minker [Minker, 1982] on default negation and
that of Minker and Rajasekar [Minker and Rajasekar, 1990] laid the groundwork
for further developments. They showed that a corresponding ﬁxpoint theory for
pure disjunctive theories, based on an operator T s
P mapping disjunctions of atoms
to disjunctions of atoms, generalized the consequence operator TP of van Emden
and Kowalski and led to the concept of a minimal Herbrand state, where a state
is a set of disjunctions of atoms.
The minimal Herbrand state for disjunctive
theories corresponds to the minimal Herbrand model of Horn theories. The work
Logic and Databases: History of Deductive Databases
601

by Przymusinski [Przymusinski, 1988] and by Rajasekar and Minker [Rajasekar
and Minker, 1990] extended the work to stratiﬁed theories.
It is clear that the dominant semantics of disjunctive logic programming with
default negation is the stable model semantics of Gelfond and Lifschitz [Gelfond
and Lifschitz, 1988] (see Section 2) and its generalizations.
Based on the sta-
ble model semantics, Gelfond and Lifschitz [Gelfond and Lifschitz, 1990] have
developed answer set semantics for disjunctive theories that include both de-
fault and logical negation.
The work in [Yuan and You, 1993] and the work
on static semantics developed by Przymusinski [Przymusinski, 1995] appear to
be related.
As shown in [Brass et al., 1998], these approaches, though diﬀer-
ently deﬁned, are also related to the D–WFS approach of [Brass and Dix, 1995;
Brass and Dix, 1999]. For alternative semantics, see Lobo et al. [Lobo et al.,
1992] and the survey articles by Minker [Minker, 1999] and by Minker and Seipel
[Minker and Seipel, 2002]. A general framework for semantics in disjunctive de-
ductive databases is presented in [Seipel, 2001].
In addition to well-founded and ASP approaches to answering queries in dis-
junctive deductive databases, there have been several other approaches.
Yuan
and Chiang [Yuan and Chiang, 1989] developed a sound and complete algorithm
for relational databases with disjunctive data. Fern´andez and Minker [Fern´andez
and Minker, 1991] developed a bottom–up evaluation of hierarchical disjunctive
deductive databases; and in [Fern´andez and Minker, 1995], developed an evalu-
ation method for perfect models of disjunctive databases. Yahya [Yahya, 1997]
developed a procedural approach to answering queries in disjunctive deductive
databases; and in [Yahya, 2000] developed a method to answering generalized
queries. [Cumbo et al., 2004] enhanced the magic–sets optimization technique –
that was originally deﬁned for (non–disjunctive) Datalog programs – to (partially)
bound queries over disjunctive Datalog programs.
Among the early papers on updates in disjunctive deductive databases were by
[Grant et al., 1993], who addressed the problem of view updates in stratiﬁed dis-
junctive databases; and by [Fern´andez et al., 1996] who develop a model theoretic
approach to view updates in disjunctive databases.
Systems and Applications
Large scale systems which can handle thousands of rules have been implemented.
Since most nonmonotonic reasoning systems can be mapped to DLPs, one now has
a mechanism to implement nonmonotonic systems. Among those systems that
have been implemented and that are available commercially are XSB, Smodels,
and DLV. XSB can handle only extended logic programs, but it contains all of the
features that are available in Prolog and extends Prolog to extended logic programs
that can handle the stable model semantics and some aspects of the well-founded.
Smodels handles stable model semantics. DLV is a disjunctive system and is being
extended to handle large systems. See Section 6 for some companies that use these
semantics.
Jack Minker, Dietmar Seipel and Carlo Zaniolo
602

For some time, there was a sense that disjunctive theories would not be needed
for applications of interest. In the meantime many papers have shown that this
is not the case. In data integration problems, when one has a resource deﬁned by
multiple rules, to determine whether a query can be answered by resource rules
only, reduces to a set of disjunctive clauses. As noted by Baral and Gelfond [Baral
and Gelfond, 1994] and the book by Baral [Baral, 2004], large classes of knowl-
edge base system applications are formulated using disjunctive clauses. Abductive
reasoning, useful in diagnosis, reduces in many instances to handling disjunctive
data. See [Sakama and Inoue, 2000] for the relationship between abductive logic
programming and disjunctive logic programming; and [You et al., 2000] for an
abductive approach in disjunctive logic programming. In addition, some prob-
lems in cognitive robotics need to be represented by disjunctive data. Many of
the disjuncts can be disambiguated by testing sensors to determine the disjunc-
tive condition that may apply, and hence reduce the planning problem to a Horn
theory. Problems that relate to nonmonotonic reasoning may be transformed to
disjunctive logic programs, and hence, solved by the existing implementations.
These applications show the relevance of disjunctive logic programming to real
world problems. Further, recent applications will be discussed in more detail in
Section 6.
The development of Disjunctive Datalog has introduced the ability to solve
problems that cannot be handled by Datalog. Systems, such as DLV, are now
available to solve large, complex, commercial applications. This has enlarged the
types of problems that can now be solved.
5.2
Complexity and Properties of Semantics
Imielinski and Vadaparty [Imielinski and Vadaparty, 1989], Vardi [Vardi, 1982]
and Imielinski [Imielinski, 1991] have investigated the complexity of answering
queries in disjunctive logic programs.
Chomicki and Subrahmanian [Chomicki
and Subrahmanian, 1990] discuss the complexity of the GCWA. For disjunctive
theories that are tractable, see [Ben-Eliyahu and Dechter, 1994]. For complexity
results for disjunctive propositional logic programs see, e.g., Eiter and Gottlob
[Eiter and Gottlob, 1995] and their paper with Dantsin and Voronkov [Dantsin
et al., 2001]. Eiter and Gottlob have accomplished signiﬁcant complexity results.
They show that disjunctive logic programs capture computationally hard problems
that cannot be handled by extended logic programs. This makes it possible to
implement extremely complex problems from the complexity class Σp
2, such as the
strategic companies problem [Eiter et al., 2000a]9.
Eiter, Gottlob, and Mannila [Eiter et al., 1997] study the expressive power
and the complexity of the minimal model semantics, the perfect model semantics,
and the stable model semantics for Disjunctive Datalog. They show that these
semantics express the same set of queries; they precisely capture the complexity
class Σp
2. Thus, unless the Polynomial Hierarchy collapses, Disjunctive Datalog
9which is Σp
2–complete as a decision problem
Logic and Databases: History of Deductive Databases
603

is more expressive than Datalog with default negation.
These results are not
only of theoretical interest; they demonstrate that problems relevant in practice
such as computing the optimal tour value in the Traveling Salesman Problem
and Eigenvector computations can be handled in Disjunctive Datalog, but not
in Datalog with default negation. In addition, they study modularity properties
of Disjunctive Datalog and investigate syntactic restrictions of the formalisms.
Eiter and Gottlob [Eiter and Gottlob, 1997] show that over the standard inﬁnite
Herbrand universe, disjunctive logic programming and normal logic programming
under the (cautious) stable model semantics coincide.
Ben–Eliyahu and Dechter [Ben-Eliyahu and Dechter, 1994] investigate tractable
cases of disjunctive theories. They introduced the concept of a head–cycle free
(HCF)program.
P is head–cycle free (HCF) if for every two predicate symbols
p and q, if p and q are on a positive directed cycle in the dependency graph GP
then there is no rule in P in which both p and q appear in the head. They show in
[Ben-Eliyahu et al., 1996] that answers to queries expressed in this language can
be computed in polynomial time. The language is suﬃciently powerful to express
all polynomial time queries. For HCF theories, it is shown in [Ben-Eliyahu and
Palopoli, 1994] that there is an algorithm that performs minimal model ﬁnding and
minimal model checking in polynomial time. An eﬃcient algorithm to solve the
(co–NP–hard) problem of checking if a model is stable in function–free disjunctive
logic programs is developed in [Leone et al., 1997].
Dix et al. [Dix et al., 1996] describe causal programs [Chomicki and Subrah-
manian, 1990], where disjunction is simulated by default negation. Disjunctive
programs are reduced to stratiﬁed non–disjunctive programs by a series of shift–
operations. They show causal semantics belongs to the ﬁrst level of the polynomial
hierarchy unlike minimal model semantics (GCWA), which is ΠP
2 –complete for
positive disjunctive programs. Causal semantics are also cumulative and rational.
The class of positive causal programs extends the class of positive HCF programs
[Ben-Eliyahu and Dechter, 1994].
In approximate reasoning, one may give up soundness or completeness of an-
swers. Eﬀorts have been developed both for deductive and disjunctive deductive
databases by Selman and Kautz [Selman and Kautz, 1996], who developed lower
and upper bounds for Horn (Datalog) databases, and compilation methods by
Cadoli [Cadoli, 1993], who developed computational and semantical approxima-
tions, and by del Val [del Val, 1995], who developed techniques for approximating
and compiling databases. See Cadoli [Cadoli, 1996] for references on compilation,
approximation and tractability of knowledge bases.
There have been no guidelines developed which semantics should be used for a
particular application, and under what circumstances. However, many complexity
results have been obtained for these semantics. Schlipf [Schlipf, 1995] and Eiter
and Gottlob [Eiter and Gottlob, 1995] summarize complexity results for alternative
semantics. A user may wish to determine the semantics to be used based upon the
complexity expected to ﬁnd answers to queries. A second way to determine the
semantics to be used for an application is through their properties. Dix proposed
Jack Minker, Dietmar Seipel and Carlo Zaniolo
604

useful criteria for determining the appropriate semantics. He developed semantics
both for normal logic programs [Dix, 1995] and disjunctive logic programs [Dix,
1992] that satisfy some of the properties that he describes.
A third criterion,
suggested by Gelfond, is the ease and faithfulness of translation from natural
language to formal logic. While some properties are adaptations and extensions
to those developed by Kraus et al. [Kraus et al., 1990] to compare nonmonotonic
theories, relevance, partial evaluation and modularity were newly developed.
5.3
Relationship to More General Forms of Non–Monotonic Reason-
ing
The ﬁeld of nonmonotonic reasoning has resulted in several alternative approaches
to perform default reasoning [McCarthy, 1980; Reiter, 1980; McDermott and
Doyle, 1980; Moore, 1984; Moore, 1985]. The articles [Minker, 1993; Eiter and
Gottlob, 1995; Cadoli and Schaerf, 1993] cite results where alternative theories
of nonmonotonic reasoning can be mapped into extended disjunctive logic pro-
grams and databases. The expressive power of a query language over a disjunctive
ground database is studied in [Bonatti and Eiter, 1996]; they show there exist
simple queries that cannot be expressed by any preferential semantics (including
minimal model semantics and various forms of circumscription), while they can be
expressed in default and autoepistemic logic. Default logic, autoepistemic logic and
some of their fragments are shown to express the same class of Boolean queries,
which turns out to be a strict subclass of the Σp
2–recognizable Boolean queries.
They prove that under the assumption that the database consists of clauses whose
length is bounded by some constant, default logic and autoepistemic logic can
express all of the Σp
2–recognizable Boolean queries, while preference–based logics
cannot.
Yuan and You [Yuan and You, 1993] investigate relationships between autoepis-
temic circumscriptionand logic programming; they use two diﬀerent belief con-
straints to deﬁne the stable circumscriptive semantics and the the well–founded
circumscriptive semantics, for autoepistemic theories. See Cadoli and Lenzerini for
complexity results concerning circumscription and closed–world reasoning [Cadoli
and Lenzerini, 1994].
DDDBs have also contributed to the null value problem: if an attribute of a
relation may have a null value, where this value is part of a known set, then one
can represent this as a disjunction. For papers on the null value problem both in
relational and DDBs see [Grant and Minker, 1986; Reiter, 1986; Zaniolo, 1984].
5.4
Extended Disjunctive Deductive Databases
Gelfond and Lifschitz [Gelfond and Lifschitz, 1990; Gelfond and Lifschitz, 1991]
pointed out the need for another form of negation next to default negation (not ).
They called it classical negation (¬), and they extended stable model semantics
to work with classical negation, i.e., they deﬁned answer set semantics. Alferes,
Logic and Databases: History of Deductive Databases
605

Pereira and Przymusinski [Alferes et al., 1998] introduced two other forms of nega-
tion, which they summarize as symmetric negations: strong negation and explicit
negation.
For logic programs with stable model semantics both coincide with
classical negation. Symmetric negation can be used to provide natural solutions
to various knowledge representation problems, such as theory and interpretation
update, and belief revision. Minker and Ruiz [Minker and Ruiz, 1994] describe
general techniques for extending semantics to extended disjunctive logic programs,
and they apply these techniques to stable models, disjunctive well–founded [Ross,
1989] and stationary semantics [Przymusinski, 1990]. In [Ruiz and Minker, 1998]
they study the semantics of extended disjunctive logic programs that simultane-
ously contain multiple kinds of default negation.
Buccafurri, Faber, and Leone
[Buccafurri et al., 2002] have proposed a new knowledge representation language
DLP<, which extends disjunctive logic programming with strong negation by in-
heritance.
Extensions of disjunctive logic programs to more general belief programs have
been considered in [Brass et al., 1998]. Buccafurri et al. introduce the language
DOL (Disjunctive Ordered Logic) and show that it is useful for diagnostic processes
based on stepwise reﬁnements, and they study the expressive power and complexity
of DOL. Buccafurri et al. [Buccafurri et al., 2000] also extended disjunctive logic
programs by weak constraints and deﬁned a semantics which tends to minimize
the number of violated instances. Logic programs have also been extended with
preferences. Brewka, Niemel¨a, and T. Syrj¨anen have investigated logic programs
with ordered disjunctions [Brewka et al., 2004] and preferences and nonmonotonic
reasoning [Brewka et al., 2008]. In [Brewka and Eiter, 1999], Brewka and Eiter
discuss priority information on extended logic programs, preferred answer sets,
and principles that an approach to handling priorities should satisfy.
6
SYSTEMS, COMPANIES AND APPLICATIONS
The past several years have seen many uses of Datalog and Disjunctive Datalog
to solve real world problems. In [Minker and Seipel, 2002], [Barcel´o and Pichler,
2012] several such problems were described: data integration, abductive reason-
ing, deduction vs. abduction, knowledge representation, medical diagnosis, travel
and tourism, graph problems, 3-colorability, and Hamiltonian path. The ability
of logic–based approaches to implement and execute these problems is signiﬁcant.
However, to make an impact, it is necessary to demonstrate that complex indus-
trial application problems can be handled, whereas conventional database systems
are unable to handle such problems. In the following two sections we describe
industrial companies that apply Datalog and/or Disjunctive Datalog systems and
several industrial applications that use such systems.
Jack Minker, Dietmar Seipel and Carlo Zaniolo
606

6.1
Companies Formed that use (Disjunctive) Datalog
Among the companies that have been formed to solve industrial applications that
require tools developed for Datalog and/or Disjunctive Datalog, are XSB Inc,
Exeura s.r.l., DLVSystem s.r.l., Neotide, Lixto Software GmbH, and LogicBlox.
To the best of the knowledge of the authors, these are the only companies.
XSB Inc.
XSB Inc.
founded by David Warren, Terrance Smith, and Kostis
Sagonas in 1998 may have been the ﬁrst company to provide a commercial sys-
tem, XSB, that utilizes the well–founded semantics. XSB system, an open–source
multi–threaded logic programming system that extends Prolog with new seman-
tic and operational features, is mostly based on the use of the powerful idea of
Tabled Logic Programming or tabling. By memorizing previously proven goals
tabling avoids repeated derivations, a feature needed to ensure termination in the
presence of cycles in graphs. Tabling can be used to evaluate normal logic pro-
grams under the well–founded semantics (WFS). Extensions to XSB also permit
disjunctive rules.
Exeura s.r.l.
Exeura s.r.l. was founded in 2001 by the developers of DLV with
the goal of developing solutions to Knowledge Management problems.
Exeura
oﬀers software for data mining, content categorization and knowledge representa-
tion and reasoning. Exeura’s products employ OntoDLV, a system for developing
enterprise applications based on an extension of Disjunctive Datalog that allows
one to express and reason on ontologies [Ricca et al., 2009]. A detailed description
of the use of OntoDLV in the Gioia–Tauro Seaport application is given in Section
6.2.
DLVSystem.
DLVSystem s.r.l. is the second of two companies formed by the
developers of DLV. DLVSystem provides the ﬁrst commercially available Disjunc-
tive Datalog system. The company was founded in 2005 by Nicola Leone, who was
also one of the founders of Exeura. DLVSystem focuses primarily on research and
development of the DLV system [Leone et al., 2006], with the purpose of licensing
the core system as well as tools for application development [Febbraro et al., 2011],
and a comprehensive suite of DLV–based solutions aimed at knowledge intensive
applications. Indeed, other than being packaged as a stand–alone solution, the
DLV system constitutes the technological basis for providing (or integrating into)
a larger solution to enable organizations solving knowledge–based business prob-
lems. Thus, DLVSystem also provides consultancy and support for its use in an
industrial context. In partnership with companies such as Exeura, the company
has been involved frequently in the development of industrial applications that
exploit DLV technology.
Furthermore, ASPIDE, a comprehensive Integrated Development Environment
for ASP [Febbraro et al., 2011] supporting the entire life–cycle of ASP develop-
ment, from (assisted) program editing to application deployment has recently been
Logic and Databases: History of Deductive Databases
607

incorporated into DLV. It is among the ﬁrst tools introduced for Datalog and/or
Disjunctive Deductive Datalog systems. ASPIDE integrates a cutting–edge editing
tool, some of whose functionalities include dynamic syntax highlighting, on–line
syntax correction, auto–completion, code–templates, quick–ﬁxes, and refactoring,
with a collection of user–friendly graphical tools for program composition, debug-
ging, proﬁling, database access, solver execution conﬁguration and output han-
dling.
Neotide.
Neotide is a commercial ﬁrm in Finland that licensed and used Smod-
els. Smodels implements stable model computation for normal logic programs.
To enable realistic applications, they have introduced new constructs including
cardinality and weight constraints [Niemel¨a and Simons, 2000]. Janhunen et al.
[Janhunen et al., 2006] extended Smodels to include disjunctive databases. See
[Niemel¨a and Simons, 2000] for additional capabilities of Smodels, including ap-
plications and comparison with state–of–the–art satisﬁability checkers. As noted
in Section 4.5, Neotide no longer licenses Smodels.
Lixto Software GmbH.
Lixto is a Datalog–based extraction engine developed
by the Lixto Software GmbH [Baumgartner et al., 2001; Gottlob et al., 2004].
The system assists the user to semi–automatically create wrapper programs for
translating relevant pieces of Html pages into Xml. Lixto provides a visual and
interactive interface to deliver to users an advanced functionality that is reﬂected
by the internal Elog language. The theoretical underpinning of the Lixto system
may be found in [Gottlob and Koch, 2004].
LogicBlox.
A commercial software company founded in 2003, has developed the
LogiQL system to provide a uniﬁed database foundation for the next generation
of smart analytical and transactional applications [Green et al., 2012]. LogiQL is
an extended version of Datalog that builds on the technology of systems such as
LDL++, Nail! and Coral described in Section 4.4 – a fact that is often emphasized
by LogicBlox along with its ongoing collaborations with researchers and academics.
In addition to the use of default negation and aggregates in stratiﬁed programs,
LogiQL allows their use in recursive predicates that are applied to acyclic relations.
Additional LogiQL features include support for integrity constraints, and an inter-
nal database providing a high–performance implementation of joins. The system
also allows the use of functions deﬁned in C++ and the deﬁnition of libraries of
compiled modules.
LogicBlox partners with other software vendors to develop applications in their
domain of expertise. Thus, they have built applications containing anywhere from
a few thousand, to up to 50,000 rules, including the following: for retailers, they
built merchandising applications and planning software to optimize their supply
chain; for IT departments, they built project planning applications to staﬀpeople
on projects; for academic researchers they built program analysis software, that
they claim beats the prior-art Java points-to analysis by orders of magnitude.
Jack Minker, Dietmar Seipel and Carlo Zaniolo
608

6.2
Industrial Applications that use (Disjunctive) Datalog 10
NASA Shuttle Planner.
Gelfond et al. [Nogueira et al., 2001] developed one of
the earliest applications using disjunctions, a planning system for the NASA Space
Shuttle, the Reaction Control System (RCS). The RCS provides the maneuvering
capabilities while the Shuttle is in orbit. Given a description of an initial state
of the RCS, a set of policies, and a collection of jets that must be pressurized, a
planning task consists in ﬁnding a sequence of actions that does not violate any
policy and that takes the RCS to a state in which the given jets are pressurized.
The model of the RCS contains 76 rules, of which several are recursive. The
planning module contains 20 rules, of which 15 encode heuristics and operating
policies and one is a disjunctive rule, implemented as a choice rule of the Smodels
solver. The rule informally states that any action may occur at any step in the
evolution of the RCS. This broad statement is constrained, in an incremental
fashion, by the encoding of heuristics and operating policies.
Time to revise a plan was tested on 2,000 instances, both automatically and
manually generated, which included various kinds of electrical and mechanical
faults.
The average time was 11 seconds per instance.
The hardest instances
took 1–2 minutes. The requirements of the customer, the United Space Alliance,
NASA was to solve a problem instance in 20 minutes or less. Flight controllers
take at least that amount of time, and cannot guarantee the correctness of the
sequences they ﬁnd. The system was tested in dry–runs to check for weaknesses in
the design of the computer program for the NASA Shuttle Planner. The customer
was satisﬁed with the results of the tests. It was never used in an actual space
mission as NASA terminated the Space Shuttle.
Team Building System for the Gioia–Tauro Seaport [Ricca et al., 2012].
The Team Building System application was implemented by Exeura S.R.L. using
OntoDLV, which in turn calls DLV. Every day at the seaport of Gioia–Tauro
in Italy, the largest trans–shipment terminal of the Mediterranean Coast, several
vessels arrive, carrying cars and trucks. The vehicles must be handled, warehoused,
processed and shipped to their ﬁnal destination.
The speciﬁc activities to be
performed depend on the type and conditions of shipment.
For example, the
vessel may be partially or fully loaded or unloaded, and some vehicles may have to
be repaired. Moreover, the time taken to process a ship is set by contract and must
be complied with. Depending on the activities to be performed, diﬀerent operators
are needed. Additional constraints are imposed by the operators’ contracts and
limitations on working hours. Given data regarding arriving and departing ships,
including information about their loads, as well as information about the available
workforce, the system needs to determine assignments of operators to teams and to
shifts in such a way as to satisfy all the constraints and contractual requirements.
The size of the problem is only partially known. In the published experiments,
10We thank Marcello Balduccini for his assistance with the NASA Shuttle Planner; and Nicola
Leone for his description of the Team Building System for the Gioia–Tauro Seaport application.
Logic and Databases: History of Deductive Databases
609

130 employees were considered. The warehousing capacity of the seaport is 15,000
vehicles.
The time to compute a solution ranges from a few seconds to a few
minutes. The system was written in the DLV dialect of ASP developed by the
founders of DLVSystem: T. Eiter, W. Farber, G. Gottlob, N. Leone, and G. Pfeifer.
The Computer used was the Intel Core 2 Due CPU P8600 at 2.40GHz with 4GB
RAM. The system has been adopted by the ICO BLG company, in charge of most
operations of this type in Gioia–Tauro.
The quality of the solutions found by
humans is substantially worse. Solutions found by the system involved 20% less
overtime. The system, developed by DLVSystem and Exeura has been in use for
several years. Feedback by the users has led to improvements in the system.
7
FUTURE EXTENSIONS NEEDED
There is a need for extensions of databases to cope with several areas and for
improvements in Datalog and Disjunctive Datalog. Among these areas are: tools
for database systems and users; data that resides on the web and massive amounts
of data, referred to as Big Data; databases that deal with dynamic domains; and
databases that contain probabilistic and causal information.
7.1
Tools for (Disjunctive) Datalog Systems and Users
Relational database technology has developed a wide range of tools that make
them convenient for users.
The development of systems in dealing with Data-
log, Disjunctive Datalog, and nonmonotonic reasoning has been done by another
community. If these systems are to be used, Leone in his invited lecture [Leone,
2007] at the Logic Programming and Nonmonotonic Reasoning 2007 workshop,
suggested several needs.
He stated that engineers are often unable to write correct and eﬃcient ASP
programs.
Users of these systems have to undergo training and tools have to
be developed for programmers. Programming environments are needed as well
as tools and techniques for debugging. Friendly interfaces should be developed
as well as answer set programming and well–founded methodologies. Work has
already started on some of these systems (see e.g., the ASPIDE system discussed
in the previous section), but needs of the ﬁeld oﬀer many opportunities for further
development.
7.2
The Web and Big Data
The Internet has created a fast–evolving new world where the whole web has
become an immense database supporting advanced queries and semantic searches.
This new reality is creating major opportunities and technical challenges that are
brieﬂy discussed next.
A ﬁrst aspect of this new reality involves answering queries using various re-
sources rather than a single relational database. The process of transforming a
Jack Minker, Dietmar Seipel and Carlo Zaniolo
610

query from the database relations to the resources is often referred to as query
folding or answering queries using views, where the views are the resources. Alon
Levy [Levy, 2000] was the ﬁrst to recognize the utility of a logic–based approach
to data integration. Grant and Minker [Grant and Minker, 2002] present a uni-
form approach that includes as special cases much of the previous work on this
subject. Their approach is logic–based using resolution. They deal with integrity
constraints, default negation, and recursion also within this framework. The use
of resources lead, in many cases, to disjunctive theories. See their bibliography for
additional references.
A second important aspect is the need to support deductive queries that ex-
ploit the complex semantic and ontological relationships provided by the Semantic
Web, which is producing new query languages, such as SPARQL, and new reason-
ing formalisms, such as Description Logic [Lukasiewicz, 2010; Eiter et al., 2008].
While the core functionality of SPARQL can be supported using standard Datalog
[Polleres, 2012], signiﬁcant extensions are needed toemulate description logic, as
demonstrated by the recent research on Datalog± [Cal´ı et al., 2011]; we expect
this topic will provide many challenges and opportunities for future research.
The large amounts of data collected from the web, and their usage in increasingly
complex and sophisticated decision–support analytics is often referred to as the Big
Data problem. The Big Data challenge has motivated calls for massively parallel
distributed systems to achieve the scalability required by Big Data applications.
In turn, this has generated much interest in using Datalog languages to express
advanced analytics and other data–intensive algorithms in ways that are conducive
to (i) massive parallelization and (ii) formal analysis of asymptotic properties such
as conﬂuence and eventual consistency [Marczak et al., 2012]. Now (ii) requires the
ability of representing and reasoning about dynamic aspects of data and rules — a
recurring problem in deductive database research for which past work is discussed
next.
7.3
Dynamic Domains and Databases
Problems in AI deal with representing and dealing with a world that is dynamic.
These problems arise in such applications as high level control of robots and in-
dustrial processes, and intelligent software agents. To handle such problems, Mc-
Carthy [McCarthy, 1963] in 1963 deﬁned the situation calculus and the importance
to represent actions and causal laws for realistic problems in AI. Cordell Green was
the ﬁrst to use the situation calculus for planning [Gre69a], however his approach
did not address the frame problem. The frame problem was initially formulated as
the problem of expressing a dynamical domain in logic without explicitly specify-
ing which conditions are not aﬀected by an action. According to Erik Sandewall1,
the ramiﬁcation problem, concerned with indirect consequences of an action, was
part of the original deﬁnition of the frame problem. Vladimir Lifschitz [Lifshitz et
al., 2000] said,
The discovery of the frame problem and the invention of nonmonotonic
Logic and Databases: History of Deductive Databases
611

formalisms that are capable of solving it may be the most important
signiﬁcant events so far in the history of research in reasoning about
actions. A large part of this story is described in Shanahan’s book
[Shanahan, 1997].
There was the belief in the AI community that a logic–based approach could not
solve the frame problem. As noted by Pirri and Reiter [Pirri and Reiter, 2000], this
critique was premature. Reiter [Reiter, 2001] in his book, Knowledge in Action,
demonstrates the importance of the situation calculus for robotics and dynamic
domains.
The event calculus, ﬁrst introduced by Kowalski and Sergot [Kowalski and
Sergot, 1986], is another important logic language used for dynamic domains.
Kowalski [Kowalski, 1992], describes a special case of the event calculus to update
databases. The book by Shanahan [Shanahan, 1997] further describes the use of
the event calculus for dynamic domains.
Levesque et al. [Levesque et al., 1997] and his colleagues developed a new logic
programming language called GOLOG whose interpreter automatically maintains
an explicit representation of the dynamic world being modeled, on the basis of
user–supplied axioms about the preconditions and eﬀects of actions and the initial
state of the world. This allows programs to reason about the state of the world
and consider the eﬀects of various possible courses of action before committing to
a particular behavior. The net eﬀect is that programs may be written at a much
higher level of abstraction than is usually possible.
Baral and Gelfond [Baral and Gelfond, 2002] discussed an architecture for in-
telligent agents based on the use of A–Prolog, a language based on ASP. They
represent agent’s knowledge about the domain and to formulate the agent’s rea-
soning tasks. They demonstrate their methodology of constructing such programs.
Additional work on dynamic systems and interfacing them with large databases
is required.
The work is important for extending the utility of nonmonotonic
reasoning and databases to reason about the state of the world in complicated
applications.
In addition, Datalog and Disjunctive Datalog systems should be
extended to include dynamic domains.
7.4
Causal Reasoning and Databases
The integration of Judea Pearl’s [Pearl, 1988] work on causality and probabilistic
reasoning with ASP as in the work of Baral, Gelfond, and Rushton [Baral et al.,
2009] was important as it expanded the applicability of systems to handle prob-
abilistic reasoning and causal reasoning. As with dynamic domains, it would be
useful if causal reasoning were incorporated into Datalog and Disjunctive Datalog
systems.
Jack Minker, Dietmar Seipel and Carlo Zaniolo
612

8
SUMMARY
As discussed in this chapter, the ﬁeld of logic and databases is an amalgamation
of logic programming and relational databases from two diﬀerent communities.
It took several years before each community took advantage of what the other
community had developed.
Relational databases were based on the relational calculus and the relational al-
gebra. The development of optimization techniques enabled the high-level declar-
ative logic–based queries of relational databases to be competitive with the low–
level procedural queries of network and hierarchical databases. Users of relational
databases were isolated from the data structures underlying the systems which
made this technology convenient for them. The result was the development of rela-
tional systems that could accommodate many relational tables and large amounts
of relations as needed by commercial applications. Additional features such as
aggregation, counting, convenient tools for programmers and users, integrity con-
straints, updates and views were added to systems. Early relational databases had
several limitations: e.g., views could not contain recursion and default negation.
Logic programming came out of the theorem proving and artiﬁcial intelligence
community. It is part of the ﬁeld of Computational Logic. The early work on
logic programming was the development of Prolog. Although Prolog was useful
for some AI problems, it was of limited use for relational databases. The top–down
approach of Prolog was useful for ﬁnding a single answer, but could not handle
massive sets of data. Additionally, the output of a query depends upon the order
of the conjuncts in the body of rules. Negation–as–failure in Prolog often does not
reﬂect what the user intends, and default negation with or without recursion was a
problem. Some of the early limitations of Prolog are now handled in XSB Prolog.
While standard Prolog cannot handle recursive rules, if the data is cyclic, XSB
Prolog can. In addition, Prolog could not handle disjunctive data and disjunctive
rules. Computational logic started with developments in AI. In the 1960s/1970s,
Cordell Green demonstrated how to do planning [Green, 1969] and McCarthy
proposed the use of the situation calculus [McCarthy, 1963] (reprinted in [Minsky,
1968]). As shown by Reiter [Reiter, 2001], the situation calculus is important for
dynamic systems and databases. It took until 2000 when a deﬁnitive solution to
McCarthy’s ‘oldest planning problem’ was given by Lifschitz et al. [Lifshitz et al.,
2000].
In the 1980s several theories of nonmonotonic logic, a subﬁeld of Computational
Logic, were developed. The main theories proposed were: cicumscription by Mc-
Carthy, [McCarthy, 1980]; default logic by Reiter [Reiter, 1980]; autoepistemic
logic by McDermott and Doyle [McDermott and Doyle, 1980]; and autoepistemic
logic by Moore [Moore, 1984; Moore, 1985]. These theories were needed to for-
malize the common sense assumption that things are as expected unless otherwise
speciﬁed. Relational databases make this assumption implicit as they use default
negation.
In the late 1980s, the logic programming community addressed the problem
Logic and Databases: History of Deductive Databases
613

of the use of default negation.
Systems were implemented to handle stratiﬁed
and non–stratiﬁed logic programs (see Sections 4.2 and 4.5). Soon thereafter, it
was shown by several individuals that a signiﬁcant portion of the three major
nonmonotonic logics could be implemented by logic programs (see [Minker, 1999;
Minker, 2011] for references).
These developments motivated the database community to recognize that the
ﬁeld of logic and databases was important for relational databases. Many commer-
cial applications require the ability to handle recursive rules and default negation.
Early work produced signiﬁcant contributions resulting in a major technology
transfer to mainstream RDBMS, where recursive queries are now part of the SQL
standards, and they are supported by most vendors using Datalog techniques such
as default negation and aggregates, and magic sets [SHP+96]. Another impor-
tant development was the implementation of Datalog prototypes. Although the
early prototypes did not evolve into commercial systems, they were useful for later
systems.
Bry [Bry, 1990] showed that the magic sets technique was a combination of
top–down and bottom–up evaluation used in logic programming. The perspective
from both communities has beneﬁtted both areas of research.
The logic programming community came to realize that they had to focus
more eﬀort on developing systems useful for commercial applications. As with
the database community, they developed prototype implementations. They took
advantage of the many accomplishments in the database community.
This led
to implementations of two semantics used by several commercial systems: the
well–founded semantics and the stable model semantics (see Section 4.5).
In the early 1990s, Minker together with his students and colleagues intro-
duced disjunctive logic programming. This overcame an important limitation of
the Prolog language, whereby many applications in AI and databases could not
be handled by Datalog can now be handled by Disjunctive Datalog. Eiter, Gott-
lob and Mannila [Eiter et al., 1997] show that disjunctive logic programs capture
computationally hard problems that cannot be handled by extended logic pro-
grams. This makes it possible to implement extremely complex problems from the
complexity class Σp
2, such as the strategic companies problem [Eiter et al., 2000a].
In contrast with Datalog, Disjunctive Datalog is confronted with multiple min-
imal models. The Minker GCWA was too complex, π0
2−complete [Chomicki and
Subrahmanian, 1990] to be used for default negation, and better approaches were
needed. Gelfond and Lifschitz [Gelfond and Lifschitz, 1990] developed an approach
that could handle default negation in disjunctive theories and also incorporated
classical negation. The approach also used the ideas they had developed for sta-
ble models for normal logic programs. They named the semantics Answer Set
Programming (ASP). SMS–ASP has become the most important semantics for
Datalog and Disjunctive Datalog.
As stated in a report [Milnikel, 1997] of a workshop at Dagstuhl, Germany in
1997, the workshop led to a cross disciplinary ﬁeld of conferences in LPNMR of
combining demonstrations of systems with submitted papers, invited talks and
Jack Minker, Dietmar Seipel and Carlo Zaniolo
614

panel discussions. This led to the Dagstuhl Initiative a few years later, described
in [Borchert et al., 2004], to develop a test bed of problems to be solved and to
improve systems. The improvements were such that several systems were made
available for commercial applications and the formation of companies. (See the
website, http://asparagus.cs.uni-potsdam.de, for additional information.) Many
of the improvements incorporated into the systems, such as magic sets and user
tools had been developed in the database community,
As described in Section 6, Datalog and Disjunctive Datalog systems have been
applied to solve diﬃcult problems in commercial applications. Several companies
have been formed based on Datalog and Disjunctive Datalog systems in the period
1998–2003.
In Section 7 we described some areas to which databases could be expanded.
Some of these areas are motivated by work in databases and others by work in AI
and computational logic. Tools needed to solve these problems will be required
to augment or extend Datalog and Disjunctive Datalog. Such developments from
AI as the solution of the frame problem, the situation calculus, and causality; and
from the database community to handle Big Data and web–based systems will be
needed.
At a workshop, Logic–Based Artiﬁcial Intelligence, [Minker, 2000] organized by
McCarthy and Minker, a discussion was held on questions posed by Melvin Fitting.
Among the questions discussed were:
1. Can logics be too complex to be used? The general consensus was “yes”, but
not a limiting factor. The general agreement was that a logic–based approach
that is too complex is evidence that the subject has not been understood at
a deep enough level.
2. Newtonian physics is to Mechanical Engineering as formal logic is to what?
It provoked the least commentary. McCarthy simply announced that the
answer was, “Computer Science.” There was nothing more to be said.
McCarthy, one of the founders of AI, made seminal contributions to the ﬁeld,
and has rarely been found to be wrong.
Computational logic opens up many
prospects for applications and extensions of logic and databases.
Datalog and
Disjunctive Datalog, which has resulted from the merger of the two ﬁelds of logic
programming and relational databases, will likely be part of the applications.
ACKNOWLEDGEMENTS
We greatly appreciate the comments of Marcello Balduccini, Michael Gelfond,
Bob Kowalski, and Nicola Leone who provided invaluable suggestions on technical
issues and references. We especially appreciate the comments by Georg Gottlob,
who served as the reader for the chapter and enlisted suggestions from his student
Michael Morak and from Nicola Leone. Their careful reading of the chapter and
Logic and Databases: History of Deductive Databases
615

detailed suggestions were very valuable. However, the authors are responsible for
any errors of fact, omissions, or judgement.
ABBREVIATIONS
AAAI: American Association for Artiﬁcial Intelligence
AMAI: Annals of Mathematics and Artiﬁcial Intelligence
CACM: Communications of the ACM
CADE: International Conference on Automated Deduction
DOOD: International Conference on Deductive and Object–Oriented Databases
ICLP: International Conference on Logic Programming
ILPS: International Logic Programming Symposium
IJCAI: International Joint Confonference on Artiﬁcial Intelligence
JACM: Journal of the ACM
JAI: Journal of Artiﬁcial Intelligence
JAR: Journal of Automated Reasoning
JCSS: Journal of Computer and System Sciences
JLP: Journal of Logic Programming
KR: Intl. Conf. on Principles of Knowledge Representation and Reasoning
LPNMR: Intl. Conf. on Logic Programming and Nonmonotonic Reasoning
PODS: ACM Symposium on Principles of Database Systems
TODS: ACM Transactions on Database Systems
TPLP: Theory and Practice of Logic Programming
BIBLIOGRAPHY
[Abiteboul, 2012] S. Abiteboul. Datalog: La renaissance. http://www.college-de-france.fr/
site/serge-abiteboul/course-2012-05-09-10h00.htm.
[Abiteboul et al., 2011] S. Abiteboul, M. Bienvenu, A. Galland, and E. Antoine. A Rule–Based
Language for Web Data Management. PODS’11, pp. 293–304, 2011.
[Apt et al., 1988] K.R. Apt, H.A. Blair, and A. Walker. Towards a Theory of Declarative Knowl-
edge. In [Minker, 1988], pp. 89–148. 1988.
[Aravindan et al., 1997] C. Aravindan, J. Dix, and I. Niemel¨a. DisLoP: Towards a Disjunctive
Logic Programming System. LPNMR’97, Springer, LNAI 1265, pp. 342–353, 1997.
[Alviano and Faber, 2011] M. Alviano, W. Faber.
Dynamic Magic Sets and Super–Coherent
Answer Set Programs. AI Commun., 24 (2), pp. 125–145, 2011.
[Abiteboul et al., 1995] S. Abiteboul, R. Hull, and V. Vianu.
Foundations of Databases.
Addison–Wesley, 1995.
[Arni et al., 2003] F. Arni, K. Ong, S. Tsur, H. Wang, and C. Zaniolo. The Deductive Database
System LDL ++. TPLP, 3(1):61–94, 2003.
[Alferes et al., 1998] J.J. Alferes, L.M. Pereira, and T.C. Przymusinski. “Classical” Negation in
NMR and LP. JAR, 20: pp. 107–142, 1998.
[Apt, 1990] K.R. Apt. Logic Programming. In J. van Leeuwen, editor, Handbook of Theoretical
Computer Science, pp. 493–574. Elsevier, 1990.
[Baral, 2004] C. Baral. Knowledge Representation, Reasoning and Declarative Problem Solving,
Cambridge University Press, 2004.
[Borchert et al., 2004] P. Borchert, C. Anger, T. Schaub, and M. Truszczynski. Towards Sys-
tematic Benchmarking in Answer Set Programming: The Dagstuhl Initiative. LPNMR’04,
pp. 3–7, 2004.
[Bu et al., 2012] Y. Bu, V. R. Borkar, M. J. Carey, J. Rosen, N. Polyzotis, T. Condie,
M. Weimer, and R. Ramakrishnan.
Scaling Datalog for Machine Learning on Big Data.
CoRR, abs/1203.0160, 2012.
Jack Minker, Dietmar Seipel and Carlo Zaniolo
616

[Brass and Dix, 1995] S. Brass and J. Dix. A General Approach to Bottom–Up Computation of
Disjunctive Semantics. In Nonmonotonic Extensions of Logic Programming. Springer, LNCS
927, pp. 127–155, 1995.
[Brass and Dix, 1999] S. Brass and J. Dix. Semantics of (Disjunctive) Logic Programs Based
on Partial Evaluation. JLP, 38(3): pp. 167–213, 1999.
[Brass et al., 1998] S. Brass, J. Dix, I. Niemelae, and T.C. Przymusinski:
A Comparison of
the Static and the Disjunctive Well–Founded Semantics and its Implementation. KR’98, pp.
74–85, Morgan Kaufmann, 1998.
[Bonatti and Eiter, 1996] P.A. Bonatti and T. Eiter. Querying Disjunctive Databases through
Nonmonotonic Logics. TCS, 160: pp. 321–363, 1996.
[Brewka and Eiter, 1999] G. Brewka and T. Eiter. Preferred Answer Sets for Extended Logic
Programs. JAI 109(1–2): pp. 297–356, 1999.
[Ben-Eliyahu and Dechter, 1994] R. Ben–Eliyahu and R. Dechter. Propositional Semantics for
Disjunctive Logic Programs. AMAI 12: pp. 53–87, 1994.
[Ben-Eliyahu and Palopoli, 1994] R. Ben–Eliyahu and L. Palopoli.
Reasoning with Minimal
Models: Eﬃcient Algorithms and Applications. KR’94, pp. 39–50, 1994.
[Ben-Eliyahu et al., 1996] R. Ben–Eliyahu, L. Palopoli, and V. Zemlyanker.
The Expressive
Power of Tractable Disjunction. ECAI’96, pp. 345–349, 1996.
[Brewka et al., 2011] G. Brewka, T. Eiter, M. Truszczynski.
Answer Set Programming at a
Glance. CACM, 54(12): pp. 92–103, 2011.
[Baumgartner et al., 2001] R. Baumgartner, S. Flesca, and G. Gottlob. Visual Web Information
Extraction with Lixto. VLDB’01, pp. 119–128, 2001.
[Buccafurri et al., 2002] F. Buccafurri, W. Faber, and N. Leone. Disjunctive Logic Programs
with Inheritance. TPLP, 2(3): pp. 293–321, 2002.
[Baral and Gelfond, 1994] C. Baral and M. Gelfond. Logic Programming and Knowledge Rep-
resentation. JLP, 19/20: pp. 73–148, 1994.
[Baral and Gelfond, 2002] C. Baral and M. Gelfond. Reasoning Agents in Dynamic Domains.
In: [Minker, 2000], pp. 253–279.
[Baral et al., 2009] C. Baral, M. Gelfond, and N. Rushton. Probabilistic Reasoning with Answer
Sets. TPLP, 9: pp. 55–144, 2009.
[Buccafurri et al., 2000] F. Buccafurri, N. Leone, and P. Rullo. Enhancing Disjunctive Datalog
by Constraints. IEEE Transactions on Knowledge and Data Engineering, 12(5): pp. 845–860,
2000.
[Bancilhon et al. , 1986] F. Bancilhon, D. Maier, Y. Sagiv, and J. D. Ullman. Magic Sets and
other Strange Ways to Implement Logic Programs. PODS’86, pp. 1–15, 1986.
[Brewka and Niemel¨a, 1998] G. Brewka and I. Niemel¨a. Report on the 7th Intl. Workshop on
Nonmonotonic Reasoning. AI Magazine 19(4), pp. 139 1998.
[Brewka et al., 2004] G. Brewka, I. Niemel¨a, and T. Syrj¨anen. Logic Programs with Ordered
Disjunction. JCI, 20: pp. 335-357, 2004.
[Brewka et al., 2008] G. Brewka, I. Niemel¨a, and M. Truszczynski. Preferences and Nonmono-
tonic Reasoning. AI Magazine, 29(4): pp. 69–78, 2008.
[Barcel´o and Pichler, 2012] P. Barcel´o and R. Pichler, editors. Datalog in Academia and Indus-
try – 2nd Int. Workshop, Datalog 2.0, Springer, LNCS 7494, 2012.
[Beeri and Ramakrishnan, 1987] C. Beeri and R. Ramakrishnan.
On the Power of Magic.
PODS’87, pp. 269–284, 1987.
[Bry, 1990] F. Bry.
Query Evaluation in Deductive Databases: Bottom–Up and Top–Down
Reconciled. Journal of Data & Knowledge Engineering, 5(4): pp. 289–312, 1990.
[Cadoli, 1993] M. Cadoli.
Semantical and Computational Aspects of Horn Approximations.
IJCAI’93, pp. 39–44, 1993.
[Cadoli, 1996] M. Cadoli. Panel on Knowledge Compilation and Approximation: Terminology,
Questions, References. In Proc. of the 4th Int. Symp. on AI and Math. (AI/Math’96), pp.
183–186, 1996.
[Cumbo et al., 2004] C. Cumbo, W. Faber, G. Greco, N. Leone.
Enhancing the Magic–Set
Method for Disjunctive Datalog Programs. ICLP’04, pp. 371–385, Springer, 2004.
Logic and Databases: History of Deductive Databases
617

[Ceri et al., 1990] S. Ceri, G. Gottlob, L. Tanca Logic Programming and Databases. Springer,
1990.
[Chimenti et al., 1990] D. Chimenti, R. Gamboa, R. Krishnamurthy, S. A. Naqvi, S. Tsur, and
C. Zaniolo. The LDL System Prototype. IEEE Trans. Knowl. Data Eng., 2(1): pp. 76–90,
1990.
[Chakravarthy et al., 1990] U.S. Chakravarthy and J. Grant and J. Minker. Logic Based Ap-
proach to Semantic Query Optimization. ACM TODS, 15(2): 1990.
[Cal´ı et al., 2011] A. Cal`ı, G. Gottlob, and A. Pieris. New Expressive Languages for Ontological
Query Answering. In AAAI, 2011.
[Chandra and Harel, 1985] A. Chandra and D. Harel. Horn Clause Queries and Generalizations.
JLP, 2(1): pp. 1–15, 1985.
[Chaudhuri, 1998] S. Chaudhuri An Overview of Query Optimization in Relational Systems.
PODS’98, pp. 34–43, 1998.
[Chang, 1978] C. Chang.
Deduce 2:
Further Investigations of Deduction in Relational
Databases. In: [Gallaire and Minker, 1978], pp. 201–236, New York, 1978.
[Calimeri et al., 2012] F. Calimeri, G. Ianni, F. Ricca. The third open Answer Set Programming
Competition. CoRR abs/1206.3111, 2012, http://arxiv.org/pdf/1206.3111.pdf.
[Cadoli and Lenzerini, 1994] M. Cadoli and M. Lenzerini.
The Complexity of Closed World
Reasoning and Circumscription. JCSS, 43: pp. 165–211, 1994.
[Clark, 1978] K.L. Clark. Negation–as–Failure. In [Gallaire and Minker, 1978], pp. 293–322.
1978.
[Cholewinski et al., 1996] P. Cholewinski, V.W. Marek, M. Truszczynski.
Default Reasoning
System DeReS. KR 1996, pp. 518–528, 1996.
[Codd, 1970] E.F. Codd. A Relational Model of Data for Large Shared Data Banks, CACM,
13(6): pp. 377–387, 1970.
[Colmerauer et al., 1973] A. Colmerauer, H. Kanoui, R. Pasero, and P. Roussel, Un Systeme de
Communication Homme–Machine en Francais, Groupe de Intelligence Artiﬁcielle Universitae
de Aix–Marseille II, TR, 1973.
[Chomicki and Subrahmanian, 1990] J. Chomicki and V.S. Subrahmanian. Generalized Closed
World Assumption is π0
2−complete. Inf. Processing Letters, 34: pp. 289–291, 1990.
[Cadoli and Schaerf, 1993] M. Cadoli and M. Schaerf. A Survey of Complexity Results for Non–
Monotonic Logics. JLP, 13: pp. 127–160, 1993.
[Chen and Warren, 1993] W. Chen and D.S. Warren. A Goal–Oriented Approach to Computing
the Well–Founded Semantics. JLP, 17(2–4): pp. 279–300, 1993.
[Dantsin et al., 2001] E. Dantsin, T. Eiter, G. Gottlob, and A. Voronkov.
Complexity and
Expressive Power of Logic Programming. ACM Comp. Surveys, 33 (3), pp. 374–425, 2001.
[del Val, 1995] A. del Val.
An Analysis of Approximate Knowledge Compilation.
IJCAI’95,
1995.
[Drescher et al., 2008] C. Drescher, M. Gebser, T. Grote, B. Kaufmann, A. Konig, M. Ostrowski,
and T. Schaub. Conﬂict–Driven Disjunctive Answer Set Solving. KR’08, AAAI Press, pp.
422-432, 2008.
[Dix et al., 1996] J. Dix, G. Gottlob, and V. Marek. Reducing Disjunctive to nondisjunctive
Semantics by shift operations. Fundamenta Informaticae, 28(1,2): pp. 87–100, 1996.
[DiPaola, 1969] R.A. DiPaola.
The Recursive Unsolvability of the Decision Problem for the
Class of Deﬁnite Formulas. JACM, 16 (2): pp. 324–327, 1969.
[Dix, 1992] J. Dix. Classifying Semantics of Disjunctive Logic Programs. In K.R. Apt, editor,
Proc. of the JICSLP’92, MIT Press, pp. 798–812, 1992.
[Dix, 1995] J. Dix. A Classiﬁcation–Theory of Semantics of Normal Logic Programs: I. Strong
Properties and II. Weak Properties. Fund. Informaticae, XXII(3): pp. 227–255 and 257–288,
1995.
[Demolombe and Jones, 1996] R. Demolombe and A.J. Jones. Integrity Constraints Revisited.
Journal of the IGPL: An Elect. J. on Pure and Applied Logic, 4(3): pp. 369–383, 1996.
[Gebser et al., 2007] M. Gebser, B. Kaufmann, A. Neumann, T. Schaub. Clasp: A Conﬂict–
Driven Answer Set Solver. LPNMR’07, pp. 260–265, 2007.
Jack Minker, Dietmar Seipel and Carlo Zaniolo
618

[Damasio et al., 1997] C.V. Damasio, L.M. Pereira and M. Schroeder. REVISE: Logic Program-
ming and Diagnosis. Springer, LNAI 1265, 1997.
[Eiter et al., 2000] T. Eiter, W. Faber, N. Leone, G. Pfeifer, and A. Polleres. Planning under
incomplete knowledge. in Proc. of the First International Conference on Computational Logic
(CL’2000), Springer, LNAI 1861, pp. 807–821, 2000.
[Eiter et al., 2000a] T. Eiter, W. Faber, N. Leone, and G. Pfeifer. Declarative problem solving
using the DLV System. In: [Minker, 2000], pp. 79– 103, 2000.
[Eiter and Gottlob, 1995] T. Eiter and G. Gottlob. On the Computation Cost of Disjunctive
Logic Programming: Propositional Case. AMAI 15(3-4): pp. 289–323, 1995.
[Eiter and Gottlob, 1997] T. Eiter and G. Gottlob. Expressiveness of Stable Model Semantics
for Disjunctive Logic Programs with Functions. JLP, 33(2): pp. 167–178, 1997.
[Eiter et al., 1997] T. Eiter, G. Gottlob, and H. Mannila Disjunctive Datalog. ACM TODS,
vol. 22(3): pp. 364–417, 1997.
[Eiter et al., 2008] T. Eiter, G. Ianni, T. Lukasiewicz, R. Schindlauer, H. Tompits. Combining
Answer Set Programming with Description Logics for the Semantic Web. Artif. Intell. 172
(12–13): pp. 1495–1539, 2008.
[Eiter et al., 1997] T. Eiter, N. Leone, and D. Sacc`a. On the Partial Semantics for Disjunctive
Deductive Databases. AMAI 19 (1–2): pp. 59–96, 1997.
[Fern´andez et al., 1996] J.A. Fern´andez, J. Grant, and J. Minker. Model Theoretic Approach
to View Updates in Deductive Databases. JAR, 17 (2): pp. 171–197, 1996.
[Fern´andez et al., 1993] J.A. Fern´andez, J. Lobo, J. Minker, and V.S. Subrahmanian. Disjunc-
tive LP + Integrity Constraints = Stable Model Semantics.
AMAI 8(3–4): pp. 449–474,
1993.
[Faber et al., 2003] W. Faber, N. Leone, and G. Pfeifer.
Aggregate Functions in DLV.
In
Proc. Answer Set Programming: Advances in Theory and Implementation, M. de Vos and A.
Provetti, editors, pp. 274–288, 2003.
[Faber et al., 2011] W. Faber, G. Pfeifer, and N. Leone. Semantics and Complexity of Recursive
Aggregates in Answer Set Programming. AI, 175 (1): pp. 278–298, 2011.
[Friesen et al., 1996] O. Friesen, A. Lefebvre, and L. Vieille.
VALIDITY: Applications of a
DOOD System. Intl. Conf. on Extending Database Technology (EDBT’96), Springer, LNCS
1057, 1996.
[Fern´andez and Minker, 1991] J. A. Fern´andez and J. Minker. Bottom–Up Evaluation of Hier-
archical Disjunctive Deductive Databases. ICLP’91, pp. 660–675, MIT Press, 1991.
[Fern´andez and Minker, 1992] J.A. Fern´andez and J. Minker. Semantics of Disjunctive Deduc-
tive Databases. Intl. Conference on Database Theory (ICDT’92), pp. 332–356, 1992. (Invited
Paper).
[Fern´andez and Minker, 1995] J.A. Fern´andez and J. Minker. Bottom–Up Computation of Per-
fect Models for Disjunctive Theories. JLP, 25 (1): pp. 33–51, 1995.
[Faber et al., 2008] W. Faber, G. Pfeifer, N. Leone, T.
Dell’Armi, and G. Ielpa. Design and
Implementation of Aggregate Functions in the DLV System, TPLP, 8 (5-6): pp. 545–580,
2008.
[Febbraro et al., 2011] O. Febbraro, K. Reale and F. Ricca. ASPIDE: Integrated Development
Environment for Answer Set Programming. LPNMR’11, Springer, LONMR 6645, pp. 317–
330, 2011.
[Gaasterland, 1992] T. Gaasterland. Cooperative Answers for Database Queries. PhD Thesis,
Univ. of MD, Dept. of Computer Science, College Park, 1992.
[Green et al., 2012] T.J. Green, M. Aref, and G. Karvounarakis. Logicblox, Platform and Lan-
guage: A Tutorial. In: [Barcel´o and Pichler, 2012], pp. 1–8.
[Gal and Minker, 1985] A. Gal and J. Minker. A Natural Language Database Interface that
Provides Cooperative Answers. 2nd Conference on AI Applications, 1985.
[Gelfond, 2006] M. Gelfond. Answer Sets in KR: A Personal Perspective. Association for Logic
Programming, 19(3), 2006.
[Gelfond, 1987] M. Gelfond. On Stratiﬁed Autoepistemic Theories. Proc. AAAI’87, pp. 207–211,
1987.
Logic and Databases: History of Deductive Databases
619

[Gottlob et al., 2004] G. Gottlob, C. Koch, R. Baumgartner, M. Herzog, S. Flesca. The Lixto
Data Extraction Project - Back and Forth Between Theory and Practice. Proc. PODS’04,
pp. 1–12, 2004.
[Gebser et al., 2009] M. Gebser, B. Kaufmann, and T. Schaub. The Conﬂict–Driven Answer
Set Solver Clasp: Progress Report, LPNMR’09, pp. 509–514, 2009.
[Gebser et al., 2011] M. Gebser, T. Grote, R. Kaminski, and T. Schaub. Reactive Answer Set
Programming. LPNMR’11, pp. 54–66, 2011.
[Gaasterland et al., 1992] T. Gaasterland, P. Godfrey, and J. Minker. Relaxation as a Platform
for Cooperative Answering. JIIS, 1: pp. 293–321, 1992.
[Gaasterland et al., 1992a] T. Gaasterland, P. Godfrey, and J. Minker. An Overview of Coop-
erative Answering. JIIS, 1(2): pp. 123–157, 1992.
[Godfrey et al., 1996] P. Godfrey, J. Gryz, and J. Minker.
Semantic Query Evaluation for
Bottom–Up Evaluation. ISMIS’96, 1996.
[Grant et al., 1993] J. Grant, J. Horty, J. Lobo, and J. Minker.
View Updates in Stratiﬁed
Disjunctive Databases. JAR, 11: pp. 249–267, 1993.
[Gottlob and Koch, 2004] G. Gottlob, C. Koch. Monadic Datalog and the Expressive Power of
Languages for Web Information Extraction. JACM, 51 (1), pp. 74–113, 2004.
[Gelfond and Lifschitz, 1988] M. Gelfond and V. Lifschitz.
The Stable Model Semantics for
Logic Programming. In Proc. of the 5th Intl. Conf. and Symp. on Logic Programming, pp.
1070–1080, MIT Press, 1988.
[Gelfond and Lifschitz, 1990] M. Gelfond and V. Lifschitz. Logic Programs with Classical Nega-
tion. ICLP’90, pp. 579–597, MIT Press, 1990.
[Gelfond and Lifschitz, 1991] M. Gelfond and V. Lifschitz. Classical Negation in Logic Programs
and Disjunctive Databases. New Gen. Comp., 9: pp. 365–385, 1991.
[Gaasterland and Lobo, 1993] T. Gaasterland and J. Lobo. Processing Negation and Disjunc-
tion in Logic Programs Through Integrity Constraints.
Journal of Intelligent Information
Systems, 2(3), 1993.
[Grant and Minker, 2002] J. Grant and J. Minker. A Logic–Based Approach to Data Integra-
tion. TPLP 2(3): pp. 323–368, 2002.
[Gurk and Minker, 1961] H. Gurk and J. Minker. The Design and Simulation of an Information
Processing System. JACM, 8(2): pp. 260-270, 1961.
[Gallaire and Minker, 1978] H. Gallaire and J. Minker, editors, Logic and Data Bases, Plenum
Press, New York, 1978.
[Grant and Minker, 1986] J. Grant and J. Minker. Answering Queries in indeﬁnite Databases &
the Null Value Problem. In P. Kanellakis, ed., Advances in Computing Research: The Theory
of Databases, pp. 247–267. 1986.
[Giannotti and Manco, 1999] F. Giannotti and G. Manco. Querying Inductive Databases via
Logic–Based User–Deﬁned Aggregates. PKDD’90, pp. 125–135, 1999.
[Gallaire et al., 1981] H. Gallaire, J. Minker, and J.-M. Nicolas, editors. Advances in Database
Theory, volume 1. Plenum Press, 1981.
[Gallaire et al., 1984] H. Gallaire, J. Minker, and J.–M. Nicolas. Logic and Databases: A De-
ductive Approach, ACM Comp. Surveys, 16 (2), 1984.
[Godfrey et al., 1994] P. Godfrey, J. Minker, and L. Novik. An Architecture for a Cooperative
Database System. In W. Litwin and T. Risch, editors, Proceedings of the First International
Conference on Applications of Databases, LNCS 819, pp. 3–24, Springer, 1994.
[Gottlob et al., 2011] G. Gottlob, G. Orsi, and A. Pieris. Ontological Queries: Rewriting and
optimization. ICDE’11, pp. 2–13, 2011.
[Gelfond et al., 1986] M. Gelfond, H. Przymusinska, and T.C. Przymusinski.
The Extended
Closed World Assumption and its Relationship to Parallel Circumscription. PODS’86, pp.
133–139, 1986.
[Giannotti et al., 1991] F. Giannotti, D. Pedreschi, D. Sacc`a, and C. Zaniolo. Non–Determinism
in Deductive Databases. DOOD’91, pp. 129–146, 1991.
[Giannotti et al., 2001] F. Giannotti, D. Pedreschi, and C. Zaniolo. Semantics and Expressive
Power of Nondeterministic Constructs in Deductive Databases. JCSS, 62(1): pp. 15–42, 2001.
Jack Minker, Dietmar Seipel and Carlo Zaniolo
620

[Green and Raphael, 1968] C. Green and B. Raphael. Research in Intelligent Question Answer-
ing Systems. Proc. ACM 23rd National Conference, pp. 169–181, 1968.
[Green and Raphael, 1968a] C. Green and B. Raphael. The Use of Theorem–Proving Techniques
in Question–Answering Systems. Proc. ACM 23rd National Conference, 1968.
[Green, 1969] C.C. Green.
Application of Theorem Proving to Problem Solving.
In D.E.
Walker L.M. Norton, editor, Proc. International Conference on Artiﬁcial Intelligence, pp.
219–240, 1969.
[Greco et al., 1990] S. Greco, C. Zaniolo, and S. Ganguly. Greedy by Choice. PODS’92, pp.
105–113, 1992.
[Han, 1989] J. Han: Compiling General Linear Recursions by Variable Connection Graph Anal-
ysis. Computational Intell. 5, pp. 12–31, 1989.
[Harel, 1980] D. Harel. Review number 36, 671 of Logic and Data Bases by H. Gallaire and
J. Minker. Computing Reviews, 21(8), pp. 367–369, 1980.
[Hellerstein, 2010] J.M. Hellerstein. Datalog Redux: Experience and Conjecture. PODS’10, pp.
1–2, 2010.
[Han and Henschen, 1987] J. Han and L. Henschen: Handling Redundancy in the Processing of
Recursive Database Queries, Proc. ACM SIGMOD’87, pp. 73–81, 1987.
[Hammer and Zdonik, 1980] M. Hammer and S. Zdonik. Knowledge–Based Query Processing.
VLDB’80, pp. 137–147, 1980.
[Imielinski, 1991] T. Imielinski. Incomplete Deductive Databases. AMAI 3: pp. 259–293, 1991.
[Imielinski and Vadaparty, 1989] T. Imielinski and K. Vadaparty. Complexity of Query process-
ing in Databases with OR–objects. Proc. PODS’89, pp. 51–65, 1989.
[Janhunen et al., 2006] T. Janhunen, I. Niemel¨a, D. Seipel, P. Simons, J.-H. You:
Unfolding
Partiality and Disjunctions in Stable Model Semantics. ACM Trans. Comput. Log. 7(1): pp.
1–37, 2006.
[Kandzia, 1997] P.–T. Kandzia. Non–Monotonic Reasoning in FLORID. LPNMR’97, pp. 400–
410, 1997.
[Kang et al., 2004] J.–H. Kang, K.–H. Hong, K.–Y. Whang, and J.–W. Chod: Generalization
of ZYT–Linearizability for Bilinear Datalog Programs, Information and Computation 188,
pp. 77–98, 2004.
[King, 1981] J. King.
Quist:
A System for Semantic Query Optimization in Relational
Databases. VLDB’81, pp. 510–517, 1981.
[Kowalski and Kuehner, 1971] R. A. Kowalski and D. Kuehner. Linear Resolution with Selection
Function. Artiﬁcial Intelligence, 2, pp. 227–260, 1971.
[Kraus et al., 1990] S. Kraus, D. Lehmann, and M. Magidor. Nonmonotonic Reasoning, Prefer-
ential Models and Cumulative Logics. Journal of Artiﬁcial Intelligence, 44(1): pp. 167–207,
1990.
[Kifer et al., 1995] M. Kifer, G. Lausen, and J. Wu. Logical Foundations of Object–Oriented
and Frame–Based Languages. JACM, 42(4), pp. 741–843, 1995.
[Kakas and Mourlas, 1997] A.C. Kakas and C. Mourlas. ACLP: Flexible Solutions to Complex
Problems. In: J. Dix , U. Furbach and A. Nerode, editors, LPNMR’97, Springer, LNAI 1265,
pp. 388–399, 1997.
[Kowalski, 1974] R.A. Kowalski. Predicate Logic as a Programming Language. Proc. of IFIP
4, pp. 569–574, 1974.
[Kowalski, 1992] R.A. Kowalski. Database Updates in the Event Calculus, JLP, 12 (162): pp.
121–46, 1992.
[Kolaitis and Papadimitriou, 1991] P. Kolaitis and C. Papadimitriou.
Why not Negation by
Fixpoint? JCSS, 43(1): pp. 125–144, 1991.
[Kowalski and Sergot, 1986] R. Kowalski and M. Sergot. A Logic–Based Calculus of Events,
New Generation Computing, 4: pp. 67–95, 1986.
[Kuhns, 1967] J.L. Kuhns. Answering Questions by Computer: A Logical Study. The Rand
Corporation, 1967.
[Loo et al., 2009] B. T. Loo, T. Condie, M. N. Garofalakis, D. E. Gay, J. M. Hellerstein, P. Ma-
niatis, R. Ramakrishnan, T. Roscoe, and I. Stoica. Declarative Networking. CACM, 52(11):
pp. 87–95, 2009.
Logic and Databases: History of Deductive Databases
621

[Leone, 2007] N. Leone. Logic Programming and Nonmonotonic Reasoning: From Theory to
Systems and Applications. LPNMR’07, Invited Talk, Springer, LNAI 3662, 2007.
[Levesque, 1984] H.J. Levesque. Foundations of a Functional Approach to Knowledge Repre-
sentation. Artiﬁcial Intelligence, 23: pp. 155–212, 1984.
[Levien, 1967] R. Levien.
A Computer System for Inference Execution and Data Retrieval.
CACM, 10: pp. 715–721, 1967.
[Levien, 1969] R. Levien.
Relational Data File: Experience with a System for Propositional
Data Storage and Inference Execution. Technical Report, The Rand Corporation, 1969.
[Levy, 2000] A.Y. Levy. Logic–Based Techniques in Data Integration. In: [Minker, 2000], pp.
575–595.
[Lierler, 2008] Y. Lierler. Abstract Answer Set Solvers. ICLP’08, LNCS 5366, pp. 377-391, 2008.
[Lifschitz, 2006] V. Lifschitz. Inventing Stable Models: A Personal Perspective. Association for
Logic Programming, 19(3), 2006.
[Lausen et al., 1998] G. Lausen, B. Lud¨ascher, and W. May. On Active Deductive Databases:
The Statelog Approach. In Transactions and Change in Logic Databases, pp. 69–106, 1998.
[Lloyd, 1987] J.W. Lloyd. Foundations of Logic Programming. Springer, 2nd ed., 1987.
[Lierler and Maratea, 2004] Y. Lierler and M. Maratea. Cmodels–2: SAT–based Answer Set
Solver Enhanced to Non–tight Programs. LPNMR’04, LNAI 2923, pp. 346-350 2004.
[Levien and Maron, 1967] R. Levien and M. Maron. A Computer System for Inference Execu-
tion and Data Retrieval. 10(11), pp. 715–721, 1967.
[Lobo et al., 1992] J. Lobo, J. Minker, and A. Rajasekar.
Foundations of Disjunctive Logic
Programming. MIT Press, 1992.
[Lifshitz et al., 2000] V. Lifschitz, N. McCain, E. Remolina, and A. Tacchella. Getting to the
Airport: The Oldest Planning Problem in AI. In [Minker, 2000], Chapter 7, pp. 157–165.
[Loveland, 1978] D.W. Loveland.
Automated Theorem Proving:
A Logical Basis.
North–
Holland, 1978.
[Leone et al., 2006] Nicola Leone, Gerald Pfeifer, Wolfgang Faber, Thomas Eiter, Georg Gott-
lob, Simona Perri, and Francesco Scarcello. The DLV System for Knowledge Representation
and Reasoning. ACM Transactions on Computational Logic, 7(3): pp. 499–562, 2006.
[Leone and Rullo, 1992] N. Leone and P. Rullo. The Safe Computation of the Well–Founded
Semantics for Logic Programming. Information Systems, 17(1), pp. 17–31, 1992.
[Levesque et al., 1997] H.J. Levesque, R. Reiter, Y. Lesp´erance, F. Lin, and R.B. Scherl.
GOLOG: A Logic Programming Language for Dynamic Domains.
JLP, 31(1–3): pp. 59–
83, 1997.
[Leone et al., 1997] N. Leone, P. Rullo, and F. Scarcello. Disjunctive Stable Models: Unfounded
Sets, Fixpoint Semantics and Computation. Information and Computation, 135: pp. 69–112,
1997.
[Levy and Sagiv, 1995] A. Levy and Y. Sagiv. Semantic Query Optimization in Datalog Pro-
grams. PODS’95, pp. 163–173, 1995.
[Lloyd and Topor, 1985] J.W. Lloyd and R.W. Topor. A Basis for Deductive Database Systems.
JLP, 2(2): pp. 93–109, 1985.
[Lukasiewicz, 2010] T. Lukasiewicz. A Novel Combination of Answer Set Programming with
Description Logics for the Semantic Web.
IEEE Trans. Knowl. Data Eng., 22 (11): pp.
1577–1592, 2010.
[Marczak et al., 2012] W. R. Marczak, P. Alvaro, N. Conway, J. M. Hellerstein, and D. Maier.
Conﬂuence Analysis for Distributed Programs: A Model–Theoretic Approach. In [Barcel´o
and Pichler, 2012], pp. 135–147, 2012.
[Manthey and Bry, 1988] R. Manthey and F. Bry. Satchmo: A Theorem Prover Implemented
in Prolog. CADE’88, pp. 415–434, 1988.
[McCarthy, 1963] J. McCarthy. Situations, Actions and Causal Laws. Technical Report Memo
2, Stanford University Artiﬁcial Intelligence Laboratory, Stanford, CA, 1963. Reprinted in
[Minsky, 1968].
[McCarthy, 1980] J. McCarthy. Circumscription – A Form of Non–Monotonic Reasoning. Jour-
nal of Artiﬁcial Intelligence, 13 (1,2): pp. 27–39, 1980.
Jack Minker, Dietmar Seipel and Carlo Zaniolo
622

[McDermott and Doyle, 1980] D. McDermott and J. Doyle. Non–Monotonic Logic I. Journal
of Artiﬁcial Intelligence, 13: pp. 41–72, 1980.
[Milnikel, 1997] R. Milnikel, Jr. Conference report 4th International Conference on Logic Pro-
gramming and Nonmonotonic Reasoning. (LPNMR’97) Dagstuhl, Germany, AI Communica-
tions 10, pp. 203–207, 1997.
[Minker, 2000] J. Minker, editor. Logic–Based Artiﬁcial Intelligence. Kluwer Academic Pub-
lishers, Boston/Dordrecht/London, 2000.
[Minker, 2011] J. Minker.
Reminiscences on the Anniversary of 30 Years of Nonmonotonic
Reasoning. In G. Brewka, W. Marek, and M. Truszczynski, editors, Nonmonotonic Reasoning.
Essays Celebrating its 30th Anniversary, pp. 295–334. College Publications, 2011. A volume
of papers presented at NonMOn at 30 meeting, Lexington, KY, USA, October 2010.
[Minsky, 1968] M. Minsky, editor. Semantic Information Processing. MIT Press, Cambridge,
MA, 1968.
[Minker, 1982] J. Minker.
On Indeﬁnite Databases and the Closed World Assumption.
CADE’82, Also in: Springer, LNCS 138, pp. 292–308, 1982.
[Minker, 1986] J. Minker, editor. Proc. of Workshop on Foundations of Deductive Databases
and Logic Programming, 1986. Department of Computer Science, University of Maryland.
[Minker, 1988] J. Minker, editor. Foundations of Deductive Databases and Logic Programming.
Morgan Kaufmann, 1988.
[Minker, 1989] J. Minker. Toward a Foundation of Disjunctive Logic Programming. In Proc. of
the North American Conference on Logic Programming, pp. 121–125. MIT Press, 1989.
[Minker, 1993] J. Minker. An Overview of Nonmonotonic Reasoning and Logic Programming.
JLP, 17(2, 3, and 4): pp. 95–126, 1993.
[Minker, 1994] J. Minker. Overview of Disjunctive Logic Programming. Journal of Artiﬁcial
Intelligence & Mathematics, 12(1-2): pp. 1–24, 1994.
[Minker, 1999] J. Minker. Logic and Databases: A 20 Year Retrospective – Updated in Honor of
Ray Reiter. In H.J. Levesque and F. Pirri, editors, Logical Foundations for Cognitive Agents:
Contributions in Honor of Ray Reiter, pp. 234–299. Springer, 1999.
[McSkimin and Minker, 1977] J. McSkimin and J. Minker. The Use of a Semantic Network in
Deductive Question-Answering Systems. IJCAI’77, pp. 50–58, 1977.
[Minker and Nicolas, 1982] J. Minker and J.–M. Nicolas. On Recursive Axioms in Deductive
Databases. Information Systems, 7(4), pp. 1–15, 1982.
[Moore, 1984] R.C. Moore.
Possible–World Semantics for Autoepistemic Logic.
In Proc. of
AAAI Workshop on Non–Mon. Reasoning, pp. 396–401, New Paltz, 1984.
[Moore, 1985] R.C. Moore.
Semantical Considerations on Nonmonotonic Logic.
Journal of
Artiﬁcial Intelligence, 25 (1): pp. 75–94, 1985.
[Mumick et al., 1990] I. S. Mumick, H. Pirahesh, and R. Ramakrishnan. The Magic of Dupli-
cates and Aggregates. VLDB’90, pp. 264–277, 1990.
[Minker and Rajasekar, 1990] J. Minker and A. Rajasekar. A Fixpoint Semantics for Disjunctive
Logic Programs. JLP, 9(1): pp. 45–74, 1990.
[Minker and Ruiz, 1994] J. Minker and C. Ruiz. Semantics for Disjunctive Logic Programs with
Explicit and Default Negation. Fundamenta Informaticae, 20(3/4): pp. 145–192, 1994. Anniv.
Issue edited by H. Rasiowa.
[Minker and Seipel, 2002] J. Minker and D. Seipel. Disjunctive Logic Programming: A Survey
and Assessment. Computational Logic: Logic Programming and Beyond, Essays in Honour
of Robert A. Kowalski, Part I, pp. 472–511, Springer, 2002.
[Minker and Sable, 1970] J. Minker and J. Sable.
Relational Data System Study.
Technical
Report RADC-TR-70-180, Rome Air Development Center, Air Force Systems Command,
Griﬃss Air Force Base, New York, September 1970. Auerbach Corporation Report AD 720-
263.
[Mazuran et al., 2013] M. Mazuran, E. Serra, and C. Zaniolo. A Declarative Extension of Horn
Clauses, and its Signiﬁcance for Datalog and its Applications. TPLP, 13 (4–5), pp. 609–623,
2013.
[Marek and Truszczy´nski, 1991] V.W. Marek and M. Truszczy´nski.
Autoepistemic Logic.
JACM, 38 (3): pp. 588–619, 1991.
Logic and Databases: History of Deductive Databases
623

[Marek and Truszczy´nski, 1999] V.W. Marek and M. Truszczy´nski. Stable Models and an Al-
ternative Logic Programming Paradigm. In: The Logic Programming Paradigm, pp. 375–398,
1999. Springer, Berlin Heidelberg.
[Morris et al., 1986] K. A. Morris, J. D. Ullman, and A. V. Gelder. Design Overview of the
Nail! System. ICLP’86, pp. 554–568, MIT Press, 1986.
[Naughton, 1987] J. Naughton: One–Sided Recursions, PODS’87, pp. 340–348, 1987.
[Naughton, 1991] J. Naughton: One–Sided Recursions, J. Comput. Syst. Sci., 42 (2), pp. 199–
236, 1991.
[Nogueira et al., 2001] M. Nogueira, M. Balduccini, M. Gelfond, R. Watson, M. Barry. A Prolog
Decision Support System for the Space Shuttle. Answer Set Programming, 2001.
[Nicolas and Gallaire, 1978] J.–M. Nicolas and H. Gallaire. Data Base: Theory vs. Interpreta-
tion. In [Gallaire and Minker, 1978], pp. 33–54, 1978.
[Nicolas, 1979] J.–M. Nicolas. Logic for Improving Integrity Checking in Relational Databases.
Acta Informatica, 18(3): pp. 227–253, 1979.
[Niemel¨a, 1999] I. Niemel¨a. Logic Programs with Stable Model Semantics as a Constraint Pro-
gramming Paradigm. AMAI, 25 (3–4): pp. 241–273, 1999.
[Niemel¨a and Simons, 2000] I. Niemel¨a and P. Simons.
Extending the Smodels System with
Cardinality and Weight Constraints. In [Minker, 2000], pp. 491–521.
[Nicolas and Syre, 1974] J.–M. Nicolas and J.–C. Syre. Natural Question Answering and Auto-
matic Deduction in System Syntex. Proc. IFIP Congress 1974, pp. 595–599, 1974.
[Naughton and Sagiv, 1987] J. Naughton and Y. Sagiv. A Decidable Class of Bounded Recur-
sions. PODS’87, pp. 227–236, 1987.
[Naughton and Sagiv, 1991] J. Naughton and Y. Sagiv: A Simple Characterization of Uniform
Boundedness for a Class of Recursions, JLP, 10(3,4): pp. 233–252, 1991.
[Niemel¨a and Simons, 1997] I. Niemel¨a and P. Simons. Smodels – An Implementation of the
Stable Model and Well–Founded Semantics for Normal Logic Programs. LPNMR’97, Springer,
LNAI 1265, pp. 420–429, 1997.
[Ricca et al., 2009] F. Ricca, L. Gallucci, R. Schindlauer, T. Dell’Armi, G. Grasso, and N. Leone.
OntoDLV: An ASP–Based System for Enterprise Ontologies. J. Log. Comput. 19(4), pp. 643–
670, 2009.
[Palopoli, 1992] L. Palopoli. Testing Logic Programs for Local Stratiﬁcation. Theor. Comput.
Sci., 103(2): pp. 205–234, 1992.
[Pearl, 1988] J. Pearl. Probabilistic Reasoning in Intelligent Systems: Networks of Plausible
Inference. Morgan Kaufmann, San Mateo, California, 1988.
[Pearce, 2008] D.J. Pearce.
Sixty Years of Stable Models.
In: M. Garcia de la Banda and
E. Pontelli, editors, ICLP’08, Springer, LNCS 5366, p. 52, 2008.
[Polleres, 2012] A. Polleres. How (Well) do Datalog, Sparql and RIF Interplay? In Barcel´o and
Pichler [Barcel´o and Pichler, 2012], pp. 27–30, 2012.
[Pirri and Reiter, 2000] F. Pirri and R. Reiter. Planning with Natural Actions in the Situation
Calculus. In: [Minker, 2000], pp. 214-231.
[Przymusinski, 1988] T.C. Przymusinski. On the Declarative Semantics of Deductive Databases
and Logic Programming. In [Minker, 1988], Chapter 5, pp. 193–216. 1988.
[Przymusinski, 1988a] T.C. Przymusinski. Perfect Model Semantics. ICLP/SLP’88, pp. 1081–
1096, 1988.
[Przymusinski, 1989] T.C. Przymusinski. Every Logic Program has a Natural Stratiﬁcation and
an Iterated Least Fixed Point Model. PODS’89, pp. 11–21, 1989.
[Przymusinski, 1990] T.C. Przymusinski. Stationary Semantics for Disjunctive Logic Programs
and Deductive Databases. In Proc. of the North Amer. Conf. on Logic Programming, pp.
40–62, 1990.
[Przymusinski, 1992] T.C. Przymusinski.
Stable Semantics for Disjunctive Programs.
New
Generation Computing, 9: pp. 401–424, 1991.
[Przymusinski, 1995] T.C. Przymusinski. Static Semantics for Normal and Disjunctive Logic
Programs. AMAI 14 (Festschrift in honor of Jack Minker): pp. 323–357, 1995.
[Raphael, 1964] B.
Raphael. SIR: A Computer Program for Semantic Information Retrieval.
MIT.
Jack Minker, Dietmar Seipel and Carlo Zaniolo
624

[Raphael, 1968] B. Raphael.
A Computer Program for Semantic Information Retrieval.
In
M. Minsky, editor, Semantic Information Processing, pp. 33–134, MIT Press, 1968.
[Reiter, 2001] R. Reiter. Knowledge In Action. The MIT Press, Cambridge, Massachusetts,
2001.
[Reiter, 1978] R. Reiter. On Closed World Data Bases. In [Gallaire and Minker, 1978], pp.
55–76. 1978.
[Reiter, 1978a] R. Reiter. Deductive Question–Answering on Relational Data Bases. In [Gallaire
and Minker, 1978], pp. 149–177, Plenum Press, 1978
[Reiter, 1980] R. Reiter. A Logic for Default Reasoning. Journal of Artiﬁcial Intelligence, 13,
pp. 81–132, 1980.
[Reiter, 1984] R. Reiter. Towards A Logical Reconstruction of Relational Database Theory. In:
M.L. Brodie, J.L. Mylopolous and J.W. Schmit, eds. On Conceptual Modelling, pp. 163–189,
1984.
[Reiter, 1986] R. Reiter. A Sound and Sometimes Complete Query Evaluation Algorithm for
Relational Databases with Null Values. JACM, 33(2): pp. 349–370, 1986.
[Reiter, 1988] R. Reiter. On Integrity Constraints. In M. Y. Vardi, editor, Proceedings of the
Second Conference on the Theoretical Aspects of Reasoning about Knowledge, pp. 97–111,
Mar. 1988.
[Ricca et al., 2012] F. Ricca, G. Grasso, M. Alviano, M. Manna, V. Lio, S. Iiritano and N. Leone.
Team–Building with Answer Set Programming in the Gioia–Tauro Seaport. TPLP, 12 (3),
pp. 361-381, 2012.
[Rajasekar and Minker, 1990] A. Rajasekar and J. Minker. On Stratiﬁed Disjunctive Programs.
AMAI 1(1–4): pp. 339–357, 1990.
[Ruiz and Minker, 1998] C. Ruiz and J. Minker. Logic Knowledge Bases with two Default Rules.
AMAI 22 (3–4): pp. 333–361, 1998.
[Robinson, 1965] J.A. Robinson, A Machine–Oriented Logic Based on the Resolution Principle,
JACM, vol. 12(1): pp. 23–41, 1965.
[Ross, 1989] K. Ross. Well–Founded Semantics for Disjunctive Logic Programs. DOOD’89, pp.
352–369, 1989.
[Ross, 1990] K.A. Ross:
Modular Stratiﬁcation and Magic Sets for Datalog Programs with
Negation. PODS’90 pp. 161–171, 1990.
[Ramakrishnan et al., 1993] R. Ramakrishnan, W. G. Roth, P. Seshadri, D. Srivastava, and
S. Sudarshan. The Coral Deductive Database System. Proc. ACM SIGMOD’93, pp. 544–545,
1993.
[Ross and Sagiv, 1997] K.A. Ross and Y. Sagiv. Monotonic Aggregation in Deductive Database.
J. Comput. Syst. Sci., 54(1): pp. 79–97, 1997.
[Rao et al., 1996] P. Rao, K. Sagonas, T. Swift, D.S. Warren, and J. Friere. XSB: A System
for Eﬃciently Computing Well–Founded Semantics. LPNMR’97, Springer, LNAI 1265, pp.
430–440, 1997.
[Ramakrishnan and Ullman, 1995] R. Ramakrishnan and J.D. Ullman, A Survey of Research
on Deductive Database Systems, JLP, 23(2): pp. 125–149, 1995.
[Sacc`a and Zaniolo, 1988] D. Sacc`a and C. Zaniolo. Diﬀerential Fixpoint Methods and Stratiﬁ-
cation of Logic Programs. JCDKB’88, pp. 49–58, 1988.
[Sacc`a, 1997] D. Sacc`a. The Expressive Power of Stable Models for Bound and Unbound Datalog
Queries. JCSS, 54(3): pp. 441–464, 1997.
[Sakama and Inoue, 2000] C. Sakama and K. Inoue. Abductive Logic Programming and Disjunc-
tive Logic Programming: their Relationship and Transferability, JLP, 44(1–3): pp. 75–100,
2000.
[Schlipf, 1995] J.S. Schlipf. Complexity and Undecideability Results for Logic Programming.
AMAI 15(3-4): pp. 257–288, 1995.
[Seipel, 2001] D. Seipel. Using Clausal Deductive Databases for Deﬁning Semantics in Disjunc-
tive Deductive Databases. AMAI 33 (2–4), pp. 347–378, 2001.
[Seipel, 1990] D. Seipel: Decomposition of Linear Recursive Logic Programs, Proc. Intl. Work-
shop on Graph–Theoretic Concepts in Computer Science, Springer, LNCS 484, pp. 291–310,
1990.
Logic and Databases: History of Deductive Databases
625

[Seipel, 1997] D. Seipel. Partial Evidential Stable Models for Disjunctive Deductive Databases.
In Proc. of the Workshop on Logic Prog. and Knowledge Representation, Springer, LNAI
1471, pp. 66–84, 1998.
[Shanahan, 1997] M. Shanahan. Solving the Frame Problem – A Mathematical Investigation of
the Common Sense Law of Inertia. MIT Press 1997, ISBN 978-0-262-19384-9, pp. I–XXXIV,
1–407.
[Shepherdson, 1988] J.C. Shepherdson. Negation in Logic Programming, In [Minker, 1988], pp.
19–88. 1988.
[Seshadri et al., 1996] P. Seshadri, J. M. Hellerstein, H. Pirahesh, T. Y. C. Leung, R. Ramakr-
ishnan, D. Srivastava, P. J. Stuckey, and S. Sudarshan. Cost–Based Optimization for Magic:
Algebra and Implementation. Proc. ACM SIGMOD’96, pp. 435–446, 1996.
[Sadri and Kowalski, 1987] F. Sadri and R. Kowalski.
A Theorem–Proving Approach to
Database Integrity. In [Minker, 1988], pp. 313-362.
[Selman and Kautz, 1996] B. Selman and H.A. Kautz.
Knowledge Compilation and Theory
Approximation. JACM, 43(2): pp. 193–224, 1996.
[Seipel et al., 1997] D. Seipel, J. Minker, and C. Ruiz. Model Generation and State Generation
for Disjunctive Logic Programs, JLP, 32(1): pp. 48–69, 1997.
[Seipel et al., 1997a] D. Seipel, J. Minker, and C. Ruiz. A Characterization of Partial Stable
Models for Disjunctive Deductive Databases, ILPS’97, pp. 245–259, MIT Press, 1997.
[Smullyan, 1956] R.M. Smullyan.
Bull, AMS62, 1956.
p. 600:
Elementary Formal System
(abstract). p. 601: On Deﬁnability by Recursion (abstract).
[Wikipedia, 2003] http://en.wikipedia.org/wiki/SQL.
[Sagonas et al., 1996] K. Sagonas, T. Swift, and D. Warren. The Limits of Fixed–Order Com-
putation. In D. Pedreschi and C. Zaniolo, editors, Logic in Databases (LID’96), pp. 355–374,
1996.
[Swift, 1994] T. Swift and D.S. Warren. An Abstract Machine for SLG Resolution: Deﬁnite
Programs. SLP’94, pp. 633–652, 1994.
[Swift, 1999] T. Swift.
Tabling for Non–Monotonic Programming.
Ann. Math. Artif. Intell.
25(3–4), pp. 201–240, 1999.
[Sacc`a, 1987] D. Sacc`a and C. Zaniolo. Implementation of Recursive Queries for a Data Lan-
guage Based on Pure Horn Logic. ICLP’87, pp. 104–135, MIT Press, 1987.
[Sacc`a, 1987a] D. Sacc`a and C. Zaniolo. Magic Counting Methods. Proc. ACM SIGMOD’87,
pp. 49–59, 1987.
[Tsur and Zaniolo, 1986] S. Tsur and C. Zaniolo.
LDL:
A Logic–Based Data–Language.
VLDB’86, 1986.
[Ullman, 1990] J.D. Ullman. Principles of Database Systems, Computer Science Press, 1990.
[Van Gelder, 1986] A. Van Gelder. Negation–as–Failure Using Tight Derivations for General
Logic Programs. SLP’86, pp. 127–138, 1986.
[Van Gelder, 1993] A. Van Gelder. The Alternating Fixpoint of Logic Programs with Negation.
JCSS, 47 (1): pp. 185–221, 1993.
[Vardi, 1982] M.Y. Vardi. The Complexity of Relational Query languages. In Proc. of the 14th
ACM Symp. on Theory of Comp. (STOC’82), pp. 137–146, 1982.
[Vieille et al., 1990] L. Vieille, P. Bayer, V. Kuechenhoﬀ, and A. Lefebvre. EKS–V1, A Short
Overview. AAAI’90 Workshop on Knowledge Base Management Systems, 1990.
[van Emden and Kowalski, 1976] M.H. van Emden and R.A. Kowalski. The Semantics of Pred-
icate Logic as a Programming Language. JACM, 23 (4): pp. 733–742, 1976.
[Van Gelder, 1993] A. V. Gelder.
Foundations of Aggregation in Deductive Databases.
DOOD’93, pp. 13–34, 1993.
[Van Gelder et al., 1991] A. Van Gelder, K.A. Ross, and J.S. Schlipf. The Well–Founded Se-
mantics for General Logic Programs. JACM, 38 (3): pp. 620–650, 1991.
[Vieille, 1986] L. Vieille.
Recursive Axioms in Deductive Databases: The Query/SubQuery
Approach. In Proc. 1st Intl. Conf. on Expert Database Systems, 1986.
[Wittocx et al., 2008] J. Wittocx, M. Marien, and M. Denecker, 2008. The idp System: A Model
Expansion System for an Extension of Classical Logic. Logic and Search (LaSh 2008), pp.
153-165, 2008.
Jack Minker, Dietmar Seipel and Carlo Zaniolo
626

[Yahya, 2000] A.H. Yahya:
Minimal Model Generation for Reﬁned Answering of Generalized
Queries in Disjunctive Deductive Databases. Journal of Data and Knowledge Engineering,
34(3), pp. 219–249, 2000.
[Yahya, 1997] A.H. Yahya. Generalized Query Answering in Disjunctive Deductive Databases:
Procedural and Nonmonotonic Aspects. LPNMR’97, Springer, LNAI 1265, pp. 325–341, 1997.
[Yuan and Chiang, 1989] L.Y. Yuan and D.-A. Chiang. A Sound and Complete Query Evalua-
tion Algorithm for Relational Databases with Disjunctive Information. PODS’89, pp. 66–74,
1989.
[Yahya and Henschen, 1985] A. Yahya and L.J. Henschen. Deduction in non–Horn Databases.
JAR, 1 (2): pp. 141–160, 1985.
[Yuan and You, 1993] L.Y. Yuan and J.-H. You.
Autoepistemic Circumscription and Logic
Programming. JAR, 10: pp. 143–160, 1993.
[You et al., 2000] J.-H. You, L.Y. Yuan, and R. Goebel. An Abductive Approach to Disjunctive
Logic Programming. JLP, 44(1–3): pp. 101–128, 2000.
[Zaniolo, 1984] C. Zaniolo. Database Relations with Null Values. JCSS, 28: pp. 142–166, 1984.
[Zaniolo et al., 1993] C. Zaniolo, N. Arni, and K. Ong. Negation and Aggregates in Recursive
Rules: the LDL ++ Approach. DOOD’93, pp. 204–221, 1993.
[Zukowski, 1997] U. Zukowski and B. Freitag. The Deductive Database System LOLA. LP-
NMR’97, pp. 375–386, 1997.
[Zhang et al., 1990] W. Zhang, C.T. Yu, and D. Troy: A Necessary and Suﬃcient Condition to
Linearize Doubly Recursive Programs in Logic Databases, ACM TODS, 15 (3): pp. 459–482,
1990.
Logic and Databases: History of Deductive Databases
627


LOGICS FOR INTELLIGENT AGENTS
AND MULTI-AGENT SYSTEMS
John-Jules Ch. Meyer
Reader: Michael J. Wooldridge
1
INTRODUCTION
In this chapter we present the history of logic as applied in the area of intelli-
gent agents and multi-agent systems [Wooldridge and Jennings, 1995; Wooldridge,
2000].
This is a quite popular ﬁeld in between computer science and artiﬁcial
intelligence (AI). Intelligent agents are software entities that display a certain
form of intelligence and autonomy, such as reactivity, proactivity and social be-
haviour (the latter if there are multiple agents around in a so-called multi-agent
system, sharing the same environment) [Wooldridge, 2000]. Single agents are com-
monly described by so-called BDI logics, modal logics that describe the beliefs, de-
sires and intentions of agents [Wooldridge and Jennings, 1995; Wooldridge, 1999;
Wooldridge, 2009], inspired by the work of the philosopher Bratman [Bratman,
1987; Bratman, 1990].
Next we turn to logics for multi-agent systems. First we will look at extensions
of BDI-like attitudes for situations where there are multiple agents involved. These
include notions such as common knowledge and mutual intentions. But also new
notions arise when we have multiple agents around. We will look particularly at
normative and strategic reasoning in multi-agent systems and the logics to describe
this. But we will begin with a short introduction to modal logic which plays a
very important role in most of the logics that we will encounter.
2
MODAL LOGIC
Modal logic is stemming from analytical philosophy to describe and analyze im-
portant philosophical notions such as knowledge and belief (epistemic / doxastic
logic), time (temporal / tense logic), action (dynamic logic) and obligations, per-
mission and prohibitions (deontic logic) [Blackburn et al., 2007].
Historically,
modal logics were developed by philosophers in the 20th century, ﬁrst only in the
form of calculi but from the 50’s also with a semantics, due to Kripke, Kanger and
Hintikka.
Handbook of the History of Logic. Volume 9: Computational Logic. 
Volume editor: Jörg Siekmann 
Series editors: Dov M. Gabbay and John Woods 
Copyright © 2014 Elsevier BV. All rights reserved.

630
John-Jules Ch. Meyer
The beautiful thing is that these logics all have a similar semantics, called pos-
sible world or Kripke semantics and revolve around a box operator ✷and its dual
diamond ⋄as additions to classical (propositional or ﬁrst-order) logic. In a neutral
reading the box operator reads as ‘necessarily’ and the diamond as ‘possibly’, but
in the various uses of modal logic the box operator gets interpretations such as
‘it is known / believed that’, ‘always in the future’, ‘after the action has been
performed it is necessarily the case that’, it is obligatory / permitted / forbidden
that’. In the propositional case where a set of AT of atomic propositions is as-
sumed, the semantics is given by a Kripke model ⟨S, R, π⟩consisting of a set S
of possible worlds, a binary, so-called accessibility relation R on S, and a truth
assignment function π yielding the truth or falsity of an atomic proposition per
possible world.
The general clause for the semantics of the box operator is truth in all accessible
worlds: for a model M and a world s occurring in the model:
M, s |= ✷ϕ ⇔M, t |= ϕ for all t with R(s, t)
The diamond operator means truth is some accessible world:
M, s |= ⋄ϕ ⇔M, t |= ϕ for some t with R(s, t)
A formula is valid (with respect to a class of models) if the formula is true in every
model and state (of that class of models). Kripke semantics gives rise to certain
validities such as
✷(ϕ →ψ) →(✷ϕ →✷ψ)
(called the K-axiom) or equivalently
(✷ϕ ∧✷(ϕ →ψ)) →✷ψ
Using the modal set-up for the various concepts mentioned above leads to logics
with diﬀerent properties for the box operators: for instance for knowledge, where
we normally write ‘K’ for the box operator, we have:
• Kϕ →ϕ, knowledge is true
• Kϕ →KKϕ, knowledge is known
• ¬Kϕ →K¬Kϕ, ignorance is known
The last one, called negative introspection, is controversial amongst philoso-
phers, but rather popular among computer scientists and AI researchers. (The
resulting logic is called S5.) When we turn to belief (denoted by a ‘B’) we see
that belief enjoys the same properties of belief being believed and disbelief being
believed, but belief need not be true. But it is generally held that belief (of a
rational agent) should be consistent:
• ¬Bff, belief is consistent

Logics for Intelligent Agents and Multi-Agent Systems
631
(The resulting logic is called KD45.) Semantically this means that the models
must satisfy certain properties, such as reﬂexive accessibility relations for the ﬁrst
formula of knowledge to become valid, transitive accessibility relations for the
second formula of knowledge to become valid, and euclidean accessibility relations
for the third formula of knowledge to become valid. And the accessibility relation
need to be serial, if the above-mentioned formula for belief is to become valid.
(Seriality means that in any world of the model there is at least one successor
state with respect to the accessibility relation.) In general there is a theory, called
correspondence theory, that studies the relation between properties of the models
(or rather frames, which are “models without truth assignment function”) and
validities in those models [van Benthem, 2001].
In the rest of this chapter we will see the ubiquitous use of modal logic in the
ﬁeld of logics for intelligent agents and multi-agent systems.
But ﬁrst we will
consider an alternative semantics for modal logic, that sometimes is used as well.
2.1
Neighbourhood semantics for modal logic
While ‘normal’ (Kripke) models of modal logic give rise to already a number of
validities that are sometimes unwanted (such as (Kϕ ∧K(ϕ →ψ)) →Kψ, in the
case of knowledge, which is sometimes referred to (part of) the logical omniscience
problem), there are also so-called minimal (or Scott-Montague) models, based
on the notion of neighbourhoods [Scott, 1970; Montague, 1970; Chellas, 1980].
A minimal / neighbourhood model is a structure ⟨S, N, π⟩, where S is a set of
possible worlds and π is a truth assignment function per world again, but N is now
a mapping from S to sets of subsets of S (these subsets are called neighbourhoods).
The truth of a box and diamond operators is now given by, for model M = ⟨S, N, π⟩
and world s:
M, s |= ✷ϕ ⇔∥ϕ∥M ∈N(s)
M, s |= ⋄ϕ ⇔S\∥ϕ∥M ̸∈N(s)
where ∥ϕ∥M = {s ∈S | M, s |= ϕ}, the truth set of ϕ in M, and \ stands for the
set-theoretic diﬀerence. So, in this semantics ✷ϕ is true in s if the truth set of ϕ is
a neighbourhood in s. Validity of a formula is deﬁned as that formula being true
in every minimal / neighbourhood model and every possible world in that model.
This semantics gives rise to a weaker logic. In particular, it does not validate
the K-axiom and when used for knowledge there is no logical omniscience (in the
traditional sense) anymore. Actually what still holds is something very weak:
|= ϕ ↔ψ ⇒
|= ✷ϕ ↔✷ψ
It is possible, though, to restore the validities for knowledge and belief as men-
tioned above by putting certain constraints on the models again [Chellas, 1980].

632
John-Jules Ch. Meyer
3
SPECIFYING SINGLE AGENT’S ATTITUDES: BDI LOGICS
At the end of the 1980’s, the philosopher Michael E. Bratman published a remark-
able book, “Intention, Plans, and Practical Reason” [Bratman, 1987], in which he
lays down a theory of how people make decisions and take action. Put very suc-
cinctly, Bratman advocates the essential use of a notion of intention besides belief
and desire in such a theory. Even more remarkably, although intended to be a
theory of human decisions, it was almost immediately picked up by AI researchers
to investigate its use for describing artiﬁcial agents, which was a new incarnation
of the ideal of AI that originated in the 1950’s as a discipline aiming at creating
artifacts that are able to behave intelligently while performing complex tasks. The
logician and mathematician Alan Turing was one of the founding fathers of this
discipline: he wrote his famous article “Computing Machinery and Intelligence”
[Turing, 1950] where he tries to answer the question ”Can machines think?” and
subsequently proposes an imitation game as a test for intelligence for a machine
(later called the Turing test). The area of AI has, with ups and downs, developed
into a substantial body of knowledge of how to do / program intelligent tasks,
and comprises such areas as search, reasoning, planning, and learning [Russell and
Norvig, 2009].
Although the notion of an agent abounds in several areas of science and philos-
ophy for quite some time, the concept of an artiﬁcial intelligent agent is relatively
new and originates at the end of the 1980s and the work of Bratman is an impor-
tant source of coining this concept. Particularly computer scientists/AI researchers
such as David Israel and Martha Pollack [Bratman et al., 1988], Phil Cohen and
Hector Levesque [Cohen and Levesque, 1990] and Anand Rao and Michael Georgeﬀ
[Rao and Georgeﬀ, 1991] have taken the ideas of Bratman as a starting point and
have thought about how to realize artifacts that take decisions in a human way.
To this end some of them devised logics to specify the behaviour of the to be
constructed agents and tried to follow Bratman’s ideas resulting in formalisations
of (parts of) Bratman’s theory.
These logics are now called BDI logics, since they (mainly) describe the attitudes
of beliefs, desires and intentions of intelligent agents. Particularly the notion of
an intention was advocated by Bratman. Very brieﬂy, intentions are the desires
that an agent chooses to commit to and he will not give up this intention unless
there is a rational reason for doing so. (This provides a key link between beliefs
and actions!)
An agent abandons an intention only under the following conditions:
• the intention has been achieved;
• he believes it is impossible to achieve it;
• he abandons another intention for which the current one is instrumental (so,
the current intention loses its purpose).
We will treat these logics brieﬂy in the following subsections. We will do so
without giving the sometimes rather complicated semantic models.
For these

Logics for Intelligent Agents and Multi-Agent Systems
633
we refer to the original papers as well as several handbook articles [Meyer and
Veltman, 2007; Meyer et al., 2014].
3.1
Cohen and Levesque’s approach to intentions
Cohen and Levesque attempted to formalize Bratmen’s theory in an inﬂuential
paper “Intention is Choice with Commitment” [Cohen and Levesque, 1990]. This
formalisation is based on linear time temporal logic [Pnueli, 1977], augmented with
modalities for beliefs and goals, and operators dealing with actions. It is presented
as a ‘tiered’ formalism with as atomic layer beliefs, goals, actions, and as molecular
layer concepts deﬁned in terms of primitives, such as achievement goals, persistent
goals and, ultimately, intention in two varieties: INTEND1 (intention to do)
and INTEND2 (intention to be).1 So given modal operators for goals and beliefs
which are of the KD (cf. Section 4.2) and KD45 kind, respectively, they deﬁne
achievement goals, persistent goals and intentions in the following way.
As mentioned above the logical language of Cohen & Levesque contains layers,
and starts out from a core layer with operators BEL for belief, GOAL for (a kind of
primitive notion of) goal, along with a number of other auxiliary operators. These
include the operators LATER ϕ, DONE i α, HAPPENS α, ✷ϕ, BEFOREϕψ
and test ϕ? with intended meanings ‘sometime in the future ϕ but not now’,
‘the action α has just been performed’, ‘the action α is next to be performed’,
‘always in the future ϕ’, ‘ϕ is true before ψ is true’, and a test on the truth of ϕ,
respectively, where the latter means that the test is a skip if ϕ is true and fails
/ aborts if ϕ is false. These operators all have either a direct semantics or are
abbreviations in the framework of Cohen and Levesque, but we refer to [Cohen
and Levesque, 1990] for further details. Using these basic operators, the following
‘derived’ operators of achievement goal, persistent goal and two types of intentions,
what I call ‘intention to do’ and ‘intention to be’, are deﬁned:
• A −GOAL i ϕ = GOAL i (LATER ϕ) ∧BEL i ¬ϕ
• P −GOAL i ϕ = A −GOAL i ϕ∧
[BEFORE(BEL i ϕ ∨BEL i ✷¬ϕ)GOAL i (LATER ϕ)]
• INTEND1 i α = P −GOAL i [DONE i (BEL i (HAPPENS α))?; α]
• INTEND2 i ϕ = P −GOAL i ∃a(DONE i [BEL i ∃b HAPPENS i b; ϕ?)∧
¬GOAL i ¬HAPPENS i a; ϕ?]?; a; ϕ?)
So, the ﬁrst clause says that achievement goals to have ϕ are goals of having
ϕ at a later time but which are currently believed to be false. Persistent goals
are achievement goals that before they are given up should be believed to be
achieved or believed to be never possible in the future. Intention to do an action
1I use here the same terminology as in deontic logic, where there is a distinction between
ought to do and ought to be [von Wright, 1980].
Alternatively one could call intention to be
also intention to bring about.

634
John-Jules Ch. Meyer
is a persistent goal of having done this action consciously, and intention to be in
a state where ϕ holds is a persistent goal of consciously having done some action
that led to ϕ, while the not happening of the actual action leading to ϕ is not
an explicit goal of the agent. The last clause is so complicated since it allows for
believing some other action leading to ϕ happening than actually was the case,
but also preventing that this actual action was undesired by the agent.
In their framework Cohen & Levesque can prove a number of properties that
corroborate their approach as a formalisation of Bratman’s theory, such as Brat-
man’s screen of admissibilty.
Informally this states that prior intentions may
inﬂuence later intentions, here coined as the property that if the agent intends to
do an action β, and it is always believed that doing an action α prevents doing β
forever, then the agent should not intend doing α ﬁrst and then β.
• (screen of admissibility)
INTEND1 i β ∧✷(BEL i [DONE i α →✷¬DONE i β]) →
¬INTEND1 i α; β
Although I believe the approach of Cohen & Levesque plays an important his-
torical role in obtaining a formal theory of intentions, especially methodologically,
trying to deﬁne notions of intention from more primitive ones, of course there are
also some limitations and considerations of concern. Firstly, by its very method-
ology, it goes against the aspect of Bratman’s philosophy that amounts to the
irreducibility of intentions to beliefs and desires! Moreover, the logic is based on
linear-time temporal logic, which does not provide the opportunity to talk about
quantifying over several possible future behaviours in a syntactic way within the
logic. This is remedied by the approach of Rao & Georgeﬀthat uses branching-
time temporal logic with the possibility of using path quantiﬁers within the logic.
3.2
Rao & Georgeﬀ’s BDI logic
Rao & Georgeﬀcame up with a diﬀerent formalisation of Bratman’s work [Rao
and Georgeﬀ, 1991; Rao and Georgeﬀ, 1998]. This formalisation and the one by
Cohen & Levesque have in common that intentions are a kind of special goals
that are committed to, and not given up to soon, but the framework as well
as the methodology is diﬀerent. Rather than using linear time temporal logic like
Cohen and Levesque do, Rao and Georgeﬀemploy a branching time temporal logic
(viz. CTL*, which originated in computer science to describe nondeterministic and
parallel processes [Clarke and Emerson, 1981; Emerson, 1990]). Another diﬀerence
is the method that they use. Rather than having a tiered formalism where intention
is deﬁned in terms of other more primitive notions, they introduce primitive modal
(box-like) operators for the notions beliefs, goals (desires) and intentions. And
then later they put constraints on the models such that there are meaningful
interactions between these modalities. So this is much more in line with Bratman’s
irreducibility of intentions to beliefs and desires. The properties they propose are
the following:

Logics for Intelligent Agents and Multi-Agent Systems
635
(In the following α is used to denote so-called O-formulas, which are formu-
las that contain no positive occurrences of the ‘inevitable’ operator (or negative
occurrences of ‘optional”) outside the scope of the modal operators BEL, GOAL
and INTEND. A typical O-formula is optional p, where p is an atomic formula.
Furthermore ϕ ranges over arbitrary formulas and e ranges over actions.)
1. GOAL(α) →BEL(α)
2. INTEND(α) →GOAL(α)
3. INTEND(does(e)) →does(e)
4. INTEND(ϕ) →BEL(INTEND(ϕ))
5. GOAL(ϕ) →BEL(GOAL(ϕ))
6. INTEND(ϕ) →GOAL(INTEND(ϕ))
7. done(e) →BEL(done(e))
8. INTEND(ϕ) →inevitable ⋄(¬INTEND(ϕ))
Let us now consider these properties deemed desirable by Rao & Georgeﬀagain.
The ﬁrst formula describes Rao & Georgeﬀ’s notion of ‘strong realism’ and con-
stitutes a kind of belief-goal compatibility: it says that the agent believes he can
optionally achieve his goals. There is some controversy on this. Interestingly, but
confusingly, Cohen & Levesque [Cohen and Levesque, 1990] adhere to a form of
realism that renders more or less the converse formula BELp →GOALp. But
we should be careful and realize that Cohen & Levesque have a diﬀerent logic in
which one cannot express options as in the branching-time framework of Rao &
Georgeﬀ. Furthermore, it seems that in the two frameworks there is a diﬀerent
understanding of goals (and beliefs) due to the very diﬀerence in ontologies of time
employed: Cohen & Levesque’s notion of time could be called ‘epistemically non-
deterministic’ or ‘epistemically branching’, while ‘real’ time is linear: the agents
envisage several future courses of time, each of them being a linear history, while
in Rao & Georgeﬀ’s approach also ‘real’ time is branching, representing options
that are available to the agent.
The second formula is a similar one to the ﬁrst. This one is called goal-intention
compatibilty, and is defended by Rao & Georgeﬀby stating that if an optionality
is intended it should also be wished for (a goal in their terms). So, Rao & Georgeﬀ
have a kind of selection ﬁlter in mind: intentions (or rather intended options) are
ﬁltered / selected goals (or rather goal (wished) options), and goal options are
selected believed options. If one views it this way, it looks rather close to Cohen
& Levesque’s “Intention is choice (chosen / selected wishes) with commitment”,
or loosely, wishes that are committed to. Here the commitment acts as a ﬁlter.
The third one says that the agent really does the primitive actions that s/he
intends to do. This means that if one adopts this as an axiom the agent is not

636
John-Jules Ch. Meyer
allowed to do something else (ﬁrst). (In our opinion this is rather strict on the
agent, since it may well be that postponing executing its intention for a while is
also an option.) On the other hand, as Rao & Georgeﬀsay, the agent may also do
things that are not intended since the converse does not hold. And also nothing
is said about the intention to do complex actions.
The fourth, ﬁfth and seventh express that the agent is conscious of its intentions,
goals and what primitive action he has done in the sense that he believes what he
intends, has as a goal and what primitive action he has just done.
The sixth one says something like that intentions are really wished for: if some-
thing is an intention then it is a goal that it is an intention.
The eighth formula states that intentions will inevitably (in every possible fu-
ture) be dropped eventually, so there is no inﬁnite deferral of the agent’s intentions.
This leaves open, whether the intention will be fulﬁlled eventually, or will be given
up for other reasons. Below we will discuss several possibilities of giving up inten-
tions according to diﬀerent types of commitment an agent may have.
It is very interesting is that BDI-logical expressions can be used to characterize
diﬀerent types of agents. Rao & Georgeﬀmention the following possibilities:
1. (blindly committed agent) INTEND(inevitable ⋄ϕ) →
inevitable(INTEND(inevitable ⋄ϕ)UBEL(ϕ))
2. (single-minded committed agent) INTEND(inevitable ⋄ϕ) →
inevitable(INTEND(inevitable ⋄ϕ)U(BEL(ϕ) ∨¬BEL(optional ⋄ϕ)))
3. (open minded committed agent) INTEND(inevitable ⋄ϕ) →
inevitable(INTEND(inevitable ⋄ϕ)U(BEL(ϕ) ∨¬GOAL(optional ⋄ϕ)))
A blindly committed agent maintains his intentions to inevitably obtaining even-
tually something until he actually believes that that something has been fulﬁlled.
A single-minded committed agent is somewhat more ﬂexible: he maintains his
intention until he believes he has achieved it or he does not believe that it can be
reached (i.e. that it is still an option in some future) anymore. Finally, the open
minded committed agent is even more ﬂexible: he can also drop his intention if it
is not a goal (desire) anymore.
Rao & Georgeﬀare then able to obtain results under which conditions the
various types of committed agents will reach their intentions. For example, for
a blindly committed agent it holds that under the assumption of the axioms we
have discussed earlier including the axiom that expresses no inﬁnite deferral of
intentions 2 :
INTEND(ϕ) →inevitable ⋄¬INTEND(ϕ)
2As the reviewer of this paper observed, this would only work for non-valid / non-tautological
assertions ϕ, since INTEND being a normal box-like operator satisﬁes the necessitation rule,
thus causing inconsistency together with this axiom. On the other hand, a tautological or valid
assertion is obviously not a true achievement goal, so exclusion of the axiom for this case is not
a true restriction, conceptually speaking.

Logics for Intelligent Agents and Multi-Agent Systems
637
that
INTEND(inevitable(⋄ϕ)) →inevitable(⋄BEL(ϕ))
expressing that if the agent intends to eventually obtain ϕ it will inevitably even-
tually believe that it has succeeded in achieving ϕ.
In his book [Wooldridge, 2000] Michael Wooldridge has extended BDICT L to
deﬁne LORA (the Logic Of Rational Agents), by incorporating an action logic.
Interestingly the way this is done resembles Cohen & Levesque’s logic as to the
syntax (with operators such as HAPPENS α for actions α), but the semantics
is branching-time `a la Rao & Georgeﬀ. In principle, LORA allows reasoning not
only about individual agents, but also about communication and other interaction
in a multi-agent system, so we will return to LORA, when we will look at logics
for multi-agent systems.
3.3
KARO Logic
The KARO formalism is yet another formalism to describe the BDI-like mental at-
titudes of intelligent agents. In contrast with the formalisms of Cohen & Levesque
and Rao & Georgeﬀits basis is dynamic logic [Harel, 1984; Harel et al., 2000],
which is a logic of action, augmented with epistemic logic (there are modalities for
knowledge and belief). On this basis the other agent notions are built. The KARO
framework has been developed in a number of papers (e.g. [van Linder et al., 1995;
van Linder et al., 1997; van der Hoek et al., 1998; Meyer et al., 1999]) as well as the
thesis of Van Linder ([van Linder, 1996]). Again we suppress semantical matters
here.
The KARO formalism is an amalgam of dynamic logic and epistemic / doxastic
logic [Meyer and van der Hoek, 1995], augmented with several additional (modal)
operators in order to deal with the motivational aspects of agents. So, besides
operators for knowledge (K), belief (B) and action ([α], “after performance of
α it holds that”), there are additional operators for ability (A) and desires (D).
Perhaps the ability operator is the most nonstandard one. It takes an action as
an argument, expressing that the agent is able to perform that action. This is
to be viewed as an intrinsic property of the agent. For example a robot with a
gripper is able to grip. Whether the agent has also the opportunity to perform the
action depends on the environment. In the example of the robot with gripper it
depends on the enviroment whether there are things to grip. In KARO ability and
opportunity are represented by diﬀerent operators. We will see the opportunity
operator directly below.
In KARO a number of operators are deﬁned as abbreviations:
• (dual) ⟨α⟩ϕ = ¬[α]¬ϕ, expressing that the agent has the opportunity to
perform α resulting in a state where ϕ holds.
• (opportunity) Oα = ⟨α⟩tt, i.e., an agent has the opportunity to do an action
iﬀthere is a successor state w.r.t. the Rα-relation;

638
John-Jules Ch. Meyer
• (practical possibility) P(α, ϕ) = Aα ∧Oα ∧⟨α⟩ϕ, i.e., an agent has the
practical possibility to do an action with result ϕ iﬀit is both able and has
the opportunity to do that action and the result of actually doing that action
leads to a state where ϕ holds;
• (can) Can(α, ϕ) = KP(α, ϕ), i.e., an agent can do an action with a certain
result iﬀit knows it has the practical possibilty to do so;
• (realisability) ✸ϕ = ∃a1, . . . , anP(a1; . . . ; an, ϕ)3, i.e., a state property ϕ is
realisable iﬀthere is a ﬁnite sequence of atomic actions of which the agent
has the practical possibility to perform it with the result ϕ;
• (goal) Gϕ = ¬ϕ∧Dϕ∧✸ϕ, i.e., a goal is a formula that is not (yet) satisﬁed,
but desired and realisable.4
• (possible intend) I(α, ϕ) = Can(α, ϕ) ∧KGϕ, i.e., an agent (possibly) in-
tends an action with a certain result iﬀthe agent can do the action with that
result and it moreover knows that this result is one of its goals.
Informally, these operators mean the following:
• The dual of the (box-type) action modality expresses that there is at least a
resulting state where a formula ϕ holds. It is important to note that in the
context of deterministic actions, i.e. actions that have at most one successor
state, this means that the only state satisﬁes ϕ, and is thus in this particular
case a stronger assertion than its dual formula [α]ϕ, which merely states that
if there are any successor states they will (all) statisfy ϕ.
• Opportunity to do an action is modelled by having at least one successor
state according to the accessibility relation associated with the action.
• Practical possibility to do an action with a certain result is modelled as
having both ability and opportunity to do the action with the appropriate
result. Note that Oα in the formula Aα ∧Oα ∧⟨α⟩ϕ is actually redundant
since it already follows from ⟨α⟩ϕ. However, to stress the opportunity aspect
it is added.
• The Can predicate applied to an action and formula expresses that the agent
is ‘conscious’ of its practical possibility to do the action resulting in a state
where the formula holds.
• A formula ϕ is realisable if there is a ‘plan’ consisting of (a sequence of)
atomic actions of which the agent has the practical possibility to do them
with ϕ as a result.
3We abuse our language here slightly, since strictly speaking we do not have quantiﬁcation in
our object language. See [Meyer et al., 1999] for a proper deﬁnition.
4In fact, here we simplify matters slightly. In [Meyer et al., 1999] we also stipulate that a goal
should be explicitly selected somehow from the desires it has, which is modelled in that paper
by means of an additional modal operator. Here we leave this out for simplicity’s sake.

Logics for Intelligent Agents and Multi-Agent Systems
639
• A formula ϕ is a goal in the KARO framework if it is not true yet, but
desired and realisable in the above meaning, that is, there is a plan of which
the agent has the practical possibility to realise it with ϕ as a result.
• An agent is said to (possibly) intend an action α with result ϕ if it ‘Can’
do this (knows that it has the practical possibility to do so), and, moreover,
knows that ϕ is a goal.
In order to manipulate both knowledge / belief and motivational matters special
actions revise, commit and uncommit are added to the language. (We assume
that we cannot nest these operators. So, e.g., commit(uncommitα) is not a well-
formed action expression. For a proper deﬁnition of the language the reader is
referred to [Meyer et al., 1999].) Moreover, the formula Com(α) is introduced to
indicate that the agent is committed to α (“has put it on its agenda, i.e. literally,
things to do”).
Deﬁning validity on the basis of the models of this logic [van Linder et al.,
1995; van Linder et al., 1997; Meyer et al., 1999] one obtains the following typical
properties (cf. [van Linder et al., 1995; Meyer et al., 1999]):
1. |= A(α; β) ↔Aα ∧[α]Aβ
2. |= Can(α; β, ϕ) ↔Can(α, P(β, ϕ))
3. |= I(α, ϕ) →K⟨α⟩ϕ
4. |= I(α, ϕ) →⟨commitα⟩Com(α)
5. |= I(α, ϕ) →¬Auncommit(α)
6. |= Com(α) →⟨uncommit(α)⟩¬Com(α)
7. |= Com(α) ∧¬Can(α, ⊤) →Can(uncommit(α), ¬Com(α))
8. |= Com(α) →KCom(α)
9. |= Com(α1; α2) →Com(α1) ∧K[α1]Com(α2)
The ﬁrst of these properties says that if the agent is able to do the sequence
α; β, then this is equivalent with that the agent is able to do α and after doing
α it is able to do β, which sounds very reasonable, but see the remark on this
below. The second states that an agent can do a sequential composition of two
actions with result ϕ iﬀthe agent can do the ﬁrst actions resulting in a state
where it has the practical possibility to do the second with ϕ as result. The third
states that if one possibly intends to do α with result ϕ then one knows that
there is a possibility of performing α resulting in a state where ϕ holds.
The
fourth asserts that if an agent possibly intends to do α with some result ϕ, it
has the opportunity to commit to α with result that it is committed to α (i.e.
α is put into its agenda). The ﬁfth says that if an agent intends to do α with

640
John-Jules Ch. Meyer
a certain purpose, then it is unable to uncommit to it (so, if it is committed to
α, it has to persevere with it).
This is the way persistence of commitment is
represented in KARO. Note that this is much more ‘concrete’ (also in the sense of
computability) than the persistence notions in the other approaches we have seen,
where temporal operators pertaining to a possibly inﬁnite future were employed
to capture them...! In KARO we have the advantage of having dedicated actions
in the action language dealing with the change of commitment that can be used to
express persistence without referring to the (inﬁnite) future, rendering the notion
of persistence much ‘more computable’. The sixth property says that if an agent is
committed to an action and it has the opportunity to uncommit to it then indeed
the commitment is removed as a result. The seventh says that whenever an agent
is committed to an action that is no longer known to be practically possible, it
knows that it can undo this impossible commitment. The eighth property states
that commitments are known to the agent. The nineth says that if an agent is
committed to a sequential composition of two actions then it is committed to the
ﬁrst one, and it knows that after doing the ﬁrst action it will be committed to the
second action.
KARO logic has as a core notion that of ability. But in the above treatment this
only works well for non-failing deterministic actions. Since it is a validity in KARO
that |= A(α; β) ↔Aα∧[α]Aβ, we get the undesirable result that in case there is no
opportunity to do α, the agent is able to do α; β for arbitrary β. For instance, if a
lion is locked in a cage and would be able to walk out but lacks the opportunity, it is
able to get out and ﬂy away! The problem here is a kind of undesired entanglement
of ability and opportunities. In [van der Hoek et al., 2000] we extend our theory
of ability to nondeterministic actions.
(Another solution is to separate results
and opportunities on the one hand and abilities on the other hand rigorously by
using two dynamic operators [α]1 and [α]2 for dealing with results with respect to
opportunities and abilities, respectively, which we have described in [van der Hoek
et al., 2000]). Finally we mention that also related notions such as attempt and
failure of actions have been studied in the literature (e.g., [Lorini and Herzig, 2008;
Broersen, 2011a]).
4
LOGICS FOR MULTI-AGENT SYSTEMS
4.1
Multi-agent logics
In the previous sections we have concentrated on single agents and how to describe
them. In this subsection we will look at two generalisations of single-agent logics
to multi-agent logics, viz. multi-agent epistemic logic and multi-agent BDI logic.
Multi-agent epistemic logic
In a multi-agent setting one can extend a single-agent framework in several ways.
To start with, with respect to the epistemic (doxastic) aspect, one can introduce

Logics for Intelligent Agents and Multi-Agent Systems
641
epistemic (doxastic) operators for every agent, resulting in a multi-modal logic,
called S5n. Models for this logic are inherently less simple and elegant as those
for the single agent case (cf. [Fagin et al., 1995; Meyer and van der Hoek, 1995]).
So then one has indexed operators Ki and Bi for agent i’s knowledge and belief,
respectively.
But one can go on and deﬁne knowledge operators that involve
a group of agents in some way. This gives rise to the notions of common and
(distributed) group knowledge.
The simplest notion is that of ‘everybody knows’, here denoted by the operator
EK. But one can also add an operator CK for ‘common knowledge’, which is
much more powerful.
Although I’ll leave out the details of the semantics again,
it is worth mentioning that the semantics of the common knowledge operator is
given by the reﬂexive-transitive closure of the union of the accessibility relations
of the individual agents.
So it is a powerful operator that quantiﬁes over all
states reachable through the accessibility relations associated with the individual
agents. This gives the power of analyzing the behavior of the agent in multi-agent
systems such as communication between agents in a setting where communication
channels are unreliable, like in the case of the Byzantine generals sending messages
to each other about a joint attack, where it appears that under circumstances
of sending messengers through enemy-controlled territory there cannot emerge
common knowledge of this attack proposal without which the attack cannot safely
take place! This phenomenon, known as the Coordinated Attack Problem, also has
impact on more technical cases involving distributed (computer) systems, where
in fact the problem originated from [Gray, 1978; Fagin et al., 1995; Meyer and van
der Hoek, 1995].
By extending the models and semantic interpretation appropriately (see, e.g.,
[Fagin et al., 1995; Meyer and van der Hoek, 1995]) we then obtain the following
properties (assuming that we have n agents):
• EKϕ ↔K1ϕ ∧. . . ∧Knϕ
• CK(ϕ →ψ) →(CKϕ →CKψ)
• CKϕ →ϕ
• CKϕ →CKCKϕ
• ¬CKϕ →CK¬CKϕ
• CKϕ →EKCKϕ
• CK(ϕ →EKϕ) →(ϕ →CKϕ)
The ﬁrst statement of this proposition shows that the ‘everybody knows’ modal-
ity is indeed what its name suggests. The next four says that common knowledge
has at least the properties of knowledge: closed under implication, it is true, and
enjoys the introspective properties. The sixth property says that common knowl-
edge is known by everybody. The last is a kind of induction principle: the premise

642
John-Jules Ch. Meyer
gives the condition under which one can ‘upgrade’ the truth of ϕ to common knowl-
edge of ϕ; this premise expresses that it is common knowledge that the truth of ϕ
is known by everybody.
As a side remark we note that these properties, in particular the last two ones,
are of the exactly the same form as those axiomatizing dynamic logic [Harel, 1984;
Harel et al., 2000]. This is explained by the fact that the C-operator is based
on a reﬂexive-transitive closure of the underlying accessibility relation as is is the
case with the [α∗] operator in dynamic logic. A further interesting link is that
with ﬁxed point theory dating back to Tarski [Tar55]. One can show (see e.g.,
[Fagin et al., 1995]) that CKϕ is a greatest ﬁxed point of the (monotone) function
EK(ϕ ∧x). This implies that from ϕ →EKϕ one can derive ϕ →Cϕ ([Fagin et
al., 1995], page 408, bottom line, with ψ = ϕ), which is essentially the same as the
last property shown above, stated as an assertion rather than a rule. (Note that
a rule ‘from ϕ derive χ’ in modal logic with a ‘reﬂexive ✷operator’ has the same
meaning as a rule ‘from ✷ϕ derive χ’.)
As to multi-agent doxastic logic one can look at similar notions of ‘everybody
believes’ and common belief. One can introduce operators EB and CB for these
notions.
Now we obtain a similar set of properties for common belief (cf. [Kraus
and Lehmann, 1986; Meyer and van der Hoek, 1995]):
• EBϕ ↔B1ϕ ∧. . . ∧Bnϕ
• CB(ϕ →ψ) →(CBϕ →CBψ)
• CBϕ →EBϕ
• CBϕ →CBCBϕ
• ¬CBϕ →CB¬CBϕ
• CBϕ →EBCBϕ
• CB(ϕ →EBϕ) →(EBϕ →CBϕ)
Note the diﬀerences with the case for knowledge due to the fact that common
belief is not based on a reﬂexive accessibility relation (speaking semantically). In
more plain terms, common belief, like belief, need not be true.
Multi-agent BDI logic
Also with respect to the other modalities one may consider multi-agent aspects. In
this subsection we focus on the notion of collective or joint intentions. We follow
ideas from [Dunin-K¸eplicz and Verbrugge, 2002] (but we give a slightly diﬀerent
but equivalent presentation of deﬁnitions). We now assume that we have belief
and intention operators Bi, Ii for every agent 1 ≤i ≤n. First we enrich the
language of multi-agent doxastic with operators EI (everybody intends) and MI
(mutual intention). (We call this a multi-agent BDI logic, although multi-agent

Logics for Intelligent Agents and Multi-Agent Systems
643
BI logic would be a more adequate name, since we leave out the modality of desire
/ goal.)
Now we get similar properties for mutual intention as we had for common belief
(but of course no introspective properties):
• EIϕ ↔I1ϕ ∧. . . ∧Inϕ
• MI(ϕ →ψ) →(MIϕ →MIψ)
• MIϕ →EIϕ
• MIϕ →EIMIϕ
• MI(ϕ →EIϕ) →(EIϕ →MIϕ)
We see that E-intentions (‘everybody intends’) and mutual intentions are de-
ﬁned in a way completely analogous with E-beliefs (‘everybody believes’) and
common beliefs, respectively. Next Dunin-K¸eplicz & Verbrugge ([Dunin-K¸eplicz
and Verbrugge, 2002]) deﬁne the notion of collective intention (CI) as follows:
• CIϕ = MIϕ ∧CBMIϕ
This deﬁnition states that collective intentions are those formulas that are mu-
tually intended and of which this mutual intention is a common belief amongst all
agents in the system.
We must mention here that in the literature there is also other work on BDI-
like logics for multi-agent systems where we encounter such notions as joint in-
tentions, joint goals and joint commitments, mostly coined in the setting of how
to specify teamwork. Seminal work was done by Cohen & Levesque [Cohen and
Levesque, 1991]. This work was a major inﬂuence on our own multi-agent version
of KARO [Aldewereld et al., 2004]. An important complication in a notion of joint
goal involves that of persistence of the goal: where in the single agent case the
agent pursues its goal until it believes it has achieved it or believes it can never
be achieved, in the context of multiple agents, the agent that realizes this, has to
inform the others of the team about it so that the group / team as a whole will
believe that this is the case and may drop the goal.
Next we consider Wooldridge’s LORA [Wooldridge, 2000] again. As we have
seen before LORA is a branching-time BDI logic combined with a (dynamic logic-
like) action logic in the style of Cohen & Levesque. But from Chapter 6 onwards
of [Wooldridge, 2000], Wooldridge also considers multi-agent aspects: collective
mental states (mutual beliefs, desires, intentions, similar to what we have seen
above), communication (including speech acts as rational actions) and cooperation
(with notions such as ability, team formation and plan formation). It is fair to
note here that a number of these topics were pioneered by Singh [Singh, 1990;
Singh, 1991; Singh, 1991a; Singh, 1993; Singh, 1994; Singh, 1998].
An interesting, rather ambitious recent development is [Dignum and Dignum,
2012]. In this paper a logic, LOA (Logic of Agent Organizations), is proposed in

644
John-Jules Ch. Meyer
which a number of matters concerning agent organizations are combined. The logic
is based on the branching-time temporal logic CTL*. It furthermore has opera-
tors for agent capability, ability, agent attempt, agent control and agent activity,
that are subsequently lifted to group notions: (joint) capability, ability, attempt,
in-control and stit (seeing to it that) [Chellas, 1995]. With this framework the au-
thors are able to express important MAS notions such as responsibility, initiative,
delegation and supervision. For example, a supervision duty is formalized as fol-
lows. Given an organization, and group of roles Z that is part of the organization
and a group of agents A playing the roles U in the organization, the supervising
duty of roles Z with respect to the group of agents V to realize ϕ is deﬁned as:
SD(Z,V )ϕ =def (IZHV Uϕ ∧⋄(HV Uϕ ∧X¬ϕ)) →IZϕ
where IZϕ stands for Z taking the initiative to achieve ϕ, HV Uϕ stands for agents
V enacting roles U attempting ϕ, ⋄is the usual ‘eventually’ operator, and X is
the next-time operator. This deﬁnition thus states that if Z initiates V to attempt
ϕ in their roles U and at some point in time this attempt fails, then the roles Z
become directly in charge of achieving ϕ.
4.2
Logics of norms and normative systems
Deontic logic
Logics about norms and normative systems have their roots in the philosophical
ﬁeld of deontic logic, where pioneers like Von Wright [von Wright, 1951] already
tried to formalize a kind of normative reasoning.
The history of deontic logic (as a formal logic) goes back at least as far as modal
logic in general, with people like Mally [Mally, 1926] attempting ﬁrst formalizations
of notions such as obligation. But, despite interesting and laudable attempts to
vindicate Mally as a serious deontic logician (e.g. [Lokhost, 2002; Lokhorst, 2010;
Lokhorst, 2012] it is generally held that deontic logic started to get serious with
the work of Von Wright [von Wright, 1951]. In this paper Von Wright proposed
an inﬂuential system (later to be known as OS, “Old System”) that is very close
to a normal modal logic (KD), which establishes the operator O (obligation) as a
necessity-style operator in a Kripke-style semantics. The characteristic axiom in
the system KD is the so-called D-axiom:
¬Off
that is, obligations are consistent. To have a feeling for this axiom, we mention
that it is equivalent with ¬(Op ∧O¬p). (In fact this is the same axiom as we
have encountered with belief in a previous section.) Semantically it amounts to
taking models in which the accessibility relation associated with the O-operator
is serial. The logic KD is now known as Standard Deontic Logic, and inspired
many philosophers, despite or perhaps even due to various paradoxes that could

Logics for Intelligent Agents and Multi-Agent Systems
645
be inferred from the system.
5 Over the years people have come to realise that
KD is simply too simple as a deontic logic. In fact, already Von Wright realized
this and came up with a “New System” NS, as early as 1964 [von Wright, 1964], in
which he tried to formalize conditional deontic logic as a ‘dyadic’ logic (a logic with
a two-argument obligation operator O(p/q), meaning that “p is obligatory under
condition q”). This gave rise to a renewed study of deontic logic in the 60s and
70s. However, some problems (paradoxes) remained. To overcome these problems
there were also approaches based on temporal logic [˚Aqvist and Hoepelman, 1981;
van Eck, 1982; Thomason, 1981; Fiadeiro and Maibaum, 1991]. More recently tem-
poral logic has also been employed to capture the intricacies of deadlines [Broersen
et al., 2004]. Meanwhile there were also attempts to reduce deontic logic to alethic
modal logic (Anderson, [Anderson, 1958]), and from the 80s also a reduction to
dynamic logic was proposed [Meyer, 1988]6, giving rise to the subﬁeld of dynamic
deontic logics. This brings to the fore another important issue in deontic logic, viz.
that of ought-to-be versus ought-to-do propositions. In the former approach the
deontic operator (such as obligation O) has a proposition as an argument, describ-
ing the situation that is at hand (obligatory), while the latter has an action as an
argument describing that this action is obligatory. This distinction is sometimes
blurred but has also received considerable attention in the deontic logic literature
(cf. [McNamara, 2010]).
Another reﬁnement of deontic logic has to do with the distinction ideal versus
actual. Standard deontic logic distinguishes these by using the deontic operators:
for instance Oϕ says that in every ideal world ϕ holds, while actually it may not be
the case (that is to say the formula ϕ does not hold). But, when trying to solve the
problems mentioned above, especially pertaining to contrary-to-duties, it may be
tempting to look at a more reﬁned distinction in which we have levels of ideality:
ideal versus subideal worlds. Approaches along these lines are [Dignum et al., 1994;
Carmo and Jones, 1996]. A similar line of approach is that taken by Craven and
Sergot [Craven and Sergot, 2008]. In this framework, which comprises a deontic
extension of the action logic C+ of Giunchiglia et al. [Giunchiglia et al., 2004]),
they incorporate green/red states as well as green/red transitions, thus rendering
a more reﬁned treatment of deontically good and bad behavior. This language is
5It goes beyond the scope of this paper to mention all these paradoxes, but to get a feeling
we mention one: in SDL it is valid that Op →O(p ∨q), which in counterintuitive if one reads
this in the following instance: if it is obligatory to mail the letter, then it is obligatory to
mail the letter or burn it.
What is paradoxical is that in the commonsense reading of this
formula it is suggested that it is left to the agent whether he will mail or burn the letter. But
this is not meant: it just says that in an ideal world where the agent mails the letter it is
(logically) also the case that in this world the agent mails the letter or burns it. Another, major,
problem is the contrary-to-duty imperative, which deals with norms that hold when other norms
are violated, such as Forrester’s paradox of gentle murder [Forrester, 1984; Meyer et al., 1998;
McNamara, 2010]
6Basically this reduced, for instance, the notion of prohibition as follows: Fα =def [α]V ,
where V stands for a violation atom, and [α]ϕ is an expression from dynamic logic, as we
saw before when we treated the KARO framework. So prohibition is equated with “leading to
violation”.

646
John-Jules Ch. Meyer
used to describe a labelled transition system and the deontic component provides
a means of specifying the deontic status (permitted/acceptable/legal/green) of
states and transitions. It features the so called green-green-green (ggg) constraint:
a green transition in a green state always leads to a green state. Or, equivalently,
any transition from a green state to a red state must itself be red!
Recently there are also approaches to deontic logic using stit theory (seeing to
it that, [Chellas, 1995; Kracht et al., 2009]). Deontic stit logics are also logics that
deal with actions but contrary to dynamic logic these actions are not named (rei-
ﬁed) in the logical language [Bartha, 1993; Horty, 2001; Broersen, 2011]. Since the
1990s also defeasible / non-monotonic approaches to deontic logic have arisen [van
der Torre, 1997; Nute, 1997]. Another recent development is that of input/output
logic [Makinson and van der Torre, 2000] that also takes its origin in the study of
conditional norms. In this approach conditional norms are not treated as bearing
truth-values as in most deontic logics. Technically in input/output logic condi-
tional norms are viewed as sets of pairs and a normative code as a set of these.
Thus, in this approach formally norms themselves have no truth value anymore,
but descriptions of normative situations, called normative propositions, have.
Other logics
A range of other logical formalisms for reasoning about normative aspects of multi-
agent systems have also been proposed.
To begin with we mention here combinations of BDI logics with logics about
norms (such as deontic logic), such as the BOID and B-DOING frameworks
[Broersen et al., 2002; Dignum et al., 2001]. Also a merge of KARO and deon-
tic logic has been proposed [Dignum et al., 1996; Dignum and van Linder, 2002].
Of these, the BOID framework is the most well-known. It highlights the interplay
between the agent’s ‘internal’ (BDI) versus ‘external’ motivations (norms / obliga-
tions), which enables one to distinguish between several agent types. For example,
a benevolent agent will give priority to norms, while an egocentric agent will not.
The system is not a logic proper, but rather a rule-based system: the system
contains rules (not unlike rules in default logic [Reiter, 1980]) which determine
extensions of formulas pertaining to beliefs, obligations, intentions and desires to
be true. The order of the rules applied to create these extensions depends on the
agent type.
An important notion in multi-agent system (MAS) is that of an institution. An
institution is any structure or mechanism of social order and cooperation govern-
ing the behavior of a set of individuals within a given community ([Wikipedia,
2013]). Apart from several computational mechanisms that have been devised for
controlling MAS, there have also been proposed dedicated logics to deal with insti-
tutional aspects, such as the ‘counts-as’ conditional, expressing that a ‘brute’ fact
counts as an ‘institutional’ fact in a certain context. Here the terminology of Searle
[Searle, 1969; Searle, 1995] is used: brute facts pertain to the real world, while
institutional facts pertain to the institutional world. Examples are [Grossi, 2007]:

Logics for Intelligent Agents and Multi-Agent Systems
647
• (constitutive) “In system S, conveyances transporting people or goods count
as vehicles”
• (classiﬁcatory) “Always, bikes count as conveyances transporting people or
goods”
• (proper classiﬁcatory) “In system S, bikes count as vehicles”
One of the ﬁrst truly logical approaches is that by Jones & Sergot [Jones and Ser-
got, 1996], which presents a study of the counts-as conditional (in a minimal modal
/ neighbourhood semantic setting). Their work was later improved by Grossi et
al. [Grossi et al., 2006; Grossi, 2007], where the three diﬀerent interpretations of
’counts-as’ mentioned above are disentangled, viz. constitutive, classiﬁcatory and
proper classiﬁcatory. which are all following a diﬀerent (modal) logic.
Given a modal logic of contexts [Meyer and van der Hoek, 1995; Lomuscio and
Sergot, 2003] with modal operators [c] (within context c, quantifying over all the
possible worlds lying within c) and ‘universal’ context operator [u] (quantifying
over all possible worlds, an S5-modality), Grossi formalizes the three counts-as
notions γ1 ⇒i
c γ2, with c a context denoting a set of possible worlds (actually this
context is given by a formula that we also will denote as c: the context is then the
set of all possible worlds satisfying formula c), as follows:
constitutive counts-as for γ1 →γ2 ∈Γ:
γ1 ⇒1
c,Γ γ2 =def [c]Γ ∧[¬c]¬Γ ∧¬[u](γ1 →γ2)
classiﬁcatory counts-as
γ1 ⇒2
c γ2 =def [c](γ1 →γ2)
proper classiﬁcatory counts-as
γ1 ⇒3
c γ2 =def [c](γ1 →γ2) ∧¬[u](γ1 →γ2)
Here the context c, Γ denotes the set of possible worlds within c that satisfy the
set of formulas Γ. So the simplest notion of counts-as is the classiﬁcatory counts
as, meaning that within the context c γ1 just implies γ2. Proper classiﬁcatory
counts-as is classiﬁcatory counts-as together with the requirement that the the
implication γ1 implying γ2 should not hold universally. Constitutive counts as
w.r.t. the context c together with the formulas Γ says that, within the context c,
Γ holds, while outside context c Γ does not hold, and moreover, the implication
γ1 implies γ2 is not universally true.

648
John-Jules Ch. Meyer
4.3
Logics for Strategic Reasoning
In the context of Multi-Agent Systems there has arisen a completely new branch
of logics, which has to do with strategies in game-theoretic sense.
One of the
ﬁrst of these was Pauly’s Coalition Logic [Pauly, 2001]. This is basically a modal
logic with Scott-Montague (neighbourhood semantics.) [Scott, 1970; Montague,
1970; Chellas, 1980]. Thus, Coalition Logic employs a modal language with as
box operator [C]ϕ, where C is a subset of agents, a coalition.
The reading of
this operator is that the coalition C can force the formula ϕ to be true.
The
interpretation is by employing neighbourhoods, which here take the form of so-
called eﬀectivity functions E : S →(P(A) →P(P(S))), where S is the set of
possible worlds and A is the set of agents. Intuitively, E(s)(C) is the collection of
sets X ⊆S such that C can force the world to be in some state of X (where X
represents a proposition). [C]ϕ is now immediately interpreted by:
M, s |= [C]ϕ ⇔∥ϕ∥M ∈E(s)(C)
As Coalition Logic is a form of minimal modal logic, it satisﬁes:
|= ϕ ↔ψ ⇒
|= [C]ϕ ↔[C]ψ
By putting constraints on the eﬀectivity functions one obtains a number of
further validities, for example,
• |= ϕ →ψ ⇒
|= [C]ϕ →[C]ψ iﬀE is outcome monotonic, i.e. for all s, C,
E(s)(C) is closed under supersets.
• |= ¬[C]ff iﬀ∅̸∈E(s)(C) for every s ∈S.
In fact, Pauly [Pauly, 2001] considers an important class of eﬀectivity func-
tions that he calls playable, which is characterized by the following set-theoretic
conditions:
• ∅̸∈E(s)(C) for every s ∈S, C ⊆A
• S ∈E(s)(C) for every s ∈S, C ⊆A
• if X ⊆Y ⊆S and X ∈E(s)(C) then Y ∈E(s)(C), for all s ∈S, C ⊆A
(outcome monotonicity)
• if S\X ̸∈E(s)(∅) then X ∈E(s)(A), for every X ⊆S, s ∈S (A-maximality)
• if C1∩C2 = ∅, X1 ∈E(s)(C1) and X2 ∈E(s)C2) then X1∩X2 ∈(E(s)(C1∪
C2), for all C1, C2 ⊆A and X1, X2 ⊆S (superadditivity)
Informally playability amounts to that there is always some world in S any
coalition can force the world in and that it cannot force the world to be in no
state at all; if a coalition can force the world to be in some state in X then it

Logics for Intelligent Agents and Multi-Agent Systems
649
can also force the world to be in some state in a (possibly) bigger set Y ; if the
empty coalition (sometimes referred to as Nature) cannot force the world to be in
a state outside X (so Nature doesn’t forbid to be in a state of X) then the grand
coalition of all agents A must be able to force the world in a state of X; and ﬁnally
if disjoint coalitions can force the world to be in states of X1 and X2, respectively,
then the union of those coalitions must be able to force the world to be in a state
of its intersection X1 ∩X2. We will return to the concept of playable eﬀectivity
functions below.
A much more expressive logic to reason about the strategies of agents is
Alternating-Time Temporal Logic (ATL), ﬁrst proposed by Alur, Henzinger and
Kupferman [Alur et al., 2002]. They introduce ATL as a third variety of tempo-
ral logic that contrary to linear-time and branching-time temporal logics that are
“natural speciﬁcation languages for closed systems”, are that for open systems.
ATL oﬀers “selective quantiﬁcation over those paths that are possible outcomes
of games, such as the game in which the system and the environment alternate
moves”. The crucial modality is the path quantiﬁer denoted ⟨⟨A⟩⟩, where A is a
set of agents, which ranges over all computations that the agents in A can force the
game into, irrespective of how the agents not in A proceed. The existential path
quantiﬁer from CTL corresponds to ⟨⟨AGT⟩⟩, where AGT is the set of all agents,
while the universal path quantiﬁer from CTL corresponds to ⟨⟨∅⟩⟩. ˚Agotnes de-
scribes ATL as “a propositional logic in which statements about what coalitions
can achieve by strategic cooperation can be expressed” [˚Agotnes, 2006]. One can
show that ATL embeds Pauly’s Coalition Logic [Goranko, 2001]. (The operator
[C]ϕ from coalition logic corresponds to ⟨⟨C⟩⟩Xϕ in ATL, where X is the nexttime
operator.)
Some validities in ATL, based on playable eﬀectivity functions, are ([Goranko,
2001]):
• ¬⟨⟨C⟩⟩Xff
• ⟨⟨C⟩⟩Xtt
• ¬⟨⟨∅⟩⟩¬ϕ →⟨⟨AGT⟩⟩Xϕ
• ⟨⟨C1⟩⟩Xϕ ∧⟨⟨C2⟩⟩Xψ →⟨⟨C1 ∪C2⟩⟩X(ϕ ∧ψ) for disjoint C1 and C2
• ⟨⟨C⟩⟩✷ϕ ↔ϕ ∧⟨⟨C⟩⟩X⟨⟨C⟩⟩✷ϕ where ✷stands for the always operator
• |= ϕ →ψ ⇒
|= ⟨⟨C⟩⟩Xϕ →⟨⟨C⟩⟩Xψ
• |= ϕ ⇒
|= ⟨⟨C⟩⟩✷ϕ
Perhaps the reader can appreciate that the operator ⟨⟨C⟩⟩is a complicated one:
it is not just a box or diamond operator in a modal logic. In fact, in [Broersen et al.,
2006] Broersen et al. show that within the STIT (seeing to it that [Chellas, 1995;
Kracht et al., 2009]) framework one can decompose the ⟨⟨C⟩⟩operator from ATL
into a number of more primitive operators within STIT. Without going into details,

650
John-Jules Ch. Meyer
we think this is an important result in order to understand ATL and its pivotal
operator better. Moreover, Broersen [2010] proposes CTL.STIT, a join of CTL
and a variant of STIT logic, that subsumes ATL and adds expressivity in order
to extend ATL to be able to express important multi-agent system veriﬁcation
properties (having to do with reasoning about norms, strategic games, knowledge
games and the like).
Finally we mention that there are also epistemic variants of ATL, called ATEL
[van der Hoek and Wooldridge, 2003a; ˚Agotnes, 2006], i.e. ATL augmented with
knowledge operators, which enables expressing properties such as “group A can
cooperate to bring about ϕ iﬀit is common knowledge in A that ψ”.
More
generally, one can express properties about resource-bounded reasoners, cryp-
tography and security and agent communication [van der Hoek and Wooldridge,
2003a]. An example of the last category is Kaϕ →⟨⟨a⟩⟩⋄KaKbϕ, expressing
“freedom of speech”: an agent can always tell the truth to other agents (and
know this!).
(Here ⋄stands for the dual of ✷, as usual.)
That ATEL is, be-
sides an expressive logic, also a complicated one, was proven by some controversy
about the proper set-up and interpretation of the logic. It appears that, if one is
not careful, agents may not have incomplete information when choosing actions,
which runs counter to the very conception of epistemic logic. A further elabo-
ration of this issue goes beyond the scope of this chapter (cf. [Jamroga, 2003;
Jamroga and van der Hoek, 2004]).
5
RELATED WORK
In this section I brieﬂy indicate logic-related work in the ﬁeld of intelligent agents
and multi-agent systems that is not (only) based on logic proper, but also employs
other techniques, such as programming techniques and model-checking algorithms.
5.1
Model-checking MAS
One of the most important applications of employing logic to describe multi-agent
systems is that one can try to verify MAS programs. Although there is also some
work on using theorem proving techniques for this (e.g., [Alechina, et al.2010]),
the most popular is employing model-checking techniques [Clarke et al., 1999]. For
instance, MOCHA [Alur et al., 1998] supports Alternating-Time Temporal Logic.
But also model checkers exist that are even more targeted at model-checking MAS.
For example, the model checker MCMAS supports speciﬁcations in a wide range
of agent-relevant logics: the branching-time temporal logic CTL, epistemic logic
(including operators of common and distributed knowledge), Alternating-Time
Temporal Logic, and deontic modalities [Lomuscio and Raimondi, 2006; Lomuscio
et al., 2009]. Interestingly, this checker works with interpreted systems [Fagin et al.,
1995] interpretations of all the modalities involved: “ATL and CTL modalities are
interpreted on the induced temporal relation given by the protocols and transition
functions, the epistemic modalities are deﬁned on the equivalence relations built

Logics for Intelligent Agents and Multi-Agent Systems
651
on the equality of local states, and the deontic modalities are interpreted on “green
states”, i.e., subsets of local states representing states of locally correct behavior
for the agent in question” [Lomuscio et al., 2009].
5.2
BDI programming languages
After the formalization of (BDI-like) agent theories, researchers started to think
of how to realize BDI -based agents, and came up with several architectures
[Wooldridge, 1999; Wooldridge, 2009]. To obtain a more systematic way of pro-
gramming these kinds of agents, Yoav Shoham in 1993 introduced even a new pro-
gramming paradigm, viz. Agent-Oriented Programming and a ﬁrst agent-oriented
programming language, viz.
AGENT0.
Later many other of these languages
emerged. To mention a few: AgentSpeak [Rao, 1996], Jason [Bordini et al., 2007],
JADEX [Pokahr et al., 2003], 3APL [Hindriks et al., 1999], 2APL [Dastani, 2008],
GOAL [de Boer et al., 2007; Hindriks, 2008]. But there are many more nowa-
days, see [Bordini et al., 2005; Bordini et al., 2006; Bordini et al., 2009]. Most
of these are basically dedicated languages for programming agents in terms of
BDI-like concepts such as beliefs, desires, goals, intentions, plans, ... Although
the languages mostly also have imperative programming-style elements, there is
logic(al reasoning) involved such as reasoning about beliefs, desires and goals rep-
resented in appropriate data bases, typically querying these bases to see whether
they make certain conditions true to ﬁre some rules that spawn plans (which
amounts to the reactive planning concept).
But normally the reasoning about
BDI aspects is much less involved than full-blown BDI logic; normally there is no
nesting of the operators, and the semantics is generally non-modal. But naturally
researchers have tried to make connections between this rather ’ﬂat’ BDI reasoning
and full-blown BDI logics, typically using the latter as speciﬁcation and veriﬁcation
logics for programs written in agent programming languages [van der Hoek and
Wooldridge, 2003; Dastani et al., 2007; Hindriks et al., 2007; Dastani et al., 2010;
Hindriks et al., 2012].
5.3
Situation Calculus and the GOLOG family of programming
languages
The Situation Calculus is a (‘ﬁrst-order-like’7 logical formalism to reason about
action and change. It was devised by John McCarthy [McCarthy, 1963] in 1963.
Since then it has gained enormous popularity, especially among AI researchers
in North America.
The area of representation of action and change has been
dominated by the so-called frame problem. Although there are several aspects of
7In fact the situation calculus is a ﬁrst-order language with as special sorts: situations, actions
and ﬂuents[Lin, 2008]. An unorthodox aspect of it is that situations, as well as actions and ﬂuents,
are ‘reiﬁed’ in the languages, i.e. represented as ﬁrst-order objects which are elements that are
semantical in nature and typically used in the metalevel description of models in traditional
ﬁrst-order logic. This has as a consequence that they can be quantiﬁed over in the language,
giving situation calculus a second-order feel [Reiter, 2001]

652
John-Jules Ch. Meyer
this problem, one of the most important issues is that of how to specify succinctly
what changes and what does not change while doing an action. Typically when
one also wants to derive what doesn’t change, so-called frame axioms are needed,
and many of them, in the order of the number n of ﬂuents (changeable predicates)
involved in the scenario at hand times the number m of actions involved [Reiter,
2001, p. 26]. Since this is deemed unmanageable in practice, this was considered a
huge problem. Reiter [Levesque et al., 1998; Reiter, 1991; Reiter, 2001] solved the
problem by the use of so-called Basic Action Theories, consisting of successor state
axioms (one per ﬂuent) and precondition axioms (one per action), reducing the
number of axioms to the order of n+m. Of course, this demands that the scenario
is exactly known: there are no hidden ﬂuents or actions in the scenario. (So the
general frame problem remains unsolved, and is inherently unsolvable. It pertains
to the question of what ﬂuents and what actions to represent in a model of the
scenario.) Based on the Situation Calculus, Reiter and Levesque [Levesque et al.,
1997] proposed an (imperative ‘ALGOL’-like) programming language GOLOG,
which is used to overcome the problem with the high complexity of ﬁrst principle
planning, in which for every goal a plan needs to be made. This is so complex
since in general there are many diﬀerent possible combinations of actions of which
a combination must be found that leads to the goal. Typically Reiter et al. use
a regression technique for this: starting out from the goal and looking backward
to see what situation may lead to the goal in subsequently 0, 1, 2 , ..., k steps
(viz. actions). GOLOG eases the problem by providing a user-deﬁned program
to give some structure to the search (this is sometimes called sketchy planning).
Since GOLOG’s semantics is given in a logical form (using the situation calculus),
a GOLOG program together with a basic action theory can be sent to a theorem
prover that obtains a (constructive) proof of the question how the goal can be
reached. More precisely, it determines what situation, expressed as a sequence
of actions, i.e., a plan, performed in the initial state, satisﬁes the goal.
Since
the original version of GOLOG there has been proposed a number of reﬁnements
of the language, each dealing with particular features such as CONGOLOG [De
Giacomo et al., 2000] and IndiGOLOG [De Giacomo et al., 2009].
6
CONCLUSION
In this chapter we have reviewed some of the history of logic as applied to an
important area of artiﬁcial intelligence, viz. the area of intelligent agents and
multi-agent systems. We have seen logics to describe the attitudes of single agents,
notably BDI logics for describing the beliefs, desires and intentions of agents. As
to multi-agent systems we have considered logics of norms and normative systems,
such as deontic logic as well as hybrids of BDI-logics and deontic logic. We also
have seen logics to describe counts-as relations in institutions. We have looked
at logics for strategic reasoning such as Coalition Logic and AT(E)L. Finally we
have brieﬂy sketched some other developments related to the logics for agent-based
systems.

Logics for Intelligent Agents and Multi-Agent Systems
653
ACKNOWLEDGEMENTS
This chapter beneﬁtted substantially from the remarks and suggestions made by
the reviewer.
BIBLIOGRAPHY
[Aldewereld et al., 2004] H.M. Aldewereld, W. van der Hoek & J.-J.Ch. Meyer, Rational Teams:
Logical Aspects of Multi-Agent Systems. Fundamenta Informaticae 63 (2-3), 2004, pp. 159–
183.
[Alechina, et al.2010] N. Alechina, M. Dastani, F. Kahn, B. Logan & J.-J. Ch. Meyer, Using
Theorem Proving to Verify Properties of Agent Programs, in: Speciﬁcation and Veriﬁcation
of Multi-Agent Systems (M. Dastani, K.V. Hindriks & J.-J. Ch. Meyer, eds.), Springer, New
York/Dordrecht/Heidelberg/London, 2010, pp. 1–33.
[Alur et al., 2002] R. Alur, T.A. Henzinger & O. Kupferman, Alternating-Time Temporal Logic,
J. ACM 49(5), 2002, pp. 672-713.
[Alur et al., 1998] R. Alur, T. Henzinger, F. Mang, S. Qadeer, S. Rajamani & S. Tasiran.
MOCHA: Modularity in model checking, in: Proc. 10th International Conference on Com-
puter Aided Veriﬁcation (CAV98), LNCS 1427. Springer, Berlin, 1998, pp. 521–525 .
[Anderson, 1958] A.R. Anderson, A Reduction of Deontic Logic to Alethic Modal Logic, Mind
67, 1958, pp. 100–103.
[Bartha, 1993] P. Bartha, Conditional Obligation, Deontic Paradoxes, and the Logic of Agency,
Annals of Mathematics and Artiﬁcial Intelligence 9(1–2), 1993, pp. 1–23.
[van Benthem, 2001] J. van Benthem, Correspondence Theory, in: Handbook of Philosophical
Logic, 2nd edition (D. Gabbay &F. Guenthner, eds.), number 3, Kluwer, 2001, pp. 325–408
[Blackburn et al., 2007] P. Blackburn, J.F.A.K. van Benthem, & F. Wolter (eds.), Handbook of
Modal Logic, Elsevier, Amsterdam, 2007.
[de Boer et al., 2007] F.S. de Boer, K.V. Hindriks, W. van der Hoek & J.-J. Ch. Meyer, Agent
Programming with Declarative Goals, J. of Applied Logic 5, 2007, pp. 277-302.
[Bordini et al., 2006] R.H. Bordini, L. Braubach, M.M. Dastani, A.E.F. Seghrouchni, J.J.
Gomez-Sanz, J. Leite, G. O’Hare, A. Pokahr & A. Ricci, (2006). A Survey of Programming
Languages and Platforms for Multi-Agent Systems, Informatica 30, 2008, pp. 33-44.
[Bordini et al., 2005] R.H. Bordini, M.M. Dastani, J. Dix & A.E.F. Seghrouchni, eds., Multi-
Agent Programming: Languages, Platforms and Applications, Springer, New York, 2005.
[Bordini et al., 2009] R.H. Bordini, M.M. Dastani, J. Dix & A.E.F. Seghrouchni, eds., Multi-
Agent Programming (Languages, Tools and Applications, Springer, Dordrecht / Heidelberg,
2009
[Bordini et al., 2007] R.H. Bordini, J.F. Hubner, & M. Wooldridge. Programming Multi-Agent
Systems in AgentSpeak Using Jason. John Wiley & Sons, Chichester, UK, 2007.
[Bratman, 1987] M.E. Bratman, Intentions, Plans, and Practical Reason, Harvard University
Press, Massachusetts, 1987.
[Bratman, 1990] M.E. Bratman. What is intention? In P. R. Cohen, J. Morgan & M. E. Pollack
(eds.), Intentions in Communication, chapter 2, pages 15–31. MIT Press, Cambridge, MA,
1990.
[Bratman et al., 1988] M.E. Bratman, D.J. Israel, M.E. Pollack: Plans and resource-bounded
practical reasoning. Computational Intelligence 4, 1988, pp. 349-355.
[Broersen, 2010] J. Broersen, CTL.STIT: Enhancing ATL to Express Important Multi-Agent
Systems Veriﬁcation Properties, in: in: Proc. of the 9th Int. Conf. on Autonomous Agents
and Multiagent Systems (AAMAS 2010) (van der Hoek, Kaminka, Lesp´erance, Luck & Sen,
eds.), Toronto, Canada, 2010, pp. 683–690.
[Broersen, 2011] J.M. Broersen, Deontic epistemic stit logic distinguishing modes of mens rea,
Journal of Applied Logic 9(2), 2011, pp. 127–152.
[Broersen, 2011a] J.M. Broersen, Modelling Attempt and Action Failure in Probabilistic stit
Logic, in: Proc. 22nd Int. J. Conf. on Artif. Intell. (IJCAI 2011), 2011, pp. 792–797.

654
John-Jules Ch. Meyer
[Broersen et al., 2002] J.M. Broersen, M.M. Dastani, J. Hulstijn & L. van der T—orre, Goal
Generation in the BOID Architecture, Cognitive Science Quarterly Journal 2(3–4), 2002, pp.
428–447.
[Broersen et al., 2004] J. Broersen, F. Dignum, V. Dignum & J.-J. Ch. Meyer, Designing a
Deontic Logic of Deadlines, in: Proc. Deontic Logic in Computer Science (DEON 2004) (A.
Lomuscio & D. Nute, eds.), LNAI 3065, Springer, Berlin, 2004, pp. 43–56.
[Broersen et al., 2006] J. Broersen, A. Herzig & N. Troquard, Embedding Alternating-Time
Temporal Logic in Strategic STIT Logic of Agency, J. of Logic and Computation 16(5), 2006,
pp. 559–578
[Carmo and Jones, 1996] J. Carmo & A.J.I. Jones, Deontic Database Constraints, Violation and
Recovery, Studia Logica 57(1), 1996, pp. 139-165.
[Chellas, 1980] B.F. Chellas, Modal Logic, An Introduction, Cambridge University Press, Cam-
bridge, UK, 1980.
[Chellas, 1995] B.F. Chellas, On bringing it about, Journal of Philosophical Logic 24, 1995, pp.
563–571.
[Clarke and Emerson, 1981] E.M. Clarke & E.A. Emerson, Design and Synthesis of Synchro-
nization Skeletons Using Branching-Time Temporal Logic, in: Proc. Workshop on Logic of
Programs, LNCS 131, Springer, Berlin, 1981, pp. 52–71.
[Clarke et al., 1999] E.M. Clarke, O. Grumberg & D.A. Peled, Model Checking, The MIT Press,
Cambridge, Massachusetts, 1999.
[Cohen and Levesque, 1990] P.R. Cohen & H.J. Levesque, Intention is Choice with Commit-
ment, Artiﬁcial Intelligence 42(3), 1990, pp. 213–261.
[Cohen and Levesque, 1991] P. Cohen & H. Levesque, Teamwork, Nous 24(4), 1991, pp. 487–
512.
[Craven and Sergot, 2008] R. Craven & M.J. Sergot, Agent Strands in the Action Language
nC+, J. of Applied Logic 6, 2008, pp. 172-191.
[Dastani, 2008] M.M. Dastani, 2APL: A Practical Agent Programming Language. International
Journal of Autonomous Agents and Multi-Agent Systems (JAAMAS), 16(3), 2008, pp. 214-
248.
[Dastani et al., 2010] M. Dastani, K.V. Hindriks & J.-J. Ch. Meyer (eds.), Speciﬁcation and
Veriﬁcation of Multi-Agent Systems, Springer, New York / Dordrecht / Heidelberg / London,
2010.
[Dastani et al., 2007] M. Dastani, B. van Riemsdijk & J.-J. Ch. Meyer, A Grounded Speciﬁca-
tion Language for Agent Programs, in: Proc. 6th Int. J. Conf. On Autonomous Agents and
Multi-Agent Systems (AAMAS07) (M. Huhns, O. Shehory, E.H. Durfee & M. Yokoo, eds.),
Honolulu, Hawai’i, USA, 2007, pp. 578-585.
[De Giacomo et al., 2000] G. De Giacomo, Y. Lesp´erance, and H. J. Levesque, ConGolog, a con-
current programming language based on the situation calculus, Artiﬁcial Intelligence Journal
121(1–2):109–169, 2000.
[De Giacomo et al., 2009] G. De Giacomo, Y. Lesp´erance, H. J. Levesque, and S. Sardina, In-
diGolog: A high-level programming language for embedded reasoning agents. In: R. H. Bor-
dini, M. Dastani, J. Dix & A. E. Fallah-Seghrouchni, eds, Multi-Agent Programming: Lan-
guages, Platforms and Applications, chapter 2, Springer, 2009, pp. 31–72.
[Dignum and Dignum, 2012] M.V. Dignum & F.P.M. Dignum, A logic of agent organizations.
Logic Journal of the IGPL 20, 2012, pp. 283–316.
[Dignum et al., 2001] F.P.M. Dignum, D. Kinny & E. Sonenberg (2001). Motivational Attitudes
of Agents: On Desires Obligations and Norms, in: Proc. 2nd int. workshop of central and
eastern europe on multi-agent systems (CEEMAS’01) (B. Dunin-Keplicz & E. Nawarecki,
eds.), Krakow, Poland, 2001, pp. 61-70.
[Dignum and van Linder, 2002] F.P.M. Dignum & B. van Linder, Modeling Social Agents: To-
wards deliberate communication, in: Agent-Based Defeasible Control in Dynamic Environ-
ments, Handbook of Defeasible Reasoning and Uncertainty Management Systems, Vol. 7,
(J.-J.Ch. Meyer & J. Treur, eds.), Kluwer, Dordrecht, 2002, pp. 357-380.
[Dignum et al., 1994] F. Dignum, J.-J. Ch. Meyer & R.J. Wieringa, A Dynamic Logic for Rea-
soning about Sub-Ideal States, in: Proc. ECAI’94 Workshop ”Artiﬁcial Normative Reason-
ing” (J. Breuker, ed.), Amsterdam, 1994, pp. 79-92.

Logics for Intelligent Agents and Multi-Agent Systems
655
[Dignum et al., 1996] F. Dignum, J.-J. Ch. Meyer, R.J. Wieringa & R. Kuiper, A Modal Ap-
proach to Intentions, Commitments and Obligations: Intention plus Commitment Yields Obli-
gation, in: Deontic Logic, Agency and Normative Systems (Proc. DEON’96) (M.A. Brown
& J. Carmo, eds.), Workshops in Computing, Springer, Berlin, 1996, pp. 80–97.
[Dunin-K¸eplicz and Verbrugge, 2002] B. Dunin-K¸eplicz & R. Verbrugge, Collective Intentions,
Fundamenta Informaticae 51(3) (2002), pp. 271–295.
[van Eck, 1982] J.A, van Eck, A System of Temporally Relative Modal and Deontic Predicate
Logic and Its Philosophical Applications, Logique et Analyse 100, 1982, pp. 249–381.
[Emerson, 1990] E.A. Emerson. Temporal and modal logic, in J. van Leeuwen, ed., Handbook of
Theoretical Computer Science, volume B: Formal Models and Semantics, chapter 14, Elsevier
Science, Amsterdam, 1990, pp. 996–1072.
[Fagin et al., 1995] R. Fagin, J.Y. Halpern, Y. Moses & M.Y. Vardi, Reasoning about Knowl-
edge, The MIT Press, Cambridge, MA, 1995.
[Fiadeiro and Maibaum, 1991] J. Fiadeiro & T. Maibaum, Temporal Reasoning over Deontic
Speciﬁcations, J. of Logic and Computation 1(3), 1991, pp. 357–395.
[Forrester, 1984] J.W. Forrester, Gentle Murder, or the Adverbial Samaritan, Journal of Phi-
losophy 81, pp. 193–196.
[Giunchiglia et al., 2004] E. Giunchiglia, J. Lee, V. Lifschitz, N. McCain & H. Turner, Non-
monotonic Causal Theories, Artiﬁcial Intelligence 153, 2004, pp. 49-104.
[Goranko, 2001] V. Goranko, Coalition Games and Alternating Temporal Logics, in: Proc. of
the 8th Conference on Theoretical Aspects of Rationality and Knowledge (TARK VIII) (J.
van Benthem, ed.), Siena, Italy, Morgan Kaufmann, 2001, pp. 259-272.
[Gray, 1978] J. Gray, Notes on Database Operating Systems, Operating Systems, 1978, pp.
393-481.
[Grossi, 2007] D. Grossi, Designing Invisible Handcuﬀs (Formal Investigations in Institutions
and Organizations for Multi-Agent Systems), PhD Thesis, Utrecht University, Utrecht, 2007.
[Grossi et al., 2006] D. Grossi, J.-J. Ch. Meyer & F. Dignum. Classiﬁcatory Aspects of Counts-
as: An Analysis in Modal Logic, Journal of Logic and Computation16(5), 2006, pp. 613–643.
[Harel, 1984] D. Harel, Dynamic Logic, in: D. Gabbay & F. Guenthner (eds.), Handbook of
Philosophical Logic, Vol. II, Reidel, Dordrecht/Boston, 1984, pp. 497–604.
[Harel et al., 2000] D. Harel, D. Kozen & J. Tiuryn, Dynamic Logic, MIT Press, Cambridge,
Massachusetts, 2000.
[Hindriks, 2008] K.V. Hindriks: Modules as Policy-Based Intentions: Modular Agent Program-
ming in GOAL, Proc. PROMAS 2007 (M. Dastani, A. El Fallah Seghrouchni, A. Ricci & M.
Winikoﬀ, eds.), LNAI 4908, Springer, Berlin / Heidelberg, 2008, pp. 156-171.
[Hindriks et al., 1999] K.V. Hindriks, F.S. de Boer, W. van der Hoek & J.-J. Ch. Meyer, Agent
Programming in 3APL, Int. J. of Autonomous Agents and Multi-Agent Systems 2(4), 1999,
pp.357–401.
[Hindriks et al., 2012] K.V. Hindriks, W. van der Hoek & J.-J. Ch. Meyer, GOAL Agents Instan-
tiate Intention Logic, in: Logic Programs, Norms and Action (Sergot Festschrift) (A. Artikis,
R. Craven, N.K. C¸i¸cekli, B. Sadighi & K. Stathis, eds), LNAI 7360, Springer, Heidelberg,
2012, pp. 196-219.
[Hindriks et al., 2007] K.V. Hindriks & J.-J. Ch. Meyer, Agent Logics as Program Logics:
Grounding KARO, in: Proc. 29th Annual German Conference on AI, KI 2006 (C. Freksa,
M. Kohlhase, & K. Schill, eds.)), Bremen, Germany, June 14-17, 2006, Proceedings, LNAI
4314, Springer, 2007, pp. 404–418
[van der Hoek et al., 1998] W. van der Hoek, B. van Linder & J.-J. Ch. Meyer, An Integrated
Modal Approach to Rational Agents, in: M. Wooldridge & A. Rao (eds.), Foundations of
Rational Agency, Applied Logic Series 14, Kluwer, Dordrecht, 1998, pp. 133–168.
[van der Hoek et al., 2000] W. van der Hoek, B. van Linder & J.-J. Ch. Meyer, On Agents That
Have the Ability to Choose, Studia Logica 65, 2000, pp. 79–119.
[van der Hoek et al., 2000] W. van der Hoek, J.-J. Ch. Meyer & J.W. van Schagen, Formalizing
Potential of Agents: The KARO Framework Revisited, in: Formalizing the Dynamics of
Information (M. Faller, S. Kaufmann & M. Pauly, eds.), CSLI Publications, (CSLI Lect.
Notes 91), Stanford, 2000, pp. 51–67.
[van der Hoek and Wooldridge, 2003] W. van der Hoek & M. Wooldridge, Towards a Logic of
Rational Agency, Logic J. of the IGPL 11(2), 2003, pp. 133–157.

656
John-Jules Ch. Meyer
[van der Hoek and Wooldridge, 2003a] W. van der Hoek & M. Wooldridge,
Cooperation,
Knowledge, and Time: Alternating-Time Temporal Epistemic Logic and Its Applications,
Studia Logica 75, 2003, pp. 125–157.
[Horty, 2001] J.F. Horty, Agency and Deontic Logic, Oxford University Press, 2001.
[Jamroga, 2003] W. Jamroga, Some Remarks on Alternating Temporal Epistemic Logic, in:
Proc. FAMAS 03 - Formal Approaches to Multi-Agnet Systems, Warsaw, Poland, 2003, pp.
133–140.
[Jamroga and van der Hoek, 2004] W. Jamroga & W. van der Hoek, Agents That Know How
to Play, Fundamenta Informaticae 63, 2004, pp. 185–219.
[Jones and Sergot, 1996] A. Jones & M. Sergot, A Formal Characterzation of Institutionalised
Power, J. of the IGPL 3, 1996, pp. 427–443.
[Kracht et al., 2009] M. Kracht, J.-J. Ch. Meyer & K. Segerberg, The Logic of Action,
in:
The Stanford Encyclopedia of Philosophy (2009 Edition), Edward N. Zalta (ed.),
URL=http://plato.stanford.edu/entries/logic-action/, ﬁrst published March 31, 2009, 29 p.
[Kraus and Lehmann, 1986] S. Kraus & D. Lehmann, Knowledge, Belief and Time, in: L. Kott
(ed.), Proceedings of the 13th Int. Colloquium on Automata, Languages and Programming,
Rennes, LNCS 226, Springer, Berlin, 1986.
[Levesque et al., 1998] H. Levesque, F. Pirri & R. Reiter, Foundations for the Situation Cal-
culus, Link¨oping Electronic Articles in Computer and Information Science 3(18), Link¨oping
University Electronic Press, Link¨oping, 1998.
[Levesque et al., 1997] H.J. Levesque, R. Reiter, Y. Lesp´erance, F. Lin & R.B. Scherl, GOLOG:
A Logic Programming Language for Dynamic Domains, J. of Logic Programming 31, 1997,
pp. 59–84.
[Lin, 2008] F. Lin, Situation Calculus, Chapter 16 of: Handbook of Knowledge Representation
(F. van Harmelen, V. Lifschitz & B. Porter, eds.), Elsevier, Amsterdam / London / New York,
2008, pp. 649–.669.
[van Linder, 1996] B. van Linder, Modal Logics for Rational agents, PhD. Thesis, Utrecht Uni-
versity, 1996.
[van Linder et al., 1995] B. van Linder, W. van der Hoek & J.-J. Ch. Meyer, Actions that Make
You Change Your Mind: Belief Revision in an Agent-Oriented Setting, in: Knowledge and
Belief in Philosophy and Artiﬁcial Intelligence (A. Laux & H. Wansing, eds.), Akademie
Verlag, Berlin, 1995, pp. 103–146.
[van Linder et al., 1997] B. van Linder, W. van der Hoek & J.-J. Ch. Meyer, Seeing is Believing
(And So Are Hearing and Jumping), Journal of Logic, Language and Information 6, 1997,
pp. 33–61.
[Lokhost, 2002] G.J.C. Lokhorst. Mally’s deontic logic. In Edward N. Zalta, ed., Stanford En-
cyclopedia of Philosophy, The Metaphysics Research Lab at the Center for the Study of
Language and Information, Stanford University, Stanford, CA, 2002.
[Lokhorst, 2010] G.J.C. Lokhorst. Where did Mally go wrong?
In:
Proc. DEON 2010 (G.
Governatori & G. Sartor, eds.), Lecture Notes in Artiﬁcial Intelligence (LNAI) 6181, Springer-
Verlag, Berlin / Heidelberg, 2010, pp. 247-258.
[Lokhorst, 2012] G.J.C. Lokhorst. An intuitionistic reformulation of Mally’s deontic logic, Jour-
nal of Philosophical Logic, published online, 2012.
[Lomuscio and Raimondi, 2006] A. Lomuscio & F. Raimondi, MCMAS: A model checker for
multi-agent systems, in: Proc. TACAS 2006, LNCS 3920, Springer, Berlin, 2006, pp. 450–454
[Lomuscio and Sergot, 2003] A. Lomuscio & M. Sergot, Deontic Interpreted Systems, Studia
Logica 75(1), 2003, pp. 63–92.
[Lomuscio et al., 2009] A. Lomuscio, H. Qu & F. Raimondi. MCMAS: A model checker for the
veriﬁcation of multi-agent systems, in: Proc. CAV09, LNCS 5643, Springer, Berlin, 2009, pp.
682–688.
[Lorini and Herzig, 2008] E. Lorini & A. Herzig, A logic of intention and attempt, Synthese
KRA 163(1), 2008, pp. 45–77.
[Makinson and van der Torre, 2000] D. Makinson & L. van der Torre, 2000. Input/output logics,
J. Philosophical Logic 29, 2000, pp. 383–408.
[Mally, 1926] E. Mally, Grundgesetze des Sollens, Elemente der Logik des Willens, Leuschner
& Lubensky, Graz, 1926.
[McCarthy, 1963] J. McCarthy. Situations, actions and causal laws. Technical Report, Stanford
University,1963; later published in: M. Minsky, ed., Semantic Information Processing, MIT
Press, Cambridge, MA, 1968, pp. 410–417. .

Logics for Intelligent Agents and Multi-Agent Systems
657
[McNamara, 2010] P.
McNamara,
Deontic
Logic,
in:
The
Stanford
Encyclo-
pedia
of
Philosophy
(Fall
2010
Edition),
Edward
N.
Zalta
(ed.),
URL
=
¡http://plato.stanford.edu/archives/fall2010/entries/logic-deontic/¿.
[Meyer, 1988] J.-J. Ch. Meyer, A Diﬀerent Approach to Deontic Logic: Deontic Logic Viewed
As a Variant of Dynamic Logic, Notre Dame J. of Formal Logic 29 (1), 1988, pp. 109-136.
[Meyer et al., 2014] J.-J. Ch. Meyer, J. Broersen & A. Herzig, BDI Logics, in: Handbook of
Epistemic Logic (H. van Ditmarsch, J. Halpern, W. van der Hoek & B. Kooi eds.), College
Publications, 2014, to appear.
[Meyer and van der Hoek, 1995] J.-J. Ch. Meyer & W. van der Hoek, Epistemic Logic for AI
and Computer Science, Cambridge Tracts in Theoretical Computer Science 41, Cambridge
University Press, 1995.
[Meyer et al., 1999] J.-J. Ch. Meyer, W. van der Hoek & B. van Linder, A Logical Approach to
the Dynamics of Commitments, Artiﬁcial Intelligence 113, 1999, 1–40.
[Meyer and Veltman, 2007] J.-J. Ch. Meyer & F. Veltman, Intelligent Agents and Common
Sense Reasoning, Chapter 18 of: P. Blackburn, J.F.A.K. van Benthem, & F. Wolter (eds.),
Handbook of Modal Logic, Elsevier, Amsterdam, 2007, pp. 991–1029.
[Meyer et al., 1998] J.-J. Ch. Meyer, R.J. Wieringa & F.P.M. Dignum, The Role of Deontic
Logic in the Speciﬁcation of Information Systems, in: Logics for Databases and Information
Systems (J. Chomicki & G. Saake, eds.), Kluwer, Boston/Dordrecht, 1998, pp. 71-115.
[Montague, 1970] R. Montague, Universal Grammar, Theoria 36, 1970, pp. 373–398.
[Nute, 1997] Donald Nute (ed.), Defeasible Deontic Logic: Essays in Nonmonotonic Normative
Reasoning. Kluwer Academic Publishers, Dordrecht, Holland, 1997.
[Pauly, 2001] M. Pauly, Logic for Social Software, ILLC Dissertations Series, Amsterdam, 2001.
[Pnueli, 1977] A. Pnueli, The Temporal Logic of Programs, in: Proc. 18th Symp. on Foundations
of Computer Science, IEEE Computer Society Press, 1977, pp. 46–57.
[Pokahr et al., 2003] A. Pokahr, L. Braubach & W. Lamersdorf, JADEX: Implementing a BDI-
infrastructure for JADE agents, EXP - in search of innovation (Special Issue on JADE), 3(3),
2003, pp. 76–85.
[Rao, 1996] A.S. Rao, AgentSpeak(L): BDI Agents Speak Out in a Logical Computable Lan-
guage, in: Agents Breaking Away (W. van der Velde & J. Perram, eds.), LNAI 1038, Springer,
Berlin, 1996, pp. 42–55.
[Rao and Georgeﬀ, 1991] A.S. Rao & M.P. Georgeﬀ, Modeling rational agents within a BDI-
architecture, in Proceedings of the Second International Conference on Principles of Knowl-
edge Representation and Reasoning (KR’91) (J. Allen, R. Fikes & E. Sandewall, eds.), Morgan
Kaufmann, 1991, pp. 473–484.
[Rao and Georgeﬀ, 1998] A.S. Rao & M.P. Georgeﬀ, Decision Procedures for BDI Logics, J. of
Logic and Computation 8(3), 1998, pp. 293–344.
[Reiter, 1980] R. Reiter, A Logic for Default Reasoning, Artiﬁcial Intelligence13, 1980, pp. 81–
132.
[Reiter, 1991] R. Reiter. The frame problem in the situation calculus: a simple solution (some-
times) and a completeness result for goal regression. In: V. Lifschitz, ed., {em Artiﬁcial
Intelligence and Mathematical Theory of Computation. Papers in Honor of John McCarthy,
Academic Press, San Diego, CA, 1991, pp. 418–420.
[Reiter, 2001] R. Reiter. Knowledge in Action: Logical Foundations for Specifying and Imple-
menting Dynamical Systems, The MIT Press, Cambridge, Massachusetts, 2001.
[Russell and Norvig, 2009] S. Russell & P. Norvig, Artiﬁcial Intelligence: A Modern Approach
(3rd edition), Prentice Hall, Upper Saddle River /Boston, 2009.
[van der Hoek et al., 2000] W. van der Hoek, J.-J. Ch. Meyer & J.W. van Schagen, Formalizing
Potential of Agents: The KARO Framework Revisited, in: Formalizing the Dynamics of
Information (M. Faller, S. Kaufmann & M. Pauly, eds.), CSLI Publications, (CSLI Lect.
Notes 91), Stanford, 2000, pp. 51-67.
[Scott, 1970] D. Scott, Advice in modal logic, in: Philosophical Problems in Logic (K. Lambert,
ed.). Reidel, Dordrecht, 1970.
[Searle, 1969] J. Searle, Speech Acts, An Essay in the Philosophy of Language, Cambridge Uni-
versity Press, Cambridge, UK, 1969.
[Searle, 1995] J. Searle, The Construction of Social Reality, Free Press, 1995.
[Shoham, 1993] Y. Shoham, Agent-Oriented Programming, Artiﬁcial Intelligence 60(1), 1993,
pp. 51–92.

658
John-Jules Ch. Meyer
[Singh, 1990] M.P. Singh, Group Intentions, in: Proc. 10th Int. Workshop on Distributed Arti-
ﬁcial Intelligence (IWDAI-90), 1990.
[Singh, 1991] M.P. Singh, Group Ability and Structure, in: Proc. of 2nd European Workshop
on Modelling Autonomous Agents in a Multi-Agent World (MAAMAW-90) (Y. Demazeau &
J.-P. M¨uller, eds.), Elsevier, Amsterdam, 1991, pp. 127–146.
[Singh, 1991a] M.P. Singh, Towards a Formal Theory of Communication for Multi-Agent Sys-
tems, in: Proc. 12th Int. J. Conf. on Artif. Intell. (IJCAI-91), Sydney, Australia, 1991, pp.
69–74.
[Singh, 1993] M.P. Singh, A Semantics for Speech Acts, Annals of Mathematics and Artiﬁcial
Intelligence 8(I-II), 1993, pp. 47–71.
[Singh, 1994] M.P. Singh, Multi-Agent Systems:
A Theoretical Framework for Intentions,
Know-How, and Communications, Springer, Heidelberg, 1994.
[Singh, 1998] M.P. Singh, The Intentions of Teams: Team Structure, Endodeixis, and Exodeixis,
in: Proc. 13th Eur. Conf. on Artif. Intell. (ECAI’98) (H. Prade, ed.), Wiley, Chichester, 1998,
pp. 303–307.
[Tarski, 1955] A.Tarski, A Lattice-theoretical Fixpoint Theorem and Its Applications, Paciﬁc
Journal of Mathematics 5:2, 1955, pp. 285–309.
[Thomason, 1981] R.H. Thomason, Deontic Logic As Founded on Tense Logic, in: New Studies
ion Deontic Logic (R. Hilpinen, ed.), Reidel, Dordrecht, 1981, pp. 165–176.
[Turing, 1950] A.M. Turing, Computing Machinery and Intelligence, Mind 59, 1950, pp. 433–
460.
[van der Torre, 1997] L.
van
der
Torre,
Reasoning
about
Obligations:
Defeasibility
in
Preference-Based Deontic Logic, PhD Thesis, Erasmus University Rotterdam, Rotterdam,
1997.
[Wikipedia, 2013] http://en.wikipedia.org/wiki/Institution, February 18th, 2013
[Wooldridge, 1999] M.J. Wooldridge, Intelligent Agents, in: Multiagent Systems (G. Weiss, ed.),
The MIT Press, Cambridge, MA, 1999, pp. 27–77.
[Wooldridge, 2000] M.J. Wooldridge, Reasoning about Rational Agents, The MIT Press, Cam-
bridge, MA, 2000.
[Wooldridge, 2009] M. Wooldridge, An Introduction to MultiAgent Systems (2nd edition), John
Wiley & Sons, Chichester, UK, 2009.
[Wooldridge and Jennings, 1995] M.J. Wooldridge & N.R. Jennings (eds.), Intelligent Agents,
Springer, Berlin, 1995.
[von Wright, 1951] G.H. von Wright, Deontic Logic, Mind 60, 1951, pp. 1-15.
[von Wright, 1964] G.H. von Wright, A New System of Deontic Logic, Danish Yearbook of Phi-
losophy 1, 1964, pp. 173-182.
[von Wright, 1980] G.H. von Wright, Problems and Prospects of Deontic Logic: A Survey, in: E.
Agazzi (ed.), Modern Logic - A Survey: Historical, Philosophical and Mathematical Aspects
of Modern Logic and Its Applications, Reidel, Dordrecht / Boston, 1980, pp. 399-423.
[˚Agotnes, 2006] T. ˚Agotnes, Action and Knowledge in Alternating-Time Temporal Logic, Syn-
these / KRA 149, 2006, pp. 375–407.
[˚Aqvist and Hoepelman, 1981] L. ˚Aqvist & J. Hoepelman, Some Theorems about a Tree Sys-
tem of Deontic Tense Logic, in: New Studies ion Deontic Logic (R. Hilpinen, ed.), Reidel,
Dordrecht, 1981, pp. 187-221.

DESCRIPTION LOGICS
Matthias Knorr and Pascal Hitzler
Readers: Franz Baader and Ian Horrocks
1
INTRODUCTION
One of the key objectives of research in Artiﬁcial Intelligence (AI) from its very
beginning is the ability to represent information on a domain of interest in a
compact way and, at the same time, to derive implicit information from this
representation. This ﬁeld in AI research is known as Knowledge Representation
and Reasoning (KRR).
Description Logics (DLs) [Baader et al., 2007a; Baader et al., 2007b; Calvanese
et al., 2001; Hitzler et al., 2010] emerged within KRR research from early network-
based approaches, by building on their structured/taxonomic organization of a
terminology of a domain of interest, but equipping it with a well-understood logic-
based semantics. In fact, concepts (classes of objects) can be interpreted as unary
predicates and roles (properties linking such classes) as binary predicates, and
complex expressions can be built using logic-based constructors in an inductive
way.
This formal underpinning proves very useful when developing inference services
in Description Logics, all the more so, as it turns out that the full expressive
power of ﬁrst-order logic is often not required. Rather, a decidable fragment of
it usually suﬃces, which paves the way for eﬃcient reasoning procedures tailored
to the particular language, i.e., to the necessary complex constructors, and to the
concrete KRR application in mind.
Overall, the research in Description Logics and the development of KRR systems
that build on DLs follow a number of important principles that distinguish the
area from others in KRR research.
First, as already pointed out, the terms in the knowledge base are organized in
a taxonomic, ontological way and the semantics based on ﬁrst-order logics strictly
adheres to the so-called open world assumption, in which inferences can only be
drawn based on the content explicitly present in the knowledge base. This makes
DLs particularly suitable for applications where these properties are beneﬁcial or
even required, such as modeling ontologies in the Semantic Web (cf. the corre-
sponding chapter in this volume).
Second, as ﬁrst introduced in [Brachman and Levesque, 1984], a crucial idea pur-
sued in DL research is the trade-oﬀbetween the expressive power of the language
Handbook of the History of Logic. Volume 9: Computational Logic. 
Volume editor: Jörg Siekmann 
Series editors: Dov M. Gabbay and John Woods 
Copyright © 2014 Elsevier BV. All rights reserved.

660
Matthias Knorr and Pascal Hitzler
(given the provided complex constructors) and the computational complexity of
reasoning in that language. In this sense, one has to balance the admitted com-
plex constructors for an application domain w.r.t. what is expressible and how
fast inferences can be obtained. One should note here, that the aim is always to
obtain a balance in which reasoning tasks are at least decidable, (independently of
the concrete computational complexity), unlike other KRR formalisms that may
trade decidability for more expressive power.
A third principle is the close connection between theory and practice. This is
witnessed throughout the history of DLs by the mutual direct impact theoretical
research and implementations (driven by actual applications, such as in natural
language processing, database management, medical informatics, and software en-
gineering to name but a few) had on each other.
Illustrating these principles, in this chapter we give an overview of Description
Logics as a KRR formalism, pointing out its historic roots (Sect. 2), its syntax
and semantics (Sect. 3), and algorithmic aspects (Sect. 4). We also discuss recent
developments in DLs (Sect. 5) before we conclude (Sect. 6). Note that the material
presented here is not meant to be exhaustive. More details can be found in [Baader
et al., 2007a; Baader et al., 2007b; Calvanese et al., 2001; Hitzler et al., 2010]. In
particular the DL Handbook [Baader et al., 2007a] oﬀers a very detailed account
on all aspects of theory, implementation, and application of Description Logics.
Furthermore, the chapter Logics for the Semantic Web (in this volume) discusses
Description Logics in the context of the Semantic Web ﬁeld, which is its currently
most prominent application area.
2
HISTORIC ROOTS
Knowledge Representation and Reasoning as a ﬁeld became considerably popular
in the early 1970s and a large variety of diﬀerent approaches emerged, whose un-
derlying motivations and rationales diﬀered substantially. Among these proposals,
logic-based formalisms stem from the idea that a formalization in ﬁrst-order pred-
icate logic is most suitable due to its generality and highly expressive language.
That, however, also means that reasoning, i.e., the computation of logical deduc-
tions, is in general not decidable, and also usually considerably less eﬃcient than
reasoning in a language that is tailored to the requirements of the application in
mind (see, e.g., [Tsarkov et al., 2004] for a more recent such comparison).
At the same time, a variety of non-logic-based formalisms were introduced.
Common to them is that they were obtained by observing human behavior on
resolving certain tasks or by performing cognitive experiments, and then creating
formal representations that model the observations and emulate intelligent be-
havior. Though based on concrete observations, it was then expected that such
systems would be applicable in general also to other problem domains. Unlike the
logic-based approaches, knowledge representation and reasoning was achieved in a
rather ad-hoc manner driven by the speciﬁc needs of an application and therefore
potentially quite diﬀerent from one application to the other.

Description Logics
661
One such early formalism is rule-based expert systems, such as MYCIN that, with
around 450 IF-THEN rules, was able to diagnose blood infections in a similar man-
ner as some experts and better than junior doctors (see [Russell and Norvig, 2010],
also for other systems similar in spirit). These systems were, however, criticized for
their lack of structure in the represented information (see, e.g., [Lehmann, 1992a]),
in the sense that neither the system nor the non-expert human reader had any
way to distinguish whether the encoded information was meaningful or not, so the
entire search space had to be considered when reasoning, and consequently the
re-use of expert systems for other purposes was rather diﬃcult.
So-called network-based structures seemed to oﬀer a solution for providing
the (expert) knowledge in a more structured way.
Network-based structures
[Lehmann, 1992a; Lehmann, 1992b] themselves also represent a variety of ap-
proaches, the ﬁrst being semantic networks [Quillian, 1967], in which a model
of human memory is created by transferring information from a dictionary into
a network of elements and their interconnections. Another prominent approach
are frame systems [Minsky, 1981], in which frames serve as prototypes and rela-
tionships between such frames are expressible. Despite their diﬀerences, common
to these early KRR formalisms is the objective to model sets of classes and the
relations between these classes in a structured, taxonomic way.
However, the lack of formal semantics for these network-based structures meant
that structured information could still be ambiguous and was therefore cause for
considerable criticism [Brachman, 1977; Hayes, 1977; Hayes, 1979; Woods, 1975].
For example, a relation between two classes of individuals could mean that either
there is some relation between the individuals of these classes, or that the relation
is true for all individuals of these related classes, or even that the relation only
holds (by default) as long as no knowledge to the contrary is explicitly available
(see also [Brachman, 1983; Palomki and Kangassalo, 2012]). The consequence of
this ambiguity is that many early systems building on network-based structures
behave diﬀerently despite appearing to be almost identical.
In [Hayes, 1979], it was realized that a formal semantics could be provided for
frames, basically by relying on ﬁrst-order predicate logic: sets of individuals can
be represented by unary predicates, and relations between such sets can be repre-
sented by binary predicates. One may wonder now whether this would not result
simply in a logic-based formalism as described before including its disadvantages.
As it turns out, network-based structures do not require the full expressiveness of
ﬁrst-order logic [Brachman and Levesque, 1985]. It suﬃces to use fragments of it,
and the varying features in those network-based structures can then be represented
by diﬀerent (Boolean) constructors resulting in diﬀerent fragments of ﬁrst-order
logic. As a consequence, it was recognized that reasoning in such structure-based
representations could be achieved by specialized reasoners without relying on full
ﬁrst-order theorem provers. In addition to that, it was discovered that there is a
trade-oﬀbetween the expressive power of the language resulting from the inductive
combination of the admitted language constructors and the computational prop-
erties of that language [Brachman and Levesque, 1984]. This also introduced the

662
Matthias Knorr and Pascal Hitzler
idea of studying computational properties in terms of computational complexity
to the area of DLs, and mapping out the computational complexity of diﬀerent
DL languages became one of the driving forces in DL research.
The ﬁrst system based on such a formal semantics was KL-ONE [Brachman and
Schmolze, 1985], which is based on structured inheritance networks [Brachman,
1977; Brachman, 1978]. As pointed out in [Nardi and Brachman, 2007], KL-ONE
introduced many of the key notions used in Description Logics, for example, the
notions of concepts and roles and the relation between them; “value restriction”
and “number restriction” as new ideas that changed the use of roles when deﬁning
concepts; and the reasoning tasks of subsumption and classiﬁcation (see Sect. 3 for
explanations on these terms). It also paved the way towards the later distinction
between TBox and ABox and provided a ﬁrst example of the close connection
between theory and practice in Description Logics. Moreover, KL-ONE triggered
the appearance of so-called hybrid systems, such as KRYPTON [Brachman et al.,
1983], that combined an expressive logic- or rule-based reasoner for the ABox with
a taxonomic reasoner for the TBox, and the examination and evaluation of KL-
ONE and similar systems then would be the starting point for description logic
systems that will be brieﬂy described in Sect. 4.1
3
SYNTAX AND SEMANTICS
Traditionally speaking, a description logic is a decidable fragment of ﬁrst-order
predicate logic,2 where decidability is obtained by disallowing function symbols
and by suitably restricting the use of quantiﬁers.
We will formally introduce
the description logic ALC (from Attributive Logic with Complement), which is
usually considered to be the most basic description logic. We will also discuss
some prominent extensions and fragments of ALC. While this is only a very brief
introduction, we refer the reader to [Baader et al., 2007a; Baader et al., 2007b;
Hitzler et al., 2010] for further details.
Let NC, NR, and NI be countably inﬁnite sets of concept names, role names, and
individual names, respectively. Concept names are also called atomic concepts or
atomic classes. Complex concepts (in short, concepts) can now be formed according
to the grammar
C ::= ⊤| ⊥| A | ¬C | C ⊓D | C ⊔D | ∃R.C | ∀R.C,
where A ∈NC is an atomic concept, R ∈NR is a role, and C, D are complex
concepts. ⊤is called the top concept, while ⊥is called the bottom concept. A
general inclusion axiom (GCI ) is a statement of the form C ⊑D, where C and
D are concepts. A TBox is a ﬁnite set of general inclusion axioms. An ABox is a
1More details on these DL systems and its predecessors can be found in the chapter on
Description Logic Systems in the DL Handbook [M¨oller and Haarslev, 2007].
2However, see Sect. 5 for pointers to recent developments, which sometimes incorporate al-
ternative semantics.

Description Logics
663
⊤
true
⊥
false
A
A(x)
¬C
¬C(x)
C ⊓D
C(x) ∧D(x)
C ⊔D
C(x) ∨D(x)
∃R.C
∃y(R(x, y) ∧D(y))
∀R.C
∀y(R(x, y) →D(y))
C ⊑D
∀x(C(x) →D(x))
C(a)
C(a)
R(a, b)
R(a, b)
Table 1. Translating description logic axioms into ﬁrst-order predicate logic.
ﬁnite set of concept assertion axioms and role assertion axioms. The former are
of the form C(a), where C is a concept and a ∈NI, and the latter are of the form
R(a, b), where a, b ∈NI and R ∈NR. An ALC knowledge base is a union of an
ABox and a TBox.
In terms of ﬁrst-order predicate logic, individuals are constants, concepts are
unary predicates, and roles are binary predicates. In fact, every axiom can be
translated directly into ﬁrst-order predicate logic as indicated in Table 1.
Of
course, this translation has to be applied recursively, with suitable variable renam-
ings. ALC indeed inherits its model-theoretic semantics from ﬁrst-order predicate
logic by means of this translation.
With the semantics in place, a number of standard inference problems can be
deﬁned. Given an ALC knowledge base K, K is called consistent if it has a model.
A concept C is satisﬁable w.r.t. K if there exists a model I of K with CI ̸= ∅, in
which case we call I a model of C w.r.t. K. Concept C is subsumed by concept D
w.r.t. K, written C ⊑K D, if CI ⊆DI holds for all models I of K. Two concepts,
C and D are equivalent w.r.t. K, written C ≡K D, if C is subsumed by D and
vice-versa, and disjoint w.r.t. K if CI ∩DI = ∅for every model I of K. Also, an
individual a is an instance of a concept C w.r.t. K, written K |= C(a), if aI ∈CI
holds for all models I of K. Likewise, a pair of individuals (a, b) is an instance of
a role r w.r.t. K, written K |= r(a, b), if (aI, bI) ∈rI holds for all models I of K.
The deﬁnition of these reasoning tasks carries over to other DLs. Depending on
the constructors available in a concrete DL, reasoning tasks can be reduced to each
other, which means that quite often only one of those tasks has to be considered
when developing inference engines. This applies for example to ALC [Baader and
Nutt, 2007], and it was shown that reasoning in ALC is decidable [Schmidt-Schauß
and Smolka, 1991], namely when establishing that concept satisﬁability in ALC
is PSpace-hard for acyclic TBoxes, i.e., where GCIs do not form cycles, and later
also ExpTime-complete in general [Schild, 1991; Donini and Massacci, 2000]. More
details on complexity and related algorithms follow in Sect. 4.

664
Matthias Knorr and Pascal Hitzler
≤nR.C
Q
max card.
Vn+1
i=1 (R(x, yi) ∧C(yi)) →W
i,j yi = yj
≥nR.C
Q
min card.
∃n
i=1yi(V
i(R(x, yi) ∧C(yi) ∧Vn
j=1 yi ̸= yj))
{a}
O
nominal
x = a
R1 ⊑R2
H
role incl.
R1(x, y) →R2(x, y)
R1 ◦· · · ◦Rn ⊑R
R
role chain
(Vn
i=1 Ri(xi, xi+1)) →R(x1, xn+1))
R−
1 ⊑R2
I
inverse
R1(y, x) →R2(x, y)
Table 2.
More description logic constructs (left column), where Ri and R are
roles, C is a class, a is an individual, and n is a non-negative integer. The second
column gives a letter which is used to identify the construct in the commonly
used description logic naming convention. In addition, the letter S is used as an
abbreviation for ALCH with transitivity axioms of the form R ◦R ⊑R. The
letter N is the restriction of Q to the case where C = ⊤. N and Q are also called
number restrictions. The last column gives a translation into ﬁrst-order predicate
logic. Role chains are also sometimes called complex role inclusion axioms.
Further reasoning tasks that are commonly not of the same computational com-
plexity can be obtained as variations of the previous ones. Classiﬁcation requires
to compute subsumptions between all concept names in the knowledge base. In-
stance retrieval focuses on the ﬁnding of instances of a given concept, while in the
realization problem, we are searching for the most speciﬁc concept C such that
K |= C(a) for a given individual a. Other non-standard reasoning tasks are also
listed as follows because of their potential interest for applications. Among them
are least common subsumer [Baader et al., 1999b; K¨usters and Molitor, 2001],
matching [Baader et al., 1999a; Baader and K¨usters, 2000] and approximation
and diﬀerence [Brandt et al., 2002]. Also of increasing importance are explana-
tions for entailed information [Horridge et al., 2008] also considered under axiom
pinpointing [Kalyanpur et al., 2005; Meyer et al., 2006; Schlobach et al., 2007;
Baader and Pe˜naloza, 2010] and conjunctive query answering [Glimm et al., 2008;
Eiter et al., 2009; Calvanese et al., 2013a].
Additional constructs have been introduced for extending ALC while retaining
decidability. We give some of the most important ones in Table 2. In some cases,
additional global syntactic restrictions have to be enforced to retain decidability.
Description logics which contain ALC are very expressive but of high com-
putational complexities (see Section 4). Description logics of comparatively low
computational complexity (e.g., PTime) have also been introduced more recently,
e.g. the logic EL++, which essentially supports only class conjunction (⊓), the top
concept ⊤, existential quantiﬁcation (∃R.C), nominals, and role chains [Baader et
al., 2005].
An example for a description logic knowledge base is given in Figure 1.
It
describes the formal deﬁnition of a so-called ontology design pattern [Gangemi,
2005] for the notion of trajectory.
The key idea behind the pattern is that a

Description Logics
665
Fix ⊑∃atTime.TemporalThing ⊓∃hasLocation.Position
⊓∃hasFix−.SemanticTrajectory
(1)
Segment ⊑∃startsFrom.Fix ⊓∃endsAt.Fix
(2)
⊤⊑≤1startsFrom.⊤
(3)
⊤⊑≤1endsAt.⊤
(4)
Segment ⊑∃hasSegment−.SemanticTrajectory
(5)
startsFrom−◦endsAt ⊑hasNext
(6)
hasNext ⊑hasSuccessor
(7)
hasSuccessor ◦hasSuccessor ⊑hasSuccessor
(8)
hasNext−⊑hasPrevious
(9)
hasSuccessor−⊑hasPredecessor
(10)
Fix ⊓¬∃endsAt.Segment ⊑StartingFix
(11)
Fix ⊓¬∃startsFrom.Segment ⊑EndingFix
(12)
Segment ⊓∃startsFrom.StartingFix ⊑StartingSegment
(13)
Segment ⊓∃endsAt.EndingFix ⊑EndingSegment
(14)
SemanticTrajectory ⊑∃hasSegment.Segment
(15)
hasSegment ◦startsFrom ⊑hasFix
(16)
hasSegment ◦endsAt ⊑hasFix
(17)
∃hasSegment.Segment ⊑SemanticTrajectory
(18)
∃hasSegment−.SemanticTrajectory ⊑Segment
(19)
∃hasFix.Segment ⊑SemanticTrajectory
(20)
∃hasFix−.SemanticTrajectory ⊑Fix
(21)
Figure 1. Example of a SRIN knowledge base. It encodes a so-called ontology
design pattern for the notion of trajectory. The example is taken from [Hu et al.,
2013].
trajectory consists of a sequence of segments, each of which has a start point and
an end point—these points (together with temporal information) are called ﬁxes
of the segment. Axiom (1) indeed states that each ﬁx has a location and carries
temporal information (both of which are not further speciﬁed in this pattern).
Furthermore, a ﬁx is always a ﬁx of some trajectory. Axiom (2) states that each
segment starts from a ﬁx and ends at a ﬁx. The cardinality statements in axioms
(3) and (4) state then that these two roles are functional in the sense that they
represent binary predicates which are in fact functions. Axiom (5) states that
every segment is indeed a segment of some trajectory. Axiom (6) uses role chains

666
Matthias Knorr and Pascal Hitzler
to ensure that the role hasNext connects each ﬁx in a trajectory directly to the
next ﬁx in the trajectory. Axioms (11) and (12) identify the ﬁrst and last ﬁx of a
trajectory, while (13) and (14) identify the ﬁrst and last segment of a trajectory.
The remaining axioms further declare relationships between the concepts and roles,
please refer to [Hu et al., 2013] for further information about this pattern and the
design rationales underlying it. The example can be expressed in the description
logic SRIN: Axioms (2), (11) to (15), (18) and (20) are in ALC, role hierarchies
(H) are used, e.g., in (7) and also transitivity is used in (8) – S stands for ALCH
plus transitivity. Role chains (R) are used, e.g., in (17), as well as cardinalities
(N) for the functionality statements in (3) and (4), and several occurrences of
inverse roles (I).
4
ALGORITHMIC ASPECTS
In the following, we discuss algorithmic advances, the systems in which these are
applied, and point out important results in computational complexity [Papadim-
itriou, 1994], thus drawing an arc from the ﬁrst still limited DL systems in the
early 1990s to the systems of today ranging from highly expressive general purpose
DL reasoners to specialized highly eﬃcient reasoners tailored to particular DLs.
In general, many of the ﬁrst DL systems employ so-called structural subsump-
tion algorithms, in which two descriptions are normalized and then their structure
is compared recursively [Nebel, 1990a; Borgida and Patel-Schneider, 1994]. These
algorithms are in general eﬃcient (polynomial) for a restricted language but in-
complete for more expressive DLs in the sense that not all possible inferences
can be derived, and diﬀerent systems adopt diﬀerent positions within that scale.
Namely, CLASSIC [Brachman et al., 1991] permits only a limited set of constructors
such that the computation is eﬃcient and complete, while other approaches, such
as LOOM [MacGregor and Bates, 1987; MacGregor, 1991] and BACK [Nebel and
von Luck, 1988; Peltason, 1991], are incomplete but allow for a more expressive
language.
Further investigations revealed that the source of incompleteness in
such systems are certain combinations of constructors in the language and that a
slight increase in the expressiveness could turn reasoning intractable [Brachman
and Levesque, 1985; Nebel, 1990b]. Additionally, all these systems employ rule-
based and/or closed-word reasoning services (mainly on the ABox) which adds
further expressiveness to the system, but causes problems since it deviates from
the formal semantics due to the additions being rather ad-hoc.
Trying to overcome the limitations of these early DL systems led to the develop-
ment of sound and complete algorithms for more expressive DLs and subsequently
to new systems, such as KRIS [Baader and Hollunder, 1991] and CRACK [Bresciani
et al., 1995], that were less eﬃcient but expressive and complete. The basic idea
behind these new tableau-based algorithms [Schmidt-Schauß and Smolka, 1991;
Donini et al., 1991; Hollunder et al., 1990] is trying to ﬁnd a proof for the unsatis-
ﬁability of a concept in a constructive way. If the proof fails, then a canonical model
representing a counterexample is obtained. Other reasoning tasks can be achieved

Description Logics
667
by reducing them into (un)satisﬁability of a concept, which is always possible for
the languages based on ALC in such systems. Initially, the high worst-case com-
plexity (ExpTime in general for ALC alone) was considered problematic [Buchheit
et al., 1993], but empirical analysis revealed that the combinations of constructors
leading to this high complexity are rarely occurring [Nebel, 1990b] and with some
optimizations, the performance of a DL system could be considerably improved
on average [Baader et al., 1992]. Due to their generality, these new systems also
turned out to be useful for comparing and benchmarking other systems [Baader
et al., 1992; Heinsohn et al., 1992].
In general, tableau-based algorithms became the dominating approach in DL
research for a number of reasons. Namely, the approach is rather ﬂexible, i.e.,
a variety of languages can be covered by simply adapting the considered tableau
expansion rules [Hollunder et al., 1990], but also, if necessary, adopting more
advanced mechanisms to ensure termination [Baader, 1991; Buchheit et al., 1993].
It also turned out that, for several DL languages, the worst-case complexity of the
algorithm is not worse than the complexity of deciding satisﬁability for the logic
[Hollunder et al., 1990], making tableaux also a widely used tool in complexity
analysis.
At the same time of the appearance of these ﬁrst tableau-based systems, an
alternative for devising algorithms and complexity analysis was introduced by
establishing relations to other logical formalisms. For example, it can easily be seen
from the translation of DLs to ﬁrst-order predicate logic in Section 3, that ALC falls
within L2 [Borgida, 1996], the two-variable fragment of ﬁrst-order predicate logic,
whose decidability was already shown in [Mortimer, 1975]. Not all of the additional
constructors shown in Table 2 can be expressed in L2, but number restrictions can
be expressed in C2, i.e., L2 extended by counting quantiﬁers, that is also decidable
[Gr¨adel et al., 1997; Pacholski et al., 1997].
However, algorithms building on
this correspondence are in general not optimal, i.e., of higher complexity than
necessary.
This diﬀers for the relation between multi-modal logic and DLs [Schild, 1991]
essentially obtained by viewing ∃R and ∀R as modalities. In fact, ALC is a variant
of the propositional multi-modal logic K, and ALC with transitive closure of roles
[Baader, 1991] matches Propositional Dynamic Logic (PDL). This not only yielded
the precise complexity of so-called ALCtrans (ExpTime-complete [Fischer and Lad-
ner, 1979]), but was also used in subsequent years to transfer known decidability
results from modal logics to DLs [Schild, 1994; De Giacomo and Lenzerini, 1994a;
De Giacomo and Lenzerini, 1994b; De Giacomo and Lenzerini, 1996]. Addition-
ally, there exists a strong similarity between algorithms for deciding satisﬁability
in PDL and the tableau-based algorithms in DLs.
As requested by applications and driven by the just mentioned correspondence
and results on tableau-based algorithms for more expressive DLs [Horrocks and
Sattler, 1999; Horrocks et al., 2000], the next generation of tableau-based DL
systems emerged at the end of the 1990s, namely FaCT [Horrocks, 1998], RACE
[Haarslev and M¨oller, 1999], and DLP [Patel-Schneider, 1999]. These reasoners

668
Matthias Knorr and Pascal Hitzler
employ considerably more expressive DL languages than before, but the use of
sophisticated optimization techniques [Horrocks, 2007] ensures that these reason-
ers are usable in practice. Continuous further improvements, incorporating and
optimizing more and more expressive features, in particular driven by the W3C
standard OWL for the Semantic Web, led to highly expressive general purpose DL
reasoners based on tableau algorithms incorporating DL languages whose worst-
case is N2ExpTime-complete, such as FaCT++ [Tsarkov and Horrocks, 2006], Pel-
let [Sirin et al., 2007], RACER [Haarslev et al., 2012] or Konclude [Steigmiller et
al., 2013].
In addition, more recently a number of approaches have been developed that
explore alternative algorithms commonly focusing on a restricted language and
aiming at more eﬃcient reasoning. Among them is KAON2 [Motik and Sattler,
2006], which is based on ordered resolution as a means of translating SHIQ DL
knowledge bases into disjunctive Datalog. Datalog-based reasoning has also been
applied to more restricted DL languages, e.g., for DLP [Grosof et al., 2003], the
Horn fragment of DLs, and for EL++ [Kr¨otzsch, 2010]. HermiT [Motik et al.,
2009] builds on a combination of hypertableau and hyperresolution for SHOIQ+.
Another approach is based on type elimination [Rudolph et al., 2008a; Rudolph
et al., 2008b; Rudolph et al., 2012] for SHIQbs extended with DL-safe rules by
basically transforming the TBox into ordered binary decision diagrams and then to
disjunctive datalog. A further line of investigation follows so-called consequence-
based approaches, such as CEL [Baader et al., 2006] for EL++, CB [Kazakov, 2009]
for Horn-SHIQ, ConDOR [Simancik et al., 2011] for ALCH, and ELK [Kazakov
et al., 2011] again for EL++, that classify the entire ontology in a bottom-up-
like fashion achieving considerable better performance than general tableau-based
algorithms.
It can be expected, that further research and new requirements from applica-
tions will push the limits of current DL systems even further. The ORE workshop
[Gon¸calves et al., 2013] may be a good indicator for novel tendencies of DL sys-
tems, including currently among others the idea of modular reasoners, as well as
reasoners for mobile devices with potentially limited resources.
5
RECENT DEVELOPMENTS
We brieﬂy discuss some of the recent research developments regarding Descrip-
tion Logics, and give some key pointers to the literature. Our list is by no means
exhaustive; an excellent way to understand the state of the art is to peruse the
proceedings of the annual Description Logic Workshop3 and to follow central Se-
mantic Web outlets, such as the International Semantic Web Conference (ISWC),
the Journal of Web Semantics (Elsevier), or the Semantic Web journal (IOS Press).
One of the major trends which started in the mid-2000s was to look at tractable
(i.e., polynomial time complexity) Description Logics. Obvious important can-
3See http://dl.kr.org/workshops/.

Description Logics
669
didates are certain Horn fragments of Description Logics [Grosof et al., 2003;
Kr¨otzsch et al., 2008; Kr¨otzsch et al., 2013], but a major stepping stone was
the discovery of EL++ [Baader et al., 2005], which does allow tractable standard
reasoning tasks, essentially, by excluding the constructors ¬, ⊔, and ∀, and its
application in the life sciences [Baader et al., 2006]. At the same time, the DL-
Lite family of Description Logics emerged [Calvanese et al., 2007], that focuses
on answering queries, basically by translating a conjunctive query by means of
the TBox into an SQL query which can be processed using data base technology.
This has gained further momentum recently with Ontology-based Data Access
(ObDA) [Calvanese et al., 2011; Calvanese et al., 2013b; Kharlamov et al., 2013],
i.e., utilizing an ontology to facilitate data access by providing views and queries
solely based on the language of the ontology. The importance of such tractable
languages is further emphasized by the fact that they were included in the latest
revision of the Web Ontology Language (OWL) standard by the World Wide Web
Consortium (W3C) [Motik et al., 2012; Hitzler et al., 2012].
Rather classical KRR topics make their reappearance in the context of Descrip-
tion Logics, usually driven by the need to enhance expressivity. These include,
e.g., fuzzy and probabilistic logics [Straccia, 2001; Bobillo and Straccia, 2009;
Lukasiewicz and Straccia, 2009; Borgwardt and Pe˜naloza, 2012; Borgwardt and
Pe˜naloza, 2013; Klinov and Parsia, 2013], temporal logics [Lutz, 2001; Sturm
and Wolter, 2002; Artale et al., 2013], and inconsistency handling, either through
bugﬁxing [Huang et al., 2005; Schlobach et al., 2007] or through paraconsistency
[Maier et al., 2013]. Novel is the emphasis on decidability and on complexity is-
sues. In particular the latter serve as a type of a-priori assessment of eﬃcient
implementability, although typical complexities are very high (see Section 4).
Important are also the relationships to other established reasoning paradigms,
in particular the relation to rule-based approaches, see, e.g., [Grosof et al., 2003;
Horrocks et al., 2004; Horrocks et al., 2005; Kr¨otzsch et al., 2013; Kr¨otzsch et
al., 2008; Kr¨otzsch et al., 2011; Krisnadhi et al., 2011] and also the chapter on
Logics for the Semantic Web in this volume. It was also argued very early that
aspects of the closed world assumption would be required for some application
contexts (see, e.g., [Grimm and Hitzler, 2008]), and so non-monotonic extensions
of description logics have been created, mostly based on established approaches in
the KRR ﬁeld [Baader and Hollunder, 1995; Donini et al., 1998; Donini et al., 2002;
Bonatti et al., 2009; Sengupta et al., 2011], and of course this has led to a combined
study of rules and non-monotonicity in relation to DLs, see, e.g., [Eiter et al., 2008;
Motik and Rosati, 2010; Knorr et al., 2011; Krisnadhi et al., 2011; Knorr et al.,
2012] and the references contained therein.
Concrete domains [Baader and Hanschke, 1991], i.e., the enhanced use of data
types, are also considered important for modeling and have drawn renewed atten-
tion recently [Lutz, 2004; Lutz et al., 2005; Lutz and Milicic, 2007]. Other investi-
gations are driven by Semantic Web-related use cases, in the wake of the adoption
of Description Logics for the W3C Web Ontology Language OWL [McGuinness
and van Harmelen, 2004; Hitzler et al., 2012], e.g., distributed knowledge bases

670
Matthias Knorr and Pascal Hitzler
[Borgida and Seraﬁni, 2003], justiﬁcations for reasoning results [Horridge et al.,
2008; Horridge et al., 2013], or enhancing eﬃciency by massive parallelization
[Schlicht and Stuckenschmidt, 2010; Mutharaju et al., 2013].
6
CONCLUSIONS
We have introduced Description Logics and described their historic roots. We also
discussed algorithmic aspects from a historic perspective and considered recent
research developments.
Description Logics can be traced back to network-based structures and frames.
Once they became established, their development was distinguished from previous
approaches to KRR by a focus on complexity and decidability. In the wake of
the Semantic Web [Hitzler et al., 2010], and in particular due to their adoption as
one of the main Semantic Web standards [McGuinness and van Harmelen, 2004;
Hitzler et al., 2012]. Research on theoretical and practical aspects of Description
Logics is still going strong, and its development in the near and intermediate future
will likely depend on further developments related to Semantic Web technologies.4
ACKNOWLEDGEMENTS
We would like to thank our readers Franz Baader and Ian Horrocks for their com-
ments that helped to improve this chapter. Pascal Hitzler acknowledges support
by the National Science Foundation under award 1017225 ”III: Small: TROn –
Tractable Reasoning with Ontologies.” Any opinions, ﬁndings, and conclusions or
recommendations expressed in this material are those of the author(s) and do not
necessarily reﬂect the views of the National Science Foundation. Matthias Knorr
acknowledges support by FCT funded project ERRO – Eﬃcient Reasoning with
Rules and Ontologies (PTDC/EIA-CCO/121823/2010) and also by FCT grant
SFRH/BPD/86970/2012.
BIBLIOGRAPHY
[Artale et al., 2013] Alessandro Artale, Roman Kontchakov, Frank Wolter, and Michael Za-
kharyaschev.
Temporal Description Logic for Ontology-Based Data Access.
In Francesca
Rossi, editor, IJCAI 2013, Proceedings of the 23rd International Joint Conference on Artiﬁ-
cial Intelligence, Beijing, China, August 3-9, 2013, pages 711–717. IJCAI/AAAI, 2013.
[Baader and Hanschke, 1991] Franz Baader and Philipp Hanschke.
A scheme for integrating
concrete domains into concept languages. In John Mylopoulos and Raymond Reiter, editors,
Proceedings of the 12th International Joint Conference on Artiﬁcial Intelligence. Sydney,
Australia, August 24-30, 1991, pages 452–457. Morgan Kaufmann, 1991.
[Baader and Hollunder, 1991] Franz Baader and Bernhard Hollunder.
KRIS:
Knowledge
Representation and Inference System. SIGART Bull., 2(3):8–14, 1991.
[Baader and Hollunder, 1995] F. Baader and B. Hollunder. Embedding Defaults into Termino-
logical Representation Systems. J. Autom. Reasoning, 14:149–180, 1995.
4See the chapter on Logics for the Semantic Web, in this volume.

Description Logics
671
[Baader and K¨usters, 2000] Franz Baader and Ralf K¨usters.
Matching Concept Descriptions
with Existential Restrictions. In Anthony G. Cohn, Fausto Giunchiglia, and Bart Selman,
editors, KR 2000, Principles of Knowledge Representation and Reasoning Proceedings of the
Seventh International Conference, Breckenridge, Colorado, USA, April 11-15, 2000, pages
261–272. Morgan Kaufmann, 2000.
[Baader and Nutt, 2007] Franz Baader and Werner Nutt. Basic Description Logics. In Franz
Baader, Diego Calvanese, Deborah L. McGuinness, Daniele Nardi, and Peter F. Patel-
Schneider, editors, The Description Logic Handbook: Theory, Implementation, and Appli-
cations, pages 43–95. Cambridge University Press, 2007.
[Baader and Pe˜naloza, 2010] Franz Baader and Rafael Pe˜naloza. Axiom Pinpointing in General
Tableaux. J. Log. Comput., 20(1):5–34, 2010.
[Baader et al., 1992] Franz Baader, Bernhard Hollunder, Bernhard Nebel, Hans-J¨urgen Prof-
itlich, and Enrico Franconi. An Empirical Analysis of Optimization Techniques for Termino-
logical Representation Systems, or Making KRIS Get a Move On. In Bernhard Nebel, Charles
Rich, and William R. Swartout, editors, Proceedings of the 3rd International Conference on
Principles of Knowledge Representation and Reasoning (KR’92). Cambridge, MA, October
25-29, 1992, pages 270–281. Morgan Kaufmann, 1992.
[Baader et al., 1999a] Franz Baader, Ralf K¨usters, Alexander Borgida, and Deborah L. McGuin-
ness. Matching in Description Logics. J. Log. Comput., 9(3):411–447, 1999.
[Baader et al., 1999b] Franz Baader, Ralf K¨usters, and Ralf Molitor. Computing Least Common
Subsumers in Description Logics with Existential Restrictions.
In Thomas Dean, editor,
Proceedings of the Sixteenth International Joint Conference on Artiﬁcial Intelligence, IJCAI
99, Stockholm, Sweden, July 31 - August 6, 1999. 2 Volumes, 1450 pages, pages 96–103.
Morgan Kaufmann, 1999.
[Baader et al., 2005] Franz Baader, Sebastian Brandt, and Carsten Lutz. Pushing the EL Enve-
lope. In Leslie Pack Kaelbling and Alessandro Saﬃotti, editors, IJCAI-05, Proceedings of the
Nineteenth International Joint Conference on Artiﬁcial Intelligence, Edinburgh, Scotland,
UK, July 30-August 5, 2005, pages 364–369. Professional Book Center, 2005.
[Baader et al., 2006] Franz Baader, Carsten Lutz, and Boontawee Suntisrivaraporn. CEL—A
Polynomial-time Reasoner for Life Science Ontologies.
In Ulrich Furbach and Natarajan
Shankar, editors, Automated Reasoning, Third International Joint Conference, IJCAR 2006,
Seattle, WA, USA, August 17-20, 2006, Proceedings, volume 4130 of Lecture Notes in Arti-
ﬁcial Intelligence, pages 287–291. Springer-Verlag, 2006.
[Baader et al., 2007a] Franz Baader, Diego Calvanese, Deborah L. McGuinness, Daniele Nardi,
and Peter F. Patel-Schneider, editors. The Description Logic Handbook: Theory, Implemen-
tation, and Applications. Cambridge University Press, 2nd edition, 2007.
[Baader et al., 2007b] Franz Baader, Ian Horrocks, and Ulrike Sattler. Description Logics. In
Frank van Harmelen, Vladimir Lifschitz, and Bruce Porter, editors, Handbook of Knowledge
Representation. Elsevier, 2007.
[Baader, 1991] Franz Baader. Augmenting Concept Languages by Transitive Closure of Roles:
An Alternative to Terminological Cycles. In John Mylopoulos and Raymond Reiter, editors,
Proceedings of the 12th International Joint Conference on Artiﬁcial Intelligence. Sydney,
Australia, August 24-30, 1991, pages 446–451. Morgan Kaufmann, 1991.
[Bobillo and Straccia, 2009] Fernando Bobillo and Umberto Straccia. Fuzzy description logics
with general t-norms and datatypes. Fuzzy Sets and Systems, 160(23):3382–3402, 2009.
[Bonatti et al., 2009] Piero A. Bonatti, Carsten Lutz, and Frank Wolter. The Complexity of
Circumscription in Description Logic. J. Artif. Intell. Res. (JAIR), 35:717–773, 2009.
[Borgida and Patel-Schneider, 1994] Alexander Borgida and Peter F. Patel-Schneider.
A Se-
mantics and Complete Algorithm for Subsumption in the CLASSIC Description Logic. J.
Artif. Intell. Res. (JAIR), 1:277–308, 1994.
[Borgida and Seraﬁni, 2003] Alexander Borgida and Luciano Seraﬁni. Distributed Description
Logics: Assimilating Information from Peer Sources. Journal on Data Semantics, 1:153–184,
2003.
[Borgida, 1996] Alexander Borgida. On the Relative Expressiveness of Description Logics and
Predicate Logics. Artif. Intell., 82(1-2):353–367, 1996.
[Borgwardt and Pe˜naloza, 2012] Stefan Borgwardt and Rafael Pe˜naloza. Undecidability of fuzzy
description logics. In Gerhard Brewka, Thomas Eiter, and Sheila A. McIlraith, editors, Princi-
ples of Knowledge Representation and Reasoning: Proceedings of the Thirteenth International
Conference, KR 2012, Rome, Italy, June 10-14, 2012. AAAI Press, 2012.

672
Matthias Knorr and Pascal Hitzler
[Borgwardt and Pe˜naloza, 2013] Stefan Borgwardt and Rafael Pe˜naloza.
The complexity of
lattice-based fuzzy description logics. J. Data Semantics, 2(1):1–19, 2013.
[Brachman and Levesque, 1984] Ronald J. Brachman and Hector J. Levesque. The Tractability
of Subsumption in Frame-Based Description Languages.
In Ronald J. Brachman, editor,
Proceedings of the National Conference on Artiﬁcial Intelligence. Austin, TX, August 6-10,
1984, pages 34–37. AAAI Press, 1984.
[Brachman and Levesque, 1985] Ronald J. Brachman and Hector J. Levesque, editors. Readings
in Knowledge Representation. Morgan Kaufman, Los Altos, 1985.
[Brachman and Schmolze, 1985] Ronald J. Brachman and James G. Schmolze. An overview of
the KL-ONE knowledge representation system. Cognitive Science, 9(2):171–216, 1985.
[Brachman et al., 1983] Ronald J. Brachman, Richard Fikes, and Hector J. Levesque. Krypton:
A Functional Approach to Knowledge Representation. IEEE Computer, 16(10):67–73, 1983.
[Brachman et al., 1991] Ronald J. Brachman,
Deborah L. McGuinness,
Peter F. Patel-
Schneider, Lori A. Resnick, and Alexander Borgida. Living with CLASSIC: When and how
to use a KL-ONE-like language. In John F. Sowa, editor, Principles of Semantic Networks,
pages 401–456. Morgan Kaufman, Los Altos, 1991.
[Brachman, 1977] Ronald J. Brachman. What’s in a Concept: Structural Foundations for Se-
mantic Networks. International Journal of Man-Machine Studies, 9(2):127–152, 1977.
[Brachman, 1978] Ronald J. Brachman. Structured inheritance networks. In Research in Natural
Language Understanding, Quarterly Progress Report No. 1, BBN Report No. 3742, pages 36–
78. Bolt, Beranek and Newman Inc., Cambridge, Mass., 1978.
[Brachman, 1983] Ronald J. Brachman. What IS-A Is and Isn’t: An Analysis of Taxonomic
Links in Semantic Networks. IEEE Computer, 16(10):30–36, 1983.
[Brandt et al., 2002] Sebastian Brandt, Ralf K¨usters, and Anni-Yasmin Turhan. Approxima-
tion and Diﬀerence in Description Logics. In Dieter Fensel, Fausto Giunchiglia, Deborah L.
McGuinness, and Mary-Anne Williams, editors, Proceedings of the Eights International Con-
ference on Principles and Knowledge Representation and Reasoning (KR-02), Toulouse,
France, April 22-25, 2002, pages 203–214. Morgan Kaufmann, 2002.
[Bresciani et al., 1995] Paolo Bresciani, Enrico Franconi, and Sergio Tessaris. Implementing and
Testing Expressive Description Logics: Preliminary Report. In Proc. of the 1995 Description
Logic Workshop (DL’95), pages 131–139, 1995.
[Buchheit et al., 1993] Martin Buchheit, Francesco M. Donini, and Andrea Schaerf. Decidable
Reasoning in Terminological Knowledge Representation Systems. J. Artif. Intell. Res. (JAIR),
1:109–138, 1993.
[Calvanese et al., 2001] Diego Calvanese, Giuseppe De Giacomo, Maurizio Lenzerini, and
Daniele Nardi.
Reasoning in Expressive Description Logics.
In John Alan Robinson and
Andrei Voronkov, editors, Handbook of Automated Reasoning, pages 1581–1634. Elsevier and
MIT Press, 2001.
[Calvanese et al., 2007] Diego Calvanese, Giuseppe De Giacomo, Domenico Lembo, Maurizio
Lenzerini, and Riccardo Rosati. Tractable Reasoning and Eﬃcient Query Answering in De-
scription Logics: The DL-Lite Family. J. Autom. Reasoning, 39(3):385–429, 2007.
[Calvanese et al., 2011] Diego Calvanese, Giuseppe De Giacomo, Domenico Lembo, Maurizio
Lenzerini, Antonella Poggi, Mariano Rodriguez-Muro, Riccardo Rosati, Marco Ruzzi, and
Domenico Fabio Savo. The MASTRO system for ontology-based data access. Semantic Web,
2(1):43–53, 2011.
[Calvanese et al., 2013a] Diego Calvanese, Giuseppe De Giacomo, Domenico Lembo, Maurizio
Lenzerini, and Riccardo Rosati. Data complexity of query answering in description logics.
Artif. Intell., 195:335–360, 2013.
[Calvanese et al., 2013b] Diego Calvanese, Martin Giese, Peter Haase, Ian Horrocks, Thomas
Hubauer, Yannis E. Ioannidis, Ernesto Jim´enez-Ruiz, Evgeny Kharlamov, Herald Kllapi,
Johan W. Kl¨uwer, Manolis Koubarakis, Steﬀen Lamparter, Ralf M¨oller, Christian Neuenstadt,
T. Nordtveit, ¨Ozg¨ur L. ¨Oz¸cep, Mariano Rodriguez-Muro, Mikhail Roshchin, Domenico Fabio
Savo, Michael Schmidt, Ahmet Soylu, Arild Waaler, and Dmitriy Zheleznyakov. Optique:
OBDA Solution for Big Data. In Philipp Cimiano, Miriam Fern´andez, Vanessa Lopez, Stefan
Schlobach, and Johanna V¨olker, editors, The Semantic Web: ESWC 2013 Satellite Events –
ESWC 2013 Satellite Events, Montpellier, France, May 26-30, 2013, Revised Selected Papers,
volume 7955 of Lecture Notes in Computer Science, pages 293–295. Springer, 2013.

Description Logics
673
[De Giacomo and Lenzerini, 1994a] Giuseppe De Giacomo and Maurizio Lenzerini. Boosting the
Correspondence between Description Logics and Propositional Dynamic Logics. In Barbara
Hayes-Roth and Richard E. Korf, editors, Proceedings of the 12th National Conference on
Artiﬁcial Intelligence, Seattle, WA, USA, July 31 - August 4, 1994, Volume 1, pages 205–212.
AAAI Press / The MIT Press, 1994.
[De Giacomo and Lenzerini, 1994b] Giuseppe De Giacomo and Maurizio Lenzerini.
Concept
Language with Number Restrictions and Fixpoints, and its Relationship with µ-calculus. In
Anthony G. Cohn, editor, Proceedings of the Eleventh European Conference on Artiﬁcial
Intelligence, Amsterdam, The Netherlands, August 8-12, pages 411–415. John Wiley and
Sons, 1994.
[De Giacomo and Lenzerini, 1996] Giuseppe De Giacomo and Maurizio Lenzerini. TBox and
ABox Reasoning in Expressive Description Logics.
In Luigia Carlucci Aiello, Jon Doyle,
and Stuart C. Shapiro, editors, Proceedings of the Fifth International Conference on Princi-
ples of Knowledge Representation and Reasoning (KR’96), Cambridge, Massachusetts, USA,
November 5-8, 1996, pages 316–327. Morgan Kaufmann, 1996.
[Donini and Massacci, 2000] Francesco M. Donini and Fabio Massacci. ExpTime tableaux for
ALC. Artif. Intell., 124(1):87–138, 2000.
[Donini et al., 1991] Francesco M. Donini, Maurizio Lenzerini, Daniele Nardi, and Werner Nutt.
The Complexity of Concept Languages. In James F. Allen, Richard Fikes, and Erik Sande-
wall, editors, Proceedings of the 2nd International Conference on Principles of Knowledge
Representation and Reasoning (KR’91). Cambridge, MA, USA, April 22-25, 1991, pages
151–162. Morgan Kaufmann, 1991.
[Donini et al., 1998] Francesco M. Donini, Maurizio Lenzerini, Daniele Nardi, Andrea Schaerf,
and Werner Nutt. An epistemic operator for description logics. Artif. Intell., 100(1-2):225–
274, 1998.
[Donini et al., 2002] Francesco M. Donini, Daniele Nardi, and Riccardo Rosati.
Description
logics of minimal knowledge and negation as failure. ACM Trans. Comput. Log., 3(2):177–
225, 2002.
[Eiter et al., 2008] Thomas Eiter, Giovambattista Ianni, Thomas Lukasiewicz, Roman Schind-
lauer, and Hans Tompits. Combining answer set programming with description logics for the
Semantic Web. Artif. Intell., 172:1495–1539, 2008.
[Eiter et al., 2009] Thomas Eiter, Carsten Lutz, Magdalena Ortiz, and Mantas Simkus. Query
Answering in Description Logics with Transitive Roles. In Craig Boutilier, editor, IJCAI 2009,
Proceedings of the 21st International Joint Conference on Artiﬁcial Intelligence, Pasadena,
California, USA, July 11-17, 2009, pages 759–764, 2009.
[Fischer and Ladner, 1979] Michael J. Fischer and Richard E. Ladner. Propositional Dynamic
Logic of Regular Programs. J. Comput. Syst. Sci., 18(2):194–211, 1979.
[Gangemi, 2005] Aldo Gangemi.
Ontology Design Patterns for Semantic Web Content.
In
Yolanda Gil, Enrico Motta, V. Richard Benjamins, and Mark A. Musen, editors, The Seman-
tic Web - ISWC 2005, 4th International Semantic Web Conference, ISWC 2005, Galway,
Ireland, November 6-10, 2005, Proceedings, volume 3729 of Lecture Notes in Computer Sci-
ence, pages 262–276. Springer, 2005.
[Glimm et al., 2008] Birte Glimm, Carsten Lutz, Ian Horrocks, and Ulrike Sattler. Conjunctive
Query Answering for the Description Logic SHIQ. J. Artif. Intell. Res. (JAIR), 31:157–204,
2008.
[Gon¸calves et al., 2013] Rafael S. Gon¸calves, Samantha Bail, Ernesto Jim´enez-Ruiz, Nicolas
Matentzoglu, Bijan Parsia, Birte Glimm, and Yevgeny Kazakov. OWL Reasoner Evaluation
(ORE) Workshop 2013 Results: Short Report. In Samantha Bail, Birte Glimm, Rafael S.
Gon¸calves, Ernesto Jim´enez-Ruiz, Yevgeny Kazakov, Nicolas Matentzoglu, and Bijan Par-
sia, editors, Informal Proceedings of the 2nd International Workshop on OWL Reasoner
Evaluation (ORE-2013), Ulm, Germany, July 22, 2013, volume 1015 of CEUR Workshop
Proceedings, pages 1–18. CEUR-WS.org, 2013.
[Gr¨adel et al., 1997] Erich Gr¨adel, Martin Otto, and Eric Rosen.
Two-Variable Logic with
Counting is Decidable. In Proceedings, 12th Annual IEEE Symposium on Logic in Computer
Science, Warsaw, Poland, June 29 - July 2, 1997, pages 306–317. IEEE Computer Society,
1997.
[Grimm and Hitzler, 2008] Stephan Grimm and Pascal Hitzler. Semantic Matchmaking of Web
Resources with Local Closed-World Reasoning.
International Journal of Electronic Com-
merce, 12(2):89–126, 2008.

674
Matthias Knorr and Pascal Hitzler
[Grosof et al., 2003] Benjamin N. Grosof, Ian Horrocks, Raphael Volz, and Stefan Decker. De-
scription logic programs: combining logic programs with description logic. In Guszt´av Henc-
sey, Bebo White, Yih-Farn Robin Chen, L´aszl´o Kov´acs, and Steve Lawrence, editors, Pro-
ceedings of the Twelfth International World Wide Web Conference, WWW 2003, Budapest,
Hungary, May 20-24, 2003, pages 48–57. ACM, 2003.
[Haarslev and M¨oller, 1999] Volker Haarslev and Ralf M¨oller. RACE System Description. In
Patrick Lambrix, Alexander Borgida, Maurizio Lenzerini, Ralf M¨oller, and Peter F. Patel-
Schneider, editors, Proceedings of the 1999 International Workshop on Description Logics
(DL’99), Link¨oping, Sweden, July 30 - August 1, 1999, volume 22 of CEUR Workshop Pro-
ceedings. CEUR-WS.org, 1999.
[Haarslev et al., 2012] Volker Haarslev, Kay Hidde, Ralf M¨oller, and Michael Wessel. The Rac-
erPro Knowledge Representation and Reasoning System. Semantic Web, 3(3):267–277, 2012.
[Hayes, 1977] Patrick J. Hayes. In Defense of Logic. In R. Reddy, editor, Proceedings of the
5th International Joint Conference on Artiﬁcial Intelligence. Cambridge, MA, August 1977,
pages 559–565. William Kaufmann, 1977.
[Hayes, 1979] Patrick J. Hayes. The logic of frames. In D. Metzing, editor, Frame conceptions
and text understanding, pages 46–61. Walter de Gruyter and Co., 1979.
Republished in
[Brachman and Levesque, 1985].
[Heinsohn et al., 1992] Jochen Heinsohn, Daniel Kudenko, Bernhard Nebel, and Hans-J¨urgen
Proﬁtlich. An Empirical Analysis of Terminological Representation Systems. In William R.
Swartout, editor, Proceedings of the 10th National Conference on Artiﬁcial Intelligence. San
Jose, CA, July 12-16, 1992, pages 767–773. AAAI Press / The MIT Press, 1992.
[Hitzler et al., 2010] Pascal Hitzler, Markus Kr¨otzsch, and Sebastian Rudolph. Foundations of
Semantic Web Technologies. Chapman & Hall/CRC, 2010.
[Hitzler et al., 2012] Pascal Hitzler, Markus Kr¨otzsch, Bijan Parsia, Peter F. Patel-Schneider,
and Sebastian Rudolph,
editors.
OWL 2 Web Ontology Language:
Primer (Sec-
ond
Edition).
W3C
Recommendation
11
December
2012,
2012.
Available
from
http://www.w3.org/TR/owl2-primer/.
[Hollunder et al., 1990] Bernhard Hollunder, Werner Nutt, and Manfred Schmidt-Schauß. Sub-
sumption Algorithms for Concept Description Languages. In 9th European Conference on
Artiﬁcial Intelligence, ECAI’90, Stockholm, Sweden, pages 348–353, 1990.
[Horridge et al., 2008] Matthew Horridge, Bijan Parsia, and Ulrike Sattler. Laconic and Precise
Justiﬁcations in OWL. In Amit P. Sheth, Steﬀen Staab, Mike Dean, Massimo Paolucci, Diana
Maynard, Timothy W. Finin, and Krishnaprasad Thirunarayan, editors, The Semantic Web -
ISWC 2008, 7th International Semantic Web Conference, ISWC 2008, Karlsruhe, Germany,
October 26-30, 2008. Proceedings, volume 5318 of Lecture Notes in Computer Science, pages
323–338. Springer, 2008.
[Horridge et al., 2013] Matthew Horridge, Samantha Bail, Bijan Parsia, and Uli Sattler. Toward
cognitive support for OWL justiﬁcations. Knowl.-Based Syst., 53:66–79, 2013.
[Horrocks and Sattler, 1999] Ian Horrocks and Ulrike Sattler. A Description Logic with Transi-
tive and Inverse Roles and Role Hierarchies. J. Log. Comput., 9(3):385–410, 1999.
[Horrocks et al., 2000] Ian Horrocks, Ulrike Sattler, and Stephan Tobies. Practical Reasoning
for Very Expressive Description Logics. Logic Journal of the IGPL, 8(3):239–263, 2000.
[Horrocks et al., 2004] Ian Horrocks, Peter F. Patel-Schneider, Harold Boley, Said Tabet,
Benjamin Grosof, and Mike Dean.
SWRL: A Semantic Web Rule Language Combin-
ing OWL and RuleML.
W3C Member Submission 21 May 2004, 2004.
Available from
http://www.w3.org/Submission/SWRL/.
[Horrocks et al., 2005] Ian Horrocks, Peter F. Patel-Schneider, Sean Bechhofer, and Dmitry
Tsarkov. OWL Rules: A Proposal and Prototype Implementation. Journal of Web Semantics,
3(1):23–40, 2005.
[Horrocks, 1998] Ian Horrocks. Using an Expressive Description Logic: FaCT or Fiction?
In
Anthony G. Cohn, Lenhart K. Schubert, and Stuart C. Shapiro, editors, Proceedings of the
Sixth International Conference on Principles of Knowledge Representation and Reasoning
(KR’98), Trento, Italy, June 2-5, 1998, pages 636–649. Morgan Kaufmann, 1998.
[Horrocks, 2007] Ian Horrocks. Implementation and Optimization Techniques. In Franz Baader,
Diego Calvanese, Deborah L. McGuinness, Daniele Nardi, and Peter F. Patel-Schneider, ed-
itors, The Description Logic Handbook: Theory, Implementation, and Applications, pages
306–346. Cambridge University Press, 2007.

Description Logics
675
[Hu et al., 2013] Yingjie Hu, Krzysztof Janowicz, David Carral Mart´ınez, Simon Scheider,
Werner Kuhn, Gary Berg-Cross, Pascal Hitzler, Mike Dean, and Dave Kolas. A Geo-ontology
Design Pattern for Semantic Trajectories. In Thora Tenbrink, John G. Stell, Antony Galton,
and Zena Wood, editors, Spatial Information Theory - 11th International Conference, COSIT
2013, Scarborough, UK, September 2-6, 2013. Proceedings, volume 8116 of Lecture Notes in
Computer Science, pages 438–456. Springer, 2013.
[Huang et al., 2005] Zhisheng Huang, Frank van Harmelen, and Annette ten Teije.
Reason-
ing with Inconsistent Ontologies. In Leslie Pack Kaelbling and Alessandro Saﬃotti, editors,
IJCAI-05, Proceedings of the Nineteenth International Joint Conference on Artiﬁcial Intel-
ligence, Edinburgh, Scotland, UK, July 30-August 5, 2005, pages 454–459. Professional Book
Center, 2005.
[Kalyanpur et al., 2005] Aditya Kalyanpur, Bijan Parsia, Evren Sirin, and James A. Hendler.
Debugging unsatisﬁable classes in OWL ontologies. J. Web Sem., 3(4):268–293, 2005.
[Kazakov et al., 2011] Yevgeny Kazakov, Markus Kr¨otzsch, and Frantisek Simancik. Concurrent
Classiﬁcation of EL Ontologies.
In Lora Aroyo, Chris Welty, Harith Alani, Jamie Taylor,
Abraham Bernstein, Lalana Kagal, Natasha Fridman Noy, and Eva Blomqvist, editors, The
Semantic Web - ISWC 2011 - 10th International Semantic Web Conference, Bonn, Germany,
October 23-27, 2011, Proceedings, Part I, volume 7031 of Lecture Notes in Computer Science,
pages 305–320. Springer, 2011.
[Kazakov, 2009] Yevgeny Kazakov. Consequence-Driven Reasoning for Horn SHIQ Ontologies.
In Craig Boutilier, editor, IJCAI 2009, Proceedings of the 21st International Joint Conference
on Artiﬁcial Intelligence, Pasadena, California, USA, July 11-17, 2009, pages 2040–2045,
2009.
[Kharlamov et al., 2013] Evgeny Kharlamov, Martin Giese, Ernesto Jim´enez-Ruiz, Martin G.
Skjæveland, Ahmet Soylu, Dmitriy Zheleznyakov, Timea Bagosi, Marco Console, Peter
Haase, Ian Horrocks, Sarunas Marciuska, Christoph Pinkel, Mariano Rodriguez-Muro, Marco
Ruzzi, Valerio Santarelli, Domenico Fabio Savo, Kunal Sengupta, Michael Schmidt, Evgenij
Thorstensen, Johannes Trame, and Arild Waaler. Optique 1.0: Semantic Access to Big Data:
The Case of Norwegian Petroleum Directorate’s FactPages.
In Eva Blomqvist and Tudor
Groza, editors, Proceedings of the ISWC 2013 Posters & Demonstrations Track, Sydney,
Australia, October 23, 2013, volume 1035 of CEUR Workshop Proceedings, pages 65–68.
CEUR-WS.org, 2013.
[Klinov and Parsia, 2013] Pavel Klinov and Bijan Parsia.
Understanding a Probabilistic De-
scription Logic via Connections to First-Order Logic of Probability.
In Fernando Bobillo,
Paulo Cesar G. da Costa, Claudia d’Amato, Nicola Fanizzi, Kathryn B. Laskey, Kenneth J.
Laskey, Thomas Lukasiewicz, Matthias Nickles, and Michael Pool, editors, Uncertainty Rea-
soning for the Semantic Web II, International Workshops URSW 2008-2010 Held at ISWC
and UniDL 2010 Held at FLoC, Revised Selected Papers, volume 7123 of Lecture Notes in
Computer Science, pages 41–58. Springer, 2013.
[Knorr et al., 2011] M. Knorr, J.J. Alferes, and P. Hitzler. Local Closed-World Reasoning with
Description Logics under the Well-founded Semantics. Artif. Intell., 175(9–10):1528–1554,
2011.
[Knorr et al., 2012] M. Knorr, P. Hitzler, and F. Maier. Reconciling OWL and non-monotonic
rules for the Semantic Web. In Luc De Raedt, Christian Bessi`ere, Didier Dubois, Patrick
Doherty, Paolo Frasconi, Fredrik Heintz, and Peter J. F. Lucas, editors, ECAI 2012, 20th
European Conference on Artiﬁcial Intelligence, 27-31 August 2012, Montpellier, France, vol-
ume 242 of Frontiers in Artiﬁcial Intelligence and Applications, pages 474–479. IOS Press,
Amsterdam, 2012.
[Krisnadhi et al., 2011] Adila Krisnadhi, Frederick Maier, and Pascal Hitzler. OWL and Rules.
In Axel Polleres, Claudia d’Amato, Marcelo Arenas, Siegfried Handschuh, Paula Kroner,
Sascha Ossowski, and Peter F. Patel-Schneider, editors, Reasoning Web. Semantic Technolo-
gies for the Web of Data – 7th International Summer School 2011, Galway, Ireland, August
23-27, 2011, Tutorial Lectures, volume 6848 of Lecture Notes in Computer Science, pages
382–415. Springer, 2011.
[Kr¨otzsch et al., 2008] Markus Kr¨otzsch, Sebastian Rudolph, and Pascal Hitzler. Description
Logic Rules. In Malik Ghallab, Constantine D. Spyropoulos, Nikos Fakotakis, and Nikolaos M.
Avouris, editors, Proceeding of the 18th European Conference on Artiﬁcial Intelligence, Pa-
tras, Greece, July 21-25, 2008, volume 178, pages 80–84, Amsterdam, The Netherlands, 2008.
IOS Press.

676
Matthias Knorr and Pascal Hitzler
[Kr¨otzsch et al., 2011] Markus Kr¨otzsch, Frederick Maier, Adila A. Krisnadhi, and Pascal Hit-
zler. A Better Uncle for OWL: Nominal Schemas for Integrating Rules and Ontologies. In
Sadagopan Srinivasan, Krithi Ramamritham, Arun Kumar, M. P. Ravindra, Elisa Bertino,
and Ravi Kumar, editors, Proceedings of the 20th International World Wide Web Conference,
WWW2011, Hyderabad, India, March/April 2011, pages 645–654. ACM, New York, 2011.
[Kr¨otzsch et al., 2013] M. Kr¨otzsch, S. Rudolph, and P. Hitzler. Complexities of Horn Descrip-
tion Logics. ACM Trans. on Comput. Log., 14(1):2–36, 2013.
[Kr¨otzsch, 2010] Markus Kr¨otzsch. Eﬃcient Inferencing for OWL EL. In Tomi Janhunen and
Ilkka Niemel¨a, editors, Logics in Artiﬁcial Intelligence - 12th European Conference, JELIA
2010, Helsinki, Finland, September 13-15, 2010. Proceedings, volume 6341 of Lecture Notes
in Computer Science, pages 234–246. Springer, 2010.
[K¨usters and Molitor, 2001] Ralf K¨usters and Ralf Molitor. Computing Least Common Sub-
sumers in ALEN. In Bernhard Nebel, editor, Proceedings of the Seventeenth International
Joint Conference on Artiﬁcial Intelligence, IJCAI 2001, Seattle, Washington, USA, August
4-10, 2001, pages 219–224. Morgan Kaufmann, 2001.
[Lehmann, 1992a] Fritz Lehmann. Semantic Networks. Computers Math. Applic., 23(2-5):1–50,
1992.
[Lehmann, 1992b] Fritz Lehmann. Semantic Networks in Artiﬁcial Intelligence. Elsevier Science
Inc., New York, NY, USA, 1992.
[Lukasiewicz and Straccia, 2009] Thomas Lukasiewicz and Umberto Straccia. Description logic
programs under probabilistic uncertainty and fuzzy vagueness. Int. J. Approx. Reasoning,
50(6):837–853, 2009.
[Lutz and Milicic, 2007] Carsten Lutz and Maja Milicic. A Tableau Algorithm for Description
Logics with Concrete Domains and General TBoxes. J. Autom. Reasoning, 38(1–3):227–259,
2007.
[Lutz et al., 2005] Carsten Lutz, Carlos Areces, Ian Horrocks, and Ulrike Sattler. Keys, Nomi-
nals, and Concrete Domains. J. Artif. Intell. Res. (JAIR), 23:667–726, 2005.
[Lutz, 2001] Carsten Lutz. Interval-based Temporal Reasoning with General TBoxes. In Bern-
hard Nebel, editor, Proceedings of the Seventeenth International Joint Conference on Arti-
ﬁcial Intelligence, IJCAI 2001, Seattle, Washington, USA, August 4-10, 2001, pages 89–96.
Morgan Kaufmann, 2001.
[Lutz, 2004] Carsten Lutz. NEXP TIME-complete description logics with concrete domains.
ACM Trans. Comput. Log., 5(4):669–705, 2004.
[MacGregor and Bates, 1987] Robert MacGregor and Raymond Bates. The LOOM knowledge
representation language. Technical Report ISI/RS-87-188, University of Southern California,
Information Science Institute, Marina de Rey (CA, USA), 1987.
[MacGregor, 1991] Robert MacGregor. The evolving technology of classiﬁcation-based knowl-
edge representation systems. In John Sowa, editor, Principles of Semantic Networks, pages
385–400. Morgan Kaufmann, Los Altos, 1991.
[Maier et al., 2013] Frederick Maier, Yue Ma, and Pascal Hitzler.
Paraconsistent OWL and
related logics. Semantic Web, 4(4):395–427, 2013.
[McGuinness and van Harmelen, 2004] Deborah McGuinness and Frank van Harmelen. OWL
Web Ontology Language: Overview. W3C Recommendation 10 February 2004, 2004. Available
from http://www.w3.org/TR/owl-features/.
[Meyer et al., 2006] Thomas Andreas Meyer, Kevin Lee, Richard Booth, and JeﬀZ. Pan. Find-
ing Maximally Satisﬁable Terminologies for the Description Logic ALC. In Proceedings, The
Twenty-First National Conference on Artiﬁcial Intelligence and the Eighteenth Innovative
Applications of Artiﬁcial Intelligence Conference, July 16-20, 2006, Boston, Massachusetts,
USA, pages 269–274. AAAI Press, 2006.
[Minsky, 1981] Marvin Minsky. A framework for representing knowledge. In John Haugeland,
editor, Mind Design:
Philosophy, Psychology, Artiﬁcial Intelligence, pages 95–128. MIT
Press, Cambridge, MA, 1981.
A longer version appeared in The Psychology of Computer
Vision (1975). Republished in [Brachman and Levesque, 1985].
[M¨oller and Haarslev, 2007] Ralf M¨oller and Volker Haarslev. Description Logic Systems. In
Franz Baader, Diego Calvanese, Deborah L. McGuinness, Daniele Nardi, and Peter F. Patel-
Schneider, editors, The Description Logic Handbook: Theory, Implementation, and Applica-
tions, pages 282–305. Cambridge University Press, 2007.
[Mortimer, 1975] Michael Mortimer. On languages with two variables. Zeitschrift f¨ur Mathe-
matische Logik und Grundlagen der Mathematik, 21:135–140, 1975.

Description Logics
677
[Motik and Rosati, 2010] Boris Motik and Riccardo Rosati. Reconciling Description Logics and
Rules. J. ACM, 57(5):1–62, 2010.
[Motik and Sattler, 2006] Boris Motik and Ulrike Sattler. A Comparison of Reasoning Tech-
niques for Querying Large Description Logic ABoxes. In Miki Hermann and Andrei Voronkov,
editors, Logic for Programming, Artiﬁcial Intelligence, and Reasoning, 13th International
Conference, LPAR 2006, Phnom Penh, Cambodia, November 13-17, 2006, Proceedings, vol-
ume 4246 of Lecture Notes in Computer Science, pages 227–241. Springer, 2006.
[Motik et al., 2009] Boris Motik, Rob Shearer, and Ian Horrocks. Hypertableau Reasoning for
Description Logics. J. Artif. Intell. Res. (JAIR), 36:165–228, 2009.
[Motik et al., 2012] Boris Motik, Bernardo Cuenca Grau, Ian Horrocks, Zhe Wu, Achille Fokoue,
and Carsten Lutz, editors.
OWL 2 Web Ontology Language:
Proﬁles (Second Edition).
W3C Recommendation 11 December 2012, 2012. Available at http://www.w3.org/TR/owl2-
proﬁles/.
[Mutharaju et al., 2013] Raghava Mutharaju, Pascal Hitzler, and Prabhaker Mateti. DistEL:
A Distributed EL+ Ontology Classiﬁer.
In Thorsten Liebig and Achille Fokoue, editors,
Proceedings of the 9th International Workshop on Scalable Semantic Web Knowledge Base
Systems, Sydney, Australia, October 21, 2013, volume 1046 of CEUR Workshop Proceedings,
pages 17–32. CEUR-WS.org, 2013.
[Nardi and Brachman, 2007] Daniele Nardi and Ronald J. Brachman. An Introduction to De-
scription Logics. In Franz Baader, Diego Calvanese, Deborah L. McGuinness, Daniele Nardi,
and Peter F. Patel-Schneider, editors, The Description Logic Handbook: Theory, Implemen-
tation, and Applications, pages 1–40. Cambridge University Press, 2007.
[Nebel and von Luck, 1988] Bernhard Nebel and Kai von Luck. Hybrid Reasoning in BACK.
In Proceedings of the 3rd International Symposium on Methodologies for Intelligent Systems
(ISMIS’88), pages 260–269. North-Holland Publ. Co., Amsterdam, 1988.
[Nebel, 1990a] Bernhard Nebel.
Reasoning and Revision in Hybrid Representation Systems,
volume 422 of Lecture Notes in Computer Science. Springer, 1990.
[Nebel, 1990b] Bernhard Nebel. Terminological Reasoning is Inherently Intractable. Artif. In-
tell., 43(2):235–249, 1990.
[Pacholski et al., 1997] Leszek Pacholski, Wieslaw Szwast, and Lidia Tendera. Complexity of
Two-Variable Logic with Counting. In Proceedings, 12th Annual IEEE Symposium on Logic in
Computer Science, Warsaw, Poland, June 29 - July 2, 1997, pages 318–327. IEEE Computer
Society, 1997.
[Palomki and Kangassalo, 2012] Jari Palomki and Hannu Kangassalo.
That IS-IN Isn’t
IS-A:
A
Further
Analysis
of
Taxonomic
Links
in
Conceptual
Modelling.
In
Car-
los Ramirez, editor, Advances in Knowledge Representation. InTech, 2012.
Available
from:
http://www.intechopen.com/books/advances-in-knowledge-representation/that-is-in-
isn-t-is-a-a-further-analysis-of-taxonomic-links-in-conceptual-modelling.
[Papadimitriou, 1994] Christos H. Papadimitriou. Computational complexity. Addison-Wesley,
1994.
[Patel-Schneider, 1999] Peter F. Patel-Schneider. DLP. In Patrick Lambrix, Alexander Borgida,
Maurizio Lenzerini, Ralf M¨oller, and Peter F. Patel-Schneider, editors, Proceedings of the 1999
International Workshop on Description Logics (DL’99), Link¨oping, Sweden, July 30 - August
1, 1999, volume 22 of CEUR Workshop Proceedings. CEUR-WS.org, 1999.
[Peltason, 1991] Christof Peltason. The BACK System - An Overview. SIGART Bull., 2(3):114–
119, 1991.
[Quillian, 1967] M. Ross Quillian.
Word concepts:
A theory and simulation of some basic
capabilities. Behavioral Science, 12:410–430, 1967.
[Rudolph et al., 2008a] Sebastian Rudolph, Markus Kr¨otzsch, and Pascal Hitzler. Description
Logic Reasoning with Decision Diagrams:
Compiling SHIQ to Disjunctive Datalog.
In
Amit P. Sheth, Steﬀen Staab, Mike Dean, Massimo Paolucci, Diana Maynard, Timothy W.
Finin, and Krishnaprasad Thirunarayan, editors, The Semantic Web - ISWC 2008, 7th Inter-
national Semantic Web Conference, ISWC 2008, Karlsruhe, Germany, October 26-30, 2008.
Proceedings, volume 5318 of Lecture Notes in Computer Science, pages 435–450. Springer,
2008.
[Rudolph et al., 2008b] Sebastian Rudolph, Markus Kr¨otzsch, and Pascal Hitzler. Terminologi-
cal Reasoning in SHIQ with Ordered Binary Decision Diagrams. In Dieter Fox and Carla P.
Gomes, editors, Proceedings of the Twenty-Third AAAI Conference on Artiﬁcial Intelligence,
AAAI 2008, Chicago, Illinois, USA, July 13-17, 2008, pages 529–534. AAAI Press, 2008.

678
Matthias Knorr and Pascal Hitzler
[Rudolph et al., 2012] Sebastian Rudolph,
Markus Kr¨otzsch,
and Pascal Hitzler.
Type-
elimination-based reasoning for the description logic SHIQbs using decision diagrams and
disjunctive Datalog. Logical Methods in Computer Science, 8(1), 2012.
[Russell and Norvig, 2010] Stuart J. Russell and Peter Norvig. Artiﬁcial Intelligence - A Modern
Approach (3. internat. ed.). Pearson Education, 2010.
[Schild, 1991] Klaus Schild. A Correspondence Theory for Terminological Logics: Preliminary
Report. In John Mylopoulos and Raymond Reiter, editors, Proceedings of the 12th Inter-
national Joint Conference on Artiﬁcial Intelligence. Sydney, Australia, August 24-30, 1991,
pages 466–471. Morgan Kaufmann, 1991.
[Schild, 1994] Klaus Schild. Terminological Cycles and the Propositional µ-Calculus. In Jon
Doyle, Erik Sandewall, and Pietro Torasso, editors, Proceedings of the 4th International Con-
ference on Principles of Knowledge Representation and Reasoning (KR’94). Bonn, Germany,
May 24-27, 1994, pages 509–520. Morgan Kaufmann, 1994.
[Schlicht and Stuckenschmidt, 2010] Anne Schlicht and Heiner Stuckenschmidt.
Peer-to-Peer
Reasoning for Interlinked Ontologies. Int. J. Semantic Computing, 4(1):27–58, 2010.
[Schlobach et al., 2007] Stefan Schlobach, Zhisheng Huang, Ronald Cornet, and Frank van
Harmelen. Debugging Incoherent Terminologies. J. Autom. Reasoning, 39(3):317–349, 2007.
[Schmidt-Schauß and Smolka, 1991] Manfred Schmidt-Schauß and Gert Smolka.
Attributive
Concept Descriptions with Complements. Artif. Intell., 48(1):1–26, 1991.
[Sengupta et al., 2011] Kunal Sengupta, Adila Alfa Krisnadhi, and Pascal Hitzler. Local Closed
World Semantics: Grounded Circumscription for OWL. In Lora Aroyo, Chris Welty, Harith
Alani, Jamie Taylor, Abraham Bernstein, Lalana Kagal, Natasha Fridman Noy, and Eva
Blomqvist, editors, The Semantic Web – ISWC 2011 – 10th International Semantic Web
Conference, Bonn, Germany, October 23-27, 2011, Proceedings, Part I, volume 7031 of Lec-
ture Notes in Computer Science, pages 617–632. Springer, Heidelberg, 2011.
[Simancik et al., 2011] Frantisek Simancik, Yevgeny Kazakov, and Ian Horrocks. Consequence-
Based Reasoning beyond Horn Ontologies. In Toby Walsh, editor, IJCAI 2011, Proceedings
of the 22nd International Joint Conference on Artiﬁcial Intelligence, Barcelona, Catalonia,
Spain, July 16-22, 2011, pages 1093–1098. IJCAI/AAAI, 2011.
[Sirin et al., 2007] Evren Sirin, Bijan Parsia, Bernardo Cuenca Grau, Aditya Kalyanpur, and
Yarden Katz. Pellet: A practical OWL-DL reasoner. Web Semantics, 5:51–53, 2007.
[Steigmiller et al., 2013] Andreas Steigmiller, Birte Glimm, and Thorsten Liebig.
Nominal
Schema Absorption.
In Francesca Rossi, editor, IJCAI 2013, Proceedings of the 23rd In-
ternational Joint Conference on Artiﬁcial Intelligence, Beijing, China, August 3-9, 2013.
IJCAI/AAAI, 2013.
[Straccia, 2001] Umberto Straccia. Reasoning within Fuzzy Description Logics. J. Artif. Intell.
Res. (JAIR), 14:137–166, 2001.
[Sturm and Wolter, 2002] Holger Sturm and Frank Wolter. A Tableau Calculus for Temporal
Description Logic: the Expanding Domain Case. J. Log. Comput., 12(5):809–838, 2002.
[Tsarkov and Horrocks, 2006] Dmitry Tsarkov and Ian Horrocks.
FaCT++ description logic
reasoner: System description. In Ulrich Furbach and Natarajan Shankar, editors, In Proceed-
ings of the International Joint Conference on Automated Reasoning (IJCAR 2006), pages
292–297. Springer, 2006.
[Tsarkov et al., 2004] Dmitry Tsarkov, Alexandre Riazanov, Sean Bechhofer, and Ian Horrocks.
Using Vampire to Reason with OWL. In Sheila A. McIlraith, Dimitris Plexousakis, and Frank
van Harmelen, editors, The Semantic Web – ISWC 2004: Third International Semantic Web
Conference,Hiroshima, Japan, November 7-11, 2004. Proceedings, volume 3298 of Lecture
Notes in Computer Science, pages 471–485. Springer, 2004.
[Woods, 1975] William A. Woods. What’s in a link: Foundations for semantic networks. In
Daniel G. Bobrow and Allan M. Collins, editors, Representation and Understanding: Studies
in Cognitive Science, pages 35–82. Academic Press, 1975. Republished in [Brachman and
Levesque, 1985].

LOGICS FOR THE SEMANTIC WEB
Pascal Hitzler, Jens Lehmann, Axel Polleres
Reader: Patrick Hayes
1
INTRODUCTION
A major international research eﬀort is currently under way to improve the exist-
ing World Wide Web (WWW), with the intention to create what is often called
the Semantic Web [Berners-Lee et al., 2001; Hitzler et al., 2010]. Driven by the
World Wide Web Consortium (W3C) and its director Sir Tim Berners-Lee (inven-
tor of the WWW), and heavily funded by many national and international research
funding agencies, Semantic Web has become an established ﬁeld of research. It
integrates methods and expertise from many subﬁelds of Computer Science and
Artiﬁcial Intelligence [Studer, 2006], and it has now reached suﬃcient maturity for
ﬁrst industrial scale applications [Hamby, 2012; Hermann, 2010]. Correspondingly,
major IT companies are starting to roll out applications involving Semantic Web
technologies; these include Apple’s Siri, IBM’s Watson system, Google’s Knowedge
Graph, Facebook’s Open Graph Protocol, and schema.org as a collaboration be-
tween major search engine providers including Microsoft, Google, and Yahoo!.
The Semantic Web ﬁeld is driven by the vision to develop powerful methods
and technologies for the reuse and integration of information on the Web. While
current information on the Web is mainly made for human consumption, it shall in
the future be made available for automated processing by intelligent systems. This
vision is based on the idea of describing the meaning—or semantics—of data on
the Web using metadata—data that describes other data—in the form of so-called
ontologies [Hitzler et al., 2010]. Ontologies are essentially knowledge bases rep-
resented using logic-based knowledge representation languages. This shall enable
access to implicit knowledge through logical deduction [Hitzler and van Harme-
len, 2010], and its use for search, integration, browsing, organization, and reuse of
information.
Of course, the idea of adopting knowledge representation languages raises the
question which of the many approaches discussed in the literature should be
adopted and promoted to Web standards (oﬃcially called W3C Recommenda-
tions). In this chapter, we give an overview of the most important present stan-
dards as well as their origins and history.
The idea that the World Wide Web shall have capabilities to convey information
for processing by intelligent systems, and not only by humans, has already been
Handbook of the History of Logic. Volume 9: Computational Logic. 
Volume editor: Jörg Siekmann 
Series editors: Dov M. Gabbay and John Woods 
Copyright © 2014 Elsevier BV. All rights reserved.

680
Pascal Hitzler, Jens Lehmann, Axel Polleres
part of its original design [Berners-Lee, 1996]. The World Wide Web was initiated
in 1990, and immediately showed exponential growth [Berners-Lee, 1996]. In the
meantime, it has become a very signiﬁcant technological infrastructure of modern
society.
In the 1990s, the Semantic Web vision1 was mainly driven by the W3C Meta-
data Activity [W3C Metadata, revision of 23 August 2002] which produced the
ﬁrst version of the Resource Description Framework (RDF) which we will discuss
in Section 2. The Semantic Web Activity [W3C Semantic Web, revision of 19
June 2013] replaced the Metadata Activity in 2001, and has installed several stan-
dards for representing knowledge on the Web, most noteably two revisions of RDF,
the Web Ontology Language (OWL) discussed in Section 3, and the Rule Inter-
change Format (RIF) discussed in Section 4. In Section 5, we discuss some of the
particular challenges which must be faced when adopting logic-based knowledge
representation languages for the Semantic Web, and in Section 6 we discuss some
of the more recent research developments and questions. Note that we give a more
detailed technical account for RDF than for OWL and RIF, because the latter
are closely related to description logics and logic programming, respectively, and
the reader is refered to the corresponding chapters in this volume for additional
background and introductions.
2
RDF AND RDF SCHEMA
The Resource Description Framework (RDF) [Manola et al., 2004; Hayes, 2004]
comprises a simple data format as well as a basic schema language, called RDF
Schema [Brickley and Guha, 2004]. While historically often termed a “medadata”
standard, that is, an exchange format for data about documents and resources,
in the meantime, RDF has been well established as a universal data exchange
format for classical data integration scenarios, and particularly for publishing and
exchanging structured data on the Web [Polleres et al., 2011].
Informally, all RDF data can be understood as a set of subject–predicate–object
triples, where all subjects and predicates are Uniform Resource Identiﬁers (URIs)2
[Berners-Lee et al., 2005], and in the object position both URIs and literal values
(such as numbers, strings, etc.) are allowed. Such a simple, triple based format
was chosen since on the one hand, it can accomodate for any kind of metadata
in the form of predicate-value pairs, and on the other hand, any more complex
relational or object-oriented data can be decomposed into such triples in a fairly
straightforward manner [Berners-Lee, 2006].3
1The term Semantic Web became popular in the aftermath of the widely cited popular science
article [Berners-Lee et al., 2001]. We were able to trace the term Semantic, in relation to Web,
back to a 1994 presentation by Tim Berners-Lee [Berners-Lee, 1994].
2URIs are a generalization of URLs.
3See also the discussion of semantic networks in the chapter on description logics, in this
volume.

Logics for the Semantic Web
681
dc:creator
<DesignIssues/TimBook-old/History.html>
<People/Berners-Lee/card#i>
http://www.w3.org
Figure 1. An RDF triple
Last, but not least, since URIs are being used as constant symbols in the lan-
guage of RDF, any RDF triple may likewise be viewed as a generalization of
a “link” on the Web; as opposed to plain links on the traditional Web, on the
Semantic Web, links can be associated with an arbitrary binary relation, which
again is represented by a URI. We note that this idea of “typed” links was already
part of Tim-Berners Lee’s original design ideas of the Web [Berners-Lee, 1993;
Berners-Lee and Fischetti, 1999]. For instance, on the website of the W3C (http:
//www.w3.org, if one wants to state that the page with the URI http://www.w3.
org/DesignIssues/TimBook-old/History.html was created by Tim Berners-
Lee, this fact may be viewed as such a typed link, and consequently as an RDF
subject-predicate-object triple, as shown in Fig. 1. Here, the URI dc:creator is
used to denote the has-creator relation between a resource and its creator. This
common view of typed links as labeled edges linking between resources also leads
to sets of RDF triples often being called “RDF graphs.”
A distinguished relation within RDF, represented by the URI rdf:type is the
is-a relation, that allows to denote membership to a certain class, where classes are
again represented by URIs, for instance the class foaf:Person. Another important
feature of RDF is that so called “blank nodes” can be used in the subject or object
positions of triples to denote unnamed or unknown resources. This allows to model
incomplete information in the form of existentials. For instance, the RDF graph
in Fig. 2 extends the information in Fig. 1 by the fact that Tim Berners-Lee is
a Person, is named “Timothy Berners-Lee” and knows some person named “Dan
Brickley”.
In the following, after giving a brief history of the RDF standard (Section 2.1),
we will present the RDF Data model along with a short discussion of diﬀerent
syntactic representations and the semantics of RDF (Section 2.2). We continue
with a discussion of RDF Schema in Section 2.3 and the query language SPARQL
in Section 2.4.

682
Pascal Hitzler, Jens Lehmann, Axel Polleres
rdf:type
dc:creator
<DesignIssues/TimBook-old/History.html>
<People/Berners-Lee/card#i>
http://www.w3.org
foaf:knows
foaf:name
“Dan Brickley”
foaf:name
“Timothy Berners-Lee”
foaf:Person
rdf:type
Figure 2. An RDF graph with a “blank” (anonymous) node
2.1
Brief History of RDF
The standardisation of RDF has been preceded by two earlier proposals for meta-
data standards in the form of W3C member submissions, namely (i) the Chan-
nel Deﬁnition Format [Ellerman, 1997] and (ii) the Meta Content Framework
(MCF) [Guha and Bray, 1997].
While the former comprised an XML format
with a ﬁxed term of metadata properties for describing information channels on
the Web, (somewhat similar to RSS nowadays), the latter (MCF) was strictly
extensible and evolved into the ﬁrst version of RDF [Lassila and Swick, 1999],
published in 1999.
Another important metadata initiative from the digital libraries community,
Dublin Core [Nilsson et al., 2008], which started around the same time but outside
of W3C, later on adopted RDF as a representation syntax, becoming one of the
most prominent RDF vocabularies, see Section 2.3 below.
The ﬁrst oﬃcial standard recommendation of RDF from 1999 was extended in
2004 by a formal deﬁnition of the Semantics of RDF [Hayes, 2004], decoupling the
syntactical representation in XML from the RDF data model.
Since then RDF has been used in various contexts and experienced wide adop-
tion. In 2009 the W3C held a workshop on future directions of RDF [Herman,
2009], discussing several extensions but also simpliﬁcations of the standard. These
extensions are currently under discussion in the ongoing W3C RDF 1.1 working
group.4
4See http://www.w3.org/2011/rdf-wg/.

Logics for the Semantic Web
683
2.2
Diﬀerent Syntactic Representations and Semantics
There are various serialisations for RDF. Fig. 3 shows diﬀerent syntactic represen-
tations of the six triples in the RDF graph from Fig. 2 in some of these serialisation
syntaxes: N-Triples [Beckett and McBride, 2004a], cf. Fig. 3(a) is a simple line-
based format that serializes one RDF triple per line terminating each line triple
with a full-stop ‘.’, enclosing URIs in angle brackets, and literals in quotes; blank
nodes are given alphanumeric identiﬁers (also called blank node label, preceded by
by the preﬁx ‘ :’. Turtle [Beckett and Berners-Lee, 2011], shown in Fig. 3(b) ex-
tends the simple N-Triples format by shortcuts making the language more legible,
such as namespace preﬁx and base URI declarations, similar to XML, for abbrevi-
ating URIs, as well as the possiblity to separate predicate-object groups for triples
with the same subject by semicolon, etc. The original RDF/XML [Beckett and
McBride, 2004b] syntax was an XML format, that encoded predicates as XML
elements, with some abbreviations, such as rdf:type triples that refer to class
membership of a node can be also directly encoded as XML elements, an example
of which is given in Fig. 3(c).
Other serialization syntaxes for RDF, which do not detail herein, include the
RDFa [Herman et al., 2013], which provides means to syntactically embed RDF
directly as markup into (X)HTML documents. We note that RDFa is particularly
similar – and in fact intertranslatable – to other metadata markup formats in
HTML such as the increasingly popular microdata format [Hickson, 2012] (which
is actively promoted by schema.org).
The fundamental diﬀerence between RDF and general XML is that the intuition
of the RDF data model is that the syntactic representation, and also the order of
triples is irrelevant, which intuitively implies a notion of equivalence between RDF
graphs that is independent of the serialization. While this RDF data model was
not formally described in the 1999 version of RDF [Lassila and Swick, 1999],
formal deﬁnitions were introduced in the speciﬁcation of 2004, where a formal
model-theoretic semantics was deﬁned [Hayes, 2004], and later reﬁned in the recent
RDF1.1 speciﬁcation [Hayes and Patel-Schneider, 2014]. For the exposition of this
formal semantics in this chapter, we will stick with a notation similar to the
one introduced in [Guti´errez et al., 2004] rather than quoting the original W3C
speciﬁcation verbatim.
DEFINITION 1. The set of RDF terms U ∪L∪B consists of elements from three
inﬁnite disjoint sets U (RDF URI references), L (RDF Literals), and B = {bj :
j ∈N} (RDF blank nodes).
Note that for the exposition herein we restrict literals to plain string literals,
in general RDF also oﬀers language tagged literals, as well as so called “typed”
literals, that is, pairs (l, d) where l ∈L is a string, and d is either a string lan-
guage tag [Phillips and Davis, 2006], or, respectively, d ∈D is a URI represent-
ing a datatype (such as e.g. http://www.w3.org/2001/XMLSchema#decimal), see
also [Biron and Malhotra, 2004].

684
Pascal Hitzler, Jens Lehmann, Axel Polleres
<http://www.w3.org/DesignIssues/TimBook-old/History.html>
<http://purl.org/dc/elements/1.1/creator>
<http://www.w3.org/People/Berners-Lee/card#i> .
<http://www.w3.org/People/Berners-Lee/card#i>
<http://xmlns.com/foaf/0.1/name>
"Timothy Berners-Lee" .
<http://www.w3.org/People/Berners-Lee/card#i>
<http://www.w3.org/1999/02/22-rdf-syntax-ns#type>
<http://xmlns.com/foaf/0.1/Person> .
<http://www.w3.org/People/Berners-Lee/card#i>
<http://xmlns.com/foaf/0.1/knows>
_:b1 .
_:b1
<http://xmlns.com/foaf/0.1/name>
"Dan Brickley" .
_:b1
<http://www.w3.org/1999/02/22-rdf-syntax-ns#type>
<http://xmlns.com/foaf/0.1/Person> .
(a)
@base <http://www.w3.org/> .
@prefix dc: <http://purl.org/dc/elements/1.1/> .
@prefix foaf: <http://xmlns.com/foaf/0.1/>.
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix rdfs: <http://http://www.w3.org/2000/01/rdf-schema#> .
<DesignIssues/TimBook-old/History.html> dc:creator <People/Berners-Lee/card#i> .
<People/Berners-Lee/card#i> foaf:name "Timothy Berners-Lee";
rdf:type foaf:Person ;
foaf:knows [ foaf:name "Dan Brickley" ;
rdf:type foaf:Person
] .
(b)
<?xml version="1.0" encoding="utf-8"?>
<rdf:RDF xmlns:dc="http://purl.org/dc/elements/1.1/"
xmlns:foaf="http://xmlns.com/foaf/0.1/"
xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#">
<rdf:Description rdf:about="http://www.w3.org/DesignIssues/TimBook-old/History.html">
<dc:creator rdf:resource="http://www.w3.org/People/Berners-Lee/card#i"/>
</rdf:Description>
<foaf:Person rdf:about="http://www.w3.org/People/Berners-Lee/card#i">
<foaf:name>Timothy Berners-Lee</foaf:name>
<foaf:knows>
<foaf:Person>
<foaf:name>Dan Brickley</foaf:name>
</foaf:Person>
</foaf:knows>
</foaf:Person>
</rdf:RDF>
(c)
Figure 3. An RDF Graph in N-Triples, Turtle, and RDF/XML syntax

Logics for the Semantic Web
685
∃x
triple(history.html, creator, card#i)∧
triple(card#i, name, "Timothy Berners-Lee"∧
triple(card#i, type, Person)∧
triple(card#i, knows, x)∧
triple(x, name, "Dan Brickley"∧
triple(x, type, Person)
(a)
∃x
creator(history.html, card#i)∧
name(card#i, "Timothy Berners-Lee"∧
Person(card#i)∧
knows(card#i, x)∧
name(x, "Dan Brickley"∧
Person(x)
(b)
∃x
history.html[creator →card#i]∧
card#i[name →"Timothy Berners-Lee"]∧
card#i : Person∧
card#i[knows →x]∧
x[name →"Dan Brickley"]∧
x : Person
(c)
Figure 4. The RDF Graph from Fig. 2 in ﬁrst-order logic using ternary encoding
with an auxiliary predicate triple, unary/binary encoding, and F-Logic-style
frames.

686
Pascal Hitzler, Jens Lehmann, Axel Polleres
DEFINITION 2. A triple (s, p, o) ∈(U ∪B) × U × (U ∪B ∪L) is called an RDF
triple, where s is called the subject, p the predicate and o the object.
An RDF graph (or, just graph) is a set of RDF triples. A (proper) subgraph is
a (proper) subset of a graph. The universe of a graph G, universe(G), is the set
of elements of U ∪B ∪L that occur in the triples of G. The vocabulary VG of a
graph G is the set universe(G) ∩(U ∪L).
Finally a triple, or graph, respectively, is called ground, if it does not contain
any blank nodes.
The intention of blank nodes in RDF suggests that graphs that only diﬀer in
the identiﬁers used for blank nodes in a concrete syntactical representation should
be considered equivalent. Likewise, a graph G1 that can be turned into a subgraph
G2 of by just renaming blank nodes in G1 to RDF terms from the universe of G2,
does not carry more information than G2 and should thus be considered “entailed”
by G2. This intention is reﬂected in the formal model-theoretic semantics of RDF
that is – in principle – based on the usual idea of ﬁrst-order interpretations, with
the caveat that elements of U both reﬂect binary relations and constants at the
same time. This leads to a somewhat non-standard deﬁnition of interpretations in
RDF.
DEFINITION 3. (from [Hayes, 2004, Section 1], slightly simpliﬁed.5) A simple
RDF interpreretation I = ⟨∆, ∆p, IEXT , IT erms, LV ⟩over an RDF vocabulary V
consists of
• a non-empty domain ∆, i.e.
the set of resources, which contains the set
LV = L ∩V .
• a non-empty set of properties ∆P , not necessariliy disjoint with ∆
• a function IEXT : ∆P →∆× ∆, which maps properties to binary relations
• a function IUL : UV ∪LV →∆, where IUL(l) = l for l ∈LV (that is, literals
are interpreted as themselves)
Finally, satisfaction of an RDF triple, or graph, respectively, under an interpre-
tation I is deﬁned as follows.
DEFINITION 4. An interpretation I satisﬁes a ground triple triple t = (s, p, o),
written I |= t if s, p, o ∈V , IT erms(p) ∈∆P and (IT erms(s), IT erms(o)) ∈
IEXT (IT erms(p)). Accordingly, a ground RDF graph G is satisﬁed under I, writ-
ten I |= G if and only if I |= t for all t ∈G. Finally, a non ground graph G′ is
satisﬁed under I if there exists an extension [IUL + A] of IUL by an assignment
A : B →∆, such that
([IT erms + A](s), [IT erms + A](o)) ∈IEXT (IT erms(p))
5As mentioned above, as opposed to [Hayes, 2004] we do not consider typed literals nor
language tagged literals here, but only plain string literals.

Logics for the Semantic Web
687
for all t ∈G.
We note that when looking at Deﬁnition 4 that the semantics of blank nodes
corresponds exactly to that of existential variables in ﬁrst-order logic.
Simple entailment between (sets of) RDF graphs is then deﬁned following usual
terminology.
DEFINITION 5. Given a set S of RDF graphs (simply) entails a graph G, written
S |= G, if every interpretation which satisﬁes every member of S also satisﬁes G.
Given the intention outlined above, entailment should also be expressible in
terms of blank node mappings.
DEFINITION 6. Here, A map is a function µ : U ∪B ∪L →UBL preserving
URIs and literals, i.e., µ(u) = u and µ(l) = l for all u ∈U and l ∈L.
Using such maps, indeed the notion of entailment between two RDF graphs can
be deﬁned via the so-called interpolation lemma from [Hayes, 2004, Section 2],
rather than in a model-theoretic way.
LEMMA 7 (Interpolation Lemma). Let G1, G2 be RDF graphs, then G1 |= G2 if
a subgraph of G1 is an instance of G2, that is, if there exists a map µ, such that
µ(G2) is a subgraph of G1.
Given G1, G2, deciding whether there exists such a map, boils down to graph
homomorphism, which is well known to be an NP-complete problem [Garey and
Johnson, 1979], and therefore also NP-completeness of simple RDF entailment
follows. Fragments of RDF where entailment is tractable include obviously ground
graphs, but also graphs where blank nodes are not used in a cyclic fashion across
triples [Pichler et al., 2008]
Obviously, due to this existential semantics of blank nodes there could be inner
redundancy in an RDF graph, that is, if there is a homomorphism of G to itself.
This redundancy is called non-leanness in RDF terminology.
DEFINITION 8. A graph G is lean if there is no map µ such that µ(G) is a proper
subgraph of G.
Unfortunately, as a consequence of the NP-completeness of simple entailment,
deciding leanness is also intractable, namely coNP-complete [Guti´errez et al.,
2004].
As a side note, let us note that it has often been critizized by practitioners
that the existential treatment of blank nodes, which leads to this high complexity,
puts an unnecessary burden on RDF users and implementers, and moreover is not
consistently followed in neighbouring standards that build on top of RDF [Mallea
et al., 2011].
Relation of RDF to other Logical Formalisms
Another way to show NP-completeness of RDF simple entailment is that RDF
entailment can straightforwardly be encoded into entailment of ﬁrst-order-logic

688
Pascal Hitzler, Jens Lehmann, Axel Polleres
formulae with existentials and conjunction only, which is well known to be just
another formulation of conjunctive query containment [Chandra and Merlin, 1977],
as shown in the following theorem, which is implicit in [de Bruijn and Heymans,
2007].
THEOREM 9. Given RDF graphs G1 and G2, we have that G1 |= G2 if and only
if T (G1) |=F OL T (G2) where a ﬁrst order theory T (G) is obtained from a graph
G as follows
T (G) = ∃x∈VG∩Bx
^
(s,p,o)∈G
triple(s, p, o)
An example for this encoding into ﬁrst-order logic is shown in Fig.4(a); Another
common way to encode RDF into ﬁrst-order logic is using unary predicates for
triples modeling an is-a relationship, i.e. rdf:type triples, and binary predicates
for all other properties, cf. Fig. 4(b). We note though that this representation is
of somewhat limited use to encode arbitrary RDF graphs, since for instance blank
nodes in the object positions of rdf:type triples, which is perfectly ﬁne in the
general setting of RDF, would result in a second-order formula.
Note that, translation to a ﬁrst-order setting in [de Bruijn and Heymans, 2007]
uses F-Logic [Kifer et al., 1995] instead of classical ﬁrst-order logic, which may
be considered as syntactic sugar; a respective encoding of RDF triples in F-Logic
frame syntax is shown in 4(c).
2.3
RDF Schema (RDFS)
The generic semantics deﬁned by simple RDF interpretations is restricted to inter-
pretations that give a special meaning to the RDF and RDFS vocabulary, that is,
for URIs in the rdf: (http://www.w3.org/1999/02/22-rdf-syntax-ns#) and
rdfs: (http://www.w3.org/2000/01/rdf-schema#) namespaces, cf. the respec-
tive preﬁx declarations in Fig 3(b) for the full URIs. This special semantics allows
to express simple ontologies, in the form of (i) deﬁning subclass and subproperty
hierarchies, and (ii) deﬁning domain and range restrictions of properties.
The RDFS semtantics restricts interpretations as per Def. 3 above such that
(i) a set of axiomatic triples, cf. [Hayes, 2004, Sections 3.1 and 4.1] are true in
any RDFS interpretation, and (ii) a set of entailment rules holds, that aﬀect how
rdfs:subClassOf, rdfs:subPropertyOf, rdfs:domain, rdfs:range, etc. triples
are interpreted. Figure 5 shows some of the RDFS axiomatic triples. Further,
with the encoding of RDF Graphs into ﬁrst-order logic from Fig. 4(a) in mind, the
RDFS entailment rules can to a large extent be approximated by the ﬁrst-order
rules shown in Table 1 (from [Eiter et al., 2008b]). The fact that these rules are
simple Horn rules and that RDF is encodable as a set of facts makes reasoning in
RDFS thus amenable to simple (Datalog) rule engines. Mu˜noz et al. [Mu˜noz et al.,
2007] have argued for a simpler set of entailment rules that leaves out inferences
that might be considered redundant for many applications of RDF and RDFS, for
instance leaving out axiomatic triples or rules like the ﬁrst seven rules in Table 1.

Logics for the Semantic Web
689
rdf:type rdf:type rdf:Property .
rdf:type rdfs:domain rdfs:Resource .
rdfs:domain rdfs:domain rdf:Property .
rdfs:range rdfs:domain rdf:Property .
rdfs:subPropertyOf rdfs:domain rdf:Property .
...
Figure 5. Some of the axiomatic triples that are true under the RDFS semantics.
Table 1. RDFS entailment rules, written as ﬁrst-order Horn rules
∀S, P, O (triple(S, P, O) ⊃triple(S, rdf:type, rdfs:Resource))
∀S, P, O (triple(S, P, O) ⊃triple(P, rdf:type, rdf:Property))
∀S, P, O (triple(S, P, O) ⊃triple(O, rdf:type, rdfs:Resource))
∀C (triple(C, rdf:type, rdfs:Class) ⊃triple(C, rdfs:subClassOf, rdfs:Resource))
∀S, C (triple(S, rdf:type, C) ⊃triple(C, rdf:type, rdfs:Class))
∀C (triple(C, rdf:type, rdfs:Class) ⊃triple(C, rdfs:subClassOf, C))
∀P (triple(P, rdf:type, rdf:Property) ⊃triple(P, rdfs:subPropertyOf, P ))
∀S, P, O (triple(S, P, O) ∧triple(P, rdfs:domain, C) ⊃triple(S, rdf:type, C))
∀S, P, O, C (triple(S, P, O) ∧triple(P, rdfs:range, C) ⊃triple(O, rdf:type, C))
∀C1, C2, C3 (triple(C1, rdfs:subClassOf, C2) ∧
triple(C2, rdfs:subClassOf, C3) ⊃triple(C1, rdfs:subClassOf, C3))
∀S, C1, C2 (triple(S, rdf:type, C1) ∧triple(C1, rdfs:subClassOf, C2) ⊃triple(S, rdf:type, C2))
∀P1, P2, P3 (triple(P1, rdfs:subPropertyOf, P2) ∧
triple(P2, rdfs:subPropertyOf, P3) ⊃triple(P1, rdfs:subPropertyOf, P3))
∀S, P1, P2, O (triple(S, P1, O) ∧triple(P1, rdfs:subPropertyOf, P2) ⊃triple(S, P2, O))
By giving a special semantics to the rdfs: vocabulary, RDF Schema enables
the meta-description of other RDF vocabularies with the goal that additional
triples will be entailed under the RDFS semantics. As such RDFS can be consid-
ered a simple ontology language, that is used within popular Web ontologies such
as the Friend-of-a-friend (foaf:) or the Dublin Core (dc:) vocabulary descrip-
tions [Brickley and Miller, 2007; Nilsson et al., 2008]. Fig. 6 shows a snippet of
the FOAF ontology, along with some additional triples that can be derived from
this RDFS ontology together with the RDF graph from Fig. 3.
Relation of RDFS to other Logical Formalisms
A mapping from RDF interpretations to ﬁrst-order logics is given in [Franconi
et al., 2005]. This picture is completed in [de Bruijn and Heymans, 2007], em-
bedding RDF(S) within the framework of F-Logic [Kifer et al., 1995], and also
covering the extensional semantics of RDFS [Hayes, 2004, Section 4.2]; addi-
tional considerations regarding special semantics of datatype literals (cf. [Hayes,

690
Pascal Hitzler, Jens Lehmann, Axel Polleres
foaf:Person rdfs:subClassOf foaf:Agent .
foaf:knows rdfs:domain foaf:Person .
foaf:knows rdfs:range foaf:Person .
foaf:name rdfs:subPropertyOf rdfs:label .
...
(a)
<People/Berners-Lee/card#i> rdf:type foaf:Agent .
<People/Berners-Lee/card#i> rdfs:label "Timothy Berners-Lee" .
_:b1
rdf:type foaf:Agent .
_:b1
rdfs:label "Dan Brickley" .
(b)
Figure 6.
(a) Some RDFS statements from the FOAF vocabulary description,
plus (b) some additional triples inferrable from these statements together with the
RDF graph from Fig. 3
2004, Section 5]) are covered in [de Bruijn and Heymans, 2010].
Another re-
cent paper by Franconi et al. [Franconi et al., 2013] discusses the logic of the
extensional RDF semantics, which is only a non-normative part of the RDF spec-
iﬁcation, in more detail; remarkably, the authors come to the conclusion that
the extensional RDFS semantics can likewise be implemented by a set of infer-
ence rules, where the closure is computable in polynomial time in a forward-
chaining manner, thus contradicting the conjecture in the original RDF spec-
iﬁcation that the extensional semantics would “require more complex inference
rules” [Hayes, 2004, Section 4.2] The core semantics of RDFS, that is reasoning
about rdfs:subClassOf, rdfs:subPropertyOf, rdfs:domain, and rdfs:range
has also been associated in the literature with a minimalistic fragment of the
Description Locigs family DL-Lite [Calvanese et al., 2007; Poggi et al., 2008;
Franconi et al., 2013]; a respective mapping from RDFS stratements to DL-Lite is
shown in Table 2. We note though that only restricted RDF graphs, that do not
use the RDF and RDFS vocabulary in a “non-standard” [de Bruijn and Heymans,
2007] fashion (e.g., using rdfs:subclass in an object position, or – as already
mentioned above – blank nodes in the object position of rdf:type triples) are
amenable to such en embedding into DL-Lite.
2.4
SPARQL
In order to facilitate queries over RDF and RDFS, the W3C has deﬁned a standard
query language, SPARQL [Harris and Seaborne, 2013], which at its core facilitates
conjunctive queries over RDF graphs. Such conjunctive queries are called Basic
Graph Patterns (BGPs) in SPARQL and syntactically expressed as RDF graphs
with (‘?’-preﬁxed) variables allowed in subject, predicate or object positions of

Logics for the Semantic Web
691
Table 2. DL-Lite axioms vs. RDF(S)
DL-Lite
RDFS
A1 ⊑A2
A1 rdfs:subClassOf A2.
∃P ⊑A
P rdfs:domain A.
∃P −⊑A
P rdfs:range A.
P1 ⊑P2
P1 rdfs:subPropertyOf P2.
A(x)
x rdf:type A.
P(x, y)
x P y.
SELECT ?X WHERE { ?X rdf:type foaf:Person .
?X foaf:knows ?Y .
?Y foaf:name "Dan Brickley"
}
answer(x) →
triple(x, type, Person) ∧triple(x, knows, y) ∧triple(y, name,′′ Dan Brickley′′)
Figure 7. A simple SPARQL query asking for persons who know someone named
“Dan Brickley” and its corresponding conjunctive query in ﬁrst-order syntax
triples to facilitate joins and projection (using the keyword ’SELECT’). Figure 7
shows a simple SPARQL query and its corresponding transcription into a conjunc-
tive query.
SPARQL allows more complex patterns on top of BGPs, such as unions of
patterns, optional query patterns and filters, where the expressivity of the
SPARQL in its version 1.0 [Prud’hommeaux, 2008] language was shown to cap-
ture Relational Algebra, or non-recursive Datalog with negation, respectively, by
Angles and Gutierrez [Angles and Gutierrez, 2008].
We note that the recent SPARQL 1.1 [Harris and Seaborne, 2013] speciﬁcation
has additional expressivity e.g. by allowing aggregates plus a basic form of regu-
lar path queries, which can no longer be captured in non-recursive Datalog with
negation alone [Polleres and Wallner, 2013]. Moreover, SPARQL in its version
1.0 was solely deﬁned in terms of RDF simple entailment, SPARQL 1.1 deﬁnes
which additional answers a SPARQL query should return under RDFS and OWL
semantics (see Section 3 below).
3
DAML/OIL AND OWL
3.1
A Brief History
While the previously described RDF and RDFS languages already allow to model
domain knowledge, they are not very expressive and often insuﬃcient for capturing
the necessary relationships and constraints. Therefore, the development of richer
representations was an early goal in the Semantic Web initiative, which eventually

692
Pascal Hitzler, Jens Lehmann, Axel Polleres
led to the OWL ontology language.
One of the main predecessors of OWL are frame based systems. While the no-
tion of frames was previously introducted in diﬀerent contexts, e.g. [Minsky, 1975],
a major development were the structured inheritance networks developed at the
end of the 70s in [Brachman, 1978]. In those systems, the core modelling struc-
tures are frames – now more commonly referred to as classes in object oriented
programming languages and ontology languages. Frames usually had speciﬁc at-
tributes (also called properties) describing them. This diﬀers from the previously
described RDFS language and OWL itself in which properties are autonomous
entities. Using domain and range axioms, RDFS properties can be used to model
frame-like structures. Another consequence of properties being autonomous enti-
ties, is that their usage in RDFS and OWL is not restricted to instances of a single
class/frame.
While early frame based systems lacked formal semantics, this deﬁcit was over-
come by the introduction of description logics (DLs).
The ﬁrst DL-based KR
system is KL-ONE [Brachman and Schmolze, 1985]. We refrain from describing
those in detail as they are already covered in the chapter on description logics in
this volume. In contrast to some frame-based systems, DLs have a clear focus on
logic based semantics and reasoning, which are now considered essential for an
ontology language [Baader, 2003]. DLs later became the formal foundation of the
OWL ontology language and enjoy an increase in popularity and usage. OWL
and the underlying DLs go far beyond early frame based systems and RDF, e.g. it
supports speciﬁc characteristics like functionality and transitivity for properties as
well as complex class expressions.
A major turning point after the gradual introduction of some key technologies
like frames and description logics more than 30 years ago was the rise of the World
Wide Web in the 90s. Web technologies, e.g. XML, had a major inﬂuence on OWL.
After the introduction of a ﬁrst RDF recommendation in 1999, standardisation
eﬀorts on the introduction of an ontological layer in the Semantic Web intensiﬁed
while a new version of RDF was developed in parallel. Ultimately, this resulted in
the Web Ontology Language OWL becoming an oﬃcial W3C recommendation in
2004, which was published together with the revised RDF W3C recommendation.
The predecessors of OWL are DAML, OIL and to a lesser extent SHOE. SHOE6 is
an extension of HTML, which was developed around 1996 and makes it possible to
annotate web documents with machine-readable information. While the project
is no longer active, it inﬂuenced the development of the DAML+OIL language.
DAML (DARPA Agent Markup Language) was a funding program in the US,
which started in 1999 and involved James Hendler and Tim Bernes-Lee.
The
program pursued the development of machine readable knowledge representation
languages for the web.
A main result of the DAML program was the DAML
language, which was already based on RDF. In parallel to the development of
DAML, the aim of OIL (Ontology Inference Layer) was to provide an infrastructure
for the Semantic Web [Fensel et al., 2001]. The authors of [Fensel et al., 2001]
6http://www.cs.umd.edu/projects/plus/SHOE/

Logics for the Semantic Web
693
state that RDFS served as starting point for OIL and they developed it into
a ”full-ﬂedged Web-based ontology language” including formal semantics. OIL
development started at the end of the 90s. Finally, in December 2000 the ﬁrst
version of the language DAML+OIL [McGuinness et al., 2002] was released. While
using DAML as a foundation, this language focused on the inclusion of the clear
semantics underlying OIL. It also used the expressive power of OIL, speciﬁcally
the SHIQ description logic [Horrocks et al., 2003]. DAML+OIL was subsequently
used as starting point for the W3C Web Ontology Working Group.
In 2004,
this working group produced the W3C recommendation OWL – Web Ontology
Language [Web Ontology Working Group, 10 February 2004]. Since then OWL
served as a backbone for knowledge representation on the web and several inference
algorithms for it, in particular for OWL DL, were developed and implemented.
As the people involved in the development of DAML, OIL and OWL often had
diﬀerent technological backgrounds, ideas and expertise than those working on the
RDF speciﬁcations, joining those two strands was a tense and diﬃcult process.
This is one of the reasons why diﬀerent OWL dialects were created with varying
compatibility with RDF.
In 2009, after several years of reﬁnements, OWL 2 became a W3C recommen-
dation [OWL Working Group, 27 October 2009] (note that the 2012 version of the
recommendation document [Hitzler et al., 2012] contains only very minor changes,
most of them editorial). OWL 2 was started as an incremental improvement of
OWL, but during the development of the language, it turned out that in sum
the required changes and addressed deﬁciencies are substantial: ”None of these
problems are severe, but, taken together, they indicate a need for a revision of
OWL 1” [Grau et al., 2008]. From a knowledge representation perspective, OWL
2 mainly builds on SROIQ(D), whereas OWL 1 mainly used SHOIN(D) – see
the description logic chapter in this volume for details.
3.2
Quick Introduction to OWL
Based on the introduction of description logics in the corresponding chapter in
this volume, we will now describe the Web Ontology Language OWL. For brevity,
we focus on the OWL DL dialect. In essence, OWL DL is based on description
logics extended by several features to make it suitable as a web ontology language,
e.g. using URIs/IRIs as identiﬁers, imports of other ontologies and annotations
of URIs and axioms. By basing OWL DL on description logics, it can make use
of the theory developed for DLs, in particular sophisticated reasoning algorithms.
In OWL, diﬀerent naming conventions are usually used compared to description
logics. OWL classes correspond to concepts in description logics and properties
correspond to roles.
While being based on description logics, OWL is also seen as a language ex-
tending RDF in the Semantic Web layer cake. However, the semantics of RDF
diﬀers from that of description logics and does in general not necessarily lead to
the same logical consequences. Due to being based on RDF and DLs, there are

694
Pascal Hitzler, Jens Lehmann, Axel Polleres
OWL expression /
OWL 2
DL syntax
Manchester syntax
axiom
Thing
⊤
Thing
Nothing
⊥
Nothing
intersectionOf
C1 ⊓. . . ⊓Cn
C1 and . . . and Cn
unionOf
C1 ⊔. . . ⊔Cn
C1 or . . . or Cn
complementOf
¬C
not C
oneOf
{x1} ⊔. . . ⊔{xn}
{x1, . . . , xn}
allValuesFrom
∀r.C
r only C
someValuesFrom
∃r.C
r some C
maxCardinality
(✓)
≤n r C
r max n
minCardinality
(✓)
≥n r C
r min n
cardinality
(✓)
≤n r C ⊓≥n r C
r exact n C
hasSelf
✓
∃s.Self
r Self
subClassOf
C1 ⊑C2
C1 SubClassOf: C2
equivalentClass
C1 ≡C2
C1 EquivalentTo: C2
disjointWith
C1 ≡¬C2
C1 DisjointWith: C2
sameAs
{x1} ≡{x2}
x1 SameAs: x2
diﬀerentFrom
{x1} ⊑¬{x2}
x1 DiﬀerentFrom: x2
domain
∀r.⊤⊑C
r Domain: C
range
⊤⊑∀r.C
r Range: C
subPropertyOf
r1 ⊑r2
r1 SubPropertyOf: r2
equivalentProperty
r1 ≡r2
r1 EquivalentTo: r2
inverseOf
r1 ≡r−
2
r1 InverseOf: r2
TransitiveProperty
r+ ⊑r
r Characteristics: Transitive
FunctionalProperty
⊤⊑≤1 r
r Characteristics: Functional
ReﬂexiveProperty
✓
Ref(r)
r Characteristics: Reﬂexive
propertyChainAxiom
✓
r1 ◦. . . ◦rn ⊑s
s SubPropertyChain:
r1 o . . . o rn
Table 3.
OWL constructs in DL and Manchester OWL syntax (excerpt).
✓
indicates that the construct is only available in OWL 2 and (✓) indicates that it
was extended in OWL 2.
two diﬀerent deﬁnitions of formal semantics: Direct Semantics, which are based
on DLs, and RDF-based Semantics.
In general, OWL oﬀers more convenience constructs than the corresponding de-
scription logics, but does not extend its expressivity. For instance, the domain and
range constructs inherited from RDF are logically redundant, i.e. can be expressed
using other constructs, but are part of the language, since they simplify modelling
for knowledge engineers.
It should be noted that OWL does not make the unique name assumption,
so diﬀerent individuals can be mapped to the same domain element. It allows
us to express equality and inequality between individuals (a = b, a ̸= b) using
owl:sameAs and owl:differentFrom. Most algorithms for description logics al-
ready supported this distinction before the OWL speciﬁcation was created. Not

Logics for the Semantic Web
695
making the unique names assumption is crucial in the Semantic Web, where it
is often the case that many knowledge bases contain information about the same
entity. In this case, a common approach is that each knowledge base uses their
own URI namespace and owl:sameAs is used to connect individuals.
Table 3 shows, for some examples, how constructs in OWL can be mapped
to description logics.
We can see that some features can be mapped directly
to description logics, e.g. union, and others are syntactic sugar, e.g. functional
properties. OWL has diﬀerent syntactic formats, in which a knowledge base can
be stored. Since it can be converted to RDF, formats like RDF/XML or Turtle
can be used. There is also a special XML syntax called OWL/XML as well as
the Manchester OWL Syntax. For details on Manchester OWL syntax (e.g. used
in the Protg editor) see [Horridge and Patel-Schneider, 2008] and the OWL 2
Manchester Syntax Speciﬁcation [OWL Working Group, 11 December 2012]. The
latter is popular in ontology editors.
However, the RDF-based syntax plays a
special role since its support is required for tools to be compliant with the OWL
standard. Examples for Manchester OWL Syntax are shown on the right column
in Table 3. Note that OWL 2 also supports the creation of datatypes as discussed
in detail in [Motik and Horrocks, 2008], but a discussion of them is omitted for
brevity in this section.
3.3
Relations to other Formalisms
OWL 1 comes in three ﬂavors: OWL Lite, OWL DL, and OWL Full. For OWL
1, OWL Lite corresponds to the description logic SHIF(D) and OWL DL to
the description logic SHOIN(D). OWL Full contains features not expressible in
description logics, but needed to be compatible with RDFS. In this sense, OWL
Full can be seen as the union of RDFS and OWL DL in terms of language features
or, alternatively, as OWL without syntactic restrictions.
The latest version OWL 2 is again split in two ﬂavors OWL 2 DL and OWL 2
Full. OWL 2 DL corresponds roughly to the logic SROIQ(D). An exception for
this are the so called keys, which essentially state that certain property values and
class memberships are suﬃcient to uniquely identify an individual. This language
feature is derived from relational database technology and cannot be expressed in
DLs. A further interesting feature of OWL 2 DL is meta-modelling via punning,
which allows to use the same URI for an entity denoting a class and an individual.
Internally, those are then semantically treated as separate entities.
As in OWL, there is also an OWL 2 Full variant introduced for RDFS com-
patibility.
In addition, three proﬁles were introduced: EL, QL, and RL. Each
proﬁle imposes, usually syntactical, restrictions on OWL in order to allow for
more eﬃcient reasoning. OWL 2 EL is aimed at applications which require ex-
pressive property modelling and is based on the logic EL++, which guarantees
polynomial reasoning time wrt. ontology size for all standard inference problems.
QL is targeted at applications with massive volumes of instance data.
In QL,
query answering can be implemented on top of conventional relational database

696
Pascal Hitzler, Jens Lehmann, Axel Polleres
systems and sound and complete conjunctive query answering methods can be im-
plemented in LOGSPACE. As in the EL proﬁle, the standard inference problems
run in polynomial time. RL is aimed at scalable applications, which however, do
not want to sacriﬁce too much expressive power. Reasoning algorithms for it can
be implemented in rule-based engines and run in polynomial time. The EL and
QL languages are subsets of OWL 2 DL, whereas RL provides two variants where
one is subset of OWL 2 Full and the other one is a subset of OWL 2 DL.
Compared to RDFS, OWL Full is much more expressive by allowing the con-
struction of complex concepts via boolean connectors (conjunctions, disjunction,
negation) as well as cardinality restrictions (minimum, maximum and exact car-
dinality). Furthermore, it includes several other features such as class disjointness
and more ﬁne-granular property modelling. Properties can be declared to be re-
ﬂexive, functional, symmetric or equivalent to other properties.
Due to those
characteristics, OWL is usually seen as a full-ﬂedged ontology language, whereas
RDFS is more suitable for lightweight vocabularies. However, while RDFS allows
reiﬁcation as a method for adding contextual information (see discussion below in
Section 6), this is not allowed in OWL DL and generally not supported in standard
description logics.
More details on the OWL language and its formal foundations can be found
in the [Hitzler et al., 2010]. For technical details, we refer to the W3C recom-
mendations, in particular those for OWL 2 [OWL Working Group, 27 October
2009].
4
RULES
Rules come in many guises.
In one of their most basic forms, they consist of
statements of the form
^
i
Ai →B,
where B (the head of the rule) and all Ai (which form the body of the rule) are
atomic formulas from ﬁrst-order predicate logic, and all variables in the rule are
considered universally quantiﬁed.7 Function symbols may or may not be allowed.
Additional logical connectives may be allowed, e.g. disjunctions or existential quan-
tiﬁers in the rule head. Constructs from other, e.g. modal or non-monotonic, logics
may be allowed. Atomic formulas in head or body may be replaced by procedural
built-ins or other executable commands. If a formal semantics is deﬁned for a
rules language, it may range from a full ﬁrst-order predicate logic semantics to an
entirely procedural speciﬁcation. Logic programming, as discussed in the chapter
by Robert Kowalski in this volume, is one rather prominent example of such a
rules language.
Many rules paradigms had already been well established in research and in-
dustry, when the World Wide Web Consortium set out to deﬁne a recommended
7The universal quantiﬁer is usually omitted.

Logics for the Semantic Web
697
standard for modeling ontologies for the Semantic Web. Rules, in particular in
the broad sense of logic programming, were a very strong contender for the base
paradigms on which to base this recommended standard. As discussed in Sec-
tion 3, description logics were eventually chosen, but a signiﬁcant research and
industrial interest remained in giving rules a more prominent role, and in the af-
termath of the 2004 W3C OWL speciﬁcation [Web Ontology Working Group, 10
February 2004], the ensuing discussions on the role of rules for the Semantic Web
were sometimes rather ﬁerce [Horrocks et al., 2005a; Kifer et al., 2005].
One of the prominently discussed paradigms was F-Logic [Kifer et al., 1995], in
its variant as a primarily syntactic extension of logic programming [Angele and
Lausen, 2004].
This included industrial strength systems [Angele, 2014], W3C
member submissions [de Bruijn et al., 2005], research investigations (e.g., [Fried-
land et al., 2004; Roman et al., 2005]), and industrial applications (e.g., [Angele
and Gesmann, 2007; Angele et al., 2008]). Central to F-Logic is its use of a frame-
based syntax.
RuleML,8 the Rule Markup Initiative, is another long-standing eﬀort which
aims at developing the Rule Markup Language RuleML “as the canonical Web
language for rules using XML markup, formal semantics, and eﬃcient implemen-
tations” (cited from http://ruleml.org). RuleML is set to encompass the entire
rule spectrum.
The Semantic Web Rules Language SWRL [Horrocks et al., 2004; Horrocks et
al., 2005b] has been presented in the aftermath of the W3C OWL speciﬁcation, as
an early eﬀort to accomodate rules modeling in a way compatible with the descrip-
tion logic paradigm. In its original formulation, SWRL simply added rules with a
ﬁrst-order predicate logic semantics to OWL, but reasoning systems and research
discussions soon converged towards reading SWRL rules in a more restricted way,
known as DL-safety, which was more akin to the Herbrand semantics usually con-
sidered in logic programming, was more readily implementable, and retained a
key design rationale of description logics, namely decidability [Motik et al., 2005;
Krisnadhi et al., 2011]. SWRL became rather prominent in the wake of OWL, and
the notion of DL-safety provided a key notion towards subsequent research into
the integration of description logics and rules – see Section 6 for pointers to more
recent developments on this issue.
The Rule Interchange Format RIF [Kifer and Boley, 2013] was ﬁnally estab-
lished as a W3C recommended standard for exchanging rules between rule sys-
tems. It draws, in part, on both F-Logic and RuleML.9 In particular, it sports a
frame-based syntax inspired by F-Logic and draws from RuleML for its normative
XML-based syntax. Set up as an exchange language, rather than as a full-blown
knowledge representation language, RIF has several dialects as well as an exten-
sible framework. Of the dialects, RIF Core [Boley et al., 2013] corresponds to
Datalog, RIF BLD (the Basic Logic Dialect) [Boley and Kifer, 2013a] corresponds
to Horn logic, and RIF PRD (the Production Rule Dialect) [de Sainte Marie et
8http://ruleml.org
9See [Kifer, 2008] and section 3.4 of http://www.w3.org/2005/rules/wiki/RIF FAQ.

698
Pascal Hitzler, Jens Lehmann, Axel Polleres
al., 2013] captures main aspects of production rule systems [Klahr et al., 1987]
which incorporate ad-hoc computational mechanisms (such as side-actions trig-
gered by rule execusions, e.g. printing a document). Each of these dialects, even the
strongly logic-based ones RIF Core and RIF BLD, sport some use of datatypes and
built-ins. RIF FLD (the Framework for Logic Dialects) [Boley and Kifer, 2013b;
Kifer, 2008] provides a means to deﬁne further RIF dialects.
Finally, as discussed above, we note that the RDFS semantics is expressible in
terms of Horn rules. On top of that, the W3C has deﬁned a combined seman-
tics for combining arbitrary Horn rules encoded in RIF [Boley and Kifer, 2013a]
with RDF(S) and OWL [de Bruijn, 2013]. In the academic literature there have
been several other proposals to extend RDF by rules beyond Horn rules, such as
ERDF [Analyti et al., 2008] which provides a syntax for normal logic programs
(that is, rules with default negation interpreted under the stable model seman-
tics [Gelfond and Lifschitz, 1988]), or N3 [Berners-Lee et al., 2008] which also
allows default negation in rule bodies; although N3’s semantics is only deﬁned
informally, its reference implementation CWM10 implements the perfect model
semantics [Przymusinski, 1988]).11
5
PARTICULAR CHALLENGES TO USING LOGIC-BASED KNOWLEDGE
REPRESENTATION ON THE WEB
The use of logic-based knowledge representation and reasoning at the scale of the
World Wide Web poses a number of particular challenges which have so far not
received primary attention in logic research. We list some of them in the following.
The added value of a good machine-processable syntax for knowledge represen-
tation formalisms is easily underestimated. However, it is a fundamental basis for
knowledge exchange and integration which needs to be approached carefully in or-
der to obtain a widest possible agreement between stakeholders. The World Wide
Web Consortium has gone a long way in establishing recommended standards for
knowledge representation for the Semantic Web, in particular through their work
on RDF [Lassila and Swick, 2004; Cyganiak and Wood, 2013], OWL [Smith et al.,
2004; Hitzler et al., 2012], and RIF [Boley et al., 2013; Boley and Kifer, 2013a],
but also by establishing special-purpose shared vocabularies based on these, e.g.
SKOS Simple Knowledge Organization System [Miles and Bechhofer, 2009], SSN
Semantic Sensor Networks [Compton et al., 2012], provenance [Groth and Moreau,
2010].
Investigating the scalability of automated reasoning approaches is, of course,
an established line of research in computer science. However, dealing with Web
scale data lifts this issue to yet another level. Shared memory parallelization of
reasoning is highly eﬀective [Kazakov et al., 2011], however it breaks down if data
size exceeds capacities. Massive distributed memory parallelization has started to
10http://www.w3.org/2000/10/swap/doc/cwm.html
11According to personal communication with Dan Connolly.

Logics for the Semantic Web
699
be investigated [Mutharaju et al., 2013; Schlicht and Stuckenschmidt, 2010; Urbani
et al., 2011; Urbani et al., 2012; Weaver and Hendler, 2009; Zhou et al., 2012], but
there is as yet insuﬃcient data for casting a verdict if distributed memory reasoning
will be able to meet this challenge. Some authors even call for the investigation of
non-deductive methods, e.g. borrowed from machine learning or data mining, as
a partial replacement for deductive approaches [Hitzler and van Harmelen, 2010].
Automated reasoning applications usually rely on clean, single-purpose, and
usually manually created or curated knowledge bases.
In a Web setting, how-
ever, it would often be an unrealistic assumption that such input would be avail-
able, or would be of suﬃciently small volume to make manual curation a fea-
sible approach. In some cases, this problem may be reduced by crowdsourcing
data curation [Acosta et al., 2013]. Nevertheless, on the Web we should expect
high-volume or high-throughput input which at the same time is multi-authored,
multi-purposed, context-dependent, contains errors and omissions, and so forth
[Hitzler and van Harmelen, 2010; Janowicz and Hitzler, 2012]. The aspects just
mentioned are often refered to as the volume (size of input data), velocity (speed
of data generation) and variety aspects of data, in fact these three V’s are usually
discussed within the much larger Big Data context, within which many Semantic
Web challenges can be located [Hitzler and Janowicz, 2013].
To give just one example of variety which is particularly challenging in a Se-
mantic Web context, consider basic geographical notions such as forest, river, or
village, which depend heavily on social agreement and tradition, and are further-
more often inﬂuenced by economic or political incentives [Janowicz and Hitzler,
2012]. This type of variety is often refered to as semantic heterogeneity, and it
cannot be overcome by simply enforcing a single deﬁnition: In fact, the diﬀerent
perspectives are often incompatible and would result in logical inconsistencies if
combined. Research on the quesion how to deal with semantic heterogeneity may,
of course, be more a question of pragmatics than of formal logic, yet the body
of literature dealing with this issue is still too small to conﬁdently locate ma-
jor promising approaches. Formal logical approaches which have been proposed
as partial solutions include fuzzy or probabilistic logics [Klinov and Parsia, 2008;
Lukasiewicz and Straccia, 2009; Straccia, 2001], paraconsistent reasoning [Maier et
al., 2013], and the use of defaults or other non-monotonic logics related to reason-
ing with knowledge and belief [Baader and Hollunder, 1995; Bonatti et al., 2009;
Donini et al., 2002; Eiter et al., 2008a; Knorr et al., 2011; Motik and Rosati, 2010;
Sengupta et al., 2011], but the larger issue remains unresolved. Others have ad-
vocated the use of ontology design patterns for meeting the challenge of semantic
heterogeneity [Gangemi, 2005; Janowicz and Hitzler, 2012], but it is currently not
clear how far this will carry.
There exist a multitude of diﬀerent knowledge representation paradigms based
on diﬀerent and often incompatible design principles. Logical features which ap-
pear useful for modeling such as uncertainty handling or autoepistemic introspec-
tion are often studied in isolation, while the high-variety setting of the Semantic
Web would suggest that combinations of features need to be taken into account in

700
Pascal Hitzler, Jens Lehmann, Axel Polleres
realistic settings. However, merging diﬀerent knowledge representation paradigms
often results in unwieldy, highly complex logics for which strong automated reason-
ing support may be diﬃcult to obtain [de Bruijn et al., 2011; de Bruijn et al., 2010;
Knorr et al., 2012]. Even W3C recommended standards, which on purpose are
designed to be of limited variety, expose this issue. The OWL 2 DL proﬁle is
essentially a traditional description logic, but if serialised in RDF (as required by
the standard), the RDF formal semantics is not equivalent to the OWL formal
semantics, and the sets of logical consequences deﬁned by these two formal se-
mantics for an OWL ﬁle (serialised in RDF) are not contained within each other.
The OWL 2 Full proﬁle was established to encompass all of both OWL 2 DL and
RDF Schema, but we are not aware of any practical use of its formal semantics.
Concerning the relationship between OWL and RIF, in contrast, the gap seems to
be closing now, as discussed in Section 6 below.
Another practical obstacle to the use of formal logic and reasoning on the Se-
mantic Web is the availability of strong and intuitive tools and interfaces, of in-
dustrial strength, which would relieve application developers from the burden of
becoming an expert in formal logic and Semantic Web technologies. Of course,
many useful tools are available, e.g.
[Lehmann, 2009; Calvanese et al., 2011;
David et al., 2011; Horridge and Bechhofer, 2011; Tudorache et al., 2013], and
some of them are of excellent quality, but a signiﬁcant gap remains to meet prac-
tical requirements.
6
RECENT DEVELOPMENTS
Concerning more recent developments and investigations concerning the use of
logic-based knowledge representation for the Semantic Web, it appears to make
sense to distinguish between theoretical advances and advances concerning dis-
semination into practice and applications.
On the theory side, a convergence of diﬀerent paradigms is currently happen-
ing, in particular with respect to the description-logic-based and the rule-based
paradigm. The following are some of the most prominent recent developments.
• The introduction of role chains and some other constructs in OWL 2 [Hitzler
et al., 2012] has made a signiﬁcant step towards closing the gap between Horn
logic and description logics, by making many more rules expressible in major
description logics [Kr¨otzsch et al., 2008; Kr¨otzsch, 2010]. Another recently
introduced syntax construct, called nominal schemas [Carral Mart´ınez et al.,
2012; Carral et al., 2013], further enables the expression of many monotonic
rules [Krisnadhi et al., 2011], up to a complete coverage of n-ary Datalog
under the Herbrand semantics [Knorr et al., 2012].
Research concerning
algorithmization and reasoning tool support are under way [Kr¨otzsch et al.,
2011; Steigmiller et al., 2013].
• Datalog has recently seen a revival due to results investigating the theoretical
and practical implications of adding existentially quantiﬁed variables to rule

Logics for the Semantic Web
701
heads. The general approach is known as existential rules and was most
prominently introduced under the name Datalog+−[Cal`ı et al., 2012]. The
paradigm on the one hand generalizes Datalog, and on the other hand is very
akin in spirit to the so-called EL family of description logics which entered
the mainstream with the introduction of the tractable12 description logic
EL++ [Baader et al., 2005] and its applications [Baader et al., 2006] – a line
of work which eventually led to the OWL 2 proﬁle known as OWL EL [Motik
et al., 27 October 2009]. Currently, existential rules are under investigation
from many diﬀerent angles and by researchers of diﬀerent backgrounds.
• Rule paradigms, and in particular logic programming in its many variants,
have long been investigated from the perspective of non-monotonic logics, in
particular in order to deal with types of commonsense reasoning related to
defaults. Similar investgations have been pursued in recent years in order
to extend description logics with such non-monotonic features, resulting in
a signiﬁcant body of work (e.g., [Baader and Hollunder, 1995; Bonatti et
al., 2009; Donini et al., 2002; Eiter et al., 2008a; Giordano et al., 2013;
Grimm and Hitzler, 2008; Huang et al., 2013; Knorr et al., 2011; Lukasiewicz
and Straccia, 2009; Motik and Rosati, 2010; Sengupta et al., 2011]) which
seems to converge towards a unifying perspective [de Bruijn et al., 2010;
Knorr et al., 2012].
The situation concerning the dissemination of logic-based methods to Web prac-
tice is much less clear, partly because these types of investigations are mainly
industry-driven and thus often not well documented in the research literature.
RDF-based reasoning is often being used, as existing RDF triple stores often pro-
vide the required support. The use of ontologies in expert-system-like applica-
tions is also rather common, in particular in industrial settings where input data
is more controlled or curateable. So-called ontology-based data access (OBDA)
[Calvanese et al., 2011], which rests on the idea of using shared ontologies as
access layers for databases, is currently under heavy research investigation and
will likely stay so for some time. For use of deep KR on the open Web major
challenges remain [Noy and McGuinness, 2013; Hitzler and van Harmelen, 2010;
Jain et al., 2010] which range from logical issues, e.g. how to deal with noisy data,
to pragmatic issues, e.g. the development of modeling best practices and strong
tool support.
Adding contextual information to RDF(S), OWL and SPARQL
A particularly relevant topic that has appeared in various disguises over the years
is the lack of means to express contextual information alongside RDF(S)& OWL
Various approaches have been proposed in the literature to extend RDF(S), OWL
12To be precise, the worst-case computational complexity of computing the class hierarchy of
all named classes is polynomial with respect to time.

702
Pascal Hitzler, Jens Lehmann, Axel Polleres
and SPARQL by contextual information such as temporal information associ-
ated to RDF triples [Gutierrez et al., 2007] and OWL statements [Motik, 2012],
fuzzy annotations [Vanekov´a et al., 2005; Mazzieri and Dragoni, 2005; Straccia,
2009], or deﬁning more general forms of “Annotated RDF” [Udrea et al., 2010;
Zimmermann et al., 2012]. Note that the importance of adding such contextual
meta-information to RDF data at the level of triples is also a topic within the
ongoing developments within RDF1.1 [Herman, 2009], where not only the se-
mantics of such extensions, but also their syntactic representation within RDF
is under discussion; while the basic RDF vocabulary oﬀers the possibility to add
meta-descriptions through so-called reiﬁcation, supported by special vocabulary
terms (rdfs:Statement, rdfs:subject, rdfs:predicate, rdfs:object), these
vocabulary terms are not given any special semantics and are often perceived as
cumbersome in practice. Alternative proposals for expresssing contextualy infor-
mation include so-called named graphs [Carroll et al., 2005], i.e., using again URIs
to identify RDF graphs themselves, which then allows to use those identiﬁers in
RDF triples to add contextual meta-information to the triples in these graphs;
diﬀerent syntax proposals for named graphs include N3 [Berners-Lee et al., 2008],
TRIG/TRIX [Carothers et al., 2013; Carroll et al., 2005], and N-Quads [Carothers,
2013].
ACKNOWLEDGEMENTS
Pascal Hitzler acknowledges support by the National Science Foundation under
award 1017225 “III: Small: TROn – Tractable Reasoning with Ontologies.” Any
opinions, ﬁndings, and conclusions or recommendations expressed in this material
are those of the author(s) and do not necessarily reﬂect the views of the National
Science Foundation.
Jens Lehmann acknowledges support by grants from the
European Union’s 7th Framework Programme provided for the projects GeoKnow
(GA no. 318159) and LOD2 (GA no. 257943).
BIBLIOGRAPHY
[Acosta et al., 2013] Maribel Acosta, Amrapali Zaveri, Elena Simperl, Dimitris Kontokostas,
S¨oren Auer, and Jens Lehmann.
Crowdsourcing linked data quality assessment.
In 12th
International Semantic Web Conference, 21-25 October 2013, Sydney, Australia, 2013.
[Analyti et al., 2008] Anastasia Analyti, Grigoris Antoniou, Carlos Viegas Dam´asio, and Gerd
Wagner. Extended RDF as a semantic foundation of rule markup languages. J. Artif. Intell.
Res. (JAIR), 32:37–94, 2008.
[Angele and Gesmann, 2007] J¨urgen Angele and Michael Gesmann.
Integration of customer
information using the semantic web. In Jorge Cardoso, Martin Hepp, and Miltiadis D. Lytras,
editors, The Semantic Web: Real-World Applications from Industry, volume 6 of Semantic
Web And Beyond Computing for Human Experience, pages 191–208. Springer, 2007.
[Angele and Lausen, 2004] J¨urgen Angele and Georg Lausen. Ontologies in F-Logic. In Stef-
fen Staab and Rudi Studer, editors, Handbook on Ontologies, International Handbooks on
Information Systems, pages 29–50. Springer, 2004.
[Angele et al., 2008] J¨urgen Angele, Michael Erdmann, and Dirk Wenke. Ontology-based knowl-
edge management in automotive engineering scenarios. In Martin Hepp, Pieter De Leenheer,

Logics for the Semantic Web
703
Aldo de Moor, and York Sure, editors, Ontology Management, Semantic Web, Semantic Web
Services, and Business Applications, volume 7 of Semantic Web And Beyond Computing for
Human Experience, pages 245–264. Springer, 2008.
[Angele, 2014] J¨urgen Angele. OntoBroker – mature and approved semantic middleware. Se-
mantic Web journal, 2014. To appear.
[Angles and Gutierrez, 2008] Renzo Angles and Claudio Gutierrez.
The expressive power of
SPARQL. In Amit P. Sheth, Steﬀen Staab, Mike Dean, Massimo Paolucci, Diana Maynard,
Timothy W. Finin, and Krishnaprasad Thirunarayan, editors, International Semantic Web
Conference (ISWC 2008), volume 5318 of LNCS, pages 114–129, Karlsruhe, Germany, 2008.
Springer.
[Baader and Hollunder, 1995] F. Baader and B. Hollunder. Embedding Defaults into Termino-
logical Representation Systems. J. Automated Reasoning, 14:149–180, 1995.
[Baader et al., 2005] Franz Baader, Sebastian Brandt, and Carsten Lutz. Pushing the EL en-
velope. In Proc. 19th Int. Joint Conf. on Artiﬁcial Intelligence (IJCAI-05), Edinburgh, UK,
2005. Morgan-Kaufmann Publishers.
[Baader et al., 2006] F. Baader, C. Lutz, and B. Suntisrivaraporn. CEL—A Polynomial-time
Reasoner for Life Science Ontologies. In U. Furbach and N. Shankar, editors, Proceedings of
the 3rd International Joint Conference on Automated Reasoning (IJCAR’06), Seattle, WA,
USA, August 17-20, 2006, volume 4130 of Lecture Notes in Artiﬁcial Intelligence, pages
287–291. Springer-Verlag, 2006.
[Baader, 2003] Franz Baader.
The description logic handbook: theory, implementation, and
applications. Cambridge University Press, 2003.
[Beckett and Berners-Lee, 2011] David Beckett and Tim Berners-Lee.
Turtle – Terse RDF
Triple Language. W3C Team Submission, March 2011. http://www.w3.org/TeamSubmission/
turtle/.
[Beckett and McBride, 2004a] David Beckett and Brian McBride. RDF Test Cases. W3C Rec-
ommendation, February 2004. http://www.w3.org/TR/rdf-testcases/.
[Beckett and McBride, 2004b] David Beckett and Brian McBride.
RDF/XML Syntax Spec-
iﬁcation (Revised).
W3C Recommendation,
February 2004.
http://www.w3.org/TR/
rdf-syntax-grammar/.
[Berners-Lee and Fischetti, 1999] Tim Berners-Lee and Marc Fischetti. Weaving the Web: The
Original Design and Ultimate Destiny of the World Wide Web by its Inventor. Harper, 1999.
[Berners-Lee et al., 2001] Tim Berners-Lee, James Hendler, and Ora Lassila.
The Semantic
Web. Scientiﬁc American, 284(5):34–43, May 2001.
[Berners-Lee et al., 2005] T. Berners-Lee, R. Fielding, and L. Masinter. Uniform resource iden-
tiﬁer (uri):
Generic syntax.
Technical Report RFC 3986, IETF, January 2005.
http:
//www.ietf.org/rfc/rfc3986.txt.
[Berners-Lee et al., 2008] Tim Berners-Lee, Dan Connolly, Lalana Kagal, Yosi Scharf, and Jim
Hendler. N3logic: A logical framework for the world wide web. TPLP, 8(3):249–269, 2008.
[Berners-Lee, 1993] Tim Berners-Lee. A Brief History of the Web. W3C Design Issues, 1993.
From http://www.w3.org/DesignIssues/TimBook-old/History.html; retr. 2013/09/24.
[Berners-Lee, 1994] Tim
Berners-Lee.
Plenary
at
WWW
Geneva
94.
http://www.w3.org/Talks/WWW94Tim/, 1994.
[Berners-Lee, 1996] Tim Berners-Lee.
The World Wide Web:
Past, present and future.
http://www.w3.org/People/Berners-Lee/1996/ppf.html, August 1996.
[Berners-Lee, 2006] Tim Berners-Lee.
Linked Data.
W3C Design Issues, July 2006.
From
http://www.w3.org/DesignIssues/LinkedData.html; retr. 2013/09/24.
[Biron and Malhotra, 2004] Paul V. Biron and Ashok Malhotra.
XML Schema part 2:
Datatypes second edition. W3C Recommendation, October 2004. http://www.w3.org/TR/
xmlschema-2/.
[Boley and Kifer, 2013a] Harold Boley and Michael Kifer, editors.
RIF Basic Logic Di-
alect (Second Edition).
W3C Recommendation 5 February 2013, 2013.
Available from
http://www.w3.org/TR/rif-bld/.
[Boley and Kifer, 2013b] Harold Boley and Michael Kifer, editors. RIF Framework for Logic
Dialects (Second Edition). W3C Recommendation 5 February 2013, 2013. Available from
http://www.w3.org/TR/rif-ﬂd/.
[Boley et al., 2013] Harold Boley, Gary Hallmark, Michael Kifer, Adrian Paschke, Axel Polleres,
and Dave Reynolds, editors. RIF Core Dialect (Second Edition). W3C Recommendation 5
February 2013, 2013. Available from http://www.w3.org/TR/rif-core/.

704
Pascal Hitzler, Jens Lehmann, Axel Polleres
[Bonatti et al., 2009] Piero A. Bonatti, Carsten Lutz, and Frank Wolter. The Complexity of
Circumscription in Description Logic. Journal of Artiﬁcial Intelligence Research, 35:717–773,
2009.
[Brachman and Schmolze, 1985] Ronald J Brachman and James G Schmolze. An overview of
the kl-one knowledge representation system*. Cognitive science, 9(2):171–216, 1985.
[Brachman, 1978] Ronald J. Brachman.
A structural paradigm for representing knowledge.
Technical Report BBN Report 3605, Bolt, Beraneck and Newman, Inc., Cambridge, MA,
1978.
[Brickley and Guha, 2004] Dan Brickley and R.V. Guha.
RDF Vocabulary Description Lan-
guage 1.0: RDF Schema. W3C Recommendation, February 2004. http://www.w3.org/TR/
rdf-schema/.
[Brickley and Miller, 2007] Dan Brickley and Libby Miller.
FOAF Vocabulary Speciﬁcation,
November 2007.
[Cal`ı et al., 2012] Andrea Cal`ı, Georg Gottlob, and Thomas Lukasiewicz. A general datalog-
based framework for tractable query answering over ontologies. Journal of Web Semantics,
14:57–83, 2012.
[Calvanese et al., 2007] D. Calvanese, G. De Giacomo, D. Lembo, M. Lenzerini, and R. Rosati.
Tractable reasoning and eﬃcient query answering in description logics: The DL-Lite family.
Journal of Automated reasoning, 39(3):385–429, 2007.
[Calvanese et al., 2011] Diego Calvanese, Giuseppe De Giacomo, Domenico Lembo, Maurizio
Lenzerini, Antonella Poggi, Mariano Rodriguez-Muro, Riccardo Rosati, Marco Ruzzi, and
Domenico Fabio Savo. The MASTRO system for ontology-based data access. Semantic Web,
2(1):43–53, 2011.
[Carothers et al., 2013] Gavin Carothers, Andy Seaborne, Chris Bizer, and Richard Cyganiak.
TriG RDF Dataset Language. W3C Last Call Working Draft, September 2013. http://www.
w3.org/TR/2013/WD-trig-20130919/.
[Carothers, 2013] Gavin Carothers.
N-Quads A line-based syntax for an RDF datasets.
W3C
Last
Call
Working
Draft,
September
2013.
http://www.w3.org/TR/2013/
WD-n-quads-20130905/.
[Carral et al., 2013] David Carral, Cong Wang, and Pascal Hitzler. Towards an eﬃcient algo-
rithm to reason over description logics extended with nominal schemas. In Wolfgang Faber
and Domenico Lembo, editors, Web Reasoning and Rule Systems – 7th International Confer-
ence, RR 2013, Mannheim, Germany, July 27-29, 2013. Proceedings, volume 7994 of Lecture
Notes in Computer Science, pages 65–79. Springer, 2013.
[Carral Mart´ınez et al., 2012] David Carral Mart´ınez, Adila Alfa Krisnadhi, and Pascal Hitzler.
Integrating OWL and rules: A syntax proposal for nominal schemas. In Pavel Klinov and
Matthew Horridge, editors, Proceedings of OWL: Experiences and Directions Workshop 2012,
Heraklion, Crete, Greece, May 27-28, 2012, volume 849 of CEUR Workshop Proceedings.
CEUR-WS.org, 2012.
[Carroll et al., 2005] Jeremy J. Carroll, Christian Bizer, Patrick J. Hayes, and Patrick Stickler.
Named graphs. Journal of Web Semantics, 3(4):247–267, 2005.
[Chandra and Merlin, 1977] Ashok K. Chandra and Philip M. Merlin. Optimal implementation
of conjunctive queries in relational data bases. In John E. Hopcroft, Emily P. Friedman, and
Michael A. Harrison, editors, Proceedings of the 9th Annual ACM Symposium on Theory of
Computing (STOC), pages 77–90, Boulder, Colorado, USA, 1977. ACM.
[Compton et al., 2012] Michael Compton, Payam M. Barnaghi, Luis Bermudez, Raul Garcia-
Castro, ´Oscar Corcho, Simon Cox, John Graybeal, Manfred Hauswirth, Cory A. Henson,
Arthur Herzog, Vincent A. Huang, Krzysztof Janowicz, W. David Kelsey, Danh Le Phuoc,
Laurent Lefort, Myriam Leggieri, Holger Neuhaus, Andriy Nikolov, Kevin R. Page, Alexandre
Passant, Amit P. Sheth, and Kerry Taylor. The SSN ontology of the W3C semantic sensor
network incubator group. Journal of Web Semantics, 17:25–32, 2012.
[Cyganiak and Wood, 2013] R.
Cyganiak
and
D.
Wood.
RDF
1.1
Concepts
and
Ab-
stract Syntax.
W3C Last Call Working Draft 23 July 2013, 2013.
Available from
http://www.w3.org/TR/2013/WD-rdf11-concepts-20130723/.
[David et al., 2011] J´erˆome David, J´erˆome Euzenat, Fran¸cois Scharﬀe, and C´assia Trojahn dos
Santos. The Alignment API 4.0. Semantic Web, 2(1):3–10, 2011.

Logics for the Semantic Web
705
[de Bruijn and Heymans, 2007] Jos de Bruijn and Stijn Heymans.
Logical foundations of
(e)RDF(S): Complexity and reasoning.
In Proceedings of the 6th International Semantic
Web Conference (ISWC2007), volume 4825 of LNCS, pages 86–99, 2007.
[de Bruijn and Heymans, 2010] Jos de Bruijn and Stijn Heymans.
Logical foundations of
RDF(S) with datatypes. Journal of Artiﬁcial Intelligence Research (JAIR), 38:535–568, 2010.
[de Bruijn et al., 2005] Jos de Bruijn, J¨urgen Angele, Harold Boley, Dieter Fensel, Pascal Hit-
zler, Michael Kifer, Reto Krummenacher, Holger Lausen, Axel Polleres, and Rudi Studer.
Web Rule Language (WRL). W3C Member Submission 09 September 2005, 2005. Available
from http://www.w3.org/Submission/WRL.
[de Bruijn et al., 2010] Jos de Bruijn, David Pearce, Axel Polleres, and Agust´ın Valverde. A
semantical framework for hybrid knowledge bases. Knowl. Inf. Syst., 25(1):81–104, 2010.
[de Bruijn et al., 2011] Jos de Bruijn, Thomas Eiter, Axel Polleres, and Hans Tompits. Em-
bedding nonground logic programs into autoepistemic logic for knowledge-base combination.
ACM Trans. Comput. Log., 12(3), 2011.
[de Bruijn, 2013] Jos de Bruijn. RIF RDF and OWL Compatibility (Second Edition). W3C
Recommendation, February 2013. http://www.w3.org/TR/rif-rdf-owl/.
[de Sainte Marie et al., 2013] Christian de Sainte Marie, Gary Hallmark, and Adrian Paschke,
editors. RIF Production Rule Dialect (Second Edition). W3C Recommendation 5 February
2013, 2013. Available from http://www.w3.org/TR/rif-prd/.
[Donini et al., 2002] F.M. Donini, D. Nardi, and R. Rosati.
Description Logics of Minimal
Knowledge and Negation as Failure. ACM Transactions on Computational Logic, 3(2):177–
225, 2002.
[Eiter et al., 2008a] T. Eiter, G. Ianni, T. Lukasiewicz, R. Schindlauer, and H. Tompits. Com-
bining answer set programming with description logics for the semantic web. Artiﬁcial Intel-
ligence, 172(12–13):1495–1539, 2008.
[Eiter et al., 2008b] Thomas Eiter, Giovambattista Ianni, Thomas Krennwallner, and Axel
Polleres. Rules and ontologies for the semantic web. In Cristina Baroglio, Piero A. Bonatti,
Jan Maluszynski, Massimo Marchiori, Axel Polleres, and Sebastian Schaﬀert, editors, Rea-
soning Web 2008, volume 5224 of LNCS, pages 1–53. Springer, San Servolo Island, Venice,
Italy, September 2008.
[Ellerman, 1997] Castedo Ellerman. Channel deﬁnition format (CDF). W3C Member Submis-
sion, March 1997. http://www.w3.org/TR/NOTE-CDFsubmit.html.
[Fensel et al., 2001] Dieter Fensel, Frank Van Harmelen, Ian Horrocks, Deborah L McGuinness,
and Peter F Patel-Schneider. OIL: An ontology infrastructure for the semantic web. Intelligent
Systems, IEEE, 16(2):38–45, 2001.
[Franconi et al., 2005] Enrico Franconi, Jos de Bruijn, and Sergio Tessaris. Logical reconstruc-
tion of normative RDF. In Workshop on OWL: Experiences and Directions (OWLED’05),
2005.
[Franconi et al., 2013] Enrico Franconi, Claudio Gutierrez, Alessandro Mosca, Giuseppe Pirr`o,
and Riccardo Rosati. The logic of extensional RDFS. In The Semantic Web-ISWC 2013,
pages 101–116. Springer, 2013.
[Friedland et al., 2004] Noah S. Friedland, Paul G. Allen, Gavin Matthews, Michael J. Wit-
brock, David Baxter, Jon Curtis, Blake Shepard, Pierluigi Miraglia, J¨urgen Angele, Steﬀen
Staab, Eddie M¨onch, Henrik Oppermann, Dirk Wenke, David J. Israel, Vinay K. Chaudhri,
Bruce W. Porter, Ken Barker, James Fan, Shaw Yi Chaw, Peter Z. Yeh, Dan Tecuci, and
Peter Clark. Project Halo: Towards a digital Aristotle. AI Magazine, 25(4):29–48, 2004.
[Gangemi, 2005] Aldo Gangemi. Ontology design patterns for semantic web content. In Yolanda
Gil, Enrico Motta, V. Richard Benjamins, and Mark A. Musen, editors, The Semantic Web
– ISWC 2005, 4th International Semantic Web Conference, ISWC 2005, Galway, Ireland,
November 6-10, 2005, Proceedings, volume 3729 of Lecture Notes in Computer Science, pages
262–276. Springer, 2005.
[Garey and Johnson, 1979] Michael R. Garey and David S. Johnson. Computers and Intractabil-
ity: A Guide to the Theory of NP-Completeness. W. H. Freeman and Company, 1979.
[Gelfond and Lifschitz, 1988] Michael Gelfond and Vladimir Lifschitz.
The stable model se-
mantics for logic programming. In Robert A. Kowalski and Kenneth Bowen, editors, 5th Int’l
Conf. on Logic Programming, pages 1070–1080, Cambridge, Massachusetts, 1988. The MIT
Press.

706
Pascal Hitzler, Jens Lehmann, Axel Polleres
[Giordano et al., 2013] Laura Giordano, Valentina Gliozzi, Nicola Olivetti, and Gian Luca Poz-
zato. A non-monotonic description logic for reasoning about typicality. Artiﬁcial Intelligence,
195:165–202, 2013.
[Grau et al., 2008] Bernardo Cuenca Grau, Ian Horrocks, Boris Motik, Bijan Parsia, Peter
Patel-Schneider, and Ulrike Sattler.
OWL 2: The next step for OWL.
Journal of Web
Semantics, 6(4):309–322, 2008.
[Grimm and Hitzler, 2008] Stephan Grimm and Pascal Hitzler. Semantic Matchmaking of Web
Resources with Local Closed-World Reasoning.
International Journal of Electronic Com-
merce, 12(2):89–126, 2008.
[Groth and Moreau, 2010] Paul Groth and Luc Moreau, editors. PROV-Overview, An Overview
of the PROV Family of Documents. W3C Working Group Note 30 April 2013, 2010. Available
from http://www.w3.org/TR/prov-overview.
[Guha and Bray, 1997] R.V. Guha and Tim Bray. Meta content framework using XML. W3C
Member Submission, June 1997. http://www.w3.org/TR/NOTE-MCF-XML/.
[Guti´errez et al., 2004] Claudio Guti´errez, Carlos A. Hurtado, and Alberto O. Mendelzon. Foun-
dations of semantic web databases.
In Proceedings of the 23rd ACM SIGACT-SIGMOD-
SIGART Symposium on Principles of Database Systems (PODS2004), pages 95–106, 2004.
[Gutierrez et al., 2007] Claudio Gutierrez, Carlos A. Hurtado, and Alejandro A. Vaisman. In-
troducing time into RDF. IEEE Trans. Knowl. Data Eng., 19(2):207–218, 2007.
[Hamby, 2012] Steve
Hamby.
2012:
The
year
of
the
Semantic
Web
–
revisited!
http://www.huﬃngtonpost.com/steve-hamby/2012-the-year-of-the-sema b 1559767.html,
Jun 2012.
[Harris and Seaborne, 2013] Steve Harris and Andy Seaborne. SPARQL 1.1 Query Language.
W3C Recommendation, March 2013. http://www.w3.org/TR/sparql11-query/.
[Hayes and Patel-Schneider, 2014] Patrick Hayes and Peter Patel-Schneider.
RDF 1.1 Se-
mantics.
W3C Proposed Recommendation, January 2014.
http://www.w3.org/TR/2014/
PR-rdf11-mt-20140109/.
[Hayes, 2004] Patrick Hayes. RDF Semantics. W3C Recommendation, February 2004. http:
//www.w3.org/TR/rdf-mt/.
[Herman et al., 2013] Ivan Herman, Ben Adida, and Manu Sporny. RDFa 1.1 Primer. W3C
Working Group Note, August 2013. http://www.w3.org/TR/xhtml-rdfa-primer/.
[Herman, 2009] Ivan Herman.
W3C Workshop – RDF Next Steps.
Workshop Report, June
2009. http://www.w3.org/2009/12/rdf-ws/Report.html.
[Hermann, 2010] Ivan
Hermann.
Semantic
web
adoption
and
applications.
http://www.w3.org/People/Ivan/CorePresentations/Applications/, Nov 2010.
[Hickson, 2012] Ian Hickson. HTML Microdata. W3C Working Draft, October 2012. http:
//www.w3.org/TR/2012/WD-microdata-20121025/, work in progress, latest version available
at http://www.w3.org/TR/microdata/.
[Hitzler and Janowicz, 2013] Pascal Hitzler and Krzysztof Janowicz. Linked data, big data, and
the 4th paradigm. Semantic Web, 4(3):233–235, 2013.
[Hitzler and van Harmelen, 2010] Pascal Hitzler and Frank van Harmelen. A reasonable seman-
tic web. Semantic Web, 1(1–2):39–44, 2010.
[Hitzler et al., 2010] Pascal Hitzler, Markus Kr¨otzsch, and Sebastian Rudolph. Foundations of
Semantic Web Technologies. Chapman & Hall/CRC, 2010.
[Hitzler et al., 2012] Pascal Hitzler, Markus Kr¨otzsch, Bijan Parsia, Peter F. Patel-Schneider,
and Sebastian Rudolph,
editors.
OWL 2 Web Ontology Language:
Primer (Sec-
ond
Edition).
W3C
Recommendation
11
December
2012,
2012.
Available
from
http://www.w3.org/TR/owl2-primer/.
[Horridge and Bechhofer, 2011] Matthew Horridge and Sean Bechhofer. The OWL API: A Java
API for OWL ontologies. Semantic Web, 2(1):11–21, 2011.
[Horridge and Patel-Schneider, 2008] Matthew Horridge and Peter F. Patel-Schneider. Manch-
ester syntax for OWL 1.1. In OWLED 2008, 4th international workshop OWL: Experiences
and Directions, 2008.
[Horrocks et al., 2003] Ian Horrocks, Peter F Patel-Schneider, and Frank Van Harmelen. From
SHIQ and RDF to OWL: The making of a web ontology language. Web semantics: science,
services and agents on the World Wide Web, 1(1):7–26, 2003.

Logics for the Semantic Web
707
[Horrocks et al., 2004] Ian Horrocks, Peter F. Patel-Schneider, Harold Boley, Said Tabet,
Benjamin Grosof, and Mike Dean.
SWRL: A Semantic Web Rule Language Combin-
ing OWL and RuleML.
W3C Member Submission 21 May 2004, 2004.
Available from
http://www.w3.org/Submission/SWRL/.
[Horrocks et al., 2005a] Ian Horrocks, Bijan Parsia, Peter F. Patel-Schneider, and James A.
Hendler. Semantic web architecture: Stack or two towers? In 3rd International Workshop on
Principles and Practice of Semantic Web Reasoning (PPSWR2005), volume 3703 of LNCS,
pages 37–41. Springer, 2005.
[Horrocks et al., 2005b] Ian Horrocks, Peter F. Patel-Schneider, Sean Bechhofer, and Dmitry
Tsarkov. OWL rules: A proposal and prototype implementation. Journal of Web Semantics,
3:23–40, 2005.
[Huang et al., 2013] Shasha Huang, Qingguo Li, and Pascal Hitzler. Reasoning with inconsis-
tencies in hybrid MKNF knowledge bases. Logic Journal of the IGPL, 21(2):263–290, 2013.
[Jain et al., 2010] Prateek Jain, Pascal Hitzler, Peter Z. Yeh, Kunal Verma, and Amit P. Sheth.
Linked Data is Merely More Data. In Dan Brickley, Vinay K. Chaudhri, Harry Halpin, and
Deborah McGuinness, editors, Linked Data Meets Artiﬁcial Intelligence, pages 82–86. AAAI
Press, Menlo Park, CA, 2010.
[Janowicz and Hitzler, 2012] Krzysztof Janowicz and Pascal Hitzler.
The Digital Earth as
knowledge engine. Semantic Web, 3(3):213–221, 2012.
[Kazakov et al., 2011] Yevgeny Kazakov, Markus Kr¨otzsch, and Frantiˇsek Simanˇc´ık. Concurrent
classiﬁcation of EL ontologies.
In Lora Aroyo, Chris Welty, Harith Alani, Jamie Taylor,
Abraham Bernstein, Lalana Kagal, Natasha Noy, and Eva Blomqvist, editors, Proceedings of
the 10th International Semantic Web Conference (ISWC’11), volume 7032 of LNCS. Springer,
2011.
[Kifer and Boley, 2013] Michael Kifer and Harold Boley,
editors.
RIF Overview (Sec-
ond Edition).
W3C Working Group Note 5 February 2013,
2013.
Available from
http://www.w3.org/TR/rif-overview/.
[Kifer et al., 1995] Michael Kifer, Geord Lausen, and James Wu. Logical foundations of object-
oriented and frame-based languages. JACM, 42(4):741–843, 1995.
[Kifer et al., 2005] Michael Kifer, Jos de Bruijn, Harold Boley, and Dieter Fensel.
A realis-
tic architecture for the semantic web. In 1st International Conference on Rules and Rule
Markup Languages for the Semantic Web (RuleML2005), volume 3791 of LNCS, pages 17–
29. Springer, 2005.
[Kifer, 2008] Michael Kifer. Rule Interchange Format: The framework. In Diego Calvanese and
Georg Lausen, editors, Web Reasoning and Rule Systems, volume 5341 of Lecture Notes in
Computer Science, pages 1–11. Springer, 2008.
[Klahr et al., 1987] David Klahr, Pat Langley, and Robert Neches, editors. Production System
Models of Learning and Development. Bradford, 1987.
[Klinov and Parsia, 2008] Pavel Klinov and Bijan Parsia. Optimization and evaluation of rea-
soning in probabilistic description logic: Towards a systematic approach. In Amit P. Sheth,
Steﬀen Staab, Mike Dean, Massimo Paolucci, Diana Maynard, Timothy W. Finin, and Krish-
naprasad Thirunarayan, editors, The Semantic Web – ISWC 2008, 7th International Seman-
tic Web Conference, ISWC 2008, Karlsruhe, Germany, October 26-30, 2008. Proceedings,
volume 5318 of Lecture Notes in Computer Science, pages 213–228. Springer, 2008.
[Knorr et al., 2011] M. Knorr, J.J. Alferes, and P. Hitzler. Local closed-world reasoning with
description logics under the well-founded semantics. Artiﬁcial Intelligence, 175(9–10):1528–
1554, 2011.
[Knorr et al., 2012] M. Knorr, P. Hitzler, and F. Maier. Reconciling OWL and non-monotonic
rules for the Semantic Web. In L. De Raedt, C. Bessiere, D. Dubois, P. Doherty, P. Fras-
coni, F. Heintz, and P. Lucas, editors, ECAI 2012, 20th European Conference on Artiﬁcial
Intelligence, 27-31 August 2012, Montpellier, France, volume 242 of Frontiers in Artiﬁcial
Intelligence and Applications, pages 474–479. IOS Press, Amsterdam, 2012.
[Krisnadhi et al., 2011] Adila Krisnadhi, Frederick Maier, and Pascal Hitzler. OWL and rules. In
Axel Polleres, Claudia d’Amato, Marcelo Arenas, Siegfried Handschuh, Paula Kroner, Sascha
Ossowski, and Peter F. Patel-Schneider, editors, Reasoning Web. Semantic Technologies for
the Web of Data – 7th International Summer School 2011, Galway, Ireland, August 23-27,
2011, Tutorial Lectures, volume 6848 of Lecture Notes in Computer Science, pages 382–415.
Springer, 2011.

708
Pascal Hitzler, Jens Lehmann, Axel Polleres
[Kr¨otzsch et al., 2008] Markus Kr¨otzsch, Sebastian Rudolph, and Pascal Hitzler. Description
Logic Rules. In Malik Ghallab, Constantine D. Spyropoulos, Nikos Fakotakis, and Nikolaos M.
Avouris, editors, Proceeding of the 18th European Conference on Artiﬁcial Intelligence, Pa-
tras, Greece, July 21-25, 2008, volume 178, pages 80–84, Amsterdam, The Netherlands, 2008.
IOS Press.
[Kr¨otzsch et al., 2011] Markus Kr¨otzsch, Frederick Maier, Adila A. Krisnadhi, and Pascal Hit-
zler.
A better uncle for OWL: Nominal schemas for integrating rules and ontologies.
In
S. Sadagopan, Krithi Ramamritham, Arun Kumar, M.P. Ravindra, Elisa Bertino, and
Ravi Kumar, editors, Proceedings of the 20th International World Wide Web Conference,
WWW2011, Hyderabad, India, March/April 2011, pages 645–654. ACM, New York, 2011.
[Kr¨otzsch, 2010] Markus Kr¨otzsch. Description Logic Rules, volume 008 of Studies on the Se-
mantic Web. IOS Press/AKA, 2010.
[Lassila and Swick, 1999] Ora Lassila and Ralph R. Swick. Resource Description Framework
(RDF) Model and Syntax Speciﬁcation.
W3C Recommendation, February 1999.
http://
www.w3.org/TR/1999/REC-rdf-syntax-19990222/.
[Lassila and Swick, 2004] O. Lassila and R. R. Swick. Resource Description Framework (RDF)
Model and Syntax Speciﬁcation. W3C Recommendation 10 February 2004, 2004. Available
from http://www.w3.org/TR/REC-rdf-syntax/.
[Lehmann, 2009] Jens Lehmann. DL-Learner: learning concepts in description logics. Journal
of Machine Learning Research (JMLR), 10:2639–2642, 2009.
[Lukasiewicz and Straccia, 2009] Thomas Lukasiewicz and Umberto Straccia. Description logic
programs under probabilistic uncertainty and fuzzy vagueness. Int. J. Approx. Reasoning,
50(6):837–853, 2009.
[Maier et al., 2013] Frederick Maier, Yue Ma, and Pascal Hitzler.
Paraconsistent OWL and
related logics. Semantic Web, 4(4):395–427, 2013.
[Mallea et al., 2011] Alejandro Mallea, Marcelo Arenas, Aidan Hogan, and Axel Polleres. On
Blank Nodes. In Proceedings of the 10th International Semantic Web Conference (ISWC
2011), volume 7031 of LNCS. Springer, October 2011.
[Manola et al., 2004] Frank Manola, Eric Miller, and Brian McBride. RDF Primer. W3C Rec-
ommendation, February 2004. http://www.w3.org/TR/rdf-primer/.
[Mazzieri and Dragoni, 2005] Mauro Mazzieri and Aldo Franco Dragoni. A Fuzzy Semantics
for Semantic Web Languages. In Paulo Cesar G. da Costa, Kathryn B. Laskey, Kenneth J.
Laskey, and Michael Pool, editors, ISWC-URSW, pages 12–22, 2005.
[McGuinness et al., 2002] Deborah L McGuinness, Richard Fikes, James Hendler, and Lynn An-
drea Stein. DAML+OIL: an ontology language for the semantic web. Intelligent Systems,
IEEE, 17(5):72–80, 2002.
[Miles and Bechhofer, 2009] A. Miles and S. Bechhofer, editors. SKOS Simple Knowledge Or-
ganization System Reference. W3C Recommendation 18 August 2009, 2009. Available from
http://www.w3.org/TR/skos-reference.
[Minsky, 1975] Marvin Minsky. A framework for representing knowledge. In The Psychology of
Computer Vision, pages 211–277. McGraw-Hill, 1975.
[Motik and Horrocks, 2008] Boris Motik and Ian Horrocks. OWL datatypes: Design and imple-
mentation. In The Semantic Web-ISWC 2008, pages 307–322. Springer, 2008.
[Motik and Rosati, 2010] B. Motik and R. Rosati.
Reconciling description logics and rules.
Journal of the ACM, 57(5), 2010.
[Motik et al., 2005] Boris Motik, Ulrike Sattler, and Rudi Studer. Query answering for OWL-
DL with rules. Journal of Web Semantics: Science, Services and Agents on the World Wide
Web, 3(1):41–60, 2005.
[Motik et al., 27 October 2009] B. Motik, B. Cuenca Grau, I. Horrocks, Z. Wu, A. Fokoue, and
C. Lutz, editors.
OWL 2 Web Ontology Language: Proﬁles.
W3C Recommendation, 27
October 2009. Available at http://www.w3.org/TR/owl2-proﬁles/.
[Motik, 2012] Boris Motik. Representing and querying validity time in rdf and owl: A logic-
based approach. J. Web Sem., 12:3–21, 2012.
[Mu˜noz et al., 2007] Sergio Mu˜noz, Jorge P´erez, and Claudio Gutierrez.
Minimal Deductive
Systems for RDF. In Enrico Franconi, Michael Kifer, and Wolfgang May, editors, Proceedings
of the 4th European Semantic Web Conference (ESWC 2007), volume 4519 of LNCS, pages
53–57, Innsbruck, Austria, June 2007.

Logics for the Semantic Web
709
[Mutharaju et al., 2013] Raghava Mutharaju, Pascal Hitzler, and Prabhaker Mateti. DistEL:
A distributed EL+ ontology classiﬁer. In Thorsten Liebig and Achille Fokoue, editors, SSWS
2013, Scalable Semantic Web Knowledge Base Systems 2013. Proceedings of the 9th Inter-
national Workshop on Scalable Semantic Web Knowledge Base Systems, co-located with the
International Semantic Web Conference (ISWC 2013), Sydney, Australia, October 21, 2013,
volume 1046 of CEUR Workshop Proceedings, pages 17–32, 2013.
[Nilsson et al., 2008] Mikael Nilsson, Andy Powell, Pete Johnston, and Ambj¨orn Naeve. Ex-
pressing Dublin Core metadata using the Resource Description Framework (RDF). DCMI
Recommendation, January 2008. http://dublincore.org/documents/dc-rdf/.
[Noy and McGuinness, 2013] Natasha Noy and Deborah McGuinness, editors. Final Report on
the 2013 NSF Workshop on Research Challenges and Opportunities in Knowledge Represen-
tation, Arlington, VA, February 7-8, 2013. National Science Foundation Workshop Report,
2013.
[OWL Working Group, 11 December 2012] W3C OWL Working Group. OWL 2 Web Ontology
Language Manchester Syntax (Second Edition). W3C Working Group Note, 11 December
2012. Available at http://www.w3.org/TR/owl2-manchester-syntax/.
[OWL Working Group, 27 October 2009] W3C OWL Working Group. OWL 2 Web Ontology
Language: Document Overview.
W3C Recommendation, 27 October 2009.
Available at
http://www.w3.org/TR/owl2-overview/.
[Phillips and Davis, 2006] A. Phillips and M. Davis. Tags for identifying languages. Technical
Report BCP 47, IETF, September 2006. http://www.rfc-editor.org/rfc/bcp/bcp47.txt.
[Pichler et al., 2008] Reinhard Pichler, Axel Polleres, Fang Wei, and Stefan Woltran. Entailment
for domain-restricted RDF. In Proceedings of the 5th European Semantic Web Conference
(ESWC2008), volume 5021 of LNCS, pages 200–214, Tenerife, Spain, 2008. Springer.
[Poggi et al., 2008] Antonella Poggi, Domenico Lembo, Diego Calvanese, Giuseppe Giacomo,
Maurizio Lenzerini, and Riccardo Rosati. Linking data to ontologies. In Stefano Spaccapietra,
editor, Journal on Data Semantics X, volume 4900 of LNS, pages 133–173. Springer, 2008.
[Polleres and Wallner, 2013] Axel Polleres and Johannes Wallner.
On the relation between
SPARQL1.1 and Answer Set Programming.
Journal of Applied Non-Classical Logics
(JANCL), 23(1–2):159–212, 2013. Special issue on Equilibrium Logic and Answer Set Pro-
gramming.
[Polleres et al., 2011] Axel Polleres, Claudia D’Amato, Marcelo Arenas, Siegfried Handschuh,
Paula Kroner, Sascha Ossowski, and Peter Patel-Schneider, editors. Reasoning Web. Semantic
Technologies for the Web of Data. (ReasoningWeb 2011), volume 6848 of LNCS. Springer,
August 2011.
[Prud’hommeaux, 2008] Eric Prud’hommeaux. SPARQL Query Language for RDF. W3C Rec-
ommendation, January 2008. http://www.w3.org/TR/rdf-sparql-query/.
[Przymusinski, 1988] Theodor C. Przymusinski.
On the Declarative Semantics of Deductive
Databases and Logic Programs. In Jack Minker, editor, Foundations of Deductive Databases
and Logic Programming, pages 193–216. Morgan Kaufmann Publishers, Inc., 1988.
[Roman et al., 2005] Dumitru Roman, Uwe Keller, Holger Lausen, Jos de Bruijn, Rub´en Lara,
Michael Stollberg, Axel Polleres, Cristina Feier, Christoph Bussler, and Dieter Fensel. Web
Service Modeling Ontology. Applied Ontology, 1(1):77–106, 2005.
[Schlicht and Stuckenschmidt, 2010] Anne Schlicht and Heiner Stuckenschmidt.
Peer-to-peer
reasoning for interlinked ontologies. Int. J. Semantic Computing, 4(1):27–58, 2010.
[Sengupta et al., 2011] Kunal Sengupta, Adila Alfa Krisnadhi, and Pascal Hitzler. Local Closed
World Semantics: Grounded Circumscription for OWL. In Lora Aroyo, Chris Welty, Harith
Alani, Jamie Taylor, Abraham Bernstein, Lalana Kagal, Natasha Fridman Noy, and Eva
Blomqvist, editors, The Semantic Web – ISWC 2011 – 10th International Semantic Web
Conference, Bonn, Germany, October 23-27, 2011, Proceedings, Part I, volume 7031 of Lec-
ture Notes in Computer Science, pages 617–632. Springer, Heidelberg, 2011.
[Smith et al., 2004] Michael K. Smith, Chris Welty, and Deborah L. McGuinness, editors. OWL
Web Ontology Language Guide. W3C Recommendation 10 February 2004, 2004. Available
from http://www.w3.org/TR/owl-guide/.
[Steigmiller et al., 2013] Andreas Steigmiller, Birte Glimm, and Thorsten Liebig.
Nominal
schema absorption.
In Francesca Rossi, editor, IJCAI 2013, Proceedings of the 23rd In-
ternational Joint Conference on Artiﬁcial Intelligence, Beijing, China, August 3-9, 2013.
IJCAI/AAAI, 2013.

710
Pascal Hitzler, Jens Lehmann, Axel Polleres
[Straccia, 2001] Umberto Straccia. Reasoning within fuzzy description logics. J. Artif. Intell.
Res. (JAIR), 14:137–166, 2001.
[Straccia, 2009] Umberto Straccia. A Minimal Deductive System for General Fuzzy RDF. In
Axel Polleres and Terrance Swift, editors, Web Reasoning and Rule Systems, Third Interna-
tional Conference, RR 2009, Chantilly, VA, USA, October 25-26, 2009, Proceedings, volume
5837 of LNCS, pages 166–181. Springer, 2009.
[Studer, 2006] Rudi Studer. The Semantic Web: Suppliers and customers. In Isabel F. Cruz,
Stefan Decker, Dean Allemang, Chris Preist, Daniel Schwabe, Peter Mika, Michael Uschold,
and Lora Aroyo, editors, The Semantic Web – ISWC 2006, 5th International Semantic Web
Conference, ISWC 2006, Athens, GA, USA, November 5-9, 2006, Proceedings, volume 4273
of Lecture Notes in Computer Science, pages 995–996. Springer, 2006.
[Tudorache et al., 2013] Tania Tudorache, Csongor Nyulas, Natalya Fridman Noy, and Mark A.
Musen. Webprot´eg´e: A collaborative ontology editor and knowledge acquisition tool for the
web. Semantic Web, 4(1):89–99, 2013.
[Udrea et al., 2010] Octavian Udrea, Diego Reforgiato Recupero, and V. S. Subrahmanian. An-
notated RDF. ACM Transactions on Computational Logic, 11(2):1–41, 2010.
[Urbani et al., 2011] Jacopo Urbani, Frank van Harmelen, Stefan Schlobach, and Henri E. Bal.
Querypie: Backward reasoning for OWL Horst over very large knowledge bases. In Lora Aroyo,
Chris Welty, Harith Alani, Jamie Taylor, Abraham Bernstein, Lalana Kagal, Natasha Fridman
Noy, and Eva Blomqvist, editors, The Semantic Web – ISWC 2011 – 10th International Se-
mantic Web Conference, Bonn, Germany, October 23-27, 2011, Proceedings, Part I, volume
7031 of Lecture Notes in Computer Science, pages 730–745. Springer, 2011.
[Urbani et al., 2012] Jacopo Urbani, Spyros Kotoulas, Jason Maassen, Frank van Harmelen, and
Henri E. Bal. WebPIE: A web-scale parallel inference engine using MapReduce. Journal of
Web Semantics, 10:59–75, 2012.
[Vanekov´a et al., 2005] Veronika Vanekov´a, J´an Bella, Peter Gursk´y, and Tom´aˇs Horv´ath. Fuzzy
RDF in the Semantic Web: Deduction and Induction. In 6th Slovak-Austrian Student Work-
shop WDA ’05, pages 16–29. Elfa Academic Press, Ltd, 2005.
[W3C Metadata, revision of 23 August 2002] Metadata
Activity
Statement.
http://www.w3.org/Metadata/Activity, revision of 23 August 2002.
[W3C Semantic Web, revision of 19 June 2013] W3C
Semantic
Web
Activity.
http://www.w3.org/2001/sw/, revision of 19 June 2013.
[Weaver and Hendler, 2009] Jesse Weaver and James A. Hendler. Parallel materialization of the
ﬁnite RDFS closure for hundreds of millions of triples. In Abraham Bernstein et al., editors,
The Semantic Web – ISWC 2009, 8th International Semantic Web Conference, ISWC 2009,
Chantilly, VA, USA, October 25-29, 2009, volume 5823 of Lecture Notes in Computer Science,
pages 682–697. Springer, 2009.
[Web Ontology Working Group, 10 February 2004] Web Ontology Working Group. OWL Web
Ontology Language Overview. W3C Recommendation, 10 February 2004. Available at http:
//www.w3.org/TR/owl-features/.
[Zhou et al., 2012] Zhangquan Zhou, Guilin Qi, Chang Liu, Pascal Hitzler, and Raghava
Mutharaju.
Reasoning with Fuzzy-EL+ ontologies using MapReduce.
In Luc De Raedt,
Christian Bessi`ere, Didier Dubois, Patrick Doherty, Paolo Frasconi, Fredrik Heintz, and Pe-
ter J. F. Lucas, editors, ECAI 2012 – 20th European Conference on Artiﬁcial Intelligence.
Including Prestigious Applications of Artiﬁcial Intelligence (PAIS-2012) System Demonstra-
tions Track, Montpellier, France, August 27-31 , 2012, volume 242 of Frontiers in Artiﬁcial
Intelligence and Applications, pages 933–934. IOS Press, 2012.
[Zimmermann et al., 2012] Antoine Zimmermann, Nuno Lopes, Axel Polleres, and Umberto
Straccia. A general framework for representing, reasoning and querying with annotated se-
mantic web data. Journal of Web Semantics (JWS), 12:72–95, March 2012.

INDEX
Ωmega, 139, 242, 348
⇒-elimination, 148
λλ type theory, 143
K-match, 188
4-color theorem, 146, 191, 192
A-Prolog, 612
Abduce(∆, Q), 66–68, 70
abduction, 556–559, 603, 606
abductive logic programming, 556–
559, 603
abductive reasoning, 573
Abella, 237, 243
Abiteboul, S., 583, 590, 591, 596
ABox, 662
abstract consistency principle, 230
abstract reduction system, 272–274
abstract strategy, 273
abstract syntax, 140, 188, 352
ACL2, 152, 162, 164, 166, 169, 170,
174, 178, 182, 190, 195, 243,
267
active DB rules, 591
ad hominem, 73n
Aditi, 598
admissibility semantics, 562
AFFIRM, 167
Agda, 139, 141, 144, 147, 243, 245
Agda2, 147
agents, 600–652
aggregated systems, 59
aggregates, 591, 592, 594, 595, 597,
600, 608, 614, 691
monotonicity, 595
user-deﬁned, 600
agsyHOL, 245
ALC, 662, 664, 666, 667
ALC knowledge base, 663
ALF, 147, 183
Alferes, J. J., 605
algebraic speciﬁcations, 260
Algol, 141, 158, 159, 416, 424, 652
Algorithm = Logic + Control, 537–
539
algorithmic approximations of logical
systems, 51
algorithmic proof systems, 47, 122
algorithmic structured consequence re-
lations, 49
ALP Newsletter, 587
α-conversion, 223, 225
alternating ﬁxed point, 562
Alternating-Time Temporal Logic (ATL),
649, 650
Alviano, M., 600
ancestor resolution, 532, 535, 536, 540
and-or tree, 525, 531, 540
and-tree, 540
Andr´eka, 542
answer
negated queries, 578
positive queries
disjunctive Datalog, 578
answer literal, 529, 536
Answer Set Programming (ASP), 315,
327, 555–556, 588, 601, 614
programs, 591
semantics, 579, 605
semantics for disjunctive theories,
602
answer sets, 598
answer to a query, 582, 584
anti-chain, 463
anti-monotonic, 577
applications, Datalog, Disjunctive Dat-
alog, 606

712
Index
approximate reasoning, deductive and
disjunctive databases, 604
AProVE, 262
APS, 274
Apt, K., 573, 586, 587
Aqvist, L., 580
Aravindan, C., 599
architecture for intelligent agents, 612
Archive of Formal Proofs, 185
argumentation, 301, 315, 317, 552,
560–562
argumentation networks, 112
arithemetic hierarchy, 446
Arni, N., 591, 596
array noncomputable, 462
artiﬁcial intelligence, 15, 135, 147, 157,
180, 571, 613, 632
ASF+SDF, 271
ASP (Answer Set Programming), 588,
600, 602, 612
systems benchmarking, 598
ASPIDE, 607, 610
ATEL, 650
atoms, 574
AUT-Π, 143
AUT-68, 142
AUT-QE, 142, 143
AUT-QE-NTI, 142, 143
AUT-SL, 143
AUT-SYNTH, 143
autoepistemic logic, 553, 587, 589, 605,
613
automata, 90
automated theorem proving, 135, 136,
139, 148, 180 184, 186, 573
Automath, 139–146, 158, 161, 168,
243, 347, 349
automation, 135, 139, 155, 162–164,
166, 168, 169, 174, 176, 177,
180, 184
automorphism, 465, 470, 478, 482,
483, 485
axiom of inﬁnity, 219
axiom of univalence, 144
B-theories, 106
B method, 194
Back, 666
Baire category, 451, 452, 463
Balduccini, M., 609n, 615
Bancilhon, F., 594, 598
Baral, C., 590, 603, 612
Barcel´o, P., 596, 606
Baumgartner, R., 608
Bayesian network, 304
BDI logics, 632
BDI programming languages, 651
Beeri, C., 594
Begriﬀsschrift, 32, 218, 344
belief, 283, 312
belief revision, 312, 324
Belnap, N. D., 580
Beluga, 243
Ben-Eliyahu, R., 603, 604
β-conversion, 216
β-redex, 223
β-reduction, 223
βη-normal form, 224
Big Data, 582, 597, 699
biinterpretability conjecture, 480, 482,
483
bilinear recursive rules, 594
Blair, H., 586
blindly committed agent, 636
Blum complexity, 497
BOID, 646
Bonatti, P. A., 605
Boole, G., 32
Boolean abgebra, 32
Boolean extensionality, 226, 240
Boolean formula, 589n
boom project, 596
Borchert, P., 598, 615
bottom-up evaluation, 591, 597
bottom-up reasoning, 527, 531
bound query, 577
bounded recursion, 582, 594
bounded truth-table reducibility, 447
Boyer, Robert, 524

Index
713
Boyer-Moore theorem prover, 166
branching degree, 473, 477
Brass, S., 602, 606
Bratman, M. E., 632
Brewka, G., 588, 589, 606
Bry, F., 597
Bu, Y., 597
Buccafurri, F., 606
Bundy, Alan, 537
C2, 667
Cadoli, M., 604, 605
CafeOBJ, 271
CakeML, 190
Cal´ı, A., 611
Calculemus, 15
Calculus of Communicating Systems,
151
Calculus of Constructions, 144, 145,
189
Cambridge LCF, 150, 151, 155
CAML, 145
CAML Light, 148, 153
candidate de reductibilit´e, 231
canonical structures, 145, 162
canonical term rewriting systems, 260
Cantor, G., 33, 34, 344
cappable degrees, 473, 474
cardinality, 170, 189
and weight constraints, 600
Carnap, R., 580
Casl, 260
causality, 311, 589, 600, 604, 612
CB, 668
CEL, 668
Ceri, S., 591
certainty
factor, 283
level, 293
qualiﬁcation, 290, 296
weakening, 295
certiﬁcate, 177–179
certiﬁcation, 176, 177, 179, 181
CGa, 350, 353–361, 363, 364, 370,
374, 383, 387–389, 391
chain recursions, 594
Chakravarthy, U. S., 585
Chandra, A., 586
Chang, C., 582
Channel Deﬁnition Format, 682
Chaudhuri, S., 585
Chen, W., 588
chess, 36
Chiang, D.-A., 602
Chimenti, D., 591
choice, 226, 231, 240
choice function, 224
Cholewinski, P., 599
Chomicki, J., 603, 604, 614
Church numeral, 225
Church, A., 33
Church-Rosser modulo, 264
Church-Rosser property, 263
Church-Turing thesis, 443
CIC, 144
CiME, 262
circumscription, 589, 605, 613
Clark
completion theory, 582
completion, 544–548
equality theory, 544, 547
Clark, K., 544–546, 549, 581, 582,
584
clasp, 589, 600
class rewrite system, 264
Classic, 666
classical higher-order logic, 215, 220
classical negation, 554
classical negation (¬), 578, 588, 605
classiﬁcation, 664
of ﬁnite simple groups, 191, 192
CLInc stack, 190, 195
closed world assumption (CWA), 291,
544, 582, 583
closed world reasoning, 605
CLP Scheme, 559
Cmodels, 589

714
Index
co-routining, 538
Coalition Logic, 648
Codd, E. F., 543, 571, 579, 583, 590
coercions, 145
CoFI, 260
cognitive robotics, 603
cognitive science, 15
Cohen, P., 633
coherence modulo, 264
coinductive types, 144, 145
coinﬁnite extension argument, 454
coinﬁnite extension technique, 449, 450
COL, 598
collaboration, 184–186
Colmerauer, A., 524, 527, 534, 538,
559, 572
commercial Datalog sysem LogicBlox,
596
commercial query language, 590
CompCert, 195
Compendium of Continuous Lattices,
162
complete problems, 443
complete proof procedure, 537
complete search space, 536
complete search strategy, 536
completion, 164, 177
axiom, 584
procedure, 263
complexity, 299, 304, 495, 574, 590,
603, 604, 614
compound statement, 160
comprehension principle, 219
computable approximation, 471
Computable model theory, 484
computably enumerable, 443, 445, 446
computably enumerable degrees, 447,
454
computably enumerable Turing degrees,
444
computational complexity, 17, 495
computational logic, 15, 19, 613
concept assertion axioms, 663
Concept-Base, 598
concrete domains, 669
concurrency, 17
concurrent logic programming, 538
conditional rewriting, 264
ConDOR, 668
conﬂuence, 263
congruence closure, 161, 163, 176
conjunctive query containment, 688
consequence
argued consequence, 302
-based approaches, 668
nonmonotonic consequence, 310
operator TP, 576, 591
relations, 44
safely supported consequence, 302
conservative extension, 152
consistency checking (SATCHMO), 597
consistent, 584
ALC knowledge base, 663
constraint logic programming, 559–
560
constraints, user, 598
contiguous degree, 477, 483
continuous count aggregates, 597
continuous operator, 576
control access policy, 308
Cooper’s algorithm, 177
cooperative answering, 585
Coq, 139, 141, 142, 144–146, 152, 156,
162, 163, 168, 169, 176, 178,
181–185, 188, 189, 195, 243,
266, 267, 343, 347–350, 353,
354, 369, 370, 374, 379, 383,
386–391
coqATP, 245
coqdoc, 146, 185
CoqIDE, 146, 183, 184
CORA, 598
correctness conditions, 162
counting, 598
CRACK, 666
creative sets, 461
credibility rules, RDS, 579
critical pairs, 263

Index
715
cross-linking, 185, 186
cube of opposition, 292
Cubo, C., 602
cuppable degree, 474, 475
Curry’s paradox, 222
Curry-Howard correspondence, 169,
170
Curry-Howard isomorphism, 139, 141,
142, 145
cut-elimination, 230
cut-simulation, 231, 240
CWA, 573
Reiter, 578
cyclic graphs, 594, 596
D-axiom, 644
D-WFS, 602
Dagstuhl Initiative, 598, 615
Dagstuhl workshops on LPNMR, 598
Damasio, C. V., 599
DAML+OIL, 691, 692, 693
Dantsin, E., 583, 603
DARPA Agent Markup Language, 692
Darvas, F., 582
data
deﬁnition language, 572
independence principle, 572
integration, 601
mining, 596
modiﬁcation language, 572
reality values, 579
data structures, 600
database design, 316
database, facts without symbols, 590
databases, 17, 571
hierarchical, 572, 613
network , 572
relational, 571
Datalog, 525, 548, 562, 572–574, 576,
582, 583, 588, 590, 592, 594,
600, 604, 614, 615, 700
aggregates, 598
-based languages and systems, 597
community committed to declar-
ative semantics, 596
databases
unique minimal model, 601
interfaces, 598
optimization, 598
storage, 598
programs, 577
systems, 591
with default negation, 589
with disjunction, 591
Datalog±, 611
DDB system EKS-V1, 597
DDBase, mixes Datalog and Prolog
evaluation, 600
de Bruijn criterion, 187
Dechter, R., 603, 604
decision procedures, 145, 152, 163,
166, 168, 178, 179, 187
decision under uncertainty, 314
declarative, 147, 153, 172–176
networking, 596
query language, 571
representation, 527, 539, 563
semantics, 591
DECLARE, 598
Dedalus project, 596, 598
DEDUCE, 582
deduction, 556
modulo, 217, 264
procedure, 580
systems for mathematics and en-
gineering, 16
deductive databases, 16, 548, 558, 572–
574, 583, 590, 594
deductive query evaluation methods
(QSQ/SLD), 597
deductively augmented data manage-
ment system (DADM), 582
deep embedding, 177, 194
default logic, 587, 589, 605, 613
local query answering, 600
default negation, 582, 583, 587, 591,
594, 598, 613

716
Index
disjunctive Datalog, stable mod-
els, 578
negation (not), 574, 605
negation in DDBs, 588
default reasoning, 308, 553, 605
deﬁnability, 465
deﬁnite clause, 535
deﬁnite clause grammar, 535
deﬁnite formula, 580
deﬁnite logic program, 574
deﬁnition of a query, 582
degree invariant solution to Post’s prob-
lem, 466
degree of unsolvability, 443, 445, 484
degree theory, 444, 445, 447, 448
del Val, A., 604
demodulation, 164
Demolombe, R., 584
Denecker, Marc, 553
density theorem, 459, 460
deontic logic, 644
dependency pairs, 262
dependent types, 144, 161, 168
depository of test problems, 599
DeReS, 599
derived rule, 149, 150, 152
descent inﬁnite, 240
description logic, 316, 611, 659, 693,
700
description logics (DLs), 659
description operator, 224
desire, 323
nonmonotonic desire, 323
determinacy, 470
diagnosis, 308, 601
diﬀerent lattices for diﬀerent aggre-
gates, 597
diﬀerential ﬁxpoint, 591, 592, 594, 596,
597
diﬀerential rules, 595
diﬀuse statement, 160
DiPaola, R. A., 580
direct semantics, 694
disjoint concepts, 663
disjunctive Datalog, 573, 582, 588,
614, 615
multiple minimal models, 601
disjunctive deductive databases (DDDBs),
572, 573, 601
disjunctive logic program, 554
syntax, 574
disjunctive logic programming (DLP),
554, 572, 573, 589,, 601, 614
foundations, 573
syntas, 574
disjunctive stable models
computation method, 589
disjunctive well-founded, 606
disjunctive Datalog, 574
DisLop, disjunctive logic program, 599
distributed computing, 17
distributed Datalog systems, 596
Dix, J., 602, 604
DL systems, 666
DL-Lite, 669
DLP
Description Logic Programs, 668
Description Logic Prover, 667
historical perspective, 601
DLV, 591, 599, 600, 609
DLV (Datalog with Or), 589
DLVSystem s.r.l., 607, 610
DOL, disjunctive ordered logic, 606
domain calculus, 590
domain closure assumption, 583
domain closure axiom, 584
domination properties, 461
dot notation, 228
doxatic logic, 585
Doyle, J., 553, 605, 613
DRa, 350, 353, 354, 360–371, 374,
375, 383, 386, 387, 391
Drescher, C., 589
Dublin Core, 682
Dung, Phan Minh, 560
dyadic logic, 645
dynamic domains, 600, 612
and databases, 611

Index
717
dynamic logic, 637
dynamic magic sets optimization, 600
ECC, 144
Eckart, J., 31, 36
ECRC in Europe, 597
Edinburgh LCF, 148, 150, 151
Edinburgh LF, 146
Edinburgh Logical Framework, 146
EDVAC report, 35
EFS, 146
EHDM, 167, 168
eigenvariables, 233
Eigenvector computations and Disjunc-
tive Datalog, 604
Eiter, T., 578, 583, 588, 589, 600,
603–606, 611, 614
EKS-VI, 598
EL++, 664, 669
EL++, 669
ELAN, 271, 272, 274
elementary type theory, 225
Elf, 146, 243
ELK, 668
Elog language, 608
Emacs, 147, 183
embedding, 153, 194, 195
embedding problem, 474, 478
Emden, M. H. van, 572, 573, 576,
581, 601
empty clause, 528, 536
ENIAC, 31
Entscheidungsproblem, 35
epistemic entrenchment, 312
epistemic state, 283
equality, 240, 257
axioms, 584
equational approach to logic, 100
equational axiom, 257
equational deduction, 258
equational logic, 257
equational theory, 258
equilibrium logic, 327
equivalent concepts, 663
Ershov diﬀerence hierarchy, 478
Eshghi, Kave, 557
η-conversion, 216
η-reduction, 223
η-rehyperhypersimple, 448
event calculus, 612
EVES, 163, 169
Evidence Algorithm, 158
exact pair, 449, 450, 468, 469, 472
Exeura s.r.l., 607, 609
goal, DLV, 607
expansion proof, 244
explicit induction, 267
explicit negation, 554, 606
expressive power, 605
Datalog, 583
Extended Calculus of Constructions,
144
extended DLPs with multiple default
negations, 606
Extended Generalized Closed World
Assumption (EGCWA), 578
extensional database (EDB), 582
extensional predicate, 549
extensional type theory, 227
extensionality, 230, 231, 240
F-logic, 688, 697
Faber, W., 591, 600, 606
Fact, 667
Fact++, 668
fact clause, 524
factorization, 179, 181
Farkas’s lemma, 179
Febbraro, O., 607
feedback loops, 181
Feit-Thompson theorem, 146, 184, 192
Fern´andez, J. A., 577, 602
ﬁnd theorems, 184
ﬁnitary methods, 34, 35
ﬁnite extension argument, 452
ﬁnite extension method, 449, 451, 453,
455, 462
ﬁnite injury

718
Index
argument, 459
method, 454
priority argument, 454
priority method, 453, 462, 463
technique, 455
First Incompleteness Theorem, 166
ﬁrst website of information about logic
program systems and appli-
cations, 598
ﬁrst-order arithmetic, 477, 483
ﬁrst-order predicate logic, 581
Fitting, M., 549, 615
ﬁxed point semantics, 539–541
ﬁxed point theory, 642
ﬁxpoint operator for stratiﬁed pro-
grams, 601
ﬁxpoint semantics of Horn clauses,
591
ﬁxpoint semantics, DLP, 573
ﬂattening, 58
ﬂex-ﬂex pairs, 235
ﬂoating-point, 190, 195, 196
Flyspeck, 179, 184–186, 193
focused proof systems, 217
forcing, 444, 450, 452, 470, 481
formal equality, 535
formal proof, 135, 179, 183, 184
formalizability in principle, 190
formalization of questions, 580
formative years of deductive databases,
580
forward reasoning, 531
Fourth ASP Competition, 599
fourth-order matching, 235
frame problem, 526, 611
frame systems, 661
frames, 692
free algebra, 258
Frege, G., 32, 33, 344
Friedberg, R. M., 453, 456, 457
Friedberg–Muˇcnik, 454, 463
theorem, 458
Friesen, O., 597
full approximation, 464, 470, 471
function spaces, 113
functional dependencies (FDs), 585
functional extensionality, 226, 230
Furbach, U., 598
Fut´o, I., 582
fuzzy logic, 328
fuzzy set, 287
G¨odel, K., 34, 35
Gaasterland, T., 585, 586
Gal, A., 585
Gallaire, H., 544, 572, 573, 581, 582,
584
Galois connection, 577
Gamma operator, 577
GCWA, 614
Generalized Closed World Assump-
tion, 578
GCWA, ΠP
2 -complete, 604
Gebser, M., 588, 589, 600
Gelder, A. Van, 577, 586, 587, 595–
598
Gelfond, M., 553, 576, 578, 587, 589,
590, 595, 602, 603, 605, 609,
612, 614, 615
Gelfond-Lifschitz reduct, 550
general inclusion axiom, 662
general techniques for extending se-
mantics to extended disjunc-
tive logic programs, 606
generalized closed world assumption
(GCWA), 573
generic sets, 452
genericity, 452, 455, 485
geometric inconsistency, 115
geometric proof rules and inconsis-
tency, 117
geometrical proof rules, 115
geometrical proofs, 118
Georgeﬀ, M., 634
GETFOL, 178
Giannotti, F., 583, 596
Gioia-Tauro Seaport, 609
Girard paradox, 144

Index
719
GL-reduct PI, 577
Glue, 598
Glue-Nail, 598
gnd(P), 574, 577
goal clause, 524, 535
goal reduction, 533
goal tree, 533, 536
Godfrey, P., 585, 586
GOLOG, 612, 651
Google, 540
Gottlob, G., 571, 583, 603–605, 608,
614, 615
Grant, J., 585, 602, 605, 611
Great Logic Programming Schism, 555
Greco, G., 600
Green, C., 529, 580, 581, 611, 613
Green, T. J., 596, 608
green-green-green (ggg) constraint, 646
grep, 184
gross indecency, 35
ground reducibility, 268
ground term, 525
grounding
intelligent, 588
limiting size, 588
Groupe Bull, 597
Grundgesetze der Arithmetik, 344
Grundlagen der Analysis, 350, 383,
386
Grundlagen der Arithmetik, 344
Guess&Check, 600
Gurk, H., 579
GYPSY, 163, 167, 194
Hahn-Banach theorem, 147
Half, 147
hammer frameworks, 180
Hammer, M., 585
Han, J., 594
hardware design, 17
hardware veriﬁcation, 151, 156, 190,
193
Harel, D., 583, 586
Harrah, D., 580
Haskell, 146, 147, 155
Hayes, Pat, 531, 537
HCI, 183
HDM, 167
head of clause, 524
head–cycle free (HCF), 604
Hellerstein, J. M., 549, 591, 596, 598
HELM, 146
Henschen, L. J., 578, 594
Herbrand base, 542
HBp, 575
Herbrand interpretation, 542, 584
Herbrand interpretation I, 576
Herbrand state
minimal, 601
Herbrand universe, 540, 574
Herbrand’s theorem, 231
HermiT, 668
Hewitt, Carl, 530
hierarchical development methodology,
167
highn, 460
higher level priority arument, 476
high-low hierarchy, 460
higher-order matching, 235
higher-order quantiﬁcation, 219
higher-order substitution, 219, 232
higher-order uniﬁcation, 155
Hilbert formulation is a proof system,
43
Hilbert, D., 34, 35
Hill, Robert, 540
Hindley-Milner, 151
Hoare logic, 194
HOL, 140, 151–156, 161, 167–171, 175,
177, 179, 182–184, 186, 187,
189, 190, 194, 195, 241, 349
HOL Light, 153, 173, 175, 177, 179,
180, 182–187, 189, 190, 241
HOL Zero, 183
HOL-in-HOL, 189
HOL4, 182, 183, 185, 189, 196
HOL88, 152, 153
hol90, 153, 182

720
Index
hol98, 153
homogeneity conjecture, 469, 470
homogeneity problem, 465, 466
homotopy type theory, 144, 146, 171
Horn clause, 524, 535–537, 581
Horn clause rules, 574, 582
Horn DB, 585
Horn logic, 700
Horn theories, 572, 573
Horn, Alfred, 524
human-computer interaction, 136, 183
hy+, 598
hyper-resolution, 531, 540–541, 576
hypersimple sets, 447
hypothetical reasoning, 299
IC-Prolog, 538
ICs, 585, 590
and computational eﬃciency, 585
IDP (Imperative-declarative Program-
ming), 589
IFIP Congress, 1974, 581
ignorance, 300, 326
complete ignorance, 288
Imielinski, T., 603
immediate consequence operator, 541,
542
Imperial College, London, 581
impredicative deﬁnitions, 219
IMPS, 156, 169, 172, 242
inclusion dependencies, 585
incomplete knowledge, 577
inconsistency, 300
inconsistency level, 296
inconsistency subset, 319
independence, 311, 312
possibilistic independence, 305
index set, 459, 460
induction, 145, 149, 153, 164–166, 169,
174, 177, 218, 231, 240, 556
by consistency, 267
by rewriting, 269
for proving properties of logic pro-
grams, 546
invariants, 240
inductionless induction, 240, 267
inductive logic programming, 600
inductive theorem, 267
inductive types, 141, 144
industrial companies for Datalog, Dis-
junctive Datalog, 606
inference rule, 137, 148–150, 153, 155,
160, 175, 176, 188, 189
inference system, 531
INFEREX language, 579
inﬁnite injury method, 457–459, 472
priority method, 462
technique, 455
inﬁnite loop, 538
inﬁnity, 226
inﬂationary inductive deﬁnition, 553
information
bipolar information, 323
fusion, 313, 317, 324
negative information, 323
positive information, 323
initial algebra, 258
initial proof-state, 381
initial segment, 452, 463, 464, 466–
469, 471, 474, 477
Inka, 267
Inoue, K., 603
input output logics, 111, 646
instance of a concept, 663
instance of a role, 663
instance retrieval, 664
instantiation, 238, 239
integration problems, 603
integrity checking (Soundcheck), 597
integrity constraints, 557, 558
as a modal logic, 584
as a theorem of the DDB, 584
as modal logic, 584
in deductive databases, 582
intelligent answering systems, 581
intelligent switches, 93
intensional, 144
intensional database (IDB), 582

Index
721
intensional predicate, 549
intention to be, 633
intention to do, 633
interactive theorem proving, 135–139,
147, 174, 186, 195
interpolation, 181
intuitionistic, 142, 169, 170
intuitionistic logic, 227
invariance under automrophisms, 465
is-a relation, 681
Isabelle, 153, 155, 156, 162, 175, 180,
182–185, 266, 343, 347–351,
353, 369
Isabelle theorem prover, 241
Isabelle/HOL, 153, 155, 156, 163, 170,
182, 195, 242, 244, 246
Isabelle/HOLCF, 169
Isabelle/JEdit, 184
Isabelle/jEdit, 183
Isabelle/ZF, 155, 169
IsaPlanner, 242
Isar, 156, 175, 349, 354, 387
iterated ﬁxpoint, 595
iterated inductive deﬁnition, 553
iterative computation, 592
Ja´skowski-Fitch, 142, 159, 175
Janhunen, T., 600, 608
Jepersen, B., 580
join order optimization, 598
Jones, A. J. I., 584
Journal of Formalized Mathematics,
185
jump, 449, 452, 456, 458, 465, 466,
468–470, 473, 476, 478, 480–
483
classes, 460, 461, 483, 485
interpolation theorem, 460
operator, 446, 448, 450
K-axiom, 630
Kakas, A. C., 599
Kalman ﬁltering, 313
Kang, J.-H., 594
KAON2, 668
KARO logic, 637
Kasher, A., 580
Kautz, H.A., 604
Kellogg, C., 582
Kepler conjecture, 192
kernel, 145, 163, 178, 187, 188, 190,
195
KeY, 243
Kifer, M., 591
King J., 585
KIV, 163, 194
KL-One, 662, 692
KL1, 538
Klahr, P., 582
Knowledge in Action, 612
knowledge
-based systems, 571, 573
bases, 590
representation, 17
representation and reasoning (KRR),
659, 660
Koch, C., 608
Kolaitis, P., 595, 596
Konclude, 668
Kowalski, R. A., 572, 573, 576, 581,
582, 584, 599, 601, 612, 615
Kraus, S., 605
Kripke semantics, 630
Kris, 666
Krypton, 662
Kuehner, D., 533, 535, 581
Kuhns, S., 579, 580
Kunen, Kenneth, 549
L2, 667
L-stable model, 577
disjunctive Datalog, 578
L4, 195, 196
labelled deductive system, 52, 293
Lachlan’s nonsplitting theorem, 476,
477, 478, 482
LAMBDA, 155, 156
λ calculus, 142
λ-Clam, 242

722
Index
λ cube, 144
λ-abstractions, 216
λ-conversion, 216
λ-terms, 222, 235
λProlog, 237, 241
Landau, 350, 353, 383–387, 389, 390,
396
large-theory ATP, 185
lattice of set containment, 576
lattice representation, 467
Lausen, G., 596
law, 15
LCF, 145, 147–153, 155, 156, 158,
163, 169, 173, 175, 177, 180,
183, 187, 190, 241
LDL, 598
LDL++, 596, 598, 600
least ﬁxpoint, 576, 577
Lebesgue measure, 463
LEGO, 145
Leibniz equality, 225, 231, 240
Leibniz, G. W., 31, 36
LeLisp, 150
Lenzerini, M., 605
LEO-II, 243, 244, 246
Leone, N., 578, 588, 589, 599, 604,
606, 607, 609n, 610, 615
Levesque, H., 612, 633
Levien, R., 579
Levy, A., 585, 611
Lewis imaging, 313
LF, 139, 141, 142, 146
Lierler, Y., 589
Lifschitz, V., 554, 576, 578, 587, 589,
595, 602, 605, 611, 613, 614
LIL, 157, 158
linear arithmetic, 145, 166, 179
linear recursion
0-sided, 582, 594
1-sided, 594
rules, 593
linear resolution, 532
linear rules, 594
Linear Temporal Logic, 195
linguistics, 15
linkage, 160
LISP, 150, 152, 153, 161, 164, 166,
168, 169, 195, 529, 534
Lixto semi-automatically creates wrap-
per programs to translate Html
pages into Xml, 608
Lixto Software GmbH, 607
Lixto, Datalog-based extraction en-
gine, 608
Lloyd, J., 548, 571, 573, 576, 584
LOA (Logic of Agent Organizations),
643
Lobo, J., 573, 585, 601, 602
local constant, 160
local query answering, 599
local strategies, 273
locally stratiﬁed
program, 591, 595
rules, 596
theories, 586
negation, 551–552
logic and databases, 571, 572, 574
logic for data description, 582
Logic of Computable Functions, 147
logic programming, 16, 308, 571, 572,
605, 613, 696
and nonmonotonic reasoning (LP-
NMR), 598
theory, 573
logic translation, 232
logic variables, 233
Logic-Base, 598
Logic-based AI (LBAI) Workshop 1999,
600
Logic-Based Artiﬁcial Intelligence, 615
logic-based formalisms, 660
Logic-Information Languages, 157
logical connectives, 224
logical design and veriﬁcation of com-
puter software and hardware,
16
logical framework, 142, 145, 153, 195
logical variable, 540

Index
723
LogicBlox, 607
LogicBlox, Inc., commercial software
company, 608
logicism, 219
LogiQL, 608
features, 608
Logosphere, 183
Logres, 598
LOLA, DDB for modular stratiﬁed
programs, 599
LONGAL, 142
LONGPAL, 142
Loo, B. T., 596
Loom, 666
LORA (the Logic Of Rational Agents),
637, 643
Loveland, D., 533, 573
low-high hierarchy, 461
LPNMR, 599
LSM, 151
Ltac, 146, 176
LTL, 195
Lukasiewicz, T., 611
M-stable model, 577
disjunctive databases, 578
disjunctive Datalog, 578
machine learning, 180, 181, 185, 596
machine translation, 157
MacLisp, 150
magic counting, 594
magic sets, 591, 594, 597, 598, 614
optimization, 602
supplementary, 594
transformation, 548, 562
Maier, D., 590
Mannila, H., 603, 614
Manthey, R., 597
many-one reducibility, 444, 447, 469
many-sorted algebras, 259
map colouring problem, 555, 557
Maple, 179
MapReduce, 540
Maratea, M., 589
Marek, V., 555, 587, 588
Marill, R., 580
Maron, M. E., 579
Martin’s conjecture, 461
Martin-L¨of’s type theory, 144, 147,
169
Maryland refutation proof procedure
3.0 (MRPPS 3.0), 582
Mathematical Proof Language, 380
mathematical vernacular, 143
Mathematics WikiProject, 186
MathLang, 349–354, 359, 360, 364,
369, 391, 392
Matita, 146, 188
Maude, 271, 272, 274
maximal set, 448, 454, 457, 460
Mazuran, M., 595, 597
MCC, 597, 598
McCarthy’s planning problem, 600
McCarthy, J., 531, 553, 580, 605, 611,
613, 615
McDermott, D., 553, 605, 613
McSkimin, J. R., 585
measure theory, 451
mechanical engineering, 615
mechatronics, 15
medical diagnosis, 588
Meta Content Framework (MCF), 682
meta-implications, 153
meta-interpreter, 537–538
meta-logic program, 526, 537
metafunctions, 178
MetaMath, 185
metamorphosis grammar, 535
MetaPRL, 145
metavariables, 155
micro-Planner, 534
Milawa, 190
Milnikel, R., Jr., 599, 614
minimal (or Scott–Montague) mod-
els, 631
minimal cover, 466, 470, 478
minimal degree, 450, 462, 463, 467,
470, 471, 478, 480

724
Index
minimal Herbrand model
MP, 576
disjunctive logic program, 577
minimal method, 464
minimal model state, 577
minimal models, 573, 587
semantics, 541–543
minimal pair, 463–465, 472, 473, 475,
478
Minker, J., 544, 571–573, 577–582,
585–588, 598, 600–602, 605,
606, 611, 614, 615
Minlog, 237
Minsky, M., 613
mixed quantiﬁer preﬁx, 236
Mizar, 136, 143, 156–163, 169, 171,
173, 175, 176, 180, 183–185,
189, 190, 192, 343, 346–351,
353, 354, 369, 370, 374, 383,
386, 387, 396
Mizar FC, 161
Mizar Mathematical Library, 161, 162,
184
Mizar MS, 161
Mizar Proof Advisor, 185
Mizar-2, 161
Mizar-3, 162
Mizar-4, 162
Mizar-PC, 158–160
Mizar-QC, 158, 160, 161
MKRP, 139
ML, 145, 148–153, 156, 161, 169, 175,
182, 190, 193, 195
MML, 161, 162, 192
Query, 184
modal logic, 89, 629
modal operator, 587
model checking, 139, 176, 588, 650
model elimination, 155, 533
model theoretic semantics
DLP, 573
normal logic programs, 573
models of equational logic, 258
modes of evaluation, 84
modularity, 605
modular stratiﬁcationn, 598
properties, Disjunctive Datalog,
604
Modus Ponens, 140, 148, 155, 177,
295, 579, 581
monotonic aggregates in recursion, 597
monotonic operator, 576
monotonicity, 44
monstrous injury method, 467
Moore, J, 524
Moore, R. C., 553, 605, 613
Morak, M., 615
Morris, K. A., 591
most-general uniﬁer, 235
Moufang identities, 266
Mourlas, C., 599
MPL, 349–352, 380, 382, 383, 391,
392
MRPPS, Maryland refutation proof
procedure system, 581
Muˇcnik, A. A., 453, 457
mulit-agent BDI logic, 642
multi agent systems, 17
multi-agent epistemic logic, 640
multiple agent logic, 318
possibilistic multiple agent logic,
319
multiple permitting, 462
Mumick, I. S., 595
MYCIN, 283, 661
n-generic sets, 452
N-Triples, 683
N´emeti, 542
NAIL!(Not another implementation of
logic!), 598
NASA Shuttle Planner, 609
natural deduction, 137, 138, 140, 142,
159, 174, 175
natural language, 143, 158, 170
understanding, 531, 534, 535
Naughton, J., 582, 594
necessity

Index
725
(strong) necessity measure, 290,
291
comparative necessity, 312
weak necessity measure, 291
needed redexes, 273
negation as failure, 327, 527, 544–
546, 548–556, 613
negation-as-ﬁnite-failure, 546, 582
rule, 584
negative body, 574
neighbourhood semantics, 631
Neotide, a Finnish commercial ﬁrm,
599, 607
licensed Smodels, 608
nested strategies, 458
network databases, 613
network-based structures, 661
Neuhold, Erich, 543
Newman’s lemma, 350, 355, 377, 380
Newtonian physics, 615
Next Century Media, Inc., 597
Nicolas, J. M., 544, 572, 573, 581,
582, 584, 585, 597
Niemel¨a, I., 555, 588, 589, 599, 600,
606, 608
Nitpick, 245
nncappable degrees, 473
noetherian induction principle, 267
Nogood learning, 589
Nogueira, M., 609
non-deterministic semantics, 108
non-functional models, 230
non-linear rules, 593
non-logic-based formalisms, 660
non-monotonic logics, 701
non-monotonic reasoning, 526, 554,
561
non-recursive deﬁnite logic program,
594
non-standard models, 221
non-stratiﬁed
deductive databases, 586
logic program, 576
systems, 598
non-stratiﬁed databases, 588
nonbranching degree, 473, 477
noncappable degree, 474
noncuppable degree, 475
noncupping degree, 474
nondeterministic choice constructs, 598
nonmonotonic reasoning, 310, 571–
573, 588, 589, 591, 594, 603,
605
normal form, 261
normal logic programs, 526, 574, 576
stratiﬁed, 576
normalization of general linear recur-
sions due to chain recursions,
594
NP-completeness, 495, 498, 583
NQTHM, 164–166, 168, 169, 174, 178
Nqthm-ACL2, 267
null value, 605
Nullstellensatz, 179
NuPRL, 139, 141, 144, 145, 156, 169,
174, 178, 182, 243, 349
OBJ, 259
object-oriented
features, 597
programming, 591
OCaml, 146, 148, 153, 188, 190
occur check, 559
OIL (Ontology Inference Layer), 692
oldest planning problem, 613
ΩMEGA, 242
one-axiom group theory, 266
one-one reducibility, 447
Ontic, 162, 180
OntoDLV, 609
ontologies, 679
Ontology-based Data Access (ObDA),
669, 701
open minded committed agent, 636
OpenTheory, 183
operational semantics, 591, 596
based on iterated applications,
595

726
Index
Operator for positive disjunctive logic
programs
T s
P, 577
T INT
P
, 577
optimal reduction strategy, 273
optimization
techniques, 571
oracle, 445
Turing machine, 443, 445
order-sorted algebras, 259
ordered rewriting, 264
ordered structures, 583
oriented equality, 256
orthogonal rewrite systems, 273
Otter, 266
Otter-λ, 243
overview of computational logic, 19
OWL, 691, 693
P-stable model, 577
PageRank, 549
PAL, 142
Palopoli, L., 595, 604
Papadimitriou, C., 595, 596
paraconsistency degree, 301
parallelization, 698
parallelizing, 184
paramodulation, 267
Parlog, 538
parsing, 588
partial, 156, 169, 171, 172, 182
evaluation, 605
Herbrand interpretation, 575, 577
stable models, 561, 578
subsumption algorithm, 585
particle bombardment, 190
Pascal, 158, 161, 167, 175, 183
pattern uniﬁcation, 236
PC-Mizar, 161, 162
Peano arithmetic, 546–547
Pearce, D., 587
Pearl, J., 612
Peirce, Charles Sanders, 556
Pelet, 668
penalty logic, 297
Pereira, L. M., 605
perfect discrimination trees, 185
perfect model, 550–552, 586
stratiﬁed logic program, 576
perfect model semantics, 592
perfect models, 595
of disjunctive databases, 602
permitting method, 455
Π0
1-classes, 484
π0
2-complete, 614
Pichler, R., 596
pinball machine, 458
Pirri, F., 612
PL/CV, 145, 156, 174, 175
Planner, 530, 533, 534, 537
Plus cupping theorem, 474, 475
Polleres, A., 611
polymorphic, 144, 148, 170, 187
polynomial hierarchy, 499, 603, 604
Poole, D., 557
POP-2, 164
positive body, 574
positive Datalog program, 574
positive disjunctive logic program, 601
possibilistic logic, 284, 293
basic, 293, 294
generalized, 324
strong possibility-based, 320, 322
timed, 317
possibilistic network, 304
possibility
(weak) possibility measure, 290,
299
comparative possibility, 286
conditional possibility, 305
guaranteed possibility measure,
290
strong possibility measure, 290,
319
possibility distribution, 287
normalization, 288
possibility theory, 283
Post canonical systems, 543

Index
727
Post’s problem, 443, 447, 453–456,
460, 466
Post’s program, 447, 448, 456, 460,
466
Post, E. L., 443, 447, 453
Potassco, the Potsdam Answer Set
Solving Collection, 600
PRA, 170
practical reasoning, 61
pre-history of deductive databases, 579
pre-uniﬁcation, 235
pre-uniﬁers, 235
predicate calculus, 571
terms, 574
predicate completion, 544
predicate transformer, 193
predicate/set variable, 238, 239
predicative, 144, 146
preferred answer sets, 606
premise selection, 180, 185
PRESS, 537
Prime Number Theorem, 192
Primitive Automath Language, 142
primitive recursive arithmetic, 169
primitive substitution, 239
primitive types, 222
Principia Mathematica, 137, 191, 219
priority
information, 606
level, 293
method, 444, 447, 450, 454, 456
tree, 458, 459
PRL, 145
probabilistic reasoning with ASP, 612
probability, 299
Baconian probability, 286
procedural interpretation of Horn clauses,
535
procedural representation, 527, 530–
531, 539, 563
production systems, 533
proforma theorem, 177
Programmar, 534
programming languages, 17
PROLOG, 36, 535, 537, 582, 590, 613
programming languages, 572
top-down problem solver, 572
Prolog II, 538, 559
promptly simple, 461
degrees, 483
sets, 473
proof
as programming, 183
as structure editing, 183
by consistency, 240, 267
by pointing, 183
General, 146, 183, 184
object, 139, 140, 178, 183
planning, 166, 177
procedure, 531
normal logic programs, 573
skeleton, 353, 370, 371, 374, 383,
386–388, 390, 391
generation, 350, 354, 362, 366,
369–371, 383, 388, 391
sketch, 351, 353, 360
space, 531
system, 43
systems, 343, 347–349, 351, 352,
391
theoretic semantics, DLP, 573
tree, 532
-length, 216
-script, 349
-state, 381
ProofPower, 153, 155, 183, 242
ProofWeb, 183, 185
Prototype Veriﬁcation System, 167
provability
degree of provability, 286
Proviola, 185
pruning technique, 588
Przymusinski, T. C., 551, 578, 586,
595, 602, 606
pseudo jump, 481
pseudo jump operator, 478, 480
PTIME-completeness, 583
Pure LISP theorem prover, 164, 166

728
Index
pure type systems, 144
PVS, 167–169, 171, 186, 194, 242,
266
Q-reducibility, 448
Q-systems, 534
QA3, 529, 530, 536
QED, 162, 163, 183
QTHM, 165
qualitative choice logic, 322
quantiﬁcation, 217
in query systems, 580
quantiﬁer alternation, 233
quantiﬁer elimination, 177
quantiﬁer games, 89, 90
quantiﬁer-free, 138, 163, 169, 177, 179
quasi-classical logic, 302
query
folding, 611
relaxation, 586
question answering system
QA-1, 580
QA-2, 580
QA-3, 580
question answering systems, 16
question-answering, 528, 534, 535, 543
quotient types, 144, 152
Race, 667
Racer, 668
raising, 236
Rajasekar, A., 573, 577, 601
Ramakrishnan, R., 573, 591, 594, 598
ramiﬁcation problem, 611
ramiﬁed type theory, 215, 219
randomness, 452, 484, 485
range-restricted, 574
ranking function, 297, 307
Rao, A., 634
Rao, P., 588, 591, 599, 600
Raphael, B., 581
rational trees, 559
RCS, maneuvering system for orbit-
ing NASA shuttle, 609
RCS, Reaction Control System, 609
RDBMS, 614
RDF
graph, 686
interpretation, 686
schema, 688
-based semantics, 694
RDFa, 683
RDFS semantics, 689
RDS system, Rand Corporation, 579
reactive automata, 88
reactive grammars, 88
reactive paradigm, 86
reactive proof system, 88
reactive proof theory, 97
read-eval-print loop, 156, 175, 183
reasonable questions, 579
recursion, 444, 598, 613
k-sided, 594
recursive predicates, 593
recursive queries
in SQL: 2003, 582
part of SQL standards, 614
processing for Datalog, 593
recursive rules in databases and logic
programs, 582
recursively enumerable, 445, 446
reductio ad absurdum, 527
reduction
eager, 273
lazy, 273
leftmost-outmost, 273
needed, 273
optimal, 273
reduction ordering, 261
redundancy, 529, 533
reﬁnement, 145, 194
reﬂection, 176, 178
reﬂexivity, 44
refutation procedure, 527
refute, 245
reiﬁcation, 702
Reiter, R., 533, 544, 553, 573, 577,
581–585, 605, 612, 613
relational

Index
729
algebra, 571, 583
algebra (RA) operators, 591
calculus, 571, 583
database management system
databases, 543, 571, 574, 579, 590,
613
facts, 582
model, 583
relative computability, 445
relevance logic, 532
reliability, 150, 186, 187, 194
replacement of equals by equals, 258
Reservations, 162
residues method in SQO, 585
resolution, 147, 155, 164, 181, 295,
527–529
weakest link resolution, 295
resolvent, 528
Resource Description Framework (RDF),
680
restricted monotonicity, 46
reverse mathematics, 484
REVISE, revises contradictory extended
logic program, 599
rewrite proof, 263
rewrite system, 261
rewriting, 145, 163–165, 168, 256
rewriting calculi and logics, 256, 271,
272
rewriting modulo a set of equalities,
264
rewriting with constraints, 265
rho-calculus, 272
Ricca, F., 609
rippling, 166
Robbins problem, 266
Robinson
resolution, 580, 581
Robinson, J. A., 527, 572
robotics, 600
role assertion axioms, 663
role inclusion axioms, 663
Ross, K. A., 577, 587, 595, 597, 598,
606
Roussel, P., 535
RRL, 269
RTA, 274
Ruiz, C., 577, 578, 587
rule clause, 524
Rule Interchange Format (RIF), 697
rule-based system, 533
RuleML, 697
Rullo, P., 588
Rushton, N., 612
Russell, B., 32, 33, 344, 345
Sable, J., 580
Sacc`a, D., 577, 578, 592, 594
Sacks’ jump theorem, 458
Sacks’ splitting theorem, 458, 473
SAD, 158
Sadri, F., 584
Sagiv, Y., 582, 585, 594, 595, 597
Sagonas, K., 588, 607
Sakama, C., 603
SAM, 137, 138, 158
Sandewall, E., 611
SAT, 138, 176, 181
solver, 589
Satallax, 245, 246
satisﬁability checking, 138
satisﬁability modulo theories, 138
satisﬁable
concept, 663
Schaerf, M., 605
Schaub, T., 600
Schlipf, J. S., 577, 587, 590, 604
Schoenﬁeld’s conjecture, 465
Schoenﬁeld’s limit lema, 471
Scott, D., 539
SDVS, 163
search strategy, 531, 536, 580
SearchAbout, 184
Second Incompleteness Theorem, 189
second-order arithmetic, 468, 469, 478,
480–482
second-order logic, 220
(RDBMS),  590

730
Index
second-order matching, 235
second-order uniﬁcation, 234
Seipel, D., 571, 573, 577, 578, 594,
600–602, 606
selective backtracking, 538
Selman, B., 604
semantic
information retrieval (SIR), 580
interpretation, 80
query optimization (SQO), 585
networks, 661
Web, 17, 185, 591, 611, 679
ontological reasoning, 597
Rules Language SWRL, 697
semantics, 121
disjunctive deductive databases
framework, 602
disjunctive logic programs, 577
normal logic programs, 576
of deﬁnite logic programs, 576
selecting based on properties, 604,
605
translation from natural language
to logic, 605
for a modular approach, 82
heterogeneity, 699
of Horn clauses, 539–542
Semi-Automated Mathematics, 137
semi-naive
evaluation, 597
ﬁxpoint optimization, 592
semi-valuation method, 230
SEMIPAL, 142
Sergot, M., 612
Seshadri, P., 591
set of support, 532
set theory, 143, 155, 158, 162, 169,
170, 187, 189, 222
set variable, 238
shadow syntax, 177, 178
shallow embedding, 194
shallow semantic corpora, 186
Shanahan M., 612
shell principle, 166
Shepherdson, J., 549, 577
Sherlock Holmes, 533
shift-operations, 604
sideways information passing, 594
Simons, P., 588, 599, 600, 608
simple entailment, 687
simple sets, 447
simple types, 215, 219, 222, 226
simpliﬁcation, 148, 150, 151, 163, 164,
166
ordering, 262
single-minded committed agent, 636
situation calculus, 526, 531, 611, 613,
651
Skolem function, 542
Skolem-L¨owenheim-Herbrand theorem,
542
Skolemization, 232, 233, 237
SL resolution, 533, 534
linear resolution with selection
function, 581
SLD refutation, 536
SLD resolution, 535–537, 594
SLDNF, 549
Sledgehammer, 180
SLG resolution, 553, 588
smallest limit ordinal, 576
Smith, T., 607
SML, 147
Smodels, 589, 599, 600
computes stable model semantics
in Datalog, 588
Smodels solvers for non-disjunctive pro-
grams, 600
SMS
SMS-ASP, 614
SMS, stable model semantics, 588
SMT, 138, 181
Smullyan, R., 542
software engineering, 17
software synthesis, 16
software veriﬁcation, 163, 193
soundness of Skolemization, 233
space hierarchy, 497

Index
731
SPARQL, 690
SPECIAL, 167
Special Interest Group of the Asso-
ciation for Computing
special linear recursions, 594
speciﬁcity, 288
principle of minimal speciﬁcity,
288, 290
sphere
system of spheres, 286
Spike, 269
SQL, 590
SQL: 2003, 575
standard, 575, 591
language, 583
SQO
bottom-up, 585
decidability results, 585
in DDBs, 585
in relational databases, 585
semantic query optimization, 585
with negation in body of a clause,
585
with recursive IDB rules, 585
SQPRQL, 611
square of opposition, 292
SRI, Stanford Research Institute, 580
SSReﬂect, 146, 176
stable circumscriptive semantics, 605
stable default negation, 583
stable expansions, 587
stable model, 553–555, 557–558, 561,
576, 595, 598, 606
history, 587
semantics (SMS), 576, 587, 602,
614
standard model, 229
of arithmetic, 220
Stanford LCF, 148, 150
Starburst, 598
static semantics, 602
stit theory (seeing to it that), 646
STP, 167
strategic companies problem, 603
Stratego, 274
strategy language, 274
stratiﬁed, 591
stratiﬁed databases, 586
stratiﬁed DDBs unique preferred min-
imal model, 586
stratiﬁed default negation, 598
stratiﬁed logic programs, 614
stratiﬁed negation, 549–551
stratiﬁed non–disjunctive, 604
stratiﬁed systems, 598
stratiﬁed, disjunctive deductive the-
ories, 602
strong homogeneity problem, 465
strong negation, 554, 606
strong normalisation, 230
strong realism, 635
strong reducibility, 444, 447, 484
structural subsumption algorithms, 666
structured-consequence relation, 50
subformula property, 231
Subrahmanian, V. S., 603, 604, 614
subsumed
concepts C by D, 663
subsumption, 185
Sugeno integral, 322
superposition, 164
calculus, 266
surgical cut, 50
surprise
degree of potential surprise, 285,
287
survey articles, 602
on automated reasoning, 19
on computational logic, 19
on logic programming, 19
Swift, T., 588
symbolic weight, 303
symmetric negation, natural solution
to update and belief revision,
606
SYNTEX, 581
Syre, J.-C., 581
Machinery (SIGLOG), 29

732
Index
Syrj¨anen, T., 606
system LDL, 597
Szeredi, P., 582
T¨arnlund, Sten-˚Ake, 542
tabeled logic programming, 607
tableau-based algorithms, 666, 667
tableaux, 155
based algorithms, 666, 667
formulation, 43
tabling, 539, 553
Tarski-Knaster Theorem, 541
TBox, 662
Team Building System, for Gioia-Tauro
Seaport, 609
Team Building System, written in DLV
dialect of ASP, 610
terminating modulo, 264
termination, 261
test set induction, 269
TEXMacs, 184
Teyjus, 237
Thagard, P., 533, 539
The International Federation for Com-
putational Logic (IFCoLog),
28
Theorem Prover, 347, 350, 353, 354,
370, 371, 374, 375, 383, 385,
392
theorem proving, 572, 613
Theorist, 556
Theory of Evidence, 62n
theory versus interpretation, 548
THF syntax, 243
third-order matching, 235
third-order uniﬁcation, 234
THM, 165
three valued model, 577
three-valued completion semantics, 549
three-valued consequence operator, 552
three-valued interpretation, 552, 561
three-valued stable model, 561
Time-Action-Revision Logic (TAR-logic),
74
time-hierarchy, 497
tmEgg, 184
TOM, 274
tools for (disjunctive) Datalog, 610
top-down and bottom-up evaluation,
614
top-down reasoning, 527, 532
topology, 227
Topor, R. W., 584
TORPA, 262
total ω-computable enumerability, 462
totalization, 171, 188
TPS proof assistant, 241, 244
TPTP infrastructure, 243
tractable
disjunctive theories, 603, 604
knowledge bases, 604
DLs, 668
transitive closure, 548, 549, 553, 592
transitivity (cut), 44
travelling salesman problem and Dis-
junctive Datalog, 604
Travis, L., 582
Truszczy´nski, M., 555, 587, 588, 598
truth-table reducibility, 444, 447, 469
TSa, 354, 357–361, 363, 383–385, 387,
391
Tsur, S., 597
TTT, 262
tuple calculus, 590
Turing, A., 33, 34, 36, 443, 632
completeness of Horn clauses, 542
degrees, 444, 446, 447
machine, 443, 496
reducibility, 443–446
Turtle, 683
Twelf, 141, 147, 183, 243
type classes, 145, 155, 162, 182
type correctness conditions, 168
type elimination, 668
type expressions, 222
type hierarchy, 220
type instantiation, 188
type system, 148, 160–162, 168–171

Index
733
type theory, 141, 142, 144–146, 155,
156, 163, 169, 182, 187
Ueda, K., 538
Ullman, J. D., 571, 573, 597, 598
unbounded query, 577
uncertainty, 283
in databases, 315
uncountable, 189
undeﬁnability of truth, 189
uniﬁcation, 233, 236
or matching modulo, 234
uniﬁer, 234, 528
uniform proof procedure, 530, 532
unique name assumption, 583, 694
unique names axiom, 584
unitary cut, 44n
univalent foundations, 171
univalent type theory, 144
Universal Type Theory, 144
universe polymorphism, 145
unsolvability, 17
updates, 598
deductive databases, 585
in disjunctive databases, 602
updating, 313
user constraints, 584, 585
utility
optimistic utility, 314
pessimistic utility, 314
v-complexes, 230
Vadaparty, K., 603
VALIDITY DDB, 597
van Emden, M., 535, 539
Van Gelder, A., 550, 553, 562
Vardi, M.Y., 603
variable capture, 187, 189
veriﬁcation conditions, 194
Verisoft, 195
Vielle, L., 597
Vienna Circle, 34
views in relational databases, 582
von Neumann, J., 31, 35
Voronkov, A., 583, 603
W3C, 679
Walker, A., 586
Warren, D., 524, 588, 599, 600, 607
Watson, 243
wavefront techniques, 594
weak truth table reducibility, 469
weak truth-table degree, 477
web
ontological reasoning
semantic web, 597
Web and big data, 610
Web Ontology Language, 693
webcrawler, 549
WebdamLog, 596
Welham, Bob, 537
well-founded, 602
circumscriptive semantics, 605
default negation, 583
models, 577, 595
semantics (WFS), 552–553, 562,
576, 577, 587, 596, 598–600,
607, 614
WFS semantics
implementation, 588
DDBs unique three valued model,
587
Whelp, 184
The Whetstone of Witte, 255
Wiedijk, F., 349
Wikipedia, 185, 186, 591
Winograd, T., 530, 534
wiskundige omgangstaal, 143
Wiskundige Taal AUTOMATH, 143
WITAS Project, 589
Wooldridge, M., 637
World Wide Web, 185, 679
WOT, 143
Wright, G. H. Von, 644
X4, 598
Xml, 591
XRay, 599
XSB, 589, 591, 598–600
indexing algorithms, 599

734
Index
Prolog, 613
tabling algorithms, 599
XSB Inc., 588, 607
D. Warren, founder, 599
Prolog, 597
deductive logic programming sys-
tem for WFS, 588
XSB, full Prolog system, handles
HiLog terms, 599
XY-stratiﬁcation, 598
XY-stratiﬁed programs, 600
Yahya, A. H., 578, 602
Yazdanian, K., 582
You, J.-H., 602, 605
Yuan, L. Y., 602, 605
Zaniolo, C., 571, 592, 596–598, 600,
605
Zdonik, S., 585
Zermelo-Fraenkel set theory, 464
Zhang, W., 594

