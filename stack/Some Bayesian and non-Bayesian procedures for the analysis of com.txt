Retrospective Theses and Dissertations
Iowa State University Capstones, Theses and
Dissertations
1989
Some Bayesian and non-Bayesian procedures for
the analysis of comparative experiments and for
small-area estimation: computational aspects,
frequentist properties, and relationships
Frederick Landis Hulting
Iowa State University
Follow this and additional works at: https://lib.dr.iastate.edu/rtd
Part of the Statistics and Probability Commons
This Dissertation is brought to you for free and open access by the Iowa State University Capstones, Theses and Dissertations at Iowa State University
Digital Repository. It has been accepted for inclusion in Retrospective Theses and Dissertations by an authorized administrator of Iowa State University
Digital Repository. For more information, please contact digirep@iastate.edu.
Recommended Citation
Hulting, Frederick Landis, "Some Bayesian and non-Bayesian procedures for the analysis of comparative experiments and for small-
area estimation: computational aspects, frequentist properties, and relationships " (1989). Retrospective Theses and Dissertations. 9053.
https://lib.dr.iastate.edu/rtd/9053

INFORMATION TO USERS 
The most advanced technology has been used to photo­
graph and reproduce this manuscript from the microfilm 
master. UMI films the text directly from the original or 
copy submitted. Thus, some thesis and dissertation copies 
are in typewriter face, while others may be from any type 
of computer printer. 
The quality of this reproduction is dependent upon the 
quality of the copy submitted. Broken or indistinct print, 
colored or poor quality illustrations and photographs, 
print bleedthrough, substandard margins, and improper 
alignment can adversely affect reproduction. 
In the unlikely event that the author did not send UMI a 
complete manuscript and there are missing pages, these 
will be noted. Also, if unauthorized copyright material 
had to be removed, a note will indicate the deletion. 
Oversize materials (e.g., maps, drawings, charts) are re­
produced by sectioning the original, beginning at the 
upper left-hand corner and continuing from left to right in 
equal sections with small overlaps. Each original is also 
photographed in one exposure and is included in reduced 
form at the back of the book. These are also available as 
one exposure on a standard 35mm slide or as a 17" x 23" 
black and white photographic print for an additional 
charge. 
Photographs included in the original manuscript have 
been reproduced xerographically in this copy. Higher 
quality 6" x 9" black and white photographic prints are 
available for any photographs or illustrations appearing 
in this copy for an additional charge. Contact UMI directly 
to order. 
University Microfilms International 
A Bell & Howell Information Company 
300 North Zeeb Road, Ann Arbor, Ml 48106-1346 USA 
313/761-4700 
800/521-0600 


Order Number 9008538 
Some Bayesian and non-Bayesian procedures for the analysis 
of comparative experiments and for small-area estimation: 
Computational aspects, fîrequentist properties, and relationships 
Hulting, Frederick Landis, Ph.D. 
Iowa State University, 1989 
U M I  
300N.ZeebRd. 
Ann Arbor, MI 48106 


Some Bayesian and non-Bayesian procedures for the analysis 
of comparative experiments and for small-area estimation: 
computational aspects, frequentist properties, and relationships 
by 
Frederick Landis Halting 
A Dissertation Submitted to the 
Graduate Faculty in Partial Fulfillment of the 
Requirements for the Degree of 
DOCTOR OF PHILOSOPHY 
Major: Statistics 
Approved: 
In Charge of Major Work 
For the Major Department 
Graduate College 
Iowa State University 
Ames, Iowa 
1989 
Signature was redacted for privacy.
Signature was redacted for privacy.
Signature was redacted for privacy.

ii 
TABLE OF CONTENTS 
ACKNOWLEDGEMENTS 
xii 
1. 
FORMULATION OF PREDICTION PROBLEM 
1 
1.1 Introduction 
1 
1.2 Comparative Experiments 
2 
1.2.1 
Description 
2 
1.2.2 
Example — Bioequivalence study 
3 
1.3 Small-Area Estimation 
5 
1.3.1 
Description 
5 
1.3.2 
Example - Crop-area prediction 
9 
1.3.3 
Related applications 
12 
1.4 Two-Part Mixed Model 
14 
1.4.1 
The model and related notation 
14 
1.4.2 
The general prediction problem 
15 
1.5 Preview of Subsequent Chapters 
17 
2. 
FREQUENTIST APPROACH TO PREDICTION 
19 
2.1 Ordinary Least Squares Procedures 
19 
2.2 
Best Linear Unbiased Point Prediction 
21 

iii 
2.3 Variance Component Estimation 
23 
2.3.1 
Empirical BLUP 
23 
2.3.2 
ANOVA and RE ML estimation 
24 
2.4 MSE of the Empirical BLUP 
25 
2.4.1 
Form of the MSE 
25 
2.4.2 
Estimation of the MSE of w 
26 
2.5 Interval Prediction Based on the Empirical BLUP 
27 
2.5.1 
General approaches 
27 
2.5.2 
Applications 
29 
2.6 Computational Aspects 
31 
2.6.1 
Basic results 
31 
2.6.2 
REML computations 
33 
2.6.3 
Derivative expressions 
34 
2.6.4 
Bootstrap computations 
36 
2.7 Examples 
36 
2.7.1 
Bioequivalence study 
36 
2.7.2 
Crop-area prediction 
44 
3. BAYESIAN APPROACH TO PREDICTION 
56 
3.1 Prior and Posterior Distributions 
57 
3.1.1 
Priors for (3 and 0 
57 
3.1.2 
Posterior distributions: f{iu |  y) and f*{io \ y) 
59 
3.1.3 
Characterizations of f*{iu | 2/) 
61 
3.2 Computational Aspects 
63 

iv 
3.2.1 
Computing /*(tu | y), P*{S | y), 
and up 
63 
3.2.2 
Computing credible sets 
67 
3.3 Related Approaches 
69 
3.3.1 
Hierarchical Bayes approach 
69 
3.3.2 
Parametric empirical Bayes approach 
71 
3.3.3 
Relationships 
73 
3.4 Examples 
77 
3.4.1 
Bioequivalence study 
77 
3.4.2 
Crop-area prediction 
79 
4. FREQUENTIST PROPERTIES OF THE PREDICTORS .... 93 
4.1 Properties of the Predictors 
93 
4.1.1 
Basic results 
93 
4.1.2 
.F-equivariance of frequentist predictors 
98 
4.1.3 
.F-Equivariance of the Bayesian predictors 
99 
4.1.4 
Some relevant criteria 
101 
4.2 Monte Carlo Study - Description 
104 
4.2.1 
The Monte Carlo method 
104 
4.2.2 
Computational aspects 
107 
4.2.3 
Design of study 
110 
4.3 Monte Carlo Study - Results 
Ill 
4.3.1 
Lithium Carbonate example 
Ill 
4.3.2 
Crop-area example 
121 
5. BIBLIOGRAPHY 
129 

V 
6. APPENDIX. PROOFS OF CHAPTER 4 RESULTS 
137 
6.1 Proof of Result 4.3 
137 
6.2 
Proof of Result 4.7 
137 
6.3 Proof of Result 4.8 
138 

vi 
LIST OF TABLES 
Table 1.1: 
Serum Levels of Lithium After Six Hours in mEq per liter (and 
Day on Which Formulation was Received) 
6 
Table 1.2; 
Data for the Example on Crop Area Prediction using Satellite 
Data - Part One 
10 
Table 1.3: 
Data for the Example on Crop Area Prediction using Satellite 
Data - Part Two 
11 
Table 2.1: 
Bioequivalence study - Fitted values for the logarithms of 
lithium serum levels (and their antilogs) 
40 
Table 2.2: 
Bioequivalence study - Estimates of treatment differences . . 
45 
Table 2.3: 
Crop-area prediction - Estimates {lu = 1^(7)) of fixed effects 
46 
Table 2.4: 
Crop-area prediction - Summary of point prediction of corn 
hectares 
53 
Table 2.5: 
Crop-area prediction - OLS and naive prediction intervals for 
corn hectares (a = 0.05) 
53 
Table 2.6: 
Crop-area prediction - Modified naive prediction intervals for 
corn hectares (a = 0.05) 
54 

vii 
Table 2.7: 
Crop-area prediction - Bootstrap prediction intervals for corn 
hectares (a = 0.05) 
54 
Table 2.8: 
Crop-area prediction - Estimated quantiles for the modified 
naive prediction intervals for corn hectares 
55 
Table 2.9: 
Crop-area prediction - Estimated quantiles for the bootstrap 
prediction intervals for corn hectares 
55 
Table 3.1: 
Bioequivalence study - Characterizations of the predictive den­
sities for three treatment differences 
78 
Table 3.2: 
Crop-area prediction - Summary of Bayesian prediction of 
corn hectares 
79 
Table 4.1: 
Monte Carlo estimates of conditional (on 7 = 0) MSEs and 
expectations (and the estimated standard errors of the Monte 
Carlo estimates) — Lithium carbonate example 
112 
Table 4.2: 
Monte Carlo estimates of conditional (on 7 > 0) MSEs and 
expectations (and the estimated standard errors of the Monte 
Carlo estimates) — Lithium carbonate example 
113 
Table 4.3: 
Monte Carlo estimates of unconditional MSEs and expecta­
tions (and the estimated standard errors of the Monte Carlo 
estimates) — Lithium carbonate example 
116 
Table 4.4: 
Monte Carlo estimates of conditional (on 7 = 0) probabilities 
of coverage (and conditional expected lengths) — Lithium car­
bonate example 
117 

viii 
Table 4.5: 
Monte Carlo estimates of conditional (on 7 > 0) probabilities 
of coverage (and conditional expected lengths) — Lithium car­
bonate example 
Table 4.6: 
Monte Carlo estimates of unconditional probabilities of cover­
age (and unconditional expected lengths) — Lithium carbon­
ate example 
Table 4.7: 
Monte Carlo estimates of unconditional probabilities of false 
coverage P{{iv -h e)sl) — Lithium carbonate example . . , . 
Table 4.8: 
Monte Carlo estimates of conditional (on 7 = 0) M S Es and 
expectations (and the estimated standard errors of the Monte 
Carlo estimates) — Crop area example 
Table 4.9: 
Monte Carlo estimates of conditional (on 7 > 0) MSEs and 
expectations (and the estimated standard errors of the Monte 
Carlo estimates) — Crop area example 
Table 4.10: 
Monte Carlo estimates of unconditional MSEs and expecta­
tions (and the estimated standard errors of the Monte Carlo 
estimates) — Crop area example 
Table 4.11: 
Monte Carlo estimates of conditional (on 7 = 0) probabilities 
of coverage (and conditional expected lengths) — Crop area 
example 
Table 4.12: 
Monte Carlo estimates of conditional (on 7 > 0) probabilities 
of coverage (and conditional expected lengths) — Crop area 
example 

ix 
Table 4.13: 
Monte Carlo estimates of unconditional probabilities of cover­
age (and unconditional expected lengths) - Crop area example 127 
Table 4.14: 
Monte Carlo estimates of unconditional probabilities of false 
coverage P{{xu + e)c7) — Lithium carbonate example .... 128 

X 
LIST OF FIGURES 
Figure 2.1: 
Bioequivalence study - Dot plot of blood serum levels of lithium 
by formulation 
38 
Figure 2.2: 
Bioequivalence study - Dot plot of blood serum levels of lithium 
by day of administration 
39 
Figure 2.3: 
Bioequivalence study - Plot of lc{'y\z) 
41 
Figure 2.4: 
Bioequivalence study - Normal plot of residuals 
42 
Figure 2.5: 
Bioequivalence study - Plot of residuals versus fitted values . 
43 
Figure 2.6: 
Crop-area prediction - Plot of reported hectares of corn versus 
pixels classified as corn 
47 
Figure 2.7: 
Crop-area prediction - Plot of reported hectares of corn versus 
pixels classified as soybeans 
48 
Figure 2.8: 
Crop-area prediction - Plot of lc{f;z) 
49 
Figure 2.9: 
Crop-area prediction - Normal plot of residuals 
50 
Figure 2.10: Crop-area prediction - Plot of residuals versus fitted values . 
51 
Figure 3.1: 
Bioequivalence study - Contour and surface plots of the Jef­
freys' prior (oT 6 
80 

xi 
Figure 3.2: 
Bioequivalence study - Contour and surface plots of the REML 
likelihood/(cr|, 7; 2) 
81 
Figure 3.3: 
Bioequivalence study - Contour and surface plots of the pos­
terior density o i O  
82 
Figure 3.4: 
Bioequi valence study - Plot of&i(7|2) 
83 
Figure 3.5: 
Bioequi valence study - Posterior densities for the three treat­
ment differences 
84 
Figure 3.6: 
Bioequi valence study - BLUPs of the treatment differences as 
a function of 7 
85 
Figure 3.7: 
Crop-area prediction - Contour and surface plots of the Jef­
freys' prior density for 0 
87 
Figure 3.8: 
Crop-Area prediction - Contour and surface plots of the REML 
likelihood/((jf,7; 2) 
88 
Figure 3.9: 
Crop-Area prediction - Contour and surface plots of the pos­
terior density oiO 
89 
Figure 3.10: Crop-Area prediction - Plot oi 
\ z) 
90 
Figure 3.11: Crop-Area prediction - Plots of the posterior densities for 
three counties 
91 
Figure 3.12: Crop-Area prediction - BLUPs of mean hectares of corn per 
segment for three counties, as a function of 7 
92 

xii 
ACKNOWLEDGEMENTS 
A dissertation is clearly not a solo project. I could never have finished this 
document were it not for the help of a great many people. 
First of all, I owe a great deal to my advisor, Dr. David Harville, for his direction 
of this work. I can't begin to detail all the ways in which he has helped me, and I 
would not have gotten far without his aid. Much of what is good I owe to him - the 
mistakes are all mine. 
Financial support from the Department of Statistics and the Office of Naval 
Research is also gratefully acknowledged. 
Special thanks to my family and also to the wonderful and crazy people with 
whom I have worked and played for the past six years - they know who they are, and 
they know how I feel about them. 
Coinciding with my arrival in Ames some six years ago was my first meeting 
with my wife Karen Jensen. While a Ph.D. is certainly a great accomplishment, I 
believe that the greatest legacy of my time in Ames will be the lifelong partnership 
that Karen and I have formed. She has provided an incredible amount of love and 
support throughout this project, as she has in so many areas of my life. It is to her, 
my best friend, that all this work is dedicated. 

1 
1. 
FORMULATION OF PREDICTION PROBLEM 
1.1 
Introduction 
Two topics that have received much attention in the statistics literature are 
the analysis of comparative experiments and small-area estimation. Typically, in 
the analysis of comparative experiments, the objective is to make inferences about 
treatment contrasts. In small-area estimation, we wish to make inferences about the 
average of some quantity over a relatively small area based on a sample that includes 
few, if any, sampling units from that area. Both problems can, in many instances, 
be formulated as special cases of the general problem of predicting the realization of 
a random variable lu based on the value of an observable random vector y, where 
y follows a mixed linear model with a single set of random effects and lu is a linear 
combination of fixed and random effects. 
The primary objective in what follows is to review and discuss, in a common 
framework and common notation, some recent results on this general prediction prob­
lem and its special cases. Our coverage includes both frequentist and Bayesian ap­
proaches. In particular, we discuss the relationships among these approaches, and 
examine the computations required to implement them in the case of unbalanced 
data. We also compare, via a Monte Carlo study, the frequentist properties of the 

2 
various Bayesian and non-Bayesian procedures. 
This chapter begins with descriptions and examples of the problems of statisti­
cal inference from comparative experiments and of small-area estimation. We then 
introduce the general prediction problem and provide an overview of the content of 
subsequent chapters. 
1.2 
Comparative Experiments 
1.2.1 
Description 
Statisticians are frequently asked to analyze data from a comparative experiment 
in which n experimental units, arranged in b blocks of possibly unequal size, have been 
allocated among t treatments in accordance with some incomplete block design (IBD). 
Often, the datum from the 
of those experimental units in the 
block that were 
assigned to the 
treatment is regarded as a realization of the random variable 
yi jk =  /^ +  n  +  
+  H jk 
(I'l) 
( k = l ,  . . .  ^nij). Here, t h e  common term 
a n d  t h e  treatment effects r j , . . . ,  
are un­
known parameters and the errors ^llli^ll2i- • • i 
are uncorrelated random vari­
ables having mean zero and common, unknown, strictly positive variance (t|. Gener­
ally, the block effects 
..., 
are regarded as unknown parameters or, alternatively, 
as random variables with zero means and common, unknown, nonnegative variance 
g that are uncorrelated with each other and with the errors 
-
Depending on which assumption is made, the block effects are referred to as fixed 
effects or random effects, and model (1.1) is referred to as a fixed-effects model or 

3 
mixed-effects model. 
The analysis of data from a comparative experiment usually focuses on inferences 
about treatment contrasts. A treatment contrast is a parametric function 0 of the 
general form 4> = 
, where 
= 0- Note that V' is estimable under the 
mixed effects version of (1.1). This linear function is also estimable under the fixed 
effects model, provided that the data are connected (e.g., Searle 1971, sec. 7.4). 
1.2.2 
Example — Bioequivalence study 
When a new formulation of a drug is introduced in the pharmaceutical sciences, 
it is necessary to establish that it is essentially equivalent to the standard formulation. 
Typically, a new formulation will be given in the same dosage form and will contain 
the same amount of active ingredient as the standard. That is, the two formulations 
will be chemically equivalent. Two chemically equivalent formulations are said to be 
therapeutically equivalent if they produce the same therapeutic effect, as measured 
by the control of a symptom or disease. The latter type of equivalence is of primary 
importance to those involved in drug development. 
The direct assessment of therapeutic equivalence through a clinical efficacy trial 
is difficult and expensive. A practical alternative, which is thought to be sufficiently 
protective of the public interest, is to infer therapeutic equivalence when the formu­
lations are shown to be biologically equivalent, that is, when they are shown to differ 
little in "the rate at, and the extent to, which they make the active ingredient avail­
able to the circulating blood" (Metzler, 1974). Comparative experiments designed to 
assess the biological equivalence of two or more formulations are called comparative 

4 
bioavailability trials or bioequivalence studies. 
In a comparative bioavailability trial, the new and standard formulations of a 
drug are administered to a group of subjects, blood samples are drawn at various 
times following administration, and drug concentrations in the serum are determined 
for each sample. To facilitate the elimination of the effects of between-subject vari­
ability from the comparisons among the formulations, each subject is used more than 
once, with a "wash-out" period between the administration of different formulations. 
Toward this end, several types of experimental designs are used: cross-over (Rocke, 
1984), latin-square (Selwyn and Hall, 1984), and incomplete block (Westlake, 1974). 
An IBD is especially useful in studies of more than two formulations, since the partic­
ipation of each subject may be limited. Once the data are obtained, interest usually 
focuses on a single aspect of the curve that relates concentration to time, for example, 
the area under the curve (AUG), the maximum concentration {Cmax)i or the average 
concentration at particular sampling times. 
As an example, consider the data presented in Table 2 of Westlake (1974). These 
data are from a comparative bioavailability study in which each of 12 patients received 
one of four formulations of lithium carbonate (A,B,C, or D) on Day 1 and a second 
one of the four on Day 8. The serum level of lithium in each patient's blood was 
recorded 2, 3, 5, 6, 9, and 12 hours subsequent to the reception of each formulation. 
For simplicity, we restrict attention to the data recorded at the end of 6 hours. The 
experimental design was a balanced incomplete block design (BIBD). 
Formulation D was the standard formulation, while A, B, and C were new. The 
objective in analyzing the data was to make inferences about the contrasts A vs. 

5 
D, B vs. D, and C vs. D, and specifically to assess the bioequivalence of the new 
formulations and the standard. By regarding the four formulations as treatments, the 
patients as blocks, and the 
as serum levels of lithium, these inferences could 
be based on model (1.1). Here, the n^j^s equal zero or one. Alternatively, inferences 
could be based on the model 
y-ijk = 
+Ti+am+ fSj 4- 
, 
(1.2) 
where m. equals 1 or 2 depending on whether the 
patient received the 
for­
mulation on Day 1 or Day 8, and a]^ and ag are unknown parameters. Model (1.2) 
differs from model (1.1) in that it allows for the possibility of a systematic difference 
between Days 1 and 8. 
For purposes of illustration, we suppose that patients 2, 5, 6 and 9 dropped out 
of the experiment prior to Day 8, in which case the data are as depicted in Table 1.1. 
1.3 Small-Area Estimation 
1.3.1 
Description 
In small-area estimation, sample data from a population, scattered over a large 
domain, are used to make inferences about the average, or total, of some quantity in 
subdomains of that population. One objective is to provide the best possible estimates 
for areas which contain few, if any, sampling units. For example, the allocation of 
federal funds to local governments is based in part on per capita income (PCI) (Fay 
and Herriot, 1979). In practice, the distribution of these monies is based on estimates 
of the PCI determined from a national census sample. When sample information for a 

Table 1.1: Serum Levels of Lithium After Six Hours in 
mEq per liter (and Day on Which Formulation 
was Received) 
Formulation 
Patient 
A*^ 
1 
0.300 (1) 
0.240 (8) 
2 
0.333 (1) 
3 
0.300 (8) 
0.200 (1) 
4 
0.360 (1) 
0.200 (8) 
5 
0.433 (1) 
6 
0.367 (1) 
7 
0.300 (8) 
0.440 (1) 
8 
0.400 (1) 
0.267 (8) 
9 
0.133 (1) 
10 
0.400 (1) 
0.267 (8) 
11 
0.367 (1) 
0.267 (8) 
12 
0.280 (8) 
0.200 (1) 
a  A: 300 mg. c ;apsule. 
b  B: 250 mg. capsule (different excipients than A). 
c  C: 450 mg. delayed release capsule. 
d  D: 300 mg, in solution. 

7 
local government area is sparse, small-area procedures attempt to borrow information 
from similar areas, usually via a model relating PCI to several auxiliary variables. 
Let y  be the variable of interest (e.g., PCI), and a  the number of "small areas" 
under consideration. One model employed in small-area estimation is 
Vij = 
+ sij, 
(1.3) 
( i  =  l , . . . , a  ; j  =  1 , . . . ,  M J .  Here, y ^j represents the value of y  for the 
unit 
sampled from the 
area, 
and 
represent the values of two vectors x' and 
k' of explanatory or indicator variables, and /3=(/3^,... ^j3p)' is a vector of unknown 
parameters. Further, the elements of the vector v = (fj,..., fa)' — one element for 
each small area — are regarded as uncorrelated random variables having mean zero 
and common variance cr^, while the e^^'s are random variables that are uncorrelated 
with each other and with the u^-'s and that have mean zero and common variance cr|. 
The quantity of interest is usually the population mean 
of y for the 
small 
area, which may be expressed as 
where 
and 
the population means of aj' and fc' for the 
small-area 
and where 
= Fj - 
Following Prasad and Rao (1986) and 
Battese and Fuller (1981), we assume that the population size 
for the 
area is 
large, in which case 
% 0 (i.e., the population is assumed to be infinite). 
Special cases of model (1.3) were examined by, among others. Fay and Herriot 
(1979), Battese and Fuller (1981), Ericksen and Kadane (1985), Stroud (1987), Demp­
ster and Raghunathan (1987), Prasad and Rao (1986), and Casella and Strang (1987). 

8 
In. particular, Battese and Fuller considered the "nested-error regression" model, 
Vij =  
ij(^ +  n  
' 
(1-5) 
In model (1.5) [and more generally in model (1.3)], it is assumed that the vector 
of explanatory variables 
ij is measured for each unit. If instead the concomitant 
information is recorded at the area level, we could apply the special case of model 
(1.5) where 
= l(i = l,...,a) to the small-area averages, which is in effect the 
approach taken by Fay and Herriot (1979). 
In model (1.5), the area effects are interpreted as a component of the regression 
model error 
In some applications it may be more reasonable to regard 
the effects 
as random deviations from a common slope. For example, in the special 
case of a single explanatory variable, we could consider the "random slope" model 
Vij =  A  4- x i j {a +  
,  
where a and the /?^'s are unknown parameters. This model is discussed by Prasad 
and Rao (1986), Dempster, Rubin and Tsutakawa (1981), and Casella and Strang 
(1987). 
In considering the estimation of small-area means under model (1.3), Ghosh and 
Lahiri (1988) and Datta and Ghosh (1989) decline to make the simplifying assumption 
= 0. 
Note that the "areas" under consideration need not be geographic regions; they 
may represent population subdomains defined by some other method of stratification. 
For example, in a manufacturing context, we may be interested in making inferences 
about the characteristics of specific batches of raw material based on a sample of 

9 
finished pieces produced from those, or similar, batches. When the sample information 
about a particular batch is meager, or nonexistent, small-area methodology may be 
useful. 
1.3.2 
Example - Crop-area prediction 
Battese, Harter, and Fuller (1988) discussed the application of small-area meth­
ods to the prediction of county crop areas using satellite information. Their data, 
which are given in Tables 1.2 and 1.3, consist of two different determinations of the 
area planted to corn and soybeans for a sample of segments in 12 north-central Iowa 
counties (a segment is 250 hectares). Reported hectares, obtained during interviews 
with farm operators in the 1978 June Enumerative Survey of the U.S. Department 
of Agriculture, provided the first determination. The second determination, taken 
from LANDSAT readings in August and September of 1978, is the number of pixels 
classified as corn or soybeans (a pixel represents about 0.45 hectares). 
Here we focus on the estimation of mean hectares of corn per segment for the 12 
counties represented in the sample. Let yij be the number of hectares of corn in the 
segment of the 
county, and let xi^j and X2ij represent the number of satellite 
pixels in a sample segment classified as corn and soybeans, respectively. Battese et 
al. (1988) applied to these data the nested-error regression model 
Vij =  /^O +  A ^ l i j  +  ^2^2ij +  n  +  Hj' 
(1-6) 
Under this model, the small-area means to be predicted are of the form 

10 
Table 1.2: Data for the Example on Crop Area Prediction using Satellite 
Data - Part One 
Reported 
Pixels in 
Mean Number of 
Hectares 
sample segments 
pixels per segment 
of Corn 
Corn 
Soybeans 
Corn 
Soybeans 
County 
"z 
3 
(%2,j) 
Cerro Gordo 
1 1 
165.76 
374 
55 
295.29 
189.70 
Hamilton 
1 1 
96.32 
209 
218 
300.40 
196.65 
Worth 
1 1 
76.08 
253 
250 
289.60 
205.28 
Humboldt 
2 1 
185.35 
432 
96 
290.74 
220.22 
2 
116.43 
.367 
178 
Franklin 
3 1 
162.08 
361 
137 
318.21 
188.06 
2 
152.04 
288 
206 
3 
161.75 
369 
165 
Pocohontas 
3 1 
92.88 
206 
218 
257.17 
247.13 
2 
149.94 
316 
221 
3 
64.75 
145 
338 
Winnebago 
3 1 
127.07 
355 
128 
291.77 
185.37 
2 
133.55 
295 
147 
3 
77.70 
223 
204 
Wright 
3 1 
206.39 
459 
77 
301.26 
221.36 
2 
108.33 
290 
217 
3 
118.17 
307 
258 

11 
Table 1.3: Data for the Example on Crop Area Prediction using Satellite 
Data - Part Two 
Reported 
Pixels in 
Mean Number of 
Hectares 
sample segments 
pixels per segment 
of Corn 
Corn 
Soybeans 
Corn 
Soybeans 
County 
H  
j  
(%%) 
Webster 
4 1 
99.96 
252 
303 
262.17 
247.09 
2 
140.43 
293 
221 
3 
98.95 
206 
222 
4 
131.04 
302 
274 
Hancock 
5 1 
114.12 
313 
190 
314.28 
198.66 
2 
100.60 
246 
270 
3 
127.88 
353 
172 
4 
116.90 
271 
228 
5 
87.41 
237 
297 
Kossuth 
5 1 
93.48 
221 
167 
298.65 
204.61 
2 
121.00 
369 
191 
3 
109,91 
343 
249 
4 
122.66 
342 
182 
5 
104.21 
294 
179 
Hardin 
5 1 
88.59 
220 
262 
325.99 
177.05 
2 
165.35 
355 
160 
3 
104.00 
261 
221 
4 
88.63 
187 
345 
5 
153.70 
350 
190 

12 
where 
and ®2i(p) 
county means of pixels per segment classified as corn 
and soybeans respectively. Satellite data are available for all segments in a county, 
not just those in the survey sample. Consequently, the values of 
and ^2i{p) 
can be obtained as the total number of pixels classified as corn or soybeans in a 
county, divided by the number of segments in that county. These values are included 
in Tables 1.2 and 1.3. 
1.3.3 
Related applications 
Many prediction problems, covering a wide variety of applications, are very sim­
ilar to small-area estimation. In this section, we describe two such problems. 
1.3.3.1 
Tensile Strength Data Hahn and Raghunathan (1988) considered 
the problem of interval estimation for the average tensile strength of bars created 
from a casting of a high-temperature alloy. Since measurements of tensile strength 
are made in a destructive manner, it is not feasible to sample many bars from a given 
casting. Consequently, very few observations from a casting are available to estimate 
the mean tensile strength of that particular casting. 
Assume we are given a sample of bars from a castings. Let y.^j be the measured 
tensile strength for the 
bar from the 
casting (i = l,...,a ; j = 1,...,%^). 
One possible model is 
Uij = fi-{• 
+ eij , 
(1.7) 
where /i is an unknown parameter that represents the mean tensile strength for the 
process, and the a^'s and Cj^-'s are uncorrelated random variables with zero means 
• 
9 
9 
and variances cr^, and ctq, respectively. The 
are regarded as measurement errors. 

13 
The quantities of interest are the casting means, that is, the {/.t + q:^')'s. Hahn and 
Raghunathan (1988) used this model to analyze data from 4 castings (see their Table 
1). 
Model (1.7) is the widely used "one-way random model." Prediction procedures 
for this model are discussed, for example, by Hill (1967), Jeske and Harville (1988), 
Morris (1983a,b), and Laird and Louis (1987). Note that if a set of covariates is 
available for each of the castings, or for the individual bars, we might instead employ 
a model of the form (1.5). 
1.3.3.2 
Lamb weight data 
Harville and Fenech (1985), and Harville (1989) 
discussed a prediction problem that arises in animal breeding. They presented data, 
from five distinct population lines of sheep, consisting of the birth weights of 62 single-
birth male lambs (see Table 1 of Harville and Fenech). Each lamb was the progeny 
of one of 23 rams, and each lamb had a different dam. Age of dam was recorded as 
belonging to one of three categories, numbered 1(1-2 years), 2 (2 - 3 years), and 
3 (over 3 years). Let y^jf^i represent the birth weight of the 
of those lambs that 
are the offspring of the 
sire in the 
population line and of a dam belonging to 
the 
age category. One model for the 
is 
+ 6, 4- TTj + 
' 
{ i  =  1,2,3; J = 1,... ,5; A; = 1,,..,mj;/ = 1,... 
Here, the age effect S.^ and 
the line effect ttj are unknown parameters. The sire (within line) effects, that is, the 
5y^,'s, and the errors, that is, the 
are uncorrelated random variables with zero 
means and variance components (t| and (t|, respectively. 

14 
Of interest is the average birth weight of an infinite number of male offspring of 
the 
sire in the 
line. Thus, one objective is to predict the value of the random 
variable 
= /^ + <5* + Try + 
, 
where 
= (22/62)^2 4-(11/62)62-H(29/62)6g, is a weighted average of the age effects 
(with weights proportional to frequency of representation). 
Note that this prediction problem is very similar to the small-area estimation 
problem. Here, the sires (within lines) play a role analogous to the small areas -
we are interested in estimating a population mean for each sire based on a (possibly 
small) sample of his offspring. 
Consider the parametric function 
= 4(r|/(cr| 
a-^). Since cr| and cr| are 
• 
• 
9 
• 
9 . 
nonnegative, so is h . Moreover, in this example, 
is interpretable as a heritability, 
which by its very nature is less than or equal to one. Assuming that 0 < 
< 1 is 
equivalent to assuming that the ratio <7|/cr| lies between 0 and 1/3. Thus, in some 
applications, it may be appropriate to impose restrictions on the variance components 
(in addition to the restrictions it| > 0 and (t| > 0). 
1.4 
Two-Part Mixed Model 
1.4.1 
The model and related notation 
The problems described in Sections 1.2 and 1.3 can be viewed as special cases of 
a more general prediction or estimation problem. Let y represent an n x 1 observable 
random vector. Suppose that y follows the mixed-effects linear model 
y = XI3 -{• Zs -\r e , 
(1.8) 

15 
where /3 is a p x 1 vector of unknown parameters and where s and e are unobservable, 
statistically independent random vectors of dimensions m x 1 and n x 1 that are dis­
tributed as MVN(0,(j|j) (multivariate normal with mean 0 and variance-covariance 
matrix cr|l) and MVN(0,crg J), respectively. Here, the variance components erf and 
(Tg are unknown parameters satisfying «rf > 0 and a-| > 0, and X and Z are given 
matrices. Define p* = rank(X), r = rank(X, Z)-rank(X), and / = n—rank(X,^). 
We assume that r > 0 and / > 0. 
Let 7= "tI/ctI, 0 = ((t|,7)', and Vy= var(y). Note that Vy = <TqI +crgZZ^ = 
(7^{I + jZZ') = £r|F, with V = F(7) = I+jZZ'. Also let /.ly = Eiy) = X(3. 
Unless otherwise indicated, we take the parameter space for 0 to be 
= {(^e,7)^ : 
(r| > 0,7 > 0}. In some contexts (such as the lamb weight example of Section 1.3.3), 
a more appropriate choice may be a restricted space of the form 
= {(<Zg,7)^ : 
«Tg > 0, o < 7 < 6}, where a and b are specified constants. 
Note that if s  were regarded as a vector of unknown parameters rather than a 
vector of random effects (in which case (1.8) would be a fixed-effects model) then 
the mean vector and variance-covariance matrix of y would be X(3 + Zs and cTg J, 
respectively. 
1.4.2 
The general prediction problem 
Clearly, the problems of small-area estimation and the estimation of treatment 
contrasts, considered in Sections 2.1 and 2.2, can be regarded as special cases of 
the problem of predicting the realization of a random variable xv of the form w = 
A'/3 + 6^8 under the mixed model (1.8) — we refer to the latter problem as the 

16 
general prediction problem. Let (.ixu = E{w), i't^,=var(m), and Vyyj—cov[y^w)^ and 
note that E{;w) = A'/3, v-io = 
and Vynj = a\'^Z6. 
Let d { y )  represent an arbitrary point predictor of the value of w .  The quantity 
d{y) — lu is called the prediction error [of d{y)]. The predictor d{y) is said to be 
unbiased if E[d{y) — lu] = 0, that is, if E[d{y) — tu) = 0 for all possible joint distri­
butions of w and y. (Unless otherwise indicated, probabilities and expectations are 
d e f i n e d  w i t h  r e s p e c t  t o  t h e  j o i n t  d i s t r i b u t i o n  o f  w  a n d  y . )  T h e  q u a n t i t y  E{[(/(î/)— 
is called the mean squared error (MSE) of the predictor d(T/), 
Subsequently we assume that A is such that A = X'u for some vector w, or 
equivalently that lu is estimable or predictable (i.e., that there exists a linear unbiased 
predictor of the value of -w). It is desirable to have an interval predictor for the value 
of lu, as well as a point predictor. Let I(y) represent an interval of values from the 
real line that may vary with the value of y. If P[wE I(y)] = 1 - a, then l{y) is said to 
be an (exact) 100(1 — a)% prediction interval for the value of ILK If P[w s I(?/)] only 
approximates, rather than equals, 1 — a for some or all possible joint distributions of 
lu and y, then l{y) is called an approximate 100(1 — a )% prediction interval. 
Depending on whether e = 0 or e 7^ 0, we refer to f [(w + e)el(y)] as the 
probability of coverage or the probability of false coverage of I(î/). It is desirable, not 
only for the probability of coverage to be equal to or close to 1 — cv, but also for the 
probability of false coverage to be small. It is also desirable for the expected length 
of l{y) to be small. 

17 
1.5 
Preview of Subsequent Chapters 
In Chapter 2 we review the traditional (frequentist) approaches to the general 
prediction problem. Our coverage includes the ordinary least squares approach to 
point prediction, the estimation of the variance components erf and cr| (including 
restricted maximum likelihood estimators), and the use of the variance-component 
estimates to obtain generalized least squares point predictors. Some exact and ap­
proximate prediction interval procedures associated with ordinary or generalized least 
squares are also developed, and the computational aspects of their implementation 
are discussed. Specifically, we show how the computations can be facilitated by diag-
onalizing a certain matrix. 
Chapter 3 presents a Bayesian approach to the general prediction problem. We 
specify a class of prior distributions, derive the corresponding posterior distributions 
of 10 given y, and describe point and interval characterizations of the posteriors 
that can be used as predictors for lu. We show how, by taking advantage of the 
computational results of Chapter 2, the normally severe computational requirements 
of the Bayesian approach can be reduced. 
As discussed in Chapter 3, the frequentist and Bayesian approaches to the mixed 
model are essentially equivalent to hierarchical Bayes and empirical Bayes approaches 
to the fixed model. We use this equivalence to show that the frequentist predictors 
can be viewed as approximations to the analogous Bayesian quantities. 
In Chapter 4 we explore the usefulness of the Bayesian predictors from the fre­
quentist point of view. In particular, we present the results of a Monte Carlo study of 
the frequentist properties of both the traditional and the Bayesian predictors (using 

18 
a noninformative prior). The results suggest that the Bayesi an approach produces 
point and interval predictors whose overall performance compares favorably with that 
of the the frequentist predictors, and that there are applications where the Bayesi an 
predictors should be used in preference to the frequentist predictors. 

19 
2. 
FREQUENTIST APPROACH TO PREDICTION 
In this chapter we briefly review traditional procedures for the general prediction 
problem. These procedures are based on a two-stage approach to making inferences 
under mixed-effects model (1.8); first estimate the error variance (t| and the vari­
ance ratio 7, and second then proceed as if Ug and 7 were known. In some cases 
modifications are made to account for the additional variability introduced by the es­
timation of cTg and 7. Applications of these procedures to the analysis of comparative 
experiments, and to small-area estimation, are discussed. 
2.1 
Ordinary Least Squares Procedures 
Let 
and s represent the components of an arbitrary solution to the linear 
system 
x'x x'z 
z'x z'z 
which are the normal equations (NE) obtained by applying ordinary least squares 
(OLS) to the fixed-effects version of model (1.8). Assuming that 
is estimable 
under the fixed-effects model, the OLS estimate of w is xu = 
-f ê's. Under either 
the mixed-effects or fixed-effects model, w is an unbiased predictor of the value of w. 
V ^ / 
z'y ) 
(2.1) 

20 
and its MSE is o-çh, where 
6' 
x'x x'z 
z'x z'z 
\' / 
(For any matrix A, A~ represents an arbitrary generalized inverse.) 
An unbiased estimator of <r| is 
= ^e//, where Se = y'y - X'y - s' Z'y 
is the residual sum of squares from the OLS analysis. The corresponding estimator 
of the MSE of w is a'^h. 
Corresponding to the OLS estimator w is the prediction interval 
I I  : lu ±  
[where tQ^j2if) denotes the upper-a/2 point of Student's t  distribution with / degrees 
of freedom]. This interval has exact 100( 1 — a)% coverage (under both the fixed-effects 
and the mixed-effects models). 
In the context of inference about treatment contrasts from IBDs, id and Interval 
II are the point and interval estimators obtained from the usual intra-block analysis 
of the data (e.g., Kempthorne, 1952, p. 370). Westlake (1972), Metzler (1974), 
and Rocke (1984) describe the following method of assessing bioequivalence based 
on these estimators. Let A be a treatment contrast representing the difference in 
mean response between two formulations. Prior to the experiment tolerance limits 
AI and A^ are chosen such that the two formulations are said to be bioequivalent 
if A| < A < Ag. The suggested procedure is to specify a value for a and compute 
Interval II for A. Then, conclude bioequi valence with 100(l-a)% "confidence" if II 
is entirely contained in [Aj, A^]-

21 
The fixed-effects version of model (1.8) also includes standard regression models; 
see Purcell and Kish (1979) for a review of their use in small-area estimation. 
2.2 
Best Linear Unbiased Point Prediction 
Suppose that 7 were known. Then, an alternative to the OLS estimator of the 
value of w is the best (minimum MSE) linear unbiased predictor (BLUP), as derived 
under the mixed-effects model. The BLUP of the value of tu is id{'y) = 
+ ô's, 
where 
is any solution to the Ait ken equations 
X ' V - ' ^ X P  = X ' V - ' ^ y  , 
(2.2) 
and s = fZ^V ^{y — Xfi) , or equivalently (for 7 > 0), where ^ and J are the first 
and second parts of any solution to the linear system 
X'X 
X'Z 
z'x 7-ij + z'z 
\ ® / 
z'y ) 
(2.3) 
(Henderson, 1975). The MSE of the BLUP is t;*(cr|,7) = (Zg ^(7) with 
$(7) = 
--iZ'v-^Z)6 
-t-[A' - 
Z'V-^X][X'V-^X]-[X - -iX'V-^ZÔ] , 
or equivalently (for 7  >  0 )  
$(7) = (A^ 6')G 
\ « /  
where G is any generalized inverse of the coefficient matrix of linear system (2.3) 
(e.g., Harville, 1976). 

22 
In the context of IBD's, the BLUP of a treatment contrast is identical to the 
combined intra-interblock estimator described by, for example, Kempthorne (1952). 
When, in small-area estimation, the model is taken to be (1.5) and xu is taken to be 
+ fj, — in which case lo is interprétable as the 
small-area mean — the 
BLUP of 10 is 
M l )  =  
= (77^) % + (4p) - 
^ 
(2-^) 
(Battese et al., 1988). If 
equals the sample mean 
the BLUP simplifies to 
M
l
)
 =
 (
—
(
1
 
I
 
( 2 . 5 )  
\ l  +  n -  
j  
\  7  +  
/  
which is a weighted average of the sample mean yi and the estimate of œ^/3. As n.i 
grows large, the coefficient 7/(7 + 
) becomes closer to one, which increases the 
weight placed on the sample mean 
from the 
small area. For small or moderate 
Jij, the weighting depends heavily on the value of 7. As 7 becomes smaller (imply­
ing greater "similarity" between areas) the BLUP increasingly relies on information 
"borrowed" from the other areas in the sample. When the areas are "dissimilar," that 
is, when 7 gets very large, considerable weight is placed on the information provided 
by the sample mean of the 
small area. 
When lu is given by (1.4), Datta and Ghosh (1989) show that the BLUP is 
(2.6) 
As 
—* 00, (2.6) converges to (2.4). 
- " z 
N :  
7 
. 7 + •"• - 1  V i  +  
JXl_ 
N-i - H 
X i{p) 

23 
2.3 
Variance Component Estimation 
2.3.1 
Empirical BLUP 
Except in special cases, the BLUP tLi(j) depends on the (unknown) value of 7. 
A predictor of the value of zv can be obtained from •iû(7) by replacing 7 with an 
estimator of 7. We refer to this predictor as the empirical BLUP. 
Let <t| and 7 be arbitrary even, translation invariant estimators of cr| and 7. 
(A possibly vector-valued function fl'(y) of y is said to be translation invariant if 
g{y + %6) = g{y) for every vector b, and it is said to be even if g{y) = g(—y)). We 
denote by w the empirical BLUP obtained from toij) by replacing 7 with 7; that is 
iv=tu{'y). For example, under the nested-error regression model (1.5), the empirical 
BLUP of the small-area mean is 
(^) 
^ 
where 
is computed as though 7 were the true value of 7. The empirical BLUP lû is 
unbiased for w, provided the expected value of w exists (Kackar and Harville, 1984). 
Harville (1977) reviews techniques for obtaining even, translation invariant es­
timators of cTg and 7. In the next section we briefly describe describe two popular 
methods: the method of fitting constants (or ANOVA method; see Henderson, 1953) 
and Patterson and Thompson's (1971) restricted maximum likelihood (REML) pro­
cedure. 

24 
2.3.2 
ANOVA and REML estimation 
The method-of-fitting-constants estimators of cr| and a-g are ô-g (defined in Sec­
tion 2.1) and âg = [{Ss/r)—d-'^]/K, where Ss = 
X'y+s^Z'y-y'Pis the sum of 
squares for s adjusted for /3, « = {l/r)tr[Z^{I - Pj^)Z], and P y= X{X^X)~X'. 
The corresponding estimator of 7 is âg/âç, which (like d^) can be negative. For 
substitution in w('y), a more appropriate estimator of 7 is the truncated estimator 
7= max(5-f/â-|, 0). 
Let z = L^y where L is an n x (n —p*) matrix such that Z'X = 0 and 
rank(X) = n — p*, so that the elements of z form a set of n — p* linearly inde­
pendent (LIN) error contrasts. Then, z ~ MVN(0,2}^y^Z). The REML approach 
to the estimation of 7 (and cTg) is to treat z, rather than y, as the data vector and 
to apply maximum likelihood. As shown by H ar ville (1974), the likelihood function 
for any set of n — p* LIN error contrasts is proportional to 
X e,:p{-{24rhy - Xh'V-Hy - Xmh 
(2.8) 
where X* in an n x p* matrix whose columns consist of any p* LIN columns of X, 
Thus, the REML estimators of 7 and cTg, to be denoted by 
and 7, respectively, 
are obtained by maximizing expression (2.8). 
Note that for fixed 7, the function /*(cr|,7; z) attains its maximum at 
= (!/ - 
- X,9)/(TI - p""). 
Substituting <7^(7) for cr| in 
z), we obtain the concentrated likelihood func­
tion /g(7;z) = /*(cr|(7),7;z). Clearly, the REML estimates can be computed by 

25 
first maximizing 
to obtain 7 and by taking âf = cr|(7), thereby reducing a 
maximization problem in two dimensions to one in a single dimension. 
Associated with cr| and 7 is the REML information matrix 
whose inverse is the large-sample variance-covariance matrix of cTg and 7. 
2.4 
MSB of the Empirical BLUP 
2.4.1 
Form of the MSB 
Let 
represent the MSE of lu. Closed-form expressions for v'^{cr'ç,f) 
are available only in very simple special cases. It is common practice to approximate 
u'^((Tg,7) by y*(cre,7); however, this approximation is naive in that it ignores the 
contribution of the estimator 7 to the variability of ib — w. (e.g., Khatri and Shah, 
1981). 
Let e{xu^y) — w  — iv represent the prediction error of w .  We can decompose this 
error into two parts: 
s { t u , y )  =  e i ( w , 2 / ; 7 )  +  £2(2/57) ,  
where 
%/;7) = ^(7) — w is the error inherent in the BLUP, and £'2{y\ l )  = 
w — w(7) is the error introduced by the estimation of 7. Kackar and Harville (1984), 
and Harville (1985), showed that when the distribution of y is MVN, £i{iu,y\'f) and 
£•2(2/; 7) are statistically independent, and the MSE of w equals 
•I'"^(<^6)7) = t'*(o'e,7) + var[£:2(2/;7)] , 
(2.9) 

26 
provided that E{xh) exists. Expression (2.9) indicates the effect of substituting an 
estimate for 7 into ^(7) is to inflate the MSE of prediction by an amount equal to 
var[£2(2/;7)]- This amount will be large if 1^(7) is sensitive to changes in 7 and the 
variability of 7 is substantial. 
2.4.2 
Estimation of the MSE of xu 
Closed-form expressions for the second term in (2.9) are available only in very 
simple special cases (e.g., Khatri and Shah, 1981). As a consequence, it is common 
practice to approximate f"^(cr|,7) by i'*(cr|,7). Kackar and Harville (1984) approxi­
mated var[£2(y)7)] by a x 6, where a = var[&Z'(7)/^7), and h equals or approximates 
the MSE E[(7 — 7)^] of 7. The corresponding approximation to the MSE of ih is 
+  a X 6. 
(2.10) 
If 7 = 7, then we might take b  to be the large-sample variance of 7. 
Estimators of i'"^(cr|,7) can be obtained from the approximations u*(cr0,7) and 
x''^(crg,7) by replacing (t| and 7 with their estimators. These estimators are u*(â|,7) 
a n d  t h e  m o r e  conservative estimator 
, 7 )  =  y*(ô - e,7) +  à  x  b  (where à  a n d  b  
represent the values of a and b a.t cr^ — è\ and 7 = 7). 
Prasad and Rao (1986) discuss the asymptotic properties of these estimators in 
the context of small-area estimation. Their results indicate that f'^(<T|,7) tends to 
underestimate the MSE of w (though of course to a lesser extent than i'*((t|,7)). 
They suggest the alternative estimator 
V = I'*((T|,7) + 2(0 X h) 
(see also Harville, 1988). 

27 
2.5 
Interval Prediction Based on the Empirical BLUP 
2.5.1 
General approaches 
An alternative to the prediction interval II is obtained by acting as though the 
quantity t{w,y), defined by t{w,y) = {-w — iu)/[v'^]^l'^ 
, is a pivotal quantity. 
Here, 
represents a nonnegative function of (t| and -y, which is to be regarded 
as an estimator of the MSE of w. As shown in Section 4.1.1, the distribution of 
t{-w,y) is symmetric about zero. We now consider three approaches to obtaining 
the percentage points of this distribution. Following Jeske and Harville (1988), we 
distinguish between naive, modified naive, and bootstrap approaches to the formation 
of prediction intervals. 
2.5.1.1 
Naive approach 
This approach consists of approximating the dis­
tribution of t{w,y) with a standard normal distribution. Let 
represent the 
upper-a/2 point of the latter distribution. Then, a 100(l-a;)% approximate predic­
tion interval is 
2.5.1.2 
Modified naive approach If the estimators 7 and 
are relatively 
precise, then the coverage probability of interval (2.11) is likely to be close to the 
specified level of 1 — a. However, if the variance of 
is relatively large, then the 
true coverage probability of interval (2.11) is likely to be somewhat lower than the 
specified level. 
A potential improvement on interval (2.11) is obtained by re-expressing t { i u , y )  

28 
as 
(2.12) 
and by acting as though (i) the numerator and denominator of expression (2.12) are 
statistically independent, (zz) the distribution of the numerator is standard normal, 
is chi-square with u degrees of freedom, that is, by acting as though the distribution 
of i{-w,y) is Student's t with u degrees of freedom. In this approach, the upper-
a/2 point of the distribution of t(iu,y) is approximated by the upper-a/2 point, say 
of Student's t distribution with û degrees of freedom, where û is an estimate 
of V. By making use of this approximation, we obtain from t{w, y) the approximate 
100(l-a)% prediction interval 
In what follows, we take i> to be the estimate of u  obtained by adopting the approach 
of Satterthwaite (1946). 
To implement this approach, we first obtain an explicit expression for u. We do so 
by equating the variance of a chi-square random variable with u degrees of freedom 
to the approximate variance of expression (2.13) — specifically to the variance of 
the approximation to (2.13) obtained by expanding D"*" (as a function of âg and 
7) in a first-order Taylor series approximation about the point (<t|,7). Defining 
a'((Te,7) = 
, 5t)"^/07j and taking S(crg,7) to be the variance-covariance 
and {Hi) there exists a scalar u  =  (/(cg,7) such that the distribution of 
(2.13) 
(2.14) 

29 
matrix of 
and 7, we obtain 
u  =  2 [t'+(<r|,7)]^/ [aVe57)J3(<^e>7)a(«^e>7)] 
This expression for u suggests, as an estimator of f, 
Ù = 2 v +  / [aVe)7)^a(ô"l»7)] , 
where B is an estimator of ^(<7^,7). 
2.5.1.3 
Bootstrap approach Suppose that for purposes of approximating 
the percentage points of the distribution of t{iu,y), we generate a random sample 
(ît'liZ/l)? •••5 (wm,%/m.) from the joint distribution of lu and y (under the model 
(1.8)), taking /3=0, 
1, and 7 = 7, and form the empirical distribution function 
of t{w,y). (As shown in Section 4.1.1, the distribution of t(w,y) depends on f3 and 
0 only through the value of 7). Let 
be the upper- and lower-(a/2) 
points, respectively, of the empirical distribution of t { i u , y ) .  Then, an approximate 
100(l-a)% prediction interval for w is given by 
lA + 
• 
(2.15) 
Interval (2.15) can be interpreted as a parametric bootstrap confidence interval (e.g., 
Efron, 1982). 
2.5.2 
Applications 
A total of nine approximate prediction intervals are generated by combining 
the three MSE estimators given in Section 2.4.2 with the three general approaches 

30 
described in Section 2.5.1. They are 
12 : 
. 13 :û,± 
. 
14: 
; 
Ihxw±i^l2{ùi)[v*{cr1,^)\^l'^ ; 
16 : w ± 
! 
17 : to 
; 
18 : [îô + 4/2 
>7)]^/^ , w + 
i 
19 : [iô + ^^yu®(a^,'y)]V2 , ^ 
: 
and 
110 : [w + 
^ 
• 
Here 
, z>2, and 
represent the values of ù  obtained by choosing 
to be 
•'^*(ô'|)7)) i;®(^e,i'), and v, respectively. Similarly, [4/2'^'q/2^' ta^/W"a/2^' 
(3) 
(3) 
f a/2' a/2^' 
the values of RQ,/2)^^a/2J' obtained by choosing 
to be 
•w*(<t|,7), 'u'^(<T§li), and t), respectively. In all nine versions, we take ô-| = ô-|, 
7=7, and set A = [//-((tI, 7)]""^. In determining 
,7) and t', we take b to be 
the large sample variance of 7. 
In the special case of the estimation of treatment contrasts from an IBD design. 
Interval 15 reduces to the confidence interval proposed by Giesbrecht (1986). Jeske 
and Harville (1988) considered Interval 16 as applied to the prediction of a group 
mean in a balanced one-way random model. McLean and Sanders (1988) investigated 
Intervals 15 and 16 in the context of some one- and two-way models, and, in a small-
area context, Fuller (1988) considered what is essentially a multivariate version of 
Interval 13. 

31 
In combining intra- and inter-block information from an incomplete block design, 
it may — depending on the design — be important to account for inaccuracies in the 
weights (Kenipthorne, 1952, sec. 23.6), Intervals 13 and 14 account for their effect on 
the MSE, Intervals 15 and 18 account for their effect on the percentage points of the 
distribution of the pivotal, and Intervals 16, 17, 19, and 110 account for their effects 
on both the MSE and the percentage points. 
2.6 
Computational Aspects 
We now consider the problem of implementing the techniques described in Sec­
tions 2.1 - 2.5. Specifically, we develop computationally convenient expressions for 
certain quantities, and present algorithms for computing the various point and inter­
val estimates. 
2.6.1 
Basic results 
The various MSE estimators and the corresponding prediction intervals depend 
on the REML estimates of the error variance cr^ and the variance ratio 7. Except 
in special cases, the REML estimates of o-g and 7 must be computed numerically 
via an iterative algorithm. Many such algorithms have been proposed — see Harville 
and Callanan (1989) for a review and comparison. These algorithms require that the 
logarithm of the likelihood function (or concentrated log-likelihood function) and its 
first- and possibly second-order derivatives (or various related quantities) be evaluated 
for each of a number of trial values of 7 and (t|. 
In this section, we give expressions for /*(cr|,7;2), 10(7), ^(7), a, and b that 

are useful when these quantities must be calculated repeatedly for different values 
of 7 and (t|. These expressions, which are due to Dempster, Selwyn, Patel, and 
Roth (1984), Harville and Fenech (1985), Harville and Call an an (1989), and Harville 
(1989), can be used to advantage in the Bayesian approach, which is to be discussed 
in Chapter 3, as well as in the frequentist approach. 
Let C = Z\l — Pq — Z^(I — 
and r  =  rank(C). Define 0 < 
< ... < Ar to be the nonzero characteristic roots of C, and take 12 to be a 
matrix of dimensions m x r whose columns are orthonormal characteristic vectors of 
C corresponding to 
..., 
Furthermore, let D = diag(A2,..., Af), and define 
The sum of squares Ss can be reexpressed as Ss = 
and hence Se can be 
reexpressed as S e  =  
y ' { I  -  P ~  
^1=1 
(Harville and Fenech, 1985, sec 3.8). 
Harville and Call an an (1989) showed that the likelihood function /*((t|,7;2:) is 
proportional to the function 
= (27ro-2)-("-P*)/2M(-,)exp{-(2<T|)-l,4(7itiSe)} , (2.16) 
where Af(7) = 
+ 
and /1(7;t; 5e) = 5e+ 
/(I 
They did so by deriving the likelihood for a particular set of n —p* LIN error contrasts 
which included 
Note that once A]^,..., Ay and R have been computed, 
the additional expenditure of computing resources required to repeatedly evaluate 
Z((Tg , 7; z) for different values of ctq and 7 is relatively small. 
Let ^ = [6 — Z^X{X^X)~\]. Analogous to expression (2.16) for the likelihood 
function are the following expressions for w(7), $(7), o, and b given by Harville 

33 
(1989): 
ît:(7) = A'(X'jt)-X'y+ 
(2.17) 
$(?) = x ' i x ' x r x +JC'C - l'^c'Rii+iDr'^DR'c 
(2.18) 
a = a-'^C'RD{I + fD)-^R'(: , 
(2.19) 
and (when b  is taken to be the large-sample variance of 7) 
^ 
^ J]] A^-/(l + 7A,j) -(??.-p*) 
t=l 
*\-l I] A^7(l + 
U=1 
21 - 1  
(2.20) 
To obtain an expression for w, note that, as 7 —> 00, the linear system (2.1) 
becomes equivalent to the linear system (2.3). Thus, lim^y—>00 ^ = w, and it follows 
from (2.17) that 
w =  X ' { X ' X ) - X ' y  + C ^ R D - ' ^ l ' ^ t  . 
(2.21) 
Harville and Fenech (1985) showed that the matrix (J-f 7X)) is positive definite 
if and only if 7 > —(1/Ar). Thus, these results will also hold for parameter spaces 
of the form 
provided that a > —(1/Ar). 
2.6.2 
REML computations 
It follows from (2.16) that the concentrated likelihood function 
is pro­
portional to 
27r 
-(7Z-/)/2 
M(7)exp{-(n -  p*)/2} , 
(2.22) 
I h - / ) '  
since cr^ may be written as [ l / { n  — p*)]^(7;5e). REML calculations are carried 
out by maximizing the logarithm of (2.22). If the maximum is unique, the problem 

34 
is essentially equivalent to finding the solution to the equation 
^ 2 1 ^ = 0 .  
(2,23) 
07 
Newton's method, which is a well-known iterative method for solving nonlinear equa­
tions, can be used to solve (2.23). However, since this method is most likely to succeed 
when the equation to be solved is nearly linear, an appealing alternative would be to 
apply Newton's method to a linearized version of (2.23). Consider the equation 
(1 + 7 A )  [1 + 7Â -F 
_ q 
(2.24) 
where A = 
This equation is equivalent to (2.23) and is linear in the 
case 
= ... = Ar = Â (Harville and Callanan, 1989), Even if Aj,..., Ar are not 
all equal, it is to be expected that (2.24) is more nearly linear than (2.23). 
To allow for the restriction that 7  > 0, we first check the value of ^log 
z)ldf 
at 7 = 0. If this value is less than or equal to zero, we set 7 = 0. Otherwise, we 
take 7 to be the root of (2.23) obatined by applying Newton's method to (2.24). See 
Harville and Callanan (1989) for details, 
2.6.3 
Derivative expressions 
Computation of the approximate prediction intervals 15-17 requires the evalua­
tion of the REML information matrix Jr(crf ,7), the partial derivatives of the MSE 
approximations given in Section 2.4.2., and the quantity dlogldf, z)/dj. The ele­
ments of the information matrix are: 
Eld'^loiiySafl = 

35 
£(s2log(VÔ7<'|] = (-1/2)(,t|)-1 ^ A;/[H-7Ail 
i=l 
= (-1/2) ^ Af/[1 + 7Aij2 
Î = 1 
(Harville and Callanan, 1989). Partial derivatives of the MSE approximations can be 
computed from the following expressions: 
5^/07 = 
+ -yD)-f jR(f + 7JD)-2#2j%'( 
5a/(9cr| = a/cl , db/dcrl = 0 , 
da/d-i = -zo-lix'ix'xyx'zb!]d[i + -^or'^DiRz'x{ x ' x y x] , 
and 
96/97 = |-tU- 2 E ^  
2 / 1  
i = l ( l + 7 A i ) 3  
2 
(n - p*) 
E— 
.i=l(l+7Ai)2^ 
Li=ll + T^iJ 
which can be easily derived from the expressions given in Section 2.6.1. 
To compute the left side of equation (2.24), we must evaluate the partial deriva­
tive oflogfc(7;z) with respect to 7. Since 
d A { - f \ t ] S e ) l d ^  = - Xl A,j^?/(1 +7AJ^ , 
i=l 
it follows that 
aiogic(7;z) 
(n-P*) f- S f e i  A ; t ? / ( H - 7 A , j ) ^  1 1^ 
A,; 
2 
I s, + 
«?/(! + 7Ai)2 j 2 Aj (1 + 7A;) • ' • 

2.6.4 
Bootstrap computations 
To compute the bootstrap prediction intervals 18 - 110, it would seem that we 
must sample pairs 
from the joint distribution of w and y. Note, however, 
that expressions in Section 2.6.1 imply that t { w , y )  depends on lu and y  only through 
the values of t, Se, and w{^) — tv. Thus, it suffices to generate random samples from 
the joint distribution of t, Se, and tv{f) — lu — as discussed by Harville (1989) — 
instead of the joint distribution of lu and y. The details of the sampling scheme are 
given in Section 4.2.2. 
2.7 
Examples 
To illustrate the use of the frequentist point and interval predictors, we apply the 
procedures discussed in this chapter to the examples of Sections 1.2.2 and 1.3.2. The 
necessary programs were coded in Fortran 77, and IMSL (1987) subroutine DEVOSF 
was used for eigenvalue and eigenvector computations. All analyses were performed 
on the NAS AS/9160 computer at Iowa State University. 
2.7.1 
Bioequivalence study 
The blood serum levels of lithium carbonate given in Table 1.1 are to be used 
to assess the bioequivalence of formulations A, B, and C relative to formulation D. 
Preliminary graphical summaries of the data are given in Figures 2.1 and 2.2. From 
Figure 2.1 we see that formulation C was generally not as available as the other three 
formulations. This is as expected, since formulation C was a delayed release capsule. 
Figure 2.2 displays a marked difference in the behavior of observed serum levels of 

37 
lithium between the two administration times (days 1 and 8). Serum levels on day 
one were generally larger than those on day eight, and they were much more variable. 
This larger variability of the serum levels on day 1 can be attributed primarily to the 
observations from formulation C. 
To assess the bioequivalence of the new formulations and the standard formula­
tion using the method described in Section 2.1, we need to specify tolerance limits 
for the treatment differences. Typically, such limits are expressed by clinicians or 
pharmacologists in terms of the "relative bioavailability" — the ratio of the mean 
serum levels — for two formulations (Metzler, 1974). The tolerance limits for the 
bioavailability ratios can be easily transformed into tolerance limits for differences in 
the logarithms of the treatment means. 
Accordingly, we fit the mixed-effects version of model (1.2) to the natural log­
arithms of the serum levels given in Table 1.1. The "fitted values" (the empirical 
BLUP's of the functions /f + rj -f am + fSj for each 
combination represented 
in the sample), and their antilogs, are presented in Table 2.1. We took 7 to be the 
REML estimate 7=0.16, obtained as the value of 7 that maximized the function 
given by (2.22) (see Figure 2.3). The "residuals" (observed — fitted) are esti­
mates of the prediction errors. Diagnostic plots of the residuals are given in Figures 
2.4 and 2.5. No significant inadequacies in the model are evident from the normal 
plot and the plot of residuals versus fitted values. 
Table 2.2 displays the point (w, and ^u) and interval (11-110) predictors for the 
Ti — 
(A vs. D), T2 — 
(B vs. D), and rg — 
(C vs. D) contrasts. The interval 
predictors are those for a = 0.05. Intervals 12—110 are similar to each other, but 

38 
Serum Levels by Formulation 
Lithium Carbonate Data 
-r-
o 
-r-
A 
F o r m u l a t i o n  
Figure 2.1: Bioequivalence study - Dot plot of blood serum levels of lithium by 
formulation 

39 
Serum Levels by Day 
lithium Carbonate Data 
1 
8 
D a y  o f  A d m i n i s t r a t i o n  
Figure 2.2: Bioequivalence study - Dot plot of blood serum levels of lithium by day 
of administration 

40 
Table 2.1: 
Bioequivalence study - Fitted values 
for the logarithms of lithium serum 
levels (and their antilogs) 
Formulation 
Patient 
A 
B 
C 
D 
1 
-1.261 
(0.283) 
-1.236 
(0.291) 
2 
-1.230 
(0.292) 
3 
-1.217 
(0.296) 
-1.7.35 
(0.176) 
4 
-1.173 
(0.309) 
-1.716 
(0.180) 
5 
-1.193 
(0.303) 
6 
-1.216 
(0.296) 
7 
-1.188 
(0.305) 
-1.162 
(0.313) 
8 
-1.187 
(0.305) 
-1.223 
(0.294) 
9 
-1.794 
(0.166) 
10 
-1.209 
(0.299) 
-1.220 
(0.295) 
11 
-1.1.58 
(0.314) 
-1.676 
(0.187) 
12 
-1.203 
-1.747 
(0.300) 
(0.174) 

41 
Concentrated Likelihood 
Lithium Carbonate Data 
0 . 3 5  -
0  .  
3 0  -
0 . 2 0  -
2.8 
0 .O 
O . S  
1 . 0 
1  . 5  
2.0 
3 . 0  
G ammo 
Figure 2.3: 
Bioequivalence study - Plot of lc{^\z) 

42 
Normal Plot of Residuals 
lithium Carbonate Data 
* « 
« • 
« 
• 
T— 
-2 
—r— 
- 1  
—r-
o 
N o r m a l  Q u a n t i  l e a  
Figure 2.4: 
Bioequivalence study - Normal plot of residuals 

43 
Residuals vs Fitted Values 
Lithium Carbonate Data 
- 2 . 0 0  
- r  
- r  
- 1 . 7 5  
- 1 . 5 0  
- i . a o  
F i t t e d  V a l u e s  
-1 .oo 
Figure 2.5: 
Bioequivalence study - Plot of residuals versus fitted values 

44 
differ substantially, in both location and length, from Interval II. In particular, they 
are about 40% shorter than Interval II. Because of the relatively small estimate of 7, 
interval predictors 12 - 110 place considerable weight on the interblock information; 
this information is not used by Interval 11. The difference between 12 - 110 and 
II is especially great for the C vs. D contrast; there are no direct within patient 
comparisons of these two formulations, so that there is relatively little intrablock 
information about this contrast. 
To make judgements about bioequivalence, we specified limits of 0.8 and 1.2 on 
the relative bioavailabilities (see Metzler, 1974). Thus, we can conclude bioequiva-
lence if the endpoints of our prediction interval (on the logartihmic scale) are between 
-0.223 and 0.182. From these data, we are unable, at a confidence level of 95%, to con­
clude that any of the new formulations A, B, and C is bioequivalent to the standard 
formulation D. 
Often in the analysis of comparative experiments, a treatment difference is de­
clared significant or not based on whether or not the interval predictor for the differ­
ence in treatment means includes zero. Using this criterion in the current example 
would lead us to conclude that the difference between C and D is significant, but that 
those between A and D and B and D are not. 
2.7.2 
Crop-area prediction 
Consider now the prediction of the mean number of hectares per segment planted 
to corn in each of the 12 counties represented in the crop-area data set of Section 
1.3.2.1. Since Figures 2.6 and 2.7 suggest that the relationship between the survey 

45 
Table 2.2: 
Bioequi valence study - Estimates of treatment 
differences 
Estimators® 
A vs. D 
(n - ^4) 
B vs. D 
(^2-^4) 
C vs. D 
(^3 - 7-4) 
w 
0.24 
0.31 
-0.14 
•w 
0.01 
0.04 
-0.51 
âlh 
0.023 
0.023 
0.028 
V* 
0.016 
0.016 
0.016 
0.017 
0.017 
0.018 
V 
0.019 
0.019 
0,020 
11 
[-0.18, 0.66] 
[-0.11, 0.73] 
[-0.60, 0.33] 
12 
[-0.24, 0.26] 
[-0.23, 0.29] 
[-0.76, -0.26] 
13 
[-0.25, 0.27] 
[-0.22, 0.30] 
[-0.77, -0.25] 
14 
[-0.26, 0.28] 
[-0.23, 0.31] 
[-0.79, -0.23] 
15 
[-0.26, 0.28] 
[-0.24, 0.31] 
[-0.78, -0.23] 
16 
[-0.27, 0.30] 
[-0.25, 0.32] 
[-0.79, -0.22] 
17 
[-0.29, 0.31] 
[-0.26, 0.33] 
[-0.81, -0.21] 
18 
[-0.29, 0.30] 
[-0.25, 0.34] 
[-0.79, -0.22] 
19 
[-0.29, 0.30] 
[-0.24, 0.34] 
[-0.79, -0.22] 
no 
[-0.29, 0.30] 
[-0.24, 0.34] 
[-0.79, -0.22] 
^ ^^=0.015 <t|=0.039, 7=0.16, and a = 0.05. 

46 
Table 2.3: 
Crop-area prediction - Estimates (w = 10(7)) of fixed 
effects 
Parameter 
w(i) 
K'(ô'§,7)]V2 14 (a = 0.05) 17 (a = 0.05) 
51.07 
25.1 
[1.9,100.2] 
[-0.23,102.4] 
h  
0.33 
0.051 
[0.23,0.43] 
[0.22,0.43] 
-0.13 
0.057 
[-0.24,-0.02] 
[-0.25,-0.02] 
data on corn acreage and the satellite data on corn and soybean acreage is linear. 
Consequently, in making our predictions, we adopt model (1.5). Model adequacy was 
checked by computing the fitted values (empirical BLUP's) and the corresponding 
residuals (observed — empirical BLUP) for each data point in the sample. The REML 
estimate of 7 (7=0.95) was used in obtaining empirical BLUP's [see Figure 2.8 for a 
plot of lc[t\z)]. Residual plots are given by Figures 2.9 and 2.10; no deficiencies in 
the model are evident from these plots. 
Empirical BLUPs of the fixed effects (/?Q, 
and (32) are displayed in Table 2.3 
along with their estimated standard errors and the corresponding interval estimates. 
Table 2.4 contains the empirical BLUPs for the county means and estimates of the 
MSE's of the empirical BLUPs, as well as the results of the OLS analysis. The weight 
7/(7+ n.^^) assigned to the sample mean y of the 
county by the empirical BLUP 
(2.7) of the county mean is also given in Table 2.4 {i = 1,...,12). The interval 
predictors of the county means are given in Tables 2.5-2.7. For all counties, the MSE 
of w was estimated to be smaller than that of zv. Consequently, the approximate 95% 
prediction intervals 12 - 110 tended to be somewhat shorter than Interval II. The 
largest differences in the MS Es and the lengths of the intervals were those for counties 
with a single observation (Cerro Gordo, Hamilton, and Worth). 

4T 
Reported Corn vs Com Pixels 
Crop Area Data 
« 
« 
* * 
*  
• .  
• « 
« 
• 
* 
1 
1 
1 
1 
1 
1 
1— 
100 
180 
200 
200 
300 
390 
400 
480 
P i x e l s  C l a s s i f i e d  a s  C o r n  
SCO 
Figure 2.6: Crop-area prediction - Plot of reported hectares of corn versus pixels 
classified as corn 

48 
Reported Corn vs Soybean Pixels 
Crop Area Data 
220 • 
c 
L 
« 
0 200 • 
0 
« 
1-
ISO -
0 
B 
« 
« 
$ 
# 
1 100 -
L 
« • • 
0 
140 • 
* 
0 
« 
* 
# 
« 
• 
I 120 -
« 
« 
• 
• 
TJ 
• 
» 
« 100 -
• 
« 
« 
• 
+» 
« 
« 
« 
L 
« 
0 
a 80 -
• 
« 
• 
«0 -
1 
1 
r 
s o  
100 
ISO 
200 
2B0 
300 
3S0 
400 
P F x e l a  C l a a s l f f e d  a s  S o y b e a n s  
Figure 2.7: Crop-area prediction - Plot of reported hectares of corn versus pixels 
classified as soybeans 

49 
Concentrated Likelihood 
Crop Area Data 
0 . 3 0  
0 . 3 0  
0.25 
0.20 
O . O S  
0 . 0 0  
0 . S 
1  . S  
0 . 0  
1  . o  
2.0 
2.8 
3 . 0  
G amma 
Figure 2.8: 
Crop-area prediction - Plot of lc{l\z) 

50 
Normal Plot of Residuals 
Crop Area Data 
* 
, •  »  
«* 
«« 
t 
I 
I 
I 
I 
I 
I 
I 
-
3
-
2
-
1
 
0
 
1 
2
 
3
 
N o r m a l  Q u a n t i t é s  
Figure 2.9: 
Crop-area prediction - Normal plot of residuals 

51 
30 • 
2 0  •  
« 
0 
] 
V 
o 
• 
-10 
—20 • 
-30 
Residuals vs Fitted Values 
Crop Area Data 
« 
* 
* 
« 
• 
' 
• 
« * 
« 
« 
« 
• • 
« 
« 
1 
1 
1 
1 
1 
1 
r 
s o  
s o  
100 
120 
140 
100 
ISO 
200 
F i t t e d  V a l u e s  
Figure 2,10: 
C!rop-area prediction - Plot of residuals versus fitted values 

We display the estimates of the upper- and lower-0.025 points of the distribution 
of t{ io, y) (defined in Section 2.6.2) in Tables 2.8 and 2.9. Table 2.8 contains the points 
needed to construct the modified naive intervals 15-17, and Table 2.9 contains those 
needed to construct the bootstrap intervals 18-110. (The points for the bootstrap 
intervals were based on a "sample" of size of 5000.) Note that intervals 12-14 all use 
the value ~o.025 ~ ^-96. Note that when v is taken to be the MSE estimator, there 
is little difference between the percentage points produced by the three approaches. 
However, some differences among the percentage points are obtained when î'*(<t|,7) 
or v®(â-g,j) is taken to be the MSE estimator. Specifically, the percentage points for 
the bootstrap approach tend to be larger in absolute value than those for the other 
two approaches 
The results given here differ slightly from those given by Battese et al. (1988), 
who used ANOVA estimates of the error variance and variance ratio and used weights 
of a slightly different form than those in the empirical BLUP (2.7). 

53 
Table 2.4: Crop-area prediction - Summary of point prediction of corn 
hectares 
County 
H 7/(7 + ^.,:^) 
w 
V* 
V 
id 
â'ih 
Cerro Gordo 
1 
0.49 122.2 81.7 92.3 102.8 119.2 
187.3 
Hamilton 
1 
0.49 126.2 
79.7 90.2 
100.8 130.0 
167.0 
Worth 
1 
0.49 106.7 
76.7 86.9 
97.0 
95.0 
153.4 
Humboldt 
2 
0.66 
108.4 57.3 64.2 
71.2 
102.1 
93.6 
Franklin 
3 
0.74 144.3 37.7 
41.3 
45.0 
148.8 
50.7 
Pocohantas 
3 
0.74 112.1 38.3 42.0 
45.6 
115.9 
52.0 
Winnebago 
3 
0.74 112.8 38.1 41.9 
45.7 
109.2 
52.5 
Wright 
3 
0.74 122.0 
39.4 43.2 
47.0 
121.7 
53.9 
Webster 
4 
0.79 
115.3 30.0 32.3 
34.7 118.4 
37.7 
Hancock 
5 
0.83 124.4 
26.0 
27.9 
29.8 124.4 
32.0 
Kossuth 
5 
0.83 106.9 
25.0 
26.9 
28.6 
103.5 
30.3 
Hardin 
5 
0.83 143.0 
28.9 
31.0 
33.2 146.0 
36.6 
Note: 0'e = 149.56 0-2 = 147.27, 7=0.95. 
Table 2.5: Crop-area prediction - OLS and naive prediction intervals for corn 
hectares (ct = 0.05) 
II 
12 
13 
County 
Cerro Gordo 
Hamilton 
Worth 
Humboldt 
Franklin 
Pocohantas 
Winnebago 
Wright 
Webster 
Hancock 
Kossuth 
Hardin 
[ 90.8 , 147.6] 
[103.2 , 156.8] 
[ 69.3 , 120.7] 
[ 82.0 , 122.1] 
[134.0 , 163.5] 
[101.0 , 130.9] 
[ 94.1 , 124.2] 
[106.5 , 137.0] 
[105.7 , 131.2] 
[112.7 , 136.2] 
[ 92.1 , 114.9] 
[133.5 , 158.6] 
[104.5 , 139.9] 
[108.7 , 143.7] 
[ 89.5 , 123.9] 
[ 93.6 , 123.2] 
[132.3 , 156.3] 
[100.0 , 124.2] 
[100.7 , 124.9] 
[109.7 , 134.3] 
[104.6 , 126.0] 
[114.4 , 134.4] 
[ 97.1 , 116.7] 
[132.5 , 153.5] 
[103.4 , 141.0] 
[107.6 , 144.8] 
[ 88.4 , 125.0] 
[ 92.7 , 124.1] 
[131.7 , 156.9] 
[ 99.4 , 124.8] 
[100.1 , 125.5] 
[109.1 , 134.9] 
[104.1 , 126.4] 
[114.0 , 134.8] 
[ 96.7 , 117.1] 
[132.1 , 153.9] 
14 
[102.3 , 142.1] 
[106.5 , 145.9] 
[ 87.4 , 126.0] 
[ 91.9 , 124.9] 
[131.1 , 157.4] 
[ 98.9 , 125.3] 
[ 99.6 , 126.0] 
[108.6 , 135.4] 
[103.8 , 126.8] 
[113.7 , 135.1] 
[ 96.4 , 117.4] 
[131.7 , 154.3] 

54 
Table 2,6: Crop-area prediction - Modiiied naive prediction in 
tervals for corn hectares (a = 0.05) 
County 
Cerro Gordo 
Hamilton 
Worth 
Humboldt 
Franklin 
Pocohantas 
Winnebago 
Wright 
Webster 
Hancock 
Kossuth 
Hardin 
15 
[103.2 , 141.2] 
[107.6 , 144.9] 
[ 88.5 , 124.9] 
[ 92.9 , 123.9] 
[131.8 , 156.8] 
[ 99.5 , 124.7] 
[100.2 , 125.4] 
[109.2 , 134.8] 
[104.1 , 126.5] 
[114.0 , 134.8] 
[ 96.7 , 117.1] 
[132.1 , 154.0] 
16 
[102.1 , 142.2] 
[106.6 , 145.9] 
[ 87.5 , 125.9] 
[ 92.1 , 124.8] 
[131.2 , 157.4] 
[ 98.9 , 125.3] 
[ 99.6 , 126.0] 
[108.6 , 135.4] 
[103.7 , 126.9] 
[113.6 , 135.2] 
[96.3 , 117.5] 
[131.6 , 154.4] 
17 
[101.1 , 143.2] 
[105.6 , 146.9] 
[ 86.5 , 126.9] 
[ 91.3 , 125.6] 
[130.5 , 158.0] 
[ 98.3 , 126.0] 
[ 99.0 , 126.6] 
[108.0 , 136.0] 
[103.2 , 127.4] 
[113.2 , 135.7] 
[ 95.8 , 118.0] 
[131.1 , 154.9] 
Table 2.7: Crop-area prediction - Bootstrap prediction inter 
vais for corn hectares (a = 0.05) 
County 
18 
19 
110 
Cerro Gordo 
Hamilton 
Worth 
Humboldt 
Franklin 
Pocohantas 
Winnebago 
Wright 
Webster 
Hancock 
Kossuth 
Hardin 
99.7 , 144.1] 
103.7 , 148.1] 
82.8 , 128.7] 
91.1 , 126.5] 
129.9 , 157.9] 
97.5 , 125.9] 
99.0 , 127.5] 
107.7 , 136.5] 
103.5 , 127.3] 
113.1 , 136.0] 
95.8 , 117.6] 
131.0 , 155.3] 
[100.5 , 143.5] 
[104.7 , 147.5] 
[ 86.2 , 127.5] 
[ 91.8 , 125.1] 
[131.0 , 157.0] 
[ 98.3 , 125.4] 
[ 99.7 , 126.8] 
[108.3 , 135.6] 
[104.0 , 126.8] 
[113.9 , 135.3] 
[ 96.5 , 117.1] 
[131.8 , 154.4] 
[100.7 , 142.9] 
[105.0 , 147.0] 
[ 86.7 , 127.2] 
[ 92.4 , 124.9] 
[130.9 , 157.2] 
[ 98.6 , 125.1] 
[ 99.9 , 116.8] 
[108.6 , 135.4] 
[104.1 , 126.6] 
[114.1 , 135.2] 
[ 96.8 , 117.0] 
[131.8 , 154.3] 

55 
Table 2.8: 
Crop-area prediction - Estimated quantiles for the modified naive 
prediction intervals for corn hectares 
15 
16 
17 
County 
^1 
^0.025(^1) 
h  ^0.025(^2) 
h  ^0.025(^3) 
Cerro Gordo 
17.5 
2.11 
19.8 
2.09 
21.8 
2.07 
Hamilton 
19.4 
2.09 
22.6 
2.07 
25.3 
2.06 
Worth 
20.8 
2,08 
24.5 
2.06 
27.5 
2.05 
Humboldt 
28.3 
2.05 
32.1 
2.04 
33.0 
2.03 
Franklin 
33.0 
2.03 
31.4 
2.04 
27.6 
2.05 
Pocohantas 
32.9 
2.03 
31.7 
2.04 
28.3 
2.05 
Winnebago 
32.8 
2.03 
32.0 
2.04 
28.8 
2.05 
Wright 
32.9 
2.03 
31.8 
2.04 
28.4 
2.05 
Webster 
32.4 
2.04 
29.2 
2.04 
25.2 
2.06 
Hancock 
31.9 
2.04 
28.3 
2.05 
24.2 
2.06 
Kossuth 
31.6 
2.04 
27.6 
2.05 
23.4 
2.07 
Hardin 
32.3 
2.04 
29.2 
2.04 
25.3 
2.06 
Table 2.9: Crop-area prediction - Estimated quantiles for the bootstrap 
prediction intervals for corn hectares 
County 
18 
19 
no 
County 
,(1) 
'n.n% 
,(3) 
"n.n2.s 
Cerro Gordo 
-2.49 
2.42 
-2.26 
2.21 
-2.12 
2.04 
Hamilton 
-2.54 
2.45 
-2.27 
2.24 
-2.11 
2.07 
Worth 
-2.47 
2.51 
-2.20 
2.23 
-2.03 
2.08 
Humboldt 
-2.28 
2.40 
-2.07 
2.09 
-1.90 
1.95 
Franklin 
-2.35 
2.21 
-2.07 
1.98 
-1.93 
1.85 
Pocohantas 
-2.37 
2.22 
-2.13 
2.06 
-2.00 
1.92 
Winnebago 
-2.23 
2.39 
-2.03 
2.16 
-1.91 
2.00 
Wright 
-2.27 
2.30 
-2.09 
2.07 
-1.95 
1.95 
Webster 
-2.16 
2.19 
-2.00 
2.03 
-1.90 
1.92 
Hancock 
-2.20 
2.27 
-1.98 
2.06 
-1.88 
1.97 
Kossuth 
-2.21 
2.14 
-2.00 
1.96 
-1.89 
1.89 
Hardin 
-2.22 
2.30 
-2.02 
2.05 
-1.94 
1.97 

56 
3. 
BAYESIAN APPROACH TO PREDICTION 
The empirical BLUP ib and the corresponding prediction intervals 12 - 110 have 
some unappealing properties that manifest themselves when 7 is zero or close to zero. 
Specifically, when 7=0, ih reduces to 
{X'X)~X'which is the BLUP for the 
submodel y = X(3 + e. Thus, when 7=6 in the estimation of treatment contrasts 
from an IBD design, w completely ignores the existence of the blocks, and, when 
7=0 in the estimation of a small-area mean, lu puts all of the weight on the estimate 
of 
and none on the average 
for the 
small area. Furthermore, when 
7 is zero or close to zero, the contribution of the estimator 7 to the variability of 
lu — w may not be satisfactorily approximated by a x 6 (Kackar and Harville, 1984). 
Consequently, when 7 is zero or close to zero, the performance of the MSE estimators 
,7) and i), and of the corresponding prediction intervals 13, 14, 16, 17, 19, and 
110 may not be altogether satisfactory. 
An alternative to the approach discussed in Chapter 2 is the Bayesian approach 
to inference about w. This approach to prediction begins with a specification of a 
prior distribution for the parameters of the distribution of y. We then calculate the 
conditional distribution of lu given y, which is known as the posterior distribution of 
lu. All inferences about lu are based on this distribution. 
In this chapter we outline a Bayesian approach to point and interval prediction 

57 
under the two-part mixed model, discuss the computational aspects of its implemen­
tation, and apply it to our examples. We also discuss two related methodologies, 
hierarchical Bayes and empirical Bayes, and explore the relationships between them. 
3,1 
Prior and Posterior Distributions 
3.1.1 
Priors for /3 and 0 
Under the mixed-effects linear model (1.8), the distribution of y has parameters 
(3 and 0 = (<Te,7) to which we must assign prior distributions. We will assume that, 
a priori, /3 and 9 are statistically independent, and take the prior distribution for /3 
to be MVN(a,£j), where a is a known pxl vector, and g is a known positive scalar. 
Then, the prior density for /3 is 
. 
(3.1) 
Since the quantity of interest lu depends on /3, it may be appropriate to choose a 
and s so that 7r]^(/3) provides little information about (3 relative to what we hope to 
obtain from the data. Such a noninformative prior will be characterized by the fact 
that 
is "flat" relative to the likelihood of y. Note that for "large" g, the prior 
distribution for (3 becomes diffuse, and may be regarded as noninformative. 
We take the prior distribution for the vector 0 to be a proper or improper dis­
tribution with p.d.f. equal or proportional to 7r2(o'|,7), where 
^2(^e>7) = <^'-'l(7)(<^e)^''2(7)exp{-(2<T|)~VT'3(7)} . 
(3.2) 
Here C?i(7), (^2(7), and ^'3(7) are arbitrary functions of 7 such that G]^(7) > 0, 
(^2(7) < [n—p* — 4)/2, and 03(7) > 0. Prior distributions of the general form (3.2) 

58 
will prove to be computationally tractable. Moreover, they are broad enough to cover 
a wide range of prior opinions. Special cases of (3.2) include the priors used by Tiao 
and Draper (1968), Hill (1965), Selwyn and Hall (1984), Box and Tiao (1968, 1973), 
Broemeling (1985), and Portnoy (1971), as well the following: 
1. Jeffreys (1961) Rule — applied to the REML likelihood l((j|,7;2): 
Gl(T) =^{{n-p*] E  A | / ( l + 7 A i ) 2  
.i=l 
U=1 
2^ 2 
^2(1) = -1 , and C?3(7) = 0 . 
(3.3) 
2. Macedo and Gianola (1988), Ghosh and Lahiri (1988), and Datta and Ghosh 
(1989): 
a , M  = 
, ,(-<,1/21-1, 
r(^o/2)r(^i/2) 
^ 2 ( 7 )  = - ( ^ 0 + ^ l ) / 2  ) and ^ 3 ( 7 )  =  ag +  « l ( 7  
,  
(3.4) 
where aQ,gQ,ai, and gi are known hyperparameters. This distribution is ob­
tained by assuming that (cr|)~^ and (cr|)"~^ are statistically independent, 
that (cTg)"^ has a Gamma (oQ/2,g'Q/2) distribution, and that (cr|)~^ has a 
Gamma {ai/2,gi/2) distribution. [Letting r(') represent the gamma function, 
the Gamma (o:,j3) distribution is the distribution with p.d.f. 
/(i|a,/3) = (r(/3)l-lc<'^exp{-/Jx} 
(i > 0) ,) 
3. Stroud (1987): 
G i ( 7 ) =  
+  
,  

59 
<^2(7) = -[N/2) - 3] , and 
= i/Q x mg , 
where r, vq, and mg are known hyperparameters. 
Note also that the uniform prior over 0 is trivially obtained by setting G]^(7) = 1, 
and G'lil) = Gg('y) = 0. 
3.1.2 
Posterior distributions; f { w  |  
y )  and /*(w | y )  
Under the mixed model (1.8), the conditional distribution of xo and y given /3 
and 0 is MVN with mean vector and covariance matrix 
< 
» 
and 
Vyj 
'^yw 
X(3 
Vyxv V  y 
X'a 
and 
Xot 
respectively. It follows that the distribution of w and y given 9 is MVN with mean 
vector and covariance matrix 
V'w 4- zA A 
f^yu) 4" sX^ 
"^yw 4" sXX 
-f- sXX^ 
respectively. Let g{iu,y | 
represent the p.d.f. of this distribution. Clearly, 
g{w,y I e) = 
I y,0) x g2{y | 0) 
where gi(w | y,0) is the p.d.f. of the conditional distribution of tu given y and 0, 
which is normal with mean 
lu^ = A'a + {v'y,^ + e\'x' ) { V y  + sXX')-^{y -  Xa) 
and variance 
= {vuo + cA' A) — {vyxu + eX^X^){Vy + eXX^) ^[vynj + sXX), 

60 
and g^iy \ 
is the p.d.f. of the conditional distribution of y given 0, which is MVN 
with mean vector Xa and covariance matrix Vy + sXX'. Thus, the distribution 
on which the Bayesian would base his/her analysis is that with p.d.f. 
f{w I y) = 
^ 
I 
I 
(3.5) 
Jn92iy 
"/W 
When £ is "large," the results of Dempster, Rubin, and Tsutakawa (1981) and 
S alias and Har ville (1981) suggest that f{xu | y) can be approximated by the p.d.f. 
f*{tu I y) oc ^^^(w I y,e)g^{z \ e)ir2io-^,f)d0. 
Here g^{iu \ y,0) is the p.d.f. of a normal distribution with mean w('y) and variance 
t'*(cr|,7), and g^iz | 0) is the p.d.f. of a set of error contrasts (which is proportional 
to (2.8)). Using (2.16), we can write 
y*(w I y) = ^ ^ i ( w  I y,O)h{0 I z)d0. 
where 
h(0 I z) = (l/Â!Î)/(<72,7;2:)7r2((r|,7) 
and 
Refer to Harville (1989) for more details. In what follows, the p.d.f. /*(w | y) will 
be regarded as the posterior density of w. 
It should be noted that f{ iu | y) and f*{iu | y) may be more properly regarded as 
predictive densities, following Berger (1985, p. 157) and Broemeling (1985, p. 16). A 
predictive density corresponds to a conditional distribution of an unobserved random 
variable given y rather than to a conditional distribution of a model parameter given 

61 
y. Typically, though, the random variable is a future observation. Since we have 
defined iv explicitly as a function only of the vectors /3 and s, we have chosen to 
r e t a i n  t h e  " p o s t e r i o r "  l a b e l  w h e n  r e f e r r i n g  t o  f { x u  |  y )  a n d  / * ( w  |  y ) ,  
3.1.3 
Characterizations of f*{xu | y )  
As a practical matter, Bayesian inference requires that various characteristics of 
the posterior be computed. In this section we describe features of f*{iv | y) which 
are analogous to quantities used in the frequentist approach. That is, we develop 
point and interval predictors for xu from a Bayesian point of view. 
While we cannot ascertain the shape of f*{w | y )  analytically, empirical evidence 
indicates that /*(w | y) will be unimodal except for a set of highly unusual %/-values. 
The mean of /*(u' | %/), 
/oo 
xuf [w I y )dxo , 
-OO 
is an important characteristic of the posterior distribution when /*( ty | 2/) is unimodal 
and nearly symmetric. The posterior mean xu^ is the best point predictor of xu in 
the sense that 
f  
{ l u - x u * f f * { w  \ y )dw , 
J —oo 
which is the posterior expected squared error loss of an arbitrary point predictor ic*, 
is minimized by taking xu* = XUQ. Note that the posterior mean is expressible as 
= JQ 
\ 
\ z)d9 = j ^xL{j)h{6 \ z )dO . 
(3.6) 
Another important characteristic of the posterior distribution of xu is the posterior 

variance, that is, the variance of f*{iu | y), which is given by 
vq = J _ ^ { w -  WQ)'^f*{w \ y)dtv 
=  
I  y,0)dxv h{e I z)de 
= 
+ M?) - 
I z)(Z6 . 
(3.7) 
The Bayesian counterpart to a prediction interval is a credible set. A set S of 
(u-values is said to be a 100(l-a)% credible set if P*(5 | y) = 1 — a, where 
I y) = 
I 
is the posterior probabihty of a set S. The smallest 100(l-a)% credible set is S{ka) =  
{•w : f*{to I y) > ka}^ where ka is a constant such that P*{S{ka) | y) = 1 - a (see, 
e.g.. Berger 1985, p. 140). This set is called the 100(1 — a)% Highest Posterior 
Density (HPD) credible set. 
The form of the HPD credible set will depend on the shape of f*[xu \ y), and. 
may not be an interval. One choice for an interval predictor based on f*{iv | y) is 
111 : [/, u] , 
with endpoints I and u such that /*(/ | y) = f*{u | y), and jP*(tt;£[/, u] | y) = 1 — a. 
(Note that for some values of y. Interval 111 may not be unique). By definition, such 
an interval is a 100(1 — a)% credible set, and, if the posterior distribution of tu is 
unimodal, it is the 100(1 — a)% HPD credible set. Since empirical evidence indicates 
that the posterior distribution of w will generally be unimodal, we adopt interval 111 
as an interval approximation to the 100(1 — a)% HPD credible set for w. 

63 
3.2 
Computational Aspects 
For the most part, attention in the literature has been restricted to those special 
cases of model (1.8) and prior distribution (3.2) that are sufficiently simple that closed-
form expressions can be obtained for the relevant posterior distributions. The results 
of Stroud (1984) are an exception - they allow for the possibility of unbalanced data, 
though they are left in a form not well-suited for computations. Ghosh and Lahiri 
(1988), and Datta and Ghosh (1989), also consider unbalanced data, but only in the 
special case where G-'i(7), ^'2(7), and (^3(7) are of the form (3.4). 
The focus here is on the efficient computation of the relevant characteristics of 
the posterior distribution of w when the data are possibly unbalanced. In particular, 
we derive efficient expressions for numerically evaluating f*(iL> | y), 
V£, and 
f*(5" I y), and develop an efficient iterative procedure for computing Interval 111. 
We do so by taking advantage of various results from Section 2.9. 
3.2.1 
Computing /*(w | y), P*(5 | y), lu^, and V£ 
Various of the expressions presented in Section 3.1.3 can be simplified by observ­
ing that they involve integrals with respect to 
of the general form 
and by then using, for example, formula A2.1.6 in Box and Tiao (1973) to show that 
Such simplification is possible only because of a judicious choice for the form of the 
prior distribution of 9. 
(where p > 0, a > 0, a > 0) , 
^ (l/a)a-(P/«)r(p/2) . 
(3.8) 

64 
For c > 0, define 
6(7, c) = [R - p* - 202(7) - c]/2 , 
5I(7,C) = M(7)G]_(7)r[6(7,c)] , 
(where r['] represents the gamma function), and 
jB2(7;^;^e) = (l/2)[/l(7;(;^e) + (;3(7)i -
Then, using (3.8), we find that 
^ /(cr2,7;z)7r2(o-|,7)c^cr| = (27r)~("~P )/^ 
X 
M(7)Gi(7)((T|)~('^~P*~2^2(7))/2 
i/o 
xexp{(-2iT|)~'[Gi(7) + .4(7;t;5e)J}<i7 
= (27r)-("-P*)/2Bi(7,2)B2(7;t;5e)"''''''^' , 
implying in particular that 
=(2r)-("-P*)/2^°°Bl(7,2)B2(7;t;5e)"''<'''^>c<7 . 
(3.9) 
Thus, the p.d.f. ^^(7 | z ) ,  defined by 
^1(7 I 2) = 
h( 0  I z)d(7^ ,  
is expressible as 
'•1(7 I 
= (fc|J-^Bi(7,2)B2(71«;5e)-''<''''^' 
where tg = 
(Note that if z were regarded as the data vector, then 
the distribution with p.d.f. 
| z) would be the posterior distribution of 7.) 

65 
Consider now the posterior p.d.f. f*{tv \ y). Once again, using (3.8), we find 
that 
By making use of expressions (3.9) and (3.10), f*{w | y) can be expressed as the 
ratio of two one-dimensional integrals. In certain relatively simple special cases, these 
integrals can be evaluated analytically — see, for example. Box and Tiao (1968, 1973). 
However, in general, they must be evaluated by numerical integration. In carrying 
out the numerical integration, the two integrands must be repeatedly evaluated for 
different values of 7. Note that, once A^,..., Ar and R have been computed, the 
repeated evaluation of the integrands requires — in light of expressions (2.17) and 
(2.18) for ^(7) and ^(7)— very little additional computation. 
x[fc|] lM(7)Gi(7)(cr|) ^(')''^)exp{-(o-2) ^Se)}d(xld'y 
( 2 ; r ) - ^ 1 ( 7 , 1 ) $ ( 7 ) - ^ / ^  
(3.10) 
Next, consider P*{S |  y). We find, using (3.10), that 
^*(•5" I 2/) = 
\y)dw 
= (27r)~ 
dw d j  .(3.11) 
By taking u = 26(7,2), 
[ty - tô(7)]^/î7 

66 
and 
s* = 
k, some W.S 
{ 
[2<l{j)B2(r,USe)]^/^ 
and applying a change of variable, the inner integral in expression (3.11) can be 
re-expressed as 
where 5[-,-] is the beta function. Then, using formula A2.1.11 in Box and Tiao 
(1973), we find that 
P*(5 I y) = 
. 
(3.12) 
where f^^(S*) is the probability that would be assigned by a Student's t distribution 
with u = 26(7,2) degrees of freedom to the set 5*. Note that if S is an interval [/, u], 
then S* is the interval 
Formula (3.8) can also be used to simplify expressions (3.6) and (3.7) for lujg 
and 
respectively. We obtain 
roo roo 
n 
roc 
= Jq 
/o 
I z)d(Tid^ = ^  iu{j)hi{j \ z)d'f , 
(3.13) 
which is a weighted average (with respect to 7) of the BLUP 1^(7), and 
= JQ JQ 
{^*(<7'eJ) + [w(7)- w^]^}/!(a I z)(fcr§(f7 
= 
^1(7,4)^2(7; 
x{^(7) + [w(7) - W Q f [ B 2 { j ; t ;  5e)][6(7,4)]}c?7. 

67 
We note that, to evaluate the expressions of this section, all integration with 
respect to 7 is performed numerically. Thus, the expressions may be easily adapted 
to situations involving restricted parameter spaces of the form 0*^. 
3.2.2 
Computing credible sets 
Interval 111 can be computed iteratively via the solution of certain nonlinear 
equations. Define S{k) to be an interval [l{k).,u{k)\^ whose endpoints are such that 
f*[l{k) I y] = f*[ii{k) I y] = k, and take ka to be a solution to the following nonlinear 
equation (in k)\ 
P * [ S { k ) \ y ] - { l - a )  =  0 .  
(3.14) 
Then, S{ka) is interval 111. We solve (3.14) iteratively by picking initial values k^~^^ 
and k^^\ and generating successive iterates 
... using 
, i a .  
I !,l - (1 -
This is the secant method (Press et al., 1986). If 
satisfies (3.14) within some 
specified tolerance, then the procedure is terminated after the 
iteration, and ka 
is set equal to k^^\ On each iteration, we must — for a new value of fc — compute 
l{k) and «(fc) as the two solutions to the following nonlinear equation (in p): 
f * { p \ y ) - k  =  0  . 
(3.16) 
Again, the secant method is used to iteratively compute the two solutions to (3.16). 
In applying the secant method to the solution of equation (3.14), we take k^^^ 
to be the value of k that would be the solution to (3.14) if P*[S{k) | y] were replaced 
by Pr[vYe5(Â;)], where A' is a normally distributed random variable with mean W Q  

68 
and variance V Q. The scalar 
is then chosen to be close to 
Specifically, 
let 
= xuQ - 
and calculate 
= 
1 y)- Then, if f 
1 2/) < 1 - a, we choose 
= A;(®) -
(fc(®)/2), otherwise we take 
= min[A;(®) + 
l2)^kmax\^ where kraax =  
(27rv^)~V2. 
Similarly, in applying the secant method to the solution of equation (3.16), we 
determine the initial values of y from an approximation to (3.16) in which /*(• |  y) 
has been replaced by the p.d.f. of X. Here, we take 
;(®)(A;) = xoQ - [-2i;^log(27r-t;^Â;^)^/'^]^/^ 
and 
•U(Q)(A!) = wjg + [-2i;^log(27ri;^Â;^)^/^]^/^ , 
and then choose l^~^\k) and u^~^^k) to be close to l(^\k) and u.(^)(A;) respectively. 
The secant method was selected because it is a derivative free method. This is 
a d v a n t a g e o u s  b e c a u s e  t h e  d e r i v a t i v e s  o f  P * [ S { k )  |  
y ]  ( w i t h  r e s p e c t  t o  k )  a n d  f * { p  |  y )  
(with respect to p) cannot be written in closed form. However, the secant method can 
be numerically unstable (Kennedy and Gentle, 1980, p. 74), in which case convergence 
is not possible. To remedy this, we investigated a hybrid procedure that combines the 
secant method with the method of bisection. The procedure is similar in spirit to the 
Dekker-Brent method (Press et al., 1986). Consider its use in solving (3.14). First, 
modify the above approach to picking initial values in order to insure that k^~^^ and 
bracket the solution ka- Then, on the 
iteration, we generate k^^^ using (3.15), 
unless 
falls outside the interval whose endpoints are 
and 
In the 
latter case we use the method of bisection: (i) set 
= 1/2(A;(^"'^) + A:(^~'^)), {ii) 

69 
set k* equal to whichever of the values 
or 
causes P*[S{k*) | y] —(1—q) 
to be opposite in sign from 
| y] — (1 — a), (iii) on the next [(i + 1)'®^] 
iteration, use k* in place of 
We continue until 
satisfies (3.14) within the 
specified tolerance. This procedure is guaranteed to converge, like bisection, and will 
generally do so at the faster rate of the secant method. 
An approximation to Interval 111 is the interval 
112 : lug ± z^i2{vq)^I'^ , 
where 
is the upper-a/2 point of the standard normal distribution. The appeal 
of Interval 112 is that its endpoints can be computed much more cheaply than those 
of Interval 111. [In fact, the endpoints of Interval 112 are used to determine initial 
values for the solution of (3.14)]. 
3.3 Related Approaches 
3.3.1 
Hierarchical Bayes approach 
The posterior p.d.f. f ( w  |  y )  (and hence /*(w | y ) )  can also be derived from a 
fully Bayesian approach to the fixed-effects version of (1.8). This approach, involving 
the use of multi-stage prior distributions, is known as hierarchical Bayes (HE). 
Regard s as a vector of unknown parameters (fixed effects) rather than as a 
vector of random effects. In this case the conditional distribution of xv and y given 
/3, s, and 
is MVN with mean vector and covariance matrix 
X'j3 + 6's 
0 
0 
X'j3 + 6's 
and 
Xj3 +  Z s  
0 afj 

70 
respectively. We form the prior distribution of /3, s, and (t| in three stages. In 
the first stage, assign s a distribution — conditional on the values of /3, cr| and a 
hyperparameter 7 — whose p.d.f. is 7rj2(s | /3,ct|,7); then, in the second stage, 
assign (3 and cr| a distribution — conditional on the value of 7 — whose p.d.f. is 
7r22(/3,(Zg 
I 7); and finally, in the third stage, assign 7 a distribution with p.d.f. 
Now, suppose that 
| /3,(r|,7) is the p.d.f. of a MVN distribution with 
mean vector 0 and covarian.ce matrix fg/, or equivalently that 
If 7r22(/3,<T| I 7) oc 
exp{-(2<Tg )'"^Gg(7)}, where 7ri{f3) is the p.d.f. 
given by (3.1), and if 7r2(7) oc (^^^(7), then clearly, the p.d.f. of the posterior dis­
tribution for w obtained by this three-stage approach is p.d.f. (3.5). Further, if s  
is assumed to be "large," the HB posterior distribution can be approximated by the 
distribution with p.d.f. f*{w |  y). 
The HB approach is described, in the context of various special cases of the fixed-
effects version of (1.8), by Ericksen and Kadane (1985), Ghosh and Lahiri (1988), 
and Datta and Ghosh (1989). This approach differs from the Bayesian approach to 
the mixed-effects model only in the way in which the distribution of s is regarded. 
Under the mixed model, distributional assumptions about s are regarded as having 
a frequentist interpretation. In the HB approach, the distribution of s is regarded as 
a prior opinion. 

71 
3.3.2 
Parametric empirical Bayes approach 
As in the HB approach, parametric empirical Bayes (PEE) inference procedures 
are based on a conditional distribution for iv given y, which is derived using a mul­
tistage prior on the parameters of the fixed-effects model. However, in this approach 
the priors are assumed to be known only up to the value of one or more hyper pa­
rameters. Before applying the Bayesian paradigm, these hyperparameters must be 
estimated from the data so as to complete the specification of the prior distribution. 
In the PEB approach as applied to the present setting, the prior distribution is 
typically specified only up to the values of cr^ and 7. Point and interval predictors are 
devised by starting with the predictors that would be appropriate if 
and 7 were 
known and by then replacing the values of cTg and 7 with estimates. Modifications 
may be introduced to account for the effects of this substitution. 
The properties of PEB inference procedures for lu (e.g., bias, MSE, or coverage 
probability) are typically defined with respect to the joint conditional distribution of 
w and y given /3 and 0 (Morris, 1983a,b). In the present context, that is equivalent 
to defining these properties with respect to the joint conditional p.d.f. g{ w,y \ (3,6) 
derived from mixed model (1.8). For example, in our context, Morris' ( 1983a,b) 
definition of a PEB confidence interval may be stated as: an interval I(y) is a 100(1-
a)% PEB confidence interval for iv if 
P°[we/(y)] = 1 — Q 
for all l3eRP and all 0eQ, where the probability operator P°{-) is defined with respect 

72 
to the joint conditional distribution of tu and y given /3 and 6. That is, if 
for all (3eRP and all Oe^l. This is the same as our definition of a 100(l-a)% prediction 
interval. Thus, the frequentist procedures in Chapter 2 are all interprétable as PEB 
procedures. The two approaches differ only in the way in which the distribution of s 
is regarded. 
If (Tg and 7 were known, then, in the limiting case (as £ —> oo and hence where 
the prior distribution of /3 is noninformative), the posterior distribution of w would 
be the distribution with p.d.f. 
| y,0), so that the posterior mean would be w(7) 
and the 100(1-q:)% HPD credible set would be 
•w(7) ± 0^/2«7e[^(7)]^/^ • 
This set has a probability of coverage of 1 — a when the probability is determined 
under the joint conditional distribution of w and y given /3 and 6, as is easily verified. 
A simple implementation of the PEB approach is to take the point predictor to be 
10(7) and the prediction interval to be 
(3.17) 
(which is similar to the naive prediction interval 12). 
However, prediction interval (3.17) may not be satisfactory in that it fails to 
account for the additional uncertainty about the value of w that comes from not 
knowing a\ and 7. Morris ( 1983a,b), Rubin (1982), and Laird and Louis (1987), 
restricting attention to a special case of the prediction problem and taking (T\ to be 

73 
known, discussed PEB intervals of the form 
w { j )  ±  
where W is chosen — on the basis of Bayesian, bootstrap, or Monte Carlo consid­
erations — so that (tg^ is a better estimator of var[w(^) - u'] than is 
To 
account for 
not being known, we could, following Berger (1985, p. 172), adopt the 
interval 
The application of the PEB approach to small-area estimation and to other spe­
cial cases of the problem of predicting the value of w has been considered by, e.g.. Fay 
and Herriot (1979), Dempster et al. (1981), Dempster et al. (1984), Laird and Ware 
(1982), Stroud (1984), Ghosh and Meeden (1986), and Dempster and Raghunathan 
(1987). For applications not covered by these previously considered special cases, the 
frequentist approaches discussed in Section 2.5 can be used to obtain PEB intervals 
for the value of xu. 
3.3.3 
Relationships 
The frequentist and Bayesian approaches to prediction under the mixed model 
were developed from different perspectives. It is useful to consider each approach from 
the viewpoint of the other. In order to discuss relationships between the approaches, 
we adopt a particular viewpoint. An extensive examination of the frequentist prop­
erties of the two approaches may be found in Chapter 4. In this section, we briefly 
discuss the usefulness of the frequentist procedures from the Bayesian perspective by 
reviewing some results from the literature on approximate Bayesian methods. These 

74 
results were derived for the purpose of approximating Bayesian answers in situations 
where the computations required to implement the full Bayesian analysis are pro­
hibitive. While we have shown that the Bayesian analysis of the current problem 
need not be computationally difficult, the results are of interest in that they demon­
strate a relationship between certain of the frequentist and Bayesian procedures. 
Consider special cases of (1.8) which admit the partitions 
y = 
( 
\  
y\ 
\ Vra 
X  =  
X  m 
s  =  
/ 
\ 
n 
•Sm 
/ 
\ 
(3.18) 
and 
Z  = diag(fci,... ,fcm) , 
(3.19) 
where the 
and 
are 
x 1 vectors, and the Xj^ are 
x p matrices. Note 
that the commonly used models for the analysis of comparative experiments and 
for small-area estimation are of this type. We will also assume that the quantities 
of interest may be written as A'/3 -t- 6s.^, where 6 is a constant. Again, treatment 
contrasts and small-area means are typically expressible in this way. 
Suppose now that y follows this special case of the model (1.8), and consider a 
Bayesian analysis in which the prior distribution for (3 and 0 is of the form considered 
in Section 3.1.1. Then, the model and prior collectively belong to a class of models 
and priors called conditionally independent hierarchical models (CIHMs) by Kass and 
Steffey (1989). For a CIHM, it is possible to obtain asymptotic approximations to 
various Bayesian quantities. Specifically, under certain regularity conditions (see 

75 
below), it follows from the results of Kass and Steffey that, 
H Q = 
H{9 )h{e I z)de =  H{h) +  O(m-l) , 
(3.20) 
and 
-  HQ)^h{e I z)de =  à S à  + 0(m-^) , 
(3.21) 
where H{0) is a function of 0, 9  is the value of 9  that maximizes h{9 |  z), à  is the 
vector of first partial derivatives of H{9) evaluated at 9, and È is the inverse of the 
negative Hessian of log[/;(0 | z)] evaluated at 9 (see equations 3.3 and 3.4 in Kass and 
StefFey). They obtain this result through an application of Laplace's method (e.g., 
Tierney and Kadane, 1986). Letting, for example, H(9) = •w{f), equation (3.20) 
provides us with an asymptotic approximation to the posterior mean xu^. The same 
approximation, derived using Taylor series, is presented by Harville (1989). 
In order for (3.20) and (3.21) to be applicable, certain regularity conditions must 
be met. Roughly, as stated by Kass et al. (1988), the requirements are that h{9 |  z) 
be dominated by a single mode, and that log[A(^ | z)] and H{9) have second-order 
Taylor series expansions. Formal statements of regularity conditions are given by 
Kass et al. ( 1989). 
It follows from Kass and StefFey's remarks that, as m —> oo, the effect of any 
given prior on the expectation and variance is of the same order as the terms neglected 
by the approximations. Thus, 0, and consequently à and Ï7, can be computed using 
a different prior than that used to construct h{9 | z), and approximations (3.20) and 
(3.21) will still hold. If we take T r 2 { c ' e =  1, then Ô is the REML estimate of 0, È  

76 
is the inverse of the matrix 
Î W h l )  = -
dlogl*{(T^,j;z) 
dode' 
and à is the same as the vector a(â-f ,7) described in Section 2.5.1.2. The implications 
for our prediction problem are summarized in the following result: 
Result 3.1 Assume that y follows the special case of the mixed model described by 
(1.8), (3.18), and (3.19), that prior distributions defined by (3.1) and (3.2) are as­
signed to (3 and 0, that xu = A'/3 + 5s.j^, and that the regularity conditions of Kass et 
al. (1989) are met. Then, 
wg = J^ iù{j)h{6 \ z)dO =  10(7) + 0{m~^) , 
^(•iZ>(7) - xo^fhiO I z)de = a'(5-|,7)J(ô-|,7)a((T|,7) + 
, 
and 
I z)dd =  v*(â'^,^)+ 0{m~^) .  
In light of expressions (3.6) and (3.7), it follows from Result 3.1 that, ih is an 
0(tti~^) approximation to lug. Furthermore, v®(ô'g,7) differs from the 0(7??"^) ap­
proximation to 
in that 
uses the expected information matrix ^^(0-^,7) 
rather than the observed information matrix 
Thus, Result 3.1 has impli­
cations for the analysis of comparative experiments (and for small-area estimation). 
Specifically, it suggests that when the number of blocks (or small areas) in the sam­
ple is large and when the requisite regularity conditions are met, we can expect the 
frequentist and Bayesian procedures to produce similar results. Empirical evidence of 
this similarity can be found in the animal breeding example discussed by Carriquiry 
(1989). 

77 
3.4 
Examples 
We now apply the Bayesian inference procedures developed in this chapter to the 
examples of Sections 1.2.2 and 1.3.2. The necessary computer programs were coded 
in Fortran 77, and the IMSL (1987) subroutines DQDAG and DQDAGI were used 
to perform the numerical integrations. All analyses were run on the NAS AS/9160 
computer at Iowa State University. 
3.4.1 
Bioequivalence study 
To carry out a Bayesian analysis of the lithium carbonate data in Table 1.1, 
we must specify a prior distribution for 0 = (f|,7)'. Let us adopt the noninfor-
mative prior (3.3) obtained by applying Jeffreys' (1961) rule to the REML likeli-
hood /(cTg ,'y; z). A plot of the prior density over a portion of ÇI is shown in Figure 
3.1. Figure 3.2 displays the REML likelihood over the same region, and a compar­
ison of the two figures clearly shows that 7r2(cr|,7) is "flat" relative to l{cr^,'y;z) 
(i.e., noninformative). The posterior density h{9 | z), which is proportional to 
7r2((T|,7) X l{(Tg,f;z), is similar in shape to, but more peaked than, /(cr|,7;z) (see 
Figure 3.3). The plot of /!]^(7 | z) in Figure 3.4 (which may be compared to the 
concentrated likelihood in Figure 2.3), also exhibits this behavior and indicates that 
the posterior mode is at 7=0. 
As in Section 2.7.1, our analysis will focus on the treatment differences 
(A vs. D), T2 — 7-4 (B vs. D), and rg — 
(C vs. D). Bayesian inference about 
these differences is based on their posterior distributions. The posterior densities are 
pictured in Figure 3.5, and selected characterizations of these densities are presented 

78 
Table 3.1: Bioequivalence study - Characterizations of the predic­
tive densities for three treatment differences 
A vs. D 
(n -T4) 
B vs. D 
(^2 - -^4) 
C vs. D 
(^3 - 7-4) 
WB 
0.03 
0.08 
-0.47 
0.015 
0.015 
0.017 
111 (a = 0.05) 
[-0.21, 0.27] 
[-0.16, 0.33] 
[-0.73, -0.21] 
112 (a = 0.05) 
[-0.21, 0.27] 
[-0.16, 0.33] 
[-0.73, -0.21] 
f *([-0.223,0.182] 1 y) 
0.838 
0.748 
0.075 
in Table 3.1. Note that the Bayesian point predictor W Q  differs somewhat from 
the frequentist predictor w in each case. To see why, note that, as shown in equation 
(3.13), WQ is obtained by averaging the BLUP w('y) with respect to /2]^(7 | z). Figure 
3.4 shows the shape of /ii(7 | z), and Figure 3.6 demonstrates how the BLUP changes 
with 7. Together they illustrate why wg differs somewhat from ih. Note that Interval 
112 proved to be an excellent approximation to Interval 111. 
To assess the bioequivalence of the new formulations to the standard formulation, 
we could use the Bayesian interval predictors in the same way as their frequentist 
counterparts. That is, we would conclude bioequivalence if Interval 111 is completely 
contained within the prespecified tolerance limits. Although the Bayesian intervals 
111 and 112 are about 20% shorter than the corresponding frequentist intervals 12-110, 
they lead to the same conclusion — none of the new formulations is bioequivalent 
to the standard. A more direct measure of bioequivalence could be obtained by 
calculating the posterior probability of the interval defined by the tolerance limits 

79 
Table 3.2: Crop-area prediction - Summary of Bayesian prediction 
of corn hectares 
County 
W f j  
111 (a = 0.05) 112 (a = 0.05) 
Cerro Gordo 
1 
122.0 86.4 
[103.4,140.4] 
[103.8,140.3] 
Hamilton 
1 
126.3 84.0 
[108.3,144.7] 
[108.4,144.3] 
Worth 
1 
106.7 97.2 
[87.0,125.5] 
[87.3,126.0] 
Humboldt 
2 
108.6 66.6 
[92.5,124.4] 
[92.6,124.6] 
Franklin 
3 
143.9 46.3 
[130.4,157.1] 
[130.5,157.2] 
Pocohantas 
3 
112.0 42.7 
[99.1,124.9] 
[99.2,124.8] 
Winnebago 
3 
113.0 43.4 
[100.1,126.0] 
[100.1,126.0] 
Wright 
3 
122.0 40.4 
[109.4,134.5] 
[109.5,134.4] 
Webster 
4 
115.1 .34.2 
[103.5,126.5] 
[103.6,126.5] 
Hancock 
5 
124.6 
27.1 
[114.3,134.8] 
[114.4,134.8] 
Kossuth 
5 
107.4 32.1 
[96.4,118.7] 
[96.3,118.5] 
Hardin 
5 
142.8 32.9 
[131.4,154.0] 
[131.5,154.0] 
(e.g., Selwyn, Dempster, and Hall (1981), Mandallaz and Mau (1981), and Selwyn 
and Hall (1984)). That is, calculate f^([A^, 
| y), where A| and 
represent 
the tolerance limits for the difference in log means. This quantity represents the 
posterior probability that the true treatment difference is within acceptable limits. 
These probabilities, for the Emits -0.223 and 0.182, are given in Table 3.1. They 
provide a numerical measure of "closeness" of each of the three new formulations to 
the standard. 
3.4.2 
Crop-area prediction 
As in the Bayesian analysis of the lithium carbonate example, 0  was assigned a 
noninformative prior density of the form (3.3). A plot of this prior density is given 
in Figure 3.7. Plots of the REML likelihood and the posterior density of 6 are given 
in Figures 3.8 and 3.9, respectively; the marginal posterior density of 7 is depicted 

80 
O . 93 
1 . 20 
1 . sa 
a . BO 
O AMMA 
Jeffreys Prior 
Lithium Carbonate Data 
PR I OR 
1 . 2 1  •  
Figure 3.1: Bioequivalence study - Contour and surface plots of the Jeffreys' prior 
for 0 
O . 1 X 
O . O0 
a 
(D < 
a  o  .  o i  
o 
w 
O . 03 ' 
O . OO PO 

81 
o . 1 2 
O  .  O B  
O . 
O . 
OO 
OAMM>K 
REML Likelihood 
Lithium Carbonate Data 
R K M L  
1 3  .  2 5  
1 .  a a  
OAMMA 
s I  OMASQ* 
Figure 3.2: Bioequivalence study - Contour and surface plots of the REML likeli­
hood /((7|,7;z) 

82 
o  •  1 a  
o  •  o o  
o  •  
o  •  0 3  •  
O . 
O O  
o o  
Posterior Surface 
Lithium Carbonate Data 
H T H B T A  
O . 3e • 
1 . 8 8  
O A M M A  
a  I  omaiPq' 
Figure 3.3: 
Bioequivalence study - Contour and surface plots of the posterior density 
'  
o i e  

83 
Posterior of Gamma 
Lithium Carbonate Data 
1.1 
1 . 0  
0 . 9  
0.8 
0 . 7  
O . S  
0 . 5  
0 . 4  
0 . 3  
0.2 
0 .  1  
0 . 0  
0.0 
O . S  
1  . 0  
1 . 8  
2.0 
2.8 
3 . 0  
Oomma 
Figure 3.4: 
Bioequivalence study - Plot of 
5^(7 | z )  

84 
A vs D 
a  .  a  
3 . 
o 
a  .  o  
a  .  o  
1 .  a  
1 .  o  
o  .  a  
o  .  o  
— 1 
a . a 
a . o 
a  .  a  
2  .  O  
1 .  a  
1 .  o  
o  .  a  
• O  •  O  
— O  # 2  
B vs D 
O . 
2 
o  .  a  
o  .  o  
— 1 
a  .  a  
s  .  o  
2  .  a  
2  .  O  
1 .  a  
1 . o 
o  .  a  
• o  
~ o  $ a  
" O •  2  
C VS D 
O . 
2 
o  .  a  
o  .  o  
— 1 .  o  
—o .  a  
—o . 2  
o  .  2  
o .  a  
Figure 3.5: Bioequi valence study - Posterior densities for the three treatment dif­
ferences 

85 
A vs D 
o  .  o a  H 
o . o a  
o  .  OH• 
o . o a -
o  .  oo-I 
a  
3  
o  
1 
Oamma 
B VS D 
o  .  
a . 
o , 
O . 00-1 
3  
O 
1 
a  
O a mm a 
C VS D 
.  s o  H 
. o a ^  
2 
O 
1 
3  
O a mm a 
Figure 3.6: Bioequivalence study - BLUPs of the treatment differences as a function 
of 7 

in Figure 3.10. Table 3.2 gives the Bayesian point and interval predictors of corn 
hectares per segment for the 12 counties. Plots of the posterior densities for three of 
the counties, Cerro Gordo, Franklin, and Hardin, are given in Figure 3.11. Plots of the 
BLUP iy(7), as a function of 7, for the same three counties are given in Figure 3.12. 
As in the lithium carbonate example, Interval 112 provided a good approximation to 
Interval 111. The Bayesian point predictor lujg and the empirical BLUP gave similar 
results, as did their associated measures of variability. Also, the Bayesian intervals 
were also very similar to Intervals 12 - 110. 

87 
O 
S  1  o o  
o  
O A M M A  
Jeffreys Prior 
Crop Area Data 
P R  I  O R  
1 . 02 
O  .  7 0  
o. a *  
2  .  S O  
1  .  
8 8  
OAMMA 
1 .  28 
O  .  O B  2 0 0  
lab 
Q . 03 
1 ab 
120 
O  .  O O  
100 
Figure 3.7: Crop-area prediction -
density for 9 
Contour and surface plots of the Jeffreys' prior 

88 
a. 80 
O • 83 
/ y / / / /  Y o.oo 
120 
10O 
W surface plots of tUe 
•O 
Ctop-A«|-
kood 

89 
Posterior Surface 
Crop Area Data 
m 
O. o o  
of the posterior density 
o.o°a<?S' 
i«o # 1 OMA®' 
-Area prediction 
.*o 
lOO 
Figure 
of 9 

90 
Posterior of Gamma 
Crop Area Data 
0 . 7  
0 . 5  
0 . 4  
0 . 3  
0.2 
0 . 0  
0 . 5  
1  . 0  
1 . 5  
2.0 
2 . 5  
3  .  
0  
Oamma 
Figure 3.10: Crop-Area prediction - Plot of /ij(7 ! z )  

91 
Cerro Gordo 
o  .  
o a  
o o  
o  .  
0 2  
o  .  0 0  
110 
1  a o  
1  s o  
1 T O  
00 
Franklin 
o  .  o a  
0 . 0 a  
0 2  •  
o  .  0 0  
1  s o  
0 0  
110 
1 T O  
Hardin 
o a  
o  .  O S  
o . 00 -I 
1  3 0  
ISO 
1 T O  
Figure 3.11: Crop-Area prediction - Plots of the posterior densities for three counties 

92 
Cerro Gordo 
1 34^ h 
123-
1 2 2 -
O 
1 
2 
3  
oamma 
Franklin 
1  s o  i  
1 3 5 -
1 30 -4 
2 
O 
1 
3  
O a mm a 
Hardin 
Figure 3.12: Crop-Area prediction - BLUPs of mean hectares of corn per segment 
for three counties, as a function of 7 

93 
4. 
FREQUENTIST PROPERTIES OF THE PREDICTORS 
In this chapter, we consider the frequentist properties [under model (1.8)] of 
the point and interval predictors discussed in Chapters 2 and 3. In particular, we 
investigate the MSE of the point predictors, the adequacy of the MSE approximations, 
and the coverage probabilities and expected lengths of the prediction intervals. We 
begin the chapter with a review of some basic results which were used in designing a 
Monte Carlo study of the frequentist properties of the predictors. We then describe 
the Monte Carlo study and results obtained from it. Throughout this chapter, we 
assume y  follows the mixed model (1.8) and w =  A'/3 +  6^s. 
4.1 
Properties of the Predictors 
4.1.1 
Basic results 
Let 
represent the group of transformations defined by 
/b,c : (w,%/) —> {cw +  X'b,cy +  X b )  .  
We say that a real-valued function g[iv^y) of lu and y  is .F-equivariant if 
g{cw +  X'b,cy +  X b )  =  cg(iu,y} + A'fe 
(4.1) 

94 
for all values of 6, c, w  and y  (e.g., Lehman, 1983). Furthermore, we say that g{-w^y) 
is relatively .F-invariant if, for all values of 6, c, w, and y, and for some real-valued 
function q{-) 
g{ciu + X'b,cy + Xb) = q{c)g{iu,y) . 
(4.2) 
Ifç(c) = 1 in (4.2), then aiw^y) is simplv said to be .F-invariant. Note that a function 
of y or 10 alone may be considered a function of both y and lu. Note also that if 
g{iu,y) satisfies (4.1) for all values of b, w, and y and for some ?(•) such that g(l) = 1, 
then g{w,y) is translation invariant. 
The following result is given by Jeske (1985, Lemma 3.3, p. 106). 
Result 4.1 A function g{w,y) of lo and y is translation invariant if and only if it 
depends on y only through z® = (z',^]^)' where z — L'y, 
= r'y + w, and r is 
such that r'X + A' = 0 (that such an r exists follows from the assumption that w is 
estimable). 
Result 4.1 implies that any .F-invariant function g{xo, y )  will depend on xo and y  only 
through z®. In that case g{xu,y) may be written as a function, say Q{z®), of 
and we say that Q{z®) is an F-invariant function of z®. Note that a function of z 
alone can be regarded as a function of z*®. 
We now present several results which will be helpful in the assessment of the 
frequentist properties of the various point and interval predictors. 
Result 4.2 Let d{y) he an T-equivariant predictor of xo and let Q{z®) be any real-
valued J^-invariant function of z ® .  Then, the conditional distribution of d{y) -  xu 
given Q(z®) is symmetric about zero. 

95 
This is a special case of Theorem 3.2 in Jeske (1985, p. 107). Note that Result 4.2 
implies that the unconditional distribution of the prediction error of an .F-equivariant 
predictor is symmetric about zero. Thus, conditionally and unconditionally, an 
equivariant predictor will be unbiased for w (provided the expected value of the 
predictor exists). 
We now present two results which will be used in the interpretation of results 
from the Monte Carlo study. 
Result 4.3 Let w* = [ly — A^/3]/crg, y* = {l/crç)[y — 
and z* = (l/(Tg)z®'. 
Also, let g{w,y) be a relatively J--invariant function of lu and y, and let Q{z®) be 
any real-valued J--invariant function of z ® .  Then, the joint distribution ofg{w*,y*) 
and Q{z*) is the same as the joint distribution of q{llae)g[w^y) and Q{z®), 
Result 4.4 Let g(w,y,e), where e > 0 is some constant, be a function of w and y 
such that for all values of h, c, xo, and y and for e > 0 
g{cw + \'b, cy + Xh,e) = g{iu, y, e/c). 
Also, let Q(z® ) be an !F-invariant function of z®, and define tu* and y* as in 
Result 4-3. Then, the joint distribution of g{w*,y*,e) and Q(z*) is the same as the 
conditional distribution of g{tu,y,e/(Te) and Q{z®), 
Results 4.3 and 4.4 indicate how the unconditional and conditional [on Q{ • )] 
distributions of relatively .F-invariant functions depend on (3 and £r|. Thus, the 
results can be used to relate unconditional or conditional [on Q{ • )] expectations 
of certain functions g{iu,y) under different choices of (3 and Oq. In particular, if 

96 
g{xu,y) is .T^-invariant, it follows from Result 4.3 that the expected value of g { w , y )  
only in a simple way. Result 4.3 is proved in the appendix. The proof of Result 4.4 
is similar and is omitted. 
The following two results, which are easily veriûed, are presented here for future 
reference. Note that part 5 of Result 4,5, when coupled with Result 4.3, can be 
used to verify the claim made in Section 2.5.1.3 concerning the distribution of the 
approximate pivotal. 
Result 4.5 Let d{y) be an J--equivariant predictor of w, let in{y) be a nonnegative 
relatively 
-invariant function of y  with q{c) =  <?, and let I { y )  =  [pi{y),pu{y)] be 
an !F-equivariant interval (the interval [{y) is said to be T-equivariant if its endpoints 
are T-equivariant). Then, 
1. The function g^^\w,y) = [d{y) — w]'^ is relatively J--invariant with q{c) = c^. 
2. The function 
does not depend on the value of (3 and cr|. When g{iu,y) is relatively .F-invariant, 
but g(c) ^ 1, the expectation of g(îi),y) does not depend on (3 and it depends on cr\ 
I 0, otherwise 
is J- -invariant. 
3. The function g^^\iu, y) = Puiy) — Pi(y) is relatively 
-invariant with g{c) 
4- The function 
(A], 
,  
i f w  +  e  s l { y )  
0, 
otherwise 

97 
is such that for all values of w, y, c, and b, and for e > 0, 
g{ciu +  A'6, c y  +  X b , e )  =  g{iu, y ,  e/c) 
5, The function 
g ^ ^ \ i u , y , t )  
is !F-invariant. 
Result 4.6 Let I{y) = [pi{y)tpu{y)], where 
P l i y )  = d{ y )  +  ki(z'^)[m(y)]^/'^ ,  
Pxiiy) =  d i y )  +  k u i z ® ) [ m i y ) ] ^ / ^  ,  
and ku{z®) are T-invariant functions of z®, d{y) is an J^-equivariant pre­
dictor o f w  and m { y )  is a  nonnegative relatively J--invariant statistic with q{c) =  c^. 
Then, l{y) is an J- -equivariant interval. 
In the following two sections, we discuss the ^-equivariance and ^-invariance 
of the statistics presented in Chapters 2 and 3. Then, in Section 4.1.4, we discuss 
how Results 4.1 - 4.6 are helpful in the assessment of the frequentist properties of 
the frequentist and Bayesian point and interval predictors. Toward these ends, we 
introduce some additional notation. Let 
1, if{d{ y )  -  w)/[m(y)]V2 < t  
0, 
otherwise 
y °  =  y°ic,b) =  c y  +  X b  

98 
represent a transformed data vector, let 
=  c z ,  and let tv° = ciu + X'b. 
Note that y° ~ MVN(X(c/3 + h),c^Vy), 
~ MVN(0,and w° ~ 
MVN(A'(c/3 + h),(P'viu). In what follows, we will sometimes append %/ or z as an 
argument to various quantities that are functionally dependent on %/ or z, respectively. 
For example, we sometimes write ô'g(y) for the REML estimator 
of (t|. 
4.1.2 
.F-equivariance of frequentist predictors 
Using equation (2.21), we see that the OLS predictor w computed under the 
transformed model is 
= c-u) +A'(X'X)-X'X6 
since 
= X. Recalling that A' = 
X for some vector u, we have 
w ( y ° )  =  cw +  \ ' b  ,  
i.e., w is .F-equivariant. By using (2.17) in a similar fashion, it can be shown that 
w{'f) is also jF-equivariant. 
It is easily verified that 
= c26'e(z) , 
= c2A(7;((z);ge(z)) , 
and /c(7;2°) oc l c { ' ) \ z ) .  Thus, the estimators ô-f, â g ,  and âg are nonnegative rel­
atively .F-invariant functions of z (with q{c) = c^), and 7 and 7 are f-invariant. 
In light of results (2.17) - (2.20), it is now clear that the empirical BLUP ih is ^-
equivariant, and that the three MSE estimators 
,7), t'®(crg,7), and v are all 
relatively .F-invaiant. 

99 
Therefore, it follows from Result 4.2 that w, like w, is unbiased for tu (provided 
that the expected value of iv exists). To conclude that prediction intervals II - 110 
are .F-equivariant, we must show that the estimates of the percentage points of the 
distribution of t{w,y) are .F-invariant. Clearly the 
points used in 12 - 14 and 
the tQ,j2if) point used in II, which do not depend on the data, are .F-invariant. 
The expressions in Section 2.6.3 imply that the quantities t>i, i>2, and 
depend 
on the data vector only through the value of 7, so, since 7 is F'-invariant, ùi^ f/g, 
and 
are all .F-invariant. Similarly, since the distribution of the simulated pivotal 
quantities depends on the data only through 7, the bootstrap percentage points are 
also F'-invariant. Thus, it follows from Result 4.6 that the intervals II - 110 are 
.F-equi variant. 
4.1.3 
F"-Equivariance of the Bayesian predictors 
In general, 
and Intervals 111 and 112 will not be .F-equivariant. However, 
they will be F'-equivariant for those members of the class of prior distributions (3.2) 
that have a certain invariance property. Note that the group of transformations T 
induces a group of transformations Q on Q defined by; 
9c 
' (c^cr|,7) • 
A function ^(<Tg,7) of (t| and 7 is said to be relatively ^-invariant if 
= 92(^)^(^0-^,7) 
for all constants k > 0 and some real-valued function 92(')' We also define a relatively 
^-invariant prior distribution to be a prior distribution on Q, whose p.d.f. is a relatively 

100 
^-invariant function of cr\ and 7. A characterization that is useful in identifying such 
priors, is given by the following result, which is verified in the Appendix: 
Result 4.7 A member of the class of prior distributions (3.2) is relatively Q-invariant 
if and only if G2{'y) = 
is a constant, and (^3(7) = 0. 
We now show that the Bayesian predictors are .F-e qui variant if 7r2(cr|,7) is rel­
atively ^-invariant. Since 111(7) is JF-equivariant, /(c^(r|, 7; 2:°) oc /(cr|,7;2:), and 
7r2(cr2,7), we have 
On ^ /Q[ew(7) + A^6]/(c^o-|,7;z°)7r2(c^(T|,7)<^^ 
= f^[cw{^) +  \ ' b ] h{e\ z ) d 9  
= 
+ A'6 , 
i.e., w q  is .F-equivariant, and hence unbiased for w, for all relatively ^-invariant 
choices of the prior p.d.f. 
within (3.2) (for which the expected value of wg 
exists). Similarly, since •y*(cr|,7) is a relatively ^-invariant function (with ^2('^) = K), 
the integrand of (3.7) is relatively .F-invariant, and so vq is relatively F-invariant 
(with g(c) = (?). Thus, it follows from Result 4.6 that Interval 112 is J^-equivariant. 
It is apparent from the definition of Interval 111 that, unless /*(w | y) is uni-
modal. Interval 111 is not necessarily unique. The .F-equivariance of Interval 111 will 
therefore depend on the "consistency" with which the algorithm of section 3.2.2 per­
forms when it is applied to f*{iv \ y) relative to when it is applied to f*{iu° | y°). 
More specifically, if 5'(A;o[%/]) = [/(^"afî/])) ^'(^«[y])] is the interval obtained in solv­
ing (3.14), then the interval 5(/i;a[î/°]) = [^(^'a[2/°]), •ii(^'a[2/°])] which is obtained in 

101 
solving 
, 
(4.3) 
must be such that 
^(^•a[y°]) = c/(A;Q[y]) + A'Ô 
(4.4) 
and 
^'(^•a[2/°]) = cti{ka[ y ] )  +  \'b . 
(4.5) 
Note that, in computing the solution to (4.3) we require, for various trial values of 
^[î/°], the solutions (with respect to p) of the equation 
f*{P I y°) - ^'[2/°] = 0 . 
(4.6) 
It is verified in the Appendix that the algorithm of section 3.2.2 is such that (4.4) and 
(4.5) are satisfied, and that, as a consequence, Interval 111 has the following property: 
Result 4.8 If 
relatively Q-invariant, then Interval 111 is J--equivariant. 
4.1.4 
Some relevant criteria 
As indicated in Chapter 3, it is not only the overall performance of the var­
ious predictors that is of interest, but also their performance conditional on 7=0. 
Each of the criteria to be used in studying the various point and interval predic­
tors can be formulated as a conditional (on 7= 0) or unconditional expectation of 
some function, say y(w, y), of w  and y .  In particular, the MSE of a  predictor d{y) 
of XV is obtained by setting ip[w^y) = g^^\iu,y), the probabilities of «overage and 
false coverage of an interval I(%/) are obtained by setting (p{w,y) = g^'^\w,y) and 

102 
ip{iu,y) = g^'^\iu,y), respectively, and the expected length of an interval I { y )  is 
obtained by setting ^p{io,y) — g^^^iu,y) (see Result 4.5). 
The major implication of Results 4.3 and 4.4 is that in studying the conditional 
(on 7 = 0) and unconditional frequentist properties of J'^-equivariant point and in­
terval predictors, as well as relatively .F-invariant estimators of the MSE, we may, 
without loss of generality, take (3=0 and (Zg =1 (since 7 is an .F-invariant function 
of z®). Properties under other choices of (3 and cTg can be easily obtained via the 
relationships in Results 4.3 and 4.4. For example, since g2(w,y) is .F-invariant, prob­
abilities of coverage do not depend on the values of (3 and <Tg. 
The expressions given in Section 2.6.1 imply that all of the point and interval pre­
dictors presented in Chapters 2 and 3 depend on w and y only through the values of 
X'{X'X)~X'y, t, 5e, and •w{j) — xu. For convenience, define 7/2 = 
X)~X^y, 
and TJ2 = w{^)—-w, and let ^(??i,i,5e,72tT) represent the p.d.f. of the joint distribu­
tion of rii, t, Se, and 7/2. As discussed by Harville (1989), 7/2 and t are independent. 
Furthermore, using the results of Harville and Call an an (1989, sec. 4), it can be shown 
that (t/j, t, 772) is independent of Se- If ^p[w,y) can be written as a function, say 
i^*(77]^,i,5e,77j), of 7/]^, f ,  5 e ,  and 7/2, then the unconditional expectation of tp{iu,y) 
can be expressed as 
/•OO 
r  
f O O  
f O O  
^ 
J - o o J r ^  J - o o J o  
S e ,  
S e , V 2 ' ^  f  )  d S e  d 7 ] i  d t  d T ] 2  
or, since 
^(^l,*,'^e,'72!7) = ^l(5'e)^2(^l I 
'72!7)^3(*;7)^4(^/2!T) , 
where 
7), and ^4(772;7) ^.re the p.d.f.'s of the distributions of S e ,  t ,  and 

103 
7/2, respectively, and <^2(^1 I 
is the p.d.f. of the conditional distribution of 
TJi I t,i]2 (Jeske, 1985, Lemma 3.1), the unconditional expectation of (p{w,y) can be 
expressed as 
11 IbT IZ 
i S e  
x^2(^i I 
4i^3(^:7X4('72:'yX( 
• 
(4.7) 
Consider now the conditional expectation of <p{iu,y} given 7=0. Recall that we 
set 7=0 if and only if 
<9 loff I r ' l ' v :  z  )  
< 0  .  
d - f  
7=0 
It follows from equation (2.25) that this is equivalent to the event 
Se + iZ 
-P*) j 
i = l  
I Li=l 
that is, the event Se >  C{t) where 
r 
.i=l 
> 0 
(4.8) 
£ { t )  =  min | 
0, - 
+ (n - p*) 
i = l  
E thi 
Lt=l 
E A i  
i=l 
Thus, the conditional expectation of ip{-w,y) given 7=0 can be expressed as 
*t 
/*oo 
r 
roo 
roo 
lP(5e > £(t))J- 
X  
l _ ^ J , ^ r J _ ^ l m ^ V * i l h t , S e , r i 2 ) h i S e ) < I S e  
x^2ivi I *5^2:7) 
7)^4(^2;7)(/( d r ] 2  . 
(4.9) 
Note also that the conditional expectation of ^{iu,y) given 7 > 0, can be expressed 
as 
[P(SE < X  
l ^ ^ / j ^ r j Z c l Q ' ^ ' ' * ^ ' f ' ' < ' 1 h t , S e , ' ! 2 m S e } d S e  
x^2(^l I ^,^2;7)c(?7lf3(^;7)^4(^2:7) 
^V2 ' (4.10) 

104 
that the probability of event (4.8) is expressible as 
P(Se > m )  = 
fZjllSe)dSe(3(t-,l) i t  , 
and that an appropriately weighted linear combination of expressions (4.9) and (4.10) 
equals the unconditional expectation of <f{w^y). 
4.2 
Monte Carlo Study - Description 
In general, integrals (4.7), (4.9), and (4.10) cannot be written in closed form, and 
it is feasible to evaluate them numerically only in certain special cases (e.g., balanced 
data; see Jeske and Harville, 1988). Typically, they must be evaluated using Monte 
Carlo methods. In this section we describe the implementation of these methods in 
our study of the frequentist properties of the frequentist and Bayesian predictors. 
Recall that we are assuming that /3=0, and (t|=1. 
4.2.1 
The Monte Carlo method 
General discussions of the use of the Monte Carlo method in the evaluation of 
definite integrals are given by Hammersley and Hanscomb (1964), Rubinstein (1981), 
and Kalos and Whitlock (1986). The following algorithm summarizes the application 
of the method to the evaluation of the unconditional expected value of ip{iv,y). 
Algorithm 4.1 For a specified value of f (and for (3= 0 and cr\ — 
generate N in­
dependent samples 
), i = 1,..., 
from the joint distribution 
with p.d.f 
Se,y]2'>^)- Then, compute 
/ = iV-l £ /(%('),fit').*!'),',;'')) , 
i=l 

105 
and take I to be an estimate of integral (4-7). Further, estimate the variance of I by 
J V - 2 ^  f/(SeW,,i(0,tW,,2(i))_/l^ 
i=l 
The distribution with p.d.f. 
5'e,î;2 ! 7)) from which we draw our samples, 
is known as the importance sampling function. In principle, we could use Algorithm 
4.1 to evaluate the conditional expectations (4.9) and (4.10) as well as the uncondi­
tional expectation (4.7). For example, by replacing (^*(5'e, //j^,^,7/2) with the function 
we could use Algorithm 4.1 to evaluate (4.9). However, a more efficient approach 
is obtained by redefining the importance sampling function. To do so, let T(u(()) 
represent the conditional probability P{Se < v{t) | t), where v{t) is a nonnegative 
function of t. Then note that, given t, t/j, and 7/2, we can write the innermost integral 
in (4.9) as 
y**('S'e,f7l,t,772) defined by 
0, 
otherwise. 
poo 
= [1 - ï(£(4)))/d 'p'{n,t,Se, nHl(Se)dSe 
(4.11) 
where 
if Se >  C(t) 
otherwise. 
Similarly, 
= T(£(t)) 
f*(ni,t,Se,r)2)e{'(Se)dSe 

106 
where 
(^ 
0, 
otherwise. 
To evaluate (4.9), we modify Algorithm 4.1 by substituting 
Se,r}2 ; 7) for 
^{Vht,Se,V2 ; ?), where 
( * { V l , t ^ S e , r ] 2  ] l )  =  ^ i { S e K 2 ( V l  \
• 
That is, we adopt the following algorithm. 
Algorithm 4.2 For a specified value off (and for [3 = 0 and 
- I), generate N 
independent samples 
)> ^ — 1, " -, A^, /rom the joint distribu­
tion with p. d.f. ^*{r)i,t,Se,ri2;j). Then, compute 
I = 
AT 
... 
' 
i=1 
- 1  N 
i = l 
and take I to be an estimate of integral (4-9). Further, estimate the variance of Î by 
N / 
x 2 
i = l ^ 
where 
C'(0 
N 
iV-l ^[l- ï ( £ ( i W ) ) )  
i=l 
- 1  
Similarly, a Monte Carlo estimate of conditional expectation (4.10) can be obtained 
from the algorithm produced by substituting 
C * { V l , t , S e , V 2 ' n )  = ^ r i S e ) ^ 2 i ^ i  | 
for 
Se,r]2',f) in Algorithm 4.1. 

107 
If we use the Monte Carlo method to evaluate the conditional expectations (4.9) 
and (4.10) of y(w, ^) given 7 = 0 and 7 > 0, respectively, then it is not necessary to 
make further use of the Monte Carlo method to evaluate the unconditional expectation 
(4.7) of ip{xu,y). By computing P{Se > ^{t)) and 1 - P{Se > 
we may 
obtain the unconditional expectation as a weighted combination of the conditonal 
expectations. 
Some specifics of the Monte Carlo evaluation of expressions (4.7), (4.9), and 
(4.10) are given in Section 4.2.2. 
4.2.2 
Computational aspects 
Assume that /3=0 and cr|=l. Then it follows from the results of Harville (1989), 
and Harville and Callanan (1989) that Se has a  chi-square distribution with n  —p* — r  
degrees of freedom, that t ~ MVN(0, f -j- 7!)), and that 7/2 ~ N(0, ^(7)). Moreover, 
the results of Harville and Callanan (1989, Sec. 4) can be used to show that the 
conditional distribution of 7]i given t and 7/2 is normal with mean 
=  ' ï \ ' i X ' X ) - x ' Z R D ^ ^ ' ^ { I  +  ' y D r h  +  < î f { f ) { X \ x ' X ) - X  
-fC'z'XiX'xrX + 
+ 7D)-'^DRZ'X{X'xrX}r]2 , 
and variance 
=  x ' { x ' x ) - \  +  - f [ \ \ x ' x r x ' z ] [ x \ x ' x ) - x ' z ] '  
-J'^{X'{X'X)~X'Z]D{I 4- -yOr'^iX'iX'xrx'Z]' 
-^{f){X'{X'X)-X - jc'z'x{x'xrx 
+ ^ ^ C ' R { I  +  i D ) - ' ^ D R Z ' X { X ' X ) - X } ^  . 

108 
The evaluation of integrals (4.7), (4.9), and (4.10) by the Monte Carlo method 
requires the generation of the random quantities i, Se, 7/2» 
^1* To generate these 
quantities, we employ the following sampling scheme: (i) First, generate t. Since 
the elements oi t = (^^,..., (r) are a set of independent normal random variables, 
we generate r N(0,1) random variables zi,,..zr and apply the transformation 
= 
(1 +  
(ii) Second, to evaluate expressions (4.7), (4.9), or (4.10) generate Se 
from the distribution with p.d.f. ^i{Se), ^|(5e), or ^^*(Se), respectively. Note that 
^l(5e) is the p.d.f of the chi-square distribution with n — p* — r degrees of freedom, 
and ^i(Se) and ^2*(5e) are the p.d.f.'s of truncated versions of that distribution. To 
generate Se from the distribution with p.d.f. ^^(Se), we first generate a Uniform(0,l) 
random variable 
then set 
where T(-) represents the c.d.f. of the chi-square distribution with n — p*—r degrees 
of freedom. This is an application of the "inverse c.d.f." method of random variate 
generation (e.g., Devroye, 1986). Similarly, to generate Se from the distribution with 
(m) Generate 7/2 by first generating a N(0,1) random variable z and by then setting 
and 7^2- Do so by first generating a N(0,1) random variable z and by then setting 
To implement this sampling scheme, we must be able to generate normal, uni­
form, and chi-square random variables, and to numerically evaluate the c.d.f. and 
5e = T-l[T(£(t))x(l-Cr) + f/] , 
p.d.f. ^|*(5'e), we generate/7 as above, and set 
5e = T-l[T(£(i)) X f^] 
Generate 
conditionally on the sampled values of t 

109 
inverse c.d.f. of a chi-square distribution. Ail of these tasks are easily performed 
using readily available subroutine libraries. 
To combine the conditional integrals to obtain the unconditional expectation 
(4.7), we must be able to compute the probability P(Se > >C(t)) or equivalently the 
probability of event (4.8). Note that the right hand side of (4.8) may be written as 
' ^ 1 = 1  
(l+7Aj) 
^5=1 ^ 
or, equivalently, as 
^.•=1 
(4.12) 
where t *  =  
4- -yA^)^/^. Since, t *  is distributed as a chi-square random variable 
with 1 degree of freedom, the left side of (4.8) is distributed as a linear combination 
of central chi-square random variables. 
By taking advantage of expression (4.12), Imhof's (1961) method can be used to 
compute P{Se > 
Let ,\;'^(m) represent a chi-square random variable with i n  
degrees of freedom, let (mi,...,m^) represent positive integers, and let (aj,...,a/) 
represent arbitrary constants. Imhof showed that 
"oo sin (?!)({/.) 
where 
and 
( l ) { u )  = (1/2) ^ m; tan 
( a j u )  —  { l / 2 ) t u  ,  
d u  
(4.13) 
J = 1 
/'(«) = n 
. 
i = i  

110 
The integral in (4.13) can be evaluated numerically; see Lin (1987) for details. By 
applying Imhof's method with i = 0, 
= n-p*—r, 
= 1, and, for j  = 2,..., r + 1, 
mj — 1 and 
we can compute P [ S e  < Z^(^)] and hence P [ S e  >  C { t ) ]  = 1 — P [ S e  <  C { t ) ] .  
4.2.3 
Design of study 
The conditional and unconditional expectations (4.7), (4.9), and (4.10) depend 
on 7, A, 6, and X (recall that we are taking/3=0 and cr| = l). Thus, to design a Monte 
Carlo study we must specify the values of 7, A, 6, and X that cover the situations 
of interest. We chose to focus on two sets of values for A, 6, X, and Z. The first set 
consists of the values of A, 6, 
and Z values encountered in the lithium-carbonate 
example in making inferences about the C vs. D contrast [based on model (1.2)]. The 
second set consists of those values encountered in the crop-area example in making 
inferences about the mean amount of corn per segment in Cerro Gordo County [based 
on model (1.6)]. We used these two sets of values in combination with the following 
five values of 7: 0.0, 0.2, 0.5, 1.0, 2.0. Thus, a total of ten situations were considered. 
To insure that all prediction procedures were .F-equivariant, the Jeffreys (1961) prior 
for 0 = (iTg,7)\ which is relatively ^-invariant, was used in the Bayesian analysis. 
To insure maximum precision, the sample size was chosen to be as large as the 
available computing resources would permit. In evaluating the frequentist point and 
interval predictors, the Bayesian point predictor, and Interval 112, the sample size for 
each situation was taken to be 10000. In evaluating the Bayesian interval 111, which 

is more computationally intensive, the sample size was taken to be 5000. The only 
properties of the bootstrap intervals 18 - 110 that were studied were those conditional 
on 7= 0; a Monte Carlo of their unconditional properties, or their properties condi­
tional on 7 > 0, was not carried out due to the extremely large amount of computing 
resources that would have been required. 
4.3 
Monte Carlo Study - Results 
Unconditional properties of the frequentist and Bayesian predictors were evalu­
ated for each of the ten situations described in Section 4.2.3. For those situations 
where 7= 0.0, 0.2, 0.5, and 1.0, we evaluated the conditional properties (on 7 = 0, 
and 7 > 0) and combined them to obtain the unconditional results. For 7=2.0 
P(7 = 0) was nearly zero, and only the unconditional results were computed. A 
value of a = 0.05 was used in the construction of all prediction intervals. Our dis­
cussion of the results is divided into two main subsections, corresponding to the two 
sets of values for A, 6, X, and Z. 
The Fortran programs used to analyze the examples in Chapters 2 and 3 were 
adapted for use in the Monte Carlo study. Subroutines from the IMSL (1987) sub­
routine library were used for random variate generation and c.d.f. calculations. All 
simulations were run on the NAS AS/9160 computer at Iowa State University. 
4.3.1 
Lithium Carbonate example 
4.3.1.1 
MSEs and MSE estimators (Tables 4.1 - 4.3) 
Unconditionally, 
for small values of 7, the empirical BLUP ib has smaller MSE than the Bayesian 

112 
Table 4.1: 
Monte Carlo estimates of conditional (on 7 = 0) 
MSEs and expectations (and the estimated stan­
dard errors of the Monte Carlo estimates) — 
Lithium carbonate example 
0 
0 
11 
0 
II 
7 = 0.5 
7 = 1.0 
MSE of TO 
0.402 
0.489 
0.597 
0.802 
(0.006) 
(0.007) 
(0.009) 
(0.011) 
0.403 
0.460 
0.532 
0.638 
(0.002) 
(0.002) 
(0.002) 
(0.002) 
0.447 
0.509 
0.590 
0.706 
(0.002) 
(0.002) 
(0.002) 
(0.003) 
m s - l i ) ]  
0.490 
0.559 
0.647 
0.775 
(0.002) 
(0.002) 
(0.002) 
(0.003) 
MSE of wQ 
0.431 
0.498 
0.584 
0.720 
MSE of wQ 
(0.009) 
(0.007) 
(0.012) 
(0.014) 
E[v^] 
0.468 
0.541 
0.639 
0,773 
E[v^] 
(0.003) 
(0.002) 
(0.004) 
(0.005) 
0 
11 
0.492 
0.353 
0.226 
0.122 

113 
Table 4.2: Monte Carlo estimates of conditional (on 7 > 0) 
MSEs and expectations (and the estimated stan­
dard errors of the Monte Carlo estimates) — 
Lithium carbonate example 
11 0 
0 
II 0 
to 
7 = 0.5 
7 = 1.0 
MSE of w 
0.463 
0.530 
0.620 
0.779 
(0.007) 
(0.008) 
(0.009) 
(0.012) 
0.343 
0.414 
0.497 
0.603 
E[u®(6-^,7)] 
(0.002) 
(0.002) 
(0.002) 
(0.003) 
E[u®(6-^,7)] 
0.382 
0.462 
0.555 
0.677 
(0.002) 
(0.002) 
(0.002) 
(0.003) 
0.421 
0.509 
0.614 
0.750 
(0.002) 
(0.002) 
(0.003) 
(0.003) 
MSE of WQ 
0.496 
0.543 
0.616 
0.749 
MSE of WQ 
(0.011) 
(0.008) 
(0.013) 
(0.015) 
0.427 
0.517 
0.628 
0.768 
(0.003) 
(0.002) 
(0.004) 
(0.005) 
0 
II 
0.508 
0.647 
0.774 
0.878 

114 
predictor xvjg. However, for larger values of 7, x v q  has the smaller MSE, and the 
difference in MSE between wjg and xh increases with 7. A similar relationship holds 
in the conditional (on 7=0) case. 
Unconditionally, the performance of the estimators of •y"'~(cr|,7) (the MSE of tû) 
was as expected. The estimator v tends to be the least biased estimator of •t;'^(cr|,7), 
and v*(âg,j) and v®(àg,^) consistently underestimate •y'''(cr|,7). For trial values 
of 7 less than 0.5, v overestimates "^(0-^,7). This was as expected, since v does 
not account for the truncation (at 0) that occurs when estimating 7 with 7(when 
7=0, estimates are truncated about 50% of the time). We also find that 
tends to 
overestimate the frequentist MSE of w^, although for small values of 7 the bias is 
slight. 
4.3.1.2 
Prediction intervals (Tables 4.4 - 4.7) 
Unconditionally, Interval 
II had coverage probabilities almost exactly at the nominal level, as expected from 
theoretical considerations. Intervals 12 - 14 exhibited low coverage probabilities, as 
did Intervals 15 and 16; the probabilities of coverage for Interval 17 were relatively 
close to the 0.95 level. For all trial values of 7, the coverage probabilities of the 
Bayesian intervals 111 and 112 are slightly below the nominal level. In contrast to 
Intervals 111 and 112, whose coverage probabilities change relatively little with 7, the 
unconditional probabilities of coverage of Intervals 12 - 14 decrease considerably as 7 
increases, and the unconditional coverage probabilities of Intervals 15 - 17 decrease 
slightly. 
Intervals 12 - 112 are all substantially shorter on average than Interval II, with 
Intervals 12 - 14 being the shortest. The lengths of Intervals 12 - 112 all increase with 

115 
the value of 7. 
Conditional on 7=0, the bootstrap and Bayesian intervals, as well as Intervals 16 
and 17, maintained adequate coverage for all trial values of 7, and their lengths were 
roughly comparable. The naive intervals had comparable length, but their probabil­
ities of coverage decreased much more rapidly as 7 increased. 
Both conditionally and unconditionally, the coverage probabilities of frequentist 
intervals 15 - 17 are strongly dependent on the behavior of the associated estimates 
of t;"*"((Tg,7). When the expected values of v*(ô-|,7), 
and v are close 
to 
the coverage probabilities of the corresponding intervals are close to 
0.95. Conversely, underestimation of v ' ^ ( c r g , j )  leads to coverage probabilities less 
than 0.95, while overestimation tends to result in coverage that is greater than 0.95. 
For v'^=v, it appears that the Student's t-distribution with û degrees of freedom 
approximates the distribution of the pivotal quantity t{iu,y) rather well. A related 
conclusion, based on the consistently low unconditional coverage probabilities of In­
terval 14, is that (for v'^=v) the tails of the distribution of the pivotal quantity t{ iu, y) 
are heavier than the tails of the standard normal distribution. 
The results on probabilities of false coverage display patterns very similar to 
those of the probabilities of coverage (see Table 4.7). Most notably, the OLS interval 
II has extremely high probabilities of false coverage, even for e = 2. The Bayesian 
intervals 111 and 112 had probabilities of false coverage that were comparable to those 
of the naive intervals, however, as discussed above, the Bayesian intervals tended to 
maintain probabilities of coverage closer to the nominal level. Note that since we are 
taking o"| = l, the value of e can be interpreted in units of cr|. 

116 
Table 4.3: 
Monte Carlo estimates of unconditional MSEs and 
expectations (and the estimated standard errors of 
the Monte Carlo estimates) — Lithium carbonate 
example 
II o 
o 
7 = 0.2 7 = 0.5 
7 = 1.0 
H 
i l l  
o 
MSE of w 
0.433 
0.515 
0.615 
0.782 
0.980 
(0.006) 
(0.007) 
(0.009) 
(0.012) 
(0.014) 
Eb*(ô-|,7)] 
0.373 
0.430 
0.505 
0.607 
0.764 
E[i;®(<r|,7)] 
(0.002) 
(0.002) 
(0.002) 
(0.003) 
(0.003) 
E[i;®(<r|,7)] 
0.414 
0.479 
0.563 
0.681 
0.861 
(0.002) 
(0.002) 
(0.002) 
(0.003) 
(0.003) 
0.455 
0.527 
0.621 
0.753 
0.957 
(0.002) 
(0.002) 
(0.003) 
(0.003) 
(0.004) 
MSE of tujg 
0.464 
0.527 
0.609 
0.745 
0.916 
MSE of tujg 
(0.010) 
(0.008) 
(0.013) 
(0.015) 
(0.015) 
Ebsl 
0.468 
0.541 
0.639 
0.773 
0.994 
Ebsl 
(0.003) 
(0.002) 
(0.004) 
(0.005) 
(0.005) 

117 
Table 4,4: 
Monte Carlo estimates of conditional (on 
7 = 0) probabilities of coverage (and con­
ditional expected lengths) — Lithium car­
bonate example^ 
o 
C
Z
3 
II 
7 = 0.2 
7 = 0.5 
7 = 1.0 
11 
0.966 
0.961 
0.964 
0.967 
(8.5) 
(8.8) 
(9.1) 
(9.4) 
12 
0.932 
0.926 
0.916 
0.896 
(2.5) 
(2.7) 
(2.9) 
(3.1) 
13 
0.946 
0.939 
0.927 
0.915 
(&6) 
(2.8) 
(34) 
(3.3) 
14 
0.956 
0.950 
0.940 
0.930 
(2.8) 
(2.9) 
(3.2) 
(3.4) 
15 
0.953 
0.947 
0.936 
0.925 
(2.6) 
(2.8) 
(3.1) 
(13) 
16 
0.962 
0.957 
0.949 
0.939 
(2.8) 
(3.0) 
(3.2) 
(15) 
IT 
0.968 
0.965 
0.960 
0.949 
(2.9) 
(3.1) 
(3.4) 
(3.7) 
18 
0.964 
0.960 
0.955 
0.943 
(2.9) 
(3.1) 
(3j) 
(3.6) 
19 
0.964 
0.960 
0.953 
0.942 
(2.9) 
(3.0) 
(3.3) 
(3.6) 
no 
0.963 
0.960 
0.953 
0.942 
(2.9) 
(3.0) 
(3.3) 
(3.6) 
111 
0.956 
0.952 
0.951 
0.942 
(2.8) 
(3.0) 
(3.2) 
(3.5) 
112 
0.955 
0.951 
0.945 
0.941 
(2.8) 
(3.0) 
(3.2) 
(3.4) 
^ Estimated standard errors for coverage 
probabilities are all < .004, and estimated 
standard errors for expected lengths 
are all < .01. 

118 
Table 4,5: 
Monte Carlo estimates of conditional (on 
7 > 0) probabilities of coverage (and con­
ditional expected lengths) — Lithium car­
bonate example® 
II 
O 
O 
7 = 0.2 
7 = 0.5 
7 = 1.0 
11 
0.9.34 
0.940 
0.946 
0.947 
(5.7) 
(6.1) 
(6.5) 
(6.7) 
12 
0.880 
0.890 
0.889 
0.881 
(2.3) 
(2.5) 
(2.7) 
(3.0) 
13 
0.896 
0.906 
0.905 
0.897 
(2.4) 
(2.6) 
(2.9) 
(3.2) 
14 
0.909 
0.920 
0.917 
0.912 
(2.5) 
(2.8) 
(3.1) 
(3.4) 
15 
0.908 
0.920 
0.918 
0.916 
(2.5) 
(2.7) 
(3.0) 
(3.3) 
16 
0.922 
0.933 
0.932 
0.932 
(2.6) 
(2.9) 
(3.2) 
(3.5) 
17 
0.936 
0.944 
0.944 
0.944 
(2.7) 
(3.0) 
(3.3) 
(3.7) 
111 
0.914 
0.929 
0.927 
0.929 
(2.5) 
(2.8) 
(3.1) 
(3.4) 
112 
0.911 
0.923 
0.921 
0.925 
(2.5) 
(2.8) 
(3.0) 
(3.4) 
® Estimated standard errors for coverage 
probabilities are all < .004, and estimated 
standard errors for expected lengths 
are all < .01. 

119 
Table 4.6: 
Monte Carlo estimates of unconditional probabilities 
of coverage (and unconditional expected lengths) — 
Lithium carbonate example^ 
7 = 0.0 
o 
11 
7 = 0.5 
O 
1
-H 
II 
7 = 2.0 
11 
0.950 
0.947 
0.950 
0.949 
0.949 
(7.1) 
(7.1) 
(7.1) 
(7.1) 
(7.1) 
12 
0.906 
0.903 
0.895 
0.883 
0.890 
(2.4) 
(2.6) 
(2.7) 
(3.0) 
(3.4) 
13 
0.921 
0.918 
0.910 
0.899 
0.895 
(2.5) 
(2.7) 
(2.9) 
(3.2) 
(3.6) 
14 
0.932 
0.931 
0.922 
0.914 
0.908 
(&6) 
(2.8) 
(3.1) 
(3.4) 
(3.8) 
15 
0.930 
0.930 
0.922 
0.917 
0.918 
(&6) 
(2.8) 
(3.0) 
(3.3) 
(&8) 
16 
0.942 
0.941 
0.936 
0.933 
0.932 
(2.T) 
(3.0) 
(3.2) 
(3.5) 
(4.0) 
17 
0.952 
0.951 
0.948 
0.945 
0.943 
(2.8) 
(3.0) 
(3.3) 
(3.7) 
(4.2) 
111 
0.935 
0.937 
0.932 
0.931 
0.936 
(2.7) 
(2.9) 
(&1) 
(3.4) 
(3.9) 
112 
0.933 
0.933 
0.926 
0.927 
0.931 
(2.6) 
(2.9) 
(3.1) 
(3.4) 
(3.8) 
® Estimated standard errors for coverage probabilities 
are all < .004, and estimated standard errors for expected 
lengths are all < .01. 

120 
Table 4.7: 
Monte Carlo estimates of unconditional proba­
bilities of false coverage P{{;w + e)sl) — Lithium 
carbonate example 
e 
o 
o 
II 
7 = 0.2 
7 = 0.5 
7 = 1.0 
7 = 2.0 
0.5 
0.939 
0.941 
0.941 
0.941 
0.938 
11 
1.0 
0.911 
0.911 
0.919 
0.911 
0.911 
2.0 
0.791 
0.789 
0.794 
0.787 
0.792 
0.5 
0.824 
0.829 
0.836 
0.836 
0.838 
12 
1.0 
0.600 
0.633 
0.671 
0.696 
0.727 
2.0 
0.118 
0.168 
0.223 
0.295 
0.371 
0.5 
0.845 
0.850 
0.859 
0.856 
0.860 
13 
1.0 
0.632 
0.663 
0.700 
0.729 
0.754 
2.0 
0.138 
0.193 
0.252 
0.328 
0.411 
0.5 
0.865 
0.870 
0.875 
0.873 
0.877 
14 
1.0 
0.665 
0.694 
0.730 
0.754 
0.780 
2.0 
0.160 
0.218 
0.282 
0.361 
0.449 
0.5 
0.861 
0.867 
0.874 
0.877 
0.885 
15 
1.0 
0.658 
0.687 
0.727 
0.755 
0.787 
2.0 
0.155 
0.213 
0.276 
0.358 
0.452 
0.5 
0.880 
0.887 
0.890 
0.895 
0.905 
16 
1.0 
0.693 
0.722 
0.756 
0.786 
0.817 
2.0 
0.180 
0.253 
0.313 
0.398 
0.495 
0.5 
0.898 
0.902 
0.907 
0.912 
0.919 
17 
1.0 
0.723 
0.752 
0.784 
0.814 
0.839 
2.0 
0.206 
0.275 
0.348 
0.441 
0.538 
0.5 
0.868 
0.875 
0.887 
0.889 
0.902 
111 
1.0 
0.673 
0.707 
0.748 
0.772 
0.804 
2.0 
0.178 
0.227 
0.297 
0.375 
0.456 
0.5 
0.862 
0.870 
0.881 
0.885 
0.895 
112 
1.0 
0.664 
0.700 
0.741 
0.765 
0.797 
2.0 
0.177 
0.222 
0.291 
0.368 
0.450 

121 
4.3.2 
Crop-area example 
The simulation results for this example differ in several respects from those for 
the lithium-carbonate example. This difference is primarily attributable to the fact 
that in the crop-area example, the linear combination lu includes random effects, 
whereas in the lithium-carbonate example, it does not. 
4.3.2.1 
MSEs and MSE estimators (Tables 4.8 - 4.10) 
For the larger 
values of 7, the unconditional MSE of lojg is smaller than that of lu, although the 
difference is not as large as in the lithium carbonate example. For 7= 0.2, 0.5, and 
1.0, the conditional (on 7=0) MSEs of xu were significantly larger than those of iuq. 
Conditional on 7=0, the overall performance of iu£ was superior to that of lu. 
For very small values of 7, each of the three estimators t'*(0'|,7), f'®(ô"|,7), and 
V tended to severely overestimate the unconditional MSE v"'"(cr|,7). However, for the 
l a r g e r  v a l u e s  o f  7 ,  v*(ô-g,j) a n d  v ® ( â g , j )  t e n d  t o  u n d e r e s t i m a t e  
7 ) ,  w h i l e  v  
is nearly unbiased. Note also that, for the larger values of 7, vjg is a nearly unbiased 
estimater of the unconditional MSE of xuq. 
4.3.2.2 
Prediction intervals (Tables 4.11 - 4.14) 
The unconditional 
probabilities of coverage suggest that the Bayesian interval 111 compares very fa­
vorably to the frequentist intervals. The unconditional probabilities of coverage of 
the Bayesian interval 111 tend to be closer to the nominal level than those of the 
frequentist interval 17, and the expected length of 111 is less than that of 17. The 
expected length of Interval 14 compares favorably with that of the Bayesian intervals, 
however, for some values of 7, its unconditional probability of coverage is significantly 

122 
Table 4.8: 
Monte Carlo estimates of conditional (on 7= 0 )  
M S Es and expectations (and the estimated stan­
dard errors of the Monte Carlo estimates) — Crop 
area example 
II 0 
0 
0 
II 
7 = 0.5 
7 = 1.0 
MSE of th 
0.032 
0.227 
0.518 
0.959 
(0.000) 
(0.003) 
(0.007) 
(0.014) 
0.032 
0.036 
0.038 
0.042 
E[U®(ct|,7)] 
(0.000) 
(0.000) 
(0.000) 
(0.000) 
E[U®(ct|,7)] 
0.055 
0.061 
0.065 
0.071 
E[i^(ô-|,7)] 
(0.000) 
(0.000) 
(0.000) 
(0.000) 
E[i^(ô-|,7)] 
0.078 
0.085 
0.092 
0.100 
(0.000) 
(0.000) 
(0.000) 
(0.000) 
MSE of IV£ 
0.053 
0.199 
0.422 
0.680 
MSE of IV£ 
(0.001) 
(0.003) 
(0.006) 
(0.010) 
0.180 
0.209 
0.238 
0.291 
(0.001) 
(0.000) 
(0.001) 
(0.002) 
0 
II 
0.548 
0.233 
0.081 
0.022 

123 
Table 4.9: 
Monte Carlo estimates of conditional (on 7 > 0) 
MSEs and expectations (and the estimated stan­
dard errors of the Monte Carlo estimates) — Crop 
area example 
II 0 
0 
7 = 0.2 
II 0 
Cn 
7 = 1.0 
MSE of ih 
0.071 
0.238 
0.436 
0.634 
(0.002) 
(0.004) 
(0.007) 
(0.009) 
0.141 
0.232 
0.354 
0.514 
(0.001) 
(0.001) 
(0.002) 
(0.002) 
0.173 
0.275 
0.407 
0.578 
• 
(0.009) 
(0.001) 
(0.002) 
(0.002) 
E|ti(^|,7)) 
0.205 
0.318 
0.461 
0.642 
(0.001) 
(0.001) 
(0.002) 
(0.002) 
MSE of IVQ 
0.104 
0.245 
0.423 
0.614 
MSE of IVQ 
(0.002) 
(0.004) 
(0.006) 
(0.009) 
E[v_g] 
0.259 
0.344 
0.458 
0.615 
E[v_g] 
(0.001) 
(0.001) 
(0.002) 
(0.002) 
P(7 = 0) 
0.452 
0.767 
0.919 
0.978 
Table 4.10; 
Monte Carlo estimates of unconditional MSEs 
and expectations (and the estimated standard er­
rors of the Monte Carlo estimates) — Crop area 
example 
0 
0 
II 
7 = 0.2 
II 0 
7 = 1.0 
II 
to 
0 
MSE of lu 
0.050 
0.235 
0.443 
0.641 
0.839 
(0.001) 
(0.003) 
(0.007) 
(0.009) 
(0.012) 
Eb*(j|,7)l 
0.081 
0.186 
0.328 
0.504 
0.706 
(0.000) 
(0.001) 
(0.002) 
(0.002) 
(0.002) 
0.108 
0.225 
0.379 
0.567 
0.773 
(0.000) 
(0.001) 
(0.002) 
(0.002) 
(0.002) 
E1«(#|,7)I 
0.135 
0.264 
0.431 
0.630 
0.840 
(0.000) 
(0.001) 
(0.002) 
(0.002) 
(0.002) 
MSE of luQ 
0.076 
0.234 
0.423 
0.615 
0.823 
MSE of luQ 
(0.001) 
(0.004) 
(0.006) 
(0.009) 
(0.012) 
E[y_g] 
0.216 
0.313 
0.440 
0.608 
0.815 
E[y_g] 
(0.000) 
(0.001) 
(0.002) 
(0.002) 
(0.003) 

124 
less than the nominal level. Furthermore, the Bayesian intervals had remarkably good 
probabilities of false coverage, especially when compared to Intervals 15 - 17. 
Conditional on 7=0, the probability of coverage of Intervals 15 - 17 tended to 
be much higher than the nominal level — when 7=0, the estimated degrees of free­
dom ùi, t>2, and 1/3 tended to be very small and consequently the percentage points 
^0/2^^1^' 
a,nd 
tended to be very large. In particular, the con­
ditional probability of coverage of Interval 17 was 1.000 for 7=0.0, 0.2, 0.5, and 1.0. 
The conditional probability of coverage of Interval 14 declined sharply with the value 
of 7 and was only 0.488 for 7=1.0. The performance of the bootstrap intervals 18 -
110 was very poor (even for 7=0), presumably due to their short length. 
Overall, the conditional (on 7=0) behavior of the Bayesian intervals 111 and 112 
was more sensible than that of Intervals 12 - 110. For 7= 0.0, 0.2, 0.5, and 1.0, 
the conditional probabilities of coverage of Interval 111 were LOGO, 0.952, 0.852, and 
0.815, respectively, and its conditional expected lengths were 1.7, 1.8, 2.0, and 2.1. 
By way of comparison, the conditional expected lengths of Interval 17 were 7.0, 7.4, 
7.6 and 8.0, respectively. 

125 
Table 4.11: 
Monte Carlo estimates of conditional (on 
7 = 0) probabilities of coverage (and con­
ditional expected lengths) — Crop area 
example^ 
II 
o 
o 
7 = 0.2 
7 = 0.5 
7 = 1.0 
11 
0.959 
0.970 
0.969 
0.978 
(4.8) 
(5.0) 
(5.1) 
(5j) 
12 
0.940 
0.557 
0.391 
0.345 
(0.7) 
(0.7) 
(0.8) 
(0.8) 
13 
0.986 
0.684 
0.485 
0.420 
(&9) 
(1.0) 
(1.0) 
(1.0) 
14 
0.996 
0.762 
0.563 
0.488 
(1.1) 
(1.1) 
(L2) 
(1.2) 
15 
1.000 
1.000 
1.000 
0.989 
(4.5) 
(4.8) 
(4.9) 
(5.1) 
16 
1.000 
1.000 
1.000 
1.000 
(5.9) 
(6.2) 
(6.4) 
(6J) 
17 
1.000 
1.000 
1.000 
1.000 
(7.0) 
(7.4) 
(7.6) 
(8.0) 
18 
0.918 
0.523 
0.360 
0.326 
(0.6) 
(0.7) 
(0.7) 
(0.7) 
19 
0.926 
0.537 
0.372 
0.332 
(0.7) 
(0.7) 
(0.7) 
(0.7) 
no 
0.931 
0.542 
0.377 
0.337 
(0.7) 
(0.7) 
(0.7) 
(0.8) 
ni 
1.000 
0.952 
0.852 
0.815 
(1.7) 
(1.8) 
(2.0) 
(2.1) 
n2 
1.000 
0.945 
0.829 
0.805 
(1.6) 
(1.8) 
(1.9) 
(2JJ 
® Estimated standard errors for coverage 
probabilities are all < .004, and estimated 
standard errors for expected lengths 
are all < .01. 

126 
Table 4.12: 
Monte Carlo estimates of conditional (on 
7 > 0) probabilities of coverage (and con­
ditional expected lengths) — Crop area 
example® 
II 
o 
o 
o 
1
1 
7 = 0.5 
7 = 1.0 
11 
0.939 
0.945 
0.947 
0.952 
(4.3) 
(4.5) 
(4.5) 
(4^0 
12 
0.990 
0.913 
0.887 
0.898 
(1.4) 
(1.9) 
(&3) 
(2.8) 
13 
0.995 
0.938 
0.913 
0.917 
(1.6) 
(2.0) 
(15) 
(3.0) 
14 
0.997 
0.956 
0.932 
0.931 
(1.8) 
(2.2) 
(2.7) 
(3.2) 
15 
0.998 
0.992 
0.972 
0.950 
(4.7) 
(4.0) 
(3.5) 
(3.3) 
16 
0.998 
0.994 
0.978 
0.959 
(4.8) 
(4.1) 
(3 5) 
(3.5) 
17 
0.998 
0.995 
0.982 
0.966 
(4.8) 
(4^0 
(3.6) 
(3.6) 
111 
0.998 
0.975 
0.951 
0.941 
(2.0) 
(2.3) 
(2.6) 
(3.1) 
112 
0.999 
0.975 
0.947 
0.936 
(2.0) 
(2.3) 
(2.6) 
(3.0) 
® Estimated standard errors for coverage 
probabilities are all < .004, and estimated 
standard errors for expected lengths 
are all < .01. 

127 
Table 4.13: 
Monte Carlo estimates of unconditional probabilities 
of coverage (and unconditional expected lengths) -
Crop area example^ 
II 
o 
o 
7 = 0.2 
7 = 0.5 
7 = 1.0 
7 = 2.0 
11 
0.950 
0.951 
0.949 
0.953 
0.952 
(4.6) 
(4j^ 
(4^0 
(4j) 
(4.6) 
12 
0.963 
0.830 
0.847 
0.886 
0.912 
(1.0) 
(1.6) 
(2.2) 
(2.8) 
(3.3) 
13 
0.990 
0.867 
0.878 
0.906 
0.925 
(L2) 
(1.8) 
(2.4) 
(3.0) 
(3.5) 
14 
0.996 
0.911 
0.901 
0.921 
0.938 
(1.4) 
(2.0) 
(2.5) 
(3.1) 
(3.6) 
15 
0.999 
0.994 
0.974 
0.951 
0.938 
(4.6) 
(4.2) 
(3.6) 
(3.4) 
(3.5) 
16 
0.999 
0.995 
0.980 
0.960 
0.948 
(&4) 
(4.6) 
(3.8) 
(3.5) 
(.3.7) 
17 
0.999 
0.996 
0.983 
0.967 
0.957 
(6.0) 
(4.8) 
(3.9) 
(3.7) 
(3.8) 
111 
0.999 
0.970 
0.943 
0.9.38 
0.944 
(L8) 
(2.2) 
(2.6) 
(3.0) 
(3.5) 
112 
0.999 
0.968 
0.937 
0.933 
0.941 
(1.8) 
(2.2) 
(2.6) 
(3.0) 
(3.5) 
® Estimated standard errors for coverage probabilities 
are all < .004, and estimated standard errors for expected 
lengths are all < .01. 

128 
Table 4.14; 
Monte Carlo estimates of unconditional prob­
a b i l i t i e s  o f  f a l s e  c o v e r a g e  P{{iu +  e ) £ l )  —  
Lithium carbonate example 
e 
o 
o 
II 
II 
o 
to 
7 = 0.5 
7 = 1.0 
o 
II 
0.5 
0.927 
0.929 
0.926 
0.930 
0.9.32 
11 
1.0 
0.863 
0.863 
0.861 
0.863 
0.867 
2.0 
0.593 
0.604 
0.598 
0.665 
0.605 
0.5 
0.437 
0.674 
0.761 
0.827 
0.874 
12 
1.0 
0.076 
0.347 
0.533 
0.665 
0.747 
2.0 
0.002 
0.023 
0.106 
0.228 
0.350 
0.5 
0.586 
0.733 
0.800 
0.856 
0.892 
13 
1.0 
0.103 
0.405 
0.583 
0.702 
0.773 
2.0 
0.002 
0.031 
0.129 
0.259 
0.382 
0.5 
0.719 
0.780 
0.831 
0.875 
0.905 
14 
1.0 
0.140 
0.461 
0.626 
0.733 
0.796 
2.0 
0.003 
0.040 
0.152 
0.293 
0.414 
0.5 
0.995 
0.973 
0.933 
0.908 
0.904 
15 
1.0 
0.961 
0.875 
0.797 
0.773 
0.787 
2.0 
0.684 
0.474 
0.344 
0.339 
0.398 
0.5 
0.996 
0.977 
0.945 
0.919 
0.916 
16 
1.0 
0.966 
0.887 
0.820 
0.799 
0.809 
2.0 
0.778 
0.514 
0.369 
0.369 
0.429 
0.5 
0.997 
0.980 
0.954 
0.933 
0.927 
17 
1.0 
0.971 
0.900 
0.840 
0.822 
0.830 
2.0 
0.764 
0.516 
0.390 
0.401 
0.457 
0.5 
0.909 
0.870 
0.874 
0.887 
0.909 
111 
1.0 
0.341 
0.559 
0.661 
0.739 
0.792 
2.0 
0.011 
0.052 
0.157 
0.279 
0.402 
0.5 
0.920 
0.865 
0.782 
0.881 
0.905 
112 
1.0 
0.305 
0.546 
0.654 
0.731 
0.792 
2.0 
0.007 
0.047 
0.156 
0.275 
0.402 

129 
5. 
BIBLIOGRAPHY 
Battese, G. E., and Fuller, W. A. (1981). "Prediction of County Crop Areas Using 
Survey and Satellite Data." In 1981 Proceedings of the Section on Survey 
Research Methods, p. 500 - 505. Washington, D.C.: American Statistical 
Association. 
Battese, G. E., Harter, R. M., and Fuller, W. A. (1988). "An Error-Components 
Model for Prediction of County Crop Areas Using Survey and Satellite Data." 
Journal of the American Statistical Association 83:28-36. 
Berger, J. 0. (1985). Statistical Decision Theory and Bayesian Analysis (2nd 
edition). New York: Springer-Verlag. 
Box, G. E. P., and Tiao, G. C. (1968). "Bayesian Estimation of Means for the 
Random Effect Model." Journal of the American Statistical Association 
63:174-181. 
Box, G. E. P., and Tiao, G. C. (1973). Bayesian Inference in Statistical Analysis. 
Reading, MA: Addison-Wesley. 
Broemeling, L. D. (1985). Bayesian Analysis of Linear Models. New York: Marcel 
Dekker. 
Carriquiry, A. L. (1989). "Bayesian Prediction and its Application to the Genetic 
Evaluation of Livestock." Unpublished Ph.D. Dissertation, Iowa State 
University, Ames, Iowa. 
Casella, G., and Strang, L. (1987). "Empirical Bayes Crop Prediction." Paper 
BU-953-M. Biometrics Unit, Cornell University, Ithaca, N.Y. 

130 
Cox, D. R. (1975). "Prediction Intervals and Empirical Bayes Confidence Intervals." 
In Perspectives in Probability and Statistics, ed. J. Gani, p. 47-55. London: 
Academic Press. 
Datta, G. S., and Ghosh, M. (1989). "Bayesian Prediction in Mixed Linear Models: 
Applications to Small Area Estimation." Technical Report. Department of 
Statistics, University of Florida, Gainesville, Florida. 
Dempster, A. P., and Raghunathan, T. E. (1987). "Using a Covariate for Small 
Area Estimation: A Common Sense Approach." In Small Area Statistics: An 
International Symposium, eds. R. Platek, J. N. K, Rao, C. E. Sarndal, and M. 
P. Singh, p. 77-90. New York: John Wiley. 
Dempster, A. P., Rubin, D. B., and Tsutakawa, R. K. (1981). "Estimation in 
Covariance Components Models." Journal of the American Statistical 
Association 76:341-353. 
Dempster, A. P., Selwyn, M. R., Patel, C. M., and Roth, A. J. (1984). "Statistical 
and Computational Aspects of Mixed Model Analysis." Applied Statistics 
33:203-214. 
Devroye, L. (1986). Non-Uniform Random Variate Generation. New York: 
Springer-Verlag. 
Efron, B. (1982), The Jacknife, the Bootstrap, and Other Resampling Plans. 
Philadelphia, PA: Society for Industrial and Applied Mathematics. 
Ericksen, E. P., and Kadane. J. B. (1985). "Estimating the Population in a Census 
Year." Journal of the American Statistical Association 80:98-131. 
Fay, R.E., and Herriot, R. A. (1979). "Estimates of Income for Small Places: An 
Application of James-Stein Procedures to Census Data." Journal of the 
American Statistical Association 74:269-277. 
Ferguson, T. S. (1967). Mathematical Statistics: A Decision Theoretic Approach. 
New York: Academic Press. 
Fuller, W. A. (1988). "Prediction with the Multivariate Components of Variance 
Model." Unpublished Manuscript. Department of Statistics, Iowa State 
University. 

131 
Fuller, W. A., and Harter, R. M. (1987). "The Multivariate Components of 
Variance Model for Small Area Estimation." In Small Area Statistics: An 
International Symposium^ eds, R. Platek, J. N. K. Rao, C. E. Sarndal, and M. 
P. Singh, p. 103-123. New York: John Wiley. 
Ghosh, M., and Lahiri, P. (1988). "A Hierarchical Bayes Approach to Small Area 
Estimation with Auxiliary Information." Technical Report 315. Department of 
Statistics, University of Florida, Gainesville, Florida. 
Ghosh, M., and Meeden, G. (1986). "Empirical Bayes Estimation in Finite 
Population Sampling." Journal of the American Statistical Association 
81:1058-1062. 
Giesbrecht, F. G. (1986). "Analysis of Data from Incomplete Block Designs." 
Biometrics 42:437-448. 
Hahn, G. J., and Raghunathan, T. E. (1988). "Combining Information From 
Various Sources: A Prediction Problem and Other Industrial Applications." 
Technometrics 30:41-52. 
Hammersley, J. M., and H an s comb, D. C. (1964). Monte Carlo Methods. London: 
Methuen. 
Harville, D. A. (1974). "Bayesian Inference for Variance Components Using Only 
Error Contrasts." Biometrika 61:383-385. 
Harville, D. A. (1976). "Extension of the Gauss-Markov Theorem to the Estimation 
of Random Effects." Annals of Statistics 4:384-395. 
Harville, D. A. (1977). "Maximum Likelihood Approaches to Variance Component 
Estimation and to Related Problems." Journal of the American Statistical 
Association 72:320-338. 
Harville, D. A. (1985). "Decomposition of Prediction Error." Journal of the 
American Statistical Association 80:132-138. 
Harville, D. A. (1988). "Mixed Model Methodology: Theoretical Justifications and 
Future Directions." To appear in 1988 Proceedings of the Statistical Computing 
Section. Washington, D.C.: American Statistical Association. 

132 
Harville, D. A. (1989). "BLUP (Best Linear Unbiased Prediction) and Beyond." To 
appear in Advances in Statistical Methods for Genetic Improvement of 
Livestock, eds. D. Gianola and K. Hammond. New York: Springer-Verlag. 
Harville, D. A., and Callanan, T. P. (1989). "Computational Aspects of 
Likelihood-Based Inference for Variance Components." To appear in Advances 
in Statistical Methods for Genetic Improvement of Livestock, eds. D. Gianola 
and K. Hammond. New York: Springer-Verlag. 
Harville, D. A., and Fenech, A. P. (1985). "Confidence Intervals for a Variance 
Ratio, or for Heritability, in an Unbalanced Mixed Linear Model." Biometrics 
41:137-152. 
Henderson, C. R. (1953). "Estimation of Variance and Covariance Components." 
Biometrics 9:226-252. 
Henderson, C. R. (1975). "Best Linear Unbiased Estimation and Prediction under a 
Selection Model." Biometrics 31:423-447. 
Hill, B. (1967). "Correlated Errors in the Random Model." Journal of the American 
Statistical Association 62:1387-1400. 
Imhof, J. P. (1961). "Computing the Distribution of Quadratic Forms in Normal 
Variables." Biometrika 48:419-426. 
IMSL (1987). STAT/Lihrary User's Manual. IMSL, Inc.: Houston. 
Jeffreys, H. (1961). Theory of Probability (3rd Edition). Oxford University Press: 
London. 
Jeske, D. R. (1985). "Prediction Intervals for the Realization of a Random Variable 
Under a General Mixed Linear Model." Unpublished Ph.D. Dissertation, Iowa 
State University, Ames, Iowa. 
Jeske, D. R., and Harville, D. A. (1988). "Prediction-Interval Procedures and 
(Fixed-Effects) Confidence-Interval Procedures for Mixed Linear Models." 
Communications in Statistics - Theory and Methods 17:1053-1087. 
Kackar, R. N., and Harville, D. A. (1984). "Approximations for Standard Errors of 
Estimators of Fixed and Random Effects in Mixed Linear Models." Journal of 
the American Statistical Association 79:853-862. 

133 
Kalos, M. H., and Whitlock, P. A. (1986). Monte Carlo Methods - Volume 1: 
Basics, New York: Wiley. 
Kass, R. E., and Steffey, D. (1989). "Approximate Bayesian Inference in 
Conditionally Independent Hierarchical Models (Parametric Empirical Bayes 
Models)." Journal of the American Statistical Association. To appear. 
Kass, R. E., Tierney, L., and Kadane, J. B. (1988). "Asymptotics in Bayesian 
Computation." In Bayesian Statistics, 3, eds. J. M. Bernardo, M. H. DeGroot, 
D. V. Lindley, and A. F. M. Smith. New York: North Holland. 
Kass, R. E., Tierney, L., and Kadane, J. B. (1989). "The Validity of Posterior 
Expansions Based on Laplace's Method." To appear in Essays in Honor of 
George Barnard^ eds. S. Geisser, J. S. Hodges, S. J. Press, and A. Zellner. New 
York: North Holland. 
Kempthorne, 0. K. (1952). Design and Analysis of Experiments, New York: John 
Wiley. 
Kennedy, W. J., and Gentle, J. E. (1980). Statistical Computing. New York: Marcel 
Dekker. 
Khatri, C. G., and Shah, K. R. (1981). "On the Unbiased Estimation of Fixed 
Effects in a Mixed Model for Growth Curves." Communications in Statistics -
Theory and Methods 10:401-406. 
Laird, N., and Louis, T. A. (1987). "Empirical Bayes Confidence Intervals Based on 
Bootstrap Samples." Journal of the American Statistical Association 
82:739-750. 
Laird, N., and Ware, J. H. (1982). "Random-Effects Models for Longitudinal Data." 
Biometrics 38:963-974. 
Lehman, E. L. (1983). Theory of Point Estimation, New York: Wiley. 
Lin, T.-H. (1987), "Confidence Intervals for the Ratio of Variance Components in a 
Mixed Linear Model with Two Variance Components." Unpublished Ph.D. 
Dissertation, Iowa State University, Ames, Iowa. 

134 
Macedo, F. W., and Gianola, D. (1988). "Bayesian Analysis of Univariate Mixed 
Models with Informative Priors." Proceedings of the XXXVIII Annual Meeting 
of the European Association of Animal Production Lisbon, Portugal. 
Mandallaz, D., and M au, J. (1981). "Comparison of Different Methods for 
Decision-Making in Bioequivalence Assessment." Biometrics 37:213-222. 
McLean, R. A., and Sanders, W. L. (1988). "Approximating Degrees of Freedom for 
Standard Errors in Mixed Linear Models." To appear in 1988 Proceedings of 
the Statistical Computing Section. Washington, D.C.: American Statistical 
Association. 
Metzler, C. M. (1974). "Bioavailability - A Problem in Equivalence." Biometrics 
30:309-317. 
Morris, C. N. (1983a). "Parametric Empirical Bayes Inference: Theory and 
Applications." Journal of the American Statistical Association 78:47-55. 
Morris, C. N. (1983b). "Parametric Empirical Bayes Confidence Intervals." In 
Scientific Inference, Data Analysis, and Robustness, eds. G. E. P. Box, T. 
Leonard, and C. F. Wu, p. 25-50. New York: Academic Press. 
Patterson, H, D., and Thompson, R. (1971). "Recovery of Interblock Information 
when Block Sizes are Unequal." Biometrika 58:545-554. 
Portnoy, S. (1971). "Formal Bayes Estimation with Application to a Random 
Effects Model." Annals of Mathematical Statistics 42:1379-1402. 
Prasad, N. G. N., and Rao, J. N. K. (1986). "On the Estimation of Mean Square 
Error of Small Area Predictors." In 1986 Proceedings of the Section on Survey 
Research Methods, p. 108-116. Washington, D.C.: American Statistical 
Association. 
Press, W. H., Flannery, B. P., Teukolsky, S. A., and Vetterling, W. T. (1986). 
Numerical Recipes: The Art of Scientific Computing. Cambridge, MA: 
Cambridge University Press. 
Purcell, N. J., and Kish, L. (1979). "Estimation for Small Domains." Biometrics 
35:365-384. 

135 
RaifFa, H., and Schlaifer, R. (1961). Applied Statistical Decision Theory. Boston: 
Harvard University Press. 
Rocke, D. M. (1984). "On Testing for Bioequivalence." Biometrics 40:225-230. 
Rubin, D. B. (1982). "Estimation in Parallel Randomized Experiments." Journal of 
Educational Statistics 6:377-401. 
Rubinstein, R. Y. (1981). Simulation and the Monte Carlo Method, New York: 
Wiley. 
S alias, W. M., and Harville, D. A. (1981). "Best Linear Recursive Estimation for 
Mixed Linear Models." Journal of the American Statistical Association 
76:860-869. 
Sattertliwaite, F. E. (1946). "An Approximate Distribution of Estimates of Variance 
Components." Biometrics Bulletin 2:110-114. 
Searle, S. R. (1971). Linear Models. New York: John Wiley. 
Selwyn, M. R., and Hall, N. R. (1984). "On Bayesian Methods for Bioequivalence." 
Biometrics 40:1103-1108. 
Selwyn, M. R., Dempster, A. P., and Hall, N. R. (1981). "A Bayesian Approach to 
Bioequivalence for the 2x2 Changeover Design." Biometrics 37:11-21. 
Stroud, T. W. F. (1984). "Bayesian Shrinkage Estimates for Regression Coefficients 
in m Populations." Communications in Statistics: Theory and Methods 
13:2085-2109. 
Stroud, T. W. F. (1987). "Bayes and Empirical Bayes Approaches to Small Area 
Estimation." In Small Area Statistics: An International Symposium., eds. R. 
Platek, J. N. K. Rao, C. E. Sarndal, and M. P. Singh, p. 124-137. New York: 
John Wiley, 
Tiao, G. C., and Draper, N. R. (1968). "Bayesian Analysis of Linear Models with 
Two Random Components with Special Reference to the BIBD." Biometrika 
55:101-117. 

136 
Tierney, L., and Kadane, J. B. (1986). "Accurate Approximations for Posterior 
Moments and Marginal Densities." Journal of the American Statistical 
Association 81:82-86. 
Westlake, W. J. (1972). "The Use of Confidence Intervals in Analysis of 
Comparative Bioavailability Trials." Journal of Pharmaceutical Sciences 
61:1340-1341. 
Westlake, W. J. (1974). "The Use of Balanced Incomplete Block Designs in 
Comparative Bioavailability Trials." Biometrics 30, 319-327. 

137 
6. 
APPENDIX. PROOFS OF CHAPTER 4 RESULTS 
6.1 
Proof of Result 4.3 
Note that the .F-invariant function Q[-) of z® (or z * )  can be written as a 
function, say V^(-), of y and lu (or y* and w*). Further, 
V { i o * , y * )  =  Q ( z * )  =  Q { z ® )  =  V { i u , y )  .  
Thus, 
P { g ( ^ * ^ y * )  <  h  , Q { z * )  <  (2} = 
< h  , 
< fg} 
= f {(/(k - A'/3]/(je, [y - A'/3]/a-e) < 
, 
V'([iy -  A'/3|/cre, [y -  A'/3]/<Te) <  ( g )  
=  •P{g(l/«^e)5(i'-',2/) <  
,  V { w , y )  <  <2} 
= •P{9(l/o-e)^(w,y) < ti ,Q(%®) < fo}' 
6.2 
Proof of Result 4.7 
For a member of the prior class (3.2) to be relatively ^-invariant, we require that 
G'i('y)(cr§)^2('y)exp{(-2cr2)-%'g('y)} oc 6'I(7)(KO-|)^^2(7) exp{(-2Kcr|)~VT'3(7)}, 

138 
or equivalently that 
exp{(-2(7|)~^G3(7)} oc «^^2(7)exp{(-2Kcr|)~Vt'3(7)}, 
for all constants k. That is, 
«^'2(T)exp{(-2o-2)-lG3(-,)(« _ 1)} 
must be constant with respect to o-g and 7. This occurs if and only if G'lil) = Go 
and 0^(7) = 0. 
6.3 
Proof of Result 4.8 
We begin by showing that 
/*(iy I y ° )  = (l/c)/*(['it; - A'6]/c | y ) ,  
(6.1) 
and then we indicate how this result can be used to verify (4.4) and (4..5). 
Note that 
I c y  + X b \ a l , t )  =  (l/c)g^([w - A'6]/c | y;(<7|/c^),7) 
and 
and so 
/*(ui I cy + Jffc) = 
L J  92i^ I (<7'e/c^),?K2((T^/c^,7) d<T^ dl 
UO 
JO 
r-ry-t 
i>nri 
/ r,,, 
\'/il 
-1 
X 
00 ^ 
[w - A'6] 
cr| 
0 
JO 
—7— 
* f ^ I 
^ I 
f ^ I 
^92 1^1 -^'7 I 7^2 I ^,7 I dxri d-f 

•1 
139 
Upon making the change of variable 6\ = 
we obtain 
f  [ w  \ c y  +  X h )  =  
g ^ { z  \ $i,'^)'!r2{0i,-r) dOi d-)' 
X(72(^ I ^1,7)7^2(^1,7) 
àj 
= (l/c)/*([iy - A'6]/c I y) 
which verifies (6.1). 
Now, for notational convenience, we rewrite (4.4) and (4.5) as 
S { k a [ y ° ] )  =  <^S{ka[y]) +  X ' b .  
(6.2) 
To verify condition (6.2), it suffices to show that: 
1. A:('~^)(y°) = ck^~^\y), and fc(®^(y°) = ck^^\y)\ 
2. If A:(^~^)(y°) = 
and k^^~^\y°) = ck^^~'^\y) then k^^\y°) = 
ck^^\y)\ and 
3. If k ( ' \ y ° )  = c k ( ' \ y ) ,  then 5(A;(0[y°]) = cS{ki')[y]) +  X ' b .  
Conditions 1, 2, and 3 insure that the sequences of intervals produced by applying 
the algorithm of Section 3.2.2 to the solution of (3.14) and (4.3), respectively, satisfy 
t h e  r e l a t i o n s h i p  
=  c 6 ' ( & ( ^ ) [ % / ] )  +  X ^ b  a n d  h e n c e  t h a t  ka[y°] a n d  ka[y], 
which are the limits of these two sequences, satisfy (6.2). 
Verifying condition 3 requires that we show that condtions similar to 1 and 2 
are satisfied by the sequences of iterates generated in the solution of (3.16) and (4.6). 
S p e c i f i c a l l y ,  f o r  g i v e n  v a l u e s  o f  k { y )  a n d  k { y ° )  ( s u c h  t h a t  c k { y )  =  k { y ° ) ) ,  

140 
3a. 
= cp(~^)[A;(y] + X'b and 
+ A'6; 
3b. If p(^~"^)[/s(y°)] = cp(^~^)[A;(y] + A^6 and 
= cp^^~'^'>[k(y] + X'b, 
then 
[&(?/] + A'6; 
The verification of conditions 1, 2, 3, 3a, and 3b is straightforward, though 
tedious, and makes use of relationship (6.1). The details are omitted. 

