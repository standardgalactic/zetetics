The Many Faces of
Information Geometry
Frank Nielsen
Information geometry [Ama16, AJLS17, Ama21] aims at
unravelling the geometric structures of families of proba-
bility distributions and at studying their uses in informa-
tion sciences. Information sciences is an umbrella term re-
grouping statistics, information theory, signal processing,
machine learning and AI, etc. Information geometry was
born independently from econometrician H. Hotelling
(1930) and statistician C. R. Rao (1945) from the math-
ematical curiosity of considering a parametric family of
probability distributions, called the statistical model, as
a Riemannian manifold equipped with the Fisher metric
tensor [Nie20]. Information geometry tackles problems by
using the concepts of differential geometry (like curvature)
with tensor calculus. In his pioneer work, Rao considered
the Riemannian geodesic distance and geodesic balls on
the manifold to study classification and hypothesis testing
problems in statistics.
Let (ğ’³, â„±, ğœ‡) denote a probability space [Kee10] (with
sample space ğ’³, ğœ-algebra â„±, and finite positive measure
Frank Nielsen is a senior researcher (Fellow) of Sony Computer Science Labo-
ratories, Inc., Tokyo, Japan. His email address is Frank.Nielsen@acm.org.
Communicated by Notices Associate Editor Richard Levine.
For permission to reprint this article, please contact:
reprint-permission@ams.org.
DOI: https://doi.org/10.1090/noti2403
ğœ‡, usually chosen as the Lebesgue mesure ğœ‡ğ¿or the count-
ing measure ğœ‡ğ‘), and consider a parametric family ğ’«=
{ğ‘ƒğœƒâˆ¶ğœƒâˆˆÎ˜} of probability distributions, all dominated
by ğœ‡. Let ğ‘ğœƒ(ğ‘¥) â‰”
ğ‘‘ğ‘ƒğœƒ(ğ‘¥)
ğ‘‘ğœ‡
denote the Radon-Nikodym deriv-
ative, the probability density function of random variable
ğ‘‹âˆ¼ğ‘ğœƒ. By definition, the Fisher Riemannian metric ğ‘”ğ¹
expressed in the ğœƒ-coordinate system is the Fisher informa-
tion matrix (FIM) of the random variable ğ‘‹: [ğ‘”ğ¹]ğœƒâ‰”ğ¼ğ‘‹(ğœƒ)
with
ğ¼ğ‘‹(ğœƒ) â‰”ğ¸ğ‘ğœƒ[ğ‘ ğœƒ(ğ‘¥)ğ‘ ğœƒ(ğ‘¥)âŠ¤] ,
where ğ‘ ğœƒ(ğ‘¥)
â‰”
âˆ‡ğœƒlog ğ‘ğœƒ(ğ‘¥) is called the score func-
tion [Kee10].
The Fisher metric is also referred to as
the Shahshahani metric in mathematical biology.
Be-
cause the FIM is the covariance matrix of the score (since
ğ¸ğ‘ğœƒ[ğ‘ ğœƒ(ğ‘¥)]
=
0), ğ¼ğ‘‹(ğœƒ) is necessarily positive semidef-
inite, and positive-definite for regular statistical mod-
els [Ama16]. The FIM is covariant under reparameteriza-
tion: for any smooth invertible mapping ğœ‚(ğœƒ) with invert-
ible Jacobian matrix [
ğœ•ğœƒğ‘—
ğœ•ğœ‚ğ‘—]
ğ‘–ğ‘—
, we have
ğ¼ğœ‚(ğœ‚) = [ğœ•ğœƒğ‘–
ğœ•ğœ‚ğ‘—
]
âŠ¤
ğ‘–ğ‘—
Ã— ğ¼ğœƒ(ğœƒ(ğœ‚)) Ã— [ğœ•ğœƒğ‘–
ğœ•ğœ‚ğ‘—
]
ğ‘–ğ‘—
.
36
NOTICES OF THE AMERICAN MATHEMATICAL SOCIETY
VOLUME 69, NUMBER 1

The Riemannian Fisher length element induced by the
Fisher metric
dğ‘ ğœƒ= âˆšdğœƒâŠ¤Ã— [ğ‘”ğ¹]ğœƒÃ— dğœƒ
is invariant under any smooth invertible reparameteriza-
tion: dğ‘ ğœƒ= dğ‘ ğœ‚with dğ‘ 2
ğœ‚= dğœ‚âŠ¤Ã— [ğ‘”ğ¹]ğœ‚Ã— dğœ‚. Nowadays,
the Riemannian manifold (ğ’«, ğ‘”ğ¹) is commonly called the
Fisher-Rao manifold [Nie20], and its induced Riemannian
geodesic length distance ğœŒğ‘”(ğœƒ1, ğœƒ2) is called the Fisher-Rao
distance ğœŒRao(ğ‘ğœƒ1, ğ‘ğœƒ2) â‰”ğœŒğ‘”ğ¹(ğœƒ1, ğœƒ2) with
ğœŒğ‘”ğ¹(ğœƒ1, ğœƒ2) â‰”âˆ«
1
0
dğ‘ ğ›¾(ğ‘¡) dğ‘¡,
where ğ›¾(ğ‘¡) denotes the Riemannian geodesic [GN14] with
the boundary conditions ğ›¾(0) = ğœƒ1 and ğ›¾(1) = ğœƒ2. Thus the
Fisher-Rao distance used to evaluate the dissimilarities be-
tween probability distributions is invariant under reparam-
eterization. For example, the Fisher-Rao distance remains
the same whether the family of normal distributions are
parameterized by ğœ†= (ğœ‡, ğœ), ğœ†â€² = (ğœ‡, ğœ2), or ğœƒ= (
ğœ‡
ğœ2 , âˆ’
1
2ğœ2 ):
ğœŒRao(ğ‘ğœ†1, ğ‘ğœ†2) = ğœŒRao(ğ‘ğœ†â€²
1, ğ‘ğœ†â€²
2) = ğœŒRao(ğ‘ğœƒ1, ğ‘ğœƒ2).
This parameterization invariance property of statistical dis-
tances highlights the power of modeling geometrically sta-
tistical models. The Fisher-Rao manifolds may have differ-
ent constant sectional curvatures ğœ…: for example, the curva-
tures of the Fisher-Rao manifolds of univariate normal dis-
tributions, univariate zero-centered multivariate normal
distributions, and categorical distributions are ğœ…= âˆ’
1
2 < 0,
ğœ…= 0, and ğœ…=
1
4 > 0, respectively. Information geometry
elucidates the role played by curvature in statistics. Since
any Riemannian manifold of dimension ğ·can be isometri-
cally embedded in a Euclidean space of dimension at most
ğ·ğ¸=
1
2ğ·(ğ·+ 1)(3ğ·+ 11) using Nashâ€™s embedding theo-
rem, we may visualize the Fisher-Rao manifold (ğ’«, ğ‘”ğ¹) as
a ğ·-dimensional surface of â„ğ·ğ¸. This extrinsic view of ge-
ometry is helpful to intuitively grasp the notion of tangent
planes ğ‘‡ğ‘ğœƒand tangent vectors ğ‘£âˆˆğ‘‡ğ‘ğœƒat any ğ‘ğœƒâˆˆğ’«and
allows one to visualize geodesics on surfaces. However, let
us point out that differential geometry defines intrinsically
these notions [GN14].
Using the Fisher metric can be justified from several the-
oretical viewpoints [Ama16]:
â€¢ First, the FIM occurs when locally approximating
the Kullback-Leibler (KL) divergence [Kee10]. In
statistics, estimating densities using the Maximum
Likelihood Estimator (MLE) or the maximum en-
tropy principle under moment constraints (Max-
Ent) can be interpreted as KL divergence mini-
mization problems [Kee10] (to be detailed below).
The KL divergence between densities ğ‘ğœƒ1 and ğ‘ğœƒ2
is defined by:
ğ·KL[ğ‘ğœƒ1 âˆ¶ğ‘ğœƒ2] â‰”âˆ«
ğ’³
ğ‘ğœƒ1(ğ‘¥) log ğ‘ğœƒ1(ğ‘¥)
ğ‘ğœƒ2(ğ‘¥)dğœ‡(ğ‘¥).
The delimiter â€œ:â€ indicates that the divergence is
oriented: ğ·KL[ğ‘ğœƒ1 âˆ¶ğ‘ğœƒ2] â‰ ğ·KL[ğ‘ğœƒ2 âˆ¶ğ‘ğœƒ1]. The
KL divergence can be expressed as
ğ·KL[ğ‘ğœƒ1 âˆ¶ğ‘ğœƒ2] = â„Ã—[ğ‘ğœƒ1 âˆ¶ğ‘ğœƒ2] âˆ’â„[ğ‘ğœƒ1],
where â„Ã—[ğ‘ğœƒ1 âˆ¶ğ‘ğœƒ2] denotes the cross-entropy
â„Ã—[ğ‘ğœƒ1
âˆ¶
ğ‘ğœƒ2]
â‰”
âˆ’âˆ«ğ‘ğœƒ1 log ğ‘ğœƒ2dğœ‡(ğ‘¥) and
â„[ğ‘ğœƒ1]
â‰”
âˆ’âˆ«ğ‘ğœƒ1 log ğ‘ğœƒ1dğœ‡(ğ‘¥) is Shannon en-
tropy. Hence, the KL divergence is also called rel-
ative entropy in information theory. The second-
order Taylor approximation of the KL divergence
yields
ğ·KL[ğ‘ğœƒâˆ¶ğ‘ğœƒ+ğ‘‘ğœƒ] = 1
2ğ‘‘ğœƒâŠ¤Ã— ğ¼ğœƒ(ğœƒ) Ã— ğ‘‘ğœƒâ‰ˆ1
2dğ‘ 2
ğœƒ.
More generally, the FIM is used in the local ap-
proximations of ğ‘“-divergences [Ama16]:
ğ¼ğ‘“[ğ‘ğœƒâˆ¶ğ‘ğœƒ+ğ‘‘ğœƒ] = 1
2ğ‘“â€³(1) ğ‘‘ğœƒâŠ¤Ã— ğ¼ğœƒ(ğœƒ) Ã— ğ‘‘ğœƒ,
where
ğ¼ğ‘“[ğ‘ğœƒ1 âˆ¶ğ‘ğœƒ2] â‰”âˆ«
ğ’³
ğ‘ğœƒ1(ğ‘¥)ğ‘“(
ğ‘ğœƒ2(ğ‘¥)
ğ‘ğœƒ1(ğ‘¥)) dğœ‡(ğ‘¥)
for a convex function ğ‘“(ğ‘¢) satisfying ğ‘“(1) = 0,
and strictly convex at 1. The KL divergence is an
ğ‘“-divergence obtained for ğ‘“(ğ‘¢) = âˆ’log(ğ‘¢) with
ğ‘“â€³(ğ‘¢) = 1. The ğ‘“-divergences are said to be sep-
arable because they can be written as integrals of
scalar divergences:
ğ¼ğ‘“[ğ‘âˆ¶ğ‘] = âˆ«
ğ’³
ğ‘–ğ‘“[ğ‘(ğ‘¥) âˆ¶ğ‘(ğ‘¥)]dğœ‡(ğ‘¥)
with ğ‘–ğ‘“[ğ‘âˆ¶ğ‘] â‰”ğ‘ğ‘“(ğ‘/ğ‘). The ğ‘“-divergences enjoy
the following monotonicity property:
ğ¼ğ‘“[ğ‘ğ‘Œâˆ¶ğ‘ğ‘Œ] â‰¤ğ¼ğ‘“[ğ‘ğ‘‹âˆ¶ğ‘ğ‘‹],
where ğ‘ğ‘Œand ğ‘ğ‘Œare the densities induced by
a Markov kernel from measurable space (ğ’³, â„±)
to measurable space (ğ’´, ğ’¢) [Ama21].
To give
a concrete example, consider the ğ‘“-divergences
between two (normalized) histograms ğ‘
=
(ğ‘1, â€¦ , ğ‘2ğ‘›) and ğ‘= (ğ‘1, â€¦ , ğ‘2ğ‘›) with 2ğ‘›bins
representing multinomial probability laws. Then
ğ¼ğ‘“[ğ‘â€² âˆ¶ğ‘â€²] â‰¤ğ¼ğ‘“[ğ‘âˆ¶ğ‘], where ğ‘â€² = (ğ‘â€²
1, â€¦ , ğ‘â€²
ğ‘›)
and ğ‘â€² = (ğ‘1, â€¦ , ğ‘â€²
ğ‘›) with ğ‘â€²
ğ‘–= ğ‘2(ğ‘–âˆ’1)+1 + ğ‘2ğ‘–
and ğ‘â€²
ğ‘–= ğ‘2(ğ‘–âˆ’1)+1 + ğ‘2ğ‘–reduced histograms ob-
tained by merging consecutive bins (a very spe-
cial deterministic Markov kernel from measurable
space ([2ğ‘›], 2[2ğ‘›]) to measurable space ([ğ‘›], 2[ğ‘›])
JANUARY 2022
NOTICES OF THE AMERICAN MATHEMATICAL SOCIETY
37

where [ğ‘ ] â‰”{1, â€¦ , ğ‘ }). The only separable statisti-
cal divergences satisfying this monotonicity prop-
erty are ğ‘“-divergences when ğ‘›> 2 [Ama16].
â€¢ A sufficient statistic [Kee10] ğ‘Œ= ğ‘‡(ğ‘‹) for the pa-
rameter ğœƒof a random variable ğ‘‹âˆ¼ğ‘ğœƒis such that
the conditional probability of ğ‘‹given ğ‘Œ= ğ‘‡(ğ‘‹)
does not depend on ğœƒ. That is, all statistical in-
formation concerning parameter ğœƒis contained in
ğ‘Œ= ğ‘‡(ğ‘‹). For example, ğ‘‡(ğ‘‹) = (ğ‘‡1(ğ‘‹), ğ‘‡2(ğ‘‹))
with ğ‘‡1(ğ‘‹) = âˆ‘
ğ‘›
ğ‘–=1 ğ‘‹ğ‘–and ğ‘‡2(ğ‘‹) = âˆ‘
ğ‘›
ğ‘–=1 ğ‘‹2
ğ‘–is
a sufficient statistic for the parameter ğœƒ= (ğœ‡, ğœ)
of a random vector (ğ‘‹1, â€¦ , ğ‘‹ğ‘›) of ğ‘›independent
and identically distributed (i.i.d.) random vari-
ables ğ‘‹1, â€¦ , ğ‘‹ğ‘›âˆ¼ğ‘ğœƒfollowing a normal distri-
bution ğ‘ğœ‡,ğœ. In general, we have ğ¼ğ‘Œ(ğœƒ) â‰¤ğ¼ğ‘‹(ğœƒ)
with equality if and only if (iff) ğ‘Œis a sufficient
statistic [Kee10].
Let us observe that for ğ‘›i.i.d.
random variables ğ‘‹ğ‘–âˆ¼ğ‘ğœƒ, we have ğ¼ğ‘‹1,â€¦,ğ‘‹ğ‘›(ğœƒ) =
ğ‘›ğ¼ğ‘‹(ğœƒ) with ğ‘‹âˆ¼ğ‘ğœƒ. Sufficiency also character-
izes the equality in the monotonicity inequality
of ğ‘“-divergences: ğ¼ğ‘“[ğ‘ğ‘Œâˆ¶ğ‘ğ‘Œ] â‰¤ğ¼ğ‘“[ğ‘ğ‘‹âˆ¶ğ‘ğ‘‹] with
equality iff ğ‘Œ= ğ‘‡(ğ‘‹) is a sufficient statistic.
â€¢ Let
Ì‚ğœƒğ‘›be an unbiased estimator of ğœƒof an i.i.d.
random vector (ğ‘‹1, â€¦ , ğ‘‹ğ‘›) âˆ¼ğ‘ğœƒ. Then the follow-
ing CramÂ´er-Rao lower bound (CRLB) on the vari-
ance of
Ì‚ğœƒğ‘›holds:
Var[ Ì‚ğœƒğ‘›] = ğ¸[( Ì‚ğœƒğ‘›âˆ’ğ¸[ Ì‚ğœƒğ‘›])
2] âª°1
ğ‘›ğ¼âˆ’1
ğœƒ(ğœƒ),
where ğ´âª°ğµiff matrix ğ´âˆ’ğµis positive semidef-
inite. The notation âª°indicates the comparison
with respect to (w.r.t.) the Loewner partial order-
ing of positive semidefinite matrices. Thus the in-
verse of the FIM provides a lower bound on the ac-
curacy of any unbiased estimator. An estimator is
said to be Fisher efficient when its variance asymp-
totically matches the CRLB when ğ‘›â†’âˆ.
The Fisher metric is the only invariant metric under Mar-
kovian morphisms of statistical models [Ama16]. How-
ever, let us point out that there are infinitely many coun-
terparts of the FIM in quantum information geometry,
and that other alternative Riemannian information met-
rics can be explored (e.g., the Wasserstein information met-
ric [Li21]).
In statistics, two special types of statistical models,
called the exponential families and the mixture families,
are often handled:
â€¢ An exponential family [Kee10] is a set of paramet-
ric densities â„°â‰”{ğ‘ğœƒ(ğ‘¥)dğœ‡} such that
ğ‘ğœƒ(ğ‘¥) â‰”exp (
ğ·
âˆ‘
ğ‘–=1
ğ‘¡ğ‘–(ğ‘¥)ğœƒğ‘–âˆ’ğ¹(ğœƒ)) ,
where the (ğ‘¡1(ğ‘¥), â€¦ , ğ‘¡ğ·(ğ‘¥)) form the minimal
sufficient statistic. Function ğ¹(ğœƒ) is used to nor-
malize the densities:
ğ¹(ğœƒ) = log (âˆ«
ğ’³
exp (
ğ·
âˆ‘
ğ‘–=1
ğ‘¡ğ‘–(ğ‘¥)ğœƒğ‘–) dğœ‡(ğ‘¥)) ,
and called the cumulant function (or log-partition
in statistical physics). ğ¹(ğœƒ) is strictly convex for
(full regular) exponential families [Kee10].
For
example, the family of ğ‘‘-variate normal distri-
butions is an exponential family of order ğ·=
ğ·(ğ·+3)
2
w.r.t. the Lebesgue measure ğœ‡ğ¿, and the
family of Poisson distributions is a discrete expo-
nential family of order ğ·= 1 w.r.t. the counting
measure ğœ‡ğ‘. Exponential families have all finite-
dimensional minimal sufficient statistics.
â€¢ A mixture family [Nie20] is a set of parametric
densities â„³
â‰”{ğ‘šğœƒ(ğ‘¥)dğœ‡} such that ğ‘šğœƒ(ğ‘¥) =
âˆ‘
ğ·
ğ‘–=1 ğœƒğ‘–ğ‘ğ‘–(ğ‘¥)+(1âˆ’âˆ‘
ğ·
ğ‘–=1 ğœƒğ‘–)ğ‘0(ğ‘¥), where functions
1, ğ‘0(ğ‘¥), ğ‘1(ğ‘¥), â€¦ , ğ‘ğ·(ğ‘¥) are linearly independent
functions. Statistical mixtures such as Gaussian
mixture models with prescribed component den-
sities are examples of mixture families. Mixture
families are closed under convex combinations. It
can be shown that the negentropy of mixture den-
sities is a strictly convex function [Nie20]: ğ¹(ğœƒ) =
âˆ’â„[ğ‘šğœƒ].
For these two types of statistical models, the Fisher
metric is a Hessian metric [Shi07] since the FIM is the
Hessian of some strictly convex potential function ğ¹(ğœƒ):
ğ¼(ğœƒ) = âˆ‡2ğ¹(ğœƒ). This is easily checked for exponential fami-
lies as the FIM can be written under mild regularity condi-
tions [Ama16] as ğ¼ğ‘‹(ğœƒ) = âˆ’ğ¸ğ‘ğœƒ[âˆ‡2 log ğ‘ğœƒ(ğ‘¥)].
In general, calculating in closed-form the Fisher-Rao
distances may be difficult since it requires to solve the
Riemannian geodesic equation with boundary conditions,
and to integrate the length elements along geodesics. For
example, although the Fisher-Rao distance between uni-
variate normal distributions is available in closed-form, we
do not have a closed-form formula for the Fisher-Rao dis-
tance between multivariate normals [Nie20]. Thus in prac-
tice, the Fisher-Rao between multivariate normals is nu-
merically approximated. We shall now explain that the
Fisher-Rao manifolds with Hessian metrics carry another
beautiful geometric dual structure which is well suited
for computation in applications: namely, these exponen-
tial/mixture families can be modeled as Hessian mani-
folds [Shi07], and are commonly called dually flat spaces
in information geometry [Ama21].
In the second half of the 20th century, information
geometry gained a momentum with the pathbreaking
work of N. Chentsov.
Chentsov shared statistician A.
Waldâ€™s viewpoint that all problems in statistics can be
38
NOTICES OF THE AMERICAN MATHEMATICAL SOCIETY
VOLUME 69, NUMBER 1

viewed as decision problems, and therefore investigated
a theoretical framework for characterizing optimal deci-
sion rules in statistics using category theory and Markov-
ian morphisms [ Ë‡Cen82]. A family of probability distri-
butions should be invariant both under smooth one-to-
one transformations of its parameter ğœƒand under trans-
formations of the corresponding random variables by
sufficient statistics.
This precisely defines the statistical
invariance.
Chentsovâ€™s breakthrough consisted in sepa-
rating the metric tensor ğ‘”(used to measure angles be-
tween vectors and lengths of vectors in a tangent plane)
from its induced Levi-Civita connection ğ‘”âˆ‡used by de-
fault on a Riemannian manifold for obtaining (locally)
length minimizing geodesics. This novel insight allowed
Chentsov to model statistical models as differentiable
manifolds equipped with affine connections âˆ‡more gen-
eral than the Levi-Civita connection of Fisher-Rao mani-
folds. More precisely, Chentsov discovered the existence of
a unique totally symmetric third-order tensor fulfilling the
statistical invariance which is nowadays called the Amari-
Chentsov tensor or skewness tensor, and used that ten-
sor to build invariant affine connections. A. Kolmogorov
called Chentsovâ€™s field of research â€œgeometrostatisticsâ€ in
Russian (translated as â€œgeometrical statisticsâ€ in the eng-
lish monograph [ Ë‡Cen82]).
In short, an affine connection âˆ‡[GN14] defines the fol-
lowing:
â€¢ A way to differentiate a vector field ğ‘‹on a man-
ifold (or more generally a tensor field ğ‘‡) by an-
other vector field ğ‘Œ: namely, the covariant deriva-
tives denoted by âˆ‡ğ‘Œğ‘‹(or âˆ‡ğ‘Œğ‘‡).
â€¢ The parallel transport âˆ
âˆ‡
ğ‘(ğ‘¡) ğ‘£for a tangent vector
ğ‘£âˆˆğ‘‡ğ‘(0) along any smooth curve ğ‘(ğ‘¡). An affine
connection allows to parallel transport vectors of
different tangent planes onto a common tangent
plane in order to measure their subtended angles
using the metric tensor in that common tangent
plane.
â€¢ âˆ‡-geodesics ğ›¾defined as autoparallel curves:
âˆ‡Ì‡ğ›¾Ì‡ğ›¾= 0. Geodesics ğ›¾âˆ‡(ğ‘¡) are calculated by solving
the second-order non-linear ordinary differential
equation (ODE):
Ìˆğœƒğ‘–+
ğ‘›
âˆ‘
ğ‘—,ğ‘˜=1
Î“ğ‘–
ğ‘—ğ‘˜Ì‡ğœƒğ‘—Ì‡ğœƒğ‘˜= 0,
ğ‘–âˆˆ{1, â€¦ , ğ·},
where ğœƒ= (ğœƒ1, â€¦ , ğœƒğ·),
Ì‡ğœƒğ‘™â‰”
ğ‘‘
dğ‘¡ğœƒğ‘™, and Î“ğ‘–
ğ‘—ğ‘˜are
the Christoffel symbols [GN14] (ğ·3 smooth func-
tions) defining the affine connection. In physics,
geodesics represent free particle trajectories.
The curvature tensor ğ‘…âˆ‡and the torsion tensor ğ‘‡âˆ‡of
a manifold (ğ‘€, ğ‘”, âˆ‡) are induced by the chosen connec-
tion âˆ‡[GN14]. The fundamental theorem of Riemannian
Î³
gF âˆ‡
Î¸1Î¸2 (t)
PÎ¸1
PÎ¸2
ÏRao(PÎ¸1, PÎ¸2) = ÏgF(Î¸1, Î¸2)
PÎ¸1
PÎ¸2
âˆ‡Î±=(âˆ‡âˆ’Î±)âˆ—
âˆ‡âˆ’Î±=(âˆ‡Î±)âˆ—
Fisher-Rao geometry
â†’Fisher-Rao geodesic distance
Dual Î±-geometry
â†’No default divergence
versus
gâˆ‡= âˆ‡Î±+âˆ‡âˆ’Î±
2
Î³âˆ‡âˆ’Î±
Î¸1Î¸2 (t)
Î³âˆ‡Î±
Î¸1Î¸2(t)
Figure 1. Fisher-Rao geometry vs. dual ğ›¼-geometry.
geometry [GN14] states that there exists a unique torsion-
free affine connection which preserves the metric, meaning
that for any two vectors ğ‘£1 and ğ‘£2 of the tangent plane ğ‘‡ğ‘,
and a smooth curve ğ‘(ğ‘¡) with ğ‘(0) = ğ‘, we have for any
ğ‘¡: ğ‘”(ğ‘£1, ğ‘£2) = ğ‘”(âˆ
âˆ‡
ğ‘(ğ‘¡) ğ‘£1, âˆ
âˆ‡
ğ‘(ğ‘¡) ğ‘£2). This unique torsion-
free affine connection is called the Levi-Civita metric con-
nection. Historically, affine connections were studied by
Â´E. Cartan in the 1920s, and used in the Einstein-Cartan
theory of gravity. Chentsov considered regular exponen-
tial families in his monograph, and by considering their in-
variance, discovered the so-called exponential connection
âˆ‡ğ‘’.
The field of information geometry was shaped by
S.-i. Amari who dreamt of a mathematical theory of
neuroscience.
Amari pioneered the dualistic statisti-
cal structures of information geometry: that is, Amari
showed that given any torsion-free affine connection âˆ‡,
there exists a dual torsion-free affine connection âˆ‡âˆ—such
that the mid-connection
âˆ‡+âˆ‡âˆ—
2
corresponds to the Levi-
Civita connection.
This duality ensures that the pri-
mal and dual parallel transports are metric compatible:
ğ‘”(ğ‘£1, ğ‘£2) = ğ‘”(âˆ
âˆ‡
ğ‘(ğ‘¡) ğ‘£1, âˆ
âˆ‡âˆ—
ğ‘(ğ‘¡) ğ‘£2). Notice that the lengths
of dually parallel-transported vectors by âˆ‡and âˆ‡âˆ—may
JANUARY 2022
NOTICES OF THE AMERICAN MATHEMATICAL SOCIETY
39

vary along ğ‘(ğ‘¡) but their inner product is kept constant. For
parametric statistical models ğ’«= {ğ‘ğœƒ}, Amari reported the
ğ›¼-geometry (for ğ›¼âˆˆâ„) which is a manifold equipped with
the Fisher metric and a pair of dual connections (âˆ‡âˆ’ğ›¼, âˆ‡ğ›¼)
coupled to the Fisher metric ğ‘”ğ¹:
âˆ‡âˆ’ğ›¼+âˆ‡ğ›¼
2
= âˆ‡0 = ğ‘”ğ¹âˆ‡.
Amariâ€™s ğ›¼-connections âˆ‡ğ›¼are defined by their Christoffel
symbols Î“ğ›¼
ğ‘˜ğ‘–,ğ‘—:
Î“ğ›¼
ğ‘˜ğ‘–,ğ‘—(ğœƒ) = ğ¸ğ‘ğœƒ[(ğœ•ğ‘˜ğœ•ğ‘–ğ‘™ğœƒ(ğ‘¥) + 1 âˆ’ğ›¼
2
ğœ•ğ‘˜ğ‘™ğœƒ(ğ‘¥)ğœ•ğ‘–ğ‘™ğœƒ(ğ‘¥)) ğœ•ğ‘—ğ‘™ğœƒ(ğ‘¥)] ,
where ğ‘™ğœƒ(ğ‘¥) â‰”log ğ‘ğœƒ(ğ‘¥) is the log-likelihood function and
ğœ•ğ‘ â‰”
ğœ•
ğœ•ğœƒğ‘ . Chentsovâ€™s exponential connection âˆ‡ğ‘’corre-
sponds to Amariâ€™s âˆ‡1 connection. The ğ‘’-connection was
also studied by Efron who defined geometrically the no-
tion of statistical curvature to study the higher-order as-
ymptotic theory of statistical estimators in a landmark pa-
per [Efr75] which has been recognized as one of the first
successful applications of differential geometry to statisti-
cal inference. The dual connection of âˆ‡ğ‘’is âˆ‡ğ‘šâ‰”(âˆ‡ğ‘’)âˆ—=
âˆ‡âˆ’1, and called the mixture connection.
This connec-
tion was proposed by P. Dawid, a discussant of Efronâ€™s
paper [Efr75]. Thus the Fisher-Rao geometry can be in-
terpreted as the 0-geometry enhanced with the Fisher-Rao
geodesic distance (Figure 1). The Riemannian geodesics
are ğ‘”âˆ‡-autoparallel and have the property to locally min-
imize the geodesic lengths [GN14].
In general, the ğ›¼-
geometry is not associated with any statistical divergence
when ğ›¼â‰ 0. But the ğ›¼-geometry may be recovered from
the divergence geometry of invariant ğ‘“-divergences on the
probability simplex [Egu83] (to be detailed below).
A connection âˆ‡is said to be flat [GN14,Shi07] when it
has zero torsion and when there exists a local coordinate
system ğœƒsuch that the Christoffel symbols Î“ğ‘˜
ğ‘–ğ‘—defining âˆ‡
expressed in that coordinate system vanish: Î“ğ‘˜
ğ‘–ğ‘—(ğœƒ) = 0. The
coordinate system ğœƒis called a âˆ‡-affine coordinate system.
In general the parallel transport of ğ‘£âˆˆğ‘‡ğ‘to ğ‘‡ğ‘is curve
dependent: âˆ
âˆ‡
ğ‘1(ğ‘¡) ğ‘£â‰ âˆ
âˆ‡
ğ‘2(ğ‘¡) ğ‘£for smooth curves ğ‘1 and
ğ‘2 with endpoints ğ‘1(0) = ğ‘2(0) = ğ‘and ğ‘1(1) = ğ‘2(1) = ğ‘.
One can visualize locally the presence of curvature of a
connection or not at a point ğ‘by considering the paral-
lel transport of a vector ğ‘£along a closed infinitesimal loop
ğ‘™encircling ğ‘(with ğ‘™(0) = ğ‘™(1)): if there is an angle defi-
ciency between ğ‘£and âˆ
âˆ‡
ğ‘™(1) ğ‘£, then the manifold has non-
zero curvature at ğ‘[Nie20]. However, the parallel trans-
port is independent of the curves linking the point ğ‘to
the point ğ‘for flat connections. It is a fundamental result
of information geometry that if (ğ‘€, ğ‘”, âˆ‡) is flat, then so is
(ğ‘€, ğ‘”, âˆ‡âˆ—) with âˆ‡âˆ—= 2ğ‘”âˆ‡âˆ’âˆ‡. We get the so-called dually
flat spaces of information geometry [Ama16] (ğ‘€, ğ‘”, âˆ‡, âˆ‡âˆ—)
which are special Hessian manifolds [Shi07] admitting a
single chart atlas. Notice that in a dually flat space, the
Levi-Civita connection is usually not flat.
In information theory, a statistical divergence like the
KL divergence is loosely speaking a potentially asymmet-
ric dissimilarity measure between probability distributions
which may fail the triangle inequality of metric distances.
In information geometry, a divergence (historically called
a contrast function [Egu83]) is a smooth dissimilarity mea-
sure ğ·(ğœƒâˆ¶ğœƒâ€²) between parameters ğœƒand ğœƒâ€² that satisfies
the following conditions:
1.
ğ·(ğœƒâˆ¶ğœƒâ€²) â‰¥0 for all ğœƒ, ğœƒâ€² with equality iff ğœƒ= ğœƒâ€².
2.
ğœ•ğ‘–ğ·(ğœƒâˆ¶ğœƒâ€²)|ğœƒâ€²=ğœƒ= ğœ•â€²
ğ‘—ğ·(ğœƒâˆ¶ğœƒâ€²)|ğœƒâ€²=ğœƒ= 0 for all ğ‘–, ğ‘—,
where ğœ•ğ‘™â‰”
ğœ•
ğœ•ğœƒğ‘™and ğœ•â€²
ğ‘™â‰”
ğœ•
ğœ•ğœƒâ€²
ğ‘™
.
3.
âˆ’[ğœ•ğ‘–ğœ•â€²
ğ‘—ğ·(ğœƒâˆ¶ğœƒâ€²)|ğœƒâ€²=ğœƒ]ğ‘–ğ‘—is a positive-definite matrix.
The (parameter) divergence ğ·(ğœƒ
âˆ¶
ğœƒâ€²) can also be
interpreted as a function on the manifold defined by
the single chart equipped with the ğœƒ-coordinate system.
Eguchi [Egu83] reported a method to build a dualistic
structure (ğ‘€, ğ‘”, âˆ‡, âˆ‡âˆ—) from any divergence ğ·(â‹…âˆ¶â‹…) as fol-
lows:
ğ‘”ğ‘–ğ‘—(ğœƒ)
=
âˆ’ğœ•ğ‘–ğœ•â€²
ğ‘—ğ·(ğœƒâˆ¶ğœƒâ€²)|ğœƒâ€²=ğœƒ,
Î“ğ‘–ğ‘—,ğ‘˜(ğœƒ)
=
âˆ’ğœ•ğ‘–ğœ•ğ‘—ğœ•â€²
ğ‘˜ğ·(ğœƒâˆ¶ğœƒâ€²)|ğœƒâ€²=ğœƒ,
Î“âˆ—
ğ‘–ğ‘—,ğ‘˜(ğœƒ)
=
âˆ’ğœ•ğ‘˜ğœ•â€²
ğ‘–ğœ•â€²
ğ‘—ğ·(ğœƒâˆ¶ğœƒâ€²)|ğœƒâ€²=ğœƒ.
It can be shown that the connections âˆ‡and âˆ‡âˆ—induced re-
spectively by Î“ğ‘–ğ‘—,ğ‘˜and Î“âˆ—
ğ‘–ğ‘—,ğ‘˜are torsion-free and dual. This
geometry is called the divergence geometry of ğ·[Ama16].
Let ğ·âˆ—(ğœƒâˆ¶ğœƒâ€²) â‰”ğ·(ğœƒâ€² âˆ¶ğœƒ) denote the dual or reverse di-
vergence, and (ğ‘€, ğ·ğ‘”, ğ·âˆ‡, ğ·âˆ‡âˆ—) the information-geometric
space induced by ğ·. Then we have ğ·âˆ—âˆ‡= ğ·âˆ‡âˆ—and ğ·âˆ—âˆ‡âˆ—=
ğ·âˆ‡. Thus symmetric divergences ğ·(ğœƒâˆ¶ğœƒâ€²) = ğ·(ğœƒâ€² âˆ¶ğœƒ)
yield self-dual connections coinciding with the Levi-Civita
connection.
Many different divergences may yield the
same divergence geometry. The divergence geometry of ğ‘“-
divergences on the ğ·-dimensional probability simplex cor-
responds to Amariâ€™s ğ›¼-geometry for ğ›¼= 3+2
ğ‘“â€´(1)
ğ‘“â€³(1) , and the
divergence geometry of ğ·Rao(ğœƒ1 âˆ¶ğœƒ2) â‰”
1
2ğœŒ2
Rao(ğ‘ğœƒ1, ğ‘ğœƒ2)
yields the 0-geometry.
In a dually flat space, we can build a canonically
Fenchel-Young (non-metric) divergence ğ´(ğœƒ1 âˆ¶ğœ‚2):
ğ´(ğœƒ1 âˆ¶ğœ‚2) â‰”ğ¹(ğœƒ1) + ğ¹âˆ—(ğœ‚2) âˆ’ğœƒâŠ¤
1 ğœ‚2,
where ğ¹âˆ—(ğœ‚) denotes the convex conjugate obtained by the
Legende-Fenchel transform:
ğ¹âˆ—(ğœ‚) = sup
ğœƒ
{ğœƒâŠ¤ğœ‚âˆ’ğ¹(ğœƒ)}.
Legende-Fenchel transform yields a dual coordinate sys-
tem ğœ‚= âˆ‡ğ¹(ğœƒ), and the Fenchel-Young inequality ğ¹(ğœƒ1) +
ğ¹âˆ—(ğœ‚2) â‰¥ğœƒâŠ¤
1 ğœ‚2 ensures that ğ´(ğœƒ1 âˆ¶ğœ‚2) â‰¥0 with equality iff
ğœ‚2 = âˆ‡ğ¹(ğœƒ1). This divergence is shown to be equivalent to
a Bregman divergence [Ama16]: ğ´(ğœƒ1 âˆ¶ğœ‚2) = ğµğ¹(ğœƒ1 âˆ¶ğœƒ2)
40
NOTICES OF THE AMERICAN MATHEMATICAL SOCIETY
VOLUME 69, NUMBER 1

P1
P2
Dual geodesic Î³âˆ‡âˆ—
Primal geodesic Î³âˆ‡
gâˆ‡= âˆ‡+âˆ‡âˆ—
2
âˆ‡-aï¬ƒne coordinate system Î¸
âˆ‡âˆ—-aï¬ƒne coordinate system Î·
Î¸1 = Î¸(P1)
Î·1 = Î·(P1)
Î·2 = Î·(P2)
Î¸2 = Î¸(P2)
Potential function F(Î¸)
Dual potential function F âˆ—(Î·)
Î· = âˆ‡F(Î¸)
Î¸ = âˆ‡F âˆ—(Î·)
Legendre-Fenchel transform
Riemannian geodesic Î³
gâˆ‡
manifold P
Figure 2. Dually ï¬‚at space (ğ‘€, ğ‘”, âˆ‡, âˆ‡âˆ—) with âˆ‡-afï¬ne coordinate system ğœƒand dual âˆ‡âˆ—-afï¬ne coordinate system ğœ‚. Primal
geodesics ğ›¾âˆ‡and dual geodesics ğ›¾âˆ‡âˆ—are linear when plotted in the ğœƒ-coordinate system and ğœ‚-coordinate system, respectively.
with ğœƒ2 = âˆ‡ğ¹âˆ—(ğœ‚2), where
ğµğ¹(ğœƒ1 âˆ¶ğœƒ2) â‰”ğ¹(ğœƒ1) âˆ’ğ¹(ğœƒ2) âˆ’(ğœƒ1 âˆ’ğœƒ2)âŠ¤âˆ‡ğ¹(ğœƒ2)
is the Bregman divergence ğµğ¹(ğœƒ1 âˆ¶ğœƒ2) between parame-
ters ğœƒ1 and ğœƒ2 induced by a smooth strictly convex function
ğ¹(ğœƒ) and âˆ‡ğ¹(ğœƒ) â‰”[
ğœ•ğ¹(ğœƒ)
ğœ•ğœƒğ‘–]
âŠ¤
ğ‘–
denotes the gradient of ğ¹(ğœƒ).
Bregman divergences are widely used in machine learning
and originated from mathematical programming. Recip-
rocally, given a Bregman divergence, we can build a dually
flat space (the Bregman divergence geometry) with Hes-
sian metric [Shi07] [ğ‘”ğ‘–ğ‘—(ğœƒ)]ğ‘–ğ‘—= âˆ‡2ğ¹(ğœƒ) (positive-definite
because ğ¹is strictly convex) from its divergence geometry.
The dual affine connections are âˆ‡and âˆ‡âˆ—with Christoffel
symbols Î“ğ‘–ğ‘—ğ‘˜(ğœƒ) = 0 and Î“âˆ—ğ‘–ğ‘—ğ‘˜(ğœ‚) = 0, respectively. It fol-
lows that in the ğœƒ-coordinate system, the primal geodesics
ğ›¾âˆ‡are linear since the âˆ‡-geodesic ODE simplifies to
Ìˆğœƒğ‘–= 0,
and in the dual ğœ‚-coordinate system, the dual geodesics
are linear since the âˆ‡âˆ—-geodesic ODE simplifies to
Ìˆğœ‚ğ‘–= 0
(Figure 2). The ğœ‚-coordinate system is said to be âˆ‡âˆ—-affine.
We have [ğ‘”ğ‘–ğ‘—(ğœƒ)]ğ‘–ğ‘—= âˆ‡ğœƒâˆ‡ğœƒğ¹(ğœƒ) = âˆ‡ğœƒğœ‚and the dual Rie-
mannian metric [ğ‘”ğ‘–ğ‘—(ğœ‚)]ğ‘–ğ‘—= âˆ‡ğœ‚âˆ‡ğœ‚ğ¹âˆ—(ğœ‚) = âˆ‡ğœ‚ğœƒ. It fol-
lows that [ğ‘”ğ‘–ğ‘—(ğœƒ)]ğ‘–ğ‘—[ğ‘”ğ‘–ğ‘—(ğœ‚)]ğ‘–ğ‘—= ğ¼ğ·Ã—ğ·, the identity matrix.
The Bregman divergence construction from a dually flat
space is defined up to affine dual coordinate transforma-
tions ğœƒâ€² = ğ´ğœƒ+ ğ‘and ğœ‚â€² = ğ´âˆ’1ğœ‚+ ğ‘.
Amariâ€™s Â±1-geometry for the exponential and mixture
families yields dually flat spaces.
Their corresponding
Bregman divergences yield the following statistical diver-
gences:
â€¢ For an exponential family [Kee10] with density
ğ‘ğœƒ(ğ‘¥) = exp(âˆ‘
ğ·
ğ‘–=1 ğ‘¡ğ‘–(ğ‘¥)ğœƒğ‘–âˆ’ğ¹(ğœƒ)), the Legendre-
Fenchel conjugate function of the cumulant func-
tion ğ¹(ğœƒ) corresponds to Shannon negentropy,
ğ¹âˆ—(ğœ‚) = âˆ’â„[ğ‘ƒğœƒ], and the Bregman divergence
ğµğ¹(ğœƒ1 âˆ¶ğœƒ2) yields the reverse Kullback-Leibler di-
vergence ğ·âˆ—
KL (or reverse relative entropy):
ğµğ¹(ğœƒ1 âˆ¶ğœƒ2) = ğ·âˆ—
KL[ğ‘ğœƒ1 âˆ¶ğ‘ğœƒ2] = ğ·KL[ğ‘ğœƒ2 âˆ¶ğ‘ğœƒ1].
â€¢ For a mixture family [Ama16,Nie20] â„³with den-
sity ğ‘šğœƒ= âˆ‘
ğ·
ğ‘–=1 ğœƒğ‘–ğ‘ğ‘–(ğ‘¥) + (1 âˆ’âˆ‘
ğ·
ğ‘–=1 ğœƒğ‘–)ğ‘0(ğ‘¥), the
Bregman generator ğ¹(ğœƒ) = âˆ’â„[ğ‘šğœƒ] is strictly con-
vex with Î˜ = Î”âˆ˜
ğ·, the open ğ·-dimensional proba-
bility simplex. The canonical Bregman divergence
amounts to calculating the Kullback-Leibler diver-
gence [Nie20]: ğµğ¹(ğœƒ1 âˆ¶ğœƒ2) = ğ·KL[ğ‘šğœƒ1 âˆ¶ğ‘šğœƒ2].
In a dually flat space (ğ‘€, ğ‘”, âˆ‡, âˆ‡âˆ—), a generalized
Pythagorean theorem holds: Let ğ‘ƒ, ğ‘„, ğ‘…be three points. A
primal geodesic ğ›¾ğ‘ƒğ‘„intersects a dual geodesic ğ›¾âˆ—
ğ‘„ğ‘…orthogo-
nally w.r.t. to the metric ğ‘”at point ğ‘„, written as ğ›¾ğ‘ƒğ‘„âŸ‚ğ‘”ğ›¾âˆ—
ğ‘„ğ‘…,
iff
(ğœƒ(ğ‘ƒ) âˆ’ğœƒ(ğ‘„))âŠ¤Ã— (ğœ‚(ğ‘…) âˆ’ğœ‚(ğ‘„)) = 0.
In that case, the following Pythagorean equality holds:
ğµğ¹(ğœƒ(ğ‘ƒ) âˆ¶ğœƒ(ğ‘„)) + ğµğ¹(ğœƒ(ğ‘„) âˆ¶ğœƒ(ğ‘…)) = ğµğ¹(ğœƒ(ğ‘ƒ) âˆ¶ğœƒ(ğ‘…)).
When ğ¹(ğœƒ) =
1
2ğœƒâŠ¤ğœƒ, we recover the usual Euclidean geome-
try (a self-dual flat space) with ğœƒ= ğœ‚(since ğœ‚= âˆ‡ğ¹(ğœƒ) = ğœƒ
and ğœƒ= âˆ‡ğ¹âˆ—(ğœ‚) = ğœ‚with ğ¹âˆ—(ğœ‚) =
1
2ğœ‚âŠ¤ğœ‚), and we have
JANUARY 2022
NOTICES OF THE AMERICAN MATHEMATICAL SOCIETY
41

ğµğ¹(ğœƒ1 âˆ¶ğœƒ2) =
1
2â€–ğœƒ1 âˆ’ğœƒ2â€–2
2, where â€– â‹…â€–2 denotes the Eu-
clidean norm. The canonical âˆ‡-divergence is ğ·âˆ‡(ğ‘ƒâˆ¶ğ‘„) =
ğµğ¹(ğœƒ(ğ‘ƒ) âˆ¶ğœƒ(ğ‘„)) and the dual âˆ‡âˆ—-divergence is
ğ·âˆ‡âˆ—(ğ‘ƒâˆ¶ğ‘„) = ğµğ¹âˆ—(ğœ‚(ğ‘ƒ) âˆ¶ğœ‚(ğ‘„))
= ğµğ¹(ğœƒ(ğ‘„) âˆ¶ğœƒ(ğ‘ƒ)) = ğ·âˆ‡(ğ‘„âˆ¶ğ‘ƒ).
A ğœƒ-flat is a submanifold ğ‘€â€² âŠ‚ğ‘€such that ğœƒ(ğ‘€â€²) â‰”
{ğœƒ(ğ‘ƒ)
âˆ¶
ğ‘ƒâˆˆğ‘€â€²} is an affine subspace of Î˜ (ğ‘€â€² is a âˆ‡-
autoparallel submanifold). Similarly, an ğœ‚-flat is a sub-
manifold ğ‘€â€³ such that ğœ‚(ğ‘€â€³) â‰”{ğœ‚(ğ‘ƒ)
âˆ¶
ğ‘ƒâˆˆğ‘€â€³} is
an affine subspace of ğ»â‰”{âˆ‡ğ¹(ğœƒ) âˆ¶ğœƒâˆˆğœƒ}. Define the
âˆ‡-projection of a point ğ‘ƒâˆˆğ‘€onto a submanifold ğ‘€â€² as
Projâˆ‡
ğ‘€â€²(ğ‘ƒ) â‰”{ğ‘„âˆˆğ‘€â€² âˆ¶ğ›¾âˆ‡
ğ‘ƒğ‘„âŸ‚ğ‘”ğ‘€â€²}.
Then the âˆ‡-projection of ğ‘ƒis guaranteed to be a unique
point when ğ‘€â€² is an ğœ‚-flat. Moreover, we have Projâˆ‡
ğ‘€â€²(ğ‘ƒ) =
arg minğ‘„âˆˆğ‘€â€² ğ·âˆ‡(ğ‘„âˆ¶ğ‘ƒ) in a dually flat space.
These information projections can be used in statisti-
cal inference as follows. Consider the probability space
(ğ’³, 2ğ’³, ğœ‡ğ‘) with finite discrete sample space ğ’³= {1, â€¦ , ğ‘š}
and ğœ‡ğ‘the counting measure.
The categorical distribu-
tions form both an exponential family and a mixture fam-
ily [Ama16]. A categorical probability mass function can
be viewed as a point lying on the (ğ‘šâˆ’1)-dimensional open
standard simplex Î”ğ‘šâˆ’1.
â€¢ The Maximum Likelihood Estimator (MLE) of ğ‘›
i.i.d. observations ğ‘¥1, â€¦ , ğ‘¥ğ‘›sampled from an ex-
ponential family density ğ‘ğœƒâˆˆâ„°is
Ì‚ğœ‚= Ë†
âˆ‡ğ¹(ğœƒ) =
1
ğ‘›âˆ‘
ğ‘›
ğ‘–=1 ğ‘¡(ğ‘¥ğ‘–). Since the MLE is equivariant [Kee10],
we have Ë†
âˆ‡ğ¹(ğœƒ)
=
âˆ‡ğ¹( Ì‚ğœƒ), and it follows that
Ì‚ğœƒ= (âˆ‡ğ¹)âˆ—( Ì‚ğœ‚) since (âˆ‡ğ¹)âˆ’1 = âˆ‡ğ¹âˆ—.
The MLE
can be interpreted as a divergence minimization
problem: minğœƒâˆˆÎ˜ ğ·KL[ğ‘ğ‘’âˆ¶ğ‘ğœƒ], where ğ‘ğ‘’(ğ‘¥) =
1
ğ‘›âˆ‘
ğ‘›
ğ‘–=1 ğ›¿ğ‘¥ğ‘–(ğ‘¥) is the empirical distribution with
the ğ›¿ğ‘¥ğ‘–â€™s denoting the Dirac distributions ğ›¿ğ‘¥ğ‘–(ğ‘¥) =
1 iff ğ‘¥= ğ‘¥ğ‘–, and 0 otherwise. The MLE can be
geometrically interpreted as an ğ‘š-projection (i.e.,
with respect to âˆ‡ğ‘š) of ğ‘ğ‘’onto the ğ‘’-flat expo-
nential family: ğ‘Ì‚ğœƒ= Projâˆ‡ğ‘š
â„°(ğ‘ğ‘’). Thus the MLE
Ì‚ğœƒis unique since â„°is ğœƒ-flat.
This result holds
more generally for estimations on curved expo-
nential families ğ’= {ğ‘ğœƒ(ğ‘)} âŠ‚â„°[Ama16]: for ex-
ample, the family of normal distributions ğ‘ğœ‡,1+ğœ‡2
with ğœƒ(ğœ‡) = (ğœ‡, 1 + ğœ‡2) is a 1D curved exponen-
tial family.
By viewing the MLE as a KL diver-
gence minimization problem, we may consider
other divergence-based estimators. A divergence
ğ·yields a ğ·-estimator by asking to solve the min-
imization problem minğœƒâˆˆÎ˜ ğ·[ğ‘ğ‘’âˆ¶ğ‘ğœƒ]. The MLE
is the ğ·KL-estimator.
Then we study the prop-
erties of various ğ·-estimators. For example, the
ğ·ğ›¾-estimator induced by the ğ›¾-divergence ğ·ğ›¾for
ğ›¾> 0 is proven to be robust to noise contamina-
tion [Ama16] but not the MLE which is based on
the KL divergence. The ğ›¾-divergences tend to the
KL divergence in the limit ğ›¾â†’0.
â€¢ The Maximum Entropy (MaxEnt) principle of
E. Jaynes [Kee10] asks for the probability density
ğ‘(ğ‘¥) which maximizes the Shannon entropy un-
der ğ·moment constraints ğ¸ğ‘[ğ‘¡1(ğ‘‹)] = ğ‘š1, ...,
ğ¸ğ‘[ğ‘¡ğ·(ğ‘‹)] = ğ‘šğ·, i.e., ğ¸ğ‘[ğ‘¡(ğ‘¥)] = ğ‘šwith ğ‘¡(ğ‘¥) =
(ğ‘¡1(ğ‘¥), â€¦ , ğ‘¡ğ·(ğ‘¥)) and ğ‘š= (ğ‘š1, â€¦ , ğ‘šğ·). It can be
shown that the MaxEnt distribution ğ‘âˆ—is a den-
sity belonging to an exponential family â„°= {ğ‘ğœƒ}
with sufficient statistics ğ‘¡(ğ‘¥).
Namely, we have
ğ‘âˆ—= ğ‘ğœƒâˆ—
ME with ğœ‚âˆ—
ME â‰”âˆ‡ğ¹(ğœƒâˆ—
ME) = ğ‘š.
The
MaxEnt problem can be rewritten as the following
minimization problem: minğ‘âˆˆâ„³ğ·KL[ğ‘âˆ¶ğ‘¢] =
minğ‘ğ·âˆ—
KL[ğ‘¢âˆ¶ğ‘], where ğ‘¢denotes the uniform
distribution on the probability simplex Î”ğ‘šâˆ’1, and
â„³â‰”{ğ‘âˆˆÎ”ğ‘šâˆ’1
âˆ¶
ğ¸ğ‘[ğ‘¡(ğ‘¥)] = ğ‘š} is an ğ‘š-
flat defined by the moment constraints.
By in-
troducing any other prior density â„(ğ‘¥), we can
thus generalize MaxEnt by the following mini-
mization problem: minğ‘âˆˆâ„³ğ·KL[ğ‘âˆ¶â„] under
the moment constraint ğ¸ğ‘[ğ‘¡(ğ‘¥)] = ğ‘š. The Max-
Ent solution ğ‘âˆ—belongs to an exponential family
â„°â‰”{ğ‘ğœƒ(ğ‘¥) = exp(âˆ‘ğ‘–ğ‘¡ğ‘–(ğ‘¥)ğœƒğ‘–âˆ’ğ¹(ğœƒ))â„(ğ‘¥)}, and we
have ğ‘âˆ—= ğ‘ğœƒâˆ—
ME such that ğœ‚âˆ—
ME â‰”âˆ‡ğ¹(ğœƒâˆ—
ME) = ğ‘š.
We interpret the MaxEnt distribution ğ‘ğœƒâˆ—
ME as the
unique ğ‘’-projection point (with respect to âˆ‡ğ‘’) of
â„onto â„³w.r.t. âˆ‡ğ‘’: ğ‘âˆ—= Projâˆ‡ğ‘’
â„³(â„).
Wong [Won18] recently generalized the Legendre-Fenchel
transformation used in dually flat spaces, and obtained a
new kind of Pythagorean theorem expressed w.r.t. RÂ´enyi
divergences.
Finally, let us mention that instead of using the invari-
ant ğ‘“-divergences of information geometry, we can use
the theory of optimal transport [PC19] to measure the
distance between any two probability measures. Optimal
transport requires defining a ground distance between ele-
ments of the sample space to model the elementary cost of
mass transportation, and measures the deviation between
two probability measures by forward pushing one measure
to another by a transportation plan. Although the optimal
transport problems between discrete probability measures
encountered in practice (i.e., finite weighted point sets) are
computationally costly to solve (amount to solve linear
programs), fast entropic-regularized methods [PC19] and
various heuristics like the sliced Wasserstein distances have
contributed to its huge success in machine learning and
computer vision. Optimal transport does not require the
probability measures to have coinciding supports, and can
even measure the distance between a discrete measure and
42
NOTICES OF THE AMERICAN MATHEMATICAL SOCIETY
VOLUME 69, NUMBER 1

Pythagoras of Samos
(c. 570-495 BC)
Pythagorasâ€™ theorem
Georg F. B. Riemann
(1826â€“1866)
metric tensor (1854)
g = gijdÎ¸i âŠ—dÎ¸j
Riemannian manifold (M, g)
Harold Hotelling
(1895-1973)
Econometrician
Fisher metric
(1930)
Sir Ronald Aylmer Fisher
(1890-1962)
Mathematical statistics
Fisher information, MLE
I(Î¸) = EpÎ¸

(âˆ‡Î¸ log pÎ¸)(âˆ‡Î¸ log pÎ¸)âŠ¤
Euclid
(ca 365-300 BC)
Elements, math. proof
Playfair axiom, Euclidean geometry
a
b
c
c2 = a2 + b2
Nikolai Ivanovich Lobachevsky
(1792-1856)
Hyperbolic geometry
(âˆ-many lines passing through a point and
// to another line)
Christian Felix Klein
(1849-1925)
Projective geometry & symmetry group
Erlangen program
Â´Elie Joseph Cartan
(1869-1951)
aï¬ƒne connections
diï¬€erential forms Ï‰
dx1
dx2
ds2 = gijdxidxj
Sir Harold Jeï¬€reys
(1891-1989)
Jeï¬€reys prior âˆ

|g|
J-divergence
Alexander P. Norden
(1904-1993)
conjugate connections wrt g
Aï¬ƒnely connected spaces
Calyampudi Radhakrishna Rao
(1920-)
Fisher-Rao distance
CramÂ´er-Rao lower bound
(1945)
Claude Elwood Shannon
(1916-2001)
Information theory
Entropy:
h(p) = âˆ’
 p log pdÎ¼
Solomon Kullback
(1907-1994)
Richard A. Leibler
(1914-2003)
KL divergence
DKL[p : q] =
 p log p
qdÎ¼
Ernest Borisovich Vinberg
(1937-2020)
characteristic functions
on homogeneous cones
Imre CsiszÂ´ar
(1938-)
information projections
f-divergences
If[p : q] =
 pf( q
p)dÎ¼
Bradley Efron
(1938-)
statistical curvature
E-connection
Ole E. Barndorï¬€-Nielsen
(1935-)
Exponential families
observed information geometry
Shun-ichi Amari
(1936-)
Information geometry
dualistic structure(M, g, âˆ‡, âˆ‡âˆ—):
Zg(X, Y ) = g (âˆ‡ZX, Y ) + g X, âˆ‡âˆ—
ZY 
dual Â±Î±-connections
(M, gF, âˆ‡âˆ’Î±, âˆ‡Î±)
Steï¬€en Lauritzen
(1947-)
statistical manifold (M, g, C)
Jean-Louis Koszul
(1921-2018)
Hirohiko Shima
homogeneous bounded domains
Hessian manifolds
D(P : Q) + D(Q : R) = D(P : R)
Generalized Pythagorasâ€™ theorem
in dually ï¬‚at space (M, g, âˆ‡, âˆ‡âˆ—)
g
âˆ‡âˆ—
âˆ‡
Q
P
R
Genesis of the Dual Structure of Information Geometry
Information geometry
Springer journal
(2018-)
Johann C. F. Gauss
(1777-1855)
diï¬€erential geometry
of surfaces
Theorema Egregium
Prasanta C. Mahalanobis
(1893-1972)
Distances in statistics
Mahalanobis distance
statistical ï¬eld (1936)
Nikolai Nikolaevich Chentsov
(1930-1992)
statistical invariance
geometrostatistics
Category theory, connections
Figure 3. Genesis of the dual structure of information geometry.
JANUARY 2022
NOTICES OF THE AMERICAN MATHEMATICAL SOCIETY
43

a continuous measure. Many fruitful interactions between
information geometry and optimal transport are investi-
gated [AKO18], and counterpart notions of the FIM and
Bregman divergences have been proposed in the probabil-
ity density space equipped with the ğ¿2-Wasserstein met-
ric [Li21].
To summarize, the problem of geometrically model-
ing a family of probability distributions (the statistical
model) is at the heart of information geometry.
The
Fisher-Rao geometry considers a Riemannian manifold
equipped with the Fisher information metric, and uses the
Riemannian geodesic length as a measure of dissimilarity
between distributions: the Fisher-Rao distance. Amariâ€™s
dual Â±ğ›¼-geometry of information geometry has revealed
the dualistic structure of affine connections coupled to the
Fisher metric. This key dualistic structure is purely geo-
metric and therefore can be used beyond the realm of
statistics (for example, when studying optimization algo-
rithms with convex barrier functions [Ama16]). Dually
flat spaces are Hessian manifolds [Shi07] with a single-
chart atlas where the Legendre-Fenchel transformation
plays an essential role to define dual coordinate systems
and dual potential functions. Dually flat spaces generalize
Euclidean geometry and enjoy a generalized Pythagorean
theorem [Ama16]. Many pioneers have contributed to the
now well-established classical dual structure of informa-
tion geometry: Figure 3 displays historically the main ac-
tors who contributed to the genesis of the dual structure
of information geometry with achieved milestones.
Recent advances in information geometry studies the
geometry of deformed exponential families and their
use in thermostatistics [Nau11], the geometry of non-
parametric models, the quantum information geometry,
the Lie group thermodynamics, and the interactions of
geometric mechanics with information geometry via sym-
plectic and contact structures. Information geometry has
found many applications beyond statistics. We refer to
the textbook [Ama16] for applications in signal processing,
data science, and machine learning. To conclude with an
application in machine learning, consider training a neu-
ral network ğ‘¦= NNğœƒ(ğ‘¥) parameterized by weights ğœƒ. The
neural network is typically trained by using the method of
gradient descent to minimize a loss function
ğ¿(ğœƒ) â‰”1
ğ‘›
ğ‘›
âˆ‘
ğ‘–=1
(ğ‘¦ğ‘–âˆ’NNğœƒ(ğ‘¥ğ‘–))2
defined by a supervised training set of ğ‘›labeled pairs
{(ğ‘¥ğ‘–, ğ‘¦ğ‘–)}, where ğ‘¦ğ‘–denotes the label of ğ‘¥ğ‘–: initialize ğœƒ0
and iteratively update ğœƒğ‘¡+1 â‰”ğœƒğ‘¡âˆ’ğ›½âˆ‡ğ¿(ğœƒğ‘¡), where ğ›½de-
notes the step size.
The ordinary gradient âˆ‡ğœƒğ¿(ğœƒ) de-
pends on the chosen parameterization, i.e., âˆ‡ğœƒğ¿(ğœƒ) â‰ 
âˆ‡ğœ‚ğ¿(ğœƒ(ğœ‚)) for a smooth invertible parameter transforma-
tion ğœ‚= ğœƒ(ğœ‚).
A better parameter-invariant gradient
has been proposed in information geometry for optimiza-
tion on Riemannian manifolds (ğ‘€, ğ‘”): the natural gra-
dient [Ama16]
Ìƒâˆ‡ğœƒğ¿(ğœƒ)
â‰”
[ğ‘”ğ‘–ğ‘—(ğœƒ)]âˆ’1
ğ‘–ğ‘—âˆ‡ğœƒğ¿(ğœƒ).
The nat-
ural gradient ensures that
Ìƒâˆ‡ğœƒğ¿(ğœƒ)
=
Ìƒâˆ‡ğœ‚ğ¿(ğœƒ(ğœ‚)).
The
natural gradient descent is used to train stochastic neu-
ral networks with parameter space modeled as a Fisher-
Rao manifold, called a neuromanifold [Ama16].
Since
2018, an eponymous journal devoted to information
geometry (INGE, https://www.springer.com/journal
/41884) is published by Springer which reports the latest
advances in the field.
References
[Ama16] Shun-ichi Amari, Information
geometry
and
its
applications, Applied Mathematical Sciences, vol. 194,
Springer, [Tokyo], 2016, DOI 10.1007/978-4-431-55978-8.
MR3495836
[Ama21] Shun-ichi Amari, Information geometry, Jpn. J. Math.
16 (2021), no. 1, 1â€“48, DOI 10.1007/s11537-020-1920-5.
MR4206647
[AKO18] Shun-ichi Amari, Ryo Karakida, and Masafumi
Oizumi, Information geometry connecting Wasserstein dis-
tance and Kullback-Leibler divergence via the entropy-relaxed
transportation problem, Inf. Geom. 1 (2018), no. 1, 13â€“37,
DOI 10.1007/s41884-018-0002-8. MR3974671
[AJLS17] Nihat Ay, JÃ¼rgen Jost, HÃ´ng VÃ¢n LÃª, and Lorenz
SchwachhÃ¶fer, Information geometry, Ergebnisse der Mathe-
matik und ihrer Grenzgebiete. 3. Folge. A Series of Modern
Surveys in Mathematics [Results in Mathematics and Re-
lated Areas. 3rd Series. A Series of Modern Surveys in Math-
ematics], vol. 64, Springer, Cham, 2017, DOI 10.1007/978-
3-319-56478-4. MR3701408
[ Ë‡Cen82] Nikolai Nikolaevich Ë‡Cencov, Statistical decision rules
and optimal inference, Translations of Mathematical Mono-
graphs, vol. 53, American Mathematical Soc., 1981.
[Efr75] Bradley Efron, Defining the curvature of a statistical prob-
lem (with applications to second order efficiency), Ann. Statist.
3 (1975), no. 6, 1189â€“1242. MR428531
[Egu83] Shinto Eguchi, Second order efficiency of minimum con-
trast estimators in a curved exponential family, Ann. Statist. 11
(1983), no. 3, 793â€“803. MR707930
[GN14] Leonor Godinho and JosÂ´e NatÂ´ario, An introduction
to Riemannian geometry: With applications to mechanics
and relativity, Universitext, Springer, Cham, 2014, DOI
10.1007/978-3-319-08666-8. MR3289090
[Kee10] Robert W. Keener, Theoretical statistics: Topics for a core
course, Springer Texts in Statistics, Springer, New York, 2010,
DOI 10.1007/978-0-387-93839-4. MR2683126
[Li21] Wuchen Li, Transport information Bregman divergences,
arXiv:2101.01162, 2021.
[Nau11] Jan Naudts, Generalised thermostatistics, Springer-
Verlag London, Ltd., London, 2011, DOI 10.1007/978-0-
85729-355-8. MR2777415
[Nie20] Frank Nielsen, An elementary introduction to informa-
tion geometry, Entropy 22 (2020), no. 10, Paper No. 1100,
61, DOI 10.3390/e22101100. MR4221069
44
NOTICES OF THE AMERICAN MATHEMATICAL SOCIETY
VOLUME 69, NUMBER 1

[PC19] Gabriel PeyrÂ´e and Marco Cuturi, Computational opti-
mal transport: With applications to data science, Foundations
and TrendsÂ® in Machine Learning 11 (2019), no. 5-6, 355â€“
607.
[Shi07] Hirohiko Shima, The geometry of Hessian structures,
World Scientific Publishing Co. Pte. Ltd., Hackensack, NJ,
2007, DOI 10.1142/9789812707536. MR2293045
[Won18] Ting-Kam Leonard Wong, Logarithmic divergences
from optimal transport and RÂ´enyi geometry, Inf. Geom. 1
(2018), no. 1, 39â€“78, DOI 10.1007/s41884-018-0012-6.
MR4010746
Frank Nielsen
Credits
The opening image is courtesy of ermess via Getty.
Figures 1â€“3 are courtesy of the author.
Photo of the author is courtesy of Maryse Beaumont.
JANUARY 2022
NOTICES OF THE AMERICAN MATHEMATICAL SOCIETY
45

