The Many Faces of
Information Geometry
Frank Nielsen
Information geometry [Ama16, AJLS17, Ama21] aims at
unravelling the geometric structures of families of proba-
bility distributions and at studying their uses in informa-
tion sciences. Information sciences is an umbrella term re-
grouping statistics, information theory, signal processing,
machine learning and AI, etc. Information geometry was
born independently from econometrician H. Hotelling
(1930) and statistician C. R. Rao (1945) from the math-
ematical curiosity of considering a parametric family of
probability distributions, called the statistical model, as
a Riemannian manifold equipped with the Fisher metric
tensor [Nie20]. Information geometry tackles problems by
using the concepts of differential geometry (like curvature)
with tensor calculus. In his pioneer work, Rao considered
the Riemannian geodesic distance and geodesic balls on
the manifold to study classification and hypothesis testing
problems in statistics.
Let (𝒳, ℱ, 𝜇) denote a probability space [Kee10] (with
sample space 𝒳, 𝜎-algebra ℱ, and finite positive measure
Frank Nielsen is a senior researcher (Fellow) of Sony Computer Science Labo-
ratories, Inc., Tokyo, Japan. His email address is Frank.Nielsen@acm.org.
Communicated by Notices Associate Editor Richard Levine.
For permission to reprint this article, please contact:
reprint-permission@ams.org.
DOI: https://doi.org/10.1090/noti2403
𝜇, usually chosen as the Lebesgue mesure 𝜇𝐿or the count-
ing measure 𝜇𝑐), and consider a parametric family 𝒫=
{𝑃𝜃∶𝜃∈Θ} of probability distributions, all dominated
by 𝜇. Let 𝑝𝜃(𝑥) ≔
𝑑𝑃𝜃(𝑥)
𝑑𝜇
denote the Radon-Nikodym deriv-
ative, the probability density function of random variable
𝑋∼𝑝𝜃. By definition, the Fisher Riemannian metric 𝑔𝐹
expressed in the 𝜃-coordinate system is the Fisher informa-
tion matrix (FIM) of the random variable 𝑋: [𝑔𝐹]𝜃≔𝐼𝑋(𝜃)
with
𝐼𝑋(𝜃) ≔𝐸𝑝𝜃[𝑠𝜃(𝑥)𝑠𝜃(𝑥)⊤] ,
where 𝑠𝜃(𝑥)
≔
∇𝜃log 𝑝𝜃(𝑥) is called the score func-
tion [Kee10].
The Fisher metric is also referred to as
the Shahshahani metric in mathematical biology.
Be-
cause the FIM is the covariance matrix of the score (since
𝐸𝑝𝜃[𝑠𝜃(𝑥)]
=
0), 𝐼𝑋(𝜃) is necessarily positive semidef-
inite, and positive-definite for regular statistical mod-
els [Ama16]. The FIM is covariant under reparameteriza-
tion: for any smooth invertible mapping 𝜂(𝜃) with invert-
ible Jacobian matrix [
𝜕𝜃𝑗
𝜕𝜂𝑗]
𝑖𝑗
, we have
𝐼𝜂(𝜂) = [𝜕𝜃𝑖
𝜕𝜂𝑗
]
⊤
𝑖𝑗
× 𝐼𝜃(𝜃(𝜂)) × [𝜕𝜃𝑖
𝜕𝜂𝑗
]
𝑖𝑗
.
36
NOTICES OF THE AMERICAN MATHEMATICAL SOCIETY
VOLUME 69, NUMBER 1

The Riemannian Fisher length element induced by the
Fisher metric
d𝑠𝜃= √d𝜃⊤× [𝑔𝐹]𝜃× d𝜃
is invariant under any smooth invertible reparameteriza-
tion: d𝑠𝜃= d𝑠𝜂with d𝑠2
𝜂= d𝜂⊤× [𝑔𝐹]𝜂× d𝜂. Nowadays,
the Riemannian manifold (𝒫, 𝑔𝐹) is commonly called the
Fisher-Rao manifold [Nie20], and its induced Riemannian
geodesic length distance 𝜌𝑔(𝜃1, 𝜃2) is called the Fisher-Rao
distance 𝜌Rao(𝑝𝜃1, 𝑝𝜃2) ≔𝜌𝑔𝐹(𝜃1, 𝜃2) with
𝜌𝑔𝐹(𝜃1, 𝜃2) ≔∫
1
0
d𝑠𝛾(𝑡) d𝑡,
where 𝛾(𝑡) denotes the Riemannian geodesic [GN14] with
the boundary conditions 𝛾(0) = 𝜃1 and 𝛾(1) = 𝜃2. Thus the
Fisher-Rao distance used to evaluate the dissimilarities be-
tween probability distributions is invariant under reparam-
eterization. For example, the Fisher-Rao distance remains
the same whether the family of normal distributions are
parameterized by 𝜆= (𝜇, 𝜎), 𝜆′ = (𝜇, 𝜎2), or 𝜃= (
𝜇
𝜍2 , −
1
2𝜍2 ):
𝜌Rao(𝑝𝜆1, 𝑝𝜆2) = 𝜌Rao(𝑝𝜆′
1, 𝑝𝜆′
2) = 𝜌Rao(𝑝𝜃1, 𝑝𝜃2).
This parameterization invariance property of statistical dis-
tances highlights the power of modeling geometrically sta-
tistical models. The Fisher-Rao manifolds may have differ-
ent constant sectional curvatures 𝜅: for example, the curva-
tures of the Fisher-Rao manifolds of univariate normal dis-
tributions, univariate zero-centered multivariate normal
distributions, and categorical distributions are 𝜅= −
1
2 < 0,
𝜅= 0, and 𝜅=
1
4 > 0, respectively. Information geometry
elucidates the role played by curvature in statistics. Since
any Riemannian manifold of dimension 𝐷can be isometri-
cally embedded in a Euclidean space of dimension at most
𝐷𝐸=
1
2𝐷(𝐷+ 1)(3𝐷+ 11) using Nash’s embedding theo-
rem, we may visualize the Fisher-Rao manifold (𝒫, 𝑔𝐹) as
a 𝐷-dimensional surface of ℝ𝐷𝐸. This extrinsic view of ge-
ometry is helpful to intuitively grasp the notion of tangent
planes 𝑇𝑝𝜃and tangent vectors 𝑣∈𝑇𝑝𝜃at any 𝑝𝜃∈𝒫and
allows one to visualize geodesics on surfaces. However, let
us point out that differential geometry defines intrinsically
these notions [GN14].
Using the Fisher metric can be justified from several the-
oretical viewpoints [Ama16]:
• First, the FIM occurs when locally approximating
the Kullback-Leibler (KL) divergence [Kee10]. In
statistics, estimating densities using the Maximum
Likelihood Estimator (MLE) or the maximum en-
tropy principle under moment constraints (Max-
Ent) can be interpreted as KL divergence mini-
mization problems [Kee10] (to be detailed below).
The KL divergence between densities 𝑝𝜃1 and 𝑝𝜃2
is defined by:
𝐷KL[𝑝𝜃1 ∶𝑝𝜃2] ≔∫
𝒳
𝑝𝜃1(𝑥) log 𝑝𝜃1(𝑥)
𝑝𝜃2(𝑥)d𝜇(𝑥).
The delimiter “:” indicates that the divergence is
oriented: 𝐷KL[𝑝𝜃1 ∶𝑝𝜃2] ≠𝐷KL[𝑝𝜃2 ∶𝑝𝜃1]. The
KL divergence can be expressed as
𝐷KL[𝑝𝜃1 ∶𝑝𝜃2] = ℎ×[𝑝𝜃1 ∶𝑝𝜃2] −ℎ[𝑝𝜃1],
where ℎ×[𝑝𝜃1 ∶𝑝𝜃2] denotes the cross-entropy
ℎ×[𝑝𝜃1
∶
𝑝𝜃2]
≔
−∫𝑝𝜃1 log 𝑝𝜃2d𝜇(𝑥) and
ℎ[𝑝𝜃1]
≔
−∫𝑝𝜃1 log 𝑝𝜃1d𝜇(𝑥) is Shannon en-
tropy. Hence, the KL divergence is also called rel-
ative entropy in information theory. The second-
order Taylor approximation of the KL divergence
yields
𝐷KL[𝑝𝜃∶𝑝𝜃+𝑑𝜃] = 1
2𝑑𝜃⊤× 𝐼𝜃(𝜃) × 𝑑𝜃≈1
2d𝑠2
𝜃.
More generally, the FIM is used in the local ap-
proximations of 𝑓-divergences [Ama16]:
𝐼𝑓[𝑝𝜃∶𝑝𝜃+𝑑𝜃] = 1
2𝑓″(1) 𝑑𝜃⊤× 𝐼𝜃(𝜃) × 𝑑𝜃,
where
𝐼𝑓[𝑝𝜃1 ∶𝑝𝜃2] ≔∫
𝒳
𝑝𝜃1(𝑥)𝑓(
𝑝𝜃2(𝑥)
𝑝𝜃1(𝑥)) d𝜇(𝑥)
for a convex function 𝑓(𝑢) satisfying 𝑓(1) = 0,
and strictly convex at 1. The KL divergence is an
𝑓-divergence obtained for 𝑓(𝑢) = −log(𝑢) with
𝑓″(𝑢) = 1. The 𝑓-divergences are said to be sep-
arable because they can be written as integrals of
scalar divergences:
𝐼𝑓[𝑝∶𝑞] = ∫
𝒳
𝑖𝑓[𝑝(𝑥) ∶𝑞(𝑥)]d𝜇(𝑥)
with 𝑖𝑓[𝑎∶𝑏] ≔𝑎𝑓(𝑏/𝑎). The 𝑓-divergences enjoy
the following monotonicity property:
𝐼𝑓[𝑝𝑌∶𝑞𝑌] ≤𝐼𝑓[𝑝𝑋∶𝑞𝑋],
where 𝑝𝑌and 𝑞𝑌are the densities induced by
a Markov kernel from measurable space (𝒳, ℱ)
to measurable space (𝒴, 𝒢) [Ama21].
To give
a concrete example, consider the 𝑓-divergences
between two (normalized) histograms 𝑝
=
(𝑝1, … , 𝑝2𝑛) and 𝑞= (𝑞1, … , 𝑞2𝑛) with 2𝑛bins
representing multinomial probability laws. Then
𝐼𝑓[𝑝′ ∶𝑞′] ≤𝐼𝑓[𝑝∶𝑞], where 𝑝′ = (𝑝′
1, … , 𝑝′
𝑛)
and 𝑞′ = (𝑞1, … , 𝑞′
𝑛) with 𝑝′
𝑖= 𝑝2(𝑖−1)+1 + 𝑝2𝑖
and 𝑞′
𝑖= 𝑞2(𝑖−1)+1 + 𝑞2𝑖reduced histograms ob-
tained by merging consecutive bins (a very spe-
cial deterministic Markov kernel from measurable
space ([2𝑛], 2[2𝑛]) to measurable space ([𝑛], 2[𝑛])
JANUARY 2022
NOTICES OF THE AMERICAN MATHEMATICAL SOCIETY
37

where [𝑠] ≔{1, … , 𝑠}). The only separable statisti-
cal divergences satisfying this monotonicity prop-
erty are 𝑓-divergences when 𝑛> 2 [Ama16].
• A sufficient statistic [Kee10] 𝑌= 𝑇(𝑋) for the pa-
rameter 𝜃of a random variable 𝑋∼𝑝𝜃is such that
the conditional probability of 𝑋given 𝑌= 𝑇(𝑋)
does not depend on 𝜃. That is, all statistical in-
formation concerning parameter 𝜃is contained in
𝑌= 𝑇(𝑋). For example, 𝑇(𝑋) = (𝑇1(𝑋), 𝑇2(𝑋))
with 𝑇1(𝑋) = ∑
𝑛
𝑖=1 𝑋𝑖and 𝑇2(𝑋) = ∑
𝑛
𝑖=1 𝑋2
𝑖is
a sufficient statistic for the parameter 𝜃= (𝜇, 𝜎)
of a random vector (𝑋1, … , 𝑋𝑛) of 𝑛independent
and identically distributed (i.i.d.) random vari-
ables 𝑋1, … , 𝑋𝑛∼𝑝𝜃following a normal distri-
bution 𝑝𝜇,𝜍. In general, we have 𝐼𝑌(𝜃) ≤𝐼𝑋(𝜃)
with equality if and only if (iff) 𝑌is a sufficient
statistic [Kee10].
Let us observe that for 𝑛i.i.d.
random variables 𝑋𝑖∼𝑝𝜃, we have 𝐼𝑋1,…,𝑋𝑛(𝜃) =
𝑛𝐼𝑋(𝜃) with 𝑋∼𝑝𝜃. Sufficiency also character-
izes the equality in the monotonicity inequality
of 𝑓-divergences: 𝐼𝑓[𝑝𝑌∶𝑞𝑌] ≤𝐼𝑓[𝑝𝑋∶𝑞𝑋] with
equality iff 𝑌= 𝑇(𝑋) is a sufficient statistic.
• Let
̂𝜃𝑛be an unbiased estimator of 𝜃of an i.i.d.
random vector (𝑋1, … , 𝑋𝑛) ∼𝑝𝜃. Then the follow-
ing Cram´er-Rao lower bound (CRLB) on the vari-
ance of
̂𝜃𝑛holds:
Var[ ̂𝜃𝑛] = 𝐸[( ̂𝜃𝑛−𝐸[ ̂𝜃𝑛])
2] ⪰1
𝑛𝐼−1
𝜃(𝜃),
where 𝐴⪰𝐵iff matrix 𝐴−𝐵is positive semidef-
inite. The notation ⪰indicates the comparison
with respect to (w.r.t.) the Loewner partial order-
ing of positive semidefinite matrices. Thus the in-
verse of the FIM provides a lower bound on the ac-
curacy of any unbiased estimator. An estimator is
said to be Fisher efficient when its variance asymp-
totically matches the CRLB when 𝑛→∞.
The Fisher metric is the only invariant metric under Mar-
kovian morphisms of statistical models [Ama16]. How-
ever, let us point out that there are infinitely many coun-
terparts of the FIM in quantum information geometry,
and that other alternative Riemannian information met-
rics can be explored (e.g., the Wasserstein information met-
ric [Li21]).
In statistics, two special types of statistical models,
called the exponential families and the mixture families,
are often handled:
• An exponential family [Kee10] is a set of paramet-
ric densities ℰ≔{𝑝𝜃(𝑥)d𝜇} such that
𝑝𝜃(𝑥) ≔exp (
𝐷
∑
𝑖=1
𝑡𝑖(𝑥)𝜃𝑖−𝐹(𝜃)) ,
where the (𝑡1(𝑥), … , 𝑡𝐷(𝑥)) form the minimal
sufficient statistic. Function 𝐹(𝜃) is used to nor-
malize the densities:
𝐹(𝜃) = log (∫
𝒳
exp (
𝐷
∑
𝑖=1
𝑡𝑖(𝑥)𝜃𝑖) d𝜇(𝑥)) ,
and called the cumulant function (or log-partition
in statistical physics). 𝐹(𝜃) is strictly convex for
(full regular) exponential families [Kee10].
For
example, the family of 𝑑-variate normal distri-
butions is an exponential family of order 𝐷=
𝐷(𝐷+3)
2
w.r.t. the Lebesgue measure 𝜇𝐿, and the
family of Poisson distributions is a discrete expo-
nential family of order 𝐷= 1 w.r.t. the counting
measure 𝜇𝑐. Exponential families have all finite-
dimensional minimal sufficient statistics.
• A mixture family [Nie20] is a set of parametric
densities ℳ
≔{𝑚𝜃(𝑥)d𝜇} such that 𝑚𝜃(𝑥) =
∑
𝐷
𝑖=1 𝜃𝑖𝑝𝑖(𝑥)+(1−∑
𝐷
𝑖=1 𝜃𝑖)𝑝0(𝑥), where functions
1, 𝑝0(𝑥), 𝑝1(𝑥), … , 𝑝𝐷(𝑥) are linearly independent
functions. Statistical mixtures such as Gaussian
mixture models with prescribed component den-
sities are examples of mixture families. Mixture
families are closed under convex combinations. It
can be shown that the negentropy of mixture den-
sities is a strictly convex function [Nie20]: 𝐹(𝜃) =
−ℎ[𝑚𝜃].
For these two types of statistical models, the Fisher
metric is a Hessian metric [Shi07] since the FIM is the
Hessian of some strictly convex potential function 𝐹(𝜃):
𝐼(𝜃) = ∇2𝐹(𝜃). This is easily checked for exponential fami-
lies as the FIM can be written under mild regularity condi-
tions [Ama16] as 𝐼𝑋(𝜃) = −𝐸𝑝𝜃[∇2 log 𝑝𝜃(𝑥)].
In general, calculating in closed-form the Fisher-Rao
distances may be difficult since it requires to solve the
Riemannian geodesic equation with boundary conditions,
and to integrate the length elements along geodesics. For
example, although the Fisher-Rao distance between uni-
variate normal distributions is available in closed-form, we
do not have a closed-form formula for the Fisher-Rao dis-
tance between multivariate normals [Nie20]. Thus in prac-
tice, the Fisher-Rao between multivariate normals is nu-
merically approximated. We shall now explain that the
Fisher-Rao manifolds with Hessian metrics carry another
beautiful geometric dual structure which is well suited
for computation in applications: namely, these exponen-
tial/mixture families can be modeled as Hessian mani-
folds [Shi07], and are commonly called dually flat spaces
in information geometry [Ama21].
In the second half of the 20th century, information
geometry gained a momentum with the pathbreaking
work of N. Chentsov.
Chentsov shared statistician A.
Wald’s viewpoint that all problems in statistics can be
38
NOTICES OF THE AMERICAN MATHEMATICAL SOCIETY
VOLUME 69, NUMBER 1

viewed as decision problems, and therefore investigated
a theoretical framework for characterizing optimal deci-
sion rules in statistics using category theory and Markov-
ian morphisms [ ˇCen82]. A family of probability distri-
butions should be invariant both under smooth one-to-
one transformations of its parameter 𝜃and under trans-
formations of the corresponding random variables by
sufficient statistics.
This precisely defines the statistical
invariance.
Chentsov’s breakthrough consisted in sepa-
rating the metric tensor 𝑔(used to measure angles be-
tween vectors and lengths of vectors in a tangent plane)
from its induced Levi-Civita connection 𝑔∇used by de-
fault on a Riemannian manifold for obtaining (locally)
length minimizing geodesics. This novel insight allowed
Chentsov to model statistical models as differentiable
manifolds equipped with affine connections ∇more gen-
eral than the Levi-Civita connection of Fisher-Rao mani-
folds. More precisely, Chentsov discovered the existence of
a unique totally symmetric third-order tensor fulfilling the
statistical invariance which is nowadays called the Amari-
Chentsov tensor or skewness tensor, and used that ten-
sor to build invariant affine connections. A. Kolmogorov
called Chentsov’s field of research “geometrostatistics” in
Russian (translated as “geometrical statistics” in the eng-
lish monograph [ ˇCen82]).
In short, an affine connection ∇[GN14] defines the fol-
lowing:
• A way to differentiate a vector field 𝑋on a man-
ifold (or more generally a tensor field 𝑇) by an-
other vector field 𝑌: namely, the covariant deriva-
tives denoted by ∇𝑌𝑋(or ∇𝑌𝑇).
• The parallel transport ∏
∇
𝑐(𝑡) 𝑣for a tangent vector
𝑣∈𝑇𝑐(0) along any smooth curve 𝑐(𝑡). An affine
connection allows to parallel transport vectors of
different tangent planes onto a common tangent
plane in order to measure their subtended angles
using the metric tensor in that common tangent
plane.
• ∇-geodesics 𝛾defined as autoparallel curves:
∇̇𝛾̇𝛾= 0. Geodesics 𝛾∇(𝑡) are calculated by solving
the second-order non-linear ordinary differential
equation (ODE):
̈𝜃𝑖+
𝑛
∑
𝑗,𝑘=1
Γ𝑖
𝑗𝑘̇𝜃𝑗̇𝜃𝑘= 0,
𝑖∈{1, … , 𝐷},
where 𝜃= (𝜃1, … , 𝜃𝐷),
̇𝜃𝑙≔
𝑑
d𝑡𝜃𝑙, and Γ𝑖
𝑗𝑘are
the Christoffel symbols [GN14] (𝐷3 smooth func-
tions) defining the affine connection. In physics,
geodesics represent free particle trajectories.
The curvature tensor 𝑅∇and the torsion tensor 𝑇∇of
a manifold (𝑀, 𝑔, ∇) are induced by the chosen connec-
tion ∇[GN14]. The fundamental theorem of Riemannian
γ
gF ∇
θ1θ2 (t)
Pθ1
Pθ2
ρRao(Pθ1, Pθ2) = ρgF(θ1, θ2)
Pθ1
Pθ2
∇α=(∇−α)∗
∇−α=(∇α)∗
Fisher-Rao geometry
→Fisher-Rao geodesic distance
Dual α-geometry
→No default divergence
versus
g∇= ∇α+∇−α
2
γ∇−α
θ1θ2 (t)
γ∇α
θ1θ2(t)
Figure 1. Fisher-Rao geometry vs. dual 𝛼-geometry.
geometry [GN14] states that there exists a unique torsion-
free affine connection which preserves the metric, meaning
that for any two vectors 𝑣1 and 𝑣2 of the tangent plane 𝑇𝑝,
and a smooth curve 𝑐(𝑡) with 𝑐(0) = 𝑝, we have for any
𝑡: 𝑔(𝑣1, 𝑣2) = 𝑔(∏
∇
𝑐(𝑡) 𝑣1, ∏
∇
𝑐(𝑡) 𝑣2). This unique torsion-
free affine connection is called the Levi-Civita metric con-
nection. Historically, affine connections were studied by
´E. Cartan in the 1920s, and used in the Einstein-Cartan
theory of gravity. Chentsov considered regular exponen-
tial families in his monograph, and by considering their in-
variance, discovered the so-called exponential connection
∇𝑒.
The field of information geometry was shaped by
S.-i. Amari who dreamt of a mathematical theory of
neuroscience.
Amari pioneered the dualistic statisti-
cal structures of information geometry: that is, Amari
showed that given any torsion-free affine connection ∇,
there exists a dual torsion-free affine connection ∇∗such
that the mid-connection
∇+∇∗
2
corresponds to the Levi-
Civita connection.
This duality ensures that the pri-
mal and dual parallel transports are metric compatible:
𝑔(𝑣1, 𝑣2) = 𝑔(∏
∇
𝑐(𝑡) 𝑣1, ∏
∇∗
𝑐(𝑡) 𝑣2). Notice that the lengths
of dually parallel-transported vectors by ∇and ∇∗may
JANUARY 2022
NOTICES OF THE AMERICAN MATHEMATICAL SOCIETY
39

vary along 𝑐(𝑡) but their inner product is kept constant. For
parametric statistical models 𝒫= {𝑝𝜃}, Amari reported the
𝛼-geometry (for 𝛼∈ℝ) which is a manifold equipped with
the Fisher metric and a pair of dual connections (∇−𝛼, ∇𝛼)
coupled to the Fisher metric 𝑔𝐹:
∇−𝛼+∇𝛼
2
= ∇0 = 𝑔𝐹∇.
Amari’s 𝛼-connections ∇𝛼are defined by their Christoffel
symbols Γ𝛼
𝑘𝑖,𝑗:
Γ𝛼
𝑘𝑖,𝑗(𝜃) = 𝐸𝑝𝜃[(𝜕𝑘𝜕𝑖𝑙𝜃(𝑥) + 1 −𝛼
2
𝜕𝑘𝑙𝜃(𝑥)𝜕𝑖𝑙𝜃(𝑥)) 𝜕𝑗𝑙𝜃(𝑥)] ,
where 𝑙𝜃(𝑥) ≔log 𝑝𝜃(𝑥) is the log-likelihood function and
𝜕𝑠≔
𝜕
𝜕𝜃𝑠. Chentsov’s exponential connection ∇𝑒corre-
sponds to Amari’s ∇1 connection. The 𝑒-connection was
also studied by Efron who defined geometrically the no-
tion of statistical curvature to study the higher-order as-
ymptotic theory of statistical estimators in a landmark pa-
per [Efr75] which has been recognized as one of the first
successful applications of differential geometry to statisti-
cal inference. The dual connection of ∇𝑒is ∇𝑚≔(∇𝑒)∗=
∇−1, and called the mixture connection.
This connec-
tion was proposed by P. Dawid, a discussant of Efron’s
paper [Efr75]. Thus the Fisher-Rao geometry can be in-
terpreted as the 0-geometry enhanced with the Fisher-Rao
geodesic distance (Figure 1). The Riemannian geodesics
are 𝑔∇-autoparallel and have the property to locally min-
imize the geodesic lengths [GN14].
In general, the 𝛼-
geometry is not associated with any statistical divergence
when 𝛼≠0. But the 𝛼-geometry may be recovered from
the divergence geometry of invariant 𝑓-divergences on the
probability simplex [Egu83] (to be detailed below).
A connection ∇is said to be flat [GN14,Shi07] when it
has zero torsion and when there exists a local coordinate
system 𝜃such that the Christoffel symbols Γ𝑘
𝑖𝑗defining ∇
expressed in that coordinate system vanish: Γ𝑘
𝑖𝑗(𝜃) = 0. The
coordinate system 𝜃is called a ∇-affine coordinate system.
In general the parallel transport of 𝑣∈𝑇𝑝to 𝑇𝑞is curve
dependent: ∏
∇
𝑐1(𝑡) 𝑣≠∏
∇
𝑐2(𝑡) 𝑣for smooth curves 𝑐1 and
𝑐2 with endpoints 𝑐1(0) = 𝑐2(0) = 𝑝and 𝑐1(1) = 𝑐2(1) = 𝑞.
One can visualize locally the presence of curvature of a
connection or not at a point 𝑝by considering the paral-
lel transport of a vector 𝑣along a closed infinitesimal loop
𝑙encircling 𝑝(with 𝑙(0) = 𝑙(1)): if there is an angle defi-
ciency between 𝑣and ∏
∇
𝑙(1) 𝑣, then the manifold has non-
zero curvature at 𝑝[Nie20]. However, the parallel trans-
port is independent of the curves linking the point 𝑝to
the point 𝑞for flat connections. It is a fundamental result
of information geometry that if (𝑀, 𝑔, ∇) is flat, then so is
(𝑀, 𝑔, ∇∗) with ∇∗= 2𝑔∇−∇. We get the so-called dually
flat spaces of information geometry [Ama16] (𝑀, 𝑔, ∇, ∇∗)
which are special Hessian manifolds [Shi07] admitting a
single chart atlas. Notice that in a dually flat space, the
Levi-Civita connection is usually not flat.
In information theory, a statistical divergence like the
KL divergence is loosely speaking a potentially asymmet-
ric dissimilarity measure between probability distributions
which may fail the triangle inequality of metric distances.
In information geometry, a divergence (historically called
a contrast function [Egu83]) is a smooth dissimilarity mea-
sure 𝐷(𝜃∶𝜃′) between parameters 𝜃and 𝜃′ that satisfies
the following conditions:
1.
𝐷(𝜃∶𝜃′) ≥0 for all 𝜃, 𝜃′ with equality iff 𝜃= 𝜃′.
2.
𝜕𝑖𝐷(𝜃∶𝜃′)|𝜃′=𝜃= 𝜕′
𝑗𝐷(𝜃∶𝜃′)|𝜃′=𝜃= 0 for all 𝑖, 𝑗,
where 𝜕𝑙≔
𝜕
𝜕𝜃𝑙and 𝜕′
𝑙≔
𝜕
𝜕𝜃′
𝑙
.
3.
−[𝜕𝑖𝜕′
𝑗𝐷(𝜃∶𝜃′)|𝜃′=𝜃]𝑖𝑗is a positive-definite matrix.
The (parameter) divergence 𝐷(𝜃
∶
𝜃′) can also be
interpreted as a function on the manifold defined by
the single chart equipped with the 𝜃-coordinate system.
Eguchi [Egu83] reported a method to build a dualistic
structure (𝑀, 𝑔, ∇, ∇∗) from any divergence 𝐷(⋅∶⋅) as fol-
lows:
𝑔𝑖𝑗(𝜃)
=
−𝜕𝑖𝜕′
𝑗𝐷(𝜃∶𝜃′)|𝜃′=𝜃,
Γ𝑖𝑗,𝑘(𝜃)
=
−𝜕𝑖𝜕𝑗𝜕′
𝑘𝐷(𝜃∶𝜃′)|𝜃′=𝜃,
Γ∗
𝑖𝑗,𝑘(𝜃)
=
−𝜕𝑘𝜕′
𝑖𝜕′
𝑗𝐷(𝜃∶𝜃′)|𝜃′=𝜃.
It can be shown that the connections ∇and ∇∗induced re-
spectively by Γ𝑖𝑗,𝑘and Γ∗
𝑖𝑗,𝑘are torsion-free and dual. This
geometry is called the divergence geometry of 𝐷[Ama16].
Let 𝐷∗(𝜃∶𝜃′) ≔𝐷(𝜃′ ∶𝜃) denote the dual or reverse di-
vergence, and (𝑀, 𝐷𝑔, 𝐷∇, 𝐷∇∗) the information-geometric
space induced by 𝐷. Then we have 𝐷∗∇= 𝐷∇∗and 𝐷∗∇∗=
𝐷∇. Thus symmetric divergences 𝐷(𝜃∶𝜃′) = 𝐷(𝜃′ ∶𝜃)
yield self-dual connections coinciding with the Levi-Civita
connection.
Many different divergences may yield the
same divergence geometry. The divergence geometry of 𝑓-
divergences on the 𝐷-dimensional probability simplex cor-
responds to Amari’s 𝛼-geometry for 𝛼= 3+2
𝑓‴(1)
𝑓″(1) , and the
divergence geometry of 𝐷Rao(𝜃1 ∶𝜃2) ≔
1
2𝜌2
Rao(𝑝𝜃1, 𝑝𝜃2)
yields the 0-geometry.
In a dually flat space, we can build a canonically
Fenchel-Young (non-metric) divergence 𝐴(𝜃1 ∶𝜂2):
𝐴(𝜃1 ∶𝜂2) ≔𝐹(𝜃1) + 𝐹∗(𝜂2) −𝜃⊤
1 𝜂2,
where 𝐹∗(𝜂) denotes the convex conjugate obtained by the
Legende-Fenchel transform:
𝐹∗(𝜂) = sup
𝜃
{𝜃⊤𝜂−𝐹(𝜃)}.
Legende-Fenchel transform yields a dual coordinate sys-
tem 𝜂= ∇𝐹(𝜃), and the Fenchel-Young inequality 𝐹(𝜃1) +
𝐹∗(𝜂2) ≥𝜃⊤
1 𝜂2 ensures that 𝐴(𝜃1 ∶𝜂2) ≥0 with equality iff
𝜂2 = ∇𝐹(𝜃1). This divergence is shown to be equivalent to
a Bregman divergence [Ama16]: 𝐴(𝜃1 ∶𝜂2) = 𝐵𝐹(𝜃1 ∶𝜃2)
40
NOTICES OF THE AMERICAN MATHEMATICAL SOCIETY
VOLUME 69, NUMBER 1

P1
P2
Dual geodesic γ∇∗
Primal geodesic γ∇
g∇= ∇+∇∗
2
∇-aﬃne coordinate system θ
∇∗-aﬃne coordinate system η
θ1 = θ(P1)
η1 = η(P1)
η2 = η(P2)
θ2 = θ(P2)
Potential function F(θ)
Dual potential function F ∗(η)
η = ∇F(θ)
θ = ∇F ∗(η)
Legendre-Fenchel transform
Riemannian geodesic γ
g∇
manifold P
Figure 2. Dually ﬂat space (𝑀, 𝑔, ∇, ∇∗) with ∇-afﬁne coordinate system 𝜃and dual ∇∗-afﬁne coordinate system 𝜂. Primal
geodesics 𝛾∇and dual geodesics 𝛾∇∗are linear when plotted in the 𝜃-coordinate system and 𝜂-coordinate system, respectively.
with 𝜃2 = ∇𝐹∗(𝜂2), where
𝐵𝐹(𝜃1 ∶𝜃2) ≔𝐹(𝜃1) −𝐹(𝜃2) −(𝜃1 −𝜃2)⊤∇𝐹(𝜃2)
is the Bregman divergence 𝐵𝐹(𝜃1 ∶𝜃2) between parame-
ters 𝜃1 and 𝜃2 induced by a smooth strictly convex function
𝐹(𝜃) and ∇𝐹(𝜃) ≔[
𝜕𝐹(𝜃)
𝜕𝜃𝑖]
⊤
𝑖
denotes the gradient of 𝐹(𝜃).
Bregman divergences are widely used in machine learning
and originated from mathematical programming. Recip-
rocally, given a Bregman divergence, we can build a dually
flat space (the Bregman divergence geometry) with Hes-
sian metric [Shi07] [𝑔𝑖𝑗(𝜃)]𝑖𝑗= ∇2𝐹(𝜃) (positive-definite
because 𝐹is strictly convex) from its divergence geometry.
The dual affine connections are ∇and ∇∗with Christoffel
symbols Γ𝑖𝑗𝑘(𝜃) = 0 and Γ∗𝑖𝑗𝑘(𝜂) = 0, respectively. It fol-
lows that in the 𝜃-coordinate system, the primal geodesics
𝛾∇are linear since the ∇-geodesic ODE simplifies to
̈𝜃𝑖= 0,
and in the dual 𝜂-coordinate system, the dual geodesics
are linear since the ∇∗-geodesic ODE simplifies to
̈𝜂𝑖= 0
(Figure 2). The 𝜂-coordinate system is said to be ∇∗-affine.
We have [𝑔𝑖𝑗(𝜃)]𝑖𝑗= ∇𝜃∇𝜃𝐹(𝜃) = ∇𝜃𝜂and the dual Rie-
mannian metric [𝑔𝑖𝑗(𝜂)]𝑖𝑗= ∇𝜂∇𝜂𝐹∗(𝜂) = ∇𝜂𝜃. It fol-
lows that [𝑔𝑖𝑗(𝜃)]𝑖𝑗[𝑔𝑖𝑗(𝜂)]𝑖𝑗= 𝐼𝐷×𝐷, the identity matrix.
The Bregman divergence construction from a dually flat
space is defined up to affine dual coordinate transforma-
tions 𝜃′ = 𝐴𝜃+ 𝑏and 𝜂′ = 𝐴−1𝜂+ 𝑐.
Amari’s ±1-geometry for the exponential and mixture
families yields dually flat spaces.
Their corresponding
Bregman divergences yield the following statistical diver-
gences:
• For an exponential family [Kee10] with density
𝑝𝜃(𝑥) = exp(∑
𝐷
𝑖=1 𝑡𝑖(𝑥)𝜃𝑖−𝐹(𝜃)), the Legendre-
Fenchel conjugate function of the cumulant func-
tion 𝐹(𝜃) corresponds to Shannon negentropy,
𝐹∗(𝜂) = −ℎ[𝑃𝜃], and the Bregman divergence
𝐵𝐹(𝜃1 ∶𝜃2) yields the reverse Kullback-Leibler di-
vergence 𝐷∗
KL (or reverse relative entropy):
𝐵𝐹(𝜃1 ∶𝜃2) = 𝐷∗
KL[𝑝𝜃1 ∶𝑝𝜃2] = 𝐷KL[𝑝𝜃2 ∶𝑝𝜃1].
• For a mixture family [Ama16,Nie20] ℳwith den-
sity 𝑚𝜃= ∑
𝐷
𝑖=1 𝜃𝑖𝑝𝑖(𝑥) + (1 −∑
𝐷
𝑖=1 𝜃𝑖)𝑝0(𝑥), the
Bregman generator 𝐹(𝜃) = −ℎ[𝑚𝜃] is strictly con-
vex with Θ = Δ∘
𝐷, the open 𝐷-dimensional proba-
bility simplex. The canonical Bregman divergence
amounts to calculating the Kullback-Leibler diver-
gence [Nie20]: 𝐵𝐹(𝜃1 ∶𝜃2) = 𝐷KL[𝑚𝜃1 ∶𝑚𝜃2].
In a dually flat space (𝑀, 𝑔, ∇, ∇∗), a generalized
Pythagorean theorem holds: Let 𝑃, 𝑄, 𝑅be three points. A
primal geodesic 𝛾𝑃𝑄intersects a dual geodesic 𝛾∗
𝑄𝑅orthogo-
nally w.r.t. to the metric 𝑔at point 𝑄, written as 𝛾𝑃𝑄⟂𝑔𝛾∗
𝑄𝑅,
iff
(𝜃(𝑃) −𝜃(𝑄))⊤× (𝜂(𝑅) −𝜂(𝑄)) = 0.
In that case, the following Pythagorean equality holds:
𝐵𝐹(𝜃(𝑃) ∶𝜃(𝑄)) + 𝐵𝐹(𝜃(𝑄) ∶𝜃(𝑅)) = 𝐵𝐹(𝜃(𝑃) ∶𝜃(𝑅)).
When 𝐹(𝜃) =
1
2𝜃⊤𝜃, we recover the usual Euclidean geome-
try (a self-dual flat space) with 𝜃= 𝜂(since 𝜂= ∇𝐹(𝜃) = 𝜃
and 𝜃= ∇𝐹∗(𝜂) = 𝜂with 𝐹∗(𝜂) =
1
2𝜂⊤𝜂), and we have
JANUARY 2022
NOTICES OF THE AMERICAN MATHEMATICAL SOCIETY
41

𝐵𝐹(𝜃1 ∶𝜃2) =
1
2‖𝜃1 −𝜃2‖2
2, where ‖ ⋅‖2 denotes the Eu-
clidean norm. The canonical ∇-divergence is 𝐷∇(𝑃∶𝑄) =
𝐵𝐹(𝜃(𝑃) ∶𝜃(𝑄)) and the dual ∇∗-divergence is
𝐷∇∗(𝑃∶𝑄) = 𝐵𝐹∗(𝜂(𝑃) ∶𝜂(𝑄))
= 𝐵𝐹(𝜃(𝑄) ∶𝜃(𝑃)) = 𝐷∇(𝑄∶𝑃).
A 𝜃-flat is a submanifold 𝑀′ ⊂𝑀such that 𝜃(𝑀′) ≔
{𝜃(𝑃)
∶
𝑃∈𝑀′} is an affine subspace of Θ (𝑀′ is a ∇-
autoparallel submanifold). Similarly, an 𝜂-flat is a sub-
manifold 𝑀″ such that 𝜂(𝑀″) ≔{𝜂(𝑃)
∶
𝑃∈𝑀″} is
an affine subspace of 𝐻≔{∇𝐹(𝜃) ∶𝜃∈𝜃}. Define the
∇-projection of a point 𝑃∈𝑀onto a submanifold 𝑀′ as
Proj∇
𝑀′(𝑃) ≔{𝑄∈𝑀′ ∶𝛾∇
𝑃𝑄⟂𝑔𝑀′}.
Then the ∇-projection of 𝑃is guaranteed to be a unique
point when 𝑀′ is an 𝜂-flat. Moreover, we have Proj∇
𝑀′(𝑃) =
arg min𝑄∈𝑀′ 𝐷∇(𝑄∶𝑃) in a dually flat space.
These information projections can be used in statisti-
cal inference as follows. Consider the probability space
(𝒳, 2𝒳, 𝜇𝑐) with finite discrete sample space 𝒳= {1, … , 𝑚}
and 𝜇𝑐the counting measure.
The categorical distribu-
tions form both an exponential family and a mixture fam-
ily [Ama16]. A categorical probability mass function can
be viewed as a point lying on the (𝑚−1)-dimensional open
standard simplex Δ𝑚−1.
• The Maximum Likelihood Estimator (MLE) of 𝑛
i.i.d. observations 𝑥1, … , 𝑥𝑛sampled from an ex-
ponential family density 𝑝𝜃∈ℰis
̂𝜂= ˆ
∇𝐹(𝜃) =
1
𝑛∑
𝑛
𝑖=1 𝑡(𝑥𝑖). Since the MLE is equivariant [Kee10],
we have ˆ
∇𝐹(𝜃)
=
∇𝐹( ̂𝜃), and it follows that
̂𝜃= (∇𝐹)∗( ̂𝜂) since (∇𝐹)−1 = ∇𝐹∗.
The MLE
can be interpreted as a divergence minimization
problem: min𝜃∈Θ 𝐷KL[𝑝𝑒∶𝑝𝜃], where 𝑝𝑒(𝑥) =
1
𝑛∑
𝑛
𝑖=1 𝛿𝑥𝑖(𝑥) is the empirical distribution with
the 𝛿𝑥𝑖’s denoting the Dirac distributions 𝛿𝑥𝑖(𝑥) =
1 iff 𝑥= 𝑥𝑖, and 0 otherwise. The MLE can be
geometrically interpreted as an 𝑚-projection (i.e.,
with respect to ∇𝑚) of 𝑝𝑒onto the 𝑒-flat expo-
nential family: 𝑝̂𝜃= Proj∇𝑚
ℰ(𝑝𝑒). Thus the MLE
̂𝜃is unique since ℰis 𝜃-flat.
This result holds
more generally for estimations on curved expo-
nential families 𝒞= {𝑝𝜃(𝑐)} ⊂ℰ[Ama16]: for ex-
ample, the family of normal distributions 𝑝𝜇,1+𝜇2
with 𝜃(𝜇) = (𝜇, 1 + 𝜇2) is a 1D curved exponen-
tial family.
By viewing the MLE as a KL diver-
gence minimization problem, we may consider
other divergence-based estimators. A divergence
𝐷yields a 𝐷-estimator by asking to solve the min-
imization problem min𝜃∈Θ 𝐷[𝑝𝑒∶𝑝𝜃]. The MLE
is the 𝐷KL-estimator.
Then we study the prop-
erties of various 𝐷-estimators. For example, the
𝐷𝛾-estimator induced by the 𝛾-divergence 𝐷𝛾for
𝛾> 0 is proven to be robust to noise contamina-
tion [Ama16] but not the MLE which is based on
the KL divergence. The 𝛾-divergences tend to the
KL divergence in the limit 𝛾→0.
• The Maximum Entropy (MaxEnt) principle of
E. Jaynes [Kee10] asks for the probability density
𝑝(𝑥) which maximizes the Shannon entropy un-
der 𝐷moment constraints 𝐸𝑝[𝑡1(𝑋)] = 𝑚1, ...,
𝐸𝑝[𝑡𝐷(𝑋)] = 𝑚𝐷, i.e., 𝐸𝑝[𝑡(𝑥)] = 𝑚with 𝑡(𝑥) =
(𝑡1(𝑥), … , 𝑡𝐷(𝑥)) and 𝑚= (𝑚1, … , 𝑚𝐷). It can be
shown that the MaxEnt distribution 𝑝∗is a den-
sity belonging to an exponential family ℰ= {𝑝𝜃}
with sufficient statistics 𝑡(𝑥).
Namely, we have
𝑝∗= 𝑝𝜃∗
ME with 𝜂∗
ME ≔∇𝐹(𝜃∗
ME) = 𝑚.
The
MaxEnt problem can be rewritten as the following
minimization problem: min𝑝∈ℳ𝐷KL[𝑝∶𝑢] =
min𝑝𝐷∗
KL[𝑢∶𝑝], where 𝑢denotes the uniform
distribution on the probability simplex Δ𝑚−1, and
ℳ≔{𝑝∈Δ𝑚−1
∶
𝐸𝑝[𝑡(𝑥)] = 𝑚} is an 𝑚-
flat defined by the moment constraints.
By in-
troducing any other prior density ℎ(𝑥), we can
thus generalize MaxEnt by the following mini-
mization problem: min𝑝∈ℳ𝐷KL[𝑝∶ℎ] under
the moment constraint 𝐸𝑝[𝑡(𝑥)] = 𝑚. The Max-
Ent solution 𝑝∗belongs to an exponential family
ℰ≔{𝑝𝜃(𝑥) = exp(∑𝑖𝑡𝑖(𝑥)𝜃𝑖−𝐹(𝜃))ℎ(𝑥)}, and we
have 𝑝∗= 𝑝𝜃∗
ME such that 𝜂∗
ME ≔∇𝐹(𝜃∗
ME) = 𝑚.
We interpret the MaxEnt distribution 𝑝𝜃∗
ME as the
unique 𝑒-projection point (with respect to ∇𝑒) of
ℎonto ℳw.r.t. ∇𝑒: 𝑝∗= Proj∇𝑒
ℳ(ℎ).
Wong [Won18] recently generalized the Legendre-Fenchel
transformation used in dually flat spaces, and obtained a
new kind of Pythagorean theorem expressed w.r.t. R´enyi
divergences.
Finally, let us mention that instead of using the invari-
ant 𝑓-divergences of information geometry, we can use
the theory of optimal transport [PC19] to measure the
distance between any two probability measures. Optimal
transport requires defining a ground distance between ele-
ments of the sample space to model the elementary cost of
mass transportation, and measures the deviation between
two probability measures by forward pushing one measure
to another by a transportation plan. Although the optimal
transport problems between discrete probability measures
encountered in practice (i.e., finite weighted point sets) are
computationally costly to solve (amount to solve linear
programs), fast entropic-regularized methods [PC19] and
various heuristics like the sliced Wasserstein distances have
contributed to its huge success in machine learning and
computer vision. Optimal transport does not require the
probability measures to have coinciding supports, and can
even measure the distance between a discrete measure and
42
NOTICES OF THE AMERICAN MATHEMATICAL SOCIETY
VOLUME 69, NUMBER 1

Pythagoras of Samos
(c. 570-495 BC)
Pythagoras’ theorem
Georg F. B. Riemann
(1826–1866)
metric tensor (1854)
g = gijdθi ⊗dθj
Riemannian manifold (M, g)
Harold Hotelling
(1895-1973)
Econometrician
Fisher metric
(1930)
Sir Ronald Aylmer Fisher
(1890-1962)
Mathematical statistics
Fisher information, MLE
I(θ) = Epθ

(∇θ log pθ)(∇θ log pθ)⊤
Euclid
(ca 365-300 BC)
Elements, math. proof
Playfair axiom, Euclidean geometry
a
b
c
c2 = a2 + b2
Nikolai Ivanovich Lobachevsky
(1792-1856)
Hyperbolic geometry
(∞-many lines passing through a point and
// to another line)
Christian Felix Klein
(1849-1925)
Projective geometry & symmetry group
Erlangen program
´Elie Joseph Cartan
(1869-1951)
aﬃne connections
diﬀerential forms ω
dx1
dx2
ds2 = gijdxidxj
Sir Harold Jeﬀreys
(1891-1989)
Jeﬀreys prior ∝

|g|
J-divergence
Alexander P. Norden
(1904-1993)
conjugate connections wrt g
Aﬃnely connected spaces
Calyampudi Radhakrishna Rao
(1920-)
Fisher-Rao distance
Cram´er-Rao lower bound
(1945)
Claude Elwood Shannon
(1916-2001)
Information theory
Entropy:
h(p) = −
 p log pdμ
Solomon Kullback
(1907-1994)
Richard A. Leibler
(1914-2003)
KL divergence
DKL[p : q] =
 p log p
qdμ
Ernest Borisovich Vinberg
(1937-2020)
characteristic functions
on homogeneous cones
Imre Csisz´ar
(1938-)
information projections
f-divergences
If[p : q] =
 pf( q
p)dμ
Bradley Efron
(1938-)
statistical curvature
E-connection
Ole E. Barndorﬀ-Nielsen
(1935-)
Exponential families
observed information geometry
Shun-ichi Amari
(1936-)
Information geometry
dualistic structure(M, g, ∇, ∇∗):
Zg(X, Y ) = g (∇ZX, Y ) + g X, ∇∗
ZY 
dual ±α-connections
(M, gF, ∇−α, ∇α)
Steﬀen Lauritzen
(1947-)
statistical manifold (M, g, C)
Jean-Louis Koszul
(1921-2018)
Hirohiko Shima
homogeneous bounded domains
Hessian manifolds
D(P : Q) + D(Q : R) = D(P : R)
Generalized Pythagoras’ theorem
in dually ﬂat space (M, g, ∇, ∇∗)
g
∇∗
∇
Q
P
R
Genesis of the Dual Structure of Information Geometry
Information geometry
Springer journal
(2018-)
Johann C. F. Gauss
(1777-1855)
diﬀerential geometry
of surfaces
Theorema Egregium
Prasanta C. Mahalanobis
(1893-1972)
Distances in statistics
Mahalanobis distance
statistical ﬁeld (1936)
Nikolai Nikolaevich Chentsov
(1930-1992)
statistical invariance
geometrostatistics
Category theory, connections
Figure 3. Genesis of the dual structure of information geometry.
JANUARY 2022
NOTICES OF THE AMERICAN MATHEMATICAL SOCIETY
43

a continuous measure. Many fruitful interactions between
information geometry and optimal transport are investi-
gated [AKO18], and counterpart notions of the FIM and
Bregman divergences have been proposed in the probabil-
ity density space equipped with the 𝐿2-Wasserstein met-
ric [Li21].
To summarize, the problem of geometrically model-
ing a family of probability distributions (the statistical
model) is at the heart of information geometry.
The
Fisher-Rao geometry considers a Riemannian manifold
equipped with the Fisher information metric, and uses the
Riemannian geodesic length as a measure of dissimilarity
between distributions: the Fisher-Rao distance. Amari’s
dual ±𝛼-geometry of information geometry has revealed
the dualistic structure of affine connections coupled to the
Fisher metric. This key dualistic structure is purely geo-
metric and therefore can be used beyond the realm of
statistics (for example, when studying optimization algo-
rithms with convex barrier functions [Ama16]). Dually
flat spaces are Hessian manifolds [Shi07] with a single-
chart atlas where the Legendre-Fenchel transformation
plays an essential role to define dual coordinate systems
and dual potential functions. Dually flat spaces generalize
Euclidean geometry and enjoy a generalized Pythagorean
theorem [Ama16]. Many pioneers have contributed to the
now well-established classical dual structure of informa-
tion geometry: Figure 3 displays historically the main ac-
tors who contributed to the genesis of the dual structure
of information geometry with achieved milestones.
Recent advances in information geometry studies the
geometry of deformed exponential families and their
use in thermostatistics [Nau11], the geometry of non-
parametric models, the quantum information geometry,
the Lie group thermodynamics, and the interactions of
geometric mechanics with information geometry via sym-
plectic and contact structures. Information geometry has
found many applications beyond statistics. We refer to
the textbook [Ama16] for applications in signal processing,
data science, and machine learning. To conclude with an
application in machine learning, consider training a neu-
ral network 𝑦= NN𝜃(𝑥) parameterized by weights 𝜃. The
neural network is typically trained by using the method of
gradient descent to minimize a loss function
𝐿(𝜃) ≔1
𝑛
𝑛
∑
𝑖=1
(𝑦𝑖−NN𝜃(𝑥𝑖))2
defined by a supervised training set of 𝑛labeled pairs
{(𝑥𝑖, 𝑦𝑖)}, where 𝑦𝑖denotes the label of 𝑥𝑖: initialize 𝜃0
and iteratively update 𝜃𝑡+1 ≔𝜃𝑡−𝛽∇𝐿(𝜃𝑡), where 𝛽de-
notes the step size.
The ordinary gradient ∇𝜃𝐿(𝜃) de-
pends on the chosen parameterization, i.e., ∇𝜃𝐿(𝜃) ≠
∇𝜂𝐿(𝜃(𝜂)) for a smooth invertible parameter transforma-
tion 𝜂= 𝜃(𝜂).
A better parameter-invariant gradient
has been proposed in information geometry for optimiza-
tion on Riemannian manifolds (𝑀, 𝑔): the natural gra-
dient [Ama16]
̃∇𝜃𝐿(𝜃)
≔
[𝑔𝑖𝑗(𝜃)]−1
𝑖𝑗∇𝜃𝐿(𝜃).
The nat-
ural gradient ensures that
̃∇𝜃𝐿(𝜃)
=
̃∇𝜂𝐿(𝜃(𝜂)).
The
natural gradient descent is used to train stochastic neu-
ral networks with parameter space modeled as a Fisher-
Rao manifold, called a neuromanifold [Ama16].
Since
2018, an eponymous journal devoted to information
geometry (INGE, https://www.springer.com/journal
/41884) is published by Springer which reports the latest
advances in the field.
References
[Ama16] Shun-ichi Amari, Information
geometry
and
its
applications, Applied Mathematical Sciences, vol. 194,
Springer, [Tokyo], 2016, DOI 10.1007/978-4-431-55978-8.
MR3495836
[Ama21] Shun-ichi Amari, Information geometry, Jpn. J. Math.
16 (2021), no. 1, 1–48, DOI 10.1007/s11537-020-1920-5.
MR4206647
[AKO18] Shun-ichi Amari, Ryo Karakida, and Masafumi
Oizumi, Information geometry connecting Wasserstein dis-
tance and Kullback-Leibler divergence via the entropy-relaxed
transportation problem, Inf. Geom. 1 (2018), no. 1, 13–37,
DOI 10.1007/s41884-018-0002-8. MR3974671
[AJLS17] Nihat Ay, Jürgen Jost, Hông Vân Lê, and Lorenz
Schwachhöfer, Information geometry, Ergebnisse der Mathe-
matik und ihrer Grenzgebiete. 3. Folge. A Series of Modern
Surveys in Mathematics [Results in Mathematics and Re-
lated Areas. 3rd Series. A Series of Modern Surveys in Math-
ematics], vol. 64, Springer, Cham, 2017, DOI 10.1007/978-
3-319-56478-4. MR3701408
[ ˇCen82] Nikolai Nikolaevich ˇCencov, Statistical decision rules
and optimal inference, Translations of Mathematical Mono-
graphs, vol. 53, American Mathematical Soc., 1981.
[Efr75] Bradley Efron, Defining the curvature of a statistical prob-
lem (with applications to second order efficiency), Ann. Statist.
3 (1975), no. 6, 1189–1242. MR428531
[Egu83] Shinto Eguchi, Second order efficiency of minimum con-
trast estimators in a curved exponential family, Ann. Statist. 11
(1983), no. 3, 793–803. MR707930
[GN14] Leonor Godinho and Jos´e Nat´ario, An introduction
to Riemannian geometry: With applications to mechanics
and relativity, Universitext, Springer, Cham, 2014, DOI
10.1007/978-3-319-08666-8. MR3289090
[Kee10] Robert W. Keener, Theoretical statistics: Topics for a core
course, Springer Texts in Statistics, Springer, New York, 2010,
DOI 10.1007/978-0-387-93839-4. MR2683126
[Li21] Wuchen Li, Transport information Bregman divergences,
arXiv:2101.01162, 2021.
[Nau11] Jan Naudts, Generalised thermostatistics, Springer-
Verlag London, Ltd., London, 2011, DOI 10.1007/978-0-
85729-355-8. MR2777415
[Nie20] Frank Nielsen, An elementary introduction to informa-
tion geometry, Entropy 22 (2020), no. 10, Paper No. 1100,
61, DOI 10.3390/e22101100. MR4221069
44
NOTICES OF THE AMERICAN MATHEMATICAL SOCIETY
VOLUME 69, NUMBER 1

[PC19] Gabriel Peyr´e and Marco Cuturi, Computational opti-
mal transport: With applications to data science, Foundations
and Trends® in Machine Learning 11 (2019), no. 5-6, 355–
607.
[Shi07] Hirohiko Shima, The geometry of Hessian structures,
World Scientific Publishing Co. Pte. Ltd., Hackensack, NJ,
2007, DOI 10.1142/9789812707536. MR2293045
[Won18] Ting-Kam Leonard Wong, Logarithmic divergences
from optimal transport and R´enyi geometry, Inf. Geom. 1
(2018), no. 1, 39–78, DOI 10.1007/s41884-018-0012-6.
MR4010746
Frank Nielsen
Credits
The opening image is courtesy of ermess via Getty.
Figures 1–3 are courtesy of the author.
Photo of the author is courtesy of Maryse Beaumont.
JANUARY 2022
NOTICES OF THE AMERICAN MATHEMATICAL SOCIETY
45

