Overcomplete Dictionaries for
Sparse Representation of Signals
Michal Aharon

ii

Overcomplete Dictionaries for
Sparse Representation of Signals
Reasearch Thesis Submitted in Partial Fulﬁllment
of The Requirements for the Degree of Doctor of Philosophy.
Michal Aharon
Submitted to the Senate of
the Technion - Israel Institute of Technology
KISLEV, 5767
HAIFA
NOVEMBER 2006

iv

This Research Thesis Was Done Under the Supervision of
Dr. Michael Elad
in the Department of Computer Science
I would like to thank my supervisor, Dr. Michael Elad, for
his support and trust throughout the course of my PhD stud-
ies. His substantial and thorough approach, together with his
genuine interest in the research subject, turned my research
work into a great experience. A special thanks is also in order
for Prof. Alfred Bruckstein, who accompanied me through-
out my studies and was an active partner in this research.
The generous ﬁnancial help of the Technion is gratefully ac-
knowledged.
Finally, I would like to thank my husband, Hanoch, for being
always beside me. This degree is dedicated to him with love.

vi

Contents
Chapter
1
Sparse Representation of Signals and Images
3
1.1
Introduction and Symbols
. . . . . . . . . . . . . . . . . . . . . .
3
1.2
The Choice of the Dictionary
. . . . . . . . . . . . . . . . . . . .
4
1.3
The Sparseland Model for Signals . . . . . . . . . . . . . . . . . .
5
1.4
The Contribution of this Work . . . . . . . . . . . . . . . . . . . .
6
1.5
The Structure of this Report . . . . . . . . . . . . . . . . . . . . .
9
2
Prior Art
11
2.1
Sparse Coding . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
11
2.1.1
Orthogonal Matching Pursuit . . . . . . . . . . . . . . . .
12
2.1.2
Basis Pursuit
. . . . . . . . . . . . . . . . . . . . . . . . .
13
2.1.3
FOCUSS . . . . . . . . . . . . . . . . . . . . . . . . . . . .
14
2.2
Theoretical Analysis of Pursuit Methods . . . . . . . . . . . . . .
15
2.2.1
Uniqueness of Sparse Representation
. . . . . . . . . . . .
16
2.2.2
Evaluating the Pursuit Algorithm Results
. . . . . . . . .
17
2.3
Design of Dictionaries
. . . . . . . . . . . . . . . . . . . . . . . .
19
2.3.1
Generalizing the K-Means? . . . . . . . . . . . . . . . . . .
19
2.3.2
Maximum Likelihood Methods . . . . . . . . . . . . . . . .
20
2.3.3
The MOD Method . . . . . . . . . . . . . . . . . . . . . .
23
2.3.4
Maximum A-posteriori Probability Approach . . . . . . . .
24
2.3.5
Unions of Orthonormal Bases
. . . . . . . . . . . . . . . .
25
2.3.6
Summary of the Prior Art . . . . . . . . . . . . . . . . . .
27
3
Uniqueness of the Dictionary
29
3.1
Posing the Problem . . . . . . . . . . . . . . . . . . . . . . . . . .
29
3.2
Statement of the Uniqueness Result . . . . . . . . . . . . . . . . .
29
3.3
The Proof . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
31
3.3.1
Stage 1: Clustering The Signals . . . . . . . . . . . . . . .
32
3.3.2
Stage 2: Detecting Pairs With Mutual Atom . . . . . . . .
33
3.3.3
Stage 3: Extracting the Mutual Atom . . . . . . . . . . . .
34

viii
3.3.4
Summarizing the Proof . . . . . . . . . . . . . . . . . . . .
35
3.4
Summary
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
35
4
The K-SVD Algorithm
37
4.1
K-Means Algorithm for Vector Quantization . . . . . . . . . . . .
37
4.2
The K-SVD Algorithm . . . . . . . . . . . . . . . . . . . . . . . .
39
4.2.1
K-SVD – Generalizing the K-Means . . . . . . . . . . . . .
39
4.2.2
K-SVD - Detailed Description . . . . . . . . . . . . . . . .
42
4.2.3
From K-SVD Back to K-Means . . . . . . . . . . . . . . .
48
4.2.4
K-SVD - Implementation Details
. . . . . . . . . . . . . .
49
4.2.5
Synthetic Experiments . . . . . . . . . . . . . . . . . . . .
50
4.3
Applications to Image Processing - Basic Results
. . . . . . . . .
53
4.3.1
Filling-In Missing Pixels . . . . . . . . . . . . . . . . . . .
57
4.3.2
Compression . . . . . . . . . . . . . . . . . . . . . . . . . .
60
5
K-SVD-Based Image Denoising
63
5.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
63
5.2
From Local to Global Bayesian Reconstruction . . . . . . . . . . .
67
5.2.1
The Sparseland Model for Image Patches . . . . . . . . . .
67
5.2.2
From Local Analysis to a Global Prior
. . . . . . . . . . .
68
5.2.3
Numerical Solution . . . . . . . . . . . . . . . . . . . . . .
70
5.3
Example-Based Sparsity and Redundancy
. . . . . . . . . . . . .
71
5.3.1
Training on the Corpus of Image Patches . . . . . . . . . .
72
5.3.2
Training on the Corrupted Image . . . . . . . . . . . . . .
73
5.4
Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
75
5.5
Summary
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
85
6
Constrained K-SVD
89
6.1
General Motivation . . . . . . . . . . . . . . . . . . . . . . . . . .
89
6.2
Non-Negative K-SVD (NN-K-SVD) . . . . . . . . . . . . . . . . .
91
6.2.1
General
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
91
6.2.2
Non-negative Decomposition . . . . . . . . . . . . . . . . .
92
6.2.3
Design of Non-Negative Dictionaries - Prior Art . . . . . .
93
6.2.4
Non-Negative K-SVD - NN-K-SVD . . . . . . . . . . . . .
94
6.2.5
A Synthetic Experiment . . . . . . . . . . . . . . . . . . .
96
6.3
Shift-Invariant K-SVD (SI-K-SVD) . . . . . . . . . . . . . . . . .
99
6.3.1
General
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
99
6.3.2
Training Shift Invariant Dictionary - Prior Art . . . . . . .
99
6.3.3
K-SVD for Shift-Invariant Dictionaries (SI-K-SVD) . . . .
100
6.3.4
SI-K-SVD - experiments . . . . . . . . . . . . . . . . . . .
104
6.3.5
Summary
. . . . . . . . . . . . . . . . . . . . . . . . . . .
106
6.4
Linear Constraints
. . . . . . . . . . . . . . . . . . . . . . . . . .
107

ix
6.4.1
Linear Constraint K-SVD (LC-K-SVD) . . . . . . . . . . .
108
6.4.2
Example - Multiscale Representation . . . . . . . . . . . .
109
6.5
Image Signature Dictionary
. . . . . . . . . . . . . . . . . . . . .
119
6.5.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . .
119
6.5.2
Training an Image Signature Dictionary
. . . . . . . . . .
121
6.5.3
Experiments . . . . . . . . . . . . . . . . . . . . . . . . . .
124
7
Steps Towards a Geometrical Study of Sparseland
129
7.1
Introduction and Motivation . . . . . . . . . . . . . . . . . . . . .
129
7.2
Representation Abilities of L-atoms . . . . . . . . . . . . . . . . .
131
7.3
Representation Abilities of a Dictionary . . . . . . . . . . . . . . .
137
7.3.1
Representation by Non-Intersecting Sets of Atoms . . . . .
139
7.3.2
Representation by Intersecting Sets of Atoms
. . . . . . .
141
7.4
Summary
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
146
8
Summary and Conclusions
149
8.1
General
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
149
8.2
Future Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
150
Bibliography
153

x

Tables
Table
5.1
Summary of the denoising PSNR results in [dB]. In each cell four
denoising results are reported. Top left: results of Portilla et at.
[87], Top right: overcomplete DCT, Bottom left: global trained dic-
tionary, Bottom right: adaptive dictionary trained on noisy image.
In each such set we highlighted the best result. All numbers are
an average over 5 experiments. The last two columns present the
average results over all images and their variance. . . . . . . . . .
78
6.1
Average detection rates (in percentage) for the multiscale synthetic
experiments. Notice the number of atoms are 9, 36 and 144 for the
zero (coarsest), ﬁrst and second level, respectively. Each experi-
ment was performed 5 times.
. . . . . . . . . . . . . . . . . . . .
117

xii

Figures
Figure
4.1
The K-Means Algorithm . . . . . . . . . . . . . . . . . . . . . . .
40
4.2
The K-SVD Algorithm. . . . . . . . . . . . . . . . . . . . . . . . .
46
4.3
Synthetic results: for each of the tested algorithms and for each
noise level, 50 trials were conducted, and their results sorted. The
graph labels represent the mean number of detected atoms (out of
50) over the ordered tests in sets of 10 experiments.
. . . . . . .
52
4.4
A collection of 500 random blocks that were used for training, sorted
by their variance.
. . . . . . . . . . . . . . . . . . . . . . . . . .
54
4.5
The learned dictionary. The atoms are sorted in an ascending or-
der of their variance, and stretched to maximal range for display
purposes.
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
55
4.6
The overcomplete separable Haar dictionary.
. . . . . . . . . . .
55
4.7
The overcomplete DCT dictionary.
. . . . . . . . . . . . . . . . .
56
4.8
The RMSE for 594 new blocks with missing pixels using the learned
dictionary, overcomplete Haar dictionary, and overcomplete DCT
dictionary.
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
58
4.9
The corrupted image (left) with the missing pixels marked as points,
and the reconstructed results by the learned dictionary, the over-
complete Haar dictionary, and the overcomplete DCT dictionary,
respectively. The diﬀerent rows are for 50% and 70% of missing
pixels. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
59
4.10 Compression results: Rate-Distortion graphs.
. . . . . . . . . . .
61
4.11 Sample compression results. . . . . . . . . . . . . . . . . . . . . .
62
5.1
Denoising Procedure using a dictionary trained on patches from
the corrupted image. For our experiments with noise level lower
than σ = 50, we used the OMP pursuit method, and set J = 10,
λ = 30/σ and C = 1.15.
. . . . . . . . . . . . . . . . . . . . . . .
76
5.2
The overcomplete DCT dictionary.
. . . . . . . . . . . . . . . . .
79
5.3
The globally trained dictionary. . . . . . . . . . . . . . . . . . . .
80

xiv
5.4
Sample from the images used for training the global dictionary.
.
80
5.5
Comparison between the three presented methods (Overcomplete
DCT, global trained dictionary and adaptive dictionary trained on
patches from the noisy image) and the results achieved recently in
[87] for three test images.
. . . . . . . . . . . . . . . . . . . . . .
81
5.6
The improvement in the denoising results after each iteration of the
K-SVD algorithm, executed on noisy patches of the image ‘Pep-
pers’.
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
82
5.7
Example of the denoising results for the image ‘Barbara’ with σ =
20 – the original, the noisy, and two restoration results.
. . . . .
83
5.8
Example of the denoising results for the image ‘Barbara’ with σ =
20 – the adaptively trained dictionary.
. . . . . . . . . . . . . . .
84
5.9
The improvement (and later, deterioration) of the denoising results
when increasing the value of λ in the averaging process in Equation
(5.8).
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
86
5.10 The eﬀect of changing the number of dictionary elements (K) on
the ﬁnal denoising results for the image ’House’ and for σ = 15.
.
86
6.1
Finding a non-negative rank-1 approximation for a matrix A = dxT
95
6.2
NN-K-SVD
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
97
6.3
On top, from left to right: True initial dictionary, K-SVD results
after 60 iterations in the no-noise test, and after 100 iterations
when the noise level was 15 SNR. On the bottom, from left to
right: Hoyer’s algorithm results in the no-noise case after 600 and
after 1500 iterations, and after 1500 iterations in the test with noise
level of 15 SNR.
. . . . . . . . . . . . . . . . . . . . . . . . . . .
98
6.4
Illustraion for approximation of a vector by a linear combination of
3 shifted versions of the atom d.
. . . . . . . . . . . . . . . . . .
103
6.5
Each symbol represents the average detection rate of 5 tests. The
tested methods were SI-K-SVD, ISL-DLA for shift invariant dictio-
naries, and the original K-SVD method (without shift invariance).
105
6.6
Detected atoms found by the SI-KSVD algorithm executed on 10×
10 patches from natural images. The found atoms of size 4 × 4 are
located in the center of each block, for visualization purposes.
. .
106
6.7
MultiScale representation scheme. ↑Li or ↓Li represent enlarge
or reduce size to level i, A(Di) and S(Di) represent analysis and
synthesis, respectively, using the i’th dictionary.
. . . . . . . . .
112
6.8
Typical execution of MS-KSVD (30dB). The graph presents the
detection ratios of each level (the zero level is the coarsest one) for
the ﬁrst 6 iterations.
. . . . . . . . . . . . . . . . . . . . . . . . .
118
6.9
Samples from the 64×64 face images used for training the multiscale
dictionaries.
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
119

xv
6.10 Multiscale dictionaries extracted by the K-SVD algorithm, pro-
cessed on real face images. The most left dictionary is the coarsest. 120
6.11 Illustration of several atoms from in an ISD. . . . . . . . . . . . .
122
6.12 Image signature dictionary of size 100 × 100, trained on patches
from real images. . . . . . . . . . . . . . . . . . . . . . . . . . . .
125
6.13 Trained image signature dictionaries for several images. For each
of these signatures we describe which image it is related to, the size
of the signature, and the additive noise power that aﬀected it. . .
126
6.14 Sample images we used for the various tests. From left to right:
Barbara, House and Peppers.
. . . . . . . . . . . . . . . . . . . .
126
6.15 Examining the shift-invariance property of the ISD. . . . . . . . .
127
7.1
Area calculation scheme. On the left - n=2 and L=1, on the middle
- n=3 and L=1, and on the right - n=3, L=2.
. . . . . . . . . . .
134
7.2
Approximation of the function tan−1 using the identity function.
134
7.3
Calculation of the surface area of a dome.
. . . . . . . . . . . . .
135
7.4
Approximated and calculated ratio, with n = 8 and L = 3, and
increasing error.
. . . . . . . . . . . . . . . . . . . . . . . . . . .
136
7.5
On the left, illustration of two overlapping strips. On the right,
illustration of the bounding area of the intersection.
. . . . . . .
142
7.6
Illustration of the coverage of a Grassmanian dictionary of dimen-
sion 10 for several redundancy levels
. . . . . . . . . . . . . . . .
147

xvi

Abstract
In recent years there has been a growing interest in the study of sparse rep-
resentation of signals. Using an overcomplete dictionary that contains prototype
signal-atoms, signals are described as linear combinations of a few of these atoms.
Applications that use sparse representation are many and include compression,
regularization in inverse problems, feature extraction, and more.
Roughly speaking, the activity in this ﬁeld concentrates around two main
problems: (i) algorithms for performing sparse decomposition and their perfor-
mance analysis, and (ii) dictionary composition methods. The ‘sparse decomposi-
tion’ activity deals with the pursuit problem - given a signal and an overcomplete
dictionary, the goal is to ﬁnd the smallest set of atoms from the dictionary to
represent it. As this problem is known to be NP-hard, several approximation
algorithms have been suggested, such as the matching pursuit, the basis pursuit,
FOCUSS, and their variants. Recent work has been also directed at the study of
these algorithms’ performance and prospects of success.
The construction of appropriate dictionaries that lead to the existence of
sparse representations for a family of signals in mind is the second challenge
addressed in recent years in this ﬁeld. Among the various contributions on this
problem, dictionary learning from examples is especially appealing as it can tune
the found dictionary to speciﬁc and narrow families of signals.
In this work we concentrate on this dictionary learning process. Starting
with a wide literature survey of pursuit techniques and existing dictionary learning

2
methods, this thesis turns to present a novel dictionary learning algorithm named
the K-SVD, develops an analysis of its uniqueness properties, and explores ways
to deploy the developed learning method to applications in image processing.
The problem setting for dictionary learning is the following - given a set of
signals assumed to have a sparse representation over an unknown dictionary D,
can we ﬁnd D? A fundamental question that must be asked beforehand is whether
D is unique, or could there be several such dictionaries describing the same set of
signals equally well. In our work we prove a uniqueness property, implying that
in the quest for D there is one ideal dictionary that we target.
At the heart of this work stands the K-SVD algorithm for learning dictio-
naries. We describe its development and analysis, and show some stylized appli-
cations to demonstrate its applicability and the advantages of trained dictionaries
in general. Variations of the K-SVD algorithm for learning structural constrained
dictionaries are also presented. Among those constraints are the non-negativity
of the dictionary and shift invariance property. This work also includes a develop-
ment of a state-of-the-art image denoising algorithm based on the K-SVD. This
contribution is important as it strengthens the message that the general model of
sparsity and redundancy, along with ﬁtted dictionaries as considered here, could
lead to the best known results in practical applications in image processing.
An underlying assumption throughout this thesis is that natural signals
are represented well using a sparse linear composition of atoms from redundant
dictionaries. We refer to this assumption as the ”Sparseland” model, and provide
an initial study of its geometrical behavior. More speciﬁcally, we provide bounds
on the expected ratio of signals that can be represented by this dictionary with
ﬁxed sparsity constraints.

Chapter 1
Sparse Representation of Signals and Images
1.1
Introduction and Symbols
Recent years have witnessed a growing interest in the search for sparse
representations of signals. Using an overcomplete dictionary matrix D ∈Rn×K
that contains K prototype signal-atoms for columns, {dj}K
j=1, a signal y ∈Rn can
be represented as a linear combination of these atoms. The representation of y
may either be exact y = Dx, or approximate, y ≈Dx, satisfying ∥y −Dx∥p ≤ϵ.
The vector x ∈RK contains the representation coeﬃcients of the signal y. In
approximation methods, typical norms used for measuring the deviation are the
ℓp-norms for p = 1, 2 and ∞. In this work we shall concentrate on the case of
p = 2.
If n < K and D is a full-rank matrix, an inﬁnite number of solutions are
available for the representation problem, hence constraints on the solution must
be set. The solution with the fewest number of nonzero coeﬃcients is certainly
an appealing representation. This sparsest representation is the solution of either
(P0)
min
x ∥x∥0 subject to y = Dx,

4
or
(P0,ϵ)
min
x ∥x∥0 subject to ∥y −Dx∥2 ≤ϵ,
where ∥·∥0 is the l0 norm, counting the nonzero entries of a vector.
Extraction of the sparsest representation is a NP-hard problem. Algorithms
for ﬁnding approximating solutions have been extensively investigated and indeed,
several eﬀective decomposition algorithms are available.
Thorough theoretical
work studied the quality of these algorithms’ solution, in order to evaluate their
similarity to the exact solutions.
In all those methods, there is a preliminary
assumption that the dictionary is known and ﬁxed. In this research we address
the issue of designing the proper dictionary, in order to better ﬁt the sparsity
model imposed.
1.2
The Choice of the Dictionary
An overcomplete dictionary D that leads to sparse representations can either
be chosen as a pre-speciﬁed set of functions, or designed by adapting its content
to ﬁt a given set of signal examples.
Choosing a pre-speciﬁed transform matrix is appealing because it is simpler.
Also, in many cases it leads to simple and fast algorithms for the evaluation of the
sparse representation. This is indeed the case for overcomplete wavelets, curvelets,
contourlets, steerable wavelet ﬁlters, short-time-Fourier transforms, and more.
Preference is typically given to tight frames that can easily be pseudo-inverted.
The success of such dictionaries in applications depends on how suitable they are
to sparsely describe the signals in question. Multiscale analysis with oriented basis
functions and a shift-invariant property are guidelines in such constructions.

5
In this research we consider a diﬀerent route for designing dictionaries D
based on learning. Our goal is to ﬁnd the dictionary D that yields sparse rep-
resentations for the training signals. We believe that such dictionaries have the
potential to outperform commonly used pre-determined dictionaries. With ever-
growing computational capabilities, computational cost may become secondary
in importance to the improved performance achievable by methods which adapt
dictionaries for special classes of signals.
1.3
The Sparseland Model for Signals
The concepts of sparsity and overcompleteness, together or separately, in
representation of signals were proved to be highly eﬀective. Applications that can
beneﬁt from them include compression, regularization in inverse problems, feature
extraction, and more.
Indeed, the success of the JPEG2000 coding standard
can be attributed to the sparsity of the wavelet coeﬃcients of natural images
[76]. In denoising, wavelet methods and shift-invariant variations that exploit an
overcomplete representation are among the most eﬀective known algorithms for
this task [30, 13, 96, 100]. Sparsity and overcompleteness have been successfully
used for dynamic range compression in images [48], separation of texture and
cartoon content in images [101, 102], inpainting [38], and more. Indeed, in this
work we adopt the sparsity and redundancy concepts for achieving state-of-the-art
denoising results.
For a signal family of interest (e.g. images), sparsity and redundancy as
described above are imposed as a descriptive model. That is to say, we assume
that each signal in this family has a (very) sparse representation over a speciﬁc and
joint dictionary. We refer hereafter to this model as Sparseland. The Sparseland

6
model is deﬁned by the following components:
• Family of signals - a set of signals with some mutual property. Diﬀerent
families may intersect or even include one another. For example, the set
of facial-image-patches is a subset of real-image-patches.
Clearly, the
narrower the family of signals in mind, the better the Sparseland model
handles its members
• Representation Error - the maximal representation error we allow for
each member in this family.
• Sparsity constraint - some constraint that forces sparsity of each coef-
ﬁcient vector. Such a constraint can be a distribution of the representa-
tions, or simply just the maximal number of non-zero entries allowed in
each representation.
• The Dictionary - The representing dictionary D that includes K (often
normalized) atoms.
The Sparseland model is the basis for all algorithms and applications described
in this work, and it will accompany us throughout this report. We consider the
quality of the results obtained in this work using this model as an empirical
evidence that validates and strengthens the Sparseland model.
1.4
The Contribution of this Work
The ﬁeld of sparse representations and the use of redundant dictionaries has
been drawing a considerable attention lately. This interest emerges from both the
practical impact of the wavelet theory on the signal/image processing commu-
nity, and also the surprising recent theoretical results on the analysis of pursuit

7
methods. This emerging post-wavelet ﬁeld, referred to herein as Sparseland, en-
compasses both theory and applications, in a way that appears to be gaining
strength and interest.
In this thesis we concentrate on one of the most interesting and fundamental
issues in this area - the design and use of data-driven dictionaries for sparse
representations. Our main aim in this work is to demonstrate the eﬀectiveness
and potential of such tailored dictionaries, as also the simplicity of their design.
There are numerous questions one should consider, when approaching the
problem of designing/training sparsifying dictionaries for signals. The most press-
ing of these problems is the issue of a feasible and simple algorithm for such a
design. In our work we develop a novel such method, coined “the K-SVD”. This
algorithm builds a dictionary that leads to sparse representations for the given
set of training signals. The K-SVD is a straight forward generalization of the
popular K-Means algorithm, used for Vector Quantization, and as such, it is easy
to understand and to implement. Our work includes a comparison of this algo-
rithm with competing methods, showing its eﬃciency and superiority. Using the
K-SVD, the task that apparently seems the hardest – training the dictionary –
becomes almost trivial and automatic.
If constraints are to be applied on the representing dictionary, variations
of the K-SVD can be naturally and easily derived, due to the original simplicity
of the algorithm’s setting. Indeed, in this work we show several such variations,
among which are the non-negativity and shift invariance properties. The bottom
line to this contribution is the fact that the K-SVD algorithm can be used with
no ”emotional involvement” in various applications.
Our ﬁrst task, after putting forward the K-SVD algorithm, is an initial

8
demonstration of its potential to applications. This is demonstrated by applying
it directly to several stylized applications in image processing and showing good
results. The question remains, though, whether the Sparseland model and the K-
SVD dictionary that serves it will be able to complete against the best available
methods today in various applications. For this purpose we develop a full scheme
for applying data-driven dictionaries for denoising of images. A simple local-based
method is used in a way that creates a global prior for the whole image. This
algorithm successfully competes and outperforms state-of-the-art image denoising
methods, putting the K-SVD and the Sparseland model in the spotlight.
In a more theoretic discussion our work thoroughly dives into several in-
teresting questions that concern sparse representation under data-driven dictio-
naries. Given a set of signals, in which each member was initially generated by
sparse combinations over some unknown dictionary, we prove that this dictionary
is necessarily unique, provided that enough signal examples are given. This proof
provides a theoretic justiﬁcation for our main concept - the quest for a sparse
representing dictionary. In a diﬀerent attempt to get a better understanding of
the Sparseland model, this work also studies the geometrical structure dictated
by sparse coding, concentrating on measuring the volume of signals that can be
sparsely approximated by a given dictionary.
To summarize, in this thesis we try to demonstrate the simplicity and ap-
plicability of using data-driven dictionaries for sparse representation of signals, a
promising area of research, which we believe has not yet reached the peak of its
bloom.

9
1.5
The Structure of this Report
We start this report in Section 2 with reviewing the work that was done
in the ﬁeld of sparse representation and redundant dictionaries.
First, several
pursuit algorithms are described and a discussion concerning the evaluation of
these algorithms’ performances is given.
Then we describe the work that was
done in the ﬁeld of dictionary learning, and review the diﬀerent approaches and
algorithms.
The theoretic question concerning the uniqueness of the underlying dictio-
nary is referred in Section 3. We prove that if a set of signals Y is created by
sparse combinations over some dictionary, under some mild conditions, this un-
derlying dictionary is necessarily unique, and no other dictionary can represent
this set of signals under the same sparsity constraints. This implies that in the
quest for D there is one ideal dictionary that we target.
In the center of this work stands a simple algorithm for dictionary design
– the K-SVD. The K-SVD algorithm, generalizing the K-means algorithm for
Vector Quantization, attempts to ﬁnd the dictionary which can best represent all
signals of interest sparsely. This algorithm is described and analyzed in detail in
Section 4, along with several examples describing stylized applications in image
processing.
The application of denoising images using learned dictionaries is revisited in
Section 5, redeveloped and tuned in a way that leads to state-of-the-art denoising
results. A limitation of the K-SVD is its ability to handle low-dimensional signals,
which prevents its deployment to wide support images. The developed denoising
algorithm described in this section uses the K-SVD locally on small image patches,
and yet these local processes are merged into a global maximum-aposteriori prob-

10
ability estimation. Another intriguing feature of the proposed scheme is the fact
that the dictionary is learned from the measured (noisy) image.
Several variations of the K-SVD for learning dictionaries with structural
constraints are suggested and discussed in Section 6. The ﬁrst constraint is non-
negativity, applied on the dictionary entries, and possibly on the coeﬃcients.
K-SVD variation for shift invariant dictionaries is then presented. Here each dic-
tionary element can serve as many diﬀerent representing atoms, shifted variations
of one another. The process of learning linear constrained dictionaries is presented
afterwards, together with a suggestion for learning multiscale representation of im-
ages. Finally, a novel dictionary structure, referred to here as ‘Image Signature
Dictionary’ (ISD) is suggested.
The atoms of this dictionary are all available
patches in some matrix, allowing cyclic shifts. The advantages of this scheme are
discussed and an algorithm for training ISDs is presented.
In Section 7 we explore the sparseland model from a geometric point of view
and confront the following problem. Given a dictionary D ∈Rn×K, a sparsity
constraint L and a maximal allowed representation error e, a signal y that can
be represented is such for which there exists a subset of L atoms from D that
approximate y with an error less than e. The set of all represented signals is a
subset of Rn. The structure of this set is studied, with emphasis on its relative
volume, in relation to the entire Rn space. We derive bounds on this size, and
show that these bounds are strongly dependent on inner properties of D such as
the linear dependencies between its atoms.

Chapter 2
Prior Art
2.1
Sparse Coding
Sparse coding is the process of computing the representation coeﬃcients,
x, based on the given signal y and the dictionary D. This process, commonly
referred to as “atom decomposition”, requires solving
(P0)
min
x ∥x∥0 subject to y = Dx,
(2.1)
or
(P0,ϵ)
min
x ∥x∥0 subject to ∥y −Dx∥2 ≤ϵ,
(2.2)
and this is typically done by a “pursuit algorithm” that ﬁnds an approximate
solution, as exact determination of sparsest representations proves to be a NP-hard
problem [15]. In this section we brieﬂy discuss several such algorithms, and their
prospects for success. Sparse coding is a necessary tool in designing dictionaries,
hence it is important to have a good overview of methods for achieving it.
The simplest pursuit algorithms are the Matching Pursuit (MP) [75], Or-
thogonal Matching Pursuit (OMP) and Order Recursive Matching Pursuit (ORMP)
[10, 16, 83, 104, 50]. These are greedy algorithms that select the dictionary atoms

12
sequentially. These methods are very simple, involving the computation of in-
ner products between the signal and the dictionary atoms, and possibly deploying
some least squares solvers or projections. Both (2.1) and (2.2) are easily addressed
by changing the stopping rule of the algorithm.
A second well known pursuit approach is the Basis Pursuit (BP) [11]. It
suggests a convexiﬁcation of the problems posed in (2.1) and (2.2), by replacing the
ℓ0-norm with an ℓ1-norm. The Focal Under-determined System Solver (FOCUSS)
is very similar, using the ℓp-norm with p ≤1, as a replacement to the ℓ0-norm
[52, 90, 89, 88]. Here, for p < 1 the similarity to the true sparsity measure is
better, but the overall problem becomes non-convex, giving rise to local minima
that may mislead in the search for solutions. Lagrange multipliers are used to
convert the constraint into a penalty term, and an iterative method is derived
based on the idea of iterated reweighed least-squares that handles the ℓp-norm as
an ℓ2 weighted norm.
2.1.1
Orthogonal Matching Pursuit
Orthogonal matching pursuit (OMP) is a greedy step-wise regression algo-
rithm [75, 10, 16, 83, 104]. At each stage this method selects the dictionary element
having the maximal projection onto the residual signal (with the assumption that
the dictionary atoms have been normalized). After each selection, the represen-
tation coeﬃcients w.r.t. the atoms chosen so far are found via least-squares. For-
mally, given a signal y ∈Rn, and a dictionary D with K ℓ2-normalized columns
{dk}K
k=1, we start by setting r0 = y, k = 1, and perform the following steps:
(1) Select the index of the next dictionary element ik = argmaxw |⟨rk−1, dw⟩|;
(2) Update the current approximation yk = argminyk ∥y −yk∥2
2, such that

13
yk ∈span {di1, di2, ..., dik}; and
(3) Update the residual rk = y −yk.
The algorithm can be stopped after a predetermined number of steps, hence after
having selected a ﬁxed number of atoms. Alternatively the stopping rule can be
based on the norm of the residual, or on the maximal inner product computed in
the next atom selection stage.
OMP is an appealing algorithm and very simple to implement. Unlike other
methods, it can easily be programmed to supply a representation with an a priori
ﬁxed number of non-zero entries – a desired outcome in the training of dictionaries.
There are several variants of the OMP that suggest (i) skipping the least-squares
and using the inner product itself as a coeﬃcient; (ii) applying least-squares per
every candidate atom, rather than just using inner-products at the selection stage;
(iii) projecting all non-selected atoms onto the space spanned by the selected
atoms before each new atom selection; (iv) doing a faster and less precise search,
where instead of searching for the maximal inner product, a nearly maximal one
is selected, thereby speeding up the search; and there are more.
2.1.2
Basis Pursuit
The Basis Pursuit (BP) algorithm [11] proposes the replacement of the ℓ0-
norm in (2.1) and (2.2) with an ℓ1-norm. Hence solutions of
(P1)
min
x ∥x∥1 subject to y = Dx,
(2.3)
in the exact representation case, and
(P1,ϵ)
min
x ∥x∥1 subject to ∥y −Dx∥≤ϵ,
(2.4)

14
in the approximate one, lead to the BP representations. Solution of (2.3) amounts
to linear programming and eﬃcient solvers for such problems exist. The approx-
imate form (2.4) leads to a quadratic programming structure, and again, there
exist eﬃcient solvers for such problems. Recent work on iterated shrinkage al-
gorithms provide highly eﬃcient and tailored methods for minimizing (2.4) in a
setting which resembles the OMP (see [35, 37, 44, 45, 14]).
2.1.3
FOCUSS
FOCUSS stands for FOCal Underdetermined System Solver [52, 90, 89].
This is an approximating algorithm for ﬁnding the solution of either (2.1) or
(2.2), by replacing the ℓ0-norm with an ℓp one for p ≤1.
For the exact case problem, (P0), this method requires solving
(Pp)
min
x ∥x∥p
p subject to y = Dx.
(2.5)
The use of a Lagrange multiplier vector λ ∈Rn here yields the Lagrangian func-
tion
L(x, λ) = ∥x∥p
p + λT(y −Dx).
(2.6)
Hence necessary conditions for a pair x, λ to be a solution of (2.5) are
∇xL(x, λ) = pΠ(x)x −DTλ = 0
and
∇λL(x, λ) = Dx −y = 0,
(2.7)
where we have deﬁned Π(x) to be a diagonal matrix with |xi|p−2 as its (i, i)th entry.
The split of the ℓp-norm derivative into a linear term multiplied by a weight matrix
is the core of the FOCUSS method, and this follows the well-known idea of iterated
reweighed least-squares [64]. Several simple steps of algebra lead to the solution
x = Π(x)−1DT  DΠ(x)−1DT−1 y.
(2.8)

15
While it is impossible to get a closed form solution for x from the above result,
an iterative replacement procedure can be proposed, where the right hand side
is computed based on the currently known xk−1, and this leads to the updating
process,
xk = Π(xk−1)−1DT  DΠ(xk−1)−1DT−1 y.
(2.9)
A regularization can, and should be introduced [88] to avoid near-zero entries in
the weight matrix Π(x).
For the treatment of (P0,ϵ) via the (Pp,ϵ) parallel expressions can be derived
quite similarly, although in this case the determination of the Lagrange multiplier
is more diﬃcult, and must be searched within the algorithm [88].
Both the BP and the FOCUSS can be motivated based on Maximum A-
posteriori Probability (MAP) estimation, and indeed several works used this rea-
soning directly [71, 82, 81, 72]. The MAP can be used to estimate the coeﬃcients
as random variables by maximizing the posterior P(x|y, D) ∝P(y|D, x)P(x).
The prior distribution on the coeﬃcient vector x is assumed to be a super-Gaussian
iid distribution that favors sparsity. For the Laplace distribution this approach is
equivalent to BP [71].
2.2
Theoretical Analysis of Pursuit Methods
Extensive study in recent years of the pursuit algorithms has established
that if the sought solution, x, is sparse enough, these techniques recover it well in
the exact case [29, 36, 26, 54, 47, 104]. Further work considered the approximated
versions and has shown stability in recovery of x [28, 105].
The recent front
of activity revisits those questions within a probabilistic setting, obtaining more
realistic assessments on pursuit algorithms performance and success [24, 25, 6].

16
The properties of the dictionary D set the limits that may be assumed on sparsity.
In this section we brieﬂy mention the main results of this study.
2.2.1
Uniqueness of Sparse Representation
For the analysis of the uniqueness of a sparse representation, as well as for
the study of pursuit algorithms’ performance, we need to deﬁne two measures of
quality for the dictionary D, the Mutual Coherence and the Spark. The Mutual
Coherence of a dictionary D, denoted by µ(D), is deﬁned as the maximal absolute
scalar product between two diﬀerent normalized atoms of D,
µ(D) = max
i̸=j
dT
i dj
 .
(2.10)
The mutual coherence of a dictionary measures the similarity between the dic-
tionary’s atoms. For an orthogonal matrix D, µ(D) = 0. For an overcomplete
matrix (K > n) we necessarily have µ(D) > 0. As we shall see next, there is an
interest in dictionaries with µ{D} as small as possible for sparse representation
purposes. If µ(D) = 1, it implies the existence of two parallel atoms, and this
causes ambiguity in the construction of sparse atom compositions. In [103] it was
shown that for a full rank dictionary of size n × K
µ ≥
s
K −n
n(K −1),
(2.11)
and equality is obtained for a family of dictionaries called Grassmannian frames.
For K ≫n the mutual coherence we can expect to have is thus of the order of
1/√n.
The Spark of a dictionary D is the smallest number of columns that form
a linearly dependent set [26]. Despite the similar deﬁnition, note that spark is
markedly diﬀerent from the matrix rank, being the greatest number of linearly

17
independent columns. A trivial relation between the spark σ{D} and the mutual
coherence µ{D} is [26]
σ{D} ≥1 +
1
µ{D}.
(2.12)
Referring to the problem posed in (2.1), we ﬁrst quote a result that poses a
condition on its solution x such that it guarantees uniqueness:
Theorem 1: (see [26]): A linear representation over m atoms (i.e., ∥x∥0 = m) is
unique if
m < σ{D}
2
.
(2.13)
The work in [28] generalizes this theorem for the solution of (2.2). It shows
that, while exact uniqueness cannot be guaranteed, an approximate one that
allows a bounded deviation can be claimed.
2.2.2
Evaluating the Pursuit Algorithm Results
The uniqueness theorem mentioned above implies that a representation with
less than σ{D}/2 non zero elements is the sparsest available one, and therefore,
if a pursuit algorithm retrieves such a solution it is necessarily the one desired.
However, in which cases may we expect the pursuit algorithms to retrieve this
exact solution, and when can we guarantee their success? Those kind of ques-
tions, concerning the connection between the pursuit algorithm’s results and the
true solutions to (2.1) or (2.2) has been studied extensively in recent years. The
following is a central result on the expected behavior of the MP and BP methods
for sparse enough solutions:
Theorem 2: (see [26, 54, 104, 47]): If the sought solution, x, for the problem

18
(2.1), satisﬁes
∥x∥0 < 1
2

1 +
1
µ{D}

(2.14)
then both BP and MP will recover it exactly.
This result suggests that for representations with less than O(√n) non-zeros,
pursuit methods can succeed. Results of similar nature and strength were devel-
oped for structured dictionaries, constructed as a concatenation of several unitary
matrices [36, 54, 27]. Such a dictionary structure is restrictive, but decomposition
under this kind of dictionaries can be done eﬃciently using the Block Coordinate
Relaxation (BCR) method [93], being a fast variant of the BP.
Recent work by Gribonval and Nielsen analyzed the (Pp) problem and
showed its equivalence to (P0), under conditions similar in ﬂavor to the spar-
sity conditions mentioned above [53]. Hence, the FOCUSS approach too enjoys
the support of some theoretical justiﬁcation, like the two discussed beforehand.
However, the analysis says nothing about local minima traps and prospects in
hitting those in the FOCUSS-algorithm.
Other work considered the approximated version (2.2) and showed stability
in recovery of x, meaning that pursuit algorithms lead to a solution in the prox-
imity of the true optimal one. This research concerns the connection between the
(P0,ϵ) and (P1,ϵ), and is reported in [28, 105, 27].
All the above results consider the worst case scenario, and the bounds de-
rived are therefore too pessimistic. The pursuit algorithms are known (empiri-
cally) to succeed in recovering sparse representations even when the number of
non zero elements is substantially beyond those bounds. Indeed, the recent front
of theoretical activity in this ﬁeld revisits the above questions from a probabilistic
point of view, obtaining more realistic assessments on pursuit algorithms perfor-

19
mance and success [24, 25, 6]. These works show that even for O(n) non-zeros1
in the representations, pursuit methods are expected to succeed with probability
one.
2.3
Design of Dictionaries
We now come to the main topic of this thesis, the training of dictionaries
based on a set of examples.
Given such a set Y = {yi}N
i=1, we consider the
Sparseland model described in section 1.3 and assume that there exists a dictionary
D that gave rise to the given signal examples via sparse combinations, i.e., we
assume that there exists D so that solving (P0) for each example yk gives a sparse
representation xk. It is in this setting that we ask what the proper dictionary D
is.
2.3.1
Generalizing the K-Means?
There is an intriguing relation between sparse representation and clustering
(i.e., vector quantization).
This connection has previously been mentioned in
several reports [40, 65, 106]. In clustering, a set of descriptive vectors {dk}K
k=1 is
learned, and each sample is represented by one of those vectors (the one closest
to it, usually in the ℓ2 distance measure). We may think of this as an extreme
sparse representation, where only one atom is allowed in the signal decomposition,
and furthermore, the coeﬃcient multiplying it must be 1.
There is a variant
of the vector quantization (VQ) coding method, called Gain-Shape VQ, where
this coeﬃcient is allowed to vary [49]. In contrast, in sparse representations as
discussed in this paper, each example is represented as a linear combination of
1 and a proper, somewhat small, constant.

20
several vectors {dk}K
k=1.
Thus, sparse representations can be referred to as a
generalization of the clustering problem.
Since the K-Means algorithm (also known as the generalized Lloyd algorithm
- GLA [49]) is the most commonly used procedure for training in the vector
quantization setting, it is natural to consider generalizations of this algorithm
when turning to the problem of dictionary training. The clustering problem and
its K-Means solution will be discussed in more detail in section 4.1, since our work
approaches the dictionary training problem by generalizing the K-Means. Here we
shall brieﬂy mention that the K-Means process applies two steps per each iteration:
(i) given {dk}K
k=1, assign the training examples to their nearest neighbor; and (ii)
given that assignment, update {dk}K
k=1 to better ﬁt the examples.
The approaches to dictionary design that have been tried so far are very
much in line with the two-step process described above. The ﬁrst step ﬁnds the
coeﬃcients given the dictionary – a step we shall refer to as “sparse coding”. Then,
the dictionary is updated assuming known and ﬁxed coeﬃcients. The diﬀerences
between the various algorithms that have been proposed are in the method used
for the calculation of coeﬃcients, and in the procedure used for modifying the
dictionary.
2.3.2
Maximum Likelihood Methods
The methods reported in [71, 82, 81, 72] use probabilistic reasoning in the
construction of D. The proposed model suggests that for every example y the
relation
y = Dx + v,
(2.15)

21
holds true with a sparse representation x and Gaussian white residual vector v
with variance σ2.
Given the examples Y = {yi}N
i=1 these works consider the
likelihood function P (Y|D) and seek the dictionary that maximizes it.
Two
assumptions are required in order to proceed - the ﬁrst is that the measurements
are drawn independently, readily providing
P (Y|D) =
N
Y
i=1
P (yi|D) .
(2.16)
The second assumption is critical and refers to the “hidden variable” x.
The
ingredients of the likelihood function are computed using the relation
P(yi|D) =
Z
P(yi, x|D)dx =
Z
P(yi|x, D) · P(x)dx.
(2.17)
Returning to the initial assumption in (2.15), we have
P(yi|x, D) = Const · exp
 1
2σ2∥Dx −yi∥2

.
(2.18)
The prior distribution of the representation vector x is assumed to be such that the
entries of x are zero-mean iid, with Cauchy [81] or Laplace distributions [71, 82].
Assuming for example a Laplace distribution we get
P(yi|D)
=
Z
P(yi|x, D) · P(x)dx
(2.19)
=
Const ·
Z
exp
 1
2σ2∥Dx −yi∥2

· exp {λ∥x∥1} dx.
This integration over x is diﬃcult to evaluate, and indeed, Olshausen and Field
[82] handled this by replacing it with the extremal value of P(yi, x|D). The overall
problem turns into
D
=
arg max
D
ΠN
i=1 max
xi {P(yi, xi|D)}
(2.20)
=
arg min
D
N
X
i=1
min
xi

∥Dxi −yi∥2 + λ∥xi∥1
	
.

22
This problem does not penalize the entries of D as it does for the ones of xi.
Thus, the solution will tend to increase the dictionary entries’ values, in order to
allow the coeﬃcients to become closer to zero. This diﬃculty has been handled
by constraining the ℓ2-norm of each basis element, so that the output variance of
the coeﬃcients is kept at an appropriate level [81].
An iterative method was suggested for solving (2.20). It includes two main
steps in each iteration: (i) calculate the coeﬃcients xi using a simple gradient
descent procedure; and then (ii) update the dictionary using [81]
D(n+1) = D(n) −η
N
X
i=1
(D(n)xi −yi)xT
i .
(2.21)
This idea of iterative reﬁnement, mentioned before as a generalization of the K-
Means algorithm, was later used again by other researchers, with some variations
[40, 65, 39, 41, 80].
A diﬀerent approach to handle the integration in (2.19) was suggested by
Lewicki and Sejnowski [72]. They approximated the posterior as a Gaussian, en-
abling an analytic solution of the integration. This allows an objective comparison
of diﬀerent image models (basis or priors). It also removes the need for the addi-
tional re-scaling that enforces the norm constraint. However, this model may be
too limited in describing the true behaviors expected. This technique and closely
related ones have been referred to as approximated ML techniques [65].
There is an interesting relation between the above method and the Inde-
pendent Component Analysis (ICA) algorithm [3]. The latter handles the case of
a complete dictionary (the number of elements equals the dimensionality) with-
out assuming additive noise. The above method is then similar to ICA in that
the algorithm can be interpreted as trying to maximize the mutual information
between the inputs (samples) and the outputs (the coeﬃcients) [81, 71, 72].

23
2.3.3
The MOD Method
An appealing dictionary training algorithm, named Method of Optimal Di-
rections (MOD), is presented by Engan et. al. [40, 39, 41]. This method follows
more closely the K-Means outline, with a sparse coding stage that uses either
OMP or FOCUSS followed by an update of the dictionary. The main contribu-
tion of the MOD method is its simple and eﬃcient way of updating the dictionary.
Assuming that the sparse coding for each example is known, we deﬁne the errors
ei = yi −Dxi. The overall representation mean square error is given by
∥E∥2
F = ∥[e1, e2, . . . , eN]∥2
F = ∥Y −DX∥2
F.
(2.22)
Here we have concatenated all the examples yi as columns of the matrix Y, and
similarly gathered the representations coeﬃcient vectors xi to build the matrix X.
The notation ∥A∥F stands for the Frobenius Norm, deﬁned as ∥A∥F =
qP
ij A2
ij.
Assuming that X is ﬁxed, we can seek an update to D such that the above
error is minimized. Taking the derivative of (2.22) with respect to D we obtain
the relation (Y −DX)XT = 0, leading to
D(n+1) = YX(n)T · (X(n)X(n)T)−1
(2.23)
MOD is closely related to the work by Olshausen and Field, with improve-
ments both in the sparse coding and the dictionary update stages. Whereas the
work in [82, 81, 71] applies a steepest descent to evaluate xi, those are evaluated
much more eﬃciently with either OMP or FOCUSS. Similarly, in updating the
dictionary, the update relation given in (2.23) is the best that can be achieved for
ﬁxed X. The iterative steepest descent update in (2.21) is far slower. Interest-
ingly, in both stages of the algorithm, the diﬀerence is in deploying a second order

24
(Newtonian) update instead of a ﬁrst-order one. Looking closely at the update
relation in (2.21), it could be written as
D(n+1)
=
D(n) + ηEX(n)T
(2.24)
=
D(n) + η(Y −D(n)X(n))X(n)T = D(n)(I −ηX(n)X(n)T) + ηYX(n)T.
Using inﬁnitely many iterations of this sort, and using small enough η, this leads
to a steady state outcome, which is exactly the MOD update matrix (2.23). Thus,
while the MOD method assumes known coeﬃcients at each iteration, and derives
the best possible dictionary, the ML method by Olshausen and Field only gets
closer to this best current solution, and then turns to calculate the coeﬃcients.
Note, however, that in both methods a normalization of the dictionary columns
is required and done.
2.3.4
Maximum A-posteriori Probability Approach
The same researchers that conceived the MOD method also suggested a
maximum a-posteriori probability (MAP) setting for the training of dictionaries,
attempting to merge the eﬃciency of the MOD with a natural way to take into
account preferences in the recovered dictionary. In [65, 41, 80, 66] a probabilis-
tic point of view is adopted, very similar to the ML methods discussed above.
However, rather than working with the likelihood function P(Y|D), the posterior
P(D|Y) is used. Using Bayes rule, we have P(D|Y) ∝P(Y|D)P(D), and thus
we can use the likelihood expression as before, and add a prior on the dictionary
as a new ingredient.
These works considered several priors P(D) and proposed corresponding
formulas for the dictionary update stage. The eﬃciency of the MOD in these
methods is manifested in the eﬃcient sparse coding, which is carried out with

25
FOCUSS. The proposed algorithms in this family deliberately avoid a direct min-
imization with respect to D as in MOD, due to the prohibitive n × n matrix
inversion required. Instead, iterative gradient descent is used.
When no prior is chosen, the update formula is the very one used by Ol-
shausen and Field, as in (2.21). A prior that constrains D to have a unit Frobenius
norm leads to the update formula
D(n+1) = D(n) + ηEXT + η · tr
 XETD(n)
D(n).
(2.25)
As can be seen, the ﬁrst two terms are the same ones as in (2.21). The last term
compensates for deviations from the constraint. This case allows diﬀerent columns
in D to have diﬀerent norm values. As a consequence, columns with small norm
values tend to be under-used, as the coeﬃcients they need are larger and as such
more penalized.
This shortcoming led to the second prior choice, constraining the columns
of D to have a unit ℓ2-norm. The new update equation formed is given by
d(n+1)
i
= d(n)
i
+ η

I −d(n)
i d(n)
i
T
E · xT
i ,
(2.26)
where xT
i is the i-th column in the matrix XT.
Compared to the MOD, this line of work provides slower training algorithms.
Simulations reported in [65, 41, 80, 66] on synthetic and real image data seem to
provide encouraging results.
2.3.5
Unions of Orthonormal Bases
The very recent work reported in [70] considers a dictionary composed as a
union of orthonormal bases
D = [D1, D2, ... , DL] ,

26
where Dj ∈Rn×n, j = 1, 2, . . . , L are orthonormal matrices. Such a dictionary
structure is quite restrictive, but its updating may potentially be made more
eﬃcient.
The coeﬃcients of the sparse representations X can be decomposed to L
pieces, each referring to a diﬀerent orthonormal-basis. Thus,
X =

XT
1 , XT
2 , ... , XT
L
T ,
where Xi is the matrix containing the coeﬃcients of the orthonormal dictionary
Di.
One of the major advantages of the union of orthonormal-bases is the rela-
tive simplicity of the pursuit algorithm needed for the sparse coding stage. The
coeﬃcients are found using the Block Coordinate Relaxation (BCR) algorithm
[93]. This is an appealing way to solve (P1,ϵ) as a sequence of simple shrinkage
steps, such that at each stage Xi is computed, while keeping all the other pieces
of X ﬁxed.
Assuming known coeﬃcients, the proposed algorithm updates each orthonor-
mal basis Dj sequentially. The update of Dj is done by ﬁrst computing the residual
matrix
Ej = [e1, e2, . . . , eN] = Y −
X
i̸=j
DiXi.
Then, by computing the singular value decomposition of the matrix EjXT
j =
UΛVT, the update of the j-th orthonormal-basis is done by Dj = UVT. This
update rule is obtained by solving a constrained least squares problem with ∥Ej −
DjXj∥2
F as the penalty term, assuming ﬁxed coeﬃcients Xj and error Ej. The
constraint is over the feasible matrices Dj, which are forced to be orthonormal.
This way the proposed algorithm improves each matrix Dj separately, by replacing

27
the role of the data matrix Y in the residual matrix Ej, as the latter should be
represented by this updated basis.
Compared to previously mentioned training algorithms, the work reported
in [70] is diﬀerent in two important ways; beyond the evident diﬀerence of using
a structured dictionary rather than a free one, a second major diﬀerence is in
the proposed sequential update of the dictionary. This update algorithm is rem-
iniscent of the updates done in the K-means. Interestingly, experimental results
reported in [70] show weak performance compared to previous methods.
This
might be explained by the unfavorable coupling of the dictionary parts and their
corresponding coeﬃcients, which is overlooked in the update.
2.3.6
Summary of the Prior Art
Almost all previous methods can essentially be interpreted as generalizations
of the K-Means algorithm, and yet, there are marked diﬀerences between these
procedures. In the quest for a successful dictionary training algorithm, there are
several desirable properties:
• Flexibility: The algorithm should be able to run with any pursuit algo-
rithm, and this way enable choosing the one adequate for the run-time
constraints, or the one planned for future usage in conjunction with the
obtained dictionary. Methods that decouple the sparse-coding stage from
the dictionary update readily have such a property. Such is the case with
the MOD and the MAP based methods.
• Simplicity: Much of the appeal of a proposed dictionary training method
has to do with how simple it is, and more speciﬁcally, how similar it is
to K-Means. We should have an algorithm that may be regarded as a

28
natural generalization of the K-Means. The algorithm should emulate the
ease with which the K-Means is explainable and implementable. Again,
the MOD seems to have made a substantial progress in this direction,
although, as we shall see, there is still room for improvement.
• Eﬃciency: The proposed algorithm should be numerically eﬃcient and
exhibit fast convergence. The above described methods are all quite slow.
The MOD, which has a second-order update formula, is nearly imprac-
tical for very large number of dictionary columns, because of the matrix
inversion step involved. Also, in all the above formulations, the dictionary
columns are updated before turning to re-evaluate the coeﬃcients. As we
shall see later, this approach inﬂicts a severe limitation on the training
speed.
• Well Deﬁned Objective: For a method to succeed, it should have a well
deﬁned objective function that measures the quality of the solution ob-
tained. This almost trivial fact was overlooked in some of the preceding
work in this ﬁeld. Hence, even though an algorithm can be designed to
greedily improve the representation MSE and the sparsity, it may hap-
pen that the algorithm leads to aimless oscillations in terms of a global
objective measure of quality.

Chapter 3
Uniqueness of the Dictionary
3.1
Posing the Problem
We are given a set of signals, each of which is known to be a sparse linear
combination of atoms from some dictionary, and consider the dictionary search
problem, namely, the extraction of a dictionary that can exactly represent each
signal sparsely. Formally, the signals are arranged as columns of the matrix Y ∈
Rn×N (n ≪N), such that Y = DX where D ∈Rn×K, and each column in X
obeys some known sparsity constraint. Our focus in this section is on whether
such a dictionary is unique, and more generally, whether the factorization of the
matrix Y under these known sparsity constraints is unique. We did not ﬁnd any
explicit reference to this question, and yet, such uniqueness is implicitly assumed
in previous works (e.g., the synthetic experiments in [88, 1] as also in Section
4.2.5).
3.2
Statement of the Uniqueness Result
We now pose a set of assumptions that are mandatory for the uniqueness
result we are about to prove. We assume the following:

30
(1) Support: The support of all representation vectors in X (its columns)
satisfy
∀1 ≤i ≤N,
∥xi∥0 = L < σ{D}
2
,
(3.1)
where σ{D} is the spark of D, deﬁned before in Section 2.2.1. Further-
more, we assume that L is known. Both the knowledge of L, and the fact
that the representations are assumed to have exactly L non-zeros (and
not less) can be easily relaxed. Still, these assumptions are posed for the
sake of simplicity of the proof.
(2) Richness: The set of examples in Y includes at least L + 1 signals for
every possible combination of L atoms in D. Thus, we assume that Y
includes at least (L+1)
 K
L

signals. This assumption will later be relaxed,
and again, it is posed mainly for simplifying the later analysis.
(3) Non-Degeneracy: Given a set of L + 1 signals that are built of the
same L atoms, their rank is expected to be L or less. We assume that
any such set leads to a rank L and not less.
Similarly, for any set of
L + 1 signals which are not generated by the same L atoms, we assume
that the rank of such set is necessarily L + 1. Both these assumptions
mean that no degeneracies in the construction of the signals are allowed.
These requirements, although seemingly complicated, only reﬂect the fact
that there are no degenerate ‘coincidences’ in the signals construction.
Analogously, considering the signals to be generated with L randomly
chosen coeﬃcients in X, these degeneracies are of zero probability.
Based on these assumptions we have the following result:

31
Theorem 1: Under the above assumptions, the factorization of Y is unique, i.e.,
the factorization Y = DX for which (i) D ∈Rn×K with normalized atoms; and
(ii) X ∈RK×N with L non-zeros in each column, is unique. This uniqueness is
up to a right-multiplication of D by a signed permutation matrix, which does not
change the desired properties of D and X.
3.3
The Proof
The proof we provide is constructive (although far from being a practical
method to deploy in practice), leading to a pair ˆD and ˆX. Clearly, it is suﬃcient to
prove an equivalence between ˆD and D (up to a signed permutation) to guarantee
a unique factorization. Given the vector yi and a dictionary D, our assumptions
imply that a solution to yi = Dxi exists with exactly L non-zeros. Since L < σ{D}
2
,
this is the sparsest possible solution and as such it is unique, due to Theorem 1 in
2.2.1. Thus, having found D, solving a set of (P0) problems we necessarily recover
the original X. Permutations and sign changes do not impact this property, and
only change the locations and signs of the entries in X to match the columns in
D. Thus, it is suﬃcient to consider the relation between the original dictionary
D and the recovered one, ˆD.
Next, we describe a coherent process with three stages that leads to ˆD, and
show that it necessarily matches the original D. The basic steps of this process are
(i) Divide the columns of Y into J =
 K
L

sets – {G1, G2, ..., GJ} – each includes
all the signals that share the same support (i.e., use the same L atoms from D,
denoted as Ωj for j = 1, 2, . . . , J); (ii) Detect pairs of sets Gi and Gj that share
exactly one mutual atom; and (iii) Extract this mutual atom and form ˆD, which
necessarily matches the original D. Furthermore, we will expand on each of these

32
steps.
3.3.1
Stage 1: Clustering The Signals
Due to our earlier support assumption, L < σ{D}/2, and the deﬁnition of
the spark, every set of L atoms from D is necessarily linearly independent and
as such spans an L-dimensional subspace. In this stage we identify those
 K
L

subspaces, and divide the signals (columns) in Y according to their embedding
subspaces.
The clustering of the columns of Y can be done by ﬁrst testing the rank of
all
  K
L+1

= (K −L) · J/(L + 1) > J sets of (L + 1)-tuples from Y. If the rank of
such a set is L + 1, it implies that this set of signals do not belong to the same
subspace, and as such it is discarded from further consideration. If the rank equals
L, it means we have found L+1 signals that belong to one of the subspaces related
to a set of atoms Sj. The richness assumption assures that such L+1 signals exist
per each of the J subspaces, and the non-degeneracy assumption prevents a set
of L + 1 signals that do not belong to the same subspace to give a rank L, and
pose as a feasible subspace. Thus, we expect to detect exactly J such successful
sets of signals from Y, and those, denoted by {Gj}J
j=1, will serve as the seeds for
the overall clustering. Note that the non-degeneracy assumption also implies that
the rank of an arbitrary set of L + 1 columns cannot be smaller than L.
Having found the seed for each of the J sets, {Gj}J
j=1, we now sweep through
all the columns in Y that are not assigned yet, and combine them to one of these
J sets, testing again the rank. Due to the non-degeneracy assumption, only one
set will give a rank L, implying that the tested column belongs to this subspace.
All other tests necessarily lead to a rank L + 1.

33
Using the above procedure, we divide all signals in Y into
 K
L

sets, each
of which includes signals that are generated by the same set of L atoms in the
original dictionary D. The support and richness assumptions ensures that each
such subspace will eventually be identiﬁed (when testing its corresponding L + 1
signals). The non-degeneracy assumption ensures that no L + 1 signals will be
mapped into the same set if they were not initially generated by the same L atoms.
3.3.2
Stage 2: Detecting Pairs With Mutual Atom
Given the J sets of signals {Gj}J
j=1, we now test the rank of all J(J −1)/2
merged pairs (order plays no role). Every two such merged sets, Gj1 and Gj2, are
leaning on two sets of atoms from D, Ωj1 and Ωj2. If the intersection between
these two sets is empty, |Ωj1 ∩Ωj2| = 0, then the rank is necessarily 2L. It cannot
be higher as each of the sets have a rank L, and cannot be smaller due to the
non-degeneracy assumption and the fact that 2L < σ{D}. If the intersection
includes one atom, |Ωj1 ∩Ωj2| = 1, the rank is necessarily 2L −1 for the same
reasons. Getting a rank smaller than 2L −1 means a larger intersection. Thus,
we take only those pairs that lead to rank 2L −1.
Let us consider the ﬁrst atom as the single interaction between pairs {Gj1, Gj2}.
How many such pairs will be found? Putting this atom aside, we remain with
K −1 atoms from which we have to choose 2L −2 atoms to participate in the
construction of the two sets. These should be divided into two sets of L−1 atoms
each, and only half count (order, as before, is redundant). Therefore, we have
M = 0.5 ·
 K−1
2L−2
 2L−2
L−1

such pairs that intersect only on the ﬁrst atom. Thus,
every atom in the original dictionary D can and will be found many times, calling
for a pruning. Both the evaluation of this atom from the intersection and the

34
pruning are parts of the next and last stage.
3.3.3
Stage 3: Extracting the Mutual Atom
Assume that we have the pair of sets {Gj1, Gj2} known to intersect on one
atom. Taking an arbitrary L signals from Gj1, they span the same L-dimensional
subspace as the atoms in Ωj1.
Thus, gathering these L signals into a matrix
Yj1 ∈Rn×L, there exists a vector v1 ∈RL×1 such that Yj1v1 is parallel to the
desired intersection atom. Similarly, taking L arbitrary members from Gj2 and
forming the matrix Yj2 ∈Rn×L, there exists a vector v2 ∈RL×1 such that Yj2v2
is parallel to the same intersection atom. Thus, we have the relationship
Yj1v1 = Yj2v2,
(3.2)
or, posed diﬀerently, this relationship leads to the homogeneous linear system of
equations
[Yj1, −Yj2] v = 0,
(3.3)
where v is a vertical concatenation of v1 and v2. The constructed matrix has n
rows and 2L columns, but we already know that its rank is 2L −1 due to the
intersection. A vector v in its null-space leads to the desired v1 and v2, and they
can be obtained as the last right singular vector in an SVD operation [51]. Having
found v1, the term Yj1v1 stands for the desired intersection atom, being parallel
to a true atom found in D up to a scalar multiplication.
Repeating the above process for each pair with a single atom intersection,
we obtain M · K candidate atoms. Starting with the ﬁrst, we sweep through this
set and seek all others that are parallel to it, pruning them. This process proceeds

35
for the second, third, and till we remain with only K atoms. These are the desired
atoms, being the columns of the original dictionary D.
3.3.4
Summarizing the Proof
Up until now we have presented a constructive algorithm for the extraction
of the dictionary D that was used in the construction of Y.
Indeed, in the
algorithm described there are multiple possibilities of choosing the pairs in the
second stage, as well as choosing the L elements that construct the matrices Yj1
and Yj2 in the third stage. Nevertheless, all possible choices lead to the same
solution D up to simple transformations. Thus, the matrix Y, created as the
product DX, is factorized as desired.
Could there be a diﬀerent feasible factorization? Let us assume that there
exists such a second diﬀerent factorization Y = eDeX. Executing the above al-
gorithm on Y must lead to the matrix eD, due to the constructive method we
have developed. On the other hand, this algorithm must also lead to D for the
same reasons. Thus, we necessarily conclude that eD must be equivalent to D,
and therefore the factorization is unique.
2
3.4
Summary
While correct, the above uniqueness proof is totally impractical. The daunt-
ing number of signals required, as well as the computational cost, prevent this
algorithm from being practical. However, keeping this method in mind, the re-
quirement on the number of signals can be relaxed, leading to a reduced com-
putational complexity. We have seen that in the algorithm developed there are
severe redundancies in building the atoms. How much lower could the number

36
of examples go and still lead to a successful factorization? As an example that
illustrates the possibilities, we could take exactly L+1 examples per each set, but
consider only 2K such sets. If those sets are chosen smartly to divide into pairs
that overlap on each of the K atoms, this would be suﬃcient for the success of
the algorithm. Thus, 2K(L + 1) signal examples could in principle be used. In
fact, even such a set could be found redundant, due to the ability to cross pairs
diﬀerently and exploit other overlaps.
However, this relaxation still prevents this method from being applied in
practice, as the algorithm still requires a sweep through all combinations of L
signals out of the whole set. Yet, it raises the theoretical question concerning the
minimal number of signals that can guarantee uniqueness of the dictionary under
sparsity constraints – a question we leave open at this stage.
Up until now, we discussed the case of an exact representation of the signals.
This constraint, by itself, is not expected to be held in practice, where some
representation error is always allowed.
In such a case, the above proof, even
under the most restrictive conditions, does not hold. We believe uniqueness in
the case of an approximated representation no longer exists, but stability does.
That is, the smaller the allowed representation error, and the larger the set of
given signals, the more we can expect the extracted dictionary to resemble the
original one. Yet, such a proof is not trivial and left here for future work.

Chapter 4
The K-SVD Algorithm
In this section we introduce the K-SVD algorithm for training of dictionaries.
This algorithm is ﬂexible and works in conjunction with any pursuit algorithm.
It is simple and designed to be a truly direct generalization of the K-Means.
As such, when forced to work with one atom per signal, it trains a dictionary
for the Gain-Shape VQ. When forced to have a unit coeﬃcient for this atom,
it exactly reproduces the K-Means algorithm.
The K-SVD is highly eﬃcient,
due to an eﬀective sparse coding, and a Gauss-Seidel-like accelerated dictionary
update method. The algorithm’s steps are coherent with each other, both working
towards the minimization of a clear overall objective function.
4.1
K-Means Algorithm for Vector Quantization
We start our discussion with a description of the K-Means, setting the no-
tation for the rest of this section. While this may seem superﬂuous, we will use
the very description of the K-Means to derive the K-SVD as its direct extension.
We then discuss some of the K-SVD properties and implementation issues.
A codebook that includes K codewords (representatives) is used to represent
a wide family of vectors (signals) Y = {yi}N
i=1 (N ≫K) by nearest neighbor

38
assignment. This leads to an eﬃcient compression or description of those signals,
as clusters in Rn surrounding the chosen codewords. As a side note we remind
the reader that based on the expectation maximization procedure, the K-Means
can be extended to suggest a fuzzy assignment and a covariance matrix per each
cluster, so that the data is modeled as a mixture of Gaussians [17].
The dictionary of VQ codewords is typically trained using the K-Means al-
gorithm, and as we have argued before, this has a close resemblance to the problem
studied in this thesis. We denote the codebook matrix by C = [c1, c2, . . . , cK],
the codewords being the columns. When C is given, each signal is represented
as its closest codeword (under ℓ2-norm distance). We can write yi = Cxi, where
xi = ej is a vector from the trivial basis, with all zero entries except a one in the
j-th position. The index j is selected such that
∀k ̸= j ∥yi −Cej∥2
2 ≤∥yi −Cek∥2
2.
(4.1)
This is considered as an extreme case of sparse coding in the sense that only one
atom is allowed to participate in the construction of yi, and the coeﬃcient is
forced to be 1. The representation MSE per yi is deﬁned as e2
i = ∥yi −Cxi∥2
2,
and the overall MSE is
E =
K
X
i=1
e2
i = ∥Y −CX∥2
F.
(4.2)
The VQ training problem is to ﬁnd a codebook C that minimizes the error E,
subject to the limited structure of X, whose columns must be taken from the
trivial basis,
min
C,X

∥Y −CX∥2
F
	
subject to
∀i, xi = ek for some k.
(4.3)
The K-Means algorithm is an iterative method used for designing the optimal
codebook for VQ [49]. In each iteration there are two stages - one for sparse

39
coding that essentially evaluates X, and one for updating the codebook. Figure
4.1 gives a more detailed description of these steps.
The sparse coding stage assumes a known codebook C(J−1), and computes a
feasible X that minimizes the value of (4.3). Similarly, the dictionary update stage
ﬁxes X as known, and seeks an update of C so as to minimize (4.3). Clearly, at
each iteration either a reduction or no change in the MSE is ensured. Furthermore,
at each such stage, the minimization step is optimal under the assumptions. Note
that we have deliberately chosen not to discuss stopping rules for the above-
described algorithm, since those vary a lot but are quite easy to handle [49].
4.2
The K-SVD Algorithm
4.2.1
K-SVD – Generalizing the K-Means
The sparse representation problem can be viewed as a generalization of
the VQ objective (4.3), in which we allow each input signal to be represented
by a linear combination of codewords, which we now call dictionary elements.
Therefore the coeﬃcients vector is now allowed more than one nonzero entry, and
these can have arbitrary values. For this case, the minimization corresponding
to Equation (4.3) generalizes to the search of the best possible dictionary for the
sparse representation of the example set Y, namely,
min
D,X

∥Y −DX∥2
F
	
subject to
∀i, ∥xi∥0 ≤T0.
(4.4)
A similar objective could be posed, considering
min
D,X
X
i
∥xi∥0
subject to
∥Y −DX∥2
F ≤ϵ,
(4.5)
for a ﬁxed value ϵ.
In this section we mainly discuss the ﬁrst problem (4.4),
although the treatment of the second formulation is very similar.

40
Task: Find the best possible codebook to represent the data samples {yi}N
i=1
by nearest neighbor, by solving
min
C,X

∥Y −CX∥2
F
	
subject to
∀i, xi = ek for some k.
Initialization : Set the codebook matrix C(0) ∈Rn×K. Set J = 1.
Repeat until convergence (use stop rule):
• Sparse Coding Stage: Partition the training samples Y into K
sets
(R(J−1)
1
, R(J−1)
2
, . . . , R(J−1)
K
),
each holding the sample indices most similar to the column
c(J−1)
k
,
R(J−1)
k
=
n
i | ∀l̸=k,
∥yi −c(J−1)
k
∥2 < ∥yi −c(J−1)
l
∥2
o
.
• Codebook Update Stage: For each column k in C(J−1), update
it by
c(J)
k
=
1
|Rk|
X
i∈R(J−1)
k
yi.
• Set J = J + 1.
Figure 4.1: The K-Means Algorithm

41
In our algorithm we minimize the expression in (4.4) iteratively. First, we
ﬁx D and aim to ﬁnd the best coeﬃcient matrix X that can be found. As ﬁnding
the truly optimal X is impossible, we use an approximation pursuit method. Any
such algorithm can be used for the calculation of the coeﬃcients, as long as it can
supply a solution with a ﬁxed and predetermined number of nonzero entries, T0.
Once the sparse coding task is done, a second stage is performed to search
for a better dictionary. This process updates one column at a time, ﬁxing all
columns in D except one, dk, and ﬁnding a new column ˜dk and new values for
its coeﬃcients that best reduce the MSE. This is markedly diﬀerent from all the
K-Means generalizations that were described in Section 2.3. All those methods
freeze X while ﬁnding a better D. Our approach is diﬀerent, as we change the
columns of D sequentially, and allow changing the relevant coeﬃcients as well. In
a sense, this approach is a more direct generalization of the K-Means algorithm,
because it updates each column separately, as done in K-Means. One may argue
that in K-Means the nonzero entries in X are ﬁxed during the improvement of ck,
but as we shall see next, this is true because in the K-Means (and the gain-shape
VQ), the column update problems are decoupled, whereas in the more general
setting this need not be the case.
The process of updating only one column of D at a time is a problem hav-
ing a straightforward solution based on the singular value decomposition (SVD).
Furthermore, allowing a change in the coeﬃcient values while updating the dictio-
nary columns accelerates convergence, since the subsequent column updates will
be based on more relevant coeﬃcients. The overall eﬀect is very much in line with
the leap from gradient descent to Gauss-Seidel methods in optimization.
Here one might be tempted to suggest skipping the step of sparse coding,

42
and using only updates of columns in D, along with their coeﬃcients, applied in a
cyclic fashion, again and again. This however will not work well, as the support of
the representations will never be changed, and such an algorithm will necessarily
fall into a low-quality solution.
4.2.2
K-SVD - Detailed Description
We shall now discuss the K-SVD in detail. Recall that our objective function
is
min
D,X

∥Y −DX∥2
F
	
subject to
∀i, ∥xi∥0 ≤T0.
(4.6)
Let us ﬁrst consider the sparse coding stage, where we assume that D is ﬁxed,
and consider the above optimization problem as a search for sparse representations
with coeﬃcients summarized in the matrix X. The penalty term can be rewritten
as
∥Y −DX∥2
F =
N
X
i=1
∥yi −Dxi∥2
2.
Therefore the problem posed in (4.6) can be decoupled to N distinct problems of
the form
i = 1, 2, . . . , N,
min
xi

∥yi −Dxi∥2
2
	
subject to
∥xi∥0 ≤T0.
(4.7)
This problem is adequately addressed by the pursuit algorithms discussed in Sec-
tion 2.1, and we have seen that if T0 is small enough, their solution is a good
approximation to the ideal one that is numerically infeasible to compute directly.
We now turn to the second, and slightly more involved process of updating
the dictionary together with the nonzero coeﬃcients. Assume that both X and D
are ﬁxed, and we put in question only one column in the dictionary, dk, and the

43
coeﬃcients that correspond to it, the k-th row in X, denoted as xk
T (this is not
the vector xk which is the k-th column in X). Returning to the objective function
(4.6), the penalty term can be rewritten as
∥Y −DX∥2
F
=
Y −
K
X
j=1
djxj
T

2
F
(4.8)
=

 
Y −
X
j̸=k
djxj
T
!
−dkxk
T

2
F
=
Ek −dkxk
T
2
F .
We have decomposed the multiplication DX to the sum of K rank-1 matrices.
Among those, K −1 terms are assumed ﬁxed, and one – the k-th – remains in
question. The matrix Ek stands for the error for all the N examples when the k-th
atom is removed. Note the resemblance between this error and the one deﬁned in
[70].
Here, it would be tempting to suggest the use of the SVD to ﬁnd alternative
dk and xk
T. The SVD ﬁnds the closest rank-1 matrix (in Frobenius norm) that
approximates Ek, and this will eﬀectively minimize the error as deﬁned in (4.8).
However, such a step will be a mistake, because the new vector xk
T is very likely to
be ﬁlled, since in such an update of dk we do not enforce the sparsity constraint.
A remedy to the above problem, however, is simple and also quite intuitive.
Deﬁne ωk as the set of indices pointing to examples {yi} that use the atom dk,
i.e., those where xk
T (i) is nonzero. Thus,
ωk = {i| 1 ≤i ≤K, xk
T(i) ̸= 0}.
(4.9)
Deﬁne Ωk as a matrix of size N × |ωk|, with ones on the (ωk(i), i)-th entries,
and zeros elsewhere. When multiplying xk
R = xk
TΩk, this shrinks the row vector
xk
T by discarding of the zero entries, resulting with the row vector xk
R of length

44
|ωk|. Similarly, the multiplication YR
k = YΩk creates a matrix of size n × |ωk|
that includes a subset of the examples that are currently using the dk atom. The
same eﬀect happens with ER
k = EkΩk, implying a selection of error columns that
correspond to examples that use the atom dk.
With this notation, we may now return to (4.8) and suggest minimization
with respect to both dk and xk
T, but this time force the solution of ˜xk
T to have the
same support as the original xk
T . This is equivalent to the minimization of
EkΩk −dkxk
TΩk
2
F =
ER
k −dkxk
R
2
F ,
(4.10)
and this time it can be done directly via SVD. In fact, the above simply takes
the error as appears in Equation (4.8) and splits it to two additive parts - the
ﬁrst that refers to the examples using the k-th atom, and the second that refers
to the remaining ones. The above change suggests to minimize only the error for
the ﬁrst set of examples.
Taking the restricted matrix ER
k , an SVD operation decomposes it to ER
k =
U∆VT. We deﬁne the solution for ˜dk as the ﬁrst column of U, and the coeﬃcient
vector xk
R as the ﬁrst column of V multiplied by ∆(1, 1). Note that in this solution
we necessarily have that (i) the columns of D remain normalized; and (ii) the
support of all representations either stays the same or gets smaller by possible
nulling of terms. Alternatively to the SVD computation, a few iterations of the
following procedure, followed by the scaling of the two vectors for normalization
of dk can derive the same solution,
˜dk = ER
k xk
R
T
xk
R · xk
R
T ,
xk
R =
˜dT
k ER
k
˜dT
k · ˜dk
.
(4.11)
We call this algorithm the “K-SVD” to parallel the name K-Means. While
K-Means applies K computations of mean (averaging) to update the codebook, the

45
K-SVD obtains the updated dictionary by K SVD computations, each determining
one column. A full description of the algorithm is given in Figure 4.2.
In the K-SVD algorithm we sweep through the columns and use always
the most updated coeﬃcients as they emerge from preceding SVD steps. Parallel
versions of this algorithm can also be considered, where all updates of the previous
dictionary are done based on the same X.
Experiments show that while this
version also converges, it yields an inferior solution, and typically requires more
than 4 times the number of iterations.
An important question that arises is whether the K-SVD converges. Let us
ﬁrst assume we can perform the sparse coding stage perfectly, retrieving the best
approximation to the signal yi that contains no more than T0 nonzero entries. In
this case, and assuming a ﬁxed dictionary D, each sparse coding step decreases the
total representation error ∥Y −DX∥2
F, posed in (4.6). Moreover, at the update
step for dk, an additional reduction or no change in the MSE is guaranteed, while
not violating the sparsity constraint. Executing a series of such steps ensures a
monotonic MSE reduction, and therefore, a convergence of the algorithm.
Unfortunately, the above claim depends on the success of pursuit algorithms
to robustly approximate the solution to (4.7), and thus convergence is not always
guaranteed. However, when T0 is small enough relative to n, the OMP, the FO-
CUSS, and the BP approximating methods are known to perform very well1 . In
those circumstances the convergence is guaranteed. We can also ensure conver-
gence by external interference - by comparing the best solution using the already
given support to the one proposed by the new run of the pursuit algorithm, and
1 While OMP can be naturally used to get a ﬁxed and pre-determined number of non-zeros
(T0), both BP and FOCUSS require some slight modiﬁcations. For example, in using FOCUSS
to derive exactly T0 non-zero coeﬃcients, the regularization parameter should be adapted while
iterating.

46
Task: Find the best dictionary to represent the data samples {yi}N
i=1 as sparse com-
positions, by solving
min
D,X

∥Y −DX∥2
F
	
subject to
∀i, ∥xi∥0 ≤T0.
Initialization : Set the dictionary matrix D(0) ∈Rn×K with ℓ2 normalized columns.
Set J = 1.
Repeat until convergence (stopping rule):
• Sparse Coding Stage: Use any pursuit algorithm to compute the repre-
sentation vectors xi for each example yi, by approximating the solution
of
i = 1, 2, . . . , N,
min
xi

∥yi −Dxi∥2
2
	
subject to
∥xi∥0 ≤T0.
• Codebook Update Stage: For each column k = 1, 2, . . . , K in D(J−1),
update it by
∗Deﬁne the set of examples that use this atom, ωk = {i| 1 ≤i ≤
N, xk
T (i) ̸= 0}.
∗Compute the overall representation error matrix, Ek, by
Ek = Y −
X
j̸=k
djxj
T .
∗Restrict Ek by choosing only the columns corresponding to ωk, and
obtain ER
k .
∗Apply SVD decomposition ER
k = U∆VT .
Choose the updated
dictionary column ˜dk to be the ﬁrst column of U.
Update the
coeﬃcient vector xk
R to be the ﬁrst column of V multiplied by
∆(1, 1).
• Set J = J + 1.
Figure 4.2: The K-SVD Algorithm.

47
adopting the better one. This way we shall always get an improvement. Practi-
cally, we saw in all our experiments that a convergence is reached, and there was
no need for such external interference.
Yet, convergence of the algorithm is not guaranteed to be to a local minimum
solution, for which each perturbation of one or more of the variables necessarily
derives an increased value of the cost function (Equation 4.6). The K-SVD, just
like the K-Means algorithm, is not guaranteed to converge to such a solution, and
may well be trapped in a stable point of the numerical algorithm. An example of
such a stable point is the following. Lets assume that the dictionary is initialized
such that all signals are better represented using only K −1 out of the available
K atoms. Such a case deﬁnitely results convergence to a sub-optimal solution
(the unused atom can be changed to better represent at least one signal without
increasing the size of the support). Several steps to avoid such unwanted solutions
are described later in Section 4.2.4. Yet still, avoiding them completely is not
promised.
To summarize the K-SVD description, we discuss its computational com-
plexity. We should consider both stages - sparse coding and dictionary update.
The complexity of the sparse coding stage depends on the selected pursuit method.
When using the OMP and neglecting the least-squares steps within, each coeﬃcent
for each signal can be found in O(nK) operations, which results with O(nNLK)
operations for the whole sparse coding stage. In the dictionary update stage there
are K updates. The average number of signals that use each atom is NL/K,
which results with an error matrix of size n × NL/K on average. Considering the
iterative method for ﬁnding the rank-1 solution, this leads to the requirement for
O(nNL/K) operations, which results a total of O(nNL) operations for the whole

48
stage, if implemented smartly (special data structures should be formed in order
to retrieve the relevant signals and coeﬃcients without additional costs). When
considering the MOD algorithm described in Section 2.3, each dictionary update
stage requires an inversion of a K × K matrix, as also O(K2N) operations for
matrix multiplications, which results a total of O(K2N) operations.
Combining the two stages, the full K-SVD algorithm requires O(nNLK)
operations for each iteration. Clearly, acceleration of the algorithm can be done by
replacing the pursuit method, as being the most expensive stage in the algorithm.
The number of iterations until convergence depends on the application and on the
initial dictionary (see Section 4.2.5).
4.2.3
From K-SVD Back to K-Means
What happens when we force T0 = 1? This case corresponds to the gain-
shape VQ, and as such, it is important as the K-SVD becomes a method for its
codebook training. When T0 = 1 the coeﬃcient matrix X has only one nonzero
entry per column. Thus, computing the error ER
k in (4.10), yields
ER
k = EkΩk =
 
Y −
X
j̸=k
djxj
T
!
Ωk = YΩk = YR
k .
(4.12)
This is because the restriction Ωk takes only those columns in Ek that use the
dk atom, and thus necessarily, they use no other atoms, implying that for all j,
xj
TΩk = 0.
The implication of the above outcome is that the SVD in the T0 = 1 case is
done directly on the set of examples in ωk. Also, the K updates of the columns of
D become independent of each other, implying that a sequential process as before,
or a parallel one, both lead to the same algorithm. We mentioned before that

49
the K-Means update of the cluster centroids could be interpreted as a sequential
process, and the discussion here sheds some further light on this interpretation.
We could further constrain our representation stage and, beyond the choice
T0 = 1, limit the nonzero entries of X to be 1. This brings us back to the classical
clustering problem as described earlier. In this case we have that xk
R is ﬁlled with
ones, thus xk
R = 1T. The K-SVD then needs to approximate the restricted error
matrix ER
k = YR
k by a rank-1 matrix dk · 1T. The solution is the mean of the
columns of YR
k , exactly as K-Means suggests.
4.2.4
K-SVD - Implementation Details
Just like the K-Means, the K-SVD algorithm is susceptible to sub-optimal
solutions.
Our experiments show that improved results can be reached if the
following variations are applied:
• When using approximation methods with a ﬁxed number of coeﬃcients,
we found that FOCUSS proves to be the best in terms of getting the best
out of each iteration. However, from a run-time point of view, OMP was
found to lead to far more eﬃcient overall algorithm.
• When a dictionary element is not being used ‘enough’ (relative to the
number of dictionary elements and to the number of samples) it could be
replaced with the least-represented signal element, after being normalized
(the representation is measured without the dictionary element that is
going to be replaced). Since the number of data elements is much larger
than the number of dictionary elements, and since our model assumption
suggests that the dictionary atoms are of equal importance, such replace-
ment is very eﬀective in avoiding local minima and over-ﬁtting.

50
• Similar to the idea of removal of unpopular elements from the dictionary,
we found that it is very eﬀective to prune the dictionary from having too-
close elements. If indeed such a pair of atoms is found (based on their
absolute inner product exceeding some threshold), one of those elements
should be removed and replaced with the least-represented signal element.
Similarly to the K-Means, we can propose a variety of techniques to further im-
prove the K-SVD algorithm. Most appealing on this list are multiscale approaches,
and tree-based training where the number of columns K is allowed to increase
during the algorithm. Initial trial of adapting the K-SVD for training multiscale
dictionary is presented in Section 6.4.2.
4.2.5
Synthetic Experiments
As in previously reported works [65, 70], we ﬁrst try the K-SVD algorithm
on synthetic signals, to test whether this algorithm recovers the original dictionary
that generated the data, and to compare its results with other reported algorithms.
The experiment we describe follows the following steps:
• Generation of the data to train on: A random matrix D (referred
to later-on as the generating dictionary) of size 20 × 50 was generated
with iid uniformly distributed entries. Each column was normalized to
a unit ℓ2-norm. Then, 1500 data signals {yi}1500
i=1 of dimension 20 were
produced, each created by a linear combination of 3 diﬀerent generating
dictionary atoms, with uniformly distributed iid coeﬃcients in random
and independent locations. White Gaussian noise with varying SNR was
added to the resulting data signals.

51
• Applying the K-SVD: The dictionary was initialized with 50 data sig-
nals. The coeﬃcients were found using OMP with a ﬁxed number of 3
coeﬃcients. The maximum number of iterations was set to 80.
• Comparison to other reported works: We implemented the MOD
algorithm, and applied it on the same data, using OMP with a ﬁxed
number of 3 coeﬃcients, and initializing in the same way. We executed
the MOD algorithm for a total number of 80 iterations. We also executed
the MAP-based algorithm of Kreutz-Delgado et. al. [65]2 . This algorithm
was executed as is, therefore using FOCUSS as its decomposition method.
Here, again, a maximum of 80 iterations were allowed.
• Results: The computed dictionary was compared against the known gen-
erating dictionary. This comparison was done by sweeping through the
columns of the generating dictionary, and ﬁnding the closest column in
the computed dictionary, measuring the distance via
1 −|dT
i ˜di|,
(4.13)
where di is a generating dictionary atom, and ˜di is its corresponding ele-
ment in the recovered dictionary. A distance less than 0.01 was considered
a success. All trials were repeated 50 times, and the number of successes
in each trial was computed. Figure 4.3 displays the results for the three
algorithms for noise levels of 10dB, 20dB, 30dB and for the noiseless
case. We should note that for diﬀerent dictionary size (e.g., 20 × 30) and
with more executed iterations, the MAP-based algorithm improves and
get closer to the K-SVD detection rates.
2 The authors of [65] have generously shared their software with us.

52
   10   
   20   
   30   
No Noise
15
20
25
30
35
40
45
50
KSVD
MOD
MAP−based
Figure 4.3: Synthetic results: for each of the tested algorithms and for each noise
level, 50 trials were conducted, and their results sorted. The graph labels represent
the mean number of detected atoms (out of 50) over the ordered tests in sets of
10 experiments.

53
4.3
Applications to Image Processing - Basic Results
We carried out several experiments on natural image data, trying to show
the practicality of the proposed algorithm and the general sparse coding theme.
We should emphasize that our tests here come only to prove the concept of using
such dictionaries with sparse representations. In the next chapter we introduce
a large-scale real problem for which we demonstrate the relevance of the K-SVD
and its high potential. The experiments we describe follow the following steps:
• Training Data: The training data was constructed as a set of 11, 000
examples of block patches of size 8 × 8 pixels, taken from a database of
face images (in various locations). A random collection of 500 such blocks,
sorted by their variance, is presented in Figure 4.4.
• Removal of the DC: Working with real image data we preferred that
all dictionary elements except one have a zero mean. The same measure
was practiced in previous work [82]. For this purpose, the mean value of
all patches was subtracted oﬀbefore applying the K-SVD algorithm.
• Running the K-SVD: We applied the K-SVD, training a dictionary
of size 64 × 441 for 80 iterations. The choice K = 441 came from our
attempt to compare the outcome to the overcomplete Haar dictionary of
the same size (see the following section). The coeﬃcients were computed
using OMP with a ﬁxed number of 10 coeﬃcients per patch. Note that
better performance can be obtained by switching to FOCUSS. We concen-
trated on OMP because of its simplicity and fast execution. The trained
dictionary is presented in Figure 4.5.

54
• Comparison Dictionaries: The trained dictionary was compared with
the overcomplete Haar dictionary which includes separable basis func-
tions, having steps of various sizes and in all locations (total of 441 ele-
ments). In addition, we build an overcomplete separable version of the
DCT dictionary by sampling the cosine wave in diﬀerent frequencies to
result with a total of 441 elements. The overcomplete Haar dictionary
and the overcomplete DCT dictionary are presented in Figures 4.6 and
4.7, respectively.
• Applications: We used the K-SVD results, denoted here as the learned
dictionary, for two diﬀerent applications on images. All tests were per-
formed on one face image which was not included in the training set. The
ﬁrst application is ﬁlling-in missing pixels: we deleted random pixels in
the image, and ﬁlled their values using the various dictionaries decompo-
sition. We then tested the compression potential of the learned dictionary
decomposition, and derived a rate-distortion graph. We hereafter describe
those experiments in more detail.
Figure 4.4: A collection of 500 random blocks that were used for training, sorted
by their variance.

55
Figure 4.5: The learned dictionary. The atoms are sorted in an ascending order
of their variance, and stretched to maximal range for display purposes.
Figure 4.6: The overcomplete separable Haar dictionary.

56
Figure 4.7: The overcomplete DCT dictionary.

57
4.3.1
Filling-In Missing Pixels
We chose one random full face image, which consists of 594 non-overlapping
blocks (none of which were used for training).
For each block, the following
procedure was conducted for r in the range {0.2, 0.9}:
(1) A fraction r of the pixels in each block, in random locations, was deleted
(set to zero).
(2) The coeﬃcients of the corrupted block under the learned dictionary, the
overcomplete Haar dictionary, and the overcomplete DCT dictionary were
found using OMP with an error bound of ∥0.02 · 1∥2, where 1 ∈Rn is a
vector of all ones3 , (allowing an error of ±5 gray-values in 8-bit images).
All projections in the OMP algorithm included only the non-corrupted
pixels, and for this purpose, the dictionary elements were normalized so
that the non-corrupted indices in each dictionary element have a unit
norm. The resulting coeﬃcient vector of the block B is denoted xB.
(3) The reconstructed block ˜B was chosen as ˜B = D · xB.
(4) The reconstruction error was set to:
q
∥B −˜B∥2
F/64 (64 is the number
of pixels in each block).
The mean reconstruction errors (for all blocks and all corruption rates) were com-
puted, and are displayed in Figure 4.8. Two corrupted images and their recon-
structions can be seen in Figure 4.9. As can be seen, higher quality recovery is
obtained using the learned dictionary.
3 The input image is scaled to the dynamic range [0, 1].

58
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
0
0.05
0.1
0.15
Ratio of Corrupted Pixels in Image
RMSE
K−SVD results
Overcomplete Haar Wavelet results
Overcomplete DCT results
Figure 4.8: The RMSE for 594 new blocks with missing pixels using the learned
dictionary, overcomplete Haar dictionary, and overcomplete DCT dictionary.

59
50 % missing pixels
Learned reconstruction
Average # coeffs: 4.0202
MAE: 0.012977
RMSE: 0.029204
Haar reconstruction
Average # coeffs: 4.7677
MAE: 0.022833
RMSE: 0.071107
OverComplete DCT reconstruction
Average # coeffs: 4.7694
MAE: 0.015719
RMSE: 0.037745
70 % missing pixels
Learned reconstruction
Average # coeffs: 3.5623
MAE: 0.020035
RMSE: 0.055643
Haar reconstruction
Average # coeffs: 3.9747
MAE: 0.032831
RMSE: 0.097571
OverComplete DCT reconstruction
Average # coeffs: 4.0539
MAE: 0.025001
RMSE: 0.063086
Figure 4.9: The corrupted image (left) with the missing pixels marked as points,
and the reconstructed results by the learned dictionary, the overcomplete Haar
dictionary, and the overcomplete DCT dictionary, respectively. The diﬀerent rows
are for 50% and 70% of missing pixels.

60
4.3.2
Compression
A compression comparison was conducted between the overcomplete learned
dictionary, the overcomplete Haar dictionary, and the overcomplete DCT dictio-
nary (as explain before), all of size 64 × 441. In addition, we compared to the
regular (unitary) DCT dictionary (used by the JPEG algorithm). The resulting
rate-distortion graph is presented in Figure 4.10. In this compression test, the
face image was partitioned (again) into 594 disjoint 8 × 8 blocks. All blocks were
coded in various rates (bits-per-pixel values), and the PSNR was measured. Let I
be the original image and ˜I be the coded image, combined by all the coded blocks.
We denote e2 as the mean squared error between I and ˜I, and
PSNR = 10 · log10
 1
e2

.
(4.14)
In each test we set an error goal e, and ﬁxed the number of bits-per-coeﬃcient
Q. For each such pair of parameters, all blocks were coded in order to achieve
the desired error goal, and the coeﬃcients were quantized to the desired number
of bits (uniform quantization, using upper and lower bounds for each coeﬃcient
in each dictionary based on the training set coeﬃcients). For the overcomplete
dictionaries, we used the OMP coding method. The rate value was deﬁned as
R = a · ♯Blocks + ♯coefs · (b + Q)
♯pixels
,
(4.15)
where
• a holds the required number of bits to code the number of coeﬃcients for
each block.
• b holds the required number of bits to code the index of the representing
atom. Both a and b values were calculated using an entropy coder.

61
• ♯Blocks is the number of blocks in the image (594).
• ♯coefs is the total number of coeﬃcients required to represent the whole
image.
• ♯pixels is the number of pixels in the image (= 64 · ♯Blocks).
0
0.5
1
1.5
2
2.5
3
3.5
4
28
30
32
34
36
38
40
42
44
46
48
50
Rate − BPP value
PSNR
KSVD
Overcomplete Haar
Overcomplete DCT
Complete DCT
Figure 4.10: Compression results: Rate-Distortion graphs.
In the unitary DCT dictionary we picked the coeﬃcients in a zig-zag order,
as done by JPEG, until the error bound e is reached. Therefore, the index of each
atom should not be coded, and the rate was deﬁned by,
R = a · ♯Blocks + ♯coefs · Q
♯pixels
,
(4.16)
with the same notation as before.

62
By sweeping through various values of e and Q we get per each dictionary
several curves in the R-D plane. Figure 4.10 presents the best obtained R-D curves
for each dictionary. As can be seen, the K-SVD dictionary outperforms all other
dictionaries, and achieves up to 1 −2dB better for bit rates less than 1.5 bits-
per-pixel (where the sparsity model holds true). Samples results are presented in
Figure 4.11. As the bit rate increases, the Sparseland model becomes less relevant,
as the representations become non-sparse and this results with a deterioration in
the performance.
K−SVD dictionary
8 bits per coefficients
PSNR =34.1564
Rate = 0.70651 BPP
Overcomplete DCT dictionary
8 bits per coefficients
PSNR =32.4021
Rate = 0.69419 BPP
Complete DCT dictionary
8 bits per coefficients
PSNR =32.3917
Rate = 0.70302 BPP
Figure 4.11: Sample compression results.

Chapter 5
K-SVD-Based Image Denoising
5.1
Introduction
In this section we address the classic image denoising problem: An ideal
image z is measured in the presence of an additive zero-mean white and homoge-
neous Gaussian noise, v, with standard deviation σ. The measured image, y, is
thus
y = z + v.
(5.1)
We desire to design an algorithm that can remove the noise from y, getting as
close as possible to the original image, z.
The image denoising problem is important, not only because of the evident
applications it serves. Being the simplest possible inverse problem, it provides
a convenient platform over which image processing ideas and techniques can be
assessed. Indeed, numerous contributions in the past 50 years or so addressed
this problem from many and diverse points of view. Statistical estimators of all
sorts, spatial adaptive ﬁlters, stochastic analysis, partial diﬀerential equations,
transform-domain methods, splines and other approximation theory methods,

64
morphological analysis, order statistics, and more, are some of the many direc-
tions explored in studying this problem. In this work we have no intention to
provide a survey of this vast activity. Instead, we intend to concentrate on apply-
ing the main concepts this thesis deals with for image denoising: the use of sparse
and redundant representations over trained dictionaries. We will use the K-SVD
within this algorithm, showing that it leads to state-of-the-art results. This way
we intend to strengthen the general claim made in this thesis, which states “The
Sparseland model for images, and the K-SVD as the means to get a sparsifying
dictionary, are both promising methods that can lead to better performance in
many applications in signal and image processing”.
Using redundant representations and sparsity as driving forces for denoising
of signals has drawn a lot of research attention in the past decade or so.
At
ﬁrst, sparsity of the unitary wavelet coeﬃcients was considered, leading to the
celebrated shrinkage algorithm [31, 22, 34, 32, 33, 95, 9, 78, 62]. One reason to turn
to redundant representations was the desire to have the shift invariance property
[12]. Also, with the growing realization that regular separable 1D wavelets are
inappropriate for handling images, several new tailored multiscale and directional
redundant transforms were introduced, including the curvelet [7, 8], contourlet
[18, 19], wedgelet [23], bandlet [74, 73], and the steerable wavelet [46, 97]. In
parallel, the introduction of the matching pursuit [75, 84] and the basis pursuit
denoising [11], gave rise to the ability to address the image denoising problem as
a direct sparse decomposition technique over redundant dictionaries. All these
lead to what is considered today as some of the best available image denoising
methods – see [87, 99, 43, 77] for few representative works.
In addressing general inverse problems in image processing using the Bayesian

65
approach, an image prior is necessary. Traditionally, this has been handled by
choosing a prior based on some simplifying assumptions, such as spatial smooth-
ness, min/max-entropy, or sparsity in some transform domain. While these com-
mon approaches lean on a guess of a mathematical expression for the image
prior, we concentrate here on example-based restoration techniques, which sug-
gest to learn the prior from images somehow. For example, assuming a spatial
smoothness-based Markov random ﬁeld prior of a speciﬁc structure, one can still
question (and thus train) the derivative ﬁlters to apply on the image, and the
robust function to use in weighting these ﬁlters’ outcome [108, 58, 91].
When this prior-learning idea is merged with sparsity and redundancy, we
return to the main motive of this thesis - training the dictionary to be used for the
restoration. In this work we consider two options: (i) training the dictionary using
patches from the corrupted image itself; or (ii) training on a corpus of patches
taken from a high-quality set of images.
In this work we use the K-SVD algorithm described above because of its
simplicity and eﬃciency for this task. Also, due to its structure, we shall see
how the training and the denoising fuse together naturally into one coherent and
iterated process, when training is done on the given image directly.
Since dictionary learning algorithms, and as such, the K-SVD, are limited
in handling small image patches, a natural diﬃculty arises: How can we use it for
general images of arbitrary size? In this work we propose a global image prior that
forces sparsity over patches in every location in the image (with overlaps). This
aligns with a similar idea, appearing in [91], for turning a local MRF-based prior
into a global one. We deﬁne a maximum a-posteriori probability (MAP) estimator
as the minimizer of a well-deﬁned global penalty term. Its numerical solution leads

66
to a simple iterated patch-by-patch sparse coding and averaging algorithm, that
is closely related to the ideas explored in [55, 56, 57], and generalizes them.
When considering the available global and multiscale alternative denoising
schemes (e.g., based on curvelet, contourlet, and steerable wavelet), it looks like
there is much to be lost in working on small patches.
Is there any chance of
getting a comparable denoising performance with a local-sparsity based method?
In that respect, the image denoising work reported in [87] is of great importance.
Beyond the speciﬁc novel and highly eﬀective algorithm described in that paper,
Portilla and his co-authors posed a clear set of comparative experiments that
standardize how image denoising algorithms should be assessed and compared
one versus the other. We make use of these exact experiments and show that the
newly proposed algorithm performs similarly, and often better, compared to the
denoising performance reported in their work. This puts this proposed algorithm
in the front row of image processing techniques.
To summarize, the novelty of this work includes the way we use local spar-
sity and redundancy as ingredients in a global Bayesian objective – this part is
described in the next Section, along with its emerging iterated numerical solver.
Also novel in this work is the idea to train dictionaries for the denoising task,
rather than use pre-chosen ones.
As already mentioned earlier, when training
is done on the corrupted image directly, the overall training-denoising algorithm
becomes fused into one iterative procedure that comprises of steps of denoising of
the image, followed by an update of the dictionary, as the K-SVD does it. This
is described in Section 5.3 in details. In Section 5.4 we show some experimental
results that demonstrate the eﬀectiveness of this algorithm.

67
5.2
From Local to Global Bayesian Reconstruction
In this section we start the presentation of the proposed denoising algorithm
by ﬁrst introducing how sparsity and redundancy are brought to use. For this, we
return to the Sparseland model described in Section 1.3 and adopt it for image
patches. Once this is set, we will discuss how local treatment on those image
patches turns into a global prior in a Bayesian reconstruction framework.
5.2.1
The Sparseland Model for Image Patches
In this section we return to the Sparseland model described in 1.3, and apply
it on image patches. We consider image patches of size √n × √n pixels, ordered
as column vectors z ∈Rn. For the construction of the Sparseland model, we need
to deﬁne a dictionary (matrix) of size D ∈Rn×K (with K > n, implying that
it is redundant). At the moment we shall assume that this matrix is known and
ﬁxed. Put loosely, the proposed model suggests that every image patch, z, could
be represented sparsely over this dictionary, i.e., the solution of
ˆx = arg min
x
∥x∥0 subject to Dx ≈z,
(5.2)
is indeed very sparse, ∥ˆx∥0 ≪n.
This model should be made more precise by replacing the rough constraint
Dx ≈z with a clear requirement to allow a bounded representation error, ∥Dx −
z∥2 ≤ϵ. Also, one needs to deﬁne how deep is the required sparsity, adding a
requirement of the form ∥ˆx∥0 ≤L ≪n, that states that the sparse representation
uses no more than L atoms from the dictionary for every image patch instance.
With the triplet (ϵ, L, D) in place, our model is well-deﬁned.
Now assume that z indeed belongs to the (ϵ, L, D)-Sparseland signals. Con-

68
sider a noisy version of it, y, contaminated by an additive zero-mean white Gaus-
sian noise with standard deviation σ. The MAP estimator for denoising this image
patch is built by solving
ˆx = arg min
x
∥x∥0 subject to ∥Dx −y∥2
2 ≤T,
(5.3)
where T is dictated by ϵ and σ. The denoised image is thus given by ˆz = Dˆx
[11, 28, 105]. Notice that the above optimization task can be changed to be
ˆx = arg min
x
∥Dx −y∥2
2 + µ∥x∥0,
(5.4)
so that the constraint becomes a penalty. For a proper choice of µ the two problems
are equivalent. We will use this alternative terminology from now on, as it makes
the presentation of later parts simpler to follow. Naturally, the above problem is
solved approximately using of the pursuit techniques mentioned earlier. Here, as
in the previous chapter, we adopt the OMP due to its simplicity.
5.2.2
From Local Analysis to a Global Prior
If we want to handle a larger image, Z, of size
√
N ×
√
N (N ≫n), and
we are still interested in using the above described model, one option is to re-
deﬁne the model with a larger dictionary. Indeed, when using this model with
a dictionary emerging from the contourlet or curvelet transforms, such scaling is
simple and natural [77].
However, when we insist on using a speciﬁc ﬁxed and small size dictionary
D ∈Rn×K, this option no longer exists. Thus, a natural question arises concerning
the use of such a small dictionary in the ﬁrst place. Two reasons come to mind:
(i) When training takes place (as we will show in the next section), only small

69
dictionaries can be composed; and furthermore, (ii) A small dictionary implies a
locality of the resulting algorithms, which simpliﬁes the overall image treatment.
We next describe possible ways to use such a small dictionary when treating
a large image. A heuristic approach is to work on smaller patches of size √n×√n
and tile the results. In doing so, visible artifacts may occur on block boundaries.
One could also propose to work on overlapping patches and average the results in
order to prevent such blockiness artifacts, as indeed practiced in [55, 56, 57]. As
we shall see next, a systematic global approach towards this problem leads to this
very option as a core ingredient in an overall algorithm.
If our knowledge on the unknown large image Z is fully expressed in the fact
that every patch in it belongs to the (ϵ, L, D)-Sparseland model, then the natural
generalization of the above MAP estimator is the replacement of (5.4) with
n
ˆxij, ˆZ
o
= arg min
xij,Z
λ∥Z −Y∥2
2 +
X
ij
µij∥xij∥0 +
X
ij
∥Dxij −RijZ∥2
2.
(5.5)
In this expression the ﬁrst term is the log-likelihood global force that demands
the proximity between the measured image, Y, and its denoised (and unknown)
version Z. Put as a constraint, this penalty would have read ∥Z−Y∥2
2 ≤Const·σ2,
and this reﬂects the direct relationship between λ and σ. The second and the third
terms are the image prior that makes sure that in the constructed image, Z, every
patch zij = RijZ of size √n × √n in every location (thus the summation by i, j)
has a sparse representation with bounded error. Similar conversion has also been
practiced by Roth and Black when handling an MRF prior [91].
The matrix Rij is an n × N matrix that extracts the (ij) block from the
image. For an
√
N×
√
N image Z, the summation over i, j includes (
√
N−√n+1)2
items, considering all image patches of size √n × √n in Z with overlaps. As to
the coeﬃcients µij, those must be location dependent, so as to comply with a set

70
of constraints of the form ∥Dxij −zij∥2
2 ≤T.
5.2.3
Numerical Solution
When the underlying dictionary D is assumed known, the proposed penalty
term in (5.5) has two kinds of unknowns: the sparse representations ˆxij per each
location, and the overall output image Z. Instead of addressing both together,
we propose a block-coordinate minimization algorithm that starts with an initial-
ization Z = Y, and then seeks the optimal ˆxij. In doing so, we get a complete
decoupling of the minimization task to many smaller ones, each of the form
ˆxij = arg min
x
µij∥x∥0 + ∥Dx −zij∥2
2,
(5.6)
handling one image patch. Solving this using the OMP is easy, gathering one
atom at a time, and stopping when the error ∥Dx−zij∥2
2 goes below T. This way,
the choice of µij has been handled implicitly. Thus, this stage works as a sliding
window sparse coding stage, operated on each block of √n × √n at a time.
Given all ˆxij, we can now ﬁx those and turn to update Z. Returning to
(5.5), we need to solve
ˆZ = arg min
z
λ∥Z −Y∥2
2 +
X
ij
∥Dˆxij −RijZ∥2
2.
(5.7)
This is a simple quadratic term that has a closed-form solution of the form
ˆZ =
 
λI +
X
ij
RT
ijRij
!−1  
λY +
X
ij
RT
ijDˆxij
!
.
(5.8)
This rather cumbersome expression may mislead, as all it says is that averaging
of the denoised patches is to be done, with some relaxation obtained by averaging
with the original noisy image. The matrix to invert in the above expression is a

71
diagonal one, and thus the calculation of (5.8) can be also done on a pixel-by-pixel
basis, following the previously described sliding window sparse coding steps.
So far we have seen that the denoising algorithm calls for sparse coding of
small patches, and an averaging of their outcomes. However, if minimization of
(5.5) is our goal, then this process should proceed. Given the updated Z, we can
repeat the sparse coding stage, this time working on patches from the already
denoised image. Once this is done, a new averaging should be calculated, and so
on, and so forth. Thus, we obtain exactly what Guleryuz suggested in his work
– iterated denoising via sparse representation, and we may regard the analysis
proposed here as a rigorous way to justify such an iterated scheme [55, 56, 57].
5.3
Example-Based Sparsity and Redundancy
The entire discussion so far has been based on the assumption that the
dictionary D ∈Rn×K is known. We can certainly make some educated guesses
as to which dictionaries to use.
In fact, following Guleryuz’s work, the DCT
seems like a plausible choice [55, 56, 57]. Indeed, we might do better by using
a redundant version of the DCT, as practiced in Section 4.3. Still, the question
remains: can we make a better choice for D based on training? In light of the
previous chapter discussion, the answer is clearly positive, as we describe shortly.
We start with the simpler (and less eﬀective) option of training the dictionary
with the K-SVD on a set of image patches taken from good quality images, and
then turn to discuss the option of training on the corrupted image itself.

72
5.3.1
Training on the Corpus of Image Patches
Given a set of image patches Z = {zj}M
j=1, each of size √n × √n, and as-
suming that they emerge from a speciﬁc (ϵ, L, D)-Sparseland model, we would like
to estimate this model parameters, (ϵ, L, D). Put formally, we seek the dictionary
D that minimizes
ε
 D, {xj}M
j=1

=
M
X
j=1

µj∥xj∥0 + ∥Dxj −zj∥2
2

.
(5.9)
Just as before, the above expression seeks to get a sparse representation per each
of the examples in Z, and obtain a small representation error. The choice for µj
dictates how those two forces should be weighted, so as to make one of them a
clear constraint. For example, constraining ∀j ∥xj∥0 = L implies speciﬁc values
for µj, while requiring ∀j ∥Dxj −zj∥2
2 ≤ϵ2 leads to others.
The K-SVD algorithm described in Section 4 proposes an iterative algo-
rithm designed to handle the above task eﬀectively [1, 2]. Adopting again the
block-coordinate descent idea, the computations of D and {xj}M
j=1 are separated.
Assuming that D is known, the penalty posed in Equation (5.9) reduces to a set
of M sparse coding operations, very much like the ones seen in Equation (5.6).
Thus, OMP can be used again to obtain the near-optimal (recall that OMP is an
approximation algorithm, and thus a true minimization is not guaranteed) set of
representation vectors {xj}M
j=1.
Assuming these representation vectors ﬁxed, the K-SVD proposes an update
of the dictionary one column at a time. Each such update can be done optimally
by performing a singular value decomposition (SVD) operation on residual data
matrices, computed only on the examples that use this atom. This way, the value
of ε
 D, {xj}M
j=1

is guaranteed to drop per an update of each dictionary atom,

73
and along with this update, the representation coeﬃcients change as well (see
Section 4 for more details).
When adopted to the denoising task at hand, a crucial step is the choice of
the examples to train on. Is there really a universal dictionary that ﬁts all images
well? If there is one, which examples shall we use to ﬁnd it? The experiments that
follow in the next section bring us to the conclusion that while a reasonably good
dictionary that ﬁts all is indeed within reach, extracting state-of-the-art denoising
performance calls for a more complex model that uses several dictionaries switched
by content – an option we do not explore in this work.
Also, since the penalty minimized here in Equation (5.9) is a highly non-
convex functional, sub-optimal solutions are likely to haunt us.
Thus, a wise
initialization could be of great worth. In our experiments we started with the
already mentioned redundant DCT, which proves to be a good dictionary choice.
This also enabled us to apply fewer iterations.
Another puzzling issue is the redundancy factor K/n – how should we choose
K, the number of columns in D. Is there an optimal choice? In this work we do
not address this important question, and simply choose a value we ﬁnd empirically
to perform well. Further work is required to explore this matter.
5.3.2
Training on the Corrupted Image
Instead of supplying an artiﬁcial set of examples to train on, as proposed
above, one could take the patches from the corrupted image, Z = {yj}M
j=1, where
M = (
√
N −√n + 1)2. Since the K-SVD dictionary learning process has in it
a noise rejection capability (see experiments reported in Section 4.3), this seems
like a natural idea. Furthermore, rather than using unrelated examples that call

74
for the universality assumption of the Sparseland model, this option tailors the
dictionary to the image treated.
At ﬁrst sight, this change in the origin of the examples to train on seems
to be of technical worth, and has no impact on the overall algorithm. However, a
close inspection of both the functional ε
 D, {xj}M
j=1

in (5.9), and the global MAP
penalty in (5.5), reveals the close resemblance between the two. This implies that
the dictionary design could be embedded within the Bayesian approach. Returning
to Equation (5.5), we can regard also D as an unknown, and deﬁne our problem
as
n
ˆD, ˆxij, ˆZ
o
=
arg min
ˆD,xij,Z
λ∥Z −Y∥2
2 +
(5.10)
X
ij
µij∥xij∥0 +
X
ij
∥Dxij −RijZ∥2
2.
Following the previously constructed algorithm, we can assume a ﬁxed D and Z,
and compute the representations ˆxij. This requires, as before, a sparse coding
stage that deploys the OMP. Given those representations, the dictionary can be
now updated, using a sequence of K SVD operations.
Once done, the output image can be computed using (5.8). However, an
update of the output image Z changes the noise level σ, which up until now has
been considered as known, and was used in the preceding two stages. Therefore,
we choose to perform several more iterations of representation computation and
dictionary update, using the same value of σ, before ﬁnding the output image Z.
This algorithm is described in detail in Figure 5.1.
In evaluating the computational complexity of this algorithm, we consider
all three stages - sparse coding (OMP process), dictionary update (these stages are
iterated J times), and ﬁnal averaging process. All stages can be done eﬃciently,
requiring O(nKLJ) operations per pixel, where n is the block dimension, K is

75
the number of atoms in the dictionary, and L is the number of nonzero elements
in each coeﬃcient vector. L depends strongly on the noise level, e.g., for σ = 10,
the average L is 2.96, and for σ = 20, the average L is 1.12.
5.4
Results
In this section we demonstrate the results achieved by applying the above
methods on several test images, and with several dictionaries. The tested images,
as also the tested noise levels, are all the same as those used in the denoising
experiments reported in [87], in order to enable a fair comparison.
Table 5.1 summarizes these denoising results for the DCT dictionary, the
globally trained dictionary, and training on the corrupted images directly (referred
to hereafter as the adaptive dictionary). In most of these experiments, the dictio-
naries used were of size 64 × 256, designed to handle image patches of size 8 × 8
pixels (n = 64, K = 256). Only in the stronger noise levels (σ = 50, 75, 100), while
applying the adaptive dictionary method, did we use larger patches of size 16×16,
resulting a dictionary of size 256 × 256. Every result reported is an average over
5 experiments with diﬀerent realizations of the noise.
The redundant DCT dictionary is described in Figure 5.2, with each of
its atoms shown as an 8 × 8 pixel image. This dictionary was also used as the
initialization for all the training algorithms that follow.
The globally trained
dictionary is shown in Figure 5.3. This dictionary was produced by the K-SVD
algorithm (executed 180 iterations, using OMP for sparse coding with L = 6),
trained on a data-set of 100, 000 8 × 8 patches (very much like the one used in
Section 4.3). Those patches were taken from an arbitrary set of clean natural
images (unrelated to the test images), some of which are shown in Figure 5.4.

76
Task: Denoise a given image Y from white and additive Gaussian white noise
with standard deviation σ.
Algorithm Parameters: n - block size, K - dictionary size, J - number of
training iterations, λ - Lagrange multiplier, and C - noise gain.
min
Z,D,A


λ∥Y −Z∥+
X
ij
µij∥xij∥0 +
X
ij
∥Dxij −RijZ∥2
2



(1) Initialization : Set Z = Y, D = overcomplete DCT dictionary.
(2) Repeat J times:
• Sparse Coding Stage: Use any pursuit algorithm to compute the
representation vectors xij for each patch RijZ, by approximating
the solution of
∀ij min
xij ∥xij∥0
s.t. ∥RijZ −Dxij∥2
2 ≤(Cσ)2.
• Dictionary Update Stage: For each column l = 1, 2, . . . , K in
D, update it by
∗Find the set of patches that use this atom,
ωl
=
{(i, j)|xij(l) ̸= 0}.
∗For each index (i, j) ∈ωl, compute its representation error
el
ij = RijZij −
X
m̸=l
dmxij(m).
∗set El as the matrix whose columns are {el
ij}(i,j)∈ωl
∗Apply SVD decomposition El = U∆VT .
Choose the
updated dictionary column ˜dl to be the ﬁrst column of
U. Update the coeﬃcient values {xij(l)}(i,j)∈ωl to be the
entries of V multiplied by ∆(1, 1).
(3) Set:
Z =

λI +
X
ij
RT
ijRij


−1 
λY +
X
ij
RT
ijDxij


Figure 5.1: Denoising Procedure using a dictionary trained on patches from the
corrupted image. For our experiments with noise level lower than σ = 50, we used
the OMP pursuit method, and set J = 10, λ = 30/σ and C = 1.15.

77
In all experiments, the denoising process included a sparse-coding of each
patch of size 8 × 8 pixels from the noisy image. Using the OMP, atoms were
accumulated till the average error passed the threshold, chosen empirically to be
ϵ = 1.15 · σ. This means that our algorithm assumes the knowledge of σ – very
much like assumed in [87]. The denoised patches were averaged, as described in
Equation (5.8), using λ = 30/σ (see below for an explanation for this choice of λ).
As was said before, we chose to apply only one iteration in the iterative process
suggested previously in 5.2.3. Following iterations require knowledge of the new
noisy parameter σ, which is unknown after ﬁrst changing Z.
When training the dictionary on overlapping patches from the noisy image
itself, each such experiment included (256 −7)2 = 62, 001 patches (all available
patches from the 256 × 256 images, and every second patch from every second
row in the 512 × 512 size images). The algorithm described in details in Figure
5.1 was applied. In this denoising method, the parameters for the stronger noise
levels (σ = 50, 75, 100) were a bit modiﬁed. The patches were set to size 16 × 16,
resulting a dictionary of size 256 × 256.
The threshold parameter was set to
ϵ = 1.05 · σ, and λ = 0.
As can be seen from Table 5.1, the results of all methods are very close to
each other in general. Averaging the results that correspond to [87] in this table
for noise levels lower than1
σ = 50, the value is 34.62dB. A similar averaging
over the DCT dictionary results gives 34.45dB, implying an average diﬀerence
of 0.17dB, in favor of Portilla’s method. This is the same case with the globally
trained dictionary, which means that our attempt to train one global dictionary for
1 The strong noise experiments are problematic to analyze, because clipping of the dynamic
range to [0, 255], as often done, causes a severe deviation from the Gaussian distribution model
assumed.

78
σ/PSNR
Lena
Barb
Boats
Fgrpt
House
Peppers
Average
σP SNR
2/42.11
43.23
43.55
43.29
43.61
42.99
43.07
43.05
42.92
44.07
44.38
43.00
43.30
43.27
43.47
0.012
0.017
43.23
43.58
43.10
43.67
41.86
43.14
42.94
42.99
44.27
44.47
42.90
43.33
43.05
43.53
0.018
0.017
5/34.15
38.49
38.51
37.79
37.93
36.97
37.09
36.68
36.48
38.65
39.07
37.31
37.67
37.65
37.79
0.014
0.016
38.48
38.60
37.32
38.08
36.64
37.22
36.56
36.65
38.86
39.37
37.65
37.78
37.59
37.95
0.016
0.017
10/28.13
35.61
35.28
34.03
33.97
33.58
33.44
32.45
32.14
35.35
35.41
33.77
33.93
34.13
34.03
0.017
0.026
35.40
35.47
33.07
34.42
33.53
33.64
32.23
32.39
35.69
35.98
34.32
34.28
34.04
34.36
0.024
0.027
15/24.61
33.90
33.38
31.86
31.63
31.70
31.38
30.14
29.71
33.64
33.49
31.74
31.76
32.16
31.89
0.024
0.032
33.60
33.70
30.61
32.37
31.63
31.73
29.86
30.06
34.03
34.32
32.37
32.22
32.02
32.40
0.030
0.035
20/22.11
32.66
32.00
30.32
29.95
30.38
29.91
28.60
28.01
32.39
32.17
30.31
30.20
30.78
30.37
0.031
0.024
32.27
32.38
28.87
30.83
30.24
30.36
28.21
28.47
32.88
33.20
30.92
30.82
30.57
31.01
0.025
0.027
25/20.17
31.69
30.89
29.13
28.65
29.37
28.78
27.45
26.65
31.40
31.03
29.21
29.01
29.71
29.17
0.037
0.037
31.20
31.32
27.57
29.60
29.17
29.28
26.94
27.26
31.82
32.15
29.84
29.73
29.42
29.89
0.035
0.036
50/14.15
28.61
27.44
25.48
24.75
26.38
25.57
24.16
22.01
28.26
27.41
25.90
25.25
26.47
25.41
0.049
0.049
27.77
28.21
24.06
26.28
25.91
26.05
22.68
23.90
27.91
28.79
26.12
25.91
25.74
26.52
0.051
0.079
75/10.63
26.84
25.63
23.65
22.83
24.79
23.85
22.40
19.28
26.41
25.10
24.00
23.12
24.68
23.3
0.061
0.053
25.81
26.27
22.54
24.20
24.02
24.35
19.73
21.99
25.33
26.52
23.78
23.84
23.54
24.53
0.070
0.086
100/8.13
25.64
24.42
22.61
21.89
23.75
22.79
21.22
17.99
25.11
23.78
22.66
21.55
23.50
22.07
0.070
0.044
24.45
24.85
21.73
22.63
22.83
23.13
18.23
20.36
23.86
24.68
21.88
22.27
22.16
22.99
0.050
0.104
Table 5.1: Summary of the denoising PSNR results in [dB]. In each cell four denoising results are reported. Top left: results
of Portilla et at. [87], Top right: overcomplete DCT, Bottom left: global trained dictionary, Bottom right: adaptive dictionary
trained on noisy image. In each such set we highlighted the best result. All numbers are an average over 5 experiments. The last
two columns present the average results over all images and their variance.

79
images performs as good as the ﬁxed redundant DCT. However, for the method
of the image-adaptive dictionary, an average of 34.86dB is obtained, giving an
average advantage of 0.24dB over Portilla’s method.
Figure 5.2: The overcomplete DCT dictionary.
In order to better visualize the results and their comparison to those in [87],
Figure 5.5 presents the diﬀerence of the denoising results of the two proposed
methods and the overcomplete DCT compared with those of [87] (which appears
as a zero straight reference line). This comparison is presented for the images
‘Peppers’, ‘House’ and ‘Barbara’.
Notice that for these images, the adaptive
dictionary outperforms the reported results of Portilla et al. for all noise levels
lower than σ = 75, while the global dictionary often achieves very close results.
In the image ‘Barbara’, however, which contains high-frequency texture areas, the
adaptive dictionary that learns the speciﬁc characteristics has a clear advantage
over the globally trained dictionary.
Figure 5.6 further describes the behavior of the denoising algorithm that uses

80
Figure 5.3: The globally trained dictionary.
Figure 5.4: Sample from the images used for training the global dictionary.

81
2
5
10
15
20
25
50
75
100
−1.2
−1
−0.8
−0.6
−0.4
−0.2
0
0.2
0.4
0.6
0.8
Peppers
σ
Difference in dB
 
 
Base for comparison
Overcomplete DCT
Global Dictionary
Adaptive Dictionary
2
5
10
15
20
25
50
75
100
−1.5
−1
−0.5
0
0.5
1
House
σ
Difference in dB
 
 
Base for comparison
Overcomplete DCT
Global Dictionary
Adaptive Dictionary
2
5
10
15
20
25
50
75
100
−2
−1.5
−1
−0.5
0
0.5
1
Barbara
σ
Difference in dB
 
 
Base for comparison
Overcomplete DCT
Global Dictionary
Adaptive Dictionary
Figure 5.5: Comparison between the three presented methods (Overcomplete
DCT, global trained dictionary and adaptive dictionary trained on patches from
the noisy image) and the results achieved recently in [87] for three test images.

82
the adaptive dictionary. Each K-SVD iteration improves the denoising results,
with the initial dictionary set to be the overcomplete DCT. A graph presenting
this consistent improvement for several noise levels is presented in this Figure.
All graphs show the improvement over the ﬁrst iteration, and therefore all curves
start at zero, going towards positive values. As can be seen, a gain of up to 1dB is
achievable. Figure 5.7 shows the results of the proposed algorithms for the image
‘Barbara’, and for σ = 20. The ﬁnal adaptive dictionary that leads to those results
is presented in Figure 5.8.
2
4
6
8
10
12
14
16
18
20
22
0
0.2
0.4
0.6
0.8
1
Difference in dB
# of iterations
↓ σ = 100
↓ σ = 75
↓ σ = 50
↓ σ = 25
↓ σ = 20
↑ σ = 15
↓ σ = 10
↓ σ = 5
↓ σ = 2
↓ σ = 1
21.7855
23.5873
26.1585
29.8478
30.8889
32.3291
34.312
37.8994
43.3404
48.3947
Figure 5.6: The improvement in the denoising results after each iteration of the
K-SVD algorithm, executed on noisy patches of the image ‘Peppers’.
We now turn to study the eﬀect of the parameter λ in Equation (5.8). As
expected, we found that a proper choice for λ is dependent on the noise level. As
the noise increases, better results are achieved with small values of λ and vice-
versa. This is indeed expected, as relatively ‘clean’ images should have a stronger

83
Original Image
Noisy Image (22.1307 dB, σ=20)
Denoised Image Using
Global Trained Dictionary (28.8528 dB)
Denoised Image Using
Adaptive Dictionary (30.8295 dB)
Figure 5.7: Example of the denoising results for the image ‘Barbara’ with σ = 20
– the original, the noisy, and two restoration results.

84
Created Adaptive Dictionary
Figure 5.8: Example of the denoising results for the image ‘Barbara’ with σ = 20
– the adaptively trained dictionary.

85
eﬀect on the outcome, while very noisy ones should eﬀect the outcome weakly, if
at all. We tested several values for this parameter, and found empirically that
the best results are achieved with λ ≈30/σ. It is interesting to see that all three
denoising methods (overcomplete DCT, global dictionary, and adaptive dictionary
trained on noisy patches), and all noise levels generally agree with this choice. In
Figure 5.9 we present the improvement (and later, deterioration) achieved when
increasing the value of λ in the averaging process (Equation (5.8)). In this Figure
one image (‘Peppers’) was tested with four noise levels (σ = 5, 10, 20, 50) and with
all three methods, resulting with 12 curves. The choice λ = 30/σ seems to be
near the peak for all these graphs.
To conclude this experimental section, we refer to our arbitrary choice of
K = 256 dictionary atoms (this choice had an eﬀect over all three experimented
methods). We conducted another experiment, which compares between several
values of K. In this experiment, we tested the denoising results of the three pro-
posed methods on the image ‘House’ for an initial noise level of σ = 15 (24.61dB)
and λ = 30/σ. The tested redundancy values (of K) were 64, 128, 256, and 512.
The average results of 4 executions (per each test) are presented in Figure 5.10.
As can be seen, the increase of the number of dictionary elements generally im-
proves the results, although this improvement is small (0−0.16dB). This increase
is most eﬀective in the adaptive dictionary method.
5.5
Summary
This chapter presented a relatively simple method for image denoising, lead-
ing to state-of-the-art performance, equivalent and sometimes surpassing recently
published leading alternatives. The proposed method is based on local opera-

86
0
10
20
30
40
50
60
70
80
90
100
−0.2
−0.15
−0.1
−0.05
0
0.05
0.1
0.15
Value of λ / σ 
Difference in dB
σ = 50
σ = 20
σ = 10
σ = 5
Figure 5.9: The improvement (and later, deterioration) of the denoising results
when increasing the value of λ in the averaging process in Equation (5.8).
64
128
256
512
33.2
33.4
33.6
33.8
34
34.2
34.4
# Dictionary Elements (k)
Average PSNR (in dB)
Redundency Graph, λ = 0
Overcomplete DCT
Global Dictionary
Adaptive Dictionary
Figure 5.10: The eﬀect of changing the number of dictionary elements (K) on the
ﬁnal denoising results for the image ’House’ and for σ = 15.

87
tions and involves sparse decompositions of each image block under one ﬁxed
over-complete dictionary, and a simple average calculations. The content of the
dictionary is of prime importance for the denoising process – we have shown that
a dictionary trained for natural real images, as well as an adaptive dictionary
trained on patches of the noisy image itself, both perform very well. The use of
the K-SVD, along with the reliance on sparsity and redundancy as guiding models,
have both led us to obtain excellent results.

88

Chapter 6
Constrained K-SVD
6.1
General Motivation
Most of the proposed methods for dictionary learning are designed to pro-
duce general dictionaries without any constraints other than the number of atoms.
However, additional constraints could emerge from a prior knowledge on the na-
ture of the data, which enforces properties on the underlying dictionary. Further-
more, structural constraints on the dictionary can lead to a more eﬃcient usage,
and in general, may be required for several applications due to computational
considerations.
Considering all the dictionary training algorithms mentioned in section 2.3,
the K-SVD algorithm is diﬀerent from the others because it updates one atom at
a time, and in each such update allows changing the relevant coeﬃcients. Each
update is a convex problem with a simple solution. This fact makes the K-SVD
attractive for the development of diﬀerent variations, which introduce additional
constraints.
In this section we refer to the problem of learning dictionaries under struc-
tural constraints by suggesting variations of the K-SVD algorithm. The ﬁrst con-
straint we refer to is the non-negativity. We allow the dictionary to consist of only

90
non-negative elements. Often (although not necessary) we restrict non-negativity
also on the coeﬃcients, and solve a variation of problem (4.4),
min
D
X
i
∥yi −Dxi∥2
2
s.t.
∀i ∥xi∥0 ≤T0, xi ≥0, di ≥0,
(6.1)
where in v ≥0 we mean that all entries of the vector v are greater or equal zero.
The second variation we consider is the creation of a shift-invariant dictio-
nary. We train a dictionary D ∈Rm×L, for m ≤n. Each signal y is represented
as a sum of shifted atoms Stdi, for St a matrix that locates a vector of size m
in the t’th location of a vector of size n. As a result, the number of representing
atoms is L · s, where s is the number of possible shifts of each atom.
In fact, the above could be generalized by considering a more general con-
straint that forces each atom in the dictionary to be the product of some known
matrix R ∈Rn×m with other unconstrained vector ˜d, such that d = R˜d. When
rank(R) < n it becomes a constraint, and the real variables to discover are the
entries of ˜d. We present a way to extend the K-SVD to address such a structure.
Up until now, representations of high dimensional signals by trained dictio-
naries were done by representing small patches of those signals, as practiced in
the previous chapter [1, 82]. Computational considerations, as well as memory
constraints, prevent the K-SVD (and other methods) from learning dictionaries
for such signals. In an attempt to overcome this diﬃculty, we consider the incor-
poration of linear constraints within the K-SVD, in a way which can lead to a
multiscale version of it, and this way handle signals of arbitrary size.
Finally, we present a totally diﬀerent approach for structuring the dictio-
nary. Instead of being a set of distinct atoms, we propose an ‘Image signature
Dictionary’ (ISD), where the dictionary is a matrix in which each patch can serve
as a representing atom. We show that such a scheme obtains shift-invariant prop-

91
erties, and overcomes other diﬃculties that arise from decomposition under a
shift-invariant dictionary. Moreover, such an approach requires much less mem-
ory than a ‘regular’ dictionary, and potentially much less computational eﬀort for
the decomposition.
For clarity and simplicity, we adopt in this chapter two deﬁnitions.
• We a use the set symbol { vi }N
i=1 to represent a matrix that contains
v1, v2, . . . , vN as its columns.
• For a matrix A and a set of indices Ω, we deﬁne AΩas the matrix A,
restricted only to the columns whose indices are in Ω.
6.2
Non-Negative K-SVD (NN-K-SVD)
6.2.1
General
For some applications, using sparse representations and overcomplete dic-
tionaries together with forcing non-negativity on both the dictionary and the coef-
ﬁcients, may lead to a better modeling of the problem, and as such to an eﬀective
method for revealing the ‘ingredients’ from which all training signals are built of
[60, 61]. The inability to subtract values from the linear combination forces the
dictionary elements to become sparser, and converge to the building blocks of the
training signals. This subject is often referred to in literature as Non-Negative
Matrix Factorization, or NMF, computing both the dictionary and the coeﬃcient
matrices, whose product approximates the signal matrix Y ≈DX. Application
for NMF are many and include dimensionality reduction [5] and analysis of data
such as audio [98], text [94], and data obtained from astronomical spectrometers
[86].

92
In what follows we describe sparse non-negative decomposition under over-
complete dictionaries. Then, we review previously reported methods for learning
non-negative dictionaries and describe the K-SVD variation for this task (referred
to as NN-K-SVD). Finally, we conduct some validation tests and prove the ad-
vantage of NN-K-SVD.
6.2.2
Non-negative Decomposition
Pursuit methods with non-negativity constraints are similar to those pre-
sented earlier. A non-negative version of BP minimizes the following convex func-
tion [60],
min
x
∥y −Dx∥2
2 + λ
X
i
xi,
subject to ∀i xi ≥0.
(6.2)
The non-negative constraint reduces the need for an absolute value over the entries
of the coeﬃcient vector x. One possible iterative technique is the following [60],
xt+1 = xt. ∗(DTy)./(DTDxt + λ),
(6.3)
where .∗and ./ represent entry-wise multiplication and division, respectively.
Kreutz-Delgado and Murray showed a non-negative version of the FOCUSS al-
gorithm [79], referred to as FOCUSS+.
They proposed to project the results
after each iteration onto the positivity constraints by setting to zero all negative
elements.
A variation of the Orthogonal Matching Pursuit (OMP) for non-negative
decomposition can also be easily derived. This includes selection of the dictionary
atom that results the largest projection on the residual signal (without applying
absolute value operation), followed by a non-negative least squares operation.

93
However, the performances of the OMP were found to be inferior than the latter
methods, and therefore it was not used in our tests.
A thorough analysis concerning the linear programming solution of the con-
vex problem,
min
x ∥x∥1 subject to y = Dx, x ≥0,
(6.4)
was recently given by Donoho and Tanner [21].
They studied the connection
between the true sparsest solution and the approximated one derived from the
solution of (6.4). Considering the intrinsic properties of the dictionary D, and
in particular, the convex hull of the point–set that contains the columns of D,
conclusions regarding the equivalence between the two problems were drawn.
6.2.3
Design of Non-Negative Dictionaries - Prior Art
A simple method for non-negative matrix factorization that ﬁnds iteratively
both the dictionary and the coeﬃcient matrices was introduced by Lee and Se-
ung in [69]. However, this method does not encourage the coeﬃcients’ sparsity,
and therefore is not designed for ﬁnding overcomplete dictionaries. In [68] they
introduced their algorithm as a method for revealing the parts constructing the
training signals, and presented their results working on a set of face images. The
corresponding dictionary elements became localized, and each element contained
diﬀerent parts of the face. Hoyer [60, 61] developed an improvement for Lee and
Seung’s algorithm, by enforcing sparsity constraints, therefore allowing the work
with overcomplete dictionaries. He repeated the same tests with similar results.

94
6.2.4
Non-Negative K-SVD - NN-K-SVD
In order to adapt the K-SVD for producing non-negative dictionaries (and
coeﬃcient matrices) two slight changes should be done. In the sparse coding stage,
an adequate pursuit method must be used, forcing non-negative coeﬃcients, as
described above.
We preferred to use the iterative method presented in [60],
also described in Equation (6.3), which is a variation of BP for non-negative
decomposition.
Furthermore, we added one change for this method, in order
to allow ﬁnding a decomposition with a pre-speciﬁed number of coeﬃcients, L.
After a couple of iterations are done, the indices of the L largest coeﬃcients are
selected, and the data is approximated by those element alone, using least-squares
with non-negativity constraint on the coeﬃcients. If we denote ωL as the indices
of the L selected atoms, then we solved
min
x ∥y −DωLx∥s.t. x ≥0.
(6.5)
using the Matlab’s function ‘lsqnonneg’, which uses the algorithm described in
[67].
In the dictionary update stage, we must force the dictionary matrix to stay
positive after each atom update. Our problem, in parallel to the one in Equation
(4.10), is therefore
min
dk,xk∥Eωk
k −dkxk
R∥s.t. dk, xk
R ≥0.
(6.6)
where ωk is the set of data indices that uses the atom dk in their representation.
As before, this problem reduces to ﬁnding the best positive rank-1 matrix that
approximate the error matrix Eωk
k , but adding a positivity constraint. This prob-
lem has the same complexity as the original SVD step, but in order to solve it we
employ an iterative technique, as described in Figure 6.1, assuming A = Eωk
k . The

95
initial solution for this method is chosen as the SVD solution, truncated to null
the negative entries. Note that the ﬁrst singular vectors can both be multiplied
by (−1) without changing the overall rank-1 approximation, and therefore both
options should be tested and compared. A full description is presented in Figure
6.1
Initialization: Set
d(i) =
 0
u1(i) < 0
u1(i)
otherwise ,
x(i) =
 0
v1(i) < 0
v1(i)
otherwise ,
where u1 and v1 are the ﬁrst singular vectors of A.
Repeat until convergence:
(1) Set: d = Ax
xT x.
Project: d(i) =
 0
d(i) < 0
d(i)
otherwise
(2) Set: x = AT d
dT d .
Project: x(i) =
 0
x(i) < 0
x(i)
otherwise
Figure 6.1: Finding a non-negative rank-1 approximation for a matrix A = dxT
We often found that the true local minimum is only slightly diﬀerent from
the initial solution supplied by the SVD projection to the non-negative space,
and therefore, we decided to skip the iterative method in cases where the initial
solution supply a suﬃcient reduction of the error. Notice that setting the negative
values in the error matrix to zero, and applying SVD, also ensures us positive
updated elements, but produces worse results.
At the end of this iterative procedure, the vector dk should be normalized

96
by dividing it by a scalar, as it construct a dictionary element, and xk should
be multiplied by the same scalar.
The full K-SVD variation for non-negative
factorization, denoted as NN-K-SVD is presented in Figure 6.2.
6.2.5
A Synthetic Experiment
The following synthetic experiment was done with the NN-K-SVD. We man-
ually generated 10 dictionary elements of size 8 × 8 pixels, containing the images
of the 10 decimal digits. Each digit was then translated by 1 pixel up/down and
left/right, constructing 9 possible conﬁgurations, resulting with a total of 90 dic-
tionary elements. This dictionary is presented on the upper left side of Figure 6.3.
A set of 3000 training signals was generated as random combinations of 5 such
atoms, with random positive coeﬃcients. The test was conducted twice, with no
noise and with 15dB noise. Those 3000 training signals were used as input to
the NN-K-SVD, which resulted with the positive dictionaries presented in the two
upper right images (for the two tests) of Figure 6.3. The NN-K-SVD run was
stopped after 60 and 100 iterations respectively. We also used the same data with
Hoyer’s algorithm [60], which was stopped after 1500 iterations. The resulted dic-
tionaries are presented in the second row of Figure 6.3. Information about each
result is given in the Figure. Note that in this test the NN-K-SVD had an ad-
vantage because it used the exact number of coeﬃcients, while Hoyer’s algorithm
was executed as is with a sparsity factor of 0.8 (see [61]) on the coeﬃcients.

97
Initialization : Set the non-negative random normalized dictionary matrix
D(0) ∈IRn×K.
Repeat until convergence,
• Sparse Coding Stage:
Use any pursuit algorithm for non-
negative decomposition to compute xi for i = 1, 2, . . . , N
min
x

∥yi −Dx∥2
2
	
subject to
∥x∥0 ≤T0 ∧∀i xi ≥0.
• Codebook Update Stage: For k = 1, 2, . . . , K
∗Deﬁne the set of examples that use dk,
ωk = {i| 1 ≤i ≤N, xi(k) ̸= 0}.
∗Compute
Ek = Y −

DX −dkxk
,
∗Restrict Ek by choosing only the columns corresponding
to ωk, and obtain Eωk
k .
∗calculate dk and xk as described in Figure 6.1. Normalize
dk.
Figure 6.2: NN-K-SVD

98
True Dictionary
Frobenius Norm: 9.4868
NN−K−SVD Results
No Noise 
60 Iterations 
Error: 0.70306
NN−K−SVD Results
15 SNR 
100 Iterations 
Error: 0.95649
Hoyer Results
No Noise 
600 Iterations 
Error: 1.4244
Hoyer Results
No Noise 
1500 Iterations 
Error: 0.81363
Hoyer Results
15 SNR 
1500 Iterations 
Error: 1.1488
Figure 6.3: On top, from left to right: True initial dictionary, K-SVD results after 60 iterations in the no-noise test, and after 100
iterations when the noise level was 15 SNR. On the bottom, from left to right: Hoyer’s algorithm results in the no-noise case after
600 and after 1500 iterations, and after 1500 iterations in the test with noise level of 15 SNR.

99
6.3
Shift-Invariant K-SVD (SI-K-SVD)
6.3.1
General
In this section we show a variation of the K-SVD algorithm for training
a shift invariant dictionary D = {di}L
i=1 ∈Rm×L for m ≤n. Each signal y is
represented as a sum of shifted atoms Stdi, for St a matrix that inserts a vector
of size m in the t’th location within a longer vector of size n, assumed to be zero
elsewhere. This means that the number of representing atoms is sL, where s is
the number of possible shifts of each atom.
6.3.2
Training Shift Invariant Dictionary - Prior Art
Several works dealt with the problem of training a shift invariant dictionary.
In [4] a variation of the original algorithm proposed by Field and Olshausen [82]
is proposed. There, a new update rule that takes into account all possible shifts
is derived, with a slight change in the coeﬃcient values, in order to preserve the
assumed sparse distribution.
In a recent work of Engan et al. [42], variations of the MOD [39] algorithm
for constrained dictionaries are presented by the name ‘ILS-DLA’ (for ‘Iterative
LS-based Dictionary Learning Algorithms’). This work includes shift-invariant
constraints (referred to there as an overlapping dictionary, with N = 1), similar
to the way we will use here. As in the MOD, the optimal constraint dictionary
matrix is found, assuming ﬁxed coeﬃcients. In Section 2.3.3, as also in [1], it was
shown that the dictionary update rule of ‘ILS-DLA’ is the expected outcome of
the iterated update in the algorithm of [4].
A diﬀerent approach was proposed in [63] by the name ‘MoTIF’ (for ‘Match-

100
ing of Time Invariant Filters’) for this exact purpose. There, an iterative and
greedy method is suggested, in which in each step, a new atom is found. This
atom is required to be best correlated to the training signals, while being the least
correlated to the other already found atoms.
6.3.3
K-SVD for Shift-Invariant Dictionaries (SI-K-SVD)
In learning shift invariant dictionaries we train a dictionary D ∈Rm×L
where m < n.
Each signal is represented by a linear combination of shifted
atoms. Let Si ∈Rn×m be a shifting matrix with only m nonzero entries which
equal 1 in the semi-diagonal that starts in the i’th row,
Si =
((i-1) lines)
(identity matrix)
(n-m-i+1 lines)


























0
. . .
0
...
0
. . .
0


[Im×m]


0
. . .
0
...
0
. . .
0




,
(6.7)
and s be the number of possible shifts. For the natural case of allowing all shifts
from i = 1 to i = n −m + 1, we obtain s = n −m + 1. One could also allow
cyclic positions getting s = n. Then, a representation of a signal y using a linear
combination of T0 elements is expressed by,
yi =
T0
X
l=1
xilSildil.
(6.8)

101
The trained dictionary should then minimize,
D = arg min
D
N
X
i=1
yi −
T0
X
j=1
xijSijdij

2
2
.
(6.9)
The sparse coding stage for a shift-invariant dictionary is not diﬀerent
from the regular sparse coding procedure, considering the full dictionary of size
n × (sL). However, the implementation can be made more eﬃcient by taking into
account the fact that the multiplication of the adjoint of the dictionary by a signal
amounts to L convolutions. For further discussions on sparse coding algorithms for
shift invariant dictionaries see [63, 4]. We note here that this sparse coding stage
is problematic, and prevents the overall training algorithm to reach a successful
solution, as will be described later.
In the dictionary update stage, the dictionary atoms are updated one
after the other. When updating the atom dk, we consider the signals it represents
as also its relevant shifts as ﬁxed, and allow changing only the values of its entries
and its coeﬃcients. As in 4.2.2, Let Ωk be the set of all data indices that use dk in
their representation. we deﬁne the representation error matrix without the atom
dk,
Ek
=
YΩk −



T0
X
l=1,djl̸=dk
xjlSjldjl



j∈Ωk
.
(6.10)
The representation error matrix Ek ∈Rn×|Ωk| should be approximated by
setting new values for dk and its coeﬃcients, but without changing their support
and shifts. Our objective is therefore written as
min
dk,{xjl|j∈Ωk,djl=dk}

Ek −



X
{l|djl=dk}
xjlSjldk



j∈Ωk

2
2
.
(6.11)

102
Notice that this problem is diﬀerent from the one we encountered before (Equa-
tion 4.10), because several of the entries in Ek are not eﬀected at all from dk,
while other entries may be approximated by a linear combination of several dif-
ferent entries in dk, because of overlapping shifts.
Therefore, a simple rank-1
approximation of the error matrix is not enough (see Figure 6.4).
For simplicity, we reduce our problem into approximating one column ei of
Ek,
min
x1,x2,...,xT k
i
,dk

ei −
T k
i
X
j=1
xijSijdk

2
2
,
(6.12)
where T k
i is the number of times the atom dk participate in the representation of
yi. Our objective is to minimize this error with respect to both xij and the atom
dk. The shifts Sij are known and assumed ﬁxed. As before, we iterate between
the update of xij and dk. Fixing the coeﬃcients xi1, xi2, . . . , xiT k
i , we optimize the
atom by setting
dk =
 X
k
X
j
xikxijST
ikSij
!−1 T k
i
X
j=1
xijST
ijei.
(6.13)
Note that the matrix to invert is symmetric of size m × m. The arguments for
which the diﬀerence between ik and ij is greater or equals m become zero. Then,
we ﬁx the value of dk and set new values for the coeﬃcients, one at a time by
xij =
dT
k ST
ijei −P
k̸=j xikdT
k ST
ijSikdk
dT
k dk
.
(6.14)
For solving for all vectors in Ek,
min
x1,x2,...,xM,dk

Ek −



T k
j
X
i=1
xjiSjidk



j∈Ωk

2
2
,
(6.15)

103
0
100
200
300
400
500
600
700
800
900
1000
−1
−0.5
0
0.5
1
1.5
2
2.5
3
3.5
4
0.7 ⋅ S45 d1
−0.6 ⋅ S152 d1
0.7 ⋅ S560 d1
The sum of the above signals
Figure 6.4: Illustraion for approximation of a vector by a linear combination of 3
shifted versions of the atom d.

104
where M = P
i∈Ωk T k
i , we concatenate the columns one after the other, creating
one long error vector ˜e ∈Rn|Ωk|, while a respective change in the shifting matrices
is done, so as to reduce the problem to the one in (6.12).
6.3.4
SI-K-SVD - experiments
For validation of the algorithm, we synthetically created 2000 signals of
dimension 20 using linear combinations of 3 shifted atoms from a dictionary
D ∈R18×45 (without allowing cyclic shifts, so that the number of possible shifts
was 3). Thus, the overall eﬀective dictionary is of size 20 × 135. The selected
indices and their shifts are randomly chosen. White Gaussian noise of several
power settings was added in order to generate SNR values of 10dB, 20dB, 30dB
and without any noise. The SI-K-SVD algorithm was executed for 80 iterations,
while its initial choice for D was the ﬁrst 18 entries in the ﬁrst 45 signals. For com-
parison, we implemented the ILS-DLA algorithm for shift invariant dictionaries
[42], which was used in a similar manner to the SI-K-SVD (same pursuit method,
same number of iterations, same initialization). We also tested the original K-
SVD algorithm on the same signals, for the same number of iterations, trying to
extract 135 dictionary atoms that correspond to the 45 atoms, in all shifts. For all
methods, the learned dictionary is compared to the initial true dictionary, using a
similar procedure as in Section 4.2.5 and in [1]. Each test, in each method and in
each noise level was executed 5 times. The average detection rate was calculated,
and is presented in Figure 6.5.
As can be seen, the advantage of the SI-K-SVD over the ISL-DLA is not big.
However, the advantage over applying a ‘regular’ scheme is clear. Moreover, we
can see that the level of noise does not eﬀect the overall results of the algorithms

105
   10   
   20   
   30   
No Noise
0
10
20
30
40
50
60
70
80
90
100
Noise level (in dB)
Detection ratio
 
 
SI−KSVD
ISL−DLA
KSVD
Figure 6.5: Each symbol represents the average detection rate of 5 tests. The
tested methods were SI-K-SVD, ISL-DLA for shift invariant dictionaries, and the
original K-SVD method (without shift invariance).

106
that force shift-invariance, while it highly eﬀects the ’regular’ method. This proves
the need for a shift-invariant variation, when the underlying dictionary is known
to hold this property.
We also implemented the SI-K-SVD on patches from real images. The size
of each patch was 10 × 10, and each dictionary elements was set to be 4 × 4. A
ﬁxed number of 5 shifted atoms was set to represent each patch, and the number
of diﬀerent atoms was set to 20 (resulting D ∈R16×20). The resulted dictionary
atoms are presented in the center of each block in Figure 6.6.
Figure 6.6: Detected atoms found by the SI-KSVD algorithm executed on 10×10
patches from natural images. The found atoms of size 4 × 4 are located in the
center of each block, for visualization purposes.
6.3.5
Summary
We introduced a variation of the K-SVD algorithm for learning shift invari-
ant dictionaries. This algorithm ensures a reduction in the representation error
in each iteration in the ‘dictionary update’ stage. However, its performances are
only partially successful.

107
We believe that the main problem of the SI-K-SVD algorithm, as also the
other training algorithms presented in this section, is the problematic sparse cod-
ing stage.
Using a prior deﬁnition, the mutual coherence of a shift invariant
dictionary is relatively large, preventing the pursuit algorithms to reach a suc-
cessful solution. We assume an adequate solution to the sparse coding problem
will directly derive more successful training abilities.
6.4
Linear Constraints
We now turn to a more general constraint, in which each atom in the dic-
tionary D is known to be a product of the form,
∀i, di = Ri ˆdi,
(6.16)
where Ri ∈Rn×m, is a known matrix of rank r, ˆdi ∈Rm and n ≥m ≥r. Note
that if n = m = r there is no constraint on di and the problem simpliﬁes to the
original K-SVD setting.
This constraint is diﬀerent from the linear constraint suggested by Engan
et al. [42], which enforces A ˜D = b, where ˜D is an arrangement of the dictionary
D as a vector. The latter is a linear constraint on the dictionary D, while our
constraint forces each di to be a linear transformation of another, un-constrained,
dictionary atom ˆdi. Some structures can be enforced by the two constraints (like
symmetry, identical entries, zero entries, etc.). Other structures, such as those
that relate to the connection between diﬀerent atoms, can be more easily enforced
using the global linear constraint in [42]. Structural constraints that are functions
of other matrices, such as forcing an atom to be an enlarged version of another one,
can be more easily applied by our approach. An example for such a requirement is
presented in Section 6.4.2, where we discuss a multiscale version of the dictionary.

108
6.4.1
Linear Constraint K-SVD (LC-K-SVD)
As before, the sparse coding stage of each iteration is not changed, as also
the deﬁnition of Ek (Equation 4.10), since we can use the eﬀective dictionary with
the given atoms and apply pursuit techniques directly. However, in the dictionary
update stage the new value for the atom dk should obey dk = Rk ˆdk. Therefore
we solve,
min
dk,xk
Ωk
Ek −dkxk
Ωk
2
F
s.t.
dk = Rk ˆdk,
(6.17)
or alternatively,
min
ˆ
dk,xk
Ωk
Ek −Rk ˆdkxk
Ωk

2
F .
(6.18)
Let the SVD factorization of R be1 ,
Rk = URΣRVT
R,
(6.19)
where UR ∈Rn×n, ΣR ∈Rn×m with r nonzero entries on the main diagonal
σ1 ≥σ2 ≥. . . ≥σr, and VR ∈Rm×m. Then,
Ek −Rk ˆdkxk
Ωk

2
F
=
Ek −URΣRVT
R ˆdkxk
Ωk

2
F
(6.20)
=
UT
REk −ΣRVT
R ˆdkxk
Ωk

2
F .
Let F = UT
REk =


F1
F2

such that F1 ∈Rr×|Ωk|, and F2 ∈R(n−r)×|Ωk|. Let
us also denote w = VT
R ˆdk =


w1
w2

such that w1 ∈Rr×1, and w2 ∈R(m−r)×1.
1 The following development is based on a personal communication with Prof.
Shmuel
Friedland

109
Finally, let Σr
R ∈Rr×r be a diagonal matrix with σ1, σ2, . . . , σr on its diagonal,
then
Ek −Rk ˆdkxk
Ωk

2
F
=



F1
F2

−ΣR


w1
w2

xk
Ωk

2
F
(6.21)
=
F1 −Σr
Rw1xk
Ωk
2
F .
Solving (6.21) requires ﬁnding the rank-1 approximation of F1. If we use, again,
the SVD factorization of the matrix F1 = UFΛFVT
F, then, in parallel to the
rank-one approximation described in (4.2.2)
w1
=
Σr
R
−1U{1}
F ,
(6.22)
xk
Ωk
=
ΛF(1, 1) · V{1}
F
T.
For ﬁnding ˆdk we solve V{1,2,...,r}
R
T ˆdk = w1, which is a linear system of r
equations with m variables, and as r ≤m at least one solution exists (if r < m
anyone of the inﬁnite number of solutions may be chosen). A ﬁnal normalization
of the atom dk = Rk ˆdk, in addition to a respective change in the coeﬃcients xk
Ωk
might also be needed, if we require normalized elements of D.
6.4.2
Example - Multiscale Representation
6.4.2.1
Multiscale representation description
In the attempt to represent images using trained dictionaries so far, only
small patches are able to be handled. Training a dictionary for full (even relatively
small)
√
N ×
√
N images is impractical – the dimension of such a dictionary atom
would be N, and the number of them should be at least N for completeness.

110
Training such a dictionary requires too much computational eﬀort, as also too
much memory.
It would also lack the ﬂexibility in working with various size
images.
Therefore, as we have already suggested in Section 4.2.4, the solution is to
train dictionaries for multiscale representation. Here we suggest to train dictio-
naries for representing a variation of the Laplacian Pyramid. Representation of
images using a multiscale approach was already suggested and applied in various
transforms, such as curvelets [20] and Countorlets [18]. Both these transforms are
redundant, and encourage sparsity. However, they are based on ﬁxed mathemati-
cal models, and do not arise from the signals they serve by training, which might
be a disadvantage, especially when treating limited families of signals. In this sec-
tion we suggest a way to combine the ideas of multiscale representation together
with the idea of trained dictionaries, leaning on the above proposed paradigm of
constrained dictionaries.
Olshausen and Sallee [92] presented an approach for learning a multiscale
representation for images. In their algorithm, several mother wavelets function
are trained, and the full image is then represented by a combination of spatially
localized such functions with possible shifts and 2 : 1 scale. The learned functions
were trained to represent the steerable pyramid, and their results were compared to
the steerable transform with a classic wavelet, achieving s slightly better quality
in representation of images for the same level of sparsity, and better denoising
results (however, worse than those presented in Section 5).
Our representation scheme for an image of size
√
N ×
√
N, where N = n·4m
for some m ∈Z, is the following. We ﬁrst refer to the coarsest version of the image
of size √n×√n that contains the lowest frequencies. This level is referred to here

111
as the zero-level, or I0 = Sm−0˜I, where Si−j ∈R(4j·n)×(4i·n) is a resizing operator
from images in level i to images in level j, and ˜I is an arrangement of the elements
in I (the given original image) as a vector. I0 is represented by one dictionary
D0 ∈Rn×L1, multiplied by a sparse coeﬃcient vector x0. Then, we represent
the diﬀerences between the enlarged version of the zero level representation I0,
S0−1D0x0 and the reduced version of I, R1 = Sm−1˜I −S0−1D0x0. R1 ∈R4n,
so we use here 4 non-intersecting blocks, each of size n, represented by another
dictionary D1 ∈Rn×L2.
The next level to represent is computed similarly, as the diﬀerence between
the enlarged version of what was already represented, and the reduced version of
the image to represent,
R2 = Sm−2˜I −
 
S0−2(D0x0) + S1−2(
4
X
b=1
B1
bD1x1
b)
!
,
(6.23)
where Bi
j is an operator that places the √n × √n block into the j’th location of
a 4i · n vector. In General, the k’th represented level will be,
Rk = Sm−k˜I −


k−1
X
l=0
Sl−k
4l
X
b=1
Bl
bDlxl
b

.
(6.24)
The dictionaries to be trained are ∆= {D0, D1, ..., Dm}.
This representation scheme is presented in Figure 6.7, where ↑Li or ↓Li
represent resizing (enlarge or reduce, respectively) to level i, A(Di) and S(Di)
represent analysis and synthesis, respectively, using the i’th dictionary.
Actually, we may imagine the representation of ˜I by a single very large
dictionary matrix DM (for multiscale) of dimension N ×
 Pm
l=1 4lLl

. The ﬁrst
L1 atoms of DM are the atoms of D0, enlarged to the maximal level, and therefore
DM
i
= S0−m·d0
i , for 1 ≤i ≤L0, Similarly, the next 41·L1 atoms of DM are enlarged

112
Figure 6.7: MultiScale representation scheme. ↑Li or ↓Li represent enlarge or
reduce size to level i, A(Di) and S(Di) represent analysis and synthesis, respec-
tively, using the i’th dictionary.

113
versions of the atoms of D1, placed in the relevant block, DM
i
= S1−mB1
bd1
j, for
L0 + 1 ≤i ≤L0 + 4L1, and b = ⌊(i −1 −L0)/L1⌋+ 1, j = ((i −L0 −1)%L1) + 1
(modulus operator), and in general,
DM
i
=
Sk−m · Bk
bdk
j,
for
(6.25)
k−1
X
l=0
4lLl + 1 ≤i ≤
k
X
l=0
4lLl
b
=
⌊(i −1 −
k−1
X
l=0
4lLl)/Lk⌋+ 1,
j
=
 
(i −
k−1
X
l=0
4lLl −1)%Lk
!
+ 1
To summarize thus far, DM is a huge dictionary that contains shifted and
enlarged versions of the atoms in ∆. To illustrate the size of such a dictionary,
we consider n = 82 and N = 2562 (m = 5), and redundancy Dl ∈R64×256 for
l = 0, 1, ...5, and get a dictionary DM of size 65536 × 349440 for representation of
images of size 256 × 256. However, rather than being a general dictionary with
∼2.3e10 free parameters, it is entirely deﬁned by the 6 small dictionaries that
construct it, containing together 70656 parameters. Of course, this dictionary
cannot and will not ever be explicitly stored.
Nevertheless, we remember its
structure, as it will guide us when training the smaller dictionaries in ∆that
construct it.
6.4.2.2
Sparse Representation of Images in a Multiscale scheme
A multiscale and sparse representation of an image as described above, each
√n × √n block in each representation level Ri should be represented using the
related dictionary Di. Each representation level may suﬀer errors, which then

114
propagate to the next level (see 6.24). Two extreme approaches can be adopted,
• Represent each level Ri without any error.
• Not to represent the coarser levels 0, 1, . . . , m−1 (xi
j = 0 for i = 1, 2, . . . , m−
1 and for all j), and represent only the blocks in Rm, which is practically
a single level representation of I.
In an eﬀective multiscale scheme, both the above extreme cases should be
avoided. Partial representation of each layer and a proper propagation of the error
should be practiced. However, such a process raises the question of how accurate
should every layer be represented – a question of allocation of eﬀort that we leave
open in this work. A possible way to deal with this problem was suggested in
[107], where a dynamic programming method is applied in order to best balance
between the number of bits used and the distortion incurred. In this work we
chose to use a ﬁxed and pre-deﬁned number of coeﬃcient T l
0 for each level l, in
parallel to (4.4). Thus we should solve
min
{xl
b|0≤l≤m,1≤b≤4l}
m
X
l=0

Rl −
4l
X
b=1
Bl
bDlxl
b

2
2
s.t.
∀l,b
xl
b

0 ≤T l
0.
(6.26)
This expression requires an adequate representation of each layer by representation
of the distinct blocks, restricting the number of non-zero entries in each block
representation from level l to be less or equal T l
0.
6.4.2.3
Training Dictionaries for a Multiscale Representation
For training the dictionaries in ∆we note two facts,
• Considering (6.25), each atom in DM is a product of some known matrix
Sk−mBk
b ∈RN×n and an arbitrary atom di ∈Dk of length n. This leads
to using the K-SVD variation for linear constraint dictionaries.

115
• Each atom di ∈Dk, for 0 ≤k ≤m appears 4k times in DM, in diﬀer-
ent, but non-overlapping, locations. This reminds us of the shift-invariant
property, where each column in the dictionary can appear in several dif-
ferent locations in a representing atom.
All this leads to some combination between the two latter K-SVD variations
we described. In updating the atom di ∈Dk, for some 0 ≤k ≤m, The main idea
is to calculate the representation error matrix Eki. We do so in two stages. First,
we calculated Ek, which is the representation error matrix without all atoms in
Dk (current level dictionary),
Ek = ˜I −
m
X
l=0,l̸=k

Sl−m
4l
X
b=1
Bl
bDlxl
b

.
(6.27)
Ek ∈RN holds the representation error excluding level k, which should be repre-
sented by Dk. For updating di ∈Dk, we then add the representation by all other
atoms in Dk, excluding dk
i ,
Eki = Ek + Sk−m
4k
X
b=1
Lk
X
j=1,j̸=i
Bk
bdk
jxl
b[j].
(6.28)
The vector Eki is the one to be minimized by setting new values for the atom dk
i
and its coeﬃcients. We collect all (4k · n) blocks from Eki for which dk
i takes part
in their representation, into columns in one matrix
˜
Eki ∈R(4k·n)×|Ωki|. Doing so
is equivalent to collecting all parts of the error matrix that overlap the nonzero
entries in the updated atom in the shift invariant dictionary. As the blocks are
non-overlapping, there could not be an entry in
˜
Eki that is aﬀected twice by the
atom dk
i . This process also neutralizes the eﬀect of the shifting matrix Bk
b. The
new problem is then,

116
min
 ˜
Eki −S0−kdk
i xk
i

2
2
(6.29)
where xk
i is a row vector that contains the nonzero values in the i’th row of all
vectors xk
b, for b = 1, 2, . . . , 4k. We assume that S0−k is full rank, so according to
(6.21), only the n ﬁrst left singular vectors are relevant for the rank-one approx-
imation. Surprisingly, those n columns are similar to the size-reduction operator
Sk−0. This means that the columns of
˜
Eki should be reduced to size n, and then
a rank-one approximation should be found.
If we assume that enlarging an image and then reducing its size back to the
original size results with the initial image (or formally, Si−jSj−i = I when j < i),
then we conclude that the resizing in Equation (6.27) can be done to level k, while
no enlargement at all is needed in (6.28).
6.4.2.4
Experiments
Synthetic tests to ﬁnd the mutiscale dictionary underlying the data were
ﬁrst applied on 1000 synthetic signals. Each signal was of size 12 × 12, while
the basic block size was set to 3 × 3 (which implied 3 levels of representation).
Each dictionary in each level consisted of 9 elements (complete dictionary), which
implies 1 + 4 + 16 = 21 diﬀerent underlying dictionaries, and 21 · 9 = 189 atoms.
Fixed numbers of coeﬃcients for representation of a block were assigned to each
level – 2, 2, and 1 to the coarsest, middle, and ﬁnest levels respectively. Therefore,
each representation of a 12×12 signal includes under this setting 2+2·4+1·16 = 26
coeﬃcients. All executions were performed for 30 iterations. The detection rate
for each level was measured in the same method as in 4.2.5. An average detection
rates of 5 diﬀerent experiment for the 3 levels and for 4 diﬀerent noise levels are

117
presented in Table 6.1.
The initialization of the learned dictionary was found to be crucial for the
succusses of the algorithm. We initialized the dictionaries by the results of another
single-scale learning process, that was executed on blocks of images, extracted
from the training set. The blocks that were used for learning the initial dictionary
D0 were y0
i = Sm−0Ii(:), where Ii is the i’th training signal, arranged as a column
vector. For extraction of the input signals for learning the initialization of the
other dictionaries, we assumed perfect representation of the layers, and set,
yl
i = Bl
b
 Sm−lIi(:) −S(l−1)−lSm−(l−1)Ii(:)

,
(6.30)
where b is some random block chosen from the 4l blocks in the image. That is,
the blocks that were used for training the initial Dl dictionary were taken from
a diﬀerences image, resulted by extracting the enlarged version of the original,
reduced to level l −1 (S(l−1)−lSm−(l−1)Ii(:)) from the original image, reduced to
level l (Sm−lIi(:)).
level
No noise
30dB
20dB
10dB
0
71.11
60
57.78
57.78
1
66.67
75.56
53.33
62.22
2
71.11
75.56
53.33
62.22
Table 6.1: Average detection rates (in percentage) for the multiscale synthetic
experiments. Notice the number of atoms are 9, 36 and 144 for the zero (coarsest),
ﬁrst and second level, respectively. Each experiment was performed 5 times.
This initialization was found successful, and enabled performing only a few
iterations until convergence. A typical progress of one of the executions is pre-
sented in Figure 6.8. As can be seen, the initial dictionaries were quite successful
before even starting the multiscale K-SVD process.

118
1
2
3
4
5
6
40
50
60
70
80
90
100
Iterations
Detection Ratio
Zero level (coarse)
First level
Second level
Figure 6.8: Typical execution of MS-KSVD (30dB). The graph presents the detec-
tion ratios of each level (the zero level is the coarsest one) for the ﬁrst 6 iterations.

119
In trying to process real images, we executed the K-SVD on real 64 × 64
images (basic block size: 8 × 8, 4 levels). The training set included front-view
aligned images, 4 of which can be seen in Figure 6.9. The mean value of each
image was reduced before training. The 4 extracted dictionaries of size 64 × 128,
were initialized by the single-level K-SVD learning process, as described above.
The resulted dictionaries after 80 iterations are presented in Figure 6.10. We can
see that the ﬁner the level of representation, the sharper and clearer are the edges.
Figure 6.9: Samples from the 64 × 64 face images used for training the multiscale
dictionaries.
6.5
Image Signature Dictionary
6.5.1
Introduction
Let D ∈RN×M be an ‘Image Signature Dictionary’ (ISD). Each signal block
y ∈Rn×m can be represented by a linear combination of blocks (atoms) of the
same size, taken from arbitrary locations in D. If ˜y and ˜D are arrangements of y
and D as columns, then,
˜y ≈
X
i
αiSi ˜D
(6.31)

120
Dictionary Atoms, Level 0 (coarsest)
Dictionary Atoms, Level 1
Dictionary Atoms, Level 2
Dictionary Atoms, Level 3 (finest)
Figure 6.10: Multiscale dictionaries extracted by the K-SVD algorithm, processed
on real face images. The most left dictionary is the coarsest.

121
where Si ∈Rnm×NM is the block selection matrix, having all zero entries except
nm entries, one in each line, that equals 1. We shall assume that for all sizes
(n, m), N · M patches from D of that size are available because we allow cyclic
transformation (see Figure 6.11).
The redundancy factor of D is
N·M
n·m for each block size (n, m).
Yet, it
requires very small space. One might say that this dictionary atoms are highly
constrained, as each atom equals 4 other ones in almost all entries except of one
row or column, and therefore its true redundancy is much smaller.
However,
such overlap between the atoms is natural for representing signals, as it is tightly
coupled with the idea of shift-invariance, although in a diﬀerent ﬂavor from the
one already discussed above. In what follows we shall describe how such dictionary
is learned and used.
6.5.2
Training an Image Signature Dictionary
Given a set of training patches yi ∈Rn×m, we search for an ISD D ∈RN×M
that can lead to the best representation of all those patches. For simplicity we
currently assume each patch is represented by a ﬁxed number of atoms (L).
min
D
X
i
 ˜yi −
L
X
j=1
αijSij ˜D

2
2
(6.32)
We follow previously reported algorithms for training dictionaries and adopt
the two stages iterated method - sparse coding and dictionary update. In the
sparse coding stage, the coeﬃcients’ supports and values are found, assuming a
known and ﬁxed dictionary. Then, the representation is assumed ﬁxed and the
dictionary is found to best minimize Equation (6.32).

122
Figure 6.11: Illustration of several atoms from in an ISD.

123
6.5.2.1
Sparse Coding
In representation of signals under such a scheme we should ﬁnd the optimal
αi and Si that minimize the expression in (6.31). The simplest way to do so is to
extract from D all possible atoms, and use one of the pursuit methods described in
Section 2.1. However, the special structure of this dictionary can be used to ﬁnd
a more eﬃcient pursuit technique. For example, all projections between a signal
and all dictionary atoms can be computed by only one inner product between the
Fourier transforms of the signal and the ISD, exploiting the equivalence between a
convolution in the space domain and inner product in the frequency domain [85].
While a straight forward projection of a signal onto all possible atoms is done in
O(nm · NM), The same result can be achieved in O(NM log(MN)), including
forward and inverse Fourier transform of the signal.
6.5.2.2
Dictionary Update Stage
Each pixel in D is common to nm diﬀerent atoms, and therefore aﬀected
from up to nm coeﬃcients.
Updating each pixel separately together with its
relevant coeﬃcients, as suggested by the K-SVD algorithm seems hopeless. We
therefore follow the MOD algorithm and derive Equation (6.32) in respect to D,
X
i
L
X
j=1
αijST
ij ˜yi −
X
i
L
X
j=1
L
X
k=1
αijαijkST
ikSij ˜D = 0,
(6.33)
implying the following update rule
˜D =
 X
i
L
X
j=1
L
X
k=1
αijαijkST
ikSij
!−1 X
i
L
X
j=1
αijST
ij ˜yi.
(6.34)
Each such update is promised to reduce the overall representation error, and if we
assume the pursuit stage is successful, the overall algorithm must converge.

124
6.5.3
Experiments
We tested the applicability of such a dictionary for denoising of images. We
adopt a similar method to the one suggested in Chapter 5. First, just as described
there, we trained a global dictionary on patches of real images which do not belong
to the test set. The reconstructed version is an average image of the represented
overlapping blocks. Here, a dictionary of size 100 × 100 was trained, using the
same training set that was used for training the global dictionary in Section 5.4,
and is presented in Figure 6.12. Its denoising results are very similar (±0.1dB) to
those reported in Table 5.1 for the global trained dictionary.
Next, the image signature dictionary is trained on the overlapping patches
from the noisy image, and the reconstructed version is an average image of the
represented blocks. The denoising results we received when we worked with a
(75 × 75) size dictionary and (8, 8) blocks are almost identical to those reported
in Table 5.1. Several Trained dictionaries are presented in Figure 6.13, and the
processed (clean) images are presented in Figure 6.14.
We also examined the shift-invariance property of the image signature dic-
tionary. Starting with an image of size 160×160 (a portion of the image ’House’),
we represent all its distinct 8×8 blocks (total of 400) using the global trained ISD
(Figure 6.12). We then shifted the selected 160×160 image in (x, y) pixels (mean-
ing x pixels down, and y pixels left), and examined the quality of representation
by shifting the used dictionary atoms respectively (or equivalently, changing the
index of the non-zero coeﬃcients). Then, we allowed each block to drop one atom
(whose coeﬃcient’s absolute value is minimal), and choose another atom (only
one) instead. The results of 3 such shifts are presented in Figure 6.15.

125
Global Image Signature Dictionary
Figure 6.12: Image signature dictionary of size 100×100, trained on patches from
real images.

126
Barbara, (75x75) Dictionary, σ = 15
House, (75x75) Dictionary, σ = 15
Peppers, (55x55) Dictionary, σ=20
House,    (55x55) Dictionary, σ=10
Figure 6.13: Trained image signature dictionaries for several images. For each of
these signatures we describe which image it is related to, the size of the signature,
and the additive noise power that aﬀected it.
Barbara
House
Peppers
Figure 6.14: Sample images we used for the various tests. From left to right:
Barbara, House and Peppers.

127
Original Reconstruction
(35.4539 dB)
Reconstruction By (1,0) Shift
Coefficients (31.0855 dB)
Reconstruction After a Change
of One Coefficient (34.3495 dB)
Original Reconstruction
(35.4539 dB)
Reconstruction By (2,2) Shift
Coefficients (24.3272 dB)
Reconstruction After a Change
of One Coefficient (31.5279 dB)
Original Reconstruction
(35.4539 dB)
Reconstruction By (3,3) Shift
Coefficients (21.7484 dB)
Reconstruction After a Change
of One Coefficient (29.5112 dB)
Figure 6.15: Examining the shift-invariance property of the ISD.

128

Chapter 7
Steps Towards a Geometrical Study of Sparseland
This entire work is based on the core assumption that interesting families of
signals (such as images) can be described as emerging from the Sparseland model.
It is true? how can we assess such an assumption?
In this section we confront some theoretical questions concerning the rep-
resentation abilities of ‘sparse coding’ schemes, and the ‘Sparseland’ model in
general. We use expressions and deﬁnitions from computational geometry and
describe the structure of the set of signals that can be sparsely approximated by
some dictionary. Also, given a speciﬁc dictionary, we set bounds on the size of the
set of signals that can be represented. Therefore, we supply tools that can help in
approximating the representation abilities of given dictionaries. We should note,
though, that the proposed study served in the chapter is a very partial one, and
further work is required to complete it.
7.1
Introduction and Motivation
The Sparseland model, described in Section 1.3, is one of the main mo-
tives of this thesis. According to this model, a family of signals can be sparsely
represented by some dictionary, and in ‘sparsely’, we mean that the number of

130
representing atoms is smaller, or even substantially smaller, than the dimension
of each represented signal. However, the number of signals that can be exactly
represented by a linear combination of L atoms from D, for L < n and a ﬁnite D,
is of measure zero in Rn. Each L atoms span a subspace of dimension L in the
n-dimensional space, which volume is negligible related to the whole space. Even
multiplying this ratio by
 K
L

, the number of possible subspaces, does not change
this fact. Therefore, on what exactly the Sparseland model relies on?
Let us start with the fact that we often work with digital signals, that bear
some kind of quantization eﬀects. In addition, Approximation is often allowed,
representing only a close version of the initial signals y. This allowed represen-
tation error ϵ = ∥˜y −y∥2 widens the possible represented subspace, and gives a
non-zero volume to the above subspaces, which can be measured. In this section
we check the representation ability of sparse coding, by measuring those relative
volumes of represented subspaces. We ﬁrst check the ratio of all signals that can
be represented by L coeﬃcients out of all signals in dimension n. We then discuss
the representation abilities of a given dictionary, and provide lower and upper
bounds for the relative represented space.
When designing a dictionary for a speciﬁc set of signals we require sparse
representation of each element in this set. However, in many cases this property is
not enough. We would also like the designed dictionary not to represent sparsely
other signals, which do not belong to this set. Such a requirement makes this
dictionary a prior that can serve well in inverse problems (such as the dictionary-
prior described in Section 5). The bounds provided here are important to validate
that only a small fraction of the signal space is covered by the Sparseland model,
in order to make sure that this prior is indeed discriminative.

131
7.2
Representation Abilities of L-atoms
In discussing the relative volume of all represented signals we will consider
only the normalized signals, which lie on an n-dimensional hyper-sphere. When
we represent signals using linear combination of dictionary atoms, a signal y can
be represented by the same atoms as its normalized version, and under the same
relative error. Therefore, this kind of constraint does not limit our discussion.
Each set of L independent atoms spans a subspace of order L. This sub-
space intersects with the n-dimensional hyper-sphere by another hyper-sphere of
dimension L. For example, lets consider a surface that passes through the origin
and intersects with the 3-dimensional unit ball whose center is the origin. This
creates a 2-dimensional ball, i.e. a circle, of the same radius (similar to the equator
drown on the globe).
We also allow a representation error which ℓ2 norm is no greater than ϵ, that
is:
∥y −Dx∥2 ≤ϵ.
(7.1)
This allowed error widens the intersection area and its eﬀect is just like a dilation
operation with an n-dimensional ball of radius ϵ. The result of such a dilation is an
area that includes signals which are outside the n-dimensional hyper-sphere, and
therefore should be intersected again with the unit n-dimensional hyper-sphere.
We go back to our simple example.
The allowed error causes the intersection
circle to become a torus, and after the intersection with the initial 3-dimensional
hyper-sphere we obtain a strip on the ball (similar to the area covered by a few
degrees of latitude northern and southern the equator). If the allowed error is
small enough, the strip is very close to being a cylinder. In this section we will

132
calculate the surface area of this generalized cylinder (in all possible dimensions)
and divide this by the surface area of the whole n dimensional hyper-sphere, in
order to get the ratio of represented signals out of all signals.
We will begin with basic geometrical facts. The surface area of a d-dimensional
hyper-sphere with radius r is given by
Sd(r) = 2πd/2rd−1
Γ(d/2) ,
(7.2)
where Γ is a function deﬁned by Γ(x) = (x−1)Γ(x−1), Γ(1) = 1 and Γ(1/2) = √π.
The volume of this hyper-sphere is
Vd(r) =
πd/2rd
Γ(d/2 + 1),
(7.3)
with the same deﬁnition for Γ.
The surface area of the intersection between the L-dimensional sub-space
and the n-dimensional sphere is SL(1) = 2πL/2rL−1
Γ(L/2) . In order to calculate the full
represented volume, we multiply the surface area of a L dimensional sphere by
the volume of an (n −L) dimensional sphere with radius ϵ, which represents the
allowed deviation in the n −L other dimensions. This multiplication gives us an
approximation, actually an upper bound, to the true covered area. As we will see
next, this approximation tends to be almost exact when the error is suﬃciently
small. The desired ratio, denoted as r(n, L, ϵ) is then:
r(n, L, ϵ) = SL(1) · Vn−L(ϵ)
Sn(1)
.
(7.4)
For better understanding this issue, and the type of approximation we are
using, we give some examples, in low dimensions.
When n = 2 and L = 1, we work in a two-dimensional world, in which all
normalized signals construct a unit circle. Our represented signals lie on lines

133
that go through the origin (sub-spaces of dimension 1). The intersection of one
line with the two-dimensional circle is two points, and allowing an error which
energy is no larger than ϵ - derives a deviation from this point of no more than
ϵ (see left side of Figure 7.1, where only one intersection point is presented). We
consider only the normalized signals in this circle (the bold sector in the ﬁgure).
The bounded length of this sector on the perimeter, denoted as l(ϵ), divided by
the whole perimeter of the circle is the desired answer. What is the exact size of
l(ϵ)? It is relatively simple to answer this answer - we create an isosceles triangle
by drawing a straight line perpendicular to the segment from the origin to the
intersection point, and draw straight lines from the origin to its sides. We then
look only at one half of the triangle, and receive tan(α) = ϵ ⇒l(ϵ) = 2 tan−1(ϵ).
The exact ratio, summing the two intersection points, is then 4 tan−1(ϵ)
2π
(recall that
r = 1). From the approximated ratio in Equation (7.4) we receive, r(2, 1, ϵ) = 2ϵ
π .
This can be derived from the exact expression by approximating tan−1(x) ≈x,
which is a third-order Taylor approximation. This approximation is illustrated in
Figure 7.2. The continuous line represents the identity function f(x) = x, where
the broken line represents f(x) = tan−1(x). We can clearly see the two graphs
coincide for small values of x.
Next, we examine n = 3 and L = 1. The represented signals lie again on a
line that goes through the origin, and intersects the unit three-dimensional sphere
in two point (see the middle illustration in Figure 7.1, where only one intersection
point is presented). In this case we imagine a cone whose base is the circle with
radius ϵ centered exactly in the intersection point. The head of the cone is in
the center of the three-dimensional ball. the surface area on the unit ball that is
bounded by this cone equals exactly 2πr(r−h), where r is the radius of the sphere,

134
e
r
(a)
r
                                                        (b)

































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































(c)
Figure 7.1: Area calculation scheme. On the left - n=2 and L=1, on the middle -
n=3 and L=1, and on the right - n=3, L=2.
0
0.5
1
1.5
2
2.5
3
3.5
0
0.5
1
1.5
2
2.5
3
3.5
angle (in radians)
(a)
Figure 7.2: Approximation of the function tan−1 using the identity function.

135
and h is the distance between the center of the sphere and the circle bounded inside
both the cone and the sphere (see Figure 7.3). Calculations similar to those done
before show that h = cos(tan−1(ϵ)). We remind the reader the surface area of a
three dimensional ball is 4πr2, and therefore the exact ratio, summing up the two
intersection points, is (1 −cos(tan−1(ϵ))). The approximation in Equation (7.4)
suggests r(3, 1, ϵ) = ϵ2
2 , which is, again, a third-order Taylor approximation.
h
r
(a)
Figure 7.3: Calculation of the surface area of a dome.
Finally, we examine the case when n = 3 and L = 2. The intersection of
a two-dimensional surface with a three-dimensional sphere is a circle of radius 1
whose center is in the origin. We allow a deviation of ϵ to each dimension, and
receive a strip that lies on the surface of this sphere. The exact surface area on the
unit sphere, resulted by projecting this strip on the sphere, equals sin (tan−1(α)).
The approximated ratio from Equation (7.4) is r(3, 2, ϵ) =
2π·2ϵ
4π
= ϵ, which is,
again, a third-order approximation of the exact ratio.
In order to ﬁnalize this discussion, we calculated the approximated ratio
of the represented area in dimension n = 8, and subspaces of dimension L = 3
with increasing representations errors. We compare our approximated ratio to

136
a measured one.
For this, we chose L atoms in dimension n, and generated
10,000,000 normalized points randomly. We then checked for each point whether
it is represented by the L atoms with an error no greater than the allowed one.
Figure 7.4 shows that up to an error of 16 degrees, the approximated ratio and
the true measured one are almost identical.
0
5
10
15
20
25
30
35
0
0.02
0.04
0.06
0.08
0.1
0.12
allowed error (in degrees)
Approximated ratio and calculated ratio, with L=3 and n=8
Approximated Ratio
Measured ratio (10,000,000) points
(a)
Figure 7.4: Approximated and calculated ratio, with n = 8 and L = 3, and
increasing error.
In the rest of this discussion we assume the allowed representation error is
small enough to neglect the above error. Using this knowledge, a simple theorem
can be stated,
Theorem 1: In order to cover a ratio p of the whole n-dimensional space using
no more than L coeﬃcients, with an Euclidean error less or equal ϵ, at least
p
r(n,L,ϵ)
subspaces are required.
The proof is trivial, emerging from the above discussion. We will illustrate

137
a simple usage of this Theorem. Let us consider we are interested in representing
at least p = 0.9 of all signals in an n = 3-dimensional space, using no more than
L = 2 atoms for each, with an Euclidean error smaller or equal ϵ = 0.01. We
calculate,
r(n, L, ϵ)
=
0.01,
(7.5)
p
r(n, L, ϵ)
=
90.
Therefore a minimum of 90 subspaces is required. Each two atoms can create
a subspace, so if we denote the number of required atoms as K, we receive the
following constraint
K
2

≥
90,
(7.6)
⇒K
≥
14,
That is, the dictionary matrix must be at least of size 3 × 14. Any smaller matrix
will necessarily derive that the ratio of represented signals is smaller than p.
7.3
Representation Abilities of a Dictionary
Given a speciﬁc dictionary D ∈Rn×K, how much of the whole space of
dimension n it can represent with an error no more than ϵ and with no more than
L atoms per-representation? We denote the answer to this question by ρ(D, L, ϵ).
Using the results from the previous section, it is easy to see that as long as D
includes at least L independent atoms (so that at least one L-dimensional subspace
can be spanned by it),
r(n, L, ϵ) ≤ρ(D, L, ϵ) ≤
K
L

· r(n, L, ϵ).
(7.7)

138
But those bounds are far from the true value of ρ(D, L, ϵ). It is obvious that there
are overlaps between diﬀerent subspaces (at least between intersecting subspaces),
and therefore, the total covered area is less than
 K
L

· r(n, L, ϵ). Also, the covered
surface area is obviously much more than r(n, L, ϵ), which represents the area of
one single subspace.
In what follows, we develop a tighter lower bound for ρ(D, L, ϵ), but ﬁrst,
we deﬁne some notations. We denote σs
min(X) (resp. σs
max(X)) as the minimal
(resp. maximal) singular value of any set of s columns from the matrix X. If s
is missing, σmin(X) (resp. σmax(X)) will serve as the minimal (resp. maximal)
singular values of the matrix X. Furthermore, if the matrix name is missing, we
refer to the dictionary matrix D, σs
min = σs
min(D), σs
max = σs
max(D). We also deﬁne
σmax(X, Y) as max{σmax(X), σmax(Y)} and σmin(X, Y) as min{σmin(X), σmin(Y)}
Theorem 2: Given a dictionary D, the ratio of represented signals, denoted by
ρ(D, L, ϵ), allowing L atoms for each representation and a representation error
less or equal ϵ that obeys
ϵ <
σmin
√
2σLmax
,
(7.8)
satisﬁes the following inequalities:
K
L

· r(n, L, ϵ) −
L−1
X
m=1
K
m

· γ(m) ≤ρ(D, L, ϵ) ≤
K
L

· r(n, L, ϵ),
(7.9)
where we deﬁne
γ(m)
=
min

r(n, L, ϵ), Sm(1) · Vn−m(R(m))
Sn(1)

,
(7.10)
R(m)
=
2ϵ
(σL−m
min )2 + (σm
max)2

σL−m
min + σm
max
σ2L−m
min
q
(σm
max)2 + (σL−m
min )2 −(σ2L−m
min
)2

.

139
The proof of the above theorem will be built in two steps. First we will deﬁne
a condition which guarantees that each non-intersecting two sets of L atoms cover
non-overlapping areas on the n-dimensional hyper-sphere. Then, we will bound
from above the overlapping area of two L-atoms sets whose intersection is not
empty. Doing so, we will be able to bound from below the coverage abilities of a
dictionary by summing the coverage of all L-atoms sets, and reducing the bounds
for the overlapping areas.
7.3.1
Representation by Non-Intersecting Sets of Atoms
Theorem 3: let D1 and D2 be two matrices of size n × L with normalized
columns, and we denote D = [D1D2].
If there exists an n-dimensional point x and two coeﬃcients vectors α1, α2 ∈
RL that obey ∥D1α1∥= 1,∥D2α2∥= 1 such that ∥D1α1 −x∥≤ϵ and ∥D2α2 −x∥≤
ϵ then ϵ >
σmin(D)
√
2σmax(D1,D2).
In other words, if we set the representation allowed error to be less than
σmin(D)
√
2σmax(D1,D2), there cannot be any point x that is represented by both D1 and D2
with an error less or equal ϵ.
Proof: Let us assume that there exists a point x and two coeﬃcients vectors
α1, −α2 ∈RL such that ∥D1α1 −x∥≤ϵ and ∥D2(−α2) −x∥≤ϵ. We then get:
D1α1 + D2α2
 =

D





α1
α2






≤2ϵ.
(7.11)
We denote the coeﬃcient vector as α =



α1
α2


, and its normalized version,
˜α =
α
∥α∥2
.
(7.12)

140
We divide each side of Equation (7.11) by ∥α∥2, and obtain
∥D˜α∥2
2 ≤
2ϵ
∥α∥2
.
(7.13)
Now, it is true that
∥D˜α∥2
2 = ˜α′D′D˜α ≥λmin(D′D)
⇒
∥D˜α∥2 ≥σmin(D).
(7.14)
We conclude that
σmin(D) ≤
2ϵ




α1
α2




2
.
(7.15)
Also, it holds true that for all vectors x and matrices A,
∥x∥2 ≥∥Ax∥2
∥A∥2
,
(7.16)
where ∥A∥2 is the induced norm1 of the matrix A. Using the fact that Diαi is
also normalized, we get
∀i ,
∥αi∥2 ≥
1
∥Di∥2
=
1
σmax(Di),
(7.17)
and then,
∥α∥2 ≥
√
2
max{∥D1∥2 , ∥D2∥2} =
√
2
σmax(D1, D2),
(7.18)
for the above and from (7.15) we get
σmin(D) ≤
√
2σmax(D1, D2)ϵ.
(7.19)
and this completes the proof.
2
1 The induced norm of a matrix A is deﬁned as maxx
∥Ax∥2
∥x∥2 , and for Euclidean vector norms
it is proved to be σmax(A).

141
The bound in the above Theorem is tight.
Let us look at the following
example, for which n = 5, L = 2. We set:
D1 =









1
1/
√
2
0
1/
√
2
0
−0.001
0.001
0









(7.20)
D2 =









1
1/
√
2
0
1/
√
2
0.001
0
0
0.001









(7.21)
In addition, we set ϵ to be 0.0005412, and x = [−0.9239, −0.3827, 0, −0.0005]T.
The vector x can be represented by the two matrices with an error of epsilon,
using the coeﬃcients vectors α1 = α2 = [−0.5412, −0.5412]T. In addition, for this
conﬁguration, it turns out that ϵ =
σmin
√
2σmax(D1,D2).
Let us now brieﬂy discuss the meaning of the above theorem. Given a dic-
tionary D, we can guarantee no overlaps between non-intersecting sub-spaces by
working with a small enough representation error, which depends on the prop-
erties of D. Setting ϵ ≤
σ2L
min
√
2σL
max assures us that no two distinct sub-spaces in D
overlap.
7.3.2
Representation by Intersecting Sets of Atoms
We now explore the possible overlap between two subspaces which share
mutual generating atoms. But before we dive into the detailed discussion, lets
ﬁrst discuss its intuition. Two sets of L-atoms, with exactly m mutual atoms
must intersect. Think about the surface spanned by x and y coordinates and the

142
one spanned by y and z. Each such surface intersects with the unit 3-dimensional
ball creating a unit-radius circle. The two circles intersect in two points - (0, 1, 0)
and (0, −1, 0). We denote this intersection of the two circles as ‘exact intersection
area’, and its shape, in the general case, is of a m-dimensional hyper-sphere, where
m is the number of mutual atoms. When those two circles become strips, because
of the allowed representation error, their intersection widens (see left side of ﬁgure
7.5). We denote the new intersection area as ‘widen intersection area’. The latter
is the one we are interested in bounding. The bound will be the surface area of
the ‘exact intersection area’, multiplied by the volume of an n −m-dimensional
hyper-sphere whose radius is the maximal distance between a point in the ‘exact
intersection area’ and a point in the ‘widen intersection area’ (see left size of Figure
7.5). The only measure which is still unknown is the above maximal distance. This
distance actually measures the maximal deviation from the mutual m-dimensional
sub-space to anyone of the 2L −m left possible directions, and therefore reﬂected
by the norm of the coeﬃcients of the non-mutual atoms.









































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































(a)























































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































	
							
	
							
	
							
	
							
	
							
	
	
	
	
	
	
	
	
(b)
Figure 7.5: On the left, illustration of two overlapping strips.
On the right,
illustration of the bounding area of the intersection.

143
Let D1D3 ∈Rn×L and D2D3 ∈Rn×L be two sets of L atoms from D,
such that D3 ∈Rn×m, and D1, D2 ∈Rn×L−m with distinct atoms from D. We
further denote ρ2L−m = σmin([D1, D2, D3]) and ρL−m = σmin{D1, D2}. An overlap
between the two corresponding covered strips includes a point x if,

[D1, D3]



α1
−γ1


−x

≤ϵ,
(7.22)
and

[D2, D3]



α2
−γ2


−x

≤ϵ.
(7.23)
As before, we write

[D1, D2, D3]






α1
−α2
γ1 −γ2







≤2ϵ.
(7.24)
The left part of the equation is greater or equal to the least singular value of the
matrix multiplied by the norm of the coeﬃcient vector,

[D1, D2, D3]






α1
−α2
γ1 −γ2







≥ρ2L−m







α1
−α2
γ1 −γ2







,
(7.25)
and we can set




α1
−α2




2
+


γ1 −γ2

2
≤

2ϵ
ρ2L−m
2
.
(7.26)
We now explore again Equation (7.24).

[D1, D2]



α1
−α2


+ D3

γ1 −γ2


≤2ϵ.
(7.27)

144
Applying norm rules and the same argument as before, we obtain
ρL−m




α1
−α2




−
D3

γ1 −γ2
 ≤

[D1, D2]



α1
−α2


+ D3

γ1 −γ2


We formulate the above two equations a bit diﬀerent, and use the inequality
from Equation (7.26), to receive,


ρL−m




α1
−α2




−2ϵ



2
≤
D32



2ϵ
ρ2L−m
2
−




α1
−α2




2

(7.28)
The above is a quadratic equation, for which the solutions bound the ℓ2
norm




α1
−α2




2
,




α1
−α2




2
≤
2ϵ
ρ2
L−m + ∥D3∥2

ρL−m + ∥D3∥
ρ2L−m
q
∥D3∥2 + ρ2
L−m −ρ2
2L−m

(7.29)




α1
−α2




2
≥
2ϵ
ρ2
L−m + ∥D3∥2

ρL−m −∥D3∥
ρ2L−m
q
∥D3∥2 + ρ2
L−m −ρ2
2L−m

The obtained lower bound is negative, and therefore uninformative. How-
ever, the upper bound gives us all the information we need in order to bound the
overlap area between the two strips.
We received an upper bound on the norm of the coeﬃcient that multi-
ply the non-mutual elements.
In order to bound the overlapped area we as-
sume the ‘worst case scenario’ in which the space spanned by the non-mutual
atoms is perpendicular to the space spanned by the mutual one, and then the
maximal distance from the mutual sub-space is bounded by this norm’s value
∥[D1, D2]



α1
−α2


∥≤∥[D1, D2]∥∥



α1
−α2


∥≤∥



α1
−α2


∥(we remind the reader

145
that the columns are normalized, so the maximal singular value of [D1, D2] must
be greater than or equal to 1).
The overlapping area will be the surface area of an hyper-sphere of dimension
m with radius 1, multiplied by the volume of an hyper-sphere of dimension n −m
with radius η(D1, D2, D3), where η(D1, D2, D3) is the bound on the coeﬃcient
norm, as written in Equation (7.29). This expression is as follows,
Sm(1) · Vn−m(η(D1, D2, D3)).
(7.30)
However, the upper bound can be bounded by itself by the area of one
strip, as the overlapped area cannot be greater than any one of the overlapping
areas. We therefore receive our ﬁnal upper bound for the overlapping area of two
strips [D1, D3] and [D2, D3], where the dimension of all atoms is n, the number
of columns of D3 is m, the number of columns in either [D1, D3] or [D2, D3] is L,
and the allowed representation error is ϵ by
min

r(n, L, ϵ), Sm(1) · Vn−m(η(D1, D2, D3))
	
.
(7.31)
We also deﬁne a bound that depends only on the number of mutual atoms
in the two strips, by deﬁning ρL−m and ρ2L−m as the minimum singular values as
required above, for all possible combination of 2 sets of L atoms with m mutual
elements.
We can now approximate a more strict lower bound on the coverage of a
dictionary D ∈Rn×K,
ρ(D) <
K
L

r(n, L, ϵ),
(7.32)
ρ(D) ≥
K
L

r(n, L, ϵ) −
L−1
X
k=1
K
k

· γ(k)

146
and this completes the proof.
2
We demonstrate the bounds we received in graphs. For this, we examine a
Grassmanian dictionary D ∈Rn×K, for which we assume the mutual coherence
is,
µ =
s
K −n
n · (K −1).
(7.33)
The mutual coherence µ is the largest oﬀ-diagonal value in the Gram-matrix
DTD. We use the Gershgorin Theorem [59] in order to bound the maximal and
minimal singular values of all sizes matrices,
σm
max ≤
p
1 + m · µ
(7.34)
σm
min ≥
p
1 −m · µ
Notice those bounds are very pessimistic, because we do not know the exact
dictionary, and cannot calculate the singular values exactly. This fact prevents us
from examining very redundant dictionaries.
Figure 7.6 displays the upper and lower bounds on the expected ratio of
the represented signals under the above terms. As can be seen, those bounds
indicate on very limit representation abilities, which we consider as an advantage
in Sparseland. Assuming those limited abilities, all we should think of when trying
to design a dictionary is an adequate representation of the signals of interest, while
the inability to represent other signals is almost guaranteed.
7.4
Summary
In this section we studied the relative volume of signals having a sparse
representation and a permissible error. However, much more remains to be done

147
10
11
12
13
14
15
1
2
3
4
5
6
7
8
9
10
11
x 10
−5
Coverage of grassmanian dictionary in dimension 10
L = 3,  representation allowed error:   0.1
Redundency Factor − K
Covered Ratio
 
 
Upper bound
Lower bound
(a)
Figure 7.6: Illustration of the coverage of a Grassmanian dictionary of dimension
10 for several redundancy levels

148
in order to use these theorems in the design of dictionaries. The main objective
should be to maximize the intersection between those represented signals and the
set of signals we desire to represent sparsely. Thus, localizing the covered area on
our geometrical sphere in the desired place.
Such a process is a great challenge that can eventually lead to an optimal
dictionary - a dictionary that can sparsely represent the signals of interest, and
in the same time, cannot represent sparsely other signals.

Chapter 8
Summary and Conclusions
8.1
General
This thesis concentrates on the subject of dictionary learning for sparse rep-
resentation of signals, based on the Sparseland model we have introduced. Under
this model, signal families (such as images) are assumed to be well represented as
sparse linear combinations of atoms from a predetermined dictionary. Naturally,
we expect that designing the appropriate dictionary will result with better treat-
ment of such signals, and lead eventually to better handling of them in various
problems, such as their reconstruction in inverse problems, their compression, and
more.
For this purpose, a novel algorithm – the K-SVD – is introduced in this
work. This algorithm can eﬃciently train a dictionary from a given set of signals
it aims to represent.
The K-SVD is the main contribution of this work, and
as such, it stands in its center. Our main aim in this work was to show that
beyond its ability to outperform its alternatives, such as the MOD algorithm,
it can be incorporated to practical applications in image processing and lead
to unprecedented performance.
State-of-the-art denoising results are achieved
using the K-SVD, when combined to a MAP-based approach that turns local

150
representation of patches into a global prior on the whole image.
Variations of this algorithm for structural constrained dictionaries are also
described in our work. Such variations are crucial for handling special cases of
interest, where the Sparseland model goes through some modiﬁcations. Such is
the case, for example, with the non-negative factorization option, and also with a
multiscale representation. Unfortunately, beyond putting the foundations for such
extensions, our work does not lead to successful applications using these modiﬁed
K-SVD methods.
We believe the ﬁeld of sparse representation by data-driven dictionaries has
a great potential, and the excellent results presented so far for denoising are only
the ﬁrst among many to come.
8.2
Future Work
Many future directions for the above-described work exist. We shall list
here several such directions that we ﬁnd as promising and intriguing:
• Building a ﬁrm model for representation of full size images, and proving
its applicability, is a great challenge. In this context, there is still much
room for improvement when it comes to a multiscale version of the K-
SVD, as we have outlined in this work. Indeed, a conceptual multiscale
representation using dictionaries of small sizes as considered here and
pursuit techniques to accompany it, both stand as major challenges. A
proper answer to these issues is likely to lead to much better handling of
signals.
• Applying trained dictionaries for other applications such as compression,
inpainting, classiﬁcation, pattern detection, denoising for video and color

151
images, and more, are all interesting directions that could and should be
explored.
• We proved uniqueness of the dictionary in the case of an exact represen-
tation. We expect that when approximated representation is assumed,
uniqueness does no longer exists. However, our experiments indicate that
a stability of the underlying dictionary does hold. Proving such a theorem
is not trivial, and is left for future work.
• When we discuss the coverage abilities of a given dictionary under known
representation and sparsity constraints, we did not refer to the most im-
portant question of localizing this covered area in the desired place. That
is, maximizing the intersection between the set of represented signals
by the designed dictionary and the set of signals we desire to represent
sparsely. Such a study is a great challenge that can eventually lead to an
optimal dictionary - a dictionary that can sparsely represent the signals
of interest, and in the same time, cannot represent sparsely other sig-
nals. Such analysis could also give a measure of quality to the model, and
enable its comparison to competitive methods, such as Markov random
Field (MRF) models.
• The Sparseland model is arbitrary and it is deﬁnitely not perfect. How
can it be extended to better describe signals? How such modiﬁcations
can be incorporated to the K-SVD algorithm? These questions are yet
to be explored, and have great potential in providing future techniques in
signal processing.

152

Bibliography
[1] M. Aharon, M. Elad, and A. M. Bruckstein.
K-svd: An algorithm for
designing of overcomplete dictionaries for sparse representation. To appear
in IEEE Trans. On Signal Processing.
[2] M. Aharon, M. Elad, and A.M. Bruckstein. On the uniqueness of overcom-
plete dictionaries, and a practical way to retrieve them. Journal of Linear
Algebra and Applications, 416(1):48–67, July 2006.
[3] A.J. Bell and T.J. Sejnowski. An information maximisation approach to
blind separation and blind deconvolution. Neural Computation, 7(6):1129–
1159, 1996.
[4] T. Blumensath and M. Davies. Sparse and shift-invariant representations of
music. IEEE trans. on Ausio, Speech, and Language Processing, 14(1):50–
57, January 2006.
[5] J. P. Brunet, P. Tamayo, T. R. Golub, and J. P. Mesirov.
Meta-
genes and molecular pattern discovery usingmatrix factorization. PNAS,
101(12):4164–4169, March 2004.
[6] E. Candes and J. Romberg.
Quantitative robust uncertainty princi-
ples and optimally sparse decompositions. Foundations-of-Computational-
Mathematics, 6(2):227–254, May 2006.
[7] E.J. Candes and D.L. Donoho. Recovering edges in ill-posed inverse prob-
lems: optimality of curvelet frames. Annals of Statistics, 30(3):784–842,
2002.
[8] E.J. Candes and D.L. Donoho. New tight frames of curvelets and the prob-
lem of approximating piecewise c2 images with piecewise c2 edges. Comm.
Pure Appl. Math., 57:219–266, February 2004.

154
[9] A. Chambolle, R.A. DeVore, N.-Y. Lee, and B.J. Lucier. Nonlinear wavelet
image processing: variational problems, compression, and noise removal
through wavelet shrinkage. IEEE Trans. Image Process., 7(3):319–335, 1998.
[10] S. Chen, S.A. Billings, and W. Luo. Orthogonal least squares methods and
their application to non-linear system identiﬁcation. International Journal
of Control, 50(5):1873–96, 1989.
[11] S.S. Chen, D.L. Donoho, and M.A. Saunders. Atomic decomposition by
basis pursuit. SIAM Review, 43(1):129–159, 2001.
[12] R. Coifman and D.L. Donoho. Translation invariant de-noising. Wavelets
and Statistics, Lecture Notes in Statistics, Springer- Verlag, pages 125–150,
1995.
[13] R. Coifman and D.L. Donoho. Translation invariant denoising. Wavelets
and Statistics, Lecture Notes in Statistics, 103:120–150, 1995.
[14] I. Daubechies, M. Defrise, and C. De-Mol. An iterative thresholding algo-
rithm for linear inverse problems with a sparsity constraint. Communica-
tions on Pure and Applied Mathematics, LVII:1413–1457, 2004.
[15] G. Davis, S. Mallat, and M. Avellaneda. Adaptive greedy approximations.
Journal of Constructive Approximation, 13:57–98, 1997.
[16] G. Davis, S. Mallat, and Z. Zhang. Adaptive time-frequency decompositions.
Optical-Engineering., 33(7):2183–91, 1994.
[17] A.P. Dempster, N.M. Laird, and D.B. Rubin. Maximum likelihood from in-
complete data via the em algorithm. Journal of the Royal Statistical Society,
Series B, 39(1):1–38, 1977.
[18] M.N. Do and M. Vetterli. Contourlets. Beyond Wavelets, J. Stoeckler and
GV Welland, Academic Press, 2003.
[19] M.N. Do and M. Vetterli. Framing pyramids. IEEE Transactions on Signal
Processing, 51:2329–2342, September 2003.
[20] D. Donoho and M. Duncan. Digital curvelet transform: strategy, implemen-
tation and experiments. Proc. Aerosense, Wavelet applications, 1999.
[21] D. L. Donoho and J. Tanner. Sparse nonnegative solution of underdeter-
mined linear equations by linear programming. Proceedings-of-the-National-
Academy-of-Sciences-of-the-United-States-of-America,
102(27):9446–9451,
2005.

155
[22] D.L. Donoho. De-noising by soft thresholding. IEEE Transactions on In-
formation Theory, 41(3):613–627, SeMayptember 1995.
[23] D.L. Donoho. Wedgelets: Nearly minimax estimation of edges. Annals Of
Statistics, 27(3):859–897, June 1998.
[24] D.L. Donoho. For most large underdetermined systems of linear equations
the minimal ℓ1-norm solution is also the sparsest solution. Technical Report
no. 2004-10, Department of Statistics, Stanford University, 2004.
[25] D.L. Donoho. For most large underdetermined systems of linear equations,
the minimal ℓ1-norm near-solution approximates the sparsest near-solution.
Technical Report no. 2004-11, Department of Statistics, Stanford University,
2004.
[26] D.L. Donoho and M. Elad. Optimally sparse representation in general (non-
orthogonal) dictionaries via l1 minimization. Proceedings of the National
Academy of Sciences, 100(5):2197–2202, 2003.
[27] D.L. Donoho and M. Elad.
On the stability of the basis pursuit in the
presence of noise. Eurasip Signal Processing Journal, 86(3):511–532, March
2006.
[28] D.L. Donoho, M. Elad, and V. Temlyakov. Stable recovery of sparse over-
complete representations in the presence of noise. IEEE Trans. On Infor-
mation Theory, 52(1):6–18, January 2006.
[29] D.L. Donoho and X. Huo. Uncertainty principles and ideal atomic decom-
position. IEEE Trans. On Information Theory, 47(7):2845–62, 1999.
[30] D.L. Donoho and I.M. Johnstone. Ideal denoising in an orthonormal basis
chosen from a library of bases. Comptes Rendus del’Academie des Sciences,
Series A, 319:1317–1322, 1994.
[31] D.L. Donoho and I.M. Johnstone. Ideal spatial adaptation by wavelet shrink-
age. Biometrika, 81(3):425–455, September 1994.
[32] D.L. Donoho and I.M. Johnstone.
Adapting to unknown smoothness
via wavelet shrinkage.
Journal of the American Statistical Association,
90(432):1200–1224, December 1995.
[33] D.L. Donoho and I.M. Johnstone. Minimax estimation via wavelet shrink-
age. Annals of Statistics, 26(3):879–921, June 1998.

156
[34] D.L. Donoho, I.M. Johnstone, G. Kerkyacharian, and D. Picard. Wavelet
shrinkage - asymptopia. Journal Of The Royal Statistical Society Series B
- Methodological, 57(2):301–337, 1995.
[35] M. Elad. Why simple shrinkage is still relevant for redundant representa-
tions. to appear in the IEEE Trans. On Information Theory, 2005.
[36] M. Elad and A.M. Bruckstein.
A generalized uncertainty principle and
sparse representation in pairs of bases. IEEE Trans. On Information Theory,
48(9):2558–2567, september 2002.
[37] M. Elad, B. Matalon, and M. Zibulevsky. Coordinate and subspace opti-
mization methods for linear least squares with non-quadratic regularization.
to appear in the Journal on Applied and Computational Harmonic Analysis,
2006.
[38] M. Elad, J.L. Starck, P. Querre, and D.L. Donoho.
Simultaneous car-
toon and texture image inpainting using morphological component analysis
(mca). Journal on Applied and Computational Harmonic Analysis, 19:340–
358, November 2005.
[39] K. Engan, S.O. Aase, and J.H. Husφy. Method of optimal directions for
frame design.
IEEE International Conference on Acoustics, Speech, and
Signal Processing, 5:2443–2446, 1999.
[40] K. Engan, S.O. Aase, and J.H. Husφy. Multi-frame compression: Theory
and design,. EURASIP Signal Processing, 80(10):2121–2140, 2000.
[41] K. Engan, B.D. Rao, and K. Kreutz-Delgado. Frame design using focuss
with method of optimal directions (mod). In Norwegian Signal Processing
Symposium, pages 65–69, 1999.
[42] K. Engan, K. Skretting, and J. H. Husφy. Family of iterative ls-based dictio-
nary learning algorithms, ils-dla, for sparse signal representation. Accepted
to Digital Signal Processing, March 2006.
[43] R. Eslami and H. Radha. Translation-invariant contourlet transform and its
application to image denoising. to appear in IEEE Transactions on Image
Processing, October 2006.
[44] M.A. Figueiredo and R.D. Nowak. An em algorithm for wavelet-based image
restoration. IEEE Trans. Image Processing, 12(8):906–916, August 2003.
[45] M.A. Figueiredo and R.D. Nowak.
A bound optimization approach to
wavelet-based image deconvolution. IEEE International Conference on Im-
age Processing, (II-782-5), 2006.

157
[46] W.T. Freeman and E.H. Adelson. The design and use of steerable ﬁlters.
IEEE Pat. Anal. Mach. Intell., 13(9):891–906, September 1991.
[47] J.J. Fuchs. On sparse representations in arbitrary redundant bases. IEEE
Trans. on Information Theory, 50(6):1341–1344, 2004.
[48] R. Gastaud and J.L. Starck. Dynamic range compression : a new method
based on wavelet transform. Astronomical Data Analysis Software and Sys-
tems Conference, Strasbourg, 2003.
[49] A. Gersho and R.M. Gray. Vector Quantization and Signal Compression.
Kluwer Academic Publishers, Norwell, MA, USA, 1991.
[50] M. Gharavi-Alkhansari and T. S. Huang. A fast orthogonal matching pur-
suit algorithm. International Conference on Acoustics Speech and Signal
Processing, Seattle, USA, pages 1389–1392, May 1998.
[51] G.H. Golub and C.F. Van-Loan. Matrix Computations. John Hopkins Uni-
versity Press, 3rd edition, Baltimor, 1996.
[52] I.F. Gorodnitsky and B.D. Rao. Sparse signal reconstruction from limited
data using FOCUSS: A re-weighted norm minimization algorithm. IEEE
Trans. On Signal Processing, 45(3):600–616, 1997.
[53] R. Gribonval and M. Nielsen. Highly sparse representations from dictionaries
are unique and independent of the sparseness measure. Aalborg University
technical report R-2003-16, 2003.
[54] R. Gribonval and M. Nielsen. Sparse decompositions in unions of bases.
IEEE Trans. on Information Theory, 49(12):3320–3325, 2003.
[55] O.G. Guleryuz. Weighted overcomplete denoising. Asilomar Conference on
Signals and Systems, Paciﬁc Grove, CA, November, 2003.
[56] O.G. Guleryuz. Nonlinear approximation based image recovery using adap-
tive sparse reconstructions and iterated denoising: Part i - theory. IEEE
Trans. on Image Processing, 15(3):539–554, March 2006.
[57] O.G. Guleryuz. Nonlinear approximation based image recovery using adap-
tive sparse reconstructions and iterated denoising: Part ii - adaptive algo-
rithms. IEEE Trans. on Image Processing, 15(3):555–571, March 2006.
[58] E. Haber and L. Tenorio. Learning regularization functionals. Inverse Prob-
lems, 19:611–626, 2003.

158
[59] M. T. Heath. Scientiﬁc Computing, An Introductory Survey. McGraw-Hill,
New York, second edition, 2002.
[60] P. O. Hoyer. Non-negative sparse coding. Neural Networks for Signal Pro-
cessing XII (Proc. IEEE Workshop on Neural Networks for Signal Process-
ing), pages 557–565, 2002.
[61] P. O. Hoyer. Non-negative matrix factorization with sparseness constraints.
Journal of Machine Learning Research, 5:1457–1469, 2004.
[62] M. Jansen. Noise Reduction by Wavelet Thresholding, volume 161. Lecture
Notes in Statistics, Springer-Verlag, New York, 2001.
[63] P. Jost, S. Lesage, P. Vandergheynst, and R. Gribonval. Motif: An eﬃcient
algorithm for learning translation invariant dictionaries. IEEE Proceedings,
Toulouse, France, May 2006.
[64] L.A. Karlovitz.
Construction of nearest points in the ℓp, p even and ℓ1
norms. Journal of Approximation Theory, 3:123–127, 1970.
[65] K. Kreutz-Delgado, J.F. Murray, B.D. Rao, K. Engan, T. Lee, and T.J.
Sejnowski. Dictionary learning algorithms for sparse representation. Neural
Computation, 15(2):349–396, 2003.
[66] K. Kreutz-Delgado and B.D. Rao. FOCUSS-based dictionary learning al-
gorithms. In Wavelet Applications in Signal and Image Processing VIII,
volume 4119-53, 2000.
[67] C.L. Lawson and R.J. Hanson. Solving Least Squares Problems. Prentice-
Hall, 1974.
[68] D. Lee and H. Seung. Learning the parts of objects by non-negative matrix
factorization. Mature, pages 788–791, 1999.
[69] D.D. Lee and H.S. Seung. Algorithms for non-negative matrix factorization.
Adv. Neural Info. Proc. Syst., 13:556–562, 2001.
[70] S. Lesage, R. Gribonval, F. Bimbot, and L. Benaroya. Learning unions of
orthonormal bases with thresholded singular value decomposition. IEEE
Intl Conf. on Acoustics, Speech and Signal Processing, 2005.
[71] M.S. Lewicki and B.A. Olshausen. A probabilistic framework for the adap-
tation and comparison of image codes. Journal of the Optical Society of
America A Optics, Image Science and Vision, 16(7):1587–1601, 1999.

159
[72] M.S. Lewicki and T.J. Sejnowski. Learning overcomplete representations.
Neural Comp., 12:337–365, 2000.
[73] S. Mallat and E. LePennec. Bandelet image approximation and compression.
to appear in SIAM Journ. of Multiscale Modeling and Simulation, 2005.
[74] S. Mallat and E. LePennec. Sparse geometric image representation with
bandelets. IEEE Trans. on Image Processing, 14(4):423–438, April 2005.
[75] S. Mallat and Z. Zhang. Matching pursuits with time-frequency dictionaries.
IEEE Trans. Signal Processing, 41(12):3397–3415, 1993.
[76] M.W. Marcellin, M.J. Gormish, A. Bilgin, and M.P. Boliek. An overview of
JPEG-2000 (2000). pages 523–541, 2000.
[77] B. Matalon, M. Elad, and M. Zibulevsky. Improved denoising of images
using modeling of the redundant contourlet transform. Proceedings of the
SPIE conference wavelets, 5914, July 2005.
[78] P. Moulin and J. Liu. Analysis of multiresolution image denoising schemes
using generalized gaussian and complexity priors. IEEE Transactions on
Information Theory, 45(3):909–919, April 1999.
[79] J. F. Murray and K. Kreutz-Delgado. Sparse image coding using learned
overcomplete dictionaries. IEEE International Workshop on Machine Learn-
ing for Signal Processing, September 2004.
[80] J.F. Murray and K. Kreutz-Delgado. An improved focuss-based learning
algorithm for solving sparse linear inverse problems. In IEEE Intl Conf. on
Signals, Systems and Computers., volume 4119-53, 2001.
[81] B.A. Olshausen and B.J. Field. Sparse coding with an overcomplete basis
set: A strategy employed by v1? Vision Research, 37:3311–3325, 1997.
[82] B.A. Olshausen and D.J. Field. Natural image statistics and eﬃcient coding.
Network: Computation in Neural Systems, 7(2):333–9, 1996.
[83] Y.C. Pati, R. Rezaiifar, and P.S. Krishnaprasad. Orthogonal matching pur-
suit: recursive function approximation with applications to wavelet decom-
position. Conference Record of The Twenty Seventh Asilomar Conference
on Signals, Systems and Computers., 1, 1993.
[84] Y.C. Pati, R. Rezaiifar, and P.S. Krishnaprasad. Orthogonal matching pur-
suit: recursive function approximation with applications to wavelet decom-
position. Proceedings of the 27 th Annual Asilomar Conference on Signals,
Systems, and Computers, 1993.

160
[85] F. Patin. An introduction to digital image processing. June 2003.
[86] P. Pauca, J. Piper, and R. Plemmons. Nonnegative matrix factorization
for spectral data analysis.
Journal of Linear Algebra and Applications,
416(1):29–47, July 2006.
[87] J. Portilla, V. Strela, M.J. Wainwright, and E.P. Simoncelli. Image denoising
using scale mixtures of gaussians in the wavelet domain. IEEE Transactions
On Image Processing, 12(11):1338–1351, November 2003.
[88] B.D. Rao, K. Engan, S.F. Cotter, J. Palmer, and K. Kreutz-Delgado. Subset
selection in noise based on diversity measure minimization. IEEE Trans. on
Signal Processing, 51(3):760–770, 2003.
[89] B.D. Rao and K. Kreutz-Delgado. Deriving algorithms for computing sparse
solutions to linear inverse problems. Conference Record of the Thirty-First
Asilomar Conference on Signals, Systems and Computers. IEEE., 1:955–9,
1998.
[90] B.D. Rao and K. Kreutz-Delgado. An aﬃne scaling methodology for best
basis selection. IEEE Trans. on Signal Processing, 47(1):187–200, 1999.
[91] S. Roth and M.J. Black. Fields of experts: A framework for learning image
priors.
IEEE Conference on Computer Vision and Pattern Recognition,
2:860–867, June 2005.
[92] P. Sallee and B. A. Olshausen. Learning sparse multiscale image represen-
tations. Adv. Neural Information Proceeding Systems, 15, 2003.
[93] S. Sardy, A.G. Bruce, and P. Tseng. Block coordinate relaxation methods
for nonparametric signal denoising with wavelet dictionaries.
Journal of
Computational and Graphical Statistics, 9:361–379, 2000.
[94] F. Shahnaz, M. Berry, P. Pauca, and R. Plemmons. Document clustering
using nonnegative matrix factorization. Journal on Information Processing
and Management, 42:373–386, 2006.
[95] E.P. Simoncelli and E.H. Adelson. Noise removal via bayesian wavelet cor-
ing. Proceedings of the International Conference on Image Processing, Laus-
sanne, Switzerland, September 1996.
[96] E.P. Simoncelli, W.T. Freeman, E.H. Adelsom, and D.J. Heeger. Shiftable
multi-scale transforms. IEEE Trans. on Information Theory, 38(2):587–607,
1992.

161
[97] E.P. Simoncelli, W.T. Freeman, E. H. Adelson, and D.H. Heeger. Shiftable
multi-scale transforms.
IEEE Trans Information Theory, 38(2):587–607,
March 1992.
[98] P. Smaragdis and J. C. Brown. Non-negative matrix factor deconvolution;
extraction of multiple sound sources from monophonic inputs. IEEE work-
shop on application of signal processing to Audio and Acoustics, pages 177–
180, October 2003.
[99] J.-L. Starck, E.J. Candes, and D.L. Donoho. The curvelet transform for
image denoising. IEEE Transactions On Image Processing, 11(6):670–684,
June 2002.
[100] J.L. Starck, E.J. Candes, and D.L. Donoho. The curvelet transform for
image denoising. IEEE Trans. on Image Processing, 11:670–684, 2002.
[101] J.L. Starck, M. Elad, and D.L. Donoho. Image decomposition: Separation
of texture from piece-wise smooth content. SPIE conference on Signal and
Image Processing: Wavelet Applications in Signal and Image Processing X,
SPIE’s 48th Annual Meeting, 3-8 August 2003, San Diego, 2003.
[102] J.L. Starck, M. Elad, and D.L. Donoho. Image decomposition via the com-
bination of sparse representations and a variational approach. IEEE Trans.
On Image Processing, 14(10):1570–1582, October 2005.
[103] T. Strohmer and R. W. Heath.
Grassmannian frames with applica-
tions to coding and communication. Applied-and-Computational-Harmonic-
Analysis, 14:257–275, 2004.
[104] J.A. Tropp. Greed is good: Algorithmic results for sparse approximation.
IEEE Trans. Inform. Theory, 50(10):2231–2242, October 2004.
[105] J.A. Tropp.
Just relax: Convex programming methods for subset selec-
tion and sparse approximation. Technical Report 04-04, ICES Report, UT-
Austin, 2004.
[106] J.A. Tropp. Topics in Sparse Approximation. PhD thesis, University of
Texas Austin, 2004.
[107] M. B. Wakin, J. K. Romberg, H. Choi, and R. G. Baraniuk. Wavelet-domain
approximation and compression of piecewise smooth images. IEEE Trans.
of Image Processing, 15(5):1071–1087, May 2006.
[108] S.C. Zhu and D. Mumford. Prior learning and gibbs reaction-diﬀusion. IEEE
Transactions on Pattern Analysis and Machine Intelligence, 19(11):1236–
1250, November 1997.

162


 	





  
 !#"$ 	&%'(	
)	&%	
 *	&+	
,-
.
	0/1( !0-
2&	3 4'567	&

80589	3$6:;+;+	&
<
=	&: 6'!>
? 	&:	&@"A520/18 6'
 '
'B-
0/1

C
	&%*D(

E"	&
F
GHG

GI+JLKNMPO3QRITS?KNMPUVO3W5X1KYNMHUZO0[KNMOHG
.
\
	
?6%]	/1
  & & '?	3!T0#!'?0/1	3]2&	&!
<
2	3'?	&* ];+-A=] '
6 A^	=
	/1	&4	& & & _-?
V	&% `6
.
9	3a	/1 ?0
V%]	/1Eb,	/c9	3
0/1


2	3'?	&* 6
<
d	&
];+-
9	3!
  
<
/1]		&
	
0
V%
<
	/1 2&	&
	3",] '	&a2	3
.
0/1;?	/1Bb	&,'!
'

!=?	&6%
*	&0
V
 0'	-
A(	&,0= T#	0=e!( 	&=Af*	&04 6''
2&	3 .
2&	3E2/1 '
<

V	&%d0
V

,"

	3
;T%	3	
<
0/1;T	/1
'
 E /g'
%	'"f ,
		&   h!;+	&'a
V	&%
`/g0/1 <
,?0'%	
?-	 	& L %	'"`!4 !!;+	
<
	3	0=_'-!`,-
.
	=,!
<
	36c7- !i,/17	0/17	&  ;j	'6'
	&;j=	
0/1	3'B	& 670/1	&
k

	&
V	&;
		&lm
;?
V	&%	'6
.
	&7	/1
	&-
  
'D%/1*	f6'=
<
	
	'	&* 7	&%	 74,-

!=_ 
V4 #-=_2&=_	
V%	& 	& '-	
.

nAop$qr&st+r&ussnTqr&psvt+nTqwTxr&yTqvpsr
x
suto!p)yxsn
nz
{
|Vr&r&sz
{
t+r&}rdns~&svAor0|Vv

r&swfo!p)r&dnpds0|
wsn?s1rqcsu1r&?s01r3os
wpr
qsur1nantjrx}n

s01r3os?r3os0urcs
os
ot_s0|Vr&~ssLwpr0gx?natj}a}r&t+sno!p:nxuv]nvs&z?}an&s0qnaqwTsuqr&ar1w

n!o!suq
{
vz8svr&zr1w
sqsvr|Pow
xr&vs&
s}x&r&vqNo
s
os
otjn|Vr~ssnq
{
n
sr&r&n
s
o!
sv
rgqt+rx}+x
{
r3ow$xr&vs&$sqsvr|owx)pr&spqrgst+~n$qr&syvr&ws0qn$qr&w~rqn)y}(|Esvwqr

r1w
s0sp
op4nvs&zx
sqsvr|ow

s01rqcsu1r&?s01r3os]&r&sw]vr&~Aos}r&t+s

s}x
tjr&s
on7qs
wxn7r&wx7qvtj|Vr&Er1s01Nop

rqr&wlo!pn~rxBq1snx
q
}Br&v~r&sotjr&n7qu1nEs0fo}p

s
xrg
rnpoAar3oso!}]qr3os
ot^qr&s~1s
{
&r3o!snaqw?wr&~!or1s
o}

vrgu!osr&wv]nqr&w]qszszx?nAowp
nAosuq
t+susLn&n]r3osnTwn]n1sn
{
}aqrqr&wn]q~rx_qw]|Z~ss
oao!rsp4zrdr3osTss&p:0qswn]rw

s0|Vr&~ssLs
s
os
ot

s+z(nxuv$qrqr&w)q~rx)0q1snx)s0(sus0r&(r01w
{
s~r3osw)quqr
ssr&z

qr3os
ot4s0x|o
s0|Vr&~ssn
{
r1sn]nL&r3os
uvnx
tjsus

nur&n
vuw]pr&su!osyvr&ws0qTr&t+s~anr&r&n]r&
n
&r3os

vx
r0gqt+rx}
n$qsvr|ow$x~s0


s0gr3os)&r&sw!o
{
n|V~r&npqsvn$nAowpn>o}:n1r&}n

nqsvr|ow


}x!ovr&n8qsvr|ownmo!pnvsps?nAo+o!nr01sn
s
ns~&syrrgnqs
qsvr&yr&r&n

]NP? `¡R¢+£P¤0¥¢N¤£
¦
{
pdx:}r&t+sn
P§¨¢£L©

n$qsvr|ow:r

P§¨¢£L©
{
n:|


ssvs}Bs0x!op(s01pmos0
ª
os
ot]|Vr&~ss
rq«&r3o!sBquqEqrqr&wnDo!p
{
&r
osn7rt+}r
|Vr&~ssdvr&sp(vr&~Ao
qrqr&wn

nqsvr|owp¬pd

P§¨¢£L©
&r3osnrt+}xAopdx­os}

}r&~s8qr3or&}
{

oAs}
n:qsvr|ow


n«xAopdx

qr3or}

{
rpwr

qsvr|own:urqs7qw$svwq:r01w
rxapr&spn_qw]psun!oq1o}_qr&w|Vr&t^vzTswvr
{
r3op4qr1r&vqsnaqwr
svr&?sqsvr|ow]s01®o}
svuw

o!p)r0qr3os}sqwT|Zs~n!oq1fo!}
qsvr|Pow
n


{
r01w
r0x7spqp
t+vr&nTvr&~No
qr1r&qxBpd}v?q

suq:r1w:+Evr&~Ao
r&^ntjrx}x
z
s
qr1r&q:p}v7qt+vr&n!oiqs
ooAEn
r0q
su1r&$s01r3osx:pdr&sp
s01rq

ot+r&Do}7szzxq7r1w
s01wszss0x
sv&r|Vr
nysp
qsrg
n1r&qx7uq¯o!?|Vr&~ssdnvs}p(nyr&p
}]qp}vr&n

vux1n?&r3osnTs
{
}]n1r&qnTvr&&upr

r&snBs&Ao!unT&s0xT}r&~sTs
qp}vr&nTn1r&qnT&s0xrds0|V~
qsvrgn

vrqx
°
vux1n:&r3osn
°
sv4qrsr&vpw:s
qps01urx:r1w
r
q
±
o!p8n~rxlo}Er&wpr3os
qr&s&cqr1r&q
{
n~}anr|Vn]n1r&qno}a&r&wp$&r3osr

np}!o
{
}

n1r&qno!}]tjx!oAx]qr&srg]qr3or&}as
n!or0Tn1r&qnBs0x|PoDs
ooATr3oswBvr&|olsus
o~Br1w
{
w~rqp
o}?n!o!r&}]wrdnr&rqpBrq
qr&}r&t+snTqr&w~rqn
n
nLr&uqx?r&s0avqr&s0xaqrxr&y

o!p8vqr&s@qrxvr&4qrwzv|V:}rx²o³ssr&p}n$ssr&z$s0v~io!}Eqr1}Aoiq15o!}
°
qr3os
ot+niot+r&
{
°
1w
r
n(qsvr|o!w¨o!p´qr&s~wsvr?svwq


s~r3osw(s
o}xs01r3o!s(&r&sw!o
ss0gx

pqr&yp+n
o

µB¶·T¸a¹
º»ºEº¼º¼&½B¾V¼&¿ÀÀ
ÁÀÂÀ0ºÀÁÀ0Ã1¼3ÄÀÅ
Æ
Ç+ÅÀÅÈÅDÄ¼&Çj¾^ÁÀÅ¼&É½7ÂÊËÅEÁÀ
ÄÀ0ÌÅÈ
Æ
ÂÍÊ½Å
Íd¼&ÅÀÍ
Î
º¼&À¿Ã1À0ÎÅ¼gÏ
¼&ÀÂ½À0Ã1À
Ä
º
º¼3ÄÀ
ÄÇ
º¼º¼&½]¾V¼&¿ÀÀLÐÂ¼&¿AÄ
Æ
Ñ&ÈÎ
ÁÀÅ¼&É½È?ÂÊËÅ
Í¼&ÂÇ+È
º¼&½È?Ç+ÅÀÅÅ?ÈÎÂÈÎ]Ñ&ÉÏ
Ò
º¼&ÍÀ0¾
¼
Ä½
º¼ÎÂº¼&À¿ÏÀ
ÄÊ½Â¼ÎÓ8Çj¼&½Å8º¼3ÄÀÓÀ0Ì8¼&»Ì¼&È8º¼º¼&½Ç+¼ÎÀÓÎ8º¼&ÂÀ0ºÀ¼Bº¼3ÄÀ
ÄÇmÄÍ
Æ
ÈÀ0Ã1À0Î
Ñ
ÈËÀ»Ç
Æ
Ð¼&ÊÀÈEº¼&ÀÓÎÎ:ÈÀ¿Ô&ÀÂAÄ¼¾VÂ
Æ
ÈÔ&À
Ä!Ã1½
Æ
Çj¼&Ó¼«¾Z¼&¼&ÀË
Ò
ÈÍdÓÅ!Ä
Æ
Ç+¼&Ç+À&ÏÈ$ÁºÀÂ¼¾PÄ½5Ä!Í8¼º»!Ä¿È
Õ×ÖØÚÙ_ÛÜÜÜ
ÈaÀÅÇÏÅaº¼3Ä!À
ÄÇÚÄÓ^ºËË¼ÎÅ
Ý
ÞßAàháâ&áNã
º¼&ÀÓÎÉaº¼Ã1¼&ÅºÎ
Ò
¼Ã1ÀÅÀ
Î
Æ
ÌaÂÍ½Ì
¼
ÂÀÈÅaÎ¼ÍÀ»È]»
Ä!¼&Ô&¼
Æ
ÁÀ0¾V¼&¿ÀÀ
¼3Ä½Ì
Æ
ºÎÍ»ÃBÁº½À¿Å(ÂÍ½
ä
ºLÀ0Ã1ÔÎÔÎ
ä
Æ
º¼º¼&½(Ç+¼ÎÀÓÎ(Áº¼3ÄÀÓÀ`ÄÍdÎÁÀÀÉÃ1¼&¼3ÄÂ)Âº¼&À¼]Âº¼&À]ÁÀ0ÌÊ¼&È
ÁÀÀË½!Ä#Ï]ÁÀÀ
ÄÅÂ¼Ã1¼ºÂ¼&½?ÁÀ0Ã1¼3ÄÀÅaº»ºBÁÀÀÉÂÇjÃ1ÉËÈ?ÁÀ0¾V¼&¿ÀÀÈ]À0Ã1Ê®ÄÓ]ÁÈÀ0º¼Ã1¼&ÂºÀ¼
Ò
Ñ&½Ì:ÁÀ0¾VÀ¿Å:¼Ã1½:¼º¼&½4º¼º¼&½È4ºÂÀ¿À¯Ä!Ç+¼&Å!Ä>ËÀËÎÈ4º½4º¼&¼&ÈÅ:ÈÔ_Á¼&»ºÎ4º¼&»AÄ¿ÈÈ
Ò
ÀÊlÄÓ
ÈÔÚÄÇj¼&Å
Æ
ÈÃ1¼ÌÅÈ
ä
º¼3ÄÀ
ÄÇjÈ¨ÄÇ+¼&Å
ä
å
æVç@èéêìëíîèïðñ¨òó
áâ
ô
ºÅÀ¼&ËÅ(È»ÊÍÅÅ$º¼º¼&½
å
º¼Ã1¼&ÅºÄÍÅAÄ
ô
ÁÀ0ÃgºÀ0Ã
ÁÀ¼&ËÅEÑ&¼3ÄÀÅlÄÓÅ4ÁÀ
ÄÀ
ÄÇBÁÀ0¾V¼&¿ÀÀÀÇ+ÀÄ!ÓBÎ¼&É7Î¼&ÂÀ&ÏAÄ
Æ
È»ÊÍÅ!Ä5Á½º¼&ÅÈ
Ò
º½4ÏÀ¼&Ç+ÅÎEÂÀÇj¾VÈ!ÄDºÃ1ÅlÄÓ
Ã1½ÄÇ+¼&ÅÈ
¼
ÁÀÉÃ1Å!Ä½?ÈÓ+ÎÂ½!ÄfÁÀ&Ï¼gÏÔ
õ
¼Ã1½?Èº¼&½]º¼º¼&½Èaº»ÊÍÅ
À0Ã1ÀÀ0Ã1¼&ÓÅ
Á
¾V¿ÀÀ
Ä
Æ
À½
ÄNÌÄ!Í4¾V¼&¿ÀÀÈ]º¼Ì
º¼&½
å
ÄÍdÅ!Ä
Æ
È½À0¾VÍ$öË
Ä½ÅÀËÏÅ
À
ô
Æ
º¼&½fÄ!Ìa¾V¼&¿ÀÀ0ÎTº¼3ÄÀ
ÄÇjÈTºÇ+ÀÅ
å
ÂÊËÅfÄ!ÍÅ!Ä
Ä½ÅÀËÏÅ
À
ÀÅÇÏÅfÄ!Í
ËÊ½?ÁÃ1À½Í4¾V¼&¿ÀÀ
ô
Æ
ÁÀ½ºÅ]Ñ¼3ÄÀÅ¼
Ò
öÏ+ºTÈÔhÄ!Ç+¼&ÅÍ4È»Ã1ÈÎ
Æ
º¼&ÀÔÌÂÅaº¼&ÀÓÎ]À0ºÍdÎ]ÁÀ
Ä#ÏºÃc¼Ã1½
õ
Èaº½À¿Å
¾V¼&¿ÀÀ
È
Ä!À
ÄÇ
Ñ&¼&ÅÀ½¼
ÁÀ0Ã1¼3ÄÀÅ
Ò
Ñ0ºÃ1ÀÈÎ
ÂÀ0ºÀ@Ñ&¼3ÄÀÅ
Æ
À¼&ÂÍÊ+½È4ÂÊËÅ
ÀÊ¼&ËÃ1À½4½¼ÈEÈÔ«Ñ¼3ÄÀÅEº»º:º¼&½E¾V¼&¿ÀÀ
Älº¼
Ò
º½Ô^ÁÓ
Æ
Ä!À
ÄÇjÈE¾Z¼&¿ÀÀÈEº½À¿Å
Âº¼&À0Î
å
Âº¼&À0Î7ÑÉÏÈ7ÁÀÅ¼&É½È7ÂÊËÅBº½DÄÀ
ÌÅÈ7ÈÔÂÅ¼3Ä!Ì
ô
º¼&½7Â¼ÎÓ
Ñ¼ºÃ
ÈÀÓÎBÈÃ1ÀÈ
÷]Ö
Ý
ÈÍ@Ï
Ò
ÂÊËÅ
ÄÀ
ÄÇ+È4Ñ&¼&ÂºÀÊÈ4º½:ÐÂÓÍÄ³ÁÀËÃ1ÅÈ4Î¼&ÂÀ&Ï4ÀÅºÀÂ¼¾Ä½
ÂÎÌÅ:ÈÔ^¼&Ó¿¼&È
Æ
Ã1À0Î$ÁÀÉ!Ä!¼Î¼
À
ÁÀÅºÀÂ¼¾Ä½$ÁÈ
È(º»ÊÍÅÅ
Ý
ñ
ßNãø+ùúûü
Öýhþÿý
ú
ã
Æ
À0Ã1Ç+Å»ÈÂ¼&¿Î8ÁÀ0¾V¿ÀÀÅÈ8ÁÀÅ¼&É½È(º½8ÁÀÂ»¼0ÎÈ
º
Æ
ÁÀÅºÀÂ¼¾Ä½¼
ÈTº»ÊÍÅÅ
Ý
 ß
ÿ
ú
ÿ#ýhþÿý
ú
ã
ÁÀÓÀ¿ÅÈ
º½7Ð¼&ÊÈ!Ä
ÓBÈÂ¼&ÅÏNÄDÈÀÓÎÈ

À
¾V¼&¿ÀÀÈ7Â¼&ÉÏ¼Ä!Ó¼3ÄÀ½È7Ð¼ÌÀÂ
Æ
¼
Ä
ºÊ
¼
Â
ÈÂ¼&¿Î
ÄÀÓÀ
È
º½
ÈÍdÇ+»È]ÈÀ0ÓÎÈ
Ò
Å½Å
ÂÏ»Å
ÂÌÀ0ÃdÀ
ÏË¼&ÓaÑ&ÀÀÇ+Ó¼cÏËÓ
Î
Ã
À
ÄÑ&¼&ÀË
È
ÂÓ
À
ºÄ¼0ÌÀº½?Ð
ÄÍÈ»AÄ¿ÈÈ
º½¼@¼3Ä½4ÁÀÅºÀÂ¼¾Ä½
ÈÎÂÏÈ
À0ºÀÅ½ÈEÑ&¼&ÂºÀÊÈ7Ñ&À0Î¼@ÁÇ+ÀÄÓBÓ¿¼ÅÈEÑ&¼&ÂºÀÊÈ7Ñ&À0Î
Ò
º¼&½¿¼º
º¼&ÓÀ0ºÊÅ
ÏÀÊËÅ³Ä!À
ÄÇ4À0ºÀÅ½È)Ñ&¼&ÂºÊÈ¼_ÈÇ+ÀÅÎ)À0Ì$¼&½ÂÈ
Æ
ÁÀÓ¿¼&ÅÈ$Î¼&ÂÀ&ÏÈ)ÀÅºÀÂ¼¾Ä½
AÄ»AÄ¨¼&»À
Ä¿À
ÈÔÑ&¼&ÂºÀÊ
Ò
ÈÍÓÅ!Ä
Æ
º¼&ÀÉÂ¼&½À0ºEº¼&½¿¼º
º¼&Ç+Ç+¼&ÓÅ
º¼&¼&ÈÅB¼3Ä!½
ÈÔd½Í¼0ÃgÎEÏ¼&ËÀÓAÄDÑ0ºÀ½7ËÀËÎ
Æ
¾Z¼&¿ÀÀÈ7Ñ0ÌÍ
È
À0Ã1¼&À»]À
Ä!Ì_¼Ã1ÀÈÄ!À
ÄÇ
¼0Î
Ò
È!Ä!½ÍÈ
ºÀÔÌÂÅÈ
ÀÀ0Ã1ÍÈ
È
ÈÃ1ÀÈ
º¼&ÈÔ
Ñ¼3ÄÀÅÈ
º¼º¼&½È_¾V¼&¿ÀÀ
ÄÍdÅÍÅÈ
Ò
º»½?ÈÍÀ
¾
ÈÔ½Í¼Ã²Ä
À¿Å
Ó
Íd¼&ÅÀÍ$È
¼3ÄÀÅÎ
ÁÀ0Ã
ÀÈÍÄAÌTÈÌÂÓÈTÐ¼ºÅ7ÁÀÓÎ¼Ã1È
º¼º¼&½ÈBÀ
Î¾Ä
È»Ê+ÍÅÎ
Æ
À0ÃgÎ¼
À
ÈÃ1¼&ÓÈTÀÉÅºÅfÄÇ+¼&ÅTº
Ä
ÈÌÂÓÈ
Ô
¼
Ò
Ã1ÀÈ(ÈÃ1¼&Â»½!Äm¼¾Z¿¼&ÈÍ´¼3Ä½Ì(ÁÀ0Ã1¼3ÄÀÅ!Ä¨º¼&½Å¾V¼&Ç
Á
È
Ý
ø
ýhþ
àháâáNã
ÿ
È¼
Ý
ø
ò
ûã
ò
ýhþ
âáNã
ÿ
Æ
ÁÀ0Ã1¼3ÄÀÅÁÃ1ÀÈÍ
Ñ&ÀÓÉ¼gÏÅ!Ä³ÁÀ&ÏAÄ»Eº¼º¼&½4¾V¼&¿ÀÀ
Ä³ÁÀÅ½º¼&ÅÈ$ÁÀ&ÂÀ0ºÀ
Ò
¼&Ô^ÈÀÓÎAÄ5ºÂ»½4ÈÍdÀ0¾
Ä!ÓEÑ¼3ÄÀÅÈ4º½:ÑÅ½!Ä>ÈÃ1ÀÈ
¿¼ÎÏ
º
È
º¼º¼&½
ÈÅ¿Ó
Èº¼&½
¼Ã1½
À0Ã1ÀÀ0Ã1¼&ÓÅ
Á
È!ÄÀ
ÄÇEÈÂ¼&¿Î$¾V¿ÀÀ
Ä
Ò
Ñ¼3ÄÀÅ
Ì
È»ÊÍÅ!ÄmÁ½º¼&À_ÈÔ
ºÅÀ¼&ËÅ



	


! 

"
#$&%
'
(
$*)

+,
 

"
#$.-
'
(
$*)
'
/
"
#$.-
'
(
$&%

	102

3
,46578694;:<,
=?>
#A@B
+
C
;D,46578
	102

0

EF5786GH
I
86F
I
	10J

2
+KF:<,

5
:<KD
 
C
9+FG+

;94K57K
#$ML
'
(
$.-
'

 
F:<

!N
:

	10;O
FJ
! 

P
;65;+,9F

Q,9


 

/
"
R9D
 


S,9


 
T6
 

	U2;3

O
FD
 
9+//

+V
F5
 
96:/




W
XY
>Z[Z<\>
#;]
>
#_^
Y
>6\`
a
N

	1b
9F;9986G9+
	U2

cdFe
f1ghiGj;hklhjAmnkop<hf
i6q7hojirmtsGuMsgiGj
v
wyx1z|{}x1z<xv
gh~s
.&

mtsGohkoiKhm!oi
j;us6ghoiGiq7hoji
e
cd

gh6ui

kh

i6p<q7i

gog6iKgsjk
m!fUpoKguk6Gus6giGjf1ghijhklhjnm!u

ikhhuo~GgkhjoiGshlo~

cdFe

e
cd
1
h
m!oig6~kg6o~hq7umtuis1uii



jf1ghinm!uQjh6hijhklhjinm!s
uMs6gi
iq7hoj;~
v
rA <¡v
gh~s
.*¢6£
e
d

gf1oG
m!
m!u¤k~hg¥jklo

ilgoGgh~s
¦
.§A¨;©
c
d
ª
jghp?m!k
««n¬­_¬[®;¯±°
;²
d
³
i´m!sotm
µ
o
mSm!kouo
¶
gh
¥oi·hm!oi
µ
iSjhklhj
¸
_¬[®¯±°
gk
;
jhlgk
gkh*uMsgi¹mthqig¥o~
1;
i6q7iºus6giHjogHgukjhlgk
6c
®!«G»
d
iotm
µ
o
mKm!kouo
¶
mtujghp­m!kiKjhklhj
rA¼}¡1z
gkRusgKk´m6m½ig¥o~
;;
jhlgk
gkh
6c;
jhlgk
µ
mtu¤uMsgjoggh~sjhklhjh
6c
®!«»
gk
6c;
jhlgk
e
d

sgh;¥h~hg¥ym!uºilghm!k


³
fUkKhk±mtuHjh¾¾hojhkg6p
1;³
dFc
gh~sºjslhooEphlj;klhjSp<loQo¿m´
e
hq7hFq
j
d
i¤jhq7~q7i¤jhui
®;À<¬­_¬
®;¯±°
µ
ÀU®AÁÂ¬­°Á
¦
h
¸
_¬[®;¯±°
Fgh
¥oi
1c
d

sHhkloq7uÃohki

i¤jghp?m!kº
¸
®;À<¬­_¬[®;¯±°
m!f1hp<o¤¥hmt~Äm!s½m!s66hiu
¢1!¨Å¢1
j;hs6~j;hq7hojrm!u
d
mtf1hp~Kohki
Æ
¨
Æ
s6h~gymtÇm!uº¾go~jhp<lho
d
1;
d
²
j;hm!k;¥i~hgoSp<hljo
d
È
ÁÉ
h
¸
Ê
ÁÉ
iSiog´miq7;¥iShkitm!f1p<iËFp<lo
¸
Ì
iokji~
d
¦Í
°
É
Î
h
¸
®
Í
°
É
Î
sSi¾Fj;q7hRi¾
mtq7kÃFp<lo

iËiogiÏmtuÐhm!o·
¸
Ì
iokji~
d
;Uª
dFe
m!uËjFq76hkºitmts6i
ÑÒ®¬­_¬[®;¯±°

Ó
¤§
w
d

´m!lhoiºÔhm!iº¾hkHjkp<loHÕg6p<i
iogym´6~

iogguk

g6jhF~Gi6p<iki

gh~s

j;hq7hukgijhlgki
;6e
d

mtf1hp~KFq76ijhq7hojhjojhkop<hf
ÖÆ
¨
ÖÆ
j;hm!k;¥GF~hgoKFq
hm!oGhokGgh~s
;1
d
1
shl´m!hujhm!k;¥HF~hgoºFq7hm!o

iHjghp?m!kH
¸
_¬[®;¯±°
Fq7jhq7hoj×m!snm!s1hiu
jhFjok
d
g6jhF~ipiGiog´mrkjoGg6jhF~
m!kouihm!oi
Uª
d
;
oFjGhm!o~KohkGg6o±m!uºilghm!k
i6q7hojKj
UªM
d
Uª
mtf1hp~iq7hojKjoFjGhm!o
¢1;!¨Å¢1;
jhFjokjhq7hojoKj;q mtsohku
Uª;
d
1³
jhusghoKjhq7hojg6orm!sq7ohkoiKjhq7hojRjoFjFq7hm!o
d
jhoFjioKjkrm´Kgh~s
µ
i´m!f1hp
µ
gh6oGuMsgijhohi6q7ohkGi
m!si6q7hojij;hi¾
Uª
d
U
h~s¤hq7uo6juiEi~¤jhq7hojAmØjhkop<hf
jhq7huiºjh
¥fU~i¤g
µ
o
mºm!kouo
¶
v
wyx1z|{}x1z<xv
µ
v
rA <¡v
h
¸
v
ÙÅ¡1Ú;Ú¡1z[[v
d
Uª
d
6c
ioFjGFq7hm!o±mtuºi¾¾i~jhmtjKkj;q7h6jKj;q7~
Uª²

ÛHÜ6ÝÞÜ6ßáà¤âRÜ6ãÃÝ
ä&å
æ
çKèéêëìí?î!ï
ð
ñ_ò­óõô6ö6÷ø
äù
ä&ú
û
çKèéêëìí?î!ï
ð
ñ_ò[ü;ý±þ
äÿ
ä&ú
 ê7ç¤éìïìé
èêêì
èêê7êç
	

ëºéë×îºëìì*éì7ç¤é;ìê
çï×îºëì
éìì
ù
ì7êìçêFéìïìé
éìëç
ú
ìçHëçéïºèêFí êêºèêFí ìçºèê!êç
ì.ì´î!ì
Eèêìïçyî"
#
ç%$ìé
&

ù
èêêêGèêìï
'
îëì
(1ù
éìëç
û
ä&ú
ä
î
¤çì)
ù;ù
èêì
î
èêêïëï
éì)%$ìé
ìêïç

*
Gèê7êì
&
éì7ì
çGê
ú
ä
ä&ú
ìïç+ìî!êç
ú
¤èê7êìEèêìïç
&
ç´î!ìHëºéì7ì
ç+$ëºê
*
êë
,$¤èêFí ìì
èê
ë.ëì6ïç
ê
î"ê)ç%ëtî
)
ä&ú
ÿ
ìî!ê
-rö6ö.
ëêFéêì/1êë
)
ä&ú
0
ìî!ê
þ2143
ëêFéê
ÿ
ä&ú5
éìçé)ìëçRçïêFí 
Mç6
ëì
Qéìïìé
#
7*óÒü98
'
ëì
:ä
èRèêì
î
èê
î")ê
Gèêë
&
ìïç;ìî!êçGê
*
ìî!ê
-rö6ö.
ìî!êìëêFéêç
þ2143
ëêFéêç
)5
ä&ú
:
çìí 6çHç7ìéç
#
î!ï
'
*
é;ë<ì
=çHç7ìéçìéì1ìºèê!ì¤èêëçºèêëçºç

&
ìïç6ìî!êçRê
*
ìî!ê
-rö6ö.
ìî!êìMëêFéêç
þ2143
çïéçRëêFéêç
ú
éì7ì
çKé;ìëì
ç
ëì>ç
ù?
ì
ð
0;ù?
èêëKèê
î"êÇî
:
ä&ú(1ù
çêéìïìé
ú

Mìë1çQèêêç¤ëØî"
Ãçê7ìºç7ìéç¤éìêï¤éïEëïéçA@ë6í
Uì1ê´î
ÿB(
ä&ú()(
çêUçéìïìééìïí<ì
ÿû
ú(
C
=ëç
é1ëìçD$ê
î!çé
&
çìí 6ç
ç7ìéç
î"CìïçCìî!êD
ìê
ê
ú
è;êêìê
çGèéêëìí?î!ïì!
=é
ç
ð
E
ó+F
ì!)ì
	
GIH*æJ
*
K
HBL)J"MON
ì
ð
1PH*æå
æQ
0;ÿ
ú
û
ìî!ê
þ2143
ëêFéê
0):
ú
 ê
îìîtítîBìî!ê
5ù
ú
ä
ê
îìîtíç%ìî!êç%ìêïtî_ì
=ì
=
Héì7ìéçéìïí<ì
5û
ú
éììçéìê
çé
ìî"
RêËçïìì
ç
#
þ2143
ëêFéê
*
ìî!êìRê
î!ïìîtí+ìï,ìî!ê
çìí 6çRç7ìéçnî"Sìïç
'
Rç7ìëïtî½ìí ìç
QéìïìéçSêì
ð
T
:;ù
U
ëì
 é;ì7ìé


5V(
ú
ÿ
çKèéêëìí?îtï_î
¤çêëêï_îëïtî2
6ëçGé1ëìçé;ìïìé)Këì6ê
ç
ð
ñ_ò[ü;ý±þ
*
î6ìç
é)
=ëìçGç7ìéçKèêìîyî"
W
FÅôX)Xô.[ø
W
5û
ú
0
ç7ìéçrîY
ëçKé1ëìçGéìïìéAînïí<ì
W
Z
ö.\[}ö.<ö
W
ëì
NBHV]J
^
éêëìçç7ìéç
*
é
=ëìç
*
êFé
ì
éìïë6í
ìë<ì
=
é
5 

_`a%bYc6de_+fSghji
klm
npo)qrsqoutqvwx
y
z|{"};~
o)qwwApoqn
"
l
Arov
q
o)qw

"o)qrsqo
Soqn
l
qw
"Sw
"r

o)qrsqo
z)S


l
qw
"pw !w

q¡"w
¢2£4¤
wow
l
qoo¥w
"r

w
"rq¡¦pqrq¡"w
l
o¥w !w
qo

"qrnq¡"wnuoqrsqo
nq¦ n;n !qon
l
v
www
n
o)¦ q;oqw;nq§n;nrsqon
l
oq !qrn%oq§w=n>o=q¡
§tq
"qn
¨/©Bª)«"¬O­
l
"Csq¥tnuoqrsqon®v
k
oqsn
l
oqqnwo
oq !q=n>o)rqoq !qon¯v>q%w
vn%sqn>or%oq¦ ws%oq !qrn
l

°/lm
oqrsqo
oqsq
twq§rS±q¡"w2"
wqxw Pq
w§§ !wx
"
qwr
o)q¡"r)²x6nqSq¡"w
l
q !wnYtwq§rn;x
³
´
µ)°
q
y
m)
n;q¡"wn;q
¶
·
oqw;nx¦¸n
¹
´
m
q
y
º
nron
l
sq%wqxw »v
k
tw
l
m)m

¼)½¾
¿ÀÁÂ¿
Ã)Ä
¼)Å
Æ
ÇÈÉÊË=À
ÌÍ!ÎÌÏÌÌÐ
ÌÀÈÂÀ
Ñ
Ì
ÑÑÒ
Ã)Ó
¼)½
ÆV½¾
Ñ
ÁÈÀ
Ô)È
Ñ
Ì
Ñ
Á¿
Õ
Ö ×PØ)ÙIÚÜÛÝÞØ)ß)à
á
ÔÈÍÈÀÔ6ÌÊÔ)ÍBÇÈÐâ
Ã)Ó
¼)½
ÆV½
Æ
¿ÉÌ
Ñ
Í!ÎÀ
ÔÌÀÈÂÀ
Ñ
ãÈ
Ñ
ÌÎ
Ì
ÑÑÒ
Ãä
¼)½
ÆV½
Ä
ÌÇÀÈÍBåÈÇÔÌæ
Ó)ç
¼)½
Ä
ÔÈÇÌÔÌÈÔÈ
Ñ
Ì
Ñ
Á
â
è
Ì
Ô)ÈÎÀé ÈÁ
ÓB¾
¼)½
Ä/½¾
Ô)ÈÍ!ÈÀÔ6ÌÂ
Ñ
Ê
Ñ
ËAÇé ÎÀ
Ñ
â%åÈÀÌÎ
ÓÆ
¼)½
Ä/½
Æ
¿ÀÈé æ¿;¿Í!ÈÀÔ¿
Ñ
â%åÈÀÌÎ
Ó)Ä
¼)½
ê
Ô)ÈÎëÈÔ
Ó¼
¼)½¼
ìÈ
Ò
ÌÏ
ä)¼
í
î
ïSð ñò%ó
ôõ!ö)÷Þø"õùúô"û
ü)ý
Ã½¾
ÔÌ
ÑÑÒ
¿ÌëÐÌþÈÀ
äÿ
ÃÅ
Æ
 
Ì
Ñ
Ì
Ñ
Ë+ÌÎ
Õ
	
	 
á
ÿB¾
ÃÅ
ÆVÅ
¿ÀÁÂ¿
ÿB¾
ÃÅ
ÆVÅ

ÂÈÇÌæ
Ì
Ñ
Ì
Ñ
ËAÌÎ
Ô)ÈÔÈÎ
Ñ
Ë
ÿÆ
ÃÅ
ÆVÅ

ìÌÌ
Ñ
Ì
Ñ
Ë+ÌÎYìÌÍ!È
Ñ
ÌÀ;åÈÍ
Ò
Ô

Ô)ÈÀÁÈÂ
ÔÈÁÈÐâ
ÿ)Ä
ÃÅ
ÆVÅ

 
Ì
Ñ
Ì
Ñ
Ë+ÌÎ
Õ
	
	 
á
ÿê
ÃÅ
ÆVÅ
ÌþÔ)Í!ÌÏ;ÌÈÏÌÍ
ÿ)Ã
ÃÅ
Ä
 
ìÈÂÀÐ;âÈÐ)Â
Õ
 
á
ÿ)ÿ
ÃÅ
Ä/Å
¿ÀÁÂ¿
ÿ)ÿ
ÃÅ
Ä/Å

ìÌÍ!È
Ñ
ÌÀ;åÈÀÌÎ
ìÈÂÀÐYìÌâÈÐ)Â

ÔÈÀÁÈÂ
ÔÈÁÈÐâ
ÿ)ÿ
ÃÅ
Ä/Å

 
ìÌÍ!È
Ñ
ÌÀ;ÇÈÐâ
ìÈÂÀÐYìÌâÈÐ)Â
Õ
 
á
¾ç)ç
ÃÅ
Ä/Å

ì)ÌÌÈÏÌÍ
¾çê
Ã½
Ä/½¼
ìÈ
Ò
ÌÏ
¾ç)Ã
ÃÅ
ê
È
Ñ
ÌÎ
ìÌë
ì)ÌÌÇÎÍ!Ì
Ñ
¾ç)Ó
ÃÅ
êVÅ
 
ÔÌÇÎÍ!Ì
Ñ
ã
Ñ
ÈÎÀ
¾çä
ÃÅ
êVÅ

ÎÀé ÈÁ

é ÈëÌÌ
¿ÐÈÇÀ
Ô)È
Ñ
Î)ÂÏ
¾ç)ÿ
ÃÅ
¼
åÈ
Ñ
ÌÀ
Ò
¿Í!ÈÀÔYÔÀÌÔÊ
¾)¾ÿ
ÃÅ
¼Å
¿ÀÁÂ¿
¾)¾ÿ
ÃÅ
¼Å

åÈ
Ñ
ÌÀ;åÈÀÌÎ
Ò
¿Í!ÈÀÔYÔÀÌÔÊ
¾Æ¾
Ã½¼)Å
Ä
ì)ÌÌÈÏÌÍ
¾Æ)ê

î
 ù!ø2ô)õ"û
ö
õ
ø$#õ%&'/÷Où
ø$"÷
 
÷Þø"õÞø$"(
)
*+-,/.0213,465
7
8:9';
<'=>
?@BA:C@BDE3FEG?FH:I?
>KJ'L
<'=
J
MONQP
E3A@B@R'E
M
ES@
T
U@BFE3DV
>XWY>
<'=
W
U
M$N[Z
E
M
@BF
MONQP
E3A@B@R'E
M
ES@
>XW<
<'=
WG=>
\
P
E3A@B@
]
REBS:R^_
Z
_%@BV
N
U@BFE3DV
RE3AEBCI
@
>XWL
<'=
WG=
J
\
P
E3A@B@
]
REBS:R^_
N
U@BFE3DV
RE3AEBCI
@
>K`a>
<'=
`
UEBS@Bb
>K`'c
d
efhgOi%j
8k;
l=>
@
M:M
S
>K`'L
l=
J
@S
U@B@BHX@R'\mU@_%E3E
>:no
pihq$rsfihtu$iu
8vw

x
y:zm{
|K}G|:|K}G~

O


h!!h!h$Oh$'%




BG:
 %3
¡


¢
£B¥¤$B¦'§B¨©
ª


¡
¤OX3
¦'¥¤$
¤$K
«¬®­¯°2±:²³­´µ
¦B¦'3¶m§B©·
¸

¹
ª
KB©·º¤$»[3§:¦'
¼


¸
XB©·:m: ©
½
¾

3G!h¿À

¢G

Á3ÂB

¤$
¤$


¢G¹



ÃÄÆÅÇÈÉÈÊË:Ì/ÍËÎÅÆÏXÇÐBÊÉÒÑ6ÓÄÔÆÓ!Ð
ÅÖÕ×ÃºÍØÑÚÙ

¢
¢G


¢
Û
ËÔÐÔÖÑ6ÓÄÔÆÓ!Ð
Å

¡
¢G


¡
Ü
ÃmÝ-Þàß6ß

ª
¢G
¢
B¦'B§BÁÆ¤$¶º¤$»[Bá§3¶¦â¨B¦' 
Á3ÂBB

¸
¢G¹
¢6

¤O
¤$XÁ3ÂBB
¦'3XB¨

¼
¢G¹
¢6
¢
mB¦'B§BÁÆ¤$¶¦'3¶ÂB¦¦ã§·
Á3ÂB
Xä
¢G¹
¡
¤$Bm£3B ã:¦
 %

½
¢G
¢6

m¦'B§BÁÆ¤$¶¤O»[O¤X¤Oã:
å
æç
Íéè:Ë:Ê'Ô
ê

½
¢G
¢6
¢
¦'áB»
¦'©§B¦'3§©
¢'ë
¢G
¢6
¡
¦'áB»
ÍÃºì
¢
¡
¢G
¢6
ª
¦'áB»
ÍËÎíÐ3îÒÓ!îðï
ç
ÑÖÈGÔÆÅÆèXÄ×ÐBÈÄÐ
¢
ª
¢G
¢6
¸
B
¤$§B %B¦'§3¶ %¥¤$B¤$»[X3¨B¶
¢
¸
¢G
¢6
¼
3XB©·mBãB
¦'3X:
¦
¢
ä


ñ%³$6ò!%ó
¾'ô
¡


B·:©m§3¶¦
¢
½
¡

¢
¦'3XB¨B¦'¶ÂB¦¦ÁÂ
¢
½
¡

¡
¨ãB
¡

¡

¡


¶
©O¤O»
õ
¤$ã»B¶
¦B¦'3¶
¡
¢
¡

¡

¢
©©O¤O»
õ
ö:¦3»3á¶m
¤$·X©
¦BÁ3÷GB3B÷
¡¡
¡

¡

¡
ÁY©O¤O»
õ
ö:¦3»3á¶mø¥¤$B¨
¡ª
¡

¡

ª
¨ã:3mBãB
¡'¸
¡
¹
ª
BãB
¡'¸
ù

òú/%Æ
û
üâýþ'ÿ 
	



! !"#
%$'&!
()*%$+,%	&-.(/
0'1

2



43'576
0'8
	!9	
43'576
:;
<=!
=
! !"#
0'8
	!9
2
43'576
>
$(%?!@A%(B
'2
	!9
0
@

43'576
7
(*%C

 !"#
D
	!9

43'576
>
:E%@(@A$(?
8
	!9BF
(%G(B+
'$($+,G
F(H
9
0
(@(%:E
I<.(/!.

+,%@
>
%G(G.-%()
FJ
9
09	K
(G(C(
LG'&?-ML@
F
9
09
N

G(C(I
F(1
O
P
QSR	TU,Q
V
W
üâýþ'ÿ XAYZR	[]\
^_Ra`R	Tb^VQdc_Z7^beZR
f(


gih!hjklmnlporqktsvujvwvl
x
y
x-q-zv{|zv{}Ah!w~s
mporjvwvl~h!qx-wAznlpzp(up-m
h4zpolpjkwAzgixlznhksm

x
y
x-q-zv{|zv{}Ah!w~s

w{pl|zpq
sv{p<gilnsp({|z}znh4zn<gkol}Ah!wgilp

gih!x-(h!ln<giorh
lvuhwvqwvlp

svujvwvl~{por<khSz}vukogi!mplzAgilplplmph!jvlnx--z
l
lph!jAznh!x-wvh4zg{n!}

svSzgix-jvh!w~lpx-<g

h!h!pousmnx-svSzv{

z}znhgi({~lph4zpo
lpsvujvwpm|zph!qp|giolphlpih!x-wvh4znsp({

hgiwAzAgiorlmnlmph!xSklgih(AA}lnl}Ah!wgil|zpqh<k}Az~lpx-w~hkJ{

(Amz

hkJ{
h4zpqAmznlpxw

<kj

lph!lpozpq
x-h!wg
h!x-h!-z
sv{p<gilp
lmplv{mnznorxuw~lp(l



מילונים יתירים לייצוג
 
דליל 
 
של  אותות
 
JJ(,-, ¡'¢vr,(£;			¤¥JS; J£J¤¦ 		£
¢J,§©¨%	§( ©ª«,¡v J¬	¢
­<®<¯-°A±³²d´¶µ<·
¸©¹,º»'¼¶½v¾½»(¿AÀrÁÃÂ«¹©¾
Ä
À¶Å-Æ-ÁºÀº%Â«¹%ÀS¹%»'¼¶½¸©¹%¼Ç
¹%À¶¿¼
¿ÁÈ
É
Ê
¾Ëº©Ì
ÆÍAÇpÍv¹%»
ÎÏAÏÐ


 
 
 
מ י ל ו נ י ם  י ת י ר י ם  ל י י צ ו ג
 
ד ל י ל 
 
ש ל  א ו ת ו ת
 
 
 
 
 
 
 
 
 
 
Ñ(ÒÓ;ÔiÕ×ÖÙØpÚÛ

