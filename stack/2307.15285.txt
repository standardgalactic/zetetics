OPTIMAL APPROXIMATION OF ZONOIDS AND UNIFORM
APPROXIMATION BY SHALLOW NEURAL NETWORKS
Jonathan W. Siegel
Department of Mathematics
Texas A&M University
College Station, TX 77843
jwsiegel@tamu.edu
September 28, 2023
ABSTRACT
We study the following two related problems. The first is to determine to what error an arbitrary
zonoid in Rd+1 can be approximated in the Hausdorff distance by a sum of n line segments. The
second is to determine optimal approximation rates in the uniform norm for shallow ReLUk neural
networks on their variation spaces. The first of these problems has been solved for d Ì¸= 2,3, but when
d = 2,3 a logarithmic gap between the best upper and lower bounds remains. We close this gap,
which completes the solution in all dimensions. For the second problem, our techniques significantly
improve upon existing approximation rates when k â‰¥1, and enable uniform approximation of both
the target function and its derivatives.
1
Introduction
A (centered) zonotope in Rd+1 (so that the sphere Sd âŠ‚Rd+1 is of dimension d) is a convex polytope P which is the
Minkowski sum of finitely many centered line segments, i.e. a body of the form
P = {x1v1 +Â·Â·Â·+xnvn, xi âˆˆ[âˆ’1,1]}
(1.1)
for some collection of vectors vi âˆˆRd+1. The number n is the number of summands of the zonotope. A zonoid is a
convex body which is a limit of zonotopes in the Hausdorff metric.
We consider the following problem: Given an arbitrary zonoid Z, how accurately can Z be approximated by a polytope
P with n-summands? Here accuracy Îµ is taken to mean that Z âŠ‚P âŠ‚(1+Îµ)Z.
This problem has been studied by a variety of authors (see for instance [7â€“10,20,22]). Of particular interest is the case
when Z = Bd+1 is the Euclidean unit ball. In this case the problem has an equivalent formulation as (see [7]): how many
directions v1,...,vn âˆˆSd are required to estimate the surface area of a convex body in Rd+1 from the volumes of its
d-dimensional projections orthogonal to each vi?
In [10], it was shown using spherical harmonics that with n summands and Z = Bd+1 the best error one can achieve is
lower bounded by
Îµ(n) â‰¥c(d)nâˆ’1
2 âˆ’3
2d .
(1.2)
When d = 2,3, this bound was matched up to logarithmic factors in [8], specifically it was shown that for general
zonoids Z, we have
Îµ(n) â‰¤C(d)
(
nâˆ’1
2 âˆ’3
2d p
log(n)
d = 2
nâˆ’1
2 âˆ’3
2d log(n)3/2
d = 3.
(1.3)
For larger values of d the result in [8] gives the worse upper bound of
Îµ(n) â‰¤C(d)nâˆ’1
2 âˆ’
1
dâˆ’1 p
log(n).
(1.4)
arXiv:2307.15285v2  [stat.ML]  26 Sep 2023

A PREPRINT - SEPTEMBER 28, 2023
In [9] (see also [22]) it was shown that these bounds can be attained using summands of equal length if Z = Bd+1.
The picture was nearly completed in [28] where it was shown that we have
Îµ(n) â‰¤C(d)
(
nâˆ’1
2 âˆ’3
2d p
log(n)
d = 2,3
nâˆ’1
2 âˆ’3
2d
d â‰¥4.
(1.5)
Moreover, it was shown that the upper bound when d â‰¥4 can be achieved using summands of equal length for all
zonoids Z.
In this work, we remove the logarithmic factors in (1.5) when d = 2,3, i.e. we prove that
Îµ(n) â‰¤C(d)nâˆ’1
2 âˆ’3
2d
(1.6)
for all d, and thus provide upper bounds exactly matching (up to a constant factor) the lower bound (1.2). To formulate
these results, we pass to the dual setting (see [10,28]). A symmetric convex body Z is a zonoid iff
âˆ¥xâˆ¥Zâˆ—:= sup
zâˆˆZ
xÂ·z =
Z
Sd |xÂ·y|dÏ„(y)
(1.7)
for a positive measure Ï„ on Sd. The body Z is a zonotope with n-summands iff Ï„ is supported on n points. Since our
error measure is scale invariant, we may assume that Ï„ is a probability distribution. Given these considerations, the
bound (1.6) follows from the following result.
Theorem 1. There exists a constant C = C(d) such that for any probability measure Ï„ on the sphere Sd, there exists a
probability measure Ï„â€² on Sd which is supported on n points, such that
sup
xâˆˆSd

Z
Sd |xÂ·y|dÏ„(y)âˆ’
Z
Sd |xÂ·y|dÏ„â€²(y)
 â‰¤Cnâˆ’1
2 âˆ’3
2d .
(1.8)
We remark that our method produces summands of unequal length (i.e. a non-uniform distribution Ï„â€²) and we do not
know whether this approximation can be achieved using summands of equal length (even for the ball Bd+1) when d < 4.
Recently, there has been renewed interest in the zonoid approximation problem due to its connection with approximation
by shallow ReLUk neural networks [2]. The ReLUk activation function (simply called ReLU when k = 1) is defined by
Ïƒk(x) = xk
+ :=
xk
x â‰¥0
0
x < 0,
(1.9)
where in the case k = 0 we interpret 00 = 1 (so that Ïƒ0 is the Heaviside function). A shallow ReLUk neural network is a
function on Rd of the form
fn(x) =
n
âˆ‘
i=1
aiÏƒk(Ï‰i Â·x+bi) =
n
âˆ‘
i=1
ai(Ï‰i Â·x+bi)k
+,
(1.10)
where the ai âˆˆR are coefficients, the Ï‰i âˆˆRd are directions, the bi âˆˆR are the offsets (or biases) and n (the number of
terms) is called the width of the network.
Shallow neural networks can be viewed as a special case of non-linear dictionary approximation. Given a Banach space
X, let D âŠ‚X be a bounded subset, i.e. âˆ¥Dâˆ¥:= supdâˆˆD âˆ¥dâˆ¥X < âˆž, which we call a dictionary. Non-linear dictionary
approximation methods seek to approximate a target function f by elements of the set
Î£n(D) =
(
n
âˆ‘
i=1
aidi, ai âˆˆR, di âˆˆD
)
(1.11)
of n-term linear combinations of dictionary elements. Note that because the elements di are not fixed, this a non-linear
set of functions. It is often important to obtain some control on the coefficients ai in a non-linear dictionary expansion.
For this reason, we introduce the set
Î£M
n (D) =
(
n
âˆ‘
i=1
aidi, ai âˆˆR, di âˆˆD,
n
âˆ‘
i=1
|ai| â‰¤M
)
(1.12)
of non-linear dictionary expansions with â„“1-bounded coefficients.
2

A PREPRINT - SEPTEMBER 28, 2023
We consider using shallow ReLUk neural networks to approximate functions and derivatives of order up to k uniformly
on a bounded domain â„¦âŠ‚Rd, so we take our Banach space X to be the Sobolev spaceW k(Lâˆž(â„¦)) (see for instance [17]),
with norm given by
âˆ¥fâˆ¥W k(Lâˆž(â„¦)) := sup
|Î±|â‰¤k
âˆ¥DÎ± fâˆ¥Lâˆž(â„¦).
(1.13)
In our analysis, without loss of generality we will often take â„¦= Bd := {x : |x| â‰¤1} to be the unit ball in Rd to simplify
the presentation.
Shallow neural networks correspond to non-linear approximation with the dictionary
D = Pd
k := {Ïƒk(Ï‰i Â·x+bi), Ï‰i âˆˆSdâˆ’1, bi âˆˆ[a,b]} âŠ‚W k(Lâˆž(â„¦)),
(1.14)
where by positive homogeneity we can take Ï‰i on the unit sphere, and the biases bi are restricted to an interval depending
upon â„¦to ensure both the boundedness and expressiveness of Pd
k (see for instance [39]). When â„¦= Bd is the unit ball
in Rd, we can take [a,b] = [âˆ’1,1] for example, since
Ïƒk(Ï‰ Â·x+b)+(âˆ’1)kÏƒk(âˆ’Ï‰ Â·xâˆ’b)
for Ï‰ âˆˆSdâˆ’1 and b âˆˆ[âˆ’1,1] spans the space of polynomials of degree at most k on Bd.
A typical class of functions considered in the context of non-linear dictionary approximation is the variation space of
the dictionary D, defined as follows. Let
B1(D) :=
âˆž
[
n=1
Î£1n(D),
(1.15)
denote the closed symmetric convex hull of the dictionary D and define the variation norm of a function f âˆˆX by
âˆ¥fâˆ¥K1(D) := inf{s > 0 : f âˆˆsB1(D)}.
(1.16)
This construction is also called the gauge of the set B1(D) (see for instance [34]), and has the property that the unit ball
of the K1(D)-norm is exactly the closed convex hull B1(D). We also write
K1(D) := { f âˆˆX; âˆ¥fâˆ¥K1(D) < âˆž}
(1.17)
for the space of functions with finite K1(D)-norm. The variation norm has been introduced in different forms in the
literature and plays an important role in statistics, signal processing, non-linear approximation, and the theory of shallow
neural networks (see for instance [4,5,12,13,19,31,32,36,37,41]).
In the case corresponding to shallow ReLUk neural networks, D = Pd
k, the variation space can equivalently be defined
via integral representations, which were studied for example in [2,16]. Specifically, we have f âˆˆK1(Pd
k) iff there exists
a (signed measure) dÂµ of bounded variation on Sdâˆ’1 Ã—[a,b] such that
f(x) =
Z
Sdâˆ’1Ã—[a,b] Ïƒk(Ï‰ Â·x+b)dÂµ(Ï‰,b)
(1.18)
pointwise almost everywhere. Moreover, the variation norm is given by
âˆ¥fâˆ¥K1(Pd
k ) = inf
Z
Sdâˆ’1Ã—[a,b] d|Âµ|(Ï‰,b), f(x) =
Z
Sdâˆ’1Ã—[a,b] Ïƒk(Ï‰ Â·x+b)dÂµ(Ï‰,b)

,
(1.19)
where the infimum above is taken over all measures with finite total variation giving such a representation of f. This is
due to the fact that
B1(Pd
k) =
Z
Sdâˆ’1Ã—[a,b] Ïƒk(Ï‰ Â·x+b)dÂµ(Ï‰,b),
Z
Sdâˆ’1Ã—[a,b] d|Âµ|(Ï‰,b) â‰¤1

,
(1.20)
which follows from Lemma 3 in [39], and an â€˜empiricalâ€™ discretization of the integral in (1.20) using the fact that
half-spaces have bounded VC-dimension [42,43] (note that the closure in (1.15) is taken in the X = W k(Lâˆž)-norm). In
fact, it follows easily from this that we get the same set B1(Pd
k), and thus the same variation space K1(Pd
k), even if we
take the closure in (1.15) with respect to a weaker norm such as L2.
One important question is how efficiently functions in the variation space K1(D) can be approximated by non-linear
dictionary expansions Î£n(D) with n terms. When the space X is a Hilbert space (or more generally a type-2 Banach
space), we have the bound [4,19,33]
inf
fnâˆˆÎ£n(D)âˆ¥f âˆ’fnâˆ¥X â‰¤Câˆ¥fâˆ¥K1(D)nâˆ’1
2 .
(1.21)
3

A PREPRINT - SEPTEMBER 28, 2023
The constant here depends only upon the norm of the dictionary âˆ¥Dâˆ¥and the type-2 constant of the space X. Moreover,
the norm of the coefficients ai can be controlled, so that if f is in B1(D) (the unit ball of K1(D)), then fn can be taken
in Î£1
n(D). This fact was first applied to neural network approximation by Jones and Barron [4,19], and forms the basis
of the dimension independent approximation rates obtained for shallow neural networks.
For many dictionaries, for example the dictionaries Pd
k corresponding to shallow neural networks, the rate (1.21) can be
significantly improved (see for instance [2,21,25,38]). For instance, in the L2-norm we get the rate
inf
fnâˆˆÎ£n(Pd
k )
âˆ¥f âˆ’fnâˆ¥L2(â„¦) â‰¤Câˆ¥fâˆ¥K1(Pd
k )nâˆ’1
2 âˆ’2k+1
2d ,
(1.22)
and this rate is optimal up to logarithmic factors if we require even mild control on the coefficients ai (for instance
|ai| â‰¤C for a constant C) [38].
In this work, we consider approximation rates for the dictionary Pd
k on the variation space K1(Pd
k) in the W m(Lâˆž)-norm
for m = 0,...,k, i.e. we consider uniform approximation of both f and its derivatives up to order m. This is a much
stronger error norm than the L2-norm, and approximating derivatives is important for applications of shallow neural
networks to scientific computing (see for instance [23,35,44]). For an arbitrary (bounded) dictionary D âŠ‚W m(Lâˆž), no
rate of approximation for K1(D) by Î£n(D) can be obtained in general, i.e. there exist dictionaries for which the rate
can be arbitrarily slow [14]. Thus, our results rely upon the special structure of the Pd
k dictionary of ReLUk atoms.
This problem has previously been considered in the case m = 0, i.e. in the Lâˆž-norm (see for instance [2,3,11,21,24,46]).
In this case, when k = 0 an approximation rate of
inf
fnâˆˆÎ£n(Pd
0)
âˆ¥f âˆ’fnâˆ¥Lâˆž(â„¦) â‰¤Câˆ¥fâˆ¥K1(Pd
0)nâˆ’1
2 âˆ’1
2d ,
(1.23)
was proved in [24] using results from geometric discrepancy theory [27]. For k = 1, the aforementioned results on
approximating zonoids by zonotopes [28] were used in [2] to get a rate of
inf
fnâˆˆÎ£n(Pd
1)
âˆ¥f âˆ’fnâˆ¥Lâˆž(Sd) â‰¤Câˆ¥fâˆ¥K1(Pd
1)
(
nâˆ’1
2 âˆ’3
2d âˆšlogn
d = 2,3
nâˆ’1
2 âˆ’3
2d
d â‰¥4
(1.24)
on the sphere Sd. Finally, when k â‰¥2, the best known result is [21]
inf
fnâˆˆÎ£n(Pd
k )
âˆ¥f âˆ’fnâˆ¥Lâˆž(â„¦) â‰¤Câˆ¥fâˆ¥K1(Pd
k )nâˆ’1
2 âˆ’1
d p
logn.
(1.25)
We remark that for all of these results, the coefficients of fn can be controlled. Specifically, if f âˆˆB1(Pd
k), then fn can
be taken in Î£1
n(Pd
k).
By refining the techniques of discrepancy theory used to obtain these Lâˆžbounds, we are able to prove the following
result, which is essentially a generalization of Theorem 1.
Theorem 2. Let k â‰¥0. For any probability distribution Ï„ on Sdâˆ’1 Ã—[âˆ’1,1] there exists a probability distribution Ï„â€²
supported on at most n points such that for any multi-index Î± with |Î±| â‰¤k we have
sup
xâˆˆBd
DÎ±
x
Z
Sd Ïƒk(Ï‰ Â·x+b)dÏ„(Ï‰,b)âˆ’
Z
Sd Ïƒk(Ï‰ Â·x+b)dÏ„â€²(Ï‰,b)
 â‰¤Cnâˆ’1
2 âˆ’2(kâˆ’|Î±|)+1
2d
,
(1.26)
where DÎ±
x denotes the Î±-th order derivative with respect to x. Here the constant C = C(d,k) depends only upon d and k.
As a Corollary, we obtain the following approximation rate.
Theorem 3. Let 0 â‰¤m â‰¤k and â„¦= Bd. Then we have the bound
inf
fnâˆˆÎ£n(Pd
k )
âˆ¥f âˆ’fnâˆ¥W m(Lâˆž(â„¦)) â‰¤Câˆ¥fâˆ¥K1(Pd
k )nâˆ’1
2 âˆ’2(kâˆ’m)+1
2d
,
(1.27)
where C = C(d,k) is a constant. Moreover, the coefficients of fn can be controlled, so if f âˆˆB1(Pd
k), then fn can be
taken in Î£1
n(Pd
k).
We remark that by scaling, Theorems 2 and 3 can easily be extended to any bounded domain â„¦âŠ‚Rd. Theorem
3 extends the approximation rates derived in [38] from the L2-norm to the Lâˆž-norm, which significantly improves
upon existing results [2,21,24] when k â‰¥1. In addition, we obtain approximation rates in the W m(Lâˆž)-norm, which
4

A PREPRINT - SEPTEMBER 28, 2023
enables derivatives and function values to be uniformly approximated simultaneously. The approximation rates given in
Theorem 3 are an important building block in obtaining approximation rates for shallow ReLUk networks on Sobolev
and Besov spaces [45].
Theorems 1 and 2 are proved using a modification of the geometric discrepancy argument used in [28], while Theorem
3 is an easy Corollary of Theorem 2. Theorem 1 follows essentially as a special case of Theorem 2 when k = 1, except
that the ReLU activation function is replaced by the absolute value function and the setting is changed to the sphere. For
this reason, we only give the complete proof of Theorem 2. The changes necessary to obtain Theorem 1 are relatively
straightforward and left to the reader. We begin by collecting the necessary geometric and combinatorial facts in Section
2. The proofs of Theorems 2 and 3 are given in Section 3.
2
Geometric Lemmas
In this section, we collect and prove the geometric Lemmas which are crucial to the proofs of Theorems 1 and 2. In
particular, in the proof of Theorem 2 we will need the following covering result.
Lemma 1. Let P be an N point subset of Sdâˆ’1 Ã—[âˆ’1,1] (i.e. a set of n halfspaces) and 0 < Î´ < 1 be given. Then there
exists a subset N âŠ‚Bd of the unit ball (depending upon both P and Î´) with |N | â‰¤(C/Î´)d such that for any x âˆˆBd
there exists a z âˆˆN with
â€¢ |xâˆ’z| â‰¤CÎ´
âˆš
d.
â€¢ |Pâˆ©{(Ï‰,b) âˆˆSdâˆ’1 Ã—[âˆ’1,1], sgn(Ï‰ Â·x+b) Ì¸= sgn(Ï‰ Â·z+b)}| â‰¤Î´N.
Here C is an absolute constant.
A version of this Lemma on the sphere, which is required for the proof of Theorem 1 was proved by Matousek [28].
Lemma 2 (Lemma 6 in [28]). Let P be an N-point subset of Sd and 0 < Î´ < 1 be given. There exists a subset N âŠ‚Sd
(depending upon both P and Î´) with |N | â‰¤CÎ´ âˆ’d such that for any x âˆˆSd, there exists a z âˆˆN with
â€¢ |xâˆ’z| â‰¤Î´.
â€¢ |Pâˆ©{y âˆˆSd, sgn(yÂ·x) Ì¸= sgn(yÂ·z)}| â‰¤Î´N.
Here C = C(d) is a constant independent of N,P and Î´.
The proof of Lemma 1 follows essentially the same ideas as the proof of Lemma 2 in [28]. However, for completeness
we give the proof here as well. The version given in Lemma 1 explicitly tracks the dimension dependence of the
constants and can be used to track the dimension dependence of the constants in Theorems 1 and 2.
We begin by recalling the relevant combinatorial background (see for instance [29], Chapter 5).
Definition 1. A set system (X,S ) consists of a set X and a collection of subsets S âŠ‚2X of X.
The particular set system which we will consider in the proof of Lemma 1 is given by
X = Sdâˆ’1 Ã—[âˆ’1,1], S =
n
{(Ï‰,b) : Ï‰ Â·x+b â‰¥0}, x âˆˆBdo
.
(2.1)
In other words, the elements are halfspaces and the sets consists of all halfspaces containing a given point x in the unit
ball.
Given a subset Y âŠ‚X, we write S |Y = {Y âˆ©S, S âˆˆS } for the system S restricted to the set Y.
Definition 2 (VC-dimension [43]). A subset Y âŠ‚X is shattered by S if S |Y = 2Y. The VC-dimension of the set system
(X,S ) is the cardinality of the largest finite subset of X which is shattered by S .
An important ingredient in the proof is the following bound on the VC-dimension of the set system given in (2.1).
Lemma 3 (Lemma 3 in [24]). The set system in (2.1) has VC-dimension bounded by d.
Finally, we will need the following packing Lemma for set systems with bounded VC-dimension.
Lemma 4. Let (X,S ) be a set system and Âµ a probability measure on the set X. Define a distance dÂµ on the collection
of subsets S by
dÂµ(S1,S2) = Âµ(S1âˆ†S2).
(2.2)
5

A PREPRINT - SEPTEMBER 28, 2023
In other words, dÂµ is the probability that a randomly chosen element from the measure Âµ will be in one set but not the
other.
Let Îµ > 0 and suppose that S1,...,SN âˆˆS are such that dÂµ(Si,Sj) â‰¥Îµ for all i Ì¸= j. Then, if (X,S ) has VC-dimension
at most d, we have
N â‰¤
C
Îµ
d
(2.3)
for an absolute constant C (we can take for instance C = 50).
This was first proved by Haussler in the case where X is a finite set and Âµ is the counting measure [18], with a weaker
result (losing a logarithmic factor) being obtained earlier by Dudley [15]. The generalization to arbitrary probability
measures Âµ follows from this result in a relatively simple manner, and has been noted in the case of certain geometric
set systems in [26].
Proof of Lemma 4. Consider drawing k independent random samples x1,...,xk from the probability distribution Âµ with
replacement. We consider the sets
Â¯Sj = {i : xi âˆˆS j} âŠ‚{1,...,k}.
(2.4)
It is clear that the VC-dimension of the set system Â¯S1,..., Â¯SN viewed as a subsets of {1,...,k} is also at most d. Applying
Theorem 1 in [18], we see that if each pair Â¯Si and Â¯S j differ in at least Î´k elements for some Î´ > 0, then
N â‰¤e(d +1)
2e
Î´
d
â‰¤
C
Î´
d
(2.5)
for C = 25 (for example).
Finally, we observe that by choosing k large enough we can guarantee that any two pairs Â¯Si and Â¯Sj differ in at least Î´k
elements for Î´ = Îµ/2 with positive probability. Indeed, for each pair of sets the fraction of elements that they differ in is
an average of Bernoulli random variables with expectation at least Îµ, since dÂµ(Si,Sj) â‰¥Îµ. Using standard concentration
inequalities combined with a union bound (for k large enough, note that N is fixed) completes the proof.
Proof of Lemma 1. Consider the set system given in (2.1) and the probability measure Âµ defined by
Âµ = 1
2Ï€ + 1
2Ï€P,
(2.6)
where Ï€ is the uniform probability measure on Sdâˆ’1 Ã—[âˆ’1,1], and Ï€P is the empirical measure associated to the set of
halfspaces P, i.e.
Ï€P = 1
|P| âˆ‘
(Ï‰,b)âˆˆP
Î´(Ï‰,b)
(2.7)
where Î´(Ï‰,b) denotes the Dirac measure at the point (Ï‰,b).
Let x1,...,xN âˆˆBd (viewed as elements of the set system S ) be a maximal set of points such that dÂµ(xi,xj) â‰¥Î´/2. By
Lemma 4 and the VC-dimension bound in Lemma 3, we have that
N â‰¤
2C
Î´
d
.
(2.8)
Moreover, given an z âˆˆBd there is an xi such that dÂµ(xi,z) < Î´/2 by the maximality of the set x1,...,xN. From the
definition of Âµ, this means that dÏ€(xi,z) < Î´ and dÏ€P(xi,z) < Î´. Given the form (2.7) of Ï€P, dÏ€P(xi,z) < Î´ is equivalent
to
|Pâˆ©{(Ï‰,b) âˆˆSdâˆ’1 Ã—[âˆ’1,1], sgn(Ï‰ Â·xi +b) Ì¸= sgn(Ï‰ Â·z+b)}| < Î´|P| = Î´N.
On the other hand, dÂµ(xi,z) < Î´ implies that
EÏ‰âˆˆSdâˆ’1 [P(xi Â·Ï‰ < b < zÂ·Ï‰ or zÂ·Ï‰ < b < xi Â·Ï‰)] = 1
2EÏ‰âˆˆSdâˆ’1|(xi âˆ’z)Â·Ï‰| < Î´,
(2.9)
where the expectation denotes an average over the sphere Sdâˆ’1, and the probability is over a uniformly random
b âˆˆ[âˆ’1,1]. It is well-known that for any fixed unit vector w âˆˆSdâˆ’1 we have
EÏ‰âˆˆSdâˆ’1|wÂ·Ï‰| â‰¥cdâˆ’1/2
(2.10)
for an absolute constant c. Together with (2.9), this implies that |xi âˆ’z| < CÎ´
âˆš
d as desired.
6

A PREPRINT - SEPTEMBER 28, 2023
3
Approximation by Shallow ReLUk Neural Networks
In this section, we give the proof of Theorem 2, which follows easily from the following Proposition.
Proposition 1. Fix an integer k â‰¥0. Let Ï„ be a probability distribution on the sphere Sdâˆ’1 Ã—[âˆ’1,1] which is supported
on N points for N sufficiently large (N â‰¥6 is sufficient). Then there exists a probability distribution Ï„â€² supported on at
most (1âˆ’c)N points such that for all multi-indices Î± with |Î±| â‰¤k we have
sup
xâˆˆBd
DÎ±
x
Z
Sdâˆ’1Ã—[âˆ’1,1] Ïƒk(Ï‰ Â·x+b)dÏ„(Ï‰,b)âˆ’
Z
Sdâˆ’1Ã—[âˆ’1,1] Ïƒk(Ï‰ Â·x+b)dÏ„â€²(Ï‰,b)
 â‰¤CNâˆ’1
2 âˆ’2(kâˆ’|Î±|)+1
2d
,
(3.1)
where DÎ±
x denotes the Î±-th order derivative with respect to x. Here C = C(d,k) and c is an absolute constant.
Proof of Theorem 2. We repeatedly apply Proposition 1 to the distribution Ï„ until the size of the support set is at most
n/2. By Proposition 1, for any multi-index Î± the error incurred in the Î±-th derivative is bounded by
Cnâˆ’1
2 âˆ’2(kâˆ’|Î±|)+1
2d
 
âˆž
âˆ‘
j=0
(1âˆ’c)j

1
2 + 2(kâˆ’|Î±|)+1
2d
!
â‰²nâˆ’1
2 âˆ’2(kâˆ’|Î±|)+1
2d
,
(3.2)
since the errors in each step form a geometric series whose sum is bounded by a multiple of its largest term (which is
the error made in the last step).
Proof of Theorem 3. Suppose without loss of generality that âˆ¥fâˆ¥K1(Pd
k ) â‰¤1, i.e. that f âˆˆB1(Pd
k).
By definition, this means that for any Îµ > 0 there exist parameters (Ï‰1,b1),...,(Ï‰N,bN) âˆˆSd Ã— [âˆ’1,1] and weights
a1,...,aN âˆˆR (for a sufficiently large N) such that
f âˆ’
N
âˆ‘
i=1
aiÏƒk(Ï‰i Â·x+bi)

W k(Lâˆž(Sd))
< Îµ,
(3.3)
and âˆ‘N
i=1 |ai| â‰¤1. The next step is to approximate the sum in (3.3) by an element in Î£1
n(Pd
k). To do this, we split the
sum into its positive and negative parts, i.e. we write
N
âˆ‘
i=1
aiÏƒk(Ï‰i Â·x+bi) = âˆ‘
ai>0
aiÏƒk(Ï‰i Â·x+bi)âˆ’âˆ‘
ai<0
|ai|Ïƒk(Ï‰i Â·x+bi).
(3.4)
By considering the positive and negative pieces separately, we essentially reduce to the case where all ai are positive. In
this case, the sum can be written
N
âˆ‘
i=1
aiÏƒk(Ï‰i Â·x+bi) =
Z
Sdâˆ’1Ã—[âˆ’1,1] Ïƒk(Ï‰ Â·x+b)dÏ„(Ï‰,b)
(3.5)
for a probablity measure Ï„ supported on at most N points.
Applying Theorem 2 gives an fn âˆˆÎ£1
n/2(Pd
k) such that
fn âˆ’
N
âˆ‘
i=1
aiÏƒk(Ï‰i Â·x+bi)

W m(Lâˆž(Sd))
â‰¤Cnâˆ’1
2 âˆ’2(kâˆ’m)+1
2d
,
(3.6)
whenever ai â‰¥0 and âˆ‘N
i=1 ai = 1. Applying this to the positive and negative parts in (3.4) and summing them gives an
fn âˆˆÎ£1
n(Pd
k) such that
âˆ¥f âˆ’fnâˆ¥W m(Lâˆž(Sd)) â‰¤Cnâˆ’1
2 âˆ’2(kâˆ’m)+1
2d
+Îµ.
(3.7)
Since Îµ > 0 was arbitrary, this completes the proof.
It remains to prove Proposition 1. The proof utilizes the ideas of geometric discrepancy theory and borrows many ideas
from the proof of Proposition 9 in [28]. However, Proposition 9 in [28] only deals with uniform distributions, and a few
key modifications are required to deal with the case of â€˜unbalancedâ€™ distributions Ï„, which enables us to remove the
logarithmic factors in all dimensions in Theorems 1, 2, and 3. In addition, dealing with the higher order smoothness of
the ReLUk activation function introduces significant technical difficulties.
7

A PREPRINT - SEPTEMBER 28, 2023
We first introduce some notation. We will need to work with symmetric tensors in order to handle higher order
derivatives of multivariate functions. Our tensors will be defined on the ambient space Rd+1 containing the sphere Sd,
so let I = {1,...,d +1} denote the relevant indexing set. A (symmetric) tensor X of order m is an array of numbers
indexed by a tuple i âˆˆIm, which satisfies
Xi = XÏ€(i)
(3.8)
for any permutation Ï€ of {1,...,m}. Here Ï€(i)j = iÏ€(j) for j = 1,...,m. Note that vectors in Rd+1 are symmetric tensors
of order one. We adopt the â„“âˆžnorm on the space of symmetric tensors, i.e.
âˆ¥Xâˆ¥:= max
iâˆˆIm |Xi|.
(3.9)
Given tensors X and Y of orders m1 and m2, their tensor product, which is a tensor of order m1 +m2, is defined in the
standard way by
(X âŠ—Y)ij = XiYj,
(3.10)
where i âˆˆIm1, j âˆˆIm2 and ij denotes concatenation. We will also write XâŠ—r for the r-fold tensor product of X with itself.
Supposing that m1 â‰¥m2, we define the contraction, which is a tensor of order m1 âˆ’m2 by
âŸ¨X,YâŸ©i = âˆ‘
jâˆˆIm2
XijYj.
(3.11)
Note that since we will be exclusively working with Rd+1 with the standard inner product, to simplify the presentation
we will not make the distinction between covariance and contravariance in the following.
We remark that repeated contraction can be written in terms of the tensor product in the following way
âŸ¨âŸ¨X,YâŸ©,ZâŸ©= âŸ¨X,Y âŠ—ZâŸ©,
(3.12)
and also note the inequality
âˆ¥âŸ¨X,YâŸ©âˆ¥â‰¤Câˆ¥Xâˆ¥âˆ¥Yâˆ¥,
(3.13)
where C = C(d,k) = (d +1)k.
Given an order 0 â‰¤m â‰¤k, we denote the m-th derivative (tensor) of the ReLUk function (with Ï‰ and b fixed) by
Ïƒ(m)
k
(x;Ï‰,b) = Dm
x [Ïƒk(Ï‰ Â·x+b)] =
(
k!
(kâˆ’m)!(Ï‰ Â·x+b)kâˆ’mÏ‰âŠ—m
Ï‰ Â·x+b â‰¥0
0
Ï‰ Â·x+b < 0.
(3.14)
In order to deal with the additional smoothness of the activation function we will need to utilize higher-order Taylor
polynomials. Given points x1,x2 âˆˆSd, parameters (Ï‰,b) âˆˆSdâˆ’1 Ã—[âˆ’1,1], an order 0 â‰¤m â‰¤k, and a number of terms
0 â‰¤r â‰¤k âˆ’m, we denote by
T m,r
x1 (x2;Ï‰,b) :=
r
âˆ‘
q=0
1
q!
D
Ïƒ(m+q)
k
(x1;Ï‰,b),(x2 âˆ’x1)âŠ—qE
(3.15)
the r-th order Taylor polynomials of Ïƒ(m)
k
(x;Ï‰,b) around x1 evaluated at x2.
Proof of Proposition 1. Note that since Ï„ is supported on N points the integral which we are trying to approximate in
Proposition 1 is given by
Z
Sdâˆ’1Ã—[âˆ’1,1] Ïƒk(Ï‰ Â·x+b)dÏ„(Ï‰,b) = âˆ‘
(Ï‰,b)âˆˆS
aÏ‰,bÏƒk(Ï‰ Â·x+b),
(3.16)
where |S| = N and the coefficients aÏ‰,b satisfy aÏ‰,b â‰¥0 and âˆ‘(Ï‰,b)âˆˆS aÏ‰,b = 1.
Let M denote the median of the coefficients aÏ‰,b and set
Sâˆ’= {(Ï‰,b) âˆˆS : aÏ‰,b â‰¤M}, S+ = {(Ï‰,b) âˆˆS : aÏ‰,b > M}.
This gives a decomposition of the sum in (3.16) in terms of its large and small coefficients
âˆ‘
(Ï‰,b)âˆˆS
aÏ‰,bÏƒk(Ï‰ Â·x+b) =
âˆ‘
(Ï‰,b)âˆˆSâˆ’
aÏ‰,bÏƒk(Ï‰ Â·x+b)+
âˆ‘
(Ï‰,b)âˆˆS+
aÏ‰,bÏƒk(Ï‰ Â·x+b).
(3.17)
We will leave the second, i.e. the large, sum untouched and approximate the small sum by
âˆ‘
(Ï‰,b)âˆˆSâˆ’
aÏ‰,bÏƒk(Ï‰ Â·x+b) â‰ˆâˆ‘
(Ï‰,b)âˆˆT
bÏ‰,bÏƒk(Ï‰ Â·x+b),
(3.18)
8

A PREPRINT - SEPTEMBER 28, 2023
where T âŠ‚Sâˆ’and |T| â‰¤(1âˆ’c)|Sâˆ’|, the new coefficients bÏ‰,b â‰¥0 and satisfy
âˆ‘
(Ï‰,b)âˆˆT
bÏ‰,b =
âˆ‘
(Ï‰,b)âˆˆSâˆ’
aÏ‰,b,
and the error of approximation satisfies (using the tensor norm we have introduced)
sup
xâˆˆSd
 âˆ‘
(Ï‰,b)âˆˆSâˆ’
aÏ‰,bÏƒ(m)
k
(x;Ï‰,b)âˆ’âˆ‘
(Ï‰,b)âˆˆT
bÏ‰,bÏƒ(m)
k
(x;Ï‰,b)
 â‰¤CNâˆ’1
2 âˆ’2(kâˆ’m)+1
2d
(3.19)
for m = 0,...,k. Setting
Ï„â€² = âˆ‘
(Ï‰,b)âˆˆT
bÏ‰,bÎ´Ï‰,b +
âˆ‘
(Ï‰,b)âˆˆS+
aÏ‰,bÎ´Ï‰,b
(3.20)
now completes the proof since |Sâˆ’| â‰¥N/2 (here Î´Ï‰,b denotes the Dirac delta distribution at (Ï‰,b)).
We now turn to the heart of the proof, which is constructing an approximation (3.18) which satisfies (3.19). Note first
that by construction, we have
max
(Ï‰,b)âˆˆSâˆ’
aÏ‰,b â‰¤M â‰¤2
N ,
(3.21)
i.e. all of the coefficients in the small half are at most 2/N. This holds since at least half (i.e. at least N/2) of the aÏ‰,b
are at least as large as the median M and âˆ‘(Ï‰,b)âˆˆS aÏ‰,b = 1.
Next we construct a multi-scale covering of the ball using Lemma 1. For l = 1,...,L with 2L > N we apply Lemma 1
with P = Sâˆ’and Î´ = 2âˆ’l to obtain a sequence of sets Nl âŠ‚Bd with |Nl| â‰¤(C2l)d such that for any x âˆˆBd there exists a
z âˆˆNl with |xâˆ’z| â‰¤C2âˆ’lâˆš
d and
|{y âˆˆSâˆ’: sgn(yÂ·x) Ì¸= sgn(yÂ·z)}| â‰¤2âˆ’l|Sâˆ’| â‰¤2âˆ’lN.
Given a point x âˆˆSd, we denote by Ï€l(x) the point z âˆˆNl satisfying these properties (if this point is not unique we
choose one arbitrarily for each x).
For each level l = 1,...,L, each point x âˆˆNl, and each index m = 0,..,k we consider the function
Ï† m
x,l(Ï‰,b) =
(
Ïƒ(m)
k
(x;Ï‰,b)âˆ’T m,kâˆ’m
Ï€lâˆ’1(x) (x;Ï‰,b)
l â‰¥2
Ïƒ(m)
k
(x;Ï‰,b)
l = 1,
(3.22)
where T m,kâˆ’m
Ï€lâˆ’1(x) (x;Ï‰,b) is the (k âˆ’m)-th order Taylor polynomial of Ïƒ(m)
k
(x;Ï‰,b) defined in (3.15).
We note the following bounds on Ï† m
x,l(Ï‰,b). First, if sgn(Ï‰ Â·x+b) = sgn(Ï‰ Â·Ï€lâˆ’1(x)+b) (for l â‰¥2), then
Ï† m
x,l(Ï‰,b) = 0.
(3.23)
This holds since on the half space {x : Ï‰ Â·x+b â‰¥0} the function Ïƒ(m)
k
(x;Ï‰,b) is a polynomial of degree k âˆ’m in x.
Thus on this half-space it is equal to its (kâˆ’m)-th order Taylor polynomial about any point. So if x and Ï€lâˆ’1(x) both lie
in this half-space, then the difference in (3.22) vanishes. On the other hand, if x and Ï€lâˆ’1(x) both lie in the complement,
then all terms in (3.22) are 0.
On the other hand, for any x âˆˆBd and (Ï‰,b) âˆˆSdâˆ’1 Ã—[âˆ’1,1] we have the bound
âˆ¥Ï† m
x,l(Ï‰,b)âˆ¥â‰¤C2âˆ’l(kâˆ’m),
(3.24)
where C =C(d,k). This holds since Ïƒ(m)
k
(x;Ï‰,b) (as a function of x) has (kâˆ’m)-th order derivatives which are bounded
by C(k) = k!2kâˆ’m for x âˆˆBd by (3.14). Thus using Taylorâ€™s theorem the difference in (3.22) is bounded by
Ïƒ(m)
k
(x;Ï‰,b)âˆ’T m,kâˆ’m
Ï€lâˆ’1(x) (x;Ï‰,b)
 â‰¤C|xâˆ’Ï€lâˆ’1(x)|kâˆ’m â‰¤C2âˆ’l(kâˆ’m),
(3.25)
for C = C(d,k). When l = 1 we also trivially obtain the bound (3.24).
The next step is to decompose the functions Ïƒ(m)
k
(x;Ï‰,b) with (Ï‰,b) âˆˆSâˆ’in terms of the Ï† m
x,l(Ï‰,b). This is captured
in the following technical Lemma.
9

A PREPRINT - SEPTEMBER 28, 2023
Lemma 5. Let Ï† m
x,l be defined by (3.22). For x âˆˆSd define xL = Ï€L(x) and xl = Ï€l(xl+1) for l < L.
Then for any m = 0,...,k, x âˆˆSd and (Ï‰,b) âˆˆSâˆ’we have
Ïƒ(m)
k
(x;Ï‰,b) =
L
âˆ‘
l=1
Ï† m
x j,j(Ï‰,b)+
kâˆ’m
âˆ‘
i=1
L
âˆ‘
l=1
D
Ï† m+i
xl,l (Ï‰,b),Î“m
i,l(x)
E
,
(3.26)
for a collection of tensors Î“m
i,l(x) depending upon x which satisfy the bound
âˆ¥Î“m
i,l(x)âˆ¥â‰¤C2âˆ’il,
(3.27)
for a constant C(d,k).
The proof of this Lemma is a technical, but relatively straightforward calculation, so we postpone it until the end of this
Section and complete the proof of Proposition 1 first.
Utilizing the decomposition (3.26) we write the LHS of (3.19) as
sup
xâˆˆSd

L
âˆ‘
l=1
"
âˆ‘
(Ï‰,b)âˆˆSâˆ’
aÏ‰,bÏ† m
xl,l(Ï‰,b)âˆ’âˆ‘
(Ï‰,b)âˆˆT
bÏ‰,bcÏ† m
xl,l(Ï‰,b)
#
+
L
âˆ‘
l=1
kâˆ’m
âˆ‘
i=1
*
âˆ‘
(Ï‰,b)âˆˆSâˆ’
aÏ‰,bÏ† m+i
xl,l (Ï‰,b)âˆ’âˆ‘
(Ï‰,b)âˆˆT
bÏ‰,bÏ† m+i
xl,l (Ï‰,b),Î“m
i,l(x)
+.
(3.28)
Utilizing the triangle inequality, the bound (3.13), and the bound (3.27), we see that it suffices to find the subset T âŠ‚Sâˆ’
with |T| â‰¤(1âˆ’c)|Sâˆ’|, and new coefficients bÏ‰,b â‰¥0 satisfying âˆ‘(Ï‰,b)âˆˆT bÏ‰,b = âˆ‘(Ï‰,b)âˆˆSâˆ’aÏ‰,b such that for m = 0,...,k
we have
L
âˆ‘
l=1
kâˆ’m
âˆ‘
i=0
2âˆ’il sup
xâˆˆNl
 âˆ‘
(Ï‰,b)âˆˆSâˆ’
aÏ‰,bÏ† m+i
x,l (Ï‰,b)âˆ’âˆ‘
(Ï‰,b)âˆˆT
bÏ‰,bÏ† m+i
x,l (Ï‰,b)
 â‰¤CNâˆ’1
2 âˆ’2(kâˆ’m)+1
2d
(3.29)
for a constant C = C(d,k).
To find this set T and new coefficients bÏ‰,b, we divide the set P = Sâˆ’into disjoint subsets P1,..,Pt of size 3 with

t[
i=1
Pi
 â‰¥|Sâˆ’|/2.
(3.30)
(Note that here we need |Sâˆ’| â‰¥3 which follows from N â‰¥6.) We denote the three elements of each set Pj by
Pj = {uj,vj,wj},
which are ordered so that the coefficients satisfy 0 â‰¤auj â‰¤avj â‰¤aw j. Note that Sâˆ’contains halfspaces and so each of
the elements uj,vj,w j consist of an (Ï‰,b) tuple.
Based upon the partition P1,...,Pt, we will use a modification of the partial coloring argument given in [28] (the idea is
originally due to Spencer [40] and Beck [6]). The main difference is in how we use a partial coloring to reduce the
number of terms in the sum over Sâˆ’.
Given a â€˜partial coloringâ€™ Ï‡ : {1,...,t} â†’{âˆ’1,0,1}, we transform the sum âˆ‘(Ï‰,b)âˆˆSâˆ’aÏ‰,bÏƒk(Ï‰ Â·x+b) in the following
way. If Ï‡(j) = 1, we remove the term corresponding to uj, double the coefficient av j of the term corresponding to vj,
and add the difference au j âˆ’av j to the coefficient aw j of the term corresponding to w j. If Ï‡(j) = âˆ’1, we do the same
but reverse the roles of uj and vj.
This results in a transformed sum âˆ‘(Ï‰,b)âˆˆT bÏ‰,bÏƒk(Ï‰ Â·x+b) over a set T âŠ‚Sâˆ’and with coefficients bÏ‰,b for (Ï‰,b) âˆˆT
described as follows. Let
Rj =
ï£±
ï£²
ï£³
/0
Ï‡(j) = 0
{uj}
Ï‡(j) = 1
{vj}
Ï‡(j) = âˆ’1,
(3.31)
denote the removed set for each Pj. Then the set T is given by
T = Sâˆ’\
 t[
j=1
Rj
!
,
(3.32)
10

A PREPRINT - SEPTEMBER 28, 2023
and for (Ï‰,b) âˆˆT the coefficients bÏ‰,b are given by
bÏ‰,b =
ï£±
ï£´
ï£´
ï£²
ï£´
ï£´
ï£³
aÏ‰,b
(Ï‰,b) /âˆˆS
j Pj
(1+ Ï‡(j))aÏ‰,b
(Ï‰,b) = v j
(1âˆ’Ï‡(j))aÏ‰,b
(Ï‰,b) = u j
aÏ‰,b + Ï‡(j)(au j âˆ’av j)
(Ï‰,b) = w j.
(3.33)
We have constructed this transformation so that
âˆ‘
(Ï‰,b)âˆˆT
bÏ‰,b =
âˆ‘
(Ï‰,b)âˆˆSâˆ’
aÏ‰,b,
the bÏ‰,b â‰¥0 since w j has the largest coefficient among the halfspaces in Pj, and for any x âˆˆSd the error in the m-th
derivative is given by
âˆ‘
(Ï‰,b)âˆˆSâˆ’
aÏ‰,bÏƒ(m)
k
(x;Ï‰,b)âˆ’âˆ‘
(Ï‰,b)âˆˆT
bÏ‰,bÏƒ(m)
k
(x;Ï‰,b) =
t
âˆ‘
j=1
Ï‡(j)
h
âˆ’au jÏƒ(m)
k
(x;uj)+av jÏƒ(m)
k
(x;vj)+(auj âˆ’av j)Ïƒ(m)
k
(x;w j)
i
.
(3.34)
Using the linearity of the derivative and the definition of the Taylor polynomial (3.15), this implies that for any x âˆˆSd,
any level l = 1,...,L, and any m = 0,...,k we have
âˆ‘
(Ï‰,b)âˆˆSâˆ’
aÏ‰,bÏ† m
x,l(Ï‰,b)âˆ’âˆ‘
(Ï‰,b)âˆˆT
bÏ‰,bÏ† m
x,l(Ï‰,b) =
t
âˆ‘
j=1
Ï‡(j)Î¨m
x,l,j,
(3.35)
where we have defined
Î¨m
x,l,j = âˆ’au jÏ† m
x,l(uj)+av jÏ† m
x,l(vj)+(auj âˆ’av j)Ï† m
x,l(w j).
(3.36)
Further, for any index j such that Ï‡(j) Ì¸= 0, we have eliminated one term (either u j or vj) from the sum. Thus
|T| = |S|âˆ’|{j : Ï‡(j) Ì¸= 0}|.
(3.37)
We proceed to find a partial coloring Ï‡ : {1,...,t} â†’{âˆ’1,0,1} with a positive fraction of non-zero entries, i.e. with
|{j : Ï‡(j) Ì¸= 0}| â‰¥ct, such that for m = 0,...,k
L
âˆ‘
l=1
kâˆ’m
âˆ‘
i=0
2âˆ’il sup
xâˆˆNl

t
âˆ‘
j=1
Ï‡(j)Î¨m+i
x,l,j
 â‰¤CNâˆ’1
2 âˆ’2(kâˆ’m)+1
2d
,
(3.38)
for a constant C = C(d,k).
By (3.35) this will guarantee that the LHS in (3.29) is sufficiently small, and by (3.37) this will guarantee that the set T
is small enough, since by (3.30)
|T| = |S|âˆ’|{j : Ï‡(j) Ì¸= 0}| â‰¤|S|âˆ’ct â‰¤

1âˆ’c
6

|Sâˆ’|.
(3.39)
The existence of such a partial coloring Ï‡ follows from a well-known technique in discrepancy theory called the partial
coloring method.
Given a (total) coloring Îµ : {1,...,t} â†’{Â±1} we consider the quantities
Em
x,l(Îµ) :=
t
âˆ‘
j=1
Îµ(j)Î¨m
x,l,j
(3.40)
for each x âˆˆNl. We would like to find a coloring Îµ such that âˆ¥Em
x,l(Îµ)âˆ¥â‰¤âˆ†m
l for all l = 1,...,L, m = 0,...,k and x âˆˆNl,
where the âˆ†m
l are suitable parameters chosen so that
L
âˆ‘
l=1
kâˆ’m
âˆ‘
i=0
2âˆ’ilâˆ†m+i
l
â‰¤CNâˆ’1
2 âˆ’2(kâˆ’m)+1
2d
,
(3.41)
for m = 0,...,k.
11

A PREPRINT - SEPTEMBER 28, 2023
One strategy would be to choose Îµ uniformly at random, bound the tail of the random variable Em
x,l(Îµ), and use
a union bound over x âˆˆNl. Unfortunately, this strategy will lose a factor âˆšlogN. The ingenious method to get
around this, due to Spencer [40] and Beck [6], is to show that instead there exist two colorings Îµ1 and Îµ2 such that
âˆ¥Em
x,l(Îµ1)âˆ’Em
x,l(Îµ2)âˆ¥â‰¤âˆ†m
l for all l = 1,...,L, m = 0,...,k, and x âˆˆNl, and such that Îµ1 and Îµ2 differ in many indices, i.e.
|{j : Îµ1(j) Ì¸= Îµ2(j)}| â‰¥ct
(3.42)
for an absolute constant c. Then Ï‡ = 1
2(Îµ1 âˆ’Îµ2) gives the desired partial coloring.
We will prove the existence of these two colorings Îµ1 and Îµ2 for suitably chosen parameters âˆ†m
l satisfying (3.41). To
help organize this calculation, it is convenient to introduce the notion of the entropy of a discrete distribution (see for
instance [1,29]). (Note that for simplicity all of the logarithms in the following are taken with base 2.)
Definition 3. Let X be a discrete random variable, i.e. the range of X is a countable set Î›. The entropy of X is defined
by
H(X) = âˆ’âˆ‘
Î»âˆˆÎ›
pÎ» log(pÎ»),
(3.43)
where pÎ» = P(X = Î») is the probability of the outcome Î».
One important property of the entropy we will use is subadditivity, i.e. if X = (X1,...,Xr), then
H(X) â‰¤
r
âˆ‘
j=1
H(Xj),
(3.44)
where we have equality in the above bound when the components Xj of X are independent.
An important component of the calculation is the following Lemma from [28] (see also [1,30]).
Lemma 6 (Lemma 11 in [28]). Let Îµ : {1,...,t} â†’{Â±1} be a uniformly random coloring. Let b be a function of Îµ
and suppose that the entropy satisfies H(b(Îµ)) â‰¤t/5. Then there exist two colorings Îµ1,Îµ2 differing in at least t/4
components such that b(Îµ1) = b(Îµ2).
We utilize this lemma in the following way. Take each entry of the (tensor-valued) random variable Em
x,l(Îµ) defined in
(3.40) and round it to the nearest multiple of the (still undetermined) parameter âˆ†m
l . This results in a random variable
bm
x,l(Îµ) = [(âˆ†m
l )âˆ’1Em
x,l(Îµ)],
(3.45)
where [Â·] denote the (component-wise) nearest integer function. Note that if bm
x,l(Îµ1) = bm
x,l(Îµ2), then it follows that
âˆ¥Em
x,l(Îµ1)âˆ’Em
x,l(Îµ2)âˆ¥â‰¤âˆ†m
l . Applying Lemma 6 and the subadditivity of the entropy (3.44), we see that if
L
âˆ‘
l=1
k
âˆ‘
m=0 âˆ‘
xâˆˆNl
H(bm
x,l(Îµ)) â‰¤t/5
(3.46)
for an appropriate choice of âˆ†m
l satisfying (3.41), then there exist two colorings Îµ1 and Îµ2 satisfying the desired condition
with c = 1/4.
It remains to choose the parameters âˆ†m
l satisfying (3.41) and to bound the sum in (3.46). For this, we utilize the
following Lemma from [28], which bounds the entropy of a â€˜roundedâ€™ random variable in terms of the tails of the
underyling real-valued random variable.
Lemma 7 (Lemma 11 in [28]). Let E be a real valued random variable satisfying the tail estimates
P(E â‰¥Î±M) â‰¤eâˆ’Î±2/2, P(E â‰¤âˆ’Î±M) â‰¤eâˆ’Î±2/2,
(3.47)
for some parameter M. Let b(E) denote the random variable obtained by rounding E to the nearest multiple of
âˆ†= 2Î»M. Then the entropy of b satisfies
H(b) â‰¤G(Î») := C0
ï£±
ï£´
ï£²
ï£´
ï£³
eâˆ’Î» 2/9
Î» â‰¥10
1
.1 < Î» < 10
âˆ’log(Î»)
Î» â‰¤.1
(3.48)
for an absolute constant C0.
12

A PREPRINT - SEPTEMBER 28, 2023
To apply this Lemma, we bound the tails of the random variables Em
x,l(Îµ). This follows using Bernsteinâ€™s inequality as
in [28]. Fix an l â‰¥2 and an x âˆˆNl. We call an index j â€˜goodâ€™ if
sgn(Ï‰ Â·x+b) = sgn(Ï‰ Â·Ï€lâˆ’1(x) = b)
(3.49)
for (Ï‰,b) = uj,vj, and w j, and â€˜badâ€™ otherwise. Using (3.23) we see that for the good indices we have
Î¨m
x,l,j = 0.
(3.50)
For the bad indices, we utilize 3.24 to get
âˆ¥Î¨m
x,l,jâˆ¥â‰¤C2âˆ’(kâˆ’m)lNâˆ’1,
(3.51)
since au j,av j â‰¤2/N by (3.21). Next, we bound the number of bad indices. An index is bad if
sgn(Ï‰ Â·x+b) Ì¸= sgn(Ï‰ Â·Ï€lâˆ’1(x)+b),
(3.52)
for (Ï‰,b) = uj,vj or w j. From the construction of Nlâˆ’1 using Lemma 2, the number of (Ï‰,b) for which (3.52) occurs
(and thus the number of bad indices) is bounded by 2âˆ’l|Sâˆ’| â‰¤C2âˆ’lN.
Thus, Bernsteinâ€™s inequality (applied only to the bad indices) gives the following bound on the components of the
random variable Em
x,l,
P
 (Em
x,l)i â‰¥Î±Mm
l

â‰¤eâˆ’Î±2/2
(3.53)
for all i âˆˆIm, where
Mm
l = C2âˆ’(kâˆ’m+ 1
2)lNâˆ’1/2,
(3.54)
for a constant C = C(d,k). The same bound holds also for the negative tails.
The proof is completed via the following calculation (see [28,30] for similar calculations). We let Î±,Î² > 0 be parameters
to be specified in a moment and define a function
Î›Î±,Î²(x) =
2Î±x
x â‰¥0
2Î²x
x â‰¤0.
(3.55)
Let Îº > 0 be another parameter and set Ï„ to be the largest integer satisfying 2dÏ„ â‰¤ÎºN. We set the discretization
parameter to be
âˆ†m
l = 2Mm
l Î›Î±,Î²(l âˆ’Ï„).
(3.56)
Fix an m with 0 â‰¤m â‰¤k. We calculate
L
âˆ‘
l=1
kâˆ’m
âˆ‘
i=0
2âˆ’ilâˆ†m+i
l
â‰¤CNâˆ’1/2
kâˆ’m
âˆ‘
i=0
âˆž
âˆ‘
l=âˆ’âˆž
2âˆ’il2âˆ’(kâˆ’mâˆ’i+ 1
2)lÎ›Î±,Î²(l âˆ’Ï„)
â‰¤CNâˆ’1/2
âˆž
âˆ‘
l=âˆ’âˆž
2âˆ’(kâˆ’m+ 1
2)lÎ›Î±,Î²(l âˆ’Ï„),
(3.57)
since all of the terms in the sum over i are the same. Making a change of variables in the last sum, we get
L
âˆ‘
l=1
kâˆ’m
âˆ‘
i=0
2âˆ’ilâˆ†m+i
l
â‰¤CNâˆ’1/22âˆ’(kâˆ’m+ 1
2)Ï„
âˆž
âˆ‘
l=âˆ’âˆž
2âˆ’(kâˆ’m+ 1
2)lÎ›Î±,Î²(l).
(3.58)
If we now choose Î± and Î² such that the above sum over l converges (this will happen as long as Î± < k âˆ’m+ 1
2 < Î²),
then we get
L
âˆ‘
l=1
kâˆ’m
âˆ‘
i=0
2âˆ’ilâˆ†m+i
l
â‰¤CNâˆ’1
2 âˆ’2(kâˆ’m)+1
2d
,
(3.59)
since by construction 2Ï„ â‰¥(1/2)(ÎºN)1/d (note the constant C depends upon the choice of Îº which will be made
shortly). This verifies (3.41).
To verify the entropy condition, we calculate
L
âˆ‘
l=1
k
âˆ‘
m=0 âˆ‘
xâˆˆNl
H(bm
x,l(Îµ)) â‰¤
L
âˆ‘
l=1
k
âˆ‘
m=0 âˆ‘
xâˆˆNl âˆ‘
iâˆˆIm
H(bm
x,l(Îµ)i)
(3.60)
13

A PREPRINT - SEPTEMBER 28, 2023
using subadditivity of the entropy. We now use Lemma 7 combined with the tail bound estimate (3.53) to get
H(bm
x,l(Îµ)i) â‰¤G(âˆ†m
l /(2Mm
l )) = G(Î›Î±,Î²(l âˆ’Ï„)).
(3.61)
Using that |Im| â‰¤C = C(d,k) and |Nl| â‰¤C2dl, we get that
L
âˆ‘
l=1
k
âˆ‘
m=0 âˆ‘
xâˆˆNl
H(bm
x,l(Îµ)) â‰¤C
L
âˆ‘
l=1
2dlG(Î›Î±,Î²(l âˆ’Ï„)) â‰¤C
âˆž
âˆ‘
l=âˆž
2dlG(Î›Î±,Î²(l âˆ’Ï„)).
(3.62)
Finally, making another change of variables, we get
L
âˆ‘
l=1
k
âˆ‘
m=0 âˆ‘
xâˆˆNl
H(bm
x,l(Îµ)) â‰¤C2dÏ„
âˆž
âˆ‘
l=âˆž
2dlG(Î›Î±,Î²(l)) â‰¤CÎºN,
(3.63)
since it is easy to verify that the above sum over l converges for any Î±,Î² > 0. Choosing Îº sufficiently small so
that CÎº â‰¤1/60 will guarantee that the condition (3.46) is satisfied (since t â‰¥N/12 by (3.30)) and this completes the
proof.
Proof of Lemma 5. We first prove that for m = 0,...,k, (Ï‰,b) âˆˆSdâˆ’1 Ã—[âˆ’1,1] and l = 1,...,L we have
Ïƒ(m)
k
(xl;Ï‰,b) =
l
âˆ‘
j=1
Ï† m
x j,j(Ï‰,b)+
kâˆ’m
âˆ‘
i=1
lâˆ’1
âˆ‘
j=1
D
Ï† m+i
x j,j (Ï‰,b),Î“m,l
i,j (x)
E
,
(3.64)
where the tensors Î“m,l
i,j (x) satisfy the bound
Î“m,l
i,j (x)
 â‰¤C2âˆ’ij,
(3.65)
for a constant C = C(d,k).
We prove this by (reverse) induction on m. Note if m = k the equation (3.64) holds since the definition of Ï† (m)
x,l in (3.22)
becomes
Ï† m
x,l(Ï‰,b) =
(
Ïƒ(m)
k
(x;Ï‰,b)âˆ’Ïƒ(m)
k
(Ï€lâˆ’1(x);Ï‰,b)
l â‰¥2
Ïƒ(m)
k
(x;Ï‰,b)
l = 1,
(3.66)
Let 0 â‰¤m â‰¤k and suppose that (3.64) holds for m+1,...,k. We will show that it also holds for m. Expanding out the
Taylor polynomial in the definition of Ï† m
x,l for x = xl we see that
Ïƒ(m)
k
(xl;Ï‰,b) = Ï† m
xl,l(Ï‰,b)+T m,kâˆ’m
xlâˆ’1
(xl;Ï‰,b)
= Ï† m
xl,l(Ï‰,b)+
kâˆ’m
âˆ‘
q=0
1
q!
D
Ïƒ(m+q)
k
(xlâˆ’1;Ï‰,b),(xl âˆ’xlâˆ’1)âŠ—qE
= Ï† m
xl,l(Ï‰,b)+Ïƒ(m)
k
(xlâˆ’1;Ï‰,b)+
kâˆ’m
âˆ‘
q=1
1
q!
D
Ïƒ(m+q)
k
(xlâˆ’1;Ï‰,b),(xl âˆ’xlâˆ’1)âŠ—qE
.
(3.67)
Applying this expansion recursively to Ïƒ(m)
k
(xlâˆ’1;Ï‰,b), we get
Ïƒ(m)
k
(xl;Ï‰,b) =
l
âˆ‘
j=1
Ï† m
x j,j(Ï‰,b)+
lâˆ’1
âˆ‘
p=1
kâˆ’m
âˆ‘
q=1
1
q!
D
Ïƒ(m+q)
k
(xp;Ï‰,b),(xp+1 âˆ’xp)âŠ—qE
.
(3.68)
Now we use the inductive assumption to expand Ïƒ(m+q)
k
(xp;Ï‰,b) using (3.64) and apply the identity (3.12) to get
Ïƒ(m)
k
(xl;Ï‰,b) =
l
âˆ‘
j=1
Ï† m
x j,j(Ï‰,b)+
lâˆ’1
âˆ‘
p=1
kâˆ’m
âˆ‘
q=1
1
q!
p
âˆ‘
j=1
D
Ï† m+q
x j,j (Ï‰,b),(xp+1 âˆ’xp)âŠ—qE
+
lâˆ’1
âˆ‘
p=1
kâˆ’m
âˆ‘
q=1
1
q!
kâˆ’mâˆ’q
âˆ‘
iâ€²=1
pâˆ’1
âˆ‘
j=1
D
Ï† m+q+iâ€²
x j,j
(Ï‰,b),Î“m+q,l
iâ€²,j
(x)âŠ—(xp+1 âˆ’xp)âŠ—qE
.
(3.69)
Rearranging this sum, we obtain
Ïƒ(m)
k
(xl;Ï‰,b) =
l
âˆ‘
j=1
Ï† m
x j,j(Ï‰,b)+
kâˆ’m
âˆ‘
i=1
lâˆ’1
âˆ‘
j=1
D
Ï† m+i
x j,j (Ï‰,b),Î“m,l
i,j (x)
E
,
(3.70)
14

A PREPRINT - SEPTEMBER 28, 2023
where the tensors Î“m,l
i,j (x) are defined recursively by
Î“m,l
i,j (x) = 1
i!
lâˆ’1
âˆ‘
p=j
(xp+1 âˆ’xp)âŠ—i +
lâˆ’1
âˆ‘
p=j
iâˆ’1
âˆ‘
q=1
1
q!Î“m+q,l
iâˆ’q,j (x)âŠ—(xp+1 âˆ’xp)âŠ—q.
(3.71)
Finally, we bound the norm âˆ¥Î“m,l
i,j (x)âˆ¥. By construction, the points xp satisfy |xp+1 âˆ’xp| â‰¤C2âˆ’p for a dimension
dependent constant C = C(d) (in the Euclidean norm which bounds the â„“âˆž-norm). This gives the bound
âˆ¥Î“m,l
i,j (x)âˆ¥â‰¤C
 
1
i!
lâˆ’1
âˆ‘
p=j
2âˆ’pj +
lâˆ’1
âˆ‘
p=j
iâˆ’1
âˆ‘
q=1
1
q!2âˆ’pqâˆ¥Î“m+q,l
iâˆ’q,j âˆ¥
!
.
(3.72)
Utilizing the inductive assumption to bound âˆ¥Î“m+q,l
iâˆ’q,j âˆ¥we get
âˆ¥Î“m,l
i,j âˆ¥â‰¤C
 
1
i!
lâˆ’1
âˆ‘
p=j
2âˆ’pj +
lâˆ’1
âˆ‘
p=j
iâˆ’1
âˆ‘
q=1
1
q!2âˆ’pq2âˆ’(iâˆ’q)j
!
â‰¤C
 
1
i!
âˆž
âˆ‘
p=j
2âˆ’pj +2âˆ’ij
âˆž
âˆ‘
p=j
âˆž
âˆ‘
q=1
1
q!2âˆ’q(pâˆ’j)
!
â‰¤C2âˆ’ij,
(3.73)
for a potentially different constant C. However, the induction is completed after a finite number (namely k +1) steps.
Thus the constant C = C(d,k) can be taken uniform in m. This proves (3.64).
To prove (3.26), we write
Ïƒ(m)
k
(x;Ï‰,b) =
h
Ïƒ(m)
k
(x;Ï‰,b)âˆ’T m,kâˆ’m
xL
(x;Ï‰,b)
i
+T m,kâˆ’m
xL
(x;Ï‰,b)
=
h
Ïƒ(m)
k
(x;Ï‰,b)âˆ’T m,kâˆ’m
xL
(x;Ï‰,b)
i
+
kâˆ’m
âˆ‘
q=0
1
q!
D
Ïƒ(m+q)
k
(xL;Ï‰,b),(xâˆ’xL)âŠ—qE
.
(3.74)
We claim that if Ï‰,b âˆˆSâˆ’, then the first term
Ïƒ(m)
k
(x;Ï‰,b)âˆ’T m,kâˆ’m
xL
(x;Ï‰,b) = 0.
(3.75)
This follows since by construction using Lemma 1, we have the bound
|Sâˆ’âˆ©{(Ï‰,b) âˆˆSdâˆ’1 Ã—[âˆ’1,1], sgn(Ï‰ Â·x+b) Ì¸= sgn(Ï‰ Â·Ï€L(x)+b)}| â‰¤2âˆ’L|Sâˆ’| < 1,
(3.76)
since 2âˆ’L < Nâˆ’1. Thus for all (Ï‰,b) âˆˆSâˆ’and x âˆˆSd, we have sgn(Ï‰ Â·x+b) = sgn(Ï‰ Â·Ï€L(x)+b), and the argument
following (3.23) implies that the difference in (3.75) vanishes.
Next, we expand each term in the sum in (3.74) using (3.64) with l = L to get (using again the identity (3.12))
Ïƒ(m)
k
(x;Ï‰,b) =
kâˆ’m
âˆ‘
q=0
1
q!
L
âˆ‘
j=1
D
Ï† m+q
xj,j (Ï‰,b),(xâˆ’xL)âŠ—qE
+
kâˆ’m
âˆ‘
q=0
1
q!
kâˆ’mâˆ’q
âˆ‘
iâ€²=1
Lâˆ’1
âˆ‘
j=1
D
Ï† m+q+iâ€²
x j,j
(Ï‰,b),Î“m+q,L
iâ€²,j
âŠ—(xâˆ’xL)âŠ—qE
.
(3.77)
Rewriting this, we get
Ïƒ(m)
k
(x;Ï‰,b) =
L
âˆ‘
j=1
Ï† m
x j,j(Ï‰,b)+
kâˆ’m
âˆ‘
i=1
L
âˆ‘
j=1
D
Ï† m+i
x j,j (Ï‰,b),Î“m
i,j(x)
E
,
(3.78)
where the tensors Î“m
i,j(x) are given by
Î“m
i,j(x) = 1
i!(xâˆ’xL)âŠ—i +
iâˆ’1
âˆ‘
q=0
Î“m+q,L
iâˆ’q,j âŠ—(xâˆ’xL)âŠ—q.
(3.79)
Finally, we bound the norm âˆ¥Î“m
i,j(x)âˆ¥. Utilizing that |xâˆ’xL| â‰¤C2âˆ’L and the bound (3.65) we get
âˆ¥Î“m
i,j(x)âˆ¥â‰¤2âˆ’Li
i!
+C
iâˆ’1
âˆ‘
q=0
2âˆ’(iâˆ’q)j2âˆ’Lq â‰¤C2âˆ’ij,
(3.80)
for a constant C(d,k), since j â‰¤L and 0 â‰¤i â‰¤k. Upon relabelling j to l this is exactly Lemma 5.
15

A PREPRINT - SEPTEMBER 28, 2023
4
Acknowledgements
We would like to thank Ron DeVore, Rob Nowak, Jinchao Xu, and Rahul Parhi for helpful discussions. This work was
supported by the National Science Foundation (DMS-2111387 and CCF-2205004).
References
[1] Noga Alon and Joel H Spencer, The probabilistic method, John Wiley & Sons, 2016.
[2] Francis Bach, Breaking the curse of dimensionality with convex neural networks, The Journal of Machine Learning
Research 18 (2017), no. 1, 629â€“681.
[3] Andrew R Barron, Neural net approximation, Proc. 7th Yale workshop on adaptive and learning systems, vol. 1,
1992, pp. 69â€“72.
[4]
, Universal approximation bounds for superpositions of a sigmoidal function, IEEE Transactions on
Information theory 39 (1993), no. 3, 930â€“945.
[5] Andrew R Barron, Albert Cohen, Wolfgang Dahmen, and Ronald A DeVore, Approximation and learning by
greedy algorithms, The Annals of Statistics (2008), 64â€“94.
[6] JÃ³zsef Beck, Some upper bounds in the theory of irregularities of distribution, Acta Arithmetica 43 (1984),
115â€“130.
[7] U Betke and P McMullen, Estimating the sizes of convex bodies from projections, Journal of the London
Mathematical Society 2 (1983), no. 3, 525â€“538.
[8] J Bourgain and J Lindenstrauss, Distribution of points on spheres and approximation by zonotopes, Israel Journal
of Mathematics 64 (1988), 25â€“31.
[9]
, Approximating the ball by a minkowski sum of segments with equal length, Discrete & computational
geometry 9 (1993), no. 2, 131â€“144.
[10] J Bourgain, J Lindenstrauss, and V Milman, Approximation of zonoids by zonotopes, Acta Mathematica 162
(1989), no. 1, 73â€“141.
[11] Gerald HL Cheang and Andrew R Barron, A better approximation for balls, Journal of Approximation Theory
104 (2000), no. 2, 183â€“203.
[12] Ronald A DeVore, Nonlinear approximation, Acta numerica 7 (1998), 51â€“150.
[13] Ronald A DeVore and Vladimir N Temlyakov, Some remarks on greedy algorithms, Advances in computational
Mathematics 5 (1996), 173â€“187.
[14] Michael J Donahue, Christian Darken, Leonid Gurvits, and Eduardo Sontag, Rates of convex approximation in
non-hilbert spaces, Constructive Approximation 13 (1997), 187â€“220.
[15] Richard M Dudley, Central limit theorems for empirical measures, The Annals of Probability (1978), 899â€“929.
[16] Weinan E, Chao Ma, and Lei Wu, The barron space and the flow-induced function spaces for neural network
models, Constructive Approximation 55 (2022), no. 1, 369â€“406.
[17] Lawrence C Evans, Partial differential equations, vol. 19, American Mathematical Soc., 2010.
[18] David Haussler, Sphere packing numbers for subsets of the boolean n-cube with bounded vapnik-chervonenkis
dimension, Journal of Combinatorial Theory, Series A 69 (1995), no. 2, 217â€“232.
[19] Lee K Jones, A simple lemma on greedy approximation in hilbert space and convergence rates for projection
pursuit regression and neural network training, The annals of Statistics (1992), 608â€“613.
[20] Antal JoÃ³s and Zsolt LÃ¡ngi, Isoperimetric problems for zonotopes, Mathematika 69 (2023), no. 2, 508â€“534.
[21] Jason M Klusowski and Andrew R Barron, Approximation by combinations of relu and squared relu ridge
functions with â„“1 and â„“0 controls, IEEE Transactions on Information Theory 64 (2018), no. 12, 7649â€“7656.
[22] J Linhart, Approximation of a ball by zonotopes using uniform distribution on the sphere, Archiv der Mathematik
53 (1989), 82â€“86.
[23] Jianfeng Lu and Yulong Lu, A priori generalization error analysis of two-layer neural networks for solving high
dimensional schrÃ¶dinger eigenvalue problems, Communications of the American Mathematical Society 2 (2022),
no. 1, 1â€“21.
[24] Limin Ma, Jonathan W Siegel, and Jinchao Xu, Uniform approximation rates and metric entropy of shallow
neural networks, Research in the Mathematical Sciences 9 (2022), no. 3, 46.
16

A PREPRINT - SEPTEMBER 28, 2023
[25] Yuly Makovoz, Random approximants and neural networks, Journal of Approximation Theory 85 (1996), no. 1,
98â€“109.
[26] JiË‡rÃ­ MatouÅ¡ek, Cutting hyperplane arrangements, Discrete & Computational Geometry 6 (1991), 385â€“406.
[27]
, Tight upper bounds for the discrepancy of half-spaces, Discrete & Computational Geometry 13 (1995),
593â€“601.
[28]
, Improved upper bounds for approximation by zonotopes, Acta Mathematica 177 (1996), 55â€“73.
[29]
, Geometric discrepancy: An illustrated guide, vol. 18, Springer Science & Business Media, 1999.
[30] JiË‡rÃ­ MatouÅ¡ek and Joel Spencer, Discrepancy in arithmetic progressions, Journal of the American Mathematical
Society 9 (1996), no. 1, 195â€“204.
[31] Greg Ongie, Rebecca Willett, Daniel Soudry, and Nathan Srebro, A function space view of bounded norm infinite
width relu nets: The multivariate case, International Conference on Learning Representations, 2019.
[32] Rahul Parhi and Robert D Nowak, Banach space representer theorems for neural networks and ridge splines, The
Journal of Machine Learning Research 22 (2021), no. 1, 1960â€“1999.
[33] Gilles Pisier, Remarques sur un rÃ©sultat non publiÃ© de b. maurey, SÃ©minaire dâ€™Analyse fonctionnelle (dit"
Maurey-Schwartz") (1981), 1â€“12.
[34] R Tyrrell Rockafellar, Convex analysis, vol. 11, Princeton university press, 1997.
[35] Jonathan W Siegel, Qingguo Hong, Xianlin Jin, Wenrui Hao, and Jinchao Xu, Greedy training algorithms for
neural networks and applications to pdes, Journal of Computational Physics 484 (2023), 112084.
[36] Jonathan W Siegel and Jinchao Xu, Approximation rates for neural networks with general activation functions,
Neural Networks 128 (2020), 313â€“321.
[37]
, High-order approximation rates for shallow neural networks with cosine and reluk activation functions,
Applied and Computational Harmonic Analysis 58 (2022), 1â€“26.
[38]
, Sharp bounds on the approximation rates, metric entropy, and n-widths of shallow neural networks,
Foundations of Computational Mathematics (2022), 1â€“57.
[39]
, Characterization of the variation spaces corresponding to shallow neural networks, Constructive
Approximation (2023), 1â€“24.
[40] Joel Spencer, Six standard deviations suffice, Transactions of the American Mathematical Society 289 (1985),
no. 2, 679â€“706.
[41] Vladimir Temlyakov, Greedy approximation, Acta Numerica 17 (2008), 235â€“409.
[42] Vladimir Vapnik, The nature of statistical learning theory, Springer science & business media, 1999.
[43] VN Vapnik and A Ya Chervonenkis, On the uniform convergence of relative frequencies of events to their
probabilities, Theory of Probability and its Applications 16 (1971), no. 2, 264.
[44] Jinchao Xu, Finite neuron method and convergence analysis, Communications in Computational Physics 28
(2020), no. 5, 1707â€“1745.
[45] Yunfei Yang and Ding-Xuan Zhou, Optimal rates of approximation by shallow ReLUk neural networks and
applications to nonparametric regression, arXiv preprint arXiv:2304.01561 (2023).
[46] Joseph E Yukich, Maxwell B Stinchcombe, and Halbert White, Sup-norm approximation bounds for networks
through probabilistic methods, IEEE Transactions on Information Theory 41 (1995), no. 4, 1021â€“1027.
17

