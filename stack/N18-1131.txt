Proceedings of NAACL-HLT 2018, pages 1446–1459
New Orleans, Louisiana, June 1 - 6, 2018. c⃝2018 Association for Computational Linguistics
A Neural Layered Model for Nested Named Entity Recognition
Meizhi Ju1,3, Makoto Miwa2,3 and Sophia Ananiadou1,3
1National Centre for Text Mining, University of Manchester, United Kingdom
2Toyota Technological Institute, Japan
3Artiﬁcial Intelligence Research Center (AIRC),
National Institute of Advanced Industrial Science and Technology (AIST), Japan
{meizhi.ju, sophia.ananiadou}@manchester.ac.uk
makoto-miwa@toyota-ti.ac.jp
Abstract
Entity mentions embedded in longer entity
mentions are referred to as nested entities.
Most named entity recognition (NER) sys-
tems deal only with the ﬂat entities and ignore
the inner nested ones, which fails to capture
ﬁner-grained semantic information in underly-
ing texts. To address this issue, we propose
a novel neural model to identify nested enti-
ties by dynamically stacking ﬂat NER layers.
Each ﬂat NER layer is based on the state-of-
the-art ﬂat NER model that captures sequen-
tial context representation with bidirectional
long short-term memory (LSTM) layer and
feeds it to the cascaded CRF layer. Our model
merges the output of the LSTM layer in the
current ﬂat NER layer to build new represen-
tation for detected entities and subsequently
feeds them into the next ﬂat NER layer. This
allows our model to extract outer entities by
taking full advantage of information encoded
in their corresponding inner entities, in an
inside-to-outside way. Our model dynamically
stacks the ﬂat NER layers until no outer enti-
ties are extracted. Extensive evaluation shows
that our dynamic model outperforms state-of-
the-art feature-based systems on nested NER,
achieving 74.7% and 72.2% on GENIA and
ACE2005 datasets, respectively, in terms of F-
score.1
1
Introduction
The task of named entity recognition (NER) in-
volves the extraction from text of names of en-
tities pertaining to semantic types such as per-
son (PER), location (LOC) and geo-political en-
tity (GPE). NER has drawn the attention of many
researchers as the ﬁrst step towards NLP applica-
tions such as entity linking (Gupta et al., 2017), re-
lation extraction (Miwa and Bansal, 2016), event
1Code
is
available
at
https://github.com/
meizhiju/layered-bilstm-crf
LOC
PER
GPE
GPE
The premier of the western Canadian province of British Columbia ...
LOC
PER
GPE
GPE
The premier of the western Canadian province of British Columbia ...
Figure 1:
A sentence from ACE2005 (Walker et al.,
2006) containing the nested 4 entities nested 3 levels
deep.
extraction (Feng et al., 2016) and co-reference res-
olution (Fragkou, 2017; Stone and Arora, 2017).
Due to the properties of natural language, many
named entities contain nested entities: embedded
names which are included in other entities, illus-
trated in Figure 1. This phenomenon is quite com-
mon in many domains (Alex et al., 2007; Byrne,
2007; Wang, 2009; M`arquez et al., 2007). How-
ever, much of the work on NER copes only with
non-nested entities which are also called ﬂat enti-
ties and neglects nested entities. This leads to loss
of potentially important information, with nega-
tive impacts on subsequent tasks.
Traditional approaches to NER mainly in-
volve two types of approaches:
supervised
learning (Ling and Weld, 2012; Marci´nczuk,
2015; Leaman and Lu, 2016) and hybrid ap-
proaches (Bhasuran et al., 2016; Rockt¨aschel
et al., 2012; Leaman et al., 2015) that combine su-
pervised learning with rules. Such approaches re-
quire either domain knowledge or heavy feature-
engineering. Recent advances in neural networks
enable NER without depending on external knowl-
edge resources through automated learning high-
level and abstract features from text (Lample et al.,
2016; Ma and Hovy, 2016; Pahuja et al., 2017;
Strubell et al., 2017).
In this paper, we propose a novel dynamic neu-
ral model for nested entity recognition, without re-
lying on any external knowledge resources or lin-
guistics features. Our model enables sequentially
1446

Mouse
interleukin-2
receptor
alpha
gene
expression
O
B-Protein
O
O
O
O
O
B-DNA
I-DNA
I-DNA
I-DNA
O
O
O
O
Embedding layer
Flat NER layer
Flat NER layer
Flat NER layer
Sequence
Dropout
Dropout
CRF
CRF
LSTM Unit
LSTM Unit
Label
Label
Word representation
Word representation
Flat NER Unit
Flat NER Unit
Mouse
interleukin-2
receptor
alpha
gene
expression
O
B-Protein
O
O
O
O
O
B-DNA
I-DNA
I-DNA
I-DNA
O
O
O
O
Embedding layer
Flat NER layer
Flat NER layer
Flat NER layer
Sequence
Dropout
CRF
LSTM Unit
Label
Word representation
Flat NER Unit
Figure 2: Overview of our layered model architecture. “interleukin-2” and “interleukin-2 receptor alpha gene” are
nested entities.
stacking ﬂat NER layers from bottom to up and
identifying entities in an end-to-end manner. The
number of stacked layers depends on the level of
entity nesting and dynamically adjusts to the input
sequences as the nested level varies from different
sequences.
Given a sequence of words, our model ﬁrst
represents each word using a low-dimensional
vector concatenated from its corresponding word
and character sequence embeddings.
Taking
the sequence of the word representation as in-
put, our ﬂat NER layer enables capturing con-
text representation by a long short-term mem-
ory (LSTM) (Hochreiter and Schmidhuber, 1997)
layer. The context representation is then fed to
a CRF layer for label prediction. Subsequently,
the context representation from the LSTM layer is
merged to build representation for each detected
entity, which is used as the input for the next ﬂat
NER layer. Our model stops detecting entities if
no entities are predicted by the current ﬂat NER
layer. Through stacking ﬂat NER layers in order,
we are able to extract entities from inside to out-
side with sharing parameters among the different
LSTM layers and CRF layers.
We gain 3.9 and 9.1 percentage point improve-
ments regarding F-score over the state-of-the-art
feature-based model on two nested entity corpora:
GENIA (Kim et al., 2003) and ACE2005 (Walker
et al., 2006), and analyze contributions of inner en-
tities to outer entity detection, drawing several key
conclusions.
In addition, experiments are conducted on a
ﬂatly annotated corpora JNLPBA (Kim et al.,
2004). Our model can be a complete NER model
as well for ﬂat entities, on the condition that it
is trained on annotations that do not account for
nested entities. We obtain 75.55% in terms of F-
score that is comparable to the state-of-the-art per-
formance.
2
Neural Layered Model
Our nested NER model is designed based on a
sequential stack of ﬂat NER layers that detects
nested entities in an end-to-end manner.
Fig-
ure 2 provides the overview of our model. Our
ﬂat NER layers are inspired by the state-of-the-
art model proposed in Lample et al. (2016). The
layer utilizes one single bidirectional LSTM layer
to represent word sequences and predict ﬂat enti-
ties by putting one single CRF layer on top of the
LSTM layer. Therefore, we refer to our model as
Layered-BiLSTM-CRF model. If any entities are
predicted, a new ﬂat NER layer is introduced and
the word sequence representation of each detected
entity by the current ﬂat NER layer is merged to
compose a representation for the entity, which is
then passed on to the new ﬂat NER layer as its in-
put. Otherwise, the model terminates stacking and
hence ﬁnishes entity detection.
In this section, we provide a brief description
of the model architecture: the ﬂat NER layers and
their stacking, the embedding layer and their train-
ing.
2.1
Flat NER layer
A ﬂat NER layer consists of an LSTM layer and
a CRF layer. The LSTM layer captures the bidi-
1447

rectional context representation of sequences and
subsequently feeds it to the CRF layer to globally
decode label sequences.
LSTM is a variant of recurrent neural networks
(RNNs) (Goller and Kuchler, 1996) that incor-
porates a memory cell to remember the past in-
formation for a long period of time.
This en-
ables capturing long dependencies, thus reducing
the gradient vanishing/explosion problem existing
in RNNs. We employ bidirectional LSTM with
no peephole connection. We refer the readers to
Hochreiter and Schmidhuber (1997) for more de-
tails of LSTM used in our work.
CRFs are used to globally predict label se-
quences for any given sequences. Given an input
sequence X = (x1, x2, . . . , xn) which is the out-
put from the LSTM layer, we maximize the log-
probability during training. In decoding, we set
transition costs between illegal transitions, e.g.,
transition from O to I-PER, as inﬁnite to restrict
illegal labels. The expected label sequence y =
(y1, y2, . . . , yn) is predicted based on maximum
scores in decoding.
2.2
Stacking ﬂat NER layers
We stack a ﬂat NER layer on the top of the cur-
rent ﬂat NER layer, aiming to extract outer en-
tities. Concretely, we merge and average current
context representation of the regions composed in
the detected entities, as described in the following
equation:
mi =
1
end −start + 1
end
X
i=start
zi,
(1)
where zi denotes the representation of the i-th
word from the ﬂat NER layer, and mi is the
merged representation for an entity. The region
starts from a position start and ends at a position
end of the sequence. This merged representation
of detected entities allows us to treat each detected
entity as a single token, and hence we are able to
make the most of inner entity information to en-
courage outer entity recognition. If the region is
detected as a non-entity, we keep the representa-
tion without any processing. The processed con-
text representation of the ﬂat NER layer is used as
the input for the next ﬂat NER layer.
2.3
Embedding layer
The input for the ﬁrst NER layer is different from
the remaining ﬂat NER layers since the ﬁrst layer
g
e
e
gene
n
Characters
Word
LSTM
LSTM
LSTM
LSTM
Word representation
Figure 3: Word representation of a word ‘gene’. We
concatenate the outputs of character embedding from
LSTM and word embedding to obtain its ﬁnal word
representation.
has no previous layers. We thus represent each
word by concatenating character sequence embed-
dings and word embeddings for the ﬁrst ﬂat NER
layer. Figure 3 describes the architecture of the
embedding layer to produce word representation.
Following the successes of Ma and Hovy (2016)
and Lample et al. (2016) in utilizing character em-
beddings on the ﬂat NER task, we also represent
each word with its character sequence to capture
the orthographic and morphological features of the
word. Each character is mapped to a randomly ini-
tialized vector through a character lookup table.
We feed the character vectors comprising a word
to a bidirectional LSTM layer and concatenate the
forward and backward representation to obtain the
word-level embedding.
Differently from the character sequence embed-
dings, the pretrained word embeddings are used
to initialize word embeddings. When evaluating
or applying the model, words that are outside of
the pretrained embeddings and training dataset are
mapped to an unknown (UNK) embedding, which
is randomly initialized during training. To train
the UNK embedding, we replace words whose fre-
quency is 1 in the training dataset with the UNK
embedding with a probability 0.5.
2.4
Training
We prepare the gold labels based on the conven-
tional BIO (Beginning, Inside, Out of entities) tag-
ging scheme to represent a label attached to each
word.
1448

As our model detects entities from inside to out-
side, we keep the same order in preparing the gold
labels for each word sequence. We call it the de-
tection order rule. Meantime, we deﬁne that each
entity region in the sequence can only be tagged
once with the same entity type, referred to as the
non-duplicate rule.
For instance, in Figure 2,
“interleukin-2” is tagged ﬁrst while “interleukin-
2 receptor alpha gene” is subsequently tagged fol-
lowing the above two rules. When assigning the
label O to non-entity regions, we only follow the
detection order rule. As a result, two gold label
sequences {O, B-Protein, O, O, O, O} and {O,
B-DNA, I-DNA, I-DNA, I-DNA, O} are assigned
to the given word sequence “Mouse interleukin-2
receptor alpha gene expression” as shown in Fig-
ure 2. With these rules, the number of labels for
each word equals the nested level of entities in the
given word sequence.
We employ mini-batch training and update the
model parameters using back-propagation through
time (BPTT) (Werbos, 1990) with Adam (Kingma
and Ba, 2014).
The model parameters include
weights, bias, transition costs, and embeddings
of characters. We disable updating the word em-
beddings.2 During training, early stopping, L2-
regularization and dropout (Hinton et al., 2012)
are used to prevent overﬁtting. Dropout is em-
ployed to the input of each ﬂat NER layer. Hyper-
parameters including batch size, number of hid-
den units in LSTM, character dimensions, dropout
rate, Adam learning rate, gradient clipping and
weight decay (L2) are all tuned with Bayesian op-
timization (Snoek et al., 2012).
3
Evaluation Settings
We employed three datasets for evaluation: GE-
NIA3 (Kim et al., 2003), ACE20054 (Walker et al.,
2006) and JNLPBA5 (Kim et al., 2004). We brieﬂy
explain the data and task settings and then intro-
duce model and experimental settings.
3.1
Data and Task Settings
We performed nested entity extraction experi-
ments on GENIA and ACE2005 while we con-
2We tried updating and disabling updating word embed-
dings. The former trial did not work.
3http://www.geniaproject.org/
genia-corpus/term-corpus
4https://catalog.ldc.upenn.edu/
ldc2006t06
5http://www.nactem.ac.uk/tsujii/GENIA/
ERtask/report.html
ducted ﬂat entity extraction on the JNLPBA
dataset. For the details of data statistics and pre-
processing, please refer to the supplementary ma-
terials.
GENIA involves 36 ﬁne-grained entity cate-
gories among total 2,000 MEDLINE abstracts.
Following the same task settings as in Finkel and
Manning (2009) and Lu and Roth (2015), we col-
lapsed all DNA subcategories as DNA. The same
setting was applied to RNA, protein, cell line and
cell type categories. We used same test portion as
Finkel and Manning (2009), Lu and Roth (2015)
and Muis and Lu (2017) for the direct comparison.
ACE2005 contains 7 ﬁne-grained entity cate-
gories. We made same modiﬁcations described in
Lu and Roth (2015) and Muis and Lu (2017) by
keeping ﬁles from bn, bw, nw and wl and spit-
ting them into training, development and testing
datasets at random following same ratio 8:1:1, re-
spectively.
JNLPBA deﬁnes both training and testing
datasets.
These two datasets are composed of
2,000 and 404 MEDLINE abstracts, respectively.
JNLPBA is originally from the GENIA cor-
pus. However, only ﬂat and topmost entities in
JNLPBA are kept while nested and discontinuous
entities are removed. Like our preprocessing on
the GENIA corpus, subcategories are collapsed
and only 5 entity types are ﬁnally reserved. We
randomly chose the 90% sentences of the original
training dataset as our training dataset and the re-
maining as our development dataset.
Precision (P), recall (R) and F-score (F) were
used for the evaluation metrics in our tasks. We
deﬁne that if the numbers of gold entities and pre-
dictions are all zeros, the evaluation metrics all
equal one hundred percent.
3.2
Model and Experimental Settings
Our model was implemented with Chainer6 (Tokui
et al., 2015).
We initialized word embeddings
in GENIA and JNLPBA with the pretrained em-
beddings trained on MEDLINE abstracts (Chiu
et al., 2016). For ACE2005, we initialized each
word with the pretrained embeddings which are
trained by Miwa and Bansal (2016). Except for
the word embeddings, parameters of word embed-
dings were initialized with a normal distribution.
For LSTM, we initialized hidden states, cell state
and all the bias terms as 0 except for the forget gate
6https://chainer.org/
1449

bias that was set as 1. For other hyper-parameters,
we chose the best hyper-parameters via Bayesian
optimization.
We refer the readers to the sup-
plemental material for the settings of the hyper-
parameters of the models and Bayesian optimiza-
tion.
For ablation tests, we compared with our
layered-BiLSTM-CRF model with two models
that produce the input for next ﬂat NER layer in
different ways. The ﬁrst model is called layered-
BiLSTM-CRF w/o layered out-of-entities which
uses the input of the current ﬂat NER layer for
out-of-entity words. We name the second model
as layered-BiLSTM-CRF w/o layered LSTM as it
skips all intermediate LSTM layers and only uses
output of embedding layer to build the input for
the next ﬂat NER layer. Please refer to supple-
mental material for the introduced two models.7
To investigate the effectiveness of our model on
different nested levels of entities, we evaluated the
model performance on each ﬂat NER layer on GE-
NIA and ACE2005 test datasets.8
When calcu-
lating precision and recall measurements, we col-
lected the predictions and gold entities from the
corresponding ﬂat NER layer. Since predicted en-
tities on a speciﬁc ﬂat NER layer might be from
other ﬂat NER layers, we deﬁned extended preci-
sion (EP), extended recall (ER) and extended F-
score (EF) to measure the performance. We cal-
culated EP by comparing the predicted entities in
a speciﬁc ﬂat NER layer with all the gold entities,
and ER by comparing the gold entities in a speciﬁc
ﬂat NER layer with all the predicted entities. EF
was calculated in the same way with F.
In addition to experiments on nested GENIA
and ACE2005 datasets, ﬂat entity recognition was
conducted on the JNLPBA dataset. We trained our
ﬂat model that only kept the ﬁrst ﬂat NER layer
and removed the following stacking layers. We
follow the hyper-parameters settings by Lample
et al. (2016) for this evaluation.
7We examined the contributions of predicted labels of the
current ﬂat NER layer to the next ﬂat NER layer. For this, we
introduced label embeddings into each test by combining the
embedding with context representation. Experiments show
that appending label embedding hurts the performance of our
model while gain slight improvements in the rest 2 models on
development datasets.
8We removed entities which were predicted in previous
ﬂat NER layers during evaluation.
4
Results and Analysis
4.1
Nested NER
Table 1 presents the comparisons of our model
with related work including the state-of-the-art
feature-based model by Muis and Lu (2017).
Our model outperforms the state-of-the-art mod-
els with 74.7% and 72.2% in terms of F-score,
achieving the new state-of-the-art in the nested
NER tasks. For GENIA, our model gained more
improvement in terms of recall with enabling ex-
tract more nested entities without reducing pre-
cision.
On ACE2005, we improved recall with
12.2 percentage points and obtained 5.1% rela-
tive error reductions. Compared with GENIA, our
model gained more improvements in ACE2005 in
terms of F-score. Two possible reasons account
for it. One reason is that ACE2005 contains more
deeper nested entities (maximum nested level is 5)
than GENIA (maximum nested level is 3) on the
test dataset. This allows our model to capture the
potentially ‘nested’ relations among nested enti-
ties. The other reason is that ACE2005 has more
nested entities (37.45%) compared with GENIA
(21.56%).
Table 2 shows the results of models on the de-
velopment datasets of GENIA and ACE2005, re-
spectively. From this table, we can see that our
model, which only utilizes context representation
for preparation of input for the next ﬂat NER layer,
performs better than the rest two models.
This
demonstrates that introducing input of the current
ﬂat NER layer such as skipping either representa-
tion for any non-entity or words or all intermedi-
ate LSTM layers hurts performance. Compared
with the layered-BiLSTM-CRF model, the drop
of the performance in the layered-BiLSTM-CRF
w/o layered out-of-entities model reﬂects the skip
of representation for out-of-entity words leads to
the decline in performance. This is because the
representation of non-entity words didn’t incorpo-
rate the current context representation as we used
the input rather than the output to represent them.
By analogy, the layered BiLSTM-CRF w/o layer
LSTM model skips representation for both entities
and non-entity words, resulting in worse perfor-
mance. This is because, when skipping all inter-
mediate LSTM layers, input of the ﬁrst ﬂat NER
layer, i.e., word embeddings, is passed to the re-
maining ﬂat NER layers. Since word embeddings
do not contain context representation, we fail to in-
corporate the context representation when we use
1450

Settings
GENIA
ACE2005
P (%)
R (%)
F (%)
P (%)
R (%)
F (%)
Finkel and Manning (2009)
75.4
65.9
70.3
-
-
-
Lu and Roth (2015)
72.5
65.2
68.7
66.3
59.2
62.5
Muis and Lu (2017)
75.4
66.8
70.8
69.1
58.1
63.1
Our model
78.5
71.3
74.7
74.2
70.3
72.2
Table 1: Comparisons of our model with the state-of-the-art models on nested NER.
Settings
GENIA
ACE2005
P (%)
R (%)
F (%)
P (%)
R (%)
F (%)
Layered-BiLSTM-CRF
78.27
75.97
77.10
75.37
69.41
72.27
Layered-BiLSTM-CRF w/o
layered non-entities
76.55
77.01
76.78
72.90
65.54
69.02
Layered-BiLSTM-CRF w/o
layered LSTM
75.76
74.60
75.18
69.94
61.94
65.70
Table 2: Performances of ablation tests on development datasets.
Entity type
P (%)
R (%)
F (%)
DNA
74.43
69.68
71.98
RNA
90.29
79.48
84.54
Protein
80.48
73.20
76.67
Cell Line
77.83
65.65
71.22
Cell Type
76.36
68.07
71.97
Overall
78.59
71.33
74.79
Table 3: Results of all entities for each type in GENIA
test dataset.
Entity type
P (%)
R (%)
F (%)
PER
78.82
77.37
78.09
LOC
54.54
43.47
48.38
ORG
63.25
54.20
58.38
GPE
76.92
78.98
77.94
VEH
61.53
48.48
54.23
WEA
66.66
53.73
59.50
FAC
49.19
35.26
41.07
Overall
74.27
70.34
72.25
Table 4:
Results of all entities for each type in
ACE2005 test dataset.
the word embeddings as the input for the ﬂat NER
layers. Therefore, we have no chance to take ad-
vantage of the context representation and instead
we only manage to use the word embeddings as
the input for ﬂat NER layers in this case.
Table 3 and Table 4 describe the performance
for each entity type in GENIA and ACE2005 test
datasets, respectively. In GENIA, our model per-
formed best in recognizing entities with type RNA.
This is because most of the entities pertaining
to RNA mainly end up either with “mRNA” or
RNA. These two words are informative indicators
of RNA entities. For entities in rest entity types,
their performances are close to the overall perfor-
mance. One possible reason is that there are many
instances to model them. This also accounts for
the high performances of entity types such as PER,
GPE in ACE2005. The small amounts of instances
of entity types like FAC in ACE2005 is one rea-
son for their under overall performances. We refer
readers to supplemental material for statistics de-
tails.
When evaluating our model on top level which
contains only outermost entities, the precision,
recall and F-score were 78.19%, 75.17% and
76.65% on GENIA test dataset. For ACE2005, the
corresponding precision, recall and F-score were
68.37%, 68.57% and 68.47%. Compared with the
overall performance listed in Table 1, we obtained
higher top level performance on GENIA but lower
performance in ACE2005. We discuss details of
this phenomena in the following tables.
Table 5 shows the performances of each ﬂat
NER layer in GENIA test dataset.
Among all
the stacking ﬂat NER layers, our model resulted
in the best performance regarding standard eval-
uation metrics on the ﬁrst ﬂat NER layer which
contains the predictions for the gold innermost
entities.
When the model went to deeper ﬂat
NER layers, the performance dropped gradually
as the number of gold entities decreased. How-
ever, the performance for predictions on each ﬂat
1451

Layer
P (%)
R (%)
F (%)
EP (%)
ER (%)
EF (%)
#Predictions
#Gold Entities
Layer 1
72.86
69.82
71.31
78.46
71.06
74.57
4,783
4,991
Layer 2
56.88
27.59
37.15
81.15
73.98
77.39
276
569
Layer 3
0.00
0.00
0.00
0.00
60.00
0.00
1
15
Table 5: Results of layer evaluation on GENIA test dataset.
Layer
P (%)
R (%)
F (%)
EP (%)
ER (%)
EF (%)
#Predictions
#Gold Entities
Layer 1
74.46
73.39
73.92
75.84
73.77
74.79
2,894
2,936
Layer 2
60.28
50.49
54.95
66.19
58.41
62.05
423
505
Layer 3
51.02
24.51
33.11
51.02
37.25
43.06
49
102
Layer 4
0.00
0.00
0.00
0.00
10.00
0.00
0
10
Layer 5
0.00
0.00
0.00
0.00
0.00
0.00
0
1
Table 6: Results of layer evaluation on ACE2005 test dataset.
NER layer was different in terms of extended eval-
uation metrics. For the ﬁrst two ﬂat NER layers,
performance of extended evaluation is better than
the performance of standard evaluation. It indi-
cates that gold entities correspond to some of the
predictions on the speciﬁc ﬂat NER layer are from
other ﬂat NER layers. This may lead to the zero
performances for the last ﬂat NER layer. In ad-
dition, performance on the second ﬂat NER layer
was higher than it was on the ﬁrst ﬂat NER layer in
terms of extended F-score. This demonstrates that
our model is able to obtain higher performance on
top level of entities than innermost entities.
Table 6 lists the results of each ﬂat NER layer on
ACE2005 test dataset. Similar to GENIA, the ﬁrst
ﬂat NER layer achieved better performance than
the rest ﬂat NER layers. Performances decreased
in a bottom-to-up manner regarding model archi-
tecture. This phenomena was the same with the
extended evaluation performances, which reﬂects
that some of the predictions in a speciﬁc ﬂat NER
layer were detected in other ﬂat NER layers. Un-
like rising tendency (except last ﬂat NER layer)
regarding extend F-score in GENIA, performance
in ACE2005 was in downtrend. This accounts for
the fact that F-score on top level was lower than it
on the ﬁst ﬂat NER layer. Even though the decline
trend in extended F-score, the ﬁrst ﬂat NER layer
contained the largest proportion of predictions for
the gold entities, the overall performance on all
nested entities showed in Table 1 was still high.
Unlike GENIA, our model in ACE2005 stopped
before reaching the maximum nested level of enti-
ties. It indicates our model failed to model the ap-
propriate nested levels. This is one of the reasons
that account for the zero predictions on the last
ﬂat NER layer. One reason is that our model The
sparse instances on the high nested levels could
be another reason that resulted in the zero perfor-
mances on the last ﬂat NER layer.
4.2
Flat NER
Compared with the state-of-the-art work on
JNLPBA (Gridach, 2017) which achieved 75.87%
in terms of F-score, our model obtained 75.55% in
F-score. Since both the model by Gridach (2017)
and our ﬂat model are based on Lample et al.
(2016), so it is reasonable that both models were
able to get comparable performance.
4.3
Error analysis
We showed the error types and their statistics both
for all nested entities and each ﬂat NER layer
on GENIA and ACE2005 test datasets.
From
ACE2005 test dataset, 28% of predictions were in-
correct in 200 sentences which were selected at
random. Among these errors, 39% of them were
because their text spans were assigned with other
entity types. We call this type of errors type error.
The main reason is that most of them are pronouns
and co-refer to other entities which are absent in
the sentence. Taking this sentence “whether that
is true now, we can not say” as an example, “we”
is annotated as ORG while our model labeled it
as PER. Lack of context information such as the
absence of co-referent entities leads our model to
make the wrong decisions. In addition, 30% of
the errors were caused by that incorrect predic-
tions were predicted as only parts of gold entities
with correct entity types. This error type is re-
ferred to as partial prediction error. This might be
due to these gold entities tend to clauses or inde-
1452

pendent sentences, thus possibly containing many
modiﬁers. For example, in this sentence “A man
who has been to Baghdad many times and can tell
us with great knowledge exactly what it’s going to
be like to ﬁght on those avenues in that sprawl-
ing city of Baghdad - Judy .”, “A man who has
been to Baghdad many times and can tell us with
great knowledge exactly what it’s going to be like
to ﬁght on those avenues in that sprawling city of
Baghdad” is annotated as PER while our model
could only extract “A man who has been to Bagh-
dad many times” and predicted it as PER.
Errors on the ﬁrst ﬂat NER layer, we got 41% in
type error and 11% of partial prediction error, re-
spectively. Apart from this, our model recognized
predictions from other ﬂat NER layers, leading to
5% errors. We deﬁne this error type as layer error.
Unlike the ﬁrst ﬂat NER layer, 26% of errors were
caused by layer error. Additionally, 17% of the er-
rors belong to type error. In particular, 22% errors
were due to the type error. As for the last ﬂat NER
layer, 40% errors were caused by partial prediction
error. The rest errors were different from the men-
tioned error types. One possible reason is that we
have less gold entities to train this ﬂat NER layer
compared with previous ﬂat NER layers. Another
reason might be the error propagation.
Similarly, 200 sentences were randomly se-
lected from GENIA test dataset. We got 20% er-
rors of predictions in the subset. Among these er-
rors, 17% and 24% of errors were separately due
to type error and partial prediction error. In addi-
tion, 24% of the predictions on the ﬁrst ﬂat NER
layer were incorrect. Among them, the top error
types were layer error, partial prediction error and
type error, accounting for 21%, 18% and 13%, re-
spectively. Errors on the second ﬂat NER layer
were mainly caused by type error and the and par-
tial prediction error.
5
Related Work
The success of neural networks has boosted the
performance of ﬂat named NER in different do-
mains (Lample et al., 2016; Ma and Hovy, 2016;
Gridach, 2017; Strubell et al., 2017). Such mod-
els achieved the state of the art without any hand-
crafted features and external knowledge resources.
Contrary to ﬂat NER, much fewer attempts have
emphasized the nested entity recognition. Exist-
ing approaches to nested NER (Shen et al., 2003;
Alex et al., 2007; Finkel and Manning, 2009; Lu
and Roth, 2015; Xu and Jiang, 2016; Muis and Lu,
2017) mainly rely on hand-crafted features. They
also failed to take advantage of the dependencies
among nested entities. Our model enables cap-
turing dependencies and automatic learning high-
level abstract features from texts.
Early work regarding nested NER involve
mainly hybrid systems that combined rules with
supervised learning algorithms.
For example,
Shen et al. (2003), Zhou et al. (2004) and Zhang
et al. (2004) employed a Hidden Markov Model
to GENIA to extract inner entities and then used
rule-based methods to obtain the outer entities.
Furthermore, Gu (2006) extracted nested entities
based on SVM which were trained separately on
both inner entities and outermost entities without
putting the hidden relations between nested enti-
ties into consideration. All these methods failed
to capture the dependencies between nested en-
tities. One trial work is that Alex et al. (2007)
separately built a inside-out and outside-in layered
CRFs which were able to use the current guesses
as the input for next layer. They also cascaded
separate CRFs of each entity type by using output
from previous CRFs as features of current CRFs,
yielding best performance in their work. One of
the main drawbacks in the cascading approach was
that it failed to handle nested entities sharing the
same entity type, which were quite common in
natural languages.
Finkel and Manning (2009) proposed a discrim-
inative constituency tree to represent each sen-
tence where the root node was used for connec-
tion. All entities were treated as phrases and repre-
sented as subtrees following the whole tree struc-
ture. Unlike our linguistic features independent
model, Finkel and Manning (2009) used a CRF-
based approach driven by entity-level features to
detect nested entities
Later on, Lu and Roth (2015) built hyper-graphs
that allow edges to connect multiple nodes to
represent both the nested entities and their refer-
ences (a.k.a. mentions). One issue in their ap-
proach is the spurious structures of hyper-graphs
as they enumerate combinations of nodes, types
and boundaries to represent entities. In addition,
they fail to encode the dependencies among em-
bedded entities using hyper-graphs. In contrast,
our model enables nested entity representation by
merging representation of multiple tokens com-
posed in the entity and considers it as the longer
1453

entity representation. This allows us to represent
outer entities based on inner entity representation,
thus managing to capture the relations between in-
ner and outer entities, and hence overcoming the
spurious entity structure problem.
As an improvement in overcoming spurious
structure issue in Lu and Roth (2015), Muis and
Lu (2017) further incorporated mention separators
along with features to yield better performance on
nested entities. Both Lu and Roth (2015) and Muis
and Lu (2017) rely on hand-crafted features to ex-
tract nested entities without incorporating hidden
dependencies in nested entities. In contrast, we
make the most of dependencies of nested entities
in our model to encourage outer entity recognition
by automatic learning of high-level and abstract
features from sequences.
Shared tasks dealing with nested entities like
SemEval-2007 Task 99 and GermEval-201410
were held in order to advance the state-of-the-
art on this issue.
Additionally, as subtasks in
KBP 201511 and KBP 201612, one of the aims
in tri-lingual Entity Discovery and Linking Track
(EDL) track was extracting nested entities from
textual documents varying from English, Chinese
and Spanish. Following this task, Xu and Jiang
(2016) ﬁrstly developed a new tagging scheme
which is based on ﬁxed-size ordinally-forgetting
encoding (FOFE) method for text fragment rep-
resentation. All the entities along their contexts
were represented using this novel tagging scheme.
Different from the extensively used LSTM-RNNs
in sequence labeling task, a feed-forward neural
network was used to predict labels on entity level
for each fragment in any of given sequences. Ad-
ditionally, Li et al. (2017) used the model pro-
posed in Lample et al. (2016) to the extract both
ﬂat entities and components composed in nested
and discontinuous entities. Another BiLSTM was
applied to combine the components to get nested
and discontinuous entities. However, these meth-
ods failed to capture and utilize the inner entity
representation to facilitate outer entity detection.
9http://nlp.cs.swarthmore.edu/semeval/
tasks/index.php
10https://sites.google.com/site/
germeval2014ner/
11https://tac.nist.gov//2015/KBP/
12https://tac.nist.gov//2016/KBP/
6
Conclusion
This paper presented a dynamic layered model
which takes full advantage of inner entity infor-
mation to encourage outer entity recognition in
an end-to-end manner. Our model is based on a
ﬂat NER layer consisting of LSTM and CRF, so
our model is able to capture context representa-
tion of input sequences and globally decode pre-
dicted labels at a ﬂat NER layer without relying
on feature-engineering. Our model automatically
stacks the ﬂat NER layers with sharing the param-
eters of LSTM and CRF in the layers. The stack-
ing continues until the current ﬂat NER layer pre-
dicts sequences as all outside of entities, which en-
ables stopping dynamically stacked ﬂat NER lay-
ers. Each ﬂat NER layer receives the merged con-
text representation as input for outer entity recog-
nition, based on the predicted entities from the
previous ﬂat NER layer. With this dynamic end-
to-end model, our model is able to outperform
existing models, achieving the-state-of-art on two
nested NER tasks. In addition, the model can be
ﬂexibly simpliﬁed as a ﬂat NER model by remov-
ing components cascaded after the ﬁrst NER layer.
Extensive evaluation shows that utilization of
inner entities signiﬁcantly encourages outer en-
tities detection with improvements of 3.9 and
9.1 percentage points in F-score on GENIA and
ACE2005, respectively. Additionally, utilization
of only current context representation contributes
to the performance improvement than use of con-
text representation from multi-layers.
Acknowledgments
We thank the anonymous reviewers for their
valuable comments.
The ﬁrst author is ﬁnan-
cially supported by the University of Manchesters
2016 Presidents Doctoral Scholar Award. Sophia
Ananiadou acknowledges BBSRC BB/P025684/1
Japan Partnering Award and BB/M006891/1 Em-
pathy.
This research has also been carried out
with funding from AIRC/AIST and results ob-
tained from a project commissioned by the New
Energy and Industrial Technology Development
Organization (NEDO).
References
Beatrice Alex, Barry Haddow, and Claire Grover.
2007. Recognising nested named entities in biomed-
ical text.
In Proceedings of the Workshop on
1454

BioNLP 2007: Biological, Translational, and Clin-
ical Language Processing. Association for Com-
putational Linguistics, Association for Computa-
tional Linguistics, Stroudsburg, PA, USA, pages 65–
72.
http://dl.acm.org/citation.cfm?
id=1572392.1572404.
Balu Bhasuran, Gurusamy Murugesan, Sabenabanu
Abdulkadhar,
and Jeyakumar Natarajan. 2016.
Stacked ensemble combined with fuzzy matching
for biomedical named entity recognition of diseases.
Journal of biomedical informatics 64(Supplement
C):1–9.
https://doi.org/https://doi.
org/10.1016/j.jbi.2016.09.009.
Kate Byrne. 2007.
Nested named entity recog-
nition in historical archive text.
In ICSC.
IEEE
Computer
Society,
pages
589–596.
http://dblp.uni-trier.de/db/conf/
semco/icsc2007.html#Byrne07.
Billy Chiu, Gamal Crichton, Anna Korhonen, and
Sampo Pyysalo. 2016.
How to train good word
embeddings for biomedical NLP.
Proceedings of
the 15th Workshop on Biomedical Natural Lan-
guage Processing pages 166–174. http://www.
aclweb.org/anthology/W16-2922.
HC Cho, N Okazaki, M Miwa, and J Tsujii. 2010.
Nersuite: a named entity recognition toolkit. Tsu-
jii Laboratory, Department of Information Sci-
ence, University of Tokyo, Tokyo, Japan http://
nersuite.nlplab.org/.
Xiaocheng Feng, Lifu Huang, Duyu Tang, Heng Ji,
Bing Qin, and Ting Liu. 2016.
A language-
independent neural network for event detection. In
Proceedings of the 54th Annual Meeting of the
Association for Computational Linguistics (Volume
2: Short Papers). Association for Computational
Linguistics, Berlin, Germany, volume 2, pages
66–71.
http://anthology.aclweb.org/
P16-2011.
Jenny Rose Finkel and Christopher D Manning. 2009.
Nested named entity recognition.
In Proceedings
of the 2009 Conference on Empirical Methods in
Natural Language Processing: Volume 1-Volume
1. Association for Computational Linguistics, As-
sociation for Computational Linguistics, Singapore,
pages 141–150.
http://www.aclweb.org/
anthology/D/D09/D09-1015.
Pavlina Fragkou. 2017. Applying named entity recog-
nition and co-reference resolution for segmenting
english texts.
Progress in Artiﬁcial Intelligence
6(4):325–346. https://doi.org/10.1007/
s13748-017-0127-3.
Christoph Goller and Andreas Kuchler. 1996. Learning
task-dependent distributed representations by back-
propagation through structure. In Neural Networks,
1996., IEEE International Conference on. IEEE,
volume 1, pages 347–352. https://doi.org/
10.1109/ICNN.1996.548916.
Mourad Gridach. 2017. Character-level neural network
for biomedical named entity recognition.
Journal
of biomedical informatics 70:85–91.
https://
doi.org/10.1016/j.jbi.2017.05.002.
Baohua Gu. 2006.
Recognizing nested named enti-
ties in GENIA corpus. In Proceedings of the HLT-
NAACL BioNLP Workshop on Linking Natural Lan-
guage and Biology. Association for Computational
Linguistics, New York, New York, LNLBioNLP
’06, pages 112–113.
http://www.aclweb.
org/anthology/W/W06/W06-3318.
Nitish Gupta, Sameer Singh, and Dan Roth. 2017.
Entity linking via joint encoding of types, de-
scriptions, and context.
In Proceedings of the
2017 Conference on Empirical Methods in Natu-
ral Language Processing. Association for Compu-
tational Linguistics, Copenhagen, Denmark, pages
2671–2680.
https://www.aclweb.org/
anthology/D17-1284.
Geoffrey E Hinton, Nitish Srivastava, Alex Krizhevsky,
Ilya
Sutskever,
and
Ruslan
R
Salakhutdinov.
2012.
Improving neural networks by prevent-
ing co-adaptation of feature detectors.
CoRR
abs/1207.0580.
http://arxiv.org/abs/
1207.0580.
Sepp Hochreiter and J¨urgen Schmidhuber. 1997.
Long short-term memory.
Neural computa-
tion 9(8):1735–1780. https://doi.org/10.
1162/neco.1997.9.8.1735.
Jin-Dong Kim, Tomoko Ohta, Yuka Tateisi, and Ju-
nichi Tsujii. 2003. GENIA corpus a semantically
annotated corpus for bio-textmining. Bioinformat-
ics 19(suppl. 1):i180–i182.
Jin-Dong Kim, Tomoko Ohta, Yoshimasa Tsuruoka,
Yuka Tateisi, and Nigel Collier. 2004.
Introduc-
tion to the bio-entity recognition task at JNLPBA.
In Proceedings of the international joint workshop
on natural language processing in biomedicine and
its applications. Association for Computational Lin-
guistics, pages 70–75.
Diederik Kingma and Jimmy Ba. 2014.
Adam:
A method for stochastic optimization.
CoRR
abs/1412.6980.
https://arxiv.org/abs/
1412.6980.
Guillaume Lample, Miguel Ballesteros, Sandeep Sub-
ramanian, Kazuya Kawakami, and Chris Dyer. 2016.
Neural architectures for named entity recognition.
In Proceedings of the 2016 Conference of the North
American Chapter of the Association for Compu-
tational Linguistics: Human Language Technolo-
gies. Association for Computational Linguistics,
San Diego, California, pages 260–270. http://
www.aclweb.org/anthology/N16-1030.
Robert Leaman and Zhiyong Lu. 2016.
Tag-
gerone:
joint named entity recognition and nor-
malization with semi-markov models. Bioinformat-
ics 32(18):2839–2846. https://doi.org/10.
1093/bioinformatics/btw343.
1455

Robert Leaman,
Chih-Hsuan Wei,
and Zhiyong
Lu. 2015.
tmChem:
a high performance ap-
proach for chemical named entity recognition
and normalization.
Journal of cheminformat-
ics 7(1):S3.
https://doi.org/10.1186/
1758-2946-7-S1-S3.
Fei Li, Meishan Zhang, Bo Tian, Bo Chen, Guo-
hong Fu, and Donghong Ji. 2017.
Recogniz-
ing irregular entities in biomedical text via deep
neural networks.
Pattern Recognition Letters
105:105–113.
https://doi.org/10.1016/
j.patrec.2017.06.009.
Xiao Ling and Daniel S. Weld. 2012.
Fine-
grained entity recognition.
In Proceedings of
the Twenty-Sixth AAAI Conference on Artiﬁcial
Intelligence. AAAI Press, AAAI’12, pages 94–
100. http://dl.acm.org/citation.cfm?
id=2900728.2900742.
Wei Lu and Dan Roth. 2015.
Joint mention extrac-
tion and classiﬁcation with mention hypergraphs. In
Proceedings of the 2015 Conference on Empirical
Methods in Natural Language Processing. Associ-
ation for Computational Linguistics, Lisbon, Por-
tugal, pages 857–867.
http://aclweb.org/
anthology/D15-1102.
Xuezhe Ma and Eduard H. Hovy. 2016. End-to-end
sequence labeling via bi-directional lstm-cnns-crf
pages 1064–1074. http://www.aclweb.org/
anthology/P16-1101.
Christopher D Manning, Mihai Surdeanu, John Bauer,
Jenny Rose Finkel, Steven Bethard, and David Mc-
Closky. 2014.
The stanford corenlp natural lan-
guage processing toolkit. In Proceedings of 52nd
Annual Meeting of the Association for Computa-
tional Linguistics: System Demonstrations. Asso-
ciation for Computational Linguistics, pages 55–
60. http://www.aclweb.org/anthology/
P14-5010.
Michał Marci´nczuk. 2015.
Automatic construc-
tion of complex features in conditional random
ﬁelds for named entities recognition.
In Pro-
ceedings of the International Conference Recent
Advances in Natural Language Processing. IN-
COMA Ltd. Shoumen, BULGARIA, Hissar, Bul-
garia, pages 413–419.
http://www.aclweb.
org/anthology/R15-1054.
Llu´ıs M`arquez, Luis Villarejo, M. A. Mart´ı, and Mar-
iona Taul´e. 2007.
Semeval-2007 task 09: Multi-
level semantic annotation of catalan and spanish.
In Proceedings of the 4th International Workshop
on Semantic Evaluations. Association for Computa-
tional Linguistics, Stroudsburg, PA, USA, SemEval
’07,
pages 42–47.
http://dl.acm.org/
citation.cfm?id=1621474.1621482.
Makoto Miwa and Mohit Bansal. 2016.
End-to-end
relation extraction using LSTMs on sequences and
tree structures pages 1105–1116. http://www.
aclweb.org/anthology/P16-1105.
Aldrian Obaja Muis and Wei Lu. 2017.
Label-
ing gaps between words:
Recognizing overlap-
ping mentions with mention separators.
In Pro-
ceedings of the 2017 Conference on Empirical
Methods in Natural Language Processing. Associ-
ation for Computational Linguistics, Copenhagen,
Denmark, pages 2598–2608.
https://www.
aclweb.org/anthology/D17-1276.
Vardaan Pahuja, Anirban Laha, Shachar Mirkin, Vikas
Raykar, Lili Kotlerman, and Guy Lev. 2017. Joint
learning of correlated sequence labeling tasks us-
ing bidirectional recurrent neural networks pages
548–552.
https://doi.org/10.21437/
Interspeech.2017-1247.
Tim Rockt¨aschel, Michael Weidlich, and Ulf Leser.
2012.
ChemSpot:
a hybrid system for chem-
ical named entity recognition.
Bioinformatics
28(12):1633–1640.
Dan Shen, Jie Zhang, Guodong Zhou, Jian Su, and
Chew-Lim Tan. 2003. Effective adaptation of a hid-
den markov model-based named entity recognizer
for biomedical domain. In Proceedings of the ACL
2003 workshop on Natural language processing in
biomedicine-Volume 13. Association for Computa-
tional Linguistics, Association for Computational
Linguistics, Sapporo, Japan, pages 49–56. https:
//doi.org/10.3115/1118958.1118965.
Jasper Snoek, Hugo Larochelle, and Ryan P Adams.
2012.
Practical bayesian optimization of ma-
chine
learning
algorithms.
In
Advances
in
neural
information
processing
systems.
Cur-
ran
Associates
Inc.,
USA,
pages
2951–2959.
http://dl.acm.org/citation.cfm?id=
2999325.2999464.
M. Stone and R. Arora. 2017.
Identifying nominals
with no head match co-references using deep learn-
ing. CoRR abs/1710.00936. https://arxiv.
org/abs/1710.00936.
Emma Strubell, Patrick Verga, David Belanger, and
Andrew McCallum. 2017.
Fast and accurate en-
tity recognition with iterated dilated convolutions.
In Proceedings of the 2017 Conference on Em-
pirical Methods in Natural Language Processing.
Association for Computational Linguistics, Copen-
hagen, Denmark, pages 2670–2680.
https://
www.aclweb.org/anthology/D17-1283.
Seiya Tokui, Kenta Oono, Shohei Hido, and Justin
Clayton. 2015.
Chainer: a next-generation open
source framework for deep learning. In Proceedings
of workshop on machine learning systems (Learn-
ingSys) in the twenty-ninth annual conference on
neural information processing systems (NIPS). vol-
ume 5.
Christopher Walker, Stephanie Strassel, Julie Medero,
and Kazuaki Maeda. 2006.
ACE 2005 multilin-
gual training corpus. Linguistic Data Consortium,
Philadelphia 57.
1456

Yefeng Wang. 2009.
Annotating and recognising
named entities in clinical notes.
In Proceedings
of the ACL-IJCNLP 2009 Student Research Work-
shop. Association for Computational Linguistics,
Stroudsburg, PA, USA, ACLstudent ’09, pages 18–
26.
http://dl.acm.org/citation.cfm?
id=1667884.1667888.
Paul J Werbos. 1990. Backpropagation through time:
what it does and how to do it. Proceedings of the
IEEE 78(10):1550–1560.
https://doi.org/
10.1109/5.58337.
Mingbin Xu and Hui Jiang. 2016.
A FOFE-based
local detection approach for named entity recog-
nition and mention detection.
arXiv preprint
arXiv:1611.00801 https://arxiv.org/abs/
1611.00801.
Jie Zhang, Dan Shen, Guodong Zhou, Jian Su,
and Chew-Lim Tan. 2004.
Enhancing HMM-
based
biomedical
named
entity
recognition
by
studying
special
phenomena.
Journal
of
biomedical
informatics
37(6):411–422.
https://doi.org/https://doi.org/
10.1016/j.jbi.2004.08.005.
Guodong Zhou, Jie Zhang, Jian Su, Dan Shen, and
Chewlim Tan. 2004. Recognizing names in biomed-
ical texts: a machine learning approach. Bioinfor-
matics 20(7):1178–1190.
https://doi.org/
10.1093/bioinformatics/bth060.
A
Data Statistics and Preprocessing
Statistics of GENIA, ACE2005 and JNLPBA are
described in Tables 7, 8 and 9, respectively.
We used NERSuite (Cho et al., 2010) for
GENIA to perform tokenization while Stanford
CoreNLP (Manning et al., 2014) was used for
ACE2005. The JNLPBA dataset has already been
went through tokenization and sentence splitting,
so we did not apply any preprocessing.
For GENIA, we had to manually revolve the fol-
lowing two issues, in addition to the above prepro-
cessing.
One of the issues we had in this corpus is the re-
moval of discontinuous entities during parsing. In
provided GENIA XML ﬁle, each ﬂat entity is an-
notated with ‘lex’ (lexical) and ‘sem’ (semantics)
attributes while discontinuous and nested entities
may have none, one or two attributes when these
entities embed with each other, making it difﬁ-
cult to extract the strictly nested ones. Taken the
text “recombinant human nm23-H1, -H2, mouse
nm23-M1, and -M2 proteins” as an example,
there are six discontinuous entities, “recombinant
human nm23-H1 protein”, “recombinant human
Item
Train
Dev.
Test
Documents
1,599
189
212
Sentences
15,022
1,669
1,855
Split percentage
81%
9%
10%
DNA
7,921
1061
1,283
RNA
730
140
117
Protein
29,032
2,338
3,098
Cell Line
3,149
340
460
Cell Type
6,021
563
617
Outermost entity
42,462
4,020
4,942
Nested level
4
3
3
Entities in level 1
42,846
4,060
4,991
Entities in level 2
3,910
381
569
Entities in level 3
91
1
15
Entities in level 4
1
0
0
Entity avg. length
2.87
3.13
2.93
Multi-token entity
33951
3554
4203
Overall entities
46,853
4,442
5,575
Table 7: Statistics of GENIA.
Item
Train
Dev.
Test
Documents
370
43
51
Sentences
9,849
1,221
1,478
FAC
924
83
173
GPE
4,725
486
671
LOC
763
81
69
ORG
3,702
479
559
PER
13,050
1,668
1,949
VEH
624
81
66
WEA
652
94
67
Outermost entity
18,455
2,285
2,724
Nested level
6
4
5
Entities in level 1
19,676
2,429
2,936
Entities in level 2
3,934
448
505
Entities in level 3
731
85
102
Entities in level 4
90
10
10
Entities in level 5
7
0
1
Entities in level 6
2
0
0
Entity avg. length
2.28
2.33
2.28
Multi-token entity
10,577
1,323
1,486
Overall entities
24,440
2,972
3,554
Table 8: Statistics of ACE2005.
H2 protein”, “recombinant mouse nm23-M1 pro-
tein”, “recombinant mouse nm23-M2 protein”,
“mouse nm23-M2” and “human nm23-H2”, and
two nested entities, “mouse nm23-M1” and “hu-
man nm23-H1”. We extract these nested entities
based on symbol * appeared ‘lex’ attribute which
1457

Item
Train
Dev.
Test
Sentences
16,691
1,855
3,856
Split percentage
90%
10%
-
DNA
8,649
884
1,056
RNA
863
88
118
Protein
27,263
3,006
5,067
Cell Line
3,459
371
500
Cell Type
6,045
673
1,921
Overall entities
46,279
5,022
8,662
Table 9: Statistics of JNLPBA.
Hyper params
Range
Best
Batch size
[16 – 256]
67
No. of hidden units
200, 250, 300
200
Dim. of char. emb.
[15 – 50]
35
Dropout rate
[0.1 – 0.5]
0.2144
Learning rate
[0.001 – 0.02]
0.00754
Gradient clipping
[5 – 50]
27
Weight decay (L2)
[10-8 – 10-3]
4.54-5
Table 10: Value range and best value of tuned hyper
parameters in GENIA.
Hyper params
Range
Best
Batch size
[16 – 256]
91
No. of hidden units
200, 250, 300
200
Dim. of char. emb.
[15 – 50]
28
Dropout rate
[0.1 – 0.5]
0.1708
Learning rate
[0.001 – 0.02]
0.00426
Gradient clipping
[5 – 50]
11
Weight decay (L2)
[10-8 – 10-3]
9.43-5
Table 11: Value range and best value of tuned hyper
parameters in ACE2005.
Hyper Parameters
Initialized Value
Acquisition Function
gp hedge
n-calls
10
n random state
None
n random starts
10
Acquisition Optimizer
lbfgs
n restarts optimizer
100
noise
gaussian
n points
50000
xi
0.1
n jobs
1
Table 12: Hyper parameters used of Bayesian Opti-
mization.
is an connection indicator of the separated texts
in discontinuous entities. Meanwhile, each of the
separated texts has no ‘sem’ attribute unless itself
is an innermost entity.
Unfortunately, there are
some inconsistent cases such as “c-fos and c-jun
transcripts” where symbol * should be in the ‘lex’
attribute as the discontinuous entity “c-fos tran-
script” is connected by “c-fos” and “transcript”
while “c-jun transcript” is connected by “c-jun”
and “transcript”. These two entities share the same
text “transcript”. However, each of them is anno-
tated with two attributes: ‘lex’ and ‘sem’, follow-
ing the same annotation for ﬂat entities. Although
it is possible to ignore the latter entity based on
‘lex’ attribute and its belonging sentence, this rule
fails to deal with entity “c-jun gene” in the exam-
ple of “c-fos and c-jun genes” as the ‘lex’ of “c-jun
gene” is mistaken as “c-jun genes”. Therefore, in
this case, we ignored “c-fos transcript” and instead
kept the “c-jun transcripts” as a ﬂat entity.
Another issue is the incomplete tokenization.
The label assignment to one word was conducted
on the word-level instead of character level, but
there are entities that correspond to parts of words.
An example is “NF-YA subunit”, which contains
two protein entities: “NF-Y” and “A subunit”. To
cope with this problem, we treat both two entities
as false negative entities in training dataset as there
are only 13 such entities in the training data set.
B
Bayesian Optimization Setting
The hyper-parameters which were tuned for our
model are listed in Table 10 and Table 11. These
hyper-parameters are tuned by Bayesian optimiza-
tion with the hyper parameters listed in Table 12.
C
Model Structure
Figure 4 shows the model architecture when we
skip all intermediate LSTM layers and only word
embeddings are used to produce the input for the
next ﬂat NER layer.
Figure 5 describes the model architecture when
we skip the representation of non-entity words to
prepare the input for the next ﬂat NER layer. Con-
cretely, we merge and average representation fol-
lowing Equation 1. For the predicted non-entity
words, however, we skip the LSTM layer and di-
rectly use their corresponding representation from
the input rather than the output context represen-
tation.
1458

O
B-DNA
I-DNA
I-DNA
I-DNA
O
O
B-protein
O
O
O
O
Mouse
interleukin-2
receptor
alpha
gene
expression
O
B-Protein
O
O
O
O
O
B-DNA
I-DNA
I-DNA
I-DNA
O
O
O
O
Gold 
labels
Embedding layer
Flat NER layer
Flat NER layer
Flat NER layer
Sequence
Inner
Outer
Dropout
Dropout
CRF
CRF
LSTM Unit
LSTM Unit
Label
Label
Word representation
Word representation
Flat NER Unit
Flat NER Unit
O
B-DNA
I-DNA
I-DNA
I-DNA
O
O
B-protein
O
O
O
O
Mouse
interleukin-2
receptor
alpha
gene
expression
O
B-Protein
O
O
O
O
O
B-DNA
I-DNA
I-DNA
I-DNA
O
O
O
O
Gold 
labels
Embedding layer
Flat NER layer
Flat NER layer
Flat NER layer
Sequence
Inner
Outer
Dropout
CRF
LSTM Unit
Label
Word representation
Flat NER Unit
Figure 4: Overview of the model architecture with skipping representation for non-entity words. “interleukin-2”
and “interleukin-2 receptor alpha gene” are nested entities.
O
B-DNA
I-DNA
I-DNA
I-DNA
O
O
B-protein
O
O
O
O
Mouse
interleukin-2
receptor
alpha
gene
expression
O
B-Protein
O
O
O
O
O
B-DNA
I-DNA
I-DNA
I-DNA
O
O
O
O
Gold 
labels
Embedding layer
Flat NER layer
Flat NER layer
Flat NER layer
Sequence
Inner
Outer
Dropout
Dropout
CRF
CRF
LSTM Unit
LSTM Unit
Label
Label
Word representation
Word representation
Flat NER Unit
Flat NER Unit
Figure 5: Overview of the model architecture with skipping representation for whole sequence. “interleukin-2”
and “interleukin-2 receptor alpha gene” are nested entities.
1459

