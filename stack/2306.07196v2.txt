Published as a conference paper at ICLR 2024
RETRIEVAL-ENHANCED CONTRASTIVE VISION-TEXT
MODELS
Ahmet Iscen
Mathilde Caron
Alireza Fathi
Cordelia Schmid
Google Research
ABSTRACT
Contrastive image-text models such as CLIP form the building blocks of many
state-of-the-art systems. While they excel at recognizing common generic concepts,
they still struggle on fine-grained entities which are rare, or even absent from the
pre-training dataset. Hence, a key ingredient to their success has been the use
of large-scale curated pre-training data aiming at expanding the set of concepts
that they can memorize during the pre-training stage. In this work, we explore an
alternative to encoding fine-grained knowledge directly into the model’s parameters:
we instead train the model to retrieve this knowledge from an external memory.
Specifically, we propose to equip existing vision-text models with the ability to
refine their embedding with cross-modal retrieved information from a memory at
inference time, which greatly improves their zero-shot predictions. Remarkably,
we show that this can be done with a light-weight, single-layer, fusion transformer
on top of a frozen CLIP. Our experiments validate that our retrieval-enhanced
contrastive (RECO) training improves CLIP performance substantially on several
challenging fine-grained tasks: for example +10.9 on Stanford Cars, +10.2 on
CUB-2011 and +7.3 on the recent OVEN benchmark, where we even outperform
the fine-tuned models on unseen classes.
1
INTRODUCTION
In the recent years, we have witnessed a surge in the development of vision-language models highly
adaptable to a broad spectrum of downstream tasks (Jia et al., 2021; Radford et al., 2021; Yu et al.,
2022; Chen et al., 2023; Singh et al., 2022). These models work by pre-training two parallel encoders
using contrastive learning (van den Oord et al., 2018) on large-scale, carefully curated, image-text
data (Radford et al., 2021). These two-tower models learn to encode images and texts into an aligned
latent space which enables appealing capabilities such as zero-shot transfer to different downstream
applications, e.g. image classification (Radford et al., 2021), image-text retrieval (Plummer et al.,
2015) or open-world recognition (Minderer et al., 2022; Liang et al., 2022). Although these models
have achieved state-of-the-art results across various generic vision-language benchmarks, we observe
that they tend to struggle on tasks requiring a more fine-grained understanding of visual or textual
entities. Our hypothesis is that this disparity stems from the fact that it is hard to align the image
and text modalities. While every image is metaphorically valued at a thousand words, it is often
paired with a short, sometimes noisy, text that neither exclusively nor comprehensively describes it.
For example, current vision-language models are good at associating images of cars with generic
concepts such as “car”, “mechanics” or “road trip”, because these are common words paired with car
images, but less at finegrained, instance-level, associations such as the specific brand, series or year
of that car. This might therefore produce poor accuracy for zero-shot fine-grained car classification.
The current path taken by the research community has been to ever scale and curate the pre-training
dataset in the hope of covering more and more image-text associations (Radford et al., 2021; Schuh-
mann et al., 2021; Alayrac et al., 2022; Chen et al., 2023). An orthogonal effort has focused instead on
memory or knowledge-based approaches (Long et al., 2022; Hu et al., 2022; Gui et al., 2021; Izacard
et al., 2022; Guu et al., 2020b; Liu et al., 2023; Shen et al., 2022). These methods, instead of statically
ingesting and memorizing all the world knowledge into model parameters, propose to rely on the
access to an external source of knowledge. For example, K-Lite (Shen et al., 2022) explores how to
improve vision-text models by enhancing the text captions with more comprehensive text definitions
1
arXiv:2306.07196v2  [cs.CV]  21 Feb 2024

Published as a conference paper at ICLR 2024
retrieved from an external dictionary, i.e. WordNet (Meyer & Gurevych, 2012) or Wiktionary (Miller,
1998). One caveat that we identify in this approach is that initial captions are augmented within their
modality only, hence limiting the potential added-value brought by the retrieved items.
To mitigate this issue, we put forth a retrieval-augmented approach that enhances the alignment
between image and text representation. A critical observation of ours is that matching representations
within the same modality is a significantly simpler task than matching representations across different
modalities. To clarify, we observe that the image representation can be effectively utilized to identify
images closely resembling the query image, or the text representation can be used to identify texts
closely resembling the query text. However, when crossing modalities, these representations are less
successful in identifying suitable matches, such as finding the text with the closest representation to a
query image representation. We utilize the inherent strength of learned image and text representations
within their respective modalities to aid the alignment across modalities. To improve their compatibil-
ity, we convert these unimodal representations into a multi-modal format, as conceptually illustrated
in Fig. 1. Utilizing a web-scale corpus of image-text pairs for retrieval, we use image representation
as a query to identify the top-k most similar images and incorporate the associated text to create a
multi-modal representation. In a parallel manner, given a text representation as a query, we find the
top-k most similar texts and integrate the associated images to create a multi-modal representation.
Through this process, we successfully transform the image and text representations into multi-modal
versions, which significantly simplifies their alignment. Our approach does not presuppose any
downstream knowledge and produces a single generic model that can be used effectively across
different tasks. We show that our method improves over original CLIP (Radford et al., 2021) or
LiT (Zhai et al., 2022) models on 11 challenging fine-grained downstream tasks.
2
RELATED WORK
Vision-text pre-training. While early works have shown the promise of representation learning from
image-text paired data (Zhang et al., 2022; Gomez et al., 2017; Joulin et al., 2016; Desai & Johnson,
2021), recent popular papers such as CLIP (Radford et al., 2021) and ALIGN (Jia et al., 2021) have
truly unleashed the potential of contrastive image-text pre-training. This paradigm simply works with
two parallel uni-modal encoders that learn to distinguish between aligned and non-aligned image-text
pairs through a cross-modal contrastive objective (van den Oord et al., 2018; Miech et al., 2020).
Appealing properties of these models are simplicity, scalability and great zero-shot performance (Xian
et al., 2018). As a result, vision-text contrastive models now form the basic building blocks of more
powerful foundational models, such as CoCa (Yu et al., 2022), Flamingo (Alayrac et al., 2022),
FLAVA (Singh et al., 2022), and PaLI (Chen et al., 2023) for example. In our work, we enhance
the capabilities of the CLIP model (Radford et al., 2021), by adding a light-weight retrieval module.
Nevertheless, our method is not specific to CLIP and can be applied to any vision-text model.
Knowledge-based vision-text models. Several works have focused on ways of improving upon
different aspects of the contrastive vision-text models, such as their training objectives (Gao et al.,
2022; Zhai et al., 2022; Dong et al., 2022) or through scaling (Cherti et al., 2022; Pham et al., 2021).
Yet, only little exploration has been done on their combination with memory or knowledge-based
techniques (Dwibedi et al., 2021; Banani et al., 2023; Liu et al., 2023; Shen et al., 2022; Fan et al.,
2023). REACT (Liu et al., 2023) retrieves image-text pairs from an external memory in order to
build a training dataset specialized for a specific downstream task. Unlike REACT (Liu et al., 2023),
our work does not require any pre-knowledge about the nature of the downstream task, and is hence
applicable in a full zero-shot transfer. Another key difference is that our model can leverage items
from the memory at inference time, while REACT uses retrieved items to automatically generate a
training set to finetune their model. Closer to our work, K-LITE (Shen et al., 2022) learns vision-text
models by leveraging external sources of knowledge (i.e. WordNet (Meyer & Gurevych, 2012) or
Wiktionary (Miller, 1998)) to complete captions with more descriptive content. Unlike our approach,
the retrieved knowledge is uni-modal (e.g. they complement text with more text) and the external
memory is not used for the image tower. Also using a knowledge-based approach but for image-only
representation learning, NNCLR (Dwibedi et al., 2021) finds the visual nearest-neighbor of each
training image from a memory for contrastive learning. LGSimCLR (Banani et al., 2023) uses the
language guidance to find most similar visual nearest-neighbor. Unlike our work, NNCLR and
LGSimCLR only learn visual representations and use retrieval to enhance their supervision during
training but not at inference.
2

Published as a conference paper at ICLR 2024
Figure 1: RECO works by complementing the frozen representations of pre-trained image-text
encoders (such as CLIP) with knowledge retrieved from an external memory. We use an image
representation as a query to identify the k most similar images and integrate their associated text
embeddings to create a multi-modal representation. Likewise, given a text representation as a query,
we find the top-k most similar texts and incorporate their associated images. The fusion of original
and retrieved embeddings is done by learning a shallow fusion model to produce improved, multi-
modal and knowledge-enhanced versions of the original embeddings. We train for alignment between
the refined embeddings, as well as between the refined and original embeddings.
Retrieval-based methods. The main argument of the retrieval-based methods is that not all the world
knowledge can be compiled into a model’s parameters. Thus, the model should also learn to rely on
items retrieved from an external memory at inference. Retrieval-based methods have shown their
promise in various NLP tasks (Khandelwal et al., 2020; Guu et al., 2020a; Lewis et al., 2020; Wang
et al., 2022; Wu et al., 2022; Borgeaud et al., 2022). More recently, there is an increasing interest in
the computer vision for retrieval-based methods as well (Blattmann et al., 2022; Fürst et al., 2022;
Chen et al., 2022; Long et al., 2022; Hu et al., 2022; Gui et al., 2021; Izacard et al., 2022; Guu et al.,
2020b; Liu et al., 2023; Shen et al., 2022; Iscen et al., 2023). Of particular interest, SuS-X (Udandarao
et al., 2023) shows that by either retrieving similar samples to the query sample from a large data-bank
like LAION can improve zero-shot classification performance of CLIP. Conceptually, SuS-X falls
under the “Cross-modal search and cross-modal fusion” variant explored in this paper (see second
scenario in Fig. 2). RA-CLIP (Xie et al., 2023) enriches the CLIP visual representation by retrieved
image and text. However, their attempt to enrich the text representation degrades the performance,
whereas we show that the retrieved data can also help produce better text representations.
3
METHOD
Our goal is to equip powerful pre-trained vision-language models (such as CLIP) with the ability to
complement their representations with cross-modal knowledge retrieved from an external memory.
We aim to do this without requiring such models to be retrained from scratch, but by simply learning
a light-weight retrieval fusion module on top of them. We emphasize that this work does not propose
a new model or loss but rather a new way of adapting pre-trained models to use relevant retrieved
knowledge at inference time. An overview of our approach, RECO, is shown in Fig. 1.
Preliminaries.
We are given a pre-trained frozen dual-encoder vision-text model f, where v =
fimage(I) is the embedding of image I, and t = ftext(T) is the embedding of text T. We say that
these embeddings are uni-modal since they are obtained purely from a single modality, either image
or text. We assume that image and text embedding spaces are already aligned, meaning that they
have been trained to produce similar representations for matching image-text pairs and dissimilar
representations for non-matching pairs (Radford et al., 2021; Jia et al., 2021; Zhai et al., 2022; van den
Oord et al., 2018). This alignment is usually obtained by minimizing the InfoNCE loss (or contrastive
loss) (van den Oord et al., 2018) between embeddings of different modalities:
LNCE(V, T) = −
n
X
i=1

log
ev⊤
i ti/τ
P
j ev⊤
i tj/τ + log
ev⊤
i ti/τ
P
j ev⊤
j ti/τ

,
(1)
where V (resp. T) is the matrix composed of the n visual (resp. text) embeddings in the minibatch
and τ is the temperature parameter. We propose to augment the text and visual embeddings, i.e. t
and v, with external cross-modal knowledge in order to enhance both their expressiveness and their
cross-modality alignment. In the following of this section, we first detail how we retrieve relevant
3

Published as a conference paper at ICLR 2024
Uni-modal search
Cross-modal search
Uni-modal search
Cross-modal search
& cross-modal fusion
& cross-modal fusion
& uni-modal fusion
& uni-modal fusion
(Ours)
Figure 2: Conceptual comparison of uni-/cross- modal search and uni-/cross- fusion. We illustrate
the different scenarios for an input image I while the scenarios for text input T are shown in Appendix.
cross-modal knowledge based on within-modality search. Second, we present how we learn to fuse
the retrieved information into the original embeddings.
3.1
RETRIEVING CROSS-MODAL EXTERNAL KNOWLEDGE
Memory. We define the external source of knowledge by a memory M = {(Ii, Ti)}M
i=1 of M
image-text pairs. We assume that M is very large and covers a broad coverage of concepts. In
practice, only a small-subset of M is relevant for a given input query. Thus, we only consider the k
most relevant items from M for each input obtained by the nearest neighbour search. We denote by
KNN(v, M) and KNN(t, M) the sets formed by the embeddings of the k most relevant items to the
queries v and t from the memory, where KNN refers to the nearest-neighbour retrieval module.
Cross-modal fusion. Our goal is to augment the text and visual original embeddings with cross-
modal knowledge, not necessarily learned during the pre-training stage. For example, given the
class name Yellow bellied flycatcher in a fine-grained bird classification problem such as CUB (Wah
et al., 2011), we first look for captions in the memory that are semantically similar to the given class
name. We then augment the class name representation with the visual representations of the retrieved
similar captions, i.e. with what an Yellow bellied flycatcher looks like. Likewise, given a visual
representation of a bird, we look for similar images in M and use their corresponding captions in the
hope that some of them might contain useful information for our problem such as the species of that
bird. Specifically, for a given text or image input, the retrieval module KNN(., M) returns items with
the opposite modality than that of the input. We use the subscripts v or t to specify the modality of
the retrieved embeddings. That is, KNNt(v, M) returns text embeddings from an image input and
KNNv(t, M) returns image embeddings for text input.
Note that we also evaluate uni-modal fusion in our experiments, i.e. complementing visual repre-
sentations with the retrieved visual knowledge and text representation with the retrieved captions.
However, we find in practice that this variant leads to poorer performance than cross-modal fusion, as
shown in Tab. 3. Intuitively, we hypothesize that this is because the signal brought by cross-modal
fusion is richer due to the complementarity of the different modalities (Iscen et al., 2023).
Uni-modal search. We choose to search relevant items in the memory M based on within-modality
similarities, which we refer to as “uni-modal search” as opposed to “cross-modal search”. Specifically,
we use text-to-text similarity (t →t) to identify suitable content from a text embedding t and image-
to-image similarity (v →v) to retrieve relevant matches from a visual embedding v. Formally, let us
denote by VM and TM all the image and text embeddings from M given by our pretrained vision-
text model f, i.e. we have VM = [fimage(I1), . . . , fimage(IM)] and TM = [ftext(T1), . . . , ftext(TM)].
The retrieval module is hence finally denoted as KNNv→v
t
(v, M) = TM
NN(v;VM), i.e. for an input
image embedding v, the k-NN search is done between v and VM, but the corresponding k-NN
indices from the text embeddings TM are selected. Similarly, we denote the retrieval process as
KNNt→t
v
(t, M) = VM
NN(t;TM) for an input text embedding t.
4

Published as a conference paper at ICLR 2024
We also evaluate cross-modal search but find that this leads to much poorer performance, especially
in fine-grained problems, as shown in Tab. 3. An explanation is that the uni-modal search is an easier
task, hence the retrieved elements are more relevant (because more similar) to the input. On the other
hand, cross-modal search suffers from the pre-trained CLIP model’s lack of fine-grained alignment
between the different modalities, resulting in noisier retrieval. Note that another advantage of uni-
versus cross- modal search is that the latter requires the pre-trained image and text encoders to be
already aligned while we can potentially let go of this hypothesis with uni-modal search.
3.2
LEARNING HOW TO FUSE THE RETRIEVED KNOWLEDGE
Our goal is to refine the original image and text embeddings v and t with the cross-modal knowledge
gathered from M. We denote these refined image and text embeddings by v and t, defined as
v = ϕimage(v, KNNv→v
t
(v, M)) and t = ϕtext(t, KNNt→t
v
(t, M)), where ϕ is the fusion model.
Transformer fusion. We model ϕimage and ϕtext as one-layer multi-head self-attention transformer
encoders (Vaswani et al., 2017; Dosovitskiy et al., 2021). Intuitively, this choice allows the original
embedding to attend to all the retrieved elements in the fusion process. Note that while the fusion
models for text and image encoders have identical architectures, they do not share parameters. In
practice, the fusion module has a total of 3.16M parameters, which corresponds to only 2% of the
total parameter count when using CLIP-B/32 as the backbone f. We have experimented with bigger
fusion modules (see Appendix) but find that this light-weight solution works well in practice. We
have also tried mean fusion of retrieved and original elements by simply averaging their embeddings
but have found in practice that it performs poorly (see Tab. 3). Intuitively, the model needs to learn
how to incorporate this new information, by, for example, learning how to omit or enhance some of
the retrieved elements.
Learning. We train the fusion model ϕ on a dataset D = {(Ii, Ti)}N
i=1 by performing retrieval
at training time from the memory M. The pre-trained encoder f is kept frozen. We minimize
the alignment loss between the refined embeddings which formally amounts to minimizing the
InfoNCE loss of Eq. (1) with the refined embeddings instead of original embeddings, i.e. minimizing
LNCE(V, T). We find that it is also sometimes beneficial to perform retrieval for only one of the
branches (text or image) at inference time depending on the nature of the downstream task (see
Tab. 4). Therefore, we also align the original and refined embeddings by minimizing the following
“cross” loss terms: LNCE(V, T) and LNCE(V, T). This allows to disable one of branches at inference
time, since refined and original embeddings are now also aligned. Overall, we minimize:
L = LNCE(V, T) + LNCE(V, T) + LNCE(V, T).
(2)
4
EXPERIMENTS
4.1
EXPERIMENTAL SETUP
Training details. We train the fusion model on top of a frozen CLIP (-B/32 or -L/14 version)
model (Radford et al., 2021). We also present a variant of RECO on top of a frozen LiT-L16L (Zhai
et al., 2022) model. We train on Conceptual Captions 12M (“CC12M”) (Changpinyo et al., 2021),
an image-text dataset containing about 10M pairs. We use a batch size of 4096, learning rate of
1e−3 decayed with a cosine schedule and weight decay of 1e−5. The temperature parameter is
learned (Radford et al., 2021). Training is done for 10 epochs, which lasts about 10 hours on a
4x4 TPUv2 pod. For the memory, we use the subset of WebLI (Chen et al., 2023) containing 1B
image-text pairs. We remove the near-duplicates of the test images from the memory. We have also
explored using smaller but publicly available memory such as LAION-400M dataset (Schuhmann
et al., 2021) and show the results in Appendix.
Evaluation datasets. We consider the following six image classification datasets: Stanford Cars
(“Cars”) (Krause et al., 2013), CUB-200-2011 (“CUB”) (Wah et al., 2011), Oxford Flowers (“Flow-
ers”) (Nilsback & Zisserman, 2008), ImageNet-1k (“Im1k”) (Russakovsky et al., 2015), Places365
(“Pl365”) (Zhou et al., 2017) and Stanford Dogs (“Dogs”) (Khosla et al., 2011). We also consider
the recent Open-domain visual entity recognition (OVEN) benchmark (Hu et al., 2023), containing
729K test images possibly belonging to 6M entity candidates. Finally, we also report performance on
text-to-image (“T→I”) and image-to-text (“I→T”) retrieval on Flickr30k (“Flickr”) (Plummer et al.,
2015) and MS COCO (“COCO”) (Lin et al., 2014) in Appendix. More details about these datasets
can be found in Appendix or in their corresponding publication.
5

Published as a conference paper at ICLR 2024
Table 1: Zero-shot transfer to image classification. We report top-1 accuracy for classification.
We show the improvements obtained with RECO on top of CLIP-R50, CLIP-B/32, CLIP-L/14 and
LiT-L16L: absolute performance gains are between brackets. For reference, we also include the
performance of K-Lite (Shen et al., 2022) and RA-CLIP (Xie et al., 2023) (other retrieval-augmented
methods) and other image-text models (Align-base (Jia et al., 2021) and PaLI-17B (Chen et al.,
2023)). We also report the total parameter count (“# par.”) of the different models (in Million).
Method
# par. Cars
CUB
Flowers Im1k
Pl365
Dogs
CLIP-R-50
102
38.6
52.0
47.2
59.2
53.1
60.6
+ RECO
114
39.8(+1.2) 62.8(+10.8) 56.2(+9.0) 59.4(+0.2) 54.0(+0.9) 64.4(+3.8)
CLIP-B/32
151
57.2
52.8
62.1
63.5
40.6
58.6
+ RECO
154
68.1(+10.9) 63.0(+10.2) 67.9(+5.8) 64.6(+1.1) 42.2(+1.6) 59.7(+1.1)
CLIP-L/14
428
75.6
61.7
75.6
75.5
42.0
72.7
+ RECO
435
82.8(+7.2) 73.4(+11.7) 79.5(+3.9) 76.1(+0.6) 43.6(+1.6) 73.9(+1.2)
LiT-L16L
638
90.5
54.5
77.4
80.2
45.2
75.7
+ RECO
652
90.8(+0.3) 74.8(+20.3) 84.1(+6.7) 80.9(+0.7) 45.4(+0.2) 81.3(+5.8)
Other approaches
K-Lite
151
10.0
–
78.6
52.3
–
–
RA-CLIP
151
–
–
–
53.5
–
26.1
Align
247
78.7
38.2
64.9
67.6
44.0
56.3
PaLI
17,000 –
–
–
72.1
–
–
Table 2: Zero-shot performance on OVEN. We report top-1 accuracy on seen and unseen categories
and their harmonic mean. We also indicate the total number of parameters of each model (“# params”).
Method
# params (M)
Seen
Unseen
Harmonic mean
Zero-shot
PaLI-17B (Chen et al., 2023)
17,000
4.4
1.2
1.9
CLIP-L/14 (Radford et al., 2021)
428
5.6
4.9
5.3
CLIP-L/14 (Radford et al., 2021) + RECO (Ours)
435
11.5 (+5.9)
13.3 (+8.4)
12.3 (+7.0)
Fine-tuning on the OVEN Seen categories
CLIP-L/14 Fusion (Hu et al., 2023)
880
33.6
4.8
8.4
PaLI-3B (Chen et al., 2023)
3,000
19.1
6.0
9.3
CLIP-L/14 CLIP2CLIP (Hu et al., 2023)
860
12.6
10.5
11.5
PaLI-17B (Chen et al., 2023)
17,000
28.3
11.2
16.1
Evaluation protocol. We evaluate in the zero-shot setting for all the considered benchmarks, meaning
that no adaptation is done to the downstream task. As common in the literature (Radford et al., 2021;
Jia et al., 2021; Singh et al., 2022; Zhai et al., 2022), we add prompts to the text of the downstream
tasks, following (Zhai et al., 2022). All evaluation protocols are in Appendix.
4.2
ZERO-SHOT TRANSFER
Image classification. In Tab. 1, we observe that RECO boosts the zero-shot performance of CLIP
and LiT on zero-shot image classification with large improvements especially on the fine-grained
datasets. For example, we improve the original CLIP-B/32 accuracy by +10.9 on Cars, +10.2 on
CUB and +5.8 on Flowers. The performance is also improved on less fine-grained benchmarks
such as ImageNet or Places, though by more moderate margins (i.e. respectively +1.1 and +1.6).
Secondly, we see in Tab. 1 that the performance gains are consistent across all vision-text backbones
(CLIP-R-50, CLIP-B/32, CLIP-L/14, and LiT-L16L). Note that LiT-L16L is pre-trained on Webli,
which is our memory bank, and we still observe the benefits of RECO. For reference, we also report
in Tab. 1 the numbers from other popular vision-text approaches (Jia et al., 2021; Chen et al., 2023).
Overall, the experiment in Tab. 1 confirms our initial motivation that retrieval from an external
memory improves zero-shot recognition tasks, especially in fine-grained settings.
Open-domain visual entity recognition (OVEN). In Tab. 2, we show the zero-shot performance of
RECO on the OVEN benchmark. We see that our method improves greatly over CLIP-L/14 on this
challenging task, with an impressive relative improvement of +132%. Note that we do not train or
fine-tune our model on the OVEN training set. Remarkably, we observe in Tab. 2 that RECO also
6

Published as a conference paper at ICLR 2024
Table 3: Uni-modal search for cross-modal fusion. We report top-1 accuracy for zero-shot image
classification. We evaluate the impact of uni-modal versus cross-modal search and uni-modal versus
cross-modal fusion. These different mechanisms are conceptually illustrated in Fig. 2. We report
absolute improvement between brackets and the average relative improvement over not using retrieval
(i.e. CLIP performance) in the last row (“Avg. rel. ∆”).
Search
Fusion
Cars
CUB
Flowers
Im1k
Pl365
Avg. rel. ∆
–
–
57.2
52.8
62.1
63.5
40.6
–
ϕ = Transformer fusion
1 Uni-modal
Cross-modal
68.1 (+10.9)
63.0 (+10.2)
67.9 (+5.8)
64.6 (+1.1)
42.5 (+1.9)
+ 9.0 %
2 Cross-modal Cross-modal
56.6 (-0.6)
53.8 (+1.0)
64.3 (+2.2)
64.3 (+0.8)
42.4 (+1.8)
+ 1.7 %
3 Uni-modal
Uni-modal
57.3 (+0.1)
51.2 (-1.6)
62.2 (+0.1)
62.1 (-1.4)
41.7 (+1.1)
−0.4 %
4 Cross-modal Uni-modal
54.0 (-3.2)
50.7 (-2.1)
61.4 (-0.7)
62.3 (-1.2)
41.2 (+0.6)
−1.9 %
ϕ = Mean fusion
5 Uni-modal
Cross-modal
46.9 (-10.3)
44.9 (-7.9)
50.5 (-11.6)
40.1 (-23.4)
23.7 (-16.9)
−21.7 %
6 Cross-modal Cross-modal
43.7 (-13.5)
45.3 (-7.5)
58.7 (-3.4)
55.2 (-8.3)
32.7 (-7.9)
−11.0 %
7 Uni-modal
Uni-modal
44.0 (-13.2)
47.2 (-5.6)
61.3 (-0.8)
55.1 (-8.4)
36.2 (-4.4)
−9.8 %
8 Cross-modal Uni-modal
33.4 (-23.8)
30.2 (-22.6)
38.9 (-23.2)
40.0 (-23.5)
24.7 (-15.9)
−33.0 %
significantly outperforms much bigger models which are directly fine-tuned for this task, for example
CLIP2CLIP (Hu et al., 2023) or PaLI-3B (Chen et al., 2023) while using respectively 2 × and 7 ×
less parameters. It even comes close to the performance of PaLI-17B while being 39 × smaller and
not using any fine-tuning.
4.3
DESIGN CHOICE ANALYSES
In this section, we validate several components of our model, namely the uni-modal search and
cross-modal fusion, training of the fusion module and the number of retrieved elements from the
memory. We also propose some qualitative examples to help understanding why RECO improves
over CLIP performance. We use ViT-CLIP-B/32 throughout this section.
Uni-modal search and cross-modal fusion. In Tab. 3, we evaluate different alternatives for our
method, namely (i) performing cross-modal search in the memory instead of uni-modal search and
(ii) fusing uni-modal items (i.e. combining text with text and image with image) instead of cross-
modal fusion. These different scenarios (uni- versus cross- modal search and fusion) are detailed
in Section 3.1 and conceptually illustrated in Fig. 2. Firstly, we observe in Tab. 3 that uni-modal
search (row 1) leads to a better performance compared to cross-modal search (row 2), with +9.0
versus +1.7 average relative improvement over CLIP. We remark that the gap is especially important
for fine-grained datasets such as Cars, CUB and Flowers. This agrees with our hypothesis that
cross-modal search suffers from the pre-trained CLIP model’s lack of fine-grained alignment between
different modalities. By contrast, using the inherent strength of image and text representations within
their respective modalities allows to retrieve relevant matches, as qualitatively observed in Fig. 4.
Secondly, we observe in Tab. 3 that uni-modal fusion (rows 3 and 4) works substantially worse than
cross-modal fusion (rows 1 and 2). Indeed, we see that augmenting text embeddings with other text
embeddings and image embeddings with other image embeddings does not bring any significant
improvement over the baseline, and even tends to hurt the performance. Intuitively, a possible
explanation is that cross-modal fusion allows us to inject complementary signal into the original
embeddings (Iscen et al., 2023). By contrast, uni-modal provides signal that is already similar to
the input, hence not as much additional information. Finally, we see in Tab. 3 that all the variants
(rows 5, 6, 7 and 8) fail when simply averaging retrieved and original embeddings instead of learning
the fusion with a transformer. This highlights the importance of learning to incorporate the retrieved
items to the original embeddings before deploying the model at inference.
Image and text retrieval fusion modules. In Tab. 4, we compare models trained to fuse only text
original embeddings (row 1), only image original embeddings (row 2) or both (row 3). We observe
that while models trained to fuse only image or text perform reasonably well on some benchmarks,
they typically lag behind on other benchmarks. For example, the model trained for only image fusion
(row 2) is strong on zero-shot Dogs benchmark but behind on CUB and COCO. Secondly, as shown
in Tab. 4, unlike the vision-only or text-only variants, our model can be used in different modes at
inference time in a flexible manner. Indeed, because we have trained it to align the refined embeddings
7

Published as a conference paper at ICLR 2024
Table 4: Image and text retrieval fusion modules. We report zero-shot top-1 accuracy for image
classification and recall@1 for image retrieval. We compare models trained only for text fusion
(row 1), image fusion (row 2) or both (row 3). Our model can be used in different modes at inference:
retrieval only for image (v), retrieval only for text (t) or retrieval for both image and text (v&t).
CUB
Dogs
COCO T→I
ϕimage ϕtext
fusion training loss
v
t
v&t Best
v
t
v&t Best
v
t
v&t Best
1
✓
LNCE(V, T)
✗59.3
✗
59.3
✗59.6
✗
59.6
✗33.3
✗
33.3
2 ✓
LNCE(V, T)
59.7 ✗
✗
59.7 59.2 ✗
✗
59.2 31.3 ✗
✗
31.3
3 ✓
✓
LNCE(V, T)+LNCE(V, T)+LNCE(V, T) 60.0 58.4 63.0 63.0 59.7 59.7 59.4 59.7 31.9 33.6 31.7 33.6
Method
Train data
CUB
CLIP-B/32
–
52.8
+ CLIP-style CC12M
44.8
+ CLIP-style CC12M + RWebli 46.8
+ RECO
CC12M + RWebli 63.0
110 30 50 70
100
% of memory (training)
52
56
60
64
CUB
train memory
full memory
1
5
10
20
k retrieved items (training)
52
56
60
64
k'=20
k'=10
k'=5
k'=2
k'=1
Figure 3: (left) Disantangling the effect of additional training and RECO. (middle) Effect of
updating the memory after training. (right) Effect of the number k of retrieved elements. We
report zero-shot top-1 accuracy on CUB. The CLIP baseline is shown with symbol 8.
with the original ones (see the cross terms in the loss 2), we can choose to disable the retrieval for one
of the branches at inference time, depending on the task. Therefore, we compare in Tab. 4 different
options at inference time: using retrieval only for the image input, only for the text input or for
both of them, denoted respectively by v, t and v&t. We observe in Tab. 4 that depending on the
nature of the task, one of these options might be preferable over the others. For example, for image
classification we see that augmenting the image embeddings with retrieved text has more positive
impact than augmenting the text embeddings, though the best of performance is obtained with both.
On the other hand, text and image retrievals seem to benefit more from augmenting the text rather
than the image side. This intuitively can be explained by the fact that text descriptions in retrieval
benchmarks are typically highly specific compared to the class names in image classification and so
augmenting with visual examples of what they refer to greatly helps the alignment. We demonstrate
qualitative examples of this hypothesis in Appendix. Overall, at inference time, one can choose the
best inference mode for a particular downstream task by validation on a held-out set.
Is the performance boost merely due to additional training? We replace RECO with an MLP layer
of the same capacity initialized from scratch. We train it in a CLIP-style manner on the subset of Webli
that we use when training RECO. We denote this subset by RWebli: it contains the k = 10 nearest-
neighbors for each CC12M datapoint retrieved from the Webli dataset, and contains 61M examples.
We observe in Fig. 3 (left) that training an extra layer on top of CLIP does not bring any gains and
even deteriorates its performance. Indeed, CLIP was extensively trained on a large dataset (Radford
et al., 2021) and additional training on a relatively small dataset deteriorates the general-purpose
property of its representations. Overall, this experiment validates that the performance gains are due
to our method and not to training an additional layer on top of CLIP.
Updating the memory after training. A clear advantage of the retrieval-based models is that the
external memory can be updated with additional, and more contemporary information. We evaluate
the effectiveness of RECO when using a larger memory that is not observed during the training. We
first create various random subsets of Webli by randomly removing a percentage of data. Then, we
train separate RECO models with each Webli subset as its memory. At inference, we evaluate each
RECO model either with the subset of memory that it was trained with, or the full Webli memory.
Results are shown in Fig. 3 (center). We observe that training and evaluating RECO with only 1%
of Webli as the memory does not show improvements compared to the CLIP baseline. However,
we observe a significant improvement when evaluating the same model with full Webli memory at
inference. This confirms that RECO is capable of utilizing an updated memory without re-training.
Effect of the number of retrieved elements. In Fig. 3 (right), we study the effect of the number of
retrieved elements in the memory. We evaluate different numbers of k-NN during the training and
inference time, i.e. we train our model with k items from the memory but use k′ at inference. We
8

Published as a conference paper at ICLR 2024
Query
Uni-modal search (Ours)
Cross-modal search
Figure 4: Qualitative examples on CUB and Cars datasets. We compare uni- versus cross- modal
search for two image queries (top) and two text queries (bottom). Uni-modal search allows to find
more suitable matches to the query, which improves the relevancy of the fused elements. We frame in
red (resp. green) the unrelevant (resp. relevant) retrieved items to be fused with the query.
see in Fig. 3 (right) that RECO generally obtains a higher performance when k′ > k at inference.
Interestingly, the performance saturates after k = 10. An explanation is that increasing the number of
retrieved elements goes with a reduction of the relevancy of the retrieved items.
Qualitative study. In Fig. 4, we provide illustrative examples of why RECO can be useful for
fine-grained image classification on CUB or Cars datasets. We compare our method with a variant
using cross-modal search instead of uni-modal search to illustrate the importance of using the inherent
strength of image-only and text-only representations. We observe in Fig. 4 that uni-modal search
allows to retrieve better matches for the query. This is because image-to-image or text-to-text search
retrieves more similar items to the query than crossing modalities. As a result, retrieved items are
more accurate, which leads to a higher accuracy for fine-grained tasks.
Limitations. A limitation of this work is that it assumes to have access to a large and rich source of
image-text pairs knowledge. While we show in Appendix that public datasets , e.g. LAION (Schuh-
mann et al., 2021), can serve this purpose, the best of performance is obtained with a large private
memory. Alternatively, one could use search engine APIs as the memory. Another limitation is that
the performance gains of RECO come at the cost of increased inference time. In practice, we use
a highly-optimized approximate k-NN algorithm (Guo et al., 2020). It takes about 14ms to query
Webli (956M examples) with a single 512-d ViT-B/32 CLIP embedding. Using retrieval at inference
time incurs an overhead of 25% compared to not using any retrieval at inference time (e.g. baseline
CLIP model), but improves the accuracy by up to 10.9.
5
CONCLUSION
In this paper, we introduce RECO, a method that enhances the fine-grained recognition capabilities of
pre-trained vision-text models. Our approach shows the importance of uni-modal retrieval, yet cross-
modal fusion for image and text inputs. We show that RECO consistently improves the performance
on 11 zero-shot tasks and that the gains are especially important in challenging fine-grained tasks.
9

Published as a conference paper at ICLR 2024
REFERENCES
Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel
Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language
model for few-shot learning. Advances in Neural Information Processing Systems, 35:23716–
23736, 2022.
Mohamed El Banani, Karan Desai, and Justin Johnson. Learning visual representations via language-
guided sampling. arXiv preprint arXiv:2302.12248, 2023.
Andreas Blattmann, Robin Rombach, Kaan Oktay, Jonas Müller, and Björn Ommer. Semi-parametric
neural image synthesis. arXiv preprint arXiv:2204.11824, 2022.
Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican,
George Bm Van Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, et al. Im-
proving language models by retrieving from trillions of tokens. In Proceedings of the International
Conference on Machine Learning (ICML), 2022.
Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool. Food-101–mining discriminative com-
ponents with random forests. In Proceedings of the European Conference on Computer Vision
(ECCV), 2014.
Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual 12m: Pushing
web-scale image-text pre-training to recognize long-tail visual concepts. In Proceedings of the
Conference on Computer Vision and Pattern Recognition (CVPR), 2021.
Wenhu Chen, Hexiang Hu, Chitwan Saharia, and William W Cohen. Re-imagen: Retrieval-augmented
text-to-image generator. arXiv preprint arXiv:2209.14491, 2022.
Xi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergiovanni, Piotr Padlewski, Daniel Salz, Sebastian
Goodman, Adam Grycner, Basil Mustafa, Lucas Beyer, et al. Pali: A jointly-scaled multilingual
language-image model. International Conference on Learning Representations (ICLR), 2023.
Mehdi Cherti, Romain Beaumont, Ross Wightman, Mitchell Wortsman, Gabriel Ilharco, Cade
Gordon, Christoph Schuhmann, Ludwig Schmidt, and Jenia Jitsev. Reproducible scaling laws for
contrastive language-image learning. arXiv preprint arXiv:2212.07143, 2022.
Terrance De Vries, Ishan Misra, Changhan Wang, and Laurens Van der Maaten. Does object
recognition work for everyone? In Proceedings of the Conference on Computer Vision and Pattern
Recognition (CVPR), 2019.
Karan Desai and Justin Johnson. Virtex: Learning visual representations from textual annotations. In
Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR), 2021.
Xiaoyi Dong, Yinglin Zheng, Jianmin Bao, Ting Zhang, Dongdong Chen, Hao Yang, Ming Zeng,
Weiming Zhang, Lu Yuan, Dong Chen, et al.
Maskclip: Masked self-distillation advances
contrastive language-image pretraining. arXiv preprint arXiv:2208.12262, 2022.
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas
Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit,
and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale.
In International Conference on Learning Representations (ICLR), 2021.
Debidatta Dwibedi, Yusuf Aytar, Jonathan Tompson, Pierre Sermanet, and Andrew Zisserman. With
a little help from my friends: Nearest-neighbor contrastive learning of visual representations. In
Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR), 2021.
Lijie Fan, Dilip Krishnan, Phillip Isola, Dina Katabi, and Yonglong Tian. Improving clip training
with language rewrites. arXiv preprint arXiv:2305.20088, 2023.
Andreas Fürst, Elisabeth Rumetshofer, Johannes Lehner, Viet T Tran, Fei Tang, Hubert Ramsauer,
David Kreil, Michael Kopp, Günter Klambauer, Angela Bitto, et al. Cloob: Modern hopfield
networks with infoloob outperform clip. Advances in Neural Information Processing Systems
(NeurIPS), 2022.
10

Published as a conference paper at ICLR 2024
Yuting Gao, Jinfeng Liu, Zihan Xu, Jun Zhang, Ke Li, Rongrong Ji, and Chunhua Shen. Pyramid-
clip: Hierarchical feature alignment for vision-language model pretraining. Advances in Neural
Information Processing Systems (NeurIPS), 2022.
Gerry. Sports100: 100 sports image classification. https://www.kaggle.com/datasets/
gpiosenka/sports-classification/metadata, 2021. Accessed: 2023-05-23.
Lluis Gomez, Yash Patel, Marçal Rusinol, Dimosthenis Karatzas, and CV Jawahar. Self-supervised
learning of visual features through embedding images into text topic spaces. In Proceedings of the
Conference on Computer Vision and Pattern Recognition (CVPR), 2017.
Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the v in vqa
matter: Elevating the role of image understanding in visual question answering. In Proceedings of
the Conference on Computer Vision and Pattern Recognition (CVPR), 2017.
Liangke Gui, Borui Wang, Qiuyuan Huang, Alex Hauptmann, Yonatan Bisk, and Jianfeng Gao. Kat:
A knowledge augmented transformer for vision-and-language. arXiv preprint arXiv:2112.08614,
2021.
Ruiqi Guo, Philip Sun, Erik Lindgren, Quan Geng, David Simcha, Felix Chern, and Sanjiv Kumar.
Accelerating large-scale inference with anisotropic vector quantization. In Proceedings of the
International Conference on Machine Learning (ICML), 2020.
Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei Chang. REALM: Retrieval
augmented language model pre-training. In Proceedings of the International Conference on
Machine Learning (ICML), 2020a.
Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei Chang. Retrieval augmented
language model pre-training. In Proceedings of the International Conference on Machine Learning
(ICML), 2020b.
Hexiang Hu, Yi Luan, Yang Chen, Urvashi Khandelwal, Mandar Joshi, Kenton Lee, Kristina
Toutanova, and Ming-Wei Chang. Open-domain visual entity recognition: Towards recognizing
millions of wikipedia entities. Proceedings of the International Conference on Computer Vision
(ICCV), 2023.
Ziniu Hu, Ahmet Iscen, Chen Sun, Zirui Wang, Kai-Wei Chang, Yizhou Sun, Cordelia Schmid,
David A Ross, and Alireza Fathi. Reveal: Retrieval-augmented visual-language pre-training with
multi-source multimodal knowledge memory. arXiv preprint arXiv:2212.05221, 2022.
Ahmet Iscen, Alireza Fathi, and Cordelia Schmid. Improving image recognition by retrieving from
web-scale image-text data. arXiv preprint arXiv:2304.05173, 2023.
Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane
Dwivedi-Yu, Armand Joulin, Sebastian Riedel, and Edouard Grave. Few-shot learning with
retrieval augmented language models. arXiv preprint arXiv:2208.03299, 2022.
Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan
Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning
with noisy text supervision. In Proceedings of the International Conference on Machine Learning
(ICML), 2021.
Armand Joulin, Laurens Van Der Maaten, Allan Jabri, and Nicolas Vasilache. Learning visual features
from large weakly supervised data. In Proceedings of the European Conference on Computer
Vision (ECCV), 2016.
Andrej Karpathy and Li Fei-Fei. Deep visual-semantic alignments for generating image descriptions.
In Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR), 2015.
Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. Generalization
through memorization: Nearest neighbor language models. International Conference on Learning
Representations (ICLR), 2020.
11

Published as a conference paper at ICLR 2024
Aditya Khosla, Nityananda Jayadevaprakash, Bangpeng Yao, and Li Fei-Fei. Novel dataset for
fine-grained image categorization. In First Workshop on Fine-Grained Visual Categorization,
CVPR, 2011.
Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations for fine-grained
categorization. In Proceedings of the International Conference on Computer Vision (ICCV), 2013.
Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen,
Yannis Kalantidis, Li-Jia Li, David A Shamma, et al. Visual genome: Connecting language and
vision using crowdsourced dense image annotations. International Journal of Computer Vision,
123, 2017.
Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal,
Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al. Retrieval-augmented gen-
eration for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems
(NeurIPS), 2020.
Feng Liang, Bichen Wu, Xiaoliang Dai, Kunpeng Li, Yinan Zhao, Hang Zhang, Peizhao Zhang, Peter
Vajda, and Diana Marculescu. Open-vocabulary semantic segmentation with mask-adapted clip.
arXiv preprint arXiv:2210.04150, 2022.
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr
Dollár, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Proceedings of
the European Conference on Computer Vision (ECCV), 2014.
Haotian Liu, Kilho Son, Jianwei Yang, Ce Liu, Jianfeng Gao, Yong Jae Lee, and Chunyuan
Li. Learning customized visual models with retrieval-augmented knowledge. arXiv preprint
arXiv:2301.07094, 2023.
Alexander Long, Wei Yin, Thalaiyasingam Ajanthan, Vu Nguyen, Pulak Purkait, Ravi Garg, Alan
Blair, Chunhua Shen, and Anton van den Hengel. Retrieval augmented classification for long-tail
visual recognition. In Proceedings of the Conference on Computer Vision and Pattern Recognition
(CVPR), 2022.
Subhransu Maji, Esa Rahtu, Juho Kannala, Matthew Blaschko, and Andrea Vedaldi. Fine-grained
visual classification of aircraft. arXiv preprint arXiv:1306.5151, 2013.
Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. Ok-vqa: A visual
question answering benchmark requiring external knowledge. In Proceedings of the Conference
on Computer Vision and Pattern Recognition (CVPR), 2019.
Christian M Meyer and Iryna Gurevych. Wiktionary: A new rival for expert-built lexicons? Exploring
the possibilities of collaborative lexicography. na, 2012.
Antoine Miech, Jean-Baptiste Alayrac, Lucas Smaira, Ivan Laptev, Josef Sivic, and Andrew Zis-
serman. End-to-end learning of visual representations from uncurated instructional videos. In
Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR), 2020.
George A Miller. WordNet: An electronic lexical database. MIT press, 1998.
Matthias Minderer, Alexey Gritsenko, Austin Stone, Maxim Neumann, Dirk Weissenborn, Alexey
Dosovitskiy, Aravindh Mahendran, Anurag Arnab, Mostafa Dehghani, Zhuoran Shen, et al. Simple
open-vocabulary object detection with vision transformers. arXiv preprint arXiv:2205.06230,
2022.
Maria-Elena Nilsback and Andrew Zisserman. Automated flower classification over a large number
of classes. In 2008 Sixth Indian Conference on Computer Vision, Graphics & Image Processing,
2008.
Hieu Pham, Zihang Dai, Golnaz Ghiasi, Kenji Kawaguchi, Hanxiao Liu, Adams Wei Yu, Jiahui Yu,
Yi-Ting Chen, Minh-Thang Luong, Yonghui Wu, et al. Combined scaling for open-vocabulary
image classification. arXiv preprint arXiv:2111.10050, 2021.
12

Published as a conference paper at ICLR 2024
Bryan A Plummer, Liwei Wang, Chris M Cervantes, Juan C Caicedo, Julia Hockenmaier, and
Svetlana Lazebnik. Flickr30k entities: Collecting region-to-phrase correspondences for richer
image-to-sentence models. In Proceedings of the International Conference on Computer Vision
(ICCV), 2015.
Vinay Uday Prabhu and Abeba Birhane. Large image datasets: A pyrrhic win for computer vision?
arXiv preprint arXiv:2006.16923, 2020.
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,
Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual
models from natural language supervision. In Proceedings of the International Conference on
Machine Learning (ICML), 2021.
Tal Ridnik, Emanuel Ben-Baruch, Asaf Noy, and Lihi Zelnik-Manor. Imagenet-21k pretraining for
the masses. arXiv preprint arXiv:2104.10972, 2021.
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang,
Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition
challenge. International journal of computer vision, 2015.
Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis,
Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m: Open dataset of
clip-filtered 400 million image-text pairs. arXiv preprint arXiv:2111.02114, 2021.
Sheng Shen, Chunyuan Li, Xiaowei Hu, Yujia Xie, Jianwei Yang, Pengchuan Zhang, Zhe Gan, Lijuan
Wang, Lu Yuan, Ce Liu, et al. K-lite: Learning transferable visual models with external knowledge.
Advances in Neural Information Processing Systems (NeurIPS), 2022.
Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh,
and Marcus Rohrbach. Towards vqa models that can read. In Proceedings of the Conference on
Computer Vision and Pattern Recognition (CVPR), 2019.
Amanpreet Singh, Ronghang Hu, Vedanuj Goswami, Guillaume Couairon, Wojciech Galuba, Marcus
Rohrbach, and Douwe Kiela. Flava: A foundational language and vision alignment model. In
Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR), 2022.
Vishaal Udandarao, Ankush Gupta, and Samuel Albanie. Sus-x: Training-free name-only transfer of
vision-language models. ICCV, 2023.
Aäron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive
coding. arXiv preprint arXiv:1807.03748, 2018.
Grant Van Horn, Oisin Mac Aodha, Yang Song, Yin Cui, Chen Sun, Alex Shepard, Hartwig Adam,
Pietro Perona, and Serge Belongie. The inaturalist species classification and detection dataset. In
Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR), 2018.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. Attention is all you need. Advances in Neural Information Processing
Systems (NeurIPS), 2017.
Catherine Wah, Steve Branson, Peter Welinder, Pietro Perona, and Serge Belongie. The caltech-ucsd
birds-200-2011 dataset. 2011.
Shuohang Wang, Yichong Xu, Yuwei Fang, Yang Liu, Siqi Sun, Ruochen Xu, Chenguang Zhu, and
Michael Zeng. Training data is more valuable than you think: A simple and effective method by
retrieving from training data. arXiv preprint arXiv:2203.08773, 2022.
Tobias Weyand, Andre Araujo, Bingyi Cao, and Jack Sim. Google landmarks dataset v2-a large-scale
benchmark for instance-level recognition and retrieval. In Proceedings of the Conference on
Computer Vision and Pattern Recognition (CVPR), 2020.
Yuhuai Wu, Markus Norman Rabe, DeLesley Hutchins, and Christian Szegedy. Memorizing trans-
formers. In ICLR, 2022.
13

Published as a conference paper at ICLR 2024
Yongqin Xian, Christoph H Lampert, Bernt Schiele, and Zeynep Akata. Zero-shot learning—a
comprehensive evaluation of the good, the bad and the ugly. Transactions on Pattern Analysis and
Machine Intelligence (TPAMI), 2018.
Jianxiong Xiao, James Hays, Krista A Ehinger, Aude Oliva, and Antonio Torralba. Sun database:
Large-scale scene recognition from abbey to zoo. In Proceedings of the Conference on Computer
Vision and Pattern Recognition (CVPR), 2010.
Chen-Wei Xie, Siyang Sun, Xiong Xiong, Yun Zheng, Deli Zhao, and Jingren Zhou. Ra-clip:
Retrieval augmented contrastive language-image pre-training. In CVPR, 2023.
Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mojtaba Seyedhosseini, and Yonghui Wu.
Coca: Contrastive captioners are image-text foundation models. arXiv preprint arXiv:2205.01917,
2022.
Xiaohua Zhai, Xiao Wang, Basil Mustafa, Andreas Steiner, Daniel Keysers, Alexander Kolesnikov,
and Lucas Beyer. Lit: Zero-shot transfer with locked-image text tuning. In Proceedings of the
Conference on Computer Vision and Pattern Recognition (CVPR), 2022.
Yuhao Zhang, Hang Jiang, Yasuhide Miura, Christopher D Manning, and Curtis P Langlotz. Con-
trastive learning of medical visual representations from paired images and text. In Machine
Learning for Healthcare Conference, 2022.
Bolei Zhou, Agata Lapedriza, Aditya Khosla, Aude Oliva, and Antonio Torralba. Places: A 10
million image database for scene recognition. IEEE Transactions on Pattern Analysis and Machine
Intelligence, 2017.
Yuke Zhu, Oliver Groth, Michael Bernstein, and Li Fei-Fei. Visual7w: Grounded question answering
in images. In Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR),
2016.
14

Published as a conference paper at ICLR 2024
Table 5: Zero-shot transfer to retrieval. We report recall@1 for retrieval. We show the improve-
ments obtained with RECO on top of CLIP-B/32 and CLIP-L/14: absolute performance gains are
between brackets. For reference, we also include the performance of other standard image-text
foundation models (Flava (Singh et al., 2022) and Align-base (Jia et al., 2021). We also report the
total parameter count (“# par.”) of the different models (in Million).
T→I
I→T
Method
# par. COCO
Flickr
COCO
Flickr
CLIP-B/32 151 30.2
61.1
51.2
80.9
+ RECO
154 33.6(+3.4) 65.7(+4.6) 52.2(+1.1) 81.8(+0.9)
CLIP-L/14 428 35.2
68.6
57.2
87.5
+ RECO
435 38.7(+3.5) 72.6(+4.0) 58.0(+0.8) 88.5(+1.0)
Other approaches
Flava
172 38.4
65.2
42.7
67.7
Align
247 40.2
72.6
55.1
86.7
A
APPENDIX
A.1
IMAGE/TEXT RETRIEVAL
Tab. 5 shows that RECO allows to boost the zero-shot performance of CLIP for image and text
retrieval even further. We observe that we can improve the recall@1 up to 4.6 points for text-to-image
retrieval. We also see smaller, but consistent gains for image-to-text retrieval.
A.2
USING LAION-400M AS THE MEMORY
In Table 6, we show that our method also works when using a public dataset as the memory bank
instead of our private source of knowledge. Indeed, we observe that using LAION-400M (Schuhmann
et al., 2021) as the memory bank for RECO gives substantial gains of performance compared to the
CLIP baseline across our different zero-shot tasks: for example +6.9 on Cars and +6.1 on CUB. This
validates that our method is generic and can work with different choices of external knowledge.
Table 6: Choice of memory bank. We report zero-shot top-1 accuracy on different image clas-
sification tasks. We evaluate RECO when using two different sources of knowledge: the non
publicly available WebLI (Chen et al., 2023) dataset (our default) and the publicly available LAION-
400M (Schuhmann et al., 2021) dataset.
Memory bank
Public
Cars
CUB
Flowers
Im1k
Pl365
None
–
57.2
52.8
62.1
63.5
40.6
WebLI (Chen et al., 2023) (default)
✗
68.1 (+10.9) 63.0 (+10.2) 67.9 (+5.8) 64.6 (+1.1) 42.2 (+1.6)
LAION-400M (Schuhmann et al., 2021)
✓
64.1 (+6.9)
58.9 (+6.1)
63.7 (+1.6) 63.4 (-0.1)
42.3 (+1.7)
A.3
MORE COMPLEX FUSION MODULE
In Table 7, we experiment with fusion modules of varying sizes. We observe that using a fusion
module of one or two layers works comparatively well. However, using larger fusion modules with
more layers, e.g. four, six or eight, deteriorates the performance. We hypothesize that this is because
increasing the capacity of the fusion creates overfitting. Overall, using a single-layer fusion module
brings large gains of performance on top of CLIP while being very light-weight to train.
A.4
END-TO-END FINETUNING VERSUS FROZEN BACKBONE
In Table 8, we evaluate the behavior of RECO when finetuning the original encoders at the same
time as the fusion module. We observe in Table 8 that the performance is comparable to keeping the
encoders frozen as in our default setting. Freezing the encoders has the advantage of requiring less
compute resources. In our implementation and using the same hardware, finetuning CLIP-B/32 along
15

Published as a conference paper at ICLR 2024
Table 7: Size of the fusion module. For each variant, we report the total number of parameters
(“# params”) in millions and the percentage of the total parameter count which is part of the fusion
modules (“% fusion params”). We report zero-shot top-1 accuracy on three image classification tasks.
# fusion layer
# params (M)
% fusion params
Cars
Flowers
Im1k
0
151.3
0%
57.2
62.1
63.5
1 (default)
154.4
2.0%
68.1
67.9
64.6
2
157.6
4.0%
67.8
69.1
64.3
4
163.9
7.7%
61.9
62.8
59.8
6
170.2
11.1%
60.6
64.5
59.6
8
176.5
14.3%
61.1
64.4
59.6
Table 8: Full finetuning versus frozen encoders. Both mechanisms produce good performance but
freezing the backbone allows for 1.6× faster training. We report zero-shot top-1 accuracy.
CLIP encoders f
Cars
Flowers
Im1k
Frozen (default)
68.1
67.9
64.6
Finetuned
68.5
67.1
64.2
with the fusion module has a training step 1.6× longer than working with frozen CLIP-B/32 and
training only the fusion module.
A.5
QUALITATIVE EXAMPLES
Zero-shot retrieval. In Figure 5, we show some qualitative examples when applying RECO to
zero-shot retrieval tasks on COCO dataset. Specifically, we aim to gain an understanding about why
the model benefits more from augmenting the text rather than the image side when applied to retrieval
downstream tasks (see Table 4 of the main paper). In Figure 5, we look at three different image-text
input pairs from the validation set of COCO and display the retrieved captions fused with the query
image as well as the retrieved images fused with the query text.
We observe in Figure 5 that the text captions of an image in COCO retrieval task usually focus on one
specific aspect of the image (for example the towel in the image of the bathroom or the British flag
on the train). We observe that the retrieved images from the input text are likely to also contain this
particular aspect of interest and hence match well with the original caption. For example, the retrieved
images from the train with a British flag caption (bottom of Figure 5) all contain representations of
vehicles with painted British flags, which is more likely to help the alignment with the original input.
On the contrary, the captions retrieved from the original image may focus on another particularity of
the image, not mentioned in the original caption. For example, the captions retrieved from the train
image contain information about train numbers, train station locations or operators. This information
is not useful for this task, because the ground-truth caption focuses on the fact that the train carriage
has a British flag on its side. This brings distracting signal instead of helping the alignment. Overall,
we think these qualitative examples help us understand why disabling retrieval for the image input
and enabling it for the text side results in a better performance in this task.
Zero-shot image classification. In Figure 6, we display some qualitative examples of RECO for zero-
shot image classification downstream tasks. Specifically, we consider several query images and their
class names and show the corresponding elements (captions for query image and images for query
class name) retrieved by our model. We observe in Figure 6 that in majority of cases, given a visual
input, searching for similar images and retrieving their corresponding captions effectively returns
descriptions containing useful information for fine-grained classification problem. For example, the
captions retrieved from the cat image at the top of Figure 6 contain the breed of that cat (siamese).
Likewise, given the class name “siamese cat”, RECO look for similar captions, for example “picture
of a siamese cat”, and returns their corresponding images. These all contain visual examples of what
a siamese cat looks like. Figure 6 show several successful examples of this mechanism and helps
giving intuition about why RECO helps for zero-shot image classification.
16

Published as a conference paper at ICLR 2024
Figure 5: Qualitative examples of RECO for image and text retrieval. We display image and
text queries on the left panel and retrieved captions and images on the right panel. We observe that
retrieved images tend to match better with the input original image than retrieved captions with the
input original text. For example, the retrieved captions from the aerial view do not mention a lot
“mountains” while this is present in the original text. Instead, they mention many specific locations,
for example lima, cuzco, arizona or afghanistan, which are not relevant to the original text description.
On the contrary, the retrieved images from the text query are semantically similar to the original
image. This qualitatively explains why the best of performance of RECO for zero-shot retrieval is
achieved by disabling retrieval on the query image and enabling it on the query text (see Table 4 of
the main paper).
Interestingly, we observe some failure cases when retrieving from a query class name which is
ambiguous in the sense that it can refer to several things. For example, the retrieved images from the
class name “prince of wales feathers” in Flowers dataset returns non useful information such as the
emblem of prince of wales or a picture of a feather. This is because “prince of wales feathers” can
refer to many things other than a flower species. We observe this behavior for several class names of
the Flowers classification benchmark which have a meaning outside of the flower species they refer
to; for example “bird of paradise”, or “bishop of llandaff” where one of the retrieved image is from
the actual person who used to be the Bishop of Llandaff, a community in Wales.
B
EVALUATION DETAILS
B.1
EVALUATION DATASETS
We report the details of each image classification dataset that we use to evaluate our model. Note that
we only use test or validation splits of each of these datasets, training sets are disregarded. Stanford
Cars (“Cars”) (Krause et al., 2013) contains 8, 041 test images of 196 fine-grained car classes. Each
17

Published as a conference paper at ICLR 2024
Figure 6: Qualitative examples of RECO for image classification. We consider several query
images and their class names (“query text”) and show the retrieved items to be fused with them. For a
given a class name, looking for similar captions and using their corresponding images usually returns
relevant visual examples. For example, the images retrieved from the class name “dungeness crab”
show examples of what dungeness crabs visually look like. However, when the class name can refer
to several things, for example “bird of paradise” which is both a flower and a bird species, then the
visual retrieved examples are not always relevant to the finegrained classification problem at hand.
class name consists of make, model and year of a car, e.g. 2012 Tesla Model S or 2012 BMW
M3 coupe. CUB-200-2011 (“CUB”) (Wah et al., 2011) consists of 5, 794 test images of 200 bird
species. The dataset contains additional annotations, such as part locations, binary attibutes, and
bounding boxes, but we do not use any of them. Oxford Flowers (“Flowers”) (Nilsback & Zisserman,
2008) has 6, 149 test images of 102 flower categories commonly found in the United Kingdom. The
dataset images contain strong visual variations, such as scale, pose, and light changes. ImageNet-1k
(“Im1k”) (Russakovsky et al., 2015), or also referred to as ILSVRC 2012, contains 50, 000 validation
images from 1, 000 classes. Class names are obtained from the synsets in WordNet (Miller, 1998),
and come from a large variety of generic concepts. Places365 (“Pl365”) (Zhou et al., 2017) has
18

Published as a conference paper at ICLR 2024
Table 9:
Standard deviation of RECO results. We run five RECO training, each one with a
different random seed. We show zero-shot image classification and retrieval results, and their standard
deviation across 5 runs.
Image classification
T→I
I→T
Method
Cars
CUB
Flowers Im1k
Pl365
Dogs
COCO
Flickr
COCO
Flickr
CLIP-B/32 57.2
52.8
62.1
63.5
40.6
58.6
30.2
61.1
51.2
80.9
RECO
68.1 ±0.3 63.0 ±0.3 67.9 ±0.4 64.6 ±0.1 42.2 ±0.1 59.7 ±0.2 33.6 ±0.1 65.7 ±0.3 52.2 ±0.3 81.8 ±0.3
36, 500 validation images from 365 generic scene categories. Some examples of class names include
living room, cottage, lecture room, pier etc. Finally, Stanford Dogs (“Dogs”) (Khosla et al., 2011)
consists of 8, 580 test images from 120 dog breeds.
We use two datasets for our retrieval experiments. Flickr30k (“Flickr”) (Plummer et al., 2015)
contains 1000 image-text pairs. Each image contains 5 sentence-level descriptions, or captions.
Similarly, MS COCO (“COCO”) (Lin et al., 2014) test set, as defined by Karpathy and Li (Karpathy
& Fei-Fei, 2015), contains 5, 000 image-text pairs, where each image contains 5 captions. We report
the performance for text-to-image (“T→I”) and image-to-text (“I→T”) retrieval on both datasets.
B.2
EVALUATION PROTOCOLS
Zero-shot image classification. We follow the standard setup (Radford et al., 2021) of embedding
each class name with the text encoder. We classify an image to the class which has the highest cosine
similarity between the image embedding and the corresping class name embedding. We report top-1
accuracy in all the image classification benchmarks. There is no variance at zero-shot evaluation time
since the inference for both text and vision encoders are deterministic.
Image and text retrieval. For image-to-text retrieval, given an input image, we rank all the text
embeddings according to their similarity to this image embedding. We report the proportion of
images that ranks the correct text within the first R positions as the recall@R. The process is the
symetric for text-to-image retrieval by switching the role of text and image. We report recall@1 in all
the retrieval tasks. There is no variance at zero-shot evaluation time since the inference for both text
and vision encoders are deterministic.
Variance and error bars. We report the performance variance on our small CLIP-B/32 setting to
make sure that observed gains are significant. We train RECO with CLIP-B/32 backbone 5 times
with different random seeds. We perform the evaluation for each model separately and report the
accuracy, averaged over 5 run, with the variance in Table 9. We observe that the standard deviation is
small across 5 runs, always below 0.4 across all benchmarks.
OVEN benchmark. OVEN benchmark (Hu et al., 2023) is created by combining 14 existing
datasets (ImageNet21k-P (Ridnik et al., 2021; Russakovsky et al., 2015), iNaturalist2017 (Van Horn
et al., 2018), Cars196 (Krause et al., 2013), SUN397 (Xiao et al., 2010), Food101 (Bossard et al.,
2014), Sports100 (Gerry, 2021), Aircraft (Maji et al., 2013), Oxford Flowers (Nilsback & Zisserman,
2008), Google Landmarks v2 (Weyand et al., 2020), and various VQA (visual question answering)
datasets (Goyal et al., 2017; Zhu et al., 2016; Krishna et al., 2017; Marino et al., 2019; Singh et al.,
2019; Gerry, 2021)) and grounding their categories to Wikipedia entities. The benchmark consists of
two splits. Entity Split measures the image recognition or retrieval capabilities of a model, whereas
the Query Split is designed as a VQA task. We focus on the Entity Split in this paper.
The Entity Splits contains training, validation, and test splits. However, since we focus on zero-shot
image classification in this paper, we ignore the training and validation splits, and evaluate our model
(trained on CC12M as discussed in Sec. 4) directly on the test set. The test set contains 729, 259
examples from 20, 549 entities. Each example belongs to a single entity. Nevertheless, total number
of candidate entities during inference is 6, 084, 494, i.e. there are more than 6M distractor entities at
inference.
Note that unlike other image classification datasets, each example is an image-text pair in OVEN.
The so-called intent text accompanies each image, and clarifies the question at hand, e.g. what is the
model of this vehicle? Similarly, each entity is also an image-text pair, containing the entity name
19

Published as a conference paper at ICLR 2024
and entity image. We simply follow the same protocol as other image classification datasets in this
paper, and only consider the example image and entity name.
C
ILLUSTRATIVE COMPARISON OF UNI-/CROSS- MODAL SEARCH AND UNI-/CROSS- FUSION
We give a conceptual comparison of uni-/cross- modal search and uni-/cross- fusion for an image
input I in the paper. We now show in Figure 7 this comparison for a text input T.
Uni-modal search
Cross-modal search
Uni-modal search
Cross-modal search
& cross-modal fusion
& cross-modal fusion
& uni-modal fusion
& uni-modal fusion
(Ours)
Figure 7: Conceptual comparison of uni-/cross- modal search and uni-/cross- fusion. We illustrate
the different scenarios for a text input T while the scenarios for image input I are shown in the main
paper.
D
NEAR-DUPLICATE FILTERING
0.65
0.70
0.75
0.80
0.85
0.90
0.95
Cosine Similarity
0
2000
4000
6000
8000
nearest neighbor
query image
0.95 similarity
nearest neighbor
query image
0.95 similarity
nearest neighbor
query image
0.95 similarity
Figure 8: Left: Cosine similarity distribution between the test image queries of CUB2011 and their 10
kNN. Right: Some of the queries and their nearest neighbors from the memory with 0.95 similarity.
Figure 8 (Left) of the PDF shows the cosine similarity distribution between the queries and memory
images on CUB2011 dataset. It is shown that most of the retrieved items have a similarity around 0.9,
which indicates that most of the retrieved elements are similar but not identical. Note that there is no
cosine similarity above 0.95. This is because we remove any memory image (and their corresponding
caption) if they have a similarity of 0.95 or above with any of the test images. Figure 8 (Right)
shows some of the kNNs with 0.95 cosine similarity. We see that the retrieved examples are not
near-duplicates, but they are conceptually similar as they should be.
E
BROADER IMPACT
We propose a retrieval-based recognition approach, where we search for similar images and text in
a large-scale memory. Data retrieved from such uncurated sources may be biased against certain
populations across the world (Prabhu & Birhane, 2020; De Vries et al., 2019). Furthermore, it is
important that the privileged user data does not exist in such data collections, in order to avoid using
the data without the consent of its owner. We acknowledge these potential misuses, and encourage
the community to utilize more fair and responsible data collections.
20

Published as a conference paper at ICLR 2024
retrieved captions 1-5
hoary 
redpoll
hoary 
redpoll
immature 
female 
northern 
cardinals
rose 
breasted 
grosbeak
purple finch
query image
image-to-image 
search
male house 
finch
birding not 
sure what's 
going on with 
this house 
finch any 
thoughts 
milwaukee wi
purple 
finch 
dsc_2669
pinson
ann elias's
signature 
teal 
stacked 
bird feeder
retrieved captions 6-10
Label: purple 
finch
retrieved captions 1-5
saltator 
gros bec
lazuli 
bunting 
pair by 
brooke
miller
lazuli 
bunting 
pair by 
brooke
miller
whiskered 
treeswift
whiskered 
treeswift 
oriental bird 
club image 
database
query image
image-to-image 
search
北印旛沼
ルリビタキの
画像
lesser 
striped 
swallow
ツバメ
yellow 
bellied 
flycatcher 
empidonax
flaviventris
ルリビタキ
iso1600
retrieved captions 6-10
Label: lazuli 
bunting
retrieved captions 1-5
house finch
beautiful 
pink colored
bird brown 
capped rosy 
finch
chechetka
чечётка 
acanthis
flammea 
redpoll
gray crowned 
rosy finch 
taylor
abbott;grey
crowned rosy 
finch
house finch 
emily gorda
query image
image-to-image 
search
tallbit
all about 
birds 
purple 
finch
red 
crossbill
house finch
зяблик 
самец;обыкн
овенный 
зяблик
retrieved captions 6-10
Label: gray
crowned rosy 
finch
Figure 9: Noisy examples. We display image queries and their labels on the left panel and 10
retrieved captions and images on the right panel. Retrieved captions are in green if they contain useful
information, red otherwise. RECO learns
F
NOISY EXAMPLES
We show the 10 retrieved captions for some of the test queries on the CUB2011 dataset on Figure 9.
We observe that RECO predicts the correct class even if the retrieved captions contain irrelevant
examples. This is because the Transformer module learns to aggregate the retrieved captions,
implicitly keeping the relevant ones and disregarding the irrelevant ones. Note that Webli contains
captions in languages other than English, which are considered noisy for CLIP, because it is trained
in an English corpus.
21

