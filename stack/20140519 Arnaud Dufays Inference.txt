 
1
Arnaud Dufays
Centre de Recherche en Economie et Statistique (CREST)
Bayesian methods for structural 
break modellings

 
2
Course Structure
●Chapter 1 :
Chapter 1 : 
Bayesian concepts and inference
●Chapter 2 :
Chapter 2 :
Structural breaks for models without path dependence
●Chapter 3 :
Chapter 3 :
Structural breaks for models with path dependence
●Chapter 4 :
Chapter 4 :
Introduction to Bayesian econometrics using Matlab

 
3
Chapter 1
●Bayesian inference : Principles (p. 4)
●Markov-chain Monte-Carlo (p. 20)
●Model selection (p. 44)

 
4
Model selection
Bayesian inference : Principles

 
5
Bayesian inference
●Model parameters (random vs fixed)
●Finite sample vs asymptotic theory
●Statistical interpretation (subjective vs objective)
Differences from classic approach : 
Differences from classic approach : 

 
6
Historical consideration
●Consider two non-independent events : A and B
●From basic axiom of probability : 
From effects to causes
From effects to causes
and
and
Bayes' rule :
Bayes' rule :
What is the probability of observing A if B has occurred ?

 
7
Generalization
●Multiple events : 
 Posterior
Posterior  
Distribution
Distribution
Prior 
Prior 
Distribution
Distribution
Normalizing 
Normalizing 
constant / 
constant / 
Marginal 
Marginal 
likelihood
likelihood
Normalizing constant : 
●Continuous case : Sum becomes an integral.

 
8
From effects to causes : Examples
One draw : A black ball
What is the probability that the ball comes from urn 1 ?
Urn 1
Urn 2
Prior distribution
Post distribution

 
9
From effects to causes : Examples
Unwin, S. D. , 'The probability of God : A simple 
Calculation that proves the ultimate truth', 2004
●A priori, God may or may not exist : P(G exists) = 0,5
●Observations : Miracles, Wars, Sins, ... 
Conclusion... You should bet the Lord exists

 
10
Principles of Bayesian inference
●Any unobserved value is random since we shall do a 
probabilistic statement on it.
Distributions on model parameters (prior to observing data)
●Data supposedly generated by the model contain 
information about the parameters.
Likelihood principle
●Through Bayes theorem, the parameter distribution is 
updated.
Posterior distribution of the parameters

 
11
Statistical Example
●The model : 
●Observations : 
●Prior distribution : 
Prior beliefs
Based on another set of observations 
●Reflects the uncertainty on the parameter
●Posterior distribution : 
Example

 
12
Calculus
Bayes' rule : 
Likelihood function : 
Prior distribution : 
The posterior must be a proper distribution : 
insured by the normalizing constant
We can drop all the terms that do not depend on 
and see if the posterior kernel comes from a known distribution

 
13
Calculus
Posterior kernel : 
 
The posterior kernel is a normal one with

 
14
Discussion
Same estimator as in the classical approach.
Data increasingly dominate the prior information.
●Bayesian inference provides an entire distribution 
only based on the observed data.
●Delivers different statistical interpretations. 

 
15
Summarizing the posterior
●Posterior means and standard deviations
●Credible intervals
●Posterior Covariance matrix
●Quantiles and graphics of the marginal 
distributions of the parameters

 
16
Criteria for statistical procedure
Caution :
Caution :
●Some Bayesian state that Bayesian inference is 'exact' in 
finite sample
●Properties (Consistency, efficiency,...) from hypothetical 
repeated samples/ large sample.
Classical
Bayesian
●Only based on the observed sample used 'coherently' 
through the likelihood principle.
No meaning since what happens in repeated sample 
is not relevant for Bayesian inference.
Based on very restrictive assumptions.

 
17
Treatment of parameters
Eases the interpretation :
●Fixed parameters in reality
95 % Confidence interval covers the true value of the 
parameter in nineteen out of twenty trials on average.
95 % credible interval gives the region of the parameter 
space where the probability of covering     is equal to 95.
Classical
Bayesian
Classical
Bayesian
●Random parameters in reality
●Fixed parameters but subjective probabilistic statement

 
18
Bayesian learning
New information makes update our belief
● Teglas, E. et al., 'Pure Reasoning in 12-Month-Old Infants as 
Probabilistic Inference'. 2011.
The posterior distribution as the prior distribution ? 
Core idea of Sequential Monte Carlo

 
19
Complex Bayesian inference
What happens if the posterior distribution is not a 
known parametric distribution ? 
Small 
Small 
dimension
dimension
●Deterministic  integration
●Direct sampling
●Importance sampling
●Sequential Monte Carlo (SMC)
●Annealed importance Sampling (SMC sampler)
Medium 
Medium 
dimension
dimension
●Markov-chain 
Markov-chain Monte
Monte Carlo (MCMC)
 Carlo (MCMC)
●Approximate Bayesian Computation (ABC)
●Variational Bayes methods
High 
High 
dimension
dimension

 
20
Model selection
Markov-chain Monte Carlo

 
21
Markov chain Monte Carlo
Posterior distribution : 
1) Not a known parametric distribution.
2) Cannot sample directly from it.
3) Parameter dimension greater than 3.
MCMC principles :  
1) (Correlated) draws of the posterior distribution.
2) LLN theorem allows to approximate any deterministic 
function of the posterior distribution.
1) Simulation of a Markov-chain exhibiting the posterior 
distribution of interest as invariant one.
MCMC Outputs : 

 
22
Invariant distribution
Markov-chain :  
●Characterizes by a number of reachable states (discrete or 
continuous), a transition probability matrix (P), a probability 
vector of being in the initial state.
●Chain that only depends on the current state for moving to 
the next
Moves according to the transition probability matrix P
Invariant distribution (q) : Independent from the initial state 
Unchanged when 
transformed by P

 
23
MC : Example
Transition 
matrix : 
Invariant Distribution
Invariant Distribution
Ergodicity
Ergodicity
From any initial point, the MC converges 
to the invariant distribution.
Based on 10000 
simulated draws

 
24
MCMC : Example
Invariant Dist. : 
Invariant Distribution
Invariant Distribution
Ergodicity
Ergodicity
Inversion of the problem
Inversion of the problem 
+   Ergodicity 

 
25
MCMC 
●Markov-chain exhibiting the posterior distribution as 
invariant one :
●Markov-chain that is ergodic
Sufficient condition :
Sufficient condition :
Reversibility condition – Detailed balance
Reversibility condition – Detailed balance
From any initial point, the MC converges to the invariant distribution.
Irreducible
Aperiodic
Positive Harris-recurrent
Sufficient condition :
Sufficient condition :
Able to visit all sets A such that
   from any starting point
Does not cycle through a finite number of sets
Beyond the scope of 
the course.

 
26
MCMC : pros and cons 
●MCMC outputs : correlated draws of the posterior.
●Posterior expectation :
●Posterior variance : 
Rate of convergence :
Rate of convergence :
● Ergodicity implies convergence from any initial point but after 
how many iterations ? 
● How many iterations after convergence ?
Burn-in period
Burn-in period
N ?
N ?
Criteria : Geweke, Gelman and Rubin, Cusum plot, ...
Criterion : Autocorrelation time

 
27
MCMC : Burn-in size
First 200 draws of the MCMC
Invariant Dist. : 
After 10000 draws

 
28
MCMC : Mixing problem
Invariant Dist. : 
First 200 draws of the MCMC
After 10000 draws

 
29
MCMC : Gibbs sampler
●Iterations on full conditional distributions
Inference on 
Dimension of blocks
 can be higher than one
MCMC
MCMC
  iterations
iterations
1
2
...
Burn-in
...
Initial state :
Initial state :

 
30
Gibbs sampler : Example
●Iterations on full conditional distributions
●Auto-Regressive process (AR) : 
Prior distributions
Prior distributions
Prior component
Prior component
Likelihood
Likelihood

 
31
Gibbs sampler : Example
●Iterations on full conditional distributions
●Auto-Regressive process (AR) : 
Prior distributions
Prior distributions
Prior component
Prior component
Likelihood
Likelihood
●If Z~IG(a,b) then 1/Z~G(a,1/b)

 
32
Gibbs sampler : Example
●AR(1) : 1000 Observations 
Prior distributions
Prior distributions

 
33
Metropolis-Hastings
●Conditional posterior distributions : too restrictive
●Metropolis-Hastings : 
1- Draw a proposal parameter from any chosen distribution
2- Accept or reject the draw  according to a probability  
which ensures that the invariant distribution of the MC is 
the posterior distribution of interest.

 
34
Metropolis-Hastings
●Let q be the proposal distribution (e.g. Normal)
●How to determine the probability function ? 
Sufficient condition :
Sufficient condition :
Let assume that 
Move from
Move from
too often
too often
Move from
Move from
too rarely
too rarely
Since it is a 
Since it is a 
probability
probability

 
35
Metropolis-Hastings
●Initialize the MCMC
●At each MCMC iteration :
●Generate a candidate from the proposal distribution
●Accept or reject the draw according to the probability
No need of the normalizing constant !
No need of the normalizing constant !

 
36
M-H : Comments
●If q is symmetric : Metropolis algorithm
then
●Random Walk Metropolis  :
●Independent M-H :
Caution :
Caution :
proposal dist : thicker tail than post dist.
●Gibbs sampler :
M-H can also be applied to multiple blocks
M-H can also be applied to multiple blocks

 
37
M-H : Comments
●Most frequent : Random Walk Metropolis -
●How to choose the variance parameter ?
Hot topic in the literature
Too many similar parameter values :
Too many similar parameter values :
Slow exploration of the support
Slow exploration of the support
Too many rejected values :
Too many rejected values :
Slow exploration of the support
Slow exploration of the support

 
38
M-H : Comments
●Most frequent : Random Walk Metropolis -
●How to choose the variance parameter ?
Hot topic in the literature
If Posterior distribution = Multivariate Normal distribution
Optimal acceptance rate
Optimal acceptance rate
Dimension 1
Dimension 1
Accept. Rate = 44 %
Accept. Rate = 44 %
Dimension 5
Dimension 5
Accept. Rate = 28 %
Accept. Rate = 28 %
Large Dimension 
Large Dimension 
,,,
,,,
Accept. Rate = 23,4 %
Accept. Rate = 23,4 %
Reference : Roberts, G. O. & Rosenthal, J. S. 'Optimal scaling for various 
Metropolis-Hastings algorithms', Statistical Science, 2001, 16, 351-367

 
39
M-H : Comments
●Most frequent : Random Walk Metropolis -
●How to choose the variance parameter ?
Common practice : Find the Optimal acceptance rate
Common practice : Find the Optimal acceptance rate
●By trials and errors
●By adaptive RW metropolis
Very demanding
Very demanding
Model dependent
Model dependent
Reference : Atchadé, Y. & Rosenthal, J. 'On adaptive Markov chain Monte Carlo 
algorithms', Bernoulli, 2005, 11(5), 815-828
●At MCMC iteration i
With
the acceptance rate at iteration i-1  
the targeted acceptance rate 

 
40
MH sampler : Example
●Adaptive RW Metropolis with block of one dimension
●GARCH process  : 
Prior distributions
Prior distributions
DGP of the simulated time series
Choice of the MCMC parameters

 
41
MH sampler : Example
●GARCH process  : 
●Acc. Rate :  
●Post means :  

 
42
MCMC : limitations
●Difficult to assess if the MC has converged to the post. Dist.
●Difficult to assess how much MCMC draws are required.
●Difficult to infer on multi-modal posterior distributions : 
RW Metropolis :
RW Metropolis : Jump from the current MCMC parameter
Unlikely to jump from one mode to another.

 
43
Model selection
Questions ?

 
44
Model selection
Model selection

 
45
Criteria
●Two popular Criteria : 
Deviance Information Criterion (DIC) 
Marginal likelihood 
Perfect when Marginal likelihood is out of reach
Intuitive criterion derived from Bayes' rule
Focus on Marginal likelihood 
Spiegelhalter, D.; Best, D.; Carlin, B. & van der Linde, A. 'Bayesian measures of model 
complexity and fit', Journal of Royal Statistical Society, Series B, 2002, 64, 583-639

 
46
Marginal likelihood
●Marginal likelihood = Normalizing constant :  
How to Choose between two models : M1 or M2 ?
Bayes' theorem
Bayes' theorem
If no subjective idea over the two models : 

 
47
Marginal likelihood
●Multiple Models :  
Bayes' theorem
Bayes' theorem
If no subjective idea over the different models : 

 
48
Bayesian Model Averaging (BMA)
●Multiple Models :  
Bayes' theorem
Bayes' theorem
Instead of choosing one model, keep them all
keep them all 
Take into account the model uncertainty
Predictive density :
Predictive density :
Point forecast :
Point forecast :

 
49
Marginal likelihood
●Quantity of interest :  
●Local Formula :  
Likelihood
Likelihood
Prior and 
Prior and 
Posterior 
Posterior 
●Ockham's razor : 
Likelihood 
Likelihood 
increases as long as the model complexity grows 
penalize the model complexity
Prior
Prior
Posterior
Posterior

 
50
Marginal likelihood
●Quantity of interest :  
If complex model with many parameters : 
Highly dimensional integration : difficult to compute.
Dimension < 3
Dimension < 3
●Numerical Integration 
●Importance sampling
Middle Dimension 
Middle Dimension 
●Bridge sampling
High Dimension 
High Dimension 
●Path sampling
●SMC sampler
●MCMC
MCMC
●Variational Bayes

 
51
Marginal likelihood by MCMC
●Quantity of interest :  
●Local Formula :  
Likelihood
Likelihood
Prior
Prior
Posterior
Posterior
The posterior density is the most tricky part.

 
52
Marginal likelihood by MCMC
●Local Formula :  
Likelihood
Likelihood
Prior
Prior
Posterior
Posterior
Chib, S. 'Marginal Likelihood from the Gibbs Output', 
Journal of the American Statistical Association, 1995, 90, 1313-1321
●Marginal likelihood from Gibbs sampler  :  
Chib, S. & Jeliazkov, I. 'Marginal Likelihood from the Metropolis-Hastings 
Output', Journal of the American Statistical Association, 2001, 96, 270-281
●Marginal likelihood from MH sampler  :  

 
53
Marginal likelihood : Example
●ML for AR processes :  
MCMC
MCMC
  iterations
iterations
Choose a high density point :   
1)
2) Prior :
Prior :
Likelihood :
Likelihood :
3)
Posterior :
Posterior :
NB :

 
54
Marginal likelihood : Example
●ML for AR processes :  
DGP of the simulated time series
●Log Marginal Likelihood :  
1          2         3         4         5         6         7         8         9        10
1          2         3         4         5         6         7         8         9        10
-3728      -3698     -3697     -3649     -3614      -3606      -3611       -3617      -3623      -3628

 
55
Model selection
Questions ?

 
 
1
Structural breaks for models without path dependence
Chapter 2

 
 
2
Chapter 2
●Motivation (p. 3)
●Change-point models (p. 8)
●Markov-switching and Change-point 
models (p. 22)
– Forward-backward algorithm
– Label switching
●References (p. 51)

 
 
3
Motivation

 
 
4
Motivation
●So far : Fixed parameters over time
Unlikely to hold for long time series
●So far : Fixed parameters over time
Many policy changes and turbulent financial periods
●Should affect the dynamics of the series
●Stylized fact of many time series
●High persistence – almost integrated series
Unit root model
No predictability !
Structural breaks can cause these stylized facts
Structural breaks can cause these stylized facts
●Long-run dynamic evolves over time

 
 
5
Long run dynamic : S&P 500
Long run volatility evolves over time
Long run volatility evolves over time
Markov-Switching GARCH
(Bauwens,Dufays,Rombouts, 2013) 
Spline GARCH
(Engle, Rangel, 2013) 

 
 
6
Example : S&P 500
No parameter dynamic 
No parameter dynamic 
Less 
Less 
persistent 
persistent 
Almost integrated
Almost integrated  

 
 
7
SB : Motivation
Why detecting structural breaks is relevant ?
●Historical analysis
Historical analysis
Better understanding of the time series dynamics
●Detection of instabilities
Detection of instabilities
Useful for systemic risk
●Forecasts
Forecasts
 Automatically select the optimal window size
Parameters adapted over time

 
 
8
Change-point models

 
 
9
Change-point models
CP models :
●Non recurrent regimes
Non recurrent regimes
Not stationary – In line with economic theories
●Forecasts
Forecasts
 Automatically select the optimal window size
Predictions based on the last sub-sample

 
 
10
Change-point models
CP models : first attempt to model structural breaks
●Chernoff and Zacks (1964) ;  Carlin, Gelfand and Smith 
Chernoff and Zacks (1964) ;  Carlin, Gelfand and Smith 
(1992) ; Stephens (1994)
(1992) ; Stephens (1994)
Modelling SB as discrete parameters to be estimated

 
 
11
Carlin, Gelfand and Smith (1992)
●Inference on one structural break (in mean or variance)
●Estimation carried out by Gibbs sampler
●The model : 
●The set of parameters : 
and
●Prior distributions :

 
 
12
Carlin, Gelfand and Smith (1992)
●Gibbs sampler : 
with
with
and
and

 
 
13
Carlin, Gelfand and Smith (1992)
●Griddy-Gibbs : 
Discrete conditional distribution 
Discrete conditional distribution 
1) Compute the posterior density for each i and normalize.
2) Draw u ~ U(0,1)
3) Find      such that 

 
 
14
Example
●US GDP growth (1959 Q2-2011 Q3) : AR(1)
●Posterior means :
1983 - Q1

 
 
15
Carlin, Gelfand and Smith (1992)
●Generic method :
●Works for many models (even with path dependence) 
●Easily extendible to M-H inference
●Limited to two regimes
●No criterion for selecting the number of regimes (one or two)
●Time-consuming if T large
Advantages
Advantages
Drawbacks
Drawbacks

 
 
16
Stephens (1994)
●Inference on multiple structural breaks (in mean or variance)
●Extension of Carlin, Gelfand and Smith
●Estimation carried out by Gibbs sampler
●Instead of one structural break :  K breaks (K+1 regimes)
●The parameter set is augmented by 
K break date parameters :
Corresponding mean parameters :
Corresponding var. parameters :
Prior distributions on break parameters :
Prior distributions on break parameters :

 
 
17
Stephens (1994)
●Gibbs sampler : if AR process as before 
with
with

 
 
18
Stephens (1994)
●Generic method :
●Works for many models (even with path dependence) 
●Easily extendible to M-H inference
Advantages
Advantages
Drawbacks
Drawbacks
●No criterion for selecting the number of regimes
●Time-consuming if T large
●Many MCMC iterations are required 
May not converge in a finite amount of time !
May not converge in a finite amount of time !

 
 
19
MCMC : Mixing problem
Invariant Dist. : 
First 200 draws of the MCMC
After 10000 draws
Correlated parameters should be jointly sampled
Correlated parameters should be jointly sampled

 
 
20
Stephens (1994)
MCMC may not converge in a finite amount of time !
MCMC may not converge in a finite amount of time !
True for Stephens' MCMC but for many others
True for Stephens' MCMC but for many others
Other example : 
●Single-move of Bauwens,Preminger,Rombouts (2011)
Critical issues : 
●Initial state of the MCMC
Bad initial state
Good initial state

 
 
21
Questions ?

 
 
22
MS and CP models

 
 
23
Chib (1996 - 1998)
Drawbacks (Stephens)
Drawbacks (Stephens)
●No criterion for selecting the number of regimes (ok in Chib
ok in Chib)
●Time-consuming if T large (ok in Chib
ok in Chib)
●Many MCMC iterations are required (ok in Chib
ok in Chib) 
Moreover
Moreover
●Algorithm for CP and MS models
●Estimation of the MLE 
Perfect for the MCMC initial state 

 
 
24
Chib (1996 - 1998)
●AR process of order 1 
●The set of parameters : 
Discrete states
●The set of parameters : 
Mean parameters
Var. parameters
Transition matrix
Introduction of a latent state vector driven by a Markov chain
Introduction of a latent state vector driven by a Markov chain

 
 
25
Chib (1996 - 1998)
is driven by a Markov-chain with transition matrix P
Example : 
at time t, active state : i
Change-point configuration
Change-point configuration
No recurrent state !
No recurrent state !

 
 
26
Chib (1996 - 1998)
is driven by a Markov-chain with transition matrix P
Markov-switching 
Markov-switching 
configuration
configuration
Recurrent states
Recurrent states
Parsimonious model
Parsimonious model
Difficult to estimate 
Difficult to estimate 
(label switching problem)
(label switching problem)

 
 
27
Chib (1996 - 1998)
Markov-switching 
Markov-switching 
configuration
configuration
Change-Point 
Change-Point 
configuration
configuration
CP and MS models are mixture models 
CP and MS models are mixture models 
with time-varying probabilities
with time-varying probabilities

 
 
28
Chib (1996 - 1998)
●AR process of order 1 
●Gibbs sampler : 
Where 
and  
Chib samples the state vector in one block !
Chib samples the state vector in one block !

 
 
29
Forward-Backward algorithm
●Discrete version of the Kalman filter.
●Two steps : 
1) Compute the forward and the predictive probabilities
2) Sample an entire state vector starting from       until   
●Observe that (without conditioning on                  for clarity) :  
●Sampling from                        is equivalent to draw 
~
~
...
~

 
 
30
Forward-Backward algorithm
●The challenge is to compute : 
●Assumption : 
Assumption :  
Model without path dependence
Model without path dependence
●AR process of order p : 
with
then 
No path dependence for AR processes
No path dependence for AR processes
●ARMA and GARCH : path dependence problem
ARMA and GARCH : path dependence problem

 
 
31
Forward-Backward algorithm
●The challenge is to compute : 
●Under assumption : 
Under assumption :  
Independent of  
Independent of   
●Two terms : 
transition matrix of the MC
1)
2)
probability of a state given the obs until t
New challenge : 
New challenge : 

 
 
32
Forward-Backward algorithm
●How to compute :  
computable 
computable  
Previous quantity ! 
Previous quantity !  
...
Starting point 
Starting point  
...
until T

 
 
33
Forward-Backward algorithm
●To summarize :
1) Compute  
Example : 
Example : 
Prediction step
Prediction step  
Update step
Update step
1st quantity
1st quantity
2nd quantity
2nd quantity

 
 
34
Forward-Backward algorithm
●At the end of 1)
1) 'Forward' matrix 
2) Draw 
3) Draw 
with 
with 

 
 
35
Chib's Gibbs sampler
Prior on P :  
Prior on P :  
●Last item of the Gibbs :  
Posterior :  
Posterior :  
with  
with  
Number of times 
where the state 
moves from i to j  

 
 
36
Model selection
●How to choose the number of breaks ?  
By Marginal likelihood
By Marginal likelihood  
●Choose a range of number of breaks : e.g. 0 to 5  
●Estimate each model from one break to five
●At each time, compute the marginal quantity
●Find the number of breaks that maximizes the ML
No break  
No break  
One break 
One break 
Five breaks
Five breaks
...
No break  
No break  
One break 
One break 
Five breaks
Five breaks

 
 
37
Model selection
●How to compute the marginal likelihood ?  
By MCMC or by Importance sampling
By MCMC or by Importance sampling  
By MCMC : Bayes' rule  
By MCMC : Bayes' rule  
Posterior density  
Posterior density  
computable
computable
1)
1)
2)
2)
22
11
Auxiliary MCMC with fixed P*
Auxiliary MCMC with fixed P*

 
 
38
Model selection
●How to compute the marginal likelihood ?  
By MCMC or by Bridge sampling
By MCMC or by Bridge sampling  
By Importance sampling  
By Importance sampling  
Where  
Where  

 
 
39
US Monthly Inflation Rate
#regimes
1
2
3
4
5
6
MLL 
-1424
-1409
-1391
-1386
-1384
-1388

 
 
40
Predictions of structural breaks
Pesaran, M. H.; Pettenuzzo, D. & Timmermann, A. 'Forecasting Time Series Subject 
to Multiple Structural Breaks', Review of Economic Studies, 2006, 73, 1057-1084
●Hierarchical distributions
Mean parameters  
Mean parameters  
Var. parameters  
Var. parameters  
●The hierarchical (random) parameters gather information 
from the different regimes
If new regime : draw parameters from the hier. dist.
If new regime : draw parameters from the hier. dist.

 
 
41
Predictions of structural breaks
●Hierarchical distributions
Mean parameters  
Mean parameters  
●A new break happens after the end of the sample (regime K+2) :
New parameters have to be drawn !
New parameters have to be drawn !
...
...
'Observations'
'Observations'
Update
Update
New regimes
New regimes
...
...
AR process : Gibbs sampler exists
AR process : Gibbs sampler exists

 
 
42
Example :
US 3-Month Treasury Bill
CP-AR(1) model exhibiting 7 regimes (according to MLL)

 
 
43
Example :
US 3-Month Treasury Bill
CP-AR(1) model exhibiting 7 regimes (according to MLL)

 
 
44
Label switching
●Issue arises in MS specification (recurrent states)
Full transition 
Full transition 
matrix
matrix
●Posterior distribution : invariant to the label of the states
Example : 
Example : 
T
Obs.
100
1
Regime 1
Regime 1
Regime 2
Regime 2
Regime 1
Regime 1
Regime 2
Regime 2
Same 
Same 
likelihood
likelihood
Misleading if it happens during the MCMC algorithm
Misleading if it happens during the MCMC algorithm

 
 
45
Example : US GDP Growth
●Great moderation : Drop of the variance
●Could be estimated by an MS-AR model

 
 
46
Example : US GDP Growth
●Posterior means are safe !
No label switching
No label switching
●Posterior means are not safe !
Label switching problem
Label switching problem

 
 
47
Label switching
●Solutions
1) Posterior distribution : invariant to the label of the states
Only if the prior is also symetric to the labeling
Only if the prior is also symetric to the labeling
Example : 
Example : 
T
Obs.
100
1
Regime 1
Regime 1
Regime 2
Regime 2
Regime 1
Regime 1
Regime 2
Regime 2
Prior distributions
Prior distributions
Constrain the prior distribution
Impossible label
Impossible label

 
 
48
Label switching
●Solutions
2) Sort out the MCMC draws after the algorithm
according to a loss function
according to a loss function
●3) Only use summary statistics invariant to label switches
Example : US GDP growth
Example : US GDP growth
Posterior means over time
Posterior means over time 
Invariant to labeling
Invariant to labeling 

 
 
49
Example : MS
US GDP Growth rate – MS-ARMA with 3 regimes

 
 
50
Chib
●Multiple breaks
●Recurrent or no recurrent states (Change-point/Markov-
switching)
●MCMC with good mixing properties
●Allow to select an optimal number of regimes
●Forecast of structural breaks
Advantages
Advantages
Drawbacks
Drawbacks
State of the art !
State of the art !
●Geometric distribution for the regime duration
●Many computation for selecting the number of regimes
●Not applicable to models with path dependence

 
 
51
Questions ?

 
 
52
References

 
 
53
References
Carlin, B., A.E. Gelfand and A.F.M. Smith, 1992, 'Hierarchical Bayesian
analysis of changepoint problems', Applied Statistics, 41, 389-405
●Carlin, Gelman and Smith (1992)
Stephens, D. A. 'Bayesian Retrospective Multiple-Changepoint Identification', Applied 
Statistics, 1994, 1, 159-178
●Stephens (1994)
Chib, S. 'Calculating Posterior Distributions and Modal Estimates in Markov Mixture 
Models', Journal of Econometrics, 1996, 75, 79-97
●Chib (1996)

 
 
54
References
Chib, S. 'Estimation and comparison of multiple change-point models', 
Journal of Econometrics, 1998, 86, 221-241
●Chib (1998)
Pesaran, M. H.; Pettenuzzo, D. & Timmermann, A. 'Forecasting Time Series Subject to 
Multiple Structural Breaks', Review of Economic Studies, 2006, 73, 1057-1084
●Pesaran, Pettenuzzo and Timmermann (2006)

 
 
55
Other CP and MS specs
●Koop and Potter (2007) – CP models
CP models without geometric durations
Inference of breaks without marginal likelihood
Extremely demanding !
Koop, G. & Potter, S. 'Estimation and Forecasting with Multiple Breaks',
 Review of Economic Studies, 2007, 74, 763-789
●Giordani and Kohn (2008) – recurrent models
Breaks modelled as mixtures (no prob. dynamic)
Inference of breaks without marginal likelihood
Parameters are subject to different breaks
Giordani, P. & Kohn, R. 'Efficient Bayesian inference for multiple change-point and mixture 
innovation models' Journal of Business and Economic Statistics, 2008, 26, 66-77

 
 
56
Other CP and MS specs
●Maheu and Song (2013) – CP models
Inference of breaks without marginal likelihood
Only adapted to AR process with normal innovations
Very fast !
Maheu, J. & Song, Y. 'A new structural break model, with an application to canadian 
inflation forecasting', International journal of forecasting, 2013, 30, 144-160
●Jochmann (2013) – CP and MS models
Inference of breaks with Dirichlet processes
Inference of CP and MS models at the same time
Predictions that encompass different number of breaks
Jochmann, M. 'Modeling U.S. Inflation Dynamics : A Bayesian Nonparametric Approach' 
Econometric Reviews, 2013, Forthcoming

 
 
57
Estimation by SMC
●Fearnhead and Liu (2007) – CP models
Exact inference by SMC
Very fast !
Fearnhead, P. & Liu, Z. 'On-line inference for multiple changepoint problems', 
Journal of Royal Statistical Society, Series B, 2007, 69 (4), 589-605
●Whiteley, Andrieu, Doucet (2013) – CP models
Unknow number of breaks
Exact inference by SMC
Faster than O(T^2)
Whiteley, N.; Andrieu, C. & Doucet, A. 'Bayesian Computational Methods for Inference 
in Multiple Change-points Models', Discussion paper, University of Bristol, 2011

 
 
58
Other references
Fruhwirth-Schnatter, S. 'Estimating Marginal Likelihoods for Mixture and Markov-
switching Models Using Bridge Sampling Techniques', Econometrics Journal, 2004, 7, 
143-167
●Marginal likelihood by Importance sampling and Bridge 
sampling
Geweke, J. 'Interpretation and Inference in Mixture Models: Simple MCMC works' 
Computational Statistics and Data Analysis, 2007, 51, 3259-3550
●Label-switching

Structural breaks for models with path dependence
Chapter 3

2
Chapter 3
●Path dependence (p. 3)
●Change-point models (p. 16)
●Markov-switching and Change-point 
models (p. 26)
– PMCMC algorithm
– IHMM-GARCH
●References (p. 43)

3
Path dependence

4
Chib's specification
●Multiple breaks
●Recurrent or no recurrent states (Change-point/Markov-
switching)
●MCMC with good mixing properties
●Allow to select an optimal number of regimes
●Forecast of structural breaks
Advantages
Advantages
Drawbacks
Drawbacks
State of the art !
State of the art !
●Geometric distribution for the regime duration
●Many computation for selecting the number of regimes
●Not applicable to models with path dependence
Not applicable to models with path dependence

5
Chib's specification
●Simplification in the Forward-backward algorithm : 
Why not applicable ?
Why not applicable ?
●If assumption does not hold :   
Chib's algorithm not available for
Chib's algorithm not available for
Example : ARMA, GARCH
Example : ARMA, GARCH
State-space model with structural breaks in parameters
State-space model with structural breaks in parameters

6
Path dependent models
CP- and MS-ARMA models
CP- and MS-ARMA models
CP- and MS-GARCH models
CP- and MS-GARCH models
Change-point
Change-point
Markov-switching
Markov-switching

7
Path dependence problem
T = 2   
T = 4   
T = 6   
ARMA
ARMA
GARCH
GARCH
Likelihood at time t depends on the whole path 
Likelihood at time t depends on the whole path 
that has been followed so far
that has been followed so far
Function 
of

8
Path dependence problem
Solutions ? 
Solutions ? 
1) Use of approximate models without path dependence
●Gray (1996), Dueker (1997), Klaassen (2002)
●Haas, Mittnik, Poella (2004)  

9
Path dependence problem
Solutions ? 
Solutions ? 
2) Stephens (1994) : Inference on multiple breaks
Drawbacks
Drawbacks
●Time-consuming if T large
●Many MCMC iterations are required 
May not converge in a finite amount of time !
May not converge in a finite amount of time !
3) Bauwens, Preminger, Rombouts (2011) : 
●Single-move MCMC

10
Single-move MCMC
CP- and MS-GARCH models
CP- and MS-GARCH models
Change-point
Change-point
Markov-switching
Markov-switching

11
Single-move MCMC
Metropolis-Hastings sampler :
Metropolis-Hastings sampler :
One state updated at a time !
Likelihood
Likelihood Transition matrix
Transition matrix

12
Example
Simulated series : 
Simulated series : 
Initial state :
Initial state :
Convergence after 100.000 MCMC iterations !
Convergence after 100.000 MCMC iterations !  

13
Single-move
●Generic method :
●Works for many CP and MS models 
Advantages
Advantages
Drawbacks
Drawbacks
●No criterion for selecting the number of regimes
●Very Time-consuming if T large (especially for MS)
●Many MCMC iterations are required :
Very difficult to assess convergence
Very difficult to assess convergence
May not converge in a finite amount of time !
May not converge in a finite amount of time !

14
Questions ?

15
Change-point models

16
D-DREAM algorithm
CP-GARCH models :
CP-GARCH models :
Come back to the Stephens' specification !
Come back to the Stephens' specification !

17
D-DREAM algorithm
Problem with Stephens' inference :
●Break dates sample one at a time (single-move) 
MCMC mixing issue
●Very demanding if T is large 
Discrete-DREAM MCMC : 
Discrete-DREAM MCMC : 
●Metropolis algorithm
●Jointly sample the break dates
●Very fast (faster than Forward-Backward)

18
D-DREAM algorithm
●Two sets of parameters to be estimated :
Continuous
Continuous  
Discrete 
Discrete   
●MCMC scheme :
Iterations
Iterations
Not a standard dist.
Not a standard dist.
Not a standard dist.
Not a standard dist.
Metropolis
Metropolis
Proposal : DREAM 
Proposal : DREAM   
Metropolis
Metropolis
Proposal : D-DREAM 
Proposal : D-DREAM   

19
D-DREAM algorithm
DDiffeRRential AAdaptative EEvolution M
Metropolis  
(Vrugt et al. 2009)
●DREAM automatically determines the size
size of the jump.
●DREAM automatically determines the direction
direction of the jump
●DREAM is well suited for multi-modal
multi-modal post. dist. 
●DREAM is well suited for high dimensional
high dimensional sampling
●DREAM is symmetric
symmetric : only a Metropolis ratio
Nevertheless only applicable to continuous parameters 
Nevertheless only applicable to continuous parameters   
Extension for discrete parameter : Discrete-DREAM 
Extension for discrete parameter : Discrete-DREAM 

20
DREAM : Example
Adaptive RW
Adaptive RW
DREAM
DREAM

21
DREAM algorithm
M parallel MCMC chains : 
...
Proposal distribution :
Proposal distribution : 
Symmetric proposal dist :
Symmetric proposal dist :
●Accept/reject the draw according to the probability
Accept/reject the draw according to the probability 

22
D-DREAM algorithm
M parallel MCMC chains : 
Continuous
Continuous  
Discrete 
Discrete   
Proposal distribution :
Proposal distribution : 
Proposal distribution :
Proposal distribution : 
Accept with probability
Accept with probability 
Accept with probability
Accept with probability 

23
Example
Initial state :
Initial state :
Convergence after 100.000 
Convergence after 100.000 
MCMC iterations !
MCMC iterations !  
Initial states  around
Initial states  around
Convergence after 3.000 
Convergence after 3.000 
MCMC iterations !
MCMC iterations !  
D-DREAM
D-DREAM
Single-move
Single-move

24
D-DREAM (2014)
●Generic method for CP models
●Inference on multiple breaks by marginal likelihood
●Very fast compared to existing algorithms
●Model selection based on many estimations
●Only applicable to CP models and specific class of recurrent 
states
Advantages
Advantages
Drawbacks
Drawbacks

25
CP and MS models

26
Particle MCMC
CP- and MS-GARCH models
CP- and MS-GARCH models
Change-point
Change-point
Markov-switching
Markov-switching

27
Particle MCMC
Sets of parameters : 
Continuous
Continuous  
State var. 
State var.   
MCMC scheme :  
1)
1)
2)
2)
3)
3)
Sampling a full state vector is unfeasible 
Sampling a full state vector is unfeasible 
due to the path dependence issue
due to the path dependence issue

28
Particle MCMC
3)
3)
Idea :
Idea : Approximate the distribution with a SMC algorithm
 Approximate the distribution with a SMC algorithm
Does not keep invariant the posterior distribution
Does not keep invariant the posterior distribution
Andrieu, Doucet and Holenstein (2010)
●Show how to incorporate the SMC into an MCMC
●Allow for Metropolis and Gibbs algorithms
●Introduce the concept of conditional SMC
Does not keep invariant the posterior distribution
Does not keep invariant the posterior distribution
With a conditional SMC, the MCMC exhibits the 
With a conditional SMC, the MCMC exhibits the 
posterior distribution as invariant one.
posterior distribution as invariant one.

29
Particle MCMC
3)
3)
Previous value
Previous value
SMC :
1) Initialisation of the particles and weights:
1) Initialisation of the particles and weights:
Iterations
Iterations
●Re-sample the particles
●Generate new states
●Compute new weights
and

30
SMC
Init.
Re sampling New states
Weights
... until T

31
Particle Gibbs
●Conditional SMC :
Conditional SMC : SMC where the previous MCMC state vector 
is ensured to survive during the entire SMC sequence.
3)
3)
●Launch a conditional SMC
●Sample a state vector as follows :
1)
1)
2)
2)
●Improvements :
1) Incorporation of the APF in the conditional SMC
2) Backward sampling as Godsill, Doucet and West (2004) 

32
Example
Initial state :
Initial state :
Initial states  around
Initial states  around
D-DREAM
D-DREAM
PMCMC
PMCMC

33
PMCMC
S&P 500 daily percentage returns 
S&P 500 daily percentage returns 
from May 20,1999 to April 25, 2011
from May 20,1999 to April 25, 2011

34
PMCMC
Various financial time series
Various financial time series

35
PMCMC (2013)
●Generic method for CP and MS models
●Inference on multiple breaks by marginal likelihood
●Very good mixing properties
●Model selection based on many estimations
●Very computationally demanding
●Difficult to calibrate the number of particles
●Difficult to implement
Advantages
Advantages
Drawbacks
Drawbacks

36
IHMM-GARCH
CP- and MS-GARCH models
CP- and MS-GARCH models
Change-point
Change-point
Markov-switching
Markov-switching

37
IHMM-GARCH
Sets of parameters : 
Continuous
Continuous  
State var. 
State var.   
MCMC scheme :  
1)
1)
2)
2)
3)
3)

38
IHMM-GARCH
3)
3)
Sampling a full state vector is infeasible 
Sampling a full state vector is infeasible 
due to the path dependence issue
due to the path dependence issue
Sampling a full state vector from an approximate model
Sampling a full state vector from an approximate model
Accept/reject according to the Metropolis-hastings ratio
Accept/reject according to the Metropolis-hastings ratio
Klaassen or Haas, Mittnik and Paolela
Klaassen or Haas, Mittnik and Paolela

39
IHMM-GARCH
Moreover, Hierarchical dirichlet processes
Hierarchical dirichlet processes are used
 
●To infer the number of regime in one estimation
●To include both CP and MS specification in one model

40
IHMM-GARCH
S&P 500 daily percentage returns 
S&P 500 daily percentage returns 
from May 20,1999 to April 25, 2011
from May 20,1999 to April 25, 2011
PMCMC
PMCMC
IHMM-GARCH
IHMM-GARCH

41
IHMM-GARCH (2014)
●Generic method for CP and MS models
●Self-determination of the number of breaks
●Self-determination of the specification (CP and/or MS)
●Predictions of breaks
●Very good mixing properties
●Fast MCMC estimation
●Difficult to implement
Advantages
Advantages
Drawbacks
Drawbacks

42
References

43
References
Bauwens, L.; Preminger, A. & Rombouts, J. 'Theory and Inference for a Markov-
switching GARCH Model', Econometrics Journal, 2010, 13, 218-244
●Bauwens, Preminger,  Rombouts (2010)
Bauwens, L.; Dufays, A. & De Backer, B. 'Estimating and forecasting structural breaks 
in financial time series', CORE discussion paper, 2011/55, 2011
●Bauwens, Dufays, De Backer (2011)
Bauwens, L.; Dufays, A. & Rombouts, J. 'Marginal Likelihood for Markov Switching and 
Change-point GARCH Models', Journal of Econometrics, 2013, 178 (3), 508-522
●Bauwens, Dufays, Rombouts (2013)

44
References
●Dufays, A. 'Infinite-State Markov-switching for Dynamic Volatility and Correlation 
Models', CORE discussion paper, 2012/43, 2012
●Dufays (2012)
He, Z. & Maheu, J. M. 'Real time detection of structural breaks in GARCH models', 
Computational Statistics & Data Analysis, 2010, 54, 2628-2640
●He, Maheu (2010)
SMC algorithm :
SMC algorithm :

45
Without path dependence
Gray, S. 'Modeling the Conditional Distribution of Interest Rates as a Regime-
Switching Process', Journal of Financial Economics, 1996, 42, 27-62
●Gray (1996)
Dueker, M. 'Markov Switching in GARCH Processes in Mean Reverting Stock Market 
Volatility', Journal of Business and Economics Statistics, 1997, 15, 26-34
●Dueker (1997)
Klaassen, F. 'Improving GARCH volatility forecasts with regime-switching GARCH', 
Empirical Economics, 2002, 27, 363-394
●Klaassen (2002)
Haas, M.; Mittnik, S. & Paolella, M. 'A New Approach to Markov-Switching GARCH 
Models', Journal of Financial Econometrics, 2004, 2, 493-530
●Haas, Mittnik and Paolella (2004)

Sparse Change-point model
Arnaud Dufays
(Centre de Recherche en Economie et Statistique)
May , 

Motivation
Limitations of standard CP models :
1. Classical inferences available for AR and ARCH models
→Difﬁcult to estimate path dependence models
(CP-ARMA,CP-GARCH).
2. Optimal number of regimes computed by Marginal likelihood
→Many useless estimations and uncontrolled penalty.
3. Each new regime increases the number of model parameters
→Over-parametrization.
4. Forecasts based on the last regime
→Uncertainties on parameters and inaccurate predictions.

Example
ARMA model
Standard
Change-Point
yt = c + βyt−1 + φϵt−1 + ϵt
yt = ci + βiyt−1 + φiϵt−1 + ϵt
No dynamic for c, β, φ
A latent variable governs
the dynamic of breaks

Contribution
CP models using shrinkage priors :
1. Adapted to estimate path dependence models
(CP-ARMA,CP-GARCH).
2. Optimal number of regimes obtained in one estimation with
user-speciﬁed penalty.
3. Controls the over-parametrization.
→Only a few parameter evolves from one regime to another.
4. Very good forecast performances.

Example
CP-ARMA
CP-ARMA with Shrinkage priors
CP-ARMA
Our CP-ARMA
σ2
1.11
0.25
0.53
1.11
0.27
0.27
(0.14) (0.04) (0.20)
(0.15) (0.04) (0.04)
MA term -0.08
-0.37
0.48
-0.34
-0.34
0.5
(0.15) (0.13) (0.29)
(0.09) (0.09) (0.19)

Outline
The CP-Model and its estimation
Shrinkage methods
Empirical applications
Conclusion

The CP-ARMA model
Let YT = {y1, ..., yT} be a time series of T observations.
The CP-ARMA model deﬁned by :
yt = c1 + β1yt−1 + φ1ϵt−1 + ϵt with ϵt ∼N(0, σ2
1) for t ≤γ1
yt = c2 + β2yt−1 + φ2ϵt−1 + ϵt with ϵt ∼N(0, σ2
2) for t ≤γ2
. . . . . . . . .
yt = cK+1 + βK+1yt−1 + φK+1ϵt−1 + ϵt for γK < t ≤T
(1)
Focus on the break dates Γ = (τ1, ..., τK)′ instead of a state vector
ST = {s1, ..., sT}
Let Θ = (c1, β1, φ1, σ2
1, ..., cK+1, βK+1, φK+1, σ2
K+1)′.
Obs. #
0
1
2
3
...
99 100 101
...
259 260 261
...
T
||
||
||
||
τ0
τ1
τ2
τ3
Regime 1
Regime 2
Regime 3

D-DREAM algorithm : speciﬁcation
The MCMC scheme is
1. π(Θ|Γ, YT)
2. π(Γ|Θ, YT)
We use a Metropolis algorithm :
DiffeRential Adaptative Evolution Metropolis (Vrugt et al. (2009))
DREAM automatically determines the size of the jump.
DREAM automatically determines the direction of the jump.
DREAM is well suited for multi-modal posterior distributions and for
high dimensional parameters.
DREAM proposal is symmetric.
However only suited for continuous parameters.
D-DREAM for Discrete parameters (Bauwens et al. (2011))

Outline
The CP-Model and its estimation
Shrinkage methods
Empirical applications
Conclusion

Shrinkage priors
How to determine which model parameter(s)
evolves from one regime to another ?
Testing all the possibilities and computing the Marginal likelihood
1. Too many posterior distributions to be estimated.
→For 4 regimes and 4 parameters by regime : 256 models.
→In the empirical example : 8 regimes (4096 models).
2. No proof that the Marginal likelihood will choose the right spec.
3. Break date parameters for each parameters : Over-parametrization.

Shrinkage priors
How to determine which model parameter(s)
evolves from one regime to another ?
Keep the speciﬁcation of a standard CP-ARMA but shrink the irrelevant
parameters and break dates toward zero.
1. Only one estimation is required.
2. Break is identiﬁed only if it improves the likelihood.
3. Long regimes : estimators tend to their distribution without shrinkage.

Transformation of the model
For applying a shrinkage prior, the CP-ARMA model becomes :
yt = (c1 +
k
X
i=2
∆ci) + (β1 +
k
X
i=2
∆βi)yt−1 + (φ1 +
k
X
i=2
∆φi)ϵt−1
+ϵt with ϵt ∼N(0, σ2
1 +
k
X
i=2
∆σ2
i ) for t ∈]γk−1, γk]
with ∆ci = ci −ci−1.
The ﬁrst regime : {c1, β1, φ1} ∼N(0, 10I3) and σ2
1 ∼U[0, 30].
Shrinkage on the other parameters :
For Example : ∆ci|τ ∼N(0, τ) and τ ∼Q.

Shrinkage priors
Shrinkage is about the distribution Q : ∆ci|τ ∼N(0, τ) and τ ∼Q.
The absolutely continuous spike-and-slab prior (Ishwaran and Rao
(2000)) :
τ = σ2κ with σ−2 ∼G(v
2, v
2) and κ|ω = ωδκ=0.00001 + (1 −ω)δκ=1.
The marginal distribution of ∆ci|ω is a mixture
of two student distributions :
Spike and Slab marginal distribution

Outline
The CP-Model and its estimation
Shrinkage methods
Empirical applications
Conclusion

US GDP growth rate 1959-2011
CP-ARMA
CP-ARMA with Shrinkage priors
CP-ARMA with Spike and Slab
Regime
2
3
c
0.03
0.04
β
0.03
0.03
φ
0.03
1.00
σ2
1.00
0.06
Only the variance and the MA parameter change over time

Monthly 3-Month US T-bill rate 1947-2002
Pesaran et al.
CP-ARMA with Shrinkage priors

Outline
The CP-Model and its estimation
Shrinkage methods
Empirical applications
Conclusion

Conclusion
→Algorithm : Inference for ARMA models with structural breaks.
Detects the parameters that change from one regime to another.
Shrinks all the irrelevant parameters toward zero.
Shrinks all the irrelevant regime.
→No need of the marginal likelihood.
→Empirical enhancements :
Could improve the interpretation of the presence of structural breaks.
Very good prediction performances.

 
 
1
Numerical session on matlab
Chapter 4

 
 
2
Chapter 4
●Introduction (p. 3) 
●Simple Gibbs and MH MCMC (p. 9)
●CP-AR models (p. 27)
– Carlin, Gelman and Smith : Griddy-Gibbs
– Chib's algorithm

 
 
3
Introduction

 
 
4
Introduction
●Four directories : 
●AR_regressions – MCMC for simulating posteriors of AR 
MCMC for simulating posteriors of AR 
models
models
●GARCH_with_MH – MH MCMC for simulating posteriors 
MH MCMC for simulating posteriors 
of a GARCH(1,1) model
of a GARCH(1,1) model
●CP_models_with_CGS – MCMC for drawing posteriors of 
MCMC for drawing posteriors of 
AR models with 2 regimes using the Carlin, Gelman and 
AR models with 2 regimes using the Carlin, Gelman and 
Smith approach
Smith approach
●CP_models_with_Chib – MCMC for posterior distributions 
MCMC for posterior distributions 
of AR models with k regimes based on Chib's algorithm
of AR models with k regimes based on Chib's algorithm
Quick start
Quick start

 
 
5
Introduction
How to use the programs ?
How to use the programs ?
Current folder : Matlab directory
Current folder : Matlab directory
Command window
Command window
Workspace
Workspace
Navigator
Navigator

 
 
6
Introduction
Go to the AR_regressions directory
Go to the AR_regressions directory
Click on US_GDP_percentage
Click on US_GDP_percentage
Current folder : 
Current folder : 
Matlab directory
Matlab directory
Command window
Command window
Workspace with data
Workspace with data
Navigator
Navigator

 
 
7
Introduction
How to run a program ?
How to run a program ?
●Open the program : 
Open the program : 
launch_AR_regression_with_Gibbs_sampler
launch_AR_regression_with_Gibbs_sampler
●Copy and paste the first line without 'function' in the 
Copy and paste the first line without 'function' in the 
command window 
command window 
●Comments to run the function are in green
Comments to run the function are in green
Instead of y : 
Instead of y : 
the data
the data
Instead of 
Instead of 
AR_lags : the 
AR_lags : the 
order of the 
order of the 
AR process
AR process
nb_MCMC : 1000 iterations
nb_MCMC : 1000 iterations

 
 
8
Introduction
What is a structure ?
What is a structure ?
●Simu is a matlab structure containing many tables
Simu is a matlab structure containing many tables
●To access the matrix 'post_beta' :
To access the matrix 'post_beta' :  
Type in command window : Simu.post_beta 
Type in command window : Simu.post_beta 

 
 
9
Simple MCMC

 
 
10
Gibbs sampler
●The model 
The model 
●The prior distributions 
The prior distributions 
●Conditional posterior distributions :
Conditional posterior distributions :

 
 
11
Gibbs sampler
●Initial values :
Initial values :
●Gibbs sampler :
Gibbs sampler :
MLE estimates
MLE estimates
Discard the first draws as burn-in period
Discard the first draws as burn-in period
Use the rest as a sample of the posterior distribution :
Use the rest as a sample of the posterior distribution :

 
 
12
Gibbs sampler
●Open the function : 
Open the function : Gibbs_regression_with_MLL
Gibbs_regression_with_MLL
●Gibbs sampler of a simple regression
●Provide the marginal log-likelihood (MLL) estimated from 
three different approaches (Chib, Importance sampling 
and Harmonic mean)
Harmonic mean : very bad estimate
Harmonic mean : very bad estimate
●The prior distributions 
The prior distributions 
Hyper-parameters of the variance
Hyper-
parameters of 
the mean 
parameters

 
 
13
Gibbs sampler
Starting values
Saving posterior draws
First block - 
the variance
First block - 
the mean parameters

 
 
14
Gibbs sampler
Starting values
Saving posterior draws
First block - 
the variance
First block - 
the mean parameters

 
 
15
Gibbs sampler
●Open the function : 
Open the function : launch_AR_regression_with_Gibbs_sampler
launch_AR_regression_with_Gibbs_sampler
[Simu] = 
launch_AR_regression_with_Gibbs_sampler(US_GDP_growt
h,1,3000,1)
In the command window, type :
In the command window, type :
●The data : US_GDP_growth
●AR order : 1
●Nb MCMC iterations : 3000
●Convergence Graphics : on

 
 
16
Gibbs sampler
●Graphics :
Graphics :
Empirical correlations between MCMC draws
Empirical correlations between MCMC draws
Variance

 
 
17
Gibbs sampler
●Graphics :
Graphics :
Cumsum plots of the two mean parameters
Cumsum plots of the two mean parameters
Empirical mean of 
the parameter
Empirical std of 
the parameter
Smooth convergence to zero

 
 
18
Gibbs sampler
●Geweke's test of MCMC convergence :
Geweke's test of MCMC convergence :
Comparison of two empirical means well separated
Comparison of two empirical means well separated
In the command window :
Simu.test_Geweke
Test for each mean parameters and return 
zero if the hypothesis is not rejected at 95%
Convergence
Convergence

 
 
19
Gibbs sampler
●Results :
Results :
       
       plot(Simu.post_beta')               plot(Simu.post_sigma')
plot(Simu.post_beta')               plot(Simu.post_sigma')
mean(Simu.post_beta')= 0,50 ; 0,32
mean(Simu.post_beta')= 0,50 ; 0,32
std(Simu.post_beta') = 0,08 ; 0,06
std(Simu.post_beta') = 0,08 ; 0,06
mean(Simu.post_sigma')= 0,70
mean(Simu.post_sigma')= 0,70
std(Simu.post_sigma') = 0,07
std(Simu.post_sigma') = 0,07
Marginal distribution 
Marginal distribution 
of the variance
of the variance

 
 
20
Gibbs sampler
●Testing for the order of the AR process :
Testing for the order of the AR process :
●Launch several times the program with different # of lags
Launch several times the program with different # of lags
●Compare the MLL obtained from the different simulations
Compare the MLL obtained from the different simulations
AR(0)
AR(1)
AR(2)
AR(3)
AR(4)
MLL HM
-274,16
-259,89
-256,36
-257,16
-254,75
MLL Chib -279,12
-269,7
-269,71
-273,72
-275,46
MLL IS
-279,12
-269,7
-269,71
-273,72
-275,46
●Harmonic mean : not reliable
Harmonic mean : not reliable
●Same Estimates from the local and the global formula
Same Estimates from the local and the global formula

 
 
21
MH sampler
●Open the function : 
Open the function : MH_regression_with_MLL
MH_regression_with_MLL
●MH sampler of a simple regression
●Provide the marginal log-likelihood (MLL) estimated from 
three different approaches (Chib, Importance sampling 
and Harmonic mean)
●Running an estimation of AR model with MH sampler : 
Running an estimation of AR model with MH sampler : 
launch_AR_regression_with_Gibbs_sampler
launch_AR_regression_with_Gibbs_sampler

 
 
22
Gibbs sampler
●Adaptation of the proposal distribution 
Adaptation of the proposal distribution 
(Atchadé and Rosenthal (2005) )
(Atchadé and Rosenthal (2005) )

 
 
23
Comparison Samplers
●AR(1)
AR(1)
       
       Gibbs sampler
Gibbs sampler
Mean= 0,50 ; 0,32
Mean= 0,50 ; 0,32
Std = 0,08 ; 0,06
Std = 0,08 ; 0,06
Mean   = 0,51 ; 0,32
Mean   = 0,51 ; 0,32
Std   = 0,07 ; 0,06
Std   = 0,07 ; 0,06
Acceptance rate :
Acceptance rate :
45 % ; 44 % ; 45 %
45 % ; 44 % ; 45 %
Metropolis-Hastings
Metropolis-Hastings

 
 
24
MH sampler
●Testing for the order of the AR process :
Testing for the order of the AR process :
●Launch several times the program with different # of lags
Launch several times the program with different # of lags
●Compare the MLL obtained from the different simulations
Compare the MLL obtained from the different simulations
AR(0)
AR(1)
AR(2)
AR(3)
AR(4)
MLL Chib (MH)
-279,12
-269,7
-269,71
-273,72
-275,46
MLL Chib (Gibbs)
-279,12
-269,7
-269,71
-273,72
-275,46
Same Estimates from Gibbs or MH samplers
Same Estimates from Gibbs or MH samplers
Marginal likelihood depends on the prior!
Marginal likelihood depends on the prior!
●Test the order of the AR process with different priors
Test the order of the AR process with different priors

 
 
25
GARCH estimation by MH MCMC
●Change the directory and go to 
Change the directory and go to GARCH_with_MH
GARCH_with_MH
●Import financial data by clicking on 
Import financial data by clicking on 
SP500_percentage_returns.mat
SP500_percentage_returns.mat
●Main matlab program 
Main matlab program MCMC_GARCH_RW
MCMC_GARCH_RW
Estimate a GARCH(1,1) model with 
Estimate a GARCH(1,1) model with 
●non adaptive RW 
non adaptive RW 
●adaptive RW
adaptive RW
Inputs :
Inputs :
●Y : financial time series 
Y : financial time series (here SP500)
(here SP500)
●nb_MCMC : number of MCMC iterations 
nb_MCMC : number of MCMC iterations 
●RW_step : Variance of the proposal distribution
RW_step : Variance of the proposal distribution
●Graph : Convergence graphics
Graph : Convergence graphics

 
 
26
GARCH estimation by MH MCMC
●Choose a RW variance and run the program
Choose a RW variance and run the program
●[Simu] = MCMC_GARCH_RW(SP500,5000,0.1)
[Simu] = MCMC_GARCH_RW(SP500,5000,0.1)
●Focus on parameter       in 
Focus on parameter       in 
Accept. Rate : 1%
Accept. Rate : 1%
Accept. Rate : 44%
Accept. Rate : 44%

 
 
27
Change-Point AR models

 
 
28
CP-AR models with CGS
●Change the directory and go to 
Change the directory and go to CP_models_with_CGS
CP_models_with_CGS
●Import data by clicking on 
Import data by clicking on US_GDP_percentage
US_GDP_percentage.mat
.mat
●Main matlab program 
Main matlab program 
Gibbs_regression_Carlin_Gelman_Smith
Gibbs_regression_Carlin_Gelman_Smith
●Estimates a CP model with 2 regimes using CGS's algorithm
Estimates a CP model with 2 regimes using CGS's algorithm
Inputs :
Inputs :
●Y : a time series 
Y : a time series (here US_GDP_growth)
(here US_GDP_growth)
●X : explanatory variables
X : explanatory variables
●nb_MCMC : number of MCMC iterations
nb_MCMC : number of MCMC iterations
●Program for estimating a CP-AR(q) model
Program for estimating a CP-AR(q) model
Launch_CP_AR_Carlin_Gelman_Smith
Launch_CP_AR_Carlin_Gelman_Smith

 
 
29
CP-AR models with CGS
●Run an estimation of a CP-AR(1) model
Run an estimation of a CP-AR(1) model
[Simu] = 
[Simu] = 
launch_CP_AR_Carlin_Gelman_Smith(US_GDP_growth,1,5000)
launch_CP_AR_Carlin_Gelman_Smith(US_GDP_growth,1,5000)
plot(Simu.post_tau')
plot(Simu.post_tau')
plot(Simu.post_sigma')
plot(Simu.post_sigma')
●Great moderation
Great moderation
●Not a symmetric distribution
Not a symmetric distribution

 
 
30
CP-AR models with CGS
●Run an estimation of a CP-AR(1) model
Run an estimation of a CP-AR(1) model
[Simu] = 
[Simu] = 
launch_CP_AR_Carlin_Gelman_Smith(US_GDP_growth,1,5000)
launch_CP_AR_Carlin_Gelman_Smith(US_GDP_growth,1,5000)
plot(Simu.post_tau')
plot(Simu.post_tau')

 
 
31
CP-AR models with Chib
●Change the directory and go to 
Change the directory and go to CP_models_with_Chib
CP_models_with_Chib
●Import data by clicking on 
Import data by clicking on US_GDP_percentage
US_GDP_percentage.mat
.mat
●Main matlab program 
Main matlab program Gibbs_regression_chib
Gibbs_regression_chib
●Estimates a CP model with k regimes using Chib's algorithm
Estimates a CP model with k regimes using Chib's algorithm
Inputs :
Inputs :
●Y : a time series 
Y : a time series (here US_GDP_growth)
(here US_GDP_growth)
●X : explanatory variables
X : explanatory variables
●Regime : number of regimes
Regime : number of regimes
●nb_MCMC : number of MCMC iterations
nb_MCMC : number of MCMC iterations
●MLL_computation : =1 if MLL must be estimated
MLL_computation : =1 if MLL must be estimated

 
 
32
CP-AR models with Chib
●Main matlab program 
Main matlab program Gibbs_regression_chib
Gibbs_regression_chib
●Estimates a CP model with k regimes using Chib's algorithm
Estimates a CP model with k regimes using Chib's algorithm
Inputs :
Inputs :
●Y : a time series 
Y : a time series (here US_GDP_growth)
(here US_GDP_growth)
●AR_lags : The order of the AR process
AR_lags : The order of the AR process
●upper_bound_regime
upper_bound_regime : Max. considered number of regimes
 : Max. considered number of regimes
●nb_MCMC : number of MCMC iterations
nb_MCMC : number of MCMC iterations
●Matlab program 
Matlab program launch_CP_model_estimations
launch_CP_model_estimations
●Estimates CP-AR models from one up to k regimes 
Estimates CP-AR models from one up to k regimes 

 
 
33
CP-AR models with Chib
●Run an estimation of CP-AR(1) models from one to 3 regimes
Run an estimation of CP-AR(1) models from one to 3 regimes
[Simu MLL] = 
[Simu MLL] = 
launch_CP_model_estimations(US_GDP_growth,1,3,10000)
launch_CP_model_estimations(US_GDP_growth,1,3,10000)
nb_MCMC
nb_MCMC
●Two regimes
Two regimes
Griddy-Gibbs
Griddy-Gibbs
Chib
Chib

 
 
34
Gibbs sampler
●The model 
The model 
●The prior distributions 
The prior distributions 
and
and
Transition prob. From 
Transition prob. From ii to 
 to ii : :
Trans. State :
Trans. State :

 
 
35
Gibbs sampler
●The model 
The model 
Priors in the program : 
Priors in the program : Gibbs_regression_chib
Gibbs_regression_chib

 
 
36
Forward-Backward
●Gibbs step :
Gibbs step :
Program for sampling a state vector : 
Program for sampling a state vector : Forward_Backward
Forward_Backward
Two steps :
Two steps :
1) Compute the forward prob.
1) Compute the forward prob.
2) Sample a state using the decomposition
2) Sample a state using the decomposition

 
 
37
Marginal likelihood
●Local formula :
Local formula :
Likelihood (by F-B)
Likelihood (by F-B)
Priors
Priors
●Third term :
Third term :
1)
1)
11
22
33

 
 
38
Marginal likelihood
2)
2)
●Run an auxiliary MCMC with fixed P* 
Run an auxiliary MCMC with fixed P* 
3)
3)
●Run second auxiliary MCMC with fixed  
Run second auxiliary MCMC with fixed  

 
 
39
Model selection
●Run an estimation of CP-AR(1) models from one to 3 regimes
Run an estimation of CP-AR(1) models from one to 3 regimes
[Simu MLL] = 
[Simu MLL] = 
launch_CP_model_estimations(US_GDP_growth,1,3,10000)
launch_CP_model_estimations(US_GDP_growth,1,3,10000)
Uninformative prior :
Uninformative prior :
#Regime
1
2
3
MLL
-275.57
-269.25
-275.10
Informative prior :
Informative prior :
#Regime
1
2
3
MLL
-265.28
-249.96
-243.52

 
 
40
Model selection
●Choose your prior according to 
Choose your prior according to 'your break sensitivity'
'your break sensitivity'
●Or use another criterion such as the predictive likelihood
Or use another criterion such as the predictive likelihood
Less impacted by the prior distributions
Less impacted by the prior distributions
No prior distributions!
No prior distributions!
Impact through the 
Impact through the 
posterior
posterior

