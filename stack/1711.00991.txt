The neighborhood lattice for encoding partial correlations in a
Hilbert space∗
Arash A. Amini, Bryon Aragam, Qing Zhou
February 7, 2019
Abstract
Neighborhood regression has been a successful approach in graphical and structural
equation modeling, with applications to learning undirected and directed graphical models.
We extend these ideas by deﬁning and studying an algebraic structure called the neighbor-
hood lattice based on a generalized notion of neighborhood regression. We show that this
algebraic structure has the potential to provide an economic encoding of all conditional
independence statements in a Gaussian distribution (or conditional uncorrelatedness in
general), even in the cases where no graphical model exists that could “perfectly” encode
all such statements. We study the computational complexity of computing these struc-
tures and show that under a sparsity assumption, they can be computed in polynomial
time, even in the absence of the assumption of perfectness to a graph. On the other hand,
assuming perfectness, we show how these neighborhood lattices may be “graphically” com-
puted using the separation properties of the so-called partial correlation graph. We also
draw connections with directed acyclic graphical models and Bayesian networks. We derive
these results using an abstract generalization of partial uncorrelatedness, called partial or-
thogonality, which allows us to use algebraic properties of projection operators on Hilbert
spaces to signiﬁcantly simplify and extend existing ideas and arguments. Consequently,
our results apply to a wide range of random objects and data structures, such as random
vectors, data matrices, and functions.
1
Introduction
Ascertaining the dependency structure of a system is a central task in machine learning and
statistics. Accordingly, there are many tools for this including regression analysis, graphical
models, factor analysis, and structural equation models (SEM). Unsurprisingly, these models
are closely related [Wer80; Pou99], and the pros and cons of each approach are well-studied.
At the same time, SEM-driven techniques have evolved as a useful tool for analyzing graphical
models in high-dimensions [GB13; LB14; AAZ16; Drt+18]. As a result, understanding the
relationship between the partial regression coeﬃcients deﬁned by a statistical model is an
important problem in statistics. In this paper, we report some interesting properties of these
partial regression coeﬃcients and connect them to the properties of the underlying partial
correlation graph (PCG). Our goals are twofold: (a) To introduce a new algebraic structure
∗This work was supported by NSF grant IIS-1546098.
1
arXiv:1711.00991v2  [math.ST]  6 Feb 2019

called the neighborhood lattice for encoding dependency structures under minimal assumptions,
and (b) In doing so, to highlight some new connections between SEM and graphical models.
To motivate our results, consider the simple case of a Gaussian random vector X =
(X1, . . . , Xd).
To understand the relationship between the variables in X, it is natural to
inquire about the partial regression coeﬃcients deﬁned by regressing each Xj onto the rest of
the nodes X−j = {X1, . . . , Xd} \ {Xj}, the so-called neighborhood regression problem [MB06].
These coeﬃcients encode a set of conditional independence (CI) relationships, namely, a zero
coeﬃcient implies that two nodes are conditionally independent given the rest of the variables.
In this way the partial regression coeﬃcients provide a convenient encoding of the dependency
structure of the random vector X. Moreover, this structure is conveniently represented via
the well-known Gaussian graphical model; see [Lau96] for details. This procedure generalizes
to various non-Gaussian models such as Ising models [RWLO10], exponential random families
[YRAL15], and semiparametric families [LLW09].
On the other hand, the (undirected) graphical model described above, in general, does
not encode all the CI statements present in the underlying Gaussian distribution. The case
where the distribution and the graph encode the same set of CI statements is known as the
perfect case. However, perfectness can be easily violated if the distribution is generated, say,
from a true directed acyclic graph (DAG), sometimes referred to as a set of recursive SEMs.
In these cases, there could be a DAG model perfect with respect to the distribution, in the
sense of encoding exactly the same set of CI statements in the distribution, via the so-called
d-separation in the DAG. Yet, there are still many distributions that are not covered by these
two cases, i.e., not perfect with respect to any directed or undirected graphical model.
In this work, we explore an alternative structure (i.e., other than a graph), which is deﬁned
by a generalization of classical neighborhood regression and can be used to economically encode
all of the CI statements in a Gaussian distribution, without assuming perfectness relative to a
graph. More precisely, we wish to ﬁnd all triplets (j, k, T) such that Xj ⊥⊥Xk | XT , where T ⊂
{1, . . . , d}\{j, k}. A complete list can be produced by regressing Xj onto XS := (Xk, k ∈S) for
general subsets S ⊂[d]j := {1, . . . , d} \ {j}, which we call generalized neighborhood regression.
Unfortunately, if we na¨ıvely consider all possible subsets, the problem explodes from a single
neighborhood [d]j into 2d−1 neighborhoods. It is thus of interest to encode these generalized
neighborhood regression problems more compactly.
One of the main contributions of this paper is to show that generalized neighborhood
regression problems have a rich algebraic structure which we call the neighborhood lattice, and
that under certain natural sparsity assumptions, this structure in fact provides an economical
encoding of all of the CI relations in X. The neighborhood lattice is interesting in and of itself,
both in how it provides an alternative to graphical models and in its intrinsic algebraic structure
which has many useful properties. Among these useful properties, we will discuss potential
computational savings when evaluating all possible conditional independence statements via
the lattice (Section 3.5) and also some connections with the complexity of learning DAG models
(Section 5.2).
In the cases where the distribution is perfect to a conditional independence graph (CIG),
we provide simple rules for graphical computation of the neighborhood lattices via the notion
of graph separation in the CIG, thus showing how these algebraic representations connect
with more familiar graphical models, when such models are possible.
We develop all our
results in the abstract setting of variables in a general Hilbert space H, where the notion
of independence is replaced with orthogonality, conditional independence is replaced with a
2

corresponding notion of partial (or conditional) orthogonality, and a CIG is replaced with a
so-called partial correlation graph. Thus, our results extend beyond the Gaussian case as long
as we replace conditional independence with the weaker notion of partial orthogonality. This
connection between the axioms of conditional independence and more general operations such
as Hilbert space projections is well-known [Pea88; Lau96; Daw01], and our results are most
naturally phrased in this language.
1.1
Contributions of this work
At a high-level, we make the following speciﬁc contributions:
• We prove that a generalized notion of neighborhood regression has the structure of a
complete, convex lattice (Theorem 1) that can be computed eﬃciently (Proposition 2).
• The resulting lattice decomposition has a one-to-one correspondence with all the CI
statements among X1 . . . , Xd (Theorem 2). In the absence of perfectness, we provide
alternative conditions under which the lattice decomposition, hence all CI statements,
can be computed in polynomial time (Theorem 3).
• Under the perfectness assumption, we provide two characterizations of these lattices in
terms of the separation properties of the PCG (Theorem 4) and its connected components
(Theorem 5).
As far as we know, the lattice property of neighborhood regression has not been noted before
in the literature. This useful property has interesting implications for the representation of
CI relations (Section 3.4) as well as connections to the problem of learning DAG models from
data (Section 5.2). As a bonus, we develop this theory in considerable generality, adopting the
idea of replacing conditional independence with partial orthogonality from previous work in
the graphical modeling literature. This also allows us to give largely self-contained proofs of
all our results using operator-theoretic arguments.
1.2
Overview of the main results
Let us give an overview of our results in the context of a motivating example. Assume that
we have a collection of random variables X1, . . . , Xd that are jointly Gaussian with covariance
matrix Σ ∈Rd×d. Let βj(S) be the coeﬃcient vector resulting from regressing Xj onto XS,
which we refer to as SEM coeﬃcients. By varying j and S, one can capture all CI relations
in the sparsity pattern (i.e. supports) of the resulting SEM coeﬃcients βj(S). Moreover, one
might be interested in these coeﬃcients in their own right, not necessarily as a means to deriving
CI relations.
Consider for example the inverse covariance matrix Γ = Σ−1 shown in Figure 1(b). The
sparsity pattern of Γ can be thought of as the adjacency matrix of an undirected graph, the
CIG associated with variables X1, . . . , Xd (Figure 1(c)). Consider for example, regressing X3
on {X8, X9, X11, X12, X14}:
β3({9, 8, 11, 12, 14}) = argmin
β ∈R5 E[X3 −(X8, X9, X11, X12, X14)β]2.
3

5
10
15
5
10
15
−1.0
−0.5
0.0
0.5
1.0
1.5
2.0
5
10
15
5
10
15
−0.6
−0.4
−0.2
0.0
0.2
0.4
0.6
0.8
1.0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
(a)
(b)
(c)
Figure 1: A Markov perfect example, with inverse covariance generated randomly: (a) covariance matrix,
(b) inverse covariance matrix, and (c) PCG. In (c) nodes are colored to help graphical computation of
Tj(S) for j = 3 and S = {8, 9, 11, 12, 14}. See Examples 2 and 4.
It turns out that in this regression, the coeﬃcients of X8 and X11 are zero. That is,
supp
 β3({9, 8, 11, 12, 14})

= {9, 12, 14}.
Which other variables besides those in S = {8, 9, 11, 12, 14} can we include in this regression
without the support changing? More generally, what are the other sets S′ ⊂[d] \ {j} such
that βj(S′) has the same support as βj(S)? Our main result (Theorem 1) shows that the set
of such S′ has a rich algebraic structure, namely, it is a convex lattice under the inclusion
ordering.
In particular, associated with every j and S, there is a smallest and a largest
set, mj(S) and Mj(S) ⊂[d] \ {j}, respectively, such that regressing Xj onto them results
in the same support for the regression coeﬃcient vector. In this example, clearly mj(S) =
{9, 12, 14}, but the largest set is often nontrivial, and in this example it turns out to be
Mj(S) = {9, 8, 11, 12, 14, 4, 5, 7, 10, 13, 15}. A consequence of this result, for example, is that
we can write
X3 = β∗
9X9 + β∗
12X12 + β∗
14X14 + ε∗
(1)
where ε∗is independent of all the variables in Xk, k ∈Mj(S). Since Mj(S) is the largest set
by deﬁnition, there is no larger set with this property.
Any set S′ including mj(S) and contained in Mj(S) has the property that supp(βj(S)) =
supp(βj(S′)), and those are the only sets with this property. We denote this collection of sets
as Tj(S) and write this in the interval notation
Tj(S) = [mj(S), Mj(S)]
= {S′ : mj(S) ⊂S′ ⊂Mj(S)}
=

S′ : {9, 12, 14} ⊂S′ ⊂{9, 8, 11, 12, 14, 4, 5, 7, 10, 13, 15}
	
.
(2)
The regression coeﬃcients in (1) will be the same for all subsets in Tj(S) and will be diﬀerent
for any subset outside Tj(S). The lattice Tj(S) compactly encodes many CI statements. In
particular, we will show (Section 3.4) that lattice (2) encodes Xi ⊥⊥Xj | XT for any
i ∈Mj(S) \ mj(S) = {8, 11, 4, 5, 7, 10, 13, 15},
and
T : mj(S) ⊂T ⊂Mj(S) \ {i}.
(3)
Moreover, Tj(S) can be computed eﬃciently with O(d) projections, a consequence of the con-
vexity of the lattice (Section 3.5).
4

We also observe that the neighborhood lattices Tj(S), S ⊂[d]\{j} provide a partition of the
power set of [d]\{j}, giving a complete picture of the relation of Xj to other variables. We refer
to this partition as the lattice decomposition of node j (Deﬁnition 4) and show that calculating
this decomposition provides us with all the CI statements involving node j (Section 3.4). We
also provide conditions under which the decomposition can be computed in polynomial time
(Section 3.5).
The covariance matrix Σ in this simple example was chosen to be perfect (see Section 4.2
for the deﬁnition), so that the reader can reason about the statements in the above discussion
by inspecting the CIG. However, the lattice property of Tj(S) (i.e., Theorems 1 and 2) is true
in general, without any perfectness assumption. It is known that without perfectness, one
cannot infer all CI statements in the joint distribution from graph separation. Therefore, the
neighborhood lattice is a more general representation for CI statements than a CIG. If Σ is
indeed perfect, we develop simple graphical rules to compute Tj(S) from the CIG associated
with Γ = Σ−1 (Theorems 4 and 5).
All these ideas carry out in the more general setting of abstract PCGs, with conditional
independence replaced with the notion of partial orthogonality (PO). Although this general
setting has appeared in the literature before, we brieﬂy give an overview in Sections 3.1, 4.1
and 4.2 using the language of projection operators which simpliﬁes many arguments.
For
completeness, other aspects of the theory of abstract PO and PCGs will be developed in
Appendix A.
1.3
Related work
Early work relating the properties of partial regression coeﬃcients in a structural equation
model to the independence properties of a Gaussian model include Wright [Wri21; Wri34],
Dempster [Dem69], and Cram´er [Cra46]. The general problem of encoding conditional inde-
pendence (and more generally, partial uncorrelatedness) has a long history. For a historical
summary and early work on this problem, we refer the reader to textbooks such as [Pea88]
and [Lau96]. Graphical models are the most common encoding, whereby conditional indepen-
dence is encoded via separation properties in a graph. These properties can be axiomatized
via the semi-graphoid and graphoid axioms, which has led to a rich literature on the axiomatic
foundations of conditional independence; see [Pea88; Stu06] and the references therein. More
recently, the literature has considered various extensions of traditional undirected and directed
graphs to encode more general independence structures [e.g. ER14; HMR14; RERS17; SL+14;
LS+18]. To the best of our knowledge, the lattice property of partial uncorrelatedness has not
yet been explored, although the connection between orthogonal projections and the axioms of
conditional independence is of course well-known [Lau96; Daw01; Whi09].
2
Preliminaries
In this section, we review some preliminaries and background material that is useful for setting
the stage of our general theory. The material on projections in Section 2.3 in particular is
essential for the deﬁnitions and results in the sequel.
5

2.1
Notation
Graph notation.
In an undirected graphical model for a random vector X = (X1, . . . , Xd),
we identify each random variable Xj, j = 1, . . . , d, with a node in an undirected graph G =
([d], E), where [d] = {1, . . . , d}.
Two nodes i and j are adjacent, or neighbors, if (i, j) ∈
E, in which case we write i ∼j, otherwise i ≁j.
A path from i to j is a sequence i =
k1, k2, . . . , kn−1, kn = j ∈[d] of distinct elements with (kl, kl+1) ∈E for each l = 1, . . . , n −1.
Given two subsets A, B ⊂[d], a path connecting A to B is any path with k1 ∈A and kn ∈B.
A subset C ⊂[d] separates A from B, denoted by A −C −B, if all paths connecting A to B
intersect C (i.e. kl ∈C for some 1 < l < n), otherwise we write ¬(A −C −B). Implicit in this
deﬁnition is that A, B and C are disjoint.
Subset notation.
For any A ⊂[d], we denote XA = {Xi : i ∈A}. We also use the shorthand
notations: {i} = i and {i, j} = ij, A ∪{i} = Ai, A ∪B = AB and so on. In addition, we let
[d]S = [d]\S = {1, . . . , d}\S. Common uses of these notational conventions are: [d]j = [d]\{j}
and [d]ij = [d]\ij = [d]\{i, j}. For a matrix Σ ∈Rd×d, and subsets A, B ⊂[d], we use ΣA,B for
the submatrix on rows and columns indexed by A and B, respectively. Single index notation
is used for principal submatrices, so that ΣA = ΣA, A. For example, Σi,j is the (i, j)th element
of Σ (using the singleton notation), whereas Σij = Σij, ij is the 2 × 2 submatrix on {i, j} and
{i, j}. Similarly, ΣAi,Bj is the submatrix indexed by rows A ∪{i} and columns B ∪{j}.
2.2
Graphical models
For completeness, we recall here some of the traditional deﬁnitions from the graphical modeling
literature. This is mainly to provide a basis for comparison, and is not needed for most of the
results we prove. A more thorough treatment can be found in [KF09; Lau96].
In the context of undirected graphs, there are three so-called Markov properties of interest:
G satisﬁes the
• pairwise Markov property if (i, j) /∈E implies that Xi ⊥⊥Xj | X[d]ij,
• local Markov property if each node is conditionally independent of all other variables
given its neighbours,
• global Markov property if A −C −B implies that XA ⊥⊥XB | XC.
The notation X1 ⊥⊥X2 | X3 means X1 is (probabilistically) independent of X2 conditioned on
X3. In general, these conditions are not equivalent and we have global =⇒local =⇒pairwise,
however, for positive distributions, these conditions are in fact equivalent [Lau96]. A graph G
satisfying the pairwise Markov property is sometimes called a conditional independence graph
for the distribution of X.
The Markov properties go in one direction: The graphical criterion (i.e. separation) implies
a probabilistic conclusion (i.e. conditional independence). The converse is not true in general;
when it is, the graph G is called perfect with respect to the joint distribution of X. Speciﬁcally,
G is called perfect if A −C −B ⇐⇒XA ⊥⊥XB | XC.
6

2.3
The lattice of projections
We will assume the reader is familiar with the basic theory of Hilbert spaces and their projec-
tions; a detailed introduction to these topics can be found in [FW10; Bla06]. For a (separable)
Hilbert space H, let B(H) be the space of bounded linear operators on H. For an operator
P ∈B(H), let ran(P) := PH := {Px : x ∈H} denote its range and P ∗its adjoint, deﬁned
via the relation ⟨x, P ∗y⟩= ⟨Px, y⟩for all x, y ∈H. Here and in the sequel, we use ⟨·, ·⟩H or
the shorthand ⟨·, ·⟩to denote the inner product of H.
We recall that P ∈B(H) is an orthogonal projection if and only if it is self-adjoint and
idempotent: P ∗= P = P 2. The set of orthogonal projections in B(H), denoted as P(H), is of
particular importance in this paper. P(H) can be partially ordered as follows:
Lemma 1. For P, Q ∈P(H), the following are equivalent:
(a) P = PQ,
(b) P = QP,
(c) ran(P) ⊂ran(Q).
When any of these conditions hold we write P ≤Q.
The equivalence of (a) and (b) follows from self-adjointness of orthogonal projections. Note
that when P and Q are both projections, PQ is not necessarily a projection (unless P and Q
commute). The above ordering makes P(H) into a complete lattice, that is, a partially ordered
set S in which each subset has both a supremum (join) and an inﬁmum (meet) in S. For
P, Q ∈P(H), we denote the supremum with P ∨Q and the inﬁmum with P ∧Q. We have
P ∧Q = the projection onto ran(P) ∩ran(Q)
P ∨Q = the projection onto the closed linear span of ran(P) ∪ran(Q).
We also let P ⊥:= I −P, the orthogonal complement of P. Note that P ∨P ⊥= I (the identity
operator) and P ∧P ⊥= {0}. Also, P ≤Q iﬀP ⊥≥Q⊥. See for example Farah and Wofsey
[FW10, Section 5, p. 24] and Blackadar [Bla06, Section II.3.2, p. 78] and the references therein.
We often consider the concrete example where H = L2(P), the space of random variables
(on a probability space with measure P) with ﬁnite second moments.
When applied to a
random variable X, P ⊥X = X −PX may be interpreted as the residual between X and its
projection PX. For this reason, when x is a vector in an arbitrary Hilbert space H we will
refer to P ⊥x as the residual of the projection Px. We also need the notion of orthogonality of
operators:
Lemma 2. For P, Q ∈B(H), the following are equivalent:
(a) ⟨Px, Qy⟩= 0, ∀x, y ∈H,
(b) P ∗Q = Q∗P = 0,
(c) ran(P) ⊥ran(Q).
When any of the conditions above hold, we call P and Q orthogonal and denote this by P ⊥Q.
3
General results on neighborhood lattices
In contrast to classical probability theory—which works with random variables and the notions
of independence and correlation—we will develop our results in the abstract setting of a general
Hilbert space H. Since this extension might be unfamiliar to readers, in Section 3.1 we give
an outline of this theory. More details on this framework can be found in Appendix A. In
particular, the notion of partial correlation among variables is replaced with that of partial
7

orthogonality (cf. (4))—which is well-deﬁned in any Hilbert space— and will play a prominent
role in the rest of the paper. Later in section, we state our main results on neighborhood
lattices and how they encode all partial orthogonality statements.
3.1
Partial orthogonality
We start with a generalized deﬁnition of partial uncorrelatedness (or conditional independence
in the case of Gaussian variables). Throughout, we write Hd = {(x1, . . . , xd) : xi ∈H, i ∈[d]}
for some positive integer1 d and a separable Hilbert space H. Fix a vector x = (x1, . . . , xd) ∈
Hd, and for any S ⊂[d], let PS ∈P(H) denote the projection onto the span of xS = {xi, i ∈S}.
For simplicity, we write Pj = P{j} to denote projection on the span of {xj}, i.e., we drop the
brackets for singleton sets.
Deﬁnition 1 (Partial orthogonality). For any disjoint triple of subsets A, S, B ⊂[d], we say
that variables xA are partially orthogonal to xB given xS if
PAP ⊥
S PB = 0,
(4)
in which case we also write PA ⊥PB | PS or (A ⊥B | S).
We will also view (4) as a ternary operation among projection operators and say that “PA
is partially orthogonal to PB given PS”. Since PAP ⊥
S PB = PBP ⊥
S PA due to self-adjointness
of projections, Deﬁnition 1 is symmetric in A and B.
Note the use of ⊥here to denote
orthogonality in H, as opposed to ⊥⊥which is reserved for probabilistic independence.
In
particular, recall that P ⊥
S := I −PS is the orthogonal complement of PS (Section 2.3). We
refer to (4) as partial orthogonality, or conditional orthogonality, a generalization of the notion
of partial uncorrelatedness of random variables. For H = L2(P), we recover the usual notion
for random variables:
Example 1. For a probability measure P, let H = L2(P), the space of square-integrable random
variables: Xi ∈L2(P) iﬀEX2
i < ∞. The inner product is ⟨X, Y ⟩L2 = EXY . Consider zero-
mean random variables X1, . . . , Xd ∈L2(P). In this case, PS corresponds to the L2 projection
on the span of XS, or regressing on (covariates) Xi, i ∈S in statistical terms. Similarly, P ⊥
S Xi
is the residual after regressing Xi on XS.
Now, ﬁx some S ⊂[d]. The usual partial correlation between Xi and Xj given XS is the
correlation between the residuals P ⊥
S Xi and P ⊥
S Xj, that is, ⟨P ⊥
S Xi, P ⊥
S Xj⟩L2. Thus, the partial
correlation is zero iﬀP ⊥
S Xi ⊥P ⊥
S Xj (orthogonality in the L2 sense). Since span{P ⊥
S Xj} =
ran(P ⊥
S Pj), using Lemma 2(c), the partial correlation is zero iﬀP ⊥
S Pi ⊥P ⊥
S Pj. This in turn
is equivalent to PiP ⊥
S Pj = 0, by Lemma 2(b). Thus, for random variables in L2, the notion of
partial orthogonality given in Deﬁnition 1 coincides with that of partial uncorrelatedness.
Let Σ = (⟨xi, xj⟩H) ∈Rd×d be the Gram matrix of the underlying variables x = (xj, j ∈
[d]). Unless otherwise stated, we will assume the following from now on:
Σ := (⟨xi, xj⟩H) ≻0.
(A1)
The ternary relation (4) among projections is also completely characterized by the Gram matrix
Σ as discussed in Appendix A.2.1.
1We assume d < ∞for simplicity, though most of our results hold also for d = ∞.
8

3.2
Neighborhood regression
It is well-known that neighborhood regression provides an alternative characterization of the
partial orthogonality relation (4). As will be shown in Section 7.2, we can verify the relation
Pi ⊥Pj | PS, by looking at the coeﬃcients of the regression of xj onto {xk, k ∈Si}, as opposed
to the residuals.
In the literature on undirected graphical models, the neighborhood of a ﬁxed node xj is
implicitly understood to be the set of all other variables, namely the collection x−j. Here, we
extend the notion of neighborhood to any subset S ⊂[d]j, which is necessary for evaluating
general conditional orthogonality statements, such as xi ⊥xj | xS.
(See Section 5 for a
discussion of how this general setup is needed for learning directed extensions of the PCG.)
The neighborhood regression problem for the node xj and a subset S ⊂[d]j is the problem of
ﬁnding the linear projection of xj onto xS:
Deﬁnition 2 (SEM coeﬃcients). For any S ⊂[d]j, let
βj(S) :=
argmin
β ∈Rd, supp(β) ⊂S
∥xj −βT x∥2
H.
(5)
We call βj(S) the SEM coeﬃcients for variable j regressed on the variables S. Here, βT x =
Pd
j=1 βjxj for any β ∈Rd and x ∈Hd.
In Section 7.2, we will show that to test Pi ⊥Pj | PS for all S ⊂[d]ij, it is enough to
have {βj(T) : T ⊂[d]j}, motivating the study of such collections (cf. Proposition 3) . It turns
out that these neighborhood regression problems have rich algebraic properties which will be
explored in the next section.
3.3
Neighborhood lattice
We now state our main results. We start by introducing the the main object of study in the
paper, which we call the neighborhood lattice.
Deﬁnition 3 (Neighborhood lattice). For any S ⊂[d]j, deﬁne a collection of subsets by
Tj(S) = {T ⊂[d]j : PT Pj = PSPj}.
(6)
In other words, for any j, Tj(S) is the collection of candidate sets T ⊂[d]j such that the
projection of xj onto {xi, i ∈T} is invariant. Deﬁnition 3 is somewhat abstract, though it
highlights the algebraic nature of the collection Tj(S) and the suﬃciency of the projection
operators {PS, S ⊂[d]} in deﬁning it. Note that Tj(S) as given in (6) is well-deﬁned even
when the Gram matrix Σ is rank deﬁcient. In order to see the usefulness of this deﬁnition,
assume in addition that Σ is (strictly) positive deﬁnite. Then we have the following alternative
representations of Tj(S) (Lemma 10 in Section A.1):
Tj(S) = {T ⊂[d]j : βj(T) = βj(S)}
=

T ⊂[d]j : supp(βj(T)) = supp(βj(S))
	
.
(7)
By deﬁnition, supp(βj(S)) ⊂S. We are primarily interested in cases where supp(βj(S)) is much
smaller than S. In addition, we would like to know all the other sets T for which supp(βj(T))
is the same as this small set.
9

Our goal in this paper is to study the algebraic properties of Tj(S), for which represen-
tation (6) is very helpful. Throughout, we ﬁx j ∈[d] and S ⊂[d]j. Our next result shows
that Tj(S) is indeed a lattice, as suggested by the name. Recall that a complete lattice is a
partially ordered set, or poset for short, in which all subsets have both a supremum (join) and
an inﬁmum (meet) [Sta97, Section 3.3]. We also need the following deﬁnition: Let (P, ≤) be
a poset and C a subposet of P. We say that C is convex (in P) if z ∈C whenever x < z < y
with x, y ∈C. A closed interval [x, y] := {z ∈P :
x ≤z ≤y} is an example of a convex
subposet [Sta97, Section 3.1].
Theorem 1. Under (A1), Tj(S), ordered by inclusion, is a complete lattice. In particular,
Tj(S) has unique minimal and maximal elements, which we denote by mj(S) and Mj(S),
respectively. Moreover, Tj(S) is convex as a subposet of 2[d]j.
Any ﬁnite nonempty lattice is in fact complete, however, the proof we give in Section 7.1
also establishes the result for an inﬁnite collection of variables {xi}. Tj(S) being convex in 2[d]j
means the following: For every two subsets S1, S2 ∈Tj(S), and any S′ ⊂[d]j, if S1 ⊂S′ ⊂S2,
then S′ ∈Tj(S). Thus, using the interval notation, we can represent the neighborhood lattice as
Tj(S) = [mj(S), Mj(S)] = {A ∈2[d]j : mj(S) ⊂A ⊂Mj(S)}.
(8)
In other words, Tj(S) is speciﬁed by its minimum and maximum elements, and it is actually
isomorphic to the lattice of the power set 2[r] with r = |Mj| −|mj|. In Section 4, we describe
the abstract sets mj(S) and Mj(S) in terms of the PCG G (Theorems 4 and 5), which gives
a useful semantic interpretation in terms of separation properties of a graph. Theorem 1 is in
fact a special case of a much more abstract result discussed in Appendix C.
Example 2. Consider the covariance matrix Σ shown in Figure 1(a), with dimension d =
15. The corresponding PCG is plotted in Figure 1(c). For j = 3 and S = {9, 8, 11, 12, 14},
the minimal set is mj(S) = {9, 12, 14}, and maximal set is Mj(S) = {4, 5, 7, 8, 9, 10, 11, 12,
13, 14, 15}. These can be easily read from the graph as will be described in Section 4. Since the
lattice can be represented as Tj(S) = [mj(S), Mj(S)], we know for example that {9, 12, 14, 5}
and {9, 12, 14, 10, 13} both belong to Tj(S). Note that these two sets are not comparable in
2[d]j, i.e., Tj(S) is not a chain (or totally ordered set) in 2[d]j. In this case, Tj(S) contains
2|Mj|−|mj| = 28 = 256 subsets, out of a total possible |2[15]3| = 214. Regressing j = 3 onto any
of these 256 subsets results in the same SEM coeﬃcients.
An alternative way to view Tj(S) is as a collection of subsets of {xi : i ∈[d]j} that have
the same explaining power as S for xj. This view can be further advanced by removing the
reference to the set S. A node j deﬁnes an equivalence relation among subsets of [d]j, where
S, T ⊂[d]j are equivalent iﬀPSPj = PT Pj. We then say that S and T have the same explaining
power for {j}. This relation partitions 2[d]j into equivalence classes, each of which is a complete
lattice according to Theorem 1. For future reference, let us give this partition a name:
Deﬁnition 4 (Lattice decomposition). For any j ∈[d], we call the partition of 2[d]j into
neighborhood lattices, the (neighborhood) lattice decomposition of node j, denoted as Tj.
More precisely, each element Tj ∈Tj is an interval Tj = [mj, Mj] as in Theorem 1, and we
have S Tj = 2[d]j and Tj ∩T ′
j = ∅for distinct Tj, T ′
j ∈Tj.
10

Remark 1. It is not hard to see that mj(S) = supp(βj(S)), that is, mj(S) represents the
so-called active set when regressing xj onto xS, which is of course of interest in regression and
graphical models. Thus, there is a one-to-one correspondence between the neighourbohood
lattices Tj(S), the equivalence classes deﬁned above, and the possible active sets for predicting
xj. Furthermore, a consequence of Theorem 1 is that there will be a largest set Mj(S) possibly
larger than the S we started with, regressing onto which has the same active set. Regressing
onto any set strictly larger than Mj(S) will change the active set.
3.4
Enumerating all partial orthogonality statements
An immediate application of Theorem 1 is that it provides an economical way of enumerating
all partial orthogonality (PO) statements that hold among variables xi, i ∈[d] without any
perfectness assumption. It is enough to focus on statements of the form Pi ⊥Pj | PT for
any T ⊂[d]ij which we represent compactly as (i ⊥j | T), implicitly assuming that T does
not include {i, j}. We further ﬁx j, i.e., restrict attention to all PO statements involving a
particular node j. As mentioned earlier, when the variables are prefect w.r.t. to a graphical
model (either undirected or directed), inferring the underlying PCG is often a good economical
way of representing all PO statements. However, without assuming perfectness, one has to
verify each statement (i ⊥j | T) for all i ̸= j and all T ⊂[d]ij. Theorem 1 shows that once
we identify a neighborhood lattice Tj, we can obtain many of these statements for free. More
precisely, we have:
Proposition 1. Let Tj = [mj, Mj] be a neighborhood lattice for node j. Assume that T and
{i} are disjoint subsets of [d]j, and T ∪{i} ∈Tj. Then (j ⊥i | T) if and only if i ∈Mj \ mj.
See Section 7.2 for the proofs of the results in this section.
Proposition 1 allows us to enumerate all PO statements assuming that we have the full
lattice decomposition Tj (Deﬁnition 4):
Theorem 2. Let Tj =

[mℓ
j, Mℓ
j ] : ℓ= 1, . . . , Kj
	
. Then j ⊥i | T if and only if there exists a
unique k ∈[Kj] such that
i ∈Mk
j \ mk
j ,
T ∈[mk
j , Mk
j \ {i}].
In short, once we have the lattice decomposition Tj for node j, then all the PO statements
involving node j can be listed as follows:
j ⊥i | T
for all
i ∈Mk
j \ mk
j ,
T ∈[mk
j , Mk
j \ {i}],
k = 1, . . . , Kj.
(9)
The total number of PO statements in (9) is
Kj
X
k=1
 |Mk
j | −|mk
j |

2|Mk
j |−|mk
j |−1,
(10)
where, for example, |Mk
j | is the cardinality of set Mk
j . (Note that the total number of all possible
PO triples involving a ﬁxed node j is (d−1)2d−2.) According to Theorem 2, there is no double-
counting in (9) and all the PO statements (involving j) are accounted for. Thus, the complexity
of enumerating all PO statements, in the absence of perfectness assumption, boils down to
11

Algorithm 1 Compute the lattice Tj(S) for a given j and S ⊂[d]j.
1: function computeLattice(j, S, Σ)
2:
βj(S) ←computeSEMcoeff(j, S, Σ)
3:
m ←supp(βj(S)).
▷This is mj(S).
4:
M ←S
5:
A ←[d] \ (S ∪{j})
6:
while A ̸= ∅do
7:
k ←(pop ﬁrst element oﬀA)
8:
P ←M ∪{k}
▷Proposal set
9:
c ←computeSEMcoeff(j, P, Σ)
10:
if supp(c) = m then M ←P
11:
end while
12:
return [m, M]
13: end function
14: function computeSEMcoeff(j,S,Σ)
15:
return βj(S) which is = Σ−1
S ΣS,j on S and 0 otherwise.
16: end function
the complexity of computing decomposition Tj. In Section 3.5, we show that computing an
individual lattice Tj has polynomial complexity and we also give suﬃcient conditions under
which computing Tj could be done in polynomial time. Thus, in the absence of a graphical
representation, the lattice decomposition provides an alternative “economical encoding” of PO
statements with potential for substantial savings.
3.5
Computational complexity
Theorem 1, combined with the observations in Remark 1, suggests an intuitive algorithm
for eﬃciently computing the entire neighborhood lattice Tj(S). Once we have the active set
mj(S), it is enough to compute Mj(S) by sequentially adding the rest of the nodes and either
including or rejecting them based on whether the active set is changed. The validity of this
approach follows from the convexity of the lattice. Once mj(S) and Mj(S) are computed, the
entire lattice is given by (8). This procedure is outlined formally in Algorithm 1. Since this
algorithm only requires d −1 −|mj(S)| = O(d) projections, we have the following perhaps
surprising result:
Proposition 2. For any j and S ⊂[d]j, it is possible to compute Tj(S) with O(d) projections.
Since each projection can be computed in polynomial time (i.e. O(d3)), a consequence of
Proposition 2 is that Tj(S) can be computed in polynomial time.
Now consider the question of computing the full lattice decomposition Tj which is needed in
general to enumerate all PO statements by Theorem 2. This can be done recursively, as detailed
in Algorithm 2. The key in this algorithm is step 4 where given a partial decomposition, say,
T′
j = {T 1
j , . . . , T K
j }, one needs to ﬁnd a set S that is not covered by T′
j, i.e., S /∈S T′
j =
SK
ℓ=1 T ℓ
j . If no such set exists, we conclude that T′
j covers the power set (i.e., S T′
j = 2[d]j),
hence it is in fact the full lattice decomposition and the algorithm terminates. For a general set
system T′
j, performing step 4 is NP-hard. However, since in our case the underlying lattices
are mutually disjoint, the problem can be solved in polynomial time:
12

Algorithm 2 Compute the lattice decomposition Tj for a given j .
1: Set T 1
j ←computeLattice(j, [d]j, Σ)
2: Initialize Tj ←{T 1
j }, K ←1 and powerSetNotCovered ←True.
3: while powerSetNotCovered do
4:
Find a set S ⊂[d]j not covered by Tj = {T 1
j , . . . , T K
j }, that is, S /∈SK
ℓ=1 T ℓ
j .
5:
if no such set S exists then
6:
powerSetNotCovered ←False.
7:
else
8:
T K+1
j
←computeLattice(j, S, Σ).
9:
Tj ←{T 1
j , . . . , T K
j , T K+1
j
}.
10:
K ←K + 1.
11:
end if
12: end while
Lemma 3. Given a set system T′
j = {T 1
j , . . . , T K
j } where T ℓ
j
= [mℓ
j, Mℓ
j ] ⊂2[d]j for all
ℓ= 1, . . . , K and T ℓ
j ∩T ℓ′
j
= ∅for ℓ̸= ℓ′, there is a polynomial-time algorithm to decide
whether T′
j covers the power set 2[d]j and if not produce a certiﬁcate (i.e., a set not covered by
T′
j). The algorithm requires at most O(d2K) set operations.
A set operation in the statement of Lemma 3 (and Theorem 3 below) involves the compu-
tation of the size of at most two set diﬀerences. We refer to the proof in Section 7.3 for more
details. Since the maximum K achieved in Algorithm 2 could potentially be much smaller
than 2d−1, the overall procedure could result in substantial savings relative to the naive ap-
proach of checking all the 2d−1 possible subsets. In general, we have the following result for
the complexity of Algorithm 2, and by proxy that of enumerating all PO statements:
Theorem 3. Assume that lattice decomposition Tj contains Kj lattices. Then, Tj can be
computed using at most O(Kj(d2Kj set+d projection)) operations. In particular, if |mj(S)| ≤k
for all S ⊂[d]j, then all partial orthogonality statements for the jth node can be computed in
polynomial time, with at most O(d2k+2 set + dk+1 projection) operations.
In other words, as long as no active set for xj has more than k elements, the computation of
the lattice decomposition is polynomial in d. The proof of Theorem 3 follows by combining
Proposition 2 and Lemma 3, and noting that under the bounded active set assumption, the
total number of lattices is bounded as Kj ≤
 d−1
k

≤dk. Whether there are other assumptions,
besides bounded active set, that could guarantee a polynomial number Kj of lattices is an
interesting open question.
For the interested reader, R code implementing Algorithms 1 and 2, as well as the graphical
computation of Section 4, is available at [Reg].
Example 3. Continuing with the example covariance matrix Σ shown in Figure 1(a), we apply
Algorithm 2 to compute the lattice decomposition Tj for node j = 3. The decomposition turns
out to have Kj = 319 lattices. Tables 1 and 2 contain some statistics about this decomposition,
namely, how many sets are covered by each lattice and the sizes of their minimum element (i.e.,
set m). For example, Table 1, shows that there are 5 lattices in the decomposition that cover
512 sets each. Similarly, there are 21 lattices whose minimal element is a set of size 5. In this
example, the maximum active set size is k = 5. We note that the bound
 d−1
k

=
 14
5

= 2002
13

Total
Number of sets covered
4
8
16
32
64
128
256
512
1024
2048
16384
Number of lattices
112
40
60
34
38
19
8
5
2
1
319
Table 1: The distribution of the number of sets covered by each lattice T = [m, M], i.e. 2|M|−|m|, for
the setup of Example 2.
size of m
0
1
2
3
4
5
number of lattices
1
12
60
134
91
21
Table 2: The distribution of the size of the minimal element of the lattice T = [m, M], i.e. set m, for
the setup of Example 2.
on the number Kj of lattices is quite conservative (Kj = 319 ≪2002) which is attributed to
the fact that many lattices in the decomposition have minimal elements of size smaller than
k. The total number of (valid) PO statements for j = 3, as given by (10), is 62,592. This
is the number of all PO statements involving node j = 3 that hold in this model, out of the
14 · 213 = 114, 688 possible such PO statements.
4
Graphical computation with PCG
So far we have developed properties of the neighborhood lattice and its computation, without
assuming the existence of a perfect graph G for the Gram matrix Σ. In this general situation,
the lattice decomposition encodes all partial orthogonality statements. When a perfect PCG
G exists, all PO statements can be read oﬀfrom graph separation in G. Then it is expected
that one can characterize the lattices from the graph G. In this section, we will show how
to compute a neighborhood lattice from a perfect PCG. We begin by generalizing the usual
notion of a PCG to the Hilbert space setting, and then extend the Markov perfectness deﬁned
in Section 2.2 based on this generalization.
4.1
Abstract partial correlation graphs
For any S ⊂[d] let PS ∈P(H) denote the projection onto the span of xS = {xi, i ∈S} and
recall that [d]ij = {1, . . . , d} \ {i, j}.
Deﬁnition 5 (Pairwise H-Markov, PCG). We say that x satisﬁes the pairwise H-Markov
property w.r.t. G if
i ≁j in G ⇐⇒PiP ⊥
S Pj = 0, for S = [d]ij,
(11)
in which case G is called a partial correlation graph of x.
Recalling Example 1 (Section 3.1), where H = L2(P) and we are dealing with random
variables X1, . . . , Xd ∈L2(P), Deﬁnition 5 deﬁnes the PCG as the graph where a missing edge
between a pair of nodes (i, j) corresponds to zero partial correlation between Xi and Xj given
the rest of the variables Xk, k ∈[d]ij. This is the usual notion of the PCG for a collection of
random variables. Deﬁnition 5 thus generalizes this notion to represent partial orthogonality
(Deﬁnition 1) in a Hilbert space.
14

In Example 1, it is well-known that if X = (X1, . . . , Xd) has a Gaussian distribution, then
the PCG deﬁned above is in fact a CIG. However, the PCG is deﬁned for any (joint) distribution
on X whose marginals have ﬁnite second moments, and in general it may not correspond to a
CIG. (It is usually sparser than a CIG.) In Appendix A.2.2, we give more concrete examples
of the abstract PCG of Deﬁnition 5 which go beyond the familiar case of Example 1.
4.2
H-Markov Perfectness
Let us now extend Deﬁnition 5. In analogy with the global Markov property in the context of
CIGs, we can deﬁne a global notion of the H-Markov property with respect to a graph:
Deﬁnition 6 (Global H-Markov). Given the setup of Deﬁnition 5, we say that x ∈Hd satisﬁes
the global H-Markov property w.r.t. G if
S separates A and B in G =⇒PAP ⊥
S PB = 0.
(12)
In the special case H = L2(P) (cf. Example 1), this reduces to the global Markov property for
an undirected graphical model, and hence Deﬁnition 6 naturally generalizes this property to
general Hilbert spaces.
We now introduce the notion of H-Markov perfectness, which is the PCG counterpart of
the notion of Markov perfectness in undirected graphical models. The deﬁnition of the global
H-Markov property in Deﬁnition 6 requires that graph separation implies partial orthogonality.
Perfectness upgrades this implication to hold both ways.
Deﬁnition 7 (Perfectness). We say that x is globally H-Markov perfect w.r.t. G if
S separates A and B in G ⇐⇒PAP ⊥
S PB = 0.
We often abbreviate “globally H-Markov perfect w.r.t. G” and just refer to x as perfect or
imperfect.
Markov perfectness is useful when making statements regarding the separation of nodes in
the PCG, in which case there is a one-to-one correspondence between separation on the graph
and the relation PA ⊥PB | PS among projections.
Remark 2 (Perfectness of Σ). There is no need to specify G in Deﬁnition 7, since it is implied
by x, due to the uniqueness of PCGs according to Deﬁnition 5. Since PCGs and H-Markov
properties can be equivalently characterized by the Gram matrix Σ = (⟨xi, xj⟩H) as discussed
in Appendix A.2.1, we can equivalently talk about perfectness of a Gram matrix Σ.
The
corresponding graph is also (uniquely) implied in this case, given by the sparsity pattern of
Σ−1; see (25) and Lemma 11.
4.3
Graphical characterization
Under the perfectness assumption (Deﬁnition 7), we can characterize the elements of Tj(S) in
terms of the separation in the underlying PCG. For any set S ⊂[d]j, deﬁne a new set by
S∗:=
\
{T ⊂S : T separates j and S \ T}.
(13)
Thus, S∗is the smallest subset of S that separates j and S \ S∗. See Lemma 7 (Section 7.4)
for the validity of this interpretation. Our next result gives the following characterization of
the minimal set mj(S), the maximal set Mj(S), and the entire lattice, via graph separation:
15

Theorem 4. Assume H-Markov perfectness. Fix j ∈[d], S ⊂[d]j, and deﬁne S∗by (13). Let
Ej(S∗) = {k : S∗separates k and j}. Then,
(a) mj(S) = S∗.
(b) Mj(S) = S∗∪Ej(S∗).
(c) Tj(S) = {S∗∪T : T ⊂Ej(S∗)}.
We next provide a characterization of the minimal and maximal sets of the neighborhood
lattice of j via connected components of the graph resulting from the removal of j:
Theorem 5. Under H-Markov perfectness, suppose that removing node j and the edges
connected to it breaks the PCG into K connected components given by the vertex subsets
G1, G2, . . . , GK ⊂[d]j. Let Sk = S ∩Gk. Then,
(a) mj(S) = U
k mj(Sk),
(b) Mj(S) = S
k Mj(Sk) = U
k Mj(Sk; Gk), where Mj(Sk; Gk) is the largest element of
Tj(Sk; Gk) := {T ⊂Gk : PT Pj = PSkPj}.
In these statements U denotes the disjoint union. Note that Tj(Sk; Gk) is the lattice restricted
to the ground set Gk ∪{j} (i.e. instead of [d]). The original lattice given in Deﬁnition 3 can be
thought of as Tj(Sk; [d]j). For illustrative purposes, some simple consequences of Theorems 4
and 5 are as follows:
(i) mj(S) = ∅iﬀthere is no path between j and S. (Separation by the empty set.)
(ii) If the PCG decomposes into two disjoint components, say G and H, then Mj(S) contains
H for any j ∈G and any S.
Both (i) and (ii) hold without the perfectness assumption, and can be shown directly. However,
the graphical view of Theorems 4 and 5 makes them immediately clear.
Example 4. Continuing with Example 2, let j = 3 and S = {9, 8, 11, 12, 14}. It is clear from
Figure 2(a) that the smallest subset S∗of S separating j from S \ S∗is {9, 12, 14}. Hence,
mj(S) = S∗according to Theorem 4. Similarly, Ej(S∗) = {4, 5, 7, 8, 10, 11, 13, 15} and Mj(S) =
S∗∪Ej(S∗). To verify Theorem 5, note that removing j breaks the PCG into three connected
components G1 = {1, 2, 8, 9, 13}, G2 = {7, 10} and G3 = {4, 5, 6, 11, 12, 14, 15}, as illustrated in
Figure 2(b). Applying Theorem 4, we can compute the restricted lattices Tj(S ∩Gk; Gk), k =
1, 2, 3. Using the notation [m, M] to represent a lattice with minimum and maximum elements
m and M, respectively, the three lattices are: [{9}, {8, 9, 13}], [{12, 14}, {4, 5, 11, 12, 14, 15}]
and [∅, {7, 10}]. It is clear that the minimal and maximal elements of the original lattice are
the disjoint union of the corresponding elements of these three lattices.
As Example 4 shows, graphical computation (assuming perfectness holds) could be more
eﬃcient than direct computation. The PCG can be constructed from the sparsity pattern of
the inverse Gram matrix, Σ−1, after which all the lattice computations reduce to establishing
certain connectivity criteria on the graph.
16

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
(a)
(b)
Figure 2: The graphical computation in Example 4 of the neighborhood lattice Tj(S) for j = 3 and
S = {9, 8, 11, 12, 14}. (a) Original PCG with j and S speciﬁed with diﬀerent colors. From this ﬁgure,
it is easy to see that S∗= {9, 12, 14} is the smallest subset of S that separates j from S \ S∗, hence
mj(S) = S∗. (b) Illustration of the three connected components which result after removing node j = 3.
As a result of Theorem 5, we can work in each component separately. For example, restricted to the left
component G1 = {1, 2, 8, 9, 13}, the minimal subset of S ∩G1 = {8, 9} that separates j = 3 from the
rest of S ∩G1 is S∗
1 = {9}. Within G1, S∗
1 separates {8, 13} from j = 3. Hence, the restricted lattice
Tj(S ∩G1; G1) = [{9}, {9, 8, 13}].
5
Directed models
Just as the undirected PCG deﬁned in Deﬁnition 5 is the L2 analogue of undirected conditional
independence graphs, it is possible to extend the general L2 concepts to directed graphs in a
way that mirrors the theory of Bayesian networks (BNs) and structural equation models. In
this section, we discuss such extensions and illustrate how these ideas are closely related to
neighborhood regression and the neighborhood lattice introduced in Section 3.3.
5.1
Directed PCGs
There are several equivalent ways to deﬁne the notion of a directed PCG, which we describe
here. Let G be a DAG on [d], and let Πj denote the parent set of node j in G. Similarly, let
Nj be the set of non-descendants of j in G (i.e., i ∈Nj if no directed path exists from j to i),
which is a well-deﬁned notion due to the acyclicity assumption.
Deﬁnition 8. We say that x = (x1, . . . , xd) ∈Hd satisﬁes a directed PCG w.r.t. G if
PiP ⊥
ΠjPj = 0,
∀j ∈[d], i ∈Nj.
(14)
This condition is modeled after the local Markov properties of BNs.
Intuitively, (14)
says that the residual after projecting xj onto its parents is orthogonal to any of xj’s non-
descendants. Alternatively, we could require that the residuals after projecting xi and xj onto
their respective parent sets are orthogonal, which gives a condition that is symmetric in i and j:
PiP ⊥
ΠiP ⊥
ΠjPj = 0,
for all distinct i, j ∈[d].
(15)
A common way to rewrite these conditions is to require that there exists a vector e ∈Hd,
and a matrix B ∈Rd×d with zero diagonal entries, whose associated graph is G (that is,
17

{(k, j) : Bkj ̸= 0} is the edge set of G) such that
(a) x = BT x + e,
(b) (BT x)i ⊥ei,
(c) ei ⊥ej,
∀i, j ∈[d].
(16)
Here, BT x is interpreted as an element of Hd with entries (BT x)j = P
k Bkjxk. The model
(16) is often called a (recursive) structural equation model for x.
We have opted to deﬁne a directed PCG via (14) owing to its familiarity from the literature
on directed graphical models. The following lemma, however, establishes the equivalence of
conditions (14-16):
Lemma 4. Let x = (x1, . . . , xd) ∈Hd be a vector satisfying (A1) and G a directed acyclic
graph. Then conditions (14), (15), and (16) are equivalent.
The equivalence of (14) and (15) is proved in Appendix B.4. The equivalence of (15) and (16)
is established via the following correspondences: (BT x)j = PΠjxj and ej = P ⊥
Πjxj. We note
that B will not be unique unless the Gram matrix Σ = (⟨xi, xj⟩) is nonsingular. Assuming
that Σ is nonsingular, the jth column of B is βj(Πj) as given by Deﬁnition 2.
It is always possible to obtain a directed PCG for a given x ∈Hd by ﬁxing an ordering of
the elements of x and performing recursive projection, i.e., projecting each element onto those
that come before it in the ordering. More precisely, ﬁx a d × d permutation matrix P, and
let ex = Px. Then, we set e1 = ex1 and proceed by projecting exj onto ex1, . . . , exj−1 for j ≥2,
and calling the residual ej. It is easy to see that this procedure leads to an SEM of the form
ex = eBT ex + e where eBT is lower triangular. In terms of the original vector x, we obtain an
SEM of the form (16) with B = B(P) = P T eBP, hence a directed PCG as deﬁned above.
In the other direction any directed PCG is obtained in this way, i.e. if (16) holds, one
can obtain a permutation matrix P such that PBP T is upper triangular (due to acyclicity
assumption). Furthermore, letting πP : [d] →[d] be the permutation associated with P and
deﬁning Sj = Sj(P) = π−1
P ({1, . . . , j}), the jth column of the matrix B obtained in this way
corresponds to the coeﬃcients βj(Sj) as given by Deﬁnition 2. More interestingly, the support
of βj(Sj) could be smaller than Sj (the original candidate parent set used in the recursive
projection).
This support will be the minimal element of the corresponding neighborhood
lattice, Tj(Sj), and will act as the (actual) parent set of j in the constructed PCG: that is,
Πj = mj(Sj) and βj(Sj) = βj(Πj). The recursive projection procedure thus establishes an
interesting connection between undirected PCGs, directed PCGs, SEM coeﬃcients, and the
neighborhood lattice which can be exploited when learning graphs from data (Section 5.2).
From the above discussion, it is clear that for any x ∈Hd, there are in fact many potential
directed PCGs: One for each possible ordering of its elements (note that some orderings might
lead to the same directed PCG). The corresponding SEM coeﬃcients B can be related to the
Cholesky factors of the inverse Gram matrix, after proper permutation: Assume that (16)
holds, P is a permutation matrix that makes PBP T upper triangular (always possible for a
DAG), and D is the diagonal Gram matrix of e. Letting Σ be the Gram matrix of x, it is not
hard to see that
Σ−1 = (I −B)D−1(I −B)T .
Performing an “upper” Cholesky decomposition on PΣ−1P T = UUT (i.e., U is an upper
triangular matrix) we obtain U = P(I −B)D−1/2P T due to the uniqueness of the Cholesky
factorization for positive deﬁnite matrices. From this interpretation, it is clear that the SEM
in (16)—alternatively a directed PCG—is not unique, i.e., for every permutation P of the
18

elements of x, there is a corresponding SEM obtained from the upper Cholesky decomposition
of PΣ−1P T . This distinguishes directed PCGs from undirected PCGs, which are always unique.
5.2
Learning directed PCGs
The SEM model (16) provides a useful, interpretable way to describe how the variables in
x = (x1, . . . , xd) relate to one another that is commonly used in applications such as biology,
social science, and machine learning. Given its apparent utility in practice, it is of signiﬁcant
interest to learn the coeﬃcient matrix B from data. Furthermore, since the matrix B can be
interpreted as a weighted adjacency matrix for the underlying DAG G, learning B also implies
learning the structural relations encoded by the graph G.
Unfortunately, since directed PCGs are not unique these models are unidentiﬁable.
In
the previous subsection we illustrated how the notion of a directed PCG is naturally related
to an implicit ordering on the variables, represented by a permutation P.
To circumvent
identiﬁability issues, one often seeks permutations that result in the sparsest DAGs, as these
represent parsimonious explanations of the variables that are simple to interpret in practice.
If we know the order of the variables that leads to the sparsest possible directed graph G,
then we can use the recursive projection procedure outlined in Section 5.1 to estimate G.
More speciﬁcally, we can use (5) with the neighborhoods Sj deﬁned previously to estimate the
support of each βj(Sj) and hence the graph G.
In practice, of course, we do not know this ordering. Thus, we must consider all possible d!
orderings of the variables. A naive algorithm would compute βj(S) for all possible neighbor-
hoods S ⊂[d]j and all j ∈[d], and check all possible permutations to ﬁnd the sparsest graph
G. This is clearly computationally infeasible. In considering all possible neighborhoods, how-
ever, one implicitly encounters the neighborhood lattices Tj(S) (Deﬁnition 3) for every possible
S ⊂[d]j and j ∈[d]. These lattices encode how much “redundancy” exists in the diﬀerent
neighborhoods, and suggests that we can reduce the total number of neighborhoods one must
consider by exploiting the algebraic structure of these lattices. This leads one to wonder: Is it
possible to exploit this redundancy in order to learn directed PCGs more eﬃciently?
Without sparsity or other simplifying assumptions, there are 2d−1d neighborhood problems
to consider, which is intractable when d is large. By imposing a natural sparsity assumption
on the neighborhood lattices, however, the total number of neighborhood problems reduces
substantially owing to the redundancies encoded by the neighborhood lattices. For example, if
we assume that |mj(S)| ≤k ≪d, for all S ⊂[d]j and j ∈[d], the total number of neighborhood
problems reduces to at most
 d
k

d ≤dk+1 ≪2d−1d. This is an immediate consequence of the
lattice construction in Section 3.3. By Remark 1, we have mj(S) = supp(βj(S)), so that this
assumption amounts to assuming that the parent sets are sparse, which is a natural assumption
to make in practice.
This suggests that one can learn all possible SEM representations of
the variables using a much smaller sample size, relative to the naive approach of solving all
possible regressions.
This has been successfully exploited for Gaussian models [AAZ16] to
achieve optimal sample complexity n = Ω(k log d) in learning recursive structural equation
models [GH17]. The results presented in the current work suggest that these ideas can be
extended much further to non-Gaussian models, a direction we intend to pursue in the future.
19

6
Discussion
In this paper, we investigated the algebraic structure of generalized neighborhood regression,
motivated by the problem of enumerating all partial orthogonality relations eﬃciently under
minimal assumptions. Our results on the neighborhood lattice are presented with a generalized
notion of partial orthogonality in a Hilbert space as the counterpart of conditional indepen-
dence. We further explored PCGs, as a generalization to conditional independence graphs, and
their connections to neighborhood lattices. Finally, we discussed some subtleties and compli-
cations in extending these ideas to directed models. Studying these extensions in more depth
is left for future work. Below, we discuss an additional avenue for future work regarding the
estimation of partial orthogonality relations from a ﬁnite sample.
From a statistical perspective, all the discussions in this paper have been at the population
level. In practical statistical applications, one has to estimate partial orthogonality relations
from a sample of size n. To be precise, consider the cases where the underlying Hilbert space H
has a stochastic component, that is, its inner product is deﬁned as ⟨x1, y1⟩H = E⟨x1, y1⟩H0 for
x1, y1 random elements from a base Hilbert space H0, e.g, H0 = R or H0 = H1([0, 1]); see the
examples in Appendix A.2.2. Now, assume that we observe independent copies x(1), . . . , x(n)
of x ∈Hd, a stochastic vector in Hd
0. Based on this sample, we wish to estimate some or all of
the partial orthogonality relations (4).
Under the perfectness assumption, according to the theorems of Section 4, the problem
reduces to estimating the support of Σ−1. If d is ﬁxed and n →∞, thresholding the inverse of
the sample Gram matrix, given by
bΣ = (bΣi,j) ∈Rd×d,
bΣi,j = 1
n
n
X
k=1
⟨x(k)
i
, x(k)
j ⟩H0,
may be suﬃcient without many assumptions on the distribution. For high-dimensional data
(d ≫n), however, it would be interesting to see if some of the well-known penalized estimators
such as the graphical lasso [FHT08] can be used to estimate the PCG without the Gaus-
sian assumptions. Much of the previous work in this area has been focused on the Gaussian
case [MB06; YL07; BEGd08; Uhl12]. Note that to recover the PCG, we need to have consistent
support recovery, perhaps the strongest form of consistency in high-dimensional problems.
Without the perfectness assumption, we have to consistently estimate the support of βj(S)
for all j and S ⊂[d]j, simultaneously. This can be done, at least in theory, by solving all the
neighborhood regression problems:
bβj(S) :=
argmin
β ∈Rd, supp(β) ⊂S
1
n
n
X
k=1
∥x(k)
j
−βT x(k)∥2
H0,
and asking whether the support of bβj(S), after thresholding or adding regularization, is con-
sistent for that of βj(S) for all j and S. This is a very challenging problem, as there are a
priori a super-exponential number of these neighborhood regressions. Here is where the lattice
property from Section 3.3 comes into play: By exploiting the lattice property, we can reduce
the total number of neighborhood regression problems that we need to look at, as detailed in
Section 5.2.
Finally, we note that the results presented here oﬀer an interesting connection between
undirected and directed graphs via the well-understood and intuitive notion of neighborhood
20

regression. While such connections have appeared in passing in previous work, our results
conﬁrm and extend these ideas in a very general setting. While there is much interest in using
undirected graphs to simplify learning directed graphs, we emphasize that these results also
oﬀer a way to go in the opposite direction. Namely, given a DAG, how can we interpret the
relationships encoded in the model in terms of the perhaps more familiar undirected PCG?
Theorems 4 and 5 provide an explicit description of how the connections in B—given by
mj(Sj)—can be characterized in terms of the underlying PCG. Thus, our results provide an
intuitive bridge between SEM and PCGs in a very general setting.
7
Proofs of the main results
Let us start with some preliminary lemmas. Recall the lattice of projections, P(H), introduced
in Section 2.3 and its meet and join operations. We have:
Lemma 5. For P, Q ∈P(H), Py = 0 and Qy = 0 ⇐⇒(P ∨Q)y = 0.
In addition, for the collection of projection operators {PS, S ⊂[d]} introduced in Deﬁni-
tion 5, we have: For any T1, T2 ⊂[d], PT1 ∨PT2 = PT1∪T2 and PT1 ∧PT2 = PT1∩T2. In particular,
S ⊂T implies PS ≤PT . All these statements extend to any number of operators and can be
argued by considering bases for the underlying subspaces. For example, for the statement in
Lemma 5, a basis for the range of P ∨Q can be built in such a way that it consists of the
union of bases for the range of P and the range of Q, from which the statement follows. The
following lemma is also key in the proofs:
Lemma 6. For S ⊂T, R = T \ S and any L ∈B(H),
PT L = PSL ⇐⇒PRP ⊥
S L = 0.
Proof. Since PS ≤PT , we have PT PS = PS, hence (PT −PS)L = PT (I −PS)L = PT P ⊥
S L. That
is, the LHS is equivalent to PT P ⊥
S L = 0. Multiplying by PR and using PR ≤PT gives the RHS,
i.e., PRP ⊥
S L = 0. Now assume that the RHS is true; since in addition we have PSP ⊥
S L = 0,
it follows that (PR ∨PS)P ⊥
S L = 0 by Lemma 5, from which we get the LHS by noting that
PR ∨PS = PS∪R.
7.1
Proof of Theorem 1
Closure under intersections: Let Q be the projection onto the range of PSPj, so that we have
Q ≤PS. Assume that T1, T2, · · · ∈Tj(S) and let R = T
k Tk. Note that Q is also the projection
onto the range of PTkPj = PSPj for all k. Hence, Q ≤PTk (and also QPTkPj = PTkPj). Since
PR = V
k PTk, it follows that Q ≤PR ≤PTk for all k. This last statement is equivalent to
PRQ = Q and PRPTk = PR for all k. Now, we have
PRPj = PRPTkPj = PR(QPTkPj) = QPTkPj = PTkPj = PSPj,
(17)
hence R ∈Tj(S). The ﬁrst equality follows from PR ≤PTk, the second and fourth since Q
projects onto range PTkPj, and the third since Q ≤PR. Thus, we have shown that Tj(S) is
closed under intersections.
21

Closure under unions: Let S∗be the minimal element of Tj(S) which exists by the previous
argument. Note that
T ∈Tj(S) ⇐⇒T ⊃S∗and PT P ⊥
S∗Pj = 0.
(18)
To see this, it is enough to note that for T ⊃S∗, we have (PT −PS∗)Pj = PT (I −PS∗)Pj =
PT P ⊥
S∗Pj, where the ﬁrst equality is by PS∗≤PT . Now, assume that T1, T2, · · · ∈Tj(S) and
let M = S
k Tk. Since Tk ∈Tj(S), we have Tk ⊃S∗and PTkP ⊥
S∗Pj = 0 for all k. It follows that
(W
k PTk)P ⊥
S∗Pj = 0 (Lemma 5). But PM = W
k PTk, that is, PMP ⊥
S∗Pj = 0, hence M ∈Tj(S)
by (18).
Convexity: Let S1, S2 ∈Tj(S) and S′ ⊂[d]j. Assume that S1 ⊂S′ ⊂S2, so that PS1 ≤
PS′ ≤PS2. We have PS1Pj = PS2Pj. Multiply on the left by PS′ and note that PS′PS1 = PS1,
and PS′PS2 = PS′. Thus, PS1Pj = PS′Pj showing that S′ ∈Tj(S). The proof is complete.
Remark 3. The argument above in terms of the projections works for an inﬁnite-dimensional
Hilbert space H and inﬁnitely many variables {x1, x2, . . . }. It also holds even when the variables
are dependent (i.e., Σ is rank-deﬁcient) if we remain at the level of projections (i.e., not map
projections onto sets of variables); see for example the general result in Appendix C.
In the ﬁnite-dimensional case H ∼= Rn, we can use the following simple argument instead
to show that Tj(S) is a lattice: Suppose that T1, T2 ∈Tj(S), i.e., PT1xj = PT2xj =: ˆxj. Then
ˆxj ∈span(xT1∩T2),
and
xj −ˆxj ⊥span(xT1∪T2)
where the ﬁrst statement uses Σ ≻0. These two imply that ˆxj is the projection of xj onto
span(xT1∩T2) as well as span(xT1∪T2), that is, T1 ∩T2 ∈Tj(S) and so is T1 ∪T2.
7.2
Proofs of Section 3.4
We start with a result that provides alternative characterization of partial orthogonality (Def-
inition 1) in terms of the SEM coeﬃcients (Deﬁnition 2). It is not hard to see that [βj(S)]S =
Σ−1
S ΣS,j (Appendix B.2). In addition, it is possible to write the condition PAP ⊥
S PB = 0 di-
rectly in terms of the SEM coeﬃcients, and also in terms of the Gram matrix Σ. We have the
following result proved in Appendix B.2:
Proposition 3. Under (A1), the following are equivalent:
(a) PAP ⊥
S PB = 0,
(b) [βj(A)]i = P
k∈S[βj(S)]k[βk(A)]i,
∀i ∈A, j ∈B,
(c) ΣB,A −ΣB,SΣ−1
S ΣS,A = 0 (replaced with ΣB,A = 0 if S = ∅),
(d) [βj(Si)]i = 0, ∀i ∈A, j ∈B.
Moreover, for any i, j ∈[d] and S ⊂[d]ij, we have
[βj(Si)]i = ⟨xi, P ⊥
S xj⟩H
⟨xi, P ⊥
S xi⟩H
.
(19)
We note that (d) can be equivalently written as [βj(Si)]i = [βi(Sj)]j = 0, ∀i ∈A, j ∈B,
due to symmetry. Most assertions in Proposition 3 are colloquially known (perhaps except
part (b)). For example, expressions similar to (19) have appeared before [Tia12], going back
to the work of [Cra46] and [Dem69].
22

Proof of Proposition 1
By assumption Ti := T ∪{i} ∈Tj = [mj, Mj] and T ∩{i, j} = ∅.
By Proposition 3(d), (j ⊥i | T) holds if and only if
[βj(Ti)]i = 0
(a)
⇐⇒i /∈mj
(b)
⇐⇒i ∈Mj \ mj
where (a) is due to supp(βj(Ti)) = mj by the lattice property, and (b) is due to assumption
i ∈Mj. The proof is complete.
Proof of Theorem 2
Consider statement j ⊥i | T which by convention implies T ∩{i, j} =
∅.
Since

[mℓ
j, Mℓ
j ], ℓ= 1, . . . , Kj
	
is a partition of 2[d]j, there is a unique k ∈[Kj] such
that T ∪{i} belongs to [mk
j , Mk
j ]. This implies that T ∈[mk
j , Mk
j \ {i}] and i ∈Mk
j . By
Proposition 1, j ⊥i | T holds iﬀi ∈Mk
j \ mk
j . The proof is complete.
7.3
Proofs of Section 3.5
The proof of Proposition 2 is immediate by an inspection of Algorithm 1 and Theorem 3 follows
by combining Proposition 2 and Lemma 3.
Proof of Lemma 3
There are multiple algorithms to solve this problem. Here we describe
one due to Brendan McKay. To simplify notation let r := d −1 = |[d]j| and identify [d]j with
[r] by relabeling if need be. We also drop the subscript j for from T ℓ
j and so on. Given disjoint
lattices T ℓ= [mℓ, Mℓ] ∈2[r], ℓ= 1, . . . , K, and any set S ⊂[r], let
Q(S) =
K
X
ℓ=1
Qℓ(S),
Qℓ(S) = |{T ∈T ℓ: S ⊂T}|.
Computing Qℓ(S) is easy and we consider it an atomic set operation. In particular, if S ̸⊂Mℓ
then Qℓ(S) = 0; otherwise, Qℓ(S) = 2|(Mℓ\mℓ)\S|. Then computing Q(S) can be done by at
most O(K) set operations. Note that for any set S ⊂[r] we have Q(S) ≤2r−|S| with equality
if and only if all possible subsets of [r] containing S are present in T := ∪K
ℓ=1T ℓ.
Now, if Q(∅) = 2r, then power set 2[r] is covered. Otherwise, there is i1 ∈[r] such that
Q({i1}) < 2r−1 which can be found by trying all the r elements of [r]. Then, one can ﬁnd
{i1, i2} such that Q({i1, i2}) < 2r−2 by trying all r possible cases for i2. Continuing in this
manner, we have either of the following two:
1. After at most r −1 rounds, we have S ⊂[d]j such that Q(S) = 0. This S has the desired
property and the algorithm terminates.
2. We have S = {i1, . . . , ir−1}, with Q(S) < 2. Since we are not in Case 1, we should have
Q(S) = 1. This means that either (a) S ∈T in which case the ground set [r] cannot
belong to T, hence we output [r], or (b) the ground set [r] ∈T in which case the original
S cannot belong to T and we output S.
In either case, the overall complexity is at most O(r2K) set operations.
23

7.4
Proof of Theorem 4
Throughout the proof of this theorem, we work under the assumption of H-Markov perfectness,
i.e., all the lemmas in this subsection are stated under that assumption. We note that
S′ ∈Tj(S) ⇐⇒Tj(S) = Tj(S′) ⇐⇒S ∈Tj(S′).
(20)
Lemma 7. Let A ⊂S ⊂[d]j. Then, A ∈Tj(S) iﬀA separates j from S \ A.
Proof. Let R = S \ A. Then, by H-Markov perfectness, A separates j from S \ A iﬀPR ⊥Pj |
PA. This means PRP ⊥
A Pj = 0 which is equivalent to PAPj = PSPj, by Lemma 6.
We can now write mj(S), the minimum element of Tj(S), as
mj(S) =
\
{A ⊂S : A ∈Tj(S)} =
\ 
A ⊂S : A separates j and S \ A.
	
the ﬁrst equality is because we can restrict to subsets of S when ﬁnding the minimum, due to
Tj(S) being closed under intersections (lattice property), and the second is by Lemma 7. This
proves part (a) of Theorem 4.
Lemma 8. Let B ⊂[d]j and A ∈Tj(S) be disjoint. Then A separates B and j if and only if
A ∪B ∈Tj(S).
Proof. Since A ∈Tj(S), we have Tj(A) = Tj(S).
Hence, A ∪B ∈Tj(S) is equivalent to
A ∪B ∈Tj(A) which is equivalent to A ∈Tj(A ∪B) by (20).
By Lemma 7, this latter
statement is equivalent to A separates j and B.
Since mj(S) is the smallest element of Tj(S), we get the following characterization:
Tj(S) = {T ⊂[d]j : mj(S) separates j and T \ mj(S)}.
(21)
To see this, ﬁx S and let S∗= mj(S). Then T ∈Tj(S) iﬀS∗⊂T by minimality and S∗
separates j and T \ S∗by Lemma 8. From (21), it follows that Mj(S) = mj(S) ∪B where B
is the largest set that can be separated from j by mj(S). This proves part (b). Part (c) also
follows from (21).
7.5
Proof of Theorem 5
Let us introduce some new notations. Recall that we write A −C −B for disjoint sets A, B
and C when C separates A and B. This means that any path from a node i ∈A to node j ∈B
should pass through a node k ∈C. This includes the case where there is no path from A to B.
Lemma 9. Separation has these properties:
• Additive property: A −C1 −B1 and A −C2 −B2 implies A −(C1 ∪C2) −(B1 ∪B2).
• Reduction property: A −C1 ⊎C2 −B and C2 −A −B implies A −C1 −B.
Proof. We only show the reduction property, for which it is enough to show that there is no
path from A to B that passes through C2 and not C1. If so, a portion of it gives a path from
C2 to B that does not pass through C1, and clearly not A since the paths do not self-cross. But
then A does not separate C2 and B, a contradiction. Figure 3 illustrates the argument.
24

A
C1
C2
B
(a)
A
C1
C2
B
(b)
Figure 3: Illustration of the reduction property. We break the cases respecting A−C1 ⊎C2 −B into two
groups, depending on whether there is a path between C1 and C2 (a), or not (b). The dashed lines are
possible paths. In none of the cases there could be a path between C2 and B because of the assumption
C2 −A −B. The same assumption precludes a path between C1 and B in cases in (a), because of the
existence of a path from C2 to C1. As seen from this ﬁgure, in all possible cases, A and B are separated
by C1.
Proof of Theorem 5
By induction we can reduce to the case where the nodes are partitioned
into two disjoint components G1 and G2, after the removal of j. This implies G1 −j −G2. For
any set A, let Ak := A ∩Gk, k = 1, 2. We have
j −A −(S \ A)
⇐⇒
j −A1 −(S1 \ A1)
and
j −A2 −(S2 \ A2)
(22)
To see this, note that the RHS implies j −A −(S1 \ A1) ∪(S2 \ A2) by the additive property.
But (S1 \ A1) ∪(S2 \ A2) = S \ A since G1 and G2 are disjoint.
Now assume the LHS. Then, clearly j −A −(S1 \ A1). But we also have A2 −j −(S1 \ A1).
Applying the reduction property, we get j −A1 −(S1 \ A1). The other implication is similar,
hence we get the RHS.
Equipped with (22), we can prove part (a). Let S∗be the smallest subset of S separating j
and S \S∗(see (13)), and let S∗
1 and S∗
2 be its components (i.e, S∗
k = S∗∩Gk, k = 1, 2). Then,
by (22) S∗
k separates j and Sk \S∗
k, for k = 1, 2. It remains to show that S∗
k is the smallest such
set (for each k). Suppose that there is a proper subset S′
1 of S∗
k that separates j and S \ S′
1.
Then, applying (22) with A1 = S′
1 and A2 = S∗
2 and A = S′ := S′
1 ∪S∗
2, we conclude that S′
separates j and S \ S′. But S′ is a proper subset of S∗, violating the assumption that S∗is the
minimal set with such property. It follows that we should have mj(S1) = S∗
1 and mj(S2) = S∗
2
which proves part (a).
For part (b), let S∗
k = mj(Sk), k = 1, 2 and S∗= mj(S). From part (a), we have S∗=
S∗
1 ∪S∗
2. Fix r ∈[d]j. We claim that
r −S∗−j
⇐⇒
r −S∗
1 −j or r −S∗
2 −j.
(23)
The ⇐implication is clear. For the other direction, assume that r is separated from j by S∗,
i.e., all the paths from r to j pass through S∗. WLOG, assume that r ∈G1 (the case r ∈G2
is similar). Suppose that there is a path from r to j that passes through S∗
2. A portion of
this path gives a path from r to S∗
2, hence to G2, that does not pass through j, contradicting
G1−j−G2. Hence all the paths from r to j should pass through S∗
1, i.e., r−S∗
1 −j, proving (23).
From (23), we conclude that Ej(S∗) = Ej(S∗
1) ∪Ej(S∗
2). Combined with characterization of
Mj(S) in Theorem 4(b), this proves the ﬁrst equality in part(b).
To see the second equality in (b), recall that Tj(Sk; Gk) is the lattice with ground set
restricted to Gk ∪{j}. First, the minimal element of Tj(Sk; Gk) is the same as that of Tj(Sk).
This is true since a subset A of Sk separates j and Sk \ A in Gk ∪{j} iﬀit does so in G.
25

(If the separation happens in Gk ∪{j} but not in G, there will be a path from Sk \ A to j
passing through some Gr, r ̸= k contradicting disjointness of Gk and Gr.) Even more directly,
a minimal subset of Tj(Sk) is a subset of Sk, hence Gk, and the restriction in Tj(Sk; Gk) is
automatically satisﬁed.
Knowing that the minimal element of Tj(Sk; Gk) is S∗
k, we invoke
Theorem 4(b) restricted to Gk ∪{j} to conclude that Mj(Sk; Gk) = S∗
k ∪Ej(S∗
k; Gk) where
Ej(S∗
k; Gk) = {i ∈Gk : S∗
k separates i and j}. Going through the argument leading to (23),
it is clear that we can replace (23) with
r −S∗−j
⇐⇒
[r −S∗
1 −j and r ∈G1] or [r −S∗
2 −j and r ∈G2]
(24)
showing that Ej(S∗) = Ej(S∗
1; G1) ∪Ej(S∗
2; G2). Combined with the expression for Mj(Sk; Gk),
the desired result follows.
Acknowledgment
This work was supported by NSF grant IIS-1546098.
References
[AAZ16]
B. Aragam, A. Amini, and Q. Zhou. “Learning directed acyclic graphs with pe-
nalized neighbourhood regression”. In: Submitted arXiv:1511.08963 (2016).
[BEGd08]
O. Banerjee, L. El Ghaoui, and A. d’Aspremont. “Model selection through sparse
maximum likelihood estimation for multivariate Gaussian or binary data”. In: The
Journal of Machine Learning Research 9 (2008), pp. 485–516.
[Bic]
T. Bice. Collection of projection operators in ﬁnite dimension and algebraic tech-
inques. MathOverﬂow.
[Bla06]
B. Blackadar. Operator Algebras. Vol. 122. Encyclopaedia of Mathematical Sci-
ences. Berlin, Heidelberg: Springer Berlin Heidelberg, 2006.
[Cra46]
H. Cram´er. Mathematical Methods of Statistics. Vol. 9. Princeton university press,
1946.
[Daw01]
A. P. Dawid. “Separoids: A mathematical framework for conditional indepen-
dence and irrelevance”. In: Annals of Mathematics and Artiﬁcial Intelligence 32.1-
4 (2001), pp. 335–372.
[Dem69]
A. P. Dempster. Elements of continuous multivariate analysis. Vol. 388. Addison-
Wesley Reading, Mass., 1969.
[Drt+18]
M. Drton et al. “Algebraic problems in structural equation modeling”. In: The 50th
Anniversary of Gr¨obner Bases. Mathematical Society of Japan. 2018, pp. 35–86.
[ER14]
R. J. Evans and T. S. Richardson. “Markovian acyclic directed mixed graphs for
discrete data”. In: The Annals of Statistics (2014), pp. 1452–1482.
[FHT08]
J. Friedman, T. Hastie, and R. Tibshirani. “Sparse inverse covariance estimation
with the Graphical Lasso”. In: Biostatistics 9.3 (2008), pp. 432–441.
[FW10]
I. Farah and E. Wofsey. “Set theory and operator algebras”. In: Appalachian Set
Theory: 2006-2012 (2010), pp. 1–51.
26

[GB13]
S. van de Geer and P. B¨uhlmann. “ℓ0-penalized maximum likelihood for sparse
directed acyclic graphs”. In: Annals of Statistics 41.2 (2013), pp. 536–567.
[GH17]
A. Ghoshal and J. Honorio. “Information-theoretic limits of Bayesian network
structure learning”. In: Proceedings of the 20th International Conference on Arti-
ﬁcial Intelligence and Statistics. Ed. by A. Singh and J. Zhu. Vol. 54. Proceedings
of Machine Learning Research. Fort Lauderdale, FL, USA: PMLR, 2017, pp. 767–
775.
[HMR14]
D. Heckerman, C. Meek, and T. Richardson. “Variations on undirected graphical
models and their relationships”. In: Kybernetika 50.3 (2014), pp. 363–377.
[KF09]
D. Koller and N. Friedman. Probabilistic Graphical Models: Principles and Tech-
niques. Vol. 2009. 4. 2009, p. 1231.
[Lau96]
S. L. Lauritzen. Graphical Models (Oxford Statistical Science Series). Oxford Uni-
versity Press, USA, 1996, p. 312.
[LB14]
P.-L. Loh and P. B¨uhlmann. “High-Dimensional Learning of Linear Causal Net-
works via Inverse Covariance Estimation”. In: Journal of Machine Learning Re-
search 15 (2014), pp. 3065–3105.
[LLW09]
H. Liu, J. Laﬀerty, and L. Wasserman. “The Nonparanormal: Semiparametric Esti-
mation of High Dimensional Undirected Graphs”. In: Journal of Machine Learning
Research 10 (2009), pp. 2295–2328.
[LS+18]
S. Lauritzen, K. Sadeghi, et al. “Unifying Markov properties for graphical models”.
In: The Annals of Statistics 46.5 (2018), pp. 2251–2278.
[MB06]
N. Meinshausen and P. B¨uhlmann. “High-dimensional graphs and variable selec-
tion with the {L}asso”. In: The Annals of Statistics 34.3 (2006), pp. 1436–1462.
[Pea88]
J. Pearl. Probabilistic reasoning in intelligent systems: Networks of plausible in-
ference. Morgan Kaufmann, 1988.
[Pou99]
M. Pourahmadi. “Joint mean-covariance models with applications to longitudinal
data: Unconstrained parameterisation”. In: Biometrika 86.3 (1999), pp. 677–690.
[Reg]
Neighborhood regression: lattice computation. https://github.com/aaamini/
reg_lattice_comp. 2017.
[RERS17]
T. S. Richardson, R. J. Evans, J. M. Robins, and I. Shpitser. “Nested Markov
properties for acyclic directed mixed graphs”. In: arXiv preprint arXiv:1701.06686
(2017).
[RWLO10]
P. Ravikumar, M. J. Wainwright, J. D. Laﬀerty, and Others. “High-dimensional
Ising model selection using ℓ1-regularized logistic regression”. In: The Annals of
Statistics 38.3 (2010), pp. 1287–1319.
[SL+14]
K. Sadeghi, S. Lauritzen, et al. “Markov properties for mixed graphs”. In: Bernoulli
20.2 (2014), pp. 676–696.
[Sta97]
R. P. Stanley. “Enumerative Combinatorics (Volume 1)”. In: Cambridge studies
in advanced mathematics (1997).
[Stu06]
M. Studeny. Probabilistic conditional independence structures. Springer Science &
Business Media, 2006.
27

[Tia12]
J. Tian. “A Criterion for Parameter Identiﬁcation in Structural Equation Models”.
In: Proceedings of the Conference on Uncertainty in Artiﬁcial Intelligence (UAI).
June 20, 2012. arXiv: 1206.5289v1 [stat.ME].
[Uhl12]
C. Uhler. “Geometry of maximum likelihood estimation in Gaussian graphical
models”. In: The Annals of Statistics 40.1 (2012), pp. 238–261.
[Wer80]
N. Wermuth. “Linear recursive equations, covariance selection, and path analysis”.
In: Journal of the American Statistical Association 75.372 (1980), pp. 963–972.
[Whi09]
J. Whittaker. Graphical models in applied multivariate statistics. Wiley Publish-
ing, 2009.
[Wri21]
S. Wright. “Correlation and causation”. In: Journal of agricultural research 20.7
(1921), pp. 557–585.
[Wri34]
S. Wright. “The method of path coeﬃcients”. In: The Annals of Mathematical
Statistics 5.3 (1934), pp. 161–215.
[YL07]
M. Yuan and Y. Lin. “Model selection and estimation in the Gaussian graphical
model”. In: Biometrika 94.1 (2007), pp. 19–35.
[YRAL15]
E. Yang, P. Ravikumar, G. I. Allen, and Z. Liu. “Graphical models via univariate
exponential family distributions”. In: Journal of Machine Learning Research 16
(2015), pp. 3813–3847.
A
Details
A.1
Details of the neighborhood lattice
Lemma 10. Representations (6) and (7) are equivalent.
Proof of Lemma 10. Let Tj(S) be as deﬁned in (6). To see the ﬁrst equality in (7), note that
βj(S)T x = PSxj. Since the Gram matrix of x is positive deﬁnite, βj(S) = βj(T) is equivalent
to βj(S)T x = βj(T)T x, and hence to PSxj = PT xj. The result follows since for any operator L,
Lxj = 0 is equivalent to LPj = 0. (We have LPjy = ⟨xj, y⟩Lxj for all y, assuming ∥xj∥H = 1.)
The second equality in (7) follows from the ﬁrst equality and positive deﬁniteness of the Gram
matrix.
A.2
PCG details
A.2.1
Partial orthogonality via Gram matrix
The following result is proved in Appendix B.1:
Lemma 11. |ΣSi,Sj| = 0 is equivalent to PiP ⊥
S Pj = 0, for all i, j and S ⊂[d]ij.
It is worth noting that the lemma is not restricted to random variables and holds in the general
Hilbert space setup of Deﬁnition 5. Since PCGs are preserved under Hilbert space isometries
(Appendix B.3), by passing to the case where x is a zero-mean Gaussian random vector with
covariance Σ (i.e., a special case of Example 1), and using the known results on Gaussian
28

pairwise conditional independence [Lau96, Prop. 5.2], we conclude that (11) in Deﬁnition 5 is
in fact equivalent to
i ≁j in G ⇐⇒[Σ−1]i,j = 0
(25)
or equivalently |ΣSi,Sj| = 0 where S = [d]ij. That is, Lemma 11, applied with with S = [d]ij,
recovers this well-known result. Note, however, that Lemma 11 is stronger than (25), since it
establishes the equivalence for any S ⊂[d]ij.
The ternary relation (4) among projections is also completely characterized by the Gram
matrix Σ:
Lemma 12. ΣB,A−ΣB,SΣ−1
S ΣS,A = 0 is equivalent to PAP ⊥
S PB = 0, for disjoint A, S, B ⊂[d].
This lemma is proved as part of Proposition 3 (Section 7.2). We refer to (4) as partial
orthogonality, or conditional orthogonality, a generalization of the notion of partial uncorrelat-
edness of random variables. Clearly the global H-Markov property implies the pairwise version
as a special case. The following result, proved in Appendix B.3, establishes the equivalence of
the pairwise and global H-Markov properties when the covariance matrix is non-degenerate.
Note that this is the L2 analogue of a similar well-known result for CIGs [Lau96, Thm. 3.9,
p. 35].
Lemma 13. Assuming Σ ≻0, the pairwise H-Markov property implies the global H-Markov
property.
The PCG, as well as the pairwise and global H-Markov properties, can be thought of
as being deﬁned based on (1) a vector x ∈Hd with a particular Gram matrix Σ, or more
abstractly based on (2) a family of projection operators on the subspaces generated by x. We
have implicitly taken the ﬁrst viewpoint, in which case we can characterize the PCG in terms
of Σ. There are however subtle diﬀerences between (1) and (2). A vector x uniquely identiﬁes
both a Gram matrix Σ and a collection of subspaces, or equivalently a collection of projection
operators {PA}. However, neither Σ nor {PA} uniquely identiﬁes the other. Nevertheless both
are useful in encoding H-Markov properties as illustrated in Lemma 11. For example, it is
clear from the projection viewpoint, that rescaling any of the components of x does not change
the subspaces and hence the H-Markov properties. This implies the following:
Lemma 14. DΣD has the same H-Markov properties as Σ for any diagonal matrix D, with
nonzero diagonal entries.
Although this can be veriﬁed directly in terms of Σ, using the characterization of Lemma 12,
the projection viewpoint makes this immediately clear. A major theme of this paper is showing
the usefulness of the projection interpretation in understanding the algebraic features of the
H-Markov property.
A.2.2
PCG Examples
Let us now give more concrete examples of the abstract PCG of Deﬁnition 5, which go beyond
the familiar case of Example 1, and also show the utility of Lemma 11.
29

Figure 4: Examples of functions in H1([0, 1]) that form a 6-nodes star-shaped PCG. Each row corre-
sponds to one star-shaped graph, and in each case the function that acts as the central node is shown
with dashed red line. See Example 5 for the details of the constructions.
Example 5 (PCG for deterministic objects). The general setup of Deﬁnition 5 allows us to
deﬁne PCGs for purely deterministic objects.
For example, let us take H = Rn with the
usual Euclidean inner product and take {xj, j ∈[d]} to be a collection of vectors in Rn. The
resulting PCG encodes a form of partial orthogonality among these vectors. For example, let
{e1, . . . , en} be an orthonormal basis of Rn and let xj = e1 + ej for j ∈[d], assuming n ≥d.
It is clear that for j ̸= 1 and any subset A ⊂[d]j containing 1, PAxj = P1xj and P ⊥
1 xj = ej.
Hence, for distinct i, j ̸= 1 and 1 ∈A ⊂[d]ij, we have xT
i P ⊥
A xj = xT
i P ⊥
1 xj = 0. In other words,
“conditional” on any subset [d]ij containing 1, i and j are orthogonal, i.e., there is no edge
between them in the PCG. One can argue with some more work that all the other edges (i.e.,
between 1 and any other node) are present. Hence the PCG is star-shaped with node 1 at the
center. The corresponding Gram matrix and its inverse are
Σ =
1
γ −∥u∥2
1
u
u
(γ −∥u∥2)I + uuT

,
Σ−1 =
 γ
−uT
−u
I

,
(26)
where γ = d/4 and u =
1
21d−1.
Here, 1d−1 is the all ones vector in Rd−1, and we have
γ −∥u∥2 = 1/4. The star shape of the PCG is immediately clear from (26) in view of (25), or
equivalently Lemma 11.
Similarly, one can take H = F for a Hilbert space of functions and let {xj} be functions
in F. For example, take H = H1([0, 1]), the Sobolev space of order 1, deﬁned as the space of
absolutely continuous functions f : [0, 1] →R with derivative f′ ∈L2([0, 1]), and f(0) = 0.
The inner product is ⟨f, g⟩H := ⟨f′, g′⟩L2([0,1]) =
R 1
0 f′(t)g′(t)dt, and the corresponding norm
is a measure of smoothness of the functions. An orthonormal basis for this space is ek(t) =
µk sin(t/µk) for k = 1, 2, . . . , where µk = 2/((2k −1)π). The exact same construction used
earlier, namely xj(t) = e1(t) + ej(t), t ∈[0, 1], j ∈[d], can be used to obtain a star PCG for
the underlying functions, since one gets the exact same Gram matrix as in (26). In this case,
in contrast to the ﬁnite-dimensional setting, we can even take d = ∞to have a star PCG with
inﬁnitely many leaves. The top row in Figure 4 shows the functions in this construction for
d = 6. Also shown as the second row is the collection xj(t) = e6(t) + ej(t), j = 1, . . . , 6 which
has the same star-shaped PCG with e6 as the central node.
Example 6 (PCG for random vectors). For a probability measure P, and n ∈N, let H =
(L2(P))n, the L2 space of random vectors X ∈Rn with bounded second moments: E∥X∥2
2 < ∞.
The inner product here is given by ⟨X, Y ⟩H = E⟨X, Y ⟩Rn = Pn
i=1 E(XiYi) where ⟨·, ·⟩Rn is the
usual Euclidean inner product in Rn. Fix integers d and m, and take a sequence {ukj, k ∈
[m], j ∈[d]} ⊂Rn of deterministic vectors, a sequence {Zkj, k ∈[m], j ∈[d]} ⊂L2(P) of
30

zero-mean (scalar) random variables, and let Xj = Pm
k=1 Zkjukj ∈Rn for j = 1, . . . , d. Clearly
each Xj ∈(L2(P))n and we have
⟨Xi, Xj⟩H =
m
X
k=1
m
X
k′=1
E[ZkiZk′j]⟨uki, uk′j⟩Rn,
i, j ∈[d]
which can model complex correlations both via the covariance matrices Sk,k′ = (E[ZkiZk′j]) ∈
Rd×d and the relative positions of {ukj} in Rn. Letting Vj = span{u1j, . . . , umj}, we observe
that Xj ∈Vj, that is Xj is a random element of Vj. Thus, we can use this setup to simulta-
neously model subspace clustering and dimension reduction, especially when assuming m ≪n
so that Vjs are of much lower dimension than the ambient space, and when some pairs of
subspaces are either identical or have large intersections. For example, to see the application
to clustering, consider the case where Vj = V (1) for some js and Vj = V (2) for others, with
V (1) ̸= V (2). Then, the vectors {Xj} are naturally divided into two clusters, and their PCG
will contain information about the two clusters.
The PCG of Deﬁnition 5 for X = (X1, . . . , Xd) has a random vector, Xj, on each node. The
geometry of the subspaces V1, . . . , Vd aﬀects this PCG of X. For example, when the subspaces
are mostly orthogonal, there will be a lot of missing edges in the PCG: If Vi ⊥Vj for i ∈A
and j ∈B, then, ⟨Xi, Xj⟩H = 0 for all i ∈A, j ∈B, irrespective of the correlation structure
of Z. Lemma 11 (with S = [d]ij), then, implies that there is no edge between A and B in the
PCG (since the inverse Gram matrix has a zero block on indices A × B).
Example 7 (PCG for random functions). In the setup of Example 6, we can replace Rn with
a Hilbert space F of real-valued functions on domain X, that is, RX . We take H = (L2(P))X ,
a space of random functions (or random processes) on domain X, with ⟨F, G⟩H = E⟨F, G⟩F.
Similar to Example 6, take a deterministic sequence {fkj ∈F, k ∈[m], j ∈[d]}, (with m = ∞
a possibility when F is an inﬁnite-dimensional space) and let Fj = Pm
k=1 Zkjfkj.
All the
discussions of the Example 6 go through. The PCG in this case has random functions {Fj} as
its nodes. Depending on what the inner product of F measures, the absence of an edge could
have diﬀerent meanings. For example, if F is a Sobolev space where the norm measures some
form of smoothness of the function, an absence of an edge between Fi and Fj includes some
information about the relative smoothness of the pair of functions.
For example, let F be the Sobolev space of Example 5 with basis functions e1, e2, . . .
given there. Consider the star construction with e1 at the center but with random weights:
xj = Z1je1 + Z2jej for j ∈[d]. Let Sk,k′ = (E[ZkiZk′j]) ∈Rd×d for k, k′ = 1, 2. Note that
S2,1 = ST
1,2. Let δi,j = 1{i ̸= j} be the Kronecker delta function. It is not hard to see that
Σi,j := ⟨xi, xj⟩=

S1,1 + ST
1,2δi,1 + S1,2δ1,j + S2,2δi,j

i,j,
i, j ∈[d].
Thus, the oﬀ-diagonal elements of the ﬁrst row of Σ (Σ1,i, i ̸= 1) are determined by the
corresponding elements of S1,1 +ST
1,2, while the oﬀ-diagonal elements of Σ outside the ﬁrst row
and column are determined by those of S1,1 alone. It is clear for example, that elements of
S2,2 besides [S2,2]1,1 have no eﬀect on the Gram matrix. The deterministic case of Example 5
corresponds to S1,1 = S1,2 = all-ones d × d matrix. The other extreme S1,1 = S1,2 = Id leads
to the empty PCG. Many other possibilities exist between these two extremes.
We note that Examples 6 and 7 provide useful statistical frameworks for modeling multivari-
ate dependencies among random functions or other high-dimensional objects, while retaining
31

some familiar aspects of regression analysis and PCGs. In this sense, they provide avenues
for extending classical multivariate statistical analysis to collections of higher-order objects
(vectors, matrices, functions, etc.).
B
Proofs of auxiliary results
We recall the following notational conventions: For a matrix Σ ∈Rd×d, and subsets A, B ⊂[d],
we use ΣA,B for the submatrix on rows and columns indexed by A and B, respectively. Single
index notation is used for principal submatrices, so that ΣA = ΣA, A. For example, Σi,j is the
(i, j)th element of Σ (using the singleton notation), whereas Σij = Σij, ij is the 2×2 submatrix
on {i, j} and {i, j}.
B.1
Proof of Lemma 11
Let H be the underlying Hilbert space and xi ∈H such that Σ = (⟨xi, xj⟩) ≻0.
Deﬁne
the operator LS : R|S| →H by LSa = P
i∈S aixi. The adjoint operator L∗
S : H →R|S| is
given by L∗
Sy = (⟨y, xi⟩)i∈S. Since Σ ≻0, we have ΣS = (⟨xi, xj⟩)i,j∈S ≻0 for all S ⊂[d].
By considering ΣS : R|S| →R|S| as an operator, we have the identiﬁcation ΣS = L∗
SLS. To
simplify notation, let Li = L{i}, so that Lia = axi for any a ∈R and L∗
i y = ⟨y, xi⟩for any
y ∈H. Then,
PS = LS(L∗
SLS)−1L∗
S = LSΣ−1
S L∗
S.
(27)
(To see (27), note that the projection PS is characterized by the residual x −PSx, for any
x ∈H, being orthogonal to ran(PS) = ran(LS), that is, x −PSx ∈[ran(LS)]⊥= ker(L∗
S).
Thus, PS is characterized by L∗
S(I −PS) = 0 which is satisﬁed by (27). Alternatively, write
PSx = LSa so that L∗
S(x −LSa) = 0 and solve for a.)
It follows that PiP ⊥
S Pj = 0 is equivalent to LiΣ−1
i L∗
i P ⊥
S LjΣ−1
j L∗
j = 0. Note that Σi and
Σj are both positive scalars (ith and jth diagonal entries of Σ). Hence we can take them
out and get the equivalent statement LiL∗
i P ⊥
S LjL∗
j = 0. We also note that Li(L∗
i P ⊥
S Lj)L∗
j =
(L∗
i P ⊥
S Lj)LiL∗
j since the expression in parentheses is a scalar. LiL∗
j is not the identically zero
operator, hence we get the equivalent statement
0 = L∗
i P ⊥
S Lj = L∗
i (I −PS)Lj = L∗
i Lj −L∗
i LSΣ−1
S L∗
SLj
= Σij −ΣT
S,iΣ−1
S ΣS,j
(28)
where ΣS,i ∈R|S| is the vector obtained from rows indexed by S in column i of Σ. By a Schur
complement argument, (28) in turn is equivalent to the determinant of
  Σij ΣT
S,i
ΣS,j ΣS

= ΣSi,Sj
being zero, as desired.
B.2
Proof of Proposition 3
Let us start by computing PSxi using the notation used in the proof of Lemma 11. Since
xj = Lj1, it is enough to write PSLi = LSΣ−1
S L∗
SLj = LS(Σ−1
S ΣS,j) (cf. (27)). This means
that Σ−1
S ΣS,j is the coeﬃcient of expansion of PAxj in the basis of {xi : i ∈S}. That is,
[βj(S)]S = Σ−1
S ΣS,j in the notation of Deﬁnition 2.
32

We can now obtain an equivalent statement to PAP ⊥
S PB = 0 in terms of the Gram matrix
Σ. The condition is equivalent to PAP ⊥
S xj = 0 for all j ∈B. This in turn is equivalent to
PAxj = PAPSxj. Since PSxj = P
i∈S[βj(S)]ixi, we have
PAPSxj =
X
i∈S
[βj(S)]i(PAxi) =
X
i∈S
[βj(S)]i
 X
k∈A
[βi(A)]kxk

.
Using PAxj = P
k∈A[βj(A)]kxk and equating the coeﬃcients of xk, we obtain the system of
equations given in (b).
To see the equivalence of (b) and (c), let IS,i be the subvector of the identity matrix I ∈Rd×d
on S × {i}. Note that we have [βj(S)]i = IT
S,iΣ−1
S ΣS,j = Σj,SΣ−1
S IS,i, for all i ∈S (in fact true
for all i ∈[d]), where the second equality is by symmetry of Σ. The RHS of ((b)) is equal to
X
i∈S
 Σj,SΣ−1
S IS,i
 Σi,AΣ−1
A IA,k

= Σj,SΣ−1
S
X
i∈S
 IS,iΣi,A

Σ−1
A IA,k,
the latter summation being equal to IS,SΣS,A = ΣS,A. Similarly, the LHS of (b) is equal to
Σj,AΣ−1
A IA,k. Hence, (b) is equivalent to
Σj,AΣ−1
A IA,k = Σj,SΣ−1
S ΣS,AΣ−1
A IA,k,
∀k ∈A, j ∈B,
or in matrix form ΣB,AΣ−1
A IA,A = ΣB,SΣ−1
S ΣS,AΣ−1
A IA,A.
Dropping the identity IA,A and
rearranging we have (ΣB,A −ΣB,SΣ−1
S ΣS,A)Σ−1
A = 0 which is equivalent to (c).
Let us now show the equivalence of (a) and (d). First we note that (a) is equivalent to
PiP ⊥
S Pj = 0 for all i ∈A and j ∈B. Alternatively, it is equivalent to ⟨xi, P ⊥
S xj⟩H = 0 for all
i ∈A and j ∈B. Let us ﬁx i ∈A and j ∈B, and show that
⟨xi, P ⊥
S xj⟩H = 0 ⇐⇒[βj(Si)]i = 0
which establishes (a) ⇐⇒(d). By deﬁnition of SEM coeﬃcients in (2), we have PSi xj =
P
k∈Si[βj(Si)]kxk. To simplify notation, let us write α = βj(Si) and u = PSixj. We have
u =
X
k∈Si
αkxk,
xj = P ⊥
Sixj + u.
(29)
Applying P ⊥
S to both sides of the ﬁrst equation, we get P ⊥
S u = αiP ⊥
S xi. (Since, P ⊥
S xk =
0, ∀k ∈S.) Taking the inner product with xi (dropping H subscript for simplicity), we obtain
αi⟨xi, P ⊥
S xi⟩= ⟨xi, P ⊥
S u⟩
= ⟨xi, P ⊥
S (xj −P ⊥
Sixj)⟩
= ⟨xi, P ⊥
S xj⟩−⟨xi, P ⊥
Sixj⟩
(Since P ⊥
Si ≤P ⊥
S , i.e. P ⊥
SiP ⊥
S = P ⊥
Si)
= ⟨xi, P ⊥
S xj⟩−⟨P ⊥
Sixi, xj⟩
(Projections are self-adjoint.)
= ⟨xi, P ⊥
S xj⟩
(Since P ⊥
Sixi = 0.)
Note that P ⊥
Si ≤P ⊥
S is equivalent to PS ≤PSi which is perhaps easier to see. Since assumption
Σ ≻0 implies ⟨xi, P ⊥
S xi⟩> 0, we have the equivalence of αi = 0 and ⟨xi, P ⊥
S xj⟩= 0 which is
the desired result. Note that we have also established (19). The proof is complete.
33

B.3
Proof of Lemma 13
The idea is to reduce the general case to the case where the underlying random variables are
Gaussian, in which case PCG is the same as CIG. The result the follows that for CIGs.
The ﬁrst step is to show that PCGs are preserved under isometries between Hilbert spaces.
Let us recall some facts about isometries: An operator L ∈B(H1, H2) between two Hilbert
spaces is called an isometry if ∥Lξ∥H2 = ∥ξ∥H1 for all ξ ∈H1, i.e. it preserves norms. Equiva-
lently (using polarization identity), it is an isometry if ⟨Lξ, Lη⟩H2 = ⟨ξ, η⟩H1, ∀ξ, η ∈H1, i.e.,
it preserves inner products. One has that L is an isometry iﬀL∗L = IH1.
Step 1 (PCGs are preserved by isometries): Let H0 := span{x1, . . . , xd} ⊂L2(P). Let K
be a d-dimensional Hilbert space and let us deﬁne the linear operator V : K →H0 by taking a
linearly independent set {y1, . . . , yd} ⊂K with and letting V yi = xi, ∀i ∈[d]. We can further
assume that {yi} are chosen such that ⟨yi, yj⟩K = Σij = ⟨xi, xj⟩H. It then follows that V is an
isometry, hence V ∗V = IK. On the other hand, V is also surjective, hence a unitary operator,
hence V V ∗= IH0.
Let QA = V ∗PAV . We claim that this is the (orthogonal) projection operator onto the span
of yA := {yi : i ∈A}. We have, for i ∈A, QAyi = V ∗PAxi = V ∗xi = V ∗V yi = yi. On the other
hand, assume z ⊥K yi, ∀i ∈A. Then, for all i ∈A, 0 = ⟨z, yi⟩K = ⟨V z, V yi⟩H0 = ⟨V z, xi⟩H0. It
follows that PAV z = 0, hence QAz = 0.
This shows that any of the pairwise or global H-Markov properties are preserved under
isometries: Note that Q⊥
A := IK −QA = V ∗(IH0 −PA)V = V ∗P ⊥
A V .
We conclude that
QAQ⊥
S QB = V ∗(PAP ⊥
S PB)V and PAP ⊥
S PB = V (QAQ⊥
S QB)V ∗. Hence,
PAP ⊥
S PB = 0 ⇐⇒QAQ⊥
S QB = 0.
(30)
Step 2 (Reduction to Gaussian case): Now take y = (y1, . . . , yd) ∼N(0, Σ), say on the
same probability space, and let K = span{y1, . . . , yd}. By (30), and using the assumption
that x satisﬁes pairwise L2 property w.r.t. some G, we conclude that y also satisﬁes pairwise
L2 property w.r.t. G. Then, due to the equivalence of orthogonality and independence for
Gaussian random variables, y satisﬁes usual pairwise Markov property (deﬁned via conditional
independence) w.r.t. G. That is, G is a CIG for y.
Invoking (30) again, it is enough to show that if A and B are separated by S, then
QAQ⊥
S QB = 0, which is equivalent to ⟨u, Q⊥
S v⟩K = 0, ∀u ∈span{yA}, v ∈span{yB}. Since S
separates A and B, we know that u and v are independent given yS. This follows from [Lau96,
Thm. 3.9, p. 35] for CIGs, since N(0, Σ) has a.e. positive density w.r.t. Lebesgue measure,
assuming Σ ≻0. In particular,
E[uv|yS] = E[u|yS] E[v|yS] = (QSu)(QSv)
(31)
where the second equality uses Gaussianty again to write E[u|yS] as the L2 projection to the
linear span of yS. Taking expectation on both sides of (31), we have ⟨u, v⟩K = ⟨QSu, QSv⟩K
from which it follows that ⟨Q⊥
S u, Q⊥
S v⟩K = 0, which in turn is equivalent to ⟨u, Q⊥
S v⟩K = 0
since Q⊥
S is self-adjoint and idempotent.
B.4
Equivalence of the two directed PCG deﬁnitions
In this appendix, we show that the two deﬁnitions of a directed PCG given in (14) and (15)
are equivalent (under the acyclicity assumption). Recall that Πj and Nj denote, respectively,
the sets of parents and non-descendants of node j in the underlying graph G.
34

Assume ﬁrst that (14) holds and take i, j ∈[d]. Since G is a DAG, one of i and j is a
non-descendant of the other, say i ∈Nj.
(If i and j are adjacent, this means that i is a
parent of j.) Then, any parent of i will be a non-descendant of j, that is, Πi ⊂Nj. It follows
that PkP ⊥
ΠjPj = 0 for k ∈{i} ∪Πi. In particular, PΠiP ⊥
ΠjPj = 0 since PΠi = W
k∈Πi Pk (see
Lemma 5), and PiP ⊥
ΠjPj = 0. Since we have
PiP ⊥
ΠjPj = Pi(PΠi + P ⊥
Πi)P ⊥
ΠjPj
= PiPΠiP ⊥
ΠjPj + PiP ⊥
ΠiP ⊥
ΠjPj
(c)
= PiP ⊥
ΠiP ⊥
ΠjPj,
(32)
we obtain PiP ⊥
ΠiP ⊥
ΠjPj = 0, that is (15) holds for i and j.
Now assume that (15) holds and take j ∈[d] and i ∈Nj. Note that it is enough to establish
identity (32). An ancestor of i is any node that has a directed path to i; let us call such a
path, a path from the top to i. Let the depth of a node i be the length of the longest path
from the top to it. We proceed by the induction on the depth of node i. For depth zero, we
have Πi = ∅, hence PΠi = 0 and (32) holds trivially. Now assume that we have the result for
all non-descendants of j at depth at most r −1, and let i ∈Nj have depth r. Any k ∈Πi is
at depth at most r −1 and belongs to Nj, hence by the induction hypothesis PkP ⊥
ΠjPj = 0 for
k ∈Πj. It follows that PΠiP ⊥
ΠjPj = 0, hence (32)(c) holds which then means that (32) holds
as a whole. The proof is complete.
C
Abstract lattice theorem
In this appendix, we present an interesting generalization of the lattice Theorem 1. The result
and its argument are due to Tristan Bice [Bic]. Let A be a von Neumann algebra and P be
its projection lattice, ordered by p ≤q
⇐⇒
p = pq
⇐⇒
p = qp, the latter equivalence
being a result of the self-adjointness of p and q. The convention is to use lower case letters
for elements of A and P. Note that a general element b ∈A is a bounded operator on the
underlying Hilbert space, i.e., b ∈B(H). Similarly p ∈P is a projection operator in B(H).
Theorem 6. Q = {q ∈P : pa = qa} is a complete sublattice of P, ∀a ∈A and p ∈P.
Proof. Let [b] be the range projection of any b ∈A, i.e., projection onto the closure of the
range of b. For any q ∈P and a ∈A, we have [qa] ≤q (since the range of qa is included in the
range of q). Also note the identity (2) b = [b]b, ∀b ∈A.
If R ⊂Q, then for all q ∈R, we have (1) [pa] = [qa] ≤q, hence [pa] is lower bound on R.
Letting r := V R, by deﬁnition of inﬁmum, [pa] ≤r ≤q, ∀q ∈R, hence [qa] ≤r ≤q, ∀q ∈R
by (1). Hence,
ra = rqa
(By r ≤q ⇐⇒r = rq)
= r[qa]qa
(By (2) with b = qa)
= [qa]qa
(By [qa] ≤r ⇐⇒[qa] = r[qa])
= qa
(By (2) with b = qa)
= pa,
35

showing that r ∈Q. So, Q is closed under inﬁma.
Since pa = qa ⇐⇒p⊥a = q⊥a (recall p⊥= 1 −p), it follows that Q⊥:= {q⊥: q ∈Q}
is also closed under inﬁma. But since q⊥≤p⊥⇐⇒p ≤q, it follows that Q is closed under
suprema, ﬁnishing the proof.
36

