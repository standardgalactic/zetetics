FAdam: Adam is a natural gradient optimizer using
diagonal empirical Fisher information
Dongseong Hwang
Google LLC
Mountain View, CA, USA
dongseong@google.com
Abstract
This paper establishes a mathematical foundation for the Adam optimizer, elucidat-
ing its connection to natural gradient descent through Riemannian and information
geometry. We rigorously analyze the diagonal empirical Fisher information matrix
(FIM) in Adam, clarifying all detailed approximations and advocating for the use
of log probability functions as loss, which should be based on discrete distributions,
due to the limitations of empirical FIM. Our analysis uncovers flaws in the original
Adam algorithm, leading to proposed corrections such as enhanced momentum
calculations, adjusted bias corrections, adaptive epsilon, and gradient clipping. We
refine the weight decay term based on our theoretical framework. Our modified
algorithm, Fisher Adam (FAdam), demonstrates superior performance across
diverse domains including LLM, ASR, and VQ-VAE, achieving state-of-the-art
results in ASR.
1
Introduction
Natural Gradient Descent (NGD) is a powerful optimization method that has shown promise in
training machine learning models. Introduced by Amari [3], and revisited recently [11], NGD
offers an alternative to traditional gradient descent by taking into account the curvature of the loss
landscape. This is achieved through the use of the Fisher information matrix (FIM), which provides a
Riemannian metric for the statistical manifold. Despite its potential benefits, NGD faces a significant
computational challenge: the calculation of FIM, which can be prohibitively expensive for large
models like deep neural networks.
Adam [12] is the de facto standard optimizer, favored for its fast convergence and practicality. While
the original paper mentions the second momentum term can be interpreted as the diagonal FIM, a
comprehensive theoretical understanding of the inverse square root FIM, akin to the approach first
seen in AdaGrad [24], has remained elusive. This is in contrast to NGD, which utilizes the inverse
FIM directly.
Our main contribution is to provide an accessible mathematical explanation of the Adam optimizer,
drawing upon fundamental concepts from Riemannian geometry and information geometry, thereby
demonstrating that Adam is an approximation of NGD. This framework enables us to illuminate the
origin of the square root term in Adam and to advocate for the use of a log probability function as the
loss function when employing Adam.
We demonstrate that Adam utilizes the diagonal empirical Fisher information, providing a detailed ex-
planation of both the diagonal approximation and the empirical approximation. Notably, the empirical
approximation suggests that the loss function should be based on discrete distributions. Furthermore,
our analysis uncovers flaws in the original Adam algorithm, leading us to propose corrections such as
averaging natural gradient by momentum, adjusting bias corrections, and introducing adaptive epsilon
Preprint. Under review.
arXiv:2405.12807v2  [cs.LG]  23 May 2024

and gradient clip. We also enhance the weight decay using principles of Riemannian geometry. Our
modified algorithm, named Fisher Adam (FAdam), demonstrates strong performance across various
domains, as evidenced by our experiments with Large Language Models (LLMs) for text, Automatic
Speech Recognition (ASR) for speech, and Vector Quantized Variational Autoencoder (VQ-VAE) for
image. In particular, our ASR experiments achieve new state-of-the-art results.
2
Background
We use the notation from differential geometry, as introduced in Appendix A.1.
2.1
Gradient on Riemannian manifold
The inner product in the tangent space of a Riemannian manifold is defined by the Riemannian metric
tensor g = gijdi ⊗dj. The Riemannian metric tensor components, gij, are symmetric and positive
definite. Therefore, gij is always invertible, and its inverse is denoted as gij.
⃗v · ⃗u := g(⃗v, ⃗u) = gijdi ⊗dj(vk∂k, ur∂r) = gijvkurdi(∂k) ⊗dj(∂r) = gijviuj
(1)
The directional derivative of a scalar field ϕ along a vector ⃗v is defined as the dot product of the
gradient of ϕ. The directional derivative can also be expressed using the covariant derivative. By
rearranging the terms with respect to ∇ϕ, we obtain the following equation for the gradient ∇ϕ in
the tangent space TθM.
∇ϕ · ⃗v = (∇ϕ)ivjgij = D⃗vϕ = vj ∂ϕ
∂θj
(2)
(∇ϕ)i = gij ∂ϕ
∂θj →∇ϕ = gij ∂ϕ
∂θj ∂i
(3)
In Euclidean space, the Riemannian metric gij is Kronecker delta, so the gradient is usually expressed
as follows:
∇ϕ = ∂ϕ
∂θi ∂i
(4)
In ML community, equation Eq. (4) is often denoted as the gradient ∇ϕ, while equation Eq. (3)
is denoted as the natural gradient ˜∇ϕ. This notation conflicts with conventions in differential
geometry. Since our paper is focused on ML, we will adhere to ML conventions from this point
forward.
Typically, the component part of tensor operations are represented using matrix multiplication. The
metric tensor component, gij, is represented by a matrix G, and ∇ϕ is represented as a column
vector.
˜∇ϕ = G−1∇ϕ
(5)
Equation 1 can also be expressed using matrix multiplication.
⃗v · ⃗u = v⊤Gu
(6)
2.2
Fisher Information Matrix (FIM) and Natural Gradient
The Fisher information quantifies the amount of information an observable random variable x (repre-
senting data) conveys about an unknown parameter θ (representing a model parameter) influencing its
probability. Given the probability mass function P(x|θ) (or probability density function p(x|θ)) for
the random variable x, the Fisher information is defined as the variance of the score function, which
is the gradient of the log-likelihood function with respect to θ. Crucially, the Fisher information is
2

inherently symmetric and positive semi-definite by definition, and it serves as a Riemannian metric
on the statistical manifold.
F (θ) := Ex∼Pθ[∇θ log P(x|θ) ∇θ log P(x|θ)⊤].
(7)
Fisher information is the expected value of the Hessian matrix. (Proof provided in Appendix A.2.)
Fisher information represents the curvature of the log-likelihood on the statistical manifold where the
model parameters θ reside.
F (θ) = −Ex∼Pθ[∇2
θ log P(x|θ)] = −Ex∼Pθ[Hθ(log P(x|θ))].
(8)
In the realm of statistical manifolds, the distance between θ and θ + d is quantified by the Kullback-
Leibler divergence DKL(P(x|θ)∥P(x|θ + d)). For an infinitesimal displacement d, a second-order
Taylor series approximation reveals the Fisher information as the underlying distance metric [3, 11,
57]. A detailed proof can be found in Appendix A.3.
DKL(P(x|θ)∥P(x|θ + d)) ≈1
2d⊤F (θ)d.
(9)
As mentioned in Eq. (6), the magnitude of a vector v in the tangent space TθM can be expressed using
the inner product, and the Fisher Information Matrix (FIM) serves as the Riemannian metric [55].
⃗v · ⃗v = ∥v∥2 = v⊤Gv = v⊤F (θ)v
(10)
In statistical manifolds, the gradient is described using the Fisher information F in Eq. (5), and this
is called the natural gradient.
˜∇ϕ = F −1∇ϕ
(11)
The intuitive interpretation of Eq. (11) is that components with higher information undergo conserva-
tive movement, while components with lower information exhibit wider movement.
FIM exhibits a close relationship with the inverse covariance matrix of the score function, denoted as
Σ−1
θ . The Mahalanobis distance, dM(x, P)2, which measures the distance between a data point x
and a distribution P, is defined as dM(x, P)2 := (x −µ)⊤Σ−1
x (x −µ) [22]. Comparing this to
Eq. (10), we discern a connection between the Fisher information and the inverse covariance matrix:
higher covariance indicates lower information. This parallel echoes the principle in information
theory where higher entropy corresponds to lower information.
3
Toward FAdam
3.1
Loss and FIM
To utilize the natural gradient Eq. (11), we need to know both the gradient term and FIM. The
gradient term requires the calculation of the loss L(θ), while the FIM requires the score function
∇θ log P(x|θ). If the loss is expressed in the form of −log P(x|θ), we eliminate the need to calculate
the score function separately. Therefore, for using natural gradient optimizers like Adam1 [12], the
loss function must be in the form of the log-likelihood. We will delve deeper into the choice of
loss function in Section 3.3.1.
˜∇L(θ) = F −1∇L(θ) = Ex∼Pθ[∇θ log P(x|θ) ∇θ log P(x|θ)⊤]−1∇θ−log P(x|θ)
(12)
The model parameter θ is updated using the given natural gradient ˜∇L(θ), where η is the learning
rate.
1We will discuss why Adam is considered a natural gradient optimizer in Section 3.4.
3

θt+1 = θt −ηF −1∇L(θ)
(13)
Natural gradient is considered a second-order method because FIM is the expected value of the Hes-
sian, as shown in Eq. (8). A comparison with Newton’s method is provided in section Appendix B.1.
3.2
Diagonal Fisher Information
One key reason for Adam [12]’s significant success compared to other second-order methods is its
memory complexity. Adam scales linearly with the number of parameters, O(N), while methods
using FIM typically scale quadratically, O(N 2). For models with billions of parameters, O(N 2) is
impractical.
Adam utilizes the diagonal Fisher information matrix. As discussed in 2.2, the inverse FIM approxi-
mates the covariance of the score function. The diagonal FIM results in the loss of all covariance
information except for the variances. Interestingly, Adam’s success story suggests that the loss of
covariance information might not be detrimental in practice.
Let ˆF (θ) denote the diagonal FIM obtained by diagonalizing FIM in Eq. (7). The loss function (12)
is greatly simplified.
ˆF (θ) := Ex∼Pθ[∇θ log P(x|θ)2]
(14)
˜∇L(θ) = ˆF −1∇L(θ) = −
∇θlog P(x|θ)
Ex∼Pθ[∇θ log P(x|θ)2]
(15)
Amari u.a. [26] proves that the off-diagonal elements of FIM are smaller than the diagonal elements
by an order of 1/√n, where n represents the number of elements in the matrix. This finding justifies
the use of the quasi-diagonal natural gradient method when the weight matrices of each layer are
sufficiently large like LLM (Large Language Models).
Meanwhile, there have been efforts to capture important off-diagonal elements. K-FAC [15] employs
a Kronecker-factored approximation to the FIM. Shampoo [23] utilizes a low-rank approximation,
while SM3 [28] maintains a cover set of the parameters. The application of off-diagonal FIM to
Adam is left for future study.
3.3
Empirical Fisher Information
To compute the diagonal FIM in Eq. (14), the expected value needs to be calculated with respect to
the parametric probabilistic model P(x|θ). While data can be sampled from the parametric model, it
is not always a straightforward process. Various sampling methods exist, such as Gibbs sampling,
Langevin Markov chain Monte Carlo (MCMC) sampling [1], Metropolis-Hastings MC sampling [6]
and the recent GFlowNet [40]. However, none of these methods are universally efficient in generating
sufficient data with high fidelity and diversity.
Instead of P(x|θ), we utilize the true data-generating distribution pdata(x) to compute FIM. Although
the exact form of pdata(x) is unknown, we have access to a training set of samples. We compute
FIM by utilizing the training set D by substituting the true distribution pdata(x) with the empirical
distribution ˆpdata(x). This approximated FIM is referred to as the empirical FIM in the statistics
community [27]. The training set might not contain enough samples of low-probability x, which
could lead to issues with the empirical FIM.
ˆF (θ) = Ex∼Pθ[∇θ log P(x|θ)2] ≈Ex∼pdata[∇θ log P(x|θ)2],
(16)
≈Ex∼ˆpdata[∇θ log P(x|θ)2] =
1
|D|
X
x∈D
∇θ log P(x|θ)2
(17)
The expected value of the loss function is also obtained from the empirical distribution rather than
the true distribution. Optimizing this cost function J(θ) is referred to as empirical risk minimization,
for similar reasons.
4

J(θ) = −Ex∼pdata[log P(x|θ)] ≈−Ex∼ˆpdata[log P(x|θ)] = −1
|D|
X
x∈D
log P(x|θ)
(18)
∇θJ(θ) = ∇θEx∈D[L(θ)] =
1
|D|
X
x∈D
∇θL(θ) = −1
|D|
X
x∈D
∇θ log P(x|θ)
(19)
Calculating the exact cost function and FIM over the entire training set is computationally expensive.
Therefore, in practice, the expected value is approximated using a minibatch B. As this approximation
further increases the uncertainty in the empirical FIM, FIM is typically estimated using an exponential
moving average (EMA). Therefore, during training, natural gradient is computed as follows:
˜∇J(θ) = ˆF −1∇J(θ) ≈−Ex∈B[∇θ log P(x|θ)]/EEMA[Ex∈B[∇θ log P(x|θ)2]]
(20)
≈−Ex∈B[∇θ log P(x|θ)]/EEMA[Ex∈B[∇θ log P(x|θ)]2]
(21)
= −g/EEMA[g2]
(22)
Let the gradient of a minibatch be denoted as g. To reuse g for calculating FIM, Adam makes another
approximation from Eq. (20) to Eq. (21). Wang u.a. [54] showed that using Eq. (20) makes Adam
less sensitive to batch size. Adam variants are recommended to use large batch sizes [17, 49] to
accurately estimate not only the gradient but also FIM.
In supervised learning, the loss function becomes conditional log-likelihood, necessitating specific
considerations detailed in Appendix B.2.
3.3.1
Discrete vs Continuous probability distributions
It is worth noting that while Adam has demonstrated superior performance to SGD in the text
domain, it has been repeatedly reported that its convergence point in the image domain is worse than
SGD [25, 32, 38, 39, 49]. Empirical evidence indicates that Adam excels when dealing with discrete
distributions, such as text inputs with categorical distributions. However, it may encounter difficulties
when handling continuous distributions, such as image inputs with Gaussian distributions.
In the image domain, using the L2 loss is common practice due to its equivalence to the negative
log-likelihood of a Gaussian distribution.
J(θ) ≈−Ex∼ˆpdata[log p(x|θ)] = −Ex∼ˆpdata
"
k log exp
x −µ(θ)
σ
2#
(23)
= −kEx∼ˆpdata

(x −µ(θ))2
= −k
|D|
X
x∈D
(x −µ(θ))2
(24)
We performed empirical approximations to calculate the expected value of FIM. Eq. (16) represents
the empirical approximation of FIM for generative models, while Eq. (51) and Eq. (57) represent the
empirical approximations for discriminative models. We hypothesize that these empirical approxima-
tions cause significantly more problems in continuous distributions than in discrete distributions. This
disparity arises from the fundamental difference in how expected values are calculated for discrete and
continuous distributions. Discrete distributions rely on probability mass functions, where the softmax
function often concentrates the majority of probability mass on a few top logits. This concentration
allows for relatively accurate empirical approximations. In contrast, continuous distributions require
integration over their probability density functions, making the estimation of their expected values
with a single sampled value an overly simplified and potentially inaccurate approximation.
Kunstner u.a. [27] argue against the use of the empirical Fisher in natural gradient descent, providing
examples solely with continuous inputs and distributions.2 Their findings support our assertion that
2Regrettably, relying on image classification (i.e., continuous inputs) or, even more restrictively, simple
regression like curve fitting (i.e., continuous distribution) to analyze Adam or FIM may limit the generalizability
of findings [27, 29, 45].
5

Adam may not perform well with continuous distributions, while the empirical success of Adam
suggests that the empirical Fisher is adequate for discrete distributions.
We propose utilizing log-likelihood loss based on discrete probability distributions. In the image
domain, this translates to using cross-entropy loss on a categorical distribution instead of the L2
loss.3 This can be implemented by modifying predictions to utilize a one-hot encoding, predicting
256 values per RGB channel. As demonstrated in Section 4.3, this modification significantly enhances
the FID (Fréchet Inception Distance) metric for VQ-VAE [18]. In a float number regression case,
Hafner u.a. [48] reported that using two-hot encoding significantly improved the prediction of
rewards and values in reinforcement learning.
The exceptional scalability of Large Language Models (LLMs) [35, 37] with model size can likely
be attributed to two factors: the inherently discrete nature of text data and the widespread use of the
Adam optimizer. As LLMs continue to evolve into foundation models [42] for various modalities,
image, speech, and video domains are also adopting discrete token representations [46, 47, 53]. This
provides further evidence that empirical FIM estimation necessitates the use of discrete distributions.
3.4
Fisher Adam
Algorithm 1 Fisher Adam (FAdam)
1: given β1 = 0.9, β2 = 0.999, ϵ = 10−8, ϵ2 = 10−4, c = 1, λ = 0.001, ρ = 0.5, ηt
2: initialize θ0, t ←0, m0 ←0N, f0 ←1N
▷FIM init to 1 as per Section 3.4.4
3: repeat
4:
t ←t + 1
5:
gt ←∇θ log Pt(θt−1)
▷Stochastic gradient as per Eq. (12)
6:
ˆβ2 ←β2(1 −βt−1
2
)/(1 −βt
2)
▷Bias correction as per Section 3.4.4
7:
ft ←( ˆβ2ft−1 + (1 −ˆβ2)g2
t )
▷EMA diagonal empirical FIM as per Section 3.4.1
8:
ˆϵ ←min(ϵ, ϵ2RMS(gt))
▷Adaptive epsilon as per Appendix B.3
9:
¯gt ←gt/(f ρ
t + ˆϵ2ρ)
▷Invariant natural gradient as per Eq. (27)
10:
¯gt ←¯gt/ max(1, RMS(¯gt)/c)
▷Clip the gradient as per Appendix B.3
11:
mt ←(β1mt−1 + (1 −β1)¯gt)
▷EMA momentum as per Section 3.4.2
12:
¯gw ←θt−1/(f ρ
t + ˆϵ2ρ)
▷Weight decay as per Eq. (28)
13:
¯gw ←¯gw/ max(1, RMS(¯gw)/c)
▷Clip weight decay as per Appendix B.3
14:
θt ←θt−1 −ηt(mt + λ¯gw)
▷Update θ as per Eq. (13)
15: until stopping criterion is met
16: return optimized parameters θt
3.4.1
Reciprocal vs Reciprocal Square-root for FIM
We have come to realize that the second momentum term in Adam [12] is actually a diagonal empirical
FIM. However, in contrast to Eq. (22) and Eq. (59), where gradients are divided by FIM, Adam
divides gradients by the square root of FIM, as shown Algorithm 1.
To precisely update θt at each training step as per Eq. (13), we ideally need an accurate estimation of
FIM. However, the empirical FIM computed by the minibatch data B is noisy. Relying on a diagonal
empirical FIM can result in zero components, which causes the natural gradient to diverge due to
division by zero. To address this and obtain a more stable diagonal empirical FIM, an exponential
moving average (EMA) is employed.
The gradient and FIM in Eq. (22) vary at every point θ. As seen in Eq. (1), not only the components
of the gradient and FIM change but also their basis. However, EMA averages the Fisher information
components obtained at different θ points, despite the fact that the basis of each FIM is different. This
is a mathematically invalid operation.4 Fortunately, in differential geometry, there exists a quantity
that remains invariant under coordinate transformations: the magnitude of a vector derived from the
3Diffusion models [31] seem to perform well with L2 loss, because the loss is calculated not only on image
samples but also at noisy diffusion steps, potentially leading to a more accurate empirical FIM estimation. This
could be one of the reasons for the success of diffusion models.
4To add tensors from different tangent spaces, they must be parallel transported to the desired tangent space.
This requires knowledge of how the manifold’s basis changes over the manifold, which is determined by the
6

dot product. Given a gradient, its magnitude can be expressed as shown in Eq. (10). This expression
is further simplifies using the diagonal FIM in Eq. (26).
∥˜∇J(θ)∥2 = ˜∇J(θ) · ˜∇J(θ) = (F −1∇J(θ))⊤F (F −1∇J(θ)) = ∇J(θ)⊤F −1∇J(θ)
(25)
≈∇J(θ)⊤ˆF −1∇J(θ) = ∥∇J(θ)/
p
ˆF ∥2
(26)
¯∇J(θ) := ∇J(θ)/
p
ˆF
(27)
As β2=0.999 is standard value in Adam, EMA has a half-life of 700 steps, meaning it averages FIM
over roughly 1000 steps. During this time, the basis can shift significantly as the model parameter
moves to a new θ coordinate on the manifold. Since the invariant magnitude is the only reliable
quantity under coordinate changes, Eq. (26) can be subjected to EMA. We utilize this value for the
natural gradient and refer to it as the invariant natural gradient, denoted as ¯∇J(θ) in Eq. (27).
This is the reason why Adam uses the square root, as shown in Algorithm 3. In section C.1.1,
we conduct an ablation study on the exponent of the FIM term, and find that the square root is the
optimal choice.
Adam variants like AdaDelta [7], AdaMax [12] and Yogi [21] modify the second momentum to
further deviate from the natural gradient. A more principled approach would focus on improving
the estimation of the empirical FIM, incorporating off-diagonal elements, and investigating how to
make using natural gradient, instead of invariant natural gradient. If we were able to achieve a more
accurate FIM estimation with smaller β2, it might be possible to eliminate the square root operation.
3.4.2
Momentum
Momentum methods [2] employ exponential moving average (EMA) of gradients along the trajectory
because the stochastic gradients estimated from the minibatch B are noisy.
At point θ on the statistical manifold, the gradient is the invariant natural gradient, as shown
in Eq. (27). Therefore, momentum should average the invariant natural gradient ¯∇J(θ) instead
of the gradient, as shown in Algorithm 1. In Algorithm 3, Adam [12] calculates the first and
second momentums separately and then combines them, which lacks a theoretical background. In
Algorithm 4, Adafactor [20], on the other hand, already follows our proposed approach. Furthermore,
the second momentum is not momentum. We refer to it as FIM.
FAdam omits zero bias correction in EMA of momentum 5, because excessive gradient descent is
unnecessary before the momentum is sufficiently accumulated. This approach can be viewed as
built-in informed warmup schedule. Adafactor implementation [59] already omits zero bias correction
for momentum, although this is not explicitly mentioned in the paper [20].
3.4.3
Weight decay on manifold
AdamW [16], which decouples the weight decay term from the loss and applies it directly to the
Adam optimizer, has demonstrated general performance improvements and is now widely used as
a standard practice. Our mathematical framework provides a sound theoretical explanation for this
observed phenomenon. The loss function should be log-likelihood, but the weight decay term has
nothing to do with the probability distribution. If the weight decay term is included in the loss, it
causes problems in estimating the FIM.
Weight decay is a great example of how auxiliary losses should be handled. If the auxiliary loss is
related to the log-likelihood, it can be included in the loss. Otherwise, it should be bypassed by
the Adam optimizer, as in the case of weight decay.
Furthermore, following Eq. (11), weight decay should also be applied as a natural gradient. Since we
already have the FIM, we can express the weight decay gradient in a similar way to Eq. (27).
gw = θ/
p
ˆF
(28)
Riemannian metric field. However, accurately knowing the FIM even at a single point is computationally
prohibitive, making knowledge of the FIM field intractable.
5mt ←mt/(1 −βt
1)
7

Intuitively, components with low Fisher information can be pushed closer to zero without significantly
impacting the model performance. Elastic weight consolidation [19] leverages a similar concept,
utilizing Fisher information to regularize the change of θ.6
It has been reported that for training large-scale models like LLMs, decoupling weight decay from
the learning rate helps stabilize training [51]. However, since our modifications make the weight
decay adaptable to the loss surface, such workarounds might not be necessary. Further research is
needed to confirm this hypothesis.
3.4.4
FAdam
Fisher Adam (FAdam), incorporating all the discussed modifications, is presented in Algorithm 1.
Fisher Adafactor (FAdafactor) modification of Adafactor is presented in Algorithm 2 in Appendix B.4.
Additionally, we have specified that FIM (f0) is initialized to 1, not 0, in Algorithm 1. This is because
FIM represents a Riemannian metric, which defaults to the identity matrix in flat space. However,
this change does not affect the logic of the algorithm because bias correction ignores the initial value.
The bias correction for FIM is adopted from Adafactor [20], which is agnostic to the initial value.
3.4.5
Convergence
Algorithm 1 does not deviate from the assumptions of the convergence analysis in the Adam paper [12],
which require β1, β2 ∈[0, 1) that satisfy β2
1/√β2 < 1. Therefore, FAdam achieves the following
guaranteed regret bound to time ratio, for all T ≥1:
lim
T →∞
R(T)
T
= lim
T →∞O( 1
√
T
) = 0
(29)
4
Experiment
4.1
LLM (text domain)
We pretrained the 1B parameter LLM model from PaLM [50] on C4 dataset [33]. The hyperpa-
rameters used for FAdafactor were β1=0.9, β2=0.99, ϵ=1e-8, and λ=0.1. In contrast, the Adafactor
hyperparameters were β1=0.9, β2=0.99, ϵ=1e-30 and λ=0.001. As demonstrated in Fig. 1a, FAdafac-
tor outperforms Adafactor.
4.2
ASR (speech domain)
The 600M parameter Conformer [30] model from the w2v-BERT [44] is one of the lowest WER
(word error rate) achieving models on the LibriSpeech dataset [14]. This model was pretrained using
w2v-BERT and then finetuned on LibriSpeech data using the RNNT loss [9]. The WER was further
improved to SoTA levels using noisy student semi-supervised finetuning [34] on LibriLight [36].
We compared Adam and FAdam using models that were fine-tuned in a semi-supervised fashion on
LibriLight data, paired with semi-supervised pseudo labels.
FAdam and Adam have identical hyperparameters as follows: β1=0.9, β2=0.98, ϵ=1e-8, and λ=0.001.
As demonstrated in Fig. 1b and Table 1, FAdam not only outperforms Adam but also establishes a
new state-of-the-art (SoTA) Word Error Rate (WER) on LibriSpeech for 600M parameter models.
LibriSpeech WERs
dev
test
dev-other
test-other
avg
Adam (w2v-BERT paper [44])
1.30
2.60
1.40
2.70
2.00
Adam
1.30
2.54
1.33
2.59
1.93
FAdam
1.29
2.49
1.34
2.49
1.90
Table 1: LibriSpeech WERs
6EWC can directly utilize FIM from FAdam, eliminating the need to compute it separately.
8

Steps
log pplx
2.4
2.6
2.8
3.0
20000
40000
60000
80000
100000
Adafactor
FAdafactor
(a) LLM 1B (text domain)
Steps
Average WER
1.900
1.925
1.950
1.975
2.000
10000
20000
30000
40000
Adam
FAdam
(b) ASR 600M (speech domain)
Steps
FID
17
19
21
23
25
27
29
31
33
35
20000
40000
60000
80000
100000
L2 + AdamW
Discrete + AdamW
Discrete + FAdam
(c) VQ-VAE 100M (image domain)
Steps
log pplx
4.0
4.5
5.0
5.5
400
500
600
700
800
900
1000
1100
1200
1300
1400
1500
ρ=0.3
ρ=0.4
ρ=0.5
ρ=0.6
ρ=0.7
ρ=0.8
ρ=0.9
ρ=1.0
(d) Exponent sweep
Figure 1: Comparison of FAdam and Adam performance. (a) Eval loss (log pplx) on 1B LLMs,
presenting FAdafactor outperforms Adafactor. (b) Average WER on LibriSpeech using 600M
Conformer models, presenting FAdam outperforms Adam. (c) FID of ImageNet generation using
100M VQ-VAE models, presenting FAdam outperforms AdamW. (d) Comparison of FIM exponents
on a 1B LLM, showing 0.5 (square root) as the optimal choice.
4.3
VQ-VAE (image domain)
We trained a 100M parameter ViT VQ-GAN model [43] on the ImageNet dataset [13]. To verify
the hypothesis from Section 3.3.1 that categorical cross-entropy (CE) loss is superior to L2 loss on
Adam, we exclusively used either CE loss or L2 loss + logit-laplace during training.7 The VQ-GAN
paper [43] introduced logit-laplace to adjust the output scale when using L2 loss.
FAdam and AdamW [16] have identical hyperparameters as follows: β1=0.9, β2=0.99, ϵ=1e-8, and
λ=1e-4. As demonstrated in Fig. 1c, FAdam outperforms AdamW, and categorical CE loss not only
outperforms L2 loss but also eliminates the need for the complex logit-laplace transformation.
4.4
Ablation study
We conducted ablation studies on each hyperparameter of FAdam. Due to page limitations in the
main paper, the results are presented in Appendix C.1.
5
Conclusion
In this work, we have revealed the mathematical foundation of the Adam optimizer, clarifying
the constraints on loss function selection and the approximations involved in its derivation. This
groundwork opens avenues for future research to develop improved optimizers by mitigating these
approximations. Building upon our theoretical foundation, we have proposed an enhanced algorithm,
FAdam, and demonstrated its effectiveness across diverse domains.
7Since our primary focus is on replacing the L2 loss, we omitted GAN and perceptual losses, making the
model effectively a VQ-VAE [18] rather than a VQ-GAN.
9

Acknowledgement
The Riemannian geometry used in this paper was learned from the tensor calculus videos on the
YouTube channel eigenchris. The intuition that Adam is related to information geometry was sparked
by lectures on the YouTube channel Enjoying Math.
References
[1] Parisi, Giorgio(1981): Correlation functions and computer simulations, 3: 378–384.
[2] Rumelhart, David E / Hinton, Geoffrey E / Williams, Ronald J(1986): Learning representations
by back-propagating errors, 6088: 533–536.
[3] Amari, Shun Ichi(1998): Natural gradient works efficiently in learning, 2: 251–276.
[4] Schraudolph, Nicol N(2002): Fast curvature matrix-vector products for second-order gradient
descent, 7: 1723–1738.
[5] Martens, James / others u.a.(2010): Deep learning via hessian-free optimization.In: ICML735–
742.
[6] Neal, Radford M / others u.a.(2011): MCMC using Hamiltonian dynamics, 11: 2.
[7] Zeiler, Matthew D(2012): Adadelta: an adaptive learning rate method.
[8] Vinyals, Oriol / Povey, Daniel(2012): Krylov subspace descent for deep learningIn: Artificial
intelligence and statistics1261–1268.
[9] Graves, Alex(2012): Sequence transduction with recurrent neural networks.
[10] Pascanu, Razvan / Mikolov, Tomas / Bengio, Yoshua(2013): On the difficulty of training
recurrent neural networksIn: International conference on machine learning1310–1318.
[11] Pascanu, Razvan / Bengio, Yoshua(2013): Revisiting natural gradient for deep networks.
[12] Kingma, Diederik P / Ba, Jimmy(2014): Adam: A method for stochastic optimization.
[13] Russakovsky, Olga u.a.(2015): Imagenet large scale visual recognition challenge211–252.
[14] Panayotov, Vassil / Chen, Guoguo / Povey, Daniel / Khudanpur, Sanjeev(2015): Librispeech:
an asr corpus based on public domain audio booksIn: 2015 IEEE international conference on
acoustics, speech and signal processing (ICASSP)5206–5210.
[15] Martens, James / Grosse, Roger(2015): Optimizing neural networks with kronecker-factored
approximate curvatureIn: International conference on machine learning2408–2417.
[16] Loshchilov, Ilya / Hutter, Frank(2017): Decoupled weight decay regularization.
[17] Smith, Samuel L / Kindermans, Pieter Jan / Ying, Chris / Le, Quoc V(2017): Don’t decay the
learning rate, increase the batch size.
[18] Van Den Oord, Aaron / Vinyals, Oriol / others u.a.(2017): Neural discrete representation
learning.
[19] Kirkpatrick, James u.a.(2017): Overcoming catastrophic forgetting in neural networks, 13:
3521–3526.
[20] Shazeer, Noam / Stern, Mitchell(2018): Adafactor: Adaptive learning rates with sublinear
memory costIn: International Conference on Machine Learning4596–4604.
[21] Zaheer, Manzil / Reddi, Sashank / Sachan, Devendra / Kale, Satyen / Kumar, Sanjiv(2018):
Adaptive methods for nonconvex optimization.
[22] Mahalanobis, Prasanta Chandra(2018): On the generalized distance in statisticsS1–S7.
10

[23] Gupta, Vineet / Koren, Tomer / Singer, Yoram(2018): Shampoo: Preconditioned stochastic
tensor optimizationIn: International Conference on Machine Learning1842–1850.
[24] Lydia, Agnes / Francis, Sagayaraj(2019): Adagrad—an optimizer for stochastic gradient descent,
5: 566–568.
[25] Luo, Liangchen / Xiong, Yuanhao / Liu, Yan / Sun, Xu(2019): Adaptive gradient methods with
dynamic bound of learning rate.
[26] Amari, Shun ichi / Karakida, Ryo / Oizumi, Masafumi(2019): Fisher information and natural
gradient learning in random deep networksIn: The 22nd International Conference on Artificial
Intelligence and Statistics694–702.
[27] Kunstner, Frederik / Hennig, Philipp / Balles, Lukas(2019): Limitations of the empirical fisher
approximation for natural gradient descent.
[28] Anil, Rohan / Gupta, Vineet / Koren, Tomer / Singer, Yoram(2019): Memory efficient adaptive
optimization.
[29] Reddi, Sashank J / Kale, Satyen / Kumar, Sanjiv(2019): On the convergence of adam and
beyond.
[30] Gulati, Anmol u.a.(2020): Conformer: Convolution-augmented transformer for speech recogni-
tion.
[31] Ho, Jonathan / Jain, Ajay / Abbeel, Pieter(2020): Denoising diffusion probabilistic models6840–
6851.
[32] Yuan, Wei / Gao, Kai Xin(2020): EAdam Optimizer: How ϵ Impact Adam.
[33] Raffel, Colin u.a.(2020): Exploring the limits of transfer learning with a unified text-to-text
transformer, 140: 1–67.
[34] Park, Daniel S / Zhang, Yu / Jia, Ye / Han, Wei / Chiu, Chung Cheng / Li, Bo / Wu, Yonghui /
Le, Quoc V(2020): Improved noisy student training for automatic speech recognition.
[35] Brown, Tom u.a.(2020): Language models are few-shot learners1877–1901.
[36] Kahn, Jacob u.a.(2020): Libri-light: A benchmark for asr with limited or no supervisionIn:
ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing
(ICASSP)7669–7673.
[37] Kaplan, Jared u.a.(2020): Scaling laws for neural language models.
[38] Zhang, Jingzhao / Karimireddy, Sai Praneeth / Veit, Andreas / Kim, Seungyeon / Reddi,
Sashank / Kumar, Sanjiv / Sra, Suvrit(2020): Why are adaptive methods good for attention
models?15383–15393.
[39] Jelassi, Samy / Mensch, Arthur / Gidel, Gauthier / Li, Yuanzhi(2021): Adam is no better than
normalized SGD: Dissecting how adaptivity improves GAN performance.
[40] Bengio, Yoshua / Lahlou, Salem / Deleu, Tristan / Hu, Edward J / Tiwari, Mo / Bengio,
Emmanuel(2021): Gflownet foundations.
[41] Gilmer, Justin u.a.(2021): A loss curvature perspective on training instability in deep learning.
[42] Bommasani, Rishi u.a.(2021): On the opportunities and risks of foundation models.
[43] Yu, Jiahui u.a.(2021): Vector-quantized image modeling with improved vqgan.
[44] Chung, Yu An / Zhang, Yu / Han, Wei / Chiu, Chung Cheng / Qin, James / Pang, Ruoming / Wu,
Yonghui(2021): W2v-bert: Combining contrastive learning and masked language modeling
for self-supervised speech pre-trainingIn: 2021 IEEE Automatic Speech Recognition and
Understanding Workshop (ASRU)244–250.
[45] Cohen, Jeremy M u.a.(2022): Adaptive gradient methods at the edge of stability.
11

[46] Borsos, Zalán u.a.(2023): Audiolm: a language modeling approach to audio generation.
[47] Team, Gemini u.a.(2023): Gemini: a family of highly capable multimodal models.
[48] Hafner, Danijar / Pasukonis, Jurgis / Ba, Jimmy / Lillicrap, Timothy(2023): Mastering diverse
domains through world models.
[49] Kunstner, Frederik / Chen, Jacques / Lavington, Jonathan Wilder / Schmidt, Mark(2023): Noise
is not the main factor behind the gap between sgd and adam on transformers, but sign descent
might be.
[50] Chowdhery, Aakanksha u.a.(2023): Palm: Scaling language modeling with pathways, 240:
1–113.
[51] Wortsman, Mitchell u.a.(2023): Small-scale proxies for large-scale transformer training insta-
bilities.
[52] Jouppi, Norm u.a.(2023): Tpu v4: An optically reconfigurable supercomputer for machine
learning with hardware support for embeddingsIn: Proceedings of the 50th Annual International
Symposium on Computer Architecture1–14.
[53] Lu, Jiasen / Clark, Christopher / Lee, Sangho / Zhang, Zichen / Khosla, Savya / Marten, Ryan /
Hoiem, Derek / Kembhavi, Aniruddha(2023): Unified-io 2: Scaling autoregressive multimodal
models with vision, language, audio, and action.
[54] Wang, Xi / Aitchison, Laurence(2024): Batch size invariant Adam.
[55] Amari, Shun ichi (2012): Differential-geometrical methods in statistics. , Springer Science &
Business Media.
[56] Bonnans, Joseph Frédéric / Gilbert, Jean Charles / Lemaréchal, Claude / Sagastizábal, Claudia
A (2006): Numerical optimization: theoretical and practical aspects. , Springer Science &
Business Media.
[57] Kristiadi, Agustinus (2018): Natural Gradient Descent
https://agustinus.kristia.de/techblog/2018/03/14/natural-gradient/.
[58] Petersen, Peter (2006): Riemannian geometry. , Springer.
[59] Shazeer, Noam (2018): Adafactor implementation
https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/
utils/adafactor.py.
12

A
Background
A.1
Tensor Calculus Notation
In this paper, we will derive the Adam algorithm from the perspective of information geometry.
Since statistical formulas are frequently used in machine learning papers, but Riemannian geometry
formulas are not, we will first clarify the notation before proceeding. We follow the notation of
Petersen [58]’s Riemannian geometry textbook.
We define the tangent space at a point θ on a manifold as TθM. Given a vector ⃗v = viei on TθM,
the basis vectors are defined as partial derivatives as follows:
ei :=
∂
∂θi := ∂i
(30)
Therefore, a vector ⃗v can be expressed using Einstein notation as follows, with vi representing the
vector components and ∂i representing the basis vectors:
⃗v = v = vi∂i
(31)
Given a covector w = wiei in the dual space of a vector space, the covector basis is defined using a
differential one-form.
ei := dθi := di
(32)
The operation between the vector basis and the covector basis is the Kronecker delta. The operation
between vector ⃗v and covector w is as follows:
di(∂j) = δi
j
(33)
w(⃗v) = viwjdi(∂j) = viwi
(34)
The covariant derivative is an extension of the directional derivative operation to vectors and tensors.
When applied to a scalar field (i.e., a scalar function), the covariant derivative is called the directional
derivative. The covariant derivative of a vector field ⃗w along a vector ⃗v is denoted as follows, where
Γk
ij represents the Christoffel symbols:
D⃗v ⃗w = vi∂i(wj∂j) = vi∂iwj∂j + viwj∂i∂j = (vi∂iwk + viwjΓk
ij)∂k
(35)
A.2
The proof of Fisher is negative Hessian
The relationship between Fisher information and the negative Hessian’s expectation can be proven as
follows:
∇2
θ log P(x|θ) = ∇2
θP(x|θ)
P(x|θ)
−∇θ log P(x|θ) ∇θ log P(x|θ)⊤
(36)
Ex∼Pθ
∇2
θP(x|θ)
P(x|θ)

=
Z
P(x|θ)∇2
θP(x|θ)
P(x|θ) dx = ∇2
θ
Z
P(x|θ)dx = ∇2
θ1 = 0
(37)
Ex∼Pθ

∇2
θ log P(x|θ)

= −Ex∼Pθ

∇θ log P(x|θ) ∇θ log P(x|θ)⊤
(38)
A.3
The proof of KL approximation
As seen in Eq. (9), the Kullback-Leibler (KL) divergence with an infinitesimal displacement d can be
approximated by the Fisher information. We provide a proof of this relationship below, following the
approach presented in Kristiadi [57].
13

DKL(P(x|θ)∥P(x|θ + d)) ≈1
2d⊤F (θ)d.
(39)
By using a second-order Taylor series approximation, the KL divergence can be approximated as
follows:
DKL(Pθ∥Pθ+d) ≈DKL(Pθ∥Pθ)+(∇θ′DKL(Pθ∥Pθ′)|θ=θ′)⊤d+ 1
2d⊤∇2
θ′DKL(Pθ∥Pθ′)d (40)
DKL(Pθ∥Pθ) is 0 by the definition of KL divergence. The first-order approximation term also
becomes 0 through the following process.
∇θ′DKL(Pθ∥Pθ′) = ((((((((
∇θ′Ex∼Pθ [log Pθ] −∇θ′Ex∼Pθ [log Pθ′] = −Ex∼Pθ [∇θ′ log Pθ′]
(41)
= −
Z
Pθ∇θ′ log Pθ′|θ=θ′dx = −
Z
Pθ
∇θ′Pθ′
Pθ′
|θ=θ′dx
(42)
= −
Z
∇θPθdx = −∇θ
Z
Pθdx = −∇θ1 = 0
(43)
The Fisher information emerges from the second-order approximation term, as shown by utilizing the
Hessian property in Eq. (8).
∇2
θ′DKL(Pθ∥Pθ′) = ((((((((
∇2
θ′Ex∼Pθ [log Pθ] −∇2
θ′Ex∼Pθ [log Pθ′]
(44)
= −Ex∼Pθ

∇2
θ′ log Pθ′|θ=θ′
= −Ex∼Pθ

∇2
θ log Pθ

(45)
= F (θ)
(46)
Therefore, equation Eq. (9) is proven.
B
Toward FAdam
B.1
Compared to Newton’s second-order method
As shown in Eq. (8), FIM is equivalent to the Hessian of the loss.
F (θ) = −Ex∼Pθ[Hθ(log P(x|θ))] = Ex∼Pθ[Hθ(L(θ))].
(47)
Therefore, θ update Eq. (13) incorporates the Hessian term.
θt+1 = θt −ηEx∼Pθ[Hθ(L(θ))]−1∇L(θ)
(48)
As FIM is the expected value of the Hessian, natural gradient optimization is considered a second
order method. Since the Fisher Information Matrix (FIM) is positive semi-definite, the Hessian term
is also positive semi-definite, ensuring a convex optimization.
Meanwhile, Newton’s method is a second-order optimization method that is effective only when
the loss function is strongly convex with a Lipschitz continuous Hessian. The update equation for
Newton’s method [56] is as follows.
θt+1 = θt −ηHθ(L(θ))−1∇L(θ)
(49)
Although the two methods were derived through vastly different processes, they both involve the
inverse of the Hessian matrix, as shown in Eq. (48) and Eq. (49). However, there is a significant
difference between the two methods. Natural gradient descent requires the loss function to be the
log-likelihood, while Newton’s method has no such restriction on the choice of loss function. Natural
14

gradient descent guarantees convergence due to the positive semi-definiteness of FIM (the expected
value of the Hessian), whereas Newton’s method does not.
Consequently, various complex techniques are employed when using Newton’s method, such as the
Gauss-Newton algorithm, conjugate gradient method, and trust region method [56]. These techniques
ensure that each step update is confined within a trust region, mitigating the risk of divergence.
This is why second-order optimization methods based on FIM have demonstrated faster convergence
compared to Newton’s method in practice [4, 5, 8]. Recent second-order optimization methods often
utilize FIM instead of the Hessian of the loss function [23, 28]. However, these methods require the
loss function to be the log-likelihood and are subject to certain constraints discussed in Section 3.2
and Section 3.3.
B.2
Conditional probability distribution for empirical Fisher Information
In supervised learning, the loss function typically becomes a joint distribution −log P(x, y|θ). Since
the expected value over P(x, θ) is generally intractable, it is replaced with the empirical distribution
ˆpdata(x) in Eq. (51). As noted in Eq. (16), this is referred to as the empirical Fisher. The training
set also provides the ground truth label y. Therefore, the loss function becomes the cross-entropy
calculated using the conditional log-likelihood, as shown in Eq. (52).
∇θJ(θ) = −Ex,y∼pdata[∇θ log P(y|x, θ) + ∇θ log P(x|θ)]
(50)
≈−Ex,y∼ˆpdata[∇θ log P(y|x, θ) +(((((((
∇θ log ˆpdata(x)]
(51)
= −Ex,y∼ˆpdata[∇θ log P(y|x, θ)] := g
(52)
The FIM, similar to the cost function, also requires the calculation of an expected value over the
joint distribution. By approximating the expectation with the empirical distribution ˆpdata(x, y) and
moving out the square as shown in Eq. (21), we obtain the following equation:
ˆF (θ) = Ex,y∼P (y|x,θ)P (x|θ)[(∇θ log P(y|x, θ) + ∇θ log P(x|θ))2]
(53)
≈Ex,y∼P (y|x,θ)ˆpdata(x)[(∇θ log P(y|x, θ) +(((((((
∇θ log ˆpdata(x))2]
(54)
≈Ex,y∼P (y|x,θ)ˆpdata(x)[∇θ log P(y|x, θ)2]
(55)
As shown in equation Eq. (22), generative models are able to reuse the gradient g for calculating
FIM. However, this reuse poses a challenge when dealing with conditional distributions. This is
because Equation Eq. (52) calculates the expected value over the label y, while Equation Eq. (55)
calculates the expected value over the conditional distribution of the model. To address this, we
introduce an additional approximation by calculating the expected value over the label y in Eq. (56).
This is commonly referred to as the empirical FIM in the ML community [27], while the statistics
community refers to the approximation in Eq. (54) as the empirical FIM, as explained in Eq. (16). To
avoid confusion, we will refer to this as the conditional empirical FIM.
ˆF (θ) ≈Ex,y∼ˆpdata[∇θ log P(y|x, θ)2]
(56)
≈Ex,y∼ˆpdata[∇θ log P(y|x, θ)]2 = g2
(57)
As seen in Eq. (21), to reuse g in Eq. (52), a non-principled approximation is made by taking the
square outside the expectation. Similar to the generative model in equation Eq. (22), we can obtain
the natural gradient with respect to a minibatch B as follows:
˜∇J(θ) = ˆF −1∇J(θ) ≈−Ex,y∈B[∇θ log P(y|x, θ)]/EEMA[Ex,y∈B[∇θ log P(y|x, θ)]2]
(58)
= −g/EEMA[g2]
(59)
Despite the numerous non-trivial approximations made to FIM, it is remarkable that Adam has
achieved such great success. Eliminating these non-trivial approximations could be a promising
direction for future research.
15

B.3
Clipping and Epsilon
Although not mentioned in the original Adam paper, it was later discovered that gradient clipping
is essential for Adam [38, 41] and is now used as a standard practice. Adafactor [20] incorporates
clipping [10] in the algorithm. It is because the diagonal empirical FIM can have zero components or
the gradient may have very large values. Therefore, clipping is applied to the invariant natural
gradient, and then the clipped gradient is used to calculate the momentum, as demonstrated in
Algorithm 1. The Adafactor implementation [59] already incorporates this approach, although it
is not explicitly mentioned in the original paper [20], as shown in Algorithm 4. Since clipping is
already incorporated into the optimizer, global clipping [10] is not necessary.8 In our experiments,
we did not observe any benefit from applying global clipping.
To prevent division by zero before clipping, ϵ is added. The standard value for ϵ in Adam is 1e-8.
Adafactor [20] uses 1e-30, but since it is added inside the square root, this translates to 1e-15 on
the Adam scale. Wortsman u.a. [51] recommends using a smaller epsilon value of 1e-15 for large-
scale models like LLMs due to the empirical observation that the root mean square (RMS) value of
gradients tends to decrease as models get larger and training progresses.
To mitigate the need for manual tuning of ϵ across different models, we propose an adaptive
mechanism that adjusts ϵ based on the gradient RMS values: ˆϵ ←min(1e-8, 1e-4 × RMS(gt)),
as shown in Algorithm 1. Our experiments in Appendix C.1.2 demonstrate that this adaptive approach
performs comparably to manual tuning, where optimal epsilon values vary across model sizes and
domains.9
Moreover, in Algorithm 1, epsilon is raised to the power of 2ρ to ensure invariance with respect to the
FIM exponent (in Section 3.4.1). To keep the epsilon values in FAdam comparable to those used in
Adam, the exponent is multiplied by 2. 10 EAdam [32] and Adafactor [20] modified the algorithm to
accumulate ϵ within EMA, but we did not observe any benefits from this approach in our experiments.
B.4
FAdafactor
Adafactor [20] is modified as shown in Algorithm 2, and we refer to this variant as Fisher Adafactor
(FAdafactor).
Algorithm 2 Fisher Adafactor (FAdafactor)
1: given β1 = 0.9, β2 = 0.999, ϵ = 10−8, ϵ2 = 10−4, c = 1, λ = 0.001, ρ = 0.5, ηt
2: initialize θ0, t ←0, m0 ←0n×m, R0 ←1n, C0 ←1⊤
m
▷FIM init to 1 as per Section 3.4.4
3: repeat
4:
t ←t + 1
5:
gt ←∇θ log Pt(θt−1)
▷Stochastic gradient gt ∈Rn×m as per Eq. (12)
6:
ˆβ2 ←β2(1 −βt−1
2
)/(1 −βt
2)
▷Bias correction as per Section 3.4.4
7:
Rt ←ˆβ2Rt−1 + (1 −ˆβ2)g2
t 1m
▷EMA column vector Rt ∈Rn
8:
Ct ←ˆβ2Ct−1 + (1 −ˆβ2)1⊤
n g2
t
▷EMA row vector Ct ∈Rm
9:
ft ←RtCt/1⊤
n Rt
▷Diagonal empirical FIM as per Section 3.4.1
10:
ˆϵ ←min(ϵ, ϵ2RMS(gt))
▷Adaptive epsilon as per Appendix B.3
11:
¯gt ←gt/(f ρ
t + ˆϵ2ρ)
▷Invariant natural gradient as per Eq. (27)
12:
¯gt ←¯gt/ max(1, RMS(¯gt)/c)
▷Clip the gradient as per Appendix B.3
13:
mt ←(β1mt−1 + (1 −β1)¯gt)
▷EMA momentum as per Section 3.4.2
14:
¯gw ←θt−1/(f ρ
t + ˆϵ2ρ)
▷Weight decay as per Eq. (28)
15:
¯gw ←¯gw/ max(1, RMS(¯gw)/c)
▷Clip weight decay as per Appendix B.3
16:
θt ←θt−1 −ηt(mt + λ¯gw)
▷Update θ as per Eq. (13)
17: until stopping criterion is met
18: return optimized parameters θt
8This feature is often included in ML frameworks as a legacy practice without a clear justification.
9We attempted to adjust ϵ based on the weight size using the formula ˆϵ = ϵ/size(θ). However, this approach
did not improve performance, likely due to the resulting epsilon values for bias and layer normalization weights
becoming unsuitable.
10Since the default ρ is 0.5, ϵ2ρ is typically equal to ϵ.
16

B.5
Adam and Adafactor
To facilitate comparison between the original algorithms, we present Adam (Algorithm 3) and
Adafactor (Algorithm 4), alongside FAdam (Algorithm 1) and FAdafactor (Algorithm 2). Since the
full Adafactor algorithm is not explicitly detailed in the original paper [20], we refer the original
implementation [59].
Remarkably, Adafactor [20] had already empirically discovered and implemented several of the
results we derived from our mathematical framework: applying natural gradient to momentum
(Section 3.4.2), omitting bias correction for momentum (Section 3.4.2), and clipping gradients
(Appendix B.3). This is akin to the invention of the steam engine before the establishment of
thermodynamics, and these empirical findings significantly bolstered our confidence in developing
our theory. However, it is peculiar that the momentum calculation incorporates the learning rate,
while weight decay remains independent of the learning rate. Accumulating ϵ through EMA is also
an unconventional approach.
Algorithm 3 Adam [12]
1: given β1 = 0.9, β2 = 0.999, ϵ = 10−8, c = 1, λ = 0.001, ηt
2: initialize θ0, t ←0, m0 ←0N, f0 ←0N
3: repeat
4:
t ←t + 1
5:
gt ←∇θ log Pt(θt−1)
6:
ft ←(β2ft−1 + (1 −β2)g2
t )/(1 −βt
2)
7:
mt ←(β1mt−1 + (1 −β1)gt)/(1 −βt
1)
8:
¯gt ←mt/(√ft + ϵ)
9:
¯gt ←¯gt/ max(1, RMS(¯gt)/c)
10:
θt ←θt−1 −ηt(¯gt + λθt−1)
11: until stopping criterion is met
12: return optimized parameters θt
Algorithm 4 Adafactor [20, 59]
1: given β1 = 0.9, β2 = 0.999, ϵ = 10−30, c = 1, λ = 0.001, ηt
2: initialize θ0, t ←0, m0 ←0n×m, R0 ←0n, C0 ←0⊤
m
3: repeat
4:
t ←t + 1
5:
gt ←∇θ log Pt(θt−1)
6:
ˆβ2 ←β2(1 −βt−1
2
)/(1 −βt
2)
7:
Rt ←ˆβ2Rt−1 + (1 −ˆβ2)(g2
t + ϵ1n1⊤
m)1m
8:
Ct ←ˆβ2Ct−1 + (1 −ˆβ2)1⊤
n (g2
t + ϵ1n1⊤
m)
9:
ft ←RtCt/1⊤
n Rt
10:
¯gt ←gt/√ft
11:
¯gt ←¯gt/ max(1, RMS(¯gt)/c)
12:
mt ←(β1mt−1 + (1 −β1)ηt¯gt)
13:
θt ←θt−1 −(mt + λθt−1)
14: until stopping criterion is met
15: return optimized parameters θt
C
Experiment
C.1
Ablation study
C.1.1
Exponent of FIM
In Section 3.4.1, we provided a theoretical explanation for the use of the square root in FIM, as
shown in Eq. (27). To explore alternative exponent values, we conducted experiments, the results
of which are presented in Fig. 1d and Table 2. Exponents above 0.5 exhibit relative stability, while
17

values below 0.3 demonstrate a sharp decline in performance. The standard value of 0.5 for the FIM
exponent in Adam [12] appears to be a well-justified choice.
Model
Metric
Steps
ρ=0.3
ρ=0.4
ρ=0.5
ρ=0.6
ρ=0.7
ρ=0.8
ρ=0.9
ρ=1.0
LLM 1B
loss
1.5k
5.4
3.9
3.8
3.9
4.2
4.3
4.3
4.3
ASR 100M
WER
8k
43.9
11.2
6.04
6.08
6.04
6.08
6.23
6.33
Table 2: Effects of Varying FIM Exponent on LLM and ASR Model Performance
C.1.2
Epsilon
We conducted experiments to evaluate the effectiveness of an adaptive epsilon strategy (as mentioned
in Appendix B.3) and to investigate the impact of different (non-adaptive) epsilon values on the
performance of LLM (1B), ASR (600M), and VQ-VAE (100M) models. The results are presented in
Table 3.
While the optimal epsilon value varies across different size and domain models, the adaptive epsilon
approach demonstrated competitive performance without requiring manual tuning, highlighting its
potential for broader applicability. Notably, manual tuning sometimes led to divergence issues with
smaller epsilon values like 1e-20 or 1e-30.
Model
Metric
Steps
Adaptive
1e-8
1e-12
1e-15
1e-20
1e-30
LLM 1B
loss
80k
2.40
2.47
2.42
2.46
NaN
NaN
ASR 600M
WER
8k
2.04
2.08
2.07
2.08
2.05
2.13
VQ-VAE 100M
FID
50k
19.6
20.9
19.9
19.6
19.7
NaN
Table 3: Impact of Epsilon on Model Performance (Lower is Better)
C.1.3
Weight decay
As shown in Eq. (28), the most significant algorithmic change in FAdam compared to Adam is the
weight decay mechanism. Therefore, we conducted experiments for weight decay.
In the ASR experiments described in Section 4.2, the baseline model utilized Adam, and we found that
the weight decay parameter (λ) used in Adam could generally be reused for FAdam. Our enhanced
weight decay mechanism played a significant role in improving FAdam’s performance, as shown in
Table 4.
LibriSpeech WERs
dev
test
dev-other
test-other
avg
Adam λ=0
1.31
2.55
1.34
2.57
1.94
Adam λ=0.001
1.30
2.54
1.33
2.59
1.93
FAdam λ=0
1.30
2.43
1.34
2.63
1.92
FAdam λ=0.001
1.29
2.49
1.34
2.49
1.90
Table 4: LibriSpeech WERs of 600M ASRs with Varying Weight Decay Parameter (λ)
In the LLM experiments described in Section 4.1, the baseline model utilized Adafactor, which, as
shown in Algorithm 4, decouples weight decay from the learning rate. Consequently, models using
Adafactor require retuning the weight decay parameter λ. While 0.001 was found to be optimal for
Adafactor, Table 5 demonstrates that 0.1 worked best for FAdafactor.
C.2
Experiments Compute Resources
The LLM 1B model was trained on 16 TPUv5 [52] devices (80GB HBM) for one day. Each training
example consisted of 2k tokens with a global batch size of 256. The ASR 600M model was fine-tuned
on 32 TPUv3 devices (8GB HBM) for half a day. Each utterance had an average duration of 12
seconds with a global batch size of 512. The VQ-VAE 100M model was trained on 16 TPUv4 devices
18

λ
1
0.1
0.01
0.001
LLM 1B
3.007
2.875
2.998
3.004
Table 5: Eval loss of 1B LLMs with Varying Weight Decay Parameter (λ)
(16GB HBM) for one day. Training was performed using images with a resolution of 256x256 and a
global batch size of 256.
19

