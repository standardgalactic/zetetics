Random Graphs 

WILEY-INTERSCIENCE 
SERIES IN DISCRETE MATHEMATICS AND OPTIMIZATION 
ADVISORY EDITORS 
RONALD L. GRAHAM 
AT & T Laboratories. Florham Park. New Jersey, U.S.A. 
JAN KAREL LENSTRA 
Department of Mathematics and Computer Science. 
Eindhoven University of Technology, Eindhoven. The Netherlands 
A complete list of titles in this series appears at the end of this volume. 

Random Graphs 
SVANTE JANSON 
Uppsala University 
Sweden 
TOMASZ LUCZAK 
Adam Mickiewicz University 
Poland 
ANDRZEJ RUCINSKI 
Emory University 
Atlanta, Georgia 
A Wiley-Interscience Publication 
JOHN WILEY & SONS, INC. 
New York · Chichester · Weinheim · Brisbane · Singapore · Toronto 

A NOTE TO THE READER 
This book has been electronically reproduced from 
digital information stored at John Wiley &. Sons, Inc. 
We are pleased that the use of this new technology 
will enable us to keep works of enduring scholarly 
value in print as long as there is reasonable demand 
for them. The content of this book is identical to 
previous printings. 
Copyright © 2000 by John Wiley & Sons, Inc. 
All rights reserved. Published simultaneously in Canada. 
No part of this publication may be reproduced, stored in a retrieval system or 
transmitted in any form or by any means, electronic, mechanical, 
photocopying, recording, scanning or otherwise, except as permitted under 
Sections 107 or 108 of the 1976 United States Copyright Act, without either 
the prior written permission of the Publisher, or authorization through 
payment of the appropriate per-copy fee to the Copyright Clearance Center, 
222 Rosewood Drive, Danvers, MA 01923, (978) 750-8400, fax (978) 750-
4744. Requests to the Publisher for permission should be addressed to the 
Permissions Department, John Wiley & Sons, Inc., 605 Third Avenue, New 
York, NY 10158-0012, (212) 850-6011, fax (212) 850-6008, E-Mail: 
PERMREQ @ WILEY.COM. 
For ordering and customer service, call 1-800-CALL-WILEY. 
Library of Congress Cataloging-in-Publication Data is available. 
ISBN 0-471-17541-2 
10 9 8 7 6 5 4 3 2 1 

Preface 
The theory of random graphs originated in a series of papers published in the 
period 1959-1968 by two outstanding Hungarian mathematicians, Paul Erdös 
and Alfred Rényi. Over the forty years that have passed since then, the 
theory has developed into an independent and fast-growing branch of discrete 
mathematics, located at the intersection of graph theory, combinatorics and 
probability theory, with applications to theoretical computer science, reliabi-
lity of transportation and communication networks, natural and social sciences 
and to discrete mathematics itself. Aside from applications, random graphs 
continue to serve as nontrivial, but simple enough models for other, more 
complex random structures, paving the road for more advanced theories. 
In the early days, the literature on the subject was scattered around se-
veral probabilistic, combinatorial and general mathematics journals. In the 
late seventies, Béla Bollobas became the leading scientist in the field and 
contributed dozens of papers, which gradually made up a framework for his 
excellent, deep and extensive monograph Random Graphs, printed in 1985. 
The appearance of that book stimulated the research even further, shaping 
up a new theory. 
Two other ingredients that added to this trend were the ongoing series 
of international conferences on random graphs and probabilistic methods in 
combinatorics held biennially in Poznan, Poland, since 1983, and the journal, 
Random Structures and Algorithms, launched by Wiley in 1990. Both have 
established a forum for the exchange of ideas and cooperation in the theory 
of random graphs and related fields. 
v 

vi 
PREFACE 
It is not accidental then that tremendous progress has been made since 
1985. Over the last decade several new, beautiful results have been proved and 
numerous fine techniques and methods have been introduced. Our goal is to 
present many of these new developments, including results on threshold func-
tions (Ch. 1), small subgraphs (Ch. 3), generalized matchings (Ch. 4), phase 
transition (Ch. 5), limit distributions (Ch. 6), chromatic number (Ch. 7), par-
tition and extremal properties (Ch. 8), Hamiltonian cycles in random regular 
graphs (Ch. 9), and zero-one laws (Ch. 10). We emphasize new techniques and 
tools such as the martingale, Talagrand and correlation inequalities (Ch. 2), 
the orthogonal decomposition (Ch. 6), the Regularity Lemma of Szemerédi 
(Ch. 8), the Contiguity Theorem (Ch. 9), and the analysis of variance (Ch. 9). 
In a sense, our book can be viewed as an update on Bollobás's 1985 book. 
However, the topics selected for the book reflect the interest of its authors and 
do not pretend to exhaust the entire field. In fact, in order not to duplicate 
Bollobás's work, we do not include subjects which are covered there, on which 
only a little progress has been made. In particular, we have no sections on 
degree sequences, long paths and cycles, automorphisms, and the diameter. 
Moreover, we restrict ourselves to the main core of the theory and focus 
on the basic models of random graphs, making no attempt to present such 
rapidly developing areas as random walks on graphs, randomized algorithms or 
complexity of Boolean functions. Likewise, we exclude random cubes, directed 
graphs and percolation. 
It has been our goal to make the book accessible to graduate students in 
mathematics and computer science. This has led to simplifications of some 
statements and proofs, which, we hope, result in better clarity of exposi-
tion. The book may be used as a textbook for a graduate course or an 
honors course for undergraduate senior mathematics and computer science 
majors. Although we do not provide problems and exercises separately, we 
often leave to the reader to complete parts of proofs or to provide proofs of re-
sults analogous to those proven. These instances, marked by the parenthetic 
phrase "(Exercise!)", can easily be picked up by the instructor and turned 
into homework assignments. The prerequisites are limited to basic courses 
in graph theory or combinatorics, elementary probability and calculus. We 
believe that the book will also be used by scientists working in the broad 
area of discrete mathematics and theoretical computer science. It is both an 
introduction for newcomers and a source of the most recent developments for 
those working in the field for many years. 
We would like to thank several friends and colleagues, without whom this 
book would be a.a.s. worse than it is. Among those whose insightful remarks 
and suggestions led to improvements of earlier drafts are: Andrzej Czygrinow, 
Dwight Duffus, Ehud Friedgut, Johan Jonasson, Michal Karonski, Yoshiharu 
Kohayakawa, Michael Krivelevich, Justyna Kurkowiak, Jifi Matousek, Bren-
dan Nagle, Yuejian Peng, Joanna Polcyn, Vojtéch Rödl, Jozef Skokan, Joel 
Spencer, Edyta Szymanska, Michelle Wagner, and Julie White. 

PREFACE 
vii 
Special thanks are due to Penny Haxell and Izolda Gorgol. Penny spent 
several days correcting our English. Without her tedious work the text would 
probably need subtitles to be understood by an American reader. Izolda 
generously exercised her editing skills providing us with electronic files of all 
figures. 
Jessica Downey, the Wiley editor, earned our deep appreciation for her 
continuous enthusiasm and support of the project. 
Finally, the three authors would like to thank each other for patience, mu-
tual encouragement and persistence in negotiations, the compromising effect 
of which is now in your hands. 
SVANTE JANSON 
TOMASZ LUCZAK 
ANDRZEJ RUCINSKI 
Uppsala, Poznan, and Atlanta 

Contents 
Preface 
Preliminaries 
1.1 
Models of random graphs 
1.2 
Notes on notation and more 
1.3 
Monotonicity 
1-4 
Asymptotic 
equivalence 
1.5 
Thresholds 
1.6 
Sharp thresholds 
Exponentially Small Probabilities 
2.1 
Independent 
summands 
2.2 
Binomial random subsets 
2.3 
Suen's inequality 
2.4 
Martingales 
2.5 
Talagrand's inequality 
2.6 
The upper tail 
Small Subgraphs 
3.1 
The containment problem 
V 
1 
1 
6 
12 
U 
18 
20 
25 
26 
30 
34 
31 
39 
48 
53 
53 
ix 

CONTENTS 
3.2 
Leading overlaps and the subgraph plot 
62 
3.3 
Subgraph count at the threshold 
66 
3.4 
The covering problem 
68 
3.5 
Disjoint copies 
75 
3.6 
Variations on the theme 
77 
4 
Matchings 
81 
4.1 
Perfect matchings 
82 
4.2 
G-factors 
89 
4-3 
Two open problems 
96 
5 
The Phase Transition 
103 
5.1 
The evolution of the random graph 
103 
5.2 
The emergence of the giant component 
107 
5.3 
The emergence of the giant: A closer look 
112 
5.4 
The structure of the giant component 
121 
5.5 
Near the critical period 
126 
5.6 
Global properties and the symmetry rule 
128 
5.7 
Dynamic properties 
134 
6 
Asymptotic Distributions 
139 
6.1 
The method of moments 
140 
6.2 
Stein's method: The Poisson case 
152 
6.3 
Stein's method: The normal case 
157 
6.4 
Projections and decompositions 
162 
6.5 
Further methods 
176 
7 
The Chromatic Number 
179 
7.1 
The stability number 
179 
7.2 
The chromatic number: A greedy approach 
184 
7.3 
The concentration of the chromatic number 
187 
7.4 
The chromatic number of dense random graphs 
190 
7.5 
The chromatic number of sparse random graphs 
192 
7.6 
Vertex partition properties 
196 
8 
Extremal and Ramsey Properties 
201 
8.1 
Heuristics and results 
202 
8.2 
Triangles: The first approach 
209 

CONTENTS 
xi 
8.3 
The Szemerédi Regularity Lemma 
8.4 
A partition theorem for random graphs 
8.5 
Triangles: An approach with perspective 
9 Random Regular Graphs 
9.1 
The configuration model 
9.2 
Small cycles 
9.3 
Hamilton cycles 
9.4 
Proofs 
9.5 
Contiguity of random regular graphs 
9.6 
A brief course in contiguity 
10 Zero-One Laws 
10.1 
Preliminaries 
10.2 Ehrenfeucht games and zero-one laws 
10.3 Filling gaps 
IO.4 Sums of models 
10.5 Separability and the speed of convergence 
212 
216 
222 
233 
235 
236 
239 
247 
256 
264 
271 
271 
273 
285 
292 
301 
References 
307 
Index of Notation 
327 
Index 
331 

1 
Preliminaries 
1.1 
MODELS OF RANDOM GRAPHS 
The notion of a random graph originated in a paper of Erdös (1947), which 
is considered by some as the first conscious application of the probabilistic 
method. It was used there to prove the existence of a graph with a specific 
Ramsey property. 
The model introduced by Erdös is very natural and can be described as 
choosing a graph at random, with equal probabilities, from the set of all 2(3) 
graphs whose vertex set is [n] = {1,2,...,n}. 
In other words, it can be 
described as the probability space (Ω, T, P), where Ω is the set of all graphs 
with vertex set [n], T is the family of all subsets of Ω, and for every ω 6 Ω 
P(w) = 2-(S). 
This probability space can also be viewed as the product of (£) binary 
spaces. In simple words, it is a result of (£) independent tosses of a fair coin, 
where "turning up heads" means "drawing an edge". 
Generally speaking, a random graph is a graph constructed by a random 
procedure. In accordance with standard definitions in probability theory, this 
is formalized by representing the "random procedure" by a probability space 
(Ω, T,V) and the "construction" by a function from the probability space into 
a suitable family of graphs. The distribution of a random graph is the induced 
probability distribution on the family of graphs; for many purposes this is the 
only relevant feature of the construction and we usually do not distinguish 
between different random graphs with the same distribution. Indeed, it is 
I 
Random Graphs 
by Svante Janson, Tomasz Luczak and Andrzej Rucinski 
Copyright © 2000 John Wiley & Sons, Inc. 

2 
PRELIMINARIES 
often convenient to define a random graph by specifying its distribution; that 
is, we specify a family of graphs and a probability distribution on it. Note, 
however, that it is not sufficient to formally define a random graph as a prob-
ability distribution only, as is sometimes done in the literature; an important 
case in which this would not do is when several random graphs are considered 
at once, for example, in the two-round exposure described at the end of this 
section. 
The word "model" is used rather loosely in the theory of random graphs. 
It may refer to a specific class of random graphs, defined as above, or perhaps 
to a specific distribution. Usually, however, there is also a parameter involved 
which measures the size of the graphs and typically it tends to infinity; there 
may also be other parameters. Needless to say, the whole theory of random 
graphs is thus asymptotic in its nature. 
Two basic models 
Nowadays, among several models of random graphs, there are two basic ones, 
the binomial model and the uniform model, both originating in the simple 
model introduced by Erdös (1947). In this book we will mainly restrict our-
selves to studying these two models. 
Given a real number p, 0 < p < 1, the binomial random graph, denoted by 
G(n,p), is defined by taking as Ω the set of all graphs on vertex set [n] and 
setting 
P ( G ) = p e c ( l - p ) ( S ) - e ° , 
where eo = |£(G)| stands for the number of edges of G. It can be viewed as a 
result of (!J) independent coin flippings, one for each pair of vertices, with the 
probability of success (i.e., drawing an edge) equal to p. For p = 1/2 this is 
the model of 1947. However, most of the random graph literature is devoted 
to cases in which p — p(n) - f O a s n - » o o . 
The binomial model is a special case of a reliability network. In this more 
general model, Ω is the family of all spanning subgraphs of a given graph F 
and P(G) = p e°(l - p)eF~ea. 
By a spanning subgraph we mean a graph G 
such that V(G) = V(F) and E(G) C E(F). 
Thus, in a reliability network, 
the edges of a given graph (network) are independently destroyed, each with 
failure probability 1 — p. One can generalize this model even further, by 
allowing different probabilities of failure at different edges. (Binomial models 
are sometimes called Bernoulli.) 
Taking F = Kn, the complete graph on n vertices, we obtain the model 
G(n,p). 
Taking F = Km,n, 
the complete bipartite graph (here either m 
is a function of n, or they are two independent parameters, typically both 
tending to infinity), we obtain the bipartite random graph G(m,n,p). 
Other 
popular models, not discussed here, are those in which the initial graph F 
is the hypercube or the n x n square lattice. The reliability network based 

MODELS OF RANDOM GRAPHS 
3 
on the infinite square lattice belongs to percolation theory (Grimmett 1992a) 
which too, as all infinite models, is beyond the scope of this book. 
The main advantage of the binomial model G(n,p) is the independence 
of presence of edges, but the drawback is that the number of edges is not 
fixed; it varies according to a binomial distribution with expectation (")p. If 
one conditions on the event that \E(G{n,p))\ 
= M, then a uniform space is 
obtained. This space can be defined directly. 
Given an integer M, 0 < M < (!?), the uniform random graph, denoted by 
G(n, M), is defined by taking as Ω the family of all graphs on the vertex set 
[n] with exactly M edges, and as P the uniform probability on Ω, 
P(G) = ( $ ) 
· 
G e 
Ω· 
This model, closely related to enumerative combinatorics, was apparently 
considered already in 1939 in an unpublished work of Erdos and Whitney on 
the connectedness of almost all graphs with n vertices and about M = | n log n 
edges. This was the model used throughout by Erdös and Rényi in their series 
of papers between 1959 and 1968, which gave rise to the theory of random 
graphs. (For an account of the contents of these eight fundamental papers, 
see Karonski and Rucinski (1997).) 
The two basic models are in many cases asymptotically equivalent, provided 
(™)p is close to M (see Section 1.4). 
The uniform random graph G(n, M) belongs to a broad family of uniform 
random graphs defined by taking the uniform distribution over a family of 
graphs T. The pioneering model from Erdos (1947) belongs here too, with 
T being the family of all graphs on a given set of vertices. Other popular 
models of this type are random trees (not studied in this book), where T 
is the family of all nn~2 trees on n labeled vertices, and random r-regular 
graphs (see Chapter 9), where T is the family of all graphs on n vertices of 
equal degree r, provided nr is even. We will use G(n, r) to denote a uniform 
random r-regular graph. It may look dangerous to use the notation G(n,p), 
G(n,M) and G(n,r) for three different things: What is G(n, 1)? In practice, 
however, the correct meaning is always clear from the context. (As for the 
three models: G(n,p) with p = 1, G(n,M) with M = 1, and G(n,r) 
with 
r = 1, each one is rather dull.) 
Both the binomial and the uniform model have their counterparts for di-
rected graphs. Besides these, there are interesting, natural random directed 
graphs which do not have analogues in the undirected case. Let us mention 
the ¿-out model, in which every vertex independently chooses k out-neighbors 
(including or excluding itself); the case of random mappings (i.e., k = 1) is 
well studied (Kolchin 1986, Aldous and Pitman 1994). Random tournaments, 
in which every edge of a complete graph assumes randomly one of the two pos-
sible orientations, have a broad literature too (Moon 1968, Gruszka, Luczak 
and Rucinski 1996, Andersson 1998). 

4 
PRELIMINARIES 
There are still other random graphs which do not fall into either category 
(binomial or uniform). For instance, in some reliability networks the vertices 
but not the edges are destroyed. Furthermore, some random graphs result 
from more complex probabilistic experiments, and here the sky is the limit. 
Restricted random graph processes constitute an interesting class of such ex-
periments, but we should better define the unrestricted case first. 
Random graph processes 
In general, a random graph process is a stochastic process that describes a 
random graph evolving in time. In other words, it is a family {G(t)}t of 
random graphs (defined on a common probability space) where the parameter 
t is interpreted as time; the time can be either discrete or continuous. The 
processes studied here will have a fixed vertex set (typically [n]), and they will 
start without any edges and grow monotonically by adding edges according 
to some rule but never deleting any. 
A simple and important random graph process {G(n, M)}M (sometimes 
called the random graph process) was introduced by Erdös and Rényi (1959) 
and has been well studied since then. It begins with no edges at time 0 and 
adds new edges, one at a time; each new edge is selected at random, uniformly 
among all edges not already present. Hence this random graph process is a 
Markov process, with time running through the set {0,1,..., (2)}· The M-th 
stage of this process can be identified with the uniform random graph G(n, M). 
The process, however, allows one to study the random graph G(n, M) as it 
evolves with M growing from 0 to ("). For example, a typical result, meaning-
ful only for random graph processes, says that, with probability approaching 
1 as n -> 00, the very edge which links the last isolated vertex with another 
vertex makes the graph connected (Bollobas and Thomason (1985); see also 
Bollobás (1985)). 
A related continuous time random graph process can be defined by assign-
ing a random variable Te to each edge e of the complete graph Kn, such 
that the (") variables Te are independent with a common continuous dis-
tribution, and then defining the edge set of {G(t)}t to consist of all e with 
Te < t. Clearly, the resulting random graph {G(t)}t0 at a fixed time to can 
be identified with the binomial random graph G(n,p), where p = P(Te < to). 
Furthermore, since almost surely no two values of the random variables Te co-
incide, we may define T^) as the random time at which the i-th edge is added. 
Then, by symmetry, G(T(¿)) is the uniform random graph G(n,i), and the 
sequence {G(T(¿))} for t = 1,..., (2), equals the ordinary random graph pro-
cess {G(n,M)}M defined above. Hence, this continuous time random graph 
process is a joint generalization of the binomial random graph, the uniform 
random graph and the standard discrete time random graph process. 
Clearly, different choices of the distribution of Te affect the model only 
trivially, by a change in the time variable. The continuous time evolving 

MODELS OF RANDOM GRAPHS 
5 
model was introduced by Stepanov (1970) with Tr exponentially distributed; 
we prefer the uniform distribution over the interval [0,1], in which case p = 
W(Te < t) = t, 0 < t < 1. Thus, we may unambiguously use the notation 
{Gin,«)}!-
Recently, a number of restricted random graph processes have been stud-
ied. In general, such a process can be defined as a random graph process 
in which edges are chosen one by one uniformly from a dynamically mod-
ified set of available pairs of vertices until this set becomes empty. More 
formally, consider a Markov chain of random edge sets Eo - 
9,Ei,...,E„ 
where Ei = {e\,...,ei\ 
and e< is chosen uniformly from a set A{ which de-
pends only on the set £?<_ι. 
In one of these restricted models, studied by Rucinski and Wormald (1992), 
the maximum degree is bounded from above by a given integer d. Thus, the 
set Ai contains only those pairs whose addition to the set 2?i_i does not create 
a vertex of degree d + 1. The graph at the end of the process may not be 
d-regular, though it is shown to be so with probability approaching 1. See 
also Wormald (1999a), where, moreover, further related processes are defined 
and studied. 
Another restricted process is studied by Erdös, Suen and Winkler (1995), 
in which it is not allowed to create a triangle. In this model it is even an open 
problem to determine the length of a typical process, measured by the number 
of edges in the final graph. It is only known that with high probability the 
process takes more than ein3/2 but fewer than C2n3/2logn steps, where c\ 
and c2 are positive constants. Recently, this result was generalized to a wide 
class of forbidden subgraphs by Osthus and Taraz (2000+). 
By forbidding cycles, one obtains a process which creates a non-uniform 
random tree (Aldous 1990), while forbidding components with more than one 
cycle leads to a random graph which still is to be studied. 
Random subsets 
The two basic models of random graphs fall into the framework of random 
subsets of a set. Monotonicity, equivalence and threshold behavior of the 
probabilities of properties of random graphs can often be proved at no extra 
cost in this general setting. Other principal examples of random subsets of a 
set include random sets of integers and random hypergraphs. In the remaining 
sections of this chapter (as well as in parts of Chapter 2) we will mainly study 
this more general random set framework. For an arbitrary set X and an 
integer k, let [X]k stand for the family of all Jk-element subsets of X. 
If 
X = [n], we will simplify this notation to [n]k. 
Let Γ be a finite set, |Γ| = N, let 0 < p < 1 and 0 < M < N. 
Then 
the random subset Γρ of Γ is obtained by flipping a coin, with probability 
pof success, for each element of Γ to determine whether the element is to 
be included in Γρ; the distribution of Γρ is the probability distribution on 
Ω = 2 r given by P(F) = plFl(l - p)l rHf| for F C Γ. Similarly, let TM be 

6 
PRELIMINARIES 
a randomly chosen element of [Γ]Μ; that is, ΓΜ has the uniform distribution 
nF) = 
{'¿í)~lforF£[r}M. 
Taking Γ = [n]2 we obtain the two basic models of random graphs defined 
above, G(n,p) and G(n,M). 
The binomial model Γρ can be generalized to ΓΡι ,P/V, where the element 
i is included with probability p¿, independently for all i - 1,..., N. 
Two-round exposure 
The two-round exposure is a successful proof technique applicable to the bi-
nomial model. It relies on viewing Γρ as a union of two independent random 
subsets ΓΡι and Γρ,, where p\ and p¿ are such that p = Pi + P2 — P1P2· (It 
is easy to see that this union indeed is distributed as Γρ - Exercise!) In the 
special case of random graphs we first generate a random graph G(n, p\) and 
then, independently, another random graph G(n,p2) on the same vertex set. 
By replacing double edges by single ones, we obtain G(n,p). 
An argument typically used in applications of the two-round exposure can 
be expressed in the following general form. Let Pi be the probability distri-
bution associated with Γρ,, and let P F be the conditional probability in Γρ 
under the condition Γρ, = F. Then for any two families Λ and B of subsets 
ofT 
PM) > Σ 
P F M ) P I ( F ) > PFoM)P1(ß), 
(1.1) 
Fee 
where FQ minimizes the probability PF(-4) over all F € B. Thus, knowing 
that Pi(ß) -> 1, in order to prove that also Ψ(Α) -* 1, it is enough to 
show that PF(-4) -» 1, for every F e B. In practice, computing the last 
probability means fixing an instance of ΓΡ1 6 B and throwing in new elements 
independently with probability pi (the second round of exposure). 
1.2 
NOTES ON NOTATION AND MORE 
Graph theory 
All graphs are simple and undirected, unless otherwise stated. We use stan-
dard notation for graphs. For example, V(G) is the vertex set of a graph G, 
E{G) is the edge set, VQ = |V(G)| is the number of vertices and ec = \E(G)\ 
is the number of edges; for typographical reasons we sometimes write the 
latter two as v(G) and e(G). In this book the size of G always means v(G) 
(and not e(G) as sometimes used by other authors). However, we also will 
call v(G) the order of G. 
Moreover, let d(G) = βσ/υσ be the density and m(G) = maxHCGd(H) 
the maximum density of G. (Note that d(G) equals half the average degree 
of G, and that some authors define d(G) as the average degree, which is twice 

NOTES ON NOTATION AND MORE 
7 
our value.) Another measure of the density of a graph G, ranging between 0 
and 1, is defined as p(G) = e(G)/(" (^'). (It is sometimes called the relative 
density of G.) 
Furthermore, 6(G) is the minimum degree, Δ((7) is the maximum degree, 
x(G) is the chromatic number, D(G) = max//cG &(H)1S t n e degeneracy num-
ber, a{G) is the stability number (the size of the largest stable, or independent, 
set of vertices), and aut(G) is the number of automorphisms of G. 
We let N(v) = NG(V) denote the neighborhood of a vertex v in G, that 
is, the set {w € V(G) : vw € E(G)}. 
Its size is called the degree of v and 
is denoted by deg(i>) = degG(t/). Similarly, if 5 C V(G), its neighborhood 
NG(S) 
= U„es NG{V) \ 5 is the set of all vertices outside 5 adjacent to at 
least one vertex in S. Moreover, we let Nc(v) 
= NG(V) U {V} and NG(S) = 
NG(S) 
U 5 denote the corresponding closed neighborhoods, which include v 
and 5, respectively. 
Any graph without edges will be called empty, while the graph with no 
vertices (and thus no edges) will be called the null graph and denoted by 0. 
Some special graphs are: the complete graph Kn on n vertices, the complete 
bipartite graph Km,n on m + n vertices, the cycle C* with k vertices, and the 
path Pk with k edges and thus k +1 vertices. A star is any graph K\t„,n 
> 0. 
We let jG denote the union of j vertex-disjoint copies of G. A matching is a 
forest consisting of isolated edges only (i.e., a graph of the form jK2, j > 0). 
If G is a graph and V C V(G), then G[V] denotes the restriction of G to 
V, defined as the graph with vertex set V and edge set E(G) Π [V]2; similarly, 
if E C [V(G)]2, G[E] denotes the graph with vertex set V(G) and edge set 
E(G) D E. A subgraph of G of the type G[V] is called induced or spanned 
by V, while a subgraph of the type G[E] is called spanning. The number of 
edges in the subgraph G[V] is sometimes denoted by ec(V) = e(V), while for 
two disjoint subsets A,B c V(G), the quantity eo{A, B) counts the number 
of edges of G with one endpoint in A and the other in B. 
By a copy of a given graph G inside another graph F we mean any, not 
necessarily induced, subgraph of F which is isomorphic to G. If the subgraph 
happens to be induced, we call it an induced copy of G. 
Although we define our random graphs as labelled, we are mainly inter-
ested in properties that are independent of the labelling, that is, properties 
that depend on the isomorphism type only. Such properties are called graph 
properties. (In contrast, "vertex 1 is isolated" is not a graph property; such 
properties will occasionally be studied too.) 
Probability 
We use Bi(n,p), Be(p) = Bi(l,p), Ρο(λ) and Ν(μ,σ2) to denote the binomial, 
Bernoulli, Poisson and normal distributions, respectively. We further write 
X 6 C, meaning that X is a random variable with distribution C. (e.g., X € 

8 
PRELIMINARIES 
N(0,1)). The distribution of a random variable X is occasionally denoted by 
C(X). 
We denote by 1[£] the indicator function of the event £, which equals 1 
if £ occurs and 0 otherwise. We will often consider random variables that 
are the indicator functions of some events; such random variables will be 
called indicator or zero-one random variables. They clearly have Bernoulli 
distributions with p = P(£), where £ is the corresponding event. 
The expected value and the variance of a random variable X (if they ex-
ist) will be denoted by E X and VarX, respectively. Thus, the well-known 
Chebyshev's inequality, which will be frequently used throughout the book, 
can be stated in the following, standard form. If Var X exists, then 
P ( | X - E X | > 0 < ^ 3 ^ , 
« > 0 . 
(1.2) 
Similarly, Markov's inequality states that, if X > 0 a.s., then 
*{X > * ) < — . 
t>0. 
(1.3) 
We denote the covariance of two random variables X and Y by Cov(X, Y). 
Recall that the variance of a (finite) sum of random variables is given by 
Var(Zi Xi) = Σ ί Σ ; Cov(X«, Xj). 
The conditional expectation of X given an event £ is denoted by E(X | £). 
We similarly write E(X \ Yi,..., 
Y*) for the conditional expectation of X given 
some random variables Y\,...,Yk; 
note that this conditional expectation is 
a function of (Κι,...,ϊ*) and thus itself a random variable. When using 
martingales (Section 2.4), we will more generally denote by E(X \ Q) the 
conditional expectation of X given a sub-a-algebra Q of T. 
Quite frequently our proofs will rely on the elementary law of total probabil-
ity which states that for any partition of the probability space Ω = £i U £2 ... 
and any random variable X defined on Ω, 
EX = 
YtE(X\£i)?(£i). 
i 
In particular, if X = 1[£], then P(£) = £ t P(£ | £ ) P(£<). 
If X\, X2, ·. - are random variables and a is a constant, we say that Xn 
converges in probability to o as n -> 00, and write Xn A a, if V{\Xn — o| > 
ε) -> 0 for every ε > 0; see, for example, Gut (1995, Chapter VI). 
One similarly defines X n A Y, where Y is another random variable, but 
then Y and every Xn have to be defined on the same probability space; this 
can be reduced to the preceding case, since Xn A Y if and only if X„ - Y -► 0. 
Let X\,X2, 
■ ■. and Z be random variables. We say that Xn converges in 
distribution to Z as n -> 00, and write Xn A Z, if P(X„ < x) -* P(£ < x) for 
every real x that is a continuity point of P(Z < 1) (Billingsley 1968, Gut 1995). 

NOTES ON NOTATION AND MORE 
9 
If Xi, X2,... 
and Z are integer-valued then, equivalently, Xn A Z if and only 
if P(X„ = Jfc) ->· F(Z = k) for every integer k. 
Note that convergence in distribution is really a property of the distribu-
tions of the random variables and does not require the variables to be defined 
on the same probability space. Nevertheless, it is customary (and convenient) 
to talk about convergence of random variables. We also use hybrid notation 
such as Xn -» N(0,1), which means Xn A Z for some (and thus every) 
random variable Z € N(0,1). 
An important special case is one in which Z is a (non-random) real con-
stant. It is easily shown that convergence in distribution to a constant is the 
same as convergence in probability, that is, λ'η -* a if and only if Xn -* o. for 
a G R. A useful fact is that if Xn -> Z and Yn A a, where a is a constant, 
then Xn + Y„ -¥ Z+a and YnXn -> aZ (Cramer's theorem), see, for example, 
Gut (1995, Theorem VI.7.5). 
The definition of convergence in distribution extends to random vectors 
with values in R* for every fixed k; this is also expressed as joint convergence 
in distribution of the components of the vectors. A powerful method for 
extending results on the real random variables to the vector-valued ones is 
known as the Cramér-Wold device (Billingsley 1968, Theorem 7.7). It states 
that (Xnl,...,Xnk) 
A (Ζι,...,Ζ») if and only if ¿2i*iXm 
4 £i*«Z< for 
every sequence of real numbers t i , . . . , i*. For more details, as well as for the 
convergence of random variables with values in even more general spaces, see 
Billingsley (1968). 
Remark 1.1. Convergence in distribution does not, in general, imply con-
vergence of the sequence of means or variances. However, in many specific 
applications we find that these sequences do, in fact, converge to the mean 
and variance of the limit distribution. 
Asymptotics 
We will often use the following standard notation for the asymptotic behavior 
of the relative order of magnitude of two sequences of numbers a„ and b„, 
depending on a parameter n -> oo. The same notation is also used in other 
situations, for example, for functions of a variable ε that tends to 0. We will 
often omit the phrase "as n -> oo" when there is no risk of confusion. For 
simplicity we assume bn > 0 for all sufficiently large n. 
• an = 0(bn) 
as n -> oo if there exist constants C and no such that 
\an\ < Cbn for n > no, i.e., if the sequence an/bn 
is bounded, except 
possibly for some small values of n for which the ratio may be undefined. 
• on = Ω(6η) as n -* oo if there exist constants c > 0 and no such that 
o« > cb„ for n > no- If a„ > 0, this is equivalent to 6„ = 
0(an)-

10 
PRELIMINARIES 
• αη = θ(6„) as n ->· οο if there exist constants C, c > 0 and n0 such that 
cbn <an< 
Cbn for n > n0, i.e., if a„ = 0(bn) and αη = Ω(6„). This is 
sometimes expressed by saying that an and 6„ are of the same order of 
magnitude. 
• an χ ό η if an = θ(6„). 
• α„ ~ 6 n if α η/6 η -*· 1. 
• αη = ο(ί>„) as n -> oo if a n/6 n -+ 0, i.e., if for every ε > 0 there exists 
ne such that \an\ < ebn for n 
>ne. 
• an < 6n or 6„ » o„ if a„ > 0 and an = o(bn). 
Since most results in this book are asymptotic, we will be frequently as-
suming in the proofs that n is sufficiently large, sometimes without explicitly 
saying so. 
Probability asymptotics 
We say that an event £„, describing a property of a random structure depend-
ing on a parameter n, holds asymptotically almost surely (abbreviated a.a.s.), 
if P(£n) -¥ 1 as n -> oo. 
Remark 1.2. In many publications on random structures the phrase "almost 
surely" or a.s. is used. However, we wish to reserve that phrase for what it 
normally means in probability theory, i.e. that the probability of an event 
equals exactly 1. It seems that the first paper where the phrase a.a.s. and 
not a.s. was used is Shamir and Upfal (1981). (Some authors use the phrase 
"almost every" or a.e. which we reject for the same reason as "almost surely". 
Others write "with high probability", or whp.) 
When discussing asymptotics of random variables, we avoid expressions like 
"X„ = 0(1) o.o.s." or uXn = o(l) a.a.s.", which may be ambiguous, since 
they combine two asymptotic notions. As a substitute we give probabilistic 
versions of some of the symbols above, denoting them with a subscript p or C. 
Let X„ be random variables and an positive real numbers. We then define: 
• Xn = 0p{an) as n -¥ oo if for every δ > 0 there exist constants C¡ and 
no such that P(|X n| < C¡an) > 1 - δ for every n > no. 
• Xn = Oc(an) 
as n -> oo if there exists a constant C such that a.a.s. 
\Xn\ < Can. 
• Xn = θρ(αη) 
as n -> oo if for every δ > 0 there exist constants c¿ > O, 
Cs > 0 and no such that P(c¿an < X n < Csan) > 1 — 5 for every n > no-
• Xn = ©c(on) as n -* oo if there exist positive constants c and C such 
that a.a.s. can < Xn < Can. 

NOTES ON NOTATION AND MORE 
11 
• X„ - op(an) as n -+ oo if for every e > 0, a.a.s. \Xn\ < εαη. 
Note that Xn = Oc{an) implies Xn = Op(an), but not conversely; indeed, 
Xn = θ£'(α„) if and only if the constant Cs in the definition of Op can 
be chosen independently of δ. For example, any sequence Xn of identically 
distributed random variables is Op(l), but such a sequence is Oc(l) only if 
the common distribution has support in a finite interval. 
Similarly, .Yn = Θο{αη) implies Xn = Θρ(α„), but not conversely. On the 
other hand, Xn = op(a„) implies X„ = Oc(a n)· 
Remark 1.3. It is easy to verify (Exercise!) that Xn = Op(an) if and only if 
for every function ω(η) -» oo, \Xn\ < ω(η)αη a.a.s. Similarly, Xn = op(an) if 
and only if for some function ω(η) -* oo, \Xn\ < αη/ω(η) 
a.a.s. 
Such notation with an unspecified sequence ω(η) is common in publications 
on random structures, but we believe that the equivalent notation Op and op 
is clearer. 
It is an immediate consequence of the definitions (Exercise!) that Xn — 
op{an) if and only if Xn/an 
A 0. Conversely, Xn A a if and only if X„ = 
a + Op(l) (and Xn A Y if and only if Xn = Y + op(l)). 
Remark 1.4. The symbol Op can also be expressed by equivalent standard 
probabilistic concepts. In fact, a sequence Xn is bounded in probability, or 
tight, if Xn = Op(l). Hence, Xn = Op(a„) if and only if the sequence 
Xn/an 
is bounded in probability (or tight). 
Dependency graphs 
Let {X¡}i€2 be a family of random variables (defined on a common probability 
space). A dependency graph for {Xi} is any graph L with vertex set V(L) = I 
such that if A and B are two disjoint subsets of X with ei{A,B) 
= 0, then 
the families {Xi}ieA 
and {X¿}¿eB are mutually independent. 
Dependency graphs will be used several times in this book. They are partic-
ularly useful when they are sparse, meaning that there is a lot of independence 
in the family 
{X,}. 
Example 1.5. In a standard situation, there is an underlying family of inde-
pendent random variables {VOJae.4. and each Xi is a function of the variables 
{Ya}a€A, for some subset Ai C A. Let S = {Ai : i e 1}. Then the graph 
L = L(S) with vertex set I and edge set {ij : Ai Π Aj φ 0} is a dependency 
graph for the family {Xi}iex 
(Exercise!). 
Example 1.6. As a special case of the preceding example, let {i/Jjgi be 
given subgraphs of the complete graph Kn and let X¡ be the indicator that 
Hi appears as a subgraph in G(n,p), that is, Xi = l[i/¿ C G(n,p)], i 6 I. 
Then L(S), with S = {E(Hi) : i € X), is a natural dependency graph with 
edge set {ij : E(Hi) Π E{Hj) φ 0} (Exercise!). 

12 
PRELIMINARIES 
Remark 1.7. In particular, if L is a dependency graph for {X¿}, then two 
variables X, and X} are independent unless there is an edge in L between i 
and j . Note, however, that this is only a necessary condition, and does not 
imply that L is a dependency graph (Exercise!). 
Remark 1.8. Another context, outside the scope of this book, in which de-
pendency graphs are used is the Lovász Local Lemma (Erdös and Lovasz 
(1975); see also Alon and Spencer (1992)). There it actually suffices to use 
a slightly weaker definition, considering only singletons B in the definition 
above. 
Remark 1.9. In our applications, there exists a natural dependency graph, 
but it should be observed that, in general, there is no canonical choice and 
the dependency graph is not unique, even if it is required to be minimal 
(Exercise!). 
The subsubsequence principle 
It is often convenient to use the well-known subsubsequence principle, which 
states that if for every subsequence of a sequence there is a subsubsequence 
converging to a limit a, then the entire sequence must converge to the same 
limit. This holds for sequences of real numbers, vectors, random variables 
(both for convergence in probability and for convergence in distribution) and, 
in general, for sequences in any topological space. 
For example, this means that if we want to prove a limit theorem for 
G(n,p), we may without loss of generality assume that an expression such as 
napb converges to some c < oo (provided, of course, that the result we want 
to prove does not depend on the limit c). 
We will be using this principle throughout the book (see, e.g., the proof of 
Proposition 1.15), sometimes without explicitly mentioning it. 
And finally . . . 
The base of all logarithms is e, unless specified otherwise. 
1.3 
MONOTONICITY 
A family of subsets Q C 2 r is called increasing if A C B and A 6 Q imply that 
B € Q. A family of subsets is decreasing if its complement in 2 r is increasing, 
or, equivalently, if the family of the complements in Γ is increasing. A family 
which is either increasing or decreasing is called monotone. 
A family Q is 
convex if A C B C C and A, C € Q imply B € Q. We identify properties of 
subsets of Γ with the corresponding families of all subsets having the property; 
we thus use the same notation and terminology for properties. 

MONOTONICITY 
13 
In the special case in which Γ = [n\2, any family Q C 2 r is a family 
of graphs and, if it is closed under isomorphism, it can be identified with 
a graph property. Some examples of increasing graph properties are "being 
connected", "containing a triangle" and "having a perfect matching". Au-
tomatically, the negations of all of them are decreasing. Natural decreasing 
graph properties include "having at least k isolated vertices", "having at most 
Jfc edges" and "being planar". The property of "having exactly k isolated ver-
tices" is an example of a convex but not monotone property, whereas "the 
largest component is a tree" is not even convex (Exercise!). 
It is reasonable to expect that the probability of a random set falling into 
an increasing family of sets gets larger when the (expected) size of the random 
set does. This is indeed the case. Lemma 1.10 below appeared first in Bollobás 
(1979). 
Lemma 1.10. Let Q be an increasing property of subsets ofT, 0 < p\ 
<p?< 
1 andO<Mi 
<M2< 
N. 
Then 
Ρ(ΓΡι G Q) < P ( r w G Q) 
and 
Ρ(ΓΜι 6 Q) < Ρ(ΓΜ2 6 Q). 
Proof. To prove the first inequality we employ a simple version of the two-
round exposure technique (see Section 1.1). Let po = (j>2— Pi)/(1— Pi)· Then 
ΓΡ2 can be viewed as a union of two independent random subsets, ΓΡ1 and Γ^. 
As then Γρ, C Γρ, and Q is increasing, the event 'T P l G Q" implies the event 
'Tpj G Q", completing the proof of first inequality. 
For the second inequality, we construct a random subset process 
{TM}M, 
similar to the random graph process defined in Section 1.1, by selecting the 
elements of Γ one by one in random order. Clearly, ΓΜ can be taken as the M-
th subset in the process. Then ΓΜ, Q ΓΜ 2 . and, as in the first part, the event 
T W l G Q" implies the event "ΓΜ2 G Q", which completes the proof. 
■ 
Trivially, each monotone property is convex. In a special case this can 
be, in a sense, reversed: if Q is convex, and for some M we have [Γ]Μ C Q 
then, for M' < M, Q behaves like an increasing property, and in particular 
P(IV 6 Q) < Ρ(ΓΜ« G Q) for all M' < M" < M (Exercise!). Similarly, 
for M" > M, Q can be treated as decreasing. A probabilistic version of this 
simple observation is stated in the next lemma. 
Lemma 1.11. Let Q be a convex property of subsets ο/Γ, and let 
Μχ,Μ,Μι 
be three integer functions of N satisfying 0 < Mi < M < M<i < N'. Then 
Ρ(Γ Μ G Q) > Ρ(Γ Μ | G Q) + Ρ(Γ Μ ι 6 Q) - 1. 
Hence, iff{TM, 
G Q) -► 1 as N -> oo, then Ρ(ΓΜ, € Q) < Ρ(Γ Μ G Q)+o(l). 
In particular, ι/Ρ(Γ Μ ; 6 Q) -► 1 05 N -> oo, i = 1,2, then Ρ(Γ Μ G Q) -> 1. 

14 
PRELIMINARIES 
Proof. The following simple proof was observed by Johan Jonasson (personal 
communication). It is easily seen that a convex property Q is the intersection 
of an increasing property Q' and a decreasing property Q". (Exercise! - Note 
that the converse is obvious.) Thus 
Ρ ( Γ Μ 6 Q) > Ρ ( Γ Μ € Q') + Ρ(Γ Μ 6 Q") - 1 
> Ρ(ΓΜι e Q·) + Ρ(ΓΜ2 e Q") - l 
> Ρ(ΓΜι € Q) + Ρ(ΓΜ, e Q) - 1. 
■ 
1.4 
ASYMPTOTIC EQUIVALENCE 
In this section we examine the asymptotic equivalence of the two models Γρ 
and ΓΜ; recall that this includes the random graphs G(n,p) and G(n, M) as 
a special case. Our goal is to establish conditions under which convergence of 
Ρ(ΓΡ € Q) implies convergence of Ρ(Γ\ί € Q) to the same limit and vice versa. 
One expects such equivalence when M is near Np. Since Γρ is a mixture of 
ΓΛ/'S for different M, the above implication is more straightforward in the 
direction from the uniform to the binomial model and then does not require 
any restriction on Q. The only tools we use are the elementary law of total 
probability and Chebyshev's inequality. Most results in this section are based 
on Luczak (1990a); in the case in which the limit is one they already appeared 
in Bollobás (1979, 1985). 
Let Γ(η) be a sequence of sets of size N(n) = |Γ(η)| -> oo. (In the example 
of main concern to us, viz. random graphs, Γ(η) = [n]2 and thus N(n) = 
(2) ·) ^ e further consider a property Q of subsets of these sets; formally the 
property corresponds to a sequence Q(n) C 2r^n^ of families of subsets of 
Γ(η), n = 1,2, 
Finally, p(n) is a given sequence of real numbers with 
0 < p(n) < 1, and M(n) is a sequence of integers with 0 < M(n) < N(n). 
We usually omit the argument n and write Γ, N, Q, p and M; moreover, we 
let q = 1 - p. 
Proposition 1.12. Let Q be an arbitrary property of subsets ofT = Γ(η) as 
above, p — p(n) 6 [0,1] and 0 < a < 1. If for every sequence M = M(n) such 
that M = Np + 0(y/Npq) 
it holds that ¥{TM € Q) -* a as n -> 00, then also 
Ρ(ΓΡ € Q) -y a as n -* 00. 
Proof. Let C be a large constant and define (for each n) 
M(C) = {M : \M - Np\ < 
Cy/Ñpq}. 

ASYMPTOTIC EQUIVALENCE 
15 
Furthermore, let M. be the element M of M(C) that minimizes Ρ(ΓΛ/ e Q). 
By the law of total probability, 
Ρ(ΓΡ 6 Q) = £ 
Ρ(ΓΡ € Q I |ΓΡ| = M) Ρ(|ΓΡ| = M) 
Λ/=0 
N 
= £ P ( r M € Q ) P ( | r p | = M) 
M=O 
> 
Σ 
Ρ(ΓΜ. € Q ) P ( | r p | = Af) 
M€A4(C) 
= Ρ(ΓΜ. 6 β)Ρ(|Γ ρ| 6-M(C)). 
By assumption, Ρ(ΓΜ. 6 Q) -> n, and using Chebyshev's inequality (1.2), we 
also have Ρ(|ΓΡ| £ M(C)) 
< Var|r p|/(CvWpg) 2 = 1/C2. Consequently, 
liminf Ρ(ΓΡ 6 Q) > aliminf Ρ(|ΓΡ| € M(C)) > a(l - 
C~2). 
n—►oo 
n—foo 
Similarly, if M* maximizes Ρ(ΓΆί 6 Q) among M É M(C), 
Ρ(ΓΡ 6 Q) < Ρ(ΓΜ· € Q) + Ρ(|ΓΡ| £ .M(C)) < Ρ(ΓΜ· € Q) + C~2, 
and 
limsupP(r p eQ)<a 
+ C~2. 
n—»oo 
The result follows by letting C -> oo. 
■ 
In the other direction no asymptotic equivalence can be true in such gener-
ality. The property of containing exactly M edges serves as a simplest coun-
terexample (Exercise!). However, the additional assumption of monotonicity 
of Q suffices. 
Proposition 1.13. Let Q be a monotone property of subsets ofT = Γ(η) as 
above, 0 < M < N and 0 <¿ a < 1. If for every sequence p = p(n) € [0,1] 
such that p = M/N + 0(y/M(N 
- M)/N3) 
it holds that Ρ(Γρ € Q) -► a, 
then Ρ{ΓΜ € Q) -> a as n -► oo. 
Proo/. We consider only the case in which Q is increasing (the decreasing case 
is similar). Let C be a large constant, po = M/N, 
qo — 1 - po, and define 
p+ = min(po + Cy/poqo/N, 1) and p_ = max(po - Cy/poqo/N,0). 
Arguing 
as in the proof of Proposition 1.12 and using Lemma 1.10, we have 
Ρ(Γ Ρ +€<2)> 
Σ 
P ( r M ' € Q ) P ( | r p + | = M') 
M'>M 
> P ( r M € Q ) P ( | r p + | > M ) 
> Ρ ( Γ Μ € 2 ) - Ρ ( | Γ Ρ + | < Μ ) 
(1.4) 

16 
PRELIMINARIES 
and similarly 
Ρ(ΓΡ_ € Q) < Ρ(Γ Μ 6 Q) + Ρ(|ΓΡ. | > M). 
(1.5) 
The cases M = 0 and M = N are trivial (Exercise!), so we may further 
assume 1 < M < N - 1, and thus Npoqo = M(N - M)/N 
> 1/2. Since |ΓΡ_ | 
has the binomial distribution with mean JVp_ and variance 
Np_(l - p_) < M(l - po + Cy/poQo/N) < Npoqo + 
Cy/Ñpoqo, 
Chebyshev's inequality (1.2) yields, with S{C) = C~2 + \/2C - 1, 
Ρ(|Γρ.| > M) < r y - ( 1 -p-}
2 < »M + Cyt*™* < Ó(C), 
(Npo-Np-)2 
- 
C2Npoqo 
and similarly Ρ(|ΓΡ+| < M) < <5(C). Since 
lim Ρ(ΓΡ+ € Q) = lim Ρ(ΓΡ_ e Q) = a 
n-»oo 
n-foo 
by assumption, the inequalities (1.4) and (1.5) yield 
a - 6{C) < liminf Ρ(ΓΜ G Q) < limsupP(r M 6 Q) < a + 6(C), 
n-*oo 
n-*oo 
and the result follows by letting C -» oo, which implies S(C) -> 0. 
■ 
Remark 1.14. In the above proof one can relax the monotonicity of Q and 
instead require only that in the range M' = M + 0(\/M(N 
- 
M)/N) 
Ρ(ΓΜ< € Q) < Ρ(ΓΜ € Q) + o(l) 
for M' < M, and 
Ρ ( Γ Μ · ε ς ) > Ρ ( Γ Μ € δ ) + ο(1) 
for M' > M. By Lemma 1.11, these conditions are satisfied whenever Q is 
convex and for some M' with M' - M ^> \/M{N 
- M)/N, 
it holds that 
limn-*«, Ρ(ΓΜ- € Q) = 1 (Exercise!). 
The next result simplifies Proposition 1.13 for a = 1 by showing that for 
convex properties Q, we have o.o.s. I V € Q provided a.a.s. TM/N € Q. 
Proposition 1.15. Let Q be a convex property of subsets of Γ and let 0 < 
M <N. 
If Ψ(ΓΜ/Ν € Q) -* 1 os n -> oo, i/»en Ρ(ΓΜ e Q ) - » l . 
Proof. We assume for simplicity that M(N — M)/N 
-► oo, leaving the cases 
in which M or N - M is bounded to the reader (Exercise!). (Note that the 
subsubsequence principle implies that it suffices to consider these three cases 
- Exercise!) 

ASYMPTOTIC EQUIVALENCE 
17 
Let Mi and M2 maximize Ρ(ΓΜ- 6 Q) among M' < M and M' > M, 
respectively. Arguing as in the proof of Proposition 1.12, we then have 
Ρ(Γ Μ/* € Q) < Ρ(ΓΜι e Q) P(|r M / i V| < M) + Ρ(|Γ Μ/ΛΓ| > A/) 
and thus, since W{\TM/N\ < M) -> 1/2 by the central limit theorem, 
1 = lim Ρ(ΓΜ/Λ/ € Ö) < 5 liminf Ρ(ΓΜι € Q) + | , 
n—»oo 
' 
* n-»oo 
which implies that ΙΠΉ,,-ΚΧ, Ρ(ΓΜι 6 Q) = 1. Similarly, lim,,-*,» P(r W j € 
Q) = 1. Since M, < M < M2, Lemma 1.11 yields Ρ(Γ Μ e Q) -> 1. 
■ 
If the convergence of Ρ(ΓΡ € Q) to 1 is reasonably fast, then the passage 
from the binomial to the uniform model can be made without any restriction 
on Q. Indeed, by the law of total probability, for any M, 
p(rP * o = Σ p(r* ^ ö)(k jp^
1 -p)
N~* 
* = 0 
^ 
' 
>P(rM?Q)(^i)pMV-P)N-M> 
from which it easily follows (Exercise!) that, taking p = 
M/N, 
Ρ ( Γ Μ ^ 2 ) < 3 ν / Μ Ρ ( Γ Μ / Λ ί $ ? 0 ) . 
(1.6) 
This inequality (Bollobás 1985, p. 35) is a slight sharpening of a result by 
Pittel (1982), and is therefore known as Pittel's inequality. 
The following simple corollary of Propositions 1.15 and 1.12 (Exercise!) is 
stated here for future reference. 
Corollary 1.16. Let Q be an increasing property of subsets of Γ, and let 
M = M(n) -► oo. Assume further that δ > 0 is fixed and 0 < {1±δ)Μ/Ν 
< 1. 
(i) // Ρ(Γ Μ / / ν 6 Q) -> 1, then Ρ(ΓΜ € Q) -► 1. 
(ii) // P(rw/A, € Q ) - > 0 , then Ρ(ΓΜ e Q) -> 0. 
(iii) // Ρ(Γ Μ € Q) - H , ίΛβη Ρ(Γ ( 1 + ί ) Μ / Ν e Q) -♦ 1. 
(iv) // Ρ(Γ Μ € Q) -♦ 0, ihen Ρ(Γ(1_4)Μ/ΑΓ e Q) -> 0. 
■ 
Remark 1.17. The results of this section indicate that in a vast majority of 
cases the properties of random graphs G(n,p) and G(n, M), where M ~ (")p, 
are very similar to each other. Even if the equivalence statements do not 
apply directly, typically repeating a proof step by step leads to an analogous 
result for the other model. Thus, in this book we very often state and prove 
theorems only in one of the two basic models. However, one should bear in 
mind that there are exceptions to this "equivalence rule of thumb" (compare, 
e.g., Theorem 3.9 with Theorem 3.11, or Theorem 6.52 with Theorem 6.58). 

18 
PRELIMINARIES 
1.5 
THRESHOLDS 
The most intriguing discovery made by Erdös and Rényi in the course of 
investigating random graphs is the phenomenon of thresholds. For many 
graph properties the limiting probability that a random graph possesses them 
jumps from 0 to 1 (or vice versa) very rapidly, that is, with a rather small 
increase in the (expected) number of edges. This behavior is not just a feature 
of random graphs; as shown by Bollobas and Thomason (1987), it holds for 
monotone properties of arbitrary random subsets (Theorem 1.24 below). 
We consider in this section, as in the previous one, a property Q of random 
subsets of a sequence Γ(π) of sets, with N(n) 
= |Γ(η)|. Throughout we 
assume that we exclusively deal with properties that are neither always true 
nor always false. 
For an increasing property Q, a sequence p = p(n) is called a threshold if 
Í0 
if 
\ l 
if 
p ( r p € Q ) - > < ; 
;; 
p * * 
( ΐ . η 
p» 
p. 
Thresholds M = M(n) for the uniform model are defined analogously by 
Í0 
if 
\ l 
if 
^ β β , . Γ :; Z « : 
a.) 
There isjreally jno need to insist that M is an integer, but we can jdways 
replace M by fM]. In order to avoid trivial complications we assume M > 1, 
or at least inf M(n) > 0. 
Thresholds for decreasing families are defined as the thresholds of their 
complements. 
Throughout the book we will often refer to the first line of (1.7) or (1.8) 
as the 0-statement and to the second line as the 1-statement of the respective 
threshold result. 
Remark 1.18. Corollary^ 1.16 implies that p is a threshold for a monotone 
property if and only if M = p\T\ is (Exercise!). Hence it does not matter 
which of the two basic models for random subsets we use. 
Remark 1.19. Strictly speaking, a threshold is not uniquely determined 
since if p is a threshold and p ~ p, then p* is a threshold too (and simi-
larly for M). Nevertheless, it is customary to talk about the threshold; this is 
convenient but it should be remembered that the threshold really is defined 
only within constant factors. 
Example 1.20. If Γ = [n] then Γρ and TM are random subsets of integers. 
Let Q be the property of containing a 3-term arithmetic progression. We will 
show in Example 3.2 that p = n~2/3 is the threshold for Q in Γρ, and so 
M = n 1/ 3 is the threshold for Q in Γ Μ · 

THRESHOLDS 
19 
Example 1.21. In the case in which Γ = [n]2 we deal with random graphs. 
We will soon learn (Theorem 3.4) that the threshold for containing a triangle 
is p = 1/n in G(n,p), and thus M = n in G(n, M). 
Remark 1.22. Suppose that we construct a random subset sequentially by 
adding random elements one by one; in other words, we consider the random 
subset process 
{VM)o 
as in the proof of Lemma 1.10. 
Define a random 
variable M as the number of elements ¿elected when the random set first 
satisfies aj*iven increasing property Q; M is often called the hitting time of 
Q. Then M < M if and only if Γ Μ € Q, and thus 
P(M <M) = Ρ(Γ Μ 6 Q). 
(1.9) 
Hence M is a threshold if and only if M = ΘΡ(Μ) (Exercise!). 
In order to investigate thresholds further, we introduce some more notation. 
For a given increasing property Q and 0 < a < 1, we define p(a) as the number 
in (0,1) for which 
Ρ(Γρ(«) € Q) = a. 
(The existence and uniqueness of this number follow because p ·-> Ρ(ΓΡ G Q) 
is a continuous, strictly increasing function; cf. Lemma 1.10. - Exercise!) We 
similarly define 
M(a) = min{M : Ρ(Γ Μ e Q) > a} 
(in this case, of course, we should not expect to have Ρ(ΓΜ(ο) € Q) = a); it 
follows that 
r(rM{a)-i 
eQ)<a< 
Ρ(Γ Μ ( α ) 6 Q). 
(1.10) 
Since Q and Γ depend on a parameter n, we also write p(a;n) and 
M(a;n). 
Proposition 1.23. Suppose that Q is an increasing property of subsets of 
Γ = Γ(η). Then p(n) is a threshold if and only ifp(a; n) x p(n) as n -¥ oo, for 
every a € (0,1). Similarly, M(n) is a threshold if and only if M{a\ n) x M(n) 
for every a € (0,1). 
Proof. Suppose first that M is a threshold. If 0 < a < 1 but M(a) yí M, then 
there existía subsequence s = (ni, n 2 , . . . ) , along which either M(a)/M 
-> 0 
or M{a)/M 
-> oo. In the first case, by (1.8), Ρ(Γ Μ ( ο ) € Q) -► 0 along 
s, which contradicts (1.10). In the second case, along s, M(a) - 1 > M 
and thus (1.8) yields Ρ(Γ Μ( α).! € Q) -> 1, which again contradicts (1.10). 
Consequently, M(a) x M holds for every a e (0,1). 
Conversely, suppose that M is nota threshold. Then there exists a sequence 
M =^M{n) such that either M/M 
-► 0 and liminfP(r M € Q) > 0, or 
M/M -» oo and limsupP(rM € Q) < 1. 

20 
PRELIMINARIES 
In the first case, there exist a > 0 and a subsequence along which Ρ(ΓΜ 6 
Q) > a, and thus M(a) < M 4C M; in the second case, similarly there exist 
a < 1 and a subsequence along which M(a) > M 3> M. 
In both cases 
M{a) jt M. 
The proof for p is almost identical so we omit it here. 
■ 
Theorem 1.24. Every monotone property has a threshold. 
Proof. Without loss of generality assume that Q is increasing. Let 0 < ε < 1, 
and let m be an integer such that (l-ε)" 1 < ε. Consider m independent copies 
Γ ( ι \ ... ,r<m> of Γρ(£). Their union is I > , with p' = 1 - (1 -p(e))m 
< τηρ{ε), 
and hence by Lemma 1.10 
Ρ(Γ(1) U · · ■ U r<m) 6 Q) < P(r m p ( f, € Q). 
On the other hand, since Q is increasing, if any Γ(ί) € Q, then Γ ( ι )υ· · ur<m) É 
Q, and thus 
Ρ(Γ(1) u ■ ■ ■ U T ( m ) tQ)< 
Ρ(Γ(<) i Q for every i) = (l - Ρ(Γρ(ί) € Q)) m 
= ( l - e ) m < ε . 
Consequently, 
P(r m p ( i) G Q) > P ^ 1 * U ■ · ■ U r ( m ) 6 Q) > 1 - e 
and thus p(l - e) < mp{e). Hence, if 0 < ε < 1/2, 
P ( < 0 < p ( l / 2 ) < p ( l - e ) < m p ( e ) , 
with m depending on e but not on the parameter n; this implies that p(e) x 
p(l/2) >: p(l - e), and Proposition 1.23 shows that, for example, p(l/2) is a 
threshold. 
^ 
The existence of a threshold M for G(n, M) can be proved similarly; it 
follows also by Remark 1.18. 
■ 
For non-monotone properties one adopts a "local" version of the definition 
of a threshold, with (1.7) being satisfied only in the vicinity of p. Observe 
that a property may have no threshold at all or it may have countably many 
thresholds (see Spencer (1991) for more on this). Convex properties have at 
most two thresholds, one of the 0-1 form and one in reverse. 
1.6 
SHARP THRESHOLDS 
We end this chapter with a discussion of some recent general results on the 
widths of thresholds. We continue with the assumptions of the preceding 
section, and let <5(ε) = ρ(1 - ε ) -p(e), 0 < ε < 1/2. We should think here of ε 

SHARP THRESHOLDS 
21 
as fixed and very small; then δ{ε) is a measure of the width of the threshold. 
We may similarly define δ\ι(ε) - Μ(1—ε) — Μ(ε) for the uniform model; note 
that by (1.9), ¿M also measures the concentration of the random variable M 
(Exercise!). 
Theorem 1.24 shows that for every fixed ε, δ(ε) = 0(p). 
More precisely, 
the proof of Theorem 1.24 implies that, for any increasing property Q and 
0 < ε < 1/2, 
1 < ρ ( 1 - ε ) / ρ ( ε ) < Γε-1 löge'1"!, 
and hence ρ(ε)/ρ(1/2) is bounded from above and below by universal con-
stants for every fixed e € (0,1). (For the uniform model T\f we similarly have 
1 < M(l - ε)/Μ(ε) < \ε~ι loge- ll·) 
However, certain monotone properties enjoy sharper thresholds than those 
guaranteed by Theorem 1.24. Sometimes 
Ρ(Γ €Q)-J° 
Íf 
P ^
1 - ^ 
( p € J ) 
ji 
if 
Ρ>(Ι 
+ 
Φ 
for every η > 0; in this case p is called a sharp threshold. Note that while 
thresholds in general are defined up to the asymptotic relation x (see Re-
mark 1.19), we have defined sharp thresholds up to ~. The existence of a 
sharp threshold is equivalent to ρ(ε;η)/ρ(1/2;η) —► 1 as n —► oo for every ε 
with 0 < ε < 1, and further to <$(ε) = o(p) for every fixed ε (Exercise!). 
In contrast, if there exists ε > 0 such that δ(ε) = θ(ρ), then the threshold 
is called coarse. 
Similarly, we define sharp and coarse thresholds for ΓΛ* ; it is easily seen 
by Corollary 1.16 that if p and M = p\T\ are corresponding thresholds for Γρ 
and ΓΆί, and moreover M —► oo (to rule out some trivial counterexamples), 
then p is a sharp threshold if and only if M is. Moreover, using the notation of 
Remark 1.22, M is a sharp threshold if and only if M/M(1/2) 
A 1 (Exercise!). 
Let us now restrict attention to random graphs G(n,p) and graph proper-
ties. 
Example 1.25. A classic example of a sharp threshold is the threshold p = 
logn/n for disappearence of isolated vertices; in this case δ(ε) = θ(1/η), see 
Corollary 3.31. This coincides with the thresholds for connectivity and for 
the existence of a perfect matching (for n even); see Chapter 4. 
Example 1.26. The property of containing a given graph as a subgraph, 
studied in detail in Chapter 3, has a coarse threshold; see, e.g., Theorem 3.9. 
Remark 1.27. There are quite natural properties with (coarse) thresholds 
which are sharp on one side but not on the other; for example "G(n,p) contains 
a cycle". Another example can be seen in Theorem 8.1. We abstain from 
giving a formal definition of such "semi-sharp" thresholds. 

22 
PRELIMINARIES 
Friedgut and Kalai (1996) showed that ¿(e) = 0(1/logn) for every mono-
tone graph property; this was improved by Bourgain and Kalai (1997) to 
0(1/log 2 -* n) for every <5 > 0, and it is conjectured that the absolute width 
is actually 0 ( 1 / log2 n), which is achieved by simple examples. (In view of 
δ(ε) = 0(p), this result is of interest mainly for thresholds that are constant 
or tend to 0 very slowly.) 
Friedgut and Kalai (1996) gave also the following version, which implies 
the 0 ( 1 / logn) estimate for arbitrary p and improves it for p -> 0. 
Theorem 1.28. For every ε with 0 < e < 1, there exists a constant Ce such 
that for every monotone graph property 
p ( l - £ ; n ) < p ( £ ; n ) + 0 / ( £ ; n ) y 2 / p ( £ ; n ) ) . 
(1.11) 
logn 
In particular, it follows that a threshold p such that logl/p = o(logn) 
is always sharp. However, if p decreases as some power of n, as for most 
properties treated in this book, Theorem 1.28 yields only an 0{j>) estimate, 
just as the simpler and more general Theorem 1.24. (The assertion in Friedgut 
and Kalai (1996) that Ce in (1.11) can be taken as Clog 1/e for some universal 
constant C is not correct; there are counterexamples with rapidly decreasing 
ε = ε(η) and ρ{ε;η).) 
A recent result by Friedgut (1999) shows that Examples 1.25 and 1.26 are 
typical, in the sense that, roughly speaking, graph properties that depend on 
containing a large subgraph have sharp thresholds. (This is not literally true, 
as is seen by the example "a random graph contains a triangle and has at least 
log n edges"; this property is essentially the same as "contains a triangle", and 
has the same coarse threshold, since the probability of obtaining a triangle in 
a random graph with fewer than logn edges is very small.) More precisely, 
Friedgut's result says that a monotone graph property with a coarse threshold 
may be approximated by the property of containing at least one of a certain 
(finite) family of small graphs as a subgraph. A precise formulation (slightly 
different from Friedgut's) is as follows. 
Theorem 1.29. Suppose that ε,η > 0 and c > 1. Then there exists k = 
fc(e,n,c) such that for every monotone graph property Q and every n for 
which p(l - ε; η)Ιρ(ε; η) > c, there exists some p with ρ(ε; n) <p< p(l - ε; η) 
and a family G%,..., Gm of graphs with at most k vertices, such that if Q' is 
the property "contains a subgraph isomorphic to some Gj", then 
P(G(n,p) G Q^Q') 
< n. 
■ 
R e m a r k 1.30. Note that the theorem is stated for a fixed n rather than as 
an asymptotic result; this is because, in general, the approximating property 
Q' may depend on n, unless we restrict attention to a subsequence. Indeed, 

SHARP THRESHOLDS 
23 
nothing prevents us from defining graph properties that depend on, say, the 
parity of n in some trivial explicit way. 
Moreover, the theorem claims only that Q' is a good approximation for 
some p 6 [ρ(ε; n),p(l —e; n)]; it is easy to construct (artificial) examples where 
different approximations are required for different p, and good approximations 
are absent for some choices of p. 
For a "natural" property, these complications are not to be expected, and 
it is reasonable to hope that the same Q' works for all n and p. 
A related result (indeed, a corollary of Theorem 1.29) by Friedgut (1999) 
shows that a coarse threshold for a monotone graph property may only be of 
the type n~a for some rational a, except that again it may be necessary to 
consider subsequences. 
Theorem 1.31. Suppose that a monotone graph property has a coarse thresh-
old p(n). 
Then there exists a partition of N = {1,2,...} into a finite num-
ber of sequences Nx,..., 
Nm and rational numbers a\,..., 
a m > 0 such that 
p{n) x n~a' for n € N,·. 
■ 
Theorems 1.29 and 1.31 and related results (Friedgut 1999, Bourgain 1999) 
can be used to show that certain properties have sharp thresholds, by showing 
that otherwise the conclusion of these results would yield a contradiction; see, 
for example, Achlioptas and Friedgut (1999) (the property of having at least 
a given chromatic number) and Friedgut and Krivelevich (2000) (Ramsey 
properties). 
We emphasize that while Theorem 1.24 holds for arbitrary monotone prop-
erties of general random subsets, the more refined results discussed here re-
quire some symmetry assumptions; we have for simplicity stated them for 
random graphs and graph properties, that is, for properties that are invari-
ant under permutations of the vertices. We consider the random graphs as 
random sets of edges, so the graph properties are properties of subsets of [n]2 
that are invariant under the permutations induced by the permutations of [n]. 
The same or similar results have been shown for other cases of random sub-
sets with certain symmetry assumptions, including random hypergraphs; see 
Friedgut and Kalai (1996), Bourgain and Kalai (1997), Friedgut (1999). The 
results depend on the type of symmetry assumed (Bourgain and Kalai 1997). 
Related results without symmetry assumptions are given by Talagrand (1994) 
and Bourgain (1999). 

2_ 
Exponentially Small 
Probabilities 
A common feature in many probabilistic arguments is the need to show that 
a random variable with large probability is not too far from its mean. One 
simple, but very useful, result of this type is Chebyshev's inequality (1.2), 
which holds for any random variable with finite variance. In this chapter we 
give several stronger inequalities valid under more restrictive assumptions, 
which for suitable random variables X and (positive) real numbers t yield 
estimates of the probability P(X > E X + t) that decrease exponentially as 
t -»· oo. 
In most cases we use the method, going back at least to Bernstein (1924), 
of applying Markov's inequality (1.3) to E e " x . Thus, for every u > 0, 
P(X > EX + t) = P(e u X > eu{EX+t)) 
< e _ , l ( E X + t ) E e u X , 
(2.1) 
and similarly, for every u < 0, 
P ( X < E ; r - t ) < e - u ( E * - , ) E e , , j r . 
(2.2) 
Then the moment generating function (or Laplace transform) E euX is esti-
mated in some way, and an optimal or near-optimal u is chosen. 
An estimate of P(|X - E X | > t) may obviously be obtained by adding 
estimates of P(X > E X + i) and P(X < E X - t). We will often give only 
one-sided estimates below, leaving the corresponding two-sided estimates to 
the reader. 
An important special case is estimating P(X = 0), which can be done by 
taking ί = E X in (2.2) (assuming E X > 0). We give several such results 
explicitly. 
25 
Random Graphs 
by Svante Janson, Tomasz Luczak and Andrzej Rucinski 
Copyright © 2000 John Wiley & Sons, Inc. 

26 
EXPONENTIALLY SMALL PROBABILITIES 
In the first section we consider random variables X that can be written as 
sums of independent terms. In the following sections we give extensions in 
various directions, covering cases with dependent summands. 
2.1 
INDEPENDENT SUMMANDS 
An important case is that in which the random variable X can be expressed 
as a sum £ " Xi of independent random variables. Then (2.1) can be written 
n 
?{X >KX 
+ t) < e _ u ( E X + t ) E e u X = e _ u ( E J f + t ) J j E e u X · , 
(2.3) 
¿=i 
and it remains to estimate the individual factors KeuXi. 
Here we will be 
mainly interested in the case in which each Xi is a random indicator variable; 
thus Xi € Be(p¿) where p¿ = P(X¿ = 1) = EX¡. Let λ = E X = 
Σ"p¿. 
The binomial case 
Consider first the case of a binomially distributed random variable X € 
Bi(n,p); this is of the type above with all p¿ = p. (Thus λ = np.) 
Then 
(2.3) yields 
P(X >A + t) < e - " ( x + t ) ( l - p + pe u) n, 
u > 0. 
The right-hand side attains its minimum at eu = (λ + t)(l - p)/(n - λ - t)p, 
assuming λ + t < n. This yields 
(
λ \x+t 
/ 
n — X 
\n-x-t 
ΛΤ7) 
( Ϊ Γ Α Γ Ϊ ) 
' 
° £ ' S » - * 
^ 
for t > n — X the probability is 0. This bound is implicit in Chernoff (1952) 
and is often called the Chernoff bound. (It appears explicitly in Okamoto 
(1958).) 
For applications, it is usually convenient to replace the right-hand side of 
(2.4) by a larger but simpler bound. Two such consequences of (2.4) are 
presented in the next theorem, together with their lower tail counterparts. 
Any of these bounds or their numerous consequences contained in Corollaries 
2.2 - 2.4 will be referred to as to Chernoff's inequality. 
Theorem 2.1. If X € Bi(n,p) and X = np, then, with ψ(χ) = (l + i)log(l + 
i) - x, x > - 1 , (and φ(χ) = oo for x < —1) 
nX>EX+t)<e^(-X^)<e^(-^^), 
t > 0; 
(2.5) 
Ρ ( Χ < Ε Χ - ί ) < 6 χ ρ ( - λ ν > ( γ ) ) < e x p ( - | r ) . 
*>0. 
(2.6) 

INDEPENDENT SUMMANDS 
27 
Proof. We can rewrite (2.4) as 
Ρ(Λ" >EX + t)< exp(-Ay>(-^) - (n - ^ ( ^ 7 ^ ) ) , 
0 < t < n - X. 
Replacing X by n - X, or by a similar argument using (2.2), we obtain also 
P(X < EX - t) < exp(-Ayp(y) - (n - Χ)φ( —^τ))- 
0 < r < λ. 
Since ψ(χ) > 0 for every x, we immediately obtain the first inequalities in (2.5) 
and (2.6). (These inequalities are trivial for t > n - λ and t > λ, respectively.) 
Since φ(0) = 0 and φ'[χ) = log(l + i ) < x, we have ψ(χ) > x 2/2 for 
- 1 < x < 0; hence the second inequality in (2.6) follows. 
Similarly, φ(0) = φ'{0) = 0 and 
"i ) = - Í — > 
1 
= ( 
χ2 
\" 
Ψ 
{X) 
l + x - ( l + x / 3 ) 3 
V 2 ( l + x / 3 ) / ' 
whence ψ(χ) > x 2/(2(l + x/3)). Thus the second inequality in (2.5) follows. 
■ 
Note that the exponents in the estimates in (2.5) are Θ(ί2) for small t, say 
t < X, but for larger t only ©(ílogí) (the first estimate) and θ(ί) (the second 
estimate). 
For small ratio t/X, the exponent in (2.5) is almost ι2/2λ. The follow-
ing corollary is sometimes convenient (cf. Alon and Spencer (1992, Theorem 
A.ll)). 
Corollary 2.2. If X 6 Bi(n,p) and X = np, then 
P(X>EX + 0<exP(4 + ¿). 
(2.7) 
Proof. The bound follows from (2.5), since (A + r/3)" 1 > (A - r/3)/A2. 
■ 
Another immediate corollary is the following two-sided estimate. 
Corollary 2.3. If X e Bi(n,p) ande > 0, then 
Ψ{\Χ -ΕΧ\ 
>εΕΧ) 
<2βχρ(-φ{ε)ΕΧ), 
(2.8) 
where φ(ε) = (1 + ε) log(l + ε) - ε. In particular, if ε < 3/2, then 
Ρ(\Χ-ΕΧ\>εΕΧ) 
< 2 e x p f - ^ E x ) . 
(2.9) 

28 
EXPONENTIALLY SMALL PROBABILITIES 
Proof. The first estimate is immediate by Theorem 2.1, since φ(—ε) > φ(ε) 
(Exercise!). The second one follows because φ(ε) > ε2/[2(1+ε/3)] > ε 2/3. 
■ 
For larger deviations, we state another handy version of (2.5). Note that 
λ does not appear explicitly in the estimate. 
Corollary 2.4. If X G Bi(n,p), A = np and c > 1, then 
*(X >x)< 
exp(-c'x), 
i > cA, 
(2.10) 
where d = logc - 1 + 1/c > 0. In particular, 
V(X >x)< 
exp(-i), 
x > 7A. 
(2.11) 
Proof. Apply (2.5) with t = x - A and note that \<p(x/\ - 1) = 
χψ(χ/\), 
where rp(y) = logy - 1 + 1/y is increasing for y > 1. Finally, note that 
^(7) > 1. 
■ 
Remark 2.5. Another simple bound is, still assuming X G Bi(n,p), 
/ 2t 2\ 
P ( A - > E X + t ) < e x p ( 
J, 
t > 0 ; 
(2.12) 
the same bound holds for ¥(X < EX — t) by symmetry. For p = 1/2 these 
bounds are better than (2.5) and (2.6), but they are worse for small p (note 
that the denominator of the exponent is n, not A). Inequality (2.12) can be 
derived from (2.4) (Exercise!). It is also a special case of Azuma's inequality 
in the version given in Remark 2.28 below (Exercise!). 
Remark 2.6. As a limiting case, obtained by taking p = A/n for any fixed 
A > 0 and letting n -> oo, (2.5)-(2.12) hold for a Poisson distributed random 
variable X G Po(A) too. 
Remark 2.7. The estimate (2.5) would not hold without the term t/3 in the 
denominator; this can be seen by considering a limiting Poisson distribution 
as in Remark 2.6, in which case ?(X > EX + t) = exp(-Q(tlogt)) as t -+ oo 
(Exercise!). 
The general case 
Now we return to the general case in which X, G Be(p<) with (possibly) 
different p¿. Let Y € Bi(n,p) with p = A/n = X3P»/n- I* ' s easily seen, taking 
the logarithm and using Jensen's inequality, that for every real it, 
EeuX 
= Y[(l+Pi(eu 
- 1)) < ( l + p ( e u - 1))" = E e " y . 
i 
Consequently, every bound for P(K - EY > t) derived from (2.1) applies to 
X too; since \ = EX = EY, the following theorem holds. 

INDEPENDENT SUMMANDS 
29 
Theorem 2.8. // Xi e Be(pj), i = l , . . . , n , are independent and X = 
£ " Xi, then (2.5) (2.12) hold, with X = EX. 
■ 
Remark 2.9. By the proof above, also (2.4) holds under the conditions of 
Theorem 2.8, and yields sharper bounds than (2.5) and (2.6). Further similar 
bounds under the same conditions and, even more generally, for any indepen-
dent random variables X¿ such that 0 < Xi < 1, are given, for example, by 
Bennett (1962), Hoeffding (1963) and Alon and Spencer (1992, Appendix A). 
We mention the following which use the variance of X rather than the mean. 
Let a2 = Var X = £ ? p¿(l - p¿), and let ψ{χ) = (1 + i) log(l + x) - i as 
above. Then (Bennett 1962, Hoeffding 1963), 
¥{X >EX 
+ t) <exp(-a2<p{t/a2)), 
t > 0; 
(2.13) 
P ( X > E X + t ) < e x p ( - ^ ^ ) , 
< > 0 ; 
(2.14) 
and, by symmetry, 
P(X < Ε Χ - ί ) < β χ ρ ( - σ ν ( * / σ 2 ) ) , 
t > 0; 
(2.15) 
P ( X < E X - t ) < e x p ( - ^ ^ ) , 
l > 0 . 
(2.16) 
The bound (2.14) is due to Bernstein, while the sharper (2.13) is due to 
Bennett (1962). Note that these are sharper than (2.5) because σ2 < λ, but 
for small pi, the difference is small and often negligible. For fairly small t, 
these bounds are quite sharp and not far from what might be hoped for, based 
on the asymptotics given by the central limit theorem (for t x σ). However, 
for large t, they can be substantially improved. 
The hypergeometric distribution 
Let m, n and N be positive integers with max(m, n) < N. The hypergeometric 
distribution with parameters N, n and m is the distribution of the random 
variable X defined by taking a set Γ with |Γ| = N and a subset Γ" C Γ with 
|Γ'| = m, and letting X = |Γ„ Π Γ'|, where Γ η is a random subset of Γ with n 
elements as in Chapter 1. (E.g., we can take Γ = [N] and Γ' = [m].) 
In other words, we draw n elements of Γ without replacement, and count the 
number of them that belong to Γ". Note that drawing with replacement would 
yield a binomial random variable; it seems reasonable that drawing without 
replacement tends to produce smaller random fluctuations, and indeed the 
bounds obtained above still hold (Hoeffding 1963). 
Theorem 2.10. Let X have the hypergeometric distribution with parameters 
N, n and m. Then (2.5)-(2.12) hold, with A = EX = 
mn/N. 
Proof. Let Y € Bi(n,m/N). 
It is not difficult to show that Ee u X < E e " y 
(Hoeffding 1963) for any real u, which yields (2.5) and (2.6), and thus (2.7)-
(2.12) too, by the argument above (Exercise!). 

30 
EXPONENTIALLY SMALL PROBABILITIES 
Alternatively, as a special case of a result by Vatutin and Mikhailov (1982), 
X has the same distribution as a certain sum of n independent indicator ran-
dom variables. (The proof is algebraic, and based on showing that the proba-
bility generating function has only real roots; there is no (known) probabilistic 
interpretation of these random indicators, which, in general, have irrational 
expectations.) Consequently we can apply Theorem 2.8. 
■ 
Remark 2.11. The second proof shows that, in fact, all bounds (2.4)-(2.16) 
hold with λ = EX = mn/N anda 2 = VarX = 
nm(N-n)(N-m)/N2(N-l). 
(The first proof yields all the bounds involving λ, but it only gives weaker 
versions of (2.13)-(2.16) with σ2 = Vary > Var X.) 
2.2 
BINOMIAL RANDOM SUBSETS 
The FKG inequality 
We begin by quoting a celebrated correlation inequality known as the FKG 
inequality (Fortuin, Kasteleyn and Ginibre 1971). (For a simple proof, see 
Grimmett and Stirzaker (1992, Problem 3.11.18(b)); for a powerful combi-
natorial generalization, see Ahlswede and Daykin (1978) or Bollobas (1986).) 
Consider a binomial random subset Γρ as in Chapter 1, or more generally 
ΓΡΙ,...,ΡΛΜ which is defined by including the element i with probability p¿, 
independently of all other elements, i = 1,..., N (assuming Γ = [TV] for no-
tational convenience). We say that a function / : 2 r -> R is increasing if for 
AC B, f(A) < f(B), and decreasing if f(A) > 
f(B). 
Theorem 2.12. // the random variables X\ and X2 are two increasing or 
two decreasing functions ο/ΓΡΐι...ιΡΛ,, then 
E(X 1X 2)>E(X 1)E(X 2). 
In particular, if Q\ and Q2 are two increasing or two decreasing families of 
subsets of Γ, then 
Ρ(ΓΡι 
PN € Gi Π Q2) > Ρ(ΓΡι 
PN € Qi) Ρ(ΓΡι 
PN € Qa). 
■ 
As an important application consider a family S of non-empty subsets 
of Γ and for each A € S let IA = 1[A C ΓΡΐι...,ΡΝ]. Note that every IA is 
increasing. Finally, let X = 5ΖΛ€<51 A , i-e. X is the number of sets Ae S that 
are contained in ΓΡ1 
PN. 
Corollary 2.13. For X = Σ Λ € 5 ?A °f ^e i0Tm Just described, 
P(X = 0)>exp(-—— 
}. 
1 
1 - max< pi J 

BINOMIAL RANDOM SUBSETS 
31 
Proof. By Theorem 2.12 and induction we immediately obtain 
P(X = 0)> JI(l-E/,t). 
A€S 
Now, using the inequalities 1 - x > e~xl{x~x) and EIA < maxp¿ we conclude 
that 
f 
EX 
1 
Í 
EX 
\ 
v 
I 
1 - max,» E IA J 
I 
1 - max¿ p¿ J 
We will soon give some similar exponential upper bounds on Ϋ(Χ = 0). First, 
however, we show a more general large deviation result. 
Upper bounds for lower tails 
We continue to study random variables of the form X = Σ/ies ^Λ as m the 
preceding subsection. For the lower tail of the distribution of X, the following 
analogue of Theorem 2.1 holds (Janson 1990b). 
Theorem 2.14. Let X = Y,AeS I A as above, and let A = EX = 
ΣΑ®1* 
αηάΔ = Σ, ΣΑΠΒ^ 
WAIB)- 
Then, with φ(χ) = (1 + x) log(l + x) - x, for 
0<t<EX, 
r ( X < E X - 1 ) < e x p ( - ^ f ^ ) < e x p ( - ¿ ) . 
Remark 2.15. Note that the definition of Δ includes the diagonal terms 
with A = B. It is often convenient to treat them separately, and we define 
Δ = 5 Σ Σ 
^ΛΙΒ). 
ΑφΒ,ΑίΛΒ^ 
(The factor \ reflects the fact that Δ is the sum of E(IAIB) 
over all unordered 
pairs {A, B) € [S]2 with A ΓΊ B Φ 0.) Thus Δ = λ + 2Δ. 
Remark 2.16. Clearly, Δ > 0 and thus Δ > A, with equality if and only if 
the sets A are disjoint and thus the random indicators IA are independent. 
In the independent case, the bounds in Theorem 2.14 are the same as (2.6); 
Theorem 2.14 is thus an extension of (the lower tail part of) Theorem 2.1. 
More importantly, in a weakly dependent case with, say, Δ = o(A) and thus 
Δ ~ A, we get almost the same bounds as in the independent case. 
Remark 2.17. There can be no corresponding general exponential bound 
for the upper tail probabilities P(X > EX + t), as is seen by the following 
example (for another counterexample, see Remark 2.50). Let A be an integer, 
let Γ = {0,...,2A2} with po = A"4, p¿ = 1 - λ~4 for 1 < i < A2 and 

32 
EXPONENTIALLY SMALL PROBABILITIES 
Pi = A-1 - A-4 + A-8 for A2 + 1 < i < 2A2, and consider the family S of the 
subsets At = {0,i} for 1 < ι < A2 and A{ = {¿} for A2 + 1 < t < 2A2. Then 
EX = A and Δ < 1. Nevertheless, for any c < oo and e > 0, if A is large 
enough, 
P(X > cA) > A~4(l - λ-4)λ2 > ÍA-" > βχρ(-ελ). 
Some partial results for the upper tail are given in Section 2.6. 
Proof of Theorem 2.11 Let *(s) = E(e~sX), s > 0. We will show first that 
-(log*(s))' > \e-*'x, 
s > 0, 
(2.17) 
which implies 
log*(s)> / 
Jo 
\e~u*/xdu 
= £ ( 1 - e-**/A). 
(2.18) 
In order to do this, we represent -$'(s) in the form 
-*'(«) = E(Xe-sX) = ] Γ Ε(/ Λβ-' χ) 
(2.19) 
A 
and for every A € S we split X = Y A + ZA, where Y A = ΣβηΛ^β ' β · Then, 
by the FKG inequality (applied to ΓΡ1 
PN conditioned of I A = 1, which is a 
random subset of the same type) and by the independence of ZA and I A we 
get, setting pA = E(IA), 
E(/ Ae-' x) = pAE(e-iY*e-z*\IA 
= 1) > pAE{e~'Y*\IA 
= l)E( e-*^) 
>PAE(e-'YA\IA 
= ims). 
(2.20) 
Recall that A = ΣΑΡΑ· 
^Tom (219) and (2.20), by applying Jensen's in-
equality twice, first to the conditional expectation and then to the sum, we 
obtain 
-(log*(s))' = ^
^ 
> ΣρΑΕ{β-γ<\ΙΑ 
= 1) 
Φ(β) 
*-¿ 
^ 
λ Σ 
\ Ρ Α <XP{-V(SYA\IA 
= 1)} 
> Aexp I - j ; jpA E(SYA\IA 
= 1) | 
= Aexp 1-{Σ 
E(YAIA)\ = \e-*'\ 

BINOMIAL RANDOM SUBSETS 
33 
We have shown (2.17) and thus (2.18). Now, by Markov's inequality (2.2) 
together with (2.18), 
logP(X < A - i ) <logE{e-'x)+s(X-t) 
< - = ( 1 - e" s A / A) + s(X - t). 
The right-hand side is minimized by choosing s = - log(l - </λ)λ/Δ, which 
yields the first bound (for t = λ, let s ->■ oo); the second follows because 
φ(χ) > i 2 / 2 for x < 0 as shown in the proof of Theorem 2.1. 
■ 
The probability of nonexistence 
Taking t = ΕΛ" in Theorem 2.14, we obtain an estimate for the probability 
of no set in S occuring, which we state separately as part of the following 
theorem (Janson, Luczak and Rucinski 1990). 
Theorem 2.18. With X = ΣΑ€δ 
I A , λ = EX and Δ as above, 
(i) 
Ρ(Χ=0) <βχρ(-λ + Δ); 
λ2 
\ 
(ii) 
P ( X = 0 ) < e x P ( - x ^ l x ) = e x P ( - ZEAHB^WAIB), 
Remark 2.19. Both parts are valid for any λ and Δ, but (i) is uninteresting 
unless Δ < A. In fact, (i) gives the better bound when Δ < A/2, while (ii) is 
better for larger Δ (Exercise!). 
Proof. Taking t = A in Theorem 2.14, or directly letting s -► oo in (2.18) and 
observing that limj^oo *(s) = Ϋ(Χ = 0), we immediately obtain (ii). 
For (i), we obtain from the proof of Theorem 2.14, with Y'A = Y A - I A , 
- l o g P ( X = 0) = - f ° ( l o g * ( S ) ) ' d s > Γ 
ΤΡΑΕ(β-'γ- 
\IA = l)ds 
Jo 
Jo 
A 
When IA = 1, we find \)YA = 1/(1 + YA) > 1 - \YA (since Y'A is an integer), 
and thus 
- logP(X = 0) > ΣρΑ 
E(l - \Y'A \IA = \) 
A 
= Σ(Ρ^"5 Ε(^^))=λ-Δ. 
■ 
A 
Remark 2.20. Boppana and Spencer (1989) gave another proof, resembling 
the proof of the Lovász Local Lemma, of a version of Theorem 2.18(i), namely, 
f(X = 0) < exp{A/(l - e)} Π ( 1 - Ε / Λ ) < βχρ{-λ + Δ/(1 - ε)}, (2.21) 

34 
EXPONENTIALLY SMALL PROBABILITIES 
where e = maxp^. 
See also Spencer (1990) for another proof of Theo-
rem 2.18(a), but with an extra factor \ in the exponent. Finally, note that 
a slightly weaker version of Theorem 2.l8(i) with the bound εχρ(-λ + 2Δ) 
follows directly from (ii), because λ2 > (λ - 2Δ)(λ + 2Δ). 
Remark 2.21. Although the bound in Theorem 2.18 and the first bound in 
(2.21) are quite close when e = maxp a is small, neither of them dominates 
the other. It is intriguing to note that the conceivable common improvement 
εχρ{Δ} Y\A(l -EIA) 
fails to be an upper bound for P(X = 0); this is seen by 
the simple example where X = I\ + h with I\ = I2 G Be(p), for which Δ = p 
and P(X = 0) = 1 - p > ep(l - p)2, see Janson (1998) for further discussion. 
The quantity Δ is a measure of the pairwise dependence between the I A 'S 
(c/. Remark 2.16). If Δ = ο(λ), then the exponents in Theorem 2.18 are 
- E X ( l + o(l)), matching asymptotically the lower bound Corollary 2.13, 
provided further maxp< -> 0. 
The development of the exponential bounds in this section were stimulated 
by the application in which X counts copies of a given graph in the random 
graph G(n,p). This will be presented in detail in Chapter 3 (c/. Theorem 3.9). 
For a generalization of Theorem 2.14 see Roos (1996). 
2.3 
SUEN'S INEQUALITY 
A drawback of the inequalities in Section 2.2 is that they apply only to the sum 
of random indicator variables with a very special structure. For example, they 
apply, as stated above, to the number of copies of a given graph in G(n,p), 
but they do not apply to the number of induced copies. 
An inequality much more general than Theorem 2.18(i), and only slightly 
weaker, was given by Suen (1990). We do not give Suen's original inequality 
here, but rather the following related results, proved by Suen's method. For 
further similar results, see Janson (1998) and Spencer (1998). 
The Suen inequalities use the concept of a dependency graph, defined in 
Section 1.2. Although formally valid for the sum of any family of random 
indicator variables, the inequalities are useful in cases in which there exists 
a sparse dependency graph. 
(No assumption is made on the type of the 
dependencies.) 
Theorem 2.22. Let Ii € Be(pj), i G I , be a finite family of Bernoulli random 
variables having a dependency graph L. Let X — JV J¿ and X = EX = Σ^Ρί· 
Moreover, write i ~ j if ij G E{L), and let Δ = | Σ Σ Η
Ε ^ ' ^ 
o n d 
S = maxi'£lk^ipk- 
Then 
(i) 
Ρ(Χ = 0)<βχρ{-λ + Δβ2Α}; 
(ii) 
P(X = 0)<exP{-min(^,A)}. 
. 

SUENS INEQUALITY 
35 
Theorem 2.23. Let U 6 Be(p¿), i £ I, be a finite family of Bernoulli random 
variables having a dependency graph L. Let X = X^¿/¿ and A = E X , and 
let also Δ and 6 be as in Theorem 2.22. 
Moreover, let Δ = A + 2Δ = 
λ + Σ Σ ί - i E(/./;)· // 0 < t < X, then 
P ( X < A - i ) < e x p ( - m i n ( ^ = , ^ ) ) . 
(2.22) 
For simplicity, we give only the proof of Theorem 2.23 here; the proof of 
Theorem 2.22 is similar (Janson 1998). Note that taking t = A in Theo-
rem 2.23 we obtain an inequality which is only slightly weaker than Theo-
rem 2.22(a). 
Remark 2.24. We have given results similar to Theorems 2.14 and 2.18, 
but with somewhat worse constants, and extra terms (typically negligible) 
involving <5. It is not known whether the terms with δ really are needed; in 
fact, it is conceivable that the estimates in Section 2.2 hold also under the 
weaker assumptions in this section. 
Proof of Theorem 2.23. Define, for real s, the random function 
F{s) = 
e'x-'x, 
and, for each subset AC I, XA = £ i e / t U and FA(s) = exp(s(EXA 
- 
XA))· 
We differentiate and obtain 
F'(s) = XF(s)-'£tIiF(s). 
(2.23) 
i 
For each index i € I , let N¿ = {i} U {j € I : i ~ j} and C/¿ = I \ iV¿; then 
X = XN¡ + Xu¡ and 
he-'x 
= Pie-'x 
+ (Ii - Pi)e-'Xv< 
- (/< - P i)(l - e-x»> )e~'x^. 
(2.24) 
Now assume that s > 0. Then, for any set AC 
I, 
0 < 1 - e~'x* 
< sXA, 
and thus, considering the cases Λ = 0 and U = 1 separately, 
{Ii - Pi){l - e~'x*) 
< IiSXA. 
(2.25) 
Choosing A = N¿, (2.23), (2.24) and (2.25) imply 
F'{s) < XF(s) - 5 > e s A - * - ]Γ(/< -Pi)e'x-X"< 
+ 
YisIiXNie'x-'x"i 
i 
i 
i 
= -e>x £ ( / i - 
ft)e-*"« 
+ e'x £ 
sUX^e"^. 
i 
i 

36 
EXPONENTIALLY SMALL PROBABILITIES 
Since, by the definition of a dependency graph, /, and Xyk are independent, 
E((/¿ - Pi)e~'Xu') = 0. Moreover, XVi > Χυ{ηυ,, which is independent of 
Iilj, and 
Χ-ΕΧυ,ηυ, = ]Γ P*<X>* + £p*<2<5. 
k$UiC\Uj 
fc~t 
k~j 
Hence, 
EF'(s) < e , A£ Σ sWiIje"Xui) 
< ee'A £ Σ E(UIje-Xv¡™>) 
i 
j€ Ni 
i 
]ζΝί 
= se'xJ^ Σ E(/¿/j)Ee-Xw'nüi 
^ se2Í' Σ Σ Ε(Λ^) Ef^nt/^s), 
s > 0. 
i 
>€JV; 
(2.26) 
We claim that 
EF^Se*'**'2", 
s>0. 
(2.27) 
In fact, using induction over |Z|, the number of random indicator variables, 
we may assume that the corresponding inequality holds for EFA(S) for every 
proper subset A of I (and all s > 0). Since the corresponding values A A and 
SA for a subset A satisfy A A < Δ and SA < Ä, it then follows from (2.26) that 
EF'(s) < se2S> £ £ Ei/i/^ei' 2^"* = se^Äe*' 2*'"* 
s ¡ ( « ' " * ) . 
· * · ■ 
Hence (2.27) follows by integration, since it obviously holds for s = 0. 
Markov's inequality (2.2) and (2.27) yield, for any s > 0, 
P(X < λ - t) < P(F(s) > e3t) < e-stEF{s) 
< e-*t+k»2~Z'"\ 
(2.28) 
We choose here s = min(t/2Ä~, l/3<5); then e2i* < e2/3 < 2, and thus 
±s2A~e2it < β2Δ < sí/2. 
Consequently, (2.28) yields 
P(X < λ - i) < e—e/2, 
which is (2.22). 
■ 

MARTINGALES 
37 
2.4 
MARTINGALES 
Martingales were first applied to random graphs by Shamir and Spencer 
(1987), followed by the spectacular success of BoUobás's (1988a) solution of 
the chromatic number problem. (These results are presented in Chapter 7.) 
We begin by recalling the definition of a martingale. Note, however, that 
for applications to random subsets, and to random graphs in particular, one 
usually uses Corollary 2.27 below, where martingales are not explicitly men-
tioned. 
Given a probability space (Ω, F, P) and an increasing sequence of sub-σ-
fields To — {0, Ω} C T\ C ·· · C Tn = T, a sequence of random variables 
Χο,Χι,. 
■ ■ ,Xn 
(with finite expectations) is called a martingale if for each 
k = 0,... ,n - 1, E(X*+1 | Tk) = Xk- In this case (with a finite sequence), 
every martingale is obtained from a random variable X by taking Xk — E(X | 
Tk), k = 0,...,n. Then Xo = EX and Xn - X. 
Also, we always have 
E{Xk+i) 
= E(Xk). 
In combinatorial applications, often Π is a finite space and T is the family 
of all subsets; each Tk then corresponds to a partition Vk of Ω, with coarser 
partitions for smaller k. If P is the uniform probability measure on Ω, then 
a sequence (Xk) is a martingale if and only if each Xk is a function Ω -» R 
that is constant on the blocks of the partition Vk, with the value on each 
block being the average value of Xn on that block. If P is another probability 
measure, then this still holds if we use suitably weighted averages. 
Azuma's inequality 
The following result appears in Azuma (1967) and is often called Azuma's 
inequality, although it also appears in Hoeffding (1963). 
Theorem 2.25. // (X*)o is a martingale with Xn = X and X0 = EX, and 
there exist constants c* > 0 such that 
\Xk - Xk-i\ < ck 
for each k <n, then, for every t > 0, 
P ( X > E X + t ) < e x p ( - - j L - T ) , 
(2.29) 
?{X < EX - t) < e x P ( - _ 4 - A 
(2.30) 
Proof. Set Yk = Xk- 
Xk-i 
and Sk = Σί=ι Yi = Xk-Xo. 
For any u > 0, by 
Markov's inequality (2.1), we have 
P(X - E X > t) = P(S„ > t) < e~ut E(e" 5-). 
(2.31) 

38 
EXPONENTIALLY SMALL PROBABILITIES 
Because Sn-i is a !Fn-1 -measurable function we also have 
E(euS") = E[E(euS" | Tn-x)\ = E[euS— E(e"y" | ^„-O]. 
Now we need the following fact: If a random variable Y satisfies EY = 0 
and - o < Y < a for some a > 0, then, for any u, 
E(euV')<eu2°2/2. 
(2.32) 
To prove (2.32), note that by the convexity of euv, 
e«V < « + * > 
+ 
^ e - u . . 
~ 
2a 
2a 
Hence, 
E(e"y) < \eua + ie- u o < eu2°2/2, 
where the last inequality follows by comparing the Taylor expansions (see 
Alon and Spencer (1992, Lemma A.6) for another proof). 
Coming back to the proof of Theorem 2.25, we conclude that E(euY" | 
•Fn-i) < e"2c-/2 and thus 
E(euS") < eu2c"/2E(euS—'). 
Iterating this inequality n times, we find E(euS") < eu Sc»/2
) and thus by 
(2.31) 
P(JT - EX > i) < e-^e" 2^ 2·/ 2; 
substituting u = t/ΣσΙ 
we find (2.29). 
The inequality (2.30) follows by symmetry. 
■ 
Remark 2.26. After a minor modification, (2.29) extends to supermartin-
gales and (2.30) to submartingales; see Wormald (1999a). 
Combinatorial setting 
In the applications to random graphs, we will use the following consequence 
of Theorem 2.25. Note that the notion of a martingale has disappeared from 
the statement. In most applications, one simply has c* = 1. (We tacitly 
assume that the function / is measurable; in the case of finite sets, this holds 
trivially.) 
Corollary 2.27. Let Z\,...,ZN 
be independent random variables, with Zk 
taking values in a set A*. Assume that a function / : Λι χ Λ2 x · · · x ΛΛΓ -> R 
satisfies the following Lipschitz condition for some numbers c^: 

TALAGRANDS INEQUALITY 39 
(L) // two vectors ζ,ζ' 6 ΠΓ ^» differ only in the kth coordinate, then 
l/(*)-/(*')l<c*. 
Then, the random variable X = f{Z\,. 
..,ΖΝ) 
satisfies, for any t > 0, 
Ρ ( Χ > Ε Χ + < ) < β χ ρ ( - - ^ Γ - ) , 
(2.33) 
T(X<EX-t)<exp(--Ári). 
(2.34) 
2 ¿vl 
Ck 
Proof. Let us define Tk to be the σ-field generated by Z\,..., 
Zk and consider 
the corresponding martingale defined by Xk = E ( / ( Z i , . . . , Zjv) | Ft), k = 
Ο,.,.,Ν. 
The assumption about / implies that Xk and Xk-i 
differ by at 
most Ck (Exercise!). The corollary now follows from Theorem 2.25. 
■ 
Remark 2.28. A more careful proof along the same lines shows that (2.33) 
and (2.34) hold with exp{-2t 2/£;f c2.} on the right-hand side, that is, the 
exponents in the estimates may be multiplied by a factor 4 (McDiarmid 1989). 
Returning to the random set Γρ, one typically defines the random variables 
Zk via the random indicators I-, = l[j € Γρ], j € Γ. Given a partition 
A\,..., 
AN of Γ, each Zk is then taken as the random vector (77 : 7 € Ak) € 
{Ο,Ι}'4*, and for a given function / : 2 r -> R, the Lipschitz condition (L) 
in Corollary 2.27 is equivalent to saying that for any two subsets 
A,BCT, 
\f{A) - f(B)\ < Ck whenever the symmetric difference of the sets A and B is 
contained in Ak ■ (We identify the set of subsets 2 r and the set of sequences 
{0,l}r·) 
When Γ = [n]2 and so Γρ = G(n,p), there are two common choices of the 
partition [n]2 = ^4iU· ■ U^4^· The vertex exposure martingale (used by Shamir 
and Spencer (1987)) corresponds to the choice N — n and Ak = [k]2 \[k- 
l] 2. 
The edge exposure martingale (used by Bollobás (1988a)) is one in which 
N = (2) and \Ak\ = 1 for each k. Note that vertex exposure requires a 
stronger condition on the function / than edge exposure, but it also gives a 
stronger result when applicable. (With Ck = 1, edge exposure is applicable 
provided the random variable X changes by at most 1 if a single edge is added 
or deleted, while vertex exposure is applicable provided the random variable 
changes by at most 1 if any number of edges incident to a single vertex are 
added and/or deleted.) 
For further similar results (and applications), see the surveys by Bollobás 
(1988b) and McDiarmid (1989). 
2.5 
TALAGRAND'S INEQUALITY 
Talagrand (1995) has given several inequalities yielding exponential estimates 
under various conditions. In particular, one of his results leads to estimates 

40 
EXPONENTIALLY SMALL PROBABILITIES 
that are similar to those obtained by Azuma's inequality in the preceding 
section, but often much stronger. 
Here we will treat only one of Talagrand's inequalities. Moreover, the 
general version (Theorem 2.37, below) is rather technical; we thus begin with 
a special case which is easily applied in a number of combinatorial settings. 
For further results and many applications, see Talagrand (1995). Other proofs 
of Theorems 2.37 and Theorem 2.39 (and of further related inequalities) are 
given by Marton (1996) and Dembo (1997). 
Combinatorial setting 
In the sequel we assume, as for Corollary 2.27, that TV > 1 is an integer and 
that Z\,..., 
ZN are some independent random variables, taking values in some 
sets Λι,..., Λ/ν, respectively. (In many applications, Λι = · · · = A/v and the 
Zi are identically distributed, but that is not necessary.) We write z = (zt)£Li 
for an element of the product space Λ = Πι Λ*. (To be precise, the sets A¿ 
are measurable spaces, that is, sets equipped with σ-ñelds of subsets, and 
the function / is tacitly assumed to be measurable; in the case of finite sets, 
this assumption is trivially true.) The two common choices of Zk and A* in 
random graph theory are given by vertex exposure and edge exposure, just as 
discussed for martingales at the end of the preceding section. 
Recall that a median of a (real valued) random variable X is a number m 
such that F(X < m) < 1/2 and F(X > m) < 1/2. A median always exists, 
but it is not always unique. 
Theorem 2.29. Suppose that Z\,...,ZN 
are independent random variables 
taking their values in some sets Λχ,..., AN, respectively. Suppose further that 
X = f{Zi,..., 
ZN), where / : Ai x · · · x AN -> R is a function such that, 
for some constants c*, k = 1,..., N, and some function ψ, the following two 
conditions hold: 
(L) // ζ,ζ' € A = fit ^i differ only in the kth coordinate, then \f(z) 
-
f{z')\<ck. 
(C) If ze A and r e R with f{z) > r, then there exists a set J C { 1 , . . . , N) 
with Σ.67 °? - ^(r)> suc^ 
^at 
for "ii!/ 6 A with y, = z¡ when i G J, 
we have f(y) > r. 
Then, for every r e R and 
t>0, 
P(X < r - t) Ψ{Χ >r)< 
e-t2/**{T). 
(2.35) 
In particular, if m is a median of X, then for every t > 0, 
P(X < m - i) < 2e-t2/4*<m) 
(2.36) 

TALAGRANDS INEQUALITY 
41 
and 
P ( X > m + i)<2e- < 2 / 4 v , ( m + t ). 
(2.37) 
Remark 2.30. The Lipschitz condition (L) is the same as the condition in 
Corollary 2.27. 
Remark 2.31. Note that the set J in (C) generally depends on z and r. The 
vector (zi)iej, which forces / > r, is called a certificate (of / > r). 
We postpone the proof of the theorem until the end of this section and 
first discuss some consequences and applications. Note that the function φ 
formally may be chosen arbitrarily such that (C) holds; however, we want to 
find a small ψ since the bounds the theorem yields are better the smaller ψ 
is. 
Remark 2.32. In most applications, c/t = 1 for all k. In this case, the first 
condition on J in (C) is \J\ < i>{r); thus, ij>(r) = N will always do, but smaller 
bounds on \J\ give better estimates. 
Comparison with Azuma's inequality 
Every function / trivially satisfies (C) with ^(r) = γ^ 
c¡ for all r; just take 
J = { 1 , . . . , N}. Thus Theorem 2.29 yields, for example, the estimate 
¥(\X -m\>t)< 
4e-' 2 / 4 ^ c ? , 
t > 0, 
(2.38) 
for any function / satisfying (L). This is very similar to Corollary 2.27. The 
conditions are the same and the conclusions differ only in that here we get 
worse constants and that the median is used instead of the mean. These 
differences are typically not important; note that Corollary 2.27 implies that 
if a > EX + (21og2]>>ÍÉ)1/2, then F(X > a) < 1/2 and thus m < a, which, 
together with a similar lower bound, yields 
| E * - m | < (21og2]TcjQ 
. 
(This, with another constant, follows also from (2.38), using arguments as in 
Example 2.33 below. - Exercise!) The constant may be improved by using 
Remark 2.28.) 
In many applications, (C) holds with a much smaller ψ; this leads to 
stronger estimates that significantly surpass Azuma's inequality. 
Example 2.33. In several interesting cases, X assumes non-negative integer 
values only, (L) holds with c* = 1, and (C) holds with φ(τ) = r for integers 
r > 1. (Equivalently, (C) holds with ip(r) = [r] for r > 0.) In this case, 
(2.35) yields 
Ψ(Χ < a) P{X >r)< 
e-<r-°):'/4r 
(2.39) 

42 
EXPONENTIALLY SMALL PROBABILITIES 
for every integer r > 1 and real a < r. Since (r - a)2/r is an increasing 
function of r > a and P(X > r) = ?{X > \r]), (2.39), in fact, holds for any 
real a and τ with a < r. (The case a < 0 is trivial.) 
Consequently, if m is a median of X, then 
P ( X < m - < ) <2e~t2/4m, 
t > 0, 
(2.40) 
and 
P(X > m + t) < 2 e - ' ^ 
< [ ^ ^ 
° * * * ™· 
(2.41) 
Hence, 
tr»,' ,;J
 
(2·
42) 
In particular, it follows that 
/•OO 
| E X - m | < E | X - m | = / 
?{\X -m\ > t)dt 
Jo 
/•m 
ΛΟΟ 
< / 
4e-i2/8m dt + / 
2e"t/8 tfí < 2>/8Írm + 16 
JO 
Jm 
(the constants can be improved). Hence, using also |m < m¥(X > m) < 
EX, 
\EX 
-m\=0{y/EX), 
which implies estimates similar to (2.40)-(2.42) for X - EX; for example, for 
some universal constant 7 > 0, 
P ( | X - E X | > i) < 4e- 7 t S / ( E X + t ), 
t > 0. 
(2.43) 
We see that if m (or, equivalently, EX) is much smaller than N, then Theo-
rem 2.29 yields much stronger estimates than Corollary 2.27. 
Example 2.34. A simple instance of the situation in Example 2.33 is a bi-
nomial random variable, or, more generally, a sum of independent Bernoulli 
random variables. In this case, we let the sets A¿ equal {0,1} and let X = 
f{Zi,..., 
ZN) = Z\ + ■ ■ ■ + ZN ■ It is easy to see that (L) and (C) hold with 
d = 1 and φ(τ) = r when r is a non-negative integer, and thus (2.39)-(2.43) 
hold. This yields estimates similar to those given in Theorem 2.1 (although 
with inferior constants). 
This example shows that it is not possible to improve the estimate (2.41) to 
P(Z > m + i) < 2e-t /4m, or something similar with other constants; consider 

TALAGRAND'S INEQUALITY 
43 
for example the limiting case of a random variable with distribution Po(l) (or 
the family Bi(n, 1/n)) and large t; compare with Remark 2.7. 
Example 2.35. A more interesting application is obtained by letting X be 
the stability number a(G(n,p)) of the random graph G(n,p), that is, the 
order of the largest independent set of vertices. It is easily seen that, using 
vertex exposure, the conditions in Example 2.33 are satisfied (Exercise!); a 
certificate of a > r (for an integer r > 1) is just any independent set of 
order r. Consequently (2.39)-(2.43) hold. We will return to this application 
in Theorem 7.4. 
The same applies to the clique number of G(n,p), that is, the order of 
the largest complete subgraph (which is just the independence number of the 
complement of the graph). 
Remark 2.36. In general, say that a function / : Πι A¿ -► {0,1,... } is a 
configuration function if for each J C [jV] there exists a set Aj C fije./ Aj °^ 
"configurations" such that: 
(i) If (xj)>eJ € Aj and J' C J then {xj)ieJ· 
6 Aj·; 
(ii) f(z) = max{|J| : (zj)j€J 
€ 
Aj). 
In other words, the configurations are certain sequences (z^,, · · ·, z>„), a sub-
sequence of a configuration is a configuration and / is the size of the largest 
configuration included in (ζχ,..., ZN). The independence and clique numbers 
in Example 2.35 above are obvious examples. 
Every configuration function satisfies (L) and (C) with c* = 1 and ip(r) = r 
for integers r > 0, so the conclusions in Example 2.33 hold. (Conversely, it 
may be shown that every such / with values in {0,..., N) is a configuration 
function. See also Talagrand (1995, (7.1.7)) for yet another characterization.) 
Moreover, Boucheron, Lugosi and Massart (2000+) have recently shown 
that for configuration functions, the inequalities (2.5) and (2.6) hold, which 
yield somewhat sharper (and simpler) estimates than the inequalities above. 
General form of Talagrand's inequality 
In order to state the general form of Talagrand's inequality, we need more 
notation. 
Assume, as above, that A i , . . . , Ajv are sets. Assume further that 
μ\,...,μΝ 
are probability measures on A i , . . . , \ N , respectively, and let P be the product 
measure μ\ x · · · x μ^ on A = Ai x · ■ · x AN. 
We define a kind of distance between a point x € A and a subset A C A in 
the following way. We first define two subsets UA{x) and VA(x) of RN: 
UA{x) = {(Si)i 
£ {0,1}* : 3y 6 A such that x¿ = y¿ for all i with s¿ = 0} 

44 
EXPONENTIALLY SMALL PROBABILITIES 
and VA{X) 
is the convex hull of UA{X)- 
Thus UA(X) 
contains the vectors 
(1[ι, φ y,])?, y G A, but also vectors with more l's. We then define 
d{A,x) = 
\ni{\\v\\2:v€VA{x)}, 
where ||u||2 = (Σ)υ?)1/2 ' s t n e usual Euclidean norm in 1 ^ ; thus d{A,x) is 
the Euclidean distance from 0 to VA{x). (If A = 0, then UA(X) = VA{x) = 0 
and we set d{A, x) = oo. On the other hand, if A is non-empty, then d{A, x) is 
finite for every i because at least ( 1 , . . . , 1) € UA{X)- 
Moreover, the infimum 
in the definition of d(A, x) is attained, since UA (X) is a finite set and thus 
VA{x) is compact.) 
Note that 
d(A,x) = 0 <ί=Φ· 0 6Vyt(x) <=> OeUA{x) 
«=> x € A. 
With this notation, we may give the general form of Talagrand's inequality. 
Theorem 2.37. For every {measurable) subset A of Λ, 
J ei^A,x) άΨ{χ) ^ _L. 
(244) 
Remark 2.38. We assume that the set A is measurable, but even so the 
function d{A,x) 
is, in general, not measurable, so the integral in (2.44) is 
not always defined as an ordinary Lebesgue integral. Of course, there is no 
problem for finite sets, and it is easy to give further sufficient conditions 
for d{A,x) 
to be measurable, but a simpler and more general approach is 
to allow d{A,x) 
to be non-measurable and interpret the integral in (2.44) 
as an iterated upper integral /*···/*■ The theorem is then valid without 
any further assumptions (by the proof given below and simple properties of 
the upper integral). Moreover, there is no problem in using this version in 
applications such as the proof of Theorem 2.39, below. (Recall that the upper 
integral / * g of a non-negative function g is defined to be the infimum of / h 
over all measurable functions h with h > o; this infimum is always attained.) 
Before proving Theorem 2.37, we use it to prove Theorem 2.29. We begin 
with a simple corollary of Theorem 2.37 (Talagrand 1995). 
Theorem 2.39. Suppose that A and B are two {measurable) subsets of Λ 
such that for some t > 0 the following separation condition holds: 
(S) For every z £ B, there exists a non-zero vector a = (aj)f 6 R^ such 
that for every y G A, 

TALAGRANDS INEQUALITY 
45 
Then 
P(A)P(B) 
<e~t2/4. 
Remark 2.40. Note that Condition (S) is not symmetric in A and B\ the 
vector Q may depend on z but not on y. 
Proof. Suppose that z € B and let a be as in Condition (S). We may assume 
that ai > 0 for every i; otherwise we replace a¿ by |α<|. Then, denoting the 
scalar product in RN by (·, ·), 
<α,β)> t||a||2 
for every s € UA(Z). 
Since a >-t (a,s) is a linear functional, this extends to 
all s in the convex hull VA(z), and thus, by the Cauchy Schwarz inequality, 
for every s 6 
VA(Z), 
ί|Η| 2<(α, 8)<||5|| 2||α|| 2 
and hence t < ||s||2. Consequently d(A,z) 
> t, for every z € B, which, 
together with Theorem 2.37, yields 
e'2/4 P(B) < i e^d^A-z) df(z) < - i - . 
■ 
Next, we use Theorem 2.39 to prove Theorem 2.29. 
Proof of Theorem 2.29. Let A = {z £ Λ : f(z) < r - t} and B = {z € Λ : 
/(*) > r}. For 2 € ß, let J be as in (C) and define 
_ fci, 
i G ·/, 
Q< 
\0, 
i i J; 
thus, by (C), ||α||2 < \/Φ(Γ). 
If, furthermore, y € A, define y' 6 Λ by 
, _ Ui, 
i € J, 
{Vi, 
i i J-
Then /(y') > r by (C), and thus f(y') - /(y) > t, while (L) implies 
l/(»)-/(»')l< Σ, 
Ci= Σ a·· 
Consequently, 
£ 
a¿ > Í > ^(rJ-'/'Halla. 

46 
EXPONENTIALLY SMALL PROBABILITIES 
If t > 0, this also shows that a φ 0, and (2.35) follows by applying Theo-
rem 2.39 with t replaced by t/y/tjj(r). The case t = 0 is trivial. 
Finally (2.36) and (2.37) follow by taking r = m and r = m+t in (2.35). ■ 
Remark 2.41. The conclusion of Theorem 2.39 can be improved to 
y/\og{l/P(A)) + x/logU/PtB)) > t/y/2 
(Talagrand 1995, Corollary 4.2.5), which, by the argument above, implies, for 
example, that (2.36) can be improved to the smaller, but more complicated, 
bound 
P ( X < m - t ) < e x p ( - ( - ^ = - v ^ g 2 ) 2 ) , 
t> y/2\og2i>{m). 
It remains to prove Theorem 2.37. We follow Talagrand (1995), and begin 
with a simple lemma. 
Lemma 2.42. Suppose that 0 < r < 1. Then 
inf er'/4rT-1 < 2 - r. 
0<τ<1 
— 
Proof. Taking r = min(2 log(l/r), l), it suffices to show that if e - 1/ 2 < r < 1, 
then 
elogarr-2logr-l < 2 - r . 
Substituting r = e - t and taking logarithms, we have to show that 
h(t) = log(2 - e_t) +12 - t > 0 
for 0 < t < 1/2. But this, in fact, holds for all r > 0 because elementary 
calculations yield ft(0) = Λ'(0) = 0 and h"{t) = 2 - 2e"7(2 - e"')2 > 0 for 
i > 0 . 
■ 
Proof of Theorem 2.37. We use induction in N, starting with the simple case 
N = 1. (The bold reader may start with the really trivial case N = 0 instead.) 
If N = 1, and A is any non-empty subset of Λ = Λι, then, as is easily seen 
from the definition, d(A, z) = 0 when z G A and d{A, z) = 1 when z $. A. 
Consequently, using e1/4 < 2 and i(2 - t) < 1 for real t, 
eid^A·') 
d?(x) = ?{A) +e !' 4(l - P(A)) < 2-V(A) 
< - ^ r . 
Λ 
r\A> 
L 
Now assume that the result holds for some N > 1. Let us write Λ(** = 
nJ"Aj and Pfc = Π* A*¿. ai»d denote elements in Λ(;ν+1) by (ι,λ), with x € 
Λ^> andA€A N +i. 

TALAGRANDS INEQUALITY 
47 
Let A be a (measurable) subset of Λ(Λ,+1), and define, for every λ 6 Λ/ν+ι, 
A(X) = {x € A{N) : (χ,λ) € A} (a section of .4). Define also B = UA<4(A) (the 
projection of A on Λ ^ ) . Each A(\) is measurable, but, in general, B is not; 
thus we also select a measurable subset Bo C B of maximal P/v measure. Note 
that thus P/v(Bo) > P;v(^(A)) for every λ. We may assume that P/v(#o) > 0, 
since otherwise P;v+i(-<4) = 0 and the result is trivial. 
The basic observation is that, for any x € Λ(/ν) and λ 6 Λ/ν+ι, 
seUA{x){x) 
= > 
(s,0)eUA((x,X)), 
teUB(x) =*· (U)ei/„((x,A)). 
It follows that if s € V,»(A)(X), ί 6 VBo(x) C VB(x) and 0 < τ < 1, then 
(s, 0) € VA ((x, λ)) and (i, 1) € VA ((x, λ)), and thus also ((1 - r)s + ri, r) 6 
V/( ((x, λ)), which yields, using the convexity of the function m->u 2, 
N 
d2{A,(x,X)) 
< II((1 - r)s + Tt,r)\\l = £ ( ( 1 - r) S i + rttf 
+ r 2 
i 
< ( l - T ) N l i + r||t||2
2 + r 2. 
Taking the infimum over s and i, we thus obtain, for every λ € Ayv+i and 
r 6 [0,1], 
d2{A,(x,X)) 
< (1 - r)d2(A(A),x) + rd 2(B 0,x) + r 2. 
Holder's inequality and the induction hypothesis now yield 
f 
eK(,t,(x.A)) 
dpN{x) 
J\iN) 
< e^ 
( [ 
e ^ W · " 
dP„(x))l_r ( ( 
βΚ<*·.-> 
d?N(x))* 
VA«") 
' 
V A < ~ > 
' 
<e\ru 
\ 
γ-τ( 
i 
v 
_ 
1 
i r»/FAr(¿(A))y-i 
PN(BO) 
v PN(BO) 
/ 
By Lemma 2.42 with r = Pjv(A(A))/PN(£0) < 1, we obtain by taking the 
infimum over r 
/ 
ei^.(».A))rf P 
( g ) < 
1 
( 2_Ρ^(Α(Α))χ 
ACN) 
m 
; - Ρ Ν ( Β 0 ) Γ 
P W ( B 0 ) 
λ 
Finally, we integrate this over A, using Fubini's theorem and the inequality 
2 - t < 1/t for t > 0, to obtain 
/ 
ei«^·*) dP„+1(z) < 
* 
(2 - !^±lii2\ 
-ΡΝ+Ι(Α)' 
■ 

48 
EXPONENTIALLY SMALL PROBABILITIES 
2.6 
THE UPPER TAIL 
As remarked above (Remark 2.17), the upper tail counterpart of Theorem 2.14 
is not true, in general. As an exponential bound is often needed also for the 
upper tail, we present here briefly a few simple ideas on how to cope with this 
problem in certain situations. For a more thorough account see Janson and 
Rucinski (2000+). 
Recall that in a random set Γρ, each element of Γ is included with the same 
probability p. Furthermore, as in Section 2.2, let S be a family of subsets of Γ. 
For the sake of clarity, we confine ourselves here to the slightly simplified case 
in which all members of S are of the same size s. 
One possible idea is to convert an upper tail probability into a lower tail 
and then to apply Theorem 2.14. This can be done by setting Z = |ΓΡ|, 
5 = [Γ]4 \ S, and X = ]£y»e,§ TA = (f) - X- As this approach is limited in 
applications only to large families S, we will not pursue it any further. 
The first result we do present was stimulated by the following problem 
(Rödl and Rucinski 1994) on a random graph obtained by a random deletion 
of vertices (c/. Section 1.1). 
Example 2.43. Let G = (V,E) be a graph with |V| = n and |E| < 7j(!J), 
0 < η < 1, and let R = Vp be a binomial random subset of the vertex set V, 
0 < p < 1. Using Proposition 2.44 below one can show that with probability 
1 _ e-0(np) w e h a v e |[Λ|2 n £ | < 2TJ(1*1) (Exercise!). 
The underlying idea is to break the family S into disjoint subfamilies of 
disjoint sets, and apply Theorem 2.1 to one subfamily. Set L = L(S) for the 
standard dependency graph of the family of indicators {IA ■ A € «S}, where 
an edge joins A and B if and only if A D B φ 0 (see Example 1.5). Note that 
the maximum degree Δ(Ζ,) can be as large as \S\ - 1. The following result has 
appeared in a slightly more complicated form in Rödl and Rucinski (1994). 
Proposition 2.44. With the notation of Section 2.2, for every t > 0, 
nX 
>EX 
+ t)< (A(L) + l)exp (~4{A{L)/1){Χ 
+t/3)) · 
Proof. A matching in 5 is a subfamily M C S consisting of pairwise disjoint 
sets. We claim that the family S can be partitioned into A(L) + 1 disjoint 
matchings, each of size equal to either \\S\/(A(L) 
+ 1)1 or [|5|/(Δ(Ζ,) + 1)J. 
Indeed, by the well known Hajnal-Szemerédi Theorem (Hajnal and Szemeredi 
1970, Bollobás 1978), the vertex set of the graph L can be partitioned into 
Δ(Ι/) + 1 independent sets of the above order, which correspond to matchings 
Mi, i = 1,..., Δ(ί,) + 1, in S. Note that for each i, \Mt\ > \S\/2(A(L) 
+ 1). 
If X > λ + ί, then, by simple averaging, there exists a matching Mi such 
that |[Γρ]·η.Μ4| > P°\Mi\ + t\Mi\/\S\. 
Since I N T I M I is a random variable 

THE UPPER TAIL 
49 
with the binomial distribution Βϊ(|ΛΊί|,ρβ), we conclude by Theorem 2.1 that. 
< ( Λ ( Ι ) + 1 ) β χ ρ ( - 4 ( Δ ( ; . ) + ' ; ) ( Λ + 1 / 3 ) ) . 
■ 
As our next example shows, Proposition 2.44 can be applied to quite sparse 
families S. 
Example 2.45. Let 5 be the family of all edge sets of triangles in the com-
plete graph [n]2. Then Δ(Ι(5)) = 3(ra-3), and by Proposition 2.44, for fixed 
p > 0 and with X — Χκ3 denoting the number of triangles in G(n,p), 
P(A" >(l + p)EX)= 
0(n)exp 
(-Θ(η2ρ3)) , 
which is a fair bound provided p » n~ 2/ 3 log1'3 n. 
The next idea also uses disjoint sets to force independence, and is based on 
Spencer (1990) (see also Alon and Spencer (1992) and Janson (1990b)). Let 
Sp be the subfamily of S consisting of those sets which are entirely contained 
in Γρ. Consider also the intersection graph Lp = L(SP) of Sp, in which each 
vertex represents one set and the edges join pairs of vertices representing pairs 
of intersecting sets. (Clearly, Lv is an induced subgraph of L defined above.) 
Let X0 count the largest number of disjoint sets of S which are present in 
Γρ. So, X is the number of vertices and Xo is the independence number of 
Lp. Furthermore, set T = Δ(Ζ,Ρ) and X\ for the size of the largest induced 
matching in Lp. Then it follows by an elementary graph theory argument 
that X < X0 + 2TX\ (Exercise!). Hence, if ad hoc estimates can be found for 
both T and X\ then it remains only to bound the upper tail of X0. 
Lemma 2.46. Ift > 0, then, with<p(x) = (l + x ) l o g ( l + x ) - x and X = EX, 
«X. 
> E X + 4 < exp ( - A , ® ) < exp ( - j j j ^ ) ■ 
Proof. Let k be an integer (0 < k < [λ + ί]) and consider the number Z of 
fc-element sequences of disjoint sets of Sp. Clearly, we have 
E Z < | 5 | V * = A*. 
If *o > λ + t, then Z > (λ + t)k = Π*=Γο ( λ + * - 0 · a n d t h u s . bY Markov's 
inequality (1.3), 
¥(Xo >X + t)<?(Z> 
(λ + ί)4) < jr^-r- 
= Π 
Τ Τ 7 — " 
(λ + t)k 
AX Λ + t - ι 

50 
EXPONENTIALLY SMALL PROBABILITIES 
If we increase k by 1, the right-hand side is multiplied by λ/(λ +1 - k), which 
is less than 1 for k < t. Hence, the right-hand side is minimized by choosing 
k — \t\. Consequently, 
logP(X0 > λ + i) < Σ 
log(A/(A + t-i))< 
/ 1οε(λ/(λ + t-x)) 
dx, 
i=o 
Jo 
which implies the first inequality of Lemma 2.46 by a straight-forward inte-
gration. The second inequality follows by the lower bound on ψ(χ) established 
at the end of the proof of Theorem 2.1. 
■ 
Remark 2.47. By choosing k = λ + t (assumed to be an integer) in the 
above proof, we obtain the weaker estimate (Erdós and Tetali 1990, Alon and 
Spencer 1992), valid for any integer k > 0, 
Remark 2.48. Note that Xo satisfies the Lipschitz condition (L) of Corollary 
2.27 with all c¿ = 1, as well as the condition (C) of Theorem 2.29 with 
φ(τ) = s\r]. Therefore, similar but weaker versions of Lemma 2.46 can be 
derived from these results (cf. Theorem 3.29 in Section 3.5). 
Example 2.49. Let X = Χχ3 be the number of triangles in G(n,p), np -> 
oo. Then Χχ is the maximum number of edge disjoint copies of the diamond 
graph K± (see Figure 4.7), and Lemma 2.46 can be applied to it. The random 
variable T can easily be bounded by using Theorem 2.1. For example, if p > 0 
is fixed and np2 -t 0, one can prove (Exercise!) that 
Ρ(Λ· > (1 + p)EX) < 2e- e ( nV) + n
2e- e< 1/ n p 2). 
This bound is meaningful if n*ps -+ oo, and better than that of Example 2.45 
if p = o(n-3/5) (Exercise!). 
Remark 2.50. Using a variant of the martingale approach, very recently Vu 
(2000+) has proved that for a class of graphs G on k vertices, the inequalities 
exp{-0 ((EXG)2/*logn)} < f{XG > (1 + p)EXG) 
< exp {-©((EXG) 1""- 1))} 
hold in a wide range of p = p(n). Here XQ is the number of copies of G in 
G(n,p). 
The lower bound provides another counterexample to the existence of an 
upper tail analogue of Theorem 2.14 (cf. Remark 2.17). The upper bound 
competes well against the results of this section. In particular, when G = K3, 

THE UPPER TAIL 
51 
Vu's bound is better than that in Example 2.49, but for p » n ^ 3 it is not 
as good as the bound presented in Example 2.45. 
Our final idea for establishing a bound on the upper tail of X incorporates 
some kind of cheating. We allow ourselves to delete some elements of Γρ 
and claim the concentration of X in the remainder. Surprisingly, such results 
turned out to be useful and even crucial in the context of partition properties 
of random graphs. This approach was developed in Rödl and Rucinski (1995) 
and is based on two elementary lemmas. We conclude this section with these 
technical lemmas. The first of them can easily be proved by a method similar 
to that used in the proof of Lemma 2.46 (Exercise!). 
Lemma 2.51. Let S C [Γ]' and 0 < p < 1. Then, for every pair of integers 
k and t, with probability at least 1 - exp ( - ,/*+tv J, there exists a set E0 C Γρ 
of size k such that Γρ \ Eo contains at most \ + t sets from S. 
■ 
Hence, substantially exceeding the expectation is exponentially unlikely, 
provided we are allowed to destroy some of the subsets in the count, by deleting 
a certain number of elements from the random set. Then, of course, there is 
the danger of losing other properties held by the random set. It turns out, 
however, that monotone properties held with exponential probabilities survive 
the deletion. The next lemma, also from Rödl and Rucinski (1995), makes 
this precise. 
For an increasing family Q of subsets of a set Γ and for a nonnegative 
integer k, let 
Qk = {A C Γ : VB C A, if |B| < k, then A \ B e Q}. 
In other words, given a property Q, the property Q* assures that Q holds 
even after deleting up to A: arbitrary elements from the set. For instance, if 
Q is the graph property of being connected, then Qk is the property of being 
{k + l)-edge-connected. 
Lemma 2.52. Let c and δ < 1 satisfy 
<5(3-logá) < c . 
(2.45) 
Then, for every increasing family Q = Q(N) of subsets of an 
N-element 
set Γ, and for every k, 0 < k < δΝρ/2, 
ί/Ρ(Γ ( 1_ ί ) ρ ¿ Q) < e~cNp 
then 
Ρ(Γ„ i Qk) < 3SRpe-WVNP 
+ e-(*2/8)JVP. 
B 
Thus, if a random binomial subset has an increasing property with proba-
bility extremely close to 1, then a slightly enlarged random subset will enjoy 
the same property, and with similar probability, even after a small fraction 
of its elements are arbitrarily destroyed. The elementary proof is left to the 
reader (Exercise!). 
In Chapter 8, in the outline of the proof of Theorem 8.23, we indicate 
how Lemmas 2.51 and 2.52 were utilized to establish thresholds for Ramsey 
properties of G(n,p). In the same proof, Lemma 2.52 alone is also applied. 

3 
Small Subgraphs 
3.1 
THE CONTAINMENT PROBLEM 
In 1960 Erdös and Rényi published the most fundamental of their random 
graphs papers (Erdös and Rényi 1960). The first problem studied there was 
that of the existence in G(n, M) of at least one copy of a given graph G. Since 
the graph G is fixed and the random graph G(n, M) grows with n -► oo, copies 
of G in G(n, M) are called small subgraphs, regardless whether G is a triangle 
or a graph with one billion vertices, as opposed to subgraphs of G(n, M) which 
grow with n like, say, a Hamilton cycle. 
Erdös and Rényi (1960) found the threshold for the property of containing 
G only in the special case in which G is a balanced graph (see Section 3.2 for 
the definition). Twenty-one years later Bollobás (1981b) settled the problem 
in full generality. Still later a simpler proof was found by Rucinski and Vince 
(1985) and we will present it here. It is a classical example of an application of 
the commonly used methods of the first and the second moment. This problem 
is also instructive in that it shows that the behavior of the expectation alone 
can be sometimes misleading. 
To better comprehend this feature and to have a gentle start, we will con-
sider first the somewhat simpler problem of finding the threshold for the 
containment of at least one arithmetic progression of length k in a random 
subset of integers [n]p, where, recall, [n] = {1,2,... ,n} and p = p(n) is the 
probability of including each element of [n], independently of the others, in 
the random subset [n]p. 
53 
Random Graphs 
by Svante Janson, Tomasz Luczak and Andrzej Rucinski 
Copyright © 2000 John Wiley & Sons, Inc. 

54 
SMALL SUBGRAPHS 
The first and second moment methods 
Before presenting the applications, let us describe the first and the second 
moment methods. As a special instance of the Markov inequality (1.3), for 
every non-negative, integer valued random variable X, the inequality 
P ( X > 0 ) < E X 
(3.1) 
holds. The first moment method relies on showing that EX n — o(l), and 
thus concluding by (3.1) that Xn = 0 o.o.s. The second moment method is 
based on Chebyshev's inequality (1.2), which implies (Exercise!) that for every 
random variable X with E X > 0 
VarX 
(EX)2 
P(*=0><7FVvF· 
(3·2) 
Hence, by showing that the right-hand side of (3.2) (with X replaced by Xn) 
converges to 0, one concludes that X„ > 0 a.a.s. By the same token one 
obtains a stronger statement, which also follows from Chebyshev's inequal-
ity: If VarXn/(EX„)2 = o(l) then Xn = EX„ + op(EX„) or, equivalently, 
X„/EX n A 1. In particular then, Xn = ©c(EX„). 
Remark 3.1. Inequality (3.2) may be improved. The Cauchy-Schwarz in-
equality applied to X = X1[X Φ 0] yields (EX)2 < EX 2P(X Φ 0), that 
is, 
P(X ¿0)> 
^ | | £ , 
(3.3) 
and thus 
T(X 
0 1 - 1 
iEX)2 
V a r * 
V a r * 
(34) 
P ( X _ 0 ) < 1 
Ε χ 2 
- 
Ε χ 2 - ( E X ) 2 + V a r X · 
(dA) 
For the purpose of showing X„ > 0 o.o.s., (3.2) is just as good as the improve-
ment (3.4), but in Chapter 7 we will see a situation where the improvement 
is essential. 
Example 3.2 (arithmetic progressions). Let X* be the number of arith-
metic progressions of length k in [n]p, where k > 2 is a fixed integer. (We 
suppress the subscript n here.) To compute E(X*) we need to know the num-
ber /(n,fc) of all arithmetic progressions of length k in [n]. In fact, we only 
care about the order of magnitude of f(n,k) which equals n2, since every 
arithmetic progression is uniquely determined by its first two elements. Let 
us number the arithmetic progressions of length k in [n] by 1,..., /(n, k) and, 
for each i = l,...,f(n,k), 
define a zero-one random variable (indicator) U 
equal to 1 if the i-th arithmetic progression of length k is entirely present in 

THE CONTAINMENT PROBLEM 
55 
[n)p and equal to 0 otherwise. With this notation, Xk = £;_"' 
^» aní*> by 
the linearity of expectation, 
E(Xk) = f(n,k)pk 
= 
e(n2pk). 
Hence, if p < n _ 2 /* then E{Xk) -> 0 as n -4 oo and, by the first moment 
method, (i.e. by (3.1)), P(Xk > 0) = o(l). 
If, on the other hand, p !» n~2/*, then E{Xk) -»· oo, but this fact alone 
is not sufficient to claim that F(Xk > 0) -»· 1. One has to work for it, using 
the second moment method. Observe that U and /, are independent if the 
i-th and j-th arithmetic progressions have no element in common; in that 
case the covariance Cov(/¿, /,) equals zero. In the remaining cases, we use the 
inequality Cov(/¿,/j) < E(J¿Jj). There are 0(n3) 
pairs (/,,/>) which share 
one element and then E(/¿Jj) = p 2* - 1, and only 0{ri2) pairs which share two 
or more elements, in which case E(/j/,-) < p*. We thus can estimate the 
variance of Xk as follows: 
/(n.fc) /(n,fc) 
Varpf*) = Σ 
Σ 
Covt/i./i) = 0(n3p2k~l 
+n2pk). 
Consequently, by the second moment method, (i.e., by (3.2)), if p » n - 2/*, 
then 
r(xk = o) = o (— + - l j ) = o(i). 
\np 
n2pk) 
Together, these results show that the threshold for existence of a fc-term arith-
metic progression in [n]p is n~2lk. 
Thresholds for subgraph containment 
Returning to small subgraphs of random graphs, we let XQ stand for the 
number of copies of a given graph G that can be found in the binomial random 
graph G(n,p). Let v = VQ and e = eo stand for the number of vertices and 
edges of G, respectively. There are exactly f{n,G) 
= (")v!/aut(G) copies 
of G in the complete graph Kn, where, recall, aut(G) denotes the number 
of automorphisms of G. For each copy G' of G in Kn define the indicator 
random variable I& = l[G(n,p) D G']. Then 
E(XG) = f(n,G)p< = 0 ( n V ) -> ( ° 
!Í 
1 co if 
p<£n-v'e 
p^>n-v'e 
and, by the first moment method, 
P(XG > 0) = o(l) 
if p « n-v/e. 
(3.5) 
Is it then true that V(XG > 0) = 1 - o(l) if p » n - ^ e ? Consider first an 
example. 

56 
SMALL SUBGRAPHS 
Example 3.3. Let H0 be the graph with 4 vertices and 5 edges and let Go 
be a graph obtained by adding one vertex to i/o and connecting it to just one 
vertex of i/o- (There are two nonisomorphic ways to do so, and it does not 
matter which one we choose - see Figure 3.1 for one version of Go·) Take 
any sequence p = p(n) such that n - 5 / 6 < p « n _ 4 / 5, say, p = n _ 9 ^ u . Then 
EXo0 
— Θ{η5ρ6) -> oo, but by (3.5) applied to Ho, a.a.s. there is no copy of 
Ho in G(n,p), and therefore there is no copy of Go either. 
Hence, things are more complicated for graphs than for arithmetic progres-
sions. It should be clear at this point that the behavior of the expectation 
is deceptive in case of Go, because Go contains a subgraph (viz. Ho) denser 
than itself, and that the right threshold should be n - 4/ 5. Indeed, this was 
confirmed by Bollobás (1981b) in the following, general result. 
Recall that m(G) is the ratio of the number of edges to the number of 
vertices in the densest subgraph of G, that is, 
m(G)= max { — : H C G, vH > θ | . 
(3.6) 
Theorem 3.4. For an arbitrary graph G with at least one edge, 
= i° ifl 
\ l 
ifl 
lim P(Gín,p) D G ) = 
IP * 
_ 1 / m ( G )' 
Proof. There are two statements to be proved, the O-statement, and the 1-
statement. To prove the former one, assume that p 4C n'1!™^ 
and let H' 
be a subgraph of G for which e(H')/v(H') 
= m(G). 
Then, by (3.5), o.o.s. 
there is no copy of H', and thus, no copy of G in G(n,p). 
To prove the 1-statement we use the second moment method; we then need 
to bound the variance of Xa from above. For future reference, we state the 
result as a lemma. We define 
Φσ = Φσ(η,ρ) = min{E(X//) : H C G, eH > 0} 
(3.7) 
and note that 
Φ σ χ 
min n » V ; 
(3-8) 
HCG,CH>0 
this quantity will be useful on several occasions in the sequel. 
Lemma 3.5. Let G be a graph vAth at least one edge. Then 
Var(XG)x(l-p) 
£ 
n2va-vHp2e0-eH 
HCG,eH>0 
x ( l - p ) 
max 
_ , ' = (1 - p ) — 
, (3.9) 
V 
y'HCG,r„>0 
EXH 
Φθ 

THE CONTAINMENT PROBLEM 
57 
where the implicit constants depend on G but not on n or p. In particular, 
VarXa = 0({EXG)2/$G), 
and if p - p(n) is bounded away from 1, then 
VarXG - ( E X G ) 2 / * C 
Proof. Using the fact that IG· and IG·· are independent if E{G')C\E{G") = 0, 
and that for each H CG there are Θ(ην"η21ν°-υ»ϊ) 
- Q(n^o-vH) 
p a i r s 
(G',G") of copies of G in the complete graph Kn with G' Π G" isomorphic 
to H, we have 
Var(XG) = Σ 
Cav(Ia·, ¡a··) = 
Σ 
^
' / G" > " E< / G' > E ( / G" M 
G',G" 
E(G')n£(G")/0 
x 
Σ 
n2t,c-"H(p2<,c_eH -p 2 e°) 
« C G , Í H > 0 
x 
Σ 
n2v°-VHf?ea-'-(\-p). 
(3.10) 
HCG,eH>0 
The simple observation below is often useful. 
Lemma 3.6. The following are equivalent, for any graph G with eG > 0. 
(i) npm<G) -¥ oo. 
(ii) nv"pe" -> oo /or every H CG with VH > 0. 
(iii) E(XH) -*■ oo for every H CG with vH > 0. 
(iv) φ σ -> oo. 
Proo/. By (3.6) (and p < 1), (i) holds if and only if npe"lv" -> oo for every 
H CG with υ// > 0; since 
EX„ x nv"pe" = 
{npe«lv")VH, 
this is equivalent to both (ii) and (iii). Finally, by the definition (3.7), Con-
dition (iv) is equivalent to E{XH) -> oo for every H CG with e# > 0; this is 
equivalent to (iii) since the case VH > 0 and e« = 0 is trivial. 
■ 
To complete the proof of Theorem 3.4, we observe that if p » n_l/m^G', 
then by Lemma 3.6 Φσ -*· oo. Consequently, (3.2) and Lemma 3.5 yield 
P(G(n,p) 2 G) = F(XG = 0) < l ^ f f i = 0(1/Φσ) = o(l). 
■ 
Remark 3.7. It follows from the above proof that if Φσ(η,ρ) -> oo, then 
not only P(G(n,p) # G) -> 1 but further XG/E(A"G) A 1. 

58 
SMALL SUBGRAPHS 
Go 
F0 
Fig. 3.1 
A non-balanced graph and a balanced extension of it. 
Remark 3.8. As we mentioned before, Erdös and Rényi proved Theorem 3.4 
already in 1960, but only in the special case of balanced graphs, that is, graphs 
G with m(G) = ec/vG, 
while the general case was proved much later by 
Bollobás (1981b). Another approach to the general case was suggested by 
Karonski and Rucinski (1983b), who proposed a "deterministic" argument to 
derive Theorem 3.4 from the Erdös and Rényi result for balanced graphs. It 
was based on a conjecture proved a few years later (Gyori, Rothschild and 
Rucinski 1985, Payan 1986) that every graph G is a subgraph of a graph F, 
called a balanced extension of G, which is balanced and, what is crucial here, 
is not denser than the densest subgraph of G, that is, m(F) = ep/vF = m(G). 
The validity of the l-statement of Theorem 3.4 for F implies its validity for 
G, since, trivially, F{XG > 0) > V(XF > 0). 
An example of a balanced extension F0 of the graph Go is presented in 
Figure 3.1. (The smallest balanced extension of a graph G can be much 
larger than G; see Rucinski and Vince (1993).) 
An exponential rate of decay 
Analyzing the proof of Theorem 3.4, one comes to the conclusion that a de-
cisive role is played by the quantity Φβ· Indeed, the two parts of the above 
proof can be combined into one pair of inequalities 
1 - Φ σ < P(G(n,p) 25 G) < 0(1/Φ σ), 
(3.11) 
which together imply Theorem 3.4, since Φ σ -» oo if and only if npm<-G) -+ oo 
by Lemma 3.6, and by a similar argument Φβ -ν 0 if and only if npm<>C) -» 0. 
What is especially nice about the inequalities (3.11) is that both sides can be 
strengthened to an exponential rate of decay. 
Theorem 3.9. Let G be a graph with at least one edge. Then, for every 
sequence p = p(n) < 1, 
exp j - γ - ^ — Φ σ | < P(G(n,p) 0 G) < βχρ{-θ(Φ σ)}. 
Proof. The left-hand inequality follows immediately from Corollary 2.13, a 
consequence of the FKG inequality, with X replaced by Xw, where E(X«-) = 

THE CONTAINMENT PROBLEM 
59 
Φο- The other inequality is implied by Theorem 2.18(ii) with S = XG and 
the /yi's replaced by /G'S. Indeed, the denominator of the exponent there 
becomes 
Σ 
Σ 
Σ 
P2ec"e"=e((EA'c.)V*G) 
(3.12) 
HCG,eH>0 
G' 
G"nG'=H 
and the right-hand inequality in Theorem 3.9 follows. 
■ 
Note that Theorem 3.9 implies Theorem 3.4. 
Remark 3.10 (Martingale approach). There are at least two other ways 
to deduce the right-hand inequality of Theorem 3.9; they are based, respec-
tively, on the martingale and Talagrand inequalities given in Chapter 2. Here 
we present the martingale approach and the other one will be given in Sec-
tion 3.5. 
We confine ourselves to the special case in which for every proper subgraph 
H of G with eH > 0, E(XH) 
» *σ· In particular, Φ σ = E{XG)- 
(The 
general case is quite involved and we refer the reader to Janson, Luczak and 
Rucinski (1990).) Let / = οΦσ for a suitably chosen constant c > 0, and 
let π = (Ai,..., A/) be an arbitrary partition of the set [n]2 into sets of size 
| A¿| < n 2 / / , i = 1,..., / . Two copies of G are called π-disjoint if for each 
index i at most one of them has an edge in Aj. Let D „ G be the maximum 
number of π-disjoint copies of G in G(n,p). Then, by Corollary 2.27, 
P(XG = 0) = P(D, t 0 = 0) < Ρ(|2?.,σ - t\ > t) < 2exp{-i 2/2/}, 
where t = E(Dn<a) < $σ· Now we need to bound t from below. Let YV¡G be 
the number of ηοη-π-disjoint pairs of copies of G. Clearly, D„,G > XG - Yn.G, 
and so t > Φα — E(Y*,G)- 
When computing E(Y„,G) 
asymptotically, we may 
ignore all pairs of G sharing at least one edge, as their expectation is ο(Φσ). 
The expected number of the other ηοη-π-disjoint pairs is 
° (f(n22f)n2{VO~2)P2e°) = °<*o//>· 
Hence, for sufficiently large c, t = Ω(Φσ) and the right-hand inequality of 
Theorem 3.9 follows. 
The uniform model G(n, M ) 
In this section we consider the containment problem for the uniform ran-
dom graph G(n,M). We note first that by Corollary 1.16 (or Remark 1.18), 
Theorem 3.4 immediately implies the corresponding result for G(n, M): the 
threshold is n 2 - 1/ m( G>. (This can also be shown directly by the first and 
second moment methods.) 
For the exponential bounds in Theorem 3.9, the situation is somewhat 
more complicated. Not only do neither part of the proof of Theorem 3.9 carry 

60 
SMALL SUBGRAPHS 
over to G(n, M), but the result cannot be true in general. Indeed, for dense 
graphs Turin's theorem (see, e.g., Bollobás (1998)) shows, for example, that 
if G = Kz and M > \n2 ~ 5(2), then G(n, M) always contains a copy of 
G, so P(G(n, M) ~fi G) = 0. More generally, by the Erdös-Stone-Simonovits 
Theorem (Erdös and Stone 1946, Erdös and Simonovits 1966, Diestel 1996, 
Bollobás 1998), the same holds for any graph G and for M > c("), provided 
c > 1 - l/(x(G) - 1) is fixed and n is large enough. However, if M is not too 
large, both inequalities in Theorem 3.9 have counterparts for G(n, M). 
For a sequence M = M(n) < (!}), define Φα by (3.7) with p = Μ / β ) , and 
note that (if G is non-empty) Φ σ < E(X K j) = {"2)p = M. 
Theorem 3.11. Let G be a graph with at least one edge. 
(i) // M >eG, 
then 
P(G(n,M) ¡Z5 G) < εχρ{-θ(Φ σ)}. 
(ii) //, in addition, either Φο < °M', where c is some small positive constant 
depending on G, or G is not bipartite and M < c("), where c < 1 — 
1/(X(G) - 1) is fixed, then 
P(G(n,M) 0 G) = 
exp{-Q(<bG)}. 
Proof. We will give several arguments which are valid for different ranges 
of M and together yield the results. We let ci,c2, -.. denote some positive 
constants depending on G only. Note that we may assume that n is large, 
since the results are trivial for any finite number of small n. 
(i) For Φσ 3> logn, the estimate in (i) follows immediately from the upper 
bound in Theorem 3.9 and Pittel's inequality (1.6). 
Alternatively and more generally, we find by monotonicity and the law of 
total probability, as in (1.5), 
P(G(n,p/2) DG)< 
P(G(n, M) D G) + P(e(G(n,p/2)) > M) 
and thus, using the Chernoff bound (2.7) (or (2.5)), Theorem 3.9 and the fact 
that Φσ(η,ρ/2) > 2_<!(σ)Φσ(η,ρ), we get 
P(G(n,M) 75 G) < P(G(n,p/2) 2 G) + 
e~M/e 
< e-c,4>c(n,p/2) 
+e-M/6 
<e-Ci*o +e-M/6 
( 3 1 3 ) 
Since M > Φσ as remarked above, (3.13) yields the upper bound 
2e~C3*°, 
which yields the sought bound e - e ' * c \ provided Φσ > a = I/C3, say. 
For Φβ < Ci, (3.13) implies further 
P(G(n, M) 75 G) < 1 - cb$G + e" M / 6, 

THE CONTAINMENT PROBLEM 
61 
which implies the result provided M > log2 n and thus e _ M / 6 < ^ο^Φα for 
large n; note that Φ<·> > n~2ea for M > 1. 
Finally, in the rather uninteresting case in which ea < M < log2 n, we 
assume for simplicity that every component of G has at least three vertices. 
(The general case follows easily by treating isolated edges and vertices sepa-
rately. - Exercise!) It is then easy to see that if we let XQ be the number of 
copies of G in G(n, M), then E(Xa) x nVGpe° = Φ σ and E{XG) ~ 
E(Xa), 
and thus, by (3.3), 
P(G(n, M) 2 G) = P(XG = 0) < 1 - οβΦο < e x p { - c ^ G } . 
(ii) To obtain a lower bound, we argue similarly. By monotonicity, 
P(G(n,2p) 2 G) < P(G(n,M) j> G) + P(e(G(n,2p)) < M) 
and thus, using Theorem 3.9 and the Chernoff bound (2.6), 
P(G(n,M) 75 G) > e~C7*° - 
e~M,i. 
If 1/2 < Φο < cgM, say, this yields the desired lower bound e _ C 9* c. 
If Φ 0 < 1/2, let H0 be a subgraph of G with EXHo = * G (in G(n,p)), 
and observe that then EXn0 < $ G (in G(n,M)). Hence, 
P(G(n,M) D G) < P(G(n,M) D H0) < EXHo < *G < 1 - e c , 0* c. 
For the remaining case in which Φσ > c%M, and thus Φσ = Θ(Μ), the 
above approach may be useless. Note that in this case, the lower bound 
ε-θ(η ρ) m Theorem 3 9 c a n D e obtained by just considering the event that 
G(n,p) is empty, which, of course, does not happen in G(n,M), 
M > 1. 
Fortunately, a simple and entirely different argument still yields the desired 
lower bound for graphs which are not bipartite. Let k = x(G) > 3, where 
X(G) is the chromatic number of G. Clearly, if G(n,M) is (k - l)-partite, 
then there is no room for a copy of G. Let us fix a partition of the vertex 
set [n] into k - 1 sets of size [n/(k - 1)J or \n/(k - 1)]. It is easy to show 
(Exercise!) that, as long as M < c(£), the probability of no edge of G(n, M) 
falling within any of the sets is at least (1 - l/(& - 1) — c)M. 
■ 
For non-bipartite graphs G, we thus have an almost complete description: 
P(G(n,M) ~t¡ G) = εχρ(-Θ(Φσ)) almost all the way up to the point where 
the probability becomes zero by the Erdös-Stone-Simonovits Theorem. 
For bipartite graphs, the result is less satisfactory. Clearly, the final ar-
gument in the proof above does not work. The condition Φσ < cM in the 
theorem is equivalent (Exercise!) to M < c'n2_1/m<2)(G), where m'2)(G) is 
defined in (3.18) in the next section, and for larger M we have no precise 
description of P(G(n, M) ~t> G). Indeed, for a general bipartite G, it is not 
even known when this probability vanishes. 

62 
SMALL SUBGRAPHS 
In fact, it is conjectured that if G is bipartite and M » n 2~ 1 / m 
, G ), then 
the probability that G(n, M) contains no copy of G tends to 0 with n faster 
than in the binomial case, and we have 
-logP(G(n,M) 75 G) = o(M). 
(3.14) 
This conjecture has been verified for cycles C4 by Füredi (1994) (see also 
Kleitman and Winston (1982)) and for all even cycles C2*, k > 2, by Haxell, 
Kohayakawa and Luczak (1995). For further results in this direction see Ko-
hayakawa, Kreuter and Steger (1998) and Luczak (2000), where it is shown, 
using a slightly generalized version of the Szemerédi Regularity Lemma (cf. 
Section 8.3), that (3.14) holds for all bipartite graphs G for which Conjec-
ture 8.35 of Chapter 8 holds. 
3.2 
LEADING OVERLAPS AND THE SUBGRAPH PLOT 
Leading overlaps 
When p = p{n) -> 0, the logarithm of the lower bound in Theorem 3.9 becomes 
asymptotically equal to -Φο· When can the same be concluded about the 
upper bound? To answer this question we introduce a related concept. 
A subgraph H' of G with e«- > 0 is called a leading overlap of G (for a given 
sequence p{n)) if liminf Ε(Χ//-)/Φσ < 00. In other words, H' is a leading 
overlap if and only if E(X//<) » Φσ does not hold. If we, for simplicity, 
assume that the sequence p(n) is sufficiently regular, so that WmE(X 
H)/$G 
exists in [l,oo] for every if C G , this is equivalent to E(XH·) 
— θ(Φσ); in 
other words, a leading overlap is a subgraph of G, which, up to a constant, 
achieves the minimum in Φσ = min« EX//· (For general p(n), this holds at 
least along a suitable subsequence.) 
Leading overlaps owe their name to the fact that they correspond to the 
leading terms of the asymptotic expression (3.9) for the variance of XG as 
well as in (3.12). 
Returning to Theorem 3.9, a detailed analysis of expression (3.12) reveals 
that the coefficient hidden in the Θ term in the upper bound in Theorem 3.9 
becomes l-o(l) if there is just one leading overlap H' of G, and the uniqueness 
holds in the strong form, that is, there is just one copy of H' in G. (The 
converse holds if we assume that p(n) is regular as above.) 
Thus, we arrive at the following corollary. 
Corollary 3.12. If p = p{n) -> 0 in such a way that H is a unique leading 
overlap of G then 
log P(G(n, p)J>G)~- 
E(H) . 
(3.15) 
■ 

LEADING OVERLAPS AND THE SUBGRAPH PLOT 
63 
e 
Éi 
i 
1
2 
3 
4 
5
" 
Fig. 3.2 A graph and its subgraph plot. 
Subgraph plot 
The leading overlaps vary with p = p{n), but there is an easy geometrical way 
to detect them all at once, by plotting in the xy-plane points that represent 
subgraphs of G and focusing on the upper boundary of the convex hull of the 
obtained set of points. 
Formally, the subgraph plot of a graph G is defined as the set of points in 
the iy-plane 
Σ(0 
= {(vH,eH) 
: H CG, v„ 
>2). 
Remark 3.13. Note that we do not include (0,0) or (1,0); for other purposes 
it might be convenient to define versions of the subgraph plot containing one 
or both these points. Similarly, it may be natural to consider only induced 
subgraphs H in the definition; for our purposes this makes no difference. 
We call the upper boundary E(G) of the convex hull of T,{G) the roof, and 
we say that a subgraph H lies on the roof if the point (VH, en) does. Observe 
that ¿(G) is a piecewise linear curve, with endpoints in (2,1) and (υσ,ββ). 
An example of a graph and its subgraph plot is presented in Figure 3.2. 
Elementary calculations yield a full description of the entire spectrum of 
leading overlaps of G. If, for simplicity, G lacks isolated vertices, then a 
subgraph H of G is a leading overlap of G for some range of p = p(n) if and 
only if it lies on the roof £(G) of the subgraph plot E(G). Moreover, the 
range of p = p(n) in which if is a leading overlap is determined by the slopes 
ajj and a# of the straight line segments to the left and to the right from 
{VHICH)- 
(Note that a^ > α^; we set α^ = oo and a j = 0 for convenience.) 
Indeed, H is a leading overlap as long as npa» = 0(1) and, at the same time, 
npa» = Ω(1); this condition is necessary too if p(n) is regular as above. (If 
G has isolated vertices, and Go is the subgraph obtained by removing them, 
then the possible leading overlaps are the subgraphs on the roof S(Go), which 
equals S(G) without its final, flat part where aj¡ = O.) 

64 
SMALL SUBGRAPHS 
As one can see in Figure 3.2, the points (s, rnaxVH=s e//), s = 3 , . . . , VG - 1, 
do not necessarily lie on the roof. In fact, there are graphs with only two roof 
subgraphs, Ki and G. They are easily characterized, assuming VG > 3, by 
the condition that for all H C G with 2 < VH < VG the inequality 
tJL^\ 
< e
~ 
(3.16) 
VH - 2 
vG - 2 
v 
' 
holds. On the other extreme, there are graphs G with as many as about 
\VG of their subgraphs being leading overlaps for various (mutually distinct) 
ranges of p. For this and other related results, see Rucinski (1991) and Luczak 
and Rucinski (1992). 
Measures of graph density 
The subgraph plot can also be used to visualize several other useful concepts. 
First, the density d(G) = CG/VG oiG (with VG > 0) equals the slope of the 
line Ld from (0,0) to the top point 
(να,εβ)· 
The maximum density m(G) = max{d(H) : H C G, VH > 0} equals the 
slope of the least steep line Lm from (0,0), such that the entire subgraph plot 
lies below or on Lm; in other words, Lm is the tangent from (0,0) to the roof. 
A graph G is called balanced if m(G) = d{G), that is, if d(H) < d{G) for 
every H C G. (In words: G does not contain a subgraph denser than itself.) 
This is equivalent to Ld = Lm, and thus G is balanced if and only if the 
subgraph plot lies below or on Ld- In Example 3.3, Ho is balanced and Go is 
not. 
A graph G is called strictly balanced if d(H) < d(G) whenever H Q G, 
which is to say that every proper subgraph of G is strictly less dense than the 
graph itself; equivalently, the subgraph plot lies strictly below L¿t except for 
the top point. Trees, regular connected graphs as well as the graph H0 from 
Example 3.3 are all strictly balanced. An example of a balanced graph that is 
not strictly balanced is the union of a cycle and a path (of length > 1) which 
are disjoint except that one endpoint of the path lies on the cycle. Another 
example is given by the disjoint union of two copies of any balanced graph, 
or by any balanced extension of a non-balanced graph (c/. the graph FQ in 
Figure 3.1). 
We will further use some related notions, which are natural, for example, 
when considering graphs with a distinguished vertex or edge. For a graph G 
with vG > 2, define SX\G) 
= eG/{vG ~ 1); let d<l>(A"i) = 0. Then define 
m(1)(G) = max{d ( 1 )(tf):ffCG}. 
(3.17) 
When va > 2, él*>(G) and m^^G) are the slopes of the line Ld
1] from (1,0) 
to the top point, and of Lm\ 
the tangent from (1,0) to the roof. 

LEADING OVERLAPS AND THE SUBGRAPH PLOT 
65 
Similarly, for a graph G with vG > 3, define d(2)(G) - {ea - \)/{vG - 2); 
let d ( 2 )(#i) = d^(2Kl) 
= 0 and S2\K2) 
= 1/2. Then define 
m[2){G) = max{d(2)(#) :HCG). 
(3.18) 
The definition of S2)(K2) 
may look artificial but turns out to be convenient 
(cf. Remark 3.14 below). Note that if eG > 2, then m(2)(G) = max{d(2)(#) : 
H CG, VH > 3}, so the special case does not matter. 
When eG > 2, d(2){G) and mS2^{G) are the slopes of the line L(j] from 
(2,1) to the top point, and of Lm , the tangent from (2,1) to the roof. 
In analogy with the above, a graph G is called Κχ-balanced if mSx^(G) = 
d(1)(G), or equivalent^, d^{H) 
< ^{G) 
for all H CG; furthermore, graphs 
with dll)(H) 
< d^{G) 
for all H C G are called strictly Ki-balanced. Anal-
ogously, we define K2-balanced and strictly K2-balanced graphs. These no-
tions have applications in the study of solitary subgraphs (see Section 3.6), 
G-factors (see Chapter 4), and Ramsey properties of random graphs (see Sec-
tion 7.6 and Chapter 8). 
Remark 3.14. Below we collect some simple but useful facts about the pa-
rameters m(G), mW(G) and m^(G). 
The proofs are left to the reader (Ex-
ercise!). 
For convenience, let m^(G) 
= m(G). 
Let Gi,...,G* be the connected 
components of G. Then m^(G) 
= maxj m ^ G j ) for i = 0,1,2. This implies 
that strictly balanced, strictly Ki-balanced, and strictly /(^-balanced graphs 
are all connected. 
We have m^(G) 
= 0 if and only if G is empty, that is, eG = 0, for every 
i. Moreover, m(G) < 1 if and only if G is a forest (and then m(G) = 1 - 1/e, 
where s is the order of the largest component), and m(G) = 1 if and only if 
the densest component of G is unicyclic. For all other graphs, m(G) > 1. 
As far as m(1)(G) is concerned, m^(G) 
= 1 if G is a non-empty forest, 
and m^^G) > 1 if G is not a forest. 
Finally, m(2)(G) = 1/2 when the maximum degree A(G) = 1 (i.e., when G 
consists of isolated edges and possibly some isolated vertices), m^2)(G) = 1 if 
G is a forest with A(G) > 2, and m^(G) 
> 1 if G is not a forest. 
In Chapter 6, we will use the following observation. 
Lemma 3.15. Ifnpm^ 
-» oo, then every leading overlap is connected. 
Proof. Suppose that H CG with e« > 0. If H is the disjoint union of two 
proper subgraphs H\ and H2, where, say, e(Hi) > 0, then Lemma 3.6 yields 
nv(H2)pe(H2) 
_> OQ a n d 
t h u s 
nv»pe" 
= η«(">)ρ'(">)ηυ(^)ρβ<^) > η·(»ι)ρ«("ι> ~ E{XH,) 
> Φσ· 
Consequently, H is not a leading overlap. 
■ 

66 
SMALL SUBGRAPHS 
Remark 3.16. For eG > 2, the slope a£ 3 defined above equals 
m(2)(G). 
Hence, under this condition, Ki is a leading overlap when npm 
(G) = Ω(1), 
and the only leading overlap when npm 
^G^ -» oo. 
Remark 3.17. Assume VQ > 3. Then the condition (3.16) characterizing 
graphs G with only two roof subgraphs (viz. K2 and G) may be expressed as 
d ( 2 )(#) < d(2)(G) for all H ς G with vH > 3; this is equivalent (Exercise!) 
to G being strictly ^-balanced, except for the two cases G = 2K2 and G 
being a union of an edge and an isolated vertex. Consequently, if G lacks 
isolated vertices, then G has only two possible leading overlaps if and only if 
G is strictly ^-balanced or G = 2Ki. 
Remark 3.18. The arboricity of a graph is defined as the least number of 
forests that together cover the edge set of the graph. This seemingly unrelated 
notion is, in fact, closely connected to the quantities just defined; by a theorem 
of Nash-Williams (1964) (see e.g. Diestel (1996)), the arboricity of G equals 
rm<»>(G)l. 
3.3 SUBGRAPH COUNT AT THE THRESHOLD 
When p = 0 ( n _ 1 / m ( G ) ) , we have Φ σ = θ(1) and, by Theorem 3.9, 
0 < liminf P(G(n,p) D G) < limsupP(G(n,p) D G) < 1. 
n-Hx> 
n-K» 
This ensures that the threshold in Theorem 3.4 cannot be sharpened. For this 
range of p = p(n) the derivation of limn_Kx, P(G(n,p) D G) may not be easy. 
However, for the class of strictly balanced graphs defined in the preceding 
section, not only the precise value of limn_»oo P(G(n>p) 3 G), but the entire 
limiting distribution of XG can be computed. 
The following result was proved independently in Bollobas (1981b) and 
Karonski and Rucinski (1983a), and generalizes earlier results about trees 
(Erdös and Rényi 1960) and complete graphs (Schürger 1979). 
Theorem 3.19. If G is a strictly balanced graph and np™^ -* c > 0, then 
XG -► Ρο(λ), the Poisson distribution with expectation λ = c" c/aut(G). 
Proof. This proof exemplifies the technique called the method of moments, 
which is presented in detail in Chapter 6; we use here the version given in 
Corollary 6.8. 
Consider the factorial moments of XG, defined as E(Xc)k 
= E[XQ{XG 
-
1) · · ■ {XG - k + 1)]. We have, for * = 1,2,..., 
E(XG)*= Σ 
Ρ(/σι···/σ. = 1) = ££ + ££', 
Gi 
Gk 

SUBGRAPH COUNT AT THE THRESHOLD 
67 
where the summation extends over all ordered fc-tuples of distinct copies of G 
in Kn, and E'k is the partial sum where the copies in a fc-tuple are mutually 
vertex disjoint. It is easy to verify (Exercise!) that 
E'k ~ (EXG)k 
~ (c" c/aut(G))*. 
This implies that XG is asymptotically Poisson if E'¿ = o(l), and it remains 
to be proved that E'k' = o(l). Let et be the minimum number of edges in a 
t-vertex union of k not mutually vertex disjoint copies of G. 
Claim. For every k>2 and k <t < kvo, we have et > tm(G). 
Proof of Claim. For a graph F define fp = m{G)vF - ep. Note that fa = 0 
and, since G is strictly balanced, /// > 0 for every proper subgraph H of G. 
We are to prove that for every graph F which is a union of k not mutually 
vertex disjoint copies of G, ¡F < 0. We will do it by induction on A:, relying 
heavily on the modularity of / , that is, on the equality 
/ F , U F 2 = / F , + ÍFt - ÍFxnFi 
(3.19) 
valid for any two graphs F\ and F 2. Let F = U«=i £*»> where each G, is a 
copy of G, and the copies are numbered so that G\ Π G<i Φ 0. For k = 2, 
(3.19) yields /G,UC 2 = -/σ,ησ 2 < 0, because G\ Π G2 is a proper subgraph 
of G. For arbitrary k > 3 we let F' = [j^ 
G¿ and H = F' Π G*. Then H 
may be any subgraph of G including G itself and the null graph, but in any 
case fa > 0. Moreover, fp> < 0 by the induction assumption. Thus 
fF = fF, + fGk -fH<0. 
■ 
Having proven the claim, we easily complete the proof of Theorem 3.19 
with one line. Indeed 
kv-l 
E'¿= £0(ny«) = o(l). 
■ 
t=k 
Remark 3.20. Using Theorem 6.10, Theorem 3.19 can be extended to joint 
convergence of several subgraph counts, with the limit variables independent 
(Exercise!). 
Still assuming that p = 0(n - 1/ m ( G , )), consider graphs other than strictly 
balanced graphs. If G is nonbalanced, then the expectation of XQ tends to 
infinity. It turns out that there is a nonrandom sequence a„(G) -»· 00, such 
that the asymptotic distribution of Xa/an{G) 
coincides with that of XH, 
where H is the largest subgraph of G for which d(H) = m{G). Clearly, H is 
balanced and we are back to the balanced case. The sequence an(G) is equal 
to the expected number of extensions of a given copy of H to a copy of G in 
the random graph G(n,p). For details, see Rucinski (1990). 

68 
SMALL SUBGRAPHS 
If a graph G is balanced but not strictly balanced, then the limiting dis-
tribution of XG is no longer Poisson. Although, in principle, as shown by 
Bollobas and Wierman (1989), the limiting distribution can be computed, 
there is no compact formula. We give only three simple examples, illustrating 
typical phenomena. 
Example 3.21. We consider three balanced but not strictly balanced graphs. 
All three have m(G) = 1, and thus we assume p = c/n for some c > 0. 
First, if G = 2C3, a union of two disjoint triangles, then a.a.s. XG = 
i X c 3 ( * c 3 - l ) (Exercise!). Since XCi Λ Z3 € Po(c3/6) by Theorem 3.19, and 
continuous functions preserve convergence of distribution (Billingsley 1968, 
Section 5), we obtain XG -* \Zz(Z$ — 1). In particular, for the probability 
of no copy of G, ?(XG = 0) -> (1 4- c 3/6)exp(-c 3/6). 
Second, if G is a disjoint union of a C3 and a C4, then a.a.s. XG = Xc3Xct ■ 
By Remark 3.20, (XC3,Xc<) 
-+ {Z3,Z4), 
with Z3 6 Po(c?/6) and ZA e 
Po(c4/8) independent. Consequently, XG -* Z3Z4. In particular, Ρ(ΛΌ = 
0) -> 1 - (1 - exp(-c 3/6))(l - exp(-c 4/8)). 
Third, if G is the whisk graph Kf, 
that is, a triangle with a pendant edge 
(see Figure 3.3), then XG -* Σ ^ ι W«> w n e r e W< € Po(3c) are independent 
of Z3 € P o ^ / ö ) and of each other. In particular, F(XG = 0) -> exp(-(l -
e _ 3 c)c 3/6). The idea behind this is that, asymptotically, there is a Po(c?/6) 
distributed number of triangles, and each triangle has a Po(3c) distributed 
number of pendant edges, each creating one copy of K$. 
For details, see 
Bollobás and Wierman (1989) or Janson (1987). 
Finally, let us mention that for p » η~ι^"ι(·α\ 
XG has an asymptotic 
normal distribution (Theorem 6.5). 
3.4 
THE COVERING PROBLEM 
The next topic covered in this chapter deals with covering every vertex of a 
random graph by a copy of a given graph G. The graph property that every 
vertex belongs to a copy of G will be denoted throughout this chapter and 
Chapter 4 by COVc- If G contains an isolated vertex (and n > VG), then, 
trivially, the property COVc coincides with the presence of a copy of H, where 
H is obtained from G by removing one isolated vertex. Since this property 
has been discussed before, throughout this section we will be assuming that 
the minimum degree of G is at least 1. 
For a particular vertex i € [n], there are possibly several positions it may 
take in a copy of G which covers it. For the purpose of classifying them, 
let us introduce the notion of a rooted graph {v,G), where G is a graph and 
v € V(G) is the root. For example, there is only one (up to isomorphism) 
rooted version of A3, while the whisk graph K% enjoys three nonisomorphic 
rooted versions (see Figure 3.3). 

WE COVERING PROBLEM 
69 
© 
* 
l 
Fig. 3.3 Three rooted versions of the whisk graph; the roots are indicated by open 
circles. 
For a rooted graph (i>,G), with vG > 1, let d(v,G) = eG/(vG - 1) and let 
m(v,G)= 
maxι 
d(v,H). 
(Thus, d{v,G) = d(1)(G) does not depend on v, but m(v,G) does, in general.) 
A rooted graph {v,G) is called balanced if d(v,G) = m(v,G) 
and strictly 
balanced if d(v, H) < d(v, G) for every proper subgraph H of G containing 
the vertex v. For instance, among the three rooted versions of Kf only one is 
strictly balanced, while the other two are not balanced (Exercise!). Note that 
a graph is strictly K\ -balanced if and only if all its rooted versions are strictly 
balanced (Exercise!). In particular, all cycles and complete graphs have only 
one rooted version, and that is strictly balanced. 
For i € [n] and v G V(G), let Ui{v) be the number of copies of (v,G) 
contained in the random graph G(n,p), in which vertex i takes the role of 
the root v, and let (/< be the total number of copies of G containing i. Then, 
similarly to the problem of containment of ordinary subgraphs, p = 
n~l^m^v,a^ 
is the threshold for the property uUi(v) > 0" (Rucinski and Vince 1986) and 
consequently, p = η-ι/"»η„€ο ">(».C) ¡s the threshold for the property uUi > 0" 
(Exercise!). 
For instance, as soon a s p » τι-3''4, a fixed vertex, say vertex 1, a.a.s. 
belongs to a copy of Kf, 
but only when p » n~2/3, 
does it belong to a 
triangle. 
However, we are mainly interested in the random variable 
i 
which counts the vertices of G(n,p) not covered by copies of G; hence COVG 
is equivalent to "WQ = 0". Theorem 3.22 below provides thresholds for the 
events COVo which, of course, depend on the structure of G. 
For a graph G, let m. = min„6c m(v, G) and M(G) = {v 6 V(G) : 
m(v, G) = m . } . For a vertex υ € M (G) let Cv be the family of all subgraphs H 
of G which contain v and satisfy the conditions d(v, H) = m. and NH(V) Φ 0, 

70 
SMALL SUBGRAPHS 
the latter condition just saying that v is not an isolated vertex in H. Further, 
let sv = min//ecv e(H), with sv = oo if C„ = 0, and s = max r € M ( C) s„. 
Finally, set a = |M(G)|/aut(G). 
Theorem 3.22. Let G be a graph with minimum degree at least 1. 
(i) If for every v € M(G) the rooted graph (v,G) is stnctly balanced, then 
hm P(G(n,p) 6 COVG) = I 
/ 
6 
n-*°° 
yl 
if anv°-lpeG 
- logn -> oo. 
Moreover, if anv°~lpea 
- logn -» c, -oo < c < oo, i/»en 
WG 4 Po(e _ c), and hence P(G(n,p) e COVc) -> exp(-e _ c). 
(ii) If s < oo, then there exist constants c, C > 0 suc/ι i/iot 
lim P(G(».p) € COVG) = i ° 
^ < c ( l o g n ) ^ / - , 
(iii) If s = oo, ίΛβη 
lim P(G(»,p) 6 COVG) = ( ° 
V / * " " ' / " · 
It is easy to check that the assumption in (i) is a special case of that 
in (ii), with m» = d^(G) 
and s = eG (Exercise!). In Case (iii), which is 
the complement of (ii), the parameter m. coincides with m{G) appearing 
in Theorem 3.4 (Exercise!); hence the threshold for covering by copies of G 
coincides with the threshold for existence of any copy at all. 
Remark 3.23. Note that the nicer the structure of G, the sharper thresh-
old one can prove. Indeed, in Case (i), a~l/e° (logn) 1/ e cn - l/ m· is a sharp 
threshold. In Case (ii), it follows from Theorem 1.31 that there exists a sharp 
threshold, although we do not know it exactly. By (ii) above, the sharp thresh-
old is of the form &(n)(logn)1''5n_1/m· for some b(n) with c < 6(n) < C; it 
is reasonable to conjecture that b(n) is a constant, but at present we cannot 
rule out the possibility that it oscillates somehow. 
In Case (iii), in contrast, the threshold is coarse. In fact, if p = cn~llm" 
for 
any fixed c > 0, and H is a minimal subgraph of G such that d{H) = m{G), 
d 
then Theorem 3.19 shows that XH -> Ρο(λ) for some λ < oo, and thus 
V{XG = 0) > ¥{XH =0)-¥ 
e~x > 0, so P(G(n,p) € C0VG) ■/> 1. 
Part (i) was proved independently by Rucinski (1992a) and, in a slightly 
disguised form, by Spencer (1990). We will present the proof of part (i) only. 
The proofs of the other two parts follow from more general results by Spencer 

THE COVERING PROBLEM 
71 
#
#
# 
L\ 
Li 
L$ 
Fig. 3.4 The lollipop graphs. 
(1990) on extension statements; see the end of this section. Before presenting 
the proof of (i), we give a few examples. 
Example 3.24. The graphs K3 and K% have strictly balanced rooted ver-
sions and, by Case (i) of Theorem 3.22, the respective thresholds for the 
properties COVK3 and COVK+ are (logn)1/3?!-2/3 and (logn) 1/ 4^- 3^, re-
spectively. In particular, for (logn)1/'4n-3/4 «C p(n) <C n - 2 / 3 a.a.s. every 
vertex belongs to a copy of Kf, but since there are only o(n) triangles, most 
vertices take the "off-triangle" position. 
Example 3.25. The threshold for COV*, equals n _ 2/ 5log 1 / I 0n by Theo-
rem 3.22(i). Consider now the lollipop graphs Lr obtained from a clique K5 
by attaching to it a path Pr (see Figure 3.4). 
Let t denote the vertex of degree one (the tail vertex). The lollipop Lx has 
m = 11/5 and M = {t}, and the rooted graph (t,Li) 
is strictly balanced; 
thus Case (i) applies. For Lj. we have m = 2 and again M = {t}, but this 
time the rooted graph (t, L2) is balanced, but not strictly balanced. Moreover, 
Ct = {L2}, so s = st = e(L2), and thus Case (ii) applies. Hence, the thresholds 
for covering every vertex of G(n,p) by copies of L\ and L2 are, respectively, 
n-s/^logn) 1/ 1 1 and n - ^ l o g n ) 1 / 1 2 . 
Finally, consider lollipops with r > 3. We have m = 2 and t € M, but this 
time the only pair {t,H) which achieves d(v,H) 
— m is such that H is the 
clique ΛΓ5 together with the tail vertex t, which is isolated in H. Thus Ct = 0, 
st = 00, Case (iii) applies, and the threshold for COV¿r coincides with that 
for the existence of Ä5, that is, n - 1/ 2. In other words, as soon as copies of K5 
begin to appear in G(n,p), every vertex is at distance at most three from one 
of them. This particular observation follows also from the known fact that 
the threshold for diameter three is n 
-2/3 l o gi/3 n (BoiioW« 1985, Chapter X), 
which is well below the threshold for existence of K$. 

72 
SMALL SUBGRAPHS 
Proof of Theorem 3.22{\). We use a mixture of the second moment method 
and the correlation inequality of Theorem 2.18(i). By the monotonicity of the 
property COVQ we may assume that nv°~1pea 
= 0(logn). Since for every 
i; $ M(G), F(Ui(v) > 0) = o(l), the decisive role in covering the vertices of 
G(n,p) is played by the rooted versions (v,G), where v G M(G). Let {VJ}, 
i = 1,...,/, be a maximal collection of vertices of M(G) for which the rooted 
graphs (VJ,G) 
are pairwise nonisomorphic. Set Üi = 5Z<=i U*(vi) anc* observe 
that 
n 
^
^ 
= |Μ|Χ 0. 
i=l 
Hence, by symmetry, E(t/i) = ^ 
E{XG) = anv°-lpec 
+ o(l). 
Further, for each v $ M(G), choose a minimal subgraph Hv C G containing 
v such that d(v, Hv) = m(v,G) 
> m., let U*(v) be the number of copies of 
(υ,Ηυ) 
in G(n,p) rooted at i, and let U¡ = ¿v*Af(G) U¡{v). Then Et/,* = 
o(l). 
Since E(WC) = n F(U\ = 0), we need a sensitive asymptotic for P(f/i = 0). 
Note that Üx + U[ = 0 implies C/j = 0. Thus, by Corollary 2.13 we have 
P(f/, = 0) > Ψψι + U{ = 0) > e- E( & 1 + l /n/(i-p) = e-E(t7,)+c(i) 
For an upper bound, let S be the family of all edge sets of rooted copies of 
{VJ,G), j = 1,...,/, in the complete graph Kn with vertex 1 as the root. For 
each A £ S, we set 
IA = 
l[ACG(n,p)}. 
We then have 
Σ 
Σ 
E(Wß) = o(En2"°"'"1p2ec-t 
A Β^Α,ΒΓιΑφϊ 
\(»,t) 
= 0 
| η 2 ν 0 - 2 ρ 2 β 0 ^ η - ( . - 1 ) ρ - Λ 
\ 
(».*) 
/ 
where s and t represent, respectively, the number of common vertices and 
edges of a pair of two copies of G, each rooted at a vertex of M{G) and both 
containing vertex 1 as the root. Such an intersection is a proper subgraph of 
G containing a vertex v e M(G) and hence, by the fact that (v,G) is strictly 
balanced, we always have 
t 
eG 
s — 1 
VQ — 1 
Thus, in our range of p(n), n'~1pi 
> ne for some ε > 0. This together with 
Theorem 2.18(i) implies that 
P(£/i = 0) < ?{0i = 0) < e - ^ 0 · ) ^ ' 1 ) . 

THE COVERING PROBLEM 
73 
Hence, 
E{WG) = ηΨ{υ1 = 0) = ne-E{0i)+o{l) 
= ne-«nVi'"Vc+·«1). 
(3.20) 
If an°c~lpea 
- logn -► oo, then E(WG) = o(l) and, by the first moment 
method, ?(WG > 0) = o(l). On the other hand, if anVa~lpe° -logn -► -co, 
then E(WG) -> oo and we apply the second moment method to WG- TO this 
end, as WG is a sum of mutually dependent indicators, it is convenient to 
express the variance of WG in the form 
Var(WG) = E(WG(WG - 1)) + E(WG) - (E(WG))2. 
We have 
E{WG{WG - 1)) = n{n - 1) Ψ(ϋι = U2 = 0) < n(n - 1) P(C/i = C72 = 0) 
and, by another application of Theorem 2.18(i) (this time to the family of all 
edge sets of copies of (VJ,G), j = 1,...,/, rooted at 1 or 2), 
P(ft =í/ 2=0)<e- 2 E ( í )' ) + o ( 1 ). 
Altogether, 
nw°-0)^¡ma)?- 
WWGW + mo)-l^°-
Similarly one can prove that when anVG~lpea — logn -¥ c, and thus 
E{WG) -> e~c by (3.20), the fc-th factorial moment E[WG(WG - 1) · ·· {WG -
k + 1)] of WG converges to e~ck for every k > 1. This proves, by Corol-
lary 6.8, that WG converges to the Poisson distribution with expectation e~c. 
Alternatively, one could apply here Stein's method (c/. Theorem 6.24). 
■ 
Extension statements 
Spencer (1990) considers a related problem with some applications to the 
zero-one laws for random graphs discussed in Chapter 10. 
Let R = {vi,... ,vr} be an independent set of vertices in a graph G. The 
pair (R,G) will be dubbed a rooted graph. For |ñ| = 1 this is the notion 
introduced at the beginning of this section. We say that a graph F satisfies 
the extension statement Ext(i?,G), or briefly, F € Ext(ii,G), if for every r-
tuple R' = {v[,...,v'r} 
of vertices of F there is a copy of G in F with v'j 
mapped onto u,, j = 1,..., r. 
Example 3.26. If G = K2 and |ñ| = 1, then F 6 Ext(Ä, G) means that there 
are no isolated vertices in F. If G = K3 and \R\ = 1, then F 6 Ext(iZ,G) 
is equivalent to F € COVG- The same is true for every vertex-transitive 
graph G; more generally, if R = {v}, then F € Ext(iZ.G) means that every 

74 
SMALL 
SUBGRAPHS 
vertex in F belongs to a copy of G where it corresponds to v. If G = P\, and 
ñ is the set of the endpoints of P*, then F € Ext(/Z, G) says that every pair 
of vertices of F is connected by a path of length k. 
We will now define notions which are straightforward generalizations of 
the case (v,G) treated above. For a rooted graph (R, G), with r — \R\, let 
d(R,G) = eG/(va 
- r) and let 
m(R,G) 
= 
max 
d(R,H). 
H RCHCG 
The rooted graph (R, G) is called balanced if for every subgraph H of G, such 
that V{H) D R, we have d(R,H) 
< d(R,G), 
and strictly balanced if this 
inequality is strict for all proper subgraphs H of G, such that V(H) D R. 
As a generalization of the families Cv appearing before Theorem 3.22 we 
now define a subgraph H containing R to be primal if d(R, H) = m{R, G) 
and grounded if at least one of vi,..., 
vT is not isolated in H. We let SR be 
the smallest number of edges in a grounded primal subgraph H, with SR = oo 
if no such subgraph exists. Finally, let 61 be the number of automorphisms 
of G that fix every element of fl, and let 02 be the number of permutations 
of R that can be extended to some automorphism of G. Then, the following 
results hold; for the proof we refer to Spencer (1990). 
Theorem 3.27. Let G be a graph with minimum degree at least I, and let 
R ψ$ be an independent set of vertices in G. 
(i) // the rooted graph (R, G) is strictly balanced, then 
lim P(G(n,p) 6 Ext(fcG)) = { ° 
*"""?? 
~ 
^ ^ " 
-¥ 
- 0 0 
- > OO. 
Moreover, ifnvo~rp'G 
- birlogn -* c, -00 < c < 00, then P(G(n,p) G 
Ext(Ä,G)) -*·βχρ(-ε-0/6'/&2) 
(ii) If SR < 00, inen iAere exist constants c, C > 0 sucA ίΛοί 
(iii) If SR = 00, t'n w/itcn case m(R,G) — m(G), 
then 
i U » P ( G ( » . p ) € E r t ( i i , G ) ) S = | 1 
^ > > n - 1 / m ( G ) . 

DISJOINT COPIES 
75 
The 1-statements in parts (ii) and (iii) (with \R\ = 1) immediately imply 
the corresponding statements in Theorem 3.22. This is not so for the 0-
statements, but the 0-statements in Theorem 3.22(ii, iii) follow by the same 
proofs as for the corresponding 0-statements in Theorem 3.27 given in Spencer 
(1990). (For the 0-statement in (iii), this is just applying Theorem 3.4.) 
Example 3.28. For a graph G, one may ask what the threshold is for the 
property that for every vertex of G(n,p) the subgraph induced by its neigh-
borhood contains a copy of G. Let G + K\ be the graph obtained by joining 
a new vertex w to every vertex of G. Then this property is equivalent to 
Ext({w},G + K\). For a strictly balanced (in the ordinary, unrooted sense) 
graph G, the rooted graph (w,G + K\) is strictly balanced and, by The-
orem 3.27(i), the desired threshold is (logn) 1/( ,' c + e c )n-" G / (" c + e c ) (which 
coincides with the threshold for COVO+JC,) (Exercise!). 
3.5 
DISJOINT COPIES 
In this section we consider a problem which will be further developed in Sec-
tion 4.2. The question we address here is: How many disjoint copies of a given 
graph G are there in a random graph G(n,p)? As the disjointness may be 
meant with respect to vertices or with respect to edges, we define two ran-
dom variables DG and DG equal to the cardinality of the largest collection of 
vertex- and edge-disjoint copies of G, respectively. Trivially, DG < DG < XG, 
but also DG < De
H for every non-empty subgraph H of G and DG < DV
H for 
every non-null subgraph H of G. Define 
Φν
α(η,ρ) = Φ£ Hf min{E(Xw) :HCG,v„>Q} 
= min(<I>G,n), 
denote Φ^ = Φο, where Φο was defined in Section 3.1, and observe that 
Φ& -i- oo if and only if Φ^ -» oo (c/. Lemma 3.6). We know from Section 3.1 
that when <t>G -> oo, then XH 
= ©c(EX w) for H C G (since Φ'α -> oo 
implies Φ% -* oo) and thus DG = Oc($G) 
and De
G = 00{Φα). 
In fact, the 
above quantities provide the correct orders of magnitude for the two random 
variables in question. 
Theorem 3.29. If Φβ -> oo, then DG = QC($G) 
and DG = 
Qc{$e
G). 
Proof. The proof below is a slight modification of that from Kreuter (1996) 
and relies on the second moment method. 
Consider first the vertex case and define an auxiliary graph Γ with vertices 
being the copies of G in G(n,p) and edges connecting pairs of copies with 
at least one vertex in common. Thus, vr = XG and ep = £ F XF, where 
the sum is taken over all unions F = Gi U Gi of two copies of G sharing at 
least one vertex. Also, any independent set of vertices in Γ corresponds to a 
vertex-disjoint collection of copies of G in G(n,p). Hence, it follows from the 

76 
SMALL SUBGRAPHS 
Turan Theorem (see, e.g., Berge (1973, p. 282)) that 
In view of this, all we need to show is that 
"MW) 
(322) 
for every union F of two vertex-intersecting copies of G. 
For convenience, set * F = nVFpeF and note that EX/? = θ(Φρ). The 
reason we prefer to use Ψ/r rather than EX// is the log-modularity property 
holding for arbitrary graphs Fx and ί^. Note also that Φ// = 0(EX//) = 
Ω(Φ&) if H C G and uH > 0. 
Now assume that F = Gi UG2, where Gi and G2 are two copies of G, and 
let H = Gi Π G2 have υ« > 0. Then 
■x,.e(t,).e(Ä)-0(Ä). 
In order to bound XF by the same quantity as EXf we will apply Cheby-
shev's inequality. For this we need to estimate the variance of XF, which, by 
Lemma 3.5, is of the order Φ/-./Φ/?. To bound, in turn, Φ'ρ from below, as-
sume that L C Fwithe¿ >0andlet£< = LnGj.i = 1,2. Then L = LiUL2, 
LuH = (L!U/í)U(L2Ui/),and(LiUír)n(L2Ui/) = H. Two applications 
of the log-modularity of Φ yield 
ΦLuH^LDH 
^LiUH^L2VH^LnH 
Φ/ = 
Φ Η 
Φ?/ 
Here Lx U H, L2 U H and L Π H are all subgraphs of G. Thus, if vLne > 0, 
then* t = n((^) 3/*3r)· 
In the special case vmH = 0, the graphs L\ and L2 are disjoint and at least 
one of them is non-empty. Assume that e¿, > 0. Then, taking into account 
that Φ/,2 > Φ£ -> oo if vi7 > 0 and *¿ 2 = 1 otherwise, we obtain 
Consequently, 
Φ £ = Φ £ Ι Φ £ , > Φ Λ , 
= Ω ( ^ - ) · 
ΦΡ=πηηΕΧ/, = Ω ( ^ Λ 
et>o 
v φ» y 
and, using the log-modularity of Φ again, 
*™-'(©-°G&)-'G&) 

VARIATIONS ON THE THEME 
77 
Hence, by Chebyshev's inequality and the assumption that Φα -> oo, 
P (xF 
> E XF + | | ) = 0{1/*V
G) = o(l), 
which proves (3.22) and completes the proof of Theorem 3.29 in the vertex 
case. 
The proof for DG follows along the same lines. But instead of repeating the 
same argument for the edge case, we present an alternative approach involving 
Markov's and Talagrand's inequalities. 
Building the auxiliary graph Γ in a similar way with the obvious modifi-
cation that now the edges of Γ join edge-intersecting copies of G in G(n,p), 
we have, by Markov's inequality (1.3), that with probability at least | , ep < 
4E(e r). We have 4E(e r) < cx{EXG)2l^G, 
for some cx > 0, and thus, by 
(3.21) modified to the edge case, and by the fact that XG = 0 c ( E X o ) . there 
is another constant c2 > 0 such that DG > C2$G with probability at least, 
say, | . 
Now, using Talagrand's inequality, we will convert \ to 1 - o ( l ) as required. 
As we are heading toward an application of Theorem 2.29, let us define Zi to 
be the indicator of the presence of the i-th edge of the random graph G(n,p), 
i = 1,2,..., N = (j). Then DG - f(Zx ,...,ZN), 
where the function / clearly 
satisfies the Lipschitz condition (L) with all Cj = 1. The other assumption of 
Theorem 2.29, Condition (C), holds with the function φ{τ) = ear for integer 
r > 0 (and thus with φ(τ) = βσΓΓ1 *°
Γ a n v r e al r > 0)· Indeed, for any 
integer r, and for any graph F containing r edge-disjoint copies of G, choose 
J to be the index set of all ear edges belonging to these copies. Then any 
other graph coinciding with F on the given edges contains r edge-disjoint 
copies of G too. Therefore, by (2.35) (c/. Example 2.33), for 0 < c3 < c2 and 
with t = (c2 - 
c3)$c, 
F(D'a < α3Φα) < 2Y{DG < **'a)*(Dh 
> Γ<*Φ&1) 
*
 2 6 X P{-
( C 2 ίαζζ°)2 } = «Φ<-»(«&» = 4Β· ■ 
Note that the last bound in the proof provides yet another proof of the 
right-hand inequality of Theorem 3.9. Indeed, 
P(XG = o) = f(De
G = 0) < ?{D'0 < c3<bG). 
3.6 
VARIATIONS ON THE THEME 
There are several other properties related to that of containing a copy of G. 
One such property is the containment of at least one induced copy of G in 
the random graph G(n,p). Another variation is counting only those copies of 
G which are vertex disjoint from all other copies of G contained in G(n,p). 

78 
SMALL SUBGRAPHS 
Fig. 3.5 The only solitary triangle in this graph is drawn in bold. 
Below we call them solitary (see Figure 3.5). Finally, we consider a special 
case of solitary subgraphs: the isolated copies of G. 
Induced subgraphs 
Let us denote by YG the number of induced copies of G in G(n,p). As we will 
see in Chapter 6, for p constant, the behavior of YG may significantly differ 
from that of XG- However, for p = o(l) they are asymptotically the same. 
Here we only explain why the event "YG > 0" has the same threshold as the 
event "XG > 0". The 0-statement of Theorem 3.4 holds for induced copies 
simply because YG < XG- Let JG· be a zero-one random variable equal to 1 
if G', a copy of G in K„, actually becomes an induced copy of G in G{n,p). 
For an application of the second moment method, observe that, assuming 
p = o(l), 
E ( J G · ) = P e c(l - p ) ( v ? ) - e o ~ E(JG<) 
and, consequently, 
E{YG) ~ 
E{XG). 
Moreover, for any two copies G' and G" of G which share at least one edge 
Cov(JG>,JG") < E{JG'JG-) 
< WG-IG") 
~ Cov(/G», JG..), 
while for any two copies with at most one vertex in common, Cov( JQ· , Jc) 
= 
0. Finally, for any two edge-disjoint copies sharing t vertices, where t > 2, 
COV(JG·, 
JG··) < P2e°, and the number of such pairs is 0{n2va~l). 
Hence, as 
in the case of ordinary subgraphs, we have (Exercise!) 
There is another way of deducing the above fact. Assume again that p = 
o(l). By Markov's inequality, ¥(XG -YG > \E(XG)) 
= o(l) and so a.a.s. 
YG > XG - J E ( A G ) · On the other hand, we know that when «$G -+ o°, 
XG/E(XG) 
A 1 and, in particular, a.a.s. XQ > |Ε(Χσ)· Hence, a.a.s. 
V G > | E ( X G ) > 0 . 

VARIATIONS ON THE THEME 
79 
The presence of an induced copy of G is not a monotone property (except 
in the trivial cases in which G is either complete or empty). It is not even 
convex (Exercise!), however, it has a second (disappearence) threshold toward 
the end of the evolution of G(n,p). In terms of q = 1 - p, it corresponds to 
the threshold for "XG° > 0" in the complementary random graph G(n,q), 
where Gc is the complement of G. Hence, the second threshold is roughly 
1 - 0(n-1/m<G'>) (Exercise!). 
Solitary subgraphs 
Let us denote by ZG the number of solitary copies of G. Clearly, ZQ < DG, 
where DG has been defined in the previous section. Observe that E(ZG) = 
E(XG)THG, 
where UQ is the conditional probability that a fixed copy of G is 
solitary, given that it is present in G(n,p). 
For a nonbalanced G, limn_»oo P(ZG > 0) = 0, since as soon as copies of G 
emerge, it can be seen by Theorem 2.18 that IIG is exponentially small with 
a power of n in the exponent, and hence P ( Z G > 0) < E ( Z G ) = Ε(ΛΌ)Πσ = 
o(l) (Exercise!). With some additional effort one can prove that for a balanced 
but not strictly balanced G, we have P( ZG > 0) = o(l) as soon as ¥(XG > 
0) -► 1 and thus limsupP(ZG > 0) is never equal to 1 (Exercise!). 
Let us assume now that G is strictly balanced. If p = Q(n~l^d^), 
then, as 
we showed in the proof of Theorem 3.19, a.a.s. there are no intersecting pairs 
of G at all, and so ZG = XG- In other words, when copies of a strictly balanced 
graph G first emerge, they are all solitary. This holds true even beyond the 
threshold, when npd^ 
-> oo sufficiently slowly. But the containment of a 
solitary copy is not monotone either, and with more and more edges in the 
random graph, the solitary copies of G become rarer until complete extinction 
occurs. The second (disappearence) threshold was detected for strictly K\-
balanced graphs by Suen (1990), and for a slightly larger subclass of strictly 
balanced graphs (including trees) by Kurkowiak and Rucinski (2000). It is 
determined, roughly, by the equation EXQ = ©(nlogn). 
The difficulty we are facing here is that the probablity Πσ depends on all 
pairs in [n]2 and, rather than finding an exact expression, one can only bound 
it, using results like Theorem 2.18 and Theorem 2.12. We remark that this 
problem was a motivation for Suen to develop his correlation inequality, some 
versions of which were discussed in Section 2.3. 
Isolated subgraphs 
A much simpler situation takes place when one counts the isolated copies of 
G, that is, assuming G is connected, the connected components of G{n,p) 
which are isomorphic to G. Let To count the isolated copies of G. This time 
E(TG) = E(yG)(l -pyo(n-vc) 
_ 0(n»Cp<ce-vCnp); h e n a ! ) ¡ f eQ > ^ 
w e 
have P(TG > 0) < E(TG) = o(l), and there are a.a.s. no isolated copies of G. 

80 
SMALL SUBGRAPHS 
The same is true if &G = «c and further p <C 1/n or p » 1/n. We urge the 
reader (Exercise!) to show that TQ has a limiting Poisson distribution when G 
is connected with CQ = VG and p ~ c/n, 0 < c < oo (Erdös and Rényi I960); 
see Example 6.29. 
It remains to consider connected graphs G such that ec < VG, that is, 
trees. These are the only small graphs which a.a.s. become components of a 
random graph. Instead of focusing on a single tree, we will count all trees of 
a given order at once. Let T„ denote the number of all v-vertex isolated trees 
in G(n,p), v = 1,2, — Then, provided np2 -*■ 0, 
Ε(Γ„) = fnV"~V"1(l -ρΓ,η-")+(;)-υ+1 ~ !^nV 
\V/ 
VI 
υ-lg-wnp 
This quantity converges to a constant if either rfp"'1 -> c > 0 or vnp — 
logn - (v - 1) log log n -> c G (-00,00). The following result, proved al-
ready by Erdös and Rényi (1960), asserts that these two conditions determine 
two thresholds for the property Tv > 0. (See also Theorem 6.38 and Exam-
ple 6.29.) 
Theorem 3.30. Let c„ = vnp - logn - {v - 1) log logn. Then 
or c„ -» 00, 
Í0 
i / n y - ' - ) 0 
\l 
i / n V " 1 -> oc 
W > 0 ) 
,, ._._.., 
. ^ ^ , ^ . ^ 
Moreover, if ηνρυ~ι 
- t e e (-00,00) or c„ -+ c > 0, ί/ien Tv -> Po(A), 
where λ = limn-»«, E(T„) € (0,00). 
■ 
The case v = 1 is special here. The random variable ΤΊ is the number 
of isolated vertices in G(n,p) and nvpv~x 
— n. 
Hence there is only one 
threshold. Furthermore, the log logn term drops out and we arrive at the 
following corollary. 
Corollary 3.31. Let cn = np - logn and let T\ be the number of isolated 
vertices in G(n,p). Then 
P(Ti > 0) (
0 
ifcn-+ 
00, 
1 
¿/ cn -► -00. 
Moreover, if cn -> c € (-00,00), then Ti -> Po(e c). 
■ 
The proof of Theorem 3.30 follows the lines of those of Theorems 3.4, 3.19 
and 3.22 (Exercise!). Another proof will be given in Example 6.28. 

4 
Matchings 
Perfect matchings play an important role in graph theory. On the one hand, 
they find a broad spectrum of applications. On the other hand, they are the 
subject of elegant theorems. The two results which characterize their exis-
tence - Hall's and Tutte's theorems - are truly beautiful pearls of the theory. 
No wonder that Erdös and Rényi, after settling the question of connectivity 
(Erdös and Rényi 1959, 1961), turned their attention to the problem of find-
ing the thresholds for existence of perfect matchings in random graphs (Erdös 
and Rényi 1964, 1966, 1968). 
The results they obtained reveal a special feature of random graphs one 
could call "the minimum degree phenomenon." Namely, it is frequently true 
that if a minimum degree condition is necessary for a property to hold, then 
a.a.s. the property holds in a random graph as soon as the condition is sat-
isfied. This phenomenon is discussed to larger extent in Section 5.1. Here 
we just mention that as soon as the last isolated vertex disappears, the ran-
dom graph becomes connected (Erdös and Rényi (1959) and, for the hitting 
version, Bollobás and Thomason (1985)). Moreover, as we will learn later in 
this chapter (Corollary 4.5 and Theorem 4.6), provided n is even, from that 
very moment the random graph also contains a perfect matching (Erdös and 
Rényi 1966, Bollobás and Thomason 1985). 
In the first section of this chapter we present a new proof of the threshold 
theorem for perfect matchings, which is based on Hall's rather than Tutte's 
theorem. This approach was originally designed to solve the problem of perfect 
tree-matchings (Luczak and Rucinski 1991), a special instance of (7-factors, 
where one looks for a vertex-disjoint union of copies of a given graph G which 
covers all vertices of G(n,p). 
Ordinary matchings correspond to the case 
81 
Random Graphs 
by Svante Janson, Tomasz Luczak and Andrzej Rucinski 
Copyright © 2000 John Wiley & Sons, Inc. 

82 
MATCHINGS 
G — A'2. Section 4.2 collects results on G-factors and partial G-factors in 
random graphs. Finally, in Section 4.3 we present recent advances on two long-
standing open problems: finding the threshold for triangle-factors in G(n,p) 
and for perfect matchings in random 3-uniform hypergraphs, the latter known 
as the Shamir problem. 
4.1 
PERFECT MATCHINGS 
A matching in a graph can be identified with a set of disjoint edges. A perfect 
matching is one which covers every vertex of the graph. Sometimes a perfect 
matching is called a 1-factor, because it is, in fact, a 1-regular spanning 
subgraph. A necessary condition for the existence of a perfect matching in 
a graph is the absence of isolated vertices. It turned out that in a random 
graph this is a.a.s. sufficient. Because of the simplicity of Hall's condition, it 
is quite straightforward to prove this result for random bipartite graphs. 
Random bipartite graphs 
Recall that a random bipartite graph, denoted here by G(n,n,p), is the relia-
bility network with the initial graph being the complete bipartite graph Kn,n 
with bipartition (Vi,V^), |ν\| = (V^l = n. In other words, G(n,n,p) is ob-
tained from Kn>n by independent removal of each edge with probability 1 - p. 
Assume that logn — log log n < np < 21ogn and suppose that the random 
graph G(n,n,p) does not have a perfect matching. Then, by the Hall Theo-
rem, there is a set S C V¿ for some i = 1,2, which violates Hall's condition, 
that is, |5| > |7V(5)|, where N(S) is the set of all vertices adjacent to at least 
one vertex from S. Let 5 be a minimal such set. Then (Exercise!), 
(i) \S\ = \N(S)\ + 1, 
(ii) |S|<rn/2l, 
(iii) every vertex in N(S) is adjacent to at least two vertices of 5. 
Set s = \S\. If s = 1, then S is an isolated vertex. If s = 2, then S consists 
of two vertices of degree 1 adjacent to the same vertex. Let us call such a 
structure a cherry (see Figure 4.1)) and let X count cherries in G(n,n,p). 
Then 
E(X) = G(nVe-2"p) = O L» ( ^ ) ' *&\ 
= o(l), 
(4.1) 
meaning that a.a.s. there are no cherries in G(n,n,p). 
Let A denote the event that there is a minimal set 5 of size s > 3 which 
violates Hall's condition. Then, using (i)-(iii), and bounding by 
ffi 
the 

PERFECT MATCHINGS 
83 
Fig. 4.1 A cherry in a graph marked with bold lines. 
number of choices to realize (iii), we obtain 
'M)sΣC)C-l)G)*"p!*"2(l-',)■,""'M, 
<Σ(=)·(£ι)""·"-
In conclusion, the threshold for having a perfect matching coincides with that 
for the disappearance of isolated vertices. The latter can be routinely found 
(Exercise!) by the method of moments (see Corollary 3.31 and Example 6.28). 
Thus, we obtain the result of Erdös and Rényi (1964). 
Theorem 4.1. 
P(G(n, n,p) has a perfect matching) 
„-2e* 
if np — log n -¥ -oo, 
if np — log n -* c, 
» / n p - l o g n -» oo. 
Remark 4.2. Consider a random bipartite graph process {Ο(η,η,Λ/)}χί_0 
defined in analogy with the standard random graph process (Section 1.1) and 
define the hitting times τχ = min{M : ¿(G(n,n, M)) > 1} and Tpm = min{M ·. 
G(n,n, M) has a perfect matching}. (Thus, trivially, n < rpm.) 
The proof of Theorem 4.1 above yields also the stronger result that a.a.s. 
Tpm = T\. Indeed, if for convenience we instead consider the corresponding 
continuous time random graph process {G(n,n,r)}o<t<i, the same calcula-
tions as in (4.1) and (4.2) show that a.a.s. there is no minimal subset S of 

84 
MATCHINGS 
size s > 2 violating Hall's condition for any t € [(logn-loglogn)/n, 2 logn/n], 
and the result follows easily (Exercise!). 
Remark 4.3. For future reference we explicitly state the error probability 
estimate 
P(G(n,n,p) has no perfect matching) = 0(ne - n p), 
(4.3) 
which is valid for all n and p. This too follows by the argument above; the 
probabilities of having an isolated vertex or a cherry are easily seen to be of this 
order (Exercise!), while a minor modification of (4.2) shows that, assuming, 
as we may, np > log n, 
PM) < J ] e 2 , n ! , - 1 p 2 " ! e - r a p / ! = 0(n5p4e"3np/2) = 0(ne-" p). 
*>3 
Ordinary random graphs 
Let us return now to the ordinary random graph G(n,p), n even. Fixing an 
n/2 by n/2 bipartition of the vertex set and ignoring the edges within each of 
the two sets, we immediately see that Theorem 4.1 implies that G{n,p) has 
a perfect matching a.a.s. as soon as np — 2logn —► oo. We will show how to 
reduce the above value of p by half so that, again, the threshold is the same 
as for the disappearance of isolated vertices (cf. Corollary 3.31). 
In fact, we will show an even stronger result due to Bollobás and Thomason 
(1985). But first let us extend the notion of a perfect matching by saying that 
a graph satisfies property PM if there is a matching covering all but at most 
one of the nonisolated vertices. It can be routinely checked (Exercise!) that 
as soon as 2np- logn - log logn -* oo, there are only isolated vertices outside 
the giant component (cf. Chapter 5). Note that this holds already when the 
number of edges in G(n,p) is only about £n logn - roughly half the threshold 
for the disappearance of isolated vertices. 
However, the main obstacle for PM is now the presence of cherries. Two 
or more of them make it impossible. If there is exactly one cherry, PM is 
still possible, provided the number of nonisolated vertices (as well as isolated 
vertices) is odd. The expected number of cherries is 
3 M p 2 ( 1 _ p)2(n-3) < n3 p2 e-2n P +6p = 
o ( 1 ) 
if 2np - logn - 2 log logn -> oo, which also holds already when there are 
about £nlogn edges in G(n,p). Again, as proved by Bollobás and Thomason 
(1985), this trivial necessary condition becomes a.a.s. sufficient. 
Theorem 4.4. Let yn = 2np - logn - 2 log logn. 77ien 
Í
0 
if yn -+ -oo, 
(l + ^e- c)e-*'" c 
ifyn^c, 
1 
t/yn->oo. 

PERFECT MATCHINGS 
85 
Ό 
e-<-c 
1 
V 
if np — log n —> — oo, 
if np — log n —► c, 
j / n p — log 7i —> oo. 
Consequently we obtain a result of Erdös and Rényi (1966). 
Corollary 4.5. 
P(G(n,p) has a perfect matching) 
The proof below is easily modified to give the corresponding hitting time 
result too (Exercise!). 
Theorem 4.6. The random graph process {G(n, M)}M 
is a.a.s. such that 
the hitting times τι = min{M : ¿(G(n, M)) > 1} and rpm = min{M : 
G(n, M) has a perfect matching} coincide. 
■ 
The original proofs of the 1-statements of Theorem 4.4 and Corollary 4.5 
were both based on Tutte's theorem. Here we propose an alternative approach, 
via Hall's theorem, by Luczak and Rucinski (1991). This approach relies on 
the following technical lemma. Given two disjoint sets of vertices in a graph, 
the bipartite subgraph induced by them consists of all edges with one endpoint 
in each set. 
Lemma 4.7. Let np = 0(logn). For every c > 0, a.a.s. every bipartite 
subgraph, induced in G(n,p) by two sets of equal size and with 
minimum 
degree at least clogn, contains a perfect matching. 
Proof. Set u = "''"¿L"* 
· By t n e first moment method it is easy to check 
(Exercise!) that a.a.s. 
(i) for every pair of disjoint subsets of vertices of size bigger than u 
there is in G(n,p) an edge between them, 
(ii) every set S of at most 2u vertices induces in G(n,p) fewer than 
(loglogn)3|S| edges. 
The rest of the proof of Lemma 4.7 is purely deterministic. Suppose G is 
an n-vertex graph satisfying (i) and (ii), and B is a bipartite subgraph of G 
induced by the bipartition (Wi,W2), 
|Wi| = \W2\ = w, δ(Β) > clogn, but 
without a perfect matching. Then, by Hall's theorem there is 5 C W\ such 
that \NB(S)\ 
< \S\. 
Case 1: \S\ < u. 
Then \S U N B(5)| < 2u, but since all the edges with one endpoint in S 
have the other endpoint in NB(S), 
there are at least clogn|5| such edges 
a 
contradiction with (ii). 
Case 2: \NB(S)\ 
>w-u. 
Then \Wi \S\ 
< \W2 \ NB(S)\ 
< u and all the edges with one endpoint in 
W2 \NB(S) 
have the other endpoint in Wi \S - again a contradiction with (ii). 

86 
MATCHINGS 
Case 3: \S\ > u,\NB(S)\ 
<w-u. 
Now |W2 \ NB(S)\ 
> u, but there is no edge between S and W2 \ NB(S) - a 
contradiction with (i). 
■ 
Proof of Theorem 4-4- Throughout the proof we assume that | logn < np < 
21ogn and n is even. 
If J/n -*■ — oo, the second moment method yields that a.a.s. there are many 
cherries in G(n,p) (Exercise!). Since already the presence of two cherries 
makes PM impossible, the O-statement follows. 
If Vn -* c, the number of cherries has asymptotically the Poisson distribu-
tion with expectation ge~c. It can also be proved that the probability that 
there is exactly one cherry and the number of isolated vertices is odd, con-
verges to ^e - cexp(—ie - c). (Advanced Exercise! Hint: Apply the two-round 
exposure - c/. Section 1.1 - with p? so small that during the second round a 
fixed cherry cannot be destroyed.) 
If Vn -► oo, then, as was shown above, a.a.s. there are no cherries at all. 
Thus, in order to prove the two latter parts of the theorem, we have to show 
that the only obstruction for PM is the presence of either at least two cherries 
or one cherry while the number of isolated vertices is even. 
The idea of the proof is as follows: Suppose that we have a graph on n 
vertices which has either no cherries at all or exactly one cherry, but then 
the number of isolated vertices is odd. Fix an arbitrary bipartition of the 
vertex set into two halves called sides. Call a vertex bad if it has either fewer 
than 2¿o log n neighbors within its own side or fewer than 355 log n neighbors 
on the other side. Assume that the graph satisfies the hypothesis of Lemma 
4.7 (say, with c = 555) and some other properties held a.a.s. by G(n,p) {viz. 
Claim below). 
Match the bad vertices first (except for the isolated vertices, of course). If 
there is an odd number of them, leave one out. If the graph has a cherry, the 
left-out vertex must be a degree one vertex of that cherry. Remove all bad 
vertices and their partners (which do not need to be bad) from the graph. 
Then adjust the bipartition so that it becomes even again, but so that the 
minimum degree of the induced bipartite subgraph has not dropped much. 
Finally, apply Lemma 4.7 and obtain a matching covering all but at most one 
of the nonisolated vertices. 
Now we turn to the details. Let X be the number of bad vertices in G(n,p), 
and let Y € Bi(§ ,p). Then E(X) = nP(vertex 1 is bad) and, by (2.6), for 
sufficiently large n, 
P(vertex 1 is bad) < 2P(F < ^ l o g n ) < n"0,ai. 
Hence, by Markov's inequality (1.3), a.a.s. X < n4/5. 
We still need to distinguish another class of vertices of low degree. Call a 
vertex small if its degree in G(n,p) is at most three. We claim that neither 
small nor bad vertices can cling together too much. 

PERFECT MATCHINGS 
87 
Claim. For every fixed integer k, a.a.s. there is no k-vertex tree in G(n,p) 
which contains more than two small vertices or more than four bad vertices. 
Proof. Let Z e Bi(n - Jfc + l,p). It is straightforward to show (Exercise!) that 
for every fixed z 
?(Z <z) = 0((npye-n") 
= 0 ( ί ^ ) = 0(η~° 49). 
The expected number of Jfc-vertex trees containing three or more small vertices 
can now be bounded from above by 
Q
fc*~
v_l G)^
2 -
2)]3=°(
η*
ρ*~
ΐη"
ι'
47)=ο(ι)· 
Similarly, the expected number of λ-vertex trees with more than four bad 
vertices is 
0 * * ~ ν _ 1 Q)[2P(K' < ^ l o g n ) ] 5 = O i n V - U n - 0 · 2 1 ) 5 ) = o(l), 
where y " e B i ( § -k + l,p). 
■ 
As a final preparatory step, it can be easily checked that a.a.s. the maximum 
degree of our random graph is at most 81ogn (Exercise!). 
We are now ready to complete the proof of Theorem 4.4. Consider a graph 
on n vertices which has either no cherries at all or exactly one cherry, but then 
the number of isolated vertices is odd. Suppose further that this graph satisfies 
the hypothesis of Lemma 4.7 (with c = ^¡) and, for a fixed bipartition, the 
hypothesis of the above claim for k up to 11. Furthermore, assume that 
there are fewer than n4^5 bad vertices and that the maximum degree Δ is at 
most 81ogn. Note that all these properties hold a.a.s. for the random graph 
G(n,p). We will show that each such a graph satisfies property PM. 
Remove the isolated vertices and, if there is an odd number of them, remove 
one additional vertex of degree one, destroying the cherry if there is any. Order 
the remaining bad vertices by degrees, from low to high: 
deg(ui) < deg(i>2) < .. .deg(t>j). 
We will match them one by one with some vertices «i, ti2, ... , u/, always 
choosing asui a vertex of a smallest possible degree. We begin with isolated 
edges as their endpoints are matched naturally. 
Suppose v\,... ,Uj_i are already matched with u\,... 
,u¿_i (some Vj may 
be matched with some v,, but then, clearly, UJ = v, and ut = w,). Let Vj_! 
denote the set {v\,ui,...,VÍ-\,UÍ-I}. 
If Vi 6 Vj-j, it is already matched to 
some vertex; otherwise we choose Uj as follows. 
If deg(t;¿) = 1, then take as u¿ the neighbor of i>j. It is available, since we 
follow the degrees from low to high, and since there are no cherries left in the 
graph. 

88 
MATCHINGS 
Uii 
υ 2 0 
(a) 
(b) 
Fig. 4.2 Scenes from the proof of Theorem 4.4. 
If 2 < deg(u¿) < 3, then v¡ has at most one neighbor within the set Vj_i, 
since otherwise there would be three small vertices on a small tree (Exercise!). 
Thus we may choose u¿ as one of the neighbors of Vi outside V¡_i. 
If deg(rjj) > 4, then Vi has at most three neighbors within the set V¿_i, 
since otherwise there would be five bad vertices on a small tree (Exercise! -
see Figure 4.2(a)). Again, we may choose u¿ as one of the neighbors of v¿ 
outside V¿_!. 
Hence, a.a.s. all nonisolated bad vertices can be matched. The removal of 
bad vertices and their partners does not affect the degrees of other vertices 
by more than eight (Exercise! - see Figure 4.2(b)). However, the remaining 
vertices may no longer form an even bipartition. In order to apply Lemma 4.7 
we have to balance them back by moving across up to n4^5 carefully chosen 
vertices. To minimize the effect on degrees in the induced bipartite graph, we 
use for this purpose a 2-independent set of vertices, that is, an independent 
set of vertices no two of which have a common neighbor (thus, the degree of 
a vertex may drop further by at most one). Trivially, there is always such a 
set of size at least η/(Δ2 + 1) (Exercise!), which is more than is needed. The 

G-FACTORS 
89 
obtained bipartite subgraph has, therefore, minimum degree at least 
—— ben - 9 > -r—- logn, 
200 
300 
and, by Lemma 4.7, it contains a perfect matching, which together with the 
previously constructed matching {v\,u\},..., 
{u/,Uf} forms a matching cov-
ering all but at most one of the nonisolated vertices. This completes the proof 
of Theorem 4.4. 
■ 
Disjoint 1-factors 
Erdos and Rényi, after establishing a threshold for the existence of at least one 
perfect matching in the bipartite random graph G(n, n,p), went on and gener-
alized their result to the existence of at least r disjoint 1-factors in G(n,n,p) 
(Erdös and Rényi 1966). Trivially, if a graph possesses fewer than r disjoint 
1-factors, then the removal of all the edges of a maximal family of disjoint 
1-factors results in a graph with no 1-factor at all and with all the degrees 
decreased by at most r — 1. Thus, if the original graph had minimum degree 
at least r, then, after the removal, there would be no isolated vertices left, 
and Hall's condition would have to be violated in a nontrivial way. By an 
argument similar to that used in the proof of Theorem 4.1 this can be shown 
to be unlikely for G(n,n,p) (Exercise!), and it follows that the threshold for 
containing at least r disjoint 1-factors in G(n,n,p) coincides with that for 
minimum degree r. The latter can easily be found by the method of moments 
(Exercise!). The respective hitting time result holds too. 
The corresponding problem for an ordinary random graph G(n,p) was 
solved much later by Shamir and Upfal (1981) via an algorithmic approach. 
Since every even Hamilton cycle is a union of two disjoint 1-factors, the so-
lution also follows, together with the hitting time version, from a result of 
Bollobas and Frieze (1985) (see Section 5.1). However, the proof of Theo-
rem 4.4 presented above can easily be adapted to yield the threshold for τ 
disjoint 1-factors as well. It boils down to showing that a.a.s. after removing 
the edges of an arbitrary subgraph of maximum degree at most r — 1, the 
remainder of G(n,p) will contain a perfect matching. The definition of a bad 
vertex remains unchanged, while a small vertex is now one with degree at 
most r + 2. The details are left to the reader (Exercise!). It follows that a.a.s. 
the hitting time for having r disjoint 1-factors coincides with the hitting time 
for having minimum degree at least r. 
4.2 G-FACTORS 
In this section we study thresholds for containment of spanning (or almost 
spanning) subgraphs in G(n,p) which are unions of vertex disjoint copies of 
a given graph. For a graph G, every disjoint union of copies of G is called 

90 
MATCHINGS 
0 
Fig. 4.3 A graph with a P2-factor marked with bold lines. 
a partial G-factor. If G = K2, this is the notion of a matching. A partial 
G-factor which is a spanning subgraph of a graph F is called a G-factor in F 
(see Figure 4.3). Our main objective is to find a threshold for the property 
of containing a G-factor by the random graph G(n,p). Observe that when 
G = K-it this is just the property of containing a perfect matching. Note 
further that using the notation of Section 3.5, G(n,p) contains a G-factor if 
and only if DV
G — n/vo-
Luczak and Rucinski (1991) have shown that for every nontrivial tree T, the 
threshold for possessing a T-factor is the same as that for the disappearance 
of isolated vertices. (Again, the corresponding hitting time result holds too.) 
Theorem 4.8. For every tree T with t > 2 vertices, assuming n is divisible 
byt, 
(
0 
ifnp- 
log n -> — 00, 
e~e" 
if np - log n -* c, 
1 
ifnp- 
log n -► 00. 
■ 
The proof, which we omit (Advanced Exercise!), is very similar to that 
of Theorem 4.4. Instead of a bipartition, we now take a i-partition, and 
construct a T-factor from t - 1 perfect matchings between the appropriate 
sets of the partition. The existence of these perfect matchings follows by the 
same argument as in the case T = Ki-
Clearly, Theorem 4.8 remains true for forests without isolated vertices. In 
general, the threshold for the property of having a G-factor is not known and 
the triangle G = K% is the smallest unknown case. But already for the whisk 
graph K% (see Figure 3.3), the problem becomes relatively easy. It seems that 
the structural asymmetry of K% helps here. In fact, there is a broad family 
of graphs G for which the threshold has been found. 

G-FACTORS 
91 
Partial G-factors 
Before we prove this result, we will consider the related, weaker property Fc(e) 
of having a partial G-factor covering all but at most εη vertices of G(n, p). For 
this property we can pinpoint the threshold very precisely (Rucinski 1992a). 
By Theorem 3.29, for FG(e) to hold, one needs to have Φ 0 = Ω("). Recall 
that the condition Φο -> oo a.a.s. guarantees the existence of at least one 
copy of G in G(n,p), and is equivalent to assuming that n _ 1 /' m ( G ) = o(p) (see 
Theorem 3.4 and Lemma 3.6). Similarly, one can show that Φβ = Ω(η) if and 
only if p = Ω(η-1/'η<,)<σ>), where m^{G) 
is defined in (3.17). This is indeed 
the right threshold. 
Theorem 4.9. For every graph G with at least one edge and for every ε > 0 
there are positive constants c and C such that 
ίθ 
i/D<cn- ,/ m C , ) ( C ) 
Proof. By the monotonicity of F<?(e) we may assume that Φ<7 -> oo. If p < 
cn-i/m 
(G\ then Φσ < dn, where d can be made arbitrarily small by picking 
c small enough. Let H achieve the minimum in Φβ. Then Φβ = Φ# = E(XH) 
and, by Chebyshev's inequality and by Lemma 3.5 (see Remark 3.7), 
rflx. - E(x„)| > A ■pt.» < i ^ ^ i 
- o (JL) - *(.). 
Hence a.a.s. XH < T5 c' n < 
vo 
^or c sma^^ enough, and it is impossible 
to cover all but at most en vertices of G(n,p) by vertex disjoint copies of G. 
Let p > Cn - 1/ m ( 1 >( G) and suppose that G{n,p) $ FG(e). Then there exists 
a subset of at least en vertices which does not contain any copy of G. The 
probability that this happens is, by Theorem 3.9, at most 
{[en]) *Μ\εη\ίΡ) 
t G) < 
2ne-c"*°^-*\ 
where c" > 0 depends on G only. By choosing C large enough, Φβ{\εή],ρ) 
> 
(c")'1^ 
and we conclude that the above quantity, and thus also P(G(n,p) £ 
Fc(e)), converges to 0. 
■ 
Thresholds for G-factors 
Based on the last result, one can find the threshold for the property of contain-
ing a G-factor for a broad class of graphs G. Let ¿(G) stand for the minimum 
degree of G. The following result was proved independently by Alon and 
Yuster (1993) and Rucinski (1992a). 

92 
MATCHINGS 
Theorem 4.10. Let G be a graph with v vertices, satisfying 6(G) < m^l\G). 
There are positive constants c and C such that 
Í0 
ifO<cn-l'm{,){G) 
lim P(G(tm,p) has a G-factor) = Γ ' 
P - ™ 
(1, ' 
n->oo 
^ 
%fp>Cn-Xlm 
<G\ 
Proof. The O-statement follows immediately from the O-statement in Theo-
rem 4.9, so we only have to prove the 1-statement. In this proof we utilize 
the "two-round exposure" technique described in Section 1.1. 
For clarity we will demonstrate the proof in the smallest case G = Kf'· 
Here m^(G) 
= 3/2. By Theorem 4.9 (with ε = 1/4), there are a.a.s. n 
disjoint triangles in G(4n,pi), where p\ = p/2. This is our property B. Now, 
fix one graph F satisfying B and choose one vertex from each triangle of a 
collection of n vertex disjoint triangles in F. 
Let A be the set of chosen vertices, and let B denote the set of vertices not 
belonging to any of these triangles. If there is a matching M of n edges, each 
with one endpoint in A and the other one in B, then these edges, together with 
the triangles, constitute a K%-factor. To prove the existence of M, consider 
the random bipartite graph G(n,n,p2) with vertex classes A and B. As 
Ρ 2>ρ/2 = Ω(η-2/3)»ί2ϋί , 
n 
by Theorem 4.1 there is a.a.s. a perfect matching in G(n,n,p2). 
When 6(G) = h > 1, we treat Λ-tuples of vertices as single vertices of the 
bipartite graph (side A) and require that ph 3> ^ 2 . This argument remains 
valid for any G as long as 0 < S(G)/m^(G) 
< 1 (Exercise!). When 6(G) = 0, 
we are done already after the first round. 
■ 
Remark 4.11. The same technique can be extended to spanning subgraphs 
other than G-factors. For example, the proof of Theorem 4.10 presented 
above, gives for free the existence of a "triangle necklace" of length n, that 
is, a cycle of length n with every vertex adjacent to one vertex of a triangle, 
the triangles mutually disjoint and also disjoint from the cycle (see Figure 
4.4). The reason is that the random graph on the set B has the edge proba-
bility p2 large enough to ensure {a.a.s.) the existence of a Hamilton cycle (c/. 
Section 5.1). 
Remark 4.12. Alon and Yuster (1993) observed that the technique from 
the above proof can be used to enlarge the family of graphs G for which 
n-i/m 
(G) j s a threshold for the property of possessing a G-factor. Indeed, 
such a family can be recursively constructed by first including all graphs G 
for which ¿G < rn^{G), and then producing new members by splitting an 
existing member into two unions of its components, and inserting fewer than 
m^(H) 
edges between them. The proof of this statement is by induction on 
the number of applications of the above recursive rule (Exercise!). Both sets 

G-FACTORS 
93 
Fig. 4.4 The triangle necklace. 
V 
Fig. 4.5 A cubic graph obtained from graphs satisfying ¿(G) < mw(G) by the proce-
dure described in Remark 4.12; the added edges are numbered in the order of addition. 
of vertices of the auxiliary bipartite graph, A and B, appearing in the proof 
of Theorem 4.10, can now consist of sets of the original vertices. Surprisingly, 
some highly regular graphs can be obtained this way (see Figure 4.5). 
Despite all these efforts, there are still many graphs for which the threshold 
for containing a G-factor is not known. Theorem 4.9 provides a lower bound. 
A better lower bound can sometimes be obtained by looking at the threshold 
for the property COVG that every vertex belongs to a copy of G (cf. Theo-
rem 3.22), which is a natural necessary condition. Although the conjecture 
that these two thresholds must always coincide (as is the case of trees) turned 
out to be false (e.g., the case G = K%\ see Example 3.24 and Theorem 4.10), 
it is still plausible to hope that it is so at least for complete graphs. If G = K3, 
the threshold for the property COVQ is, by Example 3.24, (logn) 1/ 3n - 2/ 3, a 
logarithmic improvement over the lower bound given by Theorem 4.9. 
General spanning subgraphs 
A general upper bound on the threshold for the existence of a spanning sub-
graph in terms of its maximum degree is provided by the following argument 

94 
MATCHINGS 
from Alón and Füredi (1992). Let Hn be a sequence of n-vertex graphs with 
maximum degree Δ = Δ„, η = 1,2,.... We are interested in the increasing 
property "G(n,p) D Hn". The result below says that the threshold does not 
exceed (!2&»)>/Δ. 
Theorem 4.13. // S r r T p A - logn -+ oo, then 
lim P(G(n,p) DH n) = l. 
n->oo 
In particular, this holds for p > 3(!2*ϋ)ΐ/Δ 
Proof. Recall that the Hajnal-Szemerédi Theorem (Hajnal and Szemerédi 
1970) (c/. Bollobás (1978)) asserts that the vertex set of every graph G can 
be partitioned into D independent sets of size LI^(G)|/ÖJ or [|V(G)|/D"1, for 
each integer D satisfying A(G) < D < \V(G)\. Recall also that in a graph 
an independent set is 2-independent if the neighborhoods of its members are 
mutually disjoint, and that the square of a graph G is a graph on the same 
vertex set with edges between those pairs of vertices which are at distance 
at most two in G. Now, let u = [n/(A2 + 1)J. By applying the Hajnal-
Szemerédi Theorem to the square of Hn one can split V(H„) into D = Δ2 + 1 
2-independent sets U\,..., UD, of size u or u + 1 (Exercise!). 
Let us make a corresponding split [n] = Vi U · · · U Vp with |V¿| = \Ui\, 
i = 1,..., D. We will show by induction on i that with probability at least 
1 - (i - l)Q, where Q = 0(uexp(-up*)), the subgraph G(n,p)[Vi U · · · UK] 
contains a copy of the subgraph Hn = Hn\U\ U · · · U Ui\ (property At). This 
is trivial for i — 1, and for i = D it implies Theorem 4.13, as (D — l)Q = 
0(nexp(-upA)) = o(l) by assumption. 
We expose the edges of G(n,p) in rounds, first the edges in [Vj]2, then the 
edges in [Vi U V2]2 \ [V1]2, and so forth. Assume that i > 2 and that Ai-\ 
holds. Set V = Vx U · · · U V¿_i and fix a copy of HÍ i - 1 ) on V. (The choice of 
this copy should depend only on the edges exposed so far.) Let Nx be the set 
of vertices of V corresponding to the neighbors of x € Ui in the graph //„ . 
Consider the auxiliary bipartite random graph with bipartition (t/t,V¿), 
where an edge is drawn between 1 € Ui and y 6 Vj if and only if y is adjacent 
in G(n,p) to all vertices in Nx (see Figure 4.6). It should be clear now that 
we are after a perfect matching in this bipartite graph. 
Due to the 2-independence of Ui, the appearances of the edges in the aux-
iliary graph are independent events with probabilities bounded from below 
by ρΔ. Hence we may look at the random bipartite graph G(uj,u¿,pA) in-
stead, where u¿ = \Ui\ € {u,u + 1}. By Remark 4.3, we conclude that 
with probability at least 1 - 0(uexp(-upA)) = 1 - Q there is a perfect 
matching in the auxiliary bipartite graph, and thus Ai holds. Consequently, 
P(A) > (l-Q)P(A-i) and, by induction, Ρ(Λ<) > (1-Q)<_1 > l - ( t - l ) Q , 
which completes the proof. 
■ 

G-FACTORS 
95 
í/i 
Vi 
H 
G(m,Ui,p) 
G(n,p) 
Fig. 4.6 The battleground of the proof of Theorem 4.13. 

96 
MATCHINGS 
Example 4.14. The above result is so general that it can be applied to span-
ning d-cubes. It follows that a.a.s. there exists such a d-cube in 
G{n,p), 
n = 2d, if p > 1/2 is fixed (Exercise!). Estimates of the expectation have 
suggested (c/. Alon and Füredi (1992)) that the threshold should be around 
p = 1/4 (Exercise!). This has been recently confirmed by Riordan (2000), 
who applied the second moment method supported by a detailed analysis of 
variance. 
Remark 4.15. In the case in which Hn is a union of disjoint copies of G, 
that is, when we are after a G-factor, the above bound can be easily improved 
to 0{\ogn/n)l/D(G\ 
where D(G) = max//cc <$// is the degeneracy number of 
G. Indeed, for any graph G one can order its vertices so that each vertex has 
at most D(G) neighbors among its predecessors (Exercise!). Now, provided 
npO(G) _ v(G) logn -> co, one can repeat the above proof with Ui being the 
set of the i-th vertices taken from all the copies of G which make up Hn. The 
sets Ui are clearly 2-independent. 
This, however, does not improve the bounds for the threshold for the exis-
tence of a J<3-factor. The above results yield only that, ignoring some loga-
rithmic terms, the threshold lies somewhere between n - 2 / 3 and n - 1/ 2. Real 
progress on this problem has been made recently by Krivelevich (1996a). In 
the last section of this chapter we outline his ingenious approach. 
4.3 
TWO OPEN PROBLEMS 
Triangles in graphs and triples in 3-uniform hypergraphs are, undoubtedly, 
related combinatorial objects. One can build a 3-uniform hypergraph, the 
edges of which are the triangles of a graph and, conversely, the triples of a 
3-uniform hypergraph can be replaced by triangles to form a graph. Two of 
the most challenging, unsolved problems in the theory of random structures 
are finding the thresholds for the existence of a if3-factor in the random 
graph G(n,p) and for the existence of a perfect matching (a collection of n/3 
disjoint triples) in a random 3-uniform hypergraph. The latter, known as 
the Shamir problem, goes back to at least Schmidt and Shamir (1983). The 
former, discussed in greater generality in the previous section, was probably 
first stated in Rucinski (1992b). 
Some believe that these two problems are immanently related and a solu-
tion of one of them will yield a solution of the other one. Let us point out, 
however, that in the hypergraph case the triples are (in the binomial model) 
independent from each other, while the triangles of G(n,p) are not. What 
certainly links these two problems is that, after a quiet period, recently sig-
nificant progress was made with respect to both of them. In this section an 
account on this progress is given. 

TWO OPEN PROBLEMS 
97 
Fig. 4.7 Two diamonds and a vertex, linked by a triangle, contain a Kyfactor (des-
ignated by bold lines). 
Triangle-factors 
We begin with the result of Krivelevich (1996a). Throughout, it is assumed 
that n is divisible by three. 
Theorem 4.16. There exists a constant C > 0 such that ifp = Cn~ 3 / 5, then 
a.a.s. the random graph G(n,p) contains a Kz-factor. 
Proof (Outline). Let us explain the mysterious fraction 3/5 right away. It 
is simply the reciprocal of the parameter m^(K^) 
= 5/3 of the graph K+ 
obtained from the complete graph K4 by removing one edge. This graph, 
called here the diamond, is the 'building block' of a #3-factor in the proof. 
The removal of a vertex of degree two from it leaves a triangle. Therefore, 
the two vertices of degree two are called removable. 
The key observation is that two diamonds, D\ and D-i, and one vertex v, 
linked together by a triangle with one vertex being v and the other two being 
removable vertices of, respectively, D\ and £>2, form a subgraph that contains 
a /^-factor (see Figure 4.7). 
A naive strategy for proving Theorem 4.16 could thus be as follows (for 
simplicity we assume that n is divisible by nine). Apply the two-round expo-
sure. In round one, by the 1-statement of Theorem 4.9, with e = 1/9, a.a.s. 
there is a partial diamond-factor of G(n, p) consisting of 2n/9 diamonds. All 
we need in the second round is to link each of the outstanding n/9 vertices, 
via a triangle, with two diamonds as described above. It seems that we are 
on the right track, because, for a fixed vertex v, the expected number of such 
triangles is 0(n2p3) 
= 0{n°2), 
and by Theorem 2.18(i), a.a.s. there is at least 
one for each vertex v (Exercise!). 
Unfortunately, the n/9 pairs of diamonds must be disjoint, so we have to 
proceed greedily one by one. The problem we immediately face is that toward 
the end of this procedure the expected number of available triangles drops 
dramatically. At the very extreme, it is only 4p3 for the last vertex. And 
here comes a second crucial idea. At this late phase we need to use something 
bigger than diamonds. This bigger structure should have many removable 

98 
MATCHINGS 
Fig. 4.8 
A diamond tree; the removable vertices are designated by open circles. 
vertices and be likely to occur frequently in G(n,p). Both these requirements 
are provided by diamond trees, which we now define recursively. 
We call a vertex in a graph removable if the removal of this vertex leaves 
a graph with a K^-factor. The diamond itself is the smallest diamond tree. 
Given any diamond tree T and a removable vertex v in it, a new diamond 
tree is obtained by taking the union of T and a copy of the diamond in which 
v is a vertex of degree two and the other three vertices do not belong to T. 
Each time a diamond is added in this way to a diamond tree, the number of 
vertices increases by three, while the number of removable ones increases by 
one (the other vertex of degree two in the diamond). Hence, in every diamond 
tree more than 1/3 of the vertices are removable (Exercise! - see Figure 4.8). 
Note that, given two disjoint diamond trees 7\, T2, and a vertex v, the 
expected number of triangles with one vertex at υ and the other two being 
removable vertices of, respectively, 7\ and Γ2, is at least (Exercise!) 
llVCTxMIVd^lp3 = θ(|ν(Γι)||ν(Γ 2)|η-»/ 5). 
Thus, both diamond trees should be large enough to guarantee the presence 
of the desired triangle. And we would need many of them. 
Since, obviously, one cannot fit too many large, vertex-disjoint subgraphs 
within the frame of n vertices, we will build the desired A^-factor in rounds, 
using originally more, but smaller diamond trees, and gradually turning to 
the bigger ones. In any case, we need to have many disjoint diamond trees 
at hand. As the expected number of diamonds is linear in n, on average 
each vertex belongs to a (large) constant number of them, which allows the 
diamond trees to grow without any bound. This is specified in the following 
technical lemma. We refer the reader to Krivelevich (1996a) for the proof. 
Lemma 4.17. If p = Cn~3/5, 
C > 6, then for every integer k = k(n) satis-
fying 4 < k < n/6 and k = 1 mod 3, the random graph G(n,p) contains a.a.s. 
[n/(6fc)J vertex disjoint diamond trees, each of order k. 
■ 
Equipped with this lemma, we can now furnish the proof of Theorem 4.16 
in just twenty steps. Except for the first, simple step, and for the last one, all 

TWO OPEN PROBLEMS 
99 
these steps are quite similar and, therefore, we organize them into an inductive 
statement, with the first step serving as a kick-oft*. 
Lemma 4.18. For every i = 1,2,..., 19, there is a constant d such that if 
p = Cin~3/5 then a.a.s. the random graph G(n,p) contains a partial K^-factor 
covering all but at most nx~l/20 
vertices. 
Before we outline the proof of Lemma 4.18, let us show the last step of the 
proof of Theorem 4.16. Apply the two-round exposure. After round one with 
p = Cn~3lh, 
C > 6, by Lemma 4.17 there are a.a.s. 2n 0 0 5 vertex disjoint 
diamond trees, each of order n 0 9 5/12 (we ignore floors here). These diamond 
trees occupy together a vertex set V of size n/6. 
Round two is generated in two subrounds, as in the proof of Theorem 4.13. 
First, expose the pairs of [n] \ V with p = Ci9(5n/6)-3/'5 and conclude by 
Lemma 4.18 (i = 19) applied to G([n] \ V,p), that there is a partial Ä"3-factor 
T covering all but n1/20 vertices of [n] \ V. (We assume for simplicity that 
n1/20 is an integer divisible by three.) 
In the second subround expose the pairs with at least one element in V and 
repeatedly connect each of the uncovered vertices of [n] \ V with two diamond 
trees, by a triangle linking that vertex with one removable vertex in each of 
the two trees. The probability of failure, by an application of Theorem 2.18(i), 
is smaller than n ' ^ e " 9 ' " 
) (Exercise!). Thus, a.a.s. all remaining vertices 
are included to T, creating a /^-factor of G(n,p). 
■ 
The proof of Lemma 4.18 is quite similar to the above argument, although 
a little bit more involved. 
Proof of Lemma 4-18. We use induction on i. The case i = 1 follows, as in the 
proof of the 1-statement of Theorem 4.9, by using Theorem 3.9 (Exercise!). 
Assume that the lemma is true for some i, 1 < i < 19. Apply the two-
round exposure. In round one, with p = Cn~3/5, 
C > 6, apply Lemma 4.17 
to have a.a.s. 2n 1 - , / 2° -I- n 1~ ( , + 1 ) / 2° vertex disjoint diamond trees of order 
k ~ n'/20/12 each. Let us denote the union of the vertex sets of these diamond 
trees by V. Notice that |V| ~ n/6. 
In round two take p = d(n - |V"|))~3/5 and first conclude, by the induc-
tion assumption, that a.a.s. there is a partial K3-factor T covering all but 
ni->/20 v e r t i c e s 0f jnj \ y 
Then, reduce the number of uncovered vertices by 
incorporating them, together with some diamond trees, into T at a rate of 
two diamond trees per vertex. Since the diamond trees are smaller now, we 
seek the linking triangles among all triangles with one vertex being a fixed 
uncovered vertex of [n] \ V and the other two vertices belonging to any two 
diamond trees which are available at this stage. 
During this procedure the number of available diamond trees decreases 
steadily by two, but, owing to the excess we have, even at the end there are 
still at least n1~^t+l^20 
disjoint diamond trees around. Thus, at any given 

100 
MATCHINGS 
time, the expected number of such triangles is at least of the order 
0(n2-(,+1>/,on,/,V) = Θίη1'10) , 
and the probability of failure is as small as before (Exercise!). The procedure 
ends when all vertices of [n] \ V are covered by T, leaving at that point 
ni-(i+i)/20 vertex disjoint diamond trees. Each of the diamond trees can be 
broken into a subgraph with a K"3-factor and a single vertex. Summarizing, 
a.a.s. there is a partial A^-factor covering all but at most n 1 _^ + 1^ 2 0 vertices 
ofG(n,p). 
■ 
It is worthwhile to notice that the above proof can be applied to ifr-factors 
as well as to the Shamir problem; however, in the latter case, it yields a result 
only slightly stronger than that of Schmidt and Shamir (1983) (c/. Krivelevich 
(1996a)). 
Perfect matchings in random hypergraphs 
Let P3(n,M) be the random hypergraph ([Π]3)Λ#, defined as the uniform 
model of a random subset, where the initial set is the set of all triples of [n]. 
Schmidt and Shamir (1983) showed that if M/n3'2 -► oo then a.a.s. there 
is a perfect matching in Ha(n,M). This was improved by Frieze and Janson 
(1995). 
Theorem 4.19. // Μ/ηΛ'3 -¥ oo then a.a.s. there is a perfect matching in 
He(n,Af). 
Proof (Outline). Let us begin by introducing an interim model of a random 
hypergraph, simpler to analyze than M3 (n, M), but sufficient for our task. It is 
based on a random sequence x = (xi, ·.. ,i3M), chosen uniformly at random 
from the set Ω(η, Μ) of all 3M-element sequences of integers from [n]. Then 
we define a random hypergraph H(x) as the hypergraph on vertex set [n] 
with the edges being consecutive triples of elements of x, that is, {xi, 12,13}, 
{i 4,i 5,i e}, ... , {X3M-2,X3M-I,X3M}- 
Observe that the hypergraph H(x) 
may have repeated edges as well as deficient edges with less than three vertices. 
Therefore, let W(x) be the hypergraph obtained from H(x) by deleting both 
repeated and deficient edges. 
For M' < M, conditioning on the event that |M(i)| = M', H(i) is dis-
tributed exactly as H3(n, M'), because each hypergraph with M' triples and 
vertex set [n] arises from the same number of sequences x in this way (Exer-
cise!). Moreover, if H(x) has a perfect matching then H(i) does (Exercise!). 
The above two facts, together with the monotonicity of the property of con-

TWO OPEN PROBLEMS 
101 
taining a perfect matching, imply that 
P(H(x) has a perfect matching) = P(H(x) has a perfect matching) 
= 
] T P(H(x) has a perfect matching | |H(x)| = M') P(|M(x)| = M') 
M'<M 
= 
] Γ 
Ρ(Ήΐ3 (n, Λί') has a perfect matching) P(|É(x)| = Μ') 
M'<M 
< P(M3(η,Μ) has a perfect matching). 
Thus, our goal is to show that 
P(H(x) has a perfect matching) -> 1 
as M/n*/3 
-> oo. This will be achieved by breaking the space Ω(η, Μ) ac-
cording to the degree sequence. 
The degree of an element v G [n] in a sequence x is defined as dv = 
de8x(ü) = l{* : xi ~ v)\- For ε > 0, a degree sequence d = (di,...,d n) is 
called ε-smooth if the degrees d\,..., 
d„ do not fluctuate too much, in a precise 
technical sense for which we refer the reader to Frieze and Janson (1995). It 
can be routinely proved that a.a.s. the sequence degx(r) is n4/3/M-smooth. 
Hence, to complete the proof it suffices to show that given for each n an 
ε-smooth sequence d, where ε = ε(η) —> 0, a random sequence x chosen 
uniformly from the family X(d) = {x G Ω(η,Μ) 
: degx = d} yields a.a.s. 
a random hypergraph Η(χ) which contains a perfect matching. (Here the 
probability of failure should be o(l) uniformly for all ε-smooth sequences d.) 
This can be shown using a configuration model similar to that discussed in 
detail in Chapter 9. In this model, surprisingly, the second moment method 
works! Indeed, after some tedious calculations, it was shown in Frieze and Jan-
son (1995) that if Y denotes the number of perfect matchings in the random 
hypergraph H(x) with x chosen uniformly from X(d), then E(Y)2/ E(Y2) -► 1 
as n -> oo and thus, by (3.2), P(K > 0) -> 1. 
■ 
Remark 4.20. The idea of the proof of Theorem 4.19 relies on the observa-
tion that although the second moment method does not apply directly to the 
unconditional number of perfect matchings (the right-hand side of (3.2) does 
not tend to zero unless M/n3^2 -> oo), it can be used if we first condition on 
a suitable variable (in this case the degree sequence) which is responsible for 
most of the variance. A previous instance of combining the second moment 
method with conditioning was given by Robinson and Wormald (1992), see 
Chapter 9 of this book. 
It is believed that the actual threshold for the existence of a perfect match-
ing in M3 (n, M) coincides with that for the disappearance of isolated vertices, 
that is, it occurs around inlogn. 

102 
MATCHINGS 
Remark 4.21. The fractional version of Shamir's problem asks for the exis-
tence of a nonnegative function, defined on the triples of a hypergraph, which 
totals n/3, and which for every vertex totals 1 on all the triples containing that 
vertex. The existence of a perfect matching is easily seen to be equivalent to 
the existence of such a function taking the values 0 and 1 only. It was proved 
by Krivelevich (1996b) that the threshold for the presence of a perfect frac-
tional matching in M3 (n,M) is roughly ^nlogn. Moreover, Krivelevich also 
provided the expected hitting time version: in the naturally defined random 
hypergraph process (see Remark 1.22), a.a.s. a perfect fractional matching 
exists as soon as the last isolated vertex disappears. 

5 
The Phase Transition 
Undoubtedly, the most important and by far the most influential paper about 
random graphs which has ever appeared was the article of Erdös and Rényi 
(1960), where the authors studied the changes in the structure of G(n,M) 
as M grows from 0 to ("), identifying main features of the evolution of the 
random graph. A large part of their impressive work was devoted to the 
phase transition, the spectacular period of the random graph evolution when 
the size of the largest component of G(n,M) rapidly grows from Qc(logn) 
to ©c(n)· In this chapter we try to describe and understand this intriguing 
phenomenon. We begin with some highlights of the evolution of the random 
graph and make a "historic" journey reproving, at least partly, Erdös and 
Rényi's result on the sudden "jump" of the size of the largest component. 
Then we ourselves jump over twenty years ahead to Bollobas's paper (1984a), 
which opened a new era of study of the phase transition in G(n, M). Finally, 
in the last four sections of the chapter, we present more recent developments 
concerning various features of the random graph process in this fascinating 
period. 
5.1 THE EVOLUTION OF THE RANDOM GRAPH 
The tale of G(n, M ) 
Let us consider how the properties of G(n, M) vary when n is fixed but large, 
and M grows from 0 to (!J). Clearly, when the random graph becomes denser 
its properties change; the moment when a new property appears (or disap-
103 
Random Graphs 
by Svante Janson, Tomasz Luczak and Andrzej Rucinski 
Copyright © 2000 John Wiley & Sons, Inc. 

104 
THE PHASE TRANSITION 
pears) can often be characterized by the threshold function (see Sections 1.5 
and 1.6 for a more elaborate treatment of this subject and the proof that 
for every monotone property the threshold function exists). Since the pub-
lication of Erdös and Rényi (1960), identifying the threshold functions for 
different properties has been a major task in the theory of random graphs. 
Nowadays, the threshold functions for most (but by no means all) impor-
tant graph properties have been found and the picture of the evolution of the 
random graph is fairly complete. 
The beginning stages of the random graph process are easy to study and 
describe. It is an immediate consequence of Theorem 3.4 that for a fixed Jb > 2 
and every sequence M = M(n) such that n<fe_2)/(*-1) < M < 
n'* - 1^*, 
a.a.s. G(n,M) is a forest, which contains copies of all trees of size at most 
k and no trees with more than k vertices. If M < n but M = nl~°^ 
then 
a.a.s. G(n,M) has no cycles (Exercise! - Note that this fact does not follow 
from Theorem 3.4), and the size of the largest component, although clearly 
unbounded, is op(logn). 
The evolution of G(n, M) for M = θ(η) is far more interesting. Let M = 
cn/2 where c is a positive constant. If c is small, a.a.s. all components of 
G(n, M) are trees or unicyclic, the largest of them having 0c(logn) vertices. 
As the process evolves the components increase their size, however, as long 
as c < 1, the largest components of G(n, M) merge mainly with small trees 
of size Oc(l); thus they grow slowly and quite smoothly. Nonetheless, at 
some point of the process, the largest components become so large that it 
is likely for a new edge to connect two of them. Note that the addition 
of such an edge can increase the size of the largest component significantly; 
furthermore, a new component resulting from such a fusion has greater chances 
to be joined to another component of a similar size. Thus, fairly quickly, all 
the largest components of G(n, Λί) merge into one giant component, much 
larger than any of the remaining ones. This spectacular phenomenon, now 
called "the phase transition", is the main theme of the following sections of 
this chapter. In particular, we will learn that the giant component is formed 
from smaller ones during the so called critical period, or the critical phase, 
where M = n/2 + 0(n2^3). 
The critical period separates the subcritical phase, 
where M - n / 2 <%L -n 2/ 3, from the supercritical phase, where M—n/2 » n2/3. 
We will also soon see (Theorem 5.4) that the random graphs G(n,0.49n) and 
G(n,0.51n) are dramatically different: G(n,0.49n) has no components larger 
than Oc(logn), while the giant component of G(n,0.51n) already has ©c(n) 
vertices. 
As M increases, the giant component of G(n, M) grows, catching other 
components of the graph. Because larger components are easier game and 
they are less frequent than smaller ones, they disappear from the graph earlier, 
merging with the giant. In particular, if M is about nlogn/4, then a.a.s. 
G(n, M) consists only of the giant component and some number of isolated 
vertices. Finally, when the last isolated vertex joins the giant, which occurs 
when M = n log n/2 -I- Op(n), the graph becomes connected. At the very 

THE EVOLUTION OF THE RANDOM GRAPH 
105 
same moment a perfect matching a.a.s. can be found in G(n, M) (provided, 
of course, that the number of vertices n is even). All these results were already 
shown by Erdös and Rényi (1959, 1966), but their strongest "hitting time" 
versions (see Theorem 4.6) were proved much later by Bollobás and Thomason 
(1985) (see also Bollobás (1985, Chapter· VII)). 
Soon after the random graph becomes connected, for M = j(logn + (k -
1) log log n + Op(l)), where k > 2 is a fixed natural number, the last vertex of 
degree k — 1 vanishes and a.a.s. at the very same time G(n,M) 
becomes k-
connected (see Erdös and Rényi (1961) and Bollobás and Thomason (1985)). 
The question of whether the thresholds for 2-connectivity and Hamiltonicity 
coincide remained for a long time the major open problem of the theory of 
random graphs and, finally, was settled in the affirmative by Komlós and 
Szeméredi (1983) and Bollobás (1984b). (Later, Luczak (1991e) proved that 
at this threshold G(n, M) becomes pancyclic, i.e. contains cycles of all lengths 
I = 3,4,... ,n.) More generally, let M* denote the property that a graph G 
on n vertices contains [fc/2j edge-disjoint Hamilton cycles and, if k is odd, 
a matching of size Ln/2J, disjoint from the cycles. Then, a.a.s. the random 
graph G(2n, M) has M* at the very moment when the last vertex of degree 
smaller than k disappears (Bollobás and Frieze 1985). 
As the process evolves, G(n, M) becomes denser and denser. Its minimum 
degree and the connectivity grow, and dense subgraphs gradually appear (see 
Chapter 3). When Mdn-d-1 
= 21_<,logn + Op(l) and d > 2, its diameter 
drops from d + 1 to d (Burtin (1973), Bollobás (1981a); see also Bollobás 
(1985, Chapter X)). For M ~ a{£), where 0 < a < 1 is a constant, any two 
vertices share Qc(n) neighbors and the largest complete subgraph of G(n, M) 
has ©c(logn) vertices (Theorem 7.1). Finally, for M = ("), G(n,M) 
becomes 
a complete graph. 
The fc-core of G(n, M ) 
Note that for the connectivity, the existence of a perfect matching, and the 
existence of a Hamilton cycle, an obvious necessary condition that the mini-
mum degree is large enough turns out to be a.a.s. sufficient for the random 
graph G(n, M). How deep is this "probabilistic equivalence" of, say, the prop-
erty Mfc and the property that a graph has minimum degree at least A:? This 
problem can be addressed in two ways. Bollobás, Fenner and Frieze (1990) 
studied the structure of G(n,M)|f>fc - a graph chosen at random from the 
family of all graphs with vertex set {1,2,..., n} and M edges which have min-
imum degree at least k. In particular, they proved that for a fixed k > 1, the 
threshold for the property that G(n, M)|¿>* has M* is M = ©c(nlogn) and 
is related to the fact that at this moment some "local" obstruction for M* dis-
appears from G(n,M)\s>k. 
However, G(n,M)\s>k 
is typically very different 
from G(n,M); moreover, there is no obvious way to obtain G(n,M + l)|á>* 
from G(n,M)\s>k- 
An alternative, more natural approach was proposed by 
Bollobás (1984b). It is based on the elementary observation that if a graph G 

106 
THE PHASE TRANSITION 
contains a subgraph H of minimum degree fc, then the maximal subgraph of G 
with this property is unique (Exercise!). We call such a maximal subgraph 
the k-core of G and denote it by cr*(G). (If G contains no subgraph with 
minimum degree k we say that the fc-core of G is empty.) Thus, in partic-
ular, cri(G) is obtained from G by removing all its isolated vertices, while 
cr2(G) consists of all cycles of G and the paths joining them. Now, instead 
of G(n, M), consider the behavior of cr/t(G(n, M)) as M grows from 0 to (£). 
Is it a.a.s. fc-connected? When does the 1-core of G(n, M) a.a.s. contain a 
matching saturating all but at most one of its vertices? For which M is the 
2-core of G(n, M) a.a.s. Hamiltonian? 
The study of the evolution of the fc-core cr*(G(n,M)) was initiated by 
Bollobás (1984b), who noticed that if k > 3 is fixed then a.a.s. crjt(G(n, M)) 
is fc-connected even at very early stages of the evolution, when M = 
0(n). 
This result was strengthened by Luczak (1991d), who proved that for some 
constant a > 0, which depends neither on n nor on k, a.a.s. the random graph 
process is such that for every M, 0 < M < (!j), and every fc, 3 < fc < n - 1 , the 
fc-core of G(n, M) is either empty, or larger than an and fc-connected. Note 
that, in particular, this result implies that the fc-core emerges very rapidly: 
for almost every graph process one can find a critical moment M" such that 
the fc-core of the graph at the M"-th stage of the process is empty, while 
at the very next stage the size of the fc-core "jumps" to &c(n). 
The proof 
that M" exists is not very hard, especially for large fc; the reader is invited to 
show that for every ε > 0 there exists fcf such that for fc > fcE we have a.a.s. 
\M" — fcn/2| < en (Exercise!). On the other hand, determining M " for a 
given fc > 3 up to a factor of 1 + op(l) is a challenging task accomplished only 
recently by Pittel, Spencer and Wormald (1996), who also provided precise 
bounds for the size of the fc-core at the moment it emerges. 
Unlike in the case of fc-connectivity, replacing G(n, M) by its fc-core does 
not help very much with respect to the property M*. The threshold for 
cri(G(n, M)) to have a matching which covers all except at most one of its 
vertices occurs only when M ~ n log n/4 and is related to the existence of pairs 
of vertices of degree one adjacent to the same vertex (see Theorem 4.4). More 
generally, for arbitrary fc, the fc-core of G(n, M) has property M* when M is of 
the order n logn (Luczak 1987) - as in the case of G(n, M)\g>k, at this point 
a certain local obstruction disappears from crt(G(n,M)). The property that 
the chromatic number of G(n, M) is at least fc + 1, for some fc > 3, does not 
seem to be related to the existence of any "local" substructure of G(n, M) 
at all. Clearly, the fc-core of a graph with such a chromatic number must 
be non-empty (see Lemma 7.6), but the fact that cr*(G(n,M)) φ 0 is not 
a.a.s. sufficient for x(G(n,M)) > fc + 1. For large fc this follows immediately 
from the fact that M" is close to fcn/2, and thus x(G(n, M " ) ) is about 
fc/21ogfc (see Theorem 7.16). However, for small fc, especially for fc = 3, the 
problem whether a.a.s. the thresholds for the properties crfc(G(n, M)) φ 0 
and x(G(n, M)) > fc + 1 coincide is more involved and was settled (in the 

THE EMERGENCE OF THE GIANT COMPONENT 
107 
negative) only recently by Molloy (1996) and Achlioptas and Molloy (1997) 
for details see Section 7.5. 
5.2 
THE EMERGENCE OF THE GIANT COMPONENT 
In this section we reprove a part of Erdös and Rényi's theorem on the (unex-
pected "jump" of the size of the largest component which occurs in the random 
graph when it has about n/2 edges. Although soon we will give a much more 
precise description of this phenomenon, we feel that the branching process 
argument used here can provide a better understanding of this feature and, 
most importantly, it explains why the abrupt change of the structure of the 
random graph takes place when the average degree of its vertices approaches 
one. 
Branching processes 
Since our approach is based on branching processes let us first recall some 
elementary definitions and facts concerning them. (For proofs and a more 
elaborate treatment of this topic see Athreya and Ney (1972), or any textbook 
on probability theory.) Let X be a random variable which takes values in the 
non-negative integers. The Galton-Watson 
branching process defined by X 
starts with a single particle, which produces Z\ other particles, where the 
number Z\ of first-generation particles has the same distribution as X. Each 
of the offspring particles produces, in turn, its own children, whose number 
has distribution X, independently for each particle, and so on. If by Z{ we 
denote the number of offspring in the i-th generation, then Z0 = 1, while for 
i > 1 the variable Z¿ is the sum of Z¿_i independent copies of X; clearly, 
this observation can also be used as an equivalent definition of the random 
variables Zt. Note that if Zn = 0 for some n, then Zm = 0 for all 
m>n. 
The most basic fact about branching processes states that if the expectation 
of X is larger than one, then with positive probability the process will continue 
forever, while otherwise, except for the degenerate case, with probability one 
the process will die out, that is, for some n we have Zn = 0. More precisely, 
let / : [0,1] -¥ 1 denote the probability generating function of X, defined as 
/x(i) = /(x)=5>*P(X = i). 
t>0 
Moreover, let Z = Σί>ο ^» D e t n e t o t al number of offspring in the branching 
process. The probability p = ρχ of extinction of the branching process is 
defined as 
p = P(Z < oo) = lim Ψ(Ζη = 0) . 
η-κ» 
Then the following holds. 

108 
THE PHASE TRANSITION 
Theorem 5.1. For EX < 1 we have px = 1, unless P(,Y = 1) = 1. // 
E A' > 1 and f(X = 0) > 0, then px = xo, where xo is the unique solution of 
the equation f(x) = x which belongs to the interval (0,1). 
■ 
Example 5.2. Let X e Po(c). Then 
/x(*) = f ^ e - c 
= exp(c(*-l)). 
Thus, if c > 1, the probability ρχ that the branching process defined by X 
dies out is equal to 1 - /3(c), where ß = ß(c) € (0,1) is uniquely determined 
by the equation 
ß + e~0c = l. 
(5.1) 
Example 5.3. Let Y„ € Bi(n,p), where np-+ c> 1 as n -4 oo. Since 
/y-(*) = Σ (")*V(i-p) n-' = (i - P + *P)n. 
»=o ^ ' 
for every real number x we have 
lim /yn(x) = exp(c(i - 1)) = 
fx(x), 
n-+oo 
that is, the probability generating function of Yn tends pointwise to the prob-
ability generating function of X 6 Po(c), precisely as one might expect. Thus, 
as n -> oo, the probability of extinction p(n, c) of the branching process de-
fined by Yn converges to 1 — ß{c), where /3(c) is defined as in (5.1). 
The giant component 
We will use branching processes to study the rapid growth of the size of the 
largest component in G(n,p) - the analysis of the behavior of G(n,M) is 
similar, but the fact that in G(n,p) edges appear independently from each 
other makes the argument simpler. Thus, let p = p(n) = c/n, where c is 
a positive constant. We reveal the component structure of G(n,p) step by 
step, using the following procedure. Choose a vertex v in G(n,p), find all 
neighbors Vi,..., vT of v, and mark v as saturated. Then, generate all vertices 
{«ii,...,wi,} from [n] \ {«,«i,.. -,«r} which are adjacent to v\ in G(n,p), 
so v\ becomes saturated, and continue this process until all vertices in the 
component of G(n,p) containing w are saturated. 
If during the above procedure we saturate first the vertices which lie closer 
to v, the process resembles very much the branching process. However, in 
our case, the number Xi = Xi(n,m,p) 
of new vertices we add to the com-
ponent in the i-th step, provided m of its elements have already been found, 
has binomial distribution Bi(n - m,p), whereas in the branching process the 

THE EMERGENCE OF THE GIANT COMPONENT 
109 
distribution of the immediate offspring of a particle does not depend on the 
previous history of the process. Nonetheless, while m is not very large, the 
process of generating the component containing a given vertex can be closely 
approximated by the branching process defined by a variable with binomial 
distribution Bi(n,p). Thus, one may expect that the probability that a vertex 
is contained in a "small" component is roughly given by the probability that 
the process dies out, which happens with probability 1 for c < 1. On the 
other hand, if c > 1, then with some positive probability 1 - pc the process 
continues for a long time and thus we may expect that (1 - pc + o( 1)) n vertices 
of G(n,p) belong to one giant component. 
Theorem 5.4. Let np — c, where c > 0 is a constant. 
(i) If c < 1, then a.a.s. the largest component of G(n,p) has at most 
h*c)i log n vertices. 
(ii) Let c> 1 and let ß = ß{c) € (0,1) be defined as in {5.1). Then G(n,p) 
contains a giant component of(l+op(l))ßn 
vertices. Furthermore, a.a.s. 
the size of the second largest component o/G(n,p) is at most t^'\i 
logn. 
Proof. Let us assume first that pn — c and c < 1. Note that the probability 
that a given vertex v belongs to a component of size at least k = k(n) is 
bounded from above by the probability that the sum of k = k(n) random 
variables X¿ is at least k — 1. Furthermore, Xi can be bounded from above 
by Xf, where all X* have the same binomial distribution Bi(n,p) and the 
random variables X*,..., 
X£ are independent; note that for such random 
variables we have £ * = 1 Xf € Bi(fcn,p). Thus, from (2.5) we infer that for 
n large enough the probability that G(n, p) contains a component of size at 
least k > 31ogn/(l — c)2 is bounded from above by 
n P H T x t > f c - l ) =ηψ(Υ^Χ+ 
>ck + 
(l-c)k-A 
Now let c > 1. Set k- = ^ f y , logn and k+ = n 2/ 3. First we will show 
that a.a.s. for every k, k- < k < k+, and all vertices v of G(n,p), either 
the process described above which starts at v terminates after fewer than 
fc_ steps, or at the fc-th step there are at least (c - l)fc/2 vertices in the 
component containing v that have been generated in the process but which 
are not yet saturated. In particular, no component of G(n,p) has jfc vertices, 
with fc_ < k < k+. Note first that in order to check if the process which starts 
at v produces after each k step at least (c - l)fc/2 unsaturated vertices in the 
component containing v, we need only to identify at most fc + (c-l)Jfc/2 = 
(c+ l)fc/2 vertices of this component. Hence, as in the previous case, for each 

110 
THE PHASE TRANSITION 
i, where 1 < i < k, we can bound X, from below by Xi € Bi(n - ^Jfc+.p), 
where all variables X~ are independent. Furthermore, the probability that 
either after the k first steps we produce fewer than (c — l)A/2 saturated 
vertices, or that the process dies out after the first k steps, is smaller than the 
probability that 
¿=i 
,= i 
¿ 
Thus, from the large deviation inequality (2.6) the probability that it happens 
for some vertex v of G(n, M) and for some k, fc_ < k < k+, is, for n large 
enough, bounded from above by 
< n f c + e x p ( - ^ = ^ f c _ ) = 0 ( l ) . 
Now let us consider a pair of vertices v' and v" which belong to components 
of size at least k+. What is the probability that they belong to different 
components? Let us run the process of identifying vertices of the component 
of G(n, M) containing v' for the first k+ steps. According to the fact we 
have just proved, at the end of this procedure we are left with some set V 
of vertices of the component containing υ', such that at least (c — l)A+/2 
vertices from V are unsaturated. Let us now run a similar process starting 
at the vertex v". Then, either we join v" to some of the vertices which belong 
to V, or end up with some set of vertices V" of the component containing 
v", among which at least (c - l)fc+/2 have yet to be saturated. Now the 
probability that there are no edges between as yet unsaturated vertices of V 
and V" is bounded from above by 
(1 _ p)[(c-l)*+/2]2 < e x p ( _ ( c _ 1)2^1/3/4) = o ( 1 / n 2 ) . 
Consequently, the probability that G(n, M) contains two vertices v' and v" 
which belong to two different components both of size at least k+ tends to 0 
as n -> oo. 
Thus, we have shown that a.a.s. the vertices of G(n,p) can be divided into 
two classes: "small" ones, which belong to components of size at most fc_, and 
"large" ones, contained in one large component of size at least k+. Now to 
complete the proof we need to estimate the number of small vertices. Observe 
that the probability p(n,p) that a vertex is small is bounded from above by 
the extinction probability p+ = p+(n,p) of the branching process, in which 
the distribution of the immediate offspring of a particle is given by the bino-
mial distribution Bi(n - k-,p). On the other hand, p{n,p) is bounded from 
below by p_ + o(l), where p_ = p_(n,p) is the probability of extinction for 

THE EMERGENCE OF THE GIANT COMPONENT 
111 
the branching process with distribution Bi(n,p) (the term o(l) bounds the 
probability that the branching process dies after more than k- steps.) We 
know (Example 5.3) that if np = c and n -» oo, then both p.. and p+ converge 
to 1 - ß, with ß = ß(c) < 1 defined as in (5.1). Hence the expectation of the 
number Y of small vertices is (1 - ß + o(l))n. 
Furthermore, 
E(Y(Y - 1)) < np(n,p)(k. 
+ np(n - O(fc-).P)) = (1 + o(l)){EY)2 
. 
Hence from Chebyshev's inequality (1.2), G(n,p) contains (1 - ß + op(l))n 
small vertices and the assertion follows. 
■ 
The double jump 
Although Theorem 5.4 tells about the behavior of G(n,p), from the equiva-
lence of G(n,p) and G(n, M) (Proposition 1.13) we infer that a similar sudden 
change of the size of the largest component occurs also for G(n, M), the model 
of the random graph used by Erdös and Rényi (1960). Hence, it remains to 
study the structure of G(n, M) when the expected degree of each of its ver-
tices is close to 1, that is, for M ~ n/2. Erdös and Rényi suggested that 
in this case a "double jump" occurs: the largest component of G(n, M) has 
Qp(n2/3) vertices and near the point M ~ n/2 the largest component changes 
its size twice - first from Oc(log n) to Qp(n2/3), 
and then from θ ρ(π 2 / 3) to 
0c(n). (Note that, as was mentioned in the previous section, in the evolu-
tion of the fc-core of G(n, M) for k > 3, a somewhat similar "single jump" 
can be observed). Thus, in particular, Erdös and Rényi expected that what-
ever function M = M(n) we choose, the number of vertices L\{n, M) in the 
largest component of G(n,p) can only be either Oc(logn), or θ ρ(η 2/ 3), or 
maybe &c(n). 
However, it seems that the proof of Theorem 5.4 works also 
for c = 1 -I- ε(η), provided that e(n) > 0 tends to 0 with n slowly enough. As 
can be easily checked expanding ß(c) = ß(l + ε) in a Taylor's series, in this 
case the largest component should a.a.s. have about 2εη vertices (Exercise!). 
Let us mention yet another piece of evidence against such an abrupt change 
in the size of the largest component. Choose any function r = r(n), say 
r = n4/5. Now, for every n and each of the (")! increasing sequences of 
graphs G = (G0, G i , . . . , GN), where N = (£) and the graph d has i edges for 
0 <i < N, choose the maximum value M(G) such that the largest component 
in G has at most r vertices. Finally, let MT = Mr(n) be a median of all N\ 
values of M(G). Then, by the choice of M r, 
FiL^M^^r)^ 
1/2, 
but at the same time we have also 
P(Li(n,M r) > r/2) > P(L!(n,M r + 1) > r) > 1/2, 
because by adding one edge to a graph we can at most double the number of 
vertices in the largest component. Hence, Li(n,M) must grow more or less 

112 
THE PHASE TRANSITION 
"smoothly" with n. (Note, by the way, that this argument cannot be applied 
to the evolution of the fc-core, when k > 2; in this case the addition of one 
edge can immensely increase the size of the fc-core of a graph.) It is somewhat 
surprising that the fact that near the point M ~ n/2 the size of the largest 
component must grow gradually with M was not noticed, or at least was not 
studied, for over twenty years. It was addressed only by Bollobás (1984a), 
who first described in detail the behavior of G(n, M) for M ~ n/2. 
Let us also mention that the size of the largest component of G(n, M) is 
indeed Θρ(η2/3) when M = n/2 (see Theorem 5.20 below); thus, if M = en 
and c is required to be a constant independent of n, as the value of c grows from 
zero to infinity, at c = 1 a "double jump" does occur, precisely as described 
by Erdös and Rényi. 
5.3 
THE EMERGENCE OF THE GIANT: A CLOSER LOOK 
Theorem 5.4 stated that if M = cn/2 and c < 1 then a.a.s. G(n, M) consists of 
small components, while for c > 1 a.a.s. the structure of G(n, M) is dominated 
by one giant component which contains a positive fraction of all vertices. 
The aim of this section is to study to what extent this description remains 
true when 2M/n -> 1. Thus, we show that in the subcritical phase, when 
M = n/2 — s and s » n2/3, no component of G(n, M) is significantly larger 
than the remaining ones. More precisely, if by Lr(n, M) we denote the number 
of vertices in the r-th largest component of G(n, M), then in the subcritical 
phase we have L2{n,M) 
> (1 + o p(l))Li(n,M) - 1 (Theorem 5.6). On the 
other hand, in the supercritical phase when M = n/2 + s and s » 
n2/3, 
the largest component exceeds by far all its competitors, that is, L2(n,M) 
= 
op(Li(n,M)) 
(Theorems 5.7 and 5.12). Finally, it should be mentioned that 
a systematic study of the phase transition was started by a remarkable paper 
of Bollobás (1984a) which described the most characteristic features of this 
phenomenon. 
The subcritical phase 
Let us first introduce some notation. A component H of a graph is an l-
component if it has k vertices and k + I edges for some k > 1; in such a case 
we call I the excess of H. Note that we always have I > - 1 . Furthermore, 
t = — 1 only for tree components while each 0-component is unicyclic. We 
call an ¿-component complex if its excess I is positive, that is, if it contains at 
least two cycles. Our first result states that if M is much smaller than n/2, 
then a.a.s. G(n,M) contains no complex components. 
Theorem 5.5. Let M = n/2 - s, where s = s(n) > 0. Then, the probabil-
ity that G(n, M) contains a complex component is smaller than n 2/4s 3. In 
particular, if s ~3> n 2/ 3, then a.a.s. G(n,M) contains no complex components. 

THE EMERGENCE OF THE GIANT: A CLOSER LOOK 
113 
Proof. If a graph contains a component with at least two cycles, it must 
contain a subgraph which either consists of two cycles joined by a path (or 
sharing a vertex), or is a cycle with a "diagonal path". Let X be the number 
of such subgraphs in G(n, M). Since on k given vertices one can build no 
more than k2k\ of them (Exercise!), we have 
«*>^tGM2:::i)(Sf 
* = 4 
u 
In the proof of the above result we estimated the number of complex com-
ponents very crudely; in order to study the phase transition phenomenon, we 
will need more precise information on the number of ¿-components at different 
stages of the random graph process. Thus, let Y(k,£) = Yn,M(^^) denote the 
random variable which counts ¿-components of G(n, M) on A; vertices. Then, 
for the expectation of Yn,\i{k,(), 
we have 
EY«MU) = ( ^ , θ ς / Χ ' . , ) (©)"' , 
(5.2, 
where C{k, t) is the number of connected labelled graphs with k + I edges on 
a given set of k vertices. We estimate the value of KY(k,£) 
using Stirling's 
formula 
n! = (1+ 0 ( l / n ) ) v ^ n ( n / e ) n , 
(5.3) 
the expansion of the logarithm which for 0 < x < 1/2 gives 
1 - x = exp(-x - x2/2 - i 3 / 3 - 0(x 4)), 
(5.4) 
and the asymptotic formula for the falling factorial which follows from them 
In order to simplify our further calculations, let us assume that I > - 1 
does not depend on n, k = 0(n2/3) 
and M = n/2 + s, where k <£ \s\ = o(n). 
Then, using (5.5), from (5.2) we get 
/2M\*/2M\< 
( k2 
k3 
2M\*/2M\< 
/ k2 
ks \ 
, c e , 
- ) 
( ^ ) e X p ( - 2 ¿ - 6 ^ ) ' 
( 5 6 ) 

114 
THE PHASE TRANSITION 
while (5.4) and (5.5) give 
/n-fc\"+25-2fc-2< 
/ 
3fc2 
2fc3 
2sk sk2\ 
I 
) 
~exp(-A: + —- + —r 
- ) 
V n / 
V 
2n 3n2 n 
n2 / 
(M)fc+< 
/ fc2 2*3 2sk2 
. / s ¥ u 
, . _ . 
/ 2 M \ * 
/2sfc 
2s2fc 
^/s3Jfc\\ 
Hence, (5.6) becomes 
vv 
IU Λ 
C ( f e ' ^ 
/■ i. 2s2fc 
sit2 fc3 
^/s3fcxx 
E r „ , M ( M ) ~ _ 7 ^ r e x p ( - * - ^ - + — - — + 0 ( - ) ) . (5.8) 
Finally, let us recall that the number of forests with vertex set {1,2,..., fc} 
which consist of i trees such that vertices 1,2,... ,t belong to different trees 
is given by ¿fc*-*-'. Thus, in particular, C(fc,-1) = fc*-2, while for C(fc,0) 
we have (Exercise!) 
C(fc,0) = ¿ Q iizUlijfc*-«-» = (l + 0(l/^))s/^ßkk-1'2 
. 
(5.9) 
Although (5.8) applies only for fc = 0{n2/3), 
it is not hard to show that 
if \s\ 3> n 2/ 3 the expected number of isolated trees and unicyclic components 
larger than n 2/ 3 quickly tends to 0 as n -¥ oo. 
Note also that for any given t 
EYnM(k,e){YnM(k,e) 
-1) 
=EYnM(k,e)EYn-kM-k-e(kj), 
while for fci φ fc2 
EYnM(kue)Yn,M(k2,e) 
= 
EYn,M(kiJ)EYn-kl,M-kl-i(k2J). 
Hence, for any fc_ < fc+ = 0(n2!3), 
the variance of the number of trees or 
unicyclic components of size fc, where fc_ < fc < fc+, can be estimated using 
calculations similar to the ones above. Using a somewhat more precise version 
of (5.8), the first moment method and Chebyshev's inequality, one can, after 
some work, arrive at the following result. 
Theorem 5.6. Let M = n/2 - s, where s = s(n) is such that n2/3 < s 4C n. 
Moreover, let r be a fixed natural number, which does not depend on n, and 
finally, let a = a(n) < 1/3 but a » max{s/n,log - 1 / 2(s 3/n 2)}. Then, for n 
large enough, with probability at least 1 — (n2/s3)a 
the r-th largest component 
of G(n, M) is a tree and 
(l-2a)^log£<Mn,M)<(l + 2 a ) ^ l o g £ . 
■ 
Thus, for M — τι/2-s, n 2/ 3 <S s < n, the r largest components of G(n, M) 
are all trees with (l/2+o p(l))(n 2s - 2) log(s3n-2) vertices. As a matter of fact, 
one can use (5.8) to show a much more precise result on the limit distribution 
of LT(n, M) (see Luczak (1990c, 1996)). 

THE EMERGENCE OF THE GIANT: A CLOSER LOOK 
115 
The supercritical phase 
As we have just proved, in the subcritical phase, when M — n/2 - s and 
s » n2/3, a.a.s. G(n,M) 
consists of small trees and unicyclic components 
(Theorems 5.5 and 5.6), and thus its structure is rather easy to study. The 
properties of G(n, M) in the supercritical phase, when M = n/2 + s and 
s » n2/3, are much harder to investigate. 
Let us start with a simple but profound observation on the formula (5.8): 
if k = 0(n2/3), 
then the leading factor containing s, exp{-2s2k/n2), 
does 
not depend on the sign of s. Thus, all estimates of the moments of 
Y(k,l) 
for k = 0{n2l3) 
which are true for the subcritical phase are expected to hold 
also in the supercritical phase and the behavior of the components of size 
Jfc = 0(n2/3) 
in both G(n,n/2 + s) and G(n,n/2 - s) should be similar. We 
will soon see that this is indeed the case and this vague remark can be stated 
in a rigorous way as a symmetry rule (Theorem 5.24). Here we only mention 
that estimates of the moments of Yn,M(k, t), similar to those which led us to 
Theorem 5.6, give the following result. 
Theorem 5.7. Let M — n/2 + s, where s = s(n) is such that n2/3 
< s « 
n. 
Furthermore, let r > 1 be fixed and let a = a(n) < 1/3 be such that 
a 3> max {s/n,log~1^2(s3/n2)}. 
Then, for n large enough, with probability 
at least 1 — (n2/s3)a 
among all trees and unicyclic components 
ofG(n,M) 
the r-th largest is a tree, of size contained between (1/2 — a ) n 2 s - 2 log(s3n-2) 
and (1/2 + a)n 2s _ 2log(s 3n - 2). 
■ 
Now it is time to look at the behavior of complex components of G(n, M) in 
the supercritical case. Note that our sketchy proof of Theorem 5.6 is implicitly 
based on the fact that in the subcritical phase there are many components of 
sizes close to Lx(n,M). 
Thus, we can count them, show that their expected 
number tends to infinity, and then use Chebyshev's inequality to show that 
their number is close to its expected value. Theorem 5.4 suggests that in 
the supercritical phase, if M = n/2 + o(n) and the term o(n) is positive 
and tends to zero slowly enough, then a.a.s. G(n, M) contains precisely one 
large complex component and so computing moments of Yn,M (k,£) does not 
seem to be of much use. Hence, instead of counting complex components of 
G(n, M) at one value of Μ, we look at the stages of the random graph process 
{G(n, M)}M when such components have been created. 
Let G be a graph and {v,w} be a pair of its vertices which is not an 
edge of G. We call {v, w} a k-internal juncture if both v and w belong 
to the same unicyclic component of G of k vertices. Similarly, we call a 
pair {v,w} a (k\ ,k2)-proper juncture if v belongs to a unicyclic component 
on fci vertices and tu is a vertex of a different unicyclic component on k2 
vertices. Let Z'n(M1,M2;k) 
[Ζ'^Μχ, M2; fci, k2)\ denote the number of M's, 
Mi < M < M2, such that the edge added to the graph at the M-th step 
of the random graph process {G(n, M)}M is a fc-internal juncture [(¿ι,Α^)-

116 
THE PHASE TRANSITION 
proper juncture] of G(n, M - 1). Furthermore, let 
k/l 
Zn{M1,M2;k) 
= Z'n(Ml,M2;k) 
+ £ Z,;{Ml,M2;k1,k 
- k,) 
and 
n 
Zn(Mi,M2) 
= 
YiZn(Mi,M2;k). 
Note that adding a juncture to a graph is the only way of creating a "new" 
complex component. Thus, at each moment of the random graph process, 
the number of complex components is bounded from above by the number of 
junctures added to the graph so far, and for every M, 1 < M < (£), we have 
Υ(η,Μ) = ΣΣΥη,Μ{^1)<Ζη{\,Μ). 
(5.10) 
k t>\ 
We use this fact to investigate the behavior of the complex components in 
G(n, M). Observe first that 
■ W +1.« +1,*) - (¡|)c(t.a,(<V>t) J b l / (©), 
which looks very much like the formula for EY(k,0) given in (5.2). Thus, 
after calculations similar to those given at the beginning of this section, for 
k = 0{n2!3) and M = n/2 4- s, where s = o(n), we arrive at the following 
analog of (5.8): 
EZ'(M + l,M + l;fc)~fc2C
2
(*;0) 
n K\ 
i , 
2s2k 
sk2 
k3 
„/sk3 
s3k 1\\ 
1 Jfc 
/ 2s2k 
sk2 
k3 
t 1 
SÄ:3 s3k\\ 
Similarly, for k\ and k2 such that k\ + k2 = k, and M = n/2 + s, we get 
*™.w*.«(JXiJ^/(2) 
1 
1 
/ 2s2 k 
sk2 
k3 
~ H-<5fcl*28n2eXPV 
n2 + n2 6n2 
/ 1 
1 
sk3 
s3k\\ 

THE EMERGENCE OF THE GIANT: A Cl OSER LOOK 117 
Hence, for such k and M, 
5Jfc 
/ 2s2k 
sk2 fc3 
EZ(M + l,A/ + l;fc)~^exP(-—+ 
^
-
^ 
^(i^^))- ™ 
VJfc 
We will use (5.11) to prove the following result on complex components 
(Janson 1993). 
Theorem 5.8. Let ω = ω(η) > 1 be a function of n. 
lim E z ( l , (Λ) = ^= = 1.134.... 
(5.12) 
(i) 
,._ „ „^, ΛΛ\ 5π 
n-M» 
V \ Z / / 8 > / 3 
/n parítcu/or, 
l i m i n f P ( z ( l , Q ) = l ) > 2 - ^ = 0 . 8 6 5 . . . , 
(5.13) 
and, for n large enough, 
(ii) ΤΛβ probability that during the random graph process {G(n, M)}M for 
some M, 0 < M < (£), the graph G(n,M) contains a complex compo-
nent with fewer than η2^3/ω vertices is, for large n, smaller than 1/ω. 
(iii) // M± = n/2 ± ωη2!3, then, for large n, 
P ( Z ( I , M _ ) + Z ( M + , Q ) > O ) < 1 / " · 
(5.15) 
Remark 5.9. Janson (1993) used the method of moments (Section 6.1) to 
show that the random variable Z(l, (£)) converges in distribution and so the 
limit in (5.13) exists (for its value see Theorem 5.29, below). 
Proof. Elementary but somewhat tedious calculations, which we omit here, 
show that the main contribution to 
EZ(1, ( " ) ) = £ £ E Z ( M + 1,M + l;fc) 
^ ' 
k M 
comes from the terms for which k = 0(n2/3) and M = n/2 + 0(n2/3). 
Thus, 
using (5.11) and replacing the sums over fc and s with integrals over 1 = 
sn~2/3 and y = kn~2l3 one arrives at 

118 
THE PHASE TRANSITION 
E Z ( l , Q ) = ( l + o(l))A y"°° | ° ° yexp(-2x 2y + xy2 - y 3/6)dxdy 
c 
roo 
/·οο 
= (1 + ο ( 1 ) ) ^ ^ 
y 
y e x p ( - 2 y ( x - y / 4 ) 2 - y 3 / 2 4 ) d x d y , 
which, after elementary calculations, gives (5.12). Note that the random graph 
process ends with a complete graph, so Zn{\, (£)) > 1. This fact, together 
with (5.12), implies (5.13), while (5.14) is an immediate consequence of (5.12) 
and Markov's inequality (3.1). Furthermore, (ii) follows using the estimate 
rl/ui 
roo 
V exp(-2x2y + xy2 - y3/6) dx 
dy<l/u 
JO 
J — oo 
C 
f l / ω 
/ΌΟ 
16 
Finally, in order to show (iii) we observe that 
_5_ '°° 
16 / 
/ 
y exp(-2x2y + xy2 - y3/6) dxdy 
<1/ω. 
Jo 
J\z\>v 
Let us note an important consequence of the above statement. 
Theorem 5.10. Let M > n/2 + s, where s ;» n2^3. Then with probability at 
least 1 — βη 2/ 9/« 1/ 3 the random graph G(n, M) contains exactly one complex 
component. 
Proof. Let M_ = n/2 + [s/2\ and set u; = s 1 / 3n _ 2 / 9. Theorem 5.8(i,ii) 
implies that with probability at least 1 — 3/ω, G(n, M_) contains at most ω 
complex components, each of at least η 2/ 3/ω vertices. On the other hand, 
Theorem 5.8(iii) states that with probability at least 1 - 2/ω no new complex 
components appears in the random graph process after the moment M_. 
Thus, since the final stage of the random graph process, the complete graph 
on n vertices, has a positive excess (provided n > 4), there must be at least 
one complex component in G(n, M_). 
Now consider the graphs G(n, M_) and G(n, M) as two stages of the same 
random graph process. Given that G(n, M_) is as in the preceding paragraph, 
the probability that some pair of complex components of G(n, M_) is not 
joined in G(n, M) by at least one edge is bounded from above by 
\2J\ 
M-M. 
) / 
\M-M-) 
< Ι ω
2
β χ ρ ( - | η 4 / 3 ω - 2 / Q ) 
< Iu;2exp(-u,) < 1/ω. 
Since, as we have already observed, with probability at least 1 - 2/ui, no 
new complex component is created during the process after M_ step, the 
component obtained from merging all complex components of G(n, M_) is 
the only complex component of G(n, M). 
■ 

THE EMERGENCE OF THE GIANT: A CLOSER LOOK 
119 
Theorems 5.7, 5.8, and 5.10 tell us that in the supercritical case 
G(n,M) 
consists of some number of small trees and unicyclic components of size 
op{n2lz) 
each, and one complex component of size Ωρ(η2/3). Thus, it re-
mains to estimate the size of the largest component more precisely. In order 
to do it we follow the original argument of Bollobas (1984a), who used the 
formula (5.6) to compute the number of vertices which are contained in small 
components; this, in turn, will give us a precise estimate of the size of the 
giant complex component. The proof of Bollobás's result, stated below as 
Theorem 5.11, is very ingenious and quite complicated; thus, instead of pre-
senting it here in full detail, we just say a few words on the main idea behind 
it. 
One can easily check that in the subcritical phase, where M_ = n/2 - s, 
and s ~3> n2¡3, only a negligible number of vertices (more precisely, 
Op(n2/s2) 
of them) are contained in unicyclic components and most of the vertices of 
G(n,M) belong to isolated trees. Hence, for such an M_, the value of 
f(n,M-)= £ 
kEYn,M_(k,-l) 
k=l 
is very close to n. Now suppose that we would like to estimate the value of 
/(n, M+) for M+ = n/2 + s. If follows from (5.6)-(5.8) that the leading terms 
which contain s are: exp(-2sfc/n), which comes from (1 - k/n)n+2'~2k, 
and 
(2M/n)* = (1 4· 2s/n)k. 
Thus, let us choose s > 0 in such a way that 
(*-"M?)-('♦?)■*(-?)· 
<516> 
Then the terms of /(n, M+) become very similar to those of f{n, M_), which, 
as we know, is roughly equal to n. As a matter of fact, the main difference 
comes from the factor (2M/n)' in (5.6); now, when i = - 1 , we may expect 
that 
n / 2 - s _ M_ _ f(n,M+) 
_ f{n,M+) 
n/2 + s 
M+ ~ f(n,M-) 
~ 
n 
where here by a„ ~ bn we mean that a„ and bn agree up to the second-order 
term, in our case 1 - an ~ 1 - 6„. Bollobás (1984a) (see also Bollobas (1985, 
Chapter VI)) showed that, indeed, the value of the three fractions above are 
very close to each other; furthermore, he used Chebyshev's inequality to prove 
that the expectation f(n,M+) 
closely approximates the number of vertices 
contained in small trees of G(n, M+). 
Theorem 5.11. Let M = n/2 + s, where s = s(n) » n 2 / 3, and let s be the 
function defined in (5.16). Then, for any ω = ω(η) -> oo and large enough 
n, with probability at least 1 - 1/ω 
i"2/3i 
n2 
Σ 
Wn,M(k,0)<<j?j 
Jt=3 
S 

120 
THE PHASE TRANSITION 
and 
Σ 
fcyniíí(i,-i)-^n|<4. 
■ 
I ^ J 
n + 2s I 
V« 
Let us remark that, roughly, the first part of Theorem 5.11 says that the 
total number of vertices contained in small unicyclic components is Op(n2^3) 
and thus negligible. Furthermore, since from the Taylor expansion we get 
s 
s 
4 s2 
Λ / s3 \ 
, 
- = - - ö - 2 + ° ( - s ) . 
5 1 7 
n 
n 
3 n 2 \ n 3 / 
Theorem 5.11 implies that if s » n2¡3, then, up to an error of op(n2/3), 
n-2s 
2(s + s)n 
A 
„(s2\ 
n 
ττη=— 
;r-=4s + 0 —) 
(5.18) 
n + 2s 
n + 2s 
\nJ 
vertices belong to components which are either larger than n2^3 or contain 
more than one cycle. 
From Theorems 5.7, 5.10, and 5.11 we immediately get the main result of 
this section characterizing the structure of G(n, M) for M = n/2 + s, where 
n 2/ 3 <SC s «: n. The theorem below was proved by Bollobás (1984a), under 
the somewhat stronger assumption that M — n/2 > n 2/ 3
v
/logn/2, which was 
later replaced by M-n/2 » n 2 / 3 by Luczak (1990c). (Although Theorem 5.7 
is valid only for s «C n, we state the result for all s 3> n2/3, so it covers the 
whole supercritical phase of the evolution of the random graph.) 
Theorem 5.12. Let M = n/2+5, where a = s(n) » n2/3 and let s be defined 
as in (5.16). Then, for large enough n, with probability at least l — 7n2/9s~1/3, 
\r t M\ 
2(8 + a)n|^n2/3 
and the largest component is complex, while all other components are either 
trees or unicyclic components, smaller than n2/3. 
■ 
Let us mention that the proof of Theorem 5.12 presented in Bollobás 
(1984a) (and Luczak (1990c)) was slightly different and relied strongly on 
estimates for the expected number of complex components in the supercrit-
ical phase. In order to evaluate EYn,M(k,i), 
Bollobás had to find a way to 
deal with C(k,l), 
which appears in the formula (5.2). In Bollobás (1984a) 
he obtained a particularly useful upper bound for C(k,£), showing that, for 
some absolute constant A, and k,i > 1, 
C(kJ)<(j)t,2kk^31-^2. 
(5.19) 
Luczak (1990b) observed that the right-hand side of the above inequality with 
A = e/12 approximates the value of C(k, i) quite precisely, as long as i is large 

THE STRUCTURE OF THE GIANT COMPONENT 
121 
but ( - o{k); at the same time, Bender, Canfield and McKay (1990) found a 
fairly complicated asymptotic formula for C(kJ) 
for every function I = i(k) 
as k -> oo. One can use their powerful result to provide a description of the 
asymptotic distribution of L\(n, M) better than that given by Theorem 5.12. 
For instance, one can show that for M = n/2 + s, where n 2 / 3 «: s = 
0{n), 
the distribution of the random variable Li(n,M) 
is asymptotically normal. 
The idea of the proof is very simple. For a constant r, 0 < r < 1, choose a 
function Jfcr = kr(n,M) 
such that for the random variable 
Yr = Yr(n,M) = ζ
ζ 
Yn.M(kJ) 
l>l 
k<kr 
we have EVr -* r, and for each i > 2 the factorial moment E(Yr)i tends to 0 
as n —► oo. Then, from a special (and, in fact, obvious) case of the method of 
moments (see Theorem 6.7) we infer that 
P(yr = 1) ~ P(Li(n, M) < kr) -» r 
as n -¥ oo. We should remark, however, that, because the formula for C(k, ί) 
is quite involved, finding kr(n, M) in this way is a long and not very exciting 
task (see Pittel (1990) for another approach to this problem). 
5.4 
THE STRUCTURE OF THE GIANT COMPONENT 
In this section we consider the behavior of G(n, M) in the "early supercritical 
phase", when M = n/2 + s and n2/3 «; s < n. In particular, we study 
the structure of the giant component soon after it emerges. Unfortunately, 
most of the results given here have lengthy and complicated proofs; thus, this 
part of the chapter consists mainly of heuristic arguments which, hopefully, 
shed some light on the nature of this intriguing period of the evolution of the 
random graph. 
We already know that in the supercritical phase the largest component is 
complex, but what we can say about its excess κ(η,Μ)7 
Note that a.a.s. no 
new complex component can emerge in the supercritical phase (Theorem 5.8). 
Thus, an edge added to G(n, M) in the supercritical stage can increase the 
value of κ(η, Μ) only by one, if it either connects two vertices of the largest 
component, or joins the giant to one of the unicyclic components. We first 
estimate the number κ'{η,Μ) 
of edges which, at the moment they are added to 
the graph, have both ends in the largest component of the graph. It is not hard 
to check (Exercise!) that for M0 = n/2 + 0(n 2/ 3) we have Εκ'(η,Μ 0) = 0(1) 
and thus κ'(η, Mo) = Op(l). Hence, it is enough to study the evolution of 
G(n,M) f o r M - n / 2 » n 2 / 3 . 
Let Mi = n/2 + i. Clearly, κ'{η,Μ) 
is the sum of the random variables 
Xi, where Xi = 1 if the edge added at the Mj-th stage of the random process 
is contained in the largest component of G(n, M¿ - 1), and Xi = 0 otherwise. 

122 
THE PHASE TRANSITION 
Since we restrict ourselves to the supercritical period of the random graph 
process, when, by Theorem 5.12 and (5.18), the size of the largest component 
of G(n, Μχ) is close to 4i, we have 
EXi = P(X, = 1) ~ ,„> ( ¿ , )
/ 0 
. ~ ^ 
, 
(") - n/2 - i 
n2 
and thus 
E«'(n,M)~ 
± 
EX,. 
¿ 
^ ~ ^ · 
i=ln 2/ 3] 
«=ln2/3J 
Furthermore, it is easy to check that if M¿ = n/2 + i and i > n 2 / 3, then 
θ ρ(η 2/ϊ 2) vertices of G(n, M¿ - 1) belong to unicyclic components. Thus, 
arguing as before, the expected number of edges added to the process before 
the moment M = n/2 + s, which joined the largest component of the graph 
with one of the unicyclic components can be bounded from above by 
t n?r("W"'))<:^'· 
„μ,,,ϋ-»/ 2-· 
Thus, we expect that if M = n/2 + s and n 2/ 3 <g. s <g. n, then the value of 
κ(η, Μ) should be about 16s3/3n2. The following theorem by Luczak (1990c) 
states that this is indeed the case. (See also Janson, Knuth, Luczak and Pittel 
(1993, Lemma 5) for a sharper concentration result when s < 0.5n3/4.) 
Theorem 5.13. Let ω = ω(η) -> oo and M = n/2 + s, where s > n2/3u> but 
s < η/ω. Then, with probability at least 1 - ω~0Λ, 
Remark 5.14. The random variable κ(η, Μ), appropriately normalized, has 
asymptotically normal distribution (see Janson, Knuth, Luczak and Pittel 
(1993, Theorem 13); one can also show this fact using the "straightforward 
approach" described at the end of the previous section). Then, for the stan-
dardization of κ{η,Μ), 
instead of 16s3/3n2 one should use the more precise 
expression 2(s2 - s2)/(n + 2s) with s defined by (5.16), which closely approx-
imates the value of /c(n, M) for all M = n/2 + s, where n 2/ 3 <SC s = 0{n). 
Before we describe the internal structure of the giant component let us 
recall that the 2-core or simply the core of a graph G, denoted by cr(G), is 
the maximal subgraph of G with minimum degree two. By the kernel ker(G) of 
a graph G whose components are all complex we mean a multigraph, possibly 
with loops, obtained from the core of G by replacing each path whose internal 
vertices are all of degree two by a single edge (see Figure 5.1). Note that the 

THE STRUCTURE OF THE GIANT COMPONENT 
123 
Fig. 5.1 A graph, its core and its kernel. 
minimum degree of a kernel is at least three and both the core and the kernel 
of G have the same excess as the graph itself. 
Let us consider first the behavior of | ciL(n, M)\, the random variable which 
counts vertices in the core of the largest component of G(n, M) for M = 
n/2 + s, where n 2/ 3 <& s <£. n. Theorems 5.12 and 5.13 state that for such 
an M the largest component of G(n, M) has k = (4 + op(l))s vertices and 
k + £ edges, where £ = (16 + op(l))s3/3n2. Furthermore, once we condition 
on the event that the largest component C(n, M) of G(n, M) has vertex set 
V and k +£ edges, all connected graphs with vertex set V and k + ί edges 
are equally likely to appear as C(n,M). 
Thus, the size and structure of 
| crL(n, M)\ should be similar to that of the core of C(fc, £), the graph chosen 
uniformly at random from the family of all connected graphs with vertex set 
[k] and k + £ edges, provided k ~ 4s and £ ~ 16s3/3n2. One can show that 
if i = ¿(k) tends to infinity as k -¥ oo, then with probability tending to 1 
as k -¥ oo, all except a negligible fraction of edges in the core of C(fc, i) lie 
on cycles. Hence, since there are no cycles outside the core, the number of 
edges in the core of C(Ar, ί) should be roughly the same as the number of edges 
of C(fc, I) whose removal does not disconnect the graph. Note that deleting 
such an edge from a connected graph on k vertices and k + I edges results 
in a connected graph on the same number of vertices and Jfc + I - 1 edges. 
Furthermore, each such graph can be obtained as the result of this deletion 
procedure from precisely (*) - k — I + 1 connected graphs with k + £ edges. 
Hence, the average size of the core of C(Jb, £) should be close to 
(k\C(k,t-l) 
\V 
C(k,£) ' 
which, because of (5.19) and the comment following it, is roughly equal to 
V^kl. 
Consequently, as long as £{k) = o(k), we may expect that the core of 
C(k,£) contains about \/Zk£ — ί ~ \l"ik£ vertices, which, in turn, gives 8s2¡n 

124 
THE PHASE TRANSITION 
as an estimate for |cr ¿(n,M)|, for M = n/2 + s and n1^ < s <S n. As we 
will soon see this is indeed the case. 
Since so far our intuitive argument works quite well, let us proceed further 
and estimate the number of vertices of degree three and four in cr ¿(n,M). 
Recall that /c(n,M) = (l+o p(l))16s 3/3n 2, so the number of vertices of degree 
at least three in the core (and thus in the kernel) is at most 2/t(n, M) ~ 
32s3/3n2. 
Furthermore, if the core of the largest component has roughly 
8s 2/n vertices, the average size of a tree rooted at any vertex of the core 
is about n/2s. A moment of reflection reveals that the most probable way 
to obtain a vertex of degree four in the core of the largest component is to 
connect a vertex from a tree rooted at a vertex of degree three in the core 
with some other vertex of the giant. The probability that it happens at the 
M-th stage of the process, where M = n/2 + s, should be close to 
32a3 n_ ι fn\ 
128s3 
S 3n2 2s / \2) 
~ 
3n3 ' 
Thus, the expected number of vertices of degree at least four in the core 
created in the process in the first M = n/2 + s stages can be approximated 
by 
γ > 
128r3 
32s4 
¿-" 
3n3 ~ 3n3 ' 
t=|n*/3j 
Note that the number of vertices of degree at least four in the core is, for 
s = o(n), much smaller than the number of vertices of degree at least three. 
Thus, we should expect that the core contains roughly 32s3/3n2 vertices of 
degree three in it. Observe also that the expectation of the number of vertices 
of degree at least four tends to 0 for s <g: n3/4; thus, they do not appear in the 
process until the moment when M = n/2 + Ω(η3/4). The same heuristic can 
be used to guess the number of vertices of degree t in the core. Nonetheless, we 
should mention that the following result from Luczak (1991a), which confirms 
our speculations, has been proved by somewhat different techniques. 
Theorem 5.15. Let M = n/2 + s, where n2/3 <& s <g.n, and, for i > 2, let 
Di = Di(n,M) 
denote the random variable which counts vertices of degree i 
inthecoreofG(n,M). 
Then 
Q s2 
£>2 = (l + o„(l))— , 
12 -s3 
D 3 = ( l + o p ( l ) ) - ^ . 
Moreover, for a given i > 4, £>< = Op(s'/ni~1). 
If s < n 1 _ 1 / i, a.a.s. 
no vertex o/G(n,M) has i neighbors (or more) in cr(G(n,M)), while for 
ni-i/t ^ 
s <g- n^ we 
¿ j a v e 
2 2 i s* 
Α = (1+ο„(1))τ 
PKX" 
i\ n · - · 

THE STRUCTURE OF THE GIANT COMPONENT 
125 
Remark 5.16. Since in the supercritical phase a.a.s. G(n,M) 
contains no 
complex components except the giant one, a.a.s. all vertices which have more 
than three neighbors in the core belong to the largest component of the ran-
dom graph. Furthermore, the expected number of vertices that belong to the 
cycles in the unicyclic components is of the order n/s (Exercise!). Thus, in 
the supercritical phase |cr(G(n,M))| = (1 + o p(l))|cr L(n,M)|, and the vast 
majority of the vertices of G(n, M) which have two neighbors in the core of 
G(n, M) is contained in its largest component. 
The above result implies that in the supercritical phase, as long as n2/3 <S. 
s < n, most of the vertices of the kernel of the largest component of G(n,n/2+ 
s) are of degree three. A stronger 3-regularity principle was shown by Luczak 
(1991a). It states, roughly speaking, that if M = n/2 + s and n 2 / 3 <£.s <&n, 
then G(n, M) contains an induced topological copy of a random cubic graph on 
(32 + op(l))s3/3n2 vertices, where, clearly, all vertices of this copy of degree 
three must belong to ker(G(n, M)). 
On the other hand, as was proved by 
Robinson and Wormald (1992) (see Theorem 9.20) a.a.s. the random cubic 
graph contains a Hamilton cycle. Thus, a.a.s. there exists a cycle that goes 
through nearly all vertices of the kernel, and thus contains roughly two-thirds 
of its edges. However, vertices of degree two in the core are placed on the edges 
of the kernel in a random manner. Thus, roughly two-thirds of all vertices 
of ciL(n, M), that is, about 16s2/3n of them, should lie on the cycle which 
corresponds to a Hamilton cycle of the kernel. On the other hand, each cycle 
must be contained in the core, so it cannot have more than |cr L(n,M)| = 
(8 + op(l))s2/n vertices. In fact, we can slightly improve this bound, since at 
least one-third of the paths which connect vertices of degree at least three in 
the core do not belong to such a cycle. Thus, after some computations, one 
can arrive at the following result (Luczak 1991a). 
Theorem 5.17. Let M = n/2 + s, where n2/3 « s «C n. Then the length 
of the longest cycle in G(n,M) lies between (16 + op(l))s2/3n and (7.496 + 
op(l))52/n. 
■ 
The above theorem estimates the length of the longest cycle contained in 
the giant component; we conclude this section with a few words about other 
cycles that emerge in G(n, M). As one can expect, the subcritical case is easy 
to study. If M = n/2 - s and n2^3 <C s < n the largest unicyclic component 
of G(n,M) is of size Q p(n 2/s 2), and the longest cycle has length 
Qp{n/s). 
(Exercise! Hint: use (5.9). See also Remark 5.16.) It is not much harder 
to check that in the supercritical phase, when M = n/2 + s and s » n 2/ 3, 
the size of the largest cycle of G(n, M) which is not contained in its largest 
component is Qp(n/s) 
as well (Exercise!). Hence, in the supercritical phase, 
typically the size of the largest cycle outside the largest component decreases 
when the process evolves; thus we infer that the unicyclic components with 
long cycles must merge with the largest components quite quickly and, con-
sequently, the shortest cycle inside the giant must be of length Op{n/s). 
It is 

126 
THE PHASE TRANSITION 
somewhat surprising at first sight that at the same time the largest compo-
nent of G(n, M) contains no cycles that are much shorter than n/s. 
Hence, 
in the supercritical phase, basically all long cycles are contained in the giant 
component, while all short ones lie outside it (Luczak 1991a). 
Theorem 5.18. Let s = s(n) be a function such that n 2/ 3 <C s «C n. 
(i) If M = n/2 - s, then the length of the longest cycle in G(n, M) is 
θ ρ ( η / β ) . 
(ii) If M = n/2 + s, then the length of the longest cycle not contained in 
the largest component o/G(n,M) is &p(n/s). 
Furthermore, the same is 
true for the length of the shortest cycle contained in the giant component 
o/G(n,M). 
■ 
5.5 NEAR THE CRITICAL PERIOD 
Now (at last!) we have a quick look at the critical phase, crucial for the 
phase transition phenomenon, when the giant component is just about to 
emerge. From Theorems 5.8 and 5.12 we know that this is the only time of the 
evolution when more than one complex component may simultaneously appear 
in G(n,M). Theorem 5.8 tells us also that all complex components created 
in the critical period must be quite large and they are not very numerous. 
Let us first show that all of them are of size Op(n2/3) (in fact, θ ρ(η 2/ 3) 
see 
Theorem 5.8) and their total excess is bounded in probability. 
Theorem 5.19. Let M = n/2 + 0(n2/z) 
and and let rt denote the num-
ber of i-components 
in G(n,M). 
Then Σ(>3&ι 
= Op(l) 
and all complex 
components o/G(n, M) have Op(n2^3) vertices combined. 
Proof. Let ω = ω(η) be any function such that ω -> oo as n -► oo but, 
say, ω = o(logn), and let M+ = n/2 + ω 1 / 4η 2 / 3. Consider G(n,M+) as 
the Af+-th stage of the random graph process {G(n, M)}M . Theorem 5.12 
states that a.a.s. G(n,M+) 
contains only one complex component of at most 
um2/3 vertices and Theorem 5.13 says that a.a.s. the excess of this component 
κ(η, M) is less than ω. Since all complex components that appear at the earlier 
stages of the process {G(n, M)}M are vertex-disjoint subgraphs of the giant 
component of G(n, M+), the assertion follows. 
■ 
Somewhat surprisingly, not much beyond what follows from Theorems 5.8 
and 5.19 can be said with probability tending to 1 as n -► oo about the 
component structure of G(n, AÍ) in the critical period. It turns out that with 
probability bounded away from both 0 and 1, every possible configuration 
not excluded by the above results can appear in G(n,M), provided we do 
not restrict the sizes of largest components up to a factor of 1 + o(l). In 
order to make this vague statement precise, let us consider a family of triples 

NEAR THE CRITICAL PERIOD 
127 
T = {i;a,b), where ί > - I is an integer and 0 < a < b < oo. We call such 
a triple regular if ί > 1, or (. = -1,0, but a > 0. Two triples T = (i;a,b) 
and T 
- (i';a',b') 
are non-intersecting 
if either i Φ I', or the intervals 
[a, 6] and [a',b'] are disjoint. For a positive constant d, a family of regular 
pairwise non-intersecting triples Γ = ((ίι;α\,^),(ί2;α2,Ι>2) 
· · · ,(£m;am,&m)) 
and a sequence of natural numbers t = (t\, ti,..., 
t m), let A(d, T, £) denote the 
event that G(n, M) contains precisely t¿ ¿¿-components whose sizes lie between 
din2/3 and 6<n2/3, for t = 1,2,... ,m, in G(n, M) there are no other complex 
components and, finally, no other isolated trees and unicyclic components of 
size at least dn2¡3 appear in G(n, M). The following result has been proved 
by Luczak, Pittel and Wierman (1994) and Luczak (1996). 
Theorem 5.20. Let c, d be constants such that -oo < c < oo and d > 0, 
and M = n/2 + en2/3. 
Then for every family of regular pairunse non-
intersecting triples T = (7Ί,Τ2,.. - ,Tm) and every sequence of natural num-
bers t = (t\, Í2. · · ·, tm), the limit 
p(c;d,T,t)= 
lim 
?{A(d,T,t)) 
n-«x> 
exists and 0 < p(c;d,T_,t) <l. 
■ 
The proof of Theorem 5.20 relies on precise estimates of the moments of an 
appropriate multidimensional random variable, and since it is long and quite 
technical we omit it here. 
A consequence of Theorem 5.20 (almost equivalent to it) is that if we denote 
the components of G(n, M), arranged in decreasing order, by Ci, C j , . . . , and 
X(n) is the sequence of pairs (n- 2/ 3|Ci|,e(Ci) - v{Ci)), (n~2/3|C2|,e(C2) ~ 
"(Cj)),..., then the random sequence X(n) converges in distribution, as 
n -* oo, to some random sequence X = ((X[,X"),(X2,X2),...). 
Aldous 
(1997) gave a proof of this form of the result (for G(n,p)) by a quite different 
method, which furthermore identifies the limit in terms of the excursions of 
a certain modified Brownian motion. Aldous's argument is based on expos-
ing the component structure vertex by vertex (as in the branching process 
argument in Section 5.2) together with martingale convergence techniques. 
It follows immediately from Theorems 5.17 and 5.18 that for M = n/2 + 
0{n2!3) 
each cycle contained in the largest component of G(n, M) has length 
Qpin1'3). 
As a matter of fact, since the core of a complex component is 
obtained from its kernel by randomly placing vertices of degree two on its 
edges, it is not hard to see that the following slightly stronger statement 
holds (Luczak, Pittel and Wierman 1994). 
Theorem 5.21. Let M = n/2 + 0(n2>3). 
Then any two vertices of degree 
three in the core o/G(n,M) lie at distance Qp(nl¡3) 
from each other. 
■ 
In particular, in the critical period a.a.s. G(n,M) contains no cycles with 
diagonals. We challenge the reader to find a heuristic argument that shows 
that such cycles appear in the process when M = n/2 + θ(η3^4) (Exercise!), 

128 
THE PHASE TRANSITION 
well before M = η/2+θ(π4/5) as the very first look at Theorems 5.15 and 5.17 
may suggest (for the limit probability of this property see Luczak (1991a)). 
Finally, let us say few words about the threshold for the property that 
a graph is planar, which was addressed already by Erdös and Rényi (1960) 
(see the comments on their result by Luczak and Wierman (1989)). Since for 
M = n/2 + 0(n2lz), 
a.a.s. all vertices of the core have degree two or three 
(Theorem 5.15), a.a.s. G(n,M) contains no topological copy of K$. Does 
it contain a copy of #3,3? Certainly not with probability 1 — o(l), since 
from Theorem 5.20 it follows that at any moment of the critical period, with 
probability bounded away from 0, G(n, M) consists only of trees and unicyclic 
components. However, for a given ί, once an ¿-component appears in G(n, M), 
with a non-vanishing probability its kernel can be any 3-regular multigraph 
with excess I. (Although it is not true that all such multigraphs are equally 
likely to appear as the kernel of such a component, for a small Í the corre-
sponding limit probabilities are easy to work out.) Thus, since Theorem 5.20 
implies that the probability that G(n, M) contains an ¿-component with i > 3 
is larger than some positive constant, the probability that G(n, M) is planar 
is also bounded away from zero. With some more work, one can prove the 
following, somewhat stronger result (Luczak, Pittel and Wierman 1994). 
Theorem 5.22. Let M = n/2 + cn2lz for some constant c. Then the prob-
ability that G(n, M) is planar tends to a limit ppi(c), where 0 < ppi(c) < 1. 
Furthermore, 
lim ppi(c) = 1 while 
limp Pi(c)=0. 
■ 
C - + —OO 
C—»OO 
Unfortunately, we do not know how to find the value of ppi(c), although, in 
principle, we can approximate it with arbitrary precision. For instance, the 
best known bounds for pp\ (0) are given by Janson, Knuth, Luczak and Pittel 
(1993, Theorem 8), who showed that 
0.987074 < pp,(0) < 0.999771. 
5.6 GLOBAL PROPERTIES AND THE SYMMETRY RULE 
So far we have dealt mainly with properties that hold a.a.s. for the random 
graph G(n, M); in this section we will be interested in properties of the random 
graph process {G{n,M)}M· 
Thus, for instance, instead of approximating 
some random variable Xn,M, defined for G(n,M), for a given function M = 
M(n), we will try to obtain uniform bounds for Xn,M which a.a.s. remain 
valid at many or all stages of the process {G(n, M)}M · 
Let us start with a simple example of one such global statement. 

GLOBAL PROPERTIES AND THE SYMMETRY RULE 
129 
Theorem 5.23. Let ω = ω(η) -> oo as n -¥ oo, with ω < n1/6. 
Moreover, 
for M = n/2 + s, let 
,2 
|„|3 
t±<"'M' = ( , ±te?b:)5? , o s^ 
(520) 
Then a.a.s. for the random graph process {G(n, M)}M the following hold: 
(i) if -η/ω < s < -ωη2Ι3, 
then 
fc-(n,M)<Li(n,M) 
<k+{n,M); 
(ii) if -ωη2'3 
<s< 
ωη2'3, 
then 
η2'3/ω2 
< Li(n,M) < 5ωη2/3 ; 
(iii) if ωη2,ζ < s < η/ω, then 
| L 1 ( n , M ) - 4 s | 
<u~1/4s. 
Proof. Let s¿ = ωη2/3(1 + 1 / logo;)*, where i = 0 , 1 , . . . , t and t is the smallest 
natural number for which st > η/ω. Furthermore, let M~ = n/2 - s¿ and 
Then, Theorem 5.6 implies that with probability at least 
i=0 
* 
for all i = 0 , 1 , . . . , t, we have k~(n,M~) 
< Li(n, M~) < k+(n, M~). Now it 
is enough to observe that for every M = n/2 — s, for which s¡_i < s < s«, we 
have k~(n,M~) 
< k~(n,M~) 
and k+(n,M) 
> ¿+(n,MiTL1). Hence, a.a.s. 
for each such M 
k~{n,M) < ^{η,Μ-) 
< Li(n,M) < ^(η,Μ^) 
< 
k+(n,M). 
In the very same way one can deduce (iii) from Theorem 5.12 and (5.18). 
Finally, the statement (ii) follows immediately from (i) and (iii), applied with 
s = ±ωη 2/ 3. 
■ 
The above argument strongly relies on the fact that the value of the ran-
dom variable L\{n, M) cannot decrease during the random process. Suppose, 
however, that we would like to show a similar uniform bound for the size of the 
second or, more generally, the r-th largest component of G(n, M) for a fixed 
r > 2. The subcritical phase is rather easy to deal with. Clearly, the uniform 

130 
THE PHASE TRANSITION 
upper bound for ¿i(n, M) given in Theorem 5.23 is at the same time the up-
per bound for LT{n,M). 
On the other hand, Theorem 5.6 and Theorem 5.23 
imply that a.a.s. the random graph process is such that for all M = n/2 - s 
for which s » n 2/ 3 we have Lr(n, M) < Lr(n, M + l). It is in the supercritical 
phase that the problem of studying LT(n,M) 
becomes interesting: now the 
size of LT(n,M) 
can increase when the r-th largest component merges with 
some small component, but the value of Lr(n, M) can also drop significantly, 
when one of the largest components is joined to the giant one. In this section 
we learn one method of dealing with such non-monotone random variables: 
the symmetry rule. 
Suppose that M = n/2 + s, where n2^3 <g. s <&n, and let GL (n, M) denote 
the graph obtained from G(n, M) by deleting all vertices of the largest compo-
nent. (In the unlikely case in which there are several components of maximum 
size we pick one of them uniformly at random.) Then, from Theorem 5.12 
and (5.18), GL (n, M) has n' = n - (4 ■+- op(l))s vertices. Furthermore, due to 
Theorem 5.13, it has 
M ' = n / 2 + s - ( 4 + op(l))s-Op(s) = n / 2 - ( 3 + op(l))s = n 7 2 - ( l + Op(l))s 
edges. Thus, properties of G1 (n, M) should be roughly the same as those of 
G(n', Λί'), where n' = (l+o(l))n and M' = n'/2-s. 
The following symmetry 
rule states this fact in a rigorous way. 
Theorem 5.24· Let A be any graph property. If M = n/2+s, where wn2'3 < 
s < η/ω for some function ω — ω(η) —> oo, then for n large enough 
P(GL (n,M) has A) < max{P(G(n', Μ') has A) : 
\n' - (n - 4s)| < ω~0Λ
3, 
\M' - (n'/2 - s)| < uT° 9s} + 8n 2/ 9
S- 1 / 3. 
Proof. Let n' and M' denote the number of vertices and edges in GL (n, M), 
respectively. Theorem 5.12 implies that, except for an event with probability 
at most 7n 2/ 9/s 1 / 3. 
Γη2'31 
fnJ/3l 
*=1 
* = ! 
( 5 2 2 ) 
r»"3l 
fn2/Sl 
M'= Σ (*-ΐ)Υη,*(*,-ΐ)+ Σ *y»,M(*,o). 
Assuming this, Theorem 5.11, with ω replaced by 83/2/ηω, and (5.18) show 
that, except with probability at most ηω/s3^2 < 
nl^3/s1^2, 
,„»_(„-4.)| < i + - ^ 
+Oc(-)=Oa(-). 
ω 
us1'* 
\ n / 
\ω/ 

GLOBAL PROPERTIES AND THE SYMMETRY RULE 
131 
Moreover, by estimates as in Section 5.3, it can be shown that if M — n/2 - s, 
then 
fn 2' 3l 
M{n - M) 
M 
where 
M{n - M) _ n 
16s^ 
M 
* 2 
S + 3n2 
+ °£)-i—°(á)· 
which, in fact, implies Theorem 5.13 when s is large, s > ωη5/6 (Exercise!). 
Thus by Chebyshev's inequality, again assuming (5.22), 
ψ-*-Γ5-)Ι>;)-°(τ)-^·β(τ)· 
Observe that the graph GL(n, M), once we condition on the number n' 
of its vertices and the number M' of its edges, can be viewed as a graph 
chosen uniformly at random among all graphs with n' vertices and M' edges 
in which the largest component is not larger than n-n' (a negligible additional 
weighting factor emerges in the case when the largest component of GL (n, M) 
has precisely n-n' vertices). Moreover, the values of n' and M' we are dealing 
with ensure that G(n', M') is in the subcritical phase of the evolution of the 
random graph. Therefore, we can use Theorem 5.6 with a = 1/3 to infer that, 
with probability at least 
1 _ [(n')2/("'/2 - M')3]1/3 > 1 - 2n2/3/s > 1 " Ο.δη2/9*"1/3, 
G(n',Μ') contains no component larger than n2/3, and the result follows. 
■ 
Thus, as we have already anticipated by looking at the formula (5.8), for 
n2/3 < s < n the graph GL(n,n/2 + s) behaves roughly like G(n,n/2 - s) 
with respect to any property A not vulnerable to small changes in size of the 
random graph (see also Theorem 5.18). It can be shown that an analogous 
relation between the structures of GL (n, n/2 + s) and G(n', n'/2 - s') remains 
true also for s — Ω(η). (However, in this case the dependence of n' and s' 
on n and s is more involved and the "mirror symmetry rule" is not valid any 
more.) Thus, the evolution of GL (n, M) in the supercritical phase is simi-
lar to the evolution of G(n, M) in the subcritical phase running backwards: 
for M/n -¥ oo all cycles in GL (n, M) disappear, the size of the largest com-
ponent of GL(n,M) decreases and, just before G(n, M) becomes connected, 
a.a.s. GL (n, M) consists of isolated vertices. However, the importance of the 
symmetry rule goes beyond the fact that it could give us better insight into 
the nature of the evolution of the random graph; it can be also a useful tool 
in studying global properties of the random graph process. 

132 
THE PHASE TRANSITION 
Theorem 5.25. Let r > 2 be a natural number which does not depend on n 
and let ω = ω(η) —> oo as n -+ oo. Moreover, for M = n/2 + s, let 
^"'"'-(^¡¿¡^á^· 
(523) 
Then a.a.s. t/»e random graph process {G{n, M ) } M is such that for all M — 
n/2 + s, where urn2/3 < \s\ < η/ω, we have 
k~{n,M) < Lr(n,M) 
<k+(n,M). 
Proof. As in the proof of Theorem 5.23, set s¿ = um2/3(l + l/logu;)*, where 
i = 0,1,...,t and t is the smallest natural number for which st > η/ω. 
Furthermore, let M~ = n/2 - s¿ and ^ ( n , M) be defined as in (5.21). Using 
Theorem 5.6 as before, a.a.s. for all i = 0,1,...,t we have k~(n,M~) 
< 
LT(n, M~) < k+(n, M~). Then, using the upper bound for I>i(n, M¿~) given 
by Theorem 5.23, and the "a.a.s. monotonicity" of Lr(n, M) in the subcritical 
phase discussed above, we infer that a.a.s. for every i = 1,2,..., i, and every 
M — n/2 - s for which «i_i < s < Si, 
k-(n,M) < k-{n,M-) 
< Lr(n,M") 
< Lr{n,M) < Lr{n,M-_i) < k+faM-.j 
< 
k+(n,M). 
Now set M* = n/2 + s¿ for i = 0,1,..., t. Then, arguing as before but 
now using Theorem 5.7, one can show that a.a.s. for all t = 0,1,..., t we have 
k-(n,M+) 
< ¿r(n,M,+) < 
fc+(n,M,+). 
Nonetheless, as we have already observed, this fact does not imply that 
k~(n,M) 
< Lr(n,M) 
< k+(n,M) 
for all M¿" < M < Mt
+, since dur-
ing the random graph process the largest components merge with the giant 
component and so the value of Lr(n,M) 
can go up and down. Thus, for 
t = 0,1,... ,t - 1, let A\ be the following property of a graph G: if we add 
to G Si+\ — Si edges, chosen uniformly at random from all pairs of vertices 
of G which have not yet become edges of G, then the probability that the 
largest component of such a graph is larger than k+(n, M+) = fc+(n, M~) is 
smaller than (n2s-3)i/20iogiogu, 
Similarly, for i = 1,2,...,t, let A'¡ denote 
the property that a graph G is such that if we delete from it Si - Sj_i ran-
domly chosen edges then the probability that the r-th largest component of 
the graph obtained in this way is smaller than k~(n, Mf) = k~(n, M~) is at 
most (n2
s-3y/2o log logo, ( W e d e f i n e ^ ¡ ^ ^« f ai s e i f t h e r e i s n o t r o o m i n 
G to add or delete this number of edges.) Let us emphasize that, although 
probability is involved in the definition of A'¡ and A", they are purely "de-
terministic" graph properties; for instance, A" says, roughly, that G contains 
a lot of large trees whose sizes do not drop rapidly when we delete from the 
graph a moderate number of randomly chosen edges. 

GLOBAL PROPERTIES AND THE SYMMETRY RULE 
133 
Now suppose that (1 -5/u/)n <ri <n and \s' -s¿| < ω~° 9Sj. We will show 
first that the probability that the property A\ does not hold for G(n', 
n'/2-s') 
is bounded from above by 
2 3 
\ 1/20 log log ω 
, 
2 \ ' /20 log log ω 
, 
n 2 s 3 
y / * M ° g . . g * 
/ n 2 v 
Indeed, otherwise the probability that the largest component of G(n', n'/2 -
s' - Sj + Sj+i) is larger than fc+(n, M~) would be bounded from below by 
/ 
n2S3 
v 1/20 log log ui ,
2 \ 
'/20 log log u; 
U ' + βί-β4+ΐ)β) 
W / 
/ 
2 
v 1/10 log log« 
= \(s'+ 
s¡ - 
si+l)*) 
contradicting Theorem 5.6. Similarly, the probability that G(n',n'/2-s') has 
A" is at least 
/ 
2 \ 1/20 log log ω 
1 -(5)" 
since otherwise the graph G(n, n/2 - s' - s¿ + s¿_i) obtained from G(n', n'/2 -
s') by deleting Si—a¿_i randomly chosen edges, would contradict Theorem 5.6. 
Now apply the symmetry rule (Theorem 5.24) to infer that for each i = 
0 , 1 , . . . , t — 1, with probability at least 
2 \ 1/20 log log u; 
»»,2/9 
/ 
2 \ */20 log log ω 
GL(n,M¡~) 
has property A\, and for each ι = 1,2,..., t, with probability at 
least 
2 \ 1/20 log logu/ 
1 -(5) 
the property A" holds for GL(n, 
Mi). 
Now, in the evolution of G(n, M) for M — M* to Mt^.j, we add Sj+i — s¿ 
edges. Some of them may have one or two endpoints in the largest component 
of G{n, Mi), which only may decrease the size of the largest component of 
GL{n, M), and since the property that a graph contains a component larger 
than k+[n, M+) is monotone, it follows that the probability that the largest 
component of GL (n, M) is larger than k+(n, M) > k+{n, Mf) for some M = 
n/2 + s with Si < s < si+i 
is bounded from above by the probability that 
we create a component larger than fc+(n,M+) if we add s¿+1 - s¿ randomly 
chosen edges to G¿(n,M^"). Thus, from the definition of A\ it follows that 
this probability is at most 
1/20 log log ω 
/ 2 \ 1/20 log log ω 
/ „ 2 \ l/20Ioglogu; 
/ _ 2 \ 

134 
THE PHASE TRANSITION 
Consequently, the probability that for some M = n/2 + s, where so < s < St, 
the largest component of G*" (n, M) is larger than k+(n, M) is bounded from 
above by 
t - l 
/ 
2 \ 1/20 log log ω 
= o(l). 
i — » 
/ 
9 \ 
' 
2 \ 1/20 log log ω 
For the lower bound, we instead for Λί,^., > M > M* regard G(n, M) as 
obtained from G(n, Mjí¡.1) by randomly deleting edges one by one. Some of the 
removed edges may have belonged to the largest component of G(n, M^j), 
but we continue and delete edges until si+i - s¿ of them have been removed 
from G¿(TI, Af¿í¡.j), and denote the resulting subgraph of GL {n,M^) 
by Hi. 
LetV = V{G{n,M^)) = V{Hi). 
If for some such M the r-th largest component of G(n, M) is smaller than 
k~(n,M) < fc~(n, M¿+i),sothat G(n, M) has less than r components of order 
at least k~ (n, M¿+i), then the same holds for the subgraph G(n, M)[V], and if 
we continue to delete edges, and assume that all components of GL (n, M¿lj.j) 
have orders less than 2¿~(n, Af¿+1), we see that also Hi has less than r com-
ponents of order at least k~(n, Mi+l). 
By the definition of Λ"+1, and ob-
serving that 2k~{n, Mj+i) > k+(n, Mi+i), it follows that the probability that 
Lr(n,M) < k~(n, M) for some M = n/2 + a, Sj < s < e¡+i, is at most 
(
2 \ 1/20 log log u; 
Summing over all i < t, we see that the the probability that Lr(n,M) 
< 
k~(n, M) for some M = n/2 + s, s0 < s < st is o(l) as n -> oo. 
■ 
5.7 
DYNAMIC PROPERTIES 
In the previous section we showed how to use the symmetry rule to extend 
results about G(n, M) to theorems about the behavior of the random graph 
process { 0 ( Π , Μ ) } Μ · Now we mention a few "genuine" properties of the 
random graph process which do not correspond to any property of G(n, M). 
The first problem we will consider is Erdös's question about the length £„ 
of the first cycle which appears during the random graph process. The limit 
distribution of ξη is given by the following result of Janson (1987) (see also 
Bollobás (1988b)). 
Theorem 5.26. For every j > 3 
lim Ρ(ξη =j) = I 
f't^e'^^VT^ldt. 
n-tco 
2 J0 

DYNAMIC PROPERTIES 135 
Proof. We just give the idea of the proof, omitting all technical details and 
computations which can be easily filled in by the reader (Exercise!). Thus, 
let η > 0. Choose j· > 3 and a large constant A. For i = 0 , 1 , . . . , [(1 - η)Α] 
set Mi = £¿71/2, where r¿ = i/A. 
One can apply the method of moments 
(Section 6.1) to show that for every such M< the probability that G(n, M,) 
contains no cycle tends to 
exp(-f>*/2Jfc) 
=et*'2+t1'*y/T=Ti. 
Furthermore, one can use Chebyshev's inequality to verify that for every i the 
number of pairs of vertices of G(n, M¿) connected by a path of length j - 1 is 
equal to (1/2 + o p(l))í¿ - 1n. Note that the property that a graph contains no 
cycle is decreasing and that the number of pairs of vertices joined by a path of 
length j - 1 can only increase when a new edge is added to the graph. Thus, 
since A can be chosen arbitrarily large, we infer that for every ε > 0 a.a.s. 
for every M = in/2, where 0 < t < 1 - n, the number of pairs of vertices of 
G(n, M) connected by paths of length j — \ is contained between ( l / 2 - e ) i j - 1 n 
and (1/2 + έ)ΐ>-χη. 
Moreover, if M = in/2, where 0 < t < 1 - η, then the 
probability that G(n, M) contains no cycle is (1+o(l))e'/ 2 + t / 4\/l - t. Hence 
the probability that in the first (l-n)n/2 stages of the process a cycle of length 
j appears as the first one is given by 
( l - 7 j ) n / 2 , - , - . 
, j _ , ._ 
(1 + 0(1)) 
Σ 
W r y 
"/V/n+Wn)^ _ 2 M / n 
= (\+o(l)) j1'" t'-le^+t2^VT^idt. 
Thus, letting η -► 0, for each j > 3 we get 
lim inf Ρ(ξη = j) >Pj = l f ί > - ν / 2 + ί ' / 4 ν / Γ ^ 7 dt. 
(5.24) 
Π-Κ30 
2 J0 
However, it is easy to check that Y,j>3 Pj = 1, thus (5.24) implies that 
lim Ρ(ξη = j) = Pi 
n-+oo 
for every j > 3. 
■ 
Let us remark that the limit distribution of ξη has infinite expectation; 
thus Ε ζ η -> oo and, since Εξ η < n, one can ask about the order of E£ n. 
Flajolet, Knuth and Pittel (1989) computed all the moments of the length of 
the first cycle and of the size of the component containing it; in particular, 
they showed that 
π ^ Γ ( 1 / 3 ) 
/ 6 

136 
THE PHASE TRANSITION 
The proof, based on the study of the behavior of a certain generating function, 
is much harder than that of Theorem 5.26, thus we do not give it here, but refer 
the interested reader to the original paper (see also Janson, Knuth, Luczak 
and Pittel (1993, Section 26)). 
Two other problems we would like to mention in this chapter concern the 
way the giant component emerges. Erdös proposed that we view the evolving 
graph as a "race of components". In order to express his idea in a rigorous 
way, we define recursively the leader of this race. For a graph with only one 
edge, the leader is the only component with two vertices. Now suppose that 
we add a new edge e to a graph G, in which one of the largest components, 
say, H, has been chosen as the leader. Then, e may miss the leader and 
connect two other components whose combined size is larger than \H\; in 
this case we say that a change of leader occurred and nominate the newly 
created largest component to be the leader of the graph. In all other cases 
the leader of G + e is the component that contains all the vertices of the "old" 
leader H. Erdös asked when in the random graph process the last change of 
leader takes place. Note that Theorems 5.23 and 5.25 imply that a.a.s. no 
changes of the leader occur when n 2/ 3 <SC M — n/2 < n, and the reader is 
invited to check that the same remains true for all M = η/2+Ω(η) (Exercise!). 
However, it is, in principle, possible that already in the subcritical phase one 
component starts to dominate, although its superiority becomes evident only 
later. The following result by Luczak (1990c), which we give without proof, 
shows that this is not the case, and that the last change of leader occurs at 
M = n/2 + Op(n2/3). 
Theorem 5.27. Let Lead(n, M) denote the largest number r such that the 
leader of G(n, M) does not change in the next r stages of the random graph 
process {G(n, M)}M . 
(i) Ifn2/3 
<Zis<g.n, then 
L e a d ( n ^ - S ) = e p ( l o g ( ; 3 / n 2 ) ) . 
(ii) If s » n2/3, then a.a.s. Lead(n, n/2 + s) = (£) - n / 2 - s , in other words, 
the largest component of G(n, n/2 + s) will remain the leader until the 
very end of the random graph process {G(n, M)}M · 
■ 
Finally, let us look once again at the number of complex components cre-
ated during the process {G(n, M)}M · Theorem 5.8 says that a.a.s. all complex 
components are created in the critical phase when M = n/2 + 0(n2^3). 
What 
is the probability that only one complex component appears in the process 
{G{n,M)}\¡ 
and no other complex component ever emerges? Theorem 5.8, 
the remark following it, and Theorem 5.20 imply that this probability tends 
to a limit u, where 0 < u < 1. In order to find the value of u, we describe the 
changes of the structure of G(n, M) during the critical period in yet another 
way, as a Markov chain whose stages are "graph configurations". 

DYNAMIC PROPERTIES 
137 
Thus, we say that [ri,r2,... ,r,] is the configuration of a graph G, if G 
contains precisely r¿ ¿-components for I — 1,2, ...,q and no ¿-components 
with i > q appear in G. It is not hard to see that if we add a new edge to 
a graph the value of £ , Irt increases by at most one; for instance, adding a 
juncture as an edge to a graph changes its configuration [ΓΙ,Γ2, .. .,rq] into 
[n + 1 , Γ 2 , . . · , Γ , ] . 
The following "switching theorem", proved in Janson, 
Knuth, Luczak and Pittel (1993) by a careful analysis of an appropriate gener-
ating function, describes quite precisely the way the configuration of G(n, M) 
evolves during the random graph process. 
Theorem 5.28. Let rx + 2r2 + · · · + qrq = r and δ\ + 2δ2 Η 
= 1. Then 
the limit probability that in the random graph process {G(n, M)}M the config-
uration [r\, r 2,..., rg] is followed by [ri + ¿i, r2 + ¿2, · · ·, i~q + 6q,Sq+i,...] 
is 
equal to 
f/(3r + i)(3r+|) 
if Í, = 1 
9JÜ + l)r,7(3r + l)(3r + |) 
if Sj = - 1 , Sj+l = 1 
9; 2Γ,(Γ, - l)/(3r + i)(3r + | ) 
if 
S¡ = - 2 , S2j+l = 1 
18;*Γ,·Γ*/(3Γ + i)(3r + §) 
if 6j = - 1 , Sk = - 1 , 6j+lc+1 = 1, j < k 
and there are no other possibilities. Moreover, asymptotically these probabili-
ties are independent of the history of previous configurations. 
■ 
Note that, in a way, the above theorem nicely supplements Theorem 5.20. 
Theorem 5.20 implies that, in particular, for any sequence ri,...,rq 
(e.g., 
0,2,1) of natural numbers, with probability bounded away from zero, for 
some M the graph G(n, M) has configuration [ri,..., rq] (e.g., [0,2,1]). The-
orem 5.28 states that we may choose not only such a configuration but also 
the way it was created (e.g., [1] -> [0,1] -► [1,1] -► [0,2] -> [0,1,1] -+ [1,1,1] 
-> [0,2,1]); still the probability that the configurations of G(n, M) during the 
random graph process followed such an "evolutionary path" tends to po > 0 
as n -> oo, where po = Po(r\, · · ·, rq) can be explicitly computed. 
Now the question about the limit probability that during the random graph 
process no two complex components appear at the same time, finds its sur-
prisingly simple answer. 
Theorem 5.29. The probability i/(n) that in the process {G(n,M)}M 
for 
every M, 0 < M < ("), G(n,M) 
contains at most one complex component 
tends to 
»-Π 
9ί{ί +1) 
5π 
η „„„ 
v 
' 
= — = 0.872.. 
^ (3¿ + l/2)(3¿ + 5/2) 
18 
as n -> oo. 
Proof. Theorems 5.8, 5.10 and 5.13 imply that for every e > 0 there exists 
a constant C = C(e) such that the following holds: with probability at least 

138 
THE PHASE TRANSITION 
1 -e, the random graph process is such that if in G(n, M) an ¿-component with 
I > C appears, it will remain the only complex component of the graph until 
the very end of the random graph process. Thus, the probability i/(n) can be 
approximated up to ε by the probability that the first C steps of the evolution-
ary path are [1] -v [0,1] ->■ [0,0,1] -► 
► [0,... ,0,1]. From Theorem 5.28 
the probability of this event tends to Π?=ι9^(^ + l)/(3€+ l/2)(3¿ +5/2). 
Now, to complete the proof, it is enough to let C -> oo. 
■ 

6 
Asymptotic 
Distributions 
Many questions for random combinatorial structures are qualitative; we ask 
whether some property is satisfied, for example, the existence of a certain 
substructure. Other questions are quantitative; we study some numerical 
characteristic of the random structure, for example, the number of copies of a 
certain substructure. Since the structure is random, this becomes a random 
variable and we may ask about its distribution. 
Exact formulas for the distributions of interesting combinatorial variables 
are rare, and even when they exist, they are often too complicated to be of 
much use. The main interest, therefore, centers on asymptotics and limit 
theorems. We will in this chapter describe several methods that have been 
used to prove such results, and illustrate their use with applications to random 
graphs. We concentrate on presenting the methods rather than presenting new 
results; thus we prove some results several times by different methods. 
The appropriate probabilistic notion is convergence in distribution, as de-
fined in Section 1.2. We will mainly state results on convergence in distribu-
tion of single random variables, but most of the results extend easily to joint 
convergence of several variables. For example, the Cramer-Wold device (see 
Section 1.2) applies with ease to extend all results in this chapter on normal 
convergence (Exercise!). 
Typically, random variables converge only after rescaling. We use special 
notation for the most natural and common choice: For a random variable X 
with finite non-zero variance, we define 
X = 
{X-EX)/{V&iX)1'2; 
thus X is standardized to have EX = 0 and Var X = 1. 
139 
Random Graphs 
by Svante Janson, Tomasz Luczak and Andrzej Rucinski 
Copyright © 2000 John Wiley & Sons, Inc. 

140 
ASYMPTOTIC DISTRIBUTIONS 
6.1 
THE METHOD OF MOMENTS 
The method of moments is one of the oldest methods to prove convergence in 
distribution, but it is still widely used, both because it is conceptually simple 
and because it is powerful and well adapted to combinatorial problems. The 
drawback is that it usually requires long and messy estimates. As a result, 
many theorems have first been proved by the method of moments but later 
reproved in more elegant ways by other methods. 
The moments of a random variable X are the numbers EX*, k > 1. We 
consider here only variables such that all moments exist, that is, E |X|* < oo 
for every fc > 0; we say that such random variables have finite moments. 
It is clear that the moments of a random variable are determined by the 
distribution. The converse does not always hold (see, e.g., Chung (1974)), 
but it holds in many important cases. We thus say that the distribution of 
X is determined by its moments if X has finite moments and every random 
variable with the same moments as X has the same distribution. A sufficient 
condition for the distribution of X to be determined by its moments is that 
the moment generating function E etx is finite for t in some interval around 0; 
in particular, this holds if X has a normal or Poisson distribution. 
The standard version of the method of moments can be stated as follows; 
see Chung (1974, Theorem 4.5.5) for a proof. 
Theorem 6.1. Let Z be a random variable with a distribution that is de-
termined by its moments. 
If Xi,X2,... &re random variables with finite 
moments such that EX"* -»· EZk 
as n -* oo for every integer k > 1, then 
xnAz. 
m 
The method of moments thus requires estimation of all moments of X n, 
which often leads to long calculations. Two versions of the method, which are 
formally equivalent but often more convenient for applications, are given in 
separate subsections below. 
The method of moments also applies to vector-valued variables, or, in other 
words, to joint convergence in distribution of several random variables; we now 
have to consider all mixed moments. We write Za = Zfl ■ ■ ■ Z¿* for vectors 
Z = (Zi,...,Zd) 
and Q = (ai,...,Qd). 
Theorem 6.2. Let Z = (Z\,..., 
ZA) be a random vector with a distribution 
that is determined by its moments. 
If X„ = (X ni, · ··»Xnd) «,re random 
vectors with finite moments such that E X * -»· EZ a as n -> oo for every 
multi-index a = ( a i , . . . , a¿), then X„ -> Z. 
■ 
A particularly important case is convergence to a normal distribution. We 
recall that the semi-factorial n!! is defined to be n(n- 2) ■·■ 3 1 = (2m)!/2mm!, 
when n = 2m - 1 is odd. 

THE METHOD OF MOMENTS 
141 
Corollary 6.3. // ΑΊ, A'2,... are random variables with finite moments and 
an are positive numbers such that, as n —> oo, 
VI V 
, ν ^ . ί ^ - 
l)"an 
+ °(°n). 
«*«»» k > 2 "
 
eVeTt> 
Ε.(Λ π-Β.Λ η) 
- | o ( a * ) f 
« Λ « ι * > 3 ώ « Μ , 
ífceno- l(X„-EX„) 4 N ( 0 , 1 ) and X„ A N(0,1). 
Proo/. Let X € N(0,1) be a standard normal variable and let mk = E l ' , 
k = 1,2,..., be its moments. Then, as is well known, m* = (k - 1)!! when k 
is even and m* = 0 when k is odd. 
Let Yn = a^iXn 
- EXn). 
Then EYn = 0 = mi and, by the assumption, 
EV„* -¥ mk as n -> oo for every fc > 2. Consequently, Vn -» N(0,1) by 
Theorem 6.1. 
Finally, by the assumption with k = 2, V a r ^ n ) / ^ -> 1 and, thus, using 
Cramer's theorem (Section 1.2), Xn = (a?J VaxXn)l/2Yn 
4 N(0,1) too. 
■ 
Example 6.4. Consider the subgraph count XQ in G(n,p), where n -* oo 
and p = p(n) is a function of n. Asymptotic normality under various condi-
tions has been shown by several authors. The complete result (Rucinski 1988) 
is as follows; as is shown after the proof, the conditions on p are necessary 
too. 
Theorem 6.5. Let G be a fixed graph with e<j > 0. If n —^ oo and p — p(n) 
is such that npm^G) -* oo and n2{\ - p) -» oo, then Xa -*■ N(0,1). 
Proof. There are (")t;!/aut(G) copies of G in the complete graph Kn; for 
each such copy G' we define, as in Chapter 3, the random indicator variable 
IG' = 1[G' C G(n,p)]; note that EIG· = Pe°■ Then XG = Σα· 
JG' and thus, 
for every m > 1, 
E ( A - G - E A - G r = 
Σ 
E ( ( / 0 l - E / 0 t ) ( / 0 a - E / C a ) - - - ( / C m - E / C m ) ) , 
G, 
Gm 
(6.1) 
summing over all m-tuples G i , . . . , Gm of copies of G in Kn. Let us write 
T ( G i , . . . , G m ) = E ( ( / G l - E / G l ) . . . ( / 0 m - E J 0 m ) ) , 
and for each such term in the sum, define a graph L = L(G\,... 
,Gm) with 
vertex set { 1 , . . . ,m} and an edge ij whenever d and Gj have at least one 
edge in common. Thus L is a dependency graph for the variables 7 G,,..., / G m, 
see Example 1.6. 
We now group the terms in the sum in (6.1) according to the structure 
of the graph L. Consider first the case when m is even and L consists of 
m/2 disjoint edges. There are (m - 1)!! such graphs L, and each gives the 

142 
ASYMPTOTIC DISTRIBUTIONS 
same contribution to the sum in (6.1); moreover it is easy to see that each 
contribution is (VarA'c)m/'2(l + 0(l/n)). We claim that every other graph 
L with m (even or odd) vertices contributes o(VarXc) m/ 2; since there are 
finitely many possible L for each m, the result then follows by summing over 
L and applying Corollary 6.3, with a2 = VarXo· 
In order to verify the claim, observe first that if L has an isolated vertex i, 
then every term T(G\,... 
,G m) yielding the graph L vanishes, because then 
lot - E / G , is independent of the product of the other factors. Consequently, 
such L give no contribution at all. 
In the remaining cases, every component of L has at least two vertices, 
and some component has at least three. We let c = c(L) be the number of 
components; it follows that c(L) < m/2. Moreover, we may, for convenience, 
assume that the indices are reordered in such a way that the components of 
L have vertex sets {1,.. . , η } , {τ-χ + 1 , . . . , ^ } , ... , {TC-I + l,...,r c = m}, 
and that if j $ {Ι,τχ + l,r 2 + 1,. ..,r c_i + 1}, then L contains an edge ij 
with i < j . 
Consider a term T(Gi,... ,G m) in (6.1) with such an L, let Gu) = UÍ G». 
and let Fj be the (possibly empty) subgraph of G which corresponds to 
Gf(j-i) n Gj under an isomorphism Gj S G. Note that by our assumption 
on L, e(Fj) = 0 exactly when j € { Ι , π + 1,Γ2 + 1,. ..,r c_i + 1}. 
If P < 1 /2, we estimate the term T(Gi,..., 
Gm) by taking absolute values: 
| r ( G l f . . . , G m ) | < E ( ( / C l + E / 0 l ) . - . ( / C m + E / C m ) ) . 
The product can be expanded as a sum of 2 m terms, and it is easily seen that 
among them, /G, · ■ ■ IG„ has the largest expectation, namely, p e ( G m \ Thus 
| T ( G 1 , . . . , G m ) | < 2 m E ( / G l - - - / G m ) = 0(pe(G(m>)). 
If 1/2 < p < 1, we instead use the estimate 
c(L) 
|r(G1>...,Gm)|<ElI|J0rt-E/o,J, 
fc=i 
keeping only one factor for each component of L. These c{L) factors are 
independent, and each has the expectation 
E | / G r -EIGA 
= 2p e c(l -pea) 
< 2(1 - p e o ) < 2eG(l - p ) . 
Consequently, 
T(G 1,...,G m) = 0 ( ( l - p ) c W ) . 
We may combine the two cases by introducing redundant factors in each 
of the estimates; thus, for all p € [0,1], 
T(GX 
Grn) = 0 ( ( l - p ) c < ¿ y ( G < m , ) ) . 

THE METHOD OF MOMENTS 
143 
Now, as is easily seen, e(GU)) = jea - Σ\ e(Fi), 
so> in particular, e((7(m)) = 
mea - ETeiFi)- 
Similarly, v(G{m)) = mvG - ΣΤ v(Fi) 
a n d t h u s t h e r e a r e 
at most 0(nmVG~^^v^Fi)) 
possible choices of Gi,...,Gm 
yielding L and a 
particular sequence F\,..., Fm. Consequently, fixing L and F\,..., Fm gives 
a contribution to the sum in (6.1) of the order 
0(η™<;-ΣΓ«(*>(! _p)c(¿y«e-Er«<*>). 
(6.2) 
We next recall that c{L) of the F¿ have no edges, and thus n ^ V ^ ' 
= 
„»<*) > 1; w h i j e t h e m _ ,,(£) o t h e r s h a v e e(/ri) > i a nd thus n^F^pe{Fi) 
> 
®(XF¡) > Φο, cf. (3.7). Hence, using Lemma 3.5, 
nmvc-T.T 
V(F<)(1 _ p)c(i.) pme c-E7 e(F.) 
m 
= (1 -p)c(L)(n"V0)mn(n"(F')Pe(F'))"1 
¿=i 
< (l-pyW(nv°P
e°)m/$™-c{L) 
x (1 
-Py(L\EXG)m*c¿L)~m 
x (VarX Gr/ 2((l -p)# G) c ( ¿ )- m / 2. 
(6.3) 
Now the assumptions npm(·0^ -> oo and n2(l -p) -» oo imply (1 -ρ)Φσ -*· 
oo. Indeed, as remarked in Chapter 3, npm^ 
-+ oo is equivalent to Φβ -+ oo, 
which implies (1 — ρ)Φσ -+ oo provided p < 1/2. On the other hand, when 
p > 1/2, Φα ~ n2, and thus (1 - ρ)Φο ~ rc2(l - p) -> oo. 
Moreover, as observed above, c(L) < m/2. Consequently, by (6.2) and 
(6.3), the contribution to the sum in (6.1) by terms corresponding to L 
and Fi,...,F m is o((VarAc)m/'2)· Summing over the finitely many possi-
ble sequences F\,..., Fm, this verifies our claim that the contribution by L is 
o((Var Xa)m/2), 
which completes the proof as shown above. 
■ 
To see that the conditions np™1^ -> oo and n2(l-p) -> oo are necessary for 
the conclusion, we observe that if they are violated, then for some subsequence 
either npm^ 
-> a < oo or n2(l - p) -> b < oo. In the first case (along the 
subsequence) βυρΦσ < oo and thus, by Theorem 3.9, inf P(XG = 0) > 0; 
in the second case P(G(n,p) = Kn) -> e -^ 2 > 0, and thus inff{XG 
= 
f(n, G)) > 0, where /(n, G) is the number of copies of G in the complete 
graph Kn. In both cases XQ thus assumes a single value with probability 
not tending to zero, which obviously rules out asymptotic normality (for any 
normalization). 
Remark 6.6. It is easily seen by the argument above that the conditions 
npm(G) -> oo and n2(l — p) -* oo are together equivalent to (1 — ρ)Φσ -> oo. 

144 
ASYMPTOTIC DISTRIBUTIONS 
Factorial moments 
The factorial moments of a random variable X (with finite moments) are the 
numbers 
E(X)k = E[X(X-l)·· · ( * - * +I)), 
k>0 
(with E(X)o = 1). Since the monomials {xJ}k
=0 and the (descending) facto-
rials {(i)j}*=0 form two bases of the vector space of polynomials of degree at 
most k, there exist numbers akj and bkj (independent of X), such that 
k 
E(X)k = YiakjEX' 
3=0 
and 
k 
EXk = 
YibkjE(X)j 
i=o 
for every random variable X with finite moments. (The coefficients akj and 
bkj are the Stirling numbers, see, e.g., Graham, Knuth and Patashnik (1989), 
but their values do not matter here.) It follows that if Χι, Χ2, ■. ■ and X are 
random variables, then EX* -> EXk for every k if and only if E(Xn)k -* 
E(X)t for every k. Consequently, Theorem 6.1 can be reformulated as follows. 
Theorem 6.7. Let X be a random variable with a distribution that is de-
termined by its moments. If Χι,Χτ,... 
are random variables with finite 
moments such that E(Xn)k -> Ε(Χ)* os n -> 00 for every integer k > 1, then 
x„Ax. 
u 
This form of the theorem is particularly convenient for proving convergence 
to a Poisson distribution, since the factorial moments of X € Ρο(λ) have the 
simple form E(X)* = Xk, k > 0. (In contrast, the moments of a Poisson 
variable have a more complicated form.) 
The method of moments is often applied to counting variables of the form 
S — ]Coe.A ^α' w^ e r e ^α are indicator variables; in this case (S)k counts the 
number of (ordered) fc-tuples of objects with Ia = 1, that is, 
(^)* = zJ 
AM ' * · Ό» 1 
£«i 
Q* 
where £* denotes summation over all sequences of distinct indices αι,..., a*, 
and thus the factorial moments have the useful expression 
E(S)k = Σ * E(/ai · · ·Iak) = £ * Ρ(/βι = ·· · = IQh = 1). 
(6.4) 
<Μ,...,αι 
α\,...,αι, 

THE METHOD OF MOMENTS 
145 
Corollary 6.8. Let Sn = Σα€Α„ In,a be sums of indicator variables I„.a- If 
A > 0 is such that, as n -¥ oo, 
E(5n)* = ] £ * P(/„.0l = · · · = /„,„» = 1) -» A*, 
(6.5) 
αι,...,a* 
/or every fc> 1, thenSn 
A Ρο(λ). 
■ 
Example 6.9. In Theorem 3.19, we studied the subgraph count XG of a 
strictly balanced graph G at the threshold {npmt>G) -> c > 0), and showed 
that then (6.5) is satisfied and thus XG A Ρο(λ), with λ = c"G/aut(G). 
With only minor modifications in the computation of the expectations, the 
same argument applies to the random graph G(n, M), with n(M/(")) m' G^ -+ 
c. 
Theorem 6.7 extends easily to multivariate limits. In particular, for Poisson 
limits we have the following result. 
Theorem 6.10. Let (X„ ',..., 
X„) 
be vectors of random variables, where 
m>\ 
is fixed. If X\,..., Am > 0 are such that, as n —► oo, 
E[(X(1))*I...(JC¿™))tm]-+A*».--A*r 
for every k\,..., 
km > 0, then (X{
n
l\...,X^m)) 
A (Zu...,Zm), 
where Z{ € 
Po(Ai) ore independent. 
■ 
We leave the corresponding extension of Corollary 6.8 to the reader (Exer-
cise!). 
Cumulants 
Suppose that X is a random variable with finite moments. Then the charac-
teristic function φχ{ϊ) = Ee,tx 
is infinitely differentiable, and 
ΕΧ" = 
Γ"^φχ(0). 
Similarly, log</?x(t) is infinitely differentiable in an interval around 0, and we 
define the cumulants (also known as semi-invariants) 
of X by 
Xk(X) = rk£¡;\og<px(0). 
In other words, φχ{ϊ) and log<¿>x(t) have the Taylor series £ ~ EXk&£- 
and 
H ^ X f c W ^ j - , respectively. 

146 
AS YMP TO TIC DIS TRIBUTIONS 
Remark 6.11. In general, these Taylor series do not have to converge for 
any ( φ 0. However, we are mainly interested in random variables such that 
Ee 1 | X | exists for some t > 0, and then 
0 0 
k 
°° 
k 
0 
1 
with sums converging at least for all complex z with |z| sufficiently small. 
Example 6.12. If X has the normal distribution Ν(μ,σ2), then ipx(t) 
= 
exp(i/ii - a2t2/2), 
and thus x\ — μ, x-¡ = σ2 and xk = 0, k > 3. 
Example 6.13. If X has the Poisson distribution Ρο(λ), we have ψχ(ί) 
= 
exp(X(eil - 1)), so logφχ{ϊ) = λ(ε" - 1) = £ Γ λ ^ 
and xk = λ, k > 1. 
It is obvious by successive differentiations of \θζφχ and φχ = exp(logi/>x) 
that there are simple algebraic relations between the moments and cumulants: 
Xk = qkfäX, ...,EXk) 
and EX* = q'k(x\,... 
,*ek), where qk and q'k are 
some polynomials not depending on X. Explicit expressions are easily given 
(c/. Proposition 6.16(vi, vii) below), but are not important for us, with the 
exception of 
x i = E X , 
(6.6) 
x 2 = E X 2 - (EX)2 
= VarX. 
(6.7) 
It follows that if Χχ,Χι,... 
and X are random variables, then EX* -> 
EX* for every k if and only if xk(Xn) 
-* Xk(X) for every k. Consequently, 
Theorem 6.1 can be reformulated as follows. 
Theorem 6.14. Let X be a random variable with a distribution that is de-
termined by its moments. 
If Χχ,Χ^,... 
are random variables with finite 
moments such that xk(Xn) 
—► Xk(X) as n —> oo for every integer k > 1, then 
Xn 4 X. 
■ 
Cumulants are particularly convenient for proving convergence to a normal 
distribution. It follows easily from the definition that if X is a random variable 
and a and b are real numbers, then 
»=fc 
i v > ΙΛ 
,axl(X) 
+ b, k = 1, 
xk(aX + b)= { 
iKxk(X), 
k > 2. 
Hence, Theorem 6.14 and Example 6.12 yield the following result for normal 
convergence. 
Corollary 6.15. // X i , X 2 , . . . are random variables with finite moments 
and a„ are positive numbers such that, as n -> oo, xi(Xn)la\ 
-> σ2 > 0 and 
Xk(Xn) = o(a*) when k>3, 
then a~l(Xn - EXn) Λ Ν(0,σ2) and, provided 
a2 > 0 , Xn 4N(0,1). 
■ 

THE METHOD OF MOMEN Γ5 
147 
Mixed cumulants 
For the results in the last subsection to be useful, we have to be able to 
compute, or at least estimate, the cumulants χ*(Χ η). This can often be done 
using mixed cumulants as follows. 
If Xi,..., 
Xk, k > 1, are random variables defined on the same probability 
space, then their joint characteristic function is 
Vx, 
x t(ii,...,t*) = E ( e x p ( ] £ « , * > ) ) . 
i 
If further X\,..., 
Xk have finite moments, this function is infinitely differen-
tiable on R* and we define the mixed cumulants by 
x{xu...,Xk) 
= rk
du
dk
 
dtk\owXl 
Xk(o). 
Some basic properties of the mixed cumulants are collected in the next propo-
sition. We omit the simple proofs (Leonov and Shiryaev 1959). 
Proposition 6.16. For any random variables X, Χχ,... 
with finite moments 
(defined on the same probability space), the following holds: 
(i) ><k(X) — x{X, ■ ■ ■, X), where X is repeated k times. 
(ii) x{Xi,...,Xk) 
is symmetric, i.e., κ(Χι,... 
,Xk) = κ(Χσ{1),... 
,X„(k)) 
for every permutation σ of {I,. 
..,k}. 
(iii) κ(αΧχ,Χ2,... 
,Xk) = ακ(Χι,Χ2,-. 
-,Xk), 
for any real a. 
(iv) x(X'l+X[',X2,...,Xk) 
= )<(X'l,X2,...,Xk) 
+ 
>e(X[',X2,...,Xk). 
(v) κ(Χ\,... 
,Xk) 
= 0 if {Χχ,... 
,Xk} 
can be partitioned into two (or 
more) non-empty sets of random variables which are independent of 
each other. 
(vi) x i X , , . . . , ^ ) = £ / , 
¡,(-l)'-l(l 
- Vm'r-iVTlieirXi, 
summing 
over all partitions of {1,..., 
k) into non-empty sets {/j,..., / | } , I > 1. 
(vii) E{Xl ■■■Xk) = Σ/,...../, I l L i *({χ< ■■» G 7P}), summing as in (vi). 
■ 
By Holder's inequality, | E ü j e / xi\ ^ Ujei(E\xi\k)l/k 
w h e n lzl < fc. a n d 
thus (vi) implies the useful estimate 
k 
\H(Xl,...,Xk)\<Ckl[(E\Xj\k)l'k, 
i 

148 
ASYMPTOTIC DISTRIBUTIONS 
where Ck depends on k only. We will use a more refined estimate, which we 
state as a lemma (Mikhailov 1991); recall the definition of dependency graph 
in Section 1.2 and let 
r 
J V t ( ö l , . . . ,ar) = \J{0 € V{L) :β = <α or αφ € E(L)} 
(6.8) 
denote the closed neighborhood of {ct\,... ,aT} in L. 
Lemma 6.17. Suppose that S = Σ α € ί 4Χα> where {Χα}αζΑ 
" a family of 
random variables with dependency graph L. Suppose, moreover, that r > 2 
and that M and T are numbers such that 
£ E | X a | < M 
a£A 
and, for every ct\,..., a r_i 6 A, 
Σ 
E(\Xa\\Xa„...,Xar_x)<T. 
a6Ñi.(ai 
<*r_i) 
Then 
\*r(S)\ < 
CrMTr~\ 
where CT is a constant that depends on r only. 
Proof. By Proposition 6.16(i, ii, iv) 
x r(S) = x(S,...,S) 
= 
Σ 
x(Xai,...,Xar), 
ai,...,a r 
where, by Proposition 6.16(v), every term in the sum for which {ai,... , a r } 
forms a disconnected subgraph of L vanishes. In each of thejemaining terms, 
the indices Q i , . . . , a r may be reordered such that a, £ Ν[,(αχ,... 
,α,_ι) 
when 2 < s < r. Hence 
l*r(S)| < r! Σ* 
\x(Xai,...,XQr)\, 
(6.9) 
O l 
O r 
where 51* denotes the sum over ai € A, c*2 6 Νι(αι), 
a¡ € NL(CH,Q2), 
... , 
ar € ^ ¿ ( a i , . . . , a r _ i ) . 
Define, for every sequence Qi,..., a, in A, s > 1, 
y*> -.= Σ ΠΕΠι*««ι· 

THE METHOD OF MOMENTS 
149 
summing over all partitions {/Ί ,...,/*} of { 1 , . . . , s}; by Proposition 6.16(vi), 
\x(Xai,...,Xar)\<C'rYai 
e r . 
(6.10) 
Now consider a sequence Qi,..., QS_ i in A, with 2 < s < r. Let {Λ,. · ·, /*} 
be a partition of { 1 , . . . , s}. We may assume that s 6 h; then let /{ = I\ \ {s}. 
By assumption (since we may define Q ; = a s_i for s < j < r), 
E( 
Σ 
\ Χ α \ \ Χ α ι , - . - , Χ α , - λ < Τ 
a€NL{o¡i 
ar.-i) 
and thus 
E 
Taking the expectation of both sides, we see that 
_ Σ 
ΕΠΙ*«,;Ι = Ε[ΠΙ*°.Ι _ Σ 
I*·, I] 
a.€NL(ai 
<*._,) 
«€Λ 
>6/¡ 
a.eNL{at 
,...,α._,) 
<τκγ[\χα,\. 
Multiplying this inequality by Π>=2^Πχ€/ l-*«*¡l an(* summing over all par-
titions {/i,..., / * } , we obtain on the right-hand side T times a sum, whose 
terms coincide with the terms in Yai 
α.-ι> each repeated at most s times. 
Consequently, 
/ „ 
*<»i 
a. S STΙα, 
ο,.ι · 
o . 6 N i ( o 
O i - i ) 
We next sum this inequality over Qi,..., Q,_I and obtain 
Σ* Yai 
a.<sT £ * Ya, „,_,, 
2<s<r. 
αι,.,.,α, 
α ι , . . . , α , _ , 
Since £ 
Υαχ = 5Z QE|X Q| < M by assumption, induction now yields 
Y^Yax 
Q.<s\MT'-\ 
l<a<r. 
(6.11) 
The result follows by (6.9), (6.10), and (6.11). 
■ 
Applications to asymptotic normality 
The lemma above leads to the following sufficient condition for asymptotic 
normality (Mikhailov 1991). 

150 
ASYMPTOTIC DISTRIBUTIONS 
Theorem 6.18. Suppose that (S^f 
ts a sequence of random variables such 
that Sn = Σα€»η ^ηο, where for each n, {Xna}a 
is a family of random 
variables with dependency graph Ln. Suppose further that there exist numbers 
Mn, Qn and Br such that 
52 Ε|ΑΓηα|<Μη 
(6.12) 
and, for every n and r > 1, and a\,..., 
ar € An, 
,Xncr)<BrQn. 
(6.13) 
then 
Σ 
α€Λ/Ί.„(αι,...,α 
rS„. //, as n 
E{\Xno\ | Xna, 
r) 
- * 00, 
< 
Sn A N(0, 
i ' 
· 
· 
0 
1)-
X, *«^-^)(ά) 
(6.14) 
Proof. By Lemma 6.17, with T = B r_iQ n, 
\*r{Sn)\<CrBr
rZ\MnQr
n-\ 
r>2, 
(6.15) 
and thus, with C'T = CTBT
rZ\, 
V - l 
/ U r t 2 \ r - 2 / 
„2 
\ r - 3 
Since σ* = χ 2(5„) < σ2Μηζ}η, 
by (6.15), 
/ M O2 \ r~ 2 
MS»)I < Cr(C2)p-3 ( - J p ) 
-> 0, 
r > 3. 
The result follows by Theorem 6.14. (Note that xl(Sn) 
= 0 and x2(Sn) 
= 
1.) 
■ 
Example 6.19. Consider again the subgraph count XG in G(n,p), where 
n -> oo and p = p(n) is a function of n. We will use Theorem 6.18 to give a 
new proof of Theorem 6.5. 
This time we denote the family of subgraphs of Kn that are isomorphic 
to G by {Ga}aeAn- 
We let Ia = l[Ga C G(n,p)] and Xa = Ia - EJQ, for 
simplicity omitting subscripts n. (If we assume that p(n) is bounded away 
from 1 (for example, p < 1/2), we can also apply the theorem with Xa = Ia. 
We leave this slightly simpler version to the reader. - Exercise!) Define the 
graph L„ with vertex set An by connecting every pair of indices a and β 

THE METHOD OF MOMENTS 
151 
such that the corresponding graphs Ga and G¿j have a common edge; this is 
evidently a dependency graph for {Xa} (Example 1.6). 
We verify the conditions of the theorem. First, observe that E|X Q| = 
2 Ε / β ( 1 - Ε / β ) < 2 i ; G ( l - p ) E / Q , and thus Σα&Αη
Ε 
\χ°\ < 
2vG(l-p)EXG, 
that is, (6.12) holds with M„ = 2uG(l - 
p)EXG." 
Next, suppose that r > 1 and a x,..., a r € An are given. Define F = Gai U 
•••UGQr and, for every a € An, Fa =GaC\F. 
Note that a € NLn(cn,... 
,a r) 
if and only if e(F0) ^ O. There are fewer than 2VF < 2TVa such subgraphs of 
F, and for each subgraph H C F there are 0(nvc 
'"") choices of a such that 
Fa = H, each with 
E(|X a| | Χα,,.-.,Χ»,.,) < E ( / e | XQl,...,Xar._t) 
+ EIQ < 
2pe°-e». 
Since, further, each Fa is isomorphic to a subgraph of G, it follows that 
£ 
Ε(|Χ α|μ α ι,...,^. Ι)<β Γ 
sup 
M £ = 5 r ^ , 
o6NL„(m 
ο,) 
for a suitable Br, depending on r and G. Consequently, (6.13) holds with 
Qn=EXG/*G. 
Finally, by Lemma 3.5, σ\ = Var(A"G) - (1 - P ) ( E X G ) 2 / * G - It follows 
that 
Μφ 
= W-pHfiXg)*·? 
= 
_ p)-^-^y 
(6.16) 
This verifies (6.14), since, as shown in the proof of Theorem 6.5, the conditions 
imply (1 -ρ)Φο 
-* oo. 
Theorem 6.18 thus applies; since XG — Σα ^«, and thus XG - EXG 
= 
ΣΖα Xa, the result follows. 
Remark 6.20. The results above may be improved by weakening the as-
sumptions; in fact, we know of two such improvements in different directions. 
(We do not know whether they can be combined.) 
First, as will be shown later by a different method (Theorem 6.33), it 
suffices to verify condition (6.13) in Theorem 6.18 for r = 2. 
Second, by using a theorem by Marcinkiewicz (1939), which states that the 
normal distributions are the only ones with all but a finite number of cumu-
lante vanishing, it follows (Janson 1988) that in Theorem 6.14 it is enough to 
assume that the condition >ck(Xn) -> xk(X) 
holds for k > m, for any fixed 
m < oo. (See also Grimmett (1992b).) This improvement is useful in cases 
where there is a general method to obtain desired estimates for all cumu-
lants of sufficiently large order, although the method fails for a few low-order 
ones. In particular, it leads to the following strengthening of Theorem 6.18 
(Mikhailov 1991). For an application to random graphs, see Janson (1988). 

152 
ASYMPTOTIC DISTRIBUTIONS 
Theorem 6.21. Suppose that the assumptions of Theorem, 6.18 hold, except 
that (6.14) ¿5 replaced by 
MnQ'n~l 
->0 
(6.17) 
for some real s > 2. Then the conclusion still holds, that is, 
5 n4N(0,l). 
Proof. If r > s, then 
by (6.15) (with r = 2) and, thus, by (6.15) and (6.17), xr(5„) -» 0. The 
result follows by the improvement of Theorem 6.14 just mentioned. 
■ 
Note that the proof shows that the assumption (6.17) becomes weaker as s 
increases. It can be reformulated as 
6.2 
STEIN'S METHOD: THE POISSON CASE 
A method to show convergence to the normal distribution was given by Stein 
(1972). The method has later been extended to several other limit distri-
butions; we will here only consider the simplest and most important cases, 
namely, Stein's original normal case (which is treated in the next section) 
and Chen's (1975) version for the Poisson case (which is treated below). The 
method was introduced in the theory of random graphs by Barbour (1982). 
Stein's method is well adapted to the type of sums of random variables that 
appear in many combinatorial applications; it then often leads to calculations 
very similar to those needed for estimating the second and (for the normal 
case) third moments when applying the method of moments. Consequently, 
Stein's method often requires less effort and simpler combinatorial arguments 
than the method of moments, where we have to estimate moments of arbitrary 
order. 
An important feature of Stein's method is that it does not only give con-
vergence; it actually gives an explicit upper bound for the distance between 
the distribution of a given random variable and a suitable normal or Poisson 
distribution. In other words, it is really a method to prove normal or Poisson 
approximation rather than convergence. Hence it leads to estimates of the 
rate of convergence, which, in practice, often turn out to be of the right order 
of magnitude. 

STEIN'S METHOD: THE POISSON CASE 
153 
Here we have talked about the 'distance' between two distributions without 
explaining what it is. In fact, several possible distances can be defined; for 
Poisson approximation the most useful is the following. 
The total variation distance between the distributions of two random vari-
ables X and Y is, in general, defined by 
dTv(X, Y) = sup | Ψ(Χ ZA)- 
?(Y 6 A)\, 
A 
taking the supremum over all Borel sets A. If X and V are integer valued, as 
in the cases we consider below, this is equivalent to 
dTV{x, v) = 5 Σ I p<* = *) - "*(y = *)l· 
k 
We also use hybrid notation, such as drv(X, Po(A)). 
It is easily seen that if (Xn)i° is a sequence of random variables, and (λ„),° 
is a sequence of positive numbers with λ η —► λ, then άτν{Χη, Ρο(λ„)) —»· 0 if 
and only if Xn -> Ρο(λ). Moreover, if άτν{Χη,^Ό{Χη)) 
-* 0 and λη -> oo, 
the central limit theorem for Ρο(λ„) implies that (Xn - λ η)/λ„ 
-> N(0,1). 
In particular, if further λη = E X n and Var(Xn) ~ λ„, then Xn -* N(0,1). 
Hence estimates of the total variation distance to a Poisson distribution can 
imply convergence to both Poisson distributions and normal distributions. 
(Not all cases of normal convergence are obtained in this way, however; typi-
cally we may obtain the cases when the mean and variance are asymptotically 
equal.) 
For the theoretical background for the Stein-Chen method for Poisson ap-
proximation we refer to Chen (1975), Stein (1986) and Barbour, Hoist and 
Janson (1992). These references also show how the method leads to explicit 
results such as the ones below (as well as others). A useful and rather general 
result obtained by the Stein-Chen method is the following (Barbour, Hoist 
and Janson 1992). 
Theorem 6.22. Suppose that X = ΣαεΛ^*' w^ere 
"*β la are random in-
dicator variables, and suppose that, for each a £ A, there exists a family of 
random indicator variables Jßa, ß € A \ {a}, such that 
C{{J0a}ß) 
= £{{I0}0 
I la = 1), 
(6.18) 
that is, the joint distribution of {Jßa}ß 
equals the conditional distribution of 
{Iß}ß given IQ = 1. Then, with π α =EIa 
and X = EX = ΣαςΑ π<*> 
dTV(X,Po(\)) 
< min^- 1,1) ^ 
πα(πα 
+ Y,E\Jßa 
- I0\). 
(6.19) 
a€A 
βφα 
■ 

154 
ASYMPTOTIC DISTRIBUTIONS 
One way to apply Theorem 6.22 without explicit construction of the vari-
ables Jga is via a dependency graph. In fact, if the family {/<,} has a de-
pendency graph L, then there exist random variables Jga with the right 
distribution (6.18) such that Jga = Ig when aß £ E(L), so in (6.19) it 
suffices to consider ß that are adjacent to a. For such ß we may crudely use 
| J0Q - l0\ < J0a + Ig together with the general relation 
πα E Jga = τχ0 E{Ig | /Q = 1) = ?(I0 = IQ = 1) = £(/„/„), 
(6.20) 
which yields 
7TQ E \J0Q - Ig\ < E(IaI0) 
+ παπ0. 
This leads to the following result. 
Theorem 6.23. Suppose that X = £ α € Λ Ια, where the Ia are random in-
dicator variables with a dependency graph L. 
Then, with π α = Ε / α and 
\ = EX = 5Ζο€/ί π° {and with summation over ordered pairs 
(α,β)), 
d Tv(X,Po(A))<min(A- 1,l)f^^ + 
£ 
(E(IaI0)+ 
EIaEI0)\ 
\»6Λ 
α,β: aßGB(L) 
' 
= min(A- 1,l)fvarX-EX + 2 
£ 
ir«»* + 2 £ 
π* V 
^ 
a,ß:aßeE[L) 
αζΑ 
' ■ 
A simple case of Theorem 6.22 is when J0a —10 has constant sign. We say 
that the random indicator variables (Ia)aeA are positively related if, for each 
a e A, there exist random variables J0a with the distribution (6.18), such 
that J0a > Iß for every β φ α; similarly, the variables are negatively related 
if, for each a 6 A, there exist such J0a with Jßa < Ig for every β φ a. 
For positively related variables, (6.20) yields 
na E \J0Q - I0\ = πα E(J0a - I0) = E(IaI0) 
- παπβ, 
(6.21) 
which leads to the following consequence of Theorem 6.22; note that the 
variables Jga do not appear explicitly (although their existence is essential). 
See further Barbour, Hoist and Janson (1992), where also a corresponding 
result for negatively related variables and other similar results are given. 
Theorem 6.24. Suppose that X = 5Zae4^Q> w^ere 
^ e Ia are positively 
related random indicator variables. Then, with ττα = E / a and X = E X = 
dTv{X,Po(X)) 
< mm{\-\l)(vaiX 
- EX + 2 £ 
ττΛ 
aGA 

STEINS METHOD: THE POISSON CASE 
155 
Returning to asymptotics, with variables depending on a parameter n —► 
oo, we thus see that for sums of positively related variables, a sufficient con-
dition for Poisson approximation (with an error tending to 0 as n -4 oo) is 
that the individual probabilities tend to 0 (uniformly) and that the variance 
is asymptotic to the mean. Since any Poisson distribution has the variance 
equal to the mean, the latter condition is very natural. 
Remark 6.25. Indicator variables that are positively (negatively) related are 
positively (negatively) correlated, but the converse does not hold. Correlation 
is only a pairwise property, whereas being positively or negatively related 
depends on the joint distribution of the whole family. 
Example 6.26. Consider again XQ, the number of copies of a fixed graph 
G in G(n,p), and suppose that G is strictly balanced. We proved in Theo-
rem 3.19, using the method of moments, that if np"1^) 
-> c > 0, then XQ -> 
Ρο(λ) with A = c"°/aut(G). Here we show how this result follows by the 
Stein-Chen method. The method further gives an explicit estimate 
O(n~0) 
of the rate of convergence, with β - min{t;//-e///d(G) : H C G, e# > 0} > 0, 
which we, however, leave to the reader (Exercise!). 
We write XG = 5ZG, IG· as in the proof of Theorem 6.5 and observe that 
the sum has (1 + o(l))n w c/ aut(G) terms, each having expectation pe°. Thus 
E X G ~ nVGpe°/aut(G) 
= (npd{G))va/aut(G) 
-+ X. 
(6.22) 
Moreover, since G is strictly balanced, a similar calculation yields EXH —* oo 
for every proper subgraph H of G. It follows as in (3.10), considering the 
terms with G' = G" and thus H — G separately, 
c 
χΗςσ,ί Η>ο 
Η 
' 
= ( l - p e c ) E X G + o ( l ) . 
Hence ν&τΧα/ΕΧα 
-* 1 and, since further max E J e = pe° -► 0, the result 
follows by Theorem 6.24, provided we can show that the variables IQ· are 
positively related. This can be verified as follows. 
Fix a copy G' of G in Kn. 
The conditional distribution of G(n,p) given 
IG· = 1 is the same as the distribution of the union G(n,p) U G', obtained 
by adding the edges in G' to G(n,p). Consequently, we may define JG»G· — 
1[G" C G(n,p) U G'} with G" ranging over the copies of G in K„; these 
variables have the correct joint distribution (6.18) and evidently JG"G· > 
IG··· 
In this example we may alternatively apply Theorem 6.23 with L as in the 
proof of Theorem 6.5; this yields the same result (without having to verify 
that the variables are positively related), with only a slightly worse bound for 
dTV{XG,Po(X)). 
In the preceding example, as in many others, it is easy to construct ex-
plicitly variables Jßa to show that the variables la are positively related. An 

156 
ASYMPTOTIC DISTRIBUTIONS 
alternative is to deduce the existence of suitable Jea by the following abstract 
result. (Recall that we do not really care what the variables Jga are, once we 
know that they exist with (6.18) and J(ja > Iß.) For a proof, using the FKG 
inequality (Theorem 2.12), see Barbour, Hoist and Janson (1992, Section 2.2) 
and the references given there. 
Theorem 6.27. Suppose that the indicator variables {Ia}a€A oil are increas-
ing functions of some underlying independent random variables {Yj}. 
Then 
the variables {Ια}αξΑ 
ore positively related; in particular, Theorem 6.24 aP~ 
plies to their sum. 
■ 
For example, the subgraph counts in Example 6.26 are increasing funcions 
of the edge indicators, and we see immediately that they are positively related. 
Example 6.28. Let X be the number of isolated vertices in G(n,p). Clearly, 
X = £ " Λ, where Λ = 1 [vertex i is isolated]. In this case, the indicator 
variables /< are decreasing functions of the edge indicators in G(n,p), but 
that is just as good; we can apply Theorem 6.27 with Yj being the edge 
indicators in the complement of G(n,p), or use the explicit construction Jj, — 
l\j is isolated in G»], where Gi equals G(n,p) with all edges from vertex i 
removed. Either way, it follows that the U are positively related. (In this 
example, Theorem 6.23 is not useful.) 
For example, if p = log n/n + c/n for some fixed real constant c, then 
EX = n(l - p)"- 1 ~ ne-np 
= e~c 
and 
VarX = (1 - ( Ι - ρ Γ ' ) Ε Χ + n(n - 1)((1 - p) 2"" 3 - (1 - 
p)2{n'l)) 
= EX +o(l), 
and Theorem 6.24 shows that X -¥ Po(e~c). In particular, 
P(G(n,p) has no isolated vertices) = P(X = 0) -+ 
e~e". 
(This yields another proof of Corollary 3.31.) 
The number of vertices of degree at most a given number d > 0 can be 
treated in the same way. A similar argument works also for the number 5<< 
of vertices of degree exactly d, but this time the corresponding indicators A 
are not positively related, and we use Theorem 6.22. This yields the result, 
first proved by Erdös and Rényi (1960) by the method of moments, that 
Sd -¥ Po(c) if ESd -► c < oo (which, for d > 1, happens in two ranges of p). 
For details, see Karonski and Rucinski (1987) and Barbour, Hoist and Janson 
(1992), where also further examples are given. 

STEINS METHOD: THE NORMAL CASE 
157 
Example 6.29. Let, as in Section 3.6, To be the number of isolated copies 
of G in G(n,p). Then Xo = Σ σ . IG-, where, as in Example 6.26, G' ranges 
over the copies of G in K„, but now IG· is the indicator that G' is an isolated 
subgraph of G(n,p). 
In order to define suitable random variables J c c , we first define, for a 
given G', a modification G of G(n,p) by adding all edges in G' and deleting 
every other edge in G(n,p) incident with a vertex in G'. We then define 
JCG· 
to be the indicator that G" is an isolated subgraph of G. Then G is a random 
graph distributed as G(n,p) conditioned on IG· = 1, and thus the variables 
JCG' 
have the correct joint distribution (6.18). Moreover, JG-G· 
= 0 if 
G' n G" φ 0, but G' φ G", while JG»G· > IG- if G' n G" = 0. 
It is easily seen that if G is connected and unicyclic and np -> c > 0, 
or if G is a tree of order v and either nvpv~l 
-> c > 0 or vnp - logn -
(v - 1) log logn -► c € (-οο,οο), then ETG -* λ < oo. In these cases, 
Theorem 6.22 yields Ta -* Po(A) by straightforward calculations (Exercise!). 
(Compare with Section 3.6.) 
The same argument applies also to the random varible T„ counting all 
isolated trees of order v (and not just copies of a specific trees), which proves 
the final part of Theorem 3.30. 
(These results were originally proved by Erdös and Rényi (1960) by the 
method of moments.) 
Example 6.30. We have so far applied the Stein-Chen method to the ran-
dom graph G(n,p), but it applies also to G(n,M). For example, consider 
the subgraph count XG as in Example 6.26, but now for G(n, M). The main 
difference from the G(n,p) case is that for G(n, M), we can use neither The-
orem 6.23 (because there does not exist a sparse dependency graph) nor The-
orem 6.24 (because the indicator variables la· are not positively related); 
instead we use Theorem 6.22 with the following construction. 
The conditional distribution of G(n, M) given la· = 1 is the same as the 
distribution of the random graph G obtained from G(n, M) by first adding all 
edges in G' that are not already present, and then deleting the same number 
of edges, randomly chosen among the edges outside G'. We may thus define 
JG-G· = 1[G" C G] and VG· = Σο··*β· 
JG"G·-
It is straightforward to estimate E\JC»G· 
- IG'\, which, by (6.19), yields 
an estimate of dTv(AO, P o ( E ·*<?)) for G(n, M), but we omit the details (Ex-
ercise!). 
Similar constructions apply to the other examples above. 
6.3 STEIN'S METHOD: THE NORMAL CASE 
The original version of Stein's method yields normal approximation; see Stein 
(1972, 1986) for a general description. The method was applied to random 
graphs by Barbour (1982) and Barbour, Karonski and Rucinski (1989), to 

158 
ASYMPTOTIC DISTRIBUTIONS 
which we refer for further details. In particular, Stein's method yields the 
following rather general result (Barbour, Karonski and Rucinski 1989), based 
on constructing a suitable decomposition of the studied random variable. We 
let d\ denote the distance between distributions defined by 
dx{X,Y) = sup{|E/i(X) - Eh{Y)\ : sup|A(x)| + sup|/i'(x)| < 1}; 
note that di{Xn,Y) 
-* 0 implies Xn -¥ Y. (This distance is well adapted to 
Stein's method, although other distances are more commonly used in other 
contexts; cf. Barbour, Karonski and Rucinski (1989).) 
Theorem 6.31. Suppose that W is a random variable which can be decom-
posed as follows: For some finite index sets A and Ba, a € A, and square 
integrable random variables XQ, Wa, Za, Zap, Wap and Vaß, a € A, ß G Ba: 
W=Y^Xa; 
W = Wa + Za, 
aeA; 
Za - Σ 
ZoP< 
Q e A-
0£Ba 
Wa = Wa0 + Vaß, 
a € A, β € Ba; 
where, further, E Xa — 0, Wa is independent of Xa and Wag is independent 
of the pair {Xa, Ζαβ)· Then, for some universal constant C, σ2 = Var W and 
W = a~lW, 
dx (W, N(0,1)) < Ca-3 
fe 
E(\Xa\Zl) 
a€A 
+ Σ 
Σ 
(E|*«Z0/JVa/,| + E|X aZ a / J|E|Z a + Va0\)). 
(6.23) 
aeAßeBa 
■ 
In applications, one has to construct decompositions as above of a given 
variable, keeping the right-hand side of (6.23) small. For sums of random 
variables with a lot of independence, as measured by a suitably sparse de-
pendency graph, there is a straightforward construction, which leads to the 
following result. 
Theorem 6.32. Suppose that W = Σα€ΑΧα, 
where {Xa}a<¿A w a family 
of random variables with dependency graph L and, further, EXa = 0, a G A. 
Let σ2 = VarW and assume that 0 < σ2 < oo. Then, for some universal 
constant C and with Nr{<*) the closed neighborhood of a as in (6.8), 
άχ{\νΜ0,1))<Οσ-3Σ 
Σ 
{*\XaXßXy\ + 
K\XaXß\E\Xy\). 
a^Aßn^7fr{a) 
(6.24) 

STEIN'S METHOD: THE NORMAL CASE 
159 
Proof. We apply Theorem 6.31 with Ba = Nr(a), Wa - Σ/3£7νΓ(ο) ^ ' 
Za 
= Σ,ι3ζΝΓ(α) 'Y¿> Z°H 
= 
Xß< 
W<*0 = Σ7ίΝΓ(α)υΝΓ(ί?) *T a n d V<*>* 
= 
Σ7€Ν,-(ΰ)\τνΓ(ο)
 λ ν
I I is then e a s i ly
s e e n that 
ΣΕ(\Χα\Ζΐ) 
+ Σ Σ (^\Xc,ZoßVa0\ + E\XaZa0\E\Za 
+ Va0\) 
aEA 
a€A0eBa 
and the result follows. (We may assume that ΕΧ„<οο, since otherwise the 
right-hand side of (6.24) is infinite.) 
■ 
In particular, this yields an improvement of Theorem 6.18. 
Theorem 6.33. Suppose that {Sn)f is a sequence of random variables such 
that S„ = 5ZQ6/ln Xnat where for each n, {Xna}a is a family of random 
variables with dependency graph Ln. Suppose further that there exist numbers 
Mn and Qn such that 
£ E | X n a | < M „ 
(6.25) 
α€Λ„ 
and for every a\,a2 € An, 
£ 
E{\Xna\ | Χηαι, Xna2) 
< Qn. 
(6.26) 
a€NLn 
(αι,α^) 
Letal = VarS„. Then 
d 1(5 B,N(0,l))=o(^2^). 
In particular, if 
^ ^ ^ 0 , 
(6.27) 
then 
°n 
Sn 4· N(0,1). 
Proof. By replacing XnQ by Xna - EXna (and 5„ by S„ - E5„), we may 
assume that E Xna = 0; note that (6.25) and (6.26) still hold if we replace 
M„ and Qn by 2Mn and 2Qn. 
It follows, arguing as in Lemma 6.17, that 
£ 
£ 
{E\XaXßXy\ 
+ 
E\XaXß\E\Xy\)<2MnQ2
n 

160 
ASYMPTOTIC DISTRIBUTIONS 
and the result follows by Theorem 6.32. 
■ 
Example 6.34. Consider again the subgraph count XQ in G(n,p), where 
n —> oo and p = p(n) is a function of n. 
We have XQ - EXG = Σα€Λ„ ^α> where An and Xa are as in Exam-
ple 6.19. As already shown in Example 6.19, with the dependency graph 
L„ defined there, (6.25) and (6.26) hold with M„ = 0{{\ - p)EXG)) 
and 
Q„ = 0 ( E X G / $ G ) , which yields, as in (6.16), 
MnQl/a3
n=0((l-p)-^al/2). 
Consequently, Theorem 6.33 yields a new proof of Theorem 6.5, with the 
additional information that 
d 1 ( X G ) N ( 0 , l ) ) = O ( ( l - p ) - 1 ^ 1 / 2 ) . 
Theorem 6.31 is more flexible than Theorem 6.32 and can be applied also to 
sums where all summands Xa are dependent. One such case is when counting 
the number of subsets of (the vertex set of) G(n, p) that satisfy a given semi-
induced property, that is, a property that depends only on the edges with at 
least one endpoint in the set. We begin with an example. 
Example 6.35. Denote the number of vertices of degree d in G(n,p) by Sd-
We consider a fixed d > 0 and let n -»· oo with p = p{n). Then (assuming for 
simplicity np2 = o(l); for larger p, ESd -*■ 0 rapidly), 
Consequently, 
ESd -> oo ·*=> nd+1pd -> oo and np — logn — dloglogn -+ -oo. 
Let Ii = l[i has degree d in G(n,p)], and Xi = /¿ - E/¿; then Sd - ESd = 
52" Xi. In this case, there is (in general) no independence between any two 
Xi and Xj, and we use the following construction. 
Let, for a set F C [n] and i £ [n], 
and let Xf = if -Elf. 
Moreover, for i,j €[n], let B¡ = A = [n] and 
_t. has degree d in G(n,p) \ F), 
i $ F, 
' _ < - 
ieF, 

STEINS METHOD: THE NORMAL CASE 
161 
7 · = / - 7{,} 
z, - 2^ zl}, 
n 
VÜ = D / Í 4 , - / ¿ 4 J } > ' 
n 
W y = Σ 
A'*'·7* - E V0 - E Zi. 
k=\ 
The conditions of Theorem 6.31 then are satisfied, and it is not difficult 
to show that both sums in (6.23) are 0(E5<j); see Barbour, Karonski and 
Rucinski (1989) for details. Consequently, 
d, (Sd, N(0,1)) = 0((Var Sd)~3/2 ES d). 
Moreover, a simple calculation yields 
VarSd = ^
(
" 
* ^ ( ( n - l J p - ^ V - ' d - P ^ ' ^ - ' + E S d - n - ^ E S , , ) 2 , 
and it is easily seen that if d > 1 or d = 0 and np = Ω(1), then 
V a r S d x E S d , 
and consequently 
d 1(S d,N(0,l))=O((ES ( 1)- 1/ 2); 
in particular, Sd -¥ N(0,1) if furthermore ESd -> oo. 
In the remaining case d = 0 (isolated vertices) and np -> 0, we have E So ~ 
n and VarSo ~ 2n2p, whence we only obtain d\ (S0,N(0,1)) = 
0{n~2p~3/2)\ 
this proves So -* N(0,1), provided n~4/3 < S p « n _ 1 . This result can be 
extended to the full range where VarSo -> oo, that is, when E5o -> oo and 
n2p -> oo. In fact, when np -+ 0, VarSo ~ VarSi 
Cov(So,Si) ~ 2n2p, 
and thus Var(S0 + Si) = o{n2p) = o(VarS0); consequently, So = - S i +o p(l), 
and the result follows since, as just shown, Si -» N(0,1) provided furthermore 
n2p -t oo. (See, further, Barbour, Karonski and Rucinski (1989) and Kordecki 
(1990).) 
Since Var Sd -> oo is necessary for asymptotic normality, we have proved 
the following result. 
Theorem 6.36. Ifd>0, 
then Sd -^ N(0,1) if and only if VarSd -* oo. For 
d > 1, this is equivalent to ESd -* oo, i.e., n<,+1pd -+ oo and np — logn -

162 
ASYMPTOTIC DISTRIBUTIONS 
d log log n —► — 00; for d = 0 it is equivalent to n2p —> oo and np - logn -» 
-oo. 
■ 
Remark 6.37. As remarked in Section 6.2, S¿ has an asymptotic Poisson 
distribution when ES,/ -> c < oo. In fact (Karonski and Rucinski 1987), 
the Stein-Chen method yields Poisson approximation also when ESd -> oo, 
provided np -4 0 and d > 2, or np -+ oo (in these cases Var 5¿ ~ E S<¿), which 
gives another proof (historically the first) of Theorem 6.36 in these cases. 
For a general semi-induced property V, we similarly construct a decompo-
sition of W = Σ Ia, where a ranges over the subsets of [n] of a given size and 
Ia is the indicator that a has the property V, by defining 
TF _ ί 1[α has the property V in G(n,p) \F], 
aHF 
= 0, 
and then proceeding as above. 
Another example of this is the following 
(Barbour 1982, Barbour, Karonski and Rucinski 1989). 
Theorem 6.38. Let T/t be the number of isolated trees of order k in G(n,p), 
where k > 2 is fixed. Then fk Λ N(0,1) if and only ifETk 
-+ oo, that is, 
when nkpk~l 
-> oo and knp- 
logn - (k — 1)loglogn 
-» -oo. 
Sketch of proof. We apply Theorem 6.31 with the construction just indicated. 
The sums in (6.23) are both 0(ETfc) and VarT* x ET*; thus 
d,(r t,N(0,l)) = O ( ( E T 0 - 1 / 2 ) . 
■ 
Again, as stated in Theorem 3.30 and Example 6.29, Erdös and Rényi 
(1960) proved that Tk A Po(c) when ET* -» c < oo (in both ranges of p). 
Note further that the first (threshold) part of Theorem 3.30 follows easily 
from Theorem 6.38. 
6.4 
PROJECTIONS AND DECOMPOSITIONS 
A standard method when studying asymptotic distributions is to approximate 
the studied random variable by another one, which is simpler in some sense. 
Indeed, by Cramer's theorem (see Section 1.2), if Xn - Y„ A 0 and Y„ -» Z, 
then Xn -> Z too. 
The first projection 
Consider a random variable X which is a prop/» functional, that is, a (real-
valued) variable that depends only on the isomorphism type of G(n,p). Then, 
the simplest choice of an approximating variable is a linear function aL + b 

PROJECTIONS AND DECOMPOSITIONS 
163 
of the number of edges L = e(G(n,p)) = Χκ2- 
(Here a and 6 are constants 
that may depend on n and p = p(n).) Since L € Bi(("),p), the central limit 
theorem yields L -¥ N(0,1), provided that n2p -» oo and n 2(l - p) -* oo, so 
it remains only to study the error X — aL - b. 
We choose the coefficients a and b such as to minimize the Z-2-norm of 
the error, that is, Y = aL + 6 is the projection in L2(P) of X onto the 
two-dimensional subspace of linear functions of L. This is the usual linear 
regression, and as is well known, then X - Y is orthogonal to 1 and L, which 
leads to 
_ Οον(Λ", L) _ Cov(Jr, L) 
a~ 
VarL 
- ( > ( l - p ) ' 
b = EX 
-aEL. 
Moreover, 
E(X - y ) 2 = Var(X -Y) 
= Var(X - aL) = Var X - a2 Var L. 
Now L = £ e h, summing over all edges e in Kn, where 7e is the indicator 
l[e 6 G(n,p)]. By symmetry, Cov(X,/,,) is independent of e and thus, for 
any edge e, 
a = Cov(X, L ) / ( > ( 1 - p) = Cov(X, / e)/p(l - p) 
= ( E ( X / e ) - p E X ) / p ( l - p ) 
= ( E ( X | e e G ( n , p ) ) - E I ) / ( l - p ) . 
The approximating variable aL + b is known as the first projection of X, 
and this approach to proving normality is called the first projection method. 
It can be summarized as the following theorem. 
Theorem 6.39. Suppose that Xn is a graph functional o/G(n,p), with p = 
p{n), and let 
a n = E ( X „ | e € G ( n , p ) ) 
-EXn 
for an edge e £ K„. If n2p -> oo, n 2(l - p) -► oo and 
V a r X n ~ Q p a - P ) - 1 o : n . 
(p.28) 
then Xn AN 
(0,1). 
Proof. By the discussion above, we let Yn = anLn + bn, with an = (1 - p ) _ 1 a „ 
and b„ = EXn - anELn, 
and find 
E(X„ - Yn)2 = VarX,, - a2 ( > ( 1 - p) = o(VarX„). 

164 
ASYMPTOTIC DISTRIBUTIONS 
Hence, if ση = VarXn, then E|(X„ - Υη)/ση\2 -> 0 and, furthermore, 
VarKn ~ VarXn. Consequently, 
xn-Exn 
^y n-EK n 
t 
χη-γη 
"n 
On 
On 
where (Yn-EYn)/on 
= (VarYn/ VarX,,)1/2!,, A N(0,l)and (X„-V„)/an A 
0. The result follows by Cramer's theorem (Section 1.2). 
■ 
Example 6.40. Consider again the subgraph count XQ- Given an edge e € 
Kn, there are eG(n)vo /(£) aut(G) copies of G in Kn that contain e, and thus 
a n = E(X | h = 1) - E X = ec(n)we(G)aiit(G))" V o - 1 -p e°) 
~ 2eGn"G-2(aut(G))"1pec-1(l -p). 
It is easily seen that (2)p(l-p) - 1a 2 asymptotically equals the contribution 
to Var(Xc) from the terms in (3.10) corresponding to two copies of G that 
intersect in a single edge (Exercise!). Consequently, the condition (6.28) is 
equivalent to K^ being the only leading overlap of G, that is, to npm 
^ 
-v 
oo; see Section 3.2. 
This example is typical; the method of the first projection is (usually) very 
easy to apply, but it works only sometimes and often does not give the full 
result. 
Higher projections 
It is natural to try to extend the range of the first projection method by 
projecting onto a larger space of variables, thus reducing the error in the 
approximation. 
The first projection uses only information on subgraphs of G(n,p) with 
two vertices; the next step (sometimes called the second projection) is to use 
information on the subgraphs with three vertices. Each such subgraph has 0, 
1, 2 or 3 edges, and is determined up to isomorphism by its number of edges, 
so the second projection is a linear combination of the random variables η>, 
TI, T2 and T3, where τ, counts the number of triples of vertices in G(n,p) with 
j edges between them. Equivalently, by simple algebra, the second projection 
can be expressed as a linear combination of the constant 1 and the three 
subgraph counts Χκ2 = L, Xp¡ and X/c3, where Pi is the path of length 2. 
In neither of these representations, however, are the four basis variables 
orthogonal, so it will be more convenient to use a third representation. It can 
be constructed from the subgraph counts above by the usual orthogonalization 
procedure, but we prefer to define it directly in the next subsection, at the 
same time generalizing it to larger subgraphs. 

PROJECTIONS AND DECOMPOSITIONS 
165 
A general decomposition 
Let H be a graph. Consider the (n)„„ different injective mappings from the 
vertices of H into { 1 , . . . , n). Each such mapping ψ maps H onto a copy ψ{Η) 
of H in Kn, and we define 
Sn(H) = Sn,p(H) 
= £ 
Π 
( 7 < - P ) ' 
( 6 2 9 ) 
where, as above, It — l[e 6 G(n,p)]. 
In other words, we sum Y\eeff,{Ie - p) over all copies H' of // in Kn, 
counted with multiplicities aut(Jf). Note that if we replace (I( - p) by /e in 
(6.29), we obtain &ut(H)XH-
It is obvious that Sn(H) 
depends only on the isomorphism type of H. 
Hence we may regard {S„{H)}H 
as a family of random variables, indexed by 
unlabelled graphs H. 
The simplest examples are 
5„(0) = 1 
(trivial but useful) 
Sn(K\) 
= n 
(trivial and less useful) 
Sn(K2) 
= 2 ] T (/, - p ) = 2(e(G(n,p)) - 
( » . 
e€K„ 
It is easily seen that if H has any isolated vertices, removing them changes 
Sn(H) only by a non-random factor (depending on n). Hence we may restrict 
attention to H without isolated vertices. 
Since the variables Ie - p are independent and have mean 0, two products 
Y[(Ie — p) are orthogonal unless they coincide, and the following results are 
easily obtained. 
Lemma 6.41. Suppose that H and K are graphs without isolated vertices. 
(i) If H ¿Q, then ESn{H) 
= 0. 
(ii) If H ¿<d, then 
VarS„(ff) = ES n(tf) 2 = aut(tf)(n)„„(p(l - p))e». 
(iii) // H andK are non-isomorphic, then Sn{H) andSn(K) 
are orthogonal: 
Cov(Sn(H),Sn(K)) 
= E[Sn{H)Sn(K)] 
= 0. 
■ 
We next show that the variables Sn(H) 
can be used to decompose any 
graph functional. 
Lemma 6.42. Every graph functional X o/G(n,p) has a unique expansion 
X = £ * „ ( H ) S „ ( H ) 
(6.30) 
H 

166 
ASYMPTOTIC DISTRIBUTIONS 
for some real coefficients Xn{H) = Xn,P{H), where H ranges over the unla-
belled graphs with no isolated vertices and at most n vertices. Furthermore, 
the terms in (6.30) are orthogonal and 
V a r * = Σ Xn{H)2Va.TSn(H) = Σ aut(//)(n)„„(p(l - p))'» 
Xn(H)2. 
(6.31) 
Proof. Trivially, 
x = J2x{G)l[ieY[(i-it), 
G 
e€G 
t$G 
where we sum over all graphs G with vertex set {1,... ,n). If we substitute 
h = {le - p) + P and expand, X will be expressed as a linear combination of 
terms T[teH(Ie ~P)> H C K„, and (6.30) is obtained by collecting terms with 
isomorphic H together. 
Lemma 6.41 implies that the terms in (6.30) are orthogonal, and that (6.31) 
holds. Moreover, Lemma 6.41 and (6.30) yield also 
Xn(H) = 
E(XSn(H))/ESn(H)\ 
and thus the term Xn(H) is uniquely determined. 
■ 
Here Xn(0)Sn(0) = Xn{Q) = EX, so the randomness enters through the 
terms in (6.30) with H non-null. 
The first projection equals Xn(0)Sn(0) + X„(K?)Sn{K2), and is thus ob-
tained by ignoring all terms in (6.30) with VH > 3. Similarly, the second pro-
jection equabX„(e)S„(0)+X„(tfa)S»(i^ 
that is, the sum of the four terms with v» < 3. More generally, we can select 
any set of graphs H and consider only the sum of the corresponding terms in 
(6.30) as an approximation of X. 
In order to use this idea to obtain asymptotic distributions from the de-
composition (6.30), we have to know asymptotic distributions of the basis 
variables Sn(H). We already know that S„(Ä2) = 2(L - EL) is asymptoti-
cally normal, provided p is not too close to 0 or 1; this extends to every Sn{H) 
with H connected, while disconnented H give other limits. More precisely, we 
have the following theorem, proved using a continuous time martingale limit 
theorem in Janson (1994a). (For fixed p, it was earlier proved by the method 
of moments (Janson 1990a, Janson and Nowicki 1991).) 
Theorem 6.43. Suppose that p = p(n) -> po as n -> oo, with 0 < po < 
1. Then there exist random variables U{H), where H ranges over unlabelled 
graphs, such that if H is any graph without isolated vertices for which 
npmW -> oo, 
(6.32) 
then, as n -> oo, 
n-VH/2p-tH/2Sn,p(H) 
A U(H). 
(6.33) 

PROJECTIONS AND DECOMPOSITIONS 
167 
The convergence in (6.33) holds jointly for any finite number of graphs H 
that satisfy (6.32). The limit variables U(H) are determined by the following 
properties: 
(i) // H is connected and eH > 0, then U(H) has a normal distribution 
with mean EU(H) = 0 and variance 
KU(H)2 = aut(tf )(1 - poY" ■ 
(6.34) 
(ii) // Hi,...,Hm 
are different (i.e., non-isomorphic) connected unlabelled 
graphs, then U{H\),..., 
U(Hm) are independent. 
(iii) // H has connected components Hi,...,Hm, 
each having at least one 
edge, then U(H) is a polynomial in U(Hi),.. 
.,U{Hm), 
known as the 
Wick product :U{Hi) ■ ■ ■ U(Hm):, 
see, e.g., Janson (1997) for a defini-
tion. In particular, for m = 2, 
U{H) = :t/(i/!)i/(H2): = U(Hi)U(H2) 
- E(t/(#i)t/(# 2)). 
(6.35) 
Furthermore, (6.34) holds for every H, andEU(Hi)U(H2) 
= 0ifHi 
and H2 
are two different unlabelled graphs without isolated vertices. 
■ 
We return to the study of a general graph functional X, or more formally, 
a sequence X„ of functionals of G(n,p), where p = p{n) is a given sequence. 
Then Xn has a (unique) expansion (6.30). 
The simplest case is when only a finite set of graphs H, independent of n, 
is needed in the expansion (6.30). Assume further that (6.32) holds for these 
graphs H. The asymptotic behavior of X„ then follows from Theorem 6.43, 
provided we know the asymptotic behavior of the coefficients Xn(H). 
Both 
normal and non-normal limits may be obtained by this procedure. In fact, Xn 
is asymptotically normal if and only if the terms with H connected dominate 
the decomposition (6.30). 
Even if no finite set of graphs suffices for the expansion of every Xn, it is 
frequently the case that a finite set gives a good approximation. In general, 
let Ή be a family of non-null unlabelled graphs without isolated vertices. We 
say that X„ is dominated by Ή. (for the given sequence p(n)) if, as n -* oo, 
VarX n~ Σ 
Xn(H)2VaxSn(H). 
Hen 
In this case, Xn has the same asymptotic distribution (if any) as the projection 
Σ«€ίί Xn{H)Sn(H). 
In particular, if there exists a finite dominating family 
Ή, we may apply Theorem 6.43 to this projection and obtain limit results just 
as for the case of a finite expansion. 
Remark 6.44. The first and second projection methods can now be rec-
ognized as the special cases of this procedure with the dominating families 

168 
ASYMPTOTIC DISTRIBUTIONS 
Ή. = {K2} and Ή. - \Κ·ι,Ρ·ι,Κ{\, 
respectively. Note that in these cases, all 
graphs H € Ή. are connected, and thus Xn is always asymptotically normal 
when these methods apply. 
The method extends through a truncation argument to a more general 
situation, called here asymptotically finitely dominated, where Ή is infinite, 
but for every ε > 0 a finite subset (independent of n) of Ή. suffices to yield at 
least ( l - e ) V a r X n in (6.31). 
This method yields the following rather general result, where we define 
Xn{H) = 
nv»'2p<»<2Xn(H). 
Note that by Lemma 6.41 we have, for H φ 0, 
Var(Xn(tf)Sn(tf)) ~ (1 - p) e" ant(H)Xn(H)2. 
(6.36) 
Theorem 6.45. Let Xn be a graph functional of G(n,p), where p = p(n). 
Suppose that p -¥ po € [0,1], that βη is a sequence of positive numbers, and 
that % is a family of non-null graphs without isolated vertices such that, for 
every H 
€rl, 
np"»(H) _> oo 
(6 3 7) 
and 
a{H) = lim Χ·(Η)/βη 
exists. 
(6.38) 
n-+oo 
Suppose further that 
Vzr(Xn)/ß2
n 
-» £ 
a(H)2 aiit(tf)(l - po)e" < oo. 
(6.39) 
£ 
a(H)U{H), 
(6.40) 
Then, 
Xn - E Xn d 
P" 
Hew 
where U(H) is as in Theorem 6.43. (If Ή is infinite, the sum is interpreted 
in L2.) 
Sketch of proof. The case po = 1 is trivial (and is better handled by studying 
the complementary graph, which is G(n, 1 — p)). 
Thus assume 0 < po < 1. Given e > 0, we may choose a finite subset Ήε 
οίΉ. such that ΣΗΪΗ, 
a(H)2 &ut(H)(l - po)"1 > £ f f € Wa(H) 2aut(//)(l 
-
Po)e" - e. It follows by Lemma 6.41, (6.36) and (6.38) that if Y¿ is the pro-
jection ZHCH. 
Xn(H)Sn{H), 
then Var(y„7/?n) -► ΣΗΖΗ. 
α(Η)2 
aut(/f)(1 -
Po)e" and, consequently, by (6.39), for n large, 
-¥-^κ-^ϊ< 

PROJECTIONS AND DECOMPOSITIONS 
169 
Since further Theorem 6.43 implies that Vn
f fi)„ A Σ,Ηεη, a{H)U(H) as n -> 
oo, (6.40) follows by Billingsley (1968, Theorem 4.2). 
' 
■ 
Corollary 6.46. Under the assumptions of Theorem 6.45, if further, every 
graph H 6 Ή. is connected and at least one a(H) φ 0, then Xn is asymptoti-
cally normal, 
X „ A N ( 0 , 1 ) . 
(6.41) 
Proof. If every H is connected, then each U(H) is normal and so is the sum 
Σα{Η)υ(Η); 
(6.41) follows by (6.34) and (6.39). 
■ 
Corollary 6.47. Under the assumptions of Theorem 6.45, if further, po < 1, 
Ή. is finite and a(H) φ 0 for some disconnected H € Ή, then Xn has a 
non-normal asymptotic 
distribution. 
Proof. If some H is disconnected (and po Φ 1), then U{H) is a polynomial of 
degree > 2 in normal variables and it is easily seen that the sum in (6.40) is 
such a polynomial too. Such a polynomial has a distribution with too large 
tails to be normal, see, for example, Janson (1997, Theorem 6.12). 
■ 
Remark 6.48. Suppose that po < I and that not all a(H) = 0. If Ή is finite, 
then (6.39) is, by (6.38) and Lemma 6.41, equivalent to the condition that Xn 
is dominated by Ή. If Ή. is infinite, (6.39) is stronger and, in fact, equivalent 
to Xn being asymptotically finitely dominated by Ή. 
In the normal case, we may replace the assumption (6.38) on convergence 
of the coefficients by a suitable upper bound. 
Theorem 6.49. Let Xn be a graph functional of G(n,p), where p = p(n). 
Suppose that p —> po € [0,1], and that Xn is dominated by a family Ή of 
connected graphs such that, for every H e H, npm(-H^ -> oo and the numbers 
b(H) = supÁ^tfJAVarXn) 1/ 2 
(6.42) 
n 
are finite and satisfy 
Σ 
b{H)2 aut(H) < oo. 
(6.43) 
Then, 
X „ A N ( 0 , 1 ) . 
Proof. Let /?„ = (VarX«)1/2. Then, since Xn is dominated by 7ί, 
1 = Var Χη/β2
η ~ Σ 
VaT{Xn(H)Sn(H)/0n) 
Hen 
= ^ " " " " P " ' " * ^ ) 2 ^ 5 « ™/ 3 2 , · 
(6.44) 
Hen 

170 
ASYMPTOTIC DISTRIBUTIONS 
Since Χή(Η)/βη 
= O(l) for H € Ή. by (6.42), there exists a subsequence 
along which Χ^Η)/βη 
-► a(H) for every H € Ή. and some a(Jf). Along this 
subsequence, each term in the sum in (6.44) converges, by Lemma 6.41, to 
a(i/)2 aut(#)(l -po)e" 
and is bounded, using also (6.42), by 6(H)2 aut(tf). 
Consequently, we may by (6.43) apply the dominated convergence theorem to 
(6.44), obtaining 
1 = VzxXnlßl 
-+ £ 
a{H)2 aut(ff)(l - po)e" 
Hen 
and thus (6.39) holds. By Theorem 6.45, applied along the subsequence, 
, -► N(0,1) along the subsequence. 
Moreover, given any subsequence, this argument shows that there is a sub-
asequer 
principle. 
Xn -¥ N(0,1) along the subsequence. 
ys 
subsequence with Xn -* N(0,1), and the result follows by the subsubsequence 
Applications to subgraph counts in G(n, p) 
We use the decomposition method to obtain old and new results for subgraph 
counts. 
Example 6.50. Consider again the subgraph count XQ in G(n,p), where 
n -¥ oo and p = p(n) is a function of n. We will use the results above to give 
yet another proof (our last) of Theorem 6.5. 
We can write, cf. (6.29) and the discussion there, 
*«-=555Σ Π '.· 
<M0 
v 
' 
V e€v>(G) 
summing over all injective mappings from the vertices of G into {l,...,n}. 
As in the proof of Lemma 6.42, if we substitute Ie = (Ie —p) + P, expand the 
product in (6.45) as a sum of terms of the type Π«€ν(Η)^β _ p)pe°~e" 
wi t n 
H C G and rearrange the terms, we obtain an orthogonal expansion (6.30) 
with only H CG (and without isolated vertices) appearing. Furthermore, it 
follows from this argument that, for such H, 
XGn(H)xnvo-VHpto-tH 
and thus (omitting the subscript n) 
XG(H) 
X n»c-W2 pe c-e„/2 
(g 4g) 
Now consider p = p{n). If p -> po, 0 < po < 1, then XQ{H) X n" 0 -""/ 2, and 
thus XQ is dominated by the H φ 0 with smallest v», that is, by {fa}- By 
a simple application of Theorem 6.49, XQ is asymptotically normal. (This is 
essentially the first projection method; see Remark 6.44 and Example 6.40.) 

PROJECTIONS AND DECOMPOSITIONS 
171 
If p -> 1, XQ still is dominated by {K2}, and, assuming n 2(l - p) -*■ 00, 
asymptotic normality follows similarly. 
If p -*· 0, then, by (6.46), XG is dominated by the set of non-null H C G 
for which n~"H/2p~eH/2 
is of largest order, that is, of the leading overlaps 
defined in Section 3.2. The assumption npm^ 
-» 00 implies by Lemma 3.15 
that all leading overlaps are connected. Since also m(H) < m(G) for H CG, 
Theorem 6.49 yields XG 4 N(0,1). 
Example 6.51. We next consider induced subgraph counts. Let Vc(<G(n,p)) 
be the number of induced subgraphs of G(n,p) that are isomorphic to G. We 
see, as before, that YQ has a finite decomposition (6.30), but now the sum has 
to be taken over all graphs H (without isolated vertices) with VH < ve-
in the case p -> 0, it is easy to see that the decomposition (6.30) is dom-
inated by the same terms as in the decomposition for XG, that is, by the 
terms corresponding to leading overlaps of G, except in the case when G has 
no edges. Hence, by the argument in Example 6.50, YG is asymptotically 
normal provided npm^ 
-> 00. (In the exceptional case e<? = 0, XG is de-
generate but not YG; YG is dominated by {K2} and is thus asymptotically 
normal provided n2p -* 00.) We leave the details to the reader (Exercise!). 
The case p —> 1 is reduced to the case p —► 0 by considering complements; 
YG{G(TI,P)) 
equals the number of induced copies of the complement of G in 
the complement of G(n,p), that is, in G(n, 1 - p). 
The case p -> po 6 (0,1) is more interesting. Let us, for simplicity, assume 
that p(n) = p is constant. In this case, a detailed calculation shows that, with 
"n = (n)„ c/aut(G), 
YG(K2) = ^ - ^ (eo - C y j p J j ^ O -p)Cf)-«e-i. 
More generally, for every H there is a polynomial QH such that 
Ϋα(Η) = 
^-Q„(p). 
In particular, YG{H) = 0(nv°-v») 
and thus Y¿(H) = 0(η υ σ-"«/ 2); more-
over, O can here be replaced by Θ if Qn(p) ¿ 0, while YG(H) = Y¿(H) = 0 
if QH(P) = 0. Consequently, YG is dominated by the smallest non-null graphs 
H with QH{p) T*0. 
If P Φ C G / C ^ ) , then QKi{p) Φ 0 and thus YG is dominated by {K2} (just 
as .Ve is); consequently, YG is asymptotically normal by Theorem 6.49. (This 
is essentially the first projection method; see Remark 6.44.) In this case, 
VaiYG^YG{K2)2^n2v°-2. 
If, however, p = εο/("^), then ί σ ^ ) = 0 and we have to study larger H. 
The next possibility is va = 3, which holds for two graphs H, namely, P2 and 
K3. (Recall that we only consider graphs H without isolated vertices.) Hence, 
if further YG{P2) # 0 or V b ^ ) Φ 0, then YG is dominated by 
{P2,K3}. 

172 
ASYMPTOTIC DISTRIBUTIONS 
Since both P¿ and K$ are connected, YQ is asymptotically normal in this case 
too by Theorem 6.49. (This is essentially the second projection method; see 
Remark 6.44.) In this case, however, VarVc ~ 
n2v°~3. 
The remaining possibility is that Yb(#2) = YG{PT) = ^0(^3) = 0. Graphs 
G that satisfy these conditions are known as p-proportional. For such G and p, 
it may be shown that YG{2KI) 
Φ 0, where 2/f2 is the graph consisting of two 
disjoint edges. Hence the list of possible cases ends here: YQ is dominated 
by {H : VH = 4} and Var(Yc) x n2v°~4. 
Furthermore, since 2K2 is discon-
nected, Corollary 6.47 shows that YQ is not asymptotically normal. 
More precisely, this argument yields the following result; see Barbour, 
Karonski and Rucinski (1989) and Janson (1990a, 1995a) for further details. 
Theorem 6.52. Consider G(n,p) where p is fixed, 0 < p < 1. 
(i) We have 
n i - » O ( y G _ E Y G ) 4 N ( 0 , ^ ) , 
for some σ\ > 0; 
(ii) a\ — 0 if and only if p = CGI^), 
and then 
n 3 / 2 - » c ( y o _ E y G ) ^ N ( 0 , a 3
2 ) , 
for some σ\ > 0; 
(iii) σ\ = σ\ = 0 if and only if G is p-proportional, and then 
n2-«c ( y G _ E y G ) ^ 
a ( Z 2 _ j) + 
bZ2 
where Z\,Z-i 6 N(0,1) are independent, a,b are constants, and a < 0. 
This limit is non-degenerate and not normal. 
■ 
The parameters in the limit distributions may be given explicitly; here we 
only remark that a calculation shows that, for (iii), 
a = 2p(l - p ) g 2 „ , ( p ) = - - ^ ( ^ « ( ΐ 
- p ) < 7 H o . 
Remark 6.53. It is not trivial to construct proportional graphs, and not 
even obvious that any exist at all. The smallest proportional graphs have 
eight vertices; the first example was given in Barbour, Karonski and Rucinski 
(1989), and another example is the wheel consisting of a cycle of seven vertices, 
all joined to a central vertex. Deterministic and probabilistic constructions 
showing that p-proportional graphs exist for every rational p € (0,1) are given 
by Janson and Kratochvfl (1991), Kärrman (1993), and Janson and Spencer 
(1992). 
Kärrman (1994) has further constructed a graph (with 64 vertices) that is 
proportional (with p = 1/2) and, furthermore, such that YG{H) — 0 for every 
H with VH = 4 except 2Ä2; in this case the constant b vanishes. 

PROJECTIONS AND DECOMPOSITIONS 
173 
Applications to subgraph counts in G(n, M) 
We have so far treated the decomposition method for G(n,p). In fact, the 
results extend to the continuous time random graph process {G(n, t)}t defined 
in Chapter 1, with a random graph evolving in time. The method then yields 
results on convergence of graph functionals as stochastic processes. See Janson 
(1994a) for details; an example treated there is the asymptotic normality of 
the maximum number of isolated edges during the evolution of a random 
graph. 
The process version of the method also leads to results for G(n, M) by 
considering the random time when the evolving graph has exactly M edges. 
We let p = M/(£) and define the variables Sn(H) by (6.29), now letting Ie be 
the edge indicators for <G(n, M). The expansion (6.30) is still valid, with the 
same Xn(H) 
as for G(n,p), but the terms are no longer orthogonal. Note that 
5n(K"2) = 0 for G(n, M), so clearly the term with H = K2 disappears from 
(6.30). Moreover, a further analysis shows that also terms where H contains 
an isolated edge (i.e., a component K2) require special treatment. This leads 
to the following result; for the proof we again refer to Janson (1994a), which 
also contains some extensions. 
Theorem 6.54. Let Xn be a graph functional o/G(n, M), where M = M(n) 
-* 00, such that there is a finite family Ή with Xn(H) = 0 for H $ Ή. (for 
every p). Let p = Λί(η)/(!)), and let Ή! be the subfamily of all graphs in 
Ή. with at least three vertices. Suppose that p —► po < 1, and that βη is a 
sequence of positive numbers such that for every H G Ή', 
a(H) = lim X^(H)/ßn 
exists. 
Suppose further that a(H) = 0 for every H € Ή! with two or more isolated 
edges, and that if H e H' with a(H) φ 0, then npm^H~> -> 00. Then, 
2=¿5- Λ 2 a(H)U(H), 
(6.47) 
where Ή. = {H € Ή! : every component of H has at least three vertices}, 
U(H) is as in Theorem 6.43 and a„ equals the expectation of Xn calculated 
forG(n,p). 
In particular, if a(H) = 0 for every H G Ή with two or more components 
with at least three vertices, but a(H) Φ 0 for some H G Ή', then X„ is 
asymptotically normal. 
■ 
Note that the smallest H that gives a non-normal term in (6.47) is the 
union of two copies of Pi, which has six vertices. 
Comparing the limits in Theorems 6.45 and Theorem 6.54, we see that 
the following holds, at least provided both theorems apply (and, presumably, 
more generally): If a graph functional is dominated by a family of graphs 

174 
ASYMPTOTIC DISTRIBUTIONS 
that have no isolated edges, then it has the same asymptotic distribution for 
G(n,p) and G(n, M), with M ~ p(%)- On the other hand, if the functional 
is dominated by K2 (i.e., the situation when the first projection applies), 
then the asymptotic distributions may be completely different; moreover, the 
variance for G(n,p) is of a larger magnitude than the variance for G(n, M). 
(See Pittel (1990) for a different approach to the relation between asymptotic 
distributions for the two models.) 
Example 6.55. We may now study the subgraph counts XG (arbitrary sub-
graphs) and YG (induced subgraphs), where G is fixed, for the random graph 
G(n,M), M = M(n); see Examples 6.50 and 6.51 for the corresponding re-
sults for G(n,p). We define p = M/(£), and begin by studying XG· 
Since the term with H = K2 in the decomposition (6.30) plays no role for 
G(n,M), we consider the graphs H φ $,Κι and find the ones with largest 
order of XG(H); by (6.46), these are the subgraphs H CG with v» > 3 and 
smallest order of n"HptH. 
Let us first assume that G has a component with three or more vertices, that 
is, G has a subgraph P2, and assume that npm^ 
-> 00. Then the argument in 
the proof of Lemma 3.15, together with n»(2Ka)p*(2Ka) » n
v ^ ) p e ( P , ) . shows 
that every such extremal H is connected, and it follows by Theorem 6.54 that 
Xa{G{n, M)) is asymptotically normal. We see further that the asymptotics 
of XG are the same for G(n,M) as for G{n,p) when M is so small that 
K-i is not a leading overlap, but not for larger M. In the case M « p(") 
with p € (0,1) fixed, VarXG ~ n2v°~2 for G(n,p) but VarXG x „2«o-3 for 
G(n, M) (dominated by Pi and possibly K$, when K2 is ignored). 
In the exceptional case when every component of G has at most two ver-
tices, that is, G consists of isolated vertices and edges, XG is deterministic 
if e© < 1. If eG > 2, a modification of the argument above shows that 
XG is asymptotically normal in this case too, provided n3^2p -> 00 and 
n3/2(l - p) -* 00; we omit the details. 
The normalization used in Theorem 6.54 is not the natural one by the mean 
and variance of XG, but it may be shown that in this case (and many others), 
all moments of the normalized variable converge to the corresponding mo-
ments of N(0,1), and hence convergence holds with the natural normalization 
too. We may summarize the result as follows. 
Theorem 6.56. If eG > I, M = M(n) > n1'2, (£) - M » n1'2 and 
npm(C) _> 0 0 ; where p _ Ml {I), then XG(G(n, M)) 4 N(0,1). 
■ 
Turning to the induced subgraph count YQ, we see by similar arguments 
that when p -> 0, we have (just as for G(n,p)) the same dominating graphs H 
as for XG, provided Pi C G, and thus Ye is asymptotically normal provided 
M is not too small. Again, this may be shown also in the exceptional case 
when G consists of isolated edges and vertices, provided VQ > 3 (otherwise 
YG is constant). 

PROJECTIONS AND DECOMPOSITIONS 
175 
Theorem 6.57. If vG > 3, M = M(n) 
» 
nx'2, 
p = Λί/β) -► 0 and 
npm(G) _> ^ 
ί Λ ε„ yG(G(n, M)) 4 N(0,1). 
■ 
Again the case M/ (£) -» 1 can be handled by considering the complements, 
and we obtain asymptotic normality in this case too, provided (£) - M is not 
too small. 
Finally, in the case M ss p(£), P fixed, Theorem 6.54 yields the following 
analogue of Theorem 6.52; see Janson (1994a, 1995a) for details. We let U° 
denote the set of unlabelled graphs with k vertices, none of them isolated, and 
let Ul denote the subset of connected graphs with k vertices. 
Theorem 6.58. Let 0 < p < 1 be fixed and consider G(n, M) where {for 
simplicity) M = M{n) - L(2)pJ· 
(i) We have 
n 3/ 2-" c(Y G-Ey G)4N(0,<72) 
for some σ% > 0; 
(ii) σ\ = 0 if and only if YG(H) =0forHeU§ 
= {P2, K3), and then 
n2-v°{YG-EYG)lN(0,a2
4) 
for some σ\ > 0; 
(iii) σ\ = σ\ = 0 if and only ifYG(H) 
= 0 for WeW3
0UM4
c, and then 
n 5 / 2 - « C ( y G - E F G ) 4 N ( 0 , a f ) 
for some σ\ > 0; 
(iv) σ2 = σ\ = σ\ = 0 if and only if YG{H) =0forHeU$l) 
Z/4° U Ul, and 
then 
η 3-"°(Κ σ - KYG) A a{Z\ - 1) + b{Z\ - 1) + cZ3, 
where Z\,Zi,Z% 
£ N(0,1) are independent, a,b,c are constants, and 
a φ 0. This limit is non-degenerate and not normal. 
■ 
Kärrman's (1994) example shows that there exists a graph G such that 
case (iii) occurs. We do not, however, know if there exists any graph such 
that case (iv) happens; thus we do not know whether YG{G(n, M)) is always 
asymptotically normal. 
In any case, as remarked in connection with Theorem 6.52, a p-proportional 
graph G has YG{2Ki) φ 0, and thus is not an example of (iv) in the present 
theorem. In other words, the classes of graphs that yield non-normal limits 
in Theorems 6.52 and 6.58 are disjoint. 

176 
ASYMPTOTIC DISTRIBUTIONS 
Other projections 
We have in this section studied projections using the variables Sn(H) only. 
Other projections are useful in other situations; we mention a few examples 
and references. 
A similar decomposition has been used by de Jong (1996) to extend The-
orem 6.5 to random hypergraphs. 
Andersson (1998) has studied (directed) subgraph counts in a random tour-
nament, obtaining results similar to Theorem 6.52. In that case, however, 
there is an infinite list of possible cases. 
Janson (1994b) studied the numbers of spanning trees, Hamilton cycles or 
perfect matchings in G(n,p) and G(n, M). For G(n, M), with M » n3/2 and 
(2) — M » n, these random numbers are shown to be asymptotically normal, 
by approximating with a linear function of Xp7; a kind of "first projection" 
for G(n,M). For M x n3/2, as well as for G(n,p) with p = Ω(η-1/2) and 
1 — p ~3> n - 2, the numbers are shown to be asymptotically log-normal. 
Furthermore, in Chapter 9 we will prove results on asymptotic distributions 
for random regular graphs by projecting onto functions of the cycle counts 
Zi\ in that case the basic variables Z¡ have asymptotic Poisson distributions 
and the resulting asymptotic distributions are quite different from the ones 
obtained here, see Theorem 9.12. 
6.5 
FURTHER METHODS 
Finally, we briefly mention a couple of other methods. 
Martingales 
As mentioned above, the proof of Theorem 6.43 is based on a martingale limit 
theorem; another martingale limit theorem is used by de Jong (1996). Such 
theorems may also be used directly in situations where the methods described 
above fail. We will not go into details, which are rather technical, and mention 
only one example. 
Example 6.59. Barbour, Janson, Karonski and Rucinski (1990) studied the 
number Xd of cliques of a given fixed size d > 2 in G(n,p), where a clique is 
defined as a maximal complete subgraph, that is, a Kd that is not contained 
in a Kd+i· It was shown that if p = p(n) is such that EXd -> λ < oo, then 
Xd 4 Ρο(λ), and if EXd -+ 00, then Xd A N(0,1). 
The Poisson part was proved using the Stein-Chen method. For the normal 
part, different methods were used for different ranges of p; for certain p the 
first projection method works, and for a larger range it is possible to use 
Corollary 6.46 (with the family H consisting of K2, K3,Kd and the 'multistars' 
Mdtr obtained by adding r > 1 vertices to Kd, joining them to all vertices 

FURTHER METHODS 
177 
of A'rf). For p close to the upper threshold, this fails, since then (6.39) does 
not hold; instead a martingale limit theorem was invoked directly. 
Generating functions 
A method that is widely used to find asymptotics, including asymptotic distri-
butions, for combinatorial problems is to define a suitable generating function 
and obtain results through a study of its asymptotics. This method has, how-
ever, been used rather sparsely for random graphs. We refer to Pittel (1990) 
and to Janson, Knuth, Luczak and Pittel (1993) for examples. 

7 
The Chromatic Number 
In this chapter we present results on the chromatic number which, due to 
their elegance and importance, range among the very best in the theory of 
random graphs. We begin with Frieze's beautiful method, which combines 
the second moment method with large deviation inequalities to estimate the 
independence number of random graphs. In Section 7.4 we describe Bollobas's 
ingenious argument for determining the chromatic number of dense random 
graphs - probably the most important and celebrated result on random graphs 
for the last years. Then we analyze an expose-and-merge algorithm, based on 
Matula's original idea, which is used for estimating the chromatic number in 
the sparse case. Finding the chromatic number can be viewed as a vertex 
partition problem. In the last part of the chapter we discuss this problem in 
a more general form. 
7.1 
THE STABILITY NUMBER 
Let us recall that a set of vertices of a graph G is independent, or stable, if 
it contains no edges of G. The size of the largest among such sets, denoted 
by a(G), is called the independence (or stability) number of G. In this section 
we study the behavior of a(G(n,p)) - a random variable closely related to 
the chromatic number x(G(n,p)). Note that the independence number of a 
graph is the same as the clique number of the complementary graph, so the 
results below can also be stated in terms of the clique number of G(n, 1 - p). 
179 
Random Graphs 
by Svante Janson, Tomasz Luczak and Andrzej Rucinski 
Copyright © 2000 John Wiley & Sons, Inc. 

182 
THE CHROMATIC NUMBER 
again. Thus, α^ achieves its largest value either at i\ = min{t > 1 : 6j < 1}, 
or at »2 = max{i < f c : 6 ¿ > l } + l. The reader can easily verify (Exercise!) 
that ¿i is much smaller than k, so the factor (1 — p)~' is almost negligible 
and ¿i = 0(1 +k2/n); 
more precisely, if k2/n < 1, then ¿i = 1, and otherwise 
¿i x k2/n (in fact, i'i ~ k2/n if k2/n -* oo and np -* oo). In order to find a 
lower bound for ¿2, set i2 = \k{\ - 1/log5 np)]. Then 
i»i-2-i > . 
, * 1 0 
exp(i 2p+0(i 2p 2)) > - ^ - 
> 1, 
2n log 
np 
logl np 
and so 12 > t'2. 
Let us estimate the value of ai3. 
The inequalities i2 > i2 = \k{l 
-
1/log5 np)] and (7.7) imply that for i'2 < k we have 
2 
(«)(1-P)(Í) ( 
P) 
- 
E X 
^ / 
e2nk 
\fc-«2 
/ ε<:\ 
^ί(ΓΓ^βχρ(-2(Γ^))) 
e 2nplog 1 0npexpí--log 5npj J 
= οζη'1) , 
while for t2 = k, directly from (7.7), we get a¿2 = a* = 1/EX = o(n - 1). 
Thus, the contribution to £V a> coming from the terms with large indices is 
negligible, and the sum is dominated by terms with indices close to i\. 
Let us consider two cases. If p > log2n/>/ñ, then k < 2^/ñ/logn, ¿1 = 1 
and, furthermore, for every i < i3 = flog np] and n large enough, 
„ . <2|a„ -ri-«) < e-a^^fm 
< (£)'. 
Σ
α · ^ Σ ( — ) 
+ * m a x { o i , , a í , } < — + o ( l ) = o ( l ) . 
(7.9) 
so a¿, = o(n '). Hence, 
k 
<s_1/2Jfc2v . , 
, 
^ 
Ak2 
» = 1 
» = 1 
Next, suppose that Ce/n <p < log2 n/yfñ for some large constant Ct > 0. 
Then, rather crudely, 
k 
\ 
Oi, < (1 - P)" ( Í ¿ ) < exp((l/2 + o(l))pi2) < exp( 2 log3 np^ 
and 
V o < < Armaxíoj,,^,} < fcexpf——3—) < e x p ( — 3 — ) - 1 . 
(7. 
r-r* 
V21osr no/ 
Moe n o ' 
t = l 
^2log np' 
Mog ηρ^ 

THE STABILITY NUMBER 
183 
Now, in order to get (7.4) and (7.5), it is enough to put the values of E X 
and E.Y2 given by (7.7), (7.8), (7.9) and (7.10) into (3.3) - a stronger form 
of the second moment method. 
■ 
Let us remark that, although inequality (3.2) is slightly weaker than (3.3), 
for most random graph problems it works just as well. Here, however, it is 
not the case - for small p the inequality (3.2) gives a worthless negative lower 
bound for the probability Y(X(ke) 
> 0). 
On the other hand, at first sight, the estimate (7.5) does not look terribly 
useful either - although positive it tends to 0 faster than expí-^/^/Zlog n). 
One might hope that this is because our estimates were too crude. Indeed, 
one can bound the variance of X more carefully and show that (3.3) yields 
(7.4) also for some p which tends to 0 faster than log2n/v/"· However, for 
p =r p(n) which tends to 0 very quickly the second moment method utterly 
fails. It is not hard to understand why it does so poorly in this case: if, 
say, p — n - 3/ 4, then the largest stable sets are larger than n3/4, and the 
majority of pairs of such sets share a substantial amount of elements which 
makes E(X2) much larger than (EX) 2. Quite surprisingly, (7.5) can still be 
used for the evaluation of a(G(n,p)), provided it is supplemented with a large 
deviation inequality of a martingale or Talagrand type. We owe this profound 
observation to Frieze (1990), who showed that the estimates for a(G(n,p)) 
given by (7.6) remain valid also for p < l/>/"· 
Theorem 7.4. Let ε > 0 and let k±e be defined as in (7.2). Then there exists 
a constant Ce such that for Ce/n <p = p(n) < log - 2 n a.a.s. 
A_£ < a(G(n,p)) < kt. 
Proof. By Lemma 7.3, the assertion holds for log2 n/y/ñ < p < log-2 n, and 
for the whole range of p = p(n) a.a.s. a(G(n,p)) < ke. Thus, it is enough to 
show that if Cg/n < p < log2n/y/ñ, 
then a.a.s. a(G(n,p)) > fc_e. 
Note that Talagrand's large deviation inequality, Theorem 2.29, can be 
applied to a(G(n,p)) with c¿ = 1 and rp(r) = ¡V] (see Examples 2.35 and 
2.33), and thus by (2.35) we get 
P(a(G(n,p)) < k-c - l)P(a(G(n,p)) > 
k_e/2) 
ε/2 
< e x p í - (fc_f/2 - *_Γ + 1)^ 
4fc_e/2 
) 
(7.11) 
< e x p ( - - i £ ^ ) = e x P ( - - i l - ) . 
V 8 log np/p/ 
\ 
8p log np/ 
Thus, combining (7.11) and (7.5) (for ε/2), it follows that, for large n, 
P(a(G(n,p)) < *_e) < e x p ( - 
f 
+ 
~^—) 
V 8p log np 
p log n p ' 
* exp(-í6pib;)=o{1) · 

184 
THE CHROMATIC NUMBER 
7.2 
THE CHROMATIC NUMBER: A GREEDY APPROACH 
Let us recall that the chromatic number x(G) of a graph G is the smallest 
integer I such that the vertex set of G can be partitioned into I stable sets. 
The problem of computing the value of the chromatic number of a graph has 
drawn much attention in graph theory, but in this chapter we will not use 
any sophisticated results from this area, relying mainly on known elementary 
facts about \(G) (see, e.g., Bollobás (1998) or Diestel (1996)). 
We begin our study of x(G(n,p)) with the simple observation that for 
any graph G with n vertices and stability number a(G), the chromatic num-
ber x(G) is bounded from below by \n/a(G)'\. 
Thus, the upper bounds 
for a(G(n,p)) given by Theorem 7.1 and 7.4 yield immediately that a.a.s. 
X(G(n,p)) > n/ke if p is a constant (or p = p(n) tends to 0 slowly enough), 
and x(G(n,p)) > n/ke if Ce/n <p = p(n) < log - 2 n for a large enough con-
stant C = Ce. Replacing these bounds by simpler, slightly smaller expressions 
we arrive at the following result. 
Corollary 7.5. 
(i) If p — p{n) > n~s for every δ > 0 but p < c for some c < 1 , then a.a.s. 
* ( G ( " ' P ) ) * 21ο δ 6η-"ο 8 61ο 6 όη' 
where b = 1/(1 - p ) . 
(ii) There exists a constant Co such that ifCo/n 
< p = p(n) < log- n, then 
a.a.s. 
np 
X ( G ( U ' P ) ) * 2 1 o g n p - 2 1 o g l o g n p + l " 
" 
The main question about x(G(n,p)) is whether the vertex set of G(n,p) 
can be partitioned into stable sets of nearly maximum size, that is, whether 
x(G(n,p)) = (1 + op(l))n/a(G(n,p)). 
(7.12) 
In this section we examine an algorithmic approach to this problem, and 
describe a simple algorithm coloring the vertices of G(n,p) which a.a.s. uses 
only twice as many colors as anticipated in (7.12). 
Let D(G) = max//cG δ{Η) be the degeneracy number of a graph G and, as 
in Section 3.1, let m(G) = max/fcc \E(H)\/\V(H)\. 
Much of our argument 
will rely on the following well-known simple upper bound on x(G). 
Lemma 7.6. There exists a polynomial time algorithm which colors the ver-
tices of every graph using at most 1 + D(G) colors. In particular, 
X(G) < 1 + D(G) < 1 + 2m(G). 
■ 

THE CHROMATIC NUMBER: A GREEDY APPROACH 
185 
The above fact is particularly well suited for small subgraphs of a random 
graph, which are quite sparse and thus can be effectively colored with only 
a few colors. Throughout this chapter we will use the following estimates of 
the density of small subgraphs of G(n,p). Statements (i)-(iii) below can be 
easily verified using the first moment method (Exercise!). In order to prove 
(iv) it is enough to compute the expected number of subgraphs of G(n,p) 
with m(F) > 1.45 and fewer than 0.05n vertices, and use the fact that if 
np < 1.001 then a.a.s. the size of the largest component of G(n,p) is smaller 
than 0.05n (see Theorem 5.4). 
Lemma 7.7. 
(i) There exists a constant Co such that for np > Co a.a.s. every sub-
graph F o/G(n,p) with fewer than nf log2 np vertices satisfies m(F) < 
np/ log2 np. 
(ii) Ifp < log2 n/y/ñ, then a.a.s. for every subgraph F o/G(n,p) with fewer 
than 2\/n\ogn 
vertices we have m(F) < log n. 
(iii) Ifp < n~6/7, then a.a.s. for every subgraph F o/G(n,p) with fewer than 
70-s/n log n vertices m(F) < 1.45 holds. 
(iv) If np < 1.001, then a.a.s. m(F) < 1.45 for every subgraph o/G(n,p). 
In particular, a.a.s. x(G(n,p)) 
< 3. 
■ 
Note that although it follows from the proof of Theorem 7.4 that the ex-
pected number of stable sets of size (1 — o(l))a(G(n,p)) is quite large, such 
sets have a natural tendency to cluster together and so, possibly, they do 
not cover all vertices of the random graph. However, in the first attempt to 
estimate x(G(n,p)) from above, we will defer this problem for a while, and 
instead look more closely at the stable sets which are about half the size of 
the largest one. More specifically, the following fact can be shown using the 
first moment method (Exercise!). A stable set is called maximal if it is not 
contained in any other stable set. 
Lemma 7.8. 
(i) There exists a constant Co such that if Co/n < p < log - 2 n, then with 
probability 1 - o(n~2), G(n,p) contains no maximal stable set smaller 
than (log np - 3 log log np) /p. 
(") UP > log - 3 n butp<c 
for some c < 1, then with probability 1 -o(n~2) 
every maximal stable set o/G(n,p) is larger than log6n — 31og6logfcn, 
where b = 1/(1 - p ) . 
■ 
Lemma 7.8 tells us that a.a.s. each stable set much smaller than ia(G(n,p)) 
can be extended to a bigger one. In particular, every vertex belongs to a stable 
set of size about a(G(n, p))/2. As was observed by Grimmett and McDiarmid 

186 
THE CHROMATIC NUMBER 
(1975), one can use this fact to describe an algorithm which colors the vertices 
of G(n,p) with (2 + op(l))n/a(G(n,p)) colors. 
Theorem 7.9. There exists a polynomial time algorithm CHR for which the 
following hold: 
(i) there exists a constant Co such that if Co/n < p < l/log 2n, then a.a.s. 
CHR uses no more than np/ (log np—6 log log Tip) colors to properly color 
the vertices 
ofG(n,p); 
(») «/P > log - 2 n but p < c for some c < 1, then a.a.s. the number of colors 
used by CHR to color the vertices of G(n,p) is bounded from above by 
n/(logt n - 6logt logfcn), where b = 1/(1 - p). 
Proof. We will show only the first part of the assertion; the proof of (ii) is 
similar. Note first that in a random graph G(n,p) one can find a maximal 
stable subset 5 by examining only those pairs of vertices of G(n,p) which 
have at least one end in S. Indeed, to construct S greedily start from any 
vertex, put it into 5, then check for every other vertex if it has neighbors 
among the vertices already in 5; if this is not the case add such a vertex to 5. 
Now color the vertices of S using the first color. Then, the graph obtained 
from G(n,p) by deleting the vertices of S can be viewed as the random graph 
G(n - |5|,p), so we can repeat the above greedy procedure over and over 
again, until the number of vertices in the graph drops below nf log2 np. If Co 
is sufficiently large, then, due to Lemma 7.8, the number of colors used so far 
is a.a.s. bounded from above by 
np 
np 
log(np/ log2 np) - 3 log log(np/ log2 np) 
log np - 5 log log np 
Furthermore, Lemmas 7.6 and 7.7(i) imply that a.a.s. the remaining vertices 
of the graph can be effectively colored by at most 2np/ log2 np + 1 colors. 
Consequently, a.a.s. 
X(G(n,p)) < , _ _ 
? ' 
+
Γ
^ 
+ 1 < 
Π Ρ 
log np - 5 log log np 
log2 np 
~ log np - 6 log log np 
Can we do better than the algorithm CHR and effectively color G(n,p) 
using a substantially smaller number of colors? Clearly, in order to reduce 
the number of colors by a constant factor, we need to describe a fast procedure 
IND(á) which a.a.s. finds in G(n,p) a stable set larger than (l/2+<5)a(G(n,p)) 
for some δ > 0. (The problem of the existence of IND(J) was posed by Karp 
(1976) and, as was observed by Juels and Peinado (1998), has some interesting 
cryptographic consequences.) In fact, Matula (1987) showed that, having such 
a procedure as a subroutine, one could devise an algorithm which a.a.s. colors 
the vertices of G(n,p) with fewer than (2 - á')n/a(G(n,p)) colors, for some 
positive constant <5'. 

THE CONCENTRATION OF THE CHROMATIC NUMBER 
187 
However, at this moment neither do we know what IND(6) should look like, 
nor do we have any idea how to show that such a procedure does not exist. 
Although it is possible to find in G(n,p) stable sets slightly larger than those 
whose existence is assured by Lemma 7.8, and describe algorithms which a.a.s. 
color the vertices of G(n,p) with fewer colors than CHR does, all improvements 
are only with respect to the second-order terms: all these procedures produce 
stable sets of size (1/2 + op(l))a(G(n,p)). (Examples of such algorithms were 
given, e.g., by Bollobás and Thomason (1985), Pittel and Weishaar (1997), 
and Jerrum (1992); a survey on the algorithmic theory of random graphs was 
published by Frieze and McDiarmid (1997).) Thus, the problem of existence of 
IND(<$) continues to be the most important open question of algorithmic flavor 
in the theory of random graphs. To illustrate our ignorance concerning this 
subject let us mention that even the following problem, posed independently, 
in various variants, by Jerrum (1992) and Kucera (1995), has not been settled 
so far. 
Problem 7.10. Let 0 < a < 1/2, 0 < p < 1, and let Gn,p,a be a random 
graph obtained from G(n,p) by choosing randomly a subset S of size [na\ 
of the vertex set o/G(n,p) and removing from G(n,p) all the edges with both 
ends in S. Describe a polynomial time algorithm which a.a.s. finds a maximum 
stable set in Gn)Pi0 for every a > 0. 
Note that if a > 0, then a.a.s. S is the unique maximum stable set of G„,p,0 
(Exercise!). Alon, Krivelevich and Sudakov (1998) provided an algorithm 
which a.a.s. finds 5 in a polynomial time for a = 1/2. Moreover, as observed 
by Kucera (1995), if a > 1/2, then the problem has an immediate solution: 
in this case all vertices of S a.a.s. can be identified just by inspecting their 
degrees (Exercise!). 
7.3 
THE CONCENTRATION OF THE CHROMATIC NUMBER 
From Theorems 7.1, 7.4 and 7.9 we know that for every δ > 0 and c < 1, 
and for every function p such that Cs/n < p < c for some constant C¿, 
the chromatic number x(G(n,p)) a.a.s. lies between n/a(G(n,p)) and (2 + 
<$)n/a(G(n,p)). But is it sharply concentrated, in other words, is there some 
function h(n) = /i(n,p) such that a.a.s. x(G(n,p)) = (1 + op(l))/i(n)? 
One can immediately see using the vertex exposure martingale and Corol-
lary 2.27 that x(G(n,p)) is concentrated in an interval of length Op(y/ñ). 
This 
fact, however, does not answer our question in the case of a sparse random 
graph, when the chromatic number is of an order smaller than y/ñ. Nonethe-
less, Shamir and Spencer (1987) proved a sharp concentration of x(G(n,p)) 
throughout the entire evolution of G(n,p). 
Here we present a somewhat 
stronger version of their result given by Luczak (1991c), based on an idea 
of Frieze. 

188 
THE CHROMATIC NUMBER 
Theorem 7.11. For every sequence p = p(n) there exists a function h(n) 
such that the following hold: 
(i) ifp > n-«/7, then x(G(n,p)) = (1 + ορ(1))ή(η),· 
(ii) ifp< n-W', then a.a.s. h(n) < x(G(n,p)) < h{n) + 1. 
Proof. As we indicated earlier, statement (i) is easy for a sufficiently dense ran-
dom graph. If p > po = log2 n/y/ñ, then, by Corollary 7.5, a.a.s. x(G(n,p)) > 
x(G{n,po)) > y/ñlogn/2. Corollary 2.27 applied to the vertex exposure mar-
tingale, gives 
P(|X(G(n,p)) -Ex(G(n,p))| > v^loglogn) = o(l), 
that is, in this case the assertion (i) follows with h(n) = Ex(G(n,p)). 
Assume now that p < log2 n/y/ñ and let h = h(n) be the smallest natural 
number for which 
P(x(G(n,p))</0>l/logn. 
Consequently, P(x(G(n,p)) < h) < 1/logn, and so a.a.s. x(G(n,p)) > h. 
Denote by Ϋ the number of vertices in the largest induced subgraph of G(n,p) 
which can be colored by at most h(n) colors, and set Y = n - Ϋ. We first 
prove that EV < y/n log n. Indeed, suppose that KY > y/n log n. Since 
altering the presence of the edges incident to a single vertex cannot affect the 
value of Y by more than one, Corollary 2.27 applies to the random variable Y 
with vertex exposure. This gives 
P(x(G(n,p) < Λ) = P(y = 0) < Ψ{Υ < EY - y/ñ~\o~gñ) 
< exp(-logn/2) < 1/logn , 
contradicting the choice of h. Thus EY < y/n log n and, once again using 
Corollary 2.27, we get 
P(F > 2y/nlogn) < f{Y > E V + y/nlogn) < exp(-logn/2) < 1/logn. 
Hence a.a.s. all except at most 2\/nlogn vertices of G(n,p) can be colored 
using at most h colors. Moreover, Lemmas 7.6 and 7.7(ii) imply that for such 
a function p a.a.s. each subgraph of G(n,p) with at most 2v
/nlogn vertices 
can be colored using at most 2 log3 n + 1 colors. Thus, in this case a.a.s. 
/i<X(G(n,p)) </i + 21og3n + l. 
(7.13) 
Note now that if p > n _ e / 7, then, by Corollary 7.5, a.a.s. x(G(n,p)) > 
nl/7/logn. Thus, (7.13) implies that 2log3 n +1 = o(h) and, again by (7.13), 
the assertion (i) follows. 
The argument for p < n~6^7 is slightly more involved. Let us start with 
some comments on the rather uninteresting case np < 1.001. If np -> 0, then 

THE CONCENTRATION OF THE CHROMATIC NUMBER 
189 
a.a.s. G(n,p) is a forest (see Section 5.1) and so a.a.s. 1 < x(G(n,p)) < 2. If 
n _ 1 / 1° < np < 1.001, then a.a.s. G(n,p) contains at least one edge, and thus, 
by Lemma 7.7(iv), a.a.s. 2 < x(G(n,p)j < 3. 
Thus, we may and will assume that 1.001/n < p < n~e/1. 
Then, with prob-
ability 1 - o(l/ logn), G(n,p) contains an odd cycle of size 2[(loglogn)2J + 1, 
(Exercise!) and so h > 3. Hence, Lemma 7.7(iii) and the argument presented 
above imply that there exists h = h(n) > 3 such that a.a.s. G(n,p) has the 
following properties: 
(i) all except at most 20*logn vertices of the graph can be colored with 
at most h colors; 
(ii) for every subgraph F of the graph with fewer than 70\/n log n vertices 
we have m(F) < 1.45. 
Now to complete the proof it is enough to show that every graph G = (V, E) 
with the above two properties can be colored using at most Λ + 1 colors. 
Let 5 be a subset of V such that \S\ < 2y/n log n and the vertices of V\S can 
be colored with at most h colors. We recursively define an ascending sequence 
of sets S = So C Si C · · · C St C V in such a way that \St\ < 6201 logn 
and the neighborhood of St is a stable set in G. The recursive step is simple: 
if the set S¿ has already been found and the neighborhood of 5< contains an 
edge {v,w}, we put Sj+i = S¡U {v,w}. 
Note that \Si+\\ = |5¿| + 2 and that 
e(S¿+i) > e(Si) + 3, where e(Sj) is the number of edges of G contained in S<. 
Hence, after i steps, 
|5i| = \S\ + 2i 
and 
e(S¿) > 3¿. 
But, due to property (ii), as long as i < 34>/nlogn we have 
3i < e{Si) < lA5\Si\ = 1.45|5| + 2.9i < 3y/nlogn + 2.9t 
and so the procedure must end after at most t < 30y/n log n steps. Conse-
quently, G contains a set St of size smaller than \S\ + It < 62y/n logn such 
that its neighborhood N(St) is stable in G and all vertices outside St can be 
colored by at most h colors. 
Now one can color the vertices of G with at most h+1 colors in the following 
way. All the vertices not belonging to St U N(St) are colored with the first h 
colors, while the vertices of N{St) are colored by the (h + l)st color. Finally, 
the set St, which due to Lemma 7.6 spans a subgraph with chromatic number 
at most three, can be colored by any three of the first h colors. 
■ 
Recently Alon and Krivelevich (1997), adding to the above argument one 
more ingredient, the Lovász Local Lemma, showed that x(G(n,p)) is asymp-
totically concentrated on at most two points as long as p < n~l/2~e. 
They 
also observed the following consequence of the two-point distribution, leading, 
by a suitable choice of p(n), to a one-point distribution. 

190 
THE CHROMATIC NUMBER 
Corollary 7.12. For every ε > 0 and every positive integer sequence r = 
r(n) < n 1/ 2 -* there exists a probability sequence p — p(n) such that for suffi-
ciently large n 
P(x(G(n,p)) = r ) > l ~ e . 
Proof. We may assume that e < 1 and r > 1. Define p = p(n) as the infimum 
of all real numbers p € (0,1) such that 
P ( x ( G ( n , p ) ) < r ) < e / 2 . 
Corollary 7.5 implies that p < η~ιΙ2~εΙ2, 
and thus we may apply Theo-
rem T.ll(ii) (in the extended range). It follows that h(n) = r - 1, but 
P(X(G(n,p)) = r - 1) < ε/2. 
■ 
Let us compare the obtained concentration of x(G(n,p)) with that of 
a(G(n,p)). 
Prom Theorem 7.11 we know that x(G(n,p)) is concentrated 
on at most two points when p(n) tends to 0 quickly enough, while Remark 7.2 
states that a similar two-point concentration holds for a(G(n,p)) when p 
tends to 0 very slowly with n. This connection is probably due to the relation 
x(G) > n/a(G) 
which, for random graphs, tends to become an asymptotic 
equation. The fact that Theorem 7.11 does not specify the function Λ(η), 
while the concentration function of a(G(n,p)) is explicit, indicates that deal-
ing with the chromatic number is more difficult. The reason is not hard to 
understand: the stability number is a local parameter of a random graph, 
while the chromatic number characterizes its overall structure. Nevertheless, 
in the next two sections we will find the asymptotic value of h(n). 
7.4 THE CHROMATIC NUMBER OF DENSE RANDOM GRAPHS 
When Shamir and Spencer (1987) used Azuma's inequality (Theorem 2.25) 
to show that both the stability number and, more importantly, the chromatic 
number of G(n,p), are sharply concentrated around their expectations, it was 
not expected that martingales could help in finding the asymptotic value of 
x(G(n,p)). As we saw in the previous section, Shamir and Spencer proved 
that, in particular, for every constant p there exists a sequence h(n) such that 
x(G(n,p)) = (1 + Op(l))/i(n). In view of Corollary 7.5(i) and Theorem 7.9(ii) 
we have, with b = 1/(1 — p), 
^ L _ < h{n) < » 
21ogtn - 
logtn 
It came as a great surprise when Bollobás (1988a) used martingales to show 
that the truth lies at the left endpoint of the above interval. His paper estimat-
ing x(G(n,p)) for dense random graphs was based on a beautiful, insightful 
and, at the same time, very simple argument. By the second moment method 
it was known (see Theorem 7.1) that if 0 < p < 1 is a constant, then a.a.s. 

THE CHROMATIC NUMBER OF DENSE RANDOM GRAPHS 
191 
G(n,p) contains a stable set of size k ~ 21og6n. Bollobás observed that 
one can use large deviation inequalities to show that a.a.s. each subgraph of 
G(n, p) on at least n/ log2 n vertices contains a stable set of size k not far 
from k. But then we are done! Indeed, we can color greedily disjoint, k-
element stable sets of G(n,p), until the number of uncolored vertices drops 
below n/ log2 n. Finally, we color the uncolored vertices by new, distinct colors 
(since n/log 2n is much smaller than the anticipated value of x(G(n,p)) this 
does not increase significantly the number of colors used in the procedure). 
A crucial ingredient of the above argument is an exponential upper bound 
for the probability that G(n,p) contains no large stable set; Bollobás derived 
it from the martingale inequalities in Corollary 2.27 using an elegant thinning 
argument. We will instead deduce this bound from Theorem 2.18. 
Lemma 7.13. Let 0 < p < 1 be a constant and b = 1/(1 - p). Then the 
probability that G(n, p) contains no stable set of size [2 log6 n - 2.1 log6 log6 n] 
is bounded from above by exp(-(l - p)n2/331ogíin). 
Proof. Recall that X{k) stands for the number of stable fc-element sets of the 
random graph G(n,p). By Theorem 2.18(ii), applied to the cliques in the 
complement G(n, 1 - p) of G(n,p), we have 
«*'»»-"Η-ΣΣ£(«1*«·)· 
(7,4) 
where 
_ J 1 
if A is stable in G(n,p) 
10 
otherwise, 
and the sum in the denominator is taken over all pairs A', A" such that 
\A'\ = \A"\ = k and \A' Π A"\ > 2. As in Section 7.1, setting k = r21ogtn -
2.1 logj, log6 n], one obtains 
Σ Σ Ε ^ ^ _ Σ*=2 (*)("*:*)(!-P)^-^ 
[EX{k)]2 
EX(k) 
Elementary, though tedious calculations (Exercise!) show that in the above 
sum the first term is the largest one. Thus 
ΣΣ*Χ«ΧΛ» 
< 
(fc-i)(*)(;:*)(!-pp-1 
[EX(k)}2 
- 
EX(k) 
(i-pHn-tkpiW-P)® 
331og^n 
EX(k) 
~ ( l - p ) n 2 ' 
Bollobás's result on the chromatic number of dense random graphs follows 
from the above lemma almost immediately (we present it here in a slightly 
weaker version). 

192 
THE CHROMATIC NUMBER 
Theorem 7.14. Let 0 < p < 1 be a constant and b = 1/(1 - p). Then a.a.s. 
H 
< x(G(n,p)) < 
" 
2 logt n - log6 logfc n 
~ 2 logt n - 8 logfc logft n' 
Proof. Since the lower bound for \(G(n,p)) is given by Corollary 7.5, we need 
only to prove that a.a.s. the vertices of G(n,p) can be properly colored by 
no more than n/(21og6n - 81og6logtn) colors. Note first that a.a.s. each 
subgraph of G(n,p) on at least n/ log2 n vertices contains a stable set of size 
at least 
21°gfc(r~^~") -2llo^ogb(-^r-) 
>21og 6n-71og 6log 0n. 
Mog n ' 
Mog n> 
Indeed, due to Lemma 7.13, the expected number of subgraphs of G(n,p) 
induced by at least n/ log2 n vertices containing no stable sets of this size is 
bounded from above by 
2 " e x p ( - ( 1 - p
5
) n 2 / 1 ° S
2
4 n ) < 2 " e x p ( — 4 - ) ->0. 
V. 33 \od (nl log2 n) > ~ 
V log10 n > 
Thus, a.a.s. one can greedily color all except at most n/log 2n vertices of 
G(n,p) with at most n/(2 logfc n - 71og6 log4 n) colors. Clearly, the remaining 
vertices can be generously colored each by a new color and so a.a.s. 
x(G(n,p)) < 
< 
2 logfc n - 7 log6 logfc n 
log2 n 
n 
2 logfc n-81ogfc logt n ' 
Remark 7.15. With a little more work, Bollobas's method yields the sharper 
estimate (McDiarmid 1989) 
x(G(n,p)) = 2 logfc n - 2 logfc logfc n + O c(l) 
for p constant; moreover, with the error term replaced by Oc(l/p), this holds 
for p -*■ 0 too, provided p > n~s for every «5 > 0. 
7.5 THE CHROMATIC NUMBER OF SPARSE RANDOM GRAPHS 
Unfortunately, Bollobas's ingenious argument cannot be used to determine 
the chromatic number of G(n,p) in the whole range of p = p(n). Although 
it works very well for p > n _ i , where 5 is a small positive constant, we are 
in deep trouble when the probability p = p(n) tends to zero very quickly. 
Then, the left-hand side of the inequality (7.14) tends to one as n -► oo 
and, as we have already seen when proving Theorem 7.4, finding the correct 

THE CHROMATIC NUMBER OF SPARSE RANDOM GRAPHS 
193 
asymptotic size of the largest stable set in G(n,p) requires a combination of 
a large deviation inequality and the second moment method. One may still 
hope that FVieze's approach can be used to show that the probability of G(n,p) 
containing no large stable sets of size k ~ a(G(n,p)) 
tends to 0 much faster 
than (1)~ , as required in Bollobás's method. However, a quick inspection 
of the proof of Theorem 7.4 reveals that it is based on the large deviation 
inequality (7.11), which cannot "capture" probabilities smaller than exp(-fc), 
where k ~ a(G(n,p)). 
Thus, in order to deal with small edge probability p we will need a new 
idea: Matula's expose-and-merge approach (see Matula (1987) and Matulaand 
Kucera (1990), where it was used to determine, independently from Bollobás, 
the correct size of the chromatic number of G(n, 1/2)). We follow Luczak 
(1991b) and combine this method with FVieze's argument used in the proof of 
Theorem 7.4 to find the correct asymptotic order of the chromatic number of 
a random graph basically for all values of p > C/n, where C is a sufficiently 
large constant. Here we consider only p = p(n) for which p < log - 7n; if 
p > log-7 n but p < c for some c < 1, then one can get better estimates for 
X(G(n,p)) using Bollobás's argument presented in the previous section (see 
Remark 7.15). 
Theorem 7.16. There exists CQ such that for every p = p(n) 
satisfying 
Co/n <p< log - 7 n a.a.s. 
nP 
< X(G(n,p)) < 
np 
2 log n p - 2 log log np-l-1 
' 
~ 2 log np- 40 log log np 
The proof of Theorem 7.16 is based on Lemma 7.18 below, which, in turn, 
relies on a strengthening of Theorem 7.4 given by Luczak (1991b). This result, 
stated as Lemma 7.17, can be verified by following closely Frieze's original 
argument, but since its computational part is much more involved, we state 
it without proof. 
Lemma 7.17. Let ε > 0 and k = [2(lognp - loglognp + 1 - log2 - 
ε)/ρ\. 
Then there exists a constant Ce such that for Ce/n < p(n) < log - 7n, with 
probability at least l - o ( n - 1 ) , G(n,p) contains fnlog_5(np)/fc"| disjoint stable 
sets, each of k vertices. 
■ 
Lemma 7.18. There is a constant Co such that for Co/n < p < log - 7n, 
and n large enough, with probability greater than 1 - log - 1 np, more than 
n — 2nlog~ np vertices of G(n,p) can be properly colored with fewer than 
np/ (2 log np - 38 log log np) colors. 
Proof. We will show the statement using Matula's "expose-and-merge" tech-
nique. Let Co/n <p< log - 7n, where Co is assumed to be large enough for 

194 
THE CHROMATIC NUMBER 
later estimates. Furthermore, set 
k = [(2 log np - 37 log log np)/p], 
£=fn/(fclog 2 2np)l, 
m = \n log"l7 np]. 
Choose a subset Ai of [n] uniformly at random among all subsets of [n] with 
m vertices. Since the subgraph Hi induced in G(n,p) by A\ can be viewed 
as G(m,p), by Lemma 7.17, with probability at least 1 - o(m~l) there are £ 
disjoint stable sets in Hi, each of size k. Let us choose uniformly at random 
one such family, Jj 1,... ,I\. 
This extra randomization gives each jfc-element 
subset of A\ the same overall chance of being chosen as one of these stable 
sets. We mark all vertices from the set W = (J j = 11¡ as used and all pairs of 
vertices {υ,τυ} with v,w G Ai as exposed. 
Now choose another set Ai, uniformly at random among all subsets of 
[n] \ W of m vertices, and let H'2 be the graph induced by Ai in G(n,p). 
We would like to apply Lemma 7.17 to H2 but, although H2 can be viewed 
as G(m,p), its structure may depend on the structure of Hi, because some 
pairs of vertices from Ai could be already exposed. Here comes Matula's 
ingenious recipe. Let us ignore all exposed pairs, at least for a moment, and 
for each exposed pair {v, w} perform another random experiment in which the 
probability of success is p, and connect v and w by an edge according to its 
outcome. The graph obtained this way from H2 is denoted by Hi. Note that 
Hi can no longer be viewed as a subgraph of G(n,p) - when we expose the 
exposed pairs for the second time, we might have drawn an edge between two 
vertices which are not adjacent in G(n,p), and vice versa. However, Hi has 
one great advantage: it can be identified with a random graph G(m,p) which 
is independent of H\, because Hi and Hi were generated in separate sequences 
of random experiments. Thus, we can apply Lemma 7.17 and choose in Hi, 
again randomly, a family of I disjoint sets l\,...,If, 
which are stable in Hi 
(but not necessarily in G(n,p)). Finally, we include all vertices of |j'_i If in 
the set W containing the used vertices, and all pairs from Ai in the set of 
exposed pairs (some of them may have been already marked as exposed). 
Let us repeat this procedure r = flog22 np - log19 np] times. According to 
Lemma 7.17, the probability that for some graph Hit 1 < i < r, we have not 
succeeded with the choice of the family / { , . . . ,I\ is o{r/m) = o(log-1 np). 
Thus, let us assume that during the procedure we have produced a family of 
ri disjoint sets / j , where for each i, 1 < i < r, all sets / j , 1 < j < i, are 
stable in Hi. We will use them to define a proper coloring of all except at 
most 2nlog~3 np vertices of G(n,p). 
As we have already noticed, a set /j may not be stable in G(n,p), because 
when generating Ht, we could have included in /j a pair {v,w} which was 
an edge of G(n,p), but which was exposed at one of the earlier stages of the 
algorithm. Let s be the smallest index for which {v,w} € H,. We denote the 
number of such troublesome edges by Y. 

THE CHROMATIC NUMBER OF SPARSE RANDOM GRAPHS 
195 
Now we estimate the expectation of Y. Since 1 < s < t < r, there exist Q 
choices for s and t. At the s-th stage, when we picked a set Aa of size m, the 
ends v,w € As of a troublesome edge could be chosen in one of (™) ways; the 
probability that the pair {v, w} appeared as an edge of H, and thus of G(n, p) 
is, of course, p. Now it remains to bound the probability that both v and w 
belonged to one of the sets I\,I\...,I\. 
Note first that at the ί-th stage of 
the procedure the set At, and thus also the sets /{, J|. „ , Il
t, were chosen from 
at least n - rlk > 0.5n log - 3 np vertices. Since \I¡\ = k for all i = 1,2,..., I, 
the probability that υ belonged to \Jl
i=1 If is smaller than 2(.k/n log - 3 np. 
Similarly, the probability that w was contained in the set / ' which contained 
υ is bounded from above by 2k ¡n log - 3 np. Thus, 
-^QO^s 
^ log44 np 
n2 
8n log7 np 
_5 
< —£—— 
=3 
—%p¡— = 2n log 
np. 
2 
2 log34 n p n 2 log22 np 
Hence, from Markov's inequality, we get 
P ( r > n log - 3 np) < 2 log - 2 np < 0.5 log-1 np . 
Consequently, with probability at least 1 — log - 1 np, the procedure described 
above generates a system of disjoint sets Ij, 1 < i < r, 1 < j < i, which 
contain not more than n log - 3 np troublesome edges. Let us delete one end 
of each such edge from (J!=i Uj=i I)· Then, the resulting set has at least 
rtk - n log - 3 np>n 
— 2n log - 3 np 
vertices and, on the other hand, it can be colored using not more than 
A ^ n 
np 
ri< 
Ύ + r < 
r 
k 
2 log np — 38 log log np 
colors. 
■ 
Proof of Theorem 7.16. The lower bound for x(G(n,p)) is given by Corol-
lary 7.5. In order to get the upper bound for the chromatic number of G(n,p) 
observe that, due to Lemma 7.18, with probability at least 1 - log-1 np all 
except at most 2n log - 3 np vertices of G(n, p) can be colored using at most 
np/ (2 log np- 38 log log np) colors. Furthermore, Lemmas 7.6 and 7.7(i) imply 
that with probability 1 - o(l) the subgraph induced in G(n,p) by the uncol-
ored vertices can be colored using at most 2nplog - 2 np+ 1 additional colors. 
Consequently, with probability at least 1 — o(l) — log-l{np), 
ηΌ 
2ηΌ 
* ( G ( n ' P ) ) * 21ogn P-381oglognp + ¡o?^ 
+ * 
( 7 1 5 ) 
< 
np 
^ · > 
~ 2 log np - 39 log log np' 

196 
THE CHROMATIC NUMBER 
Thus, for say, np > logn, a.a.s. the chromatic number is bounded from 
above by np/ (2 log np- 39 log log np). Finally, if C0 < np < logn, then (7.15) 
together with Theorem 7.11 implies that o.o.s. 
χ«*».ρ» < π 
S T - Ί 
+1 < 
np 
2 log np - 39 log log np 
~ 2 log np - 40 log log np 
The above theorem states that for np being a large enough constant the 
chromatic number of G(n,p) is o.o.s. about np/(21ognp). But how large must 
this constant be to guarantee that the chromatic number is at least k, for a 
given natural number k > 4? Recently, Achlioptas and Friedgut (1999) have 
shown the existence of a sequence dk(n) such that o.o.s. G(n, (d* (n) - ε)/η) 
has chromatic number at most k, while G(n, (d* (n) + e)/n) does not. The 
sequence d*(n) is certainly bounded, but it is not known (although widely 
believed) that it converges to a limit. To avoid this problem one can define 
the threshold constant c*, setting 
et = inf{d : a.o.s. x(G(n,d/n)) > k}. 
Theorem 7.16 states that for large k we have 
cfc = (2 + o(l))fclogfc, 
(7.16) 
where the o(l) stands for a quantity which tends to 0 as k -> oo. For small 
values of k the constants c* have been estimated by Chvatál (1991), Molloy 
(1996), and Achlioptas and Molloy (1999). In particular, it turns out that 
for k > 3, during the evolution of G(n,p), the first subgraph of G(n,p) of 
minimum degree k appears before the chromatic number of the random graph 
jumps to k + 1 (see Molloy (1996) and Achlioptas and Molloy (1997)). Note 
also that if k is large then the non-empty fc-core appears in the random graph 
when its expected average degree is about k (see Section 5.1). Thus, for 
large k, (7.16) implies that the fc-core, at the moment it emerges in G(n,p), 
o.o.s. has chromatic number smaller than k. 
7.6 
VERTEX PARTITION PROPERTIES 
The concept of chromatic number can be modified in various ways, and quite 
a few of its variants have been studied in the theory of random graphs (see, 
e.g., Bollobás and Thomason (1995, 1997)). In this section we consider one 
such generalization of the chromatic number, closely related to Ramsey the-
ory. Note that having chromatic number greater than r is equivalent to the 
property that every r-coloring results in an edge with both endpoints of the 
same color. This is nothing else but a special case of a general Ramsey prop-
erty for graphs, often depicted by the following Erdös-Rado arrow notation. 
Given two graphs F and G, we write 
F->{G)l 

VERTEX PARTITION PROPERTIES 197 
if for every r-coloring of the vertices of F there is a monochromatic copy of G 
in F. (A similar notion but with respect to edpe-coloring will be thoroughly 
studied in the next chapter.) Thus, x(F) > r if and only if F -» (K2)\- 
With 
this extent of generality in mind, we will refocus our interest as compared to 
the case G — Ki studied in the previous sections of this chapter, and instead 
of asking for an analogue of the chromatic number, that is, for the smallest 
number of colors r — r(n,p) for which 
G(n,p) -+ (G)l, 
we will fix r and look for a threshold probability function for the above prop-
erty. 
Intuitively, the threshold should be determined by the requirement that for 
each subgraph H of G, the number of copies of H in the random graph G(n, p) 
is of the order of the magnitude of n. Let us explain the reason behind this 
heuristic. The copies of G contained in G(n,p) are fairly uniformly distributed 
and so, when the number of copies of G is much smaller than n, most of these 
copies are almost disjoint (see Section 3.5). Thus, it seems plausible that one 
can color the vertices of G(n,p), even with just two colors, and not create a 
monochromatic copy of G. On the other hand, if a vertex of G(n,p) belongs 
on average to many copies of G, then coloring all vertices of G(n,p) one by 
one we may expect that sooner or later we will put ourselves in a position in 
which coloring a new vertex inevitably leads to a monochromatic copy of G. 
As for every three graphs H C G, and F, the property F -¥ (G)' implies 
F -¥ (H)l, the same heuristic applies to every subgraph H of G. Note now 
that for each H C G, 
E{XH) = Q(nv"pe") 
= 0(n(np e" / ( ,'"- 1 ))"''- 1). 
(7.17) 
Hence, we anticipate that the threshold for the property G(n,p) -> (G)]. 
should be of the form n~ 1 / m 1)(G) where, let us recall, for a graph G with at 
least two vertices, 
m{l)(G) = 
max 
e" 
. 
HCG,v„>2 
VH - 1 
Note that, not surprisingly, the same function appeared in Theorem 4.9 as 
the threshold function for the property Fo(e) that all but at most en vertices 
of G(n,p) can be covered by vertex disjoint copies of G. 
Theorem 7.19. For every integer r, r > 2, and for every graph G which 
contains at least one edge and, if r = 2, satisfies Δ(<?) > 2, there exist 
positive constants c and C such that 
„ - o o 
V 
\ >rl 
| j 
ifp>Cn~l'n' 
<G>. 
Remark 7.20. Note that the number of colors r does not appear in the 
exponent of the threshold function, but is hidden in the constants. 

198 
THE CHROMATIC NUMBER 
Remark 7.21. The case in which G is a matching and r = 2 is somewhat 
different. One can show using the second moment method (Exercise!) that 
if pn — c > 1 then a.a.s. G(n,p) contains at least log log n vertex disjoint 
cycles of odd length and thus a.a.s. G(n,p) -> {G)\. However, if np = c and 
0 < c < 1, then one can use the method of moments (see Section 6.1) to show 
that the number of odd cycles converges in distribution to Poisson distribution 
Ρο(λ) for some positive constant λ = \{c) (Exercise!). Furthermore, in this 
case a.a.s. G(n,p) consists of trees and unicyclic components (Theorem 5.5), 
and for such a graph F and a matching G we have F -> (G)\ if and only if F 
contains at least 2e(G) - 1 odd cycles (Exercise!). Thus, 
lim P(G(n,p) -> (G)>) = o(c), 
where 
0 < a(c) = 
f; 
~e~x < 1. 
¿=2e(G)-l *" 
Hence, in order to have a.a.s. G(n,p) fa {G)\, we need p ■C n~l. 
Remark 7.22. It is tempting to conjecture that for given G and r the two 
constants c and C can be chosen arbitrarily close to each other, that is, there 
is a single constant C such that we can take c = C — ε and C — C + e for 
every ε > 0. There is at present not much hope, however, to determine or 
even show the existence of such a C. As we mentioned in the previous section, 
this problem remains open even in the simplest case G = K^. 
A partial result has recently been obtained by Friedgut and Krivelevich 
(2000), who proved that if G is strictly K\ -balanced, then the threshold is 
sharp in the sense defined in Section 1.6; in this case it means that there 
exists such a C = C(n) = θ(1), which, however, possibly depends on n. 
Proof of Theorem 7.19. Suppose that G(n,p) -fa {G)\. Then the largest color 
class of any coloring with no monochromatic G spans a G-free subgraph of size 
at least n/r. 
The probability that this happens is, by Theorem 3.9, smaller 
than 
2nP(G(n/r,p) 0 G) < 2 ηβχρ{-ο σΦ σ(η/τ·,ρ)}, 
where Φο(η,ρ) = min{EX// : H C G, e» > 1} and the constant CQ depends 
on G only. It follows from (7.17) that for p > Cn-m<1,(G> we have Φο(η,ρ) > 
C'n, where C' grows to infinity as a function of C, so that the probability of 
G(n,p) -fa {G)\ tends to 0 for C sufficiently large. 
For the proof of the 0-statement of Theorem 7.19 we assume that p < 
CTj-i/m 
(G\ where c is a sufficiently small positive constant. The reader can 
easily check that if Tn^(G) = 1, that is, if G is a forest with at least one edge, 
then Theorem 5.5 implies that the assertion holds whenever c < 1 (Exercise!). 
Thus, without loss of generality we may assume that VG > 3 and that for 
every proper subgraph H of G with at least two vertices 
e» 
6G 
VH - 1 
VG - 1' 

VERTEX PARTITION PROPERTIES 
199 
that is, G is strictly /^-balanced (see Section 3.2). (If this was not the case, 
one could replace G with its smallest subgraph H for which en/(vH - 1) = 
m(1)(G).) This assumption implies that there are no isolated vertices in G 
and that, for each proper subgraph H of G with at least two vertices, 
nv"-lpe" 
=Ü(ne) 
(7.18) 
for some e > 0. 
Our proof will consist of two parts: a deterministic one, where we show 
that every graph F with F -> {G)\ contains a dense subgraph of a special 
type, and a probabilistic one, where we prove that a.a.s. such dense subgraphs 
do not appear in G(n,p). 
In order to show the first part of the statement we will need a number 
of definitions. Let us recall that a hypergraph Ή is a pair (V,£), where V 
denotes the set of vertices and £ is a family of subsets of V, called hyperedges. 
A hypergraph Ή has chromatic number χ(Ή) at least three if every 2-coloring 
of vertices of Ή leads to at least one monochromatic hyperedge. We say that 
Ή. is 3-edge-critical if χ(Ή) > 3 but the deletion of any hyperedge results in 
losing this property. For graphs F and G, let H(F,G) 
be the hypergraph 
with vertex set V(F), whose hyperedges are the vertex sets of all copies of G 
contained in F. Note that for each hyperedge A o{7i(F, G), we have \A\ = VQ. 
We denote by G{A) a copy of G in F which corresponds to the hyperedge A of 
"H(F, G), and by G(7ÍQ) the graph LUeWo ^(-^)' w n e r e W-o is a subhypergraph 
of Ή. Note that F -> {G)\ if and only if the chromatic number of H{F, G) is at 
least three. Furthermore, we may assume that H(F, G) is 3-edge-critical, since 
otherwise we could replace 7i{F, G) with a 3-edge-critical subgraph, ignoring 
some copies of G in F. In our further considerations we will use the following 
result about 3-edge-critical hypergraphs, the simple proof of which is left to 
the reader (Exercise!). 
Proposition 7.23. // Ή is a 3-edge-critical hypergraph, then for every hy-
peredge A of Ή. and every vertex v € A there is a hyperedge A' such that 
AnA' = {v). 
m 
A linear path (A\,...,At) 
is a hypergraph with hyperedges 
A\,...,At, 
t > 1, such that 
10 
otherwise. 
A linear [quasi-linear] cycle (Ao, A\,..., 
At) is a hypergraph which consists 
of a linear path (Ai,...,At), 
i>2, 
and a hyperedge A0 such that 
|;4o nAi\ = < 
1 
ifi = l, 
0 
for i = 2,...,i- 
1, 
s 
iii = i, 

200 
THE CHROMATIC NUMBER 
where s = 1 [s > 1], respectively). A cycle which is quasi-linear but not linear 
we will call spoiled. 
Let V be the longest linear path in H = H(F,G). 
By Proposition 7.23, V 
contains at least two hyperedges. Let i and y be two vertices which belong 
only to the first hyperedge of V, and let Ax and ,4y be two hyperedges of 
H (which, of course, correspond to two copies of G contained in F) whose 
existence is guaranteed by Proposition 7.23, that is, Α.Π Ai — {z}, z = x,y. 
By the maximally of V, \V(V) Π Az\ > 2, z = x,y. Let iz - min{i > 2 : 
Az Π Ai φ 0}, z = x,y, and assume that, say, iy < ix. 
The hyperedges 
Αι,.. ■, Aim, Ax form a quasi-linear cycle C, which is linear if and only if \AX Π 
AjJ = 1. Otherwise C is spoiled. We also have \Ay Π V(C)\ > 2. Moreover, 
there is an edge in G(Ay) which does not belong to G{C). Indeed, as 6(G) > 1, 
take any edge of G(Ay) incident to y. We call the pair (C, Ay) a cycle with 
handle. So, we have just proved a deterministic statement that if F -> {G)\ 
then the hypergraph H{F, G) contains a quasi-linear cycle with handle. 
Now we will show that o.o.s. no such structure does exist in 
H{G(n,p),G). 
Let X, Y and Z be random variables counting, respectively, linear paths 
of length at least Blogn, spoiled cycles C = (i40,· ·. ,At), and linear cycles 
with handles (C, .A) of length less than B logn + 1, in the random hypergraph 
H(G{n,p),G), 
where B = B(c,G) is a big enough constant. Straightforward 
estimates show that their expectations all converge to 0 as n -+ co. Indeed, 
E ( * ) < 
Σ 
n t ( " ° - 1 ) + V * c < n 
5 1 
(c e°)'=o(l), 
t>B logn 
t>B\ogn 
E(V) < Σ 
5 3 η»(·β-«-(·Η-ΐ)ρί«β-«Η _ 0(!) 
t>2 HCG 
and 
(
Blogn 
\ 
Σ 
Σ 
n( 1 + 1> ( , , c- 1 )- ( ," i- l )(logn)'"'p ( t + 1 ) e c- e'' ) = o(l), 
t=3 HCG 
) 
where the inner sums extend over all proper subgraphs H of G with at least 
two vertices and correspond, in case of Y, to all possible shapes of the in-
tersection G(Ao) n G(At) and, in the case of Z, to all possible shapes of the 
intersection G(A) Π G(C). The index t stands for the number of hyperedges 
in a path or cycle. The logarithmic factor in the last estimate represents the 
number of choices of the vertices at which the handle A is attached to the 
cycle. Finally, we made use of formula (7.18). 
Thus, by Markov's inequality, P(X = y = Z = 0 ) - > l a s n - » o o , which 
completes the proof of Theorem 7.19. 
■ 

8__ 
Extremal and Ramsey 
Properties 
As the reader has undoubtedly noticed, statements of many results in this 
book could well begin with the phrase: "Let G be a fixed graph and let 
G(n,p) be the random graph such that p(n) = ...". Let us recall that in 
Section 3.1 we studied the probability that G(n,p) contains a copy of G for 
different values of p = p{n), while in Section 3.3 and Chapter 6 the asymptotic 
behavior of the random variable XG which counts copies of G in G(n,p) was 
thoroughly analyzed. In Section 3.4 we dealt with the property that every ver-
tex of G(n,p) belongs to a copy of G and in Section 4.2 a connection between 
this property and the property that G(n,p) has a G-factor was considered. 
Finally, in Section 7.6, by estimating the order of the largest G-free subset of 
V(G(n,p)), we proved that 
n-i/m<»(0) 
i s 
the threshold for the property that 
every coloring of the vertices of G(n,p) with a fixed number of colors leads to 
a monochromatic copy of G. 
In this chapter we investigate further variations of this familiar theme which 
are the edge versions of the questions from Section 7.6. Specifically, we will 
study the size of the largest subgraph of G(n, p) that contains no copy of G and 
look for the threshold function p = p(n) which guarantees that each coloring 
of the edges of G(n, p) with a fixed number of colors creates a monochromatic 
copy of G. 
It turns out that dealing with edges rather than vertices makes the above 
two problems much different from their vertex counterparts. This is apparent 
already when G — K%. From the proof of Theorem 4.9 it follows that if 
n2p3 -t oo then a.a.s. every induced subgraph of G(n,p) of order en contains 
a triangle, while, on the other hand, it is very well known that every graph 
201 
Random Graphs 
by Svante Janson, Tomasz Luczak and Andrzej Rucinski 
Copyright © 2000 John Wiley & Sons, Inc. 

202 
EXTREMAL AND RAMSEY PROPERTIES 
contains a triangle-free subgraph with at least half of the edges. Thus, before 
we proceed any further, we will examine some heuristics leading to the main 
results of this chapter. 
8.1 
HEURISTICS AND RESULTS 
An edge partition problem 
Following the Erdös-Rado arrow notation, for two graphs F and G and a 
natural number r, r > 2, we write F -* (G), if every coloring of the edges 
of F with r colors creates a monochromatic copy of G. Since in the whole 
chapter we always color the edges, not vertices, we will omit the superscript 2, 
writing just F -» {G)r. Our first goal is to determine for what p = p(n) a 
random graph has the Ramsey property G(n,p) -¥ (G)r. 
Let us notice that for given G and r the property F -» (G)r is increasing, 
so it has a threshold function (Theorem 1.24). In the vertex case the threshold 
was determined by the requirement that the number of copies of G should be 
of the order of n, the number of vertices in G(n,p). When coloring the edges 
we are facing a similar situation. If the number of copies of G is much smaller 
than the number of edges of G(n,p), most copies of G are edge disjoint, and 
the edges can be colored so that no copy of G is monochromatic. Although this 
fact is hardly surprising, its proof is rather tedious. It is similar to but more 
involved than the proof of the O-statement of Theorem 7.19 in Section 7.6. 
Copies of G may locally cluster together and a substantial amount of work is 
needed to show that a proper coloring does exist (Rödl and Rucinski 1993). 
On the other hand, if the number of copies of G is much larger than the number 
of edges, one may expect that they are so uniformly distributed around the 
graph that each coloring leads to many monochromatic copies of G. The 
quantities XQ and Χκ2
 
a r e °f t n e same order of magnitude when, setting as 
usual vG = \V{G)\ and eG = \E(G)\, 
nvapeG 
= Θ(η?ρ). 
But the property F -* (G)r is hereditary with respect to taking subgraphs 
of G and, just as in studying the containment problem in Chapter 3 or the 
vertex coloring problem in Section 7.6, one has to consider all (non-empty) 
subgraphs of G, which leads to the condition 
min 
{ η · < ο ν ( σ , > } = ®(n2p), 
G'CG,e(G')>0 
or, using the notation of Chapter 3, Φσ = 
θ(η2ρ). 
Hence, the threshold function for the property G(n,p) -> (G)r should be 
p = p(n) = n-i/'»<a)(0)> where m<2>(G) was defined in (3.18). 

HEURISTICS AND RESULTS 
203 
Fig. 8.1 A sunshine graph. 
The following result proved by Rödl and Rucinski (1995) states that this 
is indeed the case, except for star forests (i.e., forests where every component 
is a star). Recall that P3 is the path with 3 edges. 
Theorem 8.1. Let r > 2, and let G be a graph with at least one edge. 
(i) If G is a star forest, then the threshold function for G(n,p) -+ (G)r is 
η-1-1/((Δ(0)-1)τ·+1) 
(ii) // r = 2 and G is a forest consisting of stars and P3 's, with at least one 
P3, then there exists a constant C such that 
Hm P(G(»,p) - (Gh) = (°' 
*P*X1!^ 
n-»°° 
yi, 
tfp>Cn 
1/m 
(°) = 
1/n, 
= C/n. 
mm(G) 
_ 
(iii) In every other case, there exist constants c = c(G,r) and C = C(G,r) 
such that 
Í0 
if D < cn-1/m<2,<G) 
Proof of (i) and (ii). Case (i), a star forest, is easy. If, for example, G is 
a single star Kl<k, then F -> (G)T for every F with A(F) > (A; - l)r + 1, 
while F τ^ (C?)r if F is a forest with smaller A(F) (Exercise!). It then follows 
that the threshold for G(n,p) -> (G)r coincides with the threshold for the 
existence of a ·ΚΊ,(*_ι)Γ+ι given by Theorem 3.4. Note that by Theorem 3.19, 
this is a coarse threshold which cannot be tightened as in Cases (ii) or (iii). 
For (ii), the other special case, assume for simplicity that G = P3. As was 
first observed by Friedgut (personal communication), the 0-statement in (ii) 
cannot be improved as in (iii), since F -> ^3)2 if F is the sunshine graph 
in Figure 8.1 (or any other graph obtained by adding pendant edges at every 
vertex of an odd cycle of length at least 5). This observation also yields the 
1-statement in (ii), with C « 1.35, the solution of C(l - e~c) - 1 (Advanced 

204 
EXTREMAL AND RAMSEY PROPERTIES 
Exercise!). The O-statement follows because G(n,p) is, for p <g. 1/n, a.a.s. a 
forest (see Section 5.1), and if F is a forest then F -ft ^3)2 (Exercise!). 
■ 
An extended outline of the proof of the 1-statement in (iii), which utilizes 
the Szemerédi Regularity Lemma, will be presented in Section 8.4. The special 
case G = K3 was first proved for an arbitrary number of colors in Rödl and 
Rucinski (1994). For the proof of the O-statement, see Luczak, Ruciriski and 
Voigt (1992) and Rödl and Rucinski (1993). 
Remark 8.2. Note the different types of dependency on r for the different 
cases. If G is a star forest, then the threshold function is a power of n where 
the power depends on r. For other G, the rate does not depend on r, which 
affects the result only through the constants c(G,r) and C{G,r); if G is as in 
(ii), we have a coarser type of threshold for r = 2 than for r > 3. 
Note also that in Case (iii) we necessarily have C(G,r) -> 00 as r -> 00. 
In fact, with 3r colors arranged in r groups of three colors each, let p = 
c{G, 3)rn~l/m 
^G' and assign randomly one of the color groups to each edge 
of G(n,p). This exhibits G(n,p) as the union of r disjoint copies of G(n,p/r), 
each of which a.a.s. can be properly colored with the three corresponding 
colors; hence a.a.s. G(n,p) A (G)3r, and thus C(G,3r) > c(G,3)r. (We do 
not know the true order of growth of C{G, r); the upper bound given by the 
proof is enormous.) 
Remark 8.3. Just as for the vertex case in Theorem 7.19, it is tempting 
to conjecture that in Case (iii), for given G and r there is a single constant 
C = C(G, r) such that we can take c = C -ε and C = C + ε for every ε > 0. 
A proof of this statement seems difficult even for the simplest choices of G and 
r. A partial result has recently been obtained by Friedgut and Krivelevich 
(2000), who proved that if r = 2 and G is a tree other than a star or P3, then 
the threshold is sharp, in the sense defined in Section 1.6, which means that 
there exists such a quantity C = C(n), which, however, possibly depends on 
n. 
A Turán-type problem 
Another problem which continues to stimulate research in the theory of ran-
dom graphs has the flavor of extremal graph theory, rather than Ramsey the-
ory. Instead of partitioning the edges of G(n,p) into several classes, one tries 
here to determine the minimum size of a subgraph of G(n,p) which guarantees 
the containment of a copy of G. This problem, unlike the partition problem, 
in general remains open. 
Let us introduce some notation. For two graphs F and G, we denote by 
ex(F, G) the number of edges in the largest subgraph of F containing no copy 
of G and set §x(F, G) for the fraction of the number of edges of F in such a 
subgraph, that is, 
ex(F,G) = max{e// : G £ H C F} 

HEURISTICS AND RESULTS 
205 
and 
éx{F,G) = 
ex(F,G)/eF. 
When eF = 0, we define ex(F, G) = 1 for every G. 
The function ex(K"„,G) has been studied extensively in extremal graph 
theory (see Bollobás's (1978) monograph). A celebrated result of Erdös, Stone 
and Simonovits (Erdös and Stone 1946, Erdös and Simonovits 1966) states 
that for each G with at least one edge 
ex(i^G) = (l - - ^ - ^ + o(l)) g ) . 
(8-1) 
or, equivalently, \\mn-*oaex{Kn>G) = íjcl-t· Thus, the asymptotic behavior 
of ex(Kn,G) 
depends exclusively on the chromatic number x{G) of the graph 
G. 
If F Φ Kn, the function ex(F,G) is much harder to study. Obviously, one 
always has ex(F,G) < 1, and equality holds if and only if G g F (unless F 
and G are both empty). Is it true that for a given graph G one can make 
ex(F, G) arbitrarily small by a suitable choice of Fl As was already noticed, 
this is not the case when G = K¡, since for every graph F we have ex(F, A3) > 
1/2. In general, an easy probabilistic argument (Exercise!) yields ex(F, G) > 
limn_>00e3c(Ä'n,G), provided x{G) > 3. This asymptotic inequality turns out 
to be exact and holds for bipartite graphs too. 
Proposition 8.4. For every graph F on n vertices and for every graph G, 
ex(F,G) 
>ex(Kn,G). 
Proof. Let F be any graph with the vertex set [n] and let H denote a graph 
on [n] such that G £ H and e# = ex(Kn,G). 
For a given permutation 
σ : [n] -¥ [n] let Η(σ) be the graph obtained from H by relabelling the 
vertices according to σ and let F{a) = F Π Η(σ). 
Finally, let <7rand be a 
random permutation of [n]. The expected number of edges in F(a r a n d) is 
e F e „ / Q 
=eFex(Kn,G)/(?\=eFéSl(Kn,G), 
so there exists a permutation σ0 such that e(F(a0)) 
> eFex(Kn,G). 
Fur-
thermore F(a0), 
as a subgraph of Η(σ0), 
contains no copy of G. 
Thus 
ex(F,G) > ex(Kn,G) 
and the assertion follows. 
■ 
Corollary 8.5. For every graph G and all m < n, we have ex(Km,G) > 
ex(A-n,G). 
Proof. Apply Proposition 8.4 with F being the union of a complete graph Km 
and n — m isolated vertices. 
■ 
Proposition 8.4 suggests that the right "Turán-type" question to ask about 
random graphs is the following: For which functions p = p(n) do we have 
ex(G(n,p),G) = (1 + op(l))ex(Kn,G)? 
(8.2) 

206 
EXTREMAL AND RAMSEY PROPERTIES 
It will follow from the next result that if (8.2) holds for some pi, then it does 
so for each p> > p\. In this result we technically assume that &{n,p\) 
and 
G(n,p2) are related in the natural way, which is to say that they are two stages 
of the same random graph process {G(n, t)}t (just as, e.g., when applying the 
two-round-exposure technique). 
Proposition 8.6. Let G be a graph with A(G) > 2, and let px = p\(n) and 
Pi = pz{n) be such that P\ <p2. 
Then 
ex(G(n,pi),G) > (1 + op(l))éx(G(n,p2),G). 
Proof. We will actually prove the corresponding statement for the uniform 
model G(n,M)·. 
//A(G) > 2 and Mx = Mi(n) < M2{n) = M2, then 
ex(G(n,Mi),G) > (l + op(l))ex(G(n,M2),G). 
(8.3) 
The proposition follows by conditioning on the number of edges in both 
G(n,pi) and G(n,p2). 
Again, we regard G(n, Mi) and G(n, M2) as two stages of the same random 
graph process {G(n, M)}M . In particular, we may view G(n, Mi) as a graph 
obtained from G(n, M2) by a random deletion of M2 — M\ edges. In order to 
show (8.3), we consider three cases which, together with the subsubsequence 
principle, yield the general result. 
(i) Mi ex(G(n,M2),G) -> oo. 
Let H be a maximal G-free subgraph of G(n,M 2), and let H' = 
HnG{n,Mi). 
Then, clearly, ex(G(n,Mi),G) > e(H') and we need to estimate e(H'). 
Note 
that e(H') has a hypergeometric distribution with mean 
^ e ( t f ) = M,ex(G(n,M 2),G), 
M2 
which, by assumption, tends to infinity. 
Thus, a standard application of 
Chebyshev's inequality yields 
e(H') = (l + o p(l))Miéx(G(n,M 2),G), 
as required. 
(ii) Mi ex(G(n, M2),G) 
<C for some C>0, 
but Μχ -* oo. 
As Mi -► oo, o.o.s. one can find more than C disjoint edges in G(n, Mi), and 
thus ex(G(n, Mi),G) > C > Mx ex(G(n, 
M2),G). 
(iii) Mi is bounded. 
In this case a.a.s. ex(G(n, Mi), G) = 1 (Exercise!) and (8.3) holds trivially. 
■ 
Remark 8.7. If A(G) = 1, in which case G is a disjoint union of edges and, 
possibly, isolated vertices, the result remains true provided pi > n_1~o(1^ 
(Exercise!). However, there are counterexamples with smaller pi and p2- For 

HEURISTICS AND RESULTS 
207 
instance, the assertion is false if G = 2A'2) P\ = an 
3 / 2 with 1 < a < 2 and 
P2 = 2n - 3/ 2 (Exercise!). 
Remark 8.8. By almost the same argument it follows that if F ( 1 ), F< 2 ),... is 
a sequence of graphs with v(F{n)) 
-> oo, A(G) > 2, pi = pi(n) < P2(n) = P2, 
and, furthermore, ex(/*¿, ,G) A oo, then 
ñ(F£\G)>(l 
+ 
op(l))ex(F£\G), 
where Fpn) is the reliability network created by randomly destroying edges of 
F^n\ independently, with probability 1 - p (Exercise!). 
Proposition 8.6 suggests that, for a fixed a satisfying lim„_>oo ex(Kn,G) 
< 
a < 1, the property that 
ex(G(n,p),G) < a, 
(8.4) 
although not monotone, behaves very much like a monotone one, and we may 
hope to find a threshold function for it. 
A necessary condition for (8.4) is not hard to determine. Note that for any 
two graphs F and G we have 
ex(F,G)>eF-#{GCF}, 
where #{G C F} is the number of copies of G contained in F, and that 
Property (8.4) is hereditary with respect to taking subgraphs of G. Thus, 
if for some p = p(n) a.a.s. §x(G(n,p),G) < a, where a < 1, then, just as 
for the partition problem, the number of copies of each subgraph G' of G in 
G(n,p) should be comparable with the number of edges of G(n,p). 
Hence, 
p = n~llm 
(G) is, again, our guess for the threshold. The reader is invited 
to compute the expectation and variance of appropriate random variables, 
making the above argument rigorous, and thus proving the following fact 
(Exercise!). 
Proposition 8.9. For every graph G with A(G) > 2 and for every 0 < a < I, 
there exists a constant c = c(G, a) such that a.a.s. 
ex(G(n,p),G) > a, 
provided p = p(n) < cn~ltm 
(G\ 
■ 
Remark 8.10. In the rather uninteresting case in which A(G) = 1, and thus 
m.W(G) = 1/2, one should instead assume that pn2 -¥ 0 or, otherwise, slightly 
modify the assertion (Exercise!). 
It is natural to conjecture that if the number of copies of G (or, more pre-
cisely, the number of copies of the subgraph of G most infrequent in G(n,p)) 
is much larger than the number of edges of G(n,p), the value of ex(G(n,p), G) 

208 
EXTREMAL AND RAMSEY PROPERTIES 
approaches ex( A'n, G) (because of Proposition 8.4 it cannot drop any further). 
More formally, one may expect that the following is true. 
Conjecture 8.11. For every graph G with A(G) > 2 and for every η > 0, 
there exists C = C(G,T/) such that a.a.s. 
ex(G(n,P),G) < (1 + 7,)ex(/<:n,G) 
whenever p = p(n) > Cn _ 1/ m l 3 ) ( G>. 
Unfortunately, at this moment we are able to verify this conjecture only 
for some special cases of G. In Section 8.2 we show its truth for G = K3 and 
present an elementary proof of Frankl and Rödl (1986). An alternative proof 
for triangles is given in Section 8.5. It relies on a modified (sparse) version of 
the Szemerédi Regularity Lemma and on the observation that Conjecture 8.11 
follows from a stronger one, Conjecture 8.35, stating a better-than-exponential 
upper bound on the probability of nonexistence of copies of G in a special 
model of a random graph. Conjecture 8.35 (or its weaker version, sufficient 
for the proof of Conjecture 8.11) is settled in the affirmative for G being 
an arbitrary cycle (Haxell, Kohayakawa and Luczak 1995, Kreuter 1997, Ko-
hayakawa, Kreuter and Steger 1998), or the complete graph K4 (Kohayakawa, 
Luczak and Rödl 1997). Thus, at least in these cases, Conjecture 8.11 holds. 
Yet another approach, which verifies Conjecture 8.11 for odd cycles, is pre-
sented by Haxell, Kohayakawa and Luczak (1996). 
Let us make a few comments on the connections between Conjecture 8.11 
and Theorem 8.1. Note first that if G is bipartite, then, according to the 
Erdös-Stone-Simonovits result (8.1), we have ex(K„,G) = o(l) and so Con-
jecture 8.11, if true, would imply the 1-statement of Theorem 8.1. However, if 
X(G) > 3, neither statement can be deduced from the other one, although, in 
a way, they both reflect the fact that in dense random graphs copies of G are 
distributed nearly as uniformly as they are in the complete graphs. The only 
case in which the partition and extremal properties get close to each other 
is that of G = K3 and r = 2. This fact is utilized in the proof presented in 
Section 8.2. 
Note also that Conjecture 8.11 and Proposition 8.9 would imply that the 
threshold function for the property ex(F, G) < a is not very much affected by 
the choice of a as long as lim„_+00ex(Kn,G) < a < 1. 
We close this introductory section with a brief account of further develop-
ments in the theory of partition and extremal properties of random structures. 
Some partial results for nonsymmetric Ramsey properties of random graphs 
were obtained by Kohayakawa and Kreuter (1997). A first step into the un-
explored area of partition properties of random hypergraphs was taken in 
Rödl and Rucinski (1998). A few results on partition and extremal prop-
erties of random subsets of integers have appeared in Rödl and Rucinski 
(1995, 1997), Graham, Rödl and Rucinski (1996), and Kohayakawa, Luczak 
and Rödl (1996). 

TRIANGLES: THE FIRST APPROACH 
209 
8.2 
TRIANGLES: THE FIRST APPROACH 
Counting monochromatic triangles 
The goal of this section is to present an elementary proof of Conjecture 8.11 
when G - K3. 
It will follow from a strengthening of the 1-statement of 
Theorem 8.1 for G = K3 and r = 2. The strengthening claims not one 
but many monochromatic triangles in every two-coloring of G(n,p). There 
are so many that there is not enough room for them in any color class with 
fewer than (¿ - η)(^)ρ edges. Only slightly weaker versions of this particular 
result were proved by Frankl and Rödl (1986) and Luczak, Rucinski and Voigt 
(1992). The proof is based on a beautiful and simple idea of Goodman (1959), 
who applied it to bound the total number of triangles in a graph and its 
complement. 
To fully appreciate this idea we begin by considering the deterministic 
case of two-colorings of the edges of the complete graph K„, or equivalently, 
the random graph G(n,p) with p = 1. Thus, let us first ask, how many 
monochromatic triangles are guaranteed if one colors the edges of Kn with 
two colors, blue and red. 
A trivial upper bound of ~{^) is provided by the probabilistic method 
(Exercise!). Surprisingly, this is asymptotically the right answer. For a given 
coloring and each v = 1,2,..., n, let b„ [r„] be the number of blue [red] edges 
incident with the vertex v. Then the total number of two-colored triangles is 
\Σ6»r« = \Σ6>- 
* - M < *(»- i)2/». 
u = l 
υ=1 
This implies Mantel's theorem (i.e., Turán's theorem for triangles - see, e.g., 
Bollobás (1998)). Indeed, let H be a graph with n vertices and e# > n 2/4. 
Let us suppose that there is no triangle in H and count the ordered pairs 
(e, t), where e is an edge of H and t is a triple of vertices containing e. On the 
one hand, the number of such pairs is precisely e«(n — 2). On the other hand, 
denoting by U, i = 1,2, the number of triples of vertices containing i edges of 
H, there are t\ + Iti < 2{t\ +12) such pairs. By the above upper bound on 
the number of two-colored triangles, treating the edges of H as those colored 
blue, we have 
(n2/4 + 3/4)(n - 2) < eH{n - 2) < 2{U + t7) < 2n(n - l) 2/8, 
which is a contradiction for n > 4. Hence H contains a triangle. 
After this deterministic rehearsal we should be ready to repeat the same 
argument for the random graph G(n,p). 
Theorem 8.12. For every ε > 0 there exists a constant C — C(e) such that 
if np2 > C, then a.a.s. each two-coloring of the edges o/G(n,p) results in at 
least (1/4 - ε)(")ρ3 monochromatic 
triangles. 

210 
EXTREMAL AND RAMSEY PROPERTIES 
The probabilistic part of the proof is contained in the following technical 
lemma, the proof of which is left to the reader. (Exercise! - The only tools 
needed are Chebyshev's and Chernoff's inequalities.) Let T denote the number 
of triangles in G(n,p), dv = deg(u) stand for the degree of vertex v, and Nv 
(also known as N(v)) be the neighborhood of v. Finally, for a vertex set A, 
let e(A) be the number of edges induced in G(n,p) by A. 
Lemma 8.13. Suppose that np2 = Ω(1). Then 
(i) | T - 0 * | = op(nV),· 
(ii) maxi<„<„ |d„ - (n - l)p| = op(np); 
(iii) maxi<„<„ \e(Nv) - (d¿)p\ = op{n2p3). 
(iv) Moreover, for every ε > 0 there exists C" = C'(e) such that ifnp2 > C, 
then a.a.s. for every v = 1,2,...,n and for each A C Nv, e(A) > 
( ^ ) p - e n V · 
■ 
Proof of Theorem 8.12. For a given blue-red coloring of the edges of G(n,p), 
let Bv [Rv] be the set of vertices adjacent to v by edges colored blue [red], 
and let zv be the number of edges joining those pairs of neighbors of v which 
are adjacent to υ by edges of different colors, that is, 
z„ = e{Bv,Rv) = e(JV„) - e(Bv) - e{Rv). 
Then, similarly to the deterministic case, there are precisely | 5Z„ zv two-
colored triangles. Recalling our previous notation bv = \B„\ and r„ = \Rv\, 
and using Lemma 8.13(iii, iv) with e/7, we find that the number of two-colored 
triangles is bounded from above by 
ϊΣ[(ϊ>-β>-('ί>+?»ν+.**Λ; 
= 2 Σ 6"r" + 7 ε" 3 ρ 3 + °p(n3P3)' 
provided np2 > C'(e/7), where C'(e/7) comes from Lemma 8.13(iv). 
Since by Lemma 8.13(ii), bv + rv = d„ = np-\-op(np) for each v, uniformly, 
we find Y,vbvrv < \n3p* + Opip^p2). This, together with Lemma 8.13(i) 
yields Theorem 8.12 with C(e) = C"(e/7). 
■ 
A Turán-type theorem for random graphs 
As a consequence of Theorem 8.12, we will now derive a special case of Con-
jecture 8.11. It can be viewed as Mantel's theorem for random graphs. 

TRIANGLES: THE FIRST APPROACH 211 
Theorem 8.14. For every η > 0 there exists C = 0(η) > 0 such that if 
p > Cn~l/2, 
then a.a.s. every subgraph o/G(n,p) with at least (1/2 + η){")ρ 
edges contains a triangle. 
Proof. In order to repeat almost literally the argument used in the determin-
istic case, we need to show first that most edges belong to nearly the expected 
number of triangles. Let X^ be the number of vertices joined to both ver-
tices i and j . Then X^ has the binomial distribution Bi(n - 2,p2) and the 
expectation around np2. 
Let Z count the edges {i,j} of G(n,p) for which 
Xij < (1 - ε)ηρ2, where e > 0 satisfies 
3(1/2 + η)(1-ε)> 
3/2 + 2ε. 
(8.5) 
We have 
E(Z) = φ ρ Ρ ί * » < (1 -e)np 2) < 
(£)pe-e2np2/3 
via the Chernoff inequality (2.6). If np2 -> oo, then K(Z) = o(n2p) and by 
Markov's inequality (1.3), Z = op(n2p). Otherwise, it is quite straightforward 
to show that E(Z(Z - 1)) ~ (E(Z))2 and thus, by Chebyshev's inequality 
a.a.s. Z < 2(2
,)pe_(eC) / 3. Thus, in either case, a.a.s. 
Z<2("\pe-W*>\ 
(8.6) 
Let us take C = 0(ε) as in Theorem 8.12, and so large that the addition 
of the term -2e~ ( e C ) / 3 to the leftmost brackets of (8.5) is negligible, in the 
sense that the inequality 
3(1/2 + η - 2e- ( e C ) 2 / 3)(l - e) > 3/2 + 2ε 
(8.7) 
holds. Note that C depends on η through ε. 
Suppose there exists a subgraph H of G(n,p) with e« > (l/2 + 7j)(2)p and 
containing no triangle. We will show that this event implies either a violation 
of (8.6) or a violation of the conclusion of Theorem 8.12, both events having 
probability converging to 0. Indeed, assuming they are not violated, let us 
count in two ways the ordered pairs (e,t) where e is an edge of H and r 
is a triangle of G(n,p) containing e. On the one hand, there are at least 
(1/2 + η - 2e~(eC)2/3)(2)p(l - ε)ηρ2 such pairs. On the other hand, viewing 
the edges of H as the blue edges and the remaining edges of G(n,p) as the red 
ones, we conclude by Theorem 8.12 that there are no more than 2 ( | + e)(!J)p3 
such pairs. This yields a contradiction to (8.7). 
■ 
Remark 8.15. Let us note that the dependence of C on the parameter η in 
Theorem 8.14 is genuine. In other words, it is not true that there exists an 
absolute constant C such that for every η > 0 a.a.s. every subgraph H of 

212 
EXTREMAL AND RAMSEY PROPERTIES 
G{n,Cn'l/2) 
with at least {1/2 + η)(^)ρ edges contains a triangle. It follows 
that, for G = tf3, the equation (8.2) holds if and only if p > n"1/2. 
To find a counterexample, set Vi = {1,2,...,n/2}, n even, and V2 = 
{n/2 + 1,... ,n), and fix C > 0. It can be proved routinely by the second 
moment method that, for sufficiently small η = T/(C), a.a.s. there are at least 
3η(^)ρ edges with both endpoints in Vi and with no common neighbor in V2. 
Moreover, it is well known that at least one half of these edges form a bipartite 
graph. The union of this bipartite graph and the bipartite graph spanned in 
G(n,p) between Vi and Vi contains no triangle and has at least (l/2 + tj)(2)p 
edges (Exercise!). 
8.3 
THE SZEMEREDI REGULARITY LEMMA 
The Szemerédi Regularity Lemma states that, roughly speaking, for every 
large graph there exists a partition of its vertex set into a small number of 
almost equal subsets, such that in most of the bipartite graphs induced by 
pairs of these subsets, the edges are, in a way, "uniformly" distributed. The 
Lemma was introduced as an important step in the proof of Szemerédi's cel-
ebrated density theorem (Szemerédi 1975) and soon after the graph theorists 
realized that the presence of such "uniform", or, as we will call them later, 
"regular" partitions could greatly simplify many existing proofs and lead to 
solutions of many open problems in graph theory. Nowadays the Szemerédi 
Regularity Lemma is one of the most powerful tools of modern graph theory 
(see, e.g., Bollobás (1998), Diestel (1996), Komlós and Simonovits (1996)), as 
well as of the theory of random structures. That is why we devote a whole 
section to various versions of this important (but purely deterministic) result. 
Regular pairs and partitions 
In order to state the Szemerédi Regularity Lemma in a mathematically rig-
orous way we need a few definitions. Throughout, H is a graph with vertex 
set V(H) and edge set E(H) and 0 < s < 1 is a real number which will 
be called the scaling factor. The role played by s will soon become clear; 
here we only mention that the two most prominent cases are s = 1 and 
s = p(H) = eH/{vZ)- 
For two disjoint subsets U,W C V{H) the (s;H)-
density d,tH(U, W) is defined as 
d.MU,w)- 
s m w r 
where e« (Í/, W) counts the edges of H joining U and W. For 0 < ε < 1, we 
say that two disjoint subsets U, W C V(H) form an (s;H,e)-regular pair if 
for every pair of their subsets U' C U, W C W, such that \U'\ > e\U\ and 

THE SZEMERÉDI REGULARITY LEMMA 
213 
\W'\ > e\W\, we have 
\ds,H(U',W')-daM(U,W)\<e, 
that is, the (s; H)-density of any pair of large subsets of the pair (U, W) does 
not deviate much from the (s; i/)-density of (U, W). 
Furthermore, let Π = (V0, Vx,..., V*) be a partition of V(H). We say that 
this partition is (s;H,e,k)-regular 
if |Vi| = |V2| = · · · = |Vt|, |Vo| < ενμ, and 
for all except at most ε(*) choices of the indices 1 < i < j < k, the pairs 
(Vi,Vj) are (s;//,e)-regular. Note the special status of V0, which, for that 
reason, will be called the exceptional class of the (s; Η,ε, fc)-regular partition 
(Vo, Vi,..., V¡t). 
We say that a partition (W0,Wi,..., 
Wk·) is a subpartition 
of a partition (Vo, Vi,..., V*) if for every 1 < i < k' there exists 1 < j , < k 
such that Wi C Vu. 
Clearly, every graph H on n vertices admits an (s;H,e, l)-regular parti-
tion and an (s;H,e,n)-regular partition. In applications, however, one rather 
needs an (s;H,e, fc)-regular partition for some fc which is bounded from be-
low and from above by some constants m and M. The Szemerédi Regularity 
Lemma provides the existence of such a partition, with M depending on e 
and m only. 
The classic case 
Let us start with the important case s = 1. It was considered by Szemerédi 
(1978) and later in most applications of his result. To simplify the notation, 
we drop the index s, when s = 1, using terms like pair density dn{U, W), 
(i/,e,fc)-regular partitions, and so forth. 
Then, the Szemerédi 
Regularity 
Lemma can be stated as follows. 
Lemma 8.16. For every ε > 0 and a natural number m there exists M = 
Μ(ε,τη) 
such that every graph H on at least m vertices admits an 
(H,e,k)-
regular partition for some k, where m < k < M. 
The idea of the proof. It is remarkable that the proof of so deep and insightful 
a result is based on a very simple idea. Let Π* = (Vi,..., Vk) be any partition 
of the set of vertices of a graph H, where |Vi| = · · · = |Vfc|. Associate with 
Ilk a real number indll*, called the index of Π*, setting 
indn* = ¿ ¿ Σ (<*"W,v,-))
a. 
Note that since the density of a pair of sets is not greater than one, the index 
of any partition is bounded by a half. 
Now, let U'k, = (\νλ,...,Wk>) 
be a subpartition of Π* into k' > k equal 
parts. Then, clearly, for every 1 < i < j < k, we have 
d»(Vi)Vj) = - ^ £ 
53 dH(Wt,Wu). 

214 
EXTREMAL AND RAMSEY PROPERTIES 
Thus, the Cauchy-Schwarz inequality gives 
{dHiVuVj))2^^- 
Σ 
Σ 
W„(WUWU))* 
(8·8) 
W.CVt W»CV, 
and, consequently, 
ind Wk, - ind Π* > 0, 
(8.9) 
where equality holds in (8.8) (and thus in (8.9)) if and only if all terms on the 
right-hand side of (8.8) are equal. 
Suppose that the partition Π* is not (i/,e,A;)-regular. Then a substantial 
fraction of the pairs (Vj, V)) are not (//,£)-regular, and one can pick a sub-
partition Wk, so that for each such pair the densities on the right-hand side 
of (8.8) differ significantly from each other. The key observation in the proof 
of Lemma 8.16 is that these differences force the index to increase. More pre-
cisely, the following holds. If k is large enough and Π* is not (H, e, A)-regular, 
then there exists a subpartition II't, of Π* such that k' is bounded from above 
by a function of k and the difference ind U'k, - ind Π* is bounded from below 
by some positive constant which depends only on ε but not on k. 
From this statement the lemma follows almost immediately. We begin with 
any partition Π*0 of V(H) into ko > m equal parts. If Π*0 is not (H,e,fco)-
regular, a new partition Π*, is constructed in such a way that its index is 
substantially larger than indnfco. We continue this way until an (H,e,k)-
regular partition is found for some k. Since for every r > 1 the difference 
ind fin, — ind Π*,.., is bounded from below by a constant which depends only 
on £ and, on the other hand, the index of every partition is bounded from 
above by a half, the procedure is guaranteed to end after at most a number 
of steps, which depends on ε and m only. 
Although it is quite easy to believe that this argument works, its detailed 
proof is rather tedious and must take care of several technicalities. For ex-
ample, one needs to use a special form of the Cauchy-Schwarz inequality to 
control the growth of ind Π* - ind Π^,. Furthermore, in the description above 
we tacitly assumed that k always divides both k' and n, whereas in the origi-
nal proof of Lemma 8.16 this problem is solved by introducing the exceptional 
class which, for the sake of simplicity of presentation, does not appear in our 
outline. We omit the details, referring the reader to Szemerédi (1978), Diestel 
(1996), or Bollobás (1998). 
■ 
The argument above can be easily modified to obtain stronger versions of 
Lemma 8.16. In particular, \i H\,...,HT 
are graphs on the same vertex set 
V, then one can mimic the proof with indíl* replaced by 
indnr* = ¿ : ¿ ¿ Σ>*,(^))2, 
1=1 i=l 
j=i+\ 

THE SZEMERÉDI REGULARITY LEMMA 
215 
and find a partition which is (//<,£, fc)-regular simultaneously for each i -
1,..., r. Thus, we arrive at the following strengthening of Lemma 8.16. 
Lemma 8.17. For all ε > 0 and natural numbers m and r there exists 
M = M(e,m,r) 
such that the following holds: For all graphs 
H\,...,HT 
on the same set V of at least m vertices, there exists a partition of V which 
is (Ht,e,k)-regular for some k, m < k < M, and every £ = 1,2,... , r. 
■ 
Sparse regularity lemma 
The Szemerédi Regularity Lemma has proved to be extremely useful in many 
combinatorial investigations. We must point out, however, two drawbacks of 
its applications. First, Szemerédi's argument gives a very poor upper bound 
on the value of Μ(ε,τη,ί), 
which grows so quickly with e -» 0 that it is al-
most useless for any quantitative estimates. The other problem is that the 
Szemerédi Regularity Lemma, as stated above, is meaningful only when one 
deals with graphs of large density. For a graph H with n vertices and, say, 
maximum degree at most ^/ñ, each partition of V(H) into k equal parts is 
(H, ε, fc)-regular, provided n is large enough. This is because the density of the 
bipartite subgraph induced by any two sets of size Ω(η) is 0(l/y/ñ) 
= o(l) 
and, therefore, does not measure effectively the distribution of the edges. 
Nonetheless, as noticed independently by Kohayakawa (1997) and Rödl (per-
sonal communication), a simple generalization of the Szemerédi Regularity 
Lemma will efficiently work for sparse graphs too. The key observation is 
that the proof of Lemma 8.16 still works for the scaled densities d,tH in-
stead of the ordinary densities d//, provided there exists a constant 6 such 
that dSiH(X,Y) 
< b for all pairs of large sets X,Y. 
The constant b is an 
upper bound for an appropriately scaled index function and guarantees that 
the procedure of taking subpartitions terminates after a bounded number of 
steps. 
More precisely, for b > 1 and 0 > 0, call a graph H (s;b,ß)-bounded 
if 
for every pair of disjoint subsets U,W C V{H) with \U\, \W\ > ßvH we have 
d,(U, W) < b. Then, a "scaled" version of the Szemerédi Regularity Lemma 
can be stated as follows. 
Lemma 8.18. For all ε > 0, b > 1 and natural numbers m and r there exist 
β = /3(ε,&,τπ,τ·) > 0 and M — M(e,b,m,r) 
such that the following holds: 
For every choice of scaling factors s i , . . . , s r , and (s¿\b,ß)-bounded graphs 
Hi, i = l,...,r, on the same set V of at least m vertices, there exists a 
partition of V which is (sf,Ht,e,k)-regular 
for some k, m < k < M, and 
every i=l,2,...,r. 
■ 
Besides the classic case of si = 1, the other instance of Lemma 8.18 which 
will be used in the forthcoming sections is the case in which si — p(H() = 
βμ, I (""') · To simplify the notation in the latter case, we say that a graph H 
is (b,ß)-bounded if it is (p(H);b,^-bounded, 
a pair is sparsely 
(H^)-regular 

216 
EXTREMAL AND RAMSEY PROPERTIES 
if it is [p(H); H, e)-regular, and a partition is sparsely (H,e,k)-regular 
if it is 
(p(H)\ H, ε, fc)-regular. Below we state this special version of Lemma 8.18. 
Lemma 8.19. For all ε > 0, b > 1 and natural numbers m and r there exist 
β = β(ε, b, m, r) > 0 and M — Μ(ε, b, m, r) such that the following holds: For 
every choice of (b, β)-bounded graphs Hi,...,Hr 
on the same set V of at least 
m vertices, there exists a partition ofV which is sparsely (Ηι,ε,ΐζ)-regular for 
some k, m < k < M, and every i = 1,2,.. . ,r. 
■ 
8.4 
A PARTITION THEOREM FOR RANDOM GRAPHS 
This section is entirely devoted to presenting an outline of the proof of the 
1-statement of Theorem 8.1. For more details, see Rödl and Rucinski (1995). 
Uniformly dense graphs 
For 0 < d < 1 and ξ > 0, we say that a graph F is (ξ,ά) -dense, if for 
every V C V(F) with |V| > £vjr the induced subgraph F[V] has density at 
least d. Note that it suffices to demand this for subsets V with |V| = \ξυρ 1; 
the property then holds for larger subsets by averaging over all their subsets 
with exactly \£VF] elements. Observe further that the complete graph Kn is 
(£,d)-dense for all choices of ξ > 0 and 0 < d < 1. It can be easily verified 
(Exercise!) that the following is true (Rödl and Rucinski 1995, Lemma 2). 
Proposition 8.20. For each 0 < d < 1 and each graph G, there exists ξ > 
0 such that every fá,d)-dense graph on n vertices contains θ(η"^σ^) copies 
ofG. 
■ 
Thus, in a sense, (ξ, d)-dense graphs imitate complete graphs. 
The heart of the proof of Theorem 8.1 is the following deterministic lemma 
which utilizes the Szemerédi Regularity Lemma in a "canonical" way. 
Lemma 8.21. For all 0 < ξ' < 1 and 0 < d < 1, and for every natural 
number r there exist ξ > 0, v > 0, and no, such that if F is a (£,d)-dense 
graph onn>n0 
vertices and E(F) = EiU- ■ UEr, then there exist io € [r] and 
V C V(F), \V\ > vn, for which the subgraph of F consisting of the vertices 
of V and of the edges of Eto Π F[V] is (ξ'',ά')-dense, where d! = d/20r. 
Remark 8.22. This lemma, together with Proposition 8.20 and inequality 
(2.6), implies Theorem 8.1 in the case in which p is a constant. 
Indeed, 
by Chernoff's inequality G(n,p) is then a.a.s. (£,p/2)-dense for any fixed ξ 
(Exercise!). Thus, by Lemma 8.21, for every r-coloring at least one of the 
color classes is (£',p/40r)-dense on a large subset V of [n], and contains, by 
Proposition 8.20, many (monochromatic) copies of G. We will see later in this 
section how Lemma 8.21 can be applied in the sparse range of p. 

A PARTITION THEOREM FOR RANDOM GRAPHS 
217 
Proof. Set 
•-[si· -\ψ\· 
<
810> 
m = R(q,t,...,t), 
(8.11) 
Γ 
where R(q, t,..., t) is the Ramsey number (see, e.g., Graham, Rothschild and 
Γ 
Spencer (1992) for definition). Furthermore, set 
\ 2 Ö i ' i ö b 7 ' 2 7 ^ j ' 
( 8 1 2 ) 
ε = mm 
, _ g ( l - e ) 
, 
t(l-e) 
„ _ 2 0 M 
ί β - ϋ Γ - ' 
" =-ΊΟ^' 
and 
ηο = —ε' 
(813) 
where M = JV/(e, 2m, r) is the constant provided by the Szemeredi Regularity 
Lemma 8.17. 
Let F be a (ξ, d)-dense graph on n > no vertices, and let E(F) — E\ U · · · U 
ET be a partition of the edge set of F. Denote by Hi = F[Et] the (spanning) 
subgraph of F consisting of the edges of Et, I = l,...,r. By Lemma 8.17 
there exists a partition V(F) = Co U C\ U · · · U C*, 2m < k < M, which is 
{Ηι,ε, fc)-regular for each I — 1,... ,r. 
As, by (8.12), at least (1 - re)(*) > (1 - £ ) £ pairs (CuCj) 
are 
(Η(,ε)-
regular for each ί = 1,2,..., r, it follows from Turan's theorem that there are 
m sets, C\,..., Cm say, such that all C£) pairs of them are (Ht, e)-regular for 
each i. 
Consider the (r + l)-coloring of [m]2, [m]2 = D0 U D\ U · · · U Dr, where 
{iJ)eD0 
if 
dF{Ci,Cj)<^ 
(8.14) 
and, for I e [r], 
{¿,j}eD< 
if 
d i / . i ^ . C , ) ^ ! ; . 
(Note that a pair may belong to more than one set D/.) By (8.11), there exists 
either a subset K C [m], |A"| = q, [K]2 C Po, or a subset L C [m], \L\ = t, 
[L]2 C D(, for some £ € [r]. The first option is impossible, since then, putting 
i = |Cj|, the set C = \Jj€K C, would have, by (8.14) and (8.10), density 
atmCW - e{F[C]) 
< ®ä2x2 
+ q® <d
 +
 
1<d 

218 
EXTREMAL AND RAMSEY PROPERTIES 
This contradicts the fact that F is (ξ, d)-dense, because, by (8.13), \C\ - qx > 
1 Af 
= ί η· Thus, for some £0 € [r], there are ί sets, C\,..., Ct say, which 
satisfy 
dHtoi.CuCi)>± 
for all 
{i,j} € [i]2. 
We will prove that the graph H = #*0[V], where V = C\ U ■ · U Ct, is 
(£'- ^ ) - d e n s e · ( N o t e t h a t- °y (813) again, |V| = tx > t&$Z 
= vn as 
required.) 
Consider V" C V of size 
in = R'ivn = rí'tei-
Then, by (8.10), 
IH< (τΗΗ 
Since by (8.13) we have x > ^-¡^ 
> 20 and also ξ' < 1, the right-hand side 
above can further be bounded from above by 3.1a:, leading to 
|V'|<3.1x. 
(8·15) 
Set i, = \V n d\, i = 1,... ,t. Then, owing to (8.15), we infer that 
t(:)*.©*Cfl<»G)· 
<-> 
Let V = V" U V" be a partition, where V" = \Jix.<exV 
n CV Clearly, 
|V"| < tex, and thus, by (8.10) and (8.12), 
\ν"'\>(ξ' 
-e)tx>2x. 
(8.17) 
Let us bound from below the number of edges in the graph H[V'"]. By the 
definition of V" and by the choice of 
C\,...,Ct, 
«(ΐΦΗ)>ΣΣ(έ-«)*<** 
where the double summation is taken over all pairs i,j, 1 < i < j < t, such 
that Xi > εχ and Xj > ex. But, by (8.16) and (8.17), 
where the single summations are taken over all i satisfying i¿ > ex. Hence 
β(Η[ν'"))>0.99(^-ε^Χ^. 
(8.18) 

A PARTITION THEOREM FOR RANDOM GRAPHS 
219 
Finally, by (8.15), (8.18) and (8.12), 
„ , m v n 
e ( g [ v r , I ) >
c ( ^ y , " ) ) > 0 - 9 9 ( g - g ) ( ' ? 1 ) 
P(H[V ]) - - j ^ ~ 
> - J i F T j - * 
(3^) 
. 0.99 / d 
\ 
1 / d 
\ 
d 
proving that the graph H is (ξ', ¿jj^-dense. 
■ 
Sketch of proof of the 1-statement of Theorem 8.1 
The proof proceeds by double induction on the number of colors r and the 
number of edges eG. As often happens with induction, it helps to gener-
alize the statement a little. Our strengthening touches all three aspects of 
Theorem 8.1: the random graph space, the property in question, and the 
probability with which this property is held by the random graph. 
(1) We replace the random graph G(n,p) with the reliability network Γρ, 
where Γ is a (£, d)-dense graph on n vertices. 
(2) We replace the partition property Γρ -»· (G)r with the property that 
every r-coloring results in Ω(η"σρ'σ) monochromatic copies of G. Moreover, 
only copies contained in complete subgraphs of Γ count. We call such copies 
nested. Note that when Γ = Kn, every copy of G is nested. 
(3) We replace the convergence of probability to 1 with the condition that 
the probability of the opposite event is 2 - n ' n PK (In what follows we will 
often use the phrase high probability, meaning precisely this.) 
More formally, we prove the following general result which implies the 1-
statement of Theorem 8.1. 
Theorem 8.23. For every graph G with at least one edge, for all integers 
r > 1 and all real numbers 0 < d < 1, there exist positive numbers ξ, a, b, C, 
and no such that if 
(i) n > n0, 
(ii) Γ is a (ξ,ά) -dense graph with n vertices, and 
(iii) p>Cn- 1/m ( :"(G) ) 
then, with probability at least 1 - 2~bn p, every τ-coloring of the edges of Γρ 
results in at least anv°pea 
monochromatic, nested copies of G. 
Sketch of proof. For both initial cases, ec = 1, r arbitrary, and r = 1, ββ 
arbitrary, every copy of G is automatically monochromatic, and all we need 
in order to validate Theorem 8.23 is to show that Γρ contains sufficiently many 
nested copies of G with sufficiently high probability. Proposition 8.20 shows 
that Γ contains θ(η υ ο) complete subgraphs K„c, and a standard application 
of Theorem 2.14 gives the required result (Exercise!). 

220 
EXTREMAL AND RAMSEY PROPERTIES 
A A M 
G 
J 
JJ 
Fig. 8.2 Graphs G, J, and JJ for G = tf3. 
Assume now that eG > 2 and r > 2, and that Theorem 8.23 is true for 
all instances with either fewer than r colors or fewer than eo edges. Our 
strategy is to apply the two-round exposure technique (cf. Section 1.1), that 
is, to represent Γρ as a union of two independent random graphs ΓΡ1 and ΓΡ1, 
where pi +P2—P1P2 = p, and p\ and P2 are suitably chosen. It is planned that 
P2 will be sufficiently bigger than ρχ, but both of the same order of magnitude. 
Let J = J(G, e) be the graph obtained from G by the removal of one fixed 
edge e, and let JJ = JJ(G, e) be obtained from the union of two copies of G 
sharing e by the removal of e (see Figure 8.2, where G = K3, J = P2 and 
JJ = C\). By the induction assumption, with high probability, there are many 
monochromatic, nested copies of J in every r-coloring of ΓΡ1. Thus, there are 
many edges of Γ, which, when added to a monochromatic, nested copy of J, 
form a copy of G, provided there are not too many copies of JJ. It is here 
where we use Lemmas 2.51 and 2.52. Indeed, we are satisfied with an upper 
bound on the number of copies of JJ on a subset of edges of ΓΡ1. By a standard 
application of the Cauchy-Schwarz inequality, there are θ(τι2) edges in Γ such 
that each of them "closes" as many as Ω(η"σ-2ρίσ_1) monochromatic, nested 
copies of J. Most of these edges are not in Γρ. Let us denote the subgraph 
of Γ consisting of all such edges by F, and associate with each edge u of F 
the color most frequently appearing in the monochromatic copies of J in Γρ, 
which, together with u, form a copy of G. The colors associated in the above 
way with the edges of F vary from edge to edge, which naturally imposes a 
partition E(F) = E\ U · · · U Er. Write Ht for the spanning subgraph of F 
with the edge set Εχ, i = 1,... ,r. 
In the second round we would like to apply the induction assumption with 
r - 1 colors to one of the graphs Hi, i = 1,2,... ,r. For this, however, we 
have to show that one of these graphs is ^',d')-dense for some ξ' > 0 and 
d' > 0. It turns out that it is easier to show first that with sufficiently high 
probability the graph F is (£o,do)-dense for some ξο and do, and then apply 
Lemma 8.21. Indeed, the proof that F is (ξο, do)-dense is similar to showing 
that F has many edges, which we have just described. Then, by Lemma 8.21, 
there is a color ¿0 G M and a large subset V C V(F) such that the graph 
H = Hi0 [V] satisfies the assumptions of Theorem 8.23. 
We are now justified in applying the induction assumption with r — 1 colors 
to H. Thus, provided that color i0 has not been used for the random graph 

A PARTITION THEOREM FOR RANDOM GRAPHS 
221 
//P2, we might conclude that the second round produces with high probability 
plenty of monochromatic, nested copies of G in i / w . On the other hand, every 
time color £0 is used on an edge of i/P2, it produces Ω(ηνα~2ρΙ°~χ) 
nested 
copies of G of color £0. Hence, if color ¿Q is used for Hp, at least Q(n2p2) 
times, we are done. It remains to clear the case in which the selected color 
is used only a few times on the edges of H^. 
As all of the above holds 
also for (1 - δ)ρι instead of p2, this last case follows from Lemma 2.52 alone 
(Exercise!). 
No matter how the adversary colored the edges emerging from the first 
round, the outcome of the second round should be successful. 
Therefore, 
the probability of failure in the second round must be much smaller than 
the reciprocal of the number of all possible r-colorings h of the edges of ΓΡ1. 
The number of edges of ΓΡ1 is, by Chernoff's inequality, with high probability, 
fewer than n2pi, and thus the number of such colorings does not exceed r n Pl. 
The probability of failure in the second round is forced to be sufficiently small 
by choosing pi sufficiently bigger than p\. 
Let us now organize the whole proof a little bit more rigorously. Let A be 
the event that there is an r-coloring h : Ε(ΓΡ) -*· [r] with fewer than anVGpea 
monochromatic, nested copies of G. 
For a copy J' of J in Γ, let cl(J') be the set of all edges u € Γ such that 
J' U {u} is isomorphic to G. Given an r-coloring h : Ε(ΓΡι) -> [r], define the 
edge sets 
Et(h) = {u € Ε(Γ) \ £(Γ Ρ 1) : \{J' C ΓΡ1 : u e cl(J') and h(J') = t)\ > z}, 
where z = cnvc-2p\°~l 
for some c, and set He(h) = (V{T),Et(h)), 
I = 
l,...,r. 
Let B be the event that for every Λ : E(TPl) -> [r] there exist an to € [r] 
and a set V C V{Y) = [n], \V\ > vn, such that the graph Hto(h)[V] is (ξ',dú-
dense and that |£(Γ Ρ ι)| < η2ρλ. Conditioning on Γρ, and fixing Λ, let Ah be 
the event that there is an extension of h, h : E(TP) -> [r], that is, h = h when 
restricted to Ε(ΓΡι), 
such that there are fewer than anv°pec 
monochromatic, 
nested copies of G. Then, 
Ρ(Λ) < Ρ(-ιθ) + Σ 
pM I Γρι = Κ) Ρ(ΓΡ1 = Κ) 
Keß 
and 
Ρ(Λ | ΓΡ1 = Κ) = Ρ \\)ΑΗ 
I ΓΡ1 = Κ j < r"'*· Ρ(Λ„0 | ΓΡι = Κ), 
where the summation is taken over all r-colorings h of the edges of ΓΡ1 = K 
and h0 maximizes the conditional probability. 
We have just outlined the proofs of the inequalities 
Ρ(β) > ΐ - 2 _ Ω ( η 2 ρ , ) 

222 
EXTREMAL AND RAMSEY PROPERTIES 
and 
?{Ah | ΓΡ1 = K) < 
2-u^2pi), 
the latter for every K € B and for every r-coloring h of the edges of K. These 
two facts imply that Ψ(Λ) = 2~Ω(η2ϊ)) for some small a. 
U 
8.5 
TRIANGLES: AN APPROACH WITH PERSPECTIVE 
In Section 8.2 we used Goodman's elegant idea to verify Conjecture 8.11 
for triangles and showed that the 1-statement of Theorem 8.1 is valid for 
G — Kz and r = 2. Now we present an entirely new approach based on 
the sparse version of the Szemerédi Regularity Lemma (Lemma 8.19) and a 
better-than-exponential estimate of the probability that a sufficiently "dense" 
and "regular" random graph contains a copy of a given graph G. So far, the 
method verifies Conjecture 8.11 and yields Theorem 8.1 (for an arbitrary 
number r of colors) only in a few small cases of G. But we hope that the 
fundamental Conjecture 8.35 stated below will soon be proved, paving the 
road to a complete solution of Conjecture 8.11. In this section we will restrict 
ourselves to the simplest case G = K3. 
We switch now to the uniform random graph G(n, M). It has the advantage 
over the binomial model G(n,p) that the relative density p(G(n, M)) is fixed 
and equal to Mj (!|). We set 
pM=p{G(n,M))=M 
I 
for convenience. Thus, in this section we will prove the following result. 
Theorem 8.24. For every η > 0 there exists C = 0(η) > 0 such that if 
M > Cn3/2 then a.a.s. every subgraph of G{n, M) with at least (1/2 + η)Μ 
edges contains a triangle. 
By Proposition 1.12, Theorem 8.24 implies Theorem 8.14. 
The idea of proof 
The proof we give contains probabilistic as well as deterministic ingredients. 
To some extent, the general framework is analogous to that of the proof of 
Theorem 8.23. The notion of a (ξ, d)-dense graph is replaced by ξ-uniformity, 
and Lemma 8.26 below has the flavor of Lemma 8.21. Both are consequences 
of the Szemerédi Regularity Lemma. However, in Section 8.4, owing to the 
chosen method of proof, we were able to use its dense version, Lemma 8.17, 
despite the fact that Theorem 8.23 deals with sparse random graphs. Here 
we do not have this option. We will directly apply the sparse version of the 
Szemerédi Regularity Lemma in the form of Lemma 8.19. 
©· 

TRIANGLES: AN APPROACH WITH PERSPECTIVE 223 
The straightforward approach, so successful in the vertex-coloring case (see 
Section 7.6), would be to show that the expected number of the triangle-free 
subgraphs of G(n, M) with M' = (1/2 + η)Μ edges tends to 0. There are, 
roughly, 2M subgraphs of G(n, M) with M' edges and each such subgraph 
can be viewed as a random graph G(n, M') on its own (formally, turn to the 
random graph process {G(n, M)}M and consider its subprocess of M' specified 
steps). Unfortunately, Theorem 3.11 implies that if, say, n3/2 < M' < \n2, 
then P(G(n, M') 2 #3) = e~B(M), 
which may not be sufficient. However, the 
lower bound on P(G(n, M) 2 A3) w a s obtained via a bound on the probability 
that G(n, M) is bipartite. The main idea of this proof is that G(n, M) is so 
far from being bipartite that each subgraph with M' edges contains a highly 
regular tripartite subgraph which is then extremely likely to contain a triangle. 
Before making the above argument rigorous, we must decide how to define 
the tripartite structure in precise, mathematical terms. For our purposes, 
given η,ρ,ε 
> 0, an (η,ρ,ε)-triplet 
is a tripartite graph T with a specified 
tripartition V{T) = Vi U V2 U V3 such that |Vi| = |V2| = |V3| > n, each of the 
pairs (Υι,ν^), 
(V^.V^) and (Vi, V3) is sparsely (T,e)-regular and the number 
of edges in each of the three bipartite graphs induced by these pairs satisfies 
If, moreover, er(Vi,Vj) = \p\Vi\\Vj\] for each pair, then the triplet is said to 
be exact. 
Hence, in order to prove Theorem 8.24, and thus Conjecture 8.11 for tri-
angles, we will show first that a.a.s. every subgraph of G(n, M) with sub-
stantially more than half of its edges contains a large triplet (Corollary 8.27), 
and then that a.a.s. every such triplet contains a triangle (Lemma 8.32). 
By conditioning on the number of edges in the triplet, we will reduce our 
considerations to tripartite random graphs with a fixed number of edges, and 
show that, if highly regular, they contain no triangle with probability (o(l))M 
(Lemma 8.30). 
Similarly one can conduct the proof of the 1-statement of Theorem 8.1 for 
G = A3 and an arbitrary number r of colors. We state without proof an 
appropriate fact as part (ii) of Lemma 8.26. 
Uniformly sparse graphs 
We say that a graph F on n vertices is ξ-uniform if for every pair of disjoint 
subsets X and Y of vertices of F such that |*|, |V| > ξη, its (p(F); F)-density 
is close to 1, or, more precisely, 
i-{<w<*,n = Äl<i + i, 

224 
EXTREMAL AND RAMSEY PROPERTIES 
and the relative density of the subgraph induced by X in F is also close to 1, 
that is, 
p(F[X}) 
e(F[X}) 
1 - ί < ^ 7 ^ = - 7 ^ Κ < 1 + ξ > 
wherep(F) = e F/(5). 
Lemma 8.25. // M/n -> oo and ξ > 0, then the random graph G(n, M) is 
a.a.s. ξ-uniform. 
■ 
The above lemma follows (Exercise!) by an easy application of Chernoff's 
bound for the hypergeometric distribution (see Theorem 2.10 and the inequal-
ity (2.9)). 
Next we show, by a nontrivial application of the sparse version of the 
Szemerédi Regularity Lemma, that every sufficiently dense subgraph of a ξ-
uniform graph contains a large triplet. 
Lemma 8.26. 
(i) For every 0 < η < 1/2 and ε > 0 there exist ξ = ξ{η,ε) > 0 and no 
such that every spanning subgraph H of a ξ-uniform graph F 
onn>no 
vertices satisfying e# > (l/2 + n)ef· contains a (£n,0.1np(F),e)-irip/et. 
(ii) For every natural number r and e > 0 there exist ξ > 0 and no such 
that for every partition E(F) = E\ U · · · U ET of the edges of a ξ-uniform 
graph F with n > n 0 vertices, there exists an to € [r] such that the graph 
Hlo = F[Eto] contains a (^n,p(F)/2r,£)-írip/eí. 
Proof. We will only prove part (i). Before plunging into this detailed and 
slightly tedious proof we strongly encourage the reader to formulate and prove 
the special case of part (i) when F — Kn (Exercise!). 
For the general case, we may assume e < 1. Set e' = en/15 and m = f20/n] 
and apply Lemma 8.19 with ε', b = 2 and r = 1. Let ξ = min(/J, l/2M,e'), 
where β = β(ε', 2, m, 1) and M = Μ(ε', 2, m, 1) are as in Lemma 8.19. Let F 
be a ^-uniform graph on n vertices and let H be a spanning subgraph of F 
satisfying e« > (1/2 + n)ep. Since ξ is smaller than n, the graph H is (2,ξ)-
bounded (Exercise!) and thus, by Lemma 8.19, there is a sparsely 
(H,e',k)-
regular partition Π = (VQ, VI, ..., V*) of vertices of H with m < k < 1/2ξ. 
Note that |Vi| > ^ η 
> ξη for i > 1. 
Call a pair (V¿, l^j), 1 < í < j < k, good if it is sparsely (Jf,e')-regular and 
satisfies 
eH(Vi, Vi) > Q.lqp(F)\Vi\\ViY 
(8-19) 
Our goal is to show that the auxiliary graph, the vertices of which are the 
sets Vi,..., Vjt and the edges represent good pairs, has more than fc2/4 edges. 
Then, by Mantel's theorem, there is a triangle in this graph. This triangle 
consists of three sets V¿,, VÍ2, VÍ3, which induce a tripartite subgraph T of H, 

TRIANGLES: AN APPROACH WITH PERSPECTIVE 
225 
with every pair being sparsely (J/,e')-regular and satisfying (8.19). Since, by 
(8.19), 
p(T) >0.toip(F)\Vx\2/^ll>) 
> ^VP(F) 
> 
^ηρ(Η), 
15 
these pairs are also sparsely (Γ, e)-regular, and it follows that T is a 
(£n,0.l77p(F),e)-triplet. 
To obtain the required lower bound on the number of good pairs, we will 
first bound from above the total number of edges of H which are not within 
the good pairs. It will turn out that the majority of the edges of H is indeed 
between the good pairs and, as there cannot be too many edges between any 
fixed pair, the lower bound on the number of good pairs will follow. 
The edges not within the good pairs can be classified into four groups: 
(a) Edges with at least one endpoint in the exceptional class Vo· For each 
i - 1,..., k, let Wi be a subset of vertices such that Wi D V0, W{ Π VJ = 0 
and \Wi\ = \ε'η\ > ξη. Then, 
e(H[V0}) < e(F[^!]) < (1 + ξ) 0 ^ ) p(F) < 0.01i,eF 
and, similarly, for each ι = 1,..., k, 
eH(V0,V{) 
< eF(Wi,Vi) 
< (1 +0|Wi||V¿|p(F) < 0.20VeF/k, 
yielding a total upper bound of 0.21τ;βρ on the number of these edges. 
(b) Edges with both endpoints in the same class Vit for any i = 1,..., k. The 
number of edges of H contained in the set V¡ is 
e(H[Vi\) < e(F[Vi\) < (1 + ξ) ( ' ^ ' W ) < (1 + OeF/mk 
< 
0MVeF/k. 
Hence the total number of edges in this category is fewer than Ο.Οδτ/βρ. 
(c) Edges between the pairs (VJ, V)), 1 < i < j < k, which are not sparsely 
(Η,ε')-regular. 
Note that, since F is ξ-uniform, for all 1 < i, j < k 
β « ( ν ^ ) < ( 1 + £ ) β Ρ Μ ^ 1 . 
Thus, the number of edges in this category is bounded from above by 
'0 
(1 + 
ξ)βρ1-^^<0.07ηερ. 
\2) 
(d) Edges between the pairs (Vi,Vj) which violate (8.19). There are no more 
than 
G) 
^O.lr^lÄUo.l^ 

226 
EXTREMAL AND RAMSEY PROPERTIES 
such edges. 
Consequently, at least 
e« - 0.44nef > (1 + n)ep/2 
edges of H join good pairs (V¿, Vj). 
On the other hand, we have just noticed in point (c) above that no bipartite 
graph spanned in H by a pair (Vi, Vj), 1 < i < j < k, has more than 
(l + £)eF^[p<(l + 7?)2eF/fc2 
edges. Thus, among the pairs (V¿, Vj), 1 < i,j < k, there must be more than 
(l + 7?)eF/2 
= l f c 2 
2(l + n)eF/fc2 
4 
good ones, and the assertion (i) follows. 
The second part can be proved in an analogous way, but, instead of Mantel's 
theorem, one must use Turin's and Ramsey's theorems (in this order) as in 
the proof of Lemma 8.21. Since this result is needed only for an alternative 
proof of Theorem 8.1 in the very special case G — A3, we leave the proof to 
the reader (Exercise!). 
■ 
Lemma 8.26(i) together with Lemma 8.25 have the following consequence. 
Corollary 8.27. For every 0 < η < 1/2 and ε > 0 there exists ξ = ξ(η, ε) > 0 
such that if M/n -*· oo, then a.a.s. every subgraph H o/G(n, M) with e(H) > 
(l/2 + n)M contains a (ξη,ΟΛηρΜ,e)-triplet. 
■ 
The conclusions of Lemma 8.26 and Corollary 8.27 may be strengthened 
to the existence of an exact triplet by the following simple fact, the proof of 
which is left to the reader. (Exercise! - Use Theorem 2.10.) 
Lemma 8.28. For every ε > 0 there exists C = C(e) > 0 such that if B is a 
bipartite graph with bipartition (Vi, Vi), |Vi| = |V2| = n, with L edges, and the 
pair (νΊ,νϊ) is sparsely {Β,ε)-regular, then for every K with Cn < K < L 
there is a subgraph B' of B with K edges and such that (Vi.Vb) is sparsely 
(B'^-regular. 
U 
Corollary 8.29. Every (η,ρ,ε)-triplet with pn > C(e) contains an exact 
{η,ρ,2ε)-triplet. 
■ 
Tripartite random graphs 
In order to state the main probabilistic ingredient of our argument we need 
to introduce one more model of a random graph. Let G3 (n, M) be a graph 
chosen uniformly at random from the family of all tripartite graphs F with 

TRIANGLES: AN APPROACH WITH PERSPECTIVE 
227 
vertex set V = Y\ U V2 U V3, where |Ki| = |V21 = |V3| = n, such that for each 
1 < * < J < 3, ep(Vi, Vj) = M. For this random graph, although "genuinely" 
tripartite, the probability of containing no triangls does not drop down to 
(o(l))M. Indeed, splitting Vx = V U V", \V'\ = \V"\ = n/2, if M < n2/A, 
then with probability at least 16 _ M there is no edge between V2 and \" and 
no edge between V3 and V" (Exercise!). Hence, with at least this probability, 
there is no triangle in (13(71, M). To make the appearence of triangles more 
likely, we condition on the event that each of the three bipartite subgraphs of 
G3 (n,M) has a highly regular structure, that is, that G3 (n,M) is a triplet. 
Lemma 8.30. For every 0 < ε < 0.01 and natural numbers n and M > 
(B/e)^2n3^, 
P(G3(n, M) is a triangle-free (n, Μ/η2,ε)-triplet) 
< e M / 1 6. 
(8.20) 
We deduce Lemma 8.30 from the following result on random subsets of a 
regular pair of sets. 
Lemma 8.31. Let 0 < ε < 0.01, t > 1, M > An2/{et) and let H be a bipartite 
graph with bipartition ( V , V") such that \V'\ = |V"| = n, e(H) = M and the 
pair (V, V") is sparsely (Η,ε)-regular. 
Furthermore, let S't and S" be two 
random sets of size t picked independently and uniformly from all t-element 
subsets of, respectively, V and V". Then, the probability that there is no edge 
between S't and S" is smaller than e'/4. 
Proof. We may assume that t < en, since otherwise the assumption that 
(V',V") is sparsely (J/,£)-regular implies that there is always an edge be-
tween 5{ and S". We will show that with probability at least 1 - ε'/3 the 
neighborhood of the set S't contains all but at most en vertices of V". This 
is all we need, as the probability that S" is contained in a fixed set of size at 
most en is not greater than 
and thus 
P(e / /(5 t',5; ,)=0)<e f/ 3 + e t < e
t / 4 . 
Let us generate S[ sequentially, picking its elements S\,...,st 
one by one, 
uniformly at random, from all currently available vertices of V. 
For i = 
1,...,ί, let Si = {si,...,Si} 
and let Wi denote the set of all vertices of V" 
which are not adjacent to any vertices of S¿. Set for convenience WO = V". 
Furthermore, let i?¿, i = 0,..., i, be the set of all vertices of V \ S¿ with fewer 
than p(H)\Wi\ neighbors in Wit where, recall, p(H) = p = M/(2
2
n). Note that 
dP,H{V',V") 
= (2n - l)/n > 1.5 for n > 2. 
Suppose that \Wt\> en, and thus \Wi\ > en for each 0 < i < t. Then, for 
each 0 < i < t, \Bi\ < en, since otherwise the pair {Bi,Wi) with its density 

228 
EXTREMAL AND RAMSEY PROPERTIES 
dp,H(Bi, Wi) smaller than 1 would yield a contradiction with the assumption 
that the pair (V, V") is sparsely (H,e)-regular. 
Hence, for each 0 < i < t - 1, 
P(ei+, € Bi) = - ί ^ τ < — 
< 1.02ε. 
n-ι 
n — t 
The supposition that \Wt\ > en has also another consequence. Let us 
consider how many times the event {«i+i € B{) holds. Each time we choose 
si+i 
outside Bi, the size of Wi is decreased by at least p(H)\Wi\ 
> pen, 
that is, \Wi+i\ - \Wi\ < -p(H)\Wi\ 
< -pen. 
This means, however, that the 
event {\Wt\ > en} implies that {SJ+I € B¿} holds at least t/2 times, since 
otherwise \Wt\ - jPVol < -¿pen 
< -etM/4n 
< -n, yielding a contradiction. 
The probability that {s¿+i 6 B{) holds for at least t/2 indices i = 1,..., t can 
be bounded from above by 
(rtJ2l)(l-0fc)ri/al< (4-08e)"2 <¿I\ 
because e < (4.08)-3. Consequently, P(|H^| > ε«) < e t / 3· 
■ 
Proof of Lemma 8.30. For given ε > 0, n and M let Aij be the event that the 
pair (Vj, Vj) is sparsely ( ^ ( η , M),e)-regular and let A = Αι,2 Π >li,3 Π >l2,3-
Further, let AC be the event that G3 (n,M) 
2 K3· 
Hence the event that 
G3 (n, M) is a triangle-free (n, M/n2, e)-triplet is /C Π A. 
Let P denote the event that at least n/2 vertices of V\ each have more than 
t = |"M/2n] neighbors in both V2 and V3. It can be easily verified (Exercise!) 
that the conjunction A\,2ftA\¿ 
implies V with plenty of room to spare. Thus 
Ρ(£ΠΛ) <P(/CnZ>n A d -
ueñóte by Ni(v) the set of all neighbors of a vertex v € V\ which belong to 
Vj, 1 = 2,3, and consider the random vectors Di = (|iV"i(u)| : v € Vi), i = 2,3. 
Let Λ be the set of all pairs of integer vectors of length n and sum M, such 
that at some \n/2] coordinates the entries of both vectors are greater than 
t. Furthermore, let G3(n, M)[V2, V3] be the subgraph of G3 (n, M) induced by 
V2 and V3, and let Ή be the set of all bipartite graphs with vertex set (V2, V3) 
which satisfy property ^2,3. Then, by the law of total probability, 
P(JCnvnA2,3) 
= 
£ 
Σ 
P(/C|{G3(n,M)[V2,V3] = / i } n { D 2 = s 2}n{D3=S3}) 
(»2,»3)6Λ HGH 
x P({D2 = s2} (Ί {D3 = s3})P(G3(n,M)[V2,1/3] = H). 
Clearly, to complete the proof it is enough to show that 
P(£ I {Q, (n, M)[V2, V3] = H) Π {D2 = s2} n {D3 = s3}) < ε 

TRIANGLES: AN APPROACH WITH PERSPECTIVE 
229 
for every triple s2,s3,H. Fix one such triple. For a vertex v G Vi, let Mv be 
the event that βΗ(Ν2{υ),Ν3(υ)) = 0. The event K. implies Πυ€ν, ^ « , which, 
in turn, implies f^ef·, Mv, where Vi is the subset of those vertices of Vi for 
which s2(u),s3(v) > t. 
Now observe that with all the degrees fixed the choices of neighborhoods 
N2(v) and N3{v) are independent of each other for all v € Vi. Moreover, for 
any tx,t2 > t, 
p(e„(s;,,s;;) = o) < P(e„(st',s;') = o), 
where S't and S" are random sets defined as in Lemma 8.31 but of sizes tx 
and t2, respectively. Hence, by Lemma 8.31, 
P(/C | {G3(n,M)[V2, V3] = H} Π {D2 = s2} Π {D3 = s3}) 
< P( f l Mv | {G3(n,M)[V2, V3] = H) Π {D2 = s2} n {D3 = s3}) 
v€V¡ 
= Π K«H(S,(.).^<.>) = 0) < P(e„(S;A") = 0)1^1 < (ε^)η/2 
Proof of Theorem 8.24 
The last ingredient of the proof of Theorem 8.24 is the following lemma. 
Lemma 8.32. For every η > 0 there exists ε — ε(η) > 0 such that for every 
£ > 0 there exists C = 0(η,ε,ξ) 
< oo such that if M > Cn3l2, then a.a.s. 
every (ξη,ηρΜ,e)-triplet contained in G(n, M) contains a triangle. 
Proof. In view of Corollary 8.29, it suffices to prove that a.a.s. every exact 
(£n,77pjtf>e)-triplet contained in G(n, M) contains a triangle. We will do this 
with e < 0.01 so small that δ = 64ε"/16 < 1, and with C = (τ/Ν/ξε/2)-1. 
For any given t with ξη < £ < n/3, let Vi, V2, V3 be three disjoint subsets of 
[n] such that |V\| = |V2| = |V3| = I. Let G{n,M)[Vi, V2, V3] be the tripartite 
subgraph of G(n, M) induced by Vi, V2 and V3 and let G3 {t; Κϊ2, Kl3, K23) be 
the random tripartite graph with vertex set Vi U V2UV3 and Ky edges between 
the sets V¡ and Vj, 1 < i < j < 3. Furthermore, let M' = M'(l) = Γ^ΡΛί^Ι· 
As a guideline for the forthcoming estimates, note that there are at most 
8n choices of Vi, V2, V"3. Observe also that for any property V the probability 
that G3{£;Kn,Ki3,K23) 
contains a spanning subgraph satisfying V which 
has M' edges across each of the three pairs (Vi, V2), (Vi, V3) and (V2, V3), can 
be bounded from above by the probability that G3 (¿, Μ') € V multiplied by 
(KM·)(M'3)(M") (Exercise!). Due to our choice of C, M'/l3'2 
> ηρΜίι/2 > 
2ηΟ{ί/η)^2 
= (8/ε)1/2, and thus we may apply Lemma 8.30 to (^(¿,Λί'). 

230 
EXTREMAL AND RAMSEY PROPERTIES 
Consequently, for any K\^,K\z, Α'23 < ^PMI2 and sufficiently large n, 
f(G(n,M)[V! ,V2,V3] contains a triangle-free exact (£,T;pM,e)-triplet 
I ec(„.M)(Vj, Vj) = Kij, 1 < i < j < 3) 
= P(G3(n; K\2, Ki3, K23) contains a triangle-free exact (£,7/pM,e)-triplet) 
- ( Μ ' ) ( Μ ' ) ( Μ ' ) ν(°*1'>Μ') 
is a * ™« Μ Γ « (¿,f?PM,e)-triplet) 
< 2^u+^i3+KMeM7l6 < 
26pM<a
e.rlpMÍ2/16 _ ¿pM<2 
Moreover, owing to Theorem 2.10, the probability that the random tripartite 
graph G(n,M)[Vi,V2,V3] has more than 2pM& edges in any of the three 
bipartite graphs it forms, can be bounded from above by 3e-3/>M/ /8. Hence, 
by the law of total probability, 
P(G(n,M)[Vi,V2, V3] contains a triangle-free exact {Ι,ηρΜ,ε)-Ιτ\\Αη\) 
<δΡΜ'2 +3e- 3' M / 2 / 8. 
Finally, summing over all t > ξη and subsets V\, V2, V3, 
P(G(n,p) contains a triangle-free exact (£n,7/pM,e)-triplet) 
n/3 
< Σ 
8" (JPM'2 +3e-3"M'2/8) < n&n (¿2M«2 +3e"3M«2/4) = o(l). 
Proof of Theorem 8.24- The theorem now follows from Corollary 8.27 and 
Lemma 8.32, by first choosing e = ε(0Λη) as in Lemma 8.32, then ξ = ξ(η,ε) 
as in Corollary 8.27, and finally C = C(0.1TJ, e, ξ) as in Lemma 8.32 again. ■ 
The argument we have just presented is more involved than the method 
based on Goodman's idea described in Section 8.2, but it has two big advan-
tages. First, it has more potential for generalizations; for example, one can 
easily observe that it can be modified to give a new proof of the 1-statement of 
Theorem 8.1 in the case G = K3 (Exercise!). More importantly, there is some 
hope that it can be generalized to show extremal results for graphs other than 
triangles - see the last subsection of this chapter. Second, the same method 
can give some information on the structure of maximal triangle-free subgraphs 
of a random graph. 
Triangle-free subgraphs 
A structural strengthening of Mantel's result is the Stability Theorem (see, 
e.g., Bollobás (1978, p. 340) and Simonovits (1983)), which states not only 
that the Turan graph (i.e., the balanced complete bipartite graph) maximizes 

TRIANGLES: AN APPROACH WITH PERSPECTIVE 
231 
the number of edges in a triangle-free graph, but that every triangle-free graph 
with the number of edges close to n 2/4 looks very much like the Turan graph. 
A precise statement of the Stability Theorem goes as follows. 
Proposition 8.33. For every a > 0 there exists ß > 0 such that every 
triangle-free graph with n vertices and at least n 2 / 4 - ßn2 edges can be turned 
into the Turan graph by adding and/or deleting at most an2 edges. 
■ 
Now, let us assume that H is a triangle-free subgraph of G(n,M) with 
M > Cn3/2 and e« > M/2, say. As in Lemma 8.26 we apply to H the sparse 
version of the Szemerédi Regularity Lemma (Lemma 8.19) for some small e > 
0 and very large m. In such a way we obtain a partition Π = (Vo, Vj,..., 14), 
in which at least fc2/4 - ßk2 out of the (*) pairs of sets Vi,..., Vk are "good", 
that is, are sparsely (/f,e)-regular and contain a fair number of edges. On 
the other hand in such a partition we a.a.s. do not find a triplet since, as we 
have shown in Lemma 8.32, a.a.s. each triplet which is contained in G(n, M) 
contains a triangle, provided the random graph is dense enough. Thus, the 
auxiliary "partition" graph, with vertices representing the sets V\,..., V* and 
edges between the good pairs, is triangle-free and, owing to Proposition 8.33, 
has a "bipartite-like" structure. This, in turn, implies that H itself must be 
bipartite-like as well. Although the technical details behind the above idea are 
not very appealing, we believe that the reader can convert it into a rigorous 
argument and show the following result (Advanced Exercise!). 
Theorem 8.34. For every constant η > 0 there exists C = 0(η) such that 
for every M > Cn3l2 
a.a.s. each triangle-free subgraph of G(n, M) with at 
least M/2 edges can be made bipartite by omitting at most ηΜ edges. 
■ 
Note also that a.a.s. the random graph G(n, M) contains no bipartite sub-
graph with more than, say, M/2 + nlogn edges (Exercise!), and thus Theo-
rem 8.34 immediately implies Theorem 8.24. 
It is, maybe, worthwhile to mention that the threshold function for the 
property that the largest triangle-free subgraph of a graph is bipartite has 
not yet been found. Thejemark made at the end of Section 8.2 suggests that 
the threshold function M for this property (if it exists) satisfies M » 
Cn3!2. 
On the other hand, Babai, Simonovits and Spencer (1990) proved that for 
M ~ n 2/4, the largest triangle-free subgraph of G(n,M) is a.a.s. bipartite. 
A stronger conjecture 
We conclude this section with some comments on Conjecture 8.11 for graphs G 
other than triangles. As we have already observed, unlike the proof from 
Section 8.2, the argument presented in this section can be easily used for any 
graph G, provided one could show a result similar to Lemma 8.30. More 
specifically, let G be any graph with vertex set {1,2,..., k} and FG {n, M) be 
a graph chosen uniformly at random from all fc-partite graphs F with vertex 
set V, u · · · U Vk such that |Vi| = · · · = |V*| = n, and eF(V¿, Vj) = M for all 

232 
EXTREMAL AND RAMSEY PROPERTIES 
1 < i < j < fc for which {i,j} is an edge of G and e/.-(V'j, Vj) = 0 otherwise. 
Let Λ be the event that each pair (Vj,Vj) is sparsely (FG(n,M),e)-regular 
and let Q be the event that ¥Q (n, M) contains no copy of G. Then the main 
probabilistic problem concerning extremal properties of random graphs (and, 
we believe, one of the most important open questions in the theory of random 
graphs) is to verify the following conjecture of Kohayakawa, Luczak and Rödl 
(1997). 
Conjecture 8.35. For every graph G and every a > 0 there exist constants 
e = e(G, a) and C = C(G, a) such that 
P ( 5 n i ) < a M , 
provided M > Cn2~l/"»(ί)<°>. 
It is not hard to see that this conjecture would imply Theorem 8.1 and, 
coupled with the Erdös-Stone-Simonovits Theorem, would also settle in the 
affirmative Conjecture 8.11, even in its stronger, extremal version analogous 
to Theorem 8.34. Unfortunately, at this moment we are unable to prove it for 
general G. One can easily see that the statement holds trivially when G is a 
tree. Füredi (1994), following ideas of Kleitman and Winston (1982), proved 
that a slightly weaker statement holds for cycles of length four, even if we 
replace Fo(n,M) by G(n,M). Haxell, Kohayakawa and Luczak (1995) and 
Kohayakawa, Kreuter and Steger (1998) extended this result to every cycle of 
even length. A somewhat weaker version of Conjecture 8.35, which, however, 
is sufficient for showing Conjecture 8.11, was proved by Kreuter (1997) for odd 
cycles, and by Kohayakawa, Luczak and Rödl (1997) for G = K\. It seems 
that with a substantial amount of work, methods from Haxell, Kohayakawa 
and Luczak (1995) and Kohayakawa, Luczak and Rödl (1997) can be used 
to prove Conjecture 8.35 for some other graphs, for example, the complete 
bipartite graphs K%¿. The two Kuratowski graphs, K3¿ and K$, are the 
smallest instances of G for which Conjecture 8.35 remains open. 

9 
Random Regular Graphs 
A regular graph is a graph with the same degree (i.e., number of edges) at 
each vertex; we say that the graph is r-regular if the common degree is r. 
Traditionally, a 3-regular graph is also called cubic. 
Note that an r-regular graph with n vertices has rn/2 edges; hence rn has 
to be even. Moreover, r < n - 1. Conversely, it is easily seen that there exist 
r-regular graphs with n vertices whenever rn is even and n > r >0. 
We define the random r-regular graph G(n, r) to be a random graph with 
the uniform distribution over all r-regular graphs on n given vertices, say [n]. 
We assume, whenever talking about G(n,r), that rn is even and n > r. 
In this chapter we will study random r-regular graphs for a fixed r > 1, 
letting the number of vertices n tend to infinity. (If r is odd, we tacitly assume 
n to be even.) The reader may think of r as quite small, for example, 3 or 4. 
(Smaller values of r are too simple, and are exceptions to many of the results 
below.) We will here not consider the case r -► oo (as some function of n), 
which is rather different. 
As we will see below, random r-regular graphs turn out to have properties 
quite different from the two basic models G(n, p) (the binomial random graph) 
and G(n, M) (the uniform random graph). For example, they are sparse but 
typically connected. 
This chapter is to a large extent based on Janson (1995b), which also 
contains similar results for random regular directed graphs; see further, for 
example, Cooper, Frieze and Molloy (1994). For further results, including sev-
eral topics not covered here, see Frieze and Luczak (1992) (chromatic number) 
and the recent survey by Wormald (1999b). 
233 
Random Graphs 
by Svante Janson, Tomasz Luczak and Andrzej Rucinski 
Copyright © 2000 John Wiley & Sons, Inc. 

234 
RANDOM REGULAR GRAPHS 
Example 9.1. The cases r = 0 and r = 1 are trivial. A 0-regular graph is an 
empty graph, and a 1-regular graph is the same as a perfect matching, that 
is, a set of disjoint edges covering all the vertices. The random graph G(n, 1) 
is thus obtained by randomly choosing one of the (n - 1)!! = n!/2 n / 2(n/2)! 
partitions of the vertex set into n/2 pairs. 
Example 9.2. Also the case r = 2 is rather simple. It is obvious that every 
component in a 2-regular graph is a cycle, so G(n, 2) is obtained by a random 
partition of the vertex set into cycles of length at least 3. This is similar 
to the cycle decomposition of a permutation, although there the cycles are 
oriented. Hence, the study of G(n, 2) becomes similar to the study of random 
permutations. 
The definition of random regular graphs that we have given is conceptually 
simple, but it is not so easy to use. (For example, there is no simple formula 
for the total number of r-regular graphs on n vertices, so we do not even know 
the probability of obtaining a given r-regular graph. An asymptotic formula 
was given by Bender and Canfield (1978); see Corollary 9.8 below.) Of course, 
we may obtain G(n,r) from G(n,p) (for any p € (0,1), but p = r/n seems 
natural) or G(n, M) (with M = rn/2) by conditioning on the graph being 
r-regular, but the probability of this event is exponentially small and this 
procedure is not very useful. 
Fortunately, there is an efficient way to generate G(n,r) that is useful both 
for theoretical studies as here and (for small r) for the practical problem of 
constructing random regular graphs in simulations. This is the configuration 
model, which will be described in the next section. 
Note also that there are other natural ways to generate regular graphs 
at random. For example, we may construct r independent random perfect 
matchings of the n given vertices (n even) and take the union of them, con-
ditioning on the event that there are no multiple edges. Another possibility 
is to take [r/2J independent Hamilton cycles on the vertices, together with a 
perfect matching in the case in which r is odd. 
A different approach is to construct the graph sequentially by adding edges 
one by one, at each step randomly (uniformly) choosing between all remaining 
edges that do not increase any vertex degree above r. 
(It was shown by 
Rucinski and Wormald (1992) that this process a.a.s. leads to an r-regular 
graph.) 
It should be emphasized that these constructions do not give uniformly dis-
tributed r-regular graphs. However, the resulting distributions are interesting 
in their own right. Moreover, it has been proved in several cases (and conjec-
tured in others) that the distribution is not too far from the uniform one, in 
the sense that properties holding a.a.s. for one of the distributions also hold 
a.a.s. for the other; this, which is expressed by saying that the distributions 
are contiguous, will be studied in Sections 9.5 and 9.6. 

THE CONFIGURATION MODEL 
235 
9.1 
THE CONFIGURATION MODEL 
Most work on random regular graphs is based on the following construction, 
due in different versions to Bender and Canfield (1978) and Bollobás (1980, 
1985). We will use Bollobás's version, which has become standard. 
Given a set V, which is to be the vertex set of the graph, we associate 
disjoint r-element sets to the elements in V. In order to be specific, let n 
and r be positive integers (with rn even), take V — [n], and consider the 
set W = [n] x [r]; A configuration is a partition of W into rn/2 pairs; these 
pairs are called edges of the configuration and the points in W are called half-
edges. The natural projection of the set W onto V = [n] (ignoring the second 
coordinate) projects each configuration F to a multigraph n(F) on V. Note 
that TT(F) may contain loops (arising from edges in F between two half-edges 
corresponding to the same vertex in V) and multiple edges (arising from sets 
of two or more edges in F whose endpoints correspond to the same pair of 
vertices in V). Thus n(F) is, in general, not a simple graph. Note, however, 
that 7r(F) is an r-regular multigraph (with multiple edges and loops counted 
in the natural way). In particular, if n(F) lacks loops and multiple edges, it 
is an r-regular graph. 
There are (rn - 1)!! = (rn)!/2 r n / 2(rn/2)! different configurations on W 
(Exercise!). When we talk about a random configuration, we will always 
(unless we explicitly specify a different distribution) mean a configuration 
chosen at random, uniformly among all possibilities. Each r-regular graph 
on V is the projection of the same number of configurations [viz. r! n), and 
it follows that if we take the projection n(F) of a random configuration and 
condition on it being a simple graph, we obtain a random r-regular graph on 
V with the uniform distribution over all such graphs (Exercise!). This is thus 
a construction of the random graph G(n,r). 
It turns out that it is often advantageous to allow loops and multiple edges 
and work with r-regular multigraphs, if necessary afterwards conditioning to 
simple graphs. We thus define the random r-regular multigraph G* (n, r) to 
be the multigraph n(F) obtained from a random configuration F. Note that 
G* (n, r) does not have the uniform distribution over all r-regular multigraphs 
on V, because different multigraphs arise from different numbers of config-
urations. In fact, it is easily seen that the probability of obtaining a given 
multigraph is proportional to a weight consisting of the product of a factor 
1/2 for each loop and a factor 1/j! for each multiple edge of multiplicity j . 
(In particular, the weight for any simple graph is 1, in accordance with the 
fact stated above that the conditional distribution over the r-regular simple 
graphs is uniform.) 
Remark 9.3. It may be shown that the distribution of G* (n, r) is contiguous 
to the uniform distribution, in the sense of Section 9.5; see Janson (1995b). 
An important feature of the configuration model is that the probability 
of obtaining a simple graph, P(G* (n, r) is simple) is bounded below by some 

236 
RANDOM REGULAR GRAPHS 
positive number (depending on r) for all n > τ\ more precisely, as will be 
shown in Section 9.2, the probability converges to exp(—(r2 -1)/4) as n -> co. 
Hence, the probability that an event occurs for the random r-regular graph 
G(n, r) is bounded by a constant times the probability that the event occurs 
for G*(n,r), which equals the probability that a corresponding event occurs 
for a random configuration. In particular, any event holding a.a.s. for G* (n, r) 
also holds a.a.s. for G(n,r) (Exercise!). 
It follows directly from the definitions that the probability that any given 
set of k disjoint edges on W is contained in a random configuration is given 
by 
_ (rn -2k- 
1)!! _ 
1 
Pk~ 
( r n - 1 ) ! ! 
"~ ( r n - l ) ( r n - 3 ) - ( r n - 2 J f c + l ) ' 
^ ' 
This will be used repeatedly in the sequel. In particular, we will use the 
following asymptotical results. 
Lemma 9.4. 
(i) If m>2 
is even, then 
(m - 1)!! = v /2m m / 2e- m / 2(l + 0(l/m)). 
(ii) // k is fixed and n —► oo, then 
Pk ~ ("»)"*· 
(iii) If rn - 2k -* oo, then 
pk ~ n-"ek{r 
- 2fc/n) r n / 2-*r- r n / 2. 
Proof, (i) follows from (m - 1)!! = m!/2 m/ 2(m/2)! and Stirling's formula, (ii) 
follows directly from (9.1), and (iii) follows from (9.1) and (i) (Exercise!). 
■ 
9.2 SMALL CYCLES 
The foundation of our results for random r-regular graphs is the study of the 
numbers of small cycles, due to Bollobás (1980, 1985) and Wormald (1981b). 
Given a (multi)graph G, we let Z* = Zk(G) denote the number of cycles of 
length k in G. Here for simple graphs we let k = 3,4,..., but for multigraphs 
we let k = 1,2,..., where Z\ is the number of loops and Zi is the number 
of pairs of parallel edges. Note that a multigraph is simple if and only if 
Zl = Z2 = 0. 
Taking the graph G to be our random graph G(n, r) or G* (n, r), Z* becomes 
a random variable. We then have the following theorem. By joint convergence 

SMALL CYCLES 
237 
of an infinite number of variables we mean joint convergence of every finite 
subset, which is equivalent to convergence in R°°. 
Theorem 9.5. Let Xk = ¿ ( r - 1)* and let Zkoo £ Ρο(λ*) be independent 
Poisson distributed random variables, k = 1,2,3,... Then the random vari-
ables Zk(G'{n,r)) 
converge in distribution to Zkoo, Zk(G' (n,r)) -> Zkoo as 
n —> oo, jointly for all fc. 
Proof. We use the method of moments, more precisely Theorem 6.10. We 
begin by computing the expectation of Zk = Zk(G*(n,r)). 
Each fc-cycle in 
G* (n, r) arises from a set of k edges in the corresponding configuration, such 
that the endpoints of the edges match properly when they are projected to 
V; with a slight abuse of language, we call such a set of k edges a fc-cycle on 
W. Let ak be the number of possible fc-cycles on W. The probability that a 
given one of them is contained in a random configuration is pk given in (9.1), 
and thus EZ* = akpk. 
In order to calculate ak, we consider oriented cycles, with a specified ini-
tial vertex and a specified direction, and note that each (unoriented) fc-cycle 
corresponds to 2fc oriented ones. Hence the number of oriented fc-cycles on 
W is 2kak. Moreover, an oriented fc-cycle on W consists of fc edges that can 
be written ((UJ,X¿), (u¿+i,yi+i)) (with indices taken modulo fc), and it is thus 
described by a sequence of fc distinct vertices vi,...,vk 
€ V and, for each 
i = 1,... ,fc, two distinct indices Pi,qi 6 [r]. This description is unique, and 
thus 
2kak = (n)k(r(r - l))k. 
(9.2) 
For fixed fc and r we thus have, as n —► oo, ak ~ ^¡¿^^(r 
— 1)* and, by 
Lemma 9.4(ii), pk ~ (rn)~h. 
Consequently, 
EZ* = a f c p * ~ — ( r - l ) f c = A f c . 
In other words, EZjt -> λ* as n —► oo, for each fc > 1. 
Before proceeding, let us note that the same argument shows that if H is 
any (multi)graph with v vertices and e edges, then the expected number of 
copies of H in G* (n, r) is 0{nv)0{n~e) 
= 0(nv~e) 
(Exercise!). In particular, 
if H is a graph with more edges than vertices, that is, v < e, the expected 
number of copies of H is 
0{n~x). 
Next, we compute factorial moments. We begin with E(Z*)2 and indicate 
the small modifications needed in the general case later. 
Note that {Zk)i is the number of ordered pairs of two distinct fc-cycles in 
G*(n,r). The two fc-cycles may or may not intersect, and we write {Zk)2 = 
Y' + Y" where Y' is the number of ordered pairs of (vertex) disjoint fc-cycles, 
while Y" is the number of ordered pairs of distinct fc-cycles having at least 
one common vertex. 

238 
RANDOM REGULAR GRAPHS 
The number Y" may be further decomposed according to the number of 
common vertices and edges, and their relative positions. This expresses Y" as 
the sum of a number of terms Yj', where the number of terms depends on k but 
not on n, and each term counts the number of copies of some (multi)graph Hj 
in G* (n,r), where H3 (being the union of two distinct cycles with at least one 
common vertex) is connected and has more than one cycle. Each such Hj has 
more edges than vertices and thus, by the result just shown, EY" — 
0(n~l) 
for each j . Summing over all j , we finally obtain 
EY" = 0{η~ι). 
Hence the main term is E V , which we compute in the same way as EZ*. 
If akk denotes the number of ordered pairs of possible fc-cycles on W that 
project to disjoint cycles on V, we obtain as above, orienting both cycles, 
(2k)2akk = (n)2k{r(r 
- l))2* ~ (2fca*)2. 
The probability that a given pair of disjoint fc-cycles on W is contained in a 
random configuration is p2* ~ (rn)~2k 
~ p\, and consequently, 
EY' = akkp2k ~ iflkPk)2 ~ A2.. 
E(Zfc)2 =EY' 
+ EY" = 
Xl+o(l). 
The same argument applies to any factorial moment E(Zt) m, and more 
generally to any joint factorial moment E(Zi)m,(Z2)m2 •••(^i)m); w e now 
consider the number of sequences of mi + 7712 + ■■ ■ +m¡ distinct cycles such 
that the first mi have length 1, the next m-i have length 2, etc. (Here I > 1 
and tni,...,mj > 0 are any fixed integers.) As before we write this number 
as Y' + Y", where Y' counts the sequences of disjoint cycles, and split Y" 
further according to the pattern of intersection of the cycles into a sum of 
terms Y", each counting the number of copies of some graph Hj in G* (n,r). 
The graphs Hj that appear here are unions of cycles, each having at least 
one component with more than one cycle, and it is easy to see that each Hj 
has more edges than vertices. Hence, we again obtain EY" = 0(n~l) 
and 
EY" = 0{n~1) 
(Exercise!). 
For Y', the same argument as above yields EY' 
~ λ^1 λ™2 · ·· Aj7", and 
summing we obtain 
E(Zi) m i (Z 2) m i · · · (Z,)mi -► \?> A™» · ■ · A|"'. 
(9.3) 
By Theorem 6.10, this implies that the joint distribution of Z\,..., 
Z\ con-
verges to the joint distribution of Ζ ι » , . . . , Z/oo> which completes the proof. 
(Note that the right-hand side of (9.3) equals E(Zioo)m, (Z2oo)m2 ■ · · (Zioo)mi ■) 
■ 
Recall that G* (n, r) is simple if and only if Zi = Z2 = 0, and that G* (n, r) 
conditioned on Zi = Z2 = 0 yields G(n,r). 

HAMILTON CYCLES 
239 
Corollary 9.6. Let A* and Zk<x be as in Theorem 9.5. Then the random 
variables Zfc(G(n,r)) converge in distribution to Zkoa, Zk(G(n,r)) 
-»· Zkoo 
as n -> oo, jointly for all k > 3. 
Proof. Directly from Theorem 9.5, conditioning on Z\ = Z2 = 0 (Exercise!). 
■ 
Corollary 9.7. If n -^ oo, then 
P(G* (n,r) is simple) -> ΡΓ(τ*-λ)ΙΛ > 0. 
Proof. Theorem 9.5 yields 
ψ(Ζ0 = Z, = 0) -> P(Z0oo = Z l o o = 0) = ε- λ«- λ 2, 
where Aj + A2 = \{r - 1) + ±(r - l) 2 = ±(r2 - 1). 
■ 
Corollary 9.8. The number L„ of labelled r-regular graphs on n nodes sat-
isfies, as n -» oo for fixed r, 
Ln ~ 
V2e-{T2-l)/A(rr'2e-r'2/rl)nnrn'2. 
Proof. The number of configurations is (rn-1)!! ~ \/2(rn/e)rn^2, 
the propor-
tion of them that yield simple graphs is e _ ( r - 1 ) / 4 +o(l) by Corollary 9.7, and 
there are r!n such configurations corresponding to each r-regular graph. 
■ 
Theorem 9.9. Any property that holds a.a.s. for G* (n, r) holds a.a.s. for 
G(n,r) too. 
Proof. Suppose that the property V holds a.a.s. for G*(n, r). Then Corol-
lary 9.7 yields 
P(G(n, r) does not have V) 
= P(G* (n, r) does not have V \ G* (n, r) is simple) 
_ P(G* (n, r) does not have V and is simple) 
~ 
P(G* (n, r) is simple) 
P(G* (n, r) does not have V) 
_ 
< —-— 
^ -^ 0. 
■ 
P(G* (n, r) is simple) 
The converse does not hold, as the trivial example of not containing a loop 
shows. 
9.3 HAMILTON CYCLES 
In the preceding section we studied the numbers of cycles of a given fixed 
length in a random regular graph. Let us now instead study Hamilton cycles, 
that is, cycles of length n (where n as usual is the number of vertices). We 

240 
RANDOM REGULAR GRAPHS 
let H(G) denote the number of Hamilton cycles in a (multi)graph G, and let 
H„ = #(G(n,r)), H'n = H(G'{n,r)). 
In the notation of Section 9.2, thus H¿ = Zn and by (9.2) and (9.1), 
EHn - anPn = 
{rn_iyi 
. 
(9.4) 
If T = 0 or 1, then there is clearly never any Hamilton cycle at all in G(n, r) or 
G* (n, r). If r = 2, then (9.4) yields, using Stirling's formula and Lemma 9.4(i), 
n!2n 
EH' = 2n(2n - 1)!! 
and thus Ei/*-»0asn->oo. Hence, by the first moment method, there is 
a.a.s. no Hamilton cycle in G*(ra,2). By Theorem 9.9, there is also a.a.s. no 
Hamilton cycle in G(n,2). 
Let us thus assume r > 3. Then (9.4), Stirling's formula and Lemma 9.4 
yield 
Eif; ~ (^)'/'C<'--1>")"(r - 2 r « - v - ' » ' V „ -
- ^-"((Λ1^Γ")". 
<w> 
Moreover, it is easily verified that 
for example because A(3) = 2/\/3, while for r > 4, 
A(r) = (r - 1) (l + — ) 
> (r - lie"1 > 1. 
Consequently, EJZ* -4 oo as n -+ oo, which suggests (but does not prove) 
that G* (n, r) typically has lots of Hamilton cycles when r > 3. 
In order to study the existence of Hamilton cycles further, we calculate the 
variance of Ji*. This is considerably more involved than the calculation just 
given for the expectation, and we give for the moment only the result (Frieze, 
Jerrum, Molloy, Robinson and Wormald 1996), postponing the proof to the 
next section. 
Lemma 9.10. // r > 3, then 
ETO2 ~ Γ ^ Ι ^ 2 " ~ 732(E//")2 
(9·6) 
and thus 
Var(//;)/(Er7;) 2-^—-. 

HAMILTON CYCLES 
241 
Recall that if Xn is any sequence of random variables with E X„ > 0 and 
Var(A'„)/(EX„)2 -> 0, or equivalently EX2/(EXn)2 
-► 1, then Chebyshev's 
inequality yields Xn/EXn 
A 1 and, in particular, V(Xn > 0) -> 1; see (1.2) 
and Section 3.1. For H¿ this fails, but just barely; Var(//*)/(E //*) 2 converges 
to a positive number. This suggests that H^l E H\ converges in distribution 
to a nondegenerate distribution, and we will soon show that this indeed is the 
case. 
We have so far considered H„, but the situation is the same for Hn; we 
will later show the corresponding results (Robinson and Wormald 1994): 
EHn~eEH'n~eyßn-l>2A(r)n, 
(9.7) 
EH2/(EHn)2 
~ e - 2 ^ - 1 * - ^ 
(9.8) 
r — 2 
and thus 
Var(tfn)/(Etfn)2 -> e-2"r-»-!— 
- 1. 
(9.9) 
T — ¿ 
Again, the second moment method fails. For example, for r = 3, the 
inequality (3.2) yields limsupP(i/„ = 0) < lim Var(H„)/(E/f„)2 = \ - 1 « 
0.104, while the sharper inequality (3.4), using P{Hn > 0) > (E Hn)2/ E H 2 -» 
e/3, yields limsupP(# n = 0) < 1 - l i m ( E # „ ) 2 / E # 2 = l - f a 0.094. This 
shows that Hamilton cycles appear in at least 90% of the realizations of G(n, 3) 
when n is large, but this estimate is not sharp. Indeed, as we will show below, 
G(n,r) a.a.s. has a Hamilton cycle for any r > 3. This was generally believed 
for a long time, and explicitly conjectured by Bollobás (1981b); partial results 
were given by various methods by Bollobás (1983) (r > 107), Fenner and 
Frieze (1984) (r > 796) and Frieze (1988) (r > 85), but the general result was 
not proved until the papers by Robinson and Wormald (1992, 1994). 
The main idea in the proof by Robinson and Wormald is to condition on 
the number of small cycles, compute the conditional variance which turns 
out to be rather small, and use Chebyshev's inequality for the conditioned 
variables. This is thus a conditioned version of the second moment method. 
The argument can be regarded as an analysis of variance; the main point 
is that most of the total variance is explained by the variance between the 
groups, whence the variance within the groups is relatively small; see Cooper, 
Frieze, Molloy and Reed (1996) for further comments (and results). 
Remark 9.11. One can similarly study random bipartite regular graphs. It 
turns out that in this case EH2/(EHn)2 
-» 1 and thus the second moment 
method applies, at least provided r = 3 (Robinson and Wormald 1984). This 
probably holds also for r > 4, but as far as we know no one has yet verified 
this. The fact that a.a.s. there exists a Hamilton cycle in a random bipartite 
regular graph with r > 4 was proved in Robinson and Wormald (1994) by the 
method indicated in Remark 9.39, below.) 

242 
RANDOM REGULAR GRAPHS 
We state a general theorem (Janson 1995b) that will be applicable also in 
several similar situations below. In our applications we let X{n = Zi„, the 
number of cycles of length i in a random regular (multi)graph. The proof, 
which is based on the argument in Robinson and Wormald (1992) and related 
to the projection methods discussed in Section 6.4, will be given in the next 
section. We define 0° = 1. 
Theorem 9.12. Let A¿ > 0 and μ* > 0, i — 1,2,..., be constants, let Si = 
ßi/Xi - 1 > —1, and suppose that for each n there are random variables 
Xin, i = 1,2,..., and Yn (defined on the same probability space), such that 
Xi„ is non-negative integer valued and EYn ψ 0 (at least for large n) and, 
furthermore, the following conditions are satisfied: 
(Al) Xi„ -> Xjoo as n -^ co, jointly for all i, where Xioo € Po(A¿) are 
independent Poisson random variables; 
(A2) For any finite sequence Χι,... ,xm of non-negative integers, 
E(Ynl[Xln 
= Xl,...,Xmn=xm}) 
πμ*' 
_μί 
Π 
Ern 
l\ xs 
(A3) Σ<λΑ?<°ο; 
_EYI 
(EYn) 
EV 2 
(A4) 
" 
-> exp(]T) AiJ2) 
as n -> oo. 
Then 
Yn 
W = J | ( l + Ji) X i~e- A , i· 
as n -> oo; 
(9.10) 
moreover, this and the convergence in (Al) hold jointly. 
Furthermore, the 
normalized variables Yn/EY„ 
are uniformly square integrable, and EW2 
= 
l i m ^ o o E ^ / Í E n , ) 2 . 
The infinite product defining W converges, so W is well defined. This is 
not obvious, so we state it together with some useful properties as the next 
theorem. The proof is postponed to the next section. 
Theorem 9.13. Suppose that A¿ > 0 and Si > - 1 and Xioo are as in Theo-
rem 9.12 and that (A3) holds. Then the infinite product defining W in (9.10) 
converges a.s. and in L2, EW = 1 and EW2 = exp(^2i AjJ2). Furthermore, 
the event W > 0 equaL·, up to a set of probability zero, the event that Xioo > 0 
for some i with <5» = —1. In particular, W > 0 a.s. if and only if Si > - 1 for 
every i. 
Note that a.s. here and below is used in the standard sense "with probability 
1"; see Remark 1.2. 

HAMILTON CYCLES 
243 
Remark 9.14. The values of λ,, μι and á, are prescribed by (Al) and (A2). 
If sup„ EXfn < oo for each ¿, as is the case in all our applications, then 
{Xin}n 
and {XinYn/&Yn}n 
are uniformly integrable (for fixed i) and we obtain (by 
Theorem 9.12 or directly from (Al) and (A2)) 
E Xin ~~* E X¿oo = λ, 
E(XinYJEYn) 
-► EXiooW 
= E l ^ t l + i i ^ - e - ^ ' = λ4(1 -Μ<) = W 
and thus 
A¿ = lim EX, n 
n—»oo 
n= iim E(xiny„)/Er„ 
n—»oo 
<5¿= lim E(XinYn)/(EXinEYn)-l 
= lim Cov(X j n,y n)/(EX i nEy„). 
n—»oo 
n—»oo 
Remark 9.15. It follows from the proof that if we have several sequences 
of variables Yn , each satisfying the conditions of Theorem 9.12 (possibly 
with different δ\ ) with the same variables X¿n, then the Yn 
converge 
jointly. It follows, for example, by the uniform square integrability, that 
EY^Y^/ÍEY^EY^) 
-> EW^wm =exP(j:iAiíi
(1)¿í2)). 
Remark 9.16. If (Al) holds, then (A2) is easily seen to be equivalent to 
(A2') For any finite sequence x i , . . . ,xm of non-negative integers, 
E(yn | Xln = xu„.,Xnn 
= xm) ^ γ[{1 
+ δί)ζ<β-Μ 
ω 
n^QO 
EYn 
¿=1 
We will in all our applications of Theorem 9.12 verify condition (A2) by 
the method of moments argument used by Robinson and Wormald (1992). 
We state this step as a separate lemma. 
Lemma 9.17. Suppose that Yn > 0 and define (when EYn > 0) a new prob-
ability measure Qn by Qn{A) = E(YnlA)/EYn 
for every event A in the prob-
ability space Ω„ where X¡„ and Yn are defined. Then (A2) is equivalent to 
(A2") Under the measures Qn, Xin -+ Ρο(μ<), jointly for all i with indepen-
dent limits. 
In particular, (A2) follows if 
m 
(A2'") EQn((Xin)jl 
■■■(Xmn)jn) -*■ JJμ* as n -+ oo, for every finite se-
1 
quence j \ , . . . , j m of non-negative integers. 
Proof. The first statement is clear by the definition of Qn, and the second 
follows by the method of moments (Theorem 6.10). 
■ 

244 
RANDOM REGULAR GRAPHS 
Remark 9.18. (A2'") can also be written 
m 
Ε(Κη(Χ,η)7ι · · · (Xmn)>m)/EYn-► I l t f , 
i 
avoiding explicit mention of Qn (Exercise!). 
For Hamilton cycles, the general theorem yields the following result; we 
postpone the proof. 
Theorem 9.19. Let r > 3 be fixed and let Hn = i/(G(n,r)) be the number 
of Hamilton cycles in a random r-regular graph on n vertices. Then 
H
n 
<*■ 
W = Π(ι-7^Γ<"'· 
<Μ·> 
E"» 
Í>3V 
( r - 1 ' 
i odd 
where Ζχοο 6 P°( 2¿ ) are independent Poisson random variables. 
As a consequence, we obtain the existence result by Robinson and Wormald 
(1992, 1994). 
Theorem 9.20. // r > 3 is fixed, then G(n,r) a.a.s. has a Hamilton cycle. 
Proof. The limiting random variable W in Theorem 9.19 is positive a.s. by 
Theorem 9.13, since <5¿ = - 2 / ( r - 1)* > - 1 . Hence Theorem 9.19 implies 
?(Hn > \) = ?(Hn/EHn 
> 0 ) -► P(W > 0) = 1. 
■ 
An immediate consequence of the existence of a Hamilton cycle is that the 
graph contains a perfect matching (provided n is even). 
Corollary 9.21. // r > 3 is fixed and n is even, then G(n,r) a.a.s. contains 
a perfect matching. 
■ 
Remark 9.22. The existence of a Hamilton cycle also proves 2-connectivity 
a.a.s. of G(n,r) for r > 3. (Again, this is false if r < 2.) However, this is 
not the historical path. Indeed, connectivity was proved much earlier and 
by much simpler methods, which also show the stronger result that G(n, r) 
a.a.s. is r-connected (Bollobas 1981b, 1985, Wormald 1981a); see also Luczak 
(1992). 
The limiting distribution in Theorem 9.19 is more complicated than the 
ones that usually appear in the theory of random graphs. The Poisson vari-
ables Zioo that appear have, however, a simple interpretation as the numbers 
of small cycles in the random regular graph. More precisely, see Theorem 9.12, 
if we let Zin be the number of cycles of length i in the graph, then Ζ<„ -*· Z*«, 
a s n - 4 o o , and this convergence in distribution holds jointly, for all i, together 

HAMILTON CYCLES 
245 
with Hn/EHn 
A W. Hence the theorem can be interpreted as saying that 
Hn/EHn 
is, with large probability, well approximated by the infinite product 
with Zioo replaced by the small cycle count Z,„, or rather by a finite product 
Π£=ι(ΐ - 2/( r - l) 2*+ 1) z" + ,' ne I / ( 2* + 1 ). (We have to let n -* oo first and 
then K -t oo in order to get convergence, because the factors e1/(2fc+1), which 
are necessary convergence factors in (9.11), act as divergence factors for any 
finite n.) 
It should perhaps not be surprising that H„ thus essentially is determined 
by the numbers of small cycles. Similar results hold for the standard models 
G(n,p) and G(n,M), where for certain ranges of p and M, \ogHn is well 
approximated by a linear function of the number of edges, and of the number of 
paths of length 2, respectively; see Janson (1994b). Those results are simpler, 
however, since log Hn then is asymptotically normal, while in the present case 
log Hn - log E Hn converges to log W which has a rather complicated infinitely 
divisible distribution. Moreover, for G(n,p) and G(n, M) it suffices to use 
one small subgraph count in the approximation, whereas here we need an 
infinite sequence. (There are parallels to this in G(n,p) for other functionals; 
see Section 6.4, Barbour, Janson, Karoriski and Rucinski (1990) and Janson 
(1994a).) 
Note that every small subgraph count for random regular graphs can be 
essentially expressed in terms of small cycle counts, since a random regular 
graph a.a.s. has no small multicyclic subgraphs. This explains why only cy-
cle counts appear in Theorem 9.19 (as Zioo), 
but it seems mysterious that 
only the odd cycle counts appear, while the even cycle counts are asymptoti-
cally independent of H„. (Recall in this connection that for random bipartite 
cubic graphs, where there are no odd cycles at all, EH*/(EHn)2 
-> 1; see 
Remark 9.11.) 
We may extend Theorem 9.19 to multigraphs. 
Theorem 9.23. Let r > 3 be fixed and let / / ' = H(G*(n,r)) be the number 
of Hamilton cycles in a random r-regular multigraph on n vertices. Then 
i odd 
where Zioo € P o ( i i ^ ) are independent Poisson random variables. 
As above, the variables Zi0o are the limits of the cycle counts Zin. 
Note that the limit distribution for multigraphs in Theorem 9.23 differs 
from the one for graphs in Theorem 9.19 only by the additional factor (l -
τττ) 
Iooe, where ZXOo € Po(IL=i) should be interpreted as (the limit of) the 
number of 1-cycles, or loops, in the multigraph. 
If r > 4, so that 1 - ^ 
> 0, then the limit random variable W is a.s. 
strictly positive also in Theorem 9.23, and thus the theorem implies that a 

246 
RANDOM REGULAR GRAPHS 
random r-regular multigraph a.as. contains a Hamilton cycle. On the other 
hand, if r = 3, then 1 - ^ τ = 0, and thus W = 0 when Z\oo > 0 but, 
as before, W > 0 a.s. when Ζχ^ = 0. Consequently, it follows from the 
joint convergence of (H¿,Zin) to (W, Ζγ,χ,) that a random cubic multigraph 
a.a.s. is Hamiltonian if and only if it lacks loops, in the sense that as n -» 
oo, the probability tends to 0 that a random cubic multigraph has one of 
these properties but not the other. (One implication is obvious for all cubic 
multigraphs; the other holds only asymptotically.) 
Remark 9.24. The joint convergence of (H^, Z\n,Z2n) 
and the fact that 
P(£ioo = Z2oo = 0) > 0 imply that the limit in Theorem 9.23 remains true if 
we condition on Z\n = Z2n = 0, that is, if we let (X \ A) denote the random 
variable X conditioned on the event A, 
(ΕΊ^ IZln = Z2n=°) ^{w'
 Zlo°
= 
Z2°°
=0) 
i>3 
V 
' 
t odd 
¿i\ 
The left-hand side is the distribution of the number Hn = H(G(n,r)) of 
Hamilton cycles in a random regular graph, normalized by the expectation 
E/i* for the random multigraph. Since the uniform integrability of ff£/ E/i* 
(see Theorem 9.12) survives the conditioning, taking the expectation yields 
EH„/EH; = E(H* I Zln = Z2n = 0)/Efí· -► E(W | Z1OO = Z2oo = 0) = e, 
and we recover Theorem 9.19 as a corollary to Theorem 9.23. 
Remark 9.25. We can use the conditioning argument in Remark 9.24 in all 
cases where Theorem 9.12 applies. We obtain, if Y„ = (Yn I -Xin = X2n = 0), 
EYjEYn 
-* E(W | Xloo = X2oo = 0) = βχρί-λιδ! - λ2<52) 
(9.13) 
and similarly 
E{Y¿)2/(EYn)2 
-> E(W2 | Xloo = X2oo = 0) 
oo 
= exp(-2Ai<51 - 2λ2<52 + £ λ«52); 
«=3 
combining these we obtain 
oo 
E(Y¿)2/(EY¿)2 -> e x p Q ^ Ó 2 ) = exp(-A1(5? - X2S2
2)EW2. 
(9.14) 
t=3 
For the Hamilton cycles in Theorem 9.23 we have ¿i = -2/(r - 1), δ2 = 0 
and λ! = (r - l)/2, and we see that (9.13) and (9.14) yield (9.7) and (9.8) 
from (9.5) and (9.6). 

PROOFS 
247 
9.4 
PROOFS 
We prove the assertions in the previous section. 
Proof of Theorem 9.13. We begin by observing that, since Xioo € Po(A¿), 
E(l + * ) * - = βλ·(ΐ+ί.)-λ. 
= 
e\ih 
and E((l + Si)x^f 
= ε
λ·<1+ί·>2-λ· = 
eA.(2<s.+i?) 
T h u S i 
d e f i n i n g 
m 
W{m) 
= Y[{l + 
Si)Xi~e-XiSi, 
i 
^y(m) j s a product of independent variables with mean 1, and hence the se-
quence (W ( m ))~ = 1 is a martingale; cf. Section 2.4 and, for example, Chung 
(1974, Chapter 9). The martingale is L2-bounded because 
m
m 
oo 
E(W{m))2 
= Y[eXiSl 
= e x p Q T ) M ? ) -► exp(^A¿(5?) < oo as m -* oo. 
1 
1 
1 
Thus, by the martingale convergence theorem (see, e.g., Chung (1974, Sec-
tion 9.4)), the limit W = limm_+oo W ( m ) exists a.s. and in L2, with EW = 
Ππι,,,-,οο E W<m> = 1 and EW2 = e x p f e ~ A«5?). We observe for future use 
that 
oo 
m 
E\W -W{m)\2 
=EW2 
-E(Wm)2 
= exp(J2xiöi) 
" β χ ρ ( Σ ***?)· 
i 
i 
(9.15) 
In order to show that W φ 0 a.s., except when X^ 
> 0 for some i with 
Si = - 1 , let us break the product defining W into two parts W\ = Π Α ^ - Ι / 2 
and W2 = n¿j>-i/2· ^or ^ ι w e observe that although there may conceivably 
be infinitely many indices in I\ = {i : St < - 1 / 2 } , Condition (A3) implies 
that Ε £ Λ Xioo = £ Λ Xi < 4 ^ ; , XiSj < oo, and similarly £ Λ \λ>6'\ < °°· 
Hence there are a.s. only finitely many non-zero Xioo, ί € A, and W\ — 
e ¿-Ί ' ' Γ7/( (1 + 6i)x'°°, where the product really is a finite product which 
is positive unless some factor vanishes, that is, unless Xioo > 0 for some i 
with Si = - 1 . 
For W2 we define Si = -<5</(l + <5¡). * 6 J2 = {i : ¿i > - ± } , and note that 
Σ / 2 Ai<5^ < oo and thus the argument above shows that 
W2 = Y[(l + 
Si)Xi°°e-Xi*i 
h 
converges a.s. with W2 < oo. However, since (1 + ¿j)(l + Si) = 1, 
W2W2 = JJe-*«(««+*') = exp(-£M?/(l + *)) > 0, 

248 
RANDOM REGULAR GRAPHS 
so W2 > 0 a.s., which completes the proof that W > 0 a.s. except when some 
factor vanishes. 
■ 
Proof of Theorem 9.12. For this proof we make, without loss of generality, two 
simplifying assumptions. First we may assume E Yn = 1. Secondly, we invoke 
a theorem by Skorokhod (1956) which implies that although the variables Xin 
and Yn originally may be defined on different probability spaces for different 
n, and X¿oo on yet another probability space, we may replace them by other 
variables having the same distributions, such that they become defined on a 
single probability space and the convergence in (Al) holds a.s., i.e. X¿„ -* X¿oo 
a.s. as n -> oo for each i. 
Fix a large integer m and define the functions 
/ n ( l l , . . . , X m ) = E(V n | X\n 
= Xl,...,Xmn 
= Xm) 
m 
/oo(xi,-.,im)= Hm / n(xi,...,x m) = TT(1 + ¿ 0 I i e - M \ 
η - κ » 
χ χ 
1 
where we used Assumption (A2) (in the form (A2')). Consider the random 
variable 
γΛ"») _ E(yn | Xlnt. . . ,Xmn) = fn(Xln> ■ ■ -,Xmn)-
By our simplifying assumption Xin -* Xioo a.s., we have a.s. Xin = Xioo for 
all i < m and sufficiently large n; thus 
lim Yn
m) 
= 
lim / „ ( X i o o , · · · , Xmoo) 
= / c o p l e o , · · · , *moo) = W<"° 
a.S., 
n-+oo 
n-»oo 
(9.16) 
with lV(m) as above. Hence, by Fatou's lemma 
m 
liminf E(y„<m))2 > E(W<m>)2 = e x p í V A<¿?V 
n-»oo 
i"' 
Consequently, since Yn 
is a conditional expectation of Yn, 
limsupE|y„ - Y^\2 
= limsup(Eyn
2 - E(Y<im))2) 
n-*oo 
n—»oo 
oo 
m 
< e x p ( £ \{δή - ex P(]T λ<<5,2). 
(9.17) 

PROOFS 
249 
Using Chebyshev's inequality (1.2). (916), (9.17) and (9.15), we now ob-
tain, for every ε > 0, 
HmsupP(|y„-W| >3ε) 
n—*oo 
< limsupP(|y„ - Vi m )| > e) + limsupP(|n¡ m ) - Wím)\ 
> ε) 
n-+oo 
π-+οο 
+ W(\W{m) 
-W\>e) 
< ε'2 lim sup E \Yn - Y^m)\2 + 0 + ε~2 E \W - W<m>|2 
n-*oo 
oo 
<2e~2 
e x p ( 5 > < 5 2 ) - e x p ( £ M 2 ) 
(9.18) 
We now let m -> oo, keeping e fixed. The right-hand side of (9.18) tends to 
0, so the left-hand side, which does not depend on m, has to vanish for each 
e > 0, which proves Vn A W. 
Finally, by (A4) and Theorem 9.13, EY¿ -+ exp(£¿ A,¿,2) = EW2, 
and 
this, together with the already established convergence in distribution, implies 
that the random variables Y„ are uniformly square integrable. 
■ 
Proof of Theorems 9.19 and 9.23. Most steps are the same for the random 
graphs and multigraphs, so we do both theorems together. In the proof we 
let i > 1 in the multigraph case but i > 3 in the graph case. 
The assertion (Al), with λ* = ^j(r - 1)', is Theorem 9.5 and Corollary 9.6. 
For (A2) we use Lemma 9.17. The computation of factorial moments is 
done in Robinson and Wormald (1992) (at least for G(n, 3)), using generating 
functions to keep track of the different possibilities for intersections of cycles. 
Here we will give a slightly different argument, using matrices. We restrict 
ourselves for the time being to random multigraphs, which (sometimes) are 
easier to handle than graphs. 
The measure Qn in Lemma 9.17 hasin this case the following interpreta-
tion. Consider the set Ω„ of all pairs (G, H) of a configuration G (on the nr 
half-edges [n] x [r]) and a set H of edges in G that projects to a Hamilton cycle 
in the multigraph G" obtained by projecting G. Pick one of these pairs (G, H) 
at random, uniformly over Ω„, and take the projection G*. This defines a ran-
dom r-regular multigraph which has distribution Q„, since the probability of 
obtaining a_ specific multigraph G* is proportional to the number of pairs 
(G,H) € Ω projecting to G*, which is proportional to H(G*) (Exercise!). 
Furthermore, by symmetry, we obtain the same distribution Qn by picking 
the pair (G, H) at random in the subset Ωη of all such pairs (G, H) such that 
the edges in H join half-edges in V x {1,2}. But these pairs are just the pairs 
(ΗυΗ', H), where H is a configuration on [n] x {1,2} projecting to a Hamil-
ton cycle and H' is any configuration on [n] x {3,... ,r}. Picking an element 
in Ω„ at random is thus the same as picking such H and H' independently 
at random, and after projecting we see that Qn equals the distribution of the 

250 
RANDOM REGULAR GRAPHS 
union of a random Hamilton cycle and a random (r - 2)-regular multigraph 
G*(n,r — 2) (both defined on the same vertex set [n]). 
We will, for simplicity, only verify 
EQ„ Zkn -► μλ 
a s n - > oo, 
(9.19) 
for a suitable μ*; the extension to mixed higher factorial moments is routine, 
following the argument in the proof of Theorem 9.5 (Exercise!). 
Fix k > 1 and assume n > k. There are ^(n)k 
~ ^nk fc-cycles in the 
complete graph on [n]. For each such cycle C we fix an orientation and label 
the edges by 1,..., k, starting at an arbitrary edge. Consider a configuration 
HUH', 
where as above H is a configuration on Wt = [n] x {1,2} projecting 
to a Hamilton cycle and H' is any configuration on W2 = [n] x {3,... ,r}. If 
C is a set of Jfc edges in H U H' that projects to C, we say that an edge in C 
is of type 1 if it belongs to H and type2 if it belongs to H'. Let us calculate 
the expected number of such "cycles" C in H U H' that project to C and have 
edges of types ¿ ι , . , . , ύ , where {ij)k € {1,2}* is a given sequence and the 
edges in C are ordered according to the ordering in the projection C. 
Let us say that the vertex in C between edges f and / + 1 has type tjii+i. 
There are two choices of half-edges in C at a vertex of type 11 (the two possible 
orderings of the two corresponding half-edges in Wi), at a vertex of type 12 or 
21 there are 2(r - 2) choices (one half-edge from W\ and one from Wi), and 
at a vertex of type 22 there are (r - 2)(r - 3) choices (two different half-edges 
from Wi). Hence, if b,j is the number of choices of the two half-edges in C at a 
vertex of type ij, we have 6U = 2, 6j2 = 621 = 2(r-2) and 622 = (r — 2)(r — 3). 
Each choice of half-edges over all k vertices specifies C completely, and it 
remains to compute the probability that this C actually is a subset of 
HUH'. 
Note that if there are fci indices I with i/ = 1, and thus k2 = k — k\ with 
¿i = 2, then each choice of half-edges specifies k\ edges in H and k2 in H'. The 
probability that the latter fc2 occur is, by Lemma 9.4(h), ~ ((r - 2)n) -* 2. For 
H we observe that there are C{n) = 2nn!/2n = 2 n - 1(n - 1)! configurations 
that project to a Hamilton cycle. If we specify k\ edges such that their 
projection is a union of paths, then the number of configurations containing 
these edges is C(n - ki) = 2 n -* 1 _ 1(n - k\ — 1)!, since we may contract these 
edges and obtain a one-to-one correspondence with Hamilton cycles on a set 
of n — ki vertices; hence the probability that H contains the given set of edges 
is asymptotically equal to (2n)~ki. 
On the other hand, a set of edges on Wi 
whose projection contains a cycle of length smaller than n is never a subset 
OÍH. 
Since H and H' are independent, it follows that the expected number of 
"cycles" C in H U H' that project to C and have edges of types ¿1,... ,i* is 
asymptotically given by, letting ik+i = ¿1, and defining ri = 2, τ2 = r - 2, 
k 
k 
Π*Μ,+,(2η)-*«((Γ - 2)n)-k> = n"* Π ^ , + , ^ 1 . 
1=1 
1=1 

PROOFS 
251 
provided kz > 0; in the remaining case ¿i = - · - = ** = 1, the expected number 
is 0. If we set a¿j = bi}/ri, sum over all sequences (ij)f and multiply by the 
number ¿ ( " ) * of fc-cycles C, we obtain 
( t , ) # ( l , . . . , l ) / = l 
( i / ) ' = l 
This equation is of the form (9.19), and we proceed to evaluate the right-hand 
side. We introduce the matrix 
A = M=Q I-i) 
(921) 
and can rewrite (Exercise!) (9.20) as 
E Q „ Z f c n - > i ( T r ¿ * - l ) . 
(9.22) 
It is easily verified (Exercise!) that the matrix A has eigenvalues r — 1 and —1, 
for example by solving the characteristic equation (1 — A) (r—3—A) — 2(r - 2) = 
0; hence TrAk = (r- 
1)* + (-1)*, and we have verified 
EQ„ Zkn -* μ* 
with 
μ* = 2ΐ((Γ"1)* + (" 1 )*~ 1)· 
( 9 2 3 ) 
As claimed above, (A2'") follows similarly (Exercise!), and (A2) follows by 
Lemma 9.17. 
With this value of μ*, we obtain 
* 
A* 
( r - 1 ) * 
l o 
Jkeven. 
( 9' 2 4 ) 
(It is the fact that <5* vanishes for even k that makes the number of Hamilton 
cycles asymptotically independent of the even cycle counts. The calculation 
above gives no intuitive explanation for this phenomenon.) 
Having verified (A2) and thus (A2') for the multigraph case, we note that 
the graph case follows immediately by specializing to xi = x2 = 0, using, in 
particular, 
EHn 
= E ( g ; | Z l B = Z 8 n = 0) 
Al<1_Aa4a 
= 
EH* 
EH* 
which we obtained in a different way in Remark 9.24. 

252 
RANDOM REGULAR GRAPHS 
The values of A¿ and á¿ yield 
¿ A,6f = Σ ?(r - I)- = log(l + ^ ) - log(l - -L·) 
1 
i odd 
and thus (for the graph case) ££° A¿<52 = log ^ 2 - ?τϊ· Hence (A3) holds, 
and the variance condition (A4) says 
ΕΗ%/(ΕΗη)2^-!— e-2/('-i) 
(9.25) 
r — 2 
in the graph case (this is (9.8)), and in the multigraph case 
EH'2/(EHtf->^-¿. 
(9.26) 
The latter result was stated in Lemma 9.10, which is proved below; this lemma 
thus completes the verification of the conditions in Theorem 9.12 for the 
multigraph case, and Theorem 9.23 follows. 
The graph case, Theorem 9.19, follows from Theorem 9.23 by conditioning 
as we observed in Remark 9.24. Alternatively, Theorem 9.23 implies (9.25) 
by Remark 9.25, which verifies (A4) for the graph case; thus Theorem 9.19 
follows by Theorem 9.12 (Exercise!). 
■ 
Remark 9.26. For r = 3, (9.25) is proved directly in Robinson and Wormald 
(1984). Robinson and Wormald (1992) use this case and they state the result 
for r > 4 (when the proof is more difficult) in Robinson and Wormald (1994). 
It remains to prove the variance estimate Lemma 9.10; we follow essentially 
Frieze, Jerrum, Molloy, Robinson and Wormald (1996). (The estimate follows 
by a long, and not very illuminating, calculation. It would be desirable to 
find another simpler, or more conceptual proof.) 
Proof of Lemma 9. JO. We first obtain an exact formula for E(/i*)2 by a com-
binatorial argument, similar to the argument used above in the proof of (9.20). 
This formula ((9.28) below) is rather complicated, however, and a non-trivial 
analytic argument then is needed to complete the proof. 
Consider the family of all pairs (Η,Η') of two n-cycles on W — [n] x [r], 
that is, two sets of edges on W that project to Hamilton cycles on [n]; if G is 
a random configuration, then 
E(#;) 2= £ 
P ( ^ U H ' C G ) . 
Η,Η' 
If the intersection H Γ\Η' consists of b edges on W, then, by (9.1), 
?{H U H' C G) = p2„-t = (rn - 4n + 26 - l)!!/(rn - 1)!!. 

PROOFS 
253 
Hence, if o„ is the number of «-cycles H on W, given by (9.2), and N(b) is 
the number of n-cycles H' that intersect a given H in b edges on W (this 
number is clearly independent of H), then 
E(H¿)2 = ^anN(b)p2n-b. 
(9.27) 
b=0 
If 6 = n, then JV(6) = 1 [Η' = H). 
If 1 < b < n, we decompose Λ/(6) = Σα=ι ^(¿>,α), where N(b,a) is the 
number of n-cycles //' such that the 6 edges on W in ΗΠΗ', 
when projected 
to [n], form a disjoint paths. (These paths are subpaths of the Hamilton cycle 
H on [n] obtained by projecting H.) 
To compute N(b, a), consider first the number of ways to choose the a paths 
in H. Fix an orientation of H. Choosing an initial vertex which begins one 
of the paths, we then can choose the successive lengths of the paths in (*Zj) 
ways, and the lengths of the gaps between them in (n~J[ *) ways. Further, 
there are n possible initial vertices, and a of them can be used for each set of 
a paths. Consequently, the number of sets of a paths with total length b is 
n /b - 1\ in - b - 1\ _ 
an 
(b\ in - b\ 
a \a - l) V a - 1 ) ~ b(n-b)\a)\ 
a ) ' 
Next, by collapsing the a paths into single vertices, we see that each oriented 
Hamilton cycle H' on [n] containing these paths defines an oriented Hamilton 
cycle on n — b vertices and, furthermore, an orientation of each of the a paths; 
this yields a bijection, and thus the number of such oriented Hamilton cycles 
H' equals 
2 ° ( n - 6 - l ) ! 
Finally, each such H' corresponds to 
(r - 2)n+a-b(r 
- 3)"-«-» 
oriented n-cycles on W, since the half-edges may be chosen in (r - 2)(r - 3) 
ways at each of the n - a - b vertices disjoint from the given paths, and in 
r — 2 ways at each of the 2a endpoints of the paths. Ignoring the orientation 
yields an additional factor \ , and multiplying the factors together we obtain 
The same formula holds for the case 6 = 0 too, in which case we set a = 0 
and N(0) = N(0,0), provided we interpret a/b as 1. 

254 
RANDOM REGULAR GRAPHS 
Consequently, we obtain from (9.27) and (9.4) the formula 
E(//;)2 
1 
v 
N{b,a)p2n-h 
(E"¿)2 
VH¿ 
0<¿ < n 
Wl 
= E ^ + 
Σ 
5 ^ 2 · Γ - - ( Γ - 1 ) - - ( Γ - 2 ) ^ · - » ( Γ - - 3 Γ - · -
n 
0<o<Kn 
rn-1)!! 
112 
(9.28) 
/ 6 \ /n - b\ (n - 6)! (rn - 4n + 26 - l)!!(n 
X W \ 
a / 
n! 
(rn - 2n - l)!!2 
The second step of the proof is to estimate this double sum. 
First consider the case r = 3. In this case, because of the factor (r—3)n_°-6 
in (9.28), we only have to consider terms with a + b = n, and (9.28) simplifies 
to 
E(#; 2) 
_ 1 _ 
ψ 
η2 
(η-α)!(η-2α-1)!!(3η-1)ϋ 
(EH·)2 
Efl; 
¿ f l ( n - a) 
(n - 2a)!n! (n - l)!!2 
(9.29) 
Using Stirling's formula, the terms in the sum can be written 
* 
, 
nl 
β"'<°/">(ΐ + θ( 
1—Λ), 
(9.30) 
ν^2πη ay/rT^äy/n - 2α + 1 
V 
V n - 2 a + l / / ' 
where 
g(x) = i log 2 - log 6 + (1 - i) log(l - x) - ±(1 - 2x) log(l - 2x) + § log 3. 
Hence the sum is dominated by the terms where g(a/n) is close to the maxi-
mum of g on [0,1/2]. We differentiate and find 
g'(x) = log 2 - log(l - x) + log(l - 2x), 
and thus the equation g'(xo) = 0 has the unique root xo = 1/3 in (0,1/2). 
Moreover, 
and it follows that xo is the unique maximum point of g in the closed interval 
[0,1/2]; the maximum value is thus 
g(x0) = | log 2 - log 6 + § log | - i log i + § log 3 = 0, 
and, consequently, g(x) < 0 for x φ XQ. 

PROOFS 
255 
Since g(xo) = £r'(xo) = 0 and ^"(xo) < 0, a Taylor expansion shows that if 
6 > 0 is small enough, then for some ci > 0 
g{x) < -ci(x - xo)2, 
\x - xo\ < 2*. 
(9.31) 
Furthermore, the set [0,1/2] \ (x0 - δ, x0 + δ) is compact, and g is continuous 
and negative there; hence, for some ci > 0, g(x) < -C2 when |x - xo| > δ. It 
follows that the terms in (9.29) with \a/n - x0| > δ are exponentially small, 
and rewriting the sum of the remaining terms as an integral, we obtain from 
(9.29) and (9.30) 
§ f £ i = -±= 
r(X0+6) 
fc»(l*j/«)e"<W/-> 
A + o(l) 
(EH*y 
V2nn 
J„(X0-s) 
where, for |x - xo| < 2δ, 
Λ»(Χ) = 
" 
0 = = ( l + O(-)) = h(x)+o(-), 
nxy/n - nxy/n - 2nx + 1 V 
\n// 
W 
with 
h{x) = 
* 
x v l - x v l —2x 
The variable substitution t = nxo + y %/rä yields 
For fixed y, Λη(χ0 + y/y/ñ + 0{l/n)) 
-»· Λ(χ0) = 2 _ 1 / 29 and, by a Taylor 
expansion using g{xo) = <?'(xo) = 0 again, 
ng{x0 + y/y/H + 0(l/n)) = \g"{xo)y2 + o(l). 
Consequently, the dominated convergence theorem, justified by (9.31), yields 
( E J ^ ) » ^ ^ . / - « , 
l °' 
d y 
= Μ Χ Ο ) | 5 " ( Χ Ο ) Γ 1 / 2 = 3, 
as asserted by (9.26). 
The proof for r > 4 is similar, but requires a more complicated two-variable 
analysis, and we omit some details. First, using Stirling's formula, the terms 
in the sum in (9.28) may be written (Exercise!) 
-Lh(a/n,b/n)en<>^n'b^(l 
+ 0(^-— 
1 
- — - ) ) , 
2πη 
V 
\mm{a,b-α,η 
- a-b)+ 1// 

256 
RANDOM REGULAR GRAPHS 
where 
g(x,y) = z l o g 2 - l o g r - l o g ( r - 1) + (1 + x - y) log(r - 2) 
+ (1 - x - y) log(r - 3) + y logy + 2(1 - y) log(l - y) 
- (y - x) log(y - x) - 2x logx - (1 - x - y) log(l - x - y) 
+ (§ - 2 + y) log(r - 4 + 2y) + § logr - (r - 2) log(r - 2) 
and (with a minor modification when x = 0,x = yorx-t-y = l) 
Hx,y) = y~1/2(l - yyl(y - *Γ1/2(1 - x - ί/Γ1/2· 
The partial derivatives of g are 
^ 
= log2 + log(r - 2) - log(r - 3) + log(y - x) - 2 logx + log(l 
-x-y), 
-ξ- = - log(r - 2) - log(r - 3) + logy - 2log(l - y) - log(y - x) 
+ log(l - x - y) + log(r - 4 + 2y). 
It is easily verified (Exercise!) that (xo,yo) = (2(r - 2)/r(r - l),2/r) is a 
critical point of g, and routine calculations show that g(xo,Vo) — 0 and that 
the Hessian matrix D2g of second derivatives is negative definite at (xo,yo), 
so that g has a local maximum there, and Det(D2g(io,yo)) = r 3(r-l) 2/4(r-
2)(r-3). Moreover, with further effort (Frieze, Jerrum, Molloy, Robinson and 
Wormald 1996), it can be shown that (xo,yo) is the unique global maximum 
point of g in the domain {(x,y) : 0 < x < y < l - x } . 
The proof is now completed similarly to the case r = 3. Substituting first 
a = [xn\, b — \_yn\ and then x = xo + ζι/y/ñ, y = y0 + ζ%Ι\/η, one finally 
obtains, arguing as above, 
E(H;2) 
(EH·)* 
= 
h(xQ,yo)Oet(D2g(xo,yo)) 
1 
roo / , ~ h ^ o ^ j e _ i ( l l l Z a ) D » e ( x e f l w ) ( , l l X > ) . j k i t f a j 
2π y.«, J_oo 
r - 2 ' 
which verifies (9.26). 
■ 
Remark 9.27. A similar, but even more complicated, variance calculation 
for the number of cycles of length i(n), where l(n) is a given sequence with 
l(n) ->■ oo but l(n) <n (with some margin), is given by Garmo (1999). 
9.5 CONTIGUITY OF RANDOM REGULAR GRAPHS 
We will in this section study different models of random regular (multi)graphs. 
Our main objective is to show that in many cases, different constructions yield 
random (multi)graphs that have distributions that are contiguous in the sense 

CONTIGUITY OF RANDOM REGULAR GRAPHS 
257 
of Section 9.6; thus, although the distributions are different, they are not too 
different. 
(More precisely, two contiguous random objects have the same 
qualitative properties; anything that holds a.a.s. for one of them holds o.a.5. 
for the other, but quantitative properties such as (asymptotical) probabilities 
may differ. A formal definition of contiguity is given in Section 9.6, together 
with some properties used below.) 
We denote contiguity of two random (multi)graphs G« and G« by G„ 
« 
Gn ■ Recall that this is an asymptotic property as n -* oo. 
Besides G(n,r) and G*(n,r) defined earlier, we will in this section also 
study the intermediate G'(n,r), 
which we define as a random regular multi-
graph G*(n,r) conditioned to have no loops; thus G'(n,r) may have multiple 
edges but no loops. 
Moreover, we let H(n) denote a random Hamilton cycle on [n]; this is a 
random 2-regular graph, but it obviously has a different distribution than 
G(n, 2) or G* (n, 2) (except for some very small n). 
We will also study multigraphs obtained by adding several random regular 
(multi)graphs; we use the notation + for the union of independent random 
(multi)graphs on the same vertex set [n]. Note that the union of simple 
graphs, in general, is not a simple graph, because double edges may arise. 
Hence we also define the "simple sum" Θ by defining Gi θ G2 to be Gi + G2 
conditioned on being simple. 
Example 9.28. G(n,1) -I- G(n, 1) + G(n, 1) denotes the random 3-regular 
multigraph obtained by taking independently three perfect matchings at ran-
dom, while G(n, 1) © G(n, 1) φ G(n, 1) denotes the random 3-regular graph 
obtained by taking three disjoint perfect matchings at random. 
After these preliminaries, we begin by observing that we have already stud-
ied H(n) + G* (n, r - 2) in the proof of Theorem 9.23, where the distribution 
of this multigraph was denoted by Q„. If we let Pn denote the distribution of 
G* (n, r), then dQn/dPn 
= E'J E H', and Proposition 9.49 and Theorem 9.23 
show that P„ and Qn are contiguous, provided the limit W in Theorem 9.23 
satisfies EW = 1 and W > 0 a.s. By Theorem 9.13, EW = I always holds, 
while W > 0 a.s. if and only if <$< > - 1 for all i; since á¿ = -2(r - l) -*, this 
holds for r > 4 but not for r = 3, in which case ¿i = — 1. We have proved the 
following. 
Theorem 9.29. // r > 4, then the random multigraphs M(n) +G*(n,r - 2) 
and G* (n, r) ore contiguous. 
■ 
For r = 3, Theorem 9.29 does not hold; this can be seen directly because 
Μ(η) + G* (n, 1) never contains any loop, while G* (n, 3) contains loops with 
positive limit probability. Indeed, this is the only obstacle, and we have the 
following substitute when r = 3. 
Theorem 9.30. The random cubic multigraphs H(n) + G* (n, 1) and G' (n, 3) 
are contiguous. 

258 
RANDOM REGULAR GRAPHS 
Proof. Let P„ be the distribution of G' (n, 3) and Qn the distribution of H(n) + 
G*(n,l). Then dQn/dPn 
= H'/EH', 
where H' = if(G'(n,3)), the number 
of Hamilton cycles in G'(n,3). It follows from Theorem 9.23 by conditioning 
on Zi„ = 0, using the same argument as in Remark 9.24, that 
K ä w ■=n(i-lfv"· 
n 
«>2 
i odd 
Since this limit variable satisfies W > 0 a.s. and E W = 1, the result follows 
by Proposition 9.49. 
■ 
If we condition on the multigraphs being simple in the last two theorems, 
using Proposition 9.50(i), we obtain the following corollary for graphs. (Note 
that r = 3 is no longer special.) 
Corollary 9.31. // r > 3, then H(n)®G(n,r-2) andG(n,r) 
are contiguous. 
In particular, taking r — 3, we see that Hl(n) Θ G(n, 1) and G(n, 3) are 
contiguous. Since G(n, 1) is a perfect matching, and thus W(n) Θ G(n, 1) by 
definition always contains a perfect matching, it follows that G(n,3) a.a.s. 
contains one too. This gives another proof of Corollary 9.21 for r = 3. A 
similar argument and induction yields Corollary 9.21 for any odd r > 3. 
We proceed to study more general unions, and begin with the small cycles 
in them. 
Theorem 9.32. Let G = Gi H 
+ G m, where m > 1 and G i , . . . , Gm are 
independent random regular (multi)graphs such that G¿ is a copy of either 
G*(n,ri) (with r¿ > 1) or ttfl(n); in the latter case we let r¿ = 2. Let r = 
Γι + ··· + rm; thus G is an r-regular multigraph. 
Let further m// be the 
number of M(n) among the G,·. Then 
Zk(G) A Ρο(μ*), 
os n -* oo, 
jointly for all k > 1 with independent limits, with 
_ (r - l)* + (m-!)(-!)*-m„ 
μ*- 
2Jfc 
' ^
1
· 
Proof. We argue as we did for the special case H(n) + G*(n,r - 2) in the 
proof of Theorem 9.23. We now have m types and let 6<j = r^j when i ¿ j , 
bu = ri(ri - 1); thus dj, = ¿»¡¿/r-j = r¿ - 5<;·, where <$<j is Kronecker's delta. 
We have to exclude the cases when all edges in the cycle are of the same 
type and this type corresponds to a graph H(n), and we obtain as before 
Z(G) -¥ Ρο(μ*), jointly for all k, with 
μ4 = 1 ( Τ Γ ( Λ * ) - τ η „ ) . 

CONTIGUITY OF RANDOM REGULAR GRAPHS 
259 
Here A is the m x m matrix (α 0)^· = 1 = (r, - ¿ο)Γ>=ι· Thus A + I is the 
matrix (rj)™J=1, which has rank 1 and therefore m - 1 vanishing eigenvalues; 
the last eigenvalue has to equal Tr(.4 + /) = r. Hence, A has the eigenvalues 
r — 1 and —1 (with multiplicity m - 1), and 
Tr(i4*) = ( r - l)* + ( m - l ) ( - l ) * . 
■ 
Before proving further instances of contiguity, let us look at a counter-
example. 
Example 9.33. Consider the three different 2-regular multigraphs G* (n,2), 
H(n), and G(n, 1) + G(n, 1). Theorem 9.32 shows that in all three cases, 
Zjfe -> Po(/ifc) (jointly for all k), with 
' ¿ , 
^ 
forG*(n,2), 
*** = \ i4r^. 
for G(".!) + Gi»- *). 
k0, 
for H(n). 
It follows that the conclusion of Corollary 9.54 is violated for any pair of these 
three random 2-regular multigraphs; hence no two of them are contiguous. 
This remains true if we condition on the multigraphs being simple, since 
we then still have Z¡. -> Ρο(μ*) for k > 3, which is just as good (or bad) for 
the application of Corollary 9.54. For example, G(n, 2) and G(n, 1) φ G(r», 1) 
are not contiguous. 
We will see below (Theorem 9.43) that such counter-examples occur only 
for 2-regular multi-graphs; in all instances of Theorem 9.32 with r > 3, G is 
contiguous to either G* (n, r) or G' (n, r). 
One conceivable method to prove this result is to follow the proofs of 
Theorems 9.29 and 9.30. We thus try to apply Theorem 9.12 with Xin 
= 
Ζ4(0*(η,Γ)) and Yn equal to the number of decompositions of the random 
multigraph G*(n,r) as a union G\ + ■ ■ ■ + Gm, where G¿ is any r¿-regular 
multigraph when G< is of the type G* (n, r¿), and G< is a Hamilton cycle when 
Gj is of the type H(n). The measure Qn in Lemma 9.17 then is the distribution 
of G. 
Condition (Al) holds by Theorem 9.5, with λ* = ±{r - 1)*, and (A2') 
holds by Theorem 9.32, with μfc = ^ ( ( r - 1)* + (m - 1)(-1)* - mH). 
Hence 
fc-ÄT1- 
(ΓΓΪ)5
 
( 9 ·
3 2 ) 

260 
RANDOM REGULAR GRAPHS 
and 
hth'h 
^^ 
= - l ( ( m - l) 2 + m2
H) log(l - - i - j ) + (m - l)m„ log(l + 
^
) 
= -±((m - l) 2 + m2
H) log(r - 2) + (m - l)m„ log(r) 
+ ¿ ( m - l - m H ) 2 l o g ( r - l ) . 
Consequently, assuming r > 3, (A3) holds, and (A4) says 
EJ/2 
/r _ j\(m-l-mH)I/2r(m-l)mH 
(EV„)2 ~* 
(r _ 2)«"-i)2+mi)/2 
(9.33) 
d 
If (9.33) holds, then Theorem 9.12 yields Yn/EYn 
4 W for a certain 
random variable W with EW = 1. It is easily seen that μ* > 0 and thus 
ok > - 1 for all Jfc > 1, except when k = 1 and all constituents Gi are either 
G*(n, 1) = G(n, 1) or M(n); in this case loops clearly are impossible. Except 
in the loopless case we thus have W > 0 a.s., and Proposition 9.49 shows that 
G and G* (n,r) are contiguous; in the loopless case we condition on X\n = 0, 
and obtain as in the proof of Theorem 9.30 that G and G' (n, r) are contiguous. 
In other words, we have shown that contiguity holds in all cases when 
(9.33) holds. While it seems quite difficult to prove (9.33) in general by present 
methods, it has been verified in a number of special cases by arguments similar 
to the proof of Lemma 9.10 above. (Some of the references below actually treat 
only the corresponding results for G(n,r), but then the case G*(n,r) can be 
shown by similar methods.) 
We begin with the simplest case. 
Theorem 9.34. // r > 3, then G* (n, r - 1) + G* (n, 1) « G* (n,r). 
Proof. In this case, the number Yn of decompositions equals the number of 
perfect matchings in G*(n,r), and the condition (9.33) is 
EY' 
Ar-D* 
( 9.3 4 ) 
(Eyn)2 
(r -2)i/2 · 
This has been proved by Bollobás and McKay (1986), which completes the 
proof. 
■ 
Another case is the following theorem, proved by Robalewska (1996). 

CONTIGUITY OF RANDOM REGULAR GRAPHS 
261 
Theorem 9.35. // r > 3, then G* (n, r - 2) + G* (n, 2) » G* (n, r). 
Proof. This is proved as Theorem 9.34; we now have to verify 
EV* 
(r - l ) ' / 2 
( E r n ) 2 ~ * ( r - 2 ) 1 / 2 ' 
(9.35) 
where Vn is the number of 2-factors of G*(n,r), that is, the number of 2-
regular submultigraphs. This can be verified by long calculations similar to 
those in the proof of Lemma 9.10; we omit the details (Robalewska 1996). 
■ 
As a simple application, we obtain the following theorem which gives partial 
justification for the intuitive feeling that increasing the degree of a random 
regular graph adds edges and makes it easier to, for example, find a specified 
subgraph. (Cf. Section 1.3, where other models of random graphs are covered.) 
Theorem 9.36. Let 2 < r < s. 
(i) Any increasing property that holds a.a.s. for G*(n,r) holds a.a.s. for 
G*(n,s) too. 
(ii) Any increasing property that holds a.a.s. for G(n,r) holds a.a.s. for 
G(n, s) too. 
Proof. First consider even n only. Any increasing property that holds a.a.s. 
for G*(n,r) holds a.a.s. for G*(n,r) + G*(n, 1) too; hence it holds a.a.s. also 
for G*(n,r + 1) by Theorem 9.34. The assertion (i) follows by induction. For 
odd n, r and s have to be even, and we argue using Theorem 9.35 instead. 
The statement (ii) is proved similarly, using G(n, r) Θ G(n, 1) « G(n, r +1) 
and G(n,r) θ G{n, 2) « G(n,r + 2), which follow from Theorem 9.34 and 
Theorem 9.35 by conditioning. 
■ 
Remark 9.37. Theorem 9.36 is not true for r = 1. For example, G* (n, 1) = 
G(n, 1) contains always a perfect matching (in fact, it is one), while G*(n,2) 
or G(n, 2) contains a perfect matching only if it contains no cycles of odd 
length, and it follows easily from Theorem 9.5 that the probability of this 
tends to 0. 
Remark 9.38. Unlike the simple monotonicity result in Lemma 1.10 for 
G(n,p) and G(n, M), it is, in general, not true that if V is an increasing prop-
erty and 2 < r < s, then P(G* (n,r) satisfies V) < P(G* (n,s) satisfies V). 
A counter-example is obtained if we take r = 2 , s = 3, n = 2 and let 
V be the property that the multigraph contains a multiple edge. 
Then 
P(G*(2,2) satisfies V) = 2/3 while P(G*(2,3) satisfies V) = 2/5. We do not 
know whether the corresponding statement for the random graphs G(n,r) 
and G(n,s) also may fail; nor do we know if the statement is always true 
asymptotically, taking limsup,,.^ of both sides. 

262 
RANDOM REGULAR GRAPHS 
Remark 9.39. The original proof by Robinson and Wormald (1992) that 
a Hamilton cycle a.a.s. exists in G(n,3) was by (a version of) the method 
used in the proof of Theorem 9.20 above. For G(n,r), r > 4 (Robinson 
and Wormald 1994), where the variance calculation as seen above is much 
harder, they preferred a slightly different method, establishing (implicitly) 
Theorem 9.34 and Theorem 9.36 for n even by the method above, which uses 
the easier variance estimate (9.34) for the number of perfect matchings. The 
case r > 4 then follows from the case r = 3 (with an extra argument when n 
is odd). 
We continue with further instances of contiguity. 
Theorem 9.40. G* (n, 1) + G* (n, 1) + G* (n, 1) ss G' (n, 3). 
Proof. This time the condition (9.33) is 
(Ey„)2 
*· 
where Yn is the number of partitions of the edge set of G* (n, 3) into three 
disjoint perfect matchings. This is verified by Janson (1995b) and Molloy, 
Robalewska, Robinson and Wormald (1997). 
■ 
Theorem 9.41. H(n) + H(n) » G'(n,4). 
Proof. This time (9.33) says 
(Eyn)2 
V i ' 
where Yn is the number of partitions of the edge set of G* (n, 4) into two 
Hamilton cycles. This is verified by Kim and Wormald (2000+). 
■ 
The theorems above are so far the only instances where (9.33) has been ver-
ified. Nevertheless, by combining them, we obtain contiguity in all remaining 
cases too. We begin with the case of G* (n, n ) + G* (n, r2). 
Lemma 9.42. If n > 1, r2 > 1 andri+r2 > 3, *AenG*(n,ri)+G*(n,r2) « 
Gm{n,ri +r 2). 
Proof. We may assume n > r2. The cases r2 = 1 and 2 are Theorems 9.34 
and 9.35. For Γχ > r2 > 3, we use induction on r2 and assume that the result 
is true for smaller r2 (and any π). 
We observe first that if Gi « G[ and G2 w G2, then Gx + G2 « G\ + G2; 
this is a consequence of Proposition 9.50(ii)(iii). Recall also that contiguity is 
an equivalence relation. 

CONTIGUITY OF RANDOM REGULAR GRAPHS 
263 
Hence, if rx > r2 > 3 and at least one of them is odd (which forces n even), 
using Theorem 9.34 twice and the induction hypothesis, 
G* (n, rO + G* (n,r2) » G* (n,n) + G* (n, 1) + G* (n,r2 - 1) 
% G*(n,r, + 1) + G*(n,r2 - 1) 
«G*(n,r! + r 2 ) , 
which verifies the induction step. If both Γγ and r2 are even (which allows 
odd n too), we argue similarly using Theorem 9.35 instead. 
■ 
Finally, we prove the full result. 
Theorem 9.43. Let G be a union as in Theorem 9.32, and assume that r > 
3. Then G is contiguous to G*(n,r) or G'(n,r) (the latter in the cases in 
which each G¿ is either G" (n, 1) or M(n) and thus loops are impossible). 
Proof. Consider first the case in which all constituents are G* (n, 1) or H(n). If 
there are constituents of both these types we begin by combining one G* (n, 1) 
and one H(n) using Theorem 9.30; if all are G* (n, 1), and there thus are r > 3 
of them, we combine three of them using Theorem 9.40; if all are H(n) we 
combine two of them using Theorem 9.41. In all three subcases this yields a 
new sum, contiguous to the original one, with one constituent G' (n, s) (s = 3 
or 4). Now we observe that by Theorem 9.34 and conditioning on no loops, 
using Proposition 9.50(i), G'(n,s) + G*(n, 1) « G'(n,s + 1) when s > 2; 
similarly by Theorem 9.29 and conditioning, G'(n,s) 4- H(n) ss G'(n,s + 2) 
when s > 2. Consequently we may absorb all remaining G*(n, 1) and M(n) 
one by one into the G'(n,s); eventually we reach G'(n,r). 
Secondly, if there is at least one constituent G* (n, r¿) with r¡ > 2, we 
may absorb all other constituents into it one by one using Lemma 9.42 and 
Theorem 9.29; eventually we reach G*(n, r). 
■ 
Corollary 9.44. Let G = Gi Φ ... Θ G m, where m > 1 and G i , . . . , Gm are 
independent random regular graphs such that G< is a copy of either G(n,r,) 
(with ri > 1) or H(n), in the latter case we let ri = 2. Let r — rx + 
h rm; 
thus G is an r-regular graph. If r > 3, then G ss G(n, r). 
Proof. By Theorem 9.43, conditioning on the multigraphs being simple; see 
Proposition 9.50(i). 
■ 
Remark 9.45. Another model (with a different distribution) of a random 2-
regular (multi)graph is obtained by taking a (uniformly distributed) random 
permutation π of [n] and drawing the n edges ι'π(ι). We denote this model by 
P(n). 
It is well known (and easily shown) that for this model, the cycle counts 
Zk -> Po(l/fc) (with independent limits); hence Corollary 9.54 shows that 
P(n) is not contiguous to any of the three 2-regular multigraphs considered 
in Example 9.33. 

264 
RANDOM REGULAR GRAPHS 
On the other hand, we conjecture that Theorem 9.43 can be extended to 
allow summands P(n) too. Indeed, by an argument similar to the proof of 
Theorem 9.29, P(n) + G* (n, r - 2) « G* (n, r) and by induction as above, the 
conjecture holds at least when there is one summand G*(n,rj). To settle the 
remaining cases, with unions of H(n) and P(n) only, it would suffice to show 
that M(n) + P(n) and P(n) + P(n) both are contiguous to G*(n,4), but this 
remains an open problem. 
It is not difficult to extend the small cycle count Theorem 9.32 to allow also 
mp summands P(n), with m// replaced by m« - mp in the formula for μ*. 
Hence condition (9.33) extends too, with the same modification, but it has 
not yet been verified for the relevant cases. (The right-hand sides of (9.33) 
are ,/Ζ/2 and 2?'22-13'2, respectively.) 
The model P(n) + ■ · ■ + P(n) of a random 2d-regular multigraph has occa-
sionally been used. A proof of the conjecture would imply that results, in the 
form of a.a.s. properties, that can be proved for this model hold for G* (n, 2d) 
and G(n, 2d) too. 
9.6 
A BRIEF COURSE IN CONTIGUITY 
We end this chapter with a discussion of contiguity in general. The purpose 
of this purely probabilistic section is to provide an easily accessible reference 
for some facts used in the preceding section. Readers who are either experts 
in probability theory or completely uninterested in it may skip this section. 
The notion of contiguity was introduced in statistics by Le Cam (1960); see 
also Le Cam (1969, 1986) and Roussas (1972). It is defined as an equivalence 
relation among sequences of probability measures on a sequence of space (Ω„). 
Definition. Let (Pn)i° and (C?n)i° be two sequences of probability measures, 
such that for each n, P„ and Qn both are defined on the same measurable 
space (Ωη,Τη)- We then say that the sequences are contiguous if for every 
sequence of measurable sets An C Ωη, 
lim P„(An) = 0 *=> lim Qn(An) = 0. 
TI—roo 
n-+oo 
There are many equivalent definitions of contiguity; some are given below 
and several others are given in the references given above. 
Contiguity is mainly used in statistics, but it seems to be a natural and 
useful property also in the study of random combinatorial structures. In 
that case, typically, Ωη is a (finite) set of some combinatorial objects of size 
n, Tn is the σ-field of all subsets of Ωη, and P„ and Qn are probability 
measures corresponding to two different ways of selecting an element of Ω„ 
"at random". In this connection, we also say that two random objects Xn 
and yn, depending on a parameter n, are contiguous if the corresponding 
sequences of distributions {C(Xn)) and (C(Y„)) are contiguous. (This entails 
that Xn and Yn take values in a common space Ωη.) 

A BRIEF COURSE IN CONTIGUITY 
265 
Several examples of contiguity for random regular graphs are given earlier 
in this chapter. Another combinatorial example was given by Winkler (1991), 
who showed that for two-dimensional partial orders, the uniform distribution 
is contiguous to the distribution of the intersection of two independent random 
linear orders. 
It is important to realize that contiguity is an asymptotic property of two 
sequences of probability measures; if we say that two probability measures (or 
distributions, or random models) are contiguous, we really mean that there 
is some parameter n (although perhaps not explicitly mentioned) that tends 
to infinity. If, as is often useful, we informally regard asymptotic results as 
statements about a fictitious infinite limiting model, then contiguity can be 
interpreted as mutual absolute continuity of the two probability measures. (In 
this context it may be observed that in the special case that Ωη, Τη, Pn and 
Q„ do not depend on n, contiguity reduces to mutual absolute continuity.) 
Note also that the definition says nothing about the rates of convergence of 
Pn(An) and Qn(An); 
these may be quite different. It is, nevertheless, possible 
to restate the definition in terms of estimates, which, however, use unknown 
functions to relate the rates. (In certain examples, it may, of course, be possi-
ble to replace these by explicit functions.) We given two such reformulations, 
which we think may be useful for a better understanding the contiguity con-
cept. 
Proposition 9.46. The sequences (Pn) and (Qn) are contiguous if and only 
if there exist a sequence εη -¥ 0 and a continuous function ψ : [0,1] -* [0,1] 
with φ(0) = 0, such that for every n and An 6 Tn 
Pn(An) 
<en + <p(Qn(An)) 
and 
Qn(An) 
<εη + 
φ(Ρη(Αη)). 
Proof. If (Pn) and (Qn) are contiguous, then for every ε > 0 there exist η(ε) 
and ¿(e) such that n > n(e) and P„(An) < δ(ε) => Qn(An) 
< ε, and similarly 
with Pn and Qn interchanged. The existence of (ε„) and φ now follows easily 
(Exercise!). The converse is obvious. 
■ 
For the next results, we recall that each Qn has a Lebesgue decomposition 
Qn = Qn+Qn 
where Qn is absolutely continuous with respect to Pn, while Qn 
and P„ are mutually singular, and that there exists a function dQn/dPn 
> 0 
(the Radon-Nikodym derivative) such that 
Qan(An)= f ~"dPn, 
Anern. 
JA„ 
arn 
(If Ω„ is countable (e.g., finite), the measures are given by probability func-
tions pn(w) and qn(w); in this case Q'n is the restriction of Q„ to the set 
{w : pn{w) = 0}, while dQn/dPn 
= qn(w)/pn(w) 
when pn(w) 
Φ 0 (and 
arbitrary otherwise).) 

266 
RANDOM REGULAR GRAPHS 
Proposition 9.47. The sequences (P„) and (Q„) are contiguous if and only 
if for every ε > 0 there exist n(e) and K{e), such that for every n > η(ε) 
there exists a set Bn 6 Tn with Pn{Bc
n) <ε andQn(Bc
n) < e, such that 
Kie)-1 < | 4 T T ^ K^ 
f°r M A» C Bn. 
(9.36) 
Remark 9.48. Here we may replace (9.36) by the equivalent condition that 
the restriction of Qn to Bn is absolutely continuous with respect to P„, with 
TO"1 
< ^ r < * (*) 
JVa.s. on Bn. 
Proof. Suppose that (P„) and (Qn) are contiguous. Fix a large number K, let 
Cn be a set such that Pn(C7„) = 0 = Q'n(Cn) (this is possible by the definition 
of singular measures), and define 
Dn = {«i € Ωη : dQn/dPn > K). 
Then 
P„(Cn U Dn) = ^ 
dPn < 1 | D ^ 
dPn = ¿Q n(D„) < ¿r. 
Hence Proposition 9.46 yields Qn(CnUZ?„) < en + yi(Ä'_1). Furthermore, for 
any An C (C„ U £>n)c, 
Qn(A„) = Qn(An) 
= | 
^jT<lPn< 
KPn(An). 
We similarly define CJ, and DJ, with the roles of P„ and Qn interchanged, and 
let Bn = {C„UDn\JCnöD'ny. 
Then Pn(B^), Qn(Bc
n) < AT"1 +ε„ + ν>(Α:-1), 
which is less than e for n > π(ε) if η(ε) and K = /C(e) are large enough. 
Conversely, if such sets Bn exist, then for large n and every A„ € Tn, 
Qn(An) < Q„(An Π Bn) + Qn(B<) < Κ(ε)Ρη(Αη) + ε. 
In particular, if Pn{An) -* 0, then limsupQn(An) < ε, for every ε > 0, and 
thus Qn(An) -4 0. By symmetry, we obtain also the converse implication and 
thus the sequences are contiguous. 
■ 
For the next result, we consider for simplicity only the case in which 
dQn/dPn converges in distribution. 
Proposition 9.49. Suppose that Ln = dQn/dPn, regarded as a random vari-
able on (Ω„, TmPn)> converges in distribution to some random variable L as 
n -> oo. Then (P„) and (Qn) are contiguous if and only if L > 0 a.s. and 
EL = 1. 

A BRIEF COURSE IN CONTIGUITY 
267 
Proof. Suppose that (F„) and (Qn) are contiguous. Let ε > 0, and let Α*(ε) 
and Bn, n > η(ε), be as in Proposition 9.47. In particular, by Remark 9.48. 
Κ(ε)'' < Ln < Κ(ε) on Bn. Thus 
P(L = 0) < P(L < Κ(ε)~ι) < limsupPn(Ln < Κ{ε)~1) 
n-»oo 
< HmsupPn(B£) <ε. 
n—>oo 
Moreover, since L„ Λ Κ(ε) -> Ι Λ Α'(ε) as n -+ oo, dominated convergence 
yields 
E L > E(L Λ Α"(ε)) = lim / 
L„ Λ Α'(ε) dPn 
> limsup / 
LndP„ = limsupQ„(£„) > 1 — e. 
n—»oo 7β„ 
n—»OO 
Since ε is arbitrary, and by Fatou's lemma EL < liminf EL n = 1, this shows 
that P(L = 0) = 0 and EL = 1. 
Conversely, suppose that P(L = 0) = 0 and EL = 1, and let ε > 0. There 
exists <5 > 0 such that P(L < δ) < ε; we may furthermore choose δ such that 
δ < 1 and P(L = δ) = 0. Then Pn(Ln < δ) -+ P(L < δ), and thus Pn{Ln < 
δ) < ε for large n, which also yields Q%(Ln <6) = JL <s Ln dP„ < εδ < ε. 
We can also find K > l/ε such that E(LΛΛ*)>1-ε and thus 
lim í(LnAK)dPn 
= 
E(L/\K)>l-£. 
n-HsoJ 
Hence, for large n, say n > η(ε), 
1 - / 
(LnAK)dPn 
= Q'n(nn) + Q*(nn)- 
f 
(LnAK)dPn 
= QWn) + f 
(Ln-LnA 
K) dPn > Q'jn„) + 1 i 
Ln dPn 
•Ίΐ, 
JLn>2K 
= Q'n(iln) + \Ql{Ln > 2K). 
ε > 
Let Nn be a set with Pn(Nn) = Q'n(N^) = 0, and define Bn = {w € JV£ : δ < 
Ln < 2K). Then, for n > η(ε), 
Pn(Bc
n) = Pn(Ln <δ) + Pn{Ln > 2K) < ε + f Ln dPn/2K < 2ε 
and 
Qn(Bc
n) = QJ,(n„) + Q%{Ln <δ)+ Qa
n(Ln > 2K) < 3ε. 
Hence Proposition 9.47 implies that (P„) and (Qn) are contiguous. 
■ 

268 
RANDOM REGULAR GRAPHS 
Contiguity is preserved by some natural operations, as the next proposi-
tion shows. In (ii) and (iii) we suppose that (Ω'η,^ή) ' s another sequence of 
measure spaces. 
Proposition 9.50. Suppose that (Pn) and (Qn) are contiguous. 
(i) // {An) is any sequence of events such that liminf Pn(An) > 0, then the 
conditioned measures P„(- \ An) and Qn(· | An) are contiguous. 
(ii) // /„ : Ω„ -> Ω'η are measurable functions, then the induced measures 
Pn o f~l and Q„ o / " ' on Ω'η are contiguous. 
(iii) // P'n
 and Q'n are contiguous probability measures on Ω'η, then the prod-
uct measures P„ x P¿ and Q„ x Q'n on Ω„ χ Ω^, are contiguous. 
Proof, (i) and (ii) follow easily from the definition (note that in (i), also 
liminf Qn(An) 
> 0 by the contiguity); for (iii) it is perhaps simplest to use 
Proposition 9.47 with Remark 9.48. We omit the details (Exercise!). 
■ 
Contiguity of two sequences (Pn) and (Qn) has several useful consequences 
for limit theorems. The definition says that any property which holds un-
der P„ with probability tending to 1 as n -> oo (i.e., asymptotically almost 
surely) also holds under Q„ with probability tending to 1 (and conversely). 
An immediate consequence is that if Xn are any random variables such that 
Xn under P n converges in probability to some constant c, then Xn converges 
in probability to the same constant c also under Qn. 
For convergence in distribution we need some further information. In the 
next two results we suppose that S and S' are two complete separable metric 
spaces, for example R, Rd or the space R°° of infinite sequences. (See Billings-
ley (1968) for details on convergence in metric spaces.) We use C(X \ P) to 
denote the distribution of X under P. 
In the first result we assume joint convergence under P„ of the variables 
Xn and the Radon-Nikodym derivatives 
dQn/dPn-
Proposition 9.51. Suppose that (P„) and (Qn) are contiguous, and let L„ = 
dQn/dPn. 
Suppose further that Xn are random variables defined on Ωη with 
values in S, such that £((X n,L„) | P„) -> C(X,L) for some random variables 
X and L (with values in S and R, respectively), defined on a probability 
space (Ω,^,Ρ). 
Then C(Xn \ Qn) -> C{X | Q), where dQ = LdP {i.e., 
Q(A) = 
fALdP,A€r). 
Proof. Note that JLdP 
= 1 by Proposition 9.49, so Q is a probability 
measure. Let / be a bounded continuous real-valued function on S. Then 
C(f(Xn)Ln 
| P n) -+ C(f{X)L). 
If / > 0, then Fatou's lemma yields 
liminf [ f(Xn)LndPn 
> f f(X)LdP. 
(9.37) 
Π-+0Ο J 
J 

A BRIEF COURSE IN CONTIGUITY 
269 
Taking first / = 1, we see that 
lim inf Qa
n = lim inf f LndPn> 
[ L dP = 1, 
(9.38) 
n—»oo 
n—»oo J 
J 
and thus / Ln dPn -» / L dP. Hence it follows, by adding a constant to / , 
that (9.37) actually holds for every bounded continuous / . But then we may 
also substitute - / i n (9.37), and obtain 
j f(Xn) dQ° = j f(Xn)Ln dPn -> J f(X)LdP = j f(X) dQ. (9.39) 
Moreover, by (9.38), Qs
n(Ωη) = 1 - <?»(Ω„) -► 0, and we obtain 
Jf(Xn)dQn^ 
j f(X)dQ 
for every such / , which gives the result. 
■ 
The next result gives a necessary condition for contiguity. 
Proposition 9.52. Suppose that (P„) and (Q„) are contiguous, and that Xn 
are random variables defined on ίϊ„ with values in S. Suppose further that 
Xn converges in distribution under both Pn and Qn·' 
£(Xn \Pn)^Px 
and 
C(Xn \ Qn) -+ Qx 
for two probability measures Ρχ andQx 
onS. ThenPx 
andQx are mutually 
absolutely continuous. 
Proof. Let L„ = dQn/dPn. 
Since EL„ < 1, the sequence (L„) is tight and we 
may, by restricting attention to a subsequence, assume that £((X n,L„) | P„) 
converges. Proposition 9.51 now implies that Qx is absolutely continuous 
with respect to Ρχ. The converse holds by symmetry. 
■ 
We combine Proposition 9.52 with the following standard result. (In fact, 
the given conditions are equivalent to mutual absolute continuity.) 
Proposition 9.53. Suppose that the measure f]j Ρο(λ<) and n¿P 0(^¿) on 
Z°° are mutually absolutely continuous. 
Then 
\¡ = 0 <ί=> λ· = 0, 
and 
y ( A i - A ; ) 2 , 
Σ-ΛΓΤλΓ<0°· 
Proof. The first conclusion is trivial. For the second, we may use the Hellinger 
integral, defined for any two probability measures μ and μ' on the same space 
by Ι(μ,μ') 
= / ( ^ J ^ - ) 
dv, where v is any σ-finite measure with μ,μ' < v. 

270 
RANDOM REGULAR GRAPHS 
It is easily seen that μ and μ' are mutually singular if and only if Ι(μ,μ') — 0, 
and that in our case 
/ (π
ρ°(
λ<)>Π
ρ°(
λ'.)) = Π
/(
ρ°(
λ»)>
ΡοΜ) = ΙΚ
( λ /* ~
A)2/2· 
\ t 
i 
I 
i 
i 
Hence the measures are mutually singular unless 
oo > £(v/Ä~- N/A])2 = J > ¿ -A;)2/(N/AT+ V ^ ) 2 
Corollary 9.54. Suppose that (P„) and (Qn) ore contiguous, and that Xkn, 
k > 1, are integer-valued random variables defined on Ωη· Suppose further 
that Xkn is asymptotically Poisson distributed under both P„ and Q„.· 
C(Xkn | Pn) -> Po(A*) 
and 
C(Xkn \ Qn) -► Po(A'fc) 
for some A* and \'k, for each k, as n -* oo, and that these hold jointly for all 
k with independent limits, for both Pn and Qn. Then 

10 
Zero-One Laws 
10.1 
PRELIMINARIES 
Most of the problems we have studied so far followed a similar scheme: given 
a graph property A estimate the asymptotic probability that A holds for a 
random graph G(n,p), where the edge probability p = p(n) varies. In this 
chapter we consider questions of a different type: we fix a function p = p(n) 
and study the asymptotic behavior of the probability that G(n,p) has A, 
where A ranges over a prescribed class of graph properties 21. Thus, in the 
following section we identify functions p = p(n) for which the zero-one law 
holds, that is, the probability that G(n,p) has A tends to either zero or 
one as n —» oo for every property A from 21. If p = p(n) is close to the 
threshold function of some property from 21, then the behavior of G(n,p) 
becomes more complicated. In particular, another phenomenon may happen: 
for each A € 21 the probability that G(n,p) has A converges (but, maybe, to 
a non-trivial limit). Several instances of p(n) satisfying such a weak zero-one 
law are given in Section 10.3. Then we introduce sum schemes of models - a 
fairly general approach which can be used to show zero-one laws for random 
discrete structures. Finally, we ask when the behavior of the probability that 
G(n,p) has some property can be effectively determined. In particular, if for 
a given function p = p(n), there is a procedure which separates all properties 
in 21 which hold a.a.s. from those which a.a.s. do not hold for G(n,p). 
In order to start our considerations we need to decide first what class of 
graph properties 21 we are going to study. Indeed, it would be naive to hope 
that for some non-trivial p = p(n) the probability that G(n,p) has A converges 
271 
Random Graphs 
by Svante Janson, Tomasz Luczak and Andrzej Rucinski 
Copyright © 2000 John Wiley & Sons, Inc. 

272 
ZERO-ONE LAWS 
for every graph property A\ the probability that G(n,p) has an even number 
of vertices oscillates between 0 and 1 as n -> oo, no matter what function 
p = p(n) we choose. Hence, we have to find a class of properties 21 which is 
wide enough to include many interesting graph properties yet does not contain 
"odd" properties such as "the number of vertices of a graph is even". 
A natural way of describing different families of properties comes from 
mathematical logic: we specify a language L, treat all graphs as models of L 
and consider the class of all properties which can be written as sentences of L. 
Therefore, in this chapter we employ notions and facts from mathematical 
logic which have not appeared in this book so far. In the remaining part 
of this section, we list basic terminology and results we will need for our 
considerations. 
We will deal only with first-order languages, whose vocabulary, besides 
Boolean symbols ->, Λ, V, quantifiers V, 3, and brackets, consists of a countable 
number of logical variables denoted by i, y, z,..., possibly with indices, the 
binary predicate of equality "=", and a finite number of additional predicates. 
Two such languages will be of special interest to us: the first-order language 
of graphs L~, which contains only one additional predicate "~", and the first-
order language of linearly ordered graphs L°ZA, which, besides "~", contains 
also a binary predicate "<". The expression "i ~ y" is interpreted as "the 
vertices x and y are adjacent", and "<" should be understood as the standard 
ordering relation in the set of natural numbers. Thus, in this part of the 
book, graphs defined on subsets of natural numbers are viewed as models of 
the languages L^. and L°Zd. Consequently, we will be mainly concerned with 
graph properties which can be expressed by sentences of L~ or L™ú. For 
example, the property that a graph contains a cycle of length 1729, and the 
property that any two vertices of a graph are connected by path of length 
123 can be expressed by sentences in both L~ and L°Zá. The property that a 
graph contains a "monotone" path of length two, 
3x3y3z(x 
<yAy<zAx~yAy~z), 
can be expressed in L°ZA but not in L~. Note that the symbol "e" is not a 
part of the vocabulary described above and consequently all logical variables 
of first-order languages must be interpreted as elements of a model, in our 
case the vertices of a graph. Hence, there is no obvious way to write in L~ 
(or L°Zá) that a graph is connected; as a matter of fact we will soon see that 
this property cannot be stated in L~. Note also that neither L~ nor L°Zá 
contains logical constants, so we cannot use the labels of vertices, and no 
sentence of L~ (or L°Zá) can express the property which, for a graph G with 
vertex set [n], means: "vertices |n/2J and (n/2J + 1 are adjacent". In L°Zd 
we can implicitly define vertices 1 and n as those without predecessors and 
successors, respectively, so it is possible to say that, for example, vertex 7 is 
adjacent to vertex n - 5; in L~ we cannot do even that. 
Finally, the quantifier depth qd(ip) of a formula φ tells us, roughly speaking, 
how many times quantifiers are nested inside ψ. More formally, the quantifier 

EHRENFEUCHT GAMES AND ZERO-ONE LAWS 
273 
depth can be defined recursively with respect to the structure of a formula by 
setting: 
Φ qd(ip) = 0 for a formula φ without quantifiers; 
• qd(i/>) = qd(-i^); 
• qd(V0 = maxj{qd(r/'t)} if either -φ = V¿ Ψί or ψ = /\¿ V¿; 
• qd(3,^(i)) = qd(t/»(i)) + 1 and qd(Vx^(x)) = qd(i/>(x)) + 1. 
Thus, for example, the property that a graph contains a cycle of length six 
but none of length four can be in an obvious way written as a sentence of L~ 
of quantifier depth six. However, it is not hard to see that the same property 
can be expressed by the following slightly more complicated sentence of L~ 
of quantifier depth four: 
' 7 ' X I V I J V X J V I 4 (-ι(->Χι = X3 Λ ~>X2 = X4 Λ X\ ~ X2 Λ X2 ~ X3 
Λ X3 ~ X4 Λ X\ ~ X4)) 
Λ (3Xl 3Ζ23Ι3 (-ιχι = x2 Λ ->xi = x3 Λ ->x2 = X3 
(10.1) 
Λ 3Χ4(-ιχι = X4 Λ ->xi ~ X4 Λ x2 ~ X4 Λ X4 ~ X3) 
Λ 3l4(->x2 = X4 Λ X\ ~ X4 Λ ->x2 ~ 14 Λ X3 ~ X4) 
Λ 3l4(->x3 = 14 Λ ii ~ X4 Λ x2 ~ x4 Λ ->x3 ~ x4)) 1 . 
If a model M satisfies the property described by a sentence rp, we sometimes 
say that M is a model for φ, and write M \= ij·. For a model M of L, we 
denote by Th£(M), or simply by Th/t(M), the set of all sentences of L of 
quantifier depth at most k which hold for M, that is, 
Thjt(M) = Th£(M) = {V € L : M μ φ and qd(V) < k) . 
Let us also mention that for any first-order language L and every k there 
are only finite number of non-equivalent properties expressed by sentences 
from Thfc (M), that is, if for each such property we choose one sentence which 
describes it, then the set Th£(M) becomes finite. 
10.2 EHRENFEUCHT GAMES AND ZERO-ONE LAWS 
In the study of combinatorial properties of discrete structures viewed as mod-
els of some languages it is sometimes convenient to use elements of the theory 
of games. In many cases such an approach provides combinatorial insight 
into otherwise purely logical considerations and often simplifies the statement 
of the problem. For analysis of first-order theories the Ehrenfeucht game, 
introduced by Ehrenfeucht (1961), has proved to be particularly useful. 

274 
ZERO-ONE LAWS 
Ehrenfeucht games 
Let M' and M" denote two models of a first-order language L and let Jfc be a 
natural number. The k-round Ehrenfeucht game, denoted by Ehr*(M',M"), 
is a perfect information game, played on models M' and M" by two players. 
In each round the first player chooses one of the models and an element in it. 
The second player must answer by picking an element in the other model. This 
procedure is repeated fc times until, at the end of the game, two sequences 
{x\,...,x'k) 
and (x",...,x'¿) 
of elements of M' and M", respectively, are 
obtained. Let us emphasize that in each step of the game the first player 
is free to change the model from which he picks an element; if in the first 
step he decides to choose x\ from M', in the second round he might either 
pick x2 6 M", or move again at M'. Thus, some of the vertices 
x\,...,x'k 
might have been selected by the first player and the others by the second 
one. The second player wins Ehr*(Μ',Μ") if the substructures spanned by 
the sequences (x[,... ,x'k) and (x",...,x'¿) 
are isomorphic as models of L; 
otherwise the game goes to the first player. The isomorphism should map x\ 
into x", thus, in the case of the language L^ the second player should assure 
that for all 1 < i, j < k we have x\ = x'j if and only if x" = x'j, and x\ ~ x'j if 
and only if x" *- x'j, whereas in the case of L°¿A we additionally require that 
x'i < xj if and only if x'¡ < x'¡. 
Example 10.1. Let V be the property that a graph has diameter at most 
two, that is, 
ViVv3z(x = y V x ~ y V ( x ~ z A z 
~y)), 
and suppose that we have two graphs G' and G" such that φ holds for G' 
but not for G". It is not hard to see that in such a case the first player has a 
sure win in the game Eru-3(G',G"). In the first two moves he should pick two 
vertices x" and x'{ which are not connected by a path of length at most two 
in G". The second player must answer by selecting non-adjacent vertices x\ 
and x'2 in G'. Then, in his last move, the first player should choose as x'3 a 
common neighbor of vertices xi and x2 in G', so the second player cannot 
duplicate his move in G" because of the choice of x" and x'{. 
Example 10.2. Let G' and G" be two graphs such that in G' the shortest 
cycle has length six whereas G" contains no cycles of length smaller than 
seven. Then, although the first player cannot build in G" a cycle of length six 
in four moves, nonetheless he can win Ehr4{G',G"). Indeed, in the first three 
moves he should choose vertices χΊ,χ^,χ^ which belong to a cycle of length 
six in G' and are such that the distance between any two of them is two. 
In response to that the second player must pick three non-adjacent vertices 
χΊ',Χ2,χ'3 in G" but, since G" contains no short cycles, either all three of 
them are adjacent to one vertex, or the distance between at least one pair 
of vertices, say x2 and x3', must be larger than two. In either case the first 
player wins if he chooses as x\ the common neighbor of x'2 and x3. 

EHRENFEUCHT GAMES AND ZERO-ONE LAWS 
275 
One might suspect that in Example 10.1 the strategy of the first player, who 
in the first two rounds picks elements from G" and in the third one switches 
to G', has something to do with the fact that in the sentence ψ two general 
quantifiers are followed by the existential one. Furthermore, the reader patient 
enough to study the formula (10.1) can easily notice that the strategy of the 
first player in Example 10.2 in a way "reflects" the structure of the sentence 
(10.1) of depth four which holds for G' but not for G". More generally, it is 
not hard to see that if the first player can win Ehr*(Μ',Μ") then, knowing 
his winning strategy, one can write down a sentence ψ of quantifier depth at 
most k which holds for M' but not for M". On the other hand, if there is a 
sentence of quantifier depth at most k which holds for only one of M' and M", 
the first player can use it to create his winning strategy for Ehrfc(M',M"). 
This observation, stated below as Lemma 10.3, can be used to replace an 
argument from mathematical logic by purely combinatorial considerations. 
Lemma 10.3. The second player has a winning strategy in the game 
Ehr t(M', M") if and only t/Thfc(M') = Thfc(M"). 
■ 
One can use the above fact to prove that certain properties cannot be 
expressed in the first-order language of graphs. We give just one such example; 
the reader can easily show using a similar argument that the properties that 
a graph contains a cycle, or a perfect matching, or that it is bipartite, or, 
finally, that it contains an even number of vertices, cannot be expressed in 
the first-order language of graph theory (Exercise!). 
Corollary 10.4. The property that a graph is connected cannot be written as 
a sentence from L~. 
Proof. Suppose that there is a sentence ψ in L„ of quantifier depth k, which 
says that a graph is connected. Furthermore, let G' be a cycle of length 
3* + 1 and let G" consist of two copies of G'. 
Then there exists a simple 
winning strategy for the second player in Ehr*{G',G"): if in the i-th move 
the first player picks a vertex which is within distance 3 * - ' from some vertex 
chosen previously, the second player should select a "corresponding" vertex 
in the second model, otherwise he may answer with any vertex which lies 
far enough from vertices already chosen. The reader is encouraged to verify 
that this strategy really works, i.e., that, basically, if the distances between 
x[ and x'2, and x" and x'2' are both larger than 3*" 1, then the first player 
cannot effectively use the fact that the distances might be different to win the 
game Ehr*(G',G"). Thus, Th*(G') = Th*(G"), contradicting the fact that 
φ € Thfc(G') while ψ $ Thfc(G"). 
■ 
Zero-one laws for very dense and very sparse random graphs 
Throughout the chapter we will repeatedly use Lemma 10.3 to establish the 
zero-one law in G(n,p) for some particular function p = p(n). Since for this 
type of problem it does not matter whether we study properties of a random 

276 
ZERO-ONE LAWS 
graph or properties of its complement, we may and will always assume that 
P = P(n) < 1/2. 
We start with a classical result on dense random graphs by Glebski, Kogan, 
Liagonkii and Talanov (1969), independently proved also by Fagin (1976). 
Theorem 10.5. // 1/2 > p = p(n) = n~o(1> then for every sentence φ 
from L~ the probability that G(n,p) satisfies φ tends either to 0 or to 1 as 
n-+oo. 
Proof. For a natural number k > 1 let <pk be the property that for every 
pair of disjoint subsets of vertices Si and S2, such that |5i| + |52| < k - 1, 
there exists a vertex which is adjacent to all vertices from Si and no vertices 
from S2- (Note, by the way, that since both S\ and S2 have prescribed size, 
ipk is a first-order sentence of quantifier depth at most it.) Then, for every 
A; > 1, we have 
* - l k - l - d 
P(G(n,p) (=-¥>*)<£ Σ 
n'^(l-p / l(l-p) / 2) n- k 
f,=0 
¿2=0 
< 
fcV-1exp(Jfc-npfc-1(l-p)*~1) 
< JfeVn'expi-n 1-"^ = o(l), 
and so a.a.s. G(n,p) satisfies φ^. On the other hand, for any two graphs 
G' and G" for which tpt holds, the second player can win Ehrk(G',G") by 
playing an obvious strategy: the fact that both <¿>* holds for both G' and G" 
guarantees that in each of the first k steps he will be able to imitate the moves 
of his opponent. Thus, a.a.s. Th*(G(n,p)) = Thfc(Gt), where Gk is any graph 
for which <pk holds (the existence of Gk follows from the probabilistic part of 
our proof). Consequently, for any sentence φ from L~ of quantifier depth k 
we have either ψ e Th*(Gt), and then 
lim P(G(n,p) \= tp) = 1, 
n—>oo 
or φ & Thk(Gk), which implies that 
Urn P(G(n,p) (= ψ) = 0. 
■ 
n-+oo 
It is worthwhile to note that the above argument proves that for every 
sentence φ of L~ of quantifier depth at most k either φ or ->φ is a consequence 
of the axiom set which consists of one sentence tpk, in other words, we showed 
that {v?fc} is a complete system of axioms for the theory which consists of the 
sentences of depth at most k from L„. Needless to say, this can be verified 
directly without invoking Ehrenfeucht games. As a matter of fact, from the 
remarks which led us to Lemma 10.3, it should be clear that virtually every 
proof which employs Ehrenfeucht games can be transformed into a purely 
logical argument, and it is only a matter of personal preference whether one 
decides to use the logical or the combinatorial approach. 

EHRENFEUCHT GAMES AND ZERO-ONE LAWS 
277 
Now let us consider the behavior of a very sparse random graph G(n,p), 
that is, the case p = p(n) < n~ l + 0 < 1 ). In order to do that we first identify 
some functions for which the zero-one law certainly does not hold. According 
to Theorem 3.19, if 
pt^cn-1-1" 
for some natural number I > 1 and a constant c > 0, then the probability 
of the first-order property saying that G(n,pt) 
contains a tree of size ί + 1 
converges to a limit which is neither zero nor one. Furthermore, from the 
same result we infer that if 
npo = c' 
for some constant c' > 0, then the probability that G{n,po) contains a triangle 
also tends to a non-trivial limit. Finally, let r, s be integers such that s > 
r - 1 > 0 and 
rnPr,* = log n + a log log n + c", 
where this time c" is any constant, not necessarily positive, and let Ar¡$ denote 
the first-order property that a graph contains a path vi ■ ■ ■ vr such that the 
total number of edges incident to the vertices v\,..., 
vr is s. 
Elementary 
application of the method of moments (Exercise!) shows that the probability 
that G(n, ρΓι,) has Ar,a tends to a non-trivial limit, which rules out pT,, as a 
possible candidate for a function for which the zero-one law holds. We will 
show that, as long as p < n _ 1 +°^), the functions pi, po and ρΓι, are, in a way, 
the only ones which do not obey the zero-one law (see Shelah and Spencer 
(1988) and Luczak and Spencer (1991), where Theorem 10.6 is proved using 
a somewhat different method). 
Theorem 10.6. Let p = p(n) be a function for which one of the following 
conditions holds: 
(i) n - 1 - 1 / ' «: p «C n-1-1/"-1"1' for some natural number 
l>\, 
(ii) ρ<^η~χ 
butp = 
n-l-°^\ 
(iii) p^ 
n-1 
but p = n~1+°'1) and for every given pair of integers r and s 
such that s >r — 1 > 0 we have either rnp — logn — slog log n —> — oo, 
or rnp — logn - slog logn -► oo. 
Then, for every sentence ψ from L~ the probability that G(n,p) satisfies ψ 
tends either to 0 or to 1 as n -> oo. 
Proof. Let ψ be a sentence from L~ of quantifier depth k. Consider first 
the simplest case n ~ 1 - 1 / ' «: p «: η~ 1 _ 1/<' + 1). In this early period of the 
evolution, a.a.s. G(n,p) consists of isolated trees of size at most ί + I and 
every tree of size at most t + 1 appears as a component of G(n,p) at least k 
times (see Theorems 3.4 and 3.19). Clearly, this property uniquely determines 
all properties of a graph described by the sentences of length at most k, since 

278 
ZERO-ONE LAWS 
in the Ehrenfeucht game on two such graphs the second player may always 
choose a vertex which lies in a tree isomorphic to one picked by the first player. 
The case when p «. n~l but p = n - 1 _ o ( l ) is only slightly more difficult. 
Set 
Ξ* = (Thfc(T) : T is a finite tree} 
and let 
Tk = {Τι,Τ2,.. .,Τ|Ξΐι|} 
be a collection of trees such that for every finite tree T there is a tree T" € 7i 
for which Th*(T) = Th*(T"). Furthermore, let G* be a graph which consists 
of fc copies of each tree from 7*. If p = p{ri) = n-1-0*1) and np -> 0 then a.a.s. 
G' — G(n,p) is a forest which contains at least k copies of each given tree 
as components (Exercise!), in particular, it contains at least k copies of each 
element from 7*. It is easy to see that this property ensures that the second 
player can win the game Ehrt(Gfc,G'). Indeed, if the first player chooses a 
vertex in some component T of G', the second one will pick a component 7" of 
Gfc such that Τ1ι*(Γ') = Thfc(T), and select a vertex in T" which guarantees 
him a win in Ehr* (Τ,Τ'). Needless to say, if at some stage of the game the 
first player decides to choose again a vertex from either T or T' the second 
player will play according to his winning strategy for Ehr* (Τ,Τ'). 
The last part of the proof is slightly more complicated. Roughly speaking, 
now the second player will use the fact that for p = n-1"1"0^ the typical 
random graph G(n,p) "locally" resembles a tree, in which every vertex has 
large degree. The only problem he might face is the presence of sparsely 
distributed short cycles and some number of vertices of small degree. 
In order to deal with short cycles, let us recall that the distance between 
two subsets of vertices V and V" of a graph is measured by the number of 
edges in the shortest path joining a vertex from V to a vertex from V". Note 
that Theorem 3.4 implies that if p = n~1+°(1) then a.a.s. any two cycles of 
G(n,p) shorter than 3*+1 lie at a distance larger than 3*+2. Elementary first 
moment calculations show also that if p 2> n - 1 then a.a.s. every vertex of 
G(n,p) which lies within distance 3*+2 from such a short cycle must have 
degree at least 3*+1 (Exercise!). Furthermore, a.a.s. G(n,p) contains at least 
k cycles of each length ί, where 3 < I < 3*+1, provided pn -* oo (Exercise!). 
Thus, although G(n,p) contains a lot of short cycles, they lie far apart from 
each other and from other "critical" structures and can be dealt with without 
much problem (see the proof of Corollary 10.4). 
Unfortunately, unlike short cycles, vertices of small degree can appear in 
clusters, so we need to classify them according to their neighborhood. We 
say that two vertices of a graph are of the same 0-type if either they have the 
same degree smaller than k, or both of them have degree at least k. Clearly, 
this is an equivalence relation which partitions the vertices, according to their 
degrees, into k +1 classes. Furthermore, for i = 1,..., k, we say that vertices 
w and w' of the graph have the same i-type if for each (i - l)-type the number 
of neighbors of w and w' which belong to that type are either the same, or 

EHRENFEUCHT GAMES AND ZERO-ONE LAWS 
279 
both at least k. Note that for every given i there are only finitely many i-
types. Moreover, to determine the i-type of a vertex v one needs only to check 
if v is the root of some finite subgraph F of G(n,p) of depth i + 1 such that 
the size of F is bounded by a function of i and k and its vertices have certain 
prescribed degrees in G(n,p). It turns out that, whenever a function p = p(n) 
fulfills the assumptions of Theorem 10.6 for any given natural numbers i and k, 
the class of all ¿-types, where 1 < i < 3* + 1, can be split into two subclasses of 
"likely" and "unlikely" ¿-types, where the division depends on the behavior of 
p with respect to ρΓι,. It can be shown that a.a.s. G(n,p) contains no vertices 
of unlikely types; on the other hand a.a.s. G(n,p) contains at least k vertices 
of each likely type, which can be chosen in such a way that they lie at distance 
at least 3*+1 from each other. A rigorous verification of this fact relies on the 
computation of the expectation and variance of the number of vertices of a 
given ¿-type. Thus, if the expected number of vertices of a given ¿-type tends 
to 0, then a.a.s. such vertices do not appear in G(n,p). On the other hand, 
if the expected number of vertices of an ¿-type tends to infinity, then one can 
use Chebyshev's inequality (1.2) to verify that, under the assumption of the 
theorem, G(n,p) a.a.s. contains many of them. Since this is a standard, but 
quite tedious argument, we neither elaborate on it, nor dare to recommend it 
to the reader as an exercise. 
Now let us consider the fc-round Ehrenfeucht game on two graphs such that 
both of them 
• have many sparsely distributed short cycles, 
• contain no vertices of unlikely ¿-types, for ¿ = 0 , 1 , . . . ,3 f c + 1, 
• contain a rich sparsely distributed system of representatives of likely 
¿-types, for i = 0 , 1 , . . . , 3* + 1, 
where each of the above items should be understood in the context of the 
probabilistic results on properties of G(n,p) we mentioned. In such a game 
the second player has the following winning strategy. If in the i-th round of 
the game the first player chooses a vertex, the second one checks whether it 
lies within distance 3*~* from either a cycle shorter than 3* + 1, or some vertex 
chosen previously: if this is the case, then he answers with an "analogous" 
vertex in the second graph (in the case of a cycle this means any vertex which 
lies at the same distance from the corresponding cycle in the other graph); 
otherwise he chooses any vertex of the same 3fc-i+1-type which lies at distance 
at least 3 f c - , + 1 from all vertices selected so far. 
■ 
Sophisticated winning strategies: The closure 
Let us assume now that p = p(n) = n-<*+<>U) for s o m e constant a e (0,1). It 
turns out that in this case the behavior of G(n, p) depends strongly on whether 
a is rational or irrational. The analysis of the case in which a is rational is 

280 
ZERO-ONE LAWS 
somewhat involved: we postpone it until the next section and concentrate on 
the result of Shelah and Spencer (1988), stating that the zero-one law holds 
for every irrational a € (0,1). 
Theorem 10.7. Let p — p(n) = n~ Q + o ( 1 ), where a € (0,1) is irrational. 
Then, for every sentence φ from L^ the probability that G(n,p) satisfies ψ 
tends either to 0 or to 1 as n —> oo. 
As in the case of Theorems 10.5 and 10.6, we prove Theorem 10.7 by show-
ing that a certain Ehrenfeucht game can be won by the second player; however 
now his strategy must be much more sophisticated. Indeed, for p(n) — n~°W 
the random graph G(n,p) is a.a.s. so dense that it contains every possible 
configuration which can emerge during the Ehrenfeucht game. Thus, the sec-
ond player can pick in each move any vertex which is appropriate at this stage 
of the game and not worry about his opponent's further moves. Now this is 
no longer the case, if p(n) = n~ a + o ( 1 ) and a € (0,1), then one can easily find 
a graph H such that a.a.s. G(n,p) contains a copy of H, but not more than 
op(n) of them. Hence, if the first player chooses in his first move a vertex in a 
copy of such an H, the second player must choose a vertex which also belongs 
to a copy of H, and at each stage of the game he must constantly analyze all 
possible strategies of his adversary. Furthermore, unlike in the game played 
on sparse graphs containing only a few "critical" structures lying far away 
from each other, here the diameter of the graph is bounded from above by 
[1/(1 - Q)1 (see Bollobás (1985, Chapter X)). Thus, there are no "isolated" 
structures and if the Ehrenfeucht game lasts long enough the first player can 
connect by a path any two vertices of the graph. 
In order to get some feeling for what the strategy of the second player should 
be, let us pick, say a = y/Z/2 = 0.86602..., and consider the Ehrenfeucht 
game of length k played on two graphs G', G" such that if a sentence φ of 
quantifier depth at most k holds a.a.s. in G(n,p) with p = p(n) = n~a, then 
it holds also for both G' and G". 
First set k = 3. 
Then G(n,p) contains Θ ρ(η 3 _ 3 β) triangles (see Re-
mark 3.7) and, since 0 < 3 - 3a < 1, we may assume that both G' and 
G" contain some triangles as well as vertices which do not belong to them. 
Thus, before his first move the second player should check at least whether 
the vertex selected by the first player lies on a triangle, and pick his vertex 
accordingly. 
Now suppose that k = 4. Then the first move of the second player should 
depend not only on whether the vertex v picked by the first player belongs to 
a triangle: he certainly must consider also larger structures containing v, such 
as cycles of size four, five, six (see Example 10.2) and, as one can easily check, 
even seven. Furthermore, he must examine whether v is adjacent to some 
triangle and, if this is the case, choose a vertex which also has this property 
since otherwise the first player can win easily. 
What should be the first move of the second player when k becomes even 
larger, say k = 15? Certainly, he must, in particular, take into account all 

EHRENFEUCHT GAMES AND ZERO-ONE LAWS 
281 
aspects of the choice of the first player we mentioned for k = 4, since otherwise 
he will lose after the first four moves. But should he care whether the vertex υ 
chosen by the first player lies within distance, say, ten from some triangle? 
It does not seem to be absolutely necessary: a.a.s. the diameter G(n,p) is 
eight and so is the diameter of G' and G", thus the second player might 
hope that he can always join the first vertex to a triangle when he notices 
that the first player is trying to do so. Hence, he must concentrate on the 
fact that the vertex v selected by the first player either belongs to a "rare" 
structure (like, in the case of a = λ/3/2, a short cycle), or at least lies close 
to it. More specifically, he should compute the "closure" of v, which consists 
of such structures, and choose in the second graph a vertex with the same 
closure as v. Now he might try to play according to the following strategy: 
if the first player chooses a vertex in the closure of the vertices chosen so 
far, then the second one picks a corresponding vertex in the closure of the 
set of vertices chosen in the other graph; if the first player decides to move 
outside the closure, then, hopefully, the second player will be able to find an 
"equivalent" vertex outside the closure in the second graph. 
Hence, the closure of a vertex or a set of vertices, should consist, roughly 
speaking, of all "rare" structures containing this vertex or set and, in the 
random graph, "rare" typically means "denser than average". Note, however, 
that our sketchy analysis of the case a = y/Z/2, k = 4, has suggested that a 
triangle which lies at distance one from a vertex should belong to its closure, 
while, on the other hand, if a vertex v belongs to a triangle it is clearly not 
necessary to include all vertices adjacent to it in the closure of v. Thus, to 
find the closure of a vertex v we should not only examine all copies of a graph 
H in G(n,p) which contain v - we must also take into account how v is placed 
in them. 
Thus, instead of graphs, we will consider rooted graphs (R, H), which con-
sist of a graph H and a subset of its vertices R C V(H) (see Section 3.4; 
although now, for technical reasons, we allow edges between vertices of R, 
they are irrelevant for our argument). The elements of R are roots, the ver-
tices from V{H) \ R are called extension vertices, and we say that an edge of 
H is proper if it does not join two roots. The number of extension vertices for 
a rooted graph (R,H) is denoted by v(R,H), 
whereas e(R,H) 
stands for the 
number of proper edges. We say that (R, H) is sparse if v(R, H) > ae(R, H), 
and dense otherwise, where, throughout the rest of this section, a denotes a 
fixed irrational number from the interval (0,1). Thus, {R, H) is dense if either 
R = V(H), or v{R,H) 
< ae{R,H). 
We say that {R,H) is rigid if (S,tf) is 
dense for every 5 with R C S C V(H), and safe if (R,H') 
is sparse for every 
subgraph H' of H with R C V{H') C V{H). 
Note that if (R,H) 
is rigid, 
then any rooted graph (R',H') 
such that R C R1, V{H') = R' U V{H), and 
H C H', is rigid as well. 
An extensive list of elementary properties of safe and rigid rooted graphs 
can be found in Shelah and Spencer (1988); here we will mention only three of 

282 
ZERO-ONE LAWS 
them. Throughout this section HuH' 
[ΗΠΗ'] denotes the graph with vertex 
set V{H)L)V(H') 
[V{H)nV(H')} 
and edge set E(H)uE(H') 
[E(H)nE(H')}. 
Fact 10.8. If both (R,H) and (V(H),H') 
are rigid, then (R,H') is rigid. 
Proof. Let R C S C V{H'). 
Then, since both (R,H) and (V(H),H') 
are 
rigid, we get 
v{S, H') = v(S Π V(H), H) + v{S U V{H), H') 
< ae(SnV(H),H) 
+ ae{SUV{H),H') 
< ae(S,H'). 
■ 
Fact 10.9. If both (R,H) and (R,H') are rigid, then (R,H'UH) 
is rigid. 
Proof. Note that (V{H),HUH') 
= {V{H)UR, V(H)liH') 
is rigid and apply 
Fact 10.8. 
■ 
Fact 10.10. If (R,H) is not safe, then there exists a subgraph H' of H such 
that R C V{H') and the rooted graph (R,H') is rigid. 
Proof. Let H' be a minimal subgraph of H such that R c V(H') and the 
rooted graph (R,H') 
is dense. Furthermore, let S" be any set such that 
R C S" C V(H') 
and let H" denote the subgraph spanned by S" in H'. 
Because of the choice of H', the rooted graph (R, H") is sparse, and so 
v(S", H') = v(R, H') - v(R, H") < ae(R, H') - ae(R, H") = ae(5", H'). 
Thus, (H, H') is rigid. 
■ 
Let (R, H) be a given rooted graph and R' be a subset of vertices of a 
graph G, such that |ñ| = |Ä'|. A pair (Ä\ Η') is an (R, H)-extension 
of R in 
G if H' is a subgraph of G such that R! C V{H') C V(G) and there exists 
a bijection / : V(H) 
-¥ V(H') 
which maps R into R' and transforms all 
proper edges of (R, H) into edges of H', that is, if {υ', v") e E(H) and either 
υ',υ" € V(H)\R,oiv' 
€ R, v" e V(H)\R, 
then also {f{v'),f{v")} 
€ E{H'). 
Note that we do not care about edges joining two roots, nor do we prohibit 
additional edges in H'. 
Finally, we introduce the crucial notion of i-closure. Let t > 0, G be a graph 
and W C V(G). The t-closure ofW is the minimal set W D W such that for 
every rigid rooted graph (R, H) with v(R, H) < t, each (R, fl^-extension of a 
subset R' of W is contained in W. The t-closure of W we denote by cU{W), 
or, if W = {ui\,...,wr}, 
we put simply cl t(tüi,...,w r). It is easy to see that 
clt(W) is uniquely defined, and that for all W and t there exists a sequence 
W = W0CWlC---CWt 
= clt(W) such that for t = 1,... J, the set Wi is 
obtained by appending to W^_i the vertices of some rigid (fí¿,/íi)-extension 
of a subset of Wi-i, where V(RÍ,HÍ) 
< t. Thus, due to Facts 10.8 and 10.9, 
{W,clt(W)) 
is a rigid extension of W. 
Our next result states that in a random graph the t-closure of a small set 
is unlikely to be very large. 

EHRENFEUCHT GAMES AND ZERO-ONE LAWS 
283 
Lemma 10.11. Let p = p(n) = n _ a + o ( 1 ), where a € (0,1) is an irrational 
number. Then for all natural numbers r and t, there exists M = 
M(a,r,t) 
such that a.a.s. for every choice of vertices v\,... 
,vT of G(n,p) we have 
|cl,(t;i,...,i; r)| < M. 
Proof. As we have already observed, for every set W and a natural number t 
there exists a sequence W = W0 C Wx C · · · C W( = c\t(W) such that Wi = 
Wi-x U V(H¡), for some rigid (Ri, //¿)-extension (R'i,H{), 
w i t h "(#·.#<) < l 
and R\ C Wj_i. Furthermore, it is easy to see that we may assume that all 
rooted graphs (RÍ,HÍ) 
are minimal rigid graphs, that is, for each rigid graph 
{Ri, Hi) such that i?¿ C Ru 0 φ V(HÍ)\RÍ 
C V{HÍ)\RÍ 
and Ht C Hi we have 
Ri = Ri and Hi = Hi. Note also that for each minimal rigid rooted graph 
(Ri,Hi) 
with v(Ri,Hi) 
< t we must have |Hj| < t/a + 1 . Indeed, otherwise 
either there exists a vertex in Ri with no neighbors in V(Hi) \ Ri, or some 
vertex v € V(Hi) \ Ri has more than 1 + 1/Q neighbors in Ri, contradicting 
the minimality of (RÍ,HÍ). 
Thus, since there are only finitely many non-
isomorphic rooted graphs (R,H) 
with \R\ < t/a + t and v(R,H) 
< t, we 
have 
et = min{oe(ñ, H) - v(R, H) : (R, H) is dense 
and 0 < v{R, H) < t, \R\ < t/a + t} > 0. 
We will show that the assertion holds with M — r + 
t\r/et\. 
The idea of the proof is rather simple. Suppose that W — {v\,..., 
vT) is 
a subset of vertices of a graph G such that | clt(lV)| > M. Let W = WQ C 
WI C · · · C Wm be a sequence of subsets of G such that for i = 1,2,..., m 
the set Wi is the result of an (Ri, /^-expansion of Wt-i, 
where (Ri, Hi) is a 
minimal rigid rooted graph with v(Ri,Hi) 
< t, and finally \Wm\ > M while for 
i = 0,..., m — 1 we have \Wt\ < M. We will show that the subgraph induced 
by Wm contains at least |W m|/a edges, whereas, due to Theorem 3.4, a.a.s. 
G(n, p) contains no subgraphs on at most M +1 vertices with density larger 
than 1/a. 
Thus, let Gt denote the subgraph of G spanned by Wit i = 0,1,...,m. 
Since d is the (Wj_i U ß», Wi-i U //¿)-extension of Wi-i, where the rooted 
graph (Wi-! U ñ¿, W{-i U Hi) is rigid, and furthermore, (Rit Hi) is a minimal 
rigid rooted graph, we have 
\Wi\ - |Wi_i | = v(Wi.x 
u Ri, Wi-X U Hi) < ae(Wi-l 
U Ri, W^x u Hi) + et. 
Then, for ¿ = 1,2,..., m, 
e(G,)-e(0,_,)>"y-'-"y'-i + a. 
a 
a 
Consequently, since \Wi\ - |W,_i| < t for i = 1,2,... ,τη, and \Wm\ > M = 
r + t\r/ei\, 
we have m > \r/et], and so 
e(Gm) 
1 
(\Wm\-r 
Γ T- -| £f N 
1 
v(Gm) 
\Wm\\ 
a 
* \ e t \ a ) - a · 

284 
ZERO-ONE LAWS 
Now we state two probabilistic lemmas on G(n,p) which will be crucial 
for constructing the winning strategy for the second player. The first one, 
Lemma 10.12, guarantees that in the first move the second player can choose 
a vertex whose closure is identical with the closure of the vertex selected by 
the first player. Lemma 10.13 assures the existence of a suitable move for the 
second player when the first one selects a vertex outside the closure. 
Lemma 10.12. Let p - p(n) = n_0,+0<1', where a G (0,1) is an irrational 
number, let {{w},H) be a rooted rigid graph such that m(H) < l/a, and let 
t > v({w},H). 
Then a.a.s. G(n,p) contains a vertex v such that clt(u) is 
isomorphic to ({w},H), where the isomorphism maps v into w. 
■ 
Lemma 10.13. Let t > 1, p = p{n) = n-Q+°<1), where a 6 (0,1) is an 
irrational number and let {R,H) be a rooted safe graph with \R\ = r. Then 
a.a.s. for every choice of vertices vi,...,vr 
o/G(n,p) there exists an (R,H)· 
extension ({vu... 
,vr},H') 
such that clt{V(H')) = clt(«i,... ,vr) U V{H'). ■ 
Unfortunately, the proofs of the above results are somewhat technical, so 
we just briefly comment on them. The existence of the required extensions 
in both lemmas is not very hard to show: in the case of Lemma 10.12 it 
is an immediate consequence of Theorem 3.4, for Lemma 10.13 it follows 
from Theorem 3.27. However, one needs also to prove that at least one such 
extension contains no vertices which can be used as the roots of rigid rooted 
graphs to extend the closure of the set we are dealing with. This can be done 
using a similar technique as employed in the proof of Lemma 10.11, but since 
the argument is long and not very instructive, we decided to omit it here. 
Proof of Theorem 10.7. Let A; be a fixed natural number. Define a sequence 
t\,...,tit 
setting i* = 0 and U = M(a,k, ti+i) for i = 1,2,.. .,k — 1, where 
M(a,r, t) is a function which fulfills the assertion of Lemma 10.11. Let G', G" 
be two graphs such that for all r < k, t < t\, and every graph H with at most 
M(a,k,ti) 
vertices, all graph properties which, according to Theorem 3.4, 
Lemmas 10.11, 10.12 and 10.13, a.a.s. hold for G(n,p), hold also for both G' 
and G" ■ We will show that the second player can win the Ehrenfeucht game 
Ehr*(G',G"), which, due to Lemma 10.3, will imply the theorem. 
A winning strategy for the second player for Ehr^C, G") has already been 
anticipated in our remarks following the statement of Theorem 10.7. We will 
show that, for i = 1,..., k, in the i-th step of the game the second player 
can make his move in such a way that the subgraph G\ induced in G' by 
cltiíií,...,^), and G" spanned in G" by cltj(i'/,. ..,i"), are isomorphic, 
where the isomorphism maps x'j into x'j for j = l,...,i. (Here and below 
x'j G V(G') and x" G V{G") stand for the vertices chosen in the j-th step of 
the game Ehr*(G',G")·) 
Since neither G' nor G" contains subgraphs H with |V(i/)| < 
M(a,k,tx) 
and m(H) > l/a (Theorem 3.4), the existence of a vertex with appropriate 
ii-closure which can be chosen in the first move by the second player is assured 

FILLING GAPS 
285 
by Lemma 10.12. Thus, let us assume that for some i, i = 1,..., k - 1, the 
subgraphs G\ and G" are isomorphic. Furthermore, without loss of generality 
we may assume that the first player decides to pick a vertex x'i+l from G'. Our 
aim is to show that there exists a vertex x'/+1 in G" such that the subgraph 
G'i+i spanned in G' by clti+l (x'j,..., x'i+l) is isomorphic to the subgraph G"+1 
induced by cl t i + 1(x",... ,x"+l) in G". We consider the following two cases. 
Case 1: x'i+l 6 clit (x[,..., 
x\). 
Then, using the isomorphism of G\ and G", the second player should choose 
in G" the vertex x'/+1 corresponding to x'i+l. 
Since 
\V(G'i+1)\ = |cl(i+1 ( ι ' „ . . . , i ' i + l ) | < M(a,k,ti+l) 
= tit 
the definition of ¿¿-closure and Fact 10.8 imply that 
v(G'i+1) = c\ti+l(x\,...,x'i+1) 
c cux;,...,*;) = V(G;). 
Similarly, we have V(G'/+1) Q V{G'¡), and so G'i+i and G"+j are isomorphic 
as subgraphs of G\ and G" respectively. 
Case 2: x'i+1 $ cl£i(x'i,... .xj). 
Let H[ denote the maximal subgraph such that 
χ',,.,.,χ; € V(H[) C cl,i+1 («'„...,i' j + 1) = V(G'i+1) 
and the rooted graph ({χΊ,... ,χ\},Η[) 
is rigid. Note that, due to Fact 10.9, 
H[ is uniquely defined and contains cltj+1 (xi,...,x\). Because H[ is chosen to 
be maximal, from Fact 10.10 it follows that the rooted graph 
{ν(Η[),ΰ'ί+1) 
is safe. Moreover, since 
\V(H¡)\ < |cl t i + 1(x',,...,x; + 1)| < M(a,k,ti+l) 
= U, 
we have 
V(H[)Cclli(x'1,...,x'i) 
= V(G'i). 
and consequently x'i+1 ^ V(ii,'). 
Now let H" be a subgraph of G" isomorphic to H'¡ C G\. Then the 
second player should use Lemma 10.13 to find a (V(H,0,G'i+1)-extension 
{V(H['),G'/+i) 
of H¡' in G" which does not enlarge the ii+i-closure of V(H['). 
Finally, as x'/+1, he should pick the vertex of (V(H"), G"+1) which corresponds 
tox' i + 1 inG' i + 1. 
■ 
10.3 FILLING GAPS 
In this section we complete the "first-order picture" of G(n,p). Thus, we look 
at the behavior of G(n,p) for probability functions p = p(n) which are "close" 
to the known thresholds, such as p = n~a, where either a = 1 + Ι/ί for some 
natural number £, or a is a positive rational number smaller than one. Finally, 
we discuss briefly what happens when we consider properties of G(n, p) which 
can be expressed in I£ r d, a much stronger language than L~ which has been 
studied so far. 

286 
ZERO-ONE LAWS 
Weak zero-one laws 
We start with the case p < n _ , + o ( , ) when, as we have already learned from 
the previous section, the behavior of G(n,p) is rather easy to study. The-
orem 10.6, which specifies the threshold functions for this range of p(n), is 
nicely supplemented by the following result, which says that at these thresh-
olds the probabilities of sentences from L~ converge, that is, the weak zero-one 
law holds. (For a more thorough analysis of the behavior of G(n,p) at this 
stage of the evolution see Lynch (1990) and Spencer and Thoma (1999).) 
Theorem 10.14. Let p = p(n) be such that one of the foUovaing conditions 
holds: 
(i) n1+l/*p -¥ c, for some natural number i > 1 and a positive constant c, 
(ii) np —> d, for some positive constant d, 
(iii) rnp — log n — s log log n -* c" for some, not necessarily positive, con-
stant c" and integers r, s such that 0 < r — 1 < s. 
Then, for every sentence φ from L^, the probability that G(n,p) satisfies 
ψ converges as n -¥ co. 
Proof. The proof closely follows the way which led us to Theorem 10.6. Let 
ni+i/ip 
_» c > o and ψ be a sentence from L~ of quantifier depth fc. Then, 
o.o.s. G(n,p) is a forest, which contains no trees of size larger than £ + 1, and 
at least fc copies of each tree of size at most i. Thus, our main concern is trees 
of size I + 1 which appear in G(n,p) with probabilities which are bounded 
away from both 0 and 1 (see Theorem 3.19). Let ΤΊ,... ,Tm be the family of 
all pairwise non-isomorphic trees with £ +1 vertices and r\,..., 
r m be natural 
numbers such that 0 < r¿ < fc for 1 < i < m. We say that a graph G has 
property φΐι,ι {r\ ,...,rm) 
if the following hold: 
(a) G is a forest; 
(b) for any tree T on at most i vertices G contains at least fc components 
isomorphic to T; 
(c) G contains no components on I + 2 vertices; 
(d) for all i, 1 < i < m, the number of copies of T¡ in G is r¿ if 0 < r< < fc - 1 , 
and at least fc if r¿ = fc. 
A simple calculation of the moments of an appropriate m-dimensional ran-
dom variable shows that for every sequence r\,...,rm 
the limit 
lim P(G(n,p) f= φ^ι[τλ,... 
,r m)) = A(ri,...,r m) 
exists (see Remark 3.20); as a matter of fact, A(ri,...,rTO) is a sum of 
certain probabilities of a multidimensional Poisson distribution. 
Now let 

FILLING GAPS 
287 
G(r¡,... 
,rm) be a graph for which <£>*,/(Γι>· · ->rm) holds. One can see imme-
diately that, for any other graph G which fulfills ipk,t(/Ί,..., 
rm), the second 
player has a winning strategy in the game Ehrjt(G,G{r\,... ,rm)). 
Thus, 
lim P(G(n,p) t= V ) = 
Y* 
A(r,,...,r m) 
n-+oo 
^—^ 
(ri 
Γ „ ) 
G(r, 
rm)(=V 
and (i) follows. 
The proofs of (ii) and (iii) are similar, although slightly longer and more 
involved, so we omit them here. 
■ 
Zero-one laws and recursive functions 
In order to complete the picture of the first-order properties of G(n,p), it 
remains to study the behavior of G(n,p) for p = p(n) = n_Q+o(1>, where 
Q € (0,1) is a rational number. We remark that for each rational a € (0,1] 
there exists a strictly balanced graph Ha such that d(Ha) = a (Rucinski and 
Vince 1986). Thus, for pna -> c > 0, the probability that G(n,p) contains 
Ha tends to a constant a = a(c) which is neither 0 nor 1 (see Theorem 3.19) 
and the zero-one law does not hold. If p = p(n) = n~ Q +°' l ) the behavior of 
G(n,p) is much more involved and thus, to simplify its description and avoid 
many technical details, we will consider only the special case a = 1/7. 
As observed by Luczak and Spencer (1991), the argument which led us to 
Theorem 10.7 can be modified to show that the zero-one law holds when p is 
slightly larger than n - 1 / 7 . 
Theorem 10.15. Let p = p(n) be such that 
p7n > 7 log η + ω(η) log log n , 
where ω — ω(η) —> oo but p = p(n) = n~1^7+°^. 
Then, for every sentence ψ 
from L^ the probability that G(n,p) satisfies ψ tends either to 0 or to 1 as 
n —> oo. 
■ 
Remark 10.16. The above result is, in a way, best possible. Indeed, for 
every natural number I and np1 = 71ogn + I log log n, the probability of the 
property that G(n, p) contains seven vertices υι,.,.,υγ 
which have precisely I 
common neighbors tends to a constant λ, where 0 < A < l , a s n - > o o 
(Exercise!). Thus, the function ω(η) in Theorem 10.15 cannot be replaced by 
any constant. 
What happens when p = n-l/7+°W 
but np7 = o(logn)? The bold reader 
who has managed to read the book until this point would probably be willing 
to bet that now, as in Theorem 10.6, we specify a spectrum of the thresh-
old functions for different properties, and in between them the zero-one law 
must hold. This is indeed what an experienced graph theorist should expect. 

288 
ZERO-ONE LAWS 
However, as shown by Shelah and Spencer (1988), the truth is much more 
surprising. 
Theorem 10.17. There exists a sentence ψ from L^ such that for every 
p = p(n) satisfying 
n-l/logloglogloglogn < n p 7 < 
ϊ ? 1 ^ 
, ^ ν 
log log log log log n 
we have 
liminfP(G(n,p) |= φ) = 0 
n—*oo 
and 
limsupP(G(n,p) |= ψ) = 1 . 
Π-+00 
Before we say a few words about the proof of Theorem 10.17 we comment 
briefly on the content of this very counterintuitive result. First of all, note 
that the theorem states that the probability of a first-order property can os-
cillate between 0 and 1 even for such odd functions as p' = n~l/7\og~* 
n 
or p" = n-i/7-i/\/iogiogiogn which certainly do not resemble any threshold 
function we have seen so far. Furthermore, there exists one sentence ψ which 
behaves eccentrically for all such functions at the same time. Finally, the 
inequality (10.2) looks very "asymmetric". Indeed, we already know from 
Theorem 10.15 that the zero-one law holds if np7 is substantially larger than 
logn, hence the upper bound for np7 cannot be replaced by a much bet-
ter one (although it is by no means best possible - see Theorem 10.19 and 
Remark 10.20 below); on the other hand the lower bound for np7 given in 
Theorem 10.17 tends to 0 much faster than any power of the logarithm. 
Can we significantly extend the range of p = p(n) for which Theorem 10.17 
holds? More precisely, can we replace in (10.2) the iterated logarithm by 
any function which tends to infinity? The following theorem by Luczak and 
Spencer (1991) answers this question in the negative. 
Theorem 10.18. There exists a function ω — ω(η) which tends to infinity 
as n —> oo such that forp = p(n) = n - 1/ 7 - 1/"' and every sentence rp from L^, 
the probability that G(n,p) satisfies ψ converges either to 0, or to 1. 
Proof. Let a^ be a sequence of irrational numbers which monotonically tends 
to infinity as i -> oo, say, <Zj = t%/2, and let Aa = {01,02,...} and p 0 i = 
„-i/7-i/oj 
Furthermore, let {φι,ψ?,...} 
be the set of all sentences from L~. 
In order to find ω(ή) we first define a decreasing sequence of infinite sets 
-Ao D A\ D A2 2 · · · and a sequence of sentences V>¿ from L~, where for every 
i > 1 either ipi = ψϊ or φί = ->V»· Thus, let us suppose that for some t > 0 the 
set Ai has already been defined. Then, by Theorem 10.7, we can partition A{ 
into two classes, according to whether for a e Ai the formula ^¿+i holds a.a.s. 
for G(n,p„), or a.a.s. G(n,pa) satisfies ->^>t+i- Let Ai+i denote the set of the 
above partition which is infinite (if both are infinite we can pick either of 

FILLING GAPS 
289 
them) and let ψ,+ ι be the sentence from {ip,+ i, ->ψί+1} which holds 0.0.5. for 
G(n,pa), for all a € -4¿+i. Moreover, choose a< e Ai such that a\ < c*2 < ■ ■■ 
and select n¿ in such a way that 1 < ni < nj < ... and for every j < i and 
n > rii 
P ( G ( n , p Q . ) | = ^ ) > l - l / i · . 
Now, it is enough to set ω(η) — a, for each n¿ <n < rij+i, i > 1. 
■ 
It turns out, however, that each function ω(η) which fulfills the assumptions 
of the above theorem tends to zero unimaginably slowly. Before we make this 
statement precise, let us recall that a recursive function / : N -> N is a 
function such that there exists a procedure which for every n computes the 
value of /(n) in a finite time using a deterministic Turing machine. (The 
reader who is not familiar with this notion is advised either to look at one of 
the equivalent definitions of a recursive function in any book on the theory 
of computation, or accept Church's thesis and view a recursive function just 
as a function which can be effectively computed.) Somewhat surprisingly at 
first sight, recursive functions play an important role in the analysis of the 
behavior of G(n,p). 
Theorem 10.19. For every recursive function ω = ω(ή) which tends to in-
finity as n —* 00, and every p = p{n), satisfying 
n'l,u 
< np7 < logn/ω , 
(10.3) 
there exists a sentence ψ from L~ such that 
liminfP(G(n,p) (= ψ) = 0 
n—foo 
and 
HmsupP(G(n,p) (= φ) = 1 . 
n—>oo 
Remark 10.20. The fact that Theorem 10.18 remains true if we replace 
n-i/iogiogiogiogiogn b y n-i/u/ ) w h e r e t h e r e c u r s i v e function ω tends to infinity 
as n -> 00, was observed by Luczak and Spencer (1991) (note, however, that 
now V does depend on ω). This lower bound for np7 is best possible in the 
sense that, as Theorem 10.18 shows, we cannot omit the assumption that u> is 
recursive. The fact that log n/log log log log log n in (10.2) can be replaced by 
logn/ω, where ω is a recursive function which tends to infinity, follows from 
the original argument of Shelah and Spencer (1988). Let us also mention that 
at this moment it is not known whether the assumption that ω is recursive is 
really necessary. It is even possible that the assertion of Theorem 10.19 holds 
as long as np7 < (7 - e) logn for some e > 0 (see Theorem 10.15). 
Although we will not give here complete proofs of Theorems 10.17 and 10.19 
(which are long and technical), we sketch the main idea of the argument so 
that the somewhat unexpected appearance of recursive functions will become 

290 
ZERO-ONE LAWS 
clear. Let us suppose that p = n~'/ 7 -'/ u', where ω — u>(n) tends to in-
finity slowly, say, slower than log log log n. For each subset {v\,...,v7} 
of 
vertices of G(n,p) let N(v\,. ..,v7) 
denote the set of common neighbors of 
v\,... ,vy. It turns out (Exercise!) that a.a.s. the maximum of |iV(vi,... ,v7)\ 
over all choices of v\,..., 
v7 differs from ω by no more than 4. Further-
more, a.a.s. for any two sets {vi,...,v7} 
and {v[,...,v7} 
and every sym-
metric binary relation R on N(v\,.. 
.,v7) U N(v[,... 
,v'7) with fewer than 
2ω elements, there exist elements i i , 12, *3ι χ4 
and j/i, ]/2,...,y2o such 
that zRz' if and only if the set N(z,z',xi,X2,X3,X4,y¿) is non-empty for 
some j/j, where 1 < i < 20. Since, clearly, \N(vy,,.. .,v7)\ 
> \N(v[,.. .,v'7)\ if 
and only if there exists a "matching" binary relation between N(v\,..., 
v7) \ 
N(v[,..., 
v7) and N(v[,..., 
υ'7) \ Ν{υχ ,...,υ7) 
which saturates each element 
from N(v[,... 
,v'7) \ N{v\,... 
,υ7), it is possible to write a formula in L^ 
which would mean "\Ν(νλ,.. 
.,v7)\ 
> \N(v[,... 
,v'7)\". 
Consequently, it is 
possible to "identify" in L^ one of the subsets {v\,... 
,υ7} which maximizes 
\N(vi,. ..,v7)\, 
in other words, it is possible to define a predicate P in L~ 
such that P(x) holds if and only if x belongs to some A = N(vi ,...,v7), 
where 
for each choice of v[,... ,v'7 we have \N(vi,...,v7)\ 
> \N{y[,...,v7)\. 
In a 
similar way one can show that a.a.s. for each symmetric ternary relation T on 
A with at most 9ω elements there are vertices x\, X2, 13, and yi, y2, · ·· ,y9o 
such that T(z,z',z") 
holds if and only if N(z,z',z",xi,X2,X3,yt) φ 0 for 
some i, 1 < i < 90. Thus, each such relation T could be, in principle, ex-
pressed in L~. One can use a few additional technical tricks to show that 
there exists a subset S C A in G(n,p) which can be defined using L~, such 
that \S\ > \A\1¡3/2 > u;1//4 and each ternary relation on A can be "encoded" 
in L 
Thus, we have arrived at the following result (in the statement we 
include all functions p = p(n) for which (10.3) holds, although when np7 does 
not tend to 0 fast enough the encoding procedure is more involved). Here, for 
simplicity, we use υ = (v\,..., 
vm) to denote sequences of vertices of length m, 
where m does not depend on n. 
Lemma 10.21. There exist formulas P*(x,v) 
««d T*{x,y,z,v) 
in L~ such 
that if ω = ω(η) < logn is any function which tends to infinity as n -► 00 
and 
η" 1 / ω < np7 < logn/ω , 
then a.a.s. for every choice ofv = vs the setS(y) = {w : P*{w,v)} 
of vertices 
o/G(n,p) has the following two properties: 
(i) either S(v) = 0, or ω1'* < \S(v)\ < n, 
(ii) for every ternary relation T defined on S(y) there exists yj such that if 
x,y,z 
6 S{v), then T(x,y,z) 
holds if and only ifT*(x,y,z,vT) 
is true 
/orG(n,p). 
Furthermore, a.a.s. there exists υ such that S(v) φ 0, that is, \S{v)\ > 

FILLING GAPS 
291 
Before we sketch the proofs of Theorems 10.17 and 10.19, we state one 
more fact about recursive functions. 
Lemma 10.22. Let g : N -» N be any function such that for some non-
decreasing recursive function f : N —> N which tends to infinity as n —» oo 
we have f(n) < g(n) < n for n > 2. Then there exists a recursive function 
h : N -► N such that /i(p(N)) contains all natural numbers. 
Proof. For any non-decreasing unbounded recursive function / : N —► N such 
that f(n) < n for n > 2, define / " : N -* N, setting 
r ( n ) = min{/(/( ; :_/(n)...)) = l } . 
k 
Obviously, /* is recursive and non-decreasing. Moreover it tends to infinity 
as n -^ oo but much slower than / . We leave to the reader the elementary 
calculations which show that the assertion holds for h — f* (Exercise!). 
■ 
Sketch of the proofs of Theorems 10.17 and 10.19. Let us suppose that the 
condition (10.3) holds for p = p(n). Furthermore, to simplify the argument 
slightly, let us assume that the recursive function ω(η) which appears in the 
assumptions of Theorem 10.19 is non-decreasing, and set f(n) = [w(n)]1/8. 
Then, by Lemma 10.21, a.a.s. G(n,p) contains a set 5, such that /(n) < \S\ < 
n, which can be encoded by P*(-,vs) 
for some vs. Furthermore, any ternary 
relation T on 5 can be encoded in L~ by the predicate Τ*(·, ·, ·,?;τ) for some 
sequence vj. Thus, for instance, there exists v- and w such that for all x, y, 
z, for which P*(x,uS), 
P*(v,vs), 
P'(z,vs), 
we have: 
(i) if Tm(x,y,w,v?) 
&ndT'(y,z,w,vß), 
then 
T*(x,z,w,vß); 
(ii) if T*(x,y,w,v-) 
and T*(y,x,w,v-), 
then x = y; 
(iii) either T*(x,y,w,v-), 
or T*(y,x, 
w,v-). 
Hence, « - and w define on S a linear order. Now, we can use Lemma 10.21 
to find an element v+ such that for x, y, z, for which P*{x,vs), 
P*(y,r¿s), 
P*{z,vs), 
the relation T*(x,y,z,v+) 
holds if and only if x + y = z, where 
i , y, z denote the positions of elements x, y and z, respectively, in the order 
determined by ¡¿- and w. Indeed, it is enough to require that the relation 
T*(x,y,z,v+) 
has all the properties which hold for the sum of two natural 
numbers; for instance, if T*{x,y,z,v+), 
then T'{x,F(y),F(z),v+), 
where 
F(y) and F(z) stand for the elements which in the linear order determined by 
and w succeed y and z, respectively. Clearly, in a similar manner we can 
encode in such a linearly ordered set S any recursive function. In particular, if 
h is a function whose existence is assured by Lemma 10.22, there exists vh such 
that for i, y, z with P'{x,vs), 
P*(y,vs), 
P'(z,vs) 
we have 
T*(x,y,z,vh) 
if and only if y = h(x). 
Now let s be a maximal element from S, that is, 

292 
ZERO-ONE LAWS 
P'(s,v_s) 
and for each i such that P'(x,vs) 
and T'(x,s,w,v-) 
we have 
x — s. Furthermore, let φ be the sentence 
3„s3„< 3„,3„+3j,(.3,3χ3ν a(t¿s, y-, w, v+, vft, s) 
Λ P*(x,?¿s) Λ P*(t/,2¿S) Λ Γ"(β,ι,ι,ϋ Λ) AT*(W,y,í,ii+) , 
where a is a long and complicated formula which contains the definition of S, 
the axioms of linear ordering, the axioms of addition, the definitions of h and s, 
and so on. The above φ belongs to L^ and expresses the property "ft(|S|) is 
even". Hence, by Lemmas 10.21 and 10.22, a s n - > o o the probability that φ 
holds for G(n,p) oscillates between 0 and 1. 
■ 
The idea of encoding the initial segments of the natural numbers by sub-
structures of a random structure has been widely used in the literature to 
show that the zero-one law does not occur. Typically, in such cases some sort 
of "recursive bound behavior", similar to that described by Theorems 10.18 
and 10.19, can be observed. A good example of such a phenomenon is the fol-
lowing theorem of Dolan and Lynch (1993), who ingeniously used the strength 
of the language L°Zd to encode the initial segments of the natural numbers 
even in random graphs with very few edges. 
Theorem 10.23. 
(i) If p = p(n) is such that n2p -> c as n -» oo for some constant c, then 
for every φ from L°Za the probability that G(n, p) satisfies ψ converges 
as n —y oo. 
(ii) There exists a function p — p(n) such that n2p -¥ oo and for every φ 
from L™d the probability that G(n, p) satisfies ψ converges. 
(iii) Ifp = p(n) < 1/2 but n2p > /(n) for some unbounded recursive function 
f : N -► N, then there exists a sentence ψ of L™d such that 
liminfP(G(n,p) ^ φ) = 0 
n—»oo 
and 
limsupP(G(n,p) \= φ) = 1 . 
■ 
10.4 SUMS OF MODELS 
In this part of the chapter we will study the behavior of random structures 
using yet another approach: a sum of models. Although it does not contribute 
much to the picture of G(n, p), which is now nearly complete, it is a simple yet 
very convenient tool which can be used to obtain different types of zero-one 
laws for various random structures. Because of the character of the book, 

SUMS OF MODELS 
293 
we present here only the simplest version of a more sophisticated notion (see 
Luczak and Shelah (1995), where more general sum schemes were used to 
study the behavior of the random graph G(n,p) defined below). 
Sum schemes 
Throughout this section L will denote a first-order language with predicates 
P\,...,Pt, 
all of them binary, and OT will stand for a family of models of 
L with binary relations R\,...,Rt. 
Suppose that we are given a ¿-tuple of 
pairs of zeros and ones r = ((rf ,rf),... 
,(Tt~iTt~))· 
A binary operation 
®:97tx9Jt-»97tisa sum scheme for 971 with signature τ if for all Mi, Μ2 € 971 
the set of elements of M = Mi ΘΜ2 is a disjoint sum of the set ί/i of elements 
of Mi and the set U2 of elements of M2, and for every i = 1,2,...,t the 
following hold: 
(i) if x,y G Ur, r = 1,2, then xRiy in M if and only if xRiy in Mr; 
(ii) if x e Ui and y € U2, then xRiy when τ* = 1, and -^xRiy whenever 
T* = 0. Similarly, for such x, y, we have yRiX for r~ = 1, and ->yRiX 
otherwise. 
Note that the operation Θ is associative but typically it is not commutative 
(unless r+ = r¿~ for all i = 1,..., t). 
Example 10.24. Let Gi and G2 be graphs with vertex sets [ni] and [TI2], 
respectively. Define G = Gi 4- G2 as a graph with vertex set [n\ + «2], such 
that {x, y} is an edge of G if and only if either 
• 1 < x, y < τ»ι and {x, y} is an edge of G\, 
or 
• ni + 1 < x,y < m + n2 and {x - nx,y - n¡} is an edge of Gi-
Thus, roughly speaking, to obtain G one should put G2 behind G\ preserving 
the order of vertices in each of the graphs. Clearly, the operation 4- is a sum 
scheme for graphs viewed as the models of either L^ or I£ r d, with signature 
((0,0)) for L~ and ((0,0),(0,1)) for L°Zá-
The relevance of sum schemes for the study of first-order properties is based 
on the following simple observation. 
Lemma 10.25. If ® : Wi x Wl -* VJl is a sum scheme for OT, then for 
all Mi,M 2 6 Wl the set Th t(Mi φ Μ2) can be computed from Thfc(Mi) 
and Th/k(M2), i.e., Τ1ι*(Μχ) = Thfc(M{) andThfc(M2) = Th t(M¿) implies 
Thk(M1 φ M2) = Th t(M| φ Μ2). 
Proof. Suppose that Thk{Mi) 
= Thk(Afi) and Thfc(M2) = Thk(M2). 
Thus, 
according to Lemma 10.3, the second player has winning strategies in both 
Ehrit(Mi,Mi) and Ehrfc(M2,M2). 
But then, he can win also ΕηΓ*(Μχ φ 
M2,M[ φ Μ'2) using his winning strategy for Ehrjfc(Mi,M{) whenever the 

294 
ZERO-ONE LAWS 
first player selects a vertex from either M\ or M\, and playing according to 
his winning strategy for Ehrjt(M2, M'2) whenever the first player picks a vertex 
from Mi or M'2. Thus, the assertion follows immediately from Lemma 10.3. ■ 
Let k be a natural number and ® : 971 x 971 —> 971 be a sum scheme for 
a family of models 971. A model M € 971 is (971, ©,fc) -persistent, or simply 
persistent, if for every M' G 971 we have Th*(M e M ' © M ) = Thjt(M). We 
will show that for every sum scheme such a persistent model can be found. 
Theorem 10.26. Every family of models 97t with a sum scheme © : 971x971 -¥ 
97t contains at least one (97t, ©, k)-persistent model for every natural number k. 
The proof of Theorem 10.26 will rely on the following property of the 
operation Θ. 
Lemma 10.27. For every k there exists m = m(k) such that if φ : 971 x 971 -► 
971 is a sum scheme for 971, then for every model M € 971 we have 
Tht(M Θ · ·· θ Μ) = Th t(M©---®M). 
m times 
m+1 times 
Proof. We will show that the assertion holds if we put, rather crudely, m(k) = 
3* + 1. Thus, let G' = 0 ^ ! M[ and G" = φ ^ 1 Mi'. 
w h e r e M{,..., Μ^ and 
Λί",..., M^,+i are identical copies of M. We will describe a winning strategy 
for the second player in the game Ehrjk(G',G"). Let us suppose that in the 
first i steps of the game the players choose vertices from copies Μ'Τχ,..., M'Ti 
and Μ"χ,...,M"., 
respectively. Then if the first player in the (i + l)-th step 
picks a vertex υ from M¿.+1 [or M".+i] the second player should select the 
corresponding vertex v from M" 
[M'ri+t] chosen in such a way that 
(a) si+1 = ri+1 if ri+l < 3*"1 [or si+l < 3*"*]; 
(b) m + 1 - si+i = m - ri+i if m - r¡+i < 3 * - ' [or m + 1 - si+i < 3*-*]; 
(c) Sj+i — Si> = Tj+i - Tj- if for some 1 < i' < i we have \ri+i —ri<\ < 3 * - ' 
[ 0 Γ | β < + ι - . 4 . | < 3 * - ' ] ; 
(d) Si+i < Si> if and only if ri+i < Ti>, for all i' = 1,2,..., i. 
As in the proof of Corollary 10.4, one can check (Exercise!) that such a strategy 
is consistent, that is, if the second player follows it from the beginning of the 
game he can always find a suitable element Si+i [or r<+i] in the (i + l)-th 
move of the game. Thus, at the end of the game, for all 1 < i, j < k we have 
Ti < Tj if and only if s¿ < Sj, and the second player wins. 
■ 
We will need also the following elementary result on finite semigroups. 

SUMS OF MODELS 
295 
Lemma 10.28. Let S be a finite semigroup for which there exists a natural 
number m such that s"' = sm+l 
for every s G S. Then S contains an element 
w such that wuw = w for every u G S. 
Proof. For x,y G S let us write x ■»-♦ y if for some s G S we have y = xsx. 
Set Az = {y G S : z ~» ¡/} and let /I be a minimal subset in the family 
{J4Z : z G 5}. Since the relation "■*-»" is transitive, for every x 6 -4, we have 
x ~* y if and only if y G Λ, and so for every x G Λ we have J4 = A^ Thus, to 
verify the assertion, it is enough to show that A consists of only one element. 
Since we may assume that m > 3, for every x G A we have xxm~2x 
— 
xm = i m + 1 g A. Observe also that the choice of A ensures that for each 
i G A the function fx : A -► A : y i-> xmyxm 
is a bijection, that is, for every 
i G A we have 
,4 = xmAxm 
= {xmyxm 
: y G A}. 
(10.4) 
Indeed, clearly xmAxm 
C Λ, and for every z £ A, there exists s G S such that 
2 = i m s x m = x m + 1sx m + 1 = xm{xsx)xm 
= x mi/x m, 
where y = xsx G i4. 
Now let x, y G A. Then, y = xsx for some s G S, and so xy = x(xs)x G A 
Furthermore, 
x myx m = xm+1yxm 
= 
xm(xy)xm 
and, because of (10.4), y — xy. Similarly, 
ymxym 
= 
ym{xy)ym 
and so y = xy = x. Hence, A consists of only one element w and the assertion 
follows. 
■ 
Proof of Theorem 10.26. Let us notice first that, due to Lemma 10.25, for 
a given k, the sum scheme Θ induces an operation θ acting on the set 
Ξ*(9Η) = {Thjt(M) : M G 501}. Since the operation Θ is associative, so is Φ, 
and consequently (Ξ*(3Κ),Θ) is a finite semigroup. Due to Lemma 10.27, 
this semigroup fulfills the assumption of Lemma 10.28 and thus contains an 
element w = Th*(M) such that for every M' G 971 
Th t(M) = Thfc(M)®Thfc(M')®Thfc(M) = Thfc(M ® M' φ M). 
U 
Remark 10.29. For a language L in which all predicates are symmetric, as in 
the case of the first-order language of graphs L~, Theorem 10.26 can be proved 
in a much simpler way. Indeed, one can easily see that then Lemma 10.27 holds 
with m(k) = k. Furthermore, the semigroup (Ξ*(9Π),φ) is commutative, and 
so as a persistent element one can choose 
( Π *)"■( Π *)'■ 

296 
ZERO-ONE LAWS 
The random graph G(n,p) 
In the previous sections of this chapter we have characterized quite precisely 
for which p = p(n) in G(n,p) the zero-one law holds; now we use sum schemes 
to study the behavior of a slightly different type of random graph, denoted by 
G(n,p). Let p be a sequence of probabilities {pi}^- 
Then, G(n,p) is a graph 
on the vertex set [n] obtained by joining each pair of vertices 1 < k 
<(<n 
with probability pt-k, independently for each such pair. Thus, in particular, 
G(n,p) is a special case of G(n,p) when all terms of the sequence p are equal 
to p. On the other hand, we will consider asymptotic properties of G(n,p) 
when n tends to infinity but the sequence p does not depend on n, thus it 
is not possible to deduce our previous results about G(n,p) from those given 
below on G(n,p), unless p does not depend on n. 
In order to simplify slightly our considerations we will study the behav-
ior of G(n,p) only for sequences p = { p » } ^ for which 0 < p¿ < 1/2. The 
assumption that p > 0 helps to avoid some pathological cases when, for ex-
ample, p is selected in such a way that no triangles are allowed in G(n,p); 
let us mention, however, that Theorems 10.31, 10.33 and 10.34 below remain 
true also for sequences which contain zero terms (Luczak and Shelah 1995). 
The upper bound for p¿ takes care of another troublesome situation when p 
contains some ones or terms which quickly approach one as i -* oo. It is easy 
to see that, under the above assumptions, a.a.s. the random graph G(n,p) 
contains many copies of any given subgraph H. Following Luczak and She-
lah (1995) we will show that if the terms of p tend to 0 fast enough, then a 
stronger result holds: a.a.s. G(n,p) contains a copy of H of a very special 
type. 
Thus, let H and G be graphs with vertex sets [m] and [n], respectively. We 
say that a subgraph H' of G spanned by vertices i + 1,..., i + τη is an exact 
copy of H if for all 1 < k < I < m the pair {k, i) is an edge of H if and only 
if {i + Jb, i + £} is an edge of G, and no vertex from H' is adjacent to vertices 
outside H'. (Thus, H' is an exact copy if it is an order-preserving induced 
copy of H on consecutive vertices which, moreover, is a sum of components 
of G.) Furthermore, such an exact copy of H' is separating if no vertex i' < i 
of G is adjacent to a vertex i" > i + m + 1, in other words, one can write 
the graph G as a sum G' + H' + G", or, maybe, G' 4- H' or H' + G" (for the 
definition of G\ + G-i see Example 10.24). 
Lemma 10.30. Let p = {pi}^ 
be such that 0 < pi < 1/2 for every i > 1, 
and let H be a graph with vertex set [m]. 
(i) //2?=iP» = °0°gn)> f*en a.a.s. G(n,p) contains an exact copy of H. 
(ii) // the series Σ^ΐρί 
converges, then a.a.s. G(n,p) contains an exact 
separating copy of H which contains no vertices with labels larger than 
mfn°-9l· 
Proof. Let us consider a family Λ of disjoint sets At = {(i-l)m 
+ l,... 
,im], 
where ί = 1,..., [n°
 9 ] . The probability that a given set At from A spans an 

SUMS OF MODELS 
297 
exact copy of H is given by p(A() = p(H)p'(Ai), where the first factor, 
P(H)= 
Π 
P O ' - J ' D 
Π 
( i - p ( | i ' - j ' | ) ) > o , 
e={i,j}€E(H) 
e' = 
{i\j')eE(H) 
remains the same for all ί, while the second one, 
tm 
(í-l)m 
n 
p'(At)= 
Π 
Π 
(l~Pr-s) 
Π 
( 1 - Λ - ) · 
r = ( / - l ) m + l 
s=l 
t=¿m+l 
may depend on I. However, from our assumptions it follows that 
¿(At) > (f[(l 
~Pi¡) 
> e x p ( - 2 m f > ) > n"005, 
so there exists a subfamily A' of A with, say, \n° 7] elements, such that for 
every A € A' the probability p'(A) is roughly the same, that is, for some 
function /(n), where n~° °5 < /(n) < 1, and every A € .4' we have 
/(n)(l-o(n- 0 1))<p'(>l)</(n). 
Let X denote the number of those sets from A' which span exact copies 
of H. Then, the expectation of X is given by 
EX = Σ 
J*A) = i1 + o(n-°x))p(H)f(n)n0-7 
> n 0 6 , 
A€A' 
whereas, for the second factorial moment of X, we get 
E[X(X-l)} 
= £ 
p(A)p(B)/lH[(l-Pu_^). 
Α,Β&Λ' 
ieAjeB 
A^B 
In order to estimate E[X(X - 1)], note that among the first n terms of every 
sequence p which fulfills the assumptions of Lemma 10.30 one can find at most 
n025 terms larger than n~°2. Thus, for all except at most 2mn025\A'\ < n 
pairs A, B € A' we have 
Π Π ^ -Pli-il) > (1 - " - ° · 2 Γ > 1 - n - 0 1 . 
»6Aj€B 
Furthermore, for all A, B ζ A' 
Π I I · 1 - Pli-il) ^ í l · 1 - P')2"1 ^ e x P ( - 4 m ¿ P i ) > n" 0 0 5 . 
Thus, 
E[X(X-l)] 
< η1·4/2(η)(1+0(η-01))+η/2(η)7ΐ005 = (EX)2(l+0(n~01)) 
, 

298 
ZERO-ONE LAWS 
and so for the variance of A' we get 
VarX = E\X{X - 1)] + EX - (EX)2 
= o((EX) 2). 
Consequently, a.a.s. X > EA'/2 > 0, due to Chebyshev's inequality. 
The second part of the lemma can be shown in a similar way, but now the 
probability that At € A is an exact separating copy of H is the product of 
three factors p(H), p'(Af) and p"(Ai), 
where 
{l— \)m 
n 
n 
,
0
0 
p"(At)= π 
Π u-?>-<)> Π^1 - w)'> « P (-2 £» f t 
is bounded away from zero whenever the series £2Si *Ρ« converges. As we 
have done before, one must choose a family A" of sets for which the probabil-
ity p(H)p'\A)p"(A) 
is roughly the same for all A € A" and use Chebyshev's 
inequality to show that a.a.s. at least one set from A" spans an exact sepa-
rating copy of H. Since the proof follows very closely the previous argument, 
we omit it here (for details see Luczak and Shelah (1995)). 
■ 
Zero-one laws for G(n,p) 
One can easily deduce from Theorem 10.26 and Lemma 10.30 that the zero-
one law holds for G(n,p) whenever the sequence p tends to 0 quickly enough. 
Theorem 10.31. Let a sequence p = {pi}~i be such that 0 < p¿ < 1/2 for 
i > 1 and 
n 
Y^Pi = o(logn) . 
Then, for every sentence ψ from L~ the probability that φ holds for G(n, p) 
tends either to 0 or to 1 as n —► oo. 
Proof. Let p fulfill the assumptions of the theorem and φ be a sentence of 
quantifier depth A: from L~. From Theorem 10.26 we know that there exists a 
graph Hk such that for every graph G_we have Thk{Hk +G+Hk) 
= T h r i f t ) . 
But Lemma 10.30 implies that a.a.s. G(n,p) contains an exact copy of Hk 4-Hk 
and thus a.a.s. Thfc(G(n,p)) = Thk(Hk), 
that is, if Hk satisfies φ, then 
lim P(G(n,p) (= φ) = 1 
n-»oo 
otherwise 
lim P(G(n,p) \= φ) = 0 . 
■ 
n-+oo 
Remark 10.32. As was observed by Luczak and Shelah (1995), the above 
result is best possible in the following sense. Let ε > 0 and φ' be a sentence 

SUMS OF MODELS 
299 
from L^ which says that a graph contains no components with [10/e] vertices. 
Then, there exists a sequence p' = {ρ\)%χ 
such that 0 <_p· < 1/2 and for 
every n > 2 we havej^" =, p\ < elogn, but liminf«-,«» P(G(n,p') \= rp') = 0 
while limsupn^^ P(G(n,p') \= φ') = 1. 
What happens when, instead of L~, we consider the stronger language L™d? 
Observe that if ψ is the first-order statement which says that the vertex with-
out predecessors (i.e., 1) is adjacent to its immediate successor (i.e., 2), then 
for every n > 2 
0 < P ( G ( n , p ) M ) = p i 
< l / 2 , 
in other words, for L"d the zero-one law does not hold. However, if the terms 
of p tend to 0 fast enough, then for every sentence ψ from UZd the probability 
that G(n,p) satisfies rp converges. 
Theorem 10.33. Let p = { p t } ^ denote a sequence such that 0 < Pi < 1/2 
for every i > 1 and the series Σ°1γ ipi converges. Then, for every sentence ψ 
from L°Zá, the probability that φ holds for G(n,p) converges as n -¥ oo. 
Proof. As usual, let ip denote a sentence of L°ZA of quantifier depth k and let p 
fulfill the assumptions of the theorem. We will show that {P(G(n,p) \= Φ)}^=ι 
is a Cauchy sequence, and so it must converge. 
Let {G (n,p)}^=1 
be a sequence of random graphs constructed in the fol-
lowing way. Take G (l,p) to be a graph with a single vertex 1. For n > 1, 
define G (n + l,p) in the following way: 
(i) if 1 < i < j < fn/2], then the pair {i,j} is an edge of G (n + l,p) if 
and only if {z,.;} is an edge of G (n,p); 
(ii) if fn/2] + 1 < i < j < n + l, then the pair {i,j} is an edge of <5'(η + 1,ρ) 
if and only if {i - 1, j - 1} is an edge of G {n,p); 
(iii) if 1 < i < [n/2] < j < n + 1 then {i,j} is an edge of G'(n + l,p) with 
probability Pj_i, independently for each such pair. 
Clearly, the n-th element of the Markov chain {G (n,p)}^=i 
can be identified 
with the graph G(n,p), and so we may assume that G(n,p) = G (n,p). Let us 
also remark that, of course, a much easier way of obtaining G(n + l,p) from 
G(n,p) is to add to G(n,p) a new vertex n + l and join it to the vertices of 
G(n,p) with appropriate probabilities. Nonetheless, our method of inserting a 
new vertex in the middle of G(n,p) has one crucial advantage: the subgraphs 
induced in G(n,p) and G(n + l,p) by the vertices with small labels are the 
same and, furthermore, the subgraphs spanned by the vertices with large 
labels in both graphs are identical as well. 
Let Hit denote a persistent graph with, say, m vertices, whose existence is 
assured by Theorem 10.26; thus, for every graph G we have Th*(/i* + G + 
Hie) = Thk(Hk). 
From Lemma 10.30 we know that the probability that a 

300 
ZERO-ONE LAWS 
separating exact copy of Hk appears somewhere at the beginning of G(n, p) 
tends to 1 as n -> oo and, similarly, by symmetry, the probability that one 
can find a separating exact copy of Hk somewhere near the end of G(n,p) 
tends to 1 as well. More precisely, with probability larger than 1 — ε'(η), for 
some function e'(n) such that lim„-+00 ε'(η) = 0, G(n,p) can be represented as 
G i 4- H'k + G2 + Hk + G3, where H'k and Hk are exact separating copies of Hk, 
and H'k contains no vertex with label larger than m\n°
 
9 ] , while no vertex with 
label smaller than n + 1 - mjn°-9] belongs to Hk. 
Now, for n' > n, let us 
consider the random graph G(n',p) viewed as a stage of the Markov chain 
described above, that is, we assume that G(n',p) was obtained from G(n,p) 
in a sequence of "middle vertex insertions". Clearly, if in this process we have 
inserted no edge {i,j} such_that j-i 
> n/2-m\n0·9} 
and either i < m[n 0 9l or 
j > n+l-m\n09], 
then G(n,p) can be represented as 
Gi+Hk+G'2+H'¿+G3 
where G\ and H'k are the same as in the decomposition of G(n,p), and Hk 
and G3 are shifted to the right by n' - n. Furthermore, the probability that 
G(n',p) contains edges {i, j) which can corrupt this decomposition is bounded 
from above by 
2mrn 0 9l 
¿ 
P Í < Σ 
iPi = e"(n), 
t=rn/2l-mfn°»l 
¿=ln/3J 
where ε"(η) tends to 0 as n -* 00. 
Hence, using the fact that Hk is persistent, with probability at least 1 -
ε'(η) - e"(n), we have 
Thfc(G(n',p)) = T M G i + H'k + G'2 + H'k' + G3) = Th*(G, + Hk + G3) 
= Th t(G, 4- H'k + G2 + H'k' + G3) = Thfc(G(n,p)). 
In particular, for n' > n, 
I P(G(n',p) \= φ) - P(G(n,p) J= V)l < e'(n) + ε"(η), 
where ε'(η)+ε"(η) 
-> 0 as n -+ 00. Thus, the sequence {P(G(n,p) (= tA)}^=i 
is Cauchy and so it must converge. 
■ 
Let us mention one more result on the first-order properties of G(n,p). 
Luczak and Shelah (1995) showed that for every ε > 0 and every sequence 
p = {pi}^! there is a sentence ψ from L™d, and a sequence p' = {p'¡}"i 
obtained from p by adding to it some number of zero terms, such that 
limsupP(G(n,p') |= i/>) - liminf Ρ ^ η , ρ ' ) \= φ) > 1 - ε . 
η-Κ5θ 
η-*οο 
However, for their construction one cannot put ε = 0 unless the series Y^/i Pi 
diverges. It is somewhat surprising that, in fact, if ¿ZiPi < 00, then for no 
sentence \p of ¿^rd the sequence {P(G(n,p) ^= rp}^ 
has both 0 and 1 as 
concentration points; that is, the following defect zero-one law holds. 

SEPARABILITY AND THE SPEED OF CONVERGENCE 
301 
Theorem 10.34. Let p = {p¿}£, be a sequence such that 0 < p{ < 1/2 for 
every i > 1 and the series Σ ~ ι P» converges. Then, for every sentence φ 
from L°Za, 
limsupP(G(n,p) \= φ) - liminf P(G(n,p) (= φ) < 1 . 
η-κ» 
n-*°° 
Proof. Let φ be a sentence of quantifier depth A; and //* be a persistent graph 
on m vertices whose existence is implied by Theorem 10.26. The probability 
that G(n,p) can be represented as H'k + G + H'¿, where H'k and H'k' are exact 
separating copies of Hk, is, for n > 3m, given by 
n - l 
\2 
m , p ) = / í * ) ) 
n ( 1 - P * ) m Í n < 2 m ' 2 Í ' n " Í } 
P(G(m,p) = H i f c)('n( 1-P«)) 
> Ρ(0(τη,ρ) = / ί * ) β χ ρ ( - 2 τ η ^ ρ ί ) 
, 
»=i 
-I 
and so it is larger than a suitably chosen constant ε > 0 which depends on 
both φ and p but not on n. Hence, because of the persistence of Hk, if Φ 
holds for Hk, then 
liminf P(G(n,p) \= φ) > ε, 
n—»oo 
and if Hk is not a model for φ, then 
lim sup P(G(η,ρ) (= φ) < 1 - ε . 
■ 
n-»oo 
Finally, we remark that there are other types of zero-one laws we did not 
consider in this chapter. For instance, one can ask when the difference P(G(n+ 
1>P) (= Φ) — Ρ(&(η,ρ) |= Φ) tends to 0 as n -> oo for every sentence φ of 
L~ or L°ZA (Shelah 1996), or study the convergence of {P(G(n,p) \= 
φ))^=ι, 
when n is chosen from some dense subset of the natural numbers (Lynch 1993). 
We should also mention that we have emphasized methods of combinatorial 
and probabilistic flavor which are not, by any means, the only ways to study 
zero-one laws for combinatorial structures (see, e.g., the survey of Compton 
(1988)). 
10.5 
SEPARABILITY AND THE SPEED OF CONVERGENCE 
If we can show that a random structure satisfies the zero-one law another 
problem immediately emerges: is it possible to determine effectively which 
properties hold for the random structure a.a.s. and which a.a.s. do not? As 
a matter of fact, such a separability question can be asked even when the 

302 
ZERO-ONE LAWS 
zero-one law does not hold. Thus, we would like to find an algorithm which, 
given a function p = p(n) and a sentence ψ from L, puts φ into one of two 
disjoint classes: one which contains all sentences of L which hold for G(n,p) 
a.a.s., and the other, which contains all sentences which a.a.s. do not hold 
for G(n,p). The sentences that are not of the above two types may be placed 
in either class, provided only that the decision will be reached after a finite 
number of steps - the algorithm is not allowed to run forever. 
Does there exist a separating procedure for G(n,p) and the language L^, 
in the case in which p = p(n) is one of the functions listed in Theorems 10.5, 
10.6, 10.7 or 10.15, where we know that the zero-one law holds? At first sight 
the positive answer seems to follow directly from the method we used in the 
proofs. Indeed, in each case we have employed a similar scheme. Given a sen-
tence φ of quantifier depth k we first determined a finite family of sentences 
Φ* = {Vt}i€/ s u c n that each sentence from Φ* holds a.a.s. for G(n,p). Then 
we showed that for any two graphs G' and G" such that both of them pos-
sess all properties from Φ* we had Th*(G') = Th*(G"), that is, we verified, 
using Lemma 10.3, that Φ* was a complete system of axioms for the theory 
containing all sentences of L of quantifier depth at most k. Now let G' be 
a graph such that G' satisfies ψ^ for every ψχ € Φ*; clearly G' can be found 
by examining one by one all graphs on n vertices, as n ranges from one to 
infinity. Then, it is enough to check whether ψ holds for G': if the answer is 
positive, then a.a.s. φ holds for G(n,p), otherwise a.a.s. G(n,p) is a model 
for -<ψ. 
Before we state the above result in a rigorous form, we need to make a 
comment of a somewhat technical nature. In order to decide on the limit 
probability that G(n,p) is a model for φ, the algorithm must be given both rp 
and the function p = p(n) in a form such that one can study the behavior of 
p(n) for large values of n. However, instead of struggling with the problem of 
how to present a function p : N -► R in a finite and accessible way, we will cheat 
a little bit and assume that we know the asymptotic behavior of p = p(n). 
Thus, for instance, in the case of a function p = p(n) for which the assumptions 
of Theorem 10.6(i) hold, we assume that there is a way to compute I such 
that n - 1 - 1 / ' < C p < n
- 1 - l / ( ' + 1 \ and use the value of / to construct the 
appropriate complete axiom system Φ^ in order to check whether a.a.s. ψ 
is satisfied in G(n,p). This idea works well for functions p = p(n) which 
fulfill the assumptions of one of Theorems 10.5, 10.6, or 10.15. When p(n) = 
n-a+o(i) for J^J irrational a € (0,1) we still need to find a compact description 
of a. However, a quick inspection of the proof of Theorem 10.7 reveals that 
we can study the behavior of G(n,p) for p = p(n) = n-o+0(1>, where a € 
(0,1) is irrational, provided we can estimate expressions like υ - ae, for all 
natural numbers υ and e. This, in turn, can be accomplished if we are given a 
subprocedure OracleQ which, for any rational r, can tell whether a < r. Thus 
we arrive at the following result. 

SEPARABILITY AND THE SPEED OF CONVERGENCE 
303 
Theorem 10.35. Let p = p(n) be a function for which we can decide that 
it fulfills the assumptions of either Theorem 10.5 or Theorem 10.15, or one 
of the assumptions of Theorem 10.6. Then there exists an algorithm which, 
given a sentence φ from L~, computes limn_+00 P(G(n,p) \= φ). 
Furthermore, for every irrational a € (0,1) and p = p{n) = n""Q+o(l) there 
exists an algorithm using 0racleo as a subprocedure which, given a sentence 
φ from L~, computes limn_KX) P(G(n,p) \= φ). 
■ 
The above result might suggest that finding a separating procedure should 
be possible at least when we can prove that the zero-one law holds. We will 
soon show that it is not the case. Our argument will be based on the following 
consequence of Trahtenbrot's theorem (Trahtenbrot 1950). 
Theorem 10.36. There is no procedure which can decide, for every sentence 
φ from L~, whether there exists a finite graph G such that G satisfies φ. 
■ 
Theorem 10.36 implies, in particular, that the minimal size of the model 
in which a sentence holds can grow very fast with the depth of the sentence. 
Corollary 10.37. There exists a sequence {ip^'ZLi of sentences from L~ 
such that for every k = 1,2,..., the sentence ψι has quantifier depth k and φ^ 
holds for some finite graph Gk, but the function 
g(k) = 
mm{v(Gk):Gk\=xpk} 
is not bounded from above by any recursive function of k. 
Proof. Suppose that the assertion does not hold and for every sentence φ from 
L^ of quantifier depth k, either φ holds for no finite graph, or it is valid for 
some finite graph of size bounded from above by a recursive (i.e., computable) 
function g(k). 
Consider a procedure which computes g(k), constructs all 
graphs on the set {1,2,...,/} for all 1 < I < k, and checks one by one 
whether φ holds for one of them. Such an algorithm would verify whether φ 
holds for a finite graph, contradicting Theorem 10.36. 
■ 
Let us look again at the random graph G(n,p) we studied in the previous 
section. In Theorem 10.31 we proved that if Σ7=ι Pi = o(logn), then for ev-
ery sentence φ from L„ the zero-one law holds. We will show that, somewhat 
surprisingly, no procedure can decide whether the limit probability of a sen-
tence is zero or one. Moreover, the speed of convergence for some sentences 
from L^ can be very, very slow. 
Theorem 10.38. Let p = {pi}^ 
be such that 0 < p¿ < 1/2 and £ " = 1 p¿ = 
o(log n). Then there is no procedure which for every sentence φ from L^ can 
decide whether a.a.s. φ holds for G(n,p). 
Furthermore, there exists a sequence {φ^^ι 
of sentences from L~ such 
that, for every k = 1,2,..., the sentence φ^ has quantifier depth k and 
lim P(G(n,p)h^*) = l , 
n-»oo 

304 
ZERO-ONE LAWS 
but the function 
f(k) = min{n : P(G(n,p) j= i/;*) > 0} 
is not bounded from above by any recursive function of k 
Proof. Let p be a sequence of probabilities which fulfills the assumptions of 
the theorem and let φ be any sentence of L^. Denote by φ the sentence 
saying that φ holds in the neighborhood of some vertex of a graph, that is, in 
order to construct φ from φ one needs to replace each occurrence of y ~ z by 
(x ~ y Ax ~ zAy ~ z), each ->(y ~ z) by (x ~ y Ax ~ zA->(y ~ z)), and add 
at the beginning of the formula the existential quantifier 3X. Then, obviously, 
φ is a sentence from L„ of quantifier depth at most one larger than φ, and φ 
has a finite model if and only if there is a finite model for φ. Furthermore, a 
graph is a model for φ if and only if one of its components satisfies φ. 
Clearly, if φ holds a.a.s. for G(n,p), then it has a finite model. Note 
however, that Lemma 10.30 implies that the converse is also true: if φ holds 
for some graph H, then a.a.s. G(n,p) contains this graph as a component, and 
thus a.a.s. G(n,p) is a model for φ. Hence, if we could effectively compute the 
limit probability for every sentence from L~, we could also decide for every φ 
from L^ whether it has a finite model, which would contradict Theorem 10.36. 
In order to show the second part of the proof, it is enough to consider 
sentences ipk corresponding to sentences Vifc from L~ chosen as in Corol-
lary 10.37. 
■ 
Finally, let us return to G(n,p) and consider the case in which p = p(n) = 
n - 1 / , 7 + o ( 1), where np7 is of an order smaller than log n, so Theorem 10.15 does 
not apply and, unless np7 tends to 0 very fast, the zero-one law does not hold 
(Theorem 10.19). Then, as observed by Dolan (1992), one can use the idea 
employed in the proof of Theorem 10.19 to show that for such a p = p(n) no 
separating procedure can be found. 
Theorem 10.39. Let p = p{n) = n~ 1 / 7 + o ( 1 ) be such that np7 = o(logn). 
Then there exists no procedure which can separate those sentences ofL~ which 
a.a.s. hold in G(n,p) from those whose probability tends to 0 os n -> oo. 
Furthermore, there exists a sequence {V'*}/£=i °f sentences from L„ such 
that, for every k — 1,2,..., the sentence φΐι has quantifier depth k and 
lim P(G(n,p) (= Vt) = 1 , 
n—►<» 
but the function 
f{k) = min{n : P(G(n,p) (= ΦΗ) > 0} 
is not bounded from above by any recursive function of k. 
Proof. Let φ be any sentence from L~ and let φ denote the sentence from L~ 
stating that there exists an element x such that φ holds in the set of all 

SEPARABILITY AND THE SPEED OF CONVERGENCE 
305 
elements y for which x ~ y. Then, as in the proof of Theorem 10.38, we infer 
that ψ holds for some finite model if and only if for each m large enough there 
exists a model for φ of m elements. 
From Lemma 10.21 we know that one can find formulas P* and T* from 
L~ such that for some function u> = ω(η), where ω(η) -+ oo as n - + o o , the 
following holds: a.a.s. G(n,p) contains a set of vertices S, \S\ > ω, which is 
defined by P* and some sequence vs, and such that every ternary relation 
on S can be encoded by T*. Now let ■ψ* be the sentence obtained from ψ in 
the following way: replace each occurrence of " i ~ y " by 
uT'(x,y,z,vT)A 
T*(y,x, 2,wT)", for each logical variable Xi used in ψ, add to it "AP*(xj,t¿5)", 
and put at the beginning the existential quantifiers 3„5 3,,τ3ζ. Then, clearly, 
V>* is a sentence from L~. Moreover, xj>* states that for some subset 5 of 
vertices of G(n,p), such that |5| > ω, there exists asymmetric binary relation 
"~"' on 5 such that S with "~"' is a finite model for rp. (Note that "~"' 
is not the adjacency relation of G(n,p).) 
Thus, if a.a.s. G(n,p) satisfies ψ*, 
then ψ and thus ψ have finite models. On the other hand, if φ has a finite 
model, then for every set S large enough there exists a symmetric binary 
relation "~"' such that S with "~"' is a model for V», and so φ* a.a.s. holds 
for G(n,p). Consequently, an algorithm separating properties which a.a.s. 
hold in G(n,p) from those which a.a.s. do not hold, could be used to decide 
if ψ has a finite model, contradicting Trahtenbrot's theorem. 
The second part of the statement follows from Corollary 10.37 in a similar 
manner. 
■ 

References 
Page numbers in italics show where the works are cited in this book. 
D. ACHLIOPTAS & E. FRIEDGUT (1999), A sharp threshold for fc-colorability. 
Random Structures Algorithms 14, 63-70. (23, 196) 
D. ACHLIOPTAS & M. MOLLOY (1997), The analysis of a list-coloring algo-
rithm on a random graph. In 38th Annual Symposium on Foundations of 
Computer Science (FOCS '97), 204-212. (107, 196) 
D. ACHLIOPTAS & M. MOLLOY (1999), Almost all graphs with 2.522n edges 
are not 3-colorable. Electronic J. Comb. 6, R29. (196) 
R. AHLSWEDE & D.E. DAYKIN (1978), An inequality for the weights of two 
families of sets, their unions and intersections. Z. Wahrscheinl. Geb. 43, 
183-185. (30) 
D. ALDOUS (1990), A random tree model associated with random graphs. 
Random Structures Algorithms 1, 383-402. (5) 
D. ALDOUS (1997), Brownian excursions, critical random graphs and the 
multiplicative coalescent. Ann. Probab. 25, 812-854. (127) 
D.J. ALDOUS & J. PITMAN (1994), Brownian bridge asymptotics for random 
mappings. Random Structures Algorithms 5, 487-512. (3) 
307 
Random Graphs 
by Svante Janson, Tomasz Luczak and Andrzej Rucinski 
Copyright © 2000 John Wiley & Sons, Inc. 

308 
REFERENCES 
N. ALON &C Z. FÜREDI (1992), Spanning subgraphs of random graphs. 
Graphs and Combinatorics (Research problem) 8, 91-94. (94, 96) 
N. ALON & M. KRIVELEVICH (1997), The concentration of the chromatic 
number of random graphs. Combinatorica 17, 303-313. (189) 
N. ALON, M. KRIVELEVICH & B. SUDAKOV (1998), Finding a large hidden 
clique in a random graph. Random Structures Algorithms 13, 457-466. 
(187) 
N. ALON & J. SPENCER (1992), The Probabilistic Method. John Wiley & 
Sons, New York. (12, 27, 29, 38, 49, 50) 
N. ALON & R. YUSTER (1993), Threshold functions for //-factors. Combin. 
Probab. Comput. 2, 137-144. (91, 92) 
P . ANDERSSON (1998), On the asymptotic distribution of subgraphs counts 
in a random tournament. Random Structures Algorithms 13, 249-260. (3, 
176) 
K.B. ATHREYA & P.E. NEY (1972), Branching processes. Die Grundlehren 
der mathematischen Wissenschaften, Band 196, Springer, Berlin. (107) 
K. AZUMA (1967), Weighted sums of certain dependent variables. Tóhoku 
Math. J. 3, 357-367. (37) 
L. BABAI, M. SIMONOVITS & J. SPENCER (1990), Extremal subgraphs of 
random graphs. J. Graph Theory 14, 599-622. (231) 
A.D. BARBOUR (1982), Poisson convergence and random graphs. Math. Proc. 
Camb. Phil. Soc. 92, 349-359. (152, 157, 162) 
A.D. BARBOUR, L. HOLST & S. JANSON (1992), Poisson 
Approximation. 
Oxford University Press, Oxford. (153, 154, 156) 
A. D. BARBOUR, S. JANSON, M. KARONSKI & A. RUCINSKI (1990), Small 
cliques in random graphs. Random Structures Algorithms 1, 403-434. 
(176, 245) 
A.D. 
BARBOUR, M. KARONSKI & A. RUCINSKI (1989), A central limit 
theorem for decomposable random variables with applications to random 
graphs. J. Combin. Theory Ser. B 47, 125-145. (157, 158, 161, 162, 172) 
E.A. BENDER & E.R. CANFIELD (1978), The asymptotic number of labeled 
graphs with given degree sequences. J. Combin. Theory Ser. A 24, 296-
307. (234, 235) 

REFERENCES 
309 
E.A. BENDER, E.R. CANFIELD & B.D. MCKAY (1990), The asymptotic 
number of labeled connected graphs with a given number of vertices and 
edges. Random Structures Algorithms 1, 127-169. (121) 
G. BENNETT (1962), Probability inequalities for the sum of independent 
random variables. J. Amer. Statist. Assoc. 57, 33-45. (29) 
C. BERGE (1973), Graphs andHypergraphs. North Holland, Amsterdam. (76) 
S. BERNSTEIN (1924), On a modification of Chebyshev's inequality and of 
the error formula of Laplace. Ann. Sei. Inst. Sav. Ukraine, Sect. Math. 1, 
38-49 (Russian). (25) 
P. BILLINGSLEY (1968), Convergence of Probability Measures. John Wiley k 
Sons, New York. (8, 9, 68, 169, 268) 
B. BOLLOBAS (1978), Extremal Graph Theory. Academic Press, London. (48, 
94, 205, 230) 
B. BOLLOBAS (1979), Graph Theory. Springer, New York. (13, 14) 
B. BOLLOBÁS (1980), A probabilistic proof of an asymptotic formula for the 
number of labelled regular graphs. Europ. J. Combinatorics 1, 311-316. 
(235, 236) 
B. BOLLOBÁS (1981a), The diameter of random graphs. Trans. Amer. Math. 
Soc 267, 41-52. (105) 
B. BOLLOBÁS (1981b), Random Graphs. In Combinatorics, 
Proceedings, 
Swansea 1981, London Math. Soc. Lecture Note Ser. 52, Cambridge Univ. 
Press, Cambridge, 80-102. (53, 56, 58, 66, 241, 244) 
B. BOLLOBÁS (1983), Almost all regular graphs are Hamiltonian. European 
J. Combin. 4, 97-106. (241) 
B. BOLLOBÁS (1984a), The evolution of random graphs. Trans. Amer. Math. 
Soc. 286, 257-274. (103, 112, 119, 120) 
B. BOLLOBÁS (1984b), The evolution of sparse graphs. In Graph Theory and 
Combinatorics, Proceedings, Cambridge Combinatorial Conf. in honour 
of Paul Erdös, ed. Β. Bollobás, Academic Press, 35-57. (105, IOS) 
B. BOLLOBÁS (1985), Random Graphs. Academic Press, London. (4, 14, 17, 
71, 105, 119, 180, 235, 236, 244, 280) 
B. BOLLOBÁS (1986), Combinatorics. Cambridge Univ. Press, Cambridge. 
(30) 

310 
REFERENCES 
B. BOLLOBÁS (1988a), The chromatic number of random graphs. Combina-
torial 8, 49-56. (37, 39, 190) 
B. BOLLOBÁS (1988b), Martingales, isoperimetric inequalities and random 
graphs. In Combinatorics, Proceedings, Eger 1987, eds. A. Hajnal, L. 
Lovasz & V.T. Sos, Colloq. Math. Soc. János Bolyai 52, North-Holland, 
Amsterdam, 113-139. (39, 134) 
B. BOLLOBÁS (1998), Modern Graph Theory. Springer, New York. (60, 184, 
209, 212, 214) 
B. BOLLOBÁS & P. ERDÖS (1976), Cliques in random graphs. Math. Proc. 
Camb. Phil Soc. 80, 419-427. (180) 
B. BOLLOBÁS, T.I. FENNER & A.M. FRIEZE (1990), Hamilton cycles in ran-
dom graphs of minimal degree at least k. In A tribute to Paul Erdös, eds. 
A. Baker, B. Bollobás & A. Hajnal, Cambridge Univ. Press, Cambridge, 
59-95. (105) 
B. BOLLOBÁS & A.M. FRIEZE (1985), On matchings and Hamiltonian cycles 
in random graphs. In Random Graphs '83, Proceedings, Poznan, 1983, 
eds. M. Karonski & A. Rucinski, North-Holland, Amsterdam-New York, 
23-46. (89, 105) 
B. BOLLOBÁS & B.D. MCKAY (1986), The number of matchings in random 
regular graphs and bipartite graphs. J. Combin. Theory Ser. B 41, 80-91. 
(260) 
B. BOLLOBÁS & A. THOMASON (1985), Random graphs of small order. In 
Random Graphs '83, Proceedings, Poznan, 1983, eds. Μ. Karonski & A. 
Rucinski, North-Holland, Amsterdam-New York, 47-97. (4, 81, 84, 105, 
187) 
B. BOLLOBÁS & A. THOMASON (1987), Threshold functions. Combinatorica 
7, 35-38. (18) 
B. BOLLOBÁS & A. THOMASON (1995), Generalized chromatic numbers of 
random graphs. Random Structures Algorithms 6, 353-356. (196) 
B. BOLLOBÁS & A. THOMASON (1997), Hereditary and monotone properties 
of graphs. In The Mathematics of Paul Erdös, II, eds. R.L. Graham & 
J. NeSetfil, Algorithms and Combinatorics, 14, Springer, Berlin, 70-78. 
(196) 
Β. BOLLOBÁS & J.C. WIERMAN (1989), Subgraph counts and containment 
probabilities of balanced and unbalanced subgraphs in a large random 

REFERENCES 
311 
graph. In Graph Theory and Its Applications: East and West, Proceed-
ings, Jinan, 1986, Ann. New York Acad. Sei. 576, New York Acad. Sei., 
New York, 63-70. (68) 
R. BOPPANA & J. SPENCER (1989), A useful elementary correlation inequal-
ity. J. Combin. Theory Ser. A 50, 305-307. (33) 
S. BOUCHERON, G. LUGOSI & P. MASSART (2000+), A sharp concentration 
inequality with applications. Random Structures Algorithms, to appear. 
(43) 
J. BOURGAIN (1999), On sharp thresholds of monotone properties. Appendix 
to Friedgut (1999), J. Amer. Math. Soc. 12, 1046-1053. (23) 
J. BOURGAIN & G. KALAI (1997), Influences of variables and threshold 
intervals under group symmetries. Geom. Fund. Anal. 7, 438-461. (22, 
23) 
Yu.D. 
BURTIN (1973), Asymptotic estimates of the diameter and indepen-
dence and domination numbers of a random graph. Dokl. Akad. Nauk 
SSSR 209, 765-768 (Russian); English transí. Soviet Math. Dokl. 14, 
497-501. (105) 
L.H.Y. CHEN (1975), Poisson approximation for dependent trials. Ann. 
Probab. 3, 534-545. (152, 153) 
H. CHERNOFF (1952), A measure of asymptotic efficiency for tests of a hy-
pothesis based on the sum of observations. Ann. Math. Statistics 23, 
493-507. (26) 
K.L. CHUNG (1974), A Course in Probability Theory. 2nd ed., Academic 
Press, New York. (140, 247) 
V. CHVATÁL (1991), Almost all graphs with 1.44n edges are 3-colorable. 
Random Structures Algorithms 2, 11-18. (196) 
K.J. COMPTON (1988), 0 - 1 Laws in Logic and Combinatorics. In Algorithms 
and Order, ed. I. Rival, NATO ASI series, Kluwer Academic Publishers, 
Dordrecht, 353-383. (301) 
C. COOPER, A.M. FRIEZE & M.J. MOLLOY (1994), Hamilton cycles in 
random regular digraphs. Combin. Probab. Comput. 3, 39-50. (233) 
C. COOPER, A.M. FRIEZE, M. MOLLOY & B. REED (1996), Perfect match-
ings in random r-regular, s-uniform hypergraphs. Combin. Probab. Com-
put. 5, 1-14. (241) 

312 
REFERENCES 
P. DE JONG (1996), A central limit theorem with applications to random 
hypergraphs. Random Structures Algorithms 8, 105-120. {176) 
A. DEMBO (1997), Information inequalities and concentration of measure. 
Ann. Probab. 25, 927-939. {40) 
R. DIESTEL (1996), Graphentheorie (German), Springer, New York; English 
transí. Graph Theory (1997), Springer, New York. {60, 66, 184, 212, 214) 
P. DOLAN (1992), Undecidable statements and random graphs. Ann. Math. 
Artificial Intelligence 6, 17-25. {304) 
P. DOLAN & J.F. LYNCH (1993), The logic of ordered random structures. 
Random Structures Algorithms 4, 429-446. {292) 
A. EHRENFEUCHT (1961), An application of games to the completeness prob-
lem for formalized theories. Fund. Math. 49, 129-141. {273) 
P. ERDÖS (1947), Some remarks on the theory of graphs. Bull. Amer. Math. 
Soc. 53, 292-294. {1, 2, 3) 
P. ERDÖS & L. LovÁsz (1975), Problems and results on 3-chromatic hy-
pergraphs and some related questions. In Infinite and Finite Sets, Vol. 
II, eds. A. Hajnal, R. Rado & V.T. Sos, Colloq. Math. Soc. J. Bolyai 10, 
North-Holland, Amsterdam, 609-627. {12) 
P. ERDÖS & A. RÉNYI (1959), On random graphs I. Publ. Math. Debrecen 
6, 290-297. {4, 81, 105) 
P. ERDÖS & A. RÉNYI (1960), On the evolution of random graphs. Publ. 
Math. Inst. Hungar. Acad. Sei. 5, 17-61. {53, 66, 80, 103, 104, 111, 128, 
156, 157, 162) 
P. ERDÖS & A. RÉNYI (1961), On the strength of connectedness of a random 
graph. Acta Math. Acad. Sei. Hungar. 12, 261-267. {81, 105) 
P. ERDÖS & A. RÉNYI (1964), On random matrices. Publ. Math. Inst. Hung. 
Acad. Set. 8, 455-461. {81, 83) 
P. ERDÖS & A. RÉNYI (1966), On the existence of a factor of degree one of 
a connected random graph. Acta Math. Acad. Sei. Hungar. 17, 359-368. 
{81, 85, 89, 105) 
P. ERDÖS & A. RÉNYI (1968), On random matrices II. Studia Sei. Math. 
Hungar. 3, 459-464. {81) 

REFERENCES 
313 
P. ERDÖS k M. SiMONOViTS (1966), A limit theorem in graph theory. Studia 
Sei. Math. Hungar. 1, 51-57. (60, 205) 
P. ERDÖS k A.H. STONE (1946), On the structure of linear graphs. Bull. 
Amer. Math. Soc. 52, 1087-1091. (60, 205) 
P. ERDÖS, S. SUEN k P. WINKLER (1995), On the size of a maximal random 
graph. Random Structures Algorithms 6, 309-318. (5) 
P. ERDÖS k P. TETALI (1990), Representations of integers as the sum of k 
terms. Random Structures Algorithms 1, 245-261. (50) 
R. FAGIN (1976), Probabilities in finite models. J. Symbolic Logic 41, 50-58. 
(276) 
T.I. FENNER k A.M. FRIEZE (1984), Hamiltonian cycles in random regular 
graphs. J. Combin. Theory Ser. B 37, 103-112. (241) 
P. FLAJOLET, D.E. KNUTH k B. PITTEL (1989), The first cycles in an 
evolving graph. Discrete Math. 75, 167-215. (135) 
C M . FORTUIN, P.W. KASTELEYN k J. GINIBRE (1971), Correlation in-
equalities on some partially ordered sets. Comm. Math. Phys. 22, 89-103. 
(30) 
P. FRANKL k V. RÖDL (1986), Large triangle-free subgraphs in graphs 
without K4. Graphs and Combinatorics 2, 135-144. (208, 209) 
E. FRIEDGUT (1999), Sharp thresholds of graph properties, and the fc-sat 
problem. J. Amer. Math. Soc. 12, 1017-1054. (22, 23) 
Ε. FRIEDGUT k G. KALAI (1996), Every monotone graph property has a 
sharp threshold. Proc. Amer. Math. Soc. 124, 2993-3002. (22, 23) 
Ε. FRIEDGUT k M. KRIVELEVICH (2000), Sharp thresholds for Ramsey 
properties of random graphs. Random Structures Algorithms, to appear. 
(23, 198, 204) 
A.M. FRIEZE (1988), Finding Hamilton cycles in sparse random graphs. J. 
Combin. Theory Ser. B 44, 230-250. (241) 
A.M. FRIEZE (1990), On the independence number of random graphs. Dis-
crete Math. 81, 171-175. (183) 
A.M. FRIEZE k S. JANSON (1995), Perfect matchings in random s-regular 
hypergraphs. Random Structures Algorithms 7, 41-57. (100, 101) 

314 
REFERENCES 
A.M. 
FRIEZE, M.R. 
JERRUM, M. 
MOLLOY, R. 
ROBINSON k 
N.C. 
WORMALD (1996), Generating and counting Hamilton cycles in random 
regular graphs. J. Algorithms 21, 176-198. {240, 252, 256) 
A.M. FRIEZE & T. LUCZAK (1992), On the independence and chromatic 
numbers of random regular graphs. J. Combin. Theory Ser. B 54, 123-
132. {233) 
A.M. FRIEZE k C. MCDIARMID (1997), Algorithmic theory of random 
graphs. Random Structures Algorithms 10, 5-42. {187) 
Z. FÜREDI (1994), Random Ramsey graphs for the four-cycle. Discrete Math. 
126, 407-410. {62, 232) 
H. GARMO (1999), The asymptotic distribution of long cycles in random 
regular graphs. Random Structures Algorithms 15, 43-92. {256) 
Y.V. GLEBSKII, D.I. KOGAN, M.I. LIAGONKII k V.A. TALANOV, (1969), 
Range and degree of realizability of formulas in the restricted predicate 
calculua. ¡Cibernética 5, 17-27 (Russian); English transí. Cybernetics 5, 
142-154. {276) 
A.W. GOODMAN (1959), On sets of acquaintances and strangers at any party. 
Amer. Math. Mon. 66, 778-783. {209) 
R.L. GRAHAM, D.E. KNUTH k O. PATASHNIK (1989), Concrete Mathemat-
ics. Addison-Wesley, Reading, Mass. {144) 
R.L. GRAHAM, V. RÖDL k A. RUCINSKI (1996), On Schur properties of 
random subsets of integers. J. Number Theory 61, 388-408. {208) 
R.L. GRAHAM, B. ROTHSCHILD k J. SPENCER (1992), Ramsey 
Theory. 
2nd ed., Wiley, New York. {217) 
G. GRIMMETT (1992a), Percolation. Springer, New York. (5) 
G. GRIMMETT (1992b), Weak convergence using higher-order cumulante. J. 
Theoret. Probab. 5, 767-773. {151) 
G. GRIMMETT k C.J.H. MCDIARMID (1975), On colouring random graphs. 
Math. Proc. Camb. Phil. Soc. 77, 313-324. {186) 
G. GRIMMETT k D.R. STIRZAKER (1992), Probability and Random Pro-
cesses: Problems and Solutions. Oxford University Press, Oxford. {30) 
J. GRUSZKA, T. LUCZAK k A. RUCINSKI (1996), On the evolution of a 
random tournament. Discrete Math. 148, 311-316. (5) 

REFERENCES 
315 
A. GUT (1995), An intermediate course in probability. Springer, New York. 
(8,9) 
E. GYÖRI, B. ROTHSCHILD & A. RUCIÑSKI (1985), Every graph is contained 
in a sparsest possible balanced graph. Math. Proc. Camb. Phil. Soc. 98, 
397-401. (58) 
A. HAJNAL & E. SZEMERÉDI (1970), Proof of a conjecture of Erdös. In 
Combinatorial Theory and its Applications, Vol. II, eds. P. Erdös, A. 
Rényi & V.T. Sos, Colloq. Math. Soc. János Bolyai 4, North-Holland, 
Amsterdam, 601-623. (48, 94) 
P.E. HAXELL, Y. KOHAYAKAWA & T. LUCZAK (1995), Turán's extremal 
problem in random graphs: forbidding even cycles. J. Combin. Theory 
Ser. B 64, 273-287. (62, 208, 232) 
P.E. HAXELL, Y. KOHAYAKAWA & T. LUCZAK (1996), Turán's extremal 
problem in random graphs: forbidding odd cycles. Combinatorica 16,133-
163. (208) 
W. HOEFFDING (1963), Probability inequalities for sums of bounded random 
variables. J. Amer. Statist. Assoc. 58, 13-30. (29, 37) 
S. JANSON (1987), Poisson convergence and Poisson processes with applica-
tions to random graphs. Stochastic Process. Appl. 26, 1-30. (68, 134) 
S. JANSON (1988), Normal convergence by higher semi-invariants with appli-
cations to sums of dependent random variables and random graphs. Ann. 
Probab. 16, 305-312. (151) 
S. JANSON (1990a), A functional limit theorem for random graphs with 
applications to subgraph count statistics. Random Structures Algorithms 
1, 15-37. (166, 172) 
S. JANSON (1990b), Poisson approximation for large deviations. Random 
Structures Algorithms 1, 221-230. (31, 49) 
S. JANSON (1993), Multicyclic components in a random graph process. Ran-
dom Structures Algorithms 4, 71-84. (117) 
S. JANSON (1994a), Orthogonal Decompositions and Functional Limit The-
orems for Random Graph Statistics. Memoirs Amer. Math. Soc. 534, 
Amer. Math. Soc, Providence, R.I. (166, 173, 175, 245) 
S. JANSON (1994b), The numbers of spanning trees, Hamilton cycles and 
perfect matchings in a random graph. Combin. Probab. Comput. 3, 97-
126. (176, 245) 

316 
REFERENCES 
S. JANSON (1995a), A graph Fourier transform and proportional graphs. 
Random Structures Algorithms 6, 341-351. (172, 175) 
S. JANSON (1995b), Random regular graphs: asymptotic distributions and 
contiguity. Combin. Probab. Comput. 4, 369-405. (233, 235, 242, 262) 
S. JANSON (1997), Gaussian Hilbert Spaces. Cambridge Univ. Press, Cam-
bridge. (167, 169) 
S. JANSON (1998), New versions of Suen's correlation inequality. Random 
Structures Algorithms 13, 467-483. (34, 35) 
S. JANSON, D.E. KNUTH, T. LUCZAK & B. PITTEL (1993), The birth of 
the giant component. Random Structures Algorithms 3, 233-358. (122, 
128, 136, 137, 177) 
S. JANSON & J. KRATOCHVÍL (1991), Proportional graphs. Random Struc-
tures Algorithms 2, 209-224. (172) 
S. JANSON, T. LUCZAK & A. RUCINSKI (1990), An exponential bound for 
the probability of nonexistence of a specified subgraph in a random graph. 
In Random Graphs '87, Proceedings, Poznan, 1987, eds. M. Karonski, J. 
Jaworski & A. Rucinski, John Wiley & Sons, Chichester, 73-87. (33, 59) 
S. JANSON & K. NOWICKI (1991), The asymptotic distributions of gener-
alized U-statistics with applications to random graphs. Probab. Theory 
Related Fields 90, 341-375. (166) 
S. JANSON & A. RUCINSKI (2000+), Upper bounds for upper tails of some 
sums of dependent random variables. In preparation. (48) 
S. JANSON & J. SPENCER (1992), Probabilistic construction of proportional 
graphs. Random Structures Algorithms 3, 127-137. (172) 
M. JERRUM (1992), Large cliques elude the metropolis process. Random 
Structures Algorithms 3, 347-359. (187) 
A. JUELS & M. PEINADO (1998), Hiding cliques for cryptographic security. 
In Proceedings of the 9th Symposium on Discrete Algorithms (SODA '98), 
ACM Press, 678-684. (186) 
M. KARONSKI k A. RUCINSKI (1983a), On the number of strictly balanced 
subgraphs of a random graph. In Graph Theory, Proceedings, Lagów, 
1981, eds. M. Borowiecki, J.W. Kennedy k M.M. Syslo, Lecture Notes in 
Math. 1018, Springer, Berlin, 79-83. (66) 

REFERENCES 
317 
M. KAROÑSKI k A. RUCIÑSKI (1983b), Problem 4. In Graphs And Other 
Combinatorial Topics, Proceedings, Third Czech. Symp. on Graph The-
ory, Prague, 1982, Teubner-Texte Math. 59, Teubner, Leipzig. (58) 
M. KAROÑSKI k A. RUCIÑSKI (1987), Poisson convergence and semi-induced 
properties of random graphs. Math. Proc. Camb. Phil. Soc. 101,291-300. 
{156, 162) 
M. KAROÑSKI k A. RUCIÑSKI (1997), The origins of the theory of random 
graphs. In The Mathematics of Paul Erdós, /, eds. R.L. Graham h J. 
Nesetfil, Algorithms and Combinatorics, 13, Springer, Berlin, 311-336. 
(3) 
R.M. KARP (1976), The probabilistic analysis of some combinatorial search 
algorithms. In Algorithms and Complexity: New Directions and Recent 
Results, Proceedings, Pittsburgh, 1976, ed. J.F. Traub, Academic Press, 
New York, 1-19. (186) 
J. KÄRRMAN (1993), Existence of proportional graphs. J. Graph Theory 17, 
207-220. (172) 
J. KÄRRMAN (1994), An example of a superproportional graph. Random 
Structures Algorithms 5, 95-98. (172, 175) 
J.H. 
KIM k 
N.C. WORMALD (2000+), Random matchings which in-
duce Hamilton cycles, and hamiltonian decompositions of random regular 
graphs. In preparation. (262) 
D.J. KLEITMAN k K.J. WINSTON (1982), On the number of graphs without 
4-cycles. Discrete Math. 41, 167-172. (62, 232) 
Y. KOHAYAKAWA (1997), Szemerédi's regularity lemma for sparse graphs. In 
Foundations of Computational Mathematics, Proceedings, Rio de Janeiro, 
1997, eds. F. Cucker k M. Shub, Springer, Berlin, 216-230. (215) 
Y. KOHAYAKAWA k B. KREUTER (1997), Threshold functions for asymmet-
ric Ramsey properties involving cycles. Random Structures 
Algorithms 
11, 245-276. (208) 
Y. KOHAYAKAWA, B. KREUTER k A. STEGER (1998), An extremal prob-
lem for random graphs and the number of graphs with large even-girth. 
Combinatorica 18, 101-120. (62, 208, 232) 
Y. KOHAYAKAWA, T. LUCZAK k V. RÖDL (1996), Arithmetic progressions 
of length three in subsets of a random set. Acta Arith. LXXV, 133-163. 
(208) 

318 
REFERENCES 
Y. KOHAYAKAWA, T. LUCZAK k V. RÖDL (1997), On if-free subgraphs 
of random graphs. Combinatorial 17, 173-213. {208, 232) 
V.F. KOLCHIN (1986), Random Mappings. Optimization Software, New York. 
(5) 
J. KOMLÓS & M. SIMONOVITS (1996), Szemerédi's Regularity Lemma and 
its applications in graph theory. In Combinatorics, Paul Erdós is Eighty, 
Vol. 2, eds. D. Miklós, V.T. Sos & T. Szönyi, Bolyai Soc. Math. Stud. 2, 
J. Bolyai Math. Soc, Budapest, 295-352. {212) 
J. KOMLÓS & E. SZEMERÉDI (1983), Limit distributions for the existence 
of Hamilton cycles in a random graph. Discrete Math. 43, 55-63. {IOS) 
W. KORDECKI (1990), Normal approximation and isolated vertices in ran-
dom graphs. In Random Graphs '87, Proceedings, Poznan, 1987, eds. M. 
Karonski, J. Jaworski & A. Rucinski, John Wiley & Sons, Chichester, 
131-139. {161) 
B. KREUTER (1996), Threshold functions for asymmetric Ramsey properties 
with respect to vertex colorings. Random Structures Algorithms 9, 335-
348. (75) 
B. KREUTER (1997), Probabilistic versions of Ramsey's and Turán's theo-
rems. Ph.D. Dissertation, Mathematisch-Naturwissenschaftliche Fakultät 
II, Institut für Informatik, Humboldt-Universität, Berlin. {208, 232) 
M. KRIVELEVICH (1996a), Triangle factors in random graphs. Combin. 
Probab. Comput. 6, 337-347. {96, 97, 98, 100) 
M. KRIVELEVICH (1996b), Perfect fractional matchings in random hyper-
graphs. Random Structures Algorithms 9, 317-334. {102) 
L. KUCERA (1995), Expected complexity of graph partitioning problems. 
Discrete Applied Math. 57, 193-212. {187) 
J. KURKOWIAK & A. RUCINSKI (2000), Solitary subgraphs of random 
graphs. Discrete Mathematics, to appear. {79) 
L. L E CAM (1960), Locally asymptotically normal families of distributions. 
Univ. of California Publ. in Statistics 3, 37-98. {264) 
L. L E CAM (1969), Theorie asymptotique de la decision statistique. Les 
Presses de l'Université de Montreal, Montreal. {264) 
L. L E CAM (1986), Asymptotic 
methods in statistical decision 
theory. 
Springer, New York. {264) 

REFERENCES 
319 
V.P. LEONOV & A.N. SHIRYAEV (1959), On a method of calculation of semi-
invariants. Tear. Veroyatnost. i Primerien. 4, 342-355 (Russian); English 
transí. Theor. Probab. Appl. 4, 31& 329. (147) 
T. LUCZAK (1987), On matchings and hamiltonian cycles in subgraphs of 
random graphs. Annals of Discrete Math. 33, 171-185. (106) 
T. LUCZAK (1990a), On the equivalence of two basic models of random 
graphs. In Random Graphs '87, Proceedings, Poznan, 1987, John Wiley 
& Sons, Chichester, eds. M. Karonski, J. Jaworski & A. Rucinski, 151-158. 
(U) 
T. LUCZAK (1990b), On the number of sparse connected graphs. Random 
Structures Algorithms 1, 171-174. (120) 
T. LUCZAK (1990C), Component behavior near the critical point of the ran-
dom graph process. Random Structures Algorithms 1, 287-310. (114, 120, 
122, 136) 
T. LUCZAK (1991a), Cycles in a random graph near the critical point. Ran-
dom Structures Algorithms 2, 421-440. (124, 125, 126, 128) 
T. LUCZAK (1991b), The chromatic number of random graphs. Combinator-
ica 11, 45-54. (193) 
T. LUCZAK (1991c), A note on the sharp concentration of the chromatic 
number of random graphs. Combinatorica 11, 295-297. (187) 
T. LUCZAK (1991d), Size and connectivity of the fc-core of a random graph. 
Discrete Math. 91, 61-68. (106) 
T. LUCZAK (1991e), Cycles in random graphs. Discrete Math. 98, 231 236. 
(105) 
T. LUCZAK (1992), Sparse random graphs with a given degree sequence. In 
Random Graphs (2), Proceedings, Poznan, 1989, eds. A.M. Frieze & T. 
Luczak, John Wiley & Sons, New York, 165-182. (244) 
T. LUCZAK (1996), The phase transition in a random graph. In Combina-
torics, Paul Erdós is Eighty, Vol. 2, eds. D. Miklós, V.T. Sos & T. Szónyi, 
Bolyai Soc. Math. Stud. 2, J. Bolyai Math. Soc, Budapest, 399-422. (114, 
127) 
T. LUCZAK (2000), On triangle-free random graphs. Random Structures Al-
gorithms, to appear. (62) 

320 
REFERENCES 
T. LUCZAK, B. PiTTEL & J.C. WIERMAN (1994), The structure of a random 
graph near the point of the phase transition. Trans. Amer. Math. Soc. 
341, 721-748. {127, 128) 
T. LUCZAK & A. RUCIÑSKI (1991), Tree-matchings in graph processes. SIAM 
J. Discrete Math. 4, 107-120. (81, 85, 90) 
T. LUCZAK & A. RUCIÑSKI (1992), Convex hulls of dense balanced graphs. 
J. Comput. Appl. Math. 41, 205-213. (64) 
T. LUCZAK, A. RUCIÑSKI & B. VOIGT (1992), Ramsey properties of random 
graphs. J. Combin. Theory Ser. B 56, 55-68. (204, 209) 
T. LUCZAK & S. SHELAH (1995), Convergence in homogeneous random 
graphs. Random Structures Algorithms 6, 371-392. (293, 296, 298, 300) 
T. LUCZAK & J. SPENCER (1991), When does the zero-one law hold? J. 
Amer. Math. Soc. 4, 451-468. (277, 287, 288, 289) 
T. LUCZAK & J.C. WIERMAN (1989), The chromatic number of random 
graphs at the double-jump threshold. Combinatorica 9, 39-49. (128) 
J.F. LYNCH (1990), Probabilities of sentences about very sparse random 
graphs. In 31st Annual Symposium on Foundations of Computer Science, 
Vol. I, II (St. Louis, MO, 1990), IEEE Comput. Soc. Press, Los Alamitos, 
Calif., 689-696. (286) 
J.F. LYNCH (1993), Convergence laws for random words. Australasian J. 
Combin. 7, 145-156. (301) 
J. MARCINKIEWICZ (1939), Sur une propriété de la loi de Gauss. Math. Z. 
44, 612-618. (151) 
K. MARTON (1996), A measure concentration inequality for contracting 
Markov chains. Geom. Funct. Anal. 6, 556-571; Erratum Geom. Funct. 
Anal. 7 (1997), 609-613. (40) 
D. MATULA (1976), The largest clique size in a random graph. Tech. Rep., 
Dept. Comp. Sei., Southern Methodist Univerity, Dallas, Texas. (180) 
D. MATULA (1987), Expose-and-merge exploration and the chromatic num-
ber of a random graph. Combinatorica 7, 275-284. (186, 193) 
D. MATULA & L. KUCERA (1990), On chromatic number of random graphs. 
In Random Graphs '87, Proceedings, Poznan, 1987, eds. M. Karonski, J. 
Jaworski k A. Ruciñski, John Wiley & Sons, Chichester, 175-188. (193) 

REFERENCES 
321 
C. McDiARMiD (1989), On the method of bounded differences. In Surveys in 
Combinatorics, Proceedings, Norwich, 1989, London Math. Soc. Lecture 
Note Ser. 141, Cambridge Univ. Press, Cambridge, 148-188. (39, 192) 
V.G. MlKHAlLOV (1991), On a theorem of Janson. Teor. Veroyatnost. i 
Primenen. 36, 168-170 (Russian); English transí. Theory Probab. Appl. 
36, 173-176. (148, 149, 151) 
M. MOLLOY (1996), A gap between the appearances of a fc-core and a (fc+1)-
chromatic graph. Random Structures Algorithms 8, 159-160. {107, 196) 
M.S.O. MOLLOY, H. ROBALEWSKA, R.W. ROBINSON & N . C WORMALD 
(1997), 1-factorizations of random regular graphs. Random Structures Al-
gorithms 10, 305-321.(262) 
J.W. MOON (1968), Topics on Tournaments. Holt, Rinehart and Winston, 
New York. (3) 
C . S T . J . A . 
NASH-WILLIAMS (1964), Decompositions of finite graphs into 
forests. J. London Math. Soc. 39, 12. (66) 
M. OKAMOTO (1958), Some inequalities relating to the partial sum of bino-
mial probabilities. Ann. Inst. Statist. Math. 10, 29-35. (26) 
D. OSTHUS & A. TARAZ (2000+), Random maximal //-free graphs. Sub-
mitted. (5) 
C. PAYAN (1986), Graphes equilibres et arboricité rationalle. European J. 
Combin. 7, 263-270. (58) 
B. PiTTEL (1982), On the probable behaviour of some algorithms for finding 
the stability number of a graph. Moth. Proc. Camb. Phil. Soc. 92, 511-
526. (17) 
B. PiTTEL (1990), On tree census and the giant component in sparse random 
graphs. Random Structures Algorithms 1, 311-342. (121, 174, ¡77) 
B. PITTEL, J. SPENCER & N.C. WORMALD (1996), Sudden emergence of a 
giant fc-core in a random graph. J. Combin. Theory Ser. B 67, 111-151. 
(106) 
B. PITTEL & R.S. WEISHAAR (1997), On-line coloring of sparse random 
graphs and random trees. J. Algorithms 23, 195-205. (187) 
O. RIORDAN (2000), Spanning subgraphs of random graphs. Combin. Probab. 
Comput, to appear. (96) 

322 
REFERENCES 
H. ROBALEWSKA (1996), 2-factors in random regular graphs. J. Graph The-
ory 23, 215-224. {260, 261) 
R.W. ROBINSON & N.C. WORMALD (1984), Existence of long cycles in 
random cubic graphs. In Enumeration and Design, eds. D.M. Jackson & 
S.A. Vanstone, Academic Press, Toronto, 251-270. {241, 252) 
R.W. ROBINSON & N.C. WORMALD (1992), Almost all cubic graphs are 
hamiltonian. Random Structures Algorithms 3, 117-125. {101, 125, 241, 
242, 243, 244, 249, 252, 262) 
R.W. ROBINSON & N.C. WORMALD (1994), Almost all regular graphs are 
hamiltonian. Random Structures Algorithms 5, 363-374. {241, 244, 252, 
262) 
V. RÖDL & A. RUCINSKI (1993), Lower bounds on probability thresholds 
for Ramsey properties. In Combinatorics, Paul Erdös is Eighty, Vol. 1, 
eds. D. Miklós, V.T. Sos & T. Szönyi, Bolyai Soc. Math. Stud., J. Bolyai 
Math. Soc, Budapest, 317 346. {202, 204) 
V. RÖDL & A. RuciNSKl (1994), Random graphs with monochromatic tri-
angles in every edge coloring. Random Structures Algorithms 5, 253-270. 
{48, 204) 
V. RÖDL & A. RUCINSKI (1995), Threshold functions for Ramsey properties. 
J. Amer. Math. Soc. 8, 917-942. {51, 203, 208, 216) 
V. RÖDL & A. RuciNSKl (1997), Rado partition theorem for random subsets 
of integers. Proc. London Math. Soc. (3) 74, 481-502. {208) 
V. RÖDL & A. RUCINSKI (1998), Ramsey properties of random hypergraphs. 
J. Combin. Theory Ser. A 81, 1-33. {208) 
M. Roos (1996), An extension of Janson's inequality. Random 
Structures 
Algorithms 8, 213-227. {34) 
G. ROUSSAS (1972), Contiguity of probability measures: some applications in 
statistics. Cambridge Univ. Press, Cambridge. {264) 
A. RUCINSKI (1988), When are small subgraphs of a random graph normally 
distributed? Probab. Theory Related Fields 78, 1-10. {141) 
A. RUCINSKI (1990), Small subgraphs of random graphs - a survey. In Ran-
dom Graphs '87, Proceedings, Poznan, 1987, eds. M. Karonski, J. Ja-
worski & A. Rucinski, John Wiley & Sons, Chichester, 283-303. (57) 

REFERENCES 
323 
A. RUCINSKI (1991), Convex hulls of graphs. Ars Combinatoria 32, 293-300. 
(64) 
A. RUCIÑSKI (1992a), Matching and covering the vertices of a random graph 
by copies of a given graph. Discrete Mathematics 105, 185-197. (70, 91) 
A. RUCIÑSKI (1992b), Open Problem. In Random Graphs (2), Proceedings, 
Poznan, 1989, eds. A.M. Frieze k T. Luczak, John Wiley k Sons, New 
York, 284. (96) 
A. RUCIÑSKI k A. VINCE (1985), Balanced graphs and the problem of 
subgraphs of a random graph. Congressus Numerantium 
49, 181-190. 
(53) 
A. RUCIÑSKI L A. VINCE (1986), Strongly balanced graphs and random 
graphs. J. Graph Theory 10, 251-264. (69, 281) 
A. RUCIÑSKI k A. VINCE (1993), The solution to an extremal problem on 
balanced extensions of graphs. J. Graph Theory 17, 417-431. (58) 
A. RUCIÑSKI k N.C. WORMALD (1992), Random graph processes with 
degree restrictions. Combin. Probab. Comput. 1, 169-180. (5, 234) 
J. SCHMIDT k E. SHAMIR (1983), A threshold for perfect matchings in 
random d-pure hypergraphs. Discrete Math. 45, 287-295. (96, 100) 
K. SCHÜRGER (1979), Limit theorems for complete subgraphs of random 
graphs. Per. Math. Hungar. 10, 47-53. (66) 
E. SHAMIR k J. SPENCER (1987), Sharp concentration of the chromatic 
number on random graphs G„,p. Combinatorica 7, 124-129. (37, 39, 187, 
190) 
E. SHAMIR k E. UPFAL (1981), On factors in random graphs. Israel J. Math. 
39, 296-302. (10, 89) 
S. SHELAH (1996), On the very weak 0-1 law for random graphs with orders. 
J. Logic Comput. 6, 137-159. (301) 
S. SHELAH k J. SPENCER (1988), Zero-one laws for sparse random graphs. 
J. Amer. Math. Soc. 1, 97-115. (277, 280, 281, 288, 289) 
M. SIMONOVITS (1983), Extremal graph theory. In Selected Topics in Graph 
Theory, 2, eds. L.W. Beineke k R.J. Wilson, Academic Press, London-
New York, 161-200. (230) 

324 
REFERENCES 
A.V. SKOROKHOD (1956), Limit theorems for stochastic processes. Teor. 
Veroyatnost. i Primerien. I, 289-319 (Russian); English transí. Theor. 
Probab. Appl. 1, 261-290. (248) 
J. SPENCER (1990), Threshold functions for extension statements. J. Combm. 
Theory Ser. A 53, 286-305. (34, 49, 70, 71, 73, 74, 75) 
J. SPENCER (1991), Threshold spectra via the Ehrenfeucht game. Proceed-
ings, ARIDAM III, New Brunswick, N.J., 1988, Discrete Appl. Math. 30, 
235-252. (20) 
J. SPENCER (1998), A useful elementary correlation inequality, II. J. Combin. 
Theory Ser. A 84, 95-98. (34) 
J. SPENCER h L. THOMA (1999), On the limit values of probabilities for 
the first order properties of graphs. In Contemporary Trends in Discrete 
Mathematics, DIM ACS Series in Discrete Mathematics and Theoretical 
Computer Science, 49, eds. R.L. Graham, J. Kratochvfl, J. Nesetfil k F.S. 
Roberts, Amer. Math. Soc, 317-336. (286) 
C. STEIN (1972), A bound for the error in the normal approximation to the 
distribution of a sum of dependent variables. In Proceedings of the Sixth 
Berkeley Symposium on Mathematical Statistics and Probability, 1970, 
Vol. II, Univ. of California Press, Berkeley, 583-602. (152, 157) 
C. STEIN (1986), Approximate Computation of Expectations. IMS, Hayward, 
Calif. (153, 157) 
V.E. STEPANOV (1970), The probability of connectedness of a random 
graph Gm(t)· Teor. Veroyatnost. i Primenen. 15, 55-68 (Russian); En-
glish transí. Theor. Probab. Appl. 15, 55-67. (5) 
W.C.S. SUEN (1990), A correlation inequality and a Poisson limit theo-
rem for nonoverlapping balanced subgraphs of a random graph. Random 
Structures Algorithms 1, 231-242. (34, 79) 
E. SZEMERÉDI (1975), On sets of integers containing no k elements in arith-
metic progression. Acta Arith. XXVII, 299-345. (212) 
E. SZEMERÉDI (1978), Regular partitions of graphs. In Problémes com-
binatoires et théoríe des graphes, Proceedings, Orsay, 1976, eds. J.-C. 
Bermond, J.-C. Fournier, M. Las Vergnas & D. Sotteau, Colloq. Internat. 
CNRS 260, CNRS, Paris, 399-401. (213, 214) 
M. TALAGRAND (1994), On Russo's approximate zero-one law. Ann. Probab. 
22, 1576-1587. (23) 

REFERENCES 
325 
M. TALAGRAND (1995), Concentration of measure and isoperimetric inequal-
ities in product spaces. Inst. Hautes Etudes Sei. Publ. Math. 81, 73-205. 
(39, 40, 43, 44, 46) 
B.A. TRAHTENBROT (1950), Impossibility of an algorithm for the decision 
problem on finite classes. Doki Akad. Nauk SSSR 70, 569-572 (Russian). 
(303) 
V.A. VATUTIN & V.G. MIKHAILOV (1982), Limit theorems for the number 
of empty cells in an equiprobable scheme for group allocation of particles. 
Teor. Veroyatnost. i Pnmenen. 
27, 684-692 (Russian); English transí. 
Theor. Probab. Appl. 27, 734-743. (30) 
V.H. Vu (2000+), A large deviation result on the number of small subgraphs 
of a random graph. Submitted. (50) 
P. WiNKLER (1991), Random orders of dimension 2. Order 7, 329-339. (265) 
N.C. 
WORMALD (1981a), The asymptotic connectivity of labelled regular 
graphs. J. Combin. Theory Ser. B 31, 156-167. (244) 
N.C. 
WORMALD (1981b), The asymptotic distribution of short cycles in 
random regular graphs. J. Combin. Theory Ser. B 31, 168-182. (236) 
N.C. WORMALD (1999a), The differential equation method for random graph 
processes and greedy algorithms. In Lectures on Approximation and Ran-
domized Algorithms, eds. M. Karonski & H.-J. Prömel, PWN, Warsaw, 
73-155. (5, 38) 
N.C. 
WORMALD (1999b), Models of random regular graphs. In Surveys in 
Combinatorics, Proceedings, Cambridge, 1999, eds. J.D. Lamb k D.A. 
Preece, London Math. Soc. Lecture Note Ser. 276, Cambridge Univ. Press, 
Cambridge, 239-298. (233) 

Index of Notation 
SETS AND NUMBERS 
SUBGRAPHS 
e 
base of natural 
logarithm, 12 
[x] 
ceiling 
[ij 
floor 
[n] 
{l,2,...,n},l 
[X]k 
fc-element 
subsets of X, 
5 
[n]* 
¿-element subsets of [n], 
5 
n!! 
semi-factorial, 140 
{x)k 
descending factorial, 
144 
VERTICES A N D E D G E S 
V(G) 
VGMG) 
E(G) 
ec,e(G) 
eG(V) 
eG(A,B) 
v{R,H) 
e(R,H) 
vertex set, 6 
number of vertices, 6 
edge set, 6 
number of edges, 6 
number of edges within 
V, 7 
number of edges between 
A and B, 7 
number of extension 
vertices, 281 
number of proper edges, 
281 
G[V) 
G[E] 
E(G) 
£(G) 
(«,G) 
(R,G) 
clt(W) 
cr*(G) 
cr(G) 
ker(G) 
DENSITIES 
induced, or spanned 
subgraph, 7 
spanning subgraph, 7 
subgraph plot, 63 
roof of subgraph plot, 63 
rooted graph, 68 
rooted graph, 73, 281 
t-closure, 282 
fc-core, 106 
2-core, 122 
kernel, 122 
d(G) 
m(G) 
dW(G) 
mW(G) 
dW(G) 
m<2>(G) 
d(v,G) 
m(v, G) 
density, 6, 64 
maximum density, 6, 56, 
64 
K\ -density, 64 
maximum K¡ -density, 
64, 197 
KVdensity, 65 
maximum ^-density, 
65 
rooted density, 69 
maximum rooted density, 
69 
327 
Random Graphs 
by Svante Janson, Tomasz Luczak and Andrzej Rucinski 
Copyright © 2000 John Wiley & Sons, Inc. 

328 
INDEX OF NOTATION 
d(R,G) 
m(R,G) 
P(G) 
PM 
d„{U,W) 
d..H(U,W) 
rooted density, 74 
maximum rooted density, 
74 
relative density, 7 
relative density of 
G(n, M), 222 
pair density, 213 
scaled pair density, 212 
DEGREES AND NEIGHBORS 
NG{v) 
NG{S) 
NG{v) 
~ÑG(S) 
Í(C) 
A(G) 
deg(u) 
neighborhood of υ, 7 
neighborhood of S, 7 
closed neighborhood of υ, 
7 
closed neighborhood of 
S, 7 
minimum degree, 7 
maximum degree, 7 
vertex degree, 7 
SPECIAL GRAPHS 
Gc 
Pk 
Kl.n 
JG 
ΚΓ 
null graph, also empty 
set, 7 
complement of G, 79 
complete graph, 7 
complete bipartite graph, 
7 
cycle, 7 
path with fc edges, 7 
star, 7 
union of disjoint copies, 7 
matching, 7 
whisk graph, 68 
lollipop graph, 71 
diamond, 97 
GRAPH PARAMETERS 
aut(G) 
a(G) 
X(G) 
D(G) 
ex(F.G) 
ex(F,G) 
number of 
automorphisms, 7 
stability, or independence 
number, 7 
chromatic number, 7 
degeneracy number, 7 
Turan number, 204 
relative Turan number, 
204 
GRAPH PROPERTIES 
COVG 
covering property, 68 
Ext(Ä, G) 
extension statement, 73 
PM 
perfect matching 
property, 84 
Fc(e) 
F -> (G)» 
F -> ( O ? 
M* 
partial G'-fattor property, 
91 
vertex Ramsey property, 
196 
edge Ramsey property, 
202 
Hamilton-matching 
property, 105 
P R O B A B I L I T Y 
P 
1[£] 
VX 
VX! 
Xk 
L 
M O M E N T S 
E 
Var 
Cov 
E(X | S) 
m 
X 
EX* 
E(X)t 
x*(X) 
probability, 1 
indicator function, 8 
characteristic function, 
145 
joint characteristic 
function, 147 
dependency graph, 11 
expectation, 8 
variance, 8 
covariance, 8 
conditional expectation, 
8 
median, 40 
standardized random 
variable, 139 
moments, 140 
factorial moments, 144 
cumulante, 145 
κ(Χι,..., Xfc) mixed cumulants, 147 
DISTRIBUTIONS 
c 
d 
-> 
Bi(n,p) 
Be(p) 
Po(A) 
Ν(μ,<τ2) 
d T V ( X , V ) 
di(X,V) 
distribution, 7 
convergence in 
distribution, 8 
convergence in 
probability, 8 
binomial distribution, 7 
Bernoulli distribution, 7 
Poisson distribution, 7 
normal distribution, 7 
total variation distance, 
153 
distance between 
distributions, 158 
A S Y M P T O T I C S 
an = 0(6„) 
a„ = Ω(6„) 
a„ = θ(6„) 
an x i»n 
big O, 9 
inverse big O, 9 
same order of magnitude, 
10 
same as αη = θ(6„), 10 

INDEX OF NO TA TION 
329 
On = o(6„) 
an < bn 
On > 6n 
a.a.s. 
asymptotic equality, 10 
little o, 10 
same as an = o(6„), 10 
same as 6„ = o(an), 
10 
asymptotically almost 
surely, 10 
S U B G R A P H 
C O U N T S 
PROBABILITY 
A S Y M P T O T I C S 
Xn = Op(an) 
probabilistic big O, 10 
Xn = Oc(an) 
stronger probabilistic big 
O, 10 
Xn = θρ(αη) 
probabilistic Θ(α η), 10 
Xn = ©c(<>n) stronger probabilistic 
θ ( α η ) , 10 
X n = Op(a„) 
probabilistic little o, 11 
R A N D O M S T R U C T U R E S 
Γ ρ 
binomial random subset, 
5 
ΓΜ 
uniform random subset, 
5 
Fpi...,ΡΛΓ 
general random subset, 
6 
{ Γ Μ } Μ 
random subset process, 
13 
G(n, p) 
binomial random graph, 
2 
G(m, n,p) 
bipartite random graph, 
2 
G(n, M) 
uniform random graph, 
3 
€(*, ¿) 
connected random graph, 
123 
G(n, r) 
random regular graph, 3, 
233 
G*(n,r) 
random regular 
multigraph, 235 
G'(n,r) 
random regular 
multigraph without 
_ 
loops, 257 
G(n,p) 
special random graph, 
296 
GL (n, M) 
G(n, M) without largest 
component, 130 
{G(t)}( 
random graph process, 4 
{G(n, M)}M 
the random graph 
process, 4 
Gn 
« G„ 
contiguity of random 
graphs, 257 
G} + Gj 
sum of random graphs, 
257 
Gi φ G2 
simple sum of random 
graphs, 257 
P(n) 
random permutation, 263 
XG 
Φ<7 
YG 
Xc 
TG 
r„ 
Sn(H) 
zk 
Xn(H) 
X'JH) 
Hn 
Un 
tr(n,M) 
Y(k,t) 
C(kJ) 
κ(η,Μ) 
subgraph count, 55 
minimum expected 
subgraph count, 56 
induced subgraph count, 7 
subgraph count in 
G(n, M), 61 
isolated subgraph count, 
79 
isolated υ-vertex trees 
count, 80 
"centralized" subgraph 
count, 165 
cycle count in G(n, r) 
andG*(n,r), 236 
decomposition 
coefficients, 166 
scaled decomposition 
coefficients, 168 
Hamilton cycle count in 
G(n,r),240 
Hamilton cycle count in 
G*(n,r), 240 
size of r-th largest 
component, 112 
¿-component count in 
G(n, M), 113 
number of connected 
graphs, 113 
excess of largest 
component of G(n, M), 
121 
78 
T H R E S H O L D S 
M 
M 
δ(ε) 
LOGIC 
threshold in G(n, p), 18 
threshold in G{n, Λ/), 18 
hitting time, 19 
width of threshold, 20 
L~ 
¿ord 
X ~y 
qd(v) 
Thfc(M) 
M \=φ 
first-order language of 
graphs, 272 
first-order language of 
ordered graphs, 272 
adjacency predicate, 272 
quantifier depth, 272 
set of sentences of depth 
at most k, 273 
M is a model for φ, 273 
Ehr*(M'.M") Ehrenfeucht game, 274 
M1 φ Μ2 
Gi + G 2 
T 
sum scheme of models, 293 
sum scheme of graphs, 293 
signature, 293 

Index 
O-statement, 18 
1-factor, 82, 89 
1-statement, 18 
2-independent set, 88, 94 
a.a.s., 10 
Almost surely, 10 
asymptotically, 10 
Analysis of variance, 96, 241 
Arboricity, 66 
Arithmetic progression, 54 
Asymptotic 
equivalence, 14 
normality, 149 
Azuma's inequality, 37 
Bounded in probability, 11 
Branching process, 107 
Characteristic function, 145 
joint, 147 
Chebyshev's inequality, 8, 25, 241 
Chernoff bound, 26 
Chernoff's inequality, 26 
Cherry, 82 
Chromatic number, 106, 184 
concentration, 187 
Closure, 282 
Coloring algorithm, 186 
Component 
complex, 112 
giant, 104, 120 
Configuration, 235 
graph, 137 
random, 235 
Connectivity, 104, 244 
Contiguity, 234, 264 
Convergence 
in distribution, 8 
in probability, 8 
Copy of graph, 7 
induced, 77 
isolated, 78, 157 
nested, 219 
solitary, 78 
Core, 105-106, 122, 124 
Cramer's theorem, 9 
Cramer-Wold device, 9 
Cumulants, 145 
mixed, 147 
Decomposition method, 165 
Degeneracy number, 7, 96, 184 
Density, 6, 64 
maximum, 6, 64 
relative, 7, 222 
Dependency graph, 11 
Diameter, 105 
Diamond tree, 98 
Diamond, 97 
Distribution 
binomial, 26 
determined by moments, 140 
hypergeometric, 29 
331 
Random Graphs 
by Svante Janson, Tomasz Luczak and Andrzej Rucinski 
Copyright © 2000 John Wiley & Sons, Inc. 

332 
INDEX 
lower tail, 31 
of random graph, 1 
upper tail, 31, 48 
Double jump, 111 
Ehrenhfeucht game, 274 
Erdös-Rado arrow notation, 196, 202 
Erdös-Stone-Simonovits Theorem, 205 
Evolution, 103 
Evolutionary path, 137 
Exact copy, 296 
separating, 296 
Exceptional class, 213 
Excess, 112 
Expose-and-merge, 193 
Extension statements, 73 
Extension 
balanced, 58 
rooted, 282 
vertex, 281 
Extinction probability, 107 
First cycle, 134 
First moment method, 54 
First-order language, 272 
FKG inequality, 30, 58 
G-factor, 90 
partial, 90 
Generating functions, 177 
Graph functional, 162 
asymptotically finitely dominated, 168 
dominated, 167 
Graph 
balanced, 58, 64 
bounded, 215 
cubic, 233 
dense, 216 
empty, 7 
K\ -balanced, 65 
^-balanced, 65 
null, 7 
order of, 6 
p-proportional, 172 
regular, 233 
size of, 6 
strictly balanced, 64 
strictly ΓνΊ-balanced, 65 
strictly KVbalanced, 65 
sunshine, 203 
uniform, 223 
whisk, 68, 90 
Hajnal-Szemerédi Theorem, 48, 94 
Half-edge, 235 
Hall Theorem, 82 
Hall's condition, 82 
Hamilton cycles, 105, 239 
Hitting time, 19, 85 
Hypercube, 2 
spanning, 96 
Hypergraph, 199 
3-edge-critical, 199 
chromatic number of, 199 
Holder's inequality, 147 
Independence number, 179 
Isolated 
subgraphs, 79 
trees, 80, 162 
vertices, 80, 84, 156, 161 
Juncture, 115 
Kernel, 122 
i-component, 112 
Laplace transform, 25 
Law of total probability, 8 
Leader, 136 
Leading overlap, 62, 171 
unique, 62 
Lipschitz condition, 38 
Log-normal, 176 
Lollipop graph, 71 
Mantel's theorem, 209-210, 224 
Markov's inequality, 8, 25 
Martingale, 37, 127, 166, 176, 247 
edge exposure, 39 
vertex exposure, 39 
Matching, 7 
Median, 40 
Minimum degree phenomenon, 81, 105 
Moment generating function, 25 
Moments 
factorial, 144 
method of, 66, 140, 237 
Monochromatic triangles, 209 
Neighborhood, 7 
closed, 7 
Pair density, 213 
scaled, 212 
Partition 
edge set, 202 
index of, 213 
regular, 213 
sparsely regular, 216 
vertex set, 196 
Perfect matching, 82, 105, 244, 261 
fractional, 102 
in hypergraph, 100 
Persistent model, 294 
Phase 
critical, 104 
subcritical, 104, 112 
supercritical, 104, 115 
transition, 104 
Projection 
first, 162, 176 
second, 164 

INDEX 
333 
Proper edge, 281 
Property 
convex, 12 
decreasing, 12 
extremal, 208 
graph, 7, 13 
increasing, 12, 261 
monotone, 12 
partition, 208 
semi-induced, 160 
Quantifier depth, 272 
Race of components, 136 
Ramsey property, 1 
edge, 202 
nonsymmetric, 208 
vertex, 196 
Random graph process, 4 
bipartite, 83 
continuous, 4 
restricted, 5 
the, 4, 85 
Random graph, 1 
binomial, 2 
bipartite, 2, 82 
distribution of, 1 
evolution, 103 
regular bipartite, 241 
regular, 3, 233 
tripartite, 226 
uniform, 3 
Random hypergraph, 100, 176, 208 
Random permutation, 205, 234, 263 
Random regular multigraph, 235 
Random subset process, 13 
Random subset, 5 
of integers, 54, 208 
of vertices, 48, 227 
Random tournament, 3, 176 
Random variable 
indicator, 8 
moments of, 140 
standardized, 139 
zero-one, 8 
Random variables 
negatively related, 154 
positively related, 154 
Recursive bound behavior, 292 
Recursive function, 289 
Regular pair, 212 
sparsely, 216 
Regularity Lemma 
sparse, 215 
Szemerédi, 213 
Reliability network, 2 
Roof, 63 
Root, 68, 281 
Rooted graph, 68, 73, 281 
balanced, 69, 74 
dense, 281 
rigid, 281 
safe, 281 
sparse, 281 
strictly balanced, 69, 74 
Scaling factor, 212 
Second moment method, 54, 241 
Semi-factorial, 140 
Semi-invariants, 145 
Separability, 301 
Shamir problem, 96 
Signature, 293 
Stability number, 7, 43, 179 
Stirling numbers, 144 
Stirling's formula, 113 
Subgraph count, 67, 141, 145, 150, 157, 
160, 164, 170, 173 
induced, 171 
Subgraph plot, 63 
Subgraph 
grounded, 74 
induced, 78, 7 
isolated, 79 
primal, 74 
solitary, 79 
spanned, 7 
spanning, 2 
triangle-free, 230 
Subpartition, 213 
Subsubsequence principle, 12 
Suen's inequality, 34 
Sum scheme, 293 
Switching theorem, 137 
Talagrand's inequality, 39 
general form, 43 
Threshold, 18 
coarse, 21 
sharp, 21 
subgraph containment, 55 
width, 21 
Total variation distance, 153 
Trahtenbrot's theorem, 303 
Triangle-factor, 97 
Triplet, 223 
exact, 223 
Turan Theorem, 76, 209 
for random graphs, 210 
Two-round exposure, 6 
Vertex degree, 7, 160 
Zero-one law, 271 
defect, 300 
weak, 271, 286 

WILEY-INTERSCIENCE 
SERIES IN DISCRETE MATHEMATICS AND OPTIMIZATION 
ADVISORY EDITORS 
RONALD L. GRAHAM 
AT & TLaboratories. Florham Park, New Jersey. U.S.A. 
JAN KAREL LENSTRA 
Department of Mathematics and Computer Science. 
Eindhoven University of Technology. Eindhoven. The Netherlands 
AARTS AND KORST · Simulated Annealing and Boltzmann Machines: A Stochastic Approach to 
Combinatorial Optimization and Neural Computing 
AARTS AND LENSTRA · Local Search in Combinatorial Optimization 
ALON, SPENCER, AND ERDÓS · The Probabilistic Method 
ANDERSON AND NASH · Linear Programming in Infinite-Dimensional Spaces: Theory and 
Application 
AZENCOTT · Simulated Annealing: Parallelization Techniques 
BARTHÉLEMY AND GUÉNOCHE · Trees and Proximity Representations 
BAZARRA, JARVIS, AND SHERALI · Linear Programming and Network Flows 
CHANDRU AND HOOKER · Optimization Methods for Logical Inference 
CHONO AND ZAK · An Introduction to Optimization 
COFFMAN AND LUEKER · Probabilistic Analysis of Packing and Partitioning Algorithms 
COOK, CUNNINGHAM, PULLEYBLANK. AND SCHRUVER · Combinatorial Optimization 
DASKIN · Network and Discrete Location: Modes, Algorithms and Applications 
DINITZ AND STINSON · Contemporary Design Theory: A Collection of Surveys 
DU AND KO · Theory of Computational Complexity 
ERICKSON · Introduction to Combinatorics 
GLOVER, KLINGHAM, AND PHILLIPS · Network Models in Optimization and Their Practical 
Problems 
GOLSHTEIN AND TRETYAKOV · Modified Lagrangians and Monotone Maps in Optimization 
GONDRAN AND MINOUX · Graphs and Algorithms (Translated by S. Vajdä) 
GRAHAM, ROTHSCHILD, AND SPENCER · Ramsey Theory, Second Edition 
GROSS AND TUCKER · Topological Graph Theory 
HALL · Combinatorial Theory, Second Edition 
JANSON, LUCZAK, AND RUCINSKI · Random Graphs 
JENSEN AND TOFT · Graph Coloring Problems 
KAPLAN · Maxima and Minima with Applications: Practical Optimization and Duality 
LAWLER, LENSTRA, RINNOOY KAN, AND SHMOYS, Editors . The Traveling Salesman 
Problem: A Guided Tour of Combinatorial Optimization 
LAYWINE AND MULLEN · Discrete Mathematics Using Latin Squares 
LEVITIN · Perturbation Theory in Mathematical Programming Applications 
MAHMOUD · Evolution of Random Search Trees 
MARTELLI · Introduction to Discrete Dynamical Systems and Chaos 
MARTELLO AND TOTH · Knapsack Problems: Algorithms and Computer Implementations 
McALOON AND TRETKOFF · Optimization and Computational Logic 
MINC · Nonnegative Matrices 
MINOUX · Mathematical Programming: Theory and Algorithms (Translated by S. Vajdä) 
MIRCHANDANI AND FRANCIS, Editors · Discrete Location Theory 
NEMHAUSER AND WOLSEY · Integer and Combinatorial Optimization 
NEMIROVSKY AND YUDIN · Problem Complexity and Method Efficiency in Optimization 
(Translated bv E. R. Dawson) 
PACH AND AGARWAL · Combinatorial Geometry 
PLESS · Introduction to the Theory of Error-Correcting Codes, Third Edition 
ROOS AND VIAL · Ph. Theory and Algorithms for Linear Optimization: An Interior Point Approach 
Random Graphs 
by Svante Janson, Tomasz Luczak and Andrzej Rucinski 
Copyright © 2000 John Wiley & Sons, Inc. 

SCHEINERMAN AND ULLMAN · Fractional Graph Theory: A Rational Approach to the Theory of 
Graphs 
SCHRUVER · Theory of Linear and Integer Programming 
TOMESCU · Problems in Combinatorics and Graph Theory (Translated by R. A. Melter) 
TUCKER · Applied Combinatorics, Second Edition 
WOLSEY · Integer Programming 
YE · Interior Point Algorithms: Theory and Analysis 

