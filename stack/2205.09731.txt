Towards Uniﬁed Keyframe Propagation Models
Patrick Esser1
Peter Michael1,2
Soumyadip Sengupta2
1Runway ML
2University of Washington
https://github.com/runwayml/guided-inpainting
Keyframes
Target Frame
Transformer
Ours
LaMa
Figure 1. Single image inpainting approaches such as LaMa [27] (third col.) cannot propagate context from keyframes (second col.) to a
target frame (ﬁrst col.). By aggregating features globally across frames, transformer-based approaches (fourth col.) can propagate coarse
context about a blue object that is visible in the keyframes but not in the target frame. However, it fails to propagate high-frequency details
about object locations and textures, resulting in repetitive patterns and artifacts. By modeling both local and global interactions within and
across frames, our approach (last col.) successfully propagates high-frequency context and accurately reconstructs the background.
Abstract
Many video editing tasks such as rotoscoping or object
removal require the propagation of context across frames.
While transformers and other attention-based approaches
that aggregate features globally have demonstrated great
success at propagating object masks from keyframes to the
whole video, they struggle to propagate high-frequency de-
tails such as textures faithfully. We hypothesize that this
is due to an inherent bias of global attention towards low-
frequency features. To overcome this limitation, we present
a two-stream approach, where high-frequency features in-
teract locally and low-frequency features interact globally.
The global interaction stream remains robust in difﬁcult sit-
uations such as large camera motions, where explicit align-
ment fails. The local interaction stream propagates high-
frequency details through deformable feature aggregation
and, informed by the global interaction stream, learns to
detect and correct errors of the deformation ﬁeld. We eval-
uate our two-stream approach for inpainting tasks, where
experiments show that it improves both the propagation of
features within a single frame as required for image inpaint-
ing, as well as their propagation from keyframes to target
frames. Applied to video inpainting, our approach leads to
44% and 26% improvements in FID and LPIPS scores.
1. Introduction
To satisfy the ever-increasing demand for video content,
new solutions for fast and simple video creation are re-
quired. By reducing the need for tedious frame-by-frame
operations, machine learning can help creators to focus on
the creative aspects of story-telling. A promising approach
to reduce manual work without limiting creativity is based
on keyframe propagation, where the desired result is spec-
iﬁed on a few frames and automatically propagated to the
entire video. This formulation and the need to propagate
context across frames is common to many video editing
tasks. For example, to separate foreground objects from the
background, object selection masks must be propagated, to
stylize videos, artistic edits of frames must be propagated,
and to remove objects from videos, the background must be
propagated from frames where it is visible (see Fig. 1).
Given this commonality, it is natural to ask whether a sin-
gle uniﬁed approach can handle all of these tasks. A promis-
ing candidate for such a solution is a transformer [30] based
architecture. Not only has it been a driving force of consol-
idation across a wide range of tasks and modalities [10,11],
but, by relying on global, afﬁnity-weighted feature aggrega-
tion, it has the potential to propagate context between multi-
ple non-aligned frames. Furthermore, transformers and re-
lated approaches that globally aggregate features have been
successfully applied to object mask propagation [4,22].
1
arXiv:2205.09731v1  [cs.CV]  19 May 2022

Target Encoder
LIF0
GIF0
Read
Deformable 
Write
Decoder
Learned Parameter
Keyframe Encoder
LIF0
GIF0
Learned Parameter
K x Cross-Frame Blocks
LIFK
GIFK
L-K x Intra-Frame Blocks
LIFL
ADD 
IMAGE
ADD 
IMAGE
+Mask/
trimap
Global 
Interaction
Local 
Interaction
Read
Write
Global 
Interaction
Local 
Interaction
Read
Write
Global 
Interaction
Local 
Interaction
Local Interaction
Global Interaction
via Convolution
via Attention
Read
Write
via Gated Fusion
Locally Interacting 
Feature Stream
Globally Interacting 
Feature Stream
Figure 2.
Overview of our model operating with two streams: A locally interacting feature (LIF) stream, relying on convolutions for
high-frequency modeling, and a globally interacting feature (GIF) stream, relying on attention for low-frequency modeling.
However, Fig. 1 shows that a direct adaptation of
transformers to keyframe-guided inpainting for object re-
moval fails to propagate details about the background from
keyframes. This observation is in agreement with the ﬁnd-
ings of [24], that global feature aggregation via attention
acts as a low-pass ﬁlter, which hinders propagation of high-
frequency details. Based on this, we present a two-stream
model consisting of a stream of locally interacting features
(LIF) and a stream of globally interacting features (GIF).
To avoid loss of high-frequency details, LIF operates on a
high-resolution representation and uses local convolutional
operations, which exhibit the opposite behavior of attention
and act as high-pass-ﬁlters [24]. High-frequency details are
then propagated with a deformable feature aggregation be-
tween LIF streams of different frames, whereas robust low-
frequency features are propagated with attention in the GIF
stream. (see Fig. 2). Experiments demonstrate that this de-
sign improves propagation both within a single frame, eval-
uated via image inpainting, as well as across frames, evalu-
ated via guided image inpainting and video inpainting.
2. Background
Transformers in Vision Following the success of trans-
formers in natural language processing [26, 30], their ar-
chitecture has been widely adopted for vision tasks [6,12].
While the attention mechanism is permutation invariant and
can model all pairwise feature interactions, its quadratic
complexity with respect to input length requires adaptions
for high-dimensional image data [10]. Many vision-speciﬁc
modiﬁcations such as windowed attention [17] introduce
assumptions about possible interactions and thus require
aligned input data, making them unsuitable for keyframe
propagation. Other adaptations are based on approximative
attentions mechanisms [5, 31] or attention on compressed
representations [7, 20] which introduces a bottleneck for
propagating features. Even without such bottlenecks, re-
cent work [24] shows that the attention mechanism is in-
herently biased towards low-frequency features. Hence, we
aim to better understand what is required for faithful prop-
agation of both low- and high-frequency content in order to
get closer to a uniﬁed approach for keyframe propagation.
Frame Propagation Propagation of information such as
color or object masks across video frames can be formu-
lated as optimization problems [1, 15]. Since these lead to
slow runtimes, deep learning-based approaches have been
explored as faster alternatives [19, 37] or as more accu-
rate but slower alternatives [3] by ﬁne-tuning models per
video. Instead of relying on a sequential forward propaga-
tion through the video, space-time memory networks [22],
which bear resemblance to single-layer transformers, en-
able propagation of object masks from arbitrary keyframes.
Inpainting Patch-based synthesis [2,32] provides an el-
egant framework that demonstrates how both image- and
video-inpainting can be formulated as propagation prob-
lems. For improved runtimes and to beneﬁt from learned
data priors, deep learning-based solutions have been pro-
posed for image [18,21,27,34,36,38] and video inpainting
[8, 9, 13, 14, 23, 33, 35]. Most video inpainting approaches
rely on sequential forward prediction in time, which lim-
its their applicability in interactive scenarios.
Closest to
our work on keyframe-guided inpainting are [41] and [39],
but both rely on accurate estimation of scene geometry
for alignment between frames, which limits applications to
static scenes [39]. In contrast, we only utilize a very rough
alignment based on optical ﬂow which is interpolated in
masked areas. While this introduces errors and deforma-
tions, our model can detect and correct them based on local
and global feature interactions (see Fig. 3).
2

keyframes
target
Aligned
LaMa
Transformer
Ours
Figure 3. Qualitative results on guided image inpainting.
3. Approach
Our goal is to develop a model that can propa-
gate information from keyframes to target frames.
Let
xtgt
∈
RHin×Win×Ctgt be the target frame and xsrc
∈
RT ×Hin×Win×Csrc be a sequence of keyframes. For inpaint-
ing, Ctgt = Csrc = 4, containing RGB intensities of masked
frames and the mask. The completion of the target frame
depends on unmasked content from both xtgt and xsrc.
3.1. Feature Encoding and Decoding
LIF Encoding The main goal of the LIF stream is to
preserve and propagate high-frequency details. To initialize
this stream, we follow previous works and use a strided con-
volutional architecture to extract features from the inputs.
By keeping the stride s relatively small while expanding the
number of feature channels c, (in practice we use a stride
s = 8 with c = 512 channels), these features, together with
a similar deconvolutional architecture, provide an almost
lossless feature encoding. We encode each keyframe and
the target frame individually and model interactions only af-
terwards. Thus, every frame represented in the LIF stream
by h0
LIF ∈RH×W ×c, where H = Hin
s and W = Win
s .
GIF Initialization The goal of the GIF stream is to
model robust low-frequency interactions within and across
frames. We initialize this stream with a set of learned pa-
rameters h0
GIF ∈RM×d, where M is the sequence length
and controls the computational complexity of the stream.
The learned parameters serve as a positional encoding.
Decoding The encoding stage results in two feature rep-
resentations, h0
LIF, h0
GIF = fenc(x) for each frame x. These
will be processed by a sequence of L blocks which produce
hL
LIF, hL
GIF. The ﬁnal output is then predicted from the hL
LIF
representation of the target frame, i.e. y = fdec(hL
LIF). Next,
we describe how the blocks map hi
LIF, hi
GIF 7→hi+1
LIF , hi+1
GIF .
3.2. Intra-Frame and Inter-Stream Interactions
The LIF stream consists of convolutional residual blocks
for local interaction, while the GIF stream consists of atten-
tion blocks for global interactions. Read and write opera-
tions exchange complementary features between them.
Read Operation Since generally M < N := H ·W, we
have to average LIF features or select a subset of them to
read into the GIF stream. We use a read module that learns
how to read features from the LIF into the GIF stream.
We assume that M = m2 is a square number and that
H ≡0 mod m and W ≡0 mod m (otherwise interpo-
lating to the closest height and width that is divisible by m).
We divide the LIF feature map into M equal-sized patches
and learn which features of the j-th patch to read into the
j-th GIF feature by forming an importance weighted sum
of projected features. If Pj ∈R( H
m · W
m )×c is the j-th patch,
read(Pj) = softmax(WSP t
j )PjWV ,
(1)
where WS ∈R1×c and WV ∈Rc×d are learnable weights.
We follow common design choices of attention and (i) use
multiple read heads with their own score and value projec-
tions and (ii) apply a Layer Normalization followed by a lin-
ear projection and a residual connection on the GIF stream.
Write Operation After processing the GIF stream with
attention blocks, we write the result back into the high-
capacity LIF stream. We use a similar approach as for the
read operation, except that the attention weights are now
computed on the transposed patch and values are computed
from the GIF stream. If Pj ∈R( H
m · W
m )×c is the j-th LIF
patch and Gj ∈R1×d is the j-th GIF feature,
write(Pj, Gj) = sigmoid(PjW t
S)GjW t
V ,
(2)
with WS ∈R1×c and WV ∈Rc×d. As in the read opera-
tion, we use multiple write heads followed by layer norm,
projection, and a residual connection on the LIF stream.
Together, a read operation, local and global interactions
in the LIF and GIF streams, followed by a write operation
represent one intra-frame block as shown in Fig. 2.
3.3. Cross-Frame Interactions
Global Cross-Frame Propagation To propagate in-
formation from keyframes to the target, we replace self-
attention in the GIF stream with cross-attention. We use a
symmetric design, where the same block is used for self-
attention on keyframes and for cross-attention on target
frames, by concatenating keys and values from keyframes.
If gsrc ∈RM×d and gtgt ∈RM×d denote GIF features from
a keyframe and the target frame, respectively, we compute
the attention operation A for keyframes and target as
A(gsrc, ∅) = softmax(gsrcW t
Q(gsrcW t
K)t)(gsrcWV )
A(gtgt, gsrc) = softmax(gtgtW t
Q([gtgt, gsrc]W t
K)t)[gtgt, gsrc]WV ,
with WQ ∈Rdk×d, WK ∈Rdk×d, and WV ∈Rd×d.
3

Method
FID ↓
LPIPS ↓
SSIM ↑
Transformer
15.19
0.173± 0.072
0.814± 0.097
Ours w/o FFC
14.19
0.155± 0.075
0.831± 0.096
LaMa
13.73
0.147± 0.077
0.836± 0.096
Ours
13.15
0.145± 0.074
0.837± 0.095
Table 1. Single Image Inpainting Results on Places [40].
Method
FID ↓
LPIPS ↓
SSIM ↑
LaMa + SpectralFuse
26.40
0.211± 0.115
0.794± 0.138
Transformer
18.18
0.175± 0.094
0.821± 0.128
Our w/o FFC
14.78
0.140± 0.082
0.851± 0.110
Ours
14.29
0.137± 0.080
0.848± 0.110
Ours (big)
13.09
0.130± 0.074
0.865± 0.101
Table 2. Guided Image Inpainting Results on Places [40].
Method
FID ↓
LPIPS ↓
SSIM ↑
VFID ↓
PVCS ↓
PCons ↑
Ours 14+prealign
4.34
0.0034
0.9630
0.0543
0.2263
42.39
Ours 14
4.45
0.0042
0.9597
0.0542
0.2339
41.45
Ours 6
4.68
0.0051
0.9581
0.0561
0.2443
41.14
LaMa [27] +ﬂow
5.28
0.0082
0.9459
0.0877
0.3151
40.22
DFCNet [33]
7.71
0.0054 (0.0056)
0.9500
0.0627
0.2240
57.80
E2FGVI [16]
8.22
0.0046
0.9585
0.0613
0.2519
33.19
FGVC [8]
9.76
0.0065 (0.0054)
0.9548
0.0564
0.2322
38.26
JointOpt [9]
7.30
0.0058 (0.0059)
0.9559
0.0530
0.2324
37.19
OPN [23]
8.71
0.0062 (0.0057)
0.9525
0.0708
0.2874
35.09
CPNet [14]
13.21
0.0056 (0.0068)
0.9574
0.0969
0.3048
43.91
STTN [35]
10.89
0.0083 (0.0065)
0.9542
0.0911
0.3149
41.12
VINet [13]
23.63
0.0090 (0.0084)
0.9481
0.1488
0.4312
50.07
Table 3. Video Inpainting Result on DEVIL [28].
Deformable Write Operation To overcome limitations
of attention-based propagation of high-frequency features,
we introduce a complementary approach based on an ex-
plicit alignment with a deformable write module that fuses
LIF features from keyframes into those of the target frame.
Let hi
LIF ∈RH×W ×c denote LIF features of keyframe i,
and φi ∈RH×W ×2 a ﬂow ﬁeld such that a position p in the
target frame corresponds to the position p+φi(p) in the i-th
keyframe. We write [hi
LIF(p+φi(p))] ∈RT ×c to denote the
stacked, bilinearly interpolated values of hi
LIF at positions
p + φi(p). The deformable write operation then aggregates
these interpolated features based on conﬁdence scores,
X
softmax([hi
LIF(p + φi(p))]W T
Q) ⊙[hi
LIF(p + φi(p))]WV
where ⊙is elementwise multiplication, WQ ∈Rc×c and
WV ∈Rc×c are learnable parameters, and the sum and soft-
max are applied along the stacked dimension. The result is
added to the LIF feature stream of the target frame.
In contrast to global cross-frame interaction, this opera-
tion can propagate high-frequency details but it is also more
susceptible to errors due to wrongly estimated ﬂow ﬁelds
φi. For the estimation of conﬁdence scores, we thus con-
catenate forward-backward consistency values of the ﬂow
ﬁeld. The ﬂow ﬁelds are estimated with a pre-trained optical
ﬂow model [29] operating at a ﬁxed resolution of 256×256
and interpolated to the spatial size of the LIF features.
4. Experiments
We evaluate the propagation capabilities of our model on
different inpainting tasks. Image inpainting requires context
propagation within a frame, while guided inpainting and
video inpainting require context propagation across frames
to predict plausible and consistent content in masked areas.
We use LaMa [27], which relies on Fast Fourier Con-
volutions (FFCs) to take global context into account, as a
strong baseline and follow its training and evaluation pro-
tocol based on the Places dataset [40]. Tab. 1 compares the
inpainting performance of different approaches under the
same computational budget, which is calibrated through the
number of blocks. A transformer approach, which is sim-
ilar to our model without the LIF stream, performs worst,
demonstrating its difﬁculties to faithfully propagate con-
text.
Our approach with spatial convolutions in the LIF
stream (Ours w/o FFC) improves performance but achieves
worse performance than LaMa. However, in combination
with FFCs in the LIF stream (Ours), our model achieves the
best performance, suggesting complementary properties of
spectral convolutions and propagation capabilities.
To evaluate context propagation across frames, we gen-
erate two synthetic keyframes for each example by apply-
ing random transformations, deformations, and masks. We
further investigate propagation in the spectral domain and
include a variant of LaMa that aggregates Fourier features
across frames (Lama+SpectralFuse). Both the quantitative
results in Tab. 2, as well as the qualitative results in Fig. 3
show that this fails to propagate context unless a keyframe
happens to be aligned with the target frame (second row in
Fig. 3). The transformer-based approach can take keyframe-
context into account but fails to propagate high-frequency
details.
Our approach performs signiﬁcantly better than
these two baselines both with and without FFCs, which
demonstrates the improved propagation capabilities.
Finally, we evaluate how well our approach generalizes
to real videos. We train a bigger variant of our best perform-
ing model for guided inpainting (Ours (big)), and use it to
generate every 20th frame of a masked video. The remain-
ing frames are propagated based on optical ﬂow. We use the
DEVIL evaluation [28] to compare different approaches in
Tab. 3. The subscript indicates the number of keyframes
used.
We also include results obtained by propagating
frames inpainted without context as Lama [27]+ﬂow. Over-
all, we observe increasing performance with an increasing
number of keyframes available, which demonstrates that
our approach is able to propagate context across frames and
that this leads to improved video inpainting performance.
5. Conclusion
We propose a novel approach for keyframe propagation
based on a two-stream approach, where one stream mod-
els the local interactions of high-frequency features and the
other models the global interactions of low-frequency fea-
tures. We show that this method improves image and video
inpainting performance. We are investigating applications
of this architecture to other tasks such as object segmenta-
tion and matting, with the goal to ﬁnd a uniﬁed approach.
4

References
[1] Aseem Agarwala,
Aaron Hertzmann,
D. Salesin,
and
Steven M. Seitz. Keyframe-based tracking for rotoscoping
and animation. In SIGGRAPH 2004, 2004. 2
[2] Connelly Barnes, Eli Shechtman, Adam Finkelstein, and
Dan B. Goldman.
Patchmatch: a randomized correspon-
dence algorithm for structural image editing. In SIGGRAPH
2009, 2009. 2
[3] Sergi Caelles, Kevis-Kokitsi Maninis, Jordi Pont-Tuset,
Laura Leal-Taix´e, Daniel Cremers, and Luc Van Gool. One-
shot video object segmentation. 2017 IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), pages
5320–5329, 2017. 2
[4] Ho Kei Cheng, Yu-Wing Tai, and Chi-Keung Tang. Rethink-
ing space-time networks with improved memory coverage
for efﬁcient video object segmentation. In NeurIPS, 2021. 1
[5] Krzysztof Choromanski, Valerii Likhosherstov, David Do-
han, Xingyou Song, Andreea Gane, Tam´as Sarl´os, Peter
Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser,
David Belanger, Lucy J. Colwell, and Adrian Weller. Re-
thinking attention with performers. CoRR, abs/2009.14794,
2020. 2
[6] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-
vain Gelly, et al. An image is worth 16x16 words: Trans-
formers for image recognition at scale.
arXiv preprint
arXiv:2010.11929, 2020. 2
[7] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming
transformers for high-resolution image synthesis.
In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition (CVPR), pages 12873–12883, June
2021. 2
[8] Chen Gao, Ayush Saraf, Jia-Bin Huang, and Johannes Kopf.
Flow-edge guided video completion. In ECCV, 2020. 2, 4,
7, 11, 12, 13
[9] Jia-Bin Huang, Sing Bing Kang, Narendra Ahuja, and Jo-
hannes Kopf. Temporally coherent completion of dynamic
video. ACM Transactions on Graphics (TOG), 35:1 – 11,
2016. 2, 4, 7
[10] Andrew Jaegle, Felix Gimeno, Andy Brock, Oriol Vinyals,
Andrew Zisserman, and Joao Carreira. Perceiver: General
perception with iterative attention. In International Confer-
ence on Machine Learning, pages 4651–4664. PMLR, 2021.
1, 2
[11] Andrej Karpathy. The ongoing consolidation in AI is incred-
ible. https://twitter.com/karpathy/status/
1468370605229547522. Accessed: 2022-03-19. 1
[12] Salman H. Khan, Muzammal Naseer, Munawar Hayat,
Syed Waqas Zamir, Fahad Shahbaz Khan, and Mubarak
Shah.
Transformers in vision:
A survey.
CoRR,
abs/2101.01169, 2021. 2
[13] Dahun Kim, Sanghyun Woo, Joon-Young Lee, and In So
Kweon. Deep video inpainting. 2019 IEEE/CVF Confer-
ence on Computer Vision and Pattern Recognition (CVPR),
pages 5785–5794, 2019. 2, 4, 7
[14] SungHo Lee, Seoung Wug Oh, DaeYeun Won, and Seon Joo
Kim. Copy-and-paste networks for deep video inpainting.
2019 IEEE/CVF International Conference on Computer Vi-
sion (ICCV), pages 4412–4420, 2019. 2, 4, 7
[15] Anat Levin, Dani Lischinski, and Yair Weiss. Colorization
using optimization. ACM SIGGRAPH 2004 Papers, 2004. 2
[16] Zhen Li, Cheng-Ze Lu, Jianhua Qin, Chun-Le Guo, and
Ming-Ming Cheng. Towards an end-to-end framework for
ﬂow-guided video inpainting. In IEEE Conference on Com-
puter Vision and Pattern Recognition (CVPR), 2022. 4, 7,
11, 12, 13
[17] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng
Zhang, Stephen Lin, and Baining Guo. Swin transformer:
Hierarchical vision transformer using shifted windows. In
Proceedings of the IEEE/CVF International Conference on
Computer Vision, pages 10012–10022, 2021. 2
[18] Yuqing Ma, Xianglong Liu, Shihao Bai, Le-Yi Wang, Aishan
Liu, Dacheng Tao, and Edwin Hancock. Region-wise gen-
erative adversarial imageinpainting for large missing areas.
ArXiv, abs/1909.12507, 2019. 2
[19] Simone Meyer, Victor Cornill`ere, Abdelaziz Djelouah,
Christopher Schroers, and Markus H. Gross.
Deep video
color propagation. In BMVC, 2018. 2
[20] Charlie Nash, Jacob Menick, Sander Dieleman, and Peter W.
Battaglia.
Generating images with sparse representations.
CoRR, abs/2103.03841, 2021. 2
[21] Kamyar Nazeri, Eric Ng, Tony Joseph, Faisal Z. Qureshi, and
Mehran Ebrahimi. Edgeconnect: Generative image inpaint-
ing with adversarial edge learning. ArXiv, abs/1901.00212,
2019. 2
[22] Seoung Wug Oh, Joon-Young Lee, N. Xu, and Seon Joo
Kim.
Video object segmentation using space-time mem-
ory networks. 2019 IEEE/CVF International Conference on
Computer Vision (ICCV), pages 9225–9234, 2019. 1, 2
[23] Seoung Wug Oh, SungHo Lee, Joon-Young Lee, and
Seon Joo Kim. Onion-peel networks for deep video com-
pletion. 2019 IEEE/CVF International Conference on Com-
puter Vision (ICCV), pages 4402–4411, 2019. 2, 4, 7
[24] Namuk Park and Songkuk Kim. How do vision transformers
work? In International Conference on Learning Represen-
tations, 2022. 2
[25] F. Perazzi, J. Pont-Tuset, B. McWilliams, L. Van Gool, M.
Gross, and A. Sorkine-Hornung. A benchmark dataset and
evaluation methodology for video object segmentation. In
Computer Vision and Pattern Recognition, 2016. 7, 11, 12,
13
[26] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya
Sutskever. Improving language understanding by generative
pre-training. 2018. 2
[27] Roman Suvorov, Elizaveta Logacheva, Anton Mashikhin,
Anastasia Remizova, Arsenii Ashukha, Aleksei Silvestrov,
Naejin Kong, Harshith Goka, Kiwoong Park, and Victor S.
Lempitsky.
Resolution-robust large mask inpainting with
fourier convolutions. ArXiv, abs/2109.07161, 2021. 1, 2,
4, 7, 11, 12, 13
[28] Ryan Szeto and Jason J. Corso. The DEVIL is in the de-
tails: A diagnostic evaluation benchmark for video inpaint-
ing. CoRR, abs/2105.05332, 2021. 4, 7
5

[29] Zachary Teed and Jia Deng. RAFT: recurrent all-pairs ﬁeld
transforms for optical ﬂow. CoRR, abs/2003.12039, 2020. 4
[30] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-
reit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia
Polosukhin. Attention is all you need. 2017. 1, 2
[31] Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, and
Hao Ma. Linformer: Self-attention with linear complexity.
arXiv preprint arXiv:2006.04768, 2020. 2
[32] Yonatan Wexler, Eli Shechtman, and Michal Irani. Space-
time completion of video.
IEEE Transactions on Pattern
Analysis and Machine Intelligence, 29, 2007. 2
[33] Rui Xu, Xiaoxiao Li, Bolei Zhou, and Chen Change Loy.
Deep ﬂow-guided video inpainting. 2019 IEEE/CVF Confer-
ence on Computer Vision and Pattern Recognition (CVPR),
pages 3718–3727, 2019. 2, 4, 7, 11, 12, 13
[34] Jiahui Yu, Zhe L. Lin, Jimei Yang, Xiaohui Shen, Xin Lu,
and Thomas S. Huang.
Free-form image inpainting with
gated convolution. 2019 IEEE/CVF International Confer-
ence on Computer Vision (ICCV), pages 4470–4479, 2019.
2
[35] Yanhong Zeng, Jianlong Fu, and Hongyang Chao. Learning
joint spatial-temporal transformations for video inpainting.
ArXiv, abs/2007.10247, 2020. 2, 4, 7
[36] Yanhong Zeng, Jianlong Fu, Hongyang Chao, and Bain-
ing Guo. Aggregated contextual transformations for high-
resolution image inpainting. ArXiv, abs/2104.01431, 2021.
2
[37] Richard Zhang, Jun-Yan Zhu, Phillip Isola, Xinyang Geng,
Angela S. Lin, Tianhe Yu, and Alexei A. Efros.
Real-
time user-guided image colorization with learned deep pri-
ors. ACM Transactions on Graphics (TOG), 36:1 – 11, 2017.
2
[38] Shengyu Zhao, Jianwei Cui, Yilun Sheng, Yue Dong, Xiao
Liang, Eric I-Chao Chang, and Yan Xu.
Large scale im-
age completion via co-modulated generative adversarial net-
works. ArXiv, abs/2103.10428, 2021. 2
[39] Yunhan Zhao, Connelly Barnes, Yuqian Zhou, Eli Shecht-
man, Sohrab Amirghodsi, and Charless Fowlkes. Geoﬁll:
Reference-based image inpainting of scenes with complex
geometry. arXiv preprint arXiv:2201.08131, 2022. 2
[40] Bolei Zhou, `Agata Lapedriza, Aditya Khosla, Aude Oliva,
and Antonio Torralba. Places: A 10 million image database
for scene recognition. IEEE Transactions on Pattern Analy-
sis and Machine Intelligence, 40:1452–1464, 2018. 4, 7
[41] Yuqian Zhou, Connelly Barnes, Eli Shechtman, and Sohrab
Amirghodsi. Transﬁll: Reference-guided image inpainting
by merging multiple color and spatial transformations. In
Proceedings of the IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition (CVPR), pages 2266–2276,
June 2021. 2
6

Supplementary Materials
We include additional details on the evaluations in Sec. A and additional qualitative results in Sec. B. Implementation details
and video results can be found at https://github.com/runwayml/guided-inpainting.
A. Evaluation Details
(Guided) Image Inpainting In Tab. 1 and 2, we use a subset of 2k images from the Places [40] validation split at a
resolution of 256 × 256 pixels. Ours (big) uses two cross-frame blocks and 12 intra-frame blocks, all other models use two
cross-frame blocks and four intra-frame blocks. We train models with a learning rate of 3.2 · 10−4 and an effective batch size
of 32, accumulated over two batches and distributed across two V100 GPUs.
For guided inpainting, we use two keyframes during training and four for the evaluation in Tab. 2. Evaluation results with
two keyframes can be found in Tab. 4. The performance of all methods improves as context from more keyframes becomes
available and their relative performance-ordering to one another remains the same.
Video Inpainting For the evaluation in Tab. 3, we use the evaluation code1 provided by the DEVIL benchmark [28].
We use the ‘ﬂickr-all‘ video subset combined with the ‘fvi-fgs-h‘ mask subset which contains large inpainting masks and
represents a challenging setting. For DFCNet [33], FGVC [8], JointOpt [9], OPN [23], CPNet [14], STTN [35] and VINet
[13] we use the pre-computed inpainting results on ‘ﬂickr-all fvi-fgs-h‘ provided by [28] and recompute the metrics. This
reproduces the results reported in [28] except for the LPIPS metrics, for which we include the results from [28] in parentheses.
In addition, we compute inpainting results from concurrent video inpainting work E2FGVI [16] using their provided code2
and E2FGVI-HQ checkpoint.
The Ours entries in Tab. 3 always refer to results computed with the Ours (big) checkpoint that contains 12 intra-frame
blocks. The number in the subscript indicates the number of keyframes. For 6, we use frames with offsets ±{10, 20, 40},
and for 14, we use frames with offsets ±{5, 10, 15, 20, 40, 60, 80} to capture more context from the video. The subscript
+prealign refers to an extra step of manually fusing context from other frames in areas where the optical ﬂow is reliable. This
brings some additional gains and hints that there is still room for improvement regarding the propagation of high-frequency
details from other frames. We hypothesize that deformable writes on higher-resolution feature maps and/or the use of skip
connections in the architecture could further improve the propagation capabilities.
To evaluate the importance of taking context from keyframes into account for video inpainting, we also include LaMa
[27]+ﬂow, which uses the big-lama checkpoint, provided3 by the authors of [27], in combination with the same ﬂow-
propagation of intermediate frames as for the Ours entries. This represents a strong non-guided baseline for video inpainting
which can achieve relatively good FID scores but fails to take the context from the video into account as indicated by bad
LPIPS scores. With the ability to take context from other frames into account, our approach can achieve the best scores for
both measures.
Method
FID ↓
LPIPS ↓
SSIM ↑
LaMa + SpectralFuse
26.74
0.212± 0.116
0.759± 0.139
Transformer
18.87
0.181± 0.096
0.802± 0.130
Our w/o FFC
17.62
0.157± 0.091
0.833± 0.118
Ours
16.90
0.154± 0.089
0.834± 0.118
Ours (big)
15.45
0.148± 0.084
0.849± 0.111
Table 4. Guided Image Inpainting Results on Places [40] using two keyframes.
B. Qualitative Results
We include additional qualitative results for guided image inpainting in Fig. 5 and 4, and a larger version of Fig. 3 in Fig. 6.
For video inpainting, we show qualitative results for DAVIS [25] sequences in Fig. 7-9. Animated versions for all DAVIS
sequences can be found online at https://github.com/runwayml/guided-inpainting. The results of other
methods for these sequences are taken from the online supplementary4 of [8] when available and recomputed otherwise.
1https://github.com/MichiganCOG/devil
2https://github.com/MCG-NKU/E2FGVI
3https://github.com/saic-mdal/lama
4https://filebox.ece.vt.edu/˜chengao/FGVC/pages/object_removal.html
7

Note that the long-term temporal stability of our method is reduced compared to other approaches, because we compute
each chunk of 20 frames from the video independently. This gives computational speed advantages by enabling parallelization
over chunks as opposed to a fully sequential processing required by other approaches. On shorter timescales, the animated
versions demonstrate that our approach exhibits signiﬁcantly less short-term temporal instabilities like ﬂickering.
keyframes
target
Aligned
LaMa
Transformer
Ours
Figure 4. Qualitative results on guided image inpainting.
8

keyframes
target
Aligned
LaMa
Transformer
Ours
Figure 5. Qualitative results on guided image inpainting.
9

keyframes
target
Aligned
LaMa
Transformer
Ours
Figure 6. A larger version of Fig. 3.
10

Frames
input
Ours 14+prealign
Ours 14
LaMa [27] +ﬂow
DFCNet [33]
E2FGVI [16]
FGVC [8]
Figure 7.
Qualitative results for video inpainting on the DAVIS [25] sequence bear. Animated version at https://github.com/
runwayml/guided-inpainting.
11

Frames
input
Ours 14+prealign
Ours 14
LaMa [27] +ﬂow
DFCNet [33]
E2FGVI [16]
FGVC [8]
Figure 8.
Qualitative results for video inpainting on the DAVIS [25] sequence breakdance-ﬂare. Animated version at https://
github.com/runwayml/guided-inpainting.
12

Frames
input
Ours 14+prealign
Ours 14
LaMa [27] +ﬂow
DFCNet [33]
E2FGVI [16]
FGVC [8]
Figure 9. Qualitative results for video inpainting on the DAVIS [25] sequence breakdance. Animated version at https://github.
com/runwayml/guided-inpainting.
13

